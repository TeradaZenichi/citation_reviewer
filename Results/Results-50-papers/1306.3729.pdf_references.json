[
    {
        "title": "A method of moments for mixture models and hidden Markov models",
        "author": [
            "A. Anandkumar",
            "D. Hsu",
            "S.M. Kakade"
        ],
        "venue": "In Conference on Learning Theory (COLT),",
        "citeRegEx": "Anandkumar et al\\.,? \\Q2012\\E",
        "shortCiteRegEx": "Anandkumar et al\\.",
        "year": 2012,
        "abstract": "Mixture models are a fundamental tool in applied statistics and machine\nlearning for treating data taken from multiple subpopulations. The current\npractice for estimating the parameters of such models relies on local search\nheuristics (e.g., the EM algorithm) which are prone to failure, and existing\nconsistent methods are unfavorable due to their high computational and sample\ncomplexity which typically scale exponentially with the number of mixture\ncomponents. This work develops an efficient method of moments approach to\nparameter estimation for a broad class of high-dimensional mixture models with\nmany components, including multi-view mixtures of Gaussians (such as mixtures\nof axis-aligned Gaussians) and hidden Markov models. The new method leads to\nrigorous unsupervised learning results for mixture models that were not\nachieved by previous works; and, because of its simplicity, it offers a viable\nalternative to EM for practical deployment.",
        "full_text": "arXiv:1203.0683v3  [cs.LG]  5 Sep 2012\nA Method of Moments for Mixture Models and\nHidden Markov Models\nAnimashree Anandkumar1, Daniel Hsu2, and Sham M. Kakade2\n1Department of EECS, University of California, Irvine\n2Microsoft Research New England\nSeptember 7, 2012\nAbstract\nMixture models are a fundamental tool in applied statistics and machine learning for treating\ndata taken from multiple subpopulations. The current practice for estimating the parameters\nof such models relies on local search heuristics (e.g., the EM algorithm) which are prone to\nfailure, and existing consistent methods are unfavorable due to their high computational and\nsample complexity which typically scale exponentially with the number of mixture components.\nThis work develops an e\ufb03cient method of moments approach to parameter estimation for a\nbroad class of high-dimensional mixture models with many components, including multi-view\nmixtures of Gaussians (such as mixtures of axis-aligned Gaussians) and hidden Markov models.\nThe new method leads to rigorous unsupervised learning results for mixture models that were\nnot achieved by previous works; and, because of its simplicity, it o\ufb00ers a viable alternative to\nEM for practical deployment.\n1\nIntroduction\nMixture models are a fundamental tool in applied statistics and machine learning for treating data\ntaken from multiple subpopulations (Titterington et al., 1985). In a mixture model, the data are\ngenerated from a number of possible sources, and it is of interest to identify the nature of the\nindividual sources. As such, estimating the unknown parameters of the mixture model from sam-\npled data\u2014especially the parameters of the underlying constituent distributions\u2014is an important\nstatistical task. For most mixture models, including the widely used mixtures of Gaussians and\nhidden Markov models (HMMs), the current practice relies on the Expectation-Maximization (EM)\nalgorithm, a local search heuristic for maximum likelihood estimation. However, EM has a number\nof well-documented drawbacks regularly faced by practitioners, including slow convergence and\nsuboptimal local optima (Redner and Walker, 1984).\nAn alternative to maximum likelihood and EM, especially in the context of mixture models,\nis the method of moments approach. The method of moments dates back to the origins of mix-\nture models with Pearson\u2019s solution for identifying the parameters of a mixture of two univariate\nE-mail: a.anandkumar@uci.edu, dahsu@microsoft.com, skakade@microsoft.com\n1\nGaussians (Pearson, 1894). In this approach, model parameters are chosen to specify a distribution\nwhose p-th order moments, for several values of p, are equal to the corresponding empirical mo-\nments observed in the data. Since Pearson\u2019s work, the method of moments has been studied and\nadapted for a variety of problems; their intuitive appeal is also complemented with a guarantee of\nstatistical consistency under mild conditions. Unfortunately, the method often runs into trouble\nwith large mixtures of high-dimensional distributions. This is because the equations determining\nthe parameters are typically based on moments of order equal to the number of model parameters,\nand high-order moments are exceedingly di\ufb03cult to estimate accurately due to their large variance.\nThis work develops a computationally e\ufb03cient method of moments based on only low-order\nmoments that can be used to estimate the parameters of a broad class of high-dimensional mixture\nmodels with many components. The resulting estimators can be implemented with standard nu-\nmerical linear algebra routines (singular value and eigenvalue decompositions), and the estimates\nhave low variance because they only involve low-order moments. The class of models covered by\nthe method includes certain multivariate Gaussian mixture models and HMMs, as well as mix-\nture models with no explicit likelihood equations. The method exploits the availability of multiple\nindirect \u201cviews\u201d of a model\u2019s underlying latent variable that determines the source distribution,\nalthough the notion of a \u201cview\u201d is rather general. For instance, in an HMM, the past, present,\nand future observations can be thought of as di\ufb00erent noisy views of the present hidden state; in\na mixture of product distributions (such as axis-aligned Gaussians), the coordinates in the output\nspace can be partitioned (say, randomly) into multiple non-redundant \u201cviews\u201d. The new method\nof moments leads to unsupervised learning guarantees for mixture models under mild rank condi-\ntions that were not achieved by previous works; in particular, the sample complexity of accurate\nparameter estimation is shown to be polynomial in the number of mixture components and other\nrelevant quantities. Finally, due to its simplicity, the new method (or variants thereof) also o\ufb00ers\na viable alternative to EM and maximum likelihood for practical deployment.\n1.1\nRelated work\nGaussian mixture models. The statistical literature on mixture models is vast (a more thorough\ntreatment can be found in the texts of Titterington et al. (1985) and Lindsay (1995)), and many\nadvances have been made in computer science and machine learning over the past decade or so, in\npart due to their importance in modern applications. The use of mixture models for clustering data\ncomprises a large part of this work, beginning with the work of Dasgupta (1999) on learning mix-\ntures of k well-separated d-dimensional Gaussians. This and subsequent work (Arora and Kannan,\n2001; Dasgupta and Schulman, 2007; Vempala and Wang, 2002; Kannan et al., 2005; Achlioptas\nand McSherry, 2005; Chaudhuri and Rao, 2008; Brubaker and Vempala, 2008; Chaudhuri et al.,\n2009) have focused on e\ufb03cient algorithms that provably recover the parameters of the constituent\nGaussians from data generated by such a mixture distribution, provided that the distance between\neach pair of means is su\ufb03ciently large (roughly either dc or kc times the standard deviation of the\nGaussians, for some c > 0). Such separation conditions are natural to expect in many clustering\napplications, and a number of spectral projection techniques have been shown to enhance the sep-\naration (Vempala and Wang, 2002; Kannan et al., 2005; Brubaker and Vempala, 2008; Chaudhuri\net al., 2009). More recently, techniques have been developed for learning mixtures of Gaussians\nwithout any separation condition (Kalai et al., 2010; Belkin and Sinha, 2010; Moitra and Valiant,\n2010), although the computational and sample complexities of these methods grow exponentially\nwith the number of mixture components k. This dependence has also been shown to be inevitable\n2\nwithout further assumptions (Moitra and Valiant, 2010).\nMethod of moments. The latter works of Belkin and Sinha (2010), Kalai et al. (2010), and Moitra\nand Valiant (2010) (as well as the algorithms of Feldman et al. (2005, 2006) for a related but di\ufb00erent\nlearning objective) can be thought of as modern implementations of the method of moments, and\ntheir exponential dependence on k is not surprising given the literature on other moment methods\nfor mixture models. In particular, a number of moment methods for both discrete and continuous\nmixture models have been developed using techniques such as the Vandermonde decompositions\nof Hankel matrices (Lindsay, 1989; Lindsay and Basak, 1993; Boley et al., 1997; Gravin et al.,\n2012). In these methods, following the spirit of Pearson\u2019s original solution, the model parameters\nare derived from the roots of polynomials whose coe\ufb03cients are based on moments up to the\n\u2126(k)-th order. The accurate estimation of such moments generally has computational and sample\ncomplexity exponential in k.\nSpectral approach to parameter estimation with low-order moments. The present work\nis based on a notable exception to the above situation, namely Chang\u2019s spectral decomposition\ntechnique for discrete Markov models of evolution (Chang, 1996) (see also Mossel and Roch (2006)\nand Hsu et al. (2009) for adaptations to other discrete mixture models such as discrete HMMs).\nThis spectral technique depends only on moments up to the third-order; consequently, the resulting\nalgorithms have computational and sample complexity that scales only polynomially in the number\nof mixture components k. The success of the technique depends on a certain rank condition of\nthe transition matrices; but this condition is much milder than separation conditions of clustering\nworks, and it remains su\ufb03cient even when the dimension of the observation space is very large (Hsu\net al., 2009). In this work, we extend Chang\u2019s spectral technique to develop a general method of\nmoments approach to parameter estimation, which is applicable to a large class of mixture models\nand HMMs with both discrete and continuous component distributions in high-dimensional spaces.\nLike the moment methods of Moitra and Valiant (2010) and Belkin and Sinha (2010), our algorithm\ndoes not require a separation condition; but unlike those previous methods, the algorithm has\ncomputational and sample complexity polynomial in k.\nSome previous spectral approaches for related learning problems only use second-order moments,\nbut these approaches can only estimate a subspace containing the parameter vectors and not the\nparameters themselves (McSherry, 2001). Indeed, it is known that the parameters of even very sim-\nple discrete mixture models are not generally identi\ufb01able from only second-order moments (Chang,\n1996)1. We note that moments beyond the second-order (speci\ufb01cally, fourth-order moments) have\nbeen exploited in the methods of Frieze et al. (1996) and Nguyen and Regev (2009) for the prob-\nlem of learning a parallelepiped from random samples, and that these methods are very related to\ntechniques used for independent component analysis (Hyv\u00a8arinen and Oja, 2000). Adapting these\ntechniques for other parameter estimation problems is an enticing possibility.\nMulti-view learning. The spectral technique we employ depends on the availability of multiple\nviews, and such a multi-view assumption has been exploited in previous works on learning mixtures\nof well-separated distributions (Chaudhuri and Rao, 2008; Chaudhuri et al., 2009). In these previous\nworks, a projection based on a canonical correlation analysis (Hotelling, 1935) between two views is\nused to reinforce the separation between the mixture components, and to cancel out noise orthogonal\nto the separation directions. The present work, which uses similar correlation-based projections,\nshows that the availability of a third view of the data can remove the separation condition entirely.\n1See Appendix G for an example of Chang (1996) demonstrating the non-identi\ufb01ability of parameters from only\nsecond-order moments in a simple class of Markov models.\n3\nThe multi-view assumption substantially generalizes the case where the component distributions\nare product distributions (such as axis-aligned Gaussians), which has been previously studied in the\nliterature (Dasgupta, 1999; Vempala and Wang, 2002; Chaudhuri and Rao, 2008; Feldman et al.,\n2005, 2006); the combination of this and a non-degeneracy assumption is what allows us to avoid\nthe sample complexity lower bound of Moitra and Valiant (2010) for Gaussian mixture models. The\nmulti-view assumption also naturally arises in many applications, such as in multimedia data with\n(say) text, audio, and video components (Blaschko and Lampert, 2008; Chaudhuri et al., 2009); as\nwell as in linguistic data, where the di\ufb00erent words in a sentence or paragraph are considered noisy\npredictors of the underlying semantics (Gale et al., 1992). In the vein of this latter example, we\nconsider estimation in a simple bag-of-words document topic model as a warm-up to our general\nmethod; even this simpler model illustrates the power of pair-wise and triple-wise (i.e., bigram and\ntrigram) statistics that were not exploited by previous works on multi-view learning.\n1.2\nOutline\nSection 2 \ufb01rst develops the method of moments in the context of a simple discrete mixture model\nmotivated by document topic modeling; an explicit algorithm and convergence analysis are also\nprovided. The general setting is considered in Section 3, where the main algorithm and its accom-\npanying correctness and e\ufb03ciency guarantee are presented. Applications to learning multi-view\nmixtures of Gaussians and HMMs are discussed in Section 4. All proofs are given in the appendix.\n1.3\nNotations\nThe standard inner product between vectors \u20d7u and \u20d7v is denoted by \u27e8\u20d7u,\u20d7v\u27e9= \u20d7u\u22a4\u20d7v. We denote the\np-norm of a vector \u20d7v by \u2225\u20d7v\u2225p. For a matrix A \u2208Rm\u00d7n, we let \u2225A\u22252 denote its spectral norm \u2225A\u22252 :=\nsup\u20d7v\u0338=\u20d70 \u2225A\u20d7v\u22252/\u2225\u20d7v\u22252, \u2225A\u2225F denote its Frobenius norm, \u03c3i(A) denote the i-th largest singular value,\nand \u03ba(A) := \u03c31(A)/\u03c3min(m,n)(A) denote its condition number. Let \u2206n\u22121 := {(p1, p2, . . . , pn) \u2208Rn :\npi \u22650 \u2200i, Pn\ni=1 pi = 1} denote the probability simplex in Rn, and let Sn\u22121 := {\u20d7u \u2208Rn : \u2225\u20d7u\u22252 = 1}\ndenote the unit sphere in Rn. Let \u20d7ei \u2208Rd denote the i-th coordinate vector whose i-th entry is 1\nand the rest are zero. Finally, for a positive integer n, let [n] := {1, 2, . . . , n}.\n2\nWarm-up: bag-of-words document topic modeling\nWe \ufb01rst describe our method of moments in the simpler context of bag-of-words models for docu-\nments.\n2.1\nSetting\nSuppose a document corpus can be partitioned by topic, with each document being assigned a single\ntopic. Further, suppose the words in a document are drawn independently from a multinomial\ndistribution corresponding to the document\u2019s topic. Let k be the number of distinct topics in the\ncorpus, d be the number of distinct words in the vocabulary, and \u2113\u22653 be the number of words in\neach document (so the documents may be quite short).\nThe generative process for a document is given as follows:\n4\n1. The document\u2019s topic is drawn according to the multinomial distribution speci\ufb01ed by the\nprobability vector \u20d7w = (w1, w2, . . . , wk) \u2208\u2206k\u22121.\nThis is modeled as a discrete random\nvariable h such that\nPr[h = j] = wj,\nj \u2208[k].\n2. Given the topic h, the document\u2019s \u2113words are drawn independently according to the multi-\nnomial distribution speci\ufb01ed by the probability vector \u20d7\u00b5h \u2208\u2206d\u22121.\nThe random vectors\n\u20d7x1, \u20d7x2, . . . , \u20d7x\u2113\u2208Rd represent the \u2113words by setting\n\u20d7xv = \u20d7ei \u21d4the v-th word in the document is i,\ni \u2208[d]\n(the reason for this encoding of words will become clear in the next section). Therefore, for\neach word v \u2208[\u2113] in the document,\nPr[\u20d7xv = \u20d7ei|h = j] = \u27e8\u20d7ei, \u20d7\u00b5j\u27e9= Mi,j,\ni \u2208[d], j \u2208[k],\nwhere M \u2208Rd\u00d7k is the matrix of conditional probabilities M := [\u20d7\u00b51|\u20d7\u00b52| \u00b7 \u00b7 \u00b7 |\u20d7\u00b5k].\nThis probabilistic model has the conditional independence structure depicted in Figure 2(a) as a\ndirected graphical model.\nWe assume the following condition on \u20d7w and M.\nCondition 2.1 (Non-degeneracy: document topic model). wj >0 for all j\u2208[k], and M has rank k.\nThis condition requires that each topic has non-zero probability, and also prevents any topic\u2019s\nword distribution from being a mixture of the other topics\u2019 word distributions.\n2.2\nPair-wise and triple-wise probabilities\nDe\ufb01ne Pairs \u2208Rd\u00d7d to be the matrix of pair-wise probabilities whose (i, j)-th entry is\nPairsi,j := Pr[\u20d7x1 = \u20d7ei, \u20d7x2 = \u20d7ej],\ni, j \u2208[d].\nAlso de\ufb01ne Triples \u2208Rd\u00d7d\u00d7d to be the third-order tensor of triple-wise probabilities whose (i, j, \u03ba)-\nth entry is\nTriplesi,j,\u03ba := Pr[\u20d7x1 = \u20d7ei, \u20d7x2 = \u20d7ej, \u20d7x3 = \u20d7e\u03ba],\ni, j, \u03ba \u2208[d].\nThe identi\ufb01cation of words with coordinate vectors allows Pairs and Triples to be viewed as expec-\ntations of tensor products of the random vectors \u20d7x1, \u20d7x2, and \u20d7x3:\nPairs = E[\u20d7x1 \u2297\u20d7x2]\nand\nTriples = E[\u20d7x1 \u2297\u20d7x2 \u2297\u20d7x3].\n(1)\nWe may also view Triples as a linear operator Triples: Rd \u2192Rd\u00d7d given by\nTriples(\u20d7\u03b7) := E[(\u20d7x1 \u2297\u20d7x2)\u27e8\u20d7\u03b7, \u20d7x3\u27e9].\nIn other words, the (i, j)-th entry of Triples(\u20d7\u03b7) for \u20d7\u03b7 = (\u03b71, \u03b72, . . . , \u03b7d) is\nTriples(\u20d7\u03b7)i,j =\nd\nX\nx=1\n\u03b7xTriplesi,j,x =\nd\nX\nx=1\n\u03b7xTriples(\u20d7ex)i,j.\nThe following lemma shows that Pairs and Triples(\u20d7\u03b7) can be viewed as certain matrix products\ninvolving the model parameters M and \u20d7w.\n5\nLemma 2.1. Pairs=M diag(\u20d7w)M \u22a4and Triples(\u20d7\u03b7)=M diag(M \u22a4\u20d7\u03b7) diag(\u20d7w)M \u22a4for all \u20d7\u03b7 \u2208Rd.\nProof. Since \u20d7x1, \u20d7x2, and \u20d7x3 are conditionally independent given h,\nPairsi,j = Pr[\u20d7x1 = \u20d7ei, \u20d7x2 = \u20d7ej] =\nk\nX\nt=1\nPr[\u20d7x1 = \u20d7ei, \u20d7x2 = \u20d7ej|h = t] \u00b7 Pr[h = t]\n=\nk\nX\nt=1\nPr[\u20d7x1 = \u20d7ei|h = t] \u00b7 Pr[\u20d7x2 = \u20d7ej|h = t] \u00b7 Pr[h = t] =\nk\nX\nt=1\nMi,t \u00b7 Mj,t \u00b7 wt\nso Pairs = M diag(\u20d7w)M \u22a4. Moreover, writing \u20d7\u03b7 = (\u03b71, \u03b72, . . . , \u03b7d),\nTriples(\u20d7\u03b7)i,j =\nd\nX\nx=1\n\u03b7x Pr[\u20d7x1 = \u20d7ei, \u20d7x2 = \u20d7ej, \u20d7x3 = \u20d7ex]\n=\nd\nX\nx=1\nk\nX\nt=1\n\u03b7x \u00b7 Mi,t \u00b7 Mj,t \u00b7 Mx,t \u00b7 wt =\nk\nX\nt=1\nMi,t \u00b7 Mj,t \u00b7 wt \u00b7 (M \u22a4\u20d7\u03b7)t\nso Triples(\u20d7\u03b7) = M diag(M \u22a4\u20d7\u03b7) diag(\u20d7w)M \u22a4.\n2.3\nObservable operators and their spectral properties\nThe pair-wise and triple-wise probabilities can be related in a way that essentially reveals the con-\nditional probability matrix M. This is achieved through a matrix called an \u201cobservable operator\u201d.\nSimilar observable operators were previously used to characterize multiplicity automata (Sch\u00a8utzenberger,\n1961; Jaeger, 2000) and, more recently, for learning discrete HMMs (via an operator parameteriza-\ntion) (Hsu et al., 2009).\nLemma 2.2. Assume Condition 2.1. Let U \u2208Rd\u00d7k and V \u2208Rd\u00d7k be matrices such that both\nU \u22a4M and V \u22a4M are invertible. Then U \u22a4PairsV is invertible, and for all \u20d7\u03b7 \u2208Rd, the \u201cobservable\noperator\u201d B(\u20d7\u03b7) \u2208Rk\u00d7k, given by\nB(\u20d7\u03b7) := (U \u22a4Triples(\u20d7\u03b7)V )(U \u22a4PairsV )\u22121,\nsatis\ufb01es\nB(\u20d7\u03b7) = (U \u22a4M) diag(M \u22a4\u20d7\u03b7)(U \u22a4M)\u22121.\nProof. Since diag(\u20d7w) \u227b0 by Condition 2.1 and U \u22a4PairsV = (U \u22a4M) diag(\u20d7w)M \u22a4V by Lemma 2.1, it\nfollows that U \u22a4PairsV is invertible by the assumptions on U and V . Moreover, also by Lemma 2.1,\nB(\u20d7\u03b7) = (U \u22a4Triples(\u20d7\u03b7)V ) (U \u22a4PairsV )\u22121\n= (U \u22a4M diag(M \u22a4\u20d7\u03b7) diag(\u20d7w)M \u22a4V ) (U \u22a4PairsV )\u22121\n= (U \u22a4M) diag(M \u22a4\u20d7\u03b7)(U \u22a4M)\u22121 (U \u22a4M diag(\u20d7w)M \u22a4V ) (U \u22a4PairsV )\u22121\n= (U \u22a4M) diag(M \u22a4\u20d7\u03b7)(U \u22a4M)\u22121.\nThe matrix B(\u20d7\u03b7) is called \u201cobservable\u201d because it is only a function of the observable variables\u2019\njoint probabilities (e.g., Pr[\u20d7x1 = \u20d7ei, \u20d7x2 = \u20d7ej]). In the case \u20d7\u03b7 = \u20d7ex for some x \u2208[d], the matrix\n6\nAlgorithm A\n1. Obtain empirical frequencies of word pairs and triples from a given sample of documents, and\nform the tables [\nPairs \u2208Rd\u00d7d and \\\nTriples \u2208Rd\u00d7d\u00d7d corresponding to the population quantities\nPairs and Triples.\n2. Let \u02c6U \u2208Rd\u00d7k and \u02c6V \u2208Rd\u00d7k be, respectively, matrices of orthonormal left and right singular\nvectors of [\nPairs corresponding to its top k singular values.\n3. Pick \u20d7\u03b7 \u2208Rd (see remark in the main text), and compute the right eigenvectors \u02c6\u03be1, \u02c6\u03be2, . . . , \u02c6\u03bek (of\nunit Euclidean norm) of\n\u02c6B(\u20d7\u03b7) := ( \u02c6U\n\u22a4\\\nTriples(\u20d7\u03b7) \u02c6V )( \u02c6U\n\u22a4[\nPairs\u02c6V )\u22121.\n(Fail if not possible.)\n4. Let \u02c6\u00b5j := \u02c6U \u02c6\u03bej/\u27e8\u20d71, \u02c6U \u02c6\u03bej\u27e9for all j \u2208[k].\n5. Return \u02c6\nM := [\u02c6\u00b51|\u02c6\u00b52| \u00b7 \u00b7 \u00b7 |\u02c6\u00b5k].\nFigure 1: Topic-word distribution estimator (Algorithm A).\nB(\u20d7ex) is similar (in the linear algebraic sense) to the diagonal matrix diag(M \u22a4\u20d7ex); the collection\nof matrices {diag(M \u22a4\u20d7ex) : x \u2208[d]} (together with \u20d7w) can be used to compute joint probabilities\nunder the model (see, e.g., Hsu et al. (2009)). Note that the columns of U \u22a4M are eigenvectors of\nB(\u20d7ex), with the j-th column having an associated eigenvalue equal to Pr[\u20d7xv = x|h = j]. If the word\nx has distinct probabilities under every topic, then B(\u20d7ex) has exactly k distinct eigenvalues, each\nhaving geometric multiplicity one and corresponding to a column of U \u22a4M.\n2.4\nTopic-word distribution estimator and convergence guarantee\nThe spectral properties of the observable operators B(\u20d7\u03b7) implied by Lemma 2.2 suggest the estima-\ntion procedure (Algorithm A) in Figure 1. The procedure is essentially a plug-in approach based on\nthe equations relating the second- and third-order moments in Lemma 2.2. We focus on estimating\nM; estimating the mixing weights \u20d7w is easily handled as a secondary step (see Appendix B.5 for\nthe estimator in the context of the general model in Section 3.1).\nOn the choice of \u20d7\u03b7. As discussed in the previous section, a suitable choice for \u20d7\u03b7 can be based\non prior knowledge about the topic-word distributions, such as \u20d7\u03b7 = \u20d7ex for some x \u2208[d] that has\ndi\ufb00erent conditional probabilities under each topic. In the absence of such information, one may\nselect \u20d7\u03b7 randomly from the subspace range( \u02c6U). Speci\ufb01cally, take \u20d7\u03b7 := \u02c6U\u20d7\u03b8 where \u20d7\u03b8 \u2208Rk is a random\nunit vector distributed uniformly over Sk\u22121.\nThe following theorem establishes the convergence rate of Algorithm A.\nTheorem 2.1. There exists a constant C > 0 such that the following holds. Pick any \u03b4 \u2208(0, 1).\nAssume the document topic model from Section 2.1 satis\ufb01es Condition 2.1. Further, assume that\nin Algorithm A, [\nPairs and \\\nTriples are, respectively, the empirical averages of N independent copies\nof \u20d7x1 \u2297\u20d7x2 and \u20d7x1 \u2297\u20d7x2 \u2297\u20d7x3; and that \u20d7\u03b7 = \u02c6U\u20d7\u03b8 where \u20d7\u03b8 \u2208Rk is an independent random unit vector\ndistributed uniformly over Sk\u22121. If\nN \u2265C \u00b7\nk7 \u00b7 ln(1/\u03b4)\n\u03c3k(M)6 \u00b7 \u03c3k(Pairs)4 \u00b7 \u03b42 ,\n7\nh\n\u20d7x1\n\u20d7x2\n\u00b7 \u00b7 \u00b7\n\u20d7x\u2113\nh1\nh2\n\u00b7 \u00b7 \u00b7\nh\u2113\n\u20d7x1\n\u20d7x2\n\u20d7x\u2113\n(a)\n(b)\nFigure 2: (a) The multi-view mixture model. (b) A hidden Markov model.\nthen with probability at least 1 \u2212\u03b4, the parameters returned by Algorithm A have the following\nguarantee: there exists a permutation \u03c4 on [k] and scalars c1, c2, . . . , ck \u2208R such that, for each\nj \u2208[k],\n\u2225cj \u02c6\u00b5j \u2212\u20d7\u00b5\u03c4(j)\u22252 \u2264C \u00b7 \u2225\u20d7\u00b5\u03c4(j)\u22252 \u00b7\nk5\n\u03c3k(M)4 \u00b7 \u03c3k(Pairs)2 \u00b7 \u03b4 \u00b7\nr\nln(1/\u03b4)\nN\n.\nThe proof of Theorem 2.1, as well as some illustrative empirical results on using Algorithm A,\nare presented in Appendix A. A few remarks about the theorem are in order.\nOn boosting the con\ufb01dence. Although the convergence depends polynomially on 1/\u03b4, where \u03b4\nis the failure probability, it is possible to boost the con\ufb01dence by repeating Step 3 of Algorithm\nA with di\ufb00erent random \u20d7\u03b7 until the eigenvalues of \u02c6B(\u20d7\u03b7) are su\ufb03ciently separated (as judged by\ncon\ufb01dence intervals).\nOn the scaling factors cj. With a larger sample complexity that depends on d, an error bound\ncan be established for \u2225\u02c6\u00b5j \u2212\u20d7\u00b5\u03c4(j)\u22251 directly (without the unknown scaling factors cj). We also\nremark that the scaling factors can be estimated from the eigenvalues of \u02c6B(\u20d7\u03b7), but we do not\npursue this approach as it is subsumed by Algorithm B anyway.\n3\nA method of moments for multi-view mixture models\nWe now consider a much broader class of mixture models and present a general method of moments\nin this context.\n3.1\nGeneral setting\nConsider the following multi-view mixture model; k denotes the number of mixture components,\nand \u2113denotes the number of views. We assume \u2113\u22653 throughout. Let \u20d7w = (w1, w2, . . . , wk) \u2208\u2206k\u22121\nbe a vector of mixing weights, and let h be a (hidden) discrete random variable with Pr[h = j] = wj\nfor all j \u2208[k]. Let \u20d7x1, \u20d7x2, . . . , \u20d7x\u2113\u2208Rd be \u2113random vectors that are conditionally independent given\nh; the directed graphical model is depicted in Figure 2(a).\nDe\ufb01ne the conditional mean vectors as\n\u20d7\u00b5v,j := E[\u20d7xv|h = j],\nv \u2208[\u2113], j \u2208[k],\nand let Mv \u2208Rd\u00d7k be the matrix whose j-th column is \u20d7\u00b5v,j. Note that we do not specify anything\nelse about the (conditional) distribution of \u20d7xv\u2014it may be continuous, discrete, or even a hybrid\ndepending on h.\nWe assume the following conditions on \u20d7w and the Mv.\n8\nCondition 3.1 (Non-degeneracy: general setting). wj > 0 for all j \u2208[k], and Mv has rank k for\nall v \u2208[\u2113].\nWe remark that it is easy to generalize to the case where views have di\ufb00erent dimensionality\n(e.g., \u20d7xv \u2208Rdv for possibly di\ufb00erent dimensions dv). For notational simplicity, we stick to the same\ndimension for each view. Moreover, Condition 3.1 can be relaxed in some cases; we discuss one\nsuch case in Section 4.1 in the context of Gaussian mixture models.\nBecause the conditional distribution of \u20d7xv is not speci\ufb01ed beyond its conditional means, it is\nnot possible to develop a maximum likelihood approach to parameter estimation. Instead, as in the\ndocument topic model, we develop a method of moments based on solving polynomial equations\narising from eigenvalue problems.\n3.2\nObservable moments and operators\nWe focus on the moments concerning {\u20d7x1, \u20d7x2, \u20d7x3}, but the same properties hold for other triples of\nthe random vectors {\u20d7xa, \u20d7xb, \u20d7xc} \u2286{\u20d7xv : v \u2208[\u2113]} as well.\nAs in (1), we de\ufb01ne the matrix P1,2 \u2208Rd\u00d7d of second-order moments, and the tensor P1,2,3 \u2208\nRd\u00d7d\u00d7d of third-order moments, by\nP1,2 := E[\u20d7x1 \u2297\u20d7x2]\nand\nP1,2,3 := E[\u20d7x1 \u2297\u20d7x2 \u2297\u20d7x3].\nAgain, P1,2,3 is regarded as the linear operator P1,2,3 : \u20d7\u03b7 7\u2192E[(\u20d7x1 \u2297\u20d7x2)\u27e8\u20d7\u03b7, \u20d7x3\u27e9].\nLemma 3.1 and Lemma 3.2 are straightforward generalizations of Lemma 2.1 and Lemma 2.2.\nLemma 3.1. P1,2=M1 diag(\u20d7w)M \u22a4\n2 and P1,2,3(\u20d7\u03b7)=M1 diag(M \u22a4\n3 \u20d7\u03b7) diag(\u20d7w)M \u22a4\n2 for all \u20d7\u03b7 \u2208Rd.\nLemma 3.2. Assume Condition 3.1.\nFor v \u2208{1, 2, 3}, let Uv \u2208Rd\u00d7k be a matrix such that\nU \u22a4\nv Mv is invertible. Then U \u22a4\n1 P1,2U2 is invertible, and for all \u20d7\u03b7 \u2208Rd, the \u201cobservable operator\u201d\nB1,2,3(\u20d7\u03b7) \u2208Rk\u00d7k, given by B1,2,3(\u20d7\u03b7) := (U \u22a4\n1 P1,2,3(\u20d7\u03b7)U2)(U \u22a4\n1 P1,2U2)\u22121, satis\ufb01es\nB1,2,3(\u20d7\u03b7) = (U \u22a4\n1 M1) diag(M \u22a4\n3 \u20d7\u03b7)(U \u22a4\n1 M1)\u22121.\nIn particular, the k roots of the polynomial \u03bb 7\u2192det(B1,2,3(\u20d7\u03b7) \u2212\u03bbI) are {\u27e8\u20d7\u03b7, \u20d7\u00b53,j\u27e9: j \u2208[k]}.\nRecall that Algorithm A relates the eigenvectors of B(\u20d7\u03b7) to the matrix of conditional means M.\nHowever, eigenvectors are only de\ufb01ned up to a scaling of each vector; without prior knowledge of\nthe correct scaling, the eigenvectors are not su\ufb03cient to recover the parameters M. Nevertheless,\nthe eigenvalues also carry information about the parameters, as shown in Lemma 3.2, and it is\npossible to reconstruct the parameters from di\ufb00erent the observation operators applied to di\ufb00erent\nvectors \u20d7\u03b7. This idea is captured in the following lemma.\nLemma 3.3. Consider the setting and de\ufb01nitions from Lemma 3.2. Let \u0398 \u2208Rk\u00d7k be an invertible\nmatrix, and let \u20d7\u03b8\u22a4\ni \u2208Rk be its i-th row. Moreover, for all i \u2208[k], let \u03bbi,1, \u03bbi,2, . . . , \u03bbi,k denote the\nk eigenvalues of B1,2,3(U3\u20d7\u03b8i) in the order speci\ufb01ed by the matrix of right eigenvectors U \u22a4\n1 M1. Let\nL \u2208Rk\u00d7k be the matrix whose (i, j)-th entry is \u03bbi,j. Then\n\u0398U \u22a4\n3 M3 = L.\n9\nObserve that the unknown parameters M3 are expressed as the solution to a linear system in\nthe above equation, where the elements of the right-hand side L are the roots of k-th degree poly-\nnomials derived from the second- and third-order observable moments (namely, the characteristic\npolynomials of the B1,2,3(U3\u20d7\u03b8i), \u2200i \u2208[k]). This template is also found in other moment methods\nbased on decompositions of a Hankel matrix. A crucial distinction, however, is that the k-th degree\npolynomials in Lemma 3.3 only involve low-order moments, whereas standard methods may involve\nup to \u2126(k)-th order moments which are di\ufb03cult to estimate (Lindsay, 1989; Lindsay and Basak,\n1993; Gravin et al., 2012).\n3.3\nMain result: general estimation procedure and sample complexity bound\nThe lemmas in the previous section suggest the estimation procedure (Algorithm B) presented in\nFigure 3.\nAlgorithm B\n1. Compute empirical averages from N independent copies of \u20d7x1\u2297\u20d7x2 to form \u02c6P1,2 \u2208Rd\u00d7d. Similarly\ndo the same for \u20d7x1 \u2297\u20d7x3 to form \u02c6P1,3 \u2208Rk\u00d7k, and for \u20d7x1 \u2297\u20d7x2 \u2297\u20d7x3 to form \u02c6P1,2,3 \u2208Rd\u00d7d\u00d7d.\n2. Let \u02c6U1 \u2208Rd\u00d7k and \u02c6U2 \u2208Rd\u00d7k be, respectively, matrices of orthonormal left and right singular\nvectors of \u02c6P1,2 corresponding to its top k singular values.\nLet \u02c6U3 \u2208Rd\u00d7k be the matrix of\northonormal right singular vectors of \u02c6P1,3 corresponding to its top k singular values.\n3. Pick an invertible matrix \u0398 \u2208Rk\u00d7k, with its i-th row denoted as \u20d7\u03b8\u22a4\ni \u2208Rk. In the absence of any\nprior information about M3, a suitable choice for \u0398 is a random rotation matrix.\nForm the matrix\n\u02c6B1,2,3( \u02c6U3\u20d7\u03b81) := ( \u02c6U \u22a4\n1 \u02c6P1,2,3( \u02c6U3\u20d7\u03b81) \u02c6U2)( \u02c6U \u22a4\n1 \u02c6P1,2 \u02c6U2)\u22121.\nCompute \u02c6R1 \u2208Rk\u00d7k (with unit Euclidean norm columns) that diagonalizes \u02c6B1,2,3( \u02c6U3\u20d7\u03b81), i.e.,\n\u02c6R\u22121\n1\n\u02c6B1,2,3( \u02c6U3\u20d7\u03b81) \u02c6R1 = diag(\u02c6\u03bb1,1, \u02c6\u03bb1,2, . . . , \u02c6\u03bb1,k).\n(Fail if not possible.)\n4. For each i \u2208{2, . . . , k}, obtain the diagonal entries \u02c6\u03bbi,1, \u02c6\u03bbi,2, . . . , \u02c6\u03bbi,k of \u02c6R\u22121\n1\n\u02c6B1,2,3( \u02c6U3\u20d7\u03b8i) \u02c6R1, and\nform the matrix \u02c6L \u2208Rk\u00d7k whose (i, j)-th entry is \u02c6\u03bbi,j.\n5. Return \u02c6\nM3 := \u02c6U3\u0398\u22121 \u02c6L.\nFigure 3: General method of moments estimator (Algorithm B).\nAs stated, the Algorithm B yields an estimator for M3, but the method can easily be applied\nto estimate Mv for all other views v. One caveat is that the estimators may not yield the same\nordering of the columns, due to the unspeci\ufb01ed order of the eigenvectors obtained in the third step\nof the method, and therefore some care is needed to obtain a consistent ordering. We outline one\nsolution in Appendix B.4.\nThe sample complexity of Algorithm B depends on the speci\ufb01c concentration properties of\n\u20d7x1, \u20d7x2, \u20d7x3. We abstract away this dependence in the following condition.\nCondition 3.2. There exist positive scalars N0, C1,2, C1,3, C1,2,3, and a function f(N, \u03b4) (decreas-\ning in N and \u03b4) such that for any N \u2265N0 and \u03b4 \u2208(0, 1),\n1. Pr\nh\n\u2225\u02c6Pa,b \u2212Pa,b\u22252 \u2264Ca,b \u00b7 f(N, \u03b4)\ni\n\u22651 \u2212\u03b4\nfor {a, b} \u2208{{1, 2}, {1, 3}},\n2. \u2200\u20d7v \u2208Rd, Pr\nh\n\u2225\u02c6P1,2,3(\u20d7v) \u2212P1,2,3(\u20d7v)\u22252 \u2264C1,2,3 \u00b7 \u2225\u20d7v\u22252 \u00b7 f(N, \u03b4)\ni\n\u22651 \u2212\u03b4.\n10\nMoreover (for technical convenience), \u02c6P1,3 is independent of \u02c6P1,2,3 (which may be achieved, say, by\nsplitting a sample of size 2N).\nFor the discrete models such as the document topic model of Section 2.1 and discrete HMMs (Mos-\nsel and Roch, 2006; Hsu et al., 2009), Condition 3.2 holds with N0 = C1,2 = C1,3 = C1,2,3 = 1, and\nf(N, \u03b4) = (1+\np\nln(1/\u03b4))/\n\u221a\nN. Using standard techniques (e.g., Chaudhuri et al. (2009); Vershynin\n(2012)), the condition can also be shown to hold for mixtures of various continuous distributions\nsuch as multivariate Gaussians.\nNow we are ready to present the main theorem of this section (proved in Appendix B.6).\nTheorem 3.1. There exists a constant C > 0 such that the following holds. Assume the three-\nview mixture model satis\ufb01es Condition 3.1 and Condition 3.2. Pick any \u01eb \u2208(0, 1) and \u03b4 \u2208(0, \u03b40).\nFurther, assume \u0398 \u2208Rk\u00d7k is an independent random rotation matrix distributed uniformly over\nthe Stiefel manifold {Q \u2208Rk\u00d7k : Q\u22a4Q = I}. If the number of samples N satis\ufb01es N \u2265N0 and\nf(N, \u03b4/k) \u2264C \u00b7 mini\u0338=j \u2225M3(\u20d7ei \u2212\u20d7ej)\u22252 \u00b7 \u03c3k(P1,2)\nC1,2,3 \u00b7 k5 \u00b7 \u03ba(M1)4\n\u00b7\n\u03b4\nln(k/\u03b4) \u00b7 \u01eb,\nf(N, \u03b4) \u2264C \u00b7 min\n(\nmini\u0338=j \u2225M3(\u20d7ei \u2212\u20d7ej)\u22252 \u00b7 \u03c3k(P1,2)2\nC1,2 \u00b7 \u2225P1,2,3\u22252 \u00b7 k5 \u00b7 \u03ba(M1)4\n\u00b7\n\u03b4\nln(k/\u03b4), \u03c3k(P1,3)\nC1,3\n)\n\u00b7 \u01eb\nwhere \u2225P1,2,3\u22252 := max\u20d7v\u0338=\u20d70 \u2225P1,2,3(\u20d7v)\u22252, then with probability at least 1 \u22125\u03b4, Algorithm B returns\n\u02c6\nM3 = [\u02c6\u00b53,1|\u02c6\u00b53,2| \u00b7 \u00b7 \u00b7 |\u02c6\u00b53,k] with the following guarantee: there exists a permutation \u03c4 on [k] such that\nfor each j \u2208[k],\n\u2225\u02c6\u00b53,j \u2212\u20d7\u00b53,\u03c4(j)\u22252 \u2264max\nj\u2032\u2208[k]\u2225\u20d7\u00b53,j\u2032\u22252 \u00b7 \u01eb.\n4\nApplications\nIn addition to the document clustering model from Section 2, a number of natural latent variable\nmodels \ufb01t into this multi-view framework. We describe two such cases in this section: Gaussian\nmixture models and HMMs, both of which have been (at least partially) studied in the literature.\nIn both cases, the estimation technique of Algorithm B leads to new learnability results that were\nnot achieved by previous works.\n4.1\nMulti-view Gaussian mixture models\nThe standard Gaussian mixture model is parameterized by a mixing weight wj, mean vector\n\u20d7\u00b5j \u2208RD, and covariance matrix \u03a3j \u2208RD\u00d7D for each mixture component j \u2208[k]. The hidden\ndiscrete random variable h selects a component j with probability Pr[h = j] = wj; the conditional\ndistribution of the observed random vector \u20d7x given h is a multivariate Gaussian with mean \u20d7\u00b5h and\ncovariance \u03a3h.\nThe multi-view assumption for Gaussian mixture models asserts that for each component j,\nthe covariance \u03a3j has a block diagonal structure \u03a3j = blkdiag(\u03a31,j, \u03a32,j, . . . , \u03a3\u2113,j) (a special case\nis an axis-aligned Gaussian). The various blocks correspond to the \u2113di\ufb00erent views of the data\n\u20d7x1, \u20d7x2, . . . , \u20d7x\u2113\u2208Rd (for d = D/\u2113), which are conditionally independent given h. The mean vector\nfor each component j is similarly partitioned into the views as \u20d7\u00b5j = (\u20d7\u00b51,j, \u20d7\u00b52,j, . . . , \u20d7\u00b5\u2113,j). Note that\nin the case of an axis-aligned Gaussian, each covariance matrix \u03a3j is diagonal, and therefore the\n11\noriginal coordinates [D] can be partitioned into \u2113= O(D/k) views (each of dimension d = \u2126(k)) in\nany way (say, randomly) provided that Condition 3.1 holds.\nCondition 3.1 requires that the conditional mean matrix Mv = [\u20d7\u00b5v,1|\u20d7\u00b5v,2| \u00b7 \u00b7 \u00b7 |\u20d7\u00b5v,k] for each view\nv have full column rank. This is similar to the non-degeneracy and spreading conditions used in\nprevious studies of multi-view clustering (Chaudhuri and Rao, 2008; Chaudhuri et al., 2009). In\nthese previous works, the multi-view and non-degeneracy assumptions are shown to reduce the\nminimum separation required for various e\ufb03cient algorithms to learn the model parameters. In\ncomparison, Algorithm B does not require a minimum separation condition at all. See Appendix D.3\nfor details.\nWhile Algorithm B recovers just the means of the mixture components (see Appendix D.4 for\ndetails concerning Condition 3.2), we remark that a slight variation can be used to recover the\ncovariances as well. Note that\nE[\u20d7xv \u2297\u20d7xv|h] = (Mv\u20d7eh) \u2297(Mv\u20d7eh) + \u03a3v,h = \u20d7\u00b5v,h \u2297\u20d7\u00b5v,h + \u03a3v,h\nfor all v \u2208[\u2113]. For a pair of vectors \u20d7\u03c6 \u2208Rd and \u20d7\u03c8 \u2208Rd, de\ufb01ne the matrix Q1,2,3(\u20d7\u03c6, \u20d7\u03c8) \u2208Rd\u00d7d of\nfourth-order moments by Q1,2,3(\u20d7\u03c6, \u20d7\u03c8) := E[(\u20d7x1 \u2297\u20d7x2)\u27e8\u20d7\u03c6, \u20d7x3\u27e9\u27e8\u20d7\u03c8, \u20d7x3\u27e9].\nProposition 4.1. Under the setting of Lemma 3.2, the matrix given by\nF1,2,3(\u20d7\u03c6, \u20d7\u03c8) := (U \u22a4\n1 Q1,2,3(\u20d7\u03c6, \u20d7\u03c8)U2)(U \u22a4\n1 P1,2U2)\u22121\nsatis\ufb01es F1,2,3(\u20d7\u03c6, \u20d7\u03c8) = (U \u22a4\n1 M1) diag(\u27e8\u20d7\u03c6, \u20d7\u00b53,t\u27e9\u27e8\u20d7\u03c8, \u20d7\u00b53,t\u27e9+ \u27e8\u20d7\u03c6, \u03a33,t \u20d7\u03c8\u27e9: t \u2208[k])(U \u22a4\n1 M1)\u22121 and hence is\ndiagonalizable (in fact, by the same matrices as B1,2,3(\u20d7\u03b7)).\nFinally, we note that even if Condition 3.1 does not hold (e.g., if \u20d7\u00b5v,j \u2261\u20d7m \u2208Rd (say) for all\nv \u2208[\u2113], j \u2208[k] so all of the Gaussians have the same mean), one may still apply Algorithm B to\nthe model (h, \u20d7y1, \u20d7y2, . . . , \u20d7y\u2113) where \u20d7yv \u2208Rd+d(d+1)/2 is the random vector that include both \ufb01rst-\nand second-order terms of \u20d7xv, i.e., \u20d7yv is the concatenation of xv and the upper triangular part of\n\u20d7xv \u2297\u20d7xv. In this case, Condition 3.1 is replaced by a requirement that the matrices\nM\u2032\nv :=\n\u0002\nE[\u20d7yv|h = 1]\nE[\u20d7yv|h = 2]\n\u00b7 \u00b7 \u00b7\nE[\u20d7yv|h = k]\n\u0003\n\u2208R(d+d(d+1)/2)\u00d7k\nof conditional means and covariances have full rank. This requirement can be met even if the means\n\u20d7\u00b5v,j of the mixture components are all the same.\n4.2\nHidden Markov models\nA hidden Markov model is a latent variable model in which a hidden state sequence h1, h2, . . . , h\u2113\nforms a Markov chain h1 \u2192h2 \u2192\u00b7 \u00b7 \u00b7 \u2192h\u2113over k possible states [k]; and given the state ht at\ntime t \u2208[k], the observation \u20d7xt at time t (a random vector taking values in Rd) is conditionally\nindependent of all other observations and states.\nThe directed graphical model is depicted in\nFigure 2(b).\nThe vector \u20d7\u03c0 \u2208\u2206k\u22121 is the initial state distribution:\nPr[h1 = i] = \u03c0i,\ni \u2208[k].\n12\nFor simplicity, we only consider time-homogeneous HMMs, although it is possible to generalize to\nthe time-varying setting. The matrix T \u2208Rk\u00d7k is a stochastic matrix describing the hidden state\nMarkov chain:\nPr[ht+1 = i|ht = j] = Ti,j,\ni, j \u2208[k], t \u2208[\u2113\u22121].\nFinally, the columns of the matrix O = [\u20d7o1|\u20d7o2| \u00b7 \u00b7 \u00b7 |\u20d7ok] \u2208Rd\u00d7k are the conditional means of the\nobservation \u20d7xt at time t given the corresponding hidden state ht:\nE[\u20d7xt|ht = i] = O\u20d7ei = \u20d7oi,\ni \u2208[k], t \u2208[\u2113].\nNote that both discrete and continuous observations are readily handled in this framework. For\ninstance, the conditional distribution of \u20d7xt given ht = i (for i \u2208[k]) could be a high-dimensional mul-\ntivariate Gaussian with mean \u20d7oi \u2208Rd. Such models were not handled by previous methods (Chang,\n1996; Mossel and Roch, 2006; Hsu et al., 2009).\nThe restriction of the HMM to three time steps, say t \u2208{1, 2, 3}, is an instance of the three-view\nmixture model.\nProposition 4.2. If the hidden variable h (from the three-view mixture model of Section 3.1) is\nidenti\ufb01ed with the second hidden state h2, then {\u20d7x1, \u20d7x2, \u20d7x3} are conditionally independent given h,\nand the parameters of the resulting three-view mixture model on (h, \u20d7x1, \u20d7x2, \u20d7x3) are\n\u20d7w := T\u20d7\u03c0,\nM1 := O diag(\u20d7\u03c0)T \u22a4diag(T\u20d7\u03c0)\u22121,\nM2:= O,\nM3 := OT.\nFrom Proposition 4.2, it is easy to verify that B3,1,2(\u20d7\u03b7) = (U \u22a4\n3 OT) diag(O\u22a4\u20d7\u03b7)(U \u22a4\n3 OT)\u22121. There-\nfore, after recovering the observation conditional mean matrix O using Algorithm B, the Markov\nchain transition matrix can be recovered using the matrix of right eigenvectors R of B3,1,2(\u20d7\u03b7) and\nthe equation (U \u22a4\n3 O)\u22121R = T (up to scaling of the columns).\nAcknowledgments\nWe thank Kamalika Chaudhuri and Tong Zhang for many useful discussions, Karl Stratos for\ncomments on an early draft, David Sontag and an anonymous reviewer for some pointers to related\nwork, and Adel Javanmard for pointing out a problem with Theorem D.1 in an earlier version of\nthe paper.\nReferences\nD. Achlioptas and F. McSherry. On spectral learning of mixtures of distributions. In COLT, 2005.\nR. Ahlswede and A. Winter.\nStrong converse for identi\ufb01cation via quantum channels.\nIEEE\nTransactions on Information Theory, 48(3):569\u2013579, 2002.\nS. Arora and R. Kannan. Learning mixtures of arbitrary Gaussians. In STOC, 2001.\nM. Belkin and K. Sinha. Polynomial learning of distribution families. In FOCS, 2010.\nM. B. Blaschko and C. H. Lampert. Correlational spectral clustering. In CVPR, 2008.\nD. L. Boley, F. T. Luk, and D. Vandevoorde. Vandermonde factorization of a Hankel matrix. In\nScienti\ufb01c Computing, 1997.\n13\nS. C. Brubaker and S. Vempala. Isotropic PCA and a\ufb03ne-invariant clustering. In FOCS, 2008.\nJ. T. Chang.\nFull reconstruction of Markov models on evolutionary trees: Identi\ufb01ability and\nconsistency. Mathematical Biosciences, 137:51\u201373, 1996.\nK. Chaudhuri and S. Rao.\nLearning mixtures of product distributions using correlations and\nindependence. In COLT, 2008.\nK. Chaudhuri, S. M. Kakade, K. Livescu, and K. Sridharan. Multi-view clustering via canonical\ncorrelation analysis. In ICML, 2009.\nS. Dasgupta. Learning mixutres of Gaussians. In FOCS, 1999.\nS. Dasgupta and A. Gupta.\nAn elementary proof of a theorem of Johnson and Lindenstrauss.\nRandom Structures and Algorithms, 22(1):60\u201365, 2003.\nS. Dasgupta and L. Schulman. A probabilistic analysis of EM for mixtures of separated, spherical\nGaussians. Journal of Machine Learning Research, 8(Feb):203\u2013226, 2007.\nJ. Feldman, R. O\u2019Donnell, and R. Servedio. Learning mixtures of product distributions over discrete\ndomains. In FOCS, 2005.\nJ. Feldman, R. O\u2019Donnell, and R. Servedio. PAC learning mixtures of axis-aligned Gaussians with\nno separation assumption. In COLT, 2006.\nA. M. Frieze, M. Jerrum, and R. Kannan. Learning linear transformations. In FOCS, 1996.\nW. A. Gale, K. W. Church, and D. Yarowsky. One sense per discourse. In 4th DARPA Speech and\nNatural Language Workshop, 1992.\nN. Gravin, J. Lasserre, D. Pasechnik, and S. Robins. The inverse moment problem for convex\npolytopes. Discrete and Computational Geometry, 2012. To appear.\nH. Hotelling. The most predictable criterion. Journal of Educational Psychology, 26(2):139\u2013142,\n1935.\nD. Hsu, S. M. Kakade, and T. Zhang. A spectral algorithm for learning hidden Markov models. In\nCOLT, 2009.\nD. Hsu, S. M. Kakade, and T. Zhang. A spectral algorithm for learning hidden Markov models.\nJournal of Computer and System Sciences, 2012. To appear.\nA. Hyv\u00a8arinen and E. Oja. Independent component analysis: algorithms and applications. Neural\nNetworks, 13(4\u20135):411\u2013430, 2000.\nH. Jaeger. Observable operator models for discrete stochastic time series. Neural Computation, 12\n(6), 2000.\nA. T. Kalai, A. Moitra, and G. Valiant. E\ufb03ciently learning mixtures of two Gaussians. In STOC,\n2010.\n14\nR. Kannan, H. Salmasian, and S. Vempala. The spectral method for general mixture models. In\nCOLT, 2005.\nB. G. Lindsay. Moment matrices: applications in mixtures. Annals of Statistics, 17(2):722\u2013740,\n1989.\nB. G. Lindsay. Mixture models: theory, geometry and applications. American Statistical Association,\n1995.\nB. G. Lindsay and P. Basak. Multivariate normal mixtures: a fast consistent method. Journal of\nthe American Statistical Association, 88(422):468\u2013476, 1993.\nF. McSherry. Spectral partitioning of random graphs. In FOCS, 2001.\nA. Moitra and G. Valiant. Settling the polynomial learnability of mixtures of Gaussians. In FOCS,\n2010.\nE. Mossel and S. Roch. Learning nonsingular phylogenies and hidden Markov models. Annals of\nApplied Probability, 16(2):583\u2013614, 2006.\nP. Q. Nguyen and O. Regev. Learning a parallelepiped: Cryptanalysis of GGH and NTRU signa-\ntures. Journal of Cryptology, 22(2):139\u2013160, 2009.\nK. Pearson. Contributions to the mathematical theory of evolution. Philosophical Transactions of\nthe Royal Society, London, A., page 71, 1894.\nR. A. Redner and H. F. Walker. Mixture densities, maximum likelihood and the EM algorithm.\nSIAM Review, 26(2):195\u2013239, 1984.\nM. P. Sch\u00a8utzenberger. On the de\ufb01nition of a family of automata. Information and Control, 4:\n245\u2013270, 1961.\nG. W. Stewart and Ji-Guang Sun. Matrix Perturbation Theory. Academic Press, 1990.\nD. M. Titterington, A. F. M. Smith, and U. E. Makov. Statistical analysis of \ufb01nite mixture distri-\nbutions. Wiley, 1985.\nS. Vempala and G. Wang. A spectral algorithm for learning mixtures of distributions. In FOCS,\n2002.\nR. Vershynin. Introduction to the non-asymptotic analysis of random matrices. In Y. Eldar and\nG. Kutyniok, editors, Compressed Sensing, Theory and Applications, chapter 5, pages 210\u2013268.\nCambridge University Press, 2012.\nA\nAnalysis of Algorithm A\nIn this appendix, we give an analysis of Algorithm A (but defer most perturbation arguments to\nAppendix C), and also present some illustrative empirical results on text data using a modi\ufb01ed\nimplementation.\n15\nA.1\nAccuracy of moment estimates\nLemma A.1. Fix \u03b4 \u2208(0, 1). Let [\nPairs be the empirical average of N independent copies of \u20d7x1 \u2297\u20d7x2,\nand let \\\nTriples be the empirical average of N independent copies of (\u20d7x1 \u2297\u20d7x2)\u27e8\u20d7\u03b7, \u20d7x3\u27e9. Then\n1. Pr\n\"\n\u2225[\nPairs \u2212Pairs\u2225F \u22641 +\np\nln(1/\u03b4)\n\u221a\nN\n#\n\u22651 \u2212\u03b4, and\n2. Pr\n\"\n\u2200\u20d7\u03b7 \u2208Rd, \u2225\\\nTriples(\u20d7\u03b7) \u2212Triples(\u20d7\u03b7)\u2225F \u2264\u2225\u20d7\u03b7\u22252(1 +\np\nln(1/\u03b4))\n\u221a\nN\n#\n\u22651 \u2212\u03b4.\nProof. The \ufb01rst claim follows from applying Lemma F.1 to the vectorizations of [\nPairs and Pairs\n(whereupon the Frobenius norm is the Euclidean norm of the vectorized matrices). For the second\nclaim, we also apply Lemma F.1 to \\\nTriples and Triples in the same way to obtain, with probability\nat least 1 \u2212\u03b4,\nd\nX\ni=1\nd\nX\nj=1\nd\nX\nx=1\n( \\\nTriplesi,j,x \u2212Triplesi,j,x)2 \u2264(1 +\np\nln(1/\u03b4))2\nN\n.\nNow condition on this event. For any \u20d7\u03b7 = (\u03b71, \u03b72, . . . , \u03b7d) \u2208Rd,\n\u2225\\\nTriples(\u20d7\u03b7) \u2212Triples(\u20d7\u03b7)\u22252\nF =\nd\nX\ni=1\nd\nX\nj=1\n\f\f\f\f\f\nd\nX\nx=1\n\u03b7x( \\\nTriplesi,j,x \u2212Triplesi,j,x)\n\f\f\f\f\f\n2\n\u2264\nd\nX\ni=1\nd\nX\nj=1\n\u2225\u20d7\u03b7\u22252\n2\nd\nX\nx=1\n( \\\nTriplesi,j,x \u2212Triplesi,j,x)2\n\u2264\u2225\u20d7\u03b7\u22252\n2(1 +\np\nln(1/\u03b4))2\nN\nwhere the \ufb01rst inequality follows by Cauchy-Schwarz.\nA.2\nProof of Theorem 2.1\nLet E1 be the event in which\n\u2225[\nPairs \u2212Pairs\u22252 \u22641 +\np\nln(1/\u03b4)\n\u221a\nN\n(2)\nand\n\u2225\\\nTriples(\u20d7v) \u2212Triples(\u20d7v)\u22252 \u2264\u2225v\u22252(1 +\np\nln(1/\u03b4))\n\u221a\nN\n(3)\nfor all \u20d7v \u2208Rd. By Lemma A.1, a union bound, and the fact that \u2225A\u22252 \u2264\u2225A\u2225F, we have Pr[E1] \u2265\n1 \u22122\u03b4. Now condition on E1, and let E2 be the event in which\n\u03b3 := min\ni\u0338=j |\u27e8\u02c6U\u20d7\u03b8, M(\u20d7ei \u2212\u20d7ej)\u27e9| = min\ni\u0338=j |\u27e8\u20d7\u03b8, \u02c6U \u22a4M(\u20d7ei \u2212\u20d7ej)\u27e9| >\n\u221a\n2\u03c3k( \u02c6U \u22a4M) \u00b7 \u03b4\n\u221a\nek\n\u0000k\n2\n\u0001\n.\n(4)\n16\nBy Lemma C.6 and the fact \u2225\u02c6U \u22a4M(\u20d7ei \u2212\u20d7ej)\u22252 \u2265\n\u221a\n2\u03c3k( \u02c6U \u22a4M), we have Pr[E2|E1] \u22651\u2212\u03b4, and thus\nPr[E1 \u2229E2] \u2265(1 \u22122\u03b4)(1 \u2212\u03b4) \u22651 \u22123\u03b4. So henceforth condition on this joint event E1 \u2229E2.\nLet \u03b50 := \u2225\\\nPairs\u2212Pairs\u22252\n\u03c3k(Pairs)\n, \u03b51 :=\n\u03b50\n1\u2212\u03b50, and \u03b52 :=\n\u03b50\n(1\u2212\u03b52\n1)\u00b7(1\u2212\u03b50\u2212\u03b52\n1). The conditions on N and the\nbound in (2) implies that \u03b50 <\n1\n1+\n\u221a\n2 \u22641\n2, so Lemma C.1 implies that \u03c3k( \u02c6U \u22a4M) \u2265\np\n1 \u2212\u03b52\n1 \u00b7\u03c3k(M),\n\u03ba( \u02c6U \u22a4M) \u2264\n\u2225M\u22252\n\u221a\n1\u2212\u03b52\n1\u00b7\u03c3k(M), and that \u02c6U \u22a4Pairs \u02c6V is invertible. By Lemma 2.2,\n\u02dcB(\u20d7\u03b7) := ( \u02c6U \u22a4Triples(\u20d7\u03b7) \u02c6V )( \u02c6U \u22a4Pairs \u02c6V )\u22121 = ( \u02c6U \u22a4M) diag(M \u22a4\u20d7\u03b7)( \u02c6U \u22a4M)\u22121.\nThus, Lemma C.2 implies\n\u2225\u02c6B(\u20d7\u03b7) \u2212\u02dcB(\u20d7\u03b7)\u22252 \u2264\u2225\\\nTriples(\u20d7\u03b7) \u2212Triples(\u20d7\u03b7)\u22252\n(1 \u2212\u03b50) \u00b7 \u03c3k(Pairs)\n+\n\u03b52\n\u03c3k(Pairs).\n(5)\nLet R := \u02c6U \u22a4M diag(\u2225\u02c6U \u22a4M\u20d7e1\u22252, \u2225\u02c6U \u22a4M\u20d7e2\u22252, . . . , \u2225\u02c6U \u22a4M\u20d7ek\u22252)\u22121 and \u03b53 := \u2225\u02c6B(\u20d7\u03b7)\u2212\u02dcB(\u20d7\u03b7)\u22252\u00b7\u03ba(R)\n\u03b3\n. Note\nthat R has unit norm columns, and that R\u22121 \u02dcB(\u20d7\u03b7)R = diag(M \u22a4\u20d7\u03b7). By Lemma C.5 and the fact\n\u2225M\u22252 \u2264\n\u221a\nk\u2225M\u22251 =\n\u221a\nk,\n\u2225R\u22121\u22252 \u2264\u03ba( \u02c6U \u22a4M) \u2264\n\u2225M\u22252\np\n1 \u2212\u03b52\n1 \u00b7 \u03c3k(M)\n\u2264\n\u221a\nk\np\n1 \u2212\u03b52\n1 \u00b7 \u03c3k(M)\n(6)\nand\n\u03ba(R) \u2264\u03ba( \u02c6U \u22a4M)2 \u2264\nk\n(1 \u2212\u03b52\n1) \u00b7 \u03c3k(M)2 .\n(7)\nThe conditions on N and the bounds in (2), (3), (4), (5), and (7) imply that \u03b53 < 1\n2. By Lemma C.3,\nthere exists a permutation \u03c4 on [k] such that, for all j \u2208[k],\n\u2225sj \u02c6\u03bej \u2212\u02c6U \u22a4\u20d7\u00b5\u03c4(j)/c\u2032\nj\u22252 = \u2225sj \u02c6\u03bej \u2212R\u20d7e\u03c4(j)\u22252 \u22644k \u00b7 \u2225R\u22121\u22252 \u00b7 \u03b53\n(8)\nwhere sj := sign(\u27e8\u02c6\u03bej, \u02c6U \u22a4\u20d7\u00b5\u03c4(j)\u27e9) and c\u2032\nj := \u2225\u02c6U \u22a4\u20d7\u00b5\u03c4(j)\u22252 \u2264\u2225\u20d7\u00b5\u03c4(j)\u22252 (the eigenvectors \u02c6\u03bej are unique up\nto sign sj because each eigenvalue has geometric multiplicity 1). Since \u20d7\u00b5\u03c4(j) \u2208range(U), Lemma C.1\nand the bounds in (8) and (6) imply\n\u2225sj \u02c6U \u02c6\u03bej \u2212\u20d7\u00b5\u03c4(j)/c\u2032\nj\u22252 \u2264\nq\n\u2225sj \u02c6\u03bej \u2212\u02c6U \u22a4\u20d7\u00b5\u03c4(j)/c\u2032\nj\u22252\n2 + \u2225\u20d7\u00b5\u03c4(j)/c\u2032\nj\u22252\n2 \u00b7 \u03b52\n1\n\u2264\u2225sj \u02c6\u03bej \u2212\u02c6U \u22a4\u20d7\u00b5\u03c4(j)/c\u2032\nj\u22252 + \u2225\u20d7\u00b5\u03c4(j)/c\u2032\nj\u22252 \u00b7 \u03b51\n\u22644k \u00b7 \u2225R\u22121\u22252 \u00b7 \u03b53 + \u03b51\n\u22644k \u00b7\n\u221a\nk\np\n1 \u2212\u03b52\n1 \u00b7 \u03c3k(M)\n\u00b7 \u03b53 + \u03b51.\nTherefore, for cj := sjc\u2032\nj\u27e8\u20d71, \u02c6U \u02c6\u03bej\u27e9, we have\n\u2225cj \u02c6\u00b5j \u2212\u20d7\u00b5\u03c4(j)\u22252 = \u2225c\u2032\njsj \u02c6U \u02c6\u03bej \u2212\u20d7\u00b5\u03c4(j)\u22252 \u2264\u2225\u20d7\u00b5\u03c4(j)\u22252 \u00b7\n\u0012\n4k \u00b7\n\u221a\nk\np\n1 \u2212\u03b52\n1 \u00b7 \u03c3k(M)\n\u00b7 \u03b53 + \u03b51\n\u0013\n.\n17\nMaking all of the substitutions into the above bound gives\n\u2225cj \u02c6\u00b5j \u2212\u20d7\u00b5\u03c4(j)\u22252\n\u2225\u20d7\u00b5\u03c4(j)\u22252\n\u2264\n4k1.5\np\n1 \u2212\u03b52\n1 \u00b7 \u03c3k(M)\n\u00b7\nk\n(1 \u2212\u03b52\n1) \u00b7 \u03c3k(M)2 \u00b7\n\u221a\nek \u00b7\n\u0000k\n2\n\u0001\np\n2(1 \u2212\u03b52\n1) \u00b7 \u03c3k(M) \u00b7 \u03b4\n\u00b7\n\u0012\u2225\\\nTriples(\u20d7\u03b7) \u2212Triples(\u20d7\u03b7)\u22252\n(1 \u2212\u03b50) \u00b7 \u03c3k(Pairs)\n+\n\u2225[\nPairs \u2212Pairs\u22252\n(1 \u2212\u03b52\n1) \u00b7 (1 \u2212\u03b50 \u2212\u03b52\n1) \u00b7 \u03c3k(Pairs)2\n\u0013\n+\n\u2225[\nPairs \u2212Pairs\u22252\n(1 \u2212\u03b50) \u00b7 \u03c3k(Pairs)\n\u2264C \u00b7\nk5\n\u03c3k(M)4 \u00b7 \u03c3k(Pairs)2 \u00b7 \u03b4 \u00b7\nr\nln(1/\u03b4)\nN\n.\nA.3\nSome illustrative empirical results\nAs a demonstration of feasibility, we applied a modi\ufb01ed version of Algorithm A to a subset of arti-\ncles from the \u201c20 Newsgroups\u201d dataset, speci\ufb01cally those in comp.graphics, rec.sport.baseball,\nsci.crypt, and soc.religion.christian, where \u20d7x1, \u20d7x2, \u20d7x3 represent three words from the begin-\nning (\ufb01rst third), middle (middle third), and end (last third) of an article.\nWe used k = 25\n(although results were similar for k \u2208{10, 15, 20, 25, 30}) and d = 5441 (after removing a standard\nset of 524 stop-words and applying Porter stemming). Instead of using a single \u20d7\u03b7 and extracting\nall eigenvectors of \u02c6B(\u20d7\u03b7), we extracted a single eigenvector \u20d7\u03bex from \u02c6B(\u20d7ex) for several words x \u2208[d]\n(these x\u2019s were chosen using an automatic heuristic based on their statistical leverage scores in\n[\nPairs). Below, for each such ( \u02c6B(\u20d7ex), \u20d7\u03bex), we report the top 15 words y ordered by \u20d7e\u22a4\ny \u02c6U\u20d7\u03bex value.\n\u02c6B(\u20d7eformat)\n\u02c6B(\u20d7egod)\n\u02c6B(\u20d7ekey)\n\u02c6B(\u20d7epolygon)\n\u02c6B(\u20d7eteam)\n\u02c6B(\u20d7etoday)\nsource\ngod\nkey\npolygon\nwin\ngame\n\ufb01nd\nwrite\nbit\ntime\ngame\ntiger\npost\njesus\nchip\nsave\nrun\nbit\nimage\nchristian\nsystem\nrefer\nteam\nrun\nfeal\nchrist\nencrypt\nbook\nyear\npitch\nintersect\npeople\ncar\nsource\ndon\nday\nemail\ntime\nrepository\nman\nwatch\nteam\nrpi\napr\nve\nroutine\ngood\ntrue\ntime\nsin\npublic\nnetcom\nscore\nlot\nproblem\nbible\nescrow\ngif\nyankees\nbook\n\ufb01le\nday\nsecure\nrecord\npitch\nlost\nprogram\nchurch\nmake\nsubscribe\nstart\ncolorado\ngif\nperson\nclipper\nchange\nbit\nfan\nbit\nbook\nwrite\nalgorithm\ntime\napr\njpeg\nlife\nnsa\nscott\nwonder\nwatch\nThe \ufb01rst and fourth topics appear to be about computer graphics (comp.graphics), the \ufb01fth and\nsixth about baseball (rec.sports.baseball), the third about encryption (sci.crypt), and the\nsecond about Christianity (soc.religion.christian).\nWe also remark that Algorithm A can be implemented so that it makes just two passes over\nthe training data, and that simple hashing or random projection tricks can reduce the memory\nrequirement to O(k2 + kd) (i.e., [\nPairs and \\\nTriples never need to be explicitly formed).\n18\nB\nProofs and details from Section 3\nIn this section, we provide ommitted proofs and discussion from Section 3.\nB.1\nProof of Lemma 3.1\nBy conditional independence,\nP1,2 = E[E[\u20d7x1 \u2297\u20d7x2|h]] = E[E[\u20d7x1|h] \u2297E[\u20d7x2|h]]\n= E[(M1\u20d7eh) \u2297(M2\u20d7eh)] = M1\n\u0012 k\nX\nt=1\nwt\u20d7et \u2297\u20d7et\n\u0013\nM \u22a4\n2 = M1 diag(\u20d7w)M \u22a4\n2 .\nSimilarly,\nP1,2,3(\u20d7\u03b7) = E[E[(\u20d7x1 \u2297\u20d7x2)\u27e8\u20d7\u03b7, \u20d7x3\u27e9|h]] = E[E[\u20d7x1|h] \u2297E[\u20d7x2|h]\u27e8\u20d7\u03b7, E[\u20d7x3|h]\u27e9]\n= E[(M1\u20d7eh) \u2297(M2\u20d7eh)\u27e8\u20d7\u03b7, M3\u20d7eh\u27e9] = M1\n\u0012 k\nX\nt=1\nwt\u20d7eh \u2297\u20d7eh\u27e8\u20d7\u03b7, M3\u20d7eh\u27e9\n\u0013\nM \u22a4\n2\n= M1 diag(M \u22a4\n3 \u20d7\u03b7) diag(\u20d7w)M \u22a4\n2 .\nB.2\nProof of Lemma 3.2\nWe have U \u22a4\n1 P1,2U2 = (U \u22a4\n1 M1) diag(\u20d7w)(M \u22a4\n2 U2) by Lemma 3.1, which is invertible by the assumptions\non Uv and Condition 3.1. Moreover, also by Lemma 3.1,\nB1,2,3(\u20d7\u03b7) = (U \u22a4\n1 P1,2,3(\u20d7\u03b7)U2) (U \u22a4\n1 P1,2U2)\u22121\n= (U \u22a4\n1 M1 diag(M \u22a4\n3 \u20d7\u03b7) diag(\u20d7w)M \u22a4\n2 U2) (U \u22a4\n1 P1,2U2)\u22121\n= (U \u22a4\n1 M1) diag(M \u22a4\n3 \u20d7\u03b7)(U \u22a4\n1 M1)\u22121 (U \u22a4\n1 M1 diag(\u20d7w)M \u22a4\n2 U2) (U \u22a4\n1 P1,2U2)\u22121\n= (U \u22a4\n1 M1) diag(M \u22a4\n3 \u20d7\u03b7)(U \u22a4\n1 M1)\u22121 (U \u22a4\n1 P1,2U2) (U \u22a4\n1 P1,2U2)\u22121\n= (U \u22a4\n1 M1) diag(M \u22a4\n3 \u20d7\u03b7)(U \u22a4\n1 M1)\u22121.\nB.3\nProof of Lemma 3.3\nBy Lemma 3.2,\n(U \u22a4\n1 M1)\u22121B1,2,3(U3\u20d7\u03b8i)(U \u22a4\n1 M1) = diag(M \u22a4\n3 U3\u20d7\u03b8i)\n= diag(\u27e8\u20d7\u03b8i, U \u22a4\n3 M3\u20d7e1\u27e9, \u27e8\u20d7\u03b8i, U \u22a4\n3 M3\u20d7e2\u27e9, . . . \u27e8\u20d7\u03b8i, U \u22a4\n3 M3\u20d7ek\u27e9)\n= diag(\u03bbi,1, \u03bbi,2, . . . , \u03bbi,k)\nfor all i \u2208[k], and therefore\nL =\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8f0\n\u27e8\u20d7\u03b81, U \u22a4\n3 M3\u20d7e1\u27e9\n\u27e8\u20d7\u03b81, U \u22a4\n3 M3\u20d7e2\u27e9\n\u00b7 \u00b7 \u00b7\n\u27e8\u20d7\u03b81, U \u22a4\n3 M3\u20d7ek\u27e9\n\u27e8\u20d7\u03b82, U \u22a4\n3 M3\u20d7e1\u27e9\n\u27e8\u20d7\u03b82, U \u22a4\n3 M3\u20d7e2\u27e9\n\u00b7 \u00b7 \u00b7\n\u27e8\u20d7\u03b82, U \u22a4\n3 M3\u20d7e3\u27e9\n...\n...\n...\n...\n\u27e8\u20d7\u03b8k, U \u22a4\n3 M3\u20d7e1\u27e9\n\u27e8\u20d7\u03b8k, U \u22a4\n3 M3\u20d7e2\u27e9\n\u00b7 \u00b7 \u00b7\n\u27e8\u20d7\u03b8k, U \u22a4\n3 M3\u20d7ek\u27e9\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fb= \u0398U \u22a4\n3 M3.\n19\nB.4\nOrdering issues\nAlthough Algorithm B only explicitly yields estimates for M3, it can easily be applied to estimate\nMv for all other views v. The main caveat is that the estimators may not yield the same ordering\nof the columns, due to the unspeci\ufb01ed order of the eigenvectors obtained in the third step of the\nmethod, and therefore some care is needed to obtain a consistent ordering. However, this ordering\nissue can be handled by exploiting consistency across the multiple views.\nThe \ufb01rst step is to perform the estimation of M3 using Algorithm B as is. Then, to estimate\nM2, one may re-use the eigenvectors in \u02c6R1 to diagonalize \u02c6B1,3,2(\u20d7\u03b7), as B1,2,3(\u20d7\u03b7) and B1,3,2(\u20d7\u03b7) share\nthe same eigenvectors. The same goes for estimating Mv for other all other views v except v = 1.\nIt remains to provide a way to estimate M1. Observe that M2 can be estimated in at least two\nways: via the operators \u02c6B1,3,2(\u20d7\u03b7), or via the operators \u02c6B3,1,2(\u20d7\u03b7). This is because the eigenvalues\nof B3,1,2(\u20d7\u03b7) and B1,3,2(\u20d7\u03b7) are the identical. Because the eigenvalues are also su\ufb03ciently separated\nfrom each other, the eigenvectors \u02c6R3 of \u02c6B3,1,2(\u20d7\u03b7) can be put in the same order as the eigenvectors\n\u02c6R1 of \u02c6B1,3,2 by (approximately) matching up their respective corresponding eigenvalues. Finally,\nthe appropriately re-ordered eigenvectors \u02c6R3 can then be used to diagonalize \u02c6B3,2,1(\u20d7\u03b7) to estimate\nM1.\nB.5\nEstimating the mixing weights\nGiven the estimate of \u02c6\nM3, one can obtain an estimate of \u20d7w using\n\u02c6w := \u02c6\nM\u2020\n3 \u02c6E[\u20d7x3]\nwhere A\u2020 denotes the Moore-Penrose pseudoinverse of A (though other generalized inverses may\nwork as well), and \u02c6E[\u20d7x3] is the empirical average of \u20d7x3. This estimator is based on the following\nobservation:\nE[\u20d7x3] = E[E[\u20d7x3|h]] = M3E[\u20d7eh] = M3 \u20d7w\nand therefore\nM\u2020\n3E[\u20d7x3] = M\u2020\n3M3 \u20d7w = \u20d7w\nsince M3 has full column rank.\nB.6\nProof of Theorem 3.1\nThe proof is similar to that of Theorem 2.1, so we just describe the essential di\ufb00erences. As before,\nmost perturbation arguments are deferred to Appendix C.\nFirst, let E1 be the event in which\n\u2225\u02c6P1,2 \u2212P1,2\u22252 \u2264C1,2 \u00b7 f(N, \u03b4),\n\u2225\u02c6P1,3 \u2212P1,3\u22252 \u2264C1,3 \u00b7 f(N, \u03b4)\nand\n\u2225\u02c6P1,2,3( \u02c6U3\u20d7\u03b8i) \u2212P1,2,3( \u02c6U3\u20d7\u03b8i)\u22252 \u2264C1,2,3 \u00b7 f(N, \u03b4/k)\nfor all i \u2208[k]. Therefore by Condition 3.2 and a union bound, we have Pr[E1] \u22651 \u22123\u03b4. Second,\nlet E2 be the event in which\n\u03b3 := min\ni\u2208[k] min\nj\u0338=j\u2032 |\u27e8\u20d7\u03b8i, \u02c6U \u22a4\n3 M3(\u20d7ej \u2212\u20d7ej\u2032)\u27e9| > minj\u0338=j\u2032 \u2225\u02c6U \u22a4\n3 M3(\u20d7ej \u2212\u20d7ej\u2032)\u22252 \u00b7 \u03b4\n\u221a\nek\n\u0000k\n2\n\u0001\nk\n20\nand\n\u03bbmax := max\ni,j\u2208[k]|\u27e8\u20d7\u03b8i, \u02c6U \u22a4\n3 M3\u20d7ej\u27e9| \u2264maxj\u2208[k] \u2225M3\u20d7ej\u22252\n\u221a\nk\n\u0010\n1 +\np\n2 ln(k2/\u03b4)\n\u0011\n.\nSince each \u20d7\u03b8i is distributed uniformly over Sk\u22121, it follows from Lemma C.6 and a union bound\nthat Pr[E2|E1] \u22651 \u22122\u03b4. Therefore Pr[E1 \u2229E2] \u2265(1 \u22123\u03b4)(1 \u22122\u03b4) \u22651 \u22125\u03b4.\nLet U3 \u2208Rd\u00d7k be the matrix of top k orthonormal left singular vectors of M3. By Lemma C.1\nand the conditions on N, we have \u03c3k( \u02c6U \u22a4\n3 U3) \u22651/2, and therefore\n\u03b3 \u2265mini\u0338=i\u2032 \u2225M3(\u20d7ei \u2212\u20d7ei\u2032)\u22252 \u00b7 \u03b4\n2\n\u221a\nek\n\u0000k\n2\n\u0001\nk\nand\n\u03bbmax\n\u03b3\n\u2264\n\u221aek3(1 +\np\n2 ln(k2/\u03b4))\n\u03b4\n\u00b7 \u03ba\u2032(M3)\nwhere\n\u03ba\u2032(M3) :=\nmaxi\u2208[m] \u2225M3\u20d7ei\u22252\nmini\u0338=i\u2032 \u2225M3(\u20d7ei \u2212\u20d7ei\u2032)\u22252\n.\nLet \u20d7\u03b7i := \u02c6U3\u20d7\u03b8i for i \u2208[k]. By Lemma C.1, \u02c6U \u22a4\n1 P1,2 \u02c6U2 is invertible, so we may de\ufb01ne \u02dcB1,2,3(\u20d7\u03b7i) :=\n( \u02c6U \u22a4\n1 P1,2,3(\u20d7\u03b7i) \u02c6U2)( \u02c6U \u22a4\n1 P1,2 \u02c6U2)\u22121. By Lemma 3.2,\n\u02dcB1,2,3(\u20d7\u03b7i) = ( \u02c6U \u22a4\n1 M1) diag(M \u22a4\n3 \u20d7\u03b7i)( \u02c6U \u22a4\n1 M1)\u22121.\nAlso de\ufb01ne R := \u02c6U \u22a4\n1 M1 diag(\u2225\u02c6U \u22a4\n1 M1\u20d7e1\u22252, \u2225\u02c6U \u22a4\n1 M1\u20d7e2\u22252, . . . , \u2225\u02c6U \u22a4\n1 M1\u20d7ek\u22252)\u22121. Using most of the same\narguments in the proof of Theorem 2.1, we have\n\u2225R\u22121\u22252 \u22642\u03ba(M1),\n(9)\n\u03ba(R) \u22644\u03ba(M1)2,\n(10)\n\u2225\u02c6B1,2,3(\u20d7\u03b7i) \u2212\u02dcB1,2,3(\u20d7\u03b7i)\u22252 \u22642\u2225\u02c6P1,2,3(\u20d7\u03b7i) \u2212P1,2,3(\u20d7\u03b7i)\u22252\n\u03c3k(P1,2)\n+ 2\u2225P1,2,3\u22252 \u00b7 \u2225\u02c6P1,2 \u2212P1,2\u22252\n\u03c3k(P1,2)2\n.\nBy Lemma C.3, the operator \u02c6B1,2,3(\u20d7\u03b71) has k distinct eigenvalues, and hence its matrix of right\neigenvectors \u02c6R1 is unique up to column scaling and ordering. This in turn implies that \u02c6R\u22121\n1\nis\nunique up to row scaling and ordering. Therefore, for each i \u2208[k], the \u02c6\u03bbi,j = \u20d7e\u22a4\nj \u02c6R\u22121\n1\n\u02c6B1,2,3(\u20d7\u03b7i) \u02c6R1\u20d7ej\nfor j \u2208[k] are uniquely de\ufb01ned up to ordering. Moreover, by Lemma C.4 and the above bounds on\n\u2225\u02c6B1,2,3(\u20d7\u03b7i) \u2212\u02dcB1,2,3(\u20d7\u03b7i)\u22252 and \u03b3, there exists a permutation \u03c4 on [k] such that, for all i, j \u2208[k],\n|\u02c6\u03bbi,j \u2212\u03bbi,\u03c4(j)| \u2264\n\u0010\n3\u03ba(R) + 16k1.5 \u00b7 \u03ba(R) \u00b7 \u2225R\u22121\u22252\n2 \u00b7 \u03bbmax/\u03b3\n\u0011\n\u00b7 \u2225\u02c6B1,2,3(\u20d7\u03b7i) \u2212\u02dcB1,2,3(\u20d7\u03b7i)\u22252\n\u2264\n\u0010\n12\u03ba(M1)2 + 256k1.5 \u00b7 \u03ba(M1)4 \u00b7 \u03bbmax/\u03b3\n\u0011\n\u00b7 \u2225\u02c6B1,2,3(\u20d7\u03b7i) \u2212\u02dcB1,2,3(\u20d7\u03b7i)\u22252\n(11)\nwhere the second inequality uses (9) and (10).\nLet \u02c6\u03bdj := (\u02c6\u03bb1,j, \u02c6\u03bb2,j, . . . , \u02c6\u03bbk,j) \u2208Rk and \u20d7\u03bdj :=\n(\u03bb1,j, \u03bb2,j, . . . , \u03bbk,j) \u2208Rk.\nObserve that \u20d7\u03bdj = \u0398 \u02c6U \u22a4\n3 M3\u20d7ej = \u0398 \u02c6U \u22a4\n3 \u20d7\u00b53,j by Lemma 3.3.\nBy the\northogonality of \u0398, the fact \u2225\u20d7v\u22252 \u2264\n\u221a\nk\u2225\u20d7v\u2225\u221efor \u20d7v \u2208Rk, and (11)\n\u2225\u0398\u22121\u02c6\u03bdj \u2212\u02c6U \u22a4\n3 \u20d7\u00b53,\u03c4(j)\u22252 = \u2225\u0398\u22121(\u02c6\u03bdj \u2212\u20d7\u03bd\u03c4(j))\u22252\n= \u2225\u02c6\u03bdj \u2212\u20d7\u03bd\u03c4(j)\u22252\n\u2264\n\u221a\nk \u00b7 \u2225\u02c6\u03bdj \u2212\u20d7\u03bd\u03c4(j)\u2225\u221e\n=\n\u221a\nk \u00b7 max\ni\n|\u02c6\u03bbi,j \u2212\u03bbi,\u03c4(j)|\n\u2264\n\u0010\n12\n\u221a\nk \u00b7 \u03ba(M1)2 + 256k2 \u00b7 \u03ba(M1)4 \u00b7 \u03bbmax/\u03b3\n\u0011\n\u00b7 \u2225\u02c6B1,2,3(\u20d7\u03b7i) \u2212\u02dcB1,2,3(\u20d7\u03b7i)\u22252.\n21\nFinally, by Lemma C.1 (as applied to \u02c6P1,3 and P1,3),\n\u2225\u02c6\u00b53,j \u2212\u20d7\u00b53,\u03c4(j)\u22252 \u2264\u2225\u0398\u22121\u02c6\u03bdj \u2212\u02c6U \u22a4\n3 \u20d7\u00b53,\u03c4(j)\u22252 + 2\u2225\u20d7\u00b53,\u03c4(j)\u22252 \u00b7 \u2225\u02c6P1,3 \u2212P1,3\u22252\n\u03c3k(P1,3)\n.\nMaking all of the substitutions into the above bound gives\n\u2225\u02c6\u00b53,j \u2212\u20d7\u00b53,\u03c4(j)\u22252 \u2264C\n6 \u00b7 k5 \u00b7 \u03ba(M1)4 \u00b7 \u03ba\u2032(M3) \u00b7 ln(k/\u03b4)\n\u03b4\n\u00b7\n\u0012C1,2,3 \u00b7 f(N, \u03b4/k)\n\u03c3k(P1,2)\n+ \u2225P1,2,3\u22252 \u00b7 C1,2 \u00b7 f(N/\u03b4)\n\u03c3k(P1,2)2\n\u0013\n+ C\n6 \u00b7 \u2225\u20d7\u00b53,\u03c4(j)\u22252 \u00b7 C1,3 \u00b7 f(N, \u03b4)\n\u03c3k(P1,3)\n\u22641\n2\n\u0010\nmax\nj\u2032\u2208[k] \u2225\u20d7\u00b53,j\u2032\u22252 + \u2225\u20d7\u00b53,\u03c4(j)\u22252\n\u0011\n\u00b7 \u01eb\n\u2264max\nj\u2032\u2208[k] \u2225\u20d7\u00b53,j\u2032\u22252 \u00b7 \u01eb.\nC\nPerturbation analysis for observable operators\nThe following lemma establishes the accuracy of approximating the fundamental subspaces (i.e.,\nthe row and column spaces) of a matrix X by computing the singular value decomposition of a\nperturbation \u02c6X of X.\nLemma C.1. Let X \u2208Rm\u00d7n be a matrix of rank k. Let U \u2208Rm\u00d7k and V \u2208Rn\u00d7k be matrices\nwith orthonormal columns such that range(U) and range(V ) are spanned by, respectively, the left\nand right singular vectors of X corresponding to its k largest singular values.\nSimilarly de\ufb01ne\n\u02c6U \u2208Rm\u00d7k and \u02c6V \u2208Rn\u00d7k relative to a matrix \u02c6X \u2208Rm\u00d7n. De\ufb01ne \u01ebX := \u2225\u02c6X \u2212X\u22252, \u03b50 :=\n\u01ebX\n\u03c3k(X),\nand \u03b51 :=\n\u03b50\n1\u2212\u03b50. Assume \u03b50 < 1\n2. Then\n1. \u03b51 < 1;\n2. \u03c3k( \u02c6X) = \u03c3k( \u02c6U \u22a4\u02c6X \u02c6V ) \u2265(1 \u2212\u03b50) \u00b7 \u03c3k(X) > 0;\n3. \u03c3k( \u02c6U \u22a4U) \u2265\np\n1 \u2212\u03b52\n1;\n4. \u03c3k( \u02c6V \u22a4V ) \u2265\np\n1 \u2212\u03b52\n1;\n5. \u03c3k( \u02c6U \u22a4X \u02c6V ) \u2265(1 \u2212\u03b52\n1) \u00b7 \u03c3k(X);\n6. for any \u02c6\u03b1 \u2208Rk and \u20d7v \u2208range(U), \u2225\u02c6U \u02c6\u03b1 \u2212\u20d7v\u22252\n2 \u2264\u2225\u02c6\u03b1 \u2212\u02c6U \u22a4\u20d7v\u22252\n2 + \u2225\u20d7v\u22252\n2 \u00b7 \u03b52\n1.\nProof. The \ufb01rst claim follows from the assumption on \u03b50. The second claim follows from the assump-\ntions and Weyl\u2019s theorem (Lemma E.1). Let the columns of \u02c6U\u22a5\u2208Rm\u00d7(m\u2212k) be an orthonormal\nbasis for the orthogonal complement of range( \u02c6U), so that \u2225\u02c6U \u22a4\n\u22a5U\u22252 \u2264\u01ebX/\u03c3k( \u02c6X) \u2264\u03b51 by Wedin\u2019s\ntheorem (Lemma E.2). The third claim then follows because \u2225\u02c6U \u22a4U\u22252\n2 = 1 \u2212\u2225\u02c6U \u22a4\n\u22a5U\u22252\n2 \u22651 \u2212\u03b52\n1.\nThe fourth claim is analogous to the third claim, and the \ufb01fth claim follows from the third and\nfourth. The sixth claim follows writing \u20d7v = U\u20d7\u03b1 for some \u20d7\u03b1 \u2208Rk, and using the decomposition\n\u2225\u02c6U \u02c6\u03b1\u2212\u20d7v\u22252\n2 = \u2225\u02c6U \u02c6\u03b1\u2212\u02c6U \u02c6U \u22a4\u20d7v\u22252\n2+\u2225\u02c6U\u22a5\u02c6U \u22a4\n\u22a5\u20d7v\u22252\n2 = \u2225\u02c6\u03b1\u2212\u02c6U \u22a4\u20d7v\u22252\n2+\u2225\u02c6U \u22a4\n\u22a5(U\u20d7\u03b1)\u22252\n2 \u2264\u2225\u02c6\u03b1\u2212\u02c6U \u22a4\u20d7v\u22252\n2+\u2225\u02c6U \u22a4\n\u22a5U\u22252\n2\u2225\u20d7\u03b1\u22252\n2 \u2264\n\u2225\u02c6\u03b1\u2212\u02c6U \u22a4\u20d7v\u22252\n2+\u2225\u20d7\u03b1\u22252\n2\u00b7\u03b52\n1 = \u2225\u02c6\u03b1\u2212\u02c6U \u22a4U\u20d7\u03b1\u22252\n2+\u2225\u20d7v\u22252\n2\u00b7\u03b52\n1 where the last inequality follows from the argument\nfor the third claim, and the last equality uses the orthonormality of the columns of U.\n22\nThe next lemma bounds the error of the observation operator in terms of the errors in estimating\nthe second-order and third-order moments.\nLemma C.2. Consider the setting and de\ufb01nitions from Lemma C.1, and let Y \u2208Rm\u00d7n and\n\u02c6Y \u2208Rm\u00d7n be given. De\ufb01ne \u03b52 :=\n\u03b50\n(1\u2212\u03b52\n1)\u00b7(1\u2212\u03b50\u2212\u03b52\n1) and \u01ebY := \u2225\u02c6Y \u2212Y \u22252. Assume \u03b50 <\n1\n1+\n\u221a\n2. Then\n1. \u02c6U \u22a4\u02c6X \u02c6V and \u02c6U \u22a4X \u02c6V are both invertible, and \u2225( \u02c6U \u22a4\u02c6X \u02c6V )\u22121 \u2212( \u02c6U \u22a4X \u02c6V )\u22121\u22252 \u2264\n\u03b52\n\u03c3k(X);\n2. \u2225( \u02c6U \u22a4\u02c6Y \u02c6V )( \u02c6U \u22a4\u02c6X \u02c6V )\u22121 \u2212( \u02c6U \u22a4Y \u02c6V )( \u02c6U \u22a4X \u02c6V )\u22121\u22252 \u2264\n\u01ebY\n(1\u2212\u03b50)\u00b7\u03c3k(X) + \u2225Y \u22252\u00b7\u03b52\n\u03c3k(X) .\nProof. Let \u02c6S := \u02c6U \u22a4\u02c6X \u02c6V and \u02dcS := \u02c6U \u22a4X \u02c6V . By Lemma C.1, \u02c6U \u22a4\u02c6X \u02c6V is invertible, \u03c3k( \u02dcS) \u2265\u03c3k( \u02c6U \u22a4U)\u00b7\n\u03c3k(X) \u00b7 \u03c3k( \u02c6V \u22a4V ) \u2265(1 \u2212\u03b52\n1) \u00b7 \u03c3k(X) (so \u02dcS is also invertible), and \u2225\u02c6S \u2212\u02dcS\u22252 \u2264\u03b50 \u00b7 \u03c3k(X) \u2264\n\u03b50\n1\u2212\u03b52\n1 \u00b7\n\u03c3k( \u02dcS). The assumption on \u03b50 implies\n\u03b50\n1\u2212\u03b52\n1 < 1; therefore the Lemma E.4 implies \u2225\u02c6S\u22121 \u2212\u02dcS\u22121\u22252 \u2264\n\u2225\u02c6S\u2212\u02dcS\u22252/\u03c3k( \u02dcS)\n1\u2212\u2225\u02c6S\u2212\u02dcS\u22252/\u03c3k( \u02dcS) \u00b7\n1\n\u03c3k( \u02dcS) \u2264\n\u03b52\n\u03c3k(X), which proves the \ufb01rst claim. For the second claim, observe that\n\u2225( \u02c6U \u22a4\u02c6Y \u02c6V )( \u02c6U \u22a4\u02c6X \u02c6V )\u22121 \u2212( \u02c6U \u22a4Y \u02c6V )( \u02c6U \u22a4X \u02c6V )\u22121\u22252\n\u2264\u2225( \u02c6U \u22a4\u02c6Y \u02c6V )( \u02c6U \u22a4\u02c6X \u02c6V )\u22121 \u2212( \u02c6U \u22a4Y \u02c6V )( \u02c6U \u22a4\u02c6X \u02c6V )\u22121\u22252 + \u2225( \u02c6U \u22a4Y \u02c6V )( \u02c6U \u22a4\u02c6X \u02c6V )\u22121 \u2212( \u02c6U \u22a4Y \u02c6V )( \u02c6U \u22a4X \u02c6V )\u22121\u22252\n\u2264\u2225\u02c6U \u22a4\u02c6Y \u02c6V \u2212\u02c6U \u22a4Y \u02c6V \u22252 \u00b7 \u2225( \u02c6U \u22a4\u02c6X \u02c6V )\u22121\u22252 + \u2225\u02c6U \u22a4Y \u02c6V \u22252 \u00b7 \u2225( \u02c6U \u22a4\u02c6X \u02c6V )\u22121 \u2212( \u02c6U \u22a4X \u02c6V )\u22121\u22252\n\u2264\n\u01ebY\n(1 \u2212\u03b50) \u00b7 \u03c3k(X) + \u2225Y \u22252 \u00b7 \u03b52\n\u03c3k(X)\nwhere the \ufb01rst inequality follows from the triangle inequality, the second follows from the sub-\nmultiplicative property of the spectral norm, and the last follows from Lemma C.1 and the \ufb01rst\nclaim.\nThe following lemma establishes standard eigenvalue and eigenvector perturbation bounds.\nLemma C.3. Let A \u2208Rk\u00d7k be a diagonalizable matrix with k distinct real eigenvalues \u03bb1, \u03bb2, . . . , \u03bbk \u2208\nR corresponding to the (right) eigenvectors \u20d7\u03be1, \u20d7\u03be2, . . . , \u20d7\u03bek \u2208Rk all normalized to have \u2225\u20d7\u03bei\u22252 = 1. Let\nR \u2208Rk\u00d7k be the matrix whose i-th column is \u20d7\u03bei. Let \u02c6A \u2208Rk\u00d7k be a matrix. De\ufb01ne \u01ebA := \u2225\u02c6A\u2212A\u22252,\n\u03b3A := mini\u0338=j |\u03bbi \u2212\u03bbj|, and \u03b53 := \u03ba(R)\u00b7\u01ebA\n\u03b3A\n. Assume \u03b53 < 1\n2. Then there exists a permutation \u03c4 on\n[k] such that the following holds:\n1. \u02c6A has k distinct real eigenvalues \u02c6\u03bb1, \u02c6\u03bb2, . . . , \u02c6\u03bbk \u2208R, and |\u02c6\u03bb\u03c4(i) \u2212\u03bbi| \u2264\u03b53 \u00b7 \u03b3A for all i \u2208[k];\n2. \u02c6A has corresponding (right) eigenvectors \u02c6\u03be1, \u02c6\u03be2, . . . , \u02c6\u03bek \u2208Rk, normalized to have \u2225\u02c6\u03bei\u22252 = 1,\nwhich satisfy \u2225\u02c6\u03be\u03c4(i) \u2212\u20d7\u03bei\u22252 \u22644(k \u22121) \u00b7 \u2225R\u22121\u22252 \u00b7 \u03b53 for all i \u2208[k];\n3. the matrix \u02c6R \u2208Rk\u00d7k whose i-th column is \u02c6\u03be\u03c4(i) satis\ufb01es \u2225\u02c6R \u2212R\u22252 \u2264\u2225\u02c6R \u2212R\u2225F \u22644k1/2(k \u2212\n1) \u00b7 \u2225R\u22121\u22252 \u00b7 \u03b53.\nProof. The Bauer-Fike theorem (Lemma E.3) implies that for every eigenvalue \u02c6\u03bbi of \u02c6A, there exists\nan eigenvalue \u03bbj of A such that |\u02c6\u03bbi \u2212\u03bbj| \u2264\u2225R\u22121( \u02c6A \u2212A)R\u22252 \u2264\u03b53 \u00b7 \u03b3A. Therefore, the assumption\non \u03b53 implies that there exists a permutation \u03c4 such that |\u02c6\u03bb\u03c4(i) \u2212\u03bbi| \u2264\u03b53 \u00b7 \u03b3A < \u03b3A\n2 . In particular,\n\f\f\f\nh\n\u03bbi \u2212\u03b3A\n2 , \u03bbi + \u03b3A\n2\ni\n\u2229{\u02c6\u03bb1, \u02c6\u03bb2, . . . , \u02c6\u03bbk}\n\f\f\f = 1,\n\u2200i \u2208[k].\n(12)\n23\nSince \u02c6A is real, all non-real eigenvalues of \u02c6A must come in conjugate pairs; so the existence of a\nnon-real eigenvalue of \u02c6A would contradict (12). This proves the \ufb01rst claim.\nFor the second claim, assume for notational simplicity that the permutation \u03c4 is the identity\npermutation. Let \u02c6R \u2208Rk\u00d7k be the matrix whose i-th column is \u02c6\u03bei. De\ufb01ne \u20d7\u03b6\u22a4\ni\n\u2208Rk to be the\ni-th row of R\u22121 (i.e., the i-th left eigenvector of A), and similarly de\ufb01ne \u02c6\u03b6\u22a4\ni \u2208Rk to be the i-th\nrow of \u02c6R\u22121. Fix a particular i \u2208[k]. Since {\u20d7\u03be1, \u20d7\u03be2, . . . , \u20d7\u03bek} forms a basis for Rk, we can write\n\u02c6\u03bei = Pk\nj=1 ci,j\u20d7\u03bej for some coe\ufb03cients ci,1, ci,2, . . . , ci,k \u2208R. We may assume ci,i \u22650 (or else we\nreplace \u02c6\u03bei with \u2212\u02c6\u03bei). The fact that \u2225\u02c6\u03bei\u22252 = \u2225\u20d7\u03bej\u22252 = 1 for all j \u2208[k] and the triangle inequality\nimply 1 = \u2225\u02c6\u03bei\u22252 \u2264ci,i\u2225\u20d7\u03bei\u22252 + P\nj\u0338=i |ci,j|\u2225\u20d7\u03bej\u22252 = ci,i + P\nj\u0338=i |ci,j|, and therefore\n\u2225\u02c6\u03bei \u2212\u20d7\u03bei\u22252 \u2264|1 \u2212ci,i|\u2225\u20d7\u03bei\u22252 +\nX\nj\u0338=i\n|ci,j\u2225\u20d7\u03bej\u22252 \u22642\nX\nj\u0338=i\n|ci,j|\nagain by the triangle inequality. Therefore, it su\ufb03ces to show |ci,j| \u22642\u2225R\u22121\u22252 \u00b7\u03b53 for j \u0338= i to prove\nthe second claim.\nObserve that A\u02c6\u03bei = A(Pk\ni\u2032=1 ci,i\u2032\u20d7\u03bei\u2032) = Pk\ni\u2032=1 ci,i\u2032\u03bbi\u2032\u20d7\u03bei\u2032, and therefore\nk\nX\ni\u2032=1\nci,i\u2032\u03bbi\u2032\u20d7\u03bei\u2032 + ( \u02c6A \u2212A)\u02c6\u03bei = \u02c6A\u02c6\u03bei = \u02c6\u03bbi \u02c6\u03bei = \u03bbi\nk\nX\ni\u2032=1\nci,i\u2032\u20d7\u03bei\u2032 + (\u02c6\u03bbi \u2212\u03bbi)\u02c6\u03bei.\nMultiplying through the above equation by \u20d7\u03b6\u22a4\nj , and using the fact that \u20d7\u03b6\u22a4\nj \u20d7\u03bei\u2032 = 1{j = i\u2032} gives\nci,j\u03bbj + \u20d7\u03b6\u22a4\ni ( \u02c6A \u2212A)\u02c6\u03bei = \u03bbici,j + (\u02c6\u03bbi \u2212\u03bbi)\u20d7\u03b6\u22a4\nj \u02c6\u03bei.\nThe above equation rearranges to (\u03bbj \u2212\u03bbi)ci,j = (\u02c6\u03bbi \u2212\u03bbi)\u20d7\u03b6\u22a4\nj \u02c6\u03bei + \u20d7\u03b6\u22a4\nj (A \u2212\u02c6A)\u02c6\u03bei and therefore\n|ci,j| \u2264\u2225\u20d7\u03b6j\u22252 \u00b7 (|\u02c6\u03bbi \u2212\u03bbi| + \u2225( \u02c6A \u2212A)\u02c6\u03bei\u22252)\n|\u03bbj \u2212\u03bbi|\n\u2264\u2225R\u22121\u22252 \u00b7 (|\u02c6\u03bbi \u2212\u03bbi| + \u2225\u02c6A \u2212A\u22252)\n|\u03bbj \u2212\u03bbi|\nby the Cauchy-Schwarz and triangle inequalities and the sub-multiplicative property of the spectral\nnorm. The bound |ci,j| \u22642\u2225R\u22121\u22252 \u00b7 \u03b53 then follows from the \ufb01rst claim.\nThe third claim follows from standard comparisons of matrix norms.\nThe next lemma gives perturbation bounds for estimating the eigenvalues of simultaneously di-\nagonalizable matrices A1, A2, . . . , Ak. The eigenvectors \u02c6R are taken from a perturbation of the \ufb01rst\nmatrix A1, and are then subsequently used to approximately diagonalize the perturbations of the\nremaining matrices A2, . . . , Ak. In practice, one may use Jacobi-like procedures to approximately\nsolve the joint eigenvalue problem.\nLemma C.4. Let A1, A2, . . . , Ak \u2208Rk\u00d7k be diagonalizable matrices that are diagonalized by the\nsame matrix invertible R \u2208Rk\u00d7k with unit length columns \u2225R\u20d7ej\u22252 = 1, such that each Ai has k\ndistinct real eigenvalues:\nR\u22121AiR = diag(\u03bbi,1, \u03bbi,2, . . . , \u03bbi,k).\nLet \u02c6A1, \u02c6A2, . . . , \u02c6Ak \u2208Rk\u00d7k be given. De\ufb01ne \u01ebA := maxi \u2225\u02c6Ai \u2212Ai\u22252, \u03b3A := mini minj\u0338=j\u2032 |\u03bbi,j \u2212\u03bbi,j\u2032|,\n\u03bbmax := maxi,j |\u03bbi,j|, \u03b53 := \u03ba(R)\u00b7\u01ebA\n\u03b3A\n, and \u03b54 := 4k1.5 \u00b7 \u2225R\u22121\u22252\n2 \u00b7 \u03b53. Assume \u03b53 < 1\n2 and \u03b54 < 1. Then\nthere exists a permutation \u03c4 on [k] such that the following holds.\n24\n1. The matrix \u02c6A1 has k distinct real eigenvalues \u02c6\u03bb1,1, \u02c6\u03bb1,2, . . . , \u02c6\u03bb1,k \u2208R, and |\u02c6\u03bb1,j \u2212\u03bb1,\u03c4(j)| \u2264\n\u03b53 \u00b7 \u03b3A for all j \u2208[k].\n2. There exists a matrix \u02c6R \u2208Rk\u00d7k whose j-th column is a right eigenvector corresponding to\n\u02c6\u03bb1,j, scaled so \u2225\u02c6R\u20d7ej\u22252 = 1 for all j \u2208[k], such that \u2225\u02c6R \u2212R\u03c4\u22252 \u2264\n\u03b54\n\u2225R\u22121\u22252 , where R\u03c4 is the\nmatrix obtained by permuting the columns of R with \u03c4.\n3. The matrix \u02c6R is invertible and its inverse satis\ufb01es \u2225\u02c6R\u22121 \u2212R\u22121\n\u03c4 \u22252 \u2264\u2225R\u22121\u22252 \u00b7\n\u03b54\n1\u2212\u03b54 ;\n4. For all i \u2208{2, 3, . . . , k} and all j \u2208[k], the (j, j)-th element of \u02c6R\u22121 \u02c6Ai \u02c6R, denoted by \u02c6\u03bbi,j :=\n\u20d7e\u22a4\nj \u02c6R\u22121 \u02c6Ai \u02c6R\u20d7ej, satis\ufb01es\n|\u02c6\u03bbi,j \u2212\u03bbi,\u03c4(j)| \u2264\n\u0012\n1 +\n\u03b54\n1 \u2212\u03b54\n\u0013\n\u00b7\n\u0012\n1 +\n\u03b54\n\u221a\nk \u00b7 \u03ba(R)\n\u0013\n\u00b7 \u03b53 \u00b7 \u03b3A\n+ \u03ba(R) \u00b7\n\u0012\n1\n1 \u2212\u03b54\n+\n1\n\u221a\nk \u00b7 \u03ba(R)\n+ 1\n\u221a\nk\n\u00b7\n\u03b54\n1 \u2212\u03b54\n\u0013\n\u00b7 \u03b54 \u00b7 \u03bbmax.\nIf \u03b54 \u22641\n2, then |\u02c6\u03bbi,j \u2212\u03bbi,\u03c4(j)| \u22643\u03b53 \u00b7 \u03b3A + 4\u03ba(R) \u00b7 \u03b54 \u00b7 \u03bbmax.\nProof. The \ufb01rst and second claims follow from applying Lemma C.3 to A1 and \u02c6A1. The third claim\nfollows from applying Lemma E.4 to \u02c6R and R\u03c4. To prove the last claim, \ufb01rst de\ufb01ne \u20d7\u03b6\u22a4\nj \u2208Rk (\u02c6\u03b6\u22a4\nj ) to\nbe the j-th row of R\u22121\n\u03c4\n( \u02c6R\u22121), and \u20d7\u03bej \u2208Rk (\u02c6\u03bej) to be the j-th column of R\u03c4 ( \u02c6R), so \u20d7\u03b6\u22a4\nj Ai\u20d7\u03bej = \u03bbi,\u03c4(j)\nand \u02c6\u03b6\u22a4\nj \u02c6Ai \u02c6\u03bej = \u20d7e\u22a4\nj \u02c6R\u22121 \u02c6Ai \u02c6R\u20d7ej = \u02c6\u03bbi,j.\nBy the triangle and Cauchy-Schwarz inequalities and the\nsub-multiplicative property of the spectral norm,\n|\u02c6\u03bbi,j \u2212\u03bbi,\u03c4(j)|\n= |\u02c6\u03b6\u22a4\nj \u02c6Ai\u02c6\u03bej \u2212\u20d7\u03b6\u22a4\nj Ai\u20d7\u03bej|\n= |\u20d7\u03b6\u22a4\nj ( \u02c6Ai \u2212Ai)\u20d7\u03bej + \u20d7\u03b6\u22a4\nj ( \u02c6Ai \u2212Ai)(\u02c6\u03bej \u2212\u20d7\u03bej) + (\u02c6\u03b6j \u2212\u20d7\u03b6j)\u22a4( \u02c6Ai \u2212Ai)\u20d7\u03bej\n+ (\u02c6\u03b6j \u2212\u20d7\u03b6j)\u22a4( \u02c6Ai \u2212Ai)(\u02c6\u03bej \u2212\u20d7\u03bej) + (\u02c6\u03b6j \u2212\u20d7\u03b6j)\u22a4Ai\u20d7\u03bej + \u20d7\u03b6\u22a4\nj Ai(\u02c6\u03bej \u2212\u20d7\u03bej) + (\u02c6\u03b6j \u2212\u20d7\u03b6j)\u22a4Ai(\u02c6\u03bej \u2212\u20d7\u03bej)|\n\u2264|\u20d7\u03b6\u22a4\nj ( \u02c6Ai \u2212Ai)\u20d7\u03bej| + |\u20d7\u03b6\u22a4\nj ( \u02c6Ai \u2212Ai)(\u02c6\u03bej \u2212\u20d7\u03bej)| + |(\u02c6\u03b6j \u2212\u20d7\u03b6j)\u22a4( \u02c6Ai \u2212Ai)\u20d7\u03bej|\n+ |(\u02c6\u03b6j \u2212\u20d7\u03b6j)\u22a4( \u02c6Ai \u2212Ai)(\u02c6\u03bej \u2212\u20d7\u03bej)| + |(\u02c6\u03b6j \u2212\u20d7\u03b6j)\u22a4Ai\u20d7\u03bej| + |\u20d7\u03b6\u22a4\nj Ai(\u02c6\u03bej \u2212\u20d7\u03bej)| + |(\u02c6\u03b6j \u2212\u20d7\u03b6j)\u22a4Ai(\u02c6\u03bej \u2212\u20d7\u03bej)|\n\u2264\u2225\u20d7\u03b6j\u22252 \u00b7 \u2225\u02c6Ai \u2212Ai\u22252 \u00b7 \u2225\u20d7\u03bej\u22252 + \u2225\u20d7\u03b6j\u22252 \u00b7 \u2225\u02c6Ai \u2212Ai\u22252 \u00b7 \u2225\u02c6\u03bej \u2212\u20d7\u03bej\u22252 + \u2225\u02c6\u03b6j \u2212\u20d7\u03b6j\u22252 \u00b7 \u2225\u02c6Ai \u2212Ai\u22252\u2225\u20d7\u03bej\u22252\n+ \u2225\u02c6\u03b6j \u2212\u20d7\u03b6j\u22252 \u00b7 \u2225\u02c6Ai \u2212Ai\u22252 \u00b7 \u2225\u02c6\u03bej \u2212\u20d7\u03bej\u22252\n+ \u2225\u02c6\u03b6j \u2212\u20d7\u03b6j\u22252 \u00b7 \u2225\u03bbi,\u03c4(j)\u20d7\u03bej\u22252 + \u2225\u03bbi,\u03c4(j)\u20d7\u03b6j\u22252 \u00b7 \u2225\u02c6\u03bej \u2212\u20d7\u03bej\u22252 + \u2225\u02c6\u03b6j \u2212\u20d7\u03b6j\u22252 \u00b7 \u2225Ai\u22252 \u00b7 \u2225\u02c6\u03bej \u2212\u20d7\u03bej\u22252.\n(13)\nObserve that \u2225\u20d7\u03b6j\u22252 \u2264\u2225R\u22121\u22252, \u2225\u20d7\u03bej\u22252 \u2264\u2225R\u22252, \u2225\u02c6\u03b6j \u2212\u20d7\u03b6j\u22252 \u2264\u2225\u02c6R\u22121 \u2212R\u22121\n\u03c4 \u22252 \u2264\u2225R\u22121\u22252 \u00b7\n\u03b54\n1\u2212\u03b54 ,\n\u2225\u02c6\u03bej \u2212\u20d7\u03bej\u22252 \u22644k \u00b7 \u2225R\u22121\u22252 \u00b7 \u03b53 (by Lemma C.3), and \u2225Ai\u22252 \u2264\u2225R\u22252 \u00b7 (maxj |\u03bbi,j|) \u00b7 \u2225R\u22121\u22252. Therefore,\n25\ncontinuing from (13), |\u02c6\u03bbi,j \u2212\u03bbi,\u03c4(j)| is bounded as\n|\u02c6\u03bbi,j \u2212\u03bbi,\u03c4(j)| \u2264\u2225R\u22121\u22252 \u00b7 \u2225R\u22252 \u00b7 \u01ebA + \u2225R\u22121\u22252 \u00b7 \u01ebA \u00b7 4k \u00b7 \u2225R\u22121\u22252 \u00b7 \u03b53 + \u2225R\u22121\u22252 \u00b7\n\u03b54\n1 \u2212\u03b54\n\u00b7 \u01ebA \u00b7 \u2225R\u22252\n+ \u2225R\u22121\u22252 \u00b7\n\u03b54\n1 \u2212\u03b54\n\u00b7 \u01ebA \u00b7 4k \u00b7 \u2225R\u22121\u22252 \u00b7 \u03b53\n+ \u03bbmax \u00b7 \u2225R\u22121\u22252 \u00b7\n\u03b54\n1 \u2212\u03b54\n\u00b7 \u2225R\u22252 + \u03bbmax \u00b7 \u2225R\u22121\u22252 \u00b7 4k \u00b7 \u2225R\u22121\u22252 \u00b7 \u03b53\n+ \u2225R\u22121\u22252 \u00b7\n\u03b54\n1 \u2212\u03b54\n\u00b7 \u2225R\u22252 \u00b7 \u03bbmax \u00b7 \u2225R\u22121\u22252 \u00b7 4k \u00b7 \u2225R\u22121\u22252 \u00b7 \u03b53\n= \u03b53 \u00b7 \u03b3A +\n\u03b54\n\u221a\nk \u00b7 \u03ba(R)\n\u00b7 \u03b53 \u00b7 \u03b3A +\n\u03b54\n1 \u2212\u03b54\n\u00b7 \u03b53 \u00b7 \u03b3A\n+\n\u03b54\n\u221a\nk \u00b7 \u03ba(R)\n\u00b7\n\u03b54\n1 \u2212\u03b54\n\u00b7 \u03b53 \u00b7 \u03b3A\n+ \u03ba(R) \u00b7\n1\n1 \u2212\u03b54\n\u00b7 \u03b54 \u00b7 \u03bbmax + 1\n\u221a\nk\n\u00b7 \u03b54 \u00b7 \u03bbmax + \u03ba(R)\n\u221a\nk\n\u00b7\n\u03b54\n1 \u2212\u03b54\n\u00b7 \u03b54 \u00b7 \u03bbmax.\nRearranging gives the claimed inequality.\nLemma C.5. Let V \u2208Rk\u00d7k be an invertible matrix, and let R \u2208Rk\u00d7k be the matrix whose j-th\ncolumn is V \u20d7ej/\u2225V \u20d7ej\u22252. Then \u2225R\u22252 \u2264\u03ba(V ), \u2225R\u22121\u22252 \u2264\u03ba(V ), and \u03ba(R) \u2264\u03ba(V )2.\nProof. We have R = V diag(\u2225V \u20d7e1\u22252, \u2225V \u20d7e2\u22252, . . . , \u2225V \u20d7ek\u22252)\u22121, so by the sub-multiplicative property\nof the spectral norm, \u2225R\u22252 \u2264\u2225V \u22252/ minj \u2225V \u20d7ej\u22252 \u2264\u2225V \u22252/\u03c3k(V ) = \u03ba(V ). Similarly, \u2225R\u22121\u22252 \u2264\n\u2225V \u22121\u22252 \u00b7 maxj \u2225V \u20d7ej\u22252 \u2264\u2225V \u22121\u22252 \u00b7 \u2225V \u22252 = \u03ba(V ).\nThe next lemma shows that randomly projecting a collection of vectors to R does not collapse\nany two too close together, nor does it send any of them too far away from zero.\nLemma C.6. Fix any \u03b4 \u2208(0, 1) and matrix A \u2208Rm\u00d7n (with m \u2264n). Let \u20d7\u03b8 \u2208Rm be a random\nvector distributed uniformly over Sm\u22121.\n1. Pr\n\u0014\nmin\ni\u0338=j |\u27e8\u20d7\u03b8, A(\u20d7ei \u2212\u20d7ej)\u27e9| > mini\u0338=j \u2225A(\u20d7ei \u2212\u20d7ej)\u22252 \u00b7 \u03b4\n\u221aem\n\u0000n\n2\n\u0001\n\u0015\n\u22651 \u2212\u03b4.\n2. Pr\n\u0014\n\u2200i \u2208[m], |\u27e8\u20d7\u03b8, A\u20d7ei\u27e9| \u2264\u2225A\u20d7ei\u22252\n\u221am\n\u0010\n1 +\np\n2 ln(m/\u03b4)\n\u0011\u0015\n\u22651 \u2212\u03b4.\nProof. For the \ufb01rst claim, let \u03b40 := \u03b4/\n\u0000n\n2\n\u0001\n. By Lemma F.2, for any \ufb01xed pair {i, j} \u2286[n] and\n\u03b2 := \u03b40/\u221ae,\nPr\n\u0014\n|\u27e8\u20d7\u03b8, A(\u20d7ei \u2212\u20d7ej)\u27e9| \u2264\u2225A(\u20d7ei \u2212\u20d7ej)\u22252 \u00b7\n1\n\u221am \u00b7 \u03b40\n\u221ae\n\u0015\n\u2264exp\n\u00121\n2(1 \u2212(\u03b42\n0/e) + ln(\u03b42\n0/e))\n\u0013\n\u2264\u03b40.\nTherefore the \ufb01rst claim follows by a union bound over all\n\u0000n\n2\n\u0001\npairs {i, j}.\nFor the second claim, apply Lemma F.2 with \u03b2 := 1 + t and t :=\np\n2 ln(m/\u03b4) to obtain\nPr\n\u0014\n|\u27e8\u20d7\u03b8, A\u20d7ei\u27e9| \u2265\u2225A\u20d7ei\u22252\n\u221am\n\u00b7 (1 + t)\n\u0015\n\u2264exp\n\u00121\n2\n\u0010\n1 \u2212(1 + t)2 + 2 ln(1 + t)\n\u0011\u0013\n\u2264exp\n\u00121\n2\n\u0010\n1 \u2212(1 + t)2 + 2t\n\u0011\u0013\n= e\u2212t2/2 = \u03b4/m.\n26\nTherefore the second claim follows by taking a union bound over all i \u2208[m].\nD\nProofs and details from Section 4\nIn this section, we provide ommitted proofs and details from Section 4.\nD.1\nProof of Proposition 4.1\nAs in the proof of Lemma 3.1, it is easy to show that\nQ1,2,3(\u20d7\u03c6, \u20d7\u03c8) = E[E[\u20d7x1|h] \u2297E[\u20d7x2|h]\u27e8\u20d7\u03c6, E[\u20d7x3 \u2297\u20d7x3|h]\u20d7\u03c8\u27e9]\n= M1E[\u20d7eh \u2297\u20d7eh\u27e8\u20d7\u03c6, (\u20d7\u00b53,h \u2297\u20d7\u00b53,h + \u03a33,h)\u20d7\u03c8\u27e9]M \u22a4\n2\n= M1 diag(\u27e8\u20d7\u03c6, \u20d7\u00b53,t\u27e9\u27e8\u20d7\u03c8, \u20d7\u00b53,t\u27e9+ \u27e8\u20d7\u03c6, \u03a33,t \u20d7\u03c8\u27e9: t \u2208[k]) diag(\u20d7w)M \u22a4\n2 .\nThe claim then follows from the same arguments used in the proof of Lemma 3.2.\nD.2\nProof of Proposition 4.2\nThe conditional independence properties follow from the HMM conditional independence assump-\ntions. To check the parameters, observe \ufb01rst that\nPr[h1 = i|h2 = j] = Pr[h2 = j|h1 = i] \u00b7 Pr[h1 = i]\nPr[h2 = j]\n= Tj,i\u03c0i\n(T\u20d7\u03c0)j\n= \u20d7ei diag(\u20d7\u03c0)T \u22a4diag(T\u20d7\u03c0)\u22121\u20d7ej\nby Bayes\u2019 rule. Therefore\nM1\u20d7ej = E[\u20d7x1|h2 = j] = OE[\u20d7eh1|h2 = j] = O diag(\u20d7\u03c0)T \u22a4diag(T\u20d7\u03c0)\u22121\u20d7ej.\nThe rest of the parameters are similar to verify.\nD.3\nLearning mixtures of product distributions\nIn this section, we show how to use Algorithm B with mixtures of product distributions in Rn that\nsatisfy an incoherence condition on the means \u20d7\u00b51, \u20d7\u00b52, . . . , \u20d7\u00b5k \u2208Rn of k component distributions.\nNote that product distributions are just a special case of the more general class of multi-view\ndistributions, which are directly handled by Algorithm B.\nThe basic idea is to randomly partition the coordinates into \u2113\u22653 \u201cviews\u201d, each of roughly the\nsame dimension. Under the assumption that the component distributions are product distributions,\nthe multi-view assumption is satis\ufb01ed. What remains to be checked is that the non-degeneracy\ncondition (Condition 3.1) is satis\ufb01ed. Theorem D.1 (below) shows that it su\ufb03ces that the original\nmatrix of component means have rank k and satisfy the following incoherence condition.\nCondition D.1 (Incoherence condition). Let \u03b4 \u2208(0, 1), \u2113\u2208[n], and M = [\u20d7\u00b51|\u20d7\u00b52| \u00b7 \u00b7 \u00b7 |\u20d7\u00b5k] \u2208Rn\u00d7k\nbe given; let M = USV \u22a4be the thin singular value decomposition of M, where U \u2208Rn\u00d7k is a\nmatrix of orthonormal columns, S = diag(\u03c31(M), \u03c32(M), . . . , \u03c3k(M)) \u2208Rk\u00d7k, and V \u2208Rk\u00d7k is\northogonal; and let\ncM := max\nj\u2208[n]\n\u001an\nk \u00b7 \u2225U \u22a4\u20d7ej\u22252\n2\n\u001b\n.\n27\nThe following inequality holds:\ncM \u22649\n32 \u00b7\nn\nk\u2113ln k\u2113\n\u03b4\n.\nNote that cM is always in the interval [1, n/k]; it is smallest when the left singular vectors in\nU have \u00b11/\u221an entries (as in a Hadamard basis), and largest when the singular vectors are the\ncoordinate axes. Roughly speaking, the incoherence condition requires that the non-degeneracy of\na matrix M be witnessed by many vertical blocks of M. When the condition is satis\ufb01ed, then with\nhigh probability, a random partitioning of the coordinates into \u2113groups induces a block partitioning\nof M into \u2113matrices M1, M2, . . . , M\u2113(with roughly equal number of rows) such that the k-th largest\nsingular value of Mv is not much smaller than that of M (for each v \u2208[\u2113]).\nChaudhuri and Rao (2008) show that under a similar condition (which they call a spreading\ncondition), a random partitioning of the coordinates into two \u201cviews\u201d preserves the separation\nbetween the means of k component distributions.\nThey then follow this preprocessing with a\nprojection based on the correlations across the two views (similar to CCA). However, their overall\nalgorithm requires a minimum separation condition on the means of the component distributions.\nIn contrast, Algorithm B does not require a minimum separation condition at all in this setting.\nTheorem D.1. Assume Condition D.1 holds. Independently put each coordinate i \u2208[n] into one\nof \u2113di\ufb00erent sets I1, I2, . . . , I\u2113chosen uniformly at random. With probability at least 1 \u2212\u03b4, for\neach v \u2208[\u2113], the matrix Mv \u2208R|Iv|\u00d7k formed by selecting the rows of M indexed by Iv, satis\ufb01es\n\u03c3k(Mv) \u2265\u03c3k(M)/(2\n\u221a\n\u2113).\nProof. Follows from Lemma D.1 (below) together with a union bound.\nLemma D.1. Assume Condition D.1 holds. Consider a random submatrix c\nM of M obtained by\nindependently deciding to include each row of M with probability 1/\u2113. Then\nPr\nh\n\u03c3k(c\nM) \u2265\u03c3k(M)/(2\n\u221a\n\u2113)\ni\n\u22651 \u2212\u03b4/\u2113.\nProof. Let z1, z2, . . . , zn \u2208{0, 1} be independent indicator random variables, each with Pr[zi =\n1] = 1/\u2113. Note that c\nM \u22a4c\nM = M \u22a4diag(z1, z2, . . . , zn)M = Pn\ni=1 ziM \u22a4eie\u22a4\ni M, and that\n\u03c3k(c\nM)2 = \u03bbmin(c\nM \u22a4c\nM) \u2265\u03bbmin(S)2 \u00b7 \u03bbmin\n\u0012 n\nX\ni=1\nziU \u22a4eie\u22a4\ni U\n\u0013\n.\nMoreover, 0 \u2aafziU \u22a4eie\u22a4\ni U \u2aaf(k/n)cMI and \u03bbmin(E[Pn\ni=1 ziU \u22a4eie\u22a4\ni U]) = 1/\u2113. By Lemma F.3 (a\nCherno\ufb00bound on extremal eigenvalues of random symmetric matrices),\nPr\n\"\n\u03bbmin\n\u0012 d\nX\nj=1\nziU \u22a4eie\u22a4\ni U\n\u0013\n\u22641\n4\u2113\n#\n\u2264k \u00b7 e\u2212(3/4)2/(2\u2113cM k/n) \u2264\u03b4/\u2113\nby the assumption on cM.\n28\nD.4\nEmpirical moments for multi-view mixtures of subgaussian distributions\nThe required concentration behavior of the empirical moments used by Algorithm B can be easily\nestablished for multi-view Gaussian mixture models using known techniques (Chaudhuri et al.,\n2009). This is clear for the second-order statistics \u02c6Pa,b for {a, b} \u2208{{1, 2}, {1, 3}}, and remains\ntrue for the third-order statistics \u02c6P1,2,3 because \u20d7x3 is conditionally independent of \u20d7x1 and \u20d7x2 given\nh. The magnitude of \u27e8\u02c6U3\u20d7\u03b8i, \u20d7x3\u27e9can be bounded for all samples (with a union bound; recall that\nwe make the simplifying assumption that \u02c6P1,3 is independent of \u02c6P1,2,3, and therefore so are \u02c6U3 and\n\u02c6P1,2,3). Therefore, one e\ufb00ectively only needs spectral norm error bounds for second-order statistics,\nas provided by existing techniques.\nIndeed, it is possible to establish Condition 3.2 in the case where the conditional distribution\nof \u20d7xv given h (for each view v) is subgaussian. Speci\ufb01cally, we assume that there exists some \u03b1 > 0\nsuch that for each view v and each component j \u2208[k],\nE\n\u0014\nexp\n\u0010\n\u03bb\u27e8\u20d7u, cov(\u20d7xv|h = j)\u22121/2(\u20d7xv \u2212E[\u20d7xv|h = j])\u27e9\n\u0011\u0015\n\u2264exp(\u03b1\u03bb2/2),\n\u2200\u03bb \u2208R, \u20d7u \u2208Sd\u22121\nwhere cov(\u20d7x|h = j) := E[(\u20d7xv \u2212E[\u20d7xv|h = j]) \u2297(\u20d7xv \u2212E[\u20d7xv|h = j])|h = j] is assumed to be positive\nde\ufb01nite. Using standard techniques (e.g., Vershynin (2012)), Condition 3.2 can be shown to hold\nunder the above conditions with the following parameters (for some universal constant c > 0):\nwmin := min\nj\u2208[k] wj\nN0 := \u03b13/2(d + log(1/\u03b4))\nwmin\nlog \u03b13/2(d + log(1/\u03b4))\nwmin\nCa,b := c \u00b7\n\u0010\nmax\nn\n\u2225cov(\u20d7xv|h = j)\u22251/2\n2\n, \u2225E[\u20d7xv|h = j]\u22252 : v \u2208{a, b}, j \u2208[k]\no\u00112\nC1,2,3 := c \u00b7\n\u0010\nmax\n\b\n\u2225cov(\u20d7xv|h = j)\u22251/2\n2\n, \u2225E[\u20d7xv|h = j]\u22252 : v \u2208[3], j \u2208[k]\no\u00113\nf(N, \u03b4) :=\nr\nk2 log(1/\u03b4)\nN\n+\ns\n\u03b13/2p\nlog(N/\u03b4)(d + log(1/\u03b4))\nwminN\n.\nE\nGeneral results from matrix perturbation theory\nThe lemmas in this section are standard results from matrix perturbation theory, taken from Stewart\nand Sun (1990).\nLemma E.1 (Weyl\u2019s theorem). Let A, E \u2208Rm\u00d7n with m \u2265n be given. Then\nmax\ni\u2208[n] |\u03c3i(A + E) \u2212\u03c3i(A)| \u2264\u2225E\u22252.\nProof. See Theorem 4.11, p. 204 in Stewart and Sun (1990).\nLemma E.2 (Wedin\u2019s theorem). Let A, E \u2208Rm\u00d7n with m \u2265n be given. Let A have the singular\nvalue decomposition\n\uf8ee\n\uf8f0\nU \u22a4\n1\nU \u22a4\n2\nU \u22a4\n3\n\uf8f9\n\uf8fbA\n\u0002\nV1\nV2\n\u0003\n=\n\uf8ee\n\uf8f0\n\u03a31\n0\n0\n\u03a32\n0\n0\n\uf8f9\n\uf8fb.\n29\nLet \u02dcA := A + E, with analogous singular value decomposition ( \u02dcU1, \u02dcU2, \u02dcU3, \u02dc\u03a31, \u02dc\u03a32, \u02dcV1 \u02dcV2). Let \u03a6 be\nthe matrix of canonical angles between range(U1) and range( \u02dcU1), and \u0398 be the matrix of canonical\nangles between range(V1) and range( \u02dcV1). If there exists \u03b4, \u03b1 > 0 such that mini \u03c3i(\u02dc\u03a31) \u2265\u03b1 + \u03b4 and\nmaxi \u03c3i(\u03a32) \u2264\u03b1, then\nmax{\u2225sin \u03a6\u22252, \u2225sin \u0398\u22252} \u2264\u2225E\u22252\n\u03b4\n.\nProof. See Theorem 4.4, p. 262 in Stewart and Sun (1990).\nLemma E.3 (Bauer-Fike theorem). Let A, E \u2208Rk\u00d7k be given. If A = V diag(\u03bb1, \u03bb2, . . . , \u03bbk)V \u22121\nfor some invertible V \u2208Rk\u00d7k, and \u02dcA := A + E has eigenvalues \u02dc\u03bb1, \u02dc\u03bb2, . . . , \u02dc\u03bbk, then\nmax\ni\u2208[k] min\nj\u2208[k]|\u02dc\u03bbi \u2212\u03bbj| \u2264\u2225V \u22121EV \u22252.\nProof. See Theorem 3.3, p. 192 in Stewart and Sun (1990).\nLemma E.4. Let A, E \u2208Rk\u00d7k be given. If A is invertible, and \u2225A\u22121E\u22252 < 1, then \u02dcA := A + E is\ninvertible, and\n\u2225\u02dcA\u22121 \u2212A\u22121\u22252 \u2264\u2225E\u22252\u2225A\u22121\u22252\n2\n1 \u2212\u2225A\u22121E\u22252\n.\nProof. See Theorem 2.5, p. 118 in Stewart and Sun (1990).\nF\nProbability inequalities\nLemma F.1 (Accuracy of empirical probabilities). Fix \u20d7\u00b5 = (\u00b51, \u00b52, . . . , \u00b5n) \u2208\u2206m\u22121. Let \u20d7x be a\nrandom vector for which Pr[\u20d7x = \u20d7ei] = \u00b5i for all i \u2208[m], and let \u20d7x1, \u20d7x2, . . . , \u20d7xn be n independent\ncopies of \u20d7x. Set \u02c6\u00b5 := (1/n) Pn\ni=1 \u20d7xi. For all t > 0,\nPr\n\u0014\n\u2225\u02c6\u00b5 \u2212\u20d7\u00b5\u22252 > 1 +\n\u221a\nt\n\u221an\n\u0015\n\u2264e\u2212t.\nProof. This is a standard application of McDiarmid\u2019s inequality (using the fact that \u2225\u02c6\u00b5 \u2212\u20d7\u00b5\u22252 has\n\u221a\n2/n bounded di\ufb00erences when a single \u20d7xi is changed), together with the bound E[\u2225\u02c6\u00b5 \u2212\u20d7\u00b5\u22252] \u2264\n1/\u221an. See Proposition 19 in Hsu et al. (2012).\nLemma F.2 (Random projection). Let \u20d7\u03b8 \u2208Rn be a random vector distributed uniformly over\nSn\u22121, and \ufb01x a vector \u20d7v \u2208Rn.\n1. If \u03b2 \u2208(0, 1), then\nPr\n\u0014\n|\u27e8\u20d7\u03b8,\u20d7v\u27e9| \u2264\u2225\u20d7v\u22252 \u00b7 1\n\u221an \u00b7 \u03b2\n\u0015\n\u2264exp\n\u00121\n2(1 \u2212\u03b22 + ln \u03b22)\n\u0013\n.\n2. If \u03b2 > 1, then\nPr\n\u0014\n|\u27e8\u20d7\u03b8,\u20d7v\u27e9| \u2265\u2225\u20d7v\u22252 \u00b7 1\n\u221an \u00b7 \u03b2\n\u0015\n\u2264exp\n\u00121\n2(1 \u2212\u03b22 + ln \u03b22)\n\u0013\n.\nProof. This is a special case of Lemma 2.2 from Dasgupta and Gupta (2003).\n30\nLemma F.3 (Matrix Cherno\ufb00bound). Let X1, X2, . . . , Xn be independent and symmetric m \u00d7 m\nrandom matrices such that 0 \u2aafXi \u2aafrI, and set l := \u03bbmin(E[X1 + X2 + \u00b7 \u00b7 \u00b7 + Xn]). For any\n\u01eb \u2208[0, 1],\nPr\n\"\n\u03bbmin\n\u0012 n\nX\ni=1\nXi\n\u0013\n\u2264(1 \u2212\u01eb) \u00b7 l\n#\n\u2264m \u00b7 e\u2212\u01eb2l/(2r).\nProof. This is a direct corollary of Theorem 19 from Ahlswede and Winter (2002).\nG\nInsu\ufb03ciency of second-order moments\nChang (1996) shows that a simple class of Markov models used in mathematical phylogenetics\ncannot be identi\ufb01ed from pair-wise probabilities alone. Below, we restate (a specialization of) this\nresult in terms of the document topic model from Section 2.1.\nProposition G.1 (Chang, 1996). Consider the model from Section 2.1 on (h, x1, x2, . . . , x\u2113) with\nparameters M and \u20d7w. Let Q \u2208Rk\u00d7k be an invertible matrix such that the following hold:\n1. \u20d71\u22a4Q = \u20d71\u22a4;\n2. MQ\u22121, Q diag(\u20d7w)M \u22a4diag(M \u20d7w)\u22121, and Q\u20d7w have non-negative entries;\n3. Q diag(\u20d7w)Q\u22a4is a diagonal matrix.\nThen the marginal distribution over (x1, x2) is identical to that in the case where the model has\nparameters \u02dc\nM := MQ\u22121 and \u02dcw := Q\u20d7w.\nA simple example for d = k = 2 can be obtained from\nM :=\n\u0014\np\n1 \u2212p\n1 \u2212p\np\n\u0015\n,\n\u20d7w :=\n\u00141/2\n1/2\n\u0015\n,\nQ :=\n\uf8ee\n\uf8f0\np\n1+\u221a\n1+4p(1\u2212p)\n2\n1 \u2212p\n1\u2212\u221a\n1+4p(1\u2212p)\n2\n\uf8f9\n\uf8fb\nfor some p \u2208(0, 1). We take p = 0.25, in which case Q satis\ufb01es the conditions of Proposition G.1,\nand\nM =\n\u00140.25\n0.75\n0.75\n0.25\n\u0015\n,\n\u20d7w =\n\u00140.5\n0.5\n\u0015\n,\n\u02dc\nM = MQ\u22121 \u2248\n\u00140.6614\n0.1129\n0.3386\n0.8871\n\u0015\n,\n\u02dcw = Q\u20d7w \u2248\n\u00140.7057\n0.2943\n\u0015\n.\nIn this case, both (M, \u20d7w) and ( \u02dc\nM, \u02dcw) give rise to the same pair-wise probabilities\nM diag(\u20d7w)M \u22a4= \u02dc\nM diag( \u02dcw) \u02dc\nM \u22a4\u2248\n\u00140.3125\n0.1875\n0.1875\n0.3125\n\u0015\n.\nHowever, the triple-wise probabilities, for \u03b7 = (1, 0), di\ufb00er: for (M, \u20d7w), we have\nM diag(M \u22a4\u03b7) diag(\u20d7w)M \u22a4\u2248\n\u00140.2188\n0.0938\n0.0938\n0.0938\n\u0015\n;\nwhile for ( \u02dc\nM, \u02dcw), we have\n\u02dc\nM diag( \u02dc\nM \u22a4\u03b7) diag( \u02dcw) \u02dc\nM \u22a4\u2248\n\u00140.2046\n0.1079\n0.1079\n0.0796\n\u0015\n.\n31\n",
        "sentence": " Anandkumar et al. (2012c) showed that for M2 and M3 of the forms in (5), it is possible to efficiently accomplish this. Then we use results from Anandkumar et al. (2012c) to convert this error into a bound on the actual parameter estimates \u03b8\u0302 = (\u03c0\u0302, B\u0302) derived from the robust tensor power method.",
        "context": "The \ufb01rst step is to perform the estimation of M3 using Algorithm B as is. Then, to estimate\nM2, one may re-use the eigenvectors in \u02c6R1 to diagonalize \u02c6B1,3,2(\u20d7\u03b7), as B1,2,3(\u20d7\u03b7) and B1,3,2(\u20d7\u03b7) share\nand the parameters of the resulting three-view mixture model on (h, \u20d7x1, \u20d7x2, \u20d7x3) are\n\u20d7w := T\u20d7\u03c0,\nM1 := O diag(\u20d7\u03c0)T \u22a4diag(T\u20d7\u03c0)\u22121,\nM2:= O,\nM3 := OT.\nFrom Proposition 4.2, it is easy to verify that B3,1,2(\u20d7\u03b7) = (U \u22a4\n3 OT) diag(O\u22a4\u20d7\u03b7)(U \u22a4\n3 OT)\u22121. There-\n\u00b7 \u00b7 \u00b7\n\u27e8\u20d7\u03b8k, U \u22a4\n3 M3\u20d7ek\u27e9\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fb= \u0398U \u22a4\n3 M3.\n19\nB.4\nOrdering issues\nAlthough Algorithm B only explicitly yields estimates for M3, it can easily be applied to estimate"
    },
    {
        "title": "Tensor decompositions for learning latent variable models",
        "author": [
            "Anandkumar",
            "Anima",
            "Ge",
            "Rong",
            "Hsu",
            "Daniel",
            "Kakade",
            "Sham M",
            "Telgarsky",
            "Matus"
        ],
        "venue": "CoRR, abs/1210.7559,",
        "citeRegEx": "Anandkumar et al\\.,? \\Q2012\\E",
        "shortCiteRegEx": "Anandkumar et al\\.",
        "year": 2012,
        "abstract": "This work considers a computationally and statistically efficient parameter\nestimation method for a wide class of latent variable models---including\nGaussian mixture models, hidden Markov models, and latent Dirichlet\nallocation---which exploits a certain tensor structure in their low-order\nobservable moments (typically, of second- and third-order). Specifically,\nparameter estimation is reduced to the problem of extracting a certain\n(orthogonal) decomposition of a symmetric tensor derived from the moments; this\ndecomposition can be viewed as a natural generalization of the singular value\ndecomposition for matrices. Although tensor decompositions are generally\nintractable to compute, the decomposition of these specially structured tensors\ncan be efficiently obtained by a variety of approaches, including power\niterations and maximization approaches (similar to the case of matrices). A\ndetailed analysis of a robust tensor power method is provided, establishing an\nanalogue of Wedin's perturbation theorem for the singular vectors of matrices.\nThis implies a robust and computationally tractable estimation approach for\nseveral popular latent variable models.",
        "full_text": "arXiv:1210.7559v4  [cs.LG]  13 Nov 2014\nJournal of Machine Learning Research 15 (2014) 2773-2832\nSubmitted 2/13; Revised 3/14; Published 8/14\nTensor Decompositions for Learning Latent Variable Models\nAnimashree Anandkumar\na.anandkumar@uci.edu\nElectrical Engineering and Computer Science\nUniversity of California, Irvine\n2200 Engineering Hall\nIrvine, CA 92697\nRong Ge\nrongge@microsoft.com\nMicrosoft Research\nOne Memorial Drive\nCambridge, MA 02142\nDaniel Hsu\ndjhsu@cs.columbia.edu\nDepartment of Computer Science\nColumbia University\n1214 Amsterdam Avenue, #0401\nNew York, NY 10027\nSham M. Kakade\nskakade@microsoft.com\nMicrosoft Research\nOne Memorial Drive\nCambridge, MA 02142\nMatus Telgarsky\nmtelgars@cs.ucsd.edu\nDepartment of Statistics\nRutgers University\n110 Frelinghuysen Road\nPiscataway, NJ 08854\nEditor: Benjamin Recht\nAbstract\nThis work considers a computationally and statistically e\ufb03cient parameter estimation\nmethod for a wide class of latent variable models\u2014including Gaussian mixture models,\nhidden Markov models, and latent Dirichlet allocation\u2014which exploits a certain tensor\nstructure in their low-order observable moments (typically, of second- and third-order).\nSpeci\ufb01cally, parameter estimation is reduced to the problem of extracting a certain (orthog-\nonal) decomposition of a symmetric tensor derived from the moments; this decomposition\ncan be viewed as a natural generalization of the singular value decomposition for matrices.\nAlthough tensor decompositions are generally intractable to compute, the decomposition\nof these specially structured tensors can be e\ufb03ciently obtained by a variety of approaches,\nincluding power iterations and maximization approaches (similar to the case of matrices).\nA detailed analysis of a robust tensor power method is provided, establishing an analogue\nof Wedin\u2019s perturbation theorem for the singular vectors of matrices. This implies a ro-\nbust and computationally tractable estimation approach for several popular latent variable\nmodels.\nc\u20dd2014 Animashree Anandkumar, Rong Ge, Daniel Hsu, Sham M. Kakade, and Matus Telgarsky.\nAnandkumar, Ge, Hsu, Kakade, and Telgarsky\nKeywords:\nlatent variable models, tensor decompositions, mixture models, topic models,\nmethod of moments, power method\n1. Introduction\nThe method of moments is a classical parameter estimation technique (Pearson, 1894) from\nstatistics which has proved invaluable in a number of application domains.\nThe basic\nparadigm is simple and intuitive: (i) compute certain statistics of the data\u2014often empirical\nmoments such as means and correlations\u2014and (ii) \ufb01nd model parameters that give rise to\n(nearly) the same corresponding population quantities. In a number of cases, the method of\nmoments leads to consistent estimators which can be e\ufb03ciently computed; this is especially\nrelevant in the context of latent variable models, where standard maximum likelihood ap-\nproaches are typically computationally prohibitive, and heuristic methods can be unreliable\nand di\ufb03cult to validate with high-dimensional data. Furthermore, the method of moments\ncan be viewed as complementary to the maximum likelihood approach; simply taking a\nsingle step of Newton-Raphson on the likelihood function starting from the moment based\nestimator (Le Cam, 1986) often leads to the best of both worlds: a computationally e\ufb03cient\nestimator that is (asymptotically) statistically optimal.\nThe primary di\ufb03culty in learning latent variable models is that the latent (hidden)\nstate of the data is not directly observed; rather only observed variables correlated with the\nhidden state are observed. As such, it is not evident the method of moments should fare\nany better than maximum likelihood in terms of computational performance: matching the\nmodel parameters to the observed moments may involve solving computationally intractable\nsystems of multivariate polynomial equations. Fortunately, for many classes of latent vari-\nable models, there is rich structure in low-order moments (typically second- and third-order)\nwhich allow for this inverse moment problem to be solved e\ufb03ciently (Cattell, 1944; Cardoso,\n1991; Chang, 1996; Mossel and Roch, 2006; Hsu et al., 2012b; Anandkumar et al., 2012c,a;\nHsu and Kakade, 2013).\nWhat is more is that these decomposition problems are often\namenable to simple and e\ufb03cient iterative methods, such as gradient descent and the power\niteration method.\n1.1 Contributions\nIn this work, we observe that a number of important and well-studied latent variable\nmodels\u2014including Gaussian mixture models, hidden Markov models, and Latent Dirichlet\nallocation\u2014share a certain structure in their low-order moments, and this permits certain\ntensor decomposition approaches to parameter estimation.\nIn particular, this decompo-\nsition can be viewed as a natural generalization of the singular value decomposition for\nmatrices.\nWhile much of this (or similar) structure was implicit in several previous works (Chang,\n1996; Mossel and Roch, 2006; Hsu et al., 2012b; Anandkumar et al., 2012c,a; Hsu and Kakade,\n2013), here we make the decomposition explicit under a uni\ufb01ed framework. Speci\ufb01cally, we\nexpress the observable moments as sums of rank-one terms, and reduce the parameter\nestimation task to the problem of extracting a symmetric orthogonal decomposition of a\nsymmetric tensor derived from these observable moments. The problem can then be solved\nby a variety of approaches, including \ufb01xed-point and variational methods.\n2774\nTensor Decompositions for Learning Latent Variable Models\nOne approach for obtaining the orthogonal decomposition is the tensor power method\nof Lathauwer et al. (2000, Remark 3). We provide a convergence analysis of this method for\northogonally decomposable symmetric tensors, as well as a detailed perturbation analysis\nfor a robust (and a computationally tractable) variant (Theorem 5.1). This perturbation\nanalysis can be viewed as an analogue of Wedin\u2019s perturbation theorem for singular vectors\nof matrices (Wedin, 1972), providing a bound on the error of the recovered decomposition\nin terms of the operator norm of the tensor perturbation. This analysis is subtle in at least\ntwo ways. First, unlike for matrices (where every matrix has a singular value decomposi-\ntion), an orthogonal decomposition need not exist for the perturbed tensor. Our robust\nvariant uses random restarts and de\ufb02ation to extract an approximate decomposition in a\ncomputationally tractable manner. Second, the analysis of the de\ufb02ation steps is non-trivial;\na na\u00a8\u0131ve argument would entail error accumulation in each de\ufb02ation step, which we show can\nin fact be avoided. When this method is applied for parameter estimation in latent variable\nmodels previously discussed, improved sample complexity bounds (over previous work) can\nbe obtained using this perturbation analysis.\nFinally, we also address computational issues that arise when applying the tensor de-\ncomposition approaches to estimating latent variable models. Speci\ufb01cally, we show that the\nbasic operations of simple iterative approaches (such as the tensor power method) can be\ne\ufb03ciently executed in time linear in the dimension of the observations and the size of the\ntraining data. For instance, in a topic modeling application, the proposed methods require\ntime linear in the number of words in the vocabulary and in the number of non-zero entries\nof the term-document matrix. The combination of this computational e\ufb03ciency and the\nrobustness of the tensor decomposition techniques makes the overall framework a promising\napproach to parameter estimation for latent variable models.\n1.2 Related Work\nThe connection between tensor decompositions and latent variable models has a long history\nacross many scienti\ufb01c and mathematical disciplines. We review some of the key works that\nare most closely related to ours.\n1.2.1 Tensor Decompositions\nThe role of tensor decompositions in the context of latent variable models dates back to early\nuses in psychometrics (Cattell, 1944). These ideas later gained popularity in chemometrics,\nand more recently in numerous science and engineering disciplines, including neuroscience,\nphylogenetics, signal processing, data mining, and computer vision. A thorough survey of\nthese techniques and applications is given by Kolda and Bader (2009). Below, we discuss a\nfew speci\ufb01c connections to two applications in machine learning and statistics, independent\ncomponent analysis and latent variable models (between which there is also signi\ufb01cant\noverlap).\nTensor decompositions have been used in signal processing and computational neuro-\nscience for blind source separation and independent component analysis (ICA) (Comon and Jutten,\n2010). Here, statistically independent non-Gaussian sources are linearly mixed in the ob-\nserved signal, and the goal is to recover the mixing matrix (and ultimately, the original\nsource signals).\nA typical solution is to locate projections of the observed signals that\n2775\nAnandkumar, Ge, Hsu, Kakade, and Telgarsky\ncorrespond to local extrema of the so-called \u201ccontrast functions\u201d which distinguish Gaus-\nsian variables from non-Gaussian variables. This method can be e\ufb00ectively implemented\nusing fast descent algorithms (Hyvarinen, 1999).\nWhen using the excess kurtosis (i.e.,\nfourth-order cumulant) as the contrast function, this method reduces to a generalization of\nthe power method for symmetric tensors (Lathauwer et al., 2000; Zhang and Golub, 2001;\nKo\ufb01dis and Regalia, 2002).\nThis case is particularly important, since all local extrema\nof the kurtosis objective correspond to the true sources (under the assumed statistical\nmodel) (Delfosse and Loubaton, 1995); the descent methods can therefore be rigorously an-\nalyzed, and their computational and statistical complexity can be bounded (Frieze et al.,\n1996; Nguyen and Regev, 2009; Arora et al., 2012b).\nHigher-order tensor decompositions have also been used to develop estimators for com-\nmonly used mixture models, hidden Markov models, and other related latent variable\nmodels, often using the the algebraic procedure of R. Jennrich (as reported in the ar-\nticle of Harshman, 1970), which is based on a simultaneous diagonalization of di\ufb00erent\nways of \ufb02attening a tensor to matrices. Jennrich\u2019s procedure was employed for parame-\nter estimation of discrete Markov models by Chang (1996) via pair-wise and triple-wise\nprobability tables; and it was later used for other latent variable models such as hidden\nMarkov models (HMMs), latent trees, Gaussian mixture models, and topic models such\nas latent Dirichlet allocation (LDA) by many others (Mossel and Roch, 2006; Hsu et al.,\n2012b; Anandkumar et al., 2012c,a; Hsu and Kakade, 2013). In these contexts, it is often\nalso possible to establish strong identi\ufb01ability results, without giving an explicit estimators,\nby invoking the non-constructive identi\ufb01ability argument of Kruskal (1977)\u2014see the article\nby Allman et al. (2009) for several examples.\nRelated simultaneous diagonalization approaches have also been used for blind source\nseparation and ICA (as discussed above), and a number of e\ufb03cient algorithms have been\ndeveloped for this problem (Bunse-Gerstner et al., 1993; Cardoso and Souloumiac, 1993;\nCardoso, 1994; Cardoso and Comon, 1996; Corless et al., 1997; Ziehe et al., 2004). A rather\ndi\ufb00erent technique that uses tensor \ufb02attening and matrix eigenvalue decomposition has\nbeen developed by Cardoso (1991) and later by De Lathauwer et al. (2007). A signi\ufb01cant\nadvantage of this technique is that it can be used to estimate overcomplete mixtures, where\nthe number of sources is larger than the observed dimension.\nThe relevance of tensor analysis to latent variable modeling has been long recognized in\nthe \ufb01eld of algebraic statistics (Pachter and Sturmfels, 2005), and many works characterize\nthe algebraic varieties corresponding to the moments of various classes of latent variable\nmodels (Drton et al., 2007; Sturmfels and Zwiernik, 2013). These works typically do not\naddress computational or \ufb01nite sample issues, but rather are concerned with basic questions\nof identi\ufb01ability.\nThe speci\ufb01c tensor structure considered in the present work is the symmetric orthog-\nonal decomposition.\nThis decomposition expresses a tensor as a linear combination of\nsimple tensor forms; each form is the tensor product of a vector (i.e., a rank-1 tensor),\nand the collection of vectors form an orthonormal basis. An important property of ten-\nsors with such decompositions is that they have eigenvectors corresponding to these basis\nvectors. Although the concepts of eigenvalues and eigenvectors of tensors is generally sig-\nni\ufb01cantly more complicated than their matrix counterpart\u2014both algebraically (Qi, 2005;\nCartwright and Sturmfels, 2013; Lim, 2005) and computationally (Hillar and Lim, 2013;\n2776\nTensor Decompositions for Learning Latent Variable Models\nKo\ufb01dis and Regalia, 2002)\u2014the special symmetric orthogonal structure we consider per-\nmits simple algorithms to e\ufb03ciently and stably recover the desired decomposition.\nIn\nparticular, a generalization of the matrix power method to symmetric tensors, introduced\nby Lathauwer et al. (2000, Remark 3) and analyzed by Ko\ufb01dis and Regalia (2002), provides\nsuch a decomposition. This is in fact implied by the characterization of Zhang and Golub\n(2001), which shows that iteratively obtaining the best rank-1 approximation of such orthog-\nonally decomposable tensors also yields the exact decomposition. We note that in general,\nobtaining such approximations for general (symmetric) tensors is NP-hard (Hillar and Lim,\n2013).\n1.2.2 Latent Variable Models\nThis work focuses on the particular application of tensor decomposition methods to estimat-\ning latent variable models, a signi\ufb01cant departure from many previous approaches in the ma-\nchine learning and statistics literature. By far the most popular heuristic for parameter esti-\nmation for such models is the Expectation-Maximization (EM) algorithm (Dempster et al.,\n1977; Redner and Walker, 1984). Although EM has a number of merits, it may su\ufb00er from\nslow convergence and poor quality local optima (Redner and Walker, 1984), requiring prac-\ntitioners to employ many additional heuristics to obtain good solutions. For some models\nsuch as latent trees (Roch, 2006) and topic models (Arora et al., 2012a), maximum like-\nlihood estimation is NP-hard, which suggests that other estimation approaches may be\nmore attractive. More recently, algorithms from theoretical computer science and machine\nlearning have addressed computational and sample complexity issues related to estimating\ncertain latent variable models such as Gaussian mixture models and HMMs (Dasgupta,\n1999; Arora and Kannan, 2005; Dasgupta and Schulman, 2007; Vempala and Wang, 2004;\nKannan et al., 2008; Achlioptas and McSherry, 2005; Chaudhuri and Rao, 2008; Brubaker and Vempala,\n2008; Kalai et al., 2010; Belkin and Sinha, 2010; Moitra and Valiant, 2010; Hsu and Kakade,\n2013; Chang, 1996; Mossel and Roch, 2006; Hsu et al., 2012b; Anandkumar et al., 2012c;\nArora et al., 2012a; Anandkumar et al., 2012a). See the works by Anandkumar et al. (2012c)\nand Hsu and Kakade (2013) for a discussion of these methods, together with the computa-\ntional and statistical hardness barriers that they face. The present work reviews a broad\nrange of latent variables where a mild non-degeneracy condition implies the symmetric\northogonal decomposition structure in the tensors of low-order observable moments.\nNotably, another class of methods, based on subspace identi\ufb01cation (Overschee and Moor,\n1996) and observable operator models/multiplicity automata (Sch\u00a8utzenberger, 1961; Jaeger,\n2000; Littman et al., 2001), have been proposed for a number of latent variable models.\nThese methods were successfully developed for HMMs by Hsu et al. (2012b), and sub-\nsequently generalized and extended for a number of related sequential and tree Markov\nmodels models (Siddiqi et al., 2010; Bailly, 2011; Boots et al., 2010; Parikh et al., 2011;\nRodu et al., 2013; Balle et al., 2012; Balle and Mohri, 2012), as well as certain classes of\nparse tree models (Luque et al., 2012; Cohen et al., 2012; Dhillon et al., 2012). These meth-\nods use low-order moments to learn an \u201coperator\u201d representation of the distribution, which\ncan be used for density estimation and belief state updates. While \ufb01nite sample bounds can\nbe given to establish the learnability of these models (Hsu et al., 2012b), the algorithms do\n2777\nAnandkumar, Ge, Hsu, Kakade, and Telgarsky\nnot actually give parameter estimates (e.g., of the emission or transition matrices in the\ncase of HMMs).\n1.3 Organization\nThe rest of the paper is organized as follows. Section 2 reviews some basic de\ufb01nitions of\ntensors. Section 3 provides examples of a number of latent variable models which, after\nappropriate manipulations of their low order moments, share a certain natural tensor struc-\nture. Section 4 reduces the problem of parameter estimation to that of extracting a certain\n(symmetric orthogonal) decomposition of a tensor. We then provide a detailed analysis of\na robust tensor power method and establish an analogue of Wedin\u2019s perturbation theorem\nfor the singular vectors of matrices. The discussion in Section 6 addresses a number of\npractical concerns that arise when dealing with moment matrices and tensors.\n2. Preliminaries\nWe introduce some tensor notations borrowed from Lim (2005). A real p-th order tensor\nA \u2208Np\ni=1 Rni is a member of the tensor product of Euclidean spaces Rni, i \u2208[p]. We\ngenerally restrict to the case where n1 = n2 = \u00b7 \u00b7 \u00b7 = np = n, and simply write A \u2208Np Rn.\nFor a vector v \u2208Rn, we use v\u2297p := v \u2297v \u2297\u00b7 \u00b7 \u00b7 \u2297v \u2208Np Rn to denote its p-th tensor power.\nAs is the case for vectors (where p = 1) and matrices (where p = 2), we may identify a\np-th order tensor with the p-way array of real numbers [Ai1,i2,...,ip : i1, i2, . . . , ip \u2208[n]], where\nAi1,i2,...,ip is the (i1, i2, . . . , ip)-th coordinate of A (with respect to a canonical basis).\nWe can consider A to be a multilinear map in the following sense: for a set of matrices\n{Vi \u2208Rn\u00d7mi : i \u2208[p]}, the (i1, i2, . . . , ip)-th entry in the p-way array representation of\nA(V1, V2, . . . , Vp) \u2208Rm1\u00d7m2\u00d7\u00b7\u00b7\u00b7\u00d7mp is\n[A(V1, V2, . . . , Vp)]i1,i2,...,ip :=\nX\nj1,j2,...,jp\u2208[n]\nAj1,j2,...,jp [V1]j1,i1 [V2]j2,i2 \u00b7 \u00b7 \u00b7 [Vp]jp,ip.\nNote that if A is a matrix (p = 2), then\nA(V1, V2) = V \u22a4\n1 AV2.\nSimilarly, for a matrix A and vector v \u2208Rn, we can express Av as\nA(I, v) = Av \u2208Rn,\nwhere I is the n \u00d7 n identity matrix. As a \ufb01nal example of this notation, observe\nA(ei1, ei2, . . . , eip) = Ai1,i2,...,ip,\nwhere {e1, e2, . . . , en} is the canonical basis for Rn.\nMost tensors A \u2208Np Rn considered in this work will be symmetric (sometimes called\nsupersymmetric), which means that their p-way array representations are invariant to\npermutations of the array indices:\ni.e., for all indices i1, i2, . . . , ip \u2208[n], Ai1,i2,...,ip =\nAi\u03c0(1),i\u03c0(2),...,i\u03c0(p) for any permutation \u03c0 on [p]. It can be checked that this reduces to the\nusual de\ufb01nition of a symmetric matrix for p = 2.\n2778\nTensor Decompositions for Learning Latent Variable Models\nThe rank of a p-th order tensor A \u2208Np Rn is the smallest non-negative integer k such\nthat A = Pk\nj=1 u1,j \u2297u2,j \u2297\u00b7 \u00b7 \u00b7 \u2297up,j for some ui,j \u2208Rn, i \u2208[p], j \u2208[k], and the symmetric\nrank of a symmetric p-th order tensor A is the smallest non-negative integer k such that\nA = Pk\nj=1 u\u2297p\nj\nfor some uj \u2208Rn, j \u2208[k].1 The notion of rank readily reduces to the usual\nde\ufb01nition of matrix rank when p = 2, as revealed by the singular value decomposition.\nSimilarly, for symmetric matrices, the symmetric rank is equivalent to the matrix rank as\ngiven by the spectral theorem. A decomposition into such rank-one terms is known as a\ncanonical polyadic decomposition (Hitchcock, 1927a,b).\nThe notion of tensor (symmetric) rank is considerably more delicate than matrix (sym-\nmetric) rank. For instance, it is not clear a priori that the symmetric rank of a tensor should\neven be \ufb01nite (Comon et al., 2008). In addition, removal of the best rank-1 approximation\nof a (general) tensor may increase the tensor rank of the residual (Stegeman and Comon,\n2010).\nThroughout, we use \u2225v\u2225= (P\ni v2\ni )1/2 to denote the Euclidean norm of a vector v, and\n\u2225M\u2225to denote the spectral (operator) norm of a matrix. We also use \u2225T\u2225to denote the\noperator norm of a tensor, which we de\ufb01ne later.\n3. Tensor Structure in Latent Variable Models\nIn this section, we give several examples of latent variable models whose low-order moments\ncan be written as symmetric tensors of low symmetric rank; some of these examples can be\ndeduced using the techniques developed in the text by McCullagh (1987). The basic form\nis demonstrated in Theorem 3.1 for the \ufb01rst example, and the general pattern will emerge\nfrom subsequent examples.\n3.1 Exchangeable Single Topic Models\nWe \ufb01rst consider a simple bag-of-words model for documents in which the words in the\ndocument are assumed to be exchangeable. Recall that a collection of random variables\nx1, x2, . . . , x\u2113are exchangeable if their joint probability distribution is invariant to permu-\ntation of the indices. The well-known De Finetti\u2019s theorem (Austin, 2008) implies that such\nexchangeable models can be viewed as mixture models in which there is a latent variable h\nsuch that x1, x2, . . . , x\u2113are conditionally i.i.d. given h (see Figure 1(a) for the corresponding\ngraphical model) and the conditional distributions are identical at all the nodes.\nIn our simpli\ufb01ed topic model for documents, the latent variable h is interpreted as\nthe (sole) topic of a given document, and it is assumed to take only a \ufb01nite number of\ndistinct values. Let k be the number of distinct topics in the corpus, d be the number of\ndistinct words in the vocabulary, and \u2113\u22653 be the number of words in each document. The\ngenerative process for a document is as follows: the document\u2019s topic is drawn according to\nthe discrete distribution speci\ufb01ed by the probability vector w := (w1, w2, . . . , wk) \u2208\u2206k\u22121.\nThis is modeled as a discrete random variable h such that\nPr[h = j] = wj,\nj \u2208[k].\n1. For even p, the de\ufb01nition is slightly di\ufb00erent (Comon et al., 2008).\n2779\nAnandkumar, Ge, Hsu, Kakade, and Telgarsky\nGiven the topic h, the document\u2019s \u2113words are drawn independently according to the dis-\ncrete distribution speci\ufb01ed by the probability vector \u00b5h \u2208\u2206d\u22121. It will be convenient to\nrepresent the \u2113words in the document by d-dimensional random vectors x1, x2, . . . , x\u2113\u2208Rd.\nSpeci\ufb01cally, we set\nxt = ei\nif and only if\nthe t-th word in the document is i,\nt \u2208[\u2113],\nwhere e1, e2, . . . ed is the standard coordinate basis for Rd.\nOne advantage of this encoding of words is that the (cross) moments of these random\nvectors correspond to joint probabilities over words. For instance, observe that\nE[x1 \u2297x2] =\nX\n1\u2264i,j\u2264d\nPr[x1 = ei, x2 = ej] ei \u2297ej\n=\nX\n1\u2264i,j\u2264d\nPr[1st word = i, 2nd word = j] ei \u2297ej,\nso the (i, j)-the entry of the matrix E[x1 \u2297x2] is Pr[1st word = i, 2nd word = j]. More\ngenerally, the (i1, i2, . . . , i\u2113)-th entry in the tensor E[x1 \u2297x2 \u2297\u00b7 \u00b7 \u00b7 \u2297x\u2113] is Pr[1st word =\ni1, 2nd word = i2, . . . , \u2113-th word = i\u2113]. This means that estimating cross moments, say, of\nx1 \u2297x2 \u2297x3, is the same as estimating joint probabilities of the \ufb01rst three words over all\ndocuments. (Recall that we assume that each document has at least three words.)\nThe second advantage of the vector encoding of words is that the conditional expectation\nof xt given h = j is simply \u00b5j, the vector of word probabilities for topic j:\nE[xt|h = j] =\nd\nX\ni=1\nPr[t-th word = i|h = j] ei =\nd\nX\ni=1\n[\u00b5j]i ei = \u00b5j,\nj \u2208[k]\n(where [\u00b5j]i is the i-th entry in the vector \u00b5j). Because the words are conditionally inde-\npendent given the topic, we can use this same property with conditional cross moments,\nsay, of x1 and x2:\nE[x1 \u2297x2|h = j] = E[x1|h = j] \u2297E[x2|h = j] = \u00b5j \u2297\u00b5j,\nj \u2208[k].\nThis and similar calculations lead one to the following theorem.\nTheorem 3.1 (Anandkumar et al., 2012c) If\nM2\n:=\nE[x1 \u2297x2]\nM3\n:=\nE[x1 \u2297x2 \u2297x3],\nthen\nM2\n=\nk\nX\ni=1\nwi \u00b5i \u2297\u00b5i\nM3\n=\nk\nX\ni=1\nwi \u00b5i \u2297\u00b5i \u2297\u00b5i.\n2780\nTensor Decompositions for Learning Latent Variable Models\nAs we will see in Section 4.3, the structure of M2 and M3 revealed in Theorem 3.1 implies\nthat the topic vectors \u00b51, \u00b52, . . . , \u00b5k can be estimated by computing a certain symmetric\ntensor decomposition. Moreover, due to exchangeability, all triples (resp., pairs) of words\nin a document\u2014and not just the \ufb01rst three (resp., two) words\u2014can be used in forming M3\n(resp., M2); see Section 6.1.\n3.2 Beyond Raw Moments\nIn the single topic model above, the raw (cross) moments of the observed words directly\nyield the desired symmetric tensor structure. In some other models, the raw moments do\nnot explicitly have this form. Here, we show that the desired tensor structure can be found\nthrough various manipulations of di\ufb00erent moments.\n3.2.1 Spherical Gaussian Mixtures: Common Covariance\nWe now consider a mixture of k Gaussian distributions with spherical covariances. We start\nwith the simpler case where all of the covariances are identical; this probabilistic model is\nclosely related to the (non-probabilistic) k-means clustering problem (MacQueen, 1967).\nLet wi \u2208(0, 1) be the probability of choosing component i \u2208[k], {\u00b51, \u00b52, . . . , \u00b5k} \u2282Rd\nbe the component mean vectors, and \u03c32I be the common covariance matrix. An observation\nin this model is given by\nx := \u00b5h + z,\nwhere h is the discrete random variable with Pr[h = i] = wi for i \u2208[k] (similar to the ex-\nchangeable single topic model), and z \u223cN(0, \u03c32I) is an independent multivariate Gaussian\nrandom vector in Rd with zero mean and spherical covariance \u03c32I.\nThe Gaussian mixture model di\ufb00ers from the exchangeable single topic model in the way\nobservations are generated. In the single topic model, we observe multiple draws (words in\na particular document) x1, x2, . . . , x\u2113given the same \ufb01xed h (the topic of the document). In\ncontrast, for the Gaussian mixture model, every realization of x corresponds to a di\ufb00erent\nrealization of h.\nTheorem 3.2 (Hsu and Kakade, 2013) Assume d \u2265k. The variance \u03c32 is the smallest\neigenvalue of the covariance matrix E[x \u2297x] \u2212E[x] \u2297E[x]. Furthermore, if\nM2\n:=\nE[x \u2297x] \u2212\u03c32I\nM3\n:=\nE[x \u2297x \u2297x] \u2212\u03c32\nd\nX\ni=1\n\u0000E[x] \u2297ei \u2297ei + ei \u2297E[x] \u2297ei + ei \u2297ei \u2297E[x]\n\u0001\n,\nthen\nM2\n=\nk\nX\ni=1\nwi \u00b5i \u2297\u00b5i\nM3\n=\nk\nX\ni=1\nwi \u00b5i \u2297\u00b5i \u2297\u00b5i.\n2781\nAnandkumar, Ge, Hsu, Kakade, and Telgarsky\n3.2.2 Spherical Gaussian Mixtures: Differing Covariances\nThe general case is where each component may have a di\ufb00erent spherical covariance. An\nobservation in this model is again x = \u00b5h + z, but now z \u2208Rd is a random vector whose\nconditional distribution given h = i (for some i \u2208[k]) is a multivariate Gaussian N(0, \u03c32\ni I)\nwith zero mean and spherical covariance \u03c32\ni I.\nTheorem 3.3 (Hsu and Kakade, 2013) Assume d \u2265k. The average variance \u00af\u03c32 :=\nPk\ni=1 wi\u03c32\ni is the smallest eigenvalue of the covariance matrix E[x \u2297x] \u2212E[x] \u2297E[x]. Let v\nbe any unit norm eigenvector corresponding to the eigenvalue \u00af\u03c32. If\nM1\n:=\nE[x(v\u22a4(x \u2212E[x]))2]\nM2\n:=\nE[x \u2297x] \u2212\u00af\u03c32I\nM3\n:=\nE[x \u2297x \u2297x] \u2212\nd\nX\ni=1\n\u0000M1 \u2297ei \u2297ei + ei \u2297M1 \u2297ei + ei \u2297ei \u2297M1\n\u0001\n,\nthen\nM2\n=\nk\nX\ni=1\nwi \u00b5i \u2297\u00b5i\nM3\n=\nk\nX\ni=1\nwi \u00b5i \u2297\u00b5i \u2297\u00b5i.\nAs shown by Hsu and Kakade (2013), M1 = Pk\ni=1 wi\u03c32\ni \u00b5i.\nNote that for the common\ncovariance case, where \u03c32\ni = \u03c32, we have that M1 = \u03c32E[x] (cf. Theorem 3.2).\n3.2.3 Independent Component Analysis (ICA)\nThe standard model for ICA (Comon, 1994; Cardoso and Comon, 1996; Hyv\u00a8arinen and Oja,\n2000; Comon and Jutten, 2010), in which independent signals are linearly mixed and cor-\nrupted with Gaussian noise before being observed, is speci\ufb01ed as follows. Let h \u2208Rk be\na latent random vector with independent coordinates, A \u2208Rd\u00d7k the mixing matrix, and z\nbe a multivariate Gaussian random vector. The random vectors h and z are assumed to be\nindependent. The observed random vector is\nx := Ah + z.\nLet \u00b5i denote the i-th column of the mixing matrix A.\nTheorem 3.4 (Comon and Jutten, 2010) De\ufb01ne\nM4\n:=\nE[x \u2297x \u2297x \u2297x] \u2212T\nwhere T is the fourth-order tensor with\n[T]i1,i2,i3,i4 := E[xi1xi2]E[xi3xi4] + E[xi1xi3]E[xi2xi4]\n+ E[xi1xi4]E[xi2xi3],\n1 \u2264i1, i2, i3, i4 \u2264k\n2782\nTensor Decompositions for Learning Latent Variable Models\n( i.e., T is the fourth derivative tensor of the function v 7\u21928\u22121E[(v\u22a4x)2]2, so M4 is the\nfourth cumulant tensor). Let \u03bai := E[h4\ni ] \u22123 for each i \u2208[k]. Then\nM4\n=\nk\nX\ni=1\n\u03bai \u00b5i \u2297\u00b5i \u2297\u00b5i \u2297\u00b5i.\nNote that \u03bai corresponds to the excess kurtosis, a measure of non-Gaussianity as \u03bai = 0 if\nhi is a standard normal random variable. Furthermore, note that A is not identi\ufb01able if h\nis a multivariate Gaussian.\nWe may derive forms similar to that of M2 and M3 from Theorem 3.1 using M4 by\nobserving that\nM4(I, I, u, v) =\nk\nX\ni=1\n\u03bai(\u00b5\u22a4\ni u)(\u00b5\u22a4\ni v) \u00b5i \u2297\u00b5i,\nM4(I, I, I, v) =\nk\nX\ni=1\n\u03bai(\u00b5\u22a4\ni v) \u00b5i \u2297\u00b5i \u2297\u00b5i\nfor any vectors u, v \u2208Rd.\n3.2.4 Latent Dirichlet Allocation (LDA)\nAn increasingly popular class of latent variable models are mixed membership models, where\neach datum may belong to several di\ufb00erent latent classes simultaneously. LDA is one such\nmodel for the case of document modeling; here, each document corresponds to a mixture\nover topics (as opposed to just a single topic). The distribution over such topic mixtures is a\nDirichlet distribution Dir(\u03b1) with parameter vector \u03b1 \u2208Rk\n++ with strictly positive entries;\nits density over the probability simplex \u2206k\u22121 := {v \u2208Rk : vi \u2208[0, 1]\u2200i \u2208[k], Pk\ni=1 vi = 1}\nis given by\np\u03b1(h) =\n\u0393(\u03b10)\nQk\ni=1 \u0393(\u03b1i)\nk\nY\ni=1\nh\u03b1i\u22121\ni\n,\nh \u2208\u2206k\u22121\nwhere\n\u03b10 := \u03b11 + \u03b12 + \u00b7 \u00b7 \u00b7 + \u03b1k.\nAs before, the k topics are speci\ufb01ed by probability vectors \u00b51, \u00b52, . . . , \u00b5k \u2208\u2206d\u22121. To\ngenerate a document, we \ufb01rst draw the topic mixture h = (h1, h2, . . . , hk) \u223cDir(\u03b1), and\nthen conditioned on h, we draw \u2113words x1, x2, . . . , x\u2113independently from the discrete\ndistribution speci\ufb01ed by the probability vector Pk\ni=1 hi\u00b5i (i.e., for each xt, we independently\nsample a topic j according to h and then sample xt according to \u00b5j). Again, we encode a\nword xt by setting xt = ei i\ufb00the t-th word in the document is i.\nThe parameter \u03b10 (the sum of the \u201cpseudo-counts\u201d) characterizes the concentration of\nthe distribution. As \u03b10 \u21920, the distribution degenerates to a single topic model (i.e., the\nlimiting density has, with probability 1, exactly one entry of h being 1 and the rest are 0).\nAt the other extreme, if \u03b1 = (c, c, . . . , c) for some scalar c > 0, then as \u03b10 = ck \u2192\u221e, the\ndistribution of h becomes peaked around the uniform vector (1/k, 1/k, . . . , 1/k) (further-\nmore, the distribution behaves like a product distribution). We are typically interested in\n2783\nAnandkumar, Ge, Hsu, Kakade, and Telgarsky\nh\nx1\nx2\n\u00b7 \u00b7 \u00b7\nx\u2113\n(a) Multi-view models\nh1\nh2\n\u00b7 \u00b7 \u00b7\nh\u2113\nx1\nx2\nx\u2113\n(b) Hidden Markov model\nFigure 1: Examples of latent variable models.\nthe case where \u03b10 is small (e.g., a constant independent of k), whereupon h typically has\nonly a few large entries. This corresponds to the setting where the documents are mainly\ncomprised of just a few topics.\nTheorem 3.5 (Anandkumar et al., 2012a) De\ufb01ne\nM1\n:=\nE[x1]\nM2\n:=\nE[x1 \u2297x2] \u2212\n\u03b10\n\u03b10 + 1M1 \u2297M1\nM3\n:=\nE[x1 \u2297x2 \u2297x3]\n\u2212\n\u03b10\n\u03b10 + 2\n\u0010\nE[x1 \u2297x2 \u2297M1] + E[x1 \u2297M1 \u2297x2] + E[M1 \u2297x1 \u2297x2]\n\u0011\n+\n2\u03b12\n0\n(\u03b10 + 2)(\u03b10 + 1)M1 \u2297M1 \u2297M1.\nThen\nM2\n=\nk\nX\ni=1\n\u03b1i\n(\u03b10 + 1)\u03b10\n\u00b5i \u2297\u00b5i\nM3\n=\nk\nX\ni=1\n2\u03b1i\n(\u03b10 + 2)(\u03b10 + 1)\u03b10\n\u00b5i \u2297\u00b5i \u2297\u00b5i.\nNote that \u03b10 needs to be known to form M2 and M3 from the raw moments. This,\nhowever, is a much weaker than assuming that the entire distribution of h is known (i.e.,\nknowledge of the whole parameter vector \u03b1).\n3.3 Multi-View Models\nMulti-view models (also sometimes called na\u00a8\u0131ve Bayes models) are a special class of Bayesian\nnetworks in which observed variables x1, x2, . . . , x\u2113are conditionally independent given a\nlatent variable h.\nThis is similar to the exchangeable single topic model, but here we\ndo not require the conditional distributions of the xt, t \u2208[\u2113] to be identical. Techniques\ndeveloped for this class can be used to handle a number of widely used models including hid-\nden Markov models (Mossel and Roch, 2006; Anandkumar et al., 2012c), phylogenetic tree\nmodels (Chang, 1996; Mossel and Roch, 2006), certain tree mixtures (Anandkumar et al.,\n2012b), and certain probabilistic grammar models (Hsu et al., 2012a).\n2784\nTensor Decompositions for Learning Latent Variable Models\nAs before, we let h \u2208[k] be a discrete random variable with Pr[h = j] = wj for all j \u2208[k].\nNow consider random vectors x1 \u2208Rd1, x2 \u2208Rd2, and x3 \u2208Rd3 which are conditionally\nindependent given h, and\nE[xt|h = j] = \u00b5t,j,\nj \u2208[k], t \u2208{1, 2, 3}\nwhere the \u00b5t,j \u2208Rdt are the conditional means of the xt given h = j. Thus, we allow the\nobservations x1, x2, . . . , x\u2113to be random vectors, parameterized only by their conditional\nmeans. Importantly, these conditional distributions may be discrete, continuous, or even a\nmix of both.\nWe \ufb01rst note the form for the raw (cross) moments.\nProposition 3.1 We have that:\nE[xt \u2297xt\u2032]\n=\nk\nX\ni=1\nwi \u00b5t,i \u2297\u00b5t\u2032,i,\n{t, t\u2032} \u2282{1, 2, 3}, t \u0338= t\u2032\nE[x1 \u2297x2 \u2297x3]\n=\nk\nX\ni=1\nwi \u00b51,i \u2297\u00b52,i \u2297\u00b53,i.\nThe cross moments do not possess a symmetric tensor form when the conditional distri-\nbutions are di\ufb00erent. Nevertheless, the moments can be \u201csymmetrized\u201d via a simple linear\ntransformation of x1 and x2 (roughly speaking, this relates x1 and x2 to x3); this leads\nto an expression from which the conditional means of x3 (i.e., \u00b53,1, \u00b53,2, . . . , \u00b53,k) can be\nrecovered. For simplicity, we assume d1 = d2 = d3 = k; the general case (with dt \u2265k) is\neasily handled using low-rank singular value decompositions.\nTheorem 3.6 (Anandkumar et al., 2012a) Assume that {\u00b5v,1, \u00b5v,2, . . . , \u00b5v,k} are lin-\nearly independent for each v \u2208{1, 2, 3}. De\ufb01ne\n\u02dcx1\n:=\nE[x3 \u2297x2]E[x1 \u2297x2]\u22121x1\n\u02dcx2\n:=\nE[x3 \u2297x1]E[x2 \u2297x1]\u22121x2\nM2\n:=\nE[\u02dcx1 \u2297\u02dcx2]\nM3\n:=\nE[\u02dcx1 \u2297\u02dcx2 \u2297x3].\nThen\nM2\n=\nk\nX\ni=1\nwi \u00b53,i \u2297\u00b53,i\nM3\n=\nk\nX\ni=1\nwi \u00b53,i \u2297\u00b53,i \u2297\u00b53,i.\nWe now discuss three examples (taken mostly from Anandkumar et al., 2012c) where the\nabove observations can be applied. The \ufb01rst two concern mixtures of product distributions,\nand the last one is the time-homogeneous hidden Markov model.\n2785\nAnandkumar, Ge, Hsu, Kakade, and Telgarsky\n3.3.1 Mixtures of Axis-Aligned Gaussians and Other Product Distributions\nThe \ufb01rst example is a mixture of k product distributions in Rn under a mild incoherence as-\nsumption (Anandkumar et al., 2012c). Here, we allow each of the k component distributions\nto have a di\ufb00erent product distribution (e.g., Gaussian distribution with an axis-aligned co-\nvariance matrix), but require the matrix of component means A := [\u00b51|\u00b52| \u00b7 \u00b7 \u00b7 |\u00b5k] \u2208Rn\u00d7k\nto satisfy a certain (very mild) incoherence condition. The role of the incoherence condition\nis explained below.\nFor a mixture of product distributions, any partitioning of the dimensions [n] into three\ngroups creates three (possibly asymmetric) \u201cviews\u201d which are conditionally independent\nonce the mixture component is selected. However, recall that Theorem 3.6 requires that\nfor each view, the k conditional means be linearly independent. In general, this may not\nbe achievable; consider, for instance, the case \u00b5i = ei for each i \u2208[k]. Such cases, where\nthe component means are very aligned with the coordinate basis, are precluded by the\nincoherence condition.\nDe\ufb01ne coherence(A) := maxi\u2208[n]{e\u22a4\ni \u03a0Aei} to be the largest diagonal entry of the orthog-\nonal projector to the range of A, and assume A has rank k. The coherence lies between k/n\nand 1; it is largest when the range of A is spanned by the coordinate axes, and it is k/n when\nthe range is spanned by a subset of the Hadamard basis of cardinality k. The incoherence\ncondition requires, for some \u03b5, \u03b4 \u2208(0, 1), coherence(A) \u2264(\u03b52/6)/ ln(3k/\u03b4). Essentially, this\ncondition ensures that the non-degeneracy of the component means is not isolated in just\na few of the n dimensions. Operationally, it implies the following.\nProposition 3.2 (Anandkumar et al., 2012c) Assume A has rank k, and\ncoherence(A) \u2264\n\u03b52/6\nln(3k/\u03b4)\nfor some \u03b5, \u03b4 \u2208(0, 1). With probability at least 1\u2212\u03b4, a random partitioning of the dimensions\n[n] into three groups (for each i \u2208[n], independently pick t \u2208{1, 2, 3} uniformly at random\nand put i in group t) has the following property. For each t \u2208{1, 2, 3} and j \u2208[k], let\n\u00b5t,j be the entries of \u00b5j put into group t, and let At := [\u00b5t,1|\u00b5t,2| \u00b7 \u00b7 \u00b7 |\u00b5t,k]. Then for each\nt \u2208{1, 2, 3}, At has full column rank, and the k-th largest singular value of At is at least\np\n(1 \u2212\u03b5)/3 times that of A.\nTherefore, three asymmetric views can be created by randomly partitioning the observed\nrandom vector x into x1, x2, and x3, such that the resulting component means for each\nview satisfy the conditions of Theorem 3.6.\n3.3.2 Spherical Gaussian Mixtures, Revisited\nConsider again the case of spherical Gaussian mixtures (cf. Section 3.2). As we shall see\nin Section 4.3, the previous techniques (based on Theorem 3.2 and Theorem 3.3) lead to\nestimation procedures when the dimension of x is k or greater (and when the k component\nmeans are linearly independent). We now show that when the dimension is slightly larger,\nsay greater than 3k, a di\ufb00erent (and simpler) technique based on the multi-view structure\ncan be used to extract the relevant structure.\n2786\nTensor Decompositions for Learning Latent Variable Models\nWe again use a randomized reduction. Speci\ufb01cally, we create three views by (i) applying\na random rotation to x, and then (ii) partitioning x \u2208Rn into three views \u02dcx1, \u02dcx2, \u02dcx3 \u2208Rd\nfor d := n/3. By the rotational invariance of the multivariate Gaussian distribution, the\ndistribution of x after random rotation is still a mixture of spherical Gaussians (i.e., a\nmixture of product distributions), and thus \u02dcx1, \u02dcx2, \u02dcx3 are conditionally independent given\nh. What remains to be checked is that, for each view t \u2208{1, 2, 3}, the matrix of conditional\nmeans of \u02dcxt for each view has full column rank. This is true with probability 1 as long as\nthe matrix of conditional means A := [\u00b51|\u00b52| \u00b7 \u00b7 \u00b7 |\u00b5k] \u2208Rn\u00d7k has rank k and n \u22653k. To\nsee this, observe that a random rotation in Rn followed by a restriction to d coordinates\nis simply a random projection from Rn to Rd, and that a random projection of a linear\nsubspace of dimension k to Rd is almost surely injective as long as d \u2265k. Applying this\nobservation to the range of A implies the following.\nProposition 3.3 (Hsu and Kakade, 2013) Assume A has rank k and that n \u22653k. Let\nR \u2208Rn\u00d7n be chosen uniformly at random among all orthogonal n \u00d7 n matrices, and set\n\u02dcx := Rx \u2208Rn and \u02dcA := RA = [R\u00b51|R\u00b52| \u00b7 \u00b7 \u00b7 |R\u00b5k] \u2208Rn\u00d7k. Partition [n] into three groups\nof sizes d1, d2, d3 with dt \u2265k for each t \u2208{1, 2, 3}. Furthermore, for each t, de\ufb01ne \u02dcxt \u2208Rdt\n(respectively,\n\u02dcAt \u2208Rdt\u00d7k) to be the subvector of \u02dcx (resp., submatrix of \u02dcA) obtained by\nselecting the dt entries (resp., rows) in the t-th group. Then \u02dcx1, \u02dcx2, \u02dcx3 are conditionally\nindependent given h; E[\u02dcxt|h = j] = \u02dcAtej for each j \u2208[k] and t \u2208{1, 2, 3}; and with\nprobability 1, the matrices \u02dcA1, \u02dcA2, \u02dcA3 have full column rank.\nIt is possible to obtain a quantitative bound on the k-th largest singular value of each At\nin terms of the k-th largest singular value of A (analogous to Proposition 3.2). One avenue\nis to show that a random rotation in fact causes \u02dcA to have low coherence, after which we\ncan apply Proposition 3.2. With this approach, it is su\ufb03cient to require n = O(k log k)\n(for constant \u03b5 and \u03b4), which results in the k-th largest singular value of each At being\na constant fraction of the k-th largest singular value of A. We conjecture that, in fact,\nn \u2265c \u00b7 k for some c > 3 su\ufb03ces.\n3.3.3 Hidden Markov Models\nOur last example is the time-homogeneous HMM for sequences of vector-valued observations\nx1, x2, . . . \u2208Rd. Consider a Markov chain of discrete hidden states y1 \u2192y2 \u2192y3 \u2192\u00b7 \u00b7 \u00b7\nover k possible states [k]; given a state yt at time t, the observation xt at time t (a random\nvector taking values in Rd) is independent of all other observations and hidden states. See\nFigure 1(b).\nLet \u03c0 \u2208\u2206k\u22121 be the initial state distribution (i.e., the distribution of y1), and T \u2208Rk\u00d7k\nbe the stochastic transition matrix for the hidden state Markov chain: for all times t,\nPr[yt+1 = i|yt = j] = Ti,j,\ni, j \u2208[k].\nFinally, let O \u2208Rd\u00d7k be the matrix whose j-th column is the conditional expectation of xt\ngiven yt = j: for all times t,\nE[xt|yt = j] = Oej,\nj \u2208[k].\n2787\nAnandkumar, Ge, Hsu, Kakade, and Telgarsky\nProposition 3.4 (Anandkumar et al., 2012c) De\ufb01ne h := y2, where y2 is the second\nhidden state in the Markov chain. Then\n\u2022 x1, x2, x3 are conditionally independent given h;\n\u2022 the distribution of h is given by the vector w := T\u03c0 \u2208\u2206k\u22121;\n\u2022 for all j \u2208[k],\nE[x1|h = j] = O diag(\u03c0)T \u22a4diag(w)\u22121ej\nE[x2|h = j] = Oej\nE[x3|h = j] = OTej.\nNote the matrix of conditional means of xt has full column rank, for each t \u2208{1, 2, 3},\nprovided that: (i) O has full column rank, (ii) T is invertible, and (iii) \u03c0 and T\u03c0 have\npositive entries.\n4. Orthogonal Tensor Decompositions\nWe now show how recovering the \u00b5i\u2019s in our aforementioned problems reduces to the prob-\nlem of \ufb01nding a certain orthogonal tensor decomposition of a symmetric tensor. We start by\nreviewing the spectral decomposition of symmetric matrices, and then discuss a generaliza-\ntion to the higher-order tensor case. Finally, we show how orthogonal tensor decompositions\ncan be used for estimating the latent variable models from the previous section.\n4.1 Review: The Matrix Case\nWe \ufb01rst build intuition by reviewing the matrix setting, where the desired decomposi-\ntion is the eigendecomposition of a symmetric rank-k matrix M = V \u039bV \u22a4, where V =\n[v1|v2| \u00b7 \u00b7 \u00b7 |vk] \u2208Rn\u00d7k is the matrix with orthonormal eigenvectors as columns, and \u039b =\ndiag(\u03bb1, \u03bb2, . . . , \u03bbk) \u2208Rk\u00d7k is diagonal matrix of non-zero eigenvalues. In other words,\nM\n=\nk\nX\ni=1\n\u03bbi viv\u22a4\ni =\nk\nX\ni=1\n\u03bbi v\u22972\ni\n.\n(1)\nSuch a decomposition is guaranteed to exist for every symmetric matrix.\nRecovery of the vi\u2019s and \u03bbi\u2019s can be viewed at least two ways. First, each vi is \ufb01xed\nunder the mapping u 7\u2192Mu, up to a scaling factor \u03bbi:\nMvi =\nk\nX\nj=1\n\u03bbj(v\u22a4\nj vi)vj = \u03bbivi\nas v\u22a4\nj vi = 0 for all j \u0338= i by orthogonality. The vi\u2019s are not necessarily the only such \ufb01xed\npoints. For instance, with the multiplicity \u03bb1 = \u03bb2 = \u03bb, then any linear combination of v1\nand v2 is similarly \ufb01xed under M. However, in this case, the decomposition in (1) is not\nunique, as \u03bb1v1v\u22a4\n1 + \u03bb2v2v\u22a4\n2 is equal to \u03bb(u1u\u22a4\n1 + u2u\u22a4\n2 ) for any pair of orthonormal vectors,\n2788\nTensor Decompositions for Learning Latent Variable Models\nu1 and u2 spanning the same subspace as v1 and v2. Nevertheless, the decomposition is\nunique when \u03bb1, \u03bb2, . . . , \u03bbk are distinct, whereupon the vj\u2019s are the only directions \ufb01xed\nunder u 7\u2192Mu up to non-trivial scaling.\nThe second view of recovery is via the variational characterization of the eigenvalues.\nAssume \u03bb1 > \u03bb2 > \u00b7 \u00b7 \u00b7 > \u03bbk; the case of repeated eigenvalues again leads to similar non-\nuniqueness as discussed above. Then the Rayleigh quotient\nu 7\u2192u\u22a4Mu\nu\u22a4u\nis maximized over non-zero vectors by v1. Furthermore, for any s \u2208[k], the maximizer of\nthe Rayleigh quotient, subject to being orthogonal to v1, v2, . . . , vs\u22121, is vs. Another way\nof obtaining this second statement is to consider the de\ufb02ated Rayleigh quotient\nu 7\u2192\nu\u22a4\u0010\nM \u2212Ps\u22121\nj=1 \u03bbjvjv\u22a4\nj\n\u0011\nu\nu\u22a4u\nand observe that vs is the maximizer.\nE\ufb03cient algorithms for \ufb01nding these matrix decompositions are well studied (Golub and van Loan,\n1996, Section 8.2.3), and iterative power methods are one e\ufb00ective class of algorithms.\nWe remark that in our multilinear tensor notation, we may write the maps u 7\u2192Mu\nand u 7\u2192u\u22a4Mu/\u2225u\u22252\n2 as\nu 7\u2192Mu \u2261u 7\u2192M(I, u),\n(2)\nu 7\u2192u\u22a4Mu\nu\u22a4u\n\u2261u 7\u2192M(u, u)\nu\u22a4u\n.\n(3)\n4.2 The Tensor Case\nDecomposing general tensors is a delicate issue; tensors may not even have unique decom-\npositions. Fortunately, the orthogonal tensors that arise in the aforementioned models have\na structure which permits a unique decomposition under a mild non-degeneracy condition.\nWe focus our attention to the case p = 3, i.e., a third order tensor; the ideas extend to\ngeneral p with minor modi\ufb01cations.\nAn orthogonal decomposition of a symmetric tensor T \u2208N3 Rn is a collection of or-\nthonormal (unit) vectors {v1, v2, . . . , vk} together with corresponding positive scalars \u03bbi > 0\nsuch that\nT =\nk\nX\ni=1\n\u03bbiv\u22973\ni\n.\n(4)\nNote that since we are focusing on odd-order tensors (p = 3), we have added the require-\nment that the \u03bbi be positive. This convention can be followed without loss of generality\nsince \u2212\u03bbiv\u2297p\ni\n= \u03bbi(\u2212vi)\u2297p whenever p is odd. Also, it should be noted that orthogonal\ndecompositions do not necessarily exist for every symmetric tensor.\nIn analogy to the matrix setting, we consider two ways to view this decomposition: a\n\ufb01xed-point characterization and a variational characterization. Related characterizations\nbased on optimal rank-1 approximations are given by Zhang and Golub (2001).\n2789\nAnandkumar, Ge, Hsu, Kakade, and Telgarsky\n4.2.1 Fixed-Point Characterization\nFor a tensor T, consider the vector-valued map\nu 7\u2192T(I, u, u)\n(5)\nwhich is the third-order generalization of (2). This can be explicitly written as\nT(I, u, u) =\nd\nX\ni=1\nX\n1\u2264j,l\u2264d\nTi,j,l(e\u22a4\nj u)(e\u22a4\nl u)ei.\nObserve that (5) is not a linear map, which is a key di\ufb00erence compared to the matrix case.\nAn eigenvector u for a matrix M satis\ufb01es M(I, u) = \u03bbu, for some scalar \u03bb. We say a\nunit vector u \u2208Rn is an eigenvector of T, with corresponding eigenvalue \u03bb \u2208R, if\nT(I, u, u) = \u03bbu.\n(To simplify the discussion, we assume throughout that eigenvectors have unit norm; oth-\nerwise, for scaling reasons, we replace the above equation with T(I, u, u) = \u03bb\u2225u\u2225u.) This\nconcept was originally introduced by Lim (2005) and Qi (2005). For orthogonally decom-\nposable tensors T = Pk\ni=1 \u03bbiv\u22973\ni\n,\nT(I, u, u) =\nk\nX\ni=1\n\u03bbi(u\u22a4vi)2vi .\nBy the orthogonality of the vi, it is clear that T(I, vi, vi) = \u03bbivi for all i \u2208[k]. Therefore\neach (vi, \u03bbi) is an eigenvector/eigenvalue pair.\nThere are a number of subtle di\ufb00erences compared to the matrix case that arise as a\nresult of the non-linearity of (5). First, even with the multiplicity \u03bb1 = \u03bb2 = \u03bb, a linear\ncombination u := c1v1 + c2v2 may not be an eigenvector. In particular,\nT(I, u, u) = \u03bb1c2\n1v1 + \u03bb2c2\n2v2 = \u03bb(c2\n1v1 + c2\n2v2)\nmay not be a multiple of c1v1 + c2v2. This indicates that the issue of repeated eigenvalues\ndoes not have the same status as in the matrix case. Second, even if all the eigenvalues\nare distinct, it turns out that the vi\u2019s are not the only eigenvectors.\nFor example, set\nu := (1/\u03bb1)v1 + (1/\u03bb2)v2. Then,\nT(I, u, u) = \u03bb1(1/\u03bb1)2v1 + \u03bb2(1/\u03bb2)2v2 = u,\nso u/\u2225u\u2225is an eigenvector. More generally, for any subset S \u2286[k], the vector\nX\ni\u2208S\n1\n\u03bbi\n\u00b7 vi\nis (proportional to) an eigenvector.\n2790\nTensor Decompositions for Learning Latent Variable Models\nAs we now see, these additional eigenvectors can be viewed as spurious. We say a unit\nvector u is a robust eigenvector of T if there exists an \u01eb > 0 such that for all \u03b8 \u2208{u\u2032 \u2208Rn :\n\u2225u\u2032 \u2212u\u2225\u2264\u01eb}, repeated iteration of the map\n\u00af\u03b8 7\u2192\nT(I, \u00af\u03b8, \u00af\u03b8)\n\u2225T(I, \u00af\u03b8, \u00af\u03b8)\u2225,\n(6)\nstarting from \u03b8 converges to u. Note that the map (6) rescales the output to have unit\nEuclidean norm. Robust eigenvectors are also called attracting \ufb01xed points of (6) (see, e.g.,\nKolda and Mayo, 2011).\nThe following theorem implies that if T has an orthogonal decomposition as given in (4),\nthen the set of robust eigenvectors of T are precisely the set {v1, v2, . . . vk}, implying that\nthe orthogonal decomposition is unique. (For even order tensors, the uniqueness is true up\nto sign-\ufb02ips of the vi.)\nTheorem 4.1 Let T have an orthogonal decomposition as given in (4).\n1. The set of \u03b8 \u2208Rn which do not converge to some vi under repeated iteration of (6)\nhas measure zero.\n2. The set of robust eigenvectors of T is equal to {v1, v2, . . . , vk}.\nThe proof of Theorem 4.1 is given in Appendix A.1, and follows readily from simple or-\nthogonality considerations. Note that every vi in the orthogonal tensor decomposition is\nrobust, whereas for a symmetric matrix M, for almost all initial points, the map \u00af\u03b8 7\u2192\nM \u00af\u03b8\n\u2225M \u00af\u03b8\u2225\nconverges only to an eigenvector corresponding to the largest magnitude eigenvalue. Also,\nsince the tensor order is odd, the signs of the robust eigenvectors are \ufb01xed, as each \u2212vi is\nmapped to vi under (6).\n4.2.2 Variational Characterization\nWe now discuss a variational characterization of the orthogonal decomposition. The gener-\nalized Rayleigh quotient (Zhang and Golub, 2001) for a third-order tensor is\nu 7\u2192T(u, u, u)\n(u\u22a4u)3/2 ,\nwhich can be compared to (3). For an orthogonally decomposable tensor, the following theo-\nrem shows that a non-zero vector u \u2208Rn is an isolated local maximizer (Nocedal and Wright,\n1999) of the generalized Rayleigh quotient if and only if u = vi for some i \u2208[k].\nTheorem 4.2 Let T have an orthogonal decomposition as given in (4), and consider the\noptimization problem\nmax\nu\u2208Rn T(u, u, u) s.t. \u2225u\u2225\u22641.\n1. The stationary points are eigenvectors of T.\n2. A stationary point u is an isolated local maximizer if and only if u = vi for some\ni \u2208[k].\n2791\nAnandkumar, Ge, Hsu, Kakade, and Telgarsky\nThe proof of Theorem 4.2 is given in Appendix A.2. It is similar to local optimality anal-\nysis for ICA methods using fourth-order cumulants (e.g., Delfosse and Loubaton, 1995;\nFrieze et al., 1996).\nAgain, we see similar distinctions to the matrix case. In the matrix case, the only local\nmaximizers of the Rayleigh quotient are the eigenvectors with the largest eigenvalue (and\nthese maximizers take on the globally optimal value). For the case of orthogonal tensor\nforms, the robust eigenvectors are precisely the isolated local maximizers.\nAn important implication of the two characterizations is that, for orthogonally decom-\nposable tensors T, (i) the local maximizers of the objective function u 7\u2192T(u, u, u)/(u\u22a4u)3/2\ncorrespond precisely to the vectors vi in the decomposition, and (ii) these local maximizers\ncan be reliably identi\ufb01ed using a simple \ufb01xed-point iteration (i.e., the tensor analogue of\nthe matrix power method). Moreover, a second-derivative test based on T(I, I, u) can be\nemployed to test for local optimality and rule out other stationary points.\n4.3 Estimation via Orthogonal Tensor Decompositions\nWe now demonstrate how the moment tensors obtained for various latent variable models\nin Section 3 can be reduced to an orthogonal form. For concreteness, we take the speci\ufb01c\nform from the exchangeable single topic model (Theorem 3.1):\nM2\n=\nk\nX\ni=1\nwi \u00b5i \u2297\u00b5i,\nM3\n=\nk\nX\ni=1\nwi \u00b5i \u2297\u00b5i \u2297\u00b5i.\n(The more general case allows the weights wi in M2 to di\ufb00er in M3, but for simplicity\nwe keep them the same in the following discussion.) We now show how to reduce these\nforms to an orthogonally decomposable tensor from which the wi and \u00b5i can be recovered.\nSee Appendix D for a discussion as to how previous approaches (Mossel and Roch, 2006;\nAnandkumar et al., 2012c,a; Hsu and Kakade, 2013) achieved this decomposition through\na certain simultaneous diagonalization method.\nThroughout, we assume the following non-degeneracy condition.\nCondition 4.1 (Non-degeneracy) The vectors \u00b51, \u00b52, . . . , \u00b5k \u2208Rd are linearly indepen-\ndent, and the scalars w1, w2, . . . , wk > 0 are strictly positive.\nObserve that Condition 4.1 implies that M2 \u2ab00 is positive semide\ufb01nite and has rank k.\nThis is often a mild condition in applications.\nWhen this condition is not met, learn-\ning is conjectured to be generally hard for both computational (Mossel and Roch, 2006)\nand information-theoretic reasons (Moitra and Valiant, 2010). As discussed by Hsu et al.\n(2012b) and Hsu and Kakade (2013), when the non-degeneracy condition does not hold,\nit is often possible to combine multiple observations using tensor products to increase the\nrank of the relevant matrices. Indeed, this observation has been rigorously formulated in\nvery recent works of Bhaskara et al. (2014) and Anderson et al. (2014) using the framework\nof smoothed analysis (Spielman and Teng, 2009).\n2792\nTensor Decompositions for Learning Latent Variable Models\n4.3.1 The Reduction\nFirst, let W \u2208Rd\u00d7k be a linear transformation such that\nM2(W, W) = W \u22a4M2W = I\nwhere I is the k \u00d7 k identity matrix (i.e., W whitens M2). Since M2 \u2ab00, we may for\nconcreteness take W := UD\u22121/2, where U \u2208Rd\u00d7k is the matrix of orthonormal eigenvectors\nof M2, and D \u2208Rk\u00d7k is the diagonal matrix of positive eigenvalues of M2. Let\n\u02dc\u00b5i := \u221awi W \u22a4\u00b5i.\nObserve that\nM2(W, W) =\nk\nX\ni=1\nW \u22a4(\u221awi\u00b5i)(\u221awi\u00b5i)\u22a4W =\nk\nX\ni=1\n\u02dc\u00b5i\u02dc\u00b5\u22a4\ni\n= I,\nso the \u02dc\u00b5i \u2208Rk are orthonormal vectors.\nNow de\ufb01ne f\nM3 := M3(W, W, W) \u2208Rk\u00d7k\u00d7k, so that\nf\nM3 =\nk\nX\ni=1\nwi (W \u22a4\u00b5i)\u22973 =\nk\nX\ni=1\n1\n\u221awi\n\u02dc\u00b5\u22973\ni .\nAs the following theorem shows, the orthogonal decomposition of f\nM3 can be obtained by\nidentifying its robust eigenvectors, upon which the original parameters wi and \u00b5i can be\nrecovered. For simplicity, we only state the result in terms of robust eigenvector/eigenvalue\npairs; one may also easily state everything in variational form using Theorem 4.2.\nTheorem 4.3 Assume Condition 4.1 and take f\nM3 as de\ufb01ned above.\n1. The set of robust eigenvectors of f\nM3 is equal to {\u02dc\u00b51, \u02dc\u00b52, . . . , \u02dc\u00b5k}.\n2. The eigenvalue corresponding to the robust eigenvector \u02dc\u00b5i of f\nM3 is equal to 1/\u221awi,\nfor all i \u2208[k].\n3. If B \u2208Rd\u00d7k is the Moore-Penrose pseudoinverse of W \u22a4, and (v, \u03bb) is a robust eigen-\nvector/eigenvalue pair of f\nM3, then \u03bbBv = \u00b5i for some i \u2208[k].\nThe theorem follows by combining the above discussion with the robust eigenvector charac-\nterization of Theorem 4.1. Recall that we have taken as convention that eigenvectors have\nunit norm, so the \u00b5i are exactly determined from the robust eigenvector/eigenvalue pairs of\nf\nM3 (together with the pseudoinverse of W \u22a4); in particular, the scale of each \u00b5i is correctly\nidenti\ufb01ed (along with the corresponding wi). Relative to previous works on moment-based\nestimators for latent variable models (e.g., Anandkumar et al., 2012c,a; Hsu and Kakade,\n2013), Theorem 4.3 emphasizes the role of the special tensor structure, which in turn makes\ntransparent the applicability of methods for orthogonal tensor decomposition.\n2793\nAnandkumar, Ge, Hsu, Kakade, and Telgarsky\n4.3.2 Local Maximizers of (Cross Moment) Skewness\nThe variational characterization provides an interesting perspective on the robust eigen-\nvectors for these latent variable models. Consider the exchangeable single topic models\n(Theorem 3.1), and the objective function\nu 7\u2192E[(x\u22a4\n1 u)(x\u22a4\n2 u)(x\u22a4\n3 u)]\nE[(x\u22a4\n1 u)(x\u22a4\n2 u)]3/2\n= M3(u, u, u)\nM2(u, u)3/2 .\nIn this case, every local maximizer u\u2217satis\ufb01es M2(I, u\u2217) = \u221awi\u00b5i for some i \u2208[k]. The\nobjective function can be interpreted as the (cross moment) skewness of the random vectors\nx1, x2, x3 along direction u.\n5. Tensor Power Method\nIn this section, we consider the tensor power method of Lathauwer et al. (2000, Remark 3)\nfor orthogonal tensor decomposition. We \ufb01rst state a simple convergence analysis for an\northogonally decomposable tensor T.\nWhen only an approximation \u02c6T to an orthogonally decomposable tensor T is available\n(e.g., when empirical moments are used to estimate population moments), an orthogonal\ndecomposition need not exist for this perturbed tensor (unlike for the case of matrices),\nand a more robust approach is required to extract the approximate decomposition. Here,\nwe propose such a variant in Algorithm 1 and provide a detailed perturbation analysis. We\nnote that alternative approaches such as simultaneous diagonalization can also be employed\n(see Appendix D).\n5.1 Convergence Analysis for Orthogonally Decomposable Tensors\nThe following lemma establishes the quadratic convergence of the tensor power method\u2014\ni.e., repeated iteration of (6)\u2014for extracting a single component of the orthogonal decom-\nposition. Note that the initial vector \u03b80 determines which robust eigenvector will be the\nconvergent point. Computation of subsequent eigenvectors can be computed with de\ufb02ation,\ni.e., by subtracting appropriate terms from T.\nLemma 5.1 Let T \u2208N3 Rn have an orthogonal decomposition as given in (4). For a vector\n\u03b80 \u2208Rn, suppose that the set of numbers |\u03bb1v\u22a4\n1 \u03b80|, |\u03bb2v\u22a4\n2 \u03b80|, . . . , |\u03bbkv\u22a4\nk \u03b80| has a unique largest\nelement. Without loss of generality, say |\u03bb1v\u22a4\n1 \u03b80| is this largest value and |\u03bb2v\u22a4\n2 \u03b80| is the\nsecond largest value. For t = 1, 2, . . . , let\n\u03b8t :=\nT(I, \u03b8t\u22121, \u03b8t\u22121)\n\u2225T(I, \u03b8t\u22121, \u03b8t\u22121)\u2225.\nThen\n\u2225v1 \u2212\u03b8t\u22252 \u2264\n\u0012\n2\u03bb2\n1\nk\nX\ni=2\n\u03bb\u22122\ni\n\u0013\n\u00b7\n\f\f\f\f\n\u03bb2v\u22a4\n2 \u03b80\n\u03bb1v\u22a4\n1 \u03b80\n\f\f\f\f\n2t+1\n.\nThat is, repeated iteration of (6) starting from \u03b80 converges to v1 at a quadratic rate.\n2794\nTensor Decompositions for Learning Latent Variable Models\nTo obtain all eigenvectors, we may simply proceed iteratively using de\ufb02ation, executing\nthe power method on T \u2212P\nj \u03bbjv\u22973\nj\nafter having obtained robust eigenvector / eigenvalue\npairs {(vj, \u03bbj)}.\nProof\nLet \u03b80, \u03b81, \u03b82, . . . be the sequence given by \u03b80 := \u03b80 and \u03b8t := T(I, \u03b8t\u22121, \u03b8t\u22121) for\nt \u22651.\nLet ci := v\u22a4\ni \u03b80 for all i \u2208[k].\nIt is easy to check that (i) \u03b8t = \u03b8t/\u2225\u03b8t\u2225, and\n(ii) \u03b8t = Pk\ni=1 \u03bb2t\u22121\ni\nc2t\ni vi.\n(Indeed, \u03b8t+1 = Pk\ni=1 \u03bbi(v\u22a4\ni \u03b8t)2vi = Pk\ni=1 \u03bbi(\u03bb2t\u22121\ni\nc2t\ni )2vi =\nPk\ni=1 \u03bb2t+1\u22121\ni\nc2t+1\ni\nvi.) Then\n1 \u2212(v\u22a4\n1 \u03b8t)2 = 1 \u2212(v\u22a4\n1 \u03b8t)2\n\u2225\u03b8t\u22252\n= 1 \u2212\n\u03bb2t+1\u22122\n1\nc2t+1\n1\nPk\ni=1 \u03bb2t+1\u22122\ni\nc2t+1\ni\n\u2264\nPk\ni=2 \u03bb2t+1\u22122\ni\nc2t+1\ni\nPk\ni=1 \u03bb2t+1\u22122\ni\nc2t+1\ni\n\u2264\u03bb2\n1\nk\nX\ni=2\n\u03bb\u22122\ni\n\u00b7\n\f\f\f\f\n\u03bb2c2\n\u03bb1c1\n\f\f\f\f\n2t+1\n.\nSince \u03bb1 > 0, we have v\u22a4\n1 \u03b8t > 0 and hence \u2225v1 \u2212\u03b8t\u22252 = 2(1 \u2212v\u22a4\n1 \u03b8t) \u22642(1 \u2212(v\u22a4\n1 \u03b8t)2) as\nrequired.\n5.2 Perturbation Analysis of a Robust Tensor Power Method\nNow we consider the case where we have an approximation \u02c6T to an orthogonally decom-\nposable tensor T. Here, a more robust approach is required to extract an approximate\ndecomposition. We propose such an algorithm in Algorithm 1, and provide a detailed per-\nturbation analysis. For simplicity, we assume the tensor \u02c6T is of size k \u00d7 k \u00d7 k as per the\nreduction from Section 4.3. In some applications, it may be preferable to work directly with\na n \u00d7 n \u00d7 n tensor of rank k \u2264n (as in Lemma 5.1); our results apply in that setting with\nlittle modi\ufb01cation.\nAlgorithm 1 Robust tensor power method\ninput symmetric tensor \u02dcT \u2208Rk\u00d7k\u00d7k, number of iterations L, N.\noutput the estimated eigenvector/eigenvalue pair; the de\ufb02ated tensor.\n1: for \u03c4 = 1 to L do\n2:\nDraw \u03b8(\u03c4)\n0\nuniformly at random from the unit sphere in Rk.\n3:\nfor t = 1 to N do\n4:\nCompute power iteration update\n\u03b8(\u03c4)\nt\n:=\n\u02dcT(I, \u03b8(\u03c4)\nt\u22121, \u03b8(\u03c4)\nt\u22121)\n\u2225\u02dcT(I, \u03b8(\u03c4)\nt\u22121, \u03b8(\u03c4)\nt\u22121)\u2225\n(7)\n5:\nend for\n6: end for\n7: Let \u03c4 \u2217:= arg max\u03c4\u2208[L]{ \u02dcT(\u03b8(\u03c4)\nN , \u03b8(\u03c4)\nN , \u03b8(\u03c4)\nN )}.\n8: Do N power iteration updates (7) starting from \u03b8(\u03c4 \u2217)\nN\nto obtain \u02c6\u03b8, and set \u02c6\u03bb := \u02dcT(\u02c6\u03b8, \u02c6\u03b8, \u02c6\u03b8).\n9: return the estimated eigenvector/eigenvalue pair (\u02c6\u03b8, \u02c6\u03bb); the de\ufb02ated tensor \u02dcT \u2212\u02c6\u03bb \u02c6\u03b8\u22973.\n2795\nAnandkumar, Ge, Hsu, Kakade, and Telgarsky\nAssume that the symmetric tensor T \u2208Rk\u00d7k\u00d7k is orthogonally decomposable, and that\n\u02c6T = T + E, where the perturbation E \u2208Rk\u00d7k\u00d7k is a symmetric tensor with small operator\nnorm:\n\u2225E\u2225:= sup\n\u2225\u03b8\u2225=1\n|E(\u03b8, \u03b8, \u03b8)|.\nIn our latent variable model applications, \u02c6T is the tensor formed by using empirical mo-\nments, while T is the orthogonally decomposable tensor derived from the population mo-\nments for the given model. In the context of parameter estimation (as in Section 4.3), E\nmust account for any error ampli\ufb01cation throughout the reduction, such as in the whitening\nstep (see, e.g., Hsu and Kakade, 2013, for such an analysis).\nThe following theorem is similar to Wedin\u2019s perturbation theorem for singular vectors\nof matrices (Wedin, 1972) in that it bounds the error of the (approximate) decomposition\nreturned by Algorithm 1 on input \u02c6T in terms of the size of the perturbation, provided that\nthe perturbation is small enough.\nTheorem 5.1 Let \u02c6T = T + E \u2208Rk\u00d7k\u00d7k, where T is a symmetric tensor with orthogonal\ndecomposition T = Pk\ni=1 \u03bbiv\u22973\ni\nwhere each \u03bbi > 0, {v1, v2, . . . , vk} is an orthonormal basis,\nand E is a symmetric tensor with operator norm \u2225E\u2225\u2264\u01eb. De\ufb01ne \u03bbmin := min{\u03bbi : i \u2208[k]},\nand \u03bbmax := max{\u03bbi : i \u2208[k]}. There exists universal constants C1, C2, C3 > 0 such that\nthe following holds. Pick any \u03b7 \u2208(0, 1), and suppose\n\u01eb \u2264C1 \u00b7 \u03bbmin\nk\n,\nN \u2265C2 \u00b7\n\u0012\nlog(k) + log log\n\u0010\u03bbmax\n\u01eb\n\u0011\u0013\n,\nand\ns\nln(L/ log2(k/\u03b7))\nln(k)\n\u00b7\n \n1 \u2212ln(ln(L/ log2(k/\u03b7))) + C3\n4 ln(L/ log2(k/\u03b7))\n\u2212\ns\nln(8)\nln(L/ log2(k/\u03b7))\n!\n\u22651.02\n \n1 +\ns\nln(4)\nln(k)\n!\n.\n(Note that the condition on L holds with L = poly(k) log(1/\u03b7).) Suppose that Algorithm 1\nis iteratively called k times, where the input tensor is \u02c6T in the \ufb01rst call, and in each\nsubsequent call, the input tensor is the de\ufb02ated tensor returned by the previous call. Let\n(\u02c6v1, \u02c6\u03bb1), (\u02c6v2, \u02c6\u03bb2), . . . , (\u02c6vk, \u02c6\u03bbk) be the sequence of estimated eigenvector/eigenvalue pairs re-\nturned in these k calls. With probability at least 1 \u2212\u03b7, there exists a permutation \u03c0 on [k]\nsuch that\n\u2225v\u03c0(j) \u2212\u02c6vj\u2225\u22648\u01eb/\u03bb\u03c0(j),\n|\u03bb\u03c0(j) \u2212\u02c6\u03bbj| \u22645\u01eb,\n\u2200j \u2208[k],\nand\n\r\r\r\rT \u2212\nk\nX\nj=1\n\u02c6\u03bbj\u02c6v\u22973\nj\n\r\r\r\r \u226455\u01eb.\nThe proof of Theorem 5.1 is given in Appendix B.\nOne important di\ufb00erence from Wedin\u2019s theorem is that this is an algorithm dependent\nperturbation analysis, speci\ufb01c to Algorithm 1 (since the perturbed tensor need not have an\n2796\nTensor Decompositions for Learning Latent Variable Models\northogonal decomposition). Furthermore, note that Algorithm 1 uses multiple restarts to\nensure (approximate) convergence\u2014the intuition is that by restarting at multiple points,\nwe eventually start at a point in which the initial contraction towards some eigenvector\ndominates the error E in our tensor. The proof shows that we \ufb01nd such a point with high\nprobability within L = poly(k) trials. It should be noted that for large k, the required\nbound on L is very close to linear in k.\nWe note that it is also possible to design a variant of Algorithm 1 that instead uses\na stopping criterion to determine if an iterate has (almost) converged to an eigenvector.\nFor instance, if \u02dcT(\u03b8, \u03b8, \u03b8) > max{\u2225\u02dcT \u2225F /\n\u221a\n2r, \u2225\u02dcT (I, I, \u03b8)\u2225F /1.05}, where \u2225\u02dcT\u2225F is the tensor\nFrobenius norm (vectorized Euclidean norm), and r is the expected rank of the unperturbed\ntensor (r = k \u2212# of de\ufb02ation steps), then it can be shown that \u03b8 must be close to one of\nthe eigenvectors, provided that the perturbation is small enough. Using such a stopping\ncriterion can reduce the number of random restarts when a good initial point is found early\non. See Appendix C for details.\nIn general, it is possible, when run on a general symmetric tensor (e.g., \u02c6T), for the\ntensor power method to exhibit oscillatory behavior (Ko\ufb01dis and Regalia, 2002, Example\n1). This is not in con\ufb02ict with Theorem 5.1, which e\ufb00ectively bounds the amplitude of\nthese oscillations; in particular, if \u02c6T = T + E is a tensor built from empirical moments, the\nerror term E (and thus the amplitude of the oscillations) can be driven down by drawing\nmore samples. The practical value of addressing these oscillations and perhaps stabilizing\nthe algorithm is an interesting direction for future research (Kolda and Mayo, 2011).\nA \ufb01nal consideration is that for speci\ufb01c applications, it may be possible to use domain\nknowledge to choose better initialization points. For instance, in the topic modeling appli-\ncations (cf. Section 3.1), the eigenvectors are related to the topic word distributions, and\nmany documents may be primarily composed of words from just single topic. Therefore,\ngood initialization points can be derived from these single-topic documents themselves, as\nthese points would already be close to one of the eigenvectors.\n6. Discussion\nIn this section, we discuss some practical and application-oriented issues related to the\ntensor decomposition approach to learning latent variable models.\n6.1 Practical Implementation Considerations\nA number of practical concerns arise when dealing with moment matrices and tensors.\nBelow, we address two issues that are especially pertinent to topic modeling applica-\ntions (Anandkumar et al., 2012c,a) or other settings where the observations are sparse.\n6.1.1 Efficient Moment Representation for Exchangeable Models\nIn an exchangeable bag-of-words model, it is assumed that the words x1, x2, . . . , x\u2113in a\ndocument are conditionally i.i.d. given the topic h. This allows one to estimate p-th order\nmoments using just p words per document. The estimators obtained via Theorem 3.1 (single\ntopic model) and Theorem 3.5 (LDA) use only up to third-order moments, which suggests\nthat each document only needs to have three words.\n2797\nAnandkumar, Ge, Hsu, Kakade, and Telgarsky\nIn practice, one should use all of the words in a document for e\ufb03cient estimation of the\nmoments. One way to do this is to average over all\n\u0000\u2113\n3\n\u0001\n\u00b7 3! ordered triples of words in a\ndocument of length \u2113. At \ufb01rst blush, this seems computationally expensive (when \u2113is large),\nbut as it turns out, the averaging can be done implicitly, as shown by Zou et al. (2013).\nLet c \u2208Rd be the word count vector for a document of length \u2113, so ci is the number of\noccurrences of word i in the document, and Pd\ni=1 ci = \u2113. Note that c is a su\ufb03cient statistic\nfor the document. Then, the contribution of this document to the empirical third-order\nmoment tensor is given by\n1\n\u0000\u2113\n3\n\u0001 \u00b7 1\n3! \u00b7\n\u0012\nc \u2297c \u2297c + 2\nd\nX\ni=1\nci (ei \u2297ei \u2297ei)\n\u2212\nd\nX\ni=1\nd\nX\nj=1\ncicj (ei \u2297ei \u2297ej) \u2212\nd\nX\ni=1\nd\nX\nj=1\ncicj (ei \u2297ej \u2297ei) \u2212\nd\nX\ni=1\nd\nX\nj=1\ncicj (ei \u2297ej \u2297ej)\n\u0013\n.\n(8)\nIt can be checked that this quantity is equal to\n1\n\u0000\u2113\n3\n\u0001 \u00b7 1\n3! \u00b7\nX\nordered word triple (x, y, z)\nex \u2297ey \u2297ez\nwhere the sum is over all ordered word triples in the document. A similar expression is\neasily derived for the contribution of the document to the empirical second-order moment\nmatrix:\n1\n\u0000\u2113\n2\n\u0001 \u00b7 1\n2! \u00b7\n\u0012\nc \u2297c \u2212diag(c)\n\u0013\n.\n(9)\nNote that the word count vector c is generally a sparse vector, so this representation allows\nfor e\ufb03cient multiplication by the moment matrices and tensors in time linear in the size of\nthe document corpus (i.e., the number of non-zero entries in the term-document matrix).\n6.1.2 Dimensionality Reduction\nAnother serious concern regarding the use of tensor forms of moments is the need to op-\nerate on multidimensional arrays with \u2126(d3) values (it is typically not exactly d3 due to\nsymmetry). When d is large (e.g., when it is the size of the vocabulary in natural language\napplications), even storing a third-order tensor in memory can be prohibitive. Sparsity is\none factor that alleviates this problem. Another approach is to use e\ufb03cient linear dimen-\nsionality reduction. When this is combined with e\ufb03cient techniques for matrix and tensor\nmultiplication that avoid explicitly constructing the moment matrices and tensors (such as\nthe procedure described above), it is possible to avoid any computational scaling more than\nlinear in the dimension d and the training sample size.\nConsider for concreteness the tensor decomposition approach for the exchangeable single\ntopic model as discussed in Section 4.3.\nUsing recent techniques for randomized linear\nalgebra computations (e.g., Halko et al., 2011), it is possible to e\ufb03ciently approximate the\nwhitening matrix W \u2208Rd\u00d7k from the second-moment matrix M2 \u2208Rd\u00d7d. To do this, one\n\ufb01rst multiplies M2 by a random matrix R \u2208Rd\u00d7k\u2032 for some k\u2032 \u2265k, and then computes the\ntop k singular vectors of the product M2R. This provides a basis U \u2208Rd\u00d7k whose span\n2798\nTensor Decompositions for Learning Latent Variable Models\nis approximately the range of M2. From here, an approximate SVD of U \u22a4M2U is used to\ncompute the approximate whitening matrix W. Note that both matrix products M2R and\nU \u22a4M2U may be performed via implicit access to M2 by exploiting (9), so that M2 need\nnot be explicitly formed. With the whitening matrix W in hand, the third-moment tensor\nf\nM3 = M3(W, W, W) \u2208Rk\u00d7k\u00d7k can be implicitly computed via (8). For instance, the core\ncomputation in the tensor power method \u03b8\u2032 := f\nM3(I, \u03b8, \u03b8) is performed by (i) computing\n\u03b7 := W\u03b8, (ii) computing \u03b7\u2032 := M3(I, \u03b7, \u03b7), and \ufb01nally (iii) computing \u03b8\u2032 := W \u22a4\u03b7\u2032. Using the\nfact that M3 is an empirical third-order moment tensor, these steps can be computed with\nO(dk + N) operations, where N is the number of non-zero entries in the term-document\nmatrix (Zou et al., 2013).\n6.2 Computational Complexity\nIt is interesting to consider the computational complexity of the tensor power method in the\ndense setting where T \u2208Rk\u00d7k\u00d7k is orthogonally decomposable but otherwise unstructured.\nEach iteration requires O(k3) operations, and assuming at most k1+\u03b4 random restarts for\nextracting each eigenvector (for some small \u03b4 > 0) and O(log(k)+log log(1/\u01eb)) iterations per\nrestart, the total running time is O(k5+\u03b4(log(k)+log log(1/\u01eb))) to extract all k eigenvectors\nand eigenvalues.\nAn alternative approach to extracting the orthogonal decomposition of T is to reorganize\nT into a matrix M \u2208Rk\u00d7k2 by \ufb02attening two of the dimensions into one. In this case, if\nT = Pk\ni=1 \u03bbiv\u22973\ni\n, then M = Pk\ni=1 \u03bbivi \u2297vec(vi \u2297vi).\nThis reveals the singular value\ndecomposition of M (assuming the eigenvalues \u03bb1, \u03bb2, . . . , \u03bbk are distinct), and therefore can\nbe computed with O(k4) operations. Therefore it seems that the tensor power method is\nless e\ufb03cient than a pure matrix-based approach via singular value decomposition. However,\nit should be noted that this matrix-based approach fails to recover the decomposition when\neigenvalues are repeated, and can be unstable when the gap between eigenvalues is small\u2014\nsee Appendix D for more discussion.\nIt is worth noting that the running times di\ufb00er by roughly a factor of \u0398(k1+\u03b4), which\ncan be accounted for by the random restarts. This gap can potentially be alleviated or\nremoved by using a more clever method for initialization. Moreover, using special structure\nin the problem (as discussed above) can also improve the running time of the tensor power\nmethod.\n6.3 Sample Complexity Bounds\nPrevious work on using linear algebraic methods for estimating latent variable models cru-\ncially rely on matrix perturbation analysis for deriving sample complexity bounds (Mossel and Roch,\n2006; Hsu et al., 2012b; Anandkumar et al., 2012c,a; Hsu and Kakade, 2013). The learning\nalgorithms in these works are plug-in estimators that use empirical moments in place of\nthe population moments, and then follow algebraic manipulations that result in the desired\nparameter estimates. As long as these manipulations can tolerate small perturbations of the\npopulation moments, a sample complexity bound can be obtained by exploiting the conver-\ngence of the empirical moments to the population moments via the law of large numbers.\nAs discussed in Appendix D, these approaches do not directly lead to practical algorithms\n2799\nAnandkumar, Ge, Hsu, Kakade, and Telgarsky\ndue to a certain ampli\ufb01cation of the error (a polynomial factor of k, which is observed in\npractice).\nUsing the perturbation analysis for the tensor power method, improved sample com-\nplexity bounds can be obtained for all of the examples discussed in Section 3. The un-\nderlying analysis remains the same as in previous works (e.g., Anandkumar et al., 2012a;\nHsu and Kakade, 2013), the main di\ufb00erence being the accuracy of the orthogonal tensor\ndecomposition obtained via the tensor power method.\nRelative to the previously cited\nworks, the sample complexity bound will be considerably improved in its dependence on\nthe rank parameter k, as Theorem 5.1 implies that the tensor estimation error (e.g., error\nin estimating f\nM3 from Section 4.3) is not ampli\ufb01ed by any factor explicitly depending on k\n(there is a requirement that the error be smaller than some factor depending on k, but this\nonly contributes to a lower-order term in the sample complexity bound). See Appendix D\nfor further discussion regarding the stability of the techniques from these previous works.\n6.4 Other Perspectives\nThe tensor power method is simply one approach for extracting the orthogonal decomposi-\ntion needed in parameter estimation. The characterizations from Section 4.2 suggest that a\nnumber of \ufb01xed point and variational techniques may be possible (and Appendix D provides\nyet another perspective based on simultaneous diagonalization). One important consider-\nation is that the model is often misspeci\ufb01ed, and therefore approaches with more robust\nguarantees (e.g., for convergence) are desirable. Our own experience with the tensor power\nmethod (as applied to exchangeable topic modeling) is that while model misspeci\ufb01cation\ndoes indeed a\ufb00ect convergence, the results can be very reasonable even after just a dozen\nor so iterations (Anandkumar et al., 2012a). Nevertheless, robustness is likely more impor-\ntant in other applications, and thus the stabilization approaches (Ko\ufb01dis and Regalia, 2002;\nRegalia and Ko\ufb01dis, 2003; Erdogan, 2009; Kolda and Mayo, 2011) may be advantageous.\nAcknowledgments\nWe thank Boaz Barak, Dean Foster, Jon Kelner, and Greg Valiant for helpful discussions.\nWe are also grateful to Hanzhang Hu, Drew Bagnell, and Martial Hebert for alerting us\nof an issue with Theorem 4.2 and suggesting a simple \ufb01x. This work was completed while\nDH was a postdoctoral researcher at Microsoft Research New England, and partly while\nAA, RG, and MT were visiting the same lab. AA is supported in part by the NSF Award\nCCF-1219234, AFOSR Award FA9550-10-1-0310 and the ARO Award W911NF-12-1-0404.\nAppendix A. Fixed-Point and Variational Characterizations of\nOrthogonal Tensor Decompositions\nWe give detailed proofs of Theorems 4.1 and 4.2 in this section for completeness.\nA.1 Proof of Theorem 4.1\nTheorem A.1 Let T have an orthogonal decomposition as given in (4).\n2800\nTensor Decompositions for Learning Latent Variable Models\n1. The set of \u03b8 \u2208Rn which do not converge to some vi under repeated iteration of (6)\nhas measure zero.\n2. The set of robust eigenvectors of T is {v1, v2, . . . , vk}.\nProof For a random choice of \u03b8 \u2208Rn (under any distribution absolutely continuous with\nrespect to Lebesgue measure), the values |\u03bb1v\u22a4\n1 \u03b8|, |\u03bb2v\u22a4\n2 \u03b8|, . . . , |\u03bbkv\u22a4\nk \u03b8| will be distinct with\nprobability 1. Therefore, there exists a unique largest value, say |\u03bbiv\u22a4\ni \u03b8| for some i \u2208[k],\nand by Lemma 5.1, we have convergence to vi under repeated iteration of (6). Thus the\n\ufb01rst claim holds.\nWe now prove the second claim. First, we show that every vi is a robust eigenvector.\nPick any i \u2208[k], and note that for a su\ufb03ciently small ball around vi, we have that for all \u03b8\nin this ball, \u03bbiv\u22a4\ni \u03b8 is strictly greater than \u03bbjv\u22a4\nj \u03b8 for j \u2208[k] \\ {i}. Thus by Lemma 5.1, vi is\na robust eigenvector. Now we show that the vi are the only robust eigenvectors. Suppose\nthere exists some robust eigenvector u not equal to vi for any i \u2208[k]. Then there exists a\npositive measure set around u such that all points in this set converge to u under repeated\niteration of (6). This contradicts the \ufb01rst claim.\nA.2 Proof of Theorem 4.2\nTheorem A.2 Let T have an orthogonal decomposition as given in (4), and consider the\noptimization problem\nmax\nu\u2208Rn T(u, u, u) s.t. \u2225u\u2225\u22641.\n1. The stationary points are eigenvectors of T.\n2. A stationary point u is an isolated local maximizer if and only if u = vi for some\ni \u2208[k].\nProof Consider the Lagrangian form of the corresponding constrained maximization prob-\nlem over unit vectors u \u2208Rn:\nL(u, \u03bb) := T(u, u, u) \u22123\n2\u03bb(u\u22a4u \u22121).\nSince\n\u2207uL(u, \u03bb) = \u2207u\n\u0012 k\nX\ni=1\n\u03bbi(v\u22a4\ni u)3 \u22123\n2\u03bb(u\u22a4u \u22121)\n\u0013\n= 3\n\u0010\nT(I, u, u) \u2212\u03bbu\n\u0011\n,\nthe stationary points u \u2208Rn (with \u2225u\u2225\u22641) satisfy\nT(I, u, u) = \u03bbu\nfor some \u03bb \u2208R, i.e., (u, \u03bb) is an eigenvector/eigenvalue pair of T.\nNow we characterize the isolated local maximizers. Observe that if u \u0338= 0 and T(I, u, u) =\n\u03bbu for \u03bb < 0, then T(u, u, u) < 0. Therefore u\u2032 = (1 \u2212\u03b4)u for any \u03b4 \u2208(0, 1) satis\ufb01es\nT(u\u2032, u\u2032, u\u2032) = (1 \u2212\u03b4)3T(u, u, u) > T(u, u, u). So such a u cannot be a local maximizer.\n2801\nAnandkumar, Ge, Hsu, Kakade, and Telgarsky\nMoreover, if \u2225u\u2225< 1 and T(I, u, u) = \u03bbu for \u03bb > 0, then u\u2032 = (1 + \u03b4)u for a small enough\n\u03b4 \u2208(0, 1) satis\ufb01es \u2225u\u2032\u2225\u22641 and T(u\u2032, u\u2032, u\u2032) = (1 + \u03b4)3T(u, u, u) > T(u, u, u). Therefore a\nlocal maximizer must have T(I, u, u) = \u03bbu for some \u03bb \u22650, and \u2225u\u2225= 1 whenever \u03bb > 0.\nExtend {v1, v2, . . . , vk} to an orthonormal basis {v1, v2, . . . , vn} of Rn. Now pick any\nstationary point u = Pn\ni=1 civi with \u03bb := T(u, u, u) = u\u22a4T(I, u, u). Then\n\u03bbic2\ni = \u03bbi(u\u22a4vi)2 = v\u22a4\ni T(I, u, u) = \u03bbv\u22a4\ni u = \u03bbci \u22650,\ni \u2208[k],\nand thus\n\u22072\nuL(u, \u03bb) = 6\nk\nX\ni=1\n\u03bbici viv\u22a4\ni \u22123\u03bbI = 3\u03bb\n\u0012\n2\nX\ni\u2208\u2126\nviv\u22a4\ni \u2212I\n\u0013\nwhere \u2126:= {i \u2208[k] : ci \u0338= 0}. This implies that for any unit vector w \u2208Rn,\nw\u22a4\u22072\nuL(u, \u03bb)w = 3\u03bb\n\u0012\n2\nX\ni\u2208\u2126\n(v\u22a4\ni w)2 \u22121\n\u0013\n.\nThe point u is an isolated local maximum if the above quantity is strictly negative for all\nunit vectors w orthogonal to u. We now consider three cases depending on the cardinality\nof \u2126and the sign of \u03bb.\n\u2022 Case 1: |\u2126| = 1 and \u03bb > 0. This means u = vi for some i \u2208[k] (as u = \u2212vi implies\n\u03bb = \u2212\u03bbi < 0). In this case,\nw\u22a4\u22072\nuL(u, \u03bb)w = 3\u03bbi(2(v\u22a4\ni w)2 \u22121) = \u22123\u03bbi < 0\nfor all w \u2208Rn satisfying (u\u22a4w)2 = (v\u22a4\ni w)2 = 0. Hence u is an isolated local maximizer.\n\u2022 Case 2: |\u2126| \u22652 and \u03bb > 0. Since |\u2126| \u22652, we may pick a strict non-empty subset\nS \u228a\u2126and set\nw := 1\nZ\n\u0012 1\nZS\nX\ni\u2208S\ncivi \u2212\n1\nZSc\nX\ni\u2208\u2126\\S\ncivi\n\u0013\nwhere ZS := P\ni\u2208S c2\ni , ZSc := P\ni\u2208\u2126\\S c2\ni , and Z :=\np\n1/ZS + 1/ZSc. It is easy to\ncheck that \u2225w\u22252 = P\ni\u2208\u2126(v\u22a4\ni w)2 = 1 and u\u22a4w = 0. Consider any open neighborhood\nU of u, and pick \u03b4 > 0 small enough so that \u02dcu :=\n\u221a\n1 \u2212\u03b42u + \u03b4w is contained in\nU. Set u0 :=\n\u221a\n1 \u2212\u03b42u. By Taylor\u2019s theorem, there exists \u01eb \u2208[0, \u03b4] such that, for\n2802\nTensor Decompositions for Learning Latent Variable Models\n\u00afu := u0 + \u01ebw, we have\nT(\u02dcu, \u02dcu, \u02dcu) = T(u0, u0, u0) + \u2207uT(u, u, u)\u22a4(\u02dcu \u2212u0)\n\f\f\f\nu=u0\n+ 1\n2(\u02dcu \u2212u0)\u22a4\u22072\nuT(u, u, u)(\u02dcu \u2212u0)\n\f\f\f\nu=\u00afu\n= (1 \u2212\u03b42)3/2\u03bb + \u03b4(1 \u2212\u03b42)\u03bbu\u22a4w + 1\n2\u03b42w\u22a4\u22072\nuT(u, u, u)w\n\f\f\f\nu=\u00afu\n= (1 \u2212\u03b42)3/2\u03bb + 0 + 3\u03b42\nk\nX\ni=1\n\u03bbi(v\u22a4\ni (u0 + \u01ebw))(v\u22a4\ni w)2\n= (1 \u2212\u03b42)3/2\u03bb + 3\u03b42p\n1 \u2212\u03b42\nk\nX\ni=1\n\u03bbici(v\u22a4\ni w)2 + 3\u03b42\u01eb\nk\nX\ni=1\n\u03bbi(v\u22a4\ni w)3\n= (1 \u2212\u03b42)3/2\u03bb + 3\u03b42p\n1 \u2212\u03b42\u03bb\nX\ni\u2208\u2126\n(v\u22a4\ni w)2 + 3\u03b42\u01eb\nk\nX\ni=1\n\u03bbi(v\u22a4\ni w)3\n= (1 \u2212\u03b42)3/2\u03bb + 3\u03b42p\n1 \u2212\u03b42\u03bb + 3\u03b42\u01eb\nk\nX\ni=1\n\u03bbi(v\u22a4\ni w)3\n=\n\u0012\n1 \u22123\n2\u03b42 + O(\u03b44)\n\u0013\n\u03bb + 3\u03b42p\n1 \u2212\u03b42\u03bb + 3\u03b42\u01eb\nk\nX\ni=1\n\u03bbi(v\u22a4\ni w)3.\nSince \u01eb \u2264\u03b4, for small enough \u03b4, the RHS is strictly greater than \u03bb. This implies that\nu is not an isolated local maximizer.\n\u2022 Case 3: |\u2126| = 0 or \u03bb = 0. Note that if |\u2126| = 0, then \u03bb = 0, so we just consider \u03bb = 0.\nConsider any open neighborhood U of u, and pick j \u2208[n] and \u03b4 > 0 small enough so\nthat \u02dcu :=\n\u221a\n1 \u2212\u03b42u + \u03b4vj is contained in U. Then\nT(\u02dcu, \u02dcu, \u02dcu) = (1 \u2212\u03b42)3/2T(u, u, u) + 3\u03bbj(1 \u2212\u03b42)\u03b4c2\nj + 3\u03bbi\np\n1 \u2212\u03b42\u03b42cj + \u03b43 > 0 = \u03bb\nfor su\ufb03ciently small \u03b4. Thus u is not an isolated local maximizer.\nFrom these exhaustive cases, we conclude that a stationary point u is an isolated local\nmaximizer if and only if u = vi for some i \u2208[k].\nWe are grateful to Hanzhang Hu, Drew Bagnell, and Martial Hebert for alerting us of\nan issue with our original statement of Theorem 4.2 and its proof, and for suggesting a\nsimple \ufb01x. The original statement used the optimization constraint \u2225u\u2225= 1 (rather than\n\u2225u\u2225\u22641), but the characterization of the decomposition with this constraint is then only\ngiven by isolated local maximizers u with the additional constraint that T(u, u, u) > 0\u2014that\nis, there can be isolated local maximizers with T(u, u, u) \u22640 that are not vectors in the\ndecomposition. The suggested \ufb01x of Hu, Bagnell, and Herbert is to relax to \u2225u\u2225\u22641, which\neliminates isolated local maximizers with T(u, u, u) \u22640; this way, the characterization of\nthe decomposition is simply the isolated local maximizers under the relaxed constraint.\n2803\nAnandkumar, Ge, Hsu, Kakade, and Telgarsky\nAppendix B. Analysis of Robust Power Method\nIn this section, we prove Theorem 5.1. The proof is structured as follows. In Appendix B.1,\nwe show that with high probability, at least one out of L random vectors will be a good\ninitializer for the tensor power iterations. An initializer is good if its projection onto an\neigenvector is noticeably larger than its projection onto other eigenvectors. We then analyze\nin Appendix B.2 the convergence behavior of the tensor power iterations. Relative to the\nproof of Lemma 5.1, this analysis is complicated by the tensor perturbation. We show that\nthere is an initial slow convergence phase (linear rate rather than quadratic), but as soon\nas the projection of the iterate onto an eigenvector is large enough, it enters the quadratic\nconvergence regime until the perturbation dominates. Finally, we show how errors accrue\ndue to de\ufb02ation in Appendix B.3, which is rather subtle and di\ufb00erent from de\ufb02ation with\nmatrix eigendecompositions.\nThis is because when some initial set of eigenvectors and\neigenvalues are accurately recovered, the additional errors due to de\ufb02ation are e\ufb00ectively\nonly lower-order terms. These three pieces are assembled in Appendix B.4 to complete the\nproof of Theorem 5.1.\nB.1 Initialization\nConsider a set of non-negative numbers \u02dc\u03bb1, \u02dc\u03bb2, . . . , \u02dc\u03bbk \u22650. For \u03b3 \u2208(0, 1), we say a unit\nvector \u03b80 \u2208Rk is \u03b3-separated relative to i\u2217\u2208[k] if\n\u02dc\u03bbi\u2217|\u03b8i\u2217,0| \u2212\nmax\ni\u2208[k]\\{i\u2217}\n\u02dc\u03bbi|\u03b8i,0| \u2265\u03b3\u02dc\u03bbi|\u03b8i\u2217,0|\n(the dependence on \u02dc\u03bb1, \u02dc\u03bb2, . . . , \u02dc\u03bbk is implicit).\nThe following lemma shows that for any constant \u03b3, with probability at least 1 \u2212\u03b7,\nat least one out of poly(k) log(1/\u03b7) i.i.d. random vectors (uniformly distributed over the\nunit sphere Sk\u22121) is \u03b3-separated relative to arg maxi\u2208[k] \u02dc\u03bbi. (For small enough \u03b3 and large\nenough k, the polynomial is close to linear in k.)\nLemma B.1 There exists an absolute constant c > 0 such that if positive integer L \u22652\nsatis\ufb01es\ns\nln(L)\nln(k) \u00b7\n \n1 \u2212ln(ln(L)) + c\n4 ln(L)\n\u2212\ns\nln(8)\nln(L)\n!\n\u2265\n1\n1 \u2212\u03b3 \u00b7\n \n1 +\ns\nln(4)\nln(k)\n!\n,\n(10)\nthe following holds. With probability at least 1/2 over the choice of L i.i.d. random vectors\ndrawn uniformly distributed over the unit sphere Sk\u22121 in Rk, at least one of the vectors is\n\u03b3-separated relative to arg maxi\u2208[k] \u02dc\u03bbi. Moreover, with the same c, L, and for any \u03b7 \u2208(0, 1),\nwith probability at least 1 \u2212\u03b7 over L \u00b7 log2(1/\u03b7) i.i.d. uniform random unit vectors, at least\none of the vectors is \u03b3-separated.\nProof Without loss of generality, assume arg maxi\u2208[k] \u02dc\u03bbi = 1. Consider a random matrix\nZ \u2208Rk\u00d7L whose entries are independent N(0, 1) random variables; we take the j-th column\nof Z to be comprised of the random variables used for the j-th random vector (before\nnormalization). Speci\ufb01cally, for the j-th random vector,\n\u03b8i,0 :=\nZi,j\nqPk\ni\u2032=1 Z2\ni\u2032,j\n,\ni \u2208[n].\n2804\nTensor Decompositions for Learning Latent Variable Models\nIt su\ufb03ces to show that with probability at least 1/2, there is a column j\u2217\u2208[L] such that\n|Z1,j\u2217| \u2265\n1\n1 \u2212\u03b3\nmax\ni\u2208[k]\\{1} |Zi,j\u2217|.\nSince maxj\u2208[L] |Z1,j| is a 1-Lipschitz function of L independent N(0, 1) random variables,\nit follows that\nPr\n\u0014\f\f\fmax\nj\u2208[L] |Z1,j| \u2212median\nh\nmax\nj\u2208[L] |Z1,j|\ni\f\f\f >\np\n2 ln(8)\n\u0015\n\u22641/4.\nMoreover,\nmedian\nh\nmax\nj\u2208[L] |Z1,j|\ni\n\u2265median\nh\nmax\nj\u2208[L] Z1,j\ni\n=: m.\nObserve that the cumulative distribution function of maxj\u2208[L] Z1,j is given by F(z) = \u03a6(z)L,\nwhere \u03a6 is the standard Gaussian CDF. Since F(m) = 1/2, it follows that m = \u03a6\u22121(2\u22121/L).\nIt can be checked that\n\u03a6\u22121(2\u22121/L) \u2265\np\n2 ln(L) \u2212ln(ln(L)) + c\n2\np\n2 ln(L)\nfor some absolute constant c > 0. Also, let j\u2217:= arg maxj\u2208[L] |Z1,j|.\nNow for each j \u2208[L], let |Z2:k,j| := max{|Z2,j|, |Z3,j|, . . . , |Zk,j|}. Again, since |Z2:k,j| is\na 1-Lipschitz function of k \u22121 independent N(0, 1) random variables, it follows that\nPr\n\u0014\n|Z2:k,j| > E\nh\n|Z2:k,j|\ni\n+\np\n2 ln(4)\n\u0015\n\u22641/4.\nMoreover, by a standard argument,\nE\nh\n|Z2:k,j|\ni\n\u2264\np\n2 ln(k).\nSince |Z2:k,j| is independent of |Z1,j| for all j \u2208[L], it follows that the previous two displayed\ninequalities also hold with j replaced by j\u2217.\nTherefore we conclude with a union bound that with probability at least 1/2,\n|Z1,j\u2217| \u2265\np\n2 ln(L) \u2212ln(ln(L)) + c\n2\np\n2 ln(L)\n\u2212\np\n2 ln(8)\nand\n|Z2:k,j\u2217| \u2264\np\n2 ln(k) +\np\n2 ln(4).\nSince L satis\ufb01es (10) by assumption, in this event, the j\u2217-th random vector is \u03b3-separated.\nB.2 Tensor Power Iterations\nRecall the update rule used in the power method. Let \u03b8t = Pk\ni=1 \u03b8i,tvi \u2208Rk be the unit\nvector at time t. Then\n\u03b8t+1 =\nk\nX\ni=1\n\u03b8i,t+1vi := \u02dcT(I, \u03b8t, \u03b8t)/\u2225\u02dcT (I, \u03b8t, \u03b8t)\u2225.\n2805\nAnandkumar, Ge, Hsu, Kakade, and Telgarsky\nIn this subsection, we assume that \u02dcT has the form\n\u02dcT =\nk\nX\ni=1\n\u02dc\u03bbiv\u22973\ni\n+ \u02dcE\n(11)\nwhere {v1, v2, . . . , vk} is an orthonormal basis, and, without loss of generality,\n\u02dc\u03bb1|\u03b81,t| = max\ni\u2208[k]\n\u02dc\u03bbi|\u03b8i,t| > 0.\nAlso, de\ufb01ne\n\u02dc\u03bbmin := min{\u02dc\u03bbi : i \u2208[k], \u02dc\u03bbi > 0},\n\u02dc\u03bbmax := max{\u02dc\u03bbi : i \u2208[k]}.\nWe further assume the error \u02dcE is a symmetric tensor such that, for some constant p > 1,\n\u2225\u02dcE(I, u, u)\u2225\u2264\u02dc\u01eb,\n\u2200u \u2208Sk\u22121;\n(12)\n\u2225\u02dcE(I, u, u)\u2225\u2264\u02dc\u01eb/p,\n\u2200u \u2208Sk\u22121 s.t. (u\u22a4v1)2 \u22651 \u2212(3\u02dc\u01eb/\u02dc\u03bb1)2.\n(13)\nIn the next two propositions (Propositions B.1 and B.2) and next two lemmas (Lemmas B.2\nand B.3), we analyze the power method iterations using \u02dcT at some arbitrary iterate \u03b8t using\nonly the property (12) of \u02dcE. But throughout, the quantity \u02dc\u01eb can be replaced by \u02dc\u01eb/p if \u03b8t\nsatis\ufb01es (\u03b8\u22a4\nt v1)2 \u22651 \u2212(3\u02dc\u01eb/\u02dc\u03bb1)2 as per property (13).\nDe\ufb01ne\nR\u03c4 :=\n\u0012\n\u03b82\n1,\u03c4\n1 \u2212\u03b82\n1,\u03c4\n\u00131/2\n,\nri,\u03c4 :=\n\u02dc\u03bb1\u03b81,\u03c4\n\u02dc\u03bbi|\u03b8i,\u03c4|\n,\n\u03b3\u03c4 := 1 \u2212\n1\nmini\u0338=1 |ri,\u03c4|,\n\u03b4\u03c4 :=\n\u02dc\u01eb\n\u02dc\u03bb1\u03b82\n1,\u03c4\n,\n\u03ba :=\n\u02dc\u03bbmax\n\u02dc\u03bb1\n(14)\nfor \u03c4 \u2208{t, t + 1}.\nProposition B.1\nmin\ni\u0338=1 |ri,t| \u2265Rt\n\u03ba ,\n\u03b3t \u22651 \u2212\u03ba\nRt\n,\n\u03b82\n1,t =\nR2\nt\n1 + R2\nt\n.\nProposition B.2\nri,t+1 \u2265r2\ni,t \u00b7\n1 \u2212\u03b4t\n1 + \u03ba\u03b4tr2\ni,t\n=\n1 \u2212\u03b4t\n1\nr2\ni,t + \u03ba\u03b4t\n,\ni \u2208[k],\n(15)\nRt+1 \u2265Rt \u00b7\n1 \u2212\u03b4t\n1 \u2212\u03b3t + \u03b4tRt\n\u22651 \u2212\u03b4t\n\u03ba\nR2\nt + \u03b4t\n.\n(16)\nProof\nLet \u02c7\u03b8t+1 := \u02dcT(I, \u03b8t, \u03b8t), so \u03b8t+1 = \u02c7\u03b8t+1/\u2225\u02c7\u03b8t+1\u2225.\nSince \u02c7\u03b8i,t+1 = \u02dcT(vi, \u03b8t, \u03b8t) =\nT(vi, \u03b8t, \u03b8t) + E(vi, \u03b8t, \u03b8t), we have\n\u02c7\u03b8i,t+1 = \u02dc\u03bbi\u03b82\ni,t + E(vi, \u03b8t, \u03b8t),\ni \u2208[k].\n2806\nTensor Decompositions for Learning Latent Variable Models\nUsing the triangle inequality and the fact \u2225E(vi, \u03b8t, \u03b8t)\u2225\u2264\u02dc\u01eb, we have\n\u02c7\u03b8i,t+1 \u2265\u02dc\u03bbi\u03b82\ni,t \u2212\u02dc\u01eb \u2265|\u03b8i,t| \u00b7\n\u0010\n\u02dc\u03bbi|\u03b8i,t| \u2212\u02dc\u01eb/|\u03b8i,t|\n\u0011\n(17)\nand\n|\u02c7\u03b8i,t+1| \u2264|\u02dc\u03bbi\u03b82\ni,t| + \u02dc\u01eb \u2264|\u03b8i,t| \u00b7\n\u0010\n\u02dc\u03bbi|\u03b8i,t| + \u02dc\u01eb/|\u03b8i,t|\n\u0011\n(18)\nfor all i \u2208[k]. Combining (17) and (18) gives\nri,t+1 =\n\u02dc\u03bb1\u03b81,t+1\n\u02dc\u03bbi|\u03b8i,t+1|\n=\n\u02dc\u03bb1\u02c7\u03b81,t+1\n\u02dc\u03bbi|\u02c7\u03b8i,t+1|\n\u2265r2\ni,t \u00b7\n1 \u2212\u03b4t\n1 +\n\u02dc\u01eb\n\u02dc\u03bbi\u03b82\ni,t\n= r2\ni,t \u00b7\n1 \u2212\u03b4t\n1 + (\u02dc\u03bbi/\u02dc\u03bb1)\u03b4tr2\ni,t\n\u2265r2\ni,t \u00b7\n1 \u2212\u03b4t\n1 + \u03ba\u03b4tr2\ni,t\n.\nMoreover, by the triangle inequality and H\u00a8older\u2019s inequality,\n\u0012 n\nX\ni=2\n[\u02c7\u03b8i,t+1]2\n\u00131/2\n=\n\u0012 n\nX\ni=2\n\u0010\n\u02dc\u03bbi\u03b82\ni,t + E(vi, \u03b8t, \u03b8t)\n\u00112\u00131/2\n\u2264\n\u0012 n\nX\ni=2\n\u02dc\u03bb2\ni \u03b84\ni,t\n\u00131/2\n+\n\u0012 n\nX\ni=2\nE(vi, \u03b8t, \u03b8t)2\n\u00131/2\n\u2264max\ni\u0338=1\n\u02dc\u03bbi|\u03b8i,t|\n\u0012 n\nX\ni=2\n\u03b82\ni,t\n\u00131/2\n+ \u02dc\u01eb\n= (1 \u2212\u03b82\n1,t)1/2 \u00b7\n\u0010\nmax\ni\u0338=1\n\u02dc\u03bbi|\u03b8i,t| + \u02dc\u01eb/(1 \u2212\u03b82\n1,t)1/2\u0011\n.\n(19)\nCombining (17) and (19) gives\n|\u03b81,t+1|\n(1 \u2212\u03b82\n1,t+1)1/2 =\n|\u02c7\u03b81,t+1|\n\u0010Pn\ni=2[\u02c7\u03b8i,t+1]2\n\u00111/2 \u2265\n|\u03b81,t|\n(1 \u2212\u03b82\n1,t)1/2 \u00b7\n\u02dc\u03bb1|\u03b81,t| \u2212\u02dc\u01eb/|\u03b81,t|\nmaxi\u0338=1 \u02dc\u03bbi|\u03b8i,t| + \u02dc\u01eb/(1 \u2212\u03b82\n1,t)1/2 .\nIn terms of Rt+1, Rt, \u03b3t, and \u03b4t, this reads\nRt+1 \u2265\n1 \u2212\u03b4t\n(1 \u2212\u03b3t)\n\u00101\u2212\u03b82\n1,t\n\u03b82\n1,t\n\u00111/2\n+ \u03b4t\n= Rt \u00b7\n1 \u2212\u03b4t\n1 \u2212\u03b3t + \u03b4tRt\n=\n1 \u2212\u03b4t\n1\u2212\u03b3t\nRt + \u03b4t\n\u22651 \u2212\u03b4t\n\u03ba\nR2\nt + \u03b4t\nwhere the last inequality follows from Proposition B.1.\nLemma B.2 Fix any \u03c1 > 1. Assume\n0 \u2264\u03b4t < min\nn\n1\n2(1 + 2\u03ba\u03c12), 1 \u22121/\u03c1\n1 + \u03ba\u03c1\no\nand \u03b3t > 2(1 + 2\u03ba\u03c12)\u03b4t.\n1. If r2\ni,t \u22642\u03c12, then ri,t+1 \u2265|ri,t|\n\u00001 + \u03b3t\n2\n\u0001\n.\n2807\nAnandkumar, Ge, Hsu, Kakade, and Telgarsky\n2. If \u03c12 < r2\ni,t, then ri,t+1 \u2265min{r2\ni,t/\u03c1,\n1\u2212\u03b4t\u22121/\u03c1\n\u03ba\u03b4t\n}.\n3. \u03b3t+1 \u2265min{\u03b3t, 1 \u22121/\u03c1}.\n4. If mini\u0338=1 r2\ni,t > (\u03c1(1 \u2212\u03b4t) \u22121)/(\u03ba\u03b4t), then Rt+1 > 1\u2212\u03b4t\u22121/\u03c1\n\u03ba\u03b4t\n\u00b7\n\u02dc\u03bbmin\n\u02dc\u03bb1 \u00b7\n1\n\u221a\nk.\n5. If Rt \u22641 + 2\u03ba\u03c12, then Rt+1 \u2265Rt\n\u00001 + \u03b3t\n3\n\u0001\n, \u03b82\n1,t+1 \u2265\u03b82\n1,t, and \u03b4t+1 \u2264\u03b4t.\nProof Consider two (overlapping) cases depending on r2\ni,t.\n\u2022 Case 1: r2\ni,t \u22642\u03c12. By (15) from Proposition B.2,\nri,t+1 \u2265r2\ni,t \u00b7\n1 \u2212\u03b4t\n1 + \u03ba\u03b4tr2\ni,t\n\u2265|ri,t| \u00b7\n1\n1 \u2212\u03b3t\n\u00b7\n1 \u2212\u03b4t\n1 + 2\u03ba\u03c12\u03b4t\n\u2265|ri,t|\n\u0010\n1 + \u03b3t\n2\n\u0011\nwhere the last inequality uses the assumption \u03b3t > 2(1+2\u03ba\u03c12)\u03b4t. This proves the \ufb01rst\nclaim.\n\u2022 Case 2: \u03c12 < r2\ni,t. We split into two sub-cases. Suppose r2\ni,t \u2264(\u03c1(1 \u2212\u03b4t) \u22121)/(\u03ba\u03b4t).\nThen, by (15),\nri,t+1 \u2265r2\ni,t \u00b7\n1 \u2212\u03b4t\n1 + \u03ba\u03b4tr2\ni,t\n\u2265r2\ni,t \u00b7\n1 \u2212\u03b4t\n1 + \u03ba\u03b4t\n\u03c1(1\u2212\u03b4t)\u22121\n\u03ba\u03b4t\n=\nr2\ni,t\n\u03c1 .\nNow suppose instead r2\ni,t > (\u03c1(1 \u2212\u03b4t) \u22121)/(\u03ba\u03b4t). Then\nri,t+1 \u2265\n1 \u2212\u03b4t\n\u03ba\u03b4t\n\u03c1(1\u2212\u03b4t)\u22121 + \u03ba\u03b4t\n= 1 \u2212\u03b4t \u22121/\u03c1\n\u03ba\u03b4t\n.\n(20)\nObserve that if mini\u0338=1 r2\ni,t \u2264(\u03c1(1 \u2212\u03b4t) \u22121)/(\u03ba\u03b4t), then ri,t+1 \u2265|ri,t| for all i \u2208[k], and\nhence \u03b3t+1 \u2265\u03b3t. Otherwise we have \u03b3t+1 > 1 \u2212\n\u03ba\u03b4t\n1\u2212\u03b4t\u22121/\u03c1 > 1 \u22121/\u03c1. This proves the third\nclaim.\nIf mini\u0338=1 r2\ni,t > (\u03c1(1 \u2212\u03b4t) \u22121)/(\u03ba\u03b4t), then we may apply the inequality (20) from the\nsecond sub-case of Case 2 above to get\nRt+1 =\n1\n\u0010P\ni\u0338=1(\u02dc\u03bb1/\u02dc\u03bbi)2/r2\ni,t+1\n\u00111/2 >\n\u00121 \u2212\u03b4t \u22121/\u03c1\n\u03ba\u03b4t\n\u0013\n\u00b7\n\u02dc\u03bbmin\n\u02dc\u03bb1\n\u00b7 1\n\u221a\nk\n.\nThis proves the fourth claim.\nFinally, for the last claim, if Rt \u22641 + 2\u03ba\u03c12, then by (16) from Proposition B.2 and the\nassumption \u03b3t > 2(1 + 2\u03ba\u03c12)\u03b4t,\nRt+1 \u2265Rt \u00b7\n1 \u2212\u03b4t\n1 \u2212\u03b3t + \u03b4tRt\n\u2265Rt \u00b7\n1 \u2212\n\u03b3t\n2(1+2\u03ba\u03c12)\n1 \u2212\u03b3t/2\n\u2265Rt\n\u0012\n1 + \u03b3t \u00b7\n\u03ba\u03c12\n1 + 2\u03ba\u03c12\n\u0013\n\u2265Rt\n\u0010\n1 + \u03b3t\n3\n\u0011\n.\nThis in turn implies that \u03b82\n1,t+1 \u2265\u03b82\n1,t via Proposition B.1, and thus \u03b4t+1 \u2264\u03b4t.\n2808\nTensor Decompositions for Learning Latent Variable Models\nLemma B.3 Assume 0 \u2264\u03b4t < 1/2 and \u03b3t > 0. Pick any \u03b2 > \u03b1 > 0 such that\n\u03b1\n(1 + \u03b1)(1 + \u03b12) \u2265\n\u02dc\u01eb\n\u03b3t\u02dc\u03bb1\n,\n\u03b1\n2(1 + \u03b1)(1 + \u03b22) \u2265\u02dc\u01eb\n\u02dc\u03bb1\n.\n1. If Rt \u22651/\u03b1, then Rt+1 \u22651/\u03b1.\n2. If 1/\u03b1 > Rt \u22651/\u03b2, then Rt+1 \u2265min{R2\nt /(2\u03ba), 1/\u03b1}.\nProof Observe that for any c > 0,\nRt \u22651\nc\n\u21d4\n\u03b82\n1,t \u2265\n1\n1 + c2\n\u21d4\n\u03b4t \u2264(1 + c2)\u02dc\u01eb\n\u02dc\u03bb1\n.\n(21)\nNow consider the following cases depending on Rt.\n\u2022 Case 1: Rt \u22651/\u03b1. In this case, we have\n\u03b4t \u2264(1 + \u03b12)\u02dc\u01eb\n\u02dc\u03bb1\n\u2264\n\u03b1\u03b3t\n1 + \u03b1\nby (21) (with c = \u03b1) and the condition on \u03b1. Combining this with (16) from Propo-\nsition B.2 gives\nRt+1 \u2265\n1 \u2212\u03b4t\n1\u2212\u03b3t\nRt + \u03b4t\n\u2265\n1 \u2212\u03b1\u03b3t\n1+\u03b1\n(1 \u2212\u03b3t)\u03b1 + \u03b1\u03b3t\n1+\u03b1\n= 1\n\u03b1.\n\u2022 Case 2: 1/\u03b2 \u2264Rt < 1/\u03b1. In this case, we have\n\u03b4t \u2264(1 + \u03b22)\u02dc\u01eb\n\u02dc\u03bb1\n\u2264\n\u03b1\n2(1 + \u03b1)\nby (21) (with c = \u03b2) and the conditions on \u03b1 and \u03b2. If \u03b4t \u22651/(2 + R2\nt /\u03ba), then (16)\nimplies\nRt+1 \u22651 \u2212\u03b4t\n\u03ba\nR2\nt + \u03b4t\n\u22651 \u22122\u03b4t\n2\u03b4t\n\u2265\n1 \u2212\n\u03b1\n1+\u03b1\n\u03b1\n1+\u03b1\n= 1\n\u03b1.\nIf instead \u03b4t < 1/(2 + R2\nt /\u03ba), then (16) implies\nRt+1 \u22651 \u2212\u03b4t\n\u03ba\nR2\nt + \u03b4t\n>\n1 \u2212\n1\n2+R2\nt /\u03ba\n\u03ba\nR2\nt +\n1\n2+R2\nt /\u03ba\n= R2\nt\n2\u03ba .\n2809\nAnandkumar, Ge, Hsu, Kakade, and Telgarsky\nB.2.1 Approximate Recovery of a Single Eigenvector\nWe now state the main result regarding the approximate recovery of a single eigenvector\nusing the tensor power method on \u02dcT. Here, we exploit the special properties of the error\n\u02dcE\u2014both (12) and (13).\nLemma B.4 There exists a universal constant C > 0 such that the following holds. Let\ni\u2217:= arg maxi\u2208[k] \u02dc\u03bbi|\u03b8i,0|. If\n\u02dc\u01eb <\n\u03b30\n2(1 + 8\u03ba) \u00b7 \u02dc\u03bbmin \u00b7 \u03b82\ni\u2217,0\nand\nN \u2265C \u00b7\n\u0012log(k\u03ba)\n\u03b30\n+ log log p\u02dc\u03bbi\u2217\n\u02dc\u01eb\n\u0013\n,\nthen after t \u2265N iterations of the tensor power method on tensor \u02dcT as de\ufb01ned in (11) and\nsatisfying (12) and (13), the \ufb01nal vector \u03b8t satis\ufb01es\n\u03b8i\u2217,t \u2265\ns\n1 \u2212\n\u0012 3\u02dc\u01eb\np\u02dc\u03bbi\u2217\n\u00132\n,\n\u2225\u03b8t \u2212vi\u2217\u2225\u2264\n4\u02dc\u01eb\np\u02dc\u03bbi\u2217,\n| \u02dcT(\u03b8t, \u03b8t, \u03b8t) \u2212\u02dc\u03bbi\u2217| \u2264\n\u0012\n27\u03ba\n\u0010\n\u02dc\u01eb\np\u03bbi\u2217\n\u00112\n+ 2\n\u0013 \u02dc\u01eb\np.\nProof\nAssume without loss of generality that i\u2217= 1.\nWe consider three phases: (i)\niterations before the \ufb01rst time t such that Rt > 1 + 2\u03ba\u03c12 = 1 + 8\u03ba (using \u03c1 := 2), (ii) the\nsubsequent iterations before the \ufb01rst time t such that Rt \u22651/\u03b1 (where \u03b1 will be de\ufb01ned\nbelow), and \ufb01nally (iii) the remaining iterations.\nWe begin by analyzing the \ufb01rst phase, i.e., the iterates in T1 := {t \u22650 : Rt \u22641+2\u03ba\u03c12 =\n1 + 8\u03ba}. Observe that the condition on \u02dc\u01eb implies\n\u03b40 =\n\u02dc\u01eb\n\u02dc\u03bb1\u03b82\n1,0\n<\n\u03b30\n2(1 + 8\u03ba) \u00b7\n\u02dc\u03bbmin\n\u02dc\u03bb1\n\u2264min\n\u001a\n\u03b30\n2(1 + 2\u03ba\u03c12),\n1 \u22121/\u03c1\n2(1 + 2\u03ba\u03c12)\n\u001b\n,\nand hence the preconditions on \u03b4t and \u03b3t of Lemma B.2 hold for t = 0. For all t \u2208T1\nsatisfying the preconditions, Lemma B.2 implies that \u03b4t+1 \u2264\u03b4t and \u03b3t+1 \u2265min{\u03b3t, 1\u22121/\u03c1},\nso the next iteration also satis\ufb01es the preconditions. Hence by induction, the preconditions\nhold for all iterations in T1. Moreover, for all i \u2208[k], we have\n|ri,0| \u2265\n1\n1 \u2212\u03b30\n;\nand while t \u2208T1: (i) |ri,t| increases at a linear rate while r2\ni,t \u22642\u03c12, and (ii) |ri,t| increases\nat a quadratic rate while \u03c12 \u2264r2\ni,t \u22641\u2212\u03b4t\u22121/\u03c1\n\u03ba\u03b4t\n. (The speci\ufb01c rates are given, respectively,\nin Lemma B.2, claims 1 and 2.) Since 1\u2212\u03b4t\u22121/\u03c1\n\u03ba\u03b4t\n\u2264\n\u02dc\u03bb1\n2\u03ba\u02dc\u01eb, it follows that mini\u0338=1 r2\ni,t \u22641\u2212\u03b4t\u22121/\u03c1\n\u03ba\u03b4t\nfor at most\n2\n\u03b30\nln\n\u0012p\n2\u03c12\n1\n1\u2212\u03b30\n\u0013\n+ ln\n\u0012ln\n\u02dc\u03bb1\n2\u03ba\u02dc\u01eb\nln\n\u221a\n2\n\u0013\n= O\n\u0012 1\n\u03b30\n+ log log\n\u02dc\u03bb1\n\u02dc\u01eb\n\u0013\n(22)\niterations in T1. As soon as mini\u0338=1 r2\ni,t > 1\u2212\u03b4t\u22121/\u03c1\n\u03ba\u03b4t\n, we have that in the next iteration,\nRt+1 > 1 \u2212\u03b4t \u22121/\u03c1\n\u03ba\u03b4t\n\u00b7\n\u02dc\u03bbmin\n\u02dc\u03bb1\n\u00b7 1\n\u221a\nk\n\u2265\n7\n\u221a\nk\n;\n2810\nTensor Decompositions for Learning Latent Variable Models\nand all the while Rt is growing at a linear rate (given in Lemma B.2, claim 5). Therefore,\nthere are at most an additional\n1 + 3\n\u03b30\nln\n\u00121 + 8\u03ba\n7/\n\u221a\nk\n\u0013\n= O\n\u0012log(k\u03ba)\n\u03b30\n\u0013\n(23)\niterations in T1 over that counted in (22).\nTherefore, by combining the counts in (22)\nand (23), we have that the number of iterations in the \ufb01rst phase satis\ufb01es\n|T1| = O\n\u0012\nlog log\n\u02dc\u03bb1\n\u02dc\u01eb + log(k\u03ba)\n\u03b30\n\u0013\n.\nWe now analyze the second phase, i.e., the iterates in T2 := {t \u22650 : t /\u2208T1, Rt < 1/\u03b1}.\nDe\ufb01ne\n\u03b1 := 3\u02dc\u01eb\n\u02dc\u03bb1\n,\n\u03b2 :=\n1\n1 + 2\u03ba\u03c12 =\n1\n1 + 8\u03ba.\nNote that for the initial iteration t\u2032 := min T2, we have that Rt\u2032 \u22651+ 2\u03ba\u03c12 = 1+ 8\u03ba = 1/\u03b2,\nand by Proposition B.1, \u03b3t\u2032 \u22651 \u2212\u03ba/(1 + 8\u03ba) > 7/8. It can be checked that \u03b4t, \u03b3t, \u03b1,\nand \u03b2 satisfy the preconditions of Lemma B.3 for this initial iteration t\u2032. For all t \u2208T2\nsatisfying these preconditions, Lemma B.3 implies that Rt+1 \u2265min{Rt, 1/\u03b1}, \u03b82\n1,t+1 \u2265\nmin{\u03b82\n1,t, 1/(1+\u03b12)} (via Proposition B.1), \u03b4t+1 \u2264max{\u03b4t, (1+\u03b1)2\u02dc\u01eb/\u02dc\u03bb1} (using the de\ufb01nition\nof \u03b4t), and \u03b3t+1 \u2265min{\u03b3t, 1 \u2212\u03b1\u03ba} (via Proposition B.1). Hence the next iteration t + 1\nalso satis\ufb01es the preconditions, and by induction, so do all iterations in T2. To bound the\nnumber of iterations in T2, observe that Rt increases at a quadratic rate until Rt \u22651/\u03b1, so\n|T2| \u2264ln\n\u0012\nln(1/\u03b1)\nln((1/\u03b2)/(2\u03ba))\n\u0013\n< ln\n\u0012ln\n\u02dc\u03bb1\n3\u02dc\u01eb\nln 4\n\u0013\n= O\n\u0012\nlog log\n\u02dc\u03bb1\n\u02dc\u01eb\n\u0013\n.\n(24)\nTherefore the total number of iterations before Rt \u22651/\u03b1 is\nO\n\u0012log(k\u03ba)\n\u03b30\n+ log log\n\u02dc\u03bb1\n\u02dc\u01eb\n\u0013\n.\nAfter Rt\u2032\u2032 \u22651/\u03b1 (for t\u2032\u2032 := max(T1 \u222aT2) + 1), we have\n\u03b82\n1,t\u2032\u2032 \u2265\n1/\u03b12\n1 + 1/\u03b12 \u22651 \u2212\u03b12 \u22651 \u2212\n\u0012 3\u02dc\u01eb\n\u02dc\u03bb1\n\u00132\n.\nTherefore, the vector \u03b8t\u2032\u2032 satis\ufb01es the condition for property (13) of \u02dcE to hold. Now we\napply Lemma B.3 using \u02dc\u01eb/p in place of \u02dc\u01eb, including in the de\ufb01nition of \u03b4t (which we call \u03b4t):\n\u03b4t :=\n\u02dc\u01eb\np\u02dc\u03bb1\u03b82\n1,t\n;\nwe also replace \u03b1 and \u03b2 with \u03b1 and \u03b2, which we set to\n\u03b1 := 3\u02dc\u01eb\np\u02dc\u03bb1\n,\n\u03b2 := 3\u02dc\u01eb\n\u02dc\u03bb1\n.\n2811\nAnandkumar, Ge, Hsu, Kakade, and Telgarsky\nIt can be checked that \u03b4t\u2032\u2032 \u2208(0, 1/2), \u03b3t\u2032\u2032 \u22651 \u22123\u02dc\u01eb\u03ba/\u03bb1 > 0,\n\u03b1\n(1 + \u03b1)(1 + \u03b12) \u2265\n\u02dc\u01eb\np(1 \u22123\u02dc\u01eb\u03ba/\u03bb1)\u02dc\u03bb1\n\u2265\n\u02dc\u01eb\np\u03b3t\u2032\u2032\u02dc\u03bb1\n,\n\u03b1\n2(1 + \u03b1)(1 + \u03b2\n2)\n\u2265\n\u02dc\u01eb\np\u02dc\u03bb1\n.\nTherefore, the preconditions of Lemma B.3 are satis\ufb01ed for the initial iteration t\u2032\u2032 in this\n\ufb01nal phase, and by the same arguments as before, the preconditions hold for all subsequent\niterations t \u2265t\u2032\u2032. Initially, we have Rt\u2032\u2032 \u22651/\u03b1 \u22651/\u03b2, and by Lemma B.3, we have that Rt\nincreases at a quadratic rate in this \ufb01nal phase until Rt \u22651/\u03b1. So the number of iterations\nbefore Rt \u22651/\u03b1 can be bounded as\nln\n\u0012\nln(1/\u03b1)\nln((1/\u03b2)/(2\u03ba))\n\u0013\n= ln\n\u0012\nln p\u02dc\u03bb1\n3\u02dc\u01eb\nln\n\u0010\n\u03bb1\n3\u02dc\u01eb \u00b7 1\n2\u03ba\n\u0011\n\u0013\n\u2264ln ln p\u02dc\u03bb1\n3\u02dc\u01eb = O\n\u0012\nlog log p\u02dc\u03bb1\n\u02dc\u01eb\n\u0013\n.\nOnce Rt \u22651/\u03b1, we have\n\u03b82\n1,t \u22651 \u2212\n\u0012 3\u02dc\u01eb\np\u02dc\u03bb1\n\u00132\n.\nSince sign(\u03b81,t) = r1,t \u2265r2\n1,t\u22121 \u00b7 (1 \u2212\u03b4t\u22121)/(1 + \u03ba\u03b4t\u22121r2\n1,t\u22121) = (1 \u2212\u03b4t\u22121)/(1 + \u03ba\u03b4t\u22121) > 0 by\nProposition B.2, we have \u03b81,t > 0. Therefore we can conclude that\n\u2225\u03b8t \u2212v1\u2225=\nq\n2(1 \u2212\u03b81,t) \u2264\ns\n2\n\u0012\n1 \u2212\nq\n1 \u2212(3\u02dc\u01eb/(p\u02dc\u03bb1))2\n\u0013\n\u22644\u02dc\u01eb/(p\u02dc\u03bb1).\nFinally,\n| \u02dcT(\u03b8t, \u03b8t, \u03b8t) \u2212\u02dc\u03bb1| =\n\f\f\f\f\u02dc\u03bb1(\u03b83\n1,t \u22121) +\nk\nX\ni=2\n\u02dc\u03bbi\u03b83\ni,t + \u02dcE(\u03b8t, \u03b8t, \u03b8t)\n\f\f\f\f\n\u2264\u02dc\u03bb1|\u03b83\n1,t \u22121| +\nk\nX\ni=2\n\u02dc\u03bbi|\u03b8i,t|\u03b82\ni,t + \u2225\u02dcE(I, \u03b8t, \u03b8t)\u2225\n\u2264\u02dc\u03bb1\n\u00001 \u2212\u03b81,t + |\u03b81,t(1 \u2212\u03b82\n1,t)|\n\u0001\n+ max\ni\u0338=1\n\u02dc\u03bbi|\u03b8i,t|\nk\nX\ni=2\n\u03b82\ni,t + \u2225\u02dcE(I, \u03b8t, \u03b8t)\u2225\n\u2264\u02dc\u03bb1\n\u00001 \u2212\u03b81,t + |\u03b81,t(1 \u2212\u03b82\n1,t)|\n\u0001\n+ max\ni\u0338=1\n\u02dc\u03bbi\nq\n1 \u2212\u03b82\n1,t\nk\nX\ni=2\n\u03b82\ni,t + \u2225\u02dcE(I, \u03b8t, \u03b8t)\u2225\n= \u02dc\u03bb1\n\u00001 \u2212\u03b81,t + |\u03b81,t(1 \u2212\u03b82\n1,t)|\n\u0001\n+ max\ni\u0338=1\n\u02dc\u03bbi(1 \u2212\u03b82\n1,t)3/2 + \u2225\u02dcE(I, \u03b8t, \u03b8t)\u2225\n\u2264\u02dc\u03bb1 \u00b7 3\n\u0012 3\u02dc\u01eb\np\u02dc\u03bb1\n\u00132\n+ \u03ba\u02dc\u03bb1 \u00b7\n\u0012 3\u02dc\u01eb\np\u02dc\u03bb1\n\u00133\n+ \u02dc\u01eb\np\n\u2264(27\u03ba \u00b7 (\u02dc\u01eb/p\u02dc\u03bb1)2 + 2)\u02dc\u01eb\np\n.\n2812\nTensor Decompositions for Learning Latent Variable Models\nB.3 De\ufb02ation\nLemma B.5 Fix some \u02dc\u01eb \u22650. Let {v1, v2, . . . , vk} be an orthonormal basis for Rk, and\n\u03bb1, \u03bb2, . . . , \u03bbk \u22650 with \u03bbmin := mini\u2208[k] \u03bbi. Also, let {\u02c6v1, \u02c6v2, . . . , \u02c6vk} be a set of unit vectors\nin Rk (not necessarily orthogonal), \u02c6\u03bb1, \u02c6\u03bb2, . . . , \u02c6\u03bbk \u22650 be non-negative scalars, and de\ufb01ne\nEi := \u03bbiv\u22973\ni\n\u2212\u02c6\u03bbi\u02c6v\u22973\ni\n,\ni \u2208[k].\nPick any t \u2208[k]. If\n|\u02c6\u03bbi \u2212\u03bbi| \u2264\u02dc\u01eb,\n\u2225\u02c6vi \u2212vi\u2225\u2264min{\n\u221a\n2, 2\u02dc\u01eb/\u03bbi}\nfor all i \u2208[t], then for any unit vector u \u2208Sk\u22121,\n\r\r\r\r\nt\nX\ni=1\nEi(I, u, u)\n\r\r\r\r\n2\n2\n\u2264\n\u0012\n4(5 + 11\u02dc\u01eb/\u03bbmin)2 + 128(1 + \u02dc\u01eb/\u03bbmin)2(\u02dc\u01eb/\u03bbmin)2\n\u0013\n\u02dc\u01eb2\nt\nX\ni=1\n(u\u22a4vi)2\n+ 64(1 + \u02dc\u01eb/\u03bbmin)2\u02dc\u01eb2\nt\nX\ni=1\n(\u02dc\u01eb/\u03bbi)2 + 2048(1 + \u02dc\u01eb/\u03bbmin)2\u02dc\u01eb2\n\u0012 t\nX\ni=1\n(\u02dc\u01eb/\u03bbi)3\n\u00132\n.\nIn particular, for any \u2206\u2208(0, 1), there exists a constant \u2206\u2032 > 0 (depending only on \u2206) such\nthat \u02dc\u01eb \u2264\u2206\u2032\u03bbmin/\n\u221a\nk implies\n\r\r\r\r\nt\nX\ni=1\nEi(I, u, u)\n\r\r\r\r\n2\n2\n\u2264\n\u0012\n\u2206+ 100\nt\nX\ni=1\n(u\u22a4vi)2\n\u0013\n\u02dc\u01eb2.\nProof For any unit vector u and i \u2208[t], the error term\nEi(I, u, u) = \u03bbi(u\u22a4vi)2vi \u2212\u02c6\u03bbi(u\u22a4\u02c6vi)2\u02c6vi\nlives in span{vi, \u02c6vi}; this space is the same as span{vi, \u02c6v\u22a5\ni }, where\n\u02c6v\u22a5\ni := \u02c6vi \u2212(v\u22a4\ni \u02c6vi)vi\nis the projection of \u02c6vi onto the subspace orthogonal to vi. Since \u2225\u02c6vi \u2212vi\u22252 = 2(1 \u2212v\u22a4\ni \u02c6vi), it\nfollows that\nci := v\u22a4\ni \u02c6vi = 1 \u2212\u2225\u02c6vi \u2212vi\u22252/2 \u22650\n(the inequality follows from the assumption \u2225\u02c6vi\u2212vi\u2225\u2264\n\u221a\n2, which in turn implies 0 \u2264ci \u22641).\nBy the Pythagorean theorem and the above inequality for ci,\n\u2225\u02c6v\u22a5\ni \u22252 = 1 \u2212c2\ni \u2264\u2225\u02c6vi \u2212vi\u22252.\nLater, we will also need the following bound, which is easily derived from the above inequal-\nities and the triangle inequality:\n|1 \u2212c3\ni | = |1 \u2212ci + ci(1 \u2212c2\ni )| \u22641 \u2212ci + |ci(1 \u2212c2\ni )| \u22641.5\u2225\u02c6vi \u2212vi\u22252.\n2813\nAnandkumar, Ge, Hsu, Kakade, and Telgarsky\nWe now express Ei(I, u, u) in terms of the coordinate system de\ufb01ned by vi and \u02c6v\u22a5\ni ,\ndepicted below. De\ufb01ne\nai := u\u22a4vi\nand\nbi := u\u22a4\u0000\u02c6v\u22a5\ni /\u2225\u02c6v\u22a5\ni \u2225\n\u0001\n.\n(Note that the part of u living in span{vi, \u02c6v\u22a5\ni }\u22a5is irrelevant for analyzing Ei(I, u, u).) We\nhave\nEi(I, u, u) = \u03bbi(u\u22a4vi)2vi \u2212\u02c6\u03bbi(u\u22a4\u02c6vi)2\u02c6vi\n= \u03bbia2\ni vi \u2212\u02c6\u03bbi\n\u0000aici + \u2225\u02c6v\u22a5\ni \u2225bi\n\u00012\u0000civi + \u02c6v\u22a5\ni\n\u0001\n= \u03bbia2\ni vi \u2212\u02c6\u03bbi\n\u0000a2\ni c2\ni + 2\u2225\u02c6v\u22a5\ni \u2225aibici + \u2225\u02c6v\u22a5\ni \u22252b2\ni\n\u0001\ncivi \u2212\u02c6\u03bbi\n\u0000aici + \u2225\u02c6v\u22a5\ni \u2225bi\n\u00012\u02c6v\u22a5\ni\n=\n\u0010\n(\u03bbi \u2212\u02c6\u03bbic3\ni )a2\ni \u22122\u02c6\u03bbi\u2225\u02c6v\u22a5\ni \u2225aibic2\ni \u2212\u02c6\u03bbi\u2225\u02c6v\u22a5\ni \u22252b2\ni ci\n\u0011\n|\n{z\n}\n=:Ai\nvi \u2212\u02c6\u03bbi\u2225\u02c6v\u22a5\ni \u2225\n\u0000aici + \u2225\u02c6v\u22a5\ni \u2225bi\n\u00012\n|\n{z\n}\n=:Bi\n\u0000\u02c6v\u22a5\ni /\u2225\u02c6v\u22a5\ni \u2225\n\u0001\n= Aivi \u2212Bi\n\u0000\u02c6v\u22a5\ni /\u2225\u02c6v\u22a5\ni \u2225\n\u0001\n.\nThe overall error can also be expressed in terms of the Ai and Bi:\n\r\r\r\r\nt\nX\ni=1\nEi(I, u, u)\n\r\r\r\r\n2\n2\n=\n\r\r\r\r\nt\nX\ni=1\nAivi \u2212\nt\nX\ni=1\nBi(\u02c6v\u22a5\ni /\u2225\u02c6v\u22a5\ni \u2225)\n\r\r\r\r\n2\n2\n\u22642\n\r\r\r\r\nt\nX\ni=1\nAivi\n\r\r\r\r\n2\n+ 2\n\r\r\r\r\nt\nX\ni=1\nBi(\u02c6v\u22a5\ni /\u2225\u02c6v\u22a5\ni \u2225)\n\r\r\r\r\n2\n2\n\u22642\nt\nX\ni=1\nA2\ni + 2\n\u0012 t\nX\ni=1\n|Bi|\n\u00132\n(25)\nwhere the \ufb01rst inequality uses the fact (x + y)2 \u22642(x2 + y2) and the triangle inequality,\nand the second inequality uses the orthonormality of the vi and the triangle inequality.\nIt remains to bound A2\ni and |Bi| in terms of |ai|, \u03bbi, and \u02dc\u01eb. The \ufb01rst term, A2\ni , can be\nbounded using the triangle inequality and the various bounds on |\u03bbi \u2212\u02c6\u03bbi|, \u2225\u02c6vi \u2212vi\u2225, \u2225\u02c6v\u22a5\ni \u2225,\nand ci:\n|Ai| \u2264(|\u03bbi \u2212\u02c6\u03bbi|c3\ni + \u03bbi|c3\ni \u22121|)a2\ni + 2(\u03bbi + |\u03bbi \u2212\u02c6\u03bbi|)\u2225\u02c6v\u22a5\ni \u2225|aibi|c2\ni + (\u03bbi + |\u03bbi \u2212\u02c6\u03bbi|)\u2225\u02c6v\u22a5\ni \u22252b2\ni ci\n\u2264(|\u03bbi \u2212\u02c6\u03bbi| + 1.5\u03bbi\u2225\u02c6vi \u2212vi\u22252 + 2(\u03bbi + |\u03bbi \u2212\u02c6\u03bbi|)\u2225\u02c6vi \u2212vi\u2225)|ai| + (\u03bbi + |\u03bbi \u2212\u02c6\u03bbi|)\u2225\u02c6vi \u2212vi\u22252\n\u2264(\u02dc\u01eb + 7\u02dc\u01eb2/\u03bbi + 4\u02dc\u01eb + 4\u02dc\u01eb2/\u03bbi)|ai| + 4\u02dc\u01eb2/\u03bbi + \u02dc\u01eb3/\u03bb2\ni\n= (5 + 11\u02dc\u01eb/\u03bbi)\u02dc\u01eb|ai| + 4(1 + \u02dc\u01eb/\u03bbi)\u02dc\u01eb2/\u03bbi,\nand therefore (via (x + y)2 \u22642(x2 + y2))\nA2\ni \u22642(5 + 11\u02dc\u01eb/\u03bbi)2\u02dc\u01eb2a2\ni + 32(1 + \u02dc\u01eb/\u03bbi)2\u02dc\u01eb4/\u03bb2\ni .\nThe second term, |Bi|, is bounded similarly:\n|Bi| \u22642(\u03bbi + |\u03bbi \u2212\u02c6\u03bbi|)\u2225\u02c6v\u22a5\ni \u22252(a2\ni + \u2225\u02c6v\u22a5\ni \u22252)\n\u22642(\u03bbi + |\u03bbi \u2212\u02c6\u03bbi|)\u2225\u02c6vi \u2212vi\u22252(a2\ni + \u2225\u02c6vi \u2212vi\u22252)\n\u22648(1 + \u02dc\u01eb/\u03bbi)(\u02dc\u01eb2/\u03bbi)a2\ni + 32(1 + \u02dc\u01eb/\u03bbi)\u02dc\u01eb4/\u03bb3\ni .\n2814\nTensor Decompositions for Learning Latent Variable Models\nTherefore, using the inequality from (25) and again (x + y)2 \u22642(x2 + y2),\n\r\r\r\r\nt\nX\ni=1\nEi(I, u, u)\n\r\r\r\r\n2\n2\n\u22642\nt\nX\ni=1\nA2\ni + 2\n\u0012\nt\nX\ni=1\n|Bi|\n\u00132\n\u22644(5 + 11\u02dc\u01eb/\u03bbmin)2\u02dc\u01eb2\nt\nX\ni=1\na2\ni + 64(1 + \u02dc\u01eb/\u03bbmin)2\u02dc\u01eb2\nt\nX\ni=1\n(\u02dc\u01eb/\u03bbi)2\n+ 2\n\u0012\n8(1 + \u02dc\u01eb/\u03bbmin)(\u02dc\u01eb2/\u03bbmin)\nt\nX\ni=1\na2\ni + 32(1 + \u02dc\u01eb/\u03bbmin)\u02dc\u01eb\nt\nX\ni=1\n(\u02dc\u01eb/\u03bbi)3\n\u00132\n\u22644(5 + 11\u02dc\u01eb/\u03bbmin)2\u02dc\u01eb2\nt\nX\ni=1\na2\ni + 64(1 + \u02dc\u01eb/\u03bbmin)2\u02dc\u01eb2\nt\nX\ni=1\n(\u02dc\u01eb/\u03bbi)2\n+ 128(1 + \u02dc\u01eb/\u03bbmin)2(\u02dc\u01eb/\u03bbmin)2\u02dc\u01eb2\nt\nX\ni=1\na2\ni\n+ 2048(1 + \u02dc\u01eb/\u03bbmin)2\u02dc\u01eb2\n\u0012 t\nX\ni=1\n(\u02dc\u01eb/\u03bbi)3\n\u00132\n=\n\u0012\n4(5 + 11\u02dc\u01eb/\u03bbmin)2 + 128(1 + \u02dc\u01eb/\u03bbmin)2(\u02dc\u01eb/\u03bbmin)2\n\u0013\n\u02dc\u01eb2\nt\nX\ni=1\na2\ni\n+ 64(1 + \u02dc\u01eb/\u03bbmin)2\u02dc\u01eb2\nt\nX\ni=1\n(\u02dc\u01eb/\u03bbi)2 + 2048(1 + \u02dc\u01eb/\u03bbmin)2\u02dc\u01eb2\n\u0012 t\nX\ni=1\n(\u02dc\u01eb/\u03bbi)3\n\u00132\n.\nB.4 Proof of the Main Theorem\nTheorem B.1 Let \u02c6T = T + E \u2208Rk\u00d7k\u00d7k, where T is a symmetric tensor with orthogonal\ndecomposition T = Pk\ni=1 \u03bbiv\u22973\ni\nwhere each \u03bbi > 0, {v1, v2, . . . , vk} is an orthonormal basis,\nand E has operator norm \u01eb := \u2225E\u2225. De\ufb01ne \u03bbmin := min{\u03bbi : i \u2208[k]}, and \u03bbmax := max{\u03bbi :\ni \u2208[k]}. There exists universal constants C1, C2, C3 > 0 such that the following holds. Pick\nany \u03b7 \u2208(0, 1), and suppose\n\u01eb \u2264C1 \u00b7 \u03bbmin\nk\n,\nN \u2265C2 \u00b7\n\u0012\nlog(k) + log log\n\u0010\u03bbmax\n\u01eb\n\u0011\u0013\n,\nand\ns\nln(L/ log2(k/\u03b7))\nln(k)\n\u00b7\n \n1 \u2212ln(ln(L/ log2(k/\u03b7))) + C3\n4 ln(L/ log2(k/\u03b7))\n\u2212\ns\nln(8)\nln(L/ log2(k/\u03b7))\n!\n\u22651.02\n \n1 +\ns\nln(4)\nln(k)\n!\n.\n2815\nAnandkumar, Ge, Hsu, Kakade, and Telgarsky\n(Note that the condition on L holds with L = poly(k) log(1/\u03b7).) Suppose that Algorithm 1\nis iteratively called k times, where the input tensor is \u02c6T in the \ufb01rst call, and in each\nsubsequent call, the input tensor is the de\ufb02ated tensor returned by the previous call. Let\n(\u02c6v1, \u02c6\u03bb1), (\u02c6v2, \u02c6\u03bb2), . . . , (\u02c6vk, \u02c6\u03bbk) be the sequence of estimated eigenvector/eigenvalue pairs re-\nturned in these k calls. With probability at least 1 \u2212\u03b7, there exists a permutation \u03c0 on [k]\nsuch that\n\u2225v\u03c0(j) \u2212\u02c6vj\u2225\u22648\u01eb/\u03bb\u03c0(j),\n|\u03bb\u03c0(j) \u2212\u02c6\u03bbj| \u22645\u01eb,\n\u2200j \u2208[k],\nand\n\r\r\r\rT \u2212\nk\nX\nj=1\n\u02c6\u03bbj\u02c6v\u22973\nj\n\r\r\r\r \u226455\u01eb.\nProof\nWe prove by induction that for each i \u2208[k] (corresponding to the i-th call to\nAlgorithm 1), with probability at least 1 \u2212i\u03b7/k, there exists a permutation \u03c0 on [k] such\nthat the following assertions hold.\n1. For all j \u2264i, \u2225v\u03c0(j) \u2212\u02c6vj\u2225\u22648\u01eb/\u03bb\u03c0(j) and |\u03bb\u03c0(j) \u2212\u02c6\u03bbj| \u226412\u01eb.\n2. The error tensor\n\u02dcEi+1 :=\n\u0012\n\u02c6T \u2212\nX\nj\u2264i\n\u02c6\u03bbj\u02c6v\u22973\nj\n\u0013\n\u2212\nX\nj\u2265i+1\n\u03bb\u03c0(j)v\u22973\n\u03c0(j) = E +\nX\nj\u2264i\n\u0010\n\u03bb\u03c0(j)v\u22973\n\u03c0(j) \u2212\u02c6\u03bbj\u02c6v\u22973\nj\n\u0011\nsatis\ufb01es\n\u2225\u02dcEi+1(I, u, u)\u2225\u226456\u01eb,\n\u2200u \u2208Sk\u22121;\n(26)\n\u2225\u02dcEi+1(I, u, u)\u2225\u22642\u01eb,\n\u2200u \u2208Sk\u22121 s.t. \u2203j \u2265i + 1 \u0005 (u\u22a4v\u03c0(j))2 \u22651 \u2212(168\u01eb/\u03bb\u03c0(j))2.\n(27)\nWe actually take i = 0 as the base case, so we can ignore the \ufb01rst assertion, and just observe\nthat for i = 0,\n\u02dcE1 = \u02c6T \u2212\nk\nX\nj=1\n\u03bbiv\u22973\ni\n= E.\nWe have \u2225\u02dcE1\u2225= \u2225E\u2225= \u01eb, and therefore the second assertion holds.\nNow \ufb01x some i \u2208[k], and assume as the inductive hypothesis that, with probability at\nleast 1 \u2212(i \u22121)\u03b7/k, there exists a permutation \u03c0 such that two assertions above hold for\ni \u22121 (call this Eventi\u22121). The i-th call to Algorithm 1 takes as input\n\u02dcTi := \u02c6T \u2212\nX\nj\u2264i\u22121\n\u02c6\u03bbj\u02c6v\u22973\nj ,\nwhich is intended to be an approximation to\nTi :=\nX\nj\u2265i\n\u03bb\u03c0(j)v\u22973\n\u03c0(j).\n2816\nTensor Decompositions for Learning Latent Variable Models\nObserve that\n\u02dcTi \u2212Ti = \u02dcEi,\nwhich satis\ufb01es the second assertion in the inductive hypothesis.\nWe may write Ti =\nPk\nl=1 \u02dc\u03bblv\u22973\nl\nwhere \u02dc\u03bbl = \u03bbl whenever \u03c0\u22121(l) \u2265i, and \u02dc\u03bbl = 0 whenever \u03c0\u22121(l) \u2264i \u22121. This\nform is used when referring to \u02dcT or the \u02dc\u03bbi in preceding lemmas (in particular, Lemma B.1\nand Lemma B.4).\nBy Lemma B.1, with conditional probability at least 1\u2212\u03b7/k given Eventi\u22121, at least one\nof \u03b8(\u03c4)\n0\nfor \u03c4 \u2208[L] is \u03b3-separated relative to \u03c0(jmax), where jmax := arg maxj\u2265i \u03bb\u03c0(j), (for\n\u03b3 = 0.01; call this Event\u2032\ni; note that the application of Lemma B.1 determines C3). Therefore\nPr[Eventi\u22121 \u2229Event\u2032\ni] = Pr[Event\u2032\ni|Eventi\u22121] Pr[Eventi\u22121] \u2265(1 \u2212\u03b7/k)(1 \u2212(i \u22121)\u03b7/k) \u2265\n1 \u2212i\u03b7/k. It remains to show that Eventi\u22121 \u2229Event\u2032\ni \u2286Eventi; so henceforth we condition\non Eventi\u22121 \u2229Event\u2032\ni.\nSet\nC1 := min\n\b\n(56 \u00b7 9 \u00b7 102)\u22121, (100 \u00b7 168)\u22121, \u2206\u2032 from Lemma B.5 with \u2206= 1/50\n\t\n.\n(28)\nFor all \u03c4 \u2208[L] such that \u03b8(\u03c4)\n0\nis \u03b3-separated relative to \u03c0(jmax), we have (i) |\u03b8(\u03c4)\njmax,0| \u22651/\n\u221a\nk,\nand (ii) that by Lemma B.4 (using \u02dc\u01eb/p := 2\u01eb, \u03ba := 1, and i\u2217:= \u03c0(jmax), and providing C2),\n| \u02dcTi(\u03b8(\u03c4)\nN , \u03b8(\u03c4)\nN , \u03b8(\u03c4)\nN ) \u2212\u03bb\u03c0(jmax)| \u22645\u01eb\n(notice by de\ufb01nition that \u03b3 \u22651/100 implies \u03b30 \u22651 \u2212/(1 + \u03b3) \u22651/101, thus it follows\nfrom the bounds on the other quantities that \u02dc\u01eb = 2p\u01eb \u226456C1 \u00b7 \u03bbmin\nk\n<\n\u03b30\n2(1+8\u03ba) \u00b7 \u02dc\u03bbmin \u00b7 \u03b82\ni\u2217,0 as\nnecessary). Therefore \u03b8N := \u03b8(\u03c4 \u2217)\nN\nmust satisfy\n\u02dcTi(\u03b8N, \u03b8N, \u03b8N) = max\n\u03c4\u2208[L]\n\u02dcTi(\u03b8(\u03c4)\nN , \u03b8(\u03c4)\nN , \u03b8(\u03c4)\nN ) \u2265max\nj\u2265i \u03bb\u03c0(j) \u22125\u01eb = \u03bb\u03c0(jmax) \u22125\u01eb.\nOn the other hand, by the triangle inequality,\n\u02dcTi(\u03b8N, \u03b8N, \u03b8N) \u2264\nX\nj\u2265i\n\u03bb\u03c0(j)\u03b83\n\u03c0(j),N + | \u02dcEi(\u03b8N, \u03b8N, \u03b8N)|\n\u2264\nX\nj\u2265i\n\u03bb\u03c0(j)|\u03b8\u03c0(j),N|\u03b82\n\u03c0(j),N + 56\u01eb\n\u2264\u03bb\u03c0(j\u2217)|\u03b8\u03c0(j\u2217),N| + 56\u01eb\nwhere j\u2217:= arg maxj\u2265i \u03bb\u03c0(j)|\u03b8\u03c0(j),N|. Therefore\n\u03bb\u03c0(j\u2217)|\u03b8\u03c0(j\u2217),N| \u2265\u03bb\u03c0(jmax) \u22125\u01eb \u221256\u01eb \u22654\n5\u03bb\u03c0(jmax).\nSquaring both sides and using the fact that \u03b82\n\u03c0(j\u2217),N + \u03b82\n\u03c0(j),N \u22641 for any j \u0338= j\u2217,\n\u0000\u03bb\u03c0(j\u2217)\u03b8\u03c0(j\u2217),N\n\u00012 \u226516\n25\n\u0000\u03bb\u03c0(jmax)\u03b8\u03c0(j\u2217),N\n\u00012 + 16\n25\n\u0000\u03bb\u03c0(jmax)\u03b8\u03c0(j),N\n\u00012\n\u226516\n25\n\u0000\u03bb\u03c0(j\u2217)\u03b8\u03c0(j\u2217),N\n\u00012 + 16\n25\n\u0000\u03bb\u03c0(j)\u03b8\u03c0(j),N\n\u00012\n2817\nAnandkumar, Ge, Hsu, Kakade, and Telgarsky\nwhich in turn implies\n\u03bb\u03c0(j)|\u03b8\u03c0(j),N| \u22643\n4\u03bb\u03c0(j\u2217)|\u03b8\u03c0(j\u2217),N|,\nj \u0338= j\u2217.\nThis means that \u03b8N is (1/4)-separated relative to \u03c0(j\u2217). Also, observe that\n|\u03b8\u03c0(j\u2217),N| \u22654\n5 \u00b7 \u03bb\u03c0(jmax)\n\u03bb\u03c0(j\u2217)\n\u22654\n5,\n\u03bb\u03c0(jmax)\n\u03bb\u03c0(j\u2217)\n\u22645\n4.\nTherefore by Lemma B.4 (using \u02dc\u01eb/p := 2\u01eb, \u03b3 := 1/4, and \u03ba := 5/4), executing another N\npower iterations starting from \u03b8N gives a vector \u02c6\u03b8 that satis\ufb01es\n\u2225\u02c6\u03b8 \u2212v\u03c0(j\u2217)\u2225\u2264\n8\u01eb\n\u03bb\u03c0(j\u2217)\n,\n|\u02c6\u03bb \u2212\u03bb\u03c0(j\u2217)| \u22645\u01eb.\nSince \u02c6vi = \u02c6\u03b8 and \u02c6\u03bbi = \u02c6\u03bb, the \ufb01rst assertion of the inductive hypothesis is satis\ufb01ed, as we\ncan modify the permutation \u03c0 by swapping \u03c0(i) and \u03c0(j\u2217) without a\ufb00ecting the values of\n{\u03c0(j) : j \u2264i \u22121} (recall j\u2217\u2265i).\nWe now argue that \u02dcEi+1 has the required properties to complete the inductive step. By\nLemma B.5 (using \u02dc\u01eb := 5\u01eb and \u2206:= 1/50, the latter providing one upper bound on C1 as\nper (28)), we have for any unit vector u \u2208Sk\u22121,\n\r\r\r\r\r\n\u0012X\nj\u2264i\n\u0010\n\u03bb\u03c0(j)v\u22973\n\u03c0(j) \u2212\u02c6\u03bbj\u02c6v\u22973\nj\n\u0011\u0013\n(I, u, u)\n\r\r\r\r\r \u2264\n\u0012\n1/50 + 100\ni\nX\nj=1\n(u\u22a4v\u03c0(j))2\n\u00131/2\n5\u01eb \u226455\u01eb.\n(29)\nTherefore by the triangle inequality,\n\u2225\u02dcEi+1(I, u, u)\u2225\u2264\u2225E(I, u, u)\u2225+\n\r\r\r\r\r\n\u0012X\nj\u2264i\n\u0010\n\u03bb\u03c0(j)v\u22973\n\u03c0(j) \u2212\u02c6\u03bbj\u02c6v\u22973\nj\n\u0011\u0013\n(I, u, u)\n\r\r\r\r\r \u226456\u01eb.\nThus the bound (26) holds.\nTo prove that (27) holds, pick any unit vector u \u2208Sk\u22121 such that there exists j\u2032 \u2265i + 1\nwith (u\u22a4v\u03c0(j\u2032))2 \u22651 \u2212(168\u01eb/\u03bb\u03c0(j\u2032))2. We have, via the second bound on C1 in (28) and the\ncorresponding assumed bound \u01eb \u2264C1 \u00b7 \u03bbmin\nk ,\n100\ni\nX\nj=1\n(u\u22a4v\u03c0(j))2 \u2264100\n\u0010\n1 \u2212(u\u22a4v\u03c0(j\u2032))2\u0011\n\u2264100\n\u0012 168\u01eb\n\u03bb\u03c0(j\u2032)\n\u00132\n\u22641\n50,\nand therefore\n\u0012\n1/50 + 100\ni\nX\nj=1\n(u\u22a4v\u03c0(j))2\n\u00131/2\n5\u01eb \u2264(1/50 + 1/50)1/25\u01eb \u2264\u01eb.\nBy the triangle inequality, we have \u2225\u02dcEi+1(I, u, u)\u2225\u22642\u01eb.\nTherefore (27) holds, so the\nsecond assertion of the inductive hypothesis holds. Thus Eventi\u22121 \u2229Event\u2032\ni \u2286Eventi, and\nPr[Eventi] \u2265Pr[Eventi\u22121 \u2229Event\u2032\ni] \u22651\u2212i\u03b7/k. We conclude that by the induction principle,\n2818\nTensor Decompositions for Learning Latent Variable Models\nthere exists a permutation \u03c0 such that two assertions hold for i = k, with probability at\nleast 1 \u2212\u03b7.\nFrom the last induction step (i = k), it is also clear from (29) that \u2225T \u2212Pk\nj=1 \u02c6\u03bbj\u02c6v\u22973\nj \u2225\u2264\n55\u01eb (in Eventk\u22121 \u2229Event\u2032\nk). This completes the proof of the theorem.\nAppendix C. Variant of Robust Power Method that uses a Stopping\nCondition\nIn this section we analyze a variant of Algorithm 1 that uses a stopping condition. The\nvariant is described in Algorithm 2. The key di\ufb00erence is that the inner for-loop is repeated\nuntil a stopping condition is satis\ufb01ed (rather than explicitly L times). The stopping condi-\ntion ensures that the power iteration is converging to an eigenvector, and it will be satis\ufb01ed\nwithin poly(k) random restarts with high probability. The condition depends on one new\nquantity, r, which should be set to r := k \u2212# de\ufb02ation steps so far (i.e., the \ufb01rst call to\nAlgorithm 2 uses r = k, the second call uses r = k \u22121, and so on).\nAlgorithm 2 Robust tensor power method with stopping condition\ninput symmetric tensor \u02dcT \u2208Rk\u00d7k\u00d7k, number of iterations N, expected rank r.\noutput the estimated eigenvector/eigenvalue pair; the de\ufb02ated tensor.\n1: repeat\n2:\nDraw \u03b80 uniformly at random from the unit sphere in Rk.\n3:\nfor t = 1 to N do\n4:\nCompute power iteration update\n\u03b8t\n:=\n\u02dcT(I, \u03b8t\u22121, \u03b8t\u22121)\n\u2225\u02dcT(I, \u03b8t\u22121, \u03b8t\u22121)\u2225\n(30)\n5:\nend for\n6: until the following stopping condition is satis\ufb01ed:\n| \u02dcT(\u03b8N, \u03b8N, \u03b8N)| \u2265max\n\u001a 1\n2\u221ar\u2225\u02dcT\u2225F ,\n1\n1.05\u2225\u02dcT(I, I, \u03b8N)\u2225F\n\u001b\n.\n7: Do N power iteration updates (30) starting from \u03b8N to obtain \u02c6\u03b8, and set \u02c6\u03bb := \u02dcT(\u02c6\u03b8, \u02c6\u03b8, \u02c6\u03b8).\n8: return the estimated eigenvector/eigenvalue pair (\u02c6\u03b8, \u02c6\u03bb); the de\ufb02ated tensor \u02dcT \u2212\u02c6\u03bb \u02c6\u03b8\u22973.\nC.1 Stopping Condition Analysis\nFor a matrix A, we use \u2225A\u2225F := (P\ni,j A2\ni,j)1/2 to denote its Frobenius norm. For a third-\norder tensor A, we use \u2225A\u2225F := (P\ni \u2225A(I, I, ei)\u22252\nF )1/2 = (P\ni \u2225A(I, I, vi)\u22252\nF )1/2.\nDe\ufb01ne \u02dcT as before in (11):\n\u02dcT :=\nk\nX\ni=1\n\u02dc\u03bbiv\u22973\ni\n+ \u02dcE.\n2819\nAnandkumar, Ge, Hsu, Kakade, and Telgarsky\nWe assume \u02dcE is a symmetric tensor such that, for some constant p > 1,\n\u2225\u02dcE(I, u, u)\u2225\u2264\u02dc\u01eb,\n\u2200u \u2208Sk\u22121;\n\u2225\u02dcE(I, u, u)\u2225\u2264\u02dc\u01eb/p,\n\u2200u \u2208Sk\u22121 s.t. (u\u22a4v1)2 \u22651 \u2212(3\u02dc\u01eb/\u02dc\u03bb1)2;\n\u2225\u02dcE\u2225F \u2264\u02dc\u01ebF .\nAssume that not all \u02dc\u03bbi are zero, and de\ufb01ne\n\u02dc\u03bbmin := min{\u02dc\u03bbi : i \u2208[k], \u02dc\u03bbi > 0},\n\u02dc\u03bbmax := max{\u02dc\u03bbi : i \u2208[k]},\n\u2113:= |{i \u2208[k] : \u02dc\u03bbi > 0}|,\n\u02dc\u03bbavg :=\n\u00121\n\u2113\nk\nX\ni=1\n\u02dc\u03bb2\ni\n\u00131/2\n.\nWe show in Lemma C.1 that if the stopping condition is satis\ufb01ed by a vector \u03b8, then\nit must be close to an eigenvector of \u02dcT. Then in Lemma C.2, we show that the stopping\ncondition is satis\ufb01ed by \u03b8N when \u03b80 is a good starting point (as per the conditions of\nLemma B.4).\nLemma C.1 Fix any vector \u03b8 = Pk\ni=1 \u03b8ivi, and let i\u2217:= arg maxi\u2208[k] \u02dc\u03bbi|\u03b8i|. Assume that\n\u2113\u22651 and that for some \u03b1 \u2208(0, 1/20) and \u03b2 \u22652\u03b1/\n\u221a\nk,\n\u02dc\u01eb \u2264\u03b1 \u00b7\n\u02dc\u03bbmin\n\u221a\nk\n,\n\u02dc\u01ebF \u2264\n\u221a\n\u2113\n\u00101\n2 \u2212\n\u03b1\n\u03b2\n\u221a\nk\n\u0011\n\u00b7 \u02dc\u03bbavg.\nIf the stopping condition\n| \u02dcT(\u03b8, \u03b8, \u03b8)| \u2265max\n\u001a \u03b2\n\u221a\n\u2113\n\u2225\u02dcT\u2225F ,\n1\n1 + \u03b1\u2225\u02dcT(I, I, \u03b8)\u2225F\n\u001b\n(31)\nholds, then\n1. \u02dc\u03bbi\u2217\u2265\u03b2\u02dc\u03bbavg/2 and \u02dc\u03bbi\u2217|\u03b8i\u2217| > 0;\n2. maxi\u0338=i\u2217\u02dc\u03bbi|\u03b8i| \u2264\n\u221a\n7\u03b1 \u00b7 \u02dc\u03bbi\u2217|\u03b8i\u2217|;\n3. \u03b8i\u2217\u22651 \u22122\u03b1.\nProof Without loss of generality, assume i\u2217= 1. First, we claim that \u02dc\u03bb1|\u03b81| > 0. By the\ntriangle inequality,\n| \u02dcT(\u03b8, \u03b8, \u03b8)| \u2264\nk\nX\ni=1\n\u02dc\u03bbi\u03b83\ni + | \u02dcE(\u03b8, \u03b8, \u03b8)| \u2264\nk\nX\ni=1\n\u02dc\u03bbi|\u03b8i|\u03b82\ni + \u02dc\u01eb \u2264\u02dc\u03bb1|\u03b81| + \u02dc\u01eb.\n2820\nTensor Decompositions for Learning Latent Variable Models\nMoreover,\n\u2225\u02dcT\u2225F \u2265\n\r\r\r\r\nk\nX\ni=1\n\u02dc\u03bbiv\u22973\ni\n\r\r\r\r\nF\n\u2212\u2225\u02dcE\u2225F\n=\n\u0012 k\nX\nj=1\n\r\r\r\r\nk\nX\ni=1\n\u02dc\u03bbiviv\u22a4\ni (v\u22a4\ni vj)\n\r\r\r\r\n2\nF\n\u00131/2\n\u2212\u2225\u02dcE\u2225F\n=\n\u0012 k\nX\nj=1\n\r\r\r\r\u02dc\u03bbjvjv\u22a4\nj\n\r\r\r\r\n2\nF\n\u00131/2\n\u2212\u2225\u02dcE\u2225F\n=\n\u0012 k\nX\nj=1\n\u02dc\u03bb2\nj\n\u00131/2\n\u2212\u2225\u02dcE\u2225F\n\u2265\n\u221a\n\u2113\u02dc\u03bbavg \u2212\u02dc\u01ebF .\nBy assumption, | \u02dcT(\u03b8, \u03b8, \u03b8)| \u2265(\u03b2/\n\u221a\n\u2113)\u2225\u02dcT \u2225F , so\n\u02dc\u03bb1|\u03b81| \u2265\u03b2\u02dc\u03bbavg \u2212\u03b2\n\u221a\n\u2113\n\u02dc\u01ebF \u2212\u02dc\u01eb \u2265\u03b2\u02dc\u03bbavg \u2212\u03b2\n\u00101\n2 \u2212\n\u03b1\n\u03b2\n\u221a\nk\n\u0011\n\u02dc\u03bbavg \u2212\u03b1\n\u221a\nk\n\u02dc\u03bbmin \u2265\u03b2\n2\n\u02dc\u03bbavg\nwhere the second inequality follows from the assumptions on \u02dc\u01eb and \u02dc\u01ebF . Since \u03b2 > 0, \u02dc\u03bbavg > 0,\nand |\u03b81| \u22641, it follows that\n\u02dc\u03bb1 \u2265\u03b2\n2\n\u02dc\u03bbavg,\n\u02dc\u03bb1|\u03b81| > 0.\nThis proves the \ufb01rst claim.\nNow we prove the second claim. De\ufb01ne \u02dc\nM := \u02dcT(I, I, \u03b8) = Pk\ni=1 \u02dc\u03bbi\u03b8iviv\u22a4\ni + \u02dcE(I, I, \u03b8) (a\nsymmetric k \u00d7 k matrix), and consider its eigenvalue decomposition\n\u02dc\nM =\nk\nX\ni=1\n\u03c6iuiu\u22a4\ni\nwhere, without loss of generality, |\u03c61| \u2265|\u03c62| \u2265\u00b7 \u00b7 \u00b7 \u2265|\u03c6k| and {u1, u2, . . . , uk} is an or-\nthonormal basis. Let M := Pk\ni=1 \u02dc\u03bbi\u03b8iviv\u22a4\ni , so \u02dc\nM = M + \u02dcE(I, I, \u03b8). Note that the \u02dc\u03bbi|\u03b8i| and\n|\u03c6i| are the singular values of M and \u02dc\nM, respectively. We now show that the assumption\non | \u02dcT(\u03b8, \u03b8, \u03b8)| implies that almost all of the energy in M is contained in its top singular\ncomponent.\nBy Weyl\u2019s theorem,\n|\u03c61| \u2264\u02dc\u03bb1|\u03b81| + \u2225\u02dc\nM \u2212M\u2225\u2264\u02dc\u03bb1|\u03b81| + \u02dc\u01eb.\nNext, observe that the assumption \u2225\u02dcT(I, I, \u03b8)\u2225F \u2264(1 + \u03b1) \u02dcT(\u03b8, \u03b8, \u03b8) is equivalent to (1 +\n\u03b1)\u03b8\u22a4\u02dc\nM\u03b8 \u2265\u2225\u02dc\nM\u2225F . Therefore, using the fact that |\u03c61| = maxu\u2208Sk\u22121 |u\u22a4\u02dc\nMu|, the triangle\n2821\nAnandkumar, Ge, Hsu, Kakade, and Telgarsky\ninequality, and the fact \u2225A\u2225F \u2264\n\u221a\nk\u2225A\u2225for any matrix A \u2208Rk\u00d7k,\n(1 + \u03b1)|\u03c61| \u2265(1 + \u03b1)\u03b8\u22a4\u02dc\nM\u03b8 \u2265\u2225\u02dc\nM\u2225F\n(32)\n\u2265\n\r\r\r\r\nk\nX\ni=1\n\u02dc\u03bbi\u03b8iviv\u22a4\ni\n\r\r\r\r\nF\n\u2212\n\r\r \u02dcE(I, I, \u03b8)\n\r\r\nF\n\u2265\n\u0012 k\nX\ni=1\n\u02dc\u03bb2\ni \u03b82\ni\n\u00131/2\n\u2212\n\u221a\nk\u2225\u02dcE(I, I, \u03b8)\u2225\n\u2265\n\u0012 k\nX\ni=1\n\u02dc\u03bb2\ni \u03b82\ni\n\u00131/2\n\u2212\n\u221a\nk\u02dc\u01eb.\nCombining these bounds on |\u03c61| gives\n\u02dc\u03bb1|\u03b81| + \u02dc\u01eb \u2265\n1\n1 + \u03b1\n\"\u0012 k\nX\ni=1\n\u02dc\u03bb2\ni \u03b82\ni\n\u00131/2\n\u2212\n\u221a\nk\u02dc\u01eb\n#\n.\n(33)\nThe assumption \u02dc\u01eb \u2264\u03b1\u02dc\u03bbmin/\n\u221a\nk implies that\n\u221a\nk\u02dc\u01eb \u2264\u03b1\u02dc\u03bbmin \u2264\u03b1\n\u0012 k\nX\ni=1\n\u02dc\u03bb2\ni \u03b82\ni\n\u00131/2\n.\nMoreover, since \u02dc\u03bb1|\u03b81| > 0 (by the \ufb01rst claim) and \u02dc\u03bb1|\u03b81| = maxi\u2208[k] \u02dc\u03bbi|\u03b8i|, it follows that\n\u02dc\u03bb1|\u03b81| \u2265\u02dc\u03bbmin max\ni\u2208[k] |\u03b8i| \u2265\n\u02dc\u03bbmin\n\u221a\nk\n,\n(34)\nso we also have\n\u02dc\u01eb \u2264\u03b1\u02dc\u03bb1|\u03b81|.\nApplying these bounds on \u02dc\u01eb to (33), we obtain\n\u02dc\u03bb1|\u03b81| \u2265\n1 \u2212\u03b1\n(1 + \u03b1)2\n\u0012 k\nX\ni=1\n\u02dc\u03bb2\ni \u03b82\ni\n\u00131/2\n\u2265\n1 \u2212\u03b1\n(1 + \u03b1)2\n\u0012\n\u02dc\u03bb2\n1\u03b82\n1 + max\ni\u0338=1\n\u02dc\u03bb2\ni \u03b82\ni\n\u00131/2\nwhich in turn implies (for \u03b1 \u2208(0, 1/20))\nmax\ni\u0338=1\n\u02dc\u03bb2\ni \u03b82\ni \u2264\n\u0012(1 + \u03b1)4\n(1 \u2212\u03b1)2 \u22121\n\u0013\n\u00b7 \u02dc\u03bb2\n1\u03b82\n1 \u22647\u03b1 \u00b7 \u02dc\u03bb2\n1\u03b82\n1.\nTherefore maxi\u0338=1 \u02dc\u03bbi|\u03b8i| \u2264\n\u221a\n7\u03b1 \u00b7 \u02dc\u03bb1|\u03b81|, proving the second claim.\nNow we prove the \ufb01nal claim. This is done by (i) showing that \u03b8 has a large projection\nonto u1, (ii) using an SVD perturbation argument to show that \u00b1u1 is close to v1, and (iii)\nconcluding that \u03b8 has a large projection onto v1.\nWe begin by showing that (u\u22a4\n1 \u03b8)2 is large. Observe that from (32), we have (1+\u03b1)2\u03c62\n1 \u2265\n\u2225\u02dc\nM\u22252\nF \u2265\u03c62\n1 + maxi\u0338=1 \u03c62\ni , and therefore\nmax\ni\u0338=1 |\u03c6i| \u2264\np\n2\u03b1 + \u03b12 \u00b7 |\u03c61|.\n2822\nTensor Decompositions for Learning Latent Variable Models\nMoreover, by the triangle inequality,\n|\u03b8\u22a4\u02dc\nM\u03b8| \u2264\nk\nX\ni=1\n|\u03c6i|(u\u22a4\ni \u03b8)2\n\u2264|\u03c61|(u\u22a4\n1 \u03b8)2 + max\ni\u0338=1 |\u03c6i|\n\u00001 \u2212(u\u22a4\n1 \u03b8)2\u0001\n= (u\u22a4\n1 \u03b8)2\u0000|\u03c61| \u2212max\ni\u0338=1 |\u03c6i|\n\u0001\n+ max\ni\u0338=1 |\u03c6i|.\nUsing (32) once more, we have |\u03b8\u22a4\u02dc\nM\u03b8| \u2265\u2225\u02dc\nM\u2225F /(1 + \u03b1) \u2265|\u03c61|/(1 + \u03b1), so\n(u\u22a4\n1 \u03b8)2 \u2265\n1\n1+\u03b1 \u2212maxi\u0338=1\n|\u03c6i|\n|\u03c61|\n1 \u2212maxi\u0338=1\n|\u03c6i|\n|\u03c61|\n= 1 \u2212\n\u03b1\n(1 + \u03b1)\n\u0010\n1 \u2212maxi\u0338=1\n|\u03c6i|\n|\u03c61|\n\u0011 \u22641 \u2212\n\u03b1\n(1 + \u03b1)(1 \u2212\n\u221a\n2\u03b1 + \u03b12)\n.\nNow we show that (u\u22a4\n1 v1)2 is also large. By the second claim, the assumption on \u02dc\u01eb, and (34),\n\u02dc\u03bb1|\u03b81| \u2212max\ni\u0338=1\n\u02dc\u03bbi|\u03b8i| > (1 \u2212\n\u221a\n7\u03b1) \u00b7 \u02dc\u03bb1|\u03b81| \u2265(1 \u2212\n\u221a\n7\u03b1) \u00b7 \u02dc\u03bbmin/\n\u221a\nk.\nCombining this with Weyl\u2019s theorem gives\n|\u03c61| \u2212max\ni\u0338=1\n\u02dc\u03bbi|\u03b8i| \u2265\u02dc\u03bb1|\u03b81| \u2212\u02dc\u01eb \u2212max\ni\u0338=1\n\u02dc\u03bbi|\u03b8i| \u2265(1 \u2212(\u03b1 +\n\u221a\n7\u03b1)) \u00b7 \u02dc\u03bbmin/\n\u221a\nk,\nso we may apply Wedin\u2019s theorem to obtain\n(u\u22a4\n1 v1)2 \u22651 \u2212\n\u0012\n\u2225\u02dcE(I, I, \u03b8)\u2225\n|\u03c61| \u2212maxi\u0338=1 \u02dc\u03bbi|\u03b8i|\n\u00132\n\u22651 \u2212\n\u0012\n\u03b1\n1 \u2212(\u03b1 +\n\u221a\n7\u03b1)\n\u00132\n.\nIt remains to show that \u03b81 = v\u22a4\n1 \u03b8 is large.\nIndeed, by the triangle inequality, Cauchy-\nSchwarz, and the above inequalities on (u\u22a4\n1 v1)2 and (u\u22a4\n1 \u03b8)2,\n|v\u22a4\n1 \u03b8| =\n\f\f\f\f\nk\nX\ni=1\n(u\u22a4\ni v1)(u\u22a4\ni \u03b8)\n\f\f\f\f\n\u2265|u\u22a4\n1 v1||u\u22a4\n1 \u03b8| \u2212\nk\nX\ni=2\n|u\u22a4\ni v1||u\u22a4\ni \u03b8|\n\u2265|u\u22a4\n1 v1||u\u22a4\n1 \u03b8| \u2212\n\u0012 k\nX\ni=2\n(u\u22a4\ni v1)2\n\u00131/2\u0012 k\nX\ni=2\n(u\u22a4\ni \u03b8)2\n\u00131/2\n= |u\u22a4\n1 v1||u\u22a4\n1 \u03b8| \u2212\n\u0012\u0010\n1 \u2212(u\u22a4\ni v1)2\u0011\u0010\n1 \u2212(u\u22a4\ni \u03b8)2\u0011\u00131/2\n\u2265\n \u0012\n1 \u2212\n\u03b1\n(1 + \u03b1)(1 \u2212\n\u221a\n2\u03b1 + \u03b12)\n\u0013\u0012\n1 \u2212\n\u0012\n\u03b1\n1 \u2212(\u03b1 +\n\u221a\n7\u03b1)\n\u00132\u0013!1/2\n\u2212\n \n\u03b1\n(1 + \u03b1)(1 \u2212\n\u221a\n2\u03b1 + \u03b12)\n\u00b7\n\u0012\n\u03b1\n1 \u2212(\u03b1 +\n\u221a\n7\u03b1)\n\u00132!1/2\n\u22651 \u22122\u03b1\n2823\nAnandkumar, Ge, Hsu, Kakade, and Telgarsky\nfor \u03b1 \u2208(0, 1/20). Moreover, by assumption we have \u02dcT(\u03b8, \u03b8, \u03b8) \u22650, and\n\u02dcT(\u03b8, \u03b8, \u03b8) =\nk\nX\ni=1\n\u02dc\u03bbi\u03b83\ni + \u02dcE(\u03b8, \u03b8, \u03b8)\n= \u02dc\u03bb1\u03b83\n1 +\nk\nX\ni=2\n\u02dc\u03bbi\u03b83\ni + \u02dcE(\u03b8, \u03b8, \u03b8)\n\u2264\u02dc\u03bb1\u03b83\n1 + max\ni\u0338=1\n\u02dc\u03bbi|\u03b8i|\nk\nX\ni=2\n\u03b82\ni + \u02dc\u01eb\n\u2264\u02dc\u03bb1\u03b83\n1 +\n\u221a\n7\u03b1\u02dc\u03bb1|\u03b81|(1 \u2212\u03b82\n1) + \u02dc\u01eb\n(by the second claim)\n\u2264\u02dc\u03bb1|\u03b81|3\n\u0012\nsign(\u03b81) +\n\u221a\n7\u03b1\n(1 \u22122\u03b1)2 \u2212\n\u221a\n7\u03b1 +\n\u03b1\n(1 \u22122\u03b1)3\n\u0013\n(since |\u03b81| \u22651 \u22122\u03b1)\n< \u02dc\u03bb1|\u03b81|3\u0010\nsign(\u03b81) + 1\n\u0011\nso sign(\u03b81) > \u22121, meaning \u03b81 > 0. Therefore \u03b81 = |\u03b81| \u22651 \u22122\u03b1. This proves the \ufb01nal\nclaim.\nLemma C.2 Fix \u03b1, \u03b2 \u2208(0, 1). Assume \u02dc\u03bbi\u2217= maxi\u2208[k] \u02dc\u03bbi and\n\u02dc\u01eb \u2264min\n\u001a\n\u03b1\n5\n\u221a\nk + 7\n, 1 \u2212\u03b2\n7\n\u001b\n\u00b7 \u02dc\u03bbi\u2217,\n\u02dc\u01ebF \u2264\n\u221a\n\u2113\u00b7 1 \u2212\u03b2\n2\u03b2\n\u00b7 \u02dc\u03bbi\u2217.\nTo the conclusion of Lemma B.4, it can be added that the stopping condition (31) is satis\ufb01ed\nby \u03b8 = \u03b8t.\nProof Without loss of generality, assume i\u2217= 1. By the triangle inequality and Cauchy-\nSchwarz,\n\u2225\u02dcT(I, I, \u03b8t)\u2225F \u2264\u02dc\u03bb1|\u03b81,t| +\nX\ni\u0338=1\n\u03bbi|\u03b8i,t| + \u2225\u02dcE(I, I, \u03b8t)\u2225F \u2264\u02dc\u03bb1|\u03b81,t| + \u02dc\u03bb1\n\u221a\nk\n\u0012X\ni\u0338=1\n\u03b82\ni,t\n\u00131/2\n+\n\u221a\nk\u02dc\u01eb\n\u2264\u02dc\u03bb1|\u03b81,t| + 3\n\u221a\nk\u02dc\u01eb\np\n+\n\u221a\nk\u02dc\u01eb.\nwhere the last step uses the fact that \u03b82\n1,t \u22651 \u2212(3\u02dc\u01eb/(p\u02dc\u03bb1))2. Moreover,\n\u02dcT(\u03b8t, \u03b8t, \u03b8t) \u2265\u02dc\u03bb1 \u2212\n\u0012\n27\n\u0010 \u02dc\u01eb\np\u03bb1\n\u00112\n+ 2\n\u0013 \u02dc\u01eb\np.\nCombining these two inequalities with the assumption on \u02dc\u01eb implies that\n\u02dcT(\u03b8t, \u03b8t, \u03b8t) \u2265\n1\n1 + \u03b1\u2225\u02dcT(I, I, \u03b8t)\u2225F .\n2824\nTensor Decompositions for Learning Latent Variable Models\nUsing the de\ufb01nition of the tensor Frobenius norm, we have\n1\n\u221a\n\u2113\n\u2225\u02dcT\u2225F \u22641\n\u221a\n\u2113\n\r\r\r\r\nk\nX\ni=1\n\u02dc\u03bbiv\u22973\ni\n\r\r\r\r\nF\n+ 1\n\u221a\n\u2113\n\u2225\u02dcE\u2225F = \u02dc\u03bbavg + 1\n\u221a\n\u2113\n\u2225\u02dcE\u2225F \u2264\u02dc\u03bbavg + 1\n\u221a\n\u2113\n\u02dc\u01ebF .\nCombining this with the above inequality implies\n\u02dcT(I, I, \u03b8t) \u2265\u03b2\n\u221a\n\u2113\n\u2225\u02dcT\u2225F .\nTherefore the stopping condition (31) is satis\ufb01ed.\nC.2 Sketch of Analysis of Algorithm 2\nThe analysis of Algorithm 2 is very similar to the proof of Theorem 5.1 for Algorithm 1, so\nhere we just sketch the essential di\ufb00erences.\nFirst, the guarantee a\ufb00orded to Algorithm 2 is somewhat di\ufb00erent than Theorem 5.1.\nSpeci\ufb01cally, it is of the following form: (i) under appropriate conditions, upon termination,\nthe algorithm returns an accurate decomposition, and (ii) the algorithm terminates after\npoly(k) random restarts with high probability.\nThe conditions on \u01eb and N are the same (but for possibly di\ufb00erent universal constants\nC1, C2). In Lemma C.1 and Lemma C.2, there is reference to a condition on the Frobenius\nnorm of E, but we may use the inequality \u2225E\u2225F \u2264k\u2225E\u2225\u2264k\u01eb so that the condition is\nsubsumed by the \u01eb condition.\nNow we outline the di\ufb00erences relative to the proof of Theorem 5.1. The basic structure\nof the induction argument is the same.\nIn the induction step, we argue that (i) if the\nstopping condition is satis\ufb01ed, then by Lemma C.1 (with \u03b1 = 0.05 and \u03b2 = 1/2), we have\na vector \u03b8N such that, for some j\u2217\u2265i,\n1. \u03bb\u03c0(j\u2217) \u2265\u03bb\u03c0(jmax)/(4\n\u221a\nk);\n2. \u03b8N is (1/4)-separated relative to \u03c0(j\u2217);\n3. \u03b8\u03c0(j\u2217),N \u22654/5;\nand (ii) the stopping condition is satis\ufb01ed within poly(k) random restarts (via Lemma B.1\nand Lemma C.2) with high probability. We now invoke Lemma B.4 to argue that executing\nanother N power iterations starting from \u03b8N gives a vector \u02c6\u03b8 that satis\ufb01es\n\u2225\u02c6\u03b8 \u2212v\u03c0(j\u2217)\u2225\u2264\n8\u01eb\n\u03bb\u03c0(j\u2217)\n,\n|\u02c6\u03bb \u2212\u03bb\u03c0(j\u2217)| \u22645\u01eb.\nThe main di\ufb00erence here, relative to the proof of Theorem 5.1, is that we use \u03ba := 4\n\u221a\nk\n(rather than \u03ba = O(1)), but this ultimately leads to the same guarantee after taking into\nconsideration the condition \u01eb \u2264C1\u03bbmin/k. The remainder of the analysis is essentially the\nsame as the proof of Theorem 5.1.\n2825\nAnandkumar, Ge, Hsu, Kakade, and Telgarsky\nAppendix D. Simultaneous Diagonalization for Tensor Decomposition\nAs discussed in the introduction, another standard approach to certain tensor decomposition\nproblems is to simultaneously diagonalize a collection of similar matrices obtained from the\ngiven tensor. We now examine this approach in the context of our latent variable models,\nwhere\nM2\n=\nk\nX\ni=1\nwi \u00b5i \u2297\u00b5i\nM3\n=\nk\nX\ni=1\nwi \u00b5i \u2297\u00b5i \u2297\u00b5i.\nLet V := [\u00b51|\u00b52| \u00b7 \u00b7 \u00b7 |\u00b5k] and D(\u03b7) := diag(\u00b5\u22a4\n1 \u03b7, \u00b5\u22a4\n2 \u03b7, . . . , \u00b5\u22a4\nk \u03b7), so\nM2\n=\nV diag(w1, w2, . . . wk)V \u22a4\nM3(I, I, \u03b7)\n=\nV diag(w1, w2, . . . wk)D(\u03b7)V \u22a4\nThus, the problem of determining the \u00b5i can be cast as a simultaneous diagonalization\nproblem: \ufb01nd a matrix X such that X\u22a4M2X and X\u22a4M3(I, I, \u03b7)X (for all \u03b7) are diagonal.\nIt is easy to see that if the \u00b5i are linearly independent, then the solution X\u22a4= V \u2020 is unique\nup to permutation and rescaling of the columns.\nWith exact moments, a simple approach is as follows. Assume for simplicity that d = k,\nand de\ufb01ne\nM(\u03b7) := M3(I, I, \u03b7)M\u22121\n2\n= V D(\u03b7)V \u22121.\nObserve that if the diagonal entries of D(\u03b7) are distinct, then the eigenvectors of M(\u03b7) are\nthe columns of V (up to permutation and scaling). This criterion is satis\ufb01ed almost surely\nwhen \u03b7 is chosen randomly from a continuous distribution over Rk.\nThe above technique (or some variant thereof) was previously used to give the e\ufb03cient\nlearnability results, where the computational and sample complexity bounds were polyno-\nmial in relevant parameters of the problem, including the rank parameter k (Mossel and Roch,\n2006; Anandkumar et al., 2012c,a; Hsu and Kakade, 2013). However, the speci\ufb01c polyno-\nmial dependence on k was rather large due to the need for the diagonal entries of D(\u03b7) to be\nwell-separated. This is because with \ufb01nite samples, M(\u03b7) is only known up to some pertur-\nbation, and thus the sample complexity bound depends inversely in (some polynomial of) the\nseparation of the diagonal entries of D(\u03b7). With \u03b7 drawn uniformly at random from the unit\nsphere in Rk, the separation was only guaranteed to be roughly 1/k2.5 (Anandkumar et al.,\n2012c) (while this may be a loose estimate, the instability is observed in practice). In con-\ntrast, using the tensor power method to approximately recover V (and hence the model\nparameters \u00b5i and wi) requires only a mild, lower-order dependence on k.\nIt should be noted, however, that the use of a single random choice of \u03b7 is quite restric-\ntive, and it is easy to see that a simultaneous diagonalization of M(\u03b7) for several choices\nof \u03b7 can be bene\ufb01cial. While the uniqueness of the eigendecomposition of M(\u03b7) is only\nguaranteed when the diagonal entries of D(\u03b7) are distinct, the simultaneous diagonaliza-\ntion of M(\u03b7(1)), M(\u03b7(2)), . . . , M(\u03b7(m)) for vectors \u03b7(1), \u03b7(2), . . . , \u03b7(m) is unique as long as the\n2826\nTensor Decompositions for Learning Latent Variable Models\ncolumns of\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8f0\n\u00b5\u22a4\n1 \u03b7(1)\n\u00b5\u22a4\n2 \u03b7(1)\n\u00b7 \u00b7 \u00b7\n\u00b5\u22a4\nk \u03b7(1)\n\u00b5\u22a4\n1 \u03b7(2)\n\u00b5\u22a4\n2 \u03b7(2)\n\u00b7 \u00b7 \u00b7\n\u00b5\u22a4\nk \u03b7(2)\n...\n...\n...\n...\n\u00b5\u22a4\n1 \u03b7(m)\n\u00b5\u22a4\n2 \u03b7(m)\n\u00b7 \u00b7 \u00b7\n\u00b5\u22a4\nk \u03b7(m)\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fb\nare distinct (i.e., for each pair of column indices i, j, there exists a row index r such\nthat the (r, i)-th and (r, j)-th entries are distinct).\nThis is a much weaker requirement\nfor uniqueness, and therefore may translate to an improved perturbation analysis.\nIn\nfact, using the techniques discussed in Section 4.3, we may even reduce the problem to\nan orthogonal simultaneous diagonalization, which may be easier to obtain.\nFurther-\nmore, a number of robust numerical methods for (approximately) simultaneously diag-\nonalizing collections of matrices have been proposed and used successfully in the liter-\nature (e.g., Bunse-Gerstner et al., 1993; Cardoso and Souloumiac, 1993; Cardoso, 1994;\nCardoso and Comon, 1996; Ziehe et al., 2004). Another alternative and a more stable ap-\nproach compared to full diagonalization is a Schur-like method which \ufb01nds a unitary matrix\nU which simultaneously triangularizes the respective matrices (Corless et al., 1997). It is an\ninteresting open question whether these techniques can yield similar improved learnability\nresults and also enjoy the attractive computational properties of the tensor power method.\nReferences\nD. Achlioptas and F. McSherry.\nOn spectral learning of mixtures of distributions.\nIn\nEighteenth Annual Conference on Learning Theory, pages 458\u2013469, 2005.\nE. S. Allman, C. Matias, and J. A. Rhodes. Identi\ufb01ability of parameters in latent structure\nmodels with many observed variables. The Annals of Statistics, 37(6A):3099\u20133132, 2009.\nA. Anandkumar, D. P. Foster, D. Hsu, S. M. Kakade, and Y.-K. Liu. A spectral algorithm\nfor latent Dirichlet allocation. In Advances in Neural Information Processing Systems 25,\n2012a.\nA. Anandkumar, D. Hsu, F. Huang, and S. M. Kakade. Learning mixtures of tree graphical\nmodels. In Advances in Neural Information Processing Systems 25, 2012b.\nA. Anandkumar, D. Hsu, and S. M. Kakade. A method of moments for mixture models\nand hidden Markov models. In Twenty-Fifth Annual Conference on Learning Theory,\nvolume 23, pages 33.1\u201333.34, 2012c.\nJ. Anderson, M. Belkin, N. Goyal, L. Rademacher, and J. Voss. The more, the merrier:\nthe blessing of dimensionality for learning large Gaussian mixtures. In Twenty-Seventh\nAnnual Conference on Learning Theory, 2014.\nS. Arora and R. Kannan. Learning mixtures of separated nonspherical Gaussians. The\nAnnals of Applied Probability, 15(1A):69\u201392, 2005.\nS. Arora, R. Ge, and A. Moitra. Learning topic models \u2014 going beyond SVD. In Fifty-Third\nIEEE Annual Symposium on Foundations of Computer Science, pages 1\u201310, 2012a.\n2827\nAnandkumar, Ge, Hsu, Kakade, and Telgarsky\nS. Arora, R. Ge, A. Moitra, and S. Sachdeva.\nProvable ICA with unknown Gaussian\nnoise, and implications for Gaussian mixtures and autoencoders. In Advances in Neural\nInformation Processing Systems 25, 2012b.\nT. Austin. On exchangeable random variables and the statistics of large graphs and hyper-\ngraphs. Probab. Survey, 5:80\u2013145, 2008.\nR. Bailly. Quadratic weighted automata: Spectral algorithm and likelihood maximization.\nJournal of Machine Learning Research, 2011.\nB. Balle and M. Mohri. Spectral learning of general weighted automata via constrained\nmatrix completion. In Advances in Neural Information Processing Systems 25, 2012.\nB. Balle, A. Quattoni, and X. Carreras. Local loss optimization in operator models: A new\ninsight into spectral learning.\nIn Twenty-Ninth International Conference on Machine\nLearning, 2012.\nM. Belkin and K. Sinha. Polynomial learning of distribution families. In Fifty-First Annual\nIEEE Symposium on Foundations of Computer Science, pages 103\u2013112, 2010.\nA. Bhaskara, M. Charikar, A. Moitra, and A. Vijayaraghavan. Smoothed analysis of ten-\nsor decompositions. In Proceedings of the 46th Annual ACM Symposium on Theory of\nComputing, 2014.\nB. Boots, S. M. Siddiqi, and G. J. Gordon. Closing the learning-planning loop with predic-\ntive state representations. In Proceedings of the Robotics Science and Systems Conference,\n2010.\nS. C. Brubaker and S. Vempala. Isotropic PCA and a\ufb03ne-invariant clustering. In Forty-\nNinth Annual IEEE Symposium on Foundations of Computer Science, 2008.\nA. Bunse-Gerstner, R. Byers, and V. Mehrmann.\nNumerical methods for simultaneous\ndiagonalization. SIAM Journal on Matrix Analysis and Applications, 14(4):927\u2013949, 1993.\nJ.-F. Cardoso. Super-symmetric decomposition of the fourth-order cumulant tensor. blind\nidenti\ufb01cation of more sources than sensors. In Acoustics, Speech, and Signal Processing,\n1991. ICASSP-91., 1991 International Conference on, pages 3109\u20133112. IEEE, 1991.\nJ.-F. Cardoso. Perturbation of joint diagonalizers. Technical Report 94D027, Signal De-\npartment, T\u00b4el\u00b4ecom Paris, 1994.\nJ.-F. Cardoso and P. Comon. Independent component analysis, a survey of some algebraic\nmethods. In IEEE International Symposium on Circuits and Systems, pages 93\u201396, 1996.\nJ.-F. Cardoso and A. Souloumiac.\nBlind beamforming for non Gaussian signals.\nIEE\nProceedings-F, 140(6):362\u2013370, 1993.\nD. Cartwright and B. Sturmfels. The number of eigenvalues of a tensor. Linear Algebra\nAppl., 438(2):942\u2013952, 2013.\n2828\nTensor Decompositions for Learning Latent Variable Models\nR. B. Cattell. Parallel proportional pro\ufb01les and other principles for determining the choice\nof factors by rotation. Psychometrika, 9(4):267\u2013283, 1944.\nJ. T. Chang. Full reconstruction of Markov models on evolutionary trees: Identi\ufb01ability\nand consistency. Mathematical Biosciences, 137:51\u201373, 1996.\nK. Chaudhuri and S. Rao. Learning mixtures of product distributions using correlations\nand independence. In Twenty-First Annual Conference on Learning Theory, pages 9\u201320,\n2008.\nS. B. Cohen, K. Stratos, M. Collins, D. P. Foster, and L. Ungar.\nSpectral learning of\nlatent-variable PCFGs. In Fiftieth Annual Meeting of the Association for Computational\nLinguistics, 2012.\nP. Comon. Independent component analysis, a new concept?\nSignal Processing, 36(3):\n287\u2013314, 1994.\nP. Comon and C. Jutten. Handbook of Blind Source Separation: Independent Component\nAnalysis and Applications. Academic Press. Elsevier, 2010.\nP. Comon, G. Golub, L.-H. Lim, and B. Mourrain. Symmetric tensors and symmetric tensor\nrank. SIAM Journal on Matrix Analysis Appl., 30(3):1254\u20131279, 2008.\nR. M. Corless, P. M. Gianni, and B. M. Trager. A reordered Schur factorization method\nfor zero-dimensional polynomial systems with multiple roots. In Proceedings of the 1997\nInternational Symposium on Symbolic and Algebraic Computation, pages 133\u2013140. ACM,\n1997.\nS. Dasgupta. Learning mixtures of Gaussians. In Fortieth Annual IEEE Symposium on\nFoundations of Computer Science, pages 634\u2013644, 1999.\nS. Dasgupta and L. Schulman. A probabilistic analysis of EM for mixtures of separated,\nspherical Gaussians. Journal of Machine Learning Research, 8(Feb):203\u2013226, 2007.\nL. De Lathauwer, J. Castaing, and J.-F. Cardoso.\nFourth-order cumulant-based blind\nidenti\ufb01cation of underdetermined mixtures. Signal Processing, IEEE Transactions on,\n55(6):2965\u20132973, 2007.\nN. Delfosse and P. Loubaton. Adaptive blind separation of independent sources: a de\ufb02ation\napproach. Signal processing, 45(1):59\u201383, 1995.\nA. P. Dempster, N. M. Laird, and D. B. Rubin. Maximum-likelihood from incomplete data\nvia the EM algorithm. J. Royal Statist. Soc. Ser. B, 39:1\u201338, 1977.\nP. Dhillon, J. Rodu, M. Collins, D. P. Foster, and L. Ungar. Spectral dependency parsing\nwith latent variables. In Joint Conference on Empirical Methods in Natural Language\nProcessing and Computational Natural Language Learning, 2012.\nM. Drton, B. Sturmfels, and S. Sullivant. Algebraic factor analysis: tetrads, pentads and\nbeyond. Probability Theory and Related Fields, 138(3):463\u2013493, 2007.\n2829\nAnandkumar, Ge, Hsu, Kakade, and Telgarsky\nA. T. Erdogan. On the convergence of ICA algorithms with symmetric orthogonalization.\nIEEE Transactions on Signal Processing, 57:2209\u20132221, 2009.\nA. M. Frieze, M. Jerrum, and R. Kannan. Learning linear transformations. In Thirty-\nSeventh Annual Symposium on Foundations of Computer Science, pages 359\u2013368, 1996.\nG. H. Golub and C. F. van Loan. Matrix Computations. Johns Hopkins University Press,\n1996.\nN. Halko, P.-G. Martinsson, and J. A. Tropp. Finding structure with randomness: Prob-\nabilistic algorithms for constructing approximate matrix decompositions. SIAM Review,\n53(2), 2011.\nR. Harshman.\nFoundations of the PARAFAC procedure: model and conditions for an\n\u2018explanatory\u2019 multi-mode factor analysis. Technical report, UCLA Working Papers in\nPhonetics, 1970.\nC. J. Hillar and L.-H. Lim. Most tensor problems are NP-hard. J. ACM, 60(6):45:1\u201345:39,\nNovember 2013. ISSN 0004-5411. doi: 10.1145/2512329.\nF. L. Hitchcock. The expression of a tensor or a polyadic as a sum of products. Journal of\nMathematics and Physics, 6:164\u2013189, 1927a.\nF. L. Hitchcock.\nMultiple invariants and generalized rank of a p-way matrix or tensor.\nJournal of Mathematics and Physics, 7:39\u201379, 1927b.\nD. Hsu and S. M. Kakade. Learning mixtures of spherical Gaussians: moment methods and\nspectral decompositions. In Fourth Innovations in Theoretical Computer Science, 2013.\nD. Hsu, S. M. Kakade, and P. Liang. Identi\ufb01ability and unmixing of latent parse trees. In\nAdvances in Neural Information Processing Systems 25, 2012a.\nD. Hsu, S. M. Kakade, and T. Zhang. A spectral algorithm for learning hidden Markov\nmodels. Journal of Computer and System Sciences, 78(5):1460\u20131480, 2012b.\nA. Hyvarinen. Fast and robust \ufb01xed-point algorithms for independent component analysis.\nNeural Networks, IEEE Transactions on, 10(3):626\u2013634, 1999.\nA. Hyv\u00a8arinen and E. Oja. Independent component analysis: algorithms and applications.\nNeural Networks, 13(4\u20135):411\u2013430, 2000.\nH. Jaeger. Observable operator models for discrete stochastic time series. Neural Comput.,\n12(6), 2000.\nA. T. Kalai, A. Moitra, and G. Valiant. E\ufb03ciently learning mixtures of two Gaussians. In\nForty-second ACM Symposium on Theory of Computing, pages 553\u2013562, 2010.\nR. Kannan, H. Salmasian, and S. Vempala. The spectral method for general mixture models.\nSIAM Journal on Computing, 38(3):1141\u20131156, 2008.\n2830\nTensor Decompositions for Learning Latent Variable Models\nE. Ko\ufb01dis and P. A. Regalia. On the best rank-1 approximation of higher-order super-\nsymmetric tensors. SIAM Journal on Matrix Analysis and Applications, 23(3):863\u2013884,\n2002.\nT. G. Kolda and B. W. Bader. Tensor decompositions and applications. SIAM review, 51\n(3):455, 2009.\nT. G. Kolda and J. R. Mayo. Shifted power method for computing tensor eigenpairs. SIAM\nJournal on Matrix Analysis and Applications, 32(4):1095\u20131124, October 2011.\nJ. B. Kruskal. Three-way arrays: rank and uniqueness of trilinear decompositions, with\napplication to arithmetic complexity and statistics.\nLinear Algebra and Appl., 18(2):\n95\u2013138, 1977.\nL. D. Lathauwer, B. D. Moor, and J. Vandewalle.\nOn the best rank-1 and rank-\n(R1, R2, ..., Rn) approximation and applications of higher-order tensors. SIAM J. Matrix\nAnal. Appl., 21(4):1324\u20131342, 2000.\nL. Le Cam. Asymptotic Methods in Statistical Decision Theory. Springer, 1986.\nL.-H. Lim. Singular values and eigenvalues of tensors: a variational approach. Proceedings of\nthe IEEE International Workshop on Computational Advances in Multi-Sensor Adaptive\nProcessing, 1:129\u2013132, 2005.\nM. Littman, R. Sutton, and S. Singh. Predictive representations of state. In Advances in\nNeural Information Processing Systems 14, pages 1555\u20131561, 2001.\nF. M. Luque, A. Quattoni, B. Balle, and X. Carreras. Spectral learning for non-deterministic\ndependency parsing. In Conference of the European Chapter of the Association for Com-\nputational Linguistics, 2012.\nJ. B. MacQueen.\nSome methods for classi\ufb01cation and analysis of multivariate observa-\ntions. In Proceedings of the Fifth Berkeley Symposium on Mathematical Statistics and\nProbability, volume 1, pages 281\u2013297. University of California Press, 1967.\nP. McCullagh. Tensor Methods in Statistics. Chapman and Hall, 1987.\nA. Moitra and G. Valiant. Settling the polynomial learnability of mixtures of Gaussians. In\nFifty-First Annual IEEE Symposium on Foundations of Computer Science, pages 93\u2013102,\n2010.\nE. Mossel and S. Roch.\nLearning nonsingular phylogenies and hidden Markov models.\nAnnals of Applied Probability, 16(2):583\u2013614, 2006.\nP. Q. Nguyen and O. Regev. Learning a parallelepiped: Cryptanalysis of GGH and NTRU\nsignatures. Journal of Cryptology, 22(2):139\u2013160, 2009.\nJ. Nocedal and S. J. Wright. Numerical Optimization. Springer, 1999.\nP. V. Overschee and B. D. Moor. Subspace Identi\ufb01cation of Linear Systems. Kluwer Aca-\ndemic Publishers, 1996.\n2831\nAnandkumar, Ge, Hsu, Kakade, and Telgarsky\nL. Pachter and B. Sturmfels. Algebraic Statistics for Computational Biology, volume 13.\nCambridge University Press, 2005.\nA. Parikh, L. Song, and E. P. Xing. A spectral algorithm for latent tree graphical models.\nIn Twenty-Eighth International Conference on Machine Learning, 2011.\nK. Pearson. Contributions to the mathematical theory of evolution. Philosophical Trans-\nactions of the Royal Society, London, A., page 71, 1894.\nL. Qi. Eigenvalues of a real supersymmetric tensor. Journal of Symbolic Computation, 40\n(6):1302\u20131324, 2005.\nR. A. Redner and H. F. Walker.\nMixture densities, maximum likelihood and the EM\nalgorithm. SIAM Review, 26(2):195\u2013239, 1984.\nP. A. Regalia and E. Ko\ufb01dis. Monotonic convergence of \ufb01xed-point algorithms for ICA.\nIEEE Transactions on Neural Networks, 14:943\u2013949, 2003.\nS. Roch. A short proof that phylogenetic tree reconstruction by maximum likelihood is\nhard. IEEE/ACM Trans. Comput. Biol. Bioinformatics, 3(1), 2006.\nJ. Rodu, D. P. Foster, W. Wu, and L. H. Ungar. Using regression for spectral estimation\nof HMMs. In Statistical Language and Speech Processing, pages 212\u2013223, 2013.\nM. P. Sch\u00a8utzenberger. On the de\ufb01nition of a family of automata. Inf. Control, 4:245\u2013270,\n1961.\nS. M. Siddiqi, B. Boots, and G. J. Gordon.\nReduced-rank hidden Markov models.\nIn\nThirteenth International Conference on Arti\ufb01cial Intelligence and Statistics, 2010.\nD. A. Spielman and S. H. Teng. Smoothed analysis: An attempt to explain the behavior of\nalgorithms in practice. Communications of the ACM, pages 76\u201384, 2009.\nA. Stegeman and P. Comon. Subtracting a best rank-1 approximation may increase tensor\nrank. Linear Algebra and Its Applications, 433:1276\u20131300, 2010.\nB. Sturmfels and P. Zwiernik. Binary cumulant varieties. Ann. Comb., (17):229\u2013250, 2013.\nS. Vempala and G. Wang. A spectral algorithm for learning mixtures models. Journal of\nComputer and System Sciences, 68(4):841\u2013860, 2004.\nP. Wedin.\nPerturbation bounds in connection with singular value decomposition.\nBIT\nNumerical Mathematics, 12(1):99\u2013111, 1972.\nT. Zhang and G. Golub. Rank-one approximation to high order tensors. SIAM Journal on\nMatrix Analysis and Applications, 23:534\u2013550, 2001.\nA. Ziehe, P. Laskov, G. Nolte, and K. R. M\u00a8uller. A fast algorithm for joint diagonaliza-\ntion with non-orthogonal transformations and its application to blind source separation.\nJournal of Machine Learning Research, 5:777\u2013800, 2004.\nJ. Zou, D. Hsu, D. Parkes, and R. P. Adams. Contrastive learning using spectral methods.\nIn Advances in Neural Information Processing Systems 26, 2013.\n2832\n",
        "sentence": "",
        "context": "Kannan et al., 2008; Achlioptas and McSherry, 2005; Chaudhuri and Rao, 2008; Brubaker and Vempala,\n2008; Kalai et al., 2010; Belkin and Sinha, 2010; Moitra and Valiant, 2010; Hsu and Kakade,\nRegalia and Ko\ufb01dis, 2003; Erdogan, 2009; Kolda and Mayo, 2011) may be advantageous.\nAcknowledgments\nWe thank Boaz Barak, Dean Foster, Jon Kelner, and Greg Valiant for helpful discussions.\n2013; Chang, 1996; Mossel and Roch, 2006; Hsu et al., 2012b; Anandkumar et al., 2012c;\nArora et al., 2012a; Anandkumar et al., 2012a). See the works by Anandkumar et al. (2012c)"
    },
    {
        "title": "Spectral learning of general weighted automata via constrained matrix completion",
        "author": [
            "B. Balle",
            "M. Mohri"
        ],
        "venue": "In Advances in Neural Information Processing Systems (NIPS),",
        "citeRegEx": "Balle and Mohri,? \\Q2012\\E",
        "shortCiteRegEx": "Balle and Mohri",
        "year": 2012,
        "abstract": "",
        "full_text": "",
        "sentence": " First, we bound the error in the compound parameters estimates M\u03022, M\u03023 using results from Tomioka et al. (2011). Then we use results from Anandkumar et al.",
        "context": null
    },
    {
        "title": "A spectral learning algorithm for finite state transducers",
        "author": [
            "B. Balle",
            "A. Quattoni",
            "X. Carreras"
        ],
        "venue": "In European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases,",
        "citeRegEx": "Balle et al\\.,? \\Q2011\\E",
        "shortCiteRegEx": "Balle et al\\.",
        "year": 2011,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " One line of work has focused on observable operator models (Hsu et al., 2009; Song et al., 2010; Parikh et al., 2012; Cohen et al., 2012; Balle et al., 2011; Balle & Mohri, 2012) in which a re-parametrization of the true parameters are recovered, which suffices for prediction and density estimation. Other work has focused on learning discriminative models, notably Balle et al. (2011) for finite state transducers (functions from strings to strings), and Balle & Mohri (2012) for weighted finite state automata (functions from strings to real numbers). Other work has focused on learning discriminative models, notably Balle et al. (2011) for finite state transducers (functions from strings to strings), and Balle & Mohri (2012) for weighted finite state automata (functions from strings to real numbers). Other work has focused on learning discriminative models, notably Balle et al. (2011) for finite state transducers (functions from strings to strings), and Balle & Mohri (2012) for weighted finite state automata (functions from strings to real numbers). Similar to Spectral Experts, Balle & Mohri (2012) used a two-step approach, where convex optimization is first used to estimate moments (the Hankel matrix in their case), after which these moments are subjected to spectral decomposition.",
        "context": null
    },
    {
        "title": "Learning mixtures of spher",
        "author": [
            "D. Hsu",
            "S.M. Kakade"
        ],
        "venue": "http://cvxr.com/cvx,",
        "citeRegEx": "2",
        "shortCiteRegEx": "2",
        "year": 2012,
        "abstract": "",
        "full_text": "",
        "sentence": " In this section, we will bound the error of the compound parameter estimates \u2016\u22062\u2016F and \u2016\u22063\u2016F , where \u22062 def = M\u03022 \u2212 M2 and \u22063 def = M\u03023 \u2212 M3. Our analysis is based on the low-rank regression framework of Tomioka et al. (2011) for tensors, which builds on Negahban & Wainwright (2009) for matrices. In this section, we will bound the error of the compound parameter estimates \u2016\u22062\u2016F and \u2016\u22063\u2016F , where \u22062 def = M\u03022 \u2212 M2 and \u22063 def = M\u03023 \u2212 M3. Our analysis is based on the low-rank regression framework of Tomioka et al. (2011) for tensors, which builds on Negahban & Wainwright (2009) for matrices.",
        "context": null
    },
    {
        "title": "Modeling with mix",
        "author": [
            "Viele",
            "Kert",
            "Tong",
            "Barbara"
        ],
        "venue": null,
        "citeRegEx": "Viele et al\\.,? \\Q2011\\E",
        "shortCiteRegEx": "Viele et al\\.",
        "year": 2011,
        "abstract": "",
        "full_text": "",
        "sentence": " Lemma 1 (Tomioka et al. (2011), Theorem 1).",
        "context": null
    },
    {
        "title": "\u03b7p(x)x \u2297p, where we have used \u03b7p to represent the vector [\u03b7p(x)]x\u2208Dp",
        "author": [
            "Tomioka"
        ],
        "venue": null,
        "citeRegEx": "Tomioka,? \\Q2011\\E",
        "shortCiteRegEx": "Tomioka",
        "year": 2011,
        "abstract": "",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "2012c, Theorem 5.1) to bound the error in the eigenvalues",
        "author": [
            "Anandkumar"
        ],
        "venue": null,
        "citeRegEx": "Anandkumar,? \\Q2012\\E",
        "shortCiteRegEx": "Anandkumar",
        "year": 2012,
        "abstract": "",
        "full_text": "",
        "sentence": "",
        "context": null
    }
]