[
    {
        "title": "Automatic dialog act segmentation and classification in multiparty meetings",
        "author": [
            "Ang et al.2005] Jeremy Ang",
            "Yang Liu",
            "Elizabeth Shriberg"
        ],
        "venue": null,
        "citeRegEx": "Ang et al\\.,? \\Q2005\\E",
        "shortCiteRegEx": "Ang et al\\.",
        "year": 2005,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " , 2000), maximum entropy (Ang et al., 2005), and naive Bayes (Lendvai and Geertzen, 2007). The 5 classes are introduced in (Ang et al., 2005).",
        "context": null
    },
    {
        "title": "A convolutional neural network for modelling sentences",
        "author": [
            "Blunsom et al.2014] Phil Blunsom",
            "Edward Grefenstette",
            "Nal Kalchbrenner"
        ],
        "venue": "In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics. Proceedings of the 52nd",
        "citeRegEx": "Blunsom et al\\.,? \\Q2014\\E",
        "shortCiteRegEx": "Blunsom et al\\.",
        "year": 2014,
        "abstract": "The ability to accurately represent sentences is central to language\nunderstanding. We describe a convolutional architecture dubbed the Dynamic\nConvolutional Neural Network (DCNN) that we adopt for the semantic modelling of\nsentences. The network uses Dynamic k-Max Pooling, a global pooling operation\nover linear sequences. The network handles input sentences of varying length\nand induces a feature graph over the sentence that is capable of explicitly\ncapturing short and long-range relations. The network does not rely on a parse\ntree and is easily applicable to any language. We test the DCNN in four\nexperiments: small scale binary and multi-class sentiment prediction, six-way\nquestion classification and Twitter sentiment prediction by distant\nsupervision. The network achieves excellent performance in the first three\ntasks and a greater than 25% error reduction in the last task with respect to\nthe strongest baseline.",
        "full_text": "A Convolutional Neural Network for Modelling Sentences\nNal Kalchbrenner\nEdward Grefenstette\n{nal.kalchbrenner, edward.grefenstette, phil.blunsom}@cs.ox.ac.uk\nDepartment of Computer Science\nUniversity of Oxford\nPhil Blunsom\nAbstract\nThe ability to accurately represent sen-\ntences is central to language understand-\ning. We describe a convolutional architec-\nture dubbed the Dynamic Convolutional\nNeural Network (DCNN) that we adopt\nfor the semantic modelling of sentences.\nThe network uses Dynamic k-Max Pool-\ning, a global pooling operation over lin-\near sequences. The network handles input\nsentences of varying length and induces\na feature graph over the sentence that is\ncapable of explicitly capturing short and\nlong-range relations.\nThe network does\nnot rely on a parse tree and is easily ap-\nplicable to any language.\nWe test the\nDCNN in four experiments: small scale\nbinary and multi-class sentiment predic-\ntion, six-way question classi\ufb01cation and\nTwitter sentiment prediction by distant su-\npervision. The network achieves excellent\nperformance in the \ufb01rst three tasks and a\ngreater than 25% error reduction in the last\ntask with respect to the strongest baseline.\n1\nIntroduction\nThe aim of a sentence model is to analyse and\nrepresent the semantic content of a sentence for\npurposes of classi\ufb01cation or generation. The sen-\ntence modelling problem is at the core of many\ntasks involving a degree of natural language com-\nprehension. These tasks include sentiment analy-\nsis, paraphrase detection, entailment recognition,\nsummarisation, discourse analysis, machine trans-\nlation, grounded language learning and image re-\ntrieval. Since individual sentences are rarely ob-\nserved or not observed at all, one must represent\na sentence in terms of features that depend on the\nwords and short n-grams in the sentence that are\nfrequently observed. The core of a sentence model\ninvolves a feature function that de\ufb01nes the process\n The  cat  sat  on  the  red  mat\n The  cat  sat  on  the  red  mat\nFigure 1: Subgraph of a feature graph induced\nover an input sentence in a Dynamic Convolu-\ntional Neural Network.\nThe full induced graph\nhas multiple subgraphs of this kind with a distinct\nset of edges; subgraphs may merge at different\nlayers. The left diagram emphasises the pooled\nnodes. The width of the convolutional \ufb01lters is 3\nand 2 respectively. With dynamic pooling, a \ufb01l-\nter with small width at the higher layers can relate\nphrases far apart in the input sentence.\nby which the features of the sentence are extracted\nfrom the features of the words or n-grams.\nVarious types of models of meaning have been\nproposed. Composition based methods have been\napplied to vector representations of word meaning\nobtained from co-occurrence statistics to obtain\nvectors for longer phrases. In some cases, com-\nposition is de\ufb01ned by algebraic operations over\nword meaning vectors to produce sentence mean-\ning vectors (Erk and Pad\u00b4o, 2008; Mitchell and\nLapata, 2008; Mitchell and Lapata, 2010; Tur-\nney, 2012; Erk, 2012; Clarke, 2012).\nIn other\ncases, a composition function is learned and ei-\nther tied to particular syntactic relations (Guevara,\n2010; Zanzotto et al., 2010) or to particular word\ntypes (Baroni and Zamparelli, 2010; Coecke et\nal., 2010; Grefenstette and Sadrzadeh, 2011; Kart-\nsaklis and Sadrzadeh, 2013; Grefenstette, 2013).\nAnother approach represents the meaning of sen-\ntences by way of automatically extracted logical\nforms (Zettlemoyer and Collins, 2005).\narXiv:1404.2188v1  [cs.CL]  8 Apr 2014\nA central class of models are those based on\nneural networks.\nThese range from basic neu-\nral bag-of-words or bag-of-n-grams models to the\nmore structured recursive neural networks and\nto time-delay neural networks based on convo-\nlutional operations (Collobert and Weston, 2008;\nSocher et al., 2011; Kalchbrenner and Blunsom,\n2013b).\nNeural sentence models have a num-\nber of advantages. They can be trained to obtain\ngeneric vectors for words and phrases by predict-\ning, for instance, the contexts in which the words\nand phrases occur. Through supervised training,\nneural sentence models can \ufb01ne-tune these vec-\ntors to information that is speci\ufb01c to a certain\ntask. Besides comprising powerful classi\ufb01ers as\npart of their architecture, neural sentence models\ncan be used to condition a neural language model\nto generate sentences word by word (Schwenk,\n2012; Mikolov and Zweig, 2012; Kalchbrenner\nand Blunsom, 2013a).\nWe de\ufb01ne a convolutional neural network archi-\ntecture and apply it to the semantic modelling of\nsentences. The network handles input sequences\nof varying length. The layers in the network in-\nterleave one-dimensional convolutional layers and\ndynamic k-max pooling layers. Dynamic k-max\npooling is a generalisation of the max pooling op-\nerator. The max pooling operator is a non-linear\nsubsampling function that returns the maximum\nof a set of values (LeCun et al., 1998). The op-\nerator is generalised in two respects.\nFirst, k-\nmax pooling over a linear sequence of values re-\nturns the subsequence of k maximum values in the\nsequence, instead of the single maximum value.\nSecondly, the pooling parameter k can be dynam-\nically chosen by making k a function of other as-\npects of the network or the input.\nThe\nconvolutional\nlayers\napply\none-\ndimensional \ufb01lters across each row of features in\nthe sentence matrix. Convolving the same \ufb01lter\nwith the n-gram at every position in the sentence\nallows the features to be extracted independently\nof their position in the sentence. A convolutional\nlayer followed by a dynamic pooling layer and\na non-linearity form a feature map. Like in the\nconvolutional networks for object recognition\n(LeCun et al., 1998), we enrich the representation\nin the \ufb01rst layer by computing multiple feature\nmaps with different \ufb01lters applied to the input\nsentence.\nSubsequent layers also have multiple\nfeature maps computed by convolving \ufb01lters with\nall the maps from the layer below. The weights at\nthese layers form an order-4 tensor. The resulting\narchitecture is dubbed a Dynamic Convolutional\nNeural Network.\nMultiple layers of convolutional and dynamic\npooling operations induce a structured feature\ngraph over the input sentence. Figure 1 illustrates\nsuch a graph. Small \ufb01lters at higher layers can cap-\nture syntactic or semantic relations between non-\ncontinuous phrases that are far apart in the input\nsentence. The feature graph induces a hierarchical\nstructure somewhat akin to that in a syntactic parse\ntree. The structure is not tied to purely syntactic\nrelations and is internal to the neural network.\nWe experiment with the network in four set-\ntings. The \ufb01rst two experiments involve predict-\ning the sentiment of movie reviews (Socher et\nal., 2013b). The network outperforms other ap-\nproaches in both the binary and the multi-class ex-\nperiments. The third experiment involves the cat-\negorisation of questions in six question types in\nthe TREC dataset (Li and Roth, 2002). The net-\nwork matches the accuracy of other state-of-the-\nart methods that are based on large sets of en-\ngineered features and hand-coded knowledge re-\nsources. The fourth experiment involves predict-\ning the sentiment of Twitter posts using distant su-\npervision (Go et al., 2009). The network is trained\non 1.6 million tweets labelled automatically ac-\ncording to the emoticon that occurs in them. On\nthe hand-labelled test set, the network achieves a\ngreater than 25% reduction in the prediction error\nwith respect to the strongest unigram and bigram\nbaseline reported in Go et al. (2009).\nThe outline of the paper is as follows. Section 2\ndescribes the background to the DCNN including\ncentral concepts and related neural sentence mod-\nels. Section 3 de\ufb01nes the relevant operators and\nthe layers of the network. Section 4 treats of the\ninduced feature graph and other properties of the\nnetwork. Section 5 discusses the experiments and\ninspects the learnt feature detectors.1\n2\nBackground\nThe layers of the DCNN are formed by a convo-\nlution operation followed by a pooling operation.\nWe begin with a review of related neural sentence\nmodels. Then we describe the operation of one-\ndimensional convolution and the classical Time-\nDelay Neural Network (TDNN) (Hinton, 1989;\nWaibel et al., 1990). By adding a max pooling\n1Code available at www.nal.co\nlayer to the network, the TDNN can be adopted as\na sentence model (Collobert and Weston, 2008).\n2.1\nRelated Neural Sentence Models\nVarious neural sentence models have been de-\nscribed. A general class of basic sentence models\nis that of Neural Bag-of-Words (NBoW) models.\nThese generally consist of a projection layer that\nmaps words, sub-word units or n-grams to high\ndimensional embeddings; the latter are then com-\nbined component-wise with an operation such as\nsummation. The resulting combined vector is clas-\nsi\ufb01ed through one or more fully connected layers.\nA model that adopts a more general structure\nprovided by an external parse tree is the Recursive\nNeural Network (RecNN) (Pollack, 1990; K\u00a8uchler\nand Goller, 1996; Socher et al., 2011; Hermann\nand Blunsom, 2013). At every node in the tree the\ncontexts at the left and right children of the node\nare combined by a classical layer. The weights of\nthe layer are shared across all nodes in the tree.\nThe layer computed at the top node gives a repre-\nsentation for the sentence. The Recurrent Neural\nNetwork (RNN) is a special case of the recursive\nnetwork where the structure that is followed is a\nsimple linear chain (Gers and Schmidhuber, 2001;\nMikolov et al., 2011). The RNN is primarily used\nas a language model, but may also be viewed as a\nsentence model with a linear structure. The layer\ncomputed at the last word represents the sentence.\nFinally, a further class of neural sentence mod-\nels is based on the convolution operation and the\nTDNN architecture (Collobert and Weston, 2008;\nKalchbrenner and Blunsom, 2013b). Certain con-\ncepts used in these models are central to the\nDCNN and we describe them next.\n2.2\nConvolution\nThe one-dimensional convolution is an operation\nbetween a vector of weights m \u2208Rm and a vector\nof inputs viewed as a sequence s \u2208Rs. The vector\nm is the \ufb01lter of the convolution. Concretely, we\nthink of s as the input sentence and si \u2208R is a sin-\ngle feature value associated with the i-th word in\nthe sentence. The idea behind the one-dimensional\nconvolution is to take the dot product of the vector\nm with each m-gram in the sentence s to obtain\nanother sequence c:\ncj = m\u22basj\u2212m+1:j\n(1)\nEquation 1 gives rise to two types of convolution\ndepending on the range of the index j. The narrow\ntype of convolution requires that s \u2265m and yields\ns1\ns1\nss\nss\nc1\nc5\nc5\nFigure 2: Narrow and wide types of convolution.\nThe \ufb01lter m has size m = 5.\na sequence c \u2208Rs\u2212m+1 with j ranging from m\nto s. The wide type of convolution does not have\nrequirements on s or m and yields a sequence c \u2208\nRs+m\u22121 where the index j ranges from 1 to s +\nm \u22121. Out-of-range input values si where i < 1\nor i > s are taken to be zero. The result of the\nnarrow convolution is a subsequence of the result\nof the wide convolution. The two types of one-\ndimensional convolution are illustrated in Fig. 2.\nThe trained weights in the \ufb01lter m correspond\nto a linguistic feature detector that learns to recog-\nnise a speci\ufb01c class of n-grams. These n-grams\nhave size n \u2264m, where m is the width of the\n\ufb01lter. Applying the weights m in a wide convo-\nlution has some advantages over applying them in\na narrow one. A wide convolution ensures that all\nweights in the \ufb01lter reach the entire sentence, in-\ncluding the words at the margins. This is particu-\nlarly signi\ufb01cant when m is set to a relatively large\nvalue such as 8 or 10. In addition, a wide convo-\nlution guarantees that the application of the \ufb01lter\nm to the input sentence s always produces a valid\nnon-empty result c, independently of the width m\nand the sentence length s. We next describe the\nclassical convolutional layer of a TDNN.\n2.3\nTime-Delay Neural Networks\nA TDNN convolves a sequence of inputs s with a\nset of weights m. As in the TDNN for phoneme\nrecognition (Waibel et al., 1990), the sequence s\nis viewed as having a time dimension and the con-\nvolution is applied over the time dimension. Each\nsj is often not just a single value, but a vector of\nd values so that s \u2208Rd\u00d7s. Likewise, m is a ma-\ntrix of weights of size d \u00d7 m. Each row of m is\nconvolved with the corresponding row of s and the\nconvolution is usually of the narrow type. Multi-\nple convolutional layers may be stacked by taking\nthe resulting sequence c as input to the next layer.\nThe Max-TDNN sentence model is based on the\narchitecture of a TDNN (Collobert and Weston,\n2008). In the model, a convolutional layer of the\nnarrow type is applied to the sentence matrix s,\nwhere each column corresponds to the feature vec-\ntor wi \u2208Rd of a word in the sentence:\ns =\n\uf8ee\n\uf8f0w1\n. . .\nws\n\uf8f9\n\uf8fb\n(2)\nTo address the problem of varying sentence\nlengths, the Max-TDNN takes the maximum of\neach row in the resulting matrix c yielding a vector\nof d values:\ncmax =\n\uf8ee\n\uf8ef\uf8f0\nmax(c1,:)\n...\nmax(cd,:)\n\uf8f9\n\uf8fa\uf8fb\n(3)\nThe aim is to capture the most relevant feature, i.e.\nthe one with the highest value, for each of the d\nrows of the resulting matrix c. The \ufb01xed-sized\nvector cmax is then used as input to a fully con-\nnected layer for classi\ufb01cation.\nThe Max-TDNN model has many desirable\nproperties. It is sensitive to the order of the words\nin the sentence and it does not depend on external\nlanguage-speci\ufb01c features such as dependency or\nconstituency parse trees. It also gives largely uni-\nform importance to the signal coming from each\nof the words in the sentence, with the exception\nof words at the margins that are considered fewer\ntimes in the computation of the narrow convolu-\ntion. But the model also has some limiting as-\npects. The range of the feature detectors is lim-\nited to the span m of the weights. Increasing m or\nstacking multiple convolutional layers of the nar-\nrow type makes the range of the feature detectors\nlarger; at the same time it also exacerbates the ne-\nglect of the margins of the sentence and increases\nthe minimum size s of the input sentence required\nby the convolution. For this reason higher-order\nand long-range feature detectors cannot be easily\nincorporated into the model. The max pooling op-\neration has some disadvantages too. It cannot dis-\ntinguish whether a relevant feature in one of the\nrows occurs just one or multiple times and it for-\ngets the order in which the features occur. More\ngenerally, the pooling factor by which the signal\nof the matrix is reduced at once corresponds to\ns\u2212m+1; even for moderate values of s the pool-\ning factor can be excessive. The aim of the next\nsection is to address these limitations while pre-\nserving the advantages.\n3\nConvolutional Neural Networks with\nDynamic k-Max Pooling\nWe model sentences using a convolutional archi-\ntecture that alternates wide convolutional layers\nK-Max pooling\n(k=3)\nFully connected \nlayer\nFolding\nWide\nconvolution\n(m=2)\nDynamic\nk-max pooling\n (k= f(s) =5)\n Projected\nsentence \nmatrix\n(s=7)\nWide\nconvolution\n(m=3)\n The cat sat on the red mat\nFigure 3: A DCNN for the seven word input sen-\ntence. Word embeddings have size d = 4. The\nnetwork has two convolutional layers with two\nfeature maps each. The widths of the \ufb01lters at the\ntwo layers are respectively 3 and 2. The (dynamic)\nk-max pooling layers have values k of 5 and 3.\nwith dynamic pooling layers given by dynamic k-\nmax pooling. In the network the width of a feature\nmap at an intermediate layer varies depending on\nthe length of the input sentence; the resulting ar-\nchitecture is the Dynamic Convolutional Neural\nNetwork. Figure 3 represents a DCNN. We pro-\nceed to describe the network in detail.\n3.1\nWide Convolution\nGiven an input sentence, to obtain the \ufb01rst layer of\nthe DCNN we take the embedding wi \u2208Rd for\neach word in the sentence and construct the sen-\ntence matrix s \u2208Rd\u00d7s as in Eq. 2. The values\nin the embeddings wi are parameters that are op-\ntimised during training. A convolutional layer in\nthe network is obtained by convolving a matrix of\nweights m \u2208Rd\u00d7m with the matrix of activations\nat the layer below. For example, the second layer\nis obtained by applying a convolution to the sen-\ntence matrix s itself. Dimension d and \ufb01lter width\nm are hyper-parameters of the network. We let the\noperations be wide one-dimensional convolutions\nas described in Sect. 2.2. The resulting matrix c\nhas dimensions d \u00d7 (s + m \u22121).\n3.2\nk-Max Pooling\nWe next describe a pooling operation that is a gen-\neralisation of the max pooling over the time di-\nmension used in the Max-TDNN sentence model\nand different from the local max pooling opera-\ntions applied in a convolutional network for object\nrecognition (LeCun et al., 1998). Given a value\nk and a sequence p \u2208Rp of length p \u2265k, k-\nmax pooling selects the subsequence pk\nmax of the\nk highest values of p. The order of the values in\npk\nmax corresponds to their original order in p.\nThe k-max pooling operation makes it possible\nto pool the k most active features in p that may be\na number of positions apart; it preserves the order\nof the features, but is insensitive to their speci\ufb01c\npositions. It can also discern more \ufb01nely the num-\nber of times the feature is highly activated in p\nand the progression by which the high activations\nof the feature change across p. The k-max pooling\noperator is applied in the network after the topmost\nconvolutional layer. This guarantees that the input\nto the fully connected layers is independent of the\nlength of the input sentence. But, as we see next, at\nintermediate convolutional layers the pooling pa-\nrameter k is not \ufb01xed, but is dynamically selected\nin order to allow for a smooth extraction of higher-\norder and longer-range features.\n3.3\nDynamic k-Max Pooling\nA dynamic k-max pooling operation is a k-max\npooling operation where we let k be a function of\nthe length of the sentence and the depth of the net-\nwork. Although many functions are possible, we\nsimply model the pooling parameter as follows:\nkl = max( ktop, \u2308L \u2212l\nL\ns\u2309)\n(4)\nwhere l is the number of the current convolutional\nlayer to which the pooling is applied and L is the\ntotal number of convolutional layers in the net-\nwork; ktop is the \ufb01xed pooling parameter for the\ntopmost convolutional layer (Sect. 3.2). For in-\nstance, in a network with three convolutional lay-\ners and ktop = 3, for an input sentence of length\ns = 18, the pooling parameter at the \ufb01rst layer\nis k1 = 12 and the pooling parameter at the sec-\nond layer is k2 = 6; the third layer has the \ufb01xed\npooling parameter k3 = ktop = 3. Equation 4\nis a model of the number of values needed to de-\nscribe the relevant parts of the progression of an\nl-th order feature over a sentence of length s. For\nan example in sentiment prediction, according to\nthe equation a \ufb01rst order feature such as a posi-\ntive word occurs at most k1 times in a sentence of\nlength s, whereas a second order feature such as a\nnegated phrase or clause occurs at most k2 times.\n3.4\nNon-linear Feature Function\nAfter (dynamic) k-max pooling is applied to the\nresult of a convolution, a bias b \u2208Rd and a non-\nlinear function g are applied component-wise to\nthe pooled matrix. There is a single bias value for\neach row of the pooled matrix.\nIf we temporarily ignore the pooling layer, we\nmay state how one computes each d-dimensional\ncolumn a in the matrix a resulting after the convo-\nlutional and non-linear layers. De\ufb01ne M to be the\nmatrix of diagonals:\nM = [diag(m:,1), . . . , diag(m:,m)]\n(5)\nwhere m are the weights of the d \ufb01lters of the wide\nconvolution. Then after the \ufb01rst pair of a convolu-\ntional and a non-linear layer, each column a in the\nmatrix a is obtained as follows, for some index j:\na = g\n\uf8eb\n\uf8ec\n\uf8edM\n\uf8ee\n\uf8ef\uf8f0\nwj\n...\nwj+m\u22121\n\uf8f9\n\uf8fa\uf8fb+ b\n\uf8f6\n\uf8f7\n\uf8f8\n(6)\nHere a is a column of \ufb01rst order features. Sec-\nond order features are similarly obtained by ap-\nplying Eq. 6 to a sequence of \ufb01rst order features\naj, ..., aj+m\u2032\u22121 with another weight matrix M\u2032.\nBarring pooling, Eq. 6 represents a core aspect\nof the feature extraction function and has a rather\ngeneral form that we return to below. Together\nwith pooling, the feature function induces position\ninvariance and makes the range of higher-order\nfeatures variable.\n3.5\nMultiple Feature Maps\nSo far we have described how one applies a wide\nconvolution, a (dynamic) k-max pooling layer and\na non-linear function to the input sentence ma-\ntrix to obtain a \ufb01rst order feature map. The three\noperations can be repeated to yield feature maps\nof increasing order and a network of increasing\ndepth. We denote a feature map of the i-th order\nby Fi. As in convolutional networks for object\nrecognition, to increase the number of learnt fea-\nture detectors of a certain order, multiple feature\nmaps Fi\n1, . . . , Fi\nn may be computed in parallel at\nthe same layer. Each feature map Fi\nj is computed\nby convolving a distinct set of \ufb01lters arranged in\na matrix mi\nj,k with each feature map Fi\u22121\nk\nof the\nlower order i \u22121 and summing the results:\nFi\nj =\nn\nX\nk=1\nmi\nj,k \u2217Fi\u22121\nk\n(7)\nwhere \u2217indicates the wide convolution.\nThe\nweights mi\nj,k form an order-4 tensor. After the\nwide convolution, \ufb01rst dynamic k-max pooling\nand then the non-linear function are applied indi-\nvidually to each map.\n3.6\nFolding\nIn the formulation of the network so far, feature\ndetectors applied to an individual row of the sen-\ntence matrix s can have many orders and create\ncomplex dependencies across the same rows in\nmultiple feature maps. Feature detectors in differ-\nent rows, however, are independent of each other\nuntil the top fully connected layer. Full depen-\ndence between different rows could be achieved\nby making M in Eq. 5 a full matrix instead of\na sparse matrix of diagonals. Here we explore a\nsimpler method called folding that does not intro-\nduce any additional parameters. After a convo-\nlutional layer and before (dynamic) k-max pool-\ning, one just sums every two rows in a feature map\ncomponent-wise. For a map of d rows, folding re-\nturns a map of d/2 rows, thus halving the size of\nthe representation. With a folding layer, a feature\ndetector of the i-th order depends now on two rows\nof feature values in the lower maps of order i \u22121.\nThis ends the description of the DCNN.\n4\nProperties of the Sentence Model\nWe describe some of the properties of the sentence\nmodel based on the DCNN. We describe the no-\ntion of the feature graph induced over a sentence\nby the succession of convolutional and pooling\nlayers. We brie\ufb02y relate the properties to those of\nother neural sentence models.\n4.1\nWord and n-Gram Order\nOne of the basic properties is sensitivity to the or-\nder of the words in the input sentence. For most\napplications and in order to learn \ufb01ne-grained fea-\nture detectors, it is bene\ufb01cial for a model to be able\nto discriminate whether a speci\ufb01c n-gram occurs\nin the input. Likewise, it is bene\ufb01cial for a model\nto be able to tell the relative position of the most\nrelevant n-grams. The network is designed to cap-\nture these two aspects. The \ufb01lters m of the wide\nconvolution in the \ufb01rst layer can learn to recognise\nspeci\ufb01c n-grams that have size less or equal to the\n\ufb01lter width m; as we see in the experiments, m in\nthe \ufb01rst layer is often set to a relatively large value\nsuch as 10. The subsequence of n-grams extracted\nby the generalised pooling operation induces in-\nvariance to absolute positions, but maintains their\norder and relative positions.\nAs regards the other neural sentence models, the\nclass of NBoW models is by de\ufb01nition insensitive\nto word order. A sentence model based on a recur-\nrent neural network is sensitive to word order, but\nit has a bias towards the latest words that it takes as\ninput (Mikolov et al., 2011). This gives the RNN\nexcellent performance at language modelling, but\nit is suboptimal for remembering at once the n-\ngrams further back in the input sentence. Sim-\nilarly, a recursive neural network is sensitive to\nword order but has a bias towards the topmost\nnodes in the tree; shallower trees mitigate this ef-\nfect to some extent (Socher et al., 2013a). As seen\nin Sect. 2.3, the Max-TDNN is sensitive to word\norder, but max pooling only picks out a single n-\ngram feature in each row of the sentence matrix.\n4.2\nInduced Feature Graph\nSome sentence models use internal or external\nstructure to compute the representation for the in-\nput sentence.\nIn a DCNN, the convolution and\npooling layers induce an internal feature graph\nover the input. A node from a layer is connected\nto a node from the next higher layer if the lower\nnode is involved in the convolution that computes\nthe value of the higher node. Nodes that are not\nselected by the pooling operation at a layer are\ndropped from the graph. After the last pooling\nlayer, the remaining nodes connect to a single top-\nmost root. The induced graph is a connected, di-\nrected acyclic graph with weighted edges and a\nroot node; two equivalent representations of an\ninduced graph are given in Fig. 1. In a DCNN\nwithout folding layers, each of the d rows of the\nsentence matrix induces a subgraph that joins the\nother subgraphs only at the root node. Each sub-\ngraph may have a different shape that re\ufb02ects the\nkind of relations that are detected in that subgraph.\nThe effect of folding layers is to join pairs of sub-\ngraphs at lower layers before the top root node.\nConvolutional networks for object recognition\nalso induce a feature graph over the input image.\nWhat makes the feature graph of a DCNN pecu-\nliar is the global range of the pooling operations.\nThe (dynamic) k-max pooling operator can draw\ntogether features that correspond to words that are\nmany positions apart in the sentence. Higher-order\nfeatures have highly variable ranges that can be ei-\nther short and focused or global and long as the\ninput sentence. Likewise, the edges of a subgraph\nin the induced graph re\ufb02ect these varying ranges.\nThe subgraphs can either be localised to one or\nmore parts of the sentence or spread more widely\nacross the sentence. This structure is internal to\nthe network and is de\ufb01ned by the forward propa-\ngation of the input through the network.\nOf the other sentence models, the NBoW is a\nshallow model and the RNN has a linear chain\nstructure.\nThe subgraphs induced in the Max-\nTDNN model have a single \ufb01xed-range feature ob-\ntained through max pooling. The recursive neural\nnetwork follows the structure of an external parse\ntree. Features of variable range are computed at\neach node of the tree combining one or more of\nthe children of the tree. Unlike in a DCNN, where\none learns a clear hierarchy of feature orders, in\na RecNN low order features like those of sin-\ngle words can be directly combined with higher\norder features computed from entire clauses. A\nDCNN generalises many of the structural aspects\nof a RecNN. The feature extraction function as\nstated in Eq. 6 has a more general form than that\nin a RecNN, where the value of m is generally 2.\nLikewise, the induced graph structure in a DCNN\nis more general than a parse tree in that it is not\nlimited to syntactically dictated phrases; the graph\nstructure can capture short or long-range seman-\ntic relations between words that do not necessar-\nily correspond to the syntactic relations in a parse\ntree.\nThe DCNN has internal input-dependent\nstructure and does not rely on externally provided\nparse trees, which makes the DCNN directly ap-\nplicable to hard-to-parse sentences such as tweets\nand to sentences from any language.\n5\nExperiments\nWe test the network on four different experiments.\nWe begin by specifying aspects of the implemen-\ntation and the training of the network. We then re-\nlate the results of the experiments and we inspect\nthe learnt feature detectors.\n5.1\nTraining\nIn each of the experiments, the top layer of the\nnetwork has a fully connected layer followed by\na softmax non-linearity that predicts the probabil-\nity distribution over classes given the input sen-\ntence.\nThe network is trained to minimise the\ncross-entropy of the predicted and true distribu-\ntions; the objective includes an L2 regularisation\nClassi\ufb01er\nFine-grained (%)\nBinary (%)\nNB\n41.0\n81.8\nBINB\n41.9\n83.1\nSVM\n40.7\n79.4\nRECNTN\n45.7\n85.4\nMAX-TDNN\n37.4\n77.1\nNBOW\n42.4\n80.5\nDCNN\n48.5\n86.8\nTable 1: Accuracy of sentiment prediction in the\nmovie reviews dataset. The \ufb01rst four results are\nreported from Socher et al. (2013b). The baselines\nNB and BINB are Naive Bayes classi\ufb01ers with,\nrespectively, unigram features and unigram and bi-\ngram features. SVM is a support vector machine\nwith unigram and bigram features. RECNTN is a\nrecursive neural network with a tensor-based fea-\nture function, which relies on external structural\nfeatures given by a parse tree and performs best\namong the RecNNs.\nterm over the parameters. The set of parameters\ncomprises the word embeddings, the \ufb01lter weights\nand the weights from the fully connected layers.\nThe network is trained with mini-batches by back-\npropagation and the gradient-based optimisation is\nperformed using the Adagrad update rule (Duchi\net al., 2011). Using the well-known convolution\ntheorem, we can compute fast one-dimensional\nlinear convolutions at all rows of an input matrix\nby using Fast Fourier Transforms. To exploit the\nparallelism of the operations, we train the network\non a GPU. A Matlab implementation processes\nmultiple millions of input sentences per hour on\none GPU, depending primarily on the number of\nlayers used in the network.\n5.2\nSentiment Prediction in Movie Reviews\nThe \ufb01rst two experiments concern the prediction\nof the sentiment of movie reviews in the Stanford\nSentiment Treebank (Socher et al., 2013b). The\noutput variable is binary in one experiment and\ncan have \ufb01ve possible outcomes in the other: neg-\native, somewhat negative, neutral, somewhat posi-\ntive, positive. In the binary case, we use the given\nsplits of 6920 training, 872 development and 1821\ntest sentences. Likewise, in the \ufb01ne-grained case,\nwe use the standard 8544/1101/2210 splits. La-\nbelled phrases that occur as subparts of the train-\ning sentences are treated as independent training\ninstances. The size of the vocabulary is 15448.\nTable 1 details the results of the experiments.\nClassi\ufb01er\nFeatures\nAcc. (%)\nHIER\nunigram, POS, head chunks\n91.0\nNE, semantic relations\nMAXENT\nunigram, bigram, trigram\n92.6\nPOS, chunks, NE, supertags\nCCG parser, WordNet\nMAXENT\nunigram, bigram, trigram\n93.6\nPOS, wh-word, head word\nword shape, parser\nhypernyms, WordNet\nSVM\nunigram, POS, wh-word\n95.0\nhead word, parser\nhypernyms, WordNet\n60 hand-coded rules\nMAX-TDNN\nunsupervised vectors\n84.4\nNBOW\nunsupervised vectors\n88.2\nDCNN\nunsupervised vectors\n93.0\nTable 2: Accuracy of six-way question classi\ufb01ca-\ntion on the TREC questions dataset. The second\ncolumn details the external features used in the\nvarious approaches. The \ufb01rst four results are re-\nspectively from Li and Roth (2002), Blunsom et al.\n(2006), Huang et al. (2008) and Silva et al. (2011).\nIn the three neural sentence models\u2014the Max-\nTDNN, the NBoW and the DCNN\u2014the word vec-\ntors are parameters of the models that are ran-\ndomly initialised; their dimension d is set to 48.\nThe Max-TDNN has a \ufb01lter of width 6 in its nar-\nrow convolution at the \ufb01rst layer; shorter phrases\nare padded with zero vectors.\nThe convolu-\ntional layer is followed by a non-linearity, a max-\npooling layer and a softmax classi\ufb01cation layer.\nThe NBoW sums the word vectors and applies a\nnon-linearity followed by a softmax classi\ufb01cation\nlayer. The adopted non-linearity is the tanh func-\ntion. The hyper parameters of the DCNN are as\nfollows. The binary result is based on a DCNN\nthat has a wide convolutional layer followed by a\nfolding layer, a dynamic k-max pooling layer and\na non-linearity; it has a second wide convolutional\nlayer followed by a folding layer, a k-max pooling\nlayer and a non-linearity. The width of the convo-\nlutional \ufb01lters is 7 and 5, respectively. The value\nof k for the top k-max pooling is 4. The num-\nber of feature maps at the \ufb01rst convolutional layer\nis 6; the number of maps at the second convolu-\ntional layer is 14. The network is topped by a soft-\nmax classi\ufb01cation layer. The DCNN for the \ufb01ne-\ngrained result has the same architecture, but the\n\ufb01lters have size 10 and 7, the top pooling parame-\nter k is 5 and the number of maps is, respectively,\n6 and 12. The networks use the tanh non-linear\nClassi\ufb01er\nAccuracy (%)\nSVM\n81.6\nBINB\n82.7\nMAXENT\n83.0\nMAX-TDNN\n78.8\nNBOW\n80.9\nDCNN\n87.4\nTable 3:\nAccuracy on the Twitter sentiment\ndataset. The three non-neural classi\ufb01ers are based\non unigram and bigram features; the results are re-\nported from (Go et al., 2009).\nfunction. At training time we apply dropout to the\npenultimate layer after the last tanh non-linearity\n(Hinton et al., 2012).\nWe see that the DCNN signi\ufb01cantly outper-\nforms the other neural and non-neural models.\nThe NBoW performs similarly to the non-neural\nn-gram based classi\ufb01ers.\nThe Max-TDNN per-\nforms worse than the NBoW likely due to the ex-\ncessive pooling of the max pooling operation; the\nlatter discards most of the sentiment features of the\nwords in the input sentence. Besides the RecNN\nthat uses an external parser to produce structural\nfeatures for the model, the other models use n-\ngram based or neural features that do not require\nexternal resources or additional annotations. In the\nnext experiment we compare the performance of\nthe DCNN with those of methods that use heavily\nengineered resources.\n5.3\nQuestion Type Classi\ufb01cation\nAs an aid to question answering, a question may\nbe classi\ufb01ed as belonging to one of many question\ntypes. The TREC questions dataset involves six\ndifferent question types, e.g. whether the question\nis about a location, about a person or about some\nnumeric information (Li and Roth, 2002).\nThe\ntraining dataset consists of 5452 labelled questions\nwhereas the test dataset consists of 500 questions.\nThe results are reported in Tab. 2. The non-\nneural approaches use a classi\ufb01er over a large\nnumber of manually engineered features and\nhand-coded resources. For instance, Blunsom et\nal. (2006) present a Maximum Entropy model that\nrelies on 26 sets of syntactic and semantic fea-\ntures including unigrams, bigrams, trigrams, POS\ntags, named entity tags, structural relations from\na CCG parse and WordNet synsets. We evaluate\nthe three neural models on this dataset with mostly\nthe same hyper-parameters as in the binary senti-\nPOSITIVE\nlovely\t \t \t \t \t comedic\t \t \t \t \t moments\t and\t \t \t \t several\t \t \t \t \t fine\t \t \t \t \t \t performances\ngood\t \t \t \t \t \t \t script\t \t \t \t \t \t ,\t \t \t \t \t \t \t good\t \t \t dialogue\t \t \t \t ,\t \t \t \t \t \t \t \t \t funny\t \t \t \t \t \t \t \nsustains\t \t \t throughout\t \t is\t \t \t \t \t \t daring\t ,\t \t \t \t \t \t \t \t \t \t \t inventive\t and\t \t \t \t \t \t \t \t \t \nwell\t \t \t \t \t \t \t written\t \t \t \t \t ,\t \t \t \t \t \t \t nicely\t acted\t \t \t \t \t \t \t and\t \t \t \t \t \t \t beautifully\t \nremarkably\t solid\t \t \t \t \t \t \t and\t \t \t \t \t subtly\t satirical\t \t \t tour\t \t \t \t \t \t de\t \t \t \t \t \t \t \t \t \t \nNEGATIVE\n,\t \t \t \t \t \t \t \t \t \t nonexistent\t plot\t \t \t \t and\t \t \t \t pretentious\t visual\t \t \t \t style\t \t \t \t \t \t \t \nit\t \t \t \t \t \t \t \t \t fails\t \t \t \t \t \t \t the\t \t \t \t \t most\t \t \t basic\t \t \t \t \t \t \t test\t \t \t \t \t \t as\t \t \t \t \t \t \t \t \t \t \nso\t \t \t \t \t \t \t \t \t stupid\t \t \t \t \t \t ,\t \t \t \t \t \t \t so\t \t \t \t \t ill\t \t \t \t \t \t \t \t \t conceived\t ,\t \t \t \t \t \t \t \t \t \t \t \n,\t \t \t \t \t \t \t \t \t \t too\t \t \t \t \t \t \t \t \t dull\t \t \t \t and\t \t \t \t pretentious\t to\t \t \t \t \t \t \t \t be\t \t \t \t \t \t \t \t \t \t \nhood\t \t \t \t \t \t \t rats\t \t \t \t \t \t \t \t butt\t \t \t \t their\t \t ugly\t \t \t \t \t \t \t \t heads\t \t \t \t \t in\t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \n'NOT'\nn't\t \t \t \t have\t \t \t \t \t any\t \t \t \t \t \t \t \t \t huge\t laughs\t \t \t \t \t \t in\t \t \t \t \t \t \t \t \t \t \t its\t \t \t \nno\t \t \t \t \t movement\t ,\t \t \t \t \t \t \t \t \t \t \t no\t \t \t ,\t \t \t \t \t \t \t \t \t \t \t not\t \t \t \t \t \t \t \t \t \t much\t \t \nn't\t \t \t \t stop\t \t \t \t \t me\t \t \t \t \t \t \t \t \t \t from\t enjoying\t \t \t \t much\t \t \t \t \t \t \t \t \t of\t \t \t \t \nnot\t \t \t \t that\t \t \t \t \t kung\t \t \t \t \t \t \t \t pow\t \t is\t \t \t \t \t \t \t \t \t \t n't\t \t \t \t \t \t \t \t \t \t funny\t \nnot\t \t \t \t a\t \t \t \t \t \t \t \t moment\t \t \t \t \t \t that\t is\t \t \t \t \t \t \t \t \t \t not\t \t \t \t \t \t \t \t \t \t false\t \n'TOO'\n,\t \t \t \t \t \t too\t \t \t \t \t \t dull\t \t \t \t \t \t \t \t and\t \t pretentious\t to\t \t \t \t \t \t \t \t \t \t \t be\t \t \t \t \t \t \t \t \neither\t too\t \t \t \t \t \t serious\t \t \t \t \t or\t \t \t too\t \t \t \t \t \t \t \t \t lighthearted\t ,\t \t \t \t \t \t \t \t \t \ntoo\t \t \t \t slow\t \t \t \t \t ,\t \t \t \t \t \t \t \t \t \t \t too\t \t long\t \t \t \t \t \t \t \t and\t \t \t \t \t \t \t \t \t \t too\t \t \t \t \t \t \t \nfeels\t \t too\t \t \t \t \t \t formulaic\t \t \t and\t \t too\t \t \t \t \t \t \t \t \t familiar\t \t \t \t \t to\t \t \t \t \t \t \t \t \nis\t \t \t \t \t too\t \t \t \t \t \t predictable\t and\t \t too\t \t \t \t \t \t \t \t \t self\t \t \t \t \t \t \t \t \t conscious\t \t \nFigure 4: Top \ufb01ve 7-grams at four feature detectors in the \ufb01rst layer of the network.\nment experiment of Sect. 5.2. As the dataset is\nrather small, we use lower-dimensional word vec-\ntors with d = 32 that are initialised with embed-\ndings trained in an unsupervised way to predict\ncontexts of occurrence (Turian et al., 2010). The\nDCNN uses a single convolutional layer with \ufb01l-\nters of size 8 and 5 feature maps. The difference\nbetween the performance of the DCNN and that of\nthe other high-performing methods in Tab. 2 is not\nsigni\ufb01cant (p < 0.09). Given that the only labelled\ninformation used to train the network is the train-\ning set itself, it is notable that the network matches\nthe performance of state-of-the-art classi\ufb01ers that\nrely on large amounts of engineered features and\nrules and hand-coded resources.\n5.4\nTwitter Sentiment Prediction with\nDistant Supervision\nIn our \ufb01nal experiment, we train the models on a\nlarge dataset of tweets, where a tweet is automat-\nically labelled as positive or negative depending\non the emoticon that occurs in it. The training set\nconsists of 1.6 million tweets with emoticon-based\nlabels and the test set of about 400 hand-annotated\ntweets. We preprocess the tweets minimally fol-\nlowing the procedure described in Go et al. (2009);\nin addition, we also lowercase all the tokens. This\nresults in a vocabulary of 76643 word types. The\narchitecture of the DCNN and of the other neural\nmodels is the same as the one used in the binary\nexperiment of Sect. 5.2. The randomly initialised\nword embeddings are increased in length to a di-\nmension of d = 60. Table 3 reports the results of\nthe experiments. We see a signi\ufb01cant increase in\nthe performance of the DCNN with respect to the\nnon-neural n-gram based classi\ufb01ers; in the pres-\nence of large amounts of training data these clas-\nsi\ufb01ers constitute particularly strong baselines. We\nsee that the ability to train a sentiment classi\ufb01er on\nautomatically extracted emoticon-based labels ex-\ntends to the DCNN and results in highly accurate\nperformance. The difference in performance be-\ntween the DCNN and the NBoW further suggests\nthat the ability of the DCNN to both capture fea-\ntures based on long n-grams and to hierarchically\ncombine these features is highly bene\ufb01cial.\n5.5\nVisualising Feature Detectors\nA \ufb01lter in the DCNN is associated with a feature\ndetector or neuron that learns during training to\nbe particularly active when presented with a spe-\nci\ufb01c sequence of input words. In the \ufb01rst layer, the\nsequence is a continuous n-gram from the input\nsentence; in higher layers, sequences can be made\nof multiple separate n-grams.\nWe visualise the\nfeature detectors in the \ufb01rst layer of the network\ntrained on the binary sentiment task (Sect. 5.2).\nSince the \ufb01lters have width 7, for each of the 288\nfeature detectors we rank all 7-grams occurring in\nthe validation and test sets according to their ac-\ntivation of the detector. Figure 5.2 presents the\ntop \ufb01ve 7-grams for four feature detectors. Be-\nsides the expected detectors for positive and nega-\ntive sentiment, we \ufb01nd detectors for particles such\nas \u2018not\u2019 that negate sentiment and such as \u2018too\u2019\nthat potentiate sentiment. We \ufb01nd detectors for\nmultiple other notable constructs including \u2018all\u2019,\n\u2018or\u2019, \u2018with...that\u2019, \u2018as...as\u2019. The feature detectors\nlearn to recognise not just single n-grams, but pat-\nterns within n-grams that have syntactic, semantic\nor structural signi\ufb01cance.\n6\nConclusion\nWe have described a dynamic convolutional neural\nnetwork that uses the dynamic k-max pooling op-\nerator as a non-linear subsampling function. The\nfeature graph induced by the network is able to\ncapture word relations of varying size. The net-\nwork achieves high performance on question and\nsentiment classi\ufb01cation without requiring external\nfeatures as provided by parsers or other resources.\nAcknowledgements\nWe thank Nando de Freitas and Yee Whye Teh\nfor great discussions on the paper. This work was\nsupported by a Xerox Foundation Award, EPSRC\ngrant number EP/F042728/1, and EPSRC grant\nnumber EP/K036580/1.\nReferences\nMarco Baroni and Roberto Zamparelli. 2010. Nouns\nare vectors, adjectives are matrices: Representing\nadjective-noun constructions in semantic space. In\nEMNLP, pages 1183\u20131193. ACL.\nPhil Blunsom, Krystle Kocik, and James R. Curran.\n2006. Question classi\ufb01cation with log-linear mod-\nels.\nIn SIGIR \u201906: Proceedings of the 29th an-\nnual international ACM SIGIR conference on Re-\nsearch and development in information retrieval,\npages 615\u2013616, New York, NY, USA. ACM.\nDaoud Clarke.\n2012.\nA context-theoretic frame-\nwork for compositionality in distributional seman-\ntics. Computational Linguistics, 38(1):41\u201371.\nBob Coecke, Mehrnoosh Sadrzadeh, and Stephen\nClark. 2010. Mathematical Foundations for a Com-\npositional Distributional Model of Meaning. March.\nRonan Collobert and Jason Weston. 2008. A uni\ufb01ed\narchitecture for natural language processing: Deep\nneural networks with multitask learning. In Interna-\ntional Conference on Machine Learning, ICML.\nJohn Duchi, Elad Hazan, and Yoram Singer.\n2011.\nAdaptive subgradient methods for online learning\nand stochastic optimization. J. Mach. Learn. Res.,\n12:2121\u20132159, July.\nKatrin Erk and Sebastian Pad\u00b4o. 2008. A structured\nvector space model for word meaning in context.\nProceedings of the Conference on Empirical Meth-\nods in Natural Language Processing - EMNLP \u201908,\n(October):897.\nKatrin Erk. 2012. Vector space models of word mean-\ning and phrase meaning: A survey. Language and\nLinguistics Compass, 6(10):635\u2013653.\nFelix A. Gers and Jrgen Schmidhuber.\n2001.\nLstm\nrecurrent networks learn simple context-free and\ncontext-sensitive languages. IEEE Transactions on\nNeural Networks, 12(6):1333\u20131340.\nAlec Go, Richa Bhayani, and Lei Huang. 2009. Twit-\nter sentiment classi\ufb01cation using distant supervision.\nProcessing, pages 1\u20136.\nEdward Grefenstette and Mehrnoosh Sadrzadeh. 2011.\nExperimental support for a categorical composi-\ntional distributional model of meaning. In Proceed-\nings of the Conference on Empirical Methods in Nat-\nural Language Processing, pages 1394\u20131404. Asso-\nciation for Computational Linguistics.\nEdward Grefenstette.\n2013.\nCategory-theoretic\nquantitative compositional distributional models\nof natural language semantics.\narXiv preprint\narXiv:1311.1539.\nEmiliano Guevara. 2010. Modelling Adjective-Noun\nCompositionality by Regression. ESSLLI\u201910 Work-\nshop on Compositionality and Distributional Se-\nmantic Models.\nKarl Moritz Hermann and Phil Blunsom. 2013. The\nRole of Syntax in Vector Space Models of Composi-\ntional Semantics. In Proceedings of the 51st Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), So\ufb01a, Bulgaria,\nAugust. Association for Computational Linguistics.\nForthcoming.\nGeoffrey\nE.\nHinton,\nNitish\nSrivastava,\nAlex\nKrizhevsky, Ilya Sutskever, and Ruslan Salakhut-\ndinov.\n2012.\nImproving neural networks by\npreventing\nco-adaptation\nof\nfeature\ndetectors.\nCoRR, abs/1207.0580.\nGeoffrey E. Hinton. 1989. Connectionist learning pro-\ncedures. Artif. Intell., 40(1-3):185\u2013234.\nZhiheng Huang, Marcus Thint, and Zengchang Qin.\n2008. Question classi\ufb01cation using head words and\ntheir hypernyms. In Proceedings of the Conference\non Empirical Methods in Natural Language Pro-\ncessing, EMNLP \u201908, pages 927\u2013936, Stroudsburg,\nPA, USA. Association for Computational Linguis-\ntics.\nNal Kalchbrenner and Phil Blunsom. 2013a. Recur-\nrent continuous translation models. In Proceedings\nof the 2013 Conference on Empirical Methods in\nNatural Language Processing, Seattle, October. As-\nsociation for Computational Linguistics.\nNal Kalchbrenner and Phil Blunsom. 2013b. Recur-\nrent Convolutional Neural Networks for Discourse\nCompositionality. In Proceedings of the Workshop\non Continuous Vector Space Models and their Com-\npositionality, So\ufb01a, Bulgaria, August. Association\nfor Computational Linguistics.\nDimitri Kartsaklis and Mehrnoosh Sadrzadeh. 2013.\nPrior disambiguation of word tensors for construct-\ning sentence vectors.\nIn Proceedings of the 2013\nConference on Empirical Methods in Natural Lan-\nguage Processing (EMNLP), Seattle, USA, October.\nAndreas K\u00a8uchler and Christoph Goller. 1996. Induc-\ntive learning in symbolic domains using structure-\ndriven recurrent neural networks. In G\u00a8unther G\u00a8orz\nand Steffen H\u00a8olldobler, editors, KI, volume 1137 of\nLecture Notes in Computer Science, pages 183\u2013197.\nSpringer.\nYann LeCun, L\u00b4eon Bottou, Yoshua Bengio, and Patrick\nHaffner. 1998. Gradient-based learning applied to\ndocument recognition.\nProceedings of the IEEE,\n86(11):2278\u20132324, November.\nXin Li and Dan Roth. 2002. Learning question clas-\nsi\ufb01ers.\nIn Proceedings of the 19th international\nconference on Computational linguistics-Volume 1,\npages 1\u20137. Association for Computational Linguis-\ntics.\nTomas Mikolov and Geoffrey Zweig. 2012. Context\ndependent recurrent neural network language model.\nIn SLT, pages 234\u2013239.\nTomas Mikolov, Stefan Kombrink, Lukas Burget, Jan\nCernock\u00b4y, and Sanjeev Khudanpur. 2011. Exten-\nsions of recurrent neural network language model.\nIn ICASSP, pages 5528\u20135531. IEEE.\nJeff Mitchell and Mirella Lapata. 2008. Vector-based\nmodels of semantic composition. In Proceedings of\nACL, volume 8.\nJeff Mitchell and Mirella Lapata. 2010. Composition\nin distributional models of semantics. Cognitive Sci-\nence, 34(8):1388\u20131429.\nJordan B. Pollack. 1990. Recursive distributed repre-\nsentations. Arti\ufb01cial Intelligence, 46:77\u2013105.\nHolger Schwenk. 2012. Continuous space translation\nmodels for phrase-based statistical machine transla-\ntion. In COLING (Posters), pages 1071\u20131080.\nJoo Silva, Lusa Coheur, AnaCristina Mendes, and An-\ndreas Wichert.\n2011.\nFrom symbolic to sub-\nsymbolic information in question classi\ufb01cation. Ar-\nti\ufb01cial Intelligence Review, 35(2):137\u2013154.\nRichard Socher, Jeffrey Pennington, Eric H. Huang,\nAndrew Y. Ng, and Christopher D. Manning. 2011.\nSemi-Supervised Recursive Autoencoders for Pre-\ndicting Sentiment Distributions. In Proceedings of\nthe 2011 Conference on Empirical Methods in Nat-\nural Language Processing (EMNLP).\nRichard Socher, Quoc V. Le, Christopher D. Manning,\nand Andrew Y. Ng.\n2013a.\nGrounded Composi-\ntional Semantics for Finding and Describing Images\nwith Sentences. In Transactions of the Association\nfor Computational Linguistics (TACL).\nRichard Socher, Alex Perelygin, Jean Wu, Jason\nChuang, Christopher D. Manning, Andrew Y. Ng,\nand Christopher Potts. 2013b. Recursive deep mod-\nels for semantic compositionality over a sentiment\ntreebank. In Proceedings of the 2013 Conference on\nEmpirical Methods in Natural Language Process-\ning, pages 1631\u20131642, Stroudsburg, PA, October.\nAssociation for Computational Linguistics.\nJoseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.\nWord representations: a simple and general method\nfor semi-supervised learning. In Proceedings of the\n48th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 384\u2013394. Association for\nComputational Linguistics.\nPeter Turney. 2012. Domain and function: A dual-\nspace model of semantic relations and compositions.\nJ. Artif. Intell. Res.(JAIR), 44:533\u2013585.\nAlexander Waibel, Toshiyuki Hanazawa, Geofrey Hin-\nton, Kiyohiro Shikano, and Kevin J. Lang. 1990.\nReadings in speech recognition. chapter Phoneme\nRecognition Using Time-delay Neural Networks,\npages 393\u2013404. Morgan Kaufmann Publishers Inc.,\nSan Francisco, CA, USA.\nFabio\nMassimo\nZanzotto,\nIoannis\nKorkontzelos,\nFrancesca Fallucchi, and Suresh Manandhar. 2010.\nEstimating linear models for compositional distri-\nbutional semantics. In Proceedings of the 23rd In-\nternational Conference on Computational Linguis-\ntics, pages 1263\u20131271. Association for Computa-\ntional Linguistics.\nLuke S. Zettlemoyer and Michael Collins.\n2005.\nLearning to map sentences to logical form: Struc-\ntured classi\ufb01cation with probabilistic categorial\ngrammars. In UAI, pages 658\u2013666. AUAI Press.\n",
        "sentence": " Several recent studies using ANNs have shown promising results, including convolutional neural networks (CNNs) (Kim, 2014; Blunsom et al., 2014; Kalchbrenner et al., 2014) and recursive neural networks (Socher et al.",
        "context": "to time-delay neural networks based on convo-\nlutional operations (Collobert and Weston, 2008;\nSocher et al., 2011; Kalchbrenner and Blunsom,\n2013b).\nNeural sentence models have a num-\nber of advantages. They can be trained to obtain\nNeural Network.\nMultiple layers of convolutional and dynamic\npooling operations induce a structured feature\ngraph over the input sentence. Figure 1 illustrates\nsuch a graph. Small \ufb01lters at higher layers can cap-\nNetwork (RNN) is a special case of the recursive\nnetwork where the structure that is followed is a\nsimple linear chain (Gers and Schmidhuber, 2001;\nMikolov et al., 2011). The RNN is primarily used\nas a language model, but may also be viewed as a"
    },
    {
        "title": "On the properties of neural machine translation: Encoderdecoder approaches",
        "author": [
            "Cho et al.2014] Kyunghyun Cho",
            "Bart van Merri\u00ebnboer",
            "Dzmitry Bahdanau",
            "Yoshua Bengio"
        ],
        "venue": "arXiv preprint arXiv:1409.1259",
        "citeRegEx": "Cho et al\\.,? \\Q2014\\E",
        "shortCiteRegEx": "Cho et al\\.",
        "year": 2014,
        "abstract": "",
        "full_text": "",
        "sentence": " We also tried a variant of the LSTM model, gated recurrent units (Cho et al., 2014), but the results were generally lower than LSTM.",
        "context": null
    },
    {
        "title": "Natural language processing (almost) from scratch",
        "author": [
            "Jason Weston",
            "L\u00e9on Bottou",
            "Michael Karlen",
            "Koray Kavukcuoglu",
            "Pavel Kuksa"
        ],
        "venue": "The Journal of Machine Learning Research,",
        "citeRegEx": "Collobert et al\\.,? \\Q2011\\E",
        "shortCiteRegEx": "Collobert et al\\.",
        "year": 2011,
        "abstract": "We propose a unified neural network architecture and learning algorithm that\ncan be applied to various natural language processing tasks including:\npart-of-speech tagging, chunking, named entity recognition, and semantic role\nlabeling. This versatility is achieved by trying to avoid task-specific\nengineering and therefore disregarding a lot of prior knowledge. Instead of\nexploiting man-made input features carefully optimized for each task, our\nsystem learns internal representations on the basis of vast amounts of mostly\nunlabeled training data. This work is then used as a basis for building a\nfreely available tagging system with good performance and minimal computational\nrequirements.",
        "full_text": "arXiv\narXiv\nNatural Language Processing (almost) from Scratch\nRonan Collobert\nronan@collobert.com\nNEC Labs America, Princeton NJ.\nJason Weston\njweston@google.com\nGoogle, New York, NY.\nL\u00b4eon Bottou\nleon@bottou.org\nMichael Karlen\nmichael.karlen@gmail.com\nKoray Kavukcuoglu\u2020\nkoray@cs.nyu.edu\nPavel Kuksa\u2021\npkuksa@cs.rutgers.edu\nNEC Labs America, Princeton NJ.\nAbstract\nWe propose a uni\ufb01ed neural network architecture and learning algorithm that can be applied\nto various natural language processing tasks including: part-of-speech tagging, chunking,\nnamed entity recognition, and semantic role labeling. This versatility is achieved by trying\nto avoid task-speci\ufb01c engineering and therefore disregarding a lot of prior knowledge.\nInstead of exploiting man-made input features carefully optimized for each task, our system\nlearns internal representations on the basis of vast amounts of mostly unlabeled training\ndata. This work is then used as a basis for building a freely available tagging system with\ngood performance and minimal computational requirements.\nKeywords:\nNatural Language Processing, Neural Networks\n1. Introduction\nWill a computer program ever be able to convert a piece of English text into a data structure\nthat unambiguously and completely describes the meaning of the natural language text?\nAmong numerous problems, no consensus has emerged about the form of such a data\nstructure. Until such fundamental Arti\ufb01cial Intelligence problems are resolved, computer\nscientists must settle for reduced objectives: extracting simpler representations describing\nrestricted aspects of the textual information.\nThese simpler representations are often motivated by speci\ufb01c applications, for instance,\nbag-of-words variants for information retrieval. These representations can also be motivated\nby our belief that they capture something more general about natural language.\nThey\ncan describe syntactic information (e.g. part-of-speech tagging, chunking, and parsing) or\nsemantic information (e.g. word-sense disambiguation, semantic role labeling, named entity\nextraction, and anaphora resolution). Text corpora have been manually annotated with such\ndata structures in order to compare the performance of various systems. The availability of\nstandard benchmarks has stimulated research in Natural Language Processing (NLP) and\n\u2020. Koray Kavukcuoglu is also with New York University, New York, NY.\n\u2021. Pavel Kuksa is also with Rutgers University, New Brunswick, NJ.\nc\u20dd2009 Ronan Collobert, Jason Weston, L\u00b4eon Bottou, Michael Karlen, Koray Kavukcuoglu and Pavel Kuksa.\narXiv:1103.0398v1  [cs.LG]  2 Mar 2011\narXiv\nCollobert, Weston, Bottou, Karlen, Kavukcuoglu and Kuksa\ne\ufb00ective systems have been designed for all these tasks. Such systems are often viewed as\nsoftware components for constructing real-world NLP solutions.\nThe overwhelming majority of these state-of-the-art systems address a benchmark\ntask by applying linear statistical models to ad-hoc features.\nIn other words, the\nresearchers themselves discover intermediate representations by engineering task-speci\ufb01c\nfeatures. These features are often derived from the output of preexisting systems, leading\nto complex runtime dependencies. This approach is e\ufb00ective because researchers leverage\na large body of linguistic knowledge. On the other hand, there is a great temptation to\noptimize the performance of a system for a speci\ufb01c benchmark. Although such performance\nimprovements can be very useful in practice, they teach us little about the means to progress\ntoward the broader goals of natural language understanding and the elusive goals of Arti\ufb01cial\nIntelligence.\nIn this contribution, we try to excel on multiple benchmarks while avoiding task-speci\ufb01c\nenginering.\nInstead we use a single learning system able to discover adequate internal\nrepresentations. In fact we view the benchmarks as indirect measurements of the relevance\nof the internal representations discovered by the learning procedure, and we posit that these\nintermediate representations are more general than any of the benchmarks. Our desire to\navoid task-speci\ufb01c engineered features led us to ignore a large body of linguistic knowledge.\nInstead we reach good performance levels in most of the tasks by transferring intermediate\nrepresentations discovered on large unlabeled datasets. We call this approach \u201calmost from\nscratch\u201d to emphasize the reduced (but still important) reliance on a priori NLP knowledge.\nThe paper is organized as follows.\nSection 2 describes the benchmark tasks of\ninterest. Section 3 describes the uni\ufb01ed model and reports benchmark results obtained with\nsupervised training. Section 4 leverages large unlabeled datasets (\u223c852 million words)\nto train the model on a language modeling task.\nPerformance improvements are then\ndemonstrated by transferring the unsupervised internal representations into the supervised\nbenchmark models. Section 5 investigates multitask supervised training. Section 6 then\nevaluates how much further improvement can be achieved by incorporating standard NLP\ntask-speci\ufb01c engineering into our systems. Drifting away from our initial goals gives us the\nopportunity to construct an all-purpose tagger that is simultaneously accurate, practical,\nand fast. We then conclude with a short discussion section.\n2. The Benchmark Tasks\nIn this section, we brie\ufb02y introduce four standard NLP tasks on which we will benchmark\nour architectures within this paper: Part-Of-Speech tagging (POS), chunking (CHUNK),\nNamed Entity Recognition (NER) and Semantic Role Labeling (SRL). For each of them,\nwe consider a standard experimental setup and give an overview of state-of-the-art systems\non this setup. The experimental setups are summarized in Table 1, while state-of-the-art\nsystems are reported in Table 2.\n2.1 Part-Of-Speech Tagging\nPOS aims at labeling each word with a unique tag that indicates its syntactic role, e.g.\nplural noun, adverb, . . . A standard benchmark setup is described in detail by Toutanova\n2\narXiv\nNatural Language Processing (almost) from Scratch\nTask\nBenchmark\nDataset\nTraining set\nTest set\n(#tokens)\n(#tokens)\n(#tags)\nPOS\nToutanova et al. (2003)\nWSJ\nsections 0\u201318\nsections 22\u201324\n( 45 )\n( 912,344 )\n( 129,654 )\nChunking\nCoNLL 2000\nWSJ\nsections 15\u201318\nsection 20\n( 42 )\n( 211,727 )\n( 47,377 )\n(IOBES)\nNER\nCoNLL 2003\nReuters\n\u201ceng.train\u201d\n\u201ceng.testb\u201d\n( 17 )\n( 203,621 )\n( 46,435 )\n(IOBES)\nSRL\nCoNLL 2005\nWSJ\nsections 2\u201321\nsection 23\n( 186 )\n( 950,028 )\n+ 3 Brown sections\n(IOBES)\n( 63,843 )\nTable 1:\nExperimental setup: for each task, we report the standard benchmark we used,\nthe dataset it relates to, as well as training and test information.\nSystem\nAccuracy\nShen et al. (2007)\n97.33%\nToutanova et al. (2003)\n97.24%\nGim\u00b4enez and M`arquez (2004)\n97.16%\n(a) POS\nSystem\nF1\nShen and Sarkar (2005)\n95.23%\nSha and Pereira (2003)\n94.29%\nKudo and Matsumoto (2001)\n93.91%\n(b) CHUNK\nSystem\nF1\nAndo and Zhang (2005)\n89.31%\nFlorian et al. (2003)\n88.76%\nKudo and Matsumoto (2001)\n88.31%\n(c) NER\nSystem\nF1\nKoomen et al. (2005)\n77.92%\nPradhan et al. (2005)\n77.30%\nHaghighi et al. (2005)\n77.04%\n(d) SRL\nTable 2: State-of-the-art systems on four NLP tasks. Performance is reported in per-word\naccuracy for POS, and F1 score for CHUNK, NER and SRL. Systems in bold will be referred\nas benchmark systems in the rest of the paper (see text).\net al. (2003). Sections 0\u201318 of Wall Street Journal (WSJ) data are used for training, while\nsections 19\u201321 are for validation and sections 22\u201324 for testing.\nThe best POS classi\ufb01ers are based on classi\ufb01ers trained on windows of text, which are\nthen fed to a bidirectional decoding algorithm during inference. Features include preceding\nand following tag context as well as multiple words (bigrams, trigrams. . . ) context, and\nhandcrafted features to deal with unknown words.\nToutanova et al. (2003), who use\nmaximum entropy classi\ufb01ers, and a bidirectional dependency network (Heckerman et al.,\n2001) at inference, reach 97.24% per-word accuracy. Gim\u00b4enez and M`arquez (2004) proposed\na SVM approach also trained on text windows, with bidirectional inference achieved with\ntwo Viterbi decoders (left-to-right and right-to-left).\nThey obtained 97.16% per-word\naccuracy.\nMore recently, Shen et al. (2007) pushed the state-of-the-art up to 97.33%,\nwith a new learning algorithm they call guided learning, also for bidirectional sequence\nclassi\ufb01cation.\n3\narXiv\nCollobert, Weston, Bottou, Karlen, Kavukcuoglu and Kuksa\n2.2 Chunking\nAlso called shallow parsing, chunking aims at labeling segments of a sentence with syntactic\nconstituents such as noun or verb phrases (NP or VP). Each word is assigned only one unique\ntag, often encoded as a begin-chunk (e.g. B-NP) or inside-chunk tag (e.g. I-NP). Chunking\nis often evaluated using the CoNLL 2000 shared task1. Sections 15\u201318 of WSJ data are\nused for training and section 20 for testing. Validation is achieved by splitting the training\nset.\nKudoh and Matsumoto (2000) won the CoNLL 2000 challenge on chunking with a F1-\nscore of 93.48%.\nTheir system was based on Support Vector Machines (SVMs).\nEach\nSVM was trained in a pairwise classi\ufb01cation manner, and fed with a window around the\nword of interest containing POS and words as features, as well as surrounding tags. They\nperform dynamic programming at test time.\nLater, they improved their results up to\n93.91% (Kudo and Matsumoto, 2001) using an ensemble of classi\ufb01ers trained with di\ufb00erent\ntagging conventions (see Section 3.2.3).\nSince then, a certain number of systems based on second-order random \ufb01elds were\nreported (Sha and Pereira, 2003; McDonald et al., 2005; Sun et al., 2008), all reporting\naround 94.3% F1 score. These systems use features composed of words, POS tags, and\ntags.\nMore recently, Shen and Sarkar (2005) obtained 95.23% using a voting classi\ufb01er scheme,\nwhere each classi\ufb01er is trained on di\ufb00erent tag representations2 (IOB, IOE, . . . ). They use\nPOS features coming from an external tagger, as well carefully hand-crafted specialization\nfeatures which again change the data representation by concatenating some (carefully\nchosen) chunk tags or some words with their POS representation. They then build trigrams\nover these features, which are \ufb01nally passed through a Viterbi decoder a test time.\n2.3 Named Entity Recognition\nNER labels atomic elements in the sentence into categories such as \u201cPERSON\u201d or\n\u201cLOCATION\u201d. As in the chunking task, each word is assigned a tag pre\ufb01xed by an indicator\nof the beginning or the inside of an entity. The CoNLL 2003 setup3 is a NER benchmark\ndataset based on Reuters data. The contest provides training, validation and testing sets.\nFlorian et al. (2003) presented the best system at the NER CoNLL 2003 challenge, with\n88.76% F1 score. They used a combination of various machine-learning classi\ufb01ers. Features\nthey picked included words, POS tags, CHUNK tags, pre\ufb01xes and su\ufb03xes, a large gazetteer\n(not provided by the challenge), as well as the output of two other NER classi\ufb01ers trained\non richer datasets. Chieu (2003), the second best performer of CoNLL 2003 (88.31% F1),\nalso used an external gazetteer (their performance goes down to 86.84% with no gazetteer)\nand several hand-chosen features.\nLater, Ando and Zhang (2005) reached 89.31% F1 with a semi-supervised approach.\nThey trained jointly a linear model on NER with a linear model on two auxiliary\nunsupervised tasks. They also performed Viterbi decoding at test time. The unlabeled\n1. See http://www.cnts.ua.ac.be/conll2000/chunking.\n2. See Table 3 for tagging scheme details.\n3. See http://www.cnts.ua.ac.be/conll2003/ner.\n4\narXiv\nNatural Language Processing (almost) from Scratch\ncorpus was 27M words taken from Reuters. Features included words, POS tags, su\ufb03xes\nand pre\ufb01xes or CHUNK tags, but overall were less specialized than CoNLL 2003 challengers.\n2.4 Semantic Role Labeling\nSRL aims at giving a semantic role to a syntactic constituent of a sentence.\nIn the\nPropBank (Palmer et al., 2005) formalism one assigns roles ARG0-5 to words that are\narguments of a verb (or more technically, a predicate) in the sentence, e.g. the following\nsentence might be tagged \u201c[John]ARG0 [ate]REL [the apple]ARG1 \u201d, where \u201cate\u201d is the\npredicate. The precise arguments depend on a verb\u2019s frame and if there are multiple verbs\nin a sentence some words might have multiple tags.\nIn addition to the ARG0-5 tags,\nthere there are several modi\ufb01er tags such as ARGM-LOC (locational) and ARGM-TMP\n(temporal) that operate in a similar way for all verbs. We picked CoNLL 20054 as our SRL\nbenchmark. It takes sections 2\u201321 of WSJ data as training set, and section 24 as validation\nset. A test set composed of section 23 of WSJ concatenated with 3 sections from the Brown\ncorpus is also provided by the challenge.\nState-of-the-art SRL systems consist of several stages: producing a parse tree, identifying\nwhich parse tree nodes represent the arguments of a given verb, and \ufb01nally classifying these\nnodes to compute the corresponding SRL tags.\nThis entails extracting numerous base\nfeatures from the parse tree and feeding them into statistical models. Feature categories\ncommonly used by these system include (Gildea and Jurafsky, 2002; Pradhan et al., 2004):\n\u2022 the parts of speech and syntactic labels of words and nodes in the tree;\n\u2022 the node\u2019s position (left or right) in relation to the verb;\n\u2022 the syntactic path to the verb in the parse tree;\n\u2022 whether a node in the parse tree is part of a noun or verb phrase;\n\u2022 the voice of the sentence: active or passive;\n\u2022 the node\u2019s head word; and\n\u2022 the verb sub-categorization.\nPradhan et al. (2004) take these base features and de\ufb01ne additional features, notably\nthe part-of-speech tag of the head word, the predicted named entity class of the argument,\nfeatures providing word sense disambiguation for the verb (they add 25 variants of 12 new\nfeature types overall). This system is close to the state-of-the-art in performance. Pradhan\net al. (2005) obtain 77.30% F1 with a system based on SVM classi\ufb01ers and simultaneously\nusing the two parse trees provided for the SRL task. In the same spirit, Haghighi et al.\n(2005) use log-linear models on each tree node, re-ranked globally with a dynamic algorithm.\nTheir system reaches 77.04% using the \ufb01ve top Charniak parse trees.\nKoomen et al. (2005) hold the state-of-the-art with Winnow-like (Littlestone, 1988)\nclassi\ufb01ers, followed by a decoding stage based on an integer program that enforces speci\ufb01c\nconstraints on SRL tags. They reach 77.92% F1 on CoNLL 2005, thanks to the \ufb01ve top\nparse trees produced by the Charniak (2000) parser (only the \ufb01rst one was provided by the\ncontest) as well as the Collins (1999) parse tree.\n4. See http://www.lsi.upc.edu/~srlconll.\n5\narXiv\nCollobert, Weston, Bottou, Karlen, Kavukcuoglu and Kuksa\n2.5 Evaluation\nIn our experiments, we strictly followed the standard evaluation procedure of each CoNLL\nchallenges for NER, CHUNK and SRL. All these three tasks are evaluated by computing the\nF1 scores over chunks produced by our models. The POS task is evaluated by computing\nthe per-word accuracy, as it is the case for the standard benchmark we refer to (Toutanova\net al., 2003). We picked the conlleval script5 for evaluating POS6, NER and CHUNK.\nFor SRL, we used the srl-eval.pl script included in the srlconll package7.\n2.6 Discussion\nWhen participating in an (open) challenge, it is legitimate to increase generalization by all\nmeans. It is thus not surprising to see many top CoNLL systems using external labeled data,\nlike additional NER classi\ufb01ers for the NER architecture of Florian et al. (2003) or additional\nparse trees for SRL systems (Koomen et al., 2005). Combining multiple systems or tweaking\ncarefully features is also a common approach, like in the chunking top system (Shen and\nSarkar, 2005).\nHowever, when comparing systems, we do not learn anything of the quality of each\nsystem if they were trained with di\ufb00erent labeled data. For that reason, we will refer to\nbenchmark systems, that is, top existing systems which avoid usage of external data and\nhave been well-established in the NLP \ufb01eld: (Toutanova et al., 2003) for POS and (Sha and\nPereira, 2003) for chunking. For NER we consider (Ando and Zhang, 2005) as they were\nusing additional unlabeled data only. We picked (Koomen et al., 2005) for SRL, keeping in\nmind they use 4 additional parse trees not provided by the challenge. These benchmark\nsystems will serve as baseline references in our experiments.\nWe marked them in bold\nin Table 2.\nWe note that for the four tasks we are considering in this work, it can be seen that for the\nmore complex tasks (with corresponding lower accuracies), the best systems proposed have\nmore engineered features relative to the best systems on the simpler tasks. That is, the POS\ntask is one of the simplest of our four tasks, and only has relatively few engineered features,\nwhereas SRL is the most complex, and many kinds of features have been designed for it.\nThis clearly has implications for as yet unsolved NLP tasks requiring more sophisticated\nsemantic understanding than the ones considered here.\n3. The Networks\nAll the NLP tasks above can be seen as tasks assigning labels to words. The traditional NLP\napproach is: extract from the sentence a rich set of hand-designed features which are then\nfed to a standard classi\ufb01cation algorithm, e.g. a Support Vector Machine (SVM), often with\na linear kernel. The choice of features is a completely empirical process, mainly based \ufb01rst\non linguistic intuition, and then trial and error, and the feature selection is task dependent,\nimplying additional research for each new NLP task. Complex tasks like SRL then require\na large number of possibly complex features (e.g., extracted from a parse tree) which can\n5. Available at http://www.cnts.ua.ac.be/conll2000/chunking/conlleval.txt.\n6. We used the \u201c-r\u201d option of the conlleval script to get the per-word accuracy, for POS only.\n7. Available at http://www.lsi.upc.es/~srlconll/srlconll-1.1.tgz.\n6\narXiv\nNatural Language Processing (almost) from Scratch\nInput Window\nLookup Table\nLinear\nHardTanh\nLinear\nText\ncat\nsat\non the mat\nFeature 1\nw1\n1\nw1\n2\n. . .\nw1\nN\n...\nFeature K\nwK\n1\nwK\n2\n. . .\nwK\nN\nLTW 1\n...\nLTW K\nM 1 \u00d7 \u00b7\nM 2 \u00d7 \u00b7\nword of interest\nd\nconcat\nn1\nhu\nn2\nhu = #tags\nFigure 1: Window approach network.\nimpact the computational cost which might be important for large-scale applications or\napplications requiring real-time response.\nInstead, we advocate a radically di\ufb00erent approach: as input we will try to pre-process\nour features as little as possible and then use a multilayer neural network (NN) architecture,\ntrained in an end-to-end fashion.\nThe architecture takes the input sentence and learns\nseveral layers of feature extraction that process the inputs. The features computed by the\ndeep layers of the network are automatically trained by backpropagation to be relevant to\nthe task. We describe in this section a general multilayer architecture suitable for all our\nNLP tasks, which is generalizable to other NLP tasks as well.\nOur architecture is summarized in Figure 1 and Figure 2. The \ufb01rst layer extracts features\nfor each word. The second layer extracts features from a window of words or from the whole\nsentence, treating it as a sequence with local and global structure (i.e., it is not treated like\na bag of words). The following layers are standard NN layers.\nNotations\nWe consider a neural network f\u03b8(\u00b7), with parameters \u03b8.\nAny feed-forward\nneural network with L layers, can be seen as a composition of functions fl\n\u03b8(\u00b7), corresponding\nto each layer l:\nf\u03b8(\u00b7) = fL\n\u03b8 (fL\u22121\n\u03b8\n(. . . f1\n\u03b8 (\u00b7) . . .)) .\nIn the following, we will describe each layer we use in our networks shown in Figure 1\nand Figure 2. We adopt few notations. Given a matrix A we denote [A]i, j the coe\ufb03cient\n7\narXiv\nCollobert, Weston, Bottou, Karlen, Kavukcuoglu and Kuksa\nInput Sentence\nLookup Table\nConvolution\nMax Over Time\nLinear\nHardTanh\nLinear\nText\nThe cat\nsat\non\nthe mat\nFeature 1\nw1\n1\nw1\n2\n. . .\nw1\nN\n...\nFeature K\nwK\n1\nwK\n2\n. . .\nwK\nN\nLTW 1\n...\nLTW K\nmax(\u00b7)\nM 2 \u00d7 \u00b7\nM 3 \u00d7 \u00b7\nd\nPadding\nPadding\nn1\nhu\nM 1 \u00d7 \u00b7\nn1\nhu\nn2\nhu\nn3\nhu = #tags\nFigure 2: Sentence approach network.\nat row i and column j in the matrix.\nWe also denote \u27e8A\u27e9dwin\ni\nthe vector obtained by\nconcatenating the dwin column vectors around the ith column vector of matrix A \u2208Rd1\u00d7d2:\nh\n\u27e8A\u27e9dwin\ni\niT\n=\n\u0010\n[A]1, i\u2212dwin/2 . . . [A]d1, i\u2212dwin/2 , . . . , [A]1, i+dwin/2 . . . [A]d1, i+dwin/2\n\u0011\n.\nAs a special case, \u27e8A\u27e91\ni represents the ith column of matrix A. For a vector v, we denote\n[v]i the scalar at index i in the vector. Finally, a sequence of element {x1, x2, . . . , xT } is\nwritten [x]T\n1 . The ith element of the sequence is [x]i.\n8\narXiv\nNatural Language Processing (almost) from Scratch\n3.1 Transforming Words into Feature Vectors\nOne of the essential key points of our architecture is its ability to perform well with the\nuse of (almost8) raw words. The ability for our method to learn good word representations\nis thus crucial to our approach. For e\ufb03ciency, words are fed to our architecture as indices\ntaken from a \ufb01nite dictionary D. Obviously, a simple index does not carry much useful\ninformation about the word. However, the \ufb01rst layer of our network maps each of these\nword indices into a feature vector, by a lookup table operation. Given a task of interest, a\nrelevant representation of each word is then given by the corresponding lookup table feature\nvector, which is trained by backpropagation.\nMore formally, for each word w \u2208D, an internal dwrd-dimensional feature vector\nrepresentation is given by the lookup table layer LTW (\u00b7):\nLTW (w) = \u27e8W\u27e91\nw ,\nwhere W \u2208Rdwrd\u00d7|D| is a matrix of parameters to be learnt, \u27e8W\u27e91\nw \u2208Rdwrd is the wth\ncolumn of W and dwrd is the word vector size (a hyper-parameter to be chosen by the user).\nGiven a sentence or any sequence of T words [w]T\n1 in D, the lookup table layer applies the\nsame operation for each word in the sequence, producing the following output matrix:\nLTW ([w]T\n1 ) =\n\u0010\n\u27e8W\u27e91\n[w]1\n\u27e8W\u27e91\n[w]2\n. . .\n\u27e8W\u27e91\n[w]T\n\u0011\n.\n(1)\nThis matrix can then be fed to further neural network layers, as we will see below.\n3.1.1 Extending to Any Discrete Features\nOne might want to provide features other than words if one suspects that these features are\nhelpful for the task of interest. For example, for the NER task, one could provide a feature\nwhich says if a word is in a gazetteer or not. Another common practice is to introduce some\nbasic pre-processing, such as word-stemming or dealing with upper and lower case. In this\nlatter option, the word would be then represented by three discrete features: its lower case\nstemmed root, its lower case ending, and a capitalization feature.\nGenerally speaking, we can consider a word as represented by K discrete features w \u2208\nD1\u00d7\u00b7 \u00b7 \u00b7\u00d7DK, where Dk is the dictionary for the kth feature. We associate to each feature a\nlookup table LTW k(\u00b7), with parameters W k \u2208Rdk\nwrd\u00d7|Dk| where dk\nwrd \u2208N is a user-speci\ufb01ed\nvector size. Given a word w, a feature vector of dimension dwrd = P\nk dk\nwrd is then obtained\nby concatenating all lookup table outputs:\nLTW 1,...,W K(w) =\n\uf8eb\n\uf8ec\n\uf8ed\nLTW 1(w1)\n...\nLTW K(wK)\n\uf8f6\n\uf8f7\n\uf8f8=\n\uf8eb\n\uf8ec\n\uf8ed\n\u27e8W 1\u27e91\nw1\n...\n\u27e8W K\u27e91\nwK\n\uf8f6\n\uf8f7\n\uf8f8.\n8. We did some pre-processing, namely lowercasing and encoding capitalization as another feature. With\nenough (unlabeled) training data, presumably we could learn a model without this processing. Ideally,\nan even more raw input would be to learn from letter sequences rather than words, however we felt that\nthis was beyond the scope of this work.\n9\narXiv\nCollobert, Weston, Bottou, Karlen, Kavukcuoglu and Kuksa\nThe matrix output of the lookup table layer for a sequence of words [w]T\n1 is then similar\nto (1), but where extra rows have been added for each discrete feature:\nLTW 1,...,W K([w]T\n1 ) =\n\uf8eb\n\uf8ec\n\uf8ec\n\uf8ed\n\u27e8W 1\u27e91\n[w1]1\n. . .\n\u27e8W 1\u27e91\n[w1]T\n...\n...\n\u27e8W K\u27e91\n[wK]1\n. . .\n\u27e8W K\u27e91\n[wK]T\n\uf8f6\n\uf8f7\n\uf8f7\n\uf8f8.\n(2)\nThese vector features in the lookup table e\ufb00ectively learn features for words in the dictionary.\nNow, we want to use these trainable features as input to further layers of trainable feature\nextractors, that can represent groups of words and then \ufb01nally sentences.\n3.2 Extracting Higher Level Features from Word Feature Vectors\nFeature vectors produced by the lookup table layer need to be combined in subsequent layers\nof the neural network to produce a tag decision for each word in the sentence. Producing\ntags for each element in variable length sequences (here, a sentence is a sequence of words)\nis a standard problem in machine-learning. We consider two common approaches which tag\none word at the time: a window approach, and a (convolutional) sentence approach.\n3.2.1 Window Approach\nA window approach assumes the tag of a word depends mainly on its neighboring words.\nGiven a word to tag, we consider a \ufb01xed size ksz (a hyper-parameter) window of words\naround this word. Each word in the window is \ufb01rst passed through the lookup table layer (1)\nor (2), producing a matrix of word features of \ufb01xed size dwrd \u00d7 ksz. This matrix can be\nviewed as a dwrd ksz-dimensional vector by concatenating each column vector, which can be\nfed to further neural network layers. More formally, the word feature window given by the\n\ufb01rst network layer can be written as:\nf1\n\u03b8 = \u27e8LTW ([w]T\n1 )\u27e9dwin\nt\n=\n\uf8eb\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ed\n\u27e8W\u27e91\n[w]t\u2212dwin/2\n...\n\u27e8W\u27e91\n[w]t\n...\n\u27e8W\u27e91\n[w]t+dwin/2\n\uf8f6\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f8\n.\n(3)\nLinear Layer\nThe \ufb01xed size vector f1\n\u03b8 can be fed to one or several standard neural\nnetwork layers which perform a\ufb03ne transformations over their inputs:\nfl\n\u03b8 = W l fl\u22121\n\u03b8\n+ bl ,\n(4)\nwhere W l \u2208Rnl\nhu\u00d7nl\u22121\nhu and bl \u2208Rnl\nhu are the parameters to be trained. The hyper-parameter\nnl\nhu is usually called the number of hidden units of the lth layer.\nHardTanh Layer\nSeveral linear layers are often stacked, interleaved with a non-linearity\nfunction, to extract highly non-linear features. If no non-linearity is introduced, our network\n10\narXiv\nNatural Language Processing (almost) from Scratch\nwould be a simple linear model. We chose a \u201chard\u201d version of the hyperbolic tangent as non-\nlinearity. It has the advantage of being slightly cheaper to compute (compared to the exact\nhyperbolic tangent), while leaving the generalization performance unchanged (Collobert,\n2004). The corresponding layer l applies a HardTanh over its input vector:\nh\nfl\n\u03b8\ni\ni = HardTanh(\nh\nfl\u22121\n\u03b8\ni\ni) ,\nwhere\nHardTanh(x) =\n\uf8f1\n\uf8f2\n\uf8f3\n\u22121\nif x < \u22121\nx\nif \u22121 <= x <= 1\n1\nif x > 1\n.\n(5)\nScoring\nFinally, the output size of the last layer L of our network is equal to the number\nof possible tags for the task of interest. Each output can be then interpreted as a score of\nthe corresponding tag (given the input of the network), thanks to a carefully chosen cost\nfunction that we will describe later in this section.\nRemark 1 (Border E\ufb00ects) The feature window (3) is not well de\ufb01ned for words near\nthe beginning or the end of a sentence. To circumvent this problem, we augment the sentence\nwith a special \u201cPADDING\u201d word replicated dwin/2 times at the beginning and the end. This\nis akin to the use of \u201cstart\u201d and \u201cstop\u201d symbols in sequence models.\n3.2.2 Sentence Approach\nWe will see in the experimental section that a window approach performs well for most\nnatural language processing tasks we are interested in. However this approach fails with\nSRL, where the tag of a word depends on a verb (or, more correctly, predicate) chosen\nbeforehand in the sentence. If the verb falls outside the window, one cannot expect this word\nto be tagged correctly. In this particular case, tagging a word requires the consideration of\nthe whole sentence. When using neural networks, the natural choice to tackle this problem\nbecomes a convolutional approach, \ufb01rst introduced by Waibel et al. (1989) and also called\nTime Delay Neural Networks (TDNNs) in the literature.\nWe describe in detail our convolutional network below. It successively takes the complete\nsentence, passes it through the lookup table layer (1), produces local features around each\nword of the sentence thanks to convolutional layers, combines these feature into a global\nfeature vector which can then be fed to standard a\ufb03ne layers (4). In the semantic role\nlabeling case, this operation is performed for each word in the sentence, and for each verb\nin the sentence. It is thus necessary to encode in the network architecture which verb we\nare considering in the sentence, and which word we want to tag. For that purpose, each\nword at position i in the sentence is augmented with two features in the way described\nin Section 3.1.1. These features encode the relative distances i \u2212posv and i \u2212posw with\nrespect to the chosen verb at position posv, and the word to tag at position posw respectively.\nConvolutional Layer\nA convolutional layer can be seen as a generalization of a window\napproach: given a sequence represented by columns in a matrix fl\u22121\n\u03b8\n(in our lookup table\nmatrix (1)), a matrix-vector operation as in (4) is applied to each window of successive\n11\narXiv\nCollobert, Weston, Bottou, Karlen, Kavukcuoglu and Kuksa\n 0\n 10\n 20\n 30\n 40\n 50\n 60\n 70\nThe\nproposed\nchanges\nalso\nwould\nallow\nexecutives\nto\nreport\nexercises\nof\noptions\nlater\nand\nless\noften\n.\n 0\n 10\n 20\n 30\n 40\n 50\n 60\n 70\nThe\nproposed\nchanges\nalso\nwould\nallow\nexecutives\nto\nreport\nexercises\nof\noptions\nlater\nand\nless\noften\n.\nFigure 3: Number of features chosen at each word position by the Max layer. We consider\na sentence approach network (Figure 2) trained for SRL. The number of \u201clocal\u201d features\noutput by the convolution layer is 300 per word. By applying a Max over the sentence,\nwe obtain 300 features for the whole sentence. It is interesting to see that the network\ncatches features mostly around the verb of interest (here \u201creport\u201d) and word of interest\n(\u201cproposed\u201d (left) or \u201coften\u201d (right)).\nwindows in the sequence. Using previous notations, the tth output column of the lth layer\ncan be computed as:\n\u27e8fl\n\u03b8\u27e91\nt = W l \u27e8fl\u22121\n\u03b8\n\u27e9dwin\nt\n+ bl\n\u2200t ,\n(6)\nwhere the weight matrix W l is the same across all windows t in the sequence. Convolutional\nlayers extract local features around each window of the given sequence. As for standard\na\ufb03ne layers (4), convolutional layers are often stacked to extract higher level features.\nIn this case, each layer must be followed by a non-linearity (5) or the network would be\nequivalent to one convolutional layer.\nMax Layer\nThe size of the output (6) depends on the number of words in the sentence\nfed to the network. Local feature vectors extracted by the convolutional layers have to be\ncombined to obtain a global feature vector, with a \ufb01xed size independent of the sentence\nlength, in order to apply subsequent standard a\ufb03ne layers.\nTraditional convolutional\nnetworks often apply an average (possibly weighted) or a max operation over the \u201ctime\u201d t\nof the sequence (6). (Here, \u201ctime\u201d just means the position in the sentence, this term stems\nfrom the use of convolutional layers in e.g. speech data where the sequence occurs over\ntime.) The average operation does not make much sense in our case, as in general most\nwords in the sentence do not have any in\ufb02uence on the semantic role of a given word to tag.\nInstead, we used a max approach, which forces the network to capture the most useful local\nfeatures produced by the convolutional layers (see Figure 3), for the task at hand. Given a\nmatrix fl\u22121\n\u03b8\noutput by a convolutional layer l \u22121, the Max layer l outputs a vector fl\n\u03b8:\nh\nfl\n\u03b8\ni\ni = max\nt\nh\nfl\u22121\n\u03b8\ni\ni, t\n1 \u2264i \u2264nl\u22121\nhu .\n(7)\nThis \ufb01xed sized global feature vector can be then fed to standard a\ufb03ne network layers (4).\nAs in the window approach, we then \ufb01nally produce one score per possible tag for the given\ntask.\n12\narXiv\nNatural Language Processing (almost) from Scratch\nScheme\nBegin\nInside\nEnd\nSingle\nOther\nIOB\nB-X\nI-X\nI-X\nB-X\nO\nIOE\nI-X\nI-X\nE-X\nE-X\nO\nIOBES\nB-X\nI-X\nE-X\nS-X\nO\nTable 3:\nVarious tagging schemes. Each word in a segment labeled \u201cX\u201d is tagged with a\npre\ufb01xed label, depending of the word position in the segment (begin, inside, end). Single\nword segment labeling is also output. Words not in a labeled segment are labeled \u201cO\u201d.\nVariants of the IOB (and IOE) scheme exist, where the pre\ufb01x B (or E) is replaced by I for\nall segments not contiguous with another segment having the same label \u201cX\u201d.\nRemark 2 The same border e\ufb00ects arise in the convolution operation (6) as in the window\napproach (3). We again work around this problem by padding the sentences with a special\nword.\n3.2.3 Tagging Schemes\nAs explained earlier, the network output layers compute scores for all the possible tags for\nthe task of interest. In the window approach, these tags apply to the word located in the\ncenter of the window. In the (convolutional) sentence approach, these tags apply to the\nword designated by additional markers in the network input.\nThe POS task indeed consists of marking the syntactic role of each word. However, the\nremaining three tasks associate labels with segments of a sentence. This is usually achieved\nby using special tagging schemes to identify the segment boundaries, as shown in Table 3.\nSeveral such schemes have been de\ufb01ned (IOB, IOE, IOBES, . . . ) without clear conclusion\nas to which scheme is better in general. State-of-the-art performance is sometimes obtained\nby combining classi\ufb01ers trained with di\ufb00erent tagging schemes (e.g. Kudo and Matsumoto,\n2001).\nThe ground truth for the NER, CHUNK, and SRL tasks is provided using two di\ufb00erent\ntagging schemes. In order to eliminate this additional source of variations, we have decided\nto use the most expressive IOBES tagging scheme for all tasks. For instance, in the CHUNK\ntask, we describe noun phrases using four di\ufb00erent tags. Tag \u201cS-NP\u201d is used to mark a noun\nphrase containing a single word. Otherwise tags \u201cB-NP\u201d, \u201cI-NP\u201d, and \u201cE-NP\u201d are used\nto mark the \ufb01rst, intermediate and last words of the noun phrase. An additional tag \u201cO\u201d\nmarks words that are not members of a chunk. During testing, these tags are then converted\nto the original IOB tagging scheme and fed to the standard performance evaluation scripts\nmentioned in Section 2.5.\n3.3 Training\nAll our neural networks are trained by maximizing a likelihood over the training data, using\nstochastic gradient ascent. If we denote \u03b8 to be all the trainable parameters of the network,\nwhich are trained using a training set T we want to maximize the following log-likelihood\n13\narXiv\nCollobert, Weston, Bottou, Karlen, Kavukcuoglu and Kuksa\nwith respect to \u03b8:\n\u03b8 7\u2192\nX\n(x, y)\u2208T\nlog p(y | x, \u03b8) ,\n(8)\nwhere x corresponds to either a training word window or a sentence and its associated\nfeatures, and y represents the corresponding tag. The probability p(\u00b7) is computed from the\noutputs of the neural network. We will see in this section two ways of interpreting neural\nnetwork outputs as probabilities.\n3.3.1 Word-Level Log-Likelihood\nIn this approach, each word in a sentence is considered independently.\nGiven an input\nexample x, the network with parameters \u03b8 outputs a score [f\u03b8(x)]i, for the ith tag with\nrespect to the task of interest. To simplify the notation, we drop x from now, and we write\ninstead [f\u03b8]i. This score can be interpreted as a conditional tag probability p(i | x, \u03b8) by\napplying a softmax (Bridle, 1990) operation over all the tags:\np(i | x, \u03b8) =\ne[f\u03b8]i\nP\nj e[f\u03b8]j\n.\n(9)\nDe\ufb01ning the log-add operation as\nlogadd\ni\nzi = log(\nX\ni\nezi) ,\n(10)\nwe can express the log-likelihood for one training example (x, y) as follows:\nlog p(y | x, \u03b8) = [f\u03b8]y \u2212logadd\nj\n[f\u03b8]j .\n(11)\nWhile this training criterion, often referred as cross-entropy is widely used for classi\ufb01cation\nproblems, it might not be ideal in our case, where there is often a correlation between the\ntag of a word in a sentence and its neighboring tags. We now describe another common\napproach for neural networks which enforces dependencies between the predicted tags in a\nsentence.\n3.3.2 Sentence-Level Log-Likelihood\nIn tasks like chunking, NER or SRL we know that there are dependencies between word\ntags in a sentence: not only are tags organized in chunks, but some tags cannot follow\nother tags. Training using a word-level approach discards this kind of labeling information.\nWe consider a training scheme which takes into account the sentence structure: given the\npredictions of all tags by our network for all words in a sentence, and given a score for going\nfrom one tag to another tag, we want to encourage valid paths of tags during training, while\ndiscouraging all other paths.\nWe consider the matrix of scores f\u03b8([x]T\n1 ) output by the network. As before, we drop the\ninput [x]T\n1 for notation simpli\ufb01cation. The element [f\u03b8]i, t of the matrix is the score output\nby the network with parameters \u03b8, for the sentence [x]T\n1 and for the ith tag, at the tth word.\n14\narXiv\nNatural Language Processing (almost) from Scratch\nWe introduce a transition score [A]i, j for jumping from i to j tags in successive words, and\nan initial score [A]i, 0 for starting from the ith tag. As the transition scores are going to be\ntrained (as are all network parameters \u03b8), we de\ufb01ne \u02dc\u03b8 = \u03b8 \u222a{[A]i, j \u2200i, j}. The score of\na sentence [x]T\n1 along a path of tags [i]T\n1 is then given by the sum of transition scores and\nnetwork scores:\ns([x]T\n1 , [i]T\n1 , \u02dc\u03b8) =\nT\nX\nt=1\n\u0010\n[A][i]t\u22121, [i]t + [f\u03b8][i]t, t\n\u0011\n.\n(12)\nExactly as for the word-level likelihood (11), where we were normalizing with respect to all\ntags using a softmax (9), we normalize this score over all possible tag paths [j]T\n1 using a\nsoftmax, and we interpret the resulting ratio as a conditional tag path probability. Taking\nthe log, the conditional probability of the true path [y]T\n1 is therefore given by:\nlog p([y]T\n1 | [x]T\n1 , \u02dc\u03b8) = s([x]T\n1 , [y]T\n1 , \u02dc\u03b8) \u2212logadd\n\u2200[j]T\n1\ns([x]T\n1 , [j]T\n1 , \u02dc\u03b8) .\n(13)\nWhile the number of terms in the logadd operation (11) was equal to the number of tags, it\ngrows exponentially with the length of the sentence in (13). Fortunately, one can compute\nit in linear time with the following standard recursion over t, taking advantage of the\nassociativity and distributivity on the semi-ring9 (R \u222a{\u2212\u221e}, logadd, +):\n\u03b4t(k) \u2206=\nlogadd\n{[j]t\n1 \u2229[j]t=k}\ns([x]t\n1, [j]t\n1, \u02dc\u03b8)\n= logadd\ni\nlogadd\n{[j]t\n1 \u2229[j]t\u22121=i \u2229[j]t=k}\ns([x]t\n1, [j]t\u22121\n1\n, \u02dc\u03b8) + [A][j]t\u22121, k + [f\u03b8]k, t\n= logadd\ni\n\u03b4t\u22121(i) + [A]i, k + [f\u03b8]k, t\n= [f\u03b8]k, t + logadd\ni\n\u0010\n\u03b4t\u22121(i) + [A]i, k\n\u0011\n\u2200k ,\n(14)\nfollowed by the termination\nlogadd\n\u2200[j]T\n1\ns([x]T\n1 , [j]T\n1 , \u02dc\u03b8) = logadd\ni\n\u03b4T (i) .\n(15)\nWe can now maximize in (8) the log-likelihood (13) over all the training pairs ([x]T\n1 , [y]T\n1 ).\nAt inference time, given a sentence [x]T\n1 to tag, we have to \ufb01nd the best tag path which\nminimizes the sentence score (12). In other words, we must \ufb01nd\nargmax\n[j]T\n1\ns([x]T\n1 , [j]T\n1 , \u02dc\u03b8) .\n(16)\nThe Viterbi algorithm is the natural choice for this inference. It corresponds to performing\nthe recursion (14) and (15), but where the logadd is replaced by a max, and then tracking\nback the optimal path through each max.\n9. In other words, read logadd as \u2295and + as \u2297.\n15\narXiv\nCollobert, Weston, Bottou, Karlen, Kavukcuoglu and Kuksa\nRemark 3 (Graph Transformer Networks) Our approach is a particular case of the\ndiscriminative forward training for graph transformer networks (GTNs) (Bottou et al., 1997;\nLe Cun et al., 1998). The log-likelihood (13) can be viewed as the di\ufb00erence between the\nforward score constrained over the valid paths (in our case there is only the labeled path)\nand the unconstrained forward score (15).\nRemark 4 (Conditional Random Fields) An important feature of equation (12) is the\nabsence of normalization.\nSumming the exponentials e [f\u03b8]i, t over all possible tags does\nnot necessarily yield the unity.\nIf this was the case, the scores could be viewed as the\nlogarithms of conditional transition probabilities, and our model would be subject to the\nlabel-bias problem that motivates Conditional Random Fields (CRFs) (La\ufb00erty et al., 2001).\nThe denormalized scores should instead be likened to the potential functions of a CRF.\nIn fact, a CRF maximizes the same likelihood (13) using a linear model instead of a\nnonlinear neural network. CRFs have been widely used in the NLP world, such as for POS\ntagging (La\ufb00erty et al., 2001), chunking (Sha and Pereira, 2003), NER (McCallum and Li,\n2003) or SRL (Cohn and Blunsom, 2005). Compared to such CRFs, we take advantage of\nthe nonlinear network to learn appropriate features for each task of interest.\n3.3.3 Stochastic Gradient\nMaximizing (8) with stochastic gradient (Bottou, 1991) is achieved by iteratively selecting\na random example (x, y) and making a gradient step:\n\u03b8 \u2190\u2212\u03b8 + \u03bb \u2202log p(y | x, \u03b8)\n\u2202\u03b8\n,\n(17)\nwhere \u03bb is a chosen learning rate. Our neural networks described in Figure 1 and Figure 2\nare a succession of layers that correspond to successive composition of functions. The neural\nnetwork is \ufb01nally composed with the word-level log-likelihood (11), or successively composed\nin the recursion (14) if using the sentence-level log-likelihood (13).\nThus, an analytical\nformulation of the derivative (17) can be computed, by applying the di\ufb00erentiation chain\nrule through the network, and through the word-level log-likelihood (11) or through the\nrecurrence (14).\nRemark 5 (Di\ufb00erentiability) Our cost functions are di\ufb00erentiable almost everywhere.\nNon-di\ufb00erentiable points arise because we use a \u201chard\u201d transfer function (5) and because\nwe use a \u201cmax\u201d layer (7) in the sentence approach network.\nFortunately, stochastic\ngradient still converges to a meaningful local minimum despite such minor di\ufb00erentiability\nproblems (Bottou, 1991, 1998). Stochastic gradient iterations that hit a non-di\ufb00erentiability\nare simply skipped.\nRemark 6 (Modular Approach) The well known \u201cback-propagation\u201d algorithm (LeCun,\n1985; Rumelhart et al., 1986) computes gradients using the chain rule. The chain rule can\nalso be used in a modular implementation.10 Our modules correspond to the boxes in Figure 1\nand Figure 2. Given derivatives with respect to its outputs, each module can independently\n10. See http://torch5.sf.net.\n16\narXiv\nNatural Language Processing (almost) from Scratch\nApproach\nPOS\nChunking\nNER\nSRL\n(PWA)\n(F1)\n(F1)\n(F1)\nBenchmark Systems\n97.24\n94.29\n89.31\n77.92\nNN+WLL\n96.31\n89.13\n79.53\n55.40\nNN+SLL\n96.37\n90.33\n81.47\n70.99\nTable 4:\nComparison in generalization performance of benchmark NLP systems with a\nvanilla neural network (NN) approach, on POS, chunking, NER and SRL tasks. We report\nresults with both the word-level log-likelihood (WLL) and the sentence-level log-likelihood\n(SLL). Generalization performance is reported in per-word accuracy rate (PWA) for POS\nand F1 score for other tasks. The NN results are behind the benchmark results, in Section 4\nwe show how to improve these models using unlabeled data.\nTask\nWindow/Conv. size\nWord dim.\nCaps dim.\nHidden units\nLearning rate\nPOS\ndwin = 5\nd0 = 50\nd1 = 5\nn1\nhu = 300\n\u03bb = 0.01\nCHUNK\n\u201d\n\u201d\n\u201d\n\u201d\n\u201d\nNER\n\u201d\n\u201d\n\u201d\n\u201d\n\u201d\nSRL\n\u201d\n\u201d\n\u201d\nn1\nhu = 300\nn2\nhu = 500\n\u201d\nTable 5:\nHyper-parameters of our networks. We report for each task the window size\n(or convolution size), word feature dimension, capital feature dimension, number of hidden\nunits and learning rate.\ncompute derivatives with respect to its inputs and with respect to its trainable parameters,\nas proposed by Bottou and Gallinari (1991). This allows us to easily build variants of our\nnetworks. For details about gradient computations, see Appendix A.\nRemark 7 (Tricks) Many tricks have been reported for training neural networks (LeCun\net al., 1998). Which ones to choose is often confusing. We employed only two of them: the\ninitialization and update of the parameters of each network layer were done according to\nthe \u201cfan-in\u201d of the layer, that is the number of inputs used to compute each output of this\nlayer (Plaut and Hinton, 1987). The fan-in for the lookup table (1), the lth linear layer (4)\nand the convolution layer (6) are respectively 1, nl\u22121\nhu and dwin\u00d7nl\u22121\nhu . The initial parameters\nof the network were drawn from a centered uniform distribution, with a variance equal to\nthe inverse of the square-root of the fan-in. The learning rate in (17) was divided by the\nfan-in, but stays \ufb01xed during the training.\n3.4 Supervised Benchmark Results\nFor POS, chunking and NER tasks, we report results with the window architecture described\nin Section 3.2.1. The SRL task was trained using the sentence approach (Section 3.2.2).\nResults are reported in Table 4, in per-word accuracy (PWA) for POS, and F1 score for all\n17\narXiv\nCollobert, Weston, Bottou, Karlen, Kavukcuoglu and Kuksa\nfrance\njesus\nxbox\nreddish\nscratched\nmegabits\n454\n1973\n6909\n11724\n29869\n87025\npersuade\nthickets\ndecadent\nwidescreen\nodd\nppa\nfaw\nsavary\ndivo\nantica\nanchieta\nuddin\nblackstock\nsympathetic\nverus\nshabby\nemigration\nbiologically\ngiorgi\njfk\noxide\nawe\nmarking\nkayak\nshaheed\nkhwarazm\nurbina\nthud\nheuer\nmclarens\nrumelia\nstationery\nepos\noccupant\nsambhaji\ngladwin\nplanum\nilias\neglinton\nrevised\nworshippers\ncentrally\ngoa\u2019uld\ngsNUMBER\nedging\nleavened\nritsuko\nindonesia\ncollation\noperator\nfrg\npandionidae\nlifeless\nmoneo\nbacha\nw.j.\nnamsos\nshirt\nmahan\nnilgiris\nTable 6: Word embeddings in the word lookup table of a SRL neural network trained from\nscratch, with a dictionary of size 100, 000. For each column the queried word is followed by\nits index in the dictionary (higher means more rare) and its 10 nearest neighbors (arbitrary\nusing the Euclidean metric).\nthe other tasks. We performed experiments both with the word-level log-likelihood (WLL)\nand with the sentence-level log-likelihood (SLL). The hyper-parameters of our networks are\nreported in Table 5. All our networks were fed with two raw text features: lower case words,\nand a capital letter feature. We chose to consider lower case words to limit the number\nof words in the dictionary.\nHowever, to keep some upper case information lost by this\ntransformation, we added a \u201ccaps\u201d feature which tells if each word was in low caps, was all\ncaps, had \ufb01rst letter capital, or had one capital. Additionally, all occurrences of sequences\nof numbers within a word are replaced with the string \u201cNUMBER\u201d, so for example both the\nwords \u201cPS1\u201d and \u201cPS2\u201d would map to the single word \u201cpsNUMBER\u201d. We used a dictionary\ncontaining the 100,000 most common words in WSJ (case insensitive). Words outside this\ndictionary were replaced by a single special \u201cRARE\u201d word.\nResults show that neural networks \u201cout-of-the-box\u201d are behind baseline benchmark\nsystems.\nLooking at all submitted systems reported on each CoNLL challenge website\nshowed us our networks performance are nevertheless in the performance ballpark of existing\napproaches. The training criterion which takes into account the sentence structure (SLL)\nseems to boost the performance for the Chunking, NER and SRL tasks, with little advantage\nfor POS. This result is in line with existing NLP studies comparing sentence-level and word-\nlevel likelihoods (Liang et al., 2008). The capacity of our network architectures lies mainly\nin the word lookup table, which contains 50\u00d7100, 000 parameters to train. In the WSJ data,\n15% of the most common words appear about 90% of the time. Many words appear only\na few times. It is thus very di\ufb03cult to train properly their corresponding 50 dimensional\nfeature vectors in the lookup table. Ideally, we would like semantically similar words to be\nclose in the embedding space represented by the word lookup table: by continuity of the\nneural network function, tags produced on semantically similar sentences would be similar.\nWe show in Table 6 that it is not the case: neighboring words in the embedding space do\nnot seem to be semantically related.\n18\narXiv\nNatural Language Processing (almost) from Scratch\n 95.5\n 96\n 96.5\n 100\n 300\n 500\n 700\n 900\n(a) POS\n 90\n 90.5\n 91\n 91.5\n 100\n 300\n 500\n 700\n 900\n(b) CHUNK\n 85\n 85.5\n 86\n 86.5\n 100\n 300\n 500\n 700\n 900\n(c) NER\n 67\n 67.5\n 68\n 68.5\n 69\n 100\n 300\n 500\n 700\n 900\n(d) SRL\nFigure 4:\nF1 score on the validation set (y-axis) versus number of hidden units (x-axis)\nfor di\ufb00erent tasks trained with the sentence-level likelihood (SLL), as in Table 4. For SRL,\nwe vary in this graph only the number of hidden units in the second layer. The scale is\nadapted for each task. We show the standard deviation (obtained over 5 runs with di\ufb00erent\nrandom initialization), for the architecture we picked (300 hidden units for POS, CHUNK\nand NER, 500 for SRL).\nWe will focus in the next section on improving these word embeddings by leveraging\nunlabeled data. We will see our approach results in a performance boost for all tasks.\nRemark 8 (Architectures) In all our experiments in this paper, we tuned the hyper-\nparameters by trying only a few di\ufb00erent architectures by validation. In practice, the choice\nof hyperparameters such as the number of hidden units, provided they are large enough, has\na limited impact on the generalization performance. In Figure 4, we report the F1 score\nfor each task on the validation set, with respect to the number of hidden units. Considering\nthe variance related to the network initialization, we chose the smallest network achieving\n\u201creasonable\u201d performance, rather than picking the network achieving the top performance\nobtained on a single run.\nRemark 9 (Training Time) Training our network is quite computationally expensive.\nChunking and NER take about one hour to train, POS takes few hours, and SRL takes\nabout three days. Training could be faster with a larger learning rate, but we prefered to\nstick to a small one which works, rather than \ufb01nding the optimal one for speed. Second\norder methods (LeCun et al., 1998) could be another speedup technique.\n4. Lots of Unlabeled Data\nWe would like to obtain word embeddings carrying more syntactic and semantic information\nthan shown in Table 6. Since most of the trainable parameters of our system are associated\nwith the word embeddings, these poor results suggest that we should use considerably\nmore training data.\nFollowing our NLP from scratch philosophy, we now describe how\nto dramatically improve these embeddings using large unlabeled datasets. We then use\nthese improved embeddings to initialize the word lookup tables of the networks described\nin Section 3.4.\n19\narXiv\nCollobert, Weston, Bottou, Karlen, Kavukcuoglu and Kuksa\n4.1 Datasets\nOur \ufb01rst English corpus is the entire English Wikipedia.11 We have removed all paragraphs\ncontaining non-roman characters and all MediaWiki markups.\nThe resulting text was\ntokenized using the Penn Treebank tokenizer script.12 The resulting dataset contains about\n631 million words.\nAs in our previous experiments, we use a dictionary containing the\n100,000 most common words in WSJ, with the same processing of capitals and numbers.\nAgain, words outside the dictionary were replaced by the special \u201cRARE\u201d word.\nOur second English corpus is composed by adding an extra 221 million words extracted\nfrom the Reuters RCV1 (Lewis et al., 2004) dataset.13 We also extended the dictionary to\n130, 000 words by adding the 30, 000 most common words in Reuters. This is useful in order\nto determine whether improvements can be achieved by further increasing the unlabeled\ndataset size.\n4.2 Ranking Criterion versus Entropy Criterion\nWe used these unlabeled datasets to train language models that compute scores describing\nthe acceptability of a piece of text. These language models are again large neural networks\nusing the window approach described in Section 3.2.1 and in Figure 1. As in the previous\nsection, most of the trainable parameters are located in the lookup tables.\nSimilar language models were already proposed by Bengio and Ducharme (2001) and\nSchwenk and Gauvain (2002). Their goal was to estimate the probability of a word given\nthe previous words in a sentence. Estimating conditional probabilities suggests a cross-\nentropy criterion similar to those described in Section 3.3.1. Because the dictionary size is\nlarge, computing the normalization term can be extremely demanding, and sophisticated\napproximations are required. More importantly for us, neither work leads to signi\ufb01cant\nword embeddings being reported.\nShannon (1951) has estimated the entropy of the English language between 0.6 and 1.3\nbits per character by asking human subjects to guess upcoming characters. Cover and King\n(1978) give a lower bound of 1.25 bits per character using a subtle gambling approach.\nMeanwhile, using a simple word trigram model, Brown et al. (1992b) reach 1.75 bits per\ncharacter. Teahan and Cleary (1996) obtain entropies as low as 1.46 bits per character\nusing variable length character n-grams. The human subjects rely of course on all their\nknowledge of the language and of the world. Can we learn the grammatical structure of the\nEnglish language and the nature of the world by leveraging the 0.2 bits per character that\nseparate human subjects from simple n-gram models? Since such tasks certainly require\nhigh capacity models, obtaining su\ufb03ciently small con\ufb01dence intervals on the test set entropy\nmay require prohibitively large training sets.14 The entropy criterion lacks dynamical range\nbecause its numerical value is largely determined by the most frequent phrases. In order to\nlearn syntax, rare but legal phrases are no less signi\ufb01cant than common phrases.\n11. Available at http://download.wikimedia.org. We took the November 2007 version.\n12. Available at http://www.cis.upenn.edu/~treebank/tokenization.html.\n13. Now available at http://trec.nist.gov/data/reuters/reuters.html.\n14. However, Klein and Manning (2002) describe a rare example of realistic unsupervised grammar induction\nusing a cross-entropy approach on binary-branching parsing trees, that is, by forcing the system to\ngenerate a hierarchical representation.\n20\narXiv\nNatural Language Processing (almost) from Scratch\nIt is therefore desirable to de\ufb01ne alternative training criteria. We propose here to use a\npairwise ranking approach (Cohen et al., 1998). We seek a network that computes a higher\nscore when given a legal phrase than when given an incorrect phrase. Because the ranking\nliterature often deals with information retrieval applications, many authors de\ufb01ne complex\nranking criteria that give more weight to the ordering of the best ranking instances (see\nBurges et al., 2007; Cl\u00b4emen\u00b8con and Vayatis, 2007). However, in our case, we do not want\nto emphasize the most common phrase over the rare but legal phrases. Therefore we use a\nsimple pairwise criterion.\nWe consider a window approach network, as described in Section 3.2.1 and Figure 1,\nwith parameters \u03b8 which outputs a score f\u03b8(x) given a window of text x = [w]dwin\n1\n. We\nminimize the ranking criterion with respect to \u03b8:\n\u03b8 7\u2192\nX\nx\u2208X\nX\nw\u2208D\nmax\nn\n0 , 1 \u2212f\u03b8(x) + f\u03b8(x(w))\no\n,\n(18)\nwhere X is the set of all possible text windows with dwin words coming from our training\ncorpus, D is the dictionary of words, and x(w) denotes the text window obtained by replacing\nthe central word of text window [w]dwin\n1\nby the word w.\nOkanohara and Tsujii (2007) use a related approach to avoiding the entropy criteria\nusing a binary classi\ufb01cation approach (correct/incorrect phrase). Their work focuses on\nusing a kernel classi\ufb01er, and not on learning word embeddings as we do here. Smith and\nEisner (2005) also propose a contrastive criterion which estimates the likelihood of the data\nconditioned to a \u201cnegative\u201d neighborhood.\nThey consider various data neighborhoods,\nincluding sentences of length dwin drawn from Ddwin. Their goal was however to perform\nwell on some tagging task on fully unsupervised data, rather than obtaining generic word\nembeddings useful for other tasks.\n4.3 Training Language Models\nThe language model network was trained by stochastic gradient minimization of the ranking\ncriterion (18), sampling a sentence-word pair (s, w) at each iteration.\nSince training times for such large scale systems are counted in weeks, it is not feasible\nto try many combinations of hyperparameters. It also makes sense to speed up the training\ntime by initializing new networks with the embeddings computed by earlier networks. In\nparticular, we found it expedient to train a succession of networks using increasingly large\ndictionaries, each network being initialized with the embeddings of the previous network.\nSuccessive dictionary sizes and switching times are chosen arbitrarily. (Bengio et al., 2009)\nprovides a more detailed discussion of this, the (as yet, poorly understood) \u201ccurriculum\u201d\nprocess.\nFor the purposes of model selection we use the process of \u201cbreeding\u201d.\nThe idea of\nbreeding is instead of trying a full grid search of possible values (which we did not have\nenough computing power for) to search for the parameters in anology to breeding biological\ncell lines. Within each line, child networks are initialized with the embeddings of their\nparents and trained on increasingly rich datasets with sometimes di\ufb00erent parameters. That\nis, suppose we have k processors, which is much less than the possible set of parameters\none would like to try. One chooses k initial parameter choices from the large set, and trains\n21\narXiv\nCollobert, Weston, Bottou, Karlen, Kavukcuoglu and Kuksa\nthese on the k processors. In our case, possible parameters to adjust are: the learning rate\n\u03bb, the word embedding dimensions d, number of hidden units n1\nhu and input window size\ndwin. One then trains each of these models in an online fashion for a certain amount of\ntime (i.e. a few days), and then selects the best ones using the validation set error rate.\nThat is, breeding decisions were made on the basis of the value of the ranking criterion (18)\nestimated on a validation set composed of one million words held out from the Wikipedia\ncorpus. In the next breeding iteration, one then chooses another set of k parameters from\nthe possible grid of values that permute slightly the most successful candidates from the\nprevious round. As many of these parameter choices can share weights, we can e\ufb00ectively\ncontinue online training retaining some of the learning from the previous iterations.\nVery long training times make such strategies necessary for the foreseeable future: if we\nhad been given computers ten times faster, we probably would have found uses for datasets\nten times bigger. However, we should say we believe that although we ended up with a\nparticular choice of parameters, many other choices are almost equally as good, although\nperhaps there are others that are better as we could not do a full grid search.\nIn the following subsections, we report results obtained with two trained language\nmodels.\nThe results achieved by these two models are representative of those achieved\nby networks trained on the full corpuses.\n\u2022 Language model LM1 has a window size dwin = 11 and a hidden layer with n1\nhu = 100\nunits. The embedding layers were dimensioned like those of the supervised networks\n(Table 5).\nModel LM1 was trained on our \ufb01rst English corpus (Wikipedia) using\nsuccessive dictionaries composed of the 5000, 10, 000, 30, 000, 50, 000 and \ufb01nally\n100, 000 most common WSJ words. The total training time was about four weeks.\n\u2022 Language model LM2 has the same dimensions. It was initialized with the embeddings\nof LM1, and trained for an additional three weeks on our second English corpus\n(Wikipedia+Reuters) using a dictionary size of 130,000 words.\n4.4 Embeddings\nBoth networks produce much more appealing word embeddings than in Section 3.4. Table 7\nshows the ten nearest neighbors of a few randomly chosen query words for the LM1 model.\nThe syntactic and semantic properties of the neighbors are clearly related to those of the\nquery word.\nThese results are far more satisfactory than those reported in Table 7 for\nembeddings obtained using purely supervised training of the benchmark NLP tasks.\n4.5 Semi-supervised Benchmark Results\nSemi-supervised learning has been the object of much attention during the last few years (see\nChapelle et al., 2006).\nPrevious semi-supervised approaches for NLP can be roughly\ncategorized as follows:\n\u2022 Ad-hoc approaches such as (Rosenfeld and Feldman, 2007) for relation extraction.\n\u2022 Self-training approaches, such as (Ue\ufb03ng et al., 2007) for machine translation,\nand (McClosky et al., 2006) for parsing. These methods augment the labeled training\n22\narXiv\nNatural Language Processing (almost) from Scratch\nfrance\njesus\nxbox\nreddish\nscratched\nmegabits\n454\n1973\n6909\n11724\n29869\n87025\naustria\ngod\namiga\ngreenish\nnailed\noctets\nbelgium\nsati\nplaystation\nbluish\nsmashed\nmb/s\ngermany\nchrist\nmsx\npinkish\npunched\nbit/s\nitaly\nsatan\nipod\npurplish\npopped\nbaud\ngreece\nkali\nsega\nbrownish\ncrimped\ncarats\nsweden\nindra\npsNUMBER\ngreyish\nscraped\nkbit/s\nnorway\nvishnu\nhd\ngrayish\nscrewed\nmegahertz\neurope\nananda\ndreamcast\nwhitish\nsectioned\nmegapixels\nhungary\nparvati\ngeforce\nsilvery\nslashed\ngbit/s\nswitzerland\ngrace\ncapcom\nyellowish\nripped\namperes\nTable 7: Word embeddings in the word lookup table of the language model neural network\nLM1 trained with a dictionary of size 100, 000. For each column the queried word is followed\nby its index in the dictionary (higher means more rare) and its 10 nearest neighbors (using\nthe Euclidean metric, which was chosen arbitrarily).\nset with examples from the unlabeled dataset using the labels predicted by the model\nitself. Transductive approaches, such as (Joachims, 1999) for text classi\ufb01cation can\nbe viewed as a re\ufb01ned form of self-training.\n\u2022 Parameter sharing approaches such as (Ando and Zhang, 2005; Suzuki and Isozaki,\n2008).\nAndo and Zhang propose a multi-task approach where they jointly train\nmodels sharing certain parameters. They train POS and NER models together with a\nlanguage model (trained on 15 million words) consisting of predicting words given the\nsurrounding tokens. Suzuki and Isozaki embed a generative model (Hidden Markov\nModel) inside a CRF for POS, Chunking and NER. The generative model is trained\non one billion words. These approaches should be seen as a linear counterpart of our\nwork. Using multilayer models vastly expands the parameter sharing opportunities\n(see Section 5).\nOur approach simply consists of initializing the word lookup tables of the supervised\nnetworks with the embeddings computed by the language models. Supervised training is\nthen performed as in Section 3.4.\nIn particular the supervised training stage is free to\nmodify the lookup tables. This sequential approach is computationally convenient because\nit separates the lengthy training of the language models from the relatively fast training of\nthe supervised networks. Once the language models are trained, we can perform multiple\nexperiments on the supervised networks in a relatively short time. Note that our procedure\nis clearly linked to the (semi-supervised) deep learning procedures of (Hinton et al., 2006;\nBengio et al., 2007; Weston et al., 2008).\nTable 8 clearly shows that this simple initialization signi\ufb01cantly boosts the generalization\nperformance of the supervised networks for each task. It is worth mentioning the larger\nlanguage model led to even better performance.\nThis suggests that we could still take\nadvantage of even bigger unlabeled datasets.\n23\narXiv\nCollobert, Weston, Bottou, Karlen, Kavukcuoglu and Kuksa\nApproach\nPOS\nCHUNK\nNER\nSRL\n(PWA)\n(F1)\n(F1)\n(F1)\nBenchmark Systems\n97.24\n94.29\n89.31\n77.92\nNN+WLL\n96.31\n89.13\n79.53\n55.40\nNN+SLL\n96.37\n90.33\n81.47\n70.99\nNN+WLL+LM1\n97.05\n91.91\n85.68\n58.18\nNN+SLL+LM1\n97.10\n93.65\n87.58\n73.84\nNN+WLL+LM2\n97.14\n92.04\n86.96\n58.34\nNN+SLL+LM2\n97.20\n93.63\n88.67\n74.15\nTable 8:\nComparison in generalization performance of benchmark NLP systems with our\n(NN) approach on POS, chunking, NER and SRL tasks. We report results with both the\nword-level log-likelihood (WLL) and the sentence-level log-likelihood (SLL). We report with\n(LMn) performance of the networks trained from the language model embeddings (Table 7).\nGeneralization performance is reported in per-word accuracy (PWA) for POS and F1 score\nfor other tasks.\n4.6 Ranking and Language\nThere is a large agreement in the NLP community that syntax is a necessary prerequisite for\nsemantic role labeling (Gildea and Palmer, 2002). This is why state-of-the-art semantic role\nlabeling systems thoroughly exploit multiple parse trees. The parsers themselves (Charniak,\n2000; Collins, 1999) contain considerable prior information about syntax (one can think of\nthis as a kind of informed pre-processing).\nOur system does not use such parse trees because we attempt to learn this information\nfrom the unlabeled data set. It is therefore legitimate to question whether our ranking\ncriterion (18) has the conceptual capability to capture such a rich hierarchical information.\nAt \ufb01rst glance, the ranking task appears unrelated to the induction of probabilistic\ngrammars that underly standard parsing algorithms. The lack of hierarchical representation\nseems a fatal \ufb02aw (Chomsky, 1956).\nHowever, ranking is closely related to an alternative description of the language\nstructure: operator grammars (Harris, 1968). Instead of directly studying the structure\nof a sentence, Harris de\ufb01nes an algebraic structure on the space of all sentences. Starting\nfrom a couple of elementary sentence forms, sentences are described by the successive\napplication of sentence transformation operators.\nThe sentence structure is revealed as\na side e\ufb00ect of the successive transformations. Sentence transformations can also have a\nsemantic interpretation.\nIn the spirit of structural linguistics, Harris describes procedures to discover sentence\ntransformation operators by leveraging the statistical regularities of the language. Such\nprocedures are obviously useful for machine learning approaches. In particular, he proposes\na test to decide whether two sentences forms are semantically related by a transformation\noperator. He \ufb01rst de\ufb01nes a ranking criterion (Harris, 1968, section 4.1):\n\u201cStarting for convenience with very short sentence forms, say ABC, we\nchoose a particular word choice for all the classes, say BqCq, except one, in\n24\narXiv\nNatural Language Processing (almost) from Scratch\nthis case A; for every pair of members Ai, Aj of that word class we ask how\nthe sentence formed with one of the members, i.e. AiBqCq compares as to\nacceptability with the sentence formed with the other member, i.e. AjBqCq.\u201d\nThese gradings are then used to compare sentence forms:\n\u201cIt now turns out that, given the graded n-tuples of words for a particular\nsentence form, we can \ufb01nd other sentences forms of the same word classes in\nwhich the same n-tuples of words produce the same grading of sentences.\u201d\nThis is an indication that these two sentence forms exploit common words with the same\nsyntactic function and possibly the same meaning. This observation forms the empirical\nbasis for the construction of operator grammars that describe real-world natural languages\nsuch as English.\nTherefore there are solid reasons to believe that the ranking criterion (18) has the\nconceptual potential to capture strong syntactic and semantic information. On the other\nhand, the structure of our language models is probably too restrictive for such goals, and\nour current approach only exploits the word embeddings discovered during training.\n5. Multi-Task Learning\nIt is generally accepted that features trained for one task can be useful for related tasks. This\nidea was already exploited in the previous section when certain language model features,\nnamely the word embeddings, were used to initialize the supervised networks.\nMulti-task learning (MTL) leverages this idea in a more systematic way. Models for\nall tasks of interests are jointly trained with an additional linkage between their trainable\nparameters in the hope of improving the generalization error. This linkage can take the form\nof a regularization term in the joint cost function that biases the models towards common\nrepresentations.\nA much simpler approach consists in having the models share certain\nparameters de\ufb01ned a priori. Multi-task learning has a long history in machine learning and\nneural networks. Caruana (1997) gives a good overview of these past e\ufb00orts.\n5.1 Joint Decoding versus Joint Training\nMultitask approaches do not necessarily involve joint training. For instance, modern speech\nrecognition systems use Bayes rule to combine the outputs of an acoustic model trained on\nspeech data and a language model trained on phonetic or textual corpora (Jelinek, 1976).\nThis joint decoding approach has been successfully applied to structurally more complex\nNLP tasks.\nSutton and McCallum (2005b) obtains improved results by combining the\npredictions of independently trained CRF models using a joint decoding process at test\ntime that requires more sophisticated probabilistic inference techniques.\nOn the other\nhand, Sutton and McCallum (2005a) obtain results somewhat below the state-of-the-art\nusing joint decoding for SRL and syntactic parsing. Musillo and Merlo (2006) also describe\na negative result at the same joint task.\nJoint decoding invariably works by considering additional probabilistic dependency\npaths between the models.\nTherefore it de\ufb01nes an implicit supermodel that describes\nall the tasks in the same probabilistic framework. Separately training a submodel only\n25\narXiv\nCollobert, Weston, Bottou, Karlen, Kavukcuoglu and Kuksa\nApproach\nPOS\nCHUNK\nNER\nSRL\n(PWA)\n(F1)\n(F1)\n(F1)\nBenchmark Systems\n97.24\n94.29\n89.31\n\u2013\nWindow Approach\nNN+SLL+LM2\n97.20\n93.63\n88.67\n\u2013\nNN+SLL+LM2+MTL\n97.22\n94.10\n88.62\n\u2013\nSentence Approach\nNN+SLL+LM2\n97.12\n93.37\n88.78\n74.15\nNN+SLL+LM2+MTL\n97.22\n93.75\n88.27\n74.29\nTable 9:\nE\ufb00ect of multi-tasking on our neural architectures. We trained POS, CHUNK\nNER in a MTL way, both for the window and sentence network approaches. SRL was only\nincluded in the sentence approach joint training. As a baseline, we show previous results\nof our window approach system, as well as additional results for our sentence approach\nsystem, when trained separately on each task. Benchmark system performance is also given\nfor comparison.\nmakes sense when the training data blocks these additional dependency paths (in the sense\nof d-separation, Pearl, 1988).\nThis implies that, without joint training, the additional\ndependency paths cannot directly involve unobserved variables. Therefore, the natural idea\nof discovering common internal representations across tasks requires joint training.\nJoint training is relatively straightforward when the training sets for the individual\ntasks contain the same patterns with di\ufb00erent labels. It is then su\ufb03cient to train a model\nthat computes multiple outputs for each pattern (Suddarth and Holden, 1991).\nUsing\nthis scheme, Sutton et al. (2007) demonstrates improvements on POS tagging and noun-\nphrase chunking using jointly trained CRFs. However the joint labeling requirement is a\nlimitation because such data is not often available. Miller et al. (2000) achieves performance\nimprovements by jointly training NER, parsing, and relation extraction in a statistical\nparsing model. The joint labeling requirement problem was weakened using a predictor to\n\ufb01ll in the missing annotations.\nAndo and Zhang (2005) propose a setup that works around the joint labeling\nrequirements. They de\ufb01ne linear models of the form fi(x) = w\u22a4\ni \u03a6(x) + v\u22a4\ni \u0398\u03a8(x) where\nfi is the classi\ufb01er for the i-th task with parameters wi and vi. Notations \u03a6(x) and \u03a8(x)\nrepresent engineered features for the pattern x. Matrix \u0398 maps the \u03a8(x) features into a low\ndimensional subspace common across all tasks. Each task is trained using its own examples\nwithout a joint labeling requirement. The learning procedure alternates the optimization\nof wi and vi for each task, and the optimization of \u0398 to minimize the average loss for all\nexamples in all tasks. The authors also consider auxiliary unsupervised tasks for predicting\nsubstructures. They report excellent results on several tasks, including POS and NER.\n26\narXiv\nNatural Language Processing (almost) from Scratch\nLookup Table\nLinear\nLookup Table\nLinear\nHardTanh\nHardTanh\nLinear\nTask 1\nLinear\nTask 2\nM 2\n(t1) \u00d7 \u00b7\nM 2\n(t2) \u00d7 \u00b7\nLTW 1\n...\nLTW K\nM 1 \u00d7 \u00b7\nn1\nhu\nn1\nhu\nn2\nhu,(t1) = #tags\nn2\nhu,(t2) = #tags\nFigure 5: Example of multitasking with NN. Task 1 and Task 2 are two tasks trained with\nthe window approach architecture presented in Figure 1. Lookup tables as well as the \ufb01rst\nhidden layer are shared. The last layer is task speci\ufb01c. The principle is the same with more\nthan two tasks.\n5.2 Multi-Task Benchmark Results\nTable 9 reports results obtained by jointly trained models for the POS, CHUNK, NER and\nSRL tasks using the same setup as Section 4.5. We trained jointly POS, CHUNK and NER\nusing the window approach network. As we mentioned earlier, SRL can be trained only\nwith the sentence approach network, due to long-range dependencies related to the verb\npredicate. We thus also trained all four tasks using the sentence approach network. In\nboth cases, all models share the lookup table parameters (2). The parameters of the \ufb01rst\nlinear layers (4) were shared in the window approach case (see Figure 5), and the \ufb01rst the\nconvolution layer parameters (6) were shared in the sentence approach networks.\nFor the window approach, best results were obtained by enlarging the \ufb01rst hidden layer\nsize to n1\nhu = 500 (chosen by validation) in order to account for its shared responsibilities.\nWe used the same architecture than SRL for the sentence approach network. The word\nembedding dimension was kept constant d0 = 50 in order to reuse the language models\nof Section 4.5.\nTraining was achieved by minimizing the loss averaged across all tasks. This is easily\nachieved with stochastic gradient by alternatively picking examples for each task and\napplying (17) to all the parameters of the corresponding model, including the shared\nparameters. Note that this gives each task equal weight. Since each task uses the training\nsets described in Table 1, it is worth noticing that examples can come from quite di\ufb00erent\n27\narXiv\nCollobert, Weston, Bottou, Karlen, Kavukcuoglu and Kuksa\nApproach\nPOS\nCHUNK\nNER\nSRL\n(PWA)\n(F1)\n(F1)\nBenchmark Systems\n97.24\n94.29\n89.31\n77.92\nNN+SLL+LM2\n97.20\n93.63\n88.67\n74.15\nNN+SLL+LM2+Su\ufb03x2\n97.29\n\u2013\n\u2013\n\u2013\nNN+SLL+LM2+Gazetteer\n\u2013\n\u2013\n89.59\n\u2013\nNN+SLL+LM2+POS\n\u2013\n94.32\n88.67\n\u2013\nNN+SLL+LM2+CHUNK\n\u2013\n\u2013\n\u2013\n74.72\nTable 10:\nComparison in generalization performance of benchmark NLP systems with\nour neural networks (NNs) using increasing task-speci\ufb01c engineering. We report results\nobtained with a network trained without the extra task-speci\ufb01c features (Section 5) and\nwith the extra task-speci\ufb01c features described in Section 6. The POS network was trained\nwith two character word su\ufb03xes; the NER network was trained using the small CoNLL\n2003 gazetteer; the CHUNK and NER networks were trained with additional POS features;\nand \ufb01nally, the SRL network was trained with additional CHUNK features.\ndatasets. The generalization performance for each task was measured using the traditional\ntesting data speci\ufb01ed in Table 1. Fortunately, none of the training and test sets overlap\nacross tasks.\nWhile we \ufb01nd worth mentioning that MTL can produce a single uni\ufb01ed architecture that\nperforms well for all these tasks, no (or only marginal) improvements were obtained with\nthis approach compared to training separate architectures per task (which still use semi-\nsupervised learning, which is somehow the most important MTL task). The next section\nshows we can leverage known correlations between tasks in more direct manner.\n6. The Temptation\nResults so far have been obtained by staying (almost15) true to our from scratch philosophy.\nWe have so far avoided specializing our architecture for any task, disregarding a lot of useful\na priori NLP knowledge. We have shown that, thanks to large unlabeled datasets, our\ngeneric neural networks can still achieve close to state-of-the-art performance by discovering\nuseful features. This section explores what happens when we increase the level of task-\nspeci\ufb01c engineering in our systems by incorporating some common techniques from the\nNLP literature. We often obtain further improvements. These \ufb01gures are useful to quantify\nhow far we went by leveraging large datasets instead of relying on a priori knowledge.\n6.1 Su\ufb03x Features\nWord su\ufb03xes in many western languages are strong predictors of the syntactic function\nof the word and therefore can bene\ufb01t the POS system. For instance, Ratnaparkhi (1996)\n15. We did some basic preprocessing of the raw input words as described in Section 3.4, hence the \u201calmost\u201d\nin the title of this article. A completely from scratch approach would presumably not know anything\nabout words at all and would work from letters only (or, taken to a further extreme, from speech or\noptical character recognition, as humans do).\n28\narXiv\nNatural Language Processing (almost) from Scratch\nuses inputs representing word su\ufb03xes and pre\ufb01xes up to four characters. We achieve this\nin the POS task by adding discrete word features (Section 3.1.1) representing the last two\ncharacters of every word. The size of the su\ufb03x dictionary was 455. This led to a small\nimprovement of the POS performance (Table 10, row NN+SLL+LM2+Su\ufb03x2). We also tried\nsu\ufb03xes obtained with the Porter (1980) stemmer and obtained the same performance as\nwhen using two character su\ufb03xes.\n6.2 Gazetteers\nState-of-the-art NER systems often use a large dictionary containing well known named\nentities (e.g. Florian et al., 2003).\nWe restricted ourselves to the gazetteer provided\nby the CoNLL challenge, containing 8, 000 locations, person names, organizations, and\nmiscellaneous entities. We trained a NER network with 4 additional word features indicating\n(feature \u201con\u201d or \u201co\ufb00\u201d) whether the word is found in the gazetteer under one of these four\ncategories. The gazetteer includes not only words, but also chunks of words. If a sentence\nchunk is found in the gazetteer, then all words in the chunk have their corresponding\ngazetteer feature turned to \u201con\u201d.\nThe resulting system displays a clear performance\nimprovement (Table 10, row NN+SLL+LM2+Gazetteer), slightly outperforming the baseline.\nA plausible explanation of this large boost over the network using only the language model\nis that gazeetters include word chunks, while we use only the word representation of our\nlanguage model. For example, \u201cunited\u201d and \u201cbicycle\u201d seen separately are likely to be non-\nentities, while \u201cunited bicycle\u201d might be an entity, but catching it would require higher\nlevel representations of our language model.\n6.3 Cascading\nWhen one considers related tasks, it is reasonable to assume that tags obtained for one task\ncan be useful for taking decisions in the other tasks. Conventional NLP systems often use\nfeatures obtained from the output of other preexisting NLP systems. For instance, Shen\nand Sarkar (2005) describe a chunking system that uses POS tags as input; Florian et al.\n(2003) describes a NER system whose inputs include POS and CHUNK tags, as well as\nthe output of two other NER classi\ufb01ers. State-of-the-art SRL systems exploit parse trees\n(Gildea and Palmer, 2002; Punyakanok et al., 2005), related to CHUNK tags, and built\nusing POS tags (Charniak, 2000; Collins, 1999).\nTable 10 reports results obtained for the CHUNK and NER tasks by adding discrete\nword features (Section 3.1.1) representing the POS tags. In order to facilitate comparisons,\ninstead of using the more accurate tags from our POS network, we use for each task the\nPOS tags provided by the corresponding CoNLL challenge. We also report results obtained\nfor the SRL task by adding word features representing the CHUNK tags (also provided by\nthe CoNLL challenge). We consistently obtain moderate improvements.\n6.4 Ensembles\nConstructing ensembles of classi\ufb01ers is a proven way to trade computational e\ufb03ciency for\ngeneralization performance (Bell et al., 2007). Therefore it is not surprising that many\nNLP systems achieve state-of-the-art performance by combining the outputs of multiple\n29\narXiv\nCollobert, Weston, Bottou, Karlen, Kavukcuoglu and Kuksa\nApproach\nPOS\nCHUNK\nNER\n(PWA)\n(F1)\n(F1)\nBenchmark Systems\n97.24\n94.29\n89.31\nNN+SLL+LM2+POS\nworst\n97.29\n93.99\n89.35\nNN+SLL+LM2+POS\nmean\n97.31\n94.17\n89.65\nNN+SLL+LM2+POS\nbest\n97.35\n94.32\n89.86\nNN+SLL+LM2+POS\nvoting ensemble\n97.37\n94.34\n89.70\nNN+SLL+LM2+POS\njoined ensemble\n97.30\n94.35\n89.67\nTable 11: Comparison in generalization performance for POS, CHUNK and NER tasks of\nthe networks obtained using by combining ten training runs with di\ufb00erent initialization.\nclassi\ufb01ers. For instance, Kudo and Matsumoto (2001) use an ensemble of classi\ufb01ers trained\nwith di\ufb00erent tagging conventions (see Section 3.2.3). Winning a challenge is of course a\nlegitimate objective. Yet it is often di\ufb03cult to \ufb01gure out which ideas are most responsible\nfor the state-of-the-art performance of a large ensemble.\nBecause neural networks are nonconvex, training runs with di\ufb00erent initial parameters\nusually give di\ufb00erent solutions.\nTable 11 reports results obtained for the CHUNK and\nNER task after ten training runs with random initial parameters. Voting the ten network\noutputs on a per tag basis (\u201cvoting ensemble\u201d) leads to a small improvement over the average\nnetwork performance. We have also tried a more sophisticated ensemble approach: the ten\nnetwork output scores (before sentence-level likelihood) were combined with an additional\nlinear layer (4) and then fed to a new sentence-level likelihood (13). The parameters of\nthe combining layers were then trained on the existing training set, while keeping the ten\nnetworks \ufb01xed (\u201cjoined ensemble\u201d). This approach did not improve on simple voting.\nThese ensembles come of course at the expense of a ten fold increase of the running\ntime. On the other hand, multiple training times could be improved using smart sampling\nstrategies (Neal, 1996).\nWe can also observe that the performance variability among the ten networks is not very\nlarge. The local minima found by the training algorithm are usually good local minima,\nthanks to the oversized parameter space and to the noise induced by the stochastic gradient\nprocedure (LeCun et al., 1998). In order to reduce the variance in our experimental results,\nwe always use the same initial parameters for networks trained on the same task (except of\ncourse for the results reported in Table 11.)\n6.5 Parsing\nGildea and Palmer (2002) o\ufb00er several arguments suggesting that syntactic parsing is a\nnecessary prerequisite for the SRL task. The CoNLL 2005 SRL benchmark task provides\nparse trees computed using both the Charniak (2000) and Collins (1999) parsers. State-of-\nthe-art systems often exploit additional parse trees such as the k top ranking parse trees\n(Koomen et al., 2005; Haghighi et al., 2005).\nIn contrast our SRL networks so far do not use parse trees at all. They rely instead\non internal representations transferred from a language model trained with an objective\n30\narXiv\nNatural Language Processing (almost) from Scratch\nlevel 0\nS\nNP\nThe luxury auto maker\nb-np\ni-np\ni-np\ne-np\nNP\nlast year\nb-np e-np\nVP\nsold\ns-vp\nNP\n1,214 cars\nb-np e-np\nPP\nin\ns-pp\nNP\nthe U.S.\nb-np e-np\nlevel 1\nS\nThe luxury auto maker last year\no\no\no\no\no\no\nVP\nsold 1,214 cars\nb-vp i-vp e-vp\nPP\nin\nthe U.S.\nb-pp i-pp e-pp\nlevel 2\nS\nThe luxury auto maker last year\no\no\no\no\no\no\nVP\nsold 1,214 cars in\nthe U.S.\nb-vp i-vp i-vp i-vp i-vp e-vp\nFigure 6: Charniak parse tree for the sentence \u201cThe luxury auto maker last year sold 1,214\ncars in the U.S.\u201d. Level 0 is the original tree. Levels 1 to 4 are obtained by successively\ncollapsing terminal tree branches. For each level, words receive tags describing the segment\nassociated with the corresponding leaf. All words receive tag \u201cO\u201d at level 3 in this example.\nfunction that captures a lot of syntactic information (see Section 4.6).\nIt is therefore\nlegitimate to question whether this approach is an acceptable lightweight replacement for\nparse trees.\nWe answer this question by providing parse tree information as additional input features\nto our system. We have limited ourselves to the Charniak parse tree provided with the\nCoNLL 2005 data.\nConsidering that a node in a syntactic parse tree assigns a label\nto a segment of the parsed sentence, we propose a way to feed (partially) this labeled\nsegmentation to our network, through additional lookup tables.\nEach of these lookup\ntables encode labeled segments of each parse tree level (up to a certain depth). The labeled\nsegments are fed to the network following a IOBES tagging scheme (see Sections 3.2.3\nand 3.1.1). As there are 40 di\ufb00erent phrase labels in WSJ, each additional tree-related\nlookup tables has 161 entries (40 \u00d7 4 + 1) corresponding to the IBES segment tags, plus the\nextra O tag.\nWe call level 0 the information associated with the leaves of the original Charniak parse\ntree. The lookup table for level 0 encodes the corresponding IOBES phrase tags for each\nwords. We obtain levels 1 to 4 by repeatedly trimming the leaves as shown in Figure 6. We\nlabeled \u201cO\u201d words belonging to the root node \u201cS\u201d, or all words of the sentence if the root\nitself has been trimmed.\nExperiments were performed using the LM2 language model using the same network\narchitectures (see Table 5) and using additional lookup tables of dimension 5 for each\n31\narXiv\nCollobert, Weston, Bottou, Karlen, Kavukcuoglu and Kuksa\nApproach\nSRL\n(valid)\n(test)\nBenchmark System (six parse trees)\n77.35\n77.92\nBenchmark System (top Charniak parse tree only)\n74.76\n\u2013\nNN+SLL+LM2\n72.29\n74.15\nNN+SLL+LM2+Charniak (level 0 only)\n74.44\n75.65\nNN+SLL+LM2+Charniak (levels 0 & 1)\n74.50\n75.81\nNN+SLL+LM2+Charniak (levels 0 to 2)\n75.09\n76.05\nNN+SLL+LM2+Charniak (levels 0 to 3)\n75.12\n75.89\nNN+SLL+LM2+Charniak (levels 0 to 4)\n75.42\n76.06\nNN+SLL+LM2+CHUNK\n\u2013\n74.72\nNN+SLL+LM2+PT0\n\u2013\n75.49\nTable 12:\nGeneralization performance on the SRL task of our NN architecture compared\nwith the benchmark system. We show performance of our system fed with di\ufb00erent levels\nof depth of the Charniak parse tree. We report previous results of our architecture with no\nparse tree as a baseline. Koomen et al. (2005) report test and validation performance using\nsix parse trees, as well as validation performance using only the top Charniak parse tree.\nFor comparison purposes, we hence also report validation performance. Finally, we report\nour performance with the CHUNK feature, and compare it against a level 0 feature PT0\nobtained by our network.\nparse tree level. Table 12 reports the performance improvements obtained by providing\nincreasing levels of parse tree information. Level 0 alone increases the F1 score by almost\n1.5%. Additional levels yield diminishing returns. The top performance reaches 76.06% F1\nscore. This is not too far from the state-of-the-art system which we note uses six parse\ntrees instead of one. Koomen et al. (2005) also report a 74.76% F1 score on the validation\nset using only the Charniak parse tree. Using the \ufb01rst three parse tree levels, we reach this\nperformance on the validation set.\nWe also reported in Table 12 our previous performance obtained with the CHUNK\nfeature (see Table 10). It is surprising to observe that adding chunking features into the\nsemantic role labeling network performs signi\ufb01cantly worse than adding features describing\nthe level 0 of the Charniak parse tree (Table 12). Indeed, if we ignore the label pre\ufb01xes\n\u201cBIES\u201d de\ufb01ning the segmentation, the parse tree leaves (at level 0) and the chunking\nhave identical labeling. However, the parse trees identify leaf sentence segments that are\noften smaller than those identi\ufb01ed by the chunking tags, as shown by Hollingshead et al.\n(2005).16\nInstead of relying on Charniak parser, we chose to train a second chunking\nnetwork to identify the segments delimited by the leaves of the Penn Treebank parse trees\n(level 0). Our network achieved 92.25% F1 score on this task (we call it PT0), while we\n16. As in (Hollingshead et al., 2005), consider the sentence and chunk labels \u201c(NP They) (VP are starting\nto buy) (NP growth stocks)\u201d. The parse tree can be written as \u201c(S (NP They) (VP are (VP starting (S\n(VP to (VP buy (NP growth stocks)))))))\u201d. The tree leaves segmentation is thus given by \u201c(NP They)\n(VP are) (VP starting) (VP to) (VP buy) (NP growth stocks)\u201d.\n32\narXiv\nNatural Language Processing (almost) from Scratch\nevaluated Charniak performance as 91.94% on the same task. As shown in Table 12, feeding\nour own PT0 prediction into the SRL system gives similar performance to using Charniak\npredictions, and is consistently better than the CHUNK feature.\n6.6 Word Representations\nIn Section 4, we adapted our neural network architecture for training a language model task.\nBy leveraging a large amount of unlabeled text data, we induced word embeddings which\nwere shown to boost generalization performance on all tasks. While we chose to stick with\none single architecture, other ways to induce word representations exist. Mnih and Hinton\n(2007) proposed a related language model approach inspired from Restricted Boltzmann\nMachines. However, word representations are perhaps more commonly infered from n-gram\nlanguage modelling rather than smoothed language models. One popular approach is the\nBrown clustering algorithm (Brown et al., 1992a), which builds hierachical word clusters\nby maximizing the bigram\u2019s mutual information.\nThe induced word representation has\nbeen used with success in a wide variety of NLP tasks, including POS (Sch\u00a8utze, 1995),\nNER (Miller et al., 2004; Ratinov and Roth, 2009), or parsing (Koo et al., 2008). Other\nrelated approaches exist, like phrase clustering (Lin and Wu, 2009) which has been shown\nto work well for NER. Finally, Huang and Yates (2009) have recently proposed a smoothed\nlanguage modelling approach based on a Hidden Markov Model, with success on POS and\nChunking tasks.\nWhile a comparison of all these word representations is beyond the scope of this paper,\nit is rather fair to question the quality of our word embeddings compared to a popular NLP\napproach. In this section, we report a comparison of our word embeddings against Brown\nclusters, when used as features into our neural network architecture. We report as baseline\nprevious results where our word embeddings are \ufb01ne-tuned for each task. We also report\nperformance when our embeddings are kept \ufb01xed during task-speci\ufb01c training. Since convex\nmachine learning algorithms are common practice in NLP, we \ufb01nally report performances\nfor the convex version of our architecture.\nFor the convex experiments, we considered the linear version of our neural networks\n(instead of having several linear layers interleaved with a non-linearity). While we always\npicked the sentence approach for SRL, we had to consider the window approach in this\nparticular convex setup, as the sentence approach network (see Figure 2) includes a Max\nlayer.\nHaving only one linear layer in our neural network is not enough to make our\narchitecture convex: all lookup-tables (for each discrete feature) must also be \ufb01xed. The\nword-lookup table is simply \ufb01xed to the embeddings obtained from our language model\nLM2. All other discrete feature lookup-tables (caps, POS, Brown Clusters...) are \ufb01xed to a\nstandard sparse representation. Using the notation introduced in Section 3.1.1, if LTW k is\nthe lookup-table of the kth discrete feature, we have W k \u2208R|Dk|\u00d7|Dk| and the representation\nof the discrete input w is obtained with:\nLTW k(w) = \u27e8W k\u27e91\nw =\n\u0012\n0, \u00b7 \u00b7 \u00b7 0,\n1\nat index w, 0, \u00b7 \u00b7 \u00b7 0\n\u0013T\n.\n(19)\nTraining our architecture in this convex setup with the sentence-level likelihood (13)\ncorresponds to training a CRF. In that respect, these convex experiments show the\nperformance of our word embeddings in a classical NLP framework.\n33\narXiv\nCollobert, Weston, Bottou, Karlen, Kavukcuoglu and Kuksa\nApproach\nPOS\nCHUNK\nNER\nSRL\n(PWA)\n(F1)\n(F1)\n(F1)\nNon-convex Approach\nLM2 (non-linear NN)\n97.29\n94.32\n89.59\n76.06\nLM2 (non-linear NN, \ufb01xed embeddings)\n97.10\n94.45\n88.79\n72.24\nBrown Clusters (non-linear NN, 130K words)\n96.92\n94.35\n87.15\n72.09\nBrown Clusters (non-linear NN, all words)\n96.81\n94.21\n86.68\n71.44\nConvex Approach\nLM2 (linear NN, \ufb01xed embeddings)\n96.69\n93.51\n86.64\n59.11\nBrown Clusters (linear NN, 130K words)\n96.56\n94.20\n86.46\n51.54\nBrown Clusters (linear NN, all words)\n96.28\n94.22\n86.63\n56.42\nTable 13:\nGeneralization performance of our neural network architecture trained with\nour language model (LM2) word embeddings, and with the word representations derived\nfrom the Brown Clusters. As before, all networks are fed with a capitalization feature.\nAdditionally, POS is using a word su\ufb03x of size 2 feature, CHUNK is fed with POS, NER\nuses the CoNLL 2003 gazetteer, and SRL is fed with levels 1\u20135 of the Charniak parse tree,\nas well as a verb position feature. We report performance with both convex and non-convex\narchitectures (300 hidden units for all tasks, with an additional 500 hidden units layer for\nSRL). We also provide results for Brown Clusters induced with a 130K word dictionary, as\nwell as Brown Clusters induced with all words of the given tasks.\nFollowing the Ratinov and Roth (2009) and Koo et al. (2008) setups, we generated 1, 000\nBrown clusters using the implementation17 from Liang (2005). To make the comparison\nfair, the clusters were \ufb01rst induced on the concatenation of Wikipedia and Reuters datasets,\nas we did in Section 4 for training our largest language model LM2, using a 130K word\ndictionary. This dictionary covers about 99% of the words in the training set of each task.\nTo cover the last 1%, we augmented the dictionary with the missing words (reaching about\n140K words) and induced Brown Clusters using the concatenation of WSJ, Wikipedia, and\nReuters.\nThe Brown clustering approach is hierarchical and generates a binary tree of clusters.\nEach word in the vocabulary is assigned to a node in the tree. Features are extracted from\nthis tree by considering the path from the root to the node containing the word of interest.\nFollowing Ratinov & Roth, we picked as features the path pre\ufb01xes of size 4, 6, 10 and 20. In\nthe non-convex experiments, we fed these four Brown Cluster features to our architecture\nusing four di\ufb00erent lookup tables, replacing our word lookup table. The size of the lookup\ntables was chosen to be 12 by validation. In the convex case, we used the classical sparse\nrepresentation (19), as for any other discrete feature.\nWe \ufb01rst report in Table 13 generalization performance of our best non-convex networks\ntrained with our LM2 language model and with Brown Cluster features. Our embeddings\nperform at least as well as Brown Clusters. Results are more mitigated in a convex setup.\nFor most task, going non-convex is better for both word representation types. In general,\n17. Available at http://www.eecs.berkeley.edu/~pliang/software.\n34\narXiv\nNatural Language Processing (almost) from Scratch\nTask\nFeatures\nPOS\nSu\ufb03x of size 2\nCHUNK\nPOS\nNER\nCoNLL 2003 gazetteer\nPT0\nPOS\nSRL\nPT0, verb position\nTable 14:\nFeatures used by SENNA implementation, for each task. In addition, all tasks\nuse \u201clow caps word\u201d and \u201ccaps\u201d features.\n\u201c\ufb01ne-tuning\u201d our embeddings for each task also gives an extra boost. Finally, using a better\nword coverage with Brown Clusters (\u201call words\u201d instead of \u201c130K words\u201d in Table 13) did\nnot help.\nMore complex features could be possibly combined instead of using a non-linear\nmodel. For instance, Turian et al. (2010) performed a comparison of Brown Clusters and\nembeddings trained in the same spirit as ours18, with additional features combining labels\nand tokens. We believe this type of comparison should be taken with care, as combining\na given feature with di\ufb00erent word representations might not have the same e\ufb00ect on each\nword representation.\n6.7 Engineering a Sweet Spot\nWe implemented a standalone version of our architecture, written in the C language.\nWe gave the name \u201cSENNA\u201d (Semantic/syntactic Extraction using a Neural Network\nArchitecture) to the resulting system. The parameters of each architecture are the ones\ndescribed in Table 5.\nAll the networks were trained separately on each task using the\nsentence-level likelihood (SLL). The word embeddings were initialized to LM2 embeddings,\nand then \ufb01ne-tuned for each task. We summarize features used by our implementation\nin Table 14, and we report performance achieved on each task in Table 15.\nThe runtime\nversion19 contains about 2500 lines of C code, runs in less than 150MB of memory, and needs\nless than a millisecond per word to compute all the tags. Table 16 compares the tagging\nspeeds for our system and for the few available state-of-the-art systems: the Toutanova et al.\n(2003) POS tagger20, the Shen et al. (2007) POS tagger21 and the Koomen et al. (2005) SRL\nsystem.22 All programs were run on a single 3GHz Intel core. The POS taggers were run\nwith Sun Java 1.6 with a large enough memory allocation to reach their top tagging speed.\n18. However they did not reach our embedding performance.\nThere are several di\ufb00erences in how they\ntrained their models that might explain this. Firstly, they may have experienced di\ufb03culties because\nthey train 50-dimensional embeddings for 269K distinct words using a comparatively small training set\n(RCV1, 37M words), unlikely to contain enough instances of the rare words. Secondly, they predict the\ncorrectness of the \ufb01nal word of each window instead of the center word (Turian et al., 2010), e\ufb00ectively\nrestricting the model to unidirectional prediction. Finally, they do not \ufb01ne tune their embeddings after\nunsupervised training.\n19. Available at http://ml.nec-labs.com/senna.\n20. Available at http://nlp.stanford.edu/software/tagger.shtml. We picked the 3.0 version (May 2010).\n21. Available at http://www.cis.upenn.edu/~xtag/spinal.\n22. Available at http://l2r.cs.uiuc.edu/~cogcomp/asoftware.php?skey=SRL.\n35\narXiv\nCollobert, Weston, Bottou, Karlen, Kavukcuoglu and Kuksa\nTask\nBenchmark\nSENNA\nPart of Speech (POS)\n(Accuracy)\n97.24 %\n97.29 %\nChunking (CHUNK)\n(F1)\n94.29 %\n94.32 %\nNamed Entity Recognition (NER)\n(F1)\n89.31 %\n89.59 %\nParse Tree level 0 (PT0)\n(F1)\n91.94 %\n92.25 %\nSemantic Role Labeling (SRL)\n(F1)\n77.92 %\n75.49 %\nTable 15: Performance of the engineered sweet spot (SENNA) on various tagging tasks. The\nPT0 task replicates the sentence segmentation of the parse tree leaves. The corresponding\nbenchmark score measures the quality of the Charniak parse tree leaves relative to the Penn\nTreebank gold parse trees.\nPOS System\nRAM (MB)\nTime (s)\nToutanova et al. (2003)\n800\n64\nShen et al. (2007)\n2200\n833\nSENNA\n32\n4\nSRL System\nRAM (MB)\nTime (s)\nKoomen et al. (2005)\n3400\n6253\nSENNA\n124\n51\nTable 16:\nRuntime speed and memory consumption comparison between state-of-the-art\nsystems and our approach (SENNA). We give the runtime in seconds for running both\nthe POS and SRL taggers on their respective testing sets. Memory usage is reported in\nmegabytes.\nThe beam size of the Shen tagger was set to 3 as recommended in the paper. Regardless\nof implementation di\ufb00erences, it is clear that our neural networks run considerably faster.\nThey also require much less memory. Our POS and SRL tagger runs in 32MB and 120MB\nof RAM respectively. The Shen and Toutanova taggers slow down signi\ufb01cantly when the\nJava machine is given less than 2.2GB and 800MB of RAM respectively, while the Koomen\ntagger requires at least 3GB of RAM.\nWe believe that a number of reasons explain the speed advantage of our system. First,\nour system only uses rather simple input features and therefore avoids the nonnegligible\ncomputation time associated with complex handcrafted features. Secondly, most network\ncomputations are dense matrix-vector operations. In contrast, systems that rely on a great\nnumber of sparse features experience memory latencies when traversing the sparse data\nstructures. Finally, our compact implementation is self-contained. Since it does not rely on\nthe outputs of disparate NLP system, it does not su\ufb00er from communication latency issues.\n7. Critical Discussion\nAlthough we believe that this contribution represents a step towards the \u201cNLP from scratch\u201d\nobjective, we are keenly aware that both our goal and our means can be criticized.\n36\narXiv\nNatural Language Processing (almost) from Scratch\nThe main criticism of our goal can be summarized as follows. Over the years, the NLP\ncommunity has developed a considerable expertise in engineering e\ufb00ective NLP features.\nWhy should they forget this painfully acquired expertise and instead painfully acquire\nthe skills required to train large neural networks? As mentioned in our introduction, we\nobserve that no single NLP task really covers the goals of NLP. Therefore we believe that\ntask-speci\ufb01c engineering (i.e. that does not generalize to other tasks) is not desirable. But\nwe also recognize how much our neural networks owe to previous NLP task-speci\ufb01c research.\nThe main criticism of our means is easier to address. Why did we choose to rely on a\ntwenty year old technology, namely multilayer neural networks? We were simply attracted\nby their ability to discover hidden representations using a stochastic learning algorithm\nthat scales linearly with the number of examples. Most of the neural network technology\nnecessary for our work has been described ten years ago (e.g. Le Cun et al., 1998). However,\nif we had decided ten years ago to train the language model network LM2 using a vintage\ncomputer, training would only be nearing completion today. Training algorithms that scale\nlinearly are most able to bene\ufb01t from such tremendous progress in computer hardware.\n8. Conclusion\nWe have presented a multilayer neural network architecture that can handle a number of\nNLP tasks with both speed and accuracy. The design of this system was determined by\nour desire to avoid task-speci\ufb01c engineering as much as possible. Instead we rely on large\nunlabeled datasets and let the training algorithm discover internal representations that\nprove useful for all the tasks of interest. Using this strong basis, we have engineered a fast\nand e\ufb03cient \u201call purpose\u201d NLP tagger that we hope will prove useful to the community.\nAcknowledgments\nWe acknowledge the persistent support of NEC for this research e\ufb00ort. We thank Yoshua\nBengio, Samy Bengio, Eric Cosatto, Vincent Etter, Hans-Peter Graf, Ralph Grishman, and\nVladimir Vapnik for their useful feedback and comments.\n37\narXiv\nCollobert, Weston, Bottou, Karlen, Kavukcuoglu and Kuksa\nAppendix A. Neural Network Gradients\nWe consider a neural network f\u03b8(\u00b7), with parameters \u03b8. We maximize the likelihood (8), or\nminimize ranking criterion (18), with respect to the parameters \u03b8, using stochastic gradient.\nBy negating the likelihood, we now assume it all corresponds to minimize a cost C(f\u03b8(\u00b7)),\nwith respect to \u03b8.\nFollowing the classical \u201cback-propagation\u201d derivations (LeCun, 1985; Rumelhart et al.,\n1986) and the modular approach shown in (Bottou, 1991), any feed-forward neural network\nwith L layers, like the ones shown in Figure 1 and Figure 2, can be seen as a composition\nof functions fl\n\u03b8(\u00b7), corresponding to each layer l:\nf\u03b8(\u00b7) = fL\n\u03b8 (fL\u22121\n\u03b8\n(. . . f1\n\u03b8 (\u00b7) . . .))\nPartionning the parameters of the network with respect to each layers 1 \u2264l \u2264L, we write:\n\u03b8 = (\u03b81, . . . , \u03b8l, . . . , \u03b8L) .\nWe are now interested in computing the gradients of the cost with respect to each \u03b8l.\nApplying the chain rule (generalized to vectors) we obtain the classical backpropagation\nrecursion:\n\u2202C\n\u2202\u03b8l\n=\n\u2202fl\n\u03b8\n\u2202\u03b8l\n\u2202C\n\u2202fl\n\u03b8\n(20)\n\u2202C\n\u2202fl\u22121\n\u03b8\n=\n\u2202fl\n\u03b8\n\u2202fl\u22121\n\u03b8\n\u2202C\n\u2202fl\n\u03b8\n.\n(21)\nIn other words, we \ufb01rst initialize the recursion by computing the gradient of the cost with\nrespect to the last layer output \u2202C/\u2202fL\n\u03b8 . Then each layer l computes the gradient respect\nto its own parameters with (20), given the gradient coming from its output \u2202C/\u2202fl\n\u03b8. To\nperform the backpropagation, it also computes the gradient with respect to its own inputs,\nas shown in (21). We now derive the gradients for each layer we used in this paper.\nLookup Table Layer\nGiven a matrix of parameters \u03b81 = W 1 and word (or discrete\nfeature) indices [w]T\n1 , the layer outputs the matrix:\nfl\n\u03b8([w]T\nl ) =\n\u0010\n\u27e8W\u27e91\n[w]1\n\u27e8W\u27e91\n[w]2\n. . .\n\u27e8W\u27e91\n[w]T\n\u0011\n.\nThe gradients of the weights \u27e8W\u27e91\ni are given by:\n\u2202C\n\u2202\u27e8W\u27e91\ni\n=\nX\n{1\u2264t\u2264T / [w]t=i}\n\u27e8\u2202C\n\u2202fl\n\u03b8\n\u27e91\ni\nThis sum equals zero if the index i in the lookup table does not corresponds to a word in\nthe sequence. In this case, the ith column of W does not need to be updated. As a Lookup\nTable Layer is always the \ufb01rst layer, we do not need to compute its gradients with respect\nto the inputs.\n38\narXiv\nNatural Language Processing (almost) from Scratch\nLinear Layer\nGiven parameters \u03b8l = (W l, bl), and an input vector fl\u22121\n\u03b8\nthe output is\ngiven by:\nfl\n\u03b8 = W lfl\u22121\n\u03b8\n+ bl .\n(22)\nThe gradients with respect to the parameters are then obtained with:\n\u2202C\n\u2202W l =\n\u0014 \u2202C\n\u2202fl\n\u03b8\n\u0015 h\nfl\u22121\n\u03b8\niT\nand \u2202C\n\u2202bl = \u2202C\n\u2202fl\n\u03b8\n,\n(23)\nand the gradients with respect to the inputs are computed with:\n\u2202C\n\u2202fl\u22121\n\u03b8\n=\nh\nW liT \u2202C\n\u2202fl\n\u03b8\n.\n(24)\nConvolution Layer\nGiven a input matrix fl\u22121\n\u03b8\n, a Convolution Layer fl\n\u03b8(\u00b7) applies a\nLinear Layer operation (22) successively on each window \u27e8fl\u22121\n\u03b8\n\u27e9dwin\nt\n(1 \u2264t \u2264T) of size\ndwin.\nUsing (23), the gradients of the parameters are thus given by summing over all\nwindows:\n\u2202C\n\u2202W l =\nT\nX\nt=1\n\u0014\n\u27e8\u2202C\n\u2202fl\n\u03b8\n\u27e91\nt\n\u0015 h\n\u27e8fl\u22121\n\u03b8\n\u27e9dwin\nt\niT\nand \u2202C\n\u2202bl =\nT\nX\nt=1\n\u27e8\u2202C\n\u2202fl\n\u03b8\n\u27e91\nt .\nAfter initializing the input gradients \u2202C/\u2202fl\u22121\n\u03b8\nto zero, we iterate (24) over all windows for\n1 \u2264t \u2264T, leading the accumulation23:\n\u27e8\u2202C\n\u2202fl\u22121\n\u03b8\n\u27e9dwin\nt\n+=\nh\nW liT\n\u27e8\u2202C\n\u2202fl\n\u03b8\n\u27e91\nt .\nMax Layer\nGiven a matrix fl\u22121\n\u03b8\n, the Max Layer computes\nh\nfl\n\u03b8\ni\ni = max\nt\nh\n\u27e8fl\u22121\n\u03b8\n\u27e91\nt\ni\ni and ai = argmax\nt\nh\n\u27e8fl\u22121\n\u03b8\n\u27e91\nt\ni\ni \u2200i ,\nwhere ai stores the index of the largest value. We only need to compute the gradient with\nrespect to the inputs, as this layer has no parameters. The gradient is given by\n\"\n\u27e8\u2202C\n\u2202fl\u22121\n\u03b8\n\u27e91\nt\n#\ni\n=\n( h\n\u27e8\u2202C\n\u2202fl\n\u03b8 \u27e91\nt\ni\ni\nif t = ai\n0\notherwise\n.\nHardTanh Layer\nGiven a vector fl\u22121\n\u03b8\n, and the de\ufb01nition of the HardTanh (5) we get\n\"\n\u2202C\n\u2202fl\u22121\n\u03b8\n#\ni\n=\n\uf8f1\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f2\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f3\n0\nif\nh\nfl\u22121\n\u03b8\ni\ni < \u22121\nh\n\u2202C\n\u2202fl\n\u03b8\ni\ni\nif \u22121 <=\nh\nfl\u22121\n\u03b8\ni\ni <= 1\n0\nif\nh\nfl\u22121\n\u03b8\ni\ni > 1\n,\nif we ignore non-di\ufb00erentiability points.\n23. We denote \u201c+=\u201d any accumulation operation.\n39\narXiv\nCollobert, Weston, Bottou, Karlen, Kavukcuoglu and Kuksa\nWord-Level Log-Likelihood\nThe network outputs a score [f\u03b8]i for each tag indexed by\ni. Following (11), if y is the true tag for a given example, the stochastic score to minimize\ncan be written as\nC(f\u03b8) = logadd\nj\n[f\u03b8]j \u2212[f\u03b8]y\nConsidering the de\ufb01nition of the logadd (10), the gradient with respect to f\u03b8 is given by\n\u2202C\n\u2202[f\u03b8]i\n=\ne[f\u03b8]i\nP\nk e[f\u03b8]k \u22121i=y\n\u2200i.\nSentence-Level Log-Likelihood\nThe network outputs a matrix where each element\n[f\u03b8]i, t gives a score for tag i at word t. Given a tag sequence [y]T\n1 and a input sequence [x]T\n1 ,\nwe maximize the likelihood (13), which corresponds to minimizing the score\nC(f\u03b8, A) = logadd\n\u2200[j]T\n1\ns([x]T\n1 , [j]T\n1 , \u02dc\u03b8)\n|\n{z\n}\nClogadd\n\u2212s([x]T\n1 , [y]T\n1 , \u02dc\u03b8) ,\nwith\ns([x]T\n1 , [y]T\n1 , \u02dc\u03b8) =\nT\nX\nt=1\n\u0010\n[A][y]t\u22121, [y]t + [f\u03b8][y]t, t\n\u0011\n.\nWe \ufb01rst initialize all gradients to zero\n\u2202C\n\u2202\n\u0002\nf\u03b8\n\u0003\ni, t\n= 0 \u2200i, t and\n\u2202C\n\u2202[A]i, j\n= 0\n\u2200i, j .\nWe then accumulate gradients over the second part of the cost \u2212s([x]T\n1 , [y]T\n1 , \u02dc\u03b8), which\ngives:\n\u2202C\n\u2202\n\u0002\nf\u03b8\n\u0003\n[y]t, t\n+= 1\n\u2202C\n\u2202[A][y]t\u22121, [y]t\n+= 1\n\u2200t .\nWe now need to accumulate the gradients over the \ufb01rst part of the cost, that is Clogadd.\nWe di\ufb00erentiate Clogadd by applying the chain rule through the recursion (14). First we\ninitialize our recursion with\n\u2202Clogadd\n\u2202\u03b4T (i) =\ne\u03b4T (i)\nP\nk e\u03b4T (k)\n\u2200i .\nWe then compute iteratively:\n\u2202Clogadd\n\u2202\u03b4t\u22121(i) =\nX\nj\n\u2202Clogadd\n\u2202\u03b4t(j)\ne\u03b4t\u22121(i)+[A]i, j\nP\nk e\u03b4t\u22121(k)+[A]k, j ,\n(25)\n40\narXiv\nNatural Language Processing (almost) from Scratch\nwhere at each step t of the recursion we accumulate of the gradients with respect to the\ninputs f\u03b8, and the transition scores [A]i, j:\n\u2202C\n\u2202\n\u0002\nf\u03b8\n\u0003\ni, t\n+=\u2202Clogadd\n\u2202\u03b4t(i)\n\u2202\u03b4t(i)\n\u2202\n\u0002\nf\u03b8\n\u0003\ni, t\n= \u2202Clogadd\n\u2202\u03b4t(i)\n\u2202C\n\u2202[A]i, j\n+=\u2202Clogadd\n\u2202\u03b4t(j)\n\u2202\u03b4t(j)\n\u2202[A]i, j\n= \u2202Clogadd\n\u2202\u03b4t(j)\ne\u03b4t\u22121(i)+[A]i, j\nP\nk e\u03b4t\u22121(k)+[A]k, j .\nRanking Criterion\nWe use the ranking criterion (18) for training our language model.\nIn this case, given a \u201cpositive\u201d example x and a \u201cnegative\u201d example x(w), we want to\nminimize:\nC(f\u03b8(x), f\u03b8(xw)) = max\nn\n0 , 1 \u2212f\u03b8(x) + f\u03b8(x(w))\no\n.\n(26)\nIgnoring the non-di\ufb00erentiability of max(0, \u00b7) in zero, the gradient is simply given by:\n \n\u2202C\n\u2202f\u03b8(x)\n\u2202C\n\u2202f\u03b8(xw)\n!\n=\n\uf8f1\n\uf8f4\n\uf8f4\n\uf8f2\n\uf8f4\n\uf8f4\n\uf8f3\n\u0012\n\u22121\n1\n\u0013\nif 1 \u2212f\u03b8(x) + f\u03b8(x(w)) > 0\n\u0012\n0\n0\n\u0013\notherwise\n.\nReferences\nR. K. Ando and T. Zhang. A framework for learning predictive structures from multiple\ntasks and unlabeled data. JMLR, 6:1817\u20131953, 11 2005.\nR. M. Bell, Y. Koren, and C. Volinsky. The BellKor solution to the Net\ufb02ix Prize. Technical\nreport, AT&T Labs, 2007. http://www.research.att.com/~volinsky/netflix.\nY. Bengio and R. Ducharme. A neural probabilistic language model. In NIPS 13, 2001.\nY. Bengio, P. Lamblin, D. Popovici, and H. Larochelle. Greedy layer-wise training of deep\nnetworks. In Advances in Neural Information Processing Systems, NIPS 19, 2007.\nY. Bengio, J. Louradour, R. Collobert, and J. Weston. Curriculum learning. In International\nConference on Machine Learning, ICML, 2009.\nL. Bottou. Stochastic gradient learning in neural networks. In Proceedings of Neuro-N\u02c6\u0131mes\n91, Nimes, France, 1991. EC2.\nL. Bottou. Online algorithms and stochastic approximations. In David Saad, editor, Online\nLearning and Neural Networks. Cambridge University Press, Cambridge, UK, 1998.\nL. Bottou and P. Gallinari. A framework for the cooperation of learning algorithms. In\nD. Touretzky and R. Lippmann, editors, Advances in Neural Information Processing\nSystems, volume 3. Morgan Kaufmann, Denver, 1991.\nL. Bottou, Y. LeCun, and Yoshua Bengio. Global training of document processing systems\nusing graph transformer networks. In Proc. of Computer Vision and Pattern Recognition,\npages 489\u2013493, Puerto-Rico, 1997. IEEE.\n41\narXiv\nCollobert, Weston, Bottou, Karlen, Kavukcuoglu and Kuksa\nJ. S. Bridle. Probabilistic interpretation of feedforward classi\ufb01cation network outputs, with\nrelationships to statistical pattern recognition. In F. Fogelman Souli\u00b4e and J. H\u00b4erault,\neditors, Neurocomputing: Algorithms, Architectures and Applications, pages 227\u2013236.\nNATO ASI Series, 1990.\nP. F. Brown, P. V. deSouza, R. L. Mercer, V. J. D. Pietra, and J C. Lai. Class-based n-gram\nmodels of natural language. Computational Linguistics, 18(4):467\u2013479, 1992a.\nP. F. Brown, V. J. Della Pietra, R. L. Mercer, S. A. Della Pietra, and J. C. Lai. An estimate\nof an upper bound for the entropy of english. Computational Linguistics, 18(1):31\u201341,\n1992b.\nC. J. C. Burges, R. Ragno, and Quoc Viet Le. Learning to rank with nonsmooth cost\nfunctions.\nIn B. Sch\u00a8olkopf, J. Platt, and T. Ho\ufb00man, editors, Advances in Neural\nInformation Processing Systems 19, pages 193\u2013200. MIT Press, Cambridge, MA, 2007.\nR. Caruana. Multitask Learning. Machine Learning, 28(1):41\u201375, 1997.\nO. Chapelle, B. Schlkopf, and A. Zien. Semi-Supervised Learning. Adaptive computation\nand machine learning. MIT Press, Cambridge, Mass., USA, 09 2006.\nE. Charniak. A maximum-entropy-inspired parser. Proceedings of the \ufb01rst conference on\nNorth American chapter of the Association for Computational Linguistics, pages 132\u2013139,\n2000.\nH. L. Chieu. Named entity recognition with a maximum entropy approach. In In Proceedings\nof the Seventh Conference on Natural Language Learning (CoNLL-2003, pages 160\u2013163,\n2003.\nN. Chomsky.\nThree models for the description of language.\nIRE Transactions on\nInformation Theory, 2(3):113\u2013124, September 1956.\nS. Cl\u00b4emen\u00b8con and N. Vayatis. Ranking the best instances. Journal of Machine Learning\nResearch, 8:2671\u20132699, December 2007.\nW. W. Cohen, R. E. Schapire, and Y. Singer. Learning to order things. Journal of Arti\ufb01cial\nIntelligence Research, 10:243\u2013270, 1998.\nT. Cohn and P. Blunsom. Semantic role labelling with tree conditional random \ufb01elds. In\nNinth Conference on Computational Natural Language (CoNLL), 2005.\nM. Collins.\nHead-Driven Statistical Models for Natural Language Parsing.\nPhD thesis,\nUniversity of Pennsylvania, 1999.\nR. Collobert. Large Scale Machine Learning. PhD thesis, Universit\u00b4e Paris VI, 2004.\nT. Cover and R. King. A convergent gambling estimate of the entropy of english. IEEE\nTransactions on Information Theory, 24(4):413\u2013421, July 1978.\n42\narXiv\nNatural Language Processing (almost) from Scratch\nR. Florian, A. Ittycheriah, H. Jing, and T. Zhang.\nNamed entity recognition through\nclassi\ufb01er combination.\nIn Proceedings of the seventh conference on Natural language\nlearning at HLT-NAACL 2003, pages 168\u2013171. Association for Computational Linguistics,\n2003.\nD. Gildea and D. Jurafsky. Automatic labeling of semantic roles. Computational Linguistics,\n28(3):245\u2013288, 2002.\nD. Gildea and M. Palmer. The necessity of parsing for predicate argument recognition.\nProceedings of the 40th Annual Meeting of the ACL, pages 239\u2013246, 2002.\nJ. Gim\u00b4enez and L. M`arquez.\nSVMTool:\nA general POS tagger generator based on\nsupport vector machines. In Proceedings of the 4th International Conference on Language\nResources and Evaluation (LREC\u201904), 2004.\nA. Haghighi, K. Toutanova, and C. D. Manning. A joint model for semantic role labeling.\nIn Proceedings of the Ninth Conference on Computational Natural Language Learning\n(CoNLL-2005). Association for Computational Linguistics, June 2005.\nZ. S. Harris. Mathematical Structures of Language. John Wiley & Sons Inc., 1968.\nD. Heckerman, D. M. Chickering, C. Meek, R. Rounthwaite, and C. Kadie. Dependency\nnetworks for inference, collaborative \ufb01ltering, and data visualization. Journal of Machine\nLearning Research, 1:49\u201375, 2001. ISSN 1532-4435.\nG. E. Hinton, S. Osindero, and Y.-W. Teh. A fast learning algorithm for deep belief nets.\nNeural Comp., 18(7):1527\u20131554, July 2006.\nK. Hollingshead, S. Fisher, and B. Roark.\nComparing and combining \ufb01nite-state and\ncontext-free parsers. In HLT \u201905: Proceedings of the conference on Human Language\nTechnology and Empirical Methods in Natural Language Processing, pages 787\u2013794.\nAssociation for Computational Linguistics, 2005.\nF. Huang and A. Yates. Distributional representations for handling sparsity in supervised\nsequence-labeling.\nIn Proceedings of the Association for Computational Linguistics\n(ACL), pages 495\u2013503. Association for Computational Linguistics, 2009.\nF. Jelinek. Continuous speech recognition by statistical methods. Proceedings of the IEEE,\n64(4):532\u2013556, 1976.\nT. Joachims. Transductive inference for text classi\ufb01cation using support vector machines.\nIn ICML, 1999.\nD. Klein and C. D. Manning. Natural language grammar induction using a constituent-\ncontext model.\nIn Thomas G. Dietterich, Suzanna Becker, and Zoubin Ghahramani,\neditors, Advances in Neural Information Processing Systems 14, pages 35\u201342. MIT Press,\nCambridge, MA, 2002.\nT. Koo, X. Carreras, and M. Collins.\nSimple semi-supervised dependency parsing.\nIn\nProceedings of the Association for Computational Linguistics (ACL), pages 595\u2013603, 2008.\n43\narXiv\nCollobert, Weston, Bottou, Karlen, Kavukcuoglu and Kuksa\nP. Koomen, V. Punyakanok, D. Roth, and W. Yih. Generalized inference with multiple\nsemantic role labeling systems (shared task paper). In Ido Dagan and Dan Gildea, editors,\nProc. of the Annual Conference on Computational Natural Language Learning (CoNLL),\npages 181\u2013184, 2005.\nT. Kudo and Y. Matsumoto. Chunking with support vector machines. In In Proceedings\nof the 2nd Meeting of the North American Association for Computational Linguistics:\nNAACL 2001, pages 1\u20138. Association for Computational Linguistics, 2001.\nT. Kudoh and Y. Matsumoto. Use of support vector learning for chunk identi\ufb01cation. In\nProceedings of CoNLL-2000 and LLL-2000, pages 142\u2013144, 2000.\nJ. La\ufb00erty, A. McCallum, and F. Pereira. Conditional random \ufb01elds: Probabilistic models\nfor segmenting and labeling sequence data. In Eighteenth International Conference on\nMachine Learning, ICML, 2001.\nY. Le Cun, L. Bottou, Y. Bengio, and P. Ha\ufb00ner.\nGradient based learning applied to\ndocument recognition. Proceedings of IEEE, 86(11):2278\u20132324, 1998.\nY. LeCun.\nA learning scheme for asymmetric threshold networks.\nIn Proceedings of\nCognitiva 85, pages 599\u2013604, Paris, France, 1985.\nY. LeCun, L. Bottou, G. B. Orr, and K.-R. M\u00a8uller. E\ufb03cient backprop. In G.B. Orr and\nK.-R. M\u00a8uller, editors, Neural Networks: Tricks of the Trade, pages 9\u201350. Springer, 1998.\nD. D. Lewis, Y. Yang, T. G. Rose, and F. Li. Rcv1: A new benchmark collection for text\ncategorization research. Journal of Machine Learning Research, 5:361\u2013397, 2004.\nP. Liang. Semi-supervised learning for natural language. Master\u2019s thesis, Massachusetts\nInstitute of Technology, 2005.\nP. Liang, H. Daum\u00b4e, III, and D. Klein. Structure compilation: trading structure for features.\nIn International conference on Machine learning (ICML), pages 592\u2013599. ACM, 2008.\nD. Lin and X. Wu.\nPhrase clustering for discriminative learning.\nIn Proceedings of\nthe Association for Computational Linguistics (ACL), pages 1030\u20131038. Association for\nComputational Linguistics, 2009.\nN. Littlestone. Learning quickly when irrelevant attributes abound: A new linear-threshold\nalgorithm. In Machine Learning, pages 285\u2013318, 1988.\nA. McCallum and Wei Li.\nEarly results for named entity recognition with conditional\nrandom \ufb01elds, feature induction and web-enhanced lexicons.\nIn Proceedings of the\nseventh conference on Natural language learning at HLT-NAACL 2003, pages 188\u2013191.\nAssociation for Computational Linguistics, 2003.\nD. McClosky, E. Charniak, and M. Johnson. E\ufb00ective self-training for parsing. Proceedings\nof HLT-NAACL 2006, 2006.\n44\narXiv\nNatural Language Processing (almost) from Scratch\nR. McDonald, K. Crammer, and F. Pereira. Flexible text segmentation with structured\nmultilabel classi\ufb01cation. In HLT \u201905: Proceedings of the conference on Human Language\nTechnology and Empirical Methods in Natural Language Processing, pages 987\u2013994.\nAssociation for Computational Linguistics, 2005.\nS. Miller, H. Fox, L. Ramshaw, and R. Weischedel. A novel use of statistical parsing to\nextract information from text.\n6th Applied Natural Language Processing Conference,\n2000.\nS. Miller, J. Guinness, and A. Zamanian.\nName tagging with word clusters and\ndiscriminative training. In Proceedings of HLT-NAACL, pages 337\u2013342, 2004.\nA Mnih and G. E. Hinton. Three new graphical models for statistical language modelling.\nIn International Conference on Machine Learning, ICML, pages 641\u2013648, 2007.\nG. Musillo and P. Merlo. Robust Parsing of the Proposition Bank. ROMAND 2006: Robust\nMethods in Analysis of Natural language Data, 2006.\nR. M. Neal. Bayesian Learning for Neural Networks. Number 118 in Lecture Notes in\nStatistics. Springer-Verlag, New York, 1996.\nD. Okanohara and J. Tsujii. A discriminative language model with pseudo-negative samples.\nProceedings of the 45th Annual Meeting of the ACL, pages 73\u201380, 2007.\nM. Palmer, D. Gildea, and P. Kingsbury. The proposition bank: An annotated corpus of\nsemantic roles. Comput. Linguist., 31(1):71\u2013106, 2005. ISSN 0891-2017.\nJ. Pearl. Probabilistic Reasoning in Intelligent Systems. Morgan Kaufman, San Mateo,\n1988.\nD. C. Plaut and G. E. Hinton. Learning sets of \ufb01lters using back-propagation. Computer\nSpeech and Language, 2:35\u201361, 1987.\nM. F. Porter. An algorithm for su\ufb03x stripping. Program, 14(3):130\u2013137, 1980.\nS. Pradhan, W. Ward, K. Hacioglu, J. Martin, and D. Jurafsky. Shallow semantic parsing\nusing support vector machines. Proceedings of HLT/NAACL-2004, 2004.\nS. Pradhan, K. Hacioglu, W. Ward, J. H. Martin, and D. Jurafsky. Semantic role chunking\ncombining complementary syntactic views. In Proceedings of the Ninth Conference on\nComputational Natural Language Learning (CoNLL-2005), pages 217\u2013220. Association\nfor Computational Linguistics, June 2005.\nV. Punyakanok, D. Roth, and W. Yih. The necessity of syntactic parsing for semantic role\nlabeling. In IJCAI, pages 1117\u20131123, 2005.\nL. Ratinov and D. Roth. Design challenges and misconceptions in named entity recognition.\nIn Proceedings of the Thirteenth Conference on Computational Natural Language Learning\n(CoNLL), pages 147\u2013155. Association for Computational Linguistics, 2009.\n45\narXiv\nCollobert, Weston, Bottou, Karlen, Kavukcuoglu and Kuksa\nA. Ratnaparkhi. A maximum entropy model for part-of-speech tagging. In Eric Brill and\nKenneth Church, editors, Proceedings of the Conference on Empirical Methods in Natural\nLanguage Processing, pages 133\u2013142. Association for Computational Linguistics, 1996.\nB. Rosenfeld and R. Feldman.\nUsing Corpus Statistics on Entities to Improve Semi-\nsupervised Relation Extraction from the Web. Proceedings of the 45th Annual Meeting\nof the ACL, pages 600\u2013607, 2007.\nD. E. Rumelhart, G. E. Hinton, and R. J. Williams.\nLearning internal representations\nby back-propagating errors. In D.E. Rumelhart and J. L. McClelland, editors, Parallel\nDistributed Processing: Explorations in the Microstructure of Cognition, volume 1, pages\n318\u2013362. MIT Press, 1986.\nH. Sch\u00a8utze. Distributional part-of-speech tagging. In Proceedings of the Association for\nComputational Linguistics (ACL), pages 141\u2013148. Morgan Kaufmann Publishers Inc.,\n1995.\nH. Schwenk and J. L. Gauvain.\nConnectionist language modeling for large vocabulary\ncontinuous speech recognition. In IEEE International Conference on Acoustics, Speech,\nand Signal Processing, pages 765\u2013768, 2002.\nF. Sha and F. Pereira. Shallow parsing with conditional random \ufb01elds. In NAACL \u201903:\nProceedings of the 2003 Conference of the North American Chapter of the Association for\nComputational Linguistics on Human Language Technology, pages 134\u2013141. Association\nfor Computational Linguistics, 2003.\nC. E. Shannon. Prediction and entropy of printed english. Bell Systems Technical Journal,\n30:50\u201364, 1951.\nH. Shen and A. Sarkar. Voting between multiple data representations for text chunking.\nAdvances in Arti\ufb01cial Intelligence, pages 389\u2013400, 2005.\nL. Shen, G. Satta, and A. K. Joshi. Guided learning for bidirectional sequence classi\ufb01cation.\nIn Proceedings of the 45th Annual Meeting of the Association for Computational\nLinguistics (ACL), 2007.\nN. A. Smith and J. Eisner. Contrastive estimation: Training log-linear models on unlabeled\ndata. In Proceedings of the 43rd Annual Meeting of the Association for Computational\nLinguistics (ACL), pages 354\u2013362. Association for Computational Linguistics, 2005.\nS. C. Suddarth and A. D. C. Holden. Symbolic-neural systems and the use of hints for\ndeveloping complex systems. International Journal of Man-Machine Studies, 35(3):291\u2013\n311, 1991.\nX. Sun, L.-P. Morency, D. Okanohara, and J. Tsujii. Modeling latent-dynamic in shallow\nparsing: a latent conditional model with improved inference. In COLING \u201908: Proceedings\nof the 22nd International Conference on Computational Linguistics, pages 841\u2013848.\nAssociation for Computational Linguistics, 2008.\n46\narXiv\nNatural Language Processing (almost) from Scratch\nC. Sutton and A. McCallum. Joint parsing and semantic role labeling. In Proceedings of\nCoNLL-2005, pages 225\u2013228, 2005a.\nC. Sutton and A. McCallum. Composition of conditional random \ufb01elds for transfer learning.\nProceedings of the conference on Human Language Technology and Empirical Methods in\nNatural Language Processing, pages 748\u2013754, 2005b.\nC. Sutton, A. McCallum, and K. Rohanimanesh. Dynamic Conditional Random Fields:\nFactorized Probabilistic Models for Labeling and Segmenting Sequence Data. JMLR, 8:\n693\u2013723, 2007.\nJ. Suzuki and H. Isozaki. Semi-supervised sequential labeling and segmentation using giga-\nword scale unlabeled data. In Proceedings of ACL-08: HLT, pages 665\u2013673, Columbus,\nOhio, June 2008. Association for Computational Linguistics.\nW. J. Teahan and J. G. Cleary. The entropy of english using ppm-based models. In In Data\nCompression Conference (DCC\u201996), pages 53\u201362. IEEE Computer Society Press, 1996.\nK. Toutanova, D. Klein, C. D. Manning, and Y. Singer. Feature-rich part-of-speech tagging\nwith a cyclic dependency network. In HLT-NAACL, 2003.\nJ. Turian, L. Ratinov, and Y. Bengio.\nWord representations:\nA simple and general\nmethod for semi-supervised learning. In Proceedings of the Association for Computational\nLinguistics (ACL), pages 384\u2013392. Association for Computational Linguistics, 2010.\nN. Ue\ufb03ng, G. Ha\ufb00ari, and A. Sarkar.\nTransductive learning for statistical machine\ntranslation. Proceedings of the 45th Annual Meeting of the ACL, pages 25\u201332, 2007.\nA. Waibel, T. Hanazawa, G. Hinton, K. Shikano, and K.J. Lang. Phoneme recognition\nusing time-delay neural networks. IEEE Transactions on Acoustics, Speech, and Signal\nProcessing, 37(3):328\u2013339, 1989.\nJ. Weston, F. Ratle, and R. Collobert. Deep learning via semi-supervised embedding. In\nProceedings of the 25th international conference on Machine learning, pages 1168\u20131175.\nACM, 2008.\n47\n",
        "sentence": " , 2014) for MRDA and SwDA, as these choices yielded the best results among all publicly available word2vec, GloVe, SENNA (Collobert, 2011; Collobert et al., 2011) and RNNLM (Mikolov et al.",
        "context": "They consider various data neighborhoods,\nincluding sentences of length dwin drawn from Ddwin. Their goal was however to perform\nwell on some tagging task on fully unsupervised data, rather than obtaining generic word\nembeddings useful for other tasks.\nof LM1, and trained for an additional three weeks on our second English corpus\n(Wikipedia+Reuters) using a dictionary size of 130,000 words.\n4.4 Embeddings\nBoth networks produce much more appealing word embeddings than in Section 3.4. Table 7\nPradhan et al. (2005)\n77.30%\nHaghighi et al. (2005)\n77.04%\n(d) SRL\nTable 2: State-of-the-art systems on four NLP tasks. Performance is reported in per-word\naccuracy for POS, and F1 score for CHUNK, NER and SRL. Systems in bold will be referred"
    },
    {
        "title": "Deep learning for efficient discriminative parsing",
        "author": [
            "Ronan Collobert"
        ],
        "venue": "In International Conference on Artificial Intelligence and Statistics,",
        "citeRegEx": "Collobert.,? \\Q2011\\E",
        "shortCiteRegEx": "Collobert.",
        "year": 2011,
        "abstract": "",
        "full_text": "",
        "sentence": " , 2014) for MRDA and SwDA, as these choices yielded the best results among all publicly available word2vec, GloVe, SENNA (Collobert, 2011; Collobert et al., 2011) and RNNLM (Mikolov et al.",
        "context": null
    },
    {
        "title": "AdobeMIT submission to the DSTC 4 Spoken Language Understanding pilot task",
        "author": [
            "Ji Young Lee",
            "Trung H. Bui",
            "Hung H. Bui"
        ],
        "venue": "In 7th International Workshop on Spoken Dialogue Systems (IWSDS)",
        "citeRegEx": "Dernoncourt et al\\.,? \\Q2016\\E",
        "shortCiteRegEx": "Dernoncourt et al\\.",
        "year": 2016,
        "abstract": "",
        "full_text": "",
        "sentence": " SVM: (Dernoncourt et al., 2016).",
        "context": null
    },
    {
        "title": "Long short-term memory",
        "author": [
            "Hochreiter",
            "Schmidhuber1997] Sepp Hochreiter",
            "J\u00fcrgen Schmidhuber"
        ],
        "venue": "Neural computation,",
        "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E",
        "shortCiteRegEx": "Hochreiter et al\\.",
        "year": 1997,
        "abstract": "",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "The ICSI meeting corpus",
        "author": [
            "Janin et al.2003] Adam Janin",
            "Don Baron",
            "Jane Edwards",
            "Dan Ellis",
            "David Gelbart",
            "Nelson Morgan",
            "Barbara Peskin",
            "Thilo Pfau",
            "Elizabeth Shriberg",
            "Andreas Stolcke"
        ],
        "venue": "In Acoustics, Speech, and Signal Processing,",
        "citeRegEx": "Janin et al\\.,? \\Q2003\\E",
        "shortCiteRegEx": "Janin et al\\.",
        "year": 2003,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " \u2022 MRDA: ICSI Meeting Recorder Dialog Act Corpus (Janin et al., 2003; Shriberg et al., 2004).",
        "context": null
    },
    {
        "title": "Backoff model training using partially observed data: application to dialog act tagging",
        "author": [
            "Ji",
            "Bilmes2006] Gang Ji",
            "Jeff Bilmes"
        ],
        "venue": "In Proceedings of the main conference on Human Language Technology Conference of the North American Chapter",
        "citeRegEx": "Ji et al\\.,? \\Q2006\\E",
        "shortCiteRegEx": "Ji et al\\.",
        "year": 2006,
        "abstract": "",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "Switchboard SWBDDAMSL shallow-discourse-function annotation coders manual",
        "author": [
            "Elizabeth Shriberg",
            "Debra"
        ],
        "venue": "Biasca",
        "citeRegEx": "Jurafsky et al\\.,? \\Q1997\\E",
        "shortCiteRegEx": "Jurafsky et al\\.",
        "year": 1997,
        "abstract": "",
        "full_text": "",
        "sentence": " \u2022 SwDA: Switchboard Dialog Act Corpus (Jurafsky et al., 1997).",
        "context": null
    },
    {
        "title": "A convolutional neural network for modelling sentences",
        "author": [
            "Edward Grefenstette",
            "Phil Blunsom"
        ],
        "venue": "arXiv preprint arXiv:1404.2188",
        "citeRegEx": "Kalchbrenner et al\\.,? \\Q2014\\E",
        "shortCiteRegEx": "Kalchbrenner et al\\.",
        "year": 2014,
        "abstract": "The ability to accurately represent sentences is central to language\nunderstanding. We describe a convolutional architecture dubbed the Dynamic\nConvolutional Neural Network (DCNN) that we adopt for the semantic modelling of\nsentences. The network uses Dynamic k-Max Pooling, a global pooling operation\nover linear sequences. The network handles input sentences of varying length\nand induces a feature graph over the sentence that is capable of explicitly\ncapturing short and long-range relations. The network does not rely on a parse\ntree and is easily applicable to any language. We test the DCNN in four\nexperiments: small scale binary and multi-class sentiment prediction, six-way\nquestion classification and Twitter sentiment prediction by distant\nsupervision. The network achieves excellent performance in the first three\ntasks and a greater than 25% error reduction in the last task with respect to\nthe strongest baseline.",
        "full_text": "A Convolutional Neural Network for Modelling Sentences\nNal Kalchbrenner\nEdward Grefenstette\n{nal.kalchbrenner, edward.grefenstette, phil.blunsom}@cs.ox.ac.uk\nDepartment of Computer Science\nUniversity of Oxford\nPhil Blunsom\nAbstract\nThe ability to accurately represent sen-\ntences is central to language understand-\ning. We describe a convolutional architec-\nture dubbed the Dynamic Convolutional\nNeural Network (DCNN) that we adopt\nfor the semantic modelling of sentences.\nThe network uses Dynamic k-Max Pool-\ning, a global pooling operation over lin-\near sequences. The network handles input\nsentences of varying length and induces\na feature graph over the sentence that is\ncapable of explicitly capturing short and\nlong-range relations.\nThe network does\nnot rely on a parse tree and is easily ap-\nplicable to any language.\nWe test the\nDCNN in four experiments: small scale\nbinary and multi-class sentiment predic-\ntion, six-way question classi\ufb01cation and\nTwitter sentiment prediction by distant su-\npervision. The network achieves excellent\nperformance in the \ufb01rst three tasks and a\ngreater than 25% error reduction in the last\ntask with respect to the strongest baseline.\n1\nIntroduction\nThe aim of a sentence model is to analyse and\nrepresent the semantic content of a sentence for\npurposes of classi\ufb01cation or generation. The sen-\ntence modelling problem is at the core of many\ntasks involving a degree of natural language com-\nprehension. These tasks include sentiment analy-\nsis, paraphrase detection, entailment recognition,\nsummarisation, discourse analysis, machine trans-\nlation, grounded language learning and image re-\ntrieval. Since individual sentences are rarely ob-\nserved or not observed at all, one must represent\na sentence in terms of features that depend on the\nwords and short n-grams in the sentence that are\nfrequently observed. The core of a sentence model\ninvolves a feature function that de\ufb01nes the process\n The  cat  sat  on  the  red  mat\n The  cat  sat  on  the  red  mat\nFigure 1: Subgraph of a feature graph induced\nover an input sentence in a Dynamic Convolu-\ntional Neural Network.\nThe full induced graph\nhas multiple subgraphs of this kind with a distinct\nset of edges; subgraphs may merge at different\nlayers. The left diagram emphasises the pooled\nnodes. The width of the convolutional \ufb01lters is 3\nand 2 respectively. With dynamic pooling, a \ufb01l-\nter with small width at the higher layers can relate\nphrases far apart in the input sentence.\nby which the features of the sentence are extracted\nfrom the features of the words or n-grams.\nVarious types of models of meaning have been\nproposed. Composition based methods have been\napplied to vector representations of word meaning\nobtained from co-occurrence statistics to obtain\nvectors for longer phrases. In some cases, com-\nposition is de\ufb01ned by algebraic operations over\nword meaning vectors to produce sentence mean-\ning vectors (Erk and Pad\u00b4o, 2008; Mitchell and\nLapata, 2008; Mitchell and Lapata, 2010; Tur-\nney, 2012; Erk, 2012; Clarke, 2012).\nIn other\ncases, a composition function is learned and ei-\nther tied to particular syntactic relations (Guevara,\n2010; Zanzotto et al., 2010) or to particular word\ntypes (Baroni and Zamparelli, 2010; Coecke et\nal., 2010; Grefenstette and Sadrzadeh, 2011; Kart-\nsaklis and Sadrzadeh, 2013; Grefenstette, 2013).\nAnother approach represents the meaning of sen-\ntences by way of automatically extracted logical\nforms (Zettlemoyer and Collins, 2005).\narXiv:1404.2188v1  [cs.CL]  8 Apr 2014\nA central class of models are those based on\nneural networks.\nThese range from basic neu-\nral bag-of-words or bag-of-n-grams models to the\nmore structured recursive neural networks and\nto time-delay neural networks based on convo-\nlutional operations (Collobert and Weston, 2008;\nSocher et al., 2011; Kalchbrenner and Blunsom,\n2013b).\nNeural sentence models have a num-\nber of advantages. They can be trained to obtain\ngeneric vectors for words and phrases by predict-\ning, for instance, the contexts in which the words\nand phrases occur. Through supervised training,\nneural sentence models can \ufb01ne-tune these vec-\ntors to information that is speci\ufb01c to a certain\ntask. Besides comprising powerful classi\ufb01ers as\npart of their architecture, neural sentence models\ncan be used to condition a neural language model\nto generate sentences word by word (Schwenk,\n2012; Mikolov and Zweig, 2012; Kalchbrenner\nand Blunsom, 2013a).\nWe de\ufb01ne a convolutional neural network archi-\ntecture and apply it to the semantic modelling of\nsentences. The network handles input sequences\nof varying length. The layers in the network in-\nterleave one-dimensional convolutional layers and\ndynamic k-max pooling layers. Dynamic k-max\npooling is a generalisation of the max pooling op-\nerator. The max pooling operator is a non-linear\nsubsampling function that returns the maximum\nof a set of values (LeCun et al., 1998). The op-\nerator is generalised in two respects.\nFirst, k-\nmax pooling over a linear sequence of values re-\nturns the subsequence of k maximum values in the\nsequence, instead of the single maximum value.\nSecondly, the pooling parameter k can be dynam-\nically chosen by making k a function of other as-\npects of the network or the input.\nThe\nconvolutional\nlayers\napply\none-\ndimensional \ufb01lters across each row of features in\nthe sentence matrix. Convolving the same \ufb01lter\nwith the n-gram at every position in the sentence\nallows the features to be extracted independently\nof their position in the sentence. A convolutional\nlayer followed by a dynamic pooling layer and\na non-linearity form a feature map. Like in the\nconvolutional networks for object recognition\n(LeCun et al., 1998), we enrich the representation\nin the \ufb01rst layer by computing multiple feature\nmaps with different \ufb01lters applied to the input\nsentence.\nSubsequent layers also have multiple\nfeature maps computed by convolving \ufb01lters with\nall the maps from the layer below. The weights at\nthese layers form an order-4 tensor. The resulting\narchitecture is dubbed a Dynamic Convolutional\nNeural Network.\nMultiple layers of convolutional and dynamic\npooling operations induce a structured feature\ngraph over the input sentence. Figure 1 illustrates\nsuch a graph. Small \ufb01lters at higher layers can cap-\nture syntactic or semantic relations between non-\ncontinuous phrases that are far apart in the input\nsentence. The feature graph induces a hierarchical\nstructure somewhat akin to that in a syntactic parse\ntree. The structure is not tied to purely syntactic\nrelations and is internal to the neural network.\nWe experiment with the network in four set-\ntings. The \ufb01rst two experiments involve predict-\ning the sentiment of movie reviews (Socher et\nal., 2013b). The network outperforms other ap-\nproaches in both the binary and the multi-class ex-\nperiments. The third experiment involves the cat-\negorisation of questions in six question types in\nthe TREC dataset (Li and Roth, 2002). The net-\nwork matches the accuracy of other state-of-the-\nart methods that are based on large sets of en-\ngineered features and hand-coded knowledge re-\nsources. The fourth experiment involves predict-\ning the sentiment of Twitter posts using distant su-\npervision (Go et al., 2009). The network is trained\non 1.6 million tweets labelled automatically ac-\ncording to the emoticon that occurs in them. On\nthe hand-labelled test set, the network achieves a\ngreater than 25% reduction in the prediction error\nwith respect to the strongest unigram and bigram\nbaseline reported in Go et al. (2009).\nThe outline of the paper is as follows. Section 2\ndescribes the background to the DCNN including\ncentral concepts and related neural sentence mod-\nels. Section 3 de\ufb01nes the relevant operators and\nthe layers of the network. Section 4 treats of the\ninduced feature graph and other properties of the\nnetwork. Section 5 discusses the experiments and\ninspects the learnt feature detectors.1\n2\nBackground\nThe layers of the DCNN are formed by a convo-\nlution operation followed by a pooling operation.\nWe begin with a review of related neural sentence\nmodels. Then we describe the operation of one-\ndimensional convolution and the classical Time-\nDelay Neural Network (TDNN) (Hinton, 1989;\nWaibel et al., 1990). By adding a max pooling\n1Code available at www.nal.co\nlayer to the network, the TDNN can be adopted as\na sentence model (Collobert and Weston, 2008).\n2.1\nRelated Neural Sentence Models\nVarious neural sentence models have been de-\nscribed. A general class of basic sentence models\nis that of Neural Bag-of-Words (NBoW) models.\nThese generally consist of a projection layer that\nmaps words, sub-word units or n-grams to high\ndimensional embeddings; the latter are then com-\nbined component-wise with an operation such as\nsummation. The resulting combined vector is clas-\nsi\ufb01ed through one or more fully connected layers.\nA model that adopts a more general structure\nprovided by an external parse tree is the Recursive\nNeural Network (RecNN) (Pollack, 1990; K\u00a8uchler\nand Goller, 1996; Socher et al., 2011; Hermann\nand Blunsom, 2013). At every node in the tree the\ncontexts at the left and right children of the node\nare combined by a classical layer. The weights of\nthe layer are shared across all nodes in the tree.\nThe layer computed at the top node gives a repre-\nsentation for the sentence. The Recurrent Neural\nNetwork (RNN) is a special case of the recursive\nnetwork where the structure that is followed is a\nsimple linear chain (Gers and Schmidhuber, 2001;\nMikolov et al., 2011). The RNN is primarily used\nas a language model, but may also be viewed as a\nsentence model with a linear structure. The layer\ncomputed at the last word represents the sentence.\nFinally, a further class of neural sentence mod-\nels is based on the convolution operation and the\nTDNN architecture (Collobert and Weston, 2008;\nKalchbrenner and Blunsom, 2013b). Certain con-\ncepts used in these models are central to the\nDCNN and we describe them next.\n2.2\nConvolution\nThe one-dimensional convolution is an operation\nbetween a vector of weights m \u2208Rm and a vector\nof inputs viewed as a sequence s \u2208Rs. The vector\nm is the \ufb01lter of the convolution. Concretely, we\nthink of s as the input sentence and si \u2208R is a sin-\ngle feature value associated with the i-th word in\nthe sentence. The idea behind the one-dimensional\nconvolution is to take the dot product of the vector\nm with each m-gram in the sentence s to obtain\nanother sequence c:\ncj = m\u22basj\u2212m+1:j\n(1)\nEquation 1 gives rise to two types of convolution\ndepending on the range of the index j. The narrow\ntype of convolution requires that s \u2265m and yields\ns1\ns1\nss\nss\nc1\nc5\nc5\nFigure 2: Narrow and wide types of convolution.\nThe \ufb01lter m has size m = 5.\na sequence c \u2208Rs\u2212m+1 with j ranging from m\nto s. The wide type of convolution does not have\nrequirements on s or m and yields a sequence c \u2208\nRs+m\u22121 where the index j ranges from 1 to s +\nm \u22121. Out-of-range input values si where i < 1\nor i > s are taken to be zero. The result of the\nnarrow convolution is a subsequence of the result\nof the wide convolution. The two types of one-\ndimensional convolution are illustrated in Fig. 2.\nThe trained weights in the \ufb01lter m correspond\nto a linguistic feature detector that learns to recog-\nnise a speci\ufb01c class of n-grams. These n-grams\nhave size n \u2264m, where m is the width of the\n\ufb01lter. Applying the weights m in a wide convo-\nlution has some advantages over applying them in\na narrow one. A wide convolution ensures that all\nweights in the \ufb01lter reach the entire sentence, in-\ncluding the words at the margins. This is particu-\nlarly signi\ufb01cant when m is set to a relatively large\nvalue such as 8 or 10. In addition, a wide convo-\nlution guarantees that the application of the \ufb01lter\nm to the input sentence s always produces a valid\nnon-empty result c, independently of the width m\nand the sentence length s. We next describe the\nclassical convolutional layer of a TDNN.\n2.3\nTime-Delay Neural Networks\nA TDNN convolves a sequence of inputs s with a\nset of weights m. As in the TDNN for phoneme\nrecognition (Waibel et al., 1990), the sequence s\nis viewed as having a time dimension and the con-\nvolution is applied over the time dimension. Each\nsj is often not just a single value, but a vector of\nd values so that s \u2208Rd\u00d7s. Likewise, m is a ma-\ntrix of weights of size d \u00d7 m. Each row of m is\nconvolved with the corresponding row of s and the\nconvolution is usually of the narrow type. Multi-\nple convolutional layers may be stacked by taking\nthe resulting sequence c as input to the next layer.\nThe Max-TDNN sentence model is based on the\narchitecture of a TDNN (Collobert and Weston,\n2008). In the model, a convolutional layer of the\nnarrow type is applied to the sentence matrix s,\nwhere each column corresponds to the feature vec-\ntor wi \u2208Rd of a word in the sentence:\ns =\n\uf8ee\n\uf8f0w1\n. . .\nws\n\uf8f9\n\uf8fb\n(2)\nTo address the problem of varying sentence\nlengths, the Max-TDNN takes the maximum of\neach row in the resulting matrix c yielding a vector\nof d values:\ncmax =\n\uf8ee\n\uf8ef\uf8f0\nmax(c1,:)\n...\nmax(cd,:)\n\uf8f9\n\uf8fa\uf8fb\n(3)\nThe aim is to capture the most relevant feature, i.e.\nthe one with the highest value, for each of the d\nrows of the resulting matrix c. The \ufb01xed-sized\nvector cmax is then used as input to a fully con-\nnected layer for classi\ufb01cation.\nThe Max-TDNN model has many desirable\nproperties. It is sensitive to the order of the words\nin the sentence and it does not depend on external\nlanguage-speci\ufb01c features such as dependency or\nconstituency parse trees. It also gives largely uni-\nform importance to the signal coming from each\nof the words in the sentence, with the exception\nof words at the margins that are considered fewer\ntimes in the computation of the narrow convolu-\ntion. But the model also has some limiting as-\npects. The range of the feature detectors is lim-\nited to the span m of the weights. Increasing m or\nstacking multiple convolutional layers of the nar-\nrow type makes the range of the feature detectors\nlarger; at the same time it also exacerbates the ne-\nglect of the margins of the sentence and increases\nthe minimum size s of the input sentence required\nby the convolution. For this reason higher-order\nand long-range feature detectors cannot be easily\nincorporated into the model. The max pooling op-\neration has some disadvantages too. It cannot dis-\ntinguish whether a relevant feature in one of the\nrows occurs just one or multiple times and it for-\ngets the order in which the features occur. More\ngenerally, the pooling factor by which the signal\nof the matrix is reduced at once corresponds to\ns\u2212m+1; even for moderate values of s the pool-\ning factor can be excessive. The aim of the next\nsection is to address these limitations while pre-\nserving the advantages.\n3\nConvolutional Neural Networks with\nDynamic k-Max Pooling\nWe model sentences using a convolutional archi-\ntecture that alternates wide convolutional layers\nK-Max pooling\n(k=3)\nFully connected \nlayer\nFolding\nWide\nconvolution\n(m=2)\nDynamic\nk-max pooling\n (k= f(s) =5)\n Projected\nsentence \nmatrix\n(s=7)\nWide\nconvolution\n(m=3)\n The cat sat on the red mat\nFigure 3: A DCNN for the seven word input sen-\ntence. Word embeddings have size d = 4. The\nnetwork has two convolutional layers with two\nfeature maps each. The widths of the \ufb01lters at the\ntwo layers are respectively 3 and 2. The (dynamic)\nk-max pooling layers have values k of 5 and 3.\nwith dynamic pooling layers given by dynamic k-\nmax pooling. In the network the width of a feature\nmap at an intermediate layer varies depending on\nthe length of the input sentence; the resulting ar-\nchitecture is the Dynamic Convolutional Neural\nNetwork. Figure 3 represents a DCNN. We pro-\nceed to describe the network in detail.\n3.1\nWide Convolution\nGiven an input sentence, to obtain the \ufb01rst layer of\nthe DCNN we take the embedding wi \u2208Rd for\neach word in the sentence and construct the sen-\ntence matrix s \u2208Rd\u00d7s as in Eq. 2. The values\nin the embeddings wi are parameters that are op-\ntimised during training. A convolutional layer in\nthe network is obtained by convolving a matrix of\nweights m \u2208Rd\u00d7m with the matrix of activations\nat the layer below. For example, the second layer\nis obtained by applying a convolution to the sen-\ntence matrix s itself. Dimension d and \ufb01lter width\nm are hyper-parameters of the network. We let the\noperations be wide one-dimensional convolutions\nas described in Sect. 2.2. The resulting matrix c\nhas dimensions d \u00d7 (s + m \u22121).\n3.2\nk-Max Pooling\nWe next describe a pooling operation that is a gen-\neralisation of the max pooling over the time di-\nmension used in the Max-TDNN sentence model\nand different from the local max pooling opera-\ntions applied in a convolutional network for object\nrecognition (LeCun et al., 1998). Given a value\nk and a sequence p \u2208Rp of length p \u2265k, k-\nmax pooling selects the subsequence pk\nmax of the\nk highest values of p. The order of the values in\npk\nmax corresponds to their original order in p.\nThe k-max pooling operation makes it possible\nto pool the k most active features in p that may be\na number of positions apart; it preserves the order\nof the features, but is insensitive to their speci\ufb01c\npositions. It can also discern more \ufb01nely the num-\nber of times the feature is highly activated in p\nand the progression by which the high activations\nof the feature change across p. The k-max pooling\noperator is applied in the network after the topmost\nconvolutional layer. This guarantees that the input\nto the fully connected layers is independent of the\nlength of the input sentence. But, as we see next, at\nintermediate convolutional layers the pooling pa-\nrameter k is not \ufb01xed, but is dynamically selected\nin order to allow for a smooth extraction of higher-\norder and longer-range features.\n3.3\nDynamic k-Max Pooling\nA dynamic k-max pooling operation is a k-max\npooling operation where we let k be a function of\nthe length of the sentence and the depth of the net-\nwork. Although many functions are possible, we\nsimply model the pooling parameter as follows:\nkl = max( ktop, \u2308L \u2212l\nL\ns\u2309)\n(4)\nwhere l is the number of the current convolutional\nlayer to which the pooling is applied and L is the\ntotal number of convolutional layers in the net-\nwork; ktop is the \ufb01xed pooling parameter for the\ntopmost convolutional layer (Sect. 3.2). For in-\nstance, in a network with three convolutional lay-\ners and ktop = 3, for an input sentence of length\ns = 18, the pooling parameter at the \ufb01rst layer\nis k1 = 12 and the pooling parameter at the sec-\nond layer is k2 = 6; the third layer has the \ufb01xed\npooling parameter k3 = ktop = 3. Equation 4\nis a model of the number of values needed to de-\nscribe the relevant parts of the progression of an\nl-th order feature over a sentence of length s. For\nan example in sentiment prediction, according to\nthe equation a \ufb01rst order feature such as a posi-\ntive word occurs at most k1 times in a sentence of\nlength s, whereas a second order feature such as a\nnegated phrase or clause occurs at most k2 times.\n3.4\nNon-linear Feature Function\nAfter (dynamic) k-max pooling is applied to the\nresult of a convolution, a bias b \u2208Rd and a non-\nlinear function g are applied component-wise to\nthe pooled matrix. There is a single bias value for\neach row of the pooled matrix.\nIf we temporarily ignore the pooling layer, we\nmay state how one computes each d-dimensional\ncolumn a in the matrix a resulting after the convo-\nlutional and non-linear layers. De\ufb01ne M to be the\nmatrix of diagonals:\nM = [diag(m:,1), . . . , diag(m:,m)]\n(5)\nwhere m are the weights of the d \ufb01lters of the wide\nconvolution. Then after the \ufb01rst pair of a convolu-\ntional and a non-linear layer, each column a in the\nmatrix a is obtained as follows, for some index j:\na = g\n\uf8eb\n\uf8ec\n\uf8edM\n\uf8ee\n\uf8ef\uf8f0\nwj\n...\nwj+m\u22121\n\uf8f9\n\uf8fa\uf8fb+ b\n\uf8f6\n\uf8f7\n\uf8f8\n(6)\nHere a is a column of \ufb01rst order features. Sec-\nond order features are similarly obtained by ap-\nplying Eq. 6 to a sequence of \ufb01rst order features\naj, ..., aj+m\u2032\u22121 with another weight matrix M\u2032.\nBarring pooling, Eq. 6 represents a core aspect\nof the feature extraction function and has a rather\ngeneral form that we return to below. Together\nwith pooling, the feature function induces position\ninvariance and makes the range of higher-order\nfeatures variable.\n3.5\nMultiple Feature Maps\nSo far we have described how one applies a wide\nconvolution, a (dynamic) k-max pooling layer and\na non-linear function to the input sentence ma-\ntrix to obtain a \ufb01rst order feature map. The three\noperations can be repeated to yield feature maps\nof increasing order and a network of increasing\ndepth. We denote a feature map of the i-th order\nby Fi. As in convolutional networks for object\nrecognition, to increase the number of learnt fea-\nture detectors of a certain order, multiple feature\nmaps Fi\n1, . . . , Fi\nn may be computed in parallel at\nthe same layer. Each feature map Fi\nj is computed\nby convolving a distinct set of \ufb01lters arranged in\na matrix mi\nj,k with each feature map Fi\u22121\nk\nof the\nlower order i \u22121 and summing the results:\nFi\nj =\nn\nX\nk=1\nmi\nj,k \u2217Fi\u22121\nk\n(7)\nwhere \u2217indicates the wide convolution.\nThe\nweights mi\nj,k form an order-4 tensor. After the\nwide convolution, \ufb01rst dynamic k-max pooling\nand then the non-linear function are applied indi-\nvidually to each map.\n3.6\nFolding\nIn the formulation of the network so far, feature\ndetectors applied to an individual row of the sen-\ntence matrix s can have many orders and create\ncomplex dependencies across the same rows in\nmultiple feature maps. Feature detectors in differ-\nent rows, however, are independent of each other\nuntil the top fully connected layer. Full depen-\ndence between different rows could be achieved\nby making M in Eq. 5 a full matrix instead of\na sparse matrix of diagonals. Here we explore a\nsimpler method called folding that does not intro-\nduce any additional parameters. After a convo-\nlutional layer and before (dynamic) k-max pool-\ning, one just sums every two rows in a feature map\ncomponent-wise. For a map of d rows, folding re-\nturns a map of d/2 rows, thus halving the size of\nthe representation. With a folding layer, a feature\ndetector of the i-th order depends now on two rows\nof feature values in the lower maps of order i \u22121.\nThis ends the description of the DCNN.\n4\nProperties of the Sentence Model\nWe describe some of the properties of the sentence\nmodel based on the DCNN. We describe the no-\ntion of the feature graph induced over a sentence\nby the succession of convolutional and pooling\nlayers. We brie\ufb02y relate the properties to those of\nother neural sentence models.\n4.1\nWord and n-Gram Order\nOne of the basic properties is sensitivity to the or-\nder of the words in the input sentence. For most\napplications and in order to learn \ufb01ne-grained fea-\nture detectors, it is bene\ufb01cial for a model to be able\nto discriminate whether a speci\ufb01c n-gram occurs\nin the input. Likewise, it is bene\ufb01cial for a model\nto be able to tell the relative position of the most\nrelevant n-grams. The network is designed to cap-\nture these two aspects. The \ufb01lters m of the wide\nconvolution in the \ufb01rst layer can learn to recognise\nspeci\ufb01c n-grams that have size less or equal to the\n\ufb01lter width m; as we see in the experiments, m in\nthe \ufb01rst layer is often set to a relatively large value\nsuch as 10. The subsequence of n-grams extracted\nby the generalised pooling operation induces in-\nvariance to absolute positions, but maintains their\norder and relative positions.\nAs regards the other neural sentence models, the\nclass of NBoW models is by de\ufb01nition insensitive\nto word order. A sentence model based on a recur-\nrent neural network is sensitive to word order, but\nit has a bias towards the latest words that it takes as\ninput (Mikolov et al., 2011). This gives the RNN\nexcellent performance at language modelling, but\nit is suboptimal for remembering at once the n-\ngrams further back in the input sentence. Sim-\nilarly, a recursive neural network is sensitive to\nword order but has a bias towards the topmost\nnodes in the tree; shallower trees mitigate this ef-\nfect to some extent (Socher et al., 2013a). As seen\nin Sect. 2.3, the Max-TDNN is sensitive to word\norder, but max pooling only picks out a single n-\ngram feature in each row of the sentence matrix.\n4.2\nInduced Feature Graph\nSome sentence models use internal or external\nstructure to compute the representation for the in-\nput sentence.\nIn a DCNN, the convolution and\npooling layers induce an internal feature graph\nover the input. A node from a layer is connected\nto a node from the next higher layer if the lower\nnode is involved in the convolution that computes\nthe value of the higher node. Nodes that are not\nselected by the pooling operation at a layer are\ndropped from the graph. After the last pooling\nlayer, the remaining nodes connect to a single top-\nmost root. The induced graph is a connected, di-\nrected acyclic graph with weighted edges and a\nroot node; two equivalent representations of an\ninduced graph are given in Fig. 1. In a DCNN\nwithout folding layers, each of the d rows of the\nsentence matrix induces a subgraph that joins the\nother subgraphs only at the root node. Each sub-\ngraph may have a different shape that re\ufb02ects the\nkind of relations that are detected in that subgraph.\nThe effect of folding layers is to join pairs of sub-\ngraphs at lower layers before the top root node.\nConvolutional networks for object recognition\nalso induce a feature graph over the input image.\nWhat makes the feature graph of a DCNN pecu-\nliar is the global range of the pooling operations.\nThe (dynamic) k-max pooling operator can draw\ntogether features that correspond to words that are\nmany positions apart in the sentence. Higher-order\nfeatures have highly variable ranges that can be ei-\nther short and focused or global and long as the\ninput sentence. Likewise, the edges of a subgraph\nin the induced graph re\ufb02ect these varying ranges.\nThe subgraphs can either be localised to one or\nmore parts of the sentence or spread more widely\nacross the sentence. This structure is internal to\nthe network and is de\ufb01ned by the forward propa-\ngation of the input through the network.\nOf the other sentence models, the NBoW is a\nshallow model and the RNN has a linear chain\nstructure.\nThe subgraphs induced in the Max-\nTDNN model have a single \ufb01xed-range feature ob-\ntained through max pooling. The recursive neural\nnetwork follows the structure of an external parse\ntree. Features of variable range are computed at\neach node of the tree combining one or more of\nthe children of the tree. Unlike in a DCNN, where\none learns a clear hierarchy of feature orders, in\na RecNN low order features like those of sin-\ngle words can be directly combined with higher\norder features computed from entire clauses. A\nDCNN generalises many of the structural aspects\nof a RecNN. The feature extraction function as\nstated in Eq. 6 has a more general form than that\nin a RecNN, where the value of m is generally 2.\nLikewise, the induced graph structure in a DCNN\nis more general than a parse tree in that it is not\nlimited to syntactically dictated phrases; the graph\nstructure can capture short or long-range seman-\ntic relations between words that do not necessar-\nily correspond to the syntactic relations in a parse\ntree.\nThe DCNN has internal input-dependent\nstructure and does not rely on externally provided\nparse trees, which makes the DCNN directly ap-\nplicable to hard-to-parse sentences such as tweets\nand to sentences from any language.\n5\nExperiments\nWe test the network on four different experiments.\nWe begin by specifying aspects of the implemen-\ntation and the training of the network. We then re-\nlate the results of the experiments and we inspect\nthe learnt feature detectors.\n5.1\nTraining\nIn each of the experiments, the top layer of the\nnetwork has a fully connected layer followed by\na softmax non-linearity that predicts the probabil-\nity distribution over classes given the input sen-\ntence.\nThe network is trained to minimise the\ncross-entropy of the predicted and true distribu-\ntions; the objective includes an L2 regularisation\nClassi\ufb01er\nFine-grained (%)\nBinary (%)\nNB\n41.0\n81.8\nBINB\n41.9\n83.1\nSVM\n40.7\n79.4\nRECNTN\n45.7\n85.4\nMAX-TDNN\n37.4\n77.1\nNBOW\n42.4\n80.5\nDCNN\n48.5\n86.8\nTable 1: Accuracy of sentiment prediction in the\nmovie reviews dataset. The \ufb01rst four results are\nreported from Socher et al. (2013b). The baselines\nNB and BINB are Naive Bayes classi\ufb01ers with,\nrespectively, unigram features and unigram and bi-\ngram features. SVM is a support vector machine\nwith unigram and bigram features. RECNTN is a\nrecursive neural network with a tensor-based fea-\nture function, which relies on external structural\nfeatures given by a parse tree and performs best\namong the RecNNs.\nterm over the parameters. The set of parameters\ncomprises the word embeddings, the \ufb01lter weights\nand the weights from the fully connected layers.\nThe network is trained with mini-batches by back-\npropagation and the gradient-based optimisation is\nperformed using the Adagrad update rule (Duchi\net al., 2011). Using the well-known convolution\ntheorem, we can compute fast one-dimensional\nlinear convolutions at all rows of an input matrix\nby using Fast Fourier Transforms. To exploit the\nparallelism of the operations, we train the network\non a GPU. A Matlab implementation processes\nmultiple millions of input sentences per hour on\none GPU, depending primarily on the number of\nlayers used in the network.\n5.2\nSentiment Prediction in Movie Reviews\nThe \ufb01rst two experiments concern the prediction\nof the sentiment of movie reviews in the Stanford\nSentiment Treebank (Socher et al., 2013b). The\noutput variable is binary in one experiment and\ncan have \ufb01ve possible outcomes in the other: neg-\native, somewhat negative, neutral, somewhat posi-\ntive, positive. In the binary case, we use the given\nsplits of 6920 training, 872 development and 1821\ntest sentences. Likewise, in the \ufb01ne-grained case,\nwe use the standard 8544/1101/2210 splits. La-\nbelled phrases that occur as subparts of the train-\ning sentences are treated as independent training\ninstances. The size of the vocabulary is 15448.\nTable 1 details the results of the experiments.\nClassi\ufb01er\nFeatures\nAcc. (%)\nHIER\nunigram, POS, head chunks\n91.0\nNE, semantic relations\nMAXENT\nunigram, bigram, trigram\n92.6\nPOS, chunks, NE, supertags\nCCG parser, WordNet\nMAXENT\nunigram, bigram, trigram\n93.6\nPOS, wh-word, head word\nword shape, parser\nhypernyms, WordNet\nSVM\nunigram, POS, wh-word\n95.0\nhead word, parser\nhypernyms, WordNet\n60 hand-coded rules\nMAX-TDNN\nunsupervised vectors\n84.4\nNBOW\nunsupervised vectors\n88.2\nDCNN\nunsupervised vectors\n93.0\nTable 2: Accuracy of six-way question classi\ufb01ca-\ntion on the TREC questions dataset. The second\ncolumn details the external features used in the\nvarious approaches. The \ufb01rst four results are re-\nspectively from Li and Roth (2002), Blunsom et al.\n(2006), Huang et al. (2008) and Silva et al. (2011).\nIn the three neural sentence models\u2014the Max-\nTDNN, the NBoW and the DCNN\u2014the word vec-\ntors are parameters of the models that are ran-\ndomly initialised; their dimension d is set to 48.\nThe Max-TDNN has a \ufb01lter of width 6 in its nar-\nrow convolution at the \ufb01rst layer; shorter phrases\nare padded with zero vectors.\nThe convolu-\ntional layer is followed by a non-linearity, a max-\npooling layer and a softmax classi\ufb01cation layer.\nThe NBoW sums the word vectors and applies a\nnon-linearity followed by a softmax classi\ufb01cation\nlayer. The adopted non-linearity is the tanh func-\ntion. The hyper parameters of the DCNN are as\nfollows. The binary result is based on a DCNN\nthat has a wide convolutional layer followed by a\nfolding layer, a dynamic k-max pooling layer and\na non-linearity; it has a second wide convolutional\nlayer followed by a folding layer, a k-max pooling\nlayer and a non-linearity. The width of the convo-\nlutional \ufb01lters is 7 and 5, respectively. The value\nof k for the top k-max pooling is 4. The num-\nber of feature maps at the \ufb01rst convolutional layer\nis 6; the number of maps at the second convolu-\ntional layer is 14. The network is topped by a soft-\nmax classi\ufb01cation layer. The DCNN for the \ufb01ne-\ngrained result has the same architecture, but the\n\ufb01lters have size 10 and 7, the top pooling parame-\nter k is 5 and the number of maps is, respectively,\n6 and 12. The networks use the tanh non-linear\nClassi\ufb01er\nAccuracy (%)\nSVM\n81.6\nBINB\n82.7\nMAXENT\n83.0\nMAX-TDNN\n78.8\nNBOW\n80.9\nDCNN\n87.4\nTable 3:\nAccuracy on the Twitter sentiment\ndataset. The three non-neural classi\ufb01ers are based\non unigram and bigram features; the results are re-\nported from (Go et al., 2009).\nfunction. At training time we apply dropout to the\npenultimate layer after the last tanh non-linearity\n(Hinton et al., 2012).\nWe see that the DCNN signi\ufb01cantly outper-\nforms the other neural and non-neural models.\nThe NBoW performs similarly to the non-neural\nn-gram based classi\ufb01ers.\nThe Max-TDNN per-\nforms worse than the NBoW likely due to the ex-\ncessive pooling of the max pooling operation; the\nlatter discards most of the sentiment features of the\nwords in the input sentence. Besides the RecNN\nthat uses an external parser to produce structural\nfeatures for the model, the other models use n-\ngram based or neural features that do not require\nexternal resources or additional annotations. In the\nnext experiment we compare the performance of\nthe DCNN with those of methods that use heavily\nengineered resources.\n5.3\nQuestion Type Classi\ufb01cation\nAs an aid to question answering, a question may\nbe classi\ufb01ed as belonging to one of many question\ntypes. The TREC questions dataset involves six\ndifferent question types, e.g. whether the question\nis about a location, about a person or about some\nnumeric information (Li and Roth, 2002).\nThe\ntraining dataset consists of 5452 labelled questions\nwhereas the test dataset consists of 500 questions.\nThe results are reported in Tab. 2. The non-\nneural approaches use a classi\ufb01er over a large\nnumber of manually engineered features and\nhand-coded resources. For instance, Blunsom et\nal. (2006) present a Maximum Entropy model that\nrelies on 26 sets of syntactic and semantic fea-\ntures including unigrams, bigrams, trigrams, POS\ntags, named entity tags, structural relations from\na CCG parse and WordNet synsets. We evaluate\nthe three neural models on this dataset with mostly\nthe same hyper-parameters as in the binary senti-\nPOSITIVE\nlovely\t \t \t \t \t comedic\t \t \t \t \t moments\t and\t \t \t \t several\t \t \t \t \t fine\t \t \t \t \t \t performances\ngood\t \t \t \t \t \t \t script\t \t \t \t \t \t ,\t \t \t \t \t \t \t good\t \t \t dialogue\t \t \t \t ,\t \t \t \t \t \t \t \t \t funny\t \t \t \t \t \t \t \nsustains\t \t \t throughout\t \t is\t \t \t \t \t \t daring\t ,\t \t \t \t \t \t \t \t \t \t \t inventive\t and\t \t \t \t \t \t \t \t \t \nwell\t \t \t \t \t \t \t written\t \t \t \t \t ,\t \t \t \t \t \t \t nicely\t acted\t \t \t \t \t \t \t and\t \t \t \t \t \t \t beautifully\t \nremarkably\t solid\t \t \t \t \t \t \t and\t \t \t \t \t subtly\t satirical\t \t \t tour\t \t \t \t \t \t de\t \t \t \t \t \t \t \t \t \t \nNEGATIVE\n,\t \t \t \t \t \t \t \t \t \t nonexistent\t plot\t \t \t \t and\t \t \t \t pretentious\t visual\t \t \t \t style\t \t \t \t \t \t \t \nit\t \t \t \t \t \t \t \t \t fails\t \t \t \t \t \t \t the\t \t \t \t \t most\t \t \t basic\t \t \t \t \t \t \t test\t \t \t \t \t \t as\t \t \t \t \t \t \t \t \t \t \nso\t \t \t \t \t \t \t \t \t stupid\t \t \t \t \t \t ,\t \t \t \t \t \t \t so\t \t \t \t \t ill\t \t \t \t \t \t \t \t \t conceived\t ,\t \t \t \t \t \t \t \t \t \t \t \n,\t \t \t \t \t \t \t \t \t \t too\t \t \t \t \t \t \t \t \t dull\t \t \t \t and\t \t \t \t pretentious\t to\t \t \t \t \t \t \t \t be\t \t \t \t \t \t \t \t \t \t \nhood\t \t \t \t \t \t \t rats\t \t \t \t \t \t \t \t butt\t \t \t \t their\t \t ugly\t \t \t \t \t \t \t \t heads\t \t \t \t \t in\t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \t \n'NOT'\nn't\t \t \t \t have\t \t \t \t \t any\t \t \t \t \t \t \t \t \t huge\t laughs\t \t \t \t \t \t in\t \t \t \t \t \t \t \t \t \t \t its\t \t \t \nno\t \t \t \t \t movement\t ,\t \t \t \t \t \t \t \t \t \t \t no\t \t \t ,\t \t \t \t \t \t \t \t \t \t \t not\t \t \t \t \t \t \t \t \t \t much\t \t \nn't\t \t \t \t stop\t \t \t \t \t me\t \t \t \t \t \t \t \t \t \t from\t enjoying\t \t \t \t much\t \t \t \t \t \t \t \t \t of\t \t \t \t \nnot\t \t \t \t that\t \t \t \t \t kung\t \t \t \t \t \t \t \t pow\t \t is\t \t \t \t \t \t \t \t \t \t n't\t \t \t \t \t \t \t \t \t \t funny\t \nnot\t \t \t \t a\t \t \t \t \t \t \t \t moment\t \t \t \t \t \t that\t is\t \t \t \t \t \t \t \t \t \t not\t \t \t \t \t \t \t \t \t \t false\t \n'TOO'\n,\t \t \t \t \t \t too\t \t \t \t \t \t dull\t \t \t \t \t \t \t \t and\t \t pretentious\t to\t \t \t \t \t \t \t \t \t \t \t be\t \t \t \t \t \t \t \t \neither\t too\t \t \t \t \t \t serious\t \t \t \t \t or\t \t \t too\t \t \t \t \t \t \t \t \t lighthearted\t ,\t \t \t \t \t \t \t \t \t \ntoo\t \t \t \t slow\t \t \t \t \t ,\t \t \t \t \t \t \t \t \t \t \t too\t \t long\t \t \t \t \t \t \t \t and\t \t \t \t \t \t \t \t \t \t too\t \t \t \t \t \t \t \nfeels\t \t too\t \t \t \t \t \t formulaic\t \t \t and\t \t too\t \t \t \t \t \t \t \t \t familiar\t \t \t \t \t to\t \t \t \t \t \t \t \t \nis\t \t \t \t \t too\t \t \t \t \t \t predictable\t and\t \t too\t \t \t \t \t \t \t \t \t self\t \t \t \t \t \t \t \t \t conscious\t \t \nFigure 4: Top \ufb01ve 7-grams at four feature detectors in the \ufb01rst layer of the network.\nment experiment of Sect. 5.2. As the dataset is\nrather small, we use lower-dimensional word vec-\ntors with d = 32 that are initialised with embed-\ndings trained in an unsupervised way to predict\ncontexts of occurrence (Turian et al., 2010). The\nDCNN uses a single convolutional layer with \ufb01l-\nters of size 8 and 5 feature maps. The difference\nbetween the performance of the DCNN and that of\nthe other high-performing methods in Tab. 2 is not\nsigni\ufb01cant (p < 0.09). Given that the only labelled\ninformation used to train the network is the train-\ning set itself, it is notable that the network matches\nthe performance of state-of-the-art classi\ufb01ers that\nrely on large amounts of engineered features and\nrules and hand-coded resources.\n5.4\nTwitter Sentiment Prediction with\nDistant Supervision\nIn our \ufb01nal experiment, we train the models on a\nlarge dataset of tweets, where a tweet is automat-\nically labelled as positive or negative depending\non the emoticon that occurs in it. The training set\nconsists of 1.6 million tweets with emoticon-based\nlabels and the test set of about 400 hand-annotated\ntweets. We preprocess the tweets minimally fol-\nlowing the procedure described in Go et al. (2009);\nin addition, we also lowercase all the tokens. This\nresults in a vocabulary of 76643 word types. The\narchitecture of the DCNN and of the other neural\nmodels is the same as the one used in the binary\nexperiment of Sect. 5.2. The randomly initialised\nword embeddings are increased in length to a di-\nmension of d = 60. Table 3 reports the results of\nthe experiments. We see a signi\ufb01cant increase in\nthe performance of the DCNN with respect to the\nnon-neural n-gram based classi\ufb01ers; in the pres-\nence of large amounts of training data these clas-\nsi\ufb01ers constitute particularly strong baselines. We\nsee that the ability to train a sentiment classi\ufb01er on\nautomatically extracted emoticon-based labels ex-\ntends to the DCNN and results in highly accurate\nperformance. The difference in performance be-\ntween the DCNN and the NBoW further suggests\nthat the ability of the DCNN to both capture fea-\ntures based on long n-grams and to hierarchically\ncombine these features is highly bene\ufb01cial.\n5.5\nVisualising Feature Detectors\nA \ufb01lter in the DCNN is associated with a feature\ndetector or neuron that learns during training to\nbe particularly active when presented with a spe-\nci\ufb01c sequence of input words. In the \ufb01rst layer, the\nsequence is a continuous n-gram from the input\nsentence; in higher layers, sequences can be made\nof multiple separate n-grams.\nWe visualise the\nfeature detectors in the \ufb01rst layer of the network\ntrained on the binary sentiment task (Sect. 5.2).\nSince the \ufb01lters have width 7, for each of the 288\nfeature detectors we rank all 7-grams occurring in\nthe validation and test sets according to their ac-\ntivation of the detector. Figure 5.2 presents the\ntop \ufb01ve 7-grams for four feature detectors. Be-\nsides the expected detectors for positive and nega-\ntive sentiment, we \ufb01nd detectors for particles such\nas \u2018not\u2019 that negate sentiment and such as \u2018too\u2019\nthat potentiate sentiment. We \ufb01nd detectors for\nmultiple other notable constructs including \u2018all\u2019,\n\u2018or\u2019, \u2018with...that\u2019, \u2018as...as\u2019. The feature detectors\nlearn to recognise not just single n-grams, but pat-\nterns within n-grams that have syntactic, semantic\nor structural signi\ufb01cance.\n6\nConclusion\nWe have described a dynamic convolutional neural\nnetwork that uses the dynamic k-max pooling op-\nerator as a non-linear subsampling function. The\nfeature graph induced by the network is able to\ncapture word relations of varying size. The net-\nwork achieves high performance on question and\nsentiment classi\ufb01cation without requiring external\nfeatures as provided by parsers or other resources.\nAcknowledgements\nWe thank Nando de Freitas and Yee Whye Teh\nfor great discussions on the paper. This work was\nsupported by a Xerox Foundation Award, EPSRC\ngrant number EP/F042728/1, and EPSRC grant\nnumber EP/K036580/1.\nReferences\nMarco Baroni and Roberto Zamparelli. 2010. Nouns\nare vectors, adjectives are matrices: Representing\nadjective-noun constructions in semantic space. In\nEMNLP, pages 1183\u20131193. ACL.\nPhil Blunsom, Krystle Kocik, and James R. Curran.\n2006. Question classi\ufb01cation with log-linear mod-\nels.\nIn SIGIR \u201906: Proceedings of the 29th an-\nnual international ACM SIGIR conference on Re-\nsearch and development in information retrieval,\npages 615\u2013616, New York, NY, USA. ACM.\nDaoud Clarke.\n2012.\nA context-theoretic frame-\nwork for compositionality in distributional seman-\ntics. Computational Linguistics, 38(1):41\u201371.\nBob Coecke, Mehrnoosh Sadrzadeh, and Stephen\nClark. 2010. Mathematical Foundations for a Com-\npositional Distributional Model of Meaning. March.\nRonan Collobert and Jason Weston. 2008. A uni\ufb01ed\narchitecture for natural language processing: Deep\nneural networks with multitask learning. In Interna-\ntional Conference on Machine Learning, ICML.\nJohn Duchi, Elad Hazan, and Yoram Singer.\n2011.\nAdaptive subgradient methods for online learning\nand stochastic optimization. J. Mach. Learn. Res.,\n12:2121\u20132159, July.\nKatrin Erk and Sebastian Pad\u00b4o. 2008. A structured\nvector space model for word meaning in context.\nProceedings of the Conference on Empirical Meth-\nods in Natural Language Processing - EMNLP \u201908,\n(October):897.\nKatrin Erk. 2012. Vector space models of word mean-\ning and phrase meaning: A survey. Language and\nLinguistics Compass, 6(10):635\u2013653.\nFelix A. Gers and Jrgen Schmidhuber.\n2001.\nLstm\nrecurrent networks learn simple context-free and\ncontext-sensitive languages. IEEE Transactions on\nNeural Networks, 12(6):1333\u20131340.\nAlec Go, Richa Bhayani, and Lei Huang. 2009. Twit-\nter sentiment classi\ufb01cation using distant supervision.\nProcessing, pages 1\u20136.\nEdward Grefenstette and Mehrnoosh Sadrzadeh. 2011.\nExperimental support for a categorical composi-\ntional distributional model of meaning. In Proceed-\nings of the Conference on Empirical Methods in Nat-\nural Language Processing, pages 1394\u20131404. Asso-\nciation for Computational Linguistics.\nEdward Grefenstette.\n2013.\nCategory-theoretic\nquantitative compositional distributional models\nof natural language semantics.\narXiv preprint\narXiv:1311.1539.\nEmiliano Guevara. 2010. Modelling Adjective-Noun\nCompositionality by Regression. ESSLLI\u201910 Work-\nshop on Compositionality and Distributional Se-\nmantic Models.\nKarl Moritz Hermann and Phil Blunsom. 2013. The\nRole of Syntax in Vector Space Models of Composi-\ntional Semantics. In Proceedings of the 51st Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), So\ufb01a, Bulgaria,\nAugust. Association for Computational Linguistics.\nForthcoming.\nGeoffrey\nE.\nHinton,\nNitish\nSrivastava,\nAlex\nKrizhevsky, Ilya Sutskever, and Ruslan Salakhut-\ndinov.\n2012.\nImproving neural networks by\npreventing\nco-adaptation\nof\nfeature\ndetectors.\nCoRR, abs/1207.0580.\nGeoffrey E. Hinton. 1989. Connectionist learning pro-\ncedures. Artif. Intell., 40(1-3):185\u2013234.\nZhiheng Huang, Marcus Thint, and Zengchang Qin.\n2008. Question classi\ufb01cation using head words and\ntheir hypernyms. In Proceedings of the Conference\non Empirical Methods in Natural Language Pro-\ncessing, EMNLP \u201908, pages 927\u2013936, Stroudsburg,\nPA, USA. Association for Computational Linguis-\ntics.\nNal Kalchbrenner and Phil Blunsom. 2013a. Recur-\nrent continuous translation models. In Proceedings\nof the 2013 Conference on Empirical Methods in\nNatural Language Processing, Seattle, October. As-\nsociation for Computational Linguistics.\nNal Kalchbrenner and Phil Blunsom. 2013b. Recur-\nrent Convolutional Neural Networks for Discourse\nCompositionality. In Proceedings of the Workshop\non Continuous Vector Space Models and their Com-\npositionality, So\ufb01a, Bulgaria, August. Association\nfor Computational Linguistics.\nDimitri Kartsaklis and Mehrnoosh Sadrzadeh. 2013.\nPrior disambiguation of word tensors for construct-\ning sentence vectors.\nIn Proceedings of the 2013\nConference on Empirical Methods in Natural Lan-\nguage Processing (EMNLP), Seattle, USA, October.\nAndreas K\u00a8uchler and Christoph Goller. 1996. Induc-\ntive learning in symbolic domains using structure-\ndriven recurrent neural networks. In G\u00a8unther G\u00a8orz\nand Steffen H\u00a8olldobler, editors, KI, volume 1137 of\nLecture Notes in Computer Science, pages 183\u2013197.\nSpringer.\nYann LeCun, L\u00b4eon Bottou, Yoshua Bengio, and Patrick\nHaffner. 1998. Gradient-based learning applied to\ndocument recognition.\nProceedings of the IEEE,\n86(11):2278\u20132324, November.\nXin Li and Dan Roth. 2002. Learning question clas-\nsi\ufb01ers.\nIn Proceedings of the 19th international\nconference on Computational linguistics-Volume 1,\npages 1\u20137. Association for Computational Linguis-\ntics.\nTomas Mikolov and Geoffrey Zweig. 2012. Context\ndependent recurrent neural network language model.\nIn SLT, pages 234\u2013239.\nTomas Mikolov, Stefan Kombrink, Lukas Burget, Jan\nCernock\u00b4y, and Sanjeev Khudanpur. 2011. Exten-\nsions of recurrent neural network language model.\nIn ICASSP, pages 5528\u20135531. IEEE.\nJeff Mitchell and Mirella Lapata. 2008. Vector-based\nmodels of semantic composition. In Proceedings of\nACL, volume 8.\nJeff Mitchell and Mirella Lapata. 2010. Composition\nin distributional models of semantics. Cognitive Sci-\nence, 34(8):1388\u20131429.\nJordan B. Pollack. 1990. Recursive distributed repre-\nsentations. Arti\ufb01cial Intelligence, 46:77\u2013105.\nHolger Schwenk. 2012. Continuous space translation\nmodels for phrase-based statistical machine transla-\ntion. In COLING (Posters), pages 1071\u20131080.\nJoo Silva, Lusa Coheur, AnaCristina Mendes, and An-\ndreas Wichert.\n2011.\nFrom symbolic to sub-\nsymbolic information in question classi\ufb01cation. Ar-\nti\ufb01cial Intelligence Review, 35(2):137\u2013154.\nRichard Socher, Jeffrey Pennington, Eric H. Huang,\nAndrew Y. Ng, and Christopher D. Manning. 2011.\nSemi-Supervised Recursive Autoencoders for Pre-\ndicting Sentiment Distributions. In Proceedings of\nthe 2011 Conference on Empirical Methods in Nat-\nural Language Processing (EMNLP).\nRichard Socher, Quoc V. Le, Christopher D. Manning,\nand Andrew Y. Ng.\n2013a.\nGrounded Composi-\ntional Semantics for Finding and Describing Images\nwith Sentences. In Transactions of the Association\nfor Computational Linguistics (TACL).\nRichard Socher, Alex Perelygin, Jean Wu, Jason\nChuang, Christopher D. Manning, Andrew Y. Ng,\nand Christopher Potts. 2013b. Recursive deep mod-\nels for semantic compositionality over a sentiment\ntreebank. In Proceedings of the 2013 Conference on\nEmpirical Methods in Natural Language Process-\ning, pages 1631\u20131642, Stroudsburg, PA, October.\nAssociation for Computational Linguistics.\nJoseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.\nWord representations: a simple and general method\nfor semi-supervised learning. In Proceedings of the\n48th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 384\u2013394. Association for\nComputational Linguistics.\nPeter Turney. 2012. Domain and function: A dual-\nspace model of semantic relations and compositions.\nJ. Artif. Intell. Res.(JAIR), 44:533\u2013585.\nAlexander Waibel, Toshiyuki Hanazawa, Geofrey Hin-\nton, Kiyohiro Shikano, and Kevin J. Lang. 1990.\nReadings in speech recognition. chapter Phoneme\nRecognition Using Time-delay Neural Networks,\npages 393\u2013404. Morgan Kaufmann Publishers Inc.,\nSan Francisco, CA, USA.\nFabio\nMassimo\nZanzotto,\nIoannis\nKorkontzelos,\nFrancesca Fallucchi, and Suresh Manandhar. 2010.\nEstimating linear models for compositional distri-\nbutional semantics. In Proceedings of the 23rd In-\nternational Conference on Computational Linguis-\ntics, pages 1263\u20131271. Association for Computa-\ntional Linguistics.\nLuke S. Zettlemoyer and Michael Collins.\n2005.\nLearning to map sentences to logical form: Struc-\ntured classi\ufb01cation with probabilistic categorial\ngrammars. In UAI, pages 658\u2013666. AUAI Press.\n",
        "sentence": " Several recent studies using ANNs have shown promising results, including convolutional neural networks (CNNs) (Kim, 2014; Blunsom et al., 2014; Kalchbrenner et al., 2014) and recursive neural networks (Socher et al.",
        "context": "to time-delay neural networks based on convo-\nlutional operations (Collobert and Weston, 2008;\nSocher et al., 2011; Kalchbrenner and Blunsom,\n2013b).\nNeural sentence models have a num-\nber of advantages. They can be trained to obtain\nNeural Network.\nMultiple layers of convolutional and dynamic\npooling operations induce a structured feature\ngraph over the input sentence. Figure 1 illustrates\nsuch a graph. Small \ufb01lters at higher layers can cap-\nNetwork (RNN) is a special case of the recursive\nnetwork where the structure that is followed is a\nsimple linear chain (Gers and Schmidhuber, 2001;\nMikolov et al., 2011). The RNN is primarily used\nas a language model, but may also be viewed as a"
    },
    {
        "title": "Dialog State Tracking Challenge 4: Handbook",
        "author": [
            "Kim et al.2015] Seokhwan Kim",
            "Luis Fernando D\u2019Haro",
            "Rafael E. Banchs",
            "Jason Williams",
            "Matthew Henderson"
        ],
        "venue": null,
        "citeRegEx": "Kim et al\\.,? \\Q2015\\E",
        "shortCiteRegEx": "Kim et al\\.",
        "year": 2015,
        "abstract": "",
        "full_text": "",
        "sentence": " \u2022 DSTC 4: Dialog State Tracking Challenge 4 (Kim et al., 2015; Kim et al., 2016).",
        "context": null
    },
    {
        "title": "The Fourth Dialog State Tracking Challenge",
        "author": [
            "Kim et al.2016] Seokhwan Kim",
            "Luis Fernando D\u2019Haro",
            "Rafael E. Banchs",
            "Jason Williams",
            "Matthew Henderson"
        ],
        "venue": "In Proceedings of the 7th International Workshop on Spoken Dialogue Systems (IWSDS)",
        "citeRegEx": "Kim et al\\.,? \\Q2016\\E",
        "shortCiteRegEx": "Kim et al\\.",
        "year": 2016,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " \u2022 DSTC 4: Dialog State Tracking Challenge 4 (Kim et al., 2015; Kim et al., 2016).",
        "context": null
    },
    {
        "title": "Convolutional neural networks for sentence classification",
        "author": [
            "Yoon Kim"
        ],
        "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing,",
        "citeRegEx": "Kim.,? \\Q2014\\E",
        "shortCiteRegEx": "Kim.",
        "year": 2014,
        "abstract": "We report on a series of experiments with convolutional neural networks (CNN)\ntrained on top of pre-trained word vectors for sentence-level classification\ntasks. We show that a simple CNN with little hyperparameter tuning and static\nvectors achieves excellent results on multiple benchmarks. Learning\ntask-specific vectors through fine-tuning offers further gains in performance.\nWe additionally propose a simple modification to the architecture to allow for\nthe use of both task-specific and static vectors. The CNN models discussed\nherein improve upon the state of the art on 4 out of 7 tasks, which include\nsentiment analysis and question classification.",
        "full_text": "Convolutional Neural Networks for Sentence Classi\ufb01cation\nYoon Kim\nNew York University\nyhk255@nyu.edu\nAbstract\nWe report on a series of experiments with\nconvolutional\nneural\nnetworks\n(CNN)\ntrained on top of pre-trained word vec-\ntors for sentence-level classi\ufb01cation tasks.\nWe show that a simple CNN with lit-\ntle hyperparameter tuning and static vec-\ntors achieves excellent results on multi-\nple benchmarks.\nLearning task-speci\ufb01c\nvectors through \ufb01ne-tuning offers further\ngains in performance.\nWe additionally\npropose a simple modi\ufb01cation to the ar-\nchitecture to allow for the use of both\ntask-speci\ufb01c and static vectors. The CNN\nmodels discussed herein improve upon the\nstate of the art on 4 out of 7 tasks, which\ninclude sentiment analysis and question\nclassi\ufb01cation.\n1\nIntroduction\nDeep learning models have achieved remarkable\nresults in computer vision (Krizhevsky et al.,\n2012) and speech recognition (Graves et al., 2013)\nin recent years. Within natural language process-\ning, much of the work with deep learning meth-\nods has involved learning word vector representa-\ntions through neural language models (Bengio et\nal., 2003; Yih et al., 2011; Mikolov et al., 2013)\nand performing composition over the learned word\nvectors for classi\ufb01cation (Collobert et al., 2011).\nWord vectors, wherein words are projected from a\nsparse, 1-of-V encoding (here V is the vocabulary\nsize) onto a lower dimensional vector space via a\nhidden layer, are essentially feature extractors that\nencode semantic features of words in their dimen-\nsions. In such dense representations, semantically\nclose words are likewise close\u2014in euclidean or\ncosine distance\u2014in the lower dimensional vector\nspace.\nConvolutional neural networks (CNN) utilize\nlayers with convolving \ufb01lters that are applied to\nlocal features (LeCun et al., 1998).\nOriginally\ninvented for computer vision, CNN models have\nsubsequently been shown to be effective for NLP\nand have achieved excellent results in semantic\nparsing (Yih et al., 2014), search query retrieval\n(Shen et al., 2014), sentence modeling (Kalch-\nbrenner et al., 2014), and other traditional NLP\ntasks (Collobert et al., 2011).\nIn the present work, we train a simple CNN with\none layer of convolution on top of word vectors\nobtained from an unsupervised neural language\nmodel. These vectors were trained by Mikolov et\nal. (2013) on 100 billion words of Google News,\nand are publicly available.1 We initially keep the\nword vectors static and learn only the other param-\neters of the model. Despite little tuning of hyper-\nparameters, this simple model achieves excellent\nresults on multiple benchmarks, suggesting that\nthe pre-trained vectors are \u2018universal\u2019 feature ex-\ntractors that can be utilized for various classi\ufb01ca-\ntion tasks. Learning task-speci\ufb01c vectors through\n\ufb01ne-tuning results in further improvements. We\n\ufb01nally describe a simple modi\ufb01cation to the archi-\ntecture to allow for the use of both pre-trained and\ntask-speci\ufb01c vectors by having multiple channels.\nOur work is philosophically similar to Razavian\net al. (2014) which showed that for image clas-\nsi\ufb01cation, feature extractors obtained from a pre-\ntrained deep learning model perform well on a va-\nriety of tasks\u2014including tasks that are very dif-\nferent from the original task for which the feature\nextractors were trained.\n2\nModel\nThe model architecture, shown in \ufb01gure 1, is a\nslight variant of the CNN architecture of Collobert\net al. (2011). Let xi \u2208Rk be the k-dimensional\nword vector corresponding to the i-th word in the\nsentence. A sentence of length n (padded where\n1https://code.google.com/p/word2vec/\narXiv:1408.5882v2  [cs.CL]  3 Sep 2014\nwait \nfor \nthe \nvideo \nand \ndo \nn't \nrent \nit \nn x k representation of \nsentence with static and \nnon-static channels \nConvolutional layer with \nmultiple filter widths and \nfeature maps \nMax-over-time \npooling \nFully connected layer \nwith dropout and  \nsoftmax output \nFigure 1: Model architecture with two channels for an example sentence.\nnecessary) is represented as\nx1:n = x1 \u2295x2 \u2295. . . \u2295xn,\n(1)\nwhere \u2295is the concatenation operator. In gen-\neral, let xi:i+j refer to the concatenation of words\nxi, xi+1, . . . , xi+j.\nA convolution operation in-\nvolves a \ufb01lter w \u2208Rhk, which is applied to a\nwindow of h words to produce a new feature. For\nexample, a feature ci is generated from a window\nof words xi:i+h\u22121 by\nci = f(w \u00b7 xi:i+h\u22121 + b).\n(2)\nHere b \u2208R is a bias term and f is a non-linear\nfunction such as the hyperbolic tangent. This \ufb01lter\nis applied to each possible window of words in the\nsentence {x1:h, x2:h+1, . . . , xn\u2212h+1:n} to produce\na feature map\nc = [c1, c2, . . . , cn\u2212h+1],\n(3)\nwith c \u2208Rn\u2212h+1. We then apply a max-over-\ntime pooling operation (Collobert et al., 2011)\nover the feature map and take the maximum value\n\u02c6c = max{c} as the feature corresponding to this\nparticular \ufb01lter. The idea is to capture the most im-\nportant feature\u2014one with the highest value\u2014for\neach feature map. This pooling scheme naturally\ndeals with variable sentence lengths.\nWe have described the process by which one\nfeature is extracted from one \ufb01lter.\nThe model\nuses multiple \ufb01lters (with varying window sizes)\nto obtain multiple features. These features form\nthe penultimate layer and are passed to a fully con-\nnected softmax layer whose output is the probabil-\nity distribution over labels.\nIn one of the model variants, we experiment\nwith having two \u2018channels\u2019 of word vectors\u2014one\nthat is kept static throughout training and one that\nis \ufb01ne-tuned via backpropagation (section 3.2).2\nIn the multichannel architecture, illustrated in \ufb01g-\nure 1, each \ufb01lter is applied to both channels and\nthe results are added to calculate ci in equation\n(2). The model is otherwise equivalent to the sin-\ngle channel architecture.\n2.1\nRegularization\nFor regularization we employ dropout on the\npenultimate layer with a constraint on l2-norms of\nthe weight vectors (Hinton et al., 2012). Dropout\nprevents co-adaptation of hidden units by ran-\ndomly dropping out\u2014i.e., setting to zero\u2014a pro-\nportion p of the hidden units during foward-\nbackpropagation. That is, given the penultimate\nlayer z = [\u02c6c1, . . . , \u02c6cm] (note that here we have m\n\ufb01lters), instead of using\ny = w \u00b7 z + b\n(4)\nfor output unit y in forward propagation, dropout\nuses\ny = w \u00b7 (z \u25e6r) + b,\n(5)\nwhere \u25e6is the element-wise multiplication opera-\ntor and r \u2208Rm is a \u2018masking\u2019 vector of Bernoulli\nrandom variables with probability p of being 1.\nGradients are backpropagated only through the\nunmasked units. At test time, the learned weight\nvectors are scaled by p such that \u02c6w = pw, and\n\u02c6w is used (without dropout) to score unseen sen-\ntences. We additionally constrain l2-norms of the\nweight vectors by rescaling w to have ||w||2 = s\nwhenever ||w||2 > s after a gradient descent step.\n2We employ language from computer vision where a color\nimage has red, green, and blue channels.\nData\nc\nl\nN\n|V |\n|Vpre|\nTest\nMR\n2\n20 10662 18765 16448\nCV\nSST-1\n5\n18 11855 17836 16262\n2210\nSST-2\n2\n19\n9613\n16185 14838\n1821\nSubj\n2\n23 10000 21323 17913\nCV\nTREC\n6\n10\n5952\n9592\n9125\n500\nCR\n2\n19\n3775\n5340\n5046\nCV\nMPQA\n2\n3\n10606\n6246\n6083\nCV\nTable 1: Summary statistics for the datasets after tokeniza-\ntion. c: Number of target classes. l: Average sentence length.\nN: Dataset size. |V |: Vocabulary size. |Vpre|: Number of\nwords present in the set of pre-trained word vectors. Test:\nTest set size (CV means there was no standard train/test split\nand thus 10-fold CV was used).\n3\nDatasets and Experimental Setup\nWe test our model on various benchmarks. Sum-\nmary statistics of the datasets are in table 1.\n\u2022 MR: Movie reviews with one sentence per re-\nview. Classi\ufb01cation involves detecting posi-\ntive/negative reviews (Pang and Lee, 2005).3\n\u2022 SST-1:\nStanford Sentiment Treebank\u2014an\nextension of MR but with train/dev/test splits\nprovided and \ufb01ne-grained labels (very pos-\nitive, positive, neutral, negative, very nega-\ntive), re-labeled by Socher et al. (2013).4\n\u2022 SST-2: Same as SST-1 but with neutral re-\nviews removed and binary labels.\n\u2022 Subj: Subjectivity dataset where the task is\nto classify a sentence as being subjective or\nobjective (Pang and Lee, 2004).\n\u2022 TREC: TREC question dataset\u2014task in-\nvolves classifying a question into 6 question\ntypes (whether the question is about person,\nlocation, numeric information, etc.) (Li and\nRoth, 2002).5\n\u2022 CR: Customer reviews of various products\n(cameras, MP3s etc.). Task is to predict pos-\nitive/negative reviews (Hu and Liu, 2004).6\n3https://www.cs.cornell.edu/people/pabo/movie-review-data/\n4http://nlp.stanford.edu/sentiment/ Data is actually provided\nat the phrase-level and hence we train the model on both\nphrases and sentences but only score on sentences at test\ntime, as in Socher et al. (2013), Kalchbrenner et al. (2014),\nand Le and Mikolov (2014). Thus the training set is an order\nof magnitude larger than listed in table 1.\n5http://cogcomp.cs.illinois.edu/Data/QA/QC/\n6http://www.cs.uic.edu/\u223cliub/FBS/sentiment-analysis.html\n\u2022 MPQA: Opinion polarity detection subtask\nof the MPQA dataset (Wiebe et al., 2005).7\n3.1\nHyperparameters and Training\nFor all datasets we use: recti\ufb01ed linear units, \ufb01lter\nwindows (h) of 3, 4, 5 with 100 feature maps each,\ndropout rate (p) of 0.5, l2 constraint (s) of 3, and\nmini-batch size of 50. These values were chosen\nvia a grid search on the SST-2 dev set.\nWe do not otherwise perform any dataset-\nspeci\ufb01c tuning other than early stopping on dev\nsets. For datasets without a standard dev set we\nrandomly select 10% of the training data as the\ndev set. Training is done through stochastic gra-\ndient descent over shuf\ufb02ed mini-batches with the\nAdadelta update rule (Zeiler, 2012).\n3.2\nPre-trained Word Vectors\nInitializing word vectors with those obtained from\nan unsupervised neural language model is a popu-\nlar method to improve performance in the absence\nof a large supervised training set (Collobert et al.,\n2011; Socher et al., 2011; Iyyer et al., 2014). We\nuse the publicly available word2vec vectors that\nwere trained on 100 billion words from Google\nNews. The vectors have dimensionality of 300 and\nwere trained using the continuous bag-of-words\narchitecture (Mikolov et al., 2013).\nWords not\npresent in the set of pre-trained words are initial-\nized randomly.\n3.3\nModel Variations\nWe experiment with several variants of the model.\n\u2022 CNN-rand: Our baseline model where all\nwords are randomly initialized and then mod-\ni\ufb01ed during training.\n\u2022 CNN-static:\nA model with pre-trained\nvectors from word2vec.\nAll words\u2014\nincluding the unknown ones that are ran-\ndomly initialized\u2014are kept static and only\nthe other parameters of the model are learned.\n\u2022 CNN-non-static: Same as above but the pre-\ntrained vectors are \ufb01ne-tuned for each task.\n\u2022 CNN-multichannel: A model with two sets\nof word vectors. Each set of vectors is treated\nas a \u2018channel\u2019 and each \ufb01lter is applied\n7http://www.cs.pitt.edu/mpqa/\nModel\nMR\nSST-1\nSST-2\nSubj\nTREC\nCR\nMPQA\nCNN-rand\n76.1\n45.0\n82.7\n89.6\n91.2\n79.8\n83.4\nCNN-static\n81.0\n45.5\n86.8\n93.0\n92.8\n84.7\n89.6\nCNN-non-static\n81.5\n48.0\n87.2\n93.4\n93.6\n84.3\n89.5\nCNN-multichannel\n81.1\n47.4\n88.1\n93.2\n92.2\n85.0\n89.4\nRAE (Socher et al., 2011)\n77.7\n43.2\n82.4\n\u2212\n\u2212\n\u2212\n86.4\nMV-RNN (Socher et al., 2012)\n79.0\n44.4\n82.9\n\u2212\n\u2212\n\u2212\n\u2212\nRNTN (Socher et al., 2013)\n\u2212\n45.7\n85.4\n\u2212\n\u2212\n\u2212\n\u2212\nDCNN (Kalchbrenner et al., 2014)\n\u2212\n48.5\n86.8\n\u2212\n93.0\n\u2212\n\u2212\nParagraph-Vec (Le and Mikolov, 2014)\n\u2212\n48.7\n87.8\n\u2212\n\u2212\n\u2212\n\u2212\nCCAE (Hermann and Blunsom, 2013)\n77.8\n\u2212\n\u2212\n\u2212\n\u2212\n\u2212\n87.2\nSent-Parser (Dong et al., 2014)\n79.5\n\u2212\n\u2212\n\u2212\n\u2212\n\u2212\n86.3\nNBSVM (Wang and Manning, 2012)\n79.4\n\u2212\n\u2212\n93.2\n\u2212\n81.8\n86.3\nMNB (Wang and Manning, 2012)\n79.0\n\u2212\n\u2212\n93.6\n\u2212\n80.0\n86.3\nG-Dropout (Wang and Manning, 2013)\n79.0\n\u2212\n\u2212\n93.4\n\u2212\n82.1\n86.1\nF-Dropout (Wang and Manning, 2013)\n79.1\n\u2212\n\u2212\n93.6\n\u2212\n81.9\n86.3\nTree-CRF (Nakagawa et al., 2010)\n77.3\n\u2212\n\u2212\n\u2212\n\u2212\n81.4\n86.1\nCRF-PR (Yang and Cardie, 2014)\n\u2212\n\u2212\n\u2212\n\u2212\n\u2212\n82.7\n\u2212\nSVMS (Silva et al., 2011)\n\u2212\n\u2212\n\u2212\n\u2212\n95.0\n\u2212\n\u2212\nTable 2: Results of our CNN models against other methods. RAE: Recursive Autoencoders with pre-trained word vectors from\nWikipedia (Socher et al., 2011). MV-RNN: Matrix-Vector Recursive Neural Network with parse trees (Socher et al., 2012).\nRNTN: Recursive Neural Tensor Network with tensor-based feature function and parse trees (Socher et al., 2013). DCNN:\nDynamic Convolutional Neural Network with k-max pooling (Kalchbrenner et al., 2014). Paragraph-Vec: Logistic regres-\nsion on top of paragraph vectors (Le and Mikolov, 2014). CCAE: Combinatorial Category Autoencoders with combinatorial\ncategory grammar operators (Hermann and Blunsom, 2013). Sent-Parser: Sentiment analysis-speci\ufb01c parser (Dong et al.,\n2014). NBSVM, MNB: Naive Bayes SVM and Multinomial Naive Bayes with uni-bigrams from Wang and Manning (2012).\nG-Dropout, F-Dropout: Gaussian Dropout and Fast Dropout from Wang and Manning (2013). Tree-CRF: Dependency tree\nwith Conditional Random Fields (Nakagawa et al., 2010). CRF-PR: Conditional Random Fields with Posterior Regularization\n(Yang and Cardie, 2014). SVMS: SVM with uni-bi-trigrams, wh word, head word, POS, parser, hypernyms, and 60 hand-coded\nrules as features from Silva et al. (2011).\nto both channels, but gradients are back-\npropagated only through one of the chan-\nnels. Hence the model is able to \ufb01ne-tune\none set of vectors while keeping the other\nstatic.\nBoth channels are initialized with\nword2vec.\nIn order to disentangle the effect of the above\nvariations versus other random factors, we elim-\ninate other sources of randomness\u2014CV-fold as-\nsignment, initialization of unknown word vec-\ntors, initialization of CNN parameters\u2014by keep-\ning them uniform within each dataset.\n4\nResults and Discussion\nResults of our models against other methods are\nlisted in table 2. Our baseline model with all ran-\ndomly initialized words (CNN-rand) does not per-\nform well on its own. While we had expected per-\nformance gains through the use of pre-trained vec-\ntors, we were surprised at the magnitude of the\ngains. Even a simple model with static vectors\n(CNN-static) performs remarkably well, giving\ncompetitive results against the more sophisticated\ndeep learning models that utilize complex pool-\ning schemes (Kalchbrenner et al., 2014) or require\nparse trees to be computed beforehand (Socher\net al., 2013). These results suggest that the pre-\ntrained vectors are good, \u2018universal\u2019 feature ex-\ntractors and can be utilized across datasets. Fine-\ntuning the pre-trained vectors for each task gives\nstill further improvements (CNN-non-static).\n4.1\nMultichannel vs. Single Channel Models\nWe had initially hoped that the multichannel ar-\nchitecture would prevent over\ufb01tting (by ensuring\nthat the learned vectors do not deviate too far\nfrom the original values) and thus work better than\nthe single channel model, especially on smaller\ndatasets. The results, however, are mixed, and fur-\nther work on regularizing the \ufb01ne-tuning process\nis warranted.\nFor instance, instead of using an\nadditional channel for the non-static portion, one\ncould maintain a single channel but employ extra\ndimensions that are allowed to be modi\ufb01ed during\ntraining.\nMost Similar Words for\nStatic Channel\nNon-static Channel\nbad\ngood\nterrible\nterrible\nhorrible\nhorrible\nlousy\nlousy\nstupid\ngood\ngreat\nnice\nbad\ndecent\nterri\ufb01c\nsolid\ndecent\nterri\ufb01c\nn\u2019t\nos\nnot\nca\nnever\nireland\nnothing\nwo\nneither\n!\n2,500\n2,500\nentire\nlush\njez\nbeautiful\nchanger\nterri\ufb01c\n,\ndecasia\nbut\nabysmally\ndragon\ndemise\na\nvaliant\nand\nTable 3:\nTop 4 neighboring words\u2014based on cosine\nsimilarity\u2014for vectors in the static channel (left) and \ufb01ne-\ntuned vectors in the non-static channel (right) from the mul-\ntichannel model on the SST-2 dataset after training.\n4.2\nStatic vs. Non-static Representations\nAs is the case with the single channel non-static\nmodel, the multichannel model is able to \ufb01ne-tune\nthe non-static channel to make it more speci\ufb01c to\nthe task-at-hand. For example, good is most sim-\nilar to bad in word2vec, presumably because\nthey are (almost) syntactically equivalent. But for\nvectors in the non-static channel that were \ufb01ne-\ntuned on the SST-2 dataset, this is no longer the\ncase (table 3). Similarly, good is arguably closer\nto nice than it is to great for expressing sentiment,\nand this is indeed re\ufb02ected in the learned vectors.\nFor (randomly initialized) tokens not in the set\nof pre-trained vectors, \ufb01ne-tuning allows them to\nlearn more meaningful representations: the net-\nwork learns that exclamation marks are associ-\nated with effusive expressions and that commas\nare conjunctive (table 3).\n4.3\nFurther Observations\nWe report on some further experiments and obser-\nvations:\n\u2022 Kalchbrenner et al.\n(2014) report much\nworse results with a CNN that has essentially\nthe same architecture as our single channel\nmodel. For example, their Max-TDNN (Time\nDelay Neural Network) with randomly ini-\ntialized words obtains 37.4% on the SST-1\ndataset, compared to 45.0% for our model.\nWe attribute such discrepancy to our CNN\nhaving much more capacity (multiple \ufb01lter\nwidths and feature maps).\n\u2022 Dropout proved to be such a good regularizer\nthat it was \ufb01ne to use a larger than necessary\nnetwork and simply let dropout regularize it.\nDropout consistently added 2%\u20134% relative\nperformance.\n\u2022 When randomly initializing words not in\nword2vec, we obtained slight improve-\nments by sampling each dimension from\nU[\u2212a, a] where a was chosen such that the\nrandomly initialized vectors have the same\nvariance as the pre-trained ones. It would be\ninteresting to see if employing more sophis-\nticated methods to mirror the distribution of\npre-trained vectors in the initialization pro-\ncess gives further improvements.\n\u2022 We brie\ufb02y experimented with another set of\npublicly available word vectors trained by\nCollobert et al. (2011) on Wikipedia,8 and\nfound that word2vec gave far superior per-\nformance. It is not clear whether this is due\nto Mikolov et al. (2013)\u2019s architecture or the\n100 billion word Google News dataset.\n\u2022 Adadelta (Zeiler, 2012) gave similar results\nto Adagrad (Duchi et al., 2011) but required\nfewer epochs.\n5\nConclusion\nIn the present work we have described a series of\nexperiments with convolutional neural networks\nbuilt on top of word2vec. Despite little tuning\nof hyperparameters, a simple CNN with one layer\nof convolution performs remarkably well. Our re-\nsults add to the well-established evidence that un-\nsupervised pre-training of word vectors is an im-\nportant ingredient in deep learning for NLP.\nAcknowledgments\nWe would like to thank Yann LeCun and the\nanonymous reviewers for their helpful feedback\nand suggestions.\n8http://ronan.collobert.com/senna/\nReferences\nY. Bengio, R. Ducharme, P. Vincent.\n2003.\nNeu-\nral Probabilitistic Language Model. Journal of Ma-\nchine Learning Research 3:1137\u20131155.\nR. Collobert, J. Weston, L. Bottou, M. Karlen, K.\nKavukcuglu, P. Kuksa.\n2011.\nNatural Language\nProcessing (Almost) from Scratch. Journal of Ma-\nchine Learning Research 12:2493\u20132537.\nJ. Duchi, E. Hazan, Y. Singer. 2011 Adaptive subgra-\ndient methods for online learning and stochastic op-\ntimization. Journal of Machine Learning Research,\n12:2121\u20132159.\nL. Dong, F. Wei, S. Liu, M. Zhou, K. Xu. 2014. A\nStatistical Parsing Framework for Sentiment Classi-\n\ufb01cation. CoRR, abs/1401.6330.\nA. Graves, A. Mohamed, G. Hinton. 2013. Speech\nrecognition with deep recurrent neural networks. In\nProceedings of ICASSP 2013.\nG. Hinton, N. Srivastava, A. Krizhevsky, I. Sutskever,\nR. Salakhutdinov.\n2012.\nImproving neural net-\nworks by preventing co-adaptation of feature detec-\ntors. CoRR, abs/1207.0580.\nK. Hermann, P. Blunsom. 2013. The Role of Syntax in\nVector Space Models of Compositional Semantics.\nIn Proceedings of ACL 2013.\nM. Hu, B. Liu. 2004. Mining and Summarizing Cus-\ntomer Reviews. In Proceedings of ACM SIGKDD\n2004.\nM. Iyyer, P. Enns, J. Boyd-Graber, P. Resnik\n2014.\nPolitical Ideology Detection Using Recursive Neural\nNetworks. In Proceedings of ACL 2014.\nN. Kalchbrenner, E. Grefenstette, P. Blunsom. 2014. A\nConvolutional Neural Network for Modelling Sen-\ntences. In Proceedings of ACL 2014.\nA. Krizhevsky, I. Sutskever, G. Hinton. 2012. Ima-\ngeNet Classi\ufb01cation with Deep Convolutional Neu-\nral Networks. In Proceedings of NIPS 2012.\nQ. Le, T. Mikolov. 2014. Distributed Represenations\nof Sentences and Documents.\nIn Proceedings of\nICML 2014.\nY. LeCun, L. Bottou, Y. Bengio, P. Haffner.\n1998.\nGradient-based learning applied to document recog-\nnition. In Proceedings of the IEEE, 86(11):2278\u2013\n2324, November.\nX. Li, D. Roth. 2002. Learning Question Classi\ufb01ers.\nIn Proceedings of ACL 2002.\nT. Mikolov, I. Sutskever, K. Chen, G. Corrado, J. Dean.\n2013.\nDistributed Representations of Words and\nPhrases and their Compositionality. In Proceedings\nof NIPS 2013.\nT. Nakagawa, K. Inui, S. Kurohashi.\n2010.\nDe-\npendency tree-based sentiment classi\ufb01cation using\nCRFs with hidden variables. In Proceedings of ACL\n2010.\nB. Pang, L. Lee.\n2004.\nA sentimental education:\nSentiment analysis using subjectivity summarization\nbased on minimum cuts.\nIn Proceedings of ACL\n2004.\nB. Pang, L. Lee. 2005. Seeing stars: Exploiting class\nrelationships for sentiment categorization with re-\nspect to rating scales. In Proceedings of ACL 2005.\nA.S. Razavian, H. Azizpour, J. Sullivan, S. Carlsson\n2014. CNN Features off-the-shelf: an Astounding\nBaseline. CoRR, abs/1403.6382.\nY. Shen, X. He, J. Gao, L. Deng, G. Mesnil. 2014.\nLearning Semantic Representations Using Convolu-\ntional Neural Networks for Web Search. In Proceed-\nings of WWW 2014.\nJ. Silva, L. Coheur, A. Mendes, A. Wichert.\n2011.\nFrom symbolic to sub-symbolic information in ques-\ntion classi\ufb01cation.\nArti\ufb01cial Intelligence Review,\n35(2):137\u2013154.\nR. Socher, J. Pennington, E. Huang, A. Ng, C. Man-\nning.\n2011.\nSemi-Supervised Recursive Autoen-\ncoders for Predicting Sentiment Distributions.\nIn\nProceedings of EMNLP 2011.\nR. Socher, B. Huval, C. Manning, A. Ng. 2012. Se-\nmantic Compositionality through Recursive Matrix-\nVector Spaces. In Proceedings of EMNLP 2012.\nR. Socher, A. Perelygin, J. Wu, J. Chuang, C. Manning,\nA. Ng, C. Potts. 2013. Recursive Deep Models for\nSemantic Compositionality Over a Sentiment Tree-\nbank. In Proceedings of EMNLP 2013.\nJ. Wiebe, T. Wilson, C. Cardie. 2005. Annotating Ex-\npressions of Opinions and Emotions in Language.\nLanguage Resources and Evaluation, 39(2-3): 165\u2013\n210.\nS. Wang, C. Manning. 2012. Baselines and Bigrams:\nSimple, Good Sentiment and Topic Classi\ufb01cation.\nIn Proceedings of ACL 2012.\nS. Wang, C. Manning. 2013. Fast Dropout Training.\nIn Proceedings of ICML 2013.\nB. Yang, C. Cardie. 2014. Context-aware Learning\nfor Sentence-level Sentiment Analysis with Poste-\nrior Regularization. In Proceedings of ACL 2014.\nW. Yih, K. Toutanova, J. Platt, C. Meek. 2011. Learn-\ning Discriminative Projections for Text Similarity\nMeasures.\nProceedings of the Fifteenth Confer-\nence on Computational Natural Language Learning,\n247\u2013256.\nW. Yih, X. He, C. Meek. 2014. Semantic Parsing for\nSingle-Relation Question Answering. In Proceed-\nings of ACL 2014.\nM. Zeiler. 2012. Adadelta: An adaptive learning rate\nmethod. CoRR, abs/1212.5701.\n",
        "sentence": " Several recent studies using ANNs have shown promising results, including convolutional neural networks (CNNs) (Kim, 2014; Blunsom et al., 2014; Kalchbrenner et al., 2014) and recursive neural networks (Socher et al.",
        "context": "space.\nConvolutional neural networks (CNN) utilize\nlayers with convolving \ufb01lters that are applied to\nlocal features (LeCun et al., 1998).\nOriginally\ninvented for computer vision, CNN models have\nsubsequently been shown to be effective for NLP\nin recent years. Within natural language process-\ning, much of the work with deep learning meth-\nods has involved learning word vector representa-\ntions through neural language models (Bengio et\nal., 2003; Yih et al., 2011; Mikolov et al., 2013)\nvations:\n\u2022 Kalchbrenner et al.\n(2014) report much\nworse results with a CNN that has essentially\nthe same architecture as our single channel\nmodel. For example, their Max-TDNN (Time\nDelay Neural Network) with randomly ini-"
    },
    {
        "title": "Token-based chunking of turninternal dialogue act sequences",
        "author": [
            "Lendvai",
            "Geertzen2007] Piroska Lendvai",
            "Jeroen Geertzen"
        ],
        "venue": "In Proceedings of the 8th SIGDIAL Workshop on Discourse and Dialogue,",
        "citeRegEx": "Lendvai et al\\.,? \\Q2007\\E",
        "shortCiteRegEx": "Lendvai et al\\.",
        "year": 2007,
        "abstract": "",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "Rnnlm-recurrent neural network language modeling toolkit",
        "author": [
            "Stefan Kombrink",
            "Anoop Deoras",
            "Lukar Burget",
            "Jan Cernocky"
        ],
        "venue": "In Proc. of the 2011 ASRU Workshop,",
        "citeRegEx": "Mikolov et al\\.,? \\Q2011\\E",
        "shortCiteRegEx": "Mikolov et al\\.",
        "year": 2011,
        "abstract": "",
        "full_text": "",
        "sentence": " , 2011) and RNNLM (Mikolov et al., 2011) word vectors.",
        "context": null
    },
    {
        "title": "Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781",
        "author": [
            "Kai Chen",
            "Greg Corrado",
            "Jeffrey Dean"
        ],
        "venue": null,
        "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E",
        "shortCiteRegEx": "Mikolov et al\\.",
        "year": 2013,
        "abstract": "",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "Distributed representations of words and phrases and their compositionality",
        "author": [
            "Ilya Sutskever",
            "Kai Chen",
            "Greg S Corrado",
            "Jeff Dean"
        ],
        "venue": "In Advances in neural information processing systems,",
        "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E",
        "shortCiteRegEx": "Mikolov et al\\.",
        "year": 2013,
        "abstract": "The recently introduced continuous Skip-gram model is an efficient method for\nlearning high-quality distributed vector representations that capture a large\nnumber of precise syntactic and semantic word relationships. In this paper we\npresent several extensions that improve both the quality of the vectors and the\ntraining speed. By subsampling of the frequent words we obtain significant\nspeedup and also learn more regular word representations. We also describe a\nsimple alternative to the hierarchical softmax called negative sampling. An\ninherent limitation of word representations is their indifference to word order\nand their inability to represent idiomatic phrases. For example, the meanings\nof \"Canada\" and \"Air\" cannot be easily combined to obtain \"Air Canada\".\nMotivated by this example, we present a simple method for finding phrases in\ntext, and show that learning good vector representations for millions of\nphrases is possible.",
        "full_text": "arXiv:1310.4546v1  [cs.CL]  16 Oct 2013\nDistributed Representations of Words and Phrases\nand their Compositionality\nTomas Mikolov\nGoogle Inc.\nMountain View\nmikolov@google.com\nIlya Sutskever\nGoogle Inc.\nMountain View\nilyasu@google.com\nKai Chen\nGoogle Inc.\nMountain View\nkai@google.com\nGreg Corrado\nGoogle Inc.\nMountain View\ngcorrado@google.com\nJeffrey Dean\nGoogle Inc.\nMountain View\njeff@google.com\nAbstract\nThe recently introduced continuous Skip-gram model is an ef\ufb01cient method for\nlearning high-quality distributed vector representations that capture a large num-\nber of precise syntactic and semantic word relationships. In this paper we present\nseveral extensions that improve both the quality of the vectors and the training\nspeed. By subsampling of the frequent words we obtain signi\ufb01cant speedup and\nalso learn more regular word representations. We also describe a simple alterna-\ntive to the hierarchical softmax called negative sampling.\nAn inherent limitation of word representations is their indifference to word order\nand their inability to represent idiomatic phrases. For example, the meanings of\n\u201cCanada\u201d and \u201cAir\u201d cannot be easily combined to obtain \u201cAir Canada\u201d. Motivated\nby this example, we present a simple method for \ufb01nding phrases in text, and show\nthat learning good vector representations for millions of phrases is possible.\n1\nIntroduction\nDistributed representations of words in a vector space help learning algorithms to achieve better\nperformance in natural language processing tasks by grouping similar words. One of the earliest use\nof word representations dates back to 1986 due to Rumelhart, Hinton, and Williams [13]. This idea\nhas since been applied to statistical language modeling with considerable success [1]. The follow\nup work includes applications to automatic speech recognition and machine translation [14, 7], and\na wide range of NLP tasks [2, 20, 15, 3, 18, 19, 9].\nRecently, Mikolov et al. [8] introduced the Skip-gram model, an ef\ufb01cient method for learning high-\nquality vector representations of words from large amounts of unstructured text data. Unlike most\nof the previously used neural network architectures for learning word vectors, training of the Skip-\ngram model (see Figure 1) does not involve dense matrix multiplications. This makes the training\nextremely ef\ufb01cient: an optimized single-machine implementation can train on more than 100 billion\nwords in one day.\nThe word representations computed using neural networks are very interesting because the learned\nvectors explicitly encode many linguistic regularities and patterns. Somewhat surprisingly, many of\nthese patterns can be represented as linear translations. For example, the result of a vector calcula-\ntion vec(\u201cMadrid\u201d) - vec(\u201cSpain\u201d) + vec(\u201cFrance\u201d) is closer to vec(\u201cParis\u201d) than to any other word\nvector [9, 8].\n1\n\u0001\u0002\u0003\u0004\n\u0005\u0006\u0007\b\u0003\t\t\t\t\t\t\t\t\t\t\t\u0007\n\u000b\f\r\u000e\u0003\u000f\u000b\u0006\t\t\t\t\t\t\u000b\b\u0003\u0007\b\u0003\n\u0001\u0002\u0003\u0010\u0011\u0004\n\u0001\u0002\u0003\u0010\u0012\u0004\n\u0001\u0002\u0003\u0013\u0012\u0004\n\u0001\u0002\u0003\u0013\u0011\u0004\nFigure 1: The Skip-gram model architecture. The training objective is to learn word vector representations\nthat are good at predicting the nearby words.\nIn this paper we present several extensions of the original Skip-gram model. We show that sub-\nsampling of frequent words during training results in a signi\ufb01cant speedup (around 2x - 10x), and\nimproves accuracy of the representations of less frequent words. In addition, we present a simpli-\n\ufb01ed variant of Noise Contrastive Estimation (NCE) [4] for training the Skip-gram model that results\nin faster training and better vector representations for frequent words, compared to more complex\nhierarchical softmax that was used in the prior work [8].\nWord representations are limited by their inability to represent idiomatic phrases that are not com-\npositions of the individual words. For example, \u201cBoston Globe\u201d is a newspaper, and so it is not a\nnatural combination of the meanings of \u201cBoston\u201d and \u201cGlobe\u201d. Therefore, using vectors to repre-\nsent the whole phrases makes the Skip-gram model considerably more expressive. Other techniques\nthat aim to represent meaning of sentences by composing the word vectors, such as the recursive\nautoencoders [15], would also bene\ufb01t from using phrase vectors instead of the word vectors.\nThe extension from word based to phrase based models is relatively simple. First we identify a large\nnumber of phrases using a data-driven approach, and then we treat the phrases as individual tokens\nduring the training. To evaluate the quality of the phrase vectors, we developed a test set of analogi-\ncal reasoning tasks that contains both words and phrases. A typical analogy pair from our test set is\n\u201cMontreal\u201d:\u201cMontreal Canadiens\u201d::\u201cToronto\u201d:\u201cToronto Maple Leafs\u201d. It is considered to have been\nanswered correctly if the nearest representation to vec(\u201cMontreal Canadiens\u201d) - vec(\u201cMontreal\u201d) +\nvec(\u201cToronto\u201d) is vec(\u201cToronto Maple Leafs\u201d).\nFinally, we describe another interesting property of the Skip-gram model. We found that simple\nvector addition can often produce meaningful results. For example, vec(\u201cRussia\u201d) + vec(\u201criver\u201d) is\nclose to vec(\u201cVolga River\u201d), and vec(\u201cGermany\u201d) + vec(\u201ccapital\u201d) is close to vec(\u201cBerlin\u201d). This\ncompositionality suggests that a non-obvious degree of language understanding can be obtained by\nusing basic mathematical operations on the word vector representations.\n2\nThe Skip-gram Model\nThe training objective of the Skip-gram model is to \ufb01nd word representations that are useful for\npredicting the surrounding words in a sentence or a document. More formally, given a sequence of\ntraining words w1, w2, w3, . . . , wT , the objective of the Skip-gram model is to maximize the average\nlog probability\n1\nT\nT\nX\nt=1\nX\n\u2212c\u2264j\u2264c,j\u0338=0\nlog p(wt+j|wt)\n(1)\nwhere c is the size of the training context (which can be a function of the center word wt). Larger\nc results in more training examples and thus can lead to a higher accuracy, at the expense of the\n2\ntraining time. The basic Skip-gram formulation de\ufb01nes p(wt+j|wt) using the softmax function:\np(wO|wI) =\nexp\n\u0010\nv\u2032\nwO\n\u22a4vwI\n\u0011\nPW\nw=1 exp\n\u0010\nv\u2032w\n\u22a4vwI\n\u0011\n(2)\nwhere vw and v\u2032\nw are the \u201cinput\u201d and \u201coutput\u201d vector representations of w, and W is the num-\nber of words in the vocabulary. This formulation is impractical because the cost of computing\n\u2207log p(wO|wI) is proportional to W, which is often large (105\u2013107 terms).\n2.1\nHierarchical Softmax\nA computationally ef\ufb01cient approximation of the full softmax is the hierarchical softmax. In the\ncontext of neural network language models, it was \ufb01rst introduced by Morin and Bengio [12]. The\nmain advantage is that instead of evaluating W output nodes in the neural network to obtain the\nprobability distribution, it is needed to evaluate only about log2(W) nodes.\nThe hierarchical softmax uses a binary tree representation of the output layer with the W words as\nits leaves and, for each node, explicitly represents the relative probabilities of its child nodes. These\nde\ufb01ne a random walk that assigns probabilities to words.\nMore precisely, each word w can be reached by an appropriate path from the root of the tree. Let\nn(w, j) be the j-th node on the path from the root to w, and let L(w) be the length of this path, so\nn(w, 1) = root and n(w, L(w)) = w. In addition, for any inner node n, let ch(n) be an arbitrary\n\ufb01xed child of n and let [[x]] be 1 if x is true and -1 otherwise. Then the hierarchical softmax de\ufb01nes\np(wO|wI) as follows:\np(w|wI) =\nL(w)\u22121\nY\nj=1\n\u03c3\n\u0010\n[[n(w, j + 1) = ch(n(w, j))]] \u00b7 v\u2032\nn(w,j)\n\u22a4vwI\n\u0011\n(3)\nwhere \u03c3(x) = 1/(1 + exp(\u2212x)). It can be veri\ufb01ed that PW\nw=1 p(w|wI) = 1. This implies that the\ncost of computing log p(wO|wI) and \u2207log p(wO|wI) is proportional to L(wO), which on average\nis no greater than log W. Also, unlike the standard softmax formulation of the Skip-gram which\nassigns two representations vw and v\u2032\nw to each word w, the hierarchical softmax formulation has\none representation vw for each word w and one representation v\u2032\nn for every inner node n of the\nbinary tree.\nThe structure of the tree used by the hierarchical softmax has a considerable effect on the perfor-\nmance. Mnih and Hinton explored a number of methods for constructing the tree structure and the\neffect on both the training time and the resulting model accuracy [10]. In our work we use a binary\nHuffman tree, as it assigns short codes to the frequent words which results in fast training. It has\nbeen observed before that grouping words together by their frequency works well as a very simple\nspeedup technique for the neural network based language models [5, 8].\n2.2\nNegative Sampling\nAn alternative to the hierarchical softmax is Noise Contrastive Estimation (NCE), which was in-\ntroduced by Gutmann and Hyvarinen [4] and applied to language modeling by Mnih and Teh [11].\nNCE posits that a good model should be able to differentiate data from noise by means of logistic\nregression. This is similar to hinge loss used by Collobert and Weston [2] who trained the models\nby ranking the data above noise.\nWhile NCE can be shown to approximately maximize the log probability of the softmax, the Skip-\ngram model is only concerned with learning high-quality vector representations, so we are free to\nsimplify NCE as long as the vector representations retain their quality. We de\ufb01ne Negative sampling\n(NEG) by the objective\nlog \u03c3(v\u2032\nwO\n\u22a4vwI) +\nk\nX\ni=1\nEwi\u223cPn(w)\nh\nlog \u03c3(\u2212v\u2032\nwi\n\u22a4vwI)\ni\n(4)\n3\n-2\n-1.5\n-1\n-0.5\n 0\n 0.5\n 1\n 1.5\n 2\n-2\n-1.5\n-1\n-0.5\n 0\n 0.5\n 1\n 1.5\n 2\nCountry and Capital Vectors Projected by PCA\nChina\nJapan\nFrance\nRussia\nGermany\nItaly\nSpain\nGreece\nTurkey\nBeijing\nParis\nTokyo\nPoland\nMoscow\nPortugal\nBerlin\nRome\nAthens\nMadrid\nAnkara\nWarsaw\nLisbon\nFigure 2: Two-dimensional PCA projection of the 1000-dimensional Skip-gram vectors of countries and their\ncapital cities. The \ufb01gure illustrates ability of the model to automatically organize concepts and learn implicitly\nthe relationships between them, as during the training we did not provide any supervised information about\nwhat a capital city means.\nwhich is used to replace every log P(wO|wI) term in the Skip-gram objective. Thus the task is to\ndistinguish the target word wO from draws from the noise distribution Pn(w) using logistic regres-\nsion, where there are k negative samples for each data sample. Our experiments indicate that values\nof k in the range 5\u201320 are useful for small training datasets, while for large datasets the k can be as\nsmall as 2\u20135. The main difference between the Negative sampling and NCE is that NCE needs both\nsamples and the numerical probabilities of the noise distribution, while Negative sampling uses only\nsamples. And while NCE approximately maximizes the log probability of the softmax, this property\nis not important for our application.\nBoth NCE and NEG have the noise distribution Pn(w) as a free parameter. We investigated a number\nof choices for Pn(w) and found that the unigram distribution U(w) raised to the 3/4rd power (i.e.,\nU(w)3/4/Z) outperformed signi\ufb01cantly the unigram and the uniform distributions, for both NCE\nand NEG on every task we tried including language modeling (not reported here).\n2.3\nSubsampling of Frequent Words\nIn very large corpora, the most frequent words can easily occur hundreds of millions of times (e.g.,\n\u201cin\u201d, \u201cthe\u201d, and \u201ca\u201d). Such words usually provide less information value than the rare words. For\nexample, while the Skip-gram model bene\ufb01ts from observing the co-occurrences of \u201cFrance\u201d and\n\u201cParis\u201d, it bene\ufb01ts much less from observing the frequent co-occurrences of \u201cFrance\u201d and \u201cthe\u201d, as\nnearly every word co-occurs frequently within a sentence with \u201cthe\u201d. This idea can also be applied\nin the opposite direction; the vector representations of frequent words do not change signi\ufb01cantly\nafter training on several million examples.\nTo counter the imbalance between the rare and frequent words, we used a simple subsampling ap-\nproach: each word wi in the training set is discarded with probability computed by the formula\nP(wi) = 1 \u2212\ns\nt\nf(wi)\n(5)\n4\nMethod\nTime [min]\nSyntactic [%]\nSemantic [%]\nTotal accuracy [%]\nNEG-5\n38\n63\n54\n59\nNEG-15\n97\n63\n58\n61\nHS-Huffman\n41\n53\n40\n47\nNCE-5\n38\n60\n45\n53\nThe following results use 10\u22125 subsampling\nNEG-5\n14\n61\n58\n60\nNEG-15\n36\n61\n61\n61\nHS-Huffman\n21\n52\n59\n55\nTable 1: Accuracy of various Skip-gram 300-dimensional models on the analogical reasoning task\nas de\ufb01ned in [8]. NEG-k stands for Negative Sampling with k negative samples for each positive\nsample; NCE stands for Noise Contrastive Estimation and HS-Huffman stands for the Hierarchical\nSoftmax with the frequency-based Huffman codes.\nwhere f(wi) is the frequency of word wi and t is a chosen threshold, typically around 10\u22125.\nWe chose this subsampling formula because it aggressively subsamples words whose frequency\nis greater than t while preserving the ranking of the frequencies. Although this subsampling for-\nmula was chosen heuristically, we found it to work well in practice. It accelerates learning and even\nsigni\ufb01cantly improves the accuracy of the learned vectors of the rare words, as will be shown in the\nfollowing sections.\n3\nEmpirical Results\nIn this section we evaluate the Hierarchical Softmax (HS), Noise Contrastive Estimation, Negative\nSampling, and subsampling of the training words. We used the analogical reasoning task1 introduced\nby Mikolov et al. [8]. The task consists of analogies such as \u201cGermany\u201d : \u201cBerlin\u201d :: \u201cFrance\u201d : ?,\nwhich are solved by \ufb01nding a vector x such that vec(x) is closest to vec(\u201cBerlin\u201d) - vec(\u201cGermany\u201d)\n+ vec(\u201cFrance\u201d) according to the cosine distance (we discard the input words from the search). This\nspeci\ufb01c example is considered to have been answered correctly if x is \u201cParis\u201d. The task has two\nbroad categories: the syntactic analogies (such as \u201cquick\u201d : \u201cquickly\u201d :: \u201cslow\u201d : \u201cslowly\u201d) and the\nsemantic analogies, such as the country to capital city relationship.\nFor training the Skip-gram models, we have used a large dataset consisting of various news articles\n(an internal Google dataset with one billion words). We discarded from the vocabulary all words\nthat occurred less than 5 times in the training data, which resulted in a vocabulary of size 692K.\nThe performance of various Skip-gram models on the word analogy test set is reported in Table 1.\nThe table shows that Negative Sampling outperforms the Hierarchical Softmax on the analogical\nreasoning task, and has even slightly better performance than the Noise Contrastive Estimation. The\nsubsampling of the frequent words improves the training speed several times and makes the word\nrepresentations signi\ufb01cantly more accurate.\nIt can be argued that the linearity of the skip-gram model makes its vectors more suitable for such\nlinear analogical reasoning, but the results of Mikolov et al. [8] also show that the vectors learned\nby the standard sigmoidal recurrent neural networks (which are highly non-linear) improve on this\ntask signi\ufb01cantly as the amount of the training data increases, suggesting that non-linear models also\nhave a preference for a linear structure of the word representations.\n4\nLearning Phrases\nAs discussed earlier, many phrases have a meaning that is not a simple composition of the mean-\nings of its individual words. To learn vector representation for phrases, we \ufb01rst \ufb01nd words that\nappear frequently together, and infrequently in other contexts. For example, \u201cNew York Times\u201d and\n\u201cToronto Maple Leafs\u201d are replaced by unique tokens in the training data, while a bigram \u201cthis is\u201d\nwill remain unchanged.\n1code.google.com/p/word2vec/source/browse/trunk/questions-words.txt\n5\nNewspapers\nNew York\nNew York Times\nBaltimore\nBaltimore Sun\nSan Jose\nSan Jose Mercury News\nCincinnati\nCincinnati Enquirer\nNHL Teams\nBoston\nBoston Bruins\nMontreal\nMontreal Canadiens\nPhoenix\nPhoenix Coyotes\nNashville\nNashville Predators\nNBA Teams\nDetroit\nDetroit Pistons\nToronto\nToronto Raptors\nOakland\nGolden State Warriors\nMemphis\nMemphis Grizzlies\nAirlines\nAustria\nAustrian Airlines\nSpain\nSpainair\nBelgium\nBrussels Airlines\nGreece\nAegean Airlines\nCompany executives\nSteve Ballmer\nMicrosoft\nLarry Page\nGoogle\nSamuel J. Palmisano\nIBM\nWerner Vogels\nAmazon\nTable 2: Examples of the analogical reasoning task for phrases (the full test set has 3218 examples).\nThe goal is to compute the fourth phrase using the \ufb01rst three. Our best model achieved an accuracy\nof 72% on this dataset.\nThis way, we can form many reasonable phrases without greatly increasing the size of the vocabu-\nlary; in theory, we can train the Skip-gram model using all n-grams, but that would be too memory\nintensive. Many techniques have been previously developed to identify phrases in the text; however,\nit is out of scope of our work to compare them. We decided to use a simple data-driven approach,\nwhere phrases are formed based on the unigram and bigram counts, using\nscore(wi, wj) =\ncount(wiwj) \u2212\u03b4\ncount(wi) \u00d7 count(wj).\n(6)\nThe \u03b4 is used as a discounting coef\ufb01cient and prevents too many phrases consisting of very infre-\nquent words to be formed. The bigrams with score above the chosen threshold are then used as\nphrases. Typically, we run 2-4 passes over the training data with decreasing threshold value, allow-\ning longer phrases that consists of several words to be formed. We evaluate the quality of the phrase\nrepresentations using a new analogical reasoning task that involves phrases. Table 2 shows examples\nof the \ufb01ve categories of analogies used in this task. This dataset is publicly available on the web2.\n4.1\nPhrase Skip-Gram Results\nStarting with the same news data as in the previous experiments, we \ufb01rst constructed the phrase\nbased training corpus and then we trained several Skip-gram models using different hyper-\nparameters. As before, we used vector dimensionality 300 and context size 5. This setting already\nachieves good performance on the phrase dataset, and allowed us to quickly compare the Negative\nSampling and the Hierarchical Softmax, both with and without subsampling of the frequent tokens.\nThe results are summarized in Table 3.\nThe results show that while Negative Sampling achieves a respectable accuracy even with k = 5,\nusing k = 15 achieves considerably better performance. Surprisingly, while we found the Hierar-\nchical Softmax to achieve lower performance when trained without subsampling, it became the best\nperforming method when we downsampled the frequent words. This shows that the subsampling\ncan result in faster training and can also improve accuracy, at least in some cases.\n2code.google.com/p/word2vec/source/browse/trunk/questions-phrases.txt\nMethod\nDimensionality\nNo subsampling [%]\n10\u22125 subsampling [%]\nNEG-5\n300\n24\n27\nNEG-15\n300\n27\n42\nHS-Huffman\n300\n19\n47\nTable 3:\nAccuracies of the Skip-gram models on the phrase analogy dataset. The models were\ntrained on approximately one billion words from the news dataset.\n6\nNEG-15 with 10\u22125 subsampling\nHS with 10\u22125 subsampling\nVasco de Gama\nLingsugur\nItalian explorer\nLake Baikal\nGreat Rift Valley\nAral Sea\nAlan Bean\nRebbeca Naomi\nmoonwalker\nIonian Sea\nRuegen\nIonian Islands\nchess master\nchess grandmaster\nGarry Kasparov\nTable 4: Examples of the closest entities to the given short phrases, using two different models.\nCzech + currency\nVietnam + capital\nGerman + airlines\nRussian + river\nFrench + actress\nkoruna\nHanoi\nairline Lufthansa\nMoscow\nJuliette Binoche\nCheck crown\nHo Chi Minh City\ncarrier Lufthansa\nVolga River\nVanessa Paradis\nPolish zolty\nViet Nam\n\ufb02ag carrier Lufthansa\nupriver\nCharlotte Gainsbourg\nCTK\nVietnamese\nLufthansa\nRussia\nCecile De\nTable 5: Vector compositionality using element-wise addition. Four closest tokens to the sum of two\nvectors are shown, using the best Skip-gram model.\nTo maximize the accuracy on the phrase analogy task, we increased the amount of the training data\nby using a dataset with about 33 billion words. We used the hierarchical softmax, dimensionality\nof 1000, and the entire sentence for the context. This resulted in a model that reached an accuracy\nof 72%. We achieved lower accuracy 66% when we reduced the size of the training dataset to 6B\nwords, which suggests that the large amount of the training data is crucial.\nTo gain further insight into how different the representations learned by different models are, we did\ninspect manually the nearest neighbours of infrequent phrases using various models. In Table 4, we\nshow a sample of such comparison. Consistently with the previous results, it seems that the best\nrepresentations of phrases are learned by a model with the hierarchical softmax and subsampling.\n5\nAdditive Compositionality\nWe demonstrated that the word and phrase representations learned by the Skip-gram model exhibit\na linear structure that makes it possible to perform precise analogical reasoning using simple vector\narithmetics. Interestingly, we found that the Skip-gram representations exhibit another kind of linear\nstructure that makes it possible to meaningfully combine words by an element-wise addition of their\nvector representations. This phenomenon is illustrated in Table 5.\nThe additive property of the vectors can be explained by inspecting the training objective. The word\nvectors are in a linear relationship with the inputs to the softmax nonlinearity. As the word vectors\nare trained to predict the surrounding words in the sentence, the vectors can be seen as representing\nthe distribution of the context in which a word appears. These values are related logarithmically\nto the probabilities computed by the output layer, so the sum of two word vectors is related to the\nproduct of the two context distributions. The product works here as the AND function: words that\nare assigned high probabilities by both word vectors will have high probability, and the other words\nwill have low probability. Thus, if \u201cVolga River\u201d appears frequently in the same sentence together\nwith the words \u201cRussian\u201d and \u201criver\u201d, the sum of these two word vectors will result in such a feature\nvector that is close to the vector of \u201cVolga River\u201d.\n6\nComparison to Published Word Representations\nMany authors who previously worked on the neural network based representations of words have\npublished their resulting models for further use and comparison: amongst the most well known au-\nthors are Collobert and Weston [2], Turian et al. [17], and Mnih and Hinton [10]. We downloaded\ntheir word vectors from the web3. Mikolov et al. [8] have already evaluated these word representa-\ntions on the word analogy task, where the Skip-gram models achieved the best performance with a\nhuge margin.\n3http://metaoptimize.com/projects/wordreprs/\n7\nModel\nRedmond\nHavel\nninjutsu\ngraf\ufb01ti\ncapitulate\n(training time)\nCollobert (50d)\nconyers\nplauen\nreiki\ncheesecake\nabdicate\n(2 months)\nlubbock\ndzerzhinsky\nkohona\ngossip\naccede\nkeene\nosterreich\nkarate\ndioramas\nrearm\nTurian (200d)\nMcCarthy\nJewell\n-\ngun\ufb01re\n-\n(few weeks)\nAlston\nArzu\n-\nemotion\n-\nCousins\nOvitz\n-\nimpunity\n-\nMnih (100d)\nPodhurst\nPontiff\n-\nanaesthetics\nMavericks\n(7 days)\nHarlang\nPinochet\n-\nmonkeys\nplanning\nAgarwal\nRodionov\n-\nJews\nhesitated\nSkip-Phrase\nRedmond Wash.\nVaclav Havel\nninja\nspray paint\ncapitulation\n(1000d, 1 day)\nRedmond Washington\npresident Vaclav Havel\nmartial arts\ngra\ufb01tti\ncapitulated\nMicrosoft\nVelvet Revolution\nswordsmanship\ntaggers\ncapitulating\nTable 6: Examples of the closest tokens given various well known models and the Skip-gram model\ntrained on phrases using over 30 billion training words. An empty cell means that the word was not\nin the vocabulary.\nTo give more insight into the difference of the quality of the learned vectors, we provide empirical\ncomparison by showing the nearest neighbours of infrequent words in Table 6. These examples show\nthat the big Skip-gram model trained on a large corpus visibly outperforms all the other models in\nthe quality of the learned representations. This can be attributed in part to the fact that this model\nhas been trained on about 30 billion words, which is about two to three orders of magnitude more\ndata than the typical size used in the prior work. Interestingly, although the training set is much\nlarger, the training time of the Skip-gram model is just a fraction of the time complexity required by\nthe previous model architectures.\n7\nConclusion\nThis work has several key contributions. We show how to train distributed representations of words\nand phrases with the Skip-gram model and demonstrate that these representations exhibit linear\nstructure that makes precise analogical reasoning possible. The techniques introduced in this paper\ncan be used also for training the continuous bag-of-words model introduced in [8].\nWe successfully trained models on several orders of magnitude more data than the previously pub-\nlished models, thanks to the computationally ef\ufb01cient model architecture. This results in a great\nimprovement in the quality of the learned word and phrase representations, especially for the rare\nentities. We also found that the subsampling of the frequent words results in both faster training\nand signi\ufb01cantly better representations of uncommon words. Another contribution of our paper is\nthe Negative sampling algorithm, which is an extremely simple training method that learns accurate\nrepresentations especially for frequent words.\nThe choice of the training algorithm and the hyper-parameter selection is a task speci\ufb01c decision,\nas we found that different problems have different optimal hyperparameter con\ufb01gurations. In our\nexperiments, the most crucial decisions that affect the performance are the choice of the model\narchitecture, the size of the vectors, the subsampling rate, and the size of the training window.\nA very interesting result of this work is that the word vectors can be somewhat meaningfully com-\nbined using just simple vector addition. Another approach for learning representations of phrases\npresented in this paper is to simply represent the phrases with a single token. Combination of these\ntwo approaches gives a powerful yet simple way how to represent longer pieces of text, while hav-\ning minimal computational complexity. Our work can thus be seen as complementary to the existing\napproach that attempts to represent phrases using recursive matrix-vector operations [16].\nWe made the code for training the word and phrase vectors based on the techniques described in this\npaper available as an open-source project4.\n4code.google.com/p/word2vec\n8\nReferences\n[1] Yoshua Bengio, R\u00b4ejean Ducharme, Pascal Vincent, and Christian Janvin. A neural probabilistic language\nmodel. The Journal of Machine Learning Research, 3:1137\u20131155, 2003.\n[2] Ronan Collobert and Jason Weston. A uni\ufb01ed architecture for natural language processing: deep neu-\nral networks with multitask learning. In Proceedings of the 25th international conference on Machine\nlearning, pages 160\u2013167. ACM, 2008.\n[3] Xavier Glorot, Antoine Bordes, and Yoshua Bengio. Domain adaptation for large-scale sentiment classi-\n\ufb01cation: A deep learning approach. In ICML, 513\u2013520, 2011.\n[4] Michael U Gutmann and Aapo Hyv\u00a8arinen. Noise-contrastive estimation of unnormalized statistical mod-\nels, with applications to natural image statistics. The Journal of Machine Learning Research, 13:307\u2013361,\n2012.\n[5] Tomas Mikolov, Stefan Kombrink, Lukas Burget, Jan Cernocky, and Sanjeev Khudanpur. Extensions of\nrecurrent neural network language model. In Acoustics, Speech and Signal Processing (ICASSP), 2011\nIEEE International Conference on, pages 5528\u20135531. IEEE, 2011.\n[6] Tomas Mikolov, Anoop Deoras, Daniel Povey, Lukas Burget and Jan Cernocky. Strategies for Training\nLarge Scale Neural Network Language Models. In Proc. Automatic Speech Recognition and Understand-\ning, 2011.\n[7] Tomas Mikolov. Statistical Language Models Based on Neural Networks. PhD thesis, PhD Thesis, Brno\nUniversity of Technology, 2012.\n[8] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Ef\ufb01cient estimation of word representations\nin vector space. ICLR Workshop, 2013.\n[9] Tomas Mikolov, Wen-tau Yih and Geoffrey Zweig. Linguistic Regularities in Continuous Space Word\nRepresentations. In Proceedings of NAACL HLT, 2013.\n[10] Andriy Mnih and Geoffrey E Hinton. A scalable hierarchical distributed language model. Advances in\nneural information processing systems, 21:1081\u20131088, 2009.\n[11] Andriy Mnih and Yee Whye Teh. A fast and simple algorithm for training neural probabilistic language\nmodels. arXiv preprint arXiv:1206.6426, 2012.\n[12] Frederic Morin and Yoshua Bengio. Hierarchical probabilistic neural network language model. In Pro-\nceedings of the international workshop on arti\ufb01cial intelligence and statistics, pages 246\u2013252, 2005.\n[13] David E Rumelhart, Geoffrey E Hintont, and Ronald J Williams. Learning representations by back-\npropagating errors. Nature, 323(6088):533\u2013536, 1986.\n[14] Holger Schwenk. Continuous space language models. Computer Speech and Language, vol. 21, 2007.\n[15] Richard Socher, Cliff C. Lin, Andrew Y. Ng, and Christopher D. Manning. Parsing natural scenes and\nnatural language with recursive neural networks. In Proceedings of the 26th International Conference on\nMachine Learning (ICML), volume 2, 2011.\n[16] Richard Socher, Brody Huval, Christopher D. Manning, and Andrew Y. Ng. Semantic Compositionality\nThrough Recursive Matrix-Vector Spaces. In Proceedings of the 2012 Conference on Empirical Methods\nin Natural Language Processing (EMNLP), 2012.\n[17] Joseph Turian, Lev Ratinov, and Yoshua Bengio. Word representations: a simple and general method for\nsemi-supervised learning. In Proceedings of the 48th Annual Meeting of the Association for Computa-\ntional Linguistics, pages 384\u2013394. Association for Computational Linguistics, 2010.\n[18] Peter D. Turney and Patrick Pantel. From frequency to meaning: Vector space models of semantics. In\nJournal of Arti\ufb01cial Intelligence Research, 37:141-188, 2010.\n[19] Peter D. Turney. Distributional semantics beyond words: Supervised learning of analogy and paraphrase.\nIn Transactions of the Association for Computational Linguistics (TACL), 353\u2013366, 2013.\n[20] Jason Weston, Samy Bengio, and Nicolas Usunier. Wsabie: Scaling up to large vocabulary image annota-\ntion. In Proceedings of the Twenty-Second international joint conference on Arti\ufb01cial Intelligence-Volume\nVolume Three, pages 2764\u20132770. AAAI Press, 2011.\n9\n",
        "sentence": "",
        "context": "Volume Three, pages 2764\u20132770. AAAI Press, 2011.\n9\n-\nCousins\nOvitz\n-\nimpunity\n-\nMnih (100d)\nPodhurst\nPontiff\n-\nanaesthetics\nMavericks\n(7 days)\nHarlang\nPinochet\n-\nmonkeys\nplanning\nAgarwal\nRodionov\n-\nJews\nhesitated\nSkip-Phrase\nRedmond Wash.\nVaclav Havel\nninja\nspray paint\ncapitulation\n(1000d, 1 day)\nCollobert (50d)\nconyers\nplauen\nreiki\ncheesecake\nabdicate\n(2 months)\nlubbock\ndzerzhinsky\nkohona\ngossip\naccede\nkeene\nosterreich\nkarate\ndioramas\nrearm\nTurian (200d)\nMcCarthy\nJewell\n-\ngun\ufb01re\n-\n(few weeks)\nAlston\nArzu\n-\nemotion\n-\nCousins\nOvitz\n-\nimpunity\n-"
    },
    {
        "title": "Dependency tree-based sentiment classification using CRFs with hidden variables",
        "author": [
            "Kentaro Inui",
            "Sadao Kurohashi"
        ],
        "venue": "In Human Language Technologies: The 2010 Annual Conference of the North American Chapter",
        "citeRegEx": "Nakagawa et al\\.,? \\Q2010\\E",
        "shortCiteRegEx": "Nakagawa et al\\.",
        "year": 2010,
        "abstract": "",
        "full_text": "",
        "sentence": " , 2011), combining SVMs with naive Bayes (Wang and Manning, 2012), and building dependency trees with Conditional Random Fields (Nakagawa et al., 2010).",
        "context": null
    },
    {
        "title": "GloVe: global vectors for word representation",
        "author": [
            "Richard Socher",
            "Christopher D Manning"
        ],
        "venue": "Proceedings of the Empiricial Methods in Natural Language Processing",
        "citeRegEx": "Pennington et al\\.,? \\Q2014\\E",
        "shortCiteRegEx": "Pennington et al\\.",
        "year": 2014,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " word vectors pretrained with GloVe on Twitter (Pennington et al., 2014) for MRDA and SwDA, as these choices yielded the best results among all publicly available word2vec, GloVe, SENNA (Collobert, 2011; Collobert et al.",
        "context": null
    },
    {
        "title": "Dialogue act classification using language models",
        "author": [
            "Reithinger",
            "Klesen1997] Norbert Reithinger",
            "Martin Klesen"
        ],
        "venue": "In EuroSpeech. Citeseer",
        "citeRegEx": "Reithinger et al\\.,? \\Q1997\\E",
        "shortCiteRegEx": "Reithinger et al\\.",
        "year": 1997,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "Dialog act tagging using memory-based learning",
        "author": [
            "Mihai Rotaru"
        ],
        "venue": "Term project, University of Pittsburgh,",
        "citeRegEx": "Rotaru.,? \\Q2002\\E",
        "shortCiteRegEx": "Rotaru.",
        "year": 2002,
        "abstract": "",
        "full_text": "",
        "sentence": " Memory-based Learning: (Rotaru, 2002).",
        "context": null
    },
    {
        "title": "Bidirectional recurrent neural networks",
        "author": [
            "Schuster",
            "Paliwal1997] Mike Schuster",
            "Kuldip K Paliwal"
        ],
        "venue": "Signal Processing, IEEE Transactions",
        "citeRegEx": "Schuster et al\\.,? \\Q1997\\E",
        "shortCiteRegEx": "Schuster et al\\.",
        "year": 1997,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "The ICSI meeting recorder dialog act (MRDA",
        "author": [
            "Raj Dhillon",
            "Sonali Bhagat",
            "Jeremy Ang",
            "Hannah Carvey"
        ],
        "venue": null,
        "citeRegEx": "Shriberg et al\\.,? \\Q2004\\E",
        "shortCiteRegEx": "Shriberg et al\\.",
        "year": 2004,
        "abstract": "",
        "full_text": "",
        "sentence": " \u2022 MRDA: ICSI Meeting Recorder Dialog Act Corpus (Janin et al., 2003; Shriberg et al., 2004).",
        "context": null
    },
    {
        "title": "From symbolic to sub-symbolic information in question classification",
        "author": [
            "Silva et al.2011] Joao Silva",
            "Lu\u0131\u0301sa Coheur",
            "Ana Cristina Mendes",
            "Andreas Wichert"
        ],
        "venue": "Artificial Intelligence Review,",
        "citeRegEx": "Silva et al\\.,? \\Q2011\\E",
        "shortCiteRegEx": "Silva et al\\.",
        "year": 2011,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " Many different approaches have been developed for short-text classification, such as using Support Vector Machines (SVMs) with rule-based features (Silva et al., 2011), combining SVMs with naive Bayes (Wang and Manning, 2012), and building dependency trees with Conditional Random Fields (Nakagawa et al.",
        "context": null
    },
    {
        "title": "Semantic compositionality through recursive matrixvector spaces",
        "author": [
            "Brody Huval",
            "Christopher D Manning",
            "Andrew Y Ng"
        ],
        "venue": "In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural",
        "citeRegEx": "Socher et al\\.,? \\Q2012\\E",
        "shortCiteRegEx": "Socher et al\\.",
        "year": 2012,
        "abstract": "",
        "full_text": "",
        "sentence": " , 2014) and recursive neural networks (Socher et al., 2012).",
        "context": null
    },
    {
        "title": "Dialogue act modeling for automatic tagging and recognition",
        "author": [
            "Klaus Ries",
            "Noah Coccaro",
            "Elizabeth Shriberg",
            "Rebecca Bates",
            "Daniel Jurafsky",
            "Paul Taylor",
            "Rachel Martin",
            "Carol Van EssDykema",
            "Marie Meteer"
        ],
        "venue": null,
        "citeRegEx": "Stolcke et al\\.,? \\Q2000\\E",
        "shortCiteRegEx": "Stolcke et al\\.",
        "year": 2000,
        "abstract": "",
        "full_text": "",
        "sentence": " Previous works on sequential short-text classification are mostly based on non-ANN approaches, such as Hidden Markov Models (HMMs) (Reithinger and Klesen, 1997), (Stolcke et al., 2000), maximum entropy (Ang et al. Its accurate detection is useful for a range of applications, from speech recognition to automatic summarization (Stolcke et al., 2000). HMM: (Stolcke et al., 2000).",
        "context": null
    },
    {
        "title": "Baselines and bigrams: Simple, good sentiment and topic classification",
        "author": [
            "Wang",
            "Manning2012] Sida Wang",
            "Christopher D Manning"
        ],
        "venue": "In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics:",
        "citeRegEx": "Wang et al\\.,? \\Q2012\\E",
        "shortCiteRegEx": "Wang et al\\.",
        "year": 2012,
        "abstract": "",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "Adadelta: An adaptive learning rate method",
        "author": [
            "Matthew D Zeiler"
        ],
        "venue": "arXiv preprint arXiv:1212.5701",
        "citeRegEx": "Zeiler.,? \\Q2012\\E",
        "shortCiteRegEx": "Zeiler.",
        "year": 2012,
        "abstract": "We present a novel per-dimension learning rate method for gradient descent\ncalled ADADELTA. The method dynamically adapts over time using only first order\ninformation and has minimal computational overhead beyond vanilla stochastic\ngradient descent. The method requires no manual tuning of a learning rate and\nappears robust to noisy gradient information, different model architecture\nchoices, various data modalities and selection of hyperparameters. We show\npromising results compared to other methods on the MNIST digit classification\ntask using a single machine and on a large scale voice dataset in a distributed\ncluster environment.",
        "full_text": "ADADELTA: AN ADAPTIVE LEARNING RATE METHOD\nMatthew D. Zeiler1,2\u2217\n1Google Inc., USA\n2New York University, USA\nABSTRACT\nWe present a novel per-dimension learning rate method for\ngradient descent called ADADELTA. The method dynami-\ncally adapts over time using only \ufb01rst order information and\nhas minimal computational overhead beyond vanilla stochas-\ntic gradient descent. The method requires no manual tuning of\na learning rate and appears robust to noisy gradient informa-\ntion, different model architecture choices, various data modal-\nities and selection of hyperparameters. We show promising\nresults compared to other methods on the MNIST digit clas-\nsi\ufb01cation task using a single machine and on a large scale\nvoice dataset in a distributed cluster environment.\nIndex Terms\u2014 Adaptive Learning Rates, Machine Learn-\ning, Neural Networks, Gradient Descent\n1. INTRODUCTION\nThe aim of many machine learning methods is to update a\nset of parameters x in order to optimize an objective function\nf(x). This often involves some iterative procedure which ap-\nplies changes to the parameters, \u2206x at each iteration of the\nalgorithm. Denoting the parameters at the t-th iteration as xt,\nthis simple update rule becomes:\nxt+1 = xt + \u2206xt\n(1)\nIn this paper we consider gradient descent algorithms which\nattempt to optimize the objective function by following the\nsteepest descent direction given by the negative of the gradi-\nent gt. This general approach can be applied to update any\nparameters for which a derivative can be obtained:\n\u2206xt = \u2212\u03b7gt\n(2)\nwhere gt is the gradient of the parameters at the t-th iteration\n\u2202f(xt)\n\u2202xt\nand \u03b7 is a learning rate which controls how large of\na step to take in the direction of the negative gradient. Fol-\nlowing this negative gradient for each new sample or batch\nof samples chosen from the dataset gives a local estimate\nof which direction minimizes the cost and is referred to as\nstochastic gradient descent (SGD) [1]. While often simple to\nderive the gradients for each parameter analytically, the gradi-\nent descent algorithm requires the learning rate hyperparam-\neter to be chosen.\n\u2217This work was done while Matthew D. Zeiler was an intern at Google.\nSetting the learning rate typically involves a tuning pro-\ncedure in which the highest possible learning rate is chosen\nby hand. Choosing higher than this rate can cause the system\nto diverge in terms of the objective function, and choosing\nthis rate too low results in slow learning. Determining a good\nlearning rate becomes more of an art than science for many\nproblems.\nThis work attempts to alleviate the task of choosing a\nlearning rate by introducing a new dynamic learning rate that\nis computed on a per-dimension basis using only \ufb01rst order\ninformation. This requires a trivial amount of extra compu-\ntation per iteration over gradient descent. Additionally, while\nthere are some hyper parameters in this method, we has found\ntheir selection to not drastically alter the results. The bene\ufb01ts\nof this approach are as follows:\n\u2022 no manual setting of a learning rate.\n\u2022 insensitive to hyperparameters.\n\u2022 separate dynamic learning rate per-dimension.\n\u2022 minimal computation over gradient descent.\n\u2022 robust to large gradients, noise and architecture choice.\n\u2022 applicable in both local or distributed environments.\n2. RELATED WORK\nThere are many modi\ufb01cations to the gradient descent algo-\nrithm.\nThe most powerful such modi\ufb01cation is Newton\u2019s\nmethod which requires second order derivatives of the cost\nfunction:\n\u2206xt = H\u22121\nt\ngt\n(3)\nwhere H\u22121\nt\nis the inverse of the Hessian matrix of second\nderivatives computed at iteration t. This determines the op-\ntimal step size to take for quadratic problems, but unfortu-\nnately is prohibitive to compute in practice for large models.\nTherefore, many additional approaches have been proposed\nto either improve the use of \ufb01rst order information or to ap-\nproximate the second order information.\n2.1. Learning Rate Annealing\nThere have been several attempts to use heuristics for estimat-\ning a good learning rate at each iteration of gradient descent.\nThese either attempt to speed up learning when suitable or to\nslow down learning near a local minima. Here we consider\nthe latter.\narXiv:1212.5701v1  [cs.LG]  22 Dec 2012\nWhen gradient descent nears a minima in the cost sur-\nface, the parameter values can oscillate back and forth around\nthe minima. One method to prevent this is to slow down the\nparameter updates by decreasing the learning rate. This can\nbe done manually when the validation accuracy appears to\nplateau. Alternatively, learning rate schedules have been pro-\nposed [1] to automatically anneal the learning rate based on\nhow many epochs through the data have been done. These ap-\nproaches typically add additional hyperparameters to control\nhow quickly the learning rate decays.\n2.2. Per-Dimension First Order Methods\nThe heuristic annealing procedure discussed above modi\ufb01es\na single global learning rate that applies to all dimensions of\nthe parameters. Since each dimension of the parameter vector\ncan relate to the overall cost in completely different ways,\na per-dimension learning rate that can compensate for these\ndifferences is often advantageous.\n2.2.1. Momentum\nOne method of speeding up training per-dimension is the mo-\nmentum method [2]. This is perhaps the simplest extension to\nSGD that has been successfully used for decades. The main\nidea behind momentum is to accelerate progress along dimen-\nsions in which gradient consistently point in the same direc-\ntion and to slow progress along dimensions where the sign\nof the gradient continues to change. This is done by keeping\ntrack of past parameter updates with an exponential decay:\n\u2206xt = \u03c1\u2206xt\u22121 \u2212\u03b7gt\n(4)\nwhere \u03c1 is a constant controlling the decay of the previous\nparameter updates. This gives a nice intuitive improvement\nover SGD when optimizing dif\ufb01cult cost surfaces such as a\nlong narrow valley. The gradients along the valley, despite\nbeing much smaller than the gradients across the valley, are\ntypically in the same direction and thus the momentum term\naccumulates to speed up progress. In SGD the progress along\nthe valley would be slow since the gradient magnitude is small\nand the \ufb01xed global learning rate shared by all dimensions\ncannot speed up progress. Choosing a higher learning rate\nfor SGD may help but the dimension across the valley would\nthen also make larger parameter updates which could lead\nto oscillations back as forth across the valley. These oscil-\nlations are mitigated when using momentum because the sign\nof the gradient changes and thus the momentum term damps\ndown these updates to slow progress across the valley. Again,\nthis occurs per-dimension and therefore the progress along the\nvalley is unaffected.\n2.2.2. ADAGRAD\nA recent \ufb01rst order method called ADAGRAD [3] has shown\nremarkably good results on large scale learning tasks in a dis-\ntributed environment [4]. This method relies on only \ufb01rst\norder information but has some properties of second order\nmethods and annealing. The update rule for ADAGRAD is\nas follows:\n\u2206xt = \u2212\n\u03b7\nqPt\n\u03c4=1 g2\u03c4\ngt\n(5)\nHere the denominator computes the \u21132 norm of all previous\ngradients on a per-dimension basis and \u03b7 is a global learning\nrate shared by all dimensions.\nWhile there is the hand tuned global learning rate, each\ndimension has its own dynamic rate. Since this dynamic rate\ngrows with the inverse of the gradient magnitudes, large gra-\ndients have smaller learning rates and small gradients have\nlarge learning rates. This has the nice property, as in second\norder methods, that the progress along each dimension evens\nout over time. This is very bene\ufb01cial for training deep neu-\nral networks since the scale of the gradients in each layer is\noften different by several orders of magnitude, so the optimal\nlearning rate should take that into account. Additionally, this\naccumulation of gradient in the denominator has the same ef-\nfects as annealing, reducing the learning rate over time.\nSince the magnitudes of gradients are factored out in\nADAGRAD, this method can be sensitive to initial conditions\nof the parameters and the corresponding gradients. If the ini-\ntial gradients are large, the learning rates will be low for the\nremainder of training. This can be combatted by increasing\nthe global learning rate, making the ADAGRAD method sen-\nsitive to the choice of learning rate. Also, due to the continual\naccumulation of squared gradients in the denominator, the\nlearning rate will continue to decrease throughout training,\neventually decreasing to zero and stopping training com-\npletely. We created our ADADELTA method to overcome the\nsensitivity to the hyperparameter selection as well as to avoid\nthe continual decay of the learning rates.\n2.3. Methods Using Second Order Information\nWhereas the above methods only utilized gradient and func-\ntion evaluations in order to optimize the objective, second\norder methods such as Newton\u2019s method or quasi-Newtons\nmethods make use of the Hessian matrix or approximations\nto it. While this provides additional curvature information\nuseful for optimization, computing accurate second order in-\nformation is often expensive.\nSince computing the entire Hessian matrix of second\nderivatives is too computationally expensive for large models,\nBecker and LecCun [5] proposed a diagonal approximation to\nthe Hessian. This diagonal approximation can be computed\nwith one additional forward and back-propagation through\nthe model, effectively doubling the computation over SGD.\nOnce the diagonal of the Hessian is computed, diag(H), the\nupdate rule becomes:\n\u2206xt = \u2212\n1\n|diag(Ht)| + \u00b5 gt\n(6)\nwhere the absolute value of this diagonal Hessian is used to\nensure the negative gradient direction is always followed and\n\u00b5 is a small constant to improve the conditioning of the Hes-\nsian for regions of small curvature.\nA recent method by Schaul et al. [6] incorporating the\ndiagonal Hessian with ADAGRAD-like terms has been intro-\nduced to alleviate the need for hand speci\ufb01ed learning rates.\nThis method uses the following update rule:\n\u2206xt = \u2212\n1\n|diag(Ht)|\nE[gt\u2212w:t]2\nE[g2\nt\u2212w:t] gt\n(7)\nwhere E[gt\u2212w:t] is the expected value of the previous w gra-\ndients and E[g2\nt\u2212w:t] is the expected value of squared gradi-\nents over the same window w. Schaul et al. also introduce a\nheuristic for this window size w (see [6] for more details).\n3. ADADELTA METHOD\nThe idea presented in this paper was derived from ADA-\nGRAD [3] in order to improve upon the two main draw-\nbacks of the method: 1) the continual decay of learning rates\nthroughout training, and 2) the need for a manually selected\nglobal learning rate. After deriving our method we noticed\nseveral similarities to Schaul et al. [6], which will be com-\npared to below.\nIn the ADAGRAD method the denominator accumulates\nthe squared gradients from each iteration starting at the be-\nginning of training. Since each term is positive, this accumu-\nlated sum continues to grow throughout training, effectively\nshrinking the learning rate on each dimension. After many it-\nerations, this learning rate will become in\ufb01nitesimally small.\n3.1. Idea 1: Accumulate Over Window\nInstead of accumulating the sum of squared gradients over all\ntime, we restricted the window of past gradients that are ac-\ncumulated to be some \ufb01xed size w (instead of size t where\nt is the current iteration as in ADAGRAD). With this win-\ndowed accumulation the denominator of ADAGRAD cannot\naccumulate to in\ufb01nity and instead becomes a local estimate\nusing recent gradients. This ensures that learning continues\nto make progress even after many iterations of updates have\nbeen done.\nSince storing w previous squared gradients is inef\ufb01cient,\nour methods implements this accumulation as an exponen-\ntially decaying average of the squared gradients. Assume at\ntime t this running average is E[g2]t then we compute:\nE[g2]t = \u03c1 E[g2]t\u22121 + (1 \u2212\u03c1) g2\nt\n(8)\nwhere \u03c1 is a decay constant similar to that used in the momen-\ntum method. Since we require the square root of this quantity\nin the parameter updates, this effectively becomes the RMS\nof previous squared gradients up to time t:\nRMS[g]t =\np\nE[g2]t + \u03f5\n(9)\nwhere a constant \u03f5 is added to better condition the denomina-\ntor as in [5]. The resulting parameter update is then:\n\u2206xt = \u2212\n\u03b7\nRMS[g]t\ngt\n(10)\nAlgorithm 1 Computing ADADELTA update at time t\nRequire: Decay rate \u03c1, Constant \u03f5\nRequire: Initial parameter x1\n1: Initialize accumulation variables E[g2]0 = 0, E[\u2206x2]0 = 0\n2: for t = 1 : T do %% Loop over # of updates\n3:\nCompute Gradient: gt\n4:\nAccumulate Gradient: E[g2]t = \u03c1E[g2]t\u22121 + (1 \u2212\u03c1)g2\nt\n5:\nCompute Update: \u2206xt = \u2212\nRMS[\u2206x]t\u22121\nRMS[g]t\ngt\n6:\nAccumulate Updates: E[\u2206x2]t = \u03c1E[\u2206x2]t\u22121+(1\u2212\u03c1)\u2206x2\nt\n7:\nApply Update: xt+1 = xt + \u2206xt\n8: end for\n3.2. Idea 2: Correct Units with Hessian Approximation\nWhen considering the parameter updates, \u2206x, being applied\nto x, the units should match. That is, if the parameter had\nsome hypothetical units, the changes to the parameter should\nbe changes in those units as well. When considering SGD,\nMomentum, or ADAGRAD, we can see that this is not the\ncase. The units in SGD and Momentum relate to the gradient,\nnot the parameter:\nunits of \u2206x \u221dunits of g \u221d\u2202f\n\u2202x \u221d\n1\nunits of x\n(11)\nassuming the cost function, f, is unitless. ADAGRAD also\ndoes not have correct units since the update involves ratios of\ngradient quantities, hence the update is unitless.\nIn contrast, second order methods such as Newton\u2019s\nmethod that use Hessian information or an approximation\nto the Hessian do have the correct units for the parameter\nupdates:\n\u2206x \u221dH\u22121g \u221d\n\u2202f\n\u2202x\n\u22022f\n\u2202x2\n\u221dunits of x\n(12)\nNoticing this mismatch of units we considered terms to\nadd to Eqn. 10 in order for the units of the update to match\nthe units of the parameters. Since second order methods are\ncorrect, we rearrange Newton\u2019s method (assuming a diagonal\nHessian) for the inverse of the second derivative to determine\nthe quantities involved:\n\u2206x =\n\u2202f\n\u2202x\n\u22022f\n\u2202x2\n\u21d2\n1\n\u22022f\n\u2202x2\n= \u2206x\n\u2202f\n\u2202x\n(13)\nSince the RMS of the previous gradients is already repre-\nsented in the denominator in Eqn. 10 we considered a mea-\nsure of the \u2206x quantity in the numerator. \u2206xt for the current\ntime step is not known, so we assume the curvature is locally\nsmooth and approximate \u2206xt by compute the exponentially\ndecaying RMS over a window of size w of previous \u2206x to\ngive the ADADELTA method:\n\u2206xt = \u2212RMS[\u2206x]t\u22121\nRMS[g]t\ngt\n(14)\nwhere the same constant \u03f5 is added to the numerator RMS as\nwell. This constant serves the purpose both to start off the \ufb01rst\niteration where \u2206x0 = 0 and to ensure progress continues to\nbe made even if previous updates become small.\nThis derivation made the assumption of diagonal curva-\nture so that the second derivatives could easily be rearranged.\nFurthermore, this is an approximation to the diagonal Hessian\nusing only RMS measures of g and \u2206x. This approximation\nis always positive as in Becker and LeCun [5], ensuring the\nupdate direction follows the negative gradient at each step.\nIn Eqn. 14 the RMS[\u2206x]t\u22121 quantity lags behind the de-\nnominator by 1 time step, due to the recurrence relationship\nfor \u2206xt. An interesting side effect of this is that the system is\nrobust to large sudden gradients which act to increase the de-\nnominator, reducing the effective learning rate at the current\ntime step, before the numerator can react.\nThe method in Eqn. 14 uses only \ufb01rst order information\nand has some properties from each of the discussed meth-\nods. The negative gradient direction for the current iteration\n\u2212gt is always followed as in SGD. The numerator acts as\nan acceleration term, accumulating previous gradients over a\nwindow of time as in momentum. The denominator is re-\nlated to ADAGRAD in that the squared gradient information\nper-dimension helps to even out the progress made in each di-\nmension, but is computed over a window to ensure progress\nis made later in training. Finally, the method relates to Schaul\net al. \u2019s in that some approximation to the Hessian is made,\nbut instead costs only one gradient computation per iteration\nby leveraging information from past updates. For the com-\nplete algorithm details see Algorithm 1.\n4. EXPERIMENTS\nWe evaluate our method on two tasks using several different\nneural network architectures. We train the neural networks\nusing SGD, Momentum, ADAGRAD, and ADADELTA in a\nsupervised fashion to minimize the cross entropy objective\nbetween the network output and ground truth labels. Compar-\nisons are done both on a local computer and in a distributed\ncompute cluster.\n4.1. Handwritten Digit Classi\ufb01cation\nIn our \ufb01rst set of experiments we train a neural network on the\nMNIST handwritten digit classi\ufb01cation task. For comparison\nwith Schaul et al. \u2019s method we trained with tanh nonlinear-\nities and 500 hidden units in the \ufb01rst layer followed by 300\nhidden units in the second layer, with the \ufb01nal softmax out-\nput layer on top. Our method was trained on mini-batches of\n100 images per batch for 6 epochs through the training set.\nSetting the hyperparameters to \u03f5 = 1e \u22126 and \u03c1 = 0.95 we\nachieve 2.00% test set error compared to the 2.10% of Schaul\net al. While this is nowhere near convergence it gives a sense\nof how quickly the algorithms can optimize the classi\ufb01cation\nobjective.\n0\n10\n20\n30\n40\n50\n  1\n1.5\n  2\n2.5\n  3\n3.5\n  4\n4.5\n  5\n5.5\n  6\nEpoch\nTest Error %\n \n \nSGD\nMOMENTUM\nADAGRAD\nADADELTA\nFig. 1. Comparison of learning rate methods on MNIST digit\nclassi\ufb01cation for 50 epochs.\nTo further analyze various methods to convergence, we\ntrain the same neural network with 500 hidden units in the \ufb01rst\nlayer, 300 hidden units in the second layer and recti\ufb01ed linear\nactivation functions in both layers for 50 epochs. We notice\nthat recti\ufb01ed linear units work better in practice than tanh, and\ntheir non-saturating nature further tests each of the methods\nat coping with large variations of activations and gradients.\nIn Fig. 1 we compare SGD, Momentum, ADAGRAD,\nand ADADELTA in optimizing the test set errors. The unal-\ntered SGD method does the worst in this case, whereas adding\nthe momentum term to it signi\ufb01cantly improves performance.\nADAGRAD performs well for the \ufb01rst 10 epochs of training,\nafter which it slows down due to the accumulations in the de-\nnominator which continually increase. ADADELTA matches\nthe fast initial convergence of ADAGRAD while continuing\nto reduce the test error, converging near the best performance\nwhich occurs with momentum.\nSGD\nMOMENTUM\nADAGRAD\n\u03f5 = 1e0\n2.26%\n89.68%\n43.76%\n\u03f5 = 1e\u22121\n2.51%\n2.03%\n2.82%\n\u03f5 = 1e\u22122\n7.02%\n2.68%\n1.79%\n\u03f5 = 1e\u22123\n17.01%\n6.98%\n5.21%\n\u03f5 = 1e\u22124\n58.10%\n16.98%\n12.59%\nTable 1. MNIST test error rates after 6 epochs of training for\nvarious hyperparameter settings using SGD, MOMENTUM,\nand ADAGRAD.\n\u03c1 = 0.9\n\u03c1 = 0.95\n\u03c1 = 0.99\n\u03f5 = 1e\u22122\n2.59%\n2.58%\n2.32%\n\u03f5 = 1e\u22124\n2.05%\n1.99%\n2.28%\n\u03f5 = 1e\u22126\n1.90%\n1.83%\n2.05%\n\u03f5 = 1e\u22128\n2.29%\n2.13%\n2.00%\nTable 2. MNIST test error rate after 6 epochs for various\nhyperparameter settings using ADADELTA.\n4.2. Sensitivity to Hyperparameters\nWhile momentum converged to a better \ufb01nal solution than\nADADELTA after many epochs of training, it was very sen-\nsitive to the learning rate selection, as was SGD and ADA-\nGRAD. In Table 1 we vary the learning rates for each method\nand show the test set errors after 6 epochs of training using\nrecti\ufb01ed linear units as the activation function. The optimal\nsettings from each column were used to generate Fig. 1. With\nSGD, Momentum, or ADAGRAD the learning rate needs to\nbe set to the correct order of magnitude, above which the so-\nlutions typically diverge and below which the optimization\nproceeds slowly. We can see that these results are highly vari-\nable for each method, compared to ADADELTA in Table 2\nin which the two hyperparameters do not signi\ufb01cantly alter\nperformance.\n4.3. Effective Learning Rates\nTo investigate some of the properties of ADADELTA we plot\nin Fig. 2 the step sizes and parameter updates of 10 randomly\nselected dimensions in each of the 3 weight matrices through-\nout training. There are several interesting things evident in\nthis \ufb01gure. First, the step sizes, or effective learning rates (all\nterms except gt from Eqn. 14) shown in the left portion of the\n\ufb01gure are larger for the lower layers of the network and much\nsmaller for the top layer at the beginning of training. This\nproperty of ADADELTA helps to balance the fact that lower\nlayers have smaller gradients due to the diminishing gradi-\n0\n100\n200\n0\n0.5\n1\nd x1\n0\n100\n200\n0\n0.5\n1\nd x2\n0\n100\n200\n0\n0.5\n1\nd x3\n0\n100\n200\n\u22120.01\n0\n0.01\n6 x1\n0\n100\n200\n\u22120.01\n0\n0.01\n6 x2\n0\n100\n200\n\u22120.01\n0\n0.01\n6 x3\nFig. 2.\nStep sizes and parameter updates shown every 60\nbatches during training the MNIST network with tanh non-\nlinearities for 25 epochs. Left: Step sizes for 10 randomly\nselected dimensions of each of the 3 weight matrices of the\nnetwork. Right: Parameters changes for the same 10 dimen-\nsions for each of the 3 weight matrices. Note the large step\nsizes in lower layers that help compensate for vanishing gra-\ndients that occur with backpropagation.\nent problem in neural networks and thus should have larger\nlearning rates.\nSecondly, near the end of training these step sizes con-\nverge to 1. This is typically a high learning rate that would\nlead to divergence in most methods, however this conver-\ngence towards 1 only occurs near the end of training when the\ngradients and parameter updates are small. In this scenario,\nthe \u03f5 constants in the numerator and denominator dominate\nthe past gradients and parameter updates, converging to the\nlearning rate of 1.\nThis leads to the last interesting property of ADADELTA\nwhich is that when the step sizes become 1, the parameter\nupdates (shown on the right of Fig. 2) tend towards zero. This\noccurs smoothly for each of the weight matrices effectively\noperating as if an annealing schedule was present.\nHowever, having no explicit annealing schedule imposed\non the learning rate could be why momentum with the proper\nhyperparameters outperforms ADADELTA later in training as\nseen in Fig. 1. With momentum, oscillations that can occur\nnear a minima are smoothed out, whereas with ADADELTA\nthese can accumulate in the numerator. An annealing sched-\nule could possibly be added to the ADADELTA method to\ncounteract this in future work.\n4.4. Speech Data\nIn the next set of experiments we trained a large-scale neu-\nral network with 4 hidden layers on several hundred hours\nof US English data collected using Voice Search, Voice IME,\nand read data. The network was trained using the distributed\nsystem of [4] in which a centralized parameter server accu-\nmulates the gradient information reported back from several\nreplicas of the neural network. In our experiments we used ei-\nther 100 or 200 such replica networks to test the performance\nof ADADELTA in a highly distributed environment.\nThe neural network is setup as in [7] where the inputs\nare 26 frames of audio, each consisting of 40 log-energy \ufb01l-\nter bank outputs.\nThe outputs of the network were 8,000\nsenone labels produced from a GMM-HMM system using\nforced alignment with the input frames. Each hidden layer\nof the neural network had 2560 hidden units and was trained\nwith either logistic or recti\ufb01ed linear nonlinearities.\nFig. 3 shows the performance of the ADADELTA method\nwhen using 100 network replicas. Notice our method ini-\ntially converges faster and outperforms ADAGRAD through-\nout training in terms of frame classi\ufb01cation accuracy on the\ntest set. The same settings of \u03f5 = 1e\u22126 and \u03c1 = 0.95 from\nthe MNIST experiments were used for this setup.\nWhen training with recti\ufb01ed linear units and using 200\nmodel replicas we also used the same settings of hyperpa-\nrameters (see Fig. 4). Despite having 200 replicates which\ninherently introduces signi\ufb01cants amount of noise to the gra-\ndient accumulations, the ADADELTA method performs well,\nquickly converging to the same frame accuracy as the other\nmethods.\n0\n20\n40\n60\n80\n100\n18\n20\n22\n24\n26\n28\n30\n32\n34\n36\n38\nTime (hours)\nFame Acc %uracy\n \n \nADAGRAD log\nADADELTA log\nFig. 3. Comparison of ADAGRAD and ADADELTA on the\nSpeech Dataset with 100 replicas using logistic nonlinearities.\n0\n20\n40\n60\n80\n100\n120\n140\n160\n15\n20\n25\n30\n35\nTime (hours)\nFrame Accuracy %\n \n \nADAGRAD relu\nADADELTA relu\nMOMENTUM relu\nFig. 4.\nComparison of ADAGRAD, Momentum, and\nADADELTA on the Speech Dataset with 200 replicas using\nrecti\ufb01ed linear nonlinearities.\n5. CONCLUSION\nIn this tech report we introduced a new learning rate method\nbased on only \ufb01rst order information which shows promis-\ning result on MNIST and a large scale Speech recognition\ndataset. This method has trivial computational overhead com-\npared to SGD while providing a per-dimension learning rate.\nDespite the wide variation of input data types, number of hid-\nden units, nonlinearities and number of distributed replicas,\nthe hyperparameters did not need to be tuned, showing that\nADADELTA is a robust learning rate method that can be ap-\nplied in a variety of situations.\nAcknowledgements We thank Geoff Hinton, Yoram\nSinger, Ke Yang, Marc\u2019Aurelio Ranzato and Jeff Dean for\nthe helpful comments and discussions regarding this work.\n6. REFERENCES\n[1] H. Robinds and S. Monro, \u201cA stochastic approximation\nmethod,\u201d Annals of Mathematical Statistics, vol. 22, pp.\n400\u2013407, 1951.\n[2] D.E. Rumelhart, G.E. Hinton, and R.J. Williams, \u201cLearn-\ning representations by back-propagating errors,\u201d Nature,\nvol. 323, pp. 533\u2013536, 1986.\n[3] J. Duchi, E. Hazan, and Y. Singer, \u201cAdaptive subgradient\nmethods for online leaning and stochastic optimization,\u201d\nin COLT, 2010.\n[4] J. Dean, G. Corrado, R. Monga, K. Chen, M. Devin,\nQ. Le, M. Mao, M. Ranzato, A. Senior, P. Tucker,\nK. Yang, and A. Ng, \u201cLarge scale distributed deep net-\nworks,\u201d in NIPS, 2012.\n[5] S. Becker and Y. LeCun, \u201cImproving the convergence of\nback-propagation learning with second order methods,\u201d\nTech. Rep., Department of Computer Science, University\nof Toronto, Toronto, ON, Canada, 1988.\n[6] T. Schaul, S. Zhang, and Y. LeCun,\n\u201cNo more pesky\nlearning rates,\u201d arXiv:1206.1106, 2012.\n[7] N. Jaitly, P. Nguyen, A. Senior, and V. Vanhoucke, \u201cAp-\nplication of pretrained deep neural networks to large vo-\ncabulary speech recognition,\u201d in Interspeech, 2012.\n",
        "sentence": " The model is trained to minimize the negative loglikelihood of predicting the correct dialog acts of the utterances in the train set, using stochastic gradient descent with the Adadelta update rule (Zeiler, 2012).",
        "context": "eventually decreasing to zero and stopping training com-\npletely. We created our ADADELTA method to overcome the\nsensitivity to the hyperparameter selection as well as to avoid\nthe continual decay of the learning rates.\nremarkably good results on large scale learning tasks in a dis-\ntributed environment [4]. This method relies on only \ufb01rst\norder information but has some properties of second order\nmethods and annealing. The update rule for ADAGRAD is\nas follows:\n\u2206xt = \u2212\n\u03b7\nADADELTA: AN ADAPTIVE LEARNING RATE METHOD\nMatthew D. Zeiler1,2\u2217\n1Google Inc., USA\n2New York University, USA\nABSTRACT\nWe present a novel per-dimension learning rate method for\ngradient descent called ADADELTA. The method dynami-"
    }
]