[
    {
        "title": "A survey of robot learning from demonstration",
        "author": [
            "B.D. Argall",
            "S. Chernova",
            "M. Veloso",
            "B. Browning"
        ],
        "venue": "Robotics and Autonomous Systems,",
        "citeRegEx": "1",
        "shortCiteRegEx": "1",
        "year": 2009,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " Therefore, Learning From Demonstrations (LFD) [1] has gained a lot of interest in the recent past. According to [1], approaches for LFD can be grouped into (i) reward-based models and (ii) imitation learning. As explained in [1], imitation learning can be considered",
        "context": null
    },
    {
        "title": "Robot learning from demonstration",
        "author": [
            "C.G. Atkeson",
            "S. Schaal"
        ],
        "venue": "In Proceedings of the 14th International Conference on Machine Learning,",
        "citeRegEx": "2",
        "shortCiteRegEx": "2",
        "year": 1997,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " 3 Imitation Learning Instead of estimating the reward as in IRL, imitation learning aims at inferring the underlying policy directly [2, 44].",
        "context": null
    },
    {
        "title": "Dynamic Programming",
        "author": [
            "R. Bellman"
        ],
        "venue": null,
        "citeRegEx": "3",
        "shortCiteRegEx": "3",
        "year": 1957,
        "abstract": "",
        "full_text": "",
        "sentence": " Only in the case of finite state and action spaces and known rewards, learning optimal policies has been solved [3]. 1 Reinforcement Learning Large state and action spaces are especially problematic for value-based RL algorithms such as value-iteration [3] or Q-learning [48] since the value function, representing the expected accumulated reward, needs to be approximated.",
        "context": null
    },
    {
        "title": "Exploiting structure in policy construction",
        "author": [
            "C. Boutilier",
            "R. Dearden",
            "M. Goldszmidt"
        ],
        "venue": "In Proceedings of the 14th International Joint Conference on Artificial Intelligence,",
        "citeRegEx": "4",
        "shortCiteRegEx": "4",
        "year": 1995,
        "abstract": "",
        "full_text": "",
        "sentence": " An alternative framework for learning the latent structure of the state space is proposed in [9] which is based on Factored Markov Decision Processs (FMDPs) [4].",
        "context": null
    },
    {
        "title": "Linear least-squares algorithms for temporal difference learning",
        "author": [
            "S.J. Bradtke",
            "A.G. Barto"
        ],
        "venue": "Machine Learning,",
        "citeRegEx": "5",
        "shortCiteRegEx": "5",
        "year": 1996,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " [33, 5]. In early approaches, a set of basis functions, often referred to as features, is linearly weighted to represent the this function [5].",
        "context": null
    },
    {
        "title": "Nonparametric Bayesian inverse reinforcement learning for multiple reward functions",
        "author": [
            "J. Choi",
            "K.-E. Kim"
        ],
        "venue": "In Advances in Neural Information Processing Systems",
        "citeRegEx": "6",
        "shortCiteRegEx": "6",
        "year": 2012,
        "abstract": "",
        "full_text": "",
        "sentence": " A Bayesian nonparametric approach is proposed in [6], utilizing an Indian Buffet Process (IBP) to model feature activations.",
        "context": null
    },
    {
        "title": "Fast simulation of truncated Gaussian distributions",
        "author": [
            "N. Chopin"
        ],
        "venue": "Statistics and Computing,",
        "citeRegEx": "7",
        "shortCiteRegEx": "7",
        "year": 2010,
        "abstract": "We consider the problem of simulating a Gaussian vector X, conditional on the\nfact that each component of X belongs to a finite interval [a_i,b_i], or a\nsemi-finite interval [a_i,+infty). In the one-dimensional case, we design a\ntable-based algorithm that is computationally faster than alternative\nalgorithms. In the two-dimensional case, we design an accept-reject algorithm.\nAccording to our calculations and our numerical studies, the acceptance rate of\nthis algorithm is bounded from below by 0.5 for semi-finite truncation\nintervals, and by 0.47 for finite intervals. Extension to 3 or more dimensions\nis discussed.",
        "full_text": "arXiv:1201.6140v1  [stat.CO]  30 Jan 2012\nFAST SIMULATION OF TRUNCATED GAUSSIAN\nDISTRIBUTIONS\nNICOLAS CHOPIN, ENSAE-CREST\nAbstract. We consider the problem of simulating a Gaussian vector X, con-\nditional on the fact that each component of X belongs to a \ufb01nite interval\n[ai, bi], or a semi-\ufb01nite interval [ai, +\u221e).\nIn the one-dimensional case, we\ndesign a table-based algorithm that is computationally faster than alternative\nalgorithms. In the two-dimensional case, we design an accept-reject algorithm.\nAccording to our calculations and our numerical studies, the acceptance rate\nof this algorithm is bounded from below by 0.5 for semi-\ufb01nite truncation in-\ntervals, and by 0.47 for \ufb01nite intervals. Extension to 3 or more dimensions is\ndiscussed.\n1. Introduction\nLet X = (X1, . . . , Xd) be a d\u2212dimensional Gaussian vector with mean \u00b5 and\ncovariance matrix \u03a3, and let [ai, bi] be d intervals, where bi may be either a real\nnumber or +\u221e. The distribution of X, conditional on the event that Xi \u2208[ai, bi],\ni = 1, . . . , d, is usually called a truncated Gaussian distribution (Johnson et al.,\n1994, Chap. 13). Without loss of generality, one may assume that \u00b5 = 0, and that\n\u03a3 has unit diagonal elements.\nNumerous statistical algorithms rely on intensive simulation of truncated Gaus-\nsian distributions. In particular, several Bayesian models generate full conditional\ndistributions of this type, either directly or through a data augmentation represent-\nation (Tanner and Wong, 1987). Thus, the corresponding Gibbs samplers (or more\ngenerally Markov chain Monte Carlo algorithms) draw repetitively from truncated\nGaussian distributions. Examples include linear regression models with ordered\nparameters (Chen and Deely, 1996) or applied to truncated data (Gelfand et al.,\n1992), probit models (Albert and Chib, 1993), multinomial probit models (Albert and Chib,\n1993; McCulloch and Rossi, 1994; Nobile, 1998), multivariate probit models (Chib and Greenberg,\n1998), multiranked probit models (Linardakis and Dellaportas, 2003), tobit mod-\nels (Chib, 1992), models used in spectroscopy (Gulam Razul et al., 2003), copula\nregression models (Pitt et al., 2006), among others.\nTo understand how intensive such MCMC algorithms can be, consider the prob-\nlem of sampling the posterior distribution of a multinomial probit model with n\nobservations and p alternatives. A solution is to perform T iterations of the Gibbs\nsampler of McCulloch and Rossi (1994), but this requires the generation of T np\nunivariate truncated Gaussian variates, a number that may exceed 1012 or even\n1015 in di\ufb03cult scenarios. Hence any improvement with respect to the computa-\ntional cost of simulating univariate truncated Gaussian distributions may lead to\nimportant savings. Another important aspect of such algorithms is that they simu-\nlate only one random variable from a given truncated Gaussian distribution, that is,\nthe parameters and the truncation intervals [ai, bi] change every time a truncated\n1\nFAST SIMULATION OF TRUNCATED GAUSSIAN DISTRIBUTIONS\n2\nGaussian variate is generated. Thus, we are interested in developing specialised\nalgorithms which are guaranteed to generate quickly one random variable from the\ndesired distribution, for all possible inputs (i.e., parameters and truncation inter-\nvals). As a corollary, these algorithms cannot a\ufb00ord a long set-up (initialisation)\ntime, where some exploration of the target density is performed in order to improve\nperformance; this type of initialisation is meaningful only when one needs to simu-\nlate many variables from one \ufb01xed distribution, and is not discussed in this paper.\nThe algorithm we propose does require a table set-up, but which is independent of\nthe input parameters.\nThe \ufb01rst part of this paper presents a table-based simulation algorithm for uni-\nvariate Gaussian distributions truncated to either a \ufb01nite interval [a, b] or a semi-\n\ufb01nite interval [a, +\u221e). In the latter case, and given the truncation point a, our\nalgorithm is up to three times faster than alternative algorithms in our simulations;\nsee below for references. Our algorithm is inspired from the Ziggurat algorithm\nof Marsaglia and Tsang (1984, 2000), which is usually considered as the fastest\nGaussian sampler, and is also very close to Ahrens (1995)\u2019s algorithm.\nAnother possible strategy for accelerating a Gibbs sampler is to \u2018block\u2019, i.e., to\nupdate jointly, two or more components of the posterior density; this often strongly\nimproves the mixing properties of the algorithm. In some of the aforementioned\nmodels, blocking requires simulating multivariate truncated Gaussian variates. We\ndevelop an accept-reject algorithm for simulating from bivariate truncated Gaussian\ndistributions. In all but one particular case for \ufb01nite intervals, we manage to prove\nformally that the acceptance rate is bounded from below by 0.22. Our numerical\nstudies seem to indicate that this bound is not optimal, and that the acceptance rate\nis bounded from below by 1/2 when the truncation intervals are semi-\ufb01nite, and by\n0.477 when they are \ufb01nite. (This remains true even when the correlation coe\ufb03cient\nget close to 1 or \u22121, that is, in situations where MCMC blocking is particularly\ne\ufb03cient.) In the former case, we explain how to generalise this algorithm in some\nsituations to truncated Gaussian distributions of dimension d, with the outcome\nthat the acceptance rate is bounded from below by 1/2d\u22121. Interestingly, some of\nthe constants that must be pre-computed for our univariate algorithm can be re-\nused so as to bypass part of the computations performed by our multi-dimensional\nalgorithms.\nWe note that independent variables from truncated Gaussian distributions may\nalso be obtained using the perfect samplers of Philippe and Robert (2003) and\nFern\u00e1ndez et al. (2007), but, for small dimensions, these algorithms are much more\nexpensive than our approach, since each sample requires running a Markov chain un-\ntil some criterion is ful\ufb01lled. (According to H\u00f6rmann and Leydold (2006), Philippe and Robert\n(2003) may not sample from the correct distribution.)\nThe paper is organised as follows. Section 2 presents our algorithm for simulating\nunivariate truncated Gaussian variables. Section 3 presents a rejection algorithm\nfor simulating bivariate Gaussian vectors, the components of which are truncated\nto semi-\ufb01nite intervals [ai, +\u221e). Section 4 does the same thing for \ufb01nite truncation\nintervals. Section 5 explains how to generalise the algorithms of Section 3 to three\nor more dimensions. Section 6 concludes.\nFAST SIMULATION OF TRUNCATED GAUSSIAN DISTRIBUTIONS\n3\n2. One-dimensional case\nFirst, we consider the problem of simulating a random variable X from a uni-\nvariate Gaussian density truncated to [a, +\u221e):\n(2.1)\np(x) =\n\u03d5(x)\n\u03a6(\u2212a)I(x \u2265a)\nfor some truncation point a, where \u03d5 and \u03a6 denote respectively the unit Gaussian\nprobability density and cumulative distribution functions; \u03d5(x) = exp(\u2212x2/2)/\n\u221a\n2\u03c0.\nThe extension to a \ufb01nite truncation interval [a, b] is explained in \u00a72.5.\n2.1. Review of current algorithms. A convenient way to generate X is to use\nthe inverse transform method:\n(2.2)\nX = \u2212\u03a6\u22121 (\u03a6(\u2212a)U) ,\nwhere U \u223cU[0, 1] is a uniform variate. Note that this expression is equivalent to\n(2.3)\nX = \u03a6\u22121 (\u03a6(a) + {1 \u2212\u03a6(a)} U) ,\nbut the latter expression is less stable numerically for large values of a, because it is\neasier to approximate \u03a6\u22121 in the left tail than in the right tail. In our experiments,\n(2.3) generates \u201cinf\u201d values when a > 9.5, while (2.2) generates \u201cinf\u201d values only\nwhen a > 37.5.\nAs noted by Glasserman (2004, Chap. 2), the inverse transform method seldom\nproduces the fastest algorithms, but it has appealing properties that may justify\nthe increased cost in some settings, in particular when used in conjunction with\nvariance reduction or quasi Monte Carlo techniques; see the same reference and\nalso e.g. Blair et al. (1976) for an overview of fast methods for evaluating \u03a6 and\n\u03a6\u22121. We now focus on specialised algorithms.\nFirst, we recall brie\ufb02y the rejection principle (e.g. Devroye, 1986, Chap. 2 or\nH\u00f6rmann et al., 2004, Chap. 2). Assume we know of a proposal density q such that\np(x) \u2264Mq(x)\nfor some M \u22651, and all x in the support of q. Then a sample from p can be\nobtained as follows: simulate X \u223cq, and accept the realisation x with probability\np(x)/Mq(x); otherwise repeat. The expected acceptance probability, a.k.a. the\nacceptance rate, equals 1/M. It is important to choose q so that a) M is small and\nb) simulating from q is cheap.\nFor a \u22650, Devroye (1986, p. 382) proposes a rejection algorithm based on the\nproposal exponential density q(x) = \u03bb exp {\u2212\u03bb(x \u2212a)}, for x > a, with \u03bb = a.\nThe acceptance rate of this algorithm is a exp(a2/2)\u03a6(a), which goes to zero as\na \u21920, so it can be used only for a \u2265a0, with say a0 = 1. For a < a0, one\nmay use instead the following trivial rejection algorithm: repeat X \u223cN(0, 1) until\nX \u2265a.\nDevroye (1986, p.\n382) mentions Marsaglia (1964)\u2019s algorithm, which\nhas the same acceptance rate, but is a bit more expensive. Geweke (1991) and\nRobert (1995) independently derive a rejection algorithm for a \u22650, based again on\nq(x) = \u03bb exp {\u2212\u03bb(x \u2212a)}, but with \u03bb = (a +\n\u221a\na2 + 4)/2, which is shown to give\nthe optimal acceptance rate. For a < 0, these authors use the same trivial sampler\nas above. In principle, these algorithms may be re\ufb01ned using ARS (Gilks and Wild,\n1992), see also H\u00f6rmann (1995) and Evans and Swartz (1998), i.e., rejected points\nare used to improve the proposal density (which is then piecewise exponential). As\nFAST SIMULATION OF TRUNCATED GAUSSIAN DISTRIBUTIONS\n4\n-3\n-2\n-1\n0\n1\n2\n3\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\nFigure 2.1. Plot of N(0, 1) density and the 2N vertical rect-\nangles, for N = 20.\nexplained in the introduction however, we are interested in situations when only\none random variate must be generated (for a given value of a); hence, since the\nacceptance rate of Devroye\u2019s and Geweke and Robert\u2019s algorithms are high enough,\nwe do not discuss this further.\nThe algorithm we propose in this paper is faster than these specialised algorithms\nfor two reasons: (a) its acceptance rate is higher, and, in fact, is almost one, for\nmost values of a; and (b) with high probability, the only \ufb02oating point operations\nthat the algorithm performs are 2 additions and 3 multiplications, whereas the\naforementioned algorithms computes a few logarithms and square roots.\n2.2. Principle of proposed algorithm. For the sake of clarity, we consider \ufb01rst\nthe simulation of a non-truncated N(0, 1) density, and consider the extension to a\ntruncated density in next section. We do not claim, however, that this algorithm is\neither interesting or novel in the non-truncated case, see below for references. The\nprinciple of the algorithm is summarised by Figure 2.1. The proposal distribution\nconsists of 2N + 2 regions: 2N vertical rectangles of equal area, and two Gaussian\ntails of the same area. For rectangle i, i = \u2212N, . . . , N \u22121, let [xi, xi+1] denote its\nleft and right x-ordinates, yi its height, i.e., yi = \u03d5(xi) \u2228\u03d5(xi+1), yi the height\nof the smaller of its two immediate neighbours, i.e., yi = \u03d5(xi) \u2227\u03d5(xi+1), and let\ndi = xi+1 \u2212xi, \u03b4i = diyi/yi. (Symbols \u2227and \u2228means \u2018min\u2019 and max\u2019 throughout\nthe paper.) All these numbers are computed beforehand and de\ufb01ned as constants\nin the program. Note that the region labelled \u2212N \u22121 (resp. N) is the left tail\n(resp. right tail) truncated at x = x\u2212N (resp. at x = xN).\nTo sample X \u223cN(0, 1), one may proceed as follows: choose randomly region\ni, sample the point (X, Y ) uniformly within the chosen region, and accept X if\nY \u2264\u03d5(X); otherwise repeat. However, if the chosen region is a rectangle, most of\nthe computation can be bypassed: one may \ufb01rst simulate Y , i.e., draw U \u223cU[0, 1]\nand set Y = yiU, without simulating X, and check that the realisation y of Y is\nsuch that y \u2264yi; recall that yi = \u03d5(xi) \u2227\u03d5(xi+1). If this condition is ful\ufb01lled, then\nthe realised pair (x, y) must be accepted whatever the value of x. Furthermore, one\ncan recycle the realisation u of U, and therefore avoid drawing a second uniform\nvariate, by simply setting x = xi + \u03b4iu.\nIn short, with high probability, the algorithm only performs the following basic\noperations:\nFAST SIMULATION OF TRUNCATED GAUSSIAN DISTRIBUTIONS\n5\ndraw a random integer i uniformly in range \u2212N \u22121, . . . , N\nif i < N and i > \u2212N \u22121 then\nsample u \u223cU[0, 1]\ny \u2190yi \u2217u\nif y \u2264yi then\nreturn xi + \u03b4i \u2217u\nend if\nend if\nA complete outline of the algorithm is given in Appendix A. When the condition\ny \u2264yi is not ful\ufb01lled, one must sample X and check that (X, Y ) is indeed under\nthe curve of the N(0, 1) density. Similarly, when the chosen region is either the left\nor right tail, one may use Devroye\u2019s algorithm in order to simulate X.\nNote that this algorithm has a slightly higher numerical precision than the rejec-\ntion algorithms mentioned in the previous section: when calculating x = xi +\u03b4i \u2217u,\nthe absolute error equals the precision of the random generator that produces u,\nsay 2\u221232 \u22482.3 \u00d7 10\u221210 for a 32 bit generator, times the small number \u03b4i, which is\ntypically of order 10\u22123.\nMany algorithms proposed in the literature already use histograms to construct a\ngood proposal density; see e.g Marsaglia and Tsang (1984), Ahrens (1993), Zaman\n(1996) or the survey in H\u00f6rmann et al. (2004, Chap. 5). In particular, the above\nalgorithm is similar to the Ziggurat algorithm (Marsaglia and Tsang, 1984, 2000),\nwhich is the default Gaussian sampler in much mathematical software, e.g. Mat-\nlab or the GNU Scienti\ufb01c Library, and most similar to Ahrens (1995)\u2019s algorithm.\nBoth algorithms already use the idea of using rectangles of equal areas, but the Zig-\ngurat algorithm is based on horizontal rectangles, while Ahrens (1995)\u2019s algorithm\nis based on vertical ones, as above. This seemingly innocuous variation greatly\nfacilitates the extension to truncated densities, as explained in next section.\n2.3. Extension to truncated Gaussians. For a \ufb01xed truncation point a, let la\ndenote the index of the region that contains a. To adapt the above algorithm to\nthe truncated density (2.1), one may choose an integer ia such that ia \u2264la, sample\nrandomly one region among ia, ia + 1, . . . , N, and proceed as explained above. In\naddition, for the regions ia, . . . , la, one must reject the random point (X, Y ) if\nX < a, as described in Appendix A.\nThe di\ufb03culty is to de\ufb01ne ia in such a way that (a) the computation of ia is\nquick, and (b) ia is as close as possible to la, so that the overall acceptance rate is\nas high as possible. We propose the following method. We choose a small width\nh > 0, and store beforehand in an integer array the following quantities:\njk = max {i : xi \u2264kh} ,\nfor all k such that kh \u2208[amin, amax]\nfor some interval [amin, amax]. As said before, all these constants are computed\nseparately, and hard-coded in the program. Then, provided a \u2208[amin, amax], ia is\ncomputed as\nia = j\u230aa/h\u230b\nwhere \u230a\u00b7\u230bstands for the \ufb02oor function. Provided h \u2264mini(xi+1 \u2212xi), each interval\n[kh, (k + 1)h) contains at most one xi, so that either ia = la or ia = la \u22121. This\nmeans that, when choosing randomly between regions ia, ia + 1, . . . , N, one must\nFAST SIMULATION OF TRUNCATED GAUSSIAN DISTRIBUTIONS\n6\n\u00002\n\u00001\n0\n1\n2\n3\na\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\nexecution time (s)\nFigure 2.2. Execution time (seconds) vs truncation point, for 108\nsimulations of Devroye\u2019s algorithm (dotted line), Geweke-Robert\nsampler (dashed line), the inverse transform algorithm (based on\nthe inverse transform approximation of Wichura (1988), as imple-\nmented in the GSL library, dash-dotted line), and our algorithm\n(solid lines), with, from left to right, Ns = 1000, Ns = 2000,\nNs = 4000.\ntreat separately the two leftmost regions ia and ia + 1, and perform the additional\ncheck mentioned above, i.e., x \u2265a, but the other regions ia + 2, . . . , N can be\ntreated exactly as explained in the previous section.\nIn our simulations, we set amin = \u22122, amax = xN\u221220, and h = x1 \u2212x0, that is,\nthe smallest of the interval ranges (xi+1 \u2212xi). A complete outline of the algorithm\nis given in Appendix A.\n2.4. Results. We implemented our algorithm, the inverse transform algorithm,\nDevroye\u2019s algorithm (using cut-o\ufb00value a0 = 0.65) and Geweke-Robert\u2019s algorithm\nin C, using the GNU Scienti\ufb01c library (GSL). Figure 2.2 plots the execution time of\n108 runs on a 2.8 Ghz desktop computer, for each algorithm and for di\ufb00erent values\nof the truncation point a. Our algorithm appears to be up to two times faster than\nGeweke-Robert\u2019s algorithm, and up to three times faster than Devroye\u2019s algorithm\nand the inverse transform method. The three solid lines correspond to di\ufb00erent\nsizes Ns, Ns = 1000, 2000, 4000, from left to right, of the \ufb01ve arrays containing\nthe constants xi, yi, yi, di, \u03b4i; note that only those values such that xi, xi+1 \u2208\n[amin, amax] need to be stored, hence Ns < 2N. Figure 2.2 shows that increasing\nNs only improves the execution time for a tiny interval of a values, so there may be\nlittle point in increasing Ns further than, say, 4000. For Ns = 4000, the acceptance\nrate is typically above 0.99 or even 0.999 for most values of a \u2208[amin, amax]. The\nincrease of the computational cost for a > 2 is due to the increasing probability\nFAST SIMULATION OF TRUNCATED GAUSSIAN DISTRIBUTIONS\n7\nof sampling X from the right tail using Devroye\u2019s algorithm. Outside of the range\nof the represented interval, the rejection algorithms have a similar computational\ncost, as they perform the same operations. For Ns = 4000 and a double precision\nimplementation, the total memory cost of the algorithm is 162 kB, a small fraction\nof the memory cache of most modern CPU\u2019s. (A CPU memory cache is a small,\nfast memory where a CPU stores data used repetitively.)\n2.5. Truncation to a \ufb01nite interval [a, b]. The extension to \ufb01nite truncation\nintervals [a, b] is straightforward. First, one determines a region index ia (resp. i\u2032\nb)\nsuch that either region ia or region ia + 1 (resp. region i\u2032\nb or region i\u2032\nb \u22121) contains\na (resp. b), using the table look-up method described in 2.3. Then, one proceeds\nas above, choosing randomly a region in the range ia, . . . , i\u2032\nb, and so on.\nHowever, a di\ufb03culty arises if (b \u2212a) is small. Suppose for instance that a and\nb fall in the same region, and that b \u2212a is small with respect to the width of\nthe region. Then, if one samples uniformly point (X, Y ) within that region, the\nprobability that a \u2264X \u2264b may be arbitrarily small.\nWe propose the following work-around: when i\u2032\nb \u2212ia \u2264kmin, say kmin = 5, use\ninstead a rejection algorithm based on Devroye (1986)\u2019s exponential proposal, but\ntruncated to [a, b], i.e.,\n(2.4)\nq(x) =\n\u03bb exp(\u2212\u03bbx)\nexp(\u2212\u03bba) \u2212exp(\u2212\u03bbb)I(a \u2264x \u2264b).\nwith \u03bb = a (resp. \u03bb = b) when b > 0 (resp. when b \u22640). The advantage of this\napproach is that it gives an acceptance rate close to 1 whatever the values of a,\nand b, subject to i\u2032\nb \u2212ia \u2264kmin; i.e., whether a and b are both in the same tail,\nor both close to 0. In the latter case, q should be close numerically to a uniform\ndistribution.\n3. Bi-dimensional case: semi-finite intervals\nWe now consider the simulation of X = (X1, X2) \u223cN2(\u00b5, \u03a3), subject to X1 \u2265a1\nand X2 \u2265a2. Without loss of generality, we set \u00b5 = (0, 0)\u2032,\n\u03a3 =\n\u0012 1\n\u03c1\n\u03c1\n1\n\u0013\n,\nand assume that a1 \u2265a2; if necessary, swap components to impose the last condi-\ntion. The joint density of the considered truncated density is, up to a constant:\n(3.1)\np(x1, x2) \u221dexp\n\u001a\n\u22121\n2\u03bd2\n\u0000x2\n1 + x2\n2 \u22122\u03c1x1x2\n\u0001\u001b\nI (x1 \u2265a1; x2 \u2265a2) ,\nwhere the short-hand \u03bd2 = 1 \u2212\u03c12 will be used throughout the rest of the paper.\nThe conditional distribution of X2|X1 = x1 is a univariate Gaussian N(\u03c1x1, \u03bd2)\ntruncated to X2 \u2265a2, which we denote from now on T N[a2,\u221e)(\u03c1x1, \u03bd2). A common\nmisconception is that the marginal density of X1 is also a truncated Gaussian\ndensity, although standard calculus leads to:\n(3.2)\np(x1) \u221d\u03d5(x1)\u03a6\n\u0012\u03c1x1 \u2212a2\n\u03bd\n\u0013\nI(x1 \u2265a1).\nIn order to simulate from (3.1), a natural strategy is to derive a rejection al-\ngorithm for the marginal (3.2), and to simulate X2 conditional on X1, using the\nalgorithm we developed in Section 2. This is basically the approach adopted here,\nFAST SIMULATION OF TRUNCATED GAUSSIAN DISTRIBUTIONS\n8\nalthough we shall see that, in some cases, it is preferable to derive a rejection\nsampler for the joint distribution (3.1). We mention brie\ufb02y that universal bivariate\nsamplers exist, see e.g. H\u00f6rmann (2000) or Leydold (2000), but as in the univariate\ncase our objective is to design a specialised algorithm that runs faster (i.e., does\nnot require a set-up time), for situations where only one random vector must be\ngenerated.\nTo derive a proposal distribution for (3.2), we substitute the \u03a6(\u00b7) factor with a\nsimpler expression derived from the two following straightforward inequalities:\n(3.3)\n1\n2 \u2264\u03a6(x) \u22641\nfor x \u22650,\n(3.4)\n\u03a6(x) \u2264c(x0)\u03d5(x)\nfor x \u2264x0 \u22640,\nwhere c(x0) = (\np\n\u03c0/2) \u2227(\u22121/x0), for x0 < 0, c(0) =\np\n\u03c0/2. We now distinguish\nbetween cases where the argument of \u03a6(\u00b7) in (3.2) is positive, negative, or both,\nover the range of possible values for x1. We consider the following cases, and treat\nthem separately:\n\u2022 case S+: either \u03c1 \u22650 and \u03c1a1 \u2212a2 \u22650, or \u03c1 < 0 and a1 \u2264\u03a6\u22121(1/3) \u2248\n\u22120.4307\n\u2022 case S\u2212: \u03c1 < 0, \u03c1a1 \u2212a2 \u22640, and a1 > \u03a6\u22121(1/3) \u2248\u22120.4307.\n\u2022 case M + : \u03c1 \u22650 and \u03c1a1 \u2212a2 < 0.\n\u2022 case M \u2212: \u03c1 < 0, \u03c1a1 \u2212a2 > 0, and a1 > \u03a6\u22121(\u22121/3) \u2248\u22120.4307.\nwhere \u2018S\u2019 stands for \u2018Simple\u2019, and \u2018M\u2019 for \u2018Mixture\u2019, as we elaborate below.\nWe now prove that, in each case, it is possible to derive a rejection algorithm\nwith an acceptance rate bounded from below for all values of \u03c1, a1 and a2.\n3.1. Case S+. Assuming \ufb01rst \u03c1 \u22650 and \u03c1a1 \u2212a2 \u22650, then, according to (3.3),\n(3.5)\n\u03a6\n\u0012\u03c1x1 \u2212a2\n\u03bd\n\u0013\n\u2208[1/2, 1]\nfor all x \u2265a1, which suggests the following proposal distribution:\nqS+(x1) \u221d\u03d5(x1)I(x1 \u2265a1),\ni.e., a T N[a1,+\u221e)(0, 1) distribution, in order to sample from the marginal p(x1). For\na given x1 simulated from qS+, the acceptance probability equals (3.5), hence the\nacceptance rate of such a rejection algorithm equals\n(3.6)\n\u02c6 +\u221e\na1\n\u03a6\n\u0012\u03c1x1 \u2212a2\n\u03bd\n\u0013 \u03d5(x1)\n\u03a6(\u2212a1) dx1,\nand is larger than 1/2 by construction.\nHowever, it is more e\ufb03cient to simulate jointly (X1, X2) as follows: sample\nX1 \u223cT N[a1,\u221e)(0, 1), X2|X1 = x1 \u223cN(\u03c1x1, \u03bd2), and accept if X2 \u2265a2; otherwise\nrepeat. It is easy to check that the latter rejection algorithm has exactly the same\nacceptance rate, i.e., (3.6), as the former, but it is faster, because it does not perform\nany evaluation of \u03a6, and because X2 is obtained \u2018for free\u2019, i.e., a second step is not\nrequired to generate X2.\nWhen \u03c1 < 0, the argument of \u03a6 in (3.6) is not positive for all x1 \u2265a1, but\nthe integral is still larger than 1/2 provided a1 \u2264\u03a6\u22121(1/3).\nTo establish this\nproperty, one may remark that (3.6) is the probability that X2 \u2265a2, conditional\non X1 \u2265a1, provided (X1, X2) \u223cN2(\u00b5, \u03a3). This probability decreases with respect\nFAST SIMULATION OF TRUNCATED GAUSSIAN DISTRIBUTIONS\n9\nto a2, a2 \u2264a1, and, for a2 = a1, this probability decreases with respect to \u2212\u03c1 and\na1. Finally, for \u03c1 = \u22121 and a1 = a2 = \u03a6\u22121(1/3), this probability equals 1/2. Thus,\nwe use Algorithm S+ also when \u03c1 < 0 and a1 \u2264\u03a6\u22121(1/3).\n3.2. Case S\u2212. If \u03c1 < 0 and \u03c1a1 \u2212a2 \u22640, then inequality (3.4) holds for all values\nof the argument x = (\u03c1x1 \u2212a2)/\u03bd, and for x0 = (\u03c1a1 \u2212a2)/\u03bd. This suggests the\nfollowing algorithm: sample X1 from proposal density\nqS\u2212(x1)\n\u221d\n\u03d5(x1)\u03d5\n\u0012\u03c1x1 \u2212a2\n\u03bd\n\u0013\nI(x1 \u2265a1)\n\u221d\n\u03d5\n\u0000x1; \u03c1a2, \u03bd2\u0001\nI(x1 \u2265a1)\nthat is, the density of truncated Gaussian distribution T N[a1,\u221e)(\u03c1a2, \u03bd2), and ac-\ncept with probability:\n(3.7)\n\u03c8\n\u0012\n\u2212\u03c1x1 \u2212a2\n\u03bd\n\u0013\n/c\n\u0012\u03c1a1 \u2212a2\n\u03bd\n\u0013\n,\nwhere \u03c8(x) = \u03a6(\u2212x)/\u03d5(x). The acceptance rate is then:\n(3.8)\nET N[a1,\u221e)(\u03c1a2,\u03bd2)\n\u0014\n\u03c8\n\u0012\n\u2212\u03c1X1 \u2212a2\n\u03bd\n\u0013\u0015\n/c\n\u0012\u03c1a1 \u2212a2\n\u03bd\n\u0013\n.\nWe show formally in Appendix B1 that this acceptance rate admits a lower bound\nwhich is larger than 0.416; our numerical studies indicate that the optimal lower\nbound may be 1/2. Intuitively, the idea behind inequality (3.4) is that \u03a6(\u2212x) \u2248\n\u03d5(x)/x for large values of x, hence the true marginal density p(x1) behaves like a\nGaussian density density times 1/(a2\u2212\u03c1x1), but the latter factor varies slowly with\nrespect to a Gaussian density, so it can be discarded in the proposal.\n3.3. Case M \u2212. If \u03c1 < 0 and \u03c1a1 \u2212a2 > 0, the quantity (\u03c1x1 \u2212a2)/\u03bd takes positive\nand negative values for x1 \u2265a1. To combine both inequalities, one may use a\nmixture proposal:\n(3.9)\nqM\u2212(x1) \u221d\u03d5(x1)I (\u03c1x1 \u2212a2 > 0) +\nr\u03c0\n2 \u03d5(x1)\u03d5\n\u0012\u03c1x1 \u2212a2\n\u03bd\n\u0013\nI (\u03c1x1 \u2212a2 < 0)\nsubject to x1 \u2265a1. To sample from the mixture proposal, choose component 1\n(corresponding to the \ufb01rst term above), with probability \u03c91/(\u03c91 + \u03c92), with\n\u03c91 = \u03a6(a2/\u03c1) \u2212\u03a6(a1)\n\u03c92 = \u03bd\n2 exp\n\u001a\n\u2212a2\n2\n2\n\u001b\n\u03a6\n\u0012\n\u2212a2\u03bd\n\u03c1\n\u0013\nand choose component 2 otherwise. If component 1 is chosen, one can use the same\nshortcut as in Algorithm S+, that is, draw X1 \u223cT N[a1,a2/\u03c1](0, 1) and X2|X1 =\nx1 \u223cN(\u03c1x1, \u03bd2), and accept the simulated pair (x1, x2) if x2 \u2265a2. If component 2 is\nchosen, the proposed value for X1 is drawn from a T N[a2/\u03c1,\u221e)(\u03c1a2, \u03bd2) distribution,\nand is accepted with probability\nr\n2\n\u03c0 \u03c8\n\u0012\n\u2212\u03c1x1 \u2212a2\n\u03bd\n\u0013\n.\nWhen a draw x1 is accepted, it is completed with x2 drawn from X2|X1 = x1 \u223c\nT N[a2,+\u221e)(\u03c1x1, \u03bd2).\nFAST SIMULATION OF TRUNCATED GAUSSIAN DISTRIBUTIONS\n10\nThe acceptance rate of this algorithm is a weighted average (with weights given\nby \u03c91 and \u03c92) of the acceptance rate of Component 1, which is larger than 1/2 by\nconstruction, and the acceptance rate of algorithm S\u2212for a2 = \u03c1a1, which is also\nbounded from below, as explained in the previous subsection.\n3.4. Case M +. If \u03c1 \u22650 and \u03c1a1 \u2212a2 \u22640, then again (\u03c1x1 \u2212a2)/\u03bd take both\nnegative and positive values for x1 \u2265a1, which suggests that we use a mixture\nproposal similar to (3.9). Unfortunately, the acceptance rate may be arbitrarily\nsmall in that case. Exact calculations are omitted for the sake of space, but it can\nbe shown that the mode of p(x1) can be arbitrary far from x1 = a2/\u03c1, the point\nwhere the two components intersect, which gives an arbitrary small acceptance rate.\nInstead, we substitute (3.4) with a slightly di\ufb00erent inequality:\n(3.10)\n\u03a6(x) \u2264d(x0)\u03d5(x)e\u03bbx\nfor x0 \u2264x \u22640\nwhere \u03bb = 0.68, d(x0) = (\np\n\u03c0/2) \u2228\u03c7(\u2212x0), and \u03c7(x) = e\u03bbx\u03a6(\u2212x)/\u03d5(x).\nThis\ninequality stems from straightforward calculus. Other values of \u03bb are also valid,\nbut in our numerical experiments, \u03bb = 0.68 seemed to be close to optimal, in terms\nof minimum acceptance rate.\nThis inequality leads to the following proposal mixture density:\nqM+(x1)\n\u221d\n\u03d5(x1)I (\u03c1x1 \u2212a2 \u22650)\n+\u03d5(x1)\u03d5\n\u0012\u03c1x1 \u2212a2\n\u03bd\n\u0013\nexp\n\u0012\u03bb(\u03c1x1 \u2212a2)\n\u03bd\n\u0013\nd\n\u0012\u03c1a1 \u2212a2\n\u03bd\n\u0013\nI (\u03c1x1 \u2212a2 < 0)\nsubject to x1 \u2265a1. The second term is proportional to a N(\u03b8, \u03bd2) density, with\n\u03b8 = \u03c1(a2+\u03bb\u03bd). To sample from this mixture, choose component 1, with probability\n\u03c41/(\u03c41 + \u03c42), choose component 2 otherwise, where\n\u03c41 = \u03a6(\u2212a2/\u03c1)\n\u03c42\n=\n\u03bd\n\u221a\n2\u03c0\n\u001a\n\u03a6\n\u0012a2/\u03c1 \u2212\u03b8\n\u03bd\n\u0013\n\u2212\u03a6\n\u0012a1 \u2212\u03b8\n\u03bd\n\u0013\u001b\nexp\n\u001a\u03b82 \u2212a2\n2 \u22122\u03bb\u03bda2\n2\u03bd2\n\u001b\nd\n\u0012\u03c1a1 \u2212a2\n\u03bd\n\u0013\n.\nIf component 1 is selected, draw X1 \u223cT N[a2/\u03c1,+\u221e)(0, 1), X2|X1 = x1 \u223cN(\u03c1x1, \u03bd2),\nand accept simulated pair (x1, x2) if x2 \u2265a2. Otherwise, draw X1 \u223cT N[a1,a2/\u03c1](\u03b8, \u03bd2),\nand accept with probability\n\u03c7\n\u0012a2 \u2212\u03c1x1\n\u03bd\n\u0013\n/d\n\u0012a2 \u2212\u03c1a1\n\u03bd\n\u0013\n,\nand, upon acceptance, complete with\nX2|X1 = x1 \u223cT N[a2,+\u221e)(\u03c1x1, \u03bd2).\nWe show formally in Appendix B2 that the acceptance rate of this algorithm is\nbounded from below by 0.22, and we found numerically that the optimal lower\nbound seems to be 1/2, see Section 3.6.\nFAST SIMULATION OF TRUNCATED GAUSSIAN DISTRIBUTIONS\n11\n3.5. Computational cost. The above algorithms, except algorithm S+, involve\na few evaluations of function \u03a6, which is expensive.\nBut such evaluations can\nbe bypassed in most cases. In algorithm S\u2212for instance, given the expression of\nacceptance probability (3.7), one should accept the proposed value x1 if and only if\n\u03a6 (rx1 + s) \u2265ut(x1)\nwhere u is an uniform variate, and the exact expression of r, s, and t are easily\ndeduced from (3.7).\nIf good, fast approximations of \u03a6 are available, such that\n\u03a6(x) \u2264\u03a6(x) \u2264\u03a6(x), it is enough to check that \u03a6 (rx1 + s) \u2265ut(x1) (resp.\n\u03a6 (rx1 + s) < ut(x1)) to accept (resp. reject) x1. It is only when ut(x1) is very\nclose to \u03a6 (rx1 + s) that an exact evaluation of \u03a6 (rx1 + s) is required.\nSuch fast, good approximations of \u03a6 may be deduced from the tables of our\nunivariate algorithm, see Section 2.3. Speci\ufb01cally, and using the same notations\nas Section 2.3, let z = rx1 + s, and assume that z \u2208[amin, amax], then one may\nset \u03a6(z) = A(j\u230az/h\u230b+ 1), where A(i) > \u03a6(xi+1) denotes the total area of all the\nregions up to region i, and may be computed beforehand and hard-coded in the\nprogram like the other constants yi, di and so on. One may de\ufb01ne similarly \u03a6(z)\nusing \u03a6(z) = 1 \u2212\u03a6(\u2212z). When z /\u2208[amax, amin], one may use the expansion of\nAbramowitz and Stegun (1965, p. 932) to derive the following upper and lower\napproximations\n\u03a6(z) = \u2212\u03d5(z)\nz\n\u001a\n1 \u22121\nz2 + 3\nz4\n\u001b\n,\n\u03a6(z) = \u2212\u03d5(z)\nz\n\u001a\n1 \u22121\nz2 + 3\nz4 \u221215\nz6\n\u001b\n,\nfor z < 0. (For z > 0, similar formulae are obtained using \u03a6(z) = 1 \u2212\u03a6(\u2212z)).\nNote also that one does not necessarily have to compute all the terms of these\nexpressions. For instance, if \u03a61(\u03b1x1 + \u03b2) < u\u03c5(x1), where \u03a61(z) = \u2212\u03d5(z)/z, then\nthe proposed value can be rejected without computing the remaining terms. This\nprinciple can be used for each term of the expansion, but, on the other hand, it is\npreferable not to expand further the expressions above, since they work well already\nfor reasonable values of amin and amax, and since that would de\ufb01ne diverging series.\nProvided the above strategy is implemented, the algorithms proposed in the\nsection are reasonably fast, since they only involve a few basic operations, and\ntheir acceptance rate is greater than 1/2 for all parameters. Algorithms M + and\nM \u2212are slightly more expensive, as they require sampling a mixture index, but note\nthat the same strategy can be implemented in order to avoid with good probability\nthe evaluation of functions \u03a6 appearing in the expression of the mixture weights.\n3.6. Numerical illustration. We simulated 105 parameters (a1, a2, \u03c1), where \u03c1 \u223c\nU[\u22121, 1], a1, a2 \u223cN(0, s2), conditional on a1 \u2265a2.\nFor each parameter, we\nevaluated the acceptance rate of our algorithm by computing the average acceptance\nprobability over the proposed draws generated by 1000 runs. Figure 3.1 reports the\nhistogram of the acceptance rates for s = 1; larger values of s give histograms that\nare even more concentrated around 1.\nIn this simulation exercise, 90% of the acceptance rates are above 0.8 and 99%\nare above 0.65; none is lower than 1/2.\nFAST SIMULATION OF TRUNCATED GAUSSIAN DISTRIBUTIONS\n12\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\n0\n5000\n10000\n15000\n20000\n25000\nFigure 3.1. Histogram of acceptance rates corresponding to 105\nsimulated vectors (a1, a2, \u03c1), where \u03c1 \u223cU[0, 1], a1, a2 \u223cN(0, 1),\nconditional on a1 \u2265a2.\n4. Bi-dimensional case: finite intervals\nWe now consider the simulation of X = (X1, X2) \u223cN2(0, \u03a3), with\n\u03a3 =\n\u0012 1\n\u03c1\n\u03c1\n1\n\u0013\n,\nconditional on X1 \u2208[a1, b1] and X2 \u2208[a2, b2], where, without loss of generality,\n\u03c1 \u22650. As in the previous section, the main di\ufb03culty is to sample from the marginal\ndensity of X1:\n(4.1)\np(x1) \u221d\u03d5(x1) [\u03a6(\u03b1x1 + \u03b21) \u2212\u03a6(\u03b1x1 + \u03b20)] I(a1 \u2264x1 \u2264b1)\nwhere \u03b1 = \u03c1/\u03bd \u22650, \u03b21 = \u2212a2/\u03bd, \u03b20 = \u2212b2/\u03bd and \u03b21 \u2265\u03b20, since the distribution\nof X2 conditional on X1 = x1 is the univariate truncated Gaussian distribution\nT N[a2,b2](\u03c1x1, \u03bd2).\nWe shall consider two situations, according to whether \u03b21 \u2212\u03b20 < \u2206or not; we\ntake \u2206= 2, which seems to close to the optimal value in our simulations, in terms\nof minimum acceptance rate.\nWhen \u03b21 \u2212\u03b20 is small (case T ), one may Taylor expand the second factor of\n(4.1), which is denoted \u03ba from now on, into:\n\u03ba(x1) = \u03a6(\u03b1x1 + \u03b21) \u2212\u03a6(\u03b1x1 + \u03b20) \u2248(\u03b21 \u2212\u03b20)\u03d5\n\u0012\n\u03b1x1 + \u03b20 + \u03b21\n2\n\u0013\n,\nhence \u03ba behaves like a Gaussian density, see the right panel of Figure 4.1. Therefore,\np(x1) is also well approximated by a Gaussian, which is the basis of algorithm T\ndetailed in 4.2.\nOtherwise, when \u03b21 \u2212\u03b20 is large, \u03ba behaves like the curve plotted in the left\npanel of Figure 4.1. In this case, called M 3 below, we cut \u03ba into at most three\npieces, and derive a mixture proposal, using ideas similar to the previous section.\n4.1. Case M 3. Let \u03b3i = \u2212\u03b2i/\u03b1, for i = 0, 1, and \u03c5 = (\u03b21 \u2212\u03b20)/2; note \u03b31 \u2264\u03b30\nsince \u03b20 \u2264\u03b21.\nWe may divide the curve of \u03ba into three parts, so as to re-use\nthe same ideas as in Section 3, that is, deriving a mixture of Gaussian proposals.\nSpeci\ufb01cally,\nFAST SIMULATION OF TRUNCATED GAUSSIAN DISTRIBUTIONS\n13\n-4\n-2\n0\n2\n4\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n-3\n-2\n-1\n0\n1\n2\n3\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\n0.35\n0.40\nFigure 4.1. Function \u03ba(x1) = \u03a6(\u03b1x1 + \u03b21) \u2212\u03a6(\u03b1x1 + \u03b20) , for\n(\u03b1, \u03b20, \u03b21) = (2, \u22125, 5) (left), and (\u03b1, \u03b20, \u03b21) = (2, \u22120.5, 0.5) (right)\n\u2022 for x1 < \u03b31, one shows easily that\n{2\u03a6(\u2206) \u22121} \u03a6(\u03b1x1 + \u03b21) \u2264\u03ba(x1) \u2264\u03a6(\u03b1x1 + \u03b21).\nWe know already that target density \u03d5(x1)\u03a6(\u03b1x1 + \u03b21)I(x1 < \u03b31) can be\nsimulated e\ufb03ciently using a particular Gaussian proposal density, see Com-\nponent 2 of Algorithm M + in Section 3.4. The above inequality indicates\nthat, using the same proposal, one should obtain an acceptance rate which\nis at least 2\u03a6(\u2206) \u22121 times the minimum acceptance rate of M +, that is,\n\u03a6(\u2206) \u22121/2 \u22480.477.\n\u2022 for x1 \u2208[\u03b31, \u03b30], \u03ba is roughly \ufb02at, and\n\u03a6(\u2206) \u22121/2 \u2264\u03ba(x1) \u22642\u03a6(\u03c5) \u22121 \u22641.\nThis suggests using proposal distribution X1 \u223cT N[\u03b31\u2228a1,\u03b30\u2227b1](0, 1), and\naccept realisation x1 with probability \u03ba(x1)/ {2\u03a6(\u03c5) \u22121}. The acceptance\nprobability is then bounded from below by \u03a6(\u2206) \u22121/2 \u22480.477.\n\u2022 for x1 > \u03b30, one has:\n{2\u03a6(\u2206) \u22121} \u03a6(\u2212\u03b1x1 \u2212\u03b20) \u2264\u03ba(x1) \u2264\u03a6(\u2212\u03b1x1 \u2212\u03b20).\nAgain, one may use the same proposal as for Component 2 of algorithm\nM \u2212, see 3.3, which should lead to an acceptance rate which is larger than\n\u03a6(\u2206) \u22121/2 \u22480.477.\nThe principle of Algorithm M 3 is therefore to draw from a mixture of at most three\ncomponents, the relative weights of which are given below, and given the chosen\ncomponent, to use one of the three strategies described above.\nDenote \u03b6l, \u03b6c, \u03b6r, the unnormalised weights of the left, centre, and right com-\nponents, respectively. In case b1 \u2228\u03b31 > 0, one has\n\u03b6l = \u03bd d(\u03b1a1 + \u03b21)\n\u221a\n2\u03c0\nexp\n\u0012m2\nl \u2212a2\n2 \u22122\u03bb\u03bda2\n2\u03bd2\n\u0013 \u001a\n\u03a6\n\u0012\u03b31 \u2212ml\n\u03bd\n\u0013\n\u2212\u03a6\n\u0012a1 \u2212ml\n\u03bd\n\u0013\u001b\nI(\u03b31 \u2265a1)\nFAST SIMULATION OF TRUNCATED GAUSSIAN DISTRIBUTIONS\n14\nwith ml = \u03c1(a2 + \u03bb\u03bd) and function d was de\ufb01ned in Section 3.4; Otherwise, one\nobtains the same expression, but with \u03bb set to 0, i.e.,\n\u03b6l = \u03bd\n2 exp\n\u0012\n\u2212a2\n2\n2\n\u0013 \u001a\n\u03a6\n\u0012\u03b31 \u2212ml\n\u03bd\n\u0013\n\u2212\u03a6\n\u0012a1 \u2212ml\n\u03bd\n\u0013\u001b\nI(\u03b31 \u2265a1)\nwith ml = \u03c1a2. In all cases,\n\u03b6c = {2\u03a6(\u03c5) \u22121} {\u03a6(\u03b30 \u2227b1) \u2212\u03a6(\u03b31 \u2228a1)} I(b1 > \u03b31; a1 < \u03b30),\n\u03b6r = \u03bd\n2 exp\n\u0012\n\u2212b2\n2\n2\n\u0013 \u001a\n\u03a6\n\u0012b1 \u2212\u03c1b2\n\u03bd\n\u0013\n\u2212\u03a6\n\u0012\u03b30 \u2212\u03c1b2\n\u03bd\n\u0013\u001b\nI(b1 > \u03b30).\nOne may show that the acceptance rate of this algorithm is bounded from below\nby 1/2 \u2212\u03a6(\u2212\u2206) \u22480.477; simulations suggest this bound is optimal. We omit the\nexact calculations, as they are similar to those of previous algorithms. We managed\nto obtain this result under the following assumptions: (i) \u03c1 \u22650; (ii) b2 \u22650 and\n(iii) either a2 \u2265a1 or b1 \u22640. It is always possible to enforce such conditions, by\neither swapping X1 and X2, or changing their signs, or both. We note also that, in\nmost of our simulated exercises, at least one component of this mixture is empty,\nand often two of them are, which makes it possible to skip calculating the weights\nand simulating the mixture index.\n4.2. Case T . As explained above, when \u03b21 \u2212\u03b20 < \u2206, a good Gaussian approxim-\nation of p(x1) is\nq(x1) \u221d\u03d5(x1)\u03d5\n\u0012\n\u03b1x1 + \u03b20 + \u03b21\n2\n\u0013\nthat is, a N(m, s2) density with\n(m, s2) =\n\u0012\n\u2212\u03b1(\u03b20 + \u03b21)\n2(1 + \u03b12) ,\n1\n1 + \u03b12\n\u0013\n.\nIn our experiments, this approximation appears to be accurate for all values of\n\u03b1, \u03b20, \u03b21 (in the sense that the rejection rate of the algorithm we now describe is\nalways larger than 0.47 in our simulations, see next subsection). On the other hand,\nit seems di\ufb03cult to apply approximations similar to those we used before. Instead,\nwe note that p(x1) is a log-concave density (Prekopa, 1973). This suggests using\neither exponential or piecewise exponential proposals, and working out a simpli\ufb01ed\nversion of ARS (Adaptive Rejection Sampling, Gilks and Wild, 1992).\nSpeci\ufb01cally, let \u03be denote the marginal log-density of X1:\n\u03be(x1) = log \u03d5(x1) + log \u03ba(x1)\nthe derivative of which is easy to compute:\n\u03be\u2032(x1) = \u2212x1 + \u03b1 \u03d5(\u03b1x1 + \u03b21) \u2212\u03d5(\u03b1x1 + \u03b20)\n\u03a6(\u03b1x1 + \u03b21) \u2212\u03a6(\u03b1x1 + \u03b20),\nwhich leads to the inequality\n\u03be(x1) \u2264\u03be(v) + \u03be\u2032(v)(x1 \u2212v)\nfor any x1, v \u2208[a1, b1]. Up to a constant, the right hand side is the log-density of the\ntruncated Exponential distribution Exp[a1,b1](\u03bb) de\ufb01ned in (2.4), with \u03bb = \u03be\u2032(v);\nnote that \u03bb may be negative. Thus, one may sample x1 \u223cExp[a1,b1] {\u03be\u2032(v)}, and\naccept with probability\nexp {\u03be(x1) \u2212\u03be(v) \u2212\u03be\u2032(v)(x1 \u2212v)} .\nFAST SIMULATION OF TRUNCATED GAUSSIAN DISTRIBUTIONS\n15\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\n0\n2000\n4000\n6000\n8000\n10000\n12000\nFigure 4.2. Histogram of acceptance rates for 105 simulated para-\nmeters (\u03c1, a1, b1, a2, b2), with \u03c1 \u223cU[\u22121, 1], a1, a2 \u223cN(0, 22), and\nbi = ai + 2ei with ei \u223cExp(1), i = 1, 2.\nObviously, the di\ufb03culty is to choose v. Since the target density is well approximated\nby a T N[a1,b1](m, s2), and assuming that a1 \u2265m (resp. b1 \u2264m), it seems reasonable\nto set v = (m + s) \u2228a1 (resp. v = (m \u2212s) \u2227b1). These values would be optimal\nif the target density would be equal to its approximation T N[a1,b1](m, s2). In case\na1 < m and b1 > m, i.e., the mean m is within [a1, b1], we use instead a mixture\nproposal, based on two well chosen points v, w \u2208[a1, b1], say v < w. Thus,\n\u03be(x1) \u2264{\u03be(v) + \u03be\u2032(v)(x1 \u2212v)} \u2227{\u03be(w) + \u03be\u2032(w)(x1 \u2212w)}\nand one may sample from the density de\ufb01ned as the exponential of the right hand\nside above (which is a piecewise exponential distribution), and accept with prob-\nability\nexp [\u03be(x1) \u2212{\u03be(v) + \u03be\u2032(v)(x1 \u2212v)} \u2227{\u03be(w) + \u03be\u2032(w)(x1 \u2212w)}] .\nNote that, in the original ARS algorithm of Gilks and Wild (1992), the proposal\nis progressively re\ufb01ned by adding a new component each time the proposed value\nis rejected. We found however that the acceptance rate of Algorithm T described\nabove is generally above 1/2, so using \ufb01xed proposals with at most two components\nseems reasonable. Obviously, the good properties of the above algorithm lie in the\ngood choice of points v, w, which was made possible by the knowledge of a good\napproximation of the target density.\nWe were not able to prove formally that the acceptance rate of Algorithm T is\nbounded from below, as in previous cases, so we performed intensive simulations\nfor assessing its properties; the acceptance rate of Algorithm T seems to converges\nto one for limiting values, say a1 \u2192\u2212\u221ebut with b1 \u2212a1 and \u03c1 kept \ufb01xed; and to\nbe bounded from below by 1/2.\n4.3. Numerical illustration. We simulated 105 parameters (a1, b1, a2, b2, \u03c1), where\n\u03c1 \u223cU[\u22121, 1], a1, a2 \u223cN(0, 22), and bi = ai + 2ei with ei \u223cExp(1), i = 1, 2. For\neach parameter, we evaluated the acceptance rate of our algorithm by computing\nthe average acceptance probability over the proposed draws generated by 1000 runs.\nFigure 4.2 reports the histogram of the acceptance rates. About 90% of these val-\nues are above 0.71, about 99% are above 0.55, and all values are above 0.47, as\nexpected.\nFAST SIMULATION OF TRUNCATED GAUSSIAN DISTRIBUTIONS\n16\n5. Generalisation to 3 or more dimensions\nWe discuss brie\ufb02y the problem of simulating d-dimensional truncated Gaussian\ndistributions, for d \u22653 and for semi-\ufb01nite truncation intervals; i.e., X \u223cNd(0d, \u03a3),\nsubject to Xi \u2265ai, i = 1, . . . , d. As we have done for d = 2, we assume without\nloss of generality that a1 \u2265. . . \u2265ad. It does not seem possible to generalise to\ndimension d \u22653 algorithms based on mixture proposals, i.e., M + and M \u2212, as the\ncorresponding mixture weights would involve intractable integrals. But algorithms\nS+ and S\u2212can be generalised to larger dimensions, as explained below, which\nmakes it possible to sample X under certain conditions on \u03a3 and a = (a1, . . . , ad).\n5.1. Extension of Algorithm S+. An obvious way of generalising Algorithm S+\nto 3 dimensions is to do the following. Let Q = (qij) = \u03a3\u22121 and \u03a312 denote the\nsub-matrix obtained by removing the last row and the last column from \u03a3, then:\n(1) Sample (X1, X2) \u223cN2(02, \u03a312) conditional on X1 \u2265a1 and X2 \u2265a2 (using\none of the algorithms discussed in Section 3)\n(2) Sample from the unconstrained conditional distribution of X3:\nX3| {X1 = x1, X2 = x2} \u223cN\n\u0012\n\u2212q13x1 + q23x2\nq33\n, 1\nq33\n\u0013\n(3) If x3 \u2265a3, accept the simulated vector (x1, x2, x3); otherwise reject and go\nto Step 1.\nOne easily shows that the acceptance rate corresponding to Step 3 is larger than 1/2\nunder the following set of conditions: q13 \u22640, q23 \u22640 and q13a1+q23a2+q33a3 \u22640.\nOne may iterate the above principle so as to extend Algorithm S+ to any di-\nmension d; i.e., for d = 4, add Step 4 where X4 is simulated from the appropriate\nconditional distribution and accept if X4 \u2265a4; provided appropriate conditions\nsimilar to those above, are iteratively veri\ufb01ed, one obtains an overall acceptance\nrate that is at least 2\u2212(d\u22121), since each rejection step (including Step 1 above for\nthe \ufb01rst two variates X1 and X2) induces an acceptance rate that is at least 1/2.\nWe note that these iterative conditions imply in particular that any pair of\ncomponents of X is positively correlated, except for (X1, X2) which may have any\ntype of correlation. There are several practical settings where this assumption is\nmet, such as in Gaussian Markov random \ufb01elds models (e.g. Rue and Held, 2005)\nwhere one would impose positive correlation between neighbour nodes. Thus, in\nsuch or other particular settings, and since the acceptance rate is expected to be\nhigher than its lower bound for most parameters, as observed in two dimensions,\nthe above algorithm may remain practical for dimensions as large as 4 or 5. For\ninstance, in a Markov chain Monte Carlo context involving truncated Gaussian\nvectors or large dimension, one may try to form a larger and larger block, by\nincluding one variable at a time, checking the recursive assumptions above, and\nstop when either they are no longer met or the acceptance rate is too small.\n5.2. Extension of Algorithm S\u2212. Again, assuming d = 3, one notes that the\nmarginal distribution of (X1, X2) is:\np(x1, x2) \u221dexp\n\uf8f1\n\uf8f2\n\uf8f3\u22121\n2\n2\nX\ni,j=1\nqijxixj\n\uf8fc\n\uf8fd\n\uf8fe\u03a6\n \n\u2212\nP2\ni=1 qi3xi + q33a3\nq1/2\n33\n!\nI (x1 \u2265a1; x2 \u2265a2)\nFAST SIMULATION OF TRUNCATED GAUSSIAN DISTRIBUTIONS\n17\nwhich suggests the following bivariate truncated Gaussian density as a proposal\ndensity:\np(x1, x2) \u221dexp\n\uf8f1\n\uf8f4\n\uf8f2\n\uf8f4\n\uf8f3\n\u22121\n2\n2\nX\ni,j=1\nqijxixj \u2212\n\u0010P2\ni=1 qi3xi + q33a3\n\u00112\n2q33\n\uf8fc\n\uf8f4\n\uf8fd\n\uf8f4\n\uf8fe\nI (x1 \u2265a1; x2 \u2265a2)\nbased on inequality (3.4). For given x1 and x2, the acceptance probability is there-\nfore:\n\u03c8\n P2\ni=1 qi3xi + q33a3\nq1/2\n33\n!\n/c\n P2\ni=1 qi3xi + q33a3\nq1/2\n33\n!\nwhere we recall that \u03c8(x) = \u03a6(\u2212x)/\u03d5(x). Using the same type of calculations as in\nSection 3.4, one may show that the expectation of the acceptance probability above\nis larger than or equal to 1/2 provided q13 \u22650, q23 \u22650, and q13a1+q23a2+q33a3 \u22650.\nAgain, this means that the overall acceptance rate is larger than or equal to 1/4.\nAs in the previous section, one may iterate the construction above, so as to\nobtain a simulation algorithm for any dimension d, the acceptance rate of which\nis bounded from below by 2\u2212(d\u22121). This requires checking recursively conditions\nsimilar to those above. The same remarks in the previous subsection relative to the\napplicability of this algorithm may be repeated here.\n6. Conclusion\nWe focused in this paper on the simulation of independent truncated Gaus-\nsian variables, but similar ideas can be used in other settings, such as importance\nsampling or MCMC. For instance, in case T , see Section 4.2, one may use the de-\nrived Gaussian approximation as an importance distribution, rather than a basis\nof an ARS algorithm. The same remark applies to most of our algorithms.\nAs brie\ufb02y mentioned in the previous section, if one needs to simulate a vector\nfrom a high-dimensional truncated Gaussian distribution using MCMC, one may\nask how to choose blocks of two or more variables, which will be updated using the\nalgorithms proposed in this paper, in a way that ensures good MCMC convergence\nproperties. A good strategy would be to form a \ufb01rst block of two variables with\nstrong (conditional) correlation, then to see if additional variables may be included\nin that block, using the conditions given in the previous section, and repeat this\nprocess until all variables are included in a block of at least two variables. But more\nresearch is required to \ufb01nd the best trade-o\ufb00in terms of convenience and e\ufb03ciency.\nOpen source programs implementing the proposed algorithms are available at\nthe author\u2019s personal page on the website of his institution, www.crest.fr.\nAcknowledgements\nThe author thanks Paul Fearnhead, Pierre L\u2019Ecuyer, Christian Robert, C\u00f4me\nRoero, H\u00e5vard Rue, Florian Pelgrin, and the referees for helpful comments.\nReferences\nAbramowitz, M. and Stegun, I. (1965). Handbook of Mathematical Functions with\nFormulas, Graphs, and Mathematical Table. Dover.\nAhrens, J. (1993). Sampling from general distributions by suboptimal division of\ndomains. Grazer Math. Berichte, (319):20.\nFAST SIMULATION OF TRUNCATED GAUSSIAN DISTRIBUTIONS\n18\nAhrens, J. (1995). A one-table method for sampling from continuous and discrete\ndistributions. Computing, 54(2):127\u2013146.\nAlbert, J. H. and Chib, S. (1993). Bayesian analysis of binary and polychotomous\nresponse data. J. Am. Statist. Assoc., 88(422):669\u201379.\nBlair, J., Edwards, C., and Johnson, J. (1976). Rational Chebyshev approximations\nfor the inverse of the error function. Mathematics of Computation.\nChen, M. and Deely, J. (1996). Bayesian analysis for a constrained linear multiple\nregression problem for predicting the new crop of apples. Journal of Agricultural,\nBiological, and Environmental Statistics, 1(4):467\u2013489.\nChib, S. (1992). Bayes inference in the Tobit censored regression model. J. Econo-\nmetrics, 51(1-2):79\u201399.\nChib, S. and Greenberg, E. (1998). Analysis of multivariate probit models. Bio-\nmetrika, 85(2):347.\nDevroye, L. (1986). Non-Uniform Random Variate Generation. Springer-Verlag,\nNew York.\nEvans, M. and Swartz, T. (1998). Random variable generation using concavity\nproperties of transformed densities. J. Comput. Graph. Statist., 7(4):514\u2013528.\nFern\u00e1ndez, P., Ferrari, P., and Grynberg, S. (2007). Perfectly random sampling of\ntruncated multinormal distributions. Adv. in Appl. Probab., 39(4):973\u2013990.\nGelfand, A., Smith, A., and Lee, T. (1992).\nBayesian Analysis of Constrained\nParameter and Truncated Data Problems Using Gibbs Sampling. J. Am. Statist.\nAssoc., 87(418):523\u2013532.\nGeweke, J. (1991). E\ufb03cient simulation from the multivariate normal and Student-t\ndistributions subject to linear constraints and the evaluation of constraint prob-\nabilities.\nComputing Science and Statistics: Proceedings of the Twenty-Third\nSymposium on the Interface, 23:571\u2013578.\nGilks, W. and Wild, P. (1992). Adaptive rejection sampling for Gibbs sampling.\nAppl. Stat., 41(2):337\u2013348.\nGlasserman, P. (2004). Monte Carlo methods in \ufb01nancial engineering. Springer\nVerlag.\nGulam Razul, S., Fitzgerald, W., and Andrieu, C. (2003). Bayesian model selection\nand parameter estimation of nuclear emission spectra using RJMCMC. Nuclear\nInst. and Methods in Physics Research, A, 497(2-3):492\u2013510.\nH\u00f6rmann, W. (1995). A rejection technique for sampling from T-concave distribu-\ntions. ACM Trans. Math. Softw., 21(2):182\u2013193.\nH\u00f6rmann, W. (2000). Algorithm 802: an automatic generator for bivariate log-\nconcave distributions. ACM Trans. Math. Softw., 26(1):201\u2013219.\nH\u00f6rmann, W., Leydold, J., and Der\ufb02inger, G. (2004). Automatic nonuniform ran-\ndom variate generation. Springer.\nH\u00f6rmann, W. and Leydold, J. (2006). A note on perfect slice sampling. Technical\nReport 29, Dept. Stats. Maths. Wirtschaftsuniv.\nJohnson, N., Kotz, S., and Balakrishnan, N. (1994). Continuous Univariate Distri-\nbutions. Wiley.\nLeydold, J. (2000). Automatic sampling with the ratio-of-uniforms method. ACM\nTrans. Math. Softw., 26(1):78\u201398.\nLinardakis, M. and Dellaportas, P. (2003). Assessment of Athens\u2019s metro passenger\nbehaviour via a multiranked Probit model. J. R. Statist. Soc. C, 52(2):185\u2013200.\nFAST SIMULATION OF TRUNCATED GAUSSIAN DISTRIBUTIONS\n19\nMarsaglia, G. (1964). Generating a variable from the tail of the normal distribution.\nTechnometrics, 6(1):101\u2013102.\nMarsaglia, G. and Tsang, W. (2000). The ziggurat method for generating random\nvariables. Journal of Statistical Software, 5(8).\nMarsaglia, G. and Tsang, W. W. (1984). A fast, easily implemented method for\nsampling from decreasing or symmetric unimodal density functions. SIAM J.\nSci. Stat. Comput., 5:349\u2013359.\nMcCulloch, R. and Rossi, P. (1994). An exact likelihood analysis of the multinomial\nprobit model. J. Econometrics, 64(1):207\u2013240.\nNobile, A. (1998). A hybrid Markov chain for the Bayesian analysis of the multi-\nnomial probit model. Statist. Comput., 8(3):229\u2013242.\nPhilippe, A. and Robert, C. (2003). Perfect simulation of positive Gaussian distri-\nbutions. Statist. Comput., 13(2):179\u2013186.\nPitt, M., Chan, D., and Kohn, R. (2006). E\ufb03cient Bayesian inference for Gaussian\ncopula regression models. Biometrika, 93:537\u2013554.\nPrekopa, A. (1973). On logarithmic concave measures and functions. Acta Sci.\nMath.(Szeged), 34:335\u2013343.\nRobert, C. P. (1995). Simulation of truncated normal variables. Statist. Comput.,\n5:121\u2013125.\nRue, H. and Held, L. (2005). Gaussian Markov Random Fields: Theory and Ap-\nplications. Chapman & Hall/CRC.\nTanner, M. and Wong, W. (1987). The calculation of posterior distributions by\ndata augmentation. J. Am. Statist. Assoc., 82(398):528\u2013540.\nWichura, M. (1988).\nAlgorithm AS 241: The percentage points of the normal\ndistribution. Appl. Stat., pages 477\u2013484.\nZaman, A. (1996). Generating random numbers from a unimodal density by cutting\ncorners. Unpublished manuscript.\nFAST SIMULATION OF TRUNCATED GAUSSIAN DISTRIBUTIONS\n20\nAppendix A: Outline of the univariate algorithm for a semi-finite\ntruncation interval\nNote Devroye(a) refers to Devroye\u2019s algorithm, Direct(a) refers to the rejec-\ntion algorithm based on the non truncated Gaussian distribution, see Section 2.1 for\ndetails. In both cases the input a is the truncation point. Pre-computed constants\nconsist of \ufb01ve \ufb02oating-point tables: (xi), (yi), (yi), (di) and (\u03b4i); one integer table:\n(jk), plus two design parameters amin, and amax.\nRequire: a {truncation point}\nEnsure: x {simulated value}\nif a < amin then\nreturn Direct(a)\nelse if a > amax then\nreturn Devroye(a)\nend if\nia \u2190j\u230aa/h\u230b\nloop\nSample integer i uniformly between ia and N\nif i = N then {rightmost region}\nreturn Devroye(xN)\nelse if i \u2264ia + 1 then {two leftmost regions}\nSample u \u223cU[0, 1]\nx = xi + di \u2217u\nif x \u2265a then\nSample v \u223cU[0, 1]\ny \u2190yi \u2217v\nif y \u2264yi then\nreturn x\nelse if y \u2264\u03d5(x) then\nreturn x\nend if\nend if\nelse {all the other regions}\nSample u \u223cU[0, 1]\ny \u2190u \u2217yi\nif y \u2264yi then {occurs with high probability}\nreturn xi + u \u2217\u03b4i\nelse\nSample v \u223cU[0, 1]\nx \u2190xi + di \u2217v\nif y \u2264\u03d5(x) then\nreturn x\nend if\nend if\nend if\nend loop\nFAST SIMULATION OF TRUNCATED GAUSSIAN DISTRIBUTIONS\n21\nAppendix B: Lower bounds for Acceptance rates\nB1. algorithm S\u2212\nLet A(a1, a2, \u03c1) the acceptance rate (3.8), which we rewrite as:\nA(a1, a2, \u03c1) = ET N[\u03b1,\u221e)(\u03b2,\u03c12) [\u03c8 (Z)] /c (\u2212\u03b1)\nwhere Z = \u2212(\u03c1X1 \u2212a2)/\u03bd \u223cT N[\u03b1,\u221e)(\u03b2, \u03c12), \u03b1 = (a2 \u2212\u03c1a1)/\u03bd, and \u03b2 = a2\u03bd. Note\nthat \u03b1 \u22650, \u03b2 \u2264\u03b1, and \u03c8 is a decreasing function. Thus, the quantity above is a\ndecreasing function of \u03b2. (To see this, one can rewrite the distribution of Z as\nZ = \u03b2 + \u03c1\u03a6\u22121\n\u0012\nU + (1 \u2212U)\u03a6(\u03b1 \u2212\u03b2\n\u03c1\n)\n\u0013\nwhere U is an uniform variate, and check that, conditional on U = u, Z is a\ndecreasing function of \u03b2.). Thus, the above quantity is larger than or equal to the\nsame quantity, but with \u03b2 = \u03b1:\nA(a1, a2, \u03c1) \u2265ET N[\u03b1,\u221e)(\u03b1,\u03c12) [\u03c8 (Z)] /c (\u2212\u03b1) = ET N[0,\u221e)(0,1) [\u03c8 (\u03c1Z\u2032 + \u03b1)] /c (\u2212\u03b1)\nwhich is a decreasing function of \u03c1, hence\n(6.1)\nA(a1, a2, \u03c1) \u2265ET N[0,\u221e)(0,1) [\u03c8 (Z\u2032 + \u03b1)] /c (\u2212\u03b1) ,\nand since c(\u2212\u03b1) = (\np\n\u03c0/2) \u2227(1/\u03b1), one can show that the bound is minimised for\n\u03b1 =\np\n\u03c0/2, which leads to:\nA(a1, a2, \u03c1) \u2265\nr\n2\n\u03c0 ET N[0,\u221e)(0,1)\n\"\n\u03c8\n \nZ\u2032 +\nr\n2\n\u03c0\n!#\n\u22480.416.\nThis lower bound is not sharp, because not all combinations of (\u03b1, \u03b2, \u03c1) are valid,\neven in the constraints \u03b1 \u22650, \u03b2 \u2264\u03b1 are taken into account; for instance, \u03b2 = \u03b1\nimplies that a1 = \u03c1a2 \u2264\u03c12a1, which is impossible if \u03c1 \u0338= 1. Our simulations suggests\nthat the optimal lower bound is 1/2, see Section 3.6.\nB2. algorithm M +\nOne easily shows that \u03c7(x)/\u03c7(x\u2032) \u22651/2 for all x, x\u2032 \u2208[0, xd], with xd \u22483.117.\nThus, if (a2 \u2212\u03c1a1)/\u03bd \u2264xd, the acceptance rate is larger than or equal to 1/2 by\nconstruction. Now assume that (a2 \u2212\u03c1a1)/\u03bd > xd; note that \u03c7(x) is an increasing\nfunction for x > xd. Since a1 \u2265a2 and \u03c1 \u22641, one has\n\u03b8 = \u03c1(a2 + \u03bb\u03bd) \u2264a2 + \u03bd(\u03bb\u03c1 \u2212xd) < a1.\nThe acceptance rate equals\n(6.2)\nET N[a1,a2/\u03c1](\u03b8,\u03bd2)\n\uf8ee\n\uf8f0\n\u03c7\n\u0010\na2\u2212\u03c1X1\n\u03bd\n\u0011\nd\n\u0000 a2\u2212\u03c1a1\n\u03bd\n\u0001\n\uf8f9\n\uf8fb= ET N[0,zmax](\u03b7,\u03c12)\n\u0014 \u03c7 (Z)\n\u03c7 (zmax)\n\u0015\nwhere zmax = (a2 \u2212\u03c1a1)/\u03bd, and \u03b7 = (a2 \u2212\u03c1\u03b8)/\u03bd > zmax; note d(zmax) = \u03c7(zmax)\nprovided zmax > 0.751, but we assumed that zmax > xd \u22483.117. The T N[0,(a2\u2212\u03c1a1)/\u03bd](\u03b7, \u03c12)\ndistribution should concentrate its mass at the right edge of interval [0, zmax],\nand \u03c7(Z)/\u03c7(zmax) should take values close to one.\nSpeci\ufb01cally, one has that\nz\u03a6(\u2212z)/\u03d5(z) \u2208(0.84, 1) for z > 2, thus\n\u03c7(z)\n\u03c7(zmax) > 0.84e\u03bb(z\u2212zmax) \u22650.5\nFAST SIMULATION OF TRUNCATED GAUSSIAN DISTRIBUTIONS\n22\nfor z \u2208[zmax \u22120.76, zmax]. Therefore (6.2) is larger than 0.5 times the probability\nthat Z \u2265zmax\u22120.76, for Z \u223cT N[0,zmax](\u03b7, \u03c12), which is larger than or equal to 0.44,\nthe probability of the same event with respect to Z \u223cT N[0,zmax](zmax, 1), for zmax.\nThis gives a lower bound for (6.2) of 0.22. We obtained sharper bounds with more\ntedious calculations (omitted here), but more importantly, our simulation studies\nindicates that the optimal lower bound is likely to be larger than or equal to 1/2.\n",
        "sentence": " We use the algorithm presented in [7] to sample from a truncated Gaussian distribution.",
        "context": "variate Gaussian distributions truncated to either a \ufb01nite interval [a, b] or a semi-\n\ufb01nite interval [a, +\u221e). In the latter case, and given the truncation point a, our\nalgorithm is up to three times faster than alternative algorithms in our simulations;\nsee below for references. Our algorithm is inspired from the Ziggurat algorithm\nof Marsaglia and Tsang (1984, 2000), which is usually considered as the fastest\nGaussian sampler, and is also very close to Ahrens (1995)\u2019s algorithm.\nalgorithm we developed in Section 2. This is basically the approach adopted here,\nFAST SIMULATION OF TRUNCATED GAUSSIAN DISTRIBUTIONS\n8\nalthough we shall see that, in some cases, it is preferable to derive a rejection"
    },
    {
        "title": "Feature reinforcement learning: State of the art",
        "author": [
            "M. Daswani",
            "P. Sunehag",
            "M. Hutter"
        ],
        "venue": "Workshop on Sequential Decision-making with Big Data. Association for the Advancement of Artificial Intelligence,",
        "citeRegEx": "8",
        "shortCiteRegEx": "8",
        "year": 2014,
        "abstract": "",
        "full_text": "",
        "sentence": " In this framework, the goal is to learn a feature mapping from the agent\u2019s history (comprised of actions, states, and rewards) to a MDP state, enabling decision learning for infinite state spaces [8].",
        "context": null
    },
    {
        "title": "Learning the structure of factored Markov decision processes in reinforcement learning problems",
        "author": [
            "T. Degris",
            "O. Sigaud",
            "P.-H. Wuillemin"
        ],
        "venue": "In Proceedings of the 23rd International Conference on Machine learning,",
        "citeRegEx": "9",
        "shortCiteRegEx": "9",
        "year": 2006,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " An alternative framework for learning the latent structure of the state space is proposed in [9] which is based on Factored Markov Decision Processs (FMDPs) [4].",
        "context": null
    },
    {
        "title": "Accelerated sampling for the Indian buffet process",
        "author": [
            "F. Doshi-Velez",
            "Z. Ghahramani"
        ],
        "venue": "In Proceedings of the 26th Annual International Conference on Machine Learning,",
        "citeRegEx": "10",
        "shortCiteRegEx": "10",
        "year": 2009,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " Thus, we need to store only rows with active elements in memory, which can be understood as realizations of K features, where the average number of active rows is given by K\u0304 = \u03b1a \u2211D d=1 \u03b2a \u03b2a+d\u22121 [13, 10]. In the second step, K new features are proposed in a Metropolis step [10, 22].",
        "context": null
    },
    {
        "title": "Are we ready for autonomous driving? The KITTI vision benchmark suite",
        "author": [
            "A. Geiger",
            "P. Lenz",
            "R. Urtasun"
        ],
        "venue": "In Proceedings of the 25th IEEE International Conference on Computer Vision and Pattern Recognition,",
        "citeRegEx": "11",
        "shortCiteRegEx": "11",
        "year": 2012,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " For this, we consider real data provided by the KITTI Vision Benchmark Suite [11] containing several challenges in urban driving.",
        "context": null
    },
    {
        "title": "Discovering latent causes in reinforcement learning",
        "author": [
            "S.J. Gershman",
            "K.A. Norman",
            "Y. Niv"
        ],
        "venue": "Current Opinion in Behavioral Sciences,",
        "citeRegEx": "12",
        "shortCiteRegEx": "12",
        "year": 2015,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " This idea has also been investigated from a psychological point of view by the concept of discovering latent causes in human behavior, which is related to learning state space representations [12].",
        "context": null
    },
    {
        "title": "Infinite latent feature models and the Indian buffet process",
        "author": [
            "Z. Ghahramani",
            "T.L. Griffiths"
        ],
        "venue": "In Advances in Neural Information Processing Systems",
        "citeRegEx": "13",
        "shortCiteRegEx": "13",
        "year": 2005,
        "abstract": "",
        "full_text": "",
        "sentence": " (2) can be easily extended to an infinite feature model by placing an IBP [13, 15] prior over A. 3 Prior for the feature activations The feature activations are modeled by means of an IBP [13, 15], assuming an infinite number of features. Placing a Beta prior with hyperparameters \u03b1a\u03b2a K? and \u03b2a over the parameter of the Binomial distribution and marginalizing over this parameter yields a Beta-Binomial distribution [13, 14]. Since we are interested in sampling from an infinite number of features, we consider the limit for K \u2192\u221e, resulting in the distribution of the activation matrix A [13, 14], Thus, we need to store only rows with active elements in memory, which can be understood as realizations of K features, where the average number of active rows is given by K\u0304 = \u03b1a \u2211D d=1 \u03b2a \u03b2a+d\u22121 [13, 10]. (11) is [13, 14] The probability of adding K features is given as [13, 14]",
        "context": null
    },
    {
        "title": "Bayesian nonparametric latent feature models",
        "author": [
            "Z. Ghahramani",
            "P. Sollich",
            "T.L. Griffiths"
        ],
        "venue": "In Bayesian Statistics",
        "citeRegEx": "14",
        "shortCiteRegEx": "14",
        "year": 2007,
        "abstract": "Abstract\n               We describe a flexible nonparametric approach to latent variable modelling in which the number of latent variables is unbounded. This approach is based on a probability distribution over equivalence classes of binary matrices with a finite number of rows, corresponding to the data points, and an unbounded number of columns, corresponding to the latent variables. Each data point can be associated with a subset of the possible latent variables, which we refer to as the latent features of that data point. The binary variables in the matrix indicate which latent feature is possessed by which data point, and there is a potentially infinite array of features. We derive the distribution over unbounded binary matrices by taking the limit of a distribution over N \u00d7 K binary matrices as K \u2192 \u221e . We define a simple generative processes for this distribution which we call the Indian buffet process (IBP; Griffiths and Ghahramani, 2005, 2006) by analogy to the Chinese restaurant process (Aldous, 1985; Pitman, 2002). The IBP has a single hyperparameter which controls both the number of feature per object and the total number of features. We describe a two-parameter generalization of the IBP which has additional flexibility, independently controlling the number of features per object and the total number of features in the matrix. The use of this distribution as a prior in an infinite latent feature model is illustrated, and Markov chain Monte Carlo algorithms for inference are described.",
        "full_text": "",
        "sentence": " In the following, we consider the two-parameter generalization [14] which allows to sample sparse as well as dense matrices. Placing a Beta prior with hyperparameters \u03b1a\u03b2a K? and \u03b2a over the parameter of the Binomial distribution and marginalizing over this parameter yields a Beta-Binomial distribution [13, 14]. Since we are interested in sampling from an infinite number of features, we consider the limit for K \u2192\u221e, resulting in the distribution of the activation matrix A [13, 14], The hyperparameters \u03b1a and \u03b2a reflect our prior belief about the number of features and the sparsity of the matrix [14]. (11) is [13, 14] The probability of adding K features is given as [13, 14]",
        "context": null
    },
    {
        "title": "The Indian buffet process: An introduction and review",
        "author": [
            "T.L. Griffiths",
            "Z. Ghahramani"
        ],
        "venue": "Journal of Machine Learning Research,",
        "citeRegEx": "15",
        "shortCiteRegEx": "15",
        "year": 2011,
        "abstract": "",
        "full_text": "",
        "sentence": " (2) can be easily extended to an infinite feature model by placing an IBP [13, 15] prior over A. 3 Prior for the feature activations The feature activations are modeled by means of an IBP [13, 15], assuming an infinite number of features.",
        "context": null
    },
    {
        "title": "An introduction to variable and feature selection",
        "author": [
            "I. Guyon",
            "A. Elisseeff"
        ],
        "venue": "Journal of Machine Learning Research,",
        "citeRegEx": "16",
        "shortCiteRegEx": "16",
        "year": 2003,
        "abstract": "",
        "full_text": "",
        "sentence": " An excellent overview is given in [16].",
        "context": null
    },
    {
        "title": "Inverse reinforcement learning using expectation maximization in mixture models",
        "author": [
            "J. Hahn",
            "A.M. Zoubir"
        ],
        "venue": "In Proceedings of the 40th IEEE International Conference on Acoustics, Speech and Signal Processing,",
        "citeRegEx": "17",
        "shortCiteRegEx": "17",
        "year": 2015,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " , in [30, 17].",
        "context": null
    },
    {
        "title": "Feature reinforcement learning: Part I",
        "author": [
            "M. Hutter"
        ],
        "venue": "Unstructured MDPs. Journal of Artificial General Intelligence,",
        "citeRegEx": "18",
        "shortCiteRegEx": "18",
        "year": 2009,
        "abstract": "",
        "full_text": "",
        "sentence": " A different concept to employ features is proposed by Hutter with the Feature Reinforcement Learning framework [18].",
        "context": null
    },
    {
        "title": "Spike and slab variable selection: Frequentist and Bayesian strategies",
        "author": [
            "H. Ishwaran",
            "J.S. Rao"
        ],
        "venue": "Annals of Statistics,",
        "citeRegEx": "19",
        "shortCiteRegEx": "19",
        "year": 2005,
        "abstract": "Variable selection in the linear regression model takes many apparent faces\nfrom both frequentist and Bayesian standpoints. In this paper we introduce a\nvariable selection method referred to as a rescaled spike and slab model. We\nstudy the importance of prior hierarchical specifications and draw connections\nto frequentist generalized ridge regression estimation. Specifically, we study\nthe usefulness of continuous bimodal priors to model hypervariance parameters,\nand the effect scaling has on the posterior mean through its relationship to\npenalization. Several model selection strategies, some frequentist and some\nBayesian in nature, are developed and studied theoretically. We demonstrate the\nimportance of selective shrinkage for effective variable selection in terms of\nrisk misclassification, and show this is achieved using the posterior from a\nrescaled spike and slab model. We also show how to verify a procedure's ability\nto reduce model uncertainty in finite samples using a specialized forward\nselection strategy. Using this tool, we illustrate the effectiveness of\nrescaled spike and slab models in reducing model uncertainty.",
        "full_text": "",
        "sentence": " In particular, we consider a sparsity-promoting mixture prior on the substates, similar to a spike and slab model [28, 19].",
        "context": null
    },
    {
        "title": "Principal component analysis",
        "author": [
            "I.T. Jolliffe"
        ],
        "venue": null,
        "citeRegEx": "20",
        "shortCiteRegEx": "20",
        "year": 2002,
        "abstract": "",
        "full_text": "",
        "sentence": " A simple approach to the considered problem would be the use of a feature extraction technique such as Principal Component Analysis (PCA) [20] or Non-negative Matrix Factorization (NMF) [23] to learn features from the observed states. 1 Feature model for learning from demonstrations We assume a linear latent feature model, similar to NMF [23] and PCA [20].",
        "context": null
    },
    {
        "title": "Planning and acting in partially observable stochastic domains",
        "author": [
            "L.P. Kaelbling",
            "M.L. Littman",
            "A.R. Cassandra"
        ],
        "venue": "Artificial Intelligence,",
        "citeRegEx": "21",
        "shortCiteRegEx": "21",
        "year": 1998,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " Since we consider a decisionmaking task, the investigated problem can be modeled by means of a Partially Observable Markov Decision Process (POMDP) [21, 42], which is defined by \u2022 a set of observations, Z, \u2022 a set of states, X , \u2022 a finite set of Nu actions, U , \u2022 a transition model, which describes the probability of entering a state after taking an action in the current state, \u2022 an observation model which explains how the observations are generated from the states, \u2022 a discount factor, which penalizes long-term rewards, \u2022 and a reward function, R. Acting according to a stochastic policy maximizes the expected return [21].",
        "context": null
    },
    {
        "title": "Nonparametric Bayesian sparse factor models with application to gene expression modeling",
        "author": [
            "D. Knowles",
            "Z. Ghahramani"
        ],
        "venue": "Annals of Applied Statistics,",
        "citeRegEx": "22",
        "shortCiteRegEx": "22",
        "year": 2011,
        "abstract": "A nonparametric Bayesian extension of Factor Analysis (FA) is proposed where\nobserved data $\\mathbf{Y}$ is modeled as a linear superposition, $\\mathbf{G}$,\nof a potentially infinite number of hidden factors, $\\mathbf{X}$. The Indian\nBuffet Process (IBP) is used as a prior on $\\mathbf{G}$ to incorporate sparsity\nand to allow the number of latent features to be inferred. The model's utility\nfor modeling gene expression data is investigated using randomly generated data\nsets based on a known sparse connectivity matrix for E. Coli, and on three\nbiological data sets of increasing complexity.",
        "full_text": "arXiv:1011.6293v2  [stat.AP]  28 Jul 2011\nThe Annals of Applied Statistics\n2011, Vol. 5, No. 2B, 1534\u20131552\nDOI: 10.1214/10-AOAS435\nc\n\u20ddInstitute of Mathematical Statistics, 2011\nNONPARAMETRIC BAYESIAN SPARSE FACTOR MODELS WITH\nAPPLICATION TO GENE EXPRESSION MODELING\nBy David Knowles1 and Zoubin Ghahramani2\nUniversity of Cambridge\nA nonparametric Bayesian extension of Factor Analysis (FA) is\nproposed where observed data Y is modeled as a linear superposi-\ntion, G, of a potentially in\ufb01nite number of hidden factors, X. The\nIndian Bu\ufb00et Process (IBP) is used as a prior on G to incorporate\nsparsity and to allow the number of latent features to be inferred. The\nmodel\u2019s utility for modeling gene expression data is investigated using\nrandomly generated data sets based on a known sparse connectivity\nmatrix for E. Coli, and on three biological data sets of increasing\ncomplexity.\n1. Introduction.\nPrincipal Components Analysis (PCA), Factor Anal-\nysis (FA) and Independent Components Analysis (ICA) are models which\nexplain observed data, yn \u2208RD, in terms of a linear superposition of inde-\npendent hidden factors, xn \u2208RK, so\nyn = Gxn + \u03b5n,\n(1)\nwhere G is the factor loading matrix and \u03b5n is a noise vector, usually as-\nsumed to be Gaussian. These algorithms can be expressed in terms of per-\nforming inference in appropriate probabilistic models. The latent factors are\nusually considered as random variables, and the mixing matrix as a parame-\nter to estimate. In both PCA and FA the latent factors are given a standard\n(zero mean, unit variance) normal prior. In PCA the noise is assumed to\nbe isotropic, whereas in FA the noise covariance is only constrained to be\ndiagonal. A standard approach in these models is to integrate out the latent\nfactors and \ufb01nd the maximum likelihood estimate of the mixing matrix. In\nReceived June 2010; revised October 2010.\n1Supported by Microsoft Research through the Roger Needham Scholarship at Wolfson\nCollege, University of Cambridge.\n2Supported by EPSRC Grant EP/F027400/1.\nKey words and phrases. Nonparametric Bayes, sparsity, factor analysis, Markov chain\nMonte Carlo, Indian bu\ufb00et process.\nThis is an electronic reprint of the original article published by the\nInstitute of Mathematical Statistics in The Annals of Applied Statistics,\n2011, Vol. 5, No. 2B, 1534\u20131552. This reprint di\ufb00ers from the original in\npagination and typographic detail.\n1\n2\nD. KNOWLES AND Z. GHAHRAMANI\nICA the latent factors are assumed to be heavy-tailed, so it is not usually\npossible to integrate them out. In this paper we take a fully Bayesian ap-\nproach, viewing not only the hidden factors but also the mixing coe\ufb03cients\nas random variables whose posterior distribution given data we aim to infer.\nSparsity plays an important role in latent feature models, and is desir-\nable for several reasons. It gives improved predictive performance, because\nfactors irrelevant to a particular dimension are not included. Sparse models\nare more readily interpretable since a smaller number of factors are asso-\nciated with observed dimensions. In many real world situations there is an\nintuitive reason why we expect sparsity: for example, in gene regulatory net-\nworks a transcription factor will only regulate genes with speci\ufb01c motifs. In\nour previous work [Knowles and Ghahramani (2007)] we investigated the\nuse of sparsity on the latent factors xn, but this formulation is not appro-\npriate in the case of modeling gene expression, where, as described above,\na transcription factor will regulate only a small set of genes, corresponding\nto sparsity in the factor loadings, G. Here we propose a novel approach\nto sparse latent factor modeling where we place sparse priors on the factor\nloading matrix, G. The Bayesian Factor Regression Model of West et al.\n(2007) is closely related to our work in this way, although the hierarchical\nsparsity prior they use is somewhat di\ufb00erent. An alternative \u201csoft\u201d approach\nto incorporating sparsity is to put a Gamma(a,b) (usually exponential, i.e.,\na = 1) prior on the precision of each element of G independently, result-\ning in the elements of G being marginally Student-t distributed a priori;\nsee Fokoue (2004), Fevotte and Godsill (2006), and Archambeau and Bach\n(2009). A LASSO-based approach to generating a sparse factor loading has\nalso been developed [Zou, Hastie and Tibshirani (2004); Witten, Tibshirani\nand Hastie (2009)]. We compare these sparsity schemes empirically in the\ncontext of gene expression modeling.\nA problematic issue with this type of model is how to choose the latent\ndimensionality of the factor space, K. Model selection can be used to choose\nbetween di\ufb00erent values of K, but generally requires signi\ufb01cant manual in-\nput and still requires the range of K over which to search to be speci\ufb01ed.\nZhang et al. (2004) applied Reversible Jump MCMC to PCA, which has\nmany of the advantages of our approach: a posterior distribution over the\nnumber of latent dimensions can be approximated, and the number of la-\ntent dimensions could potentially be unbounded. However, RJ MCMC is\nconsiderably more complex to implement for sparse Factor Analysis than\nour proposed framework.\nWe use the Indian Bu\ufb00et Process [Gri\ufb03ths and Ghahramani (2006)],\nwhich de\ufb01nes a distribution over in\ufb01nite binary matrices, to provide sparsity\nand a framework for inferring the appropriate latent dimension of the data\nset using a straightforward Gibbs sampling algorithm. The Indian Bu\ufb00et\nProcess (IBP) allows a potentially unbounded number of latent factors, so\nNONPARAMETRIC BAYESIAN SPARSE FACTOR MODELS\n3\nwe do not have to specify a maximum number of latent dimensions a priori.\nWe denote our model \u201cNSFA\u201d for \u201cNonparametric Sparse Factor Analysis.\u201d\nOur model is closely related to that of Rai and Daum\u00b4e III (2008), and is\na simultaneous development.\n2. The model.\nWe will de\ufb01ne our model in terms of equation (1). Let Z\nbe a binary matrix whose (d,k)th element represents whether observed di-\nmension d includes any contribution from factor k. We then model the mix-\ning matrix by\np(gdk|Zdk,\u03bbk) = ZdkN(gdk;0,\u03bb\u22121\nk ) + (1 \u2212Zdk)\u03b40(gdk),\n(2)\nwhere \u03bbk is the inverse variance (precision) of the kth factor and \u03b40 is a delta\nfunction (point-mass) at 0. Distributions of this type are sometimes known\nas \u201cspike and slab\u201d distributions. We allow a potentially in\ufb01nite number of\nhidden sources, so that Z has in\ufb01nitely many columns, although only a \ufb01nite\nnumber will have nonzero entries. This construction allows us to use the IBP\nto provide sparsity and de\ufb01ne a generative process for the number of latent\nfactors.\nWe assume independent Gaussian noise, \u03b5n, with diagonal covariance ma-\ntrix \u03a8. We \ufb01nd that for many applications assuming isotropic noise is too\nrestrictive, but this option is available for situations where there is strong\nprior belief that all observed dimensions should have the same noise vari-\nance. The latent factors, xn, are given Gaussian priors. Figure 1 shows the\ncomplete graphical model.\n2.1. De\ufb01ning a distribution over in\ufb01nite binary matrices.\nWe now de\ufb01ne\nour in\ufb01nite model by taking the limit of a series of \ufb01nite models.\nFig. 1.\nGraphical model.\n4\nD. KNOWLES AND Z. GHAHRAMANI\nStart with a \ufb01nite model.\nWe derive the distribution on Z by de\ufb01ning\na \ufb01nite K model and taking the limit as K \u2192\u221e. We then show how the\nin\ufb01nite case corresponds to a simple stochastic process.\nWe have D dimensions and K hidden sources. Recall that zdk of matrix Z\ntells us whether hidden source k contributes to dimension d. We assume that\nthe probability of a source k contributing to any dimension is \u03c0k, and that\nthe rows are generated independently. We \ufb01nd\nP(Z|\u03c0) =\nK\nY\nk=1\nD\nY\nd=1\nP(zdk|\u03c0k) =\nK\nY\nk=1\n\u03c0mk\nk (1 \u2212\u03c0k)D\u2212mk,\n(3)\nwhere mk = PD\nd=1 zdk is the number of dimensions to which source k con-\ntributes. The inner term of the product is a binomial distribution, so we\nchoose the conjugate Beta(r,s) distribution for \u03c0k. For now we take r = \u03b1\nK\nand s = 1, where \u03b1 is the strength parameter of the IBP. The model is\nde\ufb01ned by\n\u03c0k|\u03b1 \u223cBeta\n\u0012 \u03b1\nK ,1\n\u0013\n,\n(4)\nzdk|\u03c0k \u223cBernoulli(\u03c0k).\n(5)\nDue to the conjugacy between the binomial and beta distributions, we are\nable to integrate out \u03c0 to \ufb01nd\nP(Z) =\nK\nY\nk=1\n(\u03b1/K)\u0393(mk + \u03b1/K)\u0393(D \u2212mk + 1)\n\u0393(D + 1 + \u03b1/K)\n,\n(6)\nwhere \u0393(\u00b7) is the Gamma function.\nTake the in\ufb01nite limit.\nGri\ufb03ths and Ghahramani (2006) de\ufb01ne a scheme\nto order the nonzero rows of Z which allows us to take the limit K \u2192\u221e\nand \ufb01nd\nP(Z) =\n\u03b1K+\nQ\nh>0 Kh! exp(\u2212\u03b1HD)\nK+\nY\nk=1\n(D \u2212mk)!(mk \u22121)!\nN!\n,\n(7)\nwhere K+ is the number of active features (i.e., nonzero columns of Z),\nHD = PD\nj=1\n1\nj is the Dth harmonic number, and Kh is the number of rows\nwhose entries correspond to the binary number h.\nGo to an Indian Bu\ufb00et.\nThis distribution corresponds to a simple stochas-\ntic process, the Indian Bu\ufb00et Process. Consider a bu\ufb00et with a seemingly\nin\ufb01nite number of dishes (hidden sources) arranged in a line. The \ufb01rst cus-\ntomer (observed dimension) starts at the left and samples Poisson(\u03b1) dishes.\nNONPARAMETRIC BAYESIAN SPARSE FACTOR MODELS\n5\n(a)\n(b)\nFig. 2.\nDraws from the one parameter IBP for two di\ufb00erent values of \u03b1. (a) \u03b1 = 4.\n(b) \u03b1 = 8.\nThe ith customer moves from left to right sampling dishes with probability\nmk\ni\nwhere mk is the number of customers to have previously sampled dish k.\nHaving reached the end of the previously sampled dishes, he tries Poisson(\u03b1\ni )\nnew dishes. Figure 2 shows two draws from the IBP for two di\ufb00erent values\nof \u03b1.\nIf we apply the same ordering scheme to the matrix generated by this\nprocess as for the \ufb01nite model, we recover the correct exchangeable distri-\nbution. Since the distribution is exchangeable with respect to the customers,\nwe \ufb01nd by considering the last customer that\nP(zkt = 1|z\u2212kn) = mk,\u2212t\nD\n,\n(8)\nwhere mk,\u2212t = P\ns\u0338=t zks, which is used in sampling Z. By exchangeability\nand considering the \ufb01rst customer, the number of active sources for each\ndimension follows a Poisson(\u03b1) distribution, and the expected number of\nentries in Z is D\u03b1. We also see that the number of active features K+ =\nPD\nd=1 Poisson(\u03b1\nd ) = Poisson(\u03b1HD).\n3. Related work.\nThe Bayesian Factor Regression Model (BFRM) of\nWest et al. (2007) is closely related to the \ufb01nite version of our model. The\nkey di\ufb00erence is the use of a hierarchical sparsity prior. Each element of G\nhas prior of the form\ngdk \u223c(1 \u2212\u03c0dk)\u03b40(gdk) + \u03c0dkN(gdk;0,\u03bb\u22121\nk ).\n6\nD. KNOWLES AND Z. GHAHRAMANI\nThe \ufb01nite IBP model is equivalent to setting \u03c0dk = \u03c0k \u223cBeta(\u03b1/K,1) and\nthen integrating out \u03c0k. In BFRM a hierarchical prior is used:\n\u03c0dk \u223c(1 \u2212\u03c1k)\u03b40(\u03c0dk) + \u03c1k Beta(\u03c0dk;am,a(1 \u2212m)),\nwhere \u03c1k \u223cBeta(sr,s(1 \u2212r)). Nonzero elements of \u03c0dk are given a di\ufb00use\nprior favoring larger probabilities [a = 10, m = 0.75 are suggested in West\net al. (2007)], and \u03c1k is given a prior which strongly favors small values,\ncorresponding to a sparse solution (e.g., s = D, r = 5\nD).\nNote that on integrating out \u03c0dk, the prior on gdk is\ngdk \u223c(1 \u2212m\u03c1k)\u03b40(gdk) + m\u03c1kN(gdk;0,\u03bb\u22121\nk ).\nThis hierarchical sparsity prior is motivated by improved interpretability\nin terms of less uncertainty in the posterior as to whether an element of G\nis nonzero. However, this comes at a cost of signi\ufb01cantly increased compu-\ntation and reduced predictive performance, suggesting that the uncertainty\nremoved from the posterior was actually important.\nThe LASSO-based Sparse PCA (SPCA) method of Zou, Hastie and Tib-\nshirani (2004) and Witten, Tibshirani and Hastie (2009) has similar aims to\nour work in terms of providing a sparse variant of PCA to aid interpreta-\ntion of the results. However, since SPCA is not formulated as a generative\nmodel, it is not necessarily clear how to choose the regularization parameters\nor dimensionality without resorting to cross-validation. In our experimental\ncomparison to SPCA, we adjust the regularization constants such that each\ncomponent explains roughly the same proportion of the total variance as\nthe corresponding standard (nonsparse) principal component.\n4. Inference.\nGiven the observed data Y, we wish to infer the hidden\nsources X, which sources are active Z, the mixing matrix G, and all hyperpa-\nrameters. We use Gibbs sampling, but with Metropolis\u2013Hastings (MH) steps\nfor sampling new features. We draw samples from the marginal distribution\nof the model parameters given the data by successively sampling the condi-\ntional distributions of each parameter in turn, given all other parameters.\nSince we assume independent Gaussian noise, the likelihood function is\nP(Y|G,X,\u03c8) =\nN\nY\nn=1\n1\n(2\u03c0)D/2|\u03c8|1/2\n(9)\n\u00d7 exp\n\u0012\n\u22121\n2(yn \u2212Gxn)T \u03c8\u22121(yn \u2212Gxn)\n\u0013\n,\nwhere \u03c8 is a diagonal noise covariance matrix.\nNONPARAMETRIC BAYESIAN SPARSE FACTOR MODELS\n7\nNotation.\nWe use \u2212to denote the \u201crest\u201d of the model, that is, the values\nof all variables not explicitly conditioned upon in the current state of the\nMarkov chain. The rth row and cth column of matrix A are denoted Ar:\nand A:c respectively.\nMixture coe\ufb03cients.\nWe \ufb01rst derive a Gibbs sampling step for an in-\ndividual element of the IBP matrix, Zdk, determining whether factor k is\nactive for dimension d. Recall that \u03bbk is the precision (inverse covariance)\nof the factor loadings for the kth factor. The ratio of likelihoods can be cal-\nculated using equation (9). Integrating out the (d,k)th element of the factor\nloading matrix gdk [whose prior is given by equation (2)], we obtain\nP(Y|Zdk = 1,\u2212)\nP(Y|Zdk = 0,\u2212) =\nR\nP(Y|gdk,\u2212)N(gdk;0,\u03bb\u22121\nk )dgdk\nP(Y|gdk = 0,\u2212)\n(10)\n=\nr\n\u03bbk\n\u03bb exp\n\u00121\n2\u03bb\u00b52\n\u0013\n,\n(11)\nwhere we have de\ufb01ned \u03bb = \u03c8\u22121\nd XT\nk:Xk: + \u03bbk and \u00b5 = \u03c8\u22121\nd\n\u03bb XT\nk: \u02c6Ed: with the\nmatrix of residuals \u02c6E = Y \u2212GX evaluated with gdk = 0. The dominant\ncalculation is that for \u00b5 since the calculation for \u03bb can be cached. This\noperation is O(N) and must be calculated D \u00d7 K times, so sampling the\nIBP matrix, Z, and factor loading matrix, G, is order O(NDK).\nFrom the exchangeability of the IBP, we can imagine that dimension d\nwas the last to be observed, so that the ratio of the priors is\nP(Zdk = 1|\u2212)\nP(Zdk = 0|\u2212) =\nm\u2212d,k\nN \u22121 \u2212m\u2212d,k\n,\n(12)\nwhere m\u2212d,k is the number of dimensions for which factor k is active, ex-\ncluding the current dimension d. Multiplying equations (11) and (12) gives\nthe expression for the ratio of posterior probabilities for Zdk being 1 or 0,\nwhich is used for sampling. If Zdk is set to 1, we sample gdk|\u2212\u223cN(\u00b5,\u03bb\u22121)\nwith \u00b5,\u03bb de\ufb01ned as for equation (11).\nAdding new features.\nZ is a matrix with in\ufb01nitely many columns, but\nthe nonzero columns contribute to the likelihood and are held in memory.\nHowever, the zero columns still need to be taken into account since the\nnumber of active factors can change. Let \u03bad be the number of columns of Z\nwhich contain 1 only in row d, that is, the number of features which are\nactive only for dimension d. Note that due to the form of the prior for\nelements of Z given in equation (12), \u03bad = 0 for all d after a sampling sweep\nof Z. Figure 3 illustrates \u03bad for a sample Z matrix.\nNew features are proposed by sampling \u03bad with a MH step. It is possible\nto integrate out either the new elements of the mixing matrix, g (a 1 \u00d7 \u03bad\n8\nD. KNOWLES AND Z. GHAHRAMANI\nFig. 3.\nA diagram to illustrate the de\ufb01nition of \u03bad, for d = 10.\nvector), or the new rows of the latent feature matrix, X\u2032 (a \u03bad \u00d7 N matrix),\nbut not both. Since the latter generally has higher dimension, we choose to\nintegrate out X\u2032 and include gT as part of the proposal. Thus, the proposal\nis \u03be = {\u03bad,g}, and we propose a move \u03be \u2192\u03be\u2217with probability J(\u03be\u2217|\u03be). In\nthis case \u03be = \u2205since, as noted above, \u03bad = 0 initially. The simplest proposal,\nfollowing Meeds et al. (2006), would be to use the prior on \u03be\u2217, that is,\nJ(\u03be) = P(\u03bad|\u03b1) \u00b7 p(g|\u03bad,\u03bbk) = Poisson(\u03bad;\u03b3) \u00b7 N(g;0,\u03bb\u22121\nk ),\nwhere \u03b3 =\n\u03b1\nD\u22121.\nUnfortunately, the rate constant of the Poisson prior tends to be so small\nthat new features are very rarely proposed, resulting in slow mixing. To\nremedy this, we modify the proposal distribution for \u03bad and introduce two\ntunable parameters, \u03c0 and \u03bb:\nJ(\u03bad) = (1 \u2212\u03c0)Poisson(\u03bad;\u03bb\u03b3) + \u03c01(\u03bad = 1).\n(13)\nThus, the Poisson rate is multiplied by a factor \u03bb, and a spike at \u03bad = 1 is\nadded with mass \u03c0. The proposal is accepted with probability min(1,a\u03be\u2192\u03be\u2217)\nwhere\na\u03be\u2192\u03be\u2217= P(\u03be\u2217|\u2212,Y )J(\u03be|\u03be\u2217)\nP(\u03be|\u2212,Y )J(\u03be\u2217|\u03be)\n(14)\n= P(Y |\u03be\u2217,\u2212)P(\u03bad|\u03b1)p(g|\u03bad,\u03bbk)\nP(Y |\u2212)J(\u03bad)p(g|\u03bad,\u03bbk)\n= al \u00b7 ap,\nNONPARAMETRIC BAYESIAN SPARSE FACTOR MODELS\n9\nwhere\nal = P(Y |\u03be\u2217,\u2212)\nP(Y |\u2212) ,\n(15)\nap = P(\u03bad|\u03b1)\nJ(\u03bad)\n= Poisson(\u03bad;\u03b3)\nPoisson(\u03bad;\u03bb\u03b3).\n(16)\nNote that we take J(\u03be|\u03be\u2217) = 1 since \u03be = \u2205. To calculate the likelihood ra-\ntio, al, we need the collapsed likelihood under the new proposal:\nP(Yd:|\u03be\u2217,\u2212) =\nN\nY\nn=1\nZ\nP(Ydn|\u03be\u2217,x\u2032\nn,\u2212)P(x\u2032\nn)dx\u2032\n(17)\n=\nN\nY\nn=1\n(2\u03c0\u03c8\u22121\nd )\u22121/2(2\u03c0)\u03bad/2|M|\u22121/2\n(18)\n\u00d7 exp\n\u00121\n2(mT\nnMmn \u2212\u03c8\u22121\nd\n\u02c6E2\ndn)\n\u0013\n,\nwhere we have de\ufb01ned M = \u03c8\u22121\nd ggT +I\u03bad and mn = M\u22121\u03c8\u22121\nd g \u02c6Edn with the\nmatrix of residuals \u02c6E = Y \u2212GX. The likelihood under the current sample is\nP(Yd:|\u03be,\u2212) =\nN\nY\nn=1\n(2\u03c0\u03c8\u22121\nd )\u22121/2 exp\n\u0012\n\u22121\n2\u03c8\u22121\nd\n\u02c6E2\ndn\n\u0013\n.\n(19)\nSubstituting these likelihood terms into the expression for the ratio of like-\nlihood terms, al, gives\nal = (2\u03c0)N\u03bad/2|M|\u2212N/2 exp\n\u00121\n2\nX\nn\nmT\nnMmn\n\u0013\n.\n(20)\nWe found that appropriate scheduling of the sampler improved mixing,\nparticularly with respect to adding new features. The \ufb01nal scheme we settled\non is described in Algorithm 1.\nIBP parameters.\nWe can choose to sample the IBP strength parame-\nter \u03b1, with conjugate Gamma(e,f) prior (note that we use the inverse scale\nparameterization of the Gamma distribution). The conditional prior of equa-\ntion (7) acts as the likelihood term and the posterior update is as follows:\nP(\u03b1|Z) \u221dP(Z|\u03b1)P(\u03b1) = Gamma(\u03b1;K+ + e,f + HD),\n(21)\nwhere K+ is the number of active sources and HD = PD\nj=1\n1\nj is the Dth\nharmonic number.\nThe remaining sampling steps are standard, but are included here for\ncompleteness.\n10\nD. KNOWLES AND Z. GHAHRAMANI\nAlgorithm 1 One iteration of the NSFA sampler\nfor d = 1 to D do\nfor k = 1 to K do\nSample Zdk\nend for\nSample \u03bad\nend for\nfor n = 1 to N do\nSample X:n\nend for\nSample \u03b1,\u03c6,\u03bbg\nLatent variables.\nSampling the columns xn of the latent variable ma-\ntrix X for each t \u2208[1,...,N], we have\nP(xn|\u2212) \u221dP(yn|xn,\u2212)P(xn) = N(xn;\u00b5n,\u039b),\n(22)\nwhere we have de\ufb01ned \u039b = GT \u03c8\u22121G+I and \u00b5n = \u039b\u22121GT \u03c8\u22121yn. Note that\nsince \u039b does not depend on n, we only need to compute and invert it once\nper iteration. Calculating \u039b is order O(K2D), and inverting it is O(K3).\nCalculating \u00b5t is order O(KD) and must be calculated for all N xt\u2019s, a total\nof O(NKD). Thus, sampling X is order O(K2 + K3 + NKD).\nFactor precision.\nIf the mixture coe\ufb03cient prior precisions \u03bbk are con-\nstrained to be equal, we have \u03bbk = \u03bb \u223cGamma(c,d). The posterior update\nis then given by \u03bb|G \u223cGamma(c +\nP\nk mk\n2\n,d + P\nd,k G2\ndk).\nHowever, if the variances are allowed to be di\ufb00erent for each column\nof G, we set \u03bbk \u223cGamma(c,d), and the posterior update is given by \u03bbk|G \u223c\nGamma(c + mk\n2 ,d + P\nd G2\ndk). In this case we may also wish to share power\nacross factors, in which case we also sample d. Putting a Gamma prior on d\nsuch that d \u223cGamma(c0,d0), the posterior update is d|\u03bbk \u223cGamma(c0 +\ncK,d0 + PK\nk=1 \u03bbk).\nNoise variance.\nThe additive Gaussian noise can be constrained to be\nisotropic, in which case the inverse variance is given a Gamma prior: \u03c8\u22121\nd\n=\n\u03c8\u22121 \u223cGamma(a,b) which gives the posterior update \u03c8\u22121|\u2212\u223cGamma(a +\nND\n2 ,b + P\nd,n \u02c6E2\ndn).\nHowever, if the noise is only assumed to be independent (which we have\nfound to be more appropriate for gene expression data), then each dimen-\nsion has a separate noise variance, whose inverse is given a Gamma prior:\n\u03c8\u22121\nd\n\u223cGamma(a,b) which gives the posterior update \u03c8\u22121\nd |\u2212\u223cGamma(a +\nN\n2 ,b + P\nn E2\ndn) where the matrix of residuals \u02c6E = Y \u2212GX. We can share\nNONPARAMETRIC BAYESIAN SPARSE FACTOR MODELS\n11\npower between dimensions by giving the hyperparameter b a hyperprior\nGamma(a0,b0) resulting in the Gibbs update b|\u2212\u223cGamma(a0 + aD,b0 +\nPD\nd=1 \u03c8\u22121\nd ). This hierarchical prior results in soft coupling between the noise\nvariances in each dimension, so we will refer to this variant as sc.\n5. Results.\nWe compare the following models:\n\u2022 FA\u2014Bayesian Factor Analysis; see, for example, Kaufman and Press (1973)\nor Rowe and Press (1998).\n\u2022 AFA\u2014Factor Analysis with ARD prior to determine active sources.\n\u2022 FOK\u2014The sparse Factor Analysis method of Fokoue (2004), Fevotte and\nGodsill (2006) and Archambeau and Bach (2009).\n\u2022 SPCA\u2014The Sparse PCA method of Zou, Hastie and Tibshirani (2004).\n\u2022 BFRM\u2014Bayesian Factor Regression Model of West et al. (2007).\n\u2022 SFA\u2014Sparse Factor Analysis, using the \ufb01nite IBP.\n\u2022 NSFA\u2014The proposed model: Nonparametric Sparse Factor Analysis.\nNote that all of these models can be learned using the software package\nwe provide simply by using appropriate settings.\n5.1. Synthetic data.\nSince generating a connectivity matrix Z from the\nIBP itself would clearly bias toward our model, we instead use the D = 100\ngene by K = 16 factor E. Coli connectivity matrix derived in Kao et al.\n(2004) from RegulonDB and current literature. We ignore whether the con-\nnection is believed to be up or down regulation, resulting in a binary ma-\ntrix Z. We generate random data sets with N = 100 samples by drawing\nthe nonzero elements of G (corresponding to the elements of Z which are\nnonzero), and all elements of X, from a zero mean unit variance Gaussian,\ncalculating Y = GX + E, where E is Gaussian white noise with variance set\nto give a signal to noise ratio of 10.\nHere we will de\ufb01ne the reconstruction error Er as\nEr(G, \u02c6G) =\n1\nDK\nK\nX\nk=1\nmin\n\u02c6k\u2208{1,..., \u02c6\nK}\nD\nX\nd=1\n(Gdk \u2212Gd\u02c6k)2,\nwhere \u02c6G, \u02c6K are the inferred quantities. Although we minimize over permu-\ntations, we do not minimize over rotations since, as noted in Fokoue (2004),\nthe sparsity of the prior stops the solution being rotation invariant. We av-\nerage this error over the last ten samples of the MCMC run. This error\nfunction does not penalize inferring extra spurious factors, so we will inves-\ntigate this possibility separately. The precision and recall of active elements\nof the Z achieved by each algorithm (after thresholding for the nonsparse\nalgorithms) are presented in the Supplementary Material, but omitted here\nsince the results are consistent with the reconstruction error.\n12\nD. KNOWLES AND Z. GHAHRAMANI\nFig. 4.\nBoxplot of reconstruction errors for simulated data derived from the E. Coli con-\nnectivity matrix of Kao et al. (2004). Ten data sets were generated and the reconstruction\nerror calculated for the last ten samples from each algorithm. Numbers refer to the number\nof latent factors used, K. a1 denotes \ufb01xing \u03b1 = 1. sn denotes sharing power between noise\ndimensions.\nThe reconstruction error for each method with di\ufb00erent numbers of latent\nfeatures is shown in Figure 4. Ten random data sets were used and for the\nsampling methods (all but SPCA) the results were averaged over the last\nten samples out of 1000. Unsurprisingly, plain Factor Analysis (FA) performs\nthe worst, with increasing over\ufb01tting as the number of factors is increased.\nFor \u02c6K = 20 the variance is also very high, since the four spurious features \ufb01t\nnoise. Using an ARD prior on the features (AFA) improves the performance,\nand over\ufb01tting no longer occurs. The reconstruction error is actually less for\n\u02c6K = 20, but this is an artifact due to the reconstruction error not penalizing\nadditional spurious features in the inferred G. The Sparse PCA (SPCA) of\nZou, Hastie and Tibshirani (2004) shows improved reconstruction compared\nto the nonsparse methods (FA and AFA), but does not perform as well as the\nBayesian sparse models. Sparse factor analysis (SFA), the \ufb01nite version of\nthe full in\ufb01nite model, performs very well. The Bayesian Factor Regression\nModel (BFRM) performs signi\ufb01cantly better than the ARD factor analysis\n(AFA), but not as well as our sparse model (SFA). It is interesting that for\nBFRM the reconstruction error decreases signi\ufb01cantly with increasing \u02c6K,\nsuggesting that the default priors may actually encourage too much sparsity\nfor this data set. Fokoue\u2019s method (FOK) only performs marginally better\nthan AFA, suggesting that this \u201csoft\u201d sparsity scheme is not as e\ufb00ective at\n\ufb01nding the underlying sparsity in the data. Over\ufb01tting is also seen, with\nthe error increasing with \u02c6K. This could potentially be resolved by placing\nNONPARAMETRIC BAYESIAN SPARSE FACTOR MODELS\n13\nFig. 5.\nHistograms of the number of latent features inferred by the nonparametric sparse\nFA sampler for the last 100 samples out of 1000. Left: With \u03b1 = 1. Right: Inferring \u03b1.\nan appropriate per factor ARD-like prior over the scale parameters of the\nGamma distributions controlling the precision of elements of G. Finally, the\nNonparametric Sparse Factor Analysis (NSFA) proposed here and in Rai and\nDaum\u00b4e III (2008) performs very well. With \ufb01xed \u03b1 = 1 (a1) or inferring \u03b1,\nwe see very similar performance. Using the soft coupling (sc) variant which\nshares power between dimensions when \ufb01tting the noise variances seems to\nreduce the variance of the sampler, which is reasonable in this example since\nthe noise was in fact isotropic.\nSince the reconstruction error does not penalize spurious factors, it is im-\nportant to check that NSFA is not scoring well simply by inferring many\nadditional factors. Histograms for the number of latent features inferred for\nthe nonparametric sparse model are shown in Figure 5. This represents an\napproximate posterior over K. For \ufb01xed \u03b1 = 1 the distribution is centered\naround the true value of K = 16, with minimal bias (EK = 16.1). The vari-\nance is signi\ufb01cant (standard deviation of 1.46), but is reasonable considering\nthe noise level (SNR = 10) and that in some of the random data sets, ele-\nments of Z which are 1 could be masked by very small corresponding values\nof G. This hypothesis is supported by the results of a similar experiment\nwhere G was set equal to Z. In this case, the sampler always converged to\nat least 16 features, but would also sometimes infer spurious features from\nnoise (results not shown). When inferring \u03b1 some bias and skew are notice-\nable. The mean of the posterior is now at 18.3 with standard deviation 2.0,\nsuggesting there is little to gain from sampling \u03b1 in this data.\n5.2. Convergence.\nNSFA can su\ufb00er from slow convergence if the number\nof new features is drawn from the prior. Figure 6 shows how the di\ufb00erent\nproposals for \u03bad e\ufb00ect how quickly the sampler reaches a sensible number\nof features. If we use the prior as the proposal distribution, mixing is very\nslow, taking around 5000 iterations to converge, as shown in Figure 6(a). If\n14\nD. KNOWLES AND Z. GHAHRAMANI\n(a)\n(b)\n(c)\nFig. 6.\nThe e\ufb00ect of di\ufb00erent proposal distributions for the number of new features.\n(a) Prior. (b) Prior plus 0.1I(\u03ba = 1). (c) Factor \u03bb = 50.\na mass of 0.1 is added at \u03bad = 1 [see equation (13)], then the sampler reaches\nthe equilibrium number of features in around 1500 iterations, as shown in\nFigure 6(b). However, if we try to add features even faster, for example,\nby setting the factor \u03bb = 50 in equation (13), then the sampling noise is\ngreatly increased, as shown in Figure 6(c), and the computational cost also\nincreases signi\ufb01cantly because so many spurious features are proposed only\nto be rejected.\n5.3. Biological data: E. Coli time-series dataset.\nTo assess the perfor-\nmance of each algorithm on the biological data where no ground truth is\navailable, we calculated the test set log likelihood under the posterior. Ten\npercent of entries from Y were removed at random, ten times, to give ten\ndata sets for inference. We do not use mean square error as a measure of\npredictive performance because of the large variation in the signal to noise\nratio across gene expression level probes.\nThe test log likelihood achieved by the various algorithms on the E. Coli\ndata set from Kao et al. (2004), including 100 genes at 24 time-points, is\nshown in Figure 7(a). On this simple data set incorporating sparsity doesn\u2019t\nimprove predictive performance. Over\ufb01tting the number of latent factors\ndoes damage performance, although using the ARD or sparse prior allevi-\nates the problem. Based on predictive performance of the \ufb01nite models, \ufb01ve\nis a sensible number of features for this data set: the NSFA model infers\na median number of 4 features, with some probability of there being 5, as\nshown in Figure 7(b).\n5.4. Breast cancer data set.\nWe assess these algorithms in terms of pre-\ndictive performance on the breast cancer data set of West et al. (2007),\nincluding 226 genes across 251 individuals. We \ufb01nd that all the \ufb01nite mod-\nels are sensitive to the choice of the number of factors, K. The samplers were\nfound to have converged after around 1000 samples according to standard\nNONPARAMETRIC BAYESIAN SPARSE FACTOR MODELS\n15\n(a)\n(b)\nFig. 7.\nResults on E. Coli time-series data set from Kao et al. (2004) (N = 24, D = 100,\n3000 MCMC iterations). (a) Log likelihood of test data under each model based on the last\n100 MCMC samples. The boxplots show variation across 10 di\ufb00erent random splits of\nthe data into training and test sets. (b) Number of active latent features during a typical\nMCMC run of the NSFA model.\nmultiple chain convergence measures, so 3000 MCMC iterations were used\nfor all models. The predictive log likelihood was calculated using the \ufb01nal\n100 MCMC samples. Figure 8(a) shows test set log likelihoods for 10 ran-\ndom divisions of the data into training and test sets. Factor analysis (FA)\nshows signi\ufb01cant over\ufb01tting as the number of latent features is increased\nfrom 20 to 40. Using the ARD prior prevents this over\ufb01tting (AFA), giving\nimproved performance when using 20 features and only slightly reduced per-\nformance when 40 features are used. The sparse \ufb01nite model (SFA) shows\nan advantage over AFA in terms of predictive performance as long as un-\nder\ufb01tting does not occur: performance is comparable when using only 10\nfeatures. However, the performance of SFA is sensitive to the choice of the\nnumber of factors, K. The performance of the sparse nonparametric model\n(NSFA) is comparable to the sparse \ufb01nite model when an appropriate num-\nber of features is chosen, but avoids the time consuming model selection\nprocess. Fokoue\u2019s method (FOK) was run with K = 20 and various settings\nof the hyperparameter d which controls the overall sparsity of the solution.\nThe model\u2019s predictive performance depends strongly on the setting of this\nparameter, with results approaching the performance of the sparse models\n(SFA and NSFA) for d = 10\u22124. The performance of BFRM on this data set\nis noticeably worse than the other sparse models.\nWe now consider the computation cost of the algorithms. As described in\nSection 4, sampling Z and G takes order O(NKD) operations per iteration,\nand sampling X takes O(K2 +K3 +ND). However, for the moderate values\nencountered for data sets 1 and 2, the main computational cost is sampling\n16\nD. KNOWLES AND Z. GHAHRAMANI\n(a)\n(b)\n(c)\nFig. 8.\nResults on breast cancer data set (N = 251, D = 226, 3000 MCMC iterations).\n(a) Predictive performance: log likelihood of test (the 10% missing) data under each model\nbased on the last 100 MCMC samples. Higher values indicate better performance. The\nboxplots show variation across 10 di\ufb00erent random splits of the data into training and test\nsets. (b) CPU time (in seconds) per iteration, averaged across the 3000 iteration run. (c)\nCPU time (in seconds) per iteration divided by the number of features at that iteration,\naveraged across all iterations.\nthe nonzero elements of G, which takes O((1\u2212s)DK) where s is the sparsity\nof the model. Figure 8(c) shows the mean CPU time per iteration divided\nby the number of features at that iteration. Naturally, straight FA is the\nfastest, taking only around 0.025s per iteration per feature. The value in-\nNONPARAMETRIC BAYESIAN SPARSE FACTOR MODELS\n17\ncreases slightly with increasing K, suggesting that here the O(K2D + K3)\ncalculation and inversion of \u03bb, the precision of the conditional on X, must be\ncontributing. The computational cost of adding the ARD prior is negligible\n(AFA). The CPU time per iteration is just over double for the sparse \ufb01nite\nmodel (SFA), but the cost actually decreases with increasing K, because\nthe sparsity of the solution increases to avoid over\ufb01tting. There are fewer\nnonzero elements of G to sample per feature, so the CPU time per feature\ndecreases. The CPU time per iteration per feature for the nonparametric\nsparse model (NSFA) is somewhat higher than for the \ufb01nite model because\nof the cost of the feature birth and death process. However, Figure 8(b)\nshows the absolute CPU time per iteration, where we see that the nonpara-\nmetric model is only marginally more expensive than the \ufb01nite model of\nappropriate size \u02c6K = 15 and cheaper than choosing an unnecessarily large\n\ufb01nite model (SFA with K = 20, 40). Fokoue\u2019s method (FOK) has compara-\nble computational performance to the sparse \ufb01nite model, but interestingly\nhas increased cost for the optimal setting of d = 10\u22124. The parameter space\nfor FOK is continuous, making search easier but requiring a normal random\nvariable for every element of G. BFRM pays a considerable computational\ncost for both the hierarchical sparsity prior and the DP prior on X. SPCA\nwas not run on this data set, but results on the synthetic data in Section 5.1\nsuggest it is somewhat faster than the sampling methods, but not hugely\nso. The computational cost of SPCA is ND2 + mO(D2K + DK2 + D3) in\nthe N > D case (where m is the number of iterations to convergence) and\nND2 + mO(D2K + DK2) in the D > N case, taking the limit \u03bb \u2192\u221e. In\neither case an individual iteration of SPCA is more expensive than one sam-\npling iteration of NSFA (since K < D), but fewer iterations will generally be\nrequired to reach convergence of SPCA than are required to ensure mixing\nof NSFA.\n5.5. Prostate cancer data set.\nFigure 9 shows the predictive performance\nof AFA, FOK and NSFA on the prostate cancer data set of Yu et al. (2004),\nfor ten random splits into training and test data. The boxplots show varia-\ntion from ten random splits into training and test data. The large number\nof genes (D = 12557 across N = 171 individuals) in this data set makes in-\nference slower, but the problem is manageable since the computational com-\nplexity is linear in the number of genes. Despite the large number of genes,\nthe appropriate number of latent factors, in terms of maximizing predictive\nperformance, is still small, around 10 (NSFA infers a median of 12 factors).\nThis may seem small relative to the number of genes, but it should be noted\nthat the genes included in the breast cancer and E. Coli data sets are those\ncapturing the most variability. Surprisingly, SFA actually performed slightly\nworse on this data set than AFA. Both are highly sensitive to the number of\nlatent factors chosen. NSFA, however, gives better predictive log likelihoods\n18\nD. KNOWLES AND Z. GHAHRAMANI\nFig. 9.\nTest set log likelihoods on Prostate cancer data set from Yu et al. (2004), including\n12557 genes across 171 individuals (1000 MCMC iterations).\nthan either \ufb01nite model for any \ufb01xed number of latent factors K. Running\n1000 iterations of NSFA on this data set takes under 8 hours. BFRM and\nFOK were impractically slow to run on this data set.\n6. Discussion.\nWe have seen that in both the E. Coli and breast can-\ncer data sets that sparsity can improve predictive performance, as well as\nproviding a more easily interpretable solution. Using the IBP to provide\nsparsity is straightforward, and allows the number of latent factors to be\ninferred within a well-de\ufb01ned theoretical framework. This has several ad-\nvantages over manually choosing the number of latent factors. Choosing too\nfew latent factors damages predictive performance, as seen for the breast\ncancer data set. Although choosing too many latent factors can be com-\npensated for by using appropriate ARD-like priors, we \ufb01nd this is typically\nmore computationally expensive than the birth and death process of the\nIBP. Manual model selection is an alternative but is time consuming. Fi-\nnally, we show that running NSFA on full gene expression data sets with\n10000+ genes is feasible so long as the number of latent factors remains rel-\natively small. An interesting direction for this research is how to incorporate\nprior knowledge, for example, if certain transcription factors are known to\nregulate speci\ufb01c genes. Incorporating this knowledge could both improve the\nperformance of the model and improve interpretability by associating latent\nvariables with speci\ufb01c transcription factors. Another possibility is incorpo-\nrating correlations in the Indian Bu\ufb00et Process, which has been proposed\nfor simpler models [Doshi-Velez and Ghahramani (2009); Courville, Eck and\nBengio (2009)]. This would be appropriate in a gene expression setting where\nNONPARAMETRIC BAYESIAN SPARSE FACTOR MODELS\n19\nmultiple transcription factors might be expected to share sets of regulated\ngenes due to common motifs. Unfortunately, performing MCMC in all but\nthe simplest of these models su\ufb00ers from slow mixing.\nAcknowledgments.\nWe would like to thank the anonymous reviewers for\nhelpful comments.\nSUPPLEMENTARY MATERIAL\nGraphs of precision and recall for the synthetic data experiment. (DOI:\n10.1214/10-AOAS435SUPP; .pdf). The precision and recall of active ele-\nments of the Z matrix achieved by each algorithm (after thresholding for\nthe nonsparse algorithms) on the synthetic data experiment, described in\nSection 5.1. The results are consistent with the reconstruction error.\nREFERENCES\nArchambeau, C. and Bach, F. (2009). Sparse probabilistic projections. In Proceedings\nof the Conference on Neural Information Processing Systems (NIPS) (D. Koller,\nD. Schuurmans, Y. Bengio and L. Bottou, eds.) 73\u201380. MIT Press, Cambridge,\nMA.\nCourville, A. C., Eck, D. and Bengio, Y. (2009). An in\ufb01nite factor model hierarchy\nvia a noisy-or mechanism. In Advances in Neural Information Processing Systems 21.\nMIT Press, Cambridge, MA.\nDoshi-Velez, F. and Ghahramani, Z. (2009). Correlated non-parametric latent feature\nmodels. In Proceedings of the Twenty-Fifth Conference on Uncertainty in Arti\ufb01cial\nIntelligence 143\u2013150. AUAI Press, Arlington, VA.\nFevotte, C. and Godsill, S. J. (2006). A Bayesian approach for blind separation of\nsparse sources. IEEE Transactions on Audio, Speech, and Language Processing 14 2174\u2013\n2188.\nFokoue, E. (2004). Stochastic determination of the intrinsic structure in Bayesian fac-\ntor analysis. Technical Report No. 17, Statistical and Applied Mathematical Sciences\nInstitute.\nGriffiths, T. L. and Ghahramani, Z. (2006). In\ufb01nite latent feature models and the\nIndian Bu\ufb00et Process. In Advances in Neural Information Processing Systems 18. MIT\nPress, Cambridge, MA.\nKao, K. C., Yang, Y.-L., Boscolo, R., Sabatti, C., Roychowdhury, V. and\nLiao, J. C. (2004). Transcriptome-based determination of multiple transcription regu-\nlator activities in Escherichia coli by using network component analysis. In Proceedings\nof the National Academy of Sciences of the United States of America (PNAS) 101 641\u2013\n646. Natl. Acad. Sci., Washington, DC.\nKaufman, G. M. and Press, S. J. (1973). Bayesian factor analysis. Technical Report\nNo. 662-73, Sloan School of Management, Univ. Chicago.\nKnowles, D. and Ghahramani, Z. (2007). In\ufb01nite sparse factor analysis and in\ufb01nite in-\ndependent components analysis. In 7th International Conference on Independent Com-\nponent Analysis and Signal Separation 381\u2013388. Springer, Berlin.\nMeeds, E., Ghahramani, Z., Neal, R. and Roweis, S. (2006). Modeling dyadic data\nwith binary latent factors. In Neural Information Processing Systems 19. MIT Press,\nCambridge, MA.\n20\nD. KNOWLES AND Z. GHAHRAMANI\nRai, P. and Daum\u00b4e III, H. (2008). The in\ufb01nite hierarchical factor regression model. In\nNeural Information Processing Systems. MIT Press, Cambridge, MA.\nRowe, D. B. and Press, S. J. (1998). Gibbs sampling and hill climbing in Bayesian\nfactor analysis. Technical Report No. 255, Dept. Statistics, Univ. California, Riverside.\nWest, M., Chang, J., Lucas, J., Nevins, J. R., Wang, Q. and Carvalho, C. (2007).\nHigh-dimensional sparse factor modelling: Applications in gene expression genomics.\nTechnical report, ISDS, Duke Univ.\nWitten, D. M., Tibshirani, R. and Hastie, T. (2009). A penalized matrix decom-\nposition, with applications to sparse principal components and canonical correlation\nanalysis. Biostatistics 10 515\u2013534.\nYu, Y. P., Landsittel, D., Jing, L., Nelson, J., Ren, B., Liu, L., McDonald, C.,\nThomas, R., Dhir, R., Finkelstein, S., Michalopoulos, G., Becich, M. and\nLuo, J.-H. (2004). Gene expression alterations in prostate cancer predicting tumor\naggression and preceding development of malignancy. Journal of Clinical Oncology 22\n2790\u20132799.\nZhang, Z., Chan, K. L., Kwok, J. T. and yan Yeung, D. (2004). Bayesian inference\non principal component analysis using reversible jump Markov Chain Monte Carlo.\nIn Proceedings of the 19th National Conference on Arti\ufb01cial Intelligence, San Jose,\nCalifornia, USA 372\u2013377. AAAI Press.\nZou, H., Hastie, T. and Tibshirani, R. (2004). Sparse principal component analysis.\nJ. Comput. Graph. Statist. 15 2006.\nEngineering Department\nCambridge University\nTrumpington Street\nCambridge, CB2 1PZ\nUnited Kingdom\nE-mail: dak33@cam.ac.uk\nzoubin@eng.cam.ac.uk\n",
        "sentence": " Following [22], the feature matrix is composed of a binary activation matrix, A \u2208 {0, 1}K\u00d7D, and a weighting matrix, W \u2208 FK\u00d7D, where the relation is given by the Hadamard product, , \u03b1a \u223c Ga\u03b1a ( h (1) \u03b1A , h (2) \u03b1A ) and \u03b2a \u223c Ga\u03b2a ( h (1) \u03b2A , h (2) \u03b2A ) [22]. In the second step, K new features are proposed in a Metropolis step [10, 22]. Since the IBP tends to mix slowly, we augment this ratio with probability P of accepting a single new feature, resulting in a modified acceptance ratio which is derived in [22]. The hyperparameters \u03b1a and \u03b2a are sampled as described in [22].",
        "context": "5.1. Synthetic data.\nSince generating a connectivity matrix Z from the\nIBP itself would clearly bias toward our model, we instead use the D = 100\ngene by K = 16 factor E. Coli connectivity matrix derived in Kao et al.\nand considering the \ufb01rst customer, the number of active sources for each\ndimension follows a Poisson(\u03b1) distribution, and the expected number of\nentries in Z is D\u03b1. We also see that the number of active features K+ =\nPD\nd=1 Poisson(\u03b1\nd ) = Poisson(\u03b1HD).\nFrom the exchangeability of the IBP, we can imagine that dimension d\nwas the last to be observed, so that the ratio of the priors is\nP(Zdk = 1|\u2212)\nP(Zdk = 0|\u2212) =\nm\u2212d,k\nN \u22121 \u2212m\u2212d,k\n,\n(12)"
    },
    {
        "title": "Algorithms for non-negative matrix factorization",
        "author": [
            "D.D. Lee",
            "H.S. Seung"
        ],
        "venue": "In Advances in Neural Information Processing Systems",
        "citeRegEx": "23",
        "shortCiteRegEx": "23",
        "year": 2001,
        "abstract": "",
        "full_text": "",
        "sentence": " A simple approach to the considered problem would be the use of a feature extraction technique such as Principal Component Analysis (PCA) [20] or Non-negative Matrix Factorization (NMF) [23] to learn features from the observed states. 1 Feature model for learning from demonstrations We assume a linear latent feature model, similar to NMF [23] and PCA [20].",
        "context": null
    },
    {
        "title": "Driver behavior and situation aware brake assistance for intelligent vehicles",
        "author": [
            "J.C. McCall",
            "M.M. Trivedi"
        ],
        "venue": "Proceedings of the IEEE,",
        "citeRegEx": "24",
        "shortCiteRegEx": "24",
        "year": 2007,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " We consider the problem of analyzing a driver\u2019s behavior, which is an important task for useradaptive driver assistance systems [24, 47], in order to demonstrate the performance of the proposed model in a real-world scenario.",
        "context": null
    },
    {
        "title": "Finite Mixture Models",
        "author": [
            "G. McLachlan",
            "D. Peel"
        ],
        "venue": null,
        "citeRegEx": "25",
        "shortCiteRegEx": "25",
        "year": 2000,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " , Nz, for each observation, indicating from which policy the observed action, un, has been generated [25].",
        "context": null
    },
    {
        "title": "Modeling dyadic data with binary latent factors",
        "author": [
            "E. Meeds",
            "Z. Ghahramani",
            "R.M. Neal",
            "S.T. Roweis"
        ],
        "venue": "In Advances in Neural Information Processing Systems",
        "citeRegEx": "26",
        "shortCiteRegEx": "26",
        "year": 2006,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " As the proposal distribution is independent of the previous sample, the acceptance ratio, r, is equal to the likelihood ratio between the new and existing features [26],",
        "context": null
    },
    {
        "title": "Bayesian nonparametric inverse reinforcement learning",
        "author": [
            "B. Michini",
            "J.P. How"
        ],
        "venue": "In Proceedings of the European Conference on Machine Learning and Knowledge Discovery in Databases,",
        "citeRegEx": "27",
        "shortCiteRegEx": "27",
        "year": 2012,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " Different results on Bayesian nonparametrics for IRL, which is indirectly related to feature learning, are given in [27], where a partitioning of the state space is sought for, or [45], where complex behavior is decomposed into several, simpler behaviors that can be easily learned. One possibility to infer this number is to utilize a Chinese Restaurant Process (CRP), giving rise to a Bayesian nonparametric model [27].",
        "context": null
    },
    {
        "title": "Bayesian variable selection in linear regression",
        "author": [
            "T.J. Mitchell",
            "J.J. Beauchamp"
        ],
        "venue": "Journal of the American Statistical Association,",
        "citeRegEx": "28",
        "shortCiteRegEx": "28",
        "year": 1988,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " In particular, we consider a sparsity-promoting mixture prior on the substates, similar to a spike and slab model [28, 19].",
        "context": null
    },
    {
        "title": "Humanlevel control through deep reinforcement learning",
        "author": [
            "V. Mnih",
            "K. Kavukcuoglu",
            "D. Silver",
            "A.A. Rusu",
            "J. Veness",
            "M.G. Bellemare",
            "A. Graves",
            "M. Riedmiller",
            "A.K. Fidjeland",
            "G. Ostrovski",
            "S. Petersen",
            "C. Beattie",
            "A. Sadik",
            "I. Antonoglou",
            "H. King",
            "D. Kumaran",
            "D. Wierstra",
            "S. Legg",
            "D. Hassabis"
        ],
        "venue": "Nature, 518(7540):529\u2013533,",
        "citeRegEx": "29",
        "shortCiteRegEx": "29",
        "year": 2015,
        "abstract": "",
        "full_text": "",
        "sentence": " In [29], this approach has been extended by replacing the neural network by a deep-layered counterpart.",
        "context": null
    },
    {
        "title": "Algorithms for inverse reinforcement learning",
        "author": [
            "A.Y. Ng",
            "S.J. Russell"
        ],
        "venue": "In Proceedings of the 17th International Conference on Machine Learning,",
        "citeRegEx": "30",
        "shortCiteRegEx": "30",
        "year": 2000,
        "abstract": "",
        "full_text": "",
        "sentence": " In reward-based models, it is assumed that the agent makes its decision based on a reward which is, in the context of LFD, learned from observations (as in Inverse Reinforcement Learning (IRL) [30]). 2 Inverse Reinforcement Learning IRL is concerned with the problem of learning the reward function from observed behavior [30]. , in [30, 17].",
        "context": null
    },
    {
        "title": "Online feature selection for model-based reinforcement learning",
        "author": [
            "T. Nguyen",
            "Z. Li",
            "T. Silander",
            "T.Y. Leong"
        ],
        "venue": "In Proceedings of the 30th International Conference on Machine Learning,",
        "citeRegEx": "31",
        "shortCiteRegEx": "31",
        "year": 2013,
        "abstract": "",
        "full_text": "",
        "sentence": " This approach has been extended in [31] to an online approach, where the features are selected from a large set by means of Group LASSO.",
        "context": null
    },
    {
        "title": "Policy gradient approaches for multi-objective sequential decision making",
        "author": [
            "S. Parisi",
            "M. Pirotta",
            "N. Smacchia",
            "L. Bascetta",
            "M. Restelli"
        ],
        "venue": "In Proceedings of the International Joint Conference on Neural Networks,",
        "citeRegEx": "32",
        "shortCiteRegEx": "32",
        "year": 2014,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " Mixture polices have been investigated in multiobjective problems, where an agent aims at reaching several objectives, some of which can even be conflicting [32, 46, 41].",
        "context": null
    },
    {
        "title": "Efficient training of artificial neural networks for autonomous navigation",
        "author": [
            "D.A. Pomerleau"
        ],
        "venue": "Neural Computation,",
        "citeRegEx": "33",
        "shortCiteRegEx": "33",
        "year": 1991,
        "abstract": " The ALVINN (Autonomous Land Vehicle In a Neural Network) project addresses the problem of training artificial neural networks in real time to perform difficult perception tasks. ALVINN is a backpropagation network designed to drive the CMU Navlab, a modified Chevy van. This paper describes the training techniques that allow ALVINN to learn in under 5 minutes to autonomously control the Navlab by watching the reactions of a human driver. Using these techniques, ALVINN has been trained to drive in a variety of circumstances including single-lane paved and unpaved roads, and multilane lined and unlined roads, at speeds of up to 20 miles per hour. ",
        "full_text": "",
        "sentence": " [33, 5].",
        "context": null
    },
    {
        "title": "Markov Decision Processes: Discrete Stochastic Dynamic Programming",
        "author": [
            "M.L. Puterman"
        ],
        "venue": null,
        "citeRegEx": "34",
        "shortCiteRegEx": "34",
        "year": 1994,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " In case of single agents, it has been shown that deterministic policies are optimal solutions for MDPs [34].",
        "context": null
    },
    {
        "title": "Beam search based map estimates for the Indian buffet process",
        "author": [
            "P. Rai",
            "H. Daume"
        ],
        "venue": "In Proceedings of the 28th International Conference on Machine Learning,",
        "citeRegEx": "35",
        "shortCiteRegEx": "35",
        "year": 2011,
        "abstract": "",
        "full_text": "",
        "sentence": " , estimates of the features, F\u0302MAP [35], and the policies, \u03a6\u0302MAP.",
        "context": null
    },
    {
        "title": "Boosting structured prediction for imitation learning",
        "author": [
            "N. Ratliff",
            "D. Bradley",
            "J.A. Bagnell",
            "J. Chestnutt"
        ],
        "venue": "In Advances in Neural Information Processing Systems",
        "citeRegEx": "36",
        "shortCiteRegEx": "36",
        "year": 2007,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " Attempts to introduce new features are made in [36] as an extension of the maximum margin planning algorithm which is proposed in [37].",
        "context": null
    },
    {
        "title": "Maximum margin planning",
        "author": [
            "N.D. Ratliff",
            "J.A. Bagnell",
            "M.A. Zinkevich"
        ],
        "venue": "In Proceedings of the 23rd International Conference on Machine Learning,",
        "citeRegEx": "37",
        "shortCiteRegEx": "37",
        "year": 2006,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " , in [37, 39]. Attempts to introduce new features are made in [36] as an extension of the maximum margin planning algorithm which is proposed in [37].",
        "context": null
    },
    {
        "title": "Neural fitted Q iteration \u2013 first experiences with a data efficient neural reinforcement learning method",
        "author": [
            "M. Riedmiller"
        ],
        "venue": "In Proceedings of the 16th European Conference on Machine Learning,",
        "citeRegEx": "38",
        "shortCiteRegEx": "38",
        "year": 2005,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " Riedmiller [38] has proposed the Q-fitted value iteration where the value function is approximated by means of a neural network, where the features are learned in the layers of the network.",
        "context": null
    },
    {
        "title": "A reduction of imitation learning and structured prediction to no-regret online learning",
        "author": [
            "S. Ross",
            "G.J. Gordon",
            "D. Bagnell"
        ],
        "venue": "In Proceedings of the 14th International Conference on Artificial Intelligence and Statistics,",
        "citeRegEx": "39",
        "shortCiteRegEx": "39",
        "year": 2011,
        "abstract": "",
        "full_text": "",
        "sentence": " , in [37, 39].",
        "context": null
    },
    {
        "title": "An MDP-based recommender system",
        "author": [
            "G. Shani",
            "D. Heckerman",
            "R.I. Brafman"
        ],
        "venue": "Journal of Machine Learning Research,",
        "citeRegEx": "40",
        "shortCiteRegEx": "40",
        "year": 2005,
        "abstract": "",
        "full_text": "",
        "sentence": " Decision-making plays a crucial role in many applications, such as robot learning, driver assistance systems, and recommender systems [40].",
        "context": null
    },
    {
        "title": "Importance sampling for reinforcement learning with multiple objectives",
        "author": [
            "C.R. Shelton"
        ],
        "venue": "PhD thesis, Massachusetts Institute of Technology,",
        "citeRegEx": "41",
        "shortCiteRegEx": "41",
        "year": 2001,
        "abstract": "",
        "full_text": "",
        "sentence": " Mixture polices have been investigated in multiobjective problems, where an agent aims at reaching several objectives, some of which can even be conflicting [32, 46, 41].",
        "context": null
    },
    {
        "title": "The optimal control of partially observable Markov processes over a finite horizon",
        "author": [
            "R.D. Smallwood",
            "E.J. Sondik"
        ],
        "venue": "Operations Research,",
        "citeRegEx": "42",
        "shortCiteRegEx": "42",
        "year": 1973,
        "abstract": " This paper formulates the optimal control problem for a class of mathematical models in which the system to be controlled is characterized by a finite-state discrete-time Markov process. The states of this internal process are not directly observable by the controller; rather, he has available a set of observable outputs that are only probabilistically related to the internal state of the system. The formulation is illustrated by a simple machine-maintenance example, and other specific application areas are also discussed. The paper demonstrates that, if there are only a finite number of control intervals remaining, then the optimal payoff function is a piecewise-linear, convex function of the current state probabilities of the internal Markov process. In addition, an algorithm for utilizing this property to calculate the optimal control policy and payoff function for any finite horizon is outlined. These results are illustrated by a numerical example for the machine-maintenance problem. ",
        "full_text": "",
        "sentence": " Since we consider a decisionmaking task, the investigated problem can be modeled by means of a Partially Observable Markov Decision Process (POMDP) [21, 42], which is defined by \u2022 a set of observations, Z, \u2022 a set of states, X , \u2022 a finite set of Nu actions, U , \u2022 a transition model, which describes the probability of entering a state after taking an action in the current state, \u2022 an observation model which explains how the observations are generated from the states, \u2022 a discount factor, which penalizes long-term rewards, \u2022 and a reward function, R.",
        "context": null
    },
    {
        "title": "A Bayesian approach to policy recognition and state representation learning",
        "author": [
            "A. \u0160o\u0161i\u0107",
            "A.M. Zoubir",
            "H. Koeppl"
        ],
        "venue": null,
        "citeRegEx": "43",
        "shortCiteRegEx": "43",
        "year": 2016,
        "abstract": "Learning from demonstration (LfD) is the process of building behavioral\nmodels of a task from demonstrations provided by an expert. These models can be\nused e.g. for system control by generalizing the expert demonstrations to\npreviously unencountered situations. Most LfD methods, however, make strong\nassumptions about the expert behavior, e.g. they assume the existence of a\ndeterministic optimal ground truth policy or require direct monitoring of the\nexpert's controls, which limits their practical use as part of a general system\nidentification framework. In this work, we consider the LfD problem in a more\ngeneral setting where we allow for arbitrary stochastic expert policies,\nwithout reasoning about the optimality of the demonstrations. Following a\nBayesian methodology, we model the full posterior distribution of possible\nexpert controllers that explain the provided demonstration data. Moreover, we\nshow that our methodology can be applied in a nonparametric context to infer\nthe complexity of the state representation used by the expert, and to learn\ntask-appropriate partitionings of the system state space.",
        "full_text": "1\nA Bayesian Approach to Policy Recognition\nand State Representation Learning\nAdrian \u02c7So\u02c7si\u00b4c, Abdelhak M. Zoubir and Heinz Koeppl\nAbstract\u2014Learning from demonstration (LfD) is the process of building behavioral models of a task from demonstrations provided by\nan expert. These models can be used e.g. for system control by generalizing the expert demonstrations to previously unencountered\nsituations. Most LfD methods, however, make strong assumptions about the expert behavior, e.g. they assume the existence of a\ndeterministic optimal ground truth policy or require direct monitoring of the expert\u2019s controls, which limits their practical use as part of a\ngeneral system identi\ufb01cation framework. In this work, we consider the LfD problem in a more general setting where we allow for\narbitrary stochastic expert policies, without reasoning about the optimality of the demonstrations. Following a Bayesian methodology,\nwe model the full posterior distribution of possible expert controllers that explain the provided demonstration data. Moreover, we show\nthat our methodology can be applied in a nonparametric context to infer the complexity of the state representation used by the expert,\nand to learn task-appropriate partitionings of the system state space.\nIndex Terms\u2014learning from demonstration, policy recognition, imitation learning, Bayesian nonparametric modeling, Markov chain\nMonte Carlo, Gibbs sampling, distance dependent Chinese restaurant process\n!\n1\nINTRODUCTION\nL\nEARNING FROM DEMONSTRATION (LfD) has become a\nviable alternative to classical reinforcement learning as\na new data-driven learning paradigm for building behav-\nioral models based on demonstration data. By exploiting\nthe domain knowledge provided by an expert demonstrator,\nLfD-built models can focus on the relevant parts of a sys-\ntem\u2019s state space [1] and hence avoid the need of tedious ex-\nploration steps performed by reinforcement learners, which\noften require an impractically high number of interactions\nwith the system environment [2] and always come with the\nrisk of letting the system run into undesired or unsafe states\n[3]. In addition to that, LfD-built models have been shown\nto outperform the expert in several experiments [4], [5], [6].\nHowever, most existing LfD methods come with strong\nrequirements that limit their practical use in real-world sce-\nnarios. In particular, they often require direct monitoring of\nthe expert\u2019s controls (e.g. [5], [7], [8]) which is possible only\nunder laboratory-like conditions, or they need to interact\nwith the target system via a simulator, if not by controlling\nthe system directly (e.g. [9]). Moreover, many methods are\nrestricted to problems with \ufb01nite state spaces (e.g. [10]), or\nthey compute only point estimates of the relevant system\nparameters without providing any information about their\nlevel of con\ufb01dence (e.g. [9], [11], [12]). Last but not least, the\nexpert is typically assumed to follow an optimal determin-\nistic policy (e.g. [13]) or to at least approximate one, based\non some presupposed degree of con\ufb01dence in the optimal\n\u2022\nAdrian \u02c7So\u02c7si\u00b4c is a member of the Signal Processing Group and an associate\nmember of the Bioinspired Communication Systems Lab, Technische Uni-\nversit\u00a8at Darmstadt, Germany. E-mail: adrian.sosic@spg.tu-darmstadt.de\n\u2022\nAbdelhak M. Zoubir is the head of the Signal Processing Group, Technische\nUniversit\u00a8at Darmstadt, Germany. E-mail: zoubir@spg.tu-darmstadt.de\n\u2022\nHeinz Koeppl is the head of the Bioinspired Communication Systems Lab\nand a member of the Centre for Cognitive Science, Technische Universit\u00a8at\nDarmstadt, Germany. E-mail: heinz.koeppl@bcs.tu-darmstadt.de\nbehavior (e.g. [14]). While such an assumption may be\nreasonable in some situations (e.g. for problems in robotics\ninvolving a human demonstrator [1]), it is not appropriate\nin many others, such as in multi-agent environments, where\nan optimal deterministic policy often does not exist [15].\nIn fact, there are many situations in which the assumption\nof a deterministic expert behavior is violated. In a more\ngeneral system identi\ufb01cation setting, our goal could be, for\ninstance, to detect the deviation of an agent\u2019s policy from\nits known nominal behavior, e.g. for the purpose of fault or\nfraud detection (note that the term \u201cexpert\u201d is slightly mis-\nleading in this context). Also, there are situations in which\nwe might not want to reason about the optimality of the\ndemonstrations; for instance, when studying the exploration\nstrategy of an agent who tries to model its environment (or\nthe reactions of other agents [16]) by randomly triggering\ndifferent events. In all these cases, existing LfD methods\ncan at best approximate the behavior of the expert as they\npresuppose the existence of some underlying deterministic\nground truth policy.\nIn this work, we present a novel approach to LfD in order\nto address the above-mentioned shortcomings of existing\nmethods. Central to our work is the problem of policy\nrecognition, that is, extracting the (possibly stochastic and\nnon-optimal) policy of a system from observations of its\nbehavior. Taking a general system identi\ufb01cation view on the\nproblem, our goal is herein to make as few assumptions\nabout the expert behavior as possible. In particular, we con-\nsider the whole class of stochastic expert policies, without\never reasoning about the optimality of the demonstrations.\nAs a result of this, our hypothesis space is not restricted to a\ncertain class of ground truth policies, such as deterministic\nor softmax policies (c.f. [14]). This is in contrast to inverse\nreinforcement learning approaches (see Section 1.2), which\ninterpret the observed demonstrations as the result of some\npreceding planing procedure conducted by the expert which\narXiv:1605.01278v4  [stat.ML]  4 Aug 2017\n2\nthey try to invert. In the above-mentioned case of fault\ndetection, for example, such an inversion attempt will gen-\nerally fail since the demonstrated behavior can be arbitrarily\nfar from optimal, which renders an explanation of the data\nin terms of a simple reward function impossible.\nAnother advantage of our problem formulation is that\nthe resulting inference machinery is entirely passive, in the\nsense that we require no active control of the target system\nnor access to the action sequence performed by the expert.\nAccordingly, our method is applicable to a broader range of\nproblems than targeted by most existing LfD frameworks\nand can be used for system identi\ufb01cation in cases where\nwe cannot interact with the target system. However, our\nobjective in this paper is twofold: we not only attempt to\nanswer the question how the expert performs a given task\nbut also to infer which information is used by the expert to\nsolve it. This knowledge is captured in the form of a joint\nposterior distribution over possible expert state representa-\ntions and corresponding state controllers. As the complexity\nof the expert\u2019s state representation is unknown a priori,\nwe \ufb01nally present a Bayesian nonparametric approach to\nexplore the underlying structure of the system space based\non the available demonstration data.\n1.1\nProblem statement\nGiven a set of expert demonstrations in the form of a system\ntrajectory s = (s1, s2, . . . , sT ) \u2208ST of length T, where\nS denotes the system state space, our goal is to determine\nthe latent control policy used by the expert to generate the\nstate sequence.1 We formalize this problem as a discrete-\ntime decision-making process (i.e. we assume that the expert\nexecutes exactly one control action per trajectory state) and\nadopt the Markov decision process (MDP) formalism [17]\nas the underlying framework describing the dynamics of\nour system. More speci\ufb01cally, we consider a reduced MDP\n(S, A, T , \u03c0) which consists of a countable or uncountable\nsystem state space S, a \ufb01nite set of actions A containing\n|A| elements, a transition model T : S \u00d7 S \u00d7 A \u2192R\u22650\nwhere T (s\u2032 | s, a) denotes the probability (density) assigned\nto the event of reaching state s\u2032 after taking action a in\nstate s, and a policy \u03c0 modeling the expert\u2019s choice of\nactions.2 In the following, we assume that the expert policy\nis parametrized by a parameter \u03c9 \u2208\u2126, which we call\nthe global control parameter of the system, and we write\n\u03c0(a | s, \u03c9), \u03c0 : A \u00d7 S \u00d7 \u2126\u2192[0, 1], to denote the expert\u2019s\nlocal policy (i.e. the distribution of actions a played by the\nexpert) at any given state s under \u03c9. The set \u2126is called the\nparameter space of the policy, which speci\ufb01es the class of\nfeasible action distributions. The speci\ufb01c form of \u2126will be\ndiscussed later.\nUsing a parametric description for \u03c0 is convenient as\nit shifts the recognition task from determining the possibly\nin\ufb01nite set of local policies at all states in S to inferring the\nposterior distribution p(\u03c9 | s), which contains all informa-\ntion that is relevant for predicting the expert behavior,\np(a | s\u2217, s) =\nZ\n\u2126\n\u03c0(a | s\u2217, \u03c9)p(\u03c9 | s) d\u03c9.\n1. The generalization to multiple trajectories is straightforward as\nthey are conditionally independent given the system parameters.\n2. This reduced model is sometimes referred to as an MDP\\R (see\ne.g. [9], [18], [19]) to emphasize the nonexistence of a reward function.\nHerein, s\u2217\u2208S is some arbitrary query point and p(a | s\u2217, s)\nis the corresponding predictive action distribution. Since the\nlocal policies are coupled through the global control param-\neter \u03c9 as indicated by the above integral equation, inferring\n\u03c9 means not only to determine the individual local policies\nbut also their spatial dependencies. Consequently, learning\nthe structure of \u03c9 from demonstration data can be also\ninterpreted as learning a suitable state representation for\nthe task performed by the expert. This relationship will be\ndiscussed in detail in the forthcoming sections. In Section 3,\nwe further extend this reasoning to a nonparametric policy\nmodel whose hypothesis class \ufb01nally covers all stochastic\npolicies on S.\nFor the remainder of this paper, we make the common\nassumptions that the transition model T as well as the\nsystem state space S and the action set A are known. The\nassumption of knowing S follows naturally because we\nalready assumed that we can observe the expert acting in S.\nIn the proposed Bayesian framework, the latter assumption\ncan be easily relaxed by considering noisy or incomplete tra-\njectory data. However, as this would not provide additional\ninsights into the main principles of our method, we do not\nconsider such an extension in this work.\nThe assumption of knowing the transition dynamics T is\na simplifying one but prevents us from running into model\nidenti\ufb01ability problems: if we do not constrain our system\ntransition model in some reasonable way, any observed state\ntransition in S could be trivially explained by a correspond-\ning local adaptation of the assumed transition model T\nand, thus, there would be little hope to extract the true ex-\npert policy from the demonstration data. Assuming a \ufb01xed\ntransition model is the easiest way to resolve this model\nambiguity. However, there are alternatives which we leave\nfor future work, for example, using a parametrized family of\ntransition models for joint inference. This extension can be\nintegrated seamlessly into our Bayesian framework and is\nuseful in cases where we can constrain the system dynamics\nin a natural way, e.g. when modeling physical processes.\nAlso, it should be mentioned that we can tolerate deviations\nfrom the true system dynamics as long as our model T is\nsuf\ufb01ciently accurate to extract information about the expert\naction sequence locally, because our inference algorithm\nnaturally processes the demonstration data piece-wise in\nthe form of one-step state transitions {(st, st+1)} (see algo-\nrithmic details in Section 2 and results in Section 4.2). This\nis in contrast to planning-based approaches, where small\nmodeling errors in the dynamics can accumulate and yield\nconsistently wrong policy estimates [8], [20].\nThe requirement of knowing the action set A is less\nstringent: if A is unknown a priori, we can still assume a po-\ntentially rich class of actions, as long as the transition model\ncan provide the corresponding dynamics (see example in\nSection 4.2). For instance, we might be able to provide a\nmodel which describes the movement of a robotic arm even\nif the maximum torque that can be generated by the system\nis unknown. Figuring out which of the hypothetical actions\nare actually performed by the expert and, more importantly,\nhow they are used in a given context, shall be the task of\nour inference algorithm.\n3\n1.2\nRelated work\nThe idea of learning from demonstration has now been\naround for several decades. Most of the work on LfD has\nbeen presented by the robotics community (see [1] for a\nsurvey), but recent advances in the \ufb01eld have triggered de-\nvelopments in other research areas, such as cognitive science\n[21] and human-machine interaction [22]. Depending on the\nsetup, the problem is referred to as imitation learning [23],\napprenticeship learning [9], inverse reinforcement learning\n[13], inverse optimal control [24], preference elicitation [21],\nplan recognition [25] or behavioral cloning [5]. Most LfD\nmodels can be categorized as intentional models (with\ninverse reinforcement learning models as the primary ex-\nample), or sub-intentional models (e.g. behavioral cloning\nmodels). While the latter class only predicts an agent\u2019s\nbehavior via a learned policy representation, intentional\nmodels (additionally) attempt to capture the agent\u2019s beliefs\nand intentions, e.g. in the form of a reward function. For this\nreason, intentional models are often reputed to have better\ngeneralization abilities3; however, they typically require a\ncertain amount of task-speci\ufb01c prior knowledge in order\nto resolve the ambiguous relationship between intention\nand behavior, since there are often many ways to solve\na certain task [13]. Also, albeit being interesting from a\npsychological point of view [21], intentional models target\na much harder problem than what is actually required in\nmany LfD scenarios. For instance, it is not necessary to\nunderstand an agent\u2019s intention if we only wish to analyze\nits behavior locally.\nAnswering the question whether or not an intention-\nbased modeling of the LfD problem is advantageous, is out\nof the scope of this paper; however, we point to the com-\nprehensive discussion in [26]. Rather, we present a hybrid\nsolution containing both intentional and sub-intentional ele-\nments. More speci\ufb01cally, our method does not explicitly cap-\nture the expert\u2019s goals in the form of a reward function but\ninfers a policy model directly from the demonstration data;\nnonetheless, the presented algorithm learns a task-speci\ufb01c\nrepresentation of the system state space which encodes the\nstructure of the underlying control problem to facilitate the\npolicy prediction task. An early version of this idea can\nbe found in [27], where the authors proposed a simple\nmethod to partition a system\u2019s state space into a set of so-\ncalled control situations to learn a global system controller\nbased on a small set of informative states. However, their\nframework does not incorporate any demonstration data\nand the proposed partitioning is based on heuristics. A more\nsophisticated partitioning approach utilizing expert demon-\nstrations is shown in [11]; yet, the proposed expectation-\nmaximization framework applies to deterministic policies\nand \ufb01nite state spaces only.\nThe closest methods to ours can be probably found in\n[19] and [10]. The authors of [19] presented a nonparametric\ninverse reinforcement learning approach to cluster the ex-\npert data based on a set of learned subgoals encoded in the\nform of local rewards. Unfortunately, the required subgoal\n3. The rationale behind this is that an agent\u2019s intention is always\nspeci\ufb01c to the task being performed and can hence serve as a compact\ndescription of it [13]. However, if the intention of the agent is misunder-\nstood, then also the subsequent generalization step will trivially fail.\nassignments are learned only for the demonstration set and,\nthus, the algorithm cannot be used for action prediction\nat unvisited states unless it is extended with a non-trivial\npost-processing step which solves the subgoal assignment\nproblem. Moreover, the algorithm requires an MDP solver,\nwhich causes dif\ufb01culties for systems with uncountable state\nspaces. The sub-intentional model in [10], on the other hand,\ncan be used to learn a class of \ufb01nite state controllers directly\nfrom the expert demonstrations. Like our framework, the\nalgorithm can handle various kinds of uncertainty about\nthe data but, again, the proposed approach is limited to\ndiscrete settings. In the context of reinforcement learning,\nwe further point to the work presented in [28] whose\nauthors follow a nonparametric strategy similar to ours, to\nlearn a distribution over predictive state representations for\ndecision-making.\n1.3\nPaper outline\nThe outline of the paper is as follows: In Section 2, we\nintroduce our parametric policy recognition framework and\nderive inference algorithms for both countable and uncount-\nable state spaces. In Section 3, we consider the policy recog-\nnition problem from a nonparametric viewpoint and pro-\nvide insights into the state representation learning problem.\nSimulation results are presented in Section 4 and we give\na conclusion of our work in Section 5. In the supplement,\nwe provide additional simulation results, a note on the\ncomputational complexity of our model, as well as an in-\ndepth discussion on the issue of marginal invariance and\nthe problem of policy prediction in large states spaces.\n2\nPARAMETRIC POLICY RECOGNITION\n2.1\nFinite state spaces: the static model\nFirst, let us assume that the expert system can be modeled\non a \ufb01nite state space S and let |S| denote its cardinality.\nFor notational convenience, we represent both states and\nactions by integer values. Starting with the most general\ncase, we assume that the expert executes an individual\ncontrol strategy at each possible system state. Accordingly,\nwe introduce a set of local control parameters or local con-\ntrollers {\u03b8i}|S|\ni=1 by which we describe the expert\u2019s choice of\nactions. More speci\ufb01cally, we model the executed actions\nas categorical random variables and let the jth element of\n\u03b8i represent the probability that the expert chooses action\nj at state i. Consequently, \u03b8i lies in the (|A| \u22121)-simplex,\nwhich we denote by the symbol \u2206for brevity of notation, i.e.\n\u03b8i \u2208\u2206\u2286R|A|. Summarizing all local control parameters in\na single matrix, \u0398 \u2208\u2126\u2286\u2206|S|, we obtain the global control\nparameter of the system as already introduced in Section 1.1,\nwhich compactly captures the expert behavior. Note that\nwe denote the global control parameter here by \u0398 instead\nof \u03c9, for reasons that will become clear later. Each action a\nis thus characterized by the local policy that is induced by\nthe control parameter of the underlying state,\n\u03c0(a | s = i, \u0398) = CAT(a | \u03b8i).\nFor simplicity, we will write \u03c0(a | \u03b8i) since the state infor-\nmation is used only to select the appropriate local controller.\n4\nConsidering a \ufb01nite set of actions, it is convenient to\nplace a symmetric Dirichlet prior on the local control pa-\nrameters,\np\u03b8(\u03b8i | \u03b1) = DIR(\u03b8i | \u03b1 \u00b7 1|A|),\nwhich forms the conjugate distribution to the categorical\ndistribution over actions. Here, 1|A| denotes the vector of\nall ones of length |A|. The prior is itself parametrized by a\nconcentration parameter \u03b1 which can be further described\nby a hyperprior p\u03b1(\u03b1), giving rise to a Bayesian hierarchical\nmodel. For simplicity, we assume that the value of \u03b1 is \ufb01xed\nfor the remainder of this paper, but the extension to a full\nBayesian treatment is straightforward. The joint distribution\nof all remaining model variables is, therefore, given as\np(s, a, \u0398 | \u03b1) = p1(s1)\n|S|\nY\ni=1\np\u03b8(\u03b8i | \u03b1) . . .\n(1)\n. . . \u00d7\nT \u22121\nY\nt=1\nT (st+1 | st, at)\u03c0(at | \u03b8st),\nwhere a = (a1, a2, . . . , aT \u22121) denotes the latent action\nsequence taken by the expert and p1(s1) is the initial state\ndistribution of the system. Throughout the rest of the paper,\nwe refer to this model as the static model. The corresponding\ngraphical visualization is depicted in Fig. 1.\n2.1.1\nGibbs sampling\nFollowing a Bayesian methodology, our goal is to determine\nthe posterior distribution p(\u0398 | s, \u03b1), which contains all\ninformation necessary to make predictions about the ex-\npert behavior. For the static model in Eq. (1), the required\nmarginalization of the latent action sequence a can be\ncomputed ef\ufb01ciently because the joint distribution factorizes\nover time instants. For the extended models presented in\nlater sections, however, a direct marginalization becomes\ncomputationally intractable due to the exponential growth\nof latent variable con\ufb01gurations. As a solution to this prob-\nlem, we follow a sampling-based inference strategy which\nis later on generalized to more complex settings.\nFor the simple model described above, we \ufb01rst approxi-\nmate the joint posterior distribution p(\u0398, a | s, \u03b1) over both\ncontrollers and actions using a \ufb01nite number of Q samples,\nand then marginalize over a in a second step,\np(\u0398 | s, \u03b1) =\nX\na\np(\u0398, a | s, \u03b1)\n(2)\n\u2248\nX\na\n\uf8eb\n\uf8ed1\nQ\nQ\nX\nq=1\n\u03b4\u0398{q}a{q}(\u0398, a)\n\uf8f6\n\uf8f8= 1\nQ\nQ\nX\nq=1\n\u03b4\u0398{q}(\u0398),\nwhere (\u0398{q}, a{q}) \u223cp(\u0398, a | s, \u03b1), and \u03b4x(\u00b7) denotes\nDirac\u2019s delta function centered at x. This two-step approach\ngives rise to a simple inference procedure since the joint\nsamples {(\u0398{q}, a{q})}Q\nq=1 can be easily obtained from a\nGibbs sampling scheme, i.e. by sampling iteratively from\nthe following two conditional distributions,\np(at | a\u2212t, s, \u0398, \u03b1) \u221dT (st+1 | st, at)\u03c0(at | \u03b8st),\np(\u03b8i | \u0398\u2212i, s, a, \u03b1) \u221dp\u03b8(\u03b8i | \u03b1)\nY\nt:st=i\n\u03c0(at | \u03b8i).\nst\u22121\nst\nst+1\nat\u22121\nat\nat+1\n\u0398\n\u03b1\nz\nFig. 1: Graphical model of the policy recognition frame-\nwork. The underlying dynamical structure is that of an\nMDP whose global control parameter \u0398 is treated as a\nrandom variable with prior distribution parametrized by \u03b1.\nThe indicator node z is used for the clustering model in\nSection 2.2. Observed variables are highlighted in gray.\nHerein, a\u2212t and \u0398\u2212i refer to all actions/controllers except\nat and \u03b8i, respectively. The latter of the two expressions\nreveals that, in order to sample \u03b8i, we need to consider\nonly those actions played at the corresponding state i.\nFurthermore, the \ufb01rst expression shows that, given \u0398, all\nactions {at} can be sampled independently of each other.\nTherefore, inference can be done in parallel for all \u03b8i. This\ncan be also seen from the posterior distribution of the global\ncontrol parameter, which factorizes over states,\np(\u0398 | s, a, \u03b1) \u221d\n|S|\nY\ni=1\np\u03b8(\u03b8i | \u03b1)\nY\nt:st=i\n\u03c0(at | \u03b8i).\n(3)\nFrom the conjugacy of p\u03b8(\u03b8i | \u03b1) and \u03c0(at | \u03b8i), it follows\nthat the posterior over \u03b8i is again Dirichlet distributed with\nupdated concentration parameter. In particular, denoting by\n\u03c6i,j the number of times that action j is played at state i for\nthe current assignment of actions a,\n\u03c6i,j :=\nX\nt:st=i\n1(at = j),\n(4)\nand by collecting these quantities in the form of vectors, i.e.\n\u03c6i := [\u03c6i,1, . . . , \u03c6i,|A|], we can rewrite Eq. (3) as\np(\u0398 | s, a, \u03b1) =\n|S|\nY\ni=1\nDIR(\u03b8i | \u03c6i + \u03b1 \u00b7 1|A|).\n(5)\n2.1.2\nCollapsed Gibbs sampling\nChoosing a Dirichlet distribution as prior model for the\nlocal controllers is convenient as it allows us to arrive at\nanalytic expressions for the conditional distributions that\nare required to run the Gibbs sampler. As an alternative,\nwe can exploit the conjugacy property of p\u03b8(\u03b8i | \u03b1) and\n\u03c0(at | \u03b8i) to marginalize out the control parameters during\nthe sampling process, giving rise to a collapsed sampling\nscheme. Collapsed sampling is advantageous in two differ-\nent respects: \ufb01rst, it reduces the total number of variables\nto be sampled and, hence, the number of computations\nrequired per Gibbs iteration; second, it increases the mixing\nspeed of the underlying Markov chain that governs the\nsampling process, reducing the correlation of the obtained\n5\nsamples and, with it, the variance of the resulting policy\nestimate.\nFormally, collapsing means that we no longer approxi-\nmate the joint distribution p(\u0398, a | s, \u03b1) as done in Eq. (2),\nbut instead sample from the marginal density p(a | s, \u03b1),\np(\u0398 | s, \u03b1) =\nX\na\np(\u0398 | s, a, \u03b1)p(a | s, \u03b1)\n\u2248\nX\na\np(\u0398 | s, a, \u03b1)\n\uf8eb\n\uf8ed1\nQ\nQ\nX\nq=1\n\u03b4a{q}(a)\n\uf8f6\n\uf8f8\n= 1\nQ\nQ\nX\nq=1\np(\u0398 | s, a{q}, \u03b1),\n(6)\nwhere a{q} \u223cp(a | s, \u03b1). In contrast to the previous\napproach, the target distribution is no longer represented\nby a sum of Dirac measures but described by a product of\nDirichlet mixtures (compare Eq. (5)). The required samples\n{a{q}} can be obtained from a collapsed Gibbs sampler with\np(at | a\u2212t, s, \u03b1) \u221d\nZ\n\u2206|S| p(s, a, \u0398 | \u03b1) d\u0398\n\u221dT (st+1 | st, at)\nZ\n\u2206\np\u03b8(\u03b8st | \u03b1)\nY\nt\u2032:st\u2032=st\n\u03c0(at\u2032 | \u03b8st) d\u03b8st.\nIt turns out that the above distribution provides an easy\nsampling mechanism since the integral part, when viewed\nas a function of action at only, can be identi\ufb01ed as the con-\nditional of a Dirichlet-multinomial distribution. This distri-\nbution is then reweighted by the likelihood T (st+1 | st, at)\nof the observed transition. The \ufb01nal (unnormalized) weights\nof the resulting categorical distribution are hence given as\np(at = j | a\u2212t, s, \u03b1) \u221dT (st+1 | st, at = j) \u00b7 (\u03d5t,j + \u03b1), (7)\nwhere \u03d5t,j counts the number of occurrences of action j\namong all actions in a\u2212t played at the same state as at (that\nis, st). Explicitly,\n\u03d5t,j :=\nX\nt\u2032:st\u2032=st\nt\u2032\u0338=t\n1(at\u2032 = j).\nNote that these values can be also expressed in terms of the\nsuf\ufb01cient statistics introduced in the last section,\n\u03d5t,j = \u03c6st,j \u22121(at = j).\nAs before, actions played at different states may be sampled\nindependently of each other because they are generated by\ndifferent local controllers. Consequently, inference about \u0398\nagain decouples for all states.\n2.2\nTowards large state spaces: a clustering approach\nWhile the methodology introduced so far provides a means\nto solve the policy recognition problem in \ufb01nite state spaces,\nthe presented approaches quickly become infeasible for\nlarge spaces as, in the continuous limit, the number of\nparameters to be learned (i.e. the size of \u0398) will grow\nunbounded. In that sense, the presented methodology is\nprone to over\ufb01tting because, for larger problems, we will\nnever have enough demonstration data to suf\ufb01ciently cover\nthe whole system state space. In particular, the static model\n\u03b81\n\u03b82\n\u03b83\n\u03b84\n\u03b85\nS\nFig. 2: Schematic illustration of the clustering model. The\nstate space S is partitioned into a set of clusters {Ck}, each\ngoverned by its own local control parameter \u03b8k.\nmakes no assumptions about the structure of \u0398 but treats\nall local policies separately (see Eq. (5)); hence, we are not\nable to generalize the demonstrated behavior to regions of\nthe state space that are not directly visited by the expert.\nYet, we would certainly like to predict the expert behavior\nalso at states for which there is no trajectory data avail-\nable. Moreover, we should expect a well-designed model to\nproduce increasingly accurate predictions at regions closer\nto the observed trajectories (with the precise de\ufb01nition of\n\u201ccloseness\u201d being left open for the moment).\nA simple way to counteract the over\ufb01tting problem, in\ngeneral, is to restrict the complexity of a model by limiting\nthe number of its free parameters. In our case, we can avoid\nthe parameter space to grow unbounded by considering\nonly a \ufb01nite number of local policies that need to be shared\nbetween the states. The underlying assumption is that, at\neach state, the expert selects an action according to one\nof K local policies, with corresponding control parame-\nters {\u03b8k}K\nk=1. Accordingly, we introduce a set of indicator\nor cluster assignment variables, {zi}|S|\ni=1, zi \u2208{1, . . . , K},\nwhich map the states to their local controllers (Fig. 1).\nObviously, such an assignment implies a partitioning of the\nstate space (Fig. 2), resulting in the following K clusters,\nCk := {i : zi = k},\nk \u2208{1, . . . , K}.\nAlthough we motivated the clustering of states by the\nproblem of over\ufb01tting, partitioning a system\u2019s space is not\nonly convenient from a statistical point of view; mapping\nthe inference problem down to a lower-dimensional space\nis also reasonable for practical reasons as we are typically\ninterested in understanding an agent\u2019s behavior on a certain\ntask-appropriate scale. The following paragraphs discuss\nthese reasons in detail:\n\u2022 In practice, the observed trajectory data will always be\nnoisy since we can take our measurements only up to a\ncertain \ufb01nite precision. Even though we do not explicitly\nconsider observation noise in this paper, clustering the data\nappears reasonable in order to robustify the model against\nsmall perturbations in our observations.\n\u2022 Considering the LfD problem from a control perspective,\nthe complexity of subsequent planning steps can be poten-\ntially reduced if the system dynamics can be approximately\ndescribed on a lower-dimensional manifold of the state\nspace, meaning that the system behavior can be well repre-\nsented by a smaller set of informative states (c.f. \ufb01nite state\n6\ncontrollers [29], control situations [27]). The LfD problem\ncan then be interpreted as the problem of learning a (near-\noptimal) controller based on a small set of local policies that\ntogether provide a good approximation of the global agent\nbehavior. What remains is the question how we can \ufb01nd\nsuch a representation. The clustering approach described\nabove offers one possible solution to this problem.\n\u2022 Finally, in any real setup, it is reasonable to assume that\nthe expert itself can only execute a \ufb01nite-precision policy\ndue to its own limited sensing abilities of the system state\nspace. Consequently, the demonstrated behavior is going\nto be optimal only up to a certain \ufb01nite precision because\nthe agent is generally not able to discriminate between\narbitrary small differences of states. An interesting question\nin this context is whether we can infer the underlying state\nrepresentation of the expert by observing its reactions to the\nenvironment in the form of the resulting state trajectory. We\nwill discuss this issue in detail in Section 3.\nBy introducing the cluster assignment variables {zi}, the\njoint distribution in Eq. (1) changes into\np(s, a, z, \u0398 | \u03b1) = p1(s1)\nK\nY\nk=1\np\u03b8(\u03b8k | \u03b1) . . .\n(8)\n. . . \u00d7\nT \u22121\nY\nt=1\nT (st+1 | st, at)\u03c0(at | \u03b8zst )pz(z),\nwhere z = (z1, z2, . . . , z|S|) denotes the collection of all\nindicator variables and pz(z) is the corresponding prior\ndistribution to be further discussed in Section 2.2.3. Note\nthat the static model can be recovered as a special case of\nthe above when each state describes its own cluster, i.e. by\nsetting K = |S| and \ufb01xing zi = i (hence the name static).\nIn contrast to the static model, we now require both the\nindicator zi and the corresponding control parameter \u03b8zi\nin order to characterize the expert\u2019s behavior at a given\nstate i. Accordingly, the global control parameter of the\nmodel is \u03c9 = (\u0398, z) with underlying parameter space\n\u2126\u2286\u2206K\u00d7{1, . . . , K}|S| (see Section 1.1), and our target dis-\ntribution becomes p(\u0398, z | s, \u03b1). In what follows, we derive\nthe Gibbs and the collapsed Gibbs sampler as mechanisms\nfor approximate inference in this setting.\n2.2.1\nGibbs sampling\nAs shown by the following equations, the expressions for\nthe conditional distributions over actions and controllers\ntake a similar form to those of the static model. Here, the\nonly difference is that we no longer group the actions by\ntheir states but according to their generating local policies\nor, equivalently, the clusters {Ck},\np(at | a\u2212t, s, z, \u0398, \u03b1)\n\u221dT (st+1 | st, at) \u00b7 \u03c0(at | \u03b8zst ),\np(\u03b8k | \u0398\u2212k, s, a, z, \u03b1) \u221dp\u03b8(\u03b8k | \u03b1)\nY\nt:zst=k\n\u03c0(at | \u03b8k)\n= p\u03b8(\u03b8k | \u03b1)\nY\nt:st\u2208Ck\n\u03c0(at | \u03b8k).\nThe latter expression again takes the form of a Dirichlet\ndistribution with updated concentration parameter,\np(\u03b8k | \u0398\u2212k, s, a, z, \u03b1) = DIR(\u03b8k | \u03bek + \u03b1 \u00b7 1|A|),\nwhere \u03bek := [\u03bek,1 , . . . , \u03bek,|A|], and \u03bek,j denotes the number\nof times that action j is played at states belonging to cluster\nCk in the current assignment of a. Explicitly,\n\u03bek,j :=\nX\nt:zst=k\n1(at = j) =\nX\ni\u2208Ck\nX\nt:st=i\n1(at = j),\n(9)\nwhich is nothing but the sum of the \u03c6i,j\u2019s of the correspond-\ning states,\n\u03bek,j =\nX\ni\u2208Ck\n\u03c6i,j.\nIn addition to the actions and control parameters, we now\nalso need to sample the indicators {zi}, whose conditional\ndistributions can be expressed in terms of the corresponding\nprior model and the likelihood of the triggered actions,\np(zi | z\u2212i, s, a, \u0398, \u03b1) \u221dp(zi | z\u2212i)\nY\nt:st=i\n\u03c0(at | \u03b8zi). (10)\n2.2.2\nCollapsed Gibbs sampling\nAs before, we derive the collapsed Gibbs sampler by\nmarginalizing out the control parameters,\np(zi | z\u2212i, s, a, \u03b1) \u221d\nZ\n\u2206K p(s, a, z, \u0398 | \u03b1) d\u0398\n(11)\n\u221dp(zi | z\u2212i)\nZ\n\u2206K\nK\nY\nk=1\np\u03b8(\u03b8k | \u03b1)\nT \u22121\nY\nt=1\n\u03c0(at | \u03b8zst ) d\u0398\n\u221dp(zi | z\u2212i)\nZ\n\u2206K\nK\nY\nk=1\np\u03b8(\u03b8k | \u03b1)\n|S|\nY\ni\u2032=1\nY\nt:st=i\u2032\n\u03c0(at | \u03b8zi\u2032 ) d\u0398\n\u221dp(zi | z\u2212i)\nZ\n\u2206K\nK\nY\nk=1\np\u03b8(\u03b8k | \u03b1)\nY\ni\u2032:zi\u2032=k\nY\nt:st=i\u2032\n\u03c0(at | \u03b8k) d\u0398\n\u221dp(zi | z\u2212i)\nK\nY\nk=1\n\uf8eb\n\uf8ed\nZ\n\u2206\np\u03b8(\u03b8k | \u03b1)\nY\nt:st\u2208Ck\n\u03c0(at | \u03b8k) d\u03b8k\n\uf8f6\n\uf8f8.\nHere, we \ufb01rst grouped the actions by their associated states\nand then grouped the states themselves by the clusters\n{Ck}. Again, this distribution admits an easy sampling\nmechanism as it takes the form of a product of Dirichlet-\nmultinomials, reweighted by the conditional prior distri-\nbution over indicators. In particular, we observe that all\nactions played at some state i appear in exactly one of\nthe K integrals of the last equation. In other words, by\nchanging the value of zi (i.e. by assigning state i to another\ncluster), only two of the involved integrals are affected: the\none belonging to the previously assigned cluster, and the\none of the new cluster. Inference about the value of zi can\nthus be carried out using the following two sets of suf\ufb01cient\nstatistics:\n\u2022\n\u03c6i,j: the number of actions j played at state i,\n\u2022\n\u03c8i,j,k: the number of actions j played at states as-\nsigned to cluster Ck, excluding state i.\nThe \u03c6i,j\u2019s are the same as in Eq. (4) and their de\ufb01nition is\nrepeated here just as a reminder. For the \u03c8i,j,k\u2019s, on the other\nhand, we \ufb01nd the following explicit expression,\n\u03c8i,j,k :=\nX\ni\u2032\u2208Ck\ni\u2032\u0338=i\nX\nt:st=i\u2032\n1(at = j),\n7\nwhich can be also written in terms of the statistics used for\nthe ordinary Gibbs sampler,\n\u03c8i,j,k = \u03bek,j \u22121(i \u2208Ck) \u00b7 \u03c6i,j.\nBy collecting these quantities in a vector, i.e. \u03c8i,k\n:=\n[\u03c8i,1,k, . . . , \u03c8i,|A|,k], we end up with the following simpli-\n\ufb01ed expression,\np(zi = k | z\u2212i, s, a, \u03b1) \u221dp(zi = k | z\u2212i) . . .\n. . . \u00d7\nK\nY\nk\u2032=1\nDIRMULT(\u03c8i,k\u2032 + 1(k\u2032 = k) \u00b7 \u03c6i | \u03b1).\nFurther, we obtain the following result for the conditional\ndistribution of action at,\np(at | a\u2212t, s, z, \u03b1) \u221dT (st+1 | st, at) . . .\n. . . \u00d7\nZ\n\u2206\np\u03b8(\u03b8zst | \u03b1)\nY\nt\u2032:zst\u2032 =zst\n\u03c0(at\u2032 | \u03b8zst ) d\u03b8zst .\nBy introducing the suf\ufb01cient statistics {\u03d1t,j}, which count\nthe number of occurrences of action j among all states that\nare currently assigned to the same cluster as st (i.e. the\ncluster Czst ), excluding at itself,\n\u03d1t,j :=\nX\nt\u2032:zst\u2032 =zst\nt\u2032\u0338=t\n1(at = j),\nwe \ufb01nally arrive at the following expression,\np(at = j | a\u2212j, s, z, \u03b1) \u221d(\u03d1t,j + \u03b1) \u00b7 T (st+1 | st, at = j).\nAs for the static model, we can establish a relationship be-\ntween the statistics used for the ordinary and the collapsed\nsampler,\n\u03d1t,j = \u03bezst,j \u22121(at = j).\n2.2.3\nPrior models\nIn order to complete our model, we need to specify a prior\ndistribution over indicator variables pz(z). The following\nparagraphs present three such candidate models:\nNon-informative prior\nThe simplest of all prior models is the non-informative\nprior over partitionings, re\ufb02ecting the assumption that, a\npriori, all cluster assignments are equally likely and that the\nindicators {zi} are mutually independent. In this case, pz(z)\nis constant and, hence, the term p(zi | z\u2212i) in Eq. (10) and\nEq. (11) disappears, so that the conditional distribution of\nindicator zi becomes directly proportional to the likelihood\nof the inferred action sequence.\nMixing prior\nAnother simple yet expressive prior can be realized by the\n(\ufb01nite) Dirichlet mixture model. Instead of assuming that\nthe indicator variables are independent, the model uses a set\nof mixing coef\ufb01cients q = [q1, . . . , qK], where qk represents\nthe prior probability that an indicator variable takes on\nvalue k. The mixing coef\ufb01cients are themselves modeled by\na Dirichlet distribution, so that we \ufb01nally have\nq \u223cDIR(q | \u03b3\nK \u00b7 1K),\nzi | q \u223cCAT(zi | q),\nwhere \u03b3 is another concentration parameter, controlling the\nvariability of the mixing coef\ufb01cients. Note that the indicator\nvariables are still conditionally independent given the mixing\ncoef\ufb01cients in this model. More speci\ufb01cally, for a \ufb01xed q, the\nconditional distribution of a single indicator in Eq. (10) and\nEq. (11) takes the following simple form,\np(zi = k | z\u2212i, q) = qk.\nIf the value of q is unknown, we have two options to include\nthis prior into our model. One is to sample q additionally to\nthe remaining variables by drawing values from the follow-\ning conditional distribution during the Gibbs procedure,\np(q | s, a, z, \u0398, \u03b1) \u221dDIR(q | \u03b3\nK \u00b7 1K)\n|S|\nY\ni=1\nCAT(zi | q)\n\u221dDIR(q | \u03b6 + \u03b3\nK \u00b7 1K),\nwhere \u03b6 := [\u03b61 , . . . , \u03b6K], and \u03b6k denotes the number of\nvariables zi that map to cluster Ck,\n\u03b6k =\n|S|\nX\ni=1\n1(zi = k).\nAlternatively, we can again make use of the conjugacy prop-\nerty to marginalize out the mixing proportions q during the\ninference process, just as we did for the control parameters\nin previous sections. The result is (additional) collapsing in\nq. In this case, we simply replace the factor p(zi = k | z\u2212i)\nin the conditional distribution of zi by\np(zi = k | z\u2212i, \u03b3) \u221d(\u03b6(\u2212i)\nk\n+ \u03b3\nK ),\n(12)\nwhere \u03b6(\u2212i)\nk\nis de\ufb01ned like \u03b6k but without counting the\ncurrent value of indicator zi,\n\u03b6(\u2212i)\nk\n:=\n|S|\nX\ni\u2032=1\ni\u2032\u0338=i\n1(zi = k) = \u03b6k \u22121(zi = k).\nA detailed derivation is omitted here but follows the same\nstyle as for the collapsing in Section 2.1.2.\nSpatial prior\nBoth previous prior models assume (conditional) indepen-\ndence of the indicator variables and, hence, make no speci\ufb01c\nassumptions about their dependency structure. However,\nwe can also use the prior model to promote a certain type of\nspatial state clustering. A reasonable choice is, for instance,\nto use a model which preferably groups \u201csimilar\u201d states\ntogether (in other words, a model which favors clusterings\nthat assign those states the same local control parameter).\nSimilarity of states can be expressed, for example, by a\nmonotonically decreasing decay function f : [0, \u221e) \u2192[0, 1]\nwhich takes as input the distance between two states. The\nrequired pairwise distances can be, in turn, de\ufb01ned via some\ndistance metric \u03c7 : S \u00d7 S \u2192[0, \u221e).\nIn fact, apart from the reasons listed in Section 2.2, there\nis an additional motivation, more intrinsically related to the\ndynamics of the system, why such a clustering can be useful:\ngiven that the transition model of our system admits locally\nsmooth dynamics (which is typically the case for real-world\n8\nsystems), the resulting optimal control policy often turns\nout to be spatially smooth, too [11]. More speci\ufb01cally, under\nan optimal policy, two nearby states are highly likely to\nexperience similar controls; hence, it is reasonable to assume\na priori that both share the same local control parameter.\nFor the policy recognition task, it certainly makes sense to\nregularize the inference problem by encoding this particular\nstructure of the solution space into our model. The Potts\nmodel [30], which is a special case of a Markov random\n\ufb01eld with pairwise clique potentials [31], offers one way to\ndo this,\npz(z) \u221d\n|S|\nY\ni=1\nexp\n \n\u03b2\n2\n|S|\nX\nj=1\nj\u0338=i\nf(di,j)\u03b4(zi, zj)\n!\n.\nHere, \u03b4 denotes Kronecker\u2019s delta, di,j := \u03c7(si, sj), i, j \u2208\n{1, . . . , |S|}, are the state similarity values, and \u03b2 \u2208[0, \u221e)\nis the (inverse) temperature of the model which controls\nthe strength of the prior. From this equation, we can easily\nderive the conditional distribution of a single indicator\nvariable zi as\np(zi | z\u2212i) \u221dexp\n \n\u03b2\n|S|\nX\nj=1\nj\u0338=i\nf(di,j)\u03b4(zi, zj)\n!\n.\n(13)\nThis completes our inference framework for \ufb01nite spaces.\n2.3\nCountably in\ufb01nite and uncountable state spaces\nA major advantage of the clustering approach presented in\nthe last section is that, due to the limited number of local\npolicies to be learned from the \ufb01nite amount of demon-\nstration data, we can now apply the same methodology to\nstate spaces of arbitrary size, including countably in\ufb01nite\nand uncountable state spaces. This extension had been\npractically impossible for the static model because of the\nover\ufb01tting problem explained in Section 2.2. Nevertheless,\nthere remains a fundamental conceptual problem: a direct\nextension of the model to these spaces would imply that\nthe distribution over possible state partitionings becomes an\nin\ufb01nite-dimensional object (i.e., in the case of uncountable\nstate spaces, a distribution over functional mappings from\nstates to local controllers), requiring an in\ufb01nite number of\nindicator variables. Certainly, such an object is non-trivial to\nhandle computationally.\nHowever, while the number of latent cluster assign-\nments grows unbounded with the size of the state space,\nthe amount of observed trajectory data always remains\n\ufb01nite. A possible solution to the problem is, therefore, to\nreformulate the inference task on a reduced state space\neS := {s1, s2, . . . , sT } containing only states along the ob-\nserved trajectories. Reducing the state space in this way\nmeans that we need to consider only a \ufb01nite set of indicator\nvariables {zt}T\nt=1, one for each expert state s \u2208eS, which\nalways induces a model of \ufb01nite size. Assuming that no\nstate is visited twice, we may further use the same index\nset for both variable types.4 In order to limit the complexity\nof the dependency structure of the indicator variables for\nlarger data sets, we further let the value of indicator zt\ndepend only on a subset of the remaining variables z\u2212t as\nde\ufb01ned by some neighborhood rule N . The resulting joint\ndistribution is then given as\npz(z | s) \u221d\nT\nY\nt=1\nexp\n \n\u03b2\n2\nX\nt\u2032\u2208Nt\nf(dt,t\u2032)\u03b4(zt, zt\u2032)\n!\n,\nwhich now implicitly depends on the state sequence s\nthrough the pairwise distances dt,t\u2032 := \u03c7(st, st\u2032), t, t\u2032 \u2208\n{1, . . . , T} (hence the conditioning on s).\nThe use of a \ufb01nite number of indicator variables along\nthe expert trajectories obviously circumvents the above-\nmentioned problem of representational complexity. Nev-\nertheless, there are some caveats associated with this ap-\nproach. First of all, using a reduced state space model raises\nthe question of marginal invariance [32]: if we added a new\ntrajectory point to the data set, would this change our belief\nabout the expert policy at previously visited states? In par-\nticular, how is this different from modeling that new point\ntogether with the initial ones in the \ufb01rst place? And further,\nwhat does such a reduced model imply for unvisited states?\nCan we still use it to make predictions about their local\npolicies? These questions are, in fact, important if we plan\nto use our model to generalize the expert demonstrations to\nnew situations. For a detailed discussion on this issue, the\nreader is referred to the supplement. Here, we focus on the\ninferential aspects of the problem, which means to identify\nthe system parameters at the given trajectory states.\nAnother (but related) issue resulting from the reduced\nmodeling approach is that we lose the simple generative\ninterpretation of the process that could be used to explain\nthe data generation beforehand. In the case of \ufb01nite state\nspaces, we could think of a trajectory as being constructed\nby the following, step-wise mechanism: \ufb01rst, the prior pz(z)\nis used to generate a set of indicator variables for all states.\nIndependently, we pick some value for \u03b1 from p\u03b1(\u03b1) and\nsample K local control parameters from p\u03b8(\u03b8k | \u03b1). To\n\ufb01nally generate a trajectory, we start with an initial state\ns1, generated by p1(s1), select a random action a1 from\n\u03c0(a1 | s1, \u03b8zs1 ) and transition to a new state s2 according to\nT (s2 | s1, a1), where we select another action a2, and so on.\nSuch a directed way of thinking is possible since the \ufb01nite\nmodel naturally obeys a causal structure where later states\ndepend on earlier ones and the decisions made there. Fur-\nthermore, the cluster assignments and the local controllers\ncould be generated in advance and isolated from each other\nbecause they were modeled marginally independent.\nFor the reduced state space model, this interpretation no\nlonger applies as the model has no natural directionality. In\nfact, its variables depend on each other in a cyclic fashion:\naltering the value of a particular indicator variable (say, the\none corresponding to the last trajectory point) will have\n4. Note that we make this assumption for notational convenience\nonly and that it is not required from a mathematical point of view.\nNonetheless, for uncountable state spaces the assumption is reasonable\nsince the event of reaching the same state twice has zero probability\nfor most dynamic models. In the general case, however, the indicator\nvariables require their own index set to ensure that each system state is\nassociated with exactly one cluster, even when visited multiple times.\n9\nan effect on the values of all remaining indicators due to\ntheir spatial relationship encoded by the \u201cprior distribution\u201d\npz(z | s). Changing the values of the other indicators,\nhowever, will in\ufb02uence the actions being played at the\nrespective states which, in turn, alters the probability of\nending up with the observed trajectory in the \ufb01rst place and,\nhence, the position and value of the indicator variable we\nstarted with. Explaining the data generation of this model\nusing a simple generative process is, therefore, not possible.\nNevertheless, the individual building blocks of our\nmodel (that is, the policy, the transition model, etc.) together\nform a valid distribution over the model variables, which\ncan be readily used for parameter inference. For the reasons\nexplained above, it makes sense to de\ufb01ne this distribution in\nthe form of a discriminative model, ignoring the underlying\ngenerative aspects of the process. This is suf\ufb01cient since we\ncan always condition on the observed state sequence s,\np(a, \u0398, z | s, \u03b1) = 1\nZs\npz(z | s)\nK\nY\nk=1\np\u03b8(\u03b8k | \u03b1) . . .\n. . . \u00d7 p1(s1)\nT \u22121\nY\nt=1\nT (st+1 | st, at)\u03c0(at | \u03b8zst ).\nHerein, Zs is a data-dependent normalizing constant. The\nstructure of this distribution is illustrated by the factor graph\nshown in the supplement (Fig. S-1), which highlights the\ncircular dependence between the variables. Note that, for\nany \ufb01xed state sequence s, this distribution indeed encodes\nthe same basic properties as the \ufb01nite model in Eq. (8).\nIn particular, the conditional distributions of all remaining\nvariables remain unchanged, which allows us to apply the\nsame inference machinery that we already used in the \ufb01nite\ncase. For a deeper discussion on the difference between the\ntwo models, we again point to the supplement.\n3\nNONPARAMETRIC POLICY RECOGNITION\nIn the last section, we presented a probabilistic policy recog-\nnition framework for modeling the expert behavior using\na \ufb01nite mixture of K local policies. Basically, there are two\nsituations when such a model is useful:\n\u2022\neither, we know the true number of expert policies,\n\u2022\nor, irrespective of the true behavioral complexity, we\nwant to \ufb01nd an approximate system description in\nterms of at most K distinct control situations [27]\n(c.f. \ufb01nite state controllers [29]).\nIn all other cases, we are faced with the non-trivial prob-\nlem of choosing K. In fact, the choice of K should not\njust be considered a mathematical necessity to perform\ninference in our model. By selecting a certain value for\nK we can, of course, directly control the complexity class\nof potentially inferred expert controllers. However, from a\nsystem identi\ufb01cation point of view, it is more reasonable to\ninfer the required granularity of the state partitioning from\nthe observed expert behavior itself, instead of enforcing a\nparticular model complexity. This way, we can gain valuable\ninformation about the underlying control structure and state\nrepresentation used by the expert, which offers a possibility\nto learn a state partitioning of task-appropriate complexity\ndirectly from the demonstration data. Hence, the problem\nof selecting the right model structure should be considered\nas part of the inference problem itself.\nFrom a statistical modeling perspective, there are two\ncommon ways to approach this problem. One is to make use\nof model selection techniques in order to determine the most\nparsimonious model that is in agreement with the observed\ndata. However, choosing a particular model complexity still\nmeans that we consider only one possible explanation for\nthe data, although other explanations might be likewise\nplausible. For many inference tasks, including this one, the\nmore elegant approach is to keep the complexity \ufb02exible\nand, hence, adaptable to the data. Mathematically, this can\nbe achieved by assuming a potentially in\ufb01nite set of model\nparameters (in our case controllers) from which we activate\nonly a \ufb01nite subset to explain the particular data set at hand.\nThis alternative way of thinking opens the door to the rich\nclass of nonparametric models, which provide an integrated\nframework to formulate the inference problem over both\nmodel parameters and model complexity as a joint learning\nproblem.\n3.1\nA Dirichlet process mixture model\nThe classical way to nonparametric clustering is to use\na Dirichlet process mixture model (DPMM) [33]. These\nmodels can be obtained by starting from a \ufb01nite mixture\nmodel and letting the number of mixture components (i.e.\nthe number of local controllers) approach in\ufb01nity. In our\ncase, we start with the clustering model from Section 2.2,\nusing a mixing prior over indicator variables,\nq \u223cDIR(q | \u03b3\nK \u00b7 1K)\n\u03b8k \u223cDIR(\u03b8k | \u03b1 \u00b7 1|A|)\ns1 \u223cp1(s1)\nzi | q \u223cCAT(zi | q)\nat | st, \u0398, z \u223c\u03c0(at | \u03b8zst )\nst+1 | st, at \u223cT (st+1 | st, at) .\n(14)\nFrom these equations, we arrive at the corresponding non-\nparametric model as K goes to in\ufb01nity. For the theoretical\nfoundations of this limit, the reader is referred to the more\ngeneral literature on Dirichlet processes, such as [33], [34].\nIn this paper, we restrict ourselves to providing the resulting\nsampling mechanisms for the policy recognition problem.\nIn a DPMM, the mixing proportions q of the local\nparameters are marginalized out (that is, we use a col-\nlapsed sampler). The resulting distribution over partition-\nings is described by a Chinese restaurant process (CRP) [35]\nwhich can be derived, for instance, by considering the limit\nK \u2192\u221eof the mixing process induced by the Gibbs update\nin Eq. (12),\np(zi = k | z\u2212i, \u03b3) \u221d\n(\n\u03b6(\u2212i)\nk\nif k \u2208{1, . . . , K\u2217},\n\u03b3\nif k = K\u2217+ 1.\n(15)\nHere, K\u2217denotes the number of distinct entries in z\u2212i\nwhich are represented by the numerical values {1, . . . , K\u2217}.\nIn this model, a state joins an existing cluster (i.e. a group of\nstates whose indicators have the same value) with probabil-\nity proportional to the number of states already contained in\nthat cluster. Alternatively, it may create a new cluster with\nprobability proportional to \u03b3.\nFrom the model equations (14) it is evident that, given a\nparticular setting of indicators, the conditional distributions\nof all other variable types remain unchanged. Effectively, we\n10\nonly replaced the prior model pz(z) by the CRP. Hence, we\ncan apply the same Gibbs updates for the actions and con-\ntrollers as before and need to rederive only the conditional\ndistributions of the indicator variables under consideration\nof the above de\ufb01ned process. According to Eq. (15), we\nherein need to distinguish whether an indicator variable\ntakes a value already occupied by other indicators (i.e. it\njoins an existing cluster) or it is assigned a new value (i.e.\nit creates a new cluster). Let {\u03b8k}K\u2217\nk=1 denote the set of\ncontrol parameters associated with z\u2212i. In the \ufb01rst case\n(k \u2208{1, . . . , K\u2217}), we can then write\np(zi = k | z\u2212i, s, a, {\u03b8k\u2032}K\u2217\nk\u2032=1, \u03b1, \u03b3)\n= p(zi = k | z\u2212i, {at}t:st=i, \u03b8k, \u03b1, \u03b3)\n\u221dp(zi = k | z\u2212i, \u03b8k, \u03b1, \u03b3)p({at}t:st=i | zi = k, z\u2212i, \u03b8k, \u03b1, \u03b3)\n\u221dp(zi = k | z\u2212i, \u03b3)p({at}t:st=i | \u03b8k)\n\u221d\u03b6(\u2212i)\nk\n\u00b7\nY\nt:st=i\n\u03c0(at | \u03b8k).\nIn the second case (k = K\u2217+ 1), we instead obtain\np(zi = K\u2217+ 1 | z\u2212i, s, a, {\u03b8k}K\u2217\nk=1, \u03b1, \u03b3)\n= p(zi = K\u2217+ 1 | z\u2212i, {at}t:st=i, \u03b1, \u03b3)\n\u221dp(zi = K\u2217+ 1 | z\u2212i, \u03b1, \u03b3) . . .\n. . . \u00d7 p({at}t:st=i | zi = K\u2217+ 1, z\u2212i, \u03b1, \u03b3)\n\u221dp(zi = K\u2217+ 1 | z\u2212i, \u03b3)p({at}t:st=i | zi = K\u2217+ 1, \u03b1)\n\u221d\u03b3 \u00b7\nZ\n\u2206\np({at}t:st=i | \u03b8K\u2217+1)p\u03b8(\u03b8K\u2217+1 | \u03b1) d\u03b8K\u2217+1\n\u221d\u03b3 \u00b7\nZ\n\u2206\nY\nt:st=i\n\u03c0(at | \u03b8K\u2217+1)p\u03b8(\u03b8K\u2217+1 | \u03b1) d\u03b8K\u2217+1\n\u221d\u03b3 \u00b7 DIRMULT(\u03c6i | \u03b1).\nIf a new cluster is created, we further need to initialize the\ncorresponding control parameter \u03b8K\u2217+1 by performing the\nrespective Gibbs update, i.e. by sampling from\np(\u03b8K\u2217+1 | z, s, a, {\u03b8k}K\u2217\nk=1, \u03b1, \u03b3)\n= p(\u03b8K\u2217+1 | {at}t:zst=K\u2217+1, \u03b1)\n\u221dp\u03b8(\u03b8K\u2217+1 | \u03b1)p({at}t:zst=K\u2217+1 | \u03b8K\u2217+1)\n\u221dp\u03b8(\u03b8K\u2217+1 | \u03b1)\n\u00b7\nY\nt:zst=K\u2217+1\n\u03c0(at | \u03b8K\u2217+1)\n\u221dDIR(\u03b8K\u2217+1 | \u03beK\u2217+1 + \u03b1 \u00b7 1|A|).\nShould a cluster get unoccupied during the sampling pro-\ncess, the corresponding control parameter may be removed\nfrom the stored parameter set {\u03b8k} and the index set for\nk needs to be updated accordingly. Note that this sampling\nmechanism is a speci\ufb01c instance of Algorithm 2 described in\n[33]. A collapsed variant can be derived in a similar fashion.\n3.2\nPolicy recognition using the distance-dependent\nChinese restaurant process\nIn the previous section, we have seen that the DPMM can\nbe derived as the nonparametric limit model of a \ufb01nite\nmixture using a set of latent mixing proportions q for the\nclusters. Although the DPMM allows us to keep the number\nof active controllers \ufb02exible and, hence, adaptable to the\ncomplexity of the demonstration data, the CRP as the un-\nderlying clustering mechanism does not capture any spatial\ns1\ns2\ns3\ns4\ns5\ns6\ns7\ns8\ns9\nc1\nc2\nc3\nc4\nc5\nc6\nc7\nc8\nc9\nS\nFig. 3: Schematic illustration of the ddCRP-based clustering\napplied to the reduced state space model in Section 2.3.\nEach trajectory state is connected to some other state of\nthe sequence. The connected components of the resulting\ngraph implicitly de\ufb01ne the state clustering. Coloring of the\nbackground illustrates the spatial cluster extrapolation (see\nSection A in the supplement). Note that the underlying\ndecision-making process is assumed to be discrete in time;\nthe continuous gray line shown in the \ufb01gure is only to\nindicate the temporal ordering of the trajectory states.\ndependencies between the indicator variables. In fact, in the\nCRP, the indicators {zi} are coupled only via their relative\nfrequencies (see Eq. (15)) but not through their individual\nlocations in space, resulting in an exchangeable collection\nof random variables [35]. In fact, one could argue that the\nspatial structure of the clustering problem is a priori ignored.\nThe fact that DPMMs are nevertheless used for spatial\nclustering tasks can be explained by the particular form\nof data likelihood models that are typically used for the\nmixture components. In a Gaussian mixture model [36], for\ninstance, the spatial clusters emerge due to the unimodal\nnature of the mixture components, which encodes the local-\nity property of the model needed to obtain a meaningful\nspatial clustering of the data. For the policy recognition\nproblem, however, the DPMM is not able to exploit any\nspatial information via the data likelihood since the cluster-\ning of states is performed at the level of the inferred action\ninformation (see Eq. (10)) and not on the state sequence\nitself. Consequently, we cannot expect to obtain a smooth\nclustering of the system state space, especially when the\nexpert policies are overlapping (i.e. when they share one or\nmore common actions) so that the action information alone\nis not suf\ufb01cient to discriminate between policies. For un-\ncountable state spaces, this problem is further complicated\nby the fact that we observe at most one expert state transition\nper system state. Here, the spatial context of the data is the\nonly information which can resolve this ambiguity.\nIn order to facilitate a spatially smooth clustering, we\ntherefore need to consider non-exchangeable distributions\nover partitionings. More speci\ufb01cally, we need to design our\nmodel in such a way that, whenever a state s is \u201cclose\u201d to\nsome other state s\u2032 and assigned to some cluster Ck, then,\na priori, s\u2032 should belong to the same cluster Ck with high\nprobability. In that sense, we are looking for the nonpara-\nmetric counterpart of the Potts model. One model with such\nproperties is the distance-dependent Chinese restaurant\n11\nprocess (ddCRP) [32].5 As opposed to the traditional CRP,\nthe ddCRP explicitly takes into account the spatial structure\nof the data. This is done in the form of pairwise distances\nbetween states, which can be obtained, for instance, by\nde\ufb01ning an appropriate distance metric on the state space\n(see Section 2.2.3). Instead of assigning states to clusters as\ndone by the CRP, the ddCRP assigns states to other states\naccording to their pairwise distances. More speci\ufb01cally, the\nprobability that state i gets assigned to state j is de\ufb01ned as\np(ci = j | D, \u03bd) \u221d\n(\n\u03bd\nif i = j,\nf(di,j)\notherwise,\n(16)\nwhere \u03bd \u2208[0, \u221e) is called the self-link parameter of the pro-\ncess, D denotes the collection of all pairwise state distances,\nand ci is the \u201cto-state\u201d assignment of state i, which can be\nthought of as a directed edge on the graph de\ufb01ned on the\nset of all states (see Fig. 3). Accordingly, i and j in Eq. (16)\ncan take values {1, . . . , |S|} for the \ufb01nite state space model\nand {1, . . . , T} for our reduced state space model. The state\nclustering is then obtained as a byproduct of this mapping\nvia the connected components of the resulting graph (see\nFig. 3 again).\nReplacing the CRP by the ddCRP and following the\nsame line of argument as in [32], we obtain the required\nconditional distribution of the state assignment ci as\np(ci =j |c\u2212i,s,a,\u03b1,D,\u03bd)\u221d\n\uf8f1\n\uf8f4\n\uf8f2\n\uf8f4\n\uf8f3\n\u03bd\nj = i,\nf(di,j)\nno clusters merged,\nf(di,j)\u00b7L Czi and Czj merged,\nwhere we use the shorthand notation\nL =\nDIRMULT(\u03bezi + \u03bezj | \u03b1)\nDIRMULT(\u03bezi | \u03b1)DIRMULT(\u03bezj | \u03b1)\nfor the data likelihood term. The \u03bek,j\u2019s are de\ufb01ned as in\nEq. (9) but are based on the clustering which arises when we\nignore the current link ci. The resulting Gibbs sampler is a\ncollapsed one as the local control parameters are necessarily\nmarginalized out during the inference process.\n4\nSIMULATION RESULTS\nIn this section, we present simulation results for two types of\nsystem dynamics. As a proof-of-concept, we \ufb01rst investigate\nthe case of uncountable state spaces which we consider the\nmore challenging setting for reasons explained earlier. To\ncompare our framework with existing methods, we further\nprovide simulation results for the standard grid world\nbenchmark (see e.g. [9], [11], [19]). It should be pointed\nout, however, that establishing a fair comparison between\nLfD models is generally dif\ufb01cult due to their different\nworking principles (e.g. reward prediction vs. action predic-\ntion), objectives (system identi\ufb01cation vs. optimal control),\nrequirements (e.g. MDP solver, knowledge of the expert\u2019s\ndiscount factor, countable vs. uncountable state space), and\n5. Note that the authors of [32] avoid calling this model nonparamet-\nric since it cannot be cast as a mixture model originating from a random\nmeasure. However, we stick to this term in order to make a clear\ndistinction to the parametric models in Section 2, and to highlight the\nfact that there is no parameter K determining the number of controllers.\nFig. 4: Schematic illustration of the expert policy used in\nSection 4.1, which applies eight local controllers to sixteen\ndistinct regions. A sample trajectory is shown in color.\nassumptions (e.g. deterministic vs. stochastic expert behav-\nior). Accordingly, our goal is rather to demonstrate the\nprediction abilities of the considered models than to push\nthe models to their individual limits. Therefore, and to re-\nduce the overall computational load, we tuned most model\nhyper-parameters by hand. Our code is available at https:\n//github.com/AdrianSosic/BayesianPolicyRecognition.\n4.1\nExample 1: uncountable state space\nAs an illustrative example, we consider a dynamical sys-\ntem which describes the circular motion of an agent on\na two-dimensional state space. The actions of the agent\ncorrespond to 24 directions that divide the space of possible\nangles [0, 2\u03c0) into equally-sized intervals. More speci\ufb01cally,\naction j corresponds to the angle (j \u22121) 2\u03c0\n24 . The transition\nmodel of the system is de\ufb01ned as follows: for each selected\naction, the agent \ufb01rst makes a step of length \u00b5 = 1 in\nthe intended direction. The so-obtained position is then\ndistorted by additive zero-mean isotropic Gaussian noise of\nvariance \u03c32. This de\ufb01nes our transition kernel as\nT (st+1 | st, at = j) = N(st+1 | st + \u00b5 \u00b7 ej, \u03c32I),\n(17)\nwhere st, st+1 \u2208R2, ej denotes the two-dimensional unit\nvector pointing in the direction of action j, and I is the\ntwo-dimensional identity matrix. The overall goal of our\nagent is to describe a circular motion around the origin in\nthe best possible manner allowed by the available actions.\nHowever, due to limited sensory information, the agent is\nnot able to observe its exact position on the plane but can\nonly distinguish between certain regions of the state space,\nas illustrated by Fig. 4. Also, the agent is unsure about the\noptimal control strategy, i.e. it does not always make optimal\ndecisions but selects its actions uniformly at random from a\nsubset of actions, consisting of the optimal one and the two\nactions pointing to neighboring directions (see Fig. 4 again).\nTo increase the dif\ufb01culty of the prediction task, we further\nlet the agent change the direction of travel whenever the\ncritical distance of r = 5 to the origin is exceeded.\nHaving de\ufb01ned the expert behavior, we generate 10\nsample trajectories of length T = 100. Herein, we assume\na motion noise level of \u03c3 = 0.2 and initialize the agent\u2019s\n12\nposition uniformly at random on the unit circle. An example\ntrajectory is shown in Fig. 4. The obtained trajectory data is\nfed into the presented inference algorithms to approximate\nthe posterior distribution over expert controllers, and the\nwhole experiment is repeated in 100 Monte Carlo runs.\nFor the spatial models, we use the Euclidean metric to\ncompute the pairwise distances between states,\n\u03c7(s, s\u2032) = ||s \u2212s\u2032||2.\n(18)\nThe corresponding similarity values are calculated using a\nGaussian-shaped kernel. More speci\ufb01cally,\nfPotts(d) = exp\n \n\u2212d2\n\u03c32\nf\n!\nfor the Potts model and\nfddCRP(d) = (1 \u2212\u03f5)fPotts(d) + \u03f5\nfor the ddCRP model, with \u03c3f = 1 and a constant offset of\n\u03f5 = 0.01 which ensures that states with large distances can\nstill join the same cluster. For the Potts model, we further\nuse a neighborhood structure containing the eight closest\ntrajectory points of a state. This way, we ensure that, in\nprinciple, each local expert policy may occur at least once\nin the neighborhood of a state. The concentration parameter\nfor the local controls is set to \u03b1 = 1, corresponding to a\nuniform prior belief over local policies.\nA major drawback of the Potts model is that posterior\ninference about the temperature parameter \u03b2 is complicated\ndue to the nonlinear effect of the parameter on the nor-\nmalization of the model. Therefore, we manually selected\na temperature of \u03b2 = 1.6 based on a minimization of\nthe average policy prediction error (discussed below) via\nparameter sweeping. As opposed to this, we extend the\ninference problem for the ddCRP to the self-link parameter \u03bd\nas suggested in [32]. For this, we use an exponential prior,\np\u03bd(\u03bd) = EXP(\u03bd | \u03bb),\nwith rate parameter \u03bb = 0.1, and applied the independence\nMetropolis-Hastings algorithm [37] using p\u03bd(\u03bd) as proposal\ndistribution with an initial value of \u03bd = 1. In all our\nsimulations, the sampler quickly converged to its stationary\ndistribution, yielding posterior values for \u03bd with a mean of\n0.024 and a standard deviation of 0.023.\nTo locally compare the predicted policy with the ground\ntruth at a given state, we compute their earth mover\u2019s\ndistance (EMD) [38] with a ground distance metric measur-\ning the absolute angular difference between the involved\nactions. To track the learning progress of the algorithms,\nwe calculate the average EMD over all states of the given\ntrajectory set at each Gibbs iteration. Herein, the local policy\npredictions are computed from the single Gibbs sample of\nthe respective iteration, consisting of all sampled actions, in-\ndicators and \u2013 in case of non-collapsed sampling \u2013 the local\ncontrol parameters. The resulting mean EMDs and standard\ndeviations are depicted in Fig. 5. The inset further shows\nthe average EMD computed at non-trajectory states which\nare sampled on a regular grid (depicted in the supplement),\nre\ufb02ecting the quality of the resulting spatial prediction.\nAs expected, the \ufb01nite mixture model (using the true\nnumber of local policies, a collapsed mixing prior, and\n10 0\n10 1\n10 2\nGibbs iteration\n0.4\n0.6\n0.8\n1\n1.2\n1.4\naverage EMD\nmixing prior\nPotts (complete)\nPotts (collapsed)\nddCRP\n10 0\n10 1\n10 2\n0.4\n0.6\n0.8\n1\n1.2\n1.4\nFig. 5: Average policy prediction error at the simulated\ntrajectory states (main \ufb01gure) and at non-trajectory states\n(inset). Shown are the empirical mean values and standard\ndeviations, estimated from 100 Monte Carlo runs.\n\u03b3 = 1) is not able to learn a reasonable policy representation\nfrom the expert demonstrations since it does not explore the\nspatial structure of the data. In fact, the resulting prediction\nerror shows only a slight improvement as compared to\nan untrained model. In contrast to this, all spatial models\ncapture the expert behavior reasonably well. In agreement\nwith our reasoning in Section 2.1.2, we observe that the\ncollapsed Potts model mixes signi\ufb01cantly faster and has a\nsmaller prediction variance than the non-collapsed version.\nHowever, the ddCRP model gives the best result, both\nin terms of mixing speed (see [32] for an explanation of\nthis phenomenon) and model accuracy. Interestingly, this is\ndespite the fact that the ddCRP model additionally needs to\ninfer the number of local controllers necessary to reproduce\nthe expert behavior. The corresponding posterior distribu-\ntion, which shows a pronounced peak at the true number,\nis depicted in the supplement. There, we also provide addi-\ntional simulation results which give insights into the learned\nstate partitioning and the resulting spatial policy prediction\nerror. The results reveal that all expert motion patterns can\nbe identi\ufb01ed by our algorithm.\n4.2\nExample 2: \ufb01nite state space\nIn this section, we compare the prediction capabilities of our\nmodel to existing LfD frameworks, in particular: the max-\nimum margin method in [9] (max-margin), the maximum\nentropy approach in [12] (max-ent), and the expectation-\nmaximization algorithm in [11] (EM). For the comparison,\nwe restrict ourselves to the ddCRP model which showed\nthe best performance among all presented models.\nAs a \ufb01rst experiment, we compare all methods on a \ufb01nite\nversion of the setting in Section 4.1, which is obtained by\ndiscretizing the continuous state space into a regular grid\nS = {(x, y) \u2208Z2 : |x|, |y| \u226410}, resulting in a total of 441\nstates. The transition probabilities are chosen proportional\nto the normal densities in Eq. (17) sampled at the grid points.\nHere, we used a noise level of \u03c3 = 1 and a reduced number\nof eight actions. Probability mass \u201clying outside\u201d the \ufb01nite\ngrid area is shifted to the closest border states of the grid.\nFigure 6a delineates the average EMD over the number\nof trajectories (each of length T = 10) provided for training.\n13\n10 1\n10 2\n10 3\n10 4\nnumber of trajectories\n0\n0.5\n1\n1.5\n2\naverage EMD\nmax-ent\nmax-margin\nEM\nddCRP\n(a) learning curves (circular policy)\n10 1\n10 2\n10 3\n10 4\nnumber of trajectories\n0\n0.5\n1\n1.5\naverage EMD\nmax-ent\nmax-margin\nEM\nddCRP\n(b) learning curves (MDP policy)\n0\n0.2\n0.4\n0.6\n0.8\n1\n0\n0.5\n1\n1.5\nmax-ent\nmax-margin\nEM\nddCRP\n(c) model robustness (MDP policy)\nFig. 6: Average EMD values for the prediction task described in Section 4.2: (a) circular policy (b,c) MDP policy. Shown are\nthe empirical mean values and standard deviations, estimated from 100 Monte Carlo runs. The EMD values are computed\nbased on (a) the predicted action distributions and (b,c) the predicted next-state distributions. Note that the curves of\nmax-ent (purple) and max-margin (yellow) in sub\ufb01gure (a) lie on top of each other.\nWe observe that neither of the two intentional models (max-\nent and max-margin) is able to capture the demonstrated ex-\npert behavior. This is due to the fact that the circular expert\nmotion cannot be explained by a simple state-dependent\nreward structure but requires a more complex state-action\nreward model, which is not considered in the original for-\nmulations [9], [12]. While the EM model is indeed able to\ncapture the general trend of the data, the prediction is less\naccurate as compared to that of the ddCRP model, since it\ncannot reproduce the stochastic nature of the expert policy.\nIn fact, this difference in performance will become even\nmore pronounced for expert policies which distribute their\nprobability mass on a larger subset of actions. Therefore,\nthe ddCRP model outperforms all other models since the\nprovided expert behavior violates their assumptions.\nTo analyze how the ddCRP competes against the other\nmodels in their nominal situations, we further compare all\nalgorithms on a standard grid world task where the expert\nbehavior is obtained as the optimal response to a simple\nstate-dependent reward function. Herein, each state on the\ngrid is assigned a reward with a chance of 1%, which is then\ndrawn from a standard normal distribution. Worlds which\ncontain no reward are discarded. The discount factor 0.9,\nwhich is used to compute the expert policy (see [17]), is\nprovided as additional input for the intentional models.\nThe results are shown in Figure 6b, which illustrates that\nthe intention-based max-margin method outperforms all\nother methods for small amounts of training data. The sub-\nintentional methods (EM and ddCRP), on the other hand,\nyield better asymptotic estimates and smaller prediction\nvariances. It should be pointed out that the three reference\nmethods have a clear advantage over the ddCRP in this case\nbecause they assume a deterministic expert behavior a priori\nand do not need to infer this piece of information from the\ndata. Despite this additional challenge, the ddCRP model\nyields a competitive performance.\nFinally, we compare all approaches in terms of their\nrobustness against modeling errors. For this purpose, we\nrepeat the previous experiment with a \ufb01xed number of 1000\ntrajectories but employ a different transition model for infer-\nence than used for data generation. More speci\ufb01cally, we uti-\nlize an overly \ufb01ne-grained model consisting of 24 directions,\nassuming that the true action set is unknown, as suggested\nin Section 1.1. Additionally, we perturb the assumed model\nby multiplying (and later renormalizing) each transition\nprobability with a random number generated according to\nf(u) = tan( \u03c0\n4 (u + 1)), with u \u223cUNIFORM(\u2212\u03b7, \u03b7) and\nperturbation strength \u03b7 \u2208[0, 1]. Due the resulting model\nmismatch, a comparison to the ground truth policy based\non the predicted action distribution becomes meaningless.\nInstead, we compute the Euclidean EMDs between the\ntrue and the predicted next-state distributions, which we\nobtain by marginalizing the actions of the true/assumed\ntransition model with respect to the true/learned policy.\nFigure 6c depicts the resulting prediction performance for\ndifferent perturbation strengths \u03b7. The results con\ufb01rm that\nour approach is not only less sensitive to modeling errors as\nargued in Section 1.1; also, the prediction variance is notably\nsmaller than those of the intentional models.\n5\nCONCLUSION\nIn this work, we proposed a novel approach to LfD by\njointly learning the latent control policy of an observed\nexpert demonstrator together with a task-appropriate rep-\nresentation of the system state space. With the described\nparametric and nonparametric models, we presented two\nformulations of the same problem that can be used either\nto learn a global system controller of speci\ufb01ed complexity,\nor to infer the required model complexity from the ob-\nserved expert behavior itself. Simulation results for both\ncountable and uncountable state spaces and a comparison\nto existing frameworks demonstrated the ef\ufb01cacy of our\napproach. Most notably, the results showed that our method\nis able to learn accurate predictive behavioral models in\nsituations where intentional methods fail, i.e. when the\nexpert behavior cannot be explained as the result of a simple\nplanning procedure. This makes our method applicable to\na broader range of problems and suggests its use in a more\ngeneral system identi\ufb01cation context where we have no such\nprior knowledge about the expert behavior. Additionally,\nthe task-adapted state representation learned through our\nframework can be used for further reasoning.\n14\nREFERENCES\n[1]\nB. D. Argall, S. Chernova, M. Veloso, and B. Browning, \u201cA survey\nof robot learning from demonstration,\u201d Robotics and Autonomous\nSystems, vol. 57, no. 5, pp. 469\u2013483, 2009.\n[2]\nM. P. Deisenroth, D. Fox, and C. E. Rasmussen, \u201cGaussian pro-\ncesses for data-ef\ufb01cient learning in robotics and control,\u201d IEEE\nTransactions on Pattern Analysis and Machine Intelligence, vol. 37,\nno. 2, pp. 408\u2013423, 2015.\n[3]\nP. Abbeel and A. Y. Ng, \u201cExploration and apprenticeship learning\nin reinforcement learning,\u201d in Proc. 22nd International Conference on\nMachine Learning, 2005, pp. 1\u20138.\n[4]\nD. Michie, M. Bain, and J. Hayes-Miches, \u201cCognitive models from\nsubcognitive skills,\u201d IEE Control Engineering Series, vol. 44, pp. 71\u2013\n99, 1990.\n[5]\nC. Sammut, S. Hurst, D. Kedzier, and D. Michie, \u201cLearning to \ufb02y,\u201d\nin Proc. 9th International Workshop on Machine Learning, 1992, pp.\n385\u2013393.\n[6]\nP. Abbeel, A. Coates, and A. Y. Ng, \u201cAutonomous helicopter aer-\nobatics through apprenticeship learning,\u201d The International Journal\nof Robotics Research, 2010.\n[7]\nD. A. Pomerleau, \u201cEf\ufb01cient training of arti\ufb01cial neural networks\nfor autonomous navigation,\u201d Neural Computation, vol. 3, no. 1, pp.\n88\u201397, 1991.\n[8]\nC. G. Atkeson and S. Schaal, \u201cRobot learning from demonstra-\ntion,\u201d in Proc. 14th International Conference on Machine Learning,\nvol. 97, 1997, pp. 12\u201320.\n[9]\nP. Abbeel and A. Y. Ng, \u201cApprenticeship learning via inverse\nreinforcement learning,\u201d in Proc. 21st International Conference on\nMachine Learning, 2004.\n[10] A. Panella and P. J. Gmytrasiewicz, \u201cNonparametric Bayesian\nlearning of other agents\u2019 policies in interactive POMDPs,\u201d in\nProc. International Conference on Autonomous Agents and Multiagent\nSystems, 2015, pp. 1875\u20131876.\n[11] A. \u02c7So\u02c7si\u00b4c, A. M. Zoubir, and H. Koeppl, \u201cPolicy recognition via ex-\npectation maximization,\u201d in Proc. 41st IEEE International Conference\non Acoustics, Speech and Signal Processing, 2016.\n[12] B. D. Ziebart, A. L. Maas, J. A. Bagnell, and A. K. Dey, \u201cMaxi-\nmum entropy inverse reinforcement learning.\u201d in Proc. 23rd AAAI\nConference on Arti\ufb01cial Intelligence, 2008, pp. 1433\u20131438.\n[13] A. Y. Ng and S. J. Russell, \u201cAlgorithms for inverse reinforcement\nlearning.\u201d in Proc. 17th International Conference on Machine Learning,\n2000, pp. 663\u2013670.\n[14] D. Ramachandran and E. Amir, \u201cBayesian inverse reinforcement\nlearning,\u201d Proc. 20th International Joint Conference on Artical Intelli-\ngence, vol. 51, pp. 2586\u20132591, 2007.\n[15] S. D. Parsons, P. Gymtrasiewicz, and M. J. Wooldridge, Game theory\nand decision theory in agent-based systems.\nSpringer Science &\nBusiness Media, 2012, vol. 5.\n[16] K. Hindriks and D. Tykhonov, \u201cOpponent modelling in automated\nmulti-issue negotiation using Bayesian learning,\u201d in Proc. 7th\nInternational Joint Conference on Autonomous Agents and Multiagent\nSystems, 2008, pp. 331\u2013338.\n[17] R. S. Sutton and A. G. Barto, Reinforcement learning: An introduction.\nMIT press, 1998.\n[18] U. Syed and R. E. Schapire, \u201cA game-theoretic approach to ap-\nprenticeship learning,\u201d in Advances in Neural Information Processing\nSystems, 2007, pp. 1449\u20131456.\n[19] B. Michini and J. P. How, \u201cBayesian nonparametric inverse rein-\nforcement learning,\u201d in Machine Learning and Knowledge Discovery\nin Databases.\nSpringer, 2012, pp. 148\u2013163.\n[20] C. G. Atkeson and J. C. Santamaria, \u201cA comparison of direct and\nmodel-based reinforcement learning,\u201d in International Conference\non Robotics and Automation, 1997.\n[21] C. A. Rothkopf and C. Dimitrakakis, \u201cPreference elicitation and in-\nverse reinforcement learning,\u201d in Machine Learning and Knowledge\nDiscovery in Databases.\nSpringer, 2011, pp. 34\u201348.\n[22] O. Pietquin, \u201cInverse reinforcement learning for interactive sys-\ntems,\u201d in Proc. 2nd Workshop on Machine Learning for Interactive\nSystems, 2013, pp. 71\u201375.\n[23] S. Schaal, \u201cIs imitation learning the route to humanoid robots?\u201d\nTrends in Cognitive Sciences, vol. 3, no. 6, pp. 233\u2013242, 1999.\n[24] K. Dvijotham and E. Todorov, \u201cInverse optimal control with\nlinearly-solvable MDPs,\u201d in Proc. 27th International Conference on\nMachine Learning, 2010, pp. 335\u2013342.\n[25] E. Charniak and R. P. Goldman, \u201cA Bayesian model of plan\nrecognition,\u201d Arti\ufb01cial Intelligence, vol. 64, no. 1, pp. 53\u201379, 1993.\n[26] B. Piot, M. Geist, and O. Pietquin, \u201cLearning from demonstrations:\nIs it worth estimating a reward function?\u201d in Machine Learning and\nKnowledge Discovery in Databases.\nSpringer, 2013, pp. 17\u201332.\n[27] M. Waltz and K. Fu, \u201cA heuristic approach to reinforcement\nlearning control systems,\u201d IEEE Transactions on Automatic Control,\nvol. 10, no. 4, pp. 390\u2013398, 1965.\n[28] F. Doshi-Velez, D. Pfau, F. Wood, and N. Roy, \u201cBayesian non-\nparametric methods for partially-observable reinforcement learn-\ning,\u201d IEEE Transactions on Pattern Analysis and Machine Intelligence,\nvol. 37, no. 2, pp. 394\u2013407, 2015.\n[29] N. Meuleau, L. Peshkin, K.-E. Kim, and L. P. Kaelbling, \u201cLearning\n\ufb01nite-state controllers for partially observable environments,\u201d in\nProc. 15th Conference on Uncertainty in Arti\ufb01cial Intelligence, 1999,\npp. 427\u2013436.\n[30] R. B. Potts, \u201cSome generalized order-disorder transformations,\u201d\nin Mathematical Proceedings of the Cambridge Philosophical Society,\nvol. 48, no. 01, 1952, pp. 106\u2013109.\n[31] D. Koller and N. Friedman, Probabilistic Graphical Models: Principles\nand Techniques.\nMIT press, 2009.\n[32] D. M. Blei and P. I. Frazier, \u201cDistance dependent Chinese restau-\nrant processes,\u201d The Journal of Machine Learning Research, vol. 12,\npp. 2461\u20132488, 2011.\n[33] R. M. Neal, \u201cMarkov chain sampling methods for Dirichlet process\nmixture models,\u201d Journal of Computational and Graphical Statistics,\nvol. 9, no. 2, pp. 249\u2013265, 2000.\n[34] T. S. Ferguson, \u201cA Bayesian analysis of some nonparametric\nproblems,\u201d The Annals of Statistics, pp. 209\u2013230, 1973.\n[35] D. J. Aldous, Exchangeability and related topics.\nSpringer, 1985.\n[36] C. E. Rasmussen, \u201cThe in\ufb01nite Gaussian mixture model,\u201d in Ad-\nvances in Neural Information Processing Systems.\nMIT Press, 2000,\npp. 554\u2013560.\n[37] S. Chib and E. Greenberg, \u201cUnderstanding the Metropolis-\nHastings algorithm,\u201d The American Statistician, vol. 49, no. 4, pp.\n327\u2013335, 1995.\n[38] Y. Rubner, C. Tomasi, and L. J. Guibas, \u201cA metric for distributions\nwith applications to image databases,\u201d in 6th International Confer-\nence on Computer Vision, 1998, 1998, pp. 59\u201366.\nAdrian \u02c7So\u02c7si\u00b4c is a member of the Signal Pro-\ncessing Group and an associate member of\nthe Bioinspired Communication Systems Lab at\nTechnische Universit\u00a8at Darmstadt. Currently, he\nis working towards his Ph.D. degree under the\nsupervision of Prof. Abdelhak M. Zoubir and\nProf. Heinz Koeppl. His research interests center\naround topics from machine learning and (in-\nverse) reinforcement learning, with a focus on\nprobabilistic inference, multi-agent systems, and\nBayesian nonparametrics.\nAbdelhak M. Zoubir is professor at the De-\npartment of Electrical Engineering and Informa-\ntion Technology at Technische Universit\u00a8at Darm-\nstadt, Germany. His research interest lies in sta-\ntistical methods for signal processing with em-\nphasis on bootstrap techniques, robust detection\nand estimation, and array processing applied\nto telecommunications, radar, sonar, automotive\nmonitoring and biomedicine.\nHeinz Koeppl is professor at the Department of\nElectrical Engineering and Information Technol-\nogy at Technische Universit\u00a8at Darmstadt, Ger-\nmany. His research interests include Bayesian\ninference methods for biomolecular data and\nmethods for reconstructing large-scale biological\nor technological multi-agent systems from obser-\nvational data.\n1\nA Bayesian Approach to Policy Recognition\nand State Representation Learning\n(Supplement)\nAdrian \u02c7So\u02c7si\u00b4c, Abdelhak M. Zoubir and Heinz Koeppl\n!\nA\nMARGINAL INVARIANCE & POLICY PREDICTION\nIN LARGE STATE SPACES\nWhen we extended our reasoning to large state spaces\nin Section 2.3 using a reduced state space model (see\nFig. S-1), we inevitably arrived at the following questions:\nBy modeling the expert behavior only along observed\ntrajectories, what does the resulting model imply for the\nremaining states of the state space? Can we still use it for\npredicting their local policies? The purpose of this section\nis to provide an in-depth discussion on the implications of\nthis reduced modeling approach in the context of policy\nprediction.\nWhen investigating the above-mentioned questions from a\nprobabilistic perspective (i.e. by analyzing the induced joint\ndistribution of our model), it turns out that they are strongly\nrelated to what is known as marginal invariance [1] (some-\ntimes also referred to as marginalization property or simply\nconsistency [2]). This property states that a model is consis-\ntent in the sense that it always provides the same marginal\ndistributions for any subset of its variables, irrespective of\nthe initial model size. In other words, a marginally invariant\npolicy model yields the same answer for the given trajectory\npoints, even if we include additional states into our reduced\nset eS for which we have not observed any demonstrations.\nFor our spatial models, that is, the Potts model and\nthe ddCRP, it can be shown that this consistency property\nis indeed lacking (see [1] for a detailed discussion). This\nmeans that we cannot expect to get compatible results when\nconducting our reduced model inference on two data sets\nof different sizes. On the contrary, making predictions for\nnew states would always require to rerun our Gibbs sampler\non the augmented data set, including all additional states.\nThis brings us to the following practical dilemma: imagine\nan on-line policy recognition scenario where we observe an\nexpert controlling our system. After a certain period of time,\nwe are asked to take over control, using the experience we\nhave acquired during the observation period. Each control\ncommand, whether performed by the expert or by us, will\ntrigger a new state transition, meaning that new data points\narrive sequentially one after another. Consequently, it is\nimpossible to decide in advance which states to include in\ns1\ns2\ns3\ns4\ns5\ns6\ns7\ns8\ns9\nz1\nz2\nz3\nz4\nz5\nz6\nz7\nz8\nz9\nS\n(a) reduced state space model\nst\nT\nst+1\npz\nT\nst+2\nat\nat+1\nat+2\nzt\nzt+1\nzt+2\n\u03c0\n\u03c0\n\u03c0\n\u0398\np\u03b8\n\u03b1\n(b) corresponding factor graph\nFig. S-1: (a) Illustration of the reduced state space model,\nwhich operates on the space eS = {s1, s2, . . . , sT } of visited\ntrajectory states. Note that the underlying decision-making\nprocess is assumed to be discrete in time; the continuous\ngray line shown in the \ufb01gure is only to indicate the temporal\nordering of the trajectory states. (b) Corresponding factor\ngraph, highlighting the circular dependence between the\nvariables. The factors are de\ufb01ned by the same building\nblocks that are used for the \ufb01nite state space model. Ob-\nserved variables are shaded in gray.\n2\n-15\n-10\n-5\n0\n5\n10\n15\n-15\n-10\n-5\n0\n5\n10\n15\n0.4\n0.6\n0.8\n1\n1.2\n1.4\n1.6\n1.8\nEMD\n(a) mean prediction error\n-15\n-10\n-5\n0\n5\n10\n15\n-15\n-10\n-5\n0\n5\n10\n15\n0.2\n0.4\n0.6\n0.8\n1\nEMD\n(b) standard deviation\n-15\n-10\n-5\n0\n5\n10\n15\n-15\n-10\n-5\n0\n5\n10\n15\n1\n2\n3\n4\n5\n6\n7\n8\ncontroller\n(c) sample partitioning\n0\n5\n10\n15\n20\nnumber of controllers\n0\n0.1\n0.2\n0.3\n0.4\nprobability\n(d) inferred model complexity\n0\n0.2\n0\nprobability\n0.4\n8\n0.6\n5\n7\n6\n10\n5\n4\n15\n3\nactions\n2\n20\n1\n25\ncontrollers\n(e) posterior sample of the local controllers\nFig. S-2: Simulation results for the ddCRP model on the continuous state space task described in Section 4.1. (a) Mean\nvalues of the spatial policy prediction error. (b) Standard deviations of the spatial policy prediction error. (c) Example\npartitioning of the state space, based on the local controllers depicted in sub-\ufb01gure (e). The expert partitioning is shown\nin Fig. 4 of the main paper. (d) Posterior distribution of the number of local controllers. (e) Posterior sample of the local\ncontrollers found by the model. The results in (a,b,d) are based on 100 Monte Carlo runs while (c,e) are obtained from a\nsingle posterior sample. The \ufb01gures in the top row were rendered using a spatial resolution of 2000x2000.\nour reduced space eS and which not. A rigorous approach in\nthe above-described sense would thus require to recalibrate\nthe model after each state transition \u2013 a costly operation.\nHowever, it is evident that the resulting data set is\nnaturally divided into two disjoint parts, namely the expert\ndemonstrations and the subsequent states reached during\nexecution of the learned policy. Clearly, transitions occurring\nafter the learning phase should by no means affect our\nbelief about the expert policy and, hence, they should be\ncompletely discarded from the model. The easiest way to\nachieve this is, indeed, to \u201cfreeze\u201d the model after the\ndemonstration phase and to use the learned parameters to\nextrapolate the gathered policy information to surrounding\nstates. This can be done, for instance, by retaining the\nstructure of the involved spatial prior model to compute the\nresulting maximum a posteriori estimates for the extrapolated\nindicators of the new states, based on the inferred model\nparameters. In the case of the ddCRP, this coincides with\nthe nearest-neighbor estimate (see Eq. (16)),\n\u02c6cnew = arg max\nt\u2208{1,...,T }\nf(dt,new) = arg min\nt\u2208{1,...,T }\ndt,new.\n(S-1)\nHerein, \u02c6cnew is the estimate for the indicator of the new\nstate and dt,new denotes the distance of that state to the tth\ntrajectory point.\nNow, one could argue that the comfort of retaining a\n\ufb01nite model structure for modeling inference problems on\ncountably in\ufb01nite or uncountable state spaces comes at the\ncost of not being able to provide a consistent posterior\npredictive distribution. However, the reduced state space\napproach allows us to incorporate the spatial information\nof the data in a fairly natural manner (i.e. in the form of\npairwise distances), providing an easy way to model the\nexpert behavior. Furthermore, our results demonstrate that\nthe reduced model is able to capture the relevant spatial\nproperties of a policy suf\ufb01ciently accurate in order to make\nprofound predictions about unseen states (see also sub-\nsequent section). Whether there exist alternative tractable\nmodels with similar properties remains to be seen.\nB\nADDITIONAL SIMULATION RESULTS\nIn this section, we provide additional simulation results\nfor the ddCRP model on the continuous state space task\ndescribed in Section 4.1.\nFigure S-2a visualizes the spatial EMD prediction errors\nof the trained model in the form of a heat map, which\ncompares the ground truth expert policy at non-trajectory\npoints with the mean prediction provided by our model.\nThe test points are placed on a regular grid of size 2000x2000\ncentered around the origin. The required indicator variables\n3\nat the interpolated states are computed according to Eq. (S-\n1). In line with our expectation, the prediction error reaches\nits maximum at the policy boundaries but is comparably\nsmall within each policy region, indicating a good model\n\ufb01t. Note that the \u201cwindmill shape\u201d of the error can be\nexplained as a result of the reduced state space approach in\ncombination with the inherent asymmetry of the used data\ngeneration scheme: regions of the state space containing tra-\njectory endings are locally underrepresented in the data set\n(see example trajectory in Fig. 4 in the paper); this increases\nthe chance of assigning the end points of a trajectory to the\ncluster of the preceding region, resulting in a smearing of\nthe previous cluster into the next region.\nAlso, we can observe that the variance of the error\n(Fig. S-2b) reaches its maximum at the transition regions\nand generally grows with the distance to the supporting\ntrajectory data, re\ufb02ecting the increasing prediction uncer-\ntainty at cluster boundaries and regions far from the expert\ndemonstrations. Both \ufb01gures were computed based on the\nlearned policy representations of 100 Monte Carlo runs. Fig-\nure S-2c illustrates an example state partitioning of one such\nexperiment, using the inferred local controllers depicted in\nFig. S-2e. The result reveals that all expert motion patterns\ncould be identi\ufb01ed by our model. Note, however, that the\ntwo \ufb01gures correspond to a single Gibbs sample of the\nprocess, which is not representative for the whole posterior\ndistribution. Averaging over several experiments as done in\nFig. S-2a and Fig. S-2b is not possible at the sample level due\nto the varying dimensionality of the corresponding policy\nrepresentations (i.e. the number of learned controllers). Even\ntaking averages over samples of equal dimensionality is\nnot meaningful due to the multimodality of the posterior\ndistribution, which arises from the inherent symmetry of\nthe representation (i.e. interchanging two local controllers\ntogether with their corresponding indices yields the same\nmodel). Hence, averaging samples is possible only at the\nprediction level.\nFinally, Fig. S-2d depicts the posterior distribution of the\nnumber of local controllers used by the model, which shows\na pronounced peak at the true number used by the expert.\nC\nCOMPUTATIONAL COMPLEXITY\nThe overall computational cost of performing inference in\nour model depends largely on two factors: the complexity\nper Gibbs iteration and the mixing speed of the underlying\nMarkov chain. Each Gibbs iteration consists of up to three\nstages: 1) sampling T categorical action variables {at} from\nthe set {1, . . . , |A|}, where T is the size of the demonstra-\ntion set; 2) ddCRP model: sampling NS categorical state\nassignments {ci} from the set {1, . . . , NS}, where NS is\nthe number of states (i.e. |S| or | eS|); remaining models:\nsampling NS categorical partition assignments {zi} from\nthe set {1, . . . , K}, where K is the number of local con-\ntrollers; 3) for non-collapsed models: sampling K Dirichlet-\ndistributed control parameters {\u03b8k} on the (|A| \u22121)-\nsimplex.\nCollapsing the control parameters generally improves\nthe mixing speed of the chain (see Fig. 5 in the paper) but\nrequires that action variables belonging to the same cluster\nbe updated sequentially; hence, a non-collapsed strategy can\nbe advantageous for larger data sets. Sampling the variables\n{at}, {\u03b8k} and {zi} is computationally cheap because the\ninvolved action likelihoods {T (s\u2032 | s, a)} as well as the\nneighborhood structure N (Potts model) and the similarity\nvalues {f(di,j)} can be pre-computed. The most demanding\noperation is the update of {ci}, which requires tracking\nthe connected components of the underlying ddCRP graph.\nUsing an appropriate graph representation, this can be done\nin polylogarithmic worst case time [3].\nREFERENCES\n[1] D. M. Blei and P. I. Frazier, \u201cDistance dependent Chinese restaurant\nprocesses,\u201d The Journal of Machine Learning Research, vol. 12, pp.\n2461\u20132488, 2011.\n[2] C. E. Rasmussen, \u201cGaussian processes for machine learning.\u201d\nMIT\nPress, 2006.\n[3] B. M. Kapron, V. King, and B. Mountjoy, \u201cDynamic graph con-\nnectivity in polylogarithmic worst case time,\u201d in Proc. 24th Annual\nACM-SIAM Symposium on Discrete Algorithms, 2013, pp. 1131\u20131142.\n",
        "sentence": " A similar model, where the state space is clustered according to the played actions, is proposed in [43].",
        "context": "take a similar form to those of the static model. Here, the\nonly difference is that we no longer group the actions by\ntheir states but according to their generating local policies\nor, equivalently, the clusters {Ck},\np(at | a\u2212t, s, z, \u0398, \u03b1)\nto use a model which preferably groups \u201csimilar\u201d states\ntogether (in other words, a model which favors clusterings\nthat assign those states the same local control parameter).\nSimilarity of states can be expressed, for example, by a\np\u03b8(\u03b8k | \u03b1)\nY\ni\u2032:zi\u2032=k\nY\nt:st=i\u2032\n\u03c0(at | \u03b8k) d\u0398\n\u221dp(zi | z\u2212i)\nK\nY\nk=1\n\uf8eb\n\uf8ed\nZ\n\u2206\np\u03b8(\u03b8k | \u03b1)\nY\nt:st\u2208Ck\n\u03c0(at | \u03b8k) d\u03b8k\n\uf8f6\n\uf8f8.\nHere, we \ufb01rst grouped the actions by their associated states\nand then grouped the states themselves by the clusters"
    },
    {
        "title": "Policy recognition via expectation maximization",
        "author": [
            "A. Sosic",
            "A.M. Zoubir",
            "H. Koeppl"
        ],
        "venue": "In Proceedings of the 41st IEEE International Conference on Acoustics, Speech and Signal Processing,",
        "citeRegEx": "44",
        "shortCiteRegEx": "44",
        "year": 2016,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " 3 Imitation Learning Instead of estimating the reward as in IRL, imitation learning aims at inferring the underlying policy directly [2, 44].",
        "context": null
    },
    {
        "title": "Bayesian nonparametric inverse reinforcement learning for switched Markov decision processes",
        "author": [
            "A. Surana",
            "K. Srivastava"
        ],
        "venue": "In Proceedings of the 13th International Conference on Machine Learning and Applications,",
        "citeRegEx": "45",
        "shortCiteRegEx": "45",
        "year": 2014,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " Different results on Bayesian nonparametrics for IRL, which is indirectly related to feature learning, are given in [27], where a partitioning of the state space is sought for, or [45], where complex behavior is decomposed into several, simpler behaviors that can be easily learned.",
        "context": null
    },
    {
        "title": "Constructing stochastic mixture policies for episodic multiobjective reinforcement learning tasks",
        "author": [
            "P. Vamplew",
            "R. Dazeley",
            "E. Barker",
            "A. Kelarev"
        ],
        "venue": "In Proceedings of the 22nd Australasian Joint Conference on Advances in Artificial Intelligence,",
        "citeRegEx": "46",
        "shortCiteRegEx": "46",
        "year": 2009,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " Mixture polices have been investigated in multiobjective problems, where an agent aims at reaching several objectives, some of which can even be conflicting [32, 46, 41].",
        "context": null
    },
    {
        "title": "A forward collision warning algorithm with adaptation to driver behaviors",
        "author": [
            "J. Wang",
            "C. Yu",
            "S.E. Li",
            "L. Wang"
        ],
        "venue": "IEEE Transactions on Intelligent Transportation Systems,",
        "citeRegEx": "47",
        "shortCiteRegEx": "47",
        "year": 2016,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " We consider the problem of analyzing a driver\u2019s behavior, which is an important task for useradaptive driver assistance systems [24, 47], in order to demonstrate the performance of the proposed model in a real-world scenario.",
        "context": null
    },
    {
        "title": "Learning from Delayed Rewards",
        "author": [
            "C.J.C.H. Watkins"
        ],
        "venue": "PhD thesis,",
        "citeRegEx": "48",
        "shortCiteRegEx": "48",
        "year": 1989,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " 1 Reinforcement Learning Large state and action spaces are especially problematic for value-based RL algorithms such as value-iteration [3] or Q-learning [48] since the value function, representing the expected accumulated reward, needs to be approximated.",
        "context": null
    },
    {
        "title": "Deep inverse reinforcement learning",
        "author": [
            "M. Wulfmeier",
            "P. Ondruska",
            "I. Posner"
        ],
        "venue": "In NIPS Deep Reinforcement Learning Workshop,",
        "citeRegEx": "49",
        "shortCiteRegEx": "49",
        "year": 2015,
        "abstract": "",
        "full_text": "",
        "sentence": " Recent attempts have been made to consider a Deep Learning (DL) architecture [49] for IRL, providing means for nonlinear, hierarchical feature learning.",
        "context": null
    }
]