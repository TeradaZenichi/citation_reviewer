,abstract,faithfulness,precision,recall,semantic_similarity,pdf,pdf-faithfulness,pdf-precision_recall,pdf-semantic_similarity,precision_recall,pdf-precision,pdf-recall
A survey of robot learning from demonstration,crossref,0.0,0.0,0.0,0.6584175361596859,,,,,,,
Robot learning from demonstration,crossref,0.0,0.0,0.0,0.6983786075540236,,,,,,,
Dynamic Programming,,,,,,,,,,,,
Exploiting structure in policy construction,,,,,,,,,,,,
Linear least-squares algorithms for temporal difference learning,crossref,0.0,0.0,0.0,0.6872725770540197,,,,,,,
Nonparametric Bayesian inverse reinforcement learning for multiple reward functions,,,,,,,,,,,,
Fast simulation of truncated Gaussian distributions,arxiv Library,0.5,0.5833333333333334,0.11290322580645161,0.8337770199127279,arxiv Library,0.5,,0.8391269799340393,,0.6666666666666666,0.11267605633802817
Feature reinforcement learning: State of the art,,,,,,,,,,,,
Learning the structure of factored Markov decision processes in reinforcement learning problems,crossref,0.0,0.0,0.0,0.6836705971658166,,,,,,,
Accelerated sampling for the Indian buffet process,crossref,0.0,0.0,0.0,0.6664422590600374,,,,,,,
Are we ready for autonomous driving? The KITTI vision benchmark suite,crossref,0.0,0.0,0.0,0.6954553575370416,,,,,,,
Discovering latent causes in reinforcement learning,crossref,0.0,0.0,0.0,0.674282257999425,,,,,,,
Infinite latent feature models and the Indian buffet process,,,,,,,,,,,,
Bayesian nonparametric latent feature models,crossref,0.8,0.4576271186440678,0.25,0.8350893376788389,,,,,,,
The Indian buffet process: An introduction and review,,,,,,,,,,,,
An introduction to variable and feature selection,,,,,,,,,,,,
Inverse reinforcement learning using expectation maximization in mixture models,crossref,0.0,0.0,0.0,0.7204854090933801,,,,,,,
Feature reinforcement learning: Part I,,,,,,,,,,,,
Spike and slab variable selection: Frequentist and Bayesian strategies,arxiv Library,0.7,0.5263157894736842,0.10752688172043011,0.8290001704245904,,,,,,,
Principal component analysis,,,,,,,,,,,,
Planning and acting in partially observable stochastic domains,crossref,0.0,0.0,0.0,0.6750705925227253,,,,,,,
Nonparametric Bayesian sparse factor models with application to gene expression modeling,arxiv Library,0.5,0.2,0.18333333333333332,0.8551064255149905,arxiv Library,0.5,,0.8799980896447838,,0.2545454545454545,0.208955223880597
Algorithms for non-negative matrix factorization,,,,,,,,,,,,
Driver behavior and situation aware brake assistance for intelligent vehicles,crossref,0.0,0.0,0.0,0.7028124694605725,,,,,,,
Finite Mixture Models,crossref,0.0,0.0,0.0,0.6968925088218507,,,,,,,
Modeling dyadic data with binary latent factors,crossref,0.0,0.0,0.0,0.6690081158073306,,,,,,,
Bayesian nonparametric inverse reinforcement learning,crossref,0.0,0.0,0.0,0.6990827974368655,,,,,,,
Bayesian variable selection in linear regression,crossref,0.0,0.0,0.0,0.6663123537116329,,,,,,,
Humanlevel control through deep reinforcement learning,,,,,,,,,,,,
Algorithms for inverse reinforcement learning,,,,,,,,,,,,
Online feature selection for model-based reinforcement learning,,,,,,,,,,,,
Policy gradient approaches for multi-objective sequential decision making,crossref,0.0,0.0,0.0,0.6817759858839005,,,,,,,
Efficient training of artificial neural networks for autonomous navigation,crossref,0.0,0.0,0.0,0.6899421311701672,,,,,,,
Markov Decision Processes: Discrete Stochastic Dynamic Programming,crossref,0.0,0.0,0.0,0.6699351236586452,,,,,,,
Beam search based map estimates for the Indian buffet process,,,,,,,,,,,,
Boosting structured prediction for imitation learning,crossref,0.0,0.0,0.0,0.705358677145046,,,,,,,
Maximum margin planning,crossref,0.0,0.0,0.0,0.7086418125736889,,,,,,,
Neural fitted Q iteration â€“ first experiences with a data efficient neural reinforcement learning method,crossref,0.0,0.0,0.0,0.6763755736538394,,,,,,,
A reduction of imitation learning and structured prediction to no-regret online learning,,,,,,,,,,,,
An MDP-based recommender system,,,,,,,,,,,,
Importance sampling for reinforcement learning with multiple objectives,,,,,,,,,,,,
The optimal control of partially observable Markov processes over a finite horizon,crossref,0.8,0.3448275862068966,0.23809523809523808,0.8674684313948037,,,,,,,
A Bayesian approach to policy recognition and state representation learning,arxiv Library,0.5,0.5333333333333333,0.07766990291262135,0.7487524669735398,arxiv Library,0.2,,0.8497982506696264,,0.5333333333333333,0.125
Policy recognition via expectation maximization,crossref,0.0,0.0,0.0,0.6983786075540236,,,,,,,
Bayesian nonparametric inverse reinforcement learning for switched Markov decision processes,crossref,0.0,0.0,0.0,0.6954921483610449,,,,,,,
Constructing stochastic mixture policies for episodic multiobjective reinforcement learning tasks,crossref,0.0,0.0,0.0,0.6817759858839005,,,,,,,
A forward collision warning algorithm with adaptation to driver behaviors,crossref,0.0,0.0,0.0,0.7028124694605725,,,,,,,
Learning from Delayed Rewards,crossref,0.0,0.0,0.0,0.6840463036533202,,,,,,,
Deep inverse reinforcement learning,,,,,,,,,,,,
