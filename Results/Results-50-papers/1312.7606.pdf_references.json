[
    {
        "title": "Cooperative off-policy prediction of Markov decision processes in adaptive networks",
        "author": [
            "S.V. Macua",
            "J. Chen",
            "S. Zazo",
            "A.H. Sayed"
        ],
        "venue": "Proc. IEEE Int. Conf. Acoustics, Speech, and Signal Processing (ICASSP), Vancouver, British Columbia, Canada, May 2013, pp. 4539\u20134543.",
        "citeRegEx": "1",
        "shortCiteRegEx": null,
        "year": 2013,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " A short preliminary version dealing with a special case of this work appears in the conference publication [1]. Note, however, that if all agents followed the same behavior policy, their individual optimization problems would be identical, therefore, both the adaptation and the combination steps would pull them toward the global solution and their fixed-point estimates would be unbiased with respect to the solution of the global optimization problem (31), as stated in [1].",
        "context": null
    },
    {
        "title": "Reinforcement Learning: An Introduction",
        "author": [
            "R.S. Sutton",
            "A.G. Barto"
        ],
        "venue": null,
        "citeRegEx": "2",
        "shortCiteRegEx": "2",
        "year": 1998,
        "abstract": "",
        "full_text": "",
        "sentence": " This problem of predicting the response to a target policy different from the behavior policy is commonly referred as off-policy learning [2]. The predictions by the agents are made in the form of value functions [2], [6], [7]. Value function In order to make predictions of the reward signal, we use state value functions, v : S \u2192 R, which provide the expected cumulative sum of the reward, weighted by an exponentially-decaying time window [2], [5]\u2013[7]. Then, some algebra will show that we can write (2) as a fixed point equation, known as the Bellman equation [2], [6], [7]: v(s) = E\u03c0,P [r(i+ 1) + \u03b3r(i+ 2) + .",
        "context": null
    },
    {
        "title": "Horde: a scalable real-time architecture for learning knowledge from unsupervised sensorimotor interaction",
        "author": [
            "R.S. Sutton",
            "J. Modayil",
            "M. Delp",
            "T. Degris",
            "P.M. Pilarski",
            "A. White",
            "D. Precup"
        ],
        "venue": "Proc. Int. Conf. on Autonomous Agents and Multiagent Systems (AAMAS), vol. 2, Taipei, Taiwan, 2011, pp. 761\u2013768. November 6, 2014  DRAFT  34",
        "citeRegEx": "3",
        "shortCiteRegEx": null,
        "year": 2011,
        "abstract": "",
        "full_text": "",
        "sentence": " Off-policy learning has been claimed to be necessary when the agents need to perform tasks in complex environments because they could perform many different predictions in parallel from a single stream of data [3]\u2013[5].",
        "context": null
    },
    {
        "title": "Scaling-up knowledge for a cognizant robot",
        "author": [
            "T. Degris",
            "J. Modayil"
        ],
        "venue": "Notes AAAI Spring Symposium Series, Palo Alto, CA, USA, 2012.",
        "citeRegEx": "4",
        "shortCiteRegEx": null,
        "year": 2012,
        "abstract": "",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "Multi-timescale nexting in a reinforcement learning robot",
        "author": [
            "J. Modayil",
            "A. White",
            "R.S. Sutton"
        ],
        "venue": "Adaptive Behavior, vol. 22, no. 2, pp. 146\u2013160, 2014.",
        "citeRegEx": "5",
        "shortCiteRegEx": null,
        "year": 2014,
        "abstract": " The term \u2018nexting\u2019 has been used by psychologists to refer to the propensity of people and many other animals to continually predict what will happen next in an immediate, local, and personal sense. The ability to \u2018next\u2019 constitutes a basic kind of awareness and knowledge of one\u2019s environment. In this paper we present results with a robot that learns to next in real time, making thousands of predictions about sensory input signals at timescales from 0.1 to 8 seconds. Our predictions are formulated as a generalization of the value functions commonly used in reinforcement learning, where now an arbitrary function of the sensory input signals is used as a pseudo reward, and the discount rate determines the timescale. We show that six thousand predictions, each computed as a function of six thousand features of the state, can be learned and updated online ten times per second on a laptop computer, using the standard temporal-difference( \u03bb) algorithm with linear function approximation. This approach is sufficiently computationally efficient to be used for real-time learning on the robot and sufficiently data efficient to achieve substantial accuracy within 30 minutes. Moreover, a single tile-coded feature representation suffices to accurately predict many different signals over a significant range of timescales. We also extend nexting beyond simple timescales by letting the discount rate be a function of the state and show that nexting predictions of this more general form can also be learned with substantial accuracy. General nexting provides a simple yet powerful mechanism for a robot to acquire predictive knowledge of the dynamics of its environment. ",
        "full_text": "",
        "sentence": " Off-policy learning has been claimed to be necessary when the agents need to perform tasks in complex environments because they could perform many different predictions in parallel from a single stream of data [3]\u2013[5]. Value function In order to make predictions of the reward signal, we use state value functions, v : S \u2192 R, which provide the expected cumulative sum of the reward, weighted by an exponentially-decaying time window [2], [5]\u2013[7].",
        "context": null
    },
    {
        "title": "Puterman, Markov Decision Processes: Discrete Stochastic Dynamic Programming",
        "author": [
            "L. M"
        ],
        "venue": null,
        "citeRegEx": "6",
        "shortCiteRegEx": "6",
        "year": 1994,
        "abstract": "",
        "full_text": "",
        "sentence": " The predictions by the agents are made in the form of value functions [2], [6], [7]. Markov decision processes (MDP) We consider Markov decision processes (MDP) [6], [7] that are characterized by a finite set of states S of size S , |S|; a finite set of actions A; the kernel of transition probabilities P(s\u2032|s, a), which gives the probability of going from one state s to another state s\u2032, given an action a; and the reward function r : S \u00d7A\u00d7 S \u2192 R that the agent wants to predict, which is associated with every transition, such that r (s, a, s\u2032) denotes the reward received by a generic agent for the transition from s to s\u2032 after taking action a. Then, some algebra will show that we can write (2) as a fixed point equation, known as the Bellman equation [2], [6], [7]: v(s) = E\u03c0,P [r(i+ 1) + \u03b3r(i+ 2) + .",
        "context": null
    },
    {
        "title": "Dynamic Programming and Optimal Control, 4th ed",
        "author": [
            "D.P. Bertsekas"
        ],
        "venue": "Athena Scientific,",
        "citeRegEx": "7",
        "shortCiteRegEx": "7",
        "year": 2012,
        "abstract": "",
        "full_text": "",
        "sentence": " The predictions by the agents are made in the form of value functions [2], [6], [7]. Markov decision processes (MDP) We consider Markov decision processes (MDP) [6], [7] that are characterized by a finite set of states S of size S , |S|; a finite set of actions A; the kernel of transition probabilities P(s\u2032|s, a), which gives the probability of going from one state s to another state s\u2032, given an action a; and the reward function r : S \u00d7A\u00d7 S \u2192 R that the agent wants to predict, which is associated with every transition, such that r (s, a, s\u2032) denotes the reward received by a generic agent for the transition from s to s\u2032 after taking action a. Value function In order to make predictions of the reward signal, we use state value functions, v : S \u2192 R, which provide the expected cumulative sum of the reward, weighted by an exponentially-decaying time window [2], [5]\u2013[7]. Then, some algebra will show that we can write (2) as a fixed point equation, known as the Bellman equation [2], [6], [7]: v(s) = E\u03c0,P [r(i+ 1) + \u03b3r(i+ 2) + .",
        "context": null
    },
    {
        "title": "A convergent O(n) temporal-difference algorithm for off-policy learning with linear function approximation",
        "author": [
            "R.S. Sutton",
            "C. Szepesvari",
            "H.R. Maei"
        ],
        "venue": "Proc. Advances in Neural Information Processing Systems (NIPS) 21, Vancouver, British Columbia, Canada, 2008, pp. 1609\u20131616.",
        "citeRegEx": "8",
        "shortCiteRegEx": null,
        "year": 2008,
        "abstract": "",
        "full_text": "",
        "sentence": " It was originally proposed for the single agent scenario in [8], [9], and derived by means of the stochastic optimization of a suitable cost function. The convergence analysis of reinforcement learning algorithms is usually challenging even for the single-agent case, and studies are often restricted to the case of diminishing step-sizes [8], [9], [23]. Approximate value function as a saddle-point problem For the single agent scenario, references [8], [9] introduced efficient algorithms with convergence guarantees under general conditions. , [8], [9], [48]) that simplifies the analysis because the tuples {xk,i,ak(i),xk,i+1, rk(i)} become i.",
        "context": null
    },
    {
        "title": "Fast gradient-descent methods for temporal-difference learning with linear function approximation",
        "author": [
            "R.S. Sutton",
            "H.R. Maei",
            "D. Precup",
            "S. Bhatnagar",
            "D. Silver",
            "C. Szepesvari",
            "E. Wiewiora"
        ],
        "venue": "Proc. Int. Conf. on Machine Learning (ICML), Montreal, Quebec, Canada, 2009, pp. 993\u20131000.",
        "citeRegEx": "9",
        "shortCiteRegEx": null,
        "year": 2009,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " It was originally proposed for the single agent scenario in [8], [9], and derived by means of the stochastic optimization of a suitable cost function. As a byproduct of this derivation, we show that the GTD algorithm, motivated as a two time-scales stochastic approximation in [9], is indeed a stochastic Arrow-Hurwicz algorithm applied to the dual problem of the original formulation. The convergence analysis of reinforcement learning algorithms is usually challenging even for the single-agent case, and studies are often restricted to the case of diminishing step-sizes [8], [9], [23]. Approximate value function as a saddle-point problem For the single agent scenario, references [8], [9] introduced efficient algorithms with convergence guarantees under general conditions. [9] considered the weighted least-squares problem: In the process of doing so, first for single-agents, we shall arrive at the same gradient temporal difference method of [9] albeit by using a fundamentally different approach involving a primal-dual argument. Recursions (29a)\u2013(29b) coincide with the single-agent gradienttemporal difference (GTD2) algorithm, which was derived in [9] using a different approach. , [8], [9], [48]) that simplifies the analysis because the tuples {xk,i,ak(i),xk,i+1, rk(i)} become i. CONCLUSION Diffusion GTD maintains the efficiency of the single-agent GTD2 [9], with linear complexity in both computation time and memory footprint.",
        "context": null
    },
    {
        "title": "Distributed asynchronous deterministic and stochastic gradient optimization algorithms",
        "author": [
            "J. Tsitsiklis",
            "D. Bertsekas",
            "M. Athans"
        ],
        "venue": "IEEE Transactions on Automatic Control, vol. 31, no. 9, pp. 803\u2013812, 1986.",
        "citeRegEx": "10",
        "shortCiteRegEx": null,
        "year": 1986,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " There are several distributed strategies that can be used for this purpose, such as consensus [10]\u2013[14] and diffusion strategies [15]\u2013[18].",
        "context": null
    },
    {
        "title": "Distributed subgradient methods for multi-agent optimization",
        "author": [
            "A. Nedic",
            "A. Ozdaglar"
        ],
        "venue": "IEEE Transactions on Automatic Control, vol. 54, no. 1, pp. 48\u201361, 2009.",
        "citeRegEx": "11",
        "shortCiteRegEx": null,
        "year": 2009,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "Convergence rate analysis of distributed gossip (linear parameter) estimation: Fundamental limits and tradeoffs",
        "author": [
            "S. Kar",
            "J. Moura"
        ],
        "venue": "IEEE Journal of Selected Topics in Signal Processing, vol. 5, no. 4, pp. 674\u2013690, 2011.",
        "citeRegEx": "12",
        "shortCiteRegEx": null,
        "year": 2011,
        "abstract": "The paper considers gossip distributed estimation of a (static) distributed\nrandom field (a.k.a., large scale unknown parameter vector) observed by\nsparsely interconnected sensors, each of which only observes a small fraction\nof the field. We consider linear distributed estimators whose structure\ncombines the information \\emph{flow} among sensors (the \\emph{consensus} term\nresulting from the local gossiping exchange among sensors when they are able to\ncommunicate) and the information \\emph{gathering} measured by the sensors (the\n\\emph{sensing} or \\emph{innovations} term.) This leads to mixed time scale\nalgorithms--one time scale associated with the consensus and the other with the\ninnovations. The paper establishes a distributed observability condition\n(global observability plus mean connectedness) under which the distributed\nestimates are consistent and asymptotically normal. We introduce the\ndistributed notion equivalent to the (centralized) Fisher information rate,\nwhich is a bound on the mean square error reduction rate of any distributed\nestimator; we show that under the appropriate modeling and structural network\ncommunication conditions (gossip protocol) the distributed gossip estimator\nattains this distributed Fisher information rate, asymptotically achieving the\nperformance of the optimal centralized estimator. Finally, we study the\nbehavior of the distributed gossip estimator when the measurements fade (noise\nvariance grows) with time; in particular, we consider the maximum rate at which\nthe noise variance can grow and still the distributed estimator being\nconsistent, by showing that, as long as the centralized estimator is\nconsistent, the distributed estimator remains consistent.",
        "full_text": "arXiv:1011.1677v1  [cs.IT]  7 Nov 2010\nConvergence Rate Analysis of Distributed\nGossip (Linear Parameter) Estimation:\nFundamental Limits and Tradeoffs\nSoummya Kar and Jos\u00b4e M. F. Moura\u2217\nAbstract\nThe paper considers gossip distributed estimation of a (static) distributed random \ufb01eld (a.k.a., large\nscale unknown parameter vector) observed by sparsely interconnected sensors, each of which only ob-\nserves a small fraction of the \ufb01eld. We consider linear distributed estimators whose structure combines the\ninformation \ufb02ow among sensors (the consensus term resulting from the local gossiping exchange among\nsensors when they are able to communicate) and the information gathering measured by the sensors (the\nsensing or innovations term.) This leads to mixed time scale algorithms\u2013one time scale associated with the\nconsensus and the other with the innovations. The paper establishes a distributed observability condition\n(global observability plus mean connectedness) under which the distributed estimates are consistent\nand asymptotically normal. We introduce the distributed notion equivalent to the (centralized) Fisher\ninformation rate, which is a bound on the mean square error reduction rate of any distributed estimator;\nwe show that under the appropriate modeling and structural network communication conditions (gossip\nprotocol) the distributed gossip estimator attains this distributed Fisher information rate, asymptotically\nachieving the performance of the optimal centralized estimator. Finally, we study the behavior of the\ndistributed gossip estimator when the measurements fade (noise variance grows) with time; in particular,\nwe consider the maximum rate at which the noise variance can grow and still the distributed estimator\nbeing consistent, by showing that, as long as the centralized estimator is consistent, the distributed\nestimator remains consistent.\nKeywords: Distributed estimation, gossip, random networks, sensor networks, link failures, switching\ntopology\nThe \ufb01rst author is with the Dep. Electrical Engineering, Princeton University, Princeton, NJ. This work was performed\nwhile the \ufb01rst author was with the Dep. Electrical and Computer Engineering, Carnegie Mellon University. The second\nauthor is with the Dep. Electrical and Computer Engineering, Carnegie Mellon University, Pittsburgh, PA 15213, USA (e-mail:\nsoummyak@andrew.cmu.edu, moura@ece.cmu.edu, ph: (412)268-6341, fax: (412)268-3890.)\nWork partially supported by AFOSR grant # FA95501010291; and by NSF grant # CCF1011903.\n2\nI. INTRODUCTION\nA. Motivation\nWe consider distributed (or decentralized) estimation of a random \ufb01eld where observations are collected\nby possibly a large number of sparsely internetworked sensors. The network operates under the gossip\nrandom protocol and may be subject to random infrastructure failures (communication channels may fail\nintermittently.) There is no fusion-center and the estimation is performed locally at each sensor with inter-\nsensor message exchanges occurring at random times. Because the random \ufb01eld of interest is distributed,\neach sensor can only observe a part of the \ufb01eld, and no sensor can in isolation obtain a reasonable estimate\nof the entire \ufb01eld. This paper studies the conditions under which the distributed algorithms operating under\nthe random intermittent conditions (gossip and link failures) that we consider can achieve (asymptotically)\nperformance that is equivalent to the estimation performance of centralized optimal algorithms. To be\nmore concrete and as an abstraction of the environment1, we model it by a static vector parameter, whose\ndimension, M, can be arbitrarily large. Each sensor\u2019s observations, say for sensor n, are Mn dimensional\nnoisy measurements of a part of the (static random) \ufb01eld, where Mn \u226aM. We assume that the sensing\nrate, i.e., rate of receiving observations at each sensor, is comparable to the communication rate among\nsensors, so that sensors update their estimate at time index i by fusing appropriately their current estimate\nwith the observation (innovation) at i and the estimates at i received from those sensors with which it\nsuccessfully gossips at i. Because of the communication intermittency, the distributed estimators that we\nconsider exhibit mixed time scales: one associated with the consensus, i.e., mixing estimation updating\nresulting from receiving the estimates from the neighbors; and the other associated with the sensing or\nestimation updating from the innovations. In this paper, we consider a general class of linear distributed\ngossip networked estimators and study the conditions under which they exhibit the same estimation error\nconvergence rate as a centralized linear \ufb01eld estimator. Nonlinear distributed estimators and distributed\nestimation of time varying random \ufb01elds under the gossip protocol are considered elsewhere, [1] and [2],\nrespectively.\nWe discuss the major challenges in gossip distributed estimation and highlight the key contributions\nof the paper:\n\u2022 Infrastructure failures and gossip communication: The inter-sensor communication may be band-\nwidth and power constrained and subject to random environmental conditions. For example, the\nsensors may share a common wireless medium and, due to competing objectives, the inter-sensor\ntransmissions may be scheduled by the underlying MAC (Medium Access Control) layer to occur\nat random times; in fact, in many situations of interest, the exact medium access (MAC) protocol\n1The term environment or \ufb01eld has a generic usage here. It may correspond to sensors deployed over a domain of interest\nlike a temperature surface, or, a networked physical system instrumented with sensors. Typical examples of the latter include\ncyberphysical systems like the power grid, and networked control systems (NCS), where a network of distributed actuators are\nequipped with sensors.\n3\n(randomized) is not known or determined `apriori, the inter-sensor communications is asynchronous,\nand random data packet dropouts may occur.\n\u2022 Distributed observability: It is well known that centralized estimation requires observability condi-\ntions to be satis\ufb01ed for the estimation task to be successful2. As we will see, formulating a satisfactory\nnotion of distributed observability is not trivial. A dif\ufb01culty stems from the distributed nature of\nthe information, i.e., sensors observe only a portion of the \ufb01eld of interest. The incorporation of\nestimate fusion among the sensor nodes (consensus) together with local innovation updates suggest\nthat distributed observability should be not only a function of the sensor observations, but closely\ntied to the structural properties of the communication network governing the information \ufb02ow. These\nconditions are sensitive to the pattern of information dissemination in the network and depends on\nthe level of node cooperation, for example, gossiping. We present minimal conditions for distributed\nobservability, namely, for example, in the case of full cooperation (each node exchanges its entire\nestimate with its neighbors), we show that global observability3 and mean connectedness of the time\nvarying communication graph are suf\ufb01cient to ensure consistent parameter estimates at each sensor.\n\u2022 Distributed versus optimal centralized estimation: We show that under reasonable assumptions,\nthe gossip distributed estimators we develop, like the centralized optimal estimator, lead to consistent\nparameter estimates at each sensor. The natural question of interest is to compare the rate of\nconvergence of these schemes to the true parameter value. We adopt asymptotic normality and\nthe associated asymptotic variance as the metric for comparing different estimators. It is known\nfrom the theory of recursive estimation (centralized), that the optimum centralized estimator (under\nreasonable assumptions) achieves asymptotic variance equal to the Fisher information rate. In this\npaper, we formalize a notion of distributed Fisher information rate, i.e., a lower bound on the\nasymptotic variance of all distributed schemes and also investigate the existence of optimal distributed\nestimators achieving this lower bound. It turns out that, if the inter-sensor communication is noisy or\nquantized, the asymptotic variance of distributed estimators is always higher than their centralized\ncounterpart. On the other hand, a remarkable asymptotic time scale separation phenomenon shows\nthat, in the absence of channel noise or quantization (but presence of random link failures and\ngossip,) there exist distributed estimation schemes whose asymptotic variance equals the centralized\nFisher information rate under pragmatic conditions. In particular, it is shown that, in a Gaussian\nenvironment, a distributed estimator is equivalent to a centralized one in terms of asymptotic variance,\nand, more generally, equivalent to the best linear centralized estimator. This is signi\ufb01cant, as it shows\nthat, under reasonable assumptions, a distributed gossip estimator is as good as a centralized one, the\nlatter having access to all sensor observations at all times. We present some intuitive remarks. In a\n2Successful means the estimate sequence generated over time possesses desirable properties like consistency, asymptotic\nnormality etc.\n3Global observability corresponds to the centralized setting, where an estimator has access to the observations of all sensors\nat all times. The assumption of global observability does not mean that each sensor is observable; rather, that if there was a\ncentralized estimator with simultaneous access to all the sensor measurements, this centralized estimator would be observable.\n4\ncentralized recursive (parameter) estimation scheme, the estimate update rule involves combining the\npast estimate with the new innovation (observation), the key design parameter being the time varying\ngain or weight associated to the innovation term. Since, the observations are noisy, for parameter\nestimation, this weight sequence needs to go to zero for achieving convergence and, in fact, needs\nto be square summable to constrain the effect of the observation noise. In most cases, assuming\nindependent observations over time, the innovation gains decrease as 1/i (i being the iteration or\ntime index) for optimal estimation performance. This means that the estimation uncertainty cannot\nbe reduced at a rate 1/\n\u221a\ni, a consequence of central limit theorem type arguments. Now, consider\nthe distributed scheme. Here, the algorithm design involves two gain sequences, one for the local\ninnovations at each sensor and the other for estimate fusion (consensus) across sensors. To design\ngood performance distributed gossip estimators, the trick is in choosing the fusion or consensus\ngain properly, so that its effect decays at a slower rate than the innovation gain. In the absence of\nquantization or channel noise, it is possible to choose the consensus weight sequence such that its\nsquared sum goes to \u221e, in contrast to the innovation weight sequence whose squared sum needs to\nbe \ufb01nite. It is shown that this tuning of the different gain sequences leads to an asymptotic time scale\nseparation, the rate of information dissemination dominating the rate of reduction of uncertainty by\nobservation acquisition. This tuning is not possible in the case of quantized or noisy transmissions,\nas each consensus step introduces noise, preventing proper adjustment of the gain sequences. The\nanalysis approach that we develop is of independent interest and contributes to the theory of mixed\ntime scale stochastic approximation.4 Related to our mixed time scale algorithms is the work [4],\nwhich develops methods to analyze such algorithms in the context of simulated annealing. In [4] the\nrole of our innovation potential is played by a martingale difference term. However, in our paper, an\nadditional dif\ufb01culty with respect to [4] is that the innovation is not a martingale difference process,\nand so a key step in our analysis is to derive pathwise strong approximation results to characterize\nthe rate at which the innovation process converges to a martingale difference process.\nBrief review of the literature. We comment on the relevant literature. An early treatment of distributed\nstochastic algorithms appears in [5] (see also [6], [7], [8].) In [5], almost sure convergence is established\nfor a class of distributed stochastic algorithms in the context of distributed optimization. This line of\nwork assumes the existence of a \ufb01xed time window T, such that the union of communication graphs\nover any interval of length T is connected with probability one. Also, the stochastic noise appears only in\nthe computation of the local gradients that play the role of innovations in our approach. The conditions\nimposed on the local gradients are rather strong and implicitly assume that the individual processor\n(sensor in our terminology) dynamics are stable. Some of these conditions are relaxed in [8], which\nderives almost sure convergence and asymptotic normality for a class of constrained and unconstrained\n4By mixed time scale, we refer to stochastic algorithms where two potentials act in the same update step with different weight\nor gain sequences. This should not be confused with stochastic algorithms with coupling (see [3]), where a quickly switching\nparameter in\ufb02uences the relatively slower dynamics of another state, leading to averaged dynamics.\n5\nparallel and communicating stochastic procedures with perfect communication. On the contrary, the gossip\ndistributed estimators we develop in this paper are general mixed time-scale procedures in generic random\nenvironments and provide pathwise strong convergence rates. Our work does not impose local conditions\non the innovation processes and develops and infers connective stability based on structural network\nconditions and global observability and establishes strong invariance results relating network information\n\ufb02ow and the effect of local innovations.\nMore recently, there has been renewed interest in distributed approaches motivated by wireless sensor\nnetworks (WSN) applications. The papers [9], [10], [11], [12] study the estimation problem in static\nnetworks, where either the sensors take a single snapshot of the \ufb01eld at the start and then initiate distributed\nconsensus protocols (or more generally distributed optimization, as in [10]) to fuse the initial estimates,\nor the observation rate of the sensors is assumed to be much slower than the inter-sensor communicate\nrate, thus permitting a separation of the two time-scales. More relevant to our work are [13], [14], [15],\n[16], which consider the linear estimation problem in non-random networks, where the observation and\nconsensus protocols are incorporated in the same iteration. In [13], [15], the distributed linear estimation\nproblems are treated in the context of distributed least-mean-square (LMS) \ufb01ltering, where constant weight\nsequences are used to prove mean-square stability of the \ufb01lter. The use of non-decaying combining weights\nin [13], [15], [16] leads to a residual error; however, under appropriate assumptions, these algorithms can\nbe adapted for tracking certain time-varying parameters. The distributed LMS algorithm in [14] considers\ndecaying weight sequences, thereby establishing L2 convergence to the true parameter value. In contrast\nto these, our work quanti\ufb01es the pathwise information dissemination rate and its relation to the innovation\nrate by studying general mixed time-scale procedures. We consider structural conditions based on the\nnetwork topology and observation pattern to develop a satisfactory notion of distributed observability and\nprovide fundamental limits on the performance of distributed schemes.\nThe key difference between the current paper and the linear algorithm LU in [1] involves the use of\ndifferent weight sequences for the consensus and the innovation terms, giving to the linear distributed\nestimators here a mixed time scale behavior. On the other hand, in this paper, we assume unquantized\ntransmissions in the distributed gossip estimators. Another difference that will be noted below is the\nincorporation of a general matrix gain K into the innovation update. These modi\ufb01cations make the\ntechnical analysis of the distributed gossip linear estimators in this paper highly non-trivial and very\ndistinct from the analysis of LU in [1].\nWe brie\ufb02y comment on the organization of the rest of the paper. Section I-B sets up notation and\npreliminary concepts to be used throughout the paper. Section II formulates the distributed estimation\nproblem, introduces the algorithm GLU and the assumptions (Section II-A.) Some technical results on\nthe convergence of stochastic recurrences are established in Section III. This section also considers some\nproperties of centralized estimators, with which we compare our distributed scheme. The main results\nof the paper are stated in Section IV. Section V develops convergence properties of the GLU algorithm,\nleading to the proofs of the main theorems in Section VI. Finally, Section VII concludes the paper.\n6\nB. Notation\nWe denote the k-dimensional Euclidean space by Rk. The set of m \u00d7 n matrices with real entries\nis denoted by Rm\u00d7n. SN, SN\n+, SN\n++ refer to the subsets of symmetric, positive semide\ufb01nite, positive\nde\ufb01nite matrices in RN\u00d7N respectively. The k \u00d7 k identity matrix is denoted by Ik, while 1k, 0k denote\nrespectively the column vector of ones and zeros in Rk. The set of integers is denoted by T, whereas N\nstands for the natural numbers. T+ denotes the set of nonnegative integers and indices the iteration time\nslots throughout the paper.\nDe\ufb01ne the rank one k \u00d7 k matrix Pk by\nPk = 1\nk1k1T\nk\n(1)\nThe only non-zero eigenvalue of Pk is one, and the corresponding normalized eigenvector is\n\u0010\n1/\n\u221a\nk\n\u0011\n1k.\nThe operator \u2225\u00b7\u2225applied to a vector denotes the standard Euclidean 2-norm, while applied to matrices\ndenotes the induced 2-norm, which is equivalent to the matrix spectral radius for symmetric matrices.\nWe assume that the parameter to be estimated belongs to a subset U of the Euclidean space RM.\nThroughout the paper, the true (but unknown) value of the parameter is denoted by \u03b8\u2217. We denote a\ncanonical element of U by \u03b8. The estimate of \u03b8\u2217at time i at sensor n is denoted by xn(i) \u2208RM\u00d71.\nWithout loss of generality, we assume that the initial estimate, xn(0), at time 0 at sensor n is a non-random\nquantity.\nThroughout, we assume that all the random objects are de\ufb01ned on a common measurable space, (\u2126, F).\nIn case the true (but unknown) parameter value is \u03b8\u2217, the probability and expectation operators are denoted\nby P\u03b8\u2217[\u00b7] and E\u03b8\u2217[\u00b7], respectively. When the context is clear, we abuse notation by dropping the subscript.\nAlso, all inequalities involving random variables are to be interpreted a.s. (almost surely.)\nSpectral graph theory. We review elementary concepts from spectral graph theory. For an undirected\ngraph G = (V, E), V = [1 \u00b7 \u00b7 \u00b7 N] is the set of nodes or vertices, |V | = N, and E is the set of edges,\n|E| = M, where | \u00b7 | is the cardinality. The unordered pair (n, l) \u2208E if there exists an edge between\nnodes n and l. We only consider simple graphs, i.e., graphs devoid of self-loops and multiple edges. A\ngraph is connected if there exists a path5, between each pair of nodes. The neighborhood of node n is\n\u2126n = {l \u2208V | (n, l) \u2208E}\n(2)\nNode n has degree dn = |\u2126n| (number of edges with n as one end point.) The structure of the graph\nis described by the symmetric N \u00d7 N adjacency matrix, A = [Anl], Anl = 1, if (n, l) \u2208E, Anl = 0,\notherwise. The degree matrix is the diagonal matrix D = diag (d1 \u00b7 \u00b7 \u00b7 dN). The graph positive semi-de\ufb01nite\n5A path between nodes n and l of length m is a sequence (n = i0, i1, \u00b7 \u00b7 \u00b7 , im = l) of vertices, such that, (ik, ik+1) \u2208\nE \u22000 \u2264k \u2264m \u22121.\n7\nLaplacian matrix, L, and its ordered eigenvalues are\nL\n=\nD \u2212A\n(3)\n0 = \u03bb1(L) \u2264\u03bb2(L) \u2264\n\u00b7 \u00b7 \u00b7\n\u2264\u03bbN(L)\n(4)\nThe smallest eigenvalue \u03bb1(l) is always equal to zero, with\n\u0010\n1/\n\u221a\nN\n\u0011\n1N being the corresponding nor-\nmalized eigenvector. The multiplicity of the zero eigenvalue equals the number of connected components\nof the network; for a connected graph, \u03bb2(L) > 0. This second eigenvalue is the algebraic connectivity\nor the Fiedler value of the network; see [17], [18], [19] for detailed treatment of graphs and their spectral\ntheory.\nKronecker product: Since, we are dealing with vector parameters, most of the matrix manipulations\nwill involve Kronecker products. For example, the Kronecker product of the N \u00d7N matrix L and IM will\nbe an NM\u00d7NM matrix, denoted by L\u2297IM. Denote the NM\u00d7NM matrix P NM = PN\u2297IM = 1\nN (1N\u2297\nIM)(1N \u2297IM)T . We will deal often with matrices of the form C =\n\u0002\nINM \u2212bL \u2297IM \u2212aINM \u2212P NM\u0003\n,\nL being a graph Laplacian matrix. It follows from the properties of Kronecker products and the matrices\nL, P NM, that the eigenvalues of this matrix C are \u2212a and 1 \u2212b\u03bbn(L) \u2212a, n \u2264i \u2264N, each being\nrepeated M times.\nII. PROBLEM FORMULATION\nLet \u03b8\u2217\u2208RM\u00d71 be an M-dimensional parameter that is to be estimated by a network of N sensors.\nWe refer to \u03b8 as a parameter, although it is a vector of M parameters. Each sensor makes independent\nobservations of noise corrupted linear functions of the parameter. We assume the following observation\nmodel for the n-th sensor:\nzn(i) = Hn(i)\u03b8\u2217+ \u03b3(i)\u03b6n(i)\n(5)\nwhere:\n\b\nzn(i) \u2208RMn\u00d71\t\ni\u22650 is the independent observation sequence for the n-th sensor; {\u03b6n(i)}i\u22650 is a\nzero-mean i.i.d. noise sequence of bounded variance. For most practical sensor network applications, each\nsensor observes only a subset of Mn of the components of \u03b8, with Mn \u226aM. Under such a situation, in\nisolation, each sensor can estimate at most only a part of the parameter. However, if the sensor network\nis connected in the mean sense (see assumption (A.3)), and under appropriate observability conditions,\nwe will show that it is possible for each sensor to get a consistent estimate of the parameter \u03b8\u2217by means\nof local inter-sensor communication.\nWe formalize the assumptions on global observability, fading signal characteristics and network con-\nnectivity:\n\u2022 (A.1)Observation Noise: Recall the observation model in eqn. (5). We assume that the process,\nn\n\u03b6(i) =\n\u0002\n\u03b6T\n1 (i), \u00b7 \u00b7 \u00b7 , \u03b6T\nN(i)\n\u0003T o\ni\u22650 is an i.i.d. zero mean process, with \ufb01nite second moment. The\nobservation noise process, {\u03b3(i)\u03b6(i)}, then has non-stationary (in general) characteristics, with\nvariance increasing as \u03b32(i) over time. The non-decreasing sequence {\u03b3(i)} models the fading\n8\ncharacteristics of the parameter (signal) over time. In particular, the regime \u03b3(i) \u2192\u221ecorresponds\nto the SNR decreasing as 1/\u03b32(i) over time, whereas, \u03b3(i) = 1 for all i recovers the case of\ni.i.d. (constant SNR) observations. Also, note that the observation noises at different sensors may\nbe correlated during a particular iteration, we require only temporal independence. The spatial\ncorrelation of the observation noise makes our model applicable to practical sensor network problems,\nfor instance, for distributed target localization, where the observation noise is generally correlated\nacross sensors.\nThe following assumption on the growth rate of {\u03b3(i)} is imposed throughout:\nThere exists, 0 \u2264\u03b30 < .5, such that,\n\u03b3(i) = (i + 1)\u03b30,\n\u2200i \u2208T+\n(6)\nIn other words, we assume that the observation noise variance has sublinear growth. The sublinear\ngrowth assumption is not restrictive, and as shown in Remark 8 is in fact, necessary for centralized\nestimators to yield consistent estimates of the parameter.\n\u2022 (A.2)Observability: We require the following global observability condition. The matrix G\nG =\nN\nX\nn=1\nH\nT\nnHn\n(7)\nis full-rank. This distributed observability extends the observability condition for a centralized\nestimator to get a consistent estimate of the parameter \u03b8\u2217.\n\u2022 (A.3)Random Link Failure: In digital communications, packets may be lost at random times. To\naccount for this, we let the links (or communication channels among sensors) to fail, so that the\nedge set and the connectivity graph of the sensor network are time varying. Accordingly, the sensor\nnetwork at time i is modeled as an undirected graph, G(i) = (V, E(i)) and the graph Laplacians as\na sequence of i.i.d. Laplacian matrices {L(i)}i\u22650. We write\nL(i) = L + eL(i), \u2200i \u22650\n(8)\nwhere the mean L = E [L(i)]. We do not make any distributional assumptions on the link failure\nmodel. Although the link failures, and so the Laplacians, are independent at different times, during the\nsame iteration, the link failures can be spatially dependent, i.e., correlated. This is more general and\nsubsumes the erasure network model, where the link failures are independent over space and time.\nWireless sensor networks motivate this model since interference among the wireless communication\nchannels correlates the link failures over space, while, over time, it is still reasonable to assume that\nthe channels are memoryless or independent.\nConnectedness of the graph is an important issue. We do not require that the random instanti-\nations G(i) of the graph be connected; in fact, it is possible to have all these instantiations to\nbe disconnected. We only require that the graph stays connected on average. This is captured by\n9\nrequiring that \u03bb2\n\u0000L\n\u0001\n> 0, enabling us to capture a broad class of asynchronous communication\nmodels; for example, the random asynchronous gossip protocol analyzed in [20] satis\ufb01es \u03bb2\n\u0000L\u0001\n> 0\nand hence falls under this framework.\n\u2022 (A.4)Independence Assumptions: The sequences {L(i)}i\u2208T+ and {\u03b6(i)}i\u2208T+ are mutually inde-\npendent.\nIn Section II-A, we present the algorithm GLU for distributed parameter estimation with the linear\nobservation model (5). Starting from some initial deterministic estimate of the parameters (the initial\nstates may be random, we assume deterministic for notational simplicity), xn(0) \u2208RM\u00d71, each sensor\ngenerates by a distributed iterative algorithm a sequence of estimates, {xn(i)}i\u22650. The parameter estimate\nxn(i+1) at the n-th sensor at time i+1 is a function of: its previous estimate; the communicated estimates\nat time i of its neighboring sensors; and the new observation zn(i).\nA. Algorithm GLU\nAlgorithm GLU: Consider the parameter estimation problem with linear observation model (assump-\ntions (A.1)-(A.2)). Let x(0) = [x1(0)T , \u00b7 \u00b7 \u00b7 , xN(0)T ]T be the initial estimates of \u03b8\u2217at the sensors. The\nGLU algorithm updates the estimate xn(i) at sensor n according to the following:\nxn(i + 1) = xn(i) \u2212\u03b2(i)\nX\nl\u2208\u2126n(i)\n(xn(i) \u2212xl(i)) + \u03b1(i)KH\nT\nn\n\u0000zn(i) \u2212Hnxn(i)\n\u0001\n(9)\nThe key difference between the above scheme and the LU in [1] involves the use of different weight\nsequences for the consensus and the innovation terms, giving the former a mixed time scale behavior. On\nthe other hand, we assume unquantized transmissions in GLU. Another difference is the incorporation\nof a general matrix gain K into the innovation update. These modi\ufb01cations make the technical analysis\nof GLU highly non-trivial and different from that of LU, mostly due to the incorporation of mixed time\nscale dynamics.\nIn a compact notation, GLU may be written as:\nx(i + 1) = x(i) \u2212\u03b2(i) (L(i) \u2297IM) x(i) + \u03b1(i) (IN \u2297K) DH\n\u0000z(i) \u2212DHx(i)\n\u0001\n(10)\nWe refer to the class of distributed recursive estimation algorithms in (9) as GLU. As will be shown,\ndifferent choices of the weight sequences {\u03b1(i)}, {\u03b2(i)} lead to different convergence characteristics of\nGLU, hence the usage of the term \u2018class of algorithms\u2019. In the following, we introduce some additional\nmoment requirements and assumptions on the algorithm weight sequences:\n\u2022 (A.5)Moment Condition: There exists \u03b51 > 0, such that, the following moment exists:\nE\u03b8\nh\n\u2225\u03b6(i)\u22252+\u03b51i\n< \u221e\n(11)\n10\nThe above implies the existence of a positive function \u03ba1(\u00b7), such that,\nE\u03b8\n\"\r\r\r\rDHz(i) \u22121N \u2297\n\u0012\u0012 1\nN 1N \u2297IM\n\u0013\nDHz(i)\n\u0013\r\r\r\r\n2+\u03b51#\n\u2264\u03b32+\u03b51(i)\u03ba1(\u03b8) < \u221e\n(12)\nfor all i \u2208T+. We thus assume the existence of slightly greater than quadratic moment of the\nobservation noise process.\n\u2022 (A.6)Weight sequences: The sequences {\u03b1(i)} and {\u03b2(i)} are of the form:\n\u03b1(i) =\na\n(i + 1)\u03c41 ,\n\u03b2(i) =\nb\n(i + 1)\u03c42\n(13)\nwhere a, b > 0, 0 < \u03c42 \u2264\u03c41 \u22641. In addition, the weights satisfy the following condition:\n\u03c41 > max\n\u0012\n.5 + \u03b30, \u03c42 + \u03b30 +\n1\n2 + \u03b51\n\u0013\n(14)\nwhere max(\u00b7) denotes the maximum of .5 + \u03b30 and \u03c42 + \u03b30 +\n1\n2+\u03b51 .\nThe gain matrix K is assumed to be positive de\ufb01nite. To avoid unnecessary technicalities, we also\nassume that the matrices K and G commute, so that, KG is symmetric positive de\ufb01nite (see [21]).\nRecall, G to be the invertible Grammian PN\nn=1 H\nT\nnHn.\nRemark 1 We comment on the GLU assumptions. First, we note that the moment assumption is not\nrestrictive, and most reasonable noise models possess moments of suf\ufb01ciently high order. Also, it is easy\nto come up with a choice of algorithm parameters (\u03c41, \u03c42) given a 0 \u2264\u03b3 < .5. In fact, any choice\nof \u03c41 > .5 + \u03b30 suf\ufb01ces, as one can choose \u03c42 satisfying 0 < \u03c42 < \u03c41 \u2212.5 \u2212\u03b30. That, this choice\nsatis\ufb01es assumption (A.6) ((14)), is due to the fact, that,\n1\n2+\u03b51 < .5 for any \u03b51 > 0. Finally, a note on\nnomenclature. Often, we will use the term (\u03c41, a, \u03c42, b, K)-GLU algorithm to indicate explicitly the GLU\ndesign parameters in force.\nMarkov. Consider the \ufb01ltration, {Fx\ni }i\u22650, given by\nFx\ni = \u03c3\n\u0010\nx(0), {L(j), \u03b6(j)}0\u2264j<i\n\u0011\n(15)\nIt then follows that the random objects L(i), z(i) are independent of Fx\ni , rendering {x(i), Fx\ni }i\u2208T+ a\nMarkov process.\nB. Centralized linear estimators\nThe key focus of the paper is to compare the performance achieved by the class of GLU algorithms\nto centralized estimation schemes6. Speci\ufb01cally, we will restrict this comparison to linear centralized\nestimators only. To this end, we start by de\ufb01ning a reasonable (to be clear soon) class of centralized\n6A centralized scheme corresponds to a fusion center having access to all sensor observations at all times.\n11\nlinear7 of the parameter \u03b8.\nDe\ufb01nition 2 (Centralized linear estimator) A centralized linear estimator is a process {u(i)}i\u2208T+ evolv-\ning as\nu(i + 1) = u(i) + \u03b1c(i)\nN\nKc\nN\nX\nn=1\n\u0010\nH\nT\nnzn(i) \u2212H\nT\nnHnu(i)\n\u0011\n(16)\nHere, we assume that the weight sequence {\u03b1c(i)} is of the form\n\u03b1c(i) =\nac\n(i + 1)\u03c4c\n(17)\nfor some ac > 0 and \u03c4c \u22650. Also, Kc is a positive de\ufb01nite gain matrix that commutes with the Grammian\nG.\nA centralized linear estimator is called good, if in addition the design parameter satis\ufb01es\n.5 + \u03b30 < \u03c4c \u22641\n(18)\nRemark 3 We comment on the above de\ufb01nition and justify the nomenclature good. Clearly, different\nchoices of the gain matrix Kc and the weight sequence {\u03b1c(i)} would lead to different convergence\nproperties of the estimator {u(i)}. As shown in Proposition 7, the condition .5 + \u03b30 < \u03c4c \u22641 is\nnecessary and suf\ufb01cient for the estimator {u(i)} to be universally8 consistent from all initial conditions.\nIn particular, the best linear centralized estimator assumes the form in De\ufb01nition 2 (for a speci\ufb01c choice\nof Kc and {\u03b1c(i)}.) Hence, for all purposes, it is suf\ufb01cient to compare the distributed algorithm GLU\nwith the class of good centralized estimators de\ufb01ned above. In the following, we will restrict attention to\ngood centralized estimators only, and will often drop the term good when referring to these estimators.\nAlso, similar to the distributed GLU estimators, we will use the term (\u03c4c, ac, Kc) centralized estimator\nto indicate explicitly the design parameters in force.\nBefore proceeding to the convergence analysis of GLU under assumptions (A.1)-(A.6), we establish\nsome properties of general stochastic recursions to be used in the sequel.\nIII. SOME INTERMEDIATE RESULTS\nWe establish three approximation results to be used later. The \ufb01rst one (Lemma 4) is a stochastic\nanalogue of Lemma 18 in [1], the second one (Lemma 5) quanti\ufb01es the pathwise convergence rate in\nLemma 4. Lemma 6 is a time-varying mixed time scale version of Lemma 3 in [1]. Finally, we end this\nsection by listing some convergence properties of the centralized estimators (De\ufb01nition 2.)\n7Since we deal with linear centralized estimators only, in the following we drop the term linear when referring to centralized\nestimators.\n8By universal consistency of an algorithm, we mean that the algorithm leads to consistent estimates of the parameter \u03b8\nirrespective of the observation noise distribution, as long as the moment assumption (A.5) is satis\ufb01ed.\n12\nLemma 4 Consider the scalar time-varying linear system:\ny(i + 1) = (1 \u2212r1(i))y(i) + r2(i)\n(19)\nHere {r1(i)} is a sequence of independent random variables, such that, 0 \u2264r1(i) \u22641 a.s. with mean\nr1(i) =\na1\n(i + 1)\u03b41\n(20)\nand a1 > 0, 0 \u2264\u03b41 \u22641. Also, assume y(0) \u22650 and the sequence {r2(i)} is given by\nr2(i) =\na2\n(i + 1)\u03b42\n(21)\nwhere a2 > 0, \u03b42 \u22650. Then, if \u03b41 < \u03b42,\nlim\ni\u2192\u221ey(i) = 0 a.s.\n(22)\nProof: The assumptions imply that the sequence {y(i)} is non-negative. De\ufb01ne the process {V1(i)}\nby\nV1(i) = y(i) \u2212\ni\u22121\nX\nk=0\n\" i\u22121\nY\nl=k+1\n(1 \u2212r1(l))\n!\nr2(k)\n#\n(23)\nSince \u03b41 < \u03b42, an application of Lemma 18 in [1] yields\nlim\ni\u2192\u221e\ni\u22121\nX\nk=0\n\" i\u22121\nY\nl=k+1\n(1 \u2212r1(l))\n!\nr2(k)\n#\n= 0\n(24)\nHence, in particular, the second term on the R.H.S. is bounded and {y(i)} is well de\ufb01ned. Denote by\n{Fy(i)} the natural \ufb01ltration of the process {y(i)} and note that {V1(i)} is adapted to this \ufb01ltration.\nUsing the fact, that\ni\nX\nk=0\n\" \niY\nl=k+1\n(1 \u2212r1(l))\n!\nr2(k)\n#\n= (1 \u2212r1(i))\n\"i\u22121\nX\nk=0\n\" i\u22121\nY\nl=k+1\n(1 \u2212r1(l))\n!\nr2(k)\n##\n+ r2(i)\n(25)\nwe have, by the independence condition,\nE [V1(i + 1) | Fy(i)]\n=\nE [y(i + 1) | Fy(i)] \u2212\ni\nX\nk=0\n\" \niY\nl=k+1\n(1 \u2212r1(l))\n!\nr2(k)\n#\n=\n(1 \u2212r1(i))y(i) + r2(i) \u2212\ni\nX\nk=0\n\" \niY\nl=k+1\n(1 \u2212r1(l))\n!\nr2(k)\n#\n=\n(1 \u2212r1(i))y(i) \u2212\ni\u22121\nX\nk=0\n\" i\u22121\nY\nl=k+1\n(1 \u2212r1(l))\n!\nr2(k)\n#\n=\nV1(i) \u2212r1(i)y(i)\n(26)\n13\nThe nonnegativity of {y(i)} implies\nE [V1(i + 1) | Fy(i)] \u2264V1(i)\n(27)\nHence {V1(i)} is a supermartingale. The nonnegativity of {y(i)} and the boundedness of the terms\nPi\u22121\nk=0\nh\u0010Qi\u22121\nl=k+1(1 \u2212r1(l))\n\u0011\nr2(k)\ni\nfor all i show that {V1(i)} is bounded from below. It then follows\nthat there exists a \ufb01nite random variable V \u2217\n1 , such that,\nlim\ni\u2192\u221eV1(i) = V \u2217\n1 a.s.\n(28)\nWe then have\nlim\ni\u2192\u221ey(i)\n=\nlim\ni\u2192\u221eV1(i) + lim\ni\u2192\u221e\ni\u22121\nX\nk=0\n\" i\u22121\nY\nl=k+1\n(1 \u2212r1(l))\n!\nr2(k)\n#\n=\nV \u2217\n1\n(29)\nSince y(0) is deterministic, the sequence {y(i)} is integrable and we have\nE [y(i)] =\n iY\nk=0\n(1 \u2212r1(i))\n!\ny(0) +\ni\u22121\nX\nk=0\n\" i\u22121\nY\nl=k+1\n(1 \u2212r1(l))\n!\nr2(k)\n#\n(30)\nAn application of Lemma 18 in [1] then shows\nlim\ni\u2192\u221eE [y(i)] = 0\n(31)\nand by Fatou\u2019s lemma we conclude E [V \u2217\n1 ] = 0. Since, V \u2217\n1 is nonnegative, being the limit of the\nnonnegative sequence {y(i)}, we have\nV \u2217\n1 = 0 a.s.\n(32)\nand the claim holds.\nWe will also use the following result, which characterizes the convergence rate in the above. The proof\nis somewhat similar to the arguments in Lemma 4 and we omit it due to space limitations.\nLemma 5 Consider the scalar deterministic time-varying linear system:\ny(i + 1) = (1 \u2212r1(i))y(i) + r2(i)\n(33)\nwhere the sequences {r1(i)} and {r2(i)} satisfy the hypothesis of Lemma 4.\n\u2022 (1) Then, if \u03b41 < \u03b42 and \u03b41 < 1,\nlim\ni\u2192\u221e(i + 1)\u03b40y(i) = 0\n(34)\nfor all 0 \u2264\u03b40 < \u03b42 \u2212\u03b41.\n\u2022 (2) Let \u03b41 < \u03b42 and \u03b41 = 1. Then the above conclusion holds, if in addition a1 > \u03b40.\n\u2022 (3) All the above remain valid when r1(i) is random satisfying the conditions of Lemma 4.\n14\nLemma 6 Under the stated assumptions, there exists i1 suf\ufb01ciently large and a constant c4 > 0, such\nthat, for i \u2265i1,\nyT \u0000\u03b2(i)L \u2297I + \u03b1(i)(IN \u2297K)DH\n\u0001\ny \u2265c4\u03b1(i) \u2225y\u22252 ,\n\u2200y \u2208RNM\n(35)\nProof: The key difference from the proof of Lemma 3 in [1] is that, the matrix\n\u0000\u03b2(i)L \u2297I + \u03b1(i)(IN \u2297K)DH\n\u0001\nis not symmetric. We \ufb01rst show that the quadratic form\nyT\n\u0012\u03b2(i)\n\u03b1(i)L \u2297I + (IN \u2297K)DH\n\u0013\ny\n(36)\nis strictly greater than zero for all y \u2208RNM satisfying \u2225y\u2225= 1 for all suf\ufb01ciently large i. To this end,\nfor such y, consider the decomposition\ny = yC + yC\u22a5\n(37)\nDe\ufb01ne the symmetric matrix DK by\nDK = 1\n2\n\u0002\n(IN \u2297K)DH\n\u0003\n+ 1\n2\n\u0002\n(IN \u2297K)DH\n\u0003T\n(38)\nNoting that\nyT \u0002\n(IN \u2297K)DH\n\u0003\ny = yT DKy\n(39)\nwe have\nyT\n\u0012\u03b2(i)\n\u03b1(i)L \u2297I + (IN \u2297K)DH\n\u0013\ny\n=\nyT\n\u0012\u03b2(i)\n\u03b1(i)L \u2297I + DK\n\u0013\ny\n=\nyT\n\u0012\u03b2(i)\n\u03b1(i)L \u2297I\n\u0013\ny + yT DKy\n=\nyT\nC\u22a5\n\u0012\u03b2(i)\n\u03b1(i)L \u2297I\n\u0013\nyC\u22a5+ yT\nC\u22a5DKyC\u22a5\n+2yT\nC\u22a5DKyC + yT\nC DKyC\n\u2265\n\u03b2(i)\n\u03b1(i)\u03bb2(L) \u2225yC\u22a5\u22252 + yT\nC\u22a5DKyC\u22a5\n+2yT\nC\u22a5DKyC + yT\nC DKyC\n(40)\nNow, the symmetricity of DK implies the existence of a constant c15 > 0, large enough, such that,\nyT\nC\u22a5DKyC\u22a5\n\u2265\n\u2212c15 \u2225yC\u22a5\u22252\n(41)\nyT\nC\u22a5DKyC\n\u2265\n\u2212c15 \u2225yC\u2225\u2225yC\u22a5\u2225\n(42)\n15\nAlso, using the form yC = 1N \u2297a, for some a \u2208RM, we note that\nyT\nC DKyC\n=\nyT\nC\n\u0002\n(IN \u2297K)DH\n\u0003\nyC\n=\nN\nX\nn=1\naTKHna\n=\naKGa\n\u2265\n\u03bbmin \u2225a\u22252\n=\n\u03bbmin\nN\n\u2225yC\u22252\n(43)\nwhere the last but one step uses the fact, that the matrix KG is positive de\ufb01nite, as both K and G are\npositive de\ufb01nite and they commute. Note, in particular, that \u03bbmin > 0. Substituting the above in eqn. (40),\nwe have\nyT\n\u0012\u03b2(i)\n\u03b1(i)L \u2297I + (IN \u2297K)DH\n\u0013\ny \u2265\n\u0012\u03b2(i)\n\u03b1(i)\u03bb2(L) \u2212c15\n\u0013\n\u2225yC\u22a5\u22252 \u22122c15 \u2225yC\u2225\u2225yC\u22a5\u2225+ \u03bbmin\nN\n\u2225yC\u22252\n(44)\nSince limi\u2192\u221e\u03b2(i)/\u03b1(i) = \u221e(\u03c42 < \u03c41), we can choose i1 large enough, such that, for i \u2265i0\n\u03b2(i)\n\u03b1(i)\u03bb2(L) \u2212c15\n>\n0\n(45)\n\u03bbmin\nN\n\u0014\u03b2(i)\n\u03b1(i) \u2212c15\n\u0015\n>\nc2\n15\n(46)\nWe now verify the claim in eqn. (36) for i \u2265i1. Clearly, if yC = 0, the quadratic form reduces to\nyT\n\u0012\u03b2(i)\n\u03b1(i)L \u2297I + (IN \u2297K)DH\n\u0013\ny \u2265\n\u0012\u03b2(i)\n\u03b1(i)\u03bb2(L) \u2212c15\n\u0013\n\u2225yC\u22a5\u22252 = \u03b2(i)\n\u03b1(i)\u03bb2(L) \u2212c15 > 0\n(47)\n(Note that, the constraint that y lies on the unit circle forces \u2225yC\u22a5\u2225to be 1, if yC = 0.) On the other\nhand, if yC > 0, we have\nyT\n\u0012\u03b2(i)\n\u03b1(i)L \u2297I + (IN \u2297K)DH\n\u0013\ny \u2265\u2225yC\u22252\n\"\u0012\u03b2(i)\n\u03b1(i)\u03bb2(L) \u2212c15\n\u0013 \u2225yC\u22a5\u22252\n\u2225yC\u22252 \u22122c15\n\u2225yC\u22a5\u2225\n\u2225yC\u2225+ \u03bbmin\nN\n#\n(48)\nThe term on the R.H.S. is always strictly greater than zero by the discriminant condition of eqn. (45).\nThe assertion in eqn. (36) thus holds. Since the quadratic form is a continuous function of y, its\npositivity on the unit circle implies, there exists c4 > 0, such that,\ninf\n\u2225y\u2225=1 yT\n\u0012\u03b2(i)\n\u03b1(i)L \u2297I + (IN \u2297K)DH\n\u0013\ny \u2265c4 > 0\n(49)\nIt then follows that, for all y \u2208RNM,\nyT\n\u0012\u03b2(i)\n\u03b1(i)L \u2297I + (IN \u2297K)DH\n\u0013\ny \u2265c4 \u2225y\u22252\n(50)\n16\nand hence\nyT \u0000\u03b2(i)L \u2297I + \u03b1(i)(IN \u2297K)DH\n\u0001\ny\n=\n\u03b1(i)yT\n\u0012\u03b2(i)\n\u03b1(i)L \u2297I + (IN \u2297K)DH\n\u0013\ny\n\u2265\n\u03b1(i)c4 \u2225y\u22252\n(51)\nfor i \u2265i1.\nNote that, the condition limi\u2192\u221e\u03b2(i)/\u03b1(i) = \u221eis required for Lemma 6.\nThe following proposition justi\ufb01es the nomenclature good in De\ufb01nition 2. In particular, it shows that\nunder assumptions (A.1),(A.2),(A.5), there exists a noise distribution (Gaussian), such that, the centralized\nscheme is not consistent if \u03c4c fails to satisfy the requirement (18).\nProposition 7\n(1) Suppose the process {\u03b6(i)} is Gaussian. Consider the centralized estimator {u(i)}.\nThen, if \u03c4c \u2264\u03b30 +.5 or \u03c4c > 1, the sequence {u(i)} is not consistent from arbitrary initial condition\nu(0).\n(2) Let assumptions (A.1),(A.2),(A.5) hold. Then, a good centralized estimator is consistent (universally)\nfrom all initial conditions.\n(3) Let assumptions (A.1),(A.2),(A.5) hold. Consider a good centralized estimator with design parameters\n(\u03c4c, ac, Kc). Then, there exists a (\u03c41, a, \u03c42, b, K)-GLU estimator, such that, \u03c41 = \u03c4c, a = ac, K = Kc.\nRemark 8 As a consequence of the \ufb01rst assertion, we note that, for a centralized linear estimator to\nachieve consistency, the parameter \u03b30 should be strictly less than .5.\nProof: Due to space limitations, we omit the proof which follows from standard properties of\nstochastic recurrences and approximation ([22]).\nWe present an intuitive sketch of the proof of the \ufb01rst assertion. From (16), we note that, at time i, an\nobservation noise is incorporated on the right hand side (R.H.S.) with variance of the order (i+1)2\u03b30\u22122tauc.\nClearly, if \u03c4c \u2264.5 + \u03b30, as i \u2192\u221ethe cumulative noise adds up to \u221e. For Gaussian noise, this would\nlead to unboundedness of the estimate sequence {u(i)}. This explains the lower bound in the choice of\n\u03c4c. On the other hand, if \u03c4c > 1, the {\u03b1c} becomes summable and the updates die out quickly. Hence,\ndepending on the initial estimate u(0), it may not be possible to progress towards \u03b8\u2217. Thus, in general,\nwe need \u03c4c \u22641.\nThe second assertion follows from standard stochastic approximation arguments (see, for example [22]\nand Theorem 1 in [23].)\nThe third assertion simply states that there exists a choice of \u03c42 satisfying assumption (A.6), when\n\u03c41 = \u03c4c and K = Kc. This is immediate from Remark 1.\nIn the case \u03b30 = 1, i.e., the observation process is stationary (constant SNR), the following property\nof {u(i)} holds:\n17\nProposition 9 Suppose \u03b30 = 0 and assumptions (A.1),(A.2),(A.5) hold. Then, in addition to the consis-\ntency in Proposition 7, we have the following:\n(1) Assume \u03c4c = 1, i.e., the weight sequence {\u03b1c(i)} is of the form\n\u03b1c(i) =\nac\ni + 1\n(52)\nThen, if ac >\nN\n2\u03bbmin(KG), the normalized sequence {1/\np\n(i + 1)(u(i) \u2212\u03b8\u2217)} is asymptotically\nnormal, i.e.,\np\n(i + 1) (u(i) \u2212\u03b8\u2217) =\u21d2N(0, Sc(K))\n(53)\nwhere, the asymptotic variance is given by:\nSc(K)\n=\na2\nN 2\nZ \u221e\n0\ne\u03a31vS1e\u03a3T vdv\n(54)\n\u03a31\n=\n\u2212a\nN KG + 1\n2IM\n(55)\nS1\n=\nK (1N \u2297IM)T DHS\u03b6D\nT\nH (1N \u2297IM) KT\n(56)\n(2) Let the hypothesis of the previous assertion hold and choose Kc = K\u2217\nc = G\u22121. Then, the estimator\n{u(i)} is the best linear centralized estimator in terms of asymptotic variance irrespective of the\ndistribution of the observation noise \u03b6(i). In addition, if the observation noise sequence {\u03b6(i)} is\nGaussian, {u(i)} as de\ufb01ned above, is the optimum centralized estimator, whose asymptotic variance\nSc(K\u2217) equals the centralized Fisher information rate.\nProof: The proof of the \ufb01rst assertion is omitted due to space limitations (see [1] for similar\narguments.) That, Kc = G\u22121 yields the best linear estimator is standard (see, for example, [24].)\nIV. MAIN RESULTS\nTheorem 10 Consider a \ufb01xed 0 \u2264\u03b30 < .5. Let assumptions (A.1),(A.2),(A.5) hold.\n(1) Consider the GLU algorithm with design parameters (\u03c41, a, \u03c42, b, K) satisfying assumption (A.6).\nFor each sensor n, the estimate sequence {xn(i)} generated by the GLU is a consistent estimator\nof \u03b8\u2217, i.e.,\nP\u03b8\u2217\n\u0010\nlim\ni\u2192\u221exn(i) = \u03b8\u2217\u0011\n= 1,\n\u2200n\n(57)\n(2) Consider a centralized estimator {u(i)} corresponding to a given choice of {\u03b1c} and Kc. Choose\nK = Kc, \u03c41 = \u03c4c and \u03c42 satisfying 0 < \u03c42 < \u03c41 \u2212\u03b30 \u2212\n1\n2+\u03b51 , such that, assumptions (A.1)-(A.6)\nhold (such a choice is always possible by Proposition 7.) Also, if \u03c41 = 1, further assume that the\nconstant a in assumption (A.6) satis\ufb01es\na >\nN\u03c40\n\u03bbmin(KG)\n(58)\n18\nFor each sensor n, consider the estimate sequence {xn(i)} generated by the corresponding GLU\nalgorithm with the above design parameters. Then, for every 0 \u2264\u03c40 < \u03c41 \u2212\u03c42 \u2212\n1\n2+\u03b51 , we have\nP\u03b8\u2217\n\u0010\nlim\ni\u2192\u221e(i + 1)\u03c40 (xn(i) \u2212u(i)) = 0\n\u0011\n= 1,\n\u2200n\n(59)\nWe discuss the consequences of Theorem 10. The \ufb01rst assertion states that, as long as 0 \u2264\u03b30 < .5,\nany distributed GLU estimator yields consistent parameter estimates at every sensor. By Remark 8, this\nis precisely the class of fading parameters, a centralized estimator can estimate consistently. In other\nwords, as long as a centralized linear estimator can consistently estimate a parameter, a distributed GLU\nestimator can. This is interesting, as the range of allowable \u03b30s is independent of the network topology,\nand any random network satisfying the mean connectivity is suf\ufb01cient. The second assertion quanti\ufb01es\nthe rate at which the distributed GLU estimator converges to the centralized estimator. Again, this rate\nis independent of the network topology.\nThe following result (Theorem 11) shows in what sense the GLU algorithm is optimal. We assume\n\u03b30 = 0 in what follows. Suitable extensions to arbitrary \u03b30 may be possible, however, this would impose\nadded technicalities and digress from the main focus of the paper. Also, the notion of asymptotic variance\nas the metric for comparing different consistent estimators, is not quite clear for nonstationary recursive\nprocedures.\nTheorem 11\n(1) Recall the positive de\ufb01nite matrix G = PN\nn=1 H\nT\nnHn. Assume \u03c41 = 1, i.e., the weight\nsequence {\u03b1(i)} is of the form\n\u03b1(i) =\na\ni + 1\n(60)\nwhere a >\nN\n2\u03bbmin(KG) and K is the positive de\ufb01nite matrix gain that commutes with G. Choose any\n\u03c42 satisfying\n\u03c42 +\n1\n2 + \u03b51\n< .5\n(61)\nand note that such a choice exists as\n1\n2+\u03b51 < .5. Consider the GLU algorithm with design parameters\n(\u03c41, a, \u03c42, b, K) chosen above (this ensures that (\u03c41, a, \u03c42, b, K) satisfy assumption (A.6).) Then, the\nnormalized estimate sequence {1/\np\n(i + 1)(xn(i) \u2212\u03b8\u2217)} is asymptotically normal for each n, i.e.,\np\n(i + 1) (xn(i) \u2212\u03b8\u2217) =\u21d2N(0, Sc(K))\n(62)\nHere, the asymptotic variance Sc(K) is the same obtained by a centralized estimator in Theorem 9\nwith gain Kc = K.\n(2) Let the hypothesis of the previous assertion hold with the matrix gain K taking the value K\u2217= G\u22121.\nThen, the asymptotic variance at each sensor is Sc(K\u2217), which is the asymptotic variance achieved\nby the best linear centralized estimator (see Proposition 9.) In particular, if the observation noise\nprocess is Gaussian, the GLU estimator constructed above is asymptotically ef\ufb01cient.\n19\nWe interpret the above. The \ufb01rst assertion implies that given a centralized estimator with matrix gain\nK and satisfying the assumptions in Proposition 9, there exists a distributed GLU estimator achieving\nthe same asymptotic variance Sc(K). This result is remarkable, as the asymptotic variance Sc(K) is\nindependent of the network topology L. This is possible due to the mixed time scale behavior resulting\nfrom appropriate choice of \u03c41, \u03c42. This invariance to the network topology is not achievable by the\nsingle time scale scheme (\u03c41 = \u03c42) developed in [1]. In a sense, Theorem 11 justi\ufb01es the applicability\nand advantage of distributed estimation schemes. Apart from issues of robustness, implementing a\ncentralized estimator is much more communication intensive as it requires transmitting all sensor data to\na fusion center at all times. On the other hand, the distributed GLU algorithm requires only sparse local\ncommunication among the sensors at each step, and achieves the performance of a centralized estimator\nasymptotically. The second assertion of the theorem reemphasizes the optimality and applicability of\ndistributed estimation schemes, and shows that GLU can be designed to achieve the asymptotic variance\nof the optimal linear centralized scheme. In particular, if the observation noise process is Gaussian, GLU\nleads to asymptotically ef\ufb01cient estimators at each sensor.\nV. GLU: CONVERGENCE PROPERTIES\nAs noted earlier, the mixed time scale behavior of GLU does not permit the use of standard stochastic\napproximation tools for establishing convergence. Moreover, to be able to establish important qualitative\nproperties like asymptotic time scale separation, we need to clearly distinguish the long term effects of the\nconsensus and innovations potential. We brie\ufb02y outline the key steps involved in such a pursuit. We \ufb01rst\nidentify conditions under which the sensor estimates {xn(i)} converge to an averaged estimate {xavg(i)}\nover the network and recognize the pathwise (strong) convergence rate. This is carried out in Lemma 15.\nThe averaged estimator {xavg(i)} is not quite the centralized estimator {u(i)}, the key reason being the\naveraged local innovations is not the centralized innovation. This leads us to study the rate of convergence\nof the averaged local innovations to the centralized innovation and hence, the convergence rate of the\naveraged estimate sequence to the centralized. This is accomplished in Lemma 16. The analysis in all\nthese steps culminate to Theorems 10,11, the main results of the paper. These results identify conditions\nunder which the consistent estimate sequences {xn(i)} inherit the centralized convergence rate to \u03b8\u2217. In\nparticular, they establish suf\ufb01cient conditions for the equivalence between the distributed and centralized\nschemes in terms of asymptotic variance. The methodology developed in this work is of independent\ninterest and goes beyond the setting of distributed parameter estimation. We envision its applicability in\nthe analysis of generic dynamical systems interacting over a network.\nIn what follows, we consider the GLU algorithm with \ufb01xed design parameters (\u03c41, a, \u03c42, b, K) and\nassumptions (A.1)-(A.6) hold throughout.\nWe start by establishing pathwise boundedness of the sequence {x(i)}.\n20\nLemma 12 There exists a \ufb01nite random variable R > 0, such that,\nP\u03b8\u2217\n \nsup\ni\u2208T+\n\u2225x(i)\u2225\u2264R\n!\n= 1\n(63)\nProof: De\ufb01ne the process {y(i)} as\ny(i) = x(i) \u22121N \u2297\u03b8\u2217\n(64)\nThe assertion would follow if we establish boundedness for the process {y(i)}. From eqn. (10) we note\nthat {y(i)} satis\ufb01es the recursion:\ny(i + 1)\n=\n\u0000INM \u2212\u03b2(i)L \u2297IM \u2212\u03b1(i)(IN \u2297K)DH\n\u0001\ny(i) \u2212\u03b2(i)\n\u0010\neL(i) \u2297IM\n\u0011\ny(i)\n+\u03b1(i)(IN \u2297K)\n\u0000DHz(i) \u2212DH(1N \u2297\u03b8\u2217)\n\u0001\n(65)\nwhere we use the invariance of the Laplacian operator,\n\u0000L \u2297IM\n\u0001\n(1N \u2297\u03b8\u2217) = 0NM\nConsider the process {V2(i)} given by\nV2(i) = \u2225y(i)\u22252\n(66)\nBy using the conditional independence properties, it can be shown that,\nE\u03b8\u2217[V2(i + 1) | Fi]\n=\nV (i) + \u03b22(i)y(i)T E\u03b8\u2217\nh\neL2(i)\ni\ny(i) + \u03b12(i)E\u03b8\u2217\nh\r\rDHz(i) \u2212DH(1N \u2297\u03b8\u2217)\n\r\r2i\n\u22122yT (i)\n\u0000\u03b2(i)L \u2297IM + \u03b1(i) (IN \u2297K) DH\n\u0001\ny(i) + \u03b2iyT (i)(L \u2297IM)2y(i)\n+\u03b12(i)yT (i) \u0000(IN \u2297K) DH\n\u0001T \u0000(IN \u2297K) DH\n\u0001\ny(i)\n+2\u03b1(i)\u03b2(i)yT (i)(L \u2297IM)(IN \u2297K)y(i)\n(67)\nWe use the following inequalities:\ny(i)T E\u03b8\u2217\nh\neL2(i)\ni\ny(i)\n=\nyT\nC\u22a5(i)E\u03b8\u2217\nh\neL2(i)\ni\nyC\u22a5(i)\n\u2264\nc5 \u2225yC\u22a5(i)\u22252\n(68)\nyT (i)(L \u2297IM)2y(i)\n=\nyT\nC\u22a5(i)(L \u2297IM)2yC\u22a5(i)\n\u2264\n\u03bb2\nN(L) \u2225yC\u22a5(i)\u22252\n(69)\n2yT (i)\n\u0000\u03b2(i)L \u2297IM + \u03b1(i) (IN \u2297K) DH\n\u0001\ny(i)\n\u2265\n\u03b2(i)yT (i)(L \u2297IM)y(i) + yT (i)\n\u0000\u03b2(i)L \u2297IM\n+\u03b1(i) (IN \u2297K) DH\n\u0001\ny(i)\n\u2265\n\u03b2(i)\u03bb2(L) \u2225yC\u22a5(i)\u22252 + c4\u03b1(i) \u2225y(i)\u22252\n(70)\nWe use Lemma 6 to obtain the last inequality. Introducing additional constants to bound the quadratic\n21\nforms and the moments, we derive the following from eqn. (67):\nE\u03b8\u2217[V2(i + 1) | Fi]\n\u2264\nV2(i) \u2212\n\u0000\u03b2(i)\u03bb2(L) \u2212\u03b22(i)c5 \u2212\u03b22(i)\u03bb2\nN(L)\n\u0001\n\u2225yC\u22a5(i)\u22252\n\u2212(c4\u03b1(i) \u2212\u03b1(i)\u03b2(i)c7) \u2225y(i)\u22252 + \u03b12(i)\u03b32(i)c8 + \u03b12(i)c6 \u2225y(i)\u22252\n(71)\nwhere c8 > 0 is a constant, such that,\n\u03b12(i)E\u03b8\u2217\nh\r\rDHz(i) \u2212DH(1N \u2297\u03b8\u2217)\n\r\r2i\n= \u03b12(i)\u03b32(i)c8\n(72)\nSince \u03b22(i) goes to zero faster than \u03b2(i), the \u03b2(i) term dominates in the second expression of eqn. (71)\neventually. Similarly, the \u03b1(i) term dominates the third expression eventually. Choose c9 = max(c6, c8).\nSince, \u03b3(i) \u22651 (assumption (A.2)), there exists i2 large enough, such that, for i \u2265i2\nE\u03b8\u2217[V2(i + 1) | Fi] \u2212V2(i)\n\u2264\n\u03b12(i)\u03b32(i)c8 + \u03b12(i)c6V2(i)\n\u2264\nc9\u03b12(i)\u03b32(i)(1 + V2(i))\n(73)\nNow introduce the process\neV2(i) = (1 + V2(i))\n\u221e\nY\nk=i\n(1 + c9\u03b12(k)\u03b32(k))\n(74)\nNote that the above is well de\ufb01ned as the product Q\u221e\nk=i(1 + c9\u03b12(k)\u03b32(k)) converges for all i due to\nthe square summability of {\u03b1(i)\u03b3(i)} (assumption (A.6)). Eqn. (73) and some algebraic manipulations\nlead to\nE\u03b8\u2217\nh\neV2(i + 1) | Fi\ni\n\u2264eV2(i)\n(75)\nthus establishing that the sequence {eV2(i)} is a nonnegative supermartingale. Hence, there exists a \ufb01nite\nrandom variable eR, such that, limi\u2192\u221eeV2(i) = R a.s. We then have from eqn. (74)\nlim\ni\u2192\u221eV2(i) = eR \u22121 a.s.\n(76)\nHence, {V2(i)} is bonded pathwise and the assertion follows.\nRemark 13 A deeper investigation of the supermartingale would reveal that V2(i) in fact, converges to\nzero. This would have established the consistency of the estimators. However, to obtain strong convergence\nrates, we need to study the sample paths more critically. The rest of this subsection is devoted to this\nstudy.\nThe following lemma identi\ufb01es the rate at which the estimates converge to a network averaged estimate\nand hence characterizes the information \ufb02ow in the network.\nBefore that, we establish the following:\nProposition 14 Let assumptions (A.1)-(A.6) hold.\n22\n(1) For all i \u2208T+, de\ufb01ne\nJ1(z(i)) = (IN \u2297K)DHz(i) \u22121N \u2297\n\u0012\u0012 1\nN 1N \u2297IM\n\u0013\n(IN \u2297K)DHz(i)\n\u0013\n(77)\nThen, we have the following:\nP\u03b8\u2217\n \n1\n(i + 1)\u03b30+\n1\n2+\u03b51 +\u03b4 \u2225J1(z(i))\u2225= 0\n!\n= 0\n(78)\n(2) Recall the matrix,\nP NM = 1\nN (1N \u2297IM) (1N \u2297IM)T\n(79)\nThen, for i \u2208T+ suf\ufb01ciently large, we have\n\r\rINM \u2212\u03b2(i) (L(i) \u2297IM) \u2212P NM\r\r = 1 \u2212\u03b2(i)\u03bb2(L(i))\n(80)\nProof: For the \ufb01rst assertion, consider any \u03b52 > 0. By Chebyshev\u2019s inequality and assumption (A.5),\nP\u03b8\u2217\n \n1\n(i + 1)\n1\n2+\u03b51 +\u03b4 \u2225J1(z(i))\u2225> \u03b52\n!\n\u2264\n1\n\u03b52+\u03b51\n2\n(i + 1)1+(\u03b4+\u03b30)(2+\u03b51) E\u03b8\u2217\nh\n\u2225J1(z(i))\u22252+\u03b51i\n=\n\u03ba(\u03b8\u2217)\n\u03b52+\u03b51\n2\n1\n(i + 1)1+\u03b4(2+\u03b51)\n(81)\nSince, \u03b4 > 0, the sequence {\n1\n(i+1)1+\u03b4(2+\u03b51 ) } is square summable and we obtain\nX\ni\u2208T+\nP\u03b8\u2217\n \n1\n(i + 1)\n1\n2+\u03b51 +\u03b4 \u2225J1(z(i))\u2225> \u03b52\n!\n< \u221e\n(82)\nIt then follows from the Borel-Cantelli lemma (see [25]) that,\nP\u03b8\u2217\n \n1\n(i + 1)\n1\n2+\u03b51 +\u03b4 \u2225J1(z(i))\u2225> \u03b52 i.o.\n!\n= 0\n(83)\nwhere i.o. stands for in\ufb01nitely often. Since the above holds for \u03b52 > 0 arbitrarily small, the claim in\neqn. (78) holds by standard arguments.\nFor the second assertion, we note from the discussion on Kronecker products in Section I-B that, the\neigenvalues of the matrix\n\u0000INM \u2212\u03b2(i) (L(i) \u2297IM) \u2212P NM\u0001\nare 0 and 1\u2212\u03b2(i)\u03bbn(L(i)), i = 2, \u00b7 \u00b7 \u00b7 , N,\neach repeated M times. Since, the Laplacian eigenvalues are all bounded above by N 2 and \u03b2(i) \u21920,\nthere exists i4 \u2208T+ suf\ufb01ciently large, such that, for i \u2265i4, \u03b2(i)\u03bbn(L(i)) < 1, for all 2 \u2264n \u2264N. The\nassertion is then obvious.\nLemma 15 De\ufb01ne the averaged estimate sequence {xavg(i)} as\nxavg(i) = 1\nN (1N \u2297IM)x(i)\n(84)\n23\nThen for every \u03c40, such that,\n0 \u2264\u03c40 < \u03c41 \u2212\u03c42 \u2212\u03b30 \u2212\n1\n2 + \u03b5\n(85)\nwe have\nP\u03b8\u2217\n\u0010\nlim\ni\u2192\u221e(i + 1)\u03c40 \u0000x(i) \u22121N \u2297xavg(i)\n\u0001\n= 0\n\u0011\n= 1\n(86)\nProof: De\ufb01ne the process {y1(i)}:\nby(i) = x(i) \u22121N \u2297xavg(i)\n(87)\nRecall the matrix\nP NM = 1\nN (1N \u2297IM) (1N \u2297IM)T\n(88)\nand note that\nP NMx(i) = 1N \u2297xavg(i),\nP NM \u00001N \u2297xavg(i)\n\u0001\n= 1N \u2297xavg(i)\n(89)\nFrom eqn. (10) we then note that {y1(i)} satis\ufb01es the recursion:\nby(i + 1)\n=\n\u0000INM \u2212\u03b2(i)L \u2297IM \u2212P NM\u0001 by(i) \u2212\u03b1(i)\n\u0002\n(IN \u2297K)DHx(i)\n\u22121N \u2297\n\u0012 1\nN (1N \u2297IM)(IN \u2297K)DHx(i)\n\u0013\u0015\n+\u03b1(i) [J1(z(i))]\n(90)\nwhere J1(z(i)) is de\ufb01ned in (77). Choose \u03b4 satisfying\n0 < \u03b4 < \u03c41 \u2212\u03c42 \u2212\u03b30 \u2212\u03c40 \u2212\n1\n2 + \u03b51\n(91)\nThen, by Proposition 14, we have\nP\u03b8\u2217\n \n1\n(i + 1)\u03b30+\n1\n2+\u03b51 +\u03b4 \u2225J1(z(i))\u2225= 0\n!\n= 0\n(92)\nAlso, Lemma 12 implies\nP\u03b8\u2217\n \nsup\ni\u2208T+\n\r\r\r\r(IN \u2297K)DHx(i) \u22121N \u2297\n\u0012 1\nN (1N \u2297IM)(IN \u2297K)DHx(i)\n\u0013\r\r\r\r < \u221e\n!\n= 1\n(93)\nby the boundedness of {x(i)}. However, these pathwise bounds are not uniform over the sample paths\nand hence we use truncation arguments. For a scalar a, de\ufb01ne its truncation (a)R0 at level R0 > 0 by\n(a)R0 =\n(\na\n|a| min(|a|, R0)\nif a \u0338= 0\n0\nif a = 0\n(94)\nFor a vector, the truncation operation applies component-wise. For R0 > 0, we also consider the\n24\nsequences, {byR0(i)}i\u22650, given by\nbyR0(i + 1)\n=\n\u0000INM \u2212\u03b2(i)L \u2297IM \u2212P\n\u0001 byR0(i) \u2212\u03b1(i)\n\u0000\u0002\n(IN \u2297K)DHx(i)\u2212\n1N \u2297\n\u0012 1\nN (1N \u2297IM)(IN \u2297K)DHx(i)\n\u0013\u0015\u0013R0\n+\u03b1(i) ([J1(z(i))])R0(i+1)\n\u03b30+\n1\n2+\u03b51\n+\u03b4\n(95)\nWe will now show that for every R0 > 0,\nP\u03b8\u2217\n\u0010\nlim\ni\u2192\u221e(i + 1)\u03c40 (byR0(i)) = 0\n\u0011\n= 1\n(96)\nfor \u03c40 satisfying the hypothesis 85. That, this is suf\ufb01cient to conclude the assertion\nP\u03b8\u2217\n\u0010\nlim\ni\u2192\u221e(i + 1)\u03c40 (by(i)) = 0\n\u0011\n= 1\n(97)\nis a consequence of the following standard argument. The pathwise boundedness of the various terms\nimply that for every \u03b53 > 0, there exists R\u03b53 > 0, such that,\nP\u03b8\u2217\n \nsup\ni\u2208T+\n\r\r\r\r(IN \u2297K)DHx(i) \u22121N \u2297\n\u0012 1\nN (1N \u2297IM)(IN \u2297K)DHx(i)\n\u0013\r\r\r\r < R\u03b53\n!\n> 1 \u2212\u03b53\n(98)\nP\u03b8\u2217\n \nsup\ni\u2208T+\n\u2225J1(z(i))\u2225< R\u03b53(i + 1)\u03b30+\n1\n2+\u03b51 +\u03b4\n!\n> 1 \u2212\u03b53\n(99)\nFor (98) we use the pathwise boundedness of {x(i)} (Lemma 12), whereas, (99) holds because the a.s.\nconvergence in Lemma 14 implies convergence in probability. Clearly, the process {by(i)} agrees with\nthe process {byR\u03b53(i)} on the set where both of the above events occur. By standard manipulations, it\nthen follows, that\nP\u03b8\u2217\n \nsup\ni\u2208T+\n\r\rby(i) \u2212byR\u03b53(i)\n\r\r = 0\n!\n> 1 \u22122\u03b53\n(100)\nThe claim in eqn. (96) would then imply\nP\u03b8\u2217\n\u0010\nlim\ni\u2192\u221e(i + 1)\u03c40 (by(i)) = 0\n\u0011\n> 1 \u22122\u03b53\n(101)\nWe could then establish the assertion of the lemma by taking \u03b53 to zero.\nHence, in the following we establish the claim in eqn. (96) for every R0 > 0. To this end, consider\nthe scalar process {eyR0(i)}i\u2208T+ de\ufb01ned recursively as\neyR0(i + 1) =\n\r\rINM \u2212\u03b2(i)L(i) \u2212P NM\r\r eyR0(i) + NMR0\u03b1(i) + NMR0\u03b1(i)(i + 1)\u03b30+\n1\n2+\u03b51 +\u03b4\n(102)\n25\nwith initial condition eyR0(0) = \u2225byR0(0)\u2225. Since,\n\u2225byR0(i + 1)\u2225\n=\n\r\rINM \u2212\u03b2(i)L \u2297IM \u2212P NM\r\r \u2225byR0(i)\u2225\u2212\u03b1(i)\n\r\r\u0000\u0002\n(IN \u2297K)DHx(i)\n\u22121N \u2297\n\u0012 1\nN (1N \u2297IM)(IN \u2297K)DHx(i)\n\u0013\u0015\u0013R0\r\r\r\r\r\n+\u03b1(i)\n\r\r\r\r([J1(z(i))])R0(i+1)\n\u03b30+\n1\n2+\u03b51\n+\u03b4\r\r\r\r\n(103)\nit follows that,\n\u2225byR0(i)\u2225\u2264eyR0(i),\n\u2200i\n(104)\nBy Proposition 14, for i large enough, it can be shown that\n\r\rINM \u2212\u03b2(i)L \u2297IM \u2212P NM\r\r = 1 \u2212\u03b2(i)\u03bb2(L(i))\n(105)\nWe assume w.l.o.g. that the above holds for all i. We then have\neyR0(i + 1)\n\u2264\n(1 \u2212\u03b2(i)\u03bb2(L(i))) eyR0(i) + NMR0\u03b1(i) + NMR0\u03b1(i)(i + 1)\u03b30+\n1\n2+\u03b51 +\u03b4\n\u2264\n(1 \u2212\u03b2(i)\u03bb2(L(i))) eyR0(i) + 2NMR0\u03b1(i)(i + 1)\u03b30+\n1\n2+\u03b51 +\u03b4\n(106)\nThe above implies\neyR0(i + 1) \u2264(1 \u2212\u03b2(i)\u03bb2(L(i))) (eyR0(i)) + 2NMR0\n1\n(i + 1)\u03c41\u2212\u03b30\u2212\n1\n2+\u03b51 \u2212\u03b4\n(107)\nUsing a result from [26], we note that \u03bb2(L) > 0 implies E\u03b8\u2217[\u03bb2(L(i))] > 0 (note that this equivalence\nis not a consequence of Jensen\u2019s inequality, as the second eigenvalue is a concave function of the graph\nLaplacian.) The recursion in eqn. (108) then falls under the purview of Lemmas 4,5 (see eqns. (85,91)),\nand we have\nP\u03b8\u2217\n\u0010\nlim\ni\u2192\u221e(i + 1)\u03c40eyR0(i) = 0\n\u0011\n= 1\n(108)\nIt then follows from eqn. (104) that\nP\u03b8\u2217\n\u0010\nlim\ni\u2192\u221e(i + 1)\u03c40byR0(i) = 0\n\u0011\n= 1\n(109)\nThe assertion is then immediate.\nLemma 15 characterizes the proximity of the sensor estimates {xn(i)} to the network averaged estimate\n{xavg(i)}. To infer the convergence of the sensor estimates to \u03b8\u2217, it then suf\ufb01ces to study the limiting\nproperties of {xavg(i)}. This is achieved in two steps. In the following, we consider the class of linear\ncentralized estimators of the parameter \u03b8, and establish its relation to the network averaged estimator\n{xavg(i)}. In particular, we investigate the rate at which {xavg(i)} converges to the class of centralized\nestimators. Properties of the centralized estimators are then used to infer the convergence of {xavg(i)}\n(and hence, that of {xn(i)}) to \u03b8\u2217.\n26\nThe following result is the \ufb01rst step towards characterizing the convergence rate of the network averaged\nestimator {xavg(i)} to \u03b8\u2217. It establishes the relation between {xavg(i)} and the class of centralized\nestimators {u(i)} introduced in De\ufb01nition 2.\nLemma 16 Let {u(i)} be the centralized estimate sequence de\ufb01ned in 2 with \u03c4c = \u03c41, ac = a and\nKc = K. Then,\n(1)\nP\u03b8\u2217\n\u0010\nlim\ni\u2192\u221e\n\r\rxavg(i) \u2212u(i)\n\r\r = 0\n\u0011\n= 1\n(110)\n(2) Let \u03c40 satisfy the assumption\n0 < \u03c40 < \u03c41 \u2212\u03c42 \u2212\u03b30 \u2212\n1\n2 + \u03b51\n(111)\nAlso, if \u03c41 = 1, assume that the constant a in assumption (A.6) satis\ufb01es\na >\nN\u03c40\n\u03bbmin(KG)\n(112)\nThen,\nlim\ni\u2192\u221e(i + 1)\u03c40 \u0000xavg(i) \u2212u(i)\n\u0001\n= 0\n(113)\nProof: We note that the averaged update may be written as\nxavg(i + 1)\n=\nxavg(i) + \u03b1(i)\nN K\nN\nX\nn=1\nH\nT\nnzn(i) \u2212\u03b1(i)\nN K\nN\nX\nn=1\nH\nT\nnHnxn(i)\n=\nxavg(i) + \u03b1(i)\nN K\nN\nX\nn=1\n\u0010\nH\nT\nnzn(i) \u2212H\nT\nnHnxavg(i)\n\u0011\n\u2212\u03b1(i)\nN K\nN\nX\nn=1\nH\nT\nnHn\n\u0000xn(i) \u2212xavg(i)\u0001\n(114)\nDe\ufb01ne the process {eu(i)} by\neu(i) = xavg(i) \u2212u(i)\n(115)\nWe then have\neu(i + 1) = (IM \u2212\u03b1(i)\nN KG)eu(i) \u2212\u03b1(i)\nN K\nN\nX\nn=1\nH\nT\nnHn\n\u0000xn(i) \u2212xavg(i)\n\u0001\n(116)\nNow choose \u03b4, such that,\n0 < \u03b4 < \u03c41 \u2212\u03c42 \u2212\u03b30 \u2212\u03c40 \u2212\n1\n2 + \u03b51\n(117)\nSince \u03c40 + \u03b4 < \u03c41 \u2212\u03c42 \u2212\u03b30 \u2212\n1\n2+\u03b51 , by Lemma 15, it follows that,\nP\u03b8\u2217\n \nlim\ni\u2192\u221e(i + 1)\u03c40+\u03b4\n\r\r\r\r\r\nN\nX\nn=1\nH\nT\nnHn\n\u0000xn(i) \u2212xavg(i)\n\u0001\n\r\r\r\r\r = 0\n!\n= 1\n(118)\n27\nThen, there exists a \ufb01nite random variable R3, such that,\nP\u03b8\u2217\n \r\r\r\r\r\nN\nX\nn=1\nH\nT\nnHn\n\u0000xn(i) \u2212xavg(i)\n\u0001\n\r\r\r\r\r \u2264R3(i + 1)\u2212\u03c40\u2212\u03b4 \u2200i \u2208T+\n!\n= 1\n(119)\nNote, by hypothesis, the matrix KG is symmetric and \u03b1(i) \u21920. Hence, there exists a constant c10 > 0,\nsuch that, for suf\ufb01ciently large i,\n\r\r\r\rIM \u2212\u03b1(i)\nN KG\n\r\r\r\r \u22641 \u2212c10\u03b1(i)\nWriting \u03c9-wise and introducing another constant c11 > 0, we have\n\u2225eu(i + 1, \u03c9)\u2225\u2264(1 \u2212c10\u03b1(i)) \u2225eu(i, \u03c9)\u2225+ c11\u03b1(i)R3(\u03c9)(i + 1)\u2212\u03c40\u2212\u03b4\n(120)\nfor i greater than some suf\ufb01ciently large i4(\u03c9). We then have\n\u2225eu(i + 1, \u03c9)\u2225\u2264(1 \u2212cKG\u03b1(i)) \u2225eu(i, \u03c9)\u2225+ c11R3(\u03c9)(i + 1)\u2212\u03c41\u2212\u03c40\u2212\u03b4\n(121)\nA pathwise (\ufb01xed \u03c9) application of Lemma 4 and Lemma 5 and noting that the above holds for \u03c9 in a\nset of full measure yield the assertions.\nVI. PROOFS OF MAIN RESULTS\nA. Proof of Theorem 10\nConsider the \ufb01rst assertion. Since the GLU parameters (\u03c41, a, \u03c42, b, K) satisfy assumption (A.6), we\nnote\n.5 + \u03b30 < \u03c41 \u22641\n(122)\nChoose \u03c4c = \u03c41 and Kc = K. It then follows that the centralized estimator {u(i)} (De\ufb01nition 2) with\ndesign parameters is good. Hence, by Proposition 7 it is consistent, i.e.,\nP\u03b8\u2217\n\u0010\nlim\ni\u2192\u221eu(i) = \u03b8\u2217\u0011\n= 1\n(123)\nTaking \u03c40 = 0 in Lemma 15, we have\nP\u03b8\u2217\n\u0010\nlim\ni\u2192\u221e\n\u0000x(i) \u22121N \u2297xavg(i)\n\u0001\n= 0\n\u0011\n= 1\n(124)\nThe \ufb01rst assertion of Theorem 10 is then an immediate consequence of (123)-(124) and Lemma 16 (\ufb01rst\nassertion.)\nThe second assertion of Theorem 10 is a direct consequence of Lemma 15 and Lemma 16 (\ufb01rst\nassertion.)\n28\nB. Proof of Theorem 11\nBy hypothesis of Theorem 11, we have\n\u03c41 = 1,\n1\n2 + \u03b51\n+ \u03c42 < .5\n(125)\nHence, \u03c41 \u2212\u03c42 \u2212\n1\n2+\u03b51 > .5. Since, a >\nN\n2\u03bbmin(KG), there exists \u03b55 > 0, small enough, such that,\na > N(.5 + \u03b55)\n\u03bbmin(KG)\n(126)\nBy the above, we can always choose \u03c40 satisfying the condition:\n.5 < \u03c40 < max\n\u0012\n.5 + \u03b55, \u03c41 \u2212\u03c42 \u2212\n1\n2 + \u03b51\n\u0013\n(127)\nFor such \u03c40, we clearly have a >\nN\u03c40\n\u03bbmin(KG), and hence by Theorem 10 (second assertion), we conclude\nP\u03b8\u2217\n\u0010\nlim\ni\u2192\u221e(i + 1)\u03c40 (xn(i) \u2212u(i)) = 0\n\u0011\n= 1\n(128)\nwhere {u(i)} is the centralized estimator with design parameters (\u03c4c, ac, Kc), such that, ac = a, \u03c4c = \u03c41,\nKc = K. It then follows by Proposition 9, that,\np\n(i + 1) (u(i) \u2212\u03b8\u2217) =\u21d2N(0, Sc(K))\n(129)\nSince, \u03c40 in (128) is strictly greater than .5, the sequences {xn(i)} and {u(i)} are indistinguishable in\np\n(i + 1) scale, and it can be shown using standard properties of stochastic convergence, that,\np\n(i + 1) (xn(i) \u2212\u03b8\u2217) =\u21d2N(0, Sc(K))\n(130)\nThe second assertion follows by choosing K = K\u2217in the \ufb01rst.\nVII. CONCLUSIONS\nThe paper considers gossip linear estimation of an unknown large dimensional parameter (or large\nscale static random \ufb01eld) observed by a sparsely interconnected network of sensors operating under\nthe gossip communication protocol. We consider this problem under very general conditions on the\nnoise assumptions and communication failures (including, link or channel failures, besides the usual\nmeasurement noise assumptions.) Due to the large scale of the \ufb01eld, the sensors are local, i.e., they\nobserve only a small fraction of the \ufb01eld. To obtain a global estimate, the sensors need to cooperate.\nThe class of gossip distributed linear estimators we study combines two terms: a consensus term that\nupdates at each sensor its current estimate with the state estimates provided by the neighbor(s) when\nthey gossip; and an innovations or sensing term that updates the current sensor estimate with the new\nobservation. The linear gossip distributed estimators that we analyze exhibit a mixed time scale\u2013one\nthat is associated with the consensus and the other with the innovations. This forces us to develop new\nanalytical tools to establish their asymptotic properties. This is because in gossip distributed estimation,\n29\nthe innovation term is not a martingale difference process, as in previous work on mixed time scale\nstochastic approximation algorithms, e.g., [4]; so, a key step in our analysis is to derive pathwise strong\napproximation results to characterize the rate at which the innovation process converges to a martingale\ndifference process. The paper establishes a distributed observability condition\u2013global observability, a\ncondition on the sensing devices, i.e., the local measurements, plus mean connectedness, a structural\ncondition on the communication network as provided by gossip. We show that under this condition\nthe distributed estimators performance approaches the asymptotic performance of the optimal centralized\nestimators, namely, the distributed estimators are consistent and asymptotically normal. This is signi\ufb01cant,\nas it shows that, under reasonable assumptions, a distributed gossip estimator is as good as a centralized\none, the latter having access to all sensor observations at all times. As mentioned, the distributed gossip\nestimator has two time scales, which involves setting two gain sequences, one for the local innovations\nat each sensor and the other for estimate fusion (consensus) across sensors. To design good distributed\ngossip estimators, these gains should be chosen properly, namely, the consensus gain should decay at a\nslower rate than the innovation gain. In the absence of quantization or channel noise, the paper shows that\nit is possible to choose the consensus weight sequence such that its squared sum goes to \u221e, in contrast\nto the innovation weight sequence whose squared sum needs to be \ufb01nite. This tuning of the different gain\nsequences leads to an asymptotic time scale separation, the rate of information dissemination dominating\nthe rate of reduction of uncertainty by observation acquisition. This is not possible with quantized or\nnoisy transmissions, as each consensus step introduces noise, preventing proper adjustment of the gain\nsequences. The paper interprets the fundamental convergence results on distributed gossip estimation\nin two interesting contexts: 1) when the observations are (conditionally) independent, the distributed\nestimator achieves the same performance (in terms of asymptotic variance) as the best centralized linear\nestimator; and 2) the maximum rate at which the observation noise power (variance) can increase with\ntime and still the estimators to remain consistent is the same for the centralized and the gossip linear\ndistributed estimators.\nREFERENCES\n[1] S. Kar, J. M. F. Moura, and K. Ramanan, \u201cDistributed parameter estimation in sensor networks: nonlinear observation\nmodels and imperfect communication,\u201d August 2008, submitted to the IEEE Transactions on Information Theory, 51\npages. [Online]. Available: http://arxiv.org/abs/0809.0009\n[2] S. Kar and J. M. F. Moura, \u201cGossip and distributed Kalman \ufb01ltering: Weak consensus under weak detectability,\u201d 2010,\nsubmitted for publication.\n[3] V. S. Borkar, Stochastic Approximation: A Dynamical Systems Viewpoint.\nCambridge, UK: Cambridge University Press,\n2008.\n[4] S. B. Gelfand and S. K. Mitter, \u201cRecursive stochastic algorithms for global optimization in Rd,\u201d SIAM J. Control Optim.,\nvol. 29, no. 5, pp. 999\u20131018, September 1991.\n[5] J. N. Tsitsiklis, D. P. Bertsekas, and M. Athans, \u201cDistributed asynchronous deterministic and stochastic gradient optimization\nalgorithms,\u201d IEEE Trans. Autom. Control, vol. AC-31, no. 9, pp. 803\u2013812, 1986.\n[6] J. N. Tsitsiklis, \u201cProblems in decentralized decision making and computation,\u201d Ph.D., Massachusetts Institute of Technology,\nCambridge, MA, 1984.\n30\n[7] D. Bertsekas, J. Tsitsiklis, and M. Athans, \u201cConvergence theories of distributed iterative processes: A survey,\u201d Technical\nReport for Information and Decision Systems, Massachusetts Inst. of Technology, Cambridge, MA, 1984.\n[8] H. Kushner and G. Yin, \u201cAsymptotic properties of distributed and communicating stochastic approximation algorithms,\u201d\nSiam J. Control and Optimization, vol. 25, no. 5, pp. 1266\u20131290, Sept. 1987.\n[9] A. Das and M. Mesbahi, \u201cDistributed linear parameter estimation in sensor networks based on laplacian dynamics consensus\nalgorithm,\u201d in 3rd Annual IEEE Communications Society on Sensor and Ad Hoc Communications and Networks, vol. 2,\nReston, VA, USA, 28-28 Sept. 2006, pp. 440\u2013449.\n[10] I. D. Schizas, A. Ribeiro, and G. B. Giannakis, \u201cConsensus in ad hoc wsns with noisy links - part i: Distributed estimation\nof deterministic signals,\u201d IEEE Transactions on Signal Processing, vol. 56, no. 1, pp. 350\u2013364, January 2008.\n[11] S. Kar, S. A. Aldosari, and J. M. F. Moura, \u201cTopology for distributed inference on graphs,\u201d IEEE Transactions on Signal\nProcessing, vol. 56, no. 6, pp. 2609\u20132613, June 2008.\n[12] U. A. Khan and J. M. F. Moura, \u201cDistributing the kalman \ufb01lter for large-scale systems,\u201d IEEE Transactions on Signal\nProcessing, vol. 56, no. 10, p. 49194935, October 2008.\n[13] C. G. Lopes and A. H. Sayed, \u201cDiffusion least-mean squares over adaptive networks: Formulation and performance\nanalysis,\u201d IEEE Transactions on Signal Processing, vol. 56, no. 7, pp. 3122\u20133136, July 2008.\n[14] S. Stankovic, M. Stankovic, and D. Stipanovic, \u201cDecentralized parameter estimation by consensus based stochastic\napproximation,\u201d in 46th IEEE Conference on Decision and Control, New Orleans, LA, USA, 12-14 Dec. 2007, pp. 1535\u2013\n1540.\n[15] I. Schizas, G. Mateos, and G. Giannakis, \u201cStability analysis of the consensus-based distributed lms algorithm,\u201d in\nProceedings of the 33rd International Conference on Acoustics, Speech, and Signal Processing, Las Vegas, Nevada, USA,\nApril 1-4 2008, pp. 3289\u20133292.\n[16] S. Ram, V. Veeravalli, and A. Nedic, \u201cDistributed and recursive parameter estimation in parametrized linear state-space\nmodels,\u201d to appear in IEEE Transactions on Automatic Control.\n[17] F. R. K. Chung, Spectral Graph Theory.\nProvidence, RI : American Mathematical Society, 1997.\n[18] B. Mohar, \u201cThe Laplacian spectrum of graphs,\u201d in Graph Theory, Combinatorics, and Applications, Y. Alavi, G. Chartrand,\nO. R. Oellermann, and A. J. Schwenk, Eds.\nNew York: J. Wiley & Sons, 1991, vol. 2, pp. 871\u2013898.\n[19] B. Bollobas, Modern Graph Theory.\nNew York, NY: Springer Verlag, 1998.\n[20] S. Boyd, A. Ghosh, B. Prabhakar, and D. Shah, \u201cRandomized gossip algorithms,\u201d IEEE/ACM Trans. Netw., vol. 14, no. SI,\npp. 2508\u20132530, 2006.\n[21] R. Subramanian and K. V. Bhagwat, \u201cOn a theorem of wigner on products of positive matrices,\u201d Proceedings Mathematical\nSciences, vol. 88, no. 1, pp. 31\u201334, January 1979.\n[22] M. Nevel\u2019son and R. Has\u2019minskii, Stochastic Approximation and Recursive Estimation.\nProvidence, Rhode Island:\nAmerican Mathematical Society, 1973.\n[23] S. Kar and J. M. F. Moura, \u201cDistributed consensus algorithms in sensor networks with imperfect communication: Link\nfailures and channel noise,\u201d IEEE Transactions on Signal Processing, vol. 57, no. 1, pp. 355\u2013369, January 2009.\n[24] L. L. Scharf, Statistical Signal Processing: Detection, Estimation and Time Series Analysis.\nAddison Wesley, 1990.\n[25] O. Kallenberg, Foundations of Modern Probability, 2nd ed.\nSpringer Series in Statistics., 2002.\n[26] S. Kar and J. M. F. Moura, \u201cSensor networks with random links: Topology design for distributed consensus,\u201d IEEE\nTransactions on Signal Processing, vol. 56, no. 7, pp. 3315\u20133326, July 2008.\n",
        "sentence": "",
        "context": "author is with the Dep. Electrical and Computer Engineering, Carnegie Mellon University, Pittsburgh, PA 15213, USA (e-mail:\nsoummyak@andrew.cmu.edu, moura@ece.cmu.edu, ph: (412)268-6341, fax: (412)268-3890.)\ntopology\nThe \ufb01rst author is with the Dep. Electrical Engineering, Princeton University, Princeton, NJ. This work was performed\nwhile the \ufb01rst author was with the Dep. Electrical and Computer Engineering, Carnegie Mellon University. The second\n[6] J. N. Tsitsiklis, \u201cProblems in decentralized decision making and computation,\u201d Ph.D., Massachusetts Institute of Technology,\nCambridge, MA, 1984.\n30"
    },
    {
        "title": "Decentralized parameter estimation by consensus based stochastic approximation",
        "author": [
            "S. Stankovic",
            "M. Stankovic",
            "D. Stipanovic"
        ],
        "venue": "IEEE Transactions on Automatic Control, vol. 56, no. 3, pp. 531\u2013543, 2011.",
        "citeRegEx": "13",
        "shortCiteRegEx": null,
        "year": 2011,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "Consensus problems in networks of agents with switching topology and time-delays",
        "author": [
            "R. Olfati-Saber",
            "R. Murray"
        ],
        "venue": "IEEE Transactions on Automatic Control, vol. 49, pp. 1520\u20131533, Sep 2004.",
        "citeRegEx": "14",
        "shortCiteRegEx": null,
        "year": 2004,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " There are several distributed strategies that can be used for this purpose, such as consensus [10]\u2013[14] and diffusion strategies [15]\u2013[18].",
        "context": null
    },
    {
        "title": "Diffusion least-mean squares over adaptive networks: Formulation and performance analysis",
        "author": [
            "C. Lopes",
            "A.H. Sayed"
        ],
        "venue": "IEEE Transactions on Signal Processing, vol. 56, no. 7, pp. 3122 \u20133136, July 2008.",
        "citeRegEx": "15",
        "shortCiteRegEx": null,
        "year": 2008,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " There are several distributed strategies that can be used for this purpose, such as consensus [10]\u2013[14] and diffusion strategies [15]\u2013[18].",
        "context": null
    },
    {
        "title": "Diffusion LMS Strategies for Distributed Estimation",
        "author": [
            "F.S. Cattivelli",
            "A.H. Sayed"
        ],
        "venue": "IEEE Transactions on Signal Processing, vol. 58, no. 3, pp. 1035\u20131048, March 2010.",
        "citeRegEx": "16",
        "shortCiteRegEx": null,
        "year": 2010,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " We extend the energy conservation arguments of [16]\u2013[19] to perform a mean-squareerror (MSE) analysis of the diffusion GTD algorithm (35a)\u2013(35d) and provide convergence guarantees under sufficiently small step-sizes.",
        "context": null
    },
    {
        "title": "Diffusion adaptation strategies for distributed optimization and learning over networks",
        "author": [
            "J. Chen",
            "A.H. Sayed"
        ],
        "venue": "IEEE Transactions on Signal Processing, vol. 60, no. 8, pp. 4289\u20134305, Aug. 2012.",
        "citeRegEx": "17",
        "shortCiteRegEx": null,
        "year": 2012,
        "abstract": "We propose an adaptive diffusion mechanism to optimize a global cost function\nin a distributed manner over a network of nodes. The cost function is assumed\nto consist of a collection of individual components. Diffusion adaptation\nallows the nodes to cooperate and diffuse information in real-time; it also\nhelps alleviate the effects of stochastic gradient noise and measurement noise\nthrough a continuous learning process. We analyze the mean-square-error\nperformance of the algorithm in some detail, including its transient and\nsteady-state behavior. We also apply the diffusion algorithm to two problems:\ndistributed estimation with sparse parameters and distributed localization.\nCompared to well-studied incremental methods, diffusion methods do not require\nthe use of a cyclic path over the nodes and are robust to node and link\nfailure. Diffusion methods also endow networks with adaptation abilities that\nenable the individual nodes to continue learning even when the cost function\nchanges with time. Examples involving such dynamic cost functions with moving\ntargets are common in the context of biological networks.",
        "full_text": "1\nDiffusion Adaptation Strategies for Distributed\nOptimization and Learning over Networks\nJianshu Chen, Student Member, IEEE, and Ali H. Sayed, Fellow, IEEE\nAbstract\nWe propose an adaptive diffusion mechanism to optimize a global cost function in a distributed\nmanner over a network of nodes. The cost function is assumed to consist of a collection of individual\ncomponents. Diffusion adaptation allows the nodes to cooperate and diffuse information in real-time; it\nalso helps alleviate the effects of stochastic gradient noise and measurement noise through a continu-\nous learning process. We analyze the mean-square-error performance of the algorithm in some detail,\nincluding its transient and steady-state behavior. We also apply the diffusion algorithm to two problems:\ndistributed estimation with sparse parameters and distributed localization. Compared to well-studied\nincremental methods, diffusion methods do not require the use of a cyclic path over the nodes and\nare robust to node and link failure. Diffusion methods also endow networks with adaptation abilities\nthat enable the individual nodes to continue learning even when the cost function changes with time.\nExamples involving such dynamic cost functions with moving targets are common in the context of\nbiological networks.\nIndex Terms\nDistributed optimization, diffusion adaptation, incremental techniques, learning, energy conservation,\nbiological networks, mean-square performance, convergence, stability.\nI. INTRODUCTION\nWe consider the problem of optimizing a global cost function in a distributed manner. The cost function\nis assumed to consist of the sum of individual components, and spatially distributed nodes are used to\nManuscript received October 30, 2011; revised March 15, 2012. This work was supported in part by NSF grants CCF-1011918\nand CCF-0942936. Preliminary results related to this work are reported in the conference presentations [1] and [2].\nThe authors are with Department of Electrical Engineering, University of California, Los Angeles, CA 90095. Email: {jshchen,\nsayed}@ee.ucla.edu.\nMay 15, 2012\nDRAFT\narXiv:1111.0034v3  [math.OC]  12 May 2012\n2\nseek the common minimizer (or maximizer) through local interactions. Such problems abound in the\ncontext of biological networks, where agents collaborate with each other via local interactions for a\ncommon objective, such as locating food sources or evading predators [3]. Similar problems are common\nin distributed resource allocation applications and in online machine learning procedures. In the latter\ncase, data that are generated by the same underlying distribution are processed in a distributed manner\nover a network of learners in order to recover the model parameters (e.g., [4], [5]).\nThere are already a few of useful techniques for the solution of optimization problems in a distributed\nmanner [6]\u2013[24]. Most notable among these methods is the incremental approach [6]\u2013[9] and the con-\nsensus approach [10]\u2013[23]. In the incremental approach, a cyclic path is de\ufb01ned over the nodes and\ndata are processed in a cyclic manner through the network until optimization is achieved. However,\ndetermining a cyclic path that covers all nodes is known to be an NP-hard problem [25] and, in addition,\ncyclic trajectories are prone to link and node failures. When any of the edges along the path fails, the\nsharing of data through the cyclic trajectory is interrupted and the algorithm stops performing. In the\nconsensus approach, vanishing step-sizes are used to ensure that nodes reach consensus and converge\nto the same optimizer in steady-state. However, in time-varying environments, diminishing step-sizes\nprevent the network from continuous learning and optimization; when the step-sizes die out, the network\nstops learning. In earlier publications [26]\u2013[35], and motivated by our work on adaptation and learning\nover networks, we introduced the concept of diffusion adaptation and showed how this technique can be\nused to solve global minimum mean-square-error estimation problems ef\ufb01ciently both in real-time and\nin a distributed manner. In the diffusion approach, information is processed locally and simultaneously\nat all nodes and the processed data are diffused through a real-time sharing mechanism that ripples\nthrough the network continuously. Diffusion adaptation was applied to model complex patterns of behavior\nencountered in biological networks, such as bird \ufb02ight formations [36] and \ufb01sh schooling [3]. Diffusion\nadaptation was also applied to solve dynamic resource allocation problems in cognitive radios [37],\nto perform robust system identi\ufb01cation [38], and to implement distributed online learning in pattern\nrecognition applications [5].\nThis paper generalizes the diffusive learning process and applies it to the distributed optimization\nof a wide class of cost functions. The diffusion approach will be shown to alleviate the effect of\ngradient noise on convergence. Most other studies on distributed optimization tend to focus on the\nalmost-sure convergence of the algorithms under diminishing step-size conditions [6], [7], [39]\u2013[43], or\non convergence under deterministic conditions on the data [6]\u2013[8], [15]. In this article we instead examine\nthe distributed algorithms from a mean-square-error perspective at constant step-sizes. This is because\nMay 15, 2012\nDRAFT\n3\nconstant step-sizes are necessary for continuous adaptation, learning, and tracking, which in turn enable\nthe resulting algorithms to perform well even under data that exhibit statistical variations, measurement\nnoise, and gradient noise.\nThis paper is organized as follows. In Sec. II, we introduce the global cost function and approximate\nit by a distributed optimization problem through the use of a second-order Taylor series expansion. In\nSec. III, we show that optimizing the localized alternative cost at each node k leads naturally to diffusion\nadaptation strategies. In Sec. IV, we analyze the mean-square performance of the diffusion algorithms\nunder statistical perturbations when stochastic gradients are used. In Sec. V, we apply the diffusion\nalgorithms to two application problems: sparse distributed estimation and distributed localization. Finally,\nin Sec. VI, we conclude the paper.\nNotation. Throughout the paper, all vectors are column vectors except for the regressors {uk,i}, which\nare taken to be row vectors for simplicity of notation. We use boldface letters to denote random quantities\n(such as uk,i) and regular font letters to denote their realizations or deterministic variables (such as uk,i).\nWe write E to denote the expectation operator. We use diag{x1, . . . , xN} to denote a diagonal matrix\nconsisting of diagonal entries x1, . . . , xN, and use col{x1, . . . , xN} to denote a column vector formed\nby stacking x1, . . . , xN on top of each other. For symmetric matrices X and Y , the notation X \u2264Y\ndenotes Y \u2212X \u22650, namely, that the matrix difference Y \u2212X is positive semi-de\ufb01nite.\nII. PROBLEM FORMULATION\nThe objective is to determine, in a collaborative and distributed manner, the M \u00d71 column vector wo\nthat minimizes a global cost of the form:\nJglob(w) =\nN\nX\nl=1\nJl(w)\n(1)\nwhere Jl(w), l = 1, 2, . . . , N, are individual real-valued functions, de\ufb01ned over w \u2208RM and assumed to\nbe differentiable and strictly convex. Then, Jglob(w) in (1) is also strictly convex so that the minimizer\nwo is unique [44]. In this article we study the important case where the component functions {Jl(w)} are\nminimized at the same wo. This case is common in practice; situations abound where nodes in a network\nneed to work cooperatively to attain a common objective (such as tracking a target, locating the source\nof chemical leak, estimating a physical model, or identifying a statistical distribution). This scenario is\nalso frequent in the context of biological networks. For example, during the foraging behavior of an\nanimal group, each agent in the group is interested in determining the same vector wo that corresponds\nto the location of the food source or the location of the predator [3]. This scenario is equally common\nMay 15, 2012\nDRAFT\n4\nin online distributed machine learning problems, where data samples are often generated from the same\nunderlying distribution and they are processed in a distributed manner by different nodes (e.g., [4], [5]).\nThe case where the {Jl(w)} have different individual minimizers is studied in [45]; this situation is more\nchallenging to study. Nevertheless, it is shown in [45] that the same diffusion strategies (18)\u2013(19) of this\npaper are still applicable and nodes would converge instead to a Pareto-optimal solution.\nOur strategy to optimize the global cost Jglob(w) in a distributed manner is based on three steps.\nFirst, using a second-order Taylor series expansion, we argue that Jglob(w) can be approximated by an\nalternative localized cost that is amenable to distributed optimization \u2014 see (11). Second, each individual\nnode optimizes this alternative cost via a steepest-descent procedure that relies solely on interactions\nwithin the neighborhood of the node. Finally, the local estimates for wo are spatially combined by each\nnode and the procedure repeats itself in real-time.\nTo motivate the approach, we start by introducing a set of nonnegative coef\ufb01cients {cl,k} that satisfy:\nN\nX\nk=1\ncl,k = 1,\ncl,k = 0 if l /\u2208Nk,\nl = 1, 2, . . . , N\n(2)\nwhere Nk denotes the neighborhood of node k (including node k itself); the neighbors of node k consist\nof all nodes with which node k can share information. Each cl,k represents a weight value that node\nk assigns to information arriving from its neighbor l. Condition (2) states that the sum of all weights\nleaving each node l should be one. Using the coef\ufb01cients {cl,k}, we can express Jglob(w) from (1) as\nJglob(w) = Jloc\nk (w) +\nN\nX\nl\u0338=k\nJloc\nl\n(w)\n(3)\nwhere\nJloc\nk (w) \u225c\nX\nl\u2208Nk\ncl,kJl(w)\n(4)\nIn other words, for each node k, we are introducing a new local cost function, Jloc\nk (w), which corresponds\nto a weighted combination of the costs of its neighbors. Since the {cl,k} are all nonnegative and each\nJl(w) is convex, then Jloc\nk (w) is also a convex function (actually, the Jloc\nk (w) will be guaranteed to be\nstrongly convex in our treatment in view of Assumption 1 further ahead).\nNow, each Jloc\nl\n(w) in the second term of (3) can be approximated via a second-order Taylor series\nexpansion as:\nJloc\nl\n(w) \u2248Jloc\nl\n(wo) + \u2225w \u2212wo\u22252\n\u0393l\n(5)\nMay 15, 2012\nDRAFT\n5\n1\n2\n4\n8\n7\nk\n5\n9\n3\n6\nNk\nNk\nak;1\nck;1\nJk(w)\nJ1(w)\nJ2(w)\nJ3(w)\nJ5(w)\nJ6(w)\nJ4(w)\nJ7(w)\nJ9(w)\nJ8(w)\na1;k\nc1;k\nFig. 1.\nA network with N nodes; a cost function Jk(w) is associated with each node k. The set of neighbors of node k is\ndenoted by Nk; this set consists of all nodes with which node k can share information.\nwhere \u0393l = 1\n2\u22072\nwJloc\nl\n(wo) is the (scaled) Hessian matrix relative to w and evaluated at w =wo, and the\nnotation \u2225a\u22252\n\u03a3 denotes aT \u03a3a for any weighting matrix \u03a3. The analysis in the subsequent sections will\nshow that the second-order approximation (5) is suf\ufb01cient to ensure mean-square convergence of the\nresulting diffusion algorithm. Now, substituting (5) into the right-hand side of (3) gives:\nJglob(w) \u2248Jloc\nk (w)+\nX\nl\u0338=k\n\u2225w\u2212wo\u22252\n\u0393l+\nX\nl\u0338=k\nJloc\nl\n(wo)\n(6)\nThe last term in the above expression does not depend on the unknown w. Therefore, we can ignore it\nso that optimizing Jglob(w) is approximately equivalent to optimizing the following alternative cost:\nJglob\u2032(w) \u225cJloc\nk (w) +\nX\nl\u0338=k\n\u2225w \u2212wo\u22252\n\u0393l\n(7)\nIII. ITERATIVE DIFFUSION SOLUTION\nExpression (7) relates the original global cost (1) to the newly-de\ufb01ned local cost function Jloc\nk (w).\nThe relation is through the second term on the right-hand side of (7), which corresponds to a sum of\nquadratic terms involving the minimizer wo. Obviously, wo is not available at node k since the nodes wish\nto estimate wo. Likewise, not all Hessian matrices \u0393l are available to node k. Nevertheless, expression (7)\nsuggests a useful approximation that leads to a powerful distributed solution, as we proceed to explain.\nOur \ufb01rst step is to replace the global cost Jglob\u2032(w) by a reasonable localized approximation for it at\nevery node k. Thus, initially we limit the summation on the right-hand side of (7) to the neighbors of\nnode k and introduce the cost function:\nJglob\u2032\nk\n(w) \u225cJloc\nk (w) +\nX\nl\u2208Nk\\{k}\n\u2225w \u2212wo\u22252\n\u0393l\n(8)\nMay 15, 2012\nDRAFT\n6\nCompared with (7), the last term in (8) involves only quantities that are available in the neighborhood of\nnode k. The argument involving steps (5)\u2013(8) therefore shows us one way by which we can adjust the\nearlier local cost function Jloc\nk (w) de\ufb01ned in (4) by adding to it the last term that appears in (8). Doing\nso, we end up replacing Jloc\nk (w) by Jglob\u2032\nk\n(w), and this new localized cost function preserves the second\nterm in (3) up to a second-order approximation. This correction will help lead to a diffusion step (see\n(14)\u2013(15)).\nNow, observe that the cost in (8) includes the quantities {\u0393l}, which belong to the neighbors of node k.\nThese quantities may or may not be available. If they are known, then we can proceed with (8) and rely\non the use of the Hessian matrices \u0393l in the subsequent development. Nevertheless, the more interesting\nsituation in practice is when these Hessian matrices are not known beforehand (especially since they\ndepend on the unknown wo). For this reason, in this article, we approximate each \u0393l in (8) by a multiple\nof the identity matrix, say,\n\u0393l \u2248bl,kIM\n(9)\nfor some nonnegative coef\ufb01cients {bl,k}; observe that we are allowing the coef\ufb01cient bl,k to vary with\nthe node index k. Such approximations are common in stochastic approximation theory and help reduce\nthe complexity of the resulting algorithms \u2014 see [44, pp.20\u201328] and [46, pp.142\u2013147]. Approximation\n(9) is reasonable since, in view of the Rayleigh-Ritz characterization of eigenvalues [47], we can always\nbound the weighted squared norm \u2225w \u2212wo\u22252\n\u0393l by the unweighted squared norm as follows\n\u03bbmin(\u0393l) \u00b7 \u2225w\u2212wo\u22252 \u2264\u2225w\u2212wo\u22252\n\u0393l \u2264\u03bbmax(\u0393l) \u00b7 \u2225w\u2212wo\u22252\nThus, we replace (8) by\nJglob\u2032\u2032\nk\n(w) \u225cJloc\nk (w) +\nX\nl\u2208Nk\\{k}\nbl,k\u2225w \u2212wo\u22252\n(10)\nAs the derivation will show, we do not need to worry at this stage about how the scalars {bl,k} are\nselected; they will be embedded into other combination weights that the designer selects. If we replace\nJloc\nk (w) by its de\ufb01nition (4), we can rewrite (10) as\nJglob\u2032\u2032\nk\n(w) =\nX\nl\u2208Nk\ncl,kJl(w) +\nX\nl\u2208Nk\\{k}\nbl,k\u2225w\u2212wo\u22252\n(11)\nObserve that cost (11) is different for different nodes; this is because the choices of the weighting scalars\n{cl,k, bl,k} vary across nodes k; moreover, the neighborhoods vary with k. Nevertheless, these localized\ncost functions now constitute the important starting point for the development of diffusion strategies for\nthe online and distributed optimization of (1).\nMay 15, 2012\nDRAFT\n7\nEach node k can apply a steepest-descent iteration to minimize Jglob\u2032\u2032\nk\n(w) by moving along the negative\ndirection of the gradient (column) vector of the cost function, namely,\nwk,i = wk,i\u22121 \u2212\u00b5k\nX\nl\u2208Nk\ncl,k\u2207wJl(wk,i\u22121)\n\u2212\u00b5k\nX\nl\u2208Nk\\{k}\n2bl,k(wk,i\u22121 \u2212wo),\ni \u22650\n(12)\nwhere wk,i denotes the estimate for wo at node k at time i, and \u00b5k denotes a small constant positive\nstep-size parameter. While vanishing step-sizes, such as \u00b5k(i) = 1/i, can be used in (12), we consider in\nthis paper the case of constant step-sizes. This is because we are interested in distributed strategies that\nare able to continue adapting and learning. An important question to address therefore is how close each\nof the wk,i gets to the optimal solution wo; we answer this question later in the paper by means of a\nmean-square-error convergence analysis (see expression (88)). It will be seen then that the mean-square-\nerror (MSE) of the algorithm will be of the order of the step-size; hence, suf\ufb01ciently small step-sizes\nwill lead to suf\ufb01ciently small MSEs.\nExpression (12) adds two correction terms to the previous estimate, wk,i\u22121, in order to update it to\nwk,i. The correction terms can be added one at a time in a succession of two steps, for example, as:\n\u03c8k,i = wk,i\u22121 \u2212\u00b5k\nX\nl\u2208Nk\ncl,k\u2207wJl(wk,i\u22121)\n(13)\nwk,i = \u03c8k,i \u2212\u00b5k\nX\nl\u2208Nk\\{k}\n2bl,k(wk,i\u22121 \u2212wo)\n(14)\nStep (13) updates wk,i\u22121 to an intermediate value \u03c8k,i by using a combination of local gradient vectors.\nStep (14) further updates \u03c8k,i to wk,i by using a combination of local estimates. However, two issues\narise while examining (14):\n(a) First, iteration (14) requires knowledge of the optimizer wo. However, all nodes are running similar\nupdates to estimate the wo. By the time node k wishes to apply (14), each of its neighbors would\nhave performed its own update similar to (13) and would have available their intermediate estimates,\n{\u03c8l,i}. Therefore, we replace wo in (14) by \u03c8l,i. This step helps diffuse information over the network\nand brings into node k information that exists beyond its immediate neighborhood; this is because\neach \u03c8l,i is in\ufb02uenced by data from the neighbors of node l. We observe that this diffusive term\narises from the quadratic approximation (5) we have made to the second term in (3).\n(b) Second, the intermediate value \u03c8k,i in (13) is generally a better estimate for wo than wk,i\u22121\nsince it is obtained by incorporating information from the neighbors through (13). Therefore, we\nMay 15, 2012\nDRAFT\n8\nfurther replace wk,i\u22121 in (14) by \u03c8k,i. This step is reminiscent of incremental-type approaches to\noptimization, which have been widely studied in the literature [6]\u2013[9].\nPerforming the substitutions described in items (a) and (b) into (14), we obtain:\nwk,i = \u03c8k,i \u2212\u00b5k\nX\nl\u2208Nk\\{k}\n2bl,k(\u03c8k,i \u2212\u03c8l,i)\n(15)\nNow introduce the coef\ufb01cients\nal,k \u225c2\u00b5kbl,k\n(l\u0338=k),\nak,k \u225c1\u2212\u00b5k\nX\nl\u2208Nk\\{k}\n2bl,k\n(16)\nNote that the {al,k} are nonnegative for l \u0338= k and ak,k \u22650 for suf\ufb01ciently small step-sizes. Moreover,\nthe coef\ufb01cients {al,k} satisfy\nN\nX\nl=1\nal,k = 1,\nal,k = 0 if l /\u2208Nk\n(17)\nUsing (16) in (15), we arrive at the following Adapt-then-Combine (ATC) diffusion strategy (whose\nstructure is the same as the ATC algorithm originally proposed in [29]\u2013[31] for mean-square-error\nestimation):\n\u03c8k,i = wk,i\u22121 \u2212\u00b5k\nX\nl\u2208Nk\ncl,k\u2207wJl(wk,i\u22121)\nwk,i =\nX\nl\u2208Nk\nal,k\u03c8l,i\n(18)\nTo run algorithm (18), we only need to select combination coef\ufb01cients {al,k, cl,k} satisfying (2) and (17),\nrespectively; there is no need to worry about the intermediate coef\ufb01cients {bl,k} any more, since they\nhave been blended into the {al,k}. The ATC algorithm (18) involves two steps. In the \ufb01rst step, node k\nreceives gradient vector information from its neighbors and uses it to update its estimate wk,i\u22121 to an\nintermediate value \u03c8k,i. All other nodes in the network are performing a similar step and generating their\nintermediate estimate \u03c8l,i. In the second step, node k aggregates the estimates {\u03c8l,i} of its neighbors and\ngenerates wk,i. Again, all other nodes are performing a similar step. Similarly, if we reverse the order of\nsteps (13) and (14) to implement (12), we can motivate the following alternative Combine-then-Adapt\n(CTA) diffusion strategy (whose structure is similar to the CTA algorithm originally proposed in [26]\u2013[32]\nfor mean-square-error estimation):\n\u03c8k,i\u22121 =\nX\nl\u2208Nk\nal,kwl,i\u22121\nwk,i = \u03c8k,i\u22121 \u2212\u00b5k\nX\nl\u2208Nk\ncl,k\u2207wJl(\u03c8k,i\u22121)\n(19)\nMay 15, 2012\nDRAFT\n9\nAdaptive diffusion strategies of the above ATC and CTA types were \ufb01rst proposed and extended in [26]\u2013\n[34] for the solution of distributed mean-square-error, least-squares, and state-space estimation problems\nover networks. The special form of ATC strategy (18) for minimum-mean-square-error estimation is listed\nfurther ahead as Eq. (57) in Example 3; the same strategy as (57) also appeared in [48] albeit with a\nvanishing step-size sequence to ensure convergence towards consensus. A special case of the diffusion\nstrategy (19) (corresponding to choosing cl,k = 0 for l \u0338= k and ck,k = 1, i.e., without sharing gradient\ninformation) was used in the works [39], [40], [43] to solve distributed optimization problems that require\nall nodes to reach agreement about wo by relying on step-sizes that decay to zero with time. Diffusion\nrecursions of the forms (18) and (19) are more general than these earlier investigations in a couple of\nrespects. First, they do not only diffuse the local estimates, but they can also diffuse the local gradient\nvectors. In other words, two sets of combination coef\ufb01cients {al,k, cl,k} are used. Second, the combination\nweights {al,k} are not required to be doubly stochastic (which would require both the rows and columns\nof the weighting matrix A = [al,k] to add up to one; as seen from (17), we only require the entries on\nthe columns of A to add up to one). Finally, and most importantly, the step-size parameters {\u00b5k} in (18)\nand (19) are not required to depend on the time index i and are not required to vanish as i \u2192\u221e. Instead,\nthey can assume constant values, which is critical to endow the network with continuous adaptation and\nlearning abilities (otherwise, when step-sizes die out, the network stops learning). Constant step-sizes\nalso endow networks with tracking abilities, in which case the algorithms can track time changes in the\noptimal wo.\nConstant step-sizes will be shown further ahead to be suf\ufb01cient to guarantee agreement among the\nnodes when there is no noise in the data. However, when measurement noise and gradient noise are\npresent, using constant step-sizes does not force the nodes to attain agreement about wo (i.e., to converge\nto the same wo). Instead, the nodes will be shown to tend to individual estimates for wo that are within\na small mean-square-error (MSE) bound from the optimal solution; the bound will be proportional to the\nstep-size so that suf\ufb01ciently small step-sizes lead to small MSE values. Multi-agent systems in nature\nbehave in this manner; they do not require exact agreement among their agents but allow for \ufb02uctuations\ndue to individual noise levels (see [3], [36]). Giving individual nodes this \ufb02exibility, rather than forcing\nthem to operate in agreement with the remaining nodes, ends up leading to nodes with enhanced learning\nabilities.\nBefore proceeding to a detailed analysis of the performance of the diffusion algorithms (18)\u2013(19), we\nnote that these strategies differ in important ways from traditional consensus-based distributed solutions,\nMay 15, 2012\nDRAFT\n10\nwhich are of the following form [10], [14], [15], [18]:\nwk,i =\nX\nl\u2208Nk\nal,kwk,i\u22121 \u2212\u00b5k(i) \u00b7 \u2207wJl(wk,i\u22121)\n(20)\nusually with a time-variant step-size sequence, \u00b5k(i), that decays to zero. For example, if we set C \u225c\n[cl,k] = I in the CTA algorithm (19) and substitute the combination step into the adaptation step, we\nobtain:\nwk,i =\nX\nl\u2208Nk\nal,kwk,i\u22121 \u2212\u00b5k\u2207wJl\n X\nl\u2208Nk\nal,kwk,i\u22121\n!\n(21)\nThus, note that the gradient vector in (21) is evaluated at \u03c8k,i\u22121, while in (20) it is evaluated at wk,i\u22121.\nSince \u03c8k,i\u22121 already incorporates information from neighbors, we would expect the diffusion algorithm\nto perform better. Actually, it is shown in [49] that, for mean-square-error estimation problems, diffusion\nstrategies achieve higher convergence rate and lower mean-square-error than consensus strategies due to\nthese differences in the dynamics of the algorithms.\nIV. MEAN-SQUARE PERFORMANCE ANALYSIS\nThe diffusion algorithms (18) and (19) depend on sharing local gradient vectors \u2207wJl(\u00b7). In many\ncases of practical relevance, the exact gradient vectors are not available and approximations are instead\nused. We model the inaccuracy in the gradient vectors as some random additive noise component, say,\nof the form:\nb\u2207wJl(w) = \u2207wJl(w) + vl,i(w)\n(22)\nwhere vl,i(\u00b7) denotes the perturbation and is often referred to as gradient noise. Note that we are using\na boldface symbol v to refer to the gradient noise since it is generally stochastic in nature.\nExample 1. Assume the individual cost Jl(w) at node l can be expressed as the expected value of a\ncertain loss function Ql(\u00b7, \u00b7), i.e., Jl(w) = E{Ql(w, xl,i)}, where the expectation is with respect to the\nrandomness in the data samples {xl,i} that are collected at node l at time i. Then, if we replace the true\ngradient \u2207wJl(w) with its stochastic gradient approximation b\u2207wJl(w) = \u2207wQl(w, xl,i), we \ufb01nd that\nthe gradient noise in this case can be expressed as\nvl,i(w) = \u2207wQl(w, xl,i) \u2212\u2207wE{Ql(w, xl,i)}\n(23)\nMay 15, 2012\nDRAFT\n11\nUsing the perturbed gradient vectors (22), the diffusion algorithms (18)\u2013(19) become the following:\n(ATC)\n\u03c8k,i = wk,i\u22121\u2212\u00b5k\nX\nl\u2208Nk\ncl,k b\u2207wJl(wk,i\u22121)\nwk,i =\nX\nl\u2208Nk\nal,k\u03c8l,i\n(24)\n(CTA)\n\u03c8k,i\u22121 =\nX\nl\u2208Nk\nal,kwl,i\u22121\nwk,i = \u03c8k,i\u22121\u2212\u00b5k\nX\nl\u2208Nk\ncl,k b\u2207wJl(\u03c8k,i\u22121)\n(25)\nObserve that, starting with (24)\u2013(25), we will be using boldface letters to refer to the various estimate\nquantities in order to highlight the fact that they are also stochastic in nature due to the presence of the\ngradient noise.\nGiven the above algorithms, it is necessary to examine their performance in light of the approximation\nsteps (6)\u2013(15) that were employed to arrive at them, and in light of the gradient noise (22) that seeps\ninto the recursions. A convenient framework to carry out this analysis is mean-square analysis. In this\nframework, we assess how close the individual estimates wk,i get to the minimizer wo in the mean-square-\nerror (MSE) sense. In practice, it is not necessary to force the individual agents to reach agreement and\nto converge to the same wo using diminishing step-sizes. It is suf\ufb01cient for the nodes to converge within\nacceptable MSE bounds from wo. This \ufb02exibility is bene\ufb01cial and is common in biological networks; it\nallows nodes to learn and adapt in time-varying environments without the forced requirement of having\nto agree with neighbors.\nThe main results that we derive in this section are summarized as follows. First, we derive conditions on\nthe constant step-sizes to ensure boundedness and convergence of the mean-square-error for suf\ufb01ciently\nsmall step-sizes \u2014 see (80) and (106) further ahead. Second, despite the fact that nodes in\ufb02uence each\nother\u2019s behavior, we are able to quantify the performance of every node in the network and to derive\nclosed-form expressions for the mean-square performance at small step-sizes \u2014 see (106)\u2013(108). Finally,\nas a special case, we are able to show that constant step-sizes can still ensure that the estimates across\nall nodes converge to the optimal wo and reach agreement in the absence of noise \u2014 see Theorem 2.\nMotivated by [31], we address the mean-square-error performance of the adaptive ATC and CTA\ndiffusion strategies (24)\u2013(25) by treating them as special cases of a general diffusion structure of the\nMay 15, 2012\nDRAFT\n12\nfollowing form:\n\u03c6k,i\u22121 =\nN\nX\nl=1\np1,l,kwl,i\u22121\n(26)\n\u03c8k,i = \u03c6k,i\u22121 \u2212\u00b5k\nN\nX\nl=1\nsl,k\nh\n\u2207wJl(\u03c6k,i\u22121) + vl,i(\u03c6k,i\u22121)\ni\n(27)\nwk,i =\nN\nX\nl=1\np2,l,k\u03c8l,i\n(28)\nThe coef\ufb01cients {p1,l,k}, {sl,k}, and {p2,l,k} are nonnegative real coef\ufb01cients corresponding to the {l, k}-\nth entries of three matrices P1, S, and P2, respectively. Different choices for {P1, P2, S} correspond to\ndifferent cooperation modes. For example, the choice P1 = I, P2 = I and S = I corresponds to the\nnon-cooperative case where nodes do not interact. On the other hand, the choice P1 = I, P2 = A = [al,k]\nand S = C = [cl,k] corresponds to ATC [29]\u2013[31], while the choice P1 = A, P2 = I and S = C\ncorresponds to CTA [26]\u2013[31]. We can also set S = I in ATC and CTA to derive simpli\ufb01ed versions\nthat have no gradient exchange [29]. Furthermore, if in CTA (P2 = I), we enforce P1 = A to be doubly\nstochastic, set S = I, and use a time-decaying step-size parameter (\u00b5k(i) \u21920), then we obtain the\nunconstrained version used by [39], [43]. The matrices {P1, P2, S} are required to satisfy:\nP T\n1 1 = 1, P T\n2 1 = 1, S1 = 1\n(29)\nwhere the notation 1 denotes a vector whose entries are all equal to one.\nA. Error Recursions\nWe \ufb01rst derive the error recursions corresponding to the general diffusion formulation in (26)\u2013(28).\nIntroduce the error vectors:\n\u02dc\u03c6k,i \u225cwo\u2212\u03c6k,i, \u02dc\u03c8k,i \u225cwo\u2212\u03c8k,i, \u02dcwk,i \u225cwo\u2212wk,i\n(30)\nThen, subtracting both sides of (26)\u2013(28) from wo gives:\n\u02dc\u03c6k,i\u22121 =\nN\nX\nl=1\np1,l,k \u02dcwl,i\u22121\n(31)\n\u02dc\u03c8k,i = \u02dc\u03c6k,i\u22121 + \u00b5k\nN\nX\nl=1\nsl,k\nh\n\u2207wJl(\u03c6k,i\u22121) + vl,i(\u03c6k,i\u22121)\ni\n(32)\n\u02dcwk,i =\nN\nX\nl=1\np2,l,k \u02dc\u03c8l,i\n(33)\nMay 15, 2012\nDRAFT\n13\nExpression (32) still includes terms that depend on \u03c6k,i\u22121 and not on the error quantity, \u02dc\u03c6k,i\u22121. We\ncan \ufb01nd a relation in terms of \u02dc\u03c6k,i\u22121 by calling upon the following result from [44, p.24] for any\ntwice-differentiable function f(\u00b7):\n\u2207f(y) = \u2207f(x) +\n\u0014Z 1\n0\n\u22072f\n\u0010\nx+t(y\u2212x)\n\u0011\ndt\n\u0015\n(y \u2212x)\n(34)\nwhere \u22072f(\u00b7) denotes the Hessian matrix of f(\u00b7) and is symmetric. Now, since each component function\nJl(w) has a minimizer at wo, then, \u2207wJl(wo) = 0 for l = 1, 2, . . . , N. Applying (34) to Jl(w) using\nx = wo and y = \u03c6k,i\u22121, we get\n\u2207wJl(\u03c6k,i\u22121)\n= \u2207wJl(wo) \u2212\n\u0014Z 1\n0\n\u22072\nwJl\n\u0010\nwo \u2212t \u02dc\u03c6k,i\u22121\n\u0011\ndt\n\u0015\n\u02dc\u03c6k,i\u22121\n\u225c\u2212Hl,k,i\u22121 \u02dc\u03c6k,i\u22121\n(35)\nwhere we are introducing the symmetric random matrix\nHl,k,i\u22121 \u225c\nZ 1\n0\n\u22072\nwJl\n\u0010\nwo \u2212t \u02dc\u03c6k,i\u22121\n\u0011\ndt\n(36)\nObserve that one such matrix is associated with every edge linking two nodes (l, k); observe further that\nthis matrix changes with time since it depends on the estimate at node k. Substituting (35)\u2013(36) into (32)\nleads to:\n\u02dc\u03c8k,i =\n\"\nIM \u2212\u00b5k\nN\nX\nl=1\nsl,kHl,k,i\u22121\n#\n\u02dc\u03c6k,i\u22121\n+ \u00b5k\nN\nX\nl=1\nsl,kvl,i(\u03c6k,i\u22121)\n(37)\nWe introduce the network error vectors, which collect the error quantities across all nodes:\n\u02dc\u03c6i \u225c\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0\n\u02dc\u03c61,i\n...\n\u02dc\u03c6N,i\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb\n,\n\u02dc\u03c8i \u225c\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0\n\u02dc\u03c81,i\n...\n\u02dc\u03c8N,i\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb\n,\n\u02dcwi \u225c\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0\n\u02dcw1,i\n...\n\u02dcwN,i\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb\n(38)\nMay 15, 2012\nDRAFT\n14\nand the following block matrices:\nP1 = P1 \u2297IM, P2 = P2 \u2297IM\n(39)\nS = S \u2297IM, M = \u2126\u2297IM\n(40)\n\u2126= diag {\u00b51, . . . , \u00b5N}\n(41)\nDi\u22121 =\nN\nX\nl=1\ndiag\nn\nsl,1Hl,1,i\u22121, \u00b7 \u00b7 \u00b7 , sl,NHl,N,i\u22121\no\n(42)\ngi =\nN\nX\nl=1\ncol\nn\nsl,1vl,i(\u03c61,i\u22121), \u00b7 \u00b7 \u00b7 ,sl,Nvl,i(\u03c6N,i\u22121)\no\n(43)\nwhere the symbol \u2297denotes Kronecker products [50]. Then, recursions (31), (37) and (33) lead to:\n\u02dcwi = PT\n2 [IMN \u2212MDi\u22121]PT\n1 \u02dcwi\u22121 + PT\n2 Mgi\n(44)\nTo proceed with the analysis, we introduce the following assumption on the cost functions and gradient\nnoise, followed by a lemma on Hl,k,i\u22121.\nAssumption 1 (Bounded Hessian). Each component cost function Jl(w) has a bounded Hessian matrix,\ni.e., there exist nonnegative real numbers \u03bbl,min and \u03bbl,max such that \u03bbl,min \u2264\u03bbl,max and that for all w:\n\u03bbl,minIM \u2264\u22072\nwJl(w) \u2264\u03bbl,maxIM\n(45)\nFurthermore, the {\u03bbl,min}N\nl=1 satisfy\nN\nX\nl=1\nsl,k\u03bbl,min > 0, k = 1, 2, . . . , N\n(46)\nCondition (46) ensures that the local cost functions {Jloc\nk (w)} de\ufb01ned earlier in (4) are strongly convex\nand, hence, have a unique minimizer at wo.\nAssumption 2 (Gradient noise). There exist \u03b1 \u22650 and \u03c32\nv \u22650 such that, for all w \u2208Fi\u22121 and for all\ni, l:\nE {vl,i(w) | Fi\u22121} = 0\n(47)\nE\n\b\n\u2225vl,i(w)\u22252\t\n\u2264\u03b1E\u2225wo \u2212w\u22252 + \u03c32\nv\n(48)\nwhere Fi\u22121 denotes the past history (\u03c3\u2212\ufb01eld) of estimates {wk,j} for j \u2264i \u22121 and all k.\nMay 15, 2012\nDRAFT\n15\nLemma 1 (Bound on Hl,k,i\u22121). Under Assumption 1, the matrix Hl,k,i\u22121 de\ufb01ned in (36) is a nonnegative-\nde\ufb01nite matrix that satis\ufb01es:\n\u03bbl,minIM \u2264Hl,k,i\u22121 \u2264\u03bbl,maxIM\n(49)\nProof: It suf\ufb01ces to prove that \u03bbl,min \u2264xT Hl,k,i\u22121x \u2264\u03bbl,max for arbitrary M \u00d7 1 unit Euclidean norm\nvectors x. By (36) and (45), we have\nxT Hl,k,i\u22121x =\nZ 1\n0\nxT \u22072\nwJl\n\u0010\nwo \u2212t \u02dc\u03c6k,i\u22121\n\u0011\nx dt\n\u2264\nZ 1\n0\n\u03bbl,maxdt = \u03bbl,max\nIn a similar way, we can verify that xT Hl,k,i\u22121x \u2265\u03bbl,min.\nIn distributed subgradient methods (e.g., [15], [39], [43]), the norms of the subgradients are usually\nrequired to be uniformly bounded. Such assumption is restrictive in the unconstrained optimization of\ndifferentiable functions.\nAssumption 1 is more relaxed in that it allows the gradient vector \u2207wJl(w)\nto have unbounded norm (e.g., quadratic costs). Furthermore, condition (48) allows the variance of the\ngradient noise to grow no faster than E\u2225wo \u2212w\u22252. This condition is also more general than the uniform\nbounded assumption used in [39] (Assumptions 5.1 and 6.1), which requires instead:\nE\u2225vl,i(w)\u22252 \u2264\u03c32\nv,\nE\n\b\n\u2225vl,i(w)\u22252|Fi\u22121\n\t\n\u2264\u03c32\nv\n(50)\nFurthermore, condition (48) is similar to condition (4.3) in [51, p.635]:\nE\n\b\n\u2225vl,i(w)\u22252|Fi\u22121\n\t\n\u2264\u03b1\nh\n\u2225\u2207wJl(w)\u22252 + 1\ni\n(51)\nwhich is a combination of the \u201crelative random noise\u201d and the \u201cabsolute random noise\u201d conditions de\ufb01ned\nin [44, pp.100\u2013102]. Indeed, we can derive (48) by substituting (35) into (51), taking expectation with\nrespect to Fi\u22121, and then using (49).\nExample 2. Such a mix of \u201crelative random noise\u201d and \u201cabsolute random noise\u201d is of practical impor-\ntance. For instance, consider an example in which the loss function at node l is chosen to be of the\nfollowing quadratic form:\nQl(w, {ul,i, dl(i)}) = |dl(i) \u2212ul,iw|2\nfor some scalars {dl(i)} and 1 \u00d7 M regression vectors {ul,i}. The corresponding cost function is then:\nJl(w) = E|dl(i) \u2212ul,iw|2\n(52)\nMay 15, 2012\nDRAFT\n16\nAssume further that the data {ul,i, dl(i)} satisfy the linear regression model\ndl(i) = ul,iwo + zl(i)\n(53)\nwhere the regressors {ul,i} are zero mean and independent over time with covariance matrix Ru,l =\nE{uT\nl,iul,i}, and the noise sequence {zk(j)} is also zero mean, white, with variance \u03c32\nz,k, and independent\nof the regressors {ul,i} for all l, k, i, j. Then, using (53) and (23), the gradient noise in this case can be\nexpressed as:\nvl,i(w) = 2(Ru,l \u2212uT\nl,iul,i)(wo \u2212w) \u22122uT\nl,izl(i)\n(54)\nIt can easily be veri\ufb01ed that this noise satis\ufb01es both conditions stated in Assumption 2, namely, (47) and\nalso:\nE\n\b\n\u2225vl,i(w)\u22252\t\n\u22644E\u2225Ru,l\u2212uT\nl,iul,i\u22252 \u00b7 E\u2225wo\u2212w\u22252+4\u03c32\nz,lTr(Ru,l)\n(55)\nfor all w \u2208Fi\u22121. Note that both relative random noise and absolute random noise components appear in\n(55) and are necessary to model the statistical gradient perturbation even for quadratic costs. Such costs,\nand linear regression models of the form (53), arise frequently in the context of adaptive \ufb01lters \u2014 see,\ne.g., [9], [26]\u2013[33], [36], [46], [52]\u2013[55].\nExample 3. Quadratic costs of the form (52) are common in mean-square-error estimation for linear\nregression models of the type (53). If we use the instantaneous approximations as is common in the context\nof stochastic approximation and adaptive \ufb01ltering [44], [46], [52], then the actual gradient \u2207wJl(w) can\nbe approximated by\nb\u2207wJl(w) = \u2207wQl(w, {ul,i, dl(i)})\n= \u22122uT\nl,i[dl(i) \u2212ul,iw]\n(56)\nSubstituting into (24)\u2013(25), and assuming C = I for illustration purposes only, we arrive at the following\nATC and CTA diffusion strategies originally proposed and extended in [26]\u2013[31] for the solution of\ndistributed mean-square-error estimation problems:\n(ATC)\n\u03c8k,i = wk,i\u22121+2\u00b5kuT\nk,i[dk(i)\u2212uk,iwk,i\u22121]\nwk,i =\nX\nl\u2208Nk\nal,k\u03c8l,i\n(57)\nMay 15, 2012\nDRAFT\n17\n(CTA)\n\u03c8k,i\u22121 =\nX\nl\u2208Nk\nal,kwl,i\u22121\nwk,i = \u03c8k,i\u22121+2\u00b5kuT\nk,i[dk(i)\u2212uk,i\u03c8k,i\u22121]\n(58)\nB. Variance Relations\nThe purpose of the mean-square analysis in the sequel is to answer two questions in the presence\nof gradient perturbations. First, how small the mean-square error, E\u2225\u02dcwk,i\u22252, gets as i \u2192\u221efor any\nof the nodes k. Second, how fast this error variance tends towards its steady-state value. The \ufb01rst\nquestion pertains to steady-state performance and the second question pertains to transient/convergence\nrate performance. Answering such questions for a distributed algorithm over a network is a challenging\ntask largely because the nodes in\ufb02uence each other\u2019s behavior: performance at one node diffuses through\nthe network to the other nodes as a result of the topological constraints linking the nodes. The approach\nwe take to examine the mean-square performance of the diffusion algorithms is by studying how the\nvariance E\u2225\u02dcwk,i\u22252, or a weighted version of it, evolves over time. As the derivation will show, the\nevolution of this variance satis\ufb01es a nonlinear relation. Under some reasonable assumptions on the noise\npro\ufb01le, and the local cost functions, we will be able to bound these error variances as well as estimate\ntheir steady-state values for suf\ufb01ciently small step-sizes. We will also derive closed-form expressions that\ncharacterize the network performance. The details are as follows.\nEquating the squared weighted Euclidean norm of both sides of (44), applying the expectation operator\nand using using (47), we can show that the following variance relation holds:\nE\u2225\u02dcwi\u22252\n\u03a3 = E\nn\n\u2225\u02dcwi\u22121\u22252\n\u03a3\u2032\no\n+ E\u2225PT\n2 Mgi\u22252\n\u03a3\n\u03a3\u2032 = P1[IMN \u2212MDi\u22121]P2\u03a3PT\n2 [IMN \u2212MDi\u22121]PT\n1\n(59)\nwhere \u03a3 is a positive semi-de\ufb01nite weighting matrix that we are free to choose. The variance expression\n(59) shows how the quantity E\u2225\u02dcwi\u22252\n\u03a3 evolves with time. Observe, however, that the weighting matrix\non \u02dcwi\u22121 on the right-hand side of (59) is a different matrix, denoted by \u03a3\u2032, and this matrix is actually\nrandom in nature (while \u03a3 is deterministic). As such, result (59) is not truly a recursion. Nevertheless,\nit is possible, under a small step-size approximation, to rework variance relations such as (59) into a\nrecursion by following certain steps that are characteristic of the energy conservation approach to mean-\nsquare analysis [46].\nThe \ufb01rst step in this regard would be to replace \u03a3\u2032 by its mean E\u03a3\u2032. However, the matrix \u03a3\u2032 depends on\nthe {Hl,k,i\u22121} via Di\u22121 (see (42)). It follows from the de\ufb01nition of Hl,k,i\u22121 in (36) that \u03a3\u2032 is dependent\nMay 15, 2012\nDRAFT\n18\non \u02dc\u03c6k,i\u22121 as well, which in turn is a linear combination of the { \u02dcwl,i\u22121}. Therefore, the main challenge to\ncontinue from (59) is that \u03a3\u2032 depends on \u02dcwi\u22121. For this reason, we cannot apply directly the traditional\nstep of replacing \u03a3\u2032 in the \ufb01rst equation of (59) by E\u03a3\u2032 as is typically done in the study of stand-alone\nadaptive \ufb01lters to analyze their transient behavior [46, p.345]; in the case of conventional adaptive \ufb01lters,\nthe matrix \u03a3\u2032 is independent of \u02dcwi\u22121. To address this dif\ufb01culty, we shall adjust the argument to rely\non a set of inequality recursions that will enable us to bound the steady-state mean-square-error at each\nnode \u2014 see Theorem 1 further ahead.\nThe procedure is as follows. First, we note that \u2225x\u22252 is a convex function of x, and that expressions\n(31) and (33) are convex combinations of { \u02dcwl,i\u22121} and { \u02dc\u03c8l,i}, respectively. Then, by Jensen\u2019s inequality\n[56, p.77] and taking expectations, we obtain\nE\u2225\u02dc\u03c6k,i\u22121\u22252 \u2264\nN\nX\nl=1\np1,l,kE\u2225\u02dcwl,i\u22121\u22252\n(60)\nE\u2225\u02dcwk,i\u22252 \u2264\nN\nX\nl=1\np2,l,kE\u2225\u02dc\u03c8l,i\u22252\n(61)\nfor k = 1, . . . , N. Next, we derive a variance relation for (37). Equating the squared Euclidean norms of\nboth sides of (37), applying the expectation operator, and using (47) from Assumption 2, we get\nE\u2225\u02dc\u03c8k,i\u22252 = E\nn\n\u2225\u02dc\u03c6k,i\u22121\u22252\n\u03a3k,i\u22121\no\n+\u00b52\nkE\n\r\r\r\r\r\nN\nX\nl=1\nsl,kvl,i(\u03c6k,i\u22121)\n\r\r\r\r\r\n2\n(62)\nwhere\n\u03a3k,i\u22121 =\n\"\nIM \u2212\u00b5k\nN\nX\nl=1\nsl,kHl,k,i\u22121\n#2\n(63)\nWe call upon the following two lemmas to bound (62).\nLemma 2 (Bound on \u03a3k,i\u22121). The weighting matrix \u03a3k,i\u22121 de\ufb01ned in (63) is a symmetric, positive\nsemi-de\ufb01nite matrix, and satis\ufb01es:\n0 \u2264\u03a3k,i\u22121 \u2264\u03b32\nkIM\n(64)\nwhere\n\u03b3k \u225cmax\n(\f\f\f\f\f1\u2212\u00b5k\nN\nX\nl=1\nsl,k\u03bbl,max\n\f\f\f\f\f,\n\f\f\f\f\f1\u2212\u00b5k\nN\nX\nl=1\nsl,k\u03bbl,min\n\f\f\f\f\f\n)\n(65)\nMay 15, 2012\nDRAFT\n19\nProof: By de\ufb01nition (63) and the fact that Hl,k,i\u22121 is symmetric \u2014 see de\ufb01nition (36), the matrix\nIM\u2212\u00b5k\nPN\nl=1 sl,kHl,k,i\u22121 is also symmetric. Hence, its square, \u03a3k,i\u22121, is symmetric and also nonnegative-\nde\ufb01nite. To establish (64), we \ufb01rst use (49) to note that:\nIM \u2212\u00b5k\nN\nX\nl=1\nsl,kHl,k,i\u22121 \u2265\n \n1\u2212\u00b5k\nN\nX\nl=1\nsl,k\u03bbl,max\n!\nIM\n(66)\nIM \u2212\u00b5k\nN\nX\nl=1\nsl,kHl,k,i\u22121 \u2264\n \n1\u2212\u00b5k\nN\nX\nl=1\nsl,k\u03bbl,min\n!\nIM\n(67)\nThe matrix IM \u2212\u00b5k\nPN\nl=1 sl,kHl,k,i\u22121 may not be positive semi-de\ufb01nite because we have not speci\ufb01ed\na range for \u00b5k yet; the expressions on the right-hand side of (66)\u2013(67) may still be negative. However,\ninequalities (66)\u2013(67) imply that the eigenvalues of IM \u2212\u00b5k\nPN\nl=1 sl,kHl,k,i\u22121 are bounded as:\n\u03bb\n \nIM \u2212\u00b5k\nN\nX\nl=1\nsl,kHl,k,i\u22121\n!\n\u22651 \u2212\u00b5k\nN\nX\nl=1\nsl,k\u03bbl,max\n(68)\n\u03bb\n \nIM \u2212\u00b5k\nN\nX\nl=1\nsl,kHl,k,i\u22121\n!\n\u22641 \u2212\u00b5k\nN\nX\nl=1\nsl,k\u03bbl,min\n(69)\nBy de\ufb01nition (63), \u03a3k,i\u22121 is the square of the symmetric matrix IM\u2212\u00b5k\nPN\nl=1 sl,kHl,k,i\u22121, meaning that\n\u03bb (\u03a3k,i\u22121) =\n\"\n\u03bb\n \nIM \u2212\u00b5k\nN\nX\nl=1\nsl,kHl,k,i\u22121\n!#2\n\u22650\n(70)\nSubstituting (68)\u2013(69) into (70) leads to\n\u03bb (\u03a3k,i\u22121)\n\u2264max\n(\f\f\f\f\f1 \u2212\u00b5k\nN\nX\nl=1\nsl,k\u03bbl,max\n\f\f\f\f\f\n2\n,\n\f\f\f\f\f1 \u2212\u00b5k\nN\nX\nl=1\nsl,k\u03bbl,min\n\f\f\f\f\f\n2)\n(71)\nwhich is equivalent to (64).\nLemma 3 (Bound on noise combination). The second term on the right-hand-side of (62) satis\ufb01es:\nE\n\r\r\r\r\r\nN\nX\nl=1\nsl,kvl,i(\u03c6k,i\u22121)\n\r\r\r\r\r\n2\n\u2264\u2225S\u22252\n1 \u00b7\nh\n\u03b1E\u2225\u02dc\u03c6k,i\u22121\u22252+\u03c32\nv\ni\n(72)\nwhere \u2225S\u22251 denotes the 1-norm of the matrix S (i.e., the maximum absolute column sum).\nMay 15, 2012\nDRAFT\n20\nProof: Applying Jensen\u2019s inequality [56, p.77], it holds that\nE\n\r\r\r\r\r\nN\nX\nl=1\nsl,kvl,i(\u03c6k,i\u22121)\n\r\r\r\r\r\n2\n=\n\u0010 N\nX\nl=1\nsl,k\n\u00112\n\u00b7 E\n\r\r\r\r\r\nN\nX\nl=1\nsl,k\nPN\nl=1 sl,k\nvl,i(\u03c6k,i\u22121)\n\r\r\r\r\r\n2\n\u2264\n\u0010 N\nX\nl=1\nsl,k\n\u00112\n\u00b7\nN\nX\nl=1\nsl,k\nPN\nl=1 sl,k\nE\u2225vl,i(\u03c6k,i\u22121)\u22252\n=\n\u0010 N\nX\nl=1\nsl,k\n\u0011\n\u00b7\nN\nX\nl=1\nsl,kE\u2225vl,i(\u03c6k,i\u22121)\u22252\n\u2264\n\u0010 N\nX\nl=1\nsl,k\n\u00112\n\u00b7\nh\n\u03b1E\u2225\u02dc\u03c6k,i\u22121\u22252 + \u03c32\nv\ni\n(73)\n\u2264\u2225S\u22252\n1 \u00b7\nh\n\u03b1E\u2225\u02dc\u03c6k,i\u22121\u22252 + \u03c32\nv\ni\n(74)\nwhere inequality (73) follows by substituting (48), and (74) is obtained using the fact that \u2225S\u22251 is the\nmaximum absolute column sum and that the entries {sl,k} are nonnegative.\nSubstituting (64) and (72) into (62), we obtain:\nE\u2225\u02dc\u03c8k,i\u22252 \u2264(\u03b32\nk+\u00b52\nk\u03b1\u2225S\u22252\n1)\u00b7E\u2225\u02dc\u03c6k,i\u22121\u22252+\u00b52\nk \u2225S\u22252\n1 \u03c32\nv\n(75)\nfor k = 1, . . . , N. Finally, introduce the following network mean-square-error vectors (compare with\n(38)):\nXi =\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0\nE\u2225\u02dc\u03c61,i\u22252\n...\nE\u2225\u02dc\u03c6N,i\u22252\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb\n, Yi =\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0\nE\u2225\u02dc\u03c81,i\u22252\n...\nE\u2225\u02dc\u03c8N,i\u22252\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb\n, Wi =\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0\nE\u2225\u02dcw1,i\u22252\n...\nE\u2225\u02dcwN,i\u22252\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb\nand the matrix\n\u0393 = diag\n\b\n\u03b32\n1 + \u00b52\n1\u03b1\u2225S\u22252\n1, . . . , \u03b32\nN + \u00b52\nN\u03b1\u2225S\u22252\n1\n\t\n(76)\nThen, (60)\u2013(61) and (75) can be written as\n\uf8f1\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f2\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f3\nXi\u22121 \u2aafP T\n1 Wi\u22121\nYi \u2aaf\u0393Xi\u22121 + \u03c32\nv\u2225S\u22252\n1\u212621\nWi \u2aafP T\n2 Yi\n(77)\nMay 15, 2012\nDRAFT\n21\nwhere the notation x \u2aafy denotes that the components of vector x are less than or equal to the\ncorresponding components of vector y. We now recall the following useful fact that for any matrix\nF with nonnegative entries,\nx \u2aafy \u21d2Fx \u2aafFy\n(78)\nThis is because each entry of the vector Fy \u2212Fx = F(y \u2212x) is nonnegative. Then, combining all three\ninequalities in (77) leads to:\nWi \u2aafP T\n2 \u0393P T\n1 Wi\u22121 + \u03c32\nv\u2225S\u22252\n1 \u00b7 P T\n2 \u212621\n(79)\nC. Mean-Square Stability\nBased on (79), we can now prove that, under certain conditions on the step-size parameters {\u00b5k}, the\nmean-square-error vector Wi is bounded as i \u2192\u221e, and we use this result in the next subsection to\nevaluate the steady-state MSE for suf\ufb01ciently small step-sizes.\nTheorem 1 (Mean-Square Stability). If the step-sizes {\u00b5k} satisfy the following condition:\n0 < \u00b5k < min\n(\n2\u03c3k,max\n\u03c32\nk,max+\u03b1\u2225S\u22252\n1\n,\n2\u03c3k,min\n\u03c32\nk,min+\u03b1\u2225S\u22252\n1\n)\n(80)\nfor k = 1, . . . , N, where \u03c3k,max and \u03c3k,min are de\ufb01ned as\n\u03c3k,max \u225c\nN\nX\nl=1\nsl,k\u03bbl,max,\n\u03c3k,min \u225c\nN\nX\nl=1\nsl,k\u03bbl,min\n(81)\nthen, as i \u2192\u221e,\nlim sup\ni\u2192\u221e\n\u2225Wi\u2225\u221e\u2264\n\u0010\nmax\n1\u2264k\u2264N \u00b52\nk\n\u0011\n\u00b7 \u2225S\u22252\n1\u03c32\nv\n1 \u2212max\n1\u2264k\u2264N(\u03b32\nk + \u00b52\nk\u03b1\u2225S\u22252\n1)\n(82)\nwhere \u2225x\u2225\u221edenotes the maximum absolute entry of vector x.\nProof: See Appendix A.\nIf we let \u03b1=0 and \u03c32\nv=0 in Theorem 1, and examine the arguments leading to it, we conclude the\nvalidity of the following result, which establishes the convergence of the diffusion strategies (24)\u2013(25)\nin the absence of gradient noise (i.e., using the true gradient rather than stochastic gradient \u2014 see (18)\nand (19)).\nMay 15, 2012\nDRAFT\n22\nTheorem 2 (Convergence in Noise-free Case). If there is no gradient noise, i.e., \u03b1 = 0 and \u03c32\nv = 0, then\nthe mean-square-error vector becomes the deterministic vector Wi = col{\u2225\u02dcw1,i\u22252, \u00b7 \u00b7 \u00b7 , \u2225\u02dcwN,i\u22252}, and its\nentries converge to zero if the step-sizes {\u00b5k} satisfy the following condition:\n0 < \u00b5k <\n2\n\u03c3k,max\n(83)\nfor k = 1, . . . , N, where \u03c3k,max was de\ufb01ned in (81).\nWe observe that, in the absence of noise, the deterministic error vectors, \u02dcwk,i, will tend to zero as\ni \u2192\u221eeven with constant (i.e., non-vanishing) step-sizes. This result implies the interesting fact that, in\nthe noise-free case, the nodes can reach agreement without the need to impose diminishing step-sizes.\nD. Steady-State Performance\nExpression (80) provides a condition on the step-size parameters {\u00b5k} to ensure the mean-square\nstability of the diffusion strategies (24)\u2013(25). At the same time, expression (82) gives an upper bound on\nhow large Wi can be at steady-state. Since the \u221e-norm of a vector is de\ufb01ned as the largest absolute value\nof its entries, then (82) bounds the MSE of the worst-performing node in the network. We can derive\nclosed-form expressions for MSEs when the step-sizes are assumed to be suf\ufb01ciently small. Indeed, we\n\ufb01rst conclude from (82) that for step-sizes that are suf\ufb01ciently small, each wk,i will get closer to wo at\nsteady-state. To verify this fact, assume the step-sizes are small enough so that the nonnegative factor \u03b3k\nthat was de\ufb01ned earlier in (65) becomes\n\u03b3k = 1 \u2212\u00b5k\nN\nX\nl=1\nsl,k\u03bbl,min = 1 \u2212\u00b5k\u03c3k,min\n(84)\nwhere \u03c3k,min was given by (81). Substituting (84) into (82), we obtain:\nlim sup\ni\u2192\u221e\n\u2225Wi\u2225\u221e\n\u2264\n \nmax\n1\u2264k\u2264N \u00b52\nk\n!\n\u00b7 \u2225S\u22252\n1\u03c32\nv\n1\u2212max\n1\u2264k\u2264N\n(\n(1\u2212\u00b5k\u03c3k,min)2+\u00b52\nk\u03b1\u2225S\u22252\n1\n)\n\u2264\n \nmax\n1\u2264k\u2264N \u00b52\nk\n!\n\u00b7 \u2225S\u22252\n1\u03c32\nv\nmin\n1\u2264k\u2264N\n(\n\u00b5k\n\"\n2\u03c3k,min \u2212\u00b5k(\u03c32\nk,min + \u03b1\u2225S\u22252\n1)\n#)\nMay 15, 2012\nDRAFT\n23\n\u2264\n\u2225S\u22252\n1\u03c32\nv\nmin\n1\u2264k\u2264N\n(\n2\u03c3k,min\u2212\u00b5k(\u03c32\nk,min+\u03b1\u2225S\u22252\n1)\n) \u00b7 \u00b52\nmax\n\u00b5min\n(85)\nwhere\n\u00b5max \u225cmax\n1\u2264k\u2264N \u00b5k,\n\u00b5min \u225cmin\n1\u2264k\u2264N \u00b5k\n(86)\nFor suf\ufb01ciently small step-sizes, the denominator in (85) can be approximated as\n2\u03c3k,min\u2212\u00b5k(\u03c32\nk,min+\u03b1\u2225S\u22252\n1) \u22482\u03c3k,min\n(87)\nSubstituting into (85), we get\nlim sup\ni\u2192\u221e\n\u2225Wi\u2225\u221e\u2264\n\u2225S\u22252\n1\u03c32\nv\n2 min\n1\u2264k\u2264N \u03c3k,min\n\u00b7 \u00b52\nmax\n\u00b5min\n(88)\nTherefore, if the step-sizes are suf\ufb01ciently small, the MSE of each node becomes small as well. This\nresult is clear when all nodes use the same step-sizes such that \u00b5max = \u00b5min = \u00b5. Then, the right-hand\nside of (88) is on the order of O(\u00b5), as indicated. It follows that { \u02dcwk,i} are small in the mean-square-error\nsense at small step-sizes, which also means that the mean-square value of \u02dc\u03c6k,i\u22121 is small because it is\na convex combination of { \u02dcwk,i} (recall (31)). Then, by de\ufb01nition (36), in steady-state (for large enough\ni), the matrix Hl,k,i\u22121 can be approximated by:\nHl,k,i\u22121 \u2248\nZ 1\n0\n\u22072Jl(wo)dt = \u22072Jl(wo)\n(89)\nIn this case, the matrix Hl,k,i\u22121 is not random anymore and is not dependent on the error vector \u02dc\u03c6k,,i\u22121.\nAccordingly, in steady-state, the matrix Di\u22121 that was de\ufb01ned in (42) is not random anymore and it\nbecomes\nDi\u22121 \u2248D\u221e\u225c\nN\nX\nl=1\ndiag\nn\nsl,1\u22072\nwJl(wo), \u00b7 \u00b7 \u00b7 ,sl,N\u22072\nwJl(wo)\no\n(90)\nAs a result, in steady-state, the original error recursion (44) can be approximated by\n\u02dcwi = PT\n2 [IMN \u2212MD\u221e]PT\n1 \u02dcwi\u22121 + PT\n2 Mgi\n(91)\nTaking expectations of both sides of (91), we obtain the following mean-error recursion\nE \u02dcwi = PT\n2 [IMN \u2212MD\u221e]PT\n1 \u00b7 E \u02dcwi\u22121,\ni \u2192\u221e\n(92)\nwhich converges to zero if the matrix\nB \u225cPT\n2 [IMN \u2212MD\u221e]PT\n1\n(93)\nMay 15, 2012\nDRAFT\n24\nis stable. The stability of B can be guaranteed when the step-sizes are suf\ufb01ciently small (or chosen\naccording to (80)) \u2014 see the proof in Appendix C. Therefore, in steady-state, we have\nlim\ni\u2192\u221eE \u02dcwi = 0\n(94)\nNext, we determine an expression (rather than a bound) for the MSE. To do this, we need to evaluate the\ncovariance matrix of the gradient noise vector gi. Recall from (43) that gi depends on {\u03c6k,i\u22121}, which\nis close to wo at steady-state for small step-sizes. Therefore, it is suf\ufb01cient to determine the covariance\nmatrix of gi at wo. We denote this covariance matrix by:\nRv \u225cE{gigT\ni }\n\f\f\f\n\u03c6k,i\u22121=wo\n= E\n\uf8f1\n\uf8f2\n\uf8f3\n\" N\nX\nl=1\ncol\nn\nsl,1vl,i(wo), \u00b7 \u00b7 \u00b7 , sl,Nvl,i(wo)\no#\n\u00d7\n\" N\nX\nl=1\ncol\nn\nsl,1vl,i(wo), \u00b7 \u00b7 \u00b7 , sl,Nvl,i(wo)\no#T\uf8fc\n\uf8fd\n\uf8fe\n(95)\nIn practice, we can evaluate Rv from the expressions of {vl,i(wo)}. For example, for the case of the\nquadratic cost (52), we can substitute (54) into (95) to evaluate Rv.\nReturning to the last term in the \ufb01rst equation of (59), we can evaluate it as follows:\nE\u2225PT\n2 Mgi\u22252\n\u03a3 = EgT\ni MP2\u03a3PT\n2 Mgi\n= Tr\n\u0000\u03a3PT\n2 ME{gigT\ni }MP2\n\u0001\n= Tr\n\u0000\u03a3PT\n2 MRvMP2\n\u0001\n(96)\nUsing (90), the matrix \u03a3\u2032 in (59) becomes a deterministic quantity as well, and is given by:\n\u03a3\u2032 \u2248P1[IMN \u2212MD\u221e]P2\u03a3PT\n2 [IMN \u2212MD\u221e]PT\n1\n(97)\nSubstituting (96) and (97) into (59), an approximate variance relation is obtained for small step-sizes:\nE\u2225\u02dcwi\u22252\n\u03a3 \u2248E\u2225\u02dcwi\u22121\u22252\n\u03a3\u2032 + Tr\n\u0000\u03a3PT\n2 MRvMP2\n\u0001\n(98)\n\u03a3\u2032 \u2248P1[IMN\u2212MD\u221e]P2\u03a3PT\n2 [IMN\u2212MD\u221e]PT\n1\n(99)\nLet \u03c3 = vec(\u03a3) denote the vectorization operation that stacks the columns of a matrix \u03a3 on top of each\nother. We shall use the notation \u2225x\u22252\n\u03c3 and \u2225x\u22252\n\u03a3 interchangeably to denote the weighted squared Euclidean\nnorm of a vector. Using the Kronecker product property [57, p.147]: vec(U\u03a3V ) = (V T \u2297U)vec(\u03a3), we\ncan vectorize \u03a3\u2032 in (99) and \ufb01nd that its vector form is related to \u03a3 via the following linear relation:\nMay 15, 2012\nDRAFT\n25\n\u03c3\u2032 \u225cvec(\u03a3\u2032) \u2248F\u03c3, where, for suf\ufb01ciently small steps-sizes (so that higher powers of the step-sizes can\nbe ignored), the matrix F is given by\nF \u225c\n\u0010\nP1[IMN \u2212MD\u221e]P2\n\u0011\n\u2297\n\u0010\nP1[IMN \u2212MD\u221e]P2\n\u0011\n(100)\nHere, we used the fact that M and D\u221eare block diagonal and symmetric. Furthermore, using the property\nTr(\u03a3X) = vec(XT )T \u03c3, we can rewrite (98) as\nE\u2225\u02dcwi\u22252\n\u03c3 \u2248E\u2225\u02dcwi\u22121\u22252\nF\u03c3 +\n\u0002\nvec\n\u0000PT\n2 MRvMP2\n\u0001\u0003T\u03c3\n(101)\nIt is shown in [46, pp.344\u2013346] that recursion (101) converges to a steady-state value if the matrix F\nis stable. This condition is guaranteed when the step-sizes are suf\ufb01ciently small (or chosen according to\n(80)) \u2014 see Appendix C. Finally, denoting\nE\u2225\u02dcw\u221e\u22252\n\u03c3 \u225clim\ni\u2192\u221eE\u2225\u02dcwi\u22252\n\u03c3\n(102)\nand letting i \u2192\u221e, expression (101) becomes\nE\u2225\u02dcw\u221e\u22252\n\u03c3 \u2248E\u2225\u02dcw\u221e\u22252\nF\u03c3 +\n\u0002\nvec\n\u0000PT\n2 MRvMP2\n\u0001\u0003T \u03c3\nso that\nE\u2225\u02dcw\u221e\u22252\n(I\u2212F)\u03c3 \u2248\n\u0002\nvec\n\u0000PT\n2 MRvMP2\n\u0001\u0003T \u03c3\n(103)\nExpression (103) is a useful result: it allows us to derive several performance metrics through the proper\nselection of the free weighting parameter \u03c3 (or \u03a3). First, to be able to evaluate steady-state performance\nmetrics from (103), we need (I \u2212F) to be invertible, which is guaranteed by the stability of matrix F\n\u2014 see Appendix C. Given that (I \u2212F) is a stable matrix, we can now resort to (103) and use it to\nevaluate various performance metrics by choosing proper weighting matrices \u03a3 (or \u03c3), as it was done in\n[31] for the mean-square-error estimation problem. For example, the MSE of any node k can be obtained\nby computing E\u2225\u02dcw\u221e\u22252\nT with a block weighting matrix T that has an identity matrix at block (k, k) and\nzeros elsewhere:\nE\u2225\u02dcwk,\u221e\u22252 = E\u2225\u02dcw\u221e\u22252\nT\n(104)\nDenote the vectorized version of this matrix by tk, i.e.,\ntk \u225cvec(diag(ek) \u2297IM)\n(105)\nMay 15, 2012\nDRAFT\n26\nwhere ek is a vector whose kth entry is one and zeros elsewhere. Then, if we select \u03c3 in (103) as\n\u03c3 = (I \u2212F)\u22121tk, the term on the left-hand side becomes the desired E\u2225\u02dcwk,\u221e\u22252 and MSE for node k\nis therefore given by:\nMSEk \u2248\n\u0002\nvec\n\u0000PT\n2 MRvMP2\n\u0001\u0003T (I \u2212F)\u22121tk\n(106)\nThis value for MSEk is actually the kth entry of W\u221ede\ufb01ned as\nW\u221e\u225clim\ni\u2192\u221eWi\n(107)\nThen, we arrive at an expression for W\u221e(as opposed to the bound for it in (82), as was explained earlier;\nexpression (108) is derived under the assumption of suf\ufb01ciently small step-sizes):\nW\u221e\u2248\nn\nIN \u2297\n\u0010\u0002\nvec\n\u0000PT\n2 MRvMP2\n\u0001\u0003T (I\u2212F)\u22121\u0011o\nt\n(108)\nwhere t = col{t1, . . . , tN}. If we are interested in the network MSE, then the weighting matrix of\nE\u2225\u02dcw\u221e\u22252\nT should be chosen as T = IMN/N. Let q denote the vectorized version of IMN, i.e.,\nq \u225cvec(IMN)\n(109)\nand select \u03c3 in (103) as \u03c3 = (I\u2212F)\u22121q/N. The network MSE is then given by\nMSE \u225c1\nN\nN\nX\nk=1\nMSEk\n\u22481\nN\n\u0002\nvec\n\u0000PT\n2 MRvMP2\n\u0001\u0003T (I \u2212F)\u22121q\n(110)\nThe approximate expressions (108) and (110) hold when the step-sizes are small enough so that (90)\nholds. In the next section, we will see that they are consistent with the simulation results.\nV. SIMULATION RESULTS\nIn this section we illustrate the performance of the diffusion strategies (24)\u2013(25) by considering two\napplications. We consider a randomly generated connected network topology with a cyclic path. There\nare a total of N = 10 nodes in the network, and nodes are assumed connected when they are close\nenough geographically. In the simulations, we consider two applications: a regularized least-mean-squares\nestimation problem with sparse parameters, and a collaborative localization problem.\nMay 15, 2012\nDRAFT\n27\nA. Distributed Estimation with Sparse Data\nAssume each node k has access to data {Uk,i, dk,i}, generated according to the following model:\ndk,i = Uk,iwo + zk,i\n(111)\nwhere {Uk,i} is a sequence of K \u00d7 M i.i.d. Gaussian random matrices. The entries of each Uk,i have\nzero mean and unit variance, and zk,i \u223cN(0, \u03c32\nzIK) is the measurement noise that is temporally and\nspatially white and is independent of Ul,j for all k, l, i, j. Our objective is to estimate wo from the data\nset {Uk,i, dk,i} in a distributed manner. In many applications, the vector wo is sparse such as\nwo = [1 0 . . . 0 1]T \u2208RM\nOne way to search for sparse solutions is to consider a global cost function of the following form [58]:\nJglob(w) =\nN\nX\nl=1\nE\u2225dl,i \u2212Ul,iw\u22252\n2 + \u03c1R(w)\n(112)\nwhere R(w) and \u03c1 are the regularization function and regularization factor, respectively. A popular\nchoice is R(w) = \u2225w\u22251, which helps enforce sparsity and is convex [58]\u2013[63]. However, this choice\nis non-differentiable, and we would need to apply sub-gradient methods [44, pp.138\u2013144] for a proper\nimplementation. Instead, we use the following twice-differentiable approximation for \u2225w\u22251:\nR(w) =\nM\nX\nm=1\np\n[w]2m + \u03f52\n(113)\nwhere [w]m denotes the m-th entry of w, and \u03f5 is a small number. We see that, as \u03f5 goes to zero,\nR(w) \u2248\u2225w\u22251. Obviously, R(w) is convex, and we can apply the diffusion algorithms to minimize (112)\nin a distributed manner. To do so, we decompose the global cost into a sum of N individual costs:\nJl(w) = E\u2225dl,i \u2212Ul,iw\u22252\n2 + \u03c1\nN R(w)\n(114)\nfor l = 1, . . . , N. Then, using algorithms (18) and (19), each node k would update its estimate of wo by\nusing the gradient vectors of {Jl(w)}l\u2208Nk, which are given by:\n\u2207wJl(w) = 2E\n\u0000U T\nl,iUl,i\n\u0001\nw \u22122E\n\u0000U T\nl,idl,i\n\u0001\n+ \u03c1\nN \u2207wR(w)\n(115)\nHowever, the nodes are assumed to have access to measurements {Ul,i, dl,k} and not to the second-\norder moments E\n\u0010\nU T\nl,iUl,i\n\u0011\nand E\n\u0010\nU T\nl,idl,i\n\u0011\n. In this case, nodes can use the available measurements to\napproximate the gradient vectors in (24) and (25) as:\nb\u2207wJl(w) = 2U T\nl,i [Ul,iw\u2212dl,i]+ \u03c1\nN \u2207wR(w)\n(116)\nMay 15, 2012\nDRAFT\n28\nwhere\n\u2207wR(w) =\n\"\n[w]1\np\n[w]2\n1 + \u03f52\n\u00b7 \u00b7 \u00b7\n[w]M\nq\n[w]2\nM + \u03f52\n#T\n(117)\nIn the simulation, we set M = 50, K = 5, \u03c32\nv = 1, and wo = [1 0 . . . 0 1]T . We apply both diffusion and\nincremental methods to solve the distributed learning problem, where the incremental approach [6]\u2013[9]\nuses the following construction to determine wi:\n\u2022 Start with \u03c80,i = wi\u22121 at the node at the beginning of the incremental cycle.\n\u2022 Cycle through the nodes k = 1, . . . , N:\n\u03c8k,i = \u03c8k\u22121,i \u2212\u00b5b\u2207wJk(\u03c8k\u22121,i)\n(118)\n\u2022 Set wi \u2190\u03c8N,i.\n\u2022 Repeat.\nThe results are averaged over 100 trials. The step-sizes for ATC, CTA and non-cooperative algorithms are\nset to \u00b5 = 10\u22123, and the step-size for the incremental algorithm is set to \u00b5 = 10\u22123/N. This is because the\nincremental algorithm cycles through all N nodes every iteration. We therefore need to ensure the same\nconvergence rate for both algorithms for a fair comparison [35]. For ATC and CTA strategies, we use\nsimple averaging weights for the combination step, and for ATC and CTA with gradient exchange, we use\nMetropolis weights for {cl,k} to combine the gradients (see Table III in [31] for the de\ufb01nitions of averaging\nweights and Metropolis weights). We use expression (110) to evaluate the theoretical performance of the\ndiffusion strategies. As a remark, expression (110) gives the MSE with respect to the minimizer of the\ncost Jglob(w) in (112). In this example, the minimizer of the cost (112), denoted as \u02c6wo, is biased away\nfrom the model parameter wo in (111) when the regularization factor \u03b3 \u0338= 0. To evaluate the theoretical\nMSE with respect to wo, we use\nMSD = lim\ni\u2192\u221e\n1\nN\nN\nX\nk=1\nE\u2225wo \u2212wk,i\u22252\n= E\u2225wo \u2212\u02c6wo\u22252 + lim\ni\u2192\u221e\n1\nN\nN\nX\nk=1\nE\u2225\u02c6wo \u2212wk,i\u22252\n(119)\nwhere the second term in (119) can be evaluated by expression (110) with wo replaced by \u02c6wo. Moreover,\nin the derivation of (119), we used the fact that limi\u2192\u221eE( \u02c6wo\u2212wk,i) = 0 to eliminate the cross term, and\nthis result is due to (94) with wo there replaced by \u02c6wo. Fig. 2(a) shows the learning curves for different\nalgorithms for \u03b3 = 2 and \u03f5 = 10\u22123. We see that the diffusion and incremental schemes have similar\nperformance, and both of them have about 10 dB gain over the non-cooperation case. To examine the\nMay 15, 2012\nDRAFT\n29\n0\n500\n1000\n1500\n2000\n-30\n-25\n-20\n-15\n-10\n-5\n0\n5\nNumber of Iterations\nNetwork MSE (dB)\n \n \nIncremental\nATC (S=I)\nCTA (S=I)\nATC (S=C)\nCTA (S=C)\nNon-cooperative\n(a) Learning curves (\u03b3 = 2 and \u03f5 = 10\u22123).\n0\n2\n4\n6\n8\n10\n12\n14\n16\n18\n20\n\u221225\n\u221220\n\u221215\n\u221210\nRegularization factor \u03b3\nSteady state MSE (dB)\n0\n2\n4\n6\n8\n10\n12\n14\n16\n18\n20\n\u221225\n\u221220\n\u221215\n\u221210\nRegularization factor \u03b3\nSteady state MSE (dB)\n \n \nIncremental\nATC (S=I)\nATC (S=I), theory\nCTA(S=I)\nCTA(S=I), theory\nATC (S=C)\nATC (S=C), theory\nCTA (S=C)\nCTA (S=C), theory\n\u03f5 = 1\n\u03f5 = 10\u22122\n(b) Steady-state MSD (\u00b5 = 10\u22123).\nFig. 2.\nTransient and steady-state performance of distributed estimation with sparse parameters.\nimpact of the parameter \u03f5 and the regularization factor \u03b3, we show the steady-state MSE for different\nvalues of \u03b3 and \u03f5 in Fig. 2(b). When \u03f5 is small (\u03f5 = 10\u22122), adding a reasonable regularization (\u03b3 = 1 \u223c4)\ndecreases the steady-state MSE. However, when \u03f5 is large (\u03f5 = 1), expression (113) is no longer a good\napproximation for \u2225w\u22251, and regularization does not improve the MSE.\nB. Distributed Collaborative Localization\nThe previous example deals with a convex cost (112). Now, we consider a localization problem that\nhas a non-convex cost function and apply the same diffusion strategies to its solution. Assume each node\nis interested in locating a common target located at wo = [0 0]T . Each node k knows its position xk and\nhas a noisy measurement of the squared distance to the target:\ndk(i) = \u2225wo \u2212xk\u22252 + zk(i),\nk = 1, 2, . . . , N\nwhere zk(i) \u223cN(0, \u03c32\nz,k) is the measurement noise of node k at time i. The component cost function\nJk(w) at node k is chosen as\nJk(w) = 1\n4E\n\f\fdk(i) \u2212\u2225w \u2212xk\u22252\f\f2\n(120)\nwhere we multiply by 1/4 here to eliminate a factor of 4 that will otherwise appear in the gradient. If each\nnode k minimizes Jk(w) individually, it is not possible to solve for wo. Therefore, we use information\nfrom other nodes, and instead seek to minimize the following global cost:\nJglob(w) = 1\n4\nN\nX\nk=1\nE\n\f\fdk(i) \u2212\u2225w \u2212xk\u22252\f\f2\n(121)\nMay 15, 2012\nDRAFT\n30\nThis problem arises, for example, in cellular communication systems, where multiple base-stations are\ninterested in locating users using the measured distances between themselves and the user. Diffusion\nalgorithms (18) and (19) can be applied to solve the problem in a distributed manner. Each node k would\nupdate its estimate of wo by using the gradient vectors of {Jl(w)}l\u2208Nk, which are given by:\n\u2207wJl(w) = \u2212Edl(i) (w \u2212xl) + \u2225w \u2212xl\u22252(w \u2212xl)\n(122)\nHowever, the nodes are assumed to have access to measurements {dl(i), xl} and not to Edl(i). In this\ncase, nodes can use the available measurements to approximate the gradient vectors in (24) and (25) as:\nb\u2207wJl(w) = \u2212dl(i)(w \u2212xl) + \u2225w \u2212xl\u22252(w \u2212xl)\n(123)\nIf we do not exchange the local gradients with neighbors, i.e., if we set S = I, then the base-stations\nonly share the local estimates of the target position wo with their neighbors (no exchange of {xl}l\u2208Nk).\nWe \ufb01rst simulate the stationary case, where the target stays at wo. In Fig. 3(a), we show the MSE curves\nfor non-cooperative, ATC, CTA, and incremental algorithms. The noise variance is set to \u03c32\nz,k = 1. We\nset the step-sizes to \u00b5 = 0.0025/N for the incremental algorithm, and \u00b5 = 0.0025 for other algorithms.\nFor ATC and CTA strategies, we use simple averaging for the combination step {al,k}, and for ATC\nand CTA with gradient exchange, we use Metropolis weights for {cl,k} to combine the gradients. The\nperformance of CTA and ATC algorithms are close to each other, and both of them are close to the\nincremental scheme. In Fig. 3(b), we show the steady state MSE with respect to different values of \u00b5.\nWe also use expression (110) to evaluate the theoretical performance of the diffusion strategies. As the\nstep-size becomes small, the performances of diffusion and incremental algorithms are close, and the\nMSE decreases as \u00b5 decreases. Furthermore, we see that exchanging only local estimates (S = I) is\nenough for localization, compared to the case of exchanging both local estimates and gradients (S = C).\nNext, we apply the algorithms to a non-stationary scenario, where the target moves along a trajectory,\nas shown in Fig. 3(c). The step-size is set to \u00b5 = 0.01 for diffusion algorithms, and to \u00b5 = 0.01/N\nfor the incremental approach. To see the advantage of using a constant step-size for continuous tracking,\nwe also simulate the vanishing step-size version of the algorithm from [39], [43] (\u00b5k,i = 0.01/i). The\ndiffusion algorithms track well the target but not the non-cooperative algorithm and the algorithm from\n[39], [43], because a decaying step-size is not helpful for tracking. The tracking performance is shown\nin Fig. 3(d).\nMay 15, 2012\nDRAFT\n31\n0\n500\n1000\n1500\n2000\n-30\n-25\n-20\n-15\n-10\n-5\n0\n5\nNumber of Iterations\nAverage Network MSE (dB)\n \n \nIncremental\nATC (S=I)\nCTA (S=I)\nATC (S=C)\nCTA (S=C)\nNon-cooperation\n0\n500\n1000\n1500\n2000\n-30\n-25\n-20\n-15\n-10\n-5\n0\n5\nNumber of Iterations\nAverage Network MSE (dB)\n \n \nIncremental\nATC (S=I)\nCTA (S=I)\nATC (S=C)\nCTA (S=C)\nNon-cooperation\n0\n500\n1000\n1500\n2000\n-30\n-25\n-20\n-15\n-10\n-5\n0\n5\nNumber of Iterations\nNetwork MSE (dB)\n \n \nIncremental\nATC (S=I)\nCTA (S=I)\nATC (S=C)\nCTA (S=C)\nNon-cooperation\n0\n500\n1000\n1500\n2000\n-30\n-25\n-20\n-15\n-10\n-5\n0\n5\nNumber of Iterations\nNetwork MSE (dB)\n \n \nIncremental\nATC (S=I)\nCTA (S=I)\nATC (S=C)\nCTA (S=C)\nNon-cooperative\n(a) Learning curves for stationary target (\u00b5 = 0.0025).\n10\n\u22123\n10\n\u22122\n\u221240\n\u221235\n\u221230\n\u221225\n\u221220\n\u221215\n\u221210\n\u22125\n0\nStep\u2212size \u00b5\nSteady state Network MSE (dB)\n \n \nIncremental\nATC (S=I)\nCTA (S=I)\nATC (S=C)\nCTA (S=C)\nNon\u2212cooperative\nATC (S=I), theory\nCTA (S=I), theory\nATC (S=C), theory\nCTA (S=C), theory\n(b) Steady-state performance for stationary target.\n\u22126\n\u22124\n\u22122\n0\n2\n4\n6\n\u22126\n\u22124\n\u22122\n0\n2\n4\n6\nx (km)\ny (km)\n \n \nTarget Trajectory\nIncremental\nATC (S=I)\nCTA (S=I)\nATC (S=C)\nCTA (S=C)\nApproach in [39,43]\nNon\u2212cooperative\n(c) Tracking a moving-target by node 1 (\u00b5 = 0.01).\n0\n200\n400\n600\n800\n1000\n-30\n-20\n-10\n0\n10\n20\nNumber of Iterations\nNetwork MSE (dB)\n \n \nIncremental\nATC (S=I)\nCTA (S=I)\nATC (S=C)\nCTA (S=C)\nApproach in [39,43]\nNon-cooperative\n(d) Learning curves for moving target (\u00b5 = 0.01).\nFig. 3. Performance of distributed localization for stationary and moving targets. Diffusion strategies employ constant step-sizes,\nwhich enable continuous adaptation and learning even when the target moves (which corresponds to a changing cost function).\nVI. CONCLUSION\nThis paper proposed diffusion adaptation strategies to optimize global cost functions over a network\nof nodes, where the cost consists of several components. Diffusion adaptation allows the nodes to\nsolve the distributed optimization problem via local interaction and online learning. We used gradient\napproximations and constant step-sizes to endow the networks with continuous learning and tracking\nabilities. We analyzed the mean-square-error performance of the algorithms in some detail, including\ntheir transient and steady-state behavior. Finally, we applied the scheme to two examples: distributed\nsparse parameter estimation and distributed localization. Compared to incremental methods, diffusion\nMay 15, 2012\nDRAFT\n32\nstrategies do not require a cyclic path over the nodes, which makes them more robust to node and link\nfailure.\nAPPENDIX A\nPROOF OF MEAN-SQUARE STABILITY\nTaking the \u221e\u2212norm of both sides of (79), we obtain\n\u2225Wi\u2225\u221e\u2264\u2225P T\n2 \u0393P T\n1 \u2225\u221e\u00b7\u2225Wi\u22121\u2225\u221e+\u03c32\nv\u2225S\u22252\n1\u00b7\u2225P T\n2 \u2225\u221e\u00b7\u2225\u2126\u22252\n\u221e\n\u2264\u2225P T\n2 \u2225\u221e\u00b7 \u2225\u0393\u2225\u221e\u00b7 \u2225P T\n1 \u2225\u221e\u00b7 \u2225Wi\u22121\u2225\u221e\n+ \u03c32\nv\u2225S\u22252\n1 \u00b7 \u2225P T\n2 \u2225\u221e\u00b7 \u2225\u2126\u22252\n\u221e\n= \u2225\u0393\u2225\u221e\u00b7\u2225Wi\u22121\u2225\u221e+\n\u0010\nmax\n1\u2264k\u2264N \u00b52\nk\n\u0011\n\u00b7\u03c32\nv\u2225S\u22252\n1\n(124)\nwhere we used the fact that \u2225P T\n1 \u2225\u221e= \u2225P T\n2 \u2225\u221e= 1 because each row of P T\n1 and P T\n2 sums up to one.\nMoreover, from (76), we have\n\u2225\u0393\u2225\u221e= max\n1\u2264k\u2264N(\u03b32\nk + \u00b52\nk\u03b1\u2225S\u22252\n1)\n(125)\nIterating (124), we obtain\n\u2225Wi\u2225\u221e\u2264\u2225\u0393\u2225i\n\u221e\u00b7 \u2225W0\u2225\u221e\n+\n\u0010\nmax\n1\u2264k\u2264N \u00b52\nk\n\u0011\n\u00b7 \u03c32\nv\u2225S\u22252\n1\ni\u22121\nX\nj=0\n\u2225\u0393\u2225j\n\u221e\n(126)\nWe are going to show further ahead that condition (80) guarantees \u2225\u0393\u2225\u221e< 1. Now, given that \u2225\u0393\u2225\u221e< 1,\nthe \ufb01rst term on the right hand side of (126) converges to zero as i \u2192\u221e, and the second term on the\nright-hand side of (126) converges to:\nlim\ni\u2192\u221e\u03c32\nv\u2225S\u22252\n1\ni\u22121\nX\nj=0\n\u2225\u0393\u2225j\n\u221e=\n\u03c32\nv\u2225S\u22252\n1\n1 \u2212\u2225\u0393\u2225\u221e\n(127)\nTherefore, we establish (82) as follows:\nlim sup\ni\u2192\u221e\n\u2225Wi\u2225\u221e\u2264\n\u0010\nmax\n1\u2264k\u2264N \u00b52\nk\n\u0011\n\u00b7\n\u03c32\nv\u2225S\u22252\n1\n1 \u2212\u2225\u0393\u2225\u221e\n=\n \nmax\n1\u2264k\u2264N \u00b52\nk\n!\n\u00b7 \u2225S\u22252\n1\u03c32\nv\n1 \u2212max\n1\u2264k\u2264N(\u03b32\nk + \u00b52\nk\u03b1\u2225S\u22252\n1)\n(128)\nMay 15, 2012\nDRAFT\n33\nThe only fact that remains to prove is to show that (80) ensures \u2225\u0393\u2225\u221e< 1. From (125), we see that\nthe condition \u2225\u0393\u2225\u221e< 1 is equivalent to requiring:\n\u03b32\nk + \u00b52\nk\u03b1\u2225S\u22252\n1 < 1,\nk = 1, . . . , N.\n(129)\nThen, using (65), this is equivalent to:\n\u0010\n1 \u2212\u00b5k\nN\nX\nl=1\nsl,k\u03bbl,max\n\u00112\n+ \u00b52\nk\u03b1\u2225S\u22252\n1 < 1\n(130)\n\u0010\n1 \u2212\u00b5k\nN\nX\nl=1\nsl,k\u03bbl,min\n\u00112\n+ \u00b52\nk\u03b1\u2225S\u22252\n1 < 1\n(131)\nfor k = 1, . . . , N. Recalling the de\ufb01nitions for \u03c3k,max and \u03c3k,min in (81) and solving these two quadratic\ninequalities with respect to \u00b5k, we arrive at:\n0 < \u00b5k <\n2\u03c3k,max\n\u03c32\nk,max + \u03b1\u2225S\u22252\n1\n,\n0 < \u00b5k <\n2\u03c3k,min\n\u03c32\nk,min + \u03b1\u2225S\u22252\n1\nand we are led to (80).\nAPPENDIX B\nBLOCK MAXIMUM NORM OF A MATRIX\nConsider a block matrix X with blocks of size M \u00d7 M each. Its block maximum norm is de\ufb01ned as\n[35]:\n\u2225X\u2225b,\u221e\u225cmax\nx\u0338=0\n\u2225Xx\u2225b,\u221e\n\u2225x\u2225b,\u221e\n(132)\nwhere the block maximum norm of a vector x \u225ccol{x1, . . . , xN}, formed by stacking N vectors of size\nM each on top of each other, is de\ufb01ned as [35]:\n\u2225x\u2225b,\u221e\u225cmax\n1\u2264k\u2264N \u2225xk\u2225\n(133)\nwhere \u2225\u00b7 \u2225denotes the Euclidean norm of its vector argument.\nLemma 4 (Block maximum norm). If a block diagonal matrix X \u225cdiag{X1, . . . , XN} \u2208RNM\u00d7NM\nconsists of N blocks along the diagonal with dimension M \u00d7 M each, then the block maximum norm of\nX is bounded as\n\u2225X\u2225b,\u221e\u2264max\n1\u2264k\u2264N \u2225Xk\u2225\n(134)\nin terms of the 2-induced norms of {Xk} (largest singular values). Moreover, if X is symmetric, then\nequality holds in (134).\nMay 15, 2012\nDRAFT\n34\nProof: Note that Xx=col{X1x1,. . . ,XNxN}. Evaluating the block maximum norm of vector Xx leads\nto\n\u2225Xx\u2225b,\u221e= max\n1\u2264k\u2264N \u2225Xkxk\u2225\n\u2264max\n1\u2264k\u2264N \u2225Xk\u2225\u00b7 \u2225xk\u2225\n\u2264max\n1\u2264k\u2264N \u2225Xk\u2225\u00b7 max\n1\u2264k\u2264N \u2225xk\u2225\n(135)\nSubstituting (135) and (133) into (132), we establish (134) as\n\u2225X\u2225b,\u221e\u225cmax\nx\u0338=0\n\u2225Xx\u2225b,\u221e\n\u2225x\u2225b,\u221e\n\u2264max\nx\u0338=0\nmax1\u2264k\u2264N \u2225Xk\u2225\u00b7 max1\u2264k\u2264N \u2225xk\u2225\nmax1\u2264k\u2264N \u2225xk\u2225\n= max\n1\u2264k\u2264N \u2225Xk\u2225\n(136)\nNext, we prove that, if all the diagonal blocks of X are symmetric, then equality should hold in (136).\nTo do this, we only need to show that there exists an x0 \u0338= 0, such that\n\u2225Xx0\u2225b,\u221e\n\u2225x0\u2225b,\u221e\n= max\n1\u2264k\u2264N \u2225Xk\u2225\n(137)\nwhich would mean that\n\u2225X\u2225b,\u221e\u225cmax\nx\u0338=0\n\u2225Xx\u2225b,\u221e\n\u2225x\u2225b,\u221e\n\u2265\u2225Xx0\u2225b,\u221e\n\u2225x0\u2225b,\u221e\n= max\n1\u2264k\u2264N \u2225Xk\u2225\n(138)\nThen, combining inequalities (136) and (138), we would obtain desired equality that\n\u2225X\u2225b,\u221e= max\n1\u2264k\u2264N \u2225Xk\u2225\n(139)\nwhen X is block diagonal and symmetric. Thus, without loss of generality, assume the maximum in\n(137) is achieved by X1, i.e.,\nmax\n1\u2264k\u2264N \u2225Xk\u2225= \u2225X1\u2225\nFor a symmetric Xk, its 2-induced norm \u2225Xk\u2225(de\ufb01ned as the largest singular value of Xk) coincides with\nthe spectral radius of Xk. Let \u03bb0 denote the eigenvalue of X1 of largest magnitude, with the corresponding\nright eigenvector given by z0. Then,\nmax\n1\u2264k\u2264N \u2225Xk\u2225= |\u03bb0|,\nX1z0 = \u03bb0z0\nMay 15, 2012\nDRAFT\n35\nWe select x0 = col{z0, 0, . . . , 0}. Then, we establish (137) by:\n\u2225Xx0\u2225b,\u221e\n\u2225x0\u2225b,\u221e\n= \u2225col{X1z0, 0, . . . , 0}\u2225b,\u221e\n\u2225col{z0, 0, . . . , 0}\u2225b,\u221e\n= \u2225X1z0\u2225\n\u2225z0\u2225\n= \u2225\u03bb0z0\u2225\n\u2225z0\u2225\n= |\u03bb0| = max\n1\u2264k\u2264N \u2225Xk\u2225\nAPPENDIX C\nSTABILITY OF B AND F\nRecall the de\ufb01nitions of the matrices B and F from (93) and (100):\nB = PT\n2 [IMN \u2212MD\u221e]PT\n1\n(140)\nF =\n\u0010\nP1[IMN \u2212MD\u221e]P2\n\u0011\n\u2297\n\u0010\nP1[IMN \u2212MD\u221e]P2\n\u0011\n= BT \u2297BT\n(141)\nFrom (140)\u2013(141), we obtain (see Theorem 13.12 from [57, p.141]):\n\u03c1(F) = \u03c1(BT \u2297BT ) = [\u03c1(BT )]2 = [\u03c1(B)]2\n(142)\nwhere \u03c1(\u00b7) denotes the spectral radius of its matrix argument. Therefore, the stability of the matrix F is\nequivalent to the stability of the matrix B, and we only need to examine the stability of B. Now note\nthat the block maximum norm (see the de\ufb01nition in Appendix B) of the matrix B satis\ufb01es\n\u2225B\u2225b,\u221e\u2264\u2225IMN \u2212MD\u221e\u2225b,\u221e\n(143)\nsince the block maximum norms of P1 and P2 are one (see [35, p.4801]):\n\r\rPT\n1\n\r\r\nb,\u221e= 1,\n\r\rPT\n2\n\r\r\nb,\u221e= 1\n(144)\nMoreover, by noting that the spectral radius of a matrix is upper bounded by any matrix norm (Theorem\n5.6.9, [50, p.297]) and that IMN \u2212MD\u221eis symmetric and block diagonal, we have\n\u03c1(B) \u2264\u2225IMN \u2212MD\u221e\u2225b,\u221e= \u03c1(IMN \u2212MD\u221e)\n(145)\nTherefore, the stability of B is guaranteed by the stability of IMN \u2212MD\u221e. Next, we call upon the\nfollowing lemma to bound \u2225IMN \u2212MD\u221e\u2225b,\u221e.\nLemma 5 (Norm of IMN \u2212MD\u221e). It holds that the matrix D\u221ede\ufb01ned in (90) satis\ufb01es\n\u2225IMN \u2212MD\u221e\u2225b,\u221e\u2264max\n1\u2264k\u2264N \u03b3k\n(146)\nMay 15, 2012\nDRAFT\n36\nwhere \u03b3k is de\ufb01ned in (65).\nProof: Since D\u221eis block diagonal and symmetric, IMN \u2212MD\u221eis also block diagonal with blocks\n{IM \u2212\u00b5kDk,\u221e}, where Dk,\u221edenotes the kth diagonal block of D\u221e. Then, from (134) in Lemma 4 in\nAppendix B, it holds that\n\u2225IMN \u2212MD\u221e\u2225b,\u221e= max\n1\u2264k\u2264N \u2225IM \u2212\u00b5kDk,\u221e\u2225\n(147)\nBy the de\ufb01nition of D\u221ein (90), and using condition (45) from Assumption 1, we have\n N\nX\nl=1\nsl,k\u03bbl,min\n!\n\u00b7 IM \u2264Dk,\u221e\u2264\n N\nX\nl=1\nsl,k\u03bbl,max\n!\n\u00b7 IM\nwhich implies that\nIM \u2212\u00b5kDk,\u221e\u2265\n \n1 \u2212\u00b5k\nN\nX\nl=1\nsl,k\u03bbl,max\n!\n\u00b7 IM\n(148)\nIM \u2212\u00b5kDk,\u221e\u2264\n \n1 \u2212\u00b5k\nN\nX\nl=1\nsl,k\u03bbl,min\n!\n\u00b7 IM\n(149)\nThus, \u2225IM \u2212\u00b5kDk,\u221e\u2225\u2264\u03b3k. Substituting into (147), we get (146).\nSubstituting (146) into (145), we get:\n\u03c1(B) \u2264max\n1\u2264k\u2264N \u03b3k\n(150)\nAs long as max\n1\u2264k\u2264N \u03b3k < 1, then all the eigenvalues of B will lie within the unit circle. By the de\ufb01nition\nof \u03b3k in (65), this is equivalent to requiring\n|1 \u2212\u00b5k\u03c3k,max| < 1,\n|1 \u2212\u00b5k\u03c3k,min| < 1\nfor k = 1, . . . , N, where \u03c3k,max and \u03c3k,min are de\ufb01ned in (81). These conditions are satis\ufb01ed if we\nchoose \u00b5k such that\n0 < \u00b5k < 2/\u03c3k,max,\nk = 1, . . . , N\n(151)\nwhich is obviously guaranteed for suf\ufb01ciently small step-sizes (and also by condition (80)).\nREFERENCES\n[1] J. Chen, S.-Y. Tu, and A. H. Sayed, \u201cDistributed optimization via diffusion adaptation,\u201d in Proc. IEEE International\nWorkshop on Comput. Advances Multi-Sensor Adaptive Process. (CAMSAP), Puerto Rico, Dec. 2011, pp. 281\u2013284.\n[2] J. Chen and A. H. Sayed, \u201cPerformance of diffusion adaptation for collaborative optimization,\u201d in Proc. IEEE International\nConf. Acoustics, Speech and Signal Process. (ICASSP), Kyoto, Japan, March 2012, pp. 1\u20134.\nMay 15, 2012\nDRAFT\n37\n[3] S.-Y. Tu and A. H. Sayed, \u201cMobile adaptive networks,\u201d IEEE J. Sel. Topics. Signal Process., vol. 5, no. 4, pp. 649\u2013664,\nAug. 2011.\n[4] O. Dekel, R. Gilad-Bachrach, O. Shamir, and L. Xiao, \u201cOptimal distributed online prediction,\u201d in Proc. International Conf.\nMachin. Learning (ICML), Bellevue, USA, June 2011, pp. 713\u2013720.\n[5] Z. J. Tow\ufb01c, J. Chen, and A. H. Sayed, \u201cCollaborative learning of mixture models using diffusion adaptation,\u201d in Proc.\nIEEE Workshop on Mach. Learning Signal Process. (MLSP), Beijing, China, Sep. 2011, pp. 1\u20136.\n[6] D. P. Bertsekas, \u201cA new class of incremental gradient methods for least squares problems,\u201d SIAM J. Optim., vol. 7, no. 4,\npp. 913\u2013926, 1997.\n[7] A. Nedic and D. P. Bertsekas, \u201cIncremental subgradient methods for nondifferentiable optimization,\u201d SIAM J. Optim.,\nvol. 12, no. 1, pp. 109\u2013138, 2001.\n[8] M. G. Rabbat and R. D. Nowak, \u201cQuantized incremental algorithms for distributed optimization,\u201d IEEE J. Sel. Areas\nCommun., vol. 23, no. 4, pp. 798\u2013808, Apr. 2005.\n[9] C. G. Lopes and A. H. Sayed, \u201cIncremental adaptive strategies over distributed networks,\u201d IEEE Trans. Signal Process.,\nvol. 55, no. 8, pp. 4064\u20134077, Aug. 2007.\n[10] D. P. Bertsekas and J. N. Tsitsiklis, Parallel and Distributed Computation: Numerical Methods, 1st edition.\nAthena\nScienti\ufb01c, Singapore, 1997.\n[11] J. N. Tsitsiklis and M. Athans, \u201cConvergence and asymptotic agreement in distributed decision problems,\u201d IEEE Trans.\nAutom. Control, vol. 29, no. 1, pp. 42\u201350, Jan. 1984.\n[12] J. N. Tsitsiklis, D. P. Bertsekas, and M. Athans, \u201cDistributed asynchronous deterministic and stochastic gradient optimization\nalgorithms,\u201d IEEE Trans. Autom. Control, vol. 31, no. 9, pp. 803\u2013812, Sep. 1986.\n[13] S. Barbarossa and G. Scutari, \u201cBio-inspired sensor network design,\u201d IEEE Signal Process. Mag., vol. 24, no. 3, pp. 26\u201335,\nMay 2007.\n[14] A. Nedic and A. Ozdaglar, \u201cCooperative distributed multi-agent optimization,\u201d in Convex Optimization in Signal Processing\nand Communications, Y. Eldar and D. Palomar, Eds., pp. 340\u2013386, 2009.\n[15] \u2014\u2014, \u201cDistributed subgradient methods for multi-agent optimization,\u201d IEEE Trans. Autom. Control, vol. 54, no. 1, pp.\n48\u201361, Jan. 2009.\n[16] I. D. Schizas, A. Ribeiro, and G. B. Giannakis, \u201cConsensus in ad hoc WSNs with noisy links\u2014Part I: Distributed estimation\nof deterministic signals,\u201d IEEE Trans. Signal Process., vol. 56, no. 1, pp. 350\u2013364, 2008.\n[17] S. Kar and J. M. F. Moura, \u201cSensor networks with random links: Topology design for distributed consensus,\u201d IEEE Trans.\nSignal Process., vol. 56, no. 7, pp. 3315\u20133326, July 2008.\n[18] \u2014\u2014, \u201cConvergence rate analysis of distributed gossip (linear parameter) estimation: Fundamental limits and tradeoffs,\u201d\nIEEE J. Sel. Topics. Signal Process., vol. 5, no. 4, pp. 674\u2013690, Aug. 2011.\n[19] A. G. Dimakis, S. Kar, J. M. F. Moura, M. G. Rabbat, and A. Scaglione, \u201cGossip algorithms for distributed signal\nprocessing,\u201d Proc. of the IEEE, vol. 98, no. 11, pp. 1847\u20131864, 2010.\n[20] R. Olfati-Saber and R. M. Murray, \u201cConsensus problems in networks of agents with switching topology and time-delays,\u201d\nIEEE Trans. Autom. Control, vol. 49, no. 9, pp. 1520\u20131533, Sep. 2004.\n[21] T. C. Aysal, M. E. Yildiz, A. D. Sarwate, and A. Scaglione, \u201cBroadcast gossip algorithms for consensus,\u201d IEEE Trans.\nSignal Process., vol. 57, no. 7, pp. 2748\u20132761, 2009.\n[22] S. Sardellitti, M. Giona, and S. Barbarossa, \u201cFast distributed average consensus algorithms based on advection-diffusion\nprocesses,\u201d IEEE Trans. Signal Process., vol. 58, no. 2, pp. 826\u2013842, Feb. 2010.\nMay 15, 2012\nDRAFT\n38\n[23] L. Xiao, S. Boyd, and S. Lall, \u201cA scheme for robust distributed sensor fusion based on average consensus,\u201d in Proc. Int.\nSymp. Information Processing Sensor Networks (IPSN), Los Angeles, CA, Apr. 2005, pp. 63\u201370.\n[24] C. Eksin and A. Ribeiro, \u201cNetwork optimization with heuristic rational agents,\u201d in Proc. Asilomar Conf. on Signals Systems\nComputers, Paci\ufb01c Grove, CA, Nov. 2011, pp. 1\u20135.\n[25] R. M. Karp, \u201cReducibility among combinational problems,\u201d Complexity of Computer Computations (R. E. Miller and J.\nW. Thatcher, Eds.), pp. 85\u2013104, 1972.\n[26] C. G. Lopes and A. H. Sayed, \u201cDistributed processing over adaptive networks,\u201d in Proc. Adaptive Sensor Array Processing\nWorkshop, MIT Lincoln Laboratory, MA, June 2006, pp. 1\u20135.\n[27] C. Lopes and A. Sayed, \u201cDiffusion least-mean squares over adaptive networks,\u201d in IEEE ICASSP, vol. 3, Honolulu, HI,\nApr. 2007, pp. 917\u2013920.\n[28] A. H. Sayed and C. G. Lopes, \u201cAdaptive processing over distributed networks,\u201d IEICE Trans. Fund. Electron., Commun.\nComput. Sci., vol. E90-A, no. 8, pp. 1504\u20131510, Aug. 2007.\n[29] C. G. Lopes and A. H. Sayed, \u201cDiffusion least-mean squares over adaptive networks: Formulation and performance\nanalysis,\u201d IEEE Trans. Signal Process., vol. 56, no. 7, pp. 3122\u20133136, July 2008.\n[30] F. S. Cattivelli and A. H. Sayed, \u201cDiffusion LMS algorithms with information exchange,\u201d in Proc. Asilomar Conf. Signals,\nSyst. Comput., Paci\ufb01c Grove, CA, Nov. 2008, pp. 251\u2013255.\n[31] \u2014\u2014, \u201cDiffusion LMS strategies for distributed estimation,\u201d IEEE Trans. Signal Process., vol. 58, no. 3, pp. 1035\u20131048,\nMarch 2010.\n[32] F. S. Cattivelli, C. G. Lopes, and A. H. Sayed, \u201cA diffusion RLS scheme for distributed estimation over adaptive networks,\u201d\nin Proc. IEEE Workshop on Signal Process. Advances Wireless Comm. (SPAWC), Helsinki, Finland, June 2007, pp. 1\u20135.\n[33] \u2014\u2014, \u201cDiffusion recursive least-squares for distributed estimation over adaptive networks,\u201d IEEE Trans. Signal Process.,\nvol. 56, no. 5, pp. 1865\u20131877, May 2008.\n[34] F. S. Cattivelli and A. H. Sayed, \u201cDiffusion strategies for distributed Kalman \ufb01ltering and smoothing,\u201d IEEE Trans. Autom.\nControl, vol. 55, no. 9, pp. 2069\u20132084, Sep. 2010.\n[35] N. Takahashi, I. Yamada, and A. H. Sayed, \u201cDiffusion least-mean squares with adaptive combiners: Formulation and\nperformance analysis,\u201d IEEE Trans. Signal Process., vol. 58, no. 9, pp. 4795\u20134810, Sep. 2010.\n[36] F. S. Cattivelli and A. H. Sayed, \u201cModeling bird \ufb02ight formations using diffusion adaptation,\u201d IEEE Trans. Signal Process.,\nvol. 59, no. 5, pp. 2038\u20132051, May 2011.\n[37] P. Di Lorenzo, S. Barbarossa, and A. H. Sayed, \u201cBio-inspired swarming for dynamic radio access based on diffusion\nadaptation,\u201d in Proc. European Signal Process. Conf. (EUSIPCO), Aug. 2011, pp. 1\u20136.\n[38] S. Chouvardas, K. Slavakis, and S. Theodoridis, \u201cAdaptive robust distributed learning in diffusion sensor networks,\u201d IEEE\nTrans. Signal Process., vol. 59, no. 10, pp. 4692\u20134707, 2011.\n[39] S. S. Ram, A. Nedic, and V. V. Veeravalli, \u201cDistributed stochastic subgradient projection algorithms for convex\noptimization,\u201d J. Optim. Theory Appl., vol. 147, no. 3, pp. 516\u2013545, 2010.\n[40] P. Bianchi, G. Fort, W. Hachem, and J. Jakubowicz, \u201cConvergence of a distributed parameter estimator for sensor networks\nwith local averaging of the estimates,\u201d in Proc. IEEE ICASSP, Prague, Czech, May 2011, pp. 3764\u20133767.\n[41] D. P. Bertsekas, \u201cIncremental gradient, subgradient, and proximal methods for convex optimization: A survey,\u201d LIDS\nTechnical Report, MIT, no. 2848, 2010.\n[42] V. S. Borkar and S. P. Meyn, \u201cThe ODE method for convergence of stochastic approximation and reinforcement learning,\u201d\nSIAM J. Control Optim., vol. 38, no. 2, pp. 447\u2013469, 2000.\nMay 15, 2012\nDRAFT\n39\n[43] K. Srivastava and A. Nedic, \u201cDistributed asynchronous constrained stochastic optimization,\u201d IEEE J. Sel. Topics. Signal\nProcess., vol. 5, no. 4, pp. 772\u2013790, Aug. 2011.\n[44] B. Polyak, Introduction to Optimization.\nOptimization Software, NY, 1987.\n[45] J. Chen and A. H. Sayed, \u201cDistributed Pareto-optimal solutions via diffusion adaptation,\u201d in Proc. IEEE Statistical Signal\nProcess. Workshop (SSP), Ann Arbor, MI, Aug. 2012.\n[46] A. H. Sayed, Adaptive Filters.\nWiley, NJ, 2008.\n[47] G. H. Golub and C. F. Van Loan, Matrix Computations (3rd Edition).\nJohns Hopkins University Press, 1996.\n[48] S. S. Stankovic, M. S. Stankovic, and D. M. Stipanovic, \u201cDecentralized parameter estimation by consensus based stochastic\napproximation,\u201d IEEE Trans. Autom. Control, vol. 56, no. 3, pp. 531\u2013543, Mar. 2011.\n[49] S.-Y. Tu and A. H. Sayed, \u201cDiffusion networks outperform consensus networks,\u201d in Proc. IEEE Statistical Signal Processing\nWorkshop (SSP), Ann Arbor, MI, Aug. 2012.\n[50] R. Horn and C. Johnson, Matrix Analysis.\nCambridge University Press, 1990.\n[51] D. P. Bertsekas and J. N. Tsitsiklis, \u201cGradient convergence in gradient methods with errors,\u201d SIAM J. Optim., vol. 10,\nno. 3, pp. 627\u2013642, 2000.\n[52] S. Haykin, Adaptive Filter Theory, 2nd Edition.\nPrentice Hall, 2002.\n[53] J. Arenas-Garcia, M. Martinez-Ramon, A. Navia-Vazquez, and A. R. Figueiras-Vidal, \u201cPlant identi\ufb01cation via adaptive\ncombination of transversal \ufb01lters,\u201d Signal Processing, vol. 86, no. 9, pp. 2430\u20132438, 2006.\n[54] M. Silva and V. Nascimento, \u201cImproving the tracking capability of adaptive \ufb01lters via convex combination,\u201d IEEE Trans.\nSignal Process., vol. 56, no. 7, pp. 3137\u20133149, 2008.\n[55] S. Theodoridis, K. Slavakis, and I. Yamada, \u201cAdaptive learning in a world of projections,\u201d IEEE Signal Process. Mag.,\nvol. 28, no. 1, pp. 97\u2013123, Jan. 2011.\n[56] S. Boyd and L. Vandenberghe, Convex Optimization.\nCambridge University Press, 2004.\n[57] A. J. Laub, Matrix Analysis for Scientists and Engineers.\nSociety for Industrial and Applied Mathematics (SIAM), PA,\n2005.\n[58] P. Di Lorenzo, S. Barbarossa, and A. H. Sayed, \u201cSparse diffusion LMS for distributed adaptive estimation,\u201d in Proc. IEEE\nICASSP, Kyoto, Japan, March 2012, pp. 1\u20134.\n[59] R. Tibshirani, \u201cRegression shrinkage and selection via the lasso,\u201d J. Royal Statist. Soc. B, pp. 267\u2013288, 1996.\n[60] R. G. Baraniuk, \u201cCompressive sensing,\u201d IEEE Signal Process. Mag., vol. 24, no. 4, pp. 118\u2013121, Mar. 2007.\n[61] E. Candes, M. Wakin, and S. Boyd, \u201cEnhancing sparsity by reweighted \u21131 minimization,\u201d J. Fourier Anal. Appl., vol. 14,\nno. 5, pp. 877\u2013905, 2008.\n[62] G. Mateos, J. A. Bazerque, and G. B. Giannakis, \u201cDistributed sparse linear regression,\u201d IEEE Trans. Signal Process.,\nvol. 58, no. 10, pp. 5262\u20135276, 2010.\n[63] Y. Kopsinis, K. Slavakis, and S. Theodoridis, \u201cOnline sparse system identi\ufb01cation and signal reconstruction using projections\nonto weighted \u21131 balls,\u201d IEEE Trans. Signal Process., vol. 59, no. 3, pp. 936\u2013952, Mar. 2011.\nMay 15, 2012\nDRAFT\n",
        "sentence": " We choose the adapt-then-combine (ATC) diffusion variant for distributed optimization over networks [17], [18], [45].",
        "context": "1\nDiffusion Adaptation Strategies for Distributed\nOptimization and Learning over Networks\nJianshu Chen, Student Member, IEEE, and Ali H. Sayed, Fellow, IEEE\nAbstract\nSec. III, we show that optimizing the localized alternative cost at each node k leads naturally to diffusion\nadaptation strategies. In Sec. IV, we analyze the mean-square performance of the diffusion algorithms\nchallenging to study. Nevertheless, it is shown in [45] that the same diffusion strategies (18)\u2013(19) of this\npaper are still applicable and nodes would converge instead to a Pareto-optimal solution."
    },
    {
        "title": "Distributed Pareto optimization via diffusion strategies",
        "author": [
            "\u2014\u2014"
        ],
        "venue": "IEEE Journal of Selected Topics in Signal Processing, vol. 7, no. 2, pp. 205\u2013220, Apr. 2013.",
        "citeRegEx": "18",
        "shortCiteRegEx": null,
        "year": 2013,
        "abstract": "We consider solving multi-objective optimization problems in a distributed\nmanner by a network of cooperating and learning agents. The problem is\nequivalent to optimizing a global cost that is the sum of individual\ncomponents. The optimizers of the individual components do not necessarily\ncoincide and the network therefore needs to seek Pareto optimal solutions. We\ndevelop a distributed solution that relies on a general class of adaptive\ndiffusion strategies. We show how the diffusion process can be represented as\nthe cascade composition of three operators: two combination operators and a\ngradient descent operator. Using the Banach fixed-point theorem, we establish\nthe existence of a unique fixed point for the composite cascade. We then study\nhow close each agent converges towards this fixed point, and also examine how\nclose the Pareto solution is to the fixed point. We perform a detailed\nmean-square error analysis and establish that all agents are able to converge\nto the same Pareto optimal solution within a sufficiently small\nmean-square-error (MSE) bound even for constant step-sizes. We illustrate one\napplication of the theory to collaborative decision making in finance by a\nnetwork of agents.",
        "full_text": "1\nDistributed Pareto Optimization via Diffusion\nStrategies\nJianshu Chen, Student Member, IEEE, and Ali H. Sayed, Fellow, IEEE\nAbstract\nWe consider solving multi-objective optimization problems in a distributed manner by a network\nof cooperating and learning agents. The problem is equivalent to optimizing a global cost that is the\nsum of individual components. The optimizers of the individual components do not necessarily coincide\nand the network therefore needs to seek Pareto optimal solutions. We develop a distributed solution that\nrelies on a general class of adaptive diffusion strategies. We show how the diffusion process can be\nrepresented as the cascade composition of three operators: two combination operators and a gradient\ndescent operator. Using the Banach \ufb01xed-point theorem, we establish the existence of a unique \ufb01xed\npoint for the composite cascade. We then study how close each agent converges towards this \ufb01xed point,\nand also examine how close the Pareto solution is to the \ufb01xed point. We perform a detailed mean-square\nerror analysis and establish that all agents are able to converge to the same Pareto optimal solution\nwithin a suf\ufb01ciently small mean-square-error (MSE) bound even for constant step-sizes. We illustrate\none application of the theory to collaborative decision making in \ufb01nance by a network of agents.\nIndex Terms\nDistributed optimization, network optimization, diffusion adaptation, Pareto optimality, mean-square\nperformance, convergence, stability, \ufb01xed point, collaborative decision making.\nI. INTRODUCTION\nWe consider solving a multi-objective optimization problem in a distributed manner over a network of\nN cooperative learners (see Fig. 1). Each agent k is associated with an individual cost function Jo\nk(w);\nThe authors are with Department of Electrical Engineering, University of California, Los Angeles, CA 90095. Email: {jshchen,\nsayed}@ee.ucla.edu.\nThis work was supported in part by NSF grants CCF-1011918 and CCF-0942936. A preliminary short version of this work\nis reported in the conference publication [1].\nOctober 31, 2018\nDRAFT\narXiv:1208.2503v1  [cs.MA]  13 Aug 2012\n2\nand each of these costs may not be minimized at the same vector wo. As such, we need to seek a solution\nthat is \u201coptimal\u201d in some sense for the entire network. In these cases, a general concept of optimality\nknown as Pareto optimality is useful to characterize how good a solution is. A solution wo is said to be\nPareto optimal if there does not exist another vector w that is able to improve (i.e., reduce) any particular\ncost, say, Jo\nk(w), without degrading (increasing) some of the other costs {Jo\nl (w)}l\u0338=k. To illustrate the\nidea of Pareto optimality, let\nO \u225c{(Jo\n1(w), . . . , Jo\nN(w)) : w \u2208W} \u2286RN\n(1)\ndenote the set of achievable cost values, where W denotes the feasible set. Each point P \u2208O represents\nattained values for the cost functions {Jo\nl (w)} at a certain w \u2208W. Let us consider the two-node case\n(N = 2) shown in Fig. 2, where the shaded areas represent the set O for two situations of interest. In Fig.\n1\n2\n4\n8\n7\nk\n5\n9\n3\n6\nNk\nNk\nak;1\nck;1\nJo\nk(w)\nJ o\n1 (w)\nJ o\n2 (w)\nJ o\n3 (w)\nJ o\n5 (w)\nJ o\n6 (w)\nJ o\n4 (w)\nJ o\n7 (w)\nJ o\n9 (w)\nJ o\n8 (w)\na1;k\nc1;k\nFig. 1.\nA network of N cooperating agents; a cost function Jo\nk(w) is associated with each node k. The set of neighbors of\nnode k is denoted by Nk (including node k itself); this set consists of all nodes with which node k can share information.\n2(a), both Jo\n1(w) and Jo\n2(w) achieve their minima at the same point P = (Jo\n1(wo), Jo\n2(wo)), where wo is\nthe common minimizer. In comparison, in Fig. 2(b), Jo\n1(w) attains its minimum at point P1, while Jo\n2(w)\nattains its minimum at point P2, so that they do not have a common minimizer. Instead, all the points\non the heavy red curve between points P1 and P2 are Pareto optimal solutions. For example, starting at\npoint A on the curve, if we want to reduce the value of Jo\n1(w) without increasing the value of Jo\n2(w),\nthen we will need to move out of the achievable set O. The alternative choice that would keep us on the\ncurve is to move to another Pareto optimal point B, which would however increase the value of Jo\n2(w).\nIn other words, we need to trade the value of Jo\n2(w) for Jo\n1(w). For this reason, the curve from P1 to\nP2 is called the optimal tradeoff curve (or optimal tradeoff surface if N > 2) [2, p.183].\nOctober 31, 2018\nDRAFT\n3\nJo\n2(w)\nJo\n2(w)\nJo\n1(w)\nJo\n1(w)\nO\nP\n(a)\nJo\n2(w)\nJo\n2(w)\nJo\n1(w)\nJo\n1(w)\nO\nP1\nP1\nP2\nP2\nA\nB\nC\n(b)\nFig. 2.\nOptimal and Pareto optimal points for the case N = 2: (Left) P denotes the optimal point where both cost functions\nare minimized simultaneously and (Right) Pareto optimal points lie on the red boundary curve.\nTo solve for Pareto optimal solutions, a useful scalarization technique is often used to form an aggregate\ncost function that is the weighted sum of the component costs as follows:\nJglob(w) =\nN\nX\nl=1\n\u03c0lJo\nl (w)\n(2)\nwhere \u03c0l is a positive weight attached with the lth cost. It was shown in [2, pp.178\u2013180] that the minimizer\nof (2) is Pareto optimal for the multi-objective optimization problem. Moreover, by varying the values\nof {\u03c0l}, we are able to get different Pareto optimal points on the tradeoff curve. Observing that we can\nalways de\ufb01ne a new cost Jl(w) by incorporating the weighting scalar \u03c0l,\nJl(w) \u225c\u03c0lJo\nl (w)\n(3)\nit is suf\ufb01cient for our future discussions to focus on aggregate costs of the following form:\nJglob(w) =\nN\nX\nl=1\nJl(w)\n(4)\nIf desired, we can also add constraints to problem (4). For example, suppose there is additionally some\nconstraint of the form pT\nk w < bk at node k, where pk is M \u00d7 1 and bk is a scalar. Then, we can consider\nusing barrier functions to convert the constrained optimization problem to an unconstrained problem [2],\n[3]. For example, we can rede\ufb01ne each cost Jk(w) to be Jk(w) \u2190Jk(w) + \u03c6(pT\nk w \u2212bk), where \u03c6(x)\nis a barrier function that penalizes values of w that violate the constraint. Therefore, without loss of\ngenerality, we shall assume W = RM and only consider unconstrained optimization problems. Moreover,\nwe shall assume the {Jo\nl (w)} are differentiable and, for each given set of positive weights {\u03c0l}, the cost\nOctober 31, 2018\nDRAFT\n4\nJglob(w) in (2) or (4) is strongly convex so that the minimizer wo is unique [4]. Note that the new cost\nJl(w) in (3) depends on \u03c0l so that the wo that minimizes Jglob(w) in (4) also depends on {\u03c0l}.\nOne of the most studied approaches to the distributed solution of such optimization problems is the\nincremental approach \u2014 see, e.g., [5]\u2013[12]. In this approach, a cyclic path is de\ufb01ned over the nodes\nand data are processed in a cyclic manner through the network until optimization is achieved. However,\ndetermining a cyclic path that covers all nodes is generally an NP-hard problem [13] and, in addition,\ncyclic trajectories are vulnerable to link and node failures. Another useful distributed optimization\napproach relies on the use of consensus strategies [5], [14]\u2013[20]. In this approach, vanishing step-size\nsequences are used to ensure that agents reach consensus and agree about the optimizer in steady-state.\nHowever, in time-varying environments, diminishing step-sizes prevent the network from continuous\nlearning; when the step-sizes die out, the network stops learning.\nIn [21], we generalized our earlier work on adaptation and learning over networks [22], [23] and\ndeveloped diffusion strategies that enable the decentralized optimization of global cost functions of the\nform (4). In the diffusion approach, information is processed locally at the nodes and then diffused\nthrough a real-time sharing mechanism. In this manner, the approach is scalable, robust to node and\nlink failures, and avoids the need for cyclic trajectories. In addition, compared to the aforementioned\nconsensus solutions (such as those in [16], [19], [24]), the diffusion strategies we consider here employ\nconstant (rather than vanishing) step-sizes in order to endow the resulting networks with continuous\nlearning and tracking abilities. By keeping the step-sizes constant, the agents are able to track drifts in\nthe underlying costs and in the location of the Pareto optimal solutions. One of the main challenges in\nthe ensuing analysis becomes that of showing that the agents are still able to approach the Pareto optimal\nsolution even with constant step-sizes; in this way, the resulting diffusion strategies are able to combine\nthe two useful properties of optimality and adaptation.\nIn [21], we focused on the important case where all costs {Jl(w)} share the same optimal solution\nwo (as was the case with Fig. 2(a)); this situation arises when the agents in the network have a common\nobjective and they cooperate to solve the problem of mutual interest in a distributed manner. Examples\nabound in biological networks where agents work together, for example, to locate food sources or\nevade predators [25], and in collaborative spectrum sensing [26], system identi\ufb01cation [27], and learning\napplications [28]. In this paper, we develop the necessary theory to show that the same diffusion approach\n(described by (9)\u2013(10) below) can be used to solve the more challenging multi-objective optimization\nproblem, where the agents need to converge instead to a Pareto optimal solution. Such situations are\ncommon in the context of multi-agent decision making (see, e.g., [3] and also Sec. IV where we discuss\nOctober 31, 2018\nDRAFT\n5\none application in the context of collaborative decision in \ufb01nance). To study this more demanding scenario,\nwe \ufb01rst show that the proposed diffusion process can be represented as the cascade composition of three\noperators: two combination (aggregation) operators and one gradient-descent operator. Using the Banach\n\ufb01xed-point theorem [29, pp.299\u2013303], we establish the existence of a unique \ufb01xed point for the composite\ncascade. We then study how close each agent in the network converges towards this \ufb01xed point, and also\nexamine how close the Pareto solution is to the \ufb01xed point. We perform a detailed mean-square error\nanalysis and establish that all agents are able to converge to the same Pareto optimal solution within a\nsuf\ufb01ciently small mean-square-error (MSE) bound. We illustrate the results by considering an example\ninvolving collaborative decision in \ufb01nancial applications.\nNotation. Throughout the paper, all vectors are column vectors. We use boldface letters to denote random\nquantities (such as uk,i) and regular font to denote their realizations or deterministic variables (such as\nuk,i). We use diag{x1, . . . , xN} to denote a (block) diagonal matrix consisting of diagonal entries (blocks)\nx1, . . . , xN, and use col{x1, . . . , xN} to denote a column vector formed by stacking x1, . . . , xN on top of\neach other. The notation x \u2aafy means each entry of the vector x is less than or equal to the corresponding\nentry of the vector y.\nII. DIFFUSION ADAPTATION STRATEGIES\nIn [21], we motivated and derived diffusion strategies for distributed optimization, which are captured\nby the following general description:\n\u03c6k,i\u22121 =\nN\nX\nl=1\na1,lkwl,i\u22121\n(5)\n\u03c8k,i = \u03c6k,i\u22121 \u2212\u00b5k\nN\nX\nl=1\nclk\u2207wJl(\u03c6k,i\u22121)\n(6)\nwk,i =\nN\nX\nl=1\na2,lk\u03c8l,i\n(7)\nwhere wk,i is the local estimate for wo at node k and time i, \u00b5k is the step-size parameter used by node k,\nand {\u03c6k,i\u22121, \u03c8k,i} are intermediate estimates for wo. Moreover, \u2207wJl(\u00b7) is the (column) gradient vector\nof Jl(\u00b7) relative to w. The non-negative coef\ufb01cients {a1,lk}, {clk}, and {a2,lk} are the (l, k)-th entries of\nmatrices A1, C, and A2, respectively, and they are required to satisfy:\n\uf8f1\n\uf8f4\n\uf8f2\n\uf8f4\n\uf8f3\nAT\n1 1 = 1, AT\n2 1 = 1, C1 = 1\na1,lk = 0, a2,lk = 0, clk = 0 if l /\u2208Nk\n(8)\nOctober 31, 2018\nDRAFT\n6\nwhere 1 denotes a vector with all entries equal to one. Note from (8) that the combination coef\ufb01cients\n{a1,lk, a2,lk, clk} are nonzero only for those l \u2208Nk. Therefore, the sums in (5)\u2013(7) are con\ufb01ned within the\nneighborhood of node k. Condition (8) requires the combination matrices {A1, A2} to be left-stochastic,\nwhile C is right-stochastic. We therefore note that each node k \ufb01rst aggregates the existing estimates\nfrom its neighbors through (5) and generates the intermediate estimate \u03c6k,i\u22121. Then, node k aggregates\ngradient information from its neighborhood and updates \u03c6k,i\u22121 to \u03c6k,i through (6). All other nodes in the\nnetwork are performing these same steps simultaneously. Finally, node k aggregates the estimates {\u03c6l,i}\nthrough step (7) to update its weight estimate to wk,i.\nAlgorithm (5)\u2013(7) can be simpli\ufb01ed to several special cases for different choices of the matrices\n{A1, A2, C}. For example, the choice A1 = I, A2 = A and C = I reduces to the adapt-then-combine\n(ATC) strategy that has no exchange of gradient information [21]\u2013[23], [30]:\n\u03c8k,i = wk,i\u22121 \u2212\u00b5k\u2207wJk(wk,i\u22121)\nwk,i =\nX\nl\u2208Nk\nalk\u03c8l,i\n(ATC, C = I)\n(9)\nwhile the choice A1 = A, A2 = I and C = I reduces to the combine-then-adapt (CTA) strategy, where\nthe order of the combination and adaptation steps are reversed relative to (9) [22], [23], [30]:\n\u03c8k,i\u22121 =\nX\nl\u2208Nk\nalkwl,i\u22121\nwk,i = \u03c8k,i\u22121 \u2212\u00b5k\u2207wJk(\u03c8k,i\u22121)\n(CTA, C = I)\n(10)\nFurthermore, if in the CTA implementation (10) we enforce A to be doubly stochastic, replace \u2207wJk(\u00b7) by\na subgradient, and use a time-decaying step-size parameter (\u00b5k(i) \u21920), then we obtain the unconstrained\nversion used by [24]. In the sequel, we continue with the general recursions (5)\u2013(7), which allow us\nto examine the convergence properties of several algorithms in a uni\ufb01ed manner. The challenge we\nencounter now is to show that this same class of algorithms can still optimize the cost (4) in a distributed\nmanner when the individual costs {Jl(w)} do not necessarily have the same minimizer. This is actually a\ndemanding task, as the analysis in the coming sections reveals, and we need to introduce novel analysis\ntechniques to be able to handle this general case.\nOctober 31, 2018\nDRAFT\n7\nIII. PERFORMANCE ANALYSIS\nA. Modeling Assumptions\nIn most situations in practice, the true gradient vectors needed in (6) are not available. Instead, perturbed\nversions are available, which we model as\n[\n\u2207wJl(w) = \u2207wJl(w) + vl,i(w)\n(11)\nwhere the random noise term, vl,i(w), may depend on w and will be required to satisfy certain conditions\ngiven by (16)\u2013(17). We refer to the perturbation in (11) as gradient noise. Using (11), the diffusion\nalgorithm (5)\u2013(7) becomes the following, where we are using boldface letters for various quantities to\nhighlight the fact that they are now stochastic in nature due to the randomness in the noise component:\n\u03c6k,i\u22121 =\nN\nX\nl=1\na1,lkwl,i\u22121\n(12)\n\u03c8k,i = \u03c6k,i\u22121 \u2212\u00b5k\nN\nX\nl=1\nclk\nh\n\u2207wJl(\u03c6k,i\u22121) + vl,i(\u03c6k,i\u22121)\ni\n(13)\nwk,i =\nN\nX\nl=1\na2,lk\u03c8l,i\n(14)\nUsing (12)\u2013(14), we now proceed to examine the mean-square performance of the diffusion strategies.\nSpeci\ufb01cally, in the sequel, we study: (i) how fast and (ii) how close the estimator wk,i at each node k\napproaches the Pareto-optimal solution wo in the mean-square-error sense. We establish the convergence\nof all nodes towards the same Pareto-optimal solution within a small MSE bound. Since we are dealing\nwith individual costs that may not have a common minimizer, the approach we employ to examine the\nconvergence properties of the diffusion strategy is fundamentally different from [21]; we follow a system-\ntheoretic approach and call upon the \ufb01xed-point theorem for contractive mappings [29, pp.299\u2013303].\nTo proceed with the analysis, we introduce the following assumptions on the cost functions and gradient\nnoise. As explained in [21], these conditions are weaker than similar conditions in the literature of\ndistributed optimization; in this way, our convergence and performance results hold under more relaxed\nconditions than usually considered in the literature.\nAssumption 1 (Bounded Hessian). Each component cost function Jl(w) has a bounded Hessian matrix,\ni.e., there exist nonnegative real numbers \u03bbl,min and \u03bbl,max such that, for each k = 1, . . . , N:\n\u03bbl,minIM \u2264\u22072\nwJl(w) \u2264\u03bbl,maxIM\n(15)\nwith PN\nl=1 clk\u03bbl,min > 0.\n\u25a0\nOctober 31, 2018\nDRAFT\n8\nAssumption 2 (Gradient noise). There exist \u03b1 \u22650 and \u03c32\nv \u22650 such that, for all w \u2208Fi\u22121:\nE {vl,i(w) | Fi\u22121} = 0\n(16)\nE\n\b\n\u2225vl,i(w)\u22252\t\n\u2264\u03b1 \u00b7 E\u2225\u2207wJl(w)\u22252 + \u03c32\nv\n(17)\nfor all i, l, where Fi\u22121 denotes the past history of estimators {wk,j} for j \u2264i \u22121 and all k.\n\u25a0\nIf we choose C = I, then Assumption 1 implies that the cost functions {Jl(w)} are strongly convex1.\nThis condition can be guaranteed by adding small regularization terms. For example, we can convert a non-\nstrongly convex function J\u2032\nl(w) to a strongly convex one by rede\ufb01ning Jl(w) as Jl(w) \u2190Jl(w)+\u03f5\u2225w\u22252,\nwhere \u03f5 > 0 is a small regularization factor. We further note that, assumption (17) is a mix of the\n\u201crelative random noise\u201d and \u201cabsolute random noise\u201d model usually assumed in stochastic approximation\n[4]. Condition (17) implies that the gradient noise grows when the estimate is away from the optimum\n(large gradient). Condition (17) also states that even when the gradient vector is zero, there is still some\nresidual noise variance \u03c32\nv.\nB. Diffusion Adaptation Operators\nTo analyze the performance of the diffusion adaptation strategies, we \ufb01rst represent the mappings\nperformed by (12)\u2013(14) in terms of useful operators.\nDe\ufb01nition 1 (Combination Operator). Suppose x = col{x1, . . . , xN} is an arbitrary N \u00d71 block column\nvector that is formed by stacking M \u00d7 1 vectors x1, . . . , xN on top of each other. The combination\noperator TA : RMN \u2192RMN is de\ufb01ned as the linear mapping:\nTA(x) \u225c(AT \u2297IM) x\n(18)\nwhere A is an N \u00d7 N left stochastic matrix, and \u2297denotes the Kronecker product operation.\n\u25a0\nDe\ufb01nition 2 (Gradient-Descent Operator). Consider the same N \u00d7 1 block column vector x. Then, the\n1A differentiable function f(x) on Rn is said to be strongly convex if there exists a \u03bbmin > 0 such that f(x + y) \u2265\nf(x)+yT \u2207f(x)+\u03bbmin\u2225y\u22252/2 for any x, y \u2208Rn. And if f(x) is twice-differentiable, this is also equivalent to \u22072f(x) \u2265\u03bbminI\n[4, pp.9-10]. Strong convexity implies that the function f(x) can be lower bounded by some quadratic function.\nOctober 31, 2018\nDRAFT\n9\ngradient-descent operator TG : RMN \u2192RMN is the nonlinear mapping de\ufb01ned by:\nTG(x) \u225c\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0\nx1 \u2212\u00b51\nPN\nl=1 cl1\u2207wJl(x1)\n...\nxN \u2212\u00b5N\nPN\nl=1 clN\u2207wJl(xN)\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb\n(19)\n\u25a0\nDe\ufb01nition 3 (Power Operator). Consider the same N \u00d7 1 block vector x. The power operator P :\nRMN \u2192RN is de\ufb01ned as the mapping:\nP[x] \u225ccol{\u2225x1\u22252, . . . , \u2225xN\u22252}\n(20)\nwhere \u2225\u00b7 \u2225denotes the Euclidean norm of a vector.\n\u25a0\nWe will use the power operator to study how error variances propagate after a speci\ufb01c operator TA(\u00b7)\nor TG(\u00b7) is applied to a random vector. We remark that we are using the notation \u201cP[\u00b7]\u201d rather than \u201cP(\u00b7)\u201d\nto highlight the fact that P is a mapping from RMN to a lower dimensional space RN. In addition to\nthe above three operators, we de\ufb01ne the following aggregate vector of gradient noise that depends on the\nstate x:\nv(x) \u225c\u2212col\n(\n\u00b51\nN\nX\nl=1\ncl1vl(x1), . . . , \u00b5N\nN\nX\nl=1\nclNvl(xN)\n)\n(21)\nWith these de\ufb01nitions, we can now represent the two combination steps (12) and (14) as two combination\noperators TA1(\u00b7) and TA2(\u00b7). We can also represent the adaptation step (13) by a gradient-descent operator\nperturbed by the noise operator (21):\nbTG(x) \u225cTG(x) + v(x)\n(22)\nWe can view bTG(x) as a random operator that maps each input x \u2208RMN into an RMN random vector,\nand we use boldface letter to highlight this random nature. Let\nwi \u225ccol{w1,i, w2,i, . . . , wN,i}\n(23)\ndenote the vector that collects the estimators across all nodes. Then, the overall diffusion adaptation steps\n(12)\u2013(14) that update wi\u22121 to wi can be represented as a cascade composition of three operators:\nbTd(\u00b7) \u225cTA2 \u25e6bTG \u25e6TA1(\u00b7)\n(24)\nOctober 31, 2018\nDRAFT\n10\n2\n6664\n3\n7775\n2\n6664\n3\n7775\n+ \n+ \n+ \n(\u00a2)\n2\n6664\n3\n7775\n2\n6664\n3\n7775\n(\u00a2)\n2\n6664\n3\n7775\n2\n6664\n3\n7775\n+ \n+ \n+ \n(\u00a2)\nwi\u00a11\nwi\u00a11\n\u00c1i\u00a11\n\u00c1i\u00a11\n\u00c1i\u00a11\n\u00c1i\u00a11\n\u00c3i\u00a11\n\u00c3i\u00a11\n\u00c3i\u00a11\n\u00c3i\u00a11\nwi\nwi\n(a) TA1(\u00b7), TA2(\u00b7) and TG(\u00b7).\n(\u00a2)\n(\u00a2)\n(\u00a2)\n+ \nv(\u00a2)\nbT (\u00a2)\nbT (\u00a2)\nw \u00a1\nw\nbT (\u00a2)\nw\nw\n\u2026\u2026 \nbT (\u00a2)\nw\nw \u00a1\nbT (\u00a2)\nw\n(b) Cascade representation of diffusion adaptation.\nFig. 3.\nRepresentation of the diffusion adaptation strategy (12)\u2013(14) in terms of operators. Each diffusion adaptation step can\nbe viewed as a cascade composition of three operators: TA1(\u00b7), TG(\u00b7), and TA2(\u00b7) with gradient perturbation v(\u00b7). If v(\u00b7) = 0,\nthen bTd(\u00b7) becomes Td(\u00b7).\nwhere we use \u25e6to denote the composition of any two operators, i.e., T1 \u25e6T2(x) \u225cT1(T2(x)). If there\nis no gradient noise, then the diffusion adaptation operator (24) reduces to\nTd(\u00b7) \u225cTA2 \u25e6TG \u25e6TA1(\u00b7)\n(25)\nIn other words, the diffusion adaptation over the entire network with and without gradient noise can be\ndescribed in the following compact forms:\nwi = bTd(wi\u22121)\n(26)\nwi = Td(wi\u22121)\n(27)\nFig. 3(a) illustrates the role of the combination operator TA(\u00b7) (combination steps) and the gradient-\ndescent operator TG(\u00b7) (adaptation step). The combination operator TA(\u00b7) aggregates the estimates from\nthe neighborhood (social learning), while the gradient-descent operator TG(\u00b7) incorporates information\nfrom the local gradient vector (self-learning). In Fig. 3(b), we show that each diffusion adaptation step\ncan be represented as the cascade composition of three operators, with perturbation from the gradient\nnoise operator. Next, in Lemma 1, we examine some of the properties of the operators {TA1, TA2, TG},\nwhich are proved in Appendix A.\nLemma 1 (Useful Properties). Consider N\u00d71 block vectors x = col{x1, . . . , xN} and y = col{y1, . . . , yN}\nwith M \u00d7 1 entries {xk, yk}. Then, the operators TA(\u00b7), TG(\u00b7) and P[\u00b7] satisfy the following properties:\n1) (Linearity): TA(\u00b7) is a linear operator.\n2) (Nonnegativity): P[x] \u2ab00.\nOctober 31, 2018\nDRAFT\n11\n3) (Scaling): For any scalar a \u2208R, we have\nP[ax] = a2P[x]\n(28)\n4) (Convexity): suppose x(1), . . . , x(K) are N \u00d7 1 block vectors formed in the same manner as x, and\nlet a1, . . . , aK be non-negative real scalars that add up to one. Then,\nP[a1x(1) + \u00b7 \u00b7 \u00b7 + aKx(K)] \u2aafa1P[x(1)] + \u00b7 \u00b7 \u00b7 + aKP[x(K)]\n(29)\n5) (Additivity): Suppose x = col{x1, . . . , xN} and y = col{y1, . . . , yN} are N \u00d7 1 block random\nvectors that satisfy ExT\nk yk = 0 for k = 1, . . . , N. Then,\nEP[x + y] = EP[x] + EP[y]\n(30)\n6) (Variance relations):\nP[TA(x)] \u2aafAT P[x]\n(31)\nP[TG(x) \u2212TG(y)] \u2aaf\u03932P[x \u2212y]\n(32)\nwhere\n\u0393 \u225cdiag{\u03b31, . . . , \u03b3N}\n(33)\n\u03b3k \u225cmax{|1 \u2212\u00b5k\u03c3k,max|, |1 \u2212\u00b5k\u03c3k,min|}\n(34)\n\u03c3k,min \u225c\nN\nX\nl=1\nclk\u03bbl,min, \u03c3k,max \u225c\nN\nX\nl=1\nclk\u03bbl,max\n(35)\n7) (Block Maximum Norm): The \u221e\u2212norm of P[x] is the squared block maximum norm of x:\n\u2225P[x]\u2225\u221e= \u2225x\u22252\nb,\u221e\u225c\n\u0010\nmax\n1\u2264k\u2264N \u2225xk\u2225\n\u00112\n(36)\n8) (Preservation of Inequality): Suppose vectors x, y and matrix F have nonnegative entries, then\nx \u2aafy implies Fx \u2aafFy.\n\u25a0\nC. Transient Analysis\nUsing the operator representation developed above, we now analyze the transient behavior of the\ndiffusion algorithm (12)\u2013(14). From Fig. 3(b) and the previous discussion, we know that the stochastic\nrecursion wi = bTd(wi\u22121) is a perturbed version of the noise-free recursion wi = Td(wi\u22121). Therefore, we\n\ufb01rst study the convergence of the noise free recursion, and then analyze the effect of gradient perturbation\non the stochastic recursion.\nOctober 31, 2018\nDRAFT\n12\nIntuitively, if recursion wi = Td(wi\u22121) converges, then it should converge to a vector w\u221ethat satis\ufb01es\nw\u221e= Td(w\u221e)\n(37)\nIn other words, the vector w\u221eshould be a \ufb01xed point of the operator Td(\u00b7) [29, p.299]. We need to\nanswer four questions pertaining to the \ufb01xed point. First, does the \ufb01xed point exist? Second, is it unique?\nThird, under which condition does the recursion wi = Td(wi\u22121) converge to the \ufb01xed point? Fourth, how\nfar is the \ufb01xed point w\u221eaway from the minimizer wo of (4)? We answer the \ufb01rst two questions using\nthe Banach Fixed Point Theorem (Contraction Theorem) [29, pp.2\u20139, pp.299\u2013300]. Afterwards, we study\nconvergence under gradient perturbation. The last question will be considered in the next subsection.\nDe\ufb01nition 4 (Metric Space). A set X, whose elements we shall call points, is said to be a metric space\nif we can associate a real number d(p, q) with any two points p and q of X, such that\n(a) d(p, q) > 0 if p \u0338= q; d(p, p) = 0;\n(b) d(p, q) = d(q, p);\n(c) d(p, q) \u2264d(p, r) + d(r, q), for any r \u2208X.\nAny function d(p, q) with these three properties is called a distance function, or a metric, and we denote\na metric space X with distance d(\u00b7, \u00b7) as (X, d).\n\u25a0\nDe\ufb01nition 5 (Contraction). Let (X, d) be a metric space. A mapping T : X \u2212\u2192X is called a contraction\non X if there is a positive real number \u03b4 < 1 such that d(T(x), T(y)) \u2264\u03b4 \u00b7 d(x, y) for all x, y \u2208X\nLemma 2 (Banach Fixed Point Theorem [29]). Consider a metric space (X, d), where X \u0338= \u2205. Suppose\nthat X is complete2 and let T : X \u2192X be a contraction. Then, T has precisely one \ufb01xed point.\n\u25a0\nAs long as we can prove that the diffusion operator Td(\u00b7) is a contraction, i.e., for any two points\nx, y \u2208RMN, after we apply the operator Td(\u00b7), the distance between Td(x) and Td(y) scales down by a\nscalar that is uniformly bounded away from one, then the \ufb01xed point w\u221ede\ufb01ned in (37) exists and is\nunique. We now proceed to show that Td(\u00b7) is a contraction operator in X = RMN when the step-size\nparameters {\u00b5k} satisfy certain conditions.\nTheorem 1 (Fixed Point). Suppose the step-size parameters {\u00b5k} satisfy the following conditions\n0 < \u00b5k <\n2\n\u03c3k,max\n,\nk = 1, 2, . . . , N\n(38)\n2A metric space (X, d) is complete if any of its Cauchy sequences converges to a point in the space; a sequence {xn} is\nCauchy in (X, d) if \u2200\u03f5 > 0, there exists N such that d(xn, xm) < \u03f5 for all n, m > N.\nOctober 31, 2018\nDRAFT\n13\nThen, there exists a unique \ufb01xed point w\u221efor the unperturbed diffusion operator Td(\u00b7) in (25).\nProof: Let x = col{x1, . . . , xN} \u2208RMN\u00d71 be formed by stacking M \u00d7 1 vectors x1, . . . , xN on\ntop of each other. Similarly, let y = col{y1, . . . , yN}. The distance function d(x, y) that we will use is\ninduced from the block maximum norm (36): d(x, y) = \u2225x \u2212y\u2225b,\u221e= max1\u2264k\u2264N \u2225xk \u2212yk\u2225. From the\nde\ufb01nition of the diffusion operator Td(\u00b7) in (25), we have\nP[Td(x) \u2212Td(y)]\n(a)\n= P\nh\nTA2\n\u0010\nTG \u25e6TA1(x) \u2212TG \u25e6TA1(y)\n\u0011i\n(b)\n\u2aafAT\n2 P\nh\nTG \u25e6TA1(x) \u2212TG \u25e6TA1(y)\ni\n(c)\n\u2aafAT\n2 \u03932P[TA1(x) \u2212TA1(y)]\n(d)\n= AT\n2 \u03932P[TA1(x \u2212y)]\n(e)\n\u2aafAT\n2 \u03932AT\n1 P[x \u2212y]\n(39)\nwhere steps (a) and (d) are because of the linearity of TA1(\u00b7) and TA2(\u00b7), steps (b) and (e) are because\nof the variance relation property (31), and step (c) is due to the variance relation property (32). Taking\nthe \u221e\u2212norm of both sides of (39), we have\n\u2225P[Td(x) \u2212Td(y)]\u2225\u221e\u2264\u2225AT\n2 \u03932AT\n1 \u2225\u221e\u00b7 \u2225P[x \u2212y]\u2225\u221e\n\u2264\u2225\u0393\u22252\n\u221e\u00b7 \u2225P[x \u2212y]\u2225\u221e\n(40)\nwhere, in the second inequality, we used the fact that \u2225AT\n1 \u2225\u221e= \u2225AT\n2 \u2225\u221e= 1 since AT\n1 and AT\n2 are\nright-stochastic matrices. Using property (36), we can conclude from (40) that: \u2225Td(x) \u2212Td(y)\u2225b,\u221e\u2264\n\u2225\u0393\u2225\u221e\u00b7 \u2225x \u2212y\u2225b,\u221e. Therefore, the operator Td(\u00b7) is a contraction if \u2225\u0393\u2225\u221e< 1, which, by substituting\n(33)\u2013(34), becomes\n|1 \u2212\u00b5k\u03c3k,max| < 1,\n|1 \u2212\u00b5k\u03c3k,min| < 1,\nk = 1, . . . , N\nand we arrive at the condition (38) on the step-sizes In other words, if condition (38) holds for each\nk = 1, . . . , N, then Td(\u00b7) is a contraction operator. By Lemma 2, the operator Td(\u00b7) will have a unique\n\ufb01xed point w\u221ethat satis\ufb01es equation (37).\nGiven the existence and uniqueness of the \ufb01xed point, the third question to answer is if recursion\nwi = Td(wi\u22121) converges to this \ufb01xed point. The answer is af\ufb01mative under (38). However, we are not\ngoing to study this question separately. Instead, we will analyze the convergence of the more demanding\nnoisy recursion (26). Therefore, we now study how fast and how close the successive estimators {wi}\nOctober 31, 2018\nDRAFT\n14\ngenerated by recursion (26) approach w\u221e. Once this issue is addressed, we will then examine how close\nw\u221eis to the desired wo. Introduce the following mean-square-perturbation (MSP) vector at time i:\nMSPi \u225cEP[wi \u2212w\u221e]\n(41)\nThe k-th entry of MSPi characterizes how far away the estimate wk,i at node k and time i is from wk,\u221e\nin the mean-square sense. To study the closeness of wi to w\u221e, we shall study how the quantity MSPi\nevolves over time. By (26), (37) and the de\ufb01nitions of bTd(\u00b7) and Td(\u00b7) in (24) and (25), we obtain\nMSPi = EP[wi \u2212w\u221e]\n= EP\nh\nTA2 \u25e6bTG \u25e6TA1(wi\u22121) \u2212TA2 \u25e6TG \u25e6TA1(w\u221e)\ni\n(a)\n= EP\nh\nTA2\n\u0010\nbTG \u25e6TA1(wi\u22121) \u2212TG \u25e6TA1(w\u221e)\n\u0011i\n(b)\n\u2aafAT\n2 EP\nh\nbTG \u25e6TA1(wi\u22121) \u2212TG \u25e6TA1(w\u221e)\ni\n(c)\n= AT\n2 EP\nh\nTG\n\u0010\nTA1(wi\u22121)\n\u0011\n\u2212TG\n\u0010\nTA1(w\u221e)\n\u0011\n+ v\n\u0010\nTA1(wi\u22121)\n\u0011i\n(d)\n= AT\n2\nn\nEP\nh\nTG\n\u0010\nTA1(wi\u22121)\n\u0011\n\u2212TG\n\u0010\nTA1(w\u221e)\n\u0011i\n+ EP\nh\nv\n\u0010\nTA1(wi\u22121)\n\u0011io\n(e)\n\u2aafAT\n2 \u03932EP[TA1(wi\u22121) \u2212TA1(w\u221e)] + AT\n2 EP[v(TA1(wi\u22121))]\n(f)\n\u2aafAT\n2 \u03932AT\n1 \u00b7 EP[wi\u22121 \u2212w\u221e] + AT\n2 EP[v(TA1(wi\u22121))]\n= AT\n2 \u03932AT\n1 \u00b7 MSPi\u22121 + AT\n2 EP[v(TA1(wi\u22121))]\n(42)\nwhere step (a) is by the linearity of TA1(\u00b7), steps (b) and (f) are by property (31), step (c) is by the\nsubstitution of (22), step (d) is by Property 5 in Lemma 1 and assumption (16), and step (e) is by (32).\nTo proceed with the analysis, we establish the following lemma to bound the second term in (42).\nLemma 3 (Bound on Gradient Perturbation). It holds that\nEP[v(TA1(wi\u22121))] \u2aaf4\u03b1\u03bb2\nmax\u2225C\u22252\n1\u00b7\u21262AT\n1 \u00b7EP[wi\u22121\u2212w\u221e]+\u2225C\u22252\n1\u21262bv\n(43)\nwhere\n\u03bbmax \u225c\nmax\n1\u2264k\u2264N \u03bbk,max\n(44)\nbv \u225c4\u03b1\u03bb2\nmaxAT\n1 P[w\u221e\u22121N \u2297wo]\n+ max\n1\u2264k\u2264N{2\u03b1\u2225\u2207wJk(wo)\u22252 + \u03c32\nv}\n(45)\n\u2126\u225cdiag{\u00b51, . . . , \u00b5N}\n(46)\nOctober 31, 2018\nDRAFT\n15\nProof: By the de\ufb01nition of v(x) in (21) with x = TA1(wi\u22121) being a random vector, we get\nEP[v(x)] =\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0\n\u00b52\n1E\n\r\r\r PN\nl=1 cl1vl(x1),\n\r\r\r\n2\n...\n\u00b52\nNE\n\r\r\r\nPN\nl=1 clNvl(xN)\n\r\r\r\n2\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb\n(47)\nFor each block in (47), using Jensen\u2019s inequality, we have\nE\n\r\r\r\nN\nX\nl=1\nclkvl(xk)\n\r\r\r\n2\n=\n\u0010 N\nX\nl=1\nclk\n\u00112\n\u00b7 E\n\r\r\r\nN\nX\nl=1\nclk\nPN\nl=1 clk\nvl(xk)\n\r\r\r\n2\n\u2264\n\u0010 N\nX\nl=1\nclk\n\u00112\n\u00b7\nN\nX\nl=1\nclk\nPN\nl=1 clk\nE\u2225vl(xk)\u22252\n\u2264\u2225C\u22251\nN\nX\nl=1\nclk\nh\n\u03b1E\u2225\u2207wJl(xk)\u22252 + \u03c32\nv\ni\n(48)\nwhere \u2225\u00b7\u22251 denotes the maximum absolute column sum, and in the last step, we used (17). Using (123),\n\u2207wJl(xk) = \u2207wJl(wo) +\nh Z 1\n0\n\u22072\nwJl\n\u0010\nwo + t(xk \u2212wo)\n\u0011\ndt\ni\n(xk \u2212wo)\n(49)\nFrom (124) and the norm inequality \u2225x + y\u22252 \u22642\u2225x\u22252 + 2\u2225y\u22252, we obtain\n\u2225\u2207wJl(xk)\u22252 \u22642\u2225\u2207wJl(wo)\u22252+2\u03bb2\nl,max\u00b7\u2225xk \u2212wo\u22252\n\u22642\u2225\u2207wJl(wo)\u22252+2\u03bb2\nmax\u00b7\u2225xk \u2212wo\u22252\n(50)\nSubstituting (50) into (48), we obtain\nE\n\r\r\r\nN\nX\nl=1\nclkvl(xk)\n\r\r\r\n2\n\u2264\u2225C\u22251\nN\nX\nl=1\nclk\nh\n2\u03b1\u03bb2\nmaxE\u2225xk\u2212wo\u22252+2\u03b1\u2225\u2207wJl(wo)\u22252+\u03c32\nv\ni\n\u22642\u03b1\u03bb2\nmax\u2225C\u22252\n1 \u00b7 E\u2225xk \u2212wo\u22252 + \u2225C\u22252\n1 \u00b7 \u03c32\nv\n(51)\nwhere \u03c32\nv \u225cmax\n1\u2264l\u2264N{2\u03b1\u2225\u2207wJl(wo)\u22252 + \u03c32\nv}. Substituting (51) and x = TA1(wi\u22121) into (47) leads to\nEP[v(TA1(wi\u22121))] \u2aaf\u21262n\n2\u03b1\u2225C\u22252\n1\u03bb2\nmax \u00b7 EP[TA1(wi\u22121) \u22121N \u2297wo] + \u2225C\u22252\n1\u03c32\nv1N\no\n(a)\n= \u21262n\n2\u03b1\u2225C\u22252\n1\u03bb2\nmax \u00b7 EP\nh\nTA1(wi\u22121) \u2212TA1(1N \u2297wo)\ni\n+ \u2225C\u22252\n1\u03c32\nv1N\no\n(b)\n= \u21262n\n2\u03b1\u2225C\u22252\n1\u03bb2\nmax \u00b7 EP\nh\nTA1\n\u0010\nwi\u22121 \u22121N \u2297wo\u0011i\n+ \u2225C\u22252\n1\u03c32\nv1N\no\n(c)\n\u2aaf\u21262n\n2\u03b1\u2225C\u22252\n1\u03bb2\nmaxAT\n1 \u00b7 EP[wi\u22121 \u22121N \u2297wo] + \u2225C\u22252\n1\u03c32\nv1N\no\nOctober 31, 2018\nDRAFT\n16\n(d)\n= \u21262\n(\n2\u03b1\u2225C\u22252\n1\u03bb2\nmaxAT\n1 \u00b74EP\n\"\nwi\u22121\u2212w\u221e\n2\n+ w\u221e\u22121N \u2297wo\n2\n#\n+ \u2225C\u22252\n1\u03c32\nv1N\n)\n(e)\n\u2aaf\u21262n\n2\u03b1\u2225C\u22252\n1\u03bb2\nmaxAT\n1 \u00b7\n\u0010\n2EP[wi\u22121\u2212w\u221e]+2P[w\u221e\u22121N \u2297wo]\n\u0011\n+\u2225C\u22252\n1\u03c32\nv1N\no\n= 4\u03b1\u2225C\u22252\n1\u03bb2\nmax\u00b7\u21262AT\n1 \u00b7EP[wi\u22121\u2212w\u221e]+\u2225C\u22252\n1\u21262\u00b7bv\n(52)\nwhere step (a) is due to the fact that AT\n1 is right-stochastic so that TA1(1N \u2297wo) = 1N \u2297wo, step (b) is\nbecause of the linearity of TA1(\u00b7), step (c) is due to property (31), step (d) is a consequence of Property\n3 of Lemma 1, and step (e) is due to the convexity property (29).\nSubstituting (43) into (42), we obtain\nMSPi \u2aafAT\n2 \u0393dAT\n1 \u00b7 MSPi\u22121 + \u2225C\u22252\n1 \u00b7 AT\n2 \u21262bv\n(53)\nwhere\n\u0393d \u225c\u03932 + 4\u03b1\u03bb2\nmax\u2225C\u22252\n1 \u00b7 \u21262\n(54)\nThe following theorem gives the stability conditions on the inequality recursion (53) and derives both\nasymptotic and non-asymptotic bounds for MSP.\nTheorem 2 (Mean-Square Stability and Bounds). Suppose AT\n2 \u0393dAT\n1 is a stable matrix, i.e., \u03c1(AT\n2 \u0393dAT\n1 ) <\n1. Then, the following non-asymptotic bound holds for all i \u22650:\nMSPi \u2aaf(AT\n2 \u0393dAT\n1 )i[MSP0 \u2212MSPub\n\u221e] + MSPub\n\u221e\n(55)\nwhere MSPub\n\u221eis the asymptotic upper bound on MSP de\ufb01ned as\nMSPub\n\u221e\u225c\u2225C\u22252\n1(IN \u2212AT\n2 \u0393dAT\n1 )\u22121AT\n2 \u21262bv\n(56)\nAnd, as i \u2192\u221e, we have the following asymptotic bound\nlim sup\ni\u2192\u221e\nMSPi \u2aafMSPub\n\u221e\n(57)\nFurthermore, a suf\ufb01cient condition that guarantees the stability of the matrix AT\n2 \u0393dAT\n1 is that\n0<\u00b5k <min\n(\n\u03c3k,max\n\u03c32\nk,max+4\u03b1\u03bb2max\u2225C\u22252\n1\n,\n\u03c3k,min\n\u03c32\nk,min+4\u03b1\u03bb2max\u2225C\u22252\n1\n)\n(58)\nfor all k = 1, . . . , N, where \u03c3k,max and \u03c3k,min were de\ufb01ned earlier in (35).\nProof: Iterating inequality (53), we obtain\nMSPi \u2aaf(AT\n2 \u0393dAT\n1 )iMSP0 + \u2225C\u22252\n1 \u00b7\nh i\u22121\nX\nj=0\n(AT\n2 \u0393dAT\n1 )ji\nAT\n2 \u21262bv\n(59)\nOctober 31, 2018\nDRAFT\n17\nFor the second term in (59), we note that (I + X + \u00b7 \u00b7 \u00b7 + Xi\u22121)(I \u2212X) = I \u2212Xi. If X is a stable\nmatrix so that (I \u2212X) is invertible, then it leads to Pi\u22121\nj=0 Xj = (I \u2212Xi)(I \u2212X)\u22121. Using this relation\nand given that the matrix AT\n2 \u0393dAT\n1 is stable, we can express (59) as\nMSPi \u2aaf(AT\n2 \u0393dAT\n1 )iMSP0 + \u2225C\u22252\n1 \u00b7\nh\nIN \u2212(AT\n2 \u0393dAT\n1 )ii\n(IN \u2212AT\n2 \u0393dAT\n1 )\u22121AT\n2 \u21262bv\n= (AT\n2 \u0393dAT\n1 )i[MSP0 \u2212MSPub\n\u221e] + MSPub\n\u221e\n(60)\nLetting i \u2192\u221eon both sides of the above inequality, we get lim sup\ni\u2192\u221e\nMSPi \u2aafMSPub\n\u221e. In the last step,\nwe need to show the conditions on the step-sizes {\u00b5k} that guarantee stability of the matrix AT\n2 \u0393dAT\n1 .\nNote that the spectral radius of a matrix is upper bounded by its matrix norms. Therefore,\n\u03c1(AT\n2 \u0393dAT\n1 ) \u2264\u2225AT\n2 \u0393dAT\n1 \u2225\u221e\n\u2264\u2225AT\n2 \u2225\u221e\u00b7 \u2225\u0393d\u2225\u221e\u00b7 \u2225AT\n1 \u2225\u221e\n= \u2225\u0393d\u2225\u221e\n=\n\r\r\r\u03932 + 4\u03b1\u03bb2\nmax\u2225C\u22252\n1 \u00b7 \u21262\r\r\r\n\u221e\nIf the right-hand side of the above inequality is strictly less than one, then the matrix AT\n2 \u0393dAT\n2 is stable.\nUsing (33)\u2013(34), this condition is satis\ufb01ed by the following quadratic inequalities on \u00b5k :\n(1 \u2212\u00b5k\u03c3k,max)2 + \u00b52\nk \u00b7 4\u03b1\u03bb2\nmax\u2225C\u22252\n1 < 1\n(61)\n(1 \u2212\u00b5k\u03c3k,min)2 + \u00b52\nk \u00b7 4\u03b1\u03bb2\nmax\u2225C\u22252\n1 < 1\n(62)\nfor all k = 1, . . . , N. Solving the above inequalities, we obtain condition (58).\nThe non-asymptotic bound (55) characterizes how the MSP at each node evolves over time. It shows that\nthe MSP converges to steady state at a geometric rate determined by the spectral radius of the matrix\nAT\n2 \u0393dAT\n1 . The transient term is determined by the difference between the initial MSP and the steay-state\nMSP. At steady state, the MSP is upper bounded by MSPub\n\u221e. We now examine closely how small the\nsteady-state MSP can be for small step-size parameters {\u00b5k}. Taking the \u221e\u2212norm of both sides of (57)\nand using the relation (IN \u2212AT\n2 \u0393dAT\n1 )\u22121 = P\u221e\nj=0(AT\n2 \u0393dAT\n1 )j, we obtain\n\u2225MSPub\n\u221e\u2225\u221e=\n\r\r\r\u2225C\u22252\n1 \u00b7 (IN \u2212AT\n2 \u0393dAT\n1 )\u22121 \u00b7 AT\n2 \u21262bv\n\r\r\r\n\u221e\n\u2264\u2225C\u22252\n1 \u00b7\n \u221e\nX\nj=0\n\u2225AT\n2 \u2225j\n\u221e\u00b7 \u2225\u0393d\u2225j\n\u221e\u00b7 \u2225AT\n1 \u2225j\n\u221e\n!\n\u00b7 \u2225AT\n2 \u2225\u221e\u00b7 \u2225\u2126\u22252\n\u221e\u00b7 \u2225bv\u2225\u221e\n(a)\n\u2264\u2225C\u22252\n1 \u00b7\n \u221e\nX\nj=0\n\u2225\u0393d\u2225j\n\u221e\n!\n\u00b7\n\u0010\nmax\n1\u2264k\u2264N \u00b5k\n\u00112\n\u00b7 \u2225bv\u2225\u221e\nOctober 31, 2018\nDRAFT\n18\n= \u2225C\u22252\n1 \u00b7 \u2225bv\u2225\u221e\n1 \u2212\u2225\u0393d\u2225\u221e\n\u00b7\n\u0010\nmax\n1\u2264k\u2264N \u00b5k\n\u00112\n(63)\nwhere step (a) is because AT\n1 and AT\n2 are right-stochastic matrices so that their \u221e\u2212norms (maximum\nabsolute row sum) are one. Let \u00b5max and \u00b5min denote the maximum and minimum values of {\u00b5k},\nrespectively, and let \u03b2 \u225c\u00b5min/\u00b5max. For suf\ufb01ciently small step-sizes, by the de\ufb01nitions of \u0393d and \u0393 in\n(54) and (33), we have\n\u2225\u0393d\u2225\u221e\u2264\u2225\u0393\u22252\n\u221e+ 4\u03b1\u03bbmax\u2225C\u22252\n1 \u00b7 \u2225\u2126\u22252\n\u221e\n(a)\n=\nmax\n1\u2264k\u2264N{|1 \u2212\u00b5k\u03c3k,min|2} + 4\u03b1\u03bbmax\u00b52\nmax\u2225C\u22252\n1\n\u22641\u22122\u00b5min\u03c3min+\u00b52\nmax(\u03c32\nmax+4\u03b1\u03bbmax\u2225C\u22252\n1)\n= 1\u22122\u03b2\u00b5max\u03c3min+\u00b52\nmax(\u03c32\nmax+4\u03b1\u03bbmax\u2225C\u22252\n1)\n(64)\nwhere \u03c3max and \u03c3min are the maximum and minimum values of {\u03c3k,max} and {\u03c3k,min}, respectively,\nand step (a) holds for suf\ufb01ciently small step-sizes. Note that (63) is a monotonically increasing function\nof \u2225\u0393d\u2225\u221e. Substituting (64) into (63), we get\nlim sup\ni\u2192\u221e\n\u2225MSPi\u2225\u221e\u2264\u2225MSPub\n\u221e\u2225\u221e\u2264\n\u2225C\u22252\n1 \u00b7 \u2225bv\u2225\u221e\u00b7 \u00b5max\n2\u03b2\u03c3min\u2212\u00b5max(\u03c32max+4\u03b1\u03bbmax\u2225C\u22252\n1) \u223cO(\u00b5max)\n(65)\nNote that, for suf\ufb01ciently small step-sizes, the right-hand side of (65) is approximately \u2225C\u22252\n1\u00b7\u2225bv\u2225\u221e\n2\u03b2\u03c3min\n\u00b5max,\nwhich is on the order of O(\u00b5max). In other words, the steady-state MSP can be made be arbitrarily small\nfor small step-sizes, and the estimators wi = col{w1,i, . . . , wN,i} will be close to the \ufb01xed point w\u221e(in\nthe mean-square sense) even under gradient perturbations. To understand how close the estimate wk,i at\neach node k is to the Pareto-optimal solution wo, a natural question to consider is how close the \ufb01xed\npoint w\u221eis to 1N \u2297wo, which we study next.\nD. Bias Analysis\nOur objective is to examine how large \u22251N \u2297wo \u2212w\u221e\u22252 is when the step-sizes are small. We carry\nout the analysis in two steps: \ufb01rst, we derive an expression for \u02dcw\u221e\u225c1N \u2297wo \u2212w\u221e, and then we\nderive the conditions that guarantee small bias.\nTo begin with, recall that w\u221eis the \ufb01xed point of Td(\u00b7), to which the recursion wi = Td(wi\u22121)\nconverges. Also note that Td(\u00b7) is an operator representation of the recursions (5)\u2013(7). We let i \u2192\u221eon\nboth sides of (5)\u2013(7) and obtain\n\u03c6k,\u221e=\nN\nX\nl=1\na1,lk wl,\u221e\n(66)\nOctober 31, 2018\nDRAFT\n19\n\u03c8k,\u221e= \u03c6k,\u221e\u2212\u00b5k\nN\nX\nl=1\nclk\u2207wJl(\u03c6k,\u221e)\n(67)\nwk,\u221e=\nN\nX\nl=1\na2,lk \u03c8l,\u221e\n(68)\nwhere wk,\u221e, \u03c6k,\u221eand \u03c8k,\u221edenote the limits of wk,i, \u03c6k,i and \u03c8k,i as i \u2192\u221e, respectively. Introduce\nthe following bias vectors at node k\n\u02dcwk,\u221e\u225cwo\u2212wk,\u221e, \u02dc\u03c6k,\u221e\u225cwo\u2212\u03c6k,\u221e, \u02dc\u03c8k,\u221e\u225cwo\u2212\u03c8k,\u221e\n(69)\nSubtracting each equation of (66)\u2013(68) from wo and using relation \u2207wJl(\u03c6k,\u221e) = \u2207wJl(wo)\u2212Hlk,\u221e\u02dc\u03c6k,\u221e\nthat can be derived from Lemma 4 in Appendix A, we obtain\n\u02dc\u03c6k,\u221e=\nN\nX\nl=1\na1,lk \u02dcwl,\u221e\n(70)\n\u02dc\u03c8k,\u221e=\n\"\nIM \u2212\u00b5k\nN\nX\nl=1\nclkHlk,\u221e\n#\n\u02dc\u03c6k,\u221e+\u00b5k\nN\nX\nl=1\nclk\u2207wJl(wo)\n(71)\n\u02dcwk,\u221e=\nN\nX\nl=1\na2,lk \u02dc\u03c8l,\u221e\n(72)\nwhere Hlk,\u221eis a positive semi-de\ufb01nite symmetric matrix de\ufb01ned as\nHlk,\u221e\u225c\nZ 1\n0\n\u22072\nwJl\n\u0010\nwo\u2212t\nN\nX\nl=1\na1,lk \u02dcwl,\u221e)\n\u0011\ndt\n(73)\nIntroduce the following global vectors and matrices\n\u02dcw\u221e\u225c1N \u2297wo \u2212w\u221e= col{ \u02dcw1,\u221e, . . . , \u02dcwN,\u221e}\n(74)\nA1 \u225cA1 \u2297IM,\nA2 \u225cA2 \u2297IM,\nC \u225cC \u2297IM,\n(75)\nM \u225cdiag{\u00b51, . . . , \u00b5N} \u2297IM\n(76)\nR\u221e\u225c\nN\nX\nl=1\ndiag\nn\ncl1Hl1,\u221e, \u00b7 \u00b7 \u00b7 , clNHlN,\u221e\no\n,\n(77)\ngo \u225ccol{\u2207wJ1(wo), . . . , \u2207wJN(wo)}\n(78)\nThen, expressions (70), (72) and (71) lead to\n\u02dcw\u221e=\nh\nIMN \u2212AT\n2 (IMN \u2212MR\u221e) AT\n1\ni\u22121\nAT\n2 MCT go\n(79)\nTheorem 3 (Bias at Small Step-sizes). Suppose that AT\n2 AT\n1 is a regular right-stochastic matrix, so that\nits eigenvalue of largest magnitude is one with multiplicity one, and all other eigenvalues are strictly\nOctober 31, 2018\nDRAFT\n20\nsmaller than one. Let \u03b8T denote the left eigenvector of AT\n2 AT\n1 of eigenvalue one. Furthermore, assume\nthe following condition holds:\n\u03b8T AT\n2 \u2126CT = c01T\n(80)\nwhere \u2126\u225cdiag{\u00b51, . . . , \u00b5N} was de\ufb01ned earlier in Lemma 3, and c0 is some constant. Then,\n\u2225\u02dcw\u221e\u22252 = \u22251N \u2297wo \u2212w\u221e\u22252 \u223cO(\u00b52\nmax)\n(81)\nProof: See Appendix B.\nTherefore, as long as the network is connected (not necessarily fully connected) and condition (80) holds,\nthe bias would become arbitrarily small. For condition (80) to hold, one choice is to require the matrices\nAT\n1 and AT\n2 to be doubly stochastic, and all nodes to use the same step-size \u00b5, namely, \u2126= \u00b5IN. In that\ncase, the matrix AT\n1 AT\n2 is doubly-stochastic so that the left eigenvector of eigenvalue one is \u03b8T = 1T\nand (80) holds.\nFinally, we combine the results from Theorems 2 and 3 to bound the mean-square-error (MSE) of the\nestimators {wk,i} from the desired Pareto-optimal solution wo. Introduce the N \u00d7 1 MSE vector\nMSEi \u225cEP[ \u02dcwi]\n= EP[1N \u2297wo \u2212wi]\n= col\nn\nE\u2225\u02dcw1,i\u22252, . . . , E\u2225\u02dcwN,i\u22252o\n(82)\nUsing Properties 3\u20134 in Lemma 1, we obtain\nMSEi = EP\nh\n2\n\u00101N \u2297wo\u2212w\u221e\n2\n+ w\u221e\u2212wi\n2\n\u0011i\n\u2aaf2P[ \u02dcw\u221e]+2 EP[w\u221e\u2212wi]\n= 2P[ \u02dcw\u221e]+2 MSPi\n(83)\nTaking the \u221e\u2212norm of both sides of above inequality and using property (36), we obtain\nlim sup\ni\u2192\u221e\n\u2225MSEi\u2225\u221e\u22642\u2225P[ \u02dcw\u221e]\u2225\u221e+ 2 lim sup\ni\u2192\u221e\n\u2225MSPi\u2225\u221e\n= 2\u2225\u02dcw\u221e\u22252\nb,\u221e+ 2 lim sup\ni\u2192\u221e\n\u2225MSPi\u2225\u221e\n\u223cO(\u00b52\nmax) + O(\u00b5max)\n(84)\nwhere in the last step, we used (65) and (81), and the fact that all vector norms are equivalent. Therefore,\nas the step-sizes become small, the MSEs become small and the estimates {wk,i} get arbitrarily close\nOctober 31, 2018\nDRAFT\n21\nto the Pareto-optimal solution wo. We also observe that, for small step-sizes, the dominating steady-state\nerror is MSP, which is caused by the gradient noise and is on the order of O(\u00b5max). On the other hand,\nthe bias term is a high order component, i.e., O(\u00b52\nmax), and can be ignored.\nThe fact that the bias term \u02dcw\u221eis small also gives us a useful approximation for R\u221ein (77). Since\n\u02dcw\u221e= col{ \u02dcw1,\u221e, . . . , \u02dcwN,\u221e} is small for small step-sizes, the matrix Hlk,\u221ede\ufb01ned in (73) can be\napproximated as Hlk,\u221e\u2248\u22072\nwJl(wo). Then, by de\ufb01nition (77), we have\nR\u221e\u2248\nN\nX\nl=1\ndiag\nn\ncl1\u22072\nwJl(wo), . . . , clN\u22072\nwJl(wo)\no\n(85)\nExpressing (85) is useful for evaluating closed-form expressions of the steady-state MSE in sequel.\nE. Steady-State Performance\nSo far, we derived inequalities (84) to bound the steady-state performance, and showed that, for small\nstep-sizes, the solution at each node k approaches the same Pareto-optimal point wo. In this section,\nwe derive closed-form expressions (rather than bounds) for the steady-state MSE at small step-sizes.\nIntroduce the error vectors3\n\u02dc\u03c6k,i \u225cwo\u2212\u03c6k,i, \u02dc\u03c8k,i \u225cwo\u2212\u03c8k,i, \u02dcwk,i \u225cwo\u2212wk,i\n(86)\nand the following global random quantities\n\u02dcwi \u225ccol{ \u02dcw1,i, . . . , \u02dcwN,i}\n(87)\nRi\u22121 \u225c\nN\nX\nl=1\ndiag\nn\ncl1Hl1,i\u22121, \u00b7 \u00b7 \u00b7 , clNHlN,i\u22121\no\n(88)\nHlk,i\u22121 \u225c\nZ 1\n0\n\u22072\nwJl\n \nwo\u2212t\nN\nX\nl=1\na1,lk \u02dcwl,i\u22121\n!\ndt\n(89)\ngi \u225c\nN\nX\nl=1\ncol\nn\ncl1vl(\u03c61,i\u22121), \u00b7 \u00b7 \u00b7 , clNvl(\u03c6N,i\u22121)\no\n(90)\nThen, extending the derivation from [21, Sec. IV A], we can establish that\n\u02dcwi = AT\n2 [IMN \u2212MRi\u22121]AT\n1 \u02dcwi\u22121+AT\n2 MCT go+AT\n2 Mgi\n(91)\nAccording to (84), the error \u02dcwk,i at each node k would be small for small step-sizes and after long enough\ntime. In other words, wk,i is close to wo. And recalling from (12) that \u03c6k,i\u22121 is a convex combination\n3 In this paper, we always use the notation \u02dcw = wo \u2212w to denote the error relative to wo. For the error between w and the\n\ufb01xed point w\u221e, we do not de\ufb01ne a separate notation, but instead write w\u221e\u2212w explicitly to avoid confusion.\nOctober 31, 2018\nDRAFT\n22\nof {wl,i}, we conclude that the quantities {\u03c6l,i\u22121} are also close to wo. Therefore, we can approximate\nHlk,i\u22121, Ri\u22121 and gi in (88)\u2013(90) by\nHlk,i\u22121 \u2248\nZ 1\n0\n\u22072\nwJl(wo)dt=\u22072\nwJl(wo)\n(92)\nRi\u22121 \u2248\nN\nX\nl=1\ndiag\nn\ncl1\u22072\nwJl(wo), . . . , clN\u22072\nwJl(wo)\no\n\u2248R\u221e\n(93)\nThen, the error recursion (91) can be approximated by\n\u02dcwi = AT\n2 [IMN \u2212MR\u221e]AT\n1 \u02dcwi\u22121+AT\n2 MCT go+AT\n2 Mgi\n(94)\nFirst, let us examine the behavior of E \u02dcwi. Taking expectation of both sides of recursion (94), we obtain\nE \u02dcwi = AT\n2 [IMN \u2212MR\u221e]AT\n1 E \u02dcwi\u22121 + AT\n2 MCT go\n(95)\nThis recursion converges when the matrix AT\n2 [IMN \u2212MR\u221e]AT\n1 is stable, which is guaranteed by (38)\n(see Appendix C of [21]). Let i \u2192\u221eon both sides of (95) so that\nE \u02dcw\u221e\u225clim\ni\u2192\u221eE \u02dcwi\n=\nh\nIMN \u2212AT\n2 (IMN \u2212MR\u221e) AT\n1\ni\u22121\nAT\n2 MCT go\n(96)\nNote that E \u02dcw\u221ecoincides with (79). By Theorem 3, we know that the squared norm of this expression\nis on the order of O(\u00b52\nmax) at small step-sizes \u2014 see (81). Next, we derive closed-form expressions for\nthe MSEs, i.e., E\u2225\u02dcwk,i\u22252. Let Rv denote the covariance matrix of gi evaluated at wo:\nRv = E\n(\" N\nX\nl=1\ncol\nn\ncl1vl,i(wo), \u00b7 \u00b7 \u00b7 , clNvl,i(wo)\no#\" N\nX\nl=1\ncol\nn\ncl1vl,i(wo), \u00b7 \u00b7 \u00b7 , clNvl,i(wo)\no#T)\n(97)\nIn practice, we can evaluate Rv from the expressions of {vl,i(wo)}. Equating the squared weighted\nEuclidean \u201cnorm\u201d of both sides of (94), applying the expectation operator with assumption (16), and\nfollowing the same line of reasoning from [21], we can establish the following approximate variance\nrelation at small step-sizes:\nE\u2225\u02dcwi\u22252\n\u03a3 \u2248E\u2225\u02dcwi\u22121\u22252\n\u03a3\u2032 + Tr(\u03a3AT\n2 MRvMA2) + Tr{\u03a3AT\n2 MCT go(AT\n2 MCT go)T }\n+ 2(AT\n2 MCT go)T \u03a3AT\n2 (IMN \u2212MR\u221e) AT\n1 E \u02dcwi\u22121\n(98)\n\u03a3\u2032 \u2248A1 (IMN \u2212MR\u221e) A2\u03a3AT\n2 (IMN \u2212MR\u221e) AT\n1\n(99)\nwhere \u03a3 is a positive semi-de\ufb01nite weighting matrix that we are free to choose. Let \u03c3 = vec(\u03a3) denote\nthe vectorization operation that stacks the columns of a matrix \u03a3 on top of each other. We shall use the\nOctober 31, 2018\nDRAFT\n23\nnotation \u2225x\u22252\n\u03c3 and \u2225x\u22252\n\u03a3 interchangeably. Following the argument from [21], we can rewrite (98) as\nE\u2225\u02dcwi\u22252\n\u03c3 \u2248E\u2225\u02dcwi\u22121\u22252\nF\u03c3 + rT \u03c3 + \u03c3T Q E \u02dcwi\u22121\n(100)\nwhere\nF \u225cA1[IMN \u2212MR\u221e]A2 \u2297A1[IMN \u2212MR\u221e]A2\n(101)\nr \u225cvec\n\u0000AT\n2 MRvMA2\n\u0001\n+AT\n2 MCT go\u2297AT\n2 MCTgo\n(102)\nQ \u225c2AT\n2 (IMN \u2212MR\u221e)AT\n1 \u2297AT\n2 MCT go\n(103)\nWe already established that E \u02dcwi\u22121 on the right-hand side of (100) converges to its limit E \u02dcw\u221eunder\ncondition (38). And, it was shown in [31, pp.344-346] that such recursion converges to a steady-state\nvalue if the matrix F is stable, i.e., \u03c1(F) < 1. This condition is guaranteed when the step-sizes are\nsuf\ufb01ciently small (or chosen according to (38)) \u2014 see the proof in Appendix C of [21]. Letting i \u2192\u221e\non both sides of expression (100), we obtain:\nlim\ni\u2192\u221eE\u2225\u02dcwi\u22252\n(I\u2212F)\u03c3 \u2248(r + Q E \u02dcw\u221e)T \u03c3\n(104)\nWe can now resort to (104) and use it to evaluate various performance metrics by choosing proper\nweighting matrices \u03a3 (or \u03c3). For example, the MSE of any node k can be obtained by computing\nlimi\u2192\u221eE\u2225\u02dcwi\u22252\nT with a block weighting matrix T that has an identity matrix at block (k, k) and\nzeros elsewhere: lim\ni\u2192\u221eE\u2225\u02dcwk,i\u22252 = lim\ni\u2192\u221eE\u2225\u02dcwi\u22252\nT . Denote the vectorized version of this matrix by tk \u225c\nvec(diag(ek)\u2297IM), where ek is a vector whose kth entry is one and zeros elsewhere. Then, if we select\n\u03c3 in (104) as \u03c3 = (I \u2212F)\u22121tk, the term on the left-hand side becomes the desired limi\u2192\u221eE\u2225\u02dcwk,i\u22252\nand the MSE for node k is therefore given by:\nMSEk \u225clim\ni\u2192\u221eE\u2225\u02dcwk,i\u22252 \u2248(r + Q E \u02dcw\u221e)T (I\u2212F)\u22121tk\n(105)\nIf we are interested in the average network MSE, then it is given by\nMSE \u225c1\nN\nN\nX\nk=1\nMSEk\n(106)\nIV. APPLICATION TO COLLABORATIVE DECISION MAKING\nWe illustrate one application of the framework developed in the previous sections to the problem of\ncollaborative decision making over a network of N agents. We consider an application in \ufb01nance where\neach entry of the decision vector w denotes the amount of investment in a speci\ufb01c type of asset. Let\nthe M \u00d7 1 vector p represent the return in investment. Each entry of p represents the return for a\nOctober 31, 2018\nDRAFT\n24\nunit investment in the corresponding asset. Let p and Rp denote the mean and covariance matrix of p,\nrespectively. Then, the overall return by the agents for a decision vector w is pT w. Note that, with decision\nw, the return pT w is a (scalar) random variable with mean pT w and variance var(pT w) = wT Rpw, which\nare called the expected return and variance of the return in classical Markowitz portfolio optimization [2,\np.155], [32]\u2013[35]. These two metrics are often used to characterize the quality of the decision w: we want\nto maximize the expected return while minimizing the variance. However, solving the problem directly\nrequires all agents to know the global statistics p and Rp. What is available in practice are observations\nthat are collected at the various nodes. Suppose a subset U of the agents observes a sequence of return\nvectors {uk,i} with Euk,i = p. The subscripts k and i denote that the return is observed by node k at\ntime i. Then, we can formulate the cost functions for the nodes in set U as follows:\nJu,k(w) = \u2212E[uT\nk,iw] = \u2212pT w\n(107)\nk \u2208U \u2282{1, . . . , N}\nWe place a negative sign in (107) so that minimizing Ju,k(w) is equivalent to maximizing the expected\nreturn. Similarly, suppose there is another subset of nodes, exclusive from U and denoted by S, which\nobserves a sequence of centered return vectors {sk,i}, namely, vectors that have the same distribution as\np \u2212Ep so that E[sk,isT\nk,i] = Rp. Then, we can associate with these nodes the cost functions:\nJs,k(w) = E\nh\n|sT\nk,iw|2i\n= wT Rpw\n(108)\nk \u2208S \u2282{1, . . . , N}\nAdditionally, apart from selecting the decision vector w to maximize the return subject to minimizing\nits variance, the investment strategy w needs to satisfy other constraints such as: i) the total amount of\ninvestment should be less than a maximum value that is known only to an agent k0 \u2208K (e.g., agent k0\nis from the funding department who knows how much funding is available), ii) the investment on each\nasset be nonnegative (known to all agents), and iii) tax requirements and tax deductions4 known to agents\nin a set H. We can then formulate the following constrained multi-objective optimization problem:\nmin\nw\n( X\nk\u2208U\nJu,k(w),\nX\nk\u2208S\nJs,k(w)\n)\n(109)\n4For example, suppose the \ufb01rst and second entries of the decision vector w denote the investments on charity assets. When the\ncharity investments exceed a certain amount, say b, there would be a tax deduction. We can represent this situation by writing\nhT w \u2265b, where h \u225c[1 1 0 \u00b7 \u00b7 \u00b7 0]T .\nOctober 31, 2018\nDRAFT\n25\ns.t.\n1T w \u2264b0\n(110)\nhT\nk w \u2265bk,\nk \u2208H\n(111)\nw \u2ab00\n(112)\nUsing the scalarization technique and barrier function method from Sec. I, we convert (109)\u2013(112) into\nthe following unconstrained optimization problem (for simplicity, we only consider \u03c01 = \u00b7 \u00b7 \u00b7 = \u03c0N = 1):\nJglob(w) =\nX\nk\u2208U\n\"\nJu,k(w) +\nM\nX\nm=1\n\u03c6(\u2212eT\nmw)\n#\n+\nX\nk\u2208S\n\"\nJs,k(w) +\nM\nX\nm=1\n\u03c6(\u2212eT\nmw)\n#\n+\nX\nk\u2208H\n\"\n\u03c6(bk \u2212hT\nk w) +\nM\nX\nm=1\n\u03c6(\u2212eT\nmw)\n#\n+\nX\nk\u2208K\n\"\n\u03c6(1T w \u2212b0) +\nM\nX\nm=1\n\u03c6(\u2212eT\nmw)\n#\nwhere \u03c6(\u00b7) is a barrier function to penalize the violation of the constraints \u2014 see [3] for an example, and\nthe vector em \u2208RM is a basis vector whose entries are all zero except for a value of one at the mth entry.\nThe term PM\nm=1 \u03c6(\u2212eT\nmw) is added to each cost function to enforce the nonnegativity constraint (112),\nwhich is assumed to be known to all agents. Note that there is a \u201cdivision of labor\u201d over the network:\nthe entire set of nodes is divided into four mutually exclusive subsets {1, . . . , N} = U \u222aS \u222aH \u222aK,\nand each subset collects one type of information related to the decision. Diffusion adaptation strategies\nallow the nodes to arrive at a Pareto-optimal decision in a distributed manner over the network, and each\nsubset of nodes in\ufb02uences the overall investment strategy.\nIn our simulation, we consider a randomly generated connected network topology. There are a total\nof N = 10 nodes in the network, and nodes are assumed connected when they are close enough\ngeographically. The cardinalities of the subsets U, S, H and K are set to be 3, 4, 2 and 1, respectively. The\nnodes are partitioned into these four subsets randomly. The dimension of the decision vector is M = 5.\nThe random vectors uk,i and sk,i are generated according to the Gaussian distributions N(1, IM) and\nN(0, IM), respectively. We set b0 = 5 and the parameters {hk, bk} for k \u2208H to\nhk1 = [1 2 \u00b7 \u00b7 \u00b7 5] ,\nbk1 = 2\n(113)\nhk2 = [5 4 \u00b7 \u00b7 \u00b7 1] ,\nbk2 = 3\n(114)\nOctober 31, 2018\nDRAFT\n26\n \n \n(a) Topology of the network.\n0\n200\n400\n600\n800\n1000\n\u221225\n\u221220\n\u221215\n\u221210\n\u22125\n0\nNumber of Iterations\nNetwork MSE (dB)\n \n \nATC\nCTA\nConsensus\nCentralized\n(b) Learning curve (\u00b5 = 10\u22122).\n10\n\u22123\n10\n\u22122\n10\n\u22121\n\u221235\n\u221230\n\u221225\n\u221220\n\u221215\n\u221210\n\u22125\n0\n5\nStep\u2212Size \u00b5\nNetwork MSE (dB)\n \n \nATC\nATC theory\nCTA\nCTA theory\nConsensus\nCentralized\n(c) MSE for different values of step-sizes.\n10\n\u22123\n10\n\u22122\n10\n\u22121\n\u221250\n\u221240\n\u221230\n\u221220\n\u221210\n0\nStep\u2212Size \u00b5\nError of Fixed Point (dB)\n \n \nATC\nATC theory\nCTA\nCTA theory\nConsensus\n(d) Error of \ufb01xed point for different values of step-\nsizes.\nFig. 4.\nSimulation results for collaborative decision making.\nwhere k1 and k2 are the indices of the two nodes in the subset H. Furthermore, we use the barrier\nfunction given by (15) in [3] in our simulation with t = 10, \u03c1 = 0.1 and \u03c4 = 0.1. We set the combination\ncoef\ufb01cients {alk} to the Metropolis rule (See Table III in [23]) for both ATC and CTA strategies. The\nweights {clk} are set to clk = 1 for l = k and zero otherwise, i.e., there is no exchange of gradient\ninformation among neighbors. According to Theorem 3, such a choice will always guarantee condition\n(80) so that the bias can be made arbitrarily small for small step-sizes. In our simulation, we do not\nassume the statistics of {uk,i} and {sk,i} are known to the nodes. The only information available is their\nrealizations and the algorithms have to learn the best decision vector w from them. Therefore, we use\nOctober 31, 2018\nDRAFT\n27\nthe following stochastic gradient vector5 at each node k:\n\\\n\u2207wJk(w) =\n\uf8f1\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f2\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f3\n\u2212uk,i + PM\nm=1 \u2207w\u03c6(\u2212eT\nmw)\nk \u2208U\n2sk,i + PM\nm=1 \u2207w\u03c6(\u2212eT\nmw)\nk \u2208S\n\u2207w\u03c6(bk \u2212hT\nk w) + PM\nm=1 \u2207w\u03c6(\u2212eT\nmw)\nk \u2208H\n\u2207w\u03c6(1T w \u2212b0) + PM\nm=1 \u2207w\u03c6(\u2212eT\nmw)\nk \u2208K\n(115)\nTo compare the performance with other algorithms, we also simulate the consensus-based approach from\n[16] with the same stochastic gradient6 as (115). The algorithm is listed below:\nwk,i =\nX\nl\u2208Nk\nalkwl,i\u22121 \u2212\u00b5\\\n\u2207wJk(wk,i\u22121)\n(116)\nFurthermore, we also simulate the conventional centralized approach to such optimization problem, which\ncollects data from all nodes and implements stochstic gradient descent at the central node:\nwi = wi\u22121 \u2212\u00b5 1\nN\nN\nX\nk=1\n\\\n\u2207wJk(wi\u22121)\n(117)\nwhere the factor of 1/N is used to make the convergence rate the same as the distributed algorithms.\nThe simulatin results are shown in Fig. 4(a)\u20134(d). Fig. 4(a) shows the network topology, and Fig.\n4(b) shows the learning curves of different algorithms. We see that ATC outperforms CTA and CTA\noutperforms consensus. To further compare the steady-state performance, we plot the steady-state MSE\nfor different values of step-sizes in Fig. 4(c). We also plot the theoretical curves from (105)\u2013(106) for\nATC and CTA algorithms. We observe that all algorithms approach the performance of the centralized\nsolution when the step-sizes are small. However, diffusion algorithms always outperform the consensus-\nbased strategy; the gap between ATC and consensus algorithm is about 8 dB when \u00b5 = 0.1. We also see\nthat the theoretical curves match the simulated ones well. Finally, we recall that Theorem 3 shows that\nthe error between the \ufb01xed point w\u221eand 1 \u2297wo can be made arbitrarily small for small step-sizes, and\nthe error \u2225w\u221e\u22121 \u2297wo\u22252 is on the order of O(\u00b52). To illustrate the result, we simulate the algorithms\nusing true gradients {\u2207wJk(w)} so that they converge to their \ufb01xed point w\u221e, and we get different\nvalues of w\u221efor different step-sizes. The theoretical values for ATC and CTA can be computed from\n(79). The results are shown in Fig. 4(d). We see that the theory matches simulation, and the power of\n5For nodes in H an K, the cost functions are known precisely, so their true gradients are used.\n6The original algorithm in [16] does not use stochastic gradients but the true gradients {\u2207wJk(w)}.\nOctober 31, 2018\nDRAFT\n28\nthe \ufb01xed point error per node7 decays at 20dB per decade, which is O(\u00b52) and is consistent with (81).\nNote that diffusion algorithms outperform the consensus. Also note from (79) and (96) that the bias and\nthe \ufb01xed point error have the same expression. Therefore, diffusion algorithms have smaller bias than\nconsensus (the gap in Fig. 4(d) is as large as 5dB between ATC and consensus).\nV. CONCLUSION\nThis paper generalized diffusion adaptation strategies to perform multi-objective optimization in a\ndistributed manner over a network of nodes. We use constant step-sizes to endow the network with\ncontinuous learning and adaptation abilities via local interactions. We analyzed the mean-square-error\nperformance of the diffusion strategy, and showed that the solution at each node gets arbitrarily close to\nthe same Pareto-optimal solution for small step-sizes.\nAPPENDIX A\nPROPERTIES OF THE OPERATORS\nProperties 1-3 are straightforward from the de\ufb01nitions of TA(\u00b7) and P[\u00b7]. We therefore omit the proof\nfor brevity, and start with property 4.\n(Property 4: Convexity)\nWe can express each N \u00d7 1 block vector x(k) in the form x(k) = col{x(k)\n1 , . . . , x(k)\nN } for k = 1, . . . , N.\nThen, the convex combination of x(1), . . . , x(N) can be expressed as\nK\nX\nk=1\nal x(k) = col\n( K\nX\nk=1\nalx(k)\n1 , . . . ,\nK\nX\nk=1\nalx(k)\nN\n)\n(118)\nAccording to the de\ufb01nition of the operator P[\u00b7], and in view of the convexity of \u2225\u00b7 \u22252, we have\nP\n\" K\nX\nk=1\nal x(k)\n#\n= col\n(\r\r\r\nK\nX\nk=1\nalx(k)\n1\n\r\r\r\n2\n, . . . ,\n\r\r\r\nK\nX\nk=1\nalx(k)\nN\n\r\r\r\n2\n)\n\u2aafcol\n( K\nX\nk=1\nal\u2225x(k)\n1 \u22252, . . . ,\nK\nX\nk=1\nal\u2225x(k)\nN \u22252\n)\n=\nK\nX\nk=1\nal P[x(k)]\n(119)\n7The power of the \ufb01xed point error per node is de\ufb01ned as\n1\nN \u2225w\u221e\u22121 \u2297wo\u22252 =\n1\nN\nPN\nk=1 \u2225wk,\u221e\u2212wo\u22252.\nOctober 31, 2018\nDRAFT\n29\n(Property 5: Additivity)\nBy the de\ufb01nition of P[\u00b7] and the assumption that ExT\nk yk = 0 for each k = 1, . . . , N, we obtain\nEP[x + y] = col{E\u2225x1 + y1\u22252, . . . , E\u2225xN + yN\u22252}\n= col{E\u2225x1\u22252 + E\u2225y1\u22252, . . . , E\u2225xN\u22252 + E\u2225yN\u22252}\n= EP[x] + EP[y]\n(120)\n(Property 6: Variance Relations)\nWe \ufb01rst prove (31). From the de\ufb01nition of TA(\u00b7) in (18) and the de\ufb01nition of P[\u00b7] in (20), we express\nP[TA(x)] = col\n(\r\r\r\nN\nX\nl=1\nal1xl\n\r\r\r\n2\n, . . . ,\n\r\r\r\nN\nX\nl=1\nalNxl\n\r\r\r\n2\n)\n(121)\nSince \u2225\u00b7\u22252 is a convex function and each sum inside the squared norm operator is a convex combination\nof x1, . . . , xN (AT is right stochastic), by Jensen\u2019s inequality [2, p.77], we have\nP[TA(x)] \u2aafcol\n( N\nX\nl=1\nal1\u2225xl\u22252, . . . ,\nN\nX\nl=1\nalN\u2225xl\u22252\n)\n= AT col{\u2225x1\u22252, . . . , \u2225xN\u22252}\n= AT P[x]\n(122)\nNext, we proceed to prove (32). We need to call upon the following useful lemmas from [4, p.24], and\nLemmas 1\u20132 in [21], respectively.\nLemma 4 (Mean-Value Theorem). For any twice-differentiable function f(\u00b7), it holds that\n\u2207f(y) = \u2207f(x) +\n\u0014Z 1\n0\n\u22072f\n\u0010\nx + t(y \u2212x)\n\u0011\ndt\n\u0015\n(y \u2212x)\n(123)\nwhere \u22072f(\u00b7) denotes the Hessian of f(\u00b7), and is a symmetric matrix.\n\u25a0\nLemma 5 (Bounds on the Integral of Hessian). Under Assumption 1, the following bounds hold for any\nvectors x and y:\n\u03bbl,minIM \u2264\nZ 1\n0\n\u22072\nwJl(x + ty)dt \u2264\u03bbl,maxIM\n(124)\n\r\r\r\r\rI \u2212\u00b5k\nN\nX\nl=1\nclk\n\u0014Z 1\n0\n\u22072\nwJl(x + ty)dt\n\u0015\r\r\r\r\r \u2264\u03b3k\n(125)\nwhere \u2225\u00b7 \u2225denotes the 2\u2212induced norm, and \u03b3k, \u03c3k,min and \u03c3k,max were de\ufb01ned in (34)\u2013(35).\n\u25a0\nOctober 31, 2018\nDRAFT\n30\nBy the de\ufb01nition of the operator TG(\u00b7) in (19) and the expression (123), we express TG(x) \u2212TG(y) as\nTG(x) \u2212TG(y) =\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0\n\"\nIM \u2212\u00b51\nN\nX\nl=1\ncl1\nZ 1\n0\n\u22072\nwJl(y1+t(x1\u2212y1))dt\n#\n(x1\u2212y1)\n...\n\"\nIM \u2212\u00b5N\nN\nX\nl=1\nclN\nZ 1\n0\n\u22072\nwJl(yN +t(xN \u2212yN))dt\n#\n(xN \u2212yN)\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb\n(126)\nTherefore, using (125) and the de\ufb01nition of P[\u00b7] in (20), we obtain\nP[TG(x) \u2212TG(y)] \u2aafcol\n\b\n\u03b32\n1 \u00b7 \u2225x1 \u2212y1\u22252, . . . , \u03b32\nN \u00b7 \u2225xN \u2212yN\u22252\t\n= \u03932P[x \u2212y]\n(127)\n(Property 7: Block Maximum Norm)\nAccording to the de\ufb01nition of P[\u00b7] and the de\ufb01nition of block maximum norm [21], we have\n\u2225P[x]\u2225\u221e=\n\r\r\rcol{\u2225x1\u22252, . . . , \u2225xN\u22252}\n\r\r\r\n\u221e\n= max\n1\u2264k\u2264N \u2225xk\u22252\n=\n\u0010\nmax\n1\u2264k\u2264N \u2225xk\u2225\n\u00112\n= \u2225x\u22252\nb,\u221e\n(128)\n(Property 8: Preservation of Inequality)\nTo prove Fx \u2aafFy, it suf\ufb01ces to prove 0 \u2aafF(y \u2212x). Since x \u2aafy, we have 0 \u2aafy \u2212x, i.e., all entries\nof the vector y \u2212x are nonnegative. Furthermore, since all entries of the matrix F are nonnegative, the\nentries of the vector F(y \u2212x) are all nonnegative, which means 0 \u2aafF(y \u2212x).\nAPPENDIX B\nBIAS AT SMALL STEP-SIZES\nIt suf\ufb01ces to show that\nlim\n\u00b5max\u21920\n\u22251 \u2297wo \u2212w\u221e\u2225\n\u00b5max\n= \u03be\n(129)\nwhere \u03be is a constant independent of \u00b5max. It is known that any matrix is similar to a Jordan canonical\nform [36]. Hence, there exists an invertible matrix Y such that AT\n2 AT\n1 = Y JY \u22121, where J is the Jordan\ncanonical form of the matrix AT\n2 AT\n1 , and the columns of the matrix Y are the corresponding right\nprincipal vectors of various degrees [36, pp.82\u201388]; the right principal vector of degree one is the right\nOctober 31, 2018\nDRAFT\n31\neigenvector. Obviously, the matrices J and Y are independent of \u00b5max. Using the Kronecker product\nproperty [36, p.140]: (A \u2297B)(C \u2297D) = AC \u2297BD, we obtain\nAT\n2 AT\n1 = AT\n2 AT\n1 \u2297IM\n= (Y \u2297IM)(J \u2297IM)(Y \u22121 \u2297IM)\n(130)\nDenote \u00b5k = \u03b2k\u00b5max, where \u03b2k is some positive scalar such that 0 < \u03b2k \u22641. Substituting (130) into\n(79), we obtain\n1 \u2297wo \u2212w\u221e=\n\u0002\nIMN \u2212AT\n2 AT\n1 + AT\n2 MR\u221eAT\n1\n\u0003\u22121 AT\n2 MCT go\n= (Y \u2297IM) [IMN \u2212J \u2297IM + \u00b5maxE]\u22121 (Y \u22121 \u2297IM)AT\n2 MCT go\n(131)\nwhere\nE = (Y \u22121 \u2297IM)AT\n2 M0R\u221eAT\n1 (Y \u2297IM)\n(132)\nM0 \u225cM/\u00b5max = diag{\u03b21, . . . , \u03b2N}\n|\n{z\n}\n\u225c\u21260\n\u2297IM\n(133)\nBy (8), the matrix AT\n2 AT\n1 is right-stochastic, and since AT\n2 AT\n1 is regular, it will have an eigenvalue of\none that has multiplicity one and is strictly greater than all other eigenvalues [37]. Furthermore, the\ncorresponding left and right eigenvectors are \u03b8T and 1, with \u03b8T \u227b0 (all entries of the row vector \u03b8T are\nreal positive numbers). For this reason, we can partition J, Y \u22121 and Y in the following block forms:\nJ = diag{1, J0},\nY \u22121 = col\n\u001a \u03b8T\n\u03b8T 1, YR\n\u001b\n,\nY = [1 YL]\n(134)\nwhere J0 is an (N \u22121)\u00d7(N \u22121) matrix that contains the Jordan blocks of eigenvalues strictly within unit\ncircle, i.e., \u03c1(J0) < 1. The \ufb01rst row of the matrix Y \u22121 in (134) is normalized by \u03b8T 1 so that Y \u22121Y = I.\n(Note that Y \u22121Y = I requires the product of the \ufb01rst row of Y \u22121 and the \ufb01rst column of Y to be one:\n\u03b8T\n\u03b8T 11 = 1.) Substituting these partitionings into (132), we can express E as\nE =\n\uf8ee\n\uf8ef\uf8f0\nE11\nE12\nE21\nE22\n\uf8f9\n\uf8fa\uf8fb\n(135)\nwhere\nE11 \u225c\n\u0010 \u03b8T\n\u03b8T 1 \u2297IM\n\u0011\nAT\n2 M0R\u221eAT\n1 (1 \u2297IM)\n(136)\nE12 \u225c\n\u0010 \u03b8T\n\u03b8T 1 \u2297IM\n\u0011\nAT\n2 M0R\u221eAT\n1 (YL \u2297IM)\n(137)\nE21 \u225c(YR \u2297IM)AT\n2 M0R\u221eAT\n1 (1 \u2297IM)\n(138)\nOctober 31, 2018\nDRAFT\n32\nE22 \u225c(YR \u2297IM)AT\n2 M0R\u221eAT\n1 (YL \u2297IM)\n(139)\nObserve that the matrices E11, E12, E21 and E22 are independent of \u00b5max. Substituting (134) and (135)\ninto (131), we obtain\n1 \u2297wo\u2212w\u221e= (Y \u2297IM)\n\uf8ee\n\uf8ef\uf8f0\n\u00b5maxE11\n\u00b5maxE12\n\u00b5maxE21\nI\u2212J0 \u2297IM +\u00b5maxE22\n\uf8f9\n\uf8fa\uf8fb\n\u22121 \uf8ee\n\uf8ef\uf8f0\n1\n\u03b8T 1(\u03b8T \u2297IM)AT\n2 MCT go\n(YR \u2297IM)AT\n2 MCT go\n\uf8f9\n\uf8fa\uf8fb(140)\nLet us denote\n\uf8ee\n\uf8ef\uf8f0\nG11\nG12\nG21\nG22\n\uf8f9\n\uf8fa\uf8fb\u225c\n\uf8ee\n\uf8ef\uf8f0\n\u00b5maxE11\n\u00b5maxE12\n\u00b5maxE21 I\u2212J0 \u2297IM +\u00b5maxE22\n\uf8f9\n\uf8fa\uf8fb\n\u22121\n(141)\nFurthermore, recalling that wo is the minimizer of the global cost function (4), we have\nN\nX\nl=1\n\u2207wJl(wo) = 0\n\u21d4\n(1T \u2297IM) go = 0\n(142)\nwhich, together with condition (80), implies that\n(\u03b8T \u2297IM)AT\n2 MCT go = (\u03b8T AT\n2 \u2126CT \u2297IM)go\n= c0(1T \u2297IM)go\n= 0\n(143)\nwhere we also used the facts that AT\n2 = AT\n2 \u2297IM, CT = CT \u2297IM, M = \u2126\u2297IM and the Kronecker\nproduct property: (A \u2297B)(C \u2297D). Substituting (141) and (143) into (140) and using (133) lead to\n1 \u2297wo \u2212w\u221e= \u00b5max \u00b7 (Y \u2297IM)\n\uf8ee\n\uf8ef\uf8f0\nG12\nG22\n\uf8f9\n\uf8fa\uf8fb(YRAT\n2 \u21260CT \u2297IM)go\n(144)\nTo proceed with analysis, we need to evaluate G12 and G22. We call upon the relation from [36, pp.48]:\n\uf8ee\n\uf8ef\uf8f0\nP\nQ\nU\nV\n\uf8f9\n\uf8fa\uf8fb\n\u22121\n=\n\uf8ee\n\uf8ef\uf8f0\nP \u22121 + P \u22121QSUP \u22121\n\u2212P \u22121QS\n\u2212SUP \u22121\nS\n\uf8f9\n\uf8fa\uf8fb\n(145)\nwhere S = (V \u2212UP \u22121Q)\u22121. To apply the above relation to (141), we \ufb01rst need to verify that E11 is\ninvertible. By de\ufb01nition (136),\nE11 =\n\u0010 \u03b8T\n\u03b8T 1AT\n2 \u21260 \u2297IM\n\u0011\nR\u221e(AT\n1 1 \u2297IM)\n= (zT \u2297IM)R\u221e(1 \u2297IM)\nOctober 31, 2018\nDRAFT\n33\n=\nN\nX\nk=1\nzk\nN\nX\nl=1\nclkHlk,\u221e\n(146)\nwhere zk denotes the kth entry of the vector z \u225c\u21260A2\u03b8/\u03b8T 1 (note that all entries of z are non-negative,\ni.e., zk \u22650). Recall from (73) that Hlk,\u221eis a symmetric positive semi-de\ufb01nite matrix. Moreover, since\nzk and clk are nonnegative, we can conclude from (146) that E11 is a symmetric positive semi-de\ufb01nite\nmatrix. Next, we show that E11 is actually strictly positive de\ufb01nite. Applying (124) to the expression of\nHlk,\u221ein (73), we obtain Hlk,\u221e\u2265\u03bbl,minIM. Substituting into (146) gives:\nE11 \u2265\n\" N\nX\nk=1\nzk\nN\nX\nl=1\nclk\u03bbl,min\n#\n\u00b7 IM\n\u2265\n N\nX\nk=1\nzk\n!\nmin\n1\u2264k\u2264N\n( N\nX\nl=1\nclk\u03bbl,min\n)\n\u00b7 IM\n= 1T \u21260A2\u03b8\n\u03b8T 1\n\u00b7 min\n1\u2264k\u2264N\n( N\nX\nl=1\nclk\u03bbl,min\n)\n\u00b7 IM\n(147)\nNoting that the matrices \u21260 and A0 have nonnegative entries with some entries being positive, and that\nall entries of \u03b8 are positive, we have (1T \u21260A2\u03b8)/(\u03b8T 1) > 0. Furthermore, by Assumption 1, we know\nPN\nl=1 clk\u03bbl,min > 0 for each k = 1, . . . , N. Therefore, we conclude that E11 > 0 and is invertible.\nApplying (145) to (141), we get\nG12 = \u2212E\u22121\n11 E12G22\n(148)\nG22 =\n\u0002\nI\u2212J0 \u2297IM +\u00b5max(E22\u2212E21E\u22121\n11 ET\n12)\n\u0003\u22121\n(149)\nSubstituting (149) into (144) leads to\n1 \u2297wo \u2212w\u221e= \u00b5max \u00b7 (Y \u2297IM)\n\uf8ee\n\uf8ef\uf8f0\n\u2212E\u22121\n11 E12\nI\n\uf8f9\n\uf8fa\uf8fbG22(YRAT\n2 \u21260CT \u2297IM)go\n(150)\nSubstituting expression (150) into the left-hand side of (129), we get\nlim\n\u00b5max\u21920\n\u22251 \u2297wo \u2212w\u221e\u2225\n\u00b5max\n= lim\n\u00b5max\u21920\n\r\r\r\r\r\r\n(Y \u2297IM)\n\uf8ee\n\uf8ef\uf8f0\n\u2212E\u22121\n11 E12\nI\n\uf8f9\n\uf8fa\uf8fbG22(YRAT\n2 \u21260CT \u2297IM)go\n\r\r\r\r\r\r\n(151)\nObserve that the only term on the right-hand side of (151) that depends on \u00b5max is G22. From its\nexpression (149), we observe that, as \u00b5max \u21920, the matrix G22 tends to (I \u2212J0 \u2297IM)\u22121, which is\nindependent of \u00b5max. Therefore, the limit on the right-hand side of (151) is independent of \u00b5max.\nOctober 31, 2018\nDRAFT\n34\nREFERENCES\n[1] J. Chen and A. H. Sayed, \u201cDistributed Pareto-optimal solutions via diffusion adaptation,\u201d in Proc. IEEE Workshop on\nStatistical Signal Process. (SSP), Ann Arbor, MI, Aug. 2012, pp. 1\u20134.\n[2] S. P. Boyd and L. Vandenberghe, Convex Optimization, Cambridge University Press, 2004.\n[3] Z. J. Tow\ufb01c, J. Chen, and A. H. Sayed, \u201cDistributed throughput optimization over P2P mesh networks using diffusion\nadaptation,\u201d in Proc. IEEE International Conf. Commun. (ICC), Ottawa, Canada, June 2012, pp. 1\u20135.\n[4] B. Polyak, Introduction to Optimization, Optimization Software, NY, 1987.\n[5] D.P. Bertsekas and J.N. Tsitsiklis, Parallel and Distributed Computation: Numerical Methods, Athena Scienti\ufb01c, Belmont,\n1997.\n[6] D. P. Bertsekas, \u201cA new class of incremental gradient methods for least squares problems,\u201d SIAM J. Optim., vol. 7, no.\n4, pp. 913\u2013926, 1997.\n[7] A. Nedic and D. P. Bertsekas, \u201cIncremental subgradient methods for nondifferentiable optimization,\u201d SIAM J. Optim., vol.\n12, no. 1, pp. 109\u2013138, 2001.\n[8] A. Nedic and D. P. Bertsekas,\n\u201cConvergence rate of incremental subgradient algorithms,\u201d\nStochastic Optimization:\nAlgorithms and Applications, S. Uryasev and P. M. Pardalos, Eds., pp. 263\u2013304, 2000.\n[9] B. T. Polyak and Y. Z. Tsypkin, \u201cPseudogradient adaptation and training algorithms,\u201d Automation and Remote Control,\nvol. 12, pp. 83\u201394, 1973.\n[10] M. G. Rabbat and R. D. Nowak, \u201cQuantized incremental algorithms for distributed optimization,\u201d IEEE J. Sel. Areas\nCommun., vol. 23, no. 4, pp. 798\u2013808, 2005.\n[11] C. G. Lopes and A. H. Sayed, \u201cIncremental adaptive strategies over distributed networks,\u201d IEEE Trans. Signal Process.,\nvol. 55, no. 8, pp. 4064\u20134077, Aug. 2007.\n[12] L. Li and J. A. Chambers, \u201cA new incremental af\ufb01ne projection-based adaptive algorithm for distributed networks,\u201d Signal\nProcessing, vol. 88, no. 10, pp. 2599\u20132603, Oct. 2008.\n[13] R. M. Karp, \u201cReducibility among combinational problems,\u201d Complexity of Computer Computations, R. E. Miller and J.\nW. Thatcher, Eds., pp. 85\u2013104, 1972.\n[14] J. N. Tsitsiklis, D. P. Bertsekas, and M. Athans, \u201cDistributed asynchronous deterministic and stochastic gradient optimization\nalgorithms,\u201d IEEE Trans. Autom. Control, vol. 31, no. 9, pp. 803\u2013812, 1986.\n[15] S. Barbarossa and G. Scutari, \u201cBio-inspired sensor network design,\u201d IEEE Signal Process. Mag., vol. 24, no. 3, pp. 26\u201335,\n2007.\n[16] A. Nedic and A. Ozdaglar, \u201cDistributed subgradient methods for multi-agent optimization,\u201d IEEE Trans. Autom. Control,\nvol. 54, no. 1, pp. 48\u201361, 2009.\n[17] S. Kar and J. M. F. Moura, \u201cConvergence rate analysis of distributed gossip (linear parameter) estimation: Fundamental\nlimits and tradeoffs,\u201d IEEE J. Sel. Topics. Signal Process., vol. 5, no. 4, pp. 674\u2013690, Aug. 2011.\n[18] A. G. Dimakis, S. Kar, J. M. F. Moura, M. G. Rabbat, and A. Scaglione,\n\u201cGossip algorithms for distributed signal\nprocessing,\u201d Proc. IEEE, vol. 98, no. 11, pp. 1847\u20131864, 2010.\n[19] K. Srivastava and A. Nedic, \u201cDistributed asynchronous constrained stochastic optimization,\u201d IEEE J. Sel. Topics Signal\nProcess., vol. 5, no. 4, pp. 772\u2013790, Aug. 2011.\n[20] U. A. Khan and A. Jadbabaie, \u201cNetworked estimation under information constraints,\u201d Arxiv preprint arXiv:1111.4580,\nNov. 2011.\nOctober 31, 2018\nDRAFT\n35\n[21] J. Chen and A. H. Sayed, \u201cDiffusion adaptation strategies for distributed optimization and learning over networks,\u201d IEEE\nTrans. Signal Process., vol. 60, no. 8, pp. 4289\u20134305, Aug. 2012.\n[22] C. G. Lopes and A. H. Sayed,\n\u201cDiffusion least-mean squares over adaptive networks: Formulation and performance\nanalysis,\u201d IEEE Trans. Signal Process., vol. 56, no. 7, pp. 3122\u20133136, July 2008.\n[23] F. S. Cattivelli and A. H. Sayed, \u201cDiffusion LMS strategies for distributed estimation,\u201d IEEE Trans. Signal Process., vol.\n58, no. 3, pp. 1035\u20131048, March 2010.\n[24] S. S. Ram, A. Nedic, and V. V. Veeravalli,\n\u201cDistributed stochastic subgradient projection algorithms for convex\noptimization,\u201d J. Optim. Theory Appl., vol. 147, no. 3, pp. 516\u2013545, 2010.\n[25] S.-Y. Tu and A. H. Sayed, \u201cMobile adaptive networks,\u201d IEEE J. Sel. Topics. Signal Process., vol. 5, no. 4, pp. 649\u2013664,\nAug. 2011.\n[26] P. Di Lorenzo and S. Barbarossa, \u201cA bio-inspired swarming algorithm for decentralized access in cognitive radio,\u201d IEEE\nTrans. Signal Process., vol. 59, no. 12, pp. 6160\u20136174, Dec. 2011.\n[27] S. Chouvardas, K. Slavakis, and S. Theodoridis, \u201cAdaptive robust distributed learning in diffusion sensor networks,\u201d IEEE\nTrans. Signal Process., vol. 59, no. 10, pp. 4692\u20134707, Oct. 2011.\n[28] S. Theodoridis, K. Slavakis, and I. Yamada, \u201cAdaptive learning in a world of projections,\u201d IEEE Signal Process. Mag.,\nvol. 28, no. 1, pp. 97\u2013123, Jan. 2011.\n[29] E. Kreyszig, Introductory Functional Analysis with Applications, Wiley, NY, 1989.\n[30] N. Takahashi and I. Yamada,\n\u201cLink probability control for probabilistic diffusion least-mean squares over resource-\nconstrained networks,\u201d in IEEE ICASSP, Dallas, TX, Mar. 2010, pp. 3518\u20133521.\n[31] A. H. Sayed, Adaptive Filters, Wiley, NJ, 2008.\n[32] H. Markowitz, \u201cPortfolio selection,\u201d The Journal of Finance, vol. 7, no. 1, pp. 77\u201391, 1952.\n[33] M. Rubinstein, \u201cMarkowitz\u2019s \u201cPortfolio Selection\u201d: A Fifty-Year Retrospective,\u201d The Journal of Finance, vol. 57, no. 3,\npp. 1041\u20131045, Jun. 2002.\n[34] A. D. Fitt, \u201cMarkowitz portfolio theory for soccer spread betting,\u201d IMA Journal of Management Mathematics, vol. 20, no.\n2, pp. 167\u2013184, Apr. 2009.\n[35] E. J. Elton, M. J. Gruber, and C. R. Blake, \u201cApplications of Markowitz portfolio theory to pension fund design,\u201d in\nHandbook of Portfolio Construction, J. B. Guerard, Ed., pp. 419\u2013438. Springer, US, 2010.\n[36] A. J. Laub, Matrix Analysis for Scientists and Engineers, SIAM, PA, 2005.\n[37] R. A. Horn and C. R. Johnson, Matrix Analysis, Cambridge University Press, 1990.\nOctober 31, 2018\nDRAFT\n",
        "sentence": " There are several distributed strategies that can be used for this purpose, such as consensus [10]\u2013[14] and diffusion strategies [15]\u2013[18]. We choose the adapt-then-combine (ATC) diffusion variant for distributed optimization over networks [17], [18], [45]. APPENDIX B PROOF OF THEOREM 3 We follow an argument similar to [18], [53].",
        "context": "October 31, 2018\nDRAFT\n35\n[21] J. Chen and A. H. Sayed, \u201cDiffusion adaptation strategies for distributed optimization and learning over networks,\u201d IEEE\nTrans. Signal Process., vol. 60, no. 8, pp. 4289\u20134305, Aug. 2012.\n[22] C. G. Lopes and A. H. Sayed,\n1\nDistributed Pareto Optimization via Diffusion\nStrategies\nJianshu Chen, Student Member, IEEE, and Ali H. Sayed, Fellow, IEEE\nAbstract\nWe consider solving multi-objective optimization problems in a distributed manner by a network\nand the network therefore needs to seek Pareto optimal solutions. We develop a distributed solution that\nrelies on a general class of adaptive diffusion strategies. We show how the diffusion process can be"
    },
    {
        "title": "Diffusion adaptation over networks",
        "author": [
            "A.H. Sayed"
        ],
        "venue": "Academic Press Library in Signal Processing, R. Chellapa and S. Theodoridis, Eds. Elsevier, 2014, vol. 3, pp. 323\u2013454. Also available as arXiv:1205.4220v1, May 2012.",
        "citeRegEx": "19",
        "shortCiteRegEx": null,
        "year": 2014,
        "abstract": "Adaptive networks are well-suited to perform decentralized information\nprocessing and optimization tasks and to model various types of self-organized\nand complex behavior encountered in nature. Adaptive networks consist of a\ncollection of agents with processing and learning abilities. The agents are\nlinked together through a connection topology, and they cooperate with each\nother through local interactions to solve distributed optimization, estimation,\nand inference problems in real-time. The continuous diffusion of information\nacross the network enables agents to adapt their performance in relation to\nstreaming data and network conditions; it also results in improved adaptation\nand learning performance relative to non-cooperative agents. This article\nprovides an overview of diffusion strategies for adaptation and learning over\nnetworks. The article is divided into several sections: 1. Motivation; 2.\nMean-Square-Error Estimation; 3. Distributed Optimization via Diffusion\nStrategies; 4. Adaptive Diffusion Strategies; 5. Performance of\nSteepest-Descent Diffusion Strategies; 6. Performance of Adaptive Diffusion\nStrategies; 7. Comparing the Performance of Cooperative Strategies; 8.\nSelecting the Combination Weights; 9. Diffusion with Noisy Information\nExchanges; 10. Extensions and Further Considerations; Appendix A: Properties of\nKronecker Products; Appendix B: Graph Laplacian and Network Connectivity;\nAppendix C: Stochastic Matrices; Appendix D: Block Maximum Norm; Appendix E:\nComparison with Consensus Strategies; References.",
        "full_text": "arXiv:1205.4220v2  [cs.MA]  5 May 2013\nDIFFUSION ADAPTATION OVER NETWORKS\u2217\nAli H. Sayed\nElectrical Engineering Department\nUniversity of California at Los Angeles\nAdaptive networks are well-suited to perform decentralized information processing and optimization tasks\nand to model various types of self-organized and complex behavior encountered in nature. Adaptive networks\nconsist of a collection of agents with processing and learning abilities. The agents are linked together through\na connection topology, and they cooperate with each other through local interactions to solve distributed\noptimization, estimation, and inference problems in real-time. The continuous di\ufb00usion of information across\nthe network enables agents to adapt their performance in relation to streaming data and network conditions;\nit also results in improved adaptation and learning performance relative to non-cooperative agents. This\narticle provides an overview of di\ufb00usion strategies for adaptation and learning over networks. The article is\ndivided into several sections:\n1. Motivation.\n2. Mean-Square-Error Estimation.\n3. Distributed Optimization via Di\ufb00usion Strategies.\n4. Adaptive Di\ufb00usion Strategies.\n5. Performance of Steepest-Descent Di\ufb00usion Strategies.\n6. Performance of Adaptive Di\ufb00usion Strategies.\n7. Comparing the Performance of Cooperative Strategies.\n8. Selecting the Combination Weights.\n9. Di\ufb00usion with Noisy Information Exchanges.\n10. Extensions and Further Considerations.\n11. Appendix A: Properties of Kronecker Products.\n12. Appendix B: Graph Laplacian and Network Connectivity.\n13. Appendix C: Stochastic Matrices.\n14. Appendix D: Block Maximum Norm.\n15. Appendix E: Comparison with Consensus Strategies.\n16. References.\n\u2217The cite information for this article is as follows: A. H. Sayed, \u201cDi\ufb00usion adaptation over networks,\u201d in E-Reference\nSignal Processing, R. Chellapa and S. Theodoridis, editors, Elsevier, 2013.\nThe work was supported in part by\nNSF grants EECS-060126, EECS-0725441, CCF-0942936, and CCF-1011918. The author is with the Electrical Engineering\nDepartment, University of California, Los Angeles, CA 90095, USA. Email: sayed@ee.ucla.edu.\n1\n1\nMotivation\nConsider a collection of N agents interested in estimating the same parameter vector, wo, of size M \u00d71. The\nvector is the minimizer of some global cost function, denoted by Jglob(w), which the agents seek to optimize,\nsay,\nwo = argmin\nw\nJglob(w)\n(1)\nWe are interested in situations where the individual agents have access to partial information about the\nglobal cost function.\nIn this case, cooperation among the agents becomes bene\ufb01cial.\nFor example, by\ncooperating with their neighbors, and by having these neighbors cooperate with their neighbors, procedures\ncan be devised that would enable all agents in the network to converge towards the global optimum wo\nthrough local interactions. The objective of decentralized processing is to allow spatially distributed agents\nto achieve a global objective by relying solely on local information and on in-network processing. Through a\ncontinuous process of cooperation and information sharing with neighbors, agents in a network can be made\nto approach the global performance level despite the localized nature of their interactions.\n1.1\nNetworks and Neighborhoods\nIn this article we focus mainly on connected networks, although many of the results hold even if the network\ngraph is separated into disjoint subgraphs. In a connected network, if we pick any two arbitrary nodes,\nthen there will exist at least one path connecting them: the nodes may be connected directly by an edge\nif they are neighbors, or they may be connected by a path that passes through other intermediate nodes.\nFigure 1 provides a graphical representation of a connected network with N = 10 nodes. Nodes that are\nable to share information with each other are connected by edges. The sharing of information over these\nedges can be unidirectional or bi-directional. The neighborhood of any particular node is de\ufb01ned as the set\nof nodes that are connected to it by edges; we include in this set the node itself. The \ufb01gure illustrates the\nneighborhood of node 3, which consists of the following subset of nodes: N3 = {1, 2, 3, 5}. For each node,\nthe size of its neighborhood de\ufb01nes its degree. For example, node 3 in the \ufb01gure has degree |N3| = 4, while\nnode 8 has degree |N8| = 5. Nodes that are well connected have higher degrees. Note that we are denoting\nthe neighborhood of an arbitrary node k by Nk and its size by |Nk|. We shall also use the notation nk to\nrefer to |Nk|.\nThe neighborhood of any node k therefore consists of all nodes with which node k can exchange informa-\ntion. We assume a symmetric situation in relation to neighbors so that if node k is a neighbor of node \u2113, then\nnode \u2113is also a neighbor of node k. This does not necessarily mean that the \ufb02ow of information between\nthese two nodes is symmetrical. For instance, in future sections, we shall assign pairs of nonnegative weights\nto each edge connecting two neighboring nodes \u2014 see Fig. 2. In particular, we will assign the coe\ufb03cient a\u2113k\nto denote the weight used by node k to scale the data it receives from node \u2113; this scaling can be interpreted\nas a measure of trustworthiness or reliability that node k assigns to its interaction with node \u2113. Note that we\nare using two subscripts, \u2113k, with the \ufb01rst subscript denoting the source node (where information originates\nfrom) and the second subscript denoting the sink node (where information moves to) so that:\na\u2113k\n\u2261\na\u2113\u2192k\n(information \ufb02owing from node \u2113to node k)\n(2)\nIn this way, the alternative coe\ufb03cient ak\u2113will denote the weight used to scale the data sent from node k to\n\u2113:\nak\u2113\n\u2261\nak \u2192\u2113\n(information \ufb02owing from node k to node \u2113)\n(3)\nThe weights {ak\u2113, a\u2113k} can be di\ufb00erent, and one or both of them can be zero, so that the exchange of\ninformation over the edge connecting the neighboring nodes (k, \u2113) need not be symmetric. When one of the\n2\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nneighborhood\nof node 3\nFigure 1:\nA network consists of a collection of cooperating nodes.\nNodes that are linked by edges can share\ninformation.\nThe neighborhood of any particular node consists of all nodes that are connected to it by edges\n(including the node itself). The \ufb01gure illustrates the neighborhood of node 3, which consists of nodes {1, 2, 3, 5}.\nAccordingly, node 3 has degree 4, which is the size of its neighborhood.\nweights is zero, say, ak\u2113= 0, then this situation means that even though nodes (k, \u2113) are neighbors, node\n\u2113is either not receiving data from node k or the data emanating from node k is being annihilated before\nreaching node \u2113. Likewise, when akk > 0, then node k scales its own data, whereas akk = 0 corresponds\nto the situation when node k does not use its own data. Usually, in graphical representations like those in\nFig. 1, edges are drawn between neighboring nodes that can share information. And, it is understood that\nthe actual sharing of information is controlled by the values of the scaling weights that are assigned to the\nedge; these values can turn o\ufb00communication in one or both directions and they can also scale one direction\nmore heavily than the reverse direction, and so forth.\n1.2\nCooperation Among Agents\nNow, depending on the application under consideration, the solution vector wo from (1) may admit di\ufb00erent\ninterpretations. For example, the entries of wo may represent the location coordinates of a nutrition source\nthat the agents are trying to \ufb01nd, or the location of an accident involving a dangerous chemical leak.\nThe nodes may also be interested in locating a predator and tracking its movements over time. In these\nlocalization applications, the vector wo is usually two or three-dimensional. In other applications, the entries\nof wo may represent the parameters of some model that the network wishes to learn, such as identifying\nthe model parameters of a biological process or the occupied frequency bands in a shared communications\nmedium. There are also situations where di\ufb00erent agents in the network may be interested in estimating\ndi\ufb00erent entries of wo, or even di\ufb00erent parameter vectors wo altogether, say, {wo\nk} for node k, albeit with\nsome relation among the di\ufb00erent vectors so that cooperation among the nodes can still be rewarding. In\nthis article, however, we focus exclusively on the special (yet frequent and important) case where all agents\nare interested in estimating the same parameter vector wo.\nSince the agents have a common objective, it is natural to expect cooperation among them to be bene\ufb01cial\nin general.\nOne important question is therefore how to develop cooperation strategies that can lead to\nbetter performance than when each agent attempts to solve the optimization problem individually. Another\nimportant question is how to develop strategies that endow networks with the ability to adapt and learn in\n3\n\u2113\nk\nak\u2113\na\u2113k\nakk\na\u2113\u2113\nakk\nk\na\u21131k\na\u21132k\na\u21133k\nk\nakk\nak\u21132\nak\u21131\nak\u21133\nFigure 2: In the top part, and for emphasis purposes, we are representing the edge between nodes k and \u2113by two\nseparate directed links: one moving from k to \u2113and the other moving from \u2113to k. Two nonnegative weights are used\nto scale the sharing of information over these directed links. The scalar ak\u2113denotes the weight used to scale data\nsent from node k to \u2113, while a\u2113k denotes the weight used to scale data sent from node \u2113to k. The weights {ak,\u2113, a\u2113k}\ncan be di\ufb00erent, and one or both of them can be zero, so that the exchange of information over the edge connecting\nany two neighboring nodes need not be symmetric. The bottom part of the \ufb01gure illustrates directed links arriving\nto node k from its neighbors {\u21131, \u21132, \u21133, . . .} (left) and leaving from node k towards these same neighbors (right).\nreal-time in response to changes in the statistical properties of the data. This article provides an overview\nof results in the area of di\ufb00usion adaptation with illustrative examples. Di\ufb00usion strategies are powerful\nmethods that enable adaptive learning and cooperation over networks. There have been other useful works\nin the literature on the use of alternative consensus strategies to develop distributed optimization solutions\nover networks. Nevertheless, we explain in App. E why di\ufb00usion strategies outperform consensus strategies\nin terms of their mean-square-error stability and performance. For this reason, we focus in the body of the\nchapter on presenting the theoretical foundations for di\ufb00usion strategies and their performance.\n1.3\nNotation\nIn our treatment, we need to distinguish between random variables and deterministic quantities. For this\nreason, we use boldface letters to represent random variables and normal font to represent deterministic\n(non-random) quantities. For example, the boldface letter d denotes a random quantity, while the normal\nfont letter d denotes an observation or realization for it. We also need to distinguish between matrices and\nvectors. For this purpose, we use CAPITAL letters to refer to matrices and small letters to refer to both\nvectors and scalars; Greek letters always refer to scalars. For example, we write R to denote a covariance\nmatrix and w to denote a vector of parameters. We also write \u03c32\nv to refer to the variance of a random variable.\nTo distinguish between a vector d (small letter) and a scalar d (also a small letter), we use parentheses to\nindex scalar quantities and subscripts to index vector quantities. Thus, we write d(i) to refer to the value of\na scalar quantity d at time i, and di to refer to the value of a vector quantity d at time i. Thus, d(i) denotes\na scalar while di denotes a vector. All vectors in our presentation are column vectors, with the exception\nof the regression vector (denoted by the letter u), which will be taken to be a row vector for convenience\nof presentation. The symbol T denotes transposition, and the symbol \u2217denotes complex conjugation for\nscalars and complex-conjugate transposition for matrices. The notation col{a, b} denotes a column vector\n4\nwith entries a and b stacked on top of each other, and the notation diag{a, b} denotes a diagonal matrix\nwith entries a and b. Likewise, the notation vec(A) vectorizes its matrix argument and stacks the columns\nof A on top of each other. The notation \u2225x\u2225denotes the Euclidean norm of its vector argument, while\n\u2225x\u2225b,\u221edenotes the block maximum norm of a block vector (de\ufb01ned in App. D). Similarly, the notation\n\u2225x\u22252\n\u03a3 denotes the weighted square value, x\u2217\u03a3x. Moreover, \u2225A\u2225b,\u221edenotes the block maximum norm of a\nmatrix (also de\ufb01ned in App. D), and \u03c1(A) denotes the spectral radius of the matrix (i.e., the largest absolute\nmagnitude among its eigenvalues). Finally, IM denotes the identity matrix of size M \u00d7 M; sometimes, for\nsimplicity of notation, we drop the subscript M from IM when the size of the identity matrix is obvious from\nthe context. Table 1 provides a summary of the symbols used in the article for ease of reference.\nTable 1: Summary of notation conventions used in the article.\nd\nBoldface notation denotes random variables.\nd\nNormal font denotes realizations of random variables.\nA\nCapital letters denote matrices.\na\nSmall letters denote vectors or scalars.\n\u03b1\nGreek letters denote scalars.\nd(i)\nSmall letters with parenthesis denote scalars.\ndi\nSmall letters with subscripts denote vectors.\nT\nMatrix transposition.\n\u2217\nComplex conjugation for scalars and complex-conjugate transposition for matrices.\ncol{a, b}\nColumn vector with entries a and b.\ndiag{a, b}\nDiagonal matrix with entries a and b.\nvec(A)\nVectorizes matrix A and stacks its columns on top of each other.\n\u2225x\u2225\nEuclidean norm of its vector argument.\n\u2225x\u22252\n\u03a3\nWeighted square value x\u2217\u03a3x.\n\u2225x\u2225b,\u221e\nBlock maximum norm of a block vector \u2013 see App. D.\n\u2225A\u2225b,\u221e\nBlock maximum norm of a matrix \u2013 see App. D.\n\u2225A\u2225\n2\u2212induced norm of matrix A (its largest singular value).\n\u03c1(A)\nSpectral radius of its matrix argument.\nIM\nIdentity matrix of size M \u00d7 M; sometimes, we drop the subscript M.\n2\nMean-Square-Error Estimation\nReaders interested in the development of the distributed optimization strategies and their adaptive versions\ncan move directly to Sec. 3. The purpose of the current section is to motivate the virtues of distributed\nin-network processing, and to provide illustrative examples in the context of mean-square-error estimation.\nAdvanced readers can skip this section on a \ufb01rst reading.\nWe start our development by associating with each agent k an individual cost (or utility) function,\nJk(w). Although the algorithms presented in this article apply to more general situations, we nevertheless\nassume in our presentation that the cost functions Jk(w) are strictly convex so that each one of them has\na unique minimizer. We further assume that, for all costs Jk(w), the minimum occurs at the same value\nwo. Obviously, the choice of Jk(w) is limitless and is largely dependent on the application. It is su\ufb03cient\nfor our purposes to illustrate the main concepts underlying di\ufb00usion adaptation by focusing on the case of\nmean-square-error (MSE) or quadratic cost functions. In the sequel, we provide several examples to illustrate\nhow such quadratic cost functions arise in applications and how cooperative processing over networks can be\nbene\ufb01cial. At the same time, we note that most of the arguments in this article can be extended beyond MSE\noptimization to more general cost functions and to situations where the minimizers of the individual costs\nJk(w) need not agree with each other \u2014 as already shown in [1\u20133]; see also Sec. 10.4 for a brief summary.\nIn non-cooperative solutions, each agent would operate individually on its own cost function Jk(w) and\n5\noptimize it to determines wo, without any interaction with the other nodes. However, the analysis and\nderivations in future sections will reveal that nodes can bene\ufb01t from cooperation between them in terms of\nbetter performance (such as converging faster to wo or tracking a changing wo more e\ufb00ectively) \u2014 see, e.g.,\nTheorems 6.3\u20136.5 and Sec. 7.3.\n2.1\nApplication: Autoregressive Modeling\nOur \ufb01rst example relates to identifying the parameters of an auto-regressive (AR) model from noisy data.\nThus, consider a situation where agents are spread over some geographical region and each agent k is\nobserving realizations {dk(i)} of an AR zero-mean random process {dk(i)}, which satis\ufb01es a model of the\nform:\ndk(i) =\nM\nX\nm=1\n\u03b2mdk(i \u2212m) + vk(i)\n(4)\nThe scalars {\u03b2m} represent the model parameters that the agents wish to identify, and vk(i) represents an\nadditive zero-mean white noise process with power:\n\u03c32\nv,k\n\u2206= E |vk(i)|2\n(5)\nIt is customary to assume that the noise process is temporally white and spatially independent so that noise\nterms across di\ufb00erent nodes are independent of each other and, at the same node, successive noise samples\nare also independent of each other with a time-independent variance \u03c32\nv,k:\n\u001a\nEvk(i)v\u2217\nk(j) = 0, for all i \u0338= j (temporal whiteness)\nEvk(i)v\u2217\nm(j) = 0, for all i, j whenever k \u0338= m (spatial whiteness)\n(6)\nThe noise process vk(i) is further assumed to be independent of past signals {d\u2113(i \u2212m), m \u22651} across all\nnodes \u2113. Observe that we are allowing the noise power pro\ufb01le, \u03c32\nv,k, to vary with k. In this way, the quality\nof the measurements is allowed to vary across the network with some nodes collecting noisier data than other\nnodes. Figure 3 illustrates an example of a network consisting of N = 10 nodes spread over a region in space.\nThe \ufb01gure shows the neighborhood of node 2, which consists of nodes {1, 2, 3}.\nLinear Model\nTo illustrate the di\ufb00erence between cooperative and non-cooperative estimation strategies, let us \ufb01rst explain\nthat the data can be represented in terms of a linear model. To do so, we collect the model parameters {\u03b2m}\ninto an M \u00d7 1 column vector wo:\nwo\n\u2206= col {\u03b21, \u03b22, . . . , \u03b2M}\n(7)\nand the past data into a 1 \u00d7 M row regression vector uk,i:\nuk,i\n\u2206=\n\u0002 dk(i \u22121)\ndk(i \u22122)\n. . .\ndk(i \u2212M) \u0003\n(8)\nThen, we can rewrite the measurement equation (4) at each node k in the equivalent linear model form:\ndk(i) = uk,iwo + vk(i)\n(9)\nLinear relations of the form (9) are common in applications and arise in many other contexts (as further\nillustrated by the next three examples in this section).\nWe assume the stochastic processes {dk(i), uk,i} in (9) have zero means and are jointly wide-sense sta-\ntionary. We denote their second-order moments by:\n\u03c32\nd,k\n\u2206=\nE|dk(i)|2\n(scalar)\n(10)\nRu,k\n\u2206=\nEu\u2217\nk,iuk,i\n(M \u00d7 M)\n(11)\nrdu,k\n\u2206=\nEdk(i)u\u2217\nk,i\n(M \u00d7 1)\n(12)\n6\nx-coordinate\ny-coordinate\namplitude\nd1(i)\nd2(i)\nd3(i)\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nFigure 3: A collection of nodes, spread over a geographic region, observes realizations of an AR random process\nand cooperates to estimate the underlying model parameters {\u03b2m} in the presence of measurement noise. The noise\npower pro\ufb01le can vary over space.\nAs was the case with the noise power pro\ufb01le, we are allowing the moments {\u03c32\nd,k, Ru,k, rdu,k} to depend\non the node index k so that these moments can vary with the spatial dimension as well. The covariance\nmatrix Ru,k is, by de\ufb01nition, always non-negative de\ufb01nite. However, for convenience of presentation, we\nshall assume that Ru,k is actually positive-de\ufb01nite (and, hence, invertible):\nRu,k > 0\n(13)\nNon-Cooperative Mean-Square-Error Solution\nOne immediate result that follows from the linear model (9) is that the unknown parameter vector wo can\nbe recovered exactly by each individual node from knowledge of the local moments {rdu,k, Ru,k} alone. To\nsee this, note that if we multiply both sides of (9) by u\u2217\nk,i and take expectations we obtain\nEu\u2217\nk,idk(i)\n|\n{z\n}\nrdu,k\n=\n\u0000Eu\u2217\nk,iuk,i\n\u0001\n|\n{z\n}\nRu,k\nwo + Eu\u2217\nk,ivk(i)\n|\n{z\n}\n=0\n(14)\nso that\nrdu,k = Ru,k wo\n\u21d0\u21d2\nwo = R\u22121\nu,k rdu,k\n(15)\nIt is seen from (15) that wo is the solution to a linear system of equations and that this solution can be\ncomputed by every node directly from its moments {Ru,k, rdu,k}. It is useful to re-interpret construction\n7\n(15) as the solution to a minimum mean-square-error (MMSE) estimation problem [4, 5]. Indeed, it can\nbe veri\ufb01ed that the quantity R\u22121\nu,k rdu,k that appears in (15) is the unique solution to the following MMSE\nproblem:\nmin\nw\nE |dk(i) \u2212uk,iw|2\n(16)\nTo verify this claim, we denote the cost function that appears in (16) by\nJk(w)\n\u2206=\nE|dk(i) \u2212uk,iw|2\n(17)\nand expand it to \ufb01nd that\nJk(w)\n=\n\u03c32\nd,k \u2212w\u2217rdu,k \u2212r\u2217\ndu,kw + w\u2217Ru,kw\n(18)\nThe cost function Jk(w) is quadratic in w and it has a unique minimizer since Ru,k > 0. Di\ufb00erentiating\nJk(w) with respect to w we \ufb01nd its gradient vector:\n\u2207wJ(w) = (Ru,kw \u2212rdu,k)\u2217\n(19)\nIt is seen that this gradient vector is annihilated at the same value w = wo given by (15). Therefore, we can\nequivalently state that if each node k solves the MMSE problem (16), then the solution vector coincides with\nthe desired parameter vector wo. This observation explains why it is often justi\ufb01ed to consider mean-square-\nerror cost functions when dealing with estimation problems that involve data that satisfy linear models\nsimilar to (9).\nBesides wo, the solution of the MMSE problem (16) also conveys information about the noise level in\nthe data. Note that by substituting wo from (15) into expression (16) for Jk(w), the resulting minimum\nmean-square-error value that is attained by node k is found to be:\nMSEk\n\u2206=\nJk(wo)\n=\nE|dk(i) \u2212uk,iwo|2\n(9)\n=\nE|vk(i)|2\n=\n\u03c32\nv,k\n(20)\n\u2206=\nJk,min\nWe shall use the notation Jk(wo) and Jk,min interchangeably to denote the minimum cost value of Jk(w).\nThe above result states that, when each agent k recovers wo from knowledge of its moments {Ru,k, rdu,k}\nusing expression (15), then the agent attains an MSE performance level that is equal to the noise power at\nits location, \u03c32\nv,k. An alternative useful expression for the minimum cost can be obtained by substituting\nexpression (15) for wo into (18) and simplifying the expression to \ufb01nd that\nMSEk\n=\n\u03c32\nd,k \u2212r\u2217\ndu,kR\u22121\nu,krdu,k\n(21)\nThis second expression is in terms of the data moments {\u03c32\nd,k, rdu,k, Ru,k}.\nNon-Cooperative Adaptive Solution\nThe optimal MMSE implementation (15) for determining wo requires knowledge of the statistical information\n{rdu,k, Ru,k}. This information is usually not available beforehand. Instead, the agents are more likely to have\naccess to successive time-indexed observations {dk(i), uk,i} of the random processes {dk(i), uk,i} for i \u22650.\nIn this case, it becomes necessary to devise a scheme that would allow each node to use its measurements\nto approximate wo. It turns out that an adaptive solution is possible. In this alternative implementation,\n8\neach node k feeds its observations {dk(i), uk,i} into an adaptive \ufb01lter and evaluates successive estimates for\nwo. As time passes by, the estimates would get closer to wo.\nThe adaptive solution operates as follows. Let wk,i denote an estimate for wo that is computed by node\nk at time i based on all the observations {dk(j), uk,j, j \u2264i} it has collected up to that time instant. There\nare many adaptive algorithms that can be used to compute wk,i; some \ufb01lters are more accurate than others\n(usually, at the cost of additional complexity) [4\u20137]. It is su\ufb03cient for our purposes to consider one simple\n(yet e\ufb00ective) \ufb01lter structure, while noting that most of the discussion in this article can be extended to\nother structures. One of the simplest choices for an adaptive structure is the least-mean-squares (LMS)\n\ufb01lter, where the data are processed by each node k as follows:\nek(i)\n=\ndk(i) \u2212uk,iwk,i\u22121\n(22)\nwk,i\n=\nwk,i\u22121 + \u00b5ku\u2217\nk,iek(i),\ni \u22650\n(23)\nStarting from some initial condition, say, wk,\u22121 = 0, the \ufb01lter iterates over i \u22650. At each time instant, i,\nthe \ufb01lter uses the local data {dk(i), uk,i} at node k to compute a local estimation error, ek(i), via (22). The\nerror is then used to update the existing estimate from wk,i\u22121 to wk,i via (23). The factor \u00b5k that appears in\n(23) is a constant positive step-size parameter; usually chosen to be su\ufb03ciently small to ensure mean-square\nstability and convergence, as discussed further ahead in the article. The step-size parameter can be selected\nto vary with time as well; one popular choice is to replace \u00b5k in (23) with the following construction:\n\u00b5k(i)\n\u2206=\n\u00b5k\n\u01eb + \u2225uk,i\u22252\n(24)\nwhere \u01eb > 0 is a small positive parameter and \u00b5k > 0. The resulting \ufb01lter implementation is known as\nnormalized LMS [5] since the step-size is normalized by the squared norm of the regression vector. Other\nchoices include step-size sequences {\u00b5(i) \u22650} that satisfy both conditions:\n\u221e\nX\ni=0\n\u00b5(i) = \u221e,\n\u221e\nX\ni=0\n\u00b52(i) < \u221e\n(25)\nSuch sequences converge slowly towards zero. One example is the choice \u00b5k(i) =\n\u00b5k\ni+1. However, by virtue of\nthe fact that such step-sizes die out as i \u2192\u221e, then these choices end up turning o\ufb00adaptation. As such,\nstep-size sequences satisfying (25) are not generally suitable for applications that require continuous learning,\nespecially under non-stationary environments. For this reason, in this article, we shall focus exclusively on\nthe constant step-size case (23) in order to ensure continuous adaptation and learning.\nEquations (22)\u2013(23) are written in terms of the observed quantities {dk(i), uk,i}; these are deterministic\nvalues since they correspond to observations of the random processes {dk(i), uk,i}. Often, when we are\ninterested in highlighting the random nature of the quantities involved in the adaptation step, especially\nwhen we study the mean-square performance of adaptive \ufb01lters, it becomes more useful to rewrite the\nrecursions using the boldface notation to highlight the fact that the quantities that appear in (22)\u2013(23) are\nactually realizations of random variables. Thus, we also write:\nek(i)\n=\ndk(i) \u2212uk,iwk,i\u22121\n(26)\nwk,i\n=\nwk,i\u22121 + \u00b5ku\u2217\nk,iek(i),\ni \u22650\n(27)\nwhere {ek(i), wk,i} will be random variables as well.\nThe performance of adaptive implementations of this kind are well-understood for both cases of stationary\nwo and changing wo [4\u20137]. For example, in the stationary case, if the adaptive implementation (26)\u2013(27)\nwere to succeed in having its estimator wk,i tend to wo with probability one as i \u2192\u221e, then we would\nexpect the error signal ek(i) in (26) to tend towards the noise signal vk(i) (by virtue of the linear model (9)).\nThis means that, under this ideal scenario, the variance of the error signal ek(i) would be expected to tend\ntowards the noise variance, \u03c32\nv,k, as i \u2192\u221e. Recall from (20) that the noise variance is the least cost that\nthe MSE solution can attain. Therefore, such limiting behavior by the adaptive \ufb01lter would be desirable.\n9\nHowever, it is well-known that there is always some loss in mean-square-error performance when adaptation\nis employed due to the e\ufb00ect of gradient noise, which is caused by the algorithm\u2019s reliance on observations\n(or realizations) {dk(i), uk,i} rather than on the actual moments {rdu,k, Ru,k}. In particular, it is known\nthat for LMS \ufb01lters, the variance of ek(i) in steady-state will be larger than \u03c32\nv,k by a small amount, and\nthe size of the o\ufb00set is proportional to the step-size parameter \u00b5k (so that smaller step-sizes lead to better\nmean-square-error (MSE) performance albeit at the expense of slower convergence). It is easy to see why\nthe variance of ek(i) will be larger than \u03c32\nv,k from the de\ufb01nition of the error signal in (26). Introduce the\nweight-error vector\newk,i\n\u2206= wo \u2212wk,i\n(28)\nand the so-called a-priori error signal\nea,k(i)\n\u2206= uk,i ewk,i\u22121\n(29)\nThis second error measures the di\ufb00erence between the uncorrupted term uk,iwo and its estimator prior to\nadaptation, uk,iwk,i\u22121. It then follows from the data model (9) and from the de\ufb01ning expression (26) for\nek(i) that\nek(i)\n=\ndk(i) \u2212uk,iwk,i\u22121\n=\nuk,iwo + vk(i) \u2212uk,iwk,i\u22121\n=\nea,k(i) + vk(i)\n(30)\nSince the noise component, vk(i), is assumed to be zero-mean and independent of all other random variables,\nwe conclude that\nE|ek(i)|2 = E|ea,k(i)|2 + \u03c32\nv,k\n(31)\nThis relation holds for any time instant i; it shows that the variance of the output error, ek(i), is larger\nthan \u03c32\nv,k by an amount that is equal to the variance of the a-priori error, ea,k(i). We de\ufb01ne the \ufb01lter\nmean-square-error (MSE) and excess-mean-square-error (EMSE) as the following steady-state measures:\nMSEk\n\u2206=\nlim\ni\u2192\u221eE|ek(i)|2\n(32)\nEMSEk\n\u2206=\nlim\ni\u2192\u221eE|ea,k(i)|2\n(33)\nso that, for the adaptive implementation (compare with (20)):\nMSEk = EMSEk + \u03c32\nv,k\n(34)\nTherefore, the EMSE term quanti\ufb01es the size of the o\ufb00set in the MSE performance of the adaptive \ufb01lter.\nWe also de\ufb01ne the \ufb01lter mean-square-deviation (MSD) as the steady-state measure:\nMSDk\n\u2206=\nlim\ni\u2192\u221eE\u2225ewk,i\u22252\n(35)\nwhich measures how far wk,i is from wo in the mean-square-error sense in steady-state. It is known that the\nMSD and EMSE of LMS \ufb01lters of the form (26)\u2013(27) can be approximated for su\ufb03ciently small-step sizes\nby the following expressions [4\u20137]:\nEMSEk\n\u2248\n\u00b5k\u03c32\nv,kTr(Ru,k)/2\n(36)\nMSDk\n\u2248\n\u00b5k\u03c32\nv,kM/2\n(37)\n10\nIt is seen that the smaller the step-size parameter, the better the performance of the adaptive solution.\nCooperative Adaptation through Di\ufb00usion\nObserve from (36)\u2013(37) that even if all nodes employ the same step-size, \u00b5k = \u00b5, and even if the regression\ndata are spatially uniform so that Ru,k = Ru for all k, the mean-square-error performance across the nodes\nstill varies in accordance with the variation of the noise power pro\ufb01le, \u03c32\nv,k, across the network. Nodes with\nlarger noise power will perform worse than nodes with smaller noise power. However, since all nodes are\nobserving data arising from the same underlying model wo, it is natural to expect cooperation among the\nnodes to be bene\ufb01cial. By cooperation we mean that neighboring nodes can share information (such as\nmeasurements or estimates) with each other as permitted by the network topology. Starting in the next\nsection, we will motivate and describe algorithms that enable nodes to carry out adaptation and learning in\na cooperative manner to enhance performance.\nSpeci\ufb01cally, we are going to see that one way to achieve cooperation is by developing adaptive algorithms\nthat enable the nodes to optimize the following global cost function in a distributed manner:\nmin\nw\nN\nX\nk=1\nE|dk(i) \u2212uk,iw|2\n(38)\nwhere the global cost is the aggregate objective:\nJglob(w)\n\u2206=\nN\nX\nk=1\nE|dk(i) \u2212uk,iw|2 =\nN\nX\nk=1\nJk(w)\n(39)\nComparing (38) with (16), we see that we are now adding the individual costs, Jk(w), from across all nodes.\nNote that since the desired wo satis\ufb01es (15) at every node k, then it also satis\ufb01es\n M\nX\nk=1\nRu,k\n!\nwo =\nN\nX\nk=1\nrdu,k\n(40)\nBut it can be veri\ufb01ed that the optimal solution to (38) is given by the same wo that satis\ufb01es (40). Therefore,\nsolving the global optimization problem (38) also leads to the desired wo. In future sections, we will show\nhow cooperative and distributed adaptive schemes for solving (38), such as (153) or (154) further ahead, lead\nto improved performance in estimating wo (in terms of smaller mean-square-deviation and faster convergence\nrate) than the non-cooperative mode (26)\u2013(27), where each agent runs its own individual adaptive \ufb01lter \u2014\nsee, e.g., Theorems 6.3\u20136.5 and Sec. 7.3.\n2.2\nApplication: Tapped-Delay-Line Models\nOur second example to motivate MSE cost functions, Jk(w), and linear models relates to identifying the\nparameters of a moving-average (MA) model from noisy data. MA models are also known as \ufb01nite-impulse-\nresponse (FIR) or tapped-delay-line models.\nThus, consider a situation where agents are interested in\nestimating the parameters of an FIR model, such as the taps of a communications channel or the parameters\nof some (approximate) model of interest in \ufb01nance or biology. Assume the agents are able to independently\nprobe the unknown model and observe its response to excitations in the presence of additive noise; this\nsituation is illustrated in Fig. 4, with the probing operation highlighted for one of the nodes (node 4).\nThe schematics inside the enlarged diagram in Fig. 4 is meant to convey that each node k probes the\nmodel with an input sequence {uk(i)} and measures the resulting response sequence, {dk(i)}, in the presence\n11\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n4\nv4(i)\nd4(i)\nu4(i)\nwo\nobservation\nnoise\nprobing\nsignal\nFigure 4: The network is interested in estimating the parameter vector wo that describes an underlying tapped-\ndelay-line model. The agents are assumed to be able to independently probe the unknown system, and observe its\nresponse to excitations, under noise, as indicated in the highlighted diagram for node 4.\nof additive noise. The system dynamics for each agent k is assumed to be described by a MA model of the\nform:\ndk(i) =\nM\u22121\nX\nm=0\n\u03b2muk(i \u2212m) + vk(i)\n(41)\nIn this model, the term vk(i) again represents an additive zero-mean noise process that is assumed to be\ntemporally white and spatially independent; it is also assumed to be independent of the input process,\n{u\u2113(j)}, for all i, j and \u2113. The scalars {\u03b2m} represent the model parameters that the agents seek to identify.\nIf we again collect the model parameters into an M \u00d7 1 column vector wo:\nwo\n\u2206= col {\u03b20, \u03b21, . . . , \u03b2M\u22121}\n(42)\nand the input data into a 1 \u00d7 M row regression vector:\nuk,i\n\u2206=\n\u0002\nuk(i)\nuk(i \u22121)\n. . .\nuk(i \u2212M + 1)\n\u0003\n(43)\nthen, we can again express the measurement equation (41) at each node k in the same linear model as (9),\nnamely,\ndk(i) = uk,iwo + vk(i)\n(44)\nAs was the case with model (9), we can likewise verify that, in view of (44), the desired parameter vector\nwo satis\ufb01es the same normal equations (15), i.e.,\nrdu,k = Ru,k wo\n\u21d0\u21d2\nwo = R\u22121\nu,k rdu,k\n(45)\nwhere the moments {rdu,k, Ru,k} continue to be de\ufb01ned by expressions (11)\u2013(12) with uk,i now de\ufb01ned by\n(43). Therefore, each node k can determine wo on its own by solving the same MMSE estimation problem\n12\n(16). This solution method requires knowledge of the moments {rdu,k, Ru,k} and, according to (20), each\nagent k would then attain an MSE level that is equal to the noise power level at its location.\nAlternatively, when the statistical information {rdu,k, Ru,k} is not available, each agent k can estimate\nwo iteratively by feeding data {dk(i), uk,i} into the adaptive implementation (26)\u2013(27). In this way, each\nagent k will achieve the same performance level shown earlier in (36)\u2013(37), with the limiting performance\nbeing again dependent on the local noise power level, \u03c32\nv,k. Therefore, nodes with larger noise power will\nperform worse than nodes with smaller noise power. However, since all nodes are observing data arising from\nthe same underlying model wo, it is natural to expect cooperation among the nodes to be bene\ufb01cial. As we\nare going to see, starting from the next section, one way to achieve cooperation and improve performance\nis by developing algorithms that optimize the same global cost function (38) in an adaptive and distributed\nmanner, such as algorithms (153) and (154) further ahead.\n2.3\nApplication: Target Localization\nOur third example relates to the problem of locating a destination of interest (such as the location of a\nnutrition source or a chemical leak) or locating and tracking an object of interest (such as a predator or a\nprojectile). In several such localization applications, the agents in the network are allowed to move towards\nthe target or away from it, in which case we would end up with a mobile adaptive network [8]. Biological\nnetworks behave in this manner such as networks representing \ufb01sh schools, bird formations, bee swarms,\nbacteria motility, and di\ufb00using particles [8\u201312]. The agents may move towards the target (e.g., when it is a\nnutrition source) or away from the target (e.g., when it is a predator). In other applications, the agents may\nremain static and are simply interested in locating a target or tracking it (such as tracking a projectile).\nTo motivate mean-square-error estimation in the context of localization problems, we start with the\nsituation corresponding to a static target and static nodes. Thus, assume that the unknown location of the\ntarget in the cartesian plane is represented by the 2 \u00d7 1 vector wo = col{xo, yo}. The agents are spread over\nthe same region of space and are interested in locating the target. The location of every agent k is denoted\nby the 2 \u00d7 1 vector pk = col{xk, yk} in terms of its x and y coordinates \u2013 see Fig. 5. We assume the agents\nare aware of their location vectors. The distance between agent k and the target is denoted by ro\nk and is\nequal to:\nro\nk\n=\n\u2225wo \u2212pk\u2225\n(46)\nThe 1 \u00d7 2 unit-norm direction vector pointing from agent k towards the target is denoted by uo\nk and is given\nby:\nuo\nk\n=\n(wo \u2212pk)T\n\u2225wo \u2212pk\u2225\n(47)\nObserve from (46) and (47) that ro\nk can be expressed in the following useful inner-product form:\nro\nk = uo\nk(wo \u2212pk)\n(48)\nIn practice, agents have noisy observations of both their distance and direction vector towards the target.\nWe denote the noisy distance measurement collected by node k at time i by:\nrk(i)\n=\nro\nk + vk(i)\n(49)\nwhere vk(i) denotes noise and is assumed to be zero-mean, and temporally white and spatially independent\nwith variance\n\u03c32\nv,k\n\u2206=\nE|vk(i)|2\n(50)\nWe also denote the noisy direction vector that is measured by node k at time i by uk,i. This vector is a\nperturbed version of uo\nk. We assume that uk,i continues to start from the location of the node at pk, but\n13\ntarget\ndistance to target\nunit direction\nvector to target\nx\ny\n(0, 0)\nnode k\n\u0001\nxo\nyo\n\u0002\n\u0001\nxk\nyk\n\u0002\nuo\nk\nro\nk\nFigure 5: The distance from node k to the target is denoted by ro\nk and the unit-norm direction vector from the same\nnode to the target is denoted by uo\nk. Node k is assumed to have access to noisy measurements of {ro\nk, uo\nk}.\nthat its tip is perturbed slightly either to the left or to the right relative to the tip of uo\nk \u2014 see Fig. 6. The\nperturbation to the tip of uo\nk is modeled as being the result of two e\ufb00ects: a small deviation that occurs\nalong the direction that is perpendicular to uo\nk, and a smaller deviation that occurs along the direction of\nuo\nk. Since we are assuming that the tip of uk,i is only slightly perturbed relative to the tip of uo\nk, then it is\nreasonable to expect the amount of perturbation along the parallel direction to be small compared to the\namount of perturbation along the perpendicular direction.\nThus, we write\nuk,i\n=\nuo\nk + \u03b1k(i) uo\u22a5\nk\n+ \u03b2k(i) uo\nk\n(1 \u00d7 2)\n(51)\nwhere uo\u22a5\nk\ndenotes a unit-norm row vector that lies in the same plane and whose direction is perpendicular to\nuo\nk. The variables \u03b1k(i) and \u03b2k(i) denote zero-mean independent random noises that are temporally white\nand spatially independent with variances:\n\u03c32\n\u03b1,k\n\u2206=\nE|\u03b1k(i)|2,\n\u03c32\n\u03b2,k\n\u2206= E|\u03b2k(i)|2\n(52)\nWe assume the contribution of \u03b2k(i) is small compared to the contributions of the other noise sources, \u03b1k(i)\nand vk(i), so that\n\u03c32\n\u03b2,k \u226a\u03c32\n\u03b1,k,\n\u03c32\n\u03b2,k \u226a\u03c32\nv,k\n(53)\nThe random noises {vk(i), \u03b1k(i), \u03b2k(i)} are further assumed to be independent of each other.\nUsing (48) we \ufb01nd that the noisy measurements {rk(i), uk,i} are related to the unknown wo via:\nrk(i) = uk,i(wo \u2212pk) + zk(i)\n(54)\nwhere the modi\ufb01ed noise term zk(i) is de\ufb01ned in terms of the noises in rk(i) and uk,i as follows:\nzk(i)\n\u2206=\nvk(i) \u2212\u03b1k(i) uo\u22a5\nk\n\u00b7 (wo \u2212pk) \u2212\u03b2k(i) \u00b7 uo\nk \u00b7 (wo \u2212pk)\n=\nvk(i) \u2212\u03b2k(i) \u00b7 ro\nk\n\u2248\nvk(i)\n(55)\n14\ntarget\nuo\nk\nnode k\nuk,i\nunit-radius\ncircle\nperpendicular direction\ndetermined by uo\u22a5\nk\nactual direction\nvector\nperturbed\ndirection\nvector\nFigure 6: The tip of the noisy direction vector is modeled as being approximately perturbed away from the actual\ndirection by two e\ufb00ects: a larger e\ufb00ect caused by a deviation along the direction that is perpendicular to uo\nk, and a\nsmaller deviation along the direction that is parallel to uo\nk.\nsince, by construction,\nuo\u22a5\nk\n\u00b7 (wo \u2212pk) = 0\n(56)\nand the contribution by \u03b2k(i) is assumed to be su\ufb03ciently small. If we now introduce the adjusted signal:\ndk(i)\n\u2206= rk(i) + uk,i pk\n(57)\nthen we arrive again from (54) and (55) at the following linear model for the available measurement variables\n{dk(i), uk,i} in terms of the target location wo:\ndk(i) \u2248uk,iwo + vk(i)\n(58)\nThere is one important di\ufb00erence in relation to the earlier linear models (9) and (44), namely, the variables\n{dk(i), uk,i} in (58) do not have zero means any longer. It is nevertheless straightforward to determine the\n\ufb01rst and second-order moments of the variables {dk(i), uk,i}. First, note from (49), (51), and (57) that\nEuk,i = uo\nk,\nEdk(i) = ro\nk + uo\nkpk\n(59)\nEven in this case of non-zero means, and in view of (58), the desired parameter vector wo can still be shown\nto satisfy the same normal equations (15), i.e.,\nrdu,k = Ru,kwo\n\u21d0\u21d2\nwo = R\u22121\nu,k rdu,k\n(60)\nwhere the moments {rdu,k, Ru,k} continue to be de\ufb01ned as\nRu,k\n\u2206= Eu\u2217\nk,iuk,i,\nrdu,k\n\u2206= Eu\u2217\nk,idk(i)\n(61)\nTo verify that (60) holds, we simply multiply both sides of (58) by u\u2217\nk,i from the left, compute the expectations\nof both sides, and use the fact that vk(i) has zero mean and is assumed to be independent of {u\u2113,j} for\n15\nall times j and nodes \u2113. However, the di\ufb00erence in relation to the earlier normal equations (15) is that the\nmatrix Ru,k is not the actual covariance matrix of uk,i any longer. When uk,i is not zero mean, its covariance\nmatrix is instead de\ufb01ned as:\ncovu,k\n\u2206=\nE(uk,i \u2212uo\nk)\u2217(uk,i \u2212uo\nk)\n=\nEu\u2217\nk,iuk,i \u2212uo\u2217\nk uo\nk\n(62)\nso that\nRu,k = covu,k + uo\u2217\nk uo\nk\n(63)\nWe conclude from this relation that Ru,k is positive-de\ufb01nite (and, hence, invertible) so that expression (60)\nis justi\ufb01ed. This is because the covariance matrix, covu,k, is itself positive-de\ufb01nite. Indeed, some algebra\napplied to the di\ufb00erence uk,i \u2212uo\nk from (51) shows that\ncovu,k\n=\n\u0002 \u0000uo\u22a5\nk\n\u0001\u2217\n(uo\nk)\u2217\u0003 \u0014 \u03c32\n\u03b1,k\n\u03c32\n\u03b2,k\n\u0015 \u0014\nuo\u22a5\nk\nuo\nk\n\u0015\n(64)\nwhere the matrix\n\u0014\nuo\u22a5\nk\nuo\nk\n\u0015\n(65)\nis full rank since the rows {uo\nk, uo\u22a5\nk } are linearly independent vectors.\nTherefore, each node k can determine wo on its own by solving the same minimum mean-square-error\nestimation problem (16). This solution method requires knowledge of the moments {rdu,k, Ru,k} and, ac-\ncording to (20), each agent k would then attain an MSE level that is equal to the noise power level, \u03c32\nv,k, at\nits location.\nAlternatively, when the statistical information {rdu,k, Ru,k} is not available beforehand, each agent k can\nestimate wo iteratively by feeding data {dk(i), uk,i} into the adaptive implementation (26)\u2013(27). In this case,\neach agent k will achieve the performance level shown earlier in (36)\u2013(37), with the limiting performance\nbeing again dependent on the local noise power level, \u03c32\nv,k. Therefore, nodes with larger noise power will\nperform worse than nodes with smaller noise power. However, since all nodes are observing distances and\ndirection vectors towards the same target location wo, it is natural to expect cooperation among the nodes\nto be bene\ufb01cial. As we are going to see, starting from the next section, one way to achieve cooperation and\nimprove performance is by developing algorithms that solve the same global cost function (38) in an adaptive\nand distributed manner, by using algorithms such as (153) and (154) further ahead.\nRole of Adaptation\nThe localization application helps highlight one of the main advantages of adaptation, namely, the ability\nof adaptive implementations to learn and track changing statistical conditions. For example, in the context\nof mobile networks, where nodes can move closer or further away from a target, the location vector for each\nagent k becomes time-dependent, say, pk,i = col{xk(i), yk(i)}. In this case, the actual distance and direction\nvector between agent k and the target also vary with time and become:\nro\nk(i) = \u2225wo \u2212pk,i\u2225,\nuo\nk,i = (wo \u2212pk,i)T\n\u2225wo \u2212pk,i\u2225\n(66)\nThe noisy distance measurement to the target is then:\nrk(i)\n=\nro\nk(i) + vk(i)\n(67)\nwhere the variance of vk(i) now depends on time as well:\n\u03c32\nv,k(i)\n\u2206=\nE|vk(i)|2\n(68)\n16\nIn the context of mobile networks, it is reasonable to assume that the variance of vk(i) varies both with time\nand with the distance to the target: the closer the node is to the target, the less noisy the measurement of\nthe distance is expected to be. Similar remarks hold for the variances of the noises \u03b1k(i) and \u03b2k(i) that\nperturb the measurement of the direction vector, say,\n\u03c32\n\u03b1,k(i)\n\u2206= E|\u03b1k(i)|2,\n\u03c32\n\u03b2,k(i)\n\u2206= E|\u03b2k(i)|2\n(69)\nwhere now\nuk,i\n=\nuo\nk,i + \u03b1k(i) uo\u22a5\nk,i + \u03b2k(i) uo\nk,i\n(70)\nThe same arguments that led to (58) can be repeated to lead to the same model, except that now the means\nof the variables {dk(i), uk,i} become time-dependent as well:\nEuk,i = uo\nk,i,\nEdk(i) = ro\nk(i) + uo\nk,i pk,i\n(71)\nNevertheless, adaptive solutions (whether cooperative or non-cooperative), are able to track such time-\nvariations because these solutions work directly with the observations {dk(i), uk,i} and the successive ob-\nservations will re\ufb02ect the changing statistical pro\ufb01le of the data. In general, adaptive solutions are able to\ntrack changes in the underlying signal statistics rather well [4, 5], as long as the rate of non-stationarity is\nslow enough for the \ufb01lter to be able to follow the changes.\n2.4\nApplication: Collaborative Spectral Sensing\nOur fourth and last example to illustrate the role of mean-square-error estimation and cooperation relates\nto spectrum sensing for cognitive radio applications. Cognitive radio systems involve two types of users:\nprimary users and secondary users.\nTo avoid causing harmful interference to incumbent primary users,\nunlicensed cognitive radio devices need to detect unused frequency bands even at low signal-to-noise (SNR)\nconditions [13\u201316]. One way to carry out spectral sensing is for each secondary user to estimate the aggregated\npower spectrum that is transmitted by all active primary users, and to locate unused frequency bands within\nthe estimated spectrum. This step can be performed by the secondary users with or without cooperation.\nThus, consider a communications environment consisting of Q primary users and N secondary users. Let\nSq(ej\u03c9) denote the power spectrum of the signal transmitted by primary user q. To facilitate estimation\nof the spectral pro\ufb01le by the secondary users, we assume that each Sq(ej\u03c9) can be represented as a linear\ncombination of basis functions, {fm(ej\u03c9)}, say, B of them [17]:\nSq(ej\u03c9)\n=\nB\nX\nm=1\n\u03b2qmfm(ej\u03c9),\nq = 1, 2, . . . , Q\n(72)\nIn this representation, the scalars {\u03b2qm} denote the coe\ufb03cients of the basis expansion for user q. The variable\n\u03c9 \u2208[\u2212\u03c0, \u03c0] denotes the normalized angular frequency measured in radians/sample. The power spectrum is\noften symmetric about the vertical axis, \u03c9 = 0, and therefore it is su\ufb03cient to focus on the interval \u03c9 \u2208[0, \u03c0].\nThere are many ways by which the basis functions, {fm(ej\u03c9)}, can be selected. The following is one possible\nconstruction for illustration purposes. We divide the interval [0, \u03c0] into B identical intervals and denote\ntheir center frequencies by {\u03c9m}. We then place a Gaussian pulse at each location \u03c9m and control its width\nthrough the selection of its standard deviation, \u03c3m, i.e.,\nfm(ej\u03c9)\n\u2206= e\n\u2212(\u03c9\u2212\u03c9m)2\n\u03c32m\n(73)\nFigure 7 illustrates this construction. The parameters {\u03c9m, \u03c3m} are selected by the designer and are assumed\nto be known. For a su\ufb03ciently large number, B, of basis functions, the representation (72) can approximate\nwell a large class of power spectra.\n17\n\u03c91\n\u03c92\n\u03c93\n\u03c9B\nfm(ej\u03c9) = e\n\u2212(\u03c9\u2212\u03c9m)2\n\u03c32m\n0\nSq(ej\u03c9)\n\u03c9\nSq(ej\u03c9)\nFigure 7: The interval [0, \u03c0] is divided into B sub-intervals of equal width; the center frequencies of the sub-intervals\nare denoted by {\u03c9m}. A power spectrum Sq(ej\u03c9) is approximated as a linear combination of Gaussian basis functions\ncentered on the {\u03c9m}.\nWe collect the combination coe\ufb03cients {\u03b2qm} for primary user q into a column vector wq:\nwq\n\u2206= col {\u03b2q1, \u03b2q2, \u03b2q3, . . . , \u03b2qB}\n(B \u00d7 1)\n(74)\nand collect the basis functions into a row vector:\nf\u03c9\n\u2206=\n\u0002\nf1(ej\u03c9)\nf2(ej\u03c9)\n. . .\nfB(ej\u03c9)\n\u0003\n(1 \u00d7 B)\n(75)\nThen, the power spectrum (72) can be expressed in the alternative inner-product form:\nSq(ej\u03c9) = f\u03c9wq\n(76)\nLet pqk denote the path loss coe\ufb03cient from primary user q to secondary user k. When the transmitted\nspectrum Sq(ej\u03c9) travels from primary user q to secondary user k, the spectrum that is sensed by node k\nis pqkSq(ej\u03c9). We assume in this example that the path loss factors {pqk} are known and that they have\nbeen determined during a prior training stage involving each of the primary users with each of the secondary\nusers. The training is usually repeated at regular intervals of time to accommodate the fact that the path\nloss coe\ufb03cients can vary (albeit slowly) over time. Figure 8 depicts a cognitive radio system with 2 primary\nusers and 10 secondary users. One of the secondary users (user 5) is highlighted and the path loss coe\ufb03cients\nfrom the primary users to its location are indicated; similar path loss coe\ufb03cients can be assigned to all other\ncombinations involving primary and secondary users.\nEach user k senses the aggregate e\ufb00ect of the power spectra that are transmitted by all active primary\nusers. Therefore, adding the e\ufb00ect of all primary users, we \ufb01nd that the power spectrum that arrives at\nsecondary user k is given by:\nSk(ej\u03c9)\n=\nQ\nX\nq=1\npqkSq(ej\u03c9) + \u03c32\nk\n=\nQ\nX\nq=1\npqkf\u03c9wq + \u03c32\nk\n\u2206=\nuk,\u03c9wo + \u03c32\nk\n(77)\nwhere \u03c32\nk denotes the receiver noise power at node k, and where we introduced the following vector quantities:\nwo\n\u2206=\ncol{w1, w2, . . . , wQ}\n(BQ \u00d7 1)\n(78)\nuk,\u03c9\n\u2206=\n\u0002 p1kf\u03c9\np2kf\u03c9\n. . .\npQkf\u03c9\n\u0003\n(1 \u00d7 BQ)\n(79)\n18\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n2\n1\nsecondary user (SU)\nprimary user (PU)\nPU\nPU\nSU\np25\np15\nFigure 8: A network of secondary users in the presence of two primary users. One of the secondary users is highlighted\nand the path loss coe\ufb03cients from the primary users to its location are indicated as p15 and p25.\nThe vector wo is the collection of all combination coe\ufb03cients for all Q primary users. The vector uk,\u03c9\ncontains the path loss coe\ufb03cients from all primary users to user k. Now, at every time instant i, user k\nobservers its received power spectrum, Sk(ej\u03c9), over a discrete grid of frequencies, {\u03c9r}, in the interval [0, \u03c0]\nin the presence of additive measurement noise. We denote these measurements by:\ndk,r(i)\n=\nuk,\u03c9rwo + \u03c32\nk + vk,r(i)\nr = 1, 2, . . ., R\n(80)\nThe term vk,r(i) denotes sampling noise and is assumed to have zero mean and variance \u03c32\nv,k; it is also\nassumed to be temporally white and spatially independent; and is also independent of all other random\nvariables. Since the row vectors uk,\u03c9 in (79) are de\ufb01ned in terms of the path loss coe\ufb03cients {pqk}, and since\nthese coe\ufb03cients are estimated and subject to noisy distortions, we model the uk,\u03c9r as zero-mean random\nvariables in (80) and use the boldface notation for them.\nObserve that in this application, each node k collects R measurements at every time instant i and not\nonly a single measurement, as was the case with the three examples discussed in the previous sections\n(AR modeling, MA modeling, and localization). The implication of this fact is that we now deal with an\nestimation problem that involves vector measurements instead of scalar measurements at each node. The\nsolution structure continues to be the same. We collect the R measurements at node k at time i into vectors\nand introduce the R \u00d7 1 quantities:\ndk,i\n\u2206=\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8f0\ndk,1(i) \u2212\u03c32\nk\ndk,2(i) \u2212\u03c32\nk\n...\ndk,R(i) \u2212\u03c32\nk\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fb,\nvk,i\n\u2206=\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8f0\nvk,1(i)\nvk,2(i)\n...\nvk,R(i)\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fb\n(81)\n19\nand the regression matrix:\nU k,i\n\u2206=\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8f0\nuk,\u03c91\nuk,\u03c92\n...\nuk,\u03c9R\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fb\n(R \u00d7 QB)\n(82)\nThe time subscript in U k,i is used to model the fact that the path loss coe\ufb03cients can change over time\ndue to the possibility of node mobility. With the above notation, expression (80) is equivalent to the linear\nmodel:\ndk,i = U k,iwo + vk,i\n(83)\nCompared to the earlier examples (9), (44), and (58), the main di\ufb00erence now is that each agent k collects\na vector of measurements, dk,i, as opposed to the scalar dk(i), and its regression data are represented by\nthe matrix quantity, U k,i, as opposed to the row vector uk,i. Nevertheless, the estimation approach will\ncontinue to be the same. In cognitive network applications, the secondary users are interested in estimating\nthe aggregate power spectrum of the primary users in order for the secondary users to identify vacant\nfrequency bands that can be used by them. In the context of model (83), this amounts to determining the\nparameter vector wo since knowledge of its entries allows each secondary user to reconstruct the aggregate\npower spectrum de\ufb01ned by:\nSA(ej\u03c9)\n\u2206=\nQ\nX\nq=1\nSq(ej\u03c9) = (1T\nQ \u2297f\u03c9)wo\n(84)\nwhere the notation \u2297denotes the Kronecker product operation, and 1Q denotes a Q\u00d71 vector whose entries\nare all equal to one.\nAs before, we can again verify that, in view of (83), the desired parameter vector wo satis\ufb01es the same\nnormal equations:\nRdU,k = RU,kwo\n\u21d0\u21d2\nwo = R\u22121\nU,k RdU,k\n(85)\nwhere the moments {RdU,k, RU,k} are now de\ufb01ned by\nRdU,k\n\u2206=\nEU \u2217\nk,idk,i\n(QB \u00d7 1)\n(86)\nRU,k\n\u2206=\nEU \u2217\nk,iU k,i\n(QB \u00d7 QB)\n(87)\nTherefore, each secondary user k can determine wo on its own by solving the following minimum mean-\nsquare-error estimation problem:\nmin\nw\nE \u2225dk,i \u2212U k,iw\u22252\n(88)\nThis solution method requires knowledge of the moments {RdU,k, RU,k} and, in an argument similar to the\none that led to (20), it can be veri\ufb01ed that each agent k would attain an MSE performance level that is\nequal to the noise power level, \u03c32\nv,k, at its location.\nAlternatively, when the statistical information {RdU,k, RU,k} is not available, each secondary user k can\nestimate wo iteratively by feeding data {dk,i, Uk,i} into an adaptive implementation similar to (26)\u2013(27),\nsuch as the following vector LMS recursion:\nek,i\n=\ndk,i \u2212U k,iwk,i\u22121\n(89)\nwk,i\n=\nwk,i\u22121 + \u00b5kU \u2217\nk,iek,i\n(90)\nIn this case, each secondary user k will achieve the same performance levels shown earlier in (36)\u2013(37) with\nRu,k replaced by RU,k. The performance will again be dependent on the local noise level, \u03c32\nv,k. As a result,\nsecondary users with larger noise power will perform worse than secondary users with smaller noise power.\nHowever, since all secondary users are observing data arising from the same underlying model wo, it is\nnatural to expect cooperation among the users to be bene\ufb01cial. As we are going to see, starting from the\n20\nnext section, one way to achieve cooperation and improve performance is by developing algorithms that solve\nthe following global cost function in an adaptive and distributed manner:\nmin\nw\nN\nX\nk=1\nE \u2225dk,i \u2212U k,iw\u22252\n(91)\n3\nDistributed Optimization via Di\ufb00usion Strategies\nThe examples in the previous section were meant to illustrate how MSE cost functions and linear models are\nuseful design tools and how they arise frequently in applications. We now return to problem (1) and study\nthe distributed optimization of global cost functions such as (39), where Jglob(w) is assumed to consist of\nthe sum of individual components. Speci\ufb01cally, we are now interested in solving optimization problems of\nthe type:\nmin\nw\nN\nX\nk=1\nJk(w)\n(92)\nwhere each Jk(w) is assumed to be di\ufb00erentiable and convex over w. Although the algorithms presented in\nthis article apply to more general situations, we shall nevertheless focus on mean-square-error cost functions\nof the form:\nJk(w)\n\u2206= E|dk(i) \u2212uk,iw|2\n(93)\nwhere w is an M \u00d7 1 column vector, and the random processes {dk(i), uk,i} are assumed to be jointly\nwide-sense stationary with zero-mean and second-order moments:\n\u03c32\nd,k\n\u2206=\nE|dk(i)|2\n(94)\nRu,k\n\u2206=\nEu\u2217\nk,iuk,i > 0\n(M \u00d7 M)\n(95)\nrdu,k\n\u2206=\nEdk(i)u\u2217\nk,i\n(M \u00d7 1)\n(96)\nIt is clear that each Jk(w) is quadratic in w since, after expansion, we get\nJk(w) = \u03c32\nd,k \u2212w\u2217rdu,k \u2212r\u2217\ndu,k w + w\u2217Ru,k w\n(97)\nA completion-of-squares argument shows that Jk(w) can be expressed as the sum of two squared terms, i.e.,\nJk(w) =\n\u0010\n\u03c32\nd,k \u2212r\u2217\ndu,kR\u22121\nu,krdu,k\n\u0011\n+ (w \u2212wo)\u2217Ru,k(w \u2212wo)\n(98)\nor, more compactly,\nJk(w) = Jk,min + \u2225w \u2212wo\u22252\nRu,k\n(99)\nwhere wo denotes the minimizer of Jk(w) and is given by\nwo\n\u2206= R\u22121\nu,k rdu,k\n(100)\nand Jk,min denotes the minimum value of Jk(w) when evaluated at w = wo:\nJk,min\n\u2206= \u03c32\nd,k \u2212r\u2217\ndu,kR\u22121\nu,krdu,k = Jk(wo)\n(101)\n21\nObserve that this value is necessarily non-negative since it can be viewed as the Schur complement of the\nfollowing covariance matrix:\nE\n\u0012\u0014 d\u2217\nk(i)\nu\u2217\nk,i\n\u0015 \u0002 dk(i)\nuk,i\n\u0003\u0013\n=\n\u0014 \u03c32\nd,k\nr\u2217\ndu,k\nrdu,k\nRu,k\n\u0015\n(102)\nand covariance matrices are nonnegative-de\ufb01nite.\nThe choice of the quadratic form (93) or (97) for Jk(w) is useful for many applications, as was already\nillustrated in the previous section for examples involving AR modeling, MA modeling, localization, and\nspectral sensing. Other choices for Jk(w) are of course possible and these choices can even be di\ufb00erent for\ndi\ufb00erent nodes. It is su\ufb03cient in this article to illustrate the main concepts underlying di\ufb00usion adaptation\nby focusing on the useful case of MSE cost functions of the form (97); still, most of the derivations and\narguments in the coming sections can be extended beyond MSE optimization to more general cost functions\n\u2014 as already shown in [1\u20133]; see also Sec. 10.4.\nThe positive-de\ufb01niteness of the covariance matrices {Ru,k} ensures that each Jk(w) in (97) is strictly\nconvex, as well as Jglob(w) from (39). Moreover, all these cost functions have a unique minimum at the\nsame wo, which satis\ufb01es the normal equations:\nRu,k wo = rdu,k,\nfor every k = 1, 2, . . . , N\n(103)\nTherefore, given knowledge of {rdu,k, Ru,k}, each node can determine wo on its own by solving (103). One\nthen wonders about the need to seek distributed cooperative and adaptive solutions. There are a couple of\nreasons:\n(a) First, even for MSE cost functions, it is often the case that the required moments {rdu,k, Ru,k} are not\nknown beforehand. In this case, the optimal wo cannot be determined from the solution of the normal\nequations (103). The alternative methods that we shall describe will lead to adaptive techniques that\nenable each node k to estimate wo directly from data realizations.\n(b) Second, since adaptive strategies rely on instantaneous data, these strategies possess powerful tracking\nabilities. Even when the moments vary with time due to non-stationary behavior (such as wo changing\nwith time), these changes will be re\ufb02ected in the observed data and will in turn in\ufb02uence the behavior\nof the adaptive construction. This is one of the key advantages of adaptive strategies: they enable\nlearning and tracking in real-time.\n(c) Third, cooperation among nodes is generally bene\ufb01cial. When nodes act individually, their performance\nis limited by the noise power level at their location. In this way, some nodes can perform signi\ufb01cantly\nbetter than other nodes. On the other hand, when nodes cooperate with their neighbors and share\ninformation during the adaptation process, we will see that performance can be improved across the\nnetwork.\n3.1\nRelating the Global Cost to Neighborhood Costs\nLet us therefore consider the optimization of the following global cost function:\nJglob(w) =\nN\nX\nk=1\nJk(w)\n(104)\nwhere Jk(w) is given by (93) or (97). Our strategy to optimize Jglob(w) in a distributed manner is based\non two steps, following the developments in [1, 2, 18]. First, using a completion-of-squares argument (or,\nequivalently, a second-order Taylor series expansion), we approximate the global cost function (104) by\nan alternative local cost that is amenable to distributed optimization. Then, each node will optimize the\nalternative cost via a steepest-descent method.\n22\nTo motivate the distributed di\ufb00usion-based approach, we start by introducing a set of nonnegative coef-\n\ufb01cients {ck\u2113} that satisfy two conditions:\nfor k = 1, 2, . . ., N :\nck\u2113\u22650,\nN\nX\n\u2113=1\nck\u2113= 1,\nand\nck\u2113= 0 if \u2113/\u2208Nk\n(105)\nwhere Nk denotes the neighborhood of node k. Condition (105) means that for every node k, the sum of the\ncoe\ufb03cients {ck\u2113} that relate it to its neighbors is one. The coe\ufb03cients {ck\u2113} are free parameters that are\nchosen by the designer; obviously, as shown later in Theorem 6.8, their selection will have a bearing on the\nperformance of the resulting algorithms. If we collect the entries {ck\u2113} into an N \u00d7 N matrix C, so that the\nk\u2212th row of C is formed of {ck\u2113, \u2113= 1, 2, . . . , N}, then condition (105) translates into saying that each of\nrow of C adds up to one, i.e.,\nC1 = 1\n(106)\nwhere the notation 1 denotes an N \u00d7 1 column vector with all its entries equal to one:\n1\n\u2206= col{1, 1, . . ., 1}\n(107)\nWe say that C is a right stochastic matrix. Using the coe\ufb03cients {ck\u2113} so de\ufb01ned, we associate with each\nnode \u2113, a local cost function of the following form:\nJloc\n\u2113\n(w)\n\u2206=\nX\nk\u2208N\u2113\nck\u2113Jk(w)\n(108)\nThis cost consists of a weighted combination of the individual costs of the neighbors of node \u2113(including \u2113\nitself) \u2014 see Fig. 9. Since the {ck\u2113} are all nonnegative and each Jk(w) is strictly convex, then Jloc\n\u2113\n(w) is\nalso strictly convex and its minimizer occurs at the same w = wo. Using the alternative representation (99)\nfor the individual Jk(w), we can re-express the local cost Jloc\n\u2113\n(w) as\nJloc\n\u2113\n(w)\n=\nX\nk\u2208N\u2113\nck\u2113Jk,min +\nX\nk\u2208N\u2113\nck\u2113\u2225w \u2212wo\u22252\nRu,k\n(109)\nor, equivalently,\nJloc\n\u2113\n(w) = Jloc\n\u2113,min + \u2225w \u2212wo\u22252\nR\u2113\n(110)\nwhere Jloc\n\u2113,min corresponds to the minimum value of Jloc\n\u2113\n(w) at the minimizer w = wo:\nJloc\n\u2113,min\n\u2206=\nX\nk\u2208N\u2113\nck\u2113Jk,min\n(111)\nand R\u2113is a positive-de\ufb01nite weighting matrix de\ufb01ned by:\nR\u2113\n\u2206=\nX\nk\u2208N\u2113\nck\u2113Ru,k\n(112)\nThat is, R\u2113is a weighted combination of the covariance matrices in the neighborhood of node \u2113. Equality\n(110) amounts to a (second-order) Taylor series expansion of Jloc\n\u2113\n(w) around w = wo. Note that the right-\nhand side consists of two terms: the minimum cost and a weighted quadratic term in the di\ufb00erence (w\u2212wo).\n23\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nN3\nJ1(w)\nJ2(w)\nJ5(w)\nJ3(w)\nc13\nc23\nc53\nJloc\n3 (w) = c13J1(w) + c23J2(w) + c33J3(w) + c53J5(w)\nJloc\n3 (w)\nc13 + c23 + c33 + c53 = 1\nFigure 9: A network with N = 10 nodes. The nodes in the neighborhood of node 3 are highlighted with their\nindividual cost functions, and with the combination weights {c13, c23, c53} along the connecting edges; there is also a\ncombination weight associated with node 3 and is denoted by c33. The expression for the local cost function, Jloc\n3 (w),\nis also shown in the \ufb01gure.\nNow note that we can express Jglob(w) from (104) as follows:\nJglob(w)\n(105)\n=\nN\nX\nk=1\n N\nX\n\u2113=1\nck\u2113\n!\nJk(w)\n=\nN\nX\n\u2113=1\n N\nX\nk=1\nck\u2113Jk(w)\n!\n(108)\n=\nN\nX\n\u2113=1\nJloc\n\u2113\n(w)\n=\nJloc\nk (w) +\nN\nX\n\u2113\u0338=k\nJloc\n\u2113\n(w)\n(113)\nSubstituting (110) into the second term on the right-hand side of the above expression gives:\nJglob(w) = Jloc\nk (w) +\nX\n\u2113\u0338=k\n\u2225w \u2212wo\u22252\nR\u2113+\nX\n\u2113\u0338=k\nJloc\n\u2113,min\n(114)\nThe last term in the above expression does not depend on w. Therefore, minimizing Jglob(w) over w is\nequivalent to minimizing the following alternative global cost:\nJglob\u2032(w) = Jloc\nk (w) +\nX\n\u2113\u0338=k\n\u2225w \u2212wo\u22252\nR\u2113\n(115)\n24\nExpression (115) relates the optimization of the original global cost function, Jglob(w) or its equivalent\nJglob\u2032(w), to the newly-introduced local cost function Jloc\nk (w). The relation is through the second term on\nthe right-hand side of (115), which corresponds to a sum of quadratic factors involving the minimizer wo;\nthis term tells us how the local cost Jloc\nk (w) can be corrected to the global cost Jglob\u2032(w). Obviously, the\nminimizer wo that appears in the correction term is not known since the nodes wish to determine its value.\nLikewise, not all the weighting matrices R\u2113are available to node k; only those matrices that originate from\nits neighbors can be assumed to be available. Still, expression (115) suggests a useful way to replace Jloc\nk\nby another local cost that is closer to Jglob\u2032(w). This alternative cost will be shown to lead to a powerful\ndistributed solution to optimize Jglob(w) through localized interactions.\nOur \ufb01rst step is to limit the summation on the right-hand side of (115) to the neighbors of node k (since\nevery node k can only have access to information from its neighbors). We thus introduce the modi\ufb01ed cost\nfunction at node k:\nJglob\u2032\nk\n(w)\n\u2206= Jloc\nk (w) +\nX\n\u2113\u2208Nk\\{k}\n\u2225w \u2212wo\u22252\nR\u2113\n(116)\nThe cost functions Jloc\nk (w) and Jglob\u2032\nk\n(w) are both associated with node k; the di\ufb00erence between them is\nthat the expression for the latter is closer to the global cost function (115) that we want to optimize.\nThe weighting matrices {R\u2113} that appear in (116) may or may not be available because the second-\norder moments {Ru,\u2113} may or may not be known beforehand. If these moments are known, then we can\nproceed with the analysis by assuming knowledge of the {R\u2113}. However, the more interesting case is when\nthese moments are not known. This is generally the case in practice, especially in the context of adaptive\nsolutions and problems involving non-stationary data. Often, nodes can only observe realizations {u\u2113,i} of\nthe regression data {u\u2113,i} arising from distributions whose covariance matrices are the unknown {Ru,\u2113}.\nOne way to address the di\ufb03culty is to replace each of the weighted norms \u2225w \u2212wo\u22252\nR\u2113in (116) by a scaled\nmultiple of the un-weighted norm, say,\n\u2225w \u2212wo\u22252\nR\u2113\u2248b\u2113k \u00b7 \u2225w \u2212wo\u22252\n(117)\nwhere b\u2113k is some nonnegative coe\ufb03cient; we are even allowing its value to change with the node index k.\nThe above substitution amounts to having each node k approximate the {R\u2113} from its neighbors by multiples\nof the identity matrix\nR\u2113\u2248b\u2113k IM\n(118)\nApproximation (117) is reasonable in view of the fact that all vector norms are equivalent [19\u201321]; this norm\nproperty ensures that we can bound the weighted norm \u2225w \u2212wo\u22252\nR\u2113by some constants multiplying the\nun-weighted norm \u2225w \u2212wo\u22252, say, as:\nr1\u2225w \u2212wo\u22252 \u2264\u2225w \u2212wo\u22252\nR\u2113\u2264r2\u2225w \u2212wo\u22252\n(119)\nfor some positive constants (r1, r2). Using the fact that the {R\u2113} are Hermitian positive-de\ufb01nite matrices,\nand calling upon the Rayleigh-Ritz characterization of eigenvalues [19, 20], we can be more speci\ufb01c and\nreplace the above inequalities by\n\u03bbmin(R\u2113) \u00b7 \u2225w \u2212wo\u22252 \u2264\u2225w \u2212wo\u22252\nR\u2113\u2264\u03bbmax(R\u2113) \u00b7 \u2225w \u2212wo\u22252\n(120)\nWe note that approximations similar to (118) are common in stochastic approximation theory and they\nmark the di\ufb00erence between using a Newton\u2019s iterative method or a stochastic gradient method [5,22]; the\nformer uses Hessian matrices as approximations for R\u2113and the latter uses multiples of the identity matrix.\nFurthermore, as the derivation will reveal, we do not need to worry at this stage about how to select the\nscalars {b\u2113k}; they will end up being embedded into another set of coe\ufb03cients {a\u2113k} that will be set by the\ndesigner or adjusted by the algorithm \u2014 see (132) further ahead.\nThus, we replace (116) by\nJglob\u2032\u2032\nk\n(w) = Jloc\nk (w) +\nX\n\u2113\u2208Nk\\{k}\nb\u2113k \u2225w \u2212wo\u22252\n(121)\n25\nThe argument so far has suggested how to modify Jloc\nk (w) from (108) and replace it by the cost (121) that is\ncloser in form to the global cost function (115). If we replace Jloc\nk (w) by its de\ufb01nition (108), we can rewrite\n(121) as\nJglob\u2032\u2032\nk\n(w) =\nX\n\u2113\u2208Nk\nc\u2113k J\u2113(w) +\nX\n\u2113\u2208Nk\\{k}\nb\u2113k \u2225w \u2212wo\u22252\n(122)\nWith the exception of the variable wo, this approximate cost at node k relies solely on information that is\navailable to node k from its neighborhood. We will soon explain how to handle the fact that wo is not known\nbeforehand to node k.\n3.2\nSteepest-Descent Iterations\nNode k can apply a steepest-descent iteration to minimize Jglob\u2032\u2032\nk\n(w). Let wk,i denote the estimate for the\nminimizer wo that is evaluated by node k at time i. Starting from an initial condition wk,\u22121, node k can\ncompute successive estimates iteratively as follows:\nwk,i = wk,i\u22121 \u2212\u00b5k\nh\n\u2207wJglob\u2032\u2032\nk\n(wk,i\u22121)\ni\u2217\n,\ni \u22650\n(123)\nwhere \u00b5k is a small positive step-size parameter, and the notation \u2207wJ(a) denotes the gradient vector of the\nfunction J(w) relative to w and evaluated at w = a. The step-size parameter \u00b5k can be selected to vary with\ntime as well. One choice that is common in the optimization literature [5,22,52] is to replace \u00b5k in (123) by\nstep-size sequences {\u00b5(i) \u22650} that satisfy the two conditions (25). However, such step-size sequences are\nnot suitable for applications that require continuous learning because they turn o\ufb00adaptation as i \u2192\u221e;\nthe steepest-descent iteration (123) would stop updating since \u00b5k(i) would be tending towards zero. For this\nreason, we shall focus mainly on the constant step-size case described by (123) since we are interested in\ndeveloping distributed algorithms that will endow networks with continuous adaptation abilities.\nReturning to (123) and computing the gradient vector of (122) we get:\nwk,i = wk,i\u22121 \u2212\u00b5k\nX\n\u2113\u2208Nk\nc\u2113k [\u2207wJ\u2113(wk,i\u22121)]\u2217\u2212\u00b5k\nX\n\u2113\u2208Nk\\{k}\nb\u2113k (wk,i\u22121 \u2212wo)\n(124)\nUsing the expression for J\u2113(w) from (97) we arrive at\nwk,i = wk,i\u22121 + \u00b5k\nX\n\u2113\u2208Nk\nc\u2113k (rdu,\u2113\u2212Ru,\u2113wk,i\u22121) + \u00b5k\nX\n\u2113\u2208Nk\\{k}\nb\u2113k (wo \u2212wk,i\u22121)\n(125)\nThis iteration indicates that the update from wk,i\u22121 to wk,i involves adding two correction terms to wk,i\u22121.\nAmong many other forms, we can implement the update in two successive steps by adding one correction\nterm at a time, say, as follows:\n\u03c8k,i\n=\nwk,i\u22121 + \u00b5k\nX\n\u2113\u2208Nk\nc\u2113k (rdu,\u2113\u2212Ru,\u2113wk,i\u22121)\n(126)\nwk,i\n=\n\u03c8k,i + \u00b5k\nX\n\u2113\u2208Nk\\{k}\nb\u2113k (wo \u2212wk,i\u22121)\n(127)\nStep (126) updates wk,i\u22121 to an intermediate value \u03c8k,i by using local gradient vectors from the neighborhood\nof node k. Step (127) further updates \u03c8k,i to wk,i. However, this second step is not realizable since wo is\nnot known and the nodes are actually trying to estimate it. Two issues stand out from examining (127):\n(a) First, iteration (127) requires knowledge of the minimizer wo. Neither node k nor its neighbors know\nthe value of the minimizer; each of these nodes is actually performing steps similar to (126) and (127)\n26\nto estimate the minimizer. However, each node \u2113has a readily available approximation for wo, which\nis its local intermediate estimate \u03c8\u2113,i. Therefore, we replace wo in (127) by \u03c8\u2113,i. This step helps di\ufb00use\ninformation throughout the network. This is because each neighbor of node k determines its estimate\n\u03c8\u2113,i by processing information from its own neighbors, which process information from their neighbors,\nand so forth.\n(b) Second, the intermediate value \u03c8k,i at node k is generally a better estimate for wo than wk,i\u22121 since it\nis obtained by incorporating information from the neighbors through the \ufb01rst step (126). Therefore,\nwe further replace wk,i\u22121 in (127) by \u03c8k,i. This step is reminiscent of incremental-type approaches to\noptimization, which have been widely studied in the literature [23\u201326].\nWith the substitutions described in items (a) and (b) above, we replace the second step (127) by\nwk,i\n=\n\u03c8k,i + \u00b5k\nX\n\u2113\u2208Nk\\{k}\nb\u2113k (\u03c8\u2113,i \u2212\u03c8k,i)\n=\n\uf8eb\n\uf8ed1 \u2212\u00b5k\nX\n\u2113\u2208Nk\\{k}\nb\u2113k\n\uf8f6\n\uf8f8\u03c8k,i + \u00b5k\nX\n\u2113\u2208Nk\\{k}\nb\u2113k \u03c8\u2113,i\n(128)\nIntroduce the weighting coe\ufb03cients:\nakk\n\u2206=\n1 \u2212\u00b5k\nX\n\u2113\u2208Nk\\{k}\nb\u2113k\n(129)\na\u2113k\n\u2206=\n\u00b5kb\u2113k,\n\u2113\u2208Nk\\{k}\n(130)\na\u2113k\n\u2206=\n0,\n\u2113/\u2208Nk\n(131)\nand observe that, for su\ufb03ciently small step-sizes \u00b5k, these coe\ufb03cients are nonnegative and, moreover, they\nsatisfy the conditions:\nfor k = 1, 2, . . . , N :\na\u2113k \u22650,\nN\nX\n\u2113=1\na\u2113k = 1,\nand\na\u2113k = 0 if \u2113/\u2208Nk\n(132)\nCondition (132) means that for every node k, the sum of the coe\ufb03cients {a\u2113k} that relate it to its neighbors\nis one. Just like the {c\u2113k}, from now on, we will treat the coe\ufb03cients {a\u2113k} as free weighting parameters that\nare chosen by the designer according to (132); their selection will also have a bearing on the performance of\nthe resulting algorithms \u2014 see Theorem 6.8. If we collect the entries {a\u2113k} into an N \u00d7 N matrix A, such\nthat the k\u2212th column of A consists of {a\u2113k, \u2113= 1, 2, . . ., N}, then condition (132) translates into saying\nthat each column of A adds up to one:\nAT 1 = 1\n(133)\nWe say that A is a left stochastic matrix.\n3.3\nAdapt-then-Combine (ATC) Di\ufb00usion Strategy\nUsing the coe\ufb03cients {a\u2113k} so de\ufb01ned, we replace (126) and (128) by the following recursions for i \u22650:\n(ATC strategy)\n\u03c8k,i = wk,i\u22121 + \u00b5k\nX\n\u2113\u2208Nk\nc\u2113k (rdu,\u2113\u2212Ru,\u2113wk,i\u22121)\nwk,i =\nX\n\u2113\u2208Nk\na\u2113k \u03c8\u2113,i\n(134)\n27\nfor some nonnegative coe\ufb03cients {c\u2113k, a\u2113k} that satisfy conditions (106) and (133), namely,\nC1 = 1,\nAT 1 = 1\n(135)\nor, equivalently,\nfor k = 1, 2, . . . , N :\nc\u2113k \u22650,\nN\nX\nk=1\nc\u2113k = 1,\nc\u2113k = 0 if \u2113/\u2208Nk\na\u2113k \u22650,\nN\nX\n\u2113=1\na\u2113k = 1,\na\u2113k = 0 if \u2113/\u2208Nk\n(136)\nTo run algorithm (134), we only need to select the coe\ufb03cients {a\u2113k, c\u2113k} that satisfy (135) or (136); there is\nno need to worry about the intermediate coe\ufb03cients {b\u2113k} any longer since they have been blended into the\n{a\u2113k}. The scalars {c\u2113k, a\u2113k} that appear in (134) correspond to weighting coe\ufb03cients over the edge linking\nnode k to its neighbors \u2113\u2208Nk. Note that two sets of coe\ufb03cients are used to scale the data that are being\nreceived by node k: one set of coe\ufb03cients, {c\u2113k}, is used in the \ufb01rst step of (134) to scale the moment data\n{rdu,\u2113, Ru,\u2113}, and a second set of coe\ufb03cients, {a\u2113k}, is used in the second step of (134) to scale the estimates\n{\u03c8\u2113,i}. Figure 10 explains what the entries on the columns and rows of the combination matrices {A, C}\nstand for using an example with N = 6 and the matrix C for illustration. When the combination matrix is\nright-stochastic (as is the case with C), each of its rows would add up to one. On the other hand, when the\nmatrix is left-stochastic (as is the case with A), each of its columns would add up to one.\nC =\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0\n\u00d7\n\u00d7\nc13\n\u00d7\n\u00d7\n\u00d7\n\u00d7\n\u00d7\nc23\n\u00d7\n\u00d7\n\u00d7\nc31\nc32\nc33\nc34\nc35\nc36\n\u00d7\n\u00d7\nc43\n\u00d7\n\u00d7\n\u00d7\n\u00d7\n\u00d7\nc53\n\u00d7\n\u00d7\n\u00d7\n\u00d7\n\u00d7\nc63\n\u00d7\n\u00d7\n\u00d7\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb\ncoe\ufb03cients used to scale\ndata sent from node 3\nto its neighbors.\ncoe\ufb03cients used to scale\ndata received by node 3\nfrom its neighbors.\nFigure 10: Interpretation of the columns and rows of combination matrices. The pair of entries {ck\u2113, c\u2113k} correspond\nto weighting coe\ufb03cients used over the edge connecting nodes k and \u2113. When nodes (k, \u2113) are not neighbors, then\nthese weights are zero.\nAt every time instant i, the ATC strategy (134) performs two steps. The \ufb01rst step is an information\nexchange step where node k receives from its neighbors their moments {Ru,\u2113, rdu,\u2113}. Node k combines this\ninformation and uses it to update its existing estimate wk,i\u22121 to an intermediate value \u03c8k,i. All other nodes\nin the network are performing a similar step and updating their existing estimates {w\u2113,i\u22121} into intermediate\nestimates {\u03c8\u2113,i} by using information from their neighbors. The second step in (134) is an aggregation or\n28\nconsultation step where node k combines the intermediate estimates of its neighbors to obtain its update\nestimate wk,i. Again, all other nodes in the network are simultaneously performing a similar step. The\nreason for the name Adapt-then-Combine (ATC) strategy is that the \ufb01rst step in (134) will be shown to lead\nto an adaptive step, while the second step in (134) corresponds to a combination step. Hence, strategy (134)\ninvolves adaptation followed by combination or ATC for short. The reason for the quali\ufb01cation \u201cdi\ufb00usion\u201d\nis that the combination step in (134) allows information to di\ufb00use through the network in real time. This is\nbecause each of the estimates \u03c8\u2113,i is in\ufb02uenced by data beyond the immediate neighborhood of node k.\nIn the special case when C = I, so that no information exchange is performed but only the aggregation\nstep, the ATC strategy (134) reduces to:\n(ATC strategy without\ninformation exchange)\n\u03c8k,i = wk,i\u22121 + \u00b5k (rdu,k \u2212Ru,k wk,i\u22121)\nwk,i =\nX\n\u2113\u2208Nk\na\u2113k \u03c8\u2113,i\n(137)\nwhere the \ufb01rst step relies solely on the information {Ru,k, rdu,k} that is available locally at node k.\nObserve in passing that the term that appears in the information exchange step of (134) is related to the\ngradient vectors of the local costs {J\u2113(w)} evaluated at wk,i\u22121, i.e., it holds that\nrdu,\u2113\u2212Ru,\u2113wk,i\u22121 = \u2212[\u2207wJ\u2113(wk,i\u22121)]\u2217\n(138)\nso that the ATC strategy (134) can also be written in the following equivalent form:\n(ATC strategy)\n\u03c8k,i = wk,i\u22121 \u2212\u00b5k\nX\n\u2113\u2208Nk\nc\u2113k [\u2207wJ\u2113(wk,i\u22121)]\u2217\nwk,i =\nX\n\u2113\u2208Nk\na\u2113k \u03c8\u2113,i\n(139)\nThe signi\ufb01cance of this general form is that it is applicable to optimization problems involving more general\nlocal costs J\u2113(w) that are not necessarily quadratic in w, as detailed in [1\u20133] \u2014 see also Sec. 10.4. The\ntop part of Fig. 11 illustrates the two steps involved in the ATC procedure for a situation where node k\nhas three other neighbors labeled {1, 2, \u2113}. In the \ufb01rst step, node k evaluates the gradient vectors of its\nneighbors at wk,i\u22121, and subsequently aggregates the estimates {\u03c81,i, \u03c82,i, \u03c8\u2113,i} from its neighbors. The\ndotted arrows represent \ufb02ow of information towards node k from its neighbors. The solid arrows represent\n\ufb02ow of information from node k to its neighbors. The CTA di\ufb00usion strategy is discussed next.\n3.4\nCombine-then-Adapt (CTA) Di\ufb00usion Strategy\nSimilarly, if we return to (125) and add the second correction term \ufb01rst, then (126)\u2013(127) are replaced by:\n\u03c8k,i\u22121\n=\nwk,i\u22121 + \u00b5k\nX\n\u2113\u2208Nk\\{k}\nb\u2113k (wo \u2212wk,i\u22121)\n(140)\nwk,i\n=\n\u03c8k,i\u22121 + \u00b5k\nX\n\u2113\u2208Nk\nc\u2113k (rdu,\u2113\u2212Ru,\u2113wk,i\u22121)\n(141)\nFollowing similar reasoning to what we did before in the ATC case, we replace wo in step (140) by w\u2113,i\u22121 and\nreplace wk,i\u22121 in (141) by \u03c8k,i\u22121. We then introduce the same coe\ufb03cients {a\u2113k} and arrive at the following\n29\nk\n1\n2\n\u2113\nk\n1\n2\n\u2113\n\u03c81,i\n\u03c82,i\n\u03c8\u2113,i\n\u03c8k,i\nk\n1\n2\n\u2113\nk\n1\n2\n\u2113\nw1,i\u22121\nw2,i\u22121\nw\u2113,i\u22121\nwk,i\u22121\n\u2207wJ1(\u03c8k,i\u22121)\n\u2207wJ2(\u03c8k,i\u22121)\n\u2207wJ\u2113(\u03c8k,i\u22121)\n\u2207wJk(\u03c8k,i\u22121)\n\u2207wJ1(wk,i\u22121)\n\u2207wJ2(wk,i\u22121)\n\u2207wJ\u2113(wk,i\u22121)\n\u2207wJk(wk,i\u22121)\nATC\nCTA\nFigure 11: Illustration of the ATC and CTA strategies for a node k with three other neighbors {1, 2, \u2113}. The updates\ninvolve two steps: information exchange followed by aggregation in ATC and aggregation followed by information\nexchange in CTA. The dotted blue arrows represent the data received from the neighbors of node k, and the solid\nred arrows represent the data sent from node k to its neighbors.\ncombine-then-adapt (CTA) strategy:\n(CTA strategy)\n\u03c8k,i\u22121 =\nX\n\u2113\u2208Nk\na\u2113k w\u2113,i\u22121\nwk,i = \u03c8k,i\u22121 + \u00b5k\nX\n\u2113\u2208Nk\nc\u2113k (rdu,\u2113\u2212Ru,\u2113\u03c8k,i\u22121)\n(142)\nwhere the nonnegative coe\ufb03cients {c\u2113k, a\u2113k} satisfy the same conditions (106) and (133), namely,\nC1 = 1,\nAT 1 = 1\n(143)\nor, equivalently,\nfor k = 1, 2, . . . , N :\nc\u2113k \u22650,\nN\nX\nk=1\nc\u2113k = 1,\nc\u2113k = 0 if \u2113/\u2208Nk\na\u2113k \u22650,\nN\nX\n\u2113=1\na\u2113k = 1,\na\u2113k = 0 if \u2113/\u2208Nk\n(144)\nAt every time instant i, the CTA strategy (142) also consists of two steps. The \ufb01rst step is an aggregation\nstep where node k combines the existing estimates of its neighbors to obtain the intermediate estimate \u03c8k,i\u22121.\n30\nAll other nodes in the network are simultaneously performing a similar step and aggregating the estimates\nof their neighbors. The second step in (142) is an information exchange step where node k receives from\nits neighbors their moments {Rdu,\u2113, rdu,\u2113} and uses this information to update its intermediate estimate to\nwk,i. Again, all other nodes in the network are simultaneously performing a similar information exchange\nstep. The reason for the name Combine-then-Adapt (CTA) strategy is that the \ufb01rst step in (142) involves\na combination step, while the second step will be shown to lead to an adaptive step. Hence, strategy (142)\ninvolves combination followed by adaptation or CTA for short. The reason for the quali\ufb01cation \u201cdi\ufb00usion\u201d\nis that the combination step of (142) allows information to di\ufb00use through the network in real time.\nIn the special case when C = I, so that no information exchange is performed but only the aggregation\nstep, the CTA strategy (142) reduces to:\n(CTA strategy without\ninformation exchange)\n\u03c8k,i\u22121 =\nX\n\u2113\u2208Nk\na\u2113k w\u2113,i\u22121\nwk,i = \u03c8k,i\u22121 + \u00b5k (rdu,k \u2212Ru,k \u03c8k,i\u22121)\n(145)\nwhere the second step relies solely on the information {Ru,k, rdu,k} that is available locally at node k. Again,\nthe CTA strategy (142) can be rewritten in terms of the gradient vectors of the local costs {J\u2113(w)} as follows:\n(CTA strategy)\n\u03c8k,i\u22121 =\nX\n\u2113\u2208Nk\na\u2113k w\u2113,i\u22121\nwk,i = \u03c8k,i\u22121 \u2212\u00b5k\nX\n\u2113\u2208Nk\nc\u2113k [\u2207wJ\u2113(\u03c8k,i\u22121)]\u2217\n(146)\nThe bottom part of Fig. 11 illustrates the two steps involved in the CTA procedure for a situation where\nnode k has three other neighbors labeled {1, 2, \u2113}.\nIn the \ufb01rst step, node k aggregates the estimates\n{w1,i\u22121, w2,i\u22121, w\u2113,i\u22121} from its neighbors, and subsequently performs information exchange by evaluating\nthe gradient vectors of its neighbors at \u03c8k,i\u22121.\n3.5\nUseful Properties of Di\ufb00usion Strategies\nNote that the structure of the ATC and CTA di\ufb00usion strategies (134) and (142) are fundamentally the\nsame: the di\ufb00erence between the implementations lies in which variable we choose to correspond to the\nupdated weight estimate wk,i. In the ATC case, we choose the result of the combination step to be wk,i,\nwhereas in the CTA case we choose the result of the adaptation step to be wk,i.\nFor ease of reference, Table 2 lists the steepest-descent di\ufb00usion algorithms derived in the previous\nsections. The derivation of the ATC and CTA strategies (134) and (142) followed the approach proposed in\n[18,27]. CTA estimation schemes were \ufb01rst proposed in the works [35\u201339], and later extended in [18,27,32,33].\nThe earlier versions of CTA in [35\u201337] used the choice C = I. This form of the algorithm with C = I, and\nwith the additional constraint that the step-sizes \u00b5k should be time-dependent and decay towards zero as\ntime progresses, was later applied by [40,41] to solve distributed optimization problems that require all nodes\nto reach consensus or agreement. Likewise, special cases of the ATC estimation scheme (134), involving an\ninformation exchange step followed by an aggregation step, \ufb01rst appeared in the work [28] on di\ufb00usion least-\nsquares schemes and subsequently in the works [18,29\u201333] on distributed mean-square-error and state-space\nestimation methods. A special case of the ATC strategy (134) corresponding to the choice C = I with\ndecaying step-sizes was adopted in [34] to ensure convergence towards a consensus state. Di\ufb00usion strategies\nof the form (134) and (142) (or, equivalently, (139) and (146)) are general in several respects:\n(1) These strategies do not only di\ufb00use the local weight estimates, but they can also di\ufb00use the local\ngradient vectors. In other words, two sets of combination coe\ufb03cients {a\u2113k, c\u2113k} are used.\n31\nTable 2: Summary of steepest-descent di\ufb00usion strategies for the distributed optimization of general problems of the\nform (92), and their specialization to the case of mean-square-error (MSE) individual cost functions given by (93).\nAlgorithm\nRecursions\nReference\nATC strategy\n(general case)\n\u03c8k,i = wk,i\u22121 \u2212\u00b5k\nX\n\u2113\u2208Nk\nc\u2113k [\u2207wJ\u2113(wk,i\u22121)]\u2217\nwk,i =\nX\n\u2113\u2208Nk\na\u2113k \u03c8\u2113,i\n(139)\nATC strategy\n(MSE costs)\n\u03c8k,i = wk,i\u22121 + \u00b5k\nX\n\u2113\u2208Nk\nc\u2113k (rdu,\u2113\u2212Ru,\u2113wk,i\u22121)\nwk,i =\nX\n\u2113\u2208Nk\na\u2113k \u03c8\u2113,i\n(134)\nCTA strategy\n(general case)\n\u03c8k,i\u22121 =\nX\n\u2113\u2208Nk\na\u2113k w\u2113,i\u22121\nwk,i = \u03c8k,i\u22121 \u2212\u00b5k\nX\n\u2113\u2208Nk\nc\u2113k [\u2207wJ\u2113(\u03c8k,i\u22121)]\u2217\n(146)\nCTA strategy\n(MSE costs)\n\u03c8k,i\u22121 =\nX\n\u2113\u2208Nk\na\u2113k w\u2113,i\u22121\nwk,i = \u03c8k,i\u22121 + \u00b5k\nX\n\u2113\u2208Nk\nc\u2113k (rdu,\u2113\u2212Ru,\u2113\u03c8k,i\u22121)\n(142)\n(2) In the derivation that led to the di\ufb00usion strategies, the combination matrices C and A are only\nrequired to be right-stochastic (for C) and left-stochastic (for A). In comparison, it is common in\nconsensus-type strategies to require the corresponding combination matrix A to be doubly stochastic\n(i.e., its rows and columns should add up to one) \u2014 see, e.g., App. E and [40,42\u201344].\n(3) As the analysis in Sec. 6 will reveal, ATC and CTA strategies do not force nodes to converge to an\nagreement about the desired parameter vector wo, as is common in consensus-type strategies (see\nApp. E and [40, 45\u201351]). Forcing nodes to reach agreement on wo ends up limiting the adaptation\nand learning abilities of these nodes, as well as their ability to react to information in real-time.\nNodes in di\ufb00usion networks enjoy more \ufb02exibility in the learning process, which allows their individual\nestimates, {wk,i}, to tend to values that lie within a reasonable mean-square-deviation (MSD) level\nfrom the optimal solution, wo. Multi-agent systems in nature behave in this manner; they do not\nrequire exact agreement among their agents (see, e.g., [8\u201310]).\n(4) The step-size parameters {\u00b5k} are not required to depend on the time index i and are not required\nto vanish as i \u2192\u221e(as is common in many works on distributed optimization, e.g., [22, 40, 52, 53]).\nInstead, the step-sizes can assume constant values, which is a critical property to endow networks\nwith continuous adaptation and learning abilities. An important contribution in the study of di\ufb00usion\nstrategies is to show that distributed optimization is still possible even for constant step-sizes, in\naddition to the ability to perform adaptation, learning, and tracking. Sections 5 and 6 highlight the\nconvergence properties of the di\ufb00usion strategies \u2014 see also [1\u20133] for results pertaining to more general\ncost functions.\n32\n(5) Even the combination weights {a\u2113k, c\u2113k} can be adapted, as we shall discuss later in Sec. 8.3. In this\nway, di\ufb00usion strategies allow multiple layers of adaptation: the nodes perform adaptive processing,\nthe combination weights can be adapted, and even the topology can be adapted especially for mobile\nnetworks [8].\n4\nAdaptive Di\ufb00usion Strategies\nThe distributed ATC and CTA steepest-descent strategies (134) and (142) for determining the wo that solves\n(92)\u2013(93) require knowledge of the statistical information {Ru,k, rdu,k}. These moments are needed in order\nto be able to evaluate the gradient vectors that appear in (134) and (142), namely, the terms:\n\u2212[\u2207wJ\u2113(wk,i\u22121)]\u2217\n=\n(rdu,\u2113\u2212Ru,\u2113wk,i\u22121)\n(147)\n\u2212[\u2207wJ\u2113(\u03c8k,i\u22121)]\u2217\n=\n(rdu,\u2113\u2212Ru,\u2113\u03c8k,i\u22121)\n(148)\nfor all \u2113\u2208Nk. However, the moments {Ru,\u2113, rdu,\u2113} are often not available beforehand, which means that the\ntrue gradient vectors are generally not available. Instead, the agents have access to observations {dk(i), uk,i}\nof the random processes {dk(i), uk,i}. There are many ways by which the true gradient vectors can be\napproximated by using these observations. Recall that, by de\ufb01nition,\nRu,\u2113\n\u2206= Eu\u2217\n\u2113,iu\u2113,i,\nrdu,\u2113\n\u2206= Ed\u2113(i)u\u2217\n\u2113,i\n(149)\nOne common stochastic approximation method is to drop the expectation operator from the de\ufb01nitions of\n{Ru,\u2113, rdu,\u2113} and to use the following instantaneous approximations instead [4\u20137]:\nRu,\u2113\u2248u\u2217\n\u2113,iu\u2113,i,\nrdu,\u2113\u2248d\u2113(i)u\u2217\n\u2113,i\n(150)\nIn this case, the approximate gradient vectors become:\n(rdu,\u2113\u2212Ru,\u2113wk,i\u22121)\n\u2248\nu\u2217\n\u2113,i [d\u2113(i) \u2212u\u2113,i wk,i\u22121]\n(151)\n(rdu,\u2113\u2212Ru,\u2113\u03c8k,i\u22121)\n\u2248\nu\u2217\n\u2113,i [d\u2113(i) \u2212u\u2113,i \u03c8k,i\u22121]\n(152)\nSubstituting into the ATC and CTA steepest-descent strategies (134) and (142), we arrive at the following\nadaptive implementations of the di\ufb00usion strategies for i \u22650:\n(adaptive ATC strategy)\n\u03c8k,i = wk,i\u22121 + \u00b5k\nX\n\u2113\u2208Nk\nc\u2113k u\u2217\n\u2113,i [d\u2113(i) \u2212u\u2113,iwk,i\u22121]\nwk,i =\nX\n\u2113\u2208Nk\na\u2113k \u03c8\u2113,i\n(153)\nand\n(adaptive CTA strategy)\n\u03c8k,i\u22121 =\nX\n\u2113\u2208Nk\na\u2113k w\u2113,i\u22121\nwk,i = \u03c8k,i\u22121 + \u00b5k\nX\n\u2113\u2208Nk\nc\u2113k u\u2217\n\u2113,i [d\u2113(i) \u2212u\u2113,i \u03c8k,i\u22121]\n(154)\nwhere the coe\ufb03cients {a\u2113k, c\u2113k} are chosen to satisfy:\nfor k = 1, 2, . . . , N :\nc\u2113k \u22650,\nN\nX\nk=1\nc\u2113k = 1,\nc\u2113k = 0 if \u2113/\u2208Nk\na\u2113k \u22650,\nN\nX\n\u2113=1\na\u2113k = 1,\na\u2113k = 0 if \u2113/\u2208Nk\n(155)\n33\nThe adaptive implementations usually start from the initial conditions w\u2113,\u22121 = 0 for all \u2113, or from some\nother convenient initial values. Clearly, in view of the approximations (151)\u2013(152), the successive iterates\n{wk,i, \u03c8k,i, \u03c8k,i\u22121} that are generated by the above adaptive implementations are di\ufb00erent from the iterates\nthat result from the steepest-descent implementations (134) and (142).\nNevertheless, we shall continue\nto use the same notation for these variables for ease of reference.\nOne key advantage of the adaptive\nimplementations (153)\u2013(154) is that they enable the agents to react to changes in the underlying statistical\ninformation {rdu,\u2113, Ru,\u2113} and to changes in wo. This is because these changes end up being re\ufb02ected in the\ndata realizations {dk(i), uk,i}. Therefore, adaptive implementations have an innate tracking and learning\nability that is of paramount signi\ufb01cance in practice.\nWe say that the stochastic gradient approximations (151)\u2013(152) introduce gradient noise into each step\nof the recursive updates (153)\u2013(154). This is because the updates (153)\u2013(154) can be interpreted as corre-\nsponding to the following forms:\n(adaptive ATC strategy)\n\u03c8k,i = wk,i\u22121 \u2212\u00b5k\nX\n\u2113\u2208Nk\nc\u2113k\n\\\n[\u2207wJ\u2113(wk,i\u22121)]\n\u2217\nwk,i =\nX\n\u2113\u2208Nk\na\u2113k \u03c8\u2113,i\n(156)\nand\n(adaptive CTA strategy)\n\u03c8k,i\u22121 =\nX\n\u2113\u2208Nk\na\u2113k w\u2113,i\u22121\nwk,i = \u03c8k,i\u22121 \u2212\u00b5k\nX\n\u2113\u2208Nk\nc\u2113k\n\\\n[\u2207wJ\u2113(\u03c8k,i\u22121)]\n\u2217\n(157)\nwhere the true gradient vectors, {\u2207wJ\u2113(\u00b7)}, have been replaced by approximations, { \\\n\u2207wJ\u2113(\u00b7)} \u2014 compare\nwith (139) and (146). The signi\ufb01cance of the alternative forms (156)\u2013(157) is that they are applicable to\noptimization problems involving more general local costs J\u2113(w) that are not necessarily quadratic, as detailed\nin [2, 3]; see also Sec. 10.4. In the next section, we examine how gradient noise a\ufb00ects the performance of\nthe di\ufb00usion strategies and how close the successive estimates {wk,i} get to the desired optimal solution wo.\nTable 3 lists several of the adaptive di\ufb00usion algorithms derived in this section.\nThe operation of the adaptive di\ufb00usion strategies is similar to the operation of the steepest-descent\ndi\ufb00usion strategies of the previous section. Thus, note that at every time instant i, the ATC strategy (153)\nperforms two steps; as illustrated in Fig. 12. The \ufb01rst step is an information exchange step where node\nk receives from its neighbors their information {d\u2113(i), u\u2113,i}. Node k combines this information and uses it\nto update its existing estimate wk,i\u22121 to an intermediate value \u03c8k,i. All other nodes in the network are\nperforming a similar step and updating their existing estimates {w\u2113,i\u22121} into intermediate estimates {\u03c8\u2113,i}\nby using information from their neighbors. The second step in (153) is an aggregation or consultation step\nwhere node k combines the intermediate estimates {\u03c8\u2113,i} of its neighbors to obtain its update estimate wk,i.\nAgain, all other nodes in the network are simultaneously performing a similar step. In the special case when\nC = I, so that no information exchange is performed but only the aggregation step, the ATC strategy (153)\nreduces to:\n(adaptive ATC strategy\nwithout information exchange)\n\u03c8k,i = wk,i\u22121 + \u00b5k u\u2217\nk,i [dk(i) \u2212uk,iwk,i\u22121]\nwk,i =\nX\n\u2113\u2208Nk\na\u2113k \u03c8\u2113,i\n(158)\n34\nTable 3: Summary of adaptive di\ufb00usion strategies for the distributed optimization of general problems of the form\n(92), and their specialization to the case of mean-square-error (MSE) individual cost functions given by (93). These\nadaptive solutions rely on stochastic approximations.\nAlgorithm\nRecursions\nReference\nAdaptive ATC strategy\n(general case)\n\u03c8k,i = wk,i\u22121 \u2212\u00b5k\nX\n\u2113\u2208Nk\nc\u2113k\n\\\n[\u2207wJ\u2113(wk,i\u22121)]\n\u2217\nwk,i =\nX\n\u2113\u2208Nk\na\u2113k \u03c8\u2113,i\n(156)\nAdaptive ATC strategy\n(MSE costs)\n\u03c8k,i = wk,i\u22121 + \u00b5k\nX\n\u2113\u2208Nk\nc\u2113k u\u2217\n\u2113,i [d\u2113(i) \u2212u\u2113,iwk,i\u22121]\nwk,i =\nX\n\u2113\u2208Nk\na\u2113k \u03c8\u2113,i\n(153)\nAdaptive ATC strategy\n(MSE costs)\n(no information exchange)\n\u03c8k,i = wk,i\u22121 + \u00b5k u\u2217\nk,i [dk(i) \u2212uk,iwk,i\u22121]\nwk,i =\nX\n\u2113\u2208Nk\na\u2113k \u03c8\u2113,i\n(158)\nAdaptive CTA strategy\n(general case)\n\u03c8k,i\u22121 =\nX\n\u2113\u2208Nk\na\u2113k w\u2113,i\u22121\nwk,i = \u03c8k,i\u22121 \u2212\u00b5k\nX\n\u2113\u2208Nk\nc\u2113k\n\\\n[\u2207wJ\u2113(\u03c8k,i\u22121)]\n\u2217\n(157)\nAdaptive CTA strategy\n(MSE costs)\n\u03c8k,i\u22121 =\nX\n\u2113\u2208Nk\na\u2113k w\u2113,i\u22121\nwk,i = \u03c8k,i\u22121 + \u00b5k\nX\n\u2113\u2208Nk\nc\u2113k u\u2217\n\u2113,i [d\u2113(i) \u2212u\u2113,i \u03c8k,i\u22121]\n(154)\nAdaptive CTA strategy\n(MSE costs)\n(no information exchange)\n\u03c8k,i\u22121 =\nX\n\u2113\u2208Nk\na\u2113k w\u2113,i\u22121\nwk,i = \u03c8k,i\u22121 + \u00b5k u\u2217\nk,i [dk(i) \u2212uk,i \u03c8k,i\u22121]\n(159)\n35\nAdapt-then-Combine (ATC) Strategy\n(node k, time i)\n(b) Aggregation/consultation step:\n(a) Information exchange step:\ne\u2113(i) = d\u2113(i) \u2212u\u2113,iwk,i\u22121\n\u03c8k,i = wk,i\u22121 + \u00b5k\n\u0001\n\u2113\u2208Nk\nc\u2113k u\u2217\n\u2113,ie\u2113(i)\nwk,i =\n\u0001\n\u2113\u2208Nk\na\u2113k \u03c8\u2113,i\n\u03c81,i\n\u03c8\u2113,i\n\u03c82,i\nk\n1\n2\n\u2113\nNk\nFigure 12: Illustration of the adaptive ATC strategy, which involves two steps: information exchange followed by\naggregation.\nLikewise, at every time instant i, the CTA strategy (154) also consists of two steps \u2013 see Fig. 13. The\n\ufb01rst step is an aggregation step where node k combines the existing estimates of its neighbors to obtain the\nintermediate estimate \u03c8k,i\u22121. All other nodes in the network are simultaneously performing a similar step\nand aggregating the estimates of their neighbors. The second step in (154) is an information exchange step\nwhere node k receives from its neighbors their information {d\u2113(i), u\u2113,i} and uses this information to update\nits intermediate estimate to wk,i. Again, all other nodes in the network are simultaneously performing a\nsimilar information exchange step. In the special case when C = I, so that no information exchange is\nperformed but only the aggregation step, the CTA strategy (154) reduces to:\nCombine-then-Adapt (CTA) Strategy\n(node k, time i)\n(a) Aggregation/consultation step:\n(b) Information exchange step:\n\u03c8k,i\u22121 =\n\u0001\n\u2113\u2208Nk\na\u2113k w\u2113,i\u22121\ne\u2113(i) = d\u2113(i) \u2212u\u2113,i\u03c8k,i\u22121\nwk,i = \u03c8k,i\u22121 + \u00b5k\n\u0001\n\u2113\u2208Nk\nc\u2113k u\u2217\n\u2113,ie\u2113(i)\nw1,i\u22121\nw2,i\u22121\nw\u2113,i\u22121\nk\n1\n2\n\u2113\nNk\nFigure 13: Illustration of the adaptive CTA strategy, which involves two steps: aggregation followed by information\nexchange.\n36\n(adaptive CTA strategy\nwithout information exchange)\n\u03c8k,i\u22121 =\nX\n\u2113\u2208Nk\na\u2113k w\u2113,i\u22121\nwk,i = \u03c8k,i\u22121 + \u00b5k u\u2217\nk,i [dk(i) \u2212uk,i \u03c8k,i\u22121]\n(159)\nWe further note that the adaptive ATC and CTA strategies (153)\u2013(154) reduce to the non-cooperative\nadaptive solution (22)\u2013(23), where each node k runs its own individual LMS \ufb01lter, when the coe\ufb03cients\n{a\u2113k, c\u2113k} are selected as\na\u2113k = \u03b4\u2113k = c\u2113k\n(non-cooperative case)\n(160)\nwhere \u03b4\u2113k denotes the Kronecker delta function:\n\u03b4\u2113k\n\u2206=\n\u001a\n1,\n\u2113= k\n0,\notherwise\n(161)\nIn terms of the combination matrices A and C, this situation corresponds to setting\nA = IN = C\n(non-cooperative case)\n(162)\n5\nPerformance of Steepest-Descent Di\ufb00usion Strategies\nBefore studying in some detail the mean-square performance of the adaptive di\ufb00usion implementations (153)\u2013\n(154), and the in\ufb02uence of gradient noise, we examine \ufb01rst the convergence behavior of the steepest-descent\ndi\ufb00usion strategies (134) and (142), which employ the true gradient vectors. Doing so, will help introduce\nthe necessary notation and highlight some features of the analysis in preparation for the more challenging\ntreatment of the adaptive strategies in Sec. 6.\n5.1\nGeneral Di\ufb00usion Model\nRather than study the performance of the ATC and CTA steepest-descent strategies (134) and (142) sep-\narately, it is useful to introduce a more general description that includes the ATC and CTA recursions as\nspecial cases. Thus, consider a distributed steepest-descent di\ufb00usion implementation of the following general\nform for i \u22650:\n\u03c6k,i\u22121\n=\nX\n\u2113\u2208Nk\na1,\u2113k w\u2113,i\u22121\n(163)\n\u03c8k,i\n=\n\u03c6k,i\u22121 + \u00b5k\nX\n\u2113\u2208Nk\nc\u2113k [rdu,\u2113\u2212Ru,\u2113\u03c6k,i\u22121]\n(164)\nwk,i\n=\nX\n\u2113\u2208Nk\na2,\u2113k \u03c8\u2113,i\n(165)\nwhere the scalars {a1,\u2113k, c\u2113k, a2,\u2113k} denote three sets of non-negative real coe\ufb03cients corresponding to the\n(\u2113, k) entries of N \u00d7N combination matrices {A1, C, A2}, respectively. These matrices are assumed to satisfy\nthe conditions:\nAT\n1 1 = 1,\nC1 = 1,\nAT\n2 1 = 1\n(166)\n37\nso that {A1, A2} are left stochastic and C is right-stochastic, i.e.,\nfor k = 1, 2, . . . , N :\nc\u2113k \u22650,\nN\nX\nk=1\nc\u2113k = 1,\nc\u2113k = 0 if \u2113/\u2208Nk\na1,\u2113k \u22650,\nN\nX\n\u2113=1\na1,\u2113k = 1,\na1,\u2113k = 0 if \u2113/\u2208Nk\na2,\u2113k \u22650,\nN\nX\n\u2113=1\na2,\u2113k = 1,\na2,\u2113k = 0 if \u2113/\u2208Nk\n(167)\nDi\ufb00erent choices for {A1, C, A2} correspond to di\ufb00erent cooperation modes. For example, the choice A1 =\nIN and A2 = A corresponds to the ATC implementation (134), while the choice A1 = A and A2 = IN\ncorresponds to the CTA implementation (142). Likewise, the choice C = IN corresponds to the case in\nwhich the nodes only share weight estimates and the distributed di\ufb00usion recursions (163)\u2013(165) become\n\u03c6k,i\u22121\n=\nX\n\u2113\u2208Nk\na1,\u2113k w\u2113,i\u22121\n(168)\n\u03c8k,i\n=\n\u03c6k,i\u22121 + \u00b5k (rdu,k \u2212Ru,k \u03c6k,i\u22121)\n(169)\nwk,i\n=\nX\n\u2113\u2208Nk\na2,\u2113k \u03c8\u2113,i\n(170)\nFurthermore, the choice A1 = A2 = C = IN corresponds to the non-cooperative mode of operation, in which\ncase the recursions reduce to the classical (stand-alone) steepest-descent recursion [4\u20137], where each node\nminimizes individually its own quadratic cost Jk(w), de\ufb01ned earlier in (97):\nwk,i\n=\nwk,i\u22121 + \u00b5k [rdu,k \u2212Ru,k wk,i\u22121] ,\ni \u22650\n(171)\nTable 4: Di\ufb00erent choices for the combination matrices {A1, A2, C} in (163)\u2013(165) correspond to di\ufb00erent cooperation\nstrategies.\nA1\nA2\nC\nCooperation Mode\nIN\nA\nC\nATC strategy (134).\nIN\nA\nIN\nATC strategy (137) without information exchange.\nA\nIN\nC\nCTA strategy (142).\nA\nIN\nIN\nCTA strategy (145) without information exchange.\nIN\nIN\nIN\nnon-cooperative steepest-descent (171).\n5.2\nError Recursions\nOur objective is to examine whether, and how fast, the weight estimates {wk,i} from the distributed imple-\nmentation (163)\u2013(165) converge towards the solution wo of (92)\u2013(93). To do so, we introduce the M \u00d7 1\nerror vectors:\ne\u03c6k,i\n\u2206=\nwo \u2212\u03c6k,i\n(172)\ne\u03c8k,i\n\u2206=\nwo \u2212\u03c8k,i\n(173)\newk,i\n\u2206=\nwo \u2212wk,i\n(174)\n38\nEach of these error vectors measures the residual relative to the desired minimizer wo. Now recall from (100)\nthat\nrdu,k = Ru,k wo\n(175)\nThen, subtracting wo from both sides of the relations in (163)\u2013(165) we get\ne\u03c6k,i\u22121\n=\nX\n\u2113\u2208Nk\na1,\u2113k ew\u2113,i\u22121\n(176)\ne\u03c8k,i\n=\n \nIM \u2212\u00b5k\nX\n\u2113\u2208Nk\nc\u2113k Ru,\u2113\n!\ne\u03c6k,i\u22121\n(177)\newk,i\n=\nX\n\u2113\u2208Nk\na2,\u2113k e\u03c8\u2113,i\n(178)\nWe can describe these relations more compactly by collecting the information from across the network into\nblock vectors and matrices. We collect the error vectors from across all nodes into the following N \u00d7 1 block\nvectors, whose individual entries are of size M \u00d7 1 each:\ne\u03c8i\n\u2206=\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0\ne\u03c81,i\ne\u03c82,i\n...\ne\u03c8N,i\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb\n,\ne\u03c6i\n\u2206=\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0\ne\u03c61,i\ne\u03c62,i\n...\ne\u03c6N,i\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb\n,\newi\n\u2206=\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8f0\new1,i\new2,i\n...\newN,i\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fb\n(179)\nThe block quantities { e\u03c8i, e\u03c6i, ewi} represent the state of the errors across the network at time i. Likewise, we\nintroduce the following N \u00d7 N block diagonal matrices, whose individual entries are of size M \u00d7 M each:\nM\n\u2206=\ndiag{ \u00b51IM, \u00b52IM, . . . , \u00b5NIM }\n(180)\nR\n\u2206=\ndiag\n( X\n\u2113\u2208N1\nc\u21131 Ru,\u2113,\nX\n\u2113\u2208N2\nc\u21132 Ru,\u2113, . . . ,\nX\n\u2113\u2208NN\nc\u2113N Ru,\u2113\n)\n(181)\nEach block diagonal entry of R, say, the k-th entry, contains the combination of the covariance matrices in\nthe neighborhood of node k. We can simplify the notation by denoting these neighborhood combinations as\nfollows:\nRk\n\u2206=\nX\n\u2113\u2208Nk\nc\u2113k Ru,\u2113\n(182)\nso that R becomes\nR\n\u2206= diag { R1, R2, . . . , RN }\n(when C \u0338= I)\n(183)\nIn the special case when C = IN, the matrix R reduces to\nRu = diag{Ru,1, Ru,2, . . . , Ru,N}\n(when C = I)\n(184)\nwith the individual covariance matrices appearing on its diagonal; we denote R by Ru in this special case.\nWe further introduce the Kronecker products\nA1\n\u2206= A1 \u2297IM,\nA2\n\u2206= A2 \u2297IM\n(185)\n39\nThe matrix A1 is an N \u00d7 N block matrix whose (\u2113, k) block is equal to a1,\u2113kIM.\nLikewise, for A2.\nIn\nother words, the Kronecker transformation de\ufb01ned above simply replaces the matrices {A1, A2} by block\nmatrices {A1, A2} where each entry {a1,\u2113k, a2,\u2113k} in the original matrices is replaced by the diagonal matrices\n{a1,\u2113kIM, a2,\u2113kIM}. For ease of reference, Table 5 lists the various symbols that have been de\ufb01ned so far,\nand others that will be de\ufb01ned in the sequel.\nTable 5: De\ufb01nitions of network variables used throughout the analysis.\nVariable\nEquation\nA1 = A1 \u2297IM\n(185)\nA2 = A2 \u2297IM\n(185)\nC = C \u2297IM\n(245)\nRk =\nX\n\u2113\u2208Nk\nc\u2113k Ru,\u2113\n(182)\nR = diag {R1, R2, . . . , RN }\n(183)\nRu = diag{Ru,1, Ru,2, . . . , Ru,N}\n(241)\nRv = diag{\u03c32\nv,1, \u03c32\nv,2, . . . , \u03c32\nv,N}\n(319)\nM = diag{ \u00b51IM, \u00b52IM, . . . , \u00b5NIM }\n(180)\nS = diag{\u03c32\nv,1Ru,1, \u03c32\nv,2Ru,2, . . . , \u03c32\nv,NRu,N}\n(241)\nG = AT\n2 MCT\n(263)\nB = AT\n2 (INM \u2212MR) AT\n1\n(264)\nY = GSGT\n(280)\nF \u2248BT \u2297B\u2217\n(277)\nJk = diag{ 0M, . . . , 0M, IM, 0M, . . . , 0M }\n(294)\nTk = diag{ 0M, . . . , 0M, Ru,k, 0M, . . . , 0M }\n(298)\nReturning to (176)\u2013(178), we conclude that the following relations hold for the block quantities:\ne\u03c6i\u22121\n=\nAT\n1 ewi\u22121\n(186)\ne\u03c8i\n=\n(INM \u2212MR) e\u03c6i\u22121\n(187)\newi\n=\nAT\n2 e\u03c8i\n(188)\nso that the network weight error vector, ewi, ends up evolving according to the following dynamics:\newi = AT\n2 (INM \u2212MR) AT\n1 ewi\u22121, i \u22650\n(di\ufb00usion strategy)\n(189)\nFor comparison purposes, if each node in the network minimizes its own cost function, Jk(w), separately\nfrom the other nodes and uses the non-cooperative steepest-descent strategy (171), then the weight error\nvector across all N nodes would evolve according to the following alternative dynamics:\newi = (INM \u2212MRu) ewi\u22121, i \u22650\n(non-cooperative strategy)\n(190)\nwhere the matrices A1 and A2 do not appear, and R is replaced by Ru from (184). This recursion is a\nspecial case of (189) when A1 = A2 = C = IN.\n40\n5.3\nConvergence Behavior\nNote from (189) that the evolution of the weight error vector involves block vectors and block matrices; this\nwill be characteristic of the distributed implementations that we consider in this article. To examine the\nstability and convergence properties of recursions that involve such block quantities, it becomes useful to\nrely on a certain block vector norm. In App. D, we describe a so-called block maximum block and establish\nsome of its useful properties. The results of the appendix will be used extensively in our exposition. It is\ntherefore advisable for the reader to review the properties stated in the appendix at this stage.\nUsing the result of Lemma D.6, we can establish the following useful statement about the convergence of\nthe steepest-descent di\ufb00usion strategy (163)\u2013(165). The result establishes that all nodes end up converging\nto the optimal solution wo if the nodes employ positive step-sizes \u00b5k that are small enough; the lemma\nprovides a su\ufb03cient bound on the {\u00b5k}.\nTheorem 5.1. (Convergence to Optimal Solution) Consider the problem of optimizing the global cost\n(92) with the individual cost functions given by (93). Pick a right stochastic matrix C and left stochastic ma-\ntrices A1 and A2 satisfying (166) or (167); these matrices de\ufb01ne the network topology and how information is\nshared over neighborhoods. Assume each node in the network runs the (distributed) steepest-descent di\ufb00usion\nalgorithm (163)\u2013(165). Then, all estimates {wk,i} across the network converge to the optimal solution wo if\nthe positive step-size parameters {\u00b5k} satisfy\n\u00b5k <\n2\n\u03bbmax(Rk)\n(191)\nwhere the neighborhood covariance matrix Rk is de\ufb01ned by (182).\nProof. The weight error vector ewi converges to zero if, and only if, the coe\ufb03cient matrix AT\n2 (INM \u2212MR) AT\n1 in\n(189) is a stable matrix (meaning that all its eigenvalues lie strictly inside the unit disc).\nFrom property (605)\nestablished in App. D, we know that AT\n2 (INM \u2212MR) AT\n1 is stable if the block diagonal matrix (INM \u2212MR) is\nstable. It is now straightforward to verify that condition (191) ensures the stability of (INM \u2212MR). It follows that\newi \u2212\u21920\nas\ni \u2212\u2192\u221e\n(192)\nObserve that the stability condition (191) does not depend on the speci\ufb01c combination matrices A1 and A2.\nThus, as long as these matrices are chosen to be left-stochastic, the weight-error vectors will converge to\nzero under condition (191) no matter what {A1, A2} are. Only the combination matrix C in\ufb02uences the\ncondition on the step-size through the neighborhood covariance matrices {Rk}. Observe further that the\nstatement of the lemma does not require the network to be connected. Moreover, when C = I, in which\ncase the nodes only share weight estimates and do not share the neighborhood moments {rdu,\u2113, Ru,\u2113}, as in\n(168)\u2013(170), condition (191) becomes\n\u00b5k <\n2\n\u03bbmax(Ru,k)\n(cooperation with C = I)\n(193)\nin terms of the actual covariance matrices {Ru,k}. Results (191) and (193) are reminiscent of a classical\nresult for stand-alone steepest-descent algorithms, as in the non-cooperative case (171), where it is known\nthat the estimate by each individual node in this case will converge to wo if, and only if, its positive step-size\nsatis\ufb01es\n\u00b5k <\n2\n\u03bbmax(Ru,k)\n(non-cooperative case (171) with A1 = A2 = C = IN)\n(194)\n41\nThis is the same condition as (193) for the case C = I.\nThe following statement provides a bi-directional statement that ensures convergence of the (distributed)\nsteepest-descent di\ufb00usion strategy (163)\u2013(165) for any choice of left-stochastic combination matrices A1 and\nA2.\nTheorem 5.2. (Convergence for Arbitrary Combination Matrices) Consider the problem of opti-\nmizing the global cost (92) with the individual cost functions given by (93). Pick a right stochastic matrix\nC satisfying (166). Then, the estimates {wk,i} generated by (163)\u2013(165) converge to wo, for all choices of\nleft-stochastic matrices A1 and A2 satisfying (166) if, and only if,\n\u00b5k <\n2\n\u03bbmax(Rk)\n(195)\nProof. The result follows from property (b) of Corollary D.1, which is established in App. D.\nMore importantly, we can verify that under fairly general conditions, employing the steepest-descent\ndi\ufb00usion strategy (163)\u2013(165) enhances the convergence rate of the error vector towards zero relative to the\nnon-cooperative strategy (171). The next three results establish this fact when C is a doubly stochastic\nmatrix, i.e., it has non-negative entries and satis\ufb01es\nC1 = 1,\nCT 1 = 1\n(196)\nwith both its rows and columns adding up to one. Compared to the earlier right-stochastic condition on C\nin (105), we are now requiring\nX\n\u2113\u2208Nk\nck\u2113= 1,\nX\n\u2113\u2208Nk\nc\u2113k = 1\n(197)\nFor example, these conditions are satis\ufb01ed when C is right stochastic and symmetric. They are also satis\ufb01ed\nfor C = I, when only weight estimates are shared as in (168)\u2013(170); this latter case covers the ATC and\nCTA di\ufb00usion strategies (137) and (145), which do not involve information exchange.\nTheorem 5.3. (Convergence Rate is Enhanced: Uniform Step-Sizes) Consider the problem of opti-\nmizing the global cost (92) with the individual cost functions given by (93). Pick a doubly stochastic matrix C\nsatisfying (196) and left stochastic matrices A1 and A2 satisfying (166). Consider two modes of operation. In\none mode, each node in the network runs the (distributed) steepest-descent di\ufb00usion algorithm (163)\u2013(165).\nIn the second mode, each node operates individually and runs the non-cooperative steepest-descent algorithm\n(171). In both cases, the positive step-sizes used by all nodes are assumed to be the same, say, \u00b5k = \u00b5 for\nall k, and the value of \u00b5 is chosen to satisfy the required stability conditions (191) and (194), which are met\nby selecting\n\u00b5 <\nmin\n1\u2264k\u2264N\n\u001a\n2\n\u03bbmax(Ru,k)\n\u001b\n(198)\nIt then holds that the magnitude of the error vector, \u2225ewi\u2225, in the di\ufb00usion case decays to zero more rapidly\nthan in the non-cooperative case. In other words, di\ufb00usion cooperation enhances convergence rate.\nProof. Let us \ufb01rst establish that any positive step-size \u00b5 satisfying (198) will satisfy both stability conditions (191)\nand (194). It is obvious that (194) is satis\ufb01ed. We verify that (191) is also satis\ufb01ed when C is doubly stochastic. In\nthis case, each neighborhood covariance matrix, Rk, becomes a convex combination of individual covariance matrices\n{Ru,\u2113}, i.e.,\nRk =\nX\n\u2113\u2208Nk\nc\u2113kRu,\u2113\nwhere now\nX\n\u2113\u2208Nk\nc\u2113k = 1\n(when C is doubly stochastic)\n42\nTo proceed, we recall that the spectral norm (maximum singular value) of any matrix X is a convex function of\nX [56]. Moreover, for Hermitian matrices X, their spectral norms coincide with their spectral radii (largest eigenvalue\nmagnitude). Then, Jensen\u2019s inequality [56] states that for any convex function f(\u00b7) it holds that\nf\n X\nm\n\u03b8mXm\n!\n\u2264\nX\nm\n\u03b8mf(Xm)\nfor Hermitian matrices Xm and nonnegative scalars \u03b8m that satisfy\nX\nm\n\u03b8m = 1\nChoosing f(\u00b7) as the spectral radius function, and applying it to the de\ufb01nition of Rk above, we get\n\u03c1(Rk)\n=\n\u03c1\n\uf8eb\n\uf8edX\n\u2113\u2208Nk\nc\u2113kRu,\u2113\n\uf8f6\n\uf8f8\n\u2264\nX\n\u2113\u2208Nk\nc\u2113k \u00b7 \u03c1(Ru,\u2113)\n\u2264\nX\n\u2113\u2208Nk\nc\u2113k \u00b7\n\u0014\nmax\n1\u2264\u2113\u2264N \u03c1(Ru,\u2113)\n\u0015\n=\nmax\n1\u2264\u2113\u2264N \u03c1(Ru,\u2113)\nIn other words,\n\u03bbmax(Rk) \u2264\nmax\n1\u2264k\u2264N {\u03bbmax(Ru,k)}\nIt then follows from (198) that\n\u00b5 <\n2\n\u03bbmax(Rk),\nfor all k = 1, 2, . . . , N\nso that (191) is satis\ufb01ed as well.\nLet us now examine the convergence rate. To begin with, we note that the matrix (INM \u2212MR) that appears in\nthe weight-error recursion (189) is block diagonal:\n(INM \u2212MR) = diag{(IM \u2212\u00b5R1), (IM \u2212\u00b5R2), . . . , (IM \u2212\u00b5RN)}\nand each individual block entry, (IM \u2212\u00b5Rk), is a stable matrix since \u00b5 satis\ufb01es (191). Moreover, each of these entries\ncan be written as\nIM \u2212\u00b5Rk =\nX\n\u2113\u2208Nk\nc\u2113k(IM \u2212\u00b5Ru,\u2113)\nwhich expresses (IM \u2212\u00b5Rk) as a convex combination of stable terms (IM \u2212\u00b5Ru,\u2113). Applying Jensen\u2019s inequality\nagain we get\n\u03c1\n\uf8eb\n\uf8edX\n\u2113\u2208Nk\nc\u2113k(IM \u2212\u00b5Ru,\u2113)\n\uf8f6\n\uf8f8\u2264\nX\n\u2113\u2208Nk\nc\u2113k \u03c1(IM \u2212\u00b5Ru,\u2113)\nNow, we know from (189) that the rate of decay of ewi to zero in the di\ufb00usion case is determined by the spectral\nradius of the coe\ufb03cient matrix AT\n2 (INM \u2212MR) AT\n1 . Likewise, we know from (190) that the rate of decay of ewi to\nzero in the non-cooperative case is determined by the spectral radius of the coe\ufb03cient matrix (INM \u2212MRu). Then,\nnote that\n43\n\u03c1\n\u0010\nAT\n2 (INM \u2212MR) AT\n1\n\u0011\n(605)\n\u2264\n\u03c1(INM \u2212MR)\n=\nmax\n1\u2264k\u2264N \u03c1 (IM \u2212\u00b5Rk)\n=\nmax\n1\u2264k\u2264N \u03c1\n\uf8eb\n\uf8edX\n\u2113\u2208Nk\nc\u2113k(IM \u2212\u00b5Ru,\u2113)\n\uf8f6\n\uf8f8\n\u2264\nmax\n1\u2264k\u2264N\nX\n\u2113\u2208Nk\nc\u2113k \u03c1(IM \u2212\u00b5Ru,\u2113)\n\u2264\nmax\n1\u2264k\u2264N\nX\n\u2113\u2208Nk\nc\u2113k\n\u0012\nmax\n1\u2264\u2113\u2264N \u03c1(IM \u2212\u00b5Ru,\u2113)\n\u0013\n=\nmax\n1\u2264k\u2264N\n\uf8f1\n\uf8f2\n\uf8f3\n\u0012\nmax\n1\u2264\u2113\u2264N \u03c1(IM \u2212\u00b5Ru,\u2113)\n\u0013\n\u00b7\nX\n\u2113\u2208Nk\nc\u2113k\n\uf8fc\n\uf8fd\n\uf8fe\n=\nmax\n1\u2264k\u2264N\n\u0012\nmax\n1\u2264\u2113\u2264N \u03c1(IM \u2212\u00b5Ru,\u2113)\n\u0013\n=\nmax\n1\u2264\u2113\u2264N \u03c1(IM \u2212\u00b5Ru,\u2113)\n=\n\u03c1(INM \u2212MRu)\nTherefore, the spectral radius of AT\n2 (INM \u2212MR) AT\n1 is at most as large as the largest individual spectral radius in\nthe non-cooperative case.\nThe argument can be modi\ufb01ed to handle di\ufb00erent step-sizes across the nodes if we assume uniform covariance\ndata across the network, as stated below.\nTheorem 5.4. (Convergence Rate is Enhanced: Uniform Covariance Data) Consider the same\nsetting of Theorem 5.3.\nAssume the covariance data are uniform across all nodes, say, Ru,k = Ru is\nindependent of k. Assume further that the nodes in both modes of operation employ steps-sizes \u00b5k that are\nchosen to satisfy the required stability conditions (191) and (194), which in this case are met by:\n\u00b5k <\n2\n\u03bbmax(Ru),\nk = 1, 2, . . . , N\n(199)\nIt then holds that the magnitude of the error vector, \u2225ewi\u2225, in the di\ufb00usion case decays to zero more rapidly\nthan in the non-cooperative case. In other words, di\ufb00usion enhances convergence rate.\nProof. Since Ru,\u2113= Ru for all \u2113and C is doubly stochastic, we get Rk = Ru and INM \u2212MR = INM \u2212MRu. Then,\n\u03c1\n\u0010\nAT\n2 (INM \u2212MR) AT\n1\n\u0011\n(605)\n\u2264\n\u03c1(INM \u2212MR)\n=\n\u03c1(INM \u2212MRu)\nThe next statement considers the case of ATC and CTA strategies (137) and (145) without information\nexchange, which correspond to the case C = IN. The result establishes that these strategies always enhance\nthe convergence rate over the non-cooperative case, without the need to assume uniform step-sizes or uniform\ncovariance data.\nTheorem 5.5. (Convergence Rate is Enhanced when C = I) Consider the problem of optimizing the\nglobal cost (92) with the individual cost functions given by (93). Pick left stochastic matrices A1 and A2\nsatisfying (166) and set C = IN. This situation covers the ATC and CTA strategies (137) and (145), which\n44\ndo not involve information exchange. Consider two modes of operation. In one mode, each node in the\nnetwork runs the (distributed) steepest-descent di\ufb00usion algorithm (168)\u2013(170). In the second mode, each\nnode operates individually and runs the non-cooperative steepest-descent algorithm (171). In both cases, the\npositive step-sizes are chosen to satisfy the required stability conditions (193) and (194), which in this case\nare met by\n\u00b5k <\n2\n\u03bbmax(Ru,k),\nk = 1, 2, . . . , N\n(200)\nIt then holds that the magnitude of the error vector, \u2225ewi\u2225, in the di\ufb00usion case decays to zero more rapidly\nthan in the non-cooperative case. In other words, di\ufb00usion cooperation enhances convergence rate.\nProof. When C = IN, we get Rk = Ru,k and, therefore, R = Ru and INM \u2212MR = INM \u2212MRu. Then,\n\u03c1\n\u0010\nAT\n2 (INM \u2212MR) AT\n1\n\u0011\n(605)\n\u2264\n\u03c1(INM \u2212MR)\n=\n\u03c1(INM \u2212MRu)\nThe results of the previous theorems highlight the following important facts about the role of the combination\nmatrices {A1, A2, C} in the convergence behavior of the di\ufb00usion strategy (163)\u2013(165):\n(a) The matrix C in\ufb02uences the stability of the network through its in\ufb02uence on the bound in (191). This\nis because the matrices {Rk} depend on the entries of C. The matrices {A1, A2} do not in\ufb02uence\nnetwork stability.\n(b) The matrices {A1, A2, C} in\ufb02uence the rate of convergence of the network since they in\ufb02uence the\nspectral radius of the matrix AT\n2 (INM \u2212MR) AT\n1 , which controls the dynamics of the weight error\nvector in (189).\n6\nPerformance of Adaptive Di\ufb00usion Strategies\nWe now move on to examine the behavior of the adaptive di\ufb00usion implementations (153)\u2013(154), and the\nin\ufb02uence of both gradient noise and measurement noise on convergence and steady-state performance. Due\nto the random nature of the perturbations, it becomes necessary to evaluate the behavior of the algorithms\non average, using mean-square convergence analysis. For this reason, we shall study the convergence of the\nweight estimates both in the mean and mean-square sense. To do so, we will again consider a general di\ufb00usion\nstructure that includes the ATC and CTA strategies (153)\u2013(154) as special cases. We shall further resort\nto the boldface notation to refer to the measurements and weight estimates in order to highlight the fact\nthat they are now being treated as random variables. In this way, the update equations becomes stochastic\nupdates. Thus, consider the following general adaptive di\ufb00usion strategy for i \u22650:\n\u03c6k,i\u22121\n=\nX\n\u2113\u2208Nk\na1,\u2113k w\u2113,i\u22121\n(201)\n\u03c8k,i\n=\n\u03c6k,i\u22121 + \u00b5k\nX\n\u2113\u2208Nk\nc\u2113k u\u2217\n\u2113,i\n\u0002\nd\u2113(i) \u2212u\u2113,i \u03c6k,i\u22121\n\u0003\n(202)\nwk,i\n=\nX\n\u2113\u2208Nk\na2,\u2113k \u03c8\u2113,i\n(203)\nAs before, the scalars {a1,\u2113k, c\u2113k, a2,\u2113k} are non-negative real coe\ufb03cients corresponding to the (\u2113, k) entries\nof N \u00d7 N combination matrices {A1, C, A2}, respectively. These matrices are assumed to satisfy the same\nconditions (166) or (167). Again, di\ufb00erent choices for {A1, C, A2} correspond to di\ufb00erent cooperation modes.\nFor example, the choice A1 = IN and A2 = A corresponds to the adaptive ATC implementation (153), while\nthe choice A1 = A and A2 = IN corresponds to the adaptive CTA implementation (154). Likewise, the\n45\nchoice C = IN corresponds to the case in which the nodes only share weight estimates and the distributed\ndi\ufb00usion recursions (201)\u2013(203) become\n\u03c6k,i\u22121\n=\nX\n\u2113\u2208Nk\na1,\u2113k w\u2113,i\u22121\n(204)\n\u03c8k,i\n=\n\u03c6k,i\u22121 + \u00b5ku\u2217\nk,i\n\u0002\ndk(i) \u2212uk,i \u03c6k,i\u22121\n\u0003\n(205)\nwk,i\n=\nX\n\u2113\u2208Nk\na2,\u2113k \u03c8\u2113,i\n(206)\nFurthermore, the choice A1 = A2 = C = IN corresponds to the non-cooperative mode of operation, where\neach node runs the classical (stand-alone) least-mean-squares (LMS) \ufb01lter independently of the other nodes:\n[4\u20137]:\nwk,i\n=\nwk,i\u22121 + \u00b5kuk,i [dk(i) \u2212uk,i wk,i\u22121] ,\ni \u22650\n(207)\n6.1\nData Model\nWhen we studied the performance of the steepest-descent di\ufb00usion strategy (163)\u2013(165) we exploited result\n(175), which indicated how the moments {rdu,k, Ru,k} that appeared in the recursions related to the optimal\nsolution wo. Likewise, in order to be able to analyze the performance of the adaptive di\ufb00usion strategy\n(201)\u2013(203), we need to know how the data {dk(i), uk,i} across the network relate to wo. Motivated by the\nseveral examples presented earlier in Sec. 2, we shall assume that the data satisfy a linear model of the form:\ndk(i) = uk,iwo + vk(i)\n(208)\nwhere vk(i) is measurement noise with variance \u03c32\nv,k:\n\u03c32\nv,k\n\u2206= E|vk(i)|2\n(209)\nand where the stochastic processes {dk(i), uk,i} are assumed to be jointly wide-sense stationary with mo-\nments:\n\u03c32\nd,k\n\u2206=\nE|dk(i)|2\n(scalar)\n(210)\nRu,k\n\u2206=\nEu\u2217\nk,iuk,i > 0\n(M \u00d7 M)\n(211)\nrdu,k\n\u2206=\nEdk(i)u\u2217\nk,i\n(M \u00d7 1)\n(212)\nAll variables are assumed to be zero-mean. Furthermore, the noise process {vk(i)} is assumed to be tempo-\nrally white and spatially independent, as described earlier by (6), namely,\n\u001a\nEvk(i)v\u2217\nk(j) = 0, for all i \u0338= j (temporal whiteness)\nEvk(i)v\u2217\nm(j) = 0, for all i, j whenever k \u0338= m (spatial whiteness)\n(213)\nThe noise process vk(i) is further assumed to be independent of the regression data um,j for all k, m and\ni, j so that:\nEvk(i)u\u2217\nm,j = 0,\nfor all k, m, i, j\n(214)\nWe shall also assume that the regression data are temporally white and spatially independent so that:\nEu\u2217\nk,iu\u2113,j = Ru,k\u03b4k\u2113\u03b4ij\n(215)\nAlthough we are going to derive performance measures for the network under this independence assumption\non the regression data, it turns out that the resulting expressions continue to match well with simulation\nresults for su\ufb03ciently small step-sizes, even when the independence assumption does not hold (in a manner\nsimilar to the behavior of stand-alone adaptive \ufb01lters) [4,5].\n46\n6.2\nPerformance Measures\nOur objective is to analyze whether, and how fast, the weight estimates {wk,i} from the adaptive di\ufb00usion\nimplementation (201)\u2013(203) converge towards wo. To do so, we again introduce the M \u00d7 1 weight error\nvectors:\ne\u03c6k,i\n\u2206=\nwo \u2212\u03c6k,i\n(216)\ne\u03c8k,i\n\u2206=\nwo \u2212\u03c8k,i\n(217)\newk,i\n\u2206=\nwo \u2212wk,i\n(218)\nEach of these error vectors measures the residual relative to the desired wo in (208). We further introduce\ntwo scalar error measures:\nek(i)\n\u2206=\ndk(i) \u2212uk,iwk,i\u22121\n(output error)\n(219)\nea,k(i)\n\u2206=\nuk,i ewk,i\u22121\n(a-priori error)\n(220)\nThe \ufb01rst error measures how well the term uk,iwk,i\u22121 approximates the measured data, dk(i); in view of\n(208), this error can be interpreted as an estimator for the noise term vk(i). If node k is able to estimate wo\nwell, then ek(i) would get close to vk(i). Therefore, under ideal conditions, we would expect the variance of\nek(i) to tend towards the variance of vk(i). However, as remarked earlier in (31), there is generally an o\ufb00set\nterm for adaptive implementations that is captured by the variance of the a-priori error, ea,k(i). This second\nerror measures how well uk,iwk,i\u22121 approximates the uncorrupted term uk,iwo. Using the data model (208),\nwe can relate {ek(i), ea,k(i)} as\nek(i)\n=\nea,k + vk(i)\n(221)\nSince the noise component, vk(i), is assumed to be zero-mean and independent of all other random variables,\nwe recover (31):\nE|ek(i)|2 = E|ea,k(i)|2 + \u03c32\nv,k\n(222)\nThis relation con\ufb01rms that the variance of the output error, ek(i), is always at least as large as \u03c32\nv,k and\naway from it by an amount that is equal to the variance of the a-priori error, ea,k(i). Accordingly, in order\nto quantify the performance of any particular node in the network, we de\ufb01ne the mean-square-error (MSE)\nand excess-mean-square-error (EMSE) for node k as the following steady-state measures:\nMSEk\n\u2206=\nlim\ni\u2192\u221eE|ek(i)|2\n(223)\nEMSEk\n\u2206=\nlim\ni\u2192\u221eE|ea,k(i)|2\n(224)\nThen, it holds that\nMSEk = EMSEk + \u03c32\nv,k\n(225)\nTherefore, the EMSE term quanti\ufb01es the size of the o\ufb00set in the MSE performance of each node. We also\nde\ufb01ne the mean-square-deviation (MSD) of each node as the steady-state measure:\nMSDk\n\u2206=\nlim\ni\u2192\u221eE\u2225ewk,i\u22252\n(226)\nwhich measures how far wk,i is from wo in the mean-square-error sense.\nWe indicated earlier in (36)\u2013(37) how the MSD and EMSE of stand-alone LMS \ufb01lters in the non-\ncooperative case depend on {\u00b5k, \u03c32\nv, Ru,k}. In this section, we examine how cooperation among the nodes\n47\nin\ufb02uences their performance. Since cooperation couples the operation of the nodes, with data originating\nfrom one node in\ufb02uencing the behavior of its neighbors and their neighbors, the study of the network per-\nformance requires more e\ufb00ort than in the non-cooperative case. Nevertheless, when all is said and done,\nwe will arrive at expressions that approximate well the network performance and reveal some interesting\nconclusions.\n6.3\nError Recursions\nUsing the data model (208) and subtracting wo from both sides of the relations in (201)\u2013(203) we get\ne\u03c6k,i\u22121\n=\nX\n\u2113\u2208Nk\na1,\u2113k ew\u2113,i\u22121\n(227)\ne\u03c8k,i\n=\n \nIM \u2212\u00b5k\nX\n\u2113\u2208Nk\nc\u2113k u\u2217\n\u2113,iu\u2113,i\n!\ne\u03c6k,i\u22121 \u2212\u00b5k\nX\n\u2113\u2208Nk\nc\u2113k u\u2217\n\u2113,iv\u2113(i)\n(228)\newk,i\n=\nX\n\u2113\u2208Nk\na2,\u2113k e\u03c8\u2113,i\n(229)\nComparing the second recursion with the corresponding recursion in the steepest-descent case (176)\u2013(178),\nwe see that two new e\ufb00ects arise: the e\ufb00ect of gradient noise, which replaces the covariance matrices Ru,\u2113\nby the instantaneous approximation u\u2217\n\u2113,iu\u2113,i, and the e\ufb00ect of measurement noise, v\u2113(i).\nWe again describe the above relations more compactly by collecting the information from across the\nnetwork in block vectors and matrices. We collect the error vectors from across all nodes into the following\nN \u00d7 1 block vectors, whose individual entries are of size M \u00d7 1 each:\ne\u03c8i\n\u2206=\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0\ne\u03c81,i\ne\u03c82,i\n...\ne\u03c8N,i\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb\n,\ne\u03c6i\n\u2206=\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0\ne\u03c61,i\ne\u03c62,i\n...\ne\u03c6N,i\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb\n,\newi\n\u2206=\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8f0\new1,i\new2,i\n...\newN,i\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fb\n(230)\nThe block quantities {e\u03c8i, e\u03c6i, ewi} represent the state of the errors across the network at time i. Likewise,\nwe introduce the following N \u00d7 N block diagonal matrices, whose individual entries are of size M \u00d7 M each:\nM\n\u2206=\ndiag{ \u00b51IM, \u00b52IM, . . . , \u00b5NIM }\n(231)\nRi\n\u2206=\ndiag\n( X\n\u2113\u2208N1\nc\u21131 u\u2217\n\u2113,iu\u2113,i,\nX\n\u2113\u2208N2\nc\u21132 u\u2217\n\u2113,iu\u2113,i, . . . ,\nX\n\u2113\u2208NN\nc\u2113N u\u2217\n\u2113,iu\u2113,i\n)\n(232)\nEach block diagonal entry of Ri, say, the k-th entry, contains a combination of rank-one regression terms\ncollected from the neighborhood of node k. In this way, the matrix Ri is now stochastic and dependent\non time, in contrast to the matrix R in the steepest-descent case in (181), which was a constant matrix.\nNevertheless, it holds that\nERi = R\n(233)\nso that, on average, Ri agrees with R. We can simplify the notation by denoting the neighborhood combi-\nnations as follows:\nRk,i\n\u2206=\nX\n\u2113\u2208Nk\nc\u2113k u\u2217\n\u2113,iu\u2113,i\n(234)\nso that Ri becomes\nRi\n\u2206= diag { R1,i, R2,i, . . . , RN,i }\n(when C \u0338= I)\n(235)\n48\nAgain, compared with the matrix Rk de\ufb01ned in (182), we \ufb01nd that Rk,i is now both stochastic and time-\ndependent. Nevertheless, it again holds that\nERk,i = Rk\n(236)\nIn the special case when C = I, the matrix Ri reduces to\nRu,i\n\u2206= diag{u\u2217\n1,iu1,i, u\u2217\n2,iu2,i, . . . , u\u2217\nN,iuN,i}\n(when C = I)\n(237)\nwith\nERu,i = Ru\n(238)\nwhere Ru was de\ufb01ned earlier in (184).\nWe further introduce the following N \u00d7 1 block column vector, whose entries are of size M \u00d7 1 each:\nsi\n\u2206=\ncol{ u\u2217\n1,iv1(i), u\u2217\n2,iv2(i), . . . , u\u2217\nN,ivN(i) }\n(239)\nObviously, given that the regression data and measurement noise are zero-mean and independent of each\nother, we have\nEsi = 0\n(240)\nand the covariance matrix of si is N \u00d7 N block diagonal with blocks of size M \u00d7 M:\nS\n\u2206= Esis\u2217\ni = diag{\u03c32\nv,1Ru,1, \u03c32\nv,2Ru,2, . . . , \u03c32\nv,NRu,N}\n(241)\nReturning to (227)\u2013(229), we conclude that the following relations hold for the block quantities:\ne\u03c6i\u22121\n=\nAT\n1 ewi\u22121\n(242)\ne\u03c8i\n=\n(INM \u2212MRi) e\u03c6i\u22121 \u2212MCT si\n(243)\newi\n=\nAT\n2 e\u03c8i\n(244)\nwhere\nC\n\u2206= C \u2297IM\n(245)\nso that the network weight error vector, ewi, ends up evolving according to the following stochastic recursion:\newi = AT\n2 (INM \u2212MRi) AT\n1 ewi\u22121 \u2212AT\n2 MCT si, i \u22650\n(di\ufb00usion strategy)\n(246)\nFor comparison purposes, if each node operates individually and uses the non-cooperative LMS recursion\n(207), then the weight error vector across all N nodes would evolve according to the following stochastic\nrecursion:\newi = (INM \u2212MRu,i) ewi\u22121 \u2212Msi, i \u22650\n(non-cooperative strategy)\n(247)\nwhere the matrices A1 and A2 do not appear, and Ri is replaced by Ru,i from (237).\n49\n6.4\nConvergence in the Mean\nTaking expectations of both sides of (246) we \ufb01nd that:\nE ewi = AT\n2 (INM \u2212MR) AT\n1 \u00b7 E ewi\u22121, i \u22650\n(di\ufb00usion strategy)\n(248)\nwhere we used the fact that ewi\u22121 and Ri are independent of each other in view of our earlier assumptions\non the regression data and noise in Sec. 6.1. Comparing with the error recursion (189) in the steepest-\ndescent case, we \ufb01nd that both recursions are identical with ewi replaced by E ewi. Therefore, the convergence\nstatements from the steepest-descent case can be extended to the adaptive case to provide conditions on the\nstep-size to ensure stability in the mean, i.e., to ensure\nE ewi \u2212\u21920\nas\ni \u2212\u2192\u221e\n(249)\nWhen (249) is guaranteed, we would say that the adaptive di\ufb00usion solution is asymptotically unbiased. The\nfollowing statements restate the results of Theorems 5.1\u20135.5 in the context of mean error analysis.\nTheorem 6.1. (Convergence in the Mean) Consider the problem of optimizing the global cost (92) with\nthe individual cost functions given by (93). Pick a right stochastic matrix C and left stochastic matrices A1\nand A2 satisfying (166) or (167). Assume each node in the network measures data that satisfy the conditions\ndescribed in Sec. 6.1, and runs the adaptive di\ufb00usion algorithm (201)\u2013(203). Then, all estimators {wk,i}\nacross the network converge in the mean to the optimal solution wo if the positive step-size parameters {\u00b5k}\nsatisfy\n\u00b5k <\n2\n\u03bbmax(Rk)\n(250)\nwhere the neighborhood covariance matrix Rk is de\ufb01ned by (182). In other words, Ewk,i \u2192wo for all nodes\nk as i \u2192\u221e.\n\u25a1\nObserve again that the mean stability condition (250) does not depend on the speci\ufb01c combination matrices\nA1 and A2 that are being used. Only the combination matrix C in\ufb02uences the condition on the step-size\nthrough the neighborhood covariance matrices {Rk}. Observe further that the statement of the lemma does\nnot require the network to be connected. Moreover, when C = IN, in which case the nodes only share weight\nestimators and do not share neighborhood data {d\u2113(i), u\u2113,i} as in (204)\u2013(206), condition (250) becomes\n\u00b5k <\n2\n\u03bbmax(Ru,k)\n(adaptive cooperation with C = IN)\n(251)\nResults (250) and (251) are reminiscent of a classical result for the stand-alone LMS algorithm, as in the\nnon-cooperative case (207), where it is known that the estimator by each individual node in this case would\nconverge in the mean to wo if, and only if, its step-size satis\ufb01es\n\u00b5k <\n2\n\u03bbmax(Ru,k)\n(non-cooperative adaptation)\n(252)\nThe following statement provides a bi-directional result that ensures the mean convergence of the adaptive\ndi\ufb00usion strategy for any choice of left-stochastic combination matrices A1 and A2.\n50\nTheorem 6.2. (Mean Convergence for Arbitrary Combination Matrices) Consider the problem of\noptimizing the global cost (92) with the individual cost functions given by (93). Pick a right stochastic matrix\nC satisfying (166). Assume each node in the network measures data that satisfy the conditions described in\nSec. 6.1. Then, the estimators {wk,i} generated by the adaptive di\ufb00usion strategy (201)\u2013(203), converge in\nthe mean to wo, for all choices of left stochastic matrices A1 and A2 satisfying (166) if, and only if,\n\u00b5k <\n2\n\u03bbmax(Rk)\n(253)\n\u25a1\nAs was the case with steepest-descent di\ufb00usion strategies, the adaptive di\ufb00usion strategy (201)\u2013(203) also\nenhances the convergence rate of the mean of the error vector towards zero relative to the non-cooperative\nstrategy (207). The next results restate Theorems 5.3\u20135.5; they assume C is a doubly stochastic matrix.\nTheorem 6.3. (Mean Convergence Rate is Enhanced: Uniform Step-Sizes) Consider the problem\nof optimizing the global cost (92) with the individual cost functions given by (93). Pick a doubly stochastic\nmatrix C satisfying (196) and left stochastic matrices A1 and A2 satisfying (166). Assume each node in the\nnetwork measures data that satisfy the conditions described in Sec. 6.1. Consider two modes of operation.\nIn one mode, each node in the network runs the adaptive di\ufb00usion algorithm (201)\u2013(203). In the second\nmode, each node operates individually and runs the non-cooperative LMS algorithm (207). In both cases, the\npositive step-sizes used by all nodes are assumed to be the same, say, \u00b5k = \u00b5 for all k, and the value of \u00b5 is\nchosen to satisfy the required mean stability conditions (250) and (252), which are met by selecting\n\u00b5 <\nmin\n1\u2264k\u2264N\n\u001a\n2\n\u03bbmax(Ru,k)\n\u001b\n(254)\nIt then holds that the magnitude of the mean error vector, \u2225E ewi\u2225in the di\ufb00usion case decays to zero more\nrapidly than in the non-cooperative case. In other words, di\ufb00usion enhances convergence rate.\n\u25a1\nTheorem 6.4. (Mean Convergence Rate is Enhanced: Uniform Covariance Data) Consider the\nsame setting of Theorem 6.3. Assume the covariance data are uniform across all nodes, say, Ru,k = Ru is\nindependent of k. Assume further that the nodes in both modes of operation employ steps-sizes \u00b5k that are\nchosen to satisfy the required stability conditions (250) and (252), which in this case are met by:\n\u00b5k <\n2\n\u03bbmax(Ru),\nk = 1, 2, . . . , N\n(255)\nIt then holds that the magnitude of the mean error vector, \u2225E ewi\u2225, in the di\ufb00usion case also decays to zero\nmore rapidly than in the non-cooperative case. In other words, di\ufb00usion enhances convergence rate.\n\u25a1\nThe next statement considers the case of ATC and CTA strategies (204)\u2013(206) without information exchange,\nwhich correspond to the choice C = IN. The result establishes that these strategies always enhance the\nconvergence rate over the non-cooperative case, without the need to assume uniform step-sizes or uniform\ncovariance data.\nTheorem 6.5. (Mean Convergence Rate is Enhanced when C = I) Consider the problem of opti-\nmizing the global cost (92) with the individual cost functions given by (93). Pick left stochastic matrices A1\nand A2 satisfying (166) and set C = IN. This situation covers the ATC and CTA strategies (204)\u2013(206)\nthat do not involve information exchange. Assume each node in the network measures data that satisfy the\nconditions described in Sec. 6.1. Consider two modes of operation. In one mode, each node in the network\n51\nruns the adaptive di\ufb00usion algorithm (204)\u2013(206). In the second mode, each node operates individually and\nruns the non-cooperative LMS algorithm (207). In both cases, the positive step-sizes are chosen to satisfy\nthe required stability conditions (251) and (252), which in this case are met by\n\u00b5k <\n2\n\u03bbmax(Ru,k),\nk = 1, 2, . . . , N\n(256)\nIt then holds that the magnitude of the mean error vector, \u2225E ewi\u2225, in the di\ufb00usion case decays to zero more\nrapidly than in the non-cooperative case. In other words, di\ufb00usion cooperation enhances convergence rate.\n\u25a1\nThe results of the previous theorems again highlight the following important facts about the role of the\ncombination matrices {A1, A2, C} in the convergence behavior of the adaptive di\ufb00usion strategy (201)\u2013(203):\n(a) The matrix C in\ufb02uences the mean stability of the network through its in\ufb02uence on the bound in (250).\nThis is because the matrices {Rk} depend on the entries of C. The matrices {A1, A2} do not in\ufb02uence\nnetwork mean stability.\n(b) The matrices {A1, A2, C} in\ufb02uence the rate of convergence of the mean weight-error vector over the\nnetwork since they in\ufb02uence the spectral radius of the matrix AT\n2 (INM \u2212MR) AT\n1 , which controls the\ndynamics of the weight error vector in (248).\n6.5\nMean-Square Stability\nIt is not su\ufb03cient to ensure the stability of the weight-error vector in the mean sense. The error vectors,\n\u02dcwk,i, may be converging on average to zero but they may have large \ufb02uctuations around the zero value.\nWe therefore need to examine how small the error vectors get. To do so, we perform a mean-square-error\nanalysis. The purpose of the analysis is to evaluate how the variances E\u2225ewk,i\u22252 evolve with time and what\ntheir steady-state values are, for each node k.\nIn this section, we are particularly interested in evaluating the evolution of two mean-square-errors,\nnamely,\nE\u2225ewk,i\u22252\nand\nE|ea,k(i)|2\n(257)\nThe steady-state values of these quantities determine the MSD and EMSE performance levels at node k\nand, therefore, convey critical information about the performance of the network. Under the independence\nassumption on the regression data from Sec. 6.1, it can be veri\ufb01ed that the EMSE variance can be written\nas:\nE|ea,k(i)|2\n\u2206=\nE|uk,i ewk,i\u22121|2\n=\nE ew\u2217\nk,i\u22121u\u2217\nk,iuk,i ewk,i\u22121\n=\nE\n\u0002\nE( ew\u2217\nk,i\u22121u\u2217\nk,iuk,i ewk,i\u22121| ewk,i)\n\u0003\n=\nE ew\u2217\nk,i\u22121\n\u0002\nEu\u2217\nk,iuk,i\n\u0003 ewk,i\u22121\n=\nE ew\u2217\nk,i\u22121Ru,k ewk,i\u22121\n=\nE\u2225ewk,i\u22121\u22252\nRu,k\n(258)\nin terms of a weighted square measure with weighting matrix Ru,k. Here we are using the notation \u2225x\u22252\n\u03a3 to\ndenote the weighted square quantity x\u2217\u03a3x, for any column vector x and matrix \u03a3. Thus, we can evaluate\nmean-square-errors of the form (257) by evaluating the means of weighted square quantities of the following\nform:\nE\u2225ewk,i\u22252\n\u03a3k\n(259)\nfor an arbitrary Hermitian nonnegative-de\ufb01nite weighting matrix \u03a3k that we are free to choose. By setting\n\u03a3k to di\ufb00erent values (say, \u03a3k = I or \u03a3k = Ru,k), we can extract various types of information about\n52\nthe nodes and the network, as the discussion will reveal. The approach we follow is based on the energy\nconservation framework of [4,5,57].\nSo, let \u03a3 denote an arbitrary N \u00d7 N block Hermitian nonnegative-de\ufb01nite matrix that we are free to\nchoose, with M \u00d7 M block entries {\u03a3\u2113k}. Let \u03c3 denote the (NM)2 \u00d7 1 vector that is obtained by stacking\nthe columns of \u03a3 on top of each other, written as\n\u03c3\n\u2206= vec(\u03a3)\n(260)\nIn the sequel, it will become more convenient to work with the vector representation \u03c3 than with the matrix\n\u03a3 itself.\nWe start from the weight-error vector recursion (246) and re-write it more compactly as:\newi = Bi ewi\u22121 \u2212Gsi, i \u22650\n(261)\nwhere the coe\ufb03cient matrices Bi and G are short-hand representations for\nBi\n\u2206= AT\n2 (INM \u2212MRi) AT\n1\n(262)\nand\nG\n\u2206= AT\n2 MCT\n(263)\nNote that Bi is stochastic and time-variant, while G is constant. We denote the mean of Bi by\nB\n\u2206= EBi = AT\n2 (INM \u2212MR) AT\n1\n(264)\nwhere R is de\ufb01ned by (181). Now equating weighted square measures on both sides of (261) we get\n\u2225ewi\u22252\n\u03a3 = \u2225Bi ewi\u22121 \u2212Gsi\u22252\n\u03a3\n(265)\nExpanding the right-hand side we \ufb01nd that\n\u2225ewi\u22252\n\u03a3\n=\new\u2217\ni\u22121B\u2217\ni \u03a3Bi ewi\u22121 + s\u2217\ni GT \u03a3Gsi \u2212\new\u2217\ni\u22121B\u2217\ni \u03a3Gsi \u2212s\u2217\ni GT \u03a3Bi ewi\u22121\n(266)\nUnder expectation, the last two terms on the right-hand side evaluate to zero so that\nE\u2225ewi\u22252\n\u03a3\n=\nE\n\u0000ew\u2217\ni\u22121B\u2217\ni \u03a3Bi ewi\u22121\n\u0001\n+ E\n\u0000s\u2217\ni GT \u03a3Gsi\n\u0001\n(267)\nLet us evaluate each of the expectations on the right-hand side. The last expectation is given by\nE\n\u0000s\u2217\ni GT \u03a3Gsi\n\u0001\n=\nTr\n\u0000GT \u03a3G Esis\u2217\ni\n\u0001\n(241)\n=\nTr\n\u0000GT \u03a3GS\n\u0001\n=\nTr\n\u0000\u03a3GSGT \u0001\n(268)\nwhere S is de\ufb01ned by (241) and where we used the fact that Tr(AB) = Tr(BA) for any two matrices A and\nB of compatible dimensions. With regards to the \ufb01rst expectation on the right-hand side of (267), we have\nE\n\u0000 ew\u2217\ni\u22121B\u2217\ni \u03a3Bi ewi\u22121\n\u0001\n=\nE\n\u0002\nE\n\u0000 ew\u2217\ni\u22121B\u2217\ni \u03a3Bi ewi\u22121| ewi\u22121\n\u0001\u0003\n=\nE ew\u2217\ni\u22121 [E (B\u2217\ni \u03a3Bi)] ewi\u22121\n\u2206=\nE ew\u2217\ni\u22121\u03a3\u2032 ewi\u22121\n=\nE\u2225ewi\u22121\u22252\n\u03a3\u2032\n(269)\n53\nwhere we introduced the nonnegative-de\ufb01nite weighting matrix\n\u03a3\u2032\n\u2206=\nEB\u2217\ni \u03a3Bi\n(262)\n=\nEA1 (INM \u2212RiM) A2\u03a3AT\n2 (INM \u2212MRi) AT\n1\n=\nA1A2\u03a3AT\n2 AT\n1 \u2212A1A2\u03a3AT\n2 MRAT\n1 \u2212A1RMA2\u03a3AT\n2 AT\n1 + O(M2)\n(270)\nwhere R is de\ufb01ned by (181) and the term O(M2) denotes the following factor, which depends on the square\nof the step-sizes, {\u00b52\nk}:\nO(M2) = E\n\u0000A1RiMA2\u03a3AT\n2 MRiAT\n1\n\u0001\n(271)\nThe evaluation of the above expectation depends on higher-order moments of the regression data. While we\ncan continue with the analysis by taking this factor into account, as was done in [4,5,18,57], it is su\ufb03cient\nfor the exposition in this article to focus on the case of su\ufb03ciently small step-sizes where terms involving\nhigher powers of the step-sizes can be ignored. Therefore, we continue our discussion by letting\n\u03a3\u2032\n\u2206= A1A2\u03a3AT\n2 AT\n1 \u2212A1A2\u03a3AT\n2 MRAT\n1 \u2212A1RMA2\u03a3AT\n2 AT\n1\n(272)\nThe weighting matrix \u03a3\u2032 is fully de\ufb01ned in terms of the step-size matrix, M, the network topology through\nthe matrices {A1, A2, C}, and the regression statistical pro\ufb01le through R. Expression (272) tells us how to\nconstruct \u03a3\u2032 from \u03a3. The expression can be transformed into a more compact and revealing form if we\ninstead relate the vector forms \u03c3\u2032 = vec(\u03a3\u2032) and \u03c3 = vec(\u03a3). Using the following equalities for arbitrary\nmatrices {U, W, \u03a3} of compatible dimensions [5]:\nvec(U\u03a3W)\n=\n(W T \u2297U)\u03c3\n(273)\nTr(\u03a3W)\n=\n\u0002\nvec(W T )\n\u0003T \u03c3\n(274)\nand applying the vec operation to both sides of (272) we get\n\u03c3\u2032\n=\n(A1A2 \u2297A1A2) \u03c3 \u2212\n\u0000A1RT MA2 \u2297A1A2\n\u0001\n\u03c3 \u2212(A1A2 \u2297A1RMA2) \u03c3\nThat is,\n\u03c3\u2032\n\u2206= F\u03c3\n(275)\nwhere we are introducing the coe\ufb03cient matrix of size (NM)2 \u00d7 (NM)2:\nF\n\u2206= (A1A2 \u2297A1A2) \u2212\n\u0000A1RT MA2 \u2297A1A2\n\u0001\n\u2212(A1A2 \u2297A1RMA2)\n(276)\nA reasonable approximate expression for F for su\ufb03ciently small step-sizes is\nF \u2248BT \u2297B\u2217\n(277)\nIndeed, if we replace B from (264) into (277) and expand terms, we obtain the same factors that appear in\n(276) plus an additional term that depends on the square of the step-sizes, {\u00b52\nk}, whose e\ufb00ect can be ignored\nfor su\ufb03ciently small step-sizes.\nIn this way, using in addition property (274), we \ufb01nd that relation (267) becomes:\nE\u2225ewi\u22252\n\u03a3 = E\u2225ewi\u22121\u22252\n\u03a3\u2032 +\n\u0002\nvec\n\u0000GST GT \u0001\u0003T \u03c3\n(278)\n54\nThe last term is dependent on the network topology through the matrix G, which is de\ufb01ned in terms of\n{A2, C, M}, and the noise and regression data statistical pro\ufb01le through S. It is convenient to introduce the\nalternative notation \u2225x\u22252\n\u03c3 to refer to the weighted square quantity \u2225x\u22252\n\u03a3, where \u03c3 = vec(\u03a3). We shall use\nthese two notations interchangeably. The convenience of the vector notation is that it allows us to exploit\nthe simpler linear relation (275) between \u03c3\u2032 and \u03c3 to rewrite (278) as shown in (279) below, with the same\nweight vector \u03c3 appearing on both sides.\nTheorem 6.6. (Variance Relation) Consider the data model of Sec. 6.1 and the independence statistical\nconditions imposed on the noise and regression data, including (208)\u2013(215).\nAssume further su\ufb03ciently\nsmall step-sizes are used so that terms that depend on higher-powers of the step-sizes can be ignored. Pick\nleft stochastic matrices A1 and A2 and a right stochastic matrix C satisfying (166). Under these conditions,\nthe weight-error vector ewi = col{ ewk,i}N\nk=1 associated with a network running the adaptive di\ufb00usion strategy\n(201)\u2013(203) satis\ufb01es the following variance relation\nE\u2225ewi\u22252\n\u03c3 = E\u2225ewi\u22121\u22252\nF\u03c3 +\n\u0002\nvec\n\u0000YT \u0001\u0003T \u03c3\n(279)\nfor any Hermitian nonnegative-de\ufb01nite matrix \u03a3 with \u03c3 = vec(\u03a3), and where {S, G, F} are de\ufb01ned by (241),\n(263), and (277), and\nY\n\u2206= GSGT\n(280)\n\u25a1\nNote that relation (279) is not an actual recursion; this is because the weighting matrices {\u03c3, F\u03c3} on both\nsides of the equality are di\ufb00erent. The relation can be transformed into a true recursion by expanding it\ninto a convenient state-space model; this argument was pursued in [4,5,18,57] and is not necessary for the\nexposition here, except to say that stability of the matrix F ensures the mean-square stability of the \ufb01lter\n\u2014 this fact is also established further ahead through relation (327). By mean-square stability we mean that\neach term E\u2225ewk,i\u22252 remains bounded over time and converges to a steady-state MSDk value. Moreover, the\nspectral radius of F controls the rate of convergence of E\u2225ewi\u22252 towards its steady-state value.\nTheorem 6.7. (Mean-Square Stability) Consider the same setting of Theorem 6.6. The adaptive dif-\nfusion strategy (201)\u2013(203) is mean-square stable if, and only if, the matrix F de\ufb01ned by (276), or its\napproximation (277), is stable (i.e., all its eigenvalues lie strictly inside the unit disc). This condition is\nsatis\ufb01ed by su\ufb03ciently small positive step-sizes {\u00b5k} that also satisfy:\n\u00b5k <\n2\n\u03bbmax(Rk)\n(281)\nwhere the neighborhood covariance matrix Rk is de\ufb01ned by (182). Moreover, the convergence rate of the\nalgorithm is determined by the value [\u03c1(B)]2 (the square of the spectral radius of B).\nProof. Recall that, for two arbitrary matrices A and B of compatible dimensions, the eigenvalues of the Kronecker\nproduct A\u2297B is formed of all product combinations \u03bbi(A)\u03bbj(B) of the eigenvalues of A and B [19]. Therefore, using\nexpression (277), we have that \u03c1(F) = [\u03c1(B)]2. It follows that F is stable if, and only if, B is stable. We already noted\nearlier in Theorem 6.1 that condition (281) ensures the stability of B. Therefore, step-sizes that ensure stability in\nthe mean and are su\ufb03ciently small will also ensure mean-square stability.\nRemark. More generally, had we not ignored the second-order term (271), the expression for F would have\nbeen the following. Starting from the de\ufb01nition \u03a3\u2032 = EB\u2217\ni \u03a3Bi, we would get\n\u03c3\u2032 =\n\u0010\nEBT\ni \u2297B\u2217\ni\n\u0011\n\u03c3\n55\nso that\nF\n\u2206=\nE\n\u0010\nBT\ni \u2297B\u2217\ni\n\u0011\n(for general step-sizes)\n(282)\n=\n(A1 \u2297A1) \u00b7\nn\nI \u2212\n\u0000RT M \u2297I\n\u0001\n\u2212(I \u2297RM) + E\n\u0010\nRT\ni M \u2297RiM\n\u0011o\n\u00b7 (A2 \u2297A2)\nMean-square stability of the \ufb01lter would then require the step-sizes {\u00b5k} to be chosen such that they ensure\nthe stability of this matrix F (in addition to condition (281) to ensure mean stability).\n\u25a1\n6.6\nNetwork Mean-Square Performance\nWe can now use the variance relation (279) to evaluate the network performance, as well as the performance\nof the individual nodes, in steady-state. Since the dynamics is mean-square stable for su\ufb03ciently small\nstep-sizes, we take the limit of (279) as i \u2192\u221eand write:\nlim\ni\u2192\u221eE\u2225ewi\u22252\n\u03c3 =\nlim\ni\u2192\u221eE\u2225ewi\u22121\u22252\nF\u03c3 +\n\u0002\nvec\n\u0000YT \u0001\u0003T \u03c3\n(283)\nGrouping terms leads to the following result.\nCorollary 6.1. (Steady-State Variance Relation) Consider the same setting of Theorem 6.6.\nThe\nweight-error vector, ewi = col{ ewk,i}N\nk=1, of the adaptive di\ufb00usion strategy (201)\u2013(203) satis\ufb01es the following\nrelation in steady-state:\nlim\ni\u2192\u221eE\u2225ewi\u22252\n(I\u2212F)\u03c3 =\n\u0002\nvec\n\u0000YT \u0001\u0003T \u03c3\n(284)\nfor any Hermitian nonnegative-de\ufb01nite matrix \u03a3 with \u03c3 = vec(\u03a3), and where {F, Y} are de\ufb01ned by (277)\nand (280).\n\u25a1\nExpression (284) is a very useful relation; it allows us to evaluate the network MSD and EMSE through\nproper selection of the weighting vector \u03c3 (or, equivalently, the weighting matrix \u03a3).\nFor example, the\nnetwork MSD is de\ufb01ned as the average value:\nMSDnetwork\n\u2206=\nlim\ni\u2192\u221e\n1\nN\nN\nX\nk=1\nE\u2225ewk,i\u22252\n(285)\nwhich amounts to averaging the MSDs of the individual nodes. Therefore,\nMSDnetwork = lim\ni\u2192\u221e\n1\nN E\u2225ewi\u22252 =\nlim\ni\u2192\u221eE\u2225ewi\u22252\n1/N\n(286)\nThis means that in order to recover the network MSD from relation (284), we should select the weighting\nvector \u03c3 such that\n(I \u2212F)\u03c3 =\n1\nN vec (INM)\nSolving for \u03c3 and substituting back into (284) we arrive at the following expression for the network MSD:\nMSDnetwork =\n1\nN \u00b7\n\u0002\nvec\n\u0000YT \u0001\u0003T \u00b7 (I \u2212F)\u22121 \u00b7 vec (INM)\n(287)\n56\nLikewise, the network EMSE is de\ufb01ned as the average value\nEMSEnetwork\n\u2206=\nlim\ni\u2192\u221e\n1\nN\nN\nX\nk=1\nE|ea,k(i)|2\n=\nlim\ni\u2192\u221e\n1\nN\nN\nX\nk=1\nE\u2225ewk,i\u22252\nRu,k\n(288)\nwhich amounts to averaging the EMSEs of the individual nodes. Therefore,\nEMSEnetwork =\nlim\ni\u2192\u221e\n1\nN E\u2225ewi\u22252\ndiag{Ru,1,Ru,2,...,Ru,N} =\nlim\ni\u2192\u221e\n1\nN E\u2225ewi\u22252\nRu\n(289)\nwhere Ru is the matrix de\ufb01ned earlier by (184), and which we repeat below for ease of reference:\nRu = diag{Ru,1, Ru,2, . . . , Ru,N}\n(290)\nThis means that in order to recover the network EMSE from relation (284), we should select the weighting\nvector \u03c3 such that\n(I \u2212F)\u03c3 =\n1\nN vec (Ru)\n(291)\nSolving for \u03c3 and substituting into (284) we arrive at the following expression for the network EMSE:\nEMSEnetwork =\n1\nN \u00b7\n\u0002\nvec\n\u0000YT \u0001\u0003T \u00b7 (I \u2212F)\u22121 \u00b7 vec (Ru)\n(292)\n6.7\nMean-Square Performance of Individual Nodes\nWe can also assess the mean-square performance of the individual nodes in the network from (284). For\ninstance, the MSD of any particular node k is de\ufb01ned by\nMSDk\n\u2206=\nlim\ni\u2192\u221eE\u2225ewk,i\u22252\n(293)\nIntroduce the N \u00d7 N block diagonal matrix with blocks of size M \u00d7 M, where all blocks on the diagonal are\nzero except for an identity matrix on the diagonal block of index k, i.e.,\nJk\n\u2206= diag{ 0M, . . . , 0M, IM, 0M, . . . , 0M }\n(294)\nThen, we can express the node MSD as follows:\nMSDk\n\u2206=\nlim\ni\u2192\u221eE\u2225ewi\u22252\nJk\n(295)\nThe same argument that was used to obtain the network MSD then leads to\nMSDk =\n\u0002\nvec\n\u0000YT \u0001\u0003T \u00b7 (I \u2212F)\u22121 \u00b7 vec (Jk)\n(296)\nLikewise, the EMSE of node k is de\ufb01ned by\nEMSEk\n\u2206=\nlim\ni\u2192\u221eE|ea,k(i)|2\n=\nlim\ni\u2192\u221eE\u2225ewk,i\u22252\nRu,k\n(297)\n57\nIntroduce the N \u00d7 N block diagonal matrix with blocks of size M \u00d7 M, where all blocks on the diagonal are\nzero except for the diagonal block of index k whose value is Ru,k, i.e.,\nTk\n\u2206= diag{ 0M, . . . , 0M, Ru,k, 0M, . . . , 0M }\n(298)\nThen, we can express the node EMSE as follows:\nEMSEk\n\u2206=\nlim\ni\u2192\u221eE\u2225ewi\u22252\nTk\n(299)\nThe same argument that was used to obtain the network EMSE then leads to\nEMSEk =\n\u0002\nvec\n\u0000YT \u0001\u0003T \u00b7 (I \u2212F)\u22121 \u00b7 vec (Tk)\n(300)\nWe summarize the results in the following statement.\nTheorem 6.8. (Network Mean-Square Performance) Consider the same setting of Theorem 6.6. In-\ntroduce the 1 \u00d7 (NM)2 row vector hT de\ufb01ned by\nhT\n\u2206=\n\u0002\nvec\n\u0000YT \u0001\u0003T \u00b7 (I \u2212F)\u22121\n(301)\nwhere {F, Y} are de\ufb01ned by (277) and (280). Then the network MSD and EMSE and the individual node\nperformance measures are given by\nMSDnetwork\n=\nhT \u00b7 vec (INM) /N\n(302)\nEMSEnetwork\n=\nhT \u00b7 vec (Ru) /N\n(303)\nMSDk\n=\nhT \u00b7 vec (Jk)\n(304)\nEMSEk\n=\nhT \u00b7 vec (Tk)\n(305)\nwhere {Jk, Tk} are de\ufb01ned by (294) and (298).\n\u25a1\nWe can obviously recover from the above expressions the performance of the nodes in the non-cooperative\nimplementation (207), where each node performs its adaptation individually, by setting A1 = A2 = C = IN.\nWe can express the network MSD, and its EMSE if desired, in an alternative useful form involving a\nseries representation.\nCorollary 6.2. (Series Representation for Network MSD) Consider the same setting of Theorem 6.6.\nThe network MSD can be expressed in the following alternative series expansion form:\nMSDnetwork =\n1\nN\n\u221e\nX\nj=0\nTr\n\u0000BjYB\u2217j\u0001\n(306)\nwhere\nY\n=\nGSGT\n(307)\nG\n=\nAT\n2 MCT\n(308)\nB\n=\nAT\n2 (I \u2212MR)AT\n1\n(309)\nProof. Since F is stable when the \ufb01lter is mean-square stable, we can expand (I \u2212F)\u22121 as\n(I \u2212F)\u22121\n=\nI + F + F2 + . . .\n(277)\n=\nI +\n\u0010\nBT \u2297B\u2217\u0011\n+\n\u0010\nBT \u2297B\u2217\u00112\n+ . . .\nSubstituting into (287) and using property (274), we obtain the desired result.\n58\n6.8\nUniform Data Pro\ufb01le\nWe can simplify expressions (307)\u2013(309) for {Y, G, B} in the case when the regression covariance matrices\nare uniform across the network and all nodes employ the same step-size, i.e., when\nRu,k\n=\nRu,\nfor all k\n(uniform covariance pro\ufb01le)\n(310)\n\u00b5k\n=\n\u00b5,\nfor all k\n(uniform step-sizes)\n(311)\nand when the combination matrix C is doubly stochastic, so that\nC1 = 1,\nCT 1 = 1\n(312)\nWe refer to conditions (310)\u2013(312) as corresponding to a uniform data pro\ufb01le environment.\nThe noise\nvariances, {\u03c32\nv,k}, do not need to be uniform so that the signal-to-noise ratio (SNR) across the network can\nstill vary from node to node. The simpli\ufb01ed expressions derived in the sequel will be useful in Sec. 7 when\nwe compare the performance of various cooperation strategies.\nThus, under conditions (310)\u2013(312), expressions (180), (181), and (263) for {M, R, G} simplify to\nM\n=\n\u00b5INM\n(313)\nR\n=\nIN \u2297Ru\n(314)\nG\n=\n\u00b5AT\n2 CT\n(315)\nSubstituting these values into expression (309) for B we get\nB\n=\nAT\n2 (I \u2212MR)AT\n1\n=\n(AT\n2 \u2297I) \u00b7 (I \u2212\u00b5(I \u2297Ru)) \u00b7 (AT\n1 \u2297I)\n=\n(AT\n2 \u2297I)(AT\n1 \u2297I) \u2212\u00b5(AT\n2 \u2297I)(I \u2297Ru)(AT\n1 \u2297I)\n=\n(AT\n2 AT\n1 \u2297I) \u2212\u00b5(AT\n2 AT\n1 \u2297Ru)\n=\nAT\n2 AT\n1 \u2297(I \u2212\u00b5Ru)\n(316)\nwhere we used the useful Kronecker product identities:\n(X + Y ) \u2297Z\n=\n(X \u2297Z) + (Y \u2297Z)\n(317)\n(X \u2297Y )(W \u2297Z)\n=\n(XW \u2297Y Z)\n(318)\nfor any matrices {X, Y, Z, W} of compatible dimensions. Likewise, introduce the N \u00d7 N diagonal matrix\nwith noise variances:\nRv\n\u2206= diag{\u03c32\nv,1, \u03c32\nv,2, . . . , \u03c32\nv,N}\n(319)\nThen, expression (241) for S becomes\nS\n=\ndiag{\u03c32\nv,1Ru, \u03c32\nv,2Ru, . . . , \u03c32\nv,NRu}\n=\nRv \u2297Ru\n(320)\nIt then follows that we can simplify expression (307) for Y as:\nY\n=\n\u00b52AT\n2 CT SCA2\n=\n\u00b52 \u00b7 (AT\n2 \u2297I) \u00b7 (CT \u2297I) \u2297(Rv \u2297Ru) \u00b7 (C \u2297I) \u00b7 (A2 \u2297I)\n=\n\u00b52(AT\n2 CT RvCA2 \u2297Ru)\n(321)\n59\nCorollary 6.3. (Network MSD for Uniform Data Pro\ufb01le) Consider the same setting of Theorem 6.6\nwith the additional requirement that conditions (310)\u2013(312) for a uniform data pro\ufb01le hold. The network\nMSD is still given by the same series representation (306) where now\nY\n=\n\u00b52(AT\n2 CT RvCA2 \u2297Ru)\n(322)\nB\n=\nAT\n2 AT\n1 \u2297(I \u2212\u00b5Ru)\n(323)\nUsing these expressions, we can decouple the network MSD expression (306) into two separate factors: one\nis dependent on the step-size and data covariance {\u00b5, Ru}, and the other is dependent on the combination\nmatrices and noise pro\ufb01le {A1, A2, C, Rv}:\nMSDnetwork = \u00b52\nN\n\u221e\nX\nj=0\nTr\n\u0012\u0014\u0010\nAT\n2 AT\n1\n\u0011j \u0010\nAT\n2 CT RvCA2\n\u0011\n(A1A2)j\n\u0015\n\u2297\nh\n(I \u2212\u00b5Ru)jRu(I \u2212\u00b5Ru)ji\u0013\n(324)\nProof. Using (306) and the given expressions (322)\u2013(323) for {Y, B}, we get\nMSDnetwork = \u00b52\nN\n\u221e\nX\nj=0\nTr\n\u0012\u0014\u0010\nAT\n2 AT\n1\n\u0011j\n\u2297(I \u2212\u00b5Ru)j\n\u0015\n(AT\n2 CT RvCA2 \u2297Ru)\nh\n(A1A2)j \u2297(I \u2212\u00b5Ru)ji\u0013\nResult (324) follows from property (317).\n6.9\nTransient Mean-Square Performance\nBefore comparing the mean-square performance of various cooperation strategies, we pause to comment that\nthe variance relation (279) can also be used to characterize the transient behavior of the network, and not\njust its steady-state performance. To see this, iterating (279) starting from i = 0, we \ufb01nd that\nE\u2225ewi\u22252\n\u03c3 = E\u2225ew\u22121\u22252\nF i+1\u03c3 +\n\u0002\nvec\n\u0000YT \u0001\u0003T \u00b7\n\uf8eb\n\uf8ed\ni\nX\nj=0\nFj\u03c3\n\uf8f6\n\uf8f8\n(325)\nwhere\new\u22121\n\u2206= wo \u2212w\u22121\n(326)\nin terms of the initial condition, w\u22121. If this initial condition happens to be w\u22121 = 0, then ew\u22121 = wo.\nComparing expression (325) at time instants i and i \u22121 we can relate E\u2225ewi\u22252\n\u03c3 and E\u2225ewi\u22121\u22252\n\u03c3 as follows:\nE\u2225ewi\u22252\n\u03c3 = E\u2225ewi\u22121\u22252\n\u03c3 +\n\u0002\nvec\n\u0000YT \u0001\u0003T \u00b7 Fi\u03c3 \u2212E\u2225ew\u22121\u22252\n(I\u2212F)F i\u03c3\n(327)\nThis recursion relates the same weighted square measures of the error vectors { ewi, ewi\u22121}.\nIt therefore\ndescribes how these weighted square measures evolve over time. It is clear from this relation that, for mean-\nsquare stability, the matrix F needs to be stable so that the terms involving Fi do not grow unbounded.\nThe learning curve of the network is the curve that describes the evolution of the network EMSE over\ntime. At any time i, the network EMSE is denoted by \u03b6(i) and measured as:\n\u03b6(i)\n\u2206=\n1\nN\nN\nX\nk=1\nE|ea,k(i)|2\n(328)\n=\n1\nN\nN\nX\nk=1\nE\u2225ewk,i\u22252\nRu,k\n60\nThe above expression indicates that \u03b6(i) is obtained by averaging the EMSE of the individual nodes at time\ni. Therefore,\n\u03b6(i) =\n1\nN E\u2225ewi\u22252\ndiag{Ru,1,Ru,2,...,Ru,N} =\n1\nN E\u2225ewi\u22252\nRu\n(329)\nwhere Ru is the matrix de\ufb01ned by (290). This means that in order to evaluate the evolution of the network\nEMSE from relation (327), we simply select the weighting vector \u03c3 such that\n\u03c3 =\n1\nN vec (Ru)\n(330)\nSubstituting into (327) we arrive at the learning curve for the network.\nCorollary 6.4. (Network Learning Curve) Consider the same setting of Theorem 6.6. Let \u03b6(i) denote\nthe network EMSE at time i, as de\ufb01ned by (328). Then, the learning curve of the network corresponds to\nthe evolution of \u03b6(i) with time and is described by the following recursion over i \u22650:\n\u03b6(i) = \u03b6(i \u22121) +\n1\nN\n\u0002\nvec\n\u0000YT \u0001\u0003T \u00b7 Fi \u00b7 vec (Ru) \u2212\n1\nN E\u2225\u02dcw\u22121\u22252\n(I\u2212F)F ivec(Ru)\n(331)\nwhere {F, Y, Ru} are de\ufb01ned by (277), (280), and (290).\n\u25a1\n7\nComparing the Performance of Cooperative Strategies\nUsing the expressions just derived for the MSD of the network, we can compare the performance of various\ncooperative and non-cooperative strategies. Table 6 further ahead summarizes the results derived in this\nsection and the conditions under which they hold.\n7.1\nComparing ATC and CTA Strategies\nWe \ufb01rst compare the performance of the adaptive ATC and CTA di\ufb00usion strategies (153) and (154) when\nthey employ a doubly stochastic combination matrix A. That is, let us consider the two scenarios:\nC, A1 = A, A2 = IN\n(adaptive CTA strategy)\n(332)\nC, A1 = IN, A2 = A\n(adaptive ATC strategy)\n(333)\nwhere A is now assumed to be doubly stochastic, i.e.,\nA1 = 1,\nAT 1 = 1\n(334)\nwith its rows and columns adding up to one. For example, these conditions are satis\ufb01ed when A is left\nstochastic and symmetric. Then, expressions (307) and (309) give:\nBcta\n=\n(I \u2212MR)AT ,\nYcta = MCT SCM\n(335)\nBatc\n=\nAT (I \u2212MR),\nYatc = AT MCT SCMA\n(336)\nwhere\nA = A \u2297IM\n(337)\nFollowing [18], introduce the auxiliary nonnegative-de\ufb01nite matrix\nHj\n\u2206=\n\u0002\n(I \u2212MR)AT \u0003j \u00b7 MCT SCM \u00b7\n\u0002\n(I \u2212MR)AT \u0003\u2217j\n(338)\n61\nThen, it is immediate to verify from (306) that\nMSDnetwork\ncta\n=\n1\nN\n\u221e\nX\nj=0\nTr(Hj)\n(339)\nMSDnetwork\natc\n=\n1\nN\n\u221e\nX\nj=0\nTr(AT Hj A)\n(340)\nso that\nMSDnetwork\ncta\n\u2212MSDnetwork\natc\n=\n1\nN\n\u221e\nX\nj=0\nTr\n\u0000Hj \u2212AT HjA\n\u0001\n(341)\nNow, since A is doubly stochastic, it also holds that the enlarged matrix A is doubly stochastic. Moreover,\nfor any doubly stochastic matrix A and any nonnegative-de\ufb01nite matrix H of compatible dimensions, it\nholds that (see part (f) of Theorem C.3):\nTr(AT HA) \u2264Tr(H)\n(342)\nApplying result (342) to (341) we conclude that\nMSDnetwork\natc\n\u2264MSDnetwork\ncta\n(doubly stochastic A)\n(343)\nso that the adaptive ATC strategy (153) outperforms the adaptive CTA strategy (154) for doubly stochastic\ncombination matrices A.\n7.2\nComparing Strategies with and without Information Exchange\nWe now examine the e\ufb00ect of information exchange (C \u0338= I) on the performance of the adaptive ATC and\nCTA di\ufb00usion strategies (153)\u2013(154) under conditions (310)\u2013(312) for uniform data pro\ufb01le.\nCTA Strategies\nWe start with the adaptive CTA strategy (154), and consider two scenarios with and without information\nexchange. These scenarios correspond to the following selections in the general description (201)\u2013(203):\nC \u0338= I, A1 = A, A2 = IN\n(adaptive CTA with information exchange )\n(344)\nC = I, A1 = A, A2 = IN\n(adaptive CTA without information exchange)\n(345)\nThen, expressions (322) and (323) give:\nBcta,C\u0338=I\n=\nAT \u2297(I \u2212\u00b5Ru),\nYcta,C\u0338=I = \u00b52(CT RvC \u2297Ru)\n(346)\nBcta,C=I\n=\nAT \u2297(I \u2212\u00b5Ru),\nYcta,C=I = \u00b52(Rv \u2297Ru)\n(347)\nwhere the matrix Rv is de\ufb01ned by (319). Note that Bcta,C\u0338=I = Bcta,C=I, so we denote them simply by B in\nthe derivation that follows. Then, from expression (306) for the network MSD we get:\nMSDnetwork\ncta,C=I \u2212MSDnetwork\ncta,C\u0338=I\n=\n\u00b52\nN\n\u221e\nX\nj=0\nTr\n\u0000Bj \u0002\n(Rv \u2212CT RvC) \u2297Ru\n\u0003\nB\u2217j\u0001\n(348)\nIt follows that the di\ufb00erence in performance between both CTA implementations depends on how the matrices\nRv and CT RvC compare to each other:\n62\n(1) When Rv \u2212CT RvC \u22650, we obtain\nMSDnetwork\ncta,C=I \u2265MSDnetwork\ncta,C\u0338=I\n(when CT RvC \u2264Rv)\n(349)\nso that a CTA implementation with information exchange performs better than a CTA implementation\nwithout information exchange. Note that the condition on {Rv, C} corresponds to requiring\nCT RvC \u2264Rv\n(350)\nwhich can be interpreted to mean that the cooperation matrix C should be such that it does not\namplify the e\ufb00ect of measurement noise. For example, this situation occurs when the noise pro\ufb01le is\nuniform across the network, in which case Rv = \u03c32\nvIM. This is because it would then hold that\nRv \u2212CT RvC\n=\n\u03c32\nv(I \u2212CT C) \u22650\n(351)\nin view of the fact that (I \u2212CT C) \u22650 since C is doubly stochastic (cf. property (e) in Lemma C.3).\n(2) When Rv \u2212CT RvC \u22640, we obtain\nMSDnetwork\ncta,C=I \u2264MSDnetwork\ncta,C\u0338=I\n(when CT RvC \u2265Rv)\n(352)\nso that a CTA implementation without information exchange performs better than a CTA implementa-\ntion with information exchange. In this case, the condition on {Rv, C} indicates that the combination\nmatrix C ends up amplifying the e\ufb00ect of noise.\nATC Strategies\nWe can repeat the argument for the adaptive ATC strategy (153), and consider two scenarios with and with-\nout information exchange. These scenarios correspond to the following selections in the general description\n(201)\u2013(203):\nC \u0338= I, A1 = IN, A2 = A\n(adaptive ATC with information exchange )\n(353)\nC = I, A1 = IN, A2 = A\n(adaptive ATC without information exchange)\n(354)\nThen, expressions (322) and (323) give:\nBatc,C\u0338=I\n=\nAT \u2297(I \u2212\u00b5Ru),\nYatc,C\u0338=I = \u00b52(AT CT RvCA \u2297Ru)\n(355)\nBatc,C=I\n=\nAT \u2297(I \u2212\u00b5Ru),\nYatc,C=I = \u00b52(AT RvA \u2297Ru)\n(356)\nNote again that Batc,C\u0338=I = Batc,C=I, so we denote them simply by B. Then,\nMSDnetwork\natc,C=I \u2212MSDnetwork\natc,C\u0338=I\n=\n\u00b52\nN\n\u221e\nX\nj=0\nTr\n\u0000Bj \u0002\nAT (Rv \u2212CT RvC)A \u2297Ru\n\u0003\nB\u2217j\u0001\n(357)\nIt again follows that the di\ufb00erence in performance between both ATC implementations depends on how the\nmatrices Rv and CT RvC compare to each other and we obtain:\nMSDnetwork\natc,C=I \u2265MSDnetwork\natc,C\u0338=I\n(when CT RvC \u2264Rv)\n(358)\nand\nMSDnetwork\natc,C=I \u2264MSDnetwork\natc,C\u0338=I\n(when CT RvC \u2265Rv)\n(359)\n63\nTable 6: Comparison of the MSD performance of various cooperative strategies.\nComparison\nConditions\nMSDnetwork\natc\n\u2264MSDnetwork\ncta\nA doubly stochastic, C right stochastic.\nMSDnetwork\ncta,C\u0338=I \u2264MSDnetwork\ncta,C=I\nCT RvC \u2264Rv, C doubly stochastic, Ru,k = Ru, \u00b5k = \u00b5.\nMSDnetwork\ncta,C=I \u2264MSDnetwork\ncta,C\u0338=I\nCT RvC \u2265Rv, C doubly stochastic, Ru,k = Ru, \u00b5k = \u00b5.\nMSDnetwork\natc,C\u0338=I \u2264MSDnetwork\natc,C=I\nCT RvC \u2264Rv, C doubly stochastic, Ru,k = Ru, \u00b5k = \u00b5.\nMSDnetwork\natc,C=I \u2264MSDnetwork\natc,C\u0338=I\nCT RvC \u2265Rv, C doubly stochastic, Ru,k = Ru, \u00b5k = \u00b5.\nMSDnetwork\natc\n\u2264MSDnetwork\ncta\n\u2264MSDnetwork\nlms\n{A, C} doubly stochastic, Ru,k = Ru, \u00b5k = \u00b5.\n7.3\nComparing Di\ufb00usion Strategies with the Non-Cooperative Strategy\nWe now compare the performance of the adaptive CTA strategy (154) to the non-cooperative LMS strategy\n(207) assuming conditions (310)\u2013(312) for uniform data pro\ufb01le. These scenarios correspond to the following\nselections in the general description (201)\u2013(203):\nC, A1 = A, A2 = I\n(adaptive CTA)\n(360)\nC = I, A1 = I, A2 = I\n(non-cooperative LMS)\n(361)\nwhere A is further assumed to be doubly stochastic (along with C) so that\nA1 = 1,\nAT 1 = 1\n(362)\nThen, expressions (322) and (323) give:\nBcta\n=\nAT \u2297(I \u2212\u00b5Ru),\nYcta = \u00b52(CT RvC \u2297Ru)\n(363)\nBlms\n=\nI \u2297(I \u2212\u00b5Ru),\nYlms = \u00b52(Rv \u2297Ru)\n(364)\nNow recall that\nC = C \u2297IM\n(365)\nso that, using the Kronecker product property (317),\nYcta\n=\n\u00b52(CT RvC \u2297Ru)\n=\n\u00b52(CT \u2297IM)(Rv \u2297Ru)(C \u2297IM)\n=\n\u00b52CT (Rv \u2297Ru)C\n=\nCTYlms C\n(366)\nThen,\nMSDnetwork\nlms\n\u2212MSDnetwork\ncta\n=\n1\nN\n\u221e\nX\nj=0\nTr\n\u0010\nBj\nlmsYlmsB\u2217j\nlms\n\u0011\n\u2212\n1\nN\n\u221e\nX\nj=0\nTr\n\u0010\nBj\nctaCT YlmsCB\u2217j\ncta\n\u0011\n=\n1\nN\n\u221e\nX\nj=0\nTr\n\u0010\nB\u2217j\nlmsBj\nlmsYlms\n\u0011\n\u2212\n1\nN\n\u221e\nX\nj=0\nTr\n\u0010\nCB\u2217j\nctaBj\nctaCT Ylms\n\u0011\n=\n1\nN\n\u221e\nX\nj=0\nTr\nh\u0010\nB\u2217j\nlmsBj\nlms \u2212CB\u2217j\nctaBj\nctaCT \u0011\nYlms\ni\n(367)\n64\nLet us examine the di\ufb00erence:\nB\u2217j\nlmsBj\nlms \u2212CB\u2217j\nctaBj\nctaCT\n=\n\u0000I \u2297(I \u2212\u00b5Ru)2j\u0001\n\u2212\n\u0000CAj \u2297(I \u2212\u00b5Ru)j\u0001 \u0000AjT CT \u2297(I \u2212\u00b5Ru)j\u0001\n(317)\n=\n\u0000I \u2297(I \u2212\u00b5Ru)2j\u0001\n\u2212\n\u0000CAjAjT CT \u2297(I \u2212\u00b5Ru)2j\u0001\n=\n(I \u2212CAjAjT CT ) \u2297(I \u2212\u00b5Ru)2j\n(368)\nNow, due to the even power, it always holds that (I \u2212\u00b5Ru)2j \u22650. Moreover, since Aj and C are doubly\nstochastic, it follows that CAjAjT CT is also doubly stochastic. Therefore, the matrix (I \u2212CAjAjT CT ) is\nnonnegative-de\ufb01nite as well (cf. property (e) of Lemma C.3). It follows that\nB\u2217j\nlmsBj\nlms \u2212CB\u2217j\nctaBj\nctaCT \u22650\n(369)\nBut since Ylms \u22650, we conclude from (367) that\nMSDnetwork\nlms\n\u2265MSDnetwork\ncta\n(370)\nThis is because for any two Hermitian nonnegative-de\ufb01nite matrices A and B of compatible dimensions, it\nholds that Tr(AB) \u22650; indeed if we factor B = XX\u2217with X full rank, then Tr(AB) = Tr(X\u2217AX) \u22650. We\nconclude from this analysis that adaptive CTA di\ufb00usion performs better than non-cooperative LMS under\nuniform data pro\ufb01le conditions and doubly stochastic A. If we refer to the earlier result (343), we conclude\nthat the following relation holds:\nMSDnetwork\natc\n\u2264MSDnetwork\ncta\n\u2264MSDnetwork\nlms\n(371)\nTable 6 lists the comparison results derived in this section and lists the conditions under which the conclusions\nhold.\n8\nSelecting the Combination Weights\nThe adaptive di\ufb00usion strategy (201)\u2013(203) employs combination weights {a1,\u2113k, a2,\u2113k, c\u2113k} or, equivalently,\ncombination matrices {A1, A2, C}, where A1 and A2 are left-stochastic matrices and C is a right-stochastic\nmatrix.\nThere are several ways by which these matrices can be selected.\nIn this section, we describe\nconstructions that result in left-stochastic or doubly-stochastic combination matrices, A. When a right-\nstochastic combination matrix is needed, such as C, then it can be obtained by transposition of the left-\nstochastic constructions shown below.\n8.1\nConstant Combination Weights\nTable 7 lists a couple of common choices for selecting constant combination weights for a network with N\nnodes. Several of these constructions appeared originally in the literature on graph theory. In the table, the\nsymbol nk denotes the degree of node k, which refers to the size of its neighborhood. Likewise, the symbol\nnmax refers to the maximum degree across the network, i.e.,\nnmax =\nmax\n1\u2264k\u2264N {nk}\n(372)\nThe Laplacian rule, which appears in the second line of the table, relies on the use of the Laplacian matrix\nL of the network and a positive scalar \u03b3. The Laplacian matrix is de\ufb01ned by (574) in App. B, namely, it is\na symmetric matrix whose entries are constructed as follows [64\u201366]:\n[L]k\u2113=\n\uf8f1\n\uf8f2\n\uf8f3\nnk \u22121,\nif k = \u2113\n\u22121,\nif k \u0338= \u2113and nodes k and \u2113are neighbors\n0,\notherwise\n(373)\n65\nThe Laplacian rule can be reduced to other forms through the selection of the positive parameter \u03b3. One\nchoice is \u03b3 = 1/nmax, while another choice is \u03b3 = 1/N and leads to the maximum-degree rule. Obviously,\nit always holds that nmax \u2264N so that 1/nmax \u22651/N. Therefore, the choice \u03b3 = 1/nmax ends up assigning\nlarger weights to neighbors than the choice \u03b3 = 1/N. The averaging rule in the \ufb01rst row of the table is one\nof the simplest combination rules whereby nodes simply average data from their neighbors.\nTable 7: Selections for combination matrices A = [a\u2113k].\nEntries of Combination Matrix A\nType of A\n1. Averaging rule [68]:\na\u2113k =\n\u001a 1/nk,\nif k \u0338= \u2113are neighbors or k = \u2113\n0,\notherwise\nleft-stochastic\n2. Laplacian rule [49,69]:\nA = IN \u2212\u03b3L, \u03b3 > 0\nsymmetric and\ndoubly-stochastic\n3. Laplacian rule using \u03b3 = 1/nmax :\na\u2113k =\n\uf8f1\n\uf8f2\n\uf8f3\n1/nmax,\nif k \u0338= \u2113are neighbors\n1 \u2212(nk \u22121)/nmax,\nk = \u2113\n0,\notherwise\nsymmetric and\ndoubly-stochastic\n4. Laplacian rule using \u03b3 = 1/N(maximum-degree rule [50]) :\na\u2113k =\n\uf8f1\n\uf8f2\n\uf8f3\n1/N,\nif k \u0338= \u2113are neighbors\n1 \u2212(nk \u22121)/N,\nk = \u2113\n0,\notherwise\nsymmetric and\ndoubly-stochastic\n5. Metropolis rule [49,70,71]:\na\u2113k =\n\uf8f1\n\uf8f4\n\uf8f4\n\uf8f2\n\uf8f4\n\uf8f4\n\uf8f3\n1/ max{nk, n\u2113},\nif k \u0338= \u2113are neighbors\n1 \u2212\nX\n\u2113\u2208Nk\\{k}\na\u2113k,\nk = \u2113\n0,\notherwise\nsymmetric and\ndoubly-stochastic\n6. Relative-degree rule [29]:\na\u2113k =\n\uf8f1\n\uf8f4\n\uf8f4\n\uf8f2\n\uf8f4\n\uf8f4\n\uf8f3\nn\u2113/\n\uf8eb\n\uf8edX\nm\u2208Nk\nnm\n\uf8f6\n\uf8f8,\nif k and \u2113are neighbors or k = \u2113\n0,\notherwise\nleft-stochastic\nIn the constructions in Table 7, the values of the weights {a\u2113k} are largely dependent on the degree of the\nnodes. In this way, the number of connections that each node has in\ufb02uences the combination weights with\nits neighbors. While such selections may be appropriate in some applications, they can nevertheless degrade\nthe performance of adaptation over networks [54]. This is because such weighting schemes ignore the noise\npro\ufb01le across the network. And since some nodes can be noisier than others, it is not su\ufb03cient to rely solely\non the amount of connectivity that nodes have to determine the combination weights to their neighbors. It is\nimportant to take into account the amount of noise that is present at the nodes as well. Therefore, designing\ncombination rules that are aware of the variation in noise pro\ufb01le across the network is an important task.\n66\nIt is also important to devise strategies that are able to adapt these combination weights in response to\nvariations in network topology and data statistical pro\ufb01le. For this reason, following [58,62], we describe in\nthe next subsection one adaptive procedure for adjusting the combination weights. This procedure allows\nthe network to assign more or less relevance to nodes according to the quality of their data.\n8.2\nOptimizing the Combination Weights\nIdeally, we would like to select N \u00d7 N combination matrices {A1, A2, C} in order to minimize the network\nMSD given by (302) or (306). In [18], the selection of the combination weights was formulated as the following\noptimization problem:\nmin\n{A1,A2,C}\nMSDnetwork given by (302) or (306)\nover left and right-stochastic matrices with nonnegative entries:\nAT\n1 1 = 1,\na1,\u2113k = 0 if \u2113/\u2208Nk\nAT\n2 1 = 1,\na2,\u2113k = 0 if \u2113/\u2208Nk\nC1 = 1,\nc\u2113k = 0 if \u2113/\u2208Nk\n(374)\nWe can pursue a numerical solution to (374) in order to search for optimal combination matrices, as was\ndone in [18]. Here, however, we are interested in an adaptive solution that becomes part of the learning\nprocess so that the network can adapt the weights on the \ufb02y in response to network conditions. We illustrate\nan approximate approach from [58,62] that leads to one adaptive solution that performs reasonably well in\npractice.\nWe illustrate the construction by considering the ATC strategy (158) without information exchange where\nA1 = IN, A2 = A, and C = I. In this case, recursions (204)\u2013(206) take the form:\n\u03c8k,i\n=\nwk,i\u22121 + \u00b5ku\u2217\nk,i [dk(i) \u2212uk,iwk,i\u22121]\n(375)\nwk,i\n=\nX\n\u2113\u2208Nk\na\u2113k\u03c8\u2113,i\n(376)\nand, from (306), the corresponding network MSD performance is:\nMSDnetwork\natc\n=\n1\nN\n\u221e\nX\nj=0\nTr\n\u0010\nBj\natcYatcB\u2217j\natc\n\u0011\n(377)\nwhere\nBatc\n=\nAT (I \u2212MRu)\n(378)\nYatc\n=\nAT MSMA\n(379)\nRu\n=\ndiag{Ru,1, Ru,2, . . . , Ru,N}\n(380)\nS\n=\ndiag{\u03c32\nv,1Ru,1, \u03c32\nv,2Ru,2, . . . , \u03c32\nv,NRu,N}\n(381)\nM\n=\ndiag{\u00b51IM, \u00b52IM, . . . , \u00b5NIM}\n(382)\nA\n=\nA \u2297IM\n(383)\nMinimizing the MSD expression (377) over left-stochastic matrices A is generally non-trivial. We pursue an\napproximate solution.\nTo begin with, for compactness of notation, let r denote the spectral radius of the N \u00d7 N block matrix\nI \u2212MRu:\nr\n\u2206=\n\u03c1(I \u2212MRu)\n(384)\n67\nWe already know, in view of the mean and mean-square stability of the network, that |r| < 1. Now, consider\nthe series that appears in (377) and whose trace we wish to minimize over A. Note that its block maximum\nnorm can be bounded as follows:\n\r\r\r\r\r\r\n\u221e\nX\nj=0\nBj\natcYatcB\u2217j\natc\n\r\r\r\r\r\r\nb,\u221e\n\u2264\n\u221e\nX\nj=0\n\r\r\rBj\natc\n\r\r\r\nb,\u221e\u00b7 \u2225Yatc\u2225b,\u221e\u00b7\n\r\r\rB\u2217j\natc\n\r\r\r\nb,\u221e\n(a)\n\u2264\nN \u00b7\n\uf8eb\n\uf8ed\n\u221e\nX\nj=0\n\r\r\rBj\natc\n\r\r\r\n2\nb,\u221e\u00b7 \u2225Yatc\u2225b,\u221e\n\uf8f6\n\uf8f8\n\u2264\nN \u00b7\n\uf8eb\n\uf8ed\n\u221e\nX\nj=0\n\u2225Batc\u22252j\nb,\u221e\u00b7 \u2225Yatc\u2225b,\u221e\n\uf8f6\n\uf8f8\n(b)\n\u2264\nN \u00b7\n\uf8eb\n\uf8ed\n\u221e\nX\nj=0\nr2j \u00b7 \u2225Yatc\u2225b,\u221e\n\uf8f6\n\uf8f8\n=\nN\n1 \u2212r2 \u00b7 \u2225Yatc\u2225b,\u221e\n(385)\nwhere for step (b) we use result (602) to conclude that\n\u2225Batc\u2225b,\u221e\n=\n\u2225AT (I \u2212MRu)\u2225b,\u221e\n\u2264\n\u2225AT \u2225b,\u221e\u00b7 \u2225I \u2212MRu\u2225b,\u221e\n=\n\u2225I \u2212MRu\u2225b,\u221e\n(602)\n=\nr\n(386)\nTo justify step (a), we use result (584) to relate the norms of Bj\natc and its complex conjugate,\nh\nBj\natc\ni\u2217\n, as\n\u2225B\u2217j\natc\u2225b,\u221e\u2264N \u00b7 \u2225Bj\natc\u2225b,\u221e\n(387)\nExpression (385) then shows that the norm of the series appearing in (377) is bounded by a scaled multiple\nof the norm of Yatc, and the scaling constant is independent of A. Using property (586) we conclude that\nthere exists a positive constant c, also independent of A, such that\nTr\n\uf8eb\n\uf8ed\n\u221e\nX\nj=0\nBj\natcYatcB\u2217j\natc\n\uf8f6\n\uf8f8\u2264c \u00b7 Tr(Yatc)\n(388)\nTherefore, instead of attempting to minimize the trace of the series, the above result motivates us to minimize\nan upper bound to the trace. Thus, we consider the alternative problem of minimizing the \ufb01rst term of the\nseries (377), namely,\nmin\nA\nTr(Yatc)\nsubject to AT 1 = 1, a\u2113k \u22650, a\u2113k = 0 if \u2113/\u2208Nk\n(389)\nUsing (379), the trace of Yatc can be expressed in terms of the combination coe\ufb03cients as follows:\nTr(Yatc) =\nN\nX\nk=1\nN\nX\n\u2113=1\n\u00b52\n\u2113a2\n\u2113k \u03c32\nv,\u2113Tr(Ru,\u2113)\n(390)\n68\nso that problem (389) can be decoupled into N separate optimization problems of the form:\nmin\n{a\u2113k}N\n\u2113=1\nN\nX\n\u2113=1\n\u00b52\n\u2113a2\n\u2113k \u03c32\nv,\u2113Tr(Ru,\u2113),\nk = 1, . . . , N\nsubject to\na\u2113k \u22650,\nN\nX\n\u2113=1\na\u2113k = 1,\na\u2113k = 0 if \u2113/\u2208Nk\n(391)\nWith each node \u2113, we associate the following nonnegative noise-data-dependent measure:\n\u03b32\n\u2113\n\u2206= \u00b52\n\u2113\u00b7 \u03c32\nv,\u2113\u00b7 Tr(Ru,\u2113)\n(392)\nThis measure amounts to scaling the noise variance at node \u2113by \u00b52\n\u2113and by the power of the regression\ndata (measured through the trace of its covariance matrix). We shall refer to \u03b32\n\u2113as the noise-data variance\nproduct (or variance product, for simplicity) at node \u2113. Then, the solution of (391) is given by:\na\u2113k =\n\uf8f1\n\uf8f2\n\uf8f3\n\u03b3\u22122\n\u2113\nP\nm\u2208Nk \u03b3\u22122\nm ,\nif \u2113\u2208Nk\n0,\notherwise\n(relative-variance rule)\n(393)\nWe refer to this combination rule as the relative-variance combination rule [58]; it leads to a left-stochastic\nmatrix A. In this construction, node k combines the intermediate estimates {\u03c8\u2113,i} from its neighbors in (376)\nin proportion to the inverses of their variance products, {\u03b3\u22122\nm }. The result is physically meaningful. Nodes\nwith smaller variance products will generally be given larger weights. In comparison, the following relative-\ndegree-variance rule was proposed in [18] (a typo appears in Table III in [18], where the noise variances\nappear written in the table instead of their inverses):\na\u2113k =\n\uf8f1\n\uf8f2\n\uf8f3\nn\u2113\u03c3\u22122\nv,\u2113\nP\nm\u2208Nk nm\u03c3\u22122\nv,m ,\nif \u2113\u2208Nk\n0,\notherwise\n(relative degree-variance rule)\n(394)\nThis second form also leads to a left-stochastic combination matrix A. However, rule (394) does not take into\naccount the covariance matrices of the regression data across the network. Observe that in the special case\nwhen step-sizes, the regression covariance matrices, and the noise variances are uniform across the network,\ni.e., \u00b5k = \u00b5, Ru,k = Ru, and \u03c32\nv,k = \u03c32\nv for all k, expression (393) reduces to the simple averaging rule (\ufb01rst\nline of Table 7). In contrast, expression (394) reduces the relative degree rule (last line of Table 7).\n8.3\nAdaptive Combination Weights\nTo evaluate the combination weights (393), the nodes need to know the variance products, {\u03b32\nm}, of their\nneighbors. According to (392), the factors {\u03b32\nm} are de\ufb01ned in terms of the noise variances, {\u03c32\nv,m}, and the\nregression covariance matrices, {Tr(Ru,m)}, and these quantities are not known beforehand. The nodes only\nhave access to realizations of {dm(i), um,i}. We now describe one procedure that allows every node k to\nlearn the variance products of its neighbors in an adaptive manner. Note that if a particular node \u2113happens\nto belong to two neighborhoods, say, the neighborhood of node k1 and the neighborhood of node k2, then\n69\neach of k1 and k2 need to evaluate the variance product, \u03b32\n\u2113, of node \u2113. The procedure described below allows\neach node in the network to estimate the variance products of its neighbors in a recursive manner.\nTo motivate the algorithm, we refer to the ATC recursion (375)\u2013(376) and use the data model (208) to\nwrite for node \u2113:\n\u03c8\u2113,i = w\u2113,i\u22121 + \u00b5\u2113u\u2217\n\u2113,i [u\u2113,i ew\u2113,i\u22121 + v\u2113(i)]\n(395)\nso that, in view of our earlier assumptions on the regression data and noise in Sec. 6.1, we obtain in the limit\nas i \u2192\u221e:\nlim\ni\u2192\u221eE\n\r\r\u03c8\u2113,i \u2212w\u2113,i\u22121\n\r\r2 = \u00b52\n\u2113\u00b7\n\u0012\nlim\ni\u2192\u221eE\u2225ewi\u22121\u22252\nE(u\u2217\n\u2113,i\u2225u\u2113,i\u22252u\u2113,i)\n\u0013\n+ \u00b52\n\u2113\u00b7 \u03c32\nv,\u2113\u00b7 Tr(Ru,\u2113)\n(396)\nWe can evaluate the limit on the right-hand side by using the steady-state result (284). Indeed, we select\nthe vector \u03c3 in (284) to satisfy\n(I \u2212F)\u03c3 = vec\n\u0002\nE\n\u0000u\u2217\n\u2113,i\u2225u\u2113,i\u22252u\u2113,i\n\u0001\u0003\n(397)\nThen, from (284),\nlim\ni\u2192\u221eE\u2225ewi\u22121\u22252\nE(u\u2217\n\u2113,i\u2225u\u2113,i\u22252u\u2113,i) =\n\u0002\nvec\n\u0000YT \u0001\u0003T \u00b7 (I \u2212F)\u22121 \u00b7 vec\n\u0002\nE\n\u0000u\u2217\n\u2113,i\u2225u\u2113,i\u22252u\u2113,i\n\u0001\u0003\n(398)\nNow recall from expression (379) for Y that for the ATC algorithm under consideration we have\nY = AT MSMA\n(399)\nso that the entries of Y depend on combinations of the squared step-sizes, {\u00b52\nm, m = 1, 2, . . ., N}. This fact\nimplies that the \ufb01rst term on the right-hand side of (396) depends on products of the form {\u00b52\n\u2113\u00b52\nm}; these\nfourth-order factors can be ignored in comparison to the second-order factor \u00b52\n\u2113for small step-sizes so that\nlim\ni\u2192\u221eE\n\r\r\u03c8\u2113,i \u2212w\u2113,i\u22121\n\r\r2\n\u2248\n\u00b52\n\u2113\u00b7 \u03c32\nv,\u2113\u00b7 Tr(Ru,\u2113)\n=\n\u03b32\n\u2113\n(400)\nin terms of the desired variance product, \u03b32\n\u2113. Using the following instantaneous approximation at node k\n(where w\u2113,i\u22121 is replaced by wk,i\u22121):\nE\u2225\u03c8\u2113,i \u2212w\u2113,i\u22121\u22252 \u2248\u2225\u03c8\u2113,i \u2212wk,i\u22121\u22252\n(401)\nwe can motivate an algorithm that enables node k to estimate the variance product, \u03b32\n\u2113, of its neighbor \u2113.\nThus, let \u03b32\n\u2113k(i) denote an estimate for \u03b32\n\u2113that is computed by node k at time i. Then, one way to evaluate\n\u03b32\n\u2113k(i) is through the recursion:\n\u03b32\n\u2113k(i) = (1 \u2212\u03bdk) \u00b7 \u03b32\n\u2113k(i \u22121) + \u03bdk \u00b7 \u2225\u03c8\u2113,i \u2212wk,i\u22121\u22252\n(402)\nwhere 0 < \u03bdk \u226a1 is a positive coe\ufb03cient smaller than one. Note that under expectation, expression (402)\nbecomes\nE\u03b32\n\u2113k(i) = (1 \u2212\u03bdk) \u00b7 E\u03b32\n\u2113k(i \u22121) + \u03bdk \u00b7 E\u2225\u03c8\u2113,i \u2212wk,i\u22121\u22252\n(403)\nso that in steady-state, as i \u2192\u221e,\nlim\ni\u2192\u221eE\u03b32\n\u2113k(i) \u2248(1 \u2212\u03bdk) \u00b7 lim\ni\u2192\u221eE\u03b32\n\u2113k(i \u22121) + \u03bdk \u00b7 \u03b32\n\u2113\n(404)\nHence, we obtain\nlim\ni\u2192\u221eE\u03b32\n\u2113k(i) \u2248\u03b32\n\u2113\n(405)\n70\nThat is, the estimator \u03b32\n\u2113k(i) converges on average close to the desired variance product \u03b32\n\u2113. In this way, we\ncan replace the optimal weights (393) by the adaptive construction:\na\u2113k(i) =\n\uf8f1\n\uf8f2\n\uf8f3\n\u03b3\u22122\n\u2113k (i)\nP\nm\u2208Nk \u03b3\u22122\nmk(i),\nif \u2113\u2208Nk\n0,\notherwise\n(406)\nEquations (402) and (406) provide one adaptive construction for the combination weights {a\u2113k}.\n9\nDi\ufb00usion with Noisy Information Exchanges\nThe adaptive di\ufb00usion strategy (201)\u2013(203) relies on the fusion of local information collected from neighbor-\nhoods through the use of combination matrices {A1, A2, C}. In the previous section, we described several\nconstructions for selecting such combination matrices. We also motivated and developed an adaptive scheme\nfor the ATC mode of operation (375)\u2013(376) that computes combination weights in a manner that is aware\nof the variation of the variance-product pro\ufb01le across the network. Nevertheless, in addition to the mea-\nsurement noises {vk(i)} at the individual nodes, we also need to consider the e\ufb00ect of perturbations that\nare introduced during the exchange of information among neighboring nodes. Noise over the communication\nlinks can be due to various factors including thermal noise and imperfect channel information. Studying\nthe degradation in mean-square performance that results from these noisy exchanges can be pursued by\nstraightforward extension of the mean-square analysis of Sec. 6, as we proceed to illustrate. Subsequently,\nwe shall use the results to show how the combination weights can also be adapted in the presence of noisy\nexchange links.\n9.1\nNoise Sources over Exchange Links\nTo model noisy links, we introduce an additive noise component into each of the steps of the di\ufb00usion strategy\n(201)\u2013(203) during the operations of information exchange among the nodes. The notation becomes a bit\ncumbersome because we need to account for both the source and destination of the information that is being\nexchanged. For example, the same signal d\u2113(i) that is generated by node \u2113will be broadcast to all the\nneighbors of node \u2113. When this is done, a di\ufb00erent noise will interfere with the exchange of d\u2113(i) over each\nof the edges that link node \u2113to its neighbors. Thus, we will need to use a notation of the form d\u2113k(i), with\ntwo subscripts \u2113and k, to indicate that this is the noisy version of d\u2113(i) that is received by node k from node\n\u2113. The subscript \u2113k indicates that \u2113is the source and k is the sink, i.e., information is moving from \u2113to k.\nFor the reverse situation where information \ufb02ows from node k to \u2113, we would use instead the subscript k\u2113.\nWith this notation in mind, we model the noisy data received by node k from its neighbor \u2113as follows:\nw\u2113k,i\u22121\n=\nw\u2113,i\u22121 + v(w)\n\u2113k,i\u22121\n(407)\n\u03c8\u2113k,i\n=\n\u03c8\u2113,i + v(\u03c8)\n\u2113k,i\n(408)\nu\u2113k,i\n=\nu\u2113,i + v(u)\n\u2113k,i\n(409)\nd\u2113k(i)\n=\nd\u2113(i) + v(d)\n\u2113k (i)\n(410)\nwhere v(w)\n\u2113k,i\u22121 (M \u00d7 1), v(\u03c8)\n\u2113k,i (M \u00d7 1), and v(u)\n\u2113k,i (1 \u00d7 M) are vector noise signals, and v(d)\n\u2113k (i) is a scalar\nnoise signal. These are the noise signals that perturb exchanges over the edge linking source \u2113to sink k\n(i.e., for data sent from node \u2113to node k). The superscripts {(w), (\u03c8), (u), (d)} in each case refer to the\nvariable that these noises perturb. Figure 14 illustrates the various noise sources that perturb the exchange\nof information from node \u2113to node k. The \ufb01gure also shows the measurement noises {v\u2113(i), vk(i)} that exist\nlocally at the nodes in view of the data model (208).\n71\nk\n\u2113\n\u03c8\u2113,i\nu\u2113,i\nd\u2113(i)\nw\u2113,i\u22121\nu\u2113k,i\nd\u2113k(i)\n\u03c8\u2113k,i\nw\u2113k,i\u22121\nv(u)\n\u2113k,i\nv(\u03c8)\n\u2113k,i\nv(w)\n\u2113k,i\u22121\nsource\nsink\nv(d)\n\u2113k (i)\nv\u2113(i)\nvk(i)\nFigure 14: Additive noise sources perturb the exchange of information from node \u2113to node k. The subscript \u2113k in\nthis illustration indicates that \u2113is the source node and k is the sink node so that information is \ufb02owing from \u2113to k.\nWe assume that the following noise signals, which in\ufb02uence the data received by node k,\nn\nvk(i), v(d)\n\u2113k (i), v(w)\n\u2113k,i\u22121, v(\u03c8)\n\u2113k,i, v(u)\n\u2113k,i\no\n(411)\nare temporally white and spatially independent random processes with zero mean and variances or covariances\ngiven by\nn\n\u03c32\nv,k, \u03c32\nv,\u2113k, R(w)\nv,\u2113k, R(\u03c8)\nv,\u2113k, R(u)\nv,\u2113k\no\n(412)\nObviously, the quantities\nn\n\u03c32\nv,\u2113k, R(w)\nv,\u2113k, R(\u03c8)\nv,\u2113k, R(u)\nv,\u2113k\no\n(413)\nare all zero if \u2113/\u2208Nk or when \u2113= k. We further assume that the noise processes (411) are independent of\neach other and of the regression data um,j for all k, \u2113, m and i, j.\n9.2\nError Recursion\nUsing the perturbed data (407)\u2013(410), the adaptive di\ufb00usion strategy (201)\u2013(203) becomes\n\u03c6k,i\u22121\n=\nX\n\u2113\u2208Nk\na1,\u2113k w\u2113k,i\u22121\n(414)\n\u03c8k,i\n=\n\u03c6k,i\u22121 + \u00b5k\nX\n\u2113\u2208Nk\nc\u2113k u\u2217\n\u2113k,i [d\u2113k(i) \u2212u\u2113k,i\u03c6k,i\u22121]\n(415)\nwk,i\n=\nX\n\u2113\u2208Nk\na2,\u2113k \u03c8\u2113k,i\n(416)\n72\nObserve that the perturbed quantities {w\u2113k,i\u22121, u\u2113k,i, d\u2113k(i), \u03c8\u2113k,i}, with subscripts \u2113k, appear in (414)\u2013\n(416) in place of the original quantities {w\u2113,i\u22121, u\u2113,i, d\u2113(i), \u03c8\u2113,i} that appear in (201)\u2013(203). This is because\nthese quantities are now subject to exchange noises. As before, we are still interested in examining the\nevolution of the weight-error vectors:\newk,i\n\u2206= wo \u2212wk,i,\nk = 1, 2, . . ., N\n(417)\nFor this purpose, we again introduce the following N \u00d7 1 block vector, whose entries are of size M \u00d7 1 each:\newi\n\u2206=\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8f0\new1,i\new2,i\n...\newN,i\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fb\n(418)\nand proceed to determine a recursion for its evolution over time. The arguments are largely similar to what\nwe already did before in Sec. 6.3 and, therefore, we shall emphasize the di\ufb00erences that arise. The main\ndeviation is that we now need to account for the presence of the new noise signals; they will contribute\nadditional terms to the recursion for ewi \u2014 see (442) further ahead. We may note that some studies on\nthe e\ufb00ect of imperfect data exchanges on the performance of adaptive di\ufb00usion algorithms are considered\nin [59\u201361]. However, these earlier investigations were limited to particular cases in which only noise in the\nexchange of w\u2113,i\u22121 was considered (as in (407)), in addition to setting C = I (in which case there is no\nexchange of {d\u2113(i), u\u2113,i}), and by focusing on the CTA case for which A2 = I. Here, we consider instead the\ngeneral case that accounts for the additional sources of imperfections shown in (408)\u2013(410), in addition to\nthe general di\ufb00usion strategy (201)\u2013(203) with combination matrices {A1, A2, C}.\nTo begin with, we introduce the aggregate M \u00d7 1 zero-mean noise signals:\nv(w)\nk,i\u22121\n\u2206=\nX\n\u2113\u2208Nk\na1,\u2113k v(w)\n\u2113k,i\u22121,\nv(\u03c8)\nk,i\n\u2206=\nX\n\u2113\u2208Nk\na2,\u2113k v(\u03c8)\n\u2113k,i\n(419)\nThese noises represent the aggregate e\ufb00ect on node k of all exchange noises from the neighbors of node k\nwhile exchanging the estimates {w\u2113,i\u22121, \u03c8\u2113,i} during the two combination steps (201) and (203). The M \u00d7M\ncovariance matrices of these noises are given by\nR(w)\nv,k\n\u2206=\nX\n\u2113\u2208Nk\na2\n1,\u2113k R(w)\nv,\u2113k,\nR(\u03c8)\nv,k\n\u2206=\nX\n\u2113\u2208Nk\na2\n2,\u2113k R(\u03c8)\nv,\u2113k\n(420)\nThese expressions aggregate the exchange noise covariances in the neighborhood of node k; the covariances\nare scaled by the squared coe\ufb03cients {a2\n1,\u2113k, a2\n2,\u2113k}. We collect these noise signals, and their covariances,\nfrom across the network into N \u00d7 1 block vectors and N \u00d7 N block diagonal matrices as follows:\nv(w)\ni\u22121\n\u2206=\ncol\nn\nv(w)\n1,i\u22121, v(w)\n2,i\u22121, . . . , v(w)\nN,i\u22121\no\n(421)\nv(\u03c8)\ni\n\u2206=\ncol\nn\nv(\u03c8)\n1,i , v(\u03c8)\n2,i , . . . , v(\u03c8)\nN,i\no\n(422)\nR(w)\nv\n\u2206=\ndiag\nn\nR(w)\nv,1 , R(w)\nv,2 , . . . , R(w)\nv,N\no\n(423)\nR(\u03c8)\nv\n\u2206=\ndiag\nn\nR(\u03c8)\nv,1 , R(\u03c8)\nv,2 , . . . , R(\u03c8)\nv,N\no\n(424)\nWe further introduce the following scalar zero-mean noise signal:\nv\u2113k(i)\n\u2206= v\u2113(i) + v(d)\n\u2113k (i) \u2212v(u)\n\u2113k,i wo\n(425)\n73\nwhose variance is\n\u03c32\n\u2113k = \u03c32\nv,\u2113+ \u03c32\nv,\u2113k + wo\u2217R(u)\nv,\u2113kwo\n(426)\nIn the absence of exchange noises for the data {d\u2113(i), u\u2113,i}, the signal v\u2113k(i) would coincide with the measure-\nment noise v\u2113(i). Expression (425) is simply a re\ufb02ection of the aggregate e\ufb00ect of the noises in exchanging\n{d\u2113(i), u\u2113,i} on node k. Indeed, starting from the data model (208) and using (409)\u2013(410), we can easily\nverify that the noisy data {d\u2113k(i), u\u2113k,i} are related via:\nd\u2113k(i) = u\u2113k,iwo + v\u2113k(i)\n(427)\nWe also de\ufb01ne (compare with (234)\u2013(235) and note that we are now using the perturbed regression vectors\n{u\u2113k,i}):\nR\u2032\nk,i\n\u2206=\nX\n\u2113\u2208Nk\nc\u2113ku\u2217\n\u2113k,iu\u2113k,i\n(428)\nR\u2032\ni\n\u2206=\ndiag\n\b\nR\u2032\n1,i, R\u2032\n2,i, . . . , R\u2032\nN,i\n\t\n(429)\nIt holds that\nER\u2032\nk,i = R\u2032\nk\n(430)\nwhere\nR\u2032\nk\n\u2206=\nX\n\u2113\u2208Nk\nc\u2113k\nh\nRu,\u2113+ R(u)\nv,\u2113k\ni\n(431)\nWhen there is no noise during the exchange of the regression data, i.e., when R(u)\nv,\u2113k = 0, the expressions for\n{R\u2032\nk,i, R\u2032\ni, R\u2032\nk} reduce to expressions (234)\u2013(235) and (182) for {Rk,i, Ri, Rk}.\nLikewise, we introduce (compare with (239)):\nzk,i\n\u2206=\nX\n\u2113\u2208Nk\nc\u2113ku\u2217\n\u2113k,iv\u2113k(i)\n(432)\nzi\n\u2206=\ncol {z1,i, z2,i, . . . , zN,i}\n(433)\nCompared with the earlier de\ufb01nition for si in (239) when there is no noise over the exchange links, we see\nthat we now need to account for the various noisy versions of the same regression vector u\u2113,i and the same\nsignal d\u2113(i). For instance, the vectors u\u2113k,i and u\u2113m,i would denote two noisy versions received by nodes k\nand m for the same regression vector u\u2113,i transmitted from node \u2113. Likewise, the scalars d\u2113k(i) and d\u2113m(i)\nwould denote two noisy versions received by nodes k and m for the same scalar d\u2113(i) transmitted from node\n\u2113. As a result, the quantity zi is not zero mean any longer (in contrast to si, which had zero mean). Indeed,\nnote that\nEzk,i\n=\nX\n\u2113\u2208Nk\nc\u2113k Eu\u2217\n\u2113k,iv\u2113k(i)\n=\nX\n\u2113\u2208Nk\nc\u2113k E\n\u0010h\nu\u2113,i + v(u)\n\u2113k,i\ni\u2217\n\u00b7\nh\nv\u2113(i) + v(d)\n\u2113k (i) \u2212v(u)\n\u2113k,i woi\u0011\n=\n\u2212\n X\n\u2113\u2208Nk\nc\u2113k R(u)\nv,\u2113k\n!\nwo\n(434)\n74\nIt follows that\nEzi = \u2212\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0\nX\n\u2113\u2208N1\nc\u21131 R(u)\nv,\u21131\nX\n\u2113\u2208N2\nc\u21132 R(u)\nv,\u21132\n...\nX\n\u2113\u2208NN\nc\u2113N R(u)\nv,\u2113N\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb\nwo\n(435)\nAlthough we can continue our analysis by studying this general case in which the vectors zi do not have\nzero-mean (see [62,63]), we shall nevertheless limit our discussion in the sequel to the case in which there is\nno noise during the exchange of the regression data, i.e., we henceforth assume that:\nv(u)\n\u2113k,i = 0,\nR(u)\nv,\u2113k = 0,\nu\u2113k,i = u\u2113,i\n(assumption from this point onwards)\n(436)\nWe maintain all other noise sources, which occur during the exchange of the weight estimates {w\u2113,i\u22121, \u03c8\u2113,i}\nand the data {d\u2113(i)}. Under condition (436), we obtain\nEzi\n=\n0\n(437)\n\u03c32\n\u2113k\n=\n\u03c32\nv,\u2113+ \u03c32\nv,\u2113k\n(438)\nR\u2032\nk\n=\nX\n\u2113\u2208Nk\nc\u2113k Ru,\u2113\n(182)\n=\nRk\n(439)\nThen, the covariance matrix of each term zk,i is given by\nRz,k\n\u2206=\nX\n\u2113\u2208Nk\nc2\n\u2113k \u03c32\n\u2113k Ru,\u2113\n(440)\nand the covariance matrix of zi is N \u00d7 N block diagonal with blocks of size M \u00d7 M:\nZ\n\u2206= Eziz\u2217\ni = diag{Rz,1, Rz,2, . . . , Rz,N}\n(441)\nNow repeating the argument that led to (246) we arrive at the following recursion for the weight-error vector:\newi = AT\n2\n\u0000INM \u2212MR\u2032\ni\n\u0001\nAT\n1 ewi\u22121 \u2212AT\n2 Mzi \u2212AT\n2\n\u0000INM \u2212MR\u2032\ni\n\u0001\nv(w)\ni\u22121 \u2212v(\u03c8)\ni\n(noisy links)\n(442)\nFor comparison purposes, we repeat recursion (246) here (recall that this recursion corresponds to the case\nwhen the exchanges over the links are not subject to noise):\newi = AT\n2 (INM \u2212MRi) AT\n1 ewi\u22121 \u2212AT\n2 MCT si\n(perfect links)\n(443)\nComparing (442) and (443) we \ufb01nd that:\n(1) The covariance matrix Ri in (443) is replaced by R\u2032\ni. Recall from (429) that R\u2032\ni contains the in\ufb02uence\nof the noises that arise during the exchange of the regression data, i.e., the {R(u)\nv,\u2113k}. But since we are\nnow assuming that R(u)\nv,\u2113k = 0, then R\u2032\ni = Ri.\n75\n(2) The term CT si in (443) is replaced by zi. Recall from (432) that zi contains the in\ufb02uence of the noises\nthat arise during the exchange of the measurement data and the regression data, i.e., the {\u03c32\nv,\u2113k, R(u)\nv,\u2113k}.\n(3) Two new driving terms appear involving v(w)\ni\u22121 and v(\u03c8)\ni\n. These terms re\ufb02ect the in\ufb02uence of the noises\nduring the exchange of the weight estimates {w\u2113,i\u22121, \u03c8\u2113,i}.\n(4) Observe further that:\n(4a) The term involving v(w)\ni\u22121 accounts for noise introduced at the information-exchange step (414)\nbefore adaptation.\n(4b) The term involving zi accounts for noise introduced during the adaptation step (415).\n(4c) The term involving v(\u03c8)\ni\naccounts for noise introduced at the information-exchange step (416)\nafter adaptation.\nTherefore, since we are not considering noise during the exchange of the regression data, the weight-error\nrecursion (442) simpli\ufb01es to:\newi = AT\n2 (INM \u2212MRi) AT\n1 ewi\u22121 \u2212AT\n2 Mzi \u2212AT\n2 (INM \u2212MRi) v(w)\ni\u22121 \u2212v(\u03c8)\ni\n(noisy links)\n(444)\nwhere we used the fact that R\u2032\ni = Ri under these conditions.\n9.3\nConvergence in the Mean\nTaking expectations of both sides of (444) we \ufb01nd that the mean error vector evolves according to the\nfollowing recursion:\nE ewi = AT\n2 (INM \u2212MR) AT\n1 \u00b7 E ewi\u22121, i \u22650\n(445)\nwith R de\ufb01ned by (181).\nThis is the same recursion encountered earlier in (248) during perfect data\nexchanges. Note that had we considered noises during the exchange of the regression data, then the vector\nzi in (444) would not be zero mean and the matrix R\u2032\ni will have to be used instead of Ri. In that case, the\nrecursion for E ewi will be di\ufb00erent from (445); i.e., the presence of noise during the exchange of regression\ndata alters the dynamics of the mean error vector in an important way \u2014 see [62,63] for details on how to\nextend the arguments to this general case with a driving non-zero bias term. We can now extend Theorem 6.1\nto the current scenario.\nTheorem 9.1. (Convergence in the Mean) Consider the problem of optimizing the global cost (92) with\nthe individual cost functions given by (93). Pick a right stochastic matrix C and left stochastic matrices A1\nand A2 satisfying (166). Assume each node in the network runs the perturbed adaptive di\ufb00usion algorithm\n(414)\u2013(416). Assume further that the exchange of the variables {w\u2113,i\u22121, \u03c8\u2113,i, d\u2113(i)} is subject to additive\nnoises as in (407), (408), and (410). We assume that the regressors are exchanged unperturbed. Then, all\nestimators {wk,i} across the network will still converge in the mean to the optimal solution wo if the step-size\nparameters {\u00b5k} satisfy\n\u00b5k <\n2\n\u03bbmax(Rk)\n(446)\nwhere the neighborhood covariance matrix Rk is de\ufb01ned by (182). That is, Ewk,i \u2192wo as i \u2192\u221e.\n\u25a1\n76\n9.4\nMean-Square Convergence\nRecall from (264) that we introduced the matrix:\nB\n\u2206= AT\n2 (INM \u2212MR) AT\n1\n(447)\nWe further introduce the N \u00d7 N block matrix with blocks of size M \u00d7 M each:\nH\n\u2206= AT\n2 (INM \u2212MR)\n(448)\nThen, starting from (444) and repeating the argument that led to (279) we can establish the validity of the\nfollowing variance relation:\nE\u2225ewi\u22252\n\u03c3 = E\u2225ewi\u22121\u22252\nF\u03c3 +\n\u0014\nvec\n\u0000AT\n2 MZT MA2\n\u0001\n+ vec\n\u0012\u0010\nHR(w)T\nv\nH\u2217\u0011T \u0013\n+ vec\n\u0010\nR(\u03c8)T\nv\n\u0011\u0015T\n\u03c3\n(449)\nfor an arbitrary nonnegative-de\ufb01nite weighting matrix \u03a3 with \u03c3 = vec(\u03a3), and where F is the same matrix\nde\ufb01ned earlier either by (276) or (277). We can therefore extend the statement of Theorem 6.7 to the present\nscenario.\nTheorem 9.2. (Mean-Square Stability) Consider the same setting of Theorem 9.1. Assume su\ufb03ciently\nsmall step-sizes to justify ignoring terms that depend on higher powers of the step-sizes.\nThe perturbed\nadaptive di\ufb00usion algorithm (414)\u2013(416) is mean-square stable if, and only if, the matrix F de\ufb01ned by\n(276), or its approximation (277), is stable (i.e., all its eigenvalues are strictly inside the unit disc). This\ncondition is satis\ufb01ed by su\ufb03ciently small step-sizes {\u00b5k} that are also smaller than:\n\u00b5k <\n2\n\u03bbmax(Rk)\n(450)\nwhere the neighborhood covariance matrix Rk is de\ufb01ned by (182). Moreover, the convergence rate of the\nalgorithm is determined by [\u03c1(B)]2.\n\u25a1\nWe conclude from the previous two theorems that the conditions for the mean and mean-square convergence\nof the adaptive di\ufb00usion strategy are not a\ufb00ected by the presence of noises over the exchange links (under\nthe assumption that the regression data are exchanged without perturbation; otherwise, the convergence\nconditions would be a\ufb00ected). The mean-square performance, on the other hand, is a\ufb00ected as follows.\nIntroduce the N \u00d7 N block matrix:\nYimperfect\n\u2206= AT\n2 MZMA2 + HR(w)\nv\nH\u2217+ R(\u03c8)\nv\n(imperfect exchanges)\n(451)\nwhich should be compared with the corresponding quantity de\ufb01ned by (280) for the perfect exchanges case,\nnamely,\nYperfect = AT\n2 MCTSCMA2\n(perfect exchanges)\n(452)\nWhen perfect exchanges occur, the matrix Z reduces to CT SC. We can relate Yimperfect and Yperfect as\nfollows. Let\nR(du)\n\u2206= diag\n( X\n\u2113\u2208N1\nc2\n\u21131\u03c32\nv,\u21131Ru,\u2113,\nX\n\u2113\u2208N2\nc2\n\u21132\u03c32\nv,\u21132Ru,\u2113, . . .\nX\n\u2113\u2208NN\nc2\n\u2113N\u03c32\nv,\u2113NRu,\u2113\n)\n(453)\n77\nThen, using (438) and (441), it is straightforward to verify that\nZ = CT SC + R(du)\n(454)\nand it follows that:\nYimperfect\n=\nYperfect + AT\n2 MR(du)MA2 + HR(w)\nv\nH\u2217+ R(\u03c8)\nv\n\u2206=\nYperfect + \u2206Y\n(455)\nExpression (455) re\ufb02ects the in\ufb02uence of the noises {R(w)\nv\n, R(\u03c8)\nv\n, \u03c32\nv,\u2113k}. Substituting the de\ufb01nition (451)\ninto (449), and taking the limit as i \u2192\u221e, we obtain from the latter expression that:\nlim\ni\u2192\u221eE\u2225ewi\u22252\n(I\u2212F)\u03c3 =\n\u0002\nvec\n\u0000YT\nimperfect\n\u0001\u0003T \u03c3\n(456)\nwhich has the same form as (284); therefore, we can proceed analogously to obtain:\nMSDnetwork\nimperfect =\n1\nN \u00b7\n\u0002\nvec\n\u0000YT\nimperfect\n\u0001\u0003T \u00b7 (I \u2212F)\u22121 \u00b7 vec (INM)\n(457)\nand\nEMSEnetwork\nimperfect =\n1\nN \u00b7\n\u0002\nvec\n\u0000YT\nimperfect\n\u0001\u0003T \u00b7 (I \u2212F)\u22121 \u00b7 vec (Ru)\n(458)\nUsing (455), we see that the network MSD and EMSE deteriorate as follows:\nMSDnetwork\nimperfect\n=\nMSDnetwork\nperfect\n+\n1\nN \u00b7\n\u0002\nvec\n\u0000\u2206YT \u0001\u0003T \u00b7 (I \u2212F)\u22121 \u00b7 vec (INM)\n(459)\nEMSEnetwork\nimperfect\n=\nEMSEnetwork\nperfect\n+\n1\nN \u00b7\n\u0002\nvec\n\u0000\u2206YT \u0001\u0003T \u00b7 (I \u2212F)\u22121 \u00b7 vec (Ru)\n(460)\n9.5\nAdaptive Combination Weights\nWe can repeat the discussion from Secs. 8.2 and 8.3 to devise one adaptive scheme to adjust the combination\ncoe\ufb03cients in the noisy exchange case. We illustrate the construction by considering the ATC strategy\ncorresponding to A1 = IN, A2 = A, C = IN, so that only weight estimates are exchanged and the update\nrecursions are of the form:\n\u03c8k,i\n=\nwk,i\u22121 + \u00b5ku\u2217\nk,i[dk(i) \u2212uk,iwk,i\u22121]\n(461)\nwk,i\n=\nX\n\u2113\u2208Nk\na\u2113k \u03c8\u2113k,i\n(462)\nwhere from (408):\n\u03c8\u2113k,i = \u03c8\u2113,i + v(\u03c8)\n\u2113k,i\n(463)\nIn this case, the network MSD performance (457) becomes\nMSDnetwork\natc,C=I,imperfect =\n1\nN\n\u221e\nX\nj=0\nTr\n\u0010\nBj\natc,C=IYatc,imperfectB\u2217j\natc,C=I\n\u0011\n(464)\n78\nwhere, since now Z = S and R(w)\nv\n= 0, we have\nBatc,C=I\n=\nAT (I \u2212MRu)\n(465)\nYatc,imperfect\n=\nAT MSMA + R(\u03c8)\nv\n(466)\nR(\u03c8)\nv\n=\ndiag\nn\nR(\u03c8)\nv,1 , R(\u03c8)\nv,2 , . . . , R(\u03c8)\nv,N\no\n(467)\nR(\u03c8)\nv,k\n=\nX\n\u2113\u2208Nk\na2\n\u2113k R(\u03c8)\nv,\u2113k\n(468)\nRu\n=\ndiag{Ru,1, Ru,2, . . . , Ru,N}\n(469)\nS\n=\ndiag{\u03c32\nv,1Ru,1, \u03c32\nv,2Ru,2, . . . , \u03c32\nv,NRu,N}\n(470)\nM\n=\ndiag{\u00b51IM, \u00b52IM, . . . , \u00b5NIM}\n(471)\nA\n=\nA \u2297IM\n(472)\nTo proceed, as was the case with (389), we consider the following simpli\ufb01ed optimization problem:\nmin\nA\nTr(Yatc,imperfect)\nsubject to AT 1 = 1, a\u2113k \u22650, a\u2113k = 0 if \u2113/\u2208Nk\n(473)\nUsing (466), the trace of Yatc,imperfect can be expressed in terms of the combination coe\ufb03cients as follows:\nTr(Yatc,imperfect) =\nN\nX\nk=1\nN\nX\n\u2113=1\na2\n\u2113k\nh\n\u00b52\n\u2113\u03c32\nv,\u2113Tr(Ru,\u2113) + Tr\n\u0010\nR(\u03c8)\nv,\u2113k\n\u0011i\n(474)\nso that problem (473) can be decoupled into N separate optimization problems of the form:\nmin\n{a\u2113k}N\n\u2113=1\nN\nX\n\u2113=1\na2\n\u2113k\nh\n\u00b52\n\u2113\u03c32\nv,\u2113Tr(Ru,\u2113) + Tr\n\u0010\nR(\u03c8)\nv,\u2113k\n\u0011i\n,\nk = 1, . . . , N\nsubject to\na\u2113k \u22650,\nN\nX\n\u2113=1\na\u2113k = 1,\na\u2113k = 0 if \u2113/\u2208Nk\n(475)\nWith each node \u2113, we associate the following nonnegative variance products:\n\u03b32\n\u2113k\n\u2206= \u00b52\n\u2113\u00b7 \u03c32\nv,\u2113\u00b7 Tr(Ru,\u2113) + Tr\n\u0010\nR(\u03c8)\nv,\u2113k\n\u0011\n, k \u2208N\u2113\n(476)\nThis measure now incorporates information about the exchange noise covariances {R(\u03c8)\nv,\u2113k}. Then, the solution\nof (475) is given by:\na\u2113k =\n\uf8f1\n\uf8f2\n\uf8f3\n\u03b3\u22122\n\u2113k\nP\nm\u2208Nk \u03b3\u22122\nmk ,\nif \u2113\u2208Nk\n0,\notherwise\n(relative-variance rule)\n(477)\nWe continue to refer to this combination rule as the relative-variance combination rule [58]; it leads to a\nleft-stochastic matrix A. To evaluate the combination weights (477), the nodes need to know the variance\nproducts, {\u03b32\nmk}, of their neighbors. As before, we can motivate one adaptive construction as follows.\n79\nWe refer to the ATC recursion (461)\u2013(463) and use the data model (208) to write for node \u2113:\n\u03c8\u2113k,i = w\u2113,i\u22121 + \u00b5\u2113u\u2217\n\u2113,i [u\u2113,i ew\u2113,i\u22121 + v\u2113(i)] + v(\u03c8)\n\u2113k,i\n(478)\nso that, in view of our earlier assumptions on the regression data and noise in Secs. 6.1 and 9.1, we obtain\nin the limit as i \u2192\u221e:\nlim\ni\u2192\u221eE\n\r\r\u03c8\u2113k,i \u2212w\u2113,i\u22121\n\r\r2 = \u00b52\n\u2113\u00b7\n\u0012\nlim\ni\u2192\u221eE\u2225ewi\u22121\u22252\nE(u\u2217\n\u2113,i\u2225u\u2113,i\u22252u\u2113,i)\n\u0013\n+ \u00b52\n\u2113\u00b7\u03c32\nv,\u2113\u00b7Tr(Ru,\u2113) + Tr\n\u0010\nR(\u03c8)\nv,\u2113k\n\u0011\n(479)\nIn a manner similar to what was done before for (396), we can evaluate the limit on the right-hand side by\nusing the corresponding steady-state result (456). We select the vector \u03c3 in (456) to satisfy:\n(I \u2212F)\u03c3 = vec\n\u0002\nE\n\u0000u\u2217\n\u2113,i\u2225u\u2113,i\u22252u\u2113,i\n\u0001\u0003\n(480)\nThen, from (456),\nlim\ni\u2192\u221eE\u2225ewi\u22121\u22252\nE(u\u2217\n\u2113,i\u2225u\u2113,i\u22252u\u2113,i) =\n\u0002\nvec\n\u0000YT\natc,imperfect\n\u0001\u0003T \u00b7 (I \u2212F)\u22121 \u00b7 vec\n\u0002\nE\n\u0000u\u2217\n\u2113,i\u2225u\u2113,i\u22252u\u2113,i\n\u0001\u0003\n(481)\nNow recall from expression (466) that the entries of Yatc,imperfect depend on combinations of the squared\nstep-sizes, {\u00b52\nm, m = 1, 2, . . . , N}, and on terms involving\nn\nTr\n\u0010\nR(\u03c8)\nv,m\n\u0011o\n. This fact implies that the \ufb01rst\nterm on the right-hand side of (479) depends on products of the form {\u00b52\n\u2113\u00b52\nm}; these fourth-order factors can\nbe ignored in comparison to the second-order factor \u00b52\n\u2113for small step-sizes. Moreover, the same \ufb01rst term\non the right-hand side of (479) depends on products of the form\nn\n\u00b52\n\u2113Tr\n\u0010\nR(\u03c8)\nv,m\n\u0011o\n, which can be ignored in\ncomparison to the last term, Tr\n\u0010\nR(\u03c8)\nv,\u2113k\n\u0011\n, in (479), which does not appear multiplied by a squared step-size.\nTherefore, we can approximate:\nlim\ni\u2192\u221eE\n\r\r\u03c8\u2113k,i \u2212w\u2113,i\u22121\n\r\r2\n\u2248\n\u00b52\n\u2113\u00b7 \u03c32\nv,\u2113\u00b7 Tr(Ru,\u2113) + Tr\n\u0010\nR(\u03c8)\nv,\u2113k\n\u0011\n=\n\u03b32\n\u2113k\n(482)\nin terms of the desired variance product, \u03b32\n\u2113k. Using the following instantaneous approximation at node k\n(where w\u2113,i\u22121 is replaced by wk,i\u22121):\nE\u2225\u03c8\u2113k,i \u2212w\u2113,i\u22121\u22252 \u2248\u2225\u03c8\u2113k,i \u2212wk,i\u22121\u22252\n(483)\nwe can motivate an algorithm that enables node k to estimate the variance products \u03b32\n\u2113k. Thus, let b\u03b32\n\u2113k(i)\ndenote an estimate for \u03b32\n\u2113k that is computed by node k at time i. Then, one way to evaluate b\u03b32\n\u2113k(i) is through\nthe recursion:\nb\u03b32\n\u2113k(i) = (1 \u2212\u03bdk) \u00b7 b\u03b32\n\u2113k(i \u22121) + \u03bdk \u00b7 \u2225\u03c8\u2113k,i \u2212wk,i\u22121\u22252\n(484)\nwhere \u03bdk is a positive coe\ufb03cient smaller than one. Indeed, it can be veri\ufb01ed that\nlim\ni\u2192\u221eE b\u03b32\n\u2113k(i) \u2248\u03b32\n\u2113k\n(485)\nso that the estimator b\u03b32\n\u2113k(i) converges on average close to the desired variance product \u03b32\n\u2113k. In this way, we\ncan replace the weights (477) by the adaptive construction:\na\u2113k(i) =\n\uf8f1\n\uf8f2\n\uf8f3\nb\u03b3\u22122\n\u2113k (i)\nP\nm\u2208Nk b\u03b3\u22122\nmk(i),\nif \u2113\u2208Nk\n0,\notherwise\n(486)\nEquations (484) and (486) provide one adaptive construction for the combination weights {a\u2113k}.\n80\n10\nExtensions and Further Considerations\nSeveral extensions and variations of di\ufb00usion strategies are possible. Among those variations we mention\nstrategies that endow nodes with temporal processing abilities, in addition to their spatial cooperation\nabilities. We can also apply di\ufb00usion strategies to solve recursive least-squares and state-space estimation\nproblems in a distributed manner. In this section, we highlight select contributions in these and related\nareas.\n10.1\nAdaptive Di\ufb00usion Strategies with Smoothing Mechanisms\nIn the ATC and CTA adaptive di\ufb00usion strategies (153)\u2013(154), each node in the network shares information\nlocally with its neighbors through a process of spatial cooperation or combination. In this section, we describe\nbrie\ufb02y an extension that adds a temporal dimension to the processing at the nodes. For example, in the\nATC implementation (153), rather than have each node k rely solely on current data, {d\u2113(i), u\u2113,i, \u2113\u2208Nk},\nand on current weight estimates received from the neighbors, {\u03c8\u2113,i, \u2113\u2208Nk}, node k can be allowed to\nstore and process present and past weight estimates, say, P of them as in {\u03c8\u2113,j, j = i, i \u22121, . . . , i \u2212P + 1}.\nIn this way, previous weight estimates can be smoothed and used more e\ufb00ectively to help enhance the\nmean-square-deviation performance especially in the presence of noise over the communication links.\nTo motivate di\ufb00usion strategies with smoothing mechanisms, we continue to assume that the random\ndata {dk(i), uk,i} satisfy the modeling assumptions of Sec. 6.1. The global cost (92) continues to be the\nsame but the individual cost functions (93) are now replaced by:\nJk(w) =\nP \u22121\nX\nj=0\nqkj E |dk(i \u2212j) \u2212uk,i\u2212j w|2\n(487)\nso that\nJglob(w) =\nN\nX\nk=1\n\uf8eb\n\uf8ed\nP \u22121\nX\nj=0\nqkj E |dk(i \u2212j) \u2212uk,i\u2212j w|2\n\uf8f6\n\uf8f8\n(488)\nwhere each coe\ufb03cient qkj is a non-negative scalar representing the weight that node k assigns to data from\ntime instant i \u2212j. The coe\ufb03cients {qkj} are assumed to satisfy the normalization condition:\nqko > 0,\nP \u22121\nX\nj=0\nqkj = 1,\nk = 1, 2, . . . , N\n(489)\nWhen the random processes dk(i) and uk,i are jointly wide-sense stationary, as was assumed in Sec. 6.1, the\noptimal solution wo that minimizes (488) is still given by the same normal equations (40). We can extend the\narguments of Secs. 3 and 4 to (488) and arrive at the following version of a di\ufb00usion strategy incorporating\ntemporal processing (or smoothing) of the intermediate weight estimates [73,74]:\n\u03c6k,i\n=\nwk,i\u22121 + \u00b5k\nX\n\u2113\u2208Nk\nc\u2113k q\u2113o u\u2217\n\u2113,i [d\u2113(i)\u2212u\u2113,iwk,i\u22121]\n(adaptation)\n(490)\n\u03c8k,i\n=\nP \u22121\nX\nj=0\nfkj \u03c6k,i\u2212j\n(temporal processing or smoothing)\n(491)\nwk,i\n=\nX\n\u2113\u2208Nk\na\u2113k \u03c8\u2113,i\n(spatial processing)\n(492)\n81\nwhere the nonnegative coe\ufb03cients {c\u2113k, a\u2113k, fkj, qlo} satisfy:\nfor k = 1, 2, . . . , N :\nc\u2113k \u22650,\nN\nX\nk=1\nc\u2113k = 1,\nc\u2113k = 0 if \u2113/\u2208Nk\n(493)\na\u2113k \u22650,\nN\nX\n\u2113=1\na\u2113k = 1,\na\u2113k = 0 if \u2113/\u2208Nk\n(494)\nfkj \u22650,\nP \u22121\nX\nj=0\nfkj = 1\n(495)\n0 < q\u2113o \u22641\n(496)\nSince only the coe\ufb03cients {q\u2113o} are needed, we alternatively denote them by the simpler notation {q\u2113} in\nthe listing in Table 8. These are simply chosen as nonnegative coe\ufb03cients:\n0 < q\u2113\u22641,\n\u2113= 1, 2, . . . , N\n(497)\nNote that algorithm (490)-(492) involves three steps: (a) an adaptation step (A) represented by (490); (b) a\ntemporal \ufb01ltering or smoothing step (T) represented by (491), and a spatial cooperation step (S) represented\nby (492). These steps are illustrated in Fig. 15. We use the letters (A,T,S) to label these steps; and we use the\nsequence of letters (A,T,S) to designate the order of the steps. According to this convention, algorithm (490)-\n(492) is referred to as the ATS di\ufb00usion strategy since adaptation is followed by temporal processing, which\nis followed by spatial processing. In total, we can obtain six di\ufb00erent combinations of di\ufb00usion algorithms\nby changing the order by which the temporal and spatial combination steps are performed in relation to the\nadaptation step. The resulting variations are summarized in Table 8. When we use only the most recent\nweight vector in the temporal \ufb01ltering step (i.e., set \u03c8k,i = \u03c6k,i), which corresponds to the case P = 1, the\nalgorithms of Table 8 reduce to the ATC and CTA di\ufb00usion algorithms (153) and (154). Speci\ufb01cally, the\nvariants TSA, STA, and SAT (where spatial processing S precedes adaptation A) reduce to CTA, while the\nvariants TAS, ATS, and AST (where adaptation A precedes spatial processing S) reduce to ATC.\nk\n{d1(i), u1,i}\n{dk(i), uk,i}\n{d\u2113(i), u\u2113,i}\nadaptation\n{c\u2113k, q\u2113}\nwk,i\u22121\n\u03c6k,i\nAdaptation (A)\naggregation\n{a\u2113,k}\n{\u03c8\u2113,i}\n{d\u2113(i), u\u2113,i}\nwk,i\nSpatial Processing (S)\n\u03c81,i\n\u03c8k,i\n\u03c8\u2113,i\nk\nfiltering\n{\u03c6k,i\u2212j}\n{fkj}\n\u03c8k,i\nTemporal Processing (T)\n\u03c6k,i\n\u03c6k,i\u22121\n\u03c6k,i\u2212P +1\nFigure 15: Illustration of the three steps involved in an ATS di\ufb00usion strategy: adaptation, followed by temporal\nprocessing or smoothing, followed by spatial processing.\nThe mean-square performance analysis of the smoothed di\ufb00usion strategies can be pursued by extending\nthe arguments of Sec. 6. This step is carried out in [73, 74] for doubly stochastic combination matrices A\n82\nwhen the \ufb01ltering coe\ufb03cients {fkj} do not change with k. For instance, it is shown in [74] that whether\ntemporal processing is performed before or after adaptation, the strategy that performs adaptation before\nspatial cooperation is always better. Speci\ufb01cally, the six di\ufb00usion variants can be divided into two groups\nwith the respective network MSDs satisfying the following relations:\nGroup #1 :\nMSDnetwork\nTSA\n= MSDnetwork\nSTA\n\u2265MSDnetwork\nTAS\n(498)\nGroup #2 :\nMSDnetwork\nSAT\n\u2265MSDnetwork\nATS\n= MSDnetwork\nAST\n(499)\nNote that within groups 1 and 2, the order of the A and T operations is the same: in group 1, T precedes A\nand in group 2, A precedes T. Moreover, within each group, the order of the A and S operations determines\nperformance; the strategy that performs A before S is better.\nNote further that when P = 1, so that\ntemporal processing is not performed, then TAS reduces to ATC and TSA reduces to CTA. This conclusion\nis consistent with our earlier result (343) that ATC outperforms CTA.\nTable 8: Six di\ufb00usion strategies with temporal smoothing steps.\nTSA di\ufb00usion:\nTAS di\ufb00usion:\n\u03c6k,i\u22121 =\nP \u22121\nX\nj=0\nfkj wk,i\u2212j\u22121\n\u03c6k,i\u22121 =\nP \u22121\nX\nj=0\nfkj wk,i\u2212j\u22121\n\u03c8k,i\u22121 =\nX\n\u2113\u2208Nk\na\u2113k \u03c6\u2113,i\u22121\n\u03c8k,i = \u03c6k,i\u22121 + \u00b5k\nX\n\u2113\u2208Nk\nq\u2113c\u2113k u\u2217\n\u2113,i [d\u2113(i) \u2212u\u2113,i\u03c6k,i\u22121]\nwk,i = \u03c8k,i\u22121 + \u00b5k\nX\n\u2113\u2208Nk\nq\u2113c\u2113k u\u2217\n\u2113,i [d\u2113(i) \u2212u\u2113,i \u03c8k,i\u22121]\nwk,i =\nX\n\u2113\u2208Nk\na\u2113,k\u03c8\u2113,i\nSTA di\ufb00usion:\nATS di\ufb00usion:\n\u03c6k,i\u22121 =\nX\n\u2113\u2208Nk\na\u2113k w\u2113,i\u22121\n\u03c6k,i = wk,i\u22121 + \u00b5k\nX\n\u2113\u2208Nk\nq\u2113c\u2113k u\u2217\n\u2113,i [d\u2113(i) \u2212u\u2113,iwk,i\u22121]\n\u03c8k,i\u22121 =\nP \u22121\nX\nj=0\nfkj \u03c6k,i\u2212j\u22121\n\u03c8k,i =\nP \u22121\nX\nj=0\nfkj\u03c6k,i\u2212j\nwk,i = \u03c8k,i\u22121 + \u00b5k\nX\n\u2113\u2208Nk\nq\u2113c\u2113k u\u2217\n\u2113,i [d\u2113(i) \u2212u\u2113,i\u03c8k,i\u22121]\nwk,i =\nX\n\u2113\u2208Nk\na\u2113k\u03c8\u2113,i\nSAT di\ufb00usion:\nAST di\ufb00usion:\n\u03c6k,i\u22121 =\nX\n\u2113\u2208Nk\na\u2113kw\u2113,i\u22121\n\u03c6k,i = wk,i\u22121 + \u00b5k\nX\n\u2113\u2208Nk\nq\u2113c\u2113k u\u2217\n\u2113,i [d\u2113(i) \u2212u\u2113,iwk,i\u22121]\n\u03c8k,i = \u03c6k,i\u22121 + \u00b5k\nX\n\u2113\u2208Nk\nq\u2113c\u2113k u\u2217\n\u2113,i [d\u2113(i) \u2212u\u2113,i\u03c6k,i\u22121]\n\u03c8k,i =\nX\n\u2113\u2208Nk\na\u2113k\u03c6\u2113,i\nwk,i =\nP \u22121\nX\nj=0\nfkj\u03c8k,i\u2212j\nwk,i =\nP \u22121\nX\nj=0\nfkj\u03c8k,i\u2212j\nIn related work, reference [75] started from the CTA algorithm (159) without information exchange and\nadded a useful projection step to it between the combination step and the adaptation step; i.e., the work\nconsidered an algorithm with an STA structure (with spatial combination occurring \ufb01rst, followed by a\nprojection step, and then adaptation). The projection step uses the current weight estimate, \u03c6k,i, at node\nk and projects it onto hyperslabs de\ufb01ned by the current and past raw data. Speci\ufb01cally, the algorithm\n83\nfrom [75] has the following form:\n\u03c6k,i\u22121\n=\nX\n\u2113\u2208Nk\na\u2113k w\u2113,i\u22121\n(500)\n\u03c8k,i\u22121\n=\nP\n\u2032\nk,i[\u03c6k,i\u22121]\n(501)\nwk,i\n=\n\u03c8k,i\u22121 \u2212\u00b5k\n\uf8f1\n\uf8f2\n\uf8f3\u03c8k,i\u22121 \u2212\nP \u22121\nX\nj=0\nfkj \u00b7 Pk,i\u2212j[\u03c6k,i\u22121]\n\uf8fc\n\uf8fd\n\uf8fe\n(502)\nwhere the notation \u03c8 = Pk,i[\u03c6] refers to the act of projecting the vector \u03c6 onto the hyperslab Pk,i that\nconsists of all M \u00d7 1 vectors z satisfying (similarly for the projection P\n\u2032\nk,i):\nPk,i\n\u2206=\n{ z such that |dk(i) \u2212uk,iz| \u2264\u01ebk }\n(503)\nP\n\u2032\nk,i\n\u2206=\n{ z such that |dk(i) \u2212uk,iz| \u2264\u01eb\u2032\nk }\n(504)\nwhere {\u01ebk, \u01eb\u2032\nk} are positive (tolerance) parameters chosen by the designer to satisfy \u01eb\u2032\nk > \u01ebk. For generic\nvalues {d, u, \u01eb}, where d is a scalar and u is a row vector, the projection operator is described analytically\nby the following expression [76]:\nP[\u03c6] = \u03c6 +\n\uf8f1\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f2\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f3\nu\u2217\n\u2225u\u22252 [d \u2212\u01eb \u2212u\u03c6] ,\nif d \u2212\u01eb > u\u03c6\n0,\nif |d \u2212u\u03c6| \u2264\u01eb\nu\u2217\n\u2225u\u22252 [d + \u01eb \u2212u\u03c6] ,\nif d + \u01eb < u\u03c6\n(505)\nThe projections that appear in (501)\u2013(502) can be regarded as another example of a temporal processing\nstep.\nObserve from the middle plot in Fig. 15 that the temporal step that we are considering in the\nalgorithms listed in Table 8 is based on each node k using its current and past weight estimates, such as\n{\u03c6k,i, \u03c6k,i\u22121, . . . , \u03c6k,i\u2212P +1}, rather than only \u03c6k,i and current and past raw data {dk(i), dk(i \u22121), . . ., dk(i \u2212\nP + 1), uk,i, uk,i\u22121, . . . , uk,i\u2212P +1}. For this reason, the temporal processing steps in Table 8 tend to exploit\ninformation from across the network more broadly and the resulting mean-square error performance is\ngenerally improved relative to (500)\u2013(502).\n10.2\nDi\ufb00usion Recursive Least-Squares\nDi\ufb00usion strategies can also be applied to recursive least-squares problems to enable distributed solutions of\nleast-squares designs [28,29]; see also [72]. Thus, consider again a set of N nodes that are spatially distributed\nover some domain. The objective of the network is to collectively estimate some unknown column vector of\nlength M, denoted by wo, using a least-squares criterion. At every time instant i, each node k collects a\nscalar measurement, dk(i), which is assumed to be related to the unknown vector wo via the linear model:\ndk(i) = uk,iwo + vk(i)\n(506)\nIn the above relation, the vector uk,i denotes a row regression vector of length M, and vk(i) denotes measure-\nment noise. A snapshot of the data in the network at time i can be captured by collecting the measurements\nand noise samples, {dk(i), vk(i)}, from across all nodes into column vectors yi and vi of sizes N \u00d7 1 each,\nand the regressors {uk,i} into a matrix Hi of size N \u00d7 M:\nyi =\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8f0\nd1(i)\nd2(i)\n...\ndN(i)\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fb(N \u00d7 1),\nvi =\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8f0\nv1(i)\nv2(i)\n...\nvN(i)\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fb(N \u00d7 1),\nHi =\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8f0\nu1,i\nu2,i\n...\nuN,i\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fb(N \u00d7 M)\n(507)\n84\nLikewise, the history of the data across the network up to time i can be collected into vector quantities as\nfollows:\nYi =\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8f0\nyi\nyi\u22121\n...\ny0\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fb,\nVi =\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8f0\nvi\nvi\u22121\n...\nv0\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fb,\nHi =\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8f0\nHi\nHi\u22121\n...\nH0\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fb\n(508)\nThen, one way to estimate wo is by formulating a global least-squares optimization problem of the form:\nmin\nw\n\u2225w\u22252\n\u03a0i + \u2225Yi \u2212Hiw\u22252\nWi\n(509)\nwhere \u03a0i > 0 represents a Hermitian regularization matrix and Wi \u22650 represents a Hermitian weighting\nmatrix. Common choices for \u03a0i and Wi are\nWi\n=\ndiag{IN, \u03bbIN, . . . , \u03bbiIN}\n(510)\n\u03a0i\n=\n\u03bbi+1\u03b4\u22121\n(511)\nwhere \u03b4 > 0 is usually a large number and 0 \u226a\u03bb \u22641 is a forgetting factor whose value is generally very\nclose to one. In this case, the global cost function (509) can be written in the equivalent form:\nmin\nw\n\u03bbi+1\u2225w\u22252 +\ni\nX\nj=0\n\u03bbi\u2212j\n N\nX\nk=1\n|dk(j) \u2212uk,jw|2\n!\n(512)\nwhich is an exponentially weighted least-squares problem. We see that, for every time instant j, the squared\nerrors, |dk(j)\u2212uk,jw|2, are summed across the network and scaled by the exponential weighting factor \u03bbi\u2212j.\nThe index i denotes current time and the index j denotes a time instant in the past. In this way, data\noccurring in the remote past are scaled more heavily than data occurring closer to present time. The global\nsolution of (509) is given by [5]:\nwi = [\u03a0i + HiWiHi]\u22121 H\u2217\ni WiYi\n(513)\nand the notation wi, with a subscript i, is meant to indicate that the estimate wi is based on all data\ncollected from across the network up to time i. Therefore, the wi that is computed via (513) amounts to a\nglobal construction.\nIn [28, 29] a di\ufb00usion strategy was developed that allows nodes to approach the global solution wi by\nrelying solely on local interactions. Let wk,i denote a local estimate for wo that is computed by node k at\ntime i. The di\ufb00usion recursive-least-squares (RLS) algorithm takes the following form. For every node k,\nwe start with the initial conditions wk,\u22121 = 0 and Pk,\u22121 = \u03b4IM, where Pk,\u22121 is an M \u00d7 M matrix. Then,\nfor every time instant i, each node k performs an incremental step followed by a di\ufb00usion step as follows:\n85\nDi\ufb00usion RLS.\nStep 1 (incremental update)\n\u03c8k,i \u2190wk,i\u22121\nPk,i \u2190\u03bb\u22121Pk,i\u22121\nfor every neighboring node \u2113\u2208Nk, update :\n\u03c8k,i \u2190\u03c8k,i +\nc\u2113kPk,iu\u2217\n\u2113,i\n1 + c\u2113ku\u2113,iPk,iu\u2217\n\u2113,i\n[d\u2113,i \u2212u\u2113,i\u03c8k,i]\nPk,i \u2190Pk,i \u2212\nc\u2113kPk,iu\u2217\n\u2113,iu\u2113,iPk,i\n1 + c\u2113ku\u2113,iPk,iu\u2217\n\u2113,i\nend\nStep 2 (di\ufb00usion update)\nwk,i =\nX\n\u2113\u2208Nk\na\u2113k\u03c8\u2113,i\n(514)\nwhere the symbol \u2190denotes a sequential assignment, and where the scalars {a\u2113k, c\u2113k} are nonnegative\ncoe\ufb03cients satisfying:\nfor k = 1, 2, . . . , N :\nc\u2113k \u22650,\nN\nX\nk=1\nc\u2113k = 1,\nc\u2113k = 0 if \u2113/\u2208Nk\n(515)\na\u2113k \u22650,\nN\nX\n\u2113=1\na\u2113k = 1,\na\u2113k = 0 if \u2113/\u2208Nk\n(516)\nThe above algorithm requires that at every instant i, nodes communicate to their neighbors their mea-\nsurements {d\u2113(i), u\u2113,i} for the incremental update, and the intermediate estimates {\u03c8\u2113,i} for the di\ufb00usion\nupdate. During the incremental update, node k cycles through its neighbors and incorporates their data\ncontributions represented by {d\u2113(i), u\u2113,i} into {\u03c8k,i, Pk,i}. Every other node in the network is performing\nsimilar steps. At the end of the incremental step, neighboring nodes share their intermediate estimates\n{\u03c8\u2113,i} to undergo di\ufb00usion. Thus, at the end of both steps, each node k would have updated the quantities\n{wk,i\u22121, Pk,i\u22121} to {wk,i, Pk,i}. The quantities Pk,i are matrices of size M \u00d7 M each. Observe that the\ndi\ufb00usion RLS implementation (514) does not require the nodes to share their matrices {P\u2113,i}, which would\namount to a substantial burden in terms of communications resources since each of these matrices has M 2\nentries. Only the quantities {d\u2113(i), u\u2113,i, \u03c8\u2113,i} are shared. The mean-square performance and convergence of\nthe di\ufb00usion RLS strategy are studied in some detail in [29].\nThe incremental step of the di\ufb00usion RLS strategy (514) corresponds to performing a number of |Nk|\nsuccessive least-squares updates starting from the initial conditions {wk,i\u22121, Pk,i\u22121} and ending with the\nvalues {\u03c8k,i, Pk,i} that move on to the di\ufb00usion update step.\nIt can be veri\ufb01ed from the properties of\nrecursive least-squares solutions [4, 5] that these variables satisfy the following equations at the end of the\nincremental stage (step 1):\nP \u22121\nk,i\n=\n\u03bbP \u22121\nk,i\u22121 +\nX\n\u2113\u2208Nk\nc\u2113ku\u2217\n\u2113,iu\u2113,i\n(517)\nP \u22121\nk,i \u03c8k,i\n=\n\u03bbP \u22121\nk,i\u22121wk,i\u22121 +\nX\n\u2113\u2208Nk\nc\u2113ku\u2217\n\u2113,id\u2113(i)\n(518)\n86\nIntroduce the auxiliary M \u00d7 1 variable:\nqk,i\n\u2206= P \u22121\nk,i \u03c8k,i\n(519)\nThen, the above expressions lead to the following alternative form of the di\ufb00usion RLS strategy (514).\nAlternative form of di\ufb00usion RLS.\nwk,i\u22121 =\nX\n\u2113\u2208Nk\na\u2113k\u03c8\u2113,i\u22121\nP \u22121\nk,i\n= \u03bbP \u22121\nk,i\u22121 +\nX\n\u2113\u2208Nk\nc\u2113ku\u2217\n\u2113,iu\u2113,i\nqk,i = \u03bbP \u22121\nk,i\u22121wk,i\u22121 +\nX\n\u2113\u2208Nk\nc\u2113ku\u2217\n\u2113,id\u2113(i)\n\u03c8k,i = Pk,iqk,i\n(520)\nUnder some approximations, and for the special choices A = C and \u03bb = 1, the di\ufb00usion RLS strategy\n(520) can be reduced to a form given in [79] and which is described by the following equations:\nP \u22121\nk,i\n=\nX\n\u2113\u2208Nk\nc\u2113k\nh\nP \u22121\n\u2113,i\u22121 + u\u2217\n\u2113,iu\u2113,i\ni\n(521)\nqk,i\n=\nX\n\u2113\u2208Nk\nc\u2113k\n\u0002\nq\u2113,i\u22121 + u\u2217\n\u2113,id\u2113(i)\n\u0003\n(522)\n\u03c8k,i\n=\nPk,iqk,i\n(523)\nAlgorithm (521)\u2013(523) is motivated in [79] by using consensus-type arguments. Observe that the algorithm\nrequires the nodes to share the variables {d\u2113(i), u\u2113,i, q\u2113,i\u22121, P\u2113,i\u22121}, which corresponds to more communica-\ntions overburden than required by di\ufb00usion RLS; the latter only requires that nodes share {d\u2113(i), u\u2113,i, \u03c8\u2113,i\u22121}.\nIn order to illustrate how a special case of di\ufb00usion RLS (520) can be related to this scheme, let us set\nA = C\nand\n\u03bb = 1\n(524)\nThen, equations (520) give:\nSpecial form of di\ufb00usion RLS when A = C and \u03bb = 1.\nwk,i\u22121 =\nX\n\u2113\u2208Nk\nc\u2113k\u03c8\u2113,i\u22121\nP \u22121\nk,i\n= P \u22121\nk,i\u22121 +\nX\n\u2113\u2208Nk\nc\u2113ku\u2217\n\u2113,iu\u2113,i\nqk,i = P \u22121\nk,i\u22121wk,i\u22121 +\nX\n\u2113\u2208Nk\nc\u2113ku\u2217\n\u2113,id\u2113(i)\n\u03c8k,i = Pk,iqk,i\n(525)\n87\nComparing these equations with (521)\u2013(523), we \ufb01nd that algorithm (521)\u2013(523) of [79] would relate to the\ndi\ufb00usion RLS algorithm (520) when the following approximations are justi\ufb01ed:\nX\n\u2113\u2208Nk\nc\u2113kP \u22121\n\u2113,i\u22121\n\u2248\nP \u22121\nk,i\u22121\n(526)\nX\n\u2113\u2208Nk\nc\u2113kq\u2113,i\u22121\n=\nX\n\u2113\u2208Nk\nc\u2113kP \u22121\n\u2113,i\u22121\u03c8\u2113,i\u22121\n\u2248\nX\n\u2113\u2208Nk\nc\u2113kP \u22121\nk,i\u22121\u03c8\u2113,i\u22121\n=\nP \u22121\nk,i\u22121\nX\n\u2113\u2208Nk\nc\u2113k\u03c8\u2113,i\u22121\n(527)\n=\nP \u22121\nk,i\u22121wk,i\u22121\n(528)\nIt was indicated in [29] that the di\ufb00usion RLS implementation (514) or (520) leads to enhanced performance\nin comparison to the consensus-based update (521)\u2013(523).\n10.3\nDi\ufb00usion Kalman Filtering\nDi\ufb00usion strategies can also be applied to the solution of distributed state-space \ufb01ltering and smoothing\nproblems [30,31,33]. Here, we describe brie\ufb02y the di\ufb00usion version of the Kalman \ufb01lter; other variants and\nsmoothing \ufb01lters can be found in [33]. We assume that some system of interest is evolving according to linear\nstate-space dynamics, and that every node in the network collects measurements that are linearly related to\nthe unobserved state vector. The objective is for every node to track the state of the system over time based\nsolely on local observations and on neighborhood interactions.\nThus, consider a network consisting of N nodes observing the state vector, xi, of size n \u00d7 1 of a linear\nstate-space model. At every time i, every node k collects a measurement vector yk,i of size p \u00d7 1, which is\nrelated to the state vector as follows:\nxi+1\n=\nFixi + Gini\n(529)\nyk,i\n=\nHk,ixi + vk,i,\nk = 1, 2, . . . , N\n(530)\nThe signals ni and vk,i denote state and measurement noises of sizes n \u00d7 1 and p \u00d7 1, respectively, and they\nare assumed to be zero-mean, uncorrelated and white, with covariance matrices denoted by\nE\n\u0014 ni\nvk,i\n\u0015 \u0014 nj\nvk,j\n\u0015\u2217\n\u2206=\n\u0014 Qi\n0\n0\nRk,i\n\u0015\n\u03b4ij\n(531)\nThe initial state vector, xo, is assumed to be zero-mean with covariance matrix\nExox\u2217\no = \u03a0o > 0\n(532)\nand is uncorrelated with ni and vk,i, for all i and k. We further assume that Rk,i > 0. The parameter\nmatrices {Fi, Gi, Hk,i, Qi, Rk,i, \u03a0o} are assumed to be known by node k.\nLet bxk,i|j denote a local estimator for xi that is computed by node k at time i based solely on local\nobservations and on neighborhood data up to time j. The following di\ufb00usion strategy was proposed in\n[30,31,33] to evaluate approximate predicted and \ufb01ltered versions of these local estimators in a distributed\nmanner for data satisfying model (529)\u2013(532). For every node k, we start with bxk,0|\u22121 = 0 and Pk,0|\u22121 = \u03a0o,\nwhere Pk,0|\u22121 is an M \u00d7 M matrix. At every time instant i, every node k performs an incremental step\n88\nfollowed by a di\ufb00usion step:\nTime and measurement-form of the di\ufb00usion Kalman \ufb01lter.\nStep 1 (incremental update)\n\u03c8k,i \u2190bxk,i|i\u22121\nPk,i \u2190Pk,i|i\u22121\nfor every neighboring node \u2113\u2208Nk, update :\nRe \u2190R\u2113,i + H\u2113,iPk,iH\u2217\n\u2113,i\n\u03c8k,i \u2190\u03c8k,i + Pk,iH\u2217\n\u2113,iR\u22121\ne\n\u0002\ny\u2113,i \u2212H\u2113,i\u03c8k,i\n\u0003\nPk,i \u2190Pk,i \u2212Pk,iH\u2217\n\u2113,iR\u22121\ne H\u2113,iPk,i\nend\nStep 2 (di\ufb00usion update)\nbxk,i|i =\nX\n\u2113\u2208Nk\na\u2113k\u03c8\u2113,i\nPk,i|i = Pk,i\nbxk,i+1|i = Fibxk,i|i\nPk,i+1|i = FiPk,i|iF \u2217\ni + GiQiG\u2217\ni .\n(533)\nwhere the symbol \u2190denotes a sequential assignment, and where the scalars {a\u2113k} are nonnegative coe\ufb03cients\nsatisfying:\nfor k = 1, 2, . . . , N :\na\u2113k \u22650,\nN\nX\n\u2113=1\na\u2113k = 1,\na\u2113k = 0 if \u2113/\u2208Nk\n(534)\nThe above algorithm requires that at every instant i, nodes communicate to their neighbors their mea-\nsurement matrices H\u2113,i, the noise covariance matrices R\u2113,i, and the measurements y\u2113,i for the incremental\nupdate, and the intermediate estimators \u03c8\u2113,i for the di\ufb00usion update. During the incremental update, node\nk cycles through its neighbors and incorporates their data contributions represented by {y\u2113,i, H\u2113,i, R\u2113,i} into\n{\u03c8k,i, Pk,i}. Every other node in the network is performing similar steps. At the end of the incremental step,\nneighboring nodes share their updated intermediate estimators {\u03c8\u2113,i} to undergo di\ufb00usion. Thus, at the\nend of both steps, each node k would have updated the quantities {bxk,i|i\u22121, Pk,i|i\u22121} to {bxk,i+1|i, Pk,i+1|i}.\nThe quantities Pk,i|i\u22121 are n \u00d7 n matrices. It is important to note that even though the notation Pk,i|i\nand Pk,i|i\u22121 has been retained for these variables, as in the standard Kalman \ufb01ltering notation [5,77], these\nmatrices do not represent any longer the covariances of the state estimation errors, \u02dcxk,i|i\u22121 = xi \u2212bxk,i|i\u22121,\nbut can be related to them [33].\nAn alternative representation of the di\ufb00usion Kalman \ufb01lter may be obtained in information form by\nfurther assuming that Pk,i|i\u22121 > 0 for all k and i; a su\ufb03cient condition for this fact to hold is to requires\nthe matrices {Fi} to be invertible [77]. Thus, consider again data satisfying model (529)\u2013(532). For every\nnode k, we start with bxk,0|\u22121 = 0 and P \u22121\nk,0|\u22121 = \u03a0\u22121\no . At every time instant i, every node k performs an\nincremental step followed by a di\ufb00usion step:\n89\nInformation form of the di\ufb00usion Kalman \ufb01lter.\nStep 1 (incremental update)\nSk,i =\nX\n\u2113\u2208Nk\nH\u2217\n\u2113,iR\u22121\n\u2113,i H\u2113,i\nqk,i =\nX\n\u2113\u2208Nk\nH\u2217\n\u2113,iR\u22121\n\u2113,i y\u2113,i\nP \u22121\nk,i|i = P \u22121\nk,i|i\u22121 + Sk,i\n\u03c8k,i = bxk,i|i\u22121 + Pk,i|i\n\u0002\nqk,i \u2212Sk,ibxk,i|i\u22121\n\u0003\nStep 2: (di\ufb00usion update)\nbxk,i|i =\nX\n\u2113\u2208Nk\na\u2113k\u03c8\u2113,i\nbxk,i+1|i = Fibxk,i|i\nPk,i+1|i = FiPk,i|iF \u2217\ni + GiQiG\u2217\ni\n(535)\nThe incremental update in (535) is similar to the update used in the distributed Kalman \ufb01lter derived\nin [48]. An important di\ufb00erence in the algorithms is in the di\ufb00usion step. Reference [48] starts from a\ncontinuous-time consensus implementation and discretizes it to arrive at the following update relation:\nbxk,i|i\n=\n\u03c8k,i + \u01eb\nX\n\u2113\u2208Nk\n(\u03c8\u2113,i \u2212\u03c8k,i)\n(536)\nwhich, in order to facilitate comparison with (535), can be equivalently rewritten as:\nbxk,i|i\n=\n(1 + \u01eb \u2212nk\u01eb) \u00b7 \u03c8k,i +\nX\n\u2113\u2208Nk\\{k}\n\u01eb \u00b7 \u03c8\u2113,i\n(537)\nwhere nk denotes the degree of node k (i.e., the size of its neighborhood, Nk). In comparison, the di\ufb00usion\nstep in (535) can be written as:\nbxk,i|i\n=\nakk \u00b7 \u03c8k,i +\nX\n\u2113\u2208Nk\\{k}\na\u2113k \u00b7 \u03c8\u2113,i\n(538)\nObserve that the weights used in (537) are (1 + \u01eb \u2212nk\u01eb) for the node\u2019s estimator, \u03c8k,i, and \u01eb for all other\nestimators, {\u03c8\u2113,i}, arriving from the neighbors of node k. In contrast, the di\ufb00usion step (538) employs a\nconvex combination of the estimators {\u03c8\u2113,i} with generally di\ufb00erent weights {a\u2113k} for di\ufb00erent neighbors;\nthis choice is motivated by the desire to employ combination coe\ufb03cients that enhance the fusion of infor-\nmation at node k, as suggested by the discussion in App. D of [33]. It was veri\ufb01ed in [33] that the di\ufb00usion\nimplementation (538) leads to enhanced performance in comparison to the consensus-based update (537).\nMoreover, the weights {a\u2113k} in (538) can also be adjusted over time in order to further enhance performance,\nas discussed in [78]. The mean-square performance and convergence of the di\ufb00usion Kalman \ufb01ltering imple-\nmentations are studied in some detail in [33], along with other di\ufb00usion strategies for smoothing problems\nincluding \ufb01xed-point and \ufb01xed-lag smoothing.\n90\n10.4\nDi\ufb00usion Distributed Optimization\nThe ATC and CTA steepest-descent di\ufb00usion strategies (134) and (142) derived earlier in Sec. 3 provide\ndistributed mechanisms for the solution of global optimization problems of the form:\nmin\nw\nN\nX\nk=1\nJk(w)\n(539)\nwhere the individual costs, Jk(w), were assumed to be quadratic in w, namely,\nJk(w) = \u03c32\nd,k \u2212w\u2217rdu,k \u2212r\u2217\ndu,k w + w\u2217Ru,k w\n(540)\nfor given parameters {\u03c32\nd,k, rdu,k, Ru,k}. Nevertheless, we remarked in that section that similar di\ufb00usion\nstrategies can be applied to more general cases involving individual cost functions, Jk(w), that are not\nnecessarily quadratic in w [1\u20133]. We restate below, for ease of reference, the general ATC and CTA di\ufb00usion\nstrategies (139) and (146) that can be used for the distributed solution of global optimization problems of\nthe form (539) for more general convex functions Jk(w):\n(ATC strategy)\n\u03c8k,i = wk,i\u22121 \u2212\u00b5k\nX\n\u2113\u2208Nk\nc\u2113k [\u2207wJ\u2113(wk,i\u22121)]\u2217\nwk,i =\nX\n\u2113\u2208Nk\na\u2113k \u03c8\u2113,i\n(541)\nand\n(CTA strategy)\n\u03c8k,i\u22121 =\nX\n\u2113\u2208Nk\na\u2113k w\u2113,i\u22121\nwk,i = \u03c8k,i\u22121 \u2212\u00b5k\nX\n\u2113\u2208Nk\nc\u2113k [\u2207wJ\u2113(\u03c8k,i\u22121)]\u2217\n(542)\nfor positive step-sizes {\u00b5k} and for nonnegative coe\ufb03cients {c\u2113k, a\u2113k} that satisfy:\nfor k = 1, 2, . . . , N :\nc\u2113k \u22650,\nN\nX\nk=1\nc\u2113k = 1,\nc\u2113k = 0 if \u2113/\u2208Nk\na\u2113k \u22650,\nN\nX\n\u2113=1\na\u2113k = 1,\na\u2113k = 0 if \u2113/\u2208Nk\n(543)\nThat is, the matrix A = [a\u2113k] is left-stochastic while the matrix C = [c\u2113k] is right-stochastic:\nC1 = 1,\nAT 1 = 1\n(544)\nWe can again regard the above ATC and CTA strategies as special cases of the following general di\ufb00usion\nscheme:\n\u03c6k,i\u22121\n=\nX\n\u2113\u2208Nk\na1,\u2113k w\u2113,i\u22121\n(545)\n\u03c8k,i\n=\n\u03c6k,i\u22121 \u2212\u00b5k\nX\n\u2113\u2208Nk\nc\u2113k [\u2207wJ\u2113(\u03c6k,i\u22121)]\u2217\n(546)\nwk,i\n=\nX\n\u2113\u2208Nk\na2,\u2113k \u03c8\u2113,i\n(547)\n91\nwhere the coe\ufb03cients {a1,\u2113k, a2,\u2113k, c\u2113k} are nonnegative coe\ufb03cients corresponding to the (l, k)-th entries of\ncombination matrices {A1, A2, C} that satisfy:\nAT\n1 1 = 1,\nAT\n2 1 = 1,\nC1 = 1\n(548)\nThe convergence behavior of these di\ufb00usion strategies can be examined under both conditions of noiseless\nupdates (when the gradient vectors are available) and noisy updates (when the gradient vectors are subject\nto gradient noise). The following properties can be proven for the di\ufb00usion strategies (545)\u2013(547) [1\u20133]. The\nstatements that follow assume, for convenience of presentation, that all data are real-valued; the conditions\nwould need to be adjusted for complex-valued data.\nNoiseless Updates\nLet\nJglob(w) =\nN\nX\nk=1\nJk(w)\n(549)\ndenote the global cost function that we wish to minimize. Assume Jglob(w) is strictly convex so that its\nminimizer wo is unique. Assume further that each individual cost function Jk(w) is convex and has a mini-\nmizer at the same wo. This case is common in practice; situations abound where nodes in a network need to\nwork cooperatively to attain a common objective (such as tracking a target, locating the source of chemical\nleak, estimating a physical model, or identifying a statistical distribution). The case where the {Jk(w)} have\ndi\ufb00erent individual minimizers is studied in [1,3], where it is shown that the same di\ufb00usion strategies of this\nsection are still applicable and nodes would converge instead to a Pareto-optimal solution.\nTheorem 10.1. (Convergence to Optimal Solution:\nNoise-Free Case) Consider the problem of\nminimizing the strictly convex global cost (549), with the individual cost functions {Jk(w)} assumed to be\nconvex with each having a minimizer at the same wo. Assume that all data are real-valued and suppose the\nHessian matrices of the individual costs are bounded from below and from above as follows:\n\u03bb\u2113,minIM \u2264\u22072\nwJ\u2113(w) \u2264\u03bb\u2113,maxIM,\n\u2113= 1, 2, . . . , N\n(550)\nfor some positive constants {\u03bb\u2113,min, \u03bb\u2113,max}. Let\n\u03c3k,min\n\u2206=\nX\n\u2113\u2208Nk\nc\u2113k\u03bb\u2113,min,\n\u03c3k,max\n\u2206=\nX\n\u2113\u2208Nk\nc\u2113k\u03bb\u2113,max\n(551)\nAssume further that \u03c3k,min > 0 and that the positive step-sizes are chosen such that:\n\u00b5k \u2264\n2\n\u03c3k,max\n,\nk = 1, . . . , N\n(552)\nThen, it holds that wk,i \u2192wo as i \u2192\u221e. That is, the weight estimates generated by (545)\u2013(547) at all nodes\nwill tend towards the desired global minimizer.\n\u25a1\nWe note that in works on distributed sub-gradient methods (e.g., [40, 80]), the norms of the sub-gradients\nare usually required to be uniformly bounded. Such a requirement is restrictive in the unconstrained op-\ntimization of di\ufb00erentiable functions. Condition (550) is more relaxed since it allows the gradient vector\n\u2207wJ\u2113(w) to have unbounded norm. This extension is important because requiring bounded gradient norms,\nas opposed to bounded Hessian matrices, would exclude the possibility of using quadratic costs for the J\u2113(w)\n(since the gradient vectors would then be unbounded). And, as we saw in the body of the chapter, quadratic\ncosts play a critical role in adaptation and learning over networks.\n92\nUpdates with Gradient Noise\nIt is often the case that we do not have access to the exact gradient vectors to use in (546), but to noisy\nversions of them, say,\n\\\n\u2207wJ\u2113(\u03c6k,i\u22121)\n\u2206= \u2207wJ\u2113(\u03c6k,i\u22121) + v\u2113(e\u03c6k,i\u22121)\n(553)\nwhere the random vector variable v\u2113(\u00b7) refers to gradient noise; its value is generally dependent on the\nweight-error vector realization,\ne\u03c6k,i\u22121\n\u2206= wo \u2212\u03c6k,i\u22121\n(554)\nat which the gradient vector is being evaluated. In the presence of gradient noise, the weight estimates at the\nvarious nodes become random quantities and we denote them by the boldface notation {wk,i}. We assume\nthat, conditioned on the past history of the weight estimators at all nodes, namely,\nFi\u22121\n\u2206= {wm,j, m = 1, 2, . . ., N, j < i}\n(555)\nthe gradient noise has zero mean and its variance is upper bounded as follows:\nE\nn\nv\u2113(e\u03c6k,i\u22121) | Fi\u22121\no\n=\n0\n(556)\nE\nn\n\u2225v\u2113(e\u03c6k,i\u22121)\u22252 | Fi\u22121\no\n\u2264\n\u03b1\u2225e\u03c6k,i\u22121\u22252 + \u03c32\nv\n(557)\nfor some \u03b1 > 0 and \u03c32\nv \u22650. Condition (557) allows the variance of the gradient noise to be time-variant,\nso long as it does not grow faster than E\u2225e\u03c6k,i\u22121\u22252. This condition on the noise is more general than the\n\u201cuniform-bounded assumption\u201d that appears in [40], which required instead:\nE\nn\n\u2225v\u2113(e\u03c6k,i\u22121)\u22252o\n\u2264\u03c32\nv,\nE\nn\n\u2225v\u2113(e\u03c6k,i\u22121)\u22252 | Fi\u22121\no\n\u2264\u03c32\nv\n(558)\nThese two requirements are special cases of (557) for \u03b1 = 0. Furthermore, condition (557) is similar to\ncondition (4.3) in [81], which requires the noise variance to satisfy:\nE\nn\n\u2225v\u2113(e\u03c6k,i\u22121)\u22252 | Fi\u22121\no\n\u2264\u03b1\n\u0002\n\u2225\u2207wJ\u2113(\u03c6k,i\u22121)\u22252 + 1\n\u0003\n(559)\nThis requirement can be veri\ufb01ed to be a combination of the \u201crelative random noise\u201d and the \u201cabsolute\nrandom noise\u201d conditions de\ufb01ned in [22] \u2014 see [2].\nNow, introduce the column vector:\nzi\n\u2206=\nN\nX\n\u2113=1\ncol {c\u21131v\u2113(wo), c\u21132v\u2113(wo), . . . , c\u2113Nv\u2113(wo)}\n(560)\nand let\nZ\n\u2206= Eziz\u2217\ni\n(561)\nLet further\newi\n\u2206= col { ewi,1, ewi,2, . . . , ewi,N}\n(562)\nwhere\newk,i\n\u2206= wo \u2212wk,i\n(563)\nThen, the following result can be established [2]; it characterizes the network mean-square deviation in\nsteady-state, which is de\ufb01ned as\nMSDnetwork\n\u2206= lim\ni\u2192\u221e\n \n1\nN\nN\nX\nk=1\nE\u2225ewk,i\u22252\n!\n(564)\n93\nTheorem 10.2. (Mean-Square Stability: Noisy Case) Consider the problem of minimizing the strictly\nconvex global cost (549), with the individual cost functions {Jk(w)} assumed to be convex with each having a\nminimizer at the same wo. Assume all data are real-valued and suppose the Hessian matrices of the individual\ncosts are bounded from below and from above as stated in (550). Assume further that the di\ufb00usion strategy\n(545)\u2013(547) employs noisy gradient vectors, where the noise terms are zero mean and satisfy conditions\n(557) and (561). We select the positive step-sizes to be su\ufb03ciently small and to satisfy:\n\u00b5k < min\n(\n2\u03c3k,max\n\u03c32\nk,max + \u03b1\u2225C\u22252\n1\n,\n2\u03c3k,min\n\u03c32\nk,min + \u03b1\u2225C\u22252\n1\n)\n(565)\nfor k = 1, 2, . . ., N. Then, the di\ufb00usion strategy (545)\u2013(547) is mean-square stable and the mean-square-\ndeviation of the network is given by:\nMSDnetwork \u2248\n1\nN\n\u0002\nvec\n\u0000AT\n2 MZT MA2\n\u0001\u0003T \u00b7 (I \u2212F)\u22121 \u00b7 vec(INM)\n(566)\nwhere\nA2\n=\nA2 \u2297IM\n(567)\nM\n=\ndiag{\u00b51IM, \u00b52IM, . . . , \u00b5NIM}\n(568)\nF\n\u2248\nBT \u2297B\u2217\n(569)\nB\n=\nAT\n2 (I \u2212MR) AT\n1\n(570)\nR\n=\nN\nX\n\u2113=1\ndiag\n\b\nc\u21131\u22072\nwJ\u2113(wo), c\u21132\u22072\nwJ\u2113(wo), . . . , c\u2113N\u22072\nwJ\u2113(wo)\n\t\n(571)\n\u25a1\nAcknowledgements\nThe development of the theory and applications of di\ufb00usion adaptation over networks has bene\ufb01ted greatly\nfrom the insights and contributions of several UCLA PhD students, and several visiting graduate students\nto the UCLA Adaptive Systems Laboratory (http://www.ee.ucla.edu/asl). The assistance and contributions\nof all students are hereby gratefully acknowledged, including Cassio G. Lopes, Federico S. Cattivelli, Sheng-\nYuan Tu, Jianshu Chen, Xiaochuan Zhao, Zaid Tow\ufb01c, Chung-Kai Yu, Noriyuki Takahashi, Jae-Woo Lee,\nAlexander Bertrand, and Paolo Di Lorenzo. The author is also particularly thankful to S.-Y. Tu, J. Chen,\nX. Zhao, Z. Tow\ufb01c, and C.-K. Yu for their assistance in reviewing an earlier draft of this article.\n94\nA\nAppendix: Properties of Kronecker Products\nFor ease of reference, we collect in this appendix some useful properties of Kronecker products. All matrices\nare assumed to be of compatible dimensions; all inverses are assumed to exist whenever necessary. Let\nE = [eij]n\ni,j=1 and B = [bij]m\ni,j=1 be n \u00d7 n and m \u00d7 m matrices, respectively. Their Kronecker product is\ndenoted by E \u2297B and is de\ufb01ned as the nm \u00d7 nm matrix whose entries are given by [20]:\nE \u2297B =\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8f0\ne11B\ne12B\n. . .\ne1nB\ne21B\ne22B\n. . .\ne2nB\n...\n...\nen1B\nen2B\n. . .\nennB\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fb\n(572)\nIn other words, each entry of E is replaced by a scaled multiple of B.\nLet {\u03bbi(E), i = 1, . . . , n} and\n{\u03bbj(B), j = 1, . . . , m} denote the eigenvalues of E and B, respectively. Then, the eigenvalues of E \u2297B will\nconsist of all nm product combinations {\u03bbi(E)\u03bbj(B)}. Table 9 lists some well-known properties of Kronecker\nproducts.\nTable 9: Properties of Kronecker products.\n(E + B) \u2297C = (E \u2297C) + (B \u2297C)\n(E \u2297B)(C \u2297D) = (EC \u2297BD)\n(E \u2297B)T = ET \u2297BT\n(E \u2297B)\u2217= E\u2217\u2297B\u2217\n(E \u2297B)\u22121 = E\u22121 \u2297B\u22121\n(E \u2297B)\u2113= E\u2113\u2297B\u2113\n{\u03bb(E \u2297B)} = {\u03bbi(E)\u03bbj(B)}n,m\ni=1,j=1\ndet(E \u2297B) = (det E)m(det B)n\nTr(E \u2297B) = Tr(E)Tr(B)\nTr(EB) =\n\u0002\nvec(BT )\n\u0003T vec(E)\nvec(ECB) = (BT \u2297E)vec(C)\nB\nAppendix: Graph Laplacian and Network Connectivity\nConsider a network consisting of N nodes and L edges connecting the nodes to each other. In the construc-\ntions below, we only need to consider the edges that connect distinct nodes to each other; these edges do\nnot contain any self-loops that may exist in the graph and which connect nodes to themselves directly. In\nother words, when we refer to the L edges of a graph, we are excluding self-loops from this set; but we are\nstill allowing loops of at least length 2 (i.e., loops generated by paths covering at least 2 edges).\nThe neighborhood of any node k is denoted by Nk and it consists of all nodes that node k can share\ninformation with; these are the nodes that are connected to k through edges, in addition to node k itself.\nThe degree of node k, which we denote by nk, is de\ufb01ned as the positive integer that is equal to the size of\nits neighborhood:\nnk\n\u2206= |Nk|\n(573)\nSince k \u2208Nk, we always have nk \u22651. We further associate with the network an N \u00d7 N Laplacian matrix,\n95\ndenoted by L. The matrix L is symmetric and its entries are de\ufb01ned as follows [64\u201366]:\n[L]k\u2113=\n\uf8f1\n\uf8f2\n\uf8f3\nnk \u22121,\nif k = \u2113\n\u22121,\nif k \u0338= \u2113and nodes k and \u2113are neighbors\n0,\notherwise\n(574)\nNote that the term nk \u22121 measures the number of edges that are incident on node k, and the locations\nof the \u22121\u2032s on row k indicate the nodes that are connected to node k. We also associate with the graph\nan N \u00d7 L incidence matrix, denoted by I. The entries of I are de\ufb01ned as follows. Every column of I\nrepresents one edge in the graph. Each edge connects two nodes and its column will display two nonzero\nentries at the rows corresponding to these nodes: one entry will be +1 and the other entry will be \u22121. For\ndirected graphs, the choice of which entry is positive or negative can be used to identify the nodes from\nwhich edges emanate (source nodes) and the nodes at which edges arrive (sink nodes). Since we are dealing\nwith undirected graphs, we shall simply assign positive values to lower indexed nodes and negative values to\nhigher indexed nodes:\n[I]ke =\n\uf8f1\n\uf8f2\n\uf8f3\n+1,\nif node k is the lower-indexed node connected to edge e\n\u22121,\nif node k is the higher-indexed node connected to edge e\n0,\notherwise\n(575)\nFigure 16 shows the example of a network with N = 6 nodes and L = 8 edges. Its Laplacian and incidence\nmatrices are also shown and these have sizes 6 \u00d7 6 and 6 \u00d7 8, respectively. Consider, for example, column\n6 in the incidence matrix. This column corresponds to edge 6, which links nodes 3 and 5. Therefore, at\nlocation I36 we have a +1 and at location I56 we have \u22121. The other columns of I are constructed in a\nsimilar manner.\nL =\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0\n2\n\u22121\n\u22121\n0\n0\n0\n\u22121\n3\n\u22121\n\u22121\n0\n0\n\u22121\n\u22121\n4\n\u22121\n\u22121\n0\n0\n\u22121\n\u22121\n3\n0\n\u22121\n0\n0\n\u22121\n0\n2\n\u22121\n0\n0\n0\n\u22121\n\u22121\n2\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb\n1\n2\n3\n4\n5\n6\n1        2       3      4       5      6\n1\n2\n3\n4\n5\n6\n1\n2\n3\n4\n5\n6\n7\n8\nI =\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0\n1\n1\n0\n0\n0\n0\n0\n0\n0\n\u22121\n1\n1\n0\n0\n0\n0\n\u22121\n0\n\u22121\n0\n1\n1\n0\n0\n0\n0\n0\n\u22121\n\u22121\n0\n0\n1\n0\n0\n0\n0\n0\n\u22121\n1\n0\n0\n0\n0\n0\n0\n0\n\u22121\n\u22121\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb\nnodes\nnodes\n1\n2\n3\n4\n5\n6\n1      2      3       4      5      6\n7      8\nedges\nFigure 16: A network with N = 6 nodes and L = 8 edges. The nodes are marked 1 through 6 and the edges are\nmarked 1 through 8. The corresponding Laplacian and incidence matrices L and I are 6 \u00d7 6 and 6 \u00d7 8.\nObserve that the Laplacian and incidence matrices of a graph are related as follows:\nL = I IT\n(576)\n96\nThe Laplacian matrix conveys useful information about the topology of the graph. The following is a classical\nresult from graph theory [64\u201367].\nLemma B.1. (Laplacian and Network Connectivity) Let\n\u03b81 \u2265\u03b82 \u2265. . . \u2265\u03b8N\n(577)\ndenote the ordered eigenvalues of L. Then the following properties hold:\n(a) L is symmetric nonnegative-de\ufb01nite so that \u03b8i \u22650.\n(b) The rows of L add up to zero so that L1 = 0. This means that 1 is a right eigenvector of L corresponding\nto the eigenvalue zero.\n(c) The smallest eigenvalue is always zero, \u03b8N = 0. The second smallest eigenvalue, \u03b8N\u22121, is called the\nalgebraic connectivity of the graph.\n(d) The number of times that zero is an eigenvalue of L (i.e., its multiplicity) is equal to the number of\nconnected subgraphs.\n(e) The algebraic connectivity of a connected graph is nonzero, i.e., \u03b8N\u22121 \u0338= 0. In other words, a graph is\nconnected if, and only if, its algebraic connectivity is nonzero.\nProof. Property (a) follows from the identity L = I IT . Property (b) follows from the de\ufb01nition of L. Note that\nfor each row of L, the entries on the row add up to zero. Property (c) follows from properties (a) and (b) since\nL1 = 0 implies that zero is an eigenvalue of L. For part (d), assume the network consists of two separate connected\nsubgraphs. Then, the Laplacian matrix would have a block diagonal structure, say, of the form L = diag{L1, L2},\nwhere L1 and L2 are the Laplacian matrices of the smaller subgraphs. The smallest eigenvalue of each of these\nLaplacian matrices would in turn be zero and unique by property (e). More generally, if the graph consists of m\nconnected subgraphs, then the multiplicity of zero as an eigenvalue of L must be m. To establish property (e), \ufb01rst\nobserve that if the algebraic connectivity is nonzero then it is obvious that the graph must be connected. Otherwise,\nif the graph were disconnected, then its Laplacian matrix would be block diagonal and the algebraic multiplicity of\nzero as an eigenvalue of L would be larger than one so that \u03b8N\u22121 would be zero, which is a contradiction. For the\nconverse statement, assume the graph is connected and let x denote an arbitrary eigenvector of L corresponding to\nthe eigenvalue at zero, i.e., Lx = 0. We already know that L1 = 1 from property (b). Let us verify that x must\nbe proportional to the vector 1 so that the algebraic multiplicity of the eigenvalue at zero is one. Thus note that\nxT Lx = 0. If we denote the individual entries of x by xk, then this identity implies that for each node k:\nX\n\u2113\u2208Nk\n(xk \u2212x\u2113)2 = 0\nIt follows that the entries of x within each neighborhood have equal values. But since the graph is connected, we\nconclude that all entries of x must be equal. It follows that the eigenvector x is proportional to the vector 1, as\ndesired.\nC\nAppendix: Stochastic Matrices\nConsider N \u00d7 N matrices A with nonnegative entries, {a\u2113k \u22650}.\nThe matrix A = [a\u2113k] is said to be\nright-stochastic if it satis\ufb01es\nA1 = 1\n(right-stochastic)\n(578)\nin which case each row of A adds up to one. The matrix A is said to be left-stochastic if it satis\ufb01es\nAT 1 = 1\n(left-stochastic)\n(579)\n97\nin which case each column of A adds up to one. And the matrix is said to be doubly stochastic if both\nconditions hold so that both its columns and rows add up to one:\nA1 = 1,\nAT 1 = 1\n(doubly-stochastic)\n(580)\nStochastic matrices arise frequently in the study of adaptation over networks. This appendix lists some of\ntheir properties.\nLemma C.1. (Spectral Norm of Stochastic Matrices) Let A be an N \u00d7 N right or left or doubly\nstochastic matrix. Then, \u03c1(A) = 1 and, therefore, all eigenvalues of A lie inside the unit disc, i.e., |\u03bb(A)| \u22641.\nProof. We prove the result for right stochastic matrices; a similar argument applies to left or doubly stochastic\nmatrices. Let A be a right-stochastic matrix. Then, A1 = 1, so that \u03bb = 1 is one of the eigenvalues of A. Moreover,\nfor any matrix A, it holds that \u03c1(A) \u2264\u2225A\u2225\u221e, where \u2225\u00b7 \u2225\u221edenotes the maximum absolute row sum of its matrix\nargument. But since all rows of A add up to one, we have \u2225A\u2225\u221e= 1. Therefore, \u03c1(A) \u22641. And since we already\nknow that A has an eigenvalue at \u03bb = 1, we conclude that \u03c1(A) = 1.\nThe above result asserts that the spectral radius of a stochastic matrix is unity and that A has an eigenvalue\nat \u03bb = 1. The result, however, does not rule out the possibility of multiple eigenvalues at \u03bb = 1, or even\nother eigenvalues with magnitude equal to one. Assume, in addition, that the stochastic matrix A is regular.\nThis means that there exists an integer power jo such that all entries of Ajo are strictly positive, i.e.,\nfor all (\u2113, k), it holds that\n\u0002\nAjo\u0003\n\u2113k > 0, for some jo > 0\n(581)\nThen a result in matrix theory known as the Perron-Frobenius Theorem [20] leads to the following stronger\ncharacterization of the eigen-structure of A.\nLemma C.2. (Spectral Norm of Regular Stochastic Matrices) Let A be an N \u00d7 N right stochastic\nand regular matrix. Then:\n(a) \u03c1(A) = 1.\n(b) All other eigenvalues of A are strictly inside the unit circle (and, hence, have magnitude strictly less\nthan one).\n(c) The eigenvalue at \u03bb = 1 is simple, i.e., it has multiplicity one. Moreover, with proper sign scaling, all\nentries of the corresponding eigenvector are positive. For a right-stochastic A, this eigenvector is the\nvector 1 since A1 = 1.\n(d) All other eigenvectors associated with the other eigenvalues will have at least one negative or complex\nentry.\nProof. Part (a) follows from Lemma C.1.\nParts (b)-(d) follow from the Perron-Frobenius Theorem when A is\nregular [20].\nLemma C.3. (Useful Properties of Doubly Stochastic Matrices) Let A be an N \u00d7N doubly stochastic\nmatrix. Then the following properties hold:\n(a) \u03c1(A) = 1.\n(b) AAT and AT A are doubly stochastic as well.\n(c) \u03c1(AAT ) = \u03c1(AT A) = 1.\n(d) The eigenvalues of AAT or AT A are real and lie inside the interval [0, 1].\n98\n(e) I \u2212AAT \u22650 and I \u2212AT A \u22650.\n(f) Tr(AT HA) \u2264Tr(H), for any N \u00d7 N nonnegative-de\ufb01nite Hermitian matrix H.\nProof. Part (a) follows from Lemma C.1. For part (b), note that AAT is symmetric and AAT 1 = A1 = 1. Therefore,\nAAT is doubly stochastic. Likewise for AT A. Part (c) follows from part (a) since AAT and AT A are themselves\ndoubly stochastic matrices.\nFor part (d), note that AAT is symmetric and nonnegative-de\ufb01nite.\nTherefore, its\neigenvalues are real and nonnegative. But since \u03c1(AAT ) = 1, we must have \u03bb(AAT) \u2208[0, 1]. Likewise for the matrix\nAT A. Part (e) follows from part (d). For part (f), since AAT \u22650 and its eigenvalues lie within [0, 1], the matrix\nAAT admits an eigen-decomposition of the form:\nAAT = U\u039bU T\nwhere U is orthogonal (i.e., U \u22121 = U T ) and \u039b is diagonal with entries in the range [0, 1]. It then follows that\nTr(AT HA)\n=\nTr(AATH)\n=\nTr(U\u039bU T H)\n=\nTr(\u039bU T HU)\n(\u2217)\n\u2264\nTr(U T HU)\n=\nTr(UU T H)\n=\nTr(H)\nwhere step (\u2217) is because U T HU = U \u22121HU and, by similarity, the matrix U \u22121HU has the same eigenvalues as H.\nTherefore, U T HU \u22650. This means that the diagonal entries of U T HU are nonnegative. Multiplying U T HU by \u039b\nends up scaling the nonnegative diagonal entries to smaller values so that (\u2217) is justi\ufb01ed.\nD\nAppendix: Block Maximum Norm\nLet x = col{x1, x2, . . . , xN} denote an N \u00d7 1 block column vector whose individual entries are of size M \u00d7 1\neach. Following [52,54,55], the block maximum norm of x is denoted by \u2225x\u2225b,\u221eand is de\ufb01ned as\n\u2225x\u2225b,\u221e\n\u2206=\nmax\n1\u2264k\u2264N \u2225xk\u2225\n(582)\nwhere \u2225\u00b7\u2225denotes the Euclidean norm of its vector argument. Correspondingly, the induced block maximum\nnorm of an arbitrary N \u00d7N block matrix A, whose individual block entries are of size M \u00d7M each, is de\ufb01ned\nas\n\u2225A\u2225b,\u221e\n\u2206= max\nx\u0338=0\n\u2225Ax\u2225b,\u221e\n\u2225x\u2225b,\u221e\n(583)\nThe block maximum norm inherits the unitary invariance property of the Euclidean norm, as the following\nresult indicates [54].\nLemma D.1. (Unitary Invariance) Let U = diag{U1, U2, . . . , UN} be an N \u00d7 N block diagonal matrix\nwith M \u00d7 M unitary blocks {Uk}. Then, the following properties hold:\n(a) \u2225Ux\u2225b,\u221e= \u2225x\u2225b,\u221e\n(b) \u2225UAU\u2217\u2225b,\u221e= \u2225A\u2225b,\u221e\nfor all block vectors x and block matrices A of appropriate dimensions.\n\u25a1\n99\nThe next result provides useful bounds for the block maximum norm of a block matrix.\nLemma D.2. (Useful Bounds) Let A be an arbitrary N \u00d7 N block matrix with blocks A\u2113k of size M \u00d7 M\neach. Then, the following results hold:\n(a) The norms of A and its complex conjugate are related as follows:\n\u2225A\u2217\u2225b,\u221e\u2264N \u00b7 \u2225A\u2225b,\u221e\n(584)\n(b) The norm of A is bounded as follows:\nmax\n1\u2264\u2113,k\u2264N \u2225A\u2113k\u2225\u2264\u2225A\u2225b,\u221e\u2264N \u00b7\n\u0012\nmax\n1\u2264\u2113,k\u2264N \u2225A\u2113k\u2225\n\u0013\n(585)\nwhere \u2225\u00b7 \u2225denotes the 2\u2212induced norm (or maximum singular value) of its matrix argument.\n(c) If A is Hermitian and nonnegative-de\ufb01nite (A \u22650), then there exist \ufb01nite positive constants c1 and c2\nsuch that\nc1 \u00b7 Tr(A) \u2264\u2225A\u2225b,\u221e\u2264c2 \u00b7 Tr(A)\n(586)\nProof. Part (a) follows directly from part (b) by noting that\n\u2225A\u2217\u2225b,\u221e\n\u2264\nN \u00b7\n\u0012\nmax\n1\u2264\u2113,k\u2264N \u2225A\u2217\n\u2113k\u2225\n\u0013\n=\nN \u00b7\n\u0012\nmax\n1\u2264\u2113,k\u2264N \u2225A\u2113k\u2225\n\u0013\n\u2264\nN \u00b7 \u2225A\u2225b,\u221e\nwhere the equality in the second step is because \u2225A\u2217\n\u2113k\u2225= \u2225A\u2113k\u2225; i.e., complex conjugation does not alter the 2\u2212induced\nnorm of a matrix.\nTo establish part (b), we consider arbitrary N \u00d7 1 block vectors x with entries x = col{x1, x2, . . . , xN} and where\neach xk is M \u00d7 1. Then, note that\n\u2225Ax\u2225b,\u221e\n=\nmax\n1\u2264\u2113\u2264N\n\r\r\r\r\r\nN\nX\nk=1\nA\u2113kxk\n\r\r\r\r\r\n\u2264\nmax\n1\u2264\u2113\u2264N\n N\nX\nk=1\n\u2225A\u2113k\u2225\u00b7 \u2225xk\u2225\n!\n\u2264\n \nmax\n1\u2264\u2113\u2264N\nN\nX\nk=1\n\u2225A\u2113k\u2225\n!\n\u00b7 max\n1\u2264k\u2264N \u2225xk\u2225\n\u2264\n \nmax\n1\u2264\u2113\u2264N\nN\nX\nk=1\nmax\n1\u2264k\u2264N \u2225A\u2113k\u2225\n!\n\u00b7 \u2225x\u2225b,\u221e\n\u2264\nN \u00b7\n\u0012\nmax\n1\u2264\u2113,k\u2264N \u2225A\u2113k\u2225\n\u0013\n\u00b7 \u2225x\u2225b,\u221e\nso that\n\u2225A\u2225b,\u221e\n\u2206=\nmax\nx\u0338=0\n\u2225Ax\u2225b,\u221e\n\u2225x\u2225b,\u221e\n\u2264N \u00b7\n\u0012\nmax\n1\u2264\u2113,k\u2264N \u2225A\u2113k\u2225\n\u0013\nwhich establishes the upper bound in (585).\nTo establish the lower bound, we assume without loss of generality that max1\u2264\u2113,k\u2264N \u2225A\u2113k\u2225is attained at \u2113= 1\nand k = 1. Let \u03c31 denote the largest singular value of A11 and let {v1, u1} denote the corresponding M \u00d7 1 right and\nleft singular vectors. That is,\n\u2225A11\u2225= \u03c31,\nA11v1 = \u03c31u1\n(587)\n100\nwhere v1 and u1 have unit norms. We now construct an N \u00d7 1 block vector xo as follows:\nxo\n\u2206= col{v1, 0M, 0M, . . . , 0M}\n(588)\nThen, obviously,\n\u2225xo\u2225b,\u221e= 1\n(589)\nand\nAxo = col{A11v1, A21v1, . . . , AN1v1}\n(590)\nIt follows that\n\u2225Axo\u2225b,\u221e\n=\nmax { \u2225A11v1\u2225, \u2225A21v1\u2225, . . . , \u2225AN1v1\u2225}\n\u2265\n\u2225A11v1\u2225\n=\n\u2225\u03c31u1\u2225\n=\n\u03c31\n=\n\u2225A11\u2225\n=\nmax\n1\u2264\u2113,k\u2264N \u2225A\u2113k\u2225\n(591)\nTherefore, by the de\ufb01nition of the block maximum norm,\n\u2225A\u2225b,\u221e\n\u2206=\nmax\nx\u0338=0\n\u0012\u2225Ax\u2225b,\u221e\n\u2225x\u2225b,\u221e\n\u0013\n\u2265\n\u2225Axo\u2225b,\u221e\n\u2225xo\u2225b,\u221e\n=\n\u2225Axo\u2225b,\u221e\n\u2265\nmax\n1\u2264\u2113,k\u2264N \u2225A\u2113k\u2225\n(592)\nwhich establishes the lower bound in (585).\nTo establish part (c), we start by recalling that all norms on \ufb01nite-dimensional vector spaces are equivalent [20,21].\nThis means that if \u2225\u00b7 \u2225a and \u2225\u00b7 \u2225d denote two di\ufb00erent matrix norms, then there exist positive constants c1 and c2\nsuch that for any matrix X,\nc1 \u00b7 \u2225X\u2225a \u2264\u2225X\u2225d \u2264c2 \u00b7 \u2225X\u2225a\n(593)\nNow, let \u2225A\u2225\u2217denote the nuclear norm of the square matrix A. It is de\ufb01ned as the sum of its singular values:\n\u2225A\u2225\u2217\n\u2206=\nX\nm\n\u03c3m(A)\n(594)\nSince A is Hermitian and nonnegative-de\ufb01nite, its eigenvalues coincide with its singular values and, therefore,\n\u2225A\u2225\u2217=\nX\nm\n\u03bbm(A) = Tr(A)\nNow applying result (593) to the two norms \u2225A\u2225b,\u221eand \u2225A\u2225\u2217we conclude that\nc1 \u00b7 Tr(A) \u2264\u2225A\u2225b,\u221e\u2264c2 \u00b7 Tr(A)\n(595)\nas desired.\nThe next result relates the block maximum norm of an extended matrix to the \u221e\u2212norm (i.e., maximum\nabsolute row sum) of the originating matrix. Speci\ufb01cally, let A be an N \u00d7 N matrix with bounded entries\nand introduce the block matrix\nA\n\u2206= A \u2297IM\n(596)\nThe extended matrix A has blocks of size M \u00d7 M each.\n101\nLemma D.3. (Relation to Maximum Absolute Row Sum) Let A and A be related as in (596). Then,\nthe following properties hold:\n(a) \u2225A\u2225b,\u221e= \u2225A\u2225\u221e, where the notation \u2225\u00b7 \u2225\u221edenotes the maximum absolute row sum of its argument.\n(b) \u2225A\u2217\u2225b,\u221e\u2264N \u00b7 \u2225A\u2225b,\u221e.\nProof. The results are obvious for a zero matrix A. So assume A is nonzero. Let x = col{x1, x2, . . . , xN} denote an\narbitrary N \u00d7 1 block vector whose individual entries {xk} are vectors of size M \u00d7 1 each. Then,\n\u2225Ax\u2225b,\u221e\n=\nmax\n1\u2264k\u2264N\n\r\r\r\r\r\nN\nX\n\u2113=1\nak\u2113x\u2113\n\r\r\r\r\r\n\u2264\nmax\n1\u2264k\u2264N\n N\nX\n\u2113=1\n|ak\u2113| \u00b7 \u2225x\u2113\u2225\n!\n\u2264\n \nmax\n1\u2264k\u2264N\nN\nX\n\u2113=1\n|ak\u2113|\n!\n\u00b7 max\n1\u2264\u2113\u2264N \u2225x\u2113\u2225\n=\n\u2225A\u2225\u221e\u00b7 \u2225x\u2225b,\u221e\n(597)\nso that\n\u2225A\u2225b,\u221e\n\u2206=\nmax\nx\u0338=0\n\u2225Ax\u2225b,\u221e\n\u2225x\u2225b,\u221e\n\u2264\u2225A\u2225\u221e\n(598)\nThe argument so far establishes that \u2225A\u2225b,\u221e\u2264\u2225A\u2225\u221e. Now, let ko denote the row index that corresponds to the\nmaximum absolute row sum of A, i.e.,\n\u2225A\u2225\u221e=\nN\nX\n\u2113=1\n|ako\u2113|\nWe construct an N \u00d7 1 block vector z = col{z1, z2, . . . , zN}, whose M \u00d7 1 entries {z\u2113} are chosen as follows:\nz\u2113= sign(ako\u2113) \u00b7 e1\nwhere e1 is the M \u00d7 1 basis vector:\ne1 = col{1, 0, 0, . . . , 0}\nand the sign function is de\ufb01ned as\nsign(a) =\n\u001a\n1,\nif a \u22650\n\u22121,\notherwise\nThen, note that z \u0338= 0 for any nonzero matrix A, and\n\u2225z\u2225b,\u221e=\nmax\n1\u2264\u2113\u2264N \u2225z\u2113\u2225= 1\n102\nMoreover,\n\u2225A\u2225b,\u221e\n\u2206=\nmax\nx\u0338=0\n\u2225Ax\u2225b,\u221e\n\u2225x\u2225b,\u221e\n\u2265\n\u2225Az\u2225b,\u221e\n\u2225z\u2225b,\u221e\n=\n\u2225Az\u2225b,\u221e\n=\nmax\n1\u2264k\u2264N\n\r\r\r\r\r\nN\nX\n\u2113=1\nak\u2113z\u2113\n\r\r\r\r\r\n\u2265\n\r\r\r\r\r\nN\nX\n\u2113=1\nako\u2113z\u2113\n\r\r\r\r\r\n=\n\r\r\r\r\r\nN\nX\n\u2113=1\nako\u2113\u00b7 sign(ako\u2113)e1\n\r\r\r\r\r\n=\nN\nX\n\u2113=1\n|ako\u2113| \u00b7 \u2225e1\u2225\n=\nN\nX\n\u2113=1\n|ako\u2113|\n=\n\u2225A\u2225\u221e\n(599)\nCombining this result with (598) we conclude that \u2225A\u2225b,\u221e= \u2225A\u2225\u221e, which establishes part (a). Part (b) follows from\nthe statement of part (a) in Lemma D.2.\nThe next result establishes a useful property for the block maximum norm of right or left stochastic matrices;\nsuch matrices arise as combination matrices for distributed processing over networks as in (166) and (185).\nLemma D.4. (Right and Left Stochastic Matrices) Let C be an N \u00d7 N right stochastic matrix, i.e.,\nits entries are nonnegative and it satis\ufb01es C1 = 1. Let A be an N \u00d7 N left stochastic matrix, i.e., its entries\nare nonnegative and it satis\ufb01es AT 1 = 1. Introduce the block matrices\nAT\n\u2206= AT \u2297IM,\nC\n\u2206= C \u2297IM\n(600)\nThe matrices A and C have blocks of size M \u00d7 M each. It holds that\n\u2225AT \u2225b,\u221e= 1,\n\u2225C\u2225b,\u221e= 1\n(601)\nProof. Since AT and C are right stochastic matrices, it holds that \u2225AT \u2225\u221e= 1 and \u2225C\u2225\u221e= 1. The desired result\nthen follows from part (a) of Lemma D.3.\nThe next two results establish useful properties for the block maximum norm of a block diagonal matrix\ntransformed by stochastic matrices; such transformations arise as coe\ufb03cient matrices that control the evo-\nlution of weight error vectors over networks, as in (189).\nLemma D.5. (Block Diagonal Hermitian Matrices) Consider an N \u00d7 N block diagonal Hermitian\nmatrix D = diag{D1, D2, . . . , DN}, where each Dk is M \u00d7 M Hermitian. It holds that\n\u03c1(D) =\nmax\n1\u2264k\u2264N \u03c1(Dk) = \u2225D\u2225b,\u221e\n(602)\n103\nwhere \u03c1(\u00b7) denotes the spectral radius (largest eigenvalue magnitude) of its argument. That is, the spectral\nradius of D agrees with the block maximum norm of D, which in turn agrees with the largest spectral radius\nof its block components.\nProof. We already know that the spectral radius of any matrix X satis\ufb01es \u03c1(X ) \u2264\u2225X \u2225, for any induced matrix\nnorm [19,20]. Applying this result to D we readily get that \u03c1(D) \u2264\u2225D\u2225b,\u221e. We now establish the reverse inequality,\nnamely, \u2225D\u2225b,\u221e\u2264\u03c1(D). Thus, pick an arbitrary N \u00d7 1 block vector x with entries {x1, x2, . . . , xN}, where each xk\nis M \u00d7 1. From de\ufb01nition (583) we have\n\u2225D\u2225b,\u221e\n\u2206=\nmax\nx\u0338=0\n\u2225Dx\u2225b,\u221e\n\u2225x\u2225b,\u221e\n=\nmax\nx\u0338=0\n\u0012\n1\n\u2225x\u2225b,\u221e\n\u00b7 max\n1\u2264k\u2264N \u2225Dkxk\u2225\n\u0013\n\u2264\nmax\nx\u0338=0\n\u0012\n1\n\u2225x\u2225b,\u221e\u00b7 max\n1\u2264k\u2264N (\u2225Dk\u2225\u00b7 \u2225xk\u2225)\n\u0013\n=\nmax\nx\u0338=0\nmax\n1\u2264k\u2264N\n\u0012\n\u2225Dk\u2225\u00b7\n\u2225xk\u2225\n\u2225x\u2225b,\u221e\n\u0013\n\u2264\nmax\n1\u2264k\u2264N \u2225Dk\u2225\n=\nmax\n1\u2264k\u2264N \u03c1(Dk)\n(603)\nwhere the notation \u2225Dk\u2225denotes the 2\u2212induced norm of Dk (i.e., its largest singular value). But since Dk is assumed\nto be Hermitian, its 2\u2212induced norm agrees with its spectral radius, which explains the last equality.\nLemma D.6. (Block Diagonal Matrix Transformed by Left Stochastic Matrices) Consider an\nN \u00d7 N block diagonal Hermitian matrix D = diag{D1, D2, . . . , DN}, where each Dk is M \u00d7 M Hermitian.\nLet A1 and A2 be N \u00d7N left stochastic matrices, i.e., their entries are nonnegative and they satisfy AT\n1 1 = 1\nand AT\n2 1 = 1. Introduce the block matrices\nAT\n1 = AT\n1 \u2297IM,\nAT\n2\n\u2206= AT\n2 \u2297IM\n(604)\nThe matrices A1 and A2 have blocks of size M \u00d7 M each. Then it holds that\n\u03c1\n\u0000AT\n2 \u00b7 D \u00b7 AT\n1\n\u0001\n\u2264\u03c1(D)\n(605)\nProof. Since the spectral radius of any matrix never exceeds any induced norm of the same matrix, we have that\n\u03c1\n\u0010\nAT\n2 \u00b7 D \u00b7 AT\n1\n\u0011\n\u2264\n\r\r\r AT\n2 \u00b7 D \u00b7 AT\n1\n\r\r\r\nb,\u221e\n\u2264\n\r\r\rAT\n2\n\r\r\r\nb,\u221e\u00b7 \u2225D\u2225b,\u221e\u00b7\n\r\r\rAT\n1\n\r\r\r\nb,\u221e\n(601)\n=\n\u2225D\u2225b,\u221e\n(602)\n=\n\u03c1(D)\n(606)\nIn view of the result of Lemma D.5, we also conclude from (605) that\n\u03c1\n\u0000AT\n2 \u00b7 D \u00b7 AT\n1\n\u0001\n\u2264\nmax\n1\u2264k\u2264N \u03c1(Dk)\n(607)\n104\nIt is worth noting that there are choices for the matrices {A1, A2, D} that would result in strict inequality\nin (605). Indeed, consider the special case:\nD =\n\u0014 2\n0\n0\n1\n\u0015\n,\nAT\n1 =\n\u0014\n1\n3\n2\n3\n2\n3\n1\n3\n\u0015\n,\nAT\n2 =\n\u0014\n1\n3\n2\n3\n2\n3\n1\n3\n\u0015\nThis case corresponds to N = 2 and M = 1 (scalar blocks). Then,\nAT\n2 DAT\n1 =\n\u0014\n2\n3\n2\n3\n2\n3\n1\n\u0015\nand it is easy to verify that\n\u03c1(D) = 2,\n\u03c1(AT\n2 DAT\n1 ) \u22481.52\nThe following conclusions follow as corollaries to the statement of Lemma D.6, where by a stable matrix X\nwe mean one whose eigenvalues lie strictly inside the unit circle.\nCorollary D.1. (Stability Properties) Under the same setting of Lemma D.6, the following conclusions\nhold:\n(a) The matrix AT\n2 DAT\n1 is stable whenever D is stable.\n(b) The matrix AT\n2 DAT\n1 is stable for all possible choices of left stochastic matrices A1 and A2 if, and only\nif, D is stable.\nProof. Since D is block diagonal, part (a) follows immediately from (605) by noting that \u03c1(D) < 1 whenever D is\nstable. [This statement \ufb01xes the argument that appeared in App. I of [18] and Lemma 2 of [33]. Since the matrix\nX in App. I of [18] and the matrix M in Lemma 2 of [33] are block diagonal, the \u2225\u00b7 \u2225b,\u221enorm should replace the\n\u2225\u00b7 \u2225\u03c1 norm used there, as in the proof that led to (606) and as already done in [54].] For part (b), assume \ufb01rst that\nD is stable, then AT\n2 DAT\n1 will also be stable by part (a) for any left-stochastic matrices A1 and A2. To prove the\nconverse, assume that AT\n2 DAT\n1 is stable for any choice of left stochastic matrices A1 and A2. Then, AT\n2 DAT\n1 is stable\nfor the particular choice A1 = I = A2 and it follows that D must be stable.\nE\nAppendix: Comparison with Consensus Strategies\nConsider a connected network consisting of N nodes. Each node has a state or measurement value xk,\npossibly a vector of size M \u00d7 1. All nodes in the network are interested in evaluating the average value of\ntheir states, which we denote by\nwo\n\u2206=\n1\nN\nN\nX\nk=1\nxk\n(608)\nA centralized solution to this problem would require each node to transmit its measurement xk to a fusion\ncenter. The central processor would then compute wo using (608) and transmit it back to all nodes. This\ncentralized mode of operation su\ufb00ers from at least two limitations. First, it requires communications and\npower resources to transmit the data back and forth between the nodes and the central processor; this\nproblem is compounded if the fusion center is stationed at a remote location. Second, the architecture has a\ncritical point of failure represented by the central processor; if it fails, then operations would need to be halted.\nConsensus Recursion\nThe consensus strategy provides an elegant distributed solution to the same problem, whereby nodes interact\nlocally with their neighbors and are able to converge to wo through these interactions. Thus, consider an\n105\narbitrary node k and assign nonnegative weights {a\u2113k} to the edges linking k to its neighbors \u2113\u2208Nk. For\neach node k, the weights {a\u2113k} are assumed to add up to one so that\nfor k = 1, 2, . . ., N :\na\u2113k \u22650,\nN\nX\n\u2113=1\na\u2113k = 1,\na\u2113k = 0 if \u2113/\u2208Nk\n(609)\nThe resulting combination matrix is denoted by A and its k\u2212th column consists of the entries {a\u2113k, \u2113=\n1, 2, . . . , N}. In view of (609), the combination matrix A is seen to satisfy AT 1 = 1. That is, A is left-\nstochastic. The consensus strategy can be described as follows. Each node k operates repeatedly on the data\nfrom its neighbors and updates its state iteratively according to the rule:\nwk,n =\nX\n\u2113\u2208Nk\na\u2113k w\u2113,n\u22121,\nn > 0\n(610)\nwhere w\u2113,n\u22121 denotes the state of node \u2113at iteration n \u22121, and wk,n denotes the updated state of node k\nafter iteration n. The initial conditions are\nwk,o = xk,\nk = 1, 2, . . ., N\n(611)\nIf we collect the states of all nodes at iteration n into a column vector, say,\nzn\n\u2206= col{w1,n, w2,n, . . . , wN,n}\n(612)\nThen, the consensus iteration (610) can be equivalently rewritten in vector form as follows:\nzn = AT zn\u22121, n > 0\n(613)\nwhere\nAT = AT \u2297IM\n(614)\nThe initial condition is\nzo\n\u2206=\ncol{x1, x2, . . . , xN}\n(615)\nError Recursion\nNote that we can express the average value, wo, from (608) in the form\nwo = 1\nN \u00b7 (1T \u2297IM) \u00b7 zo\n(616)\nwhere 1 is the vector of size M \u00d7 1 and whose entries are all equal to one. Let\newk,n = wo \u2212wk,n\n(617)\ndenote the weight error vector for node k at iteration n; it measures how far the iterated state is from the\ndesired average value wo. We collect all error vectors across the network into an N \u00d7 1 block column vector\nwhose entries are of size M \u00d7 1 each:\newn\n\u2206=\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8f0\new1,n\new2,n\n...\newN,n\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fb\n(618)\n106\nThen,\newn = (1 \u2297IM)wo \u2212zn\n(619)\nConvergence Conditions\nThe following result is a classical result on consensus strategies [42\u201344]. It provides conditions under which\nthe state of all nodes will converge to the desired average, wo, so that ewn will tend to zero.\nTheorem E.1. (Convergence to Consensus) For any initial states {xk}, the successive iterates wk,n\ngenerated by the consensus iteration (610) converge to the network average value wo as n \u2192\u221eif, and only\nif, the following three conditions are met:\nAT 1\n=\n1\n(620)\nA1\n=\n1\n(621)\n\u03c1\n\u0012\nAT \u22121\nN 11T\n\u0013\n<\n1\n(622)\nThat is, the combination matrix A needs to be doubly stochastic, and the matrix AT \u22121\nN 11T needs to be\nstable.\nProof. (Su\ufb03ciency). Assume \ufb01rst that the three conditions stated in the theorem hold. Since A is doubly stochastic,\nthen so is any power of A, say, An for any n \u22650, so that\n[An]T 1 = 1,\nAn1 = 1\n(623)\nUsing this fact, it is straightforward to verify by induction the validity of the following equality:\n\u0012\nAT \u22121\nN 11T\n\u0013n\n=\n[An]T \u22121\nN 11T\n(624)\nLikewise, using the Kronecker product identities\n(E + B) \u2297C\n=\n(E \u2297C) + (B \u2297C)\n(625)\n(E \u2297B)(C \u2297D)\n=\n(EC \u2297BD)\n(626)\n(E \u2297B)n\n=\nEn \u2297Bn\n(627)\nfor matrices {E, B, C, D} of compatible dimensions, we observe that\n(An)T \u2212\n1\nN \u00b7 (1 \u2297IM) \u00b7 (1T \u2297IM)\n=\nh\n(An)T \u2297IM\ni\n\u2212\n1\nN \u00b7 (11T \u2297IM)\n=\n\u0014\n(An)T \u2212\n1\nN \u00b7 11T\n\u0015\n\u2297IM\n(624)\n=\n\u0012\nAT \u22121\nN 11T\n\u0013n\n\u2297IM\n=\n\u0014\u0012\nAT \u22121\nN 11T\n\u0013\n\u2297IM\n\u0015n\n(628)\nIterating (613) we \ufb01nd that\nzn = [An]T zo\n(629)\nand, hence, from (616) and (619),\newn\n=\n\u2212\n\u0014\n(An)T \u2212\n1\nN \u00b7 (1 \u2297IM) \u00b7 (1T \u2297IM)\n\u0015\n\u00b7 zo\n(628)\n=\n\u2212\n\u0014\u0012\nAT \u22121\nN 11T\n\u0013\n\u2297IM\n\u0015n\n\u00b7 zo\n(630)\n107\nNow recall that, for two arbitrary matrices C and D of compatible dimensions, the eigenvalues of the Kronecker\nproduct C \u2297D is formed of all product combinations \u03bbi(C)\u03bbj(D) of the eigenvalues of C and D [19]. We conclude\nfrom this property, and from the fact that AT \u22121\nN 11T is stable, that the coe\ufb03cient matrix\n\u0012\nAT \u2212\n1\nN \u00b7 11T\n\u0013\n\u2297IM\nis also stable. Therefore,\newn\n\u21920\nas n \u2192\u221e\n(631)\n(Necessity). In order for zn in (629) to converge to (1 \u2297IM)wo, for any initial state zo, it must hold that\nlim\nn\u2192\u221e(An)T \u00b7 zo =\n1\nN \u00b7 (1 \u2297IM) \u00b7 (1T \u2297IM) \u00b7 zo\n(632)\nfor any zo. This implies that we must have\nlim\nn\u2192\u221e(An)T =\n1\nN \u00b7 (11T \u2297IM)\n(633)\nor, equivalently,\nlim\nn\u2192\u221e(An)T =\n1\nN 11T\n(634)\nThis in turn implies that we must have\nlim\nn\u2192\u221eAT \u00b7 (An)T = AT \u00b7 1\nN 11T\n(635)\nBut since\nlim\nn\u2192\u221eAT \u00b7 (An)T =\nlim\nn\u2192\u221e\n\u0000An+1\u0001T =\nlim\nn\u2192\u221e(An)T\n(636)\nwe conclude from (634) and (635) that it must hold that\n1\nN 11T =\n1\nN AT \u00b7 11T\n(637)\nThat is,\n1\nN\n\u0010\nAT 1 \u22121\n\u0011\n\u00b7 1T = 0\n(638)\nfrom which we conclude that we must have AT 1 = 1. Similarly, we can show that A1 = 1 by studying the limit\nof (An)T AT . Therefore, A must be a doubly stochastic matrix. Now using the fact that A is doubly stochastic, we\nknow that (624) holds. It follows that in order for condition (634) to be satis\ufb01ed, we must have\n\u03c1\n\u0012\nAT \u22121\nN 11T\n\u0013\n< 1\n(639)\nRate of Convergence\nFrom (630) we conclude that the rate of convergence of the error vectors { ewk,n} to zero is determined by\nthe spectrum of the matrix\nAT \u22121\nN 11T\n(640)\nNow since A is a doubly stochastic matrix, we know that it has an eigenvalue at \u03bb = 1. Let us denote the\neigenvalues of A by \u03bbk(A) and let us order them in terms of their magnitudes as follows:\n0 \u2264|\u03bbM(A)| \u2264. . . \u2264|\u03bb3(A)| \u2264|\u03bb2(A)| \u22641\n(641)\nwhere \u03bb1(A) = 1. Then, the eigenvalues of the coe\ufb03cient matrix (AT \u22121\nN 11T ) are equal to\n{ \u03bbM(A), , . . . , \u03bb3(A), \u03bb2(A), 0 }\n(642)\nIt follows that the magnitude of \u03bb2(A) becomes the spectral radius of AT \u22121\nN 11T . Then condition (639)\nensures that |\u03bb2(A)| < 1. We therefore arrive at the following conclusion.\n108\nCorollary E.1. (Rate of Convergence of Consensus) Under conditions (620)\u2013(622), the rate of con-\nvergence of the successive iterates {wk,n} towards the network average wo in the consensus strategy (610) is\ndetermined by the second largest eigenvalue magnitude of A, i.e., by |\u03bb2(A)| as de\ufb01ned in (641).\n\u25a1\nIt is worth noting that doubly stochastic matrices A that are also regular satisfy conditions (620)\u2013(622).\nThis is because, as we already know from Lemma C.2, the eigenvalues of such matrices satisfy |\u03bbm(A)| < 1,\nfor m = 2, 3, . . ., N, so that condition (622) is automatically satis\ufb01ed.\nCorollary E.2. (Convergence for Regular Combination Matrices) Any doubly-stochastic and regular\nmatrix A satis\ufb01es the three conditions (620)\u2013(622) and, therefore, ensures the convergence of the consensus\niterates {wk,n} generated by (610) towards wo as n \u2192\u221e.\n\u25a1\nA regular combination matrix A would result when the two conditions listed below are satis\ufb01ed by the graph\nconnecting the nodes over which the consensus iteration is applied.\nCorollary E.3. (Su\ufb03cient Condition for Regularity) Assume the combination matrix A is doubly\nstochastic and that the graph over which the consensus iteration (610) is applied satis\ufb01es the following two\nconditions:\n(a) The graph is connected. This means that there exists a path connecting any two arbitrary nodes in the\nnetwork. In terms of the Laplacian matrix that is associated with the graph (see Lemma B.1), this\nmeans that the second smallest eigenvalue of the Laplacian is nonzero.\n(b) a\u2113k = 0 if, and only if, \u2113/\u2208Nk. That is, the combination weights are strictly positive between any two\nneighbors, including akk > 0.\nThen, the corresponding matrix A will be regular and, therefore, the consensus iterates {wk,n} generated by\n(610) will converge towards wo as n \u2192\u221e.\nProof. We \ufb01rst establish that conditions (a) and (b) imply that A is a regular matrix, namely, that there should\nexist an integer jo > 0 such that\nh\nAjoi\n\u2113k > 0\n(643)\nfor all (\u2113, k). To begin with, by the rules of matrix multiplication, the (\u2113, k) entry of the i\u2212th power of A is given by:\nh\nAii\n\u2113k =\nN\nX\nm1=1\nN\nX\nm2=1\n. . .\nN\nX\nmi\u22121=1\na\u2113m1am1m2 . . . ami\u22121k\n(644)\nThe summand in (644) is nonzero if, and only if, there is some sequence of indices (\u2113, m1, . . . , mi\u22121, k) that forms\na path from node \u2113to node k. Since the network is assumed to be connected, there exists a minimum (and \ufb01nite)\ninteger value i\u2113k such that a path exists from node \u2113to node k using i\u2113k edges and that\nh\nAi\u2113ki\n\u2113k > 0\nIn addition, by induction, if\n\u0002\nAi\u2113k\u0003\n\u2113k > 0, then\nh\nAi\u2113k+1i\n\u2113k\n=\nN\nX\nm=1\nh\nAi\u2113ki\n\u2113m amk\n\u2265\nh\nAi\u2113ki\n\u2113k akk\n>\n0\nLet\njo =\nmax\n1\u2264k,\u2113\u2264N {i\u2113k}\n109\nThen, property (643) holds for all (\u2113, k). And we conclude from (581) that A is a regular matrix. It then follows from\nCorollary E.2 that the consensus iterates {wk,n} converge to the average network value wo.\nComparison with Di\ufb00usion Strategies\nObserve that in comparison to di\ufb00usion strategies, such as the ATC strategy (153), the consensus iteration\n(610) employs the same quantities wk,\u00b7 on both sides of the iteration. In other words, the consensus con-\nstruction keeps iterating on the same set of vectors until they converge to the average value wo. Moreover,\nthe index n in the consensus algorithm is an iteration index. In contrast, di\ufb00usion strategies employ di\ufb00erent\nquantities on both sides of the combination step in (153), namely, wk,i and {\u03c8\u2113,i}; the latter variables have\nbeen processed through an information exchange step and are updated (or \ufb01ltered) versions of the w\u2113,i\u22121. In\naddition, each step of the di\ufb00usion strategy (153) can incorporate new data, {d\u2113(i), u\u2113,i}, that are collected\nby the nodes at every time instant. Moreover, the index i in the di\ufb00usion implementation is a time index\n(and not an iteration index); this is because di\ufb00usion strategies are inherently adaptive and perform online\nlearning. Data keeps streaming in and di\ufb00usion incorporates the new data into the update equations at\nevery time instant. As a result, di\ufb00usion strategies are able to respond to data in an adaptive manner, and\nthey are also able to solve general optimization problems: the vector wo in adaptive di\ufb00usion iterations is\nthe minimizer of a global cost function (cf. (92)), while the vector wo in consensus iterations is the average\nvalue of the initial states of the nodes (cf. (608)).\nMoreover, it turns out that di\ufb00usion strategies in\ufb02uence the evolution of the network dynamics in an\ninteresting and advantageous manner in comparison to consensus strategies. We illustrate this point by\nmeans of an example.\nConsider initially the ATC strategy (158) without information exchange, whose\nupdate equation we repeat below for ease of reference:\n\u03c8k,i\n=\nwk,i\u22121 + \u00b5ku\u2217\nk,i [dk(i) \u2212uk,iwk,i\u22121]\n(645)\nwk,i\n=\nX\n\u2113\u2208Nk\na\u2113k \u03c8\u2113,i\n(ATC di\ufb00usion )\n(646)\nThese recursions were derived in the body of the article as an e\ufb00ective distributed solution for optimizing\n(92)\u2013(93).\nNote that they involve two steps, where the weight estimator wk,i\u22121 is \ufb01rst updated to the\nintermediate estimator \u03c8k,i, before the intermediate estimators from across the neighborhood are combined\nto obtain wk,i. Both steps of ATC di\ufb00usion (645)\u2013(646) can be combined into a single update as follows:\nwk,i =\nX\n\u2113\u2208Nk\na\u2113k\n\u0002\nw\u2113,i\u22121 + \u00b5\u2113u\u2217\n\u2113,i (d\u2113(i) \u2212u\u2113,iw\u2113,i\u22121)\n\u0003\n(ATC di\ufb00usion)\n(647)\nLikewise, consider the CTA strategy (159) without information exchange, whose update equation we also\nrepeat below:\n\u03c8k,i\u22121\n=\nX\n\u2113\u2208Nk\na\u2113k w\u2113,i\u22121\n(CTA di\ufb00usion )\n(648)\nwk,i\n=\n\u03c8k,i\u22121 + \u00b5ku\u2217\nk,i\n\u0002\ndk(i) \u2212uk,i\u03c8k,i\u22121\n\u0003\n(649)\nAgain, the CTA strategy involves two steps: the weight estimators {w\u2113,i\u22121} from the neighborhood of node\nk are \ufb01rst combined to yield the intermediate estimator \u03c8k,i\u22121, which is subsequently updated to wk,i. Both\nsteps of CTA di\ufb00usion can also be combined into a single update as follows:\nwk,i =\nX\n\u2113\u2208Nk\na\u2113k w\u2113,i\u22121 + \u00b5ku\u2217\nk,i\n\"\ndk(i) \u2212uk,i\nX\n\u2113\u2208Nk\na\u2113kw\u2113,i\u22121\n#\n(CTA di\ufb00usion)\n(650)\n110\nNow, motivated by the consensus iteration (610), and based on a procedure for distributed optimization\nsuggested in [52] (see expression (7.1) in that reference), some works in the literature (e.g., [45, 53, 82\u201388])\nconsidered distributed strategies that correspond to the following form for the optimization problem under\nconsideration (see, e.g., expression (1.20) in [53] and expression (9) in [87]):\nwk,i =\nX\n\u2113\u2208Nk\na\u2113k w\u2113,i\u22121 + \u00b5ku\u2217\nk,i [dk(i) \u2212uk,iwk,i\u22121]\n(consensus strategy)\n(651)\nThis strategy can be derived by following the same argument we employed earlier in Secs. 3.2 and 4 to\narrive at the di\ufb00usion strategies, namely, we replace wo in (127) by w\u2113,i\u22121 and then apply the instantaneous\napproximations (150). Note that the same variable wk,\u00b7 appears on both sides of the equality in (651).\nThus, compared with the ATC di\ufb00usion strategy (647), the update from wk,i\u22121 to wk,i in the consensus\nimplementation (651) is only in\ufb02uenced by data {dk(i), uk,i} from node k. In contrast, the ATC di\ufb00usion\nstructure (645)\u2013(646) helps incorporate the in\ufb02uence of the data {d\u2113(i), u\u2113,i} from across the neighborhood\nof node k into the update of wk,i, since these data are re\ufb02ected in the intermediate estimators {\u03c8\u2113,i}.\nLikewise, the contrast with the CTA di\ufb00usion strategy (650) is clear, where the right-most term in (650)\nrelies on a combination of all estimators from across the neighborhood of node k, and not only on wk,i\u22121 as\nin the consensus strategy (651). These facts have desirable implications on the evolution of the weight-error\nvectors across di\ufb00usion networks. Some simple algebra, similar to what we did in Sec. 6, will show that the\nmean of the extended error vector for the consensus strategy (651) evolves according to the recursion:\nE ewi =\n\u0000AT \u2212MRu\n\u0001\n\u00b7 E ewi\u22121, i \u22650\n(consensus strategy)\n(652)\nwhere Ru is the block diagonal covariance matrix de\ufb01ned by (184) and ewi is the aggregate error vector\nde\ufb01ned by (230). We can compare the above mean error dynamics with the ones that correspond to the\nATC and CTA di\ufb00usion strategies (645)\u2013(646) and (648)\u2013(650); their error dynamics follow as special cases\nfrom (248) by setting A1 = I = C and A2 = A for ATC and A2 = I = C and A1 = A for CTA:\nE ewi = AT (INM \u2212MRu) \u00b7 E ewi\u22121, i \u22650\n(ATC di\ufb00usion)\n(653)\nand\nE ewi = (INM \u2212MRu) AT \u00b7 E ewi\u22121, i \u22650\n(CTA di\ufb00usion)\n(654)\nWe observe that the coe\ufb03cient matrices that control the evolution of E ewi are di\ufb00erent in all three cases. In\nparticular,\nconsensus strategy (652) is stable in the mean\n\u21d0\u21d2\n\u03c1\n\u0000AT \u2212MRu\n\u0001\n< 1\n(655)\nATC di\ufb00usion (653) is stable in the mean\n\u21d0\u21d2\n\u03c1\n\u0002\nAT (INM \u2212MRu)\n\u0003\n< 1\n(656)\nCTA di\ufb00usion (654) is stable in the mean\n\u21d0\u21d2\n\u03c1\n\u0002\n(INM \u2212MRu) AT \u0003\n< 1\n(657)\nIt follows that the mean stability of the consensus network is sensitive to the choice of the combination\nmatrix A. This is not the case for the di\ufb00usion strategies. This is because from property (605) established in\nApp. D, we know that the matrices AT (INM \u2212MRu) and (INM \u2212MRu) AT are stable if (INM \u2212MRu)\nis stable. Therefore, we can select the step-sizes to satisfy \u00b5k < 2/\u03bbmax(Ru,k) for the ATC or CTA di\ufb00usion\nstrategies and ensure their mean stability regardless of the combination matrix A. This also means that\nthe di\ufb00usion networks will be mean stable whenever the individual nodes are mean stable, regardless of the\ntopology de\ufb01ned by A. In contrast, for consensus networks, the network can exhibit unstable mean behavior\n111\neven if all its individual nodes are stable in the mean. For further details and other results on the mean-\nsquare performance of di\ufb00usion networks in relation to consensus networks, the reader is referred to [89,90].\nAcknowledgement. The development of the theory and applications of di\ufb00usion adaptation over networks\nhas bene\ufb01ted greatly from the insights and contributions of several UCLA Ph.D. students, and several visiting\ngraduate students to the UCLA Adaptive Systems Laboratory (http://www.ee.ucla.edu/asl). The assistance\nand contributions of all students are hereby gratefully acknowledged, including Cassio G. Lopes, Federico S.\nCattivelli, Sheng-Yuan Tu, Jianshu Chen, Xiaochuan Zhao, Zaid Tow\ufb01c, Chung-Kai Yu, Noriyuki Takahashi,\nJae-Woo Lee, Alexander Bertrand, and Paolo Di Lorenzo. The author is also particularly thankful to S.-Y.\nTu, J. Chen, X. Zhao, Z. Tow\ufb01c, and C.-K. Yu for their assistance in reviewing an earlier draft of this chapter.\nReferences\n[1] J. Chen and A. H. Sayed, \u201cOn the limiting behavior of distributed optimization strategies,\u201d Proc. 50th Annual\nAllerton Conference on Communication, Control, and Computing, pp. 1\u20138, Allerton, IL, October 2012.\n[2] J. Chen and A. H. Sayed, \u201cDi\ufb00usion adaptation strategies for distributed optimization and learning over net-\nworks,\u201d IEEE Trans. Signal Processing, vol. 60, no. 8, pp. 4289\u20134305, August 2012.\n[3] J. Chen and A. H. Sayed, \u201cDistributed Pareto optimization via di\ufb00usion strategies,\u201d IEEE J. Selected Topics in\nSignal Processing, vol. 7, no. 2, pp. 205\u2013220, April 2013.\n[4] A. H. Sayed, Fundamentals of Adaptive Filtering, Wiley, NJ, 2003.\n[5] A. H. Sayed, Adaptive Filters, Wiley, NJ, 2008.\n[6] S. Haykin, Adaptive Filter Theory, 4th edition, Prentice Hall, NJ, 2002.\n[7] B. Widrow and S. D. Stearns, Adaptive Signal Processing, Prentice Hall, NJ, 1985.\n[8] S.-Y. Tu and A. H. Sayed, \u201cMobile adaptive networks,\u201d IEEE J. Sel. Topics. Signal Process., vol. 5, no. 4,\npp. 649\u2013664, Aug. 2011.\n[9] F. Cattivelli and A. H. Sayed, \u201cModeling bird \ufb02ight formations using di\ufb00usion adaptation,\u201d IEEE Transactions\non Signal Processing, vol. 59, no. 5, pp. 2038\u20132051, May 2011.\n[10] J. Li and A. H. Sayed, \u201cModeling bee swarming behavior through di\ufb00usion adaptation with asymmetric infor-\nmation sharing,\u201d EURASIP Journal on Advances in Signal Processing, 2012:18, doi:10.1186/1687-6180-2012-18,\n2012.\n[11] J. Chen and A. H. Sayed, \u201cBio-inspired cooperative optimization with application to bacteria motility,\u201d Proc.\nICASSP, Prague, Czech Republic, pp. 5788\u20135791, May 2011.\n[12] A. H. Sayed and F. A. Sayed, \u201cDi\ufb00usion adaptation over networks of particles subject to Brownian \ufb02uctuations,\u201d\nProc. Asilomar Conference on Signals, Systems, and Computers, pp. 685\u2013690, Paci\ufb01c Grove, CA, November 2011.\n[13] J. Mitola and G. Q. Maguire, \u201cCognitive radio: Making software radios more personal,\u201dIEEE Personal Commun.,\nvol. 6, pp. 13-18, 1999.\n[14] S. Haykin, \u201cCognitive radio: Brain-empowered wireless communications,\u201d IEEE J. Sel. Areas Commun., vol.\n23, no. 2, pp. 201-220, Feb. 2005.\n[15] Z. Quan, W. Zhang, S. J. Shellhammer, and A. H. Sayed, \u201cOptimal spectral feature detection for spectrum\nsensing at very low SNR,\u201d IEEE Transactions on Communications, vol. 59, no. 1, pp. 201-212, January 2011.\n[16] Q. Zou, S. Zheng, and A. H. Sayed, \u201cCooperative sensing via sequential detection,\u201d IEEE Transactions on Signal\nProcessing, vol. 58, no. 12, pp. 6266-6283, December 2010.\n[17] P. Di Lorenzo, S. Barbarossa, and A. H. Sayed, \u201cBio-inspired swarming for dynamic radio access based on\ndi\ufb00usion adaptation,\u201d Proc. EUSIPCO, pp. 402-406, Barcelona, Spain, August 2011.\n[18] F. S. Cattivelli and A. H. Sayed, \u201cDi\ufb00usion LMS strategies for distributed estimation,\u201d IEEE Trans. Signal\nProcess., vol. 58, no. 3, pp. 1035\u20131048, March 2010.\n[19] Golub, G. H. and C. F. Van Loan (1996), Matrix Computations, 3rd edition, The John Hopkins University Press,\nBaltimore.\n112\n[20] R. A. Horn and C. R. Johnson, Matrix Analysis, Cambridge University Press, 2003.\n[21] E. Kreyszig, Introductory Functional Analysis with Applications, Wiley, NY, 1989.\n[22] B. Poljak, Introduction to Optimization, Optimization Software, NY, 1987.\n[23] D. P. Bertsekas, \u201cA new class of incremental gradient methods for least squares problems,\u201d SIAM J. Optim.,\nvol. 7, no. 4, pp. 913\u2013926, 1997.\n[24] A. Nedic and D. P. Bertsekas, \u201cIncremental subgradient methods for nondi\ufb00erentiable optimization,\u201d SIAM J.\nOptim., vol. 12, no. 1, pp. 109\u2013138, 2001.\n[25] M. G. Rabbat and R. D. Nowak, \u201cQuantized incremental algorithms for distributed optimization,\u201d IEEE J. Sel.\nAreas Commun., vol. 23, no. 4, pp. 798\u2013808, 2005.\n[26] C. G. Lopes and A. H. Sayed, \u201cIncremental adaptive strategies over distributed networks,\u201d IEEE Trans. Signal\nProcess., vol. 55, no. 8, pp. 4064\u20134077, Aug. 2007.\n[27] F. S. Cattivelli and A. H. Sayed, \u201cDi\ufb00usion LMS algorithms with information exchange,\u201d Proc. Asilomar Conf.\nSignals, Syst. Comput., Paci\ufb01c Grove, CA, pp. 251\u2013255, Nov. 2008.\n[28] F. S. Cattivelli, C. G. Lopes, and A. H. Sayed, \u201cA di\ufb00usion RLS scheme for distributed estimation over adaptive\nnetworks,\u201d Proc. IEEE Workshop on Signal Process. Advances Wireless Comm. (SPAWC), Helsinki, Finland,\npp. 1\u20135, June 2007.\n[29] F. S. Cattivelli, C. G. Lopes, and A. H. Sayed, \u201cDi\ufb00usion recursive least-squares for distributed estimation over\nadaptive networks,\u201d IEEE Trans. Signal Process., vol. 56, no. 5, pp. 1865\u20131877, May 2008.\n[30] F. S. Cattivelli, C. G. Lopes, and A. H. Sayed, \u201cDi\ufb00usion strategies for distributed Kalman \ufb01ltering: Formulation\nand performance analysis,\u201d Proc. IAPR Workshop on Cognitive Inf. Process.(CIP), Santorini, Greece, pp. 36\u201341,\nJune 2008.\n[31] F. S. Cattivelli and A. H. Sayed, \u201cDi\ufb00usion mechanisms for \ufb01xed-point distributed Kalman smoothing,\u201d Proc.\nEUSIPCO, Lausanne, Switzerland, pp. 1\u20134, Aug. 2008.\n[32] A. H. Sayed and F. Cattivelli, \u201cDistributed adaptive learning mechanisms,\u201d Handbook on Array Processing and\nSensor Networks, S. Haykin and K. J. Ray Liu, Eds., pp. 695\u2013722, Wiley, NJ, 2009.\n[33] F. Cattivelli and A. H. Sayed, \u201cDi\ufb00usion strategies for distributed Kalman \ufb01ltering and smoothing,\u201d IEEE\nTransactions on Automatic Control, vol. 55, no. 9, pp. 2069\u20132084, Sep. 2010.\n[34] S. S. Stankovic, M. S. Stankovic, and D. S. Stipanovic, \u201cDecentralized parameter estimation by consensus based\nstochastic approximation,\u201d IEEE Trans. on Autom. Control, vol. 56, no. 3, pp. 531\u2013543, Mar. 2011.\n[35] C. G. Lopes and A. H. Sayed, \u201cDistributed processing over adaptive networks,\u201d in Proc. Adaptive Sensor Array\nProcessing Workshop, MIT Lincoln Laboratory, MA, pp.1\u20135, June 2006.\n[36] A. H. Sayed and C. G. Lopes, \u201cAdaptive processing over distributed networks,\u201d IEICE Trans. Fund. of Electron.,\nCommun. and Comput. Sci., vol. E90-A, no. 8, pp. 1504\u20131510, 2007.\n[37] C. G. Lopes and A. H. Sayed, \u201cDi\ufb00usion least-mean-squares over adaptive networks,\u201d Proc. IEEE ICASSP,\nHonolulu, Hawaii, vol. 3, pp. 917-920, April 2007.\n[38] C. G. Lopes and A. H. Sayed, \u201cSteady-state performance of adaptive di\ufb00usion least-mean squares,\u201d Proc. IEEE\nWorkshop on Statistical Signal Processing (SSP), pp. 136-140, Madison, WI, August 2007.\n[39] C. G. Lopes and A. H. Sayed, \u201cDi\ufb00usion least-mean squares over adaptive networks: Formulation and perfor-\nmance analysis,\u201d IEEE Trans. Signal Process., vol. 56, no. 7, pp. 3122\u20133136, July 2008.\n[40] S. S. Ram, A. Nedic, and V. V. Veeravalli, \u201cDistributed stochastic subgradient projection algorithms for convex\noptimization,\u201d J. Optim. Theory Appl., vol. 147, no. 3, pp. 516\u2013545, 2010.\n[41] P. Bianchi, G. Fort, W. Hachem, and J. Jakubowicz, \u201cConvergence of a distributed parameter estimator for\nsensor networks with local averaging of the estimates,\u201d Proc. IEEE ICASSP, Prague, Czech, pp. 3764\u20133767, May\n2011.\n[42] M. H. DeGroot, \u201cReaching a consensus,\u201d Journal of the American Statistical Association, vol. 69, no. 345, pp.\n118\u2013121, 1974.\n[43] R. L. Berger, \u201cA necessary and su\ufb03cient condition for reaching a consensus using DeGroot\u2019s method,\u201d Journal\nof the American Statistical Association, vol. 76, no. 374, pp. 415-418, Jun. 1981.\n113\n[44] J. Tsitsiklis and M. Athans, \u201cConvergence and asymptotic agreement in distributed decision problems,\u201d IEEE\nTrans. Autom. Control, vol. 29, no. 1, pp. 42\u201350, Jan. 1984.\n[45] A. Jadbabaie, J. Lin, and A. S. Morse, \u201cCoordination of groups of mobile autonomous agents using nearest\nneighbor rules,\u201d IEEE Trans. Autom. Control, vol. 48, no. 6, pp. 988\u20131001, Jun. 2003.\n[46] R. Olfati-Saber and R. M. Murray, \u201cConsensus problems in networks of agents with switching topology and\ntime-delays,\u201d IEEE Trans. Autom. Control, vol. 49, pp. 1520\u20131533, Sep. 2004.\n[47] R. Olfati-Saber, \u201cDistributed Kalman \ufb01lter with embedded consensus \ufb01lters,\u201d Proc. 44th IEEE Conf. Decision\nControl, pp. 8179\u20138184, Sevilla, Spain, Dec. 2005.\n[48] R. Olfati-Saber, \u201cDistributed Kalman \ufb01ltering for sensor networks,\u201d Proc. 46th IEEE Conf. Decision Control,\npp. 5492\u20135498, New Orleans, LA, Dec. 2007.\n[49] L. Xiao and S. Boyd, \u201cFast linear iterations for distributed averaging,\u201d Syst. Control Lett., vol. 53, no. 1, pp.\n65\u201378, Sep. 2004.\n[50] L. Xiao, S. Boyd, and S. Lall, \u201cA scheme for robust distributed sensor fusion based on average consensus,\u201d Proc.\nIPSN, 2005, pp. 63\u201370, Los Angeles, CA, April 2005.\n[51] U. A. Khan and J. M. F. Moura, \u201cDistributing the Kalman \ufb01lter for large-scale systems,\u201d IEEE Trans. Signal\nProcessing, vol. 56, no. 10, pp. 4919\u20134935, Oct. 2008.\n[52] D. P. Bertsekas and J. N. Tsitsiklis, Parallel and Distributed Computation: Numerical Methods, 1st edition,\nAthena Scienti\ufb01c, Singapore, 1997.\n[53] A. Nedic and A. Ozdaglar, \u201cCooperative distributed multi-agent optimization,\u201d in Convex Optimization in Signal\nProcessing and Communications, Y. Eldar and D. Palomar (Eds.), Cambridge University Press, pp. 340-386, 2010.\n[54] N. Takahashi, I. Yamada, and A. H. Sayed, \u201cDi\ufb00usion least-mean-squares with adaptive combiners: Formulation\nand performance analysis,\u201d IEEE Trans. on Signal Processing, vol. 9, pp. 4795-4810, Sep. 2010.\n[55] N. Takahashi and I. Yamada, \u201cParallel algorithms for variational inequalities over the cartesian product of the\nintersections of the \ufb01xed point sets of nonexpansive mappings,\u201d J. Approx. Theory, vol. 153, no. 2, pp. 139\u2013160,\nAug. 2008.\n[56] S. Boyd and L. Vandenberghe, Convex Optimization, Cambridge University Press, 2004.\n[57] T. Y. Al-Na\ufb00ouri and A. H. Sayed, \u201cTransient analysis of data-normalized adaptive \ufb01lters,\u201dIEEE Transactions\non Signal Processing, vol. 51, no. 3, pp. 639\u2013652, March 2003.\n[58] S-Y. Tu and A. H. Sayed, \u201cOptimal combination rules for adaptation and learning over networks,\u201d Proc. IEEE\nInternational Workshop on Computational Advances in Multi-Sensor Adaptive Processing (CAMSAP), San Juan,\nPuerto Rico, pp. 317\u2013320, December 2011.\n[59] R. Abdolee and B. Champagne, \u201cDi\ufb00usion LMS algorithms for sensor networks over non-ideal inter-sensor\nwireless channels,\u201d Proc. IEEE Int. Conf. Dist. Comput. Sensor Systems (DCOSS), pp. 1\u20136, Barcelona, Spain,\nJune 2011.\n[60] A. Khalili, M. A. Tinati, A. Rastegarnia, and J. A. Chambers, \u201cSteady state analysis of di\ufb00usion LMS adaptive\nnetworks with noisy links,\u201d IEEE Trans. Signal Processing, vol. 60, no. 2, pp. 974\u2013979, Feb. 2012.\n[61] S-Y. Tu and A. H. Sayed, \u201cAdaptive networks with noisy links,\u201d Proc. IEEE Globecom, pp. 1\u20135, Houston, TX,\nDecember 2011.\n[62] X. Zhao, S-Y. Tu, and A. H. Sayed, \u201cDi\ufb00usion adaptation over networks under imperfect information exchange\nand non-stationary data,\u201d IEEE Transactions on Signal Processing, vol. 60, no. 7, pp. 3460\u20133475, July 2012.\n[63] X. Zhao and A. H. Sayed, \u201cCombination weights for di\ufb00usion strategies with imperfect information exchange,\u201d\nProc. IEEE ICC, pp. 1\u20135, Ottawa, Canada, June 2012.\n[64] D. M. Cvetkovi\u00b4c, M. Doob, and H. Sachs, Spectra of Graphs: Theory and Applications, Wiley, NY, 1998.\n[65] B. Bollobas, Modern Graph Theory, Springer, 1998.\n[66] W. Kocay and D. L. Kreher, Graphs, Algorithms and Optimization, Chapman & Hall/CRC Press, Boca Raton,\n2005.\n[67] M. Fiedler, \u201cAlgebraic connectivity of graphs,\u201d Czech. Math. J., vol. 23, pp. 298\u2013305, 1973.\n114\n[68] V. D. Blondel, J. M. Hendrickx, A. Olshevsky, and J. N. Tsitsiklis, \u201cConvergence in multiagent coordination,\nconsensus, and \ufb02ocking,\u201d Proc. Joint 44th IEEE Conf. on Decision and Control and European Control Conf.\n(CDC-ECC), pp. 2996-3000, Seville, Spain, Dec. 2005.\n[69] D. S. Scherber and H. C. Papadopoulos, \u201cLocally constructed algorithms for distributed computations in ad-hoc\nnetworks,\u201d Proc. Information Processing in Sensor Networks (IPSN), pp. 11-19, Berkeley, CA, April 2004.\n[70] N. Metropolis, A. W. Rosenbluth, M. N. Rosenbluth, A. H. Teller, and E. Teller, \u201cEquations of state calculations\nby fast computing machines,\u201d Journal of Chemical Physics, vol. 21, no. 6, pp. 1087-1092, 1953.\n[71] W. K. Hastings, \u201cMonte Carlo sampling methods using Markov chains and their applications,\u201d Biometrika, vol.\n57, no. 1, pp. 97-109, 1970.\n[72] A. H. Sayed and C. Lopes, \u201cDistributed recursive least-squares strategies over adaptive networks,\u201d Proc. 40th\nAsilomar Conference on Signals, Systems and Computers, Paci\ufb01c Grove, CA, pp. 233-237, October-November,\n2006.\n[73] J-W. Lee, S-E. Kim, W-J. Song, and A. H. Sayed, \u201cSpatio-temporal di\ufb00usion mechanisms for adaptation over\nnetworks,\u201d Proc. EUSIPCO, pp. 1040-1044, Barcelona, Spain, August-September 2011.\n[74] J-W. Lee, S-E. Kim, W-J. Song, and A. H. Sayed, \u201cSpatio-temporal di\ufb00usion strategies for estimation and\ndetection over networks,\u201d IEEE Trans. Signal Processing, vol. 60, no. 8, pp. 4017\u20134034, August 2012.\n[75] S. Chouvardas, K. Slavakis, and S. Theodoridis, \u201cAdaptive robust distributed learning in di\ufb00usion sensor net-\nworks,\u201d IEEE Trans. on Signal Processing, vol. 59, no. 10, pp. 4692\u20134707, Oct. 2011.\n[76] K. Slavakis, Y. Kopsinis, and S. Theodoridis, \u201cAdaptive algorithm for sparse system identi\ufb01cation using projec-\ntions onto weighted \u21131 balls,\u201d Proc. IEEE ICASSP, pp. 3742\u20133745, Dallas, TX, March 2010.\n[77] T. Kailath, A. H. Sayed, and B. Hassibi, Linear Estimation, Prentice Hall, NJ, 2000.\n[78] F. Cattivelli and A. H. Sayed, \u201cDi\ufb00usion distributed Kalman \ufb01ltering with adaptive weights,\u201d Proc. Asilomar\nConference on Signals, Systems and Computers, pp. 908\u2013912, Paci\ufb01c Grove, CA, November 2009.\n[79] L. Xiao, S. Boyd and S. Lall, \u201cA space-time di\ufb00usion scheme peer-to-peer least-squares-estimation,\u201d Proc.\nInformation Processing in Sensor Networks (IPSN), pp. 168\u2013176, Nashville, TN, April 2006.\n[80] A. Nedic and A. Ozdaglar, \u201cDistributed subgradient methods for multi-agent optimization,\u201d IEEE Trans. Autom.\nControl, vol. 54, no. 1, pp. 48\u201361, Jan. 2009.\n[81] D. P. Bertsekas and J. N. Tsitsiklis, \u201cGradient convergence in gradient methods with errors,\u201d SIAM J. Optim.,\nvol. 10, no. 3, pp. 627\u2013642, 2000.\n[82] S. Barbarossa, and G. Scutari, \u201cBio-inspired sensor network design,\u201d IEEE Signal Processing Magazine, vol. 24,\nno. 3, pp. 26\u201335, May 2007.\n[83] R. Olfati-Saber, \u201cKalman-consensus \ufb01lter: Optimality, stability, and performance,\u201d Proc. IEEE CDC, pp. 7036\u2013\n7042, Shangai, China, 2009.\n[84] I. D. Schizas, G. Mateos, and G. B. Giannakis, \u201cDistributed LMS for consensus-based in-network adaptive\nprocessing,\u201d IEEE Transactions on Signal Processing, vol. 57, no. 6, pp. 2365\u20132382, June 2009.\n[85] G. Mateos, I. D. Schizas, and G. B. Giannakis, \u201cPerformance analysis of the consensus-based distributed LMS\nalgorithm,\u201d EURASIP J. Adv. Signal Process., pp. 1\u201319, 2009.\n[86] S. Kar and J. M. F. Moura, \u201cDistributed consensus algorithms in sensor networks: Link failures and channel\nnoise,\u201d IEEE Trans. Signal Process., vol. 57, no. 1, pp. 355\u2013369, Jan. 2009.\n[87] S. Kar and J. M. F. Moura, \u201cConvergence rate analysis of distributed gossip (linear parameter) estimation:\nFundamental limits and tradeo\ufb00s,\u201d IEEE Journal on Selected Topics on Signal Processing, vol. 5, no. 4, pp. 674\u2013\n690, August 2011.\n[88] A. G. Dimakis, S. Kar, J. M. F. Moura, M. G. Rabbat, and A. Scaglione, \u201cGossip algorithms for distributed\nsignal processing,\u201d Proc. IEEE, vol. 98, no. 11, pp. 1847\u20131864, November 2010.\n[89] S-Y. Tu and A. H. Sayed, \u201cDi\ufb00usion networks outperform consensus networks,\u201d Proc. IEEE Statistical Signal\nProcessing Workshop, pp. 313\u2013316, Ann Arbor, Michigan, August 2012.\n[90] S.-Y. Tu and A. H. Sayed, \u201cDi\ufb00usion strategies outperform consensus strategies for distributed estimation over\nadaptive networks,\u201d IEEE Trans. Signal Processing, vol. 60, no. 12, pp. 6217\u20136234, Dec. 2012.\n115\n",
        "sentence": " There are several forms of diffusion; recent overviews appear in [19]\u2013[21]. In order to find the global saddle-point of the aggregate Lagrangian (33) in a cooperative and stochastic manner, we apply diffusion strategies [19]\u2013[21]. , there exists j > 0 such that all entries of C are strictly positive) [19], [46]. We extend the energy conservation arguments of [16]\u2013[19] to perform a mean-squareerror (MSE) analysis of the diffusion GTD algorithm (35a)\u2013(35d) and provide convergence guarantees under sufficiently small step-sizes. , the clk elements of C) are obtained independently by each node following an averaging rule [19], [50], such that equal weight is given to any member of the neighborhood, including itself (i.",
        "context": "mentations are studied in some detail in [33], along with other di\ufb00usion strategies for smoothing problems\nincluding \ufb01xed-point and \ufb01xed-lag smoothing.\n90\n10.4\nDi\ufb00usion Distributed Optimization\n31\nTable 2: Summary of steepest-descent di\ufb00usion strategies for the distributed optimization of general problems of the\nform (92), and their specialization to the case of mean-square-error (MSE) individual cost functions given by (93).\nAlgorithm\nThe ATC and CTA steepest-descent di\ufb00usion strategies (134) and (142) derived earlier in Sec. 3 provide\ndistributed mechanisms for the solution of global optimization problems of the form:\nmin\nw\nN\nX\nk=1\nJk(w)\n(539)"
    },
    {
        "title": "Diffusion strategies for adaptation and learning over networks",
        "author": [
            "A.H. Sayed",
            "S.-Y. Tu",
            "J. Chen",
            "X. Zhao",
            "Z.J. Towfic"
        ],
        "venue": "IEEE Signal Processing Magazine, vol. 30, no. 3, pp. 155\u2013171, May 2013.",
        "citeRegEx": "20",
        "shortCiteRegEx": null,
        "year": 2013,
        "abstract": "",
        "full_text": "",
        "sentence": " In particular, the analysis in [20]\u2013[22] shows that consensus networks combine local data and in-neighborhood information asymmetrically, which can make the state of consensus networks grow unbounded even when all individual agents are mean stable in isolation.",
        "context": null
    },
    {
        "title": "Adaptive networks",
        "author": [
            "A.H. Sayed"
        ],
        "venue": "Proceedings of the IEEE, vol. 102, no. 4, pp. 460\u2013497, April 2014.",
        "citeRegEx": "21",
        "shortCiteRegEx": null,
        "year": 2014,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " There are several forms of diffusion; recent overviews appear in [19]\u2013[21]. Motivated by recent results on network behavior in [21], [45], we note that, through collaboration, each agent may contribute to the network with its own experience. In order to find the global saddle-point of the aggregate Lagrangian (33) in a cooperative and stochastic manner, we apply diffusion strategies [19]\u2013[21].",
        "context": null
    },
    {
        "title": "Diffusion strategies outperform consensus strategies for distributed estimation over adaptive networks",
        "author": [
            "S.-Y. Tu",
            "A.H. Sayed"
        ],
        "venue": "IEEE Transactions on Signal Processing, vol. 60, no. 12, pp. 6217\u20136234, 2012.",
        "citeRegEx": "22",
        "shortCiteRegEx": null,
        "year": 2012,
        "abstract": "Adaptive networks consist of a collection of nodes with adaptation and\nlearning abilities. The nodes interact with each other on a local level and\ndiffuse information across the network to solve estimation and inference tasks\nin a distributed manner. In this work, we compare the mean-square performance\nof two main strategies for distributed estimation over networks: consensus\nstrategies and diffusion strategies. The analysis in the paper confirms that\nunder constant step-sizes, diffusion strategies allow information to diffuse\nmore thoroughly through the network and this property has a favorable effect on\nthe evolution of the network: diffusion networks are shown to converge faster\nand reach lower mean-square deviation than consensus networks, and their\nmean-square stability is insensitive to the choice of the combination weights.\nIn contrast, and surprisingly, it is shown that consensus networks can become\nunstable even if all the individual nodes are stable and able to solve the\nestimation task on their own. When this occurs, cooperation over the network\nleads to a catastrophic failure of the estimation task. This phenomenon does\nnot occur for diffusion networks: we show that stability of the individual\nnodes always ensures stability of the diffusion network irrespective of the\ncombination topology. Simulation results support the theoretical findings.",
        "full_text": "arXiv:1205.3993v2  [cs.IT]  28 Aug 2012\n1\nDiffusion Strategies Outperform Consensus\nStrategies for Distributed Estimation over\nAdaptive Networks\nSheng-Yuan Tu, Student Member, IEEE and Ali H. Sayed, Fellow, IEEE\nAbstract\nAdaptive networks consist of a collection of nodes with adaptation and learning abilities. The nodes\ninteract with each other on a local level and diffuse information across the network to solve estimation\nand inference tasks in a distributed manner. In this work, we compare the mean-square performance of\ntwo main strategies for distributed estimation over networks: consensus strategies and diffusion strategies.\nThe analysis in the paper con\ufb01rms that under constant step-sizes, diffusion strategies allow information\nto diffuse more thoroughly through the network and this property has a favorable effect on the evolution\nof the network: diffusion networks are shown to converge faster and reach lower mean-square deviation\nthan consensus networks, and their mean-square stability is insensitive to the choice of the combination\nweights. In contrast, and surprisingly, it is shown that consensus networks can become unstable even if\nall the individual nodes are stable and able to solve the estimation task on their own. When this occurs,\ncooperation over the network leads to a catastrophic failure of the estimation task. This phenomenon\ndoes not occur for diffusion networks: we show that stability of the individual nodes always ensures\nstability of the diffusion network irrespective of the combination topology. Simulation results support\nthe theoretical \ufb01ndings.\nIndex Terms\nCopyright (c) 2012 IEEE. Personal use of this material is permitted. However, permission to use this material for any other\npurposes must be obtained from the IEEE by sending a request to pubs-permissions@ieee.org.\nThis work was supported in part by NSF grants CCF-1011918 and CCF-0942936. An earlier version of this work appeared\nin [1]. The authors are with the Department of Electrical Engineering, University of California, Los Angeles (e-mail:\nshinetu@ee.ucla.edu; sayed@ee.ucla.edu).\n2\nAdaptive networks, diffusion strategy, consensus strategy, mean stability, mean-square stability, mean-\nsquare-error performance, combination weights.\nI. INTRODUCTION\nAdaptive networks consist of a collection of spatially distributed nodes that are linked together through\na topology and that cooperate with each other through local interactions. Adaptive networks are well-\nsuited to perform decentralized information processing and inference tasks [2], [3] and to model complex\nand self-organized behavior encountered in biological systems [4], [5].\nWe examine two types of fully decentralized strategies, namely, consensus strategies and diffusion\nstrategies. The consensus strategy was originally proposed in the statistics literature [6] and has since\nthen been developed into an elegant procedure to enforce agreement among cooperating nodes. Average\nconsensus and gossip algorithms have been studied extensively in recent years, especially in the control\nliterature [7]\u2013[12], and applied to the study of multi-agent formations [13], [14], distributed optimization\n[15], [16], and distributed estimation problems [17]\u2013[19]. Original implementations of the consensus\nstrategy relied on the use of two time-scales [20]\u2013[22]: one time-scale for the collection of measurements\nacross the nodes and another time-scale to iterate suf\ufb01ciently enough over the collected data to attain\nagreement before the process is repeated. Unfortunately, two time-scale implementations hinder the ability\nto perform real-time recursive estimation and adaptation when measurement data keep streaming in. For\nthis reason, in this work, we focus instead on consensus implementations that operate in a single time-\nscale. Such implementations appear in several recent works, including [16]\u2013[19], and are largely motivated\nby the procedure developed earlier in [15], [23] for the solution of distributed optimization problems.\nThe second class of algorithms that we consider deals with diffusion strategies, which were originally\nintroduced for the solution of distributed estimation and adaptation problems in [2], [3], [24]\u2013[26]. The\nmain motivation for the introduction of diffusion strategies in these works was the desire to develop\ndistributed schemes that are able to respond in real-time to continuous streaming of data at the nodes by\noperating over a single time-scale. A useful overview of diffusion strategies appears in [27]. Since their\ninception, diffusion strategies have been applied to model various forms of complex behavior encountered\nin nature [4], [5]; they have also been adopted to solve distributed optimization problems advantageously\nin [28]\u2013[30]; and have been studied under varied conditions in [31]\u2013[34] as well. Diffusion strategies\nare inherently single time-scale implementations and are therefore naturally amenable to real-time and\nrecursive implementations. It turns out that the dynamics of the consensus and diffusion strategies differ\nin important ways, which in turn impact the mean-square behavior of the respective networks in a\n3\nfundamental manner.\nThe analysis in this paper will con\ufb01rm that under constant step-sizes, diffusion strategies allow informa-\ntion to diffuse more thoroughly through networks and this property has a favorable effect on the evolution\nof the network. It will be shown that diffusion networks converge faster and reach lower mean-square\ndeviation than consensus networks, and their mean-square stability is insensitive to the choice of the\ncombination weights. In comparison, and surprisingly, it is shown that consensus networks can become\nunstable even if all the individual nodes are stable and able to solve estimation task on their own. In other\nwords, the learning curve of a cooperative consensus network can diverge even if the learning curves for\nthe non-cooperative individual nodes converge. When this occurs, cooperation over the network leads to\na catastrophic failure of the estimation task. This behavior does not occur for diffusion networks: we\nwill show that stability of the individual nodes is suf\ufb01cient to ensure stability of the diffusion network\nregardless of the combination weights. The properties revealed in this paper indicate that there needs\nto be some care with the use of consensus strategies for adaptation because they can lead to network\nfailure even if the individual nodes are stable and well-behaved. The analysis also suggests that diffusion\nstrategies provide a proper way to enforce cooperation over networks; their operation is such that diffusion\nnetworks will always remain stable irrespective of the combination topology.\nII. ESTIMATION STRATEGIES OVER NETWORKS\nConsider a network consisting of N nodes distributed over a spatial domain. Two nodes are said to\nbe neighbors if they can exchange information. The neighborhood of node k is denoted by Nk. The\nnodes in the network would like to estimate an unknown M \u00d7 1 vector, w\u25e6. At every time instant, i,\neach node k is able to observe realizations {dk(i), uk,i} of a scalar random process dk(i) and a 1 \u00d7 M\nvector random process uk,i with a positive-de\ufb01nite covariance matrix, Ru,k = Eu\u2217\nk,iuk,i > 0, where\nE denotes the expectation operator. All vectors in our treatment are column vectors with the exception\nof the regression vector, uk,i, which is taken to be a row vector for convenience of presentation. The\nrandom processes {dk(i), uk,i} are related to w\u25e6via the linear regression model [35]:\ndk(i) = uk,iw\u25e6+ vk(i)\n(1)\nwhere vk(i) is measurement noise with variance \u03c32\nv,k and assumed to be temporally white and spatially\nindependent, i.e.,\nEv\u2217\nk(i)vl(j) = \u03c32\nv,k \u00b7 \u03b4kl \u00b7 \u03b4ij\n(2)\n4\nin terms of the Kronecker delta function. The regression data uk,i are likewise assumed to be temporally\nwhite and spatially independent. The noise vk(i) and the regressors {ul,j} are assumed to be independent\nof each other for all {k, l, i, j}. All random processes are assumed to be zero mean. Note that we use\nboldface letters to denote random quantities and normal letters to denote their realizations or deterministic\nquantities. Models of the form (1) are useful in capturing many situations of interest, such as estimating\nthe parameters of some underlying physical phenomenon, tracking a moving target by a collection of\nnodes, or estimating the location of a nutrient source or predator in biological networks (see, e.g., [4],\n[5], [35]); these models are also useful in the study of the performance limits of combinations of adaptive\n\ufb01lters [36]\u2013[39].\nThe objective of the network is to estimate w\u25e6in a distributed manner through an online learning\nprocess. The nodes estimate w\u25e6by seeking to minimize the following global cost function:\nJglob(w) \u225c\nN\nX\nk=1\nE|dk(i) \u2212uk,iw|2.\n(3)\nIn the sequel, we describe the algorithms pertaining to the consensus and diffusion strategies that we\nstudy in this article, in addition to the non-cooperative mode of operation. Afterwards, we move on to the\nmain theme of this work, which is to show why diffusion networks outperform consensus networks. We\nmay remark that the same strategies can be used to optimize global cost functions where the individual\ncosts are not necessarily quadratic in w as in (3). Most of the mean-square analysis performed here can\nbe extended to this more general scenario \u2014 see, e.g., [30], [40] and the references therein.\nA. Non-Cooperative Strategy\nIn the non-cooperative mode of operation, each node k operates independently of the other nodes and\nestimates w\u25e6by means of a local LMS adaptive \ufb01lter applied to its data {dk(i), uk,i}. The \ufb01lter update\ntakes the following form [35], [41]:\n(non-cooperative strategy)\nwk,i = wk,i\u22121 + \u00b5ku\u2217\nk,i[dk(i) \u2212uk,iwk,i\u22121]\n(4)\nwhere \u00b5k > 0 is the constant step-size used by node k. In (4), the vector wk,i denotes the estimate for\nw\u25e6that is computed by node k at time i. Note that for the underlying model where Ru,k > 0 for all k,\nevery individual node can employ (4) to estimate w\u25e6independently if desired. Studies allowing for other\nobservability conditions for diffusion and consensus strategies, including possibly singular covariance\nmatrices, appear in [18], [42].\n5\nB. Cooperative Strategies\nIn the cooperative mode of operation, nodes interact with their neighbors by sharing information. In\nthis article, we study three cooperative strategies for distributed estimation.\nB.1. Consensus Strategy: The consensus strategy often appears in the literature in the following form\n(see, e.g., Eq. (1.20) in [16], Eq. (19) in [17], and Eq. (9) in [18]):\nwk,i = wk,i\u22121 \u2212\u00b5k(i) \u00b7\nX\nl\u2208Nk\\{k}\nbl,k(wk,i\u22121 \u2212wl,i\u22121) + \u00b5k(i) \u00b7 u\u2217\nk,i[dk(i) \u2212uk,iwk,i\u22121]\n(5)\nwhere {bl,k} is a set of nonnegative coef\ufb01cients. It should be noted that in most works on consensus\nimplementations, especially in the context of distributed optimization problems [16]\u2013[18], [23], [28], the\nstep-sizes {\u00b5k(i)} that are used in (5) depend on the time-index i and are required to satisfy\n\u221e\nX\ni=0\n\u00b5k(i) = \u221eand\n\u221e\nX\ni=0\n\u00b52\nk(i) < \u221e.\n(6)\nIn other words, for each node k, the step-size sequence \u00b5k(i) is required to vanish as i \u2192\u221e. Under\nsuch conditions, it is known that consensus strategies allow the nodes to reach agreement about w\u25e6[16],\n[18], [43], [44]. Here, instead, we will use constant step-sizes {\u00b5k}. This is because we are interested in\nstudying the adaptation and learning abilities of the networks. Constant step-sizes are critical to endow\nnetworks with continuous adaptation and tracking abilities; otherwise, under (6), once the step-sizes have\ndecayed to zero, the network stops adapting and learning is turned off.\nWe can rewrite recursion (5) in a more compact and revealing form by combining the \ufb01rst two terms\non the right-hand side of (5) and by introducing the following coef\ufb01cients:\nal,k \u225c\n\uf8f1\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f2\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f3\n1 \u2212P\nj\u2208Nk\\{k} \u00b5kbj,k,\nif l = k\n\u00b5kbl,k,\nif l \u2208Nk \\ {k}\n0,\notherwise\n(7)\nIn this way, recursion (5) can be rewritten equivalently as (see, e.g., expression (7.1) in [23] and expression\n(1.20) in [16]):\n(consensus strategy)\nwk,i =\nX\nl\u2208Nk\nal,kwl,i\u22121 + \u00b5ku\u2217\nk,i[dk(i) \u2212uk,iwk,i\u22121]\n(8)\nThe entry al,k denotes the weight that node k assigns to the estimate wl,i\u22121 received from its neighbor\nl (see Fig. 1); note that the weights {al,k} are nonnegative for l \u0338= k and that ak,k is nonnegative for\nsuf\ufb01ciently small step-sizes. If we collect the nonnegative weights {al,k} into an N \u00d7 N matrix A, then\n6\nFig. 1.\nA connected network showing the neighborhood of node k, denoted by Nk. The weight al,k scales the data transmitted\nfrom node l to node k over the edge linking them.\nit follows from (7) that the combination matrix A satis\ufb01es the following properties:\nal,k \u22650, AT 1 = 1, and al,k = 0 if l /\u2208Nk\n(9)\nwhere 1 is a vector of size N with all entries equal to one. That is, the weights on the links arriving\nat node k add up to one, which is equivalent to saying that the matrix A is left-stochastic. Moreover, if\ntwo nodes l and k are not linked, then their corresponding entry al,k is zero.\nB.2. ATC Diffusion Strategy: Diffusion strategies for the optimization of (3) in a fully decentralized\nmanner were derived in [2], [3], [24]\u2013[26], [30] by applying a completion-of-squares argument, followed\nby a stochastic approximation step and an incremental approximation step \u2014 see [27]. The adapt-then-\ncombine (ATC) form of the diffusion strategy is described by the following update equations [3]:\n(ATC diffusion strategy)\n\u03c8k,i = wk,i\u22121 + \u00b5ku\u2217\nk,i[dk(i) \u2212uk,iwk,i\u22121]\nwk,i =\nX\nl\u2208Nk\nal,k\u03c8l,i\n.\n(10)\nThe above strategy consists of two steps. The \ufb01rst step of (10) involves local adaptation, where node k\nuses its own data {dk(i), uk,i} to update its weight estimate from wk,i\u22121 to an intermediate value \u03c8k,i.\nThe second step of (10) is a consultation (combination) step where the intermediate estimates {\u03c8l,i} from\nthe neighborhood of node k are combined through weights {al,k} that satisfy (9) to obtain the updated\nweight estimate wk,i.\nB.3. CTA Diffusion Strategy: Another variant of the diffusion strategy is the combine-then-adapt (CTA)\nform, which is described by the following update equations [2]:\n(CTA diffusion strategy)\n\u03c8k,i\u22121 =\nX\nl\u2208Nk\nal,kwl,i\u22121\nwk,i = \u03c8k,i\u22121 + \u00b5ku\u2217\nk,i[dk(i) \u2212uk,i\u03c8k,i\u22121]\n.\n(11)\n7\nThus, comparing the ATC and CTA strategies, we note that the order of the consultation and adaptation\nsteps are simply reversed. The \ufb01rst step of (11) involves a consultation step, where the existing estimates\n{wl,i\u22121} from the neighbors of node k are combined through the weights {al,k}. The second step of\n(11) is a local adaptation step, where node k uses its own data {dk(i), uk,i} to update its weight estimate\nfrom the intermediate value \u03c8k,i\u22121 to wk,i.\nB.4. Comparing Diffusion and Consensus Strategies: For ease of comparison, we rewrite below the\nrecursions that correspond to the consensus (8), ATC diffusion (10), and CTA diffusion (11) strategies\nin a single update:\n(consensus)\nwk,i =\nX\nl\u2208Nk\nal,kwl,i\u22121 + \u00b5ku\u2217\nk,i[dk(i) \u2212uk,iwk,i\u22121]\n(12)\n(ATC diffusion)\nwk,i =\nX\nl\u2208Nk\nal,k\n\u0000wl,i\u22121 + \u00b5lu\u2217\nl,i[dl(i) \u2212ul,iwl,i\u22121]\n\u0001\n(13)\n(CTA diffusion)\nwk,i =\nX\nl\u2208Nk\nal,kwl,i\u22121 + \u00b5ku\u2217\nk,i\n\"\ndk(i) \u2212uk,i\n X\nl\u2208Nk\nal,kwl,i\u22121\n!#\n.\n(14)\nNote that the \ufb01rst terms on the right hand side of these recursions are all the same. For the second terms,\nonly variable wk,i\u22121 appears in the consensus strategy (12), while the diffusion strategies (13)-(14)\nincorporate the estimates {wl,i\u22121} from the neighborhood of node k into the update of wk,i. Moreover,\nin contrast to the consensus (12) and CTA diffusion (14) strategies, the ATC diffusion strategy (13)\nfurther incorporates the in\ufb02uence of the data {dl(i), ul,i} from the neighborhood of node k into the\nupdate of wk,i. These differences in the order by which the computations are performed have important\nimplications on the evolution of the weight-error vectors across consensus and diffusion networks. It is\nimportant to note that the diffusion strategies (13)-(14) are able to incorporate additional information into\ntheir processing steps without being more complex than the consensus strategy. All three strategies have\nthe same computational complexity and require sharing the same amount of data (see Table I), as can\nbe ascertained by comparing the actual implementations (8), (10), and (11). The key fact to note is that\nthe diffusion implementations \ufb01rst generate an intermediate state variable, which is subsequently used in\nthe \ufb01nal update. This important ordering of the calculations has a critical in\ufb02uence on the performance\nof the algorithms, as we now move on to reveal.\nIII. MEAN-SQUARE PERFORMANCE ANALYSIS\nThe mean-square performance of diffusion networks has been studied in detail in [2], [3], [27] by\napplying energy conservation arguments [35], [45]. Following [3], we will \ufb01rst show how to carry out\n8\nTABLE I\nComparison of the number of complex multiplications and additions per iteration, as well as the number of M \u00d7 1 vectors that\nare exchanged for each iteration of the algorithms at every node k. In the table, the symbol nk denotes the degree of node k,\ni.e., the size of its neighborhood Nk. Observe that all three strategies have exactly the same computational complexity.\nATC diffusion (10)\nCTA diffusion (11)\nConsensus (8)\nMultiplications\n(nk + 2)M\n(nk + 2)M\n(nk + 2)M\nAdditions\n(nk + 1)M\n(nk + 1)M\n(nk + 1)M\nVector exchanges\nnk\nnk\nnk\nthe performance analysis in a uni\ufb01ed manner that covers both diffusion and consensus strategies (see\nTable II further ahead, which highlights how the parameters for both strategies differ). Subsequently, we\nuse the resulting performance expressions to carry out detailed comparisons and to establish and highlight\nsome surprising and interesting differences in performance.\nA. Network Error Recursion\nLet the error vector for an arbitrary node k be denoted by\n\u02dcwk,i \u225cw\u25e6\u2212wk,i.\n(15)\nWe collect all error vectors and step-sizes across the network into a block vector and block matrix:\n\u02dcwi \u225ccol { \u02dcw1,i, \u02dcw2,i, \u00b7 \u00b7 \u00b7 , \u02dcwN,i}\n(16)\nM \u225cdiag{\u00b51IM, \u00b52IM, \u00b7 \u00b7 \u00b7 , \u00b5NIM}\n(17)\nwhere the notation col{\u00b7} denotes the vector that is obtained by stacking its arguments on top of each\nother, and the notation diag{\u00b7} constructs a diagonal matrix from its arguments. We further introduce the\nextended combination matrix:\nA \u225cA \u2297IM\n(18)\nwhere the symbol \u2297denotes the Kronecker product of two matrices. This construction replaces each\nentry al,k in A by the M \u00d7 M diagonal matrix al,kIM in A. Then, if we start from (12), (13), or (14),\nand use model (1), some straightforward algebra similar to [3], [27] shows that the global error vector\n\u02dcwi for the various strategies evolves according to the following recursion:\n\u02dcwi = Bi \u00b7 \u02dcwi\u22121 \u2212yi\n(19)\n9\nTABLE II\nThe network weight error vector evolves according to the recursion \u02dcwi = Bi \u00b7 \u02dcwi\u22121 \u2212yi, where the variables {Bi, yi}, and\ntheir respective means or covariances, are listed below for three cooperative strategies and the non-cooperative strategy.\nATC diffusion (10)\nCTA diffusion (11)\nConsensus (8)\nNon-cooperative (4)\nBi\nAT (INM \u2212MRi)\n(INM \u2212MRi)AT\nAT \u2212MRi\nINM \u2212MRi\nB \u225cEBi\nAT (INM \u2212MR)\n(INM \u2212MR)AT\nAT \u2212MR\nINM \u2212MR\nyi\nAT Msi\nMsi\nMsi\nMsi\nY \u225cEyiy\u2217\ni\nAT MSMA\nMSM\nMSM\nMSM\nwhere the quantities Bi and yi are listed in Table II and where Ri is a block diagonal matrix and si is\na block column vector:\nRi \u225cdiag{u\u2217\n1,iu1,i, u\u2217\n2,iu2,i, \u00b7 \u00b7 \u00b7 , u\u2217\nN,iuN,i}\n(20)\nsi \u225ccol{u\u2217\n1,iv1,i, u\u2217\n2,iv2,i, \u00b7 \u00b7 \u00b7 , u\u2217\nN,ivN,i}.\n(21)\nThe coef\ufb01cient matrix Bi is an N \u00d7 N block matrix with blocks of size M \u00d7 M each. Likewise, the\ndriving vector yi is an N \u00d7 1 block vector with entries that are M \u00d7 1 each. The matrix Bi controls\nthe evolution of the network error vector \u02dcwi. It is obvious from Table II that this matrix is different for\neach of the strategies under consideration. We shall verify in the sequel that the differences have critical\nrami\ufb01cations when we compare consensus and diffusion strategies. Note in passing that any of these\nthree distributed strategies degenerates to the non-cooperative strategy (4) when A = IN.\nB. Mean Stability\nWe start our analysis by examining the stability in the mean of the networks, i.e., the stability of\nthe recursion for E \u02dcwi. Thus, note that the matrices {Bi} in Table II are random matrices due to the\nrandomness of the regressors {uk,i} in Ri. In other words, the evolution of the networks is stochastic in\nnature. Now, since the regressors {uk,i} are temporally white and spatially independent, then the {Bi}\nare independent of \u02dcwi\u22121 for any of the strategies. Moreover, since the {uk,i, vk(i)} are independent of\neach other, then the {yi} are zero mean. Taking expectation of both sides of (19), we \ufb01nd that the mean\nof \u02dcwi evolves in time according to the recursion:\nE \u02dcwi = B \u00b7 E \u02dcwi\u22121\n(22)\n10\nwhere B \u225cEBi is shown in Table II and\nR \u225cERi = diag{Ru,1, Ru,2, \u00b7 \u00b7 \u00b7 , Ru,N}.\n(23)\nThe necessary and suf\ufb01cient condition to ensure mean stability of the network (namely, E \u02dcwi \u21920 as\ni \u2192\u221e) is therefore to select step-sizes {\u00b5k} that ensure [3]:\n\u03c1(B) < 1\n(24)\nwhere \u03c1(\u00b7) denotes the spectral radius of its matrix argument. Note that the coef\ufb01cient matrices {B}\nthat control the evolution of E \u02dcwi are different in the cases listed in Table II. These differences lead to\ninteresting conclusions.\nB.1. Comparison of Mean Stability: To begin with, the matrix B is block diagonal in the non-cooperative\ncase and equal to\nBncop = INM \u2212MR.\n(25)\nTherefore, for each of the individual nodes to be stable in the mean, it is necessary and suf\ufb01cient that\nthe step-sizes {\u00b5k} be selected to satisfy\n\u03c1(Bncop) = max\n1\u2264k\u2264N \u03c1(IM \u2212\u00b5kRu,k) < 1\n(26)\nsince the matrices M from (17) and R from (23) are block diagonal. Condition (26) is equivalent to\n(stability in the non-cooperative case)\n0 < \u00b5k <\n2\n\u03bbmax(Ru,k)\nfor k = 1, 2, . . . , N\n(27)\nwhere \u03bbmax(\u00b7) denotes the maximum eigenvalue of its Hermitian matrix argument. Condition (27)\nguarantees that when each node acts individually and applies the LMS recursion (4), then the mean\nof its weight error vector will tend asymptotically to zero. That is, by selecting the step-sizes to satisfy\n(27), all individual nodes will be stable in the mean.\nNow consider the matrix B in the consensus case; it is equal to\nBcons = AT \u2212MR.\n(28)\nIt is seen in this case that the stability of Bcons depends on A. The fact that the stability of the consensus\nstrategy is sensitive to the choice of the combination matrix is known in the consensus literature for\nthe conventional implementation for computing averages and which does not involve streaming data or\ngradient noise [6], [46]. Here, we are studying the more demanding case of the single time-scale consensus\niteration (8) in the presence of both noisy and streaming data. It is clear from (28) that the choice of\nA can destroy the stability of the consensus network even when the step-sizes are chosen according to\n11\n(27) and all nodes are stable on their own. This behavior does not occur for diffusion networks where\nthe matrices {B} for the ATC and CTA diffusion strategies are instead given by\nBatc = AT(INM \u2212MR) and Bcta = (INM \u2212MR)AT .\n(29)\nThe following result clari\ufb01es these statements.\nTheorem 1 (Spectral properties of B). It holds that\n\u03c1(Batc) = \u03c1(Bcta) \u2264\u03c1(Bncop)\n(30)\nirrespective of the choice of the left-stochastic matrices A. Moreover, if the combination matrix A is\nsymmetric, then the eigenvalues of Bcons are less than or equal to the corresponding eigenvalues of\nBncop, i.e.,\n\u03bbl(Bcons) \u2264\u03bbl(Bncop)\nfor l = 1, 2, . . . , NM\n(31)\nwhere the eigenvalues {\u03bbl(\u00b7)} are arranged in decreasing order, i.e., \u03bbl1(\u00b7) \u2265\u03bbl2(\u00b7) if l1 \u2264l2.\nProof: See Appendix A.\nResult (30) establishes the important conclusion that the coef\ufb01cient matrix B for the diffusion strategies\nis stable whenever Bncop (or, from (26), each of the matrices {IM \u2212\u00b5kRu,k}) is stable; this conclusion\nis independent of A. The stability of the matrices {IM \u2212\u00b5kRu,k} is ensured by any step-size satisfying\n(27). Therefore, stability of the individual nodes will always guarantee the stability of B in the ATC and\nCTA diffusion cases, regardless of the choice of A. This is not the case for the consensus strategy (8);\neven when the step-sizes {\u00b5k} are selected to satisfy (27) so that all individual nodes are mean stable, the\nmatrix Bcons can still be unstable depending on the choice of A (and, therefore, on the network topology\nas well). Therefore, if we start from a collection of nodes that are behaving in a stable manner on their\nown, and if we connect them through a topology and then apply consensus to solve the same estimation\nproblem through cooperation, then the network may end up being unstable and the estimation task can\nfail drastically (see Fig. 2 further ahead). Moreover, it is further shown in Appendix A that when A is\nsymmetric, the consensus strategy is mean-stable for step-sizes satisfying:\n0 < \u00b5k < 1 + \u03bbmin(A)\n\u03bbmax(Ru,k)\nfor k = 1, 2, . . . , N.\n(32)\nNote from (9) that since A is a left-stochastic matrix, its spectral radius is equal to one and one of its\neigenvalues is also equal to one [47], i.e., \u03bb1(A) = \u03c1(A) = 1. This implies that the upper bound in (32)\nis less than the upper bound in (27) so that diffusion networks are stable over a wider range of step-sizes.\n12\nActually, the upper bound in (32) can be much smaller than the one in (27) or even zero because \u03bbmin(A)\ncan be negative or equal to \u22121.\nWhat if some of the nodes are unstable in the mean to begin with? How would the behavior of the\ndiffusion and consensus strategies differ? Assume that there is at least one individual unstable node, i.e.,\n\u03bbl(Bncop) \u2264\u22121 for some l so that \u03c1(Bncop) \u22651. Then, we observe from (30) that the spectral radius of\nBatc can still be smaller than one even if \u03c1(Bncop) \u22651. It follows that even if some individual node is\nunstable, the diffusion strategies can still be stable if we properly choose A. In other words, diffusion\ncooperation has a stabilizing effect on the network. In contrast, if there is at least one individual unstable\nnode and the combination matrix A is symmetric, then from (31), no matter how we choose A, the\n\u03c1(Bcons) will be larger than or equal to one and the consensus network will be unstable.\nThe above results suggest that fusing results from neighborhoods according to the consensus strategy\n(8) is not necessarily the best thing to do because it can lead to instability and catastrophic failure. On the\nother hand, fusing the results from neighbors via diffusion ensures stability regardless of the topology.\nB.2. Example: Two-Node Networks: To illustrate these important observations, let us consider an ex-\nample consisting of two cooperating nodes; in this case, it is possible to carry out the calculations\nanalytically in order to highlight the various patterns of behavior. Later, in the simulations section, we\nillustrate the behavior for networks with multiple nodes. Thus, consider a network consisting of N = 2\nnodes. For simplicity, we assume the weight vector w\u25e6is a scalar, and Ru,1 = \u03c32\nu,1 and Ru,2 = \u03c32\nu,2.\nWithout loss of generality, we assume \u00b51\u03c32\nu,1 \u2264\u00b52\u03c32\nu,2. The combination matrix for this example is of\nthe form (Fig. 2):\nAT =\n\uf8ee\n\uf8f01 \u2212a\na\nb\n1 \u2212b\n\uf8f9\n\uf8fb.\n(33)\nwith a, b \u2208[0, 1]. When desired, a symmetric A can be selected by simply setting a = b. Then, using\n(33), we get\nBatc =\n\uf8ee\n\uf8f0(1 \u2212\u00b51\u03c32\nu,1)(1 \u2212a)\n(1 \u2212\u00b52\u03c32\nu,2)a\n(1 \u2212\u00b51\u03c32\nu,1)b\n(1 \u2212\u00b52\u03c32\nu,2)(1 \u2212b)\n\uf8f9\n\uf8fb\n(34)\nBcons =\n\uf8ee\n\uf8f01 \u2212a \u2212\u00b51\u03c32\nu,1\na\nb\n1 \u2212b \u2212\u00b52\u03c32\nu,2\n\uf8f9\n\uf8fb.\n(35)\nWe \ufb01rst assume that\n0 < \u00b51\u03c32\nu,1 \u2264\u00b52\u03c32\nu,2 < 2\n(36)\n13\nso that both individual nodes are stable in the mean by virtue of (27). Then, by Theorem 1, the ATC\ndiffusion network will also be stable in the mean for any choice of the parameters {a, b}. We now verify\nthat there are choices for {a, b} that will turn the consensus network unstable. Speci\ufb01cally, we verify\nbelow that if a and b happen to satisfy\na + b \u22652 \u2212\u00b51\u03c32\nu,1\n(37)\nthen consensus will lead to unstable network behavior even though both individual nodes are stable.\nIndeed, note \ufb01rst that the minimum eigenvalue of Bcons is given by:\n\u03bbmin(Bcons) =\n(2 \u2212a \u2212b \u2212\u00b51\u03c32\nu,1 \u2212\u00b52\u03c32\nu,2) \u2212\n\u221a\nD\n2\n(38)\nwhere\nD \u225c(\u2212a + b \u2212\u00b51\u03c32\nu,1 + \u00b52\u03c32\nu,2)2 + 4ab\n= (a + b + \u00b51\u03c32\nu,1 \u2212\u00b52\u03c32\nu,2)2 + 4b(\u00b52\u03c32\nu,2 \u2212\u00b51\u03c32\nu,1).\n(39)\nFrom the \ufb01rst equality of (39), we know that D \u22650 and, hence, \u03bbmin(Bcons) is real. When (36)-(37) are\nsatis\ufb01ed, we have that (a + b + \u00b51\u03c32\nu,1 \u2212\u00b52\u03c32\nu,2) and 4b(\u00b52\u03c32\nu,2 \u2212\u00b51\u03c32\nu,1) in the second equality of (39)\nare nonnegative. It follows that the consensus network is unstable since\n\u03bbmin(Bcons) \u2264\n(2 \u2212a \u2212b \u2212\u00b51\u03c32\nu,1 \u2212\u00b52\u03c32\nu,2) \u2212(a + b + \u00b51\u03c32\nu,1 \u2212\u00b52\u03c32\nu,2)\n2\n\u2264\u22121.\n(40)\nIn Fig. 2(a), we set \u00b51\u03c32\nu,1 = 0.4 and \u00b52\u03c32\nu,2 = 0.6 so that each individual node is stable. If we now set\na = b = 0.85, then (37) is satis\ufb01ed and the consensus strategy becomes unstable.\nNext, we consider an example satisfying\n0 < \u00b51\u03c32\nu,1 < 2 \u2264\u00b52\u03c32\nu,2\n(41)\nso that node 1 is still stable, whereas node 2 becomes unstable. From the \ufb01rst equality of (39), we again\nconclude that\n\u03bbmin(Bcons) \u2264\n(2 \u2212a \u2212b \u2212\u00b51\u03c32\nu,1 \u2212\u00b52\u03c32\nu,2) \u2212| \u2212a + b \u2212\u00b51\u03c32\nu,1 + \u00b52\u03c32\nu,2|\n2\n=\n\uf8f1\n\uf8f4\n\uf8f2\n\uf8f4\n\uf8f3\n1 \u2212a \u2212\u00b51\u03c32\nu,1,\nif a + \u00b51\u03c32\nu,1 \u2265b + \u00b52\u03c32\nu,2\n1 \u2212b \u2212\u00b52\u03c32\nu,2,\notherwise\n\u2264\u22121.\n(42)\nThat is, in this second case, no matter how we choose the parameters {a, b}, the consensus network is\nalways unstable. In contrast, the diffusion network is able to stabilize the network. To see this, we set\n14\nFig. 2.\nTransient network MSD over time with N = 2. (a) \u00b51\u03c32\nu,1 = 0.4, \u00b52\u03c32\nu,2 = 0.6, and a = b = 0.85. As seen in the\nright plot, the consensus strategy is unstable even when the individual nodes are stable. (b) \u00b51\u03c32\nu,1 = 0.4, \u00b52\u03c32\nu,2 = 2.4, and\na = 1 \u2212b = 0.2 so that node 2 is unstable. As seen in the right plot, the diffusion strategies are able to stabilize the network\neven when the non-cooperative and consensus strategies are unstable.\nb = 1 \u2212a so that the eigenvalues of Batc in (34) are {0, 1 \u2212\u00b51\u03c32\nu,1 \u2212(\u00b52\u03c32\nu,2 \u2212\u00b51\u03c32\nu,1)a}. Some algebra\nshows that the diffusion network is stable if a satis\ufb01es\n0 \u2264a <\n2 \u2212\u00b51\u03c32\nu,1\n\u00b52\u03c32\nu,2 \u2212\u00b51\u03c32\nu,1\n.\n(43)\nIn Fig. 2(b), we set \u00b51\u03c32\nu,1 = 0.4 and \u00b51\u03c32\nu,1 = 2.4 so that node 1 is stable, but node 2 is unstable. If\nwe now set a = 1 \u2212b = 0.2, then (43) is satis\ufb01ed and the diffusion strategies become stable even when\nthe non-cooperative and consensus strategies are unstable.\nC. Mean-Square Stability\nWe now examine the stability in the mean-square sense of the consensus and diffusion strategies. Let\n\u03a3 denote an arbitrary nonnegative-de\ufb01nite matrix that we are free to choose. From (19), we get the\nfollowing weighted variance relation for suf\ufb01ciently small step-sizes:\nE\u2225\u02dcwi\u22252\n\u03a3 \u2248E\u2225\u02dcwi\u22121\u22252\nB\u2217\u03a3B + Tr(\u03a3Y)\n(44)\n15\nwhere the notation \u2225x\u22252\n\u03a3 denotes the weighted square quantity x\u2217\u03a3x and Y \u225cEyiy\u2217\ni appears in Table\nII with the covariance matrix S de\ufb01ned by:\nS \u225cEsis\u2217\ni = diag{\u03c32\nv,1Ru,1, \u03c32\nv,2Ru,2, . . . , \u03c32\nv,NRu,N}.\n(45)\nAs shown in [3], [27], [48], step-sizes that satisfy (24) and are suf\ufb01ciently small will also ensure mean-\nsquare stability of the network (namely, E\u2225\u02dcwi\u22252\n\u03a3 \u2192c < \u221eas i \u2192\u221e). Therefore, we \ufb01nd again that, for\nin\ufb01nitesimally small step-sizes, the mean-square stability of consensus networks is sensitive to the choice\nof A, whereas the mean-square stability of diffusion networks is not affected by A. In the next section, we\nwill examine \u03c1(B) more closely for the various strategies listed in Table II and establish that diffusion\nnetworks are not only more stable than consensus networks but also lead to better mean-square-error\nperformance as well.\nD. Mean-Square Deviation\nThe mean-square deviation (MSD) measure is used to assess how well the nodes in the network\nestimate the weight vector, w\u25e6. The MSD at node k is de\ufb01ned as follows:\nMSDk \u225clim\ni\u2192\u221eE\u2225\u02dcwk,i\u22252\n(46)\nwhere \u2225\u00b7 \u2225denotes the Euclidean norm for vectors. The network MSD is de\ufb01ned as the average MSD\nacross the network, i.e.,\nMSD \u225c1\nN\nN\nX\nk=1\nMSDk.\n(47)\nIterating (44), we can obtain a series expression for the network MSD as:\nMSD = 1\nN\n\u221e\nX\nj=0\nTr[BjY(B\u2217)j] .\n(48)\nWe can also obtain a series expansion for the MSD at each individual node k as follows:\nMSDk =\n\u221e\nX\nj=0\nTr\n\u0002\n(eT\nk \u2297IM) \u00b7 BjYB\u2217j \u00b7 (ek \u2297IM)\n\u0003\n(49)\nwhere ek denotes the kth column of the identity matrix IN. Expressions (49)-(48) relate the MSDs directly\nto the quantities {B, Y} from Table II.\n16\nTABLE III\nVARIABLES FOR COOPERATIVE AND NON-COOPERATIVE IMPLEMENTATIONS WHEN \u00b5k = \u00b5 AND Ru,k = Ru.\nATC diffusion (10)\nCTA diffusion (11)\nConsensus (8)\nNon-cooperative (4)\nB\nAT \u2297IM \u2212AT \u2297\u00b5Ru\nAT \u2297IM \u2212AT \u2297\u00b5Ru\nAT \u2297IM \u2212IN \u2297\u00b5Ru\nIN \u2297IM \u2212IN \u2297\u00b5Ru\n\u03bbl,m(B)\n\u03bbl(A)(1 \u2212\u00b5\u03bbm(Ru))\n\u03bbl(A)(1 \u2212\u00b5\u03bbm(Ru))\n\u03bbl(A) \u2212\u00b5\u03bbm(Ru)\n1 \u2212\u00b5\u03bbm(Ru)\nY\n\u00b52(AT \u03a3vA) \u2297Ru\n\u00b52\u03a3v \u2297Ru\n\u00b52\u03a3v \u2297Ru\n\u00b52\u03a3v \u2297Ru\nsb\u2217\nl,mYsb\nl,m\n\u00b52\u03bbm(Ru)|\u03bbl(A)|2 \u00b7 s\u2217\nl \u03a3vsl\n\u00b52\u03bbm(Ru) \u00b7 s\u2217\nl \u03a3vsl\n\u00b52\u03bbm(Ru) \u00b7 s\u2217\nl \u03a3vsl\n\u00b52\u03bbm(Ru) \u00b7 s\u2217\nl \u03a3vsl\nIV. COMPARISON OF MEAN-SQUARE PERFORMANCE FOR HOMOGENEOUS AGENTS\nIn the previous section, we compared the stability of the various estimation strategies in the mean and\nmean-square senses. In particular, we established that stability of the individual nodes ensures stability\nof diffusion networks irrespective of the combination topology. In the sequel, we shall assume that the\nstep-sizes are suf\ufb01ciently small so that conditions (27) and (32) hold and the diffusion and consensus\nnetworks are stable in the mean and mean-square sense; as well as the individual nodes. Under these\nconditions, the networks achieve steady-state operation. We now use the MSD expressions derived above\nto establish that ATC diffusion achieves lower (and, hence, better) MSD values than the consensus,\nCTA, and non-cooperative strategies. In this way, diffusion strategies do not only ensure stability of the\ncooperative behavior but they also lead to improved mean-square-error performance. We establish these\nresults under the following reasonable condition.\nAssumption 1. All nodes in the network use the same step-size, \u00b5k = \u00b5, and they observe data arising\nfrom the same covariance data so that Ru,k = Ru for all k. In other words, we are dealing with a network\nof homogeneous nodes interacting with each other. In this way, it is possible to quantify the differences\nin performance without biasing the results by differences in the adaptation mechanism (step-sizes) or in\nthe covariance matrices of the regression data at the nodes.\nUnder Assumption 1, it holds that M = \u00b5INM and R = IN \u2297Ru, and thus the matrices B and Y in\nTable II reduce to the expressions shown in Table III, where we introduced the diagonal matrix\n\u03a3v \u225cdiag{\u03c32\nv,1, \u03c32\nv,2, . . . , \u03c32\nv,N} > 0.\n(50)\nNote that the ATC and CTA diffusion strategies now have the same coef\ufb01cient matrix B. We explain in\nthe sequel the terms that appear in the last row of Table III.\n17\nA. Spectral Properties of B\nAs mentioned before, the stability and mean-square-error performance of the various algorithms depend\non the corresponding matrix B; therefore, in this section, we examine more closely the eigen-structure\nof B. For the distributed strategies (diffusion and consensus), the eigen-structure of B will depend on\nthe combination matrix A. Thus, let rl and sl (l = 1, 2, . . . , N) denote an arbitrary pair of right and left\neigenvectors of AT corresponding to the eigenvalue \u03bbl(A). That is,\nAT rl = \u03bbl(A)rl and s\u2217\nl AT = \u03bbl(A)s\u2217\nl .\n(51)\nWe scale the vectors rl and sl to satisfy:\n\u2225rl\u2225= 1 and s\u2217\nl rl = 1 for all l.\n(52)\nRecall that \u03bb1(A) = \u03c1(A) = 1. Furthermore, we let zm (m = 1, 2, . . . , M) denote the eigenvector of the\ncovariance matrix Ru that is associated with the eigenvalue \u03bbm(Ru). That is,\nRuzm = \u03bbm(Ru)zm.\n(53)\nSince Ru is Hermitian and positive-de\ufb01nite, the {zm} are orthonormal, i.e., z\u2217\nm2zm1 = \u03b4m1m2, and the\n{\u03bbm(Ru)} are positive. The following result describes the eigen-structure of the matrix B in terms of\nthe eigen-structures of {AT , Ru} for the diffusion and consensus algorithms of Table III. Note that the\nresults for any of these distributed strategies collapse to the result for the non-cooperative strategy when\nwe set \u03bbl(A) = 1 for all l.\nLemma 1 (Eigen-structure of B under diffusion and consensus). The matrices {B} appearing in Table\nIII for the diffusion and consensus strategies have right and left eigenvectors {rb\nl,m, sb\nl,m} given by:\nrb\nl,m = rl \u2297zm and sb\nl,m = sl \u2297zm\n(54)\nwith the corresponding eigenvalues, \u03bbl,m(B), shown in Table III for l = 1, 2, . . . , N and m = 1, 2, . . . , M.\nNote that while the eigenvectors are the same for the diffusion and consensuses strategies, the corre-\nsponding eigenvalues are different.\nProof: We only consider the diffusion case and denote its coef\ufb01cient matrix by Bdiff = AT \u2297IM \u2212\nAT \u2297\u00b5Ru; the same argument applies to the consensus strategy. We multiply Bdiff by the rb\nl,m de\ufb01ned\n18\nin (54) from the right and obtain\nBdiff \u00b7 rb\nl,m = (AT \u2297IM \u2212AT \u2297\u00b5Ru) \u00b7 (rl \u2297zm)\n= \u03bbl(A) \u00b7 (rl \u2297zm) \u2212\u03bbl(A) \u00b7 \u00b5\u03bbm(Ru) \u00b7 (rl \u2297zm)\n= \u03bbl(A)(1 \u2212\u00b5\u03bbm(Ru)) \u00b7 rb\nl,m\n(55)\nwhere we used the Kronecker product property (A\u2297B)(C \u2297D) = AC \u2297BD for matrices {A, B, C, D}\nof compatible dimensions [35]. In a similar manner, we can verify that Bdiff has left eigenvector sb\nl,m\nde\ufb01ned in (54) with the corresponding eigenvalue \u03bbl,m(B) from Table III.\nTheorem 2 (Spectral radius of B under diffusion and consensus). Under Assumption 1, it holds that\n\u03c1(Bdi\ufb00) = \u03c1(Bncop) \u2264\u03c1(Bcons)\n(56)\nwhere equality holds if A = IN or when the step-size satis\ufb01es:\n0 < \u00b5 \u2264min\nl\u0338=1\n1 \u2212|\u03bbl(A)|\n\u03bbmin(Ru) + \u03bbmax(Ru).\n(57)\nProof: See Appendix B.\nNote that the upper bound in (57) is even smaller than the one in (32) and, therefore, can again be\nvery small or even zero. It follows that there is generally a wide range of step-sizes over which \u03c1(Bcons)\nis greater than \u03c1(Bdi\ufb00). When this happens, the convergence rate of diffusion networks is superior to\nthe convergence rate of consensus networks; in particular, the quantities E \u02dcwi and E\u2225\u02dcwi\u22252 will converge\nfaster towards their steady-state values over diffusion networks than over consensus networks.\nB. Network MSD Performance\nWe now compare the MSD performance. Note that the expressions for the individual MSD in (49) and\nthe network MSD in (48) depend on B in a nontrivial manner. To simplify these MSD expressions, we\nintroduce the following assumption on the combination matrix.\nAssumption 2. The combination matrix A is diagonalizable, i.e., there exists an invertible matrix U and\na diagonal matrix \u039b such that\nAT = U\u039bU \u22121\n(58)\nwith\nU =\nh\nr1\nr2\n\u00b7 \u00b7 \u00b7\nrN\ni\n,\nU \u22121 = col{s\u2217\n1, s\u2217\n2, . . . , s\u2217\nN}\n(59)\n\u039b = diag{\u03bb1(A), \u03bb2(A), . . . , \u03bbN(A)}.\n(60)\n19\nThat is, the columns of U consist of the right eigenvectors of AT and the rows of U \u22121 consist of the left\neigenvectors of AT , as de\ufb01ned by (51).\nNote that, besides condition (52), it follows from Assumption 2 that s\u2217\nl2rl1 = \u03b4l1l2. Furthermore, any\nsymmetric combination matrix A is diagonalizable and therefore satis\ufb01es condition (58) automatically.\nActually, when A is symmetric, more can be said about its eigenvectors. In that case, the matrix U will\nbe orthogonal so that U \u22121 = U T and it will further hold that r\u2217\nl2rl1 = \u03b4l1l2. Assumption 2 allows the\nanalysis to apply to important cases in which A is not necessarily symmetric but is still diagonalizable\n(such as when A is constructed according to the uniform rule by assigning to the links of node k weights\nthat are equal to the inverse of its degree, nk). We can now simplify the MSD expressions by using the\neigen-decomposition of B from Lemma 1 and the above eigen-decomposition of A.\nLemma 2 (MSD expressions). The MSD at node k from (49) can be expressed as:\nMSDk =\nN\nX\nl1=1\nN\nX\nl2=1\nM\nX\nm=1\n(eT\nk rl1) \u00b7 sb\u2217\nl1,mYsb\nl2,m \u00b7 (r\u2217\nl2ek)\n1 \u2212\u03bbl1,m(B)\u03bb\u2217\nl2,m(B)\n.\n(61)\nFurthermore, if the right eigenvectors {rl} of AT are approximately orthonormal, i.e.,\nr\u2217\nl2rl1 \u2248\u03b4l1l2\n(62)\nthen the network MSD from (48) can be approximated by:\nMSD \u2248\nN\nX\nl=1\nM\nX\nm=1\nsb\u2217\nl,mYsb\nl,m\nN \u00b7 (1 \u2212|\u03bbl,m(B)|2).\n(63)\nProof: See Appendix C.\nNote that any symmetric combination matrix A satis\ufb01es condition (62) since, as mentioned above, its\nright eigenvectors can be chosen to be orthonormal.\nUsing the expressions for \u03bbl,m(B) and sb\u2217\nl,mYsb\nl,m from Table III and substituting into (63), we can\nobtain the network MSD expressions for the various strategies. The following result shows how these\nMSD values compare to each other.\nTheorem 3 (Comparing network MSDs). If condition (62) is satis\ufb01ed, then the ATC diffusion strategy\nachieves the lowest network MSD in comparison to the other strategies (CTA diffusion, consensus, and\nnon-cooperative). More speci\ufb01cally, it holds that\nMSDatc \u2264MSDcta \u2264MSDncop\n(64)\nMSDatc \u2264MSDcons.\n(65)\n20\nFig. 3.\nNetwork MSD comparison with N = 2 and \u00b5\u03c32\nu = 0.4. The consensus strategy is unstable when the parameters a\nand b lie above the dashed line in region I.\nFurthermore, if 1 \u2264\u00b5\u03bbmin(Ru) < 2, the consensus strategy is the worst even in comparison to the\nnon-cooperative strategy:\nMSDatc \u2264MSDcta \u2264MSDncop \u2264MSDcons.\n(66)\nProof: See Appendix D.\nTherefore, the ATC diffusion strategy outperforms consensus, CTA diffusion, and non-cooperative\nstrategies when condition (62) is satis\ufb01ed. However, the relation among MSDcta, MSDcons, and MSDncop\ndepends on the combination matrix A. To illustrate this fact, we reconsider the two-node network from\nSection III.B with \u03c32\nu,1 = \u03c32\nu,2 = \u03c32\nu, \u00b51 = \u00b52 = \u00b5, and 0 < \u00b5\u03c32\nu < 1. Furthermore, to ensure the\nstability of the consensus strategy and from (37), the parameters {a, b} in (33) are now chosen to satisfy\na + b < 2 \u2212\u00b5\u03c32\nu. In this case, the eigenvalues of the combination matrix A in (33) are {1, 1 \u2212a \u2212b}.\nIt can be veri\ufb01ed from (63) and Table III that the CTA diffusion strategy achieves lower network MSD\n(better mean-square performance) than the consensus strategy if\n\uf8f1\n\uf8f4\n\uf8f2\n\uf8f4\n\uf8f3\nMSDcons \u2264MSDcta,\nif 0 \u2264a + b \u22642(1\u2212\u00b5\u03c32\nu)\n2\u2212\u00b5\u03c32\nu\nMSDcons \u2265MSDcta,\nif 2(1\u2212\u00b5\u03c32\nu)\n2\u2212\u00b5\u03c32\nu\n\u2264a + b < 2 \u2212\u00b5\u03c32\nu\n(67)\nSimilarly, the network MSDs of the consensus and non-cooperative strategies have the following relation:\n\uf8f1\n\uf8f4\n\uf8f2\n\uf8f4\n\uf8f3\nMSDcons \u2264MSDncop,\nif 0 \u2264a + b \u22642(1 \u2212\u00b5\u03c32\nu)\nMSDcons \u2265MSDncop,\nif 2(1 \u2212\u00b5\u03c32\nu) \u2264a + b < 2 \u2212\u00b5\u03c32\nu\n(68)\nCombining (67)-(68), we can divide the a \u00d7 b plane into three regions, as shown in Fig. 3, where each\nregion corresponds to one possible relation among MSDcta, MSDcons, and MSDncop.\n21\nC. MSD of Individual Nodes\nIn Theorem 3, we established that the ATC diffusion strategy performs the best in terms of the average\nnetwork MSD. It is still not clear how well the individual nodes perform under each strategy. It is\ngenerally more challenging to compare diffusion and consensus strategies in terms of the MSDs of their\nindividual nodes due to the structure of the matrix B for the consensus strategy. Nevertheless, this can\nbe accomplished as follows. We observe from (61) and Table III that the {MSDk} for the CTA diffusion\nand consensus strategies differ only in the value of \u03bbl,m(B). From Table III, the difference between the\nvalues of \u03bbl,m(B) for these two strategies is\n\u03bbl,m(Bcta) \u2212\u03bbl,m(Bcons) = \u00b5\u03bbm(Ru) \u00b7 (1 \u2212\u03bbl(A)) = O(\u00b5)\n(69)\nwhere the term O(\u00b5) denotes a factor that is of the order of the step-size \u00b5. It follows that for suf\ufb01ciently\nsmall step-sizes, expression (69) is close to zero and the CTA diffusion and consensus strategies will\nexhibit similar MSDs at the individual nodes, i.e., MSDcta,k \u2248MSDcons,k for all k. As a result, in the\nfollowing, we only compare MSDatc,k, MSDcta,k, and MSDncop,k. In particular, we will show that under\ncertain conditions on the combination matrix A, the ATC diffusion strategy continues to perform the best\nin terms of the MSD at the individual nodes in comparison to the other strategies. To do so, starting\nfrom (61) and the expressions for {\u03bbl,k(B), Y} in Table III, we can express the MSD at node k for the\nATC diffusion strategy as:\nMSDatc,k =\nM\nX\nm=1\n\u00b52\u03bbm(Ru)\nN\nX\nl1,l2=1\n\u03bbl1(A)\u03bb\u2217\nl2(A) \u00b7 (eT\nk rl1s\u2217\nl1\u03a3vsl2r\u2217\nl2ek)\n1 \u2212\u03bbl1(A)\u03bb\u2217\nl2(A) \u00b7 (1 \u2212\u00b5\u03bbm(Ru))2 \u225c\nM\nX\nm=1\nMSDatc,k(m)\n(70)\nwhere we introduced the notation MSDatc,k(m) to denote the MSD component at node k that is contributed\nby the mth eigenvalue of Ru, i.e.,\nMSDatc,k(m) = \u00b52\u03bbm(Ru)\nN\nX\nl1,l2=1\n\u03bbl1(A)\u03bb\u2217\nl2(A) \u00b7 (eT\nk rl1s\u2217\nl1\u03a3vsl2r\u2217\nl2ek)\n1 \u2212\u03bbl1(A)\u03bb\u2217\nl2(A) \u00b7 (1 \u2212\u00b5\u03bbm(Ru))2 .\n(71)\nIn a similar vein, we can de\ufb01ne the corresponding MSDk(m) terms for the other strategies. We list these\nterms in Table IV in two equivalent forms (we will use the series form later). We \ufb01rst have the following\nuseful preliminary result.\nLemma 3 (Useful comparisons). The following ratios are positive and independent of the node index k:\nMSDncop,k(m) \u2212MSDatc,k(m)\nMSDncop,k(m) \u2212MSDcta,k(m) =\n1\n(1 \u2212\u00b5\u03bbm(Ru))2 > 0\n(72)\nMSDncop,k(m) \u2212MSDatc,k(m)\nMSDcta,k(m) \u2212MSDatc,k(m) =\n1\n1 \u2212(1 \u2212\u00b5\u03bbm(Ru))2 > 0.\n(73)\n22\nTABLE IV\nEXPRESSIONS FOR MSDk(m) IN SERIES FORM AND EIGEN-FORM.\nATC Diffusion\n(10)\nSeries form\n\u00b52\u03bbm(Ru) P\u221e\nj=0(1 \u2212\u00b5\u03bbm(Ru))2j \u00b7 eT\nk AT(j+1)\u03a3vAj+1ek\nEigen-form\n\u00b52\u03bbm(Ru) PN\nl1,l2=1\n\u03bbl1(A)\u03bb\u2217\nl2(A)\u00b7(eT\nk rl1s\u2217\nl1\u03a3vsl2r\u2217\nl2ek)\n1\u2212\u03bbl1(A)\u03bb\u2217\nl2(A)\u00b7(1\u2212\u00b5\u03bbm(Ru))2\nCTA Diffusion\n(11)\nSeries form\n\u00b52\u03bbm(Ru) P\u221e\nj=0(1 \u2212\u00b5\u03bbm(Ru))2j \u00b7 eT\nk ATj\u03a3vAjek\nEigen-form\n\u00b52\u03bbm(Ru) PN\nl1,l2=1\neT\nk rl1s\u2217\nl1\u03a3vsl2r\u2217\nl2ek\n1\u2212\u03bbl1(A)\u03bb\u2217\nl2(A)\u00b7(1\u2212\u00b5\u03bbm(Ru))2\nNon-\ncooperative\n(4)\nSeries form\n\u00b52\u03bbm(Ru) P\u221e\nj=0(1 \u2212\u00b5\u03bbm(Ru))2j \u00b7 eT\nk \u03a3vek\nEigen-form\n\u00b52\u03bbm(Ru) PN\nl1,l2=1\neT\nk rl1s\u2217\nl1\u03a3vsl2r\u2217\nl2ek\n1\u2212(1\u2212\u00b5\u03bbm(Ru))2\nProof: From the eigen-forms of {MSDk(m)} in Table IV, the differences between MSDatc,k(m),\nMSDcta,k(m), and MSDncop,k(m) are given by:\nMSDncop,k(m) \u2212MSDatc,k(m) =\n\u00b52\u03bbm(Ru)\n1 \u2212(1 \u2212\u00b5\u03bbm(Ru))2 \u00b7 ck(m)\n(74)\nMSDncop,k(m) \u2212MSDcta,k(m) = \u00b52\u03bbm(Ru) \u00b7 (1 \u2212\u00b5\u03bbm(Ru))2\n1 \u2212(1 \u2212\u00b5\u03bbm(Ru))2\n\u00b7 ck(m)\n(75)\nMSDcta,k(m) \u2212MSDatc,k(m) = \u00b52\u03bbm(Ru) \u00b7 ck(m)\n(76)\nwhere\nck(m) =\nN\nX\nl1,l2=1\n\u0002\n1 \u2212\u03bbl1(A)\u03bb\u2217\nl2(A)\n\u0003\n\u00b7 (eT\nk rl1s\u2217\nl1\u03a3vsl2r\u2217\nl2ek)\n1 \u2212\u03bbl1(A)\u03bb\u2217\nl2(A) \u00b7 (1 \u2212\u00b5\u03bbm(Ru))2\n.\n(77)\nThen, dividing (74) by (75) and (74) by (76), we arrive at (72)-(73).\nLemma 4 (Useful ordering). The relation among MSDatc,k(m), MSDcta,k(m), and MSDncop,k(m) is\neither\nMSDatc,k(m) \u2264MSDcta,k(m) \u2264MSDncop,k(m)\n(78)\nor\nMSDatc,k(m) \u2265MSDcta,k(m) \u2265MSDncop,k(m).\n(79)\nProof: Assume \ufb01rst that MSDatc,k(m) \u2264MSDncop,k(m). Then, using (72), we get MSDncop,k(m)\u2212\nMSDcta,k(m) \u22650. Similarly, from (73), we get MSDcta,k(m) \u2212MSDatc,k(m) \u22650. We conclude that\n23\nrelation (78) holds in this case. Assume instead that MSDatc,k(m) \u2265MSDncop,k(m). Then, a similar\nargument will show that (79) should hold.\nThe above result is useful since it allows us to deduce the relation among MSDatc,k(m), MSDcta,k(m),\nand MSDncop,k(m) by only knowing the relation between any two of them. To proceed, we note that\nwe can alternatively express the MSDk(m) terms in an equivalent series form. For example, expression\n(71) can be written as:\nMSDatc,k(m) = \u00b52\u03bbm(Ru)\n\u221e\nX\nj=0\nN\nX\nl1,l2=1\n(1 \u2212\u00b5\u03bbm(Ru))2j \u00b7 \u03bbj+1\nl1\n(A) \u00b7 \u03bb\u2217(j+1)\nl2\n(A) \u00b7 (eT\nk rl1s\u2217\nl1\u03a3vsl2r\u2217\nl2ek)\n= \u00b52\u03bbm(Ru)\n\u221e\nX\nj=0\n(1 \u2212\u00b5\u03bbm(Ru))2j \u00b7 eT\nk\n N\nX\nl1=1\n\u03bbj+1\nl1\n(A)rl1s\u2217\nl1\n!\n\u03a3v\n N\nX\nl2=1\n\u03bb\u2217(j+1)\nl2\n(A)sl2r\u2217\nl2\n!\nek\n= \u00b52\u03bbm(Ru)\n\u221e\nX\nj=0\n(1 \u2212\u00b5\u03bbm(Ru))2j \u00b7 eT\nk AT(j+1)\u03a3vAj+1ek.\n(80)\nIn a similar manner, we can obtain the corresponding MSDk(m) series forms for the other strategies and\nwe list these in Table IV. In the following, we provide conditions to guarantee that the individual node\nperformance in the ATC diffusion strategy outperforms the other strategies.\nTheorem 4 (Comparing individual MSDs). If the combination matrix A satis\ufb01es\n\u03a3v \u2212AT \u03a3vA \u22650\n(81)\nwhere \u03a3v is the noise variance (diagonal) matrix de\ufb01ned by (50), then:\nMSDatc,k \u2264MSDcta,k \u2264MSDncop,k.\n(82)\nProof: From the series forms of {MSDk(m)} in Table IV, the difference MSDcta,k(m)\u2212MSDatc,k(m)\nis given by:\nMSDcta,k(m) \u2212MSDatc,k(m) = \u00b52\u03bbm(Ru)\n\u221e\nX\nj=0\n(1 \u2212\u00b5\u03bbm(Ru))2jeT\nk ATj \u0000\u03a3v \u2212AT \u03a3vA\u0001\nAjek.\n(83)\nSince \u03a3v \u2212AT \u03a3vA \u22650, we conclude that MSDcta,k(m) \u2265MSDatc,k(m) for all m. Then, applying\nLemma 4, we obtain relation (82).\nCondition (81) essentially means that the combination matrix A should not magnify the noise effect\nacross the network. However, in general, condition (81) is restrictive in the sense that over the set of\nfeasible diagonalizable left-stochastic matrices A satisfying al,k = 0 if l /\u2208Nk, the set of combination\nmatrices A satisfying (81) can be small. We illustrate this situation by reconsidering the two-node network\n24\n(33) for which\n\u03a3v \u2212AT \u03a3vA =\n\uf8ee\n\uf8f0\n2at \u2212a2(1 + t)\n\u2212(1 \u2212a)bt \u2212a(1 \u2212b)\n\u2212(1 \u2212a)bt \u2212a(1 \u2212b)\n2b \u2212b2(1 + t)\n\uf8f9\n\uf8fb\n(84)\nwhere t = \u03c32\nv,1/\u03c32\nv,2 denotes the ratio of noise variances at nodes 1 and 2. Note from\ndet(\u03a3v \u2212AT \u03a3vA) = \u2212(a \u2212bt)2 \u22640\n(85)\nthat equality holds in (85) if, and only if,\na = tb.\n(86)\nThat is, when a \u0338= tb, the matrix (\u03a3v \u2212AT\u03a3vA) has two eigenvalues with different signs. Thus, the only\nway to ensure \u03a3v \u2212AT \u03a3vA \u22650 in this case is to set a = tb and, thus, the matrix (\u03a3v \u2212AT \u03a3vA) will\nhave at least one eigenvalue at zero since its determinant will be zero. To ensure \u03a3v \u2212AT \u03a3vA \u22650, its\nother eigenvalue, which is equal to b(1 + t2)(2 \u2212b \u2212bt), needs to be greater than or equal to zero. It\nfollows that b must satisfy:\n0 \u2264b \u2264\n2\n1 + t.\n(87)\nMoreover, since a and b must lie within the interval [0, 1], we conclude from (86) that b must also satisfy:\n0 \u2264b \u2264min{1, 1/t}.\n(88)\nIt can be veri\ufb01ed that condition (88) implies condition (87) since min{1, 1/t} \u22642/(1 + t). That is, for\nany left-stochastic matrix A from (33) satisfying a = tb and (88), relation (82) holds and both nodes\nimprove their own MSDs by employing the diffusion strategies. Note that condition (86) represents a\nline segment in the unit square a, b \u2208[0, 1] (see Fig. 4). In the following, we relax condition (81) with\na mild constraint on the network topology.\nIn addition to Assumption 2, we further assume that the combination matrix A is primitive (also called\nregular). This means that there exists an integer j such that the jth power of A has positive entries,\n[Aj]l,k > 0 for all l and k [47]. We remark that for any connected network (where a path always exists\nbetween any two arbitrary nodes), if the combination weights {al,k} satisfy al,k > 0 for l \u2208Nk, then A\nis primitive. Now, since A is primitive, it follows from the Perron-Frobenius Theorem [47] that (AT )j\nconverges to the rank-one matrix:\nlim\nj\u2192\u221e(AT )j = r1sT\n1 .\n(89)\nFrom (9) and (52), r1 and s1 satisfy:\nr1 =\n1\n\u221a\nN\nand sT\n1 1\n\u221a\nN\n= 1.\n(90)\n25\nFig. 4.\nComparison of individual node MSD using N = 2 and t = \u03c32\nv,1/\u03c32\nv,2. There exists a step-size region such that\nMSDatc,k < MSDcta,k < MSDncop,k for k = 1, 2 when the parameters a and b lie in the shaded regions. The dashed lines\nindicate condition (86).\nTheorem 5 (Comparing individual MSDs for regular networks). For any primitive and diagonalizable\ncombination matrix A, if\nsT\n1 \u03a3vs1\nN\n< \u03c32\nv,k\n(91)\nfor all k, then there exists \u00b5\u25e6> 0 so that for any step-size \u00b5 satisfying 0 < \u00b5 \u2264\u00b5\u25e6, it holds:\nMSDatc,k < MSDcta,k < MSDncop,k.\n(92)\nProof: See Appendix E.\nWe show in Appendix F that for any primitive A, condition (81) implies condition (91). To illustrate\nthese two conditions, we consider again the two-node network. It can be veri\ufb01ed that sT\n1 for AT in (33)\nhas the form sT\n1 =\nh\u221a\n2b/(a + b)\n\u221a\n2a/(a + b)\ni\n. Then, some algebra shows that condition (91) becomes\n(t \u22121)a + 2bt > 0 and 2a + (1 \u2212t)b > 0.\n(93)\nRecall that t = \u03c32\nv,1/\u03c32\nv,2. We illustrate condition (93), along with condition (86), in Fig. 4. We observe\nthat condition (86), shown as the dashed lines, is contained in condition (93), shown as the shaded\nregions, and that compared to condition (86), condition (93) enlarges the region of A for which the ATC\ndiffusion strategy performs the best in terms of the individual MSD performance.\nV. SIMULATION RESULTS\nWe consider a network with 20 nodes and random topology. The regression covariance matrix Ru\nis diagonal with entries randomly generated from [2, 4], and the noise variances {\u03c32\nv,k} are randomly\n26\nFig. 5.\nNetwork topology and noise and data power pro\ufb01les at the nodes. The number next to a node denotes the node index.\nTABLE V\nCOMBINATION RULES USED IN THE SIMULATIONS, al,k = 0 IF l /\u2208Nk\nName\nRule\nRelative-variance [48]\nal,k = \u03c3\u22122\nv,l / P\nj\u2208Nk \u03c3\u22122\nv,j\nUniform [27]\nal,k = 1/nk\nMetropolis [49]\nal,k =\n\uf8f1\n\uf8f4\n\uf8f2\n\uf8f4\n\uf8f3\n1 \u2212P\nj\u0338=k ak,j,\nif l = k\n1/ max{nk, nl},\nif l \u2208Nk \\ {k}\ngenerated over [\u221230, \u221210] dB (see Fig. 5). The network estimates a 10 \u00d7 1 (i.e., M = 10) unknown\nvector w\u25e6with every entry equal to 1/\n\u221a\n10.\nThe transient network MSD over time is shown on the left hand side of Fig. 6 with three possible\ncombination rules: relative-variance [48], uniform [27], and Metropolis [49] (see Table V). Note that the\nmatrix A for the Metropolis rule is symmetric. The step-size \u00b5 is set to \u00b5 = 0.02. We observe that, as\nexpected, the ATC diffusion strategy outperforms the other strategies, especially for the relative-variance\nrule. It also suggests that some conventional choices of combination weights, such as the Metropolis rule,\nmay not be the most suitable for adaptation in the presence of both noisy and streaming data because\nsuch weights do not take into account the noise pro\ufb01le across the nodes (see, e.g., [27], [48] for more\ndetails on this issue). We further show the steady-state MSD at the individual nodes on the right hand\nside of Fig. 6. We observe that the ATC diffusion strategy achieves the lowest MSD at each node in\ncomparison to the other strategies. These observations are in agreement with the results predicted by the\n27\nFig. 6.\nTransient network MSD over time (left, with peak values normalized to 0dB) and steady-state MSD at the individual\nnodes (right) for (a)-(b) the relative-variance, (c)-(d) uniform, and (e)-(f) Metropolis rules. The dashed lines on the left/right\nhand side indicate the theoretical network/individual MSD from (63)/(61) for the ATC diffusion strategy.\ntheoretical analysis. The theoretical expressions for MSDs from (49)-(48) are also depicted in Fig. 6 for\nthe ATC diffusion strategy and match well with simulations.\nWe further compare the mean-square performance of the distributed strategies for larger step-sizes. We\nset the step-size to \u00b5 = 0.075 and use the relative-variance combination rule. The transient network MSD\nover time is shown on the left hand side of Fig. 7. We observe that the ATC and CTA diffusion strategies\nhave the same convergence rate and converge faster than the consensus strategy. Moreover, the diffusion\nstrategies achieve lower network MSD than the consensus strategy. We also show the steady-state MSD\n28\nFig. 7.\nTransient network MSD over time (left) and steady-state MSD at the individual nodes (right) for the relative-variance\ncombination rule using \u00b5 = 0.075.\nat the individual nodes on the right hand side of Fig. 7. We see again that ATC diffusion performs the\nbest in comparison to the other strategies at each individual node.\nVI. CONCLUDING REMARKS\nWe compared analytically several cooperative estimation strategies, including ATC diffusion, CTA\ndiffusion, and consensus for distributed estimation over networks. The results show that diffusion networks\nare more stable than consensus networks. Moreover, the stability of diffusion networks is independent of\nthe combination weights, whereas consensus networks can become unstable even if all individual nodes\nare stable. Furthermore, in steady-state, the ATC diffusion strategy performs the best not only in terms\nof the network MSD, but also in terms of the MSDs at the individual nodes.\nAPPENDIX A\nPROOF OF THEOREM 1\nFirst, note that the matrices {B} for the ATC and CTA diffusion strategies given by (29) have the same\neigenvalues (and, therefore, the same spectral radius) because for any matrices X and Y of compatible\ndimensions, the matrix products XY and Y X have the same eigenvalues [47]. So let us evaluate the\nspectral radius of Batc. To do so, we introduce a convenient block matrix norm, and denote it by \u2225\u00b7 \u2225b;\nit is de\ufb01ned as follows. Let X be an N \u00d7 N block matrix with blocks of size M \u00d7 M each. Its block\nmatrix norm is de\ufb01ned as:\n\u2225X\u2225b \u225cmax\n1\u2264k\u2264N\n N\nX\nl=1\n\u2225Xk,l\u22252\n!\n(94)\nwhere Xk,l denotes the (k, l)th block of X and \u2225\u00b7 \u22252 denotes the 2-induced norm (largest singular value)\nof its matrix argument. Now, since {INM, M, R} are block diagonal matrices, the following property\n29\nholds:\n\u2225INM \u2212MR\u2225b = max\n1\u2264k\u2264N \u2225IM \u2212\u00b5kRu,k\u22252 = max\n1\u2264k\u2264N \u03c1(IM \u2212\u00b5kRu,k) = \u03c1(Bncop)\n(95)\nwhere we used the fact that the 2-induced norm of any Hermitian matrix coincides with its spectral\nradius. In addition, since A is a left-stochastic matrix, it holds that\n\u2225AT \u2225b = max\n1\u2264k\u2264N\n N\nX\nl=1\n\u2225al,kIM\u22252\n!\n= max\n1\u2264k\u2264N\n N\nX\nl=1\nal,k\n!\n= 1.\n(96)\nAccordingly, using the fact that the spectral radius of a matrix is upper bounded by any norm of the\nmatrix [47], we get:\n\u03c1(Batc) \u2264\u2225AT (INM \u2212MR)\u2225b \u2264\u2225AT \u2225b \u00b7 \u2225INM \u2212MR\u2225b = \u03c1(Bncop)\n(97)\nwhich establishes (30).\nNow, assume A is symmetric. Since it is also left-stochastic, it follows that its eigenvalues are real\nand lie inside the interval [\u22121, 1]. Therefore, (INM \u2212AT ) is nonnegative-de\ufb01nite. Moreover, since M\nand R commute, i.e., RM = MR, it can be veri\ufb01ed that Bcons in (28) and Bncop in (25) are Hermitian.\nIn addition, the matrices Bcons and Bncop are related as follows:\nBncop = Bcons + (INM \u2212AT)\n(98)\nwith (INM \u2212AT ) \u22650. Using Weyl\u2019s Theorem1 [47], we arrive at (31). Following a similar argument, it\nholds for symmetric A that\n\u03bbl {\u03bbmin(A) \u00b7 INM \u2212MR} \u2264\u03bbl(Bcons)\nfor l = 1, 2, . . . , NM.\n(99)\nThus, the matrix Bcons is stable (namely, \u22121 < \u03bbl(Bcons) < 1 for l = 1, 2, . . . , NM) if\n\u03bbl (\u03bbmin(A) \u00b7 INM \u2212MR) > \u22121\n(100)\n\u03bbl(Bncop) < 1\n(101)\nfor l = 1, 2, . . . , NM, or, equivalently,\n\u03bbmin(A) \u2212\u00b5k\u03bbm(Ru,k) > \u22121\n(102)\n1 \u2212\u00b5k\u03bbm(Ru,k) < 1\n(103)\n1Let {D\u2032, D, \u2206D} be M \u00d7 M Hermitian matrices with ordered eigenvalues {\u03bbm(D\u2032), \u03bbm(D), \u03bbm(\u2206D)}, i.e., \u03bb1(D) \u2265\n\u03bb2(D) \u2265. . . \u2265\u03bbM(D), and likewise for the eigenvalues of {D\u2032, \u2206D}. Weyl\u2019s Theorem states that if D\u2032 = D + \u2206D, then\n\u03bbm(D) + \u03bbM(\u2206D) \u2264\u03bbm(D\u2032) \u2264\u03bbm(D) + \u03bb1(\u2206D)\nfor 1 \u2264m \u2264M. When \u2206D \u22650, it holds that \u03bbm(D\u2032) \u2265\u03bbm(D).\n30\nfor k = 1, 2, . . . , N and m = 1, 2, . . . , M. We then arrive at (32).\nAPPENDIX B\nPROOF OF THEOREM 2\nFor the diffusion strategies, from Table III and since \u03c1(A) = 1, we have\n\u03c1(Bdiff) = \u03c1[AT \u2297(IM \u2212\u00b5Ru)] = \u03c1(A) \u00b7 \u03c1(IM \u2212\u00b5Ru) = \u03c1(IM \u2212\u00b5Ru) = \u03c1(Bncop).\n(104)\nMoreover, since 1 \u2208{\u03bbl(A)}, we have\n\u03c1(Bncop) =\nmax\n1\u2264m\u2264M |1 \u2212\u00b5\u03bbm(Ru)| \u2264max\n1\u2264l\u2264N\nmax\n1\u2264m\u2264M |\u03bbl(A) \u2212\u00b5\u03bbm(Ru)| = \u03c1(Bcons)\n(105)\nand we arrive at (56). It is obvious that when A = IN, then equality in (105) holds and \u03c1(Bncop) =\n\u03c1(Bcons). We now consider the case when A \u0338= IN. Note that the spectral radius of Bncop is given by\n\u03c1(Bncop) = max{1 \u2212\u00b5\u03bbmin(Ru), \u22121 + \u00b5\u03bbmax(Ru)}.\n(106)\nWe \ufb01rst verify that equality in (105) holds only when \u03c1(Bncop) = 1 \u2212\u00b5\u03bbmin(Ru). Indeed, if \u03c1(Bncop) =\n\u22121 + \u00b5\u03bbmax(Ru) \u22650, we have that \u00b5\u03bbmax(Ru) \u22651 and we get from (105) that\n\u03c1(Bcons) = max\n1\u2264l\u2264N\nmax\n1\u2264m\u2264M |\u03bbl(A) \u2212\u00b5\u03bbm(Ru)|\n\u2265|\u03bbl(A) \u2212\u00b5\u03bbmax(Ru)|\n\u2265|Re{\u03bbl(A)} \u2212\u00b5\u03bbmax(Ru)|\n= \u2212Re{\u03bbl(A)} + \u00b5\u03bbmax(Ru)\n(107)\nsince Re{\u03bbl(A)} \u22641 where Re{\u00b7} denotes the real part of its argument. Since A \u0338= IN, there exists\nsome l such that Re{\u03bbl(A)} < 1 and then \u03c1(Bcons) > \u22121 + \u00b5\u03bbmax(Ru) = \u03c1(Bncop). Now, assume that\n\u03c1(Bncop) = 1 \u2212\u00b5\u03bbmin(Ru). Then, equality in (105) holds if\n|\u03bbl(A) \u2212\u00b5\u03bbm(Ru)| \u2264\u03c1(Bncop)\n(108)\nfor all l and m. It is obvious that relation (108) holds for l = 1 since \u03bb1(A) = 1 and\n\u03c1(Bncop) =\nmax\n1\u2264m\u2264M |1 \u2212\u00b5\u03bbm(Ru)|\n\u2265|\u03bb1(A) \u2212\u00b5\u03bbm(Ru)|.\n(109)\nFor l = 2, 3, . . . , N, by the triangular inequality of norms, we have that |\u03bbl(A) \u2212\u00b5\u03bbm(Ru)| \u2264|\u03bbl(A)| +\n\u00b5\u03bbmax(Ru). Hence, the inequality in (108) holds if\n|\u03bbl(A)| + \u00b5\u03bbmax(Ru) \u22641 \u2212\u00b5\u03bbmin(Ru)\n(110)\nfor l = 2, 3, . . . , N and we arrive at (57).\n31\nAPPENDIX C\nPROOF OF LEMMA 2\nFrom Lemma 1, the eigen-decomposition for the matrix power Bj is given by:\nBj =\nN\nX\nl=1\nM\nX\nm=1\n\u03bbj\nl,m(B) \u00b7 rb\nl,msb\u2217\nl,m.\n(111)\nUsing (111), we can rewrite the MSD at node k from (49) as:\nMSDk =\n\u221e\nX\nj=0\nN\nX\nl1,l2=1\nM\nX\nm1,m2=1\nTr\nh\n\u03bbj\nl1,m1(B)\u03bb\u2217j\nl2,m2(B) \u00b7 (eT\nk \u2297IM) \u00b7 rb\nl1,m1sb\u2217\nl1,m1Ysb\nl2,m2rb\u2217\nl2,m2 \u00b7 (ek \u2297IM)\ni\n=\nN\nX\nl1,l2=1\nM\nX\nm1,m2=1\n\u0010\nrb\u2217\nl2,m2(ek \u2297IM)(eT\nk \u2297IM)rb\nl1,m1\n\u0011\n\u00b7\n\u0010\nsb\u2217\nl1,m1Ysb\nl2,m2\n\u0011\n1 \u2212\u03bbl1,m1(B)\u03bb\u2217\nl2,m2(B)\n(112)\nwhere we used Tr(AB) = Tr(BA) and the expression for the in\ufb01nite sum of a geometric series. Using\n(54), we have:\nrb\u2217\nl2,m2(ek \u2297IM)(eT\nk \u2297IM)rb\nl1,m1 = (r\u2217\nl2ekeT\nk rl1) \u2297(z\u2217\nm2zm1) = (r\u2217\nl2ekeT\nk rl1) \u00b7 \u03b4m1m2\n(113)\nsince the eigenvectors {zm} are orthonormal. Substituting (113) into (112), we arrive at (61). Likewise,\nfrom (47) and (61), the network MSD is given by\nMSD = 1\nN\nN\nX\nl1,l2=1\nM\nX\nm=1\n\u0010PN\nk=1 r\u2217\nl2ekeT\nk rl1\n\u0011\n\u00b7 sb\u2217\nl1,mYsb\nl2,m\n1 \u2212\u03bbl1,m(B)\u03bb\u2217\nl2,m(B)\n.\n(114)\nFrom assumption (62), we can establish (63) since\nN\nX\nk=1\nr\u2217\nl2ekeT\nk rl1 = r\u2217\nl2 \u00b7 IN \u00b7 rl1 \u2248\u03b4l1l2.\n(115)\nAPPENDIX D\nPROOF OF THEOREM 3\nWe \ufb01rst verify that MSDatc \u2264MSDcta, MSDcta \u2264MSDncop, and MSDatc \u2264MSDcons. We show the\nresult by verifying that the individual terms on the right hand side of (63) for the various strategies have\nthe same ordering. That is, from (63) and Table III, we verify that the following ratios, which correspond\nto MSDatc \u2264MSDcta, MSDcta \u2264MSDncop, and MSDatc \u2264MSDcons, respectively, are upper bounded\n32\nby one:\n|\u03bbl(A)|2 \u22641\n(116)\n1 \u2212(1 \u2212\u00b5\u03bbm(Ru))2\n1 \u2212|\u03bbl(A)|2 \u00b7 (1 \u2212\u00b5\u03bbm(Ru))2 \u22641\n(117)\n|\u03bbl(A)|2 \u00001 \u2212|\u03bbl(A) \u2212\u00b5\u03bbm(Ru)|2\u0001\n1 \u2212|\u03bbl(A)|2 \u00b7 (1 \u2212\u00b5\u03bbm(Ru))2\n\u22641\n(118)\nfor all l and m. Note that relations (116)-(117) hold since |\u03bbl(A)| \u22641 for all l in view of the fact that A\nis left-stochastic and, hence, \u03c1(A) = 1. We therefore established (64). On the other hand, relation (118)\nwould hold if, and only if,\n|\u03bbl(A)|2 \u0002\n1 + (1 \u2212\u00b5\u03bbm(Ru))2 \u2212|\u03bbl(A) \u2212\u00b5\u03bbm(Ru)|2\u0003\n\u22641.\n(119)\nTo establish that (119) is true for all l and m, we introduce the compact notation \u03bb = \u03bbl(A), \u03b4 =\n\u00b5\u03bbm(Ru), and consider the following function of two variables:\nf(\u03bb, \u03b4) \u225c|\u03bb|2 \u0002\n1 + (1 \u2212\u03b4)2 \u2212|\u03bb \u2212\u03b4|2\u0003\nwith |\u03bb| \u22641, \u03b4 \u2208(0, 2), and |\u03bb \u2212\u03b4| < 1.\n(120)\nThe range for \u03b4 ensures condition (27) and the stability of the diffusion network, while the range for\n|\u03bb \u2212\u03b4| ensures that the consensus network is stable, i.e., |\u03bbl,m(Bcons)| < 1 for all l and m. Then, we\nwould like to show that f(\u03bb, \u03b4) \u22641. Since \u03bb is generally complex-valued, we denote the real part of \u03bb\nby \u03bbr. Then, the term |\u03bb \u2212\u03b4|2 in (120) is given by |\u03bb \u2212\u03b4|2 = |\u03bb|2 + \u03b42 \u22122\u03bbr\u03b4 and f(\u03bb, \u03b4) from (120)\nbecomes\nf(\u03bb, \u03b4) = \u2212|\u03bb|4 + 2(1 \u2212\u03b4 + \u03bbr\u03b4)|\u03bb|2.\n(121)\nSince f(\u03bb, \u03b4) is linear in \u03b4, the maximum value of f(\u03bb, \u03b4) in (121) over \u03b4 occurs at the end points of\n\u03b4. Since \u03b4 \u2208(0, 2) and |\u03bbr \u2212\u03b4| \u2264|\u03bb \u2212\u03b4| < 1, we conclude that 0 < \u03b4 < 1 + \u03bbr. Substituting the end\npoints of \u03b4 into (121), we have\nf(\u03bb, 0) = \u2212(|\u03bb|2 \u22121)2 + 1 \u22641\n(122)\nf(\u03bb, 1 + \u03bbr) = \u2212|\u03bb|4 + 2\u03bb2\nr|\u03bb|2 \u2264|\u03bb|4 \u22641\n(123)\nwhere we used the fact that \u03bb2\nr \u2264|\u03bb|2 and |\u03bb| \u22641. We therefore established (65).\nLet us now examine what happens when the step-size is such that 1 \u2264\u00b5\u03bbmin(Ru) < 2. Again, from\n(63) and Table III, we establish that MSDncop \u2264MSDcons this conclusion by showing that the ratio of\nthe individual terms appearing in the sums (63) is upper bounded by one:\n1 \u2212|\u03bbl(A) \u2212\u00b5\u03bbm(Ru)|2\n1 \u2212(1 \u2212\u00b5\u03bbm(Ru))2\n\u22641\n(124)\n33\nfor all l and m. Condition (124) is equivalent to showing that\n|\u03bbl(A) \u2212\u00b5\u03bbm(Ru)|2 \u2212(1 \u2212\u00b5\u03bbm(Ru))2 = |\u03bb|2 \u22122\u03bbr\u03b4 \u2212(1 \u22122\u03b4) \u22650\n(125)\nwhere we used the notation from (120). Relation (125) holds since \u03b4 \u2265\u00b5\u03bbmin(Ru) \u22651 \u2265|\u03bb| \u2265|\u03bbr|\nand then\n|\u03bb|2 \u22122\u03bbr\u03b4 \u2212(1 \u22122\u03b4) \u2265\u03bb2\nr \u22122\u03bbr\u03b4 \u2212(1 \u22122\u03b4) = (1 \u2212\u03bbr)(2\u03b4 \u22121 \u2212\u03bbr) \u22650.\n(126)\nAPPENDIX E\nPROOF OF THEOREM 5\nFrom the series forms of {MSDk(m)} in Table IV, the difference between MSDcta,k(m) and MSDncop,k(m)\ncan be expressed as:\nMSDncop,k(m) \u2212MSDcta,k(m) = \u00b52\u03bbm(Ru)\n\u221e\nX\nj=0\n(1 \u2212\u00b5\u03bbm(Ru))2jeT\nk\n\u0000\u03a3v \u2212ATj\u03a3vAj\u0001\nek.\n(127)\nFrom (89), we have\nlim\nj\u2192\u221eeT\nk\n\u0000\u03a3v \u2212ATj\u03a3vAj\u0001\nek = \u03c32\nv,k \u2212eT\nk r1sT\n1 \u03a3vs1rT\n1 ek.\n(128)\nTherefore, there exists an integer Jm such that for any \u03b5 > 0,\neT\nk\n\u0000\u03a3v \u2212ATj\u03a3vAj\u0001\nek \u2265\u03c32\nv,k \u2212eT\nk r1sT\n1 \u03a3vs1rT\n1 ek \u2212\u03b5 \u225c\u2206\n(129)\nfor all j \u2265Jm. From (90), \u2206in (129) becomes \u2206= \u03c32\nv,k \u2212sT\n1 \u03a3vs1/N \u2212\u03b5. From condition (91), we\nare able to choose \u03b5 small enough such that \u2206is strictly greater than zero. Therefore, expression (127)\nis lower bounded by:\nMSDncop,k(m) \u2212MSDcta,k(m) \u2265\u00b52\u03bbm(Ru)\n\uf8ee\n\uf8f0\u2212z + \u2206\u00b7\n\u221e\nX\nj=Jm\n(1 \u2212\u00b5\u03bbm(Ru))2j\n\uf8f9\n\uf8fb\n(130)\nwhere the term z \u22650 is an upper bound for the \ufb01rst Jm terms of the summation in (127), i.e.,\n\f\f\f\f\f\f\nJm\u22121\nX\nj=0\n(1 \u2212\u00b5\u03bbm(Ru))2jeT\nk\n\u0000\u03a3v \u2212ATj\u03a3vAj\u0001\nek\n\f\f\f\f\f\f\n\u2264z < \u221e.\n(131)\nIt can be veri\ufb01ed that the series inside the brackets of (130) is strictly decreasing in \u00b5 \u2208(0, 1/\u03bbm(Ru)).\nIn addition,\nlim\n\u00b5\u21920\n\uf8eb\n\uf8ed\n\u221e\nX\nj=Jm\n(1 \u2212\u00b5\u03bbm(Ru))2j\n\uf8f6\n\uf8f8= \u221e.\n(132)\n34\nThus, there exists a \u00b5\u25e6\nm > 0 such that the sum inside the bracket of (130) becomes positive and, hence,\nMSDncop,k(m) \u2212MSDcta,k(m) > 0\n(133)\nfor all 0 < \u00b5 \u2264\u00b5\u25e6\nm. Repeating the above argument, we will obtain a collection of step-size bounds\n{\u00b5\u25e6\n1, \u00b5\u25e6\n2, . . . , \u00b5\u25e6\nM}. We then choose \u00b5\u25e6= min{\u00b5\u25e6\n1, \u00b5\u25e6\n2, . . . , \u00b5\u25e6\nM} so that relation (133) holds for all m.\nThen, applying Lemma 4, we arrive at (92) for any \u00b5 satisfying 0 < \u00b5 \u2264\u00b5\u25e6.\nAPPENDIX F\nCONDITION (81) IMPLIES CONDITION (91) WHEN A IS PRIMITIVE\nIt follows from (81) that ATj\u03a3vAj \u2212AT(j+1)\u03a3vAj+1 \u22650 for any nonnegative integer j and then\nJ\nX\nj=0\n\u0010\nATj\u03a3vAj \u2212AT(j+1)\u03a3vAj+1\u0011\n= \u03a3v \u2212AT(J+1)\u03a3vAJ+1 \u22650.\n(134)\nSince A is primitive, as J tends to in\ufb01nity, we get from (89) that\nlim\nJ\u2192\u221e\n\u0010\n\u03a3v \u2212AT(J+1)\u03a3vAJ+1\u0011\n= \u03a3v \u2212r1sT\n1 \u03a3vs1rT\n1 \u22650.\n(135)\nUsing (90), we conclude that\ndet(\u03a3v \u2212r1sT\n1 \u03a3vs1rT\n1 ) = det(\u03a3v) \u00b7 det\n\u0012\nIN \u2212\u03a3\u22121\nv 1 \u00b7 sT\n1 \u03a3vs1\nN\n1T\n\u0013\n\u22650.\n(136)\nSince for any column vectors {x, y} of size N, it holds that det(IN \u2212x \u00b7 yT) = 1 \u2212yT \u00b7 x, relation (136)\nimplies that the following must hold:\n\u0012\n1 \u2212sT\n1 \u03a3vs1\nN\n1T \u00b7 \u03a3\u22121\nv 1\n\u0013\n\u22650.\n(137)\nHowever, by the Cauchy-Schwarz inequality [47] and using the fact that sT\n1 1/\n\u221a\nN = 1 from (90), we\nhave\nsT\n1 \u03a3vs1\nN\n1T \u00b7 \u03a3\u22121\nv 1 =\n N\nX\nl=1\n\u03c32\nv,l\ns2\nl,1\nN\n!\n\u00b7\n N\nX\nl=1\n\u03c3\u22122\nv,l\n!\n\u2265\n N\nX\nl=1\nsl,1\n\u221a\nN\n!2\n=\n\u0012sT\n1 1\n\u221a\nN\n\u00132\n= 1\n(138)\nwhere sl,1 denotes the lth entry of s1. Therefore, relation (137) can hold only with equality in (138). In\nturn, equality in (138) holds if, and only if, there exists a constant c such that sl,1/\n\u221a\nN = c \u00b7 \u03c3\u22122\nv,l for all\nl. By the fact that sT\n1 1/\n\u221a\nN = 1, we get:\nsl,1\n\u221a\nN\n=\n\u03c3\u22122\nv,l\nPN\nm=1 \u03c3\u22122\nv,m\n(139)\nand arrive at (91) since\n\u03c32\nv,k \u2212sT\n1 \u03a3vs1\nN\n= \u03c32\nv,k \u2212\n1\nPN\nl=1 \u03c3\u22122\nv,l\n> \u03c32\nv,k \u2212\n1\n\u03c3\u22122\nv,k\n= 0.\n(140)\n35\nREFERENCES\n[1] S. Y. Tu and A. H. Sayed, \u201cDiffusion networks outperform consensus networks,\u201d Proc. IEEE Workshop on Statistical\nSignal Processing (SSP), pp. 1\u20134, Ann Arbor, MI, Aug. 2012. See also the online version http://arxiv.org/abs/1205.3993\navailable as arXiv:1205.3993, May 2012.\n[2] C. G. Lopes and A. H. Sayed, \u201cDiffusion least-mean squares over adaptive networks: Formulation and performance\nanalysis,\u201d IEEE Trans. on Signal Processing, vol. 56, no. 7, pp. 3122\u20133136, Jul. 2008.\n[3] F. S. Cattivelli and A. H. Sayed, \u201cDiffusion LMS strategies for distributed estimation,\u201d IEEE Trans. on Signal Processing,\nvol. 58, no. 3, pp. 1035\u20131048, Mar. 2010.\n[4] S. Y. Tu and A. H. Sayed, \u201cMobile adaptive networks,\u201d IEEE J. Selected Topics on Signal Processing, vol. 5, no. 4, pp.\n649\u2013664, Aug. 2011.\n[5] F. Cattivelli and A. H. Sayed, \u201cModeling bird \ufb02ight formations using diffusion adaptation,\u201d IEEE Trans. on Signal\nProcessing, vol. 59, no. 5, pp. 2038\u20132051, May 2011.\n[6] M. H. DeGroot, \u201cReaching a consensus,\u201d Journal of the American Statistical Association, vol. 69, no. 345, pp. 118\u2013121,\nMar. 1974.\n[7] L. Xiao and S. Boyd, \u201cFast linear iteration for distributed averaging,\u201d Systems and Control Letters, pp. 65\u201378, 2004.\n[8] S. Boyd, A. Ghosh, B. Prabhakar, and D. Shah, \u201cRandomized gossip algorithms,\u201d IEEE Trans. on Information Theory,\nvol. 52, no. 6, pp. 2508\u20132530, Jun. 2006.\n[9] R. Olfati-Saber and R. M. Murray, \u201cConsensus problems in networks of agents with switching topology and time-delays,\u201d\nIEEE Trans. on Automatic Control, vol. 49, no. 9, pp. 1520\u20131533, Sep. 2004.\n[10] T. C. Aysal, M. E. Yildiz, A. D. Sarwate, and A. Scaglione, \u201cBroadcast gossip algorithms for consensus,\u201d IEEE Trans. on\nSignal Processing, vol. 57, no. 7, pp. 2748\u20132761, July 2009.\n[11] S. Sardellitti, M. Giona, and S. Barbarossa, \u201cFast distributed average consensus algorithms based on advection-diffusion\nprocesses,\u201d IEEE Trans. on Signal Processing, vol. 58, no. 2, pp. 826\u2013842, Feb. 2010.\n[12] D. Jakovetic, J. Xavier, and J. M. F. Moura, \u201cWeight optimization for consenus algorithms with correlated switching\ntopology,\u201d IEEE Trans. on Signal Processing, vol. 58, no. 7, pp. 3788\u20133801, July 2010.\n[13] A. Jadbabaie, J. Lin, and A. S. Morse, \u201cCoordination of groups of mobile autonomous agents using nearest neighbor rules,\u201d\nIEEE Trans. on Automatic Control, vol. 48, no. 6, pp. 988\u20131001, Jun. 2003.\n[14] W. Ren and R. Beard, \u201cConsensus seeking in multiagent systems under dynamically changing interaction topologies,\u201d IEEE\nTrans. on Automatic Control, vol. 50, no. 5, pp. 655\u2013661, May 2005.\n[15] J. N. Tsitsiklis, J. N. Bertsekas, and M. Athans, \u201cDistributed asynchronous deterministic and stochastic gradient optimization\nalgorithms,\u201d IEEE Trans. on Autom. Control, vol. 31, no. 9, pp. 803\u2013812, Sep. 1986.\n[16] A. Nedic and A. Ozdaglar, \u201cCooperative distributed multi-agent optimization,\u201d in the book Convex Optimization in Signal\nProcessing and Communications, Y. Eldar and D. Palomar (Eds.), Cambridge University Press, pp. 340\u2013386, 2009.\n[17] A. G. Dimakis, S. Kar, J. M. F. Moura, M. G. Rabbat, and A. Scaglione, \u201cGossip algorithms for distributed signal\nprocessing,\u201d Proc. of the IEEE, vol. 98, no. 11, pp. 1847\u20131864, Nov. 2010.\n[18] S. Kar and J. M. F. Moura, \u201cConvergence rate analysis of distributed gossip (linear parameter) estimation: Fundamental\nlimits and tradeoffs,\u201d IEEE J. Selected Topics in Signal Processing, vol. 5, no. 5, pp. 674\u2013690, Aug. 2011.\n[19] I. Schizas, G. Mateos, and G. Giannakis, \u201cDistributed LMS for consensus-based in-network adaptive processing,\u201d IEEE\nTrans. on Signal Processing, vol. 57, no. 6, pp. 2365\u20132382, Jun. 2009.\n36\n[20] L. Xiao, S. Boyd, and S. Lall, \u201cA space-time diffusion scheme peer-to-peer least-squares-estimation,\u201d Proc. Information\nProcessing in Sensor Networks (IPSN), pp. 168\u2013176, Nashville, TN, Apr. 2006.\n[21] S. Barbarossa and G. Scutari, \u201cBio-inspired sensor network design,\u201d IEEE Signal Processing Magazine, vol. 24, no. 3, pp.\n26\u201335, May 2007.\n[22] B. Johansson, T. Keviczky, M. Johansson, and K. Johansson, \u201cSubgradient methods and consensus algorithms for solving\nconvex optimization problems,\u201d Proc. IEEE CDC, pp. 4185\u20134190, Cancun, Mexico, Dec. 2008.\n[23] D. P. Bertsekas and J. N. Tsitsiklis, Parallel and Distributed Computation: Numerical Methods.\nAthena Scienti\ufb01c,\nSingapore, 1997.\n[24] C. G. Lopes and A. H. Sayed, \u201cDistributed processing over adaptive networks,\u201d Proc. Adaptive Sensor Array Processing\nWorkshop, pp. 1\u20135, MIT Lincoln Laboratory, MA, Jun. 2006.\n[25] A. H. Sayed and C. G. Lopes, \u201cAdaptive processing over distributed networks,\u201d IEICE Trans. on Fundamentals of\nElectronics,Communications and Computer Sciences, vol. E90-A, no. 8, pp. 1504\u20131510, 2007.\n[26] F. S. Cattivelli and A. H. Sayed, \u201cDiffusion LMS algorithms with information exchange,\u201d Proc. Asilomar Conference on\nSignals, Systems and Computers, pp. 251\u2013255, Paci\ufb01c Grove, CA, Oct. 2008.\n[27] A. H. Sayed, \u201cDiffusion adaptation over networks,\u201d available online at http://arxiv.org/abs/1205.4220 as manuscript\narXiv:1205.4220v1 [cs.MA], May 2012. To appear in E-Reference Signal Processing, R. Chellapa and S. Theodoridis,\neditors, Elsevier, 2013.\n[28] S. Ram, A. Nedic, and V. V. Veeravalli, \u201cDistributed stochastic subgradient projection algorithms for convex optimization,\u201d\nJournal of Optimization Theory and Applications, vol. 147, no. 3, pp. 516\u2013545, 2010.\n[29] K. Srivastava and A. Nedic, \u201cDistributed asynchronous constrained stochastic optimization,\u201d IEEE J. Selected Topics on\nSignal Processing, vol. 5, no. 4, pp. 772\u2013790, Aug. 2011.\n[30] J. Chen and A. H. Sayed, \u201cDiffusion adaptation strategies for distributed optimization and learning over networks,\u201d IEEE\nTrans. on Signal Processing, vol. 60, no. 8, pp. 4289\u20134305, Aug 2012.\n[31] L. Li and J. A. Chambers, \u201cDistributed adaptive estimation based on the APA algorithm over diffusion networks with\nchanging topology,\u201d Proc. IEEE Statist. Signal Process. Workshop, pp. 757\u2013760, Cardiff, Wales, Sep. 2009.\n[32] N. Takahashi and I. Yamada, \u201cLink probability control for probabilistic diffusion least-mean squares over resource-\nconstrained networks,\u201d Proc. IEEE ICASSP, pp. 3518\u20133521, Dallas, TX, Mar. 2010.\n[33] S. Chouvardas, K. Slavakis, and S. Theodoridis, \u201cAdaptive robust distributed learning in diffusion sensor networks,\u201d IEEE\nSignal Processing Magazine, vol. 59, no. 10, pp. 4692\u20134707, Oct. 2011.\n[34] R. Abdolee and B. Champagne, \u201cDiffusion LMS algorithms for sensor networks over non-ideal inter-sensor wireless\nchannels,\u201d Proc. IEEE Int. Conf. Dist. Comput. Sensor Systems (DCOSS), pp. 1\u20136, Barcelona, Spain, Jun. 2011.\n[35] A. H. Sayed, Adaptive Filters.\nNJ. Wiley, 2008.\n[36] J. Arenas-Garcia, V. Gomez-Verdejo, and A. R. Figueiras-Vidal, \u201cNew algorithms for improved adaptive convex combination\nof LMS transversal \ufb01lters,\u201d IEEE Trans. on Instrumentation and Measurement, vol. 54, no. 6, pp. 2239\u20132249, Dec. 2005.\n[37] R. Candido, M. T. M. Silva, and V. H. Nascimento, \u201cTransient and steady-state analysis of the af\ufb01ne combination of two\nadaptive \ufb01lters,\u201d IEEE Trans. on Signal Processing, vol. 58, no. 8, pp. 4064\u20134078, Aug. 2010.\n[38] S. S. Kozat and A. T. Erdogan, \u201cCompetitive linear estimation under model uncertainties,\u201d IEEE Trans. on Signal Processing,\nvol. 58, no. 4, pp. 2388\u20132393, Apr. 2010.\n[39] Y. Xia, L. Li, J. Cao, M. Golz, and D. P. Mandic, \u201cA collaborative \ufb01ltering approach for quasi-brain-death EEG analysis,\u201d\nProc. IEEE ICASSP, pp. 645\u2013648, Prague, Czech Republic, May 2011.\n37\n[40] Z. Tow\ufb01c, J. Chen, and A. H. Sayed, \u201cOn the generalization ability of online learners,\u201d Proc. IEEE Workshop on Machine\nLearning for Signal Processing (MLSP), Santander, Spain, Sep. 2012.\n[41] S. Haykin, Adaptive Filter Theory.\nPrentice Hall, 2002.\n[42] R. Abdolee, B. Champagne, and A. H. Sayed, \u201cDiffusion LMS for source and process estimation in sensor networks,\u201d\nProc. IEEE Workshop on Statistical Signal Processing (SSP), pp. 1\u20134, Ann Arbor, MI, Aug. 2012.\n[43] P. Braca, S. Marano, V. Matta, and P. Willett, \u201cAsymptotic optimality of running consensus in testing binary hypotheses,\u201d\nIEEE Trans. on Signal Processing, vol. 58, no. 2, pp. 814\u2013825, Feb. 2010.\n[44] D. Bajovic, D. Jakovetic, J. M. F. Moura, J. Xavier, and B. Sinopoli, \u201cLarge deviations performance of consen-\nsus+innovations distributed detection with non-gaussian observations,\u201d available on arXiv:1111.4555v2, Apr, 2012.\n[45] T. Y. Al-Naffouri and A. H. Sayed, \u201cTransient analysis of data-normalized adaptive \ufb01lters,\u201d IEEE Trans. on Signal\nProcessing, vol. 51, no. 3, pp. 639\u2013652, Mar. 2003.\n[46] R. L. Berger, \u201cA necessary and suf\ufb01cient condition for reaching a consensus using degroots method,\u201d Journal of the\nAmerican Statistical Association, vol. 76, no. 374, pp. 118\u2013121, 1981.\n[47] R. Horn and C. R. Johnson, Matrix Analysis.\nCambridge University Press, 1985.\n[48] X. Zhao, S. Y. Tu, and A. H. Sayed, \u201cDiffusion adaptation over networks under imperfect information exchange and\nnon-stationary data,\u201d IEEE Trans. on Signal Processing, vol. 60, no. 7, pp. 3460\u20133475, Jul. 2012.\n[49] L. Xiao, S. Boyd, and S. Lall, \u201cA scheme for robust distributed sensor fusion based on average consensus,\u201d Proc. Information\nProcessing Sensor Networks (IPSN), pp. 63\u201370, Los Angeles, CA, Apr. 2005.\n",
        "sentence": " It has been shown in [22] that the dynamics of diffusion networks leads to enhanced stability and lower mean- In particular, the analysis in [20]\u2013[22] shows that consensus networks combine local data and in-neighborhood information asymmetrically, which can make the state of consensus networks grow unbounded even when all individual agents are mean stable in isolation.",
        "context": "of the network: diffusion networks are shown to converge faster and reach lower mean-square deviation\nthan consensus networks, and their mean-square stability is insensitive to the choice of the combination\nare more stable than consensus networks. Moreover, the stability of diffusion networks is independent of\nthe combination weights, whereas consensus networks can become unstable even if all individual nodes\nrecursive implementations. It turns out that the dynamics of the consensus and diffusion strategies differ\nin important ways, which in turn impact the mean-square behavior of the respective networks in a\n3\nfundamental manner."
    },
    {
        "title": "GQ(\u03bb): A general gradient algorithm for temporal-difference prediction learning with eligibility traces",
        "author": [
            "H.R. Maei",
            "R.S. Sutton"
        ],
        "venue": "Proc. Conference on Artificial General Intelligence (AGI), vol. 1, Lugano, Switzerland, 2010, pp. 91\u201396. November 6, 2014  DRAFT  35",
        "citeRegEx": "23",
        "shortCiteRegEx": null,
        "year": 2010,
        "abstract": "",
        "full_text": "",
        "sentence": " The convergence analysis of reinforcement learning algorithms is usually challenging even for the single-agent case, and studies are often restricted to the case of diminishing step-sizes [8], [9], [23]. By using importance sampling, reference [23] showed that we can write the gradient inside (24a) in terms of moment values of the behavior policy as follows: X\u22a4D\u03c6 (X\u03b8i + (IS \u2212 \u03b3P )Xwi \u2212 r)",
        "context": null
    },
    {
        "title": "QD-learning: A collaborative distributed strategy for multi-agent reinforcement learning through consensus + innovations",
        "author": [
            "S. Kar",
            "J.M.F. Moura",
            "H.V. Poor"
        ],
        "venue": "IEEE Transactions on Signal Processing, vol. 61, no. 7, pp. 1848\u20131862, 2013.",
        "citeRegEx": "24",
        "shortCiteRegEx": null,
        "year": 1848,
        "abstract": "",
        "full_text": "",
        "sentence": " For example, the work in [24] proposes a useful algorithm, named QD-learning, which is a distributed implementation of Q-learning using consensus-based stochastic approximation. However, QD-learning is developed in [24] under the assumption of perfect-knowledge of the state. In comparison, the analysis in [24] employs a diminishing step-size that dies out as time progresses and, therefore, turns off adaptation and is not able to track concept drifts in the data.",
        "context": null
    },
    {
        "title": "Residual algorithms: Reinforcement learning with function approximation",
        "author": [
            "L. Baird"
        ],
        "venue": "Proc. Int. Conf. on Machine Learning (ICML), Tahoe City, CA, USA, 1995, pp. 30\u201337.",
        "citeRegEx": "25",
        "shortCiteRegEx": null,
        "year": 1995,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " Here, we study the case in which the agents only know a feature representation of the state, which is used to build a parametric approximation of the value function, allowing us to tackle large problems, for which Q-learning schemes can diverge [25], [26]. Another related work [27] analyzes the performance of cooperative distributed asynchronous estimation of linearly approximated value functions using standard temporal difference (TD), but it is well known that TD learning with parametric approximation schemes can diverge when the agents learn off-policy [25], [26].",
        "context": null
    },
    {
        "title": "An analysis of temporal-difference learning with function approximation",
        "author": [
            "J.N. Tsitsiklis",
            "B. Van Roy"
        ],
        "venue": "IEEE Transactions on Automatic Control, vol. 42, no. 5, pp. 674\u2013690, 1997.",
        "citeRegEx": "26",
        "shortCiteRegEx": null,
        "year": 1997,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " Here, we study the case in which the agents only know a feature representation of the state, which is used to build a parametric approximation of the value function, allowing us to tackle large problems, for which Q-learning schemes can diverge [25], [26]. Another related work [27] analyzes the performance of cooperative distributed asynchronous estimation of linearly approximated value functions using standard temporal difference (TD), but it is well known that TD learning with parametric approximation schemes can diverge when the agents learn off-policy [25], [26]. , [26], [31], [32]) and it is promising mainly because it leads to solutions with low computational demands. To address this issue, one approach is to solve instead the projected Bellman equation [26]:",
        "context": null
    },
    {
        "title": "The Borkar-Meyn theorem for asynchronous stochastic approximations",
        "author": [
            "S. Bhatnagar"
        ],
        "venue": "Systems and Control Letters, vol. 60, no. 7, pp. 472\u2013478, 2011.",
        "citeRegEx": "27",
        "shortCiteRegEx": null,
        "year": 2011,
        "abstract": "",
        "full_text": "",
        "sentence": " Another related work [27] analyzes the performance of cooperative distributed asynchronous estimation of linearly approximated value functions using standard temporal difference (TD), but it is well known that TD learning with parametric approximation schemes can diverge when the agents learn off-policy [25], [26]. In addition, although the algorithm in [27] is distributed, in the sense that there is no fusion center, it requires full connectivity (i.",
        "context": null
    },
    {
        "title": "Distributed value functions",
        "author": [
            "J. Schneider",
            "W.-K. Wong",
            "A. Moore",
            "M. Riedmiller"
        ],
        "venue": "Proc. Int. Conf. on Machine Learning (ICML), Bled, Slovenia, 1999, pp. 371\u2013378.",
        "citeRegEx": "28",
        "shortCiteRegEx": null,
        "year": 1999,
        "abstract": "",
        "full_text": "",
        "sentence": " Other related\u2014but more heuristic\u2014approaches include [28], [29].",
        "context": null
    },
    {
        "title": "Efficient distributed reinforcement learning through agreement",
        "author": [
            "P. Varshavskaya",
            "L. Kaelbling",
            "D. Rus"
        ],
        "venue": "Distributed Autonomous Robotic Systems 8, H. Asama, H. Kurokawa, J. Ota, and K. Sekiyama, Eds. Springer Berlin Heidelberg, 2009, pp. 367\u2013378.",
        "citeRegEx": "29",
        "shortCiteRegEx": null,
        "year": 2009,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " Other related\u2014but more heuristic\u2014approaches include [28], [29].",
        "context": null
    },
    {
        "title": "Should one compute the temporal difference fix point or minimize the Bellman residual? The unified oblique projection view",
        "author": [
            "B. Scherrer"
        ],
        "venue": "Proc. Int. Conf. on Machine Learning (ICML), Haifa, Israel, 2010, pp. 959\u2013966.",
        "citeRegEx": "31",
        "shortCiteRegEx": null,
        "year": 2010,
        "abstract": "We investigate projection methods, for evaluating a linear approximation of\nthe value function of a policy in a Markov Decision Process context. We\nconsider two popular approaches, the one-step Temporal Difference fix-point\ncomputation (TD(0)) and the Bellman Residual (BR) minimization. We describe\nexamples, where each method outperforms the other. We highlight a simple\nrelation between the objective function they minimize, and show that while BR\nenjoys a performance guarantee, TD(0) does not in general. We then propose a\nunified view in terms of oblique projections of the Bellman equation, which\nsubstantially simplifies and extends the characterization of (schoknecht,2002)\nand the recent analysis of (Yu & Bertsekas, 2008). Eventually, we describe some\nsimulations that suggest that if the TD(0) solution is usually slightly better\nthan the BR solution, its inherent numerical instability makes it very bad in\nsome cases, and thus worse on average.",
        "full_text": "arXiv:1011.4362v1  [cs.AI]  19 Nov 2010\nShould one compute the Temporal Di\ufb00erence \ufb01x point or minimize\nthe Bellman Residual ? The uni\ufb01ed oblique projection view\nBruno Scherrer\nscherrer@loria.fr\nLORIA - INRIA Lorraine - Campus Scienti\ufb01que - BP 239\n54506 Vand\u0153uvre-l`es-Nancy CEDEX\nFRANCE\nAbstract\nWe investigate projection methods, for eval-\nuating a linear approximation of the value\nfunction of a policy in a Markov Decision\nProcess context.\nWe consider two popular\napproaches, the one-step Temporal Di\ufb00er-\nence \ufb01x-point computation (TD(0)) and the\nBellman Residual (BR) minimization.\nWe\ndescribe examples, where each method out-\nperforms the other.\nWe highlight a sim-\nple relation between the objective function\nthey minimize, and show that while BR en-\njoys a performance guarantee, TD(0) does\nnot in general.\nWe then propose a uni\ufb01ed\nview in terms of oblique projections of the\nBellman equation, which substantially sim-\npli\ufb01es and extends the characterization of\nSchoknecht (2002) and the recent analysis of\nYu & Bertsekas (2008).\nEventually, we de-\nscribe some simulations that suggest that if\nthe TD(0) solution is usually slightly better\nthan the BR solution, its inherent numerical\ninstability makes it very bad in some cases,\nand thus worse on average.\nIntroduction\nWe consider linear approximations of the value func-\ntion of the policy in the framework of Markov Deci-\nsion Processes (MDP). We focus on two popular meth-\nods: the computation of the projected Tempo-\nral Di\ufb00erence \ufb01xed point (TD(0), TD for short),\nwhich Antos et al. (2008); Farahmand et al. (2008);\nSutton et al. (2009) have recently presented as the\nminimization of the mean-square projected Bellman\nAppearing in Proceedings of the 27 th International Confer-\nence on Machine Learning, Haifa, Israel, 2010. Copyright\n2010 by the author(s)/owner(s).\nEquation, and the minimization of the mean-\nsquare Bellman Residual (BR). In this article, we\npresent some new analytical and empirical data, that\nshed some light on both approaches.\nThe paper is\norganized as follows. Section 1 describes the MDP lin-\near approximation framework and the two projection\nmethods.\nSection 2 presents small MDP examples,\nwhere each method outperforms the other.\nSection\n3 highlights a simple relation between the quantities\nTD and BR optimize, and show that while BR enjoys a\nperformance guarantee, TD does not in general. Sec-\ntion 4 contains the main contribution of this paper:\nwe describe a uni\ufb01ed view in terms of oblique projec-\ntions of the Bellman equation, which simpli\ufb01es and\nextends the characterization of Schoknecht (2002) and\nthe recent analysis of Yu & Bertsekas (2008). Eventu-\nally, Section 5 presents some simulations, that address\nthe following practical questions: which of the method\ngives the best approximation? and how useful is our\nanalysis for selecting it a priori?\n1. Framework and Notations\nThe model\nWe consider an MDP with a \ufb01xed pol-\nicy, that is an uncontrolled discrete-time dynamic sys-\ntem with instantaneous rewards.\nWe assume that\nthere is a state space X of \ufb01nite size N. When at\nstate i \u2208{1, .., N}, there is a transition probability\npij of getting to the next state j. Let ik the state of\nthe system at time k. At each time step, the system is\ngiven a reward \u03b3kr(ik) where r is the instantaneous re-\nward function, and 0 < \u03b3 < 1 is a discount factor.\nThe value at state i is de\ufb01ned as the total expected re-\nturn: v(i) := limN\u2192\u221eE\nhPN\u22121\nk=0 \u03b3kr(ik)\n\f\f\f i0 = i\ni\n. We\nwrite P the N \u00d7 N stochastic matrix whose elements\nare pij. v can be seen as a vector of RN. v is known\nto be the unique \ufb01xed point of the Bellman operator:\nT v := r + \u03b3Pv, that is v solves the Bellman Equation\nv = T v and is equal to L\u22121r where L = I \u2212\u03b3P.\nTD or BR? The uni\ufb01ed oblique projection view\nApproximation Scheme\nWhen the size N of the\nstate space is large, one usually comes down to solving\nthe Bellman Equation approximately. One possibility\nis to look for an approximate solution \u02c6v in some speci\ufb01c\nsmall space. The simplest and best understood choice\nis a linear parameterization: \u2200i, \u02c6v(i) = Pm\nj=1 wj\u03c6j(i)\nwhere m \u226aN, the \u03c6j are some feature functions that\nshould capture the general shape of v, and wj are the\nweights that characterize the approximate value \u02c6v. For\nall i and j, write \u03c6j the N-dimensional vector corre-\nsponding to the jth feature function and \u03c6(i) the m-\ndimensional vector giving the features of state i. For\nany vector of matrix X, denote X\u2032 its transpose. The\nfollowing N \u00d7 m feature matrix \u03a6 = (\u03c61 . . . \u03c6m) =\n(\u03c6(i1) . . . \u03c6(iN))\u2032 leads to write the parameterization\nof v in a condensed matrix form:\n\u02c6v = \u03a6w, where\nw = (w1, ..., wm) is the m-dimensional weight vec-\ntor. We will now on denote span (\u03a6) this subspace\nof RN and assume that the vectors \u03c61, ..., \u03c6m form a\nlinearly independent set.\nSome approximation \u02c6v of v can be obtained by mini-\nmizing \u02c6v 7\u2192\u2225\u02c6v \u2212v\u2225for some norm \u2225\u00b7\u2225, that is equiva-\nlently by projecting v onto span (\u03a6) orthogonally with\nrespect to \u2225\u00b7 \u2225. In a very general way, any symmetric\npositive de\ufb01nite matrix Q of RN induces a quadratic\nnorm \u2225\u00b7 \u2225Q on RN as follows: \u2225v\u2225Q = \u221av\u2032Qv.\nIt\nis well known that the orthogonal projection with re-\nspect to such a norm, which we will denote \u03a0\u2225\u00b7\u2225Q,\nhas the following closed form: \u03a0\u2225\u00b7\u2225Q = \u03a6\u03c0\u2225\u00b7\u2225Q where\n\u03c0\u2225\u00b7\u2225Q = (\u03a6\u2032Q\u03a6)\u22121\u03a6\u2032Q is the linear application from\nRN to Rm that returns the coordinates of the pro-\njection of a point in the basis (\u03c61, . . . , \u03c6m).\nWith\nthese notations, the following relations \u03c0\u2225\u00b7\u2225Q\u03a6 = I\nand \u03c0\u2225\u00b7\u2225Q\u03a0\u2225\u00b7\u2225Q = \u03c0\u2225\u00b7\u2225Q hold.\nIn an MDP approximation context, where one is mod-\neling a stochastic system, one usually considers a spe-\nci\ufb01c kind of norm/projection. Let \u03be = (\u03bei) be some\ndistribution on X such that \u03be > 0 (it assigns a positive\nprobability to all states). Let \u039e be the diagonal matrix\nwith the elements of \u03be on the diagonal. Consider the\northogonal projection of RN onto the feature space\nspan (\u03a6) with respect to the \u03be-weighted quadratic\nnorm \u2225v\u2225\u03be =\nqPN\nj=1 \u03beivi2 =\n\u221a\nv\u2032\u039ev.\nFor clarity\nof exposition, we will denote this speci\ufb01c projection\n\u03a0 := \u03a0\u2225\u00b7\u2225\u039e = \u03a6\u03c0 where \u03c0 := \u03c0\u2225\u00b7\u2225\u039e = (\u03a6\u2032\u039e\u03a6)\u22121\u03a6\u2032\u039e.\nIdeally, one would like to compute the \u201cbest\u201d approx-\nimation\n\u02c6vbest = \u03a6wbest with wbest = \u03c0v = \u03c0L\u22121r.\nThis can be done with algorithms like TD(1) /\nLSTD(1)(Bertsekas & Tsitsiklis, 1996; Boyan, 2002),\nbut they require simulating in\ufb01nitely long trajectories\nand usually su\ufb00er from a high variance. The projec-\ntions methods, which we focus on in this paper, are\nalternatives that only consider one-step samples.\nTD(0) \ufb01x point method\nThe principle of the\nTD(0) method (TD for short) is to look for a \ufb01xed\npoint of \u03a0T , that is, one looks for \u02c6vT D in the space\nspan (\u03a6) satisfying \u02c6vT D = \u03a0T \u02c6vT D.\nAssuming that\nthe matrix inverse below exists1, it can be proved2\nthat \u02c6vT D = \u03a6wT D with\nwT D = (\u03a6\u2032\u039eL\u03a6)\u22121\u03a6\u2032\u039er\n(1)\nAs\npointed\nout\nby\nAntos et al.\n(2008);\nFarahmand et al.\n(2008);\nSutton et al.\n(2009),\nwhen the inverse exists, the above computation is\nequivalent to minimizing for \u02c6v \u2208span (\u03a6) the TD\nerror ET D(\u02c6v) := \u2225\u02c6v \u2212\u03a0T \u02c6v\u2225\u03be down to 03.\nBR minimization method\nThe principle of the\nBellman Residual (BR) method is to look for \u02c6v \u2208\nspan (\u03a6) so that it minimizes the norm of the Bellman\nResidual, that is the quantity EBR(\u02c6v) := \u2225\u02c6v \u2212T \u02c6v\u2225\u03be.\nSince \u02c6v is of the form \u03a6w, it can be seen that EBR(\u02c6v) =\n\u2225\u03a6w \u2212\u03b3P\u03a6w \u2212r\u2225\u03be = \u2225\u03a8w \u2212r\u2225\u03be using the notation\n\u03a8 = L\u03a6. Using standard linear least squares argu-\nments, one can see that the minimum BR is obtained\nfor \u02c6vBR = \u03a6wBR with\nwBR = (\u03a8\u2032\u039e\u03a8)\u22121\u03a8\u2032\u039er.\n(2)\nNote that in this case, the above inverse always exists\n(Schoknecht, 2002).\n2. Two simple examples\nExample 1\nConsider the 2 state MDP such that\nP =\n\u0010\n0\n1\n0\n1\n\u0011\n. Denote the rewards r1 and r2. One\nthus have v(1) = r1 + \u03b3r2\n1\u2212\u03b3 and v(2) =\nr2\n1\u2212\u03b3 . Consider\nthe one-feature linear approximation with \u03a6 = (1 2)\u2032,\nwith uniform distribution \u03be = (.5 .5)\u2032.\n\u03a6\u2032\u039e\u03a6 =\n5\n2,\ntherefore \u03c0 =\n\u0000 1\n5\n2\n5\n\u0001\n, and the weight of the best approx-\nimation is wbest = \u03c0v = 1\n5r1 +\n2+\u03b3\n5(1\u2212\u03b3)r2. This example\nhas been proposed by Bertsekas & Tsitsiklis (1996) in\norder to show that \ufb01tted Value Iteration can diverge\nif the samples are not generated by the stationary\ndistribution of the policy. In (Bertsekas & Tsitsiklis,\n1996), the authors only consider the case r1 = r2 = 0\n1This is not necessary the case, as the forthcoming Ex-\nample 1 (Section 2) shows.\n2Section 4 will generalize this derivation.\n3This remark is also true if we replace \u2225\u00b7 \u2225\u03be by any\nequivalent norm \u2225\u00b7 \u2225. This observation lead Sutton et al.\n(2009) to propose original o\ufb00-policy gradient algorithms\nfor computing the TD solution.\nTD or BR? The uni\ufb01ed oblique projection view\nTD\nBR\n 0\n 0.2\n 0.4\n 0.6\n 0.8\n 1\ngamma\n 0\n 0.5\n 1\n 1.5\n 2\n 2.5\n 3\ntheta\n 0.1\n 1\n 10\n 100\n 1000\n 10000\nerr ratio\n 1\n 10\n 100\n 1000\n 10000\n 0\n 0.2\n 0.4\n 0.6\n 0.8\n 1\nerr ratio\ngamma\nTD\nBR\nFigure 1. Error ratio (in log scale) between the TD/BR\nprojection methods and the best approximation for Exam-\nple 1, with respect to the discount factor \u03b3 and the param-\neter \u03b8 of the reward (Left). It turns out that these surfaces\ndo not depend on \u03b8 so we also draw the graph with respect\nto \u03b3 only (Right).\nso that this diverging result was true even though\nthe exact value function v(0) = v(1) = 0 did be-\nlong to the feature space. In the case r1 = r2 = 0,\nthe TD and BR methods do calculate the exact so-\nlution (we will see later that this is indeed a general\nfact when the exact value function belongs to the fea-\nture space).\nWe thus extend this model by taking\n(r1, r2) \u0338= (0, 0). As a scaling of the reward is trans-\nlated exactly in the approximation, we consider the\ngeneral form (r1, r2) = (cos \u03b8, sin \u03b8).\nConsider the TD solution:\none has \u03a6\u2032\u039e =\n\u0000 1\n2 1\n\u0001\n,\n(I \u2212\u03b3P)\u03a6 = (1 \u22122\u03b3 1 \u2212\u03b3), thus (\u03a6\u2032\u039e\u03a8) = 5\n2 \u22123\u03b3\nand \u03a6\u2032\u039er =\nr1\n2 + r2.\nEventually the weight of the\nTD approximation is wT D =\nr1+2r2\n5\u22126\u03b3 .\nOne notices\nhere that the value \u03b3 = 5/6 is singular. Now, con-\nsider the BR solution. One can see that (\u03a8\u2032\u039e\u03a8)\u22121 =\n(1\u22122\u03b3)2+(2\u22122\u03b3)2\n2\nand \u03a8\u2032\u039er = (1\u22122\u03b3)r1+(2\u22122\u03b3)r2\n2\n. Thus,\nthe weight of the BR approximation is wBR\n=\n(1\u22122\u03b3)r1+(2\u22122\u03b3)r2\n(1\u22122\u03b3)2+(2\u22122\u03b3)2 .\nFor all these approximations, one can compute the\nsquared error e with respect to the optimal solution\nv: For any weight w \u2208{wbest, wT D, wBR}, e(w) =\n\u2225v \u2212\u03a6w\u22252\n\u03be = 1\n2(v(1) \u2212w)2 + 1\n2(v(2) \u22122w)2. In Fig-\nure 1, we plot the squared error ratios\ne(wT D)\ne(wbest) and\ne(wBR)\ne(wbest) on a log scale (they are by de\ufb01nition greater\nthan 1) with respect to \u03b8 and \u03b3.\nIt turns out that\nthese ratios do not depend on \u03b8 (instead of showing\nthis through painful arithmetic manipulations, we will\ncome back to this point and prove it later on). This\nFigure also displays the graph with respect to \u03b3 only.\nWe can observe that for any choice of reward function\nand discount factor, the BR method returns a bet-\nter value than the TD method.\nAlso, when \u03b3 is in\nthe neighborhood of 5\n6, the TD error ratio tends to \u221e\nwhile BR\u2019s stays bounded. This Example shows that\nthere exists MDPs where the BR is consistenly better\nthan the TD method, which can give an unbounded\nerror. One should however not conclude too quickly\nthat BR is always better than TD. The literature con-\ntains several arguments in favor of TD, one of which\nis considered in the following Example.\nExample 2\nSutton et al. (2009) recently described a\n3-state MDP example where the TD method computes\nthe best projection while BR does not. The idea be-\nhind this 3-state example can be described in a quite\ngeneral way4: Suppose we have a k + l-state MDP,\nof which the Bellman Equation has a block triangular\nstructure: v1 = \u03b3P1v1 + r1 / v2 = \u03b3P21v1 + P22v2 + r2\nwhere v1 \u2208Rk and v2 \u2208Rl (the concatenation of\nthe vectors v1 and v2 form the value function). Sup-\npose also that the approximation subspace span (\u03a6)\nis Rk \u00d7 S2 where S2 is a subspace of Rl. For the \ufb01rst\ncomponent v1, the approximation space is the entire\nspace Rk.\nWith TD, we obtain the exact value for\nthe k \ufb01rst components of the value, while with Bell-\nman residual minimization, we do not: satisfying the\n\ufb01rst equation exactly is traded for decreasing the error\nin satisfying the second one (which also involves v1).\nIn an optimal control context, the example above can\nhave quite dramatic implications, as v1 can be related\nto the costs at some future states accessible from those\nstates associated with v2, and the future costs are all\nthat matters when making decisions.\nOverall, the two methods generate di\ufb00erent types of\nbiases, and distribute error in di\ufb00erent manners. In\norder to gain some more insight, we now turn on to\nsome analytical facts about them.\n3. A Relation and Stability Issues\nThough several works have compared and considered\nboth methods (Schoknecht, 2002; Lagoudakis & Parr,\n2003; Munos, 2003; Yu & Bertsekas, 2008), the follow-\ning simple fact has, to our knowledge, never been em-\nphasized per se:\nProposition 1 The BR is an upper bound of the TD\nerror, and more precisely:\n\u2200\u02c6v \u2208span (\u03a6) , EBR(\u02c6v)2 = ET D(\u02c6v)2 + \u2225T \u02c6v \u2212\u03a0T \u02c6v\u22252\n\u03be.\nProof This simply follows from Pythagore, as \u03a0T \u02c6v \u2212\nT \u02c6v is orthogonal to span (\u03a6) and \u02c6v \u2212\u03a0T \u02c6v belongs to\nspan (\u03a6).\n\u25a0\nThis implies that if one can make the BR small, then\nthe TD Error will also be small. In the limit case where\n4The rest of this section is strongly inspired by a per-\nsonal communication with Yu.\nTD or BR? The uni\ufb01ed oblique projection view\none can make the BR equal to 0, then the TD Error\nis also 0.\nOne\nof\nthe\nmotivation\nfor\nminimizing\nthe\nBR\nis historically related to a well-known result of\nWilliams & Baird (1993): \u2200\u02c6v, \u2225v \u2212\u02c6v\u2225\u221e\u2264\n1\n1\u2212\u03b3 \u2225T \u02c6v \u2212\n\u02c6v\u2225\u221e. Since one considers the weighted quadratic norm\nin practice5, the related result6 that really makes sense\nhere is:\n\u2200\u02c6v, \u2225v \u2212\u02c6v\u2225\u03be \u2264\n\u221a\nC(\u03be)\n1\u2212\u03b3 \u2225T \u02c6v \u2212\u02c6v\u2225\u03be where\nC(\u03be) := maxi,j\npij\n\u03bei\nis a \u201cconcentration coe\ufb03cient\u201d,\nthat can be seen as some measure of the stochastic-\nity of the MDP7. This result shows that it is sound\nto minimize the BR, since it controls (through a con-\nstant) the approximation error \u2225v \u2212\u02c6vBR\u2225\u03be.\nOn the TD side, there does not exist any similar re-\nsult. Actually, the fact that one can build examples\n(like Example 1) where the TD projection is numeri-\ncally unstable implies that one cannot prove such a re-\nsult. Proposition 1 allows to understand better the TD\nmethod: by minimizing the TD Error, one only min-\nimizes one part of the BR, or equivalently this means\nthat one does not care about the term \u2225T v \u2212\u03a0T v\u22252\n\u03be,\nwhich may be interpreted as a measure of adequacy\nof the projection \u03a0 with the Bellman operator T . In\nExample 1, the approximation error of the TD pro-\njection goes to in\ufb01nity because this adequacy term di-\nverges. In (Munos & Szepesv\u00b4ari, 2008), the authors\nuse an algorithm based on the TD Error and make an\nassumption on this adequacy term (there called the in-\nherent Bellman error of the approximation space), so\nthat their algorithm can be proved convergent.\nA complementary view on the potential instability\nof TD, has been referred to as a norm incompatibil-\nity issue (Bertsekas & Tsitsiklis, 1996; Guestrin et al.,\n2001), and can be revisited through the notion of con-\ncentration coe\ufb03cient. Stochastic matrices P statisfy\n\u2225P\u2225\u221e= 1, which makes the Bellman operator T \u03b3-\ncontracting, and thus its \ufb01xed point is well-de\ufb01ned.\nThe orthogonal projection with respect to \u2225\u00b7 \u2225\u03be is\nsuch that \u2225\u03a0\u2225\u03be = 1.\nThus P and \u03a0 are of norm\n1, but for di\ufb00erent norms.\nUnfortunately, a gen-\neral (tight) bound for linear projections is \u2225\u03a0\u2225\u221e\u2264\n5Mainly\nbecause\nit\nis\ncomputationnally\neasier\nthan\ndoing\na\nmax-norm\nminimization,\nsee\nhowever\n(Guestrin et al., 2001) for an attempt of doing max-norm\nprojection.\n6The proof is a consequence of Jensen\u2019s inequality and\nthe arguments are very close to the ones in (Munos, 2003).\n7If \u03be is the uniform law, then there always exists such\na C(\u03be) \u2208(1, N) where one recalls that N is the size of the\nstate space; in such a case, C(\u03be) is minimal if all next-states\nare chosen with the uniform law, and maximal as soon as\nthere exists a deterministic transition. See (Munos, 2003)\nfor more discussion on this coe\ufb03cient.\n1+\n\u221a\nN\n2\n(Thompson, 1996) and it can be shown8 that\n\u2225P\u2225\u03be \u2264\np\nC(\u03be) (which can thus also be of the order\nof\n\u221a\nN). Consequently, \u2225\u03a0P\u2225\u221eand \u2225\u03a0P\u2225\u03be may be\ngreater than 1, and thus the \ufb01xed point of the pro-\njected Bellman equation may not be well-de\ufb01ned. A\nknown exception where the composition \u03a0P has norm\n1, is when one can prove that \u2225P\u2225\u03be = 1 (as for instance\nwhen \u03be is the stationary distribution of P) and in\nthis case we know from Bertsekas & Tsitsiklis (1996);\nTsitsiklis & Van Roy (1997) that\n\u2225v \u2212\u02c6vT D\u2225\u03be \u2264\n1\np\n1 \u2212\u03b32 \u2225v \u2212\u02c6vbest\u2225\u03be.\n(3)\nAnother notable such exception is when \u2225\u03a0\u2225max = 1,\nas in the so-called \u201caverager\u201d approximation (Gordon,\n1995). However, in general, the stability of TD is dif-\n\ufb01cult to guarantee.\n4. The uni\ufb01ed oblique projection view\nIn the TD approach, we consider \ufb01nding the \ufb01xed\npoint of the composition of an orthogonal projection\n\u03a0 and the Bellman operator T . Suppose now we con-\nsider using a (non necessarily orthogonal) projection\n\u03a0 onto span (\u03c6), that is any linear operator that sat-\nis\ufb01es \u03a02 = \u03a0 and whose range is span (\u03a6). In their\nmost general form, such operators are called oblique\nprojections and can be written \u03a0X\n= \u03a6\u03c0X with\n\u03c0X = (X\u2032\u03a6)\u22121X\u2032. The parameter X speci\ufb01es the pro-\njection direction: precisely, \u03a0X is the projection onto\nspan (\u03a6) orthogonally to span (X). As for the orthog-\nonal projections, the following relations \u03c0X\u03a6 = I and\n\u03c0X\u03a0X = \u03c0X hold. Recall that L = I \u2212\u03b3P. We are\nready to state the main result of this paper:\nProposition 2 Write XT D = \u039e\u03a6 and XBR = \u039eL\u03a6.\n(1) The TD \ufb01x point computation and the BR min-\nimization are solutions (respectively with X = XT D\nand X\n= XBR) of the projected equation \u02c6vX\n=\n\u03a0XT \u02c6vX. (2) When it exists, the solution of this pro-\njected equation is the projection of v onto span (\u03a6) or-\nthogonally to span (L\u2032X), i.e. formally \u02c6vX = \u03a0L\u2032X v.\nProof We begin by showing part (2). Writing \u02c6vX =\n\u03a6wX, the \ufb01xed point equation is: \u03a6wX = \u03a0X(r +\n\u03b3P\u03a6wx). Multiplying on both sides by \u03c0X, one ob-\ntains: wX = \u03c0X(r + \u03b3P\u03a6wx) and therefore wX =\n(I \u2212\u03b3\u03c0XP\u03a6)\u22121\u03c0Xr. Using the de\ufb01nition of \u03c0X, one\n8One can prove that for all x, \u2225Px\u22252\n\u03be \u2264\u2225x\u22252\n\u03beP\n\u2264\nC(\u03be)\u2225x\u22252\n\u03be. The argument for the \ufb01rst inequality involves\nJensen\u2019s inequality and is again close to what is done in\n(Munos, 2003).\nTD or BR? The uni\ufb01ed oblique projection view\nobtains:\nwX\n=\n(I \u2212\u03b3(X\u2032\u03a6)\u22121X\u2032P\u03a6)\u22121(X\u2032\u03a6)\u22121X\u2032r\n=\n\u0002\n(X\u2032\u03a6)(I \u2212\u03b3(X\u2032\u03a6)\u22121X\u2032P\u03a6)\n\u0003\u22121 X\u2032r\n=\n(X\u2032(I \u2212\u03b3P)\u03a6)\u22121X\u2032r\n(4)\n=\n(X\u2032L\u03a6)\u22121X\u2032Lv\n=\n\u03c0L\u2032X v\nwhere we enventually used r = Lv.\nThe proof of part (1) now follows. The fact that TD\nis a special case with X = \u039e\u03a6 is trivial by construc-\ntion since then \u03a0X is the orthogonal projection with\nrespect to \u2225\u00b7 \u2225\u03be. When X = \u039eL\u03a6, one simply needs\nto observe from Equations 2 and 4 and the de\ufb01nition\nof \u03a8 = L\u03a6 that wX = wBR.\n\u25a0\nBeyond its nice and simple geometric \ufb02avour, a direct\nconsequence of Proposition 2 is that it allows to derive\ntight error bounds for TD, BR, and any other method\nfor general X. For any square matrix M, write \u03c3(M)\nits spectral radius.\nProposition 3 For any choice of X, the approxima-\ntion error satis\ufb01es:\n\u2225v \u2212\u02c6vX\u2225\u03be\n\u2264\n\u2225\u03a0L\u2032X\u2225\u03be\u2225v \u2212\u02c6vbest\u2225\u03be\n(5)\n=\np\n\u03c3(ABCB\u2032)\u2225v \u2212\u02c6vbest\u2225\u03be\nwhere A\n=\n\u03a6\u2032\u039e\u03a6,\nB\n=\n(X\u2032L\u03a6)\u22121 and C\n=\nXL\u039e\u22121L\u2032X are matrices of size m \u00d7 m.\nThus, for any X, the ampli\ufb01cation of the smallest er-\nror \u2225v \u2212\u02c6vbest\u2225\u03be depends on the norm of the associated\noblique projection, which can be estimated as the spec-\ntral radius of the product of small matrices. A simple\ncorollary of this Proposition is the following: if the real\nvalue v belongs to the feature space span (\u03a6) (in such\na case v = \u02c6vbest) then all oblique projection methods\n\ufb01nd it (\u02c6vX = v).\nProof of Proposition 3\nProposition 2 implies that\nv \u2212\u02c6vX = (I \u2212\u03a0L\u2032X)v = (I \u2212\u03a0L\u2032X)(I \u2212\u03a0\u039e\u03a6)v. where\nwe used the fact that \u03a0L\u2032X\u03a0\u039e\u03a6 = \u03a0\u039e\u03a6 since \u03a0L\u2032X and\n\u03a0\u039e\u03a6 are projections onto span (\u03a6). Taking the norm,\none obtains \u2225v \u2212\u02c6vX\u2225\u03be \u2264\u2225I \u2212\u03a0L\u2032X\u2225\u03be\u2225v \u2212\u03a0\u039e\u03a6v\u2225\u03be =\n\u2225\u03a0L\u2032X\u2225\u03be\u2225v \u2212\u02c6vbest\u2225\u03be where we used the de\ufb01nition of\n\u02c6vbest, and the fact that \u2225I \u2212\u03a0L\u2032X\u2225\u03be = \u2225\u03a0L\u2032X\u2225\u03be since\n\u03a0L\u2032X is a (non-trivial) projection (see e.g.\n(Szyld,\n2006)). Thus Equation 5 holds.\nIn order to evaluate the norm in terms of small size\nmatrices, one will use the following Lemma on the pro-\njection matrix \u03a0L\u2032X = \u03a6\u03c0L\u2032X:\nLemma 1 (Yu & Bertsekas (2008)) Let Y be an\nN \u00d7 m matrix, and Z a m \u00d7 N matrix, then \u2225Y Z\u22252\n\u03be =\n\u03c3\n\u0000(Y \u2032\u039eY )(Z\u039e\u22121Z\u2032)\n\u0001\n.\nThus,\n\u2225\u03a0L\u2032X\u22252\n\u03be\n=\n\u2225\u03a6\u03c0L\u2032X\u22252\n\u03be\n=\n\u03c3[(\u03a6\u2032\u039e\u03a6)(\u03c0L\u2032X\u039e\u22121(\u03c0L\u2032X)\u2032)]\n=\n\u03c3[\u03a6\u2032\u039e\u03a6(X\u2032L\u03a6)\u22121X\u2032L\u039e\u22121L\u2032X(\u03a6\u2032L\u2032X)\u22121]\n=\n\u03c3[ABCB\u2032].\n\u25a0\nProposition 2 is closely related to the work of\n(Schoknecht, 2002), in which the author derived the\nfollowing characterization of the TD and BR solutions:\nProposition 4 (Schoknecht (2002)) The TD \ufb01x\npoint computation and the BR minimization are or-\nthogonal projections of the value v respectively induced\nby the seminorm \u2225\u00b7\u2225QT D\n9 with QT D = L\u2032\u039e\u03a6\u03a6\u2032\u039eL and\nby the norm \u2225\u00b7 \u2225QBR with QBR = L\u2032\u039eL.\nThis \u201corthogonal projection\u201d characterization and our\n\u201coblique projection\u201d characterization are in fact equiv-\nalent.\nOn the one hand for BR, it is immediate\nto notice that \u03a0\u2225\u00b7\u2225QBR = \u03a0L\u2032XBR.\nOn the other\nhand for TD, writing Y = L\u2032XT D, one simply needs\nto notice that \u03a0L\u2032XT D\n= \u03a0Y\n= \u03a6(Y \u2032\u03a6)\u22121Y \u2032 =\n\u03a6(Y \u2032\u03a6)\u22121(\u03a6\u2032Y )\u22121(\u03a6\u2032Y )Y \u2032 = \u03a6(\u03a6\u2032Y Y \u2032\u03a6)\u22121\u03a6\u2032Y Y \u2032 =\n\u03a0\u2225\u00b7\u2225QT D .\nThe work of Schoknecht (2002) suggests\nthat TD and BR are optimal for di\ufb00erent criteria,\nsince both look for some \u02c6v \u2208span (\u03a6) that minimizes\n\u2225\u02c6v \u2212v\u2225for some (semi)norm \u2225\u00b7 \u2225. Curiously, our re-\nsult suggests that neither is optimal, since neither uses\nthe best projection direction X\u2217:= L\u2032\u22121\u039e\u03a6 for which\n\u02c6vX\u2217= \u03a0L\u2032X\u2217v = \u03a0\u039e\u03a6v = \u02c6vbest and this supports the\nempirical evidence that there is no clear \u201cwinner\u201d be-\ntween TD and BR.\nOur main results, stated in Propositions 2 and 3,\nconstitutes a revisit of the work of Yu & Bertsekas\n(2008), where the authors similarly derived error\nbounds for TD and BR. Our approach mimicks theirs:\n1) we derive a linear relation between the projec-\ntion \u02c6v, the real value v and the best projection \u02c6vbest,\nthen 2) analyze the norm of the matrices involved\nin this relation in terms of spectral radius of small\nmatrices (through Lemma 1, which is taken from\n(Yu & Bertsekas, 2008)). From a purely quantitative\npoint of view, our bounds are identical to the ones de-\nrived there. Two immediate consequences of this quan-\ntitative equivalence are that, as in (Yu & Bertsekas,\n9This is a seminorm because the matrix QT D is only\nsemide\ufb01nite (since \u03a6\u03a6\u2032 has rank smaller than m < N).\nThe corresponding projection can still be well de\ufb01ned\n(i.e. each point has exactly one projection) provided that\nspan (\u03a6) \u2229{x; \u2225x\u2225QT D = 0} = {0}.\nTD or BR? The uni\ufb01ed oblique projection view\n2008), (1) our bound is tight in the sense that there\nexists a worst choice for the reward for which it\nholds with equality, and (2) it is always better than\nthat of Equation 3 from Bertsekas & Tsitsiklis (1996);\nTsitsiklis & Van Roy (1997).\nHowever, our work is\nqualitatively di\ufb00erent: by highlighting the oblique pro-\njection relation between \u02c6v and v, not only do we pro-\nvide a clear geometric intuition for both methods, but\nwe also greatly simplify the form of the results and\ntheir proofs (see (Yu & Bertsekas, 2008) for details).\nLast but not least, there is globally a signi\ufb01cant dif-\nference between our work and the two works we have\njust mentionned. The analysis we propose is uni\ufb01ed for\nTD and BR (and even extends to potential new meth-\nods through other choices of the parameter X), while\nthe results in (Schoknecht, 2002) and (Yu & Bertsekas,\n2008) are proved independently for each method. We\nhope that our uni\ufb01ed approach will help understand-\ning better the pros and cons of TD, BR, and related\nalternative approaches.\n\u03b3 = 0.9\nE[TD wins]\n0.5\n 0\n 5\n 10\n 15\n 20\n 25\n 30\nproj. space dim.\n 0\n 5\n 10\n 15\n 20\n 25\n 30\nspace dim.\n 0\n 0.1\n 0.2\n 0.3\n 0.4\n 0.5\n 0.6\n 0.7\n 0.8\n 0.9\n\u03b3 = 0.95\nE[TD wins]\n0.5\n 0\n 5\n 10\n 15\n 20\n 25\n 30\nproj. space dim.\n 0\n 5\n 10\n 15\n 20\n 25\n 30\nspace dim.\n 0\n 0.1\n 0.2\n 0.3\n 0.4\n 0.5\n 0.6\n 0.7\n 0.8\n\u03b3 = 0.99\nE[TD wins]\n0.5\n 0\n 5\n 10\n 15\n 20\n 25\n 30\nproj. space dim.\n 0\n 5\n 10\n 15\n 20\n 25\n 30\nspace dim.\n 0\n 0.1\n 0.2\n 0.3\n 0.4\n 0.5\n 0.6\n 0.7\n 0.8\n\u03b3 = 0.999\nE[TD wins]\n0.5\n 0\n 5\n 10\n 15\n 20\n 25\n 30\nproj. space dim.\n 0\n 5\n 10\n 15\n 20\n 25\n 30\nspace dim.\n 0\n 0.1\n 0.2\n 0.3\n 0.4\n 0.5\n 0.6\n 0.7\n 0.8\nFigure 2. TD win ratio.\n5. An Empirical Comparison\nIn order to further compare the TD and the BR projec-\ntions, we have made some empirical comparison, which\nwe describe now. We consider spaces of dimensions\nn = 2, 3, .., 30. For each n, we consider projections of\ndimensions k = 1, 2, .., n. For each (n, k) couple, we\ngenerate 20 random projections (through random ma-\ntrices10 \u03a6 of size (n, k) and random weight vectors \u03be)\nand 20 random (uncontrolled) chain like MDP: from\neach state i, there is a probability pi (chosen randomly\nuniformly on (0, 1)) to get to state i + 1 and a proba-\nbility 1 \u2212pi to stay in i (the last state is absorbing);\n10Each entry is a random uniform number between -1\nand 1.\n\u03b3 = 0.9\nE[good prediction]\n0.5\n 0\n 5\n 10\n 15\n 20\n 25\n 30\nproj. space dim.\n 0\n 5\n 10\n 15\n 20\n 25\n 30\nspace dim.\n 0\n 0.2\n 0.4\n 0.6\n 0.8\n 1\n\u03b3 = 0.95 E[good prediction]\n0.5\n 0\n 5\n 10\n 15\n 20\n 25\n 30\nproj. space dim.\n 0\n 5\n 10\n 15\n 20\n 25\n 30\nspace dim.\n 0\n 0.2\n 0.4\n 0.6\n 0.8\n 1\n\u03b3 = 0.99 E[good prediction]\n0.5\n 0\n 5\n 10\n 15\n 20\n 25\n 30\nproj. space dim.\n 0\n 5\n 10\n 15\n 20\n 25\n 30\nspace dim.\n 0\n 0.2\n 0.4\n 0.6\n 0.8\n 1\n\u03b3 = 0.999E[good prediction]\n0.5\n 0\n 5\n 10\n 15\n 20\n 25\n 30\nproj. space dim.\n 0\n 5\n 10\n 15\n 20\n 25\n 30\nspace dim.\n 0\n 0.2\n 0.4\n 0.6\n 0.8\n 1\nFigure 3. Prediction of the best method through Prop. 3\n\u03b3 = 0.9\nE[TD_err/BR_err]\n1\n 0\n 5\n 10\n 15\n 20\n 25\n 30\nproj. space dim.\n 0\n 5\n 10\n 15\n 20\n 25\n 30\nspace dim.\n 1\n 10\n 100\n\u03b3 = 0.95\nE[TD_err/BR_err]\n1\n 0\n 5\n 10\n 15\n 20\n 25\n 30\nproj. space dim.\n 0\n 5\n 10\n 15\n 20\n 25\n 30\nspace dim.\n 1\n 10\n 100\n 1000\n\u03b3 = 0.99\nE[TD_err/BR_err]\n1\n 0\n 5\n 10\n 15\n 20\n 25\n 30\nproj. space dim.\n 0\n 5\n 10\n 15\n 20\n 25\n 30\nspace dim.\n 1\n 10\n 100\n\u03b3 = 0.999 E[TD_err/BR_err]\n1\n 0\n 5\n 10\n 15\n 20\n 25\n 30\nproj. space dim.\n 0\n 5\n 10\n 15\n 20\n 25\n 30\nspace dim.\n 1\n 10\n 100\nFigure 4. Expectation of eT D/eBR.\nthe reward is a random vector. For the 20 \u00d7 20 re-\nsulting combinations, we compute the real value v, its\nexact projection \u02c6vbest, the TD \ufb01x point \u02c6vT D, and the\nBR projection \u02c6vBR. We then deduce the best error\ne = \u2225v \u2212\u02c6vbest\u2225\u03be, the TD error eT D = \u2225v \u2212\u02c6vT D\u2225\u03be\nand the BR eBR = \u2225v \u2212\u02c6vBR\u2225\u03be.\nWe also compute\nthe bounds of Proposition 3 for both methods: bT D\nand bBR. Each such experiment is done for 4 di\ufb00erent\nvalues of the discount factor \u03b3: 0.9, 0.95, 0.99, 0.999.\nUsing this raw data on 20 \u00d7 20 problems, we compute\nfor each (n, k) couple some statistics, which we de-\nscribe now. All the graphs that we display shows the\ndimension of the space N and of the projected space\nm on the x \u2212y axes. The z axis correspond to the\ndi\ufb00erent statistics of interest.\nFigure 2 shows the proportion of sampled problems\nwhere TD method returns a better approximation\nthan BR (i.e. the expectation of the indicator func-\ntion of eT D < eBR). It turns out that this ratio is\nTD or BR? The uni\ufb01ed oblique projection view\n\u03b3 = 0.9\nE[TD_err/err]\n 0\n 5\n 10\n 15\n 20\n 25\n 30\nproj. space dim.\n 0\n 5\n 10\n 15\n 20\n 25\n 30\nspace dim.\n 1\n 10\n 100\n 1000\n\u03b3 = 0.9\nE[BR_err/err]\n 0\n 5\n 10\n 15\n 20\n 25\n 30\nproj. space dim.\n 0\n 5\n 10\n 15\n 20\n 25\n 30\nspace dim.\n 1\n 10\n 100\n 1000\n\u03b3 = 0.95\nE[TD_err/err]\n 0\n 5\n 10\n 15\n 20\n 25\n 30\nproj. space dim.\n 0\n 5\n 10\n 15\n 20\n 25\n 30\nspace dim.\n 1\n 10\n 100\n 1000\n\u03b3 = 0.95\nE[BR_err/err]\n 0\n 5\n 10\n 15\n 20\n 25\n 30\nproj. space dim.\n 0\n 5\n 10\n 15\n 20\n 25\n 30\nspace dim.\n 1\n 10\n 100\n 1000\n\u03b3 = 0.99\nE[TD_err/err]\n 0\n 5\n 10\n 15\n 20\n 25\n 30\nproj. space dim.\n 0\n 5\n 10\n 15\n 20\n 25\n 30\nspace dim.\n 1\n 10\n 100\n 1000\n\u03b3 = 0.99\nE[BR_err/err]\n 0\n 5\n 10\n 15\n 20\n 25\n 30\nproj. space dim.\n 0\n 5\n 10\n 15\n 20\n 25\n 30\nspace dim.\n 1\n 10\n 100\n 1000\n\u03b3 = 0.999\nE[TD_err/err]\n 0\n 5\n 10\n 15\n 20\n 25\n 30\nproj. space dim.\n 0\n 5\n 10\n 15\n 20\n 25\n 30\nspace dim.\n 1\n 10\n 100\n 1000\n\u03b3 = 0.999\nE[BR_err/err]\n 0\n 5\n 10\n 15\n 20\n 25\n 30\nproj. space dim.\n 0\n 5\n 10\n 15\n 20\n 25\n 30\nspace dim.\n 1\n 10\n 100\n 1000\nFigure 5. (Left) Expectation of eT D/e and (Right) of\neBR/e.\nconsistently greater than 1\n2, which means that the TD\nmethod is usually better than the BR method. Figure\n3 presents the ratio of time the bounds we have pre-\nsented in Propostion 4 correctly guesses which method\nis the best (i.e. the expectation of the indicator func-\ntion of [eT D < eBR] = [bT D < bBR]).\nUnless the\nfeature space dimension is close to the state space di-\nmension, the bounds do not appear very useful for\nsuch a decision. Figure 4 displays the expectation of\neT D/eBR. One can observe that, on average, this ex-\npectation is bigger than 1, that is the BR tends to\nbe better, on average, than the TD error. This may\nlook contradictory with our interpretation of Figure\n2, but the explanation is the following: when the BR\nmethod is better than the TD method, it is by a larger\ngap than when it is the other way round. We believe\nthis corresponds to the situation when the TD method\nin unstable. Figure 5 allows to con\ufb01rm this point: it\nshows the expectation of the relative approximation\nerrors with respect to the best possible error, that is\nthe expectation of eT D/e and eBR/e. One observes on\nall charts that this average relative quality of the TD\n\ufb01x point has lots of pikes (corresponding to numerical\ninstabilities), while that of the BR method is smooth.\n6. Conclusion and Future Work\nWe have presented the TD \ufb01x point and the BR mini-\nmization methods for approximating the value of some\nMDP \ufb01xed policy. We have described two original ex-\namples: in the former, the BR method is consistently\nbetter than the TD method, while the latter (which\ngeneralizes the spirit of the example of Sutton et al.\n(2009)) is best treated by TD. Proposition 1 highlights\nthe close relation between the objective criteria that\ncorrespond to both methods. It shows that minimiz-\ning the BR implies minimizing the TD error and some\nextra \u201cadequacy\u201d term, which happens to be crucial\nfor numerical stability.\nOur main contribution, stated in Proposition 2, pro-\nvides a new viewpoint for comparing the two pro-\njection methods, and potential ideas for alternatives.\nBoth TD and BR can be characterized as solving a pro-\njected \ufb01xed point equation and this is to our knowledge\nnew for BR. Also, the solutions to both methods are\nsome oblique projection of the value v and this is to our\nknowledge new for TD and BR. Eventually, this simple\ngeometric characterization allows to derive some tight\nerror bounds (Proposition 3). We have discussed the\nclose relations of our results with those of Schoknecht\n(2002) and Yu & Bertsekas (2008), and argued that\nour work simpli\ufb01es and extends them.\nThough ap-\nparently new to the Reinforcement Learning commu-\nnity, the very idea of oblique projections of \ufb01xed point\nequations has been studied in the Numerical Analysis\ncommunity (see e.g. Saad (2003)). In the future, we\nplan to study more carefully this literature, and par-\nticularly investigate whether it may further contribute\nto the MDP context.\nConcerning the practical question of choosing among\nthe two methods TD and BR, the situation can be\nsummarized as follows: the BR method is sounder\nthan the TD method, since the former has a perfor-\nmance guarantee while the latter will never have one\nin general. Extensive simulations (on random chain-\nlike problems of size up to 30 states, and for many\nprojection of all the possible space sizes) further sug-\ngest the following facts: (a) the TD solution is more\noften better than the BR solution; (b) however some-\ntimes, TD failed dramatically; (c) overall, this makes\nBR better on average. Equivalently, one may say that\nTD is more risky than BR.\nEven if TD is more risky, there remains several reasons\nTD or BR? The uni\ufb01ed oblique projection view\nwhy one may want to use it in practice, and which our\nstudy did not focus on. In large scale problems, one\nusually estimates the m \u00d7 m linear systems through\nsampling. Sampling based methods for BR are more\nconstraining since they generally require double sam-\npling. Independently, the fact, highlighted by Propos-\ntion 1, that the BR is an upper bound of the TD error,\nsuggests two things. First, we believe that the vari-\nance of the BR problem is higher than that of the TD\nproblem; thus, given a \ufb01xed amount of samples, the\nTD solution might be less a\ufb00ected by the correspond-\ning stochastic noise than the BR one. More generally,\nthe BR problem may be harder to solve than the TD\nproblem, and from a numerical viewpoint, the latter\nmay provide better solutions. Eventually, we only dis-\ncussed the TD(0) \ufb01x point method, that is the speci\ufb01c\nvariant of TD(\u03bb) (Bertsekas & Tsitsiklis, 1996; Boyan,\n2002) where \u03bb = 0. Values of \u03bb > 0 solve some of the\nweaknesses of TD(0): it can be show that the stabil-\nity issues disappear for values of \u03bb close to 1, and the\noptimal projection \u02c6vbest is obtained when \u03bb = 1. Fur-\nther analytical and empirical comparisons of TD(\u03bb)\nwith the algorithms we have considered here (and with\nsome \u201cBR(\u03bb)\u201d algorithm) constitute future research.\nEventually, a somewhat disappointing observation of\nour study is that the bounds of Proposition 3, which\nare the tightest possible bounds independent of the\nreward function, did not prove useful for deciding a\npriori which of the two methods one should trust bet-\nter (recall the results showed in Figure 3). Extending\nthem in a way that would take the reward into ac-\ncount, as well as trying to exploit our original uni\ufb01ed\nvision of the bounds (Propositions 2 and 3) are some\npotential tracks for improvement.\nAcknowlegments\nThe author would like to thank Janey Yu for helpful\ndiscussions, and the anonymous reviewers for provid-\ning comments that helped to improve the presentation\nof the paper.\nReferences\nAntos, A., Szepesv\u00b4ari, C., and Munos, R.\nLearn-\ning near-optimal policies with bellman-residual min-\nimization based \ufb01tted policy iteration and a single\nsample path. Machine Learning, 71(1):89\u2013129, 2008.\nBertsekas, D.P. and Tsitsiklis, J.N.\nNeurodynamic\nProgramming. Athena Scienti\ufb01c, 1996.\nBoyan, J. A. Technical update: Least-squares tempo-\nral di\ufb00erence learning. Machine Learning, 49:233\u2013\n246, 2002.\nFarahmand, A.M., Ghavamzadeh, M., Szepesv\u00b4ari, C.,\nand Mannor, S.\nRegularized policy iteration.\nIn\nNIPS, 2008.\nGordon, G. Stable function approximation in dynamic\nprogramming. In ICML, 1995.\nGuestrin, C., Koller, D., and Parr, R. Max-norm pro-\njections for factored mdps. In IJCAI, 2001.\nLagoudakis, M. G. and Parr, R. Least-squares policy\niteration. JMLR, 4:1107\u20131149, 2003.\nMunos, R. Error bounds for approximate policy iter-\nation. In ICML, 2003.\nMunos, R. and Szepesv\u00b4ari, C. Finite-time bounds for\n\ufb01tted value iteration. JMLR, 9:815\u2013857, 2008. ISSN\n1532-4435.\nSaad, Y. Iterative Methods for Sparse Linear Systems,\n2nd edition. SIAM, Philadelpha, PA, 2003.\nSchoknecht, R. Optimality of reinforcement learning\nalgorithms with linear function approximation. In\nNIPS, pp. 1555\u20131562, 2002.\nSutton, R. S., Maei, H. R., Precup, D., Bhatna-\ngar, S., Silver, D., Szepesv\u00b4ari, C., and Wiewiora,\nE.\nFast gradient-descent methods for temporal-\ndi\ufb00erence learning with linear function approxima-\ntion. In ICML, 2009.\nSzyld, D.B. The many proofs of an identity on the\nnorm of oblique projections. Numerical Algorithms,\n42:309\u2013323, 2006.\nThompson, A.C.\nMinkowski Geometry.\nCambridge\nUniversity Press, 1996.\nTsitsiklis, J.N. and Van Roy, B.\nAn analysis of\ntemporal-di\ufb00erence learning with function approx-\nimation. IEEE Transactions on Automatic Control,\n42(5):674\u2013690, 1997.\nWilliams, R. J. and Baird, L. C. Tight performance\nbounds on greedy policies based on imperfect value\nfunctions.\nTechnical report, College of Computer\nScience, Northeastern University, 1993.\nYu, H. and Bertsekas, D.P. New error bounds for ap-\nproximations from projected linear equations. Tech-\nnical Report C-2008-43, Dept. Computer Science,\nUniv. of Helsinki, July 2008.\n",
        "sentence": " , [26], [31], [32]) and it is promising mainly because it leads to solutions with low computational demands.",
        "context": "problem, and from a numerical viewpoint, the latter\nmay provide better solutions. Eventually, we only dis-\ncussed the TD(0) \ufb01x point method, that is the speci\ufb01c\nvariant of TD(\u03bb) (Bertsekas & Tsitsiklis, 1996; Boyan,\nfor numerical stability.\nOur main contribution, stated in Proposition 2, pro-\nvides a new viewpoint for comparing the two pro-\njection methods, and potential ideas for alternatives.\nBoth TD and BR can be characterized as solving a pro-\nther analytical and empirical comparisons of TD(\u03bb)\nwith the algorithms we have considered here (and with\nsome \u201cBR(\u03bb)\u201d algorithm) constitute future research.\nEventually, a somewhat disappointing observation of"
    },
    {
        "title": "Parametric value function approximation: A unified view",
        "author": [
            "M. Geist",
            "O. Pietquin"
        ],
        "venue": "IEEE Symp. on Adaptive Dynamic Programming And Reinforcement Learning (ADPRL), Paris, France, 2011, pp. 9\u201316.",
        "citeRegEx": "32",
        "shortCiteRegEx": null,
        "year": 2011,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " , [26], [31], [32]) and it is promising mainly because it leads to solutions with low computational demands.",
        "context": null
    },
    {
        "title": "Basis function adaptation in temporal difference reinforcement learning",
        "author": [
            "I. Menache",
            "S. Mannor",
            "N. Shimkin"
        ],
        "venue": "Annals of Operations Research, vol. 134, pp. 215\u2013238, 2005.",
        "citeRegEx": "33",
        "shortCiteRegEx": null,
        "year": 2005,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " , [33]\u2013[38] ).",
        "context": null
    },
    {
        "title": "An analysis of linear models, linear value-function approximation, and feature selection for reinforcement learning",
        "author": [
            "R. Parr",
            "L. Li",
            "G. Taylor",
            "C. Painter-Wakefield",
            "M. Littman"
        ],
        "venue": "Proc. Int. Conf. on Machine Learning (ICML), Helsinki, Finland, 2008, pp. 752\u2013759.",
        "citeRegEx": "34",
        "shortCiteRegEx": null,
        "year": 2008,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "Basis function adaptation methods for cost approximation in MDP",
        "author": [
            "H. Yu",
            "D.P. Bertsekas"
        ],
        "venue": "Proc. IEEE Symp. on Adaptive Dynamic Programming and Reinforcement Learning (ADPRL), Nashville, TN, USA, 2009, pp. 74\u201381.",
        "citeRegEx": "35",
        "shortCiteRegEx": null,
        "year": 2009,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "Learning representation and control in Markov decision processes: New frontiers",
        "author": [
            "S. Mahadevan"
        ],
        "venue": "Foundations and Trends in Machine Learning, vol. 1, no. 4, pp. 403\u2013565, Apr. 2009.",
        "citeRegEx": "36",
        "shortCiteRegEx": null,
        "year": 2009,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "Predictive state temporal difference learning",
        "author": [
            "B. Boots",
            "G.J. Gordon"
        ],
        "venue": "Proc. Advances in Neural Information Processing Systems (NIPS) 23, 2010, pp. 271\u2013279.",
        "citeRegEx": "37",
        "shortCiteRegEx": null,
        "year": 2010,
        "abstract": "We propose a new approach to value function approximation which combines\nlinear temporal difference reinforcement learning with subspace identification.\nIn practical applications, reinforcement learning (RL) is complicated by the\nfact that state is either high-dimensional or partially observable. Therefore,\nRL methods are designed to work with features of state rather than state\nitself, and the success or failure of learning is often determined by the\nsuitability of the selected features. By comparison, subspace identification\n(SSID) methods are designed to select a feature set which preserves as much\ninformation as possible about state. In this paper we connect the two\napproaches, looking at the problem of reinforcement learning with a large set\nof features, each of which may only be marginally useful for value function\napproximation. We introduce a new algorithm for this situation, called\nPredictive State Temporal Difference (PSTD) learning. As in SSID for predictive\nstate representations, PSTD finds a linear compression operator that projects a\nlarge set of features down to a small set that preserves the maximum amount of\npredictive information. As in RL, PSTD then uses a Bellman recursion to\nestimate a value function. We discuss the connection between PSTD and prior\napproaches in RL and SSID. We prove that PSTD is statistically consistent,\nperform several experiments that illustrate its properties, and demonstrate its\npotential on a difficult optimal stopping problem.",
        "full_text": "Predictive State Temporal Difference Learning\nByron Boots\nMachine Learning Department\nCarnegie Mellon University\nPittsburgh, PA 15213\nbeb@cs.cmu.edu\nGeoffrey J. Gordon\nMachine Learning Department\nCarnegie Mellon University\nPittsburgh, PA 15213\nggordon@cs.cmu.edu\nAbstract\nWe propose a new approach to value function approximation which combines lin-\near temporal difference reinforcement learning with subspace identi\ufb01cation. In\npractical applications, reinforcement learning (RL) is complicated by the fact that\nstate is either high-dimensional or partially observable. Therefore, RL methods\nare designed to work with features of state rather than state itself, and the suc-\ncess or failure of learning is often determined by the suitability of the selected\nfeatures. By comparison, subspace identi\ufb01cation (SSID) methods are designed to\nselect a feature set which preserves as much information as possible about state.\nIn this paper we connect the two approaches, looking at the problem of reinforce-\nment learning with a large set of features, each of which may only be marginally\nuseful for value function approximation. We introduce a new algorithm for this\nsituation, called Predictive State Temporal Difference (PSTD) learning. As in\nSSID for predictive state representations, PSTD \ufb01nds a linear compression op-\nerator that projects a large set of features down to a small set that preserves the\nmaximum amount of predictive information. As in RL, PSTD then uses a Bellman\nrecursion to estimate a value function. We discuss the connection between PSTD\nand prior approaches in RL and SSID. We prove that PSTD is statistically consis-\ntent, perform several experiments that illustrate its properties, and demonstrate its\npotential on a dif\ufb01cult optimal stopping problem.\n1\nIntroduction and Related Work\nWe examine the problem of estimating a policy\u2019s value function within a decision process in a\nhigh dimensional and partially-observable environment, when the parameters of the process are\nunknown. In this situation, a common strategy is to employ a linear architecture and represent\nthe value function as a linear combination of features of (sequences of) observations. A popular\nfamily of model-free algorithms called temporal difference (TD) algorithms [1] can then be used\nto estimate the parameters of the value function. Least-squares TD (LSTD) algorithms [2, 3, 4]\nexploit the linearity of the value function to \ufb01nd the optimal parameters in a least-squares sense\nfrom time-adjacent samples of features.\nUnfortunately, choosing a good set of features is hard. The features must be predictive of future\nreward, and the set of features must be small relative to the amount of training data, or TD learning\nwill be prone to over\ufb01tting. The problem of selecting a small set of reasonable features has been\napproached from a number of different perspectives. In many domains, the features are selected by\nhand according to expert knowledge; however, this task can be dif\ufb01cult and time consuming in prac-\ntice. Therefore, a considerable amount of research has been devoted to the problem of automatically\nidentifying features that support value function approximation.\nMuch of this research is devoted to \ufb01nding sets of features when the dynamical system is known, but\nthe state space is large and dif\ufb01cult to work with. For example, in a large fully observable Markov\n1\narXiv:1011.0041v2  [cs.LG]  18 Jan 2011\ndecision process (MDP), it is often easier to estimate the value function from a low dimensional set\nof features than by using state directly. So, several approaches attempt to automatically discover a\nsmall set of features from a given larger description of an MDP, e.g., by using a spectral analysis\nof the state-space transition graph to discover a low-dimensional feature set that preserves the graph\nstructure [5, 6, 7].\nPartially observable Markov decision processes (POMDPs) extend MDPs to situations where the\nstate is not directly observable [8, 9, 10]. In this circumstance, an agent can plan using a continuous\nbelief state with dimensionality equal to the number of hidden states in the POMDP. When the num-\nber of hidden states is large, dimensionality reduction in POMDPs can be achieved by projecting a\nhigh dimensional belief space to a lower dimensional one; of course, the dif\ufb01culty is to \ufb01nd a projec-\ntion which preserves decision quality. Strategies for \ufb01nding good projections include value-directed\ncompression [11] and non-negative matrix factorization [12, 13]. The resulting model after compres-\nsion is a Predictive State Representation (PSR) [14, 15], an Observable Operator Model [16], or a\nmultiplicity automaton [17]. Moving to one of these representations can often compress a POMDP\nby a large factor with little or no loss in accuracy: examples exist with arbitrarily large lossless\ncompression factors, and in practice, we can often achieve large compression ratios with little loss.\nThe drawback of all of the approaches enumerated above is that they \ufb01rst assume that the dynamical\nsystem model is known, and only then give us a way of \ufb01nding a compact representation and a\nvalue function. In practice, we would like to be able to \ufb01nd a good set of features, without prior\nknowledge of the system model. Kolter and Ng [18] contend with this problem from a sparse feature\nselection standpoint. Given a large set of possibly-relevant features of observations, they proposed\naugmenting LSTD by applying an L1 penalty to the coef\ufb01cients, forcing LSTD to select a sparse set\nof features for value function estimation. The resulting algorithm, LARS-TD, works well in certain\nsituations (for example, see Section 5.1), but only if our original large set of features contains a small\nsubset of highly-relevant features.\nRecently, Parr et al. looked at the problem of value function estimation from the perspective of\nboth model-free and model-based reinforcement learning [19]. The model-free approach estimates\na value function directly from sample trajectories, i.e., from sequences of feature vectors of visited\nstates. The model-based approach, by contrast, \ufb01rst learns a model and then computes the value\nfunction from the learned model. Parr et al. compared LSTD (a model-free method) to a model-\nbased method in which we \ufb01rst learn a linear model by viewing features as a proxy for state (leading\nto a linear transition matrix that predicts future features from past features), and then compute a\nvalue function from this approximate model. Parr et al. demonstrated that these two approaches\ncompute exactly the same value function [19], formalizing a fact that has been recognized to some\ndegree before [2].\nIn the current paper, we build on this insight, while simultaneously \ufb01nding a compact set of features\nusing powerful methods from system identi\ufb01cation. First, we look at the problem of improving\nLSTD from a model-free predictive-bottleneck perspective: given a large set of features of history,\nwe devise a new TD method called Predictive State Temporal Difference (PSTD) learning that esti-\nmates the value function through a bottleneck that preserves only predictive information (Section 3).\nIntuitively, this approach has some of the same bene\ufb01ts as LARS-TD: by \ufb01nding a small set of pre-\ndictive features, we avoid over\ufb01tting and make learning more data-ef\ufb01cient. However, our method\ndiffers in that we identify a small subspace of features instead of a sparse subset of features. Hence,\nPSTD and LARS-TD are applicable in different situations: as we show in our experiments below,\nPSTD is better when we have many marginally-relevant features, while LARS-TD is better when\nwe have a few highly-relevant features hidden among many irrelevant ones.\nSecond, we look at the problem of value function estimation from a model-based perspective (Sec-\ntion 4). Instead of learning a linear transition model from features, as in [19], we use subspace\nidenti\ufb01cation [20, 21] to learn a PSR from our samples. Then we compute a value function via\nthe Bellman equations for our learned PSR. This new approach has a substantial bene\ufb01t: while the\nlinear feature-to-feature transition model of [19] does not seem to have any common uses outside\nthat paper, PSRs have been proposed numerous times on their own merits (including being invented\nindependently at least three times), and are a strict generalization of POMDPs.\nJust as Parr et al. showed for the two simpler methods, we show that our two improved methods\n(model-free and model-based) are equivalent. This result yields some appealing theoretical bene\ufb01ts:\n2\nfor example, PSTD features can be explicitly interpreted as a statistically consistent estimate of the\ntrue underlying system state. And, the feasibility of \ufb01nding the true value function can be shown\nto depend on the linear dimension of the dynamical system, or equivalently, the dimensionality of\nthe predictive state representation\u2014not on the cardinality of the POMDP state space. Therefore our\nrepresentation is naturally \u201ccompressed\u201d in the sense of [11], speeding up convergence.\nThe improved methods also yield practical bene\ufb01ts; we demonstrate these bene\ufb01ts with several ex-\nperiments. First, we compare PSTD to LSTD and LARS-TD on a synthetic example using different\nsets of features to illustrate the strengths and weaknesses of each algorithm. Next, we apply PSTD\nto a dif\ufb01cult optimal stopping problem for pricing high-dimensional \ufb01nancial derivatives. A signif-\nicant amount of work has gone into hand tuning features for this problem. We show that, if we add\na large number of weakly relevant features to these hand-tuned features, PSTD can \ufb01nd a predictive\nsubspace which performs much better than competing approaches, improving on the best previously\nreported result for this particular problem by a substantial margin.\nThe theoretical and empirical results reported here suggest that, for many applications where LSTD\nis used to compute a value function, PSTD can be simply substituted to produce better results.\n2\nValue Function Approximation\nWe start from a discrete time dynamical system with a set of states S, a set of actions A, a distribution\nover initial states \u03c00, a state transition function T, a reward function R, and a discount factor \u03b3 \u2208\n[0, 1]. We seek a policy \u03c0, a mapping from states to actions. The notion of a value function is of\ncentral importance in reinforcement learning: for a given policy \u03c0, the value of state s is de\ufb01ned\nas the expected discounted sum of rewards obtained when starting in state s and following policy\n\u03c0, J\u03c0(s) = E [P\u221e\nt=0 \u03b3tR(st) | s0 = s, \u03c0]. It is well known that the value function must obey the\nBellman equation\nJ\u03c0(s) = R(s) + \u03b3\nX\ns\u2032\nJ\u03c0(s\u2032) Pr[s\u2032 | s, \u03c0(s)]\n(1)\nIf we know the transition function T, and if the set of states S is suf\ufb01ciently small, we can use (1)\ndirectly to solve for the value function J\u03c0. We can then execute the greedy policy for J\u03c0, setting the\naction at each state to maximize the right-hand side of (1).\nHowever, we consider instead the harder problem of estimating the value function when s is a par-\ntially observable latent variable, and when the transition function T is unknown. In this situation,\nwe receive information about s through observations from a \ufb01nite set O. Our state (i.e., the informa-\ntion which we can use to make decisions) is not an element of S but a history (an ordered sequence\nof action-observation pairs h = ah\n1oh\n1 . . . ah\nt oh\nt that have been executed and observed prior to time\nt). If we knew the transition model T, we could use h to infer a belief distribution over S, and\nuse that belief (or a compression of that belief) as a state instead; below, we will discuss how to\nlearn a compressed belief state. Because of partial observability, we can only hope to predict reward\nconditioned on history, R(h) = E[R(s) | h], and we must choose actions as a function of history,\n\u03c0(h) instead of \u03c0(s).\nLet H be the set of all possible histories. H is often very large or in\ufb01nite, so instead of \ufb01nding a\nvalue separately for each history, we focus on value functions that are linear in features of histories\nJ\u03c0(s) = wT\u03c6H(h)\n(2)\nHere w \u2208Rj is a parameter vector and \u03c6H(h) \u2208Rj is a feature vector for a history h. So, we can\nrewrite the Bellman equation as\nwT\u03c6H(h) = R(h) + \u03b3\nX\no\u2208O\nwT\u03c6H(h\u03c0o) Pr[h\u03c0o | h\u03c0]\n(3)\nwhere h\u03c0o is history h extended by taking action \u03c0(h) and observing o.\n2.1\nLeast Squares Temporal Difference Learning\nIn general we don\u2019t know the transition probabilities Pr[h\u03c0o | h], but we do have samples of state\nfeatures \u03c6H\nt = \u03c6H(ht), next-state features \u03c6H\nt+1 = \u03c6H(ht+1), and immediate rewards Rt = R(ht).\n3\nWe can thus estimate the Bellman equation\nwT\u03c6H\n1:k \u2248R1:k + \u03b3wT\u03c6H\n2:k+1\n(4)\n(Here we have used the notation \u03c6H\n1:k to mean the matrix whose columns are \u03c6H\nt for t = 1 . . . k.)\nWe can can immediately attempt to estimate the parameter w by solving the linear system in the\nleast squares sense: \u02c6wT = R1:k\n\u0000\u03c6H\n1:k \u2212\u03b3\u03c6H\n2:k+1\n\u0001\u2020, where \u2020 indicates the Moore\u2013Penrose pseudo-\ninverse. However, this solution is biased [3], since the independent variables \u03c6H\nt \u2212\u03b3\u03c6H\nt+1 are noisy\nsamples of the expected difference E[\u03c6H(h) \u2212\u03b3 P\no\u2208O \u03c6H(h\u03c0o) Pr[h\u03c0o | h]]. In other words,\nestimating the value function parameters w is an error-in-variables problem.\nThe least squares temporal difference (LSTD) algorithm provides a consistent estimate of the in-\ndependent variables by right multiplying the approximate Bellman equation (Equation 4) by \u03c6H\nt\nT.\nThe quantity \u03c6H\nt\nT can be viewed as an instrumental variable [3], i.e., a measurement that is corre-\nlated with the true independent variables, but uncorrelated with the noise in our estimates of these\nvariables.1 The value function parameter w may then be estimated as follows:\n\u02c6wT = 1\nk\nk\nX\nt=1\nRt\u03c6H\nt\nT\n \n1\nk\nk\nX\nt=1\n\u03c6H\nt \u03c6H\nt\nT \u2212\u03b3\nk\nk\nX\nt=1\n\u03c6H\nt+1\u03c6H\nt\nT\n!\u22121\n(5)\nAs the amount of data k increases, the empirical covariance matrices \u03c6H\n1:k\u03c6H\n1:k\nT/k and\n\u03c6H\n2:k+1\u03c6H\n1:k\nT/k converge with probability 1 to their population values, and so our estimate of the ma-\ntrix to be inverted in (5) is consistent. Therefore, as long as this matrix is nonsingular, our estimate\nof the inverse is also consistent, and our estimate of w therefore converges to the true parameters\nwith probability 1.\n3\nPredictive Features\nAlthough LSTD provides a consistent estimate of the value function parameters w, in practice, the\npotential size of the feature vectors can be a problem. If the number of features is large relative to\nthe number of training samples, then the estimation of w is prone to over\ufb01tting. This problem can\nbe alleviated by choosing some small set of features that only contain information that is relevant\nfor value function approximation. However, with the exception of LARS-TD [18], there has been\nlittle work on the problem of how to select features automatically for value function approximation\nwhen the system model is unknown; and of course, manual feature selection depends on not-always-\navailable expert guidance.\nWe approach the problem of \ufb01nding a good set of features from a bottleneck perspective. That\nis, given some signal from history, in this case a large set of features, we would like to \ufb01nd a\ncompression that preserves only relevant information for predicting the value function J\u03c0. As we\nwill see in Section 4, this improvement is directly related to spectral identi\ufb01cation of PSRs.\n3.1\nTests and Features of the Future\nWe \ufb01rst need to de\ufb01ne precisely the task of predicting the future. Just as a history is an ordered\nsequence of action-observation pairs executed prior to time t, we de\ufb01ne a test of length i to be an\nordered sequence of action-observation pairs \u03c4 = a1o1 . . . aioi that can be executed and observed\nafter time t [14]. The prediction for a test \u03c4 after a history h, written \u03c4(h), is the probability that we\nwill see the test observations \u03c4 O = o1 . . . oi, given that we intervene [22] to execute the test actions\n\u03c4 A = a1 . . . ai:\n\u03c4(h) = Pr[\u03c4 O | h, do(\u03c4 A)]\nIf Q = {\u03c41, . . . , \u03c4n} is a set of tests, we write Q(h) = (\u03c41(h), . . . , \u03c4n(h))T for the corresponding\nvector of test predictions.\nWe can generalize the notion of a test to a feature of the future, a linear combination of several tests\nsharing a common action sequence. For example, if \u03c41 and \u03c42 are two tests with \u03c4 A\n1 = \u03c4 A\n2 \u2261\u03c4 A,\n1The LSTD algorithm can also be theoretically justi\ufb01ed as the result of an application of the Bellman\noperator followed by an orthogonal projection back onto the row space of \u03c6H [4].\n4\nthen we can make a feature \u03c6 = 3\u03c41 + \u03c42. This feature is executed if we intervene to do(\u03c4 A), and if\nit is executed its value is 3I(\u03c4 O\n1 )+I(\u03c4 O\n2 ), where I(o1 . . . oi) stands for an indicator random variable,\ntaking the value 0 or 1 depending on whether we observe the sequence of observations o1 . . . oi. The\nprediction of \u03c6 given h is \u03c6(h) \u2261E(\u03c6 | h, do(\u03c4 A)) = 3\u03c41(h) + \u03c42(h).\nWhile linear combinations of tests may seem restrictive, our de\ufb01nition is actually very expressive:\nwe can represent an arbitrary function of a \ufb01nite sequence of future observations. To do so, we take a\ncollection of tests, each of which picks out one possible realization of the sequence, and weight each\ntest by the value of the function conditioned on that realization. For example, if our observations are\nintegers 1, 2, . . . , 10, we can write the square of the next observation as P10\no=1 o2I(o), and the mean\nof the next two observations as P10\no=1\nP10\no\u2032=1\n1\n2(o + o\u2032)I(o, o\u2032).\nThe restriction to a common action sequence is necessary: without this restriction, all the tests\nmaking up a feature could never be executed at once. Once we move to feature predictions, however,\nit makes sense to lift this restriction: we will say that any linear combination of feature predictions\nis also a feature prediction, even if the features involved have different action sequences.\nAction sequences raise some problems with obtaining empirical estimates of means and covariances\nof features of the future: e.g., it is not always possible to get a sample of a particular feature\u2019s value\non every time step, and the feature we choose to sample at one step can restrict which features we\ncan sample at subsequent steps. In order to carry out our derivations without running into these\nproblems repeatedly, we will assume for the rest of the paper that we can reset our system after\nevery sample, and get a new history independently distributed as ht \u223c\u03c9 for some distribution \u03c9.\n(With some additional bookkeeping we could remove this assumption [23], but this bookkeeping\nwould unnecessarily complicate our derivations.)\nFurthermore, we will introduce some new language, again to keep derivations simple: if we have a\nvector of features of the future \u03c6T , we will pretend that we can get a sample \u03c6T\nt in which we evaluate\nall of our features starting from a single history ht, even if the different elements of \u03c6T require us\nto execute different action sequences. When our algorithms call for such a sample, we will instead\nuse the following trick to get a random vector with the correct expectation (and somewhat higher\nvariance, which doesn\u2019t matter for any of our arguments): write \u03c4 A\n1 , \u03c4 A\n2 , . . . for the different action\nsequences, and let \u03b61, \u03b62, . . . > 0 be a probability distribution over these sequences. We pick a single\naction sequence \u03c4 A\na according to \u03b6, and execute \u03c4 A\na to get a sample \u02c6\u03c6T of the features which depend\non \u03c4 A\na . We then enter \u02c6\u03c6T /\u03b6a into the corresponding coordinates of \u03c6T\nt , and \ufb01ll in zeros everywhere\nelse. It is easy to see that the expected value of our sample vector is then correct: the probability of\nselection \u03b6a and the weighting factor 1/\u03b6a cancel out. We will write E(\u03c6T | ht, do(\u03b6)) to stand for\nthis expectation.\nNone of the above tricks are actually necessary in our experiments with stopping problems: we\nsimply execute the \u201ccontinue\u201d action on every step, and use only sequences of \u201ccontinue\u201d actions in\nevery test and feature.\n3.2\nFinding Predictive Features Through a Bottleneck\nIn order to \ufb01nd a predictive feature compression, we \ufb01rst need to determine what we would like\nto predict. Since we are interested in value function approximation, the most relevant prediction is\nthe value function itself; so, we could simply try to predict total future discounted reward given a\nhistory. Unfortunately, total discounted reward has high variance, so unless we have a lot of data,\nlearning will be dif\ufb01cult.\nWe can reduce variance by including other prediction tasks as well. For example, predicting indi-\nvidual rewards at future time steps, while not strictly necessary to predict total discounted reward,\nseems highly relevant, and gives us much more immediate feedback. Similarly, future observations\nhopefully contain information about future reward, so trying to predict observations can help us pre-\ndict reward better. Finally, in any speci\ufb01c RL application, we may be able to add problem-speci\ufb01c\nprediction tasks that will help focus our attention on relevant information: for example, in a path-\nplanning problem, we might try to predict which of several goal states we will reach (in addition to\nhow much it will cost to get there).\n5\nWe can represent all of these prediction tasks as features of the future: e.g., to predict which goal we\nwill reach, we add a distinct observation at each goal state, or to predict individual rewards, we add\nindividual rewards as observations.2 We will write \u03c6T\nt for the vector of all features of the \u201cfuture at\ntime t,\u201d i.e., events starting at time t + 1 and continuing forward.\nSo, instead of remembering a large arbitrary set of features of history, we want to \ufb01nd a small\nsubspace of features of history that is relevant for predicting features of the future. We will call this\nsubspace a predictive compression, and we will write the value function as a linear function of only\nthe predictive compression of features.\nTo \ufb01nd our predictive compression, we will use reduced-rank regression [24]. We de\ufb01ne the follow-\ning empirical covariance matrices between features of the future and features of histories:\nb\u03a3T ,H = 1\nk\nk\nX\nt=1\n\u03c6T\nt \u03c6H\nt\nT\nb\u03a3H,H = 1\nk\nk\nX\nt=1\n\u03c6H\nt \u03c6H\nt\nT\n(6)\nLet LH be the lower triangular Cholesky factor of b\u03a3H,H. Then we can \ufb01nd a predictive compression\nof histories by a singular value decomposition (SVD) of the weighted covariance: write\nUDVT \u2248b\u03a3T ,HL\u2212T\nH\n(7)\nfor a truncated SVD [25] of the weighted covariance, where U are the left singular vectors, VT are\nthe right singular vectors, and D is the diagonal matrix of singular values. The number of columns\nof U, V, or D is equal to the number of retained singular values.3 Then we de\ufb01ne\nbU = UD1/2\n(8)\nto be the mapping from the low-dimensional compressed space up to the high-dimensional space of\nfeatures of the future.\nGiven bU, we would like to \ufb01nd a compression operator V that optimally predicts features of the\nfuture through the bottleneck de\ufb01ned by bU. The least squares estimate can be found by minimizing\nthe loss\nL(V ) =\n\r\r\r\u03c6T\n1:k \u2212bUV \u03c6H\n1:k\n\r\r\r\n2\nF\n(9)\nwhere \u2225\u00b7 \u2225F denotes the Frobenius norm. We can \ufb01nd the minimum by taking the derivative of this\nloss with respect to V , setting it to zero, and solving for V (see Appendix, Section A for details),\ngiving us:\nbV = arg min\nV\nL(V ) = bU Tb\u03a3T ,H(b\u03a3H,H)\u22121\n(10)\nBy weighting different features of the future differently, we can change the approximate compression\nin interesting ways. For example, as we will see in Section 4.2, scaling up future reward by a constant\nfactor results in a value-directed compression\u2014but, unlike previous ways to \ufb01nd value-directed\ncompressions [11], we do not need to know a model of our system ahead of time. For another\nexample, de\ufb01ne LT to be the lower triangular Cholesky factor of the empirical covariance of future\nfeatures b\u03a3T ,T . Then, if we scale features of the future by L\u2212T\nT , the singular value decomposition\nwill preserve the largest possible amount of mutual information between features of the future and\nfeatures of history. This is equivalent to canonical correlation analysis [26, 27], and the matrix D\nbecomes a diagonal matrix of canonical correlations between futures and histories.\n2If we don\u2019t wish to reveal extra information by adding additional observations, we can instead add the\ncorresponding feature predictions as observations; these predictions, by de\ufb01nition, reveal no additional infor-\nmation. To save the trouble of computing these predictions, we can use realized feature values rather than\npredictions in our learning algorithms below, at the cost of some extra variance: the expectation of the realized\nfeature value is the same as the expectation of the predicted feature value.\n3If our empirical estimate b\u03a3T ,H were exact, we could keep all nonzero singular values to \ufb01nd the smallest\npossible compression that does not lose any predictive power. In practice, though, there will be noise in our\nestimate, and b\u03a3T ,HL\u2212T\nH\nwill be full rank. If we know the true rank n of \u03a3T ,H, we can choose the \ufb01rst n\nsingular values to de\ufb01ne a subspace for compression. Or, we can choose a smaller subspace that results in an\napproximate compression: by selectively dropping columns of U corresponding to small singular values, we can\ntrade off compression against predictive power. Directions of larger variance in features of the future correspond\nto larger singular values in the SVD, so we minimize prediction error by truncating the smallest singular values.\nBy contrast with an SVD of the unscaled covariance, we do not attempt to minimize reconstruction error for\nfeatures of history, since features of history are standardized when we multiply by the inverse Cholesky factor.\n6\n3.3\nPredictive State Temporal Difference Learning\nNow that we have found a predictive compression operator bV via Equation 10, we can replace the\nfeatures of history \u03c6H\nt with the compressed features bV \u03c6H\nt in the Bellman recursion, Equation 4.\nDoing so results in the following approximate Bellman equation:\nwT bV \u03c6H\n1:k \u2248R1:k + \u03b3wT bV \u03c6H\n2:k+1\n(11)\nThe least squares solution for w is still prone to an error-in-variables problem. The variable \u03c6H\nis still correlated with the true independent variables and uncorrelated with noise, and so we can\nagain use it as an instrumental variable to unbias the estimate of w. De\ufb01ne the additional empirical\ncovariance matrices:\nb\u03a3R,H = 1\nk\nk\nX\nt=1\nRt\u03c6H\nt\nT\nb\u03a3H+,H = 1\nk\nk\nX\nt=1\n\u03c6H\nt+1\u03c6H\nt\nT\n(12)\nThen, the corrected Bellman equation is:\n\u02c6wT bV b\u03a3H,H = b\u03a3R,H + \u03b3 \u02c6wT bV b\u03a3H+,H\nand solving for \u02c6w gives us the Predictive State Temporal Difference (PSTD) learning algorithm:\n\u02c6wT = b\u03a3R,H\n\u0010\nbV b\u03a3H,H \u2212\u03b3 bV b\u03a3H+,H\n\u0011\u2020\n(13)\nSo far we have provided some intuition for why predictive features should be better than arbitrary\nfeatures for temporal difference learning. Below we will show an additional bene\ufb01t: the model-\nfree algorithm in Equation 13 is, under some circumstances, equivalent to a model-based value\nfunction approximation method which uses subspace identi\ufb01cation to learn Predictive State Repre-\nsentations [20, 21].\n4\nPredictive State Representations\nA predictive state representation (PSR) [14] is a compact and complete description of a dynami-\ncal system. Unlike POMDPs, which represent state as a distribution over a latent variable, PSRs\nrepresent state as a set of predictions of tests.\nFormally, a PSR consists of \ufb01ve elements \u27e8A, O, Q, s1, F\u27e9. A is a \ufb01nite set of possible actions,\nand O is a \ufb01nite set of possible observations. Q is a core set of tests, i.e., a set whose vector of\npredictions Q(h) is a suf\ufb01cient statistic for predicting the success probabilities of all tests. F is\nthe set of functions f\u03c4 which embody these predictions: \u03c4(h) = f\u03c4(Q(h)). And, m1 = Q(\u03f5) is\nthe initial prediction vector. In this work we will restrict ourselves to linear PSRs, in which all\nprediction functions are linear: f\u03c4(Q(h)) = rT\n\u03c4 Q(h) for some vector r\u03c4 \u2208R|Q|. Finally, a core set\nQ for a linear PSR is said to be minimal if the tests in Q are linearly independent [16, 15], i.e., no\none test\u2019s prediction is a linear function of the other tests\u2019 predictions.\nSince Q(h) is a suf\ufb01cient statistic for all tests, it is a state for our PSR: i.e., we can remember just\nQ(h) instead of h itself. After action a and observation o, we can update Q(h) recursively: if we\nwrite Mao for the matrix with rows rT\nao\u03c4 for \u03c4 \u2208Q, then we can use Bayes\u2019 Rule to show:\nQ(hao) =\nMaoQ(h)\nPr[o | h, do(a)] =\nMaoQ(h)\nmT\u221eMaoQ(h)\n(14)\nwhere m\u221eis a normalizer, de\ufb01ned by mT\n\u221eQ(h) = 1 for all h.\nIn addition to the above PSR parameters, we need a few additional de\ufb01nitions for reinforcement\nlearning: a reward function R(h) = \u03b7TQ(h) mapping predictive states to immediate rewards, a\ndiscount factor \u03b3 \u2208[0, 1] which weights the importance of future rewards vs. present ones, and a\npolicy \u03c0(Q(h)) mapping from predictive states to actions. (Specifying a reward in terms of the core\ntest predictions Q(h) is fully general: e.g., if we want to add a unit reward for some test \u03c4 \u0338\u2208Q, we\ncan instead equivalently set \u03b7 := \u03b7 + r\u03c4, where r\u03c4 is de\ufb01ned (as above) so that \u03c4(h) = rT\n\u03c4 Q(h).)\nInstead of ordinary PSRs, we will work with transformed PSRs (TPSRs) [20, 21]. TPSRs are a\ngeneralization of regular PSRs: a TPSR maintains a small number of suf\ufb01cient statistics which are\n7\nlinear combinations of a (potentially very large) set of test probabilities. That is, a TPSR maintains\na small number of feature predictions instead of test predictions. TPSRs have exactly the same\npredictive abilities as regular PSRs, but are invariant under similarity transforms: given an invertible\nmatrix S, we can transform m1 \u2192Sm1, mT\n\u221e\u2192mT\n\u221eS\u22121, and Mao \u2192SMaoS\u22121 without changing\nthe corresponding dynamical system, since pairs S\u22121S cancel in Eq. 14. The main bene\ufb01t of TPSRs\nover regular PSRs is that, given any core set of tests, low dimensional parameters can be found\nusing spectral matrix decomposition and regression instead of combinatorial search. In this respect,\nTPSRs are closely related to the transformed representations of LDSs and HMMs found by subspace\nidenti\ufb01cation [28, 29, 27, 30].\n4.1\nLearning Transformed PSRs\nLet Q be a minimal core set of tests for a dynamical system, with cardinality n = |Q| equal to the\nlinear dimension of the system. Then, let T be a larger core set of tests (not necessarily minimal,\nand possibly even with |T | countably in\ufb01nite). And, let H be the set of all possible histories. (|H| is\n\ufb01nite or countably in\ufb01nite, depending on whether our system is \ufb01nite-horizon or in\ufb01nite-horizon.)\nAs before, write \u03c6H\nt \u2208R\u2113for a vector of features of history at time t, and write \u03c6T\nt \u2208R\u2113for a vector\nof features of the future at time t. Since T is a core set of tests, by de\ufb01nition we can compute any test\nprediction \u03c4(h) as a linear function of T (h). And, since feature predictions are linear combinations\nof test predictions, we can also compute any feature prediction \u03c6(h) as a linear function of T (h).\nWe de\ufb01ne the matrix \u03a6T \u2208R\u2113\u00d7|T | to embody our predictions of future features: that is, an entry of\n\u03a6T is the weight of one of the tests in T for calculating the prediction of one of the features in \u03c6T .\nBelow we de\ufb01ne several covariance matrices, Equation 15(a\u2013d), in terms of the observable quantities\n\u03c6T\nt , \u03c6H\nt , at, and ot, and show how these matrices relate to the parameters of the underlying PSR.\nThese relationships then lead to our learning algorithm, Eq. 17 below.\nFirst we de\ufb01ne \u03a3H,H, the covariance matrix of features of histories, as E[\u03c6H\nt \u03c6H\nt\nT | ht \u223c\u03c9]. Given\nk samples, we can approximate this covariance:\n[b\u03a3H,H]i,j = 1\nk\nk\nX\nt=1\n\u03c6H\nit \u03c6H\njt =\u21d2b\u03a3H,H = 1\nk \u03c6H\n1:k\u03c6H\n1:k\nT.\n(15a)\nAs k \u2192\u221e, the empirical covariance b\u03a3H,H converges to the true covariance \u03a3H,H with probability 1.\nNext we de\ufb01ne \u03a3S,H, the cross covariance of states and features of histories. Writing st = Q(ht)\nfor the (unobserved) state at time t, let\n\u03a3S,H = E\n\u00141\nk s1:k\u03c6H\n1:k\nT\n\f\f\f\f ht \u223c\u03c9 (\u2200t)\n\u0015\nWe cannot directly estimate \u03a3S,H from data, but this matrix will appear as a factor in several of the\nmatrices that we de\ufb01ne below.\n8\nNext we de\ufb01ne \u03a3T ,H, the cross covariance matrix of the features of tests and histories: \u03a3T ,H \u2261\nE[\u03c6T\nt \u03c6H\nt\nT | ht \u223c\u03c9, do(\u03b6)]. The true covariance is the expectation of the sample covariance b\u03a3T ,H:\n[b\u03a3T ,H]i,j \u22611\nk\nk\nX\nt=1\n\u03c6T\ni,t\u03c6H\nj,t\n[\u03a3T ,H]i,j = E\n\"\n1\nk\nk\nX\nt=1\n\u03c6T\ni,t\u03c6H\nj,t\n\f\f\f\f\f ht \u223c\u03c9 (\u2200t), do(\u03b6) (\u2200t)\n#\n= E\n\"\n1\nk\nk\nX\nt=1\nE\n\u0002\n\u03c6T\ni,t | ht, do(\u03b6)\n\u0003\n\u03c6H\nj,t\n\f\f\f\f\f ht \u223c\u03c9 (\u2200t), do(\u03b6) (\u2200t)\n#\n= E\n\"\n1\nk\nk\nX\nt=1\nX\n\u03c4\u2208T\n\u03a6T\ni,\u03c4\u03c4(ht)\u03c6H\nj,t\n\f\f\f\f\f ht \u223c\u03c9 (\u2200t)\n#\n= E\n\"\n1\nk\nk\nX\nt=1\nX\n\u03c4\u2208T\n\u03a6T\ni,\u03c4rT\n\u03c4 Q(ht)\u03c6H\nj,t\n\f\f\f\f\f ht \u223c\u03c9 (\u2200t)\n#\n=\nX\n\u03c4\u2208T\n\u03a6T\ni,\u03c4rT\n\u03c4 E\n\"\n1\nk\nk\nX\nt=1\nQ(ht)\u03c6H\nj,t\n\f\f\f\f\f ht \u223c\u03c9 (\u2200t)\n#\n=\nX\n\u03c4\u2208T\n\u03a6T\ni,\u03c4rT\n\u03c4 E\n\"\n1\nk\nk\nX\nt=1\nst\u03c6H\nj,t\n\f\f\f\f\f ht \u223c\u03c9 (\u2200t)\n#\n=\u21d2\u03a3T ,H = \u03a6T R\u03a3S,H\n(15b)\nwhere the vector r\u03c4 is the linear function that speci\ufb01es the probability of the test \u03c4 given the proba-\nbilities of tests in the core set Q, and the matrix R has all of the r\u03c4 vectors as rows.\nThe above derivation shows that, because of our assumptions about the linear dimension of the\nsystem, the matrix \u03a3T ,H has factors R \u2208R|T |\u00d7n and \u03a3S,H \u2208Rn\u00d7\u2113. Therefore, the rank of \u03a3T ,H\nis no more than n, the linear dimension of the system. We can also see that, since the size of \u03a3T ,H\nis \ufb01xed but the number of samples k is increasing, the empirical covariance b\u03a3T ,H converges to the\ntrue covariance \u03a3T ,H with probability 1.\nNext we de\ufb01ne \u03a3H,ao,H, a set of matrices, one for each action-observation pair, that represent the\ncovariance between features of history before and after taking action a and observing o. In the\nfollowing, It(o) is an indicator variable for whether we see observation o at step t.\nb\u03a3H,ao,H \u22611\nk\nk\nX\nt=1\n\u03c6H\nt+1It(o)\u03c6H\nt\nT\n\u03a3H,ao,H \u2261E\nh\nb\u03a3H,ao,H\n\f\f\f ht \u223c\u03c9 (\u2200t), do(a) (\u2200t)\ni\n= E\n\"\n1\nk\nk\nX\nt=1\n\u03c6H\nt+1It(o)\u03c6H\nt\nT\n\f\f\f\f\f ht \u223c\u03c9 (\u2200t), do(a) (\u2200t)\n#\n(15c)\nSince the dimensions of each b\u03a3H,ao,H are \ufb01xed, as k \u2192\u221ethese empirical covariances converge to\nthe true covariances \u03a3H,ao,H with probability 1.\n9\nFinally we de\ufb01ne \u03a3R,H \u2261E[Rt\u03c6H\nt\nT | ht \u223c\u03c9], and approximate the covariance (in this case a\nvector) of reward and features of history:\nb\u03a3R,H \u22611\nk\nk\nX\nt=1\nRt\u03c6H\nt\nT\n\u03a3R,H \u2261E\nh\nb\u03a3R,H\n\f\f\f ht \u223c\u03c9 (\u2200t)\ni\n= E\n\"\n1\nk\nk\nX\nt=1\nRt\u03c6H\nt\nT\n\f\f\f\f\f ht \u223c\u03c9 (\u2200t)\n#\n= E\n\"\n1\nk\nk\nX\nt=1\n\u03b7TQ(ht)\u03c6H\nt\nT\n\f\f\f\f\f ht \u223c\u03c9 (\u2200t)\n#\n= \u03b7TE\n\"\n1\nk\nk\nX\nt=1\nst\u03c6H\nt\nT\n\f\f\f\f\f ht \u223c\u03c9 (\u2200t)\n#\n= \u03b7T\u03a3S,H\n(15d)\nAgain, as k \u2192\u221e, b\u03a3R,H converges to \u03a3R,H with probability 1.\nWe now wish to use the above-de\ufb01ned matrices to learn a TPSR from data. To do so we need to\nmake a somewhat-restrictive assumption: we assume that our features of history are rich enough to\ndetermine the state of the system, i.e., the regression from \u03c6H to s is exact: st = \u03a3S,H\u03a3\u22121\nH,H\u03c6H\nt .\nWe discuss how to relax this assumption below in Section 4.3. We also need a matrix U such that\nU T\u03a6T R is invertible; with probability 1 a random matrix satis\ufb01es this condition, but as we will see\nbelow, it is useful to choose U via SVD of a scaled version of \u03a3T ,H as described in Sec. 3.2.\nUsing our assumptions we can show a useful identity for \u03a3H,ao,H:\n\u03a3S,H\u03a3\u22121\nH,H\u03a3H,ao,H = E\n\"\n1\nk\nk\nX\nt=1\n\u03a3S,H\u03a3\u22121\nH,H\u03c6H\nt+1It(o)\u03c6H\nt\nT\n\f\f\f\f\f ht \u223c\u03c9 (\u2200t), do(a) (\u2200t)\n#\n= E\n\"\n1\nk\nk\nX\nt=1\nst+1It(o)\u03c6H\nt\nT\n\f\f\f\f\f ht \u223c\u03c9 (\u2200t), do(a) (\u2200t)\n#\n= E\n\"\n1\nk\nk\nX\nt=1\nMaost\u03c6H\nt\nT\n\f\f\f\f\f ht \u223c\u03c9 (\u2200t)\n#\n= Mao\u03a3S,H\n(16)\nThis identity is at the heart of our learning algorithm: it shows that \u03a3H,ao,H contains a hidden copy\nof Mao, the main TPSR parameter that we need to learn. We would like to recover Mao via Eq. 16,\nMao = \u03a3S,H\u03a3\u22121\nH,H\u03a3H,ao,H\u03a3\u2020\nS,H; but of course we do not know \u03a3S,H. Fortunately, though, it turns\nout that we can use U T\u03a3T ,H as a stand-in, as described below, since this matrix differs from \u03a3S,H\nonly by an invertible transform (Eq. 15b).\nWe now show how to recover a TPSR from the matrices \u03a3T ,H, \u03a3H,H, \u03a3R,H, \u03a3H,ao,H, and U.\nSince a TPSR\u2019s predictions are invariant to a similarity transform of its parameters, our algorithm\n10\nonly recovers the TPSR parameters to within a similarity transform.\nbt \u2261U T\u03a3T ,H(\u03a3H,H)\u22121\u03c6H\nt\n= U T\u03a6T R\u03a3S,H(\u03a3H,H)\u22121\u03c6H\nt\n= (U T\u03a6T R)st\n(17a)\nBao \u2261U T\u03a3T ,H(\u03a3H,H)\u22121\u03a3H,ao,H(U T\u03a3T ,H)\u2020\n= U T\u03a6T R\u03a3S,H(\u03a3H,H)\u22121\u03a3H,ao,H(U T\u03a3T ,H)\u2020\n= (U T\u03a6T R)Mao \u03a3S,H(U T\u03a3T ,H)\u2020\n= (U T\u03a6T R)Mao(U T\u03a6T R)\u22121(U T\u03a6T R)\u03a3S,H(U T\u03a3T ,H)\u2020\n= (U T\u03a6T R)Mao(U T\u03a6T R)\u22121\n(17b)\nbT\n\u03b7 \u2261\u03a3R,H(U T\u03a3T ,H)\u2020\n= \u03b7T\u03a3S,H(U T\u03a3T ,H)\u2020\n= \u03b7T(U T\u03a6T R)\u22121(U T\u03a6T R)\u03a3S,H(U T\u03a3T ,H)\u2020\n= \u03b7T(U T\u03a6T R)\u22121\n(17c)\nOur PSR learning algorithm is simple: simply replace each true covariance matrix in Eq. 17 by its\nempirical estimate. Since the empirical estimates converge to their true values with probability 1 as\nthe sample size increases, our learning algorithm is clearly statistically consistent.\n4.2\nPredictive State Temporal Difference Learning (Revisited)\nFinally, we are ready to show that the model-free PSTD learning algorithm introduced in Section 3.3\nis equivalent to a model-based algorithm built around PSR learning. For a \ufb01xed policy \u03c0, a TPSR\u2019s\nvalue function is a linear function of state, J\u03c0(s) = wTb, and is the solution of the TPSR Bellman\nequation [31]: for all b, wTb = bT\n\u03b7b + \u03b3 P\no\u2208O wTB\u03c0ob, or equivalently,\nwT = bT\n\u03b7 + \u03b3\nX\no\u2208O\nwTB\u03c0o\nIf we substitute in our learned PSR parameters from Equations 17(a\u2013c), we get\n\u02c6wT = b\u03a3R,H(U Tb\u03a3T ,H)\u2020 + \u03b3\nX\no\u2208O\n\u02c6wTU Tb\u03a3T ,H(b\u03a3H,H)\u22121b\u03a3H,\u03c0o,H(U Tb\u03a3T ,H)\u2020\n\u02c6wTU Tb\u03a3T ,H = b\u03a3R,H + \u03b3 \u02c6wTU Tb\u03a3T ,H(b\u03a3H,H)\u22121b\u03a3H+,H\nsince, by comparing Eqs. 15c and 12, we can see that P\no\u2208O b\u03a3H,\u03c0o,H = b\u03a3H+,H. Now, suppose\nthat we de\ufb01ne bU and bV by Eqs. 8 and 10, and let U = bU as suggested above in Sec. 4.1. Then\nU Tb\u03a3T ,H = bV b\u03a3H,H, and\n\u02c6wT bV b\u03a3H,H = b\u03a3R,H + \u03b3 \u02c6wT bV b\u03a3H+,H\n\u02c6wT = b\u03a3R,H\n\u0010\nbV b\u03a3H,H \u2212\u03b3 bV b\u03a3H+,H\n\u0011\u2020\n(18)\nEq. 18 is exactly the PSTD algorithm (Eq. 13). So, we have shown that, if we learn a PSR by the\nsubspace identi\ufb01cation algorithm of Sec. 4.1 and then compute its value function via the Bellman\nequation, we get the exact same answer as if we had directly learned the value function via the\nmodel-free PSTD method. In addition to adding to our understanding of both methods, an important\ncorollary of this result is that PSTD is a statistically consistent algorithm for PSR value function\napproximation\u2014to our knowledge, the \ufb01rst such result for a TD method.\nPSTD learning is related to value-directed compression of POMDPs [11]. If we learn a TPSR from\ndata generated by a POMDP, then the TPSR state is exactly a linear compression of the POMDP\nstate [15, 20]. The compression can be exact or approximate, depending on whether we include\nenough features of the future and whether we keep all or only some nonzero singular values in our\nbottleneck. If we include only reward as a feature of the future, we get a value-directed compression\n11\nin the sense of Poupart and Boutilier [11]. If desired, we can tune the degree of value-directedness\nof our compression by scaling the relative variance of our features: the higher the variance of the\nreward feature compared to other features, the more value-directed the resulting compression will\nbe. Our work signi\ufb01cantly diverges from previous work on POMDP compression in one important\nrespect: prior work assumes access to the true POMDP model, while we make no such assumption,\nand learn a compressed representation directly from data.\n4.3\nInsights from Subspace Identi\ufb01cation\nThe close connection to subspace identi\ufb01cation for PSRs provides additional insight into the tem-\nporal difference learning procedure. In Equation 17 we made the assumption that the features\nof history are rich enough to completely determine the state of the dynamical system. In fact,\nusing theory developed in [21], it is possible to relax this assumption and instead assume that\nstate is merely correlated with features of history. In this case, we need to introduce a new set\nof covariance matrices \u03a3T ,ao,H \u2261E[\u03c6T\nt It(o)\u03c6H\nt\nT | ht \u223c\u03c9, do(a, \u03b6)], one for each action-\nobservation pair, that represent the covariance between features of history before and features\nof tests after taking action a and observing o.\nWe can then estimate the TPSR transition ma-\ntrices as bBao = bU Tb\u03a3T ,ao,H(bU Tb\u03a3T ,H)\u2020 (see [21] for proof details).\nThe value function pa-\nrameter w can be estimated as \u02c6wT = b\u03a3R,H(bU Tb\u03a3T ,H)\u2020(I \u2212P\no\u2208O bU Tb\u03a3T ,ao,H(bU Tb\u03a3T ,H)\u2020)\u2020 =\nb\u03a3R,H(bU Tb\u03a3T ,H \u2212P\no\u2208O bU Tb\u03a3T ,ao,H)\u2020 (the proof is similar to Equation 18). Since we no longer\nassume that state is completely speci\ufb01ed by features of history, we can no longer apply the learned\nvalue function to bU\u03a3T ,H(\u03a3H,H)\u22121\u03c6t at each time t. Instead we need to learn a full PSR model and\n\ufb01lter with the model to estimate state. Details on this procedure can be found in [21].\n5\nExperimental Results\nWe designed several experiments to evaluate the properties of the PSTD learning algorithm. In\nthe \ufb01rst set of experiments we look at the comparative merits of PSTD with respect to LSTD and\nLARS-TD when applied to the problem of estimating the value function of a reduced-rank POMDP.\nIn the second set of experiments, we apply PSTD to a benchmark optimal stopping problem (pricing\na \ufb01ctitious \ufb01nancial derivative), and show that PSTD outperforms competing approaches.\n5.1\nEstimating the Value Function of a RR-POMDP\nWe evaluate the PSTD learning algorithm on a synthetic example derived from [32]. The problem is\nto \ufb01nd the value function of a policy in a partially observable Markov decision Process (POMDP).\nThe POMDP has 4 latent states, but the policy\u2019s transition matrix is low rank: the resulting belief\ndistributions can be represented in a 3-dimensional subspace of the original belief simplex. A reward\nof 1 is given in the \ufb01rst and third latent state and a reward of 0 in the other two latent states (see\nAppendix, Section B). The system emits 2 possible observations, con\ufb02ating information about the\nlatent states.\nWe perform 3 experiments, comparing the performance of LSTD, LARS-TD, PSTD, and PSTD as\nformulated in Section 4.3 (which we call PSTD2) when different sets of features are used. In each\ncase we compare the value function estimated by each algorithm to the true value function computed\nby J\u03c0 = R(I \u2212\u03b3T \u03c0)\u22121.\nIn the \ufb01rst experiment we execute the policy \u03c0 for 1000 time steps. We split the data into overlapping\nhistories and tests of length 5, and sample 10 of these histories and tests to serve as centers for\nGaussian radial basis functions. We then evaluate each basis function at every remaining sample.\nThen, using these features, we learned the value function using LSTD, LARS-TD, PSTD with linear\ndimension 3, and PSTD2 with linear dimension 3 (Figure 1(A)).4 In this experiment, PSTD and\nPSTD2 both had lower mean squared error than the other approaches. For the second experiment,\nwe added 490 random features to the 10 good features and then attempted to learn the value function\nwith each of the 3 algorithms (Figure 1(B)). In this case, LSTD and PSTD both had dif\ufb01culty \ufb01tting\n4Comparing LSTD and PSTD is straightforward; the two methods differ only by the compression operator\nbV .\n12\n1\n2\n3\n4\n0\n5\n10\n15\n20\n25\n30\n0.95\n1.00\n1.05\n1.10\n1.15\n1.20\n1.25\n1.30\nSquared Error (log scale)\nState\nState\nState\nExpected Reward\nPolicy Iteration\nA.\nB.\nC.\nD.\nLSTD (16)\nLSTD\nPSTD\nLARS-TD\nThreshold\nLSTD\nPSTD\nLARS-TD\n0\n10\n1\n10\n2\n10\n3\n10\n4\n10\n5\n10\n\u22121\n10\n0\n10\n1\n10\n2\n10\n3\n10\n4\n1\n2\n3\n4\n10\n\u22122\n10\n0\n10\n2\n10\n4\n10\n6\n10\n8\n10\n10\n1\n2\n3\n4\nPSTD2\nFigure 1: Experimental Results. Error bars indicate standard error. (A) Estimating the value function\nwith a small number of informative features. PSTD and PSTD2 both do well. (B) Estimating the\nvalue function with a small set of informative features and a large set of random features. LARS-TD\nis designed for this scenario and dramatically outperforms PSTD and LSTD, however it does not\noutperform PSTD2. (C) Estimating the value function with a large set of semi-informative features.\nPSTD is able to determine a small set of compressed features that retain the maximal amount of\ninformation about the value function, outperforming LSTD by a very large margin. (D) Pricing a\nhigh-dimensional derivative via policy iteration. The y-axis is expected reward for the current policy\nat each iteration. The optimal threshold strategy (sell if price is above a threshold [33]) is in black,\nLSTD (16 canonical features) is in blue, LSTD (on the full 220 features) is cyan, LARS-TD (feature\nselection from set of 220) is in green, and PSTD (16 dimensions, compressing 220 features (16 +\n204)) is in red.\nthe value function due to the large number of irrelevant features in both tests and histories and the\nrelatively small amount of training data. LARS-TD, designed for precisely this scenario, was able\nto select the 10 relevant features and estimate the value function better by a substantial margin.\nSurprisingly, in this experiment PSTD2 not only outperformed PSTD but bested even LARS-TD.\nFor the third experiment, we increased the number of sampled features from 10 to 500. In this case,\neach feature was somewhat relevant, but the number of features was relatively large compared to the\namount of training data. This situation occurs frequently in practice: it is often easy to \ufb01nd a large\nnumber of features that are at least somewhat related to state. PSTD and PSTD2 both outperform\nLARS-TD and each of these subspace and subset selection methods outperform LSTD by a large\nmargin by ef\ufb01ciently estimating the value function (Figure 1(C)).\n5.2\nPricing A High-dimensional Financial Derivative\nDerivatives are \ufb01nancial contracts with payoffs linked to the future prices of basic assets such as\nstocks, bonds and commodities. In some derivatives the contract holder has no choices, but in\nmore complex cases, the contract owner must make decisions\u2014e.g., with early exercise the contract\nholder can decide to terminate the contract at any time and receive payments based on prevailing\nmarket conditions. In these cases, the value of the derivative depends on how the contract holder\nacts. Deciding when to exercise is therefore an optimal stopping problem: at each point in time,\nthe contract holder must decide whether to continue holding the contract or exercise. Such stopping\nproblems provide an ideal testbed for policy evaluation methods, since we can easily collect a single\ndata set which is suf\ufb01cient to evaluate any policy: we just choose the \u201ccontinue\u201d action forever. (We\ncan then evaluate the \u201cstop\u201d action easily in any of the resulting states, since the immediate reward\nis given by the rules of the contract, and the next state is the terminal state by de\ufb01nition.)\nWe consider the \ufb01nancial derivative introduced by Tsitsiklis and Van Roy [33]. The derivative\ngenerates payoffs that are contingent on the prices of a single stock. At the end of a given day, the\nholder may opt to exercise. At exercise the owner receives a payoff equal to the current price of the\nstock divided by the price 100 days beforehand. We can think of this derivative as a \u201cpsychic call\u201d:\nthe owner gets to decide whether s/he would like to have bought an ordinary 100-day European call\noption, at the then-current market price, 100 days ago.\nIn our simulation (and unknown to the investor), the underlying stock price follows a geometric\nBrownian motion with volatility \u03c3 = 0.02 and continuously compounded short term growth rate\n13\n\u03c1 = 0.0004. Assuming stock prices \ufb02uctuate only on days when the market is open, these parameters\ncorrespond to an annual growth rate of \u223c10%. In more detail, if wt is a standard Brownian motion,\nthen the stock price pt evolves as \u2207pt = \u03c1pt\u2207t + \u03c3pt\u2207wt, and we can summarize relevant state\nat the end of each day as a vector xt \u2208R100, with xt =\n\u0010\npt\u221299\npt\u2212100 , pt\u221298\npt\u2212100 , . . . ,\npt\npt\u2212100\n\u0011T\n. The ith\ndimension xt(i) represents the amount a $1 investment in a stock at time t \u2212100 would grow to at\ntime t \u2212100 + i. This process is Markov and ergodic [33, 34]: xt and xt+100 are independent and\nidentically distributed. The immediate reward for exercising the option is G(x) = x(100), and the\nimmediate reward for continuing to hold the option is 0. The discount factor \u03b3 = e\u2212\u03c1 is determined\nby the growth rate; this corresponds to assuming that the risk-free interest rate is equal to the stock\u2019s\ngrowth rate, meaning that the investor gains nothing in expectation by holding the stock itself.\nThe value of the derivative, if the current state is x, is given by V \u2217(x) = supt E[\u03b3tG(xt) | x0 = x].\nOur goal is to calculate an approximate value function V (x) = wT\u03c6H(x), and then use this value\nfunction to generate a stopping time min{t | G(xt) \u2265V (xt)}. To do so, we sample a sequence\nof 1,000,000 states xt \u2208R100 and calculate features \u03c6H of each state. We then perform policy\niteration on this sample, alternately estimating the value function under a given policy and then\nusing this value function to de\ufb01ne a new greedy policy \u201cstop if G(xt) \u2265wT\u03c6H(xt).\u201d\nWithin the above strategy, we have two main choices: which features do we use, and how do we\nestimate the value function in terms of these features. For value function estimation, we used LSTD,\nLARS-TD, or PSTD. In each case we re-used our 1,000,000-state sample trajectory for all iterations:\nwe start at the beginning and follow the trajectory as long as the policy chooses the \u201ccontinue\u201d action,\nwith reward 0 at each step. When the policy executes the \u201cstop\u201d action, the reward is G(x) and the\nnext state\u2019s features are all 0; we then restart the policy 100 steps in the future, after the process\nhas fully mixed. For feature selection, we are fortunate: previous researchers have hand-selected a\n\u201cgood\u201d set of 16 features for this data set through repeated trial and error (see Appendix, Section B\nand [33, 34]). We greatly expand this set of features, then use PSTD to synthesize a small set of high-\nquality combined features. Speci\ufb01cally, we add the entire 100-step state vector, the squares of the\ncomponents of the state vector, and several additional nonlinear features, increasing the total number\nof features from 16 to 220. We use histories of length 1, tests of length 5, and (for comparison\u2019s\nsake) we choose a linear dimension of 16. Tests (but not histories) were value-directed by reducing\nthe variance of all features except reward by a factor of 100.\nFigure 1D shows results. We compared PSTD (reducing 220 to 16 features) to LSTD with either\nthe 16 hand-selected features or the full 220 features, as well as to LARS-TD (220 features) and to\na simple thresholding strategy [33]. In each case we evaluated the \ufb01nal policy on 10,000 new ran-\ndom trajectories. PSTD outperformed each of its competitors, improving on the next best approach,\nLARS-TD, by 1.75 percentage points. In fact, PSTD performs better than the best previously re-\nported approach [33, 34] by 1.24 percentage points. These improvements correspond to appreciable\nfractions of the risk-free interest rate (which is about 4 percentage points over the 100 day window\nof the contract), and therefore to signi\ufb01cant arbitrage opportunities: an investor who doesn\u2019t know\nthe best strategy will consistently undervalue the security, allowing an informed investor to buy it\nfor below its expected value.\n6\nConclusion\nIn this paper, we attack the feature selection problem for temporal difference learning. Although\nwell-known temporal difference algorithms such as LSTD can provide asymptotically unbiased es-\ntimates of value function parameters in linear architectures, they can have trouble in \ufb01nite samples:\nif the number of features is large relative to the number of training samples, then they can have\nhigh variance in their value function estimates. For this reason, in real-world problems, a substantial\namount of time is spent selecting a small set of features, often by trial and error [33, 34].\nTo remedy this problem, we present the PSTD algorithm, a new approach to feature selection for\nTD methods, which demonstrates how insights from system identi\ufb01cation can bene\ufb01t reinforcement\nlearning. PSTD automatically chooses a small set of features that are relevant for prediction and\nvalue function approximation. It approaches feature selection from a bottleneck perspective, by\n\ufb01nding a small set of features that preserves only predictive information. Because of the focus\non predictive information, the PSTD approach is closely connected to PSRs: under appropriate\n14\nassumptions, PSTD\u2019s compressed set of features is asymptotically equivalent to TPSR state, and\nPSTD is a consistent estimator of the PSR value function.\nWe demonstrate the merits of PSTD compared to two popular alternative algorithms, LARS-TD\nand LSTD, on a synthetic example, and argue that PSTD is most effective when approximating a\nvalue function from a large number of features, each of which contains at least a little information\nabout state. Finally, we apply PSTD to a dif\ufb01cult optimal stopping problem, and demonstrate the\npractical utility of the algorithm by outperforming several alternative approaches and topping the\nbest reported previous results.\nAcknowledgements\nByron Boots was supported by the NSF under grant number EEEC-0540865. Byron Boots and\nGeoffrey J. Gordon were supported by ONR MURI grant number N00014-09-1-1052.\nReferences\n[1] R. S. Sutton. Learning to predict by the methods of temporal differences. Machine Learning,\n3(1):9\u201344, 1988.\n[2] Justin A. Boyan. Least-squares temporal difference learning. In Proc. Intl. Conf. Machine\nLearning, pages 49\u201356. Morgan Kaufmann, San Francisco, CA, 1999.\n[3] Steven J. Bradtke and Andrew G. Barto. Linear least-squares algorithms for temporal differ-\nence learning. In Machine Learning, pages 22\u201333, 1996.\n[4] Michail G. Lagoudakis and Ronald Parr. Least-squares policy iteration. J. Mach. Learn. Res.,\n4:1107\u20131149, 2003.\n[5] Sridhar Mahadevan. Representation policy iteration. In Proceedings of the Proceedings of the\nTwenty-First Conference Annual Conference on Uncertainty in Arti\ufb01cial Intelligence (UAI-05),\npages 372\u2013379, Arlington, Virginia, 2005. AUAI Press.\n[6] Sridhar Mahadevan. Samuel meets amarel: automating value function approximation using\nglobal state space analysis. In AAAI\u201905: Proceedings of the 20th national conference on Arti-\n\ufb01cial intelligence, pages 1000\u20131005. AAAI Press, 2005.\n[7] Jeff Johns, Sridhar Mahadevan, and Chang Wang. Compact spectral bases for value function\napproximation using kronecker factorization. In AAAI\u201907: Proceedings of the 22nd national\nconference on Arti\ufb01cial intelligence, pages 559\u2013564. AAAI Press, 2007.\n[8] K. J. Astr\u00a8om. Optimal control of Markov decision processes with incomplete state estimation.\nJournal of Mathematical Analysis and Applications, 10:174\u2013205, 1965.\n[9] E. J. Sondik. The Optimal Control of Partially Observable Markov Processes. PhD thesis,\nStanford University, 1971.\n[10] Anthony R. Cassandra, Leslie P. Kaelbling, and Michael R. Littman. Acting optimally in\npartially observable stochastic domains. In Proc. AAAI, 1994.\n[11] Pascal Poupart and Craig Boutilier. Value-directed compression of pomdps. In NIPS, pages\n1547\u20131554, 2002.\n[12] Xin Li, William K. W. Cheung, Jiming Liu, and Zhili Wu. A novel orthogonal nmf-based belief\ncompression for pomdps. In ICML \u201907: Proceedings of the 24th international conference on\nMachine learning, pages 537\u2013544, New York, NY, USA, 2007. ACM.\n[13] Georgios Theocharous and Sridhar Mahadevan. Compressing pomdps using locality preserv-\ning non-negative matrix factorization, 2010.\n[14] Michael Littman, Richard Sutton, and Satinder Singh. Predictive representations of state. In\nAdvances in Neural Information Processing Systems (NIPS), 2002.\n[15] Satinder Singh, Michael James, and Matthew Rudary. Predictive state representations: A new\ntheory for modeling dynamical systems. In Proc. UAI, 2004.\n[16] Herbert Jaeger. Observable operator models for discrete stochastic time series. Neural Com-\nputation, 12:1371\u20131398, 2000.\n15\n[17] Eyal Even-dar. Y.: Planning in pomdps using multiplicity automata. In In: Proceedings of 21st\nConference on Uncertainty in Arti\ufb01cial Intelligence (UAI, pages 185\u2013192, 2005.\n[18] J. Zico Kolter and Andrew Y. Ng. Regularization and feature selection in least-squares tempo-\nral difference learning. In ICML \u201909: Proceedings of the 26th Annual International Conference\non Machine Learning, pages 521\u2013528, New York, NY, USA, 2009. ACM.\n[19] Ronald Parr, Lihong Li, Gavin Taylor, Christopher Painter-Wake\ufb01eld, and Michael L. Littman.\nAn analysis of linear models, linear value-function approximation, and feature selection for\nreinforcement learning. In ICML \u201908: Proceedings of the 25th international conference on\nMachine learning, pages 752\u2013759, New York, NY, USA, 2008. ACM.\n[20] Matthew Rosencrantz, Geoffrey J. Gordon, and Sebastian Thrun. Learning low dimensional\npredictive representations. In Proc. ICML, 2004.\n[21] Byron Boots, Sajid M. Siddiqi, and Geoffrey J. Gordon. Closing the learning-planning loop\nwith predictive state representations. In Proceedings of Robotics: Science and Systems VI,\n2010.\n[22] Judea Pearl. Causality: models, reasoning, and inference. Cambridge University Press, 2000.\n[23] Michael Bowling, Peter McCracken, Michael James, James Neufeld, and Dana Wilkinson.\nLearning predictive state representations using non-blind policies. In Proc. ICML, 2006.\n[24] Gregory C. Reinsel and Rajabather Palani Velu. Multivariate Reduced-rank Regression: The-\nory and Applications. Springer, 1998.\n[25] Gene H. Golub and Charles F. Van Loan. Matrix Computations. The Johns Hopkins University\nPress, 1996.\n[26] Harold Hotelling. The most predictable criterion. Journal of Educational Psychology, 26:139\u2013\n142, 1935.\n[27] S. Soatto and A. Chiuso. Dynamic data factorization. Technical report, UCLA, 2001.\n[28] P. Van Overschee and B. De Moor. Subspace Identi\ufb01cation for Linear Systems: Theory, Im-\nplementation, Applications. Kluwer, 1996.\n[29] Tohru Katayama. Subspace Methods for System Identi\ufb01cation. Springer-Verlag, 2005.\n[30] Daniel Hsu, Sham Kakade, and Tong Zhang. A spectral algorithm for learning hidden Markov\nmodels. In COLT, 2009.\n[31] Michael R. James, Ton Wessling, and Nikos A. Vlassis. Improving approximate value iteration\nusing memories and predictive state representations. In AAAI, 2006.\n[32] Sajid Siddiqi, Byron Boots, and Geoffrey J. Gordon. Reduced-rank hidden Markov models. In\nProceedings of the Thirteenth International Conference on Arti\ufb01cial Intelligence and Statistics\n(AISTATS-2010), 2010.\n[33] John N. Tsitsiklis and Benjamin Van Roy. Optimal stopping of markov processes: Hilbert\nspace theory, approximation algorithms, and an application to pricing high-dimensional \ufb01nan-\ncial derivatives. IEEE Transactions on Automatic Control, 44:1840\u20131851, 1997.\n[34] David Choi and Benjamin Roy. A generalized kalman \ufb01lter for \ufb01xed point approximation and\nef\ufb01cient temporal-difference learning. Discrete Event Dynamic Systems, 16(2):207\u2013239, 2006.\n16\nAppendix\nA\nDetermining the Compression Operator\nWe \ufb01nd a compression operator V that optimally predicts test-features through the CCA bottleneck\nde\ufb01ned by bU. The least squares estimate can be found by minimizing the following loss\nL(V ) =\n\r\r\r\u03c6T\n1:k \u2212bUV \u03c6H\n1:k\n\r\r\r\n2\nF\nbV = arg min\nV\nL(V )\nwhere \u2225\u00b7 \u2225F denotes the Frobenius norm. We can \ufb01nd bV by taking a derivative of this loss L with\nrespect to V , setting it to zero, and solving for V\nL = 1\nk tr\n\u0010\n(\u03c6T\n1:k \u2212bUV \u03c6H\n1:k)(\u03c6T\n1:k \u2212bUV \u03c6H\n1:k)T\u0011\n= 1\nk tr\n\u0010\n\u03c6T\n1:k\nT\u03c6T\n1:k \u22122\u03c6T\n1:k\nT bUV \u03c6H\n1:k + \u03c6H\n1:k\nTV T bU T bUV \u03c6H\n1:k\n\u0011\n=\u21d2dL = \u22122tr\n\u0010\n\u03c6H\n1:k\nTdV T bU T\u03c6T\n1:k\n\u0011\n+ 2tr\n\u0010\n\u03c6H\n1:k\nTdV T bU T bUV b\u03c6H\n1:k\n\u0011\n=\u21d2dL = \u22122tr\n\u0010\ndV T bU T\u03c6T\n1:k\u03c6H\n1:k\nT\u0011\n+ 2tr\n\u0010\ndV T bU T bUV b\u03c6H\n1:k\u03c6H\n1:k\nT\u0011\n=\u21d2dL = \u22122tr\n\u0010\ndV T bU Tb\u03a3T ,H\n\u0011\n+ 2tr\n\u0010\ndV T bU T bUV b\u03a3H,H\n\u0011\n=\u21d2\ndL\ndV T = \u22122tr\n\u0010\nbU Tb\u03a3T ,H\n\u0011\n+ 2tr\n\u0010\nbU T bUV b\u03a3H,H\n\u0011\n=\u21d20 = \u2212bU Tb\u03a3T ,H + bU T bUV b\u03a3H,H\n=\u21d2bV = (bU T bU)\u22121 bU Tb\u03a3T ,H(b\u03a3H,H)\u22121\n= bU Tb\u03a3T ,H(b\u03a3H,H)\u22121\nB\nExperimental Results\nB.1\nRR-POMDP\nThe RR-POMDP parameters are:\n[m = 4 hidden states, n = 2 observations, k = 3 transition matrix rank].\nT \u03c0 =\n\uf8ee\n\uf8ef\uf8f0\n0.7829\n0.1036\n0.0399\n0.0736\n0.1036\n0.4237\n0.4262\n0.0465\n0.0399\n0.4262\n0.4380\n0.0959\n0.0736\n0.0465\n0.0959\n0.7840\n\uf8f9\n\uf8fa\uf8fb\nO =\n\u0014\n1\n0\n1\n0\n0\n1\n0\n1\n\u0015\nThe discount factor is \u03b3 = 0.9.\nB.2\nPricing a \ufb01nancial derivative\nBasis functions\nThe \ufb01st 16 are the basis functions suggested by Van Roy; for full description\nand justi\ufb01cation see [33, 34]. The \ufb01rst functions consist of a constant, the reward, the minimal and\n17\nmaximal returns, and how long ago they occurred:\n\u03c61(x) = 1\n\u03c62(x) = G(x)\n\u03c63(x) =\nmin\ni=1,...,100 x(i) \u22121\n\u03c64(x) =\nmax\ni=1,...,100 x(i) \u22121\n\u03c65(x) = arg min\ni=1,...,100\nx(i) \u22121\n\u03c66(x) = arg max\ni=1,...,100\nx(i) \u22121\nThe next set of basis functions summarize the characteristics of the basic shape of the 100 day\nsample path. They are the inner product of the path with the \ufb01rst four Legendre polynomial degrees.\nLet j = i/50 \u22121.\n\u03c67(x) =\n1\n100\n100\nX\ni=1\nx(i) \u22121\n\u221a\n2\n\u03c68(x) =\n1\n100\n100\nX\ni=1\nx(i)\nr\n3\n2j\n\u03c69(x) =\n1\n100\n100\nX\ni=1\nx(i)\nr\n5\n2\n\u00123j2 \u22121\n2\n\u0013\n\u03c610(x) =\n1\n100\n100\nX\ni=1\nx(i)\nr\n7\n2\n\u00125j3 \u22123j\n2\n\u0013\nNonlinear combinations of basis functions:\n\u03c611(x) = \u03c62(x)\u03c63(x)\n\u03c612(x) = \u03c62(x)\u03c64(x)\n\u03c613(x) = \u03c62(x)\u03c67(x)\n\u03c614(x) = \u03c62(x)\u03c68(x)\n\u03c615(x) = \u03c62(x)\u03c69(x)\n\u03c616(x) = \u03c62(x)\u03c610(x)\nIn order to improve our results, we added a large number of additional basis functions to these\nhand-picked 16. PSTD will compress these features for us, so we can use as many additional basis\nfunctions as we would like. First we de\ufb01ned 4 additional basis functions consisting of the inner\nproducts of the 100 day sample path with the 5th and 6th Legende polynomials and we added the\ncorresponding nonlinear combinations of basis functions:\n\u03c617(x) =\n1\n100\n100\nX\ni=1\nx(i)\nr\n9\n2\n\u001235j4 \u221230x2 + 3\n8\n\u0013\n\u03c618(x) =\n1\n100\n100\nX\ni=1\nx(i)\nr\n11\n2\n\u001263j5 \u221270j3 + 15j\n8\n\u0013\n\u03c619(x) = \u03c62(x)\u03c617(x)\n\u03c620(x) = \u03c62(x)\u03c618(x)\nFinally we added the the entire sample path and the squared sample path:\n\u03c621:120 = x1:100\n\u03c6121:220 = x2\n1:100\n18\n",
        "sentence": "",
        "context": "2010.\n[22] Judea Pearl. Causality: models, reasoning, and inference. Cambridge University Press, 2000.\n[23] Michael Bowling, Peter McCracken, Michael James, James Neufeld, and Dana Wilkinson.\nAcknowledgements\nByron Boots was supported by the NSF under grant number EEEC-0540865. Byron Boots and\nGeoffrey J. Gordon were supported by ONR MURI grant number N00014-09-1-1052.\nReferences\nputation, 12:1371\u20131398, 2000.\n15\n[17] Eyal Even-dar. Y.: Planning in pomdps using multiplicity automata. In In: Proceedings of 21st\nConference on Uncertainty in Arti\ufb01cial Intelligence (UAI, pages 185\u2013192, 2005."
    },
    {
        "title": "Sketch-based linear value function approximation",
        "author": [
            "M.G. Bellemare",
            "J. Veness",
            "M. Bowling"
        ],
        "venue": "Proc. Advances in Neural Information Processing Systems (NIPS) 25, 2012, pp. 2222\u20132230.",
        "citeRegEx": "38",
        "shortCiteRegEx": null,
        "year": 2012,
        "abstract": "",
        "full_text": "",
        "sentence": " , [33]\u2013[38] ).",
        "context": null
    },
    {
        "title": "Adaptive Filters",
        "author": [
            "A.H. Sayed"
        ],
        "venue": null,
        "citeRegEx": "39",
        "shortCiteRegEx": "39",
        "year": 2008,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " Using (15), it can also be verified that the solution w that minimizes JPB(w) satisfies the following normal equations [39]: B\u22a4(X\u22a4DX)\u22121Bw\u22c6 = B\u22a4(X\u22a4DX)\u22121X\u22a4Dr\u03c0 (16) Since \u2016P \u2016\u221e = 1 and \u03b3 < 1, we can bound the spectral radius of \u03b3P \u03c0 by \u03c1(\u03b3P ) \u2264 \u2016\u03b3P \u2016\u221e = \u03b3 < 1 (17) Thus, the inverse (IS \u2212 \u03b3P \u03c0)\u22121 exists. Using the Kronecker product property vec(Y \u03a3Z) = (Z\u22a4 \u2297 Y )vec(\u03a3) [39], we can vectorize \u03a3\u2032 in (67) and find that its vector form is related to \u03a3 via the following linear relation: \u03c3\u2032 , vec(\u03a3\u2032) = F\u03c3, where the matrix F is given by F , (( I2MN \u2212 \u03bcR\u22a4 ) C ) \u2297 (( I2MN \u2212 \u03bcR\u22a4 ) C ) + \u03bcE [( (Ri+1 \u2212R\u22a4)C ) \u2297 ( (Ri+1 \u2212R\u22a4)C )] To study the convergence of (71) we will expand it into a state-space model following [39], [49]. By the Cayley-Hamilton Theorem [39], we know that every matrix satisfies its characteristic equation (i.",
        "context": null
    },
    {
        "title": "Numerical solution of saddle point problems",
        "author": [
            "M. Benzi",
            "G.H. Golub",
            "J. Liesen"
        ],
        "venue": "Acta Numerica, vol. 14, pp. 1\u2013137, 2005.",
        "citeRegEx": "41",
        "shortCiteRegEx": null,
        "year": 2005,
        "abstract": "Large linear systems of saddle point type arise in a wide variety of applications throughout computational science and engineering. Due to their indefiniteness and often poor spectral properties, such linear systems represent a significant challenge for solver developers. In recent years there has been a surge of interest in saddle point problems, and numerous solution techniques have been proposed for this type of system. The aim of this paper is to present and discuss a large selection of solution methods for linear systems in saddle point form, with an emphasis on iterative methods for large and sparse problems.",
        "full_text": "",
        "sentence": " , [41], [42, Ch.",
        "context": null
    },
    {
        "title": "Introduction to Optimization",
        "author": [
            "B.T. Polyak"
        ],
        "venue": "Optimization Software Inc.,",
        "citeRegEx": "42",
        "shortCiteRegEx": "42",
        "year": 1987,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "Studies in Linear and Non-linear Programming",
        "author": [
            "K.J. Arrow",
            "L. Hurwicz",
            "H. Uzawa"
        ],
        "venue": null,
        "citeRegEx": "43",
        "shortCiteRegEx": "43",
        "year": 1958,
        "abstract": "",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "On the limiting behavior of distributed optimization strategies",
        "author": [
            "J. Chen",
            "A.H. Sayed"
        ],
        "venue": "Proc. Annual Allerton Conference on Communication, Control, and Computing, Monticello, IL, USA, October 2012, pp. 1535\u20131542.",
        "citeRegEx": "45",
        "shortCiteRegEx": null,
        "year": 2012,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " Motivated by recent results on network behavior in [21], [45], we note that, through collaboration, each agent may contribute to the network with its own experience. We choose the adapt-then-combine (ATC) diffusion variant for distributed optimization over networks [17], [18], [45].",
        "context": null
    },
    {
        "title": "Non-negative Matrices and Markov Chains",
        "author": [
            "E. Seneta"
        ],
        "venue": null,
        "citeRegEx": "46",
        "shortCiteRegEx": "46",
        "year": 2006,
        "abstract": "",
        "full_text": "",
        "sentence": " , there exists j > 0 such that all entries of C are strictly positive) [19], [46].",
        "context": null
    },
    {
        "title": "The O.D.E. method for convergence of stochastic approximation and reinforcement learning",
        "author": [
            "V.S. Borkar",
            "S. Meyn"
        ],
        "venue": "SIAM Journal on Control and Optimization, vol. 38, pp. 447\u2013469, 1999.",
        "citeRegEx": "48",
        "shortCiteRegEx": null,
        "year": 1999,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " , [8], [9], [48]) that simplifies the analysis because the tuples {xk,i,ak(i),xk,i+1, rk(i)} become i.",
        "context": null
    },
    {
        "title": "Transient analysis of data-normalized adaptive filters",
        "author": [
            "T.Y. Al-Naffouri",
            "A.H. Sayed"
        ],
        "venue": "IEEE Transactions on Signal Processing, vol. 51, no. 3, pp. 639\u2013652, 2003.",
        "citeRegEx": "49",
        "shortCiteRegEx": null,
        "year": 2003,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " To study the convergence of (71) we will expand it into a state-space model following [39], [49].",
        "context": null
    },
    {
        "title": "Convergence in multiagent coordination, consensus, and flocking",
        "author": [
            "V. Blondel",
            "J. Hendrickx",
            "A. Olshevsky",
            "J. Tsitsiklis"
        ],
        "venue": "Proc. IEEE Conf. on Decision and Control, and European Control Conf. (CDC-ECC), Seville, Spain, 2005, pp. 2996\u2013 3000.",
        "citeRegEx": "50",
        "shortCiteRegEx": null,
        "year": 2005,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " , the clk elements of C) are obtained independently by each node following an averaging rule [19], [50], such that equal weight is given to any member of the neighborhood, including itself (i.",
        "context": null
    },
    {
        "title": "Asynchronous adaptation and learning over networks \u2014 Part II: Performance analysis",
        "author": [
            "X. Zhao",
            "A.H. Sayed"
        ],
        "venue": "submitted for publication. Also available as arXiv:1312.5438, Dec. 2013.",
        "citeRegEx": "51",
        "shortCiteRegEx": null,
        "year": 2013,
        "abstract": "",
        "full_text": "",
        "sentence": " Using the same technique proposed in [51], [52], we appeal to eigenvalue perturbation analysis to examine the spectral radius of (100).",
        "context": null
    },
    {
        "title": "The learning behavior of adaptive networks \u2014 Part I: Transient analysis",
        "author": [
            "J. Chen",
            "A.H. Sayed"
        ],
        "venue": "submitted for publication. Also available as arXiv:1312.7581, Dec. 2013.",
        "citeRegEx": "52",
        "shortCiteRegEx": null,
        "year": 2013,
        "abstract": "",
        "full_text": "",
        "sentence": " Using the same technique proposed in [51], [52], we appeal to eigenvalue perturbation analysis to examine the spectral radius of (100).",
        "context": null
    },
    {
        "title": "Performance limits for distributed estimation over LMS adaptive networks",
        "author": [
            "X. Zhao",
            "A.H. Sayed"
        ],
        "venue": "IEEE Transactions on Signal Processing, vol. 60, no. 10, pp. 5107\u20135124, 2012. November 6, 2014  DRAFT",
        "citeRegEx": "53",
        "shortCiteRegEx": null,
        "year": 2012,
        "abstract": "In this work we analyze the mean-square performance of different strategies\nfor distributed estimation over least-mean-squares (LMS) adaptive networks. The\nresults highlight some useful properties for distributed adaptation in\ncomparison to fusion-based centralized solutions. The analysis establishes\nthat, by optimizing over the combination weights, diffusion strategies can\ndeliver lower excess-mean-square-error than centralized solutions employing\ntraditional block or incremental LMS strategies. We first study in some detail\nthe situation involving combinations of two adaptive agents and then extend the\nresults to generic N-node ad-hoc networks. In the later case, we establish\nthat, for sufficiently small step-sizes, diffusion strategies can outperform\ncentralized block or incremental LMS strategies by optimizing over\nleft-stochastic combination weighting matrices. The results suggest more\nefficient ways for organizing and processing data at fusion centers, and\npresent useful adaptive strategies that are able to enhance performance when\nimplemented in a distributed manner.",
        "full_text": "arXiv:1206.3728v1  [cs.IT]  17 Jun 2012\n1\nPerformance Limits for Distributed Estimation\nOver LMS Adaptive Networks\nXiaochuan Zhao, Student Member, IEEE, and Ali H. Sayed, Fellow, IEEE\nAbstract\nIn this work we analyze the mean-square performance of different strategies for distributed estimation\nover least-mean-squares (LMS) adaptive networks. The results highlight some useful properties for\ndistributed adaptation in comparison to fusion-based centralized solutions. The analysis establishes that,\nby optimizing over the combination weights, diffusion strategies can deliver lower excess-mean-square-\nerror than centralized solutions employing traditional block or incremental LMS strategies. We \ufb01rst study\nin some detail the situation involving combinations of two adaptive agents and then extend the results\nto generic N-node ad-hoc networks. In the later case, we establish that, for suf\ufb01ciently small step-sizes,\ndiffusion strategies can outperform centralized block or incremental LMS strategies by optimizing over\nleft-stochastic combination weighting matrices. The results suggest more ef\ufb01cient ways for organizing\nand processing data at fusion centers, and present useful adaptive strategies that are able to enhance\nperformance when implemented in a distributed manner.\nIndex Terms\nAdaptive networks, distributed estimation, centralized estimation, diffusion LMS, fusion center, in-\ncremental strategy, diffusion strategy, energy conservation.\nThis work was supported in part by NSF grants CCF-0942936 and CCF-1011918. A short early version of this work was\npresented in IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), Prague, Czech Republic,\nMay 2011 [1].\nThe authors are with Department of Electrical Engineering, University of California, Los Angeles, CA 90095 Email:\n{xzhao, sayed}@ee.ucla.edu.\nNovember 5, 2018\nDRAFT\n2\nI. INTRODUCTION\nT\nHIS work examines the dynamics that results when adaptive nodes are allowed to interact with\neach other. Through cooperation, some interesting behavior occurs that is not observed when the\nnodes operate independently. For example, if one adaptive agent has worse performance than another\nindependent adaptive agent, can both agents cooperate with each other in such a manner that the\nperformance of both agents improves? What if N agents are interacting with each other? Can all agents\nimprove their performance relative to the non-cooperative case even when some of them are noisier than\nothers? Does cooperation need to be performed in a centralized manner or is distributed cooperation\nsuf\ufb01cient to achieve this goal? Starting with two adaptive nodes, we derive analytical expressions for the\nmean-square performance of the nodes under some conditions on the measurement data. The expressions\nare then used to compare the performance of various (centralized and distributed) adaptive strategies. The\nanalysis reveals a useful fact that arises as a result of the cooperation between the nodes; it establishes\nthat, by optimizing over the combination weights, diffusion least-mean-squares (LMS) strategies for\ndistributed estimation can deliver lower excess-mean-square-error (EMSE) than a centralized solution\nemploying traditional block or incremental LMS strategies. We \ufb01rst study in some detail the situation\ninvolving combinations of two adaptive nodes for which the performance levels can be characterized\nanalytically. Subsequently, we extend the conclusion to N-node ad-hoc networks. Reference [2] provides\nan overview of diffusion strategies for adaptation and learning over networks.\nIt is worth noting that the performance of diffusion algorithms was already studied in some detail in the\nearlier works [3], [4]. These works derived expressions for the network EMSE and mean-square-deviation\n(MSD) in terms of the combination weights that are used during the adaptation process. The results in\n[3], [4] were mainly concerned in comparing the performance of diffusion (i.e., distributed cooperative)\nstrategies with non-cooperative strategies. In the cooperative case, nodes share information with each\nother, whereas they behave independently of each other in the non-cooperative case. In the current work,\nwe are instead interested in comparing diffusion or distributed cooperative strategies against centralized\n(as opposed to non-cooperative) strategies. In the centralized framework, a fusion center has access to all\ndata collected from across the network, whereas in the non-cooperative setting nodes have access only\nto their individual data. Therefore, \ufb01nding conditions under which diffusion strategies can perform well\nin comparison to centralized solutions is generally a demanding task.\nWe start our study by considering initially the case of two interacting adaptive agents. Though struc-\nturally simple, two-node networks are important in their own right. For instance, two-antenna receivers are\nNovember 5, 2018\nDRAFT\n3\nprevalent in many communication systems. The data received by the antennas could either be transferred\nto a central processor for handling or processed cooperatively and locally at the antennas. Which mode\nof processing can lead to better performance and how? Some of the results in this article help provide\nanswers to these questions. In addition, two-node adaptive agents can serve as good models for how\nestimates can be combined at master nodes that connect larger sub-networks together. There has also\nbeen useful work in the literature on examining the performance of combinations of two adaptive \ufb01lters\n[5]\u2013[9]. The main difference between two-node adaptive networks and combinations of two adaptive \ufb01lters\nis that in the network case the measurement and regression data are fully distributed and also different\nacross nodes, whereas the \ufb01lters share the same measurement and regression data in \ufb01lter combinations\n[5]\u2013[9]. For this reason, the study of adaptive networks is more challenging and their dynamics is richer.\nThe results in this work will reveal that distributed diffusion LMS strategies can outperform centralized\nblock or incremental LMS strategies through proper selection of the combination weights. The expressions\nfor the combination weights end up depending on knowledge of the noise variances, which are generally\nunavailable to the nodes. Nevertheless, the expressions suggest a useful adaptive construction. Motivated\nby the analysis, we propose an adaptive method for adjusting the combination weights by relying solely\non the available data. Simulation results illustrate the \ufb01ndings.\nNotation: We use lowercase letters to denote vectors, uppercase letters for matrices, plain letters for\ndeterministic variables, and boldface letters for random variables. We also use (\u00b7)T to denote transposition,\n(\u00b7)\u2217for conjugate transposition, (\u00b7)\u22121 for matrix inversion, Tr(\u00b7) for the trace of a matrix, \u03c1(\u00b7) for the\nspectral radius of a matrix, \u2297for the Kronecker product, vec(A) for stacking the columns of A on top\nof each other, and diag(A) for constructing a vector by using the diagonal entries of A. All vectors in\nour treatment are column vectors, with the exception of the regression vectors, uk,i, which are taken to\nbe row vectors for convenience of presentation.\nA. Non-Cooperative Adaptation by Two Nodes\nWe refer to the two nodes as nodes 1 and 2. Both nodes are assumed to measure data that satisfy a\nlinear regression model of the form:\ndk(i) = uk,iwo + vk(i)\n(1)\nfor k = 1, 2, where wo is a deterministic but unknown M \u00d7 1 vector, dk(i) is a random measurement\ndatum at time i, uk,i is a random 1 \u00d7 M regression vector at time i, and vk(i) is a random noise signal\nalso at time i. We adopt the following assumptions on the statistical properties of the data {uk,i, vk(i)}.\nNovember 5, 2018\nDRAFT\n4\nFig. 1.\nNodes 1 and 2 process the data independently by means of two local LMS \ufb01lters.\nAssumption 1 (Statistical properties of the data):\n1) The regression data uk,i are temporally white and spatially independent random variables with zero\nmean and uniform covariance matrix Ru,k \u225cEu\u2217\nk,iuk,i > 0.\n2) The noise signals vk(i) are temporally white and spatially independent random variables with zero\nmean and variances \u03c32\nv,k.\n3) The regressors uk,i and noise signals vl(j) are mutually-independent for all k and l, i and j.\nIt is worth noting that we do not assume Gaussian distributions for either the regressors or the noise\nsignals. We note that the temporal independence assumption on the regressors may be invalid in general,\nespecially for tapped-delay implementations where the regressions at each node would exhibit a shift\nstructure. However, there have been extensive studies in the stochastic approximation literature showing\nthat, for stand-alone adaptive \ufb01lters, results based on the temporal independence assumption, such as (12)\nand (13) further ahead, still match well with actual \ufb01lter performance when the step-size is suf\ufb01ciently\nsmall [10]\u2013[16]. Thus, we shall adopt the following assumption throughout this work.\nAssumption 2 (Small step-sizes): The step-sizes are suf\ufb01ciently small, i.e., \u00b5k \u226a1, so that terms\ndepending on higher-order powers of the step-sizes can be ignored, and such that the adaptive strategies\ndiscussed in this work are mean-square stable (in the manner de\ufb01ned further ahead).\nWe are interested in the situation where one node is less noisy than the other. Thus, without loss of\ngenerality, we assume that the noise variance of node 2 is less than that of node 1, i.e.,\n\u03c32\nv,2 < \u03c32\nv,1\n(2)\nThe nodes are interested in estimating the unknown parameter wo. Assume initially that each node k\nindependently adopts the famed LMS algorithm [16]\u2013[18] to update its weight estimate (as illustrated in\nFig. 1) according to the following rule:\nwk,i = wk,i\u22121 + \u00b5ku\u2217\nk,i [dk(i) \u2212uk,iwk,i\u22121]\n(3)\nNovember 5, 2018\nDRAFT\n5\nfor k = 1, 2, where \u00b5k is a positive constant step-size parameter. The steady-state performance of an\nadaptive algorithm is usually assessed in terms of its mean-square error (MSE), EMSE, and MSD, which\nare de\ufb01ned as follows. If we introduce the error quantities:\nek(i) \u225cdk(i) \u2212uk,iwk,i\u22121\n(4)\newk,i \u225cwo \u2212wk,i\n(5)\nea,k(i) \u225cuk,i e\nwk,i\u22121\n(6)\nthen the MSE, EMSE, and MSD for node k are de\ufb01ned as the following steady-state values:\nMSEk \u225clim\ni\u2192\u221eE|ek(i)|2\n(7)\nEMSEk \u225clim\ni\u2192\u221eE|ea,k(i)|2\n(8)\nMSDk \u225clim\ni\u2192\u221eE\u2225ewk,i\u22252\n(9)\nwhere the notation \u2225\u00b7 \u2225denotes the Euclidean norm of its vector argument. Substituting expression (1)\ninto the de\ufb01nition for ek(i) in (4), it is easy to verify that the errors {ek(i), ea,k(i)} are related as follows:\nek(i) = ea,k(i) + vk(i)\n(10)\nfor k = 1, 2. Since the terms vk(i) and ea,k(i) are independent of each other, it readily follows that the\nMSE and EMSE performance measures at each node are related to each other through the noise variance:\nMSEk = EMSEk + \u03c32\nv,k\n(11)\nTherefore, it is suf\ufb01cient to examine the EMSE and MSD as performance metrics for adaptive algorithms.\nUnder Assumption 2, the EMSE and MSD of each LMS \ufb01lter in (3) are known to be well approximated\nby [16]\u2013[24]:\nEMSEk \u22481\n2 \u00b5k\u03c32\nv,kTr(Ru,k)\n(12)\nand\nMSDk \u22481\n2 \u00b5k\u03c32\nv,kM\n(13)\nfor k = 1, 2. To proceed, we further assume that both nodes employ the same step-size and observe data\narising from the same underlying distribution.\nAssumption 3 (Uniform step-sizes and data covariance): It is assumed that both nodes employ identi-\ncal step-sizes, i.e., \u00b51 = \u00b52 = \u00b5, and that they observe data arising from the same statistical distribution,\ni.e., Ru,1 = Ru,2 = Ru.\nNovember 5, 2018\nDRAFT\n6\nUnder Assumption 3, expression (12) con\ufb01rms the expected result that node 2 will achieve a lower\nEMSE than node 1 because node 2 has lower noise variance than node 1. The interesting question that\nwe would like to consider is whether it is possible to improve the EMSE performance for both nodes if\nthey are allowed to cooperate with each other in some manner. The arguments in this work will answer\nthis question in the af\ufb01rmative and will present distributed cooperative schemes that are able to achieve\nthis goal, not only for two-node networks but also for N-node ad-hoc networks (see Sec. VI).\nII. TWO CENTRALIZED ADAPTIVE ALGORITHMS\nOne form of cooperation can be realized by connecting the two nodes to a fusion center, which\nwould collect the data from the nodes and and use them to estimate wo. Fusion centers are generally\nmore powerful than the individual nodes and can, in principle, implement more sophisticated estimation\nprocedures than the individual nodes. In order to allow for a fair comparison between implementations\nof similar nature at the fusion center and remotely at the nodes, we assume that the fusion center is\nlimited to implementing LMS-type solutions as well, albeit in a centralized manner. In this work, the\nfusion center is assumed to operate on the data in one of two ways. The \ufb01rst method is illustrated in\nFig. 2a and we refer to it as block LMS. In this method, the fusion center receives data from the nodes\nand updates its estimate for wo according to the following:\nwi = wi\u22121 + \u00b5\u2032\n\uf8ee\n\uf8ef\uf8f0\nu1,i\nu2,i\n\uf8f9\n\uf8fa\uf8fb\n\u2217\uf8eb\n\uf8ec\n\uf8ed\n\uf8ee\n\uf8ef\uf8f0\nd1(i)\nd2(i)\n\uf8f9\n\uf8fa\uf8fb\u2212\n\uf8ee\n\uf8ef\uf8f0\nu1,i\nu2,i\n\uf8f9\n\uf8fa\uf8fbwi\u22121\n\uf8f6\n\uf8f7\n\uf8f8\n(14)\nwith a constant positive step-size \u00b5\u2032. The second method is illustrated in Fig. 2b and we refer to it as\nincremental LMS. In this method, the fusion center still receives data from the nodes but operates on\nthem sequentially by incorporating one set of measurements at a time as follows:\n\uf8f1\n\uf8f4\n\uf8f2\n\uf8f4\n\uf8f3\n\u03c6i = wi\u22121 + \u00b5\u2032u\u2217\n1,i [d1(i) \u2212u1,iwi\u22121]\nwi = \u03c6i + \u00b5\u2032u\u2217\n2,i [d2(i) \u2212u2,i\u03c6i]\n(15)\nWe see from (15) that the fusion center in this case \ufb01rst uses the data from node 1 to update wi\u22121 to\nan intermediate value \u03c6i, and then uses the data from node 2 to get wi. Method (15) is a special case\nof the incremental LMS strategy introduced and studied in [25] and is motivated by useful incremental\napproaches to distributed optimization [26]\u2013[31]. We observe from (14) and (15) that in going from\nwi\u22121 to wi, the block and incremental LMS algorithms employ two sets of data for each such update;\nin comparison, the conventional LMS algorithm used by the stand-alone nodes in (3) employs one set\nof data for each update of their respective weight estimates.\nNovember 5, 2018\nDRAFT\n7\n(a) Block LMS adaptation.\n(b) Incremental LMS adaptation.\nFig. 2.\nTwo centralized strategies using data from nodes at a fusion center.\nWe de\ufb01ne the EMSE and MSD for block LMS (14) and incremental LMS (15) as follows:\nEMSEblk/inc \u225c1\n2 lim\ni\u2192\u221eE\u2225ea,i\u22252\n(16)\nMSDblk/inc \u225clim\ni\u2192\u221eE\u2225e\nwi\u22252\n(17)\nwhere the a priori error ea,i is now a 2 \u00d7 1 vector:\nea,i \u225c\n\uf8ee\n\uf8ef\uf8f0\nu1,i ewi\u22121\nu2,i ewi\u22121\n\uf8f9\n\uf8fa\uf8fb\n(18)\nNote that in (16) we are scaling the de\ufb01nition of the EMSE by 1/2 because the squared Euclidean-norm\nin (16) involves the sum of the two error components from (18). We shall explain later in Sec. VI that in\norder to ensure a fair comparison of the performance of the various algorithms (including non-cooperative,\ndistributed, and centralized), we will need to set the step-size as (see (66))\n\u00b5\u2032 = \u00b5\n2\n(19)\nThis normalization will help ensure that the rates of convergence of the various strategies that are being\ncompared are similar.\nNow, compared to the non-cooperative method (3) where the nodes act individually, it can be shown that\nthe two centralized algorithms (14) and (15) lead to improved mean-square performance (the arguments\nfurther ahead in Sec. IV-F establish this conclusion among several other properties). Speci\ufb01cally, the\nNovember 5, 2018\nDRAFT\n8\nEMSE obtained by the two centralized algorithms (14) and (15) will be smaller than the average EMSE\nobtained by the two non-cooperative nodes in (3). The question that we would like to explore is whether\ndistributed cooperation between the nodes can lead to superior performance even in comparison to\nthe centralized algorithms (14) and (15). To address this question, we shall consider distributed LMS\nalgorithms of the diffusion-type from [3], [4], and which are further studied in [32]\u2013[40]. Reference [2]\nprovides an overview of diffusion strategies. Adaptive diffusion strategies have several useful properties:\nthey are scalable and robust, enhance stability, and enable nodes to adapt and learn through localized\ninteractions. There are of course other useful algorithms for distributed estimation that rely instead on\nconsensus-type strategies, e.g., [41]\u2013[43]. Nevertheless, diffusion strategies have been shown to lead\nto superior mean-square-error performance in comparison to consensus-type strategies (see, e.g., [38],\n[39]). For this reason, we focus on comparing adaptive diffusion strategies with the centralized block and\nincremental LMS approaches. The arguments further ahead will show that diffusion algorithms are able\nto exploit the spatial diversity in the data more fully than the centralized implementations and can lead\nto better steady-state mean-square performance than the block and incremental algorithms (14) and (15),\nwhen all algorithms converge in the mean-square sense at the same rate. We shall establish these results\ninitially for the case of two interacting adaptive agents, and then discuss the generalization for N-node\nnetworks in Sec. VI.\nIII. ADAPTIVE DIFFUSION STRATEGIES\nDiffusion LMS algorithms are distributed strategies that consist of two steps [2]\u2013[4]: updating the\nweight estimate using local measurement data (the adaptation step) and aggregating the information from\nthe neighbors (the combination step). According to the order of these two steps, diffusion algorithms can\nbe categorized into two classes: Combine-then-Adapt (CTA) (as illustrated in Fig. 3a):\n\uf8f1\n\uf8f4\n\uf8f2\n\uf8f4\n\uf8f3\n\u03c6k,i\u22121 = a1kw1,i\u22121 + a2kw2,i\u22121\nwk,i = \u03c6k,i\u22121 + \u00b5ku\u2217\nk,i [dk(i) \u2212uk,i\u03c6k,i\u22121]\n(20)\nand Adapt-then-Combine (ATC) (as illustrated in Fig. 3b):\n\uf8f1\n\uf8f4\n\uf8f2\n\uf8f4\n\uf8f3\n\u03c8k,i = wk,i\u22121 + \u00b5ku\u2217\nk,i [dk(i) \u2212uk,iwk,i\u22121]\nwk,i = a1k\u03c81,i + a2k\u03c82,i\n(21)\nfor k = 1, 2, where the {\u00b5k} are positive step-sizes and the {alk} denote convex combination coef\ufb01cients\nused by nodes 1 and 2. The coef\ufb01cients are nonnegative and they satisfy\na1k \u22650,\na2k \u22650,\na1k + a2k = 1\n(22)\nNovember 5, 2018\nDRAFT\n9\n(a) CTA diffusion adaptation.\n(b) ATC diffusion adaptation.\nFig. 3.\nTwo diffusion strategies using combination coef\ufb01cients {\u03b1, 1 \u2212\u03b1, \u03b2, 1 \u2212\u03b2}.\nfor k = 1, 2. We collect these coef\ufb01cients into a 2 \u00d7 2 matrix A and denote them more compactly by\n{\u03b1, 1 \u2212\u03b1} for node 1 and {1 \u2212\u03b2, \u03b2} for node 2:\nA \u225c\n\uf8ee\n\uf8ef\uf8f0\na11\na12\na21\na22\n\uf8f9\n\uf8fa\uf8fb=\n\uf8ee\n\uf8ef\uf8f0\n\u03b1\n1 \u2212\u03b2\n1 \u2212\u03b1\n\u03b2\n\uf8f9\n\uf8fa\uf8fb\n(23)\nwhere \u03b1, \u03b2 \u2208[0, 1]. Note that when \u03b1 = \u03b2 = 1, both CTA algorithm (20) and the ATC algorithm\n(21) reduce to the non-cooperative LMS update given by (3); we shall exclude this case for diffusion\nalgorithms. Observe that the order of adaptation and combination steps are different for CTA and ATC\nimplementations. The ATC algorithm (21) is known to outperform the CTA algorithm (20) because the\nformer shares updated weight estimates in comparison to the latter, and these estimates are expected to\nbe less noisy; see the analysis further ahead and also [2], [4].\nAn important factor affecting the mean-square performance of diffusion LMS algorithms is the choice\nof the combination coef\ufb01cients \u03b1 and \u03b2. Different combination rules have been proposed in the literature,\nsuch as uniform, Laplacian, maximum degree, Metropolis, relative degree, relative degree-variance, and\nrelative variance (which were listed in Table III of reference [4]; see also [2], [37]). Apart from these static\ncombination rules, where the coef\ufb01cients are kept constant over time, adaptive rules are also possible. In\nthe adaptive case, the combination weights can be adjusted regularly so that the network can respond to\nNovember 5, 2018\nDRAFT\n10\nreal-time node conditions [2], [3], [34], [37], [40].\nNow that we have introduced the various strategies (non-cooperative LMS, block LMS, incremental\nLMS, ATC and CTA diffusion LMS), we proceed to derive expressions for the optimal mean-square\nperformance of the diffusion algorithms (20) and (21). The analysis will highlight some useful properties\nfor distributed algorithms in comparison to centralized counterparts. For example, the results will establish\nthat the diffusion strategies using optimized combination weights perform better than the centralized\nsolutions (14) and (15). Obviously, by assuming knowledge of the network topology, a fusion center can\nimplement the optimized diffusion strategies centrally and therefore attain the same performance as the\ndistributed solution. We are not interested in such situations where the fusion center implements solutions\nthat are fundamentally distributed in nature. We are instead interested in comparing truly distributed\nsolutions of the diffusion type (20) and (21) with traditional centralized solutions of the block and\nincremental LMS types (14) and (15); all with similar levels of LMS complexity.\nIV. PERFORMANCE ANALYSIS FOR TWO-NODE ADAPTIVE NETWORKS\nWe rely on the energy conservation arguments [16] to conduct the mean-square performance analysis\nof two-node LMS adaptive networks. We \ufb01rst compute the individual and network EMSE and MSD for\nthe CTA and ATC algorithms (20) and (21), and then deal with the block and incremental algorithms (14)\nand (15). The analysis in the sequel is carried out under Assumptions 1\u20133 and condition (19). Assumption\n2 helps ensure the mean-square convergence of the various adaptive strategies that we are considering\nhere \u2014 see, e.g., [2]\u2013[4], [16]. By mean-square convergence of the distributed and centralized algorithms,\nwe mean that E ewk,i \u21920, E e\nwi \u21920, and E\u2225e\nwk,i\u22252 and E\u2225e\nwi\u22252 tend to constant bounded values as\ni \u2192\u221e. In addition, Assumption 3 and condition (19) will help enforce similar convergence rates for all\nstrategies.\nA. EMSE and MSD for Non-Cooperative Nodes\nUnder Assumptions 1\u20133 and as mentioned before, it is known that the EMSE and MSD of the two\nstand-alone LMS \ufb01lters in (3), which operate independently of each other, are given by\nEMSEind,k \u2248\n\u00b5\u03c32\nv,kTr(Ru)\n2\n(24)\nand\nMSDind,k \u2248\n\u00b5\u03c32\nv,kM\n2\n(25)\nNovember 5, 2018\nDRAFT\n11\nfor k = 1, 2. Using (24), the average EMSE and MSD of both nodes are\nEMSEind \u2248\u00b5Tr(Ru)\n2\n\u03c32\nv,1 + \u03c32\nv,2\n2\n(26)\nand\nMSDind \u2248\u00b5M\n2\n\u03c32\nv,1 + \u03c32\nv,2\n2\n(27)\nB. EMSE and MSD for Diffusion Algorithms\nRather than study CTA and ATC algorithms separately, we follow the analysis in [2], [4] and consider\na more general algorithm structure that includes CTA and ATC as special cases. We derive expressions\nfor the node EMSE and MSD for the general structure and then specialize the results for CTA and ATC.\nThus, consider a diffusion strategy of the following general form:\n\u03c6k,i\u22121 = p1kw1,i\u22121 + p2kw2,i\u22121\n(28)\n\u03c8k,i = \u03c6k,i\u22121 + \u00b5u\u2217\nk,i [dk(i) \u2212uk,i\u03c6k,i\u22121]\n(29)\nwk,i = q1k\u03c81,i + q2k\u03c82,i\n(30)\nwhere {plk, qlk} are the nonnegative entries of 2\u00d72 matrices {P, Q}. The CTA algorithm (20) corresponds\nto the special choice P = A and Q = I2 while the ATC algorithm (21) corresponds to the special choice\nP = I2 and Q = A, where I2 denotes the 2 \u00d7 2 identity matrix. From (23), it can be veri\ufb01ed that the\neigenvalues of A are {1, \u03b1 + \u03b2 \u22121}. In the cooperative case, we rule out the choice \u03b1 = \u03b2 = 1 so\nthat the two eigenvalues of A are distinct and, hence, A is diagonalizable. Let A = TDT \u22121 denote the\neigen-decomposition of A:\n\uf8ee\n\uf8ef\uf8f0\n\u03b1\n1 \u2212\u03b2\n1 \u2212\u03b1\n\u03b2\n\uf8f9\n\uf8fa\uf8fb\n|\n{z\n}\nA\n=\n1\n2 \u2212\u03b1 \u2212\u03b2\n\uf8ee\n\uf8ef\uf8f0\n1 \u2212\u03b2\n1\n1 \u2212\u03b1\n\u22121\n\uf8f9\n\uf8fa\uf8fb\n|\n{z\n}\nT\n\u00b7\n\uf8ee\n\uf8ef\uf8f0\n1\n0\n0\n\u03b1 + \u03b2 \u22121\n\uf8f9\n\uf8fa\uf8fb\n|\n{z\n}\nD\n\u00b7\n\uf8ee\n\uf8ef\uf8f0\n1\n1\n1 \u2212\u03b1\n\u03b2 \u22121\n\uf8f9\n\uf8fa\uf8fb\n|\n{z\n}\nT \u22121\n(31)\nand let \u03bbm denote the mth eigenvalue of Ru whose size is M \u00d7 M. We derive in Appendix A the\nfollowing expression for the EMSE at node k:\nEMSEdiff,k \u2248\u00b52\nM\nX\nm=1\n\u03bb2\nmvec(T TQTRvQT)T(I4 \u2212\u03bemD \u2297D)\u22121vec(T \u22121EkkT \u2212T)\n(32)\nfor k = 1, 2, where\n\u03bem \u225c1 \u22122\u00b5\u03bbm,\nRv \u225cdiag{\u03c32\nv,1, \u03c32\nv,2}\n(33)\nNovember 5, 2018\nDRAFT\n12\nand Ekk are 2 \u00d7 2 matrices that are given by\nE11 = diag{1, 0},\nE22 = diag{0, 1}\n(34)\nLikewise, we can derive the MSD at node k:\nMSDdiff,k \u2248\u00b52\nM\nX\nm=1\n\u03bbmvec(T TQTRvQT)T(I4 \u2212\u03bemD \u2297D)\u22121vec(T \u22121EkkT \u2212T)\n(35)\nfor k = 1, 2. Comparing (32) and (35) we note that \u03bb2\nm in (32) is replaced by \u03bbm in (35); all the other\nfactors are identical.\nC. EMSE and MSD of CTA Diffusion LMS\nSetting Q = I2, we specialize (32) to obtain the EMSE expression for the CTA algorithm:\nEMSEcta,k \u2248\u00b52\nM\nX\nm=1\n\u03bb2\nmvec(T TRvT)T(I4 \u2212\u03bemD \u2297D)\u22121vec(T \u22121EkkT \u2212T)\n(36)\nfor k = 1, 2. Substituting (31) into (36), some algebra will show that the network EMSE for the CTA\nalgorithm, which is de\ufb01ned as the average EMSE of the individual nodes, is given by\nEMSEcta \u2248\nM\nX\nm=1\n\u00b52\u03bb2\nm\n(2 \u2212\u03b1 \u2212\u03b2)2\n\"\n\u03c32\nv,1(1 \u2212\u03b2)2 + \u03c32\nv,2(1 \u2212\u03b1)2\n1 \u2212\u03bem\n+ (\u03b1 \u2212\u03b2)[\u03c32\nv,2(1 \u2212\u03b1) \u2212\u03c32\nv,1(1 \u2212\u03b2)]\n1 \u2212\u03bem(\u03b1 + \u03b2 \u22121)\n+\n(\u03c32\nv,1 + \u03c32\nv,2)[(1 \u2212\u03b1)2 + (1 \u2212\u03b2)2]\n2[1 \u2212\u03bem(\u03b1 + \u03b2 \u22121)2]\n#\n(37)\nWe argue in Appendix B that, under Assumption 2 (i.e., for suf\ufb01ciently small step-sizes), the network\nEMSE in (37) is essentially minimized when {\u03b1, \u03b2} are chosen as\n\u03b1 =\n\u03c3\u22122\nv,1\n\u03c3\u22122\nv,1 + \u03c3\u22122\nv,2\n,\n\u03b2 =\n\u03c3\u22122\nv,2\n\u03c3\u22122\nv,1 + \u03c3\u22122\nv,2\n(38)\nThis choice coincides with the relative degree-variance rule proposed in [4].1 In the sequel we will\ncompare the performance of the diffusion strategies that result from this choice of combination weights\nagainst the performance of the block and incremental LMS strategies (14) and (15).\nThe value of (37) that corresponds to the choice (38) is then given by\nEMSE\nopt\ncta \u2248\n\u03c32\nv,1\u03c32\nv,2\n\u03c32\nv,1 + \u03c32\nv,2\n\u00b5Tr(Ru)\n2\n+\n\u00b52(\u03c34\nv,1 + \u03c34\nv,2)\n2 (\u03c32\nv,1 + \u03c32\nv,2)\nM\nX\nm=1\n\u03bb2\nm\n(39)\n1There is a typo in Table III of [4], where the noise variances for the relative degree-variance rule should appear inverted.\nNovember 5, 2018\nDRAFT\n13\nand the corresponding EMSE values at the nodes are\nEMSEopt\ncta,k \u2248\n\u03c32\nv,1\u03c32\nv,2\n\u03c32\nv,1 + \u03c32\nv,2\n\u00b5Tr(Ru)\n2\n+\n\u00b52\u03c34\nv,k\n\u03c32\nv,1 + \u03c32\nv,2\nM\nX\nm=1\n\u03bb2\nm\n(40)\nfor k = 1, 2. Similarly, the network MSD is approximately minimized for the same choice (38) and its\nvalue is given by\nMSD\nopt\ncta \u2248\n\u03c32\nv,1\u03c32\nv,2\n\u03c32\nv,1 + \u03c32\nv,2\n\u00b5M\n2\n+\n\u03c34\nv,1 + \u03c34\nv,2\n\u03c32\nv,1 + \u03c32\nv,2\n\u00b52Tr(Ru)\n2\n(41)\nThe corresponding MSD values at the nodes are\nMSDopt\ncta,k \u2248\n\u03c32\nv,1\u03c32\nv,2\n\u03c32\nv,1 + \u03c32\nv,2\n\u00b5M\n2\n+\n\u03c34\nv,k\n\u03c32\nv,1 + \u03c32\nv,2\n\u00b52Tr(Ru)\n(42)\nfor k = 1, 2. We shall refer to the CTA diffusion algorithm that uses (38) as the optimal CTA implemen-\ntation. Note that selecting the coef\ufb01cients as in (38) requires knowledge of the noise variances at both\nnodes. This information is usually unavailable. Nevertheless, it is possible to develop adaptive strategies\nto adjust the coef\ufb01cients {\u03b1, \u03b2} on the \ufb02y based on the available data without requiring the nodes to\nknow beforehand the noise pro\ufb01le in the network (see [2], [37], [40] and (103) and (106) further ahead).\nWe therefore continue the analysis by assuming the nodes are able to determine (or learn) the coef\ufb01cients\n(38).\nD. EMSE and MSD of ATC Diffusion LMS\nLikewise, setting Q = A, we specialize (32) to obtain the EMSE expression for the ATC algorithm:\nEMSEatc,k \u2248\u00b52\nM\nX\nm=1\n\u03bb2\nmvec(T TATRvAT)T(I4 \u2212\u03bemD \u2297D)\u22121vec(T \u22121EkkT \u2212T)\n(43)\nfor k = 1, 2. Following similar arguments to the CTA case, the network EMSE is given by\nEMSEatc \u2248\nM\nX\nm=1\n\u00b52\u03bbm\n(2 \u2212\u03b1 \u2212\u03b2)2\n\"\n\u03c32\nv,1(1 \u2212\u03b2)2 + \u03c32\nv,2(1 \u2212\u03b1)2\n1 \u2212\u03bem\n+ (\u03b1 \u2212\u03b2)(\u03b1 + \u03b2 \u22121)[\u03c32\nv,2(1 \u2212\u03b1) \u2212\u03c32\nv,1(1 \u2212\u03b2)]\n1 \u2212\u03bem(\u03b1 + \u03b2 \u22121)\n+\n(\u03c32\nv,1 + \u03c32\nv,2)(\u03b1 + \u03b2 \u22121)2[(1 \u2212\u03b1)2 + (1 \u2212\u03b2)2]\n2 [1 \u2212\u03bem(\u03b1 + \u03b2 \u22121)2]\n#\n(44)\nWe can again verify that, under Assumption 2, expression (44) is approximately minimized for the same\nchoice (38) (see Appendix B). The resulting network EMSE value is given by\nEMSE\nopt\natc \u2248\n\u03c32\nv,1\u03c32\nv,2\n\u03c32\nv,1 + \u03c32\nv,2\n\u00b5Tr(Ru)\n2\n(45)\nNovember 5, 2018\nDRAFT\n14\nand the corresponding EMSE values at the nodes are\nEMSEopt\natc,k \u2248\n\u03c32\nv,1\u03c32\nv,2\n\u03c32\nv,1 + \u03c32\nv,2\n\u00b5Tr(Ru)\n2\n(46)\nfor k = 1, 2. Similarly, the network MSD is approximately minimized for the same choice (38); its value\nis given by\nMSD\nopt\natc \u2248\n\u03c32\nv,1\u03c32\nv,2\n\u03c32\nv,1 + \u03c32\nv,2\n\u00b5M\n2\n(47)\nand the corresponding MSD values at the nodes are\nMSDopt\natc,k \u2248\n\u03c32\nv,1\u03c32\nv,2\n\u03c32\nv,1 + \u03c32\nv,2\n\u00b5M\n2\n(48)\nfor k = 1, 2. We shall refer to the ATC diffusion algorithm that uses (38) as the optimal ATC implemen-\ntation.\nE. Uniform CTA and ATC Diffusion LMS\nUniform CTA and ATC diffusion LMS correspond to the choice \u03b1 = \u03b2 = 0.5, which means that the\ntwo nodes equally trust each other\u2019s estimates. This situation coincides with the uniform combination\nrule [4]. According to (37) and (35), the network EMSE and MSD for uniform CTA are\nEMSE\nunf\ncta \u2248\n\u03c32\nv,1 + \u03c32\nv,2\n4\n \n\u00b5Tr(Ru)\n2\n+ \u00b52\nM\nX\nm=1\n\u03bb2\nm\n!\n(49)\nand\nMSD\nunf\ncta \u2248\n\u03c32\nv,1 + \u03c32\nv,2\n4\n\u0012\u00b5M\n2\n+ \u00b52Tr(Ru)\n\u0013\n(50)\nSimilarly, according to (44) and (35), the network EMSE and MSD for uniform ATC are\nEMSE\nunf\natc \u2248\n\u03c32\nv,1 + \u03c32\nv,2\n4\n\u00b5Tr(Ru)\n2\n(51)\nand\nMSD\nunf\natc \u2248\n\u03c32\nv,1 + \u03c32\nv,2\n4\n\u00b5M\n2\n(52)\nNovember 5, 2018\nDRAFT\n15\nF. EMSE and MSD of Block LMS and Incremental LMS\nIn Appendix C, we derive the EMSE and MSD for the block LMS implementation (14) and arrive at\nEMSEblk \u2248\n\u03c32\nv,1 + \u03c32\nv,2\n2\n\u00b5\u2032Tr(Ru)\n2\n(53)\nand\nMSDblk \u2248\n\u03c32\nv,1 + \u03c32\nv,2\n2\n\u00b5\u2032M\n2\n(54)\nWith regards to the incremental LMS algorithm (15), we note from Assumption 2 that the step-size \u00b5\u2032\nis suf\ufb01ciently small so that we can assume \u00b5\u2032Tr(Ru) \u226a1. Then, from (15) we get\nwi = wi\u22121 + \u00b5\u2032[u\u2217\n1,i(d1(i) \u2212u1,iwi\u22121) + u\u2217\n2,i(d2(i) \u2212u2,iwi\u22121)] + \u00b5\u20322\u2225u2,i\u22252u\u2217\n1,i(d1(i) \u2212u1,iwi\u22121)\n|\n{z\n}\nO(\u00b5\u20322)\n\u2248wi\u22121 + \u00b5\u2032\n\uf8ee\n\uf8ef\uf8f0\nu1,i\nu2,i\n\uf8f9\n\uf8fa\uf8fb\n\u2217\uf8eb\n\uf8ec\n\uf8ed\n\uf8ee\n\uf8ef\uf8f0\nd1(i)\nd2(i)\n\uf8f9\n\uf8fa\uf8fb\u2212\n\uf8ee\n\uf8ef\uf8f0\nu1,i\nu2,i\n\uf8f9\n\uf8fa\uf8fbwi\u22121\n\uf8f6\n\uf8f7\n\uf8f8\n(55)\nwhich means that the incremental LMS update (15) can be well approximated by the block LMS update\n(14). Then, the EMSE of incremental LMS (15) can be well approximated by (reference [25] provides\na more detailed analysis of the performance of adaptive incremental LMS strategies):\nEMSEinc \u2248\n\u03c32\nv,1 + \u03c32\nv,2\n2\n\u00b5\u2032Tr(Ru)\n2\n(56)\nand its MSD as\nMSDinc \u2248\u03c32\nv,1 + \u03c32\nv,2\n2\n\u00b5\u2032M\n2\n(57)\nIt is worth noting that although (53) and (56) are similar for small step-sizes, incremental LMS actually\noutperforms block LMS [44] because the former uses the intermediate estimate \u03c6i during one step of the\nupdate in (15) while the latter does not. The intermediate estimate \u03c6i is generally \u201cless noisy\u201d than wi\u22121\nso that incremental LMS generally outperforms block LMS. However, we shall not distinguish between\nincremental LMS and block LMS in this work, when we compare their performance with other strategies\nin the sequel.\nG. Summary\nWe list the expressions for the network EMSE and MSD for the various strategies under Assumptions\n1\u20133 in Table I, and the expressions for the individual nodes in Tables II and III, respectively. It is\nworth noting from these expressions that the EMSE is dependent on the step-size parameter. In order\nNovember 5, 2018\nDRAFT\n16\nTABLE I\nNETWORK EMSE AND MSD FOR VARIOUS STRATEGIES OVER TWO-NODE LMS ADAPTIVE NETWORKS\nType\nNetwork EMSE\nNetwork MSD\nOptimal ATC (21)\nc1\u03c32\nharm\n(45)\nc\u2032\n1\u03c32\nharm\n(47)\nOptimal CTA (20)\nc1\u03c32\nharm + c2\n\u00002\u03c32\narth \u2212\u03c32\nharm\n\u0001\n(39)\nc\u2032\n1\u03c32\nharm + c\u2032\n2\n\u00002\u03c32\narth \u2212\u03c32\nharm\n\u0001\n(41)\nUniform ATC (21)\nc1\u03c32\narth\n(51)\nc\u2032\n1\u03c32\narth\n(52)\nUniform CTA (20)\n(c1 + c2) \u03c32\narth\n(49)\n(c\u2032\n1 + c\u2032\n2) \u03c32\narth\n(50)\nBlock LMS (14)\n2c3\u03c32\narth\n(53)\n2c\u2032\n3\u03c32\narth\n(54)\nIncremental LMS (15)\n2c3\u03c32\narth\n(56)\n2c\u2032\n3\u03c32\narth\n(57)\nStand-alone LMS (3)\n2c1\u03c32\narth\n(26)\n2c\u2032\n1\u03c32\narth\n(27)\n1 \u03bb \u225ccol{\u03bb1, . . . , \u03bbN} consists of the eigenvalues of Ru, \u03c32\narth \u225c\n\u03c32\nv,1+\u03c32\nv,2\n2\nand \u03c32\nharm \u225c\n2\u03c32\nv,1\u03c32\nv,2\n\u03c32\nv,1+\u03c32\nv,2 .\n2 c1 \u225c\u00b5Tr(Ru)\n4\n, c2 \u225c\u00b52\u2225\u03bb\u22252\n2\n, c3 \u225c\u00b5\u2032Tr(Ru)\n4\n, c\u2032\n1 \u225c\u00b5M\n4 , c\u2032\n2 \u225c\u00b52Tr(Ru)\n2\n, and c\u2032\n3 \u225c\u00b5\u2032M\n4 .\nTABLE II\nEMSE FOR THE INDIVIDUAL NODES IN VARIOUS STRATEGIES OVER TWO-NODE LMS ADAPTIVE NETWORKS\nType\nEMSE of Node 1\nEMSE of Node 2\nOptimal ATC (21)\nc1\u03c32\nharm\n(46)\nc1\u03c32\nharm\n(46)\nOptimal CTA (20)\nc1\u03c32\nharm + c2\n\u03c34\nv,1\n\u03c32\narth\n(40)\nc1\u03c32\nharm + c2\n\u03c34\nv,2\n\u03c32\narth\n(40)\nStand-alone LMS (3)\n2c1\u03c32\nv,1\n(24)\n2c1\u03c32\nv,2\n(24)\nTABLE III\nMSD FOR THE INDIVIDUAL NODES IN VARIOUS STRATEGIES OVER TWO-NODE LMS ADAPTIVE NETWORKS\nType\nMSD of Node 1\nMSD of Node 2\nOptimal ATC (21)\nc\u2032\n1\u03c32\nharm\n(48)\nc\u2032\n1\u03c32\nharm\n(48)\nOptimal CTA (20)\nc\u2032\n1\u03c32\nharm + c\u2032\n2\n\u03c34\nv,1\n\u03c32\narth\n(42)\nc\u2032\n1\u03c32\nharm + c\u2032\n2\n\u03c34\nv,2\n\u03c32\narth\n(42)\nStand-alone LMS (3)\n2c\u2032\n1\u03c32\nv,1\n(25)\n2c\u2032\n1\u03c32\nv,2\n(25)\nNovember 5, 2018\nDRAFT\n17\nto compare the EMSE of the algorithms in a fair manner, the step-sizes need to be tuned appropriately\nbecause algorithms generally differ in terms of their convergence rates and steady-state performance. Some\nalgorithms converge faster but may have larger EMSE. Others may have smaller EMSE but converge\nslower. Therefore, the step-sizes should be adjusted in such a way that all algorithms exhibit similar\nconvergence rates. Then, under these conditions, the EMSE values can be fairly compared. We proceed\nto explain this issue in greater detail in the next section.\nV. PERFORMANCE COMPARISON FOR VARIOUS ADAPTIVE STRATEGIES\nAdaptive algorithms differ in their mean-square convergence rates and in their steady-state mean-\nsquare error performance. In order to ensure a fair comparison among algorithms, we should either \ufb01x\ntheir convergence rates at the same value and then compare the resulting steady-state performance, or we\nshould \ufb01x the steady-state performance and then compare the convergence rates. To clarify this procedure\nfurther, we consider the concept of \u201coperation curves (OC)\u201d.\nA. Operation Curves for Adaptive Strategies\nThe OC of an algorithm has two axes: the horizontal axis represents its EMSE and the vertical axis\nrepresents its (mean-square) convergence rate. Each point on the OC corresponds to a choice of the\nstep-size parameter. Now the EMSE and convergence rate of an adaptive implementation, such as stand-\nalone LMS, are both dependent on the step-size parameter used by the algorithm. For example, under\nAssumptions 1\u20133, the EMSE of a stand-alone LMS \ufb01lter of the type (3), denoted by \u03b6(\u00b5), is a function\nof \u00b5 and is given by [16]:\n\u03b6(\u00b5) \u2248\u00b5\u03c32\nvTr(Ru)\n2\n(58)\nThe function \u03b6(\u00b5) is monotonically increasing in \u00b5. It is clear from (58) that the smaller the value of \u00b5,\nthe lower the EMSE (which is desirable). However, a smaller step-size \u00b5 results in slower mean-square\nconvergence. This is because, under Assumptions 1\u20133, the modes of convergence for a stand-alone LMS\nimplementation (3) are approximately given by [16, p. 360]:\n\u03bem \u225c1 \u22122\u00b5\u03bbm\n(59)\nfor m = 1, . . . , M, where the {\u03bbm} are the eigenvalues of Ru. The value of \u03bem that is closest to the\nunit circle determines the rate of convergence of E ewi and E\u2225e\nwi\u22252 towards their steady-state values. It\nis clear from (59) that the smaller \u00b5 is, the closer the mode is to the unit circle, and the slower the\nconvergence of the algorithm will be. Hence, under Assumption 2,\nNovember 5, 2018\nDRAFT\n18\n\u2022 Increasing \u00b5 results in faster convergence at the cost of a higher (worse) EMSE.\n\u2022 Decreasing \u00b5 results in slower convergence but a lower (better) EMSE.\nFor this reason, in order to compare fairly the performance of various algorithms, we need to jointly\nexamine their EMSE and convergence rates. It is worth noting that the concept of operation curves\ncan also be applied to other steady-state performance metrics such as the MSD. However, due to space\nlimitations, we focus on the EMSE in this work.\n1) Operation Curve for Stand-Alone LMS: For stand-alone LMS \ufb01lters, under Assumptions 1\u20133, the\naverage network EMSE is given by (26) and the dominant mode (the one that is closest to the unit circle)\nis given by\nmodeind \u22481 \u22122\u00b5\u03bbmin(Ru)\n(60)\nwhere \u03bbmin(\u00b7) denotes the smallest eigenvalue of its matrix argument.\n2) Operation Curve for CTA Diffusion LMS: Based on Assumptions 1\u20133, the expressions for the\nnetwork EMSE of optimal CTA and uniform CTA are given by (39) and (49), respectively. Meanwhile,\nfrom expression (43) in [4], we know that the modes of convergence for CTA algorithms are determined\nby the eigenvalues of [A \u2297(IM \u2212\u00b5RT\nu)] \u2297[A \u2297(IM \u2212\u00b5Ru)]. Now recall from (31) that A has two real\neigenvalues at {1, \u03b1 + \u03b2 \u22121}. The second eigenvalue is smaller than one in magnitude. Therefore, the\ndominant mode for CTA algorithms is given by\nmodeopt\ncta \u2248modeunf\ncta \u22481 \u22122\u00b5\u03bbmin(Ru)\n(61)\n3) Operation Curve for ATC Diffusion LMS: Based on Assumptions 1\u20133, the network EMSE for\noptimal ATC and uniform ATC are given by (45) and (51), respectively. The modes of mean-square\nconvergence for ATC algorithms are also determined by the eigenvalues of [A \u2297(IM \u2212\u00b5RT\nu)] \u2297[A \u2297\n(IM \u2212\u00b5Ru)] [4]. Therefore, the dominant mode for ATC is also\nmodeopt\natc \u2248modeunf\natc \u22481 \u22122\u00b5\u03bbmin(Ru)\n(62)\n4) Operation Curves for Block LMS and Incremental LMS: The EMSE for block LMS and incremental\nLMS are given by (53) and (56), respectively. In Appendix C, we show that their dominant mode is\nmodeblk \u2248modeinc \u22481 \u22124\u00b5\u2032\u03bbmin(Ru)\n(63)\nWe plot the operation curves for all algorithms in Fig. 4. From the \ufb01gure we observe that (i) optimal\nATC and optimal CTA have similar performance and outperform all other strategies; (ii) block LMS\nNovember 5, 2018\nDRAFT\n19\nand incremental LMS have similar performance to uniform ATC and uniform CTA; (iii) the non-\ncooperative stand-alone LMS implementation has the worst performance. In the following, we shall \ufb01x\nthe convergence rate at the same value for all strategies and then compare their EMSE levels analytically.\nB. Common Convergence Rate\nAs was mentioned before, the performance of each algorithm is dictated by two factors: its steady-state\nEMSE and its mean-square convergence rate, and both factors are functions of the step-size \u00b5. In order\nto make a fair comparison among the algorithms, we shall \ufb01x one factor and then compare them in terms\nof the other factor, and vice versa.\nFrom (60), (61), and (62), we know that ATC algorithms, CTA algorithms, and stand-alone LMS \ufb01lters\nhave (approximately) the same dominant mode for mean-square convergence:\nmode1 \u225c1 \u22122\u00b5\u03bbmin(Ru)\n(64)\nFor block LMS and incremental LMS, from (63), their dominant mode of convergence is approximately\nmode2 \u225c1 \u22124\u00b5\u2032\u03bbmin(Ru)\n(65)\nIn order to make all algorithms converge at the same rate, i.e., mode1 = mode2, we enforce the relation:\n\u00b5 = 2\u00b5\u2032\n(66)\nAn intuitive explanation for (66) is that, for a set of data {d1(i), d2(i); u1,i, u2,i}, incremental LMS\nperforms two successive iterations while each stand-alone LMS \ufb01lter performs only one iteration. For\nthis reason, the step-size of incremental LMS needs to be half the value of that for stand-alone LMS\nin order for both classes of algorithms to converge at the same rate. Based on condition (66), we now\nmodify Table I into Table IV, and proceed to compare the EMSE for various strategies. Although we\nfocus on comparing the EMSE performance, similar arguments can be applied to the MSD performance\nof the various strategies.\nC. Comparing Network EMSE\nWe use Table IV to compare the network EMSE. First, the harmonic and arithmetic means of {\u03c32\nv,1, \u03c32\nv,2}\nare de\ufb01ned as\n\u03c32\nharm \u225c\n2\u03c32\nv,1\u03c32\nv,2\n\u03c32\nv,1 + \u03c32\nv,2\n,\n\u03c32\narth \u225c\n\u03c32\nv,1 + \u03c32\nv,2\n2\n(67)\nNovember 5, 2018\nDRAFT\n20\nFig. 4.\nOperation curves for various algorithms when M = 10, Ru = IM, \u03c32\nv,1 = 0.01, and \u03c32\nv,2 = 0.001.\nTABLE IV\nNETWORK EMSE VALUES FROM TABLE I USING \u00b5 = 2\u00b5\u2032\nType\nNetwork EMSE\nAcronym\nOpt. ATC (21)\nc1\u03c32\nharm\n(45)\nEMSE\nopt\natc\nOpt. CTA (20)\nc1\u03c32\nharm + c2\n\u00002\u03c32\narth \u2212\u03c32\nharm\n\u0001\n(39)\nEMSE\nopt\ncta\nUnf. ATC (21)\nc1\u03c32\narth\n(51)\nEMSE\nunf\natc\nUnf. CTA (20)\n(c1 + c2) \u03c32\narth\n(49)\nEMSE\nunf\ncta\nBlk. LMS (14)\nc1\u03c32\narth\n(53)\nEMSEblk\nInc. LMS (15)\nc1\u03c32\narth\n(56)\nEMSEinc\nStd. LMS (3)\n2c1\u03c32\narth\n(26)\nEMSEind\n1 \u03c32\narth \u225c\n\u03c32\nv,1+\u03c32\nv,2\n2\nand \u03c32\nharm \u225c\n2\u03c32\nv,1\u03c32\nv,2\n\u03c32\nv,1+\u03c32\nv,2 , where \u03c32\nv,2 < \u03c32\nv,1.\n2 c1 \u225c\u00b5Tr(Ru)\n4\nand c2 \u225c\u00b52\u2225\u03bb\u22252\n2\n.\nand it holds that \u03c32\nharm < \u03c32\narth. As a result, it is easy to verify that\nEMSE\nopt\natc < EMSE\nopt\ncta\nand\nEMSE\nunf\natc < EMSE\nunf\ncta\n(68)\nalthough their values are close to each other since c2 is proportional to \u00b52 and \u00b5 is assumed to be\nsuf\ufb01ciently small by Assumption 2. For CTA-type algorithms, it is further straightforward to verify that\nEMSE\nopt\ncta < EMSE\nunf\ncta < EMSEind\n(69)\nNovember 5, 2018\nDRAFT\n21\nTABLE V\nCOMPARING NETWORK EMSES FOR VARIOUS ALGORITHMS\nOpt. ATC\nOpt. CTA\nUnf. ATC\nUnf. CTA\nBlk./Inc. LMS1\nStd. LMS\nOpt. ATC\nbetter3\nbetter\nbetter\nbetter\nbetter\nOpt. CTA\nworse3\nbetter2\nbetter\nbetter2\nbetter\nUnf. ATC\nworse\nworse2\nbetter3\nequal\nbetter\nUnf. CTA\nworse\nworse\nworse3\nworse3\nbetter\nBlk./Inc. LMS1\nworse\nworse2\nequal\nbetter3\nbetter\nStd. LMS\nworse\nworse\nworse\nworse\nworse\n1 The step-sizes for block and incremental LMS are half the value for the other algorithms.\n2 If 2\u00b5\u03c32\nu < (\u03c32\narth \u2212\u03c32\nharm)/(2\u03c32\narth \u2212\u03c32\nharm), which is generally true under Assumption 2.\n3 By a small margin on the order of \u00b52.\nsince, under Assumption 2,\nc2 < c1 =\u21d2c2(\u03c32\narth \u2212\u03c32\nharm) < c1(\u03c32\narth \u2212\u03c32\nharm)\n=\u21d2c1\u03c32\nharm + c2\u03c32\narth < c1\u03c32\narth + c2\u03c32\nharm\n=\u21d2c1\u03c32\nharm + 2c2\u03c32\narth < (c1 + c2)\u03c32\narth + c2\u03c32\nharm\n=\u21d2c1\u03c32\nharm + c2(2\u03c32\narth \u2212\u03c32\nharm) < (c1 + c2)\u03c32\narth\nSimilarly, for ATC-type algorithms, we get\nEMSE\nopt\natc < EMSE\nunf\natc < EMSEind\n(70)\nThe relation between optimal CTA and uniform ATC depends on the parameters {\u03c32\nharm, \u03c32\narth, c1, c2} since\nEMSE\nopt\ncta < EMSE\nunf\natc \u21d0\u21d2c2\nc1\n< \u03c32\narth \u2212\u03c32\nharm\n2\u03c32\narth \u2212\u03c32\nharm\n(71)\nwhich is usually true under Assumption 2. Uniform ATC, block LMS, and incremental LMS have the\nsame performance:\nEMSEblk = EMSEinc = EMSE\nunf\natc\n(72)\nNovember 5, 2018\nDRAFT\n22\nHence, optimal ATC outperforms block LMS and incremental LMS:\nEMSE\nopt\natc < EMSEblk/inc\n(73)\nThe relations between optimal CTA, block LMS, and incremental LMS also depend on the parameters\n{\u03c32\nharm, \u03c32\narth, c1, c2} since\nEMSE\nopt\ncta < EMSEblk/inc \u21d0\u21d2c2\nc1\n< \u03c32\narth \u2212\u03c32\nharm\n2\u03c32\narth \u2212\u03c32\nharm\n(74)\nwhich is the same condition as (71). Block LMS and incremental LMS outperform uniform CTA:\nEMSEblk/inc < EMSE\nunf\ncta\n(75)\nbut only by a small margin since c2 is proportional to \u00b52. We summarize the network EMSE relationships\nin Table V. Entries of Table V should be read from left to right. For example, the entry (in italics) on\nthe second row and third column should be read to mean: \u201coptimal ATC is better than optimal CTA (i.e.,\nit results in lower EMSE)\u201d.\nD. Comparing Individual Node EMSE\nWe compare the EMSE of node 1 under various strategies using Table II. First, node 1 in optimal\nATC outperforms that in optimal CTA: EMSEopt\natc,1 < EMSEopt\ncta,1. But more importantly, node 1 in optimal\nCTA outperforms that in stand-alone LMS because EMSEind,1 < EMSEopt\ncta,1 when c2 < c1, which is true\nunder Assumption 2. Recall that node 1 has larger noise variance than node 2. Therefore, ATC and CTA\ncooperation helps it attain better EMSE value than what it would obtain if it operates independently.\nLikewise, we compare the EMSE of node 2 using Table II. Node 2 in optimal ATC performs better\nthan that in optimal CTA: EMSEopt\natc,2 < EMSEopt\ncta,2. Again, and importantly, node 2 in optimal CTA\noutperforms that in stand-alone LMS because EMSEopt\ncta,2 < EMSEind,2 when c2 < c1, which is again true\nunder Assumption 2. Although node 2 has less noise than node 1, it still bene\ufb01ts from cooperating with\nnode 1 and is able to reduce its EMSE below what it would obtain if it operates independently.\nThe relations between the EMSE for both nodes 1 and 2 under various strategies are the same \u2014 node\n2 always outperforms node 1 due to the lower noise level. Table VI summarizes the results.\nE. Simulations Results\nWe compare the network EMSE for various strategies in Fig. 5. The length of wo is M = 10 and\nits entries are randomly selected. The regression data {uk,i} and noise signals {vk(i)} are i.i.d. white\nGaussian distributed with zero mean and Ru = IM, \u03c32\nv,1 = 0.01, and \u03c32\nv,2 = 0.002. The results are\nNovember 5, 2018\nDRAFT\n23\nTABLE VI\nCOMPARING INDIVIDUAL NODE EMSE VALUES FOR VARIOUS STRATEGIES\nOpt. ATC\nOpt. CTA\nStd. LMS\nOpt. ATC\nbetter\nbetter\nOpt. CTA\nworse\nbetter\nStd. LMS\nworse\nworse\nFig. 5.\nComparison of network EMSE when M = 10, Ru = IM, \u03c32\nv,1 = 0.01, \u03c32\nv,2 = 0.002, and \u00b5 = 0.01.\naveraged over 500 trials. From the simulation results, we can see that although centralized algorithms\nlike (14) and (15) can offer a better estimate than the non-cooperative LMS algorithms (3), they can be\noutperformed by the diffusion strategies (20) and (21) . When the combination coef\ufb01cients of ATC or\nCTA algorithms are chosen according to the relative degree-variance rule (38), these diffusion strategies\ncan achieve lower EMSE by a signi\ufb01cant margin. In addition, we compare the EMSE of each node in\nthe network for various strategies in Figs. 6a\u20136b.\nVI. PERFORMANCE OF N-NODE ADAPTIVE NETWORKS\nIn the previous sections, we focused on two-node networks and were able to analytically characterize\ntheir performance, and to establish the superiority of the diffusion strategies over the centralized block\nor incremental LMS implementations. We now extend the results to N-node ad-hoc networks. First, we\nNovember 5, 2018\nDRAFT\n24\n(a) Node 1.\n(b) Node 2.\nFig. 6.\nComparison of individual EMSE when M = 10, Ru = IM, \u03c32\nv,1 = 0.01, \u03c32\nv,2 = 0.002, and \u00b5 = 0.01.\nestablish that for suf\ufb01ciently small step-sizes and for any doubly-stochastic combination matrix A, i.e.,\nits rows and columns add up to one, the ATC diffusion strategy matches the performance of centralized\nblock LMS. Second, we argue that by optimizing over the larger class of left-stochastic combination\nmatrices, which include doubly-stochastic matrices as well, the performance of ATC can be improved\nrelative to block LMS. Third, we provide a fully-distributed construction for the combination weights in\norder to minimize the network EMSE for ATC. We illustrate the results by focusing on ATC strategies\nbut they apply to CTA strategies as well.\nThus, consider a connected network consisting of N-nodes. Each node k collects measurement data\nthat satisfy the linear regression model (1). The noise variance at each node k is \u03c32\nv,k. We continue to\nuse Assumptions 1\u20133. Each node k runs the following ATC diffusion strategy [4]:\n\uf8f1\n\uf8f4\n\uf8f4\n\uf8f2\n\uf8f4\n\uf8f4\n\uf8f3\n\u03c8k,i = wk,i\u22121 + \u00b5u\u2217\nk,i [dk(i) \u2212uk,iwk,i\u22121]\nwk,i =\nX\nl\u2208Nk\nalk\u03c8l,i\n(76)\nwhere alk denotes the positive weight that node k assigns to data arriving from its neighbor l; these\nweights are collected into an N \u00d7 N combination matrix A, and Nk consists of all neighbors of node k\nincluding k itself. The weights {alk} satisfy the following properties:\nX\nl\u2208Nk\nalk = 1,\nalk > 0 if l \u2208Nk, and alk = 0 if l /\u2208Nk\n(77)\nNovember 5, 2018\nDRAFT\n25\nA. EMSE and MSD for ATC Diffusion LMS\nObserve that A is a left-stochastic matrix (the entries on each of its columns add up to one). Let\nA = TDT \u22121 denote the eigen-decomposition of A, where T is a real and invertible matrix and D is\nin the real Jordan canonical form [45], [46]. We assume that A is a primitive/regular matrix, meaning\nthat there exists an integer m such that all entries of Am are strictly positive [45], [47]. This condition\nessentially states that for any two nodes in the network, there is a path of length m linking them. Since we\nassume a connected network and allow for loops because of (77), it follows that A satis\ufb01es the regularity\ncondition [2], [45], [47]. Then, from the Perron-Frobenius theorem [45], [47], the largest eigenvalue in\nmagnitude of A is unique and is equal to one. Therefore, D has the following form:\nD =\n\uf8ee\n\uf8ef\uf8f0\n1\nJ\n\uf8f9\n\uf8fa\uf8fb\n(78)\nwhere the N \u22121 \u00d7 N \u22121 matrix J consists of real stable Jordan blocks. From Appendix A, the network\nEMSE and MSD for ATC diffusion are given by\nEMSEatc \u2248\u00b52\nN\nM\nX\nm=1\n\u03bb2\nmvec(DTT TRvTD)T(IN 2 \u2212\u03bemD \u2297D)\u22121vec(T \u22121T \u2212T)\n(79)\nMSDatc \u2248\u00b52\nN\nM\nX\nm=1\n\u03bbmvec(DTT TRvTD)T(IN 2 \u2212\u03bemD \u2297D)\u22121vec(T \u22121T \u2212T)\n(80)\nwhere \u03bem is given by (59) and\nRv \u225cdiag{\u03c32\nv,1, . . . , \u03c32\nv,N}\n(81)\nFrom (59) and (78), we get\n\u00b5(IN 2 \u2212\u03bemD \u2297D)\u22121 =\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0\n(2\u03bbm)\u22121\n\u00b5(I \u2212\u03bemJ)\u22121\n\u00b5(I \u2212\u03bemJ)\u22121\n\u00b5(I \u2212\u03bemJ \u2297J)\u22121\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb\n(82)\nwhere, to simplify the notation, we are omitting the subscripts of the identity matrices. Since J is stable\nand 0 < \u03bem < 1 under Assumption 2, we have\n\u00b5(I \u2212\u03bemJ)\u22121 = \u00b5(I \u2212J + 2\u00b5\u03bbmJ)\u22121 \u2248\u00b5(I \u2212J)\u22121 = O(\u00b5)\n(83)\n\u00b5(I \u2212\u03bemJ \u2297J)\u22121 = \u00b5(I \u2212J \u2297J + 2\u00b5\u03bbmJ \u2297J)\u22121 \u2248\u00b5(I \u2212J \u2297J)\u22121 = O(\u00b5)\n(84)\nNovember 5, 2018\nDRAFT\n26\nTherefore, by Assumption 2, we can ignore all blocks on the diagonal of (82) with the exception of the\nleft-most corner entry so that:\n\u00b5(IN 2 \u2212\u03bemD \u2297D)\u22121 \u2248(2\u03bbm)\u22121E11 \u2297E11\n(85)\nwhere E11 now denotes the N \u00d7 N matrix given by E11 = diag{1, 0, 0, . . . , 0}. Then,\n\u00b5 vec(DTT TRvTD)T(IN 2 \u2212\u03bemD \u2297D)\u22121vec(T \u22121T \u2212T)\n\u2248vec(DTT TRvTD)T[(2\u03bbm)\u22121E11 \u2297E11]vec(T \u22121T \u2212T)\n= (2\u03bbm)\u22121vec(Rv)T(TE11T \u22121 \u2297TE11T \u22121)vec(IN)\n(86)\nwhere we used the fact that DE11 = E11 because of (78) and vec(ABC) = (CT\u2297A)vec(B) for matrices\n{A, B, C} of compatible dimensions. Now, note that TE11T \u22121 is a rank-one matrix determined by the\nouter product of the left- and right-eigenvectors of A corresponding to the unique eigenvalue at one.\nSince A is left-stochastic, this left-eigenvector can be selected as the all-one vector 1, i.e., AT1 = 1. Let\nus denote the right-eigenvector by y and normalize its element-sum to one, i.e., Ay = y and yT1 = 1.\nIt follows from the Perron-Frobenius theorem [45], [47] that all entries of y are nonnegative and located\nwithin the range [0, 1]. We then get TE11T \u22121 = y1T. Thus, from (86), the network EMSE (79) can be\nrewritten as\nEMSEatc \u2248\u00b5Tr(Ru)\n2N\nvec(Rv)T(y1T \u2297y1T)vec(IN)\n= \u00b5Tr(Ru)\n2N\nvec(Rv)Tvec(y1T1yT)\n(87)\nThat is,\nEMSEatc \u2248\u00b5Tr(Ru)\n2\nyTRvy\n(88)\nSimilarly, the network MSD (80) can be rewritten as\nMSDatc \u2248\u00b5M\n2 yTRvy\n(89)\nB. EMSE and MSD for Block and Incremental LMS\nFor N-nodes, the block LMS recursion (14) is replaced by\nwi = wi\u22121 + \u00b5\u2032\nN\nX\nk=1\nu\u2217\nk,i [dk(i) \u2212uk,iwi\u22121]\n(90)\nNovember 5, 2018\nDRAFT\n27\nand the incremental LMS recursion (15) is replaced by\n\uf8f1\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f2\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f3\nfor every i:\ninitialize with \u03c80 = wi\u22121\nfor every k = 1, 2, . . . , N, repeat :\n\u03c8k = \u03c8k\u22121 + \u00b5\u2032u\u2217\nk,i [dk(i) \u2212uk,i\u03c8k\u22121]\nset wi = \u03c8N\nend\n(91)\nIn order for block and incremental LMS to converge at the same rate as diffusion ATC, we must set\ntheir step-sizes to \u00b5\u2032 = \u00b5/N (compare with (66)). Following an argument similar to the one presented\nin Appendix C, we can derive the EMSE and MSD for the block LMS strategy (90) as\nEMSEblk \u2248\u00b5Tr(Ru)\n2\nTr(Rv)\nN 2\n(92)\nand\nMSDblk \u2248\u00b5M\n2\nTr(Rv)\nN 2\n(93)\nrespectively. A similar argument to (55) (see also expression (84) in [25]) leads to the conclusion that\nthe performance of incremental LMS (91) can be well approximated by that of block LMS for small\nstep-sizes2. Therefore,\nEMSEinc \u2248\u00b5Tr(Ru)\n2\nTr(Rv)\nN 2\n(94)\nand\nMSDinc \u2248\u00b5M\n2\nTr(Rv)\nN 2\n(95)\nFor this reason, we shall not distinguish between block LMS and incremental LMS in the sequel.\nC. Comparing Network EMSE\nObserve that the EMSE expression (88) for ATC diffusion LMS and (92) for block and incremental\nLMS only differ by a scaling factor, namely, yTRvy versus Tr (Rv) /N 2. Then, ATC diffusion would\n2Again, we remark that in general incremental LMS outperforms block LMS [44]; however, their performance are similar\nwhen the step-size is suf\ufb01ciently small [25, App. A].\nNovember 5, 2018\nDRAFT\n28\noutperform block LMS and incremental LMS when\nyTRvy < Tr(Rv)\nN 2\n(96)\nwhere Rv is diagonal and given by (81). We assume that the noise variance of at least one node in\nthe network is different from the other noise variances to exclude the case in which the noise pro\ufb01le is\nuniform across the network (in which case Rv would be a scaled multiple of the identity matrix). Thus,\nnote that, if we select the combination matrix A to be doubly-stochastic, i.e., A1 = 1 and AT1 = 1,\nthen it is straightforward to see that y = 1/N so that\nyTRvy = Tr(Rv)\nN 2\n(97)\nThis result means that, for suf\ufb01ciently small step-sizes and for any doubly-stochastic matrix A, the\nEMSE performance of ATC diffusion and block LMS match each other. However, as indicated by (77),\nthe diffusion LMS strategy can employ a broader class of combination matrices, namely, left-stochastic\nmatrices. If we optimize over the larger set of left-stochastic combination matrices and in view of (97),\nwe would expect\nEMSEblk \u2248EMSEatc(Adoubly-stochastic) \u2265EMSEatc(Aopt)\n(98)\nwhere Aopt is the optimal combination matrix that solves the following optimization problem:\nAopt \u225carg min\nA\u2208A\n\u00b5Tr(Ru)\n2\nyTRvy\nsubject to\nAy = y, 1Ty = 1\n(99)\nwhere A denotes the set consisting of all N \u00d7 N left-stochastic matrices whose entries {alk} satisfy the\nconditions in (77). We show next how to determine left-stochastic matrices that solve (99).\nFirst note that the optimization problem (99) is equivalent to the following non-convex problem:\nminimize\nA\u2208A, y\u2208R+\nyTRvy\nsubject to\nAy = y,\n1Ty = 1\n(100)\nwhere R+ denotes the N \u00d7 1 nonnegative vector space. We solve this problem in two steps. First we\nsolve the relaxed problem:\nminimize\ny\u2208R+\nyTRvy\nsubject to\n1Ty = 1\n(101)\nSince Rv is positive de\ufb01nite and diagonal, the closed-form solution for (101) is given by\nyo \u225c\nR\u22121\nv 1\n1TR\u22121\nv 1\n(102)\nNovember 5, 2018\nDRAFT\n29\nNext, if we can determine a primitive left-stochastic matrix A whose right eigenvector associated to\neigenvalue 1 coincides with yo, then we would obtain a solution to (100). Indeed, note that any primitive\nleft-stochastic matrix A can be regarded as the probability transition matrix of an irreducible aperiodic\nMarkov chain (based on the connected topology and condition (77) on the weights) [48], [49]. In that\ncase, a vector yo that satis\ufb01es Ayo = yo would correspond to the stationary distribution vector for the\nMarkov chain. Now given an arbitrary vector yo, whose entries are positive and add up to one, it is\nknown how to construct a left-stochastic matrix A that would satisfy Ayo = yo. A procedure due to\nHastings [50] was used in [51] to construct such matrices. Applying the procedure to our vector yo given\nby (102), we arrive at the following combination rule, which we shall refer to as the Hastings rule (we\nmay add that there are many other choices for A that would satisfy the same requirement Ayo = yo):\nHastings rule:\nalk =\n\uf8f1\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f2\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f3\n\u03c32\nv,k\nmax{|Nk|\u03c32\nv,k, |Nl|\u03c32\nv,l},\nl \u2208Nk\\{k}\n1 \u2212\nX\nl\u2208Nk\\{k}\nalk,\nl = k\n(103)\nwhere |Nk| denotes the cardinality of Nk. It is worth noting that the Hastings rule is a fully-distributed\nsolution \u2014 each node k only needs to obtain the degree-variance product (|Nl|\u22121)\u03c32\nv,l from its neighbor\nl to compute the corresponding combination weight alk. By using the Hastings rule (103), the vector yo\nin (102) is attained and the EMSE expression (88) is therefore minimized. The minimum value of (88)\nis then given by\nEMSE\nopt\natc \u2248\u00b5Tr(Ru)\n2\nyoTRvyo = \u00b5Tr(Ru)\n2\n1\nTr(R\u22121\nv )\n(104)\nCompared to the EMSE of block and incremental LMS (92) and (94), we conclude that diffusion strategies\nusing the Hastings rule (103) achieve a lower EMSE level under Assumption 2. This is because, from\nthe Cauchy-Schwarz inequality [16], we have\nN 2 < Tr(Rv)Tr(R\u22121\nv ) \u21d0\u21d2\n1\nTr(R\u22121\nv ) < Tr(Rv)\nN 2\n(105)\nwhen the entries on the diagonal of Rv are not uniform (as we assumed at the beginning of this subsection).\nIn real applications, where the noise variances are unavailable, each node can estimate its own noise\nvariance recursively by using the following iteration:\nb\u03c32\nv,k(i) = (1 \u2212\u03bdk)b\u03c32\nv,k(i \u22121) + \u03bdk|dk(i) \u2212uk,iwk,i\u22121|2\n(106)\nRemark: In the two-node case, we determined the combination weights (38) by seeking coef\ufb01cients\nthat essentially minimize the EMSE expressions (37) and (44). The argument in Appendix B expressed\nNovember 5, 2018\nDRAFT\n30\n(a) Network topology and noise pro\ufb01le.\n(b) EMSE curves.\nFig. 7.\nSimulated EMSE curves and theoretical results for ATC diffusion versus block LMS for a network with N = 20 nodes.\nthe EMSE as the sum of two factors: a dominant factor that depends on \u00b5 and a less dominant factor\nthat depends on higher powers of \u00b5. In the N-node network case, we instead used the small step-size\napproximation to arrive at expressions (88) and (89), which correspond only to the dominant terms in the\nEMSE and MSD expressions and depend on \u00b5. We can regard (88) and (89) as \ufb01rst-order approximations\nfor the performance of the network for suf\ufb01ciently small step-sizes.\nD. Simulation Results\nWe simulate ATC diffusion LMS versus block LMS over a connected network with N = 20 nodes.\nThe unknown vector wo of length M = 3 is randomly generated. We adopt Ru = IM, \u00b5 = 0.005 for ATC\ndiffusion LMS, and \u00b5\u2032 = \u00b5/N = 0.00025 for block LMS. The network topology and the pro\ufb01le of noise\nvariances {\u03c32\nv,k} are plotted in Fig. 7a. For ATC algorithms, we simulate three different combination rules:\nthe \ufb01rst one is the (left-stochastic) adaptive Hastings rule (103) using (106) and without the knowledge\nof noise variances, the second one is the Hastings rule (103) with the knowledge of noise variance, and\nthe third one is the (doubly-stochastic) Metropolis rule [4], [52] (which is a simpli\ufb01ed version of the\nHastings rule):\nMetropolis rule:\nalk =\n\uf8f1\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f2\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f3\n1\nmax{|Nk|, |Nl|},\nl \u2208Nk\\{k}\n1 \u2212\nX\nl\u2208Nk\\{k}\nalk,\nl = k\n(107)\nNovember 5, 2018\nDRAFT\n31\nWe adopted \u03bdk = 0.1 and \u00b5 = 0.0054 for the adaptive Hastings rule (103)\u2013(106) to match the convergence\nrate of the other algorithms. We also consider the non-cooperative LMS case for comparison purposes.\nThe EMSE learning curves are obtained by averaging over 50 experiments and are plotted in Fig. 7b.\nIt can be seen that ATC diffusion LMS with Metropolis weights exhibits almost the same convergence\nbehavior as block LMS in transient phase and attains a steady-state value that is less than 1 dB worse\nthan block LMS. In comparison, ATC diffusion LMS using adaptive Hastings weights (where the noise\nvariances are estimated through (106)) has almost the same learning curve as ATC using Hastings weights\nwith the knowledge of the noise variances; both of them are able to attain about 7 dB gain over block\nLMS at steady-state.\nVII. CONCLUSION\nIn this work we derived the EMSE levels for different strategies over LMS adaptive networks and\ncompared their performance. The results establish that diffusion LMS strategies can deliver lower EMSE\nthan centralized solutions employing traditional block or incremental LMS strategies. We \ufb01rst studied\nthe case of networks involving two cooperating nodes, where closed-form expressions for the EMSE\nand MSD can be derived. Subsequently, we extended the conclusion to generic N-node networks and\nestablished again that, for suf\ufb01ciently small step-sizes, diffusion strategies can outperform centralized\nblock LMS strategies by optimizing over left-stochastic combination matrices. It is worth noting that\nalthough the optimized combination rules rely on knowledge of the noise statistics, it is possible to\nemploy adaptive strategies like (106) to adjust these coef\ufb01cients on the \ufb02y without requiring explicit\nknowledge of the noise pro\ufb01le \u2014 in this way, the Hastings rule (103) can be implemented in a manner\nsimilar to the adaptive relative variance rule [2], [37], [40]. Clearly, the traditional block and incremental\nimplementations (90) and (91) can be modi\ufb01ed to incorporate information about the noise pro\ufb01le as well.\nIn that case, it can be argued that diffusion strategies are still able to match the EMSE performance of\nthese modi\ufb01ed centralized algorithms.\nAPPENDIX A\nEMSE EXPRESSION FOR GENERAL DIFFUSION LMS WITH N-NODES\nUnder Assumptions 1\u20133, the EMSE expression for node k of the general diffusion strategy (28)\u2013(30)\nis given by Eq. (39) from reference [4] (see also [2]):\nEMSEk \u2248[vec(YT)]T(IN 2M 2 \u2212F)\u22121vec(Ekk \u2297Ru)\n(108)\nNovember 5, 2018\nDRAFT\n32\nwhere\nY = \u00b52(QTRvQ) \u2297Ru\n(109)\nF = BT \u2297B\u2217\n(110)\nB = (QTP T) \u2297(IM \u2212\u00b5Ru)\n(111)\nand Ekk = diag{0, . . . , 0, 1, 0, . . . , 0} is an N\u00d7N all-zero matrix except for the kth entry on the diagonal,\nwhich is equal to one. Since for ATC algorithms, P = IN and Q = A, and for CTA algorithms, P = A\nand Q = IN, we know that PQ = A for both cases. Therefore, we get\nB = AT \u2297(IM \u2212\u00b5Ru)\n(112)\nWe can reduce (108) into the form (32), which is more suitable for our purposes, by introducing the\neigen-decompositions of Ru and A. Thus, let Ru = U\u039bU \u2217denote the eigen-decomposition of Ru, where\nU is unitary and \u039b is diagonal with positive entries. Let also A = TDT \u22121 denote the eigen-decomposition\nof the real matrix A, where T is real and invertible and D is in the real Jordan canonical form [45],\n[46]. Then, the eigen-decomposition of B is given by\nB = (TDT \u22121)T \u2297[U(IM \u2212\u00b5\u039b)U \u2217]\n= (T \u2212T \u2297U)[DT \u2297(IM \u2212\u00b5\u039b)](T T \u2297U \u2217)\n(113)\nand the eigen-decomposition of F is then given by\nF = {(T \u2212T \u2297U)[DT \u2297(IM \u2212\u00b5\u039b)](T T \u2297U \u2217)}T \u2297{(T \u2212T \u2297U)[DT \u2297(IM \u2212\u00b5\u039b)](T T \u2297U \u2217)}\u2217\n= X{[DT \u2297(IM \u2212\u00b5\u039b)]T \u2297[DT \u2297(IM \u2212\u00b5\u039b)]\u2217}X \u22121\n= X{[D \u2297(IM \u2212\u00b5\u039b)] \u2297[D \u2297(IM \u2212\u00b5\u039b)]}X \u22121\n= X(G \u2297G)X \u22121\n(114)\nwhere we used the facts that {D, \u039b} are real and \u039b is diagonal, and introduced the matrices:\nX \u225c(T T \u2297U \u2217)T \u2297(T T \u2297U \u2217)\u2217\n(115)\nG \u225cD \u2297(IM \u2212\u00b5\u039b)\n(116)\nNovember 5, 2018\nDRAFT\n33\nThen, from (109)\u2013(115), we get\nX Tvec(YT) = [(T T \u2297U \u2217) \u2297(T \u2217\u2297U T)] \u00b7 \u00b52vec(QTRvQ \u2297RT\nu)\n= \u00b52vec\nh\n(T \u2217\u2297U T)(QTRvQ \u2297RT\nu)(T \u2297U \u2217T)\ni\n= \u00b52vec(T TQTRvQT \u2297\u039b)\n(117)\nwhere we used the fact that T is real. Likewise, we get\nX \u22121vec(Ekk \u2297Ru) = [(T \u2212T \u2297U)T \u2297(T \u2212T \u2297U)\u2217] \u00b7 vec(Ekk \u2297Ru)\n= vec(T \u22121EkkT \u2212T \u2297\u039b)\n(118)\nThen, from (114)\u2013(118), the EMSE expression in (108) can be rewritten as\nEMSEk \u2248[vec(YT)]TX(IN 2M 2 \u2212G \u2297G)\u22121X \u22121vec(Ekk \u2297Ru)\n= \u00b52[vec(T TQTRvQT \u2297\u039b)]T(IN 2M 2 \u2212G \u2297G)\u22121vec(T \u22121EkkT \u2212T \u2297\u039b)\n(119)\nUsing the fact that G in (116) is stable under Assumption 2, we can further obtain\nEMSEk \u2248\u00b52[vec(T TQTRvQT \u2297\u039b)]T\n\uf8eb\n\uf8ed\n\u221e\nX\nj=0\nGj \u2297Gj\n\uf8f6\n\uf8f8vec(T \u22121EkkT \u2212T \u2297\u039b)\n= \u00b52[vec(T TQTRvQT \u2297\u039b)]T\n\u221e\nX\nj=0\nvec\nh\nGj(T \u22121EkkT \u2212T \u2297\u039b)GTji\n= \u00b52\n\u221e\nX\nj=0\nTr\nh\n(T TQTRvQT \u2297\u039b)Gj(T \u22121EkkT \u2212T \u2297\u039b)GTji\n(120)\nwhere we used the identities vec(ABC) = (CT \u2297A)vec(B) and Tr(AB) = [vec(AT)]Tvec(B) for\nmatrices {A, B, C} of compatible dimensions. From (115), we get\nTr\nh\n(T TQTRvQT \u2297\u039b)Gj(T \u22121EkkT \u2212T \u2297\u039b)GTji\n= Tr\nn\n(T TQTRvQT \u2297\u039b)[Dj \u2297(IM \u2212\u00b5\u039b)j](T \u22121EkkT \u2212T \u2297\u039b)[DTj \u2297(IM \u2212\u00b5\u039b)j]\no\n= Tr\nh\nT TQTRvQTDjT \u22121EkkT \u2212TDTj \u2297\u039b(IM \u2212\u00b5\u039b)j\u039b(IM \u2212\u00b5\u039b)ji\n= Tr\nh\n\u039b(IM \u2212\u00b5\u039b)j\u039b(IM \u2212\u00b5\u039b)j \u2297T TQTRvQTDjT \u22121EkkT \u2212TDTji\n=\nM\nX\nm=1\n\u03bb2\nm(1 \u2212\u00b5\u03bbm)2jTr(T TQTRvQTDjT \u22121EkkT \u2212TDTj)\n=\nM\nX\nm=1\n\u03bb2\nm(1 \u2212\u00b5\u03bbm)2j[vec(T TQTRvQT)]T(Dj \u2297Dj)vec(T \u22121EkkT \u2212T)\n(121)\nNovember 5, 2018\nDRAFT\n34\nwhere we used the identity Tr(A \u2297B) = Tr(B \u2297A) for square matrices {A, B} and the fact that\n\u039b(IM \u2212\u00b5\u039b)j\u039b(IM \u2212\u00b5\u039b)j is diagonal. Substituting (121) back into (120) leads to\nEMSEk \u2248\u00b52\nM\nX\nm=1\n\u03bb2\nm[vec(T TQTRvQT)]T\n\uf8ee\n\uf8f0\n\u221e\nX\nj=0\n(1 \u2212\u00b5\u03bbm)2jDj \u2297Dj\n\uf8f9\n\uf8fbvec(T \u22121EkkT \u2212T)\n= \u00b52\nM\nX\nm=1\n\u03bb2\nm[vec(T TQTRvQT)]T[IN 2 \u2212(1 \u2212\u00b5\u03bbm)2D \u2297D]\u22121vec(T \u22121EkkT \u2212T)\n\u2248\u00b52\nM\nX\nm=1\n\u03bb2\nm[vec(T TQTRvQT)]T[IN 2 \u2212(1 \u22122\u00b5\u03bbm)D \u2297D]\u22121vec(T \u22121EkkT \u2212T)\n(122)\nwhere (1 \u2212\u00b5\u03bbm)2 \u22481 \u22122\u00b5\u03bbm due to Assumption 2.\nAPPENDIX B\nMINIMIZING THE NETWORK PERFORMANCE FOR DIFFUSION LMS\nTo minimize the network EMSE for CTA given by (37), we introduce two auxiliary variables \u03b7 and \u03b8\nsuch that \u03b1 + \u03b2 = 1 + \u03b7 and 1 \u2212\u03b2 = \u03b8(1 \u2212\u03b1), where \u22121 \u2264\u03b7 < 1 and \u03b8 > 0. The network EMSE (37)\ncan be rewritten as\nEMSEcta \u2248\u00b52\u03c32\nv,1\nM\nX\nm=1\n\u03bb2\nm\n(1 + \u03b8)2\n\u0014 \u03b82 + \u03b3\n1 \u2212\u03bem\n+ (1 \u2212\u03b8)(\u03b8 \u2212\u03b3)\n1 \u2212\u03bem\u03b7\n+ 1 + \u03b3\n2\n1 + \u03b82\n1 \u2212\u03bem\u03b72\n\u0015\n(123)\nwhere \u03b3 \u225c\u03c32\nv,2/\u03c32\nv,1 < 1 and 0 < \u03bem < 1 is given by (33) under Assumption 2. Minimizing expression\n(123) in closed-form over both variables {\u03b8, \u03b7} is generally non-trivial. We exploit the fact that the step-\nsize is suf\ufb01ciently small to help locate the values of \u03b8 and \u03b7 that approximately minimize the value of\n(123). For this purpose, we \ufb01rst substitute (33) into (123) and use Assumption 2 to note that\nEMSEcta \u2248\n\u00b5Tr(Ru)\u03c32\nv,1\n2\n\u03b82 + \u03b3\n(1 + \u03b8)2 + O(\u00b52)\n(124)\nExpression (124) writes the EMSE as the sum of two factors: the \ufb01rst factor is linear in the step-size and\ndepends only on \u03b8, and the second factor depends on higher-order powers of the step-size. For suf\ufb01ciently\nsmall step-sizes, the \ufb01rst factor is dominant and we can ignore the second factor. Doing so allows us to\nestimate the value of \u03b8 that minimizes (123). Observe that the \ufb01rst factor on RHS of (124) is minimized\nat \u03b8o = \u03b3 because\n\u03b32 + \u03b82 \u22652\u03b8\u03b3\n=\u21d2\n\u03b82\u03b3 + \u03b3 + \u03b32 + \u03b82 \u2265\u03b82\u03b3 + \u03b3 + 2\u03b8\u03b3\n=\u21d2\n\u03b82 + \u03b3\n(1 + \u03b8)2 \u2265\n\u03b3\n1 + \u03b3\n(125)\nWe now substitute \u03b8o = \u03b3 back into the original expression (123) for the network EMSE to \ufb01nd that:\nEMSEcta \u2248\u00b52\u03c32\nv,1\nM\nX\nm=1\n\u03bb2\nm\n1 + \u03b3\n\u0014\n\u03b3\n1 \u2212\u03bem\n+\n1 + \u03b32\n2(1 \u2212\u03bem\u03b72)\n\u0015\n(126)\nNovember 5, 2018\nDRAFT\n35\nWe use this form to minimize the higher-order terms of \u00b5 over the variable \u03b7. It is obvious that expression\n(126) is minimized at \u03b7o = 0. The value of EMSE under \u03b8o = \u03b3 and \u03b7o = 0 is then given by\nEMSEcta(\u03b8o = \u03b3, \u03b7o = 0) \u2248\u03c32\nv,1\n\"\n\u03b3\n1 + \u03b3\n\u00b5Tr(Ru)\n2\n+ 1 + \u03b32\n2(1 + \u03b3)\nM\nX\nm=1\n\u00b52\u03bb2\nm\n#\n(127)\nSimilarly, we can employ the same approximate argument to \ufb01nd that the solution (\u03b8o, \u03b7o) essentially\nminimizes the network MSD under Assumption 2; the corresponding value of the MSD is\nMSDcta(\u03b8o = \u03b3, \u03b7o = 0) \u2248\u03c32\nv,1\n\u0014\n\u03b3\n1 + \u03b3\n\u00b5M\n2\n+ 1 + \u03b32\n1 + \u03b3\n\u00b52Tr(Ru)\n2\n\u0015\n(128)\nThe solution {\u03b8o = \u03b3, \u03b7o = 0} translates into (38), where 0 < \u03b3 < 1.\nIn a similar manner, in order to minimize the network EMSE of ATC given by (44), we introduce two\nauxiliary variables \u03b7 and \u03b8 such that \u03b1 + \u03b2 = 1 + \u03b7 and 1 \u2212\u03b2 = \u03b8(1 \u2212\u03b1), where \u22121 \u2264\u03b7 < 1 and\n\u03b8 > 0. Then, from (44) we have\nEMSEatc \u2248\u00b52\u03c32\nv,1\nM\nX\nm=1\n\u03bb2\nm\n\u03bem\n\u0014\n1\n(1 + \u03b8)2\n\u0012 \u03b82 + \u03b3\n1 \u2212\u03bem\n+ (1 \u2212\u03b8)(\u03b8 \u2212\u03b3)\n1 \u2212\u03bem\u03b7\n+ 1 + \u03b3\n2\n1 + \u03b82\n1 \u2212\u03bem\u03b72\n\u0013\n\u22121 + \u03b3\n2\n\u0015\n(129)\nfor which we can again motivate the selection {\u03b8o = \u03b3, \u03b7o = 0}. The value of the network EMSE at\n\u03b8o = \u03b3 and \u03b7o = 0 is then given by\nEMSEatc(\u03b8o = \u03b3, \u03b7o = 0) \u2248\u03c32\nv,1\n\u03b3\n1 + \u03b3\n\u00b5Tr(Ru)\n2\n(130)\nAPPENDIX C\nDERIVATION OF EMSE FOR BLOCK LMS NETWORKS\nWe start from (14). To simplify the notation, we rewrite (1) and (14) as\ndi = Uiwo + vi\n(131)\nwi = wi\u22121 + \u00b5U \u2217\ni (di \u2212Uiwi\u22121)\n(132)\nwhere\nUi \u225ccol{u1,i, u2,i}\n(133)\ndi \u225ccol{d1(i), d2(i)}\n(134)\nvi \u225ccol{v1(i), v2(i)}\n(135)\nThe error recursion is then given by\newi = (IM \u2212\u00b5U \u2217\ni Ui) ewi\u22121 \u2212\u00b5U \u2217\ni vi\n(136)\nNovember 5, 2018\nDRAFT\n36\nLet \u03a3 be an arbitrary M \u00d7 M positive semi-de\ufb01nite matrix that we are free to choose. Using (136), we\ncan evaluate the weighted square quantity \u2225e\nwi\u22252\n\u03a3 \u2261e\nw\u2217\ni \u03a3 ewi. Doing so and taking expectations under\nAssumption 1, we arrive at the following weighted variance relation [16], [44]:\nE\u2225e\nwi\u22252\n\u03a3 = E\u2225e\nwi\u22121\u22252\n\u03a3\u2032 + \u00b52E\u2225U \u2217\ni vi\u22252\n\u03a3\n(137)\nwhere\n\u03a3\u2032 \u225cE(IM \u2212\u00b5U \u2217\ni Ui)\u03a3(IM \u2212\u00b5U \u2217\ni Ui)\n\u2248\u03a3 \u22122\u00b5Ru\u03a3 \u22122\u00b5\u03a3Ru\n(138)\nwhere, in view of Assumption 2, we are dropping higher-order terms in \u00b5. Let again Ru = U\u039bU \u2217denote\nthe eigen-decomposition of Ru. We then introduce the transformed quantities:\nwi \u225cU \u2217wi,\nU i \u225cUiU\n(139)\n\u03a3 \u225cU \u2217\u03a3U,\n\u03a3\n\u2032 \u225cU \u2217\u03a3\u2032U\n(140)\nRelation (137) is accordingly transformed into\nE\u2225wi\u22252\n\u03a3 = E\u2225wi\u22121\u22252\n\u03a3\n\u2032 + \u00b52E\u2225U\n\u2217\ni vi\u22252\n\u03a3\n(141)\nwhere\n\u03a3\n\u2032 \u2248\u03a3 \u22122\u00b5\u039b\u03a3 \u22122\u00b5\u03a3\u039b\n(142)\nSince we are free to choose \u03a3, or equivalently, \u03a3, let \u03a3 be diagonal and nonnegative. Then, it can be\nveri\ufb01ed that \u03a3\n\u2032 is also diagonal and nonnegative under Assumptions 1\u20133 so that\n\u03a3\n\u2032 \u2248(IM \u22124\u00b5\u039b)\u03a3\n(143)\nUnder Assumption 1, the second term on the right-hand side of (141) evaluates to\n\u00b52E\u2225U\n\u2217\ni vi\u22252\n\u03a3 = \u00b52Tr[Rv(E Ui\u03a3U \u2217\ni )]\n(144)\nwhere\nE Ui\u03a3U \u2217\ni = E\n\uf8ee\n\uf8ef\uf8f0\nu1,i\u03a3u\u2217\n1,i\nu1,i\u03a3u\u2217\n2,i\nu2,i\u03a3u\u2217\n1,i\nu2,i\u03a3u\u2217\n2,i\n\uf8f9\n\uf8fa\uf8fb= Tr(\u03a3\u039b)I2\n(145)\nTherefore, we get\n\u00b52E\u2225U\n\u2217\ni vi\u22252\n\u03a3 = \u00b52Tr(Rv)Tr(\u03a3\u039b)\n(146)\nNovember 5, 2018\nDRAFT\n37\nWhen the \ufb01lter is mean-square stable, taking the limit as i \u2192\u221eof both sides of (141) and selecting\n\u03a3 = IM/4\u00b5, we get\nEMSEblk \u2248\u00b5Tr(Ru)\n2\n\u03c32\nv,1 + \u03c32\nv,2\n2\n(147)\nLikewise, by selecting \u03a3 = \u039b\u22121/4\u00b5 and taking the limit of both sides of (141) as i \u2192\u221e, we arrive at\nMSDblk \u2248\u00b5M\n2\n\u03c32\nv,1 + \u03c32\nv,2\n2\n(148)\nACKNOWLEDGMENT\nThe authors would like to acknowledge useful feedback from Ph.D. student Z. Tow\ufb01c on Sec. VI-C.\nREFERENCES\n[1] X. Zhao and A. H. Sayed, \u201cPerformance limits of LMS-based adaptive networks,\u201d in Proc. IEEE Int. Conf. Acoust., Speech,\nSignal Process. (ICASSP), Prague, Czech Republic, May 2011, pp. 3768\u20133771.\n[2] A. H. Sayed, \u201cDiffusion adaptation over netowrks,\u201d available online at http://arxiv.org/abs/1205.4220 as manuscript\narXiv:1205.4220v1 [cs.MA], May 2012.\n[3] C. G. Lopes and A. H. Sayed, \u201cDiffusion least-mean squares over adaptive networks: Formulation and performance\nanalysis,\u201d IEEE Trans. Signal Process., vol. 56, no. 7, pp. 3122\u20133136, Jul. 2008.\n[4] F. S. Cattivelli and A. H. Sayed, \u201cDiffusion LMS strategies for distributed estimation,\u201d IEEE Trans. Signal Process., vol. 58,\nno. 3, pp. 1035\u20131048, Mar. 2010.\n[5] J. Arenas-Garcia, V. Gomez-Verdejo, and A. R. Figueiras-Vidal, \u201cNew algorithms for improved adaptive convex combination\nof LMS transversal \ufb01lters,\u201d IEEE Trans. Instrum. Meas., vol. 54, no. 6, pp. 2239\u20132249, Dec. 2005.\n[6] J. Arenas-Garcia, A. Figueiras-Vidal, and A. H. Sayed, \u201cMean-square performance of a convex combination of two adaptive\n\ufb01lters,\u201d IEEE Trans. Signal Process., vol. 54, no. 3, pp. 1078\u20131090, Mar. 2006.\n[7] D. Mandic, P. Vayanos, C. Boukis, B. Jelfs, S. L. Goh, T. Gautama, and T. Rutkowski, \u201cCollaborative adaptive learning\nusing hybrid \ufb01lters,\u201d in Proc. IEEE Int. Conf. Acoust., Speech, Signal Process. (ICASSP), Honolulu, HI, Apr. 2007, pp.\n921\u2013924.\n[8] M. T. M. Silva and V. H. Nascimento, \u201cImproving the tracking capability of adaptive \ufb01lters via convex combination,\u201d IEEE\nTrans. Signal Process., vol. 56, no. 7, pp. 3137\u20133149, Jul. 2008.\n[9] R. Candido, M. T. M. Silva, and V. H. Nascimento, \u201cTransient and steady-state analysis of the af\ufb01ne combination of two\nadaptive \ufb01lters,\u201d IEEE Trans. Signal Process., vol. 58, no. 8, pp. 4064\u20134078, Aug. 2010.\n[10] L. Ljung, \u201cOn positive real transfer functions and the convergence of some recursive schemes,\u201d IEEE Trans. Autom. Control,\nvol. 22, no. 4, pp. 539\u2013551, Aug. 1977.\n[11] \u2014\u2014, \u201cAnalysis of recursive stochastic algorithms,\u201d IEEE Trans. Autom. Control, vol. 22, no. 4, pp. 551\u2013575, Aug. 1977.\n[12] A. Benveniste, M. Metivier, and P. Priouret, Adaptive Algorithms and Stochastic Approximations.\nNew York: Springer-\nVerlag, 1990.\n[13] O. Macchi, Adaptive Processing: The Least Mean Squares Approach with Applications in Transmission. New York: Wiley,\n1995.\nNovember 5, 2018\nDRAFT\n38\n[14] V. Solo and X. Kong, Adaptive Signal Processing Algorithms: Stability and Performance.\nEnglewood Cliffs, NJ: Prentice\nHall, 1995.\n[15] H. J. Kushner and G. Yin, Stochastic Approximation and Recursive Algorithms and Applications.\nBerlin and New York:\nSpringer-Verlag, 2003.\n[16] A. H. Sayed, Adaptive Filters.\nNJ: Wiley, 2008.\n[17] B. Widrow and S. D. Stearns, Adaptive Signal Processing.\nEnglewood Cliffs, NJ: Prentice Hall, 1985.\n[18] S. Haykin, Adaptive Filter Theory.\nEnglewood Cliffs, NJ: Prentice Hall, 2002.\n[19] B. Widrow, J. M. McCool, M. G. Larimore, and C. R. Johnson Jr., \u201cStationary and nonstationary learning characterisitcs\nof the LMS adaptive \ufb01lter,\u201d Proc. IEEE, vol. 64, no. 8, pp. 1151\u20131162, Aug. 1976.\n[20] L. Horowitz and K. Senne, \u201cPerformance advantage of complex LMS for controlling narrow-band adaptive arrays,\u201d IEEE\nTrans. Acoust., Speech, Signal Process., vol. 29, no. 3, pp. 722\u2013736, Jun. 1981.\n[21] S. Jones, R. C. III, and W. Reed, \u201cAnalysis of error-gradient adaptive linear estimators for a class of stationary dependent\nprocesses,\u201d IEEE Trans. Inf. Theory, vol. 28, no. 2, pp. 318\u2013329, Mar. 1982.\n[22] W. A. Gardner, \u201cLearning characterisitcs of stochastic-gradient-descent algorithms: A general study, analysis, and critique,\u201d\nSignal Process., vol. 6, no. 2, pp. 113\u2013133, Apr. 1984.\n[23] A. Feuer and E. Weinstein, \u201cConvergence analysis of LMS \ufb01lters with uncorrelated Gaussian data,\u201d IEEE Trans. Acoust.,\nSpeech, Signal Process., vol. 33, no. 1, pp. 222\u2013230, Feb. 1985.\n[24] J. B. Foley and F. M. Boland, \u201cA note on the convergence analysis of LMS adaptive \ufb01lters with Gaussian data,\u201d IEEE\nTrans. Acoust., Speech, Signal Process., vol. 36, no. 7, pp. 1087\u20131089, Jul. 1988.\n[25] C. G. Lopes and A. H. Sayed, \u201cIncremental adaptive strategies over distributed networks,\u201d IEEE Trans. Signal Process.,\nvol. 48, no. 8, pp. 223\u2013229, Aug. 2007.\n[26] B. T. Polyak and Y. Z. Tsypkin, \u201cPseudogradient adaptation and training algorithms,\u201d Automat. Remote Contr., vol. 34,\npp. 377\u2013397, 1973.\n[27] D. P. Bertsekas, \u201cA new class of incremental gradient methods for least squares problems,\u201d SIAM J. Optim., vol. 7, no. 4,\npp. 913\u2013926, 1997.\n[28] D. P. Bertsekas and J. N. Tsitsiklis, Parallel and Distributed Computation: Numerical Methods. Belmont: Athena Scienti\ufb01c,\n1997.\n[29] A. Nedic and D. Bertsekas, \u201cConvergence rate of incremental subgradient algorithms,\u201d in Stochastic Optimization:\nAlgorithms and Applications, S. Uryasev and P. M. Pardalos, Eds.\nKluwer Academic Publishers, 2000, pp. 263\u2013304.\n[30] A. Nedic and D. P. Bertsekas, \u201cIncremental subgradient methods for nondifferentiable optimization,\u201d SIAM J. Optim.,\nvol. 12, no. 1, pp. 109\u2013138, 2001.\n[31] M. G. Rabbat and R. D. Nowak, \u201cQuantized incremental algorithms for distributed optimization,\u201d IEEE J. Sel. Areas\nCommun., vol. 23, no. 4, pp. 798\u2013808, Apr. 2005.\n[32] L. Li and J. A. Chambers, \u201cDistributed adaptive estimation based on the APA algorithm over diffusion netowrks with\nchanging topology,\u201d in Proc. IEEE Workshop Stat. Signal Process. (SSP), Cardiff, UK, Aug. / Sep. 2009, pp. 757\u2013760.\n[33] F. S. Cattivelli and A. H. Sayed, \u201cDiffusion strategies for distributed Kalman \ufb01ltering and smoothing,\u201d IEEE Trans. Autom.\nControl, vol. 55, no. 9, pp. 2069\u20132084, Sep. 2010.\n[34] N. Takahashi, I. Yamada, and A. H. Sayed, \u201cDiffusion least-mean squares with adaptive combiners: Formulation and\nperformance analysis,\u201d IEEE Trans. Signal Process., vol. 58, no. 9, pp. 4795\u20134810, Sep. 2010.\n[35] N. Takahashi and I. Yamada, \u201cLink probability control for probabilistic diffusion least-mean squares over resource-\nNovember 5, 2018\nDRAFT\n39\nconstrained networks,\u201d in Proc. IEEE Int. Conf. Acoust., Speech, Signal Process. (ICASSP), Dallas, TX, Mar. 2010, pp.\n3518\u20133521.\n[36] S. Chouvardas, K. Slavakis, and S. Theodoridis, \u201cAdaptive robust distributed learning in diffusion sensor networks,\u201d IEEE\nTrans. Signal Process., vol. 59, no. 10, pp. 4692\u20134707, Oct. 2011.\n[37] S.-Y. Tu and A. H. Sayed, \u201cOptimal combination rules for adaptation and learning over netowrks,\u201d in Proc. IEEE Int.\nWorkshop Comput. Advances Multi-Sensor Adapt. Process. (CAMSAP), San Juan, Puerto Rico, Dec. 2011, pp. 317\u2013320.\n[38] \u2014\u2014, \u201cDiffusion netowrks outperform consensus netowrks,\u201d in Proc. IEEE Workshop Stat. Signal Process. (SSP), Ann\nArbor, MI, Aug. 2012, pp. 1\u20134.\n[39] \u2014\u2014, \u201cDiffusion strategies outperform consensus strategies for distributed estimation over adaptive netowrks,\u201d available\nonline at http://http://arxiv.org/abs/1205.3993 as manuscript arXiv:1205.3993v1 [cs.IT], May 2012.\n[40] X. Zhao, S.-Y. Tu, and A. H. Sayed, \u201cDiffusion adaptation over netowrks under imperfect information exchange and\nnon-stationary data,\u201d to appear in IEEE Trans. Signal Process., vol. 60, no. 7, Jul. 2012, also available online at\nhttp://arxiv.org/abs/1112.6212 as manuscript arXiv:1112.6212v3 [math.OC], Dec 2011.\n[41] I. D. Schizas, G. Mateos, and G. B. Giannakis, \u201cDistributed LMS for consensus-based in-network adaptive processing,\u201d\nIEEE Trans. Signal Process., vol. 57, no. 6, pp. 2365\u20132382, Jun. 2009.\n[42] A. G. Dimakis, S. Kar, J. M. F. Moura, M. G. Rabbat, and A. Scaglione, \u201cGossip algorithms for distributed signal\nprocessing,\u201d Proc. IEEE, vol. 98, no. 11, pp. 1847\u20131864, Nov. 2010.\n[43] S. Kar and J. M. F. Moura, \u201cConvergence rate analysis of distributed gossip (linear parameter) estimation: Fundamental\nlimits and tradeoffs,\u201d IEEE J. Sel. Top. Signal Process., vol. 5, no. 4, pp. 674\u2013690, Aug. 2011.\n[44] F. S. Cattivelli and A. H. Sayed, \u201cAnalysis of spatial and incremental LMS processing for distributed estimation,\u201d IEEE\nTrans. Signal Process., vol. 59, no. 4, pp. 1465\u20131480, Apr. 2011.\n[45] R. A. Horn and C. R. Johnson, Matrix Analysis.\nCambridge, UK: Cambridge Univ. Press, 1985.\n[46] A. J. Laub, Matrix Analysis for Scientists and Engineers.\nPA: SIAM, 2005.\n[47] A. Berman and R. J. Plemmons, Nonnegative Matrices in the Mathematical Sciences.\nPA: SIAM, 1994.\n[48] A. Papoulis and S. U. Pillai, Probability, Random Variables and Stochastic Processes, 4th ed.\nNew York: McGraw-Hill,\n2002.\n[49] S. Meyn and R. L. Tweedie, Markov Chains and Stochastic Stability, 2nd ed.\nCambridge, UK: Cambridge Univ. Press,\n2009.\n[50] W. K. Hastings, \u201cMonte Carlo sampling methods using Markov chains and their applications,\u201d Biometrika, vol. 57, no. 1,\npp. 97\u2013109, Apr. 1970.\n[51] S. Boyd, P. Diaconis, and L. Xiao, \u201cFastest mixing Markov chain on a graph,\u201d SIAM Review, vol. 46, no. 4, pp. 667\u2013689,\nDec. 2004.\n[52] N. Metropolis, A. W. Rosenbluth, M. N. Rosenbluth, A. H. Teller, and E. Teller, \u201cEquations of state calculations by fast\ncomputing machines,\u201d J. Chem. Phys., vol. 21, no. 6, pp. 1087\u20131092, 1953.\nNovember 5, 2018\nDRAFT\n",
        "sentence": " APPENDIX B PROOF OF THEOREM 3 We follow an argument similar to [18], [53].",
        "context": "(123). For this purpose, we \ufb01rst substitute (33) into (123) and use Assumption 2 to note that\nEMSEcta \u2248\n\u00b5Tr(Ru)\u03c32\nv,1\n2\n\u03b82 + \u03b3\n(1 + \u03b8)2 + O(\u00b52)\n(124)\nv,2\n2\n(148)\nACKNOWLEDGMENT\nThe authors would like to acknowledge useful feedback from Ph.D. student Z. Tow\ufb01c on Sec. VI-C.\nREFERENCES\n(106)\nRemark: In the two-node case, we determined the combination weights (38) by seeking coef\ufb01cients\nthat essentially minimize the EMSE expressions (37) and (44). The argument in Appendix B expressed\nNovember 5, 2018\nDRAFT\n30"
    }
]