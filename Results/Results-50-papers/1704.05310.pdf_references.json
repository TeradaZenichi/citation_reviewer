[
    {
        "title": "Learning to see by moving",
        "author": [
            "P. Agrawal",
            "J. Carreira",
            "J. Malik"
        ],
        "venue": "In ICCV,",
        "citeRegEx": "Agrawal et al\\.,? \\Q2015\\E",
        "shortCiteRegEx": "Agrawal et al\\.",
        "year": 2015,
        "abstract": "The dominant paradigm for feature learning in computer vision relies on\ntraining neural networks for the task of object recognition using millions of\nhand labelled images. Is it possible to learn useful features for a diverse set\nof visual tasks using any other form of supervision? In biology, living\norganisms developed the ability of visual perception for the purpose of moving\nand acting in the world. Drawing inspiration from this observation, in this\nwork we investigate if the awareness of egomotion can be used as a supervisory\nsignal for feature learning. As opposed to the knowledge of class labels,\ninformation about egomotion is freely available to mobile agents. We show that\ngiven the same number of training images, features learnt using egomotion as\nsupervision compare favourably to features learnt using class-label as\nsupervision on visual tasks of scene recognition, object recognition, visual\nodometry and keypoint matching.",
        "full_text": "Learning to See by Moving\nPulkit Agrawal\nUC Berkeley\npulkitag@eecs.berkeley.edu\nJo\u02dcao Carreira\nUC Berkeley\ncarreira@eecs.berkeley.edu\nJitendra Malik\nUC Berkeley\nmalik@eecs.berkeley.edu\nAbstract\nThe current dominant paradigm for feature learning in\ncomputer vision relies on training neural networks for the\ntask of object recognition using millions of hand labelled\nimages. Is it also possible to learn useful features for a di-\nverse set of visual tasks using any other form of supervision\n? In biology, living organisms developed the ability of vi-\nsual perception for the purpose of moving and acting in the\nworld. Drawing inspiration from this observation, in this\nwork we investigate if the awareness of egomotion can be\nused as a supervisory signal for feature learning. As op-\nposed to the knowledge of class labels, information about\negomotion is freely available to mobile agents. We show\nthat using the same number of training images, features\nlearnt using egomotion as supervision compare favourably\nto features learnt using class-label as supervision on the\ntasks of scene recognition, object recognition, visual odom-\netry and keypoint matching.\n\u201dWe move in order to see and we see in order to move\u201d\nJ.J Gibson\n1. Introduction\nRecent advances in computer vision have shown that vi-\nsual features learnt by neural networks trained for the task\nof object recognition using more than a million labelled im-\nages are useful for many computer vision tasks like seman-\ntic segmentation, object detection and action classi\ufb01cation\n[18, 10, 1, 33]. However, object recognition is one among\nmany tasks for which vision is used. For example, humans\nuse visual perception for recognizing objects, understand-\ning spatial layouts of scenes and performing actions such\nas moving around in the world. Is there something special\nabout the task of object recognition or is it the case that use-\nful visual representations can be learnt through other modes\nof supervision? Clearly, biological agents perform complex\nvisual tasks and it is unlikely that they require external su-\npervision in form of millions of labelled examples. Unla-\nbelled visual data is freely available and in theory this data\ncan be used to learn useful visual representations. However,\nuntil now unsupervised learning approaches [4, 22, 29, 32]\nhave not yet delivered on their promise and are nowhere to\nbe seen in current applications on complex real world im-\nagery.\nBiological agents use perceptual systems for obtaining\nsensory information about their environment that enables\nthem to act and accomplish their goals [13, 9]. Both biolog-\nical and robotic agents employ their motor system for exe-\ncuting actions in their environment. Is it also possible that\nthese agents can use their own motor system as a source of\nsupervision for learning useful perceptual representations?\nMotor theories of perception have a long history [13, 9],\nbut there has been little work in formulating computational\nmodels of perception that make use of motor information.\nIn this work we focus on visual perception and present a\nmodel based on egomotion (i.e. self motion) for learning\nuseful visual representations. When we say useful visual\nrepresentations [34], we mean representations that possess\nthe following two characteristics - (1) ability to perform\nmultiple visual tasks and (2) ability of performing new vi-\nsual tasks by learning from only a few labeled examples\nprovided by an extrinsic teacher.\nMobile agents are naturally aware of their egomotion\n(i.e. self-motion) through their own motor system. In other\nwords, knowledge of egomotion is \u201cfreely\u201d available. For\nexample, the vestibular system provides the sense of orien-\ntation in many mammals. In humans and other animals, the\nbrain has access to information about eye movements and\nthe actions performed by the animal [9]. A mobile robotic\nagent can estimate its egomotion either from the motor com-\nmands it issues to move or from odometry sensors such as\ngyroscopes and accelerometers mounted on the agent itself.\nWe propose that useful visual representations can be\nlearnt by performing the simple task of correlating visual\nstimuli with egomotion. A mobile agent can be treated like\na camera moving in the world and thus the knowledge of\negomotion is the same as the knowledge of camera motion.\nUsing this insight, we pose the problem of correlating vi-\nsual stimuli with egomotion as the problem of predicting\nthe camera transformation from the consequent pairs of im-\n1\narXiv:1505.01596v2  [cs.CV]  14 Sep 2015\nages that the agent receives while it moves. Intuitively, the\ntask of predicting camera transformation between two im-\nages should force the agent to learn features that are adept\nat identifying visual elements that are present in both the\nimages (i.e. visual correspondence). In the past, features\nsuch as SIFT, that were hand engineered for \ufb01nding corre-\nspondences were also found to be very useful for tasks such\nas object recognition [23, 19]. This suggests that egomotion\nbased learning can also result in features that are useful for\nsuch tasks.\nIn order to test our hypothesis of feature learning using\negomotion, we trained multilayer neural networks to pre-\ndict the camera transformation between pairs of images. As\na proof of concept, we \ufb01rst demonstrate the usefulness of\nour approach on the MNIST dataset [21]. We show that\nfeatures learnt using our method outperform previous ap-\nproaches of unsupervised feature learning when class-label\nsupervision is available only for a limited number of exam-\nples (section 3.4) Next, we evaluated the ef\ufb01cacy of our ap-\nproach on real world imagery. For this purpose, we used im-\nage and odometry data recorded from a car moving through\nurban scenes, made available as part of the KITTI [12] and\nthe San Francisco (SF) city [7] datasets. This data mim-\nics the scenario of a robotic agent moving around in the\nworld. The quality of features learnt from this data were\nevaluated on four tasks (1) Scene recognition on SUN [38]\n(section 5.1), (2) Visual odometery (section 5.4), (3) Key-\npoint matching (section 5.3) and (4) Object recognition on\nImagenet [31] (section 5.2). Our results show that for the\nsame amount of training data, features learnt using egomo-\ntion as supervision compare favorably to features learnt us-\ning class-label as supervision. We also show that egomotion\nbased pretraining outperforms a previous approach based on\nslow feature analysis for unsupervised learning from videos\n[37, 14, 25]. To the best of our knowledge, this work pro-\nvides the \ufb01rst effective demonstration of learning visual rep-\nresentations from non-visual access to egomotion informa-\ntion in real world setting.\nThe rest of this paper is organized as following: In sec-\ntion 2 we discuss the related work, in section 3, 4, 5 we\npresent the method, dataset details and we conclude with\nthe discussion in section 6.\n2. Related Work\nPast work in unsupervised learning has been domi-\nnated by approaches that pose feature learning as the prob-\nlem of discovering compact and rich representations of\nimages that are also suf\ufb01cient to reconstruct the images\n[6, 3, 22, 32, 27, 30]. Another line of work has focused on\nlearning features that are invariant to transformations either\nfrom video [37, 14, 25] or from images [11, 29]. [24] per-\nform feature learning by modeling spatial transformations\nusing boltzmann machines, but donot evaluate the quality\nof learnt features.\nDespite a lot of work in unsupervised learning (see [4]\nfor a review), a method that works on complex real world\nimagery is yet to be developed. An alternative to unsuper-\nvised learning is to learn features using intrinsic reward sig-\nnals that are freely available to a system (i.e self-supervised\nlearning). For instance, [15] used intrinsic reward signals\navailable to a robot for learning features that predict path\ntraversability, while [28] trained neural networks for driv-\ning vehicles directly from visual input.\nIn this work we propose to use non-visual access to ego-\nmotion information as a form of self-supervision for visual\nfeature learning. Unlike any other previous work, we show\nthat our method works on real world imagery. Closest to our\nmethod is the the work of transforming auto-encoders [16]\nthat used egomotion to reconstruct the transformed image\nfrom an input source image. This work was purely con-\nceptual in nature and the quality of learned features was not\nevaluated. In contrast, our method uses egomotion as super-\nvision by predicting the transformation between two images\nusing a siamese-like network model [8].\nOur method can also be seen as an instance of feature\nlearning from videos. [37, 14, 25] perform feature learn-\ning from videos by imposing the constraint that tempo-\nrally close frames should have similar feature representa-\ntions (i.e. slow feature analysis) without accounting for ei-\nther the camera motion or the motion of objects in the scene.\nIn many settings the camera motion dominates the motion\ncontent of the video. Our key observation is that knowl-\nedge of camera motion (i.e. egomotion) is freely available\nto mobile agents and can be used as a powerful source of\nself-supervision.\n3. A Simple Model of Motion-based Learning\nWe model the visual system of the agent with a Con-\nvolutional Neural Network (CNN, [20]). The agent opti-\nmizes its visual representations (i.e. updating the weights\nof the CNN) by minimizing the error between the egomo-\ntion information (i.e. camera transformation) obtained from\nits motor system and egomotion predicted using its visual\ninputs only.\nPerforming this task is equivalent to train-\ning a CNN with two streams (i.e. Siamese Style CNN or\nSCNN[8]) that takes two images as inputs and predicts the\negomotion that the agent underwent as it moved between\nthe two spatial locations from which the two images were\nobtained. In order to learn useful visual representations, the\nagent continuously performs this task as it moves around in\nits environment.\nIn this work we use the pretraining-\ufb01netuning paradigm\nfor evaluating the utility of learnt features. Pretraining is the\nprocess of optimizing the weights of a randomly initialized\nCNN for an auxiliary task that is not the same as the target\ntask. Finetuning is the process of modifying the weights of\n2\nFigure 1: Exploring the utility of egomotion as supervision for learning useful visual features. A mobile agent equipped\nwith visual sensors receives a sequence of images as inputs while it moves in its environment. The movement of the agent\nis equivalent to the movement of a camera. In this work, egomotion based learning is posed as the problem of predicting\ncamera transformation from image pairs. The top and bottom rows of the \ufb01gure show some sample image pairs from the SF\nand KITTI datasets that were used for feature learning.\na pretrained CNN for the given target task. Our experiments\ncompare the utility of features learnt using egomotion based\npretraining against class-label based and slow-feature based\npretraining on multiple target tasks.\n3.1. Two Stream Architecture\nEach stream of the CNN independently computes fea-\ntures for one image.\nBoth streams share the same ar-\nchitecture and the same set of weights and consequently\nperform the same set of operations for computing fea-\ntures. The individual streams have been called as Base-\nCNN (BCNN). Features from two BCNNs are concatenated\nand passed downstream into another CNN called as the Top-\nCNN (TCNN) (see \ufb01gure 2). TCNN is responsible for using\nthe BCNN features to predict the camera transformation be-\ntween the input pair of images. After pretraining, the TCNN\nis removed and a single BCNN is used as a standard CNN\nfor feature computation for the target task.\n3.2. Shorthand for CNN architectures\nThe abbreviations Ck, Fk, P, D, Op represent a convo-\nlutional(C) layer with k \ufb01lters, a fully-connected(F) layer\nwith k \ufb01lters, pooling(P), dropout(D) and the output(Op)\nlayers respectively. We used ReLU non-linearity after every\nconvolutional/fully-connected layer, except for the output\nlayer. The dropout layer was always used with dropout of\n0.5. The output layer was a fully connected layer with num-\nber of units equal to the number of desired outputs. As an\nexample of our notation, C96-P-F500-D refers to a network\nwith 96 \ufb01lters in the convolution layer followed by ReLU\nnon-linearity, a pooling layer, a fully-connected layer with\n500 unit, ReLU non-linearity and a dropout layer. We used\nL1\nTransformation\nL2\nLk\nF1\nF2\nBase-CNN Stream-1\nBase-CNN  Stream-2\nTop-CNN\nFigure 2: Description of the method for feature learning.\nVisual features are learnt by training a Siamese style Con-\nvolutional Neural Network (SCNN, [8]) that takes as inputs\ntwo images and predicts the transformation between the im-\nages (i.e. egomotion). Each stream of the SCNN (called\nas Base-CNN or BCNN) computes features for one image.\nThe outputs of two BCNNs are concatenated and passed\nas inputs to a second multilayer CNN called as the Top-\nCNN (TCNN) (shown as layers F1, F2). The two BCNNs\nhave the same architecture and share weights. After feature\nlearning, TCNN is discarded and a single BCNN stream is\nused as a standard CNN for extracting features for perform-\ning target tasks like scene recognition.\n[17] for training all our models.\n3.3. Slow Feature Analysis (SFA) Baseline\nSlow Feature Analysis (SFA) is a method for feature\nlearning based on the principle that useful features change\n3\nslowly in time.\nWe used the following contrastive loss\nformulation of SFA [8, 25],\nL(xt1, xt2, W) =\n(\nD(xt1, xt2)\nif |t1 \u2212t2| \u2264T\n1 \u2212max\n\u00000, m \u2212D(xt1, xt2)\n\u0001\nif |t1 \u2212t2| > T\n(1)\nwhere, L is the loss, xt1, xt2 refer to feature representa-\ntions of frames observed at times t1, t2 respectively, W are\nthe parameters that specify the feature extraction process, D\nis a measure of distance with parameter, m is a prede\ufb01ned\nmargin and T is a prede\ufb01ned time threshold for determining\nwhether the two frames are temporally close or not. In this\nwork, xt are features computed using a CNN with weights\nW and D was chosen to be the L2 distance. SFA pretrain-\ning was performed using two stream architectures that took\npairs of images as inputs and produced outputs xt1, xt2 as\noutputs from the two streams respectively.\n3.4. Proof of Concept using MNIST\nOn MNIST, egomotion was emulated by generating syn-\nthetic data consisting of random transformation (transla-\ntions and rotations) of digit images. From the training set of\n60K images, digits were randomly sampled and then trans-\nformed using two different sets of random transformations\nto generate image pairs. CNNs were trained for predicting\nthe transformations between these image pairs.\n3.4.1\nData\nFor egomotion based pretraining, relative translation be-\ntween the digits was constrained to be an integer value in\nthe range [-3, 3] and relative rotation \u03b8 was constrained to\nlie within the range [-30\u25e6, 30\u25e6]. The prediction of transfor-\nmation was posed as a classi\ufb01cation task with three sepa-\nrate soft-max losses (one each for translation along X, Y\naxes and the rotation about Z-axis).\nSCNN was trained\nto minimize the sum of these three losses.\nTranslations\nalong X, Y were separately binned into seven uniformly\nspaced bins each. The rotations were binned into bins of\nsize 3\u25e6each resulting into a total of 20 bins (or classes). For\nSFA based pretraining, image pairs with relative translation\nin the range [-1, 1] and relative rotation within [-3\u25e6, 3\u25e6]\nwere considered to be temporally close to each other (see\nequation 1). A total of 5 million image pairs were used for\nboth pretraining procedures.\n3.4.2\nNetwork Architectures\nWe experimented with multiple BCNN architectures and\nchose the optimal architecture for each pretraining method\nseparately. For egmotion based pretraining, the two BCNN\nstreams were concatenated using the TCNN: F1000-D-Op.\nTable 1: Comparison of various pretraining methods on\nMNIST reveals that egomotion based pretraining outper-\nforms many previous approaches for unsupervised learning.\nThe performance is reported as the error rate.\nMethod\n# examples for \ufb01netuning\n100 300 1000\n10000\nAutoencoder [17]\n24.1 12.2 7.7\n4.8\nRanzato et al. [29]\n-\n7.18 3.21\n0.85\nLee et al. [22]\n-\n-\n2.62\n-\nTrain from Scratch 20.1 8.3\n4.5\n1.6\nSFA (m=10)\n11.2 6.4\n3.5\n2.1\nSFA (m=100)\n11.9 6.4\n4.8\n4.7\nEgomotion (ours)\n8.7\n3.6\n2.0\n0.9\nPretraining was performed for 40K iterations (i.e. 5M ex-\namples) using an initial learning rate of 0.01 which was re-\nduced by a factor of 2 after every 10K iterations.\nThe following architecture was used for \ufb01netuning:\nBCNN-F500-D-Op.\nIn order to evaluate the quality of\nBCNN features, the learning rate of all layers in the BCNN\nwere set to 0 during \ufb01netuning for digit classi\ufb01cation. Fine-\ntuning was performed for 4K iterations (which is equivalent\nto training for 50 epochs for the 10K labelled training ex-\namples) with a constant learning rate of 0.01.\n3.4.3\nResults\nThe BCNN features were evaluated by computing the er-\nror rates on the task of digit classi\ufb01cation using 100, 300,\n1K and 10K class-labelled examples for training. These\nsets were constructed by randomly sampling digits from the\nstandard training set of 60K digits. For this part of the ex-\nperiment, the original digit images were used (i.e. without\nany transformations or data augmentation). The standard\ntest set of 10K digits was used for evaluation and error rates\naveraged across 3 runs are reported in table 1.\nThe BCNN architecture: C96-P-C256-P, was found to\nbe optimal for egomotion and SFA based pretraining and\nalso for training from scratch (i.e. random weight initial-\nization). Results for other architectures are provided in the\nsupplementary material. For SFA based pretraining, we ex-\nperimented with multiple values of the margin m and found\nthat m = 10, 100 led to the best performance. Our method\noutperforms convolutional deep belief networks [22], a pre-\nvious approach based on learning features invariant to trans-\nformations [29] and SFA based pretraining.\n4\n4. Learning Visual Features From Egomotion\nin Natural Environments\nWe used two main sources of real world data for feature\nlearning: the KITTI and SF datasets, which were collected\nusing cameras and odometry sensors mounted on a car driv-\ning through urban scenes. Details about the data, the exper-\nimental procedure, the network architectures and the results\nare provided in sections 4.1, 4.2, 4.3 and 5 respectively.\n4.1. KITTI Dataset\nThe KITTI dataset provided odometry and image data\nrecorded during 11 short trips of variable length made by\na car moving through urban landscapes. The total number\nof frames in the entire dataset was 23,201. Out of 11, 9\nsequences were used for training and 2 for validation. The\ntotal number of images in the training set was 20,501.\nThe odometry data was used to compute the camera\ntransformation between pairs of images recorded from the\ncar.\nThe direction in which the camera pointed was as-\nsumed to be the Z axis and the image plane was taken to\nbe the XY plane. X-axis and Y-axis refer to horizontal and\nvertical directions in the image plane. As signi\ufb01cant cam-\nera transformations in the KITTI data were either due to\ntranslations along the Z/X axis or rotation about the Y axis,\nonly these three dimensions were used to express the cam-\nera transformation. The rotation was represented as the eu-\nler angle about the Y-axis. The task of predicting the trans-\nformation between pair of images was posed as a classi\ufb01ca-\ntion problem. The three dimensions of camera transforma-\ntion were individually binned into 20 uniformly spaced bins\neach. The training image pairs were selected from frames\nthat were at most \u00b17 frames apart to ensure that images in\nany given pair would have a reasonable overlap. For SFA\nbased pretraining, pairs of frames that were separated by at-\nmost \u00b17 frames were considered to be temporally close to\neach other.\nThe SCNN was trained to predict camera transformation\nfrom pairs of 227 \u00d7 227 pixel sized image regions extracted\nfrom images of overall size 370 \u00d7 1226 pixels. For each\nimage pair, the coordinates for cropping image regions were\nrandomly chosen. Figure 1 illustrates typical image crops.\n4.2. SF Dataset\nSF dataset provides camera transformation between \u2248\n136K pairs of images (constructed from a set of 17,357\nunique images). This dataset was constructed using Google\nStreetView [7]. \u2248130K image pairs were used for training\nand \u22486K pairs for validation.\nJust like KITTI, the task of predicting camera trans-\nformation was posed as a classi\ufb01cation problem. Unlike\nKITTI, signi\ufb01cant camera transformation was found along\nall six dimensions of transformation (i.e. the 3 euler angles\n(a) KITTI-Net\n(b) SF-Net\nFigure 3: Visualization of layer 1 \ufb01lters learnt by egomotion\nbased pretraining on (a) KITTI and (b) SF datasets. A large\nmajority of layer-1 \ufb01lters are color detectors and some of\nthem are edge detectors. This is expected as color is a useful\ncue for determining correspondences between image pairs.\nand the 3 translations). Since, it is unreasonable to expect\nthat visual features can be used to infer big camera trans-\nformations, rotations between [-30\u25e6, 30\u25e6] were binned into\n10 uniformly spaced bins and two extra bins were used for\nrotations larger and smaller than 30\u25e6and -30\u25e6respectively.\nThe three translations were individually binned into 10 uni-\nformly spaced bins each. Images were resized to a size of\n360 \u00d7 480 and image regions of size 227 \u00d7 227 were used\nfor training the SCNN.\n4.3. Network Architecture\nBCNN closely followed the architecture of \ufb01rst \ufb01ve\nAlexNet layers [18]: C96-P-C256-P-C384-C384-C256-P.\nTCNN architecture was: C256-C128-F500-D-Op. The con-\nvolutional \ufb01lters in the TCNN were of spatial size 3\u00d73. The\nnetworks were trained for 60K iterations with a batch size\nof 128. The initial learning rate was set to 0.001 and was\nreduced by a factor of two after every 20K iterations.\nWe term the networks pretrained using egomotion on\nKITTI and SF datasets as KITTI-Net and SF-Net respec-\ntively.\nThe net pretrained on KITTI with SFA is called\nKITTI-SFA-Net.\nFigure 3 shows the layer-1 \ufb01lters of\nKITTI-Net and SF-Net. A large majority of layer-1 \ufb01lters\nare color detectors, while some of them are edge detectors.\nAs color is a useful cue for determining correspondences\nbetween closeby frames of a video sequence, learning of\ncolor detectors as layer-1 \ufb01lters is not surprising. The frac-\ntion of \ufb01lters that detect edges is higher for the SF-Net. This\nis not surprising either, because higher fraction of images in\nthe SF dataset contain structured objects like buildings and\ncars.\n5. Evaluating Motion-based Learning\nFor evaluating the merits of the proposed approach, fea-\ntures learned using egomotion based supervision were com-\npared against features learned using class-label and SFA\nbased supervision on the challenging tasks of scene recogni-\n5\ntion, intra-class keypoint matching and visual odometry and\nobject recognition. The ultimate goal of feature learning is\nto \ufb01nd features that can generalize from only a few super-\nvised examples on a new task. Therefore it makes sense to\nevaluate the quality of features when only a few labelled ex-\namples for the target task are provided. Consequently, the\nscene and object recognition experiments were performed\nin the setting when only 1-20 labelled examples per class\nwere available for \ufb01netuning.\nThe KITTI-Net and SF-Net (examples of models trained\nusing egomotion based supervision) were trained using only\nonly \u224820K unique images. To make a fair comparison\nwith class-label based supervision, a model with AlexNet\narchitecture was trained using only 20K images taken from\nthe training set of ILSVRC12 challenge (i.e. 20 examples\nper class). This model has been referred to as AlexNet-20K.\nIn addition, some experiments presented in this work also\nmake comparison with AlexNet models trained with 100K\nand 1M images that have been named as AlexNet-100K and\nAlexNet-1M respectively.\n5.1. Scene Recognition\nSUN dataset consisting of 397 indoor/outdoor scene cat-\negories was used for evaluating scene recognition perfor-\nmance. This dataset provides 10 standard splits of 5 and 20\ntraining images per class and a standard test set of 50 im-\nages per class. Due to time limitation of running 10 runs of\nthe experiment, we evaluated the performance using only 3\ntrain/test splits.\nFor evaluating the utility of CNN features produced by\ndifferent layers, separate linear (SoftMax) classi\ufb01ers were\ntrained on features produced by individual CNN layers\n(i.e. BCNN layers of KITTI-Net, KITTI-SFA-Net and SF-\nNet). Table 2 reports recognition accuracy (averaged over\n3 train/test splits) for various networks considered in this\nstudy. KITTI-Net outperforms SF-Net and is comparable\nto AlexNet-20K. This indicates that given a \ufb01xed budget\nof pretraining images, egomotion based supervision learns\nfeatures that are almost as good as the features using class-\nbased supervision on the task of scene recognition. The per-\nformance of features computed by layers 1-3 (abbreviated\nas L1, L2, L3 in table 2) of the KITTI-SFA-Net and KITTI-\nNet is comparable, whereas layer 4, 5 features of KITTI-Net\nsigni\ufb01cantly outperform layer 4, 5 features of KITTI-SFA-\nNet. This indicates that egomotion based pretraining results\ninto learning of higher-level features, while SFA based pre-\ntraining results into learning of lower-level features only.\nThe KITTI-Net outperforms GIST[26], which was\nspeci\ufb01cally developed for scene classi\ufb01cation, but is out-\nperformed by Dense SIFT with spatial pyramid matching\n(SPM) kernel [19]. The KITTI-Net was trained using lim-\nited visual data (\u224820Kframes) containing visual imagery\nof limited diversity. The KITTI data mainly contains images\nof roads, buildings, cars, few pedestrians, trees and some\nvegetation. It is in fact surprising that a network trained on\ndata with such little diversity is competitive on classifying\nindoor and outdoor scenes with the AlexNet-20K that was\ntrained on a much more diverse set of images. We believe\nthat with more diverse training data for egomotion based\nlearning, the performance of learnt features will be better\nthan currently reported numbers.\nThe KITTI-Net outperformed the SF-Net except for the\nperformance of layer 1 (L1). As it was possible to extract a\nlarger number of image region pairs from the KITTI dataset\nas compared to the SF dataset (see section 4.1, 4.2), the\nresult that KITTI-Net outperforms SF-Net is not surprising.\nBecause KITTI-Net was found to be superior to the SF-Net\nin this experiment, the KITTI-Net was used for all other\nexperiments described in this paper.\n5.2. Object Recognition\nIf egomotion based pretraining learns useful features for\nobject recognition, then a net initialized with KITTI-Net\nweights should outperform a net initialized with random\nweights on the task of object recognition. For testing this,\nwe trained CNNs using 1, 5, 10 and 20 images per class\nfrom the ILSVRC-2012 challenge. As this dataset contains\n1000 classes, the total number of training examples avail-\nable for training for these networks were 1K, 5K, 10K and\n20K respectively. All layers of KITTI-Net, KITTI-SFA-Net\nand AlexNet-Scratch (i.e. CNN with random weight initial-\nization) were \ufb01netuned for image classi\ufb01cation.\nThe results of the experiment presented in table 3\nshow that egomotion based supervision (KITTI-Net) clearly\noutperforms SFA based supervision(KITTI-SFA-Net) and\nAlexNet-Scratch. As expected, the improvement offered by\nmotion-based pretraining is larger when the number of ex-\namples provided for the target task are fewer. These result\nshow that egomotion based pretraining learns features use-\nful for object recognition.\n5.3. Intra-Class Keypoint Matching\nIdentifying the same keypoint of an object across differ-\nent instances of the same object class is an important visual\ntask. Visual features learned using egomotion, SFA and\nclass-label based supervision were evaluated for this task\nusing keypoint annotations on the PASCAL dataset [5].\nKeypoint matching was computed in the following way:\nFirst, ground-truth object bounding boxes (GT-BBOX)\nfrom PASCAL-VOC2012 dataset were extracted and re-\nsized (while preserving the aspect ratio) to ensure that the\nsmaller side of the boxes was of length 227 pixels. Next,\nfeature maps from layers 2-5 of various CNNs were com-\nputed for every GT-BBOX. The keypoint matching score\nwas computed between all pairs of GT-BBOX belonging to\nthe same object class. For given pair of GT-BBOX, the fea-\n6\nTable 2: Comparing the accuracy of neural networks pre-trained using motion-based and class-label based supervision for\nthe task of scene recognition on the SUN dataset. The performance of layers 1-6 (labelled as L1-L6) of these networks was\nevaluated after \ufb01netuning the network using 5/20 images per class from the SUN dataset. The performance of the KITTI-Net\n(i.e. motion-based pretraining) fares favorably with a network pretrained on Imagenet (i.e. class-based pretraining) with the\nsame number of pretraining images (i.e. 20K).\nMethod\nPretrain Supervision #Pretrain #Finetune L1 L2\nL3\nL4\nL5\nL6 #Finetune L1\nL2\nL3\nL4\nL5\nL6\nAlexNet-1M\nClass-Label\n1M\n5\n5.3 10.5 12.1 12.5 18.0 23.6\n20\n11.8 22.2 25.0 26.8 33.3 37.6\nAlexNet-20K\n20K\n5\n4.9 6.3\n6.6\n6.3\n6.6\n6.7\n20\n8.7 12.6 12.4 11.9 12.5 12.4\nKITTI-SFA-Net\nSlowness\n20.5K\n5\n4.5 5.7\n6.2\n3.4\n0.5\n-\n20\n8.2 11.2 12.0 7.3\n1.1\n-\nSF-Net\nEgomotion\n18K\n5\n4.4 5.2\n4.9\n5.1\n4.7\n-\n20\n8.6 11.6 10.9 10.4 9.1\n-\nKITTI-Net\n20.5K\n5\n4.3 6.0\n5.9\n5.8\n6.4\n-\n20\n7.9 12.2 12.1 11.7 12.4\n-\nGIST [38]\nHuman\n-\n5\n6.2\n20\n11.6\nSPM [38]\nHuman\n-\n5\n8.4\n20\n16.0\nTable 3: Top-5 accuracy on the task of object recognition\non the ILSVRC-12 validation set. AlexNet-Scratch refers\nto a net with AlexNet architecture initialized with randomly\nweights. The weights of KITTI-Net and KITTI-SFA-Net\nwere learned using egomotion based and SFA based su-\npervision on the KITTI dataset respectively. All the net-\nworks were \ufb01netuned using 1, 5, 10, 20 examples per class.\nThe KITTI-Net clearly outperforms AlexNet-Scratch and\nKITTI-SFA-Net.\nMethod\n1\n5\n10\n20\nAlexNet-Scratch\n1.1 3.1 5.9 14.1\nKITTI-SFA-Net (Slowness) 1.5 3.9 6.1 14.9\nKITTI-Net (Egomotion)\n2.3 5.1 8.6 15.8\ntures associated with keypoints in the \ufb01rst image were used\nto predict the location of the same keypoints in the second\nimage. The normalized pixel distance between the actual\nand predicted keypoint locations was taken as the error in\nkeypoint matching. More details about this procedure have\nbeen provided in the supp. materials.\nIt is natural to expect that accuracy of keypoint matching\nwould depend on the camera transformation between the\ntwo viewpoints of the object(i.e. viewpoint distance). In\norder to make a holistic evaluation of the utility of features\nlearnt by different pretraining methods on this task, match-\ning error was computed as a function of viewpoint distance\n[36]. Figure 4 reports the matching error averaged across\nall keypoints, all pairs of GT-BBOX and all classes using\nfeatures extracted from layers conv-3 and conv-4.\nKITTI-Net trained only with 20K unique frames was\nsuperior to AlexNet-20K and AlexNet-100K and inferior\nonly to AlexNet-1M. A net with AlexNet architecture ini-\ntialized with random weights (AlexNet-Rand), surprisingly\nperformed better than AlexNet-20K. One possible expla-\nTable 4: Comparing the accuracy of various pretraining\nmethods on the task of visual odometry.\nMethod\nTranslation Acc. Rotation Acc.\n\u03b4X\n\u03b4Y\n\u03b4Z\n\u03b4\u03b81\n\u03b4\u03b82\n\u03b4\u03b83\nSF-Net\n40.2 58.2\n38.4\n45.0 44.8 40.5\nKITTI-Net 43.4 57.9\n40.2\n48.4 44.0 41.0\nAlexNet-1M 41.8 58.0\n39.0\n46.0 44.5 40.5\nnation for this observation is that with only 20K exam-\nples, features learnt by AlexNet-20K only capture coarse\nglobal appearance of objects and are therefore poor at key-\npoint matching. SIFT has been hand engineered for \ufb01nd-\ning correspondences across images and performs as well\nas the best AlexNet-1M features for this task (i.e. conv-4\nfeatures). KITTI-Net also signi\ufb01cantly outperforms KITTI-\nSFA-Net. These results indicate that features learnt by ego-\nmotion based pretraining are superior to SFA and class-\nlabel based pretraining for the task of keypoint matching.\n5.4. Visual Odometry\nVisual odometry is the task of estimating the camera\ntransformation between image pairs. All layers of KITTI-\nNet and AlexNet-1M were \ufb01netuned for 25K iterations us-\ning the training set of SF dataset on the task of visual odom-\netry (see section 4.2 for task description). The performance\nof various CNNs was evaluated on the validation set of SF\ndataset and the results are reported in table 4.\nPerformance of KITTI-Net was either superior or com-\nparable to AlexNet-1M on this task. As the evaluation was\nmade on the SF dataset itself, it was not surprising that on\nsome metrics SF-Net outperformed KITTI-Net. The results\nof this experiment indicate that egomotion based feature\nlearning is superior to class-label based feature learning on\nthe task of visual odometry.\n7\nMaximum viewpoint range\n0  \n18 \n36 \n54 \n72 \n90 \n108\n126\n144\n162\nMean error\n0.17\n0.19\n0.21\n0.23\n0.25\n0.27\n0.29\nKittiNet-20k-cv3\nKittiNet-SFA-20k-cv3\nAlexNet-rand-cv3\nAlexNet-20k-cv3\nAlexNet-100k-cv3\nAlexNet-1M-cv3\nSIFT\nMaximum viewpoint range\n0  \n18 \n36 \n54 \n72 \n90 \n108\n126\n144\n162\nMean error\n0.15\n0.17\n0.19\n0.21\n0.23\n0.25\n0.27\nKittiNet-20k-cv4\nKittiNet-SFA-20k-cv4\nAlexNet-rand-cv4\nAlexNet-20k-cv4\nAlexNet-100k-cv4\nAlexNet-1M-cv4\nSIFT\nFigure 4: Intra-class keypoint matching error as a function of viewpoint distance averaged over 20 PASCAL objects using\nfeatures from layers conv3 (left) and conv4 (right) of various CNNs used in this work. Please see the text for more details.\n6. Discussion\nIn this work, we have shown that egomotion is a useful\nsource of intrinsic supervision for visual feature learning in\nmobile agents. In contrast to class labels, knowledge of ego-\nmotion is \u201dfreely\u201d available. On MNIST, egomotion-based\nfeature learning outperforms many previous unsupervised\nmethods of feature learning. Given the same budget of pre-\ntraining images, on task of scene recognition, egomotion-\nbased learning performs almost as well as class-label-based\nlearning. Further, egomotion based features outperform fea-\ntures learnt by a CNN trained using class-label supervision\non two orders of magnitude more data (AlexNet-1M) on the\ntask of visual odometry and one order of magnitude more\ndata on the task of intra-class keypoint matching. In ad-\ndition to demonstrating the utility of egomotion based su-\npervision, these results also suggest that features learnt by\nclass-label based supervision are not optimal for all visual\ntasks. This means that future work should look at what\nkinds of pretraining are useful for what tasks.\nOne potential criticism of our work is that we have\ntrained and evaluated high capacity deep models on rela-\ntively little data (e.g. only 20K unique images available on\nthe KITTI dataset). In theory, we could have learnt bet-\nter features by downsizing the networks. For example, in\nour experiments with MNIST we found that pretraining a\n2-layer network instead of 3-layer results in better perfor-\nmance (table 1). In this work, we have made a conscious\nchoice of using standard deep models because the main\ngoal of this work was not to explore novel feature extrac-\ntion architectures but to investigate the value of egmotion\nfor learning visual representations on architectures known\nto perform well on practical applications. Future research\nfocused on exploring architectures that are better suited for\negomotion based learning can only make a stronger case\nfor this line of work.\nWhile egomotion is freely avail-\nable to mobile agents, there are currently no publicly avail-\nable datasets as large as Imagenet. Consequently, we were\nunable to evaluate the utility of motion-based supervision\nacross the full spectrum of training set sizes.\nIn this work, we chose to \ufb01rst pretrain our models using\na base task (i.e. egomotion) and then \ufb01netune these mod-\nels for target tasks. An equally interesting setting is that\nof online learning where the agent has continuous access\nto intrinsic supervision (such as egomotion) and occasional\nexplicit access to extrinsic teacher signal (such as the class\nlabels). We believe that such a training procedure is likely\nto result in learning of better features. Our intuition behind\nthis is that seeing different views of the same instance of an\nobject (say) car, may not be suf\ufb01cient to learn that different\ninstances of the car class should be grouped together. The\noccasional extrinsic signal about object labels may prove\nuseful for the agent to learn such concepts. Also, current\nwork makes use of passively collected egomotion data and\nit would be interesting to investigate if it is possible to learn\nbetter visual representations if the agent can actively decide\non how to explores its environment (i.e. active learning [2]).\nAcknowledgements\nThis work was supported in part by ONR MURI-\nN00014-14-1-0671. Pulkit Agrawal was partially supported\nby Fulbright Science and Technology Fellowship.\nJo\u02dcao\nCarreira was supported by the Portuguese Science Founda-\ntion, FCT, under grant SFRH/BPD/84194/2012. We grate-\nfully acknowledge NVIDIA corporation for the donation of\nTesla GPUs for this research.\nAppendix\nA. Keypoint Matching Score\nConsider images of two instances of the same object\nclass (for example airplane images as shown in \ufb01rst row\nof \ufb01gure 5) for which keypoint matching score needs to be\ncomputed.\nThe images are pre-processed in the following way:\n8\n\u2022 Crop the groundtruth bounding box from the image.\n\u2022 Pad the images by 30 pixels along each dimension.\n\u2022 Resize each image so that the smallest side is 227 pix-\nels. The aspect ratio of the image is preserved.\nA.1. Keypoint Matching using CNN\nAssume that the lth layer of the CNN is used for feature\ncomputation. The feature map produced by the lth layer is\nof dimensionality I \u00d7 J \u00d7 M, where (I, J) are the spatial\ndimensions and M is the number of \ufb01lters in the lth layer.\nThus, the lth layer produces a M dimensional feature vector\nfor each of the I \u00d7 J grid position in the feature map.\nThe coordinates of the keypoints are provided in the im-\nage coordinate system [5]. For the keypoints in the \ufb01rst\nimage, we \ufb01rst determine their grid position in the I \u00d7 J\nfeature map. Each grid position has an associated receptive\n\ufb01eld in the image. The keypoints are assigned to the grid\npositions for which the center of receptive \ufb01eld is closest\nto the keypoints. This means that each keypoint is assigned\none location in the feature map.\nLet the M dimensional feature vector associated with the\nkth keypoint in the \ufb01rst image be F k\n1 . Let the M dimen-\nsional feature vector at grid location Cij for the second im-\nage be F2(Cij). The location of matching keypoint in the\nsecond image is determined by solving:\nC\u2217= argminCij\u2225F k\n1 \u2212F2(Cij)\u22252\n(2)\nC\u2217is transformed into the image coordinate system by com-\nputing the center of receptive \ufb01eld (in the image) associated\nwith this grid position. Let this transformed coordinates be\nCim\n\u2217\nand the coordinates of the corresponding keypoint (in\nthe second image) be Cim\ngt . The matching error for the kth\nkeypoint (Ek) is de\ufb01ned as:\nEk = \u2225Cim\n\u2217\n\u2212Cim\ngt \u22252\nL2\nD\n(3)\nwhere, L2\nD is the length of diagonal (in pixels) of the second\nimage. As different images have different sizes, dividing\nby L2\nD normalizes for the difference in sizes. The matching\nerror for a pair of images of instances belonging to the same\nclass is calculated as:\nEinstance =\nPK\nk=1 Ek\nK\n(4)\nThe average matching error across all pairs of the in-\nstance of the same class is given by Eclass:\nEclass =\nP\ninstance Einstance\n#pairs\n(5)\nwhere, #pairs is the number of pairs of object instances\nbelonging to the same class. In Figure 4 of the main pa-\nper we report the matching error averaged across all the 20\nclasses.\nA.2. Keypoint Matching using SIFT\nSIFT features are extracted using a square window of\nsize 72 pixels and a stride of 8 pixels using the open source\ncode from [35]. The stride of 8 pixels was chosen to have a\nfair comparison with the CNN features. The CNN features\nwere computed with a stride of 8 for layer conv-2 and stride\nof 16 for layers conv-3, conv-4 and conv-5 respectively. The\nmatching error using SIFT was calculated in the same way\nas for the CNNs.\nA.3. Effect of Viewpoint on Keypoint Matching\nIntuitively, matching instances of the same object that are\nrelated by a large transformation (i.e. viewpoint distance)\nshould be harder than matching instances with a small view-\npoint distance. Therefore, in order to obtain a holistic un-\nderstanding of the accuracy of features in performing key-\npoint matching it is instructive to study the accuracy of\nmatching as a function of viewpoint distance.\n[36] aligned instances of the same class (from PASCAL-\nVOC-2012) in a global coordinate system and provide a ro-\ntation matrix (R) for each instance in the class. To measure\nthe viewpoint distance, we computed the riemannian met-\nric on the manifold of rotation matrices ||log(RiRT\nj )||F ,\nwhere log is the matrix logarithm, ||.||F is the Frobenius\nnorm of the matrix and Ri, Rj are the rotation matrices for\nthe ith, jth instances respectively. We binned the distances\ninto 10 uniform bins (of 18\u25e6each). In Figure 4 of the main\npaper we show the mean error in keypoint matching in each\nof these viewpoints bin. The matching error in the kth bin is\ncalculated by considering all the instances with a viewpoint\ndistance \u2264k\u00d718\u25e6, for k \u2208[1, 10]. As expected we \ufb01nd that\nkeypoint matching is worse for larger viewpoint distances.\nA.4. Matching Error for layers 2 and 5\nReferences\n[1] P. Agrawal, R. Girshick, and J. Malik. Analyzing the perfor-\nmance of multilayer neural networks for object recognition.\nIn Computer Vision\u2013ECCV 2014, pages 329\u2013344. Springer,\n2014.\n[2] R. Bajcsy.\nActive perception.\nProceedings of the IEEE,\n76(8):966\u20131005, 1988.\n[3] H. Barlow.\nUnsupervised learning.\nNeural computation,\n1(3):295\u2013311, 1989.\n[4] Y. Bengio, A. C. Courville, and P. Vincent. Unsupervised\nfeature learning and deep learning: A review and new per-\nspectives. CoRR, abs/1206.5538, 1, 2012.\n[5] L. Bourdev, S. Maji, T. Brox, and J. Malik. Detecting people\nusing mutually consistent poselet activations. In Computer\nVision\u2013ECCV 2010, pages 168\u2013181. Springer, 2010.\n[6] H. Bourlard and Y. Kamp. Auto-association by multilayer\nperceptrons and singular value decomposition.\nBiological\ncybernetics, 59(4-5):291\u2013294, 1988.\n9\nAlexNet-20K\nAlexNet-100K\nAlexNet-1M\nKittiNet-20K\nSIFT\nFigure 5: Example matchings between pairs of objects (randomly chosen) with viewpoints within 60 degrees of each other,\nfor classes \u201daeroplane\u201d, \u201dbottle\u201d, \u201ddog\u201d, \u201dperson\u201d and \u201dtvmonitor\u201d from PASCAL VOC. The matchings have been shown\nfor features from layer conv-4 of AlexNet-20K, AlexNet-100K, AlexNet-1M, KittiNet-20K and SIFT. The left image shows\nthe ground truth keypoints that were matched with the keypoints in the right image. Right images shows the location of\nthe ground truth keypoint (shown by solid dot) and lines joining the predicted keypoint location (tip of the line) with the\nground keypoint location. Please see section A for details of keypoint matching procedure and \ufb01gure 4 in the main paper for\nnumerical results. This \ufb01gure is best seen in color and with zoom.\n10\nMaximum viewpoint range\n0  \n18 \n36 \n54 \n72 \n90 \n108\n126\n144\n162\nMean error\n0.17\n0.19\n0.21\n0.23\n0.25\n0.27\n0.29\nKittiNet-20k-p2\nKittiNet-SFA-20k-p2\nAlexNet-rand-p2\nAlexNet-20k-p2\nAlexNet-100k-p2\nAlexNet-1M-p2\nSIFT\nMaximum viewpoint range\n0  \n18 \n36 \n54 \n72 \n90 \n108\n126\n144\n162\nMean error\n0.17\n0.19\n0.21\n0.23\n0.25\n0.27\n0.29\nKittiNet-20k-cv5\nKittiNet-SFA-20k-cv5\nAlexNet-rand-cv5\nAlexNet-20k-cv5\nAlexNet-100k-cv5\nAlexNet-1M-cv5\nSIFT\nFigure 6: Intra-class keypoint matching error as a function of viewpoint distance averaged over 20 PASCAL objects using\nfeatures extracted from layers pool-2 (left) and conv-5 (right) of various networks used in this work. Please see section 5.3\nfor more details.\n[7] D. M. Chen, G. Baatz, K. Koser, S. S. Tsai, R. Vedantham,\nT. Pylvanainen, K. Roimela, X. Chen, J. Bach, M. Pollefeys,\net al. City-scale landmark identi\ufb01cation on mobile devices.\nIn Computer Vision and Pattern Recognition (CVPR), 2011\nIEEE Conference on, pages 737\u2013744, 2011.\n[8] S. Chopra, R. Hadsell, and Y. LeCun. Learning a similar-\nity metric discriminatively, with application to face veri\ufb01ca-\ntion.\nIn Computer Vision and Pattern Recognition, 2005.\nCVPR 2005. IEEE Computer Society Conference on, vol-\nume 1, pages 539\u2013546. IEEE, 2005.\n[9] J. E. Cutting. Perception with an eye for motion, volume 177.\n[10] J. Donahue, Y. Jia, O. Vinyals, J. Hoffman, N. Zhang,\nE. Tzeng, and T. Darrell. Decaf: A deep convolutional acti-\nvation feature for generic visual recognition. arXiv preprint\narXiv:1310.1531, 2013.\n[11] P. Fischer, A. Dosovitskiy, and T. Brox. Descriptor match-\ning with convolutional neural networks: a comparison to sift.\narXiv preprint arXiv:1405.5769, 2014.\n[12] A. Geiger, P. Lenz, C. Stiller, and R. Urtasun. Vision meets\nrobotics: The kitti dataset. International Journal of Robotics\nResearch (IJRR), 2013.\n[13] J. J. Gibson. The Ecological Approach to Visual Perception.\nHoughton Mif\ufb02in, 1979.\n[14] R. Goroshin, J. Bruna, J. Tompson, D. Eigen, and Y. LeCun.\nUnsupervised feature learning from temporal data.\narXiv\npreprint arXiv:1504.02518, 2015.\n[15] R. Hadsell, P. Sermanet, J. Ben, A. Erkan, J. Han, B. Flepp,\nU. Muller, and Y. LeCun.\nOnline learning for offroad\nrobots: Using spatial label propagation to learn long-range\ntraversability.\nIn Proc. of Robotics: Science and Systems\n(RSS), volume 11, page 32, 2007.\n[16] G. E. Hinton, A. Krizhevsky, and S. D. Wang. Transforming\nauto-encoders. In Arti\ufb01cial Neural Networks and Machine\nLearning\u2013ICANN, pages 44\u201351. Springer, 2011.\n[17] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Gir-\nshick, S. Guadarrama, and T. Darrell. Caffe: Convolutional\narchitecture for fast feature embedding. In Proceedings of\nthe ACM International Conference on Multimedia, pages\n675\u2013678. ACM, 2014.\n[18] A. Krizhevsky, I. Sutskever, and G. E. Hinton.\nImagenet\nclassi\ufb01cation with deep convolutional neural networks. In\nAdvances in neural information processing systems, pages\n1097\u20131105, 2012.\n[19] S. Lazebnik, C. Schmid, and J. Ponce.\nBeyond bags of\nfeatures: Spatial pyramid matching for recognizing natural\nscene categories. In Computer Vision and Pattern Recogni-\ntion, 2006 IEEE Computer Society Conference on, volume 2,\npages 2169\u20132178. IEEE, 2006.\n[20] Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E.\nHoward, W. Hubbard, and L. D. Jackel. Backpropagation\napplied to handwritten zip code recognition. Neural compu-\ntation, 1(4):541\u2013551, 1989.\n[21] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-\nbased learning applied to document recognition. Proceed-\nings of the IEEE, 86(11):2278\u20132324, 1998.\n[22] H. Lee, R. Grosse, R. Ranganath, and A. Y. Ng. Convolu-\ntional deep belief networks for scalable unsupervised learn-\ning of hierarchical representations.\nIn Proceedings of the\n26th Annual International Conference on Machine Learning,\npages 609\u2013616. ACM, 2009.\n[23] D. G. Lowe. Object recognition from local scale-invariant\nfeatures. In Computer vision, 1999. The proceedings of the\nseventh IEEE international conference on, volume 2, pages\n1150\u20131157. Ieee, 1999.\n[24] R. Memisevic and G. E. Hinton. Learning to represent spatial\ntransformations with factored higher-order boltzmann ma-\nchines. Neural Computation, 22(6):1473\u20131492, 2010.\n[25] H. Mobahi, R. Collobert, and J. Weston. Deep learning from\ntemporal coherence in video. In Proceedings of the 26th An-\nnual International Conference on Machine Learning, pages\n737\u2013744. ACM, 2009.\n[26] A. Oliva and A. Torralba. Building the gist of a scene: The\nrole of global image features in recognition.\nProgress in\nbrain research, 155:23\u201336, 2006.\n11\n[27] B. A. Olshausen et al. Emergence of simple-cell receptive\n\ufb01eld properties by learning a sparse code for natural images.\nNature, 381(6583):607\u2013609, 1996.\n[28] D. A. Pomerleau. Alvinn: An autonomous land vehicle in a\nneural network. Technical report, DTIC Document, 1989.\n[29] M. Ranzato, F. J. Huang, Y.-L. Boureau, and Y. LeCun. Un-\nsupervised learning of invariant feature hierarchies with ap-\nplications to object recognition.\nIn Computer Vision and\nPattern Recognition, 2007. CVPR\u201907. IEEE Conference on,\npages 1\u20138. IEEE, 2007.\n[30] M. Ranzato, A. Szlam, J. Bruna, M. Mathieu, R. Collobert,\nand S. Chopra.\nVideo (language) modeling: a baseline\nfor generative models of natural videos.\narXiv preprint\narXiv:1412.6604, 2014.\n[31] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh,\nS. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein,\nA. C. Berg, and L. Fei-Fei. ImageNet Large Scale Visual\nRecognition Challenge. International Journal of Computer\nVision (IJCV), 2015.\n[32] R. Salakhutdinov and G. E. Hinton. Deep boltzmann ma-\nchines. In International Conference on Arti\ufb01cial Intelligence\nand Statistics, pages 448\u2013455, 2009.\n[33] K. Simonyan and A. Zisserman. Two-stream convolutional\nnetworks for action recognition in videos.\nIn Advances\nin Neural Information Processing Systems, pages 568\u2013576,\n2014.\n[34] S. Soatto. Visual scene representations: Suf\ufb01ciency, min-\nimality, invariance and approximations.\narXiv preprint\narXiv:1411.7676, 2014.\n[35] A. Vedaldi and B. Fulkerson. Vlfeat: An open and portable\nlibrary of computer vision algorithms. In Proceedings of the\ninternational conference on Multimedia, pages 1469\u20131472.\nACM, 2010.\n[36] S. Vicente, J. Carreira, L. Agapito, and J. Batista.\nRe-\nconstructing pascal voc.\nIn Computer Vision and Pattern\nRecognition (CVPR), 2014 IEEE Conference on, pages 41\u2013\n48. IEEE, 2014.\n[37] L. Wiskott and T. J. Sejnowski. Slow feature analysis: Un-\nsupervised learning of invariances.\nNeural computation,\n14(4):715\u2013770, 2002.\n[38] J. Xiao, J. Hays, K. A. Ehinger, A. Oliva, and A. Torralba.\nSun database: Large-scale scene recognition from abbey to\nzoo. In Computer vision and pattern recognition (CVPR),\n2010 IEEE conference on, pages 3485\u20133492. IEEE, 2010.\n12\n",
        "sentence": " Others have shown that temporal coherence in videos also provides a signal that can be used to learn powerful visual features (Agrawal et al., 2015; Jayaraman & Grauman, 2015; Wang & Gupta, 2015). Others have shown that temporal coherence in videos also provides a signal that can be used to learn powerful visual features (Agrawal et al., 2015; Jayaraman & Grauman, 2015; Wang & Gupta, 2015). In particular, Wang & Gupta (2015) show that such features provide promising performance on ImageNet. , Agrawal et al. (2015); Doersch et al. , Agrawal et al. (2015); Doersch et al. (2015); Pathak et al. , Agrawal et al. (2015); Doersch et al. (2015); Pathak et al. (2016); Wang & Gupta (2015) and Zhang et al. , Agrawal et al. (2015); Doersch et al. (2015); Pathak et al. (2016); Wang & Gupta (2015) and Zhang et al. , Agrawal et al. (2015); Doersch et al. (2015); Pathak et al. (2016); Wang & Gupta (2015) and Zhang et al. (2016). Finally we compare to state-ofthe-art hand-made features, i.",
        "context": "[25] H. Mobahi, R. Collobert, and J. Weston. Deep learning from\ntemporal coherence in video. In Proceedings of the 26th An-\nnual International Conference on Machine Learning, pages\n737\u2013744. ACM, 2009.\nusing a siamese-like network model [8].\nOur method can also be seen as an instance of feature\nlearning from videos. [37, 14, 25] perform feature learn-\ning from videos by imposing the constraint that tempo-\ntion as supervision compare favorably to features learnt us-\ning class-label as supervision. We also show that egomotion\nbased pretraining outperforms a previous approach based on\nslow feature analysis for unsupervised learning from videos"
    },
    {
        "title": "Diffrac: a discriminative and flexible framework for clustering",
        "author": [
            "F. Bach",
            "Z. Harchaoui"
        ],
        "venue": "In NIPS,",
        "citeRegEx": "Bach and Harchaoui,? \\Q2007\\E",
        "shortCiteRegEx": "Bach and Harchaoui",
        "year": 2007,
        "abstract": "",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "Greedy layer-wise training of deep networks",
        "author": [
            "Y. Bengio",
            "P. Lamblin",
            "D. Popovici",
            "H. Larochelle"
        ],
        "venue": "In NIPS,",
        "citeRegEx": "Bengio et al\\.,? \\Q2007\\E",
        "shortCiteRegEx": "Bengio et al\\.",
        "year": 2007,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " Many have also used autoencoders with a reconstruction loss (Bengio et al., 2007; Ranzato et al., 2007; Masci et al., 2011).",
        "context": null
    },
    {
        "title": "Finding actors and actions in movies",
        "author": [
            "P. Bojanowski",
            "F. Bach",
            "I. Laptev",
            "J. Ponce",
            "C. Schmid",
            "J. Sivic"
        ],
        "venue": "In ICCV,",
        "citeRegEx": "Bojanowski et al\\.,? \\Q2013\\E",
        "shortCiteRegEx": "Bojanowski et al\\.",
        "year": 2013,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " , 2014) or video/text alignment (Bojanowski et al., 2013; 2014; Ramanathan et al., 2014). , 2014) or video/text alignment (Bojanowski et al., 2013; 2014; Ramanathan et al., 2014). In this work, we show that a similar framework can be designed for neural networks. As opposed to Xu et al. (2004), we address the empty assignment problems by restricting the set of possible reassignments to permutations rather than using global linear constrains the assignments.",
        "context": null
    },
    {
        "title": "Weakly supervised action labeling in videos under ordering constraints",
        "author": [
            "P. Bojanowski",
            "R. Lajugie",
            "F. Bach",
            "I. Laptev",
            "J. Ponce",
            "C. Schmid",
            "J. Sivic"
        ],
        "venue": "In ECCV,",
        "citeRegEx": "Bojanowski et al\\.,? \\Q2014\\E",
        "shortCiteRegEx": "Bojanowski et al\\.",
        "year": 2014,
        "abstract": "We are given a set of video clips, each one annotated with an {\\em ordered}\nlist of actions, such as \"walk\" then \"sit\" then \"answer phone\" extracted from,\nfor example, the associated text script. We seek to temporally localize the\nindividual actions in each clip as well as to learn a discriminative classifier\nfor each action. We formulate the problem as a weakly supervised temporal\nassignment with ordering constraints. Each video clip is divided into small\ntime intervals and each time interval of each video clip is assigned one action\nlabel, while respecting the order in which the action labels appear in the\ngiven annotations. We show that the action label assignment can be determined\ntogether with learning a classifier for each action in a discriminative manner.\nWe evaluate the proposed model on a new and challenging dataset of 937 video\nclips with a total of 787720 frames containing sequences of 16 different\nactions from 69 Hollywood movies.",
        "full_text": "Weakly Supervised Action Labeling in Videos\nUnder Ordering Constraints\nPiotr Bojanowski1\u22c6\nR\u00b4emi Lajugie1\u22c6\u22c6\nFrancis Bach1\u22c6\u22c6\nIvan Laptev1\u22c6\nJean Ponce2\u22c6\nCordelia Schmid1\u22c6\u22c6\u22c6\nJosef Sivic1\u22c6\n1INRIA\n2\u00b4Ecole Normale Sup\u00b4erieure\nAbstract. We are given a set of video clips, each one annotated with\nan ordered list of actions, such as \u201cwalk\u201d then \u201csit\u201d then \u201canswer phone\u201d\nextracted from, for example, the associated text script. We seek to tem-\nporally localize the individual actions in each clip as well as to learn a\ndiscriminative classi\ufb01er for each action. We formulate the problem as a\nweakly supervised temporal assignment with ordering constraints. Each\nvideo clip is divided into small time intervals and each time interval of\neach video clip is assigned one action label, while respecting the order in\nwhich the action labels appear in the given annotations. We show that\nthe action label assignment can be determined together with learning\na classi\ufb01er for each action in a discriminative manner. We evaluate the\nproposed model on a new and challenging dataset of 937 video clips with\na total of 787720 frames containing sequences of 16 di\ufb00erent actions from\n69 Hollywood movies.\n1\nIntroduction\nSigni\ufb01cant progress towards action recognition in realistic video settings has\nbeen achieved in the past few years [20,22,24,28,33]. However action recognition\nis often cast as a classi\ufb01cation or detection problem using fully annotated data,\nwhere the temporal boundaries of individual actions, e.g. in the form of pre-\nsegmented video clips, are given during training. The goal of this paper is to\nexploit the supervisory power of the temporal ordering of actions in a video\nstream, as illustrated in \ufb01gure 1.\nGathering fully annotated videos with accurately time-stamped action labels\nis quite time consuming in practice. This limits the utility of fully supervised\nmachine learning techniques on large-scale data. Using data redundancy, weakly\nand semi-supervised methods are a promising alternative in this case. On the\nother hand, it is easy to gather videos with some level of textual annotation but\npoor temporal localization, from movie scripts for example. This type of weak\nsupervisory signal has been used before in classi\ufb01cation [20] and temporal lo-\ncalization [5] tasks. However, the crucial information on the ordering of actions\n\u22c6WILLOW project-team, DI/ENS, ENS/INRIA/CNRS UMR 8548, Paris, France.\n\u22c6\u22c6SIERRA project-team, DI/ENS, ENS/INRIA/CNRS UMR 8548, Paris, France.\n\u22c6\u22c6\u22c6LEAR team, INRIA Grenoble Rh\u02c6one-Alpes, France.\narXiv:1407.1208v1  [cs.CV]  4 Jul 2014\n2\nPiotr Bojanowski et al.\nFig. 1. Examples of video clips with associated actions sequence annotations such as\nprovided in our dataset. Both examples contain the same set of actions but occurring\nin a di\ufb00erent order. In this work we use the type and order of events as a supervisory\nsignal to learn a classi\ufb01er of each action and temporally localize each action in the\nvideo.\nhas, to the best of our knowledge, been ignored so far in the weakly supervised\nsetting. Following recent work on discriminative clustering [2,35], image [16] and\nvideo [4] cosegmentation, we propose to exploit this information in a discrimina-\ntive framework where both the action model and the optimal assignments under\ntemporal constraints are learned together.\n1.1\nRelated Work\nThe temporal ordering of actions, e.g. in the form of Markov models or ac-\ntion grammars, have been used to constrain action prediction in videos [11,13,19,21,27,32]\nThese kinds of spatial and temporal constraints have been also used in the con-\ntext of group activity recognition [1,18]. Similar to us, these papers exploit the\ntemporal structure of videos, but focus on inferring action sequences from noisy\nbut pre-de\ufb01ned action detectors, often in constrained surveillance and labora-\ntory settings with a limited number of actions and static cameras. In contrast,\nin this work we explore the temporal structure of actions for learning action\nclassi\ufb01ers in a weakly supervised set-up and show results on challenging videos\nfrom feature length movies.\nRelated is also work on recognition of composite activities [26], where\natomic action models (\u201ccut\u201d, \u201copen\u201d) are learned given full supervision on a\ncooking video dataset. Composite activity models (\u201cprepare pizza\u201d) are learned\non top of the atomic actions, using the prediction scores for the atomic actions\nas features. Annotations are, however, used without taking into account the\nordering of actions.\nWeakly Supervised Action Labeling in Videos Under Ordering Constraints\n3\nTemporal models for recognition of individual actions have been ex-\nplored in e.g. [20,24,31]. Implicit models in the form of temporal pyramids have\nbeen used with bag-of-features representations [20]. Others have used more ex-\nplicit temporal models in the form of, e.g. latent action parts [24] or hidden\nMarkov models [31]. Contrary to these methods, we do not use an a priori model\nof the temporal structure of individual actions, but instead exploit the given or-\ndering constraints between actions to learn better individual actions models.\nWeak supervision for learning actions has been explored in [4,5,20].\nThese methods use uncertain temporal annotations of actions provided by movie\nscripts. Contrary to these works our method learns multiple actions simultane-\nously and incorporates temporal ordering constraints on action labels obtained,\ne.g. from the movie scripts.\nDynamic time warping algorithms (DTW) can be used to match tem-\nporal sequences, and are extensively used in speech recognition, e.g. [7,25]. In\ncomputer vision, the temporal order of events has been exploited in [23], where\na DTW-like algorithm is used at test time to improve the performance of non-\nmaximum suppression on the output of pre-trained action detectors.\nDiscriminative clustering is an unsupervised method that partitions data\nby minimizing a discriminative objective, optimizing over both classi\ufb01ers and\nlabels [2,35]. Convex formulations of discriminative clustering have been explored\nin [2,8]. In computer vision these methods have been successfully applied to\nco-segmentation [17]. The approach presented in this paper is inspired by this\nframework, but adds to it the use of ordering constraints.\nIn this work, we make use of the Frank-Wolfe algorithm (a.k.a conditional\ngradient) to minimize our cost function. The Frank-Wolfe algorithm [6,15] is a\nclassical convex optimization procedure that permits optimizing a continuously\ndi\ufb00erentiable convex function over a convex compact domain only by optimizing\nlinear functions over the domain. In particular, it does not require any projection\nsteps. It has recently received increased attention in the context of large-scale\noptimization [9,15].\n1.2\nProblem Statement and Contributions\nThe temporal assignment problem addressed in the rest of this paper and illus-\ntrated by Fig. 1 can be stated as follows: We are given a set of N video clips (or\nclips for short in what follows). A clip is de\ufb01ned as a contiguous video segment\nconsisting of F frames, and may correspond, for example, to a scene (as de\ufb01ned\nin a movie script) or a collection of subsequent shots. Each clip is divided into\nT small time intervals (chunks of videos consisting of F/T = 10 frames in our\ncase), and annotated by an ordered list of K elements taken from some action\nset A of size A = |A| (that may consist of labels such as \u201copen door\u201d, \u201cstand\nup\u201d, \u201canswer phone\u201d, etc., as in Fig. 1 for example). Note that clips are not of\nthe same length but for the sake of simplicity, we assume they are. We address\nthe problem of assigning to each time interval of each clip one action in A, re-\nspecting the order in which the actions appear in the original annotation list\n(Fig. 2).\n4\nPiotr Bojanowski et al.\nFig. 2.\nRight: The goal is to \ufb01nd assignment of video intervals 1 to T (x-axis) to\nthe ordered list of action annotations a(1) to a(K) indexed by integer k from 1 to K\n(y-axis). Left: The ordered annotation index k is mapped, through mapping a to action\nlabels from the set A, in that example a(3) =\u201cEat\u201d. To preserve the given ordering\nof annotations we only consider assignments M that are non-decreasing. One such\nassignment m is shown in red.\nContributions. We make the following contributions: (i) we propose a discrim-\ninative clustering model (section 2) that handles weak supervision in the form\nof temporal ordering constraints and recovers a classi\ufb01er for each action to-\ngether with the temporal localization of each action in each video clip; (ii)\nwe design a convex relaxation of the proposed model and show it can be ef-\n\ufb01ciently solved using the conditional gradient (Frank-Wolfe) algorithm (sec-\ntion 3); and \ufb01nally (iii) we demonstrate improved performance of our model\non a new action dataset for the tasks of temporal localization (section 6) and\naction classi\ufb01cation (section 7). All the data and code are publicly available at\nhttp://www.di.ens.fr/willow/research/ordering.\n2\nDiscriminative Clustering with Ordering Constraints\nIn this section we describe the proposed discriminative clustering model that\nincorporates label ordering constraints. The input is a set of video clips, each\nannotated with an ordered list of action labels specifying the sequence of ac-\ntions present in the clip. The output is the temporal assignment of actions to\nindividual time intervals in each clip respecting the ordering constraint provided\nby the annotations together with a learnt classi\ufb01er for each action, common for\nall clips. In the following, we \ufb01rst formulate the temporal assignment of actions\nto individual frames as discriminative clustering (section 2.1), then introduce a\nparametrization of temporal assignments using indicator variables (section 2.2),\nand \ufb01nally we describe the choice of a loss function for the discriminative clus-\ntering that leads to a convex cost (section 2.3).\n2.1\nProblem Formulation\nLet us now formalize the temporal assignment problem. We denote by xn(t) in Rd\nsome local descriptor of video clip number n during time interval number t. For\nWeakly Supervised Action Labeling in Videos Under Ordering Constraints\n5\nevery k in {1, . . . , K}, we also de\ufb01ne an(k) as the element of A corresponding\nto annotation number k (Fig. 2). Note that the set of actions A itself is not\nordered: even if we represent A by a table for convenience, the elements of this\ntable are action labels and have no natural order. The annotations, on the other\nhand, are ordered, for example according to where they occur in a movie script,\nand are represented by some integer between 1 and K. Thus an maps (ordered)\nannotation indices onto (unordered) actions, and depends of course on the video\nclip under annotation. Parts of any video clip may belong to the background. To\naccount for this fact, a dummy label \u2205is inserted in the annotation list between\nevery consecutive pair of actual labels.\nLet us denote by M the set of admissible assignments on {1, . . . , T}, that is,\nthe set of sequences m = (m1, . . . , mT ) with elements in {1, . . . , K} such that\nm1 = 1, mT = K, and mt+1 = mt or mt+1 = mt + 1 for all t in {1, . . . , T \u22121}\nSuch an assignment is illustrated in Fig. 2.\nLet us also denote by F the space of classi\ufb01ers of interest, by \u2126: F \u2192R\nsome regularizer on this space and by \u2113: A \u00d7 RA \u2192R+ an appropriate loss\nfunction. For a given clip n and a \ufb01xed classi\ufb01er f, the problem of assigning the\nclip intervals to the annotation sequence can be written as the minimization of\nthe cost function:\nE(m, f, n) = 1\nT\nT\nX\nt=1\n\u2113(an(mt), f(xn(t)))\n(1)\nwith respect to assignment m in M. The regularizer \u2126prevents over\ufb01tting and\nwe therefore de\ufb01ne a scalar parameter \u03bb to control this e\ufb00ect. Jointly learning\nthe classi\ufb01ers and solving the assignment problem corresponds to the following\noptimization problem:\nmin\nf\u2208F\n\" N\nX\nn=1\nmin\nm\u2208M E(m, f, n)\n#\n+ \u03bb\u2126(f).\n(2)\n2.2\nParameterization Using an Assignment Matrix\nAs will be shown in the following sections, it is convenient to reformulate our\nproblem in terms of indicator variables. The corresponding multi-class loss is\n\u2113: {0, 1}A \u00d7 RA \u2192R+, and the classi\ufb01ers are functions f : Rd \u2192RA. For a clip\nn, let us de\ufb01ne the assignment matrix Zn \u2208RT \u00d7A which is composed of entries\nzn\nta such that zn\nta = 1 if the interval t of clip n is assigned to class a.\nLet Zn\nt denote the row vector of dimension A corresponding to the t-th row\nof Zn . The cost function E(m, f, n), de\ufb01ned in Eq. (1) can be rewritten as\n1\nT\nPT\nt=1 \u2113(Zn\nt , f(xn(t))).\nNote: To avoid cumbersome double summations, we suppose from now that\nwe work with a single clip. This allows us to drop the superscript notation, we\nreplace Zn by Z and skip the sum over clips. We also replace the descriptor\nnotation xn(t) by xt and the row extraction notation Zn\nt by Zt. This is without\n6\nPiotr Bojanowski et al.\nm = [1,1,1,2,2,2,3,3,4,4,4,4,5,5,6,6,6]\na = [2,4,1,2,3,2]\na(m) = [2,2,2,4,4,4,1,1,2,2,2,2,3,3,2,2,2]\nFig. 3.\nIllustration of the correspondence between temporal assignments (left) and\nassociated valid assignment matrices that map action labels a to time intervals t (right).\nLeft: a valid assignment non-decreasing mt = k. Right: the corresponding assignment\nmatrix Z. One can build the assignment matrix Z given the assignment m and the\nannotation sequence a by putting a 1 at index (t, a(mt)) in Z for every t. One obtains\nm given Z by iteratively constructing a sequence of integers of length T such that\nmt+1 = mt if the t-th and (t + 1)-th row of Z are identical, and mt+1 = mt + 1\notherwise.\nloss of generality, and our method as described in the sequel handles multiple\nclips with some simple bookkeeping.\nBecause of temporal constraints, we want the assignment matrices Z to cor-\nrespond to valid assignments m. This amounts to imposing some constraints\non Z. Let us therefore de\ufb01ne Z, the set of all valid assignment matrices as:\nZ =\n\b\nZ \u2208{0, 1}T \u00d7A | \u2203m \u2208M, s.t., \u2200t, Zta = 1 \u21d0\u21d2a(mt) = a\n\t\n.\n(3)\nThere is a bijection between the sets Z and M. For each m in M there ex-\nists a unique corresponding Z in Z and vice versa. Figure 3 gives an intuitive\nillustration of this bijection.\nThe set Z is a subset of the set of stochastic matrices (positive matrices\nwhose rows sum up to 1), formed by the matrices whose columns consist of\nexactly K blocks of contiguous ones occurring in a prede\ufb01ned order (K = 6 in\nFig. 3). There are as many elements in Z as ways of choosing (K \u22121) transitions\namong (T \u22121) possibilities, thus |Z| =\n\u0000T \u22121\nK\u22121\n\u0001\n, which can be extremely large in\nour setting (in our setting T \u2248100 and K \u224810). Furthermore, it is very di\ufb03cult\nto describe explicitly the algebraic constraints on stochastic matrices that de\ufb01ne\nZ. This point will prove important in Sec. 3 when we propose an optimization\nalgorithm for learning our model. Using these notations, Eq. (2) is equivalent to:\nmin\nf\u2208F,Z\u2208Z\n1\nT\nT\nX\nt=1\n\u2113(Zt, f(xt)) + \u03bb\u2126(f).\n(4)\n2.3\nQuadratic Cost Functions\nWe now choose speci\ufb01c functions \u2113and f that will lead to a quadratic cost\nfunction. This choice leads, to a convex relaxation of our problem. We use multi-\nWeakly Supervised Action Labeling in Videos Under Ordering Constraints\n7\nclass linear classi\ufb01ers of the form f(x) = xT W + b, where W \u2208Rd\u00d7A and b \u2208\nR1\u00d7A. We choose the square loss function, regularized with the Frobenius norm\nof W, because in that case the optimal parameters W and b can be computed in\nclosed form through matrix inversion. Let X be the matrix in RT \u00d7d formed by\nthe concatenation of all 1\u00d7d matrices xt. For this choice of loss and regularizer,\nour objective function can be rewritten using the matrices de\ufb01ned above as:\n1\nT\nT\nX\nt=1\n\u2113(Zt, f(xt)) + \u03bb\u2126(f) = 1\nT \u2225Z \u2212XW \u2212b\u22252\nF + \u03bb\n2 \u2225W\u22252\nF .\n(5)\nThis is exactly a ridge regression cost. Minimizing this cost with respect to W and\nb for \ufb01xed Z can be done in closed form [2,10]. Setting the partial derivatives with\nrespect to W and b to zero and plugging the solution back yields the following\nequivalent problem:\nmin\nZ\u2208Z Tr\n\u0000ZZT B\n\u0001\n, where B = 1\nT \u03a0T (IT \u2212X\n\u0000XT \u03a0T X + T\u03bbId\n\u0001\u22121 XT )\u03a0T , (6)\nand the matrix \u03a0p is the p \u00d7 p centering matrix Ip \u22121\np1p1T\np . This corresponds\nto implicitly learning the classi\ufb01er while \ufb01nding the optimal Z by solving a\nquadratic optimisation problem in Z. The implicit classi\ufb01er parameters W and\nb are shared among all video clips and can be recovered in closed-form as:\nW = (XT \u03a0dX + \u03bbI)\u22121XT \u03a0T Z\u2217D1/2,\nb = 1\nT 1T (Z\u2217\u2212Xw)D1/2.\n(7)\n3\nConvex Relaxation and the Frank-Wolfe Algorithm\nIn Sec. 2, we have seen that our model can be interpreted as the minimization\nof a convex quadratic function (B is positive semide\ufb01nite) over a very large but\ndiscrete domain. As is usual for this type of hard combinatorial optimization\nproblem, we replace the discrete set Z by its convex hull Z. This allows us\nto \ufb01nd a continuous solution of the relaxed problem using an appropriate and\ne\ufb03cient algorithm for convex optimization.\n3.1\nThe Frank-Wolfe Algorithm\nWe want to carry out the minimization of a convex function over a complex\npolytope Z, de\ufb01ned as the convex hull of a large but \ufb01nite set of integer points\nde\ufb01ned by the constraints associated with admissible assignments. When it is\npossible to optimize a linear function over a constraint set of this kind, but other\nusual operations (like projections) are not tractable, a good way to optimize a\nconvex objective function is to use the iterative Frank-Wolfe algorithm (a.k.a.\nconditional gradient method) [3,6]. We show in Sec. 3.2 that we can minimize\nlinear functions over Z, so this is an appropriate choice in our case.\nThe idea behind the Frank-Wolfe algorithm is rather simple. An a\ufb03ne ap-\nproximation of the objective function is minimized yielding a point Z\u2217on the\n8\nPiotr Bojanowski et al.\nFig. 4. Illustration of a Frank-Wolfe step (see [15] for more details). Left: the domain\nZ interest, objective function, and its linearization at current point. Right: top view\nof Z. Note that, in the algorithm, we actually minimize a linear function at each step.\nAdding a constant to it does not a\ufb00ect the solution of the minimization problem, it is\nequivalent to minimizing a\ufb03ne functions. That is why, we depicted an hyperplane that\nseems shifted from the origin.\nedge of Z. Then a convex combination of Z\u2217and the current point Z is com-\nputed. This is repeated until convergence. The interpolation parameter \u03b3 can\nbe chosen either by using the universal step size\n2\np+1, where p is the iteration\ncounter (see [15] and references therein) or, in the case of quadratic functions,\nby solving a univariate quadratic equation. In our implementation, we use the\nlatter. A good feature of the Frank-Wolfe algorithm is that it provides for free a\nduality gap (referred to as the linearization duality gap [15]) that can be used as\na certi\ufb01cate of sub-optimality and stopping criterion. The procedure is described\nin the special case of our relaxed problem in Algorithm 1. Figure 4 illustrates\none step of the optimization.\n3.2\nLinear Function Minimization over Z\nIt is possible to minimize linear functions over the integral set Z. Simple ar-\nguments (see for instance Prop B.21 of [3]) show that the solution over Z is\nalso a solution over Z. We will therefore focus on the minimization problem\non Z and keep in mind that it also gives a solution over Z as required by\nthe Frank-Wolfe algorithm. Minimizing a linear function on Z amounts to solv-\ning the problem: minZ\u2208Z Tr\n\u0000CT Z\n\u0001\n= PT\nt=1\nPA\na=1 ZtaCta, where C is a ma-\nk \u21900\nwhile Tr(\u2207f(Zk)(Zk \u2212Z\u2217)) \u2265\u03f5 do\nCompute the current gradient in Z, \u2207f(Zk) = ZT\nk B.\nSolve minZ\u2208Z Tr(Z\u2207f(Zk)) using dynamic programming.\nCompute the optimal Frank-Wolfe step size \u03b3.\nZk+1 = Zk + \u03b3(Z\u2217\u2212Zk)\nk \u2190k + 1.\nend\nAlgorithm 1: The Frank-Wolfe optimization procedure.\nWeakly Supervised Action Labeling in Videos Under Ordering Constraints\n9\ntrix in RT \u00d7A. Using the equivalence between the assignment matrix (Z) and\nthe plain assignment (m) representations (Fig. 3), this is equivalent to solving\nminm\u2208M\nPT\nt=1\nPA\na=1 1a(mt)=aCta. To better deal with the temporal struc-\nture of the assignment, let us denote by D \u2208RT \u00d7K the matrix with entries\nDtk = Cta(k). The minimization problem then becomes minm\u2208M\nPT\nt=1 Dtmt,\nwhich can be solved using dynamic time warping. Indeed, let us de\ufb01ne for all\nt \u2208{1, . . . , T} and k \u2208{1, . . . , K}: P \u2217\nt (k) = minm\u2208M\nPt\ns=1 Dsms. We can think\nof P \u2217\nt (k) as the cost of the optimal path from (1, 1) to (t, k) in the graph de\ufb01ned\nby admissible assignments, and we have the following dynamic programming\nrecursion: P \u2217\nt (k) = Dtk + min(P \u2217\nt\u22121(k \u22121), P \u2217\nt\u22121(k)).\nThe optimal value P \u2217\nT (K) can be computed in O(TK) using dynamic pro-\ngramming, by precomputing the matrix D, incrementally computing the corre-\nsponding P \u2217\nt (k) values, and maintaining at each node (t, k) back pointers to the\nappropriate neighbors.\n3.3\nRounding\nAt convergence, the Frank-Wolfe algorithm \ufb01nds the (non-integer) global opti-\nmum Z\u2217of Eq. (6) over Z. Given Z\u2217, we want to \ufb01nd an appropriate nearby\npoint Z in Z. The simplest geometric rounding scheme consists in \ufb01nding the\nclosest point of Z according to the Frobenius distance : minZ\u2208Z \u2225Z\u2217\u2212Z\u22252\nF .\nExpanding the norm yields: \u2225Z\u2217\u2212Z\u22252\nF = Tr(Z\u2217T Z\u2217) + Tr(ZT Z) \u22122Tr(Z\u2217T Z).\nSince Z\u2217is \ufb01xed, its norm is a constant. Moreover, since Z is an element of\nZ, its squared norm is constant and equal to T. The rounding problem is there-\nfore equivalent to: minZ\u2208Z \u22122Tr(Z\u2217T Z), that is to the minimization of a linear\nfunction over Z. This can be done, as in Sec. 3.2, using dynamic programming.\n4\nPractical Concerns\nIn this section, we detail some re\ufb01nements of our model. First we show how\nto tackle a semi-supervised setting where some time-stamped annotations are\navailable. Secondly, we discuss how to avoid the trivial solutions, a common\nissue in discriminative clustering methods [16,2,8].\n4.1\nSemi-supervised Setting\nLet us suppose that we are given some fully annotated clips (in the sense that\nthey are labeled with time-stamped annotations), corresponding to a total of L\ntime intervals. For every interval l we have a descriptor Xl in Rd and a class label\nal in A. We can incorporate this data by modifying the optimization problem\nas follows:\nmin\nf\u2208F\n\u0014\nmin\nm\u2208M E(m, f, n)\n\u0015\n+ 1\nL\nL\nX\nl=1\n\u2113(al, f(Xl)) + \u03bb\u2126(f).\n(8)\nThis supervised model does not change the optimization procedure, which\nremains valid.\n10\nPiotr Bojanowski et al.\n4.2\nMinimum size constraints\nThere are two inherent problems with discriminative clustering First, the con-\nstant assignment matrix is typically a trivial optimum. As explained in [8] this\noccurs when the optimization domain is symmetric over permutations of the\nlabels of the assignment matrices. Due to our temporal constraints, the set Z is\nnot symmetric and thus we are not subject to this e\ufb00ect.\nThe second di\ufb03culty is linked to the use of the centering matrix \u03a0T in the\nexpression of the quadratic cost matrix B. Indeed, we notice that the constant\nvector of length T is an eigen vector of \u03a0T . Therefore, the column-wise constant\nmatrices are trivial solutions to our problem. These piecewise-constant solutions\nare not admissible for our problem due to the temporal constraints. In practice\nhowever, we have have observed that the algorithm returned an assignment with\nalmost all points being a\ufb00ected to the background label \u2205. We consider two\nways to get rid of the trivial solutions.\n4.3\nLinear penalty.\nTo avoid solutions with dominant classes we add constraints over the fraction of\nclip intervals a\ufb00ected to each class. Ideally, we would like to incorporate a hard\nconstraint over the proportions of each class as in [16], that is, to add to the\nproblem formulated in Eq. (6), a constraint of the type:\n\u2200a \u2208{1, . . . , A} , na\nmin \u2264Tr\n\u0000ZT Ua\n\u0001\n\u2264na\nmax,\n(9)\nwhere Ua \u2208Rn\u00d7A is the indicator matrix with 0 everywhere except on the a-\nth column which is 1. This constraint would make all operations described in\nSec. 3.2 intractable: indeed, dynamic programming cannot be modi\ufb01ed so that\nit respects a constraint of minimal and maximal proportions.\nInstead, a simple method for avoiding trivial solutions is to add to the\nobjective function a Lagrangian multiplier corresponding to the desired hard\nconstraints, which we will set by validation. We therefore incorporate a linear\npenalty (in Z) in our objective function. The multiplier corresponding to this\nnew term is de\ufb01ned as a vector \u03ba \u2208RK. The \ufb01nal objective function then be-\ncomes:\nmin\nZ\u2208Z\nTr\n\u0000ZZT A\n\u0001\n+ Tr\n\u0000\u03ba1T Z\n\u0001\n.\n(10)\nNote that, with this simple modi\ufb01cation, we can still use Alg. 1.\n4.4\nBalancing the loss.\nOur constraint set is heavily unbalanced towards the \u2205class. A common way to\ndeal with unbalanced datasets, is to weight the di\ufb00erent classes appropriately:\nInstead of considering in Eq. (5) the standard least square regression problem,\nwe propose to associate di\ufb00erent weights to di\ufb00erent labels. If we denote by\nD \u2208RA\u00d7A the diagonal matrix containing the weights of each class, the square\nWeakly Supervised Action Labeling in Videos Under Ordering Constraints\n11\nloss of Eq. (5) becomes \u2225(Z \u2212XW \u2212b)D\u22252\nF + \u03bb\n2 \u2225WD\u22252\nF . The actual values of\nD are obtained by validation. Note that this approach di\ufb00ers from so called re-\nweighted least squares (see for instance [10]), since here we weight labels and not\ninstances. Following [2], a simple computation shows that the matrix B remains\nunchanged. Thus, our algorithm is unchanged except in the computation of the\nFrank Wolfe gradient.\n5\nDataset and Features\nDataset. Our input data consists of challenging video clips annotated with\nsequences of actions. One possible source for such data is movies with their\nassociated scripts [4,5,20,30]. The annotations provided by this kind of data are\nnoisy and do not provide ground-truth time-stamps for evaluation. To address\nthis issue, we have constructed a new action dataset, containing clips annotated\nby sequences of actions. We have taken the 69 movies from which the clips of the\nHollywood2 dataset were extracted [20], and manually added full time-stamped\nannotation for 16 classes (12 of these classes are already present in Hollywood2).\nTo build clips that form our input data, we search in the annotations for action\nchains containing at least two elements. To do so, we pad the temporal action\nannotations by 250 frames and search for overlapping intervals. A chain of such\noverlapping annotations forms one video clip with associated action sequence in\nour dataset. In the end we obtain 937 clips, with number of actions ranging from\n2 to 11. We subdivide each clip into temporal intervals of length 10 frames. Clips\ncontain on average 84 intervals, the shortest containing 11, the longest 289.\nFeature representation. We have to de\ufb01ne a feature vector for every interval\nof a clip. We build a bag-of-words vector xt per interval t. Recall that intervals\nare of length 10 frames. To aggregate enough features, we decided to pool fea-\ntures from the 30-frame-long window centered on the interval. We compute video\ndescriptors following [34]. We generate vocabularies of size 2000 for HOF fea-\ntures. We restricted ourselves to one channel to improve the running time, while\nbeing aware that by doing so we sacri\ufb01ce some performance. In our informal\nexperiments, we also tried the MBH channels yielding very close performance.\nWe use the Hellinger kernel to obtain the explicit feature map by square-rooting\nthe l1 normalized histograms. Now every data point is associated with a vector\nxt in R2000.\n6\nAction Labeling Experiments\nExperimental Setup. To carry out the action labeling experiments, we split\n90% of the dataset into three parts (Fig. 5) that we denote Sup (for supervised),\nEval (for evaluation) and Val (for validation). Sup is the part of data that has\ntime-stamped annotations, and it is used only in the semi-supervised setting de-\nscribed in Sec. 4.1. Val is the set of examples on which we automatically adjust\nthe hyper-parameters for our method (\u03bb, \u03ba, D). In practice we \ufb01x the Val set to\n12\nPiotr Bojanowski et al.\nFig. 5. Splitting of the data described in Sec. 6.\ncontain 5% of the dataset. This set is provided with fully time-stamped annota-\ntions, but these are not used during the cost optimization. None of the reported\nresults are computed on this set. We evaluate the quality of the assignment on\nthe Eval set. Note that we carry out the Frank-Wolfe optimization on the union\nof all three sets. The annotations from the Sup set are used to constrain Z in the\nsemi-supervised setup while those from the Val set are only used for choosing our\nhyper parameters. The supervisory information used over the rest of the data\nare the ordered annotations without time stamps. Please also keep in mind that\nthere are no \u201ctraining\u201d and \u201ctesting\u201d phases per se in this primary assignment\ntask. All our experiments are conducted over \ufb01ve random splits of the data. This\nallows us to present results with error bars.\nPerformance Measure. Several measures may be used to evaluate the perfor-\nmance of discriminative clustering algorithms. Some authors propose to use the\noutput classi\ufb01er to perform a classi\ufb01cation task [5,35] or use the output parti-\ntion of the data as a solution of the segmentation task [16]. Yet another way to\nevaluate is to use a loss between partitions [12] as in [2]. Note that because of\ntemporal constraints, for every clip we have a set of corresponding (prediction,\nground-truth) pairs. We have thus chosen to measure the assignment quality\nfor every ground-truth action interval I* and prediction I as |I \u2229I\u2217|/|I|. This\nmeasure is similar to the standard Jaccard measure used for comparing ensem-\nbles [14]. Therefore, with a slight abuse of notation, we refer to this measure as\nthe Jaccard measure. This performance measure is well suited for our problem\nsince it respects the following properties: (1) it is high if the action predicted is\nincluded in the ground-truth annotation, (2) it is low if the prediction is bigger\nthan the annotation, (3) it is lowest if the prediction is out of the annotation, (4)\nit does not take into account the prediction of the background class. The score\nis averaged across all ground-truth intervals. The perfect score of 1 is achieved\nwhen all actions are aligned to the correct annotations, but accurate temporal\nsegmentation is not required as long as the predicted labels are within the ground\ntruth interval.\nBaselines. We compare our method to the three following baselines. All these\nare trained using the same features as the ones used for our method. For all base-\nlines, we round the obtained solution Z using the scheme described in Sec. 3.3.\nNormalized Cuts (NCUT). We compare our method to normalized cuts (or spec-\ntral clustering) [29]. Let us de\ufb01ne B as the symmetric Laplacian of the matrix\nE: B = I \u2212D\u22121\n2 ED\u22121\n2 , where D = Diag (E1). E measures both the proximity\nand appearance similarity of intervals. For all (i, j) in {1, . . . , T}2, we compute:\nEij = e\u2212\u03b1|i\u2212j|\u2212\u03b2d\u03c72(Xi,Xj) 1|i\u2212j|<dmin, where d\u03c72 is the Chi-squared distance.\nMore precisely, we minimize over all cuts Z the cost g(Z) = Tr\n\u0000ZZT B\n\u0001\n. g is\nWeakly Supervised Action Labeling in Videos Under Ordering Constraints\n13\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.2\n0.25\n0.3\n0.35\n0.4\n0.45\n0.5\nFraction of annotated data\nAverage Jaccard measure\nWeakly\u2212supervised setting\n \n \nOur method (weak)\nSL\nNCUT\nBojanowski et al. (weak)\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.2\n0.25\n0.3\n0.35\n0.4\n0.45\n0.5\nFraction of annotated data\nAverage Jaccard measure\nSemi\u2212supervised setting\n \n \nOur method (semi)\nSL\nBojanowski et al. (semi)\nFig. 6. Alignment evaluation for all considered models. Left: weakly-supervised meth-\nods. This graph is shown for various fractions of fully supervised data only to compare\nto the SL baseline. Weak methods do not make use of this supervision. Right: semi-\nsupervised methods. See supplementary material for qualitative results.\nconvex (B is positive semide\ufb01nite) and we can use the Frank-Wolfe optimization\nscheme developed for our model. Intuitively, this baseline is searching for a par-\ntition of the video such that time intervals falling into the same segments have\nclose-by features according to the Chi-squared distance.\nBojanowski et al. [4]. We also consider our own implementation of the weakly-\nsupervised approach proposed in [4]. We replace our ordering constraints by the\ncorresponding \u201cat least one\u201d constraints. When an action is mentioned in the\nsequence, we require it appears at least once in the clip. This corresponds to a\nset of linear constraints on Z. We adapt this technique in order to work on our\ndataset. Indeed, the available implementation requires storing a square matrix\nof the size of the problem. Instead, we choose to minimize the convex objective\nof [4] using the Frank-Wolfe algorithm which is more scalable.\nSupervised Square Loss (SL). For completeness, we also compare our method to\na fully supervised approach. We train a classi\ufb01er using the square loss over the\nannotated Sup set and score all time intervals in Eval. We use the square loss\nsince it is used in our method and all other baselines.\nWeakly Supervised Setup. In this setup, all baselines except (SL) have only\naccess to weak supervision in the form of ordering constraints. Figure 6 (left)\nillustrates the quality of the predicted asignmentss and compares our method to\nbaselines. Our method performs better than all other weakly-supervised meth-\nods. Both the Bojanowski et al. and NCUT baselines have low scores in the\nweakly-supervised setting. This shows the advantage of exploiting temporal con-\nstraints as weak supervisory signal. The fully supervised baseline (blue) eventu-\nally recovers a better alignment than our method as the fraction of fully anno-\ntated data increases. This occurs (when the red line crosses the blue line) at the\n25% mark, as the supervised data makes up for the lack of ordering constraints.\nFully time-stamped annotated data are expensive to produce whereas movies\nscripts are often easy to get. It appears thus that manually annotated videos are\nnot always necessary since good performance is reached simply by using weak\nsupervision. Figure 7 shows the results for all weakly-supervised methods for all\n14\nPiotr Bojanowski et al.\nclasses. We notice that we outperform the baselines on the most frequent classes\n(such as \u201cOpen Door\u201d, \u201cSit Down\u201d and \u201cStand Up\u201d).\nSemi-supervised Setup. Figure 6 (right) illustrates the performance of our\nmodel when some supervised data is available. The fraction of the supervised\ndata is given on the x-axis. First, note that our semi-supervised method (red)\nis always and consistently (Cf error bars) above the square loss baseline (blue).\nOf course, during the optimization, our method has access to weak annotations\nover the whole dataset, and to full annotations on the Sup set whereas the SL\nbaseline has access only to the latter. This demonstrates the bene\ufb01ts of exploiting\ntemporal constraints during learning. The semi-supervised Bojanowski et al.\nbaseline (orange) has low performance, but it improves with the amount of full\nsupervision provided.\n7\nClassi\ufb01cation Experiments\nThe experiments in the previous section evaluate the quality of the recovered\nassignment matrix Z. Here we evaluate instead the quality of the recovered\nclassi\ufb01ers on a held-out test set of data for an action classi\ufb01cation task. We\nrecover these classi\ufb01ers as explained later in this section. We can treat them\nas K independent, one-versus-rest classi\ufb01ers and use them to score the samples\nfrom the test set. We evaluate this performance by computing per-class precision\nand recall and report the corresponding average precision for each class.\nExperimental setup. The models are trained following the procedure de-\nscribed in the previous section. To test the performance of our classi\ufb01ers, we\nuse the held out set of clips. This set is made of 10% of the clips from the origi-\nnal data. The clips from this set are identical in nature to the ones used to train\nthe models. We also perform multiple random splits to report results with error\nbars.\nRecovering the classi\ufb01ers. One of the nice features of our method is that we\ncan estimate the implicit classi\ufb01ers corresponding to our solution Z\u2217. We do so\nusing the expression from Eq. 7.\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\nAverage Jaccard measure\n \n \nAnswerPhone\nDriveCar\nEat\nFightPerson\nGetOutCar\nGrabHand\nHandShake\nHugPerson\nKiss\nOpenCarDoor\nOpenDoor\nRun\nSitDown\nSitUp\nStandUp\nThreatenPerson\nNCUT\nOur method (weak)\nBojanowski et al. (weak)\nFig. 7. Alignment performance for various weakly-supervised methods for all classes.\nWeakly Supervised Action Labeling in Videos Under Ordering Constraints\n15\nBaselines. We compare the classi\ufb01ers obtained by our method to those obtained\nby the Bojanowski et al. baseline [4]. We also compare them to the classi\ufb01ers\nlearned using the (SL) baseline.\nWeakly Supervised Setup. Classi\ufb01cation results are presented in Fig. 8 (left).\nWe observe a behavior similar to the action labeling experiment. But the super-\nvised classi\ufb01er (SL) trained on the Sup set using the square loss (blue) always\nperforms worse than our model (red). This can be explained by the fact that the\nproposed model makes use of mode data. Even though our model has only access\nto weak annotation, it can prove su\ufb03cient to train good classi\ufb01ers. The weakly-\nsupervised method from Bojanowski et al. (orange) is performing worst, exactly\nas in the previous task. This can be explained by the fact that this method does\nnot have access to full supervision or ordering constraints.\nSemi-supervised Setup. In the semi-supervised setting (Fig. 8 (left)), our\nmethod (red) performs better than the supervised SL baseline (blue). The ac-\ntion model we recover is consistently better than the one obtained using only\nfully supervised data. Thus, our method is able to perform well semi-supervised\nlearning. The Bojanowski et al. baseline (orange) improves when the fraction\nof annotated examples increases. Nonetheless, we see that making use of order-\ning constraints as used by our method signigicantly improves over simple linear\ninequalities (\u201cat least one\u201d constraints as formulated in [4]).\nAcknowledgements. This work was supported by the European integrated project\nAXES, the MSR-INRIA laboratory, EIT-ICT labs, a Google Research Award, a PhD\nfellowship from the EADS Foundation, the Institut Universitaire de France and ERC\ngrants ALLEGRO, VideoWorld, Activia and Sierra.\nReferences\n1. Amer, M.R., Todorovic, S., Fern, A., Zhu, S.C.: Monte carlo tree search for schedul-\ning activity recognition. In: ICCV (2013)\n2. Bach, F., Harchaoui, Z.: DIFFRAC: a discriminative and \ufb02exible framework for\nclustering. In: NIPS (2007)\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.15\n0.2\n0.25\nFraction of annotated data\nmean average precision\nWeakly\u2212supervised setting\n \n \nOur method (weak)\nSL\nBojanowski et al. (weak)\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.15\n0.2\n0.25\nFraction of annotated data\nmean average precision\nSemi\u2212supervised setting\n \n \nOur method (semi)\nSL\nBojanowski et al. (semi)\nFig. 8. Classi\ufb01cation performance for various models. Left: weakly-supervised meth-\nods. Right: semi-supervised methods. Qualitative results in supp. material.\n16\nPiotr Bojanowski et al.\n3. Bertsekas, D.: Nonlinear Programming. Athena Scienti\ufb01c (1999)\n4. Bojanowski, P., Bach, F., Laptev, I., Ponce, J., Schmid, C., Sivic, J.: Finding\nActors and Actions in Movies. In: ICCV (2013)\n5. Duchenne, O., Laptev, I., Sivic, J., Bach, F., Ponce, J.: Automatic annotation of\nhuman actions in video. In: ICCV (2009)\n6. Frank, M., Wolfe, P.: An algorithm for quadratic programming. Naval Research\nLogistics Quarterly (1956)\n7. Gold, B., Morgan, N., Ellis, D.: Speech and Audio Signal Processing - Processing\nand Perception of Speech and Music, Second Edition. Wiley (2011)\n8. Guo, Y., Schuurmans, D.: Convex Relaxations of Latent Variable Training. In:\nNIPS (2007)\n9. Harchaoui, Z.: Conditional gradient algorithms for machine learning. In: NIPS\nWorkshop (2012)\n10. Hastie, T., Tibshirani, R., Friedman, J.: The elements of statistical learning: data\nmining, inference and prediction. Springer (2009)\n11. Hongeng, S., Nevatia, R.: Large-scale event detection using semi-hidden markov\nmodels. In: ICCV (2003)\n12. Hubert, L., Arabie, P.: Comparing partitions. Journal of classi\ufb01cation (1985)\n13. Ivanov, Y.A., Bobick, A.F.: Recognition of visual activities and interactions by\nstochastic parsing. PAMI (2000)\n14. Jaccard, P.: The distribution of the \ufb02ora in the alpine zone. New Phytologist (1912)\n15. Jaggi, M.: Revisiting Frank-Wolfe: Projection-free sparse convex optimization. In:\nICML (2013)\n16. Joulin, A., Bach, F., Ponce, J.: Discriminative Clustering for Image Co-\nsegmentation. In: CVPR (2010)\n17. Joulin, A., Bach, F., Ponce, J.: Multi-class cosegmentation. In: CVPR (2012)\n18. Khamis, S., Morariu, V.I., Davis, L.S.: Combining per-frame and per-track cues\nfor multi-person action recognition. In: ECCV (2012)\n19. Kwak, S., Han, B., Han, J.H.: Scenario-based video event recognition by constraint\n\ufb02ow. In: CVPR (2011)\n20. Laptev, I., Marszalek, M., Schmid, C., Rozenfeld, B.: Learning realistic human\nactions from movies. In: CVPR (2008)\n21. Laxton, B., Lim, J., Kriegman, D.J.: Leveraging temporal, contextual and ordering\nconstraints for recognizing complex activities in video. In: CVPR (2007)\n22. Liu, J., Kuipers, B., Savarese, S.: Recognizing human actions by attributes. In:\nCVPR (2011)\n23. Nguyen, M.H., Lan, Z.Z., la Torre, F.D.: Joint segmentation and classi\ufb01cation of\nhuman actions in video. In: CVPR (2011)\n24. Niebles, J.C., Chen, C.W., Li, F.F.: Modeling Temporal Structure of Decomposable\nMotion Segments for Activity Classi\ufb01cation. In: ECCV (2010)\n25. Rabiner, L.R., Juang, B.H.: Fundamentals of speech recognition. Prentice Hall\n(1993)\n26. Rohrbach, M., Regneri, M., Andriluka, M., Amin, S., Pinkal, M., Schiele, B.: Script\nData for Attribute-Based Recognition of Composite Activities. In: ECCV (2012)\n27. Ryoo, M.S., Aggarwal, J.K.: Recognition of composite human activities through\ncontext-free grammar based representation. In: CVPR (2006)\n28. Sadanand, S., Corso, J.J.: Action bank: A high-level representation of activity in\nvideo. In: CVPR (2012)\n29. Shi, J., Malik, J.: Normalized Cuts and Image Segmentation. In: CVPR (1997)\n30. Sivic, J., Everingham, M., Zisserman, A.: \u201dWho are you?\u201d - Learning person spe-\nci\ufb01c classi\ufb01ers from video. In: CVPR (2009)\nWeakly Supervised Action Labeling in Videos Under Ordering Constraints\n17\n31. Tang, K., Fei-Fei, L., Koller, D.: Learning latent temporal structure for complex\nevent detection. In: CVPR (2012)\n32. Vu, V.T., Bremond, F., Thonnat, M.: Automatic video interpretation: A novel\nalgorithm for temporal scenario recognition. In: IJCAI (2003)\n33. Wang, H., Kl\u00a8aser, A., Schmid, C., Liu, C.L.: Action recognition by dense trajecto-\nries. In: CVPR (2011)\n34. Wang, H., Schmid, C.: Action Recognition with Improved Trajectories. In: ICCV\n(2013)\n35. Xu, L., Neufeld, J., Larson, B., Schuurmans, D.: Maximum Margin Clustering. In:\nNIPS (2004)\n",
        "sentence": "",
        "context": "\u22c6\u22c6SIERRA project-team, DI/ENS, ENS/INRIA/CNRS UMR 8548, Paris, France.\n\u22c6\u22c6\u22c6LEAR team, INRIA Grenoble Rh\u02c6one-Alpes, France.\narXiv:1407.1208v1  [cs.CV]  4 Jul 2014\n2\nPiotr Bojanowski et al.\nAXES, the MSR-INRIA laboratory, EIT-ICT labs, a Google Research Award, a PhD\nfellowship from the EADS Foundation, the Institut Universitaire de France and ERC\ngrants ALLEGRO, VideoWorld, Activia and Sierra.\nReferences\nLogistics Quarterly (1956)\n7. Gold, B., Morgan, N., Ellis, D.: Speech and Audio Signal Processing - Processing\nand Perception of Speech and Music, Second Edition. Wiley (2011)\n8. Guo, Y., Schuurmans, D.: Convex Relaxations of Latent Variable Training. In:"
    },
    {
        "title": "Learning feature representations with k-means",
        "author": [
            "A. Coates",
            "A. Ng"
        ],
        "venue": "In Neural Networks: Tricks of the Trade. Springer,",
        "citeRegEx": "Coates and Ng,? \\Q2012\\E",
        "shortCiteRegEx": "Coates and Ng",
        "year": 2012,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "Histograms of oriented gradients for human detection",
        "author": [
            "N. Dalal",
            "B. Triggs"
        ],
        "venue": "In CVPR,",
        "citeRegEx": "Dalal and Triggs,? \\Q2005\\E",
        "shortCiteRegEx": "Dalal and Triggs",
        "year": 2005,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " We use the training set of ImageNet to learn our convolutional network (Deng et al., 2009). This dataset is composed of 1, 281, 167 images that belong to 1, 000 object categories. For the transfer learning experiments, we also consider PASCAL VOC 2007. In addition to fully supervised approaches (Krizhevsky et al., 2012), we compare our method to several unsupervised approaches, i.e., autoencoder, GAN and BiGAN as reported in Donahue et al. (2016). We also compare to selfsupervised approaches, i.",
        "context": null
    },
    {
        "title": "Imagenet: A large-scale hierarchical image database",
        "author": [
            "J. Deng",
            "W. Dong",
            "R. Socher",
            "L.J. Li",
            "K. Li",
            "L. Fei-Fei"
        ],
        "venue": "In CVPR,",
        "citeRegEx": "Deng et al\\.,? \\Q2009\\E",
        "shortCiteRegEx": "Deng et al\\.",
        "year": 2009,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " In addition, we propose an online algorithm able to scale to massive image databases like ImageNet (Deng et al., 2009). We use the training set of ImageNet to learn our convolutional network (Deng et al., 2009).",
        "context": null
    },
    {
        "title": "Deep generative image models using a laplacian pyramid of adversarial networks",
        "author": [
            "E.L. Denton",
            "S. Chintala",
            "R. Fergus"
        ],
        "venue": "In NIPS,",
        "citeRegEx": "Denton et al\\.,? \\Q2015\\E",
        "shortCiteRegEx": "Denton et al\\.",
        "year": 2015,
        "abstract": "In this paper we introduce a generative parametric model capable of producing\nhigh quality samples of natural images. Our approach uses a cascade of\nconvolutional networks within a Laplacian pyramid framework to generate images\nin a coarse-to-fine fashion. At each level of the pyramid, a separate\ngenerative convnet model is trained using the Generative Adversarial Nets (GAN)\napproach (Goodfellow et al.). Samples drawn from our model are of significantly\nhigher quality than alternate approaches. In a quantitative assessment by human\nevaluators, our CIFAR10 samples were mistaken for real images around 40% of the\ntime, compared to 10% for samples drawn from a GAN baseline model. We also show\nsamples from models trained on the higher resolution images of the LSUN scene\ndataset.",
        "full_text": "Deep Generative Image Models using a\nLaplacian Pyramid of Adversarial Networks\nEmily Denton\u2217\nDept. of Computer Science\nCourant Institute\nNew York University\nSoumith Chintala\u2217\nArthur Szlam\nRob Fergus\nFacebook AI Research\nNew York\nAbstract\nIn this paper we introduce a generative parametric model capable of producing\nhigh quality samples of natural images. Our approach uses a cascade of convo-\nlutional networks within a Laplacian pyramid framework to generate images in\na coarse-to-\ufb01ne fashion. At each level of the pyramid, a separate generative con-\nvnet model is trained using the Generative Adversarial Nets (GAN) approach [10].\nSamples drawn from our model are of signi\ufb01cantly higher quality than alternate\napproaches. In a quantitative assessment by human evaluators, our CIFAR10 sam-\nples were mistaken for real images around 40% of the time, compared to 10% for\nsamples drawn from a GAN baseline model. We also show samples from models\ntrained on the higher resolution images of the LSUN scene dataset.\n1\nIntroduction\nBuilding a good generative model of natural images has been a fundamental problem within com-\nputer vision. However, images are complex and high dimensional, making them hard to model\nwell, despite extensive efforts. Given the dif\ufb01culties of modeling entire scene at high-resolution,\nmost existing approaches instead generate image patches. In contrast, in this work, we propose\nan approach that is able to generate plausible looking scenes at 32 \u00d7 32 and 64 \u00d7 64. To do this,\nwe exploit the multi-scale structure of natural images, building a series of generative models, each\nof which captures image structure at a particular scale of a Laplacian pyramid [1]. This strategy\nbreaks the original problem into a sequence of more manageable stages. At each scale we train a\nconvolutional network-based generative model using the Generative Adversarial Networks (GAN)\napproach of Goodfellow et al. [10]. Samples are drawn in a coarse-to-\ufb01ne fashion, commencing\nwith a low-frequency residual image. The second stage samples the band-pass structure at the next\nlevel, conditioned on the sampled residual. Subsequent levels continue this process, always condi-\ntioning on the output from the previous scale, until the \ufb01nal level is reached. Thus drawing samples\nis an ef\ufb01cient and straightforward procedure: taking random vectors as input and running forward\nthrough a cascade of deep convolutional networks (convnets) to produce an image.\nDeep learning approaches have proven highly effective at discriminative tasks in vision, such as\nobject classi\ufb01cation [3]. However, the same level of success has not been obtained for generative\ntasks, despite numerous efforts [13, 24, 28]. Against this background, our proposed approach makes\na signi\ufb01cant advance in that it is straightforward to train and sample from, with the resulting samples\nshowing a surprising level of visual \ufb01delity, indicating a better density model than prior methods.\n1.1\nRelated Work\nGenerative image models are well studied, falling into two main approaches: non-parametric and\nparametric. The former copy patches from training images to perform, for example, texture synthesis\n[6] or super-resolution [8]. More ambitiously, entire portions of an image can be in-painted, given a\nsuf\ufb01ciently large training dataset [12]. Early parametric models addressed the easier problem of tex-\n\u2217denotes equal contribution.\n1\narXiv:1506.05751v1  [cs.CV]  18 Jun 2015\nture synthesis [2, 31, 20], with Portilla & Simoncelli [20] making use of a steerable pyramid wavelet\nrepresentation [25], similar to our use of a Laplacian pyramid. For image processing tasks, models\nbased on marginal distributions of image gradients are effective [18, 23], but are only designed for\nimage restoration rather than being true density models (so cannot sample an actual image). Very\nlarge Gaussian mixture models [32] and sparse coding models of image patches [29] can also be\nused but suffer the same problem.\nA wide variety of deep learning approaches involve generative parametric models. Restricted Boltz-\nmann machines [13, 16, 19, 21], Deep Boltzmann machines [24, 7], Denoising auto-encoders [28]\nall have a generative decoder that reconstructs the image from the latent representation. Variational\nauto-encoders [15, 22] provide probabilistic interpretation which facilitates sampling. However, for\nall these methods convincing samples have only been shown on simple datasets such as MNIST\nand NORB, possibly due to training complexities which limit their applicability to larger and more\nrealistic images.\nSeveral recent papers have proposed novel generative models. Dosovitskiy et al. [5] showed how a\nconvnet can draw chairs with different shapes and viewpoints. While our model also makes use of\nconvnets, it is able to sample general scenes and objects. The DRAW model of Gregor et al. [11]\nused an attentional mechanism with an RNN to generate images via a trajectory of patches, showing\nsamples of MNIST and CIFAR10 images. Sohl-Dickstein et al. [26] use a diffusion-based process\nfor deep unsupervised learning and the resulting model is able to produce reasonable CIFAR10 sam-\nples. Theis and Bethge [27] employ LSTMs to capture spatial dependencies and show convincing\ninpainting results of natural textures.\nOur work builds on the GAN approach of Goodfellow et al. [10] which works well for smaller\nimages (e.g. MNIST) but cannot directly handle large ones, unlike our method. Most relevant to our\napproach is the preliminary work of Mirza and Osindero [17] and Gauthier [9] who both propose\nconditional versions of the GAN model. The former shows MNIST samples, while the latter focuses\nsolely on frontal face images. Our approach also uses several forms of conditional GAN model but\nis much more ambitious in its scope.\n2\nApproach\nThe basic building block of our approach is the generative adversarial network (GAN) of Goodfellow\net al. [10]. After reviewing this, we introduce our LAPGAN model which integrates a conditional\nform of GAN model into the framework of a Laplacian pyramid.\n2.1\nGenerative Adversarial Networks\nThe GAN approach [10] is a framework for training generative models, which we brie\ufb02y explain in\nthe context of image data. The method pits two networks against one another: a generative model G\nthat captures the data distribution and a discriminative model D that distinguishes between samples\ndrawn from G and images drawn from the training data. In our approach, both G and D are convo-\nlutional networks. The former takes as input a noise vector z drawn from a distribution pNoise(z) and\noutputs an image \u02dch. The discriminative network D takes an image as input stochastically chosen\n(with equal probability) to be either \u02dch \u2013 as generated from G, or h \u2013 a real image drawn from the\ntraining data pData(h). D outputs a scalar probability, which is trained to be high if the input was\nreal and low if generated from G. A minimax objective is used to train both models together:\nmin\nG max\nD Eh\u223cpData(h)[log D(h)] + Ez\u223cpNoise(z)[log(1 \u2212D(G(z)))]\n(1)\nThis encourages G to \ufb01t pData(h) so as to fool D with its generated samples \u02dch. Both G and D\nare trained by backpropagating the loss in Eqn. 1 through their respective models to update the\nparameters.\nThe conditional generative adversarial net (CGAN) is an extension of the GAN where both networks\nG and D receive an additional vector of information l as input. This might contain, say, information\nabout the class of the training example h. The loss function thus becomes\nmin\nG max\nD Eh,l\u223cpData(h,l)[log D(h, l)] + Ez\u223cpNoise(z),l\u223cpl(l)[log(1 \u2212D(G(z, l), l))]\n(2)\nwhere pl(l) is, for example, the prior distribution over classes. This model allows the output of\nthe generative model to be controlled by the conditioning variable l. Mirza and Osindero [17] and\n2\nGauthier [9] both explore this model with experiments on MNIST and faces, using l as a class\nindicator. In our approach, l will be another image, generated from another CGAN model.\n2.2\nLaplacian Pyramid\nThe Laplacian pyramid [1] is a linear invertible image representation consisting of a set of band-pass\nimages, spaced an octave apart, plus a low-frequency residual. Formally, let d(.) be a downsampling\noperation which blurs and decimates a j \u00d7 j image I, so that d(I) is a new image of size j/2 \u00d7 j/2.\nAlso, let u(.) be an upsampling operator which smooths and expands I to be twice the size, so u(I)\nis a new image of size 2j \u00d7 2j. We \ufb01rst build a Gaussian pyramid G(I) = [I0, I1, . . . , IK], where\nI0 = I and Ik is k repeated applications\u2217of d(.) to I. K is the number of levels in the pyramid,\nselected so that the \ufb01nal level has very small spatial extent (\u22648 \u00d7 8 pixels).\nThe coef\ufb01cients hk at each level k of the Laplacian pyramid L(I) are constructed by taking the\ndifference between adjacent levels in the Gaussian pyramid, upsampling the smaller one with u(.)\nso that the sizes are compatible:\nhk = Lk(I) = Gk(I) \u2212u(Gk+1(I)) = Ik \u2212u(Ik+1)\n(3)\nIntuitively, each level captures image structure present at a particular scale. The \ufb01nal level of the\nLaplacian pyramid hK is not a difference image, but a low-frequency residual equal to the \ufb01nal\nGaussian pyramid level, i.e.\nhK = IK. Reconstruction from a Laplacian pyramid coef\ufb01cients\n[h1, . . . , hK] is performed using the backward recurrence:\nIk = u(Ik+1) + hk\n(4)\nwhich is started with IK = hK and the reconstructed image being I = Io. In other words, starting\nat the coarsest level, we repeatedly upsample and add the difference image h at the next \ufb01ner level\nuntil we get back to the full resolution image.\n2.3\nLaplacian Generative Adversarial Networks (LAPGAN)\nOur proposed approach combines the conditional GAN model with a Laplacian pyramid represen-\ntation. The model is best explained by \ufb01rst considering the sampling procedure. Following training\n(explained below), we have a set of generative convnet models {G0, . . . , GK}, each of which cap-\ntures the distribution of coef\ufb01cients hk for natural images at a different level of the Laplacian pyra-\nmid. Sampling an image is akin to the reconstruction procedure in Eqn. 4, except that the generative\nmodels are used to produce the hk\u2019s:\n\u02dcIk = u(\u02dcIk+1) + \u02dchk = u(\u02dcIk+1) + Gk(zk, u(\u02dcIk+1))\n(5)\nThe recurrence starts by setting \u02dcIK+1 = 0 and using the model at the \ufb01nal level GK to generate a\nresidual image \u02dcIK using noise vector zK: \u02dcIK = GK(zK). Note that models at all levels except the\n\ufb01nal are conditional generative models that take an upsampled version of the current image \u02dcIk+1 as\na conditioning variable, in addition to the noise vector zk. Fig. 1 shows this procedure in action for\na pyramid with K = 3 using 4 generative models to sample a 64 \u00d7 64 image.\nThe generative models {G0, . . . , GK} are trained using the CGAN approach at each level of the\npyramid. Speci\ufb01cally, we construct a Laplacian pyramid from each training image I. At each level\n\u2217i.e. I2 = d(d(I)).\nG2 \n~ I3 \nG3 \nz2 \n~ h2 \nz3 \nG1 \nz1 \nG0 \nz0 \n~ I2 \nl2 \n~ I0 \nh0 \n~ \nI1 \n~ \n~ h1 \nl1 \nl0 \nFigure 1: The sampling procedure for our LAPGAN model. We start with a noise sample z3 (right side) and\nuse a generative model G3 to generate \u02dcI3. This is upsampled (green arrow) and then used as the conditioning\nvariable (orange arrow) l2 for the generative model at the next level, G2. Together with another noise sample\nz2, G2 generates a difference image \u02dch2 which is added to l2 to create \u02dcI2. This process repeats across two\nsubsequent levels to yield a \ufb01nal full resolution sample I0.\n3\nG0 \nl2 \n~ I3 \nG3 \nD0 \nz0 \nD1 \nD2 \nh2 \n~ h2 \nz3 \nD3 \nI3 \nI2 \nI2 \nI3 \nReal/Generated? \nReal/ \nGenerated? \nG1 \nz1 \nG2 \nz2 \nReal/Generated? \nReal/ \nGenerated? \nl0 \nI = I0 \nh0 \nI1 \nI1 \nl1 \n~ h1 \nh1 \nh0 \n~ \nFigure 2: The training procedure for our LAPGAN model. Starting with a 64x64 input image I from our\ntraining set (top left): (i) we take I0 = I and blur and downsample it by a factor of two (red arrow) to produce\nI1; (ii) we upsample I1 by a factor of two (green arrow), giving a low-pass version l0 of I0; (iii) with equal\nprobability we use l0 to create either a real or a generated example for the discriminative model D0. In the real\ncase (blue arrows), we compute high-pass h0 = I0 \u2212l0 which is input to D0 that computes the probability of\nit being real vs generated. In the generated case (magenta arrows), the generative network G0 receives as input\na random noise vector z0 and l0. It outputs a generated high-pass image \u02dch0 = G0(z0, l0), which is input to\nD0. In both the real/generated cases, D0 also receives l0 (orange arrow). Optimizing Eqn. 2, G0 thus learns\nto generate realistic high-frequency structure \u02dch0 consistent with the low-pass image l0. The same procedure is\nrepeated at scales 1 and 2, using I1 and I2. Note that the models at each level are trained independently. At\nlevel 3, I3 is an 8\u00d78 image, simple enough to be modeled directly with a standard GANs G3 & D3.\nwe make a stochastic choice (with equal probability) to either (i) construct the coef\ufb01cients hk either\nusing the standard procedure from Eqn. 3, or (ii) generate them using Gk:\n\u02dchk = Gk(zk, u(Ik+1))\n(6)\nNote that Gk is a convnet which uses a coarse scale version of the image lk = u(Ik+1) as an input,\nas well as noise vector zk. Dk takes as input hk or \u02dchk, along with the low-pass image lk (which is\nexplicitly added to hk or \u02dchk before the \ufb01rst convolution layer), and predicts if the image was real or\ngenerated. At the \ufb01nal scale of the pyramid, the low frequency residual is suf\ufb01ciently small that it\ncan be directly modeled with a standard GAN: \u02dchK = GK(zK) and DK only has hK or \u02dchK as input.\nThe framework is illustrated in Fig. 2.\nBreaking the generation into successive re\ufb01nements is the key idea in this work. Note that we give\nup any \u201cglobal\u201d notion of \ufb01delity; we never make any attempt to train a network to discriminate\nbetween the output of a cascade and a real image and instead focus on making each step plausible.\nFurthermore, the independent training of each pyramid level has the advantage that it is far more\ndif\ufb01cult for the model to memorize training examples \u2013 a hazard when high capacity deep networks\nare used.\nAs described, our model is trained in an unsupervised manner. However, we also explore variants\nthat utilize class labels. This is done by add a 1-hot vector c, indicating class identity, as another\nconditioning variable for Gk and Dk.\n3\nModel Architecture & Training\nWe apply our approach to three datasets: (i) CIFAR10 \u2013 32\u00d732 pixel color images of 10 different\nclasses, 100k training samples with tight crops of objects; (ii) STL \u2013 96\u00d796 pixel color images of\n10 different classes, 100k training samples (we use the unlabeled portion of data); and (iii) LSUN\n[30] \u2013 \u223c10M images of 10 different natural scene types, downsampled to 64\u00d764 pixels.\nFor each dataset, we explored a variety of architectures for {Gk, Dk}. We now detail the best\nperforming models, selected using a combination of log-likelihood and visual appearance of the\nsamples. Complete Torch speci\ufb01cation \ufb01les for all models are provided in supplementary material\n[4]. For all models, the noise vector zk is drawn from a uniform [-1,1] distribution.\n4\n3.1\nCIFAR10 and STL\nInitial scale: This operates at 8 \u00d7 8 resolution, using densely connected nets for both GK & DK\nwith 2 hidden layers and ReLU non-linearities. DK uses Dropout and has 600 units/layer vs 1200\nfor GK. zK is a 100-d vector.\nSubsequent scales: For CIFAR10, we boost the training set size by taking four 28 \u00d7 28 crops from\nthe original images. Thus the two subsequent levels of the pyramid are 8 \u219214 and 14 \u219228. For\nSTL, we have 4 levels going from 8 \u219216 \u219232 \u219264 \u219296. For both datasets, Gk & Dk are\nconvnets with 3 and 2 layers, respectively (see [4]). The noise input zk to Gk is presented as a 4th\n\u201ccolor plane\u201d to low-pass lk, hence its dimensionality varies with the pyramid level. For CIFAR10,\nwe also explore a class conditional version of the model, where a vector c encodes the label. This is\nintegrated into Gk & Dk by passing it through a linear layer whose output is reshaped into a single\nplane feature map which is then concatenated with the 1st layer maps. The loss in Eqn. 2 is trained\nusing SGD with an initial learning rate of 0.02, decreased by a factor of (1 + 4 \u00d7 10\u22125) at each\nepoch. Momentum starts at 0.5, increasing by 0.0008 at epoch up to a maximum of 0.8. During\ntraining, we monitor log-likelihood using a Parzen-window estimator and retain the best performing\nmodel. Training time depends on the models size and pyramid level, with smaller models taking\nhours to train and larger models taking several days.\n3.2\nLSUN\nThe larger size of this dataset allows us to train a separate LAPGAN model for each the 10 different\nscene classes. During evaluation, so that we may understand the variation captured by our models,\nwe commence the sampling process with validation set images\u2020, downsampled to 4 \u00d7 4 resolution.\nThe four subsequent scales 4 \u21928 \u219216 \u219232 \u219264 use a common architecture for Gk & Dk at\neach level. Gk is a 5-layer convnet with {64, 368, 128, 224} feature maps and a linear output layer.\n7 \u00d7 7 \ufb01lters, ReLUs, batch normalization [14] and Dropout are used at each hidden layer. Dk has\n3 hidden layers with {48, 448, 416} maps plus a sigmoid output. See [4] for full details. Note that\nGk and Dk are substantially larger than those used for CIFAR10 and STL, as afforded by the larger\ntraining set.\n4\nExperiments\nWe evaluate our approach using 3 different methods: (i) computation of log-likelihood on a held\nout image set; (ii) drawing sample images from the model and (iii) a human subject experiment that\ncompares (a) our samples, (b) those of baseline methods and (c) real images.\n4.1\nEvaluation of Log-Likelihood\nA traditional method for evaluating generative models is to measure their log-likelihood on a held\nout set of images. But, like the original GAN method [10], our approach does not have a direct\nway of computing the probability of an image. Goodfellow et al. [10] propose using a Gaussian\nParzen window estimate to compute log-likelihoods. Despite showing poor performance in high\ndimensional spaces, this approach is the best one available for estimating likelihoods of models\nlacking an explicitly represented density function.\nOur LAPGAN model allows for an alternative method of estimating log-likelihood that exploits the\nmulti-scale structure of the model. This new approach uses a Gaussian Parzen window estimate to\ncompute a probability at each scale of the Laplacian pyramid. We use this procedure, described in\ndetail in Appendix A, to compute the log-likelihoods for CIFAR10 and STL images (both at 32\u00d732\nresolution). The parameter \u03c3 (controlling the Parzen window size) was chosen using the validation\nset. We also compute the Parzen window based log-likelihood estimates of the standard GAN [10]\nmodel, using 50k samples for both the CIFAR10 and STL estimates. Table 1 shows our model\nachieving a signi\ufb01cantly higher log-likelihood on both datasets. Comparisons to further approaches,\nnotably [26], are problematic due to different normalizations used on the data.\n4.2\nModel Samples\nWe show samples from models trained on CIFAR10, STL and LSUN datasets. Additional samples\ncan be found in the supplementary material [4].\n\u2020These were not used in any way during training.\n5\nModel\nCIFAR10\nSTL (@32\u00d732)\nGAN [10]\n-3617 \u00b1 353\n-3661 \u00b1 347\nLAPGAN\n-1799 \u00b1 826\n-2906 \u00b1 728\nTable 1: Parzen window based log-likelihood estimates for a standard GAN, our proposed LAPGAN\nmodel on CIFAR10 and STL datasets.\nFig. 3 shows samples from our models trained on CIFAR10. Samples from the class conditional\nLAPGAN are organized by class. Our reimplementation of the standard GAN model [10] produces\nslightly sharper images than those shown in the original paper. We attribute this improvement to\nthe introduction of data augmentation. The LAPGAN samples improve upon the standard GAN\nsamples. They appear more object-like and have more clearly de\ufb01ned edges. Conditioning on a\nclass label improves the generations as evidenced by the clear object structure in the conditional\nLAPGAN samples. The quality of these samples compares favorably with those from the DRAW\nmodel of Gregor et al. [11] and also Sohl-Dickstein et al. [26]. The rightmost column of each image\nshows the nearest training example to the neighboring sample (in L2 pixel-space). This demonstrates\nthat our model is not simply copying the input examples.\nFig. 4(a) shows samples from our LAPGAN model trained on STL. Here, we lose clear object shape\nbut the samples remain sharp. Fig. 4(b) shows the generation chain for random STL samples.\nFig. 5 shows samples from LAPGAN models trained on three LSUN categories (tower, bedroom,\nchurch front). The 4 \u00d7 4 validation image used to start the generation process is shown in the \ufb01rst\ncolumn, along with 10 different 64 \u00d7 64 samples, which illustrate the inherent variation captured\nby the model. Collectively, these show the models capturing long-range structure within the scenes,\nbeing able to recompose scene elements into credible looking images. To the best of our knowledge,\nno other generative model has been able to produce samples of this complexity. The substantial\ngain in quality over the CIFAR10 and STL samples is likely due to the much larger training LSUN\ntraining set which allowed us to train bigger and deeper models.\n4.3\nHuman Evaluation of Samples\nTo obtain a quantitative measure of quality of our samples, we asked 15 volunteers to participate\nin an experiment to see if they could distinguish our samples from real images. The subjects were\npresented with the user interface shown in Fig. 6(right) and shown at random four different types\nof image: samples drawn from three different GAN models trained on CIFAR10 ((i) LAPGAN, (ii)\nclass conditional LAPGAN and (iii) standard GAN [10]) and also real CIFAR10 images. After being\npresented with the image, the subject clicked the appropriate button to indicate if they believed the\nimage was real or generated. Since accuracy is a function of viewing time, we also randomly pick the\npresentation time from one of 11 durations ranging from 50ms to 2000ms, after which a gray mask\nimage is displayed. Before the experiment commenced, they were shown examples of real images\nfrom CIFAR10. After collecting \u223c10k samples from the volunteers, we plot in Fig. 6 the fraction of\nimages believed to be real for the four different data sources, as a function of presentation time. The\ncurves show our models produce samples that are far more realistic than those from standard GAN\n[10].\n5\nDiscussion\nBy modifying the approach in [10] to better respect the structure of images, we have proposed a\nconceptually simple generative model that is able to produce high-quality sample images that are\nboth qualitatively and quantitatively better than other deep generative modeling approaches. A key\npoint in our work is giving up any \u201cglobal\u201d notion of \ufb01delity, and instead breaking the generation\ninto plausible successive re\ufb01nements. We note that many other signal modalities have a multiscale\nstructure that may bene\ufb01t from a similar approach.\n6\nCC-LAPGAN: Airplane \nCC-LAPGAN: Automobile \nCC-LAPGAN: Bird \nCC-LAPGAN: Cat \nCC-LAPGAN: Deer \nCC-LAPGAN: Dog \nCC-LAPGAN: Frog \nCC-LAPGAN: Horse \nCC-LAPGAN: Ship \nCC-LAPGAN: Truck \nGAN [14] \nLAPGAN \nFigure 3: CIFAR10 samples: our class conditional CC-LAPGAN model, our LAPGAN model and\nthe standard GAN model of Goodfellow [10]. The yellow column shows the training set nearest\nneighbors of the samples in the adjacent column.\n(a)\n(b)\nFigure 4: STL samples: (a) Random 96x96 samples from our LAPGAN model. (b) Coarse-to-\ufb01ne\ngeneration chain.\n7\nFigure 5: 64 \u00d7 64 samples from three different LSUN LAPGAN models (top: tower, middle: bed-\nroom, bottom: church front). The \ufb01rst column shows the 4 \u00d7 4 validation set image used to start the\ngeneration process, with subsequent columns showing different draws from the model.\n8\n  50\n  75\n 100\n 150\n 200\n 300\n 400\n 650\n1000\n2000\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90\n100\nPresentation time (ms)\n% classified real\n \n \nReal\nCC\u2212LAPGAN\nLAPGAN\nGAN\nFigure 6: Left: Human evaluation of real CIFAR10 images (red) and samples from Goodfellow\net al. [10] (magenta), our LAPGAN (blue) and a class conditional LAPGAN (green). The error\nbars show \u00b11\u03c3 of the inter-subject variability. Around 40% of the samples generated by our class\nconditional LAPGAN model are realistic enough to fool a human into thinking they are real images.\nThis compares with \u226410% of images from the standard GAN model [10], but is still a lot lower\nthan the > 90% rate for real images. Right: The user-interface presented to the subjects.\nAppendix A\nTo describe the log-likelihood computation in our model, let us consider a two scale pyramid for\nthe moment. Given a (vectorized) j \u00d7 j image I, denote by l = d(I) the coarsened image, and\nh = I \u2212u(d(I)) to be the high pass. In this section, to simplify the computations, we use a slightly\ndifferent u operator than the one used to generate the images displayed in Fig. 3. Namely, here we\ntake d(I) to be the mean over each disjoint block of 2 \u00d7 2 pixels, and take u to be the operator that\nremoves the mean from each 2 \u00d7 2 block. Since u has rank 3d2/4, in this section, we write h in an\northonormal basis of the range of u, then the (linear) mapping from I to (l, h) is unitary. We now\nbuild a probability density p on Rd2 by\np(I) = q0(l, h)q1(l) = q0(d(I), h(I))q1(d(I));\nin a moment we will carefully de\ufb01ne the functions qi. For now, suppose that qi \u22650,\nR\nq1(l) dl = 1,\nand for each \ufb01xed l,\nR\nq0(l, h) dh = 1. Then we can check that p has unit integral:\nZ\np dI =\nZ\nq0(d(I), h(I))q1(d(I))dI =\nZ Z\nq0(l, h)q1(l) dl dh = 1.\nNow we de\ufb01ne the qi with Parzen window approximations to the densities of each of the scales.\nFor q1, we take a set of training samples l1, ...., lN0, and construct the density function q1(l) \u223c\nPN1\ni=1 e||l\u2212li||2/\u03c31. We \ufb01x l = d(I) to de\ufb01ne q0(I) = q0(l, h) \u223cPN0\ni=1 e||h\u2212hi||2/\u03c30.For pyramids\nwith more levels, we continue in the same way for each of the \ufb01ner scales. Note we always use the\ntrue low pass at each scale, and measure the true high pass against the high pass samples generated\nfrom the model. Thus for a pyramid with K levels, the \ufb01nal log likelihood will be: log(qK(lK)) +\nPK\u22121\nk=0 log(qk(lk, hk)).\n9\nReferences\n[1] P. J. Burt, Edward, and E. H. Adelson. The laplacian pyramid as a compact image code. IEEE Transac-\ntions on Communications, 31:532\u2013540, 1983.\n[2] J. S. De Bonet. Multiresolution sampling procedure for analysis and synthesis of texture images. In\nProceedings of the 24th annual conference on Computer graphics and interactive techniques, pages 361\u2013\n368. ACM Press/Addison-Wesley Publishing Co., 1997.\n[3] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. Imagenet: A large-scale hierarchical image\ndatabase. In CVPR, pages 248\u2013255. IEEE, 2009.\n[4] E. Denton, S. Chintala, A. Szlam, and R. Fergus. Deep generative image models using a laplacian pyramid\nof adversarial networks: Supplementary material. http://soumith.ch/eyescream.\n[5] A. Dosovitskiy, J. T. Springenberg, and T. Brox. Learning to generate chairs with convolutional neural\nnetworks. arXiv preprint arXiv:1411.5928, 2014.\n[6] A. A. Efros and T. K. Leung. Texture synthesis by non-parametric sampling. In ICCV, volume 2, pages\n1033\u20131038. IEEE, 1999.\n[7] S. A. Eslami, N. Heess, C. K. Williams, and J. Winn. The shape boltzmann machine: a strong model of\nobject shape. International Journal of Computer Vision, 107(2):155\u2013176, 2014.\n[8] W. T. Freeman, T. R. Jones, and E. C. Pasztor. Example-based super-resolution. Computer Graphics and\nApplications, IEEE, 22(2):56\u201365, 2002.\n[9] J. Gauthier. Conditional generative adversarial nets for convolutional face generation. Class Project for\nStanford CS231N: Convolutional Neural Networks for Visual Recognition, Winter semester 2014 2014.\n[10] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Ben-\ngio. Generative adversarial nets. In NIPS, pages 2672\u20132680. 2014.\n[11] K. Gregor, I. Danihelka, A. Graves, and D. Wierstra. DRAW: A recurrent neural network for image\ngeneration. CoRR, abs/1502.04623, 2015.\n[12] J. Hays and A. A. Efros. Scene completion using millions of photographs. ACM Transactions on Graphics\n(TOG), 26(3):4, 2007.\n[13] G. E. Hinton and R. R. Salakhutdinov. Reducing the dimensionality of data with neural networks. Science,\n313(5786):504\u2013507, 2006.\n[14] S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep network training by reducing internal\ncovariate shift. arXiv preprint arXiv:1502.03167v3, 2015.\n[15] D. P. Kingma and M. Welling. Auto-encoding variational bayes. ICLR, 2014.\n[16] A. Krizhevsky, G. E. Hinton, et al. Factored 3-way restricted boltzmann machines for modeling natural\nimages. In AISTATS, pages 621\u2013628, 2010.\n[17] M. Mirza and S. Osindero. Conditional generative adversarial nets. CoRR, abs/1411.1784, 2014.\n[18] B. A. Olshausen and D. J. Field. Sparse coding with an overcomplete basis set: A strategy employed by\nv1? Vision research, 37(23):3311\u20133325, 1997.\n[19] S. Osindero and G. E. Hinton. Modeling image patches with a directed hierarchy of markov random\n\ufb01elds. In J. Platt, D. Koller, Y. Singer, and S. Roweis, editors, NIPS, pages 1121\u20131128. 2008.\n[20] J. Portilla and E. P. Simoncelli. A parametric texture model based on joint statistics of complex wavelet\ncoef\ufb01cients. International Journal of Computer Vision, 40(1):49\u201370, 2000.\n[21] M. Ranzato, V. Mnih, J. M. Susskind, and G. E. Hinton. Modeling natural images using gated MRFs.\nIEEE Transactions on Pattern Analysis & Machine Intelligence, (9):2206\u20132222, 2013.\n[22] D. J. Rezende, S. Mohamed, and D. Wierstra. Stochastic backpropagation and variational inference in\ndeep latent gaussian models. arXiv preprint arXiv:1401.4082, 2014.\n[23] S. Roth and M. J. Black. Fields of experts: A framework for learning image priors. In In CVPR, pages\n860\u2013867, 2005.\n[24] R. Salakhutdinov and G. E. Hinton. Deep boltzmann machines. In AISTATS, pages 448\u2013455, 2009.\n[25] E. P. Simoncelli, W. T. Freeman, E. H. Adelson, and D. J. Heeger.\nShiftable multiscale transforms.\nInformation Theory, IEEE Transactions on, 38(2):587\u2013607, 1992.\n[26] J. Sohl-Dickstein, E. A. Weiss, N. Maheswaranathan, and S. Ganguli. Deep unsupervised learning using\nnonequilibrium thermodynamics. CoRR, abs/1503.03585, 2015.\n[27] L. Theis and M. Bethge. Generative image modeling using spatial LSTMs. Dec 2015.\n[28] P. Vincent, H. Larochelle, Y. Bengio, and P.-A. Manzagol. Extracting and composing robust features with\ndenoising autoencoders. In ICML, pages 1096\u20131103, 2008.\n[29] J. Wright, Y. Ma, J. Mairal, G. Sapiro, T. S. Huang, and S. Yan. Sparse representation for computer vision\nand pattern recognition. Proceedings of the IEEE, 98(6):1031\u20131044, 2010.\n[30] Y. Zhang, F. Yu, S. Song, P. Xu, A. Seff, and J. Xiao. Large-scale scene understanding challenge. In\nCVPR Workshop, 2015.\n[31] S. C. Zhu, Y. Wu, and D. Mumford. Filters, random \ufb01elds and maximum entropy (frame): Towards a\nuni\ufb01ed theory for texture modeling. International Journal of Computer Vision, 27(2):107\u2013126, 1998.\n[32] D. Zoran and Y. Weiss. From learning models of natural image patches to whole image restoration. In\nICCV, 2011.\n10\n",
        "sentence": " Among those approaches, generative adversarial networks (GANs) (Goodfellow et al., 2014; Denton et al., 2015; Donahue et al., 2016) share another similarity with our approach, namely they are explicitly minimizing a discriminative loss to learn their features. , 2014; Denton et al., 2015; Donahue et al., 2016) share another similarity with our approach, namely they are explicitly minimizing a discriminative loss to learn their features. While these models cannot learn an inverse mapping, Donahue et al. (2016) recently proposed to add an encoder to extract visual features from GANs. Recently, a lot of work has explored self-supervison: leveraging supervision contained in the input signal (Doersch et al., 2015; Noroozi & Favaro, 2016; Pathak et al., 2016). In the same vein as word2vec (Mikolov et al., 2013), Doersch et al. (2015) show that spatial context is a strong signal to learn visual features. Recently, a lot of work has explored self-supervison: leveraging supervision contained in the input signal (Doersch et al., 2015; Noroozi & Favaro, 2016; Pathak et al., 2016). In the same vein as word2vec (Mikolov et al., 2013), Doersch et al. (2015) show that spatial context is a strong signal to learn visual features. Noroozi & Favaro (2016) have further extended this work.",
        "context": "Generative Adversarial Networks\nThe GAN approach [10] is a framework for training generative models, which we brie\ufb02y explain in\nthe context of image data. The method pits two networks against one another: a generative model G\nsolely on frontal face images. Our approach also uses several forms of conditional GAN model but\nis much more ambitious in its scope.\n2\nApproach\nThe basic building block of our approach is the generative adversarial network (GAN) of Goodfellow\na coarse-to-\ufb01ne fashion. At each level of the pyramid, a separate generative con-\nvnet model is trained using the Generative Adversarial Nets (GAN) approach [10].\nSamples drawn from our model are of signi\ufb01cantly higher quality than alternate"
    },
    {
        "title": "Unsupervised visual representation learning by context prediction",
        "author": [
            "C. Doersch",
            "A. Gupta",
            "A. Efros"
        ],
        "venue": "In CVPR,",
        "citeRegEx": "Doersch et al\\.,? \\Q2015\\E",
        "shortCiteRegEx": "Doersch et al\\.",
        "year": 2015,
        "abstract": "This work explores the use of spatial context as a source of free and\nplentiful supervisory signal for training a rich visual representation. Given\nonly a large, unlabeled image collection, we extract random pairs of patches\nfrom each image and train a convolutional neural net to predict the position of\nthe second patch relative to the first. We argue that doing well on this task\nrequires the model to learn to recognize objects and their parts. We\ndemonstrate that the feature representation learned using this within-image\ncontext indeed captures visual similarity across images. For example, this\nrepresentation allows us to perform unsupervised visual discovery of objects\nlike cats, people, and even birds from the Pascal VOC 2011 detection dataset.\nFurthermore, we show that the learned ConvNet can be used in the R-CNN\nframework and provides a significant boost over a randomly-initialized ConvNet,\nresulting in state-of-the-art performance among algorithms which use only\nPascal-provided training set annotations.",
        "full_text": "Unsupervised Visual Representation Learning by Context Prediction\nCarl Doersch1,2\nAbhinav Gupta1\nAlexei A. Efros2\n1 School of Computer Science\n2 Dept. of Electrical Engineering and Computer Science\nCarnegie Mellon University\nUniversity of California, Berkeley\nAbstract\nThis work explores the use of spatial context as a source\nof free and plentiful supervisory signal for training a rich\nvisual representation. Given only a large, unlabeled image\ncollection, we extract random pairs of patches from each\nimage and train a convolutional neural net to predict the po-\nsition of the second patch relative to the \ufb01rst. We argue that\ndoing well on this task requires the model to learn to recog-\nnize objects and their parts. We demonstrate that the fea-\nture representation learned using this within-image context\nindeed captures visual similarity across images. For exam-\nple, this representation allows us to perform unsupervised\nvisual discovery of objects like cats, people, and even birds\nfrom the Pascal VOC 2011 detection dataset. Furthermore,\nwe show that the learned ConvNet can be used in the R-\nCNN framework [21] and provides a signi\ufb01cant boost over\na randomly-initialized ConvNet, resulting in state-of-the-\nart performance among algorithms which use only Pascal-\nprovided training set annotations.\n1. Introduction\nRecently, new computer vision methods have leveraged\nlarge datasets of millions of labeled examples to learn rich,\nhigh-performance visual representations [32]. Yet efforts\nto scale these methods to truly Internet-scale datasets (i.e.\nhundreds of billions of images) are hampered by the sheer\nexpense of the human annotation required. A natural way\nto address this dif\ufb01culty would be to employ unsupervised\nlearning, which aims to use data without any annotation.\nUnfortunately, despite several decades of sustained effort,\nunsupervised methods have not yet been shown to extract\nuseful information from large collections of full-sized, real\nimages. After all, without labels, it is not even clear what\nshould be represented.\nHow can one write an objective\nfunction to encourage a representation to capture, for ex-\nample, objects, if none of the objects are labeled?\nInterestingly, in the text domain, context has proven to\nbe a powerful source of automatic supervisory signal for\nlearning representations [3, 41, 9, 40]. Given a large text\ncorpus, the idea is to train a model that maps each word\nto a feature vector, such that it is easy to predict the words\n_ \n_ \n? \n? \nExample: \nQuestion 1: \nQuestion 2: \nFigure 1. Our task for learning patch representations involves ran-\ndomly sampling a patch (blue) and then one of eight possible\nneighbors (red). Can you guess the spatial con\ufb01guration for the\ntwo pairs of patches? Note that the task is much easier once you\nhave recognized the object!\nAnswer key: Q1: Bottom right Q2: Top center\nin the context (i.e., a few words before and/or after) given\nthe vector. This converts an apparently unsupervised prob-\nlem (\ufb01nding a good similarity metric between words) into\na \u201cself-supervised\u201d one: learning a function from a given\nword to the words surrounding it. Here the context predic-\ntion task is just a \u201cpretext\u201d to force the model to learn a\ngood word embedding, which, in turn, has been shown to\nbe useful in a number of real tasks, such as semantic word\nsimilarity [40].\nOur paper aims to provide a similar \u201cself-supervised\u201d\nformulation for image data: a supervised task involving pre-\ndicting the context for a patch. Our task is illustrated in Fig-\nures 1 and 2. We sample random pairs of patches in one of\neight spatial con\ufb01gurations, and present each pair to a ma-\nchine learner, providing no information about the patches\u2019\noriginal position within the image. The algorithm must then\nguess the position of one patch relative to the other. Our\nunderlying hypothesis is that doing well on this task re-\nquires understanding scenes and objects, i.e. a good visual\nrepresentation for this task will need to extract objects and\ntheir parts in order to reason about their relative spatial lo-\ncation. \u201cObjects,\u201d after all, consist of multiple parts that\ncan be detected independently of one another, and which\n1\narXiv:1505.05192v3  [cs.CV]  16 Jan 2016\noccur in a speci\ufb01c spatial con\ufb01guration (if there is no spe-\nci\ufb01c con\ufb01guration of the parts, then it is \u201cstuff\u201d [1]). We\npresent a ConvNet-based approach to learn a visual repre-\nsentation from this task. We demonstrate that the resulting\nvisual representation is good for both object detection, pro-\nviding a signi\ufb01cant boost on PASCAL VOC 2007 compared\nto learning from scratch, as well as for unsupervised object\ndiscovery / visual data mining. This means, surprisingly,\nthat our representation generalizes across images, despite\nbeing trained using an objective function that operates on a\nsingle image at a time. That is, instance-level supervision\nappears to improve performance on category-level tasks.\n2. Related Work\nOne way to think of a good image representation is as\nthe latent variables of an appropriate generative model. An\nideal generative model of natural images would both gener-\nate images according to their natural distribution, and be\nconcise in the sense that it would seek common causes\nfor different images and share information between them.\nHowever, inferring the latent structure given an image is in-\ntractable for even relatively simple models. To deal with\nthese computational issues, a number of works, such as\nthe wake-sleep algorithm [25], contrastive divergence [24],\ndeep Boltzmann machines [48], and variational Bayesian\nmethods [30, 46] use sampling to perform approximate in-\nference.\nGenerative models have shown promising per-\nformance on smaller datasets such as handwritten dig-\nits [25, 24, 48, 30, 46], but none have proven effective for\nhigh-resolution natural images.\nUnsupervised representation learning can also be formu-\nlated as learning an embedding (i.e. a feature vector for\neach image) where images that are semantically similar are\nclose, while semantically different ones are far apart. One\nway to build such a representation is to create a supervised\n\u201cpretext\u201d task such that an embedding which solves the task\nwill also be useful for other real-world tasks. For exam-\nple, denoising autoencoders [56, 4] use reconstruction from\nnoisy data as a pretext task: the algorithm must connect\nimages to other images with similar objects to tell the dif-\nference between noise and signal. Sparse autoencoders also\nuse reconstruction as a pretext task, along with a sparsity\npenalty [42], and such autoencoders may be stacked to form\na deep representation [35, 34]. (however, only [34] was suc-\ncessfully applied to full-sized images, requiring a million\nCPU hours to discover just three objects). We believe that\ncurrent reconstruction-based algorithms struggle with low-\nlevel phenomena, like stochastic textures, making it hard to\neven measure whether a model is generating well.\nAnother pretext task is \u201ccontext prediction.\u201d A strong\ntradition for this kind of task already exists in the text do-\nmain, where \u201cskip-gram\u201d [40] models have been shown to\ngenerate useful word representations. The idea is to train a\n3 \n2 \n1 \n5 \n4 \n8 \n7 \n6 \n); Y = 3 \n, \nX = ( \nFigure 2. The algorithm receives two patches in one of these eight\npossible spatial arrangements, without any context, and must then\nclassify which con\ufb01guration was sampled.\nmodel (e.g. a deep network) to predict, from a single word,\nthe n preceding and n succeeding words. In principle, sim-\nilar reasoning could be applied in the image domain, a kind\nof visual \u201c\ufb01ll in the blank\u201d task, but, again, one runs into the\nproblem of determining whether the predictions themselves\nare correct [12], unless one cares about predicting only very\nlow-level features [14, 33, 53]. To address this, [39] predicts\nthe appearance of an image region by consensus voting of\nthe transitive nearest neighbors of its surrounding regions.\nOur previous work [12] explicitly formulates a statistical\ntest to determine whether the data is better explained by a\nprediction or by a low-level null hypothesis model.\nThe key problem that these approaches must address is\nthat predicting pixels is much harder than predicting words,\ndue to the huge variety of pixels that can arise from the same\nsemantic object. In the text domain, one interesting idea is\nto switch from a pure prediction task to a discrimination\ntask [41, 9]. In this case, the pretext task is to discriminate\ntrue snippets of text from the same snippets where a word\nhas been replaced at random. A direct extension of this to\n2D might be to discriminate between real images vs. im-\nages where one patch has been replaced by a random patch\nfrom elsewhere in the dataset. However, such a task would\nbe trivial, since discriminating low-level color statistics and\nlighting would be enough. To make the task harder and\nmore high-level, in this paper, we instead classify between\nmultiple possible con\ufb01gurations of patches sampled from\nthe same image, which means they will share lighting and\ncolor statistics, as shown on Figure 2.\nAnother line of work in unsupervised learning from im-\nages aims to discover object categories using hand-crafted\nfeatures and various forms of clustering (e.g. [51, 47]\nlearned a generative model over bags of visual words). Such\nrepresentations lose shape information, and will readily dis-\n2\ncover clusters of, say, foliage. A few subsequent works have\nattempted to use representations more closely tied to shape\n[36, 43], but relied on contour extraction, which is dif\ufb01cult\nin complex images. Many other approaches [22, 29, 16]\nfocus on de\ufb01ning similarity metrics which can be used in\nmore standard clustering algorithms;\n[45], for instance,\nre-casts the problem as frequent itemset mining. Geom-\netry may also be used to for verifying links between im-\nages [44, 6, 23], although this can fail for deformable ob-\njects.\nVideo can provide another cue for representation learn-\ning. For most scenes, the identity of objects remains un-\nchanged even as appearance changes with time. This kind\nof temporal coherence has a long history in visual learning\nliterature [18, 59], and contemporaneous work shows strong\nimprovements on modern detection datasets [57].\nFinally, our work is related to a line of research on dis-\ncriminative patch mining [13, 50, 28, 37, 52, 11], which has\nemphasized weak supervision as a means of object discov-\nery. Like the current work, they emphasize the utility of\nlearning representations of patches (i.e. object parts) before\nlearning full objects and scenes, and argue that scene-level\nlabels can serve as a pretext task. For example, [13] trains\ndetectors to be sensitive to different geographic locales, but\nthe actual goal is to discover speci\ufb01c elements of architec-\ntural style.\n3. Learning Visual Context Prediction\nWe aim to learn an image representation for our pre-\ntext task, i.e., predicting the relative position of patches\nwithin an image. We employ Convolutional Neural Net-\nworks (ConvNets), which are well known to learn complex\nimage representations with minimal human feature design.\nBuilding a ConvNet that can predict a relative offset for a\npair of patches is, in principle, straightforward: the network\nmust feed the two input patches through several convolu-\ntion layers, and produce an output that assigns a probability\nto each of the eight spatial con\ufb01gurations (Figure 2) that\nmight have been sampled (i.e. a softmax output). Note,\nhowever, that we ultimately wish to learn a feature embed-\nding for individual patches, such that patches which are vi-\nsually similar (across different images) would be close in\nthe embedding space.\nTo achieve this, we use a late-fusion architecture shown\nin Figure 3: a pair of AlexNet-style architectures [32] that\nprocess each patch separately, until a depth analogous to\nfc6 in AlexNet, after which point the representations are\nfused. For the layers that process only one of the patches,\nweights are tied between both sides of the network, such\nthat the same fc6-level embedding function is computed for\nboth patches. Because there is limited capacity for joint\nreasoning\u2014i.e., only two layers receive input from both\npatches\u2014we expect the network to perform the bulk of the\nPatch 2 \nPatch 1 \npool1 (3x3,96,2) \npool1 (3x3,96,2) \nLRN1 \nLRN1 \npool2 (3x3,384,2) \npool2 (3x3,384,2) \nLRN2 \nLRN2 \nfc6 (4096) \nfc6 (4096) \nconv5 (3x3,256,1) \nconv5 (3x3,256,1) \nconv4 (3x3,384,1) \nconv4 (3x3,384,1) \nconv3 (3x3,384,1) \nconv3 (3x3,384,1) \nconv2 (5x5,384,2) \nconv2 (5x5,384,2) \nconv1 (11x11,96,4) \nconv1 (11x11,96,4) \nfc7 (4096) \nfc8 (4096) \nfc9 (8) \npool5 (3x3,256,2) \npool5 (3x3,256,2) \nFigure 3. Our architecture for pair classi\ufb01cation. Dotted lines in-\ndicate shared weights. \u2018conv\u2019 stands for a convolution layer, \u2018fc\u2019\nstands for a fully-connected one, \u2018pool\u2019 is a max-pooling layer, and\n\u2018LRN\u2019 is a local response normalization layer. Numbers in paren-\ntheses are kernel size, number of outputs, and stride (fc layers have\nonly a number of outputs). The LRN parameters follow [32]. All\nconv and fc layers are followed by ReLU nonlinearities, except fc9\nwhich feeds into a softmax classi\ufb01er.\nsemantic reasoning for each patch separately. When design-\ning the network, we followed AlexNet where possible.\nTo obtain training examples given an image, we sample\nthe \ufb01rst patch uniformly, without any reference to image\ncontent. Given the position of the \ufb01rst patch, we sample the\nsecond patch randomly from the eight possible neighboring\nlocations as in Figure 2.\n3.1. Avoiding \u201ctrivial\u201d solutions\nWhen designing a pretext task, care must be taken to en-\nsure that the task forces the network to extract the desired\ninformation (high-level semantics, in our case), without tak-\ning \u201ctrivial\u201d shortcuts.\nIn our case, low-level cues like\nboundary patterns or textures continuing between patches\ncould potentially serve as such a shortcut. Hence, for the\nrelative prediction task, it was important to include a gap\nbetween patches (in our case, approximately half the patch\nwidth). Even with the gap, it is possible that long lines span-\nning neighboring patches could could give away the correct\nanswer. Therefore, we also randomly jitter each patch loca-\ntion by up to 7 pixels (see Figure 2).\nHowever, even these precautions are not enough: we\nwere surprised to \ufb01nd that, for some images, another triv-\nial solution exists. We traced the problem to an unexpected\nculprit: chromatic aberration. Chromatic aberration arises\nfrom differences in the way the lens focuses light at differ-\nent wavelengths. In some cameras, one color channel (com-\nmonly green) is shrunk toward the image center relative to\nthe others [5, p. 76]. A ConvNet, it turns out, can learn to lo-\ncalize a patch relative to the lens itself (see Section 4.2) sim-\n3\nInput \nRandom Initialization \nImageNet AlexNet \nOurs \nFigure 4. Examples of patch clusters obtained by nearest neighbors. The query patch is shown on the far left. Matches are for three different\nfeatures: fc6 features from a random initialization of our architecture, AlexNet fc7 after training on labeled ImageNet, and the fc6 features\nlearned from our method. Queries were chosen from 1000 randomly-sampled patches. The top group is examples where our algorithm\nperforms well; for the middle AlexNet outperforms our approach; and for the bottom all three features work well.\nply by detecting the separation between green and magenta\n(red + blue). Once the network learns the absolute location\non the lens, solving the relative location task becomes triv-\nial. To deal with this problem, we experimented with two\ntypes of pre-processing. One is to shift green and magenta\ntoward gray (\u2018projection\u2019). Speci\ufb01cally, let a = [\u22121, 2, \u22121]\n(the \u2019green-magenta color axis\u2019 in RGB space). We then\nde\ufb01ne B = I \u2212aT a/(aaT ), which is a matrix that sub-\ntracts the projection of a color onto the green-magenta color\naxis. We multiply every pixel value by B. An alternative ap-\nproach is to randomly drop 2 of the 3 color channels from\neach patch (\u2018color dropping\u2019), replacing the dropped colors\nwith Gaussian noise (standard deviation \u223c1/100 the stan-\ndard deviation of the remaining channel). For qualitative\nresults, we show the \u2018color-dropping\u2019 approach, but found\nboth performed similarly; for the object detection results,\nwe show both results.\nImplementation Details: We use Caffe [27], and train on\nthe ImageNet [10] 2012 training set ( 1.3M images), using\nonly the images and discarding the labels. First, we resize\neach image to between 150K and 450K total pixels, preserv-\ning the aspect-ratio. From these images, we sample patches\nat resolution 96-by-96. For computational ef\ufb01ciency, we\nonly sample the patches from a grid like pattern, such that\neach sampled patch can participate in as many as 8 separate\npairings. We allow a gap of 48 pixels between the sampled\npatches in the grid, but also jitter the location of each patch\nin the grid by \u22127 to 7 pixels in each direction. We prepro-\ncess patches by (1) mean subtraction (2) projecting or drop-\nping colors (see above), and (3) randomly downsampling\nsome patches to as little as 100 total pixels, and then upsam-\npling it, to build robustness to pixelation. When applying\n4\nInitial layout, with sampled patches in red \nImage layout \nis discarded We can recover image layout automatically Cannot recover layout with color removed \nFigure 5. We trained a network to predict the absolute (x, y) coordinates of randomly sampled patches. Far left: input image. Center left:\nextracted patches. Center right: the location the trained network predicts for each patch shown on the left. Far right: the same result after\nour color projection scheme. Note that the far right patches are shown after color projection; the operation\u2019s effect is almost unnoticeable.\nsimple SGD to train the network, we found that the network\npredictions would degenerate to a uniform prediction over\nthe 8 categories, with all activations for fc6 and fc7 col-\nlapsing to 0. This meant that the optimization became per-\nmanently stuck in a saddle point where it ignored the input\nfrom the lower layers (which helped minimize the variance\nof the \ufb01nal output), and therefore that the net could not tune\nthe lower-level features and escape the saddle point. Hence,\nour \ufb01nal implementation employs batch normalization [26],\nwithout the scale and shift (\u03b3 and \u03b2), which forces the net-\nwork activations to vary across examples. We also \ufb01nd that\nhigh momentum values (e.g. .999) accelerated learning. For\nexperiments, we use a ConvNet trained on a K40 GPU for\napproximately four weeks.\n4. Experiments\nWe \ufb01rst demonstrate the network has learned to associate\nsemantically similar patches, using simple nearest-neighbor\nmatching. We then apply the trained network in two do-\nmains. First, we use the model as \u201cpre-training\u201d for a stan-\ndard vision task with only limited training data: speci\ufb01cally,\nwe use the VOC 2007 object detection. Second, we evalu-\nate visual data mining, where the goal is to start with an\nunlabeled image collection and discover object classes. Fi-\nnally, we analyze the performance on the layout prediction\n\u201cpretext task\u201d to see how much is left to learn from this su-\npervisory signal.\n4.1. Nearest Neighbors\nRecall our intuition that training should assign similar\nrepresentations to semantically similar patches. In this sec-\ntion, our goal is to understand which patches our network\nconsiders similar. We begin by sampling random 96x96\npatches, which we represent using fc6 features (i.e. we re-\nmove fc7 and higher shown in Figure 3, and use only one\nof the two stacks). We \ufb01nd nearest neighbors using normal-\nized correlation of these features. Results for some patches\n(selected out of 1000 random queries) are shown in Fig-\nure 4. For comparison, we repeated the experiment using\nfc7 features from AlexNet trained on ImageNet (obtained\nby upsampling the patches), and using fc6 features from our\narchitecture but without any training (random weights ini-\npool5 \nconv6 (3x3,4096,1) \nconv6b (1x1,1024,1) \nfc7 (4096) \n\u2026 \nImage (227x227) \nfc8 (21) \npool6 (3x3,1024,2) \nFigure 6. Our architecture for Pascal\nVOC detection. Layers from conv1\nthrough pool5 are copied from our\npatch-based network (Figure 3). The\nnew \u2019conv6\u2019 layer is created by con-\nverting the fc6 layer into a convolu-\ntion layer. Kernel sizes, output units,\nand stride are given in parentheses, as\nin Figure 3.\ntialization). As shown in Figure 4, the matches returned by\nour feature often capture the semantic information that we\nare after, matching AlexNet in terms of semantic content (in\nsome cases, e.g. the car wheel, our matches capture pose\nbetter). Interestingly, in a few cases, random (untrained)\nConvNet also does reasonably well.\n4.2. Aside: Learnability of Chromatic Aberration\nWe noticed in early nearest-neighbor experiments that\nsome patches retrieved match patches from the same ab-\nsolute location in the image, regardless of content, be-\ncause those patches displayed similar aberration. To further\ndemonstrate this phenomenon, we trained a network to pre-\ndict the absolute (x, y) coordinates of patches sampled from\nImageNet. While the overall accuracy of this regressor is\nnot very high, it does surprisingly well for some images:\nfor the top 10% of images, the average (root-mean-square)\nerror is .255, while chance performance (always predict-\ning the image center) yields a RMSE of .371. Figure 5\nshows one such result. Applying the proposed \u201cprojection\u201d\nscheme increases the error on the top 10% of images to .321.\n4.3. Object Detection\nPrevious work on the Pascal VOC challenge [15] has\nshown that pre-training on ImageNet (i.e., training a Con-\nvNet to solve the ImageNet challenge) and then \u201c\ufb01ne-\ntuning\u201d the network (i.e. re-training the ImageNet model\nfor PASCAL data) provides a substantial boost over training\non the Pascal training set alone [21, 2]. However, as far as\nwe are aware, no works have shown that unsupervised pre-\ntraining on images can provide such a performance boost,\nno matter how much data is used.\nSince we are already using a ConvNet, we adopt the cur-\n5\nVOC-2007 Test\naero\nbike\nbird\nboat\nbottle\nbus\ncar\ncat\nchair\ncow\ntable\ndog\nhorse\nmbike\nperson\nplant\nsheep\nsofa\ntrain\ntv\nmAP\nDPM-v5[17]\n33.2\n60.3\n10.2\n16.1\n27.3\n54.3\n58.2\n23.0\n20.0\n24.1\n26.7\n12.7\n58.1\n48.2\n43.2\n12.0\n21.1\n36.1\n46.0\n43.5\n33.7\n[8] w/o context\n52.6\n52.6\n19.2\n25.4\n18.7\n47.3\n56.9\n42.1\n16.6\n41.4\n41.9\n27.7\n47.9\n51.5\n29.9\n20.0\n41.1\n36.4\n48.6\n53.2\n38.5\nRegionlets[58]\n54.2\n52.0\n20.3\n24.0\n20.1\n55.5\n68.7\n42.6\n19.2\n44.2\n49.1\n26.6\n57.0\n54.5\n43.4\n16.4\n36.6\n37.7\n59.4\n52.3\n41.7\nScratch-R-CNN[2]\n49.9\n60.6\n24.7\n23.7\n20.3\n52.5\n64.8\n32.9\n20.4\n43.5\n34.2\n29.9\n49.0\n60.4\n47.5\n28.0\n42.3\n28.6\n51.2\n50.0\n40.7\nScratch-Ours\n52.6\n60.5\n23.8\n24.3\n18.1\n50.6\n65.9\n29.2\n19.5\n43.5\n35.2\n27.6\n46.5\n59.4\n46.5\n25.6\n42.4\n23.5\n50.0\n50.6\n39.8\nOurs-projection\n58.4\n62.8\n33.5\n27.7\n24.4\n58.5\n68.5\n41.2\n26.3\n49.5\n42.6\n37.3\n55.7\n62.5\n49.4\n29.0\n47.5\n28.4\n54.7\n56.8\n45.7\nOurs-color-dropping\n60.5\n66.5\n29.6\n28.5\n26.3\n56.1\n70.4\n44.8\n24.6\n45.5\n45.4\n35.1\n52.2\n60.2\n50.0\n28.1\n46.7\n42.6\n54.8\n58.6\n46.3\nOurs-Yahoo100m\n56.2\n63.9\n29.8\n27.8\n23.9\n57.4\n69.8\n35.6\n23.7\n47.4\n43.0\n29.5\n52.9\n62.0\n48.7\n28.4\n45.1\n33.6\n49.0\n55.5\n44.2\nImageNet-R-CNN[21]\n64.2\n69.7\n50\n41.9\n32.0\n62.6\n71.0\n60.7\n32.7\n58.5\n46.5\n56.1\n60.6\n66.8\n54.2\n31.5\n52.8\n48.9\n57.9\n64.7\n54.2\nK-means-rescale [31]\n55.7\n60.9\n27.9\n30.9\n12.0\n59.1\n63.7\n47.0\n21.4\n45.2\n55.8\n40.3\n67.5\n61.2\n48.3\n21.9\n32.8\n46.9\n61.6\n51.7\n45.6\nOurs-rescale [31]\n61.9\n63.3\n35.8\n32.6\n17.2\n68.0\n67.9\n54.8\n29.6\n52.4\n62.9\n51.3\n67.1\n64.3\n50.5\n24.4\n43.7\n54.9\n67.1\n52.7\n51.1\nImageNet-rescale [31]\n64.0\n69.6\n53.2\n44.4\n24.9\n65.7\n69.6\n69.2\n28.9\n63.6\n62.8\n63.9\n73.3\n64.6\n55.8\n25.7\n50.5\n55.4\n69.3\n56.4\n56.5\nVGG-K-means-rescale\n56.1\n58.6\n23.3\n25.7\n12.8\n57.8\n61.2\n45.2\n21.4\n47.1\n39.5\n35.6\n60.1\n61.4\n44.9\n17.3\n37.7\n33.2\n57.9\n51.2\n42.4\nVGG-Ours-rescale\n71.1\n72.4\n54.1\n48.2\n29.9\n75.2\n78.0\n71.9\n38.3\n60.5\n62.3\n68.1\n74.3\n74.2\n64.8\n32.6\n56.5\n66.4\n74.0\n60.3\n61.7\nVGG-ImageNet-rescale\n76.6\n79.6\n68.5\n57.4\n40.8\n79.9\n78.4\n85.4\n41.7\n77.0\n69.3\n80.1\n78.6\n74.6\n70.1\n37.5\n66.0\n67.5\n77.4\n64.9\n68.6\nTable 1. Mean Average Precision on VOC-2007.\nrent state-of-the-art R-CNN pipeline [21]. R-CNN works\non object proposals that have been resized to 227x227. Our\nalgorithm, however, is aimed at 96x96 patches. We \ufb01nd that\ndownsampling the proposals to 96x96 loses too much detail.\nInstead, we adopt the architecture shown in Figure 6. As\nabove, we use only one stack from Figure 3. Second, we re-\nsize the convolution layers to operate on inputs of 227x227.\nThis results in a pool5 that is 7x7 spatially, so we must con-\nvert the previous fc6 layer into a convolution layer (which\nwe call conv6) following [38]. Note our conv6 layer has\n4096 channels, where each unit connects to a 3x3 region\nof pool5. A conv layer with 4096 channels would be quite\nexpensive to connect directly to a 4096-dimensional fully-\nconnected layer. Hence, we add another layer after conv6\n(called conv6b), using a 1x1 kernel, which reduces the di-\nmensionality to 1024 channels (and adds a nonlinearity).\nFinally, we feed the outputs through a pooling layer to a\nfully connected layer (fc7) which in turn connects to a \ufb01-\nnal fc8 layer which feeds into the softmax. We \ufb01ne-tune\nthis network according to the procedure described in [21]\n(conv6b, fc7, and fc8 start with random weights), and use\nfc7 as the \ufb01nal representation. We do not use bounding-\nbox regression, and take the appropriate results from [21]\nand [2].\nTable 1 shows our results. Our architecture trained from\nscratch (random initialization) performs slightly worse than\nAlexNet trained from scratch. However, our pre-training\nmakes up for this, boosting the from-scratch number by\n6% MAP, and outperforms an AlexNet-style model trained\nfrom scratch on Pascal by over 5%. This puts us about 8%\nbehind the performance of R-CNN pre-trained with Ima-\ngeNet labels [21]. This is the best result we are aware of\non VOC 2007 without using labels outside the dataset. We\nran additional baselines initialized with batch normaliza-\ntion, but found they performed worse than the ones shown.\nTo understand the effect of various dataset biases [55],\nwe also performed a preliminary experiment pre-training\non a randomly-selected 2M subset of the Yahoo/Flickr 100-\nmillion Dataset [54], which was collected entirely automat-\nically. The performance after \ufb01ne-tuning is slightly worse\nthan Imagenet, but there is still a considerable boost over\nthe from-scratch model.\nIn the above \ufb01ne-tuning experiments, we removed the\nbatch normalization layers by estimating the mean and vari-\nance of the conv- and fc- layers, and then rescaling the\nweights and biases such that the outputs of the conv and fc\nlayers have mean 0 and variance 1 for each channel. Recent\nwork [31], however, has shown empirically that the scal-\ning of the weights prior to \ufb01netuning can have a strong im-\npact on test-time performance, and argues that our previous\nmethod of removing batch normalization leads too poorly\nscaled weights. They propose a simple way to rescale the\nnetwork\u2019s weights without changing the function that the\nnetwork computes, such that the network behaves better\nduring \ufb01netuning. Results using this technique are shown\nin Table 1. Their approach gives a boost to all methods, but\ngives less of a boost to the already-well-scaled ImageNet-\ncategory model. Note that for this comparison, we used\nfast-rcnn [20] to save compute time, and we discarded all\npre-trained fc-layers from our model, re-initializing them\nwith the K-means procedure of [31] (which was used to ini-\ntialize all layers in the \u201cK-means-rescale\u201d row). Hence, the\nstructure of the network during \ufb01ne-tuning and testing was\nthe same for all models.\nConsidering that we have essentially in\ufb01nite data to train\nour model, we might expect that our algorithm should also\nprovide a large boost to higher-capacity models such as\nVGG [49]. To test this, we trained a model following the 16-\nlayer structure of [49] for the convolutional layers on each\nside of the network (the \ufb01nal fc6-fc9 layers were the same\nas in Figure 3). We again \ufb01ne-tuned the representation on\nPascal VOC using fast-rcnn, by transferring only the conv\nlayers, again following Kr\u00a8ahenb\u00a8uhl et al. [31] to re-scale\nthe transferred weights and initialize the rest. As a base-\nline, we performed a similar experiment with the ImageNet-\npretrained 16-layer model of [49] (though we kept pre-\ntrained fc layers rather than re-initializing them), and also\nby initializing the entire network with K-means [31]. Train-\n6\nLower Better\nHigher Better\nMean Median 11.25\u25e622.5\u25e630\u25e6\nScratch\n38.6\n26.5\n33.1\n46.8\n52.5\nUnsup. Tracking [57]\n34.2\n21.9\n35.7\n50.6\n57.0\nOurs\n33.2\n21.3\n36.0\n51.2\n57.8\nImageNet Labels\n33.3\n20.8\n36.7\n51.7\n58.1\nTable 2. Accuracy on NYUv2.\ning time was considerably longer\u2014about 8 weeks on a Titan\nX GPU\u2014but the the network outperformed the AlexNet-\nstyle model by a considerable margin. Note the model ini-\ntialized with K-means performed roughly on par with the\nanalogous AlexNet model, suggesting that most of the boost\ncame from the unsupervised pre-training.\n4.4. Geometry Estimation\nThe results of Section 4.3 suggest that our representa-\ntion is sensitive to objects, even though it was not originally\ntrained to \ufb01nd them. This raises the question: Does our\nrepresentation extract information that is useful for other,\nnon-object-based tasks? To \ufb01nd out, we \ufb01ne-tuned our net-\nwork to perform the surface normal estimation on NYUv2\nproposed in Fouhey et al. [19], following the \ufb01netuning pro-\ncedure of Wang et al. [57] (hence, we compare directly to\nthe unsupervised pretraining results reported there).\nWe\nused the color-dropping network, restructuring the fully-\nconnected layers as in Section 4.3. Surprisingly, our re-\nsults are almost equivalent to those obtained using a fully-\nlabeled ImageNet model. One possible explanation for this\nis that the ImageNet categorization task does relatively little\nto encourage a network to pay attention to geometry, since\nthe geometry is largely irrelevant once an object is identi-\n\ufb01ed. Further evidence of this can be seen in seventh row of\nFigure 4: the nearest neighbors for ImageNet AlexNet are\nall car wheels, but they are not aligned well with the query\npatch.\n4.5. Visual Data Mining\nVisual data mining [44, 13, 50, 45], or unsupervised ob-\nject discovery [51, 47, 22], aims to use a large image col-\nlection to discover image fragments which happen to depict\nthe same semantic objects. Applications include dataset vi-\nsualization, content-based retrieval, and tasks that require\nrelating visual data to other unstructured information (e.g.\nGPS coordinates [13]).\nFor automatic data mining, our\napproach from section 4.1 is inadequate: although object\npatches match to similar objects, textures match just as\nreadily to similar textures. Suppose, however, that we sam-\npled two non-overlapping patches from the same object.\nNot only would the nearest neighbor lists for both patches\nshare many images, but within those images, the nearest\nneighbors would be in roughly the same spatial con\ufb01gura-\ntion. For texture regions, on the other hand, the spatial con-\n\ufb01gurations of the neighbors would be random, because the\ntexture has no global layout.\nTo implement this, we \ufb01rst sample a constellation of\nfour adjacent patches from an image (we use four to reduce\nthe likelihood of a matching spatial arrangement happen-\ning by chance). We \ufb01nd the top 100 images which have\nthe strongest matches for all four patches, ignoring spatial\nlayout. We then use a type of geometric veri\ufb01cation [7]\nto \ufb01lter away the images where the four matches are not\ngeometrically consistent. Because our features are more\nsemantically-tuned, we can use a much weaker type of ge-\nometric veri\ufb01cation than [7]. Finally, we rank the different\nconstellations by counting the number of times the top 100\nmatches geometrically verify.\nImplementation Details: To compute whether a set of four\nmatched patches geometrically veri\ufb01es, we \ufb01rst compute\nthe best-\ufb01tting square S to the patch centers (via least-\nsquares), while constraining that side of S be between 2/3\nand 4/3 of the average side of the patches. We then compute\nthe squared error of the patch centers relative to S (normal-\nized by dividing the sum-of-squared-errors by the square of\nthe side of S). The patch is geometrically veri\ufb01ed if this\nnormalized squared error is less than 1. When sampling\npatches do not use any of the data augmentation preprocess-\ning steps (e.g. downsampling). We use the color-dropping\nversion of our network.\nWe applied the described mining algorithm to Pascal\nVOC 2011, with no pre-\ufb01ltering of images and no addi-\ntional labels. We show some of the resulting patch clusters\nin Figure 7. The results are visually comparable to our pre-\nvious work [12], although we discover a few objects that\nwere not found in [12], such as monitors, birds, torsos, and\nplates of food. The discovery of birds and torsos\u2014which\nare notoriously deformable\u2014provides further evidence for\nthe invariances our algorithm has learned. We believe we\nhave covered all objects discovered in [12], with the ex-\nception of (1) trusses and (2) railroad tracks without trains\n(though we do discover them with trains). For some objects\nlike dogs, we discover more variety and rank the best ones\nhigher. Furthermore, many of the clusters shown in [12] de-\npict gratings (14 out of the top 100), whereas none of ours\ndo (though two of our top hundred depict diffuse gradients).\nAs in [12], we often re-discover the same object multiple\ntimes with different viewpoints, which accounts for most of\nthe gaps between ranks in Figure 7. The main disadvan-\ntages of our algorithm relative to [12] are 1) some loss of\npurity, and 2) that we cannot currently determine an object\nmask automatically (although one could imagine dynami-\ncally adding more sub-patches to each proposed object).\nTo ensure that our algorithm has not simply learned an\nobject-centric representation due to the various biases [55]\nin ImageNet, we also applied our algorithm to 15,000 Street\nView images from Paris (following [13]). The results in\nFigure 8 show that our representation captures scene lay-\n7\n1 \n4 \n25 \n30 \n46 \n7 \n12 \n29 \n35 \n73 \n88 \n131 \n121 \n142 \n229 \n240 \n351 \n179 \n187 \n232 \n256 \n464 \n70 \n71 \n1 \n4 \n25 \n30 \n46 \n7 \n12 \n29 \n35 \n73 \n88 \n131 \n121 \n142 \n229 \n240 \n351 \n179 \n187 \n232 \n464 \n70 \n71 \nFigure 7. Object clusters discovered by our algorithm. The number beside each cluster indicates its ranking, determined by the fraction of\nthe top matches that geometrically veri\ufb01ed. For all clusters, we show the raw top 7 matches that veri\ufb01ed geometrically. The full ranking is\navailable on our project webpage.\nout and architectural elements. For this experiment, to rank\nclusters, we use the de-duplication procedure originally pro-\nposed in [13].\n4.5.1\nQuantitative Results\nAs part of the qualitative evaluation, we applied our algo-\nrithm to the subset of Pascal VOC 2007 selected in [50]:\nspeci\ufb01cally, those containing at least one instance of bus,\ndining table, motorbike, horse, sofa, or train, and evaluate\nvia a purity coverage curve following [12]. We select 1000\nsets of 10 images each for evaluation. The evaluation then\nsorts the sets by purity: the fraction of images in the clus-\nter containing the same category. We generate the curve by\nwalking down the ranking. For each point on the curve, we\nplot average purity of all sets up to a given point in the rank-\ning against coverage: the fraction of images in the dataset\nthat are contained in at least one of the sets up to that point.\nAs shown in Figure 9, we have gained substantially in terms\nof coverage, suggesting increased invariance for our learned\nfeature. However, we have also lost some highly-pure clus-\nters compared to [12]\u2014which is not very surprising consid-\nering that our validation procedure is considerably simpler.\nImplementation Details: We initialize 16,384 clusters by\nsampling patches, mining nearest neighbors, and geomet-\nric veri\ufb01cation ranking as described above. The resulting\nclusters are highly redundant. The cluster selection proce-\ndure of [12] relies on a likelihood ratio score that is cali-\nbrated across clusters, which is not available to us. To se-\nlect clusters, we \ufb01rst select the top 10 geometrically-veri\ufb01ed\nneighbors for each cluster. Then we iteratively select the\nhighest-ranked cluster that contributes at least one image to\nour coverage score. When we run out of images that aren\u2019t\nincluded in the coverage score, we choose clusters to cover\neach image at least twice, and then three times, and so on.\n4.6. Accuracy on the Relative Prediction Task Task\nCan we improve the representation by further training\non our relative prediction pretext task?\nTo \ufb01nd out, we\nbrie\ufb02y analyze classi\ufb01cation performance on pretext task\n8\n1: \n5: \n6: \n13: \n18: \n4: \n42: \n53: \nFigure 8. Clusters discovered and automatically ranked via our al-\ngorithm (\u00a7 4.5) from the Paris Street View dataset.\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nCoverage\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nPurity\nPurity-Coverage for Proposed Objects\nVisual Words .63 (.37)\nRussel et al. .66 (.38)\nHOG Kmeans .70 (.40)\nSingh et al. .83 (.47)\nDoersch et al. .83 (.48)\nOur Approach .87 (.48)\nFigure 9. Purity vs coverage for objects discovered on a subset of\nPascal VOC 2007. The numbers in the legend indicate area under\nthe curve (AUC). In parentheses is the AUC up to a coverage of .5.\nitself. We sampled 500 random images from Pascal VOC\n2007, sampled 256 pairs of patches from each, and clas-\nsi\ufb01ed them into the eight relative-position categories from\nFigure 2. This gave an accuracy of 38.4%, where chance\nperformance is 12.5%, suggesting that the pretext task is\nquite hard (indeed, human performance on the task is simi-\nlar). To measure possible over\ufb01tting, we also ran the same\nexperiment on ImageNet, which is the dataset we used for\ntraining. The network was 39.5% accurate on the training\nset, and 40.3% accurate on the validation set (which the net-\nwork never saw during training), suggesting that little over-\n\ufb01tting has occurred.\nOne possible reason why the pretext task is so dif\ufb01cult\nis because, for a large fraction of patches within each im-\nage, the task is almost impossible. Might the task be easiest\nfor image regions corresponding to objects? To test this\nhypothesis, we repeated our experiment using only patches\nsampled from within Pascal object ground-truth bounding\nboxes. We select only those boxes that are at least 240 pix-\nels on each side, and which are not labeled as truncated,\noccluded, or dif\ufb01cult. Surprisingly, this gave essentially the\nsame accuracy of 39.2%, and a similar experiment only on\ncars yielded 45.6% accuracy. So, while our algorithm is\nsensitive to objects, it is almost as sensitive to the layout of\nthe rest of the image.\nAcknowledgements We thank Xiaolong Wang and Pulkit Agrawal for\nhelp with baselines, Berkeley and CMU vision group members for many\nfruitful discussions, and Jitendra Malik for putting gelato on the line. This\nwork was partially supported by Google Graduate Fellowship to CD, ONR\nMURI N000141010934, Intel research grant, an NVidia hardware grant,\nand an Amazon Web Services grant.\nReferences\n[1] E. H. Adelson. On seeing stuff: the perception of materials by hu-\nmans and machines. In Photonics West 2001-Electronic Imaging,\n2001. 2\n[2] P. Agrawal, R. Girshick, and J. Malik. Analyzing the performance of\nmultilayer neural networks for object recognition. In ECCV. 2014.\n5, 6\n[3] R. K. Ando and T. Zhang. A framework for learning predictive struc-\ntures from multiple tasks and unlabeled data. JMLR, 2005. 1\n[4] Y. Bengio, E. Thibodeau-Laufer, G. Alain, and J. Yosinski. Deep\ngenerative stochastic networks trainable by backprop. ICML, 2014.\n2\n[5] D. Brewster and A. D. Bache. Treatise on optics. Blanchard and Lea,\n1854. 3\n[6] O. Chum, M. Perdoch, and J. Matas. Geometric min-hashing: Find-\ning a (thick) needle in a haystack. In CVPR, 2009. 3\n[7] O. Chum, J. Philbin, J. Sivic, M. Isard, and A. Zisserman. Total\nrecall: Automatic query expansion with a generative feature model\nfor object retrieval. In ICCV, 2007. 7\n[8] R. G. Cinbis, J. Verbeek, and C. Schmid. Segmentation driven object\ndetection with Fisher vectors. In ICCV, 2013. 6\n[9] R. Collobert and J. Weston. A uni\ufb01ed architecture for natural lan-\nguage processing: Deep neural networks with multitask learning. In\nICML, 2008. 1, 2\n[10] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. Im-\nagenet: A large-scale hierarchical image database. In CVPR, 2009.\n4\n[11] C. Doersch, A. Gupta, and A. A. Efros. Mid-level visual element\ndiscovery as discriminative mode seeking. In NIPS, 2013. 3\n[12] C. Doersch, A. Gupta, and A. A. Efros. Context as supervisory sig-\nnal: Discovering objects with predictable context. In ECCV. 2014.\n2, 7, 8\n[13] C. Doersch, S. Singh, A. Gupta, J. Sivic, and A. A. Efros. What\nmakes Paris look like Paris? SIGGRAPH, 2012. 3, 7\n[14] J. Domke, A. Karapurkar, and Y. Aloimonos. Who killed the directed\nmodel? In CVPR, 2008. 2\n[15] M. Everingham, L. Van Gool, C. K. Williams, J. Winn, and A. Zisser-\nman. The pascal visual object classes (voc) challenge. IJCV, 2010.\n5\n[16] A. Faktor and M. Irani. clustering by composition\u2013unsupervised dis-\ncovery of image categories. In ECCV. 2012. 3\n[17] P. Felzenszwalb, R. Girshick, D. McAllester, and D. Ramanan. Ob-\nject detection with discriminatively trained part-based models. PAMI,\n2010. 6\n9\n[18] P. F\u00a8oldi\u00b4ak. Learning invariance from transformation sequences. Neu-\nral Computation, 1991. 3\n[19] D. F. Fouhey, A. Gupta, and M. Hebert. Data-driven 3D primitives\nfor single image understanding. In ICCV, 2013. 7\n[20] R. Girshick. Fast r-cnn. In ICCV, 2015. 6\n[21] R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich feature hier-\narchies for accurate object detection and semantic segmentation. In\nCVPR, 2014. 1, 5, 6\n[22] K. Grauman and T. Darrell. Unsupervised learning of categories from\nsets of partially matching image features. In CVPR, 2006. 3, 7\n[23] K. Heath, N. Gelfand, M. Ovsjanikov, M. Aanjaneya, and L. J.\nGuibas. Image webs: Computing and exploiting connectivity in im-\nage collections. In CVPR, 2010. 3\n[24] G. Hinton, S. Osindero, and Y.-W. Teh. A fast learning algorithm for\ndeep belief nets. Neural computation, 2006. 2\n[25] G. E. Hinton, P. Dayan, B. J. Frey, and R. M. Neal. The \u201cwake-\nsleep\u201d algorithm for unsupervised neural networks.\nProceedings.\nIEEE, 1995. 2\n[26] S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep\nnetwork training by reducing internal covariate shift. arXiv preprint\narXiv:1502.03167, 2015. 5\n[27] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick,\nS. Guadarrama, and T. Darrell. Caffe: Convolutional architecture for\nfast feature embedding. In ACM-MM, 2014. 4\n[28] M. Juneja, A. Vedaldi, C. V. Jawahar, and A. Zisserman. Blocks that\nshout: Distinctive parts for scene classi\ufb01cation. In CVPR, 2013. 3\n[29] G. Kim, C. Faloutsos, and M. Hebert. Unsupervised modeling of\nobject categories using link analysis techniques. In CVPR, 2008. 3\n[30] D. P. Kingma and M. Welling.\nAuto-encoding variational bayes.\n2014. 2\n[31] P. Kr\u00a8ahenb\u00a8uhl, C. Doersch, J. Donahue, and T. Darrell.\nData-\ndependent initializations of convolutional neural networks.\narXiv\npreprint arXiv:1511.06856, 2015. 6\n[32] A. Krizhevsky, I. Sutskever, and G. Hinton. Imagenet classi\ufb01cation\nwith deep convolutional neural networks. In NIPS, 2012. 1, 3\n[33] H. Larochelle and I. Murray. The neural autoregressive distribution\nestimator. In AISTATS, 2011. 2\n[34] Q. V. Le. Building high-level features using large scale unsupervised\nlearning. In ICASSP, 2013. 2\n[35] H. Lee, A. Battle, R. Raina, and A. Y. Ng. Ef\ufb01cient sparse coding\nalgorithms. In NIPS, 2006. 2\n[36] Y. J. Lee and K. Grauman. Foreground focus: Unsupervised learning\nfrom partially matching images. IJCV, 2009. 3\n[37] Q. Li, J. Wu, and Z. Tu. Harvesting mid-level visual concepts from\nlarge-scale internet images. In CVPR, 2013. 3\n[38] J. Long, E. Shelhamer, and T. Darrell. Fully convolutional networks\nfor semantic segmentation. arXiv preprint arXiv:1411.4038, 2014. 6\n[39] T. Malisiewicz and A. Efros. Beyond categories: The visual memex\nmodel for reasoning about object relationships. In NIPS, 2009. 2\n[40] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean. Dis-\ntributed representations of words and phrases and their composition-\nality. In NIPS, 2013. 1, 2\n[41] D. Okanohara and J. Tsujii. A discriminative language model with\npseudo-negative samples. In ACL, 2007. 1, 2\n[42] B. A. Olshausen and D. J. Field. Emergence of simple-cell receptive\n\ufb01eld properties by learning a sparse code for natural images. Nature,\n1996. 2\n[43] N. Payet and S. Todorovic. From a set of shapes to object discovery.\nIn ECCV. 2010. 3\n[44] T. Quack, B. Leibe, and L. Van Gool. World-scale mining of objects\nand events from community photo collections. In CIVR, 2008. 3, 7\n[45] K. Rematas, B. Fernando, F. Dellaert, and T. Tuytelaars. Dataset\n\ufb01ngerprints: Exploring image collections through data mining. In\nCVPR, 2015. 3, 7\n[46] D. J. Rezende, S. Mohamed, and D. Wierstra. Stochastic backpropa-\ngation and approximate inference in deep generative models. ICML,\n2014. 2\n[47] B. C. Russell, W. T. Freeman, A. A. Efros, J. Sivic, and A. Zisserman.\nUsing multiple segmentations to discover objects and their extent in\nimage collections. In CVPR, 2006. 2, 7\n[48] R. Salakhutdinov and G. E. Hinton. Deep boltzmann machines. In\nICAIS, 2009. 2\n[49] K. Simonyan and A. Zisserman. Very deep convolutional networks\nfor large-scale image recognition. CoRR, 2014. 6\n[50] S. Singh, A. Gupta, and A. A. Efros. Unsupervised discovery of\nmid-level discriminative patches. In ECCV, 2012. 3, 7, 8\n[51] J. Sivic, B. C. Russell, A. A. Efros, A. Zisserman, and W. T. Freeman.\nDiscovering objects and their location in images. In ICCV, 2005. 2,\n7\n[52] J. Sun and J. Ponce. Learning discriminative part detectors for image\nclassi\ufb01cation and cosegmentation. In ICCV, 2013. 3\n[53] L. Theis and M. Bethge. Generative image modeling using spatial\nlstms. In NIPS, 2015. 2\n[54] B. Thomee, D. A. Shamma, G. Friedland, B. Elizalde, K. Ni,\nD. Poland, D. Borth, and L.-J. Li. The new data and new challenges\nin multimedia research. arXiv preprint arXiv:1503.01817, 2015. 6\n[55] A. Torralba and A. A. Efros. Unbiased look at dataset bias. In CVPR,\n2011. 6, 7\n[56] P. Vincent, H. Larochelle, Y. Bengio, and P.-A. Manzagol. Extract-\ning and composing robust features with denoising autoencoders. In\nICML, 2008. 2\n[57] X. Wang and A. Gupta. Unsupervised learning of visual representa-\ntions using videos. In ICCV, 2015. 3, 7\n[58] X. Wang, M. Yang, S. Zhu, and Y. Lin. Regionlets for generic object\ndetection. In ICCV, 2013. 6\n[59] L. Wiskott and T. J. Sejnowski. Slow feature analysis:unsupervised\nlearning of invariances. Neural Computation, 2002. 3\n10\n",
        "sentence": " They either try to capture a signal from the source as a form of selfsupervision (Doersch et al., 2015; Wang & Gupta, 2015) or learn the underlying distribution of images (Vincent et al. Recently, a lot of work has explored self-supervison: leveraging supervision contained in the input signal (Doersch et al., 2015; Noroozi & Favaro, 2016; Pathak et al., 2016). We compare our model with several self-supervised approaches (Wang & Gupta, 2015; Doersch et al., 2015; Zhang et al., 2016) and an unsupervised approach, i. We compare our model with several self-supervised approaches (Wang & Gupta, 2015; Doersch et al., 2015; Zhang et al., 2016) and an unsupervised approach, i.e., Donahue et al. (2016). Note that self-supervised approaches use losses specifically designed for visual features. 8 Doersch et al. (2015) 30. 8 Doersch et al. (2015) 30.4 Zhang et al. (2016) 35. 8 Doersch et al. (2015) 30.4 Zhang et al. (2016) 35.2 Noroozi & Favaro (2016) 38.",
        "context": "ing and composing robust features with denoising autoencoders. In\nICML, 2008. 2\n[57] X. Wang and A. Gupta. Unsupervised learning of visual representa-\ntions using videos. In ICCV, 2015. 3, 7\nUnsupervised Visual Representation Learning by Context Prediction\nCarl Doersch1,2\nAbhinav Gupta1\nAlexei A. Efros2\n1 School of Computer Science\n2 Dept. of Electrical Engineering and Computer Science\nCarnegie Mellon University\n4\n[11] C. Doersch, A. Gupta, and A. A. Efros. Mid-level visual element\ndiscovery as discriminative mode seeking. In NIPS, 2013. 3\n[12] C. Doersch, A. Gupta, and A. A. Efros. Context as supervisory sig-"
    },
    {
        "title": "Adversarial feature learning",
        "author": [
            "J. Donahue",
            "P. Kr\u00e4henb\u00fchl",
            "T. Darrell"
        ],
        "venue": "arXiv preprint arXiv:1605.09782,",
        "citeRegEx": "Donahue et al\\.,? \\Q2016\\E",
        "shortCiteRegEx": "Donahue et al\\.",
        "year": 2016,
        "abstract": "The ability of the Generative Adversarial Networks (GANs) framework to learn\ngenerative models mapping from simple latent distributions to arbitrarily\ncomplex data distributions has been demonstrated empirically, with compelling\nresults showing that the latent space of such generators captures semantic\nvariation in the data distribution. Intuitively, models trained to predict\nthese semantic latent representations given data may serve as useful feature\nrepresentations for auxiliary problems where semantics are relevant. However,\nin their existing form, GANs have no means of learning the inverse mapping --\nprojecting data back into the latent space. We propose Bidirectional Generative\nAdversarial Networks (BiGANs) as a means of learning this inverse mapping, and\ndemonstrate that the resulting learned feature representation is useful for\nauxiliary supervised discrimination tasks, competitive with contemporary\napproaches to unsupervised and self-supervised feature learning.",
        "full_text": "Published as a conference paper at ICLR 2017\nADVERSARIAL FEATURE LEARNING\nJeff Donahue\njdonahue@cs.berkeley.edu\nComputer Science Division\nUniversity of California, Berkeley\nPhilipp Kr\u00e4henb\u00fchl\nphilkr@utexas.edu\nDepartment of Computer Science\nUniversity of Texas, Austin\nTrevor Darrell\ntrevor@eecs.berkeley.edu\nComputer Science Division\nUniversity of California, Berkeley\nABSTRACT\nThe ability of the Generative Adversarial Networks (GANs) framework to learn\ngenerative models mapping from simple latent distributions to arbitrarily complex\ndata distributions has been demonstrated empirically, with compelling results\nshowing that the latent space of such generators captures semantic variation in\nthe data distribution. Intuitively, models trained to predict these semantic latent\nrepresentations given data may serve as useful feature representations for auxiliary\nproblems where semantics are relevant. However, in their existing form, GANs\nhave no means of learning the inverse mapping \u2013 projecting data back into the\nlatent space. We propose Bidirectional Generative Adversarial Networks (BiGANs)\nas a means of learning this inverse mapping, and demonstrate that the resulting\nlearned feature representation is useful for auxiliary supervised discrimination tasks,\ncompetitive with contemporary approaches to unsupervised and self-supervised\nfeature learning.\n1\nINTRODUCTION\nDeep convolutional networks (convnets) have become a staple of the modern computer vision pipeline.\nAfter training these models on a massive database of image-label pairs like ImageNet (Russakovsky\net al., 2015), the network easily adapts to a variety of similar visual tasks, achieving impressive\nresults on image classi\ufb01cation (Donahue et al., 2014; Zeiler & Fergus, 2014; Razavian et al., 2014)\nor localization (Girshick et al., 2014; Long et al., 2015) tasks. In other perceptual domains such as\nnatural language processing or speech recognition, deep networks have proven highly effective as\nwell (Bahdanau et al., 2015; Sutskever et al., 2014; Vinyals et al., 2015; Graves et al., 2013). However,\nall of these recent results rely on a supervisory signal from large-scale databases of hand-labeled data,\nignoring much of the useful information present in the structure of the data itself.\nMeanwhile, Generative Adversarial Networks (GANs) (Goodfellow et al., 2014) have emerged as a\npowerful framework for learning generative models of arbitrarily complex data distributions. The\nGAN framework learns a generator mapping samples from an arbitrary latent distribution to data, as\nwell as an adversarial discriminator which tries to distinguish between real and generated samples\nas accurately as possible. The generator\u2019s goal is to \u201cfool\u201d the discriminator by producing samples\nwhich are as close to real data as possible. When trained on databases of natural images, GANs\nproduce impressive results (Radford et al., 2016; Denton et al., 2015).\nInterpolations in the latent space of the generator produce smooth and plausible semantic variations,\nand certain directions in this space correspond to particular semantic attributes along which the data\ndistribution varies. For example, Radford et al. (2016) showed that a GAN trained on a database of\nhuman faces learns to associate particular latent directions with gender and the presence of eyeglasses.\nA natural question arises from this ostensible \u201csemantic juice\u201d \ufb02owing through the weights of\ngenerators learned using the GAN framework: can GANs be used for unsupervised learning of rich\nfeature representations for arbitrary data distributions? An obvious issue with doing so is that the\n1\narXiv:1605.09782v7  [cs.LG]  3 Apr 2017\nPublished as a conference paper at ICLR 2017\nfeatures\ndata\nz\nG\nG(z)\nx\nE\nE(x)\nG(z), z\nx, E(x)\nD\nP(y)\nFigure 1: The structure of Bidirectional Generative Adversarial Networks (BiGAN).\ngenerator maps latent samples to generated data, but the framework does not include an inverse\nmapping from data to latent representation.\nHence, we propose a novel unsupervised feature learning framework, Bidirectional Generative\nAdversarial Networks (BiGAN). The overall model is depicted in Figure 1. In short, in addition to\nthe generator G from the standard GAN framework (Goodfellow et al., 2014), BiGAN includes an\nencoder E which maps data x to latent representations z. The BiGAN discriminator D discriminates\nnot only in data space (x versus G(z)), but jointly in data and latent space (tuples (x, E(x)) versus\n(G(z), z)), where the latent component is either an encoder output E(x) or a generator input z.\nIt may not be obvious from this description that the BiGAN encoder E should learn to invert the\ngenerator G. The two modules cannot directly \u201ccommunicate\u201d with one another: the encoder never\n\u201csees\u201d generator outputs (E(G(z)) is not computed), and vice versa. Yet, in Section 3, we will both\nargue intuitively and formally prove that the encoder and generator must learn to invert one another\nin order to fool the BiGAN discriminator.\nBecause the BiGAN encoder learns to predict features z given data x, and prior work on GANs has\ndemonstrated that these features capture semantic attributes of the data, we hypothesize that a trained\nBiGAN encoder may serve as a useful feature representation for related semantic tasks, in the same\nway that fully supervised visual models trained to predict semantic \u201clabels\u201d given images serve as\npowerful feature representations for related visual tasks. In this context, a latent representation z may\nbe thought of as a \u201clabel\u201d for x, but one which came for \u201cfree,\u201d without the need for supervision.\nAn alternative approach to learning the inverse mapping from data to latent representation is to\ndirectly model p(z|G(z)), predicting generator input z given generated data G(z). We\u2019ll refer to\nthis alternative as a latent regressor, later arguing (Section 4.1) that the BiGAN encoder may be\npreferable in a feature learning context, as well as comparing the approaches empirically.\nBiGANs are a robust and highly generic approach to unsupervised feature learning, making no\nassumptions about the structure or type of data to which they are applied, as our theoretical results will\ndemonstrate. Our empirical studies will show that despite their generality, BiGANs are competitive\nwith contemporary approaches to self-supervised and weakly supervised feature learning designed\nspeci\ufb01cally for a notoriously complex data distribution \u2013 natural images.\nDumoulin et al. (2016) independently proposed an identical model in their concurrent work, exploring\nthe case of a stochastic encoder E and the ability of such models to learn in a semi-supervised setting.\n2\nPRELIMINARIES\nLet pX(x) be the distribution of our data for x \u2208\u2126X (e.g. natural images). The goal of generative\nmodeling is capture this data distribution using a probabilistic model. Unfortunately, exact modeling\nof this probability density function is computationally intractable (Hinton et al., 2006; Salakhutdinov\n& Hinton, 2009) for all but the most trivial models. Generative Adversarial Networks (GANs) (Good-\n2\nPublished as a conference paper at ICLR 2017\nfellow et al., 2014) instead model the data distribution as a transformation of a \ufb01xed latent distribution\npZ(z) for z \u2208\u2126Z. This transformation, called a generator, is expressed as a deterministic feed\nforward network G : \u2126Z \u2192\u2126X with pG(x|z) = \u03b4 (x \u2212G(z)) and pG(x) = Ez\u223cpZ [pG(x|z)]. The\ngoal is to train a generator such that pG(x) \u2248pX(x).\nThe GAN framework trains a generator, such that no discriminative model D : \u2126X 7\u2192[0, 1] can\ndistinguish samples of the data distribution from samples of the generative distribution. Both generator\nand discriminator are learned using the adversarial (minimax) objective min\nG max\nD V (D, G), where\nV (D, G) := Ex\u223cpX [log D(x)] + Ex\u223cpG [log (1 \u2212D(x))]\n|\n{z\n}\nEz\u223cpZ[log(1\u2212D(G(z)))]\n(1)\nGoodfellow et al. (2014) showed that for an ideal discriminator the objective C(G)\n:=\nmaxD V (D, G) is equivalent to the Jensen-Shannon divergence between the two distributions pG\nand pX.\nThe adversarial objective 1 does not directly lend itself to an ef\ufb01cient optimization, as each step in\nthe generator G requires a full discriminator D to be learned. Furthermore, a perfect discriminator\nno longer provides any gradient information to the generator, as the gradient of any global or local\nmaximum of V (D, G) is 0. To provide a strong gradient signal nonetheless, Goodfellow et al. (2014)\nslightly alter the objective between generator and discriminator updates, while keeping the same \ufb01xed\npoint characteristics. They also propose to optimize (1) using an alternating optimization switching\nbetween updates to the generator and discriminator. While this optimization is not guaranteed to\nconverge, empirically it works well if the discriminator and generator are well balanced.\nDespite the empirical strength of GANs as generative models of arbitrary data distributions, it is not\nclear how they can be applied as an unsupervised feature representation. One possibility for learning\nsuch representations is to learn an inverse mapping regressing from generated data G(z) back to\nthe latent input z. However, unless the generator perfectly models the data distribution pX, a nearly\nimpossible objective for a complex data distribution such as that of high-resolution natural images,\nthis idea may prove insuf\ufb01cient.\n3\nBIDIRECTIONAL GENERATIVE ADVERSARIAL NETWORKS\nIn Bidirectional Generative Adversarial Networks (BiGANs) we not only train a generator, but\nadditionally train an encoder E : \u2126X \u2192\u2126Z. The encoder induces a distribution pE(z|x) =\n\u03b4(z \u2212E(x)) mapping data points x into the latent feature space of the generative model. The\ndiscriminator is also modi\ufb01ed to take input from the latent space, predicting PD(Y |x, z), where\nY = 1 if x is real (sampled from the real data distribution pX), and Y = 0 if x is generated (the\noutput of G(z), z \u223cpZ).\nThe BiGAN training objective is de\ufb01ned as a minimax objective\nmin\nG,E max\nD V (D, E, G)\n(2)\nwhere\nV (D, E, G) := Ex\u223cpX\n\u0002\nEz\u223cpE(\u00b7|x) [log D(x, z)]\n|\n{z\n}\nlog D(x,E(x))\n\u0003\n+ Ez\u223cpZ\n\u0002\nEx\u223cpG(\u00b7|z) [log (1 \u2212D(x, z))]\n|\n{z\n}\nlog(1\u2212D(G(z),z))\n\u0003\n.\n(3)\nWe optimize this minimax objective using the same alternating gradient based optimization as\nGoodfellow et al. (2014). See Section 3.4 for details.\nBiGANs share many of the theoretical properties of GANs (Goodfellow et al., 2014), while addition-\nally guaranteeing that at the global optimum, G and E are each other\u2019s inverse. BiGANs are also\nclosely related to autoencoders with an \u21130 loss function. In the following sections we highlight some\nof the appealing theoretical properties of BiGANs.\nDe\ufb01nitions\nLet pGZ(x, z) := pG(x|z)pZ(z) and pEX(x, z) := pE(z|x)pX(x) be the joint distri-\nbutions modeled by the generator and encoder respectively. \u2126:= \u2126X \u00d7 \u2126Z is the joint latent and\n3\nPublished as a conference paper at ICLR 2017\ndata space. For a region R \u2286\u2126,\nPEX(R) :=\nR\n\u2126pEX(x, z)1[(x,z)\u2208R] d(x, z) =\nR\n\u2126X pX(x)\nR\n\u2126Z pE(z|x)1[(x,z)\u2208R] dz dx\nPGZ(R) :=\nR\n\u2126pGZ(x, z)1[(x,z)\u2208R] d(x, z) =\nR\n\u2126Z pZ(z)\nR\n\u2126X pG(x|z)1[(x,z)\u2208R] dx dz\nare probability measures over that region. We also de\ufb01ne\nPX(RX) :=\nR\n\u2126X pX(x)1[x\u2208RX] dx\nPZ(RZ) :=\nR\n\u2126Z pZ(z)1[z\u2208RZ] dz\nas measures over regions RX \u2286\u2126X and RZ \u2286\u2126Z. We refer to the set of features and data samples\nin the support of PX and PZ as \u02c6\u2126X := supp(PX) and \u02c6\u2126Z := supp(PZ) respectively. DKL (P || Q)\nand DJS (P || Q) respectively denote the Kullback-Leibler (KL) and Jensen-Shannon divergences\nbetween probability measures P and Q. By de\ufb01nition,\nDKL (P || Q) := Ex\u223cP [log fP Q(x)]\nDJS (P || Q) := 1\n2\n\u0010\nDKL\n\u0010\nP\n\f\f\f\n\f\f\f P +Q\n2\n\u0011\n+ DKL\n\u0010\nQ\n\f\f\f\n\f\f\f P +Q\n2\n\u0011\u0011\n,\nwhere fP Q := dP\ndQ is the Radon-Nikodym (RN) derivative of measure P with respect to measure Q,\nwith the de\ufb01ning property that P(R) =\nR\nR fP Q dQ. The RN derivative fP Q : \u21267\u2192R\u22650 is de\ufb01ned\nfor any measures P and Q on space \u2126such that P is absolutely continuous with respect to Q: i.e.,\nfor any R \u2286\u2126, P(R) > 0 =\u21d2Q(R) > 0.\n3.1\nOPTIMAL DISCRIMINATOR, GENERATOR, & ENCODER\nWe start by characterizing the optimal discriminator for any generator and encoder, following Good-\nfellow et al. (2014). This optimal discriminator then allows us to reformulate objective (3), and show\nthat it reduces to the Jensen-Shannon divergence between the joint distributions PEX and PGZ.\nProposition 1 For any E and G, the optimal discriminator D\u2217\nEG := arg maxD V (D, E, G) is the\nRadon-Nikodym derivative fEG :=\ndPEX\nd(PEX+PGZ) : \u21267\u2192[0, 1] of measure PEX with respect to\nmeasure PEX + PGZ.\nProof. Given in Appendix A.1.\nThis optimal discriminator now allows us to characterize the optimal generator and encoder.\nProposition 2 The encoder and generator\u2019s objective for an optimal discriminator C(E, G) :=\nmaxD V (D, E, G) = V (D\u2217\nEG, E, G) can be rewritten in terms of the Jensen-Shannon divergence\nbetween measures PEX and PGZ as C(E, G) = 2 DJS (PEX || PGZ ) \u2212log 4.\nProof. Given in Appendix A.2.\nTheorem 1 The global minimum of C(E, G) is achieved if and only if PEX = PGZ. At that point,\nC(E, G) = \u2212log 4 and D\u2217\nEG = 1\n2.\nProof. From Proposition 2, we have that C(E, G) = 2 DJS (PEX || PGZ ) \u2212log 4. The Jensen-\nShannon divergence DJS (P || Q) \u22650 for any P and Q, and DJS (P || Q) = 0 if and only if P = Q.\nTherefore, the global minimum of C(E, G) occurs if and only if PEX = PGZ, and at this point the\nvalue is C(E, G) = \u2212log 4. Finally, PEX = PGZ implies that the optimal discriminator is chance:\nD\u2217\nEG =\ndPEX\nd(PEX+PGZ) =\ndPEX\n2 dPEX = 1\n2. \u25a1\nThe optimal discriminator, encoder, and generator of BiGAN are similar to the optimal discriminator\nand generator of the GAN framework (Goodfellow et al., 2014). However, an important difference is\nthat BiGAN optimizes a Jensen-Shannon divergence between a joint distribution over both data X\nand latent features Z. This joint divergence allows us to further characterize properties of G and E,\nas shown below.\n3.2\nOPTIMAL GENERATOR & ENCODER ARE INVERSES\nWe \ufb01rst present an intuitive argument that, in order to \u201cfool\u201d a perfect discriminator, a deterministic\nBiGAN encoder and generator must invert each other. (Later we will formally state and prove this\n4\nPublished as a conference paper at ICLR 2017\nproperty.) Consider a BiGAN discriminator input pair (x, z). Due to the sampling procedure, (x, z)\nmust satisfy at least one of the following two properties:\n(a) x \u2208\u02c6\u2126X \u2227E(x) = z\n(b) z \u2208\u02c6\u2126Z \u2227G(z) = x\nIf only one of these properties is satis\ufb01ed, a perfect discriminator can infer the source of (x, z) with\ncertainty: if only (a) is satis\ufb01ed, (x, z) must be an encoder pair (x, E(x)) and D\u2217\nEG(x, z) = 1; if\nonly (b) is satis\ufb01ed, (x, z) must be a generator pair (G(z), z) and D\u2217\nEG(x, z) = 0.\nTherefore, in order to fool a perfect discriminator at (x, z) (so that 0 < D\u2217\nEG(x, z) < 1), E and\nG must satisfy both (a) and (b). In this case, we can substitute the equality E(x) = z required\nby (a) into the equality G(z) = x required by (b), and vice versa, giving the inversion properties\nx = G(E(x)) and z = E(G(z)).\nFormally, we show in Theorem 2 that the optimal generator and encoder invert one another almost\neverywhere on the support \u02c6\u2126X and \u02c6\u2126Z of PX and PZ.\nTheorem 2 If E and G are an optimal encoder and generator, then E = G\u22121 almost everywhere;\nthat is, G(E(x)) = x for PX-almost every x \u2208\u2126X, and E(G(z)) = z for PZ-almost every z \u2208\u2126Z.\nProof. Given in Appendix A.4.\nWhile Theorem 2 characterizes the encoder and decoder at their optimum, due to the non-convex\nnature of the optimization, this optimum might never be reached. Experimentally, Section 4 shows\nthat on standard datasets, the two are approximate inverses; however, they are rarely exact inverses. It\nis thus also interesting to show what objective BiGAN optimizes in terms of E and G. Next we show\nthat BiGANs are closely related to autoencoders with an \u21130 loss function.\n3.3\nRELATIONSHIP TO AUTOENCODERS\nAs argued in Section 1, a model trained to predict features z given data x should learn useful semantic\nrepresentations. Here we show that the BiGAN objective forces the encoder E to do exactly this: in\norder to fool the discriminator at a particular z, the encoder must invert the generator at that z, such\nthat E(G(z)) = z.\nTheorem 3 The encoder and generator objective given an optimal discriminator C(E, G) :=\nmaxD V (D, E, G) can be rewritten as an \u21130 autoencoder loss function\nC(E, G) = Ex\u223cpX\nh\n1[E(x)\u2208\u02c6\u2126Z\u2227G(E(x))=x] log fEG(x, E(x))\ni\n+\nEz\u223cpZ\nh\n1[G(z)\u2208\u02c6\u2126X\u2227E(G(z))=z] log (1 \u2212fEG(G(z), z))\ni\nwith log fEG \u2208(\u2212\u221e, 0) and log (1 \u2212fEG) \u2208(\u2212\u221e, 0) PEX-almost and PGZ-almost everywhere.\nProof. Given in Appendix A.5.\nHere the indicator function 1[G(E(x))=x] in the \ufb01rst term is equivalent to an autoencoder with \u21130 loss,\nwhile the indicator 1[E(G(z))=z] in the second term shows that the BiGAN encoder must invert the\ngenerator, the desired property for feature learning. The objective further encourages the functions\nE(x) and G(z) to produce valid outputs in the support of PZ and PX respectively. Unlike regular\nautoencoders, the \u21130 loss function does not make any assumptions about the structure or distribution\nof the data itself; in fact, all the structural properties of BiGAN are learned as part of the discriminator.\n3.4\nLEARNING\nIn practice, as in the GAN framework (Goodfellow et al., 2014), each BiGAN module D, G, and E\nis a parametric function (with parameters \u03b8D, \u03b8G, and \u03b8E, respectively). As a whole, BiGAN can be\noptimized using alternating stochastic gradient steps. In one iteration, the discriminator parameters\n\u03b8D are updated by taking one or more steps in the positive gradient direction \u2207\u03b8DV (D, E, G),\nthen the encoder parameters \u03b8E and generator parameters \u03b8G are together updated by taking a step\nin the negative gradient direction \u2212\u2207\u03b8E,\u03b8GV (D, E, G). In both cases, the expectation terms of\n5\nPublished as a conference paper at ICLR 2017\nV (D, E, G) are estimated using mini-batches of n samples {x(i) \u223cpX}n\ni=1 and {z(i) \u223cpZ}n\ni=1\ndrawn independently for each update step.\nGoodfellow et al. (2014) found that an objective in which the real and generated labels Y are swapped\nprovides stronger gradient signal to G. We similarly observed in BiGAN training that an \u201cinverse\u201d\nobjective provides stronger gradient signal to G and E. For ef\ufb01ciency, we also update all modules\nD, G, and E simultaneously at each iteration, rather than alternating between D updates and G, E\nupdates. See Appendix B for details.\n3.5\nGENERALIZED BIGAN\nIt is often useful to parametrize the output of the generator G and encoder E in a different, usually\nsmaller, space \u2126\u2032\nX and \u2126\u2032\nZ rather than the original \u2126X and \u2126Z. For example, for visual feature\nlearning, the images input to the encoder should be of similar resolution to images used in the\nevaluation. On the other hand, generating high resolution images remains dif\ufb01cult for current\ngenerative models. In this situation, the encoder may take higher resolution input while the generator\noutput and discriminator input remain low resolution.\nWe generalize the BiGAN objective V (D, G, E) (3) with functions gX : \u2126X 7\u2192\u2126\u2032\nX and gZ : \u2126Z 7\u2192\n\u2126\u2032\nZ, and encoder E : \u2126X 7\u2192\u2126\u2032\nZ, generator G : \u2126Z 7\u2192\u2126\u2032\nX, and discriminator D : \u2126\u2032\nX \u00d7\u2126\u2032\nZ 7\u2192[0, 1]:\nEx\u223cpX\n\u0002\nEz\u2032\u223cpE(\u00b7|x) [log D(gX(x), z\u2032)]\n|\n{z\n}\nlog D(gX(x),E(x))\n\u0003\n+ Ez\u223cpZ\n\u0002\nEx\u2032\u223cpG(\u00b7|z) [log (1 \u2212D(x\u2032, gZ(z)))]\n|\n{z\n}\nlog(1\u2212D(G(z),gZ(z)))\n\u0003\nAn identity gX(x) = x and gZ(z) = z (and \u2126\u2032\nX = \u2126X, \u2126\u2032\nZ = \u2126Z) yields the original objective. For\nvisual feature learning with higher resolution encoder inputs, gX is an image resizing function that\ndownsamples a high resolution image x \u2208\u2126X to a lower resolution image x\u2032 \u2208\u2126\u2032\nX, as output by the\ngenerator. (gZ is identity.)\nIn this case, the encoder and generator respectively induce probability measures PEX\u2032 and\nPGZ\u2032 over regions R\n\u2286\n\u2126\u2032 of the joint space \u2126\u2032\n:=\n\u2126\u2032\nX \u00d7 \u2126\u2032\nZ, with PEX\u2032(R)\n:=\nR\n\u2126X\nR\n\u2126\u2032\nX\nR\n\u2126\u2032\nZ pEX(x, z\u2032)1[(x\u2032,z\u2032)\u2208R]\u03b4(gX(x) \u2212x\u2032) dz\u2032 dx\u2032 dx =\nR\n\u2126X pX(x)1[(gX(x),E(x))\u2208R] dx,\nand PGZ\u2032 de\ufb01ned analogously. For optimal E and G, we can show PEX\u2032 = PGZ\u2032: a generalization\nof Theorem 1. When E and G are deterministic and optimal, Theorem 2 \u2013 that E and G invert one\nanother \u2013 can also be generalized: \u2203z\u2208\u02c6\u2126Z{E(x) = gZ(z) \u2227G(z) = gX(x)} for PX-almost every\nx \u2208\u2126X, and \u2203x\u2208\u02c6\u2126X{E(x) = gZ(z) \u2227G(z) = gX(x)} for PZ-almost every z \u2208\u2126Z.\n4\nEVALUATION\nWe evaluate the feature learning capabilities of BiGANs by \ufb01rst training them unsupervised as\ndescribed in Section 3.4, then transferring the encoder\u2019s learned feature representations for use in\nauxiliary supervised learning tasks. To demonstrate that BiGANs are able to learn meaningful feature\nrepresentations both on arbitrary data vectors, where the model is agnostic to any underlying structure,\nas well as very high-dimensional and complex distributions, we evaluate on both permutation-invariant\nMNIST (LeCun et al., 1998) and on the high-resolution natural images of ImageNet (Russakovsky\net al., 2015).\nIn all experiments, each module D, G, and E is a parametric deep (multi-layer) network. The BiGAN\ndiscriminator D(x, z) takes data x as its initial input, and at each linear layer thereafter, the latent\nrepresentation z is transformed using a learned linear transformation to the hidden layer dimension\nand added to the non-linearity input.\n4.1\nBASELINE METHODS\nBesides the BiGAN framework presented above, we considered alternative approaches to learning\nfeature representations using different GAN variants.\nDiscriminator\nThe discriminator D in a standard GAN takes data samples x \u223cpX as input, making\nits learned intermediate representations natural candidates as feature representations for related tasks.\n6\nPublished as a conference paper at ICLR 2017\nBiGAN\nD\nLR\nJLR\nAE (\u21132)\nAE (\u21131)\n97.39\n97.30\n97.44\n97.13\n97.58\n97.63\nTable 1: One Nearest Neighbors (1NN) classi\ufb01cation accuracy (%) on the permutation-invariant\nMNIST (LeCun et al., 1998) test set in the feature space learned by BiGAN, Latent Regressor (LR),\nJoint Latent Regressor (JLR), and an autoencoder (AE) using an \u21131 or \u21132 distance.\nG(z)\nx\nG(E(x))\nFigure 2: Qualitative results for permutation-invariant MNIST BiGAN training, including generator\nsamples G(z), real data x, and corresponding reconstructions G(E(x)).\nThis alternative is appealing as it requires no additional machinery, and is the approach used for\nunsupervised feature learning in Radford et al. (2016). On the other hand, it is not clear that the task of\ndistinguishing between real and generated data requires or bene\ufb01ts from intermediate representations\nthat are useful as semantic feature representations. In fact, if G successfully generates the true data\ndistribution pX(x), D may ignore the input data entirely and predict P(Y = 1) = P(Y = 1|x) = 1\n2\nunconditionally, not learning any meaningful intermediate representations.\nLatent regressor\nWe consider an alternative encoder training by minimizing a reconstruction loss\nL(z, E(G(z))), after or jointly during a regular GAN training, called latent regressor or joint latent\nregressor respectively. We use a sigmoid cross entropy loss L as it naturally maps to a uniformly\ndistributed output space. Intuitively, a drawback of this approach is that, unlike the encoder in a\nBiGAN, the latent regressor encoder E is trained only on generated samples G(z), and never \u201csees\u201d\nreal data x \u223cpX. While this may not be an issue in the theoretical optimum where pG(x) = pX(x)\nexactly \u2013 i.e., G perfectly generates the data distribution pX \u2013 in practice, for highly complex data\ndistributions pX, such as the distribution of natural images, the generator will almost never achieve\nthis perfect result. The fact that the real data x are never input to this type of encoder limits its utility\nas a feature representation for related tasks, as shown later in this section.\n4.2\nPERMUTATION-INVARIANT MNIST\nWe \ufb01rst present results on permutation-invariant MNIST (LeCun et al., 1998). In the permutation-\ninvariant setting, each 28\u00d728 digit image must be treated as an unstructured 784D vector (Goodfellow\net al., 2013). In our case, this condition is met by designing each module as a multi-layer perceptron\n(MLP), agnostic to the underlying spatial structure in the data (as opposed to a convnet, for example).\nSee Appendix C.1 for more architectural and training details. We set the latent distribution pZ =\n[U(\u22121, 1)]50 \u2013 a 50D continuous uniform distribution.\nTable 1 compares the encoding learned by a BiGAN-trained encoder E with the baselines described\nin Section 4.1, as well as autoencoders (Hinton & Salakhutdinov, 2006) trained directly to minimize\neither \u21132 or \u21131 reconstruction error. The same architecture and optimization algorithm is used across\nall methods. All methods, including BiGAN, perform at roughly the same level. This result is not\noverly surprising given the relative simplicity of MNIST digits. For example, digits generated by\nG in a GAN nearly perfectly match the data distribution (qualitatively), making the latent regressor\n(LR) baseline method a reasonable choice, as argued in Section 4.1. Qualitative results are presented\nin Figure 2.\n4.3\nIMAGENET\nNext, we present results from training BiGANs on ImageNet LSVRC (Russakovsky et al., 2015),\na large-scale database of natural images. GANs trained on ImageNet cannot perfectly reconstruct\n7\nPublished as a conference paper at ICLR 2017\nD\nE\nNoroozi & Favaro (2016)\nG\nAlexNet-based D\nKrizhevsky et al. (2012)\nFigure 3: The convolutional \ufb01lters learned by the three modules (D, G, and E) of a BiGAN (left,\ntop-middle) trained on the ImageNet (Russakovsky et al., 2015) database. We compare with the\n\ufb01lters learned by a discriminator D trained with the same architecture (bottom-middle), as well as\nthe \ufb01lters reported by Noroozi & Favaro (2016), and by Krizhevsky et al. (2012) for fully supervised\nImageNet training (right).\nG(z)\nx\nG(E(x))\nx\nG(E(x))\nx\nG(E(x))\nFigure 4: Qualitative results for ImageNet BiGAN training, including generator samples G(z), real\ndata x, and corresponding reconstructions G(E(x)).\nthe data, but often capture some interesting aspects. Here, each of D, G, and E is a convnet. In all\nexperiments, the encoder E architecture follows AlexNet (Krizhevsky et al., 2012) through the \ufb01fth\nand last convolution layer (conv5). We also experiment with an AlexNet-based discriminator D as\na baseline feature learning approach. We set the latent distribution pZ = [U(\u22121, 1)]200 \u2013 a 200D\ncontinuous uniform distribution. Additionally, we experiment with higher resolution encoder input\nimages \u2013 112 \u00d7 112 rather than the 64 \u00d7 64 used elsewhere \u2013 using the generalization described in\nSection 3.5. See Appendix C.2 for more architectural and training details.\nQualitative results\nThe convolutional \ufb01lters learned by each of the three modules are shown in\nFigure 3. We see that the \ufb01lters learned by the encoder E have clear Gabor-like structure, similar to\nthose originally reported for the fully supervised AlexNet model (Krizhevsky et al., 2012). The \ufb01lters\nalso have similar \u201cgrouping\u201d structure where one half (the bottom half, in this case) is more color\nsensitive, and the other half is more edge sensitive. (This separation of the \ufb01lters occurs due to the\nAlexNet architecture maintaining two separate \ufb01lter paths for computational ef\ufb01ciency.)\nIn Figure 4 we present sample generations G(z), as well as real data samples x and their BiGAN re-\nconstructions G(E(x)). The reconstructions, while certainly imperfect, demonstrate empirically that\n8\nPublished as a conference paper at ICLR 2017\nconv1\nconv2\nconv3\nconv4\nconv5\nRandom (Noroozi & Favaro, 2016)\n48.5\n41.0\n34.8\n27.1\n12.0\nWang & Gupta (2015)\n51.8\n46.9\n42.8\n38.8\n29.8\nDoersch et al. (2015)\n53.1\n47.6\n48.7\n45.6\n30.4\nNoroozi & Favaro (2016)*\n57.1\n56.0\n52.4\n48.3\n38.1\nBiGAN (ours)\n56.2\n54.4\n49.4\n43.9\n33.3\nBiGAN, 112 \u00d7 112 E (ours)\n55.3\n53.2\n49.3\n44.4\n34.8\nTable 2: Classi\ufb01cation accuracy (%) for the ImageNet LSVRC (Russakovsky et al., 2015) validation\nset with various portions of the network frozen, or reinitialized and trained from scratch, following\nthe evaluation from Noroozi & Favaro (2016). In, e.g., the conv3 column, the \ufb01rst three layers\n\u2013 conv1 through conv3 \u2013 are transferred and frozen, and the last layers \u2013 conv4, conv5, and fully\nconnected layers \u2013 are reinitialized and trained fully supervised for ImageNet classi\ufb01cation. BiGAN is\ncompetitive with these contemporary visual feature learning methods, despite its generality. (*Results\nfrom Noroozi & Favaro (2016) are not directly comparable to those of the other methods as a different\nbase convnet architecture with larger intermediate feature maps is used.)\nthe BiGAN encoder E and generator G learn approximate inverse mappings, as shown theoretically\nin Theorem 2. In Appendix C.2, we present nearest neighbors in the BiGAN learned feature space.\nImageNet classi\ufb01cation\nFollowing Noroozi & Favaro (2016), we evaluate by freezing the \ufb01rst\nN layers of our pretrained network and randomly reinitializing and training the remainder fully\nsupervised for ImageNet classi\ufb01cation. Results are reported in Table 2.\nVOC classi\ufb01cation, detection, and segmentation\nWe evaluate the transferability of BiGAN rep-\nresentations to the PASCAL VOC (Everingham et al., 2014) computer vision benchmark tasks,\nincluding classi\ufb01cation, object detection, and semantic segmentation. The classi\ufb01cation task involves\nsimple binary prediction of presence or absence in a given image for each of 20 object categories.\nThe object detection and semantic segmentation tasks go a step further by requiring the objects to\nbe localized, with semantic segmentation requiring this at the \ufb01nest scale: pixelwise prediction of\nobject identity. For detection, the pretrained model is used as the initialization for Fast R-CNN (Gir-\nshick, 2015) (FRCN) training; and for semantic segmentation, the model is used as the initialization\nfor Fully Convolutional Network (Long et al., 2015) (FCN) training, in each case replacing the\nAlexNet (Krizhevsky et al., 2012) model trained fully supervised for ImageNet classi\ufb01cation. We\nreport results on each of these tasks in Table 3, comparing BiGANs with contemporary approaches\nto unsupervised (Kr\u00e4henb\u00fchl et al., 2016) and self-supervised (Doersch et al., 2015; Agrawal et al.,\n2015; Wang & Gupta, 2015; Pathak et al., 2016) feature learning in the visual domain, as well as the\nbaselines discussed in Section 4.1.\n4.4\nDISCUSSION\nDespite making no assumptions about the underlying structure of the data, the BiGAN unsupervised\nfeature learning framework offers a representation competitive with existing self-supervised and even\nweakly supervised feature learning approaches for visual feature learning, while still being a purely\ngenerative model with the ability to sample data x and predict latent representation z. Furthermore,\nBiGANs outperform the discriminator (D) and latent regressor (LR) baselines discussed in Section 4.1,\ncon\ufb01rming our intuition that these approaches may not perform well in the regime of highly complex\ndata distributions such as that of natural images. The version in which the encoder takes a higher\nresolution image than output by the generator (BiGAN 112 \u00d7 112 E) performs better still, and this\nstrategy is not possible under the LR and D baselines as each of those modules take generator outputs\nas their input.\nAlthough existing self-supervised approaches have shown impressive performance and thus far tended\nto outshine purely unsupervised approaches in the complex domain of high-resolution images, purely\nunsupervised approaches to feature learning or pre-training have several potential bene\ufb01ts.\n9\nPublished as a conference paper at ICLR 2017\nFRCN\nFCN\nClassi\ufb01cation\nDetection\nSegmentation\n(% mAP)\n(% mAP)\n(% mIU)\ntrained layers\nfc8\nfc6-8\nall\nall\nall\nsup.\nImageNet (Krizhevsky et al., 2012)\n77.0\n78.8\n78.3\n56.8\n48.0\nself-sup.\nAgrawal et al. (2015)\n31.2\n31.0\n54.2\n43.9\n-\nPathak et al. (2016)\n30.5\n34.6\n56.5\n44.5\n30.0\nWang & Gupta (2015)\n28.4\n55.6\n63.1\n47.4\n-\nDoersch et al. (2015)\n44.7\n55.1\n65.3\n51.1\n-\nunsup.\nk-means (Kr\u00e4henb\u00fchl et al., 2016)\n32.0\n39.2\n56.6\n45.6\n32.6\nDiscriminator (D)\n30.7\n40.5\n56.4\n-\n-\nLatent Regressor (LR)\n36.9\n47.9\n57.1\n-\n-\nJoint LR\n37.1\n47.9\n56.5\n-\n-\nAutoencoder (\u21132)\n24.8\n16.0\n53.8\n41.9\n-\nBiGAN (ours)\n37.5\n48.7\n58.9\n46.2\n34.9\nBiGAN, 112 \u00d7 112 E (ours)\n41.7\n52.5\n60.3\n46.9\n35.2\nTable 3: Classi\ufb01cation and Fast R-CNN (Girshick, 2015) detection results for the PASCAL VOC\n2007 (Everingham et al., 2014) test set, and FCN (Long et al., 2015) segmentation results on the\nPASCAL VOC 2012 validation set, under the standard mean average precision (mAP) or mean\nintersection over union (mIU) metrics for each task. Classi\ufb01cation models are trained with various\nportions of the AlexNet (Krizhevsky et al., 2012) model frozen. In the fc8 column, only the linear\nclassi\ufb01er (a multinomial logistic regression) is learned \u2013 in the case of BiGAN, on top of randomly\ninitialized fully connected (FC) layers fc6 and fc7. In the fc6-8 column, all three FC layers are trained\nfully supervised with all convolution layers frozen. Finally, in the all column, the entire network is\n\u201c\ufb01ne-tuned\u201d. BiGAN outperforms other unsupervised (unsup.) feature learning approaches, including\nthe GAN-based baselines described in Section 4.1, and despite its generality, is competitive with\ncontemporary self-supervised (self-sup.) feature learning approaches speci\ufb01c to the visual domain.\nBiGAN and other unsupervised learning approaches are agnostic to the domain of the data. The\nself-supervised approaches are speci\ufb01c to the visual domain, in some cases requiring weak super-\nvision from video unavailable in images alone. For example, the methods are not applicable in the\npermutation-invariant MNIST setting explored in Section 4.2, as the data are treated as \ufb02at vectors\nrather than 2D images.\nFurthermore, BiGAN and other unsupervised approaches needn\u2019t suffer from domain shift between\nthe pre-training task and the transfer task, unlike self-supervised methods in which some aspect of the\ndata is normally removed or corrupted in order to create a non-trivial prediction task. In the context\nprediction task (Doersch et al., 2015), the network sees only small image patches \u2013 the global image\nstructure is unobserved. In the context encoder or inpainting task (Pathak et al., 2016), each image\nis corrupted by removing large areas to be \ufb01lled in by the prediction network, creating inputs with\ndramatically different appearance from the uncorrupted natural images seen in the transfer tasks.\nOther approaches (Agrawal et al., 2015; Wang & Gupta, 2015) rely on auxiliary information un-\navailable in the static image domain, such as video, egomotion, or tracking. Unlike BiGAN, such\napproaches cannot learn feature representations from unlabeled static images.\nWe \ufb01nally note that the results presented here constitute only a preliminary exploration of the space\nof model architectures possible under the BiGAN framework, and we expect results to improve sig-\nni\ufb01cantly with advancements in generative image models and discriminative convolutional networks\nalike.\nACKNOWLEDGMENTS\nThe authors thank Evan Shelhamer, Jonathan Long, and other Berkeley Vision labmates for helpful\ndiscussions throughout this work. This work was supported by DARPA, AFRL, DoD MURI award\nN000141110688, NSF awards IIS-1427425 and IIS-1212798, and the Berkeley Arti\ufb01cial Intelligence\nResearch laboratory. The GPUs used for this work were donated by NVIDIA.\n10\nPublished as a conference paper at ICLR 2017\nREFERENCES\nPulkit Agrawal, Joao Carreira, and Jitendra Malik. Learning to see by moving. In ICCV, 2015.\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\nlearning to align and translate. In ICLR, 2015.\nEmily L. Denton, Soumith Chintala, Arthur Szlam, and Rob Fergus. Deep generative image models\nusing a Laplacian pyramid of adversarial networks. In NIPS, 2015.\nCarl Doersch, Abhinav Gupta, and Alexei A. Efros. Unsupervised visual representation learning by\ncontext prediction. In ICCV, 2015.\nJeff Donahue, Yangqing Jia, Oriol Vinyals, Judy Hoffman, Ning Zhang, Eric Tzeng, and Trevor\nDarrell. DeCAF: A deep convolutional activation feature for generic visual recognition. In ICML,\n2014.\nVincent Dumoulin, Ishmael Belghazi, Ben Poole, Alex Lamb, Martin Arjovsky, Olivier Mastropietro,\nand Aaron Courville. Adversarially learned inference. arXiv:1606.00704, 2016.\nMark Everingham, S. M. Ali Eslami, Luc Van Gool, Christopher K. I. Williams, John Winn, and\nAndrew Zisserman. The PASCAL Visual Object Classes challenge: A retrospective. IJCV, 2014.\nRoss Girshick. Fast R-CNN. In ICCV, 2015.\nRoss Girshick, Jeff Donahue, Trevor Darrell, and Jitendra Malik. Rich feature hierarchies for accurate\nobject detection and semantic segmentation. In CVPR, 2014.\nIan Goodfellow, David Warde-Farley, Mehdi Mirza, Aaron Courville, and Yoshua Bengio. Maxout\nnetworks. In ICML, 2013.\nIan Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,\nAaron Courville, and Yoshua Bengio. Generative adversarial nets. In NIPS, 2014.\nAlex Graves, Abdel-rahman Mohamed, and Geoffrey E. Hinton. Speech recognition with deep\nrecurrent neural networks. In ICASSP, 2013.\nGeoffrey E. Hinton and Ruslan R. Salakhutdinov. Reducing the dimensionality of data with neural\nnetworks. Science, 2006.\nGeoffrey E. Hinton, Simon Osindero, and Yee-Whye Teh. A fast learning algorithm for deep belief\nnets. Neural Computation, 2006.\nSergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by\nreducing internal covariate shift. In ICML, 2015.\nYangqing Jia, Evan Shelhamer, Jeff Donahue, Sergey Karayev, Jonathan Long, Ross Girshick, Sergio\nGuadarrama, and Trevor Darrell. Caffe: Convolutional architecture for fast feature embedding.\narXiv:1408.5093, 2014.\nDiederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\nPhilipp Kr\u00e4henb\u00fchl, Carl Doersch, Jeff Donahue, and Trevor Darrell. Data-dependent initializations\nof convolutional neural networks. In ICLR, 2016.\nAlex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. ImageNet classi\ufb01cation with deep convolu-\ntional neural networks. In NIPS, 2012.\nYann LeCun, L\u00e9on Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to\ndocument recognition. Proc. IEEE, 1998.\nJonathan Long, Evan Shelhamer, and Trevor Darrell. Fully convolutional networks for semantic\nsegmentation. In CVPR, 2015.\nAndrew L. Maas, Awni Y. Hannun, and Andrew Y. Ng. Recti\ufb01er nonlinearities improve neural\nnetwork acoustic models. In ICML, 2013.\n11\nPublished as a conference paper at ICLR 2017\nMehdi Noroozi and Paolo Favaro. Unsupervised learning of visual representations by solving jigsaw\npuzzles. In ECCV, 2016.\nDeepak Pathak, Philipp Kr\u00e4henb\u00fchl, Jeff Donahue, Trevor Darrell, and Alexei A. Efros. Context\nencoders: Feature learning by inpainting. In CVPR, 2016.\nAlec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep\nconvolutional generative adversarial networks. In ICLR, 2016.\nAli Razavian, Hossein Azizpour, Josephine Sullivan, and Stefan Carlsson. CNN features off-the-shelf:\nan astounding baseline for recognition. In CVPR Workshops, 2014.\nOlga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang,\nAndrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Fei-Fei Li. ImageNet\nlarge scale visual recognition challenge. IJCV, 2015.\nRuslan Salakhutdinov and Geoffrey E. Hinton. Deep Boltzmann machines. In AISTATS, 2009.\nIlya Sutskever, Oriol Vinyals, and Quoc V. Le. Sequence to sequence learning with neural networks.\nIn NIPS, 2014.\nTheano Development Team. Theano: A Python framework for fast computation of mathematical\nexpressions. arXiv:1605.02688, 2016.\nOriol Vinyals, \u0141ukasz Kaiser, Terry Koo, Slav Petrov, Ilya Sutskever, and Geoffrey E. Hinton.\nGrammar as a foreign language. In NIPS, 2015.\nXiaolong Wang and Abhinav Gupta. Unsupervised learning of visual representations using videos.\nIn ICCV, 2015.\nMatthew D. Zeiler and Rob Fergus. Visualizing and understanding convolutional networks. In ECCV,\n2014.\n12\nPublished as a conference paper at ICLR 2017\nAPPENDIX A\nADDITIONAL PROOFS\nA.1\nPROOF OF PROPOSITION 1 (OPTIMAL DISCRIMINATOR)\nProposition 1 For any E and G, the optimal discriminator D\u2217\nEG := arg maxD V (D, E, G) is the\nRadon-Nikodym derivative fEG :=\ndPEX\nd(PEX+PGZ) : \u21267\u2192[0, 1] of measure PEX with respect to\nmeasure PEX + PGZ.\nProof. For measures P and Q on space \u2126, with P absolutely continuous with respect to Q, the RN\nderivative fP Q := dP\ndQ exists, and we have\nEx\u223cP [g(x)] =\nR\n\u2126g dP =\nR\n\u2126g dP\ndQ dQ =\nR\n\u2126gfP Q dQ = Ex\u223cQ [fP Q(x)g(x)] .\n(4)\nLet the probability measure PEG :=\nPEX+PGZ\n2\ndenote the average of measures PEX and PGZ.\nBoth PEX and PGZ are each absolutely continuous with respect to PEG. Hence the RN derivatives\nfEG :=\ndPEX\nd(PEX+PGZ) = 1\n2\ndPEX\ndPEG and fGE :=\ndPGZ\nd(PEX+PGZ) = 1\n2\ndPGZ\ndPEG exist and sum to 1:\nfEG + fGE =\ndPEX\nd(PEX+PGZ) +\ndPGZ\nd(PEX+PGZ) = d(PEX+PGZ)\nd(PEX+PGZ) = 1.\n(5)\nWe use (4) and (5) to rewrite the objective V (3) as a single expectation under measure PEG:\nV (D, E, G) = E(x,z)\u223cPEX [log D(x, z)] + E(x,z)\u223cPGZ [log (1 \u2212D(x, z))]\n= E(x,z)\u223cPEG[2fEG\n| {z }\ndPEX\ndPEG\n(x, z) log D(x, z)] + E(x,z)\u223cPEG[2fGE\n| {z }\ndPGZ\ndPEG\n(x, z) log (1 \u2212D(x, z))]\n= 2 E(x,z)\u223cPEG [fEG(x, z) log D(x, z) + fGE(x, z) log (1 \u2212D(x, z))]\n= 2 E(x,z)\u223cPEG [fEG(x, z) log D(x, z) + (1 \u2212fEG(x, z)) log (1 \u2212D(x, z))] .\nNote that arg maxy {a log y + (1 \u2212a) log(1 \u2212y)} = a for any a \u2208[0, 1]. Thus, D\u2217\nEG = fEG. \u25a1\nA.2\nPROOF OF PROPOSITION 2 (ENCODER AND GENERATOR OBJECTIVE)\nProposition 2 The encoder and generator\u2019s objective for an optimal discriminator C(E, G) :=\nmaxD V (D, E, G) = V (D\u2217\nEG, E, G) can be rewritten in terms of the Jensen-Shannon divergence\nbetween measures PEX and PGZ as C(E, G) = 2 DJS (PEX || PGZ ) \u2212log 4.\nProof. Using Proposition 1 along with (5) (1 \u2212D\u2217\nEG = 1 \u2212fEG = fGE) we rewrite the objective\nC(E, G) = maxDV (D, E, G) = V (D\u2217\nEG, E, G)\n= E(x,z)\u223cPEX [log D\u2217\nEG(x, z)] + E(x,z)\u223cPGZ [log (1 \u2212D\u2217\nEG(x, z))]\n= E(x,z)\u223cPEX [log fEG(x, z)] + E(x,z)\u223cPGZ [log fGE(x, z)]\n= E(x,z)\u223cPEX [log (2fEG(x, z))] + E(x,z)\u223cPGZ [log (2fGE(x, z))] \u2212log 4\n= DKL (PEX || PEG ) + DKL (PGZ || PEG ) \u2212log 4\n= DKL\n\u0000PEX\n\f\f\f\f PEX+PGZ\n2\n\u0001\n+ DKL\n\u0000PGZ\n\f\f\f\f PEX+PGZ\n2\n\u0001\n\u2212log 4\n= 2 DJS (PEX || PGZ ) \u2212log 4. \u25a1\nA.3\nMEASURE DEFINITIONS FOR DETERMINISTIC E AND G\nWhile Theorem 1 and Propositions 1 and 2 hold for any encoder pE(z|x) and generator pG(x|z),\nstochastic or deterministic, Theorems 2 and 3 assume the encoder E and generator G are deterministic\nfunctions; i.e., with conditionals pE(z|x) = \u03b4(z \u2212E(x)) and pG(x|z) = \u03b4(x \u2212G(z)) de\ufb01ned as \u03b4\nfunctions.\n13\nPublished as a conference paper at ICLR 2017\nFor use in the proofs of those theorems, we simplify the de\ufb01nitions of measures PEX and PGZ given\nin Section 3 for the case of deterministic functions E and G below:\nPEX(R) =\nR\n\u2126X pX(x)\nR\n\u2126Z pE(z|x)1[(x,z)\u2208R] dz dx\n=\nR\n\u2126X pX(x)\n\u0010R\n\u2126Z \u03b4(z \u2212E(x))1[(x,z)\u2208R] dz\n\u0011\ndx\n=\nR\n\u2126X pX(x)1[(x,E(x))\u2208R] dx\nPGZ(R) =\nR\n\u2126Z pZ(z)\nR\n\u2126X pG(x|z)1[(x,z)\u2208R] dx dz\n=\nR\n\u2126Z pZ(z)\n\u0010R\n\u2126X \u03b4(x \u2212G(z))1[(x,z)\u2208R] dx\n\u0011\ndz\n=\nR\n\u2126Z pZ(z)1[(G(z),z)\u2208R] dz\nA.4\nPROOF OF THEOREM 2 (OPTIMAL GENERATOR AND ENCODER ARE INVERSES)\nTheorem 2 If E and G are an optimal encoder and generator, then E = G\u22121 almost everywhere;\nthat is, G(E(x)) = x for PX-almost every x \u2208\u2126X, and E(G(z)) = z for PZ-almost every z \u2208\u2126Z.\nProof. Let R0\nX := {x \u2208\u2126X : x \u0338= G(E(x))} be the region of \u2126X in which the inversion property\nx = G(E(x)) does not hold. We will show that, for optimal E and G, R0\nX has measure zero under\nPX (i.e., PX(R0\nX) = 0) and therefore x = G(E(x)) holds PX-almost everywhere.\nLet R0 := {(x, z) \u2208\u2126: z = E(x) \u2227x \u2208R0\nX} be the region of \u2126such that (x, E(x)) \u2208R0 if and\nonly if x \u2208R0\nX. We\u2019ll use the de\ufb01nitions of PEX and PGZ for deterministic E and G (Appendix A.3),\nand the fact that PEX = PGZ for optimal E and G (Theorem 1).\nPX(R0\nX) =\nR\n\u2126X pX(x)1[x\u2208R0\nX] dx\n=\nR\n\u2126X pX(x)1[(x,E(x))\u2208R0] dx\n= PEX(R0)\n= PGZ(R0)\n=\nR\n\u2126Z pZ(z)1[(G(z),z)\u2208R0] dz\n=\nR\n\u2126Z pZ(z)1[z=E(G(z)) \u2227G(z)\u2208R0\nX] dz\n=\nR\n\u2126Z pZ(z)\n1[z=E(G(z)) \u2227G(z)\u0338=G(E(G(z)))]\n|\n{z\n}\n=0 for any z, as z=E(G(z)) =\u21d2G(z)=G(E(G(z)))\ndz\n= 0.\nHence region R0\nX has measure zero (PX(R0\nX) = 0), and the inversion property x = G(E(x)) holds\nPX-almost everywhere.\nAn analogous argument shows that R0\nZ := {z \u2208\u2126Z : z \u0338= E(G(z))} has measure zero on PZ (i.e.,\nPZ(R0\nZ) = 0) and therefore z = E(G(z)) holds PZ-almost everywhere. \u25a1\nA.5\nPROOF OF THEOREM 3 (RELATIONSHIP TO AUTOENCODERS)\nAs shown in Proposition 2 (Section 3), the BiGAN objective is equivalent to the Jensen-Shannon\ndivergence between PEX and PGZ. We now go a step further and show that this Jensen-Shannon\ndivergence is closely related to a standard autoencoder loss. Omitting the 1\n2 scale factor, a KL\ndivergence term of the Jensen-Shannon divergence is given as\nDKL\n\u0000PEX\n\f\f\f\f PEX+PGZ\n2\n\u0001\n= log 2 +\nZ\n\u2126\nlog\ndPEX\nd(PEX + PGZ) dPEX\n= log 2 +\nZ\n\u2126\nlog f dPEX,\n(6)\nwhere we abbreviate as f the Radon-Nikodym derivative fEG :=\ndPEX\nd(PEX+PGZ) \u2208[0, 1] de\ufb01ned in\nProposition 1 for most of this proof.\n14\nPublished as a conference paper at ICLR 2017\nWe\u2019ll make use of the de\ufb01nitions of PEX and PGZ for deterministic E and G found in Appendix A.3.\nThe integral term of the KL divergence expression given in (6) over a particular region R \u2286\u2126will\nbe denoted by\nF(R) :=\nZ\nR\nlog\ndPEX\nd (PEX + PGZ) dPEX =\nZ\nR\nlog f dPEX.\nNext we will show that f > 0 holds PEX-almost everywhere, and hence F is always well de\ufb01ned\nand \ufb01nite. We then show that F is equivalent to an autoencoder-like reconstruction loss function.\nProposition 3 f > 0 PEX-almost everywhere.\nProof. Let Rf=0 := {(x, z) \u2208\u2126: f(x, z) = 0} be the region of \u2126in which f = 0. Using the\nde\ufb01nition of the Radon-Nikodym derivative f, the measure PEX(Rf=0) =\nR\nRf=0 f d(PEX +\nPGZ) =\nR\nRf=0 0 d(PEX + PGZ) = 0 is zero. Hence f > 0 PEX-almost everywhere. \u25a1\nProposition 3 ensures that log f is de\ufb01ned PEX-almost everywhere, and F(R) is well-de\ufb01ned. Next\nwe will show that F(R) mimics an autoencoder with \u21130 loss, meaning F is zero for any region in\nwhich G(E(x)) \u0338= x, and non-zero otherwise.\nProposition 4 The KL divergence F outside the support of PGZ is zero: F(\u2126\\ supp(PGZ)) = 0.\nWe\u2019ll \ufb01rst show that in region RS := \u2126\\ supp(PGZ), we have f = 1 PEX-almost everywhere.\nLet Rf<1 := {(x, z) \u2208RS : f(x, z) < 1} be the region of RS in which f < 1. Let\u2019s assume that\nPEX(Rf<1) > 0 has non-zero measure. Then, using the de\ufb01nition of the Radon-Nikodym derivative,\nPEX(Rf<1) =\nR\nRf<1 f d(PEX + PGZ) =\nR\nRf<1\nf\n|{z}\n\u2264\u03b5<1\ndPEX +\nR\nRf<1 f dPGZ\n|\n{z\n}\n0\n\u2264\u03b5PEX(Rf<1)\n< PEX(Rf<1),\nwhere \u03b5 is a constant smaller than 1. But PEX(Rf<1) < PEX(Rf<1) is a contradiction; hence\nPEX(Rf<1) = 0 and f = 1 PEX-almost everywhere in RS, implying log f = 0 PEX-almost\neverywhere in RS. Hence F(RS) = 0. \u25a1\nBy de\ufb01nition, F(\u2126\\ supp(PEX)) = 0 is also zero. The only region where F might be non-zero is\nR1 := supp(PEX) \u2229supp(PGZ).\nProposition 5 f < 1 PEX-almost everywhere in R1.\nLet Rf=1 :=\n\b\n(x, z) \u2208R1 : f(x, z) = 1\n\t\nbe the region in which f = 1. Let\u2019s assume the set\nRf=1 \u0338= \u2205is not empty. By de\ufb01nition of the support1, PEX(Rf=1) > 0 and PGZ(Rf=1) > 0. The\nRadon-Nikodym derivative on Rf=1 is then given by\nPEX(Rf=1) =\nR\nRf=1 f d(PEX + PGZ) =\nR\nRf=1 1 d(PEX + PGZ) = PEX(Rf=1) + PGZ(Rf=1),\nwhich implies PGZ(Rf=1) = 0 and contradicts the de\ufb01nition of support. Hence Rf=1 = \u2205and\nf < 1 PEX-almost everywhere on R1, implying log f < 0 PEX-almost everywhere. \u25a1\nTheorem 3 The encoder and generator objective given an optimal discriminator C(E, G) :=\nmaxD V (D, E, G) can be rewritten as an \u21130 autoencoder loss function\nC(E, G) = Ex\u223cpX\nh\n1[E(x)\u2208\u02c6\u2126Z\u2227G(E(x))=x] log fEG(x, E(x))\ni\n+\nEz\u223cpZ\nh\n1[G(z)\u2208\u02c6\u2126X\u2227E(G(z))=z] log (1 \u2212fEG(G(z), z))\ni\nwith log fEG \u2208(\u2212\u221e, 0) and log (1 \u2212fEG) \u2208(\u2212\u221e, 0) PEX-almost and PGZ-almost everywhere.\nProof. Proposition 4 (F(\u2126\\ supp(PGZ)) = 0) and F(\u2126\\ supp(PEX)) = 0 imply that R1 :=\nsupp(PEX) \u2229supp(PGZ) is the only region of \u2126where F may be non-zero; hence F(\u2126) = F(R1).\n1We use the de\ufb01nition U \u2229C \u0338= \u2205=\u21d2\u00b5(U \u2229C) > 0 here.\n15\nPublished as a conference paper at ICLR 2017\nNote that\nsupp(PEX) = {(x, E(x)) : x \u2208\u02c6\u2126X}\nsupp(PGZ) = {(G(z), z) : z \u2208\u02c6\u2126Z}\n=\u21d2R1 := supp(PEX) \u2229supp(PGZ) = {(x, z) : E(x) = z \u2227x \u2208\u02c6\u2126X \u2227G(z) = x \u2227z \u2208\u02c6\u2126Z}\nSo a point (x, E(x)) is in R1 if x \u2208\u02c6\u2126X, E(x) \u2208\u02c6\u2126Z, and G(E(x)) = x. (We can omit the\nx \u2208\u02c6\u2126X condition from inside an expectation over PX, as PX-almost all x /\u2208\u02c6\u2126X have 0 probability.)\nTherefore,\nDKL\n\u0000PEX\n\f\f\f\f PEX+PGZ\n2\n\u0001\n\u2212log 2 = F(\u2126) = F(R1)\n=\nR\nR1 log f(x, z) dPEX\n=\nR\n\u21261[(x,z)\u2208R1] log f(x, z) dPEX\n= E(x,z)\u223cPEX\n\u0002\n1[(x,z)\u2208R1] log f(x, z)\n\u0003\n= Ex\u223cpX\n\u0002\n1[(x,E(x))\u2208R1] log f(x, E(x))\n\u0003\n= Ex\u223cpX\nh\n1[E(x)\u2208\u02c6\u2126Z\u2227G(E(x))=x] log f(x, E(x))\ni\n.\nFinally, with Propositions 3 and 5, we have f \u2208(0, 1) PEX-almost everywhere in R1, and therefore\nlog f \u2208(\u2212\u221e, 0), taking a \ufb01nite and strictly negative value PEX-almost everywhere.\nAn analogous argument (along with the fact that fEG + fGE = 1) lets us rewrite the other KL\ndivergence term\nDKL\n\u0000PGZ\n\f\f\f\f PEX+PGZ\n2\n\u0001\n\u2212log 2 = Ez\u223cpZ\nh\n1[G(z)\u2208\u02c6\u2126X\u2227E(G(z))=z] log fGE(G(z), z)\ni\n= Ez\u223cpZ\nh\n1[G(z)\u2208\u02c6\u2126X\u2227E(G(z))=z] log (1 \u2212fEG(G(z), z))\ni\nThe Jensen-Shannon divergence is the mean of these two KL divergences, giving C(E, G):\nC(E, G) = 2 DJS (PEX || PGZ ) \u2212log 4\n= DKL\n\u0000PEX\n\f\f\f\f PEX+PGZ\n2\n\u0001\n+ DKL\n\u0000PGZ\n\f\f\f\f PEX+PGZ\n2\n\u0001\n\u2212log 4\n= Ex\u223cpX\nh\n1[E(x)\u2208\u02c6\u2126Z\u2227G(E(x))=x] log fEG(x, E(x))\ni\n+\nEz\u223cpZ\nh\n1[G(z)\u2208\u02c6\u2126X\u2227E(G(z))=z] log (1 \u2212fEG(G(z), z))\ni\n\u25a1\nAPPENDIX B\nLEARNING DETAILS\nIn this section we provide additional details on the BiGAN learning protocol summarized in Sec-\ntion 3.4. Goodfellow et al. (2014) found for GAN training that an objective in which the real and\ngenerated labels Y are swapped provides stronger gradient signal to G. We similarly observed in\nBiGAN training that an \u201cinverse\u201d objective \u039b (with the same \ufb01xed point characteristics as V ) provides\nstronger gradient signal to G and E, where\n\u039b(D, G, E) = Ex\u223cpX\n\u0002\nEz\u223cpE(\u00b7|x) [log (1 \u2212D(x, z))]\n|\n{z\n}\nlog(1\u2212D(x,E(x)))\n\u0003\n+ Ez\u223cpZ\n\u0002\nEx\u223cpG(\u00b7|z) [log D(x, z)]\n|\n{z\n}\nlog D(G(z),z)\n\u0003\n.\nIn practice, \u03b8G and \u03b8E are updated by moving in the positive gradient direction of this inverse\nobjective \u2207\u03b8E,\u03b8G\u039b, rather than the negative gradient direction of the original objective.\nWe also observed that learning behaved similarly when all parameters \u03b8D, \u03b8G, \u03b8E were updated\nsimultaneously at each iteration rather than alternating between \u03b8D updates and \u03b8G, \u03b8E updates, so\nwe took the simultaneous updating (non-alternating) approach for computational ef\ufb01ciency. (For\nstandard GAN training, simultaneous updates of \u03b8D, \u03b8G performed similarly well, so our standard\nGAN experiments also follow this protocol.)\n16\nPublished as a conference paper at ICLR 2017\nAPPENDIX C\nMODEL AND TRAINING DETAILS\nIn the following sections we present additional details on the models and training protocols used in\nthe permutation-invariant MNIST and ImageNet evaluations presented in Section 4.\nOptimization\nFor unsupervised training of BiGANs and baseline methods, we use the Adam\noptimizer (Kingma & Ba, 2015) to compute parameter updates, following the hyperparameters (initial\nstep size \u03b1 = 2 \u00d7 10\u22124, momentum \u03b21 = 0.5 and \u03b22 = 0.999) used by Radford et al. (2016).\nThe step size \u03b1 is decayed exponentially to \u03b1 = 2 \u00d7 10\u22126 starting halfway through training. The\nmini-batch size is 128. \u21132 weight decay of 2.5 \u00d7 10\u22125 is applied to all multiplicative weights in\nlinear layers (but not to the learned bias \u03b2 or scale \u03b3 parameters applied after batch normalization).\nWeights are initialized from a zero-mean normal distribution with a standard deviation of 0.02, with\none notable exception: BiGAN discriminator weights that directly multiply z inputs to be added to\nspatial convolution outputs have initializations scaled by the convolution kernel size \u2013 e.g., for a 5 \u00d7 5\nkernel, weights are initialized with a standard deviation of 0.5, 25 times the standard initialization.\nSoftware & hardware\nWe implement BiGANs and baseline feature learning methods using the\nTheano (Theano Development Team, 2016) framework, based on the convolutional GAN implemen-\ntation provided by Radford et al. (2016). ImageNet transfer learning experiments (Section 4.3) use\nthe Caffe (Jia et al., 2014) framework, per the Fast R-CNN (Girshick, 2015) and FCN (Long et al.,\n2015) reference implementations. Most computation is performed on an NVIDIA Titan X or Tesla\nK40 GPU.\nC.1\nPERMUTATION-INVARIANT MNIST\nIn all permutation-invariant MNIST experiments (Section 4.2), D, G, and E each consist of two\nhidden layers with 1024 units. The \ufb01rst hidden layer is followed by a non-linearity; the second is\nfollowed by (parameter-free) batch normalization (Ioffe & Szegedy, 2015) and a non-linearity. The\nsecond hidden layer in each case is the input to a linear prediction layer of the appropriate size. In D\nand E, a leaky ReLU (Maas et al., 2013) non-linearity with a \u201cleak\u201d of 0.2 is used; in G, a standard\nReLU non-linearity is used. All models are trained for 400 epochs.\nC.2\nIMAGENET\nIn all ImageNet experiments (Section 4.3), the encoder E architecture follows AlexNet (Krizhevsky\net al., 2012) through the \ufb01fth and last convolution layer (conv5), with local response normalization\n(LRN) layers removed and batch normalization (Ioffe & Szegedy, 2015) (including the learned scaling\nand bias) with leaky ReLU non-linearity applied to the output of each convolution at unsupervised\ntraining time. (For supervised evaluation, batch normalization is not used, and the pre-trained scale\nand bias is merged into the preceding convolution\u2019s weights and bias.)\nIn most experiments, both the discriminator D and generator G architecture are those used by Radford\net al. (2016), consisting of a series of four 5 \u00d7 5 convolutions (or \u201cdeconvolutions\u201d \u2013 fractionally-\nstrided convolutions \u2013 for the generator G) applied with 2 pixel stride, each followed by batch\nnormalization and recti\ufb01ed non-linearity.\nThe sole exception is our discriminator baseline feature learning experiment, in which we let the\ndiscriminator D be the AlexNet variant described above. Generally, using AlexNet (or similar convnet\narchitecture) as the discriminator D is detrimental to the visual \ufb01delity of the resulting generated\nimages, likely due to the relatively large convolutional \ufb01lter kernel size applied to the input image, as\nwell as the max-pooling layers, which explicitly discard information in the input. However, for fair\ncomparison of the discriminator\u2019s feature learning abilities with those of BiGANs, we use the same\narchitecture as used in the BiGAN encoder.\nPreprocessing\nTo produce a data sample x, we \ufb01rst sample an image from the database, and resize\nit proportionally such that its shorter edge has a length of 72 pixels. Then, a 64 \u00d7 64 crop is randomly\nselected from the resized image. The crop is \ufb02ipped horizontally with probability 1\n2. Finally, the crop\nis scaled to [\u22121, 1], giving the sample x.\n17\nPublished as a conference paper at ICLR 2017\nQuery\n#1\n#2\n#3\n#4\nFigure 5: For the query images used in Kr\u00e4henb\u00fchl et al. (2016) (left), nearest neighbors (by minimum\ncosine distance) from the ImageNet LSVRC (Russakovsky et al., 2015) training set in the fc6 feature\nspace of the ImageNet-trained BiGAN encoder E. (The fc6 weights are set randomly; this space is a\nrandom projection of the learned conv5 feature space.)\nTiming\nA single epoch (one training pass over the 1.2 million images) of BiGAN training takes\nroughly 40 minutes on a Titan X GPU. Models are trained for 100 epochs, for a total training time of\nunder 3 days.\nNearest neighbors\nIn Figure 5 we present nearest neighbors in the feature space of the BiGAN\nencoder E learned in unsupervised ImageNet training.\n18\n",
        "sentence": " Several strategies exist to learn deep convolutional features with no annotation (Donahue et al., 2016). While some of these approaches obtain promising performance in transfer learning (Donahue et al., 2016; Wang & Gupta, 2015), they do not explicitly aim to learn discriminative features. We test the quality of our features on several image classification problems, following the setting of Donahue et al. (2016). We are on par with state-of-the-art unsupervised and self-supervised learning approaches while being much simpler to train and to scale. Among those approaches, generative adversarial networks (GANs) (Goodfellow et al., 2014; Denton et al., 2015; Donahue et al., 2016) share another similarity with our approach, namely they are explicitly minimizing a discriminative loss to learn their features. For the transfer learning experiments, we follow the guideline described in Donahue et al. (2016). 4. We precisely follow the training and testing procedure that is specific to each of the datasets following Donahue et al. (2016). Like BiGANs (Donahue et al., 2016), NAT does not make any assumption about the domain but of the structure of its features. Among unsupervised approaches, NAT compares favorably to BiGAN (Donahue et al., 2016). BiGAN (Donahue et al., 2016) 32. , BiGAN (Donahue et al., 2016). , BiGAN (Donahue et al., 2016). Noroozi & Favaro (2016) uses a significantly larger amount of features than the original AlexNet. Depending on the task, we finetune all layers in the network, or solely the classifier, following Donahue et al. (2016). In all experiments, the parameters of the convolutional layers are initialized with the ones obtained with our unsupervised approach. Depending on the task, we finetune all layers in the network, or solely the classifier, following Donahue et al. (2016). In all experiments, the parameters of the convolutional layers are initialized with the ones obtained with our unsupervised approach. The parameters of the classification layers are initialized with gaussian weights. We get rid of batch normalization layers and use a data-dependent rescaling of the parameters (Kr\u00e4henb\u00fchl et al., 2015). Table 4 shows the comparison between our model and other unsupervised approaches. The results for other methods are taken from Donahue et al. (2016) except for Zhang et al. Depending on the task, we finetune all layers in the network, or solely the classifier, following Donahue et al. (2016). In all experiments, the parameters of the convolutional layers are initialized with the ones obtained with our unsupervised approach. The parameters of the classification layers are initialized with gaussian weights. We get rid of batch normalization layers and use a data-dependent rescaling of the parameters (Kr\u00e4henb\u00fchl et al., 2015). Table 4 shows the comparison between our model and other unsupervised approaches. The results for other methods are taken from Donahue et al. (2016) except for Zhang et al. (2016). 4 BiGAN (Donahue et al., 2016) 52. The GAN and autoencoder baselines are from Donahue et al. (2016). We report mean average prevision as customary on PASCAL VOC. performs slightly better than the best performing BiGAN model (Donahue et al., 2016).",
        "context": "the GAN-based baselines described in Section 4.1, and despite its generality, is competitive with\ncontemporary self-supervised (self-sup.) feature learning approaches speci\ufb01c to the visual domain.\nunsupervised approaches to feature learning or pre-training have several potential bene\ufb01ts.\n9\nPublished as a conference paper at ICLR 2017\nFRCN\nFCN\nClassi\ufb01cation\nDetection\nSegmentation\n(% mAP)\n(% mAP)\n(% mIU)\ntrained layers\nfc8\nfc6-8\nall\nall\nall\nsup.\nencoders: Feature learning by inpainting. In CVPR, 2016.\nAlec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep\nconvolutional generative adversarial networks. In ICLR, 2016."
    },
    {
        "title": "Discriminative unsupervised feature learning with convolutional neural networks",
        "author": [
            "A. Dosovitskiy",
            "J. Springenberg",
            "M. Riedmiller",
            "T. Brox"
        ],
        "venue": "In NIPS,",
        "citeRegEx": "Dosovitskiy et al\\.,? \\Q2014\\E",
        "shortCiteRegEx": "Dosovitskiy et al\\.",
        "year": 2014,
        "abstract": "",
        "full_text": "",
        "sentence": " Some attempts were made with retrieval based approaches (Dosovitskiy et al., 2014) and clustering (Yang et al. Several approaches have been recently proposed to tackle the problem of deep unsupervised learning (Coates & Ng, 2012; Mairal et al., 2014; Dosovitskiy et al., 2014). , 2014; Dosovitskiy et al., 2014). Some of them are based on a clustering loss (Xie et al., 2016; Yang et al., 2016; Liao et al., 2016), but they are not tested at a scale comparable to that of supervised convnet training. Coates & Ng (2012) uses k-means to pre-train convnets, by learning each layer sequentially in a bottom-up fashion. , 2014; Dosovitskiy et al., 2014). Some of them are based on a clustering loss (Xie et al., 2016; Yang et al., 2016; Liao et al., 2016), but they are not tested at a scale comparable to that of supervised convnet training. Coates & Ng (2012) uses k-means to pre-train convnets, by learning each layer sequentially in a bottom-up fashion. In our work, we train the convnet end-to-end with a loss that shares similarities with k-means. Closer to our work, Dosovitskiy et al. (2014) proposes to train convnets by solving a retrieval problem. If d is larger than n, this formulation would be similar to the framework of Dosovitskiy et al. (2014), and is impractical for large n. If d is larger than n, this formulation would be similar to the framework of Dosovitskiy et al. (2014), and is impractical for large n. On the other hand, if d is smaller than n, this formulation is equivalent to the discriminative clustering approach of Bach & Harchaoui (2007). Choosing such targets makes very strong assumptions on the nature of the underlying problem.",
        "context": null
    },
    {
        "title": "The PASCAL visual object classes (VOC) challenge",
        "author": [
            "M. Everingham",
            "L. Van Gool",
            "C.K.I. Williams",
            "J. Winn",
            "A. Zisserman"
        ],
        "venue": null,
        "citeRegEx": "Everingham et al\\.,? \\Q2010\\E",
        "shortCiteRegEx": "Everingham et al\\.",
        "year": 2010,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " We then evaluate the quality of our features by comparing them to state-of-the-art unsupervised approaches on several auxiliary supervised tasks, namely object classification on ImageNet and object classification and detection of PASCAL VOC 2007 (Everingham et al., 2010).",
        "context": null
    },
    {
        "title": "Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position",
        "author": [
            "K. Fukushima"
        ],
        "venue": "Biological Cybernetics,",
        "citeRegEx": "Fukushima,? \\Q1980\\E",
        "shortCiteRegEx": "Fukushima",
        "year": 1980,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " In recent years, convolutional neural networks, or convnets (Fukushima, 1980; LeCun et al., 1989) have pushed the limits of computer vision (Krizhevsky et al.",
        "context": null
    },
    {
        "title": "Fast r-cnn",
        "author": [
            "R. Girshick"
        ],
        "venue": "In CVPR,",
        "citeRegEx": "Girshick,? \\Q2015\\E",
        "shortCiteRegEx": "Girshick",
        "year": 2015,
        "abstract": "This paper proposes a Fast Region-based Convolutional Network method (Fast\nR-CNN) for object detection. Fast R-CNN builds on previous work to efficiently\nclassify object proposals using deep convolutional networks. Compared to\nprevious work, Fast R-CNN employs several innovations to improve training and\ntesting speed while also increasing detection accuracy. Fast R-CNN trains the\nvery deep VGG16 network 9x faster than R-CNN, is 213x faster at test-time, and\nachieves a higher mAP on PASCAL VOC 2012. Compared to SPPnet, Fast R-CNN trains\nVGG16 3x faster, tests 10x faster, and is more accurate. Fast R-CNN is\nimplemented in Python and C++ (using Caffe) and is available under the\nopen-source MIT License at https://github.com/rbgirshick/fast-rcnn.",
        "full_text": "Fast R-CNN\nRoss Girshick\nMicrosoft Research\nrbg@microsoft.com\nAbstract\nThis paper proposes a Fast Region-based Convolutional\nNetwork method (Fast R-CNN) for object detection. Fast\nR-CNN builds on previous work to ef\ufb01ciently classify ob-\nject proposals using deep convolutional networks. Com-\npared to previous work, Fast R-CNN employs several in-\nnovations to improve training and testing speed while also\nincreasing detection accuracy. Fast R-CNN trains the very\ndeep VGG16 network 9\u00d7 faster than R-CNN, is 213\u00d7 faster\nat test-time, and achieves a higher mAP on PASCAL VOC\n2012. Compared to SPPnet, Fast R-CNN trains VGG16 3\u00d7\nfaster, tests 10\u00d7 faster, and is more accurate. Fast R-CNN\nis implemented in Python and C++ (using Caffe) and is\navailable under the open-source MIT License at https:\n//github.com/rbgirshick/fast-rcnn.\n1. Introduction\nRecently, deep ConvNets [14, 16] have signi\ufb01cantly im-\nproved image classi\ufb01cation [14] and object detection [9, 19]\naccuracy. Compared to image classi\ufb01cation, object detec-\ntion is a more challenging task that requires more com-\nplex methods to solve. Due to this complexity, current ap-\nproaches (e.g., [9, 11, 19, 25]) train models in multi-stage\npipelines that are slow and inelegant.\nComplexity arises because detection requires the ac-\ncurate localization of objects, creating two primary chal-\nlenges. First, numerous candidate object locations (often\ncalled \u201cproposals\u201d) must be processed. Second, these can-\ndidates provide only rough localization that must be re\ufb01ned\nto achieve precise localization. Solutions to these problems\noften compromise speed, accuracy, or simplicity.\nIn this paper, we streamline the training process for state-\nof-the-art ConvNet-based object detectors [9, 11]. We pro-\npose a single-stage training algorithm that jointly learns to\nclassify object proposals and re\ufb01ne their spatial locations.\nThe resulting method can train a very deep detection\nnetwork (VGG16 [20]) 9\u00d7 faster than R-CNN [9] and 3\u00d7\nfaster than SPPnet [11]. At runtime, the detection network\nprocesses images in 0.3s (excluding object proposal time)\nwhile achieving top accuracy on PASCAL VOC 2012 [7]\nwith a mAP of 66% (vs. 62% for R-CNN).1\n1.1. R-CNN and SPPnet\nThe Region-based Convolutional Network method (R-\nCNN) [9] achieves excellent object detection accuracy by\nusing a deep ConvNet to classify object proposals. R-CNN,\nhowever, has notable drawbacks:\n1. Training is a multi-stage pipeline. R-CNN \ufb01rst \ufb01ne-\ntunes a ConvNet on object proposals using log loss.\nThen, it \ufb01ts SVMs to ConvNet features. These SVMs\nact as object detectors, replacing the softmax classi-\n\ufb01er learnt by \ufb01ne-tuning. In the third training stage,\nbounding-box regressors are learned.\n2. Training is expensive in space and time. For SVM\nand bounding-box regressor training, features are ex-\ntracted from each object proposal in each image and\nwritten to disk.\nWith very deep networks, such as\nVGG16, this process takes 2.5 GPU-days for the 5k\nimages of the VOC07 trainval set. These features re-\nquire hundreds of gigabytes of storage.\n3. Object detection is slow. At test-time, features are\nextracted from each object proposal in each test image.\nDetection with VGG16 takes 47s / image (on a GPU).\nR-CNN is slow because it performs a ConvNet forward\npass for each object proposal, without sharing computation.\nSpatial pyramid pooling networks (SPPnets) [11] were pro-\nposed to speed up R-CNN by sharing computation. The\nSPPnet method computes a convolutional feature map for\nthe entire input image and then classi\ufb01es each object pro-\nposal using a feature vector extracted from the shared fea-\nture map. Features are extracted for a proposal by max-\npooling the portion of the feature map inside the proposal\ninto a \ufb01xed-size output (e.g., 6 \u00d7 6). Multiple output sizes\nare pooled and then concatenated as in spatial pyramid pool-\ning [15]. SPPnet accelerates R-CNN by 10 to 100\u00d7 at test\ntime. Training time is also reduced by 3\u00d7 due to faster pro-\nposal feature extraction.\n1All timings use one Nvidia K40 GPU overclocked to 875 MHz.\narXiv:1504.08083v2  [cs.CV]  27 Sep 2015\nSPPnet also has notable drawbacks. Like R-CNN, train-\ning is a multi-stage pipeline that involves extracting fea-\ntures, \ufb01ne-tuning a network with log loss, training SVMs,\nand \ufb01nally \ufb01tting bounding-box regressors. Features are\nalso written to disk. But unlike R-CNN, the \ufb01ne-tuning al-\ngorithm proposed in [11] cannot update the convolutional\nlayers that precede the spatial pyramid pooling. Unsurpris-\ningly, this limitation (\ufb01xed convolutional layers) limits the\naccuracy of very deep networks.\n1.2. Contributions\nWe propose a new training algorithm that \ufb01xes the disad-\nvantages of R-CNN and SPPnet, while improving on their\nspeed and accuracy. We call this method Fast R-CNN be-\ncause it\u2019s comparatively fast to train and test. The Fast R-\nCNN method has several advantages:\n1. Higher detection quality (mAP) than R-CNN, SPPnet\n2. Training is single-stage, using a multi-task loss\n3. Training can update all network layers\n4. No disk storage is required for feature caching\nFast R-CNN is written in Python and C++ (Caffe\n[13]) and is available under the open-source MIT Li-\ncense\nat\nhttps://github.com/rbgirshick/\nfast-rcnn.\n2. Fast R-CNN architecture and training\nFig. 1 illustrates the Fast R-CNN architecture. A Fast\nR-CNN network takes as input an entire image and a set\nof object proposals. The network \ufb01rst processes the whole\nimage with several convolutional (conv) and max pooling\nlayers to produce a conv feature map. Then, for each ob-\nject proposal a region of interest (RoI) pooling layer ex-\ntracts a \ufb01xed-length feature vector from the feature map.\nEach feature vector is fed into a sequence of fully connected\n(fc) layers that \ufb01nally branch into two sibling output lay-\ners: one that produces softmax probability estimates over\nK object classes plus a catch-all \u201cbackground\u201d class and\nanother layer that outputs four real-valued numbers for each\nof the K object classes. Each set of 4 values encodes re\ufb01ned\nbounding-box positions for one of the K classes.\n2.1. The RoI pooling layer\nThe RoI pooling layer uses max pooling to convert the\nfeatures inside any valid region of interest into a small fea-\nture map with a \ufb01xed spatial extent of H \u00d7 W (e.g., 7 \u00d7 7),\nwhere H and W are layer hyper-parameters that are inde-\npendent of any particular RoI. In this paper, an RoI is a\nrectangular window into a conv feature map. Each RoI is\nde\ufb01ned by a four-tuple (r, c, h, w) that speci\ufb01es its top-left\ncorner (r, c) and its height and width (h, w).\nDeep\nConvNet\nConv\nfeature map\nRoI\nprojection\nRoI\npooling\nlayer\nFCs\nRoI feature\nvector\nsoftmax\nbbox\nregressor\nOutputs:\nFC\nFC\nFor each RoI\nFigure 1. Fast R-CNN architecture. An input image and multi-\nple regions of interest (RoIs) are input into a fully convolutional\nnetwork. Each RoI is pooled into a \ufb01xed-size feature map and\nthen mapped to a feature vector by fully connected layers (FCs).\nThe network has two output vectors per RoI: softmax probabilities\nand per-class bounding-box regression offsets. The architecture is\ntrained end-to-end with a multi-task loss.\nRoI max pooling works by dividing the h \u00d7 w RoI win-\ndow into an H \u00d7 W grid of sub-windows of approximate\nsize h/H \u00d7 w/W and then max-pooling the values in each\nsub-window into the corresponding output grid cell. Pool-\ning is applied independently to each feature map channel,\nas in standard max pooling. The RoI layer is simply the\nspecial-case of the spatial pyramid pooling layer used in\nSPPnets [11] in which there is only one pyramid level. We\nuse the pooling sub-window calculation given in [11].\n2.2. Initializing from pre-trained networks\nWe experiment with three pre-trained ImageNet [4] net-\nworks, each with \ufb01ve max pooling layers and between \ufb01ve\nand thirteen conv layers (see Section 4.1 for network de-\ntails). When a pre-trained network initializes a Fast R-CNN\nnetwork, it undergoes three transformations.\nFirst, the last max pooling layer is replaced by a RoI\npooling layer that is con\ufb01gured by setting H and W to be\ncompatible with the net\u2019s \ufb01rst fully connected layer (e.g.,\nH = W = 7 for VGG16).\nSecond, the network\u2019s last fully connected layer and soft-\nmax (which were trained for 1000-way ImageNet classi\ufb01-\ncation) are replaced with the two sibling layers described\nearlier (a fully connected layer and softmax over K +1 cat-\negories and category-speci\ufb01c bounding-box regressors).\nThird, the network is modi\ufb01ed to take two data inputs: a\nlist of images and a list of RoIs in those images.\n2.3. Fine-tuning for detection\nTraining all network weights with back-propagation is an\nimportant capability of Fast R-CNN. First, let\u2019s elucidate\nwhy SPPnet is unable to update weights below the spatial\npyramid pooling layer.\nThe root cause is that back-propagation through the SPP\nlayer is highly inef\ufb01cient when each training sample (i.e.\nRoI) comes from a different image, which is exactly how\nR-CNN and SPPnet networks are trained. The inef\ufb01ciency\nstems from the fact that each RoI may have a very large\nreceptive \ufb01eld, often spanning the entire input image. Since\nthe forward pass must process the entire receptive \ufb01eld, the\ntraining inputs are large (often the entire image).\nWe propose a more ef\ufb01cient training method that takes\nadvantage of feature sharing during training. In Fast R-\nCNN training, stochastic gradient descent (SGD) mini-\nbatches are sampled hierarchically, \ufb01rst by sampling N im-\nages and then by sampling R/N RoIs from each image.\nCritically, RoIs from the same image share computation\nand memory in the forward and backward passes. Making\nN small decreases mini-batch computation. For example,\nwhen using N = 2 and R = 128, the proposed training\nscheme is roughly 64\u00d7 faster than sampling one RoI from\n128 different images (i.e., the R-CNN and SPPnet strategy).\nOne concern over this strategy is it may cause slow train-\ning convergence because RoIs from the same image are cor-\nrelated. This concern does not appear to be a practical issue\nand we achieve good results with N = 2 and R = 128\nusing fewer SGD iterations than R-CNN.\nIn addition to hierarchical sampling, Fast R-CNN uses a\nstreamlined training process with one \ufb01ne-tuning stage that\njointly optimizes a softmax classi\ufb01er and bounding-box re-\ngressors, rather than training a softmax classi\ufb01er, SVMs,\nand regressors in three separate stages [9, 11]. The compo-\nnents of this procedure (the loss, mini-batch sampling strat-\negy, back-propagation through RoI pooling layers, and SGD\nhyper-parameters) are described below.\nMulti-task loss.\nA Fast R-CNN network has two sibling\noutput layers. The \ufb01rst outputs a discrete probability distri-\nbution (per RoI), p = (p0, . . . , pK), over K + 1 categories.\nAs usual, p is computed by a softmax over the K+1 outputs\nof a fully connected layer. The second sibling layer outputs\nbounding-box regression offsets, tk =\n\u0000tk\nx, tk\ny, tk\nw, tk\nh\n\u0001\n, for\neach of the K object classes, indexed by k. We use the pa-\nrameterization for tk given in [9], in which tk speci\ufb01es a\nscale-invariant translation and log-space height/width shift\nrelative to an object proposal.\nEach training RoI is labeled with a ground-truth class u\nand a ground-truth bounding-box regression target v. We\nuse a multi-task loss L on each labeled RoI to jointly train\nfor classi\ufb01cation and bounding-box regression:\nL(p, u, tu, v) = Lcls(p, u) + \u03bb[u \u22651]Lloc(tu, v),\n(1)\nin which Lcls(p, u) = \u2212log pu is log loss for true class u.\nThe second task loss, Lloc, is de\ufb01ned over a tuple of\ntrue bounding-box regression targets for class u, v\n=\n(vx, vy, vw, vh), and a predicted tuple tu = (tu\nx , tu\ny , tu\nw, tu\nh ),\nagain for class u. The Iverson bracket indicator function\n[u \u22651] evaluates to 1 when u \u22651 and 0 otherwise. By\nconvention the catch-all background class is labeled u = 0.\nFor background RoIs there is no notion of a ground-truth\nbounding box and hence Lloc is ignored. For bounding-box\nregression, we use the loss\nLloc(tu, v) =\nX\ni\u2208{x,y,w,h}\nsmoothL1(tu\ni \u2212vi),\n(2)\nin which\nsmoothL1(x) =\n(\n0.5x2\nif |x| < 1\n|x| \u22120.5\notherwise,\n(3)\nis a robust L1 loss that is less sensitive to outliers than the\nL2 loss used in R-CNN and SPPnet. When the regression\ntargets are unbounded, training with L2 loss can require\ncareful tuning of learning rates in order to prevent exploding\ngradients. Eq. 3 eliminates this sensitivity.\nThe hyper-parameter \u03bb in Eq. 1 controls the balance be-\ntween the two task losses. We normalize the ground-truth\nregression targets vi to have zero mean and unit variance.\nAll experiments use \u03bb = 1.\nWe note that [6] uses a related loss to train a class-\nagnostic object proposal network. Different from our ap-\nproach, [6] advocates for a two-network system that sepa-\nrates localization and classi\ufb01cation. OverFeat [19], R-CNN\n[9], and SPPnet [11] also train classi\ufb01ers and bounding-box\nlocalizers, however these methods use stage-wise training,\nwhich we show is suboptimal for Fast R-CNN (Section 5.1).\nMini-batch sampling.\nDuring \ufb01ne-tuning, each SGD\nmini-batch is constructed from N = 2 images, chosen uni-\nformly at random (as is common practice, we actually iter-\nate over permutations of the dataset). We use mini-batches\nof size R = 128, sampling 64 RoIs from each image. As\nin [9], we take 25% of the RoIs from object proposals that\nhave intersection over union (IoU) overlap with a ground-\ntruth bounding box of at least 0.5. These RoIs comprise\nthe examples labeled with a foreground object class, i.e.\nu \u22651. The remaining RoIs are sampled from object pro-\nposals that have a maximum IoU with ground truth in the in-\nterval [0.1, 0.5), following [11]. These are the background\nexamples and are labeled with u = 0. The lower threshold\nof 0.1 appears to act as a heuristic for hard example mining\n[8]. During training, images are horizontally \ufb02ipped with\nprobability 0.5. No other data augmentation is used.\nBack-propagation through RoI pooling layers.\nBack-\npropagation routes derivatives through the RoI pooling\nlayer. For clarity, we assume only one image per mini-batch\n(N = 1), though the extension to N > 1 is straightforward\nbecause the forward pass treats all images independently.\nLet xi \u2208R be the i-th activation input into the RoI pool-\ning layer and let yrj be the layer\u2019s j-th output from the r-\nth RoI. The RoI pooling layer computes yrj = xi\u2217(r,j), in\nwhich i\u2217(r, j) = argmaxi\u2032\u2208R(r,j) xi\u2032. R(r, j) is the index\nset of inputs in the sub-window over which the output unit\nyrj max pools. A single xi may be assigned to several dif-\nferent outputs yrj.\nThe RoI pooling layer\u2019s backwards function computes\npartial derivative of the loss function with respect to each\ninput variable xi by following the argmax switches:\n\u2202L\n\u2202xi\n=\nX\nr\nX\nj\n[i = i\u2217(r, j)] \u2202L\n\u2202yrj\n.\n(4)\nIn words, for each mini-batch RoI r and for each pooling\noutput unit yrj, the partial derivative \u2202L/\u2202yrj is accumu-\nlated if i is the argmax selected for yrj by max pooling.\nIn back-propagation, the partial derivatives \u2202L/\u2202yrj are al-\nready computed by the backwards function of the layer\non top of the RoI pooling layer.\nSGD hyper-parameters.\nThe fully connected layers used\nfor softmax classi\ufb01cation and bounding-box regression are\ninitialized from zero-mean Gaussian distributions with stan-\ndard deviations 0.01 and 0.001, respectively. Biases are ini-\ntialized to 0. All layers use a per-layer learning rate of 1 for\nweights and 2 for biases and a global learning rate of 0.001.\nWhen training on VOC07 or VOC12 trainval we run SGD\nfor 30k mini-batch iterations, and then lower the learning\nrate to 0.0001 and train for another 10k iterations. When\nwe train on larger datasets, we run SGD for more iterations,\nas described later. A momentum of 0.9 and parameter decay\nof 0.0005 (on weights and biases) are used.\n2.4. Scale invariance\nWe explore two ways of achieving scale invariant ob-\nject detection: (1) via \u201cbrute force\u201d learning and (2) by us-\ning image pyramids. These strategies follow the two ap-\nproaches in [11]. In the brute-force approach, each image\nis processed at a pre-de\ufb01ned pixel size during both training\nand testing. The network must directly learn scale-invariant\nobject detection from the training data.\nThe multi-scale approach, in contrast, provides approx-\nimate scale-invariance to the network through an image\npyramid. At test-time, the image pyramid is used to ap-\nproximately scale-normalize each object proposal. During\nmulti-scale training, we randomly sample a pyramid scale\neach time an image is sampled, following [11], as a form of\ndata augmentation. We experiment with multi-scale train-\ning for smaller networks only, due to GPU memory limits.\n3. Fast R-CNN detection\nOnce a Fast R-CNN network is \ufb01ne-tuned, detection\namounts to little more than running a forward pass (assum-\ning object proposals are pre-computed). The network takes\nas input an image (or an image pyramid, encoded as a list\nof images) and a list of R object proposals to score. At\ntest-time, R is typically around 2000, although we will con-\nsider cases in which it is larger (\u224845k). When using an\nimage pyramid, each RoI is assigned to the scale such that\nthe scaled RoI is closest to 2242 pixels in area [11].\nFor each test RoI r, the forward pass outputs a class\nposterior probability distribution p and a set of predicted\nbounding-box offsets relative to r (each of the K classes\ngets its own re\ufb01ned bounding-box prediction). We assign a\ndetection con\ufb01dence to r for each object class k using the\nestimated probability Pr(class = k | r)\n\u2206= pk. We then\nperform non-maximum suppression independently for each\nclass using the algorithm and settings from R-CNN [9].\n3.1. Truncated SVD for faster detection\nFor whole-image classi\ufb01cation, the time spent comput-\ning the fully connected layers is small compared to the conv\nlayers. On the contrary, for detection the number of RoIs\nto process is large and nearly half of the forward pass time\nis spent computing the fully connected layers (see Fig. 2).\nLarge fully connected layers are easily accelerated by com-\npressing them with truncated SVD [5, 23].\nIn this technique, a layer parameterized by the u \u00d7 v\nweight matrix W is approximately factorized as\nW \u2248U\u03a3tV T\n(5)\nusing SVD. In this factorization, U is a u \u00d7 t matrix com-\nprising the \ufb01rst t left-singular vectors of W, \u03a3t is a t \u00d7 t\ndiagonal matrix containing the top t singular values of W,\nand V is v \u00d7 t matrix comprising the \ufb01rst t right-singular\nvectors of W. Truncated SVD reduces the parameter count\nfrom uv to t(u + v), which can be signi\ufb01cant if t is much\nsmaller than min(u, v). To compress a network, the single\nfully connected layer corresponding to W is replaced by\ntwo fully connected layers, without a non-linearity between\nthem. The \ufb01rst of these layers uses the weight matrix \u03a3tV T\n(and no biases) and the second uses U (with the original bi-\nases associated with W). This simple compression method\ngives good speedups when the number of RoIs is large.\n4. Main results\nThree main results support this paper\u2019s contributions:\n1. State-of-the-art mAP on VOC07, 2010, and 2012\n2. Fast training and testing compared to R-CNN, SPPnet\n3. Fine-tuning conv layers in VGG16 improves mAP\n4.1. Experimental setup\nOur experiments use three pre-trained ImageNet models\nthat are available online.2 The \ufb01rst is the CaffeNet (essen-\ntially AlexNet [14]) from R-CNN [9]. We alternatively refer\n2https://github.com/BVLC/caffe/wiki/Model-Zoo\nmethod\ntrain set\naero bike\nbird\nboat bottle\nbus\ncar\ncat\nchair\ncow\ntable\ndog\nhorse mbike persn plant sheep\nsofa\ntrain\ntv\nmAP\nSPPnet BB [11]\u2020\n07 \\ diff 73.9 72.3 62.5 51.5\n44.4\n74.4 73.0 74.4\n42.3\n73.6\n57.7\n70.3\n74.6\n74.3\n54.2\n34.0\n56.4\n56.4\n67.9 73.5\n63.1\nR-CNN BB [10]\n07\n73.4 77.0 63.4 45.4\n44.6\n75.1 78.1 79.8\n40.5\n73.7\n62.2\n79.4\n78.1\n73.1\n64.2\n35.6\n66.8\n67.2 70.4\n71.1\n66.0\nFRCN [ours]\n07\n74.5 78.3 69.2 53.2\n36.6\n77.3 78.2 82.0\n40.7\n72.7\n67.9\n79.6\n79.2\n73.0\n69.0\n30.1\n65.4\n70.2\n75.8 65.8\n66.9\nFRCN [ours]\n07 \\ diff 74.6 79.0 68.6 57.0\n39.3\n79.5 78.6 81.9\n48.0\n74.0\n67.4\n80.5\n80.7\n74.1\n69.6\n31.8\n67.1\n68.4\n75.3\n65.5\n68.1\nFRCN [ours]\n07+12\n77.0 78.1 69.3 59.4\n38.3\n81.6 78.6 86.7\n42.8\n78.8\n68.9\n84.7\n82.0\n76.6\n69.9\n31.8\n70.1\n74.8 80.4\n70.4\n70.0\nTable 1. VOC 2007 test detection average precision (%). All methods use VGG16. Training set key: 07: VOC07 trainval, 07 \\ diff: 07\nwithout \u201cdif\ufb01cult\u201d examples, 07+12: union of 07 and VOC12 trainval. \u2020SPPnet results were prepared by the authors of [11].\nmethod\ntrain set\naero bike\nbird\nboat bottle\nbus\ncar\ncat\nchair\ncow\ntable\ndog\nhorse mbike persn plant sheep\nsofa\ntrain\ntv\nmAP\nBabyLearning\nProp.\n77.7 73.8 62.3 48.8\n45.4\n67.3 67.0 80.3\n41.3\n70.8\n49.7\n79.5\n74.7\n78.6\n64.5\n36.0\n69.9\n55.7\n70.4 61.7\n63.8\nR-CNN BB [10]\n12\n79.3 72.4 63.1 44.0\n44.4\n64.6 66.3 84.9\n38.8\n67.3\n48.4\n82.3\n75.0\n76.7\n65.7\n35.8\n66.2\n54.8\n69.1 58.8\n62.9\nSegDeepM\n12+seg\n82.3 75.2 67.1 50.7\n49.8\n71.1 69.6 88.2\n42.5\n71.2\n50.0\n85.7\n76.6\n81.8\n69.3\n41.5\n71.9\n62.2\n73.2\n64.6\n67.2\nFRCN [ours]\n12\n80.1 74.4 67.7 49.4\n41.4\n74.2 68.8 87.8\n41.9\n70.1\n50.2\n86.1\n77.3\n81.1\n70.4\n33.3\n67.0\n63.3\n77.2 60.0\n66.1\nFRCN [ours]\n07++12\n82.0 77.8 71.6 55.3\n42.4\n77.3 71.7 89.3\n44.5\n72.1\n53.7\n87.7\n80.0\n82.5\n72.7\n36.6\n68.7\n65.4\n81.1\n62.7\n68.8\nTable 2. VOC 2010 test detection average precision (%). BabyLearning uses a network based on [17]. All other methods use VGG16.\nTraining set key: 12: VOC12 trainval, Prop.: proprietary dataset, 12+seg: 12 with segmentation annotations, 07++12: union of VOC07\ntrainval, VOC07 test, and VOC12 trainval.\nmethod\ntrain set\naero bike\nbird\nboat bottle\nbus\ncar\ncat\nchair\ncow\ntable\ndog\nhorse mbike persn plant sheep\nsofa\ntrain\ntv\nmAP\nBabyLearning\nProp.\n78.0 74.2 61.3 45.7\n42.7\n68.2 66.8 80.2\n40.6\n70.0\n49.8\n79.0\n74.5\n77.9\n64.0\n35.3\n67.9\n55.7\n68.7 62.6\n63.2\nNUS NIN c2000\nUnk.\n80.2 73.8 61.9 43.7\n43.0\n70.3 67.6 80.7\n41.9\n69.7\n51.7\n78.2\n75.2\n76.9\n65.1\n38.6\n68.3\n58.0\n68.7\n63.3\n63.8\nR-CNN BB [10]\n12\n79.6 72.7 61.9 41.2\n41.9\n65.9 66.4 84.6\n38.5\n67.2\n46.7\n82.0\n74.8\n76.0\n65.2\n35.6\n65.4\n54.2\n67.4 60.3\n62.4\nFRCN [ours]\n12\n80.3 74.7 66.9 46.9\n37.7\n73.9 68.6 87.7\n41.7\n71.1\n51.1\n86.0\n77.8\n79.8\n69.8\n32.1\n65.5\n63.8\n76.4 61.7\n65.7\nFRCN [ours]\n07++12\n82.3 78.4 70.8 52.3\n38.7\n77.8 71.6 89.3\n44.2\n73.0\n55.0\n87.5\n80.5\n80.8\n72.0\n35.1\n68.3\n65.7 80.4\n64.2\n68.4\nTable 3. VOC 2012 test detection average precision (%). BabyLearning and NUS NIN c2000 use networks based on [17]. All other\nmethods use VGG16. Training set key: see Table 2, Unk.: unknown.\nto this CaffeNet as model S, for \u201csmall.\u201d The second net-\nwork is VGG CNN M 1024 from [3], which has the same\ndepth as S, but is wider. We call this network model M,\nfor \u201cmedium.\u201d The \ufb01nal network is the very deep VGG16\nmodel from [20]. Since this model is the largest, we call\nit model L. In this section, all experiments use single-scale\ntraining and testing (s = 600; see Section 5.2 for details).\n4.2. VOC 2010 and 2012 results\nOn these datasets, we compare Fast R-CNN (FRCN, for\nshort) against the top methods on the comp4 (outside data)\ntrack from the public leaderboard (Table 2, Table 3).3 For\nthe NUS NIN c2000 and BabyLearning methods, there are\nno associated publications at this time and we could not\n\ufb01nd exact information on the ConvNet architectures used;\nthey are variants of the Network-in-Network design [17].\nAll other methods are initialized from the same pre-trained\nVGG16 network.\nFast R-CNN achieves the top result on VOC12 with a\nmAP of 65.7% (and 68.4% with extra data). It is also two\norders of magnitude faster than the other methods, which\nare all based on the \u201cslow\u201d R-CNN pipeline. On VOC10,\n3http://host.robots.ox.ac.uk:8080/leaderboard\n(accessed April 18, 2015)\nSegDeepM [25] achieves a higher mAP than Fast R-CNN\n(67.2% vs. 66.1%). SegDeepM is trained on VOC12 train-\nval plus segmentation annotations; it is designed to boost\nR-CNN accuracy by using a Markov random \ufb01eld to reason\nover R-CNN detections and segmentations from the O2P\n[1] semantic-segmentation method.\nFast R-CNN can be\nswapped into SegDeepM in place of R-CNN, which may\nlead to better results.\nWhen using the enlarged 07++12\ntraining set (see Table 2 caption), Fast R-CNN\u2019s mAP in-\ncreases to 68.8%, surpassing SegDeepM.\n4.3. VOC 2007 results\nOn VOC07, we compare Fast R-CNN to R-CNN and\nSPPnet.\nAll methods start from the same pre-trained\nVGG16 network and use bounding-box regression.\nThe\nVGG16 SPPnet results were computed by the authors of\n[11]. SPPnet uses \ufb01ve scales during both training and test-\ning. The improvement of Fast R-CNN over SPPnet illus-\ntrates that even though Fast R-CNN uses single-scale train-\ning and testing, \ufb01ne-tuning the conv layers provides a large\nimprovement in mAP (from 63.1% to 66.9%).\nR-CNN\nachieves a mAP of 66.0%. As a minor point, SPPnet was\ntrained without examples marked as \u201cdif\ufb01cult\u201d in PASCAL.\nRemoving these examples improves Fast R-CNN mAP to\n68.1%. All other experiments use \u201cdif\ufb01cult\u201d examples.\n4.4. Training and testing time\nFast training and testing times are our second main re-\nsult. Table 4 compares training time (hours), testing rate\n(seconds per image), and mAP on VOC07 between Fast R-\nCNN, R-CNN, and SPPnet. For VGG16, Fast R-CNN pro-\ncesses images 146\u00d7 faster than R-CNN without truncated\nSVD and 213\u00d7 faster with it. Training time is reduced by\n9\u00d7, from 84 hours to 9.5. Compared to SPPnet, Fast R-\nCNN trains VGG16 2.7\u00d7 faster (in 9.5 vs. 25.5 hours) and\ntests 7\u00d7 faster without truncated SVD or 10\u00d7 faster with it.\nFast R-CNN also eliminates hundreds of gigabytes of disk\nstorage, because it does not cache features.\nFast R-CNN\nR-CNN\nSPPnet\nS\nM\nL\nS\nM\nL\n\u2020L\ntrain time (h)\n1.2\n2.0\n9.5\n22\n28\n84\n25\ntrain speedup\n18.3\u00d7 14.0\u00d7\n8.8\u00d7\n1\u00d7\n1\u00d7\n1\u00d7\n3.4\u00d7\ntest rate (s/im)\n0.10\n0.15\n0.32\n9.8 12.1 47.0\n2.3\n\u25b7with SVD\n0.06\n0.08\n0.22\n-\n-\n-\n-\ntest speedup\n98\u00d7\n80\u00d7 146\u00d7\n1\u00d7\n1\u00d7\n1\u00d7\n20\u00d7\n\u25b7with SVD\n169\u00d7\n150\u00d7 213\u00d7\n-\n-\n-\n-\nVOC07 mAP\n57.1\n59.2\n66.9 58.5 60.2 66.0\n63.1\n\u25b7with SVD\n56.5\n58.7\n66.6\n-\n-\n-\n-\nTable 4. Runtime comparison between the same models in Fast R-\nCNN, R-CNN, and SPPnet. Fast R-CNN uses single-scale mode.\nSPPnet uses the \ufb01ve scales speci\ufb01ed in [11]. \u2020Timing provided by\nthe authors of [11]. Times were measured on an Nvidia K40 GPU.\nTruncated SVD.\nTruncated SVD can reduce detection\ntime by more than 30% with only a small (0.3 percent-\nage point) drop in mAP and without needing to perform\nadditional \ufb01ne-tuning after model compression. Fig. 2 il-\nlustrates how using the top 1024 singular values from the\n25088 \u00d7 4096 matrix in VGG16\u2019s fc6 layer and the top 256\nsingular values from the 4096\u00d74096 fc7 layer reduces run-\ntime with little loss in mAP. Further speed-ups are possi-\nble with smaller drops in mAP if one \ufb01ne-tunes again after\ncompression.\nroi_pool5\n5.4% (17ms)\nother\n3.5% (11ms)\nfc6\n38.7% (122ms)\nconv\n46.3% (146ms)\nfc7\n6.2% (20ms)\nForward pass timing\nmAP 66.9% @ 320ms / image\nroi_pool5\n7.9% (17ms)\nother\n5.1% (11ms)\nfc6\n17.5% (37ms)\nconv\n67.8% (143ms)\nfc7\n1.7% (4ms)\nForward pass timing (SVD)\nmAP 66.6% @ 223ms / image\nFigure 2. Timing for VGG16 before and after truncated SVD. Be-\nfore SVD, fully connected layers fc6 and fc7 take 45% of the time.\n4.5. Which layers to \ufb01ne-tune?\nFor the less deep networks considered in the SPPnet pa-\nper [11], \ufb01ne-tuning only the fully connected layers ap-\npeared to be suf\ufb01cient for good accuracy. We hypothesized\nthat this result would not hold for very deep networks. To\nvalidate that \ufb01ne-tuning the conv layers is important for\nVGG16, we use Fast R-CNN to \ufb01ne-tune, but freeze the\nthirteen conv layers so that only the fully connected layers\nlearn. This ablation emulates single-scale SPPnet training\nand decreases mAP from 66.9% to 61.4% (Table 5). This\nexperiment veri\ufb01es our hypothesis: training through the RoI\npooling layer is important for very deep nets.\nlayers that are \ufb01ne-tuned in model L SPPnet L\n\u2265fc6 \u2265conv3 1\n\u2265conv2 1\n\u2265fc6\nVOC07 mAP\n61.4\n66.9\n67.2\n63.1\ntest rate (s/im)\n0.32\n0.32\n0.32\n2.3\nTable 5. Effect of restricting which layers are \ufb01ne-tuned for\nVGG16. Fine-tuning \u2265fc6 emulates the SPPnet training algo-\nrithm [11], but using a single scale. SPPnet L results were ob-\ntained using \ufb01ve scales, at a signi\ufb01cant (7\u00d7) speed cost.\nDoes this mean that all conv layers should be \ufb01ne-tuned?\nIn short, no. In the smaller networks (S and M) we \ufb01nd\nthat conv1 is generic and task independent (a well-known\nfact [14]). Allowing conv1 to learn, or not, has no mean-\ningful effect on mAP. For VGG16, we found it only nec-\nessary to update layers from conv3 1 and up (9 of the 13\nconv layers). This observation is pragmatic: (1) updating\nfrom conv2 1 slows training by 1.3\u00d7 (12.5 vs. 9.5 hours)\ncompared to learning from conv3 1; and (2) updating from\nconv1 1 over-runs GPU memory. The difference in mAP\nwhen learning from conv2 1 up was only +0.3 points (Ta-\nble 5, last column). All Fast R-CNN results in this paper\nusing VGG16 \ufb01ne-tune layers conv3 1 and up; all experi-\nments with models S and M \ufb01ne-tune layers conv2 and up.\n5. Design evaluation\nWe conducted experiments to understand how Fast R-\nCNN compares to R-CNN and SPPnet, as well as to eval-\nuate design decisions. Following best practices, we per-\nformed these experiments on the PASCAL VOC07 dataset.\n5.1. Does multi-task training help?\nMulti-task training is convenient because it avoids man-\naging a pipeline of sequentially-trained tasks. But it also has\nthe potential to improve results because the tasks in\ufb02uence\neach other through a shared representation (the ConvNet)\n[2]. Does multi-task training improve object detection ac-\ncuracy in Fast R-CNN?\nTo test this question, we train baseline networks that\nuse only the classi\ufb01cation loss, Lcls, in Eq. 1 (i.e., setting\nS\nM\nL\nmulti-task training?\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\nstage-wise training?\n\u2713\n\u2713\n\u2713\ntest-time bbox reg?\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\nVOC07 mAP\n52.2\n53.3\n54.6\n57.1\n54.7\n55.5\n56.6\n59.2\n62.6\n63.4\n64.0\n66.9\nTable 6. Multi-task training (forth column per group) improves mAP over piecewise training (third column per group).\n\u03bb = 0). These baselines are printed for models S, M, and L\nin the \ufb01rst column of each group in Table 6. Note that these\nmodels do not have bounding-box regressors. Next (second\ncolumn per group), we take networks that were trained with\nthe multi-task loss (Eq. 1, \u03bb = 1), but we disable bounding-\nbox regression at test time. This isolates the networks\u2019 clas-\nsi\ufb01cation accuracy and allows an apples-to-apples compar-\nison with the baseline networks.\nAcross all three networks we observe that multi-task\ntraining improves pure classi\ufb01cation accuracy relative to\ntraining for classi\ufb01cation alone. The improvement ranges\nfrom +0.8 to +1.1 mAP points, showing a consistent posi-\ntive effect from multi-task learning.\nFinally, we take the baseline models (trained with only\nthe classi\ufb01cation loss), tack on the bounding-box regression\nlayer, and train them with Lloc while keeping all other net-\nwork parameters frozen. The third column in each group\nshows the results of this stage-wise training scheme: mAP\nimproves over column one, but stage-wise training under-\nperforms multi-task training (forth column per group).\n5.2. Scale invariance: to brute force or \ufb01nesse?\nWe compare two strategies for achieving scale-invariant\nobject detection: brute-force learning (single scale) and im-\nage pyramids (multi-scale). In either case, we de\ufb01ne the\nscale s of an image to be the length of its shortest side.\nAll single-scale experiments use s = 600 pixels; s may\nbe less than 600 for some images as we cap the longest im-\nage side at 1000 pixels and maintain the image\u2019s aspect ra-\ntio. These values were selected so that VGG16 \ufb01ts in GPU\nmemory during \ufb01ne-tuning. The smaller models are not\nmemory bound and can bene\ufb01t from larger values of s; how-\never, optimizing s for each model is not our main concern.\nWe note that PASCAL images are 384 \u00d7 473 pixels on av-\nerage and thus the single-scale setting typically upsamples\nimages by a factor of 1.6. The average effective stride at the\nRoI pooling layer is thus \u224810 pixels.\nIn the multi-scale setting, we use the same \ufb01ve scales\nspeci\ufb01ed in [11] (s \u2208{480, 576, 688, 864, 1200}) to facili-\ntate comparison with SPPnet. However, we cap the longest\nside at 2000 pixels to avoid exceeding GPU memory.\nTable 7 shows models S and M when trained and tested\nwith either one or \ufb01ve scales. Perhaps the most surpris-\ning result in [11] was that single-scale detection performs\nalmost as well as multi-scale detection. Our \ufb01ndings con-\nSPPnet ZF\nS\nM\nL\nscales\n1\n5\n1\n5\n1\n5\n1\ntest rate (s/im)\n0.14\n0.38\n0.10\n0.39\n0.15\n0.64\n0.32\nVOC07 mAP\n58.0\n59.2\n57.1\n58.4\n59.2\n60.7\n66.9\nTable 7. Multi-scale vs. single scale. SPPnet ZF (similar to model\nS) results are from [11]. Larger networks with a single-scale offer\nthe best speed / accuracy tradeoff. (L cannot use multi-scale in our\nimplementation due to GPU memory constraints.)\n\ufb01rm their result: deep ConvNets are adept at directly learn-\ning scale invariance. The multi-scale approach offers only\na small increase in mAP at a large cost in compute time\n(Table 7). In the case of VGG16 (model L), we are lim-\nited to using a single scale by implementation details. Yet it\nachieves a mAP of 66.9%, which is slightly higher than the\n66.0% reported for R-CNN [10], even though R-CNN uses\n\u201cin\ufb01nite\u201d scales in the sense that each proposal is warped to\na canonical size.\nSince single-scale processing offers the best tradeoff be-\ntween speed and accuracy, especially for very deep models,\nall experiments outside of this sub-section use single-scale\ntraining and testing with s = 600 pixels.\n5.3. Do we need more training data?\nA good object detector should improve when supplied\nwith more training data. Zhu et al. [24] found that DPM [8]\nmAP saturates after only a few hundred to thousand train-\ning examples. Here we augment the VOC07 trainval set\nwith the VOC12 trainval set, roughly tripling the number\nof images to 16.5k, to evaluate Fast R-CNN. Enlarging the\ntraining set improves mAP on VOC07 test from 66.9% to\n70.0% (Table 1). When training on this dataset we use 60k\nmini-batch iterations instead of 40k.\nWe perform similar experiments for VOC10 and 2012,\nfor which we construct a dataset of 21.5k images from the\nunion of VOC07 trainval, test, and VOC12 trainval. When\ntraining on this dataset, we use 100k SGD iterations and\nlower the learning rate by 0.1\u00d7 each 40k iterations (instead\nof each 30k). For VOC10 and 2012, mAP improves from\n66.1% to 68.8% and from 65.7% to 68.4%, respectively.\n5.4. Do SVMs outperform softmax?\nFast R-CNN uses the softmax classi\ufb01er learnt during\n\ufb01ne-tuning instead of training one-vs-rest linear SVMs\npost-hoc, as was done in R-CNN and SPPnet. To under-\nstand the impact of this choice, we implemented post-hoc\nSVM training with hard negative mining in Fast R-CNN.\nWe use the same training algorithm and hyper-parameters\nas in R-CNN.\nmethod\nclassi\ufb01er\nS\nM\nL\nR-CNN [9, 10]\nSVM\n58.5\n60.2\n66.0\nFRCN [ours]\nSVM\n56.3\n58.7\n66.8\nFRCN [ours]\nsoftmax\n57.1\n59.2\n66.9\nTable 8. Fast R-CNN with softmax vs. SVM (VOC07 mAP).\nTable 8 shows softmax slightly outperforming SVM for\nall three networks, by +0.1 to +0.8 mAP points. This ef-\nfect is small, but it demonstrates that \u201cone-shot\u201d \ufb01ne-tuning\nis suf\ufb01cient compared to previous multi-stage training ap-\nproaches. We note that softmax, unlike one-vs-rest SVMs,\nintroduces competition between classes when scoring a RoI.\n5.5. Are more proposals always better?\nThere are (broadly) two types of object detectors: those\nthat use a sparse set of object proposals (e.g., selective\nsearch [21]) and those that use a dense set (e.g., DPM [8]).\nClassifying sparse proposals is a type of cascade [22] in\nwhich the proposal mechanism \ufb01rst rejects a vast number of\ncandidates leaving the classi\ufb01er with a small set to evaluate.\nThis cascade improves detection accuracy when applied to\nDPM detections [21]. We \ufb01nd evidence that the proposal-\nclassi\ufb01er cascade also improves Fast R-CNN accuracy.\nUsing selective search\u2019s quality mode, we sweep from 1k\nto 10k proposals per image, each time re-training and re-\ntesting model M. If proposals serve a purely computational\nrole, increasing the number of proposals per image should\nnot harm mAP.\n103\n104\nNumber of object proposals\n49\n51\n53\n56\n58\n61\n63\n66\nmAP\nSel. Search (SS)\nSS (2k) + Rand Dense\nSS replace Dense\n45k Dense Softmax\n45k Dense SVM\n49\n51\n53\n56\n58\n61\n63\n66\nAverage Recall\nSS Avg. Recall\nFigure 3. VOC07 test mAP and AR for various proposal schemes.\nWe \ufb01nd that mAP rises and then falls slightly as the pro-\nposal count increases (Fig. 3, solid blue line). This exper-\niment shows that swamping the deep classi\ufb01er with more\nproposals does not help, and even slightly hurts, accuracy.\nThis result is dif\ufb01cult to predict without actually running\nthe experiment. The state-of-the-art for measuring object\nproposal quality is Average Recall (AR) [12]. AR correlates\nwell with mAP for several proposal methods using R-CNN,\nwhen using a \ufb01xed number of proposals per image. Fig. 3\nshows that AR (solid red line) does not correlate well with\nmAP as the number of proposals per image is varied. AR\nmust be used with care; higher AR due to more proposals\ndoes not imply that mAP will increase. Fortunately, training\nand testing with model M takes less than 2.5 hours. Fast\nR-CNN thus enables ef\ufb01cient, direct evaluation of object\nproposal mAP, which is preferable to proxy metrics.\nWe also investigate Fast R-CNN when using densely\ngenerated boxes (over scale, position, and aspect ratio), at\na rate of about 45k boxes / image. This dense set is rich\nenough that when each selective search box is replaced by\nits closest (in IoU) dense box, mAP drops only 1 point (to\n57.7%, Fig. 3, blue triangle).\nThe statistics of the dense boxes differ from those of\nselective search boxes. Starting with 2k selective search\nboxes, we test mAP when adding a random sample of\n1000 \u00d7 {2, 4, 6, 8, 10, 32, 45} dense boxes. For each exper-\niment we re-train and re-test model M. When these dense\nboxes are added, mAP falls more strongly than when adding\nmore selective search boxes, eventually reaching 53.0%.\nWe also train and test Fast R-CNN using only dense\nboxes (45k / image). This setting yields a mAP of 52.9%\n(blue diamond). Finally, we check if SVMs with hard nega-\ntive mining are needed to cope with the dense box distribu-\ntion. SVMs do even worse: 49.3% (blue circle).\n5.6. Preliminary MS COCO results\nWe applied Fast R-CNN (with VGG16) to the MS\nCOCO dataset [18] to establish a preliminary baseline. We\ntrained on the 80k image training set for 240k iterations and\nevaluated on the \u201ctest-dev\u201d set using the evaluation server.\nThe PASCAL-style mAP is 35.9%; the new COCO-style\nAP, which also averages over IoU thresholds, is 19.7%.\n6. Conclusion\nThis paper proposes Fast R-CNN, a clean and fast update\nto R-CNN and SPPnet. In addition to reporting state-of-the-\nart detection results, we present detailed experiments that\nwe hope provide new insights. Of particular note, sparse\nobject proposals appear to improve detector quality. This\nissue was too costly (in time) to probe in the past, but be-\ncomes practical with Fast R-CNN. Of course, there may ex-\nist yet undiscovered techniques that allow dense boxes to\nperform as well as sparse proposals. Such methods, if de-\nveloped, may help further accelerate object detection.\nAcknowledgements.\nI thank Kaiming He, Larry Zitnick,\nand Piotr Doll\u00b4ar for helpful discussions and encouragement.\nReferences\n[1] J. Carreira, R. Caseiro, J. Batista, and C. Sminchisescu. Se-\nmantic segmentation with second-order pooling. In ECCV,\n2012. 5\n[2] R. Caruana. Multitask learning. Machine learning, 28(1),\n1997. 6\n[3] K. Chat\ufb01eld, K. Simonyan, A. Vedaldi, and A. Zisserman.\nReturn of the devil in the details: Delving deep into convo-\nlutional nets. In BMVC, 2014. 5\n[4] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-\nFei. ImageNet: A large-scale hierarchical image database.\nIn CVPR, 2009. 2\n[5] E. Denton, W. Zaremba, J. Bruna, Y. LeCun, and R. Fergus.\nExploiting linear structure within convolutional networks for\nef\ufb01cient evaluation. In NIPS, 2014. 4\n[6] D. Erhan, C. Szegedy, A. Toshev, and D. Anguelov. Scalable\nobject detection using deep neural networks. In CVPR, 2014.\n3\n[7] M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn, and\nA. Zisserman. The PASCAL Visual Object Classes (VOC)\nChallenge. IJCV, 2010. 1\n[8] P. Felzenszwalb, R. Girshick, D. McAllester, and D. Ra-\nmanan. Object detection with discriminatively trained part\nbased models. TPAMI, 2010. 3, 7, 8\n[9] R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich fea-\nture hierarchies for accurate object detection and semantic\nsegmentation. In CVPR, 2014. 1, 3, 4, 8\n[10] R. Girshick, J. Donahue, T. Darrell, and J. Malik. Region-\nbased convolutional networks for accurate object detection\nand segmentation. TPAMI, 2015. 5, 7, 8\n[11] K. He, X. Zhang, S. Ren, and J. Sun. Spatial pyramid pooling\nin deep convolutional networks for visual recognition.\nIn\nECCV, 2014. 1, 2, 3, 4, 5, 6, 7\n[12] J. H. Hosang, R. Benenson, P. Doll\u00b4ar, and B. Schiele. What\nmakes for effective detection proposals?\narXiv preprint\narXiv:1502.05082, 2015. 8\n[13] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Gir-\nshick, S. Guadarrama, and T. Darrell. Caffe: Convolutional\narchitecture for fast feature embedding. In Proc. of the ACM\nInternational Conf. on Multimedia, 2014. 2\n[14] A. Krizhevsky, I. Sutskever, and G. Hinton. ImageNet clas-\nsi\ufb01cation with deep convolutional neural networks. In NIPS,\n2012. 1, 4, 6\n[15] S. Lazebnik, C. Schmid, and J. Ponce.\nBeyond bags of\nfeatures: Spatial pyramid matching for recognizing natural\nscene categories. In CVPR, 2006. 1\n[16] Y. LeCun, B. Boser, J. Denker, D. Henderson, R. Howard,\nW. Hubbard, and L. Jackel.\nBackpropagation applied to\nhandwritten zip code recognition. Neural Comp., 1989. 1\n[17] M. Lin, Q. Chen, and S. Yan. Network in network. In ICLR,\n2014. 5\n[18] T. Lin, M. Maire, S. Belongie, L. Bourdev, R. Girshick,\nJ. Hays, P. Perona, D. Ramanan, P. Doll\u00b4ar, and C. L. Zit-\nnick. Microsoft COCO: common objects in context. arXiv\ne-prints, arXiv:1405.0312 [cs.CV], 2014. 8\n[19] P. Sermanet, D. Eigen, X. Zhang, M. Mathieu, R. Fergus,\nand Y. LeCun. OverFeat: Integrated Recognition, Localiza-\ntion and Detection using Convolutional Networks. In ICLR,\n2014. 1, 3\n[20] K. Simonyan and A. Zisserman. Very deep convolutional\nnetworks for large-scale image recognition. In ICLR, 2015.\n1, 5\n[21] J. Uijlings, K. van de Sande, T. Gevers, and A. Smeulders.\nSelective search for object recognition. IJCV, 2013. 8\n[22] P. Viola and M. Jones. Rapid object detection using a boosted\ncascade of simple features. In CVPR, 2001. 8\n[23] J. Xue, J. Li, and Y. Gong.\nRestructuring of deep neural\nnetwork acoustic models with singular value decomposition.\nIn Interspeech, 2013. 4\n[24] X. Zhu, C. Vondrick, D. Ramanan, and C. Fowlkes. Do we\nneed more training data or better models for object detec-\ntion? In BMVC, 2012. 7\n[25] Y. Zhu, R. Urtasun, R. Salakhutdinov, and S. Fidler.\nsegDeepM: Exploiting segmentation and context in deep\nneural networks for object detection. In CVPR, 2015. 1,\n5\n",
        "sentence": " , 2016), leading to important progress in a variety of tasks, like object detection (Girshick, 2015) or image segmentation (Pinheiro et al.",
        "context": "veloped, may help further accelerate object detection.\nAcknowledgements.\nI thank Kaiming He, Larry Zitnick,\nand Piotr Doll\u00b4ar for helpful discussions and encouragement.\nReferences\n[1] J. Carreira, R. Caseiro, J. Batista, and C. Sminchisescu. Se-\nsegmentation. In CVPR, 2014. 1, 3, 4, 8\n[10] R. Girshick, J. Donahue, T. Darrell, and J. Malik. Region-\nbased convolutional networks for accurate object detection\nand segmentation. TPAMI, 2015. 5, 7, 8\nproved image classi\ufb01cation [14] and object detection [9, 19]\naccuracy. Compared to image classi\ufb01cation, object detec-\ntion is a more challenging task that requires more com-\nplex methods to solve. Due to this complexity, current ap-"
    },
    {
        "title": "Generative adversarial nets",
        "author": [
            "I. Goodfellow",
            "J. Pouget-Abadie",
            "M. Mirza",
            "B. Xu",
            "D. Warde-Farley",
            "S. Ozair",
            "A. Courville",
            "Y. Bengio"
        ],
        "venue": "In NIPS,",
        "citeRegEx": "Goodfellow et al\\.,? \\Q2014\\E",
        "shortCiteRegEx": "Goodfellow et al\\.",
        "year": 2014,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " Traditional examples of this approach are variational autoencoders (Kingma & Welling, 2013), generative adversarial networks (Goodfellow et al., 2014), and to a lesser extent, noisy autoencoders (Vincent et al. Among those approaches, generative adversarial networks (GANs) (Goodfellow et al., 2014; Denton et al., 2015; Donahue et al., 2016) share another similarity with our approach, namely they are explicitly minimizing a discriminative loss to learn their features.",
        "context": null
    },
    {
        "title": "Classes for fast maximum entropy training",
        "author": [
            "J. Goodman"
        ],
        "venue": "In ICASSP,",
        "citeRegEx": "Goodman,? \\Q2001\\E",
        "shortCiteRegEx": "Goodman",
        "year": 2001,
        "abstract": "Maximum entropy models are considered by many to be one of the most promising\navenues of language modeling research. Unfortunately, long training times make\nmaximum entropy research difficult. We present a novel speedup technique: we\nchange the form of the model to use classes. Our speedup works by creating two\nmaximum entropy models, the first of which predicts the class of each word, and\nthe second of which predicts the word itself. This factoring of the model leads\nto fewer non-zero indicator functions, and faster normalization, achieving\nspeedups of up to a factor of 35 over one of the best previous techniques. It\nalso results in typically slightly lower perplexities. The same trick can be\nused to speed training of other machine learning techniques, e.g. neural\nnetworks, applied to any problem with a large number of outputs, such as\nlanguage modeling.",
        "full_text": "",
        "sentence": " However, computing this loss is linear in the number of targets, making it impractical for large output spaces (Goodman, 2001). However, computing this loss is linear in the number of targets, making it impractical for large output spaces (Goodman, 2001). While there are workarounds to scale these losses to large output spaces, Tygert et al. (2017) has recently shown that using a squared `2 distance works well in many supervised settings, as long as the final activations are unit normalized.",
        "context": null
    },
    {
        "title": "Deep residual learning for image recognition",
        "author": [
            "K. He",
            "X. Zhang",
            "S. Ren",
            "J. Sun"
        ],
        "venue": null,
        "citeRegEx": "He et al\\.,? \\Q2016\\E",
        "shortCiteRegEx": "He et al\\.",
        "year": 2016,
        "abstract": "Deeper neural networks are more difficult to train. We present a residual\nlearning framework to ease the training of networks that are substantially\ndeeper than those used previously. We explicitly reformulate the layers as\nlearning residual functions with reference to the layer inputs, instead of\nlearning unreferenced functions. We provide comprehensive empirical evidence\nshowing that these residual networks are easier to optimize, and can gain\naccuracy from considerably increased depth. On the ImageNet dataset we evaluate\nresidual nets with a depth of up to 152 layers---8x deeper than VGG nets but\nstill having lower complexity. An ensemble of these residual nets achieves\n3.57% error on the ImageNet test set. This result won the 1st place on the\nILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100\nand 1000 layers.\n  The depth of representations is of central importance for many visual\nrecognition tasks. Solely due to our extremely deep representations, we obtain\na 28% relative improvement on the COCO object detection dataset. Deep residual\nnets are foundations of our submissions to ILSVRC & COCO 2015 competitions,\nwhere we also won the 1st places on the tasks of ImageNet detection, ImageNet\nlocalization, COCO detection, and COCO segmentation.",
        "full_text": "Deep Residual Learning for Image Recognition\nKaiming He\nXiangyu Zhang\nShaoqing Ren\nJian Sun\nMicrosoft Research\n{kahe, v-xiangz, v-shren, jiansun}@microsoft.com\nAbstract\nDeeper neural networks are more dif\ufb01cult to train. We\npresent a residual learning framework to ease the training\nof networks that are substantially deeper than those used\npreviously. We explicitly reformulate the layers as learn-\ning residual functions with reference to the layer inputs, in-\nstead of learning unreferenced functions. We provide com-\nprehensive empirical evidence showing that these residual\nnetworks are easier to optimize, and can gain accuracy from\nconsiderably increased depth. On the ImageNet dataset we\nevaluate residual nets with a depth of up to 152 layers\u20148\u00d7\ndeeper than VGG nets [41] but still having lower complex-\nity. An ensemble of these residual nets achieves 3.57% error\non the ImageNet test set. This result won the 1st place on the\nILSVRC 2015 classi\ufb01cation task. We also present analysis\non CIFAR-10 with 100 and 1000 layers.\nThe depth of representations is of central importance\nfor many visual recognition tasks. Solely due to our ex-\ntremely deep representations, we obtain a 28% relative im-\nprovement on the COCO object detection dataset. Deep\nresidual nets are foundations of our submissions to ILSVRC\n& COCO 2015 competitions1, where we also won the 1st\nplaces on the tasks of ImageNet detection, ImageNet local-\nization, COCO detection, and COCO segmentation.\n1. Introduction\nDeep convolutional neural networks [22, 21] have led\nto a series of breakthroughs for image classi\ufb01cation [21,\n50, 40]. Deep networks naturally integrate low/mid/high-\nlevel features [50] and classi\ufb01ers in an end-to-end multi-\nlayer fashion, and the \u201clevels\u201d of features can be enriched\nby the number of stacked layers (depth). Recent evidence\n[41, 44] reveals that network depth is of crucial importance,\nand the leading results [41, 44, 13, 16] on the challenging\nImageNet dataset [36] all exploit \u201cvery deep\u201d [41] models,\nwith a depth of sixteen [41] to thirty [16]. Many other non-\ntrivial visual recognition tasks [8, 12, 7, 32, 27] have also\n1http://image-net.org/challenges/LSVRC/2015/\nand\nhttp://mscoco.org/dataset/#detections-challenge2015.\n0\n1\n2\n3\n4\n5\n6\n0 \n10\n20\niter. (1e4)\ntraining error (%)\n \n \n0\n1\n2\n3\n4\n5\n6\n0\n10\n20\niter. (1e4)\ntest error (%)\n \n \n56-layer\n20-layer\n56-layer\n20-layer\nFigure 1. Training error (left) and test error (right) on CIFAR-10\nwith 20-layer and 56-layer \u201cplain\u201d networks. The deeper network\nhas higher training error, and thus test error. Similar phenomena\non ImageNet is presented in Fig. 4.\ngreatly bene\ufb01ted from very deep models.\nDriven by the signi\ufb01cance of depth, a question arises: Is\nlearning better networks as easy as stacking more layers?\nAn obstacle to answering this question was the notorious\nproblem of vanishing/exploding gradients [1, 9], which\nhamper convergence from the beginning.\nThis problem,\nhowever, has been largely addressed by normalized initial-\nization [23, 9, 37, 13] and intermediate normalization layers\n[16], which enable networks with tens of layers to start con-\nverging for stochastic gradient descent (SGD) with back-\npropagation [22].\nWhen deeper networks are able to start converging, a\ndegradation problem has been exposed: with the network\ndepth increasing, accuracy gets saturated (which might be\nunsurprising) and then degrades rapidly.\nUnexpectedly,\nsuch degradation is not caused by over\ufb01tting, and adding\nmore layers to a suitably deep model leads to higher train-\ning error, as reported in [11, 42] and thoroughly veri\ufb01ed by\nour experiments. Fig. 1 shows a typical example.\nThe degradation (of training accuracy) indicates that not\nall systems are similarly easy to optimize. Let us consider a\nshallower architecture and its deeper counterpart that adds\nmore layers onto it. There exists a solution by construction\nto the deeper model: the added layers are identity mapping,\nand the other layers are copied from the learned shallower\nmodel. The existence of this constructed solution indicates\nthat a deeper model should produce no higher training error\nthan its shallower counterpart. But experiments show that\nour current solvers on hand are unable to \ufb01nd solutions that\n1\narXiv:1512.03385v1  [cs.CV]  10 Dec 2015\nidentity\nweight layer\nweight layer\nrelu\nrelu\nF(x)\u0001+\u0001x\nx\nF(x)\nx\nFigure 2. Residual learning: a building block.\nare comparably good or better than the constructed solution\n(or unable to do so in feasible time).\nIn this paper, we address the degradation problem by\nintroducing a deep residual learning framework.\nIn-\nstead of hoping each few stacked layers directly \ufb01t a\ndesired underlying mapping, we explicitly let these lay-\ners \ufb01t a residual mapping. Formally, denoting the desired\nunderlying mapping as H(x), we let the stacked nonlinear\nlayers \ufb01t another mapping of F(x) := H(x)\u2212x. The orig-\ninal mapping is recast into F(x)+x. We hypothesize that it\nis easier to optimize the residual mapping than to optimize\nthe original, unreferenced mapping. To the extreme, if an\nidentity mapping were optimal, it would be easier to push\nthe residual to zero than to \ufb01t an identity mapping by a stack\nof nonlinear layers.\nThe formulation of F(x)+x can be realized by feedfor-\nward neural networks with \u201cshortcut connections\u201d (Fig. 2).\nShortcut connections [2, 34, 49] are those skipping one or\nmore layers. In our case, the shortcut connections simply\nperform identity mapping, and their outputs are added to\nthe outputs of the stacked layers (Fig. 2). Identity short-\ncut connections add neither extra parameter nor computa-\ntional complexity. The entire network can still be trained\nend-to-end by SGD with backpropagation, and can be eas-\nily implemented using common libraries (e.g., Caffe [19])\nwithout modifying the solvers.\nWe present comprehensive experiments on ImageNet\n[36] to show the degradation problem and evaluate our\nmethod. We show that: 1) Our extremely deep residual nets\nare easy to optimize, but the counterpart \u201cplain\u201d nets (that\nsimply stack layers) exhibit higher training error when the\ndepth increases; 2) Our deep residual nets can easily enjoy\naccuracy gains from greatly increased depth, producing re-\nsults substantially better than previous networks.\nSimilar phenomena are also shown on the CIFAR-10 set\n[20], suggesting that the optimization dif\ufb01culties and the\neffects of our method are not just akin to a particular dataset.\nWe present successfully trained models on this dataset with\nover 100 layers, and explore models with over 1000 layers.\nOn the ImageNet classi\ufb01cation dataset [36], we obtain\nexcellent results by extremely deep residual nets. Our 152-\nlayer residual net is the deepest network ever presented on\nImageNet, while still having lower complexity than VGG\nnets [41].\nOur ensemble has 3.57% top-5 error on the\nImageNet test set, and won the 1st place in the ILSVRC\n2015 classi\ufb01cation competition. The extremely deep rep-\nresentations also have excellent generalization performance\non other recognition tasks, and lead us to further win the\n1st places on: ImageNet detection, ImageNet localization,\nCOCO detection, and COCO segmentation in ILSVRC &\nCOCO 2015 competitions. This strong evidence shows that\nthe residual learning principle is generic, and we expect that\nit is applicable in other vision and non-vision problems.\n2. Related Work\nResidual Representations. In image recognition, VLAD\n[18] is a representation that encodes by the residual vectors\nwith respect to a dictionary, and Fisher Vector [30] can be\nformulated as a probabilistic version [18] of VLAD. Both\nof them are powerful shallow representations for image re-\ntrieval and classi\ufb01cation [4, 48]. For vector quantization,\nencoding residual vectors [17] is shown to be more effec-\ntive than encoding original vectors.\nIn low-level vision and computer graphics, for solv-\ning Partial Differential Equations (PDEs), the widely used\nMultigrid method [3] reformulates the system as subprob-\nlems at multiple scales, where each subproblem is respon-\nsible for the residual solution between a coarser and a \ufb01ner\nscale. An alternative to Multigrid is hierarchical basis pre-\nconditioning [45, 46], which relies on variables that repre-\nsent residual vectors between two scales. It has been shown\n[3, 45, 46] that these solvers converge much faster than stan-\ndard solvers that are unaware of the residual nature of the\nsolutions. These methods suggest that a good reformulation\nor preconditioning can simplify the optimization.\nShortcut Connections. Practices and theories that lead to\nshortcut connections [2, 34, 49] have been studied for a long\ntime. An early practice of training multi-layer perceptrons\n(MLPs) is to add a linear layer connected from the network\ninput to the output [34, 49]. In [44, 24], a few interme-\ndiate layers are directly connected to auxiliary classi\ufb01ers\nfor addressing vanishing/exploding gradients. The papers\nof [39, 38, 31, 47] propose methods for centering layer re-\nsponses, gradients, and propagated errors, implemented by\nshortcut connections. In [44], an \u201cinception\u201d layer is com-\nposed of a shortcut branch and a few deeper branches.\nConcurrent with our work, \u201chighway networks\u201d [42, 43]\npresent shortcut connections with gating functions [15].\nThese gates are data-dependent and have parameters, in\ncontrast to our identity shortcuts that are parameter-free.\nWhen a gated shortcut is \u201cclosed\u201d (approaching zero), the\nlayers in highway networks represent non-residual func-\ntions.\nOn the contrary, our formulation always learns\nresidual functions; our identity shortcuts are never closed,\nand all information is always passed through, with addi-\ntional residual functions to be learned. In addition, high-\n2\nway networks have not demonstrated accuracy gains with\nextremely increased depth (e.g., over 100 layers).\n3. Deep Residual Learning\n3.1. Residual Learning\nLet us consider H(x) as an underlying mapping to be\n\ufb01t by a few stacked layers (not necessarily the entire net),\nwith x denoting the inputs to the \ufb01rst of these layers. If one\nhypothesizes that multiple nonlinear layers can asymptoti-\ncally approximate complicated functions2, then it is equiv-\nalent to hypothesize that they can asymptotically approxi-\nmate the residual functions, i.e., H(x) \u2212x (assuming that\nthe input and output are of the same dimensions).\nSo\nrather than expect stacked layers to approximate H(x), we\nexplicitly let these layers approximate a residual function\nF(x) := H(x) \u2212x. The original function thus becomes\nF(x)+x. Although both forms should be able to asymptot-\nically approximate the desired functions (as hypothesized),\nthe ease of learning might be different.\nThis reformulation is motivated by the counterintuitive\nphenomena about the degradation problem (Fig. 1, left). As\nwe discussed in the introduction, if the added layers can\nbe constructed as identity mappings, a deeper model should\nhave training error no greater than its shallower counter-\npart.\nThe degradation problem suggests that the solvers\nmight have dif\ufb01culties in approximating identity mappings\nby multiple nonlinear layers. With the residual learning re-\nformulation, if identity mappings are optimal, the solvers\nmay simply drive the weights of the multiple nonlinear lay-\ners toward zero to approach identity mappings.\nIn real cases, it is unlikely that identity mappings are op-\ntimal, but our reformulation may help to precondition the\nproblem. If the optimal function is closer to an identity\nmapping than to a zero mapping, it should be easier for the\nsolver to \ufb01nd the perturbations with reference to an identity\nmapping, than to learn the function as a new one. We show\nby experiments (Fig. 7) that the learned residual functions in\ngeneral have small responses, suggesting that identity map-\npings provide reasonable preconditioning.\n3.2. Identity Mapping by Shortcuts\nWe adopt residual learning to every few stacked layers.\nA building block is shown in Fig. 2. Formally, in this paper\nwe consider a building block de\ufb01ned as:\ny = F(x, {Wi}) + x.\n(1)\nHere x and y are the input and output vectors of the lay-\ners considered.\nThe function F(x, {Wi}) represents the\nresidual mapping to be learned. For the example in Fig. 2\nthat has two layers, F = W2\u03c3(W1x) in which \u03c3 denotes\n2This hypothesis, however, is still an open question. See [28].\nReLU [29] and the biases are omitted for simplifying no-\ntations. The operation F + x is performed by a shortcut\nconnection and element-wise addition. We adopt the sec-\nond nonlinearity after the addition (i.e., \u03c3(y), see Fig. 2).\nThe shortcut connections in Eqn.(1) introduce neither ex-\ntra parameter nor computation complexity. This is not only\nattractive in practice but also important in our comparisons\nbetween plain and residual networks. We can fairly com-\npare plain/residual networks that simultaneously have the\nsame number of parameters, depth, width, and computa-\ntional cost (except for the negligible element-wise addition).\nThe dimensions of x and F must be equal in Eqn.(1).\nIf this is not the case (e.g., when changing the input/output\nchannels), we can perform a linear projection Ws by the\nshortcut connections to match the dimensions:\ny = F(x, {Wi}) + Wsx.\n(2)\nWe can also use a square matrix Ws in Eqn.(1). But we will\nshow by experiments that the identity mapping is suf\ufb01cient\nfor addressing the degradation problem and is economical,\nand thus Ws is only used when matching dimensions.\nThe form of the residual function F is \ufb02exible. Exper-\niments in this paper involve a function F that has two or\nthree layers (Fig. 5), while more layers are possible. But if\nF has only a single layer, Eqn.(1) is similar to a linear layer:\ny = W1x + x, for which we have not observed advantages.\nWe also note that although the above notations are about\nfully-connected layers for simplicity, they are applicable to\nconvolutional layers. The function F(x, {Wi}) can repre-\nsent multiple convolutional layers. The element-wise addi-\ntion is performed on two feature maps, channel by channel.\n3.3. Network Architectures\nWe have tested various plain/residual nets, and have ob-\nserved consistent phenomena. To provide instances for dis-\ncussion, we describe two models for ImageNet as follows.\nPlain Network. Our plain baselines (Fig. 3, middle) are\nmainly inspired by the philosophy of VGG nets [41] (Fig. 3,\nleft). The convolutional layers mostly have 3\u00d73 \ufb01lters and\nfollow two simple design rules: (i) for the same output\nfeature map size, the layers have the same number of \ufb01l-\nters; and (ii) if the feature map size is halved, the num-\nber of \ufb01lters is doubled so as to preserve the time com-\nplexity per layer. We perform downsampling directly by\nconvolutional layers that have a stride of 2. The network\nends with a global average pooling layer and a 1000-way\nfully-connected layer with softmax. The total number of\nweighted layers is 34 in Fig. 3 (middle).\nIt is worth noticing that our model has fewer \ufb01lters and\nlower complexity than VGG nets [41] (Fig. 3, left). Our 34-\nlayer baseline has 3.6 billion FLOPs (multiply-adds), which\nis only 18% of VGG-19 (19.6 billion FLOPs).\n3\n7x7 conv, 64, /2\npool, /2\n3x3 conv, 64\n3x3 conv, 64\n3x3 conv, 64\n3x3 conv, 64\n3x3 conv, 64\n3x3 conv, 64\n3x3 conv, 128, /2\n3x3 conv, 128\n3x3 conv, 128\n3x3 conv, 128\n3x3 conv, 128\n3x3 conv, 128\n3x3 conv, 128\n3x3 conv, 128\n3x3 conv, 256, /2\n3x3 conv, 256\n3x3 conv, 256\n3x3 conv, 256\n3x3 conv, 256\n3x3 conv, 256\n3x3 conv, 256\n3x3 conv, 256\n3x3 conv, 256\n3x3 conv, 256\n3x3 conv, 256\n3x3 conv, 256\n3x3 conv, 512, /2\n3x3 conv, 512\n3x3 conv, 512\n3x3 conv, 512\n3x3 conv, 512\n3x3 conv, 512\navg pool\nfc 1000\nimage\n3x3 conv, 512\n3x3 conv, 64\n3x3 conv, 64\npool, /2\n3x3 conv, 128\n3x3 conv, 128\npool, /2\n3x3 conv, 256\n3x3 conv, 256\n3x3 conv, 256\n3x3 conv, 256\npool, /2\n3x3 conv, 512\n3x3 conv, 512\n3x3 conv, 512\npool, /2\n3x3 conv, 512\n3x3 conv, 512\n3x3 conv, 512\n3x3 conv, 512\npool, /2\nfc 4096\nfc 4096\nfc 1000\nimage\noutput \nsize: 112\noutput \nsize: 224\noutput \nsize: 56\noutput \nsize: 28\noutput \nsize: 14\noutput \nsize: 7\noutput \nsize: 1\nVGG-19\n34-layer plain\n7x7 conv, 64, /2\npool, /2\n3x3 conv, 64\n3x3 conv, 64\n3x3 conv, 64\n3x3 conv, 64\n3x3 conv, 64\n3x3 conv, 64\n3x3 conv, 128, /2\n3x3 conv, 128\n3x3 conv, 128\n3x3 conv, 128\n3x3 conv, 128\n3x3 conv, 128\n3x3 conv, 128\n3x3 conv, 128\n3x3 conv, 256, /2\n3x3 conv, 256\n3x3 conv, 256\n3x3 conv, 256\n3x3 conv, 256\n3x3 conv, 256\n3x3 conv, 256\n3x3 conv, 256\n3x3 conv, 256\n3x3 conv, 256\n3x3 conv, 256\n3x3 conv, 256\n3x3 conv, 512, /2\n3x3 conv, 512\n3x3 conv, 512\n3x3 conv, 512\n3x3 conv, 512\n3x3 conv, 512\navg pool\nfc 1000\nimage\n34-layer residual\nFigure 3. Example network architectures for ImageNet. Left: the\nVGG-19 model [41] (19.6 billion FLOPs) as a reference. Mid-\ndle: a plain network with 34 parameter layers (3.6 billion FLOPs).\nRight: a residual network with 34 parameter layers (3.6 billion\nFLOPs). The dotted shortcuts increase dimensions. Table 1 shows\nmore details and other variants.\nResidual Network. Based on the above plain network, we\ninsert shortcut connections (Fig. 3, right) which turn the\nnetwork into its counterpart residual version. The identity\nshortcuts (Eqn.(1)) can be directly used when the input and\noutput are of the same dimensions (solid line shortcuts in\nFig. 3). When the dimensions increase (dotted line shortcuts\nin Fig. 3), we consider two options: (A) The shortcut still\nperforms identity mapping, with extra zero entries padded\nfor increasing dimensions. This option introduces no extra\nparameter; (B) The projection shortcut in Eqn.(2) is used to\nmatch dimensions (done by 1\u00d71 convolutions). For both\noptions, when the shortcuts go across feature maps of two\nsizes, they are performed with a stride of 2.\n3.4. Implementation\nOur implementation for ImageNet follows the practice\nin [21, 41]. The image is resized with its shorter side ran-\ndomly sampled in [256, 480] for scale augmentation [41].\nA 224\u00d7224 crop is randomly sampled from an image or its\nhorizontal \ufb02ip, with the per-pixel mean subtracted [21]. The\nstandard color augmentation in [21] is used. We adopt batch\nnormalization (BN) [16] right after each convolution and\nbefore activation, following [16]. We initialize the weights\nas in [13] and train all plain/residual nets from scratch. We\nuse SGD with a mini-batch size of 256. The learning rate\nstarts from 0.1 and is divided by 10 when the error plateaus,\nand the models are trained for up to 60 \u00d7 104 iterations. We\nuse a weight decay of 0.0001 and a momentum of 0.9. We\ndo not use dropout [14], following the practice in [16].\nIn testing, for comparison studies we adopt the standard\n10-crop testing [21]. For best results, we adopt the fully-\nconvolutional form as in [41, 13], and average the scores\nat multiple scales (images are resized such that the shorter\nside is in {224, 256, 384, 480, 640}).\n4. Experiments\n4.1. ImageNet Classi\ufb01cation\nWe evaluate our method on the ImageNet 2012 classi\ufb01-\ncation dataset [36] that consists of 1000 classes. The models\nare trained on the 1.28 million training images, and evalu-\nated on the 50k validation images. We also obtain a \ufb01nal\nresult on the 100k test images, reported by the test server.\nWe evaluate both top-1 and top-5 error rates.\nPlain Networks. We \ufb01rst evaluate 18-layer and 34-layer\nplain nets. The 34-layer plain net is in Fig. 3 (middle). The\n18-layer plain net is of a similar form. See Table 1 for de-\ntailed architectures.\nThe results in Table 2 show that the deeper 34-layer plain\nnet has higher validation error than the shallower 18-layer\nplain net. To reveal the reasons, in Fig. 4 (left) we com-\npare their training/validation errors during the training pro-\ncedure. We have observed the degradation problem - the\n4\nlayer name output size\n18-layer\n34-layer\n50-layer\n101-layer\n152-layer\nconv1\n112\u00d7112\n7\u00d77, 64, stride 2\nconv2 x\n56\u00d756\n3\u00d73 max pool, stride 2\n\u0014\n3\u00d73, 64\n3\u00d73, 64\n\u0015\n\u00d72\n\u0014\n3\u00d73, 64\n3\u00d73, 64\n\u0015\n\u00d73\n\uf8ee\n\uf8f0\n1\u00d71, 64\n3\u00d73, 64\n1\u00d71, 256\n\uf8f9\n\uf8fb\u00d73\n\uf8ee\n\uf8f0\n1\u00d71, 64\n3\u00d73, 64\n1\u00d71, 256\n\uf8f9\n\uf8fb\u00d73\n\uf8ee\n\uf8f0\n1\u00d71, 64\n3\u00d73, 64\n1\u00d71, 256\n\uf8f9\n\uf8fb\u00d73\nconv3 x\n28\u00d728\n\u0014\n3\u00d73, 128\n3\u00d73, 128\n\u0015\n\u00d72\n\u0014\n3\u00d73, 128\n3\u00d73, 128\n\u0015\n\u00d74\n\uf8ee\n\uf8f0\n1\u00d71, 128\n3\u00d73, 128\n1\u00d71, 512\n\uf8f9\n\uf8fb\u00d74\n\uf8ee\n\uf8f0\n1\u00d71, 128\n3\u00d73, 128\n1\u00d71, 512\n\uf8f9\n\uf8fb\u00d74\n\uf8ee\n\uf8f0\n1\u00d71, 128\n3\u00d73, 128\n1\u00d71, 512\n\uf8f9\n\uf8fb\u00d78\nconv4 x\n14\u00d714\n\u0014\n3\u00d73, 256\n3\u00d73, 256\n\u0015\n\u00d72\n\u0014\n3\u00d73, 256\n3\u00d73, 256\n\u0015\n\u00d76\n\uf8ee\n\uf8f0\n1\u00d71, 256\n3\u00d73, 256\n1\u00d71, 1024\n\uf8f9\n\uf8fb\u00d76\n\uf8ee\n\uf8f0\n1\u00d71, 256\n3\u00d73, 256\n1\u00d71, 1024\n\uf8f9\n\uf8fb\u00d723\n\uf8ee\n\uf8f0\n1\u00d71, 256\n3\u00d73, 256\n1\u00d71, 1024\n\uf8f9\n\uf8fb\u00d736\nconv5 x\n7\u00d77\n\u0014\n3\u00d73, 512\n3\u00d73, 512\n\u0015\n\u00d72\n\u0014\n3\u00d73, 512\n3\u00d73, 512\n\u0015\n\u00d73\n\uf8ee\n\uf8f0\n1\u00d71, 512\n3\u00d73, 512\n1\u00d71, 2048\n\uf8f9\n\uf8fb\u00d73\n\uf8ee\n\uf8f0\n1\u00d71, 512\n3\u00d73, 512\n1\u00d71, 2048\n\uf8f9\n\uf8fb\u00d73\n\uf8ee\n\uf8f0\n1\u00d71, 512\n3\u00d73, 512\n1\u00d71, 2048\n\uf8f9\n\uf8fb\u00d73\n1\u00d71\naverage pool, 1000-d fc, softmax\nFLOPs\n1.8\u00d7109\n3.6\u00d7109\n3.8\u00d7109\n7.6\u00d7109\n11.3\u00d7109\nTable 1. Architectures for ImageNet. Building blocks are shown in brackets (see also Fig. 5), with the numbers of blocks stacked. Down-\nsampling is performed by conv3 1, conv4 1, and conv5 1 with a stride of 2.\n0\n10\n20\n30\n40\n50\n20\n30\n40\n50\n60\niter. (1e4)\nerror (%)\n \n \nplain-18\nplain-34\n0\n10\n20\n30\n40\n50\n20\n30\n40\n50\n60\niter. (1e4)\nerror (%)\n \n \nResNet-18\nResNet-34\n18-layer\n34-layer\n18-layer\n34-layer\nFigure 4. Training on ImageNet. Thin curves denote training error, and bold curves denote validation error of the center crops. Left: plain\nnetworks of 18 and 34 layers. Right: ResNets of 18 and 34 layers. In this plot, the residual networks have no extra parameter compared to\ntheir plain counterparts.\nplain\nResNet\n18 layers\n27.94\n27.88\n34 layers\n28.54\n25.03\nTable 2. Top-1 error (%, 10-crop testing) on ImageNet validation.\nHere the ResNets have no extra parameter compared to their plain\ncounterparts. Fig. 4 shows the training procedures.\n34-layer plain net has higher training error throughout the\nwhole training procedure, even though the solution space\nof the 18-layer plain network is a subspace of that of the\n34-layer one.\nWe argue that this optimization dif\ufb01culty is unlikely to\nbe caused by vanishing gradients. These plain networks are\ntrained with BN [16], which ensures forward propagated\nsignals to have non-zero variances. We also verify that the\nbackward propagated gradients exhibit healthy norms with\nBN. So neither forward nor backward signals vanish. In\nfact, the 34-layer plain net is still able to achieve compet-\nitive accuracy (Table 3), suggesting that the solver works\nto some extent. We conjecture that the deep plain nets may\nhave exponentially low convergence rates, which impact the\nreducing of the training error3. The reason for such opti-\nmization dif\ufb01culties will be studied in the future.\nResidual Networks. Next we evaluate 18-layer and 34-\nlayer residual nets (ResNets). The baseline architectures\nare the same as the above plain nets, expect that a shortcut\nconnection is added to each pair of 3\u00d73 \ufb01lters as in Fig. 3\n(right). In the \ufb01rst comparison (Table 2 and Fig. 4 right),\nwe use identity mapping for all shortcuts and zero-padding\nfor increasing dimensions (option A). So they have no extra\nparameter compared to the plain counterparts.\nWe have three major observations from Table 2 and\nFig. 4. First, the situation is reversed with residual learn-\ning \u2013 the 34-layer ResNet is better than the 18-layer ResNet\n(by 2.8%). More importantly, the 34-layer ResNet exhibits\nconsiderably lower training error and is generalizable to the\nvalidation data. This indicates that the degradation problem\nis well addressed in this setting and we manage to obtain\naccuracy gains from increased depth.\nSecond, compared to its plain counterpart, the 34-layer\n3We have experimented with more training iterations (3\u00d7) and still ob-\nserved the degradation problem, suggesting that this problem cannot be\nfeasibly addressed by simply using more iterations.\n5\nmodel\ntop-1 err.\ntop-5 err.\nVGG-16 [41]\n28.07\n9.33\nGoogLeNet [44]\n-\n9.15\nPReLU-net [13]\n24.27\n7.38\nplain-34\n28.54\n10.02\nResNet-34 A\n25.03\n7.76\nResNet-34 B\n24.52\n7.46\nResNet-34 C\n24.19\n7.40\nResNet-50\n22.85\n6.71\nResNet-101\n21.75\n6.05\nResNet-152\n21.43\n5.71\nTable 3. Error rates (%, 10-crop testing) on ImageNet validation.\nVGG-16 is based on our test. ResNet-50/101/152 are of option B\nthat only uses projections for increasing dimensions.\nmethod\ntop-1 err.\ntop-5 err.\nVGG [41] (ILSVRC\u201914)\n-\n8.43\u2020\nGoogLeNet [44] (ILSVRC\u201914)\n-\n7.89\nVGG [41] (v5)\n24.4\n7.1\nPReLU-net [13]\n21.59\n5.71\nBN-inception [16]\n21.99\n5.81\nResNet-34 B\n21.84\n5.71\nResNet-34 C\n21.53\n5.60\nResNet-50\n20.74\n5.25\nResNet-101\n19.87\n4.60\nResNet-152\n19.38\n4.49\nTable 4. Error rates (%) of single-model results on the ImageNet\nvalidation set (except \u2020 reported on the test set).\nmethod\ntop-5 err. (test)\nVGG [41] (ILSVRC\u201914)\n7.32\nGoogLeNet [44] (ILSVRC\u201914)\n6.66\nVGG [41] (v5)\n6.8\nPReLU-net [13]\n4.94\nBN-inception [16]\n4.82\nResNet (ILSVRC\u201915)\n3.57\nTable 5. Error rates (%) of ensembles. The top-5 error is on the\ntest set of ImageNet and reported by the test server.\nResNet reduces the top-1 error by 3.5% (Table 2), resulting\nfrom the successfully reduced training error (Fig. 4 right vs.\nleft). This comparison veri\ufb01es the effectiveness of residual\nlearning on extremely deep systems.\nLast, we also note that the 18-layer plain/residual nets\nare comparably accurate (Table 2), but the 18-layer ResNet\nconverges faster (Fig. 4 right vs. left). When the net is \u201cnot\noverly deep\u201d (18 layers here), the current SGD solver is still\nable to \ufb01nd good solutions to the plain net. In this case, the\nResNet eases the optimization by providing faster conver-\ngence at the early stage.\nIdentity vs. Projection Shortcuts. We have shown that\n3x3, 64\n1x1, 64\nrelu\n1x1, 256\nrelu\nrelu\n3x3, 64\n3x3, 64\nrelu\nrelu\n64-d\n256-d\nFigure 5. A deeper residual function F for ImageNet. Left: a\nbuilding block (on 56\u00d756 feature maps) as in Fig. 3 for ResNet-\n34. Right: a \u201cbottleneck\u201d building block for ResNet-50/101/152.\nparameter-free, identity shortcuts help with training. Next\nwe investigate projection shortcuts (Eqn.(2)). In Table 3 we\ncompare three options: (A) zero-padding shortcuts are used\nfor increasing dimensions, and all shortcuts are parameter-\nfree (the same as Table 2 and Fig. 4 right); (B) projec-\ntion shortcuts are used for increasing dimensions, and other\nshortcuts are identity; and (C) all shortcuts are projections.\nTable 3 shows that all three options are considerably bet-\nter than the plain counterpart. B is slightly better than A. We\nargue that this is because the zero-padded dimensions in A\nindeed have no residual learning. C is marginally better than\nB, and we attribute this to the extra parameters introduced\nby many (thirteen) projection shortcuts. But the small dif-\nferences among A/B/C indicate that projection shortcuts are\nnot essential for addressing the degradation problem. So we\ndo not use option C in the rest of this paper, to reduce mem-\nory/time complexity and model sizes. Identity shortcuts are\nparticularly important for not increasing the complexity of\nthe bottleneck architectures that are introduced below.\nDeeper Bottleneck Architectures. Next we describe our\ndeeper nets for ImageNet. Because of concerns on the train-\ning time that we can afford, we modify the building block\nas a bottleneck design4. For each residual function F, we\nuse a stack of 3 layers instead of 2 (Fig. 5). The three layers\nare 1\u00d71, 3\u00d73, and 1\u00d71 convolutions, where the 1\u00d71 layers\nare responsible for reducing and then increasing (restoring)\ndimensions, leaving the 3\u00d73 layer a bottleneck with smaller\ninput/output dimensions. Fig. 5 shows an example, where\nboth designs have similar time complexity.\nThe parameter-free identity shortcuts are particularly im-\nportant for the bottleneck architectures. If the identity short-\ncut in Fig. 5 (right) is replaced with projection, one can\nshow that the time complexity and model size are doubled,\nas the shortcut is connected to the two high-dimensional\nends. So identity shortcuts lead to more ef\ufb01cient models\nfor the bottleneck designs.\n50-layer ResNet: We replace each 2-layer block in the\n4Deeper non-bottleneck ResNets (e.g., Fig. 5 left) also gain accuracy\nfrom increased depth (as shown on CIFAR-10), but are not as economical\nas the bottleneck ResNets. So the usage of bottleneck designs is mainly due\nto practical considerations. We further note that the degradation problem\nof plain nets is also witnessed for the bottleneck designs.\n6\n34-layer net with this 3-layer bottleneck block, resulting in\na 50-layer ResNet (Table 1). We use option B for increasing\ndimensions. This model has 3.8 billion FLOPs.\n101-layer and 152-layer ResNets: We construct 101-\nlayer and 152-layer ResNets by using more 3-layer blocks\n(Table 1). Remarkably, although the depth is signi\ufb01cantly\nincreased, the 152-layer ResNet (11.3 billion FLOPs) still\nhas lower complexity than VGG-16/19 nets (15.3/19.6 bil-\nlion FLOPs).\nThe 50/101/152-layer ResNets are more accurate than\nthe 34-layer ones by considerable margins (Table 3 and 4).\nWe do not observe the degradation problem and thus en-\njoy signi\ufb01cant accuracy gains from considerably increased\ndepth. The bene\ufb01ts of depth are witnessed for all evaluation\nmetrics (Table 3 and 4).\nComparisons with State-of-the-art Methods. In Table 4\nwe compare with the previous best single-model results.\nOur baseline 34-layer ResNets have achieved very compet-\nitive accuracy. Our 152-layer ResNet has a single-model\ntop-5 validation error of 4.49%. This single-model result\noutperforms all previous ensemble results (Table 5). We\ncombine six models of different depth to form an ensemble\n(only with two 152-layer ones at the time of submitting).\nThis leads to 3.57% top-5 error on the test set (Table 5).\nThis entry won the 1st place in ILSVRC 2015.\n4.2. CIFAR-10 and Analysis\nWe conducted more studies on the CIFAR-10 dataset\n[20], which consists of 50k training images and 10k test-\ning images in 10 classes. We present experiments trained\non the training set and evaluated on the test set. Our focus\nis on the behaviors of extremely deep networks, but not on\npushing the state-of-the-art results, so we intentionally use\nsimple architectures as follows.\nThe plain/residual architectures follow the form in Fig. 3\n(middle/right). The network inputs are 32\u00d732 images, with\nthe per-pixel mean subtracted. The \ufb01rst layer is 3\u00d73 convo-\nlutions. Then we use a stack of 6n layers with 3\u00d73 convo-\nlutions on the feature maps of sizes {32, 16, 8} respectively,\nwith 2n layers for each feature map size. The numbers of\n\ufb01lters are {16, 32, 64} respectively. The subsampling is per-\nformed by convolutions with a stride of 2. The network ends\nwith a global average pooling, a 10-way fully-connected\nlayer, and softmax. There are totally 6n+2 stacked weighted\nlayers. The following table summarizes the architecture:\noutput map size\n32\u00d732\n16\u00d716\n8\u00d78\n# layers\n1+2n\n2n\n2n\n# \ufb01lters\n16\n32\n64\nWhen shortcut connections are used, they are connected\nto the pairs of 3\u00d73 layers (totally 3n shortcuts). On this\ndataset we use identity shortcuts in all cases (i.e., option A),\nmethod\nerror (%)\nMaxout [10]\n9.38\nNIN [25]\n8.81\nDSN [24]\n8.22\n# layers\n# params\nFitNet [35]\n19\n2.5M\n8.39\nHighway [42, 43]\n19\n2.3M\n7.54 (7.72\u00b10.16)\nHighway [42, 43]\n32\n1.25M\n8.80\nResNet\n20\n0.27M\n8.75\nResNet\n32\n0.46M\n7.51\nResNet\n44\n0.66M\n7.17\nResNet\n56\n0.85M\n6.97\nResNet\n110\n1.7M\n6.43 (6.61\u00b10.16)\nResNet\n1202\n19.4M\n7.93\nTable 6. Classi\ufb01cation error on the CIFAR-10 test set. All meth-\nods are with data augmentation. For ResNet-110, we run it 5 times\nand show \u201cbest (mean\u00b1std)\u201d as in [43].\nso our residual models have exactly the same depth, width,\nand number of parameters as the plain counterparts.\nWe use a weight decay of 0.0001 and momentum of 0.9,\nand adopt the weight initialization in [13] and BN [16] but\nwith no dropout. These models are trained with a mini-\nbatch size of 128 on two GPUs. We start with a learning\nrate of 0.1, divide it by 10 at 32k and 48k iterations, and\nterminate training at 64k iterations, which is determined on\na 45k/5k train/val split. We follow the simple data augmen-\ntation in [24] for training: 4 pixels are padded on each side,\nand a 32\u00d732 crop is randomly sampled from the padded\nimage or its horizontal \ufb02ip. For testing, we only evaluate\nthe single view of the original 32\u00d732 image.\nWe compare n = {3, 5, 7, 9}, leading to 20, 32, 44, and\n56-layer networks. Fig. 6 (left) shows the behaviors of the\nplain nets. The deep plain nets suffer from increased depth,\nand exhibit higher training error when going deeper. This\nphenomenon is similar to that on ImageNet (Fig. 4, left) and\non MNIST (see [42]), suggesting that such an optimization\ndif\ufb01culty is a fundamental problem.\nFig. 6 (middle) shows the behaviors of ResNets. Also\nsimilar to the ImageNet cases (Fig. 4, right), our ResNets\nmanage to overcome the optimization dif\ufb01culty and demon-\nstrate accuracy gains when the depth increases.\nWe further explore n = 18 that leads to a 110-layer\nResNet. In this case, we \ufb01nd that the initial learning rate\nof 0.1 is slightly too large to start converging5. So we use\n0.01 to warm up the training until the training error is below\n80% (about 400 iterations), and then go back to 0.1 and con-\ntinue training. The rest of the learning schedule is as done\npreviously. This 110-layer network converges well (Fig. 6,\nmiddle). It has fewer parameters than other deep and thin\n5With an initial learning rate of 0.1, it starts converging (<90% error)\nafter several epochs, but still reaches similar accuracy.\n7\n0\n1\n2\n3\n4\n5\n6\n0\n5\n10\n20\niter. (1e4)\nerror (%)\n \n \nplain-20\nplain-32\nplain-44\nplain-56\n0\n1\n2\n3\n4\n5\n6\n0\n5\n10\n20\niter. (1e4)\nerror (%)\n \n \nResNet-20\nResNet-32\nResNet-44\nResNet-56\nResNet-110\n56-layer\n20-layer\n110-layer\n20-layer\n4\n5\n6\n0\n1\n5\n10\n20\niter. (1e4)\nerror (%)\n \n \nresidual-110\nresidual-1202\nFigure 6. Training on CIFAR-10. Dashed lines denote training error, and bold lines denote testing error. Left: plain networks. The error\nof plain-110 is higher than 60% and not displayed. Middle: ResNets. Right: ResNets with 110 and 1202 layers.\n0\n20\n40\n60\n80\n100\n1\n2\n3\nlayer index (sorted by magnitude)\nstd\n \n \nplain-20\nplain-56\nResNet-20\nResNet-56\nResNet-110\n0\n20\n40\n60\n80\n100\n1\n2\n3\nlayer index (original)\nstd\n \n \nplain-20\nplain-56\nResNet-20\nResNet-56\nResNet-110\nFigure 7. Standard deviations (std) of layer responses on CIFAR-\n10. The responses are the outputs of each 3\u00d73 layer, after BN and\nbefore nonlinearity. Top: the layers are shown in their original\norder. Bottom: the responses are ranked in descending order.\nnetworks such as FitNet [35] and Highway [42] (Table 6),\nyet is among the state-of-the-art results (6.43%, Table 6).\nAnalysis of Layer Responses. Fig. 7 shows the standard\ndeviations (std) of the layer responses. The responses are\nthe outputs of each 3\u00d73 layer, after BN and before other\nnonlinearity (ReLU/addition).\nFor ResNets, this analy-\nsis reveals the response strength of the residual functions.\nFig. 7 shows that ResNets have generally smaller responses\nthan their plain counterparts. These results support our ba-\nsic motivation (Sec.3.1) that the residual functions might\nbe generally closer to zero than the non-residual functions.\nWe also notice that the deeper ResNet has smaller magni-\ntudes of responses, as evidenced by the comparisons among\nResNet-20, 56, and 110 in Fig. 7. When there are more\nlayers, an individual layer of ResNets tends to modify the\nsignal less.\nExploring Over 1000 layers. We explore an aggressively\ndeep model of over 1000 layers.\nWe set n = 200 that\nleads to a 1202-layer network, which is trained as described\nabove. Our method shows no optimization dif\ufb01culty, and\nthis 103-layer network is able to achieve training error\n<0.1% (Fig. 6, right).\nIts test error is still fairly good\n(7.93%, Table 6).\nBut there are still open problems on such aggressively\ndeep models. The testing result of this 1202-layer network\nis worse than that of our 110-layer network, although both\ntraining data\n07+12\n07++12\ntest data\nVOC 07 test\nVOC 12 test\nVGG-16\n73.2\n70.4\nResNet-101\n76.4\n73.8\nTable 7. Object detection mAP (%) on the PASCAL VOC\n2007/2012 test sets using baseline Faster R-CNN. See also Ta-\nble 10 and 11 for better results.\nmetric\nmAP@.5\nmAP@[.5, .95]\nVGG-16\n41.5\n21.2\nResNet-101\n48.4\n27.2\nTable 8. Object detection mAP (%) on the COCO validation set\nusing baseline Faster R-CNN. See also Table 9 for better results.\nhave similar training error. We argue that this is because of\nover\ufb01tting. The 1202-layer network may be unnecessarily\nlarge (19.4M) for this small dataset. Strong regularization\nsuch as maxout [10] or dropout [14] is applied to obtain the\nbest results ([10, 25, 24, 35]) on this dataset. In this paper,\nwe use no maxout/dropout and just simply impose regular-\nization via deep and thin architectures by design, without\ndistracting from the focus on the dif\ufb01culties of optimiza-\ntion. But combining with stronger regularization may im-\nprove results, which we will study in the future.\n4.3. Object Detection on PASCAL and MS COCO\nOur method has good generalization performance on\nother recognition tasks. Table 7 and 8 show the object de-\ntection baseline results on PASCAL VOC 2007 and 2012\n[5] and COCO [26]. We adopt Faster R-CNN [32] as the de-\ntection method. Here we are interested in the improvements\nof replacing VGG-16 [41] with ResNet-101. The detection\nimplementation (see appendix) of using both models is the\nsame, so the gains can only be attributed to better networks.\nMost remarkably, on the challenging COCO dataset we ob-\ntain a 6.0% increase in COCO\u2019s standard metric (mAP@[.5,\n.95]), which is a 28% relative improvement. This gain is\nsolely due to the learned representations.\nBased on deep residual nets, we won the 1st places in\nseveral tracks in ILSVRC & COCO 2015 competitions: Im-\nageNet detection, ImageNet localization, COCO detection,\nand COCO segmentation. The details are in the appendix.\n8\nReferences\n[1] Y. Bengio, P. Simard, and P. Frasconi. Learning long-term dependen-\ncies with gradient descent is dif\ufb01cult. IEEE Transactions on Neural\nNetworks, 5(2):157\u2013166, 1994.\n[2] C. M. Bishop.\nNeural networks for pattern recognition.\nOxford\nuniversity press, 1995.\n[3] W. L. Briggs, S. F. McCormick, et al. A Multigrid Tutorial. Siam,\n2000.\n[4] K. Chat\ufb01eld, V. Lempitsky, A. Vedaldi, and A. Zisserman. The devil\nis in the details: an evaluation of recent feature encoding methods.\nIn BMVC, 2011.\n[5] M. Everingham, L. Van Gool, C. K. Williams, J. Winn, and A. Zis-\nserman. The Pascal Visual Object Classes (VOC) Challenge. IJCV,\npages 303\u2013338, 2010.\n[6] S. Gidaris and N. Komodakis. Object detection via a multi-region &\nsemantic segmentation-aware cnn model. In ICCV, 2015.\n[7] R. Girshick. Fast R-CNN. In ICCV, 2015.\n[8] R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich feature hier-\narchies for accurate object detection and semantic segmentation. In\nCVPR, 2014.\n[9] X. Glorot and Y. Bengio. Understanding the dif\ufb01culty of training\ndeep feedforward neural networks. In AISTATS, 2010.\n[10] I. J. Goodfellow, D. Warde-Farley, M. Mirza, A. Courville, and\nY. Bengio. Maxout networks. arXiv:1302.4389, 2013.\n[11] K. He and J. Sun. Convolutional neural networks at constrained time\ncost. In CVPR, 2015.\n[12] K. He, X. Zhang, S. Ren, and J. Sun. Spatial pyramid pooling in deep\nconvolutional networks for visual recognition. In ECCV, 2014.\n[13] K. He, X. Zhang, S. Ren, and J. Sun. Delving deep into recti\ufb01ers:\nSurpassing human-level performance on imagenet classi\ufb01cation. In\nICCV, 2015.\n[14] G. E. Hinton, N. Srivastava, A. Krizhevsky, I. Sutskever, and\nR. R. Salakhutdinov. Improving neural networks by preventing co-\nadaptation of feature detectors. arXiv:1207.0580, 2012.\n[15] S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural\ncomputation, 9(8):1735\u20131780, 1997.\n[16] S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep\nnetwork training by reducing internal covariate shift. In ICML, 2015.\n[17] H. Jegou, M. Douze, and C. Schmid. Product quantization for nearest\nneighbor search. TPAMI, 33, 2011.\n[18] H. Jegou, F. Perronnin, M. Douze, J. Sanchez, P. Perez, and\nC. Schmid. Aggregating local image descriptors into compact codes.\nTPAMI, 2012.\n[19] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick,\nS. Guadarrama, and T. Darrell. Caffe: Convolutional architecture for\nfast feature embedding. arXiv:1408.5093, 2014.\n[20] A. Krizhevsky. Learning multiple layers of features from tiny im-\nages. Tech Report, 2009.\n[21] A. Krizhevsky, I. Sutskever, and G. Hinton. Imagenet classi\ufb01cation\nwith deep convolutional neural networks. In NIPS, 2012.\n[22] Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard,\nW. Hubbard, and L. D. Jackel. Backpropagation applied to hand-\nwritten zip code recognition. Neural computation, 1989.\n[23] Y. LeCun, L. Bottou, G. B. Orr, and K.-R. M\u00a8uller. Ef\ufb01cient backprop.\nIn Neural Networks: Tricks of the Trade, pages 9\u201350. Springer, 1998.\n[24] C.-Y. Lee, S. Xie, P. Gallagher, Z. Zhang, and Z. Tu.\nDeeply-\nsupervised nets. arXiv:1409.5185, 2014.\n[25] M. Lin, Q. Chen, and S. Yan. Network in network. arXiv:1312.4400,\n2013.\n[26] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan,\nP. Doll\u00b4ar, and C. L. Zitnick. Microsoft COCO: Common objects in\ncontext. In ECCV. 2014.\n[27] J. Long, E. Shelhamer, and T. Darrell. Fully convolutional networks\nfor semantic segmentation. In CVPR, 2015.\n[28] G. Mont\u00b4ufar, R. Pascanu, K. Cho, and Y. Bengio. On the number of\nlinear regions of deep neural networks. In NIPS, 2014.\n[29] V. Nair and G. E. Hinton. Recti\ufb01ed linear units improve restricted\nboltzmann machines. In ICML, 2010.\n[30] F. Perronnin and C. Dance. Fisher kernels on visual vocabularies for\nimage categorization. In CVPR, 2007.\n[31] T. Raiko, H. Valpola, and Y. LeCun. Deep learning made easier by\nlinear transformations in perceptrons. In AISTATS, 2012.\n[32] S. Ren, K. He, R. Girshick, and J. Sun. Faster R-CNN: Towards\nreal-time object detection with region proposal networks. In NIPS,\n2015.\n[33] S. Ren, K. He, R. Girshick, X. Zhang, and J. Sun. Object detection\nnetworks on convolutional feature maps. arXiv:1504.06066, 2015.\n[34] B. D. Ripley. Pattern recognition and neural networks. Cambridge\nuniversity press, 1996.\n[35] A. Romero, N. Ballas, S. E. Kahou, A. Chassang, C. Gatta, and\nY. Bengio. Fitnets: Hints for thin deep nets. In ICLR, 2015.\n[36] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma,\nZ. Huang, A. Karpathy, A. Khosla, M. Bernstein, et al. Imagenet\nlarge scale visual recognition challenge. arXiv:1409.0575, 2014.\n[37] A. M. Saxe, J. L. McClelland, and S. Ganguli. Exact solutions to\nthe nonlinear dynamics of learning in deep linear neural networks.\narXiv:1312.6120, 2013.\n[38] N. N. Schraudolph. Accelerated gradient descent by factor-centering\ndecomposition. Technical report, 1998.\n[39] N. N. Schraudolph. Centering neural network gradient factors. In\nNeural Networks: Tricks of the Trade, pages 207\u2013226. Springer,\n1998.\n[40] P. Sermanet, D. Eigen, X. Zhang, M. Mathieu, R. Fergus, and Y. Le-\nCun.\nOverfeat: Integrated recognition, localization and detection\nusing convolutional networks. In ICLR, 2014.\n[41] K. Simonyan and A. Zisserman. Very deep convolutional networks\nfor large-scale image recognition. In ICLR, 2015.\n[42] R. K. Srivastava, K. Greff, and J. Schmidhuber. Highway networks.\narXiv:1505.00387, 2015.\n[43] R. K. Srivastava, K. Greff, and J. Schmidhuber. Training very deep\nnetworks. 1507.06228, 2015.\n[44] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Er-\nhan, V. Vanhoucke, and A. Rabinovich. Going deeper with convolu-\ntions. In CVPR, 2015.\n[45] R. Szeliski. Fast surface interpolation using hierarchical basis func-\ntions. TPAMI, 1990.\n[46] R. Szeliski. Locally adapted hierarchical basis preconditioning. In\nSIGGRAPH, 2006.\n[47] T. Vatanen, T. Raiko, H. Valpola, and Y. LeCun. Pushing stochas-\ntic gradient towards second-order methods\u2013backpropagation learn-\ning with transformations in nonlinearities.\nIn Neural Information\nProcessing, 2013.\n[48] A. Vedaldi and B. Fulkerson. VLFeat: An open and portable library\nof computer vision algorithms, 2008.\n[49] W. Venables and B. Ripley. Modern applied statistics with s-plus.\n1999.\n[50] M. D. Zeiler and R. Fergus. Visualizing and understanding convolu-\ntional neural networks. In ECCV, 2014.\n9\nA. Object Detection Baselines\nIn this section we introduce our detection method based\non the baseline Faster R-CNN [32] system. The models are\ninitialized by the ImageNet classi\ufb01cation models, and then\n\ufb01ne-tuned on the object detection data. We have experi-\nmented with ResNet-50/101 at the time of the ILSVRC &\nCOCO 2015 detection competitions.\nUnlike VGG-16 used in [32], our ResNet has no hidden\nfc layers. We adopt the idea of \u201cNetworks on Conv fea-\nture maps\u201d (NoC) [33] to address this issue. We compute\nthe full-image shared conv feature maps using those lay-\ners whose strides on the image are no greater than 16 pixels\n(i.e., conv1, conv2 x, conv3 x, and conv4 x, totally 91 conv\nlayers in ResNet-101; Table 1). We consider these layers as\nanalogous to the 13 conv layers in VGG-16, and by doing\nso, both ResNet and VGG-16 have conv feature maps of the\nsame total stride (16 pixels). These layers are shared by a\nregion proposal network (RPN, generating 300 proposals)\n[32] and a Fast R-CNN detection network [7]. RoI pool-\ning [7] is performed before conv5 1. On this RoI-pooled\nfeature, all layers of conv5 x and up are adopted for each\nregion, playing the roles of VGG-16\u2019s fc layers. The \ufb01nal\nclassi\ufb01cation layer is replaced by two sibling layers (classi-\n\ufb01cation and box regression [7]).\nFor the usage of BN layers, after pre-training, we com-\npute the BN statistics (means and variances) for each layer\non the ImageNet training set. Then the BN layers are \ufb01xed\nduring \ufb01ne-tuning for object detection. As such, the BN\nlayers become linear activations with constant offsets and\nscales, and BN statistics are not updated by \ufb01ne-tuning. We\n\ufb01x the BN layers mainly for reducing memory consumption\nin Faster R-CNN training.\nPASCAL VOC\nFollowing [7, 32], for the PASCAL VOC 2007 test set,\nwe use the 5k trainval images in VOC 2007 and 16k train-\nval images in VOC 2012 for training (\u201c07+12\u201d). For the\nPASCAL VOC 2012 test set, we use the 10k trainval+test\nimages in VOC 2007 and 16k trainval images in VOC 2012\nfor training (\u201c07++12\u201d). The hyper-parameters for train-\ning Faster R-CNN are the same as in [32]. Table 7 shows\nthe results. ResNet-101 improves the mAP by >3% over\nVGG-16. This gain is solely because of the improved fea-\ntures learned by ResNet.\nMS COCO\nThe MS COCO dataset [26] involves 80 object cate-\ngories. We evaluate the PASCAL VOC metric (mAP @\nIoU = 0.5) and the standard COCO metric (mAP @ IoU =\n.5:.05:.95). We use the 80k images on the train set for train-\ning and the 40k images on the val set for evaluation. Our\ndetection system for COCO is similar to that for PASCAL\nVOC. We train the COCO models with an 8-GPU imple-\nmentation, and thus the RPN step has a mini-batch size of\n8 images (i.e., 1 per GPU) and the Fast R-CNN step has a\nmini-batch size of 16 images. The RPN step and Fast R-\nCNN step are both trained for 240k iterations with a learn-\ning rate of 0.001 and then for 80k iterations with 0.0001.\nTable 8 shows the results on the MS COCO validation\nset. ResNet-101 has a 6% increase of mAP@[.5, .95] over\nVGG-16, which is a 28% relative improvement, solely con-\ntributed by the features learned by the better network. Re-\nmarkably, the mAP@[.5, .95]\u2019s absolute increase (6.0%) is\nnearly as big as mAP@.5\u2019s (6.9%). This suggests that a\ndeeper network can improve both recognition and localiza-\ntion.\nB. Object Detection Improvements\nFor completeness, we report the improvements made for\nthe competitions. These improvements are based on deep\nfeatures and thus should bene\ufb01t from residual learning.\nMS COCO\nBox re\ufb01nement. Our box re\ufb01nement partially follows the it-\nerative localization in [6]. In Faster R-CNN, the \ufb01nal output\nis a regressed box that is different from its proposal box. So\nfor inference, we pool a new feature from the regressed box\nand obtain a new classi\ufb01cation score and a new regressed\nbox. We combine these 300 new predictions with the orig-\ninal 300 predictions. Non-maximum suppression (NMS) is\napplied on the union set of predicted boxes using an IoU\nthreshold of 0.3 [8], followed by box voting [6]. Box re-\n\ufb01nement improves mAP by about 2 points (Table 9).\nGlobal context.\nWe combine global context in the Fast\nR-CNN step. Given the full-image conv feature map, we\npool a feature by global Spatial Pyramid Pooling [12] (with\na \u201csingle-level\u201d pyramid) which can be implemented as\n\u201cRoI\u201d pooling using the entire image\u2019s bounding box as the\nRoI. This pooled feature is fed into the post-RoI layers to\nobtain a global context feature. This global feature is con-\ncatenated with the original per-region feature, followed by\nthe sibling classi\ufb01cation and box regression layers. This\nnew structure is trained end-to-end.\nGlobal context im-\nproves mAP@.5 by about 1 point (Table 9).\nMulti-scale testing. In the above, all results are obtained by\nsingle-scale training/testing as in [32], where the image\u2019s\nshorter side is s = 600 pixels. Multi-scale training/testing\nhas been developed in [12, 7] by selecting a scale from a\nfeature pyramid, and in [33] by using maxout layers. In\nour current implementation, we have performed multi-scale\ntesting following [33]; we have not performed multi-scale\ntraining because of limited time. In addition, we have per-\nformed multi-scale testing only for the Fast R-CNN step\n(but not yet for the RPN step). With a trained model, we\ncompute conv feature maps on an image pyramid, where the\nimage\u2019s shorter sides are s \u2208{200, 400, 600, 800, 1000}.\n10\ntraining data\nCOCO train\nCOCO trainval\ntest data\nCOCO val\nCOCO test-dev\nmAP\n@.5\n@[.5, .95]\n@.5\n@[.5, .95]\nbaseline Faster R-CNN (VGG-16)\n41.5\n21.2\nbaseline Faster R-CNN (ResNet-101)\n48.4\n27.2\n+box re\ufb01nement\n49.9\n29.9\n+context\n51.1\n30.0\n53.3\n32.2\n+multi-scale testing\n53.8\n32.5\n55.7\n34.9\nensemble\n59.0\n37.4\nTable 9. Object detection improvements on MS COCO using Faster R-CNN and ResNet-101.\nsystem\nnet\ndata\nmAP\nareo\nbike\nbird\nboat\nbottle\nbus\ncar\ncat\nchair\ncow\ntable\ndog\nhorse\nmbike person\nplant\nsheep\nsofa\ntrain\ntv\nbaseline\nVGG-16\n07+12\n73.2\n76.5 79.0 70.9 65.5 52.1 83.1 84.7 86.4 52.0 81.9 65.7 84.8 84.6 77.5 76.7 38.8 73.6 73.9 83.0 72.6\nbaseline\nResNet-101\n07+12\n76.4\n79.8 80.7 76.2 68.3 55.9 85.1 85.3 89.8 56.7 87.8 69.4 88.3 88.9 80.9 78.4 41.7 78.6 79.8 85.3 72.0\nbaseline+++ ResNet-101\nCOCO+07+12\n85.6\n90.0 89.6 87.8 80.8 76.1 89.9 89.9 89.6 75.5 90.0 80.7 89.6 90.3 89.1 88.7 65.4 88.1 85.6 89.0 86.8\nTable 10. Detection results on the PASCAL VOC 2007 test set. The baseline is the Faster R-CNN system. The system \u201cbaseline+++\u201d\ninclude box re\ufb01nement, context, and multi-scale testing in Table 9.\nsystem\nnet\ndata\nmAP\nareo\nbike\nbird\nboat\nbottle\nbus\ncar\ncat\nchair\ncow\ntable\ndog\nhorse\nmbike person\nplant\nsheep\nsofa\ntrain\ntv\nbaseline\nVGG-16\n07++12\n70.4\n84.9 79.8 74.3 53.9 49.8 77.5 75.9 88.5 45.6 77.1 55.3 86.9 81.7 80.9 79.6 40.1 72.6 60.9 81.2 61.5\nbaseline\nResNet-101\n07++12\n73.8\n86.5 81.6 77.2 58.0 51.0 78.6 76.6 93.2 48.6 80.4 59.0 92.1 85.3 84.8 80.7 48.1 77.3 66.5 84.7 65.6\nbaseline+++ ResNet-101 COCO+07++12\n83.8\n92.1 88.4 84.8 75.9 71.4 86.3 87.8 94.2 66.8 89.4 69.2 93.9 91.9 90.9 89.6 67.9 88.2 76.8 90.3 80.0\nTable 11. Detection results on the PASCAL VOC 2012 test set (http://host.robots.ox.ac.uk:8080/leaderboard/\ndisplaylb.php?challengeid=11&compid=4). The baseline is the Faster R-CNN system. The system \u201cbaseline+++\u201d include\nbox re\ufb01nement, context, and multi-scale testing in Table 9.\nWe select two adjacent scales from the pyramid following\n[33]. RoI pooling and subsequent layers are performed on\nthe feature maps of these two scales [33], which are merged\nby maxout as in [33]. Multi-scale testing improves the mAP\nby over 2 points (Table 9).\nUsing validation data. Next we use the 80k+40k trainval set\nfor training and the 20k test-dev set for evaluation. The test-\ndev set has no publicly available ground truth and the result\nis reported by the evaluation server. Under this setting, the\nresults are an mAP@.5 of 55.7% and an mAP@[.5, .95] of\n34.9% (Table 9). This is our single-model result.\nEnsemble. In Faster R-CNN, the system is designed to learn\nregion proposals and also object classi\ufb01ers, so an ensemble\ncan be used to boost both tasks. We use an ensemble for\nproposing regions, and the union set of proposals are pro-\ncessed by an ensemble of per-region classi\ufb01ers. Table 9\nshows our result based on an ensemble of 3 networks. The\nmAP is 59.0% and 37.4% on the test-dev set. This result\nwon the 1st place in the detection task in COCO 2015.\nPASCAL VOC\nWe revisit the PASCAL VOC dataset based on the above\nmodel. With the single model on the COCO dataset (55.7%\nmAP@.5 in Table 9), we \ufb01ne-tune this model on the PAS-\nCAL VOC sets. The improvements of box re\ufb01nement, con-\ntext, and multi-scale testing are also adopted. By doing so\nval2\ntest\nGoogLeNet [44] (ILSVRC\u201914)\n-\n43.9\nour single model (ILSVRC\u201915)\n60.5\n58.8\nour ensemble (ILSVRC\u201915)\n63.6\n62.1\nTable 12. Our results (mAP, %) on the ImageNet detection dataset.\nOur detection system is Faster R-CNN [32] with the improvements\nin Table 9, using ResNet-101.\nwe achieve 85.6% mAP on PASCAL VOC 2007 (Table 10)\nand 83.8% on PASCAL VOC 2012 (Table 11)6. The result\non PASCAL VOC 2012 is 10 points higher than the previ-\nous state-of-the-art result [6].\nImageNet Detection\nThe ImageNet Detection (DET) task involves 200 object\ncategories. The accuracy is evaluated by mAP@.5. Our\nobject detection algorithm for ImageNet DET is the same\nas that for MS COCO in Table 9. The networks are pre-\ntrained on the 1000-class ImageNet classi\ufb01cation set, and\nare \ufb01ne-tuned on the DET data. We split the validation set\ninto two parts (val1/val2) following [8]. We \ufb01ne-tune the\ndetection models using the DET training set and the val1\nset. The val2 set is used for validation. We do not use other\nILSVRC 2015 data. Our single model with ResNet-101 has\n6http://host.robots.ox.ac.uk:8080/anonymous/3OJ4OJ.html,\nsubmitted on 2015-11-26.\n11\nLOC\nmethod\nLOC\nnetwork\ntesting LOC error\non GT CLS\nclassi\ufb01cation\nnetwork\ntop-5 LOC error\non predicted CLS\nVGG\u2019s [41]\nVGG-16\n1-crop\n33.1 [41]\nRPN\nResNet-101 1-crop\n13.3\nRPN\nResNet-101 dense\n11.7\nRPN\nResNet-101 dense\nResNet-101\n14.4\nRPN+RCNN ResNet-101 dense\nResNet-101\n10.6\nRPN+RCNN\nensemble\ndense\nensemble\n8.9\nTable 13. Localization error (%) on the ImageNet validation. In\nthe column of \u201cLOC error on GT class\u201d ([41]), the ground truth\nclass is used. In the \u201ctesting\u201d column, \u201c1-crop\u201d denotes testing\non a center crop of 224\u00d7224 pixels, \u201cdense\u201d denotes dense (fully\nconvolutional) and multi-scale testing.\n58.8% mAP and our ensemble of 3 models has 62.1% mAP\non the DET test set (Table 12). This result won the 1st place\nin the ImageNet detection task in ILSVRC 2015, surpassing\nthe second place by 8.5 points (absolute).\nC. ImageNet Localization\nThe ImageNet Localization (LOC) task [36] requires to\nclassify and localize the objects. Following [40, 41], we\nassume that the image-level classi\ufb01ers are \ufb01rst adopted for\npredicting the class labels of an image, and the localiza-\ntion algorithm only accounts for predicting bounding boxes\nbased on the predicted classes. We adopt the \u201cper-class re-\ngression\u201d (PCR) strategy [40, 41], learning a bounding box\nregressor for each class. We pre-train the networks for Im-\nageNet classi\ufb01cation and then \ufb01ne-tune them for localiza-\ntion. We train networks on the provided 1000-class Ima-\ngeNet training set.\nOur localization algorithm is based on the RPN frame-\nwork of [32] with a few modi\ufb01cations. Unlike the way in\n[32] that is category-agnostic, our RPN for localization is\ndesigned in a per-class form. This RPN ends with two sib-\nling 1\u00d71 convolutional layers for binary classi\ufb01cation (cls)\nand box regression (reg), as in [32]. The cls and reg layers\nare both in a per-class from, in contrast to [32]. Speci\ufb01-\ncally, the cls layer has a 1000-d output, and each dimension\nis binary logistic regression for predicting being or not be-\ning an object class; the reg layer has a 1000\u00d74-d output\nconsisting of box regressors for 1000 classes. As in [32],\nour bounding box regression is with reference to multiple\ntranslation-invariant \u201canchor\u201d boxes at each position.\nAs in our ImageNet classi\ufb01cation training (Sec. 3.4), we\nrandomly sample 224\u00d7224 crops for data augmentation.\nWe use a mini-batch size of 256 images for \ufb01ne-tuning. To\navoid negative samples being dominate, 8 anchors are ran-\ndomly sampled for each image, where the sampled positive\nand negative anchors have a ratio of 1:1 [32]. For testing,\nthe network is applied on the image fully-convolutionally.\nTable 13 compares the localization results. Following\n[41], we \ufb01rst perform \u201coracle\u201d testing using the ground truth\nclass as the classi\ufb01cation prediction. VGG\u2019s paper [41] re-\nmethod\ntop-5 localization err\nval\ntest\nOverFeat [40] (ILSVRC\u201913)\n30.0\n29.9\nGoogLeNet [44] (ILSVRC\u201914)\n-\n26.7\nVGG [41] (ILSVRC\u201914)\n26.9\n25.3\nours (ILSVRC\u201915)\n8.9\n9.0\nTable 14. Comparisons of localization error (%) on the ImageNet\ndataset with state-of-the-art methods.\nports a center-crop error of 33.1% (Table 13) using ground\ntruth classes. Under the same setting, our RPN method us-\ning ResNet-101 net signi\ufb01cantly reduces the center-crop er-\nror to 13.3%. This comparison demonstrates the excellent\nperformance of our framework. With dense (fully convolu-\ntional) and multi-scale testing, our ResNet-101 has an error\nof 11.7% using ground truth classes. Using ResNet-101 for\npredicting classes (4.6% top-5 classi\ufb01cation error, Table 4),\nthe top-5 localization error is 14.4%.\nThe above results are only based on the proposal network\n(RPN) in Faster R-CNN [32]. One may use the detection\nnetwork (Fast R-CNN [7]) in Faster R-CNN to improve the\nresults. But we notice that on this dataset, one image usually\ncontains a single dominate object, and the proposal regions\nhighly overlap with each other and thus have very similar\nRoI-pooled features. As a result, the image-centric training\nof Fast R-CNN [7] generates samples of small variations,\nwhich may not be desired for stochastic training. Motivated\nby this, in our current experiment we use the original R-\nCNN [8] that is RoI-centric, in place of Fast R-CNN.\nOur R-CNN implementation is as follows. We apply the\nper-class RPN trained as above on the training images to\npredict bounding boxes for the ground truth class. These\npredicted boxes play a role of class-dependent proposals.\nFor each training image, the highest scored 200 proposals\nare extracted as training samples to train an R-CNN classi-\n\ufb01er. The image region is cropped from a proposal, warped\nto 224\u00d7224 pixels, and fed into the classi\ufb01cation network\nas in R-CNN [8]. The outputs of this network consist of two\nsibling fc layers for cls and reg, also in a per-class form.\nThis R-CNN network is \ufb01ne-tuned on the training set us-\ning a mini-batch size of 256 in the RoI-centric fashion. For\ntesting, the RPN generates the highest scored 200 proposals\nfor each predicted class, and the R-CNN network is used to\nupdate these proposals\u2019 scores and box positions.\nThis method reduces the top-5 localization error to\n10.6% (Table 13). This is our single-model result on the\nvalidation set. Using an ensemble of networks for both clas-\nsi\ufb01cation and localization, we achieve a top-5 localization\nerror of 9.0% on the test set. This number signi\ufb01cantly out-\nperforms the ILSVRC 14 results (Table 14), showing a 64%\nrelative reduction of error. This result won the 1st place in\nthe ImageNet localization task in ILSVRC 2015.\n12\n",
        "sentence": " , 1989) have pushed the limits of computer vision (Krizhevsky et al., 2012; He et al., 2016), leading to important progress in a variety of tasks, like object detection (Girshick, 2015) or image segmentation (Pinheiro et al.",
        "context": "[36] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma,\nZ. Huang, A. Karpathy, A. Khosla, M. Bernstein, et al. Imagenet\nlarge scale visual recognition challenge. arXiv:1409.0575, 2014.\n[8] R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich feature hier-\narchies for accurate object detection and semantic segmentation. In\nCVPR, 2014.\n[9] X. Glorot and Y. Bengio. Understanding the dif\ufb01culty of training\nserman. The Pascal Visual Object Classes (VOC) Challenge. IJCV,\npages 303\u2013338, 2010.\n[6] S. Gidaris and N. Komodakis. Object detection via a multi-region &\nsemantic segmentation-aware cnn model. In ICCV, 2015.\n[7] R. Girshick. Fast R-CNN. In ICCV, 2015."
    },
    {
        "title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift",
        "author": [
            "S. Ioffe",
            "C. Szegedy"
        ],
        "venue": "arXiv preprint arXiv:1502.03167,",
        "citeRegEx": "Ioffe and Szegedy,? \\Q2015\\E",
        "shortCiteRegEx": "Ioffe and Szegedy",
        "year": 2015,
        "abstract": "Training Deep Neural Networks is complicated by the fact that the\ndistribution of each layer's inputs changes during training, as the parameters\nof the previous layers change. This slows down the training by requiring lower\nlearning rates and careful parameter initialization, and makes it notoriously\nhard to train models with saturating nonlinearities. We refer to this\nphenomenon as internal covariate shift, and address the problem by normalizing\nlayer inputs. Our method draws its strength from making normalization a part of\nthe model architecture and performing the normalization for each training\nmini-batch. Batch Normalization allows us to use much higher learning rates and\nbe less careful about initialization. It also acts as a regularizer, in some\ncases eliminating the need for Dropout. Applied to a state-of-the-art image\nclassification model, Batch Normalization achieves the same accuracy with 14\ntimes fewer training steps, and beats the original model by a significant\nmargin. Using an ensemble of batch-normalized networks, we improve upon the\nbest published result on ImageNet classification: reaching 4.9% top-5\nvalidation error (and 4.8% test error), exceeding the accuracy of human raters.",
        "full_text": "arXiv:1502.03167v3  [cs.LG]  2 Mar 2015\nBatch Normalization: Accelerating Deep Network Training by\nReducing Internal Covariate Shift\nSergey Ioffe\nGoogle Inc., sioffe@google.com\nChristian Szegedy\nGoogle Inc., szegedy@google.com\nAbstract\nTraining Deep Neural Networks is complicated by the fact\nthat the distribution of each layer\u2019s inputs changes during\ntraining, as the parameters of the previous layers change.\nThis slows down the training by requiring lower learning\nrates and careful parameter initialization, and makes it no-\ntoriously hard to train models with saturating nonlineari-\nties. We refer to this phenomenon as internal covariate\nshift, and address the problem by normalizing layer in-\nputs. Our method draws its strength from making normal-\nization a part of the model architecture and performing the\nnormalization for each training mini-batch. Batch Nor-\nmalization allows us to use much higher learning rates and\nbe less careful about initialization. It also acts as a regu-\nlarizer, in some cases eliminating the need for Dropout.\nApplied to a state-of-the-art image classi\ufb01cation model,\nBatch Normalization achieves the same accuracy with 14\ntimes fewer training steps, and beats the original model\nby a signi\ufb01cant margin.\nUsing an ensemble of batch-\nnormalized networks, we improve upon the best published\nresult on ImageNet classi\ufb01cation: reaching 4.9% top-5\nvalidation error (and 4.8% test error), exceeding the ac-\ncuracy of human raters.\n1\nIntroduction\nDeep learning has dramatically advanced the state of the\nart in vision, speech, and many other areas.\nStochas-\ntic gradient descent (SGD) has proved to be an effec-\ntive way of training deep networks, and SGD variants\nsuch as momentum (Sutskever et al., 2013) and Adagrad\n(Duchi et al., 2011) have been used to achieve state of the\nart performance. SGD optimizes the parameters \u0398 of the\nnetwork, so as to minimize the loss\n\u0398 = arg min\n\u0398\n1\nN\nN\nX\ni=1\n\u2113(xi, \u0398)\nwhere x1...N is the training data set. With SGD, the train-\ning proceeds in steps, and at each step we consider a mini-\nbatch x1...m of size m. The mini-batch is used to approx-\nimate the gradient of the loss function with respect to the\nparameters, by computing\n1\nm\n\u2202\u2113(xi, \u0398)\n\u2202\u0398\n.\nUsing mini-batches of examples, as opposed to one exam-\nple at a time, is helpful in several ways. First, the gradient\nof the loss over a mini-batch is an estimate of the gradient\nover the training set, whose quality improves as the batch\nsize increases. Second, computation over a batch can be\nmuch more ef\ufb01cient than m computations for individual\nexamples, due to the parallelism afforded by the modern\ncomputing platforms.\nWhile stochastic gradient is simple and effective, it\nrequires careful tuning of the model hyper-parameters,\nspeci\ufb01cally the learning rate used in optimization, as well\nas the initial values for the model parameters. The train-\ning is complicated by the fact that the inputs to each layer\nare affected by the parameters of all preceding layers \u2013 so\nthat small changes to the network parameters amplify as\nthe network becomes deeper.\nThe change in the distributions of layers\u2019 inputs\npresents a problem because the layers need to continu-\nously adapt to the new distribution. When the input dis-\ntribution to a learning system changes, it is said to experi-\nence covariate shift (Shimodaira, 2000). This is typically\nhandled via domain adaptation (Jiang, 2008). However,\nthe notion of covariate shift can be extended beyond the\nlearning system as a whole, to apply to its parts, such as a\nsub-network or a layer. Consider a network computing\n\u2113= F2(F1(u, \u03981), \u03982)\nwhere F1 and F2 are arbitrary transformations, and the\nparameters \u03981, \u03982 are to be learned so as to minimize\nthe loss \u2113. Learning \u03982 can be viewed as if the inputs\nx = F1(u, \u03981) are fed into the sub-network\n\u2113= F2(x, \u03982).\nFor example, a gradient descent step\n\u03982 \u2190\u03982 \u2212\u03b1\nm\nm\nX\ni=1\n\u2202F2(xi, \u03982)\n\u2202\u03982\n(for batch size m and learning rate \u03b1) is exactly equivalent\nto that for a stand-alone network F2 with input x. There-\nfore, the input distribution properties that make training\nmore ef\ufb01cient \u2013 such as having the same distribution be-\ntween the training and test data \u2013 apply to training the\nsub-network as well. As such it is advantageous for the\ndistribution of x to remain \ufb01xed over time. Then, \u03982 does\n1\nnot have to readjust to compensate for the change in the\ndistribution of x.\nFixed distribution of inputs to a sub-network would\nhave positive consequences for the layers outside the sub-\nnetwork, as well. Consider a layer with a sigmoid activa-\ntion function z = g(Wu + b) where u is the layer input,\nthe weight matrix W and bias vector b are the layer pa-\nrameters to be learned, and g(x) =\n1\n1+exp(\u2212x). As |x|\nincreases, g\u2032(x) tends to zero. This means that for all di-\nmensions of x = Wu+b except those with small absolute\nvalues, the gradient \ufb02owing down to u will vanish and the\nmodel will train slowly. However, since x is affected by\nW, b and the parameters of all the layers below, changes\nto those parameters during training will likely move many\ndimensions of x into the saturated regime of the nonlin-\nearity and slow down the convergence.\nThis effect is\nampli\ufb01ed as the network depth increases.\nIn practice,\nthe saturation problem and the resulting vanishing gradi-\nents are usually addressed by using Recti\ufb01ed Linear Units\n(Nair & Hinton, 2010) ReLU(x) = max(x, 0), careful\ninitialization (Bengio & Glorot, 2010; Saxe et al., 2013),\nand small learning rates. If, however, we could ensure\nthat the distribution of nonlinearity inputs remains more\nstable as the network trains, then the optimizer would be\nless likely to get stuck in the saturated regime, and the\ntraining would accelerate.\nWe refer to the change in the distributions of internal\nnodes of a deep network, in the course of training, as In-\nternal Covariate Shift. Eliminating it offers a promise of\nfaster training. We propose a new mechanism, which we\ncall Batch Normalization, that takes a step towards re-\nducing internal covariate shift, and in doing so dramati-\ncally accelerates the training of deep neural nets. It ac-\ncomplishes this via a normalization step that \ufb01xes the\nmeans and variances of layer inputs. Batch Normalization\nalso has a bene\ufb01cial effect on the gradient \ufb02ow through\nthe network, by reducing the dependence of gradients\non the scale of the parameters or of their initial values.\nThis allows us to use much higher learning rates with-\nout the risk of divergence. Furthermore, batch normal-\nization regularizes the model and reduces the need for\nDropout (Srivastava et al., 2014). Finally, Batch Normal-\nization makes it possible to use saturating nonlinearities\nby preventing the network from getting stuck in the satu-\nrated modes.\nIn Sec. 4.2, we apply Batch Normalization to the best-\nperforming ImageNet classi\ufb01cation network, and show\nthat we can match its performance using only 7% of the\ntraining steps, and can further exceed its accuracy by a\nsubstantial margin. Using an ensemble of such networks\ntrained with Batch Normalization, we achieve the top-5\nerror rate that improves upon the best known results on\nImageNet classi\ufb01cation.\n2\nTowards\nReducing\nInternal\nCovariate Shift\nWe de\ufb01ne Internal Covariate Shift as the change in the\ndistribution of network activations due to the change in\nnetwork parameters during training. To improve the train-\ning, we seek to reduce the internal covariate shift. By\n\ufb01xing the distribution of the layer inputs x as the training\nprogresses, we expect to improve the training speed. It has\nbeen long known (LeCun et al., 1998b; Wiesler & Ney,\n2011) that the network training converges faster if its in-\nputs are whitened \u2013 i.e., linearly transformed to have zero\nmeans and unit variances, and decorrelated. As each layer\nobserves the inputs produced by the layers below, it would\nbe advantageous to achieve the same whitening of the in-\nputs of each layer. By whitening the inputs to each layer,\nwe would take a step towards achieving the \ufb01xed distri-\nbutions of inputs that would remove the ill effects of the\ninternal covariate shift.\nWe could consider whitening activations at every train-\ning step or at some interval, either by modifying the\nnetwork directly or by changing the parameters of the\noptimization algorithm to depend on the network ac-\ntivation values (Wiesler et al., 2014; Raiko et al., 2012;\nPovey et al., 2014; Desjardins & Kavukcuoglu).\nHow-\never, if these modi\ufb01cations are interspersed with the op-\ntimization steps, then the gradient descent step may at-\ntempt to update the parameters in a way that requires\nthe normalization to be updated, which reduces the ef-\nfect of the gradient step. For example, consider a layer\nwith the input u that adds the learned bias b, and normal-\nizes the result by subtracting the mean of the activation\ncomputed over the training data: bx = x \u2212E[x] where\nx = u + b, X = {x1...N} is the set of values of x over\nthe training set, and E[x] =\n1\nN\nPN\ni=1 xi. If a gradient\ndescent step ignores the dependence of E[x] on b, then it\nwill update b \u2190b + \u2206b, where \u2206b \u221d\u2212\u2202\u2113/\u2202bx. Then\nu + (b + \u2206b) \u2212E[u + (b + \u2206b)] = u + b \u2212E[u + b].\nThus, the combination of the update to b and subsequent\nchange in normalization led to no change in the output\nof the layer nor, consequently, the loss. As the training\ncontinues, b will grow inde\ufb01nitely while the loss remains\n\ufb01xed. This problem can get worse if the normalization not\nonly centers but also scales the activations. We have ob-\nserved this empirically in initial experiments, where the\nmodel blows up when the normalization parameters are\ncomputed outside the gradient descent step.\nThe issue with the above approach is that the gradient\ndescent optimization does not take into account the fact\nthat the normalization takes place. To address this issue,\nwe would like to ensure that, for any parameter values,\nthe network always produces activations with the desired\ndistribution. Doing so would allow the gradient of the\nloss with respect to the model parameters to account for\nthe normalization, and for its dependence on the model\nparameters \u0398. Let again x be a layer input, treated as a\n2\nvector, and X be the set of these inputs over the training\ndata set. The normalization can then be written as a trans-\nformation\nbx = Norm(x, X)\nwhich depends not only on the given training example x\nbut on all examples X \u2013 each of which depends on \u0398 if\nx is generated by another layer. For backpropagation, we\nwould need to compute the Jacobians\n\u2202Norm(x, X)\n\u2202x\nand \u2202Norm(x, X)\n\u2202X\n;\nignoring the latter term would lead to the explosion de-\nscribed above. Within this framework, whitening the layer\ninputs is expensive, as it requires computing the covari-\nance matrix Cov[x] = Ex\u2208X [xxT ] \u2212E[x]E[x]T and its\ninverse square root, to produce the whitened activations\nCov[x]\u22121/2(x \u2212E[x]), as well as the derivatives of these\ntransforms for backpropagation. This motivates us to seek\nan alternative that performs input normalization in a way\nthat is differentiable and does not require the analysis of\nthe entire training set after every parameter update.\nSome\nof\nthe\nprevious\napproaches\n(e.g.\n(Lyu & Simoncelli,\n2008))\nuse\nstatistics\ncomputed\nover a single training example, or, in the case of image\nnetworks, over different feature maps at a given location.\nHowever, this changes the representation ability of a\nnetwork by discarding the absolute scale of activations.\nWe want to a preserve the information in the network, by\nnormalizing the activations in a training example relative\nto the statistics of the entire training data.\n3\nNormalization\nvia\nMini-Batch\nStatistics\nSince the full whitening of each layer\u2019s inputs is costly\nand not everywhere differentiable, we make two neces-\nsary simpli\ufb01cations. The \ufb01rst is that instead of whitening\nthe features in layer inputs and outputs jointly, we will\nnormalize each scalar feature independently, by making it\nhave the mean of zero and the variance of 1. For a layer\nwith d-dimensional input x = (x(1) . . . x(d)), we will nor-\nmalize each dimension\nbx(k) = x(k) \u2212E[x(k)]\np\nVar[x(k)]\nwhere the expectation and variance are computed over the\ntraining data set. As shown in (LeCun et al., 1998b), such\nnormalization speeds up convergence, even when the fea-\ntures are not decorrelated.\nNote that simply normalizing each input of a layer may\nchange what the layer can represent. For instance, nor-\nmalizing the inputs of a sigmoid would constrain them to\nthe linear regime of the nonlinearity. To address this, we\nmake sure that the transformation inserted in the network\ncan represent the identity transform. To accomplish this,\nwe introduce, for each activation x(k), a pair of parameters\n\u03b3(k), \u03b2(k), which scale and shift the normalized value:\ny(k) = \u03b3(k)bx(k) + \u03b2(k).\nThese parameters are learned along with the original\nmodel parameters, and restore the representation power\nof the network. Indeed, by setting \u03b3(k) =\np\nVar[x(k)] and\n\u03b2(k) = E[x(k)], we could recover the original activations,\nif that were the optimal thing to do.\nIn the batch setting where each training step is based on\nthe entire training set, we would use the whole set to nor-\nmalize activations. However, this is impractical when us-\ning stochastic optimization. Therefore, we make the sec-\nond simpli\ufb01cation: since we use mini-batches in stochas-\ntic gradient training, each mini-batch produces estimates\nof the mean and variance of each activation. This way, the\nstatistics used for normalization can fully participate in\nthe gradient backpropagation. Note that the use of mini-\nbatches is enabled by computation of per-dimension vari-\nances rather than joint covariances; in the joint case, reg-\nularization would be required since the mini-batch size is\nlikely to be smaller than the number of activations being\nwhitened, resulting in singular covariance matrices.\nConsider a mini-batch B of size m. Since the normal-\nization is applied to each activation independently, let us\nfocus on a particular activation x(k) and omit k for clarity.\nWe have m values of this activation in the mini-batch,\nB = {x1...m}.\nLet the normalized values be bx1...m, and their linear trans-\nformations be y1...m. We refer to the transform\nBN\u03b3,\u03b2 : x1...m \u2192y1...m\nas the Batch Normalizing Transform. We present the BN\nTransform in Algorithm 1. In the algorithm, \u01eb is a constant\nadded to the mini-batch variance for numerical stability.\nInput: Values of x over a mini-batch: B = {x1...m};\nParameters to be learned: \u03b3, \u03b2\nOutput: {yi = BN\u03b3,\u03b2(xi)}\n\u00b5B \u21901\nm\nm\nX\ni=1\nxi\n// mini-batch mean\n\u03c32\nB \u21901\nm\nm\nX\ni=1\n(xi \u2212\u00b5B)2\n// mini-batch variance\nbxi \u2190xi \u2212\u00b5B\np\n\u03c32\nB + \u01eb\n// normalize\nyi \u2190\u03b3bxi + \u03b2 \u2261BN\u03b3,\u03b2(xi)\n// scale and shift\nAlgorithm 1: Batch Normalizing Transform, applied to\nactivation x over a mini-batch.\nThe BN transform can be added to a network to manip-\nulate any activation. In the notation y = BN\u03b3,\u03b2(x), we\n3\nindicate that the parameters \u03b3 and \u03b2 are to be learned,\nbut it should be noted that the BN transform does not\nindependently process the activation in each training ex-\nample. Rather, BN\u03b3,\u03b2(x) depends both on the training\nexample and the other examples in the mini-batch. The\nscaled and shifted values y are passed to other network\nlayers. The normalized activations bx are internal to our\ntransformation, but their presence is crucial. The distri-\nbutions of values of any bx has the expected value of 0\nand the variance of 1, as long as the elements of each\nmini-batch are sampled from the same distribution, and\nif we neglect \u01eb.\nThis can be seen by observing that\nPm\ni=1 bxi = 0 and\n1\nm\nPm\ni=1 bx2\ni = 1, and taking expec-\ntations. Each normalized activation bx(k) can be viewed as\nan input to a sub-network composed of the linear trans-\nform y(k) = \u03b3(k)bx(k) + \u03b2(k), followed by the other pro-\ncessing done by the original network. These sub-network\ninputs all have \ufb01xed means and variances, and although\nthe joint distribution of these normalized bx(k) can change\nover the course of training, we expect that the introduc-\ntion of normalized inputs accelerates the training of the\nsub-network and, consequently, the network as a whole.\nDuring training we need to backpropagate the gradi-\nent of loss \u2113through this transformation, as well as com-\npute the gradients with respect to the parameters of the\nBN transform. We use chain rule, as follows (before sim-\npli\ufb01cation):\n\u2202\u2113\n\u2202bxi =\n\u2202\u2113\n\u2202yi \u00b7 \u03b3\n\u2202\u2113\n\u2202\u03c32\nB = Pm\ni=1\n\u2202\u2113\n\u2202bxi \u00b7 (xi \u2212\u00b5B) \u00b7 \u22121\n2 (\u03c32\nB + \u01eb)\u22123/2\n\u2202\u2113\n\u2202\u00b5B =\n\u0012 Pm\ni=1\n\u2202\u2113\n\u2202bxi \u00b7\n\u22121\n\u221a\n\u03c32\nB+\u01eb\n\u0013\n+\n\u2202\u2113\n\u2202\u03c32\nB \u00b7\nPm\ni=1 \u22122(xi\u2212\u00b5B)\nm\n\u2202\u2113\n\u2202xi =\n\u2202\u2113\n\u2202bxi \u00b7\n1\n\u221a\n\u03c32\nB+\u01eb +\n\u2202\u2113\n\u2202\u03c32\nB \u00b7 2(xi\u2212\u00b5B)\nm\n+\n\u2202\u2113\n\u2202\u00b5B \u00b7 1\nm\n\u2202\u2113\n\u2202\u03b3 = Pm\ni=1\n\u2202\u2113\n\u2202yi \u00b7 bxi\n\u2202\u2113\n\u2202\u03b2 = Pm\ni=1\n\u2202\u2113\n\u2202yi\nThus, BN transform is a differentiable transformation that\nintroduces normalized activations into the network. This\nensures that as the model is training, layers can continue\nlearning on input distributions that exhibit less internal co-\nvariate shift, thus accelerating the training. Furthermore,\nthe learned af\ufb01ne transform applied to these normalized\nactivations allows the BN transform to represent the iden-\ntity transformation and preserves the network capacity.\n3.1\nTraining and Inference with Batch-\nNormalized Networks\nTo Batch-Normalize a network, we specify a subset of ac-\ntivations and insert the BN transform for each of them,\naccording to Alg. 1. Any layer that previously received\nx as the input, now receives BN(x). A model employing\nBatch Normalization can be trained using batch gradient\ndescent, or Stochastic Gradient Descent with a mini-batch\nsize m > 1, or with any of its variants such as Adagrad\n(Duchi et al., 2011). The normalization of activations that\ndepends on the mini-batch allows ef\ufb01cient training, but is\nneither necessary nor desirable during inference; we want\nthe output to depend only on the input, deterministically.\nFor this, once the network has been trained, we use the\nnormalization\nbx =\nx \u2212E[x]\np\nVar[x] + \u01eb\nusing the population, rather than mini-batch, statistics.\nNeglecting \u01eb, these normalized activations have the same\nmean 0 and variance 1 as during training. We use the un-\nbiased variance estimate Var[x] =\nm\nm\u22121 \u00b7 EB[\u03c32\nB], where\nthe expectation is over training mini-batches of size m and\n\u03c32\nB are their sample variances. Using moving averages in-\nstead, we can track the accuracy of a model as it trains.\nSince the means and variances are \ufb01xed during inference,\nthe normalization is simply a linear transform applied to\neach activation. It may further be composed with the scal-\ning by \u03b3 and shift by \u03b2, to yield a single linear transform\nthat replaces BN(x). Algorithm 2 summarizes the proce-\ndure for training batch-normalized networks.\nInput: Network N with trainable parameters \u0398;\nsubset of activations {x(k)}K\nk=1\nOutput: Batch-normalized network for inference, Ninf\nBN\n1: Ntr\nBN \u2190N\n// Training BN network\n2: for k = 1 . . . K do\n3:\nAdd transformation y(k) = BN\u03b3(k),\u03b2(k)(x(k)) to\nNtr\nBN (Alg. 1)\n4:\nModify each layer in Ntr\nBN with input x(k) to take\ny(k) instead\n5: end for\n6: Train\nNtr\nBN\nto\noptimize\nthe\nparameters\n\u0398 \u222a\n{\u03b3(k), \u03b2(k)}K\nk=1\n7: Ninf\nBN \u2190Ntr\nBN\n// Inference BN network with frozen\n// parameters\n8: for k = 1 . . . K do\n9:\n// For clarity, x \u2261x(k), \u03b3 \u2261\u03b3(k), \u00b5B \u2261\u00b5(k)\nB , etc.\n10:\nProcess multiple training mini-batches B, each of\nsize m, and average over them:\nE[x] \u2190EB[\u00b5B]\nVar[x] \u2190\nm\nm\u22121EB[\u03c32\nB]\n11:\nIn Ninf\nBN, replace the transform y = BN\u03b3,\u03b2(x) with\ny =\n\u03b3\n\u221a\nVar[x]+\u01eb \u00b7 x +\n\u0000\u03b2 \u2212\n\u03b3 E[x]\n\u221a\nVar[x]+\u01eb\n\u0001\n12: end for\nAlgorithm 2: Training a Batch-Normalized Network\n3.2\nBatch-Normalized Convolutional Net-\nworks\nBatch Normalization can be applied to any set of acti-\nvations in the network. Here, we focus on transforms\n4\nthat consist of an af\ufb01ne transformation followed by an\nelement-wise nonlinearity:\nz = g(Wu + b)\nwhere W and b are learned parameters of the model, and\ng(\u00b7) is the nonlinearity such as sigmoid or ReLU. This for-\nmulation covers both fully-connected and convolutional\nlayers. We add the BN transform immediately before the\nnonlinearity, by normalizing x = Wu+b. We could have\nalso normalized the layer inputs u, but since u is likely\nthe output of another nonlinearity, the shape of its distri-\nbution is likely to change during training, and constraining\nits \ufb01rst and second moments would not eliminate the co-\nvariate shift. In contrast, Wu + b is more likely to have\na symmetric, non-sparse distribution, that is \u201cmore Gaus-\nsian\u201d (Hyv\u00a8arinen & Oja, 2000); normalizing it is likely to\nproduce activations with a stable distribution.\nNote that, since we normalize Wu+b, the bias b can be\nignored since its effect will be canceled by the subsequent\nmean subtraction (the role of the bias is subsumed by \u03b2 in\nAlg. 1). Thus, z = g(Wu + b) is replaced with\nz = g(BN(Wu))\nwhere the BN transform is applied independently to each\ndimension of x = Wu, with a separate pair of learned\nparameters \u03b3(k), \u03b2(k) per dimension.\nFor convolutional layers, we additionally want the nor-\nmalization to obey the convolutional property \u2013 so that\ndifferent elements of the same feature map, at different\nlocations, are normalized in the same way. To achieve\nthis, we jointly normalize all the activations in a mini-\nbatch, over all locations. In Alg. 1, we let B be the set of\nall values in a feature map across both the elements of a\nmini-batch and spatial locations \u2013 so for a mini-batch of\nsize m and feature maps of size p \u00d7 q, we use the effec-\ntive mini-batch of size m\u2032 = |B| = m \u00b7 p q. We learn a\npair of parameters \u03b3(k) and \u03b2(k) per feature map, rather\nthan per activation. Alg. 2 is modi\ufb01ed similarly, so that\nduring inference the BN transform applies the same linear\ntransformation to each activation in a given feature map.\n3.3\nBatch Normalization enables higher\nlearning rates\nIn traditional deep networks, too-high learning rate may\nresult in the gradients that explode or vanish, as well as\ngetting stuck in poor local minima.\nBatch Normaliza-\ntion helps address these issues. By normalizing activa-\ntions throughout the network, it prevents small changes\nto the parameters from amplifying into larger and subop-\ntimal changes in activations in gradients; for instance, it\nprevents the training from getting stuck in the saturated\nregimes of nonlinearities.\nBatch Normalization also makes training more resilient\nto the parameter scale. Normally, large learning rates may\nincrease the scale of layer parameters, which then amplify\nthe gradient during backpropagationand lead to the model\nexplosion.\nHowever, with Batch Normalization, back-\npropagation through a layer is unaffected by the scale of\nits parameters. Indeed, for a scalar a,\nBN(Wu) = BN((aW)u)\nand we can show that\n\u2202BN((aW)u)\n\u2202u\n= \u2202BN(Wu)\n\u2202u\n\u2202BN((aW)u)\n\u2202(aW)\n= 1\na \u00b7 \u2202BN(Wu)\n\u2202W\nThe scale does not affect the layer Jacobian nor, con-\nsequently, the gradient propagation.\nMoreover, larger\nweights lead to smaller gradients, and Batch Normaliza-\ntion will stabilize the parameter growth.\nWe further conjecture that Batch Normalization may\nlead the layer Jacobians to have singular values close to 1,\nwhich is known to be bene\ufb01cial for training (Saxe et al.,\n2013). Consider two consecutive layers with normalized\ninputs, and the transformation between these normalized\nvectors: bz = F(bx). If we assume that bx and bz are Gaussian\nand uncorrelated, and that F(bx) \u2248Jbx is a linear transfor-\nmation for the given model parameters, then both bx and bz\nhave unit covariances, and I = Cov[bz] = JCov[bx]JT =\nJJT . Thus, JJT = I, and so all singular values of J\nare equal to 1, which preserves the gradient magnitudes\nduring backpropagation. In reality, the transformation is\nnot linear, and the normalized values are not guaranteed to\nbe Gaussian nor independent, but we nevertheless expect\nBatch Normalization to help make gradient propagation\nbetter behaved. The precise effect of Batch Normaliza-\ntion on gradient propagation remains an area of further\nstudy.\n3.4\nBatch Normalization regularizes the\nmodel\nWhen training with Batch Normalization, a training ex-\nample is seen in conjunction with other examples in the\nmini-batch, and the training network no longer produc-\ning deterministic values for a given training example. In\nour experiments, we found this effect to be advantageous\nto the generalization of the network. Whereas Dropout\n(Srivastava et al., 2014) is typically used to reduce over-\n\ufb01tting, in a batch-normalized network we found that it can\nbe either removed or reduced in strength.\n4\nExperiments\n4.1\nActivations over time\nTo verify the effects of internal covariate shift on train-\ning, and the ability of Batch Normalization to combat it,\nwe considered the problem of predicting the digit class on\nthe MNIST dataset (LeCun et al., 1998a). We used a very\nsimple network, with a 28x28 binary image as input, and\n5\n10K\n20K\n30K\n40K\n50K\n0.7\n0.8\n0.9\n1\n \n \nWithout BN\nWith BN\n\u22122\n0\n2\n\u22122\n0\n2\n(a)\n(b) Without BN\n(c) With BN\nFigure 1: (a) The test accuracy of the MNIST network\ntrained with and without Batch Normalization, vs. the\nnumber of training steps. Batch Normalization helps the\nnetwork train faster and achieve higher accuracy.\n(b,\nc) The evolution of input distributions to a typical sig-\nmoid, over the course of training, shown as {15, 50, 85}th\npercentiles. Batch Normalization makes the distribution\nmore stable and reduces the internal covariate shift.\n3 fully-connected hidden layers with 100 activations each.\nEach hidden layer computes y = g(Wu+b) with sigmoid\nnonlinearity, and the weights W initialized to small ran-\ndom Gaussian values. The last hidden layer is followed\nby a fully-connected layer with 10 activations (one per\nclass) and cross-entropy loss. We trained the network for\n50000 steps, with 60 examples per mini-batch. We added\nBatch Normalization to each hidden layer of the network,\nas in Sec. 3.1. We were interested in the comparison be-\ntween the baseline and batch-normalized networks, rather\nthan achieving the state of the art performance on MNIST\n(which the described architecture does not).\nFigure 1(a) shows the fraction of correct predictions\nby the two networks on held-out test data, as training\nprogresses.\nThe batch-normalized network enjoys the\nhigher test accuracy. To investigate why, we studied in-\nputs to the sigmoid, in the original network N and batch-\nnormalized network Ntr\nBN (Alg. 2) over the course of train-\ning. In Fig. 1(b,c) we show, for one typical activation from\nthe last hidden layer of each network, how its distribu-\ntion evolves. The distributions in the original network\nchange signi\ufb01cantly over time, both in their mean and\nthe variance, which complicates the training of the sub-\nsequent layers. In contrast, the distributions in the batch-\nnormalized network are much more stable as training pro-\ngresses, which aids the training.\n4.2\nImageNet classi\ufb01cation\nWe applied Batch Normalization to a new variant of the\nInception network (Szegedy et al., 2014), trained on the\nImageNet classi\ufb01cation task (Russakovsky et al., 2014).\nThe network has a large number of convolutional and\npooling layers, with a softmax layer to predict the image\nclass, out of 1000 possibilities. Convolutional layers use\nReLU as the nonlinearity. The main difference to the net-\nwork described in (Szegedy et al., 2014) is that the 5 \u00d7 5\nconvolutional layers are replaced by two consecutive lay-\ners of 3 \u00d7 3 convolutions with up to 128 \ufb01lters. The net-\nwork contains 13.6 \u00b7 106 parameters, and, other than the\ntop softmax layer, has no fully-connected layers. More\ndetails are given in the Appendix. We refer to this model\nas Inception in the rest of the text. The model was trained\nusing a version of Stochastic Gradient Descent with mo-\nmentum (Sutskever et al., 2013), using the mini-batch size\nof 32. The training was performed using a large-scale, dis-\ntributed architecture (similar to (Dean et al., 2012)). All\nnetworks are evaluated as training progresses by comput-\ning the validation accuracy @1, i.e.\nthe probability of\npredicting the correct label out of 1000 possibilities, on\na held-out set, using a single crop per image.\nIn our experiments, we evaluated several modi\ufb01cations\nof Inception with Batch Normalization. In all cases, Batch\nNormalization was applied to the input of each nonlinear-\nity, in a convolutional way, as described in section 3.2,\nwhile keeping the rest of the architecture constant.\n4.2.1\nAccelerating BN Networks\nSimply adding Batch Normalization to a network does not\ntake full advantage of our method. To do so, we further\nchanged the network and its training parameters, as fol-\nlows:\nIncrease learning rate. In a batch-normalized model,\nwe have been able to achieve a training speedup from\nhigher learning rates, with no ill side effects (Sec. 3.3).\nRemove Dropout. As described in Sec. 3.4, Batch Nor-\nmalization ful\ufb01lls some of the same goals as Dropout. Re-\nmoving Dropout from Modi\ufb01ed BN-Inception speeds up\ntraining, without increasing over\ufb01tting.\nReduce the L2 weight regularization. While in Incep-\ntion an L2 loss on the model parameters controls over\ufb01t-\nting, in Modi\ufb01ed BN-Inception the weight of this loss is\nreduced by a factor of 5. We \ufb01nd that this improves the\naccuracy on the held-out validation data.\nAccelerate the learning rate decay. In training Incep-\ntion, learning rate was decayed exponentially. Because\nour network trains faster than Inception, we lower the\nlearning rate 6 times faster.\nRemove Local Response Normalization While Incep-\ntion and other networks (Srivastava et al., 2014) bene\ufb01t\nfrom it, we found that with Batch Normalization it is not\nnecessary.\nShuf\ufb02e training examples more thoroughly. We enabled\nwithin-shard shuf\ufb02ing of the training data, which prevents\nthe same examples from always appearing in a mini-batch\ntogether. This led to about 1% improvements in the val-\nidation accuracy, which is consistent with the view of\nBatch Normalization as a regularizer (Sec. 3.4): the ran-\ndomization inherent in our method should be most bene-\n\ufb01cial when it affects an example differently each time it is\nseen.\nReduce the photometric distortions.\nBecause batch-\nnormalized networks train faster and observe each train-\ning example fewer times, we let the trainer focus on more\n\u201creal\u201d images by distorting them less.\n6\n5M\n10M\n15M\n20M\n25M\n30M\n0.4\n0.5\n0.6\n0.7\n0.8\nInception\nBN\u2212Baseline\nBN\u2212x5\nBN\u2212x30\nBN\u2212x5\u2212Sigmoid\nSteps to match Inception\nFigure 2: Single crop validation accuracy of Inception\nand its batch-normalized variants, vs.\nthe number of\ntraining steps.\nModel\nSteps to 72.2%\nMax accuracy\nInception\n31.0 \u00b7 106\n72.2%\nBN-Baseline\n13.3 \u00b7 106\n72.7%\nBN-x5\n2.1 \u00b7 106\n73.0%\nBN-x30\n2.7 \u00b7 106\n74.8%\nBN-x5-Sigmoid\n69.8%\nFigure 3: For Inception and the batch-normalized\nvariants, the number of training steps required to\nreach the maximum accuracy of Inception (72.2%),\nand the maximum accuracy achieved by the net-\nwork.\n4.2.2\nSingle-Network Classi\ufb01cation\nWe evaluated the following networks, all trained on the\nLSVRC2012 training data, and tested on the validation\ndata:\nInception: the network described at the beginning of\nSection 4.2, trained with the initial learning rate of 0.0015.\nBN-Baseline: Same as Inception with Batch Normal-\nization before each nonlinearity.\nBN-x5: Inception with Batch Normalization and the\nmodi\ufb01cations in Sec. 4.2.1. The initial learning rate was\nincreased by a factor of 5, to 0.0075. The same learning\nrate increase with original Inception caused the model pa-\nrameters to reach machine in\ufb01nity.\nBN-x30: Like BN-x5, but with the initial learning rate\n0.045 (30 times that of Inception).\nBN-x5-Sigmoid: Like BN-x5, but with sigmoid non-\nlinearity g(t) =\n1\n1+exp(\u2212x) instead of ReLU. We also at-\ntempted to train the original Inception with sigmoid, but\nthe model remained at the accuracy equivalent to chance.\nIn Figure 2, we show the validation accuracy of the\nnetworks, as a function of the number of training steps.\nInception reached the accuracy of 72.2% after 31 \u00b7 106\ntraining steps. The Figure 3 shows, for each network,\nthe number of training steps required to reach the same\n72.2% accuracy, as well as the maximum validation accu-\nracy reached by the network and the number of steps to\nreach it.\nBy only using Batch Normalization (BN-Baseline), we\nmatch the accuracy of Inception in less than half the num-\nber of training steps. By applying the modi\ufb01cations in\nSec. 4.2.1, we signi\ufb01cantly increase the training speed of\nthe network. BN-x5 needs 14 times fewer steps than In-\nception to reach the 72.2% accuracy. Interestingly, in-\ncreasing the learning rate further (BN-x30) causes the\nmodel to train somewhat slower initially, but allows it to\nreach a higher \ufb01nal accuracy. It reaches 74.8% after 6\u00b7106\nsteps, i.e. 5 times fewer steps than required by Inception\nto reach 72.2%.\nWe also veri\ufb01ed that the reduction in internal covari-\nate shift allows deep networks with Batch Normalization\nto be trained when sigmoid is used as the nonlinearity,\ndespite the well-known dif\ufb01culty of training such net-\nworks. Indeed, BN-x5-Sigmoid achieves the accuracy of\n69.8%. Without Batch Normalization, Inception with sig-\nmoid never achieves better than 1/1000 accuracy.\n4.2.3\nEnsemble Classi\ufb01cation\nThe current reported best results on the ImageNet Large\nScale Visual Recognition Competition are reached by the\nDeep Image ensemble of traditional models (Wu et al.,\n2015) and the ensemble model of (He et al., 2015). The\nlatter reports the top-5 error of 4.94%, as evaluated by the\nILSVRC server. Here we report a top-5 validation error of\n4.9%, and test error of 4.82% (according to the ILSVRC\nserver). This improves upon the previous best result, and\nexceeds the estimated accuracy of human raters according\nto (Russakovsky et al., 2014).\nFor our ensemble, we used 6 networks. Each was based\non BN-x30, modi\ufb01ed via some of the following: increased\ninitial weights in the convolutional layers; using Dropout\n(with the Dropout probability of 5% or 10%, vs. 40%\nfor the original Inception); and using non-convolutional,\nper-activation Batch Normalization with last hidden lay-\ners of the model. Each network achieved its maximum\naccuracy after about 6 \u00b7 106 training steps. The ensemble\nprediction was based on the arithmetic average of class\nprobabilities predicted by the constituent networks. The\ndetails of ensemble and multicrop inference are similar to\n(Szegedy et al., 2014).\nWe demonstrate in Fig. 4 that batch normalization al-\nlows us to set new state-of-the-art by a healthy margin on\nthe ImageNet classi\ufb01cation challenge benchmarks.\n5\nConclusion\nWe have presented a novel mechanism for dramatically\naccelerating the training of deep networks. It is based on\nthe premise that covariate shift, which is known to com-\nplicate the training of machine learning systems, also ap-\n7\nModel\nResolution\nCrops\nModels\nTop-1 error\nTop-5 error\nGoogLeNet ensemble\n224\n144\n7\n-\n6.67%\nDeep Image low-res\n256\n-\n1\n-\n7.96%\nDeep Image high-res\n512\n-\n1\n24.88\n7.42%\nDeep Image ensemble\nvariable\n-\n-\n-\n5.98%\nBN-Inception single crop\n224\n1\n1\n25.2%\n7.82%\nBN-Inception multicrop\n224\n144\n1\n21.99%\n5.82%\nBN-Inception ensemble\n224\n144\n6\n20.1%\n4.9%*\nFigure 4: Batch-Normalized Inception comparison with previous state of the art on the provided validation set com-\nprising 50000 images. *BN-Inception ensemble has reached 4.82% top-5 error on the 100000 images of the test set of\nthe ImageNet as reported by the test server.\nplies to sub-networks and layers, and removing it from\ninternal activations of the network may aid in training.\nOur proposed method draws its power from normalizing\nactivations, and from incorporating this normalization in\nthe network architecture itself. This ensures that the nor-\nmalization is appropriately handled by any optimization\nmethod that is being used to train the network. To en-\nable stochastic optimization methods commonly used in\ndeep network training, we perform the normalization for\neach mini-batch, and backpropagate the gradients through\nthe normalization parameters. Batch Normalization adds\nonly two extra parameters per activation, and in doing so\npreserves the representation ability of the network. We\npresented an algorithm for constructing, training, and per-\nforming inference with batch-normalized networks. The\nresulting networks can be trained with saturating nonlin-\nearities, are more tolerant to increased training rates, and\noften do not require Dropout for regularization.\nMerely adding Batch Normalization to a state-of-the-\nart image classi\ufb01cation model yields a substantial speedup\nin training. By further increasing the learning rates, re-\nmoving Dropout, and applying other modi\ufb01cations af-\nforded by Batch Normalization, we reach the previous\nstate of the art with only a small fraction of training steps\n\u2013 and then beat the state of the art in single-network image\nclassi\ufb01cation. Furthermore, by combining multiple mod-\nels trained with Batch Normalization, we perform better\nthan the best known system on ImageNet, by a signi\ufb01cant\nmargin.\nInterestingly, our method bears similarity to the stan-\ndardization layer of (G\u00a8ulc\u00b8ehre & Bengio, 2013), though\nthe two methods stem from very different goals, and per-\nform different tasks. The goal of Batch Normalization\nis to achieve a stable distribution of activation values\nthroughout training, and in our experiments we apply it\nbefore the nonlinearity since that is where matching the\n\ufb01rst and second moments is more likely to result in a\nstable distribution. On the contrary, (G\u00a8ulc\u00b8ehre & Bengio,\n2013) apply the standardization layer to the output of the\nnonlinearity, which results in sparser activations. In our\nlarge-scale image classi\ufb01cation experiments, we have not\nobserved the nonlinearity inputs to be sparse, neither with\nnor without Batch Normalization. Other notable differ-\nentiating characteristics of Batch Normalization include\nthe learned scale and shift that allow the BN transform\nto represent identity (the standardization layer did not re-\nquire this since it was followed by the learned linear trans-\nform that, conceptually, absorbs the necessary scale and\nshift), handling of convolutional layers, deterministic in-\nference that does not depend on the mini-batch, and batch-\nnormalizing each convolutional layer in the network.\nIn this work, we have not explored the full range of\npossibilities that Batch Normalization potentially enables.\nOur future work includes applications of our method to\nRecurrent Neural Networks (Pascanu et al., 2013), where\nthe internal covariate shift and the vanishing or exploding\ngradients may be especially severe, and which would al-\nlow us to more thoroughly test the hypothesis that normal-\nization improves gradient propagation (Sec. 3.3). We plan\nto investigate whether Batch Normalization can help with\ndomain adaptation, in its traditional sense \u2013 i.e. whether\nthe normalization performed by the network would al-\nlow it to more easily generalize to new data distribu-\ntions, perhaps with just a recomputation of the population\nmeans and variances (Alg. 2). Finally, we believe that fur-\nther theoretical analysis of the algorithm would allow still\nmore improvements and applications.\nReferences\nBengio, Yoshua and Glorot, Xavier. Understanding the\ndif\ufb01culty of training deep feedforward neural networks.\nIn Proceedings of AISTATS 2010, volume 9, pp. 249\u2013\n256, May 2010.\nDean, Jeffrey, Corrado, Greg S., Monga, Rajat, Chen, Kai,\nDevin, Matthieu, Le, Quoc V., Mao, Mark Z., Ranzato,\nMarc\u2019Aurelio, Senior, Andrew, Tucker, Paul, Yang, Ke,\nand Ng, Andrew Y. Large scale distributed deep net-\nworks. In NIPS, 2012.\nDesjardins, Guillaume and Kavukcuoglu, Koray. Natural\nneural networks. (unpublished).\nDuchi, John, Hazan, Elad, and Singer, Yoram. Adaptive\nsubgradient methods for online learning and stochastic\n8\noptimization. J. Mach. Learn. Res., 12:2121\u20132159,July\n2011. ISSN 1532-4435.\nG\u00a8ulc\u00b8ehre, C\u00b8 aglar and Bengio, Yoshua. Knowledge mat-\nters: Importance of prior information for optimization.\nCoRR, abs/1301.4083, 2013.\nHe, K., Zhang, X., Ren, S., and Sun, J. Delving Deep\ninto Recti\ufb01ers: Surpassing Human-Level Performance\non ImageNet Classi\ufb01cation. ArXiv e-prints, February\n2015.\nHyv\u00a8arinen, A. and Oja, E. Independent component anal-\nysis: Algorithms and applications. Neural Netw., 13\n(4-5):411\u2013430, May 2000.\nJiang, Jing. A literature survey on domain adaptation of\nstatistical classi\ufb01ers, 2008.\nLeCun, Y., Bottou, L., Bengio, Y., and Haffner, P.\nGradient-based learning applied to document recog-\nnition. Proceedings of the IEEE, 86(11):2278\u20132324,\nNovember 1998a.\nLeCun, Y., Bottou, L., Orr, G., and Muller, K. Ef\ufb01cient\nbackprop. In Orr, G. and K., Muller (eds.), Neural Net-\nworks: Tricks of the trade. Springer, 1998b.\nLyu, S and Simoncelli, E P. Nonlinear image representa-\ntion using divisive normalization. In Proc. Computer\nVision and Pattern Recognition, pp. 1\u20138. IEEE Com-\nputer Society, Jun 23-28 2008. doi: 10.1109/CVPR.\n2008.4587821.\nNair, Vinod and Hinton, Geoffrey E. Recti\ufb01ed linear units\nimprove restricted boltzmann machines. In ICML, pp.\n807\u2013814. Omnipress, 2010.\nPascanu, Razvan, Mikolov, Tomas, and Bengio, Yoshua.\nOn the dif\ufb01culty of training recurrent neural networks.\nIn Proceedings of the 30th International Conference on\nMachine Learning, ICML 2013, Atlanta, GA, USA, 16-\n21 June 2013, pp. 1310\u20131318, 2013.\nPovey, Daniel, Zhang, Xiaohui, and Khudanpur, San-\njeev.\nParallel training of deep neural networks with\nnatural gradient and parameter averaging.\nCoRR,\nabs/1410.7455, 2014.\nRaiko, Tapani, Valpola, Harri, and LeCun, Yann. Deep\nlearning made easier by linear transformations in per-\nceptrons. In International Conference on Arti\ufb01cial In-\ntelligence and Statistics (AISTATS), pp. 924\u2013932, 2012.\nRussakovsky, Olga, Deng, Jia, Su, Hao, Krause, Jonathan,\nSatheesh, Sanjeev, Ma, Sean, Huang, Zhiheng, Karpa-\nthy, Andrej, Khosla, Aditya, Bernstein, Michael, Berg,\nAlexander C., and Fei-Fei, Li. ImageNet Large Scale\nVisual Recognition Challenge, 2014.\nSaxe, Andrew M., McClelland, James L., and Ganguli,\nSurya.\nExact solutions to the nonlinear dynamics\nof learning in deep linear neural networks.\nCoRR,\nabs/1312.6120, 2013.\nShimodaira, Hidetoshi.\nImproving predictive inference\nunder covariate shift by weighting the log-likelihood\nfunction. Journal of Statistical Planning and Inference,\n90(2):227\u2013244, October 2000.\nSrivastava, Nitish, Hinton, Geoffrey, Krizhevsky, Alex,\nSutskever, Ilya, and Salakhutdinov, Ruslan. Dropout:\nA simple way to prevent neural networks from over\ufb01t-\nting. J. Mach. Learn. Res., 15(1):1929\u20131958, January\n2014.\nSutskever, Ilya, Martens, James, Dahl, George E., and\nHinton, Geoffrey E.\nOn the importance of initial-\nization and momentum in deep learning.\nIn ICML\n(3), volume 28 of JMLR Proceedings, pp. 1139\u20131147.\nJMLR.org, 2013.\nSzegedy, Christian, Liu, Wei, Jia, Yangqing, Sermanet,\nPierre, Reed, Scott, Anguelov, Dragomir, Erhan, Du-\nmitru, Vanhoucke, Vincent, and Rabinovich, An-\ndrew.\nGoing deeper with convolutions.\nCoRR,\nabs/1409.4842, 2014.\nWiesler, Simon and Ney, Hermann. A convergence anal-\nysis of log-linear training. In Shawe-Taylor, J., Zemel,\nR.S., Bartlett, P., Pereira, F.C.N., and Weinberger, K.Q.\n(eds.), Advances in Neural Information Processing Sys-\ntems 24, pp. 657\u2013665, Granada, Spain, December 2011.\nWiesler, Simon, Richard, Alexander, Schl\u00a8uter, Ralf, and\nNey, Hermann. Mean-normalized stochastic gradient\nfor large-scale deep learning.\nIn IEEE International\nConference on Acoustics, Speech, and Signal Process-\ning, pp. 180\u2013184, Florence, Italy, May 2014.\nWu, Ren, Yan, Shengen, Shan, Yi, Dang, Qingqing, and\nSun, Gang. Deep image: Scaling up image recognition,\n2015.\nAppendix\nVariant of the Inception Model Used\nFigure 5 documents the changes that were performed\ncompared to\nthe\narchitecture with\nrespect\nto\nthe\nGoogleNet archictecture. For the interpretation of this\ntable, please consult (Szegedy et al., 2014). The notable\narchitecture changes compared to the GoogLeNet model\ninclude:\n\u2022 The 5\u00d75 convolutional layers are replaced by two\nconsecutive 3\u00d73 convolutional layers.\nThis in-\ncreases the maximum depth of the network by 9\n9\nweight layers. Also it increases the number of pa-\nrameters by 25% and the computational cost is in-\ncreased by about 30%.\n\u2022 The number 28\u00d728 inception modules is increased\nfrom 2 to 3.\n\u2022 Inside the modules, sometimes average, sometimes\nmaximum-pooling is employed. This is indicated in\nthe entries corresponding to the pooling layers of the\ntable.\n\u2022 There are no across the board pooling layers be-\ntween any two Inception modules, but stride-2 con-\nvolution/pooling layers are employed before the \ufb01l-\nter concatenation in the modules 3c, 4e.\nOur model employed separable convolution with depth\nmultiplier 8 on the \ufb01rst convolutional layer. This reduces\nthe computational cost while increasing the memory con-\nsumption at training time.\n10\ntype\npatch size/\nstride\noutput\nsize\ndepth\n#1\u00d71\n#3\u00d73\nreduce\n#3\u00d73\ndouble #3\u00d73\nreduce\ndouble\n#3\u00d73\nPool +proj\nconvolution*\n7\u00d77/2\n112\u00d7112\u00d764\n1\nmax pool\n3\u00d73/2\n56\u00d756\u00d764\n0\nconvolution\n3\u00d73/1\n56\u00d756\u00d7192\n1\n64\n192\nmax pool\n3\u00d73/2\n28\u00d728\u00d7192\n0\ninception (3a)\n28\u00d728\u00d7256\n3\n64\n64\n64\n64\n96\navg + 32\ninception (3b)\n28\u00d728\u00d7320\n3\n64\n64\n96\n64\n96\navg + 64\ninception (3c)\nstride 2\n28\u00d728\u00d7576\n3\n0\n128\n160\n64\n96\nmax + pass through\ninception (4a)\n14\u00d714\u00d7576\n3\n224\n64\n96\n96\n128\navg + 128\ninception (4b)\n14\u00d714\u00d7576\n3\n192\n96\n128\n96\n128\navg + 128\ninception (4c)\n14\u00d714\u00d7576\n3\n160\n128\n160\n128\n160\navg + 128\ninception (4d)\n14\u00d714\u00d7576\n3\n96\n128\n192\n160\n192\navg + 128\ninception (4e)\nstride 2\n14\u00d714\u00d71024\n3\n0\n128\n192\n192\n256\nmax + pass through\ninception (5a)\n7\u00d77\u00d71024\n3\n352\n192\n320\n160\n224\navg + 128\ninception (5b)\n7\u00d77\u00d71024\n3\n352\n192\n320\n192\n224\nmax + 128\navg pool\n7\u00d77/1\n1\u00d71\u00d71024\n0\nFigure 5: Inception architecture\n11\n",
        "sentence": "",
        "context": "ceptrons. In International Conference on Arti\ufb01cial In-\ntelligence and Statistics (AISTATS), pp. 924\u2013932, 2012.\nRussakovsky, Olga, Deng, Jia, Su, Hao, Krause, Jonathan,\nSatheesh, Sanjeev, Ma, Sean, Huang, Zhiheng, Karpa-\nIn IEEE International\nConference on Acoustics, Speech, and Signal Process-\ning, pp. 180\u2013184, Florence, Italy, May 2014.\nWu, Ren, Yan, Shengen, Shan, Yi, Dang, Qingqing, and\nSun, Gang. Deep image: Scaling up image recognition,\n2015.\nAppendix\njeev.\nParallel training of deep neural networks with\nnatural gradient and parameter averaging.\nCoRR,\nabs/1410.7455, 2014.\nRaiko, Tapani, Valpola, Harri, and LeCun, Yann. Deep\nlearning made easier by linear transformations in per-"
    },
    {
        "title": "Learning image representations tied to ego-motion",
        "author": [
            "D. Jayaraman",
            "K. Grauman"
        ],
        "venue": "In ICCV,",
        "citeRegEx": "Jayaraman and Grauman,? \\Q2015\\E",
        "shortCiteRegEx": "Jayaraman and Grauman",
        "year": 2015,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "A convex relaxation for weakly supervised classifiers",
        "author": [
            "A. Joulin",
            "F. Bach"
        ],
        "venue": "In ICML,",
        "citeRegEx": "Joulin and Bach,? \\Q2012\\E",
        "shortCiteRegEx": "Joulin and Bach",
        "year": 2012,
        "abstract": "This paper introduces a general multi-class approach to weakly supervised\nclassification. Inferring the labels and learning the parameters of the model\nis usually done jointly through a block-coordinate descent algorithm such as\nexpectation-maximization (EM), which may lead to local minima. To avoid this\nproblem, we propose a cost function based on a convex relaxation of the\nsoft-max loss. We then propose an algorithm specifically designed to\nefficiently solve the corresponding semidefinite program (SDP). Empirically,\nour method compares favorably to standard ones on different datasets for\nmultiple instance learning and semi-supervised learning as well as on\nclustering tasks.",
        "full_text": "A convex relaxation for weakly supervised classi\ufb01ers\nArmand Joulin\narmand.joulin@ens.fr\nWILLOW-SIERRA project-teams, INRIA - Ecole Normale Sup\u00b4erieure\nFrancis Bach\nfrancis.bach@inria.fr\nSIERRA project-team, INRIA - Ecole Normale Sup\u00b4erieure\nAbstract\nThis paper introduces a general multi-class\napproach to weakly supervised classi\ufb01ca-\ntion.\nInferring the labels and learning the\nparameters of the model is usually done\njointly through a block-coordinate descent\nalgorithm such as expectation-maximization\n(EM), which may lead to local minima. To\navoid this problem, we propose a cost func-\ntion based on a convex relaxation of the\nsoft-max loss.\nWe then propose an al-\ngorithm speci\ufb01cally designed to e\ufb03ciently\nsolve the corresponding semide\ufb01nite pro-\ngram (SDP). Empirically, our method com-\npares favorably to standard ones on di\ufb00erent\ndatasets for multiple instance learning and\nsemi-supervised learning, as well as on clus-\ntering tasks.\n1. Introduction\nDiscriminative supervised classi\ufb01ers have proved to be\nvery accurate data-driven tools for learning the rela-\ntionship between input variables and certain labels.\nUsually, for these methods to work, the labeling of the\ntraining data needs to be complete and precise. How-\never, in many practical situations, this requirement is\nimpossible to meet because of the challenges posed by\nthe acquisition of detailed data annotations. This typ-\nically leads to partial or ambiguous labelings.\nDi\ufb00erent weakly supervised methods have been pro-\nposed to tackle this issue.\nIn the semi-supervised\nframework (Chapelle et al., 2006), only a small num-\nber of points are labeled, and the goal is to use the un-\nlabeled points to improve the performance of the clas-\nsi\ufb01er. In the multiple-instance learning (MIL) frame-\nwork introduced by Dietterich & Lathrop (1997), bags\nof instances are labeled together instead of individu-\nally, and some instances belonging to the same bag\nAppearing in Proceedings of the 29 th International Confer-\nence on Machine Learning, Edinburgh, Scotland, UK, 2012.\nCopyright 2012 by the author(s)/owner(s).\nmay have di\ufb00erent true labels.\nFinally, in the am-\nbiguous labeling setting (Jin & Ghahramani, 2003;\nHullermeier & Beringer, 2006), each point is associ-\nated with multiple potential labels.\nMore generally, in all these frameworks, the points are\nassociated with observable partial labels and the im-\nplicit or explicit goal is to jointly estimate their true\nlatent labels and learn a classi\ufb01er based on these la-\nbels. This usually leads to a non-convex cost function\nwhich is often optimized with a greedy method or a\ncoordinate descent algorithm such as the expectation-\nmaximization (EM) procedure. These methods usu-\nally converge to a local minimum, and their initializa-\ntion remains an open practical problem.\nIn this paper, we propose a simple and general frame-\nwork which can be used for any of the aforementioned\nproblems. We explicitly learn the true latent label and\nthe classi\ufb01er parameters. We also propose a convex re-\nlaxation of our cost function and an e\ufb03cient algorithm\nto minimize it. More precisely, we use a discrimative\nclassi\ufb01er with a soft-max loss, and our convex relax-\nation extends the work of Guo & Schuurmans (2008).\nOur main contributions are:\n\u2022 a full convex relaxation of the soft-max loss func-\ntion with intercept, which can be applied to a\nlarge set of multiclass problems with any level of\nsupervision,\n\u2022 a novel convex cost function for weakly supervised\nand unsupervised problems and,\n\u2022 a dedicated and e\ufb03cient optimization procedure.\nWe develop our framework for the general weakly su-\npervised case. We propose results on both toy exam-\nples as proof of concept of our claims, and on standard\nMIL and semi-supervised learning (SSL) datasets.\n1.1. Related work\nMultiple instance learning (MIL) has received\nmuch attention because of its wide range of applica-\ntions. First used for drug activity prediction, it has\nalso been used in the vision community for di\ufb00erent\nproblems such as scene classi\ufb01cation (Maron & Ratan,\nA convex relaxation for weakly supervised classi\ufb01ers\n1998), object detection (Viola et al., 2006), object\ntracking in video (Babenko et al., 2009), and image\ndatabase retrieval (Yang, 2000). Many MIL methods\nhave been developed in the past decade. For example,\nsome are based on boosting (Auer & Ortner, 2004),\nothers on nearest neighbors (Wang & Zucker, 2000),\non neural networks (Zhang & Zhou, 2006), on decision\ntrees (Blockeel et al., 2005), or the construction of an\nappropriate kernel (Wang et al., 2008; G\u00a8artner et al.,\n2002; Kwok & Cheung, 2007). Much of the work in\nthe MIL community has focused on the use of dis-\ncriminative classi\ufb01ers, the most popular one being the\nsupport vector machine (SVM) (Andrews et al., 2003;\nChen & Wang, 2004; Gehler & Chapelle, 2007).\nIn\nthis paper, we concentrate on the logisitic loss which\nmakes little di\ufb00erence with the hinge loss with the ad-\nditional advantage of being twice di\ufb00erentiable. Note\nthat this loss has already been used in the context\nof MIL (Xu & Frank, 2004; Ray & Craven, 2005), but\nwith di\ufb00erent optimization schemes.\nMany semi-supervised learning (SSL) methods\nhave also been proposed in the past decade (see,\ne.g., Chapelle et al., 2006; Zhu, 2006). For example,\nsome are based on maximizing the margin with an\nSVM framework (Joachims, 1999; Bennett & Demiriz,\n1998; Xu & Schuurmans, 2005), and others use the\nunlabeled data for regularization (Belkin et al., 2004)\nor co-training of weak classi\ufb01ers (Blum & Mitchell,\n1998).\nDiscriminative clustering provides a principled\nway to reuse existing supervised learning machin-\nery while explicitly estimating the latent labels. For\nexample, following the SVM approach of Xu et al.\n(2005), algorithms using linear discriminant anal-\nysis (De la Torre & Takeo, 2006) or ridge regres-\nsion (Bach & Harchaoui, 2007) have been proposed.\nThese methods often fail in the multiclass case,\nwhereas we show that the soft-max loss with intercept\nworks well in this setting. A common issue for discrim-\ninative clustering is that a perfect separation is reached\nby assigning the same label to all of the points. In most\nof the previously cited methods, this issue is adressed\nby adding linear constraints on the size of each clus-\nter. In this paper we use instead a natural cluster-size\nbalancing term corresponding to an entropy penaliza-\ntion (Chapelle et al., 2006; Joulin et al., 2010).\nThe link between SSL and MIL has been widely\nstudied in the community.\nFor example, in the\ncontext of image segmentation with text annota-\ntion, Barnard et al. (2003) propose a general weakly\nsupervised model based on a multi-modal extension to\na mixture of latent Dirichlet allocation.\nAn impor-\ntant issue with this family of generative models is that\nlearning the parameters is often untractable. Another\nexample is Zhou & Xu (2007) who use the relation be-\ntween MIL and SSL to develop a method for MIL.\nThe idea of using a convex cost function in the weakly\nsupervision context has been already studied in dif-\nferent contexts such as, for example, ambiguous la-\nbeling (Cour et al., 2009) or discriminative cluster-\ning (Xu et al., 2005; Bach & Harchaoui, 2007). In this\npaper, we are interested in the convex relaxation of\na general multiclass loss function, i.e., the soft-max\nloss. Guo & Schuurmans (2008) propose a related re-\nlaxation but do not consider the intercept in the linear\nclassi\ufb01er. We extend their work to the case of linear\nclassi\ufb01ers with an intercept and show in the exper-\niment section, why this di\ufb00erence is crucial when it\ncomes to classi\ufb01cation. Note that by using kernels, we\ncan use non-linear classi\ufb01ers as well. Also, our ded-\nicated optimization scheme is more scalable than the\none developed in Guo & Schuurmans (2008) and could\nbe applied to their problem as well.\n2. Proposed model\n2.1. Notations\nWe suppose that we observe I bags of instances. For i\nin {1, . . . , I}, Ni is the set of instances in the i-th bag,\nand Ni = |Ni| is its cardinality. We denote by N =\nP\ni Ni the total number of instances. In each bag i, an\ninstance n in Ni is associated with a feature xn \u2208X\nand a label yn in L, in certain feature and label space.\nIn this paper, we suppose that this label is common\nto all the instances of a same bag and explain only\npartially the instances contained in the bag. We are\nthus interested in \ufb01nding a latent label zn \u2208P which\nwould give a better understanding of the data.\nWe\ndenote by P and L the cardinalities of P and L. We\nalso assume that the latent label zn of an instance n\ncan only take its values in a subset Pyn of P which\ndepends on the label yn of the bag. The variables yn\nand zn are associated with their canonical vectorial\nrepresentation, i.e., znp = 1 if the instance n has a\nlatent label of p and 0 otherwise.\nWe denote by z\nthe N \u00d7 P matrix with rows zn.\nInstance reweighting.\nIn many problems, a set of\ninstances can be bigger than the other, this is the case\nfor example in a one-vs-all classifer where the number\nof positive instances is often very small compared to\nthe number of negative examples. A side-contribution\nof this work is to consider explicitly a reweighting of\nthe data to avoid undesired side e\ufb00ects: Each point\nis associated with a weight \u03c0n \u22650 which denotes its\nimportance compared to others. Some examples are\nthe uniform case, i.e., \u03c0n =\n1\nN or when bags have to\nbe reweighted, i.e., \u03c0n =\n1\nINi for n in the bag i. We\ndenote by \u03c0 the vector with entries equal to \u03c0n. Note\nthat \u03c0 \u22650 and P\nn \u03c0n = 1.\nThis setting is very general, so let us now show how it\napplies to several concrete settings.\nA convex relaxation for weakly supervised classi\ufb01ers\nSemi-supervised learning.\nGiven a set of true la-\nbels P and Nl points with known label, there are Nl+1\nbags, i.e., one for each labeled point and one for all the\nunlabeled instances. The set L is equal to P plus a la-\nbel for the unlabeled bag (i.e., L = P + 1). The true\nlabel of an instance in a positive bag is \ufb01xed whereas\nin the unlabeled bag it can take any value in P.\nUnsupervised learning.\nThis is an extreme case\nof the semi-supervised framework with only the unla-\nbeled bag.\nMultiple instance learning. There are two possible\nlabels for a bag (L = 2), i.e., positive (yn = 1) or\nnegative (yn = 0). The true label zn of an instance n in\na negative bag is necessarily negative (zn = 0) and in a\npositive bag it can be either positive or negative (P1 =\n{0, 1}).\nAmbiguous labelling. Each bag is associated with\na set of possible true labels Pl. The set of partial labels\nis thus the combination of all possible subsets of P, i.e.,\neach label l \u2208L represents a subset of P (L = 2P ).\n2.2. Problem formulation\nThe goal of a discriminative weakly supervised classi-\n\ufb01er is to \ufb01nd the latent labels z that minimize the value\nof a regularized discriminative loss function.\nMore\nprecisely, given some latent label z and some feature\nmap \u03c6 : X 7\u2192IRd (note that \u03c6 could be explicitly\nde\ufb01ned or implicitly given through a positive-de\ufb01nite\nkernel), we train a multi-class discriminative classi\ufb01er\nto \ufb01nd the parameters w \u2208IRP \u00d7d and b \u2208IRP that\nminimize:\nL(z, w, b) =\nN\nX\nn=1\n\u03c0n\u2113(zn, wT \u03c6(xn) + b),\nwhere \u2113: IRP \u00d7 IRP 7\u2192IR is a loss function.\nIn\nthis paper, we are interested in the multi-class set-\nting where a natural choice for \u2113is the soft-max\nloss function (Hastie et al., 2001).\nNote that for a\ngiven instance n, the set of possible true labels de-\npends on the the label y of its bag, our loss func-\ntion \u2113(zn, wT \u03c6(xn) + b) then takes the following form:\n\u2212\nX\nl\u2208L\nynl\nX\np\u2208Pl\nznp log\n\u0012\nexp(wT\np \u03c6(xn) + bp)\nP\nk\u2208Pl exp(wT\nk \u03c6(xn) + bk)\n\u0013\n,\nwhere wT\np is the p\u2013th row of wT and bp the p\u2013th entry\nof b.\nCluster-size balancing term.\nIn many unsuper-\nvised or weakly supervised problems, a common issue\nis that assigning the same label to all the instances\nleads to perfect separation. In the MIL community,\nthis is equivalent to considering all the bags as neg-\native and a common solution is to add a non-convex\nconstraint which enforces at least one point per pos-\nitive bag to be positive.\nAnother solution used in\nthe discriminative clustering community is to add con-\nstraints on the number of elements per class and per\nbag (Xu et al., 2005; Bach & Harchaoui, 2007). De-\nspite good results, this solution introduces extra pa-\nrameters and may be hard to extend to other frame-\nworks such as MIL, where a positive bag may not\nhave any negative instances. Another common tech-\nnique is to encourage the proportion of points per\nclass and per bag to be close to uniform.\nAn ap-\npropriate penalty term for achieving this is the en-\ntropy (i.e., h(v) = \u2212P\nk vk log(vk)) of the proportions\nof points per bag and per latent label, leading to:\nH(z) =\nX\ni\u2208I\nh\n\u0012 X\nn\u2208Ni\n\u03c0nzn\n\u0013\n.\nPenalizing by this entropy turns out to be equivalent\nto maximizing the log-likelihood of a graphical model\nwhere the features xn explain the labels yn through\nthe latent labels zn (Joulin et al., 2010). An important\nconsequence is that the natural weight of this penalty\nin the cost function is 1, so we do not add any extra\nparameters.\nTo avoid over-\ufb01tting, we penalize the norm of w, lead-\ning to the following cost function:\nf(z, w, b) = L(z, w, b) \u2212H(z) + \u03bb\n2P \u2225w\u22252\nF ,\nwhere \u03bb > 0 is the regularization parameter and the\nproblem thus takes the following form:\nmin\n\u2200n\u2264N, zn\u2208SPyn\nmin\nw\u2208IRd\u00d7P , b\u2208IRP f(z, w, b),\n(1)\nwhere SP = {t \u2208IRP | t \u22650, tT 1P = 1} is the sim-\nplex in IRP . To avoid cumbersome double subscripts,\nwe suppose that any instance n in a bag with a la-\nbel yn (which is common to the entire bag), has a\nlatent label zn in P instead of Pyn.\nIn the next section we show how to obtain a convex\nrelaxation of this problem.\n3. Convex relaxation\nAn interesting feature of the soft-max cost function\nis its link to the entropy through the Fenchel conju-\ngate (Boyd & Vandenberghe, 2003), i.e., given a P-\ndimensional vector t, the log-partition can be written\nas log\n\u0010 PP\np=1 exp(tp)\n\u0011\n= maxv\u2208SP\nPP\np=1 vptp + h(v).\nSubstituting in the loss function, the weakly super-\nvised problem de\ufb01ned in Eq. (1) can be reformulated\nas:\nmin\nz\u2208SN\nP\nmax\nq\u2208SN\nP\nX\ni\u2208I\nX\nn\u2208Ni\n\u03c0nh(qn) \u2212H(z) + g(z, q),\n(2)\nA convex relaxation for weakly supervised classi\ufb01ers\nstep\nInner loop update\nInner loop duality gap\nOuter loop proximal\nOuter loop duality gap\ncomplexity\nO(N 2)\nO(N 2)\nO(N 3)\nO(N)\nFigure 1. Complexity of the di\ufb00erent steps in our algorithm.\nwhere q is an N \u00d7 P matrix with n-th row qT\nn ,\nand g(z, q) is equal to:\nmin\nw\u2208IRP \u00d7d\nb\u2208IRP\nX\ni\u2208I\nX\nn\u2208Ni\n\u03c0n(qn \u2212zn)T (wT \u03c6(xn) + b) + \u03bb\n2P \u2225w\u22252\nF .\nMinimizing this function w.r.t. the intercept b leads\nto an intercept constraint on the dual variables, i.e,\n(q \u2212z)T \u03c0 = 0. The minimization w.r.t. w leads to a\nclosed-form expression for g:\ng(z, q)\n=\n\u2212P\n2\u03bbtr\n\u0000(q \u2212z)(q \u2212z)T K\n\u0001\n,\nwhere\nK is the positive de\ufb01nite kernel matrix asso-\nciated with the reweighted mapping \u03c6, i.e., with en-\ntries equal to Knm = \u03c0n\u03c6(xn)T \u03c6(xm)\u03c0m. The cost\nfunction is not convex in general in z since it is the\nmaximum over a set indexed by q of concave functions\nin z. A common way of dealing with this issue is to\nrelax the problem into a semide\ufb01nite program (SDP)\nin zzT . Unfortunately, our cost function does not di-\nrectly depend on zzT , but a reparametrization in terms\nof q inspired by Guo & Schuurmans (2008) allows us\nto get around this technical di\ufb03culty.\nReparametrization in q.\nWe reparametrize the\nproblem by introducing an N \u00d7 N matrix \u2126such\nthat q = \u2126z (Guo & Schuurmans, 2008).\nThe in-\ntercept constraint and the normalization constraint\non q (i.e., q1K = 1N) become constraints over \u2126, i.e.,\nrespectively \u2126T \u03c0 = \u03c0 and \u21261N = 1N.\nTranslating\nthe addition of an intercept to a linear classi\ufb01er into a\nsimple constraint on the columns of \u2126provides a sig-\nni\ufb01cant improvement over Guo & Schuurmans (2008),\nas shown in Section 5.1. This reparametrization has\nthe side-e\ufb00ect of introducing a non-convex term in the\ncost function since the entropies over qn in Eq. (2) is\nreplaced by an entropy over the n\u2013th row of \u2126z which\nis not jointly concave/convex in \u2126and z.\nTight upper-bound on the entropy. We show in\nthe supplementary material that the entropy in q can\nbe bounded by a di\ufb00erence of entropy in \u2126and z, up\nto an additive constant C0:\nX\ni\u2208I\nX\nn\u2208Ni\n\u03c0nh(qn) \u2264\u2212\nX\nn\n\u03c0nh(\u2126n) + H(z) + C0.\n(3)\nThis upper-bound is tight in the sense that given a dis-\ncrete value of z (i.e., before the relaxation), the maxi-\nmum of the left part among discrete values of q is equal\nto the maximum of the right part among correspond-\ning discrete values of \u2126. Note also that the term in z\nappearing in Eq. (3) cancels out with the entropy term\nin Eq. (2). This relaxation leads to the minimizition\nof the following function of z:\nmax\n\u2126\u2208O \u2212P\n2\u03bbtr\n\u0000zzT (I \u2212\u2126)T K(I \u2212\u2126)\n\u0001\n\u2212\nX\nn\n\u03c0nh(\u2126n),\nwhere O = {\u2126| \u21261N = 1N, \u2126T \u03c0 = \u03c0, \u2126\u22650}. This\nproblem depends on z solely through the matrix zzT ,\nand can thus be relaxed into an SDP in zzT .\nReparametrization in z. With the change of vari-\nable Z = zzT , we have the maximum of a set of linear\nfunctions of Z, which is convex. However, the set Z of\npossible values for Z is non-convex since it is de\ufb01ned\nby:\n(\ndiag(Z) = 1N, Z \u22650, Z \u2ab00,\nrank(Z) = k \u22121.\n(4)\nLet us review these constraints:\n\u2022 In practice, the piecewise-positivity constraint is\nnot necessary and removing it leads to a matrix Z\nwith entries in [\u22121, 1] since Z is positive semi-\nde\ufb01nite with ones on the diagonal.\n\u2022 The rank constraint is the main source of non-\nconvexity, and will be removed, thus leading to a\nconvex relaxation.\n\u2022 The rest of the constraints de\ufb01nes the elliptope:\nEN\n=\n{Z \u2208IRN\u00d7N | diag(Z) = 1N, Z \u2ab00}.\nNote that an additional linear constraint may be\nneeded depending on the considered weakly supervised\nproblem. We give below some examples:\n\u2022 In the case of MIL, this constraint takes the form\nof Z\u2212= 1N\u22121T\nN\u2212, where N\u2212is the number of\nnegative examples, and Z\u2212is the restriction of Z\nto the negative bags.\n\u2022 \u201cMust-not-link\u201d constraints on the instances can\nbe handled: If two bags i and j have labels yi\nand yj such that the set of possible latent labels\nare dissimilar (i.e., Pli \u2229Plj = \u2205), we can con-\nstrain the submatrix Zij to be equal to 0. These\nconstraints are of particular interest in the case\nof SSL, where labeled bags with di\ufb00erent labels\nshould not be assigned to the same latent label.\nIn the rest of this paper, we consider the speci\ufb01c cases\nof SSL, MIL and discriminative clustering:\n\u2022 In SSL, we can reduce the dimensionality of Z:\nSince all the values of z with a same known label\nare equal, it is equivalent to replace them by a\nA convex relaxation for weakly supervised classi\ufb01ers\nsingle element in Z. Denoting by Nu is the num-\nber of unlabeled points, P the number of labels\nand NR = Nu + P, this is equivalent to consid-\nering a matrix RT ZR instead of Z, where R is\na N \u00d7 NR matrix whose restriction to the unla-\nbeled bags is the identity and all other entries are\nzero except for Rn(Nu+l) which is equal to 1 if the\ninstance n has a known label l.\n\u2022 In MIL, the same reduction can be done with P =\n1 and Nu denoting the total number of positive\ninstances.\n\u2022 Discriminative\nclustering\nis\nsimilar\nto\nSSL\nwith P = 0.\nBy taking into account all of these modi\ufb01cations and\nby dropping the rank constraint, we replace the non-\nconvex set Z by the elliptope ENR, leading to the min-\nimization of g(Z) over ENR, where g(Z) is equal to:\nmax\n\u2126\u2208O \u2212P\n2\u03bbtr\n\u0000ZR(I \u2212\u2126)TK(I \u2212\u2126)RT \u0001\n\u2212\nX\nn\n\u03c0nh(\u2126n). (5)\nIn the next section we propose an e\ufb03cient algorithm\nto solve this convex optimization problem.\n4. Optimization\nSince our optimization involves a maximization in our\ninner loop, it cannot be solved directly by a general-\npurpose toolbox. We propose an algorithm dedicated\nto our case. In the rest of this paper we refer to the\nmaximization as the inner loop and the overall mini-\nmization of our cost function as the outer loop.\n4.1. Inner loop\nEvaluating the cost function de\ufb01ned in Eq. (5) involves\nthe maximization of the sum of the entropy of \u2126and\na function T de\ufb01ned as:\nT(\u2126)\n=\n\u22121\n2\u03bbtr\n\u0000(I \u2212\u2126)RT ZR(I \u2212\u2126)T K\n\u0001\n.\nWe\nuse\na\nproximal\nmethod\nwith\na\nreweighted\nKullback-Leibler (KL) divergence which naturally en-\nforces the point-wise positivity contraint in W, and\nleads to an e\ufb03cient Bregman projection with a KL di-\nvergence (an I-projection to be more precise) on the\nrest of the constraints de\ufb01ning W.\nMore precisely,\ngiven a point \u21260, the proximal update is given by max-\nimizing the following function:\nlD(\u2126)=tr\n\u0000\u2126T\u2207T(\u21260)\n\u0001\n\u2212\nX\nn\n\u03c0nh(\u2126n)\u2212LD\u03c0(\u2126\u2225\u21260), (6)\nwhere L is the Lipschitz constant of \u2207T and D\u03c0 is a\nreweighted KL divergence de\ufb01ned as:\nD\u03c0(\u2126\u2225\u21260) =\nX\ni\nX\nn\u2208Ni\n\u03c0n\nN\nX\nm=1\n\u2126nm log\n\u0012\u2126nm\n\u21260nm\n\u0013\n.\nThe I-projection can be done e\ufb03ciently with an iter-\native proportional \ufb01tting procedure (IPFP), which is\nguaranteed to converge to the global minimum with\nlinear convergence rate (Fienberg, 1970).\nNote that to obtain a faster convergence of the inner\nloop, we may take advantage of a low-rank decomposi-\ntion of K and RT ZR and we use an accelerated prox-\nimal scheme on the logarithm of \u2126(Beck & Teboulle,\n2009). To control the distance from the optimum \u2126\u2217,\nwe can use a provably correct duality gap which can be\ncomputed e\ufb03ciently (details are in the supplementary\nmaterial).\n4.2. Outer loop\nThe outer loop minimizes g(Z) as de\ufb01ned in Eq. (5)\nover the elliptope ENR.\nMany approaches have\nbeen\nproposed\nto\nsolve\nthis\ntype\nof\nproblems\n(Goemans & Williamson,\n1995;\nBurer & Monteiro,\n2003; Journ\u00b4ee et al., 2010) but, to the best of our\nknowledge, they all assume that the function and its\ngradient can be computed e\ufb03ciently and put the em-\nphasis on the projection. This is not the case in our\nproblem, and we thus propose a method adapted to\nour particular setting.\nFirst, to simplify the projection on the ENR, we re-\nplace our cost function g(Z) by its diagonally rescaled\nversion gR(Z) = g(diag(Z)\u22121/2Zdiag(Z)\u22121/2). Note\nthat even if this function is in general non-convex, it\ncoincides with g(Z) on ENR, making its restriction to\nthis set convex. This modi\ufb01cation allows us to rescale\nthe diagonal of any update Z to a diagonal equal to 1N\nwithout modifying the value of our cost function.\nOur minimization of gR over the elliptope is also\nbased on a proximal method with a Bregman diver-\ngence to guarantee updates that stay in the feasible\nset. A natural choice for the Bregman divergence is\nthe KL divergence based on the von Neumann en-\ntropy, i.e, the entropy of the eigenvalues of a ma-\ntrix (see more details in the supplementary material).\nThis divergence guarantees that each update has non-\nnegative eigenvalues.\nGiven a point Z0, its update\ncan then be obtained in closed-form as the diagonally\nrescaled version of V Diag(exp(diag( 1\nt E)))V T , where V\nand E are the eigenvectors and the eigenvalues of\n\u2212\u2207gR(Z0)+t log(Z0) and t is a positive step size com-\nputed using a line-search with backtracking.\nAs in the inner loop, we use a computationnally\ntractable provable duality gap, i.e., \u2212NR\u03bbmin, where\n\u03bbmin is the lowest eigenvalue of \u2207gR(Z) (see details in\nthe supplementary material).\n4.3. Rounding\nMany rounding schemes can be applied with simi-\nlar performances. Following Bach & Harchaoui (2007)\nA convex relaxation for weakly supervised classi\ufb01ers\n(a)\n(b)\n(c)\n(d)\n(e)\nFigure 2. (a) The clustering problem, (b) the given kernel matrix K\n= xxT , (c) the matrix Z obtained with\n(Bach & Harchaoui, 2007), (d) the matrix Z obtained with no intercept and (e) our method (best seen in color).\n0\n10\n20\n0.5\n0.6\n0.7\n0.8\n0.9\nNoise dimension\nClustering accuracy\n \n \nOurs\nK\u2212means\n(a)\n(b)\n(c)\nFigure 3. (a) The matrix obtained with our method and\n(b) its corresponding clusters.\n(c) Comparison with k-\nmeans on noise robustness (P = 3, N = 300).\nand Joulin et al. (2010), we use k-means clustering on\nthe eigenvectors associated with the k highest eigen-\nvalues (Ng et al., 2001) to obtain a rounded solution\nz\u2217. This z\u2217is then used to initialize an EM procedure\nto solve the problem de\ufb01ned in Eq. (1) and obtain the\nparameters (w, b) of the classi\ufb01er, leading to \ufb01ner de-\ntails not caught by the convex relaxation.\nA speci\ufb01city of the MIL framework is that strictly no\npoint from a negative bag should be classi\ufb01ed as pos-\nitive, which leads to adding to Eq. (1), the following\nlinear constraints on the parameters of the classi\ufb01er:\n\u2200i \u2208I\u2212, n \u2208Ni, wT\n0 \u03c6(xn) + b0 \u2265wT\n1 \u03c6(xn) + b1.\n(7)\nWe add these hard constraints in the M-step (opti-\nmization over w and b) of the EM procedure.\nThe\nprojection over this set of linear constraints is per-\nformed e\ufb03ciently with an homotopy algorithm in the\ndual (Mairal et al., 2010).\n5. Results\nImplementation.\nOur algorithm is implemented in\nMATLAB and takes from 1 to 5 minutes for 500 points.\nNote that we can e\ufb03ciently compute the solutions for\ndi\ufb00erent values of \u03bb using warm restarts. Our overall\ncomplexity is O(N 3) but we can scale up to several\nthousands of points.\nThe complexity of the di\ufb00er-\nent steps in our algorithm is given in Figure 1. On\nlarger datasets, we can use our relaxation on subsets\nof instances or on pre-clustering the instances (with k-\nmeans) and use it to initialize the EM on the complete\ndataset.\n5.1. Discriminative clustering\nIn this section, we compare our method to two dif-\nferent discriminative clustering methods for the mul-\nticlass case: the SDP relaxation of the soft-max prob-\nlem with no intercept (Guo & Schuurmans, 2008) and\nthe discriminative clustering framework introduced by\nBach and Harchaoui (2007).\nThe latter comparison\nis relevant since they propose a convex cost function\nbased on the square loss with intercept.\nWe consider in Figure 2, as a proof of concept, two\ntoy examples where the goal is to \ufb01nd 3 and 5 clus-\nters with linear kernels and N = 500.\nEven if the\nclusters are linearly separable, the set of values of w\nand b which leads to a perfect separation is very small\n(Figure 2, panel (a)), making the problem challeng-\ning. For fair comparison, we test di\ufb00erent regulariza-\ntion parameters and show the one leading to the best\nperformances. We show the matrix Z obtained for the\nthree methods as well as the matrix K = xxT in Fig-\nure 2. We see that our method clearly obtains a bet-\nter estimation of the class assignment compared to the\nothers, showing the importance of both the soft-max\nloss and the intercept.\nIn panels (a) and (b) of Figure 3, we also show that\nour method works with non-linear kernels in a multi-\nclass setting. Finally, in the panel (c) of Figure 3, we\nshow a comparison with k-means as we increase the\nnumber of dimensions containing only noise, following\nthe setup of Bach & Harchaoui (2007). Our setting is\nthe 3-cluster problem shown in Figure 2 with an RBF\nkernel and N = 300.\nWe see that our algorithm is\nmore robust than k-means.\n5.2. Multiple instance learning\nIn Figure 4, we show some comparisons with other MIL\nmethods on standard datasets (Dietterich & Lathrop,\n1997; Andrews et al., 2003) for a variety of tasks:\nA convex relaxation for weakly supervised classi\ufb01ers\nAlgorithm\nMusk1\nTiger\nElephant\nFox\nTrec1\nCitation k-NN (Wang & Zucker, 2000)\n91.3\n78.0\n80.5\n60.0\n87.0\nEM-DD (Zhang & Goldman, 2001)\n84.8\n72.1\n78.3\n56.1\n85.8\nmi-SVM (Andrews et al., 2003)\n87.4\n78.9\n82.0\n58.2\n93.6\nMI-SVM (Andrews et al., 2003)\n77.9\n84.0\n81.4\n59.4\n93.9\nPPMM Kernel (Wang et al., 2008)\n95.6\n80.2\n82.4\n60.3\n93.3\nRandom init / Uniform\n71.1\n69.0\n74.5\n61.0\n81.3\nTandom init / Weight\n76.6\n71.0\n74.5\n59.0\n84.4\nNo intercept / Uniform\n75.0 \u00b1 19.5\n67.8 \u00b1 10.4\n77.3 \u00b1 9.2\n51.3 \u00b1 6.4\n87.5 \u00b1 5.2\nNo intercept / Weight\n77.8 \u00b1 15.7\n71.0 \u00b1 10.8\n78.9 \u00b1 9.8\n52.1 \u00b1 5.0\n87.3 \u00b1 5.6\nOurs / Uniform\n84.4 \u00b1 14.0\n73.0 \u00b1 8.2\n86.7 \u00b1 3.5\n57.5 \u00b1 5.9\n93.0 \u00b1 4.7\nOurs / Weight\n87.7 \u00b1 13.3\n78.0 \u00b1 5.4\n83.9 \u00b1 4.2\n62.5 \u00b1 6.4\n89.0 \u00b1 6.2\nFigure 4. Accuracy of our approach and of standard methods for MIL. We evaluate our method with and without the\nintercept and with two types of weights. In bold, the signi\ufb01cantly best performances.\nDataset\nLinear\nNonlinear\nEntropy-Reg.\nOurs (Linear)\nOurs (Nonlinear)\nDigit1\n79.41\n82.23\n75.56\n84.57 \u00b1 0.67\n75.45 \u00b1 2.88\nBCI\n49.96\n50.85\n52.29\n52.22 \u00b1 1.13\n50.21 \u00b1 1.09\nl=10\ng241c\n79.05\n75.29\n52.64\n87.15 \u00b1 0.21\n87.29 \u00b1 0.42\ng241d\n53.65\n49.92\n54.19\n54.44 \u00b1 9.09\n53.15 \u00b1 10.09\nUSPS\n69.34\n74.80\n79.75\n57.08 \u00b1 13.34\n79.48 \u00b1 0.50\nDigit1\n81.95\n93.85\n92.72\n91.24 \u00b1 1.66\n93.31 \u00b1 0.97\nBCI\n57.33\n66.75\n71.11\n78.12 \u00b1 2.26\n64.04 \u00b1 0.87\nl=100\ng241c\n81.82\n81.54\n79.03\n86.02 \u00b1 0.72\n85.13 \u00b1 0.71\ng241d\n76.24\n77.58\n74.64\n77.11 \u00b1 1.65\n73.03 \u00b1 3.02\nUSPS\n78.88\n90.23\n87.79\n71.62 \u00b1 2.62\n73.04 \u00b1 0.19\nFigure 5. Comparison in accuracy on SSL databases with methods proposed in (Chapelle et al., 2006).\nIn bold, the\nsigni\ufb01cantly best performances.\na drug activity prediction (musk), image classi\ufb01ca-\ntion (fox, tiger and elephant), and text classi\ufb01cation\n(trec1).\nFor\ncomparison,\nwe\nuse\nthe\nsetting\ndescribed\nby Andrews et al. (2003), where we create 10 random\nsplits of the data, train on 90% of them and test on the\nremaining 10%. We test our algorithm with and with-\nout the intercept and with uniform or bag-speci\ufb01c (i.e.,\n1\nINi for instances in the bag i) weights, and compare it\nto some classical MIL algorithms. Note that we have\nonly tried a linear kernel, and we select the regulariza-\ntion parameter using a 2-fold cross-validation for each\nsplit. Our algorithm obtains comparable performances\nwith methods dedicated to the MIL problem.\n5.3. Semi-supervised learning\nFor the SSL setting, we choose the standard SSL\ndatasets and we compare with methods proposed\nin Chapelle et al. (2006).\nThe benchmarks (Linear\nand Nonlinear) are based on a SVM formulation and\nthe benchmark (Entropy-Reg.) uses an entropy regu-\nlarization. We use our method with either a linear or\na RBF kernel. To \ufb01x our parameters, we follow the\nexperimental setup of Chapelle et al. (2006). Each set\ncontains 1500 points and either l = 10 or 100 of them\nare labeled.\nWe show the results in Figure 5.\nAs\nexpected, since the benchmarks and our formulation\nare very related, the performances are mostly similar\nwhen l = 100. However, when l = 10, our method\nis more robust and its performances get signi\ufb01cantly\nhigher showing that a convex relaxation is less sensible\nto noise and poorly labeled data.\n6. Conclusion\nIn this paper, we propose a convex relaxation of a\ngeneral cost function for weakly supervised problems.\nWe show the importance of a tight convex relaxation\ncompared to relaxation where either the related linear\nclassi\ufb01er has been approximated (absence of intercept)\nor the loss function (square-loss instead of the soft-\nmax loss). Our comparison with standard non-convex\nmethods for MIL and SSL shows the importance of\nthe initialization for robustness of the approach. We\nbelieve that convex relaxation is a powerful tool to ob-\ntain good initializations to non-convex problems. The\ntrade-o\ufb00is that these methods are usually not scal-\nable which suggest to use them on subsets of points or\nafter a quantization step to initialize a more e\ufb03cient\nalgorithm, such as EM.\nAcknowledgements. This paper was partially sup-\nported by the European Research Council (SIERRA\nand VIDEOWORLD projects).\nA convex relaxation for weakly supervised classi\ufb01ers\nReferences\nAndrews, S., Tsochantaridis, I., and Hofmann, T. Support\nvector machines for multiple-instance learning. In NIPS,\n2003.\nAuer, P. and Ortner, R. A boosting approach to multiple\ninstance learning. In ECML, 2004.\nBabenko, B., Yang, M-H., and Belongie, S. Visual tracking\nwith online multiple instance learning. In CVPR, 2009.\nBach, F. and Harchaoui, Z. Di\ufb00rac : a discriminative and\n\ufb02exible framework for clustering. In NIPS, 2007.\nBarnard, K., Duygulu, P., Forsyth, D., de Freitas, N., Blei,\nD., and Jordan, M. I.\nMatching words and pictures.\nJMLR, 3:1107\u20131135, 2003.\nBeck, A. and Teboulle, M.\nA fast iterative shrinkage-\nthresholding algorithm for linear inverse problems.\nSIAM Journal on Imaging Sciences, 2(1):183\u2013202, 2009.\nBelkin, M., Matveeva, I., and Niyogi, P. Regularization\nand semi-supervised learning on large graphs. In COLT,\n2004.\nBennett, K. and Demiriz, A. Semi-supervised support vec-\ntor machines. In NIPS, 1998.\nBlockeel, H., Page, D., and Srinivasan, A. Multi-instance\ntree learning. In ICML, 2005.\nBlum, A. and Mitchell, T. Combining labeled and unla-\nbeled data with co-training. In COLT, 1998.\nBoyd, S. and Vandenberghe, L.\nConvex Optimization.\nCambridge U. P., 2003.\nBurer, S. and Monteiro, R. D. C. A nonlinear programming\nalgorithm for solving semide\ufb01nite programs via low-rank\nfactorization. Mathematical Programming, 2003.\nChapelle, O., Sch\u00a8olkopf, B., and Zien, A. Semi-Supervised\nLearning. MIT press, 2006.\nChen, Y. and Wang, James Z.\nImage categorization by\nlearning and reasoning with regions. JMLR, 2004.\nCour, T., Sapp, Ben, Jordan, Chris, and Taskar, Ben.\nLearning from ambiguously labeled images. In CVPR,\n2009.\nDe la Torre, F. and Takeo, K. Discriminative cluster anal-\nysis. In ICML, 2006.\nDietterich, T. G. and Lathrop, R. H. Solving the multiple-\ninstance problem with axis-parallel rectangles. Arti\ufb01cial\nIntelligence, 89:31\u201371, 1997.\nFienberg, S.E.\nAn iterative procedure for estimation in\ncontingency tables. The Annals of Mathematical Statis-\ntics, pp. 907\u2013917, 1970.\nG\u00a8artner, T., Flach, P. A., Kowalczyk, A., and Smola, A. J.\nMulti\u2013instance kernels. In ICML, 2002.\nGehler, P. V. and Chapelle, O. Deterministic annealing for\nmultiple-instance learning. In AISTATS, 2007.\nGoemans, M. X. and Williamson, D.P. Improved approx-\nimation algorithms for maximum cut and satis\ufb01ability\nproblems using semide\ufb01nite programming.\nJournal of\nthe ACM, 42:1115\u20131145, 1995.\nGuo, Y. and Schuurmans, D. Convex relaxations of latent\nvariable training. In NIPS, 2008.\nHastie, T., Tibshirani, R., and Friedman, J. The Elements\nof Statistical Learning. Springer-Verlag, 2001.\nHullermeier, E. and Beringer, J. Learning from ambigu-\nously labeled examples. In IDA, 2006.\nJin, R. and Ghahramani, Z. Learning with multiple labels.\nIn NIPS, 2003.\nJoachims, T. Transductive inference for text classi\ufb01cation\nusing support vector machines. 1999.\nJoulin, A., Bach, F., and Ponce, J. E\ufb03cient optimization\nfor discriminative latent class models. In NIPS, 2010.\nJourn\u00b4ee, M., Bach, F., Absil, P.-A., and Sepulchre, R. Low-\nrank optimization on the cone of positive semide\ufb01nite\nmatrices. SIAM Journal on Optimization, 20(5):2327\u2013\n2351, 2010.\nKwok, J. T. and Cheung, P. Marginalized multi-instance\nkernels. In IJCAI, 2007.\nMairal, J., Bach, F., Ponce, J., and Sapiro, G. Online learn-\ning for matrix factorization and sparse coding. JMLR,\n2010.\nMaron, O. and Ratan, A. L. Multiple-instance learning for\nnatural scene classi\ufb01cation. In ICML, 1998.\nNg, A. Y., Jordan, M. I., and Weiss, Y. On spectral clus-\ntering: Analysis and an algorithm. In NIPS, 2001.\nRay, S. and Craven, M. Supervised Versus Multiple In-\nstance Learning: An Empirical Comparison. In ICML,\n2005.\nViola, P., Platt, John C., and Zhang, C. Multiple instance\nboosting for object detection. In NIPS, 2006.\nWang, H.-Y., Yang, Q., and Zha, H. Adaptive p-posterior\nmixture-model kernels for multiple instance learning. In\nICML, 2008.\nWang, J. and Zucker, J. Solving multiple-instance problem:\nA lazy learning approach. In ICML, 2000.\nXu, L. and Schuurmans, D.\nUnsupervised and semi-\nsupervised multi-class support vector machines.\nIn\nAAAI, 2005.\nXu, L., Neufeld, J., Larson, B., and Schuurmans, D. Max-\nimum margin clustering. In NIPS, 2005.\nXu, X. and Frank, E. Logistic regression and boosting for\nlabeled bags of instances. In KDDM, 2004.\nYang, C. Image database retrieval with multiple-instance\nlearning techniques. In ICDE, 2000.\nZhang, M. and Zhou, Z. Adapting rbf neural networks to\nmulti-instance learning.\nNeural Processing Letters, 23\n(1):1\u201326, 2006.\nZhang, Q. and Goldman, S. A.\nEm-dd:\nAn improved\nmultiple-instance learning technique. In NIPS, 2001.\nZhou, Z. and Xu, J. On the relation between multi-instance\nlearning and semi-supervised learning. In ICML, 2007.\nZhu, X. Semi-supervised learning literature survey, 2006.\n",
        "sentence": "",
        "context": "of instances are labeled together instead of individu-\nally, and some instances belonging to the same bag\nAppearing in Proceedings of the 29 th International Confer-\nence on Machine Learning, Edinburgh, Scotland, UK, 2012.\nJournal of\nthe ACM, 42:1115\u20131145, 1995.\nGuo, Y. and Schuurmans, D. Convex relaxations of latent\nvariable training. In NIPS, 2008.\nHastie, T., Tibshirani, R., and Friedman, J. The Elements\nof Statistical Learning. Springer-Verlag, 2001.\nLearning. MIT press, 2006.\nChen, Y. and Wang, James Z.\nImage categorization by\nlearning and reasoning with regions. JMLR, 2004.\nCour, T., Sapp, Ben, Jordan, Chris, and Taskar, Ben.\nLearning from ambiguously labeled images. In CVPR,\n2009."
    },
    {
        "title": "Discriminative clustering for image co-segmentation",
        "author": [
            "A. Joulin",
            "F. Bach",
            "J. Ponce"
        ],
        "venue": "In CVPR,",
        "citeRegEx": "Joulin et al\\.,? \\Q2010\\E",
        "shortCiteRegEx": "Joulin et al\\.",
        "year": 2010,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " It has been successfully applied to several computer vision applications, like object discovery (Joulin et al., 2010; Tang et al., 2014) or video/text alignment (Bojanowski et al.",
        "context": null
    },
    {
        "title": "Learning visual features from large weakly supervised data",
        "author": [
            "A. Joulin",
            "L. van der Maaten",
            "A. Jabri",
            "N. Vasilache"
        ],
        "venue": null,
        "citeRegEx": "Joulin et al\\.,? \\Q2016\\E",
        "shortCiteRegEx": "Joulin et al\\.",
        "year": 2016,
        "abstract": "Convolutional networks trained on large supervised dataset produce visual\nfeatures which form the basis for the state-of-the-art in many computer-vision\nproblems. Further improvements of these visual features will likely require\neven larger manually labeled data sets, which severely limits the pace at which\nprogress can be made. In this paper, we explore the potential of leveraging\nmassive, weakly-labeled image collections for learning good visual features. We\ntrain convolutional networks on a dataset of 100 million Flickr photos and\ncaptions, and show that these networks produce features that perform well in a\nrange of vision problems. We also show that the networks appropriately capture\nword similarity, and learn correspondences between different languages.",
        "full_text": "Learning Visual Features from Large Weakly Supervised Data\nArmand Joulin\u2217\najoulin@fb.com\nLaurens van der Maaten\u2217\nlvdmaaten@fb.com\nAllan Jabri\najabri@fb.com\nNicolas Vasilache\nntv@fb.com\nFacebook AI Research\n770 Broadway, New York NY 10003\nAbstract\nConvolutional networks trained on large supervised\ndataset produce visual features which form the basis for\nthe state-of-the-art in many computer-vision problems. Fur-\nther improvements of these visual features will likely require\neven larger manually labeled data sets, which severely lim-\nits the pace at which progress can be made. In this pa-\nper, we explore the potential of leveraging massive, weakly-\nlabeled image collections for learning good visual features.\nWe train convolutional networks on a dataset of 100 million\nFlickr photos and captions, and show that these networks\nproduce features that perform well in a range of vision prob-\nlems. We also show that the networks appropriately capture\nword similarity, and learn correspondences between differ-\nent languages.\n1. Introduction\nRecent studies have shown that using visual features ex-\ntracted from convolutional networks trained on large object\nrecognition datasets [26, 43, 46] can lead to state-of-the-\nart results on many vision problems including \ufb01ne-grained\nclassi\ufb01cation [21, 39], object detection [15], and segmenta-\ntion [36]. The success of these networks has been largely\nfueled by the development of large, manually annotated\ndatasets such as Imagenet [7]. This suggests that in order\nto further improve the quality of visual features, convolu-\ntional networks should be trained on larger datasets.\nAt the same time, this begs the question whether fully\nsupervised approaches are the right way forward to learn-\ning better models. In particular, the manual annotation of\never larger image datasets is very time-consuming1, which\nmakes it a non-scalable solution to improving recognition\nperformances. Moreover, manually selecting and annotat-\ning images often introduces a strong bias towards a spe-\n* Both authors contributed equally.\n1For instance, the development of the COCO dataset [29] took more\nthan 20, 000 annotator hours spread out over two years.\nthe veranda hotel\nportixol palma\nplane approaching zrh\navro regional jet rj\narticle in the local \npaper about all the \nunusual things found\nat otto s home\nstudent housing by \nlungaard tranberg \narchitects in copenhagen \nclick here to see where \nthis photo was taken\nthis was another one with my old digital\ncamera i like the way it looks for some things \nthough slow and lower resolution than new \ncameras another problem is that it s a bit of\na brick to carry and is a pain unless you re\ncarrying a bag with some room it s nearly x x\nand weighs ounces new one is x x and weighs\nounces i underexposed this one a bit did\n exposure bracketing script underexposure on\nthat camera looks melty yummy \ngold kodak \ufb01lm like\nnot as impressive as\nembankment that s for sure\nFigure 1. Six randomly picked photos from the Flickr 100M\ndataset and the corresponding descriptions.\nci\ufb01c task [37, 48]. Another problem of fully supervised ap-\nproaches is that they appear rather inef\ufb01cient compared to\nhow humans learn to recognize objects: unsupervised and\nweakly supervised learning plays an important role in hu-\nman vision [9], as a result of which humans do not need to\nsee thousands of images of, say, aardvarks to obtain a good\ngrasp of what an aardvark looks like.\nIn this paper, we depart from the fully supervised learn-\ning paradigm and ask the question: can we learn high-\nquality visual features from scratch without using any fully\nsupervised data? We perform a series of experiments in\nwhich we train models on a large collection of images and\ntheir associated captions. This type of data is available in\ngreat abundance via photo-sharing websites: speci\ufb01cally,\nwe use a publicly available dataset of 100 million Flickr im-\nages and captions [47] (see Figure 1 for six randomly picked\nFlickr photos and corresponding descriptions).\nLearning\nvisual representations on such weakly supervised data has\n1\narXiv:1511.02251v1  [cs.CV]  6 Nov 2015\nthree major advantages: (1) there is a near-in\ufb01nite amount\nof weakly supervised data available2, (2) the training data is\nnot biased towards solving a speci\ufb01c task, and (3) it is much\nmore similar to how humans learn to solve vision.\nWe present experiments showing that convolutional net-\nworks can learn to identify words that are relevant to a par-\nticular image, despite being trained on very noisy targets.\nMore importantly, our experiments show that the visual fea-\ntures learned by weakly-supervised models are as good as\nthose learned by models that were trained on Imagenet,\nsuggesting that good visual representations can be learned\nwithout full supervision. Moreover, our experiments reveal\nsome bene\ufb01ts of training on weakly supervised data such as\nthe Flickr dataset: our models learn word embeddings that\nare both grounded in vision and capture important seman-\ntic information, for instance, on word similarity and analo-\ngies. Because our training data is multilingual, our models\nalso relate words from different languages by observing that\nthey are frequently assigned to similar visual inputs.\n2. Related Work\nThis study is not the \ufb01rst to explore alternatives to\ntraining convolutional networks on manually annotated\ndatasets [6, 10, 41, 59]. In particular, Chen and Gupta [6]\npropose a curriculum-learning approach that trains convolu-\ntional networks on \u201ceasy\u201d examples retrieved from Google\nImages, and then \ufb01netunes the models on weakly labeled\nimage-hashtag pairs.\nTheir results suggest that such a\ntwo-stage approach outperforms models trained on solely\nimage-hashtag data. This result is most likely due to the\nlimited size of the dataset that was used for training (\u223c1.2\nmillion images): our results show substantial performance\nimprovements can be obtained by training on much larger\nimage-word datasets. Izadinia et al. [20] also \ufb01netune pre-\ntrained convolutional networks on a dataset of Flickr images\nusing a vocabulary of 5, 000 words. By contrast, this study\ntrains convolutional networks from scratch on 100 million\nimages associated with 100, 000 words. Ni et al. [32] also\ntrain convolutional networks on tens of millions of image-\nword pairs, but their study focuses on systems issues and\ndoes not report recognition performances.\nSeveral studies have used weakly supervised data in\nimage-recognition pipelines that use pre-de\ufb01ned visual fea-\ntures.\nIn particular, Li and Fei-Fei [27] present a hier-\narchical topic model that performs simultaneous dataset\nconstruction and incremental learning of object recogni-\ntion models. Li et al. [28] learn mid-level representations\nby training a multiple-instance learning variant of SVMs\non hand-crafted low-level features extracted from images\ndownloaded using Google Image search. Denton et al. [8]\n2The combined number of photo uploads via various platforms was\nestimated to be 1.8 billion photos per day in 2014 [30].\nlearn (user-conditional) embeddings of images and hashtags\non a large collection of Instagram photos and hashtags. Tor-\nresani et al. [49] train a collection of weak object classi\ufb01ers\nand use the classi\ufb01er outputs as additional image features\n(in addition to low-level image features). In contrast to these\nstudies, we backpropagate the learning signal through the\nentire pipeline, allowing us to learn visual features.\nIn contrast to our work, many prior studies also attempt\nto explicitly discard low-quality labels by developing al-\ngorithms that identify relevant image-hashtag pairs from a\nweakly labeled dataset [12, 35, 54]. These studies solely\naim to create a \u201cclean\u201d dataset and do not explore the train-\ning of recognition pipelines on noisy data. By contrast, we\nstudy the training of a full image-recognition pipeline; our\nresults suggest that \u201clabel cleansing\u201d may not be necessary\nto learn good visual features if the amount of weakly super-\nvised training data is suf\ufb01ciently large.\nOur work is also related to prior studies on multi-\nmodal embedding [44, 56] that explore approaches such\nas kernel canonical component analysis [16, 18], restricted\nBoltzmann machines [45], topic models [22], and log-\nbilinear models [25]. Some works co-embed images and\nwords [14], whereas others co-embed images and sentences\nor n-grams [13, 23, 53]. Frome et al. [14] show that con-\nvolutional networks trained jointly on annotated image data\nand a large corpus of unannotated texts can be used for zero-\nshot learning. Our work is different from those prior studies\nin that we train convolutional networks solely on weakly su-\npervised data.\n3. Weakly Supervised Learning of Convnets\nWe train our models on the publicly available Flickr\n100M data set [47]. The data set contains approximately\n99.2 million photos with associated titles, hashtags, and\ncaptions. We will release our code and models upon publi-\ncation of the paper.\nPreprocessing. We preprocessed the text by removing all\nnumbers and punctuation (e.g., the # character for hash-\ntags), removing all accents and special characters, and\nlower-casing. We then used the Penn Treebank tokenizer3\nto tokenize the titles and captions into words, and used\nall hashtags and words as targets for the photos. We re-\nmove the 500 most common words (e.g., \u201cthe\u201d, \u201cof\u201d, and\n\u201cand\u201d) and because the tail of the word distribution is very\nlong [1], we restrict ourselves to predicting only the K =\n{1, 000; 10, 000; 100, 000} most common words. For these\ndictionary sizes, the average number of targets per photo is\n3.72, 5.62, and 6.81, respectively. The target for each im-\nage is a bag of all the words in the dictionary associated\nwith that image, i.e., a multi-label vector y \u2208{0, 1}K. The\nimages were preprocessed by rescaling them to 256\u00d7256\n3https://www.cis.upenn.edu/\u02dctreebank/tokenizer.sed\npixels, cropping a central region of 224\u00d7224 pixels, sub-\ntracting the mean pixel value of each image, and dividing\neach image by the standard deviation of its pixel values.\nNetwork architecture. We experimented with two con-\nvolutional network architectures, viz., the AlexNet archi-\ntecture [26] and the GoogLeNet architecture [46].\nThe\nAlexNet architecture is a seven-layer architecture that uses\nmax-pooling and recti\ufb01ed linear units at each layer; it has\nbetween 15M and 415M parameters depending on the vo-\ncabulary size. The GoogLeNet architecture is a narrower,\ntwelve-layer architecture that has a shallow auxiliary clas-\nsi\ufb01er to help learning; it holds the state-of-the-art on the Im-\nageNet ILSVRC2014 dataset [42]. Our GoogLeNet models\nhad between 4M and 404M parameters depending on vo-\ncabulary size. For exact details on both architectures, we\nrefer the reader to [26] and [46], respectively\u2014our archi-\ntectures only deviate from the architectures described there\nin the size of their \ufb01nal output layer.\nLoss functions.\nWe denote the training set by D\n=\n{(xn, yn)}n=1,...,N with the D-dimensional observation\nx \u2208RD and the multi-label vector y \u2208{0, 1}K.\nWe\nparametrize the mapping f(x; \u03b8) from observation x \u2208RD\nto some intermediate embedding e \u2208RE by a convolutional\nnetwork with parameters \u03b8; and the mapping from that em-\nbedding e to a label y \u2208{0, 1}K by sign(W\u22a4e), where\nW is an E \u00d7 K matrix. The parameters \u03b8 and W are op-\ntimized jointly to minimize a one-versus-all or multi-class\nlogistic loss. The one-versus-all logistic loss sums binary\nclassi\ufb01er losses over all classes:\nN\nX\nn=1\nK\nX\nk=1\nynk\nNk\nlog \u03c3(f(xn; \u03b8)) + 1 \u2212ynk\nN \u2212Nk\nlog(1 \u2212\u03c3(f(xn, \u03b8))),\nwhere \u03c3(x) = 1/(1 + exp(\u2212x)) is the sigmoid function\nand Nk is the number of positive examples for the class\nk. The multi-class logistic loss minimizes the negative sum\nof the log-probabilities over all positive labels. Herein, the\nprobabilities are computed using a softmax layer:\n\u2113(\u03b8, W; D) = \u22121\nN\nN\nX\nn=1\nK\nX\nk=1\nynk log\n\"\nexp(w\u22a4\nk f(xn; \u03b8))\nPK\nk\u2032=1 exp(w\u22a4\nk\u2032f(xn; \u03b8))\n#\n.\nIn preliminary experiments, we also considered a pairwise\nranking loss [50, 53]. This loss only updates two columns\nof W per training example (corresponding to a positive\nand a negative label). We found that when training convo-\nlutional networks end-to-end, these sparse updates signi\ufb01-\ncantly slow down training, which is why we did not con-\nsider ranking loss further in this study.\nClass balancing. The distribution of words in our dataset\nfollows a Zipf distribution [1]: much of its probability mass\nis accounted for by a few classes. If we are not careful about\nhow we sample training instances, these classes dominate\nthe learning, which may lead to poor general-purpose vi-\nsual features [2]. We follow Mikolov et al. [31] and sam-\nple instances uniformly per class. Speci\ufb01cally, we select a\ntraining example by picking a word uniformly at random\nand randomly selecting an image associated with that word.\nAll the other words are considered negative for the corre-\nsponding image, even words that are also associated with\nthat image. Although this procedure potentially leads to\nnoisier gradients, it works well in practice.\nTraining. We trained our models with stochastic gradient\ndescent (SGD) on batches of size 128. In all experiments,\nwe set the initial learning rate to 0.1 and after every sweep\nthrough a million images (an \u201cepoch\u201d), we compute the pre-\ndiction error on a held-out validation set. When the vali-\ndation error has increased after an \u201cepoch\u201d, we divide the\nlearning rate by 2 and continue training; but we use each\nlearning rate for at least 10 epochs. We stopped training\nwhen the learning rate became smaller than 10\u22126. AlexNet\ntakes up to two weeks to train on a setup with 4 GPUs, while\ntraining a GoogLeNet takes up to three weeks.\nLarge dictionary. Training a network on 100, 000 classes\nis computationally expensive: a full forward-backward pass\nthrough the last linear layer with a single batch takes\nroughly 1, 600ms (compared to 400ms for the rest of the\nnetwork).\nTo circumvent this problem, we only update\nthe weights that correspond to classes present in a train-\ning batch. This means we update at most 128 columns of\nW per batch, instead of all 100, 000 columns. We found\nsuch \u201cstochastic gradient descent over targets\u201d to work very\nwell in practice: it reduced the training time of our largest\nmodels from months to weeks.\nWhilst our stochastic approximation is consistent for the\none-versus-all loss, it is not for the multi-class logistic loss:\nin the worst-case scenario, the \u201capproximate\u201d logistic loss\ncan be arbitrarily far from the true loss. However, we ob-\nserve that the approximation works well in practice, and up-\nper and lower bounds on the expected value of the approx-\nimate loss suggest that, indeed, it is closely related to the\ntrue loss. Denoting sk = exp\n\u0000w\u22a4\nk f(xn; \u03b8)\n\u0001\nand the set of\nsampled classes by C (with |C| \u2264K) and leaving out con-\nstant terms for brevity, it is trivial to see that the expected\napproximate loss never overestimates the true loss:\nE\n\"\nlog\nX\nc\u2208C\nsc\n#\n\u2264log\n K\nX\nk=1\nsk\n!\n= log(Z).\nAssuming that \u2200k : sk \u226514, we use Markov\u2019s inequality to\nobtain a lower bound on the expected approximate loss, too:\nE\n\"\nlog\nX\nc\u2208C\nsc\n#\n\u2265P\n \n1\n|C|\nX\nc\u2208C\nsc \u22651\nK Z\n! \u0012\nlog |C|\nK + log Z\n\u0013\n.\n4This assumption can always be satis\ufb01ed by adding a constant inside\nthe exponentials of both the numerator and the denominator of the softmax.\nDictionary size K\nType\nNetwork\n1, 000 10, 000 100, 000\nAlexNet\n8.27\n4.01\n1.61\nPretrained\nGoogLeNet\n13.20\n4.76\n1.54\nAlexNet\n17.98\n6.27\n2.56\nEnd-to-end GoogLeNet\n20.21\n6.47\n\u2013\nTable 1. Precision@10 on held-out test data of word prediction\nmodels on the Flickr 100M dataset for three different dictionary\nsizes K.\nWe present results for (1) logistic regressors trained\non features extracted from convolutional networks that were pre-\ntrained on Imagenet and (2) convolutional networks trained end-\nto-end using multiclass logistic loss. Higher values are better.\nFlickr Word Prediction\nsize of Flickr training set (in millions) \u2192\nprecision@10 \u2192\nPascal VOC\nmAP \u2192\nFigure 2. Lefthand side: Precision@10 of by weakly supervised\nAlexNets trained on Flickr datasets of different sizes on a held-out\ntest set, using K =1, 000 (in red) and a single crop. For reference,\nwe also show the precision@10 of logistic regression trained on\nfeatures from convolutional networks trained on ImageNet with\nand without jittering (in blue and black, respectively). Righthand\nside: Mean average precision on Pascal VOC 2007 dataset ob-\ntained by logistic regressors trained on features extracted from\nAlexNet trained on Flickr (in red) and ImageNet with and with-\nout jittering (in blue and black). Higher values are better.\nThis bound relates the sample average of sc to its expected\nvalue, and is exact when |C| \u2192K. The lower bound only\ncontains an additive constant log(|C|/K), which shows that\nthe approximate loss is closely related to the true loss.\n4. Experiments\nTo assess the quality of our weakly-supervised convo-\nlutional networks, we performed three sets of experiments:\n(1) experiments measuring the ability of the models to pre-\ndict words given an image, (2) transfer-learning experi-\nments measuring the quality of the visual features learned\nby our models in a range of computer-vision tasks, and (3)\nexperiments evaluating the quality of the word embeddings\nlearned by the networks.\n4.1. Experiment 1: Associated Word Prediction\nExperimental setup. We measure the ability of our mod-\nels to predict words that are associated with an image using\nthe precision@k on a test set of 1 million Flickr images,\nvintage\nautumn\nabandoned\ngig\nrijksmuseum\nart\nFigure 3. Six test images with high scores for different words.\nThe scores were computed using an AlexNet trained on the Flickr\ndataset with a dictionary size of K =100, 000.\nwhich we held out until after all our models were trained.\nPrecision@k is a suitable measure for assessing word pre-\ndiction performance because (1) it corresponds naturally to\nuse cases in which a user retrieves images using a text query\nand inspects only the top k results and (2) it is robust to the\nfact that targets are noisy, i.e., that images may have words\nassigned to them that do not describe their visual content.\nResults. Table 1 presents the precision@10 of word pre-\ndiction models trained on the Flickr dataset using dictionar-\nies with 1, 000, 10, 000, and 100, 000 words5. As a base-\nline, we train L2-regularized logistic regressors on features\nproduced by convolutional networks trained on the Ima-\ngenet dataset6; the regularization parameter was tuned on\na held-out validation set. The results of this experiment\nshow that end-to-end training of convolutional networks on\nthe Flickr dataset works substantially better than training a\nclassi\ufb01er on features extracted from an Imagenet-pretrained\nnetwork: end-to-end training leads to a relative gain of 45\nto 110% in precision@10. This suggests that the features\nlearned by networks on the Imagenet dataset are too tai-\nlored to the speci\ufb01c set of classes in that dataset. The re-\nsults also show that the relative differences between the\nGoogLeNet and AlexNet architectures are smaller on the\nFlickr 100M dataset than on the Imagenet dataset, possibly,\nbecause GoogLeNet has less capacity than AlexNet.\nIn preliminary experiments, we also trained models us-\n5Our GoogLeNet networks with K = 100, 000 words did not \ufb01nish\ntraining by the submission deadline. We will update the paper with those\nresults as they become available.\n6The Imagenet models were trained 224\u00d7224 crops that where ran-\ndomly selected from 256\u00d7256 pixel input images. We applied photomet-\nric jittering on the input images [19], and trained using SGD with batches\nof 128 images. Our pretrained networks perform on par with the state-of-\nthe-art on ImageNet: a single AlexNet obtains a top-5 test error of 24.0%\non a single crop, and our GoogLeNet obtains a top-5 error of 10.7%.\nFigure 4. t-SNE map of 20, 000 Flickr test images based on features extracted from the last layer of an AlexNet trained with K = 1, 000.\nA full-resolution map is presented in the supplemental material. The inset shows a cluster of sports.\ning one-versus-all logistic loss: using a dictionary of K =\n1, 000 words, such a model achieves a precision@10 of\n16.43 (compared to 17.98 for multiclass logistic loss). We\nsurmise this is due to the problems one-versus-all logistic\nloss has in dealing with class imbalance: because the num-\nber of negative examples is much higher than the number\nof positive examples (for the most frequent class, more than\n95.0% of the data is still negative), the rebalancing weight\nin front of the positive term is very high, which leads to\nspikes in the gradient magnitude that hamper SGD training.\nWe tried various reweighting schemes to counter this effect,\nbut nevertheless, multiclass logistic loss consistently out-\nperformed one-versus-all logistic loss in our experiments.\nTo investigate the performance of our models as a func-\ntion of the amount of training data, we also performed ex-\nperiments in which we varied the Flickr training set size.\nThe lefthand side of Figure 2 presents the resulting learn-\ning curves for the AlexNet architecture with K = 1, 000.\nThe \ufb01gure shows that there is a clear bene\ufb01t of training on\nlarger datasets: the word prediction performance of the net-\nworks increases substantially when the training set is in-\ncreased beyond 1 million images (which is roughly the size\nof Imagenet); for our networks, it only levels out after \u223c50\nmillion images.\nTo illustrate the kinds of words for which our models\nlearn good representations, we show a high-scoring test im-\nage for six different words in Figure 3. To obtain more in-\nsight into the features learned by the models, we applied\nt-SNE [51, 52] to features extracted from the penultimate\nlayer of an AlexNet trained on 1, 000 words. This produces\nmaps in which images with similar visual features are close\ntogether; Figure 4 shows such a map of 20, 000 Flickr test\nimages. The inset shows a \u201csports\u201d cluster that was formed\nby the visual features; interestingly, it contains visually very\ndissimilar sports ranging from baseball to \ufb01eld hockey, ice\nhockey and rollerskating. Whilst all sports are grouped to-\ngether, the individual sports are still clearly separable: the\nmodel can capture this multi-level structure because the im-\nages sometimes occur with the word \u201csports\u201d and some-\ntimes with the name of the individual sport itself. A model\ntrained on classi\ufb01cation datasets such as Pascal VOC is un-\nlikely to learn similar structure unless an explicit target tax-\nonomy is de\ufb01ned (as in the Imagenet dataset). Our results\nsuggest that such taxonomies can be learned from weakly\nlabeled data instead.\n4.2. Experiment 2: Transfer Learning\nExperimental setup. To assess the quality of the visual fea-\ntures learned by our models, we performed transfer-learning\nexperiments on seven test datasets comprising a range of\ncomputer-vision tasks: (1) the MIT Indoor dataset [38],\n(2) the MIT SUN dataset [55], (3) the Stanford 40 Actions\ndataset [57], (4) the Oxford Flowers dataset [33], (5) the\nSports dataset [17], (6) the ImageNet ILSVRC 2014 dataset\n[42], and (7) the Pascal VOC 2007 dataset [11]. We applied\nthe same preprocessing as before on all datasets: we resized\nthe images to 224\u00d7224 pixels, subtracted their mean pixel\nvalue, and divided by their standard deviation.\nFollowing [40], we compute the output of the penulti-\nmate layer for an input image and use this output as a fea-\nture representation for the corresponding image. We eval-\nuate features obtained from Flickr-trained networks as well\nas Imagenet-trained networks, and we also perform exper-\niments where we combine both features by concatenating\nthem. We train L2-regularized logistic regressors on the\nfeatures to predict the classes corresponding to each of the\ndatasets. For all datasets except the Imagenet and Pascal\nVOC datasets, we report classi\ufb01cation accuracies on a sep-\narate, held-out test set. For Imagenet, we report classi\ufb01ca-\ntion errors on the validation set. For Pascal VOC, we report\naverage precisions on the test set as is customary for that\ndataset. As before, we use convolutional networks trained\non the Imagenet dataset as baseline. Additional details on\nthe setup of the transfer-learning experiments are presented\nin the supplemental material.\nResults. Table 3 presents the classi\ufb01cation accuracies\u2014\naveraged over 10 runs\u2014of logistic regressors on six datasets\nfor both fully supervised and weakly supervised feature-\nDataset\nModel\nmAP\nAlexNet\n75.7\n61.9\n66.9\n66.5\n29.3\n56.1\n73.5\n68.0\n47.1\n40.9\n57.4\n60.0\n74.0\n63.2\n86.2\n38.8\n57.9\n45.5\n75.7\n51.1\n59.8\nImagenet\nGoogLeNet\n91.3\n84.0\n88.4\n87.2\n42.4\n79.6\n87.3\n85.0\n59.1\n66.5\n69.5\n83.3\n86.6\n82.9\n88.4\n57.5\n75.8\n64.6\n89.5\n73.8\n77.1\nAlexNet\n84.0\n72.2\n70.2\n77.0\n29.5\n60.8\n79.3\n69.5\n49.2\n40.5\n54.0\n57.1\n79.2\n64.6\n90.2\n43.0\n47.5\n44.1\n85.0\n50.7\n62.4\nFlickr\nGoogLeNet\n91.5\n83.7\n84.1\n88.5\n41.7\n78.0\n86.8\n84.0\n54.7\n55.5\n63.3\n78.5.\n86.0\n77.4\n91.1\n51.3\n60.8\n52.7\n91.9\n60.9\n73.2\nAlexNet\n82.96 70.32 73.28 76.29 32.21 61.84 79.81 72.91 51.56 43.82 60.77 63.32 78.63 67.72 90.26 45.45 53.15 49.14\n84.8\n55.8\n64.7\nCombined GoogLeNet\n94.09 85.03 89.71 88.47 49.35 81.47\n88.1\n85.2\n60.51 68.37 71.65 85.81 88.87 85.22 88.69 60.45 77.26 66.61 90.71 74.49\n79.0\nTable 2. Pascal VOC 2007 dataset: Average precision (AP) per class and mean average precision (mAP) of classi\ufb01ers trained on features\nextracted with networks trained on the Imagenet and the Flickr dataset (using K =1, 000 words). Higher values are better.\nIndoor\nOxford Flowers\nStanford 40 Actions\nMIT SUN\nImagenet (no jittering)\nImagenet (jittering)\nFlickr\nclassi\ufb01cation accuracy \u2192\nsize of Flickr training set (in millions) \u2192\nFigure 5. Average classi\ufb01cation accuracy (averaged over ten runs)\nof logistic regressors trained on features produced by weakly su-\npervised AlexNets trained on Flickr image-caption datasets of dif-\nferent sizes on six different datasets (in red). For reference, we also\nshow the classi\ufb01cation accuracy of classi\ufb01ers trained on features\nfrom convolutional networks trained on ImageNet without jitter-\ning (in black) and with jittering (in blue). Dashed lines indicate\nthe standard deviation across runs. Higher values are better.\nproduction networks, as well as for a combination of both\nnetworks. Table 2 presents the average precision on the Pas-\ncal VOC 2007 dataset. Our weakly supervised models were\ntrained on a dictionary of K = 1, 000 words (we obtained\nsimilar results for models trained on 10, 000 and 100, 000\nwords; see the supplementary material). The results in the\ntables show that using the AlexNet architecture, weakly su-\npervised networks learn visual features of similar quality as\nfully supervised networks. This is quite remarkable because\nthe networks learned these features without any strong su-\npervision.\nAdmittedly, weakly supervised networks perform poorly\non the \ufb02owers dataset: Imagenet-trained networks produce\nbetter features for that dataset, presumably, because the Im-\nagenet dataset itself focuses strongly on \ufb01ne-grained classi-\n\ufb01cation. Interestingly, fully supervised networks do learn\nbetter features than weakly supervised networks when a\nDataset\nModel\nIndoor SUN Action Flower Sports ImNet\nAlexNet\n53.82\n41.40\n51.27\n80.28\n86.07\n53.63\nImagenet\nGoogLeNet\n64.00\n48.76\n67.10\n79.05\n95.91\n69.89\nAlexNet\n53.19\n42.67\n51.69\n69.72\n86.79\n34.93\nFlickr\nGoogLeNet\n55.56\n44.43\n52.84\n65.80\n87.40\n33.62\nAlexNet\n58.76\n47.27\n56.35\n83.28\n87.50\n\u2013\nCombined GoogLeNet\n67.87\n55.04\n69.19\n83.74\n95.79\n\u2013\nTable 3. Classi\ufb01cation accuracies on held-out test data of L2-\nregularized logistic regressors obtained on six datasets (MIT In-\ndoor, MIT SUN, Stanford 40 Actions, Oxford Flowers, Sports, and\nImageNet) based on feature representations obtained from convo-\nlutional networks trained on the Imagenet and the Flickr dataset\n(using K = 1, 000 words and a single crop). Errors are averaged\nover 10 runs. Higher values are better.\nGoogLeNet architecture is used: this result is in line with\nthe results from 4.1, which suggest that GoogLeNet has too\nlittle capacity to learn optimal models on the Flickr data.\nThe substantial performance improvements we observe in\nexperiments in which features from both networks are com-\nbined suggest that the features learned by both models com-\nplement each other. We note that achieving state-of-the-art\nresults [5, 34, 39, 60] on these datasets requires the devel-\nopment of tailored pipelines, e.g., using many image trans-\nformations and model ensembles, which is out of the scope\nof this paper.\nWe also measured the transfer-learning performance as a\nfunction of the Flickr training set size. The results of these\nexperiments with the AlexNet architecture and K = 1, 000\nare presented in Figure 5 for four of the datasets (Indoor,\nMIT SUN, Stanford 40 Actions, and Oxford Flowers); and\nin the righthand side of Figure 2 for the Pascal VOC dataset.\nThe results are in line with those in 4.1: they show that tens\nof millions of images are required to learn good feature-\nproduction networks on weakly supervised data.\n4.3. Experiment 3: Assessing Word Embeddings\nThe weights in the last layer of our networks can be\nviewed as an embedding of the words. This word embed-\nding is, however, different from those learned by language\nmodels such as word2vec [31] that learn embeddings based\non word co-occurrence: it is constructed without ever ob-\nserving two words co-occurring (recall that during training,\nwe use a single, randomly selected word as target for an im-\nage). This means that structure in the word embedding can\nonly be learned when the network notices that two words\nare assigned to images with a similar visual structure. We\nperform two sets of experiments to assess the quality of the\nword embeddings learned by our networks: (1) experiments\ninvestigating how well the word embeddings represent se-\nmantic information and (2) experiments investigating the\nability of the embeddings to learn correspondences between\ndifferent languages.\nSemantic information. We evaluate our word embeddings\non two datasets that capture different types of semantic in-\nformation: (1) a syntactic-semantic questions dataset [31]\nand (2) the MEN word similarity dataset [4]. The syntactic-\nsemantic dataset contains 8, 869 semantic and 10, 675 syn-\ntactic questions of the form \u201cA is to B as C is to D\u201d. Fol-\nlowing [31], we predict D by \ufb01nding the word embed-\nding vector wD that has the highest cosine similarity with\nwB\u2212wA+wC (excluding A, B, and C from the search), and\nmeasure the number of times we predict the correct word D.\nThe MEN dataset contains 3, 000 word pairs spanning 751\nunique words\u2014all of which appear in the ESP Game image\ndataset\u2014with an associated similarity rating. The similar-\nity ratings are averages of ratings provided by a dozen hu-\nman annotators. Following [24] and others, we measure the\nquality of word embeddings by the Spearman\u2019s rank cor-\nrelation of the cosine similarity of the word pairs and the\nhuman-provided similarity rating for those pairs. In all ex-\nperiments, we excluded word quadruples / pairs that con-\ntained words that are not in our dictionary. We repeated the\nexperiments for three dictionary sizes. As a baseline, we\nmeasured the performance of word2vec models that were\ntrained on all comments in the Flickr dataset (using only\nthe words in the dictionary).\nThe prediction accuracies of our experiments on the\nsyntactic-semantic dataset for three dictionary sizes are pre-\nsented in Table 4. Table 5 presents the rank correlations for\nour word embeddings on the MEN dataset (for three vo-\ncabulary sizes). As before, we only included word pairs\nfor which both words appeared in the vocabulary. The re-\nsults of these experiments show that our weakly supervised\nmodels, indeed, learned meaningful semantic structure. The\nresults also show that the quality of our word embeddings\nis lower than that of word2vec, because unlike our models,\nword2vec observes word co-occurrences during training.\nWe also made t-SNE maps of the embedding of 10, 000\nwords in Figure 6. The insets highlight six \u201ctopics\u201d: (1)\nmusical performance, (2) sunsets, (3) female and male\n\ufb01rst names, (4) gardening, (5) photography, and (6) mili-\ntary. These topics were identi\ufb01ed solely because the words\nin them are associated with images containing similar vi-\nsual content: for instance, \ufb01rst names are likely to be as-\nsigned to photos showing one or a few persons. Interest-\ningly, the \u201csunset\u201d and \u201cgardening\u201d topics show examples\nModel\nK=1, 000 K=10, 000 K=100, 000\nAlexNet\n67.91\n29.29\n0.85\nGoogLeNet\n71.92\n24.06\n\u2013\nword2vec\n71.92\n61.35\n47.24\nAlexNet + word2vec\n74.79\n57.26\n44.35\nGoogLeNet + word2vec\n75.36\n56.05\n\u2013\nTable 4. Prediction accuracy of predicting D in questions \u201cA is to\nB like C is to D\u201d using convolutional-network word embeddings\nand word2vec on the syntactic-semantic dataset, using three dic-\ntionary sizes. Questions containing words not in the dictionary\nwere removed. Higher values are better.\nModel\nK=1, 000 K=10, 000 K=100, 000\nAlexNet\n73.77\n75.73\n67.35\nGoogLeNet\n75.72\n75.89\n\u2013\nword2vec\n75.25\n77.53\n77.91\nAlexNet + word2vec\n78.17\n79.24\n78.57\nGoogLeNet + word2vec\n78.75\n79.11\n\u2013\nTable 5. Spearman\u2019s rank correlation of cosine similarities be-\ntween convolutional-network (and word2vec) word embeddings\nand human similarity judgements on the MEN dataset. Word pairs\ncontaining words not in the dictionary were removed. Higher val-\nues are better.\nof grouping of words from different languages.\nFor in-\nstance, \u201csonne\u201d, \u201csoleil\u201d, \u201csole\u201d mean \u201csun\u201d in German,\nFrench, and Italian, respectively; and \u201cgarten\u201d and \u201cgia-\nrdino\u201d are the German and Italian words for garden.\nMulti-lingual correspondences.\nTo further investigate\nthe ability of our models to \ufb01nd correspondences between\nwords from different languages, we selected pairs of words\nfrom an English-French dictionary7 for which: (1) both the\nEnglish and the French word are in the Flickr dictionary and\n(2) the English and the French word are different. This pro-\nduced 309 English-French word pairs for models trained on\nK = 10, 000 words, and 3, 008 English-French word pairs\nfor models trained on K = 100, 000 words. We measured\nthe quality of the multi-lingual word correspondences in the\nembeddings by taking a word in one language and rank-\ning the words in the other language according to their co-\nsine similarity with the query word. We measure the preci-\nsion@k of the predicted word ranking, using both English\nand French words as query words.\nTable 6 presents the results of this experiment: for a\nnon-trivial number of words, our procedure correctly iden-\nti\ufb01ed the French translation of an English word, and vice\nversa. Finding the English counterpart of a French word\nis harder then the other way around, presumably, because\nthere are more English than French words in the dictionary:\nthis means the English word embeddings are better opti-\nmized than the French ones. In Table 7, we show the ten\n7http://www-lium.univ-lemans.fr/\u02dcschwenk/\nnnmt-shared-task/\nFigure 6. t-SNE map of 10, 000 words based on their embeddings as learned by a weakly supervised convolutional network trained on the\nFlickr dataset. Note that all the semantic information represented in the word embeddings is the result of observing that these words are\nassigned to images with similar visual content (the model did not observe word co-occurrences during training). A full-resolution version\nof the map is provided in the supplemental material.\nK\nQuery \u2192Response\nk = 1 k = 5 k = 10\nEnglish \u2192French\n33.01\n50.16\n55.34\n10, 000\nFrench \u2192English\n23.95\n50.16\n56.63\nEnglish \u2192French\n12.30\n22.24\n26.50\n100, 000 French \u2192English\n10.11\n18.78\n23.44\nTable 6. Precision@k of identifying the French counterpart of an\nEnglish word (and vice-versa) for two dictionary sizes, at three\ndifferent levels of k. Chance level (with k = 1) is 0.0032 for\nK =10, 000 words and 0.00033 for K =100, 000 words. Higher\nvalues are better.\nEnglish\nFrench\nEnglish\nFrench\noas\noea\nuzbekistan\nouzbekistan\ninfrared\ninfrarouge\nmushroom\nchampignons\ntomatoes\ntomates\n\ufb01lmed\nserveur\nbookshop\nlibrairie\nmauritania\nmauritanie\nserver\napocalyptique\npencils\ncrayons\nTable 7. Ten highest-scoring pairs of words, as measured by the\ncosine similarity between the corresponding word embeddings.\nCorrect pairs of words are colored green, and incorrect pairs are\ncolored red according to the dictionary. The word \u201coas\u201d is an ab-\nbreviation for the Organization of American States.\nmost similar word pairs, measured by the cosine similar-\nity between their word embeddings. These word pairs sug-\ngest that models trained on Flickr data \ufb01nd correspondences\nbetween words that have clear visual representations, such\nas \u201ctomatoes\u201d or \u201cbookshop\u201d. Interestingly, the identi\ufb01ed\nEnglish-French matches appear to span a broad set of do-\nmains, including objects such as \u201cpencils\u201d, locations such\nas \u201cmauritania\u201d, and concepts such as \u201cinfrared\u201d.\n5. Discussion and Future Work\nThis study demonstrates that convolutional networks can\nbe trained from scratch without any manual annotation and\nshows that good features can be learned from weakly super-\nvised data. Indeed, our models learn features that are nearly\non par with those learned from an image collection with\nover a million manually de\ufb01ned labels, and achieve good\nresults on a variety of datasets. (Obtaining state-of-the-art\nresults requires averaging predictions over many crops and\nmodels, which is outside the scope of this paper.) More-\nover, our results show that weakly supervised models can\nlearn semantic structure from image-word co-occurrences.\nIn addition, our results lead to three main recommen-\ndations for future work in learning models from weakly\nsupervised data.\nFirst, our results suggest that the best-\nperforming models on the Imagenet dataset are not opti-\nmal for weakly supervised learning. We surmise that cur-\nrent models have insuf\ufb01cient capacity for learning from the\ncomplex Flickr dataset. Second, multi-class logistic loss\nperforms remarkably well in our experiments even though\nit is not tailored to multi-label settings. Presumably, our\napproximate multiclass loss works very well on large dic-\ntionaries because it shares properties with losses known to\nwork well in that setting [31, 50, 53]. Third, it is essential\nto sample data uniformly per class to learn good visual fea-\ntures [2]. Uniform sampling per class ensures that frequent\nclasses in the training data do not dominate the learned fea-\ntures, which makes them better suited for transfer learning.\nIn future work, we aim to combine our weakly su-\npervised vision models with a language model such as\nword2vec [31] to perform, for instance, visual question an-\nswering [3, 58]. We also intend to further investigate the\nability of our models to learn visual hierarchies, such as the\n\u201csports\u201d example in Section 4.2.\nAcknowledgements\nWe thank Ronan Collobert, Tomas Mikolov, Alexey\nSpiridinov, Rob Fergus, Florent Perronnin, L\u00b4eon Bottou and\nthe rest of the FAIR team for code support and helpful dis-\ncussions.\nReferences\n[1] L.A. Adamic and B.A. Huberman. Zipf\u2019s law and the\ninternet. Glottometrics, 3:143\u2013150, 2002. 2, 3\n[2] Z. Akata, F. Perronnin, Z. Harchaoui, and C. Schmid.\nGood practice in large-scale learning for image classi-\n\ufb01cation. IEEE Transactions on Pattern Analysis and\nMachine Intelligence, 36(3):507\u2013520, 2014. 3, 8\n[3] S. Antol, A. Agrawal, J. Lu, M. Mitchell, D. Batra,\nC.L. Zitnick, and D. Parikh. VQA: Visual question\nanswering, 2015. 8\n[4] E. Bruni, G. Boleda, M. Baroni, and N.K. Tran. Dis-\ntributional semantics in technicolor. In Proceedings of\nthe Annual Meeting of the Association for Computa-\ntional Linguistics, pages 136\u2013145, 2012. 7\n[5] K. Chat\ufb01eld, V. Lempitsky, A. Vedaldi, and A. Zisser-\nman. The devil is in the details: an evaluation of re-\ncent feature encoding methods. In BMVC, volume 2,\npage 8, 2011. 6\n[6] X. Chen and A. Gupta. Webly supervised learning of\nconvolutional networks. In Proceedings of the Inter-\nnational Conference on Computer Vision, 2015. 2\n[7] J. Deng, W. Dong, R. Socher, L. J. Li, K. Li, and\nL. Fei-Fei. Imagenet: A large-scale hierarchical im-\nage database. In Computer Vision and Pattern Recog-\nnition, (CVPR), 2009. 1\n[8] E. Denton, J. Weston, M. Paluri, L. Bourdev, and\nR. Fergus. User conditional hashtag prediction for im-\nages. In Proceedings of the SIGKDD Conference on\nKnowledge Discovery and Data Mining, 2015. 2\n[9] J.J. DiCarlo, D. Zoccolan, and N.C. Rust NC. How\ndoes the brain solve visual object recognition? Neu-\nron, 73(3):415\u2013434, 2012. 1\n[10] S. K Divvala, A. Farhadi, and C. Guestrin.\nLearn-\ning everything about anything: Webly-supervised vi-\nsual concept learning. In Computer Vision and Pattern\nRecognition (CVPR), 2014. 2\n[11] M. Everingham, S.M.A. Eslami, L. Van Gool, C.K.I.\nWilliams, J. Winn, and A. Zisserman. The pascal vi-\nsual object classes challenge \u2014 a retrospective. Inter-\nnational Journal on Computer Vision, 111(1):98\u2013136,\n2015. 5\n[12] J. Fan, Y. Shen, N. Zhou, and Y. Gao.\nHarvesting\nlarge-scale weakly tagged image databases from the\nweb. In Proceedings of the IEEE Conference on Com-\nputer Vision and Pattern Recognition, pages 802\u2013809,\n2010. 2\n[13] Ali Farhadi, Mohsen Hejrati, Mohammad Amin\nSadeghi, Peter Young, Cyrus Rashtchian, Julia Hock-\nenmaier, and David Forsyth.\nEvery picture tells a\nstory: Generating sentences from images. In ECCV,\npages 15\u201329. Springer, 2010. 2\n[14] A. Frome, G. Corrado, J. Shlens, S. Bengio, J. Dean,\nand T. Mikolov. Devise: A deep visual-semantic em-\nbedding model. In Advances in Neural Information\nProcessing Systems, pages 2121\u20132129, 2013. 2\n[15] R. Girshick, J. Donahue, T. Darrell, and J. Malik.\nRich feature hierarchies for accurate object detection\nand semantic segmentation. In Computer Vision and\nPattern Recognition (CVPR), pages 580\u2013587. IEEE,\n2014. 1\n[16] Y. Gong, Q. Ke, M. Isard, and S. Lazebnik. A multi-\nview embedding space for modeling internet images,\ntags, and their semantics.\nInternational journal of\ncomputer vision, 106(2):210\u2013233, 2014. 2\n[17] A. Gupta, A. Kembhavi, and L.S. Davis. Observing\nhuman-object interactions: Using spatial and func-\ntional compatibility for recognition. IEEE Transac-\ntions on Pattern Analysis and Machine Intelligence,\n31(10), 2009. 5\n[18] M. Hodosh, P. Young, and J. Hockenmaier. Framing\nimage description as a ranking task: Data, models and\nevaluation metrics. Journal of Arti\ufb01cial Intelligence\nResearch, pages 853\u2013899, 2013. 2\n[19] A.G. Howard. Some improvements on deep convolu-\ntional neural network based image classi\ufb01cation. In\narXiv 1312.5402, 2013. 4\n[20] H. Izadinia, B.C. Russell, A. Farhadi, M.D. Hoffman,\nand A. Hertzmann. Deep classi\ufb01ers from image tags\nin the wild. In Proceedings of the 2015 Workshop on\nCommunity-Organized Multimodal Mining: Opportu-\nnities for Novel Solutions, pages 13\u201318. ACM, 2015.\n2\n[21] M. Jaderberg, K. Simonyan, A. Zisserman, and\nK. Kavukcuoglu.\nSpatial transformer networks.\nIn\narXiv 1506.02025, 2015. 1\n[22] Y. Jia, M. Salzmann, and T. Darrell. Learning cross-\nmodality similarity for multinomial data.\nIn ICCV,\npages 2407\u20132414. IEEE, 2011. 2\n[23] A. Karpathy, A. Joulin, and L. Fei Fei. Deep fragment\nembeddings for bidirectional image sentence map-\nping. In Advances in neural information processing\nsystems, pages 1889\u20131897, 2014. 2\n[24] D. Kiela and L. Bottou.\nLearning image embed-\ndings using convolutional neural networks for im-\nproved multi-modal semantics. In Proceedings of the\nConference on Empirical Methods in Natural Lan-\nguage Processing, 2014. 7\n[25] R. Kiros, R. Salakhutdinov, and R. Zemel. Multimodal\nneural language models. In Proceedings of the 31st In-\nternational Conference on Machine Learning (ICML-\n14), pages 595\u2013603, 2014. 2\n[26] A. Krizhevsky, I. Sutskever, and G.E. Hinton.\nIm-\nagenet classi\ufb01cation with deep convolutional neural\nnetworks. In Advances in Neural Information Process-\ning Systems, 2012. 1, 3\n[27] L.-J. Li and L. Fei-Fei.\nOptimol: automatic online\npicture collection via incremental model learning. In-\nternation Journal of Computer Vision, 2010. 2\n[28] Q. Li, J. Wu, and Z. Tu. Harvesting mid-level visual\nconcepts from large-scale internet images. In Proceed-\nings of the IEEE Conference on Computer Vision and\nPattern Recognition, 2013. 2\n[29] T. Lin, M. Maire, S. Belongie, J. Hays, Pves.ietro Per-\nona, D. Ramanan, P. Doll\u00b4ar, and L. Zitnick. Microsoft\ncoco: Common objects in context. In ECCV 2014,\n2014. 1\n[30] M. Meeker. Internet trends 2014. Technical report,\nKleiner, Perkins, Cau\ufb01eld & Byers, 2014. 2\n[31] T. Mikolov, K. Chen, G. Corrado, and J. Dean. Ef-\n\ufb01cient estimation of word representations in vector\nspace. In arXiv 1301.3781, 2013. 3, 6, 7, 8\n[32] K. Ni, R. Pearce, E. Wang, K. Boakye, B. Van Essen,\nD. Borth, and B. Chen. Large-scale deep learning on\nthe yfcc100m dataset. In arXiv 1502.03409, 2015. 2\n[33] M.-E. Nilsback and A. Zisserman. Automated \ufb02ower\nclassi\ufb01cation over a large number of classes. In Pro-\nceedings of the Indian Conference on Computer Vi-\nsion, Graphics and Image Processing, 2008. 5\n[34] M. Oquab, L. Bottou, I. Laptev, and J. Sivic. Learning\nand transferring mid-level image representations using\nconvolutional neural networks.\nIn Computer Vision\nand Pattern Recognition (CVPR), 2014 IEEE Confer-\nence on, pages 1717\u20131724. IEEE, 2014. 6\n[35] V. Ordonez, G. Kulkarni, and T.L. Berg.\nIm2text:\nDescribing images using 1 million captioned pho-\ntographs. In Advances in Neural Information Process-\ning Systems, pages 1143\u20131151, 2011. 2\n[36] P. Pinheiro, R. Collobert, and P. Doll\u00b4ar. Learning to\nsegment object candidates. In Advances in Neural Im-\nage Processing, 2016. 1\n[37] J. Ponce, T.L. Berg, M. Everingham, D.A. Forsyth,\nM. Hebert, S. Lazebnik, M. Marszalek, C. Schmid,\nB.C. Russell, A. Torralba, C.K.I. Williams, J. Zhang,\nand A. Zisserman. Dataset issues in object recogni-\ntion.\nIn Lecture Notes in Computer Science 4170,\npages 29\u201348, 2006. 1\n[38] A. Quattoni and A.Torralba.\nRecognizing indoor\nscenes. In IEEE Conference on Computer Vision and\nPattern Recognition, 2009. 5\n[39] A. Razavian, H. Azizpour, J. Sullivan, and S. Carls-\nson. Cnn features off-the-shelf: an astounding base-\nline for recognition.\nIn Computer Vision and Pat-\ntern Recognition Workshops (CVPRW), pages 512\u2013\n519. IEEE, 2014. 1, 6\n[40] A. Sharif Razavian, H. Azizpour, J. Sullivan, and\nS. Carlsson. CNN features off-the-shelf: an astound-\ning baseline for recognition.\nIn arXiv 1403.6382,\n2014. 5\n[41] M. Rubinstein, A. Joulin, J. Kopf, and C. Liu. Unsu-\npervised joint object discovery and segmentation in in-\nternet images. In Computer Vision and Pattern Recog-\nnition (CVPR), 2013. 2\n[42] O.\nRussakovsky,\nJ.\nDeng,\nH.\nSu,\nJ.\nKrause,\nS. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla,\nM. Bernstein, A.C. Berg, and L. Fei-Fei. Imagenet\nlarge scale visual recognition challenge. International\nJournal of Computer Vision, pages 1\u201342, 2015. 3, 5\n[43] K. Simonyan and A. Zisserman.\nVery deep convo-\nlutional networks for large-scale image recognition.\nIn Proceedings of the International Conference on\nLearning Representations, 2015. 1\n[44] R. Socher, M. Ganjoo, C.D. Manning, and A. Ng.\nZero-shot learning through cross-modal transfer. In\nAdvances in Neural Information Processing Systems,\npages 935\u2013943, 2013. 2\n[45] N. Srivastava and R. Salakhutdinov.\nMultimodal\nlearning with deep boltzmann machines.\nIn Ad-\nvances in neural information processing systems,\npages 2222\u20132230, 2012. 2\n[46] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed,\nD. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabi-\nnovich. Going deeper with convolutions. In Proceed-\nings of the IEEE Conference on Computer Vision and\nPattern Recognition, 2015. 1, 3\n[47] B. Thomee, D.A. Shamma, G. Friedland, B. Elizalde,\nK. Ni, D. Poland, D. Borth, and L.-J. Li. The new data\nand new challenges in multimedia research. In arXiv\n1503.01817, 2015. 1, 2\n[48] A. Torralba and A.A. Efros. Unbiased look at dataset\nbias. In Proceedings of the IEEE Conference on Com-\nputer Vision and Pattern Recognition, pages 1521\u2013\n1528, 2011. 1\n[49] L. Torresani, M. Szummer, and A. Fitzgibbon.\nEf-\n\ufb01cient object category recognition using classemes.\nIn Proceedings of the European Conference on Com-\nputer Vision, 2010. 2\n[50] N. Usunier, D. Buffoni, and P. Gallinari.\nRanking\nwith ordered weighted pairwise classi\ufb01cation. In Pro-\nceedings of the International Conference on Machine\nLearning, pages 1057\u20131064, 2009. 3, 8\n[51] L.J.P. van der Maaten. Accelerating t-SNE using tree-\nbased algorithms. Journal of Machine Learning Re-\nsearch, 15(Oct):3221\u20133245, 2014. 5\n[52] L.J.P. van der Maaten and G.E. Hinton. Visualizing\ndata using t-SNE. Journal of Machine Learning Re-\nsearch, 9(Nov):2579\u20132605, 2008. 5\n[53] J. Weston, S. Bengio, and N. Usunier. Wsabie: Scaling\nup to large vocabulary image annotation. In Proceed-\nings of the International Joint Conference on Arti\ufb01cial\nIntelligence, 2011. 2, 3, 8\n[54] Y. Xia, X. Cao, F. Wen, and J. Sun. Well begun is half\ndone: Generating high-quality seeds for automatic im-\nage dataset construction from web. In Proceedings of\nthe European Conference on Computer Vision, 2014.\n2\n[55] J. Xiao, J. Hays, K. Ehinger, A. Oliva, and A. Tor-\nralba.\nSun database: Large-scale scene recognition\nfrom abbey to zoo. In Proceedings of the IEEE Con-\nference on Computer Vision and Pattern Recognition,\n2010. 5\n[56] Y. Yang, C. Teo, H. Daum\u00b4e III, and Y. Aloimonos.\nCorpus-guided sentence generation of natural images.\nIn Proceedings of the Conference on Empirical Meth-\nods in Natural Language Processing, pages 444\u2013454.\nAssociation for Computational Linguistics, 2011. 2\n[57] B. Yao, X. Jiang, A. Khosla, A.L. Lin, L.J. Guibas,\nand L. Fei-Fei. Human action recognition by learning\nbases of action attributes and parts. In International\nConference on Computer Vision, 2011. 5\n[58] L. Yu, E. Park, A.C. Berg, and T.L. Berg.\nVisual\nmadlibs: Fill in the blank description generation and\nquestion answering. In Proceedings of the Interna-\ntional Conference on Computer Vision, 2015. 8\n[59] B. Zhou, V. Jagadeesh, and R. Piramuthu.\nCon-\nceptlearner: Discovering visual concepts from weakly\nlabeled image collections. arXiv:1411.5328, 2014. 2\n[60] B. Zhou, A. Lapedriza, J. Xiao, A. Torralba, and\nA. Oliva. Learning deep features for scene recogni-\ntion using places database. In Advances in Neural In-\nformation Processing Systems, pages 487\u2013495, 2014.\n6\n",
        "sentence": " , 2014) or weaklysupervised data (Joulin et al., 2016). , 2004; Bach & Harchaoui, 2007; Krause et al., 2010; Joulin & Bach, 2012). In particular, Bach & Harchaoui (2007) shows that the ridge regression loss could be use to learn discriminative clusters.",
        "context": "Several studies have used weakly supervised data in\nimage-recognition pipelines that use pre-de\ufb01ned visual fea-\ntures.\nIn particular, Li and Fei-Fei [27] present a hier-\narchical topic model that performs simultaneous dataset\nwithout full supervision. Moreover, our experiments reveal\nsome bene\ufb01ts of training on weakly supervised data such as\nthe Flickr dataset: our models learn word embeddings that\nare both grounded in vision and capture important seman-\nstudy the training of a full image-recognition pipeline; our\nresults suggest that \u201clabel cleansing\u201d may not be necessary\nto learn good visual features if the amount of weakly super-\nvised training data is suf\ufb01ciently large."
    },
    {
        "title": "Auto-encoding variational bayes",
        "author": [
            "D. Kingma",
            "M. Welling"
        ],
        "venue": "arXiv preprint arXiv:1312.6114,",
        "citeRegEx": "Kingma and Welling,? \\Q2013\\E",
        "shortCiteRegEx": "Kingma and Welling",
        "year": 2013,
        "abstract": "This paper employs the Auto-Encoding Variational Bayes (AEVB) estimator based on Stochastic Gradient Variational Bayes (SGVB), designed to optimize recognition models for challenging posterior distributions and large-scale datasets. It has been applied to the mnist dataset and extended to form a Dynamic Bayesian Network (DBN) in the context of time series. The paper delves into Bayesian inference, variational methods, and the fusion of Variational Autoencoders (VAEs) and variational techniques. Emphasis is placed on reparameterization for achieving efficient optimization. AEVB employs VAEs as an approximation for intricate posterior distributions.",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "Self-organized formation of topologically correct feature maps",
        "author": [
            "T. Kohonen"
        ],
        "venue": "Biological cybernetics,",
        "citeRegEx": "Kohonen,? \\Q1982\\E",
        "shortCiteRegEx": "Kohonen",
        "year": 1982,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " Similar to self-organizing maps (Kohonen, 1982; Martinetz & Schulten, 1991), we map deep features to a set of predefined representations in a low dimensional space. This family of unsupervised methods aims at learning a low dimensional representation of the data that preserves certain topological properties (Kohonen, 1982; Vesanto & Alhoniemi, 2000).",
        "context": null
    },
    {
        "title": "Data-dependent initializations of convolutional neural networks",
        "author": [
            "Kr\u00e4henb\u00fchl",
            "Philipp",
            "Doersch",
            "Carl",
            "Donahue",
            "Jeff",
            "Darrell",
            "Trevor"
        ],
        "venue": "arXiv preprint arXiv:1511.06856,",
        "citeRegEx": "Kr\u00e4henb\u00fchl et al\\.,? \\Q2015\\E",
        "shortCiteRegEx": "Kr\u00e4henb\u00fchl et al\\.",
        "year": 2015,
        "abstract": "",
        "full_text": "",
        "sentence": " We get rid of batch normalization layers and use a data-dependent rescaling of the parameters (Kr\u00e4henb\u00fchl et al., 2015).",
        "context": null
    },
    {
        "title": "Discriminative clustering by regularized information maximization",
        "author": [
            "A. Krause",
            "P. Perona",
            "R.G. Gomes"
        ],
        "venue": "In NIPS,",
        "citeRegEx": "Krause et al\\.,? \\Q2010\\E",
        "shortCiteRegEx": "Krause et al\\.",
        "year": 2010,
        "abstract": "",
        "full_text": "",
        "sentence": " Many methods have been proposed to use discriminative losses for clustering (Xu et al., 2004; Bach & Harchaoui, 2007; Krause et al., 2010; Joulin & Bach, 2012).",
        "context": null
    },
    {
        "title": "Imagenet classification with deep convolutional neural networks",
        "author": [
            "A. Krizhevsky",
            "I. Sutskever",
            "G. Hinton"
        ],
        "venue": "In NIPS,",
        "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E",
        "shortCiteRegEx": "Krizhevsky et al\\.",
        "year": 2012,
        "abstract": "We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5% and 17.0%, respectively, which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully connected layers we employed a recently developed regularization method called \"dropout\" that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3%, compared to 26.2% achieved by the second-best entry.",
        "full_text": "",
        "sentence": " , 1989) have pushed the limits of computer vision (Krizhevsky et al., 2012; He et al., 2016), leading to important progress in a variety of tasks, like object detection (Girshick, 2015) or image segmentation (Pinheiro et al. We show the potential of our approach by training end-to-end on ImageNet a standard architecture, namely AlexNet (Krizhevsky et al., 2012) with no supervision. We use the same multi-layer perceptron (MLP) as in Krizhevsky et al. (2012) for the classifier. In addition to this preprocessing, we also perform all the standard image transformations that are commonly applied in the supervised setting (Krizhevsky et al., 2012), such as random cropping and flipping of images. In addition to fully supervised approaches (Krizhevsky et al., 2012), we compare our method to several unsupervised approaches, i.",
        "context": null
    },
    {
        "title": "The hungarian method for the assignment problem",
        "author": [
            "H.W. Kuhn"
        ],
        "venue": "Naval research logistics quarterly,",
        "citeRegEx": "Kuhn,? \\Q1955\\E",
        "shortCiteRegEx": "Kuhn",
        "year": 1955,
        "abstract": "AbstractThis paper has been presented with  the Best Paper Award. It will appear in\nprint in Volume 52, No. 1, February  2005.",
        "full_text": "",
        "sentence": " (5) to this set, the linear assignment problem in P can be solved exactly with the Hungarian algorithm (Kuhn, 1955), but at the prohibitive cost of O(n).",
        "context": null
    },
    {
        "title": "Handwritten digit recognition with a back-propagation network",
        "author": [
            "Y. LeCun",
            "B. Boser",
            "J.S. Denker",
            "D. Henderson",
            "R.E. Howard",
            "W. Hubbard",
            "L.D. Jackel"
        ],
        "venue": "In NIPS,",
        "citeRegEx": "LeCun et al\\.,? \\Q1989\\E",
        "shortCiteRegEx": "LeCun et al\\.",
        "year": 1989,
        "abstract": "",
        "full_text": "",
        "sentence": " In recent years, convolutional neural networks, or convnets (Fukushima, 1980; LeCun et al., 1989) have pushed the limits of computer vision (Krizhevsky et al.",
        "context": null
    },
    {
        "title": "Learning deep parsimonious representations",
        "author": [
            "R. Liao",
            "A. Schwing",
            "R. Zemel",
            "R. Urtasun"
        ],
        "venue": "In NIPS,",
        "citeRegEx": "Liao et al\\.,? \\Q2016\\E",
        "shortCiteRegEx": "Liao et al\\.",
        "year": 2016,
        "abstract": "",
        "full_text": "",
        "sentence": " , 2014) and clustering (Yang et al., 2016; Liao et al., 2016), but they are hard to scale and have only been tested on small datasets. Some of them are based on a clustering loss (Xie et al., 2016; Yang et al., 2016; Liao et al., 2016), but they are not tested at a scale comparable to that of supervised convnet training.",
        "context": null
    },
    {
        "title": "Least squares quantization in pcm",
        "author": [
            "S. Lloyd"
        ],
        "venue": "Transactions on information theory,",
        "citeRegEx": "Lloyd,? \\Q1982\\E",
        "shortCiteRegEx": "Lloyd",
        "year": 1982,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " Our approach also shares some similarities with standard clustering approches like k-means (Lloyd, 1982) or discriminative clustering (Bach & Harchaoui, 2007).",
        "context": null
    },
    {
        "title": "Object recognition from local scale-invariant features",
        "author": [
            "D. Lowe"
        ],
        "venue": "In ICCV,",
        "citeRegEx": "Lowe,? \\Q1999\\E",
        "shortCiteRegEx": "Lowe",
        "year": 1999,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " Using this preprocessing is not surprising since most hand-made features like SIFT or HoG are based on image gradients (Lowe, 1999; Dalal & Triggs, 2005).",
        "context": null
    },
    {
        "title": "Convolutional kernel networks",
        "author": [
            "J. Mairal",
            "P. Koniusz",
            "Z. Harchaoui",
            "C. Schmid"
        ],
        "venue": "In NIPS,",
        "citeRegEx": "Mairal et al\\.,? \\Q2014\\E",
        "shortCiteRegEx": "Mairal et al\\.",
        "year": 2014,
        "abstract": "An important goal in visual recognition is to devise image representations\nthat are invariant to particular transformations. In this paper, we address\nthis goal with a new type of convolutional neural network (CNN) whose\ninvariance is encoded by a reproducing kernel. Unlike traditional approaches\nwhere neural networks are learned either to represent data or for solving a\nclassification task, our network learns to approximate the kernel feature map\non training data. Such an approach enjoys several benefits over classical ones.\nFirst, by teaching CNNs to be invariant, we obtain simple network architectures\nthat achieve a similar accuracy to more complex ones, while being easy to train\nand robust to overfitting. Second, we bridge a gap between the neural network\nliterature and kernels, which are natural tools to model invariance. We\nevaluate our methodology on visual recognition tasks where CNNs have proven to\nperform well, e.g., digit recognition with the MNIST dataset, and the more\nchallenging CIFAR-10 and STL-10 datasets, where our accuracy is competitive\nwith the state of the art.",
        "full_text": "arXiv:1406.3332v2  [cs.CV]  14 Nov 2014\nConvolutional Kernel Networks\nJulien Mairal, Piotr Koniusz, Zaid Harchaoui, and Cordelia Schmid\nInria\u2217\nfirstname.lastname@inria.fr\nAbstract\nAn important goal in visual recognition is to devise image representations that are\ninvariant to particular transformations. In this paper, we address this goal with a\nnew type of convolutional neural network (CNN) whose invariance is encoded by\na reproducing kernel. Unlike traditional approaches where neural networks are\nlearned either to represent data or for solving a classi\ufb01cation task, our network\nlearns to approximate the kernel feature map on training data.\nSuch an approach enjoys several bene\ufb01ts over classical ones. First, by teach-\ning CNNs to be invariant, we obtain simple network architectures that achieve a\nsimilar accuracy to more complex ones, while being easy to train and robust to\nover\ufb01tting. Second, we bridge a gap between the neural network literature and\nkernels, which are natural tools to model invariance. We evaluate our methodol-\nogy on visual recognition tasks where CNNs have proven to perform well, e.g.,\ndigit recognition with the MNIST dataset, and the more challenging CIFAR-10\nand STL-10 datasets, where our accuracy is competitive with the state of the art.\n1\nIntroduction\nWe have recently seen a revival of attention given to convolutional neural networks (CNNs) [22]\ndue to their high performance for large-scale visual recognition tasks [15, 21, 30]. The architecture\nof CNNs is relatively simple and consists of successive layers organized in a hierarchical fashion;\neach layer involves convolutions with learned \ufb01lters followed by a pointwise non-linearity and a\ndownsampling operation called \u201cfeature pooling\u201d. The resulting image representation has been em-\npirically observed to be invariant to image perturbations and to encode complex visual patterns [33],\nwhich are useful properties for visual recognition. Training CNNs remains however dif\ufb01cult since\nhigh-capacity networks may involve billions of parameters to learn, which requires both high com-\nputational power, e.g., GPUs, and appropriate regularization techniques [18, 21, 30].\nThe exact nature of invariance that CNNs exhibit is also not precisely understood. Only recently, the\ninvariance of related architectures has been characterized; this is the case for the wavelet scattering\ntransform [8] or the hierarchical models of [7]. Our work revisits convolutional neural networks,\nbut we adopt a signi\ufb01cantly different approach than the traditional one. Indeed, we use kernels [26],\nwhich are natural tools to model invariance [14]. Inspired by the hierarchical kernel descriptors\nof [2], we propose a reproducing kernel that produces multi-layer image representations.\nOur main contribution is an approximation scheme called convolutional kernel network (CKN) to\nmake the kernel approach computationally feasible. Our approach is a new type of unsupervised\nconvolutional neural network that is trained to approximate the kernel map. Interestingly, our net-\nwork uses non-linear functions that resemble recti\ufb01ed linear units [1, 30], even though they were not\nhandcrafted and naturally emerge from an approximation scheme of the Gaussian kernel map.\nBy bridging a gap between kernel methods and neural networks, we believe that we are opening\na fruitful research direction for the future. Our network is learned without supervision since the\n\u2217LEAR team, Inria Grenoble, Laboratoire Jean Kuntzmann, CNRS, Univ. Grenoble Alpes, France.\n1\nlabel information is only used subsequently in a support vector machine (SVM). Yet, we achieve\ncompetitive results on several datasets such as MNIST [22], CIFAR-10 [20] and STL-10 [13] with\nsimple architectures, few parameters to learn, and no data augmentation. Open-source code for\nlearning our convolutional kernel networks is available on the \ufb01rst author\u2019s webpage.\n1.1\nRelated Work\nThere have been several attempts to build kernel-based methods that mimic deep neural networks;\nwe only review here the ones that are most related to our approach.\nArc-cosine kernels.\nKernels for building deep large-margin classi\ufb01ers have been introduced\nin [10]. The multilayer arc-cosine kernel is built by successive kernel compositions, and each layer\nrelies on an integral representation. Similarly, our kernels rely on an integral representation, and\nenjoy a multilayer construction. However, in contrast to arc-cosine kernels: (i) we build our se-\nquence of kernels by convolutions, using local information over spatial neighborhoods (as opposed\nto compositions, using global information); (ii) we propose a new training procedure for learning a\ncompact representation of the kernel in a data-dependent manner.\nMultilayer derived kernels.\nKernels with invariance properties for visual recognition have been\nproposed in [7]. Such kernels are built with a parameterized \u201cneural response\u201d function, which con-\nsists in computing the maximal response of a base kernel over a local neighborhood. Multiple layers\nare then built by iteratively renormalizing the response kernels and pooling using neural response\nfunctions. Learning is performed by plugging the obtained kernel in an SVM. In contrast to [7], we\npropagate information up, from lower to upper layers, by using sequences of convolutions. Further-\nmore, we propose a simple and effective data-dependent way to learn a compact representation of\nour kernels and show that we obtain near state-of-the-art performance on several benchmarks.\nHierarchical kernel descriptors.\nThe kernels proposed in [2, 3] produce multilayer image repre-\nsentations for visual recognition tasks. We discuss in details these kernels in the next section: our\npaper generalizes them and establishes a strong link with convolutional neural networks.\n2\nConvolutional Multilayer Kernels\nThe convolutional multilayer kernel is a generalization of the hierarchical kernel descriptors intro-\nduced in computer vision [2, 3]. The kernel produces a sequence of image representations that are\nbuilt on top of each other in a multilayer fashion. Each layer can be interpreted as a non-linear trans-\nformation of the previous one with additional spatial invariance. We call these layers image feature\nmaps1, and formally de\ufb01ne them as follows:\nDe\ufb01nition 1. An image feature map \u03d5 is a function \u03d5 : \u2126\u2192H, where \u2126is a (usually discrete)\nsubset of [0, 1]d representing normalized \u201ccoordinates\u201d in the image and H is a Hilbert space.\nFor all practical examples in this paper, \u2126is a two-dimensional grid and corresponds to different\nlocations in a two-dimensional image. In other words, \u2126is a set of pixel coordinates. Given z\nin \u2126, the point \u03d5(z) represents some characteristics of the image at location z, or in a neighborhood\nof z. For instance, a color image of size m \u00d7 n with three channels, red, green, and blue, may be\nrepresented by an initial feature map \u03d50 : \u21260 \u2192H0, where \u21260 is an m \u00d7 n regular grid, H0 is the\nEuclidean space R3, and \u03d50 provides the color pixel values. With the multilayer scheme, non-trivial\nfeature maps will be obtained subsequently, which will encode more complex image characteristics.\nWith this terminology in hand, we now introduce the convolutional kernel, \ufb01rst, for a single layer.\nDe\ufb01nition 2 (Convolutional Kernel with Single Layer). Let us consider two images represented\nby two image feature maps, respectively \u03d5 and \u03d5\u2032 : \u2126\u2192H, where \u2126is a set of pixel locations,\nand H is a Hilbert space. The one-layer convolutional kernel between \u03d5 and \u03d5\u2032 is de\ufb01ned as\nK(\u03d5, \u03d5\u2032) :=\nX\nz\u2208\u2126\nX\nz\u2032\u2208\u2126\n\u2225\u03d5(z)\u2225H \u2225\u03d5\u2032(z\u2032)\u2225H e\u2212\n1\n2\u03b22 \u2225z\u2212z\u2032\u2225\n2\n2e\u2212\n1\n2\u03c32 \u2225\u02dc\u03d5(z)\u2212\u02dc\u03d5\u2032(z\u2032)\u2225\n2\nH,\n(1)\n1In the kernel literature, \u201cfeature map\u201d denotes the mapping between data points and their representation in\na reproducing kernel Hilbert space (RKHS) [26]. Here, feature maps refer to spatial maps representing local\nimage characteristics at everly location, as usual in the neural network literature [22].\n2\nwhere \u03b2 and \u03c3 are smoothing parameters of Gaussian kernels, and \u02dc\u03d5(z) := (1/ \u2225\u03d5(z)\u2225H) \u03d5(z)\nif \u03d5(z) \u0338= 0 and \u02dc\u03d5(z) = 0 otherwise. Similarly, \u02dc\u03d5\u2032(z\u2032) is a normalized version of \u03d5\u2032(z\u2032).2\nIt is easy to show that the kernel K is positive de\ufb01nite (see Appendix A). It consists of a sum of\npairwise comparisons between the image features \u03d5(z) and \u03d5\u2032(z\u2032) computed at all spatial locations z\nand z\u2032 in \u2126. To be signi\ufb01cant in the sum, a comparison needs the corresponding z and z\u2032 to be\nclose in \u2126, and the normalized features \u02dc\u03d5(z) and \u02dc\u03d5\u2032(z\u2032) to be close in the feature space H. The\nparameters \u03b2 and \u03c3 respectively control these two de\ufb01nitions of \u201ccloseness\u201d. Indeed, when \u03b2 is\nlarge, the kernel K is invariant to the positions z and z\u2032 but when \u03b2 is small, only features placed\nat the same location z = z\u2032 are compared to each other. Therefore, the role of \u03b2 is to control how\nmuch the kernel is locally shift-invariant. Next, we will show how to go beyond one single layer,\nbut before that, we present concrete examples of simple input feature maps \u03d50 : \u21260 \u2192H0.\nGradient map.\nAssume that H0 =R2 and that \u03d50(z) provides the two-dimensional gradient of the\nimage at pixel z, which is often computed with \ufb01rst-order differences along each dimension. Then,\nthe quantity \u2225\u03d50(z)\u2225H0 is the gradient intensity, and \u02dc\u03d50(z) is its orientation, which can be charac-\nterized by a particular angle\u2014that is, there exists \u03b8 in [0; 2\u03c0] such that \u02dc\u03d50(z) = [cos(\u03b8), sin(\u03b8)]. The\nresulting kernel K is exactly the kernel descriptor introduced in [2, 3] for natural image patches.\nPatch map.\nIn that setting, \u03d50 associates to a location z an image patch of size m \u00d7 m centered\nat z. Then, the space H0 is simply Rm\u00d7m, and \u02dc\u03d50(z) is a contrast-normalized version of the patch,\nwhich is a useful transformation for visual recognition according to classical \ufb01ndings in computer\nvision [19]. When the image is encoded with three color channels, patches are of size m \u00d7 m \u00d7 3.\nWe now de\ufb01ne the multilayer convolutional kernel, generalizing some ideas of [2].\nDe\ufb01nition 3 (Multilayer Convolutional Kernel). Let us consider a set \u2126k\u20131 \u2286[0, 1]d and a Hilbert\nspace Hk\u20131. We build a new set \u2126k and a new Hilbert space Hk as follows:\n(i) choose a patch shape Pk de\ufb01ned as a bounded symmetric subset of [\u22121, 1]d, and a set of coor-\ndinates \u2126k such that for all location zk in \u2126k, the patch {zk} + Pk is a subset of \u2126k\u20131;3 In other\nwords, each coordinate zk in \u2126k corresponds to a valid patch in \u2126k\u20131 centered at zk.\n(ii) de\ufb01ne the convolutional kernel Kk on the \u201cpatch\u201d feature maps Pk \u2192Hk\u20131, by replacing\nin (1): \u2126by Pk, H by Hk\u20131, and \u03c3, \u03b2 by appropriate smoothing parameters \u03c3k, \u03b2k. We denote\nby Hk the Hilbert space for which the positive de\ufb01nite kernel Kk is reproducing.\nAn image represented by a feature map \u03d5k\u20131 : \u2126k\u20131 \u2192Hk\u20131 at layer k\u20131 is now encoded in the k-th\nlayer as \u03d5k : \u2126k \u2192Hk, where for all zk in \u2126k, \u03d5k(zk) is the representation in Hk of the patch\nfeature map z 7\u2192\u03d5k\u20131(zk + z) for z in Pk.\nConcretely, the kernel Kk between two patches of \u03d5k\u20131 and \u03d5\u2032\nk\u20131 at respective locations zk and z\u2032\nk is\nX\nz\u2208Pk\nX\nz\u2032\u2208Pk\n\u2225\u03d5k\u20131(zk + z)\u2225\u2225\u03d5\u2032\nk\u20131(z\u2032\nk + z\u2032)\u2225e\n\u2212\n1\n2\u03b22\nk \u2225z\u2212z\u2032\u2225\n2\n2e\n\u2212\n1\n2\u03c32\nk \u2225\u02dc\u03d5k\u20131(zk+z)\u2212\u02dc\u03d5\u2032\nk\u20131(z\u2032\nk+z\u2032)\u2225\n2\n,\n(2)\nwhere \u2225.\u2225is the Hilbertian norm of Hk\u20131. In Figure 1(a), we illustrate the interactions between the\nsets of coordinates \u2126k, patches Pk, and feature spaces Hk across layers. For two-dimensional grids,\na typical patch shape is a square, for example P := {\u22121/n, 0, 1/n} \u00d7 {\u22121/n, 0, 1/n} for a 3 \u00d7 3\npatch in an image of size n \u00d7 n. Information encoded in the k-th layer differs from the (k\u20131)-th one\nin two aspects: \ufb01rst, each point \u03d5k(zk) in layer k contains information about several points from\nthe (k\u20131)-th layer and can possibly represent larger patterns; second, the new feature map is more\nlocally shift-invariant than the previous one due to the term involving the parameter \u03b2k in (2).\nThe multilayer convolutional kernel slightly differs from the hierarchical kernel descriptors of [2]\nbut exploits similar ideas. Bo et al. [2] de\ufb01ne indeed several ad hoc kernels for representing local\ninformation in images, such as gradient, color, or shape. These kernels are close to the one de\ufb01ned\nin (1) but with a few variations. Some of them do not use normalized features \u02dc\u03d5(z), and these kernels\nuse different weighting strategies for the summands of (1) that are specialized to the image modality,\ne.g., color, or gradient, whereas we use the same weight \u2225\u03d5(z)\u2225H \u2225\u03d5\u2032(z\u2032)\u2225H for all kernels. The\ngeneric formulation (1) that we propose may be useful per se, but our main contribution comes in\nthe next section, where we use the kernel as a new tool for learning convolutional neural networks.\n2When \u2126is not discrete, the notation P in (1) should be replaced by the Lebesgue integral\nR\nin the paper.\n3For two sets A and B, the Minkowski sum A + B is de\ufb01ned as {a + b : a \u2208A, b \u2208B}.\n3\n\u21260\n\u03d50(z0) \u2208H0\n{z1} + P1\n\u03d51(z1) \u2208H1\n\u21261\n{z2} + P2\n\u21262\n\u03d52(z2) \u2208H2\n(a) Hierarchy of image feature maps.\n\u2126\u2032\nk\u20131\n\u03bek\u20131(z)\n\u03c8k\u20131(zk\u20131)\n(patch extraction)\n{zk\u20131}+P\u2032\nk\u20131\nconvolution\n+ non-linearity\npk\n\u03b6k(zk\u20131)\n\u2126k\u20131\nGaussian \ufb01ltering\n+ downsampling\n= pooling\n\u2126\u2032\nk\n\u03bek(z)\n(b) Zoom between layer k\u20131 and k of the CKN.\nFigure 1: Left: concrete representation of the successive layers for the multilayer convolutional\nkernel. Right: one layer of the convolutional neural network that approximates the kernel.\n3\nTraining Invariant Convolutional Kernel Networks\nGeneric schemes have been proposed for approximating a non-linear kernel with a linear one, such\nas the Nystr\u00a8om method and its variants [5, 31], or random sampling techniques in the Fourier do-\nmain for shift-invariant kernels [24]. In the context of convolutional multilayer kernels, such an\napproximation is critical because computing the full kernel matrix on a database of images is com-\nputationally infeasible, even for a moderate number of images (\u224810 000) and moderate number of\nlayers. For this reason, Bo et al. [2] use the Nystr\u00a8om method for their hierarchical kernel descriptors.\nIn this section, we show that when the coordinate sets \u2126k are two-dimensional regular grids, a\nnatural approximation for the multilayer convolutional kernel consists of a sequence of spatial con-\nvolutions with learned \ufb01lters, pointwise non-linearities, and pooling operations, as illustrated in\nFigure 1(b). More precisely, our scheme approximates the kernel map of K de\ufb01ned in (1) at layer k\nby \ufb01nite-dimensional spatial maps \u03bek : \u2126\u2032\nk \u2192Rpk, where \u2126\u2032\nk is a set of coordinates related to \u2126k,\nand pk is a positive integer controlling the quality of the approximation. Consider indeed two images\nrepresented at layer k by image feature maps \u03d5k and \u03d5\u2032\nk, respectively. Then,\n(A) the corresponding maps \u03bek and \u03be\u2032\nk are learned such that K(\u03d5k\u20131, \u03d5\u2032\nk\u20131) \u2248\u27e8\u03bek, \u03be\u2032\nk\u27e9, where \u27e8., .\u27e9\nis the Euclidean inner-product acting as if \u03bek and \u03be\u2032\nk were vectors in R|\u2126\u2032\nk|pk;\n(B) the set \u2126\u2032\nk is linked to \u2126k by the relation \u2126\u2032\nk = \u2126k + P\u2032\nk where P\u2032\nk is a patch shape, and\nthe quantities \u03d5k(zk) in Hk admit \ufb01nite-dimensional approximations \u03c8k(zk) in R|P\u2032\nk|pk; as\nillustrated in Figure 1(b), \u03c8k(zk) is a patch from \u03bek centered at location zk with shape P\u2032\nk;\n(C) an activation map \u03b6k : \u2126k\u20131 7\u2192Rpk is computed from \u03bek\u20131 by convolution with pk \ufb01lters\nfollowed by a non-linearity. The subsequent map \u03bek is obtained from \u03b6k by a pooling operation.\nWe call this approximation scheme a convolutional kernel network (CKN). In comparison to CNNs,\nour approach enjoys similar bene\ufb01ts such as ef\ufb01cient prediction at test time, and involves the same\nset of hyper-parameters: number of layers, numbers of \ufb01lters pk at layer k, shape P\u2032\nk of the \ufb01lters,\nsizes of the feature maps. The other parameters \u03b2k, \u03c3k can be automatically chosen, as discussed\nlater. Training a CKN can be argued to be as simple as training a CNN in an unsupervised man-\nner [25] since we will show that the main difference is in the cost function that is optimized.\n3.1\nFast Approximation of the Gaussian Kernel\nA key component of our formulation is the Gaussian kernel. We start by approximating it by a linear\noperation with learned \ufb01lters followed by a pointwise non-linearity. Our starting point is the next\nlemma, which can be obtained after a simple calculation.\n4\nLemma 1 (Linear expansion of the Gaussian Kernel). For all x and x\u2032 in Rm, and \u03c3 > 0,\ne\u2212\n1\n2\u03c32 \u2225x\u2212x\u2032\u22252\n2 =\n\u0012 2\n\u03c0\u03c32\n\u0013 m\n2 Z\nw\u2208Rm e\u22121\n\u03c32 \u2225x\u2212w\u22252\n2e\u22121\n\u03c32 \u2225x\u2032\u2212w\u22252\n2dw.\n(3)\nThe lemma gives us a mapping of any x in Rm to the function w 7\u2192\n\u221a\nCe\u2212(1/\u03c32)\u2225x\u2212w\u22252\n2 in L2(Rm),\nwhere the kernel is linear, and C is the constant in front of the integral. To obtain a \ufb01nite-dimensional\nrepresentation, we need to approximate the integral with a weighted \ufb01nite sum, which is a classical\nproblem arising in statistics (see [29] and chapter 8 of [6]). Then, we consider two different cases.\nSmall dimension, m \u22642.\nWhen the data lives in a compact set of Rm, the integral in (3) can be\napproximated by uniform sampling over a large enough set. We choose such a strategy for two types\nof kernels from Eq. (1): (i) the spatial kernels e\n\u2212\n\u0010\n1\n2\u03b22\n\u0011\n\u2225z\u2212z\u2032\u2225\n2\n2; (ii) the terms e\u2212(\n1\n2\u03c32 )\u2225\u02dc\u03d5(z)\u2212\u02dc\u03d5\u2032(z\u2032)\u2225\n2\nH\nwhen \u03d5 is the \u201cgradient map\u201d presented in Section 2. In the latter case, H = R2 and \u02dc\u03d5(z) is the\ngradient orientation. We typically sample a few orientations as explained in Section 4.\nHigher dimensions.\nTo prevent the curse of dimensionality, we learn to approximate the kernel on\ntraining data, which is intrinsically low-dimensional. We optimize importance weights \u03b7 = [\u03b7l]p\nl=1\nin Rp\n+ and sampling points W = [wl]p\nl=1 in Rm\u00d7p on n training pairs (xi, yi)i=1,...,n in Rm \u00d7 Rm:\nmin\n\u03b7\u2208Rp\n+,W\u2208Rm\u00d7p\n\u0014 1\nn\nn\nX\ni=1\n\u0010\ne\u2212\n1\n2\u03c32 \u2225xi\u2212yi\u22252\n2 \u2212\np\nX\nl=1\n\u03b7le\u22121\n\u03c32 \u2225xi\u2212wl\u22252\n2e\u22121\n\u03c32 \u2225yi\u2212wl\u22252\n2\n\u00112\u0015\n.\n(4)\nInterestingly, we may already draw some links with neural networks. When applied to unit-norm\nvectors xi and yi, problem (4) produces sampling points wl whose norm is close to one. After\nlearning, a new unit-norm point x in Rm is mapped to the vector [\u221a\u03b7le\u2212(1/\u03c32)\u2225x\u2212wl\u22252\n2]p\nl=1 in Rp,\nwhich may be written as [f(w\u22a4\nl x)]p\nl=1, assuming that the norm of wl is always one, where f is the\nfunction u 7\u2192e(2/\u03c32)(u\u22121) for u = w\u22a4\nl x in [\u22121, 1]. Therefore, the \ufb01nite-dimensional representation\nof x only involves a linear operation followed by a non-linearity, as in typical neural networks. In\nFigure 2, we show that the shape of f resembles the \u201crecti\ufb01ed linear unit\u201d function [30].\nu\nf(u)\nf(u) = e(2/\u03c32)(u\u22121)\nf(u) = max(u, 0)\n0\n1\n-1\nFigure 2: In dotted red, we plot the \u201crecti\ufb01ed linear unit\u201d function u 7\u2192max(u, 0). In blue, we plot\nnon-linear functions of our network for typical values of \u03c3 that we use in our experiments.\n3.2\nApproximating the Multilayer Convolutional Kernel\nWe have now all the tools in hand to build our convolutional kernel network. We start by making as-\nsumptions on the input data, and then present the learning scheme and its approximation principles.\nThe zeroth layer.\nWe assume that the input data is a \ufb01nite-dimensional map \u03be0 : \u2126\u2032\n0 \u2192Rp0, and\nthat \u03d50 : \u21260 \u2192H0 \u201cextracts\u201d patches from \u03be0. Formally, there exists a patch shape P\u2032\n0 such that\n\u2126\u2032\n0 = \u21260 + P\u2032\n0, H0 = Rp0|P\u2032\n0|, and for all z0 in \u21260, \u03d50(z0) is a patch of \u03be0 centered at z0. Then,\nproperty (B) described at the beginning of Section 3 is satis\ufb01ed for k = 0 by choosing \u03c80 = \u03d50.\nThe examples of input feature maps given earlier satisfy this \ufb01nite-dimensional assumption: for the\ngradient map, \u03be0 is the gradient of the image along each direction, with p0 = 2, P\u2032\n0 = {0} is a 1\u00d71\npatch, \u21260 =\u2126\u2032\n0, and \u03d50 =\u03be0; for the patch map, \u03be0 is the input image, say with p0 =3 for RGB data.\nThe convolutional kernel network.\nThe zeroth layer being characterized, we present in Algo-\nrithms 1 and 2 the subsequent layers and how to learn their parameters in a feedforward manner. It\nis interesting to note that the input parameters of the algorithm are exactly the same as a CNN\u2014that\nis, number of layers and \ufb01lters, sizes of the patches and feature maps (obtained here via the sub-\nsampling factor). Ultimately, CNNs and CKNs only differ in the cost function that is optimized for\nlearning the \ufb01lters and in the choice of non-linearities. As we show next, there exists a link between\nthe parameters of a CKN and those of a convolutional multilayer kernel.\n5\nAlgorithm 1 Convolutional kernel network - learning the parameters of the k-th layer.\ninput \u03be1\nk\u20131, \u03be2\nk\u20131, . . . : \u2126\u2032\nk\u20131 \u2192Rpk\u20131 (sequence of (k\u20131)-th maps obtained from training images);\nP\u2032\nk\u20131 (patch shape); pk (number of \ufb01lters); n (number of training pairs);\n1: extract at random n pairs (xi, yi) of patches with shape P\u2032\nk\u20131 from the maps \u03be1\nk\u20131, \u03be2\nk\u20131, . . .;\n2: if not provided by the user, set \u03c3k to the 0.1 quantile of the data (\u2225xi \u2212yi\u22252)n\ni=1;\n3: unsupervised learning: optimize (4) to obtain the \ufb01lters Wk in R|P\u2032\nk\u20131|pk\u20131\u00d7pk and \u03b7k in Rpk;\noutput Wk, \u03b7k, and \u03c3k (smoothing parameter);\nAlgorithm 2 Convolutional kernel network - computing the k-th map form the (k\u20131)-th one.\ninput \u03bek\u20131 : \u2126\u2032\nk\u20131 \u2192Rpk\u20131 (input map); P\u2032\nk\u20131 (patch shape); \u03b3k \u22651 (subsampling factor); pk (num-\nber of \ufb01lters); \u03c3k (smoothing parameter); Wk = [wkl]pk\nl=1 and \u03b7k = [\u03b7kl]pk\nl=1 (layer parameters);\n1: convolution and non-linearity: de\ufb01ne the activation map \u03b6k : \u2126k\u20131 \u2192Rpk as\n\u03b6k : z 7\u2192\u2225\u03c8k\u20131(z)\u22252\n\u0014\u221a\u03b7kle\n\u22121\n\u03c32\nk \u2225\u02dc\n\u03c8k\u20131(z)\u2212wkl\u2225\n2\n2\n\u0015pk\nl=1\n,\n(5)\nwhere \u03c8k\u20131(z) is a vector representing a patch from \u03bek\u20131 centered at z with shape P\u2032\nk\u20131, and the\nvector \u02dc\u03c8k\u20131(z) is an \u21132-normalized version of \u03c8k\u20131(z). This operation can be interpreted as a\nspatial convolution of the map \u03bek\u20131 with the \ufb01lters wkl followed by pointwise non-linearities;\n2: set \u03b2k to be \u03b3k times the spacing between two pixels in \u2126k\u20131;\n3: feature pooling: \u2126\u2032\nk is obtained by subsampling \u2126k\u20131 by a factor \u03b3k and we de\ufb01ne a new map\n\u03bek : \u2126\u2032\nk \u2192Rpk obtained from \u03b6k by linear pooling with Gaussian weights:\n\u03bek : z 7\u2192\np\n2/\u03c0\nX\nu\u2208\u2126k\u20131\ne\n\u22121\n\u03b22\nk\n\u2225u\u2212z\u22252\n2\u03b6k(u).\n(6)\noutput \u03bek : \u2126\u2032\nk \u2192Rpk (new map);\nApproximation principles.\nWe proceed recursively to show that the kernel approximation prop-\nerty (A) is satis\ufb01ed; we assume that (B) holds at layer k\u20131, and then, we show that (A) and (B) also\nhold at layer k. This is suf\ufb01cient for our purpose since we have previously assumed (B) for the ze-\nroth layer. Given two images feature maps \u03d5k\u20131 and \u03d5\u2032\nk\u20131, we start by approximating K(\u03d5k\u20131, \u03d5\u2032\nk\u20131)\nby replacing \u03d5k\u20131(z) and \u03d5\u2032\nk\u20131(z\u2032) by their \ufb01nite-dimensional approximations provided by (B):\nK(\u03d5k\u20131, \u03d5\u2032\nk\u20131) \u2248\nX\nz,z\u2032\u2208\u2126k\u20131\n\u2225\u03c8k\u20131(z)\u22252 \u2225\u03c8\u2032\nk\u20131(z\u2032)\u22252 e\n\u2212\n1\n2\u03b22\nk \u2225z\u2212z\u2032\u2225\n2\n2e\n\u2212\n1\n2\u03c32\nk \u2225\u02dc\n\u03c8k\u20131(z)\u2212\u02dc\n\u03c8\u2032\nk\u20131(z\u2032)\u2225\n2\n2.\n(7)\nThen, we use the \ufb01nite-dimensional approximation of the Gaussian kernel involving \u03c3k and\nK(\u03d5k\u20131, \u03d5\u2032\nk\u20131) \u2248\nX\nz,z\u2032\u2208\u2126k\u20131\n\u03b6k(z)\u22a4\u03b6\u2032\nk(z\u2032)e\n\u2212\n1\n2\u03b22\nk \u2225z\u2212z\u2032\u2225\n2\n2,\n(8)\nwhere \u03b6k is de\ufb01ned in (5) and \u03b6\u2032\nk is de\ufb01ned similarly by replacing \u02dc\u03c8 by \u02dc\u03c8\u2032. Finally, we approximate\nthe remaining Gaussian kernel by uniform sampling on \u2126\u2032\nk, following Section 3.1. After exchanging\nsums and grouping appropriate terms together, we obtain the new approximation\nK(\u03d5k\u20131, \u03d5\u2032\nk\u20131) \u22482\n\u03c0\nX\nu\u2208\u2126\u2032\nk\n\u0012 X\nz\u2208\u2126k\u20131\ne\n\u22121\n\u03b22\nk\n\u2225z\u2212u\u22252\n2\u03b6k(z)\n\u0013\u22a4\u0012\nX\nz\u2032\u2208\u2126k\u20131\ne\n\u22121\n\u03b22\nk \u2225z\u2032\u2212u\u2225\n2\n2\u03b6\u2032\nk(z\u2032)\n\u0013\n,\n(9)\nwhere the constant 2/\u03c0 comes from the multiplication of the constant 2/(\u03c0\u03b22\nk) from (3) and the\nweight \u03b22\nk of uniform sampling orresponding to the square of the distance between two pixels of \u2126\u2032\nk.4\nAs a result, the right-hand side is exactly \u27e8\u03bek, \u03be\u2032\nk\u27e9, where \u03bek is de\ufb01ned in (6), giving us property (A).\nIt remains to show that property (B) also holds, speci\ufb01cally that the quantity (2) can be approximated\nby the Euclidean inner-product \u27e8\u03c8k(zk), \u03c8\u2032\nk(z\u2032\nk)\u27e9with the patches \u03c8k(zk) and \u03c8\u2032\nk(z\u2032\nk) of shape P\u2032\nk;\nwe assume for that purpose that P\u2032\nk is a subsampled version of the patch shape Pk by a factor \u03b3k.\n4The choice of \u03b2k in Algorithm 2 is driven by signal processing principles. The feature pooling step can\nindeed be interpreted as a downsampling operation that reduces the resolution of the map from \u2126k\u20131 to \u2126k by\nusing a Gaussian anti-aliasing \ufb01lter, whose role is to reduce frequencies above the Nyquist limit.\n6\nWe remark that the kernel (2) is the same as (1) applied to layer k\u20131 by replacing \u2126k\u20131 by {zk}+Pk.\nBy doing the same substitution in (9), we immediately obtain an approximation of (2). Then, all\nGaussian terms are negligible for all u and z that are far from each other\u2014say when \u2225u\u2212z\u22252 \u22652\u03b2k.\nThus, we may replace the sums P\nu\u2208\u2126\u2032\nk\nP\nz,z\u2032\u2208{zk}+Pk by P\nu\u2208{zk}+P\u2032\nk\nP\nz,z\u2032\u2208\u2126k\u20131, which has the\nsame set of \u201cnon-negligible\u201d terms. This yields exactly the approximation \u27e8\u03c8k(zk), \u03c8\u2032\nk(z\u2032\nk)\u27e9.\nOptimization.\nRegarding problem (4), stochastic gradient descent (SGD) may be used since a\npotentially in\ufb01nite amount of training data is available. However, we have preferred to use L-BFGS-\nB [9] on 300 000 pairs of randomly selected training data points, and initialize W with the K-means\nalgorithm. L-BFGS-B is a parameter-free state-of-the-art batch method, which is not as fast as SGD\nbut much easier to use. We always run the L-BFGS-B algorithm for 4 000 iterations, which seems\nto ensure convergence to a stationary point. Our goal is to demonstrate the preliminary performance\nof a new type of convolutional network, and we leave as future work any speed improvement.\n4\nExperiments\nWe now present experiments that were performed using Matlab and an L-BFGS-B solver [9] inter-\nfaced by Stephen Becker. Each image is represented by the last map \u03bek of the CKN, which is used\nin a linear SVM implemented in the software package LibLinear [16]. These representations are\ncentered, rescaled to have unit \u21132-norm on average, and the regularization parameter of the SVM is\nalways selected on a validation set or by 5-fold cross-validation in the range 2i, i = \u221215 . . ., 15.\nThe patches P\u2032\nk are typically small; we tried the sizes m \u00d7 m with m = 3, 4, 5 for the \ufb01rst\nlayer, and m = 2, 3 for the upper ones. The number of \ufb01lters pk in our experiments is in the\nset {50, 100, 200, 400, 800}. The downsampling factor \u03b3k is always chosen to be 2 between two con-\nsecutive layers, whereas the last layer is downsampled to produce \ufb01nal maps \u03bek of a small size\u2014say,\n5\u00d75 or 4\u00d74. For the gradient map \u03d50, we approximate the Gaussian kernel e(1/\u03c32\n1)\u2225\u03d50(z)\u2212\u03d5\u2032\n0(z\u2032)\u2225H0\nby uniformly sampling p1 = 12 orientations, setting \u03c31 = 2\u03c0/p1. Finally, we also use a small off-\nset \u03b5 to prevent numerical instabilities in the normalization steps \u02dc\u03c8(z) = \u03c8(z)/ max(\u2225\u03c8(z)\u22252, \u03b5).\n4.1\nDiscovering the Structure of Natural Image Patches\nUnsupervised learning was \ufb01rst used for discovering the underlying structure of natural image\npatches by Olshausen and Field [23]. Without making any a priori assumption about the data ex-\ncept a parsimony principle, the method is able to produce small prototypes that resemble Gabor\nwavelets\u2014that is, spatially localized oriented basis functions. The results were found impressive by\nthe scienti\ufb01c community and their work received substantial attention. It is also known that such\nresults can also be achieved with CNNs [25]. We show in this section that this is also the case for\nconvolutional kernel networks, even though they are not explicitly trained to reconstruct data.\nFollowing [23], we randomly select a database of 300 000 whitened natural image patches of\nsize 12 \u00d7 12 and learn p = 256 \ufb01lters W using the formulation (4). We initialize W with Gaussian\nrandom noise without performing the K-means step, in order to ensure that the output we obtain is\nnot an artifact of the initialization. In Figure 3, we display the \ufb01lters associated to the top-128 largest\nweights \u03b7l. Among the 256 \ufb01lters, 197 exhibit interpretable Gabor-like structures and the rest was\nless interpretable. To the best of our knowledge, this is the \ufb01rst time that the explicit kernel map of\nthe Gaussian kernel for whitened natural image patches is shown to be related to Gabor wavelets.\n4.2\nDigit Classi\ufb01cation on MNIST\nThe MNIST dataset [22] consists of 60 000 images of handwritten digits for training and 10 000\nfor testing. We use two types of initial maps in our networks: the \u201cpatch map\u201d, denoted by CNK-\nPM and the \u201cgradient map\u201d, denoted by CNK-GM. We follow the evaluation methodology of [25]\nFigure 3: Filters obtained by the \ufb01rst layer of the convolutional kernel network on natural images.\n7\nTr.\nCNN\nScat-1\nScat-2\nCKN-GM1\nCKN-GM2\nCKN-PM1\nCKN-PM2\n[32]\n[18]\n[19]\nsize\n[25]\n[8]\n[8]\n(12/50)\n(12/400)\n(200)\n(50/200)\n300\n7.18\n4.7\n5.6\n4.39\n4.24\n5.98\n4.15\nNA\n1K\n3.21\n2.3\n2.6\n2.60\n2.05\n3.23\n2.76\nNA\n2K\n2.53\n1.3\n1.8\n1.85\n1.51\n1.97\n2.28\nNA\n5K\n1.52\n1.03\n1.4\n1.41\n1.21\n1.41\n1.56\nNA\n10K\n0.85\n0.88\n1\n1.17\n0.88\n1.18\n1.10\nNA\n20K\n0.76\n0.79\n0.58\n0.89\n0.60\n0.83\n0.77\nNA\n40K\n0.65\n0.74\n0.53\n0.68\n0.51\n0.64\n0.58\nNA\n60K\n0.53\n0.70\n0.4\n0.58\n0.39\n0.63\n0.53\n0.47\n0.45\n0.53\nTable 1: Test error in % for various approaches on the MNIST dataset without data augmentation.\nThe numbers in parentheses represent the size p1 and p2 of the feature maps at each layer.\nfor comparison when varying the training set size. We select the regularization parameter of the\nSVM by 5-fold cross validation when the training size is smaller than 20 000, or otherwise, we\nkeep 10 0000 examples from the training set for validation. We report in Table 1 the results obtained\nfor four simple architectures. CKN-GM1 is the simplest one: its second layer uses 3\u00d73 patches and\nonly p2 = 50 \ufb01lters, resulting in a network with 5 400 parameters. Yet, it achieves an outstanding\nperformance of 0.58% error on the full dataset. The best performing, CKN-GM2, is similar to\nCKN-GM1 but uses p2 = 400 \ufb01lters. When working with raw patches, two layers (CKN-PM2)\ngives better results than one layer. More details about the network architectures are provided in the\nsupplementary material. In general, our method achieves a state-of-the-art accuracy for this task\nsince lower error rates have only been reported by using data augmentation [11].\n4.3\nVisual Recognition on CIFAR-10 and STL-10\nWe now move to the more challenging datasets CIFAR-10 [20] and STL-10 [13]. We select the\nbest architectures on a validation set of 10 000 examples from the training set for CIFAR-10, and\nby 5-fold cross-validation on STL-10. We report in Table 2 results for CKN-GM, de\ufb01ned in the\nprevious section, without exploiting color information, and CKN-PM when working on raw RGB\npatches whose mean color is subtracted. The best selected models have always two layers, with 800\n\ufb01lters for the top layer. Since CKN-PM and CKN-GM exploit a different information, we also report\na combination of such two models, CKN-CO, by concatenating normalized image representations\ntogether. The standard deviations for STL-10 was always below 0.7%. Our approach appears to\nbe competitive with the state of the art, especially on STL-10 where only one method does better\nthan ours, despite the fact that our models only use 2 layers and require learning few parameters.\nNote that better results than those reported in Table 2 have been obtained in the literature by using\neither data augmentation (around 90% on CIFAR-10 for [18, 30]), or external data (around 70% on\nSTL-10 for [28]). We are planning to investigate similar data manipulations in the future.\nMethod\n[12]\n[27]\n[18]\n[13]\n[4]\n[17]\n[32]\nCKN-GM\nCKN-PM\nCKN-CO\nCIFAR-10\n82.0\n82.2\n88.32\n79.6\nNA\n83.96\n84.87\n74.84\n78.30\n82.18\nSTL-10\n60.1\n58.7\nNA\n51.5\n64.5\n62.3\nNA\n60.04\n60.25\n62.32\nTable 2: Classi\ufb01cation accuracy in % on CIFAR-10 and STL-10 without data augmentation.\n5\nConclusion\nIn this paper, we have proposed a new methodology for combining kernels and convolutional neural\nnetworks. We show that mixing the ideas of these two concepts is fruitful, since we achieve near\nstate-of-the-art performance on several datasets such as MNIST, CIFAR-10, and STL10, with simple\narchitectures and no data augmentation. Some challenges regarding our work are left open for the\nfuture. The \ufb01rst one is the use of supervision to better approximate the kernel for the prediction task.\nThe second consists in leveraging the kernel interpretation of our convolutional neural networks to\nbetter understand the theoretical properties of the feature spaces that these networks produce.\nAcknowledgments\nThis work was partially supported by grants from ANR (project MACARON ANR-14-CE23-0003-\n01), MSR-Inria joint centre, European Research Council (project ALLEGRO), CNRS-Mastodons\nprogram (project GARGANTUA), and the LabEx PERSYVAL-Lab (ANR-11-LABX-0025).\n8\nReferences\n[1] Y. Bengio. Learning deep architectures for AI. Found. Trends Mach. Learn., 2009.\n[2] L. Bo, K. Lai, X. Ren, and D. Fox. Object recognition with hierarchical kernel descriptors. In Proc.\nCVPR, 2011.\n[3] L. Bo, X. Ren, and D. Fox. Kernel descriptors for visual recognition. In Adv. NIPS, 2010.\n[4] L. Bo, X. Ren, and D. Fox. Unsupervised feature learning for RGB-D based object recognition. In\nExperimental Robotics, 2013.\n[5] L. Bo and C. Sminchisescu. Ef\ufb01cient match kernel between sets of features for visual recognition. In Adv.\nNIPS, 2009.\n[6] L. Bottou, O. Chapelle, D. DeCoste, and J. Weston. Large-Scale Kernel Machines (Neural Information\nProcessing). The MIT Press, 2007.\n[7] J. V. Bouvrie, L. Rosasco, and T. Poggio. On invariance in hierarchical models. In Adv. NIPS, 2009.\n[8] J. Bruna and S. Mallat. Invariant scattering convolution networks. IEEE T. Pattern Anal., 35(8):1872\u2013\n1886, 2013.\n[9] R. H. Byrd, P. Lu, J. Nocedal, and C. Zhu. A limited memory algorithm for bound constrained optimiza-\ntion. SIAM J. Sci. Comput., 16(5):1190\u20131208, 1995.\n[10] Y. Cho and L. K. Saul. Large-margin classi\ufb01cation in in\ufb01nite neural networks. Neural Comput., 22(10),\n2010.\n[11] D. Ciresan, U. Meier, and J. Schmidhuber. Multi-column deep neural networks for image classi\ufb01cation.\nIn Proc. CVPR, 2012.\n[12] A. Coates and A. Y. Ng. Selecting receptive \ufb01elds in deep networks. In Adv. NIPS, 2011.\n[13] A. Coates, A. Y. Ng, and H. Lee. An analysis of single-layer networks in unsupervised feature learning.\nIn Proc. AISTATS, 2011.\n[14] D. Decoste and B. Sch\u00a8olkopf. Training invariant support vector machines. Mach. Learn., 46(1-3):161\u2013\n190, 2002.\n[15] J. Donahue, Y. Jia, O. Vinyals, J. Hoffman, N. Zhang, E. Tzeng, and T. Darrell. DeCAF: A deep convo-\nlutional activation feature for generic visual recognition. preprint arXiv:1310.1531, 2013.\n[16] R.-E. Fan, K.-W. Chang, C.-J. Hsieh, X.-R. Wang, and C.-J. Lin. LIBLINEAR: A library for large linear\nclassi\ufb01cation. J. Mach. Learn. Res., 9:1871\u20131874, 2008.\n[17] R. Gens and P. Domingos. Discriminative learning of sum-product networks. In Adv. NIPS, 2012.\n[18] I. J. Goodfellow, D. Warde-Farley, M. Mirza, A. Courville, and Y. Bengio. Maxout networks. In Proc.\nICML, 2013.\n[19] K. Jarrett, K. Kavukcuoglu, M. Ranzato, and Y. LeCun. What is the best multi-stage architecture for\nobject recognition? In Proc. ICCV, 2009.\n[20] A. Krizhevsky and G. Hinton. Learning multiple layers of features from tiny images. Tech. Rep., 2009.\n[21] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classi\ufb01cation with deep convolutional neural\nnetworks. In Adv. NIPS, 2012.\n[22] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition.\nP. IEEE, 86(11):2278\u20132324, 1998.\n[23] B. A. Olshausen and D. J. Field. Emergence of simple-cell receptive \ufb01eld properties by learning a sparse\ncode for natural images. Nature, 381(6583):607\u2013609, 1996.\n[24] A. Rahimi and B. Recht. Random features for large-scale kernel machines. In Adv. NIPS, 2007.\n[25] M. Ranzato, F.-J. Huang, Y-L. Boureau, and Y. LeCun. Unsupervised learning of invariant feature hierar-\nchies with applications to object recognition. In Proc. CVPR, 2007.\n[26] J. Shawe-Taylor and N. Cristianini. Kernel methods for pattern analysis. 2004.\n[27] K. Sohn and H. Lee. Learning invariant representations with local transformations. In Proc. ICML, 2012.\n[28] K. Swersky, J. Snoek, and R. P. Adams. Multi-task Bayesian optimization. In Adv. NIPS, 2013.\n[29] G. Wahba. Spline models for observational data. SIAM, 1990.\n[30] L. Wan, M. D. Zeiler, S. Zhang, Y. LeCun, and R. Fergus. Regularization of neural networks using\ndropconnect. In Proc. ICML, 2013.\n[31] C. Williams and M. Seeger. Using the Nystr\u00a8om method to speed up kernel machines. In Adv. NIPS, 2001.\n[32] M. D. Zeiler and R. Fergus. Stochastic pooling for regularization of deep convolutional neural networks.\nIn Proc. ICLR, 2013.\n[33] M. D. Zeiler and R. Fergus. Visualizing and understanding convolutional networks. In Proc. ECCV, 2014.\n9\nA\nPositive De\ufb01niteness of K\nTo show that the kernel K de\ufb01ned in (1) is positive de\ufb01nite (p.d.), we simply use elementary rules from the\nkernel literature described in Sections 2.3.2 and 3.4.1 of [26]. A linear combination of p.d. kernels with non-\nnegative weights is also p.d. (see Proposition 3.22 of[26]), and thus it is suf\ufb01cient to show that for all z, z\u2032 in \u2126,\nthe following kernel on \u2126\u2192H is p.d.:\n(\u03d5, \u03d5\u2032) 7\u2192\n\r\r\u03d5(z)\n\r\r\nH\n\r\r\u03d5\u2032(z\u2032)\n\r\r\nH e\u2212\n1\n2\u03c32 \u2225\u02dc\n\u03d5(z)\u2212\u02dc\u03d5\u2032(z\u2032)\u2225\n2\nH.\nSpeci\ufb01cally, it is also suf\ufb01cient to show that the following kernel on H is p.d.:\n(\u03c6, \u03c6\u2032) 7\u2192\n\r\r\u03c6\n\r\r\nH\n\r\r\u03c6\u2032\r\r\nH e\n\u2212\n1\n2\u03c32\n\r\r\r\r\n\u03c6\n\u2225\u03c6\u2225H \u2212\n\u03c6\u2032\n\u2225\u03c6\u2032\u2225H\n\r\r\r\r\n2\nH.\nwith the convention \u03c6/\u2225\u03c6\u2225H = 0 if \u03c6 = 0. This is a pointwise product of two kernels and is p.d. when\neach of the two kernels is p.d. The \ufb01rst one is obviously p.d.: (\u03c6, \u03c6\u2032) 7\u2192\u2225\u03c6\u2225H \u2225\u03c6\u2032\u2225H. The second one is a\ncomposition of the Gaussian kernel\u2014which is p.d.\u2014, with feature maps \u03c6/\u2225\u03c6\u2225H of a normalized linear kernel\nin H. This composition is p.d. according to Proposition 3.22, item (v) of [26] since the normalization does not\nremove the positive-de\ufb01niteness property.\nB\nList of Architectures Reported in the Experiments\nWe present in details the architectures used in the paper in Table 3.\nArch.\nN\nm1\np1\n\u03b31\nm2\np2\nS\n\u266fparam\nMNIST\nCKN-GM1\n2\n1 \u00d7 1\n12\n2\n3 \u00d7 3\n50\n4 \u00d7 4\n5 400\nCKN-GM2\n2\n1 \u00d7 1\n12\n2\n3 \u00d7 3\n400\n3 \u00d7 3\n43 200\nCKN-PM1\n1\n5 \u00d7 5\n200\n2\n-\n-\n4 \u00d7 4\n5 000\nCKN-PM2\n2\n5 \u00d7 5\n50\n2\n2 \u00d7 2\n200\n6 \u00d7 6\n41 250\nCIFAR-10\nCKN-GM\n2\n1 \u00d7 1\n12\n2\n2 \u00d7 2\n800\n4 \u00d7 4\n38 400\nCKN-PM\n2\n2 \u00d7 2\n100\n2\n2 \u00d7 2\n800\n4 \u00d7 4\n321 200\nSTL-10\nCKN-GM\n2\n1 \u00d7 1\n12\n2\n3 \u00d7 3\n800\n4 \u00d7 4\n86 400\nCKN-PM\n2\n3 \u00d7 3\n50\n2\n3 \u00d7 3\n800\n3 \u00d7 3\n361 350\nTable 3: List of architectures reported in the paper. N is the number of layers; p1 and p2 represent\nthe number of \ufb01lters are each layer; m1 and m2 represent the size of the patches P\u2032\n1 and P\u2032\n2 that are\nof size m1 \u00d7 m1 and m2 \u00d7 m2 on their respective feature maps \u03b61 and \u03b62; \u03b31 is the subsampling\nfactor between layer 1 and layer 2; S is the size of the output feature map, and the last column\nindicates the number of parameters that the network has to learn.\n10\n",
        "sentence": " Several approaches have been recently proposed to tackle the problem of deep unsupervised learning (Coates & Ng, 2012; Mairal et al., 2014; Dosovitskiy et al., 2014).",
        "context": "[13] A. Coates, A. Y. Ng, and H. Lee. An analysis of single-layer networks in unsupervised feature learning.\nIn Proc. AISTATS, 2011.\n[14] D. Decoste and B. Sch\u00a8olkopf. Training invariant support vector machines. Mach. Learn., 46(1-3):161\u2013\n190, 2002.\n2010.\n[11] D. Ciresan, U. Meier, and J. Schmidhuber. Multi-column deep neural networks for image classi\ufb01cation.\nIn Proc. CVPR, 2012.\n[12] A. Coates and A. Y. Ng. Selecting receptive \ufb01elds in deep networks. In Adv. NIPS, 2011.\narchitectures and no data augmentation. Some challenges regarding our work are left open for the\nfuture. The \ufb01rst one is the use of supervision to better approximate the kernel for the prediction task."
    },
    {
        "title": "neural-gas\u201d network learns topologies",
        "author": [
            "T. Martinetz",
            "Schulten",
            "K. A"
        ],
        "venue": null,
        "citeRegEx": "Martinetz et al\\.,? \\Q1991\\E",
        "shortCiteRegEx": "Martinetz et al\\.",
        "year": 1991,
        "abstract": "",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "Stacked convolutional auto-encoders for hierarchical feature extraction",
        "author": [
            "J. Masci",
            "U. Meier",
            "D. Cire\u015fan",
            "J. Schmidhuber"
        ],
        "venue": "In ICANN,",
        "citeRegEx": "Masci et al\\.,? \\Q2011\\E",
        "shortCiteRegEx": "Masci et al\\.",
        "year": 2011,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " Many have also used autoencoders with a reconstruction loss (Bengio et al., 2007; Ranzato et al., 2007; Masci et al., 2011). , 2007) or a deconvolutional network (Masci et al., 2011; Zhao et al., 2016) but can be more sophisticated, like a PixelCNN network (van den Oord et al.",
        "context": null
    },
    {
        "title": "Efficient estimation of word representations in vector space",
        "author": [
            "T. Mikolov",
            "K. Chen",
            "G. Corrado",
            "J. Dean"
        ],
        "venue": "arXiv preprint arXiv:1301.3781,",
        "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E",
        "shortCiteRegEx": "Mikolov et al\\.",
        "year": 2013,
        "abstract": "We propose two novel model architectures for computing continuous vector\nrepresentations of words from very large data sets. The quality of these\nrepresentations is measured in a word similarity task, and the results are\ncompared to the previously best performing techniques based on different types\nof neural networks. We observe large improvements in accuracy at much lower\ncomputational cost, i.e. it takes less than a day to learn high quality word\nvectors from a 1.6 billion words data set. Furthermore, we show that these\nvectors provide state-of-the-art performance on our test set for measuring\nsyntactic and semantic word similarities.",
        "full_text": "Ef\ufb01cient Estimation of Word Representations in\nVector Space\nTomas Mikolov\nGoogle Inc., Mountain View, CA\ntmikolov@google.com\nKai Chen\nGoogle Inc., Mountain View, CA\nkaichen@google.com\nGreg Corrado\nGoogle Inc., Mountain View, CA\ngcorrado@google.com\nJeffrey Dean\nGoogle Inc., Mountain View, CA\njeff@google.com\nAbstract\nWe propose two novel model architectures for computing continuous vector repre-\nsentations of words from very large data sets. The quality of these representations\nis measured in a word similarity task, and the results are compared to the previ-\nously best performing techniques based on different types of neural networks. We\nobserve large improvements in accuracy at much lower computational cost, i.e. it\ntakes less than a day to learn high quality word vectors from a 1.6 billion words\ndata set. Furthermore, we show that these vectors provide state-of-the-art perfor-\nmance on our test set for measuring syntactic and semantic word similarities.\n1\nIntroduction\nMany current NLP systems and techniques treat words as atomic units - there is no notion of similar-\nity between words, as these are represented as indices in a vocabulary. This choice has several good\nreasons - simplicity, robustness and the observation that simple models trained on huge amounts of\ndata outperform complex systems trained on less data. An example is the popular N-gram model\nused for statistical language modeling - today, it is possible to train N-grams on virtually all available\ndata (trillions of words [3]).\nHowever, the simple techniques are at their limits in many tasks. For example, the amount of\nrelevant in-domain data for automatic speech recognition is limited - the performance is usually\ndominated by the size of high quality transcribed speech data (often just millions of words). In\nmachine translation, the existing corpora for many languages contain only a few billions of words\nor less. Thus, there are situations where simple scaling up of the basic techniques will not result in\nany signi\ufb01cant progress, and we have to focus on more advanced techniques.\nWith progress of machine learning techniques in recent years, it has become possible to train more\ncomplex models on much larger data set, and they typically outperform the simple models. Probably\nthe most successful concept is to use distributed representations of words [10]. For example, neural\nnetwork based language models signi\ufb01cantly outperform N-gram models [1, 27, 17].\n1.1\nGoals of the Paper\nThe main goal of this paper is to introduce techniques that can be used for learning high-quality word\nvectors from huge data sets with billions of words, and with millions of words in the vocabulary. As\nfar as we know, none of the previously proposed architectures has been successfully trained on more\n1\narXiv:1301.3781v3  [cs.CL]  7 Sep 2013\nthan a few hundred of millions of words, with a modest dimensionality of the word vectors between\n50 - 100.\nWe use recently proposed techniques for measuring the quality of the resulting vector representa-\ntions, with the expectation that not only will similar words tend to be close to each other, but that\nwords can have multiple degrees of similarity [20]. This has been observed earlier in the context\nof in\ufb02ectional languages - for example, nouns can have multiple word endings, and if we search for\nsimilar words in a subspace of the original vector space, it is possible to \ufb01nd words that have similar\nendings [13, 14].\nSomewhat surprisingly, it was found that similarity of word representations goes beyond simple\nsyntactic regularities. Using a word offset technique where simple algebraic operations are per-\nformed on the word vectors, it was shown for example that vector(\u201dKing\u201d) - vector(\u201dMan\u201d) + vec-\ntor(\u201dWoman\u201d) results in a vector that is closest to the vector representation of the word Queen [20].\nIn this paper, we try to maximize accuracy of these vector operations by developing new model\narchitectures that preserve the linear regularities among words. We design a new comprehensive test\nset for measuring both syntactic and semantic regularities1, and show that many such regularities\ncan be learned with high accuracy. Moreover, we discuss how training time and accuracy depends\non the dimensionality of the word vectors and on the amount of the training data.\n1.2\nPrevious Work\nRepresentation of words as continuous vectors has a long history [10, 26, 8]. A very popular model\narchitecture for estimating neural network language model (NNLM) was proposed in [1], where a\nfeedforward neural network with a linear projection layer and a non-linear hidden layer was used to\nlearn jointly the word vector representation and a statistical language model. This work has been\nfollowed by many others.\nAnother interesting architecture of NNLM was presented in [13, 14], where the word vectors are\n\ufb01rst learned using neural network with a single hidden layer. The word vectors are then used to train\nthe NNLM. Thus, the word vectors are learned even without constructing the full NNLM. In this\nwork, we directly extend this architecture, and focus just on the \ufb01rst step where the word vectors are\nlearned using a simple model.\nIt was later shown that the word vectors can be used to signi\ufb01cantly improve and simplify many\nNLP applications [4, 5, 29]. Estimation of the word vectors itself was performed using different\nmodel architectures and trained on various corpora [4, 29, 23, 19, 9], and some of the resulting word\nvectors were made available for future research and comparison2. However, as far as we know, these\narchitectures were signi\ufb01cantly more computationally expensive for training than the one proposed\nin [13], with the exception of certain version of log-bilinear model where diagonal weight matrices\nare used [23].\n2\nModel Architectures\nMany different types of models were proposed for estimating continuous representations of words,\nincluding the well-known Latent Semantic Analysis (LSA) and Latent Dirichlet Allocation (LDA).\nIn this paper, we focus on distributed representations of words learned by neural networks, as it was\npreviously shown that they perform signi\ufb01cantly better than LSA for preserving linear regularities\namong words [20, 31]; LDA moreover becomes computationally very expensive on large data sets.\nSimilar to [18], to compare different model architectures we de\ufb01ne \ufb01rst the computational complex-\nity of a model as the number of parameters that need to be accessed to fully train the model. Next,\nwe will try to maximize the accuracy, while minimizing the computational complexity.\n1The test set is available at www.fit.vutbr.cz/\u02dcimikolov/rnnlm/word-test.v1.txt\n2http://ronan.collobert.com/senna/\nhttp://metaoptimize.com/projects/wordreprs/\nhttp://www.fit.vutbr.cz/\u02dcimikolov/rnnlm/\nhttp://ai.stanford.edu/\u02dcehhuang/\n2\nFor all the following models, the training complexity is proportional to\nO = E \u00d7 T \u00d7 Q,\n(1)\nwhere E is number of the training epochs, T is the number of the words in the training set and Q is\nde\ufb01ned further for each model architecture. Common choice is E = 3 \u221250 and T up to one billion.\nAll models are trained using stochastic gradient descent and backpropagation [26].\n2.1\nFeedforward Neural Net Language Model (NNLM)\nThe probabilistic feedforward neural network language model has been proposed in [1]. It consists\nof input, projection, hidden and output layers. At the input layer, N previous words are encoded\nusing 1-of-V coding, where V is size of the vocabulary. The input layer is then projected to a\nprojection layer P that has dimensionality N \u00d7 D, using a shared projection matrix. As only N\ninputs are active at any given time, composition of the projection layer is a relatively cheap operation.\nThe NNLM architecture becomes complex for computation between the projection and the hidden\nlayer, as values in the projection layer are dense. For a common choice of N = 10, the size of the\nprojection layer (P) might be 500 to 2000, while the hidden layer size H is typically 500 to 1000\nunits. Moreover, the hidden layer is used to compute probability distribution over all the words in the\nvocabulary, resulting in an output layer with dimensionality V . Thus, the computational complexity\nper each training example is\nQ = N \u00d7 D + N \u00d7 D \u00d7 H + H \u00d7 V,\n(2)\nwhere the dominating term is H \u00d7 V . However, several practical solutions were proposed for\navoiding it; either using hierarchical versions of the softmax [25, 23, 18], or avoiding normalized\nmodels completely by using models that are not normalized during training [4, 9]. With binary tree\nrepresentations of the vocabulary, the number of output units that need to be evaluated can go down\nto around log2(V ). Thus, most of the complexity is caused by the term N \u00d7 D \u00d7 H.\nIn our models, we use hierarchical softmax where the vocabulary is represented as a Huffman binary\ntree. This follows previous observations that the frequency of words works well for obtaining classes\nin neural net language models [16]. Huffman trees assign short binary codes to frequent words, and\nthis further reduces the number of output units that need to be evaluated: while balanced binary tree\nwould require log2(V ) outputs to be evaluated, the Huffman tree based hierarchical softmax requires\nonly about log2(Unigram perplexity(V )). For example when the vocabulary size is one million\nwords, this results in about two times speedup in evaluation. While this is not crucial speedup for\nneural network LMs as the computational bottleneck is in the N \u00d7D\u00d7H term, we will later propose\narchitectures that do not have hidden layers and thus depend heavily on the ef\ufb01ciency of the softmax\nnormalization.\n2.2\nRecurrent Neural Net Language Model (RNNLM)\nRecurrent neural network based language model has been proposed to overcome certain limitations\nof the feedforward NNLM, such as the need to specify the context length (the order of the model N),\nand because theoretically RNNs can ef\ufb01ciently represent more complex patterns than the shallow\nneural networks [15, 2]. The RNN model does not have a projection layer; only input, hidden and\noutput layer. What is special for this type of model is the recurrent matrix that connects hidden\nlayer to itself, using time-delayed connections. This allows the recurrent model to form some kind\nof short term memory, as information from the past can be represented by the hidden layer state that\ngets updated based on the current input and the state of the hidden layer in the previous time step.\nThe complexity per training example of the RNN model is\nQ = H \u00d7 H + H \u00d7 V,\n(3)\nwhere the word representations D have the same dimensionality as the hidden layer H. Again, the\nterm H \u00d7 V can be ef\ufb01ciently reduced to H \u00d7 log2(V ) by using hierarchical softmax. Most of the\ncomplexity then comes from H \u00d7 H.\n3\n2.3\nParallel Training of Neural Networks\nTo train models on huge data sets, we have implemented several models on top of a large-scale\ndistributed framework called DistBelief [6], including the feedforward NNLM and the new models\nproposed in this paper. The framework allows us to run multiple replicas of the same model in\nparallel, and each replica synchronizes its gradient updates through a centralized server that keeps\nall the parameters. For this parallel training, we use mini-batch asynchronous gradient descent with\nan adaptive learning rate procedure called Adagrad [7]. Under this framework, it is common to use\none hundred or more model replicas, each using many CPU cores at different machines in a data\ncenter.\n3\nNew Log-linear Models\nIn this section, we propose two new model architectures for learning distributed representations\nof words that try to minimize computational complexity. The main observation from the previous\nsection was that most of the complexity is caused by the non-linear hidden layer in the model. While\nthis is what makes neural networks so attractive, we decided to explore simpler models that might\nnot be able to represent the data as precisely as neural networks, but can possibly be trained on much\nmore data ef\ufb01ciently.\nThe new architectures directly follow those proposed in our earlier work [13, 14], where it was\nfound that neural network language model can be successfully trained in two steps: \ufb01rst, continuous\nword vectors are learned using simple model, and then the N-gram NNLM is trained on top of these\ndistributed representations of words. While there has been later substantial amount of work that\nfocuses on learning word vectors, we consider the approach proposed in [13] to be the simplest one.\nNote that related models have been proposed also much earlier [26, 8].\n3.1\nContinuous Bag-of-Words Model\nThe \ufb01rst proposed architecture is similar to the feedforward NNLM, where the non-linear hidden\nlayer is removed and the projection layer is shared for all words (not just the projection matrix);\nthus, all words get projected into the same position (their vectors are averaged). We call this archi-\ntecture a bag-of-words model as the order of words in the history does not in\ufb02uence the projection.\nFurthermore, we also use words from the future; we have obtained the best performance on the task\nintroduced in the next section by building a log-linear classi\ufb01er with four future and four history\nwords at the input, where the training criterion is to correctly classify the current (middle) word.\nTraining complexity is then\nQ = N \u00d7 D + D \u00d7 log2(V ).\n(4)\nWe denote this model further as CBOW, as unlike standard bag-of-words model, it uses continuous\ndistributed representation of the context. The model architecture is shown at Figure 1. Note that the\nweight matrix between the input and the projection layer is shared for all word positions in the same\nway as in the NNLM.\n3.2\nContinuous Skip-gram Model\nThe second architecture is similar to CBOW, but instead of predicting the current word based on the\ncontext, it tries to maximize classi\ufb01cation of a word based on another word in the same sentence.\nMore precisely, we use each current word as an input to a log-linear classi\ufb01er with continuous\nprojection layer, and predict words within a certain range before and after the current word. We\nfound that increasing the range improves quality of the resulting word vectors, but it also increases\nthe computational complexity. Since the more distant words are usually less related to the current\nword than those close to it, we give less weight to the distant words by sampling less from those\nwords in our training examples.\nThe training complexity of this architecture is proportional to\nQ = C \u00d7 (D + D \u00d7 log2(V )),\n(5)\nwhere C is the maximum distance of the words. Thus, if we choose C = 5, for each training word\nwe will select randomly a number R in range < 1; C >, and then use R words from history and\n4\nw(t-2)\nw(t+1)\nw(t-1)\nw(t+2)\nw(t)\nSUM\n       INPUT         PROJECTION         OUTPUT\nw(t)\n          INPUT         PROJECTION      OUTPUT\nw(t-2)\nw(t-1)\nw(t+1)\nw(t+2)\n                   CBOW                                                   Skip-gram\nFigure 1: New model architectures. The CBOW architecture predicts the current word based on the\ncontext, and the Skip-gram predicts surrounding words given the current word.\nR words from the future of the current word as correct labels. This will require us to do R \u00d7 2\nword classi\ufb01cations, with the current word as input, and each of the R + R words as output. In the\nfollowing experiments, we use C = 10.\n4\nResults\nTo compare the quality of different versions of word vectors, previous papers typically use a table\nshowing example words and their most similar words, and understand them intuitively. Although\nit is easy to show that word France is similar to Italy and perhaps some other countries, it is much\nmore challenging when subjecting those vectors in a more complex similarity task, as follows. We\nfollow previous observation that there can be many different types of similarities between words, for\nexample, word big is similar to bigger in the same sense that small is similar to smaller. Example\nof another type of relationship can be word pairs big - biggest and small - smallest [20]. We further\ndenote two pairs of words with the same relationship as a question, as we can ask: \u201dWhat is the\nword that is similar to small in the same sense as biggest is similar to big?\u201d\nSomewhat surprisingly, these questions can be answered by performing simple algebraic operations\nwith the vector representation of words. To \ufb01nd a word that is similar to small in the same sense as\nbiggest is similar to big, we can simply compute vector X = vector(\u201dbiggest\u201d)\u2212vector(\u201dbig\u201d)+\nvector(\u201dsmall\u201d). Then, we search in the vector space for the word closest to X measured by cosine\ndistance, and use it as the answer to the question (we discard the input question words during this\nsearch). When the word vectors are well trained, it is possible to \ufb01nd the correct answer (word\nsmallest) using this method.\nFinally, we found that when we train high dimensional word vectors on a large amount of data, the\nresulting vectors can be used to answer very subtle semantic relationships between words, such as\na city and the country it belongs to, e.g. France is to Paris as Germany is to Berlin. Word vectors\nwith such semantic relationships could be used to improve many existing NLP applications, such\nas machine translation, information retrieval and question answering systems, and may enable other\nfuture applications yet to be invented.\n5\nTable 1: Examples of \ufb01ve types of semantic and nine types of syntactic questions in the Semantic-\nSyntactic Word Relationship test set.\nType of relationship\nWord Pair 1\nWord Pair 2\nCommon capital city\nAthens\nGreece\nOslo\nNorway\nAll capital cities\nAstana\nKazakhstan\nHarare\nZimbabwe\nCurrency\nAngola\nkwanza\nIran\nrial\nCity-in-state\nChicago\nIllinois\nStockton\nCalifornia\nMan-Woman\nbrother\nsister\ngrandson\ngranddaughter\nAdjective to adverb\napparent\napparently\nrapid\nrapidly\nOpposite\npossibly\nimpossibly\nethical\nunethical\nComparative\ngreat\ngreater\ntough\ntougher\nSuperlative\neasy\neasiest\nlucky\nluckiest\nPresent Participle\nthink\nthinking\nread\nreading\nNationality adjective\nSwitzerland\nSwiss\nCambodia\nCambodian\nPast tense\nwalking\nwalked\nswimming\nswam\nPlural nouns\nmouse\nmice\ndollar\ndollars\nPlural verbs\nwork\nworks\nspeak\nspeaks\n4.1\nTask Description\nTo measure quality of the word vectors, we de\ufb01ne a comprehensive test set that contains \ufb01ve types\nof semantic questions, and nine types of syntactic questions. Two examples from each category are\nshown in Table 1. Overall, there are 8869 semantic and 10675 syntactic questions. The questions\nin each category were created in two steps: \ufb01rst, a list of similar word pairs was created manually.\nThen, a large list of questions is formed by connecting two word pairs. For example, we made a\nlist of 68 large American cities and the states they belong to, and formed about 2.5K questions by\npicking two word pairs at random. We have included in our test set only single token words, thus\nmulti-word entities are not present (such as New York).\nWe evaluate the overall accuracy for all question types, and for each question type separately (se-\nmantic, syntactic). Question is assumed to be correctly answered only if the closest word to the\nvector computed using the above method is exactly the same as the correct word in the question;\nsynonyms are thus counted as mistakes. This also means that reaching 100% accuracy is likely\nto be impossible, as the current models do not have any input information about word morphology.\nHowever, we believe that usefulness of the word vectors for certain applications should be positively\ncorrelated with this accuracy metric. Further progress can be achieved by incorporating information\nabout structure of words, especially for the syntactic questions.\n4.2\nMaximization of Accuracy\nWe have used a Google News corpus for training the word vectors. This corpus contains about\n6B tokens. We have restricted the vocabulary size to 1 million most frequent words. Clearly, we\nare facing time constrained optimization problem, as it can be expected that both using more data\nand higher dimensional word vectors will improve the accuracy. To estimate the best choice of\nmodel architecture for obtaining as good as possible results quickly, we have \ufb01rst evaluated models\ntrained on subsets of the training data, with vocabulary restricted to the most frequent 30k words.\nThe results using the CBOW architecture with different choice of word vector dimensionality and\nincreasing amount of the training data are shown in Table 2.\nIt can be seen that after some point, adding more dimensions or adding more training data provides\ndiminishing improvements. So, we have to increase both vector dimensionality and the amount\nof the training data together. While this observation might seem trivial, it must be noted that it is\ncurrently popular to train word vectors on relatively large amounts of data, but with insuf\ufb01cient size\n6\nTable 2:\nAccuracy on subset of the Semantic-Syntactic Word Relationship test set, using word\nvectors from the CBOW architecture with limited vocabulary. Only questions containing words from\nthe most frequent 30k words are used.\nDimensionality / Training words\n24M\n49M\n98M\n196M\n391M\n783M\n50\n13.4\n15.7\n18.6\n19.1\n22.5\n23.2\n100\n19.4\n23.1\n27.8\n28.7\n33.4\n32.2\n300\n23.2\n29.2\n35.3\n38.6\n43.7\n45.9\n600\n24.0\n30.1\n36.5\n40.8\n46.6\n50.4\nTable 3: Comparison of architectures using models trained on the same data, with 640-dimensional\nword vectors. The accuracies are reported on our Semantic-Syntactic Word Relationship test set,\nand on the syntactic relationship test set of [20]\nModel\nSemantic-Syntactic Word Relationship test set\nMSR Word Relatedness\nArchitecture\nSemantic Accuracy [%]\nSyntactic Accuracy [%]\nTest Set [20]\nRNNLM\n9\n36\n35\nNNLM\n23\n53\n47\nCBOW\n24\n64\n61\nSkip-gram\n55\n59\n56\n(such as 50 - 100). Given Equation 4, increasing amount of training data twice results in about the\nsame increase of computational complexity as increasing vector size twice.\nFor the experiments reported in Tables 2 and 4, we used three training epochs with stochastic gradi-\nent descent and backpropagation. We chose starting learning rate 0.025 and decreased it linearly, so\nthat it approaches zero at the end of the last training epoch.\n4.3\nComparison of Model Architectures\nFirst we compare different model architectures for deriving the word vectors using the same training\ndata and using the same dimensionality of 640 of the word vectors. In the further experiments, we\nuse full set of questions in the new Semantic-Syntactic Word Relationship test set, i.e. unrestricted to\nthe 30k vocabulary. We also include results on a test set introduced in [20] that focuses on syntactic\nsimilarity between words3.\nThe training data consists of several LDC corpora and is described in detail in [18] (320M words,\n82K vocabulary). We used these data to provide a comparison to a previously trained recurrent\nneural network language model that took about 8 weeks to train on a single CPU. We trained a feed-\nforward NNLM with the same number of 640 hidden units using the DistBelief parallel training [6],\nusing a history of 8 previous words (thus, the NNLM has more parameters than the RNNLM, as the\nprojection layer has size 640 \u00d7 8).\nIn Table 3, it can be seen that the word vectors from the RNN (as used in [20]) perform well mostly\non the syntactic questions. The NNLM vectors perform signi\ufb01cantly better than the RNN - this is\nnot surprising, as the word vectors in the RNNLM are directly connected to a non-linear hidden\nlayer. The CBOW architecture works better than the NNLM on the syntactic tasks, and about the\nsame on the semantic one. Finally, the Skip-gram architecture works slightly worse on the syntactic\ntask than the CBOW model (but still better than the NNLM), and much better on the semantic part\nof the test than all the other models.\nNext, we evaluated our models trained using one CPU only and compared the results against publicly\navailable word vectors. The comparison is given in Table 4. The CBOW model was trained on subset\n3We thank Geoff Zweig for providing us the test set.\n7\nTable 4: Comparison of publicly available word vectors on the Semantic-Syntactic Word Relation-\nship test set, and word vectors from our models. Full vocabularies are used.\nModel\nVector\nTraining\nAccuracy [%]\nDimensionality\nwords\nSemantic\nSyntactic\nTotal\nCollobert-Weston NNLM\n50\n660M\n9.3\n12.3\n11.0\nTurian NNLM\n50\n37M\n1.4\n2.6\n2.1\nTurian NNLM\n200\n37M\n1.4\n2.2\n1.8\nMnih NNLM\n50\n37M\n1.8\n9.1\n5.8\nMnih NNLM\n100\n37M\n3.3\n13.2\n8.8\nMikolov RNNLM\n80\n320M\n4.9\n18.4\n12.7\nMikolov RNNLM\n640\n320M\n8.6\n36.5\n24.6\nHuang NNLM\n50\n990M\n13.3\n11.6\n12.3\nOur NNLM\n20\n6B\n12.9\n26.4\n20.3\nOur NNLM\n50\n6B\n27.9\n55.8\n43.2\nOur NNLM\n100\n6B\n34.2\n64.5\n50.8\nCBOW\n300\n783M\n15.5\n53.1\n36.1\nSkip-gram\n300\n783M\n50.0\n55.9\n53.3\nTable 5: Comparison of models trained for three epochs on the same data and models trained for\none epoch. Accuracy is reported on the full Semantic-Syntactic data set.\nModel\nVector\nTraining\nAccuracy [%]\nTraining time\nDimensionality\nwords\n[days]\nSemantic\nSyntactic\nTotal\n3 epoch CBOW\n300\n783M\n15.5\n53.1\n36.1\n1\n3 epoch Skip-gram\n300\n783M\n50.0\n55.9\n53.3\n3\n1 epoch CBOW\n300\n783M\n13.8\n49.9\n33.6\n0.3\n1 epoch CBOW\n300\n1.6B\n16.1\n52.6\n36.1\n0.6\n1 epoch CBOW\n600\n783M\n15.4\n53.3\n36.2\n0.7\n1 epoch Skip-gram\n300\n783M\n45.6\n52.2\n49.2\n1\n1 epoch Skip-gram\n300\n1.6B\n52.2\n55.1\n53.8\n2\n1 epoch Skip-gram\n600\n783M\n56.7\n54.5\n55.5\n2.5\nof the Google News data in about a day, while training time for the Skip-gram model was about three\ndays.\nFor experiments reported further, we used just one training epoch (again, we decrease the learning\nrate linearly so that it approaches zero at the end of training). Training a model on twice as much\ndata using one epoch gives comparable or better results than iterating over the same data for three\nepochs, as is shown in Table 5, and provides additional small speedup.\n4.4\nLarge Scale Parallel Training of Models\nAs mentioned earlier, we have implemented various models in a distributed framework called Dis-\ntBelief. Below we report the results of several models trained on the Google News 6B data set,\nwith mini-batch asynchronous gradient descent and the adaptive learning rate procedure called Ada-\ngrad [7]. We used 50 to 100 model replicas during the training. The number of CPU cores is an\n8\nTable 6:\nComparison of models trained using the DistBelief distributed framework. Note that\ntraining of NNLM with 1000-dimensional vectors would take too long to complete.\nModel\nVector\nTraining\nAccuracy [%]\nTraining time\nDimensionality\nwords\n[days x CPU cores]\nSemantic\nSyntactic\nTotal\nNNLM\n100\n6B\n34.2\n64.5\n50.8\n14 x 180\nCBOW\n1000\n6B\n57.3\n68.9\n63.7\n2 x 140\nSkip-gram\n1000\n6B\n66.1\n65.1\n65.6\n2.5 x 125\nTable 7: Comparison and combination of models on the Microsoft Sentence Completion Challenge.\nArchitecture\nAccuracy [%]\n4-gram [32]\n39\nAverage LSA similarity [32]\n49\nLog-bilinear model [24]\n54.8\nRNNLMs [19]\n55.4\nSkip-gram\n48.0\nSkip-gram + RNNLMs\n58.9\nestimate since the data center machines are shared with other production tasks, and the usage can\n\ufb02uctuate quite a bit. Note that due to the overhead of the distributed framework, the CPU usage of\nthe CBOW model and the Skip-gram model are much closer to each other than their single-machine\nimplementations. The result are reported in Table 6.\n4.5\nMicrosoft Research Sentence Completion Challenge\nThe Microsoft Sentence Completion Challenge has been recently introduced as a task for advancing\nlanguage modeling and other NLP techniques [32]. This task consists of 1040 sentences, where one\nword is missing in each sentence and the goal is to select word that is the most coherent with the\nrest of the sentence, given a list of \ufb01ve reasonable choices. Performance of several techniques has\nbeen already reported on this set, including N-gram models, LSA-based model [32], log-bilinear\nmodel [24] and a combination of recurrent neural networks that currently holds the state of the art\nperformance of 55.4% accuracy on this benchmark [19].\nWe have explored the performance of Skip-gram architecture on this task. First, we train the 640-\ndimensional model on 50M words provided in [32]. Then, we compute score of each sentence in\nthe test set by using the unknown word at the input, and predict all surrounding words in a sentence.\nThe \ufb01nal sentence score is then the sum of these individual predictions. Using the sentence scores,\nwe choose the most likely sentence.\nA short summary of some previous results together with the new results is presented in Table 7.\nWhile the Skip-gram model itself does not perform on this task better than LSA similarity, the scores\nfrom this model are complementary to scores obtained with RNNLMs, and a weighted combination\nleads to a new state of the art result 58.9% accuracy (59.2% on the development part of the set and\n58.7% on the test part of the set).\n5\nExamples of the Learned Relationships\nTable 8 shows words that follow various relationships. We follow the approach described above: the\nrelationship is de\ufb01ned by subtracting two word vectors, and the result is added to another word. Thus\nfor example, Paris - France + Italy = Rome. As it can be seen, accuracy is quite good, although\nthere is clearly a lot of room for further improvements (note that using our accuracy metric that\n9\nTable 8: Examples of the word pair relationships, using the best word vectors from Table 4 (Skip-\ngram model trained on 783M words with 300 dimensionality).\nRelationship\nExample 1\nExample 2\nExample 3\nFrance - Paris\nItaly: Rome\nJapan: Tokyo\nFlorida: Tallahassee\nbig - bigger\nsmall: larger\ncold: colder\nquick: quicker\nMiami - Florida\nBaltimore: Maryland\nDallas: Texas\nKona: Hawaii\nEinstein - scientist\nMessi: mid\ufb01elder\nMozart: violinist\nPicasso: painter\nSarkozy - France\nBerlusconi: Italy\nMerkel: Germany\nKoizumi: Japan\ncopper - Cu\nzinc: Zn\ngold: Au\nuranium: plutonium\nBerlusconi - Silvio\nSarkozy: Nicolas\nPutin: Medvedev\nObama: Barack\nMicrosoft - Windows\nGoogle: Android\nIBM: Linux\nApple: iPhone\nMicrosoft - Ballmer\nGoogle: Yahoo\nIBM: McNealy\nApple: Jobs\nJapan - sushi\nGermany: bratwurst\nFrance: tapas\nUSA: pizza\nassumes exact match, the results in Table 8 would score only about 60%). We believe that word\nvectors trained on even larger data sets with larger dimensionality will perform signi\ufb01cantly better,\nand will enable the development of new innovative applications. Another way to improve accuracy is\nto provide more than one example of the relationship. By using ten examples instead of one to form\nthe relationship vector (we average the individual vectors together), we have observed improvement\nof accuracy of our best models by about 10% absolutely on the semantic-syntactic test.\nIt is also possible to apply the vector operations to solve different tasks. For example, we have\nobserved good accuracy for selecting out-of-the-list words, by computing average vector for a list of\nwords, and \ufb01nding the most distant word vector. This is a popular type of problems in certain human\nintelligence tests. Clearly, there is still a lot of discoveries to be made using these techniques.\n6\nConclusion\nIn this paper we studied the quality of vector representations of words derived by various models on\na collection of syntactic and semantic language tasks. We observed that it is possible to train high\nquality word vectors using very simple model architectures, compared to the popular neural network\nmodels (both feedforward and recurrent). Because of the much lower computational complexity, it\nis possible to compute very accurate high dimensional word vectors from a much larger data set.\nUsing the DistBelief distributed framework, it should be possible to train the CBOW and Skip-gram\nmodels even on corpora with one trillion words, for basically unlimited size of the vocabulary. That\nis several orders of magnitude larger than the best previously published results for similar models.\nAn interesting task where the word vectors have recently been shown to signi\ufb01cantly outperform the\nprevious state of the art is the SemEval-2012 Task 2 [11]. The publicly available RNN vectors were\nused together with other techniques to achieve over 50% increase in Spearman\u2019s rank correlation\nover the previous best result [31]. The neural network based word vectors were previously applied\nto many other NLP tasks, for example sentiment analysis [12] and paraphrase detection [28]. It can\nbe expected that these applications can bene\ufb01t from the model architectures described in this paper.\nOur ongoing work shows that the word vectors can be successfully applied to automatic extension\nof facts in Knowledge Bases, and also for veri\ufb01cation of correctness of existing facts. Results\nfrom machine translation experiments also look very promising. In the future, it would be also\ninteresting to compare our techniques to Latent Relational Analysis [30] and others. We believe that\nour comprehensive test set will help the research community to improve the existing techniques for\nestimating the word vectors. We also expect that high quality word vectors will become an important\nbuilding block for future NLP applications.\n10\n7\nFollow-Up Work\nAfter the initial version of this paper was written, we published single-machine multi-threaded C++\ncode for computing the word vectors, using both the continuous bag-of-words and skip-gram archi-\ntectures4. The training speed is signi\ufb01cantly higher than reported earlier in this paper, i.e. it is in the\norder of billions of words per hour for typical hyperparameter choices. We also published more than\n1.4 million vectors that represent named entities, trained on more than 100 billion words. Some of\nour follow-up work will be published in an upcoming NIPS 2013 paper [21].\nReferences\n[1] Y. Bengio, R. Ducharme, P. Vincent. A neural probabilistic language model. Journal of Ma-\nchine Learning Research, 3:1137-1155, 2003.\n[2] Y. Bengio, Y. LeCun. Scaling learning algorithms towards AI. In: Large-Scale Kernel Ma-\nchines, MIT Press, 2007.\n[3] T. Brants, A. C. Popat, P. Xu, F. J. Och, and J. Dean. Large language models in machine\ntranslation. In Proceedings of the Joint Conference on Empirical Methods in Natural Language\nProcessing and Computational Language Learning, 2007.\n[4] R. Collobert and J. Weston. A Uni\ufb01ed Architecture for Natural Language Processing: Deep\nNeural Networks with Multitask Learning. In International Conference on Machine Learning,\nICML, 2008.\n[5] R. Collobert, J. Weston, L. Bottou, M. Karlen, K. Kavukcuoglu and P. Kuksa. Natural Lan-\nguage Processing (Almost) from Scratch. Journal of Machine Learning Research, 12:2493-\n2537, 2011.\n[6] J. Dean, G.S. Corrado, R. Monga, K. Chen, M. Devin, Q.V. Le, M.Z. Mao, M.A. Ranzato, A.\nSenior, P. Tucker, K. Yang, A. Y. Ng., Large Scale Distributed Deep Networks, NIPS, 2012.\n[7] J.C. Duchi, E. Hazan, and Y. Singer. Adaptive subgradient methods for online learning and\nstochastic optimization. Journal of Machine Learning Research, 2011.\n[8] J. Elman. Finding Structure in Time. Cognitive Science, 14, 179-211, 1990.\n[9] Eric H. Huang, R. Socher, C. D. Manning and Andrew Y. Ng. Improving Word Representations\nvia Global Context and Multiple Word Prototypes. In: Proc. Association for Computational\nLinguistics, 2012.\n[10] G.E. Hinton, J.L. McClelland, D.E. Rumelhart. Distributed representations. In: Parallel dis-\ntributed processing: Explorations in the microstructure of cognition. Volume 1: Foundations,\nMIT Press, 1986.\n[11] D.A. Jurgens, S.M. Mohammad, P.D. Turney, K.J. Holyoak. Semeval-2012 task 2: Measuring\ndegrees of relational similarity. In: Proceedings of the 6th International Workshop on Semantic\nEvaluation (SemEval 2012), 2012.\n[12] A.L. Maas, R.E. Daly, P.T. Pham, D. Huang, A.Y. Ng, and C. Potts. Learning word vectors for\nsentiment analysis. In Proceedings of ACL, 2011.\n[13] T. Mikolov. Language Modeling for Speech Recognition in Czech, Masters thesis, Brno Uni-\nversity of Technology, 2007.\n[14] T. Mikolov, J. Kopeck\u00b4y, L. Burget, O. Glembek and J. \u02c7Cernock\u00b4y. Neural network based lan-\nguage models for higly in\ufb02ective languages, In: Proc. ICASSP 2009.\n[15] T. Mikolov, M. Kara\ufb01\u00b4at, L. Burget, J. \u02c7Cernock\u00b4y, S. Khudanpur. Recurrent neural network\nbased language model, In: Proceedings of Interspeech, 2010.\n[16] T. Mikolov, S. Kombrink, L. Burget, J. \u02c7Cernock\u00b4y, S. Khudanpur. Extensions of recurrent neural\nnetwork language model, In: Proceedings of ICASSP 2011.\n[17] T. Mikolov, A. Deoras, S. Kombrink, L. Burget, J. \u02c7Cernock\u00b4y. Empirical Evaluation and Com-\nbination of Advanced Language Modeling Techniques, In: Proceedings of Interspeech, 2011.\n4The code is available at https://code.google.com/p/word2vec/\n11\n[18] T. Mikolov, A. Deoras, D. Povey, L. Burget, J. \u02c7Cernock\u00b4y. Strategies for Training Large Scale\nNeural Network Language Models, In: Proc. Automatic Speech Recognition and Understand-\ning, 2011.\n[19] T. Mikolov. Statistical Language Models based on Neural Networks. PhD thesis, Brno Univer-\nsity of Technology, 2012.\n[20] T. Mikolov, W.T. Yih, G. Zweig. Linguistic Regularities in Continuous Space Word Represen-\ntations. NAACL HLT 2013.\n[21] T. Mikolov, I. Sutskever, K. Chen, G. Corrado, and J. Dean. Distributed Representations of\nWords and Phrases and their Compositionality. Accepted to NIPS 2013.\n[22] A. Mnih, G. Hinton. Three new graphical models for statistical language modelling. ICML,\n2007.\n[23] A. Mnih, G. Hinton. A Scalable Hierarchical Distributed Language Model. Advances in Neural\nInformation Processing Systems 21, MIT Press, 2009.\n[24] A. Mnih, Y.W. Teh. A fast and simple algorithm for training neural probabilistic language\nmodels. ICML, 2012.\n[25] F. Morin, Y. Bengio. Hierarchical Probabilistic Neural Network Language Model. AISTATS,\n2005.\n[26] D. E. Rumelhart, G. E. Hinton, R. J. Williams. Learning internal representations by back-\npropagating errors. Nature, 323:533.536, 1986.\n[27] H. Schwenk. Continuous space language models. Computer Speech and Language, vol. 21,\n2007.\n[28] R. Socher, E.H. Huang, J. Pennington, A.Y. Ng, and C.D. Manning. Dynamic Pooling and\nUnfolding Recursive Autoencoders for Paraphrase Detection. In NIPS, 2011.\n[29] J. Turian, L. Ratinov, Y. Bengio. Word Representations: A Simple and General Method for\nSemi-Supervised Learning. In: Proc. Association for Computational Linguistics, 2010.\n[30] P. D. Turney. Measuring Semantic Similarity by Latent Relational Analysis. In: Proc. Interna-\ntional Joint Conference on Arti\ufb01cial Intelligence, 2005.\n[31] A. Zhila, W.T. Yih, C. Meek, G. Zweig, T. Mikolov. Combining Heterogeneous Models for\nMeasuring Relational Similarity. NAACL HLT 2013.\n[32] G. Zweig, C.J.C. Burges. The Microsoft Research Sentence Completion Challenge, Microsoft\nResearch Technical Report MSR-TR-2011-129, 2011.\n12\n",
        "sentence": " In the same vein as word2vec (Mikolov et al., 2013), Doersch et al.",
        "context": "tations. NAACL HLT 2013.\n[21] T. Mikolov, I. Sutskever, K. Chen, G. Corrado, and J. Dean. Distributed Representations of\nWords and Phrases and their Compositionality. Accepted to NIPS 2013.\n[12] A.L. Maas, R.E. Daly, P.T. Pham, D. Huang, A.Y. Ng, and C. Potts. Learning word vectors for\nsentiment analysis. In Proceedings of ACL, 2011.\n[13] T. Mikolov. Language Modeling for Speech Recognition in Czech, Masters thesis, Brno Uni-\n1.4 million vectors that represent named entities, trained on more than 100 billion words. Some of\nour follow-up work will be published in an upcoming NIPS 2013 paper [21].\nReferences"
    },
    {
        "title": "Unsupervised learning of visual representations by solving jigsaw puzzles",
        "author": [
            "M. Noroozi",
            "P. Favaro"
        ],
        "venue": "arXiv preprint arXiv:1603.09246,",
        "citeRegEx": "Noroozi and Favaro,? \\Q2016\\E",
        "shortCiteRegEx": "Noroozi and Favaro",
        "year": 2016,
        "abstract": "In this paper we study the problem of image representation learning without\nhuman annotation. By following the principles of self-supervision, we build a\nconvolutional neural network (CNN) that can be trained to solve Jigsaw puzzles\nas a pretext task, which requires no manual labeling, and then later repurposed\nto solve object classification and detection. To maintain the compatibility\nacross tasks we introduce the context-free network (CFN), a siamese-ennead CNN.\nThe CFN takes image tiles as input and explicitly limits the receptive field\n(or context) of its early processing units to one tile at a time. We show that\nthe CFN includes fewer parameters than AlexNet while preserving the same\nsemantic learning capabilities. By training the CFN to solve Jigsaw puzzles, we\nlearn both a feature mapping of object parts as well as their correct spatial\narrangement. Our experimental evaluations show that the learned features\ncapture semantically relevant content. Our proposed method for learning visual\nrepresentations outperforms state of the art methods in several transfer\nlearning benchmarks.",
        "full_text": "Unsupervised Learning of Visual\nRepresentations by Solving Jigsaw Puzzles\nMehdi Noroozi and Paolo Favaro\nInstitute for Informatiks\nUniversity of Bern\n{noroozi,paolo.favaro}@inf.unibe.ch\nAbstract. In this paper we study the problem of image representation\nlearning without human annotation. By following the principles of self-\nsupervision, we build a convolutional neural network (CNN) that can\nbe trained to solve Jigsaw puzzles as a pretext task, which requires no\nmanual labeling, and then later repurposed to solve object classi\ufb01cation\nand detection. To maintain the compatibility across tasks we introduce\nthe context-free network (CFN), a siamese-ennead CNN. The CFN takes\nimage tiles as input and explicitly limits the receptive \ufb01eld (or context)\nof its early processing units to one tile at a time. We show that the CFN\nincludes fewer parameters than AlexNet while preserving the same se-\nmantic learning capabilities. By training the CFN to solve Jigsaw puzzles,\nwe learn both a feature mapping of object parts as well as their correct\nspatial arrangement. Our experimental evaluations show that the learned\nfeatures capture semantically relevant content. Our proposed method for\nlearning visual representations outperforms state of the art methods in\nseveral transfer learning benchmarks.\n1\nIntroduction\nVisual tasks, such as object classi\ufb01cation and detection, have been successfully\napproached through the supervised learning paradigm [1,11,25,36], where one\nuses labeled data to train a parametric model. However, as manually labeled\ndata can be costly, unsupervised learning methods are gaining momentum.\nRecently, Doersch et al. [10], Wang and Gupta [39] and Agrawal et al. [2]\nhave explored a novel paradigm for unsupervised learning called self-supervised\nlearning. The main idea is to exploit di\ufb00erent labelings that are freely available\nbesides or within visual data, and to use them as intrinsic reward signals to learn\ngeneral-purpose features. [10] uses the relative spatial co-location of patches in\nimages as a label. [39] uses object correspondence obtained through tracking in\nvideos, and [2] uses ego-motion information obtained by a mobile agent such as\nthe Google car [7]. The features obtained with these approaches have been suc-\ncessfully transferred to classi\ufb01cation and detections tasks, and their performance\nis very encouraging when compared to features trained in a supervised manner.\nA fundamental di\ufb00erence between [10] and [39,2] is that the former method\nuses single images as the training set and the other two methods exploit mul-\ntiple images related either through a temporal or a viewpoint transformation.\narXiv:1603.09246v3  [cs.CV]  22 Aug 2017\n2\nM. Noroozi and P. Favaro\n(a)\n(b)\n(c)\nFig. 1: Learning image representations by solving Jigsaw puzzles. (a) The image\nfrom which the tiles (marked with green lines) are extracted. (b) A puzzle ob-\ntained by shu\ufb04ing the tiles. Some tiles might be directly identi\ufb01able as object\nparts, but others are ambiguous (e.g., have similar patterns) and their identi-\n\ufb01cation is much more reliable when all tiles are jointly evaluated. In contrast,\nwith reference to (c), determining the relative position between the central tile\nand the top two tiles from the left can be very challenging [10].\nWhile it is true that biological agents typically make use of multiple images and\nalso integrate additional sensory information, such as ego-motion, it is also true\nthat single snapshots may carry more information than we have been able to ex-\ntract so far. This work shows that this is indeed the case. We introduce a novel\nself-supervised task, the Jigsaw puzzle reassembly problem (see Fig. 1), which\nbuilds features that yield high performance when transferred to detection and\nclassi\ufb01cation tasks.\nWe argue that solving Jigsaw puzzles can be used to teach a system that\nan object is made of parts and what these parts are. The association of each\nseparate puzzle tile to a precise object part might be ambiguous. However, when\nall the tiles are observed, the ambiguities might be eliminated more easily be-\ncause the tile placement is mutually exclusive. This argument is supported by\nour experimental validation. Training a Jigsaw puzzle solver takes about 2.5\ndays compared to 4 weeks of [10]. Also, there is no need to handle chromatic\naberration or to build robustness to pixelation. Moreover, the features are highly\ntransferrable to detection and classi\ufb01cation and yield the highest performance\nto date for an unsupervised method.\n2\nRelated work\nThis work falls in the area of representation/feature learning, which is an unsu-\npervised learning problem [3]. Representation learning is concerned with building\nintermediate representations of data useful to solve machine learning tasks. It\nalso involves transfer learning [41], as one applies and repurposes features that\nhave been learned by solving the Jigsaw puzzle to other tasks such as object\nclassi\ufb01cation and detection. In our experiments we do so via the pre-training +\nUnsupervised Learning of Visual Representations by Solving Jigsaw Puzzles\n3\n\ufb01ne-tuning scheme, as in prior work [2]. Pre-training corresponds to the feature\nlearning that we obtain with our Jigsaw puzzle solver. Fine-tuning is instead the\nprocess of updating the weights obtained during pre-training to solve another\ntask (object classi\ufb01cation or detection).\nUnsupervised Learning. There is a rich literature in unsupervised learning\nof visual representations [5]. Most techniques build representations by exploit-\ning general-purpose priors such as smoothness, sharing of factors, factors orga-\nnized hierarchically, belonging to a low-dimension manifold, temporal and spa-\ntial coherence, and sparsity. Unfortunately, a general criterion to design a visual\nrepresentation is not available. Nonetheless, a natural choice is the goal of dis-\nentangling factors of variation. For example, several factors such as the object\nshapes, the object materials, and the light sources, combine to create complex\ne\ufb00ects such as shadows, shading, color patterns and re\ufb02ections in images. Ideal\nfeatures would separate each of these factors so that other learning tasks (e.g.,\nclassi\ufb01cation based on just shape or surface materials) can be handled more\neasily. In this work we design features to separate the appearance from the ar-\nrangement (geometry) of parts of objects.\nBecause of the relevance to contemporary research and to this work, we\ndiscuss mainly methods in deep learning. In general one can group unsuper-\nvised learning methods into: probabilistic, direct mapping (autoencoders), and\nmanifold learning ones. Probabilistic methods divide variables of a network into\nobserved and latent ones. Learning is then associated with determining model\nparameters that maximize the likelihood of the latent variables given the obser-\nvations. A family of popular probabilistic models is the Restricted Boltzmann\nMachine (RBM) [37,18], which makes training tractable by imposing a bipar-\ntite graph between latent and observed variables. Unfortunately, these models\nbecome intractable when multiple layers are present and are not designed to pro-\nduce features in an e\ufb03cient manner. The direct mapping approach focuses on\nthe latter aspect and is typically built via autoencoders [6,19,29]. Autoencoders\nspecify explicitly the feature extraction function (encoder) in a parametric form\nas well as the mapping from the feature back to the input (decoder). These di-\nrect mappings are trained by minimizing the reconstruction error between the\ninput and the output produced by the autoencoder (obtained by applying the\nencoder and decoder sequentially). A remarkable example of a very large scale\nautoencoder is the work of Le et al. [26]. Their results showed that robust human\nand cat faces as well as human body detectors could be built without human\nlabeling.\nIf the data structure suggests that data points might concentrate around a\nmanifold, then manifold learning techniques can be employed [34,4]. This rep-\nresentation allows to map directly smooth variations of the factors to smooth\nvariations of the observations. Some of the issues with manifold learning tech-\nniques are that they might require computing nearest neighbors (which scales\nquadratically with the number of samples) and that they need a su\ufb03ciently\nhigh density of samples around the manifold (and this becomes more di\ufb03cult to\nachieve with high-dimensional manifolds).\n4\nM. Noroozi and P. Favaro\nIn the context of Computer Vision, it is worth mentioning some early work on\nunsupervised learning of models for classi\ufb01cation. For instance [13,40] introduced\nmethods to build a probabilistic representation of objects as constellations of\nparts. A limitation is the high computational complexity of these models. As we\nwill see later, training the Jigsaw puzzle solver also amounts to building a model\nof both appearance and con\ufb01guration of the parts.\nSelf-supervised Learning. This learning strategy is a recent variation on the\nunsupervised learning theme that exploits labeling that comes for \u201cfree\u201d with\nthe data [10,39,2]. We make a distinction between labels that are easily accessible\nand are associated with a non-visual signal (for example, ego-motion [2], but also\none could consider audio, text and so on), and labels that are obtained from the\nstructure of the data [10,39]. Our work relates to the latter case as we simply\nre-use the input images and exploit the pixel arrangement as a label.\nDoersch et al. [10] train a convolutional network to classify the relative po-\nsition between two image patches. One tile is kept in the middle of a 3 \u00d7 3 grid\nand the other tile can be placed in any of the other 8 available locations (up to\nsome small random shift). In Fig. 1 (c) we show an example where the relative\nlocation between the central tile and the top-left and top-middle tiles is ambigu-\nous. In contrast, the Jigsaw puzzle problem is solved by observing all the tiles\nat the same time. This allows the trained network to intersect all ambiguity sets\nand possibly reduce them to a singleton.\nThe method of Wang and Gupta [39] builds a metric to de\ufb01ne similarity be-\ntween patches. Three patches are used as input, where two patches are matched\nvia tracking in a video and the third one is arbitrarily chosen. The main advan-\ntage of this method is that labeling requires just using a tracking method (they\nuse SURF interest points to detect initial bounding boxes and then tracking\nvia the KCF method [17]). The matched patches will have intraclass variability\ndue to changes in illumination, occlusion, viewpoint, pose, occlusions, and clut-\nter factors. However, because the underlying object is the same, the estimated\nfeatures may not necessarily cluster patches with two di\ufb00erent instances of the\nsame object (i.e., based on their semantic content). The method proposed by\nAgrawal et al. [2] exploits labeling (egomotion) provided by other sensors. The\nadvantage is that this labeling is freely available in most cases or is quite easy to\nobtain. They show that egomotion is a useful supervisory signal when learning\nfeatures. They train a siamese network to estimate egomotion from two image\nframes and compare it to the egomotion measured with odometry sensors. The\ntrained features will build an invariance similar to that of [39]. However, because\nthe object identity is the same in both images, the intraclass variability may be\nlimited. With two images of the same instance, learned features focus on their\nsimilarities (such as color and texture) rather than their high-level structure. In\ncontrast, the Jigsaw puzzle approach ignores similarities between tiles (such as\ncolor and texture), as they do not help their localization, and focuses instead\non their di\ufb00erences. In Fig. 2 we illustrate this concept with two examples: Two\ncars that have di\ufb00erent colors and two dogs with di\ufb00erent fur patterns. The\nfeatures learned to solve puzzles in one (car/dog) image will apply also to the\nUnsupervised Learning of Visual Representations by Solving Jigsaw Puzzles\n5\nFig. 2: Most of the shape of these 2 pairs of images is the same (two separate\ninstances within the same categories). However, some low-level statistics are\ndi\ufb00erent (color and texture). The Jigsaw puzzle solver learns to ignore such\nstatistics when they do not help the localization of parts.\nother (car/dog) image as they will be invariant to shared patterns. The ability\nof the Jigsaw puzzle solver to cluster together object parts can be seen in the\ntop 16 activations shown in Fig. 4 and in the image retrieval samples in Fig. 5.\nJigsaw Puzzles. Jigsaw puzzles have been associated with learning since their\ninception. They were introduced in 1760 by John Spilsbury as a pretext to help\nchildren learn geography. The \ufb01rst puzzle was a map attached to a wooden\nboard, which was then sawed in pieces corresponding to countries [38]. Studies\nin Psychonomic show that Jigsaw puzzles can be used to assess visuospatial\nprocessing in humans [33]. Indeed, the Hooper Visual Organization Test [20] is\nroutinely used to measures an individual\u2019s ability to organize visual stimuli. This\ntest uses puzzles with line drawings of simple objects and requires the patient to\nrecognize the object without moving the tiles. Instead of using Jigsaw puzzles to\nassess someone\u2019s visuospatial processing ability, in this paper we propose to use\nJigsaw puzzles to develop a visuospatial representation of objects in the context\nof CNNs.\nThere is also a sizeable literature on solving Jigsaw puzzles computationally\n(see, for example, [32,14,31]). However, these methods rely on the shape of the\ntiles or on texture especially in the proximity of the borders of the tiles. These\nare cues that we avoid when training the Jigsaw puzzle solver, as they do not\ncarry useful information when learning a part detector.\n3\nSolving Jigsaw Puzzles\nAt the present time, the design of convolutional neural networks (CNN) is still\nan art that relies on extensive experience. Here we provide a brief discussion of\nhow we arrived at a convolutional architecture capable of solving Jigsaw puzzles\nwhile learning general-purpose features.\nAn immediate approach to solve Jigsaw puzzles is to stack the tiles of the\npuzzle along the channels (i.e., the input data would have 9 \u00d7 3 = 27 channels)\nand then correspondingly increase the depth of the \ufb01lters of the \ufb01rst convolu-\ntional layer in AlexNet [25]. The problem with this design is that the network\nprefers to identify correlations between low-level texture statistics across tiles\n6\nM. Noroozi and P. Favaro\nrather than between the high-level primitives. Low-level statistics, such as simi-\nlar structural patterns and texture close to the boundaries of the tile, are simple\ncues that humans actually use to solve Jigsaw puzzles. However, solving a Jig-\nsaw puzzle based on these cues does not require any understanding of the global\nobject. Thus, here we present a network that delays the computation of statis-\ntics across di\ufb00erent tiles (see Fig. 3). The network \ufb01rst computes features based\nonly on the pixels within each tile (one row in Fig. 3). Then, it \ufb01nds the parts\narrangement just by using these features (last fully connected layers in Fig. 3).\nThe objective is to force the network to learn features that are as representative\nand discriminative as possible of each object part for the purpose of determining\ntheir relative location.\n3.1\nThe Context-Free Architecture\nWe build a siamese-ennead convolutional network (see1 Fig. 3), where each row\nup to the \ufb01rst fully connected layer (fc6) uses the AlexNet architecture [25] with\nshared weights. Similar schemes were used in prior work [10,39,2]. The outputs\nof all fc6 layers are concatenated and given as input to fc7. All the layers in the\nrows share the same weights up to and including fc6.\nWe call this architecture the context-free network (CFN) because the data\n\ufb02ow of each patch is explicitly separated until the fully connected layer and\ncontext is handled only in the last fully connected layers. We verify that this\narchitecture performs as well as AlexNet in the classi\ufb01cation task on the Im-\nageNet 2012 dataset [8]. In this test we resize the input images to 225 \u00d7 225\npixels, split them into a 3 \u00d7 3 grid and then feed the full 75 \u00d7 75 tiles to the\nnetwork. We \ufb01nd that the CFN achieves 57.1% top-1 accuracy while AlexNet\nachieves 57.4% top-1 accuracy. However, the CFN architecture is more compact\nthan AlexNet. It depends on only 27.5M parameters, while AlexNet uses 61M\nparameters. The fc6 layer includes 4\u00d74\u00d7256\u00d7512 \u223c2M parameters while the\nfc6 layer of AlexNet includes 6 \u00d7 6 \u00d7 256 \u00d7 4096 \u223c37.5M parameters. However,\nthe fc7 layer in our architecture includes 2M parameters more than the same\nlayer in AlexNet.\nThis network can thus be used interchangeably for di\ufb00erent tasks including\ndetection and classi\ufb01cation. In the next section we show how to train the CFN\nfor the Jigsaw puzzle reassembly.\n3.2\nThe Jigsaw Puzzle Task\nTo train the CFN we de\ufb01ne a set of Jigsaw puzzle permutations, e.g., a tile\ncon\ufb01guration S = (3, 1, 2, 9, 5, 4, 8, 7, 6), and assign an index to each entry. We\nrandomly pick one such permutation, rearrange the 9 input patches according to\n1 In earlier versions of this publication we reported transfer learning results where\nAlexNet had a stride 2 in the \ufb01rst convolutional layer as used during the training\nfor the puzzle task. This arXiv version introduces new updated results. See Fig. 3\ncaption for more information and the Experiments section.\nUnsupervised Learning of Visual Representations by Solving Jigsaw Puzzles\n7\nFig. 3: Context Free Network. The \ufb01gure illustrates how a puzzle is generated\nand solved. We randomly crop a 225 \u00d7 225 pixel window from an image (red\ndashed box), divide it into a 3 \u00d7 3 grid, and randomly pick a 64 \u00d7 64 pixel tiles\nfrom each 75 \u00d7 75 pixel cell. These 9 tiles are reordered via a randomly chosen\npermutation from a prede\ufb01ned permutation set and are then fed to the CFN. The\ntask is to predict the index of the chosen permutation (technically, we de\ufb01ne as\noutput a probability vector with 1 at the 64-th location and 0 elsewhere). The\nCFN is a siamese-ennead CNN. For simplicity, we do not indicate the max-\npooling and ReLU layers. These shared layers are implemented exactly as in\nAlexNet [25]. In the transfer learning experiments we show results with\nthe trained weights transferred on AlexNet (precisely, stride 4 on the\n\ufb01rst layer). The training in the transfer learning experiment is the\nsame as in the other competing methods. Notice instead, that during\nthe training on the puzzle task, we set the stride of the \ufb01rst layer of\nthe CFN to 2 instead of 4.\nthat permutation, and ask the CFN to return a vector with the probability value\nfor each index. Given 9 tiles, there are 9! = 362,880 possible permutations. From\nour experimental validation, we found that the permutation set is an important\nfactor on the performance of the representation that the network learns. We\nperform an ablation study on the impact of the permutation set in subsection 4.2.\n3.3\nTraining the CFN\nThe output of the CFN can be seen as the conditional probability density func-\ntion (pdf) of the spatial arrangement of object parts (or scene parts) in a part-\nbased model, i.e.,\np(S|A1, A2, . . . , A9) = p(S|F1, F2, . . . , F9)\n9\nY\ni=1\np(Fi|Ai)\n(1)\nwhere S is the con\ufb01guration of the tiles, Ai is the i-th part appearance of the\nobject, and {Fi}i=1,...,9 form the intermediate feature representation. Our ob-\njective is to train the CFN so that the features Fi have semantic attributes that\ncan identify the relative position between parts.\n8\nM. Noroozi and P. Favaro\nGiven the limited amount of data that we can use to build an approximation\nof this very high-dimensional pdf, close attention must be paid to the training\nstrategy. One problem is when the CFN learns to associate each appearance Ai to\nan absolute position. In this case, the features Fi would carry no semantic\nmeaning, but just information about an arbitrary 2D position. This\nproblem could happen if we generate just 1 Jigsaw puzzle per image. Then, the\nCFN could learn to cluster patches only based on their absolute position in the\npuzzle, and not on their textural/structural content. If we write the con\ufb01guration\nS as a list of tile positions S = (L1, . . . , L9) then in this case the conditional pdf\np(S|F1, F2, . . . , F9) would factorize into independent terms\np(L1, . . . , L9|F1, F2, . . . , F9) =\n9\nY\ni=1\np(Li|Fi)\n(2)\nwhere each tile location Li is fully determined by the corresponding feature Fi.\nMore in general, a self-supervised learning system might lead to representa-\ntions that are suitable to solve the pre-text task, but not the target tasks, e.g.,\nobject classi\ufb01cation, detection, and segmentation. In this regard, an important\nfactor to learn better representations is to prevent our model from taking these\nundesirable solutions, such as the one just described above, to solve the pre-text\ntask. We call these solutions shortcuts. Other shortcuts that the model can use to\nsolve the Jigsaw puzzle task include exploiting low-level statistics, such as edge\ncontinuity, the pixel intensity/color distribution, and chromatic aberration.\nTo avoid shortcuts we employ multiple techniques. To prevent mapping the\nappearance to an absolute position we feed multiple Jigsaw puzzles of the same\nimage to the CFN (an average of 69 out of 1000 possible puzzle con\ufb01gurations)\nand make sure that the tiles are shu\ufb04ed as much as possible by choosing con\ufb01g-\nurations with su\ufb03ciently large average Hamming distance. In this way the same\ntile would have to be assigned to multiple positions (possibly all 9) thus mak-\ning the mapping of features Fi to any absolute position equally likely. To avoid\nshortcuts due to edge continuity and pixel intensity distribution we also leave a\nrandom gap between the tiles. This discourages the CFN from learning low-level\nstatistics and was also done in [10]. During training we resize each input image\nuntil either the height or the width matches 256 pixels and preserve the origi-\nnal aspect ratio. Then, we crop a random region from the resized image of size\n225 \u00d7 225 and split it into a 3 \u00d7 3 grid of 75 \u00d7 75 pixels tiles. We then extract a\n64 \u00d7 64 region from each tile by introducing random shifts and feed them to the\nnetwork. Thus, we have an average gap of 11 pixels between the tiles. However,\nthe gaps may range from a minimum of 0 pixels to a maximum of 22 pixels. To\navoid shortcuts due to chromatic aberration we jitter the color channels and use\ngrayscale images (see more details in the Experiments section). In subsection 4.2\nwe perform ablation studies on the techniques we use to prevent the shortcuts.\nWe used Ca\ufb00e [23] and modi\ufb01ed the code to choose random image patches\nand permutations during the training time. This allowed us to keep the dataset\nsmall (1.3M images from ImageNet) and the training e\ufb03cient, while the CFN\nUnsupervised Learning of Visual Representations by Solving Jigsaw Puzzles\n9\ncould see an average of 69 di\ufb00erent puzzles per image (that is about 90M di\ufb00erent\nJigsaw puzzles).\n3.4\nImplementation Details\nWe use stochastic gradient descent without batch normalization [21] on one\nTitan X GPU. The training uses 1.3M color images of 256\u00d7256 pixels and mini-\nbatches with a batch size of 256 images. The images are resized by preserving\nthe aspect ratio until either the height or the width matches 256 pixels. Then\nthe other dimension is cropped to 256 pixels. The training converges after 350K\niterations with a basic learning rate of 0.01 and takes 59.5 hours in total (\u223c2.5\ndays). If we take 122% = 3072cores@1000Mhz\n2880cores@875Mhz = 6,144GFLOPS\n5,040GFLOPS as the best possible\nperformance ratio between the Titan X and the Tesla K40 (used for [10]) we can\npredict that the CFN would have taken \u223c72.5 hours (\u223c3 days) on a Tesla K40.\nWe compute that on average each image is used 350K \u00d7 256/1.3M \u224369 times.\nThat is, we solve on average 69 Jigsaw puzzles per image.\n4\nExperiments\nWe \ufb01rst evaluate the performance of our learned representations on di\ufb00erent\ntransfer learning benchmarks. We then perform ablation studies on our proposed\nmethod. We also visualize the neurons of the intermediate layers of our network.\nFinally, we compare our features with those of [10,39] both qualitatively and\nquantitatively on image retrieval.\n4.1\nTransfer Learning\nWe evaluate our learned features as pre-trained weights for classi\ufb01cation, detec-\ntion, and semantic segmentation tasks on the PASCAL VOC dataset[12]. We\nalso introduce a novel benchmark to evaluate methods for unsupervised/self-\nsupervised representation learning. After training the CFN on the self-supervised\nlearning task, we use the CFN weights to initialize all the conv layers of a stan-\ndard AlexNet network (stride 4 on the \ufb01rst layer). Then, we retrain the rest of\nthe network from scratch (Gaussian noise as initial weights) for object classi\ufb01-\ncation on ImageNet dataset. Notice that while during the training of the Jigsaw\ntask we use stride 2 in the \ufb01rst layer of our CFN, we use a standard AlexNet\n(stride 4 on the \ufb01rst layer) to make the comparison with competing methods in\nall the experiments directly comparable.\nPascal VOC We \ufb01ne-tune the Jigsaw task features on the classi\ufb01cation task\non PASCAL VOC 2007 by using the framework of Kr\u00a8ahenb\u00a8uhl et al. [24] and\non the object detection task by using the Fast R-CNN [16] framework. We also\n\ufb01ne-tune our weights for the semantic segmentation task using the framework\n[27] on the PASCAL VOC 2012 dataset. Because our fully connected layers are\n10\nM. Noroozi and P. Favaro\nTable 1: Results on PASCAL VOC 2007 Detection and Classi\ufb01cation. The results\nof the other methods are taken from Pathak et al. [30].\nMethod\nPretraining time\nSupervision\nClassi\ufb01cation\nDetection\nSegmentation\nKrizhevskyet al. [25]\n3 days\n1000 class labels\n78.2%\n56.8%\n48.0%\nWang and Gupta[39]\n1 week\nmotion\n58.4%\n44.0%\n-\nDoersch et al. [10]\n4 weeks\ncontext\n55.3%\n46.6%\n-\nPathak et al. [30]\n14 hours\ncontext\n56.5%\n44.5%\n29.7%\nOurs\n2.5 days\ncontext\n67.6%\n53.2%\n37.6%\ndi\ufb00erent from those of the standard AlexNet, we select one row of the CFN (up\nto conv5), copy only the weights of the convolutional layers, and \ufb01ll the fully\nconnected layers with Gaussian random weights with mean 0.1 and standard\ndeviation 0.001. The results are summarized in Table 1.\nOur features achieve 53.2% mAP using multi-scale training and testing,\n67.6% in classi\ufb01cation, and 37.6% in semantic segmentation thus outperforming\nall other methods and closing the gap with features obtained with supervision.\nImageNet Classi\ufb01cation Yosinski et al. [41] have shown that the last layers\nof AlexNet are speci\ufb01c to the task and dataset used for training, while the \ufb01rst\nlayers are general-purpose. In the context of transfer learning, this transition\nfrom general-purpose to task-speci\ufb01c determines where in the network one should\nextract the features. In this section we try to understand where this transition\noccurs in our learned representation. We repurpose our weights, [10], and [39] to\nthe classi\ufb01cation task on the ImageNet 2012 dataset [8]. Table 2 summarizes the\nresults. The analysis consists of training each network with the labeled data from\nImageNet 2012 by locking a subset of the layers and by initializing the unlocked\nlayers with random values. If we train AlexNet, we obtain the reference maximum\naccuracy of 57.4%. Our method achieves 34.6% when only fully connected layers\nare trained. There is a signi\ufb01cant improvement (from 34.6% to 45.3%) when\nthe conv5 layer is also trained. This shows that the conv5 layer starts to be\nspecialized on the Jigsaw puzzle reassembly task.\nWe also perform a novel experiment to understand whether semantic clas-\nsi\ufb01cation is useful to solve Jigsaw puzzles, and thus to see how much object\nclassi\ufb01cation and Jigsaw puzzle reassembly tasks are related. We take the pre-\ntrained AlexNet and transfer its features to solve Jigsaw puzzles. We also use\nthe same locking scheme to see the transferability of features at di\ufb00erent layers.\nThe performance is shown in Table 3. Compared to the maximum accuracy of\nthe Jigsaw task, 88%, we can see that semantic training is quite helpful towards\nrecognizing object parts. Indeed, the performance is very high up to conv4.\n4.2\nAblation Studies\nWe perform ablation studies on our proposed methods to show the impact of\neach component during the training of Jigsaw task. We train under di\ufb00erent\nUnsupervised Learning of Visual Representations by Solving Jigsaw Puzzles\n11\nTable 2: Comparison of classi\ufb01cation results on ImageNet 2012 [9]. The numbers\nare obtained by averaging 10 random crops predictions.\nconv1\nconv2\nconv3\nconv4\nconv5\nCFN\n54.7\n52.8\n49.7\n45.3\n34.6\nDoersch et al. [10]\n53.1\n47.6\n48.7\n45.6\n30.4\nWang and Gupta [39]\n51.8\n46.9\n42.8\n38.8\n29.8\nRandom\n48.5\n41.0\n34.8\n27.1\n12.0\nTable 3: Transfer learning of AlexNet from a classi\ufb01cation task to the Jigsaw\npuzzle reassembly problem. The j-th column indicates that all layers from conv1\nto conv-j were locked and all subsequent layers were randomly initialized and\nretrained. Notice how the \ufb01rst 4 layers provide very good features for solving\npuzzles. This shows that object classi\ufb01cation and the Jigsaw puzzle problems\nare related.\nconv1\nconv2\nconv3\nconv4\nconv5\nAlexNet [25]\n88\n87\n86\n83\n74\nscenarios and evaluate the performance on detection task on PASCAL VOC\n2007.\nPermutation Set. The permutation set controls the ambiguity of the task.\nIf the permutations are close to each other, the Jigsaw puzzle task is more\nchallenging and ambiguous. For example, if the di\ufb00erence between two di\ufb00erent\npermutations lies only in the position of two tiles and there are two similar\ntiles in the image, the prediction of the right solution will be impossible. The\nchallenge here is a weaker version of what happens in the method of Doersch et\nal. [10]. To show this issue quantitatively, we compare the performance of the\nlearned representation on the PASCAL VOC 2007 detection task by generating\nseveral permutation sets based on the following three criteria:\nI) Cardinality. We train the network with a di\ufb00erent number of permutations\nand see what impact this has on the learned features. We \ufb01nd that as the total\nnumber of permutations increases, the training on the Jigsaw task becomes more\nand more di\ufb03cult. Also, we \ufb01nd that the performance of the detection task\nincreases with a growing number of permutations.\nII) Average Hamming distance. We use a subset of 1000 permutations and select\nthem based on their Hamming distance (i.e., the number of di\ufb00erent tile loca-\ntions between 2 permutations S1 and S2). One can see that the average Ham-\nming distance between permutations controls the di\ufb03culty of the Jigsaw puzzle\n12\nM. Noroozi and P. Favaro\nTable 4: Ablation study on the impact of the permutation set.\nNumber of\nAverage hamming\nMinimum hamming\nJigsaw task\nDetection\npermutations\ndistance\ndistance\naccuracy\nperformance\n1000\n8.00\n2\n71\n53.2\n1000\n6.35\n2\n62\n51.3\n1000\n3.99\n2\n54\n50.2\n100\n8.08\n2\n88\n52.6\n95\n8.08\n3\n90\n52.4\n85\n8.07\n4\n91\n52.7\n71\n8.07\n5\n92\n52.8\n35\n8.13\n6\n94\n52.6\n10\n8.57\n7\n97\n49.2\n7\n8.95\n8\n98\n49.6\n6\n9\n9\n99\n49.7\nTable 5: Ablation study on the impact of the shortcuts.\nGap\nNormalization\nColor jittering\nJigsaw task accuracy\nDetection performance\n\u0017\n\u0013\n\u0013\n98\n47.7\n\u0013\n\u0017\n\u0013\n90\n43.5\n\u0013\n\u0013\n\u0017\n89\n51.1\n\u0013\n\u0013\n\u0013\n88\n52.6\nAlgorithm 1. Generation of the maximal Hamming distance permutation set\nInput: N\n\\\\ number of permutations\nOutput: P\n\\\\ maximal permutation set\n1: \u00afP \u2190all permutations [ \u00afP1, . . . , \u00afP9!]\n\\\\ \u00afP is a 9 \u00d7 9! matrix\n2: P \u2190\u2205\n3: j \u223cU[1, 9!]\n\\\\ uniform sample out of 9! permutations\n4: i \u21901\n5: repeat\n6:\nP \u2190[P \u00afPj]\n\\\\ add permutation \u00afPj to P\n7:\n\u00afP \u2190[ \u00afP1, . . . , \u00afPj\u22121, \u00afPj+1, . . . ]\n\\\\ remove \u00afPj from \u00afP\n8:\nD \u2190Hamming(P, P \u2032)\n\\\\ D is an i \u00d7 (9! \u2212i) matrix\n9:\n\u00afD \u21901T D\n\\\\ \u00afD is a 1 \u00d7 (9! \u2212i) row vector\n10:\nj \u2190arg maxk \u00afDk\n\\\\ \u00afDk denotes the k-th entry of \u00afD\n11:\ni \u2190i + 1\n12: until i \u2264N\nreassembly task, and it also correlates with the object detection performance.\nWe \ufb01nd that as the average Hamming distance increases, the CFN yields lower\nJigsaw puzzle solving errors and lower object detection errors with \ufb01ne-tuning.\nIn the Experiments section we compare the performance on object detection of\nCFNs trained with 3 choices for the Hamming distance: minimal, average and\nmaximal (see Table 4). From those tests we can see that large Hamming dis-\ntances are desirable. We generate this permutation set iteratively via a greedy\nalgorithm. We begin with an empty permutation set and at each iteration select\nthe one that has the desired Hamming distance to the current permutation set.\nUnsupervised Learning of Visual Representations by Solving Jigsaw Puzzles\n13\nAlgorithm 1 provides more details about the algorithm. For the minimal and\nmiddle case, the arg maxk function at line 10 is replaced by arg mink and uni-\nform sampling respectively. Note that the permutation set is generated before\ntraining.\nIII) Minimum hamming distance. To increase the minimum possible distance\nbetween permutations, we remove similar permutations in a maximal set with\n100 initial entries. As argued before, the minimum distance helps to make the\ntask less ambiguous. The performance results showing the impact of each com-\nponent are summarized in Table 4. The best performing permutation set is a\ntrade o\ufb00between the number of permutations and how dissimilar they are from\neach other.\nThe outcome of this ablation study seems to point to the following \ufb01nal\nconsideration:\nA good self-supervised task is neither simple nor ambiguous.\nPreventing Shortcuts In a self-supervised learning method, shortcuts exploit\ninformation useful for solving the pre-text task, but not for a target task, such\nas detection. Similar to [10], we experimentally show that the CFN can take the\nfollowing shortcuts to solve the Jigsaw Puzzle task:\nLow level statistics. Adjacent patches include similar low-level statistics like the\nmean and standard deviation of the pixel intensities. This allows the model to\n\ufb01nd the arrangement of the patches. To avoid this shortcut, we normalize the\nmean and the standard deviation of each patch independently.\nEdge continuity. A strong cue to solve Jigsaw puzzles is the continuity of edges.\nWe select the 64\u00d764 pixel tiles randomly from the 85\u00d785 pixel cells. This allows\nus to have a 21 pixel gap between tiles.\nChromatic Aberration. Chromatic aberration is a relative spatial shift between\ncolor channels that increases from the images center to the borders. This type of\ndistortion helps the network to estimate the tile positions. To avoid this shortcut,\nwe use three techniques: i) We crop the central square of the original image and\nresize it to 255 \u00d7 255; ii) We train the network with both color and grayscale\nimages. Our training set is a composition of grayscale and color images with\na ratio of 30% to 70%; iii) We (spatially) jitter the color channels of the color\nimages of each tile randomly by \u00b10, \u00b11, \u00b12 pixels.\nTable 5 shows the performance of transfer learning our CFN, trained un-\nder di\ufb00erent combinations of the above techniques to avoid shortcuts, to the\ndetection task on Pascal VOC.\n14\nM. Noroozi and P. Favaro\n4.3\nCFN \ufb01lter activations\nSome recent work has devoted e\ufb00orts towards the visualization of CNNs to bet-\nter understand how they work and how we can exploit them [42,35,28,22]. Some\nof these works aim at obtaining the input image that best represents a category\naccording to a given neural network. This has shown that CNNs retain important\ninformation about the categories. Here instead we analyze the CFN by consid-\nering the units at each layer as object part detectors as in [15]. We extract 1M\npatches from the ImageNet validation set (20 randomly sampled 64\u00d764 patches)\nand feed them as input to the CFN. At each layer (conv1, conv2, conv3, conv4,\nconv5) we consider the outputs of one channel and compute their \u21131 norm. We\nthen rank the patches based on the \u21131 norm and select the top 16 ones that be-\nlong to di\ufb00erent images. Since each layer has several channels, we hand-pick the\n6 most signi\ufb01cant ones. In Fig. 4 we show the top-16 activation patches for only 6\nchannels per layer. These activations show that the CFN features correspond to\npatterns sharing similar shapes and that there is a good correspondence based\non object parts (in particular see the conv4 activations for dog parts). Some\nchannels seem to be good face detectors (see conv3, but the same detectors can\nbe seen in other channels, not shown, in conv4 and conv5) and others seem to\nbe good texture detectors (e.g., grass, water, fur). In Fig. 4(f) we also show the\n\ufb01lters of the conv1 layer of the CFN. We can see that these \ufb01lters are quite\nstrong and our transfer learning experiments in the next sections show that they\nare as e\ufb00ective as those trained in a supervised manner.\n4.4\nImage Retrieval\nWe also evaluate the features qualitatively (see Fig. 5) and quantitatively (see\nFig. 6) for image retrieval with a simple image ranking.\nWe \ufb01nd the nearest neighbors (NN) of pool5 features using the bounding\nboxes of the PASCAL VOC 2007 test set as query and bounding boxes of the\ntrainval set as the retrieval entries. We discard bounding boxes with fewer than\n10K pixels inside. In Fig. 5 we show some examples of image retrievals (top-4)\nobtained by ranking the images based on the inner product between normalized\nfeatures of a query image and normalized features of the retrieval set. We can see\nthat the features of the CFN are very sensitive to objects with similar shape and\noften these are within the same category. In Fig. 6 we compare CFN with the\npre-trained AlexNet, [10], [39], and AlexNet with random weights. The precision-\nrecall plots show that [10] and CFN features perform equally well. However,\nthe real potential of CFN features is demonstrated when the feature metric is\nlearned. In Table 2 we can see how CFN features surpass other features trained\nin an unsupervised way by a good margin. In that test the dataset (ImageNet)\nis more challenging because there are more categories and the bounding box is\nnot used.\nUnsupervised Learning of Visual Representations by Solving Jigsaw Puzzles\n15\n(a) conv1 activations\n(b) conv2 activations\n(c) conv3 activations\n(d) conv4 activations\n(e) conv5 activations\n(f) conv1 \ufb01lters without color jittering\n(g) conv1 \ufb01lters with color jittering\nFig. 4: Visualization of the top 16 activations for 6 units of the conv1, conv2,\nconv3, conv4, conv5 layers in our CFN trained without blocking chromatic aber-\nration. (f),(g) we show the \ufb01lters of conv1 trained without and with blocking\nchromatic aberration. The selection of the top activations is identical to the vi-\nsualization method of Girshick et al. [15], except that we compute the average\nresponse rather than the maximum. We show some of the most signi\ufb01cant units.\nWe can see that in the \ufb01rst (a) and second (b) layers the \ufb01lters specialize on\ndi\ufb00erent types of textures. On the third layer (c) the \ufb01lters become more special-\nized and we have a \ufb01rst face detector (later layers will also have face detectors in\nsome units) and some part detectors (e.g., the bottom corner of the butter\ufb02ies\nwing). On the fourth layer (d) we have already quite a number of part detectors.\nWe purposefully choose all the dog part detectors: head top, head center, neck,\nback legs, and front legs. Notice the intraclass variation of the parts. Lastly, the\n\ufb01fth convolutional layer (e) has some other part detectors and some scene part\ndetectors.\n16\nM. Noroozi and P. Favaro\n(a)\n(b)\n(c)\n(d)\n(e)\n(f)\nFig. 5: Image retrieval (qualitative evaluation). (a) query images; (b) top-4\nmatches with AlexNet; (c) top-4 matches with the CFN trained without block-\ning chromatic aberration; (d) top-4 matches with Doersch et al. [10]; (e) top-4\nmatches with Wang and Gupta [39]; (f) top-4 matches with AlexNet with ran-\ndom weights.\nRecall\n0\n0.2\n0.4\n0.6\n0.8\n1\nPrecision\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\nSupervised\nCFN\nDoersch et al\nWang and Gupta\nRandom\nFig. 6: Image retrieval (quantitative evaluation). We compare the precision-recall\nfor image retrieval on the PASCAL VOC 2007. The ranking of the retrieved\nimages is based on the inner products between normalized features extracted\nfrom a pre-trained AlexNet, the CFN, Doersch et al. [10], Wang and Gupta [39]\nand from AlexNet with random weights. The performance of CFN and [10] are\nvery similar when using this simple ranking metric. When the metric is instead\nlearned with two fully connected layers, then we see that CFN features yield a\nclearly higher performance than all other features from self-supervised learning\nmethods (see Table 2).\nUnsupervised Learning of Visual Representations by Solving Jigsaw Puzzles\n17\n5\nConclusions\nWe have introduced the context-free network (CFN), a CNN whose features\ncan be easily transferred between detection/classi\ufb01cation and Jigsaw puzzle re-\nassembly tasks. The network is trained in an unsupervised manner by using the\nJigsaw puzzle as a pretext task. We have built a training scheme that generates,\non average, 69 puzzles for 1.3M images and converges in only 2.5 days. The key\nidea is that by solving Jigsaw puzzles the CFN learns to identify each tile as an\nobject part and how parts are assembled in an object. The learned features are\nevaluated on both classi\ufb01cation and detection and the experiments show that\nwe outperform the previous state of the art. More importantly, the performance\nof these features is closing the gap with those learned in a supervised manner.\nWe believe that there is a lot of untapped potential in self-supervised learning\nand in the future it will provide a valid alternative to costly human annotation.\nReferences\n1. Agrawal, P., Girshick, R., Malik, J.: Analyzing the performance of multilayer neural\nnetworks for object recognition. ECCV (2014)\n2. Agrawal, P., Carreira, J., Malik, J.: Learning to see by moving. ICCV (2015)\n3. Barlow, H.B.: Unsupervised learning. Neural Computation (1989)\n4. Belkin, M., Niyogi, P.: Laplacian eigenmaps for dimensionality reduction and data\nrepresentation. Neural Computation (2003)\n5. Bengio, Y., Courville, A., Vincent, P.: Representation learning: A review and new\nperspectives. PAMI (2013)\n6. Boulard, H., Kamp, Y.: Auto-association by multilayer perceptrons and singular\nvalue decomposition. Biological Cybernetics (1988)\n7. Chen, D.M., Baatz, G., Koser, K., Tsai, S.S., Vedantham, R., Pylvanainen, T.,\nRoimela, K., Chen, X., Bach, J., Pollefeys, M., Girod, B., Grzeszczuk, R.: City-\nscale landmark identi\ufb01cation on mobile devices. CVPR (2011)\n8. Deng, J., Dong, W., Socher, R., Li, L.J., Li, K., Fei-Fei, L.: Imagenet: A large-scale\nhierarchical image database. CVPR (2009)\n9. Deng, J., Dong, W., Socher, R., Li, L.J., Li, K., , Fei-Fei, L.: Imagenet: A large-scale\nhierarchical image database. CVPR (2009)\n10. Doersch, C., Gupta, A., Efros, A.A.: Unsupervised visual representation learning\nby context prediction. ICCV (2015)\n11. Donahue, J., Jia, Y., Vinyals, O., Ho\ufb00man, J., Zhang, N., Tzeng, E., Darrell, T.:\nDecaf: A deep convolutional activation feature for generic visual recognition. ICML\n(2014)\n12. Everingham, M., Eslami, S.M.A., Gool, L.V., Williams, C.K.I., Winn, J., Zisser-\nman, A.: The pascal visual object classes challenge: A retrospective. IJCV (2014)\n13. Fergus, R., Perona, P., Zisserman, A.: Object class recognition by unsupervised\nscale-invariant learning. CVPR (2003)\n14. Freeman, H., Garder, L.: Apictorial jigsaw puzzles: The computer solution of a\nproblem in pattern recognition. IEEE Transactions on Electronic Computers (1964)\n15. Girshick, R., Donahue, J., Darrell, T., Malik, J.: Rich feature hierarchies for accu-\nrate object detection and semantic segmentation. CVPR (2014)\n16. Girshick, R.: Fast r-cnn. ICCV (2015)\n18\nM. Noroozi and P. Favaro\n17. Henriques, J.F., Caseiro, R., Martins, P., Batista, J.: High-speed tracking with\nkernelized correlation \ufb01lters. PAMI (2015)\n18. Hinton, G.E., Sejnowski, T.J.: Learning and relearning in boltzmann machines.\nParallel Distributed Processing: Explorations in the Microstructure of Cognition,\nVol. 1 (1986)\n19. Hinton, G.E., Zemel, R.S.: Autoencoders, minimum description length and\nhelmholtz free energy. NIPS (1993)\n20. Hooper, H.: The Hooper Visual Organization Test. Western Psychological Services,\nLos Angeles, CA (1983)\n21. Io\ufb00e, S., Szegedy, C.: Batch normalization: Accelerating deep network training by\nreducing internal covariate shift. ICML (2015)\n22. Jason, Y., Je\ufb00, C., Anh, N., Thomas, F., Hod, L.: Understanding neural networks\nthrough deep visualization. Deep Learning Workshop, ICML (2015)\n23. Jia, Y., Shelhamer, E., Donahue, J., Karayev, S., Long, J., Girshick, R., Guadar-\nrama, S., Darrell, T.: Ca\ufb00e: Convolutional architecture for fast feature embedding.\nACM-MM (2014)\n24. Kr\u00a8ahenb\u00a8uhl, P., Doersch, C., Donahue, J., Darrell, T.: Data-dependent initializa-\ntions of convolutional neural networks. ICLR (2016)\n25. Krizhevsky, A., Sutskever, I., Hinton, G.E.: Imagenet classi\ufb01cation with deep con-\nvolutional neural networks. NIPS (2012)\n26. Le, Q., Ranzato, M., Monga, R., Devin, M., Chen, K., Corrado, G., Dean, J.,\nNg, A.: Building high-level features using large scale unsupervised learning. ICML\n(2012)\n27. Long, J., Shelhamer, E., Darrell, T.: Fully convolutional networks for semantic\nsegmentation. In: CVPR (2015)\n28. Mahendran, A., Vedaldi, A.: Understanding deep image representations by invert-\ning them. CVPR (2015)\n29. Olshausen, B.A., Field, D.J.: \u201dsparse coding with an overcomplete basis set: A\nstrategy employed by v1? \u201d. Vision Research (1997)\n30. Pathak, D., Kr\u00a8ahenb\u00a8uhl, P., Donahue, J., Darrell, T., Efros, A.A.: Context en-\ncoders: Feature learning by inpainting. CVPR (2016)\n31. Pomeranz, D., Shemesh, M., Ben-Shahar, O.: A fully automated greedy square\njigsaw puzzle solver. CVPR (2011)\n32. Pomeranz, D.: Solving the square jigsaw problem. Ph.D. thesis, Ben-Gurion Uni-\nversity of the Negev (2012)\n33. Richardson, J., Vecchi, T.: A jigsaw-puzzle imagery task for assessing active visu-\nospatial processes in old and young people. Behavior Research Methods, Instru-\nments, & Computers (2002)\n34. Roweis, S.T., Saul, L.K.: Nonlinear dimensionality reduction by locally linear em-\nbedding. Science (2000)\n35. Simonyan, K., Vedaldi, A., Zisserman, A.: Deep inside convolutional networks:\nVisualising image classi\ufb01cation models and saliency maps. ICLR (2014)\n36. Simonyan, K., Zisserman, A.: Two-stream convolutional networks for action recog-\nnition in videos. NIPS (2014)\n37. Smolensky, P.: Information processing in dynamical systems: Foundations of har-\nmony theory. Parallel Distributed Processing (1986)\n38. Tybon, R.: Generating Solutions to the Jigsaw Puzzle Problem. Ph.D. thesis, Grif-\n\ufb01th University (2004)\n39. Wang, X., Gupta, A.: Unsupervised learning of visual representations using videos.\nICCV (2015)\nUnsupervised Learning of Visual Representations by Solving Jigsaw Puzzles\n19\n40. Weber, M., Weilling, M., Perona, P.: Unsupervised learning of models for recogni-\ntion. ECCV (2000)\n41. Yosinski, J., Clune, J., Bengio, Y., Lipson, H.: How transferable are features in\ndeep neural networks? NIPS (2014)\n42. Zeiler, M.D., Fergus, R.: Visualizing and understanding convolutional networks.\nECCV (2014)\n",
        "sentence": "",
        "context": "We believe that there is a lot of untapped potential in self-supervised learning\nand in the future it will provide a valid alternative to costly human annotation.\nReferences\nnition in videos. NIPS (2014)\n37. Smolensky, P.: Information processing in dynamical systems: Foundations of har-\nmony theory. Parallel Distributed Processing (1986)\n38. Tybon, R.: Generating Solutions to the Jigsaw Puzzle Problem. Ph.D. thesis, Grif-\nversity of the Negev (2012)\n33. Richardson, J., Vecchi, T.: A jigsaw-puzzle imagery task for assessing active visu-\nospatial processes in old and young people. Behavior Research Methods, Instru-\nments, & Computers (2002)"
    },
    {
        "title": "Learning and transferring mid-level image representations using convolutional neural networks",
        "author": [
            "M. Oquab",
            "L. Bottou",
            "I. Laptev",
            "J. Sivic"
        ],
        "venue": "In CVPR,",
        "citeRegEx": "Oquab et al\\.,? \\Q2014\\E",
        "shortCiteRegEx": "Oquab et al\\.",
        "year": 2014,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " Key to this success is their ability to produce features that easily transfer to new domains when trained on massive databases of labeled images (Razavian et al., 2014; Oquab et al., 2014) or weaklysupervised data (Joulin et al.",
        "context": null
    },
    {
        "title": "Context encoders: Feature learning by inpainting",
        "author": [
            "D. Pathak",
            "P. Krahenbuhl",
            "J. Donahue",
            "T. Darrell",
            "A. Efros"
        ],
        "venue": null,
        "citeRegEx": "Pathak et al\\.,? \\Q2016\\E",
        "shortCiteRegEx": "Pathak et al\\.",
        "year": 2016,
        "abstract": "We present an unsupervised visual feature learning algorithm driven by\ncontext-based pixel prediction. By analogy with auto-encoders, we propose\nContext Encoders -- a convolutional neural network trained to generate the\ncontents of an arbitrary image region conditioned on its surroundings. In order\nto succeed at this task, context encoders need to both understand the content\nof the entire image, as well as produce a plausible hypothesis for the missing\npart(s). When training context encoders, we have experimented with both a\nstandard pixel-wise reconstruction loss, as well as a reconstruction plus an\nadversarial loss. The latter produces much sharper results because it can\nbetter handle multiple modes in the output. We found that a context encoder\nlearns a representation that captures not just appearance but also the\nsemantics of visual structures. We quantitatively demonstrate the effectiveness\nof our learned features for CNN pre-training on classification, detection, and\nsegmentation tasks. Furthermore, context encoders can be used for semantic\ninpainting tasks, either stand-alone or as initialization for non-parametric\nmethods.",
        "full_text": "Context Encoders: Feature Learning by Inpainting\nDeepak Pathak\nPhilipp Kr\u00a8ahenb\u00a8uhl\nJeff Donahue\nTrevor Darrell\nAlexei A. Efros\nUniversity of California, Berkeley\n{pathak,philkr,jdonahue,trevor,efros}@cs.berkeley.edu\nAbstract\nWe present an unsupervised visual feature learning algo-\nrithm driven by context-based pixel prediction. By analogy\nwith auto-encoders, we propose Context Encoders \u2013 a con-\nvolutional neural network trained to generate the contents\nof an arbitrary image region conditioned on its surround-\nings.\nIn order to succeed at this task, context encoders\nneed to both understand the content of the entire image,\nas well as produce a plausible hypothesis for the missing\npart(s). When training context encoders, we have experi-\nmented with both a standard pixel-wise reconstruction loss,\nas well as a reconstruction plus an adversarial loss. The\nlatter produces much sharper results because it can better\nhandle multiple modes in the output. We found that a con-\ntext encoder learns a representation that captures not just\nappearance but also the semantics of visual structures. We\nquantitatively demonstrate the effectiveness of our learned\nfeatures for CNN pre-training on classi\ufb01cation, detection,\nand segmentation tasks. Furthermore, context encoders can\nbe used for semantic inpainting tasks, either stand-alone or\nas initialization for non-parametric methods.\n1. Introduction\nOur visual world is very diverse, yet highly structured,\nand humans have an uncanny ability to make sense of this\nstructure. In this work, we explore whether state-of-the-art\ncomputer vision algorithms can do the same. Consider the\nimage shown in Figure 1a. Although the center part of the\nimage is missing, most of us can easily imagine its content\nfrom the surrounding pixels, without having ever seen that\nexact scene. Some of us can even draw it, as shown on Fig-\nure 1b. This ability comes from the fact that natural images,\ndespite their diversity, are highly structured (e.g. the regular\npattern of windows on the facade). We humans are able to\nunderstand this structure and make visual predictions even\nwhen seeing only parts of the scene. In this paper, we show\nThe code, trained models and more inpainting results are available at\nthe author\u2019s project website.\n(a) Input context\n(b) Human artist\n(c) Context Encoder\n(L2 loss)\n(d) Context Encoder\n(L2 + Adversarial loss)\nFigure 1: Qualitative illustration of the task. Given an im-\nage with a missing region (a), a human artist has no trouble\ninpainting it (b). Automatic inpainting using our context\nencoder trained with L2 reconstruction loss is shown in (c),\nand using both L2 and adversarial losses in (d).\nthat it is possible to learn and predict this structure using\nconvolutional neural networks (CNNs), a class of models\nthat have recently shown success across a variety of image\nunderstanding tasks.\nGiven an image with a missing region (e.g., Fig. 1a), we\ntrain a convolutional neural network to regress to the miss-\ning pixel values (Fig. 1d). We call our model context en-\ncoder, as it consists of an encoder capturing the context of\nan image into a compact latent feature representation and a\ndecoder which uses that representation to produce the miss-\ning image content. The context encoder is closely related to\nautoencoders [3,20], as it shares a similar encoder-decoder\narchitecture.\nAutoencoders take an input image and try\n1\narXiv:1604.07379v2  [cs.CV]  21 Nov 2016\nto reconstruct it after it passes through a low-dimensional\n\u201cbottleneck\u201d layer, with the aim of obtaining a compact fea-\nture representation of the scene. Unfortunately, this feature\nrepresentation is likely to just compresses the image content\nwithout learning a semantically meaningful representation.\nDenoising autoencoders [38] address this issue by corrupt-\ning the input image and requiring the network to undo the\ndamage. However, this corruption process is typically very\nlocalized and low-level, and does not require much seman-\ntic information to undo. In contrast, our context encoder\nneeds to solve a much harder task: to \ufb01ll in large missing\nareas of the image, where it can\u2019t get \u201chints\u201d from nearby\npixels. This requires a much deeper semantic understanding\nof the scene, and the ability to synthesize high-level features\nover large spatial extents. For example, in Figure 1a, an en-\ntire window needs to be conjured up \u201cout of thin air.\u201d This\nis similar in spirit to word2vec [30] which learns word rep-\nresentation from natural language sentences by predicting a\nword given its context.\nLike autoencoders, context encoders are trained in a\ncompletely unsupervised manner. Our results demonstrate\nthat in order to succeed at this task, a model needs to both\nunderstand the content of an image, as well as produce a\nplausible hypothesis for the missing parts. This task, how-\never, is inherently multi-modal as there are multiple ways\nto \ufb01ll the missing region while also maintaining coherence\nwith the given context. We decouple this burden in our loss\nfunction by jointly training our context encoders to mini-\nmize both a reconstruction loss and an adversarial loss. The\nreconstruction (L2) loss captures the overall structure of the\nmissing region in relation to the context, while the the ad-\nversarial loss [16] has the effect of picking a particular mode\nfrom the distribution. Figure 1 shows that using only the re-\nconstruction loss produces blurry results, whereas adding\nthe adversarial loss results in much sharper predictions.\nWe evaluate the encoder and the decoder independently.\nOn the encoder side, we show that encoding just the con-\ntext of an image patch and using the resulting feature to\nretrieve nearest neighbor contexts from a dataset produces\npatches which are semantically similar to the original (un-\nseen) patch. We further validate the quality of the learned\nfeature representation by \ufb01ne-tuning the encoder for a va-\nriety of image understanding tasks, including classi\ufb01ca-\ntion, object detection, and semantic segmentation.\nWe\nare competitive with the state-of-the-art unsupervised/self-\nsupervised methods on those tasks. On the decoder side, we\nshow that our method is often able to \ufb01ll in realistic image\ncontent. Indeed, to the best of our knowledge, ours is the\n\ufb01rst parametric inpainting algorithm that is able to give rea-\nsonable results for semantic hole-\ufb01lling (i.e. large missing\nregions). The context encoder can also be useful as a bet-\nter visual feature for computing nearest neighbors in non-\nparametric inpainting methods.\n2. Related work\nComputer vision has made tremendous progress on se-\nmantic image understanding tasks such as classi\ufb01cation, ob-\nject detection, and segmentation in the past decade. Re-\ncently, Convolutional Neural Networks (CNNs) [13, 27]\nhave greatly advanced the performance in these tasks [15,\n26,28]. The success of such models on image classi\ufb01cation\npaved the way to tackle harder problems, including unsu-\npervised understanding and generation of natural images.\nWe brie\ufb02y review the related work in each of the sub-\ufb01elds\npertaining to this paper.\nUnsupervised learning\nCNNs trained for ImageNet [37]\nclassi\ufb01cation with over a million labeled examples learn\nfeatures which generalize very well across tasks [9]. How-\never, whether such semantically informative and gener-\nalizable features can be learned from raw images alone,\nwithout any labels, remains an open question.\nSome of\nthe earliest work in deep unsupervised learning are au-\ntoencoders [3, 20]. Along similar lines, denoising autoen-\ncoders [38] reconstruct the image from local corruptions, to\nmake encoding robust to such corruptions. While context\nencoders could be thought of as a variant of denoising au-\ntoencoders, the corruption applied to the model\u2019s input is\nspatially much larger, requiring more semantic information\nto undo.\nWeakly-supervised and self-supervised learning\nVery\nrecently, there has been signi\ufb01cant interest in learning\nmeaningful representations using weakly-supervised and\nself-supervised learning. One useful source of supervision\nis to use the temporal information contained in videos. Con-\nsistency across temporal frames has been used as supervi-\nsion to learn embeddings which perform well on a num-\nber of tasks [17, 34]. Another way to use consistency is to\ntrack patches in frames of video containing task-relevant at-\ntributes and use the coherence of tracked patches to guide\nthe training [39]. Ego-motion read off from non-vision sen-\nsors has been used as supervisory signal to train visual fea-\ntures et al. [1,21].\nMost closely related to the present paper are efforts at\nexploiting spatial context as a source of free and plentiful\nsupervisory signal. Visual Memex [29] used context to non-\nparametrically model object relations and to predict masked\nobjects in scenes, while [6] used context to establish cor-\nrespondences for unsupervised object discovery. However,\nboth approaches relied on hand-designed features and did\nnot perform any representation learning. Recently, Doer-\nsch et al. [7] used the task of predicting the relative positions\nof neighboring patches within an image as a way to train\nan unsupervised deep feature representations. We share the\nsame high-level goals with Doersch et al. but fundamentally\ndiffer in the approach: whereas [7] are solving a discrimina-\ntive task (is patch A above patch B or below?), our context\nencoder solves a pure prediction problem (what pixel inten-\nsities should go in the hole?). Interestingly, similar distinc-\ntion exist in using language context to learn word embed-\ndings: Collobert and Weston [5] advocate a discriminative\napproach, whereas word2vec [30] formulate it as word pre-\ndiction. One important bene\ufb01t of our approach is that our\nsupervisory signal is much richer: a context encoder needs\nto predict roughly 15,000 real values per training example,\ncompared to just 1 option among 8 choices in [7]. Likely\ndue in part to this difference, our context encoders take far\nless time to train than [7]. Moreover, context based predic-\ntion is also harder to \u201ccheat\u201d since low-level image features,\nsuch as chromatic aberration, do not provide any meaning-\nful information, in contrast to [7] where chromatic aberra-\ntion partially solves the task. On the other hand, it is not yet\nclear if requiring faithful pixel generation is necessary for\nlearning good visual features.\nImage generation\nGenerative models of natural images\nhave enjoyed signi\ufb01cant research interest [16, 24, 35]. Re-\ncently, Radford et al. [33] proposed new convolutional ar-\nchitectures and optimization hyperparameters for Genera-\ntive Adversarial Networks (GAN) [16] producing encour-\naging results. We train our context encoders using an ad-\nversary jointly with reconstruction loss for generating in-\npainting results. We discuss this in detail in Section 3.2.\nDosovitskiy et al. [10] and Rifai et al. [36] demonstrate\nthat CNNs can learn to generate novel images of particular\nobject categories (chairs and faces, respectively), but rely on\nlarge labeled datasets with examples of these categories. In\ncontrast, context encoders can be applied to any unlabeled\nimage database and learn to generate images based on the\nsurrounding context.\nInpainting and hole-\ufb01lling\nIt is important to point out\nthat our hole-\ufb01lling task cannot be handled by classical in-\npainting [4, 32] or texture synthesis\n[2, 11] approaches,\nsince the missing region is too large for local non-semantic\nmethods to work well. In computer graphics, \ufb01lling in large\nholes is typically done via scene completion [19], involv-\ning a cut-paste formulation using nearest neighbors from a\ndataset of millions of images. However, scene completion\nis meant for \ufb01lling in holes left by removing whole objects,\nand it struggles to \ufb01ll arbitrary holes, e.g. amodal comple-\ntion of partially occluded objects. Furthermore, previous\ncompletion relies on a hand-crafted distance metric, such as\nGist [31] for nearest-neighbor computation which is infe-\nrior to a learned distance metric. We show that our method\nis often able to inpaint semantically meaningful content in\na parametric fashion, as well as provide a better feature for\nnearest neighbor-based inpainting methods.\nFigure 2: Context Encoder. The context image is passed\nthrough the encoder to obtain features which are connected\nto the decoder using channel-wise fully-connected layer as\ndescribed in Section 3.1. The decoder then produces the\nmissing regions in the image.\n3. Context encoders for image generation\nWe now introduce context encoders: CNNs that predict\nmissing parts of a scene from their surroundings. We \ufb01rst\ngive an overview of the general architecture, then provide\ndetails on the learning procedure and \ufb01nally present various\nstrategies for image region removal.\n3.1. Encoder-decoder pipeline\nThe overall architecture is a simple encoder-decoder\npipeline. The encoder takes an input image with missing\nregions and produces a latent feature representation of that\nimage. The decoder takes this feature representation and\nproduces the missing image content. We found it important\nto connect the encoder and the decoder through a channel-\nwise fully-connected layer, which allows each unit in the\ndecoder to reason about the entire image content. Figure 2\nshows an overview of our architecture.\nEncoder\nOur encoder is derived from the AlexNet archi-\ntecture [26]. Given an input image of size 227\u00d7227, we use\nthe \ufb01rst \ufb01ve convolutional layers and the following pooling\nlayer (called pool5) to compute an abstract 6 \u00d7 6 \u00d7 256\ndimensional feature representation. In contrast to AlexNet,\nour model is not trained for ImageNet classi\ufb01cation; rather,\nthe network is trained for context prediction \u201cfrom scratch\u201d\nwith randomly initialized weights.\nHowever, if the encoder architecture is limited only to\nconvolutional layers, there is no way for information to di-\nrectly propagate from one corner of the feature map to an-\nother. This is so because convolutional layers connect all\nthe feature maps together, but never directly connect all lo-\ncations within a speci\ufb01c feature map. In the present archi-\ntectures, this information propagation is handled by fully-\nconnected or inner product layers, where all the activations\nare directly connected to each other. In our architecture, the\nlatent feature dimension is 6 \u00d7 6 \u00d7 256 = 9216 for both\nencoder and decoder. This is so because, unlike autoen-\ncoders, we do not reconstruct the original input and hence\nneed not have a smaller bottleneck. However, fully connect-\ning the encoder and decoder would result in an explosion in\nthe number of parameters (over 100M!), to the extent that\nef\ufb01cient training on current GPUs would be dif\ufb01cult. To\nalleviate this issue, we use a channel-wise fully-connected\nlayer to connect the encoder features to the decoder, de-\nscribed in detail below.\nChannel-wise fully-connected layer\nThis layer is essen-\ntially a fully-connected layer with groups, intended to prop-\nagate information within activations of each feature map. If\nthe input layer has m feature maps of size n \u00d7 n, this layer\nwill output m feature maps of dimension n \u00d7 n. However,\nunlike a fully-connected layer, it has no parameters connect-\ning different feature maps and only propagates information\nwithin feature maps. Thus, the number of parameters in\nthis channel-wise fully-connected layer is mn4, compared\nto m2n4 parameters in a fully-connected layer (ignoring the\nbias term). This is followed by a stride 1 convolution to\npropagate information across channels.\nDecoder\nWe now discuss the second half of our pipeline,\nthe decoder, which generates pixels of the image using\nthe encoder features.\nThe \u201cencoder features\u201d are con-\nnected to the \u201cdecoder features\u201d using a channel-wise fully-\nconnected layer.\nThe channel-wise fully-connected layer is followed by\na series of \ufb01ve up-convolutional layers [10, 28, 40] with\nlearned \ufb01lters, each with a recti\ufb01ed linear unit (ReLU) acti-\nvation function. A up-convolutional is simply a convolution\nthat results in a higher resolution image. It can be under-\nstood as upsampling followed by convolution (as described\nin [10]), or convolution with fractional stride (as described\nin [28]). The intuition behind this is straightforward \u2013 the\nseries of up-convolutions and non-linearities comprises a\nnon-linear weighted upsampling of the feature produced by\nthe encoder until we roughly reach the original target size.\n3.2. Loss function\nWe train our context encoders by regressing to the\nground truth content of the missing (dropped out) region.\nHowever, there are often multiple equally plausible ways to\n\ufb01ll a missing image region which are consistent with the\ncontext. We model this behavior by having a decoupled\njoint loss function to handle both continuity within the con-\ntext and multiple modes in the output. The reconstruction\n(L2) loss is responsible for capturing the overall structure of\nthe missing region and coherence with regards to its context,\nbut tends to average together the multiple modes in predic-\ntions. The adversarial loss [16], on the other hand, tries\nto make prediction look real, and has the effect of picking a\nparticular mode from the distribution. For each ground truth\n(a) Central region\n(b) Random block\n(c) Random region\nFigure 3: An example of image x with our different region\nmasks \u02c6\nM applied, as described in Section 3.3.\nimage x, our context encoder F produces an output F(x).\nLet \u02c6\nM be a binary mask corresponding to the dropped im-\nage region with a value of 1 wherever a pixel was dropped\nand 0 for input pixels. During training, those masks are au-\ntomatically generated for each image and training iterations,\nas described in Section 3.3. We now describe different com-\nponents of our loss function.\nReconstruction Loss\nWe use a normalized masked L2\ndistance as our reconstruction loss function, Lrec,\nLrec(x) = \u2225\u02c6\nM \u2299(x \u2212F((1 \u2212\u02c6\nM) \u2299x))\u22252\n2,\n(1)\nwhere \u2299is the element-wise product operation. We experi-\nmented with both L1 and L2 losses and found no signi\ufb01cant\ndifference between them. While this simple loss encour-\nages the decoder to produce a rough outline of the predicted\nobject, it often fails to capture any high frequency detail\n(see Fig. 1c). This stems from the fact that the L2 (or L1)\nloss often prefer a blurry solution, over highly accurate tex-\ntures. We believe this happens because it is much \u201csafer\u201d\nfor the L2 loss to predict the mean of the distribution, be-\ncause this minimizes the mean pixel-wise error, but results\nin a blurry averaged image. We alleviated this problem by\nadding an adversarial loss.\nAdversarial Loss\nOur adversarial loss is based on Gener-\native Adversarial Networks (GAN) [16]. To learn a genera-\ntive model G of a data distribution, GAN proposes to jointly\nlearn an adversarial discriminative model D to provide loss\ngradients to the generative model. G and D are paramet-\nric functions (e.g., deep networks) where G : Z \u2192X\nmaps samples from noise distribution Z to data distribution\nX. The learning procedure is a two-player game where an\nadversarial discriminator D takes in both the prediction of\nG and ground truth samples, and tries to distinguish them,\nwhile G tries to confuse D by producing samples that ap-\npear as \u201creal\u201d as possible. The objective for discriminator is\nlogistic likelihood indicating whether the input is real sam-\nFigure 4: Semantic Inpainting results on held-out images for context encoder trained using reconstruction and adversarial\nloss. First three rows are examples from ImageNet, and bottom two rows are from Paris StreetView Dataset. See more results\non author\u2019s project website.\nple or predicted one:\nmin\nG max\nD\nEx\u2208X [log(D(x))] + Ez\u2208Z[log(1 \u2212D(G(z)))]\nThis method has recently shown encouraging results in\ngenerative modeling of images [33]. We thus adapt this\nframework for context prediction by modeling generator by\ncontext encoder; i.e., G \u225cF. To customize GANs for this\ntask, one could condition on the given context information;\ni.e., the mask \u02c6\nM \u2299x. However, conditional GANs don\u2019t\ntrain easily for context prediction task as the adversarial dis-\ncriminator D easily exploits the perceptual discontinuity in\ngenerated regions and the original context to easily classify\npredicted versus real samples. We thus use an alternate for-\nmulation, by conditioning only the generator (not the dis-\ncriminator) on context.\nWe also found results improved\nwhen the generator was not conditioned on a noise vector.\nHence the adversarial loss for context encoders, Ladv, is\nLadv = max\nD\nEx\u2208X [log(D(x))\n+ log(1 \u2212D(F((1 \u2212\u02c6\nM) \u2299x)))],\n(2)\nwhere, in practice, both F and D are optimized jointly us-\ning alternating SGD. Note that this objective encourages the\nentire output of the context encoder to look realistic, not just\nthe missing regions as in Equation (1).\nJoint Loss\nWe de\ufb01ne the overall loss function as\nL = \u03bbrecLrec + \u03bbadvLadv.\n(3)\nCurrently, we use adversarial loss only for inpainting exper-\niments as AlexNet [26] architecture training diverged with\njoint adversarial loss. Details follow in Sections 5.1, 5.2.\n3.3. Region masks\nThe input to a context encoder is an image with one or\nmore of its regions \u201cdropped out\u201d; i.e., set to zero, assuming\nzero-centered inputs. The removed regions could be of any\nshape, we present three different strategies here:\nCentral region The simplest such shape is the central\nsquare patch in the image, as shown in Figure 3a. While this\nworks quite well for inpainting, the network learns low level\nimage features that latch onto the boundary of the central\nmask. Those low level image features tend not to generalize\nwell to images without masks, hence the features learned\nare not very general.\nInput Context\nContext Encoder Content-Aware Fill\nFigure 5: Comparison with Content-Aware Fill (Photoshop\nfeature based on [2]) on held-out images.\nOur method\nworks better in semantic cases (top row) and works slightly\nworse in textured settings (bottom row).\nRandom block To prevent the network from latching on\nthe the constant boundary of the masked region, we ran-\ndomize the masking process. Instead of choosing a sin-\ngle large mask at a \ufb01xed location, we remove a number of\nsmaller possibly overlapping masks, covering up to 1\n4 of the\nimage. An example of this is shown in Figure 3b. How-\never, the random block masking still has sharp boundaries\nconvolutional features could latch onto.\nRandom region To completely remove those bound-\naries, we experimented with removing arbitrary shapes\nfrom images, obtained from random masks in the PASCAL\nVOC 2012 dataset [12]. We deform those shapes and paste\nin arbitrary places in the other images (not from PASCAL),\nagain covering up to 1\n4 of the image. Note that we com-\npletely randomize the region masking process, and do not\nexpect or want any correlation between the source segmen-\ntation mask and the image. We merely use those regions to\nprevent the network from learning low-level features corre-\nsponding to the removed mask. See example in Figure 3c.\nIn practice, we found region and random block masks\nproduce a similarly general feature, while signi\ufb01cantly out-\nperforming the central region features. We use the random\nregion dropout for all our feature based experiments.\n4. Implementation details\nThe pipeline was implemented in Caffe [22] and Torch.\nWe used the recently proposed stochastic gradient descent\nsolver, ADAM [23] for optimization. The missing region in\nthe masked input image is \ufb01lled with constant mean value.\nHyper-parameter details are discussed in Sections 5.1, 5.2.\nPool-free encoders We experimented with replacing all\npooling layers with convolutions of the same kernel size\nand stride. The overall stride of the network remains the\nsame, but it results in \ufb01ner inpainting. Intuitively, there is\nno reason to use pooling for reconstruction based networks.\nMethod\nMean L1 Loss\nMean L2 Loss\nPSNR (higher better)\nNN-inpainting (HOG features)\n19.92%\n6.92%\n12.79 dB\nNN-inpainting (our features)\n15.10%\n4.30%\n14.70 dB\nOur Reconstruction (joint)\n09.37%\n1.96%\n18.58 dB\nTable 1:\nSemantic Inpainting accuracy for Paris StreetView\ndataset on held-out images. NN inpainting is basis for [19].\nIn classi\ufb01cation, pooling provides spatial invariance, which\nmay be detrimental for reconstruction-based training. To be\nconsistent with prior work, we still use the original AlexNet\narchitecture (with pooling) for all feature learning results.\n5. Evaluation\nWe now evaluate the encoder features for their seman-\ntic quality and transferability to other image understanding\ntasks. We experiment with images from two datasets: Paris\nStreetView [8] and ImageNet [37] without using any of the\naccompanying labels. In Section 5.1, we present visualiza-\ntions demonstrating the ability of the context encoder to \ufb01ll\nin semantic details of images with missing regions. In Sec-\ntion 5.2, we demonstrate the transferability of our learned\nfeatures to other tasks, using context encoders as a pre-\ntraining step for image classi\ufb01cation, object detection, and\nsemantic segmentation. We compare our results on these\ntasks with those of other unsupervised or self-supervised\nmethods, demonstrating that our approach outperforms pre-\nvious methods.\n5.1. Semantic Inpainting\nWe train context encoders with the joint loss function de-\n\ufb01ned in Equation (3) for the task of inpainting the missing\nregion. The encoder and discriminator architecture is simi-\nlar to that of discriminator in [33], and decoder is similar to\ngenerator in [33]. However, the bottleneck is of 4000 units\n(in contrast to 100 in [33]); see supplementary material. We\nused the default solver hyper-parameters suggested in [33].\nWe use \u03bbrec = 0.999 and \u03bbadv = 0.001. However, a few\nthings were crucial for training the model. We did not con-\ndition the adversarial loss (see Section 3.2) nor did we add\nnoise to the encoder. We use a higher learning rate for con-\ntext encoder (10 times) to that of adversarial discriminator.\nTo further emphasize the consistency of prediction with the\ncontext, we predict a slightly larger patch that overlaps with\nthe context (by 7px). During training, we use higher weight\n(10\u00d7) for the reconstruction loss in this overlapping region.\nThe qualitative results are shown in Figure 4. Our model\nperforms generally well in inpainting semantic regions of\nan image.\nHowever, if a region can be \ufb01lled with low-\nlevel textures, texture synthesis methods, such as [2, 11],\ncan often perform better (e.g. Figure 5). For semantic in-\npainting, we compare against nearest neighbor inpainting\n(which forms the basis of Hays et al. [19]) and show that\nImage\nOurs(L2)\nOurs(Adv)\nOurs(L2+Adv)\nNN-Inpainting w/ our features\nNN-Inpainting w/ HOG\nFigure 6: Semantic Inpainting using different methods on held-out images. Context Encoder with just L2 are well aligned,\nbut not sharp. Using adversarial loss, results are sharp but not coherent. Joint loss alleviate the weaknesses of each of them.\nThe last two columns are the results if we plug-in the best nearest neighbor (NN) patch in the masked region.\nour reconstructions are well-aligned semantically, as seen\non Figure 6. It also shows that joint loss signi\ufb01cantly im-\nproves the inpainting over both reconstruction and adver-\nsarial loss alone. Moreover, using our learned features in\na nearest-neighbor style inpainting can sometimes improve\nresults over a hand-designed distance metrics. Table 1 re-\nports quantitative results on StreetView Dataset.\n5.2. Feature Learning\nFor\nconsistency\nwith\nprior\nwork,\nwe\nuse\nthe\nAlexNet [26] architecture for our encoder.\nUnfortu-\nnately, we did not manage to make the adversarial loss\nconverge with AlexNet, so we used just the reconstruction\nloss. The networks were trained with a constant learning\nrate of 10\u22123 for the center-region masks.\nHowever, for\nrandom region corruption, we found a learning rate of 10\u22124\nto perform better. We apply dropout with a rate of 0.5 just\nfor the channel-wise fully connected layer, since it has\nmore parameters than other layers and might be prone to\nover\ufb01tting. The training process is fast and converges in\nabout 100K iterations: 14 hours on a Titan X GPU. Figure 7\nshows inpainting results for context encoder trained with\nrandom region corruption using reconstruction loss.\nTo\nevaluate the quality of features, we \ufb01nd nearest neighbors\nFigure 7: Arbitrary region inpainting for context encoder\ntrained with reconstruction loss on held-out images.\nto the masked part of image just by using the features from\nthe context, see Figure 8. Note that none of the methods\never see the center part of any image, whether a query\nor dataset image.\nOur features retrieve decent nearest\nneighbors just from context, even though actual prediction\nis blurry with L2 loss.\nAlexNet features also perform\ndecently as they were trained with 1M labels for semantic\ntasks, HOG on the other hand fail to get the semantics.\n5.2.1\nClassi\ufb01cation pre-training\nFor this experiment, we \ufb01ne-tune a standard AlexNet clas-\nsi\ufb01er on the PASCAL VOC 2007 [12] from a number of su-\npervised, self-supervised and unsupervised initializations.\nWe train the classi\ufb01er using random cropping, and then\nevaluate it using 10 random crops per test image. We av-\nerage the classi\ufb01er output over those random crops. Table 2\nshows the standard mean average precision (mAP) score for\nall compared methods.\nA random initialization performs roughly 25% below\nan ImageNet-trained model; however, it does not use any\nlabels. Context encoders are competitive with concurrent\nself-supervised feature learning methods [7,39] and signif-\nicantly outperform autoencoders and Agrawal et al. [1].\n5.2.2\nDetection pre-training\nOur second set of quantitative results involves using our\nfeatures for object detection.\nWe use Fast R-CNN [14]\nframework (FRCN). We replace the ImageNet pre-trained\nnetwork with our context encoders (or any other baseline\nmodel).\nIn particular, we take the pre-trained encoder\nweights up to the pool5 layer and re-initialize the fully-\nOurs\nOurs\nHOG\nHOG\nAlexNet\nAlexNet\nFigure 8: Context Nearest Neighbors. Center patches whose context (not shown here) are close in the embedding space\nof different methods (namely our context encoder, HOG and AlexNet). Note that the appearance of these center patches\nthemselves was never seen by these methods. But our method brings them close just from their context.\nPretraining Method\nSupervision\nPretraining time\nClassi\ufb01cation\nDetection\nSegmentation\nImageNet [26]\n1000 class labels\n3 days\n78.2%\n56.8%\n48.0%\nRandom Gaussian\ninitialization\n< 1 minute\n53.3%\n43.4%\n19.8%\nAutoencoder\n-\n14 hours\n53.8%\n41.9%\n25.2%\nAgrawal et al. [1]\negomotion\n10 hours\n52.9%\n41.8%\n-\nWang et al. [39]\nmotion\n1 week\n58.7%\n47.4%\n-\nDoersch et al. [7]\nrelative context\n4 weeks\n55.3%\n46.6%\n-\nOurs\ncontext\n14 hours\n56.5%\n44.5%\n30.0%\nTable 2: Quantitative comparison for classi\ufb01cation, detection and semantic segmentation. Classi\ufb01cation and Fast-RCNN\nDetection results are on the PASCAL VOC 2007 test set. Semantic segmentation results are on the PASCAL VOC 2012\nvalidation set from the FCN evaluation described in Section 5.2.3, using the additional training data from [18], and removing\noverlapping images from the validation set [28].\nconnected layers. We then follow the training and evalu-\nation procedures from FRCN and report the accuracy (in\nmAP) of the resulting detector.\nOur results on the test set of the PASCAL VOC 2007 [12]\ndetection challenge are reported in Table 2. Context en-\ncoder pre-training is competitive with the existing meth-\nods achieving signi\ufb01cant boost over the baseline. Recently,\nKr\u00a8ahenb\u00a8uhl et al. [25] proposed a data-dependent method\nfor rescaling pre-trained model weights. This signi\ufb01cantly\nimproves the features in Doersch et al. [7] up to 65.3%\nfor classi\ufb01cation and 51.1% for detection. However, this\nrescaling doesn\u2019t improve results for other methods, includ-\ning ours.\n5.2.3\nSemantic Segmentation pre-training\nOur last quantitative evaluation explores the utility of con-\ntext encoder training for pixel-wise semantic segmentation.\nFully convolutional networks [28] (FCNs) were proposed as\nan end-to-end learnable method of predicting a semantic la-\nbel at each pixel of an image, using a convolutional network\npre-trained for ImageNet classi\ufb01cation. We replace the clas-\nsi\ufb01cation pre-trained network used in the FCN method with\nour context encoders, afterwards following the FCN train-\ning and evaluation procedure for direct comparison with\ntheir original CaffeNet-based result.\nOur results on the PASCAL VOC 2012 [12] validation\nset are reported in Table 2. In this setting, we outperform a\nrandomly initialized network as well as a plain autoencoder\nwhich is trained simply to reconstruct its full input.\n6. Conclusion\nOur context encoders trained to generate images condi-\ntioned on context advance the state of the art in semantic\ninpainting, at the same time learn feature representations\nthat are competitive with other models trained with auxil-\niary supervision.\nAcknowledgements\nThe authors would like to thank\nAmanda Buster for the artwork on Fig. 1b, as well as Shub-\nham Tulsiani and Saurabh Gupta for helpful discussions.\nThis work was supported in part by DARPA, AFRL, In-\ntel, DoD MURI award N000141110688, NSF awards IIS-\n1212798, IIS-1427425, and IIS-1536003, the Berkeley Vi-\nsion and Learning Center and Berkeley Deep Drive.\nReferences\n[1] P. Agrawal, J. Carreira, and J. Malik. Learning to see by\nmoving. ICCV, 2015. 2, 7, 8\n[2] C. Barnes, E. Shechtman, A. Finkelstein, and D. Goldman.\nPatchmatch: A randomized correspondence algorithm for\nstructural image editing. ACM Transactions on Graphics,\n2009. 3, 6\n[3] Y. Bengio. Learning deep architectures for ai. Foundations\nand trends in Machine Learning, 2009. 1, 2\n[4] M. Bertalmio, G. Sapiro, V. Caselles, and C. Ballester. Image\ninpainting. In Computer graphics and interactive techniques,\n2000. 3\n[5] R. Collobert and J. Weston. A uni\ufb01ed architecture for natural\nlanguage processing: Deep neural networks with multitask\nlearning. In ICML, 2008. 3\n[6] C. Doersch, A. Gupta, and A. A. Efros. Context as supervi-\nsory signal: Discovering objects with predictable context. In\nECCV, 2014. 2\n[7] C. Doersch, A. Gupta, and A. A. Efros. Unsupervised visual\nrepresentation learning by context prediction. ICCV, 2015.\n2, 3, 7, 8\n[8] C. Doersch, S. Singh, A. Gupta, J. Sivic, and A. Efros. What\nmakes paris look like paris? ACM Transactions on Graphics,\n2012. 6\n[9] J. Donahue, Y. Jia, O. Vinyals, J. Hoffman, N. Zhang,\nE. Tzeng, and T. Darrell. Decaf: A deep convolutional ac-\ntivation feature for generic visual recognition. ICML, 2014.\n2\n[10] A. Dosovitskiy, J. T. Springenberg, and T. Brox. Learning to\ngenerate chairs with convolutional neural networks. CVPR,\n2015. 3, 4\n[11] A. Efros and T. K. Leung.\nTexture synthesis by non-\nparametric sampling. In ICCV, 1999. 3, 6\n[12] M. Everingham, S. A. Eslami, L. Van Gool, C. K. Williams,\nJ. Winn, and A. Zisserman. The Pascal Visual Object Classes\nchallenge: A retrospective. IJCV, 2014. 6, 7, 8\n[13] K. Fukushima. Neocognitron: A self-organizing neural net-\nwork model for a mechanism of pattern recognition unaf-\nfected by shift in position. Biological cybernetics, 1980. 2\n[14] R. Girshick. Fast r-cnn. ICCV, 2015. 7\n[15] R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich fea-\nture hierarchies for accurate object detection and semantic\nsegmentation. In CVPR, 2014. 2\n[16] I. Goodfellow,\nJ. Pouget-Abadie,\nM. Mirza,\nB. Xu,\nD. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. Gen-\nerative adversarial nets. In NIPS, 2014. 2, 3, 4\n[17] R. Goroshin, J. Bruna, J. Tompson, D. Eigen, and Y. LeCun.\nUnsupervised learning of spatiotemporally coherent metrics.\nICCV, 2015. 2\n[18] B. Hariharan, P. Arbel\u00b4aez, L. Bourdev, S. Maji, and J. Malik.\nSemantic contours from inverse detectors. In ICCV, 2011. 8\n[19] J. Hays and A. A. Efros. Scene completion using millions of\nphotographs. SIGGRAPH, 2007. 3, 6\n[20] G. E. Hinton and R. R. Salakhutdinov. Reducing the dimen-\nsionality of data with neural networks. Science, 2006. 1,\n2\n[21] D. Jayaraman and K. Grauman. Learning image representa-\ntions tied to ego-motion. In ICCV, 2015. 2\n[22] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. B.\nGirshick, S. Guadarrama, and T. Darrell. Caffe: Convolu-\ntional architecture for fast feature embedding. In ACM Mul-\ntimedia, 2014. 6\n[23] D. Kingma and J. Ba. Adam: A method for stochastic opti-\nmization. ICLR, 2015. 6\n[24] D. P. Kingma and M. Welling. Auto-encoding variational\nbayes. ICLR, 2014. 3\n[25] P. Kr\u00a8ahenb\u00a8uhl, C. Doersch, J. Donahue, and T. Darrell. Data-\ndependent initializations of convolutional neural networks.\nICLR, 2016. 8\n[26] A. Krizhevsky, I. Sutskever, and G. E. Hinton. ImageNet\nclassi\ufb01cation with deep convolutional neural networks. In\nNIPS, 2012. 2, 3, 5, 7, 8, 10\n[27] Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E.\nHoward, W. Hubbard, and L. D. Jackel. Backpropagation\napplied to handwritten zip code recognition. Neural compu-\ntation, 1989. 2\n[28] J. Long, E. Shelhamer, and T. Darrell. Fully convolutional\nnetworks for semantic segmentation. In CVPR, 2015. 2, 4, 8\n[29] T. Malisiewicz and A. Efros. Beyond categories: The visual\nmemex model for reasoning about object relationships. In\nNIPS, 2009. 2\n[30] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and\nJ. Dean. Distributed representations of words and phrases\nand their compositionality. In NIPS, 2013. 2, 3\n[31] A. Oliva and A. Torralba. Building the gist of a scene: The\nrole of global image features in recognition.\nProgress in\nbrain research, 2006. 3\n[32] S. Osher, M. Burger, D. Goldfarb, J. Xu, and W. Yin. An it-\nerative regularization method for total variation-based image\nrestoration. Multiscale Modeling & Simulation, 2005. 3\n[33] A. Radford, L. Metz, and S. Chintala. Unsupervised repre-\nsentation learning with deep convolutional generative adver-\nsarial networks. ICLR, 2016. 3, 5, 6, 10\n[34] V. Ramanathan, K. Tang, G. Mori, and L. Fei-Fei. Learn-\ning temporal embeddings for complex video analysis. ICCV,\n2015. 2\n[35] M. Ranzato, V. Mnih, J. M. Susskind, and G. E. Hinton.\nModeling natural images using gated mrfs. PAMI, 2013. 3\n[36] S. Rifai, Y. Bengio, A. Courville, P. Vincent, and M. Mirza.\nDisentangling factors of variation for facial expression\nrecognition. In ECCV, 2012. 3\n[37] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh,\nS. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein,\nA. C. Berg, and L. Fei-Fei. Imagenet large scale visual recog-\nnition challenge. IJCV, 2015. 2, 6\n[38] P. Vincent, H. Larochelle, Y. Bengio, and P.-A. Manzagol.\nExtracting and composing robust features with denoising au-\ntoencoders. In ICML, 2008. 2\n[39] X. Wang and A. Gupta. Unsupervised learning of visual rep-\nresentations using videos. ICCV, 2015. 2, 7, 8\n[40] M. D. Zeiler and R. Fergus. Visualizing and understanding\nconvolutional networks. In ECCV, 2014. 4\nSupplementary Material\nIn this section, we present the architectural details of our\ncontext-encoders, and show additional qualitative results.\nContext encoders are not only able to inpaint semantic de-\ntails in the missing part of an input image, but also learn\nfeatures transferable to other tasks. We discuss the imple-\nmentation details for each of these in following sections.\nA. Semantic Inpainting\nContext encoders for inpainting are trained jointly with\nreconstruction and adversarial loss as discussed in Sec-\ntion 5.1. The inpainting results are slightly worse if we use\n227 \u00d7 227 directly. So, we resize images to 128 \u00d7 128\nand then train our joint loss with the resized images. The\nencoder and discriminator architecture is similar to that of\ndiscriminator in [33], and decoder is similar to generator\nin [33]; the bottleneck is of 4000 units. We used batch\nnormalization in both context encoder and discriminator.\nReLU [26] non-linearity is used in decoder, while leaky\nReLU [33] is used in both encoder and discriminator.\nIn case of arbitrary region inpainting, adversarial dis-\ncriminator compares the full real image and the full gen-\nerated image. We do not condition the adversarial discrimi-\nnator with mask, see (2). If the discriminator sees the mask,\nit \ufb01gures out the perceptual discontinuity of generated part\nfrom the real part and easily classi\ufb01es the real v/s the gen-\nerated image, i.e., the process doesn\u2019t train. Moreover, par-\nticularly for center region inpainting, this process can be\ncomputationally simpli\ufb01ed by producing center only and\nnot showing discriminator the context boundary (or in other\nwords, not showing the mask). The exact architecture for\ncenter region dropout is shown in Figure 9a.\nB. Feature Learning\nWe use the AlexNet [26] architecture for encoder so that\nwe can compare the learned features with the prior works,\nwhich are trained using Imagenet labels and other un/self-\nsupervised techniques. The encoder is Alexnet until pool5,\nfollowed by channel-wise fully connected layer and decoder\nis a series of upconvolutional layers until we reach the tar-\nget size. The input image size is 227 \u00d7 227. Unfortunately,\nwe couldn\u2019t train adversary with Alexnet Encoder, so it is\ntrained with reconstruction loss. See Figure 9b for exact\narchitecture details. For pre-training experiments in Sec-\ntion 5.2, we randomly initialize the fully-connected lay-\ners, i.e., fc6 and fc7, while starting from context encoder\nweights.\nC. Additional Results\nFinally, we show additional inpainting results using our\ncontext-encoders in Figure 10. These results, in compari-\nson to nearest-neighbor inpainting, show that: (a) The fea-\ntures learned by context-encoder are semantically meaning-\nful and retrieve neighboring patches just by looking at the\ncontext. This is also veri\ufb01ed quantitatively in Table 2. (b)\nOur context encoder doesn\u2019t memorize the examples from\ntraining set. It rather produces realistic and coherent in-\npainting results which are much better than nearest neighbor\ninpainting both qualitatively (Figure 10) and quantitatively\n(Table 1).\nReconstruc*on  \nLoss (L2) \n64 \n64 \n64 \n32 \n32 64 \n128 \n256 \n512 \n16 \n8 \n4 \n8 \n4 \n4000 \n  4x4  \n(conv) \n  4x4  \n(conv) \n  4x4  \n(conv) \n  4x4  \n(conv) \n  4x4  \n(conv) \n512 \n4 \n4 \n  4x4  \n(uconv) \n256 \n8 \n8 \n  4x4  \n(uconv) \n128 \n16 \n  4x4  \n(uconv) \n32 \n32 \n  4x4  \n(uconv) \n  4x4  \n(uconv) \n64 \n64 \n64 \n  4x4  \n(conv) \n16 \n16 \n32 \n32 64 \n128 \n256 \n512 \n16 \n8 \n4 \n8 \n4 \n  4x4  \n(conv) \n  4x4  \n(conv) \n  4x4  \n(conv) \n  4x4  \n(conv) \n  4x4  \n(conv) \n16 \n64 \n64 \n128 \n128 \nreal  \nor  \nfake \nEncoder \nDecoder \nAdversarial Discriminator \n(a) Context encoder trained with joint reconstruction and adversarial loss for semantic inpainting. This illustration is shown for center region dropout.\nSimilar architecture holds for arbitrary region dropout as well. See Section 3.2.\nReconstruc*on  \nLoss (L2) \n9216 \n256 \n6 \n6 \n(reshape) \n128 \n11 \n11 \n  5x5 \n(uconv) \n64 \n21 \n  5x5  \n(uconv) \n41 \n41 \n  5x5  \n(uconv) \n  5x5  \n(uconv) \n32 \n21 \n227 \n227 \nEncoder \nDecoder \n     AlexNet  \n(un*l pool5) \n64 \n81 \n81 \n  5x5  \n(uconv) \n3 \n161 \n161 \n227 \n227 \n9216 \n(resize) \nChannel-wise \nFully  \nConnected \n(b) Context encoder trained with reconstruction loss for feature learning by \ufb01lling in arbitrary region dropouts in the input.\nFigure 9: Context encoder training architectures.\nImage\nOurs(L2)\nOurs(Adv)\nOurs(L2+Adv)\nNN-Inpainting w/ our features\nNN-Inpainting w/ HOG\nFigure 10: Semantic Inpainting using different methods on held-out images. Context Encoder with just L2 are well aligned,\nbut not sharp. Using adversarial loss, results are sharp but not coherent. Joint loss alleviate the weaknesses of each of them.\nThe last two columns are the results if we plug-in the best nearest neighbor (NN) patch in the masked region.\nFigure 10: Semantic Inpainting using different methods on held-out images. Context Encoder with just L2 are well aligned,\nbut not sharp. Using adversarial loss, results are sharp but not coherent. Joint loss alleviate the weaknesses of each of them.\nThe last two columns are the results if we plug-in the best nearest neighbor (NN) patch in the masked region.\n",
        "sentence": " Recently, a lot of work has explored self-supervison: leveraging supervision contained in the input signal (Doersch et al., 2015; Noroozi & Favaro, 2016; Pathak et al., 2016).",
        "context": "self-supervised learning. One useful source of supervision\nis to use the temporal information contained in videos. Con-\nsistency across temporal frames has been used as supervi-\nsion to learn embeddings which perform well on a num-\nsors has been used as supervisory signal to train visual fea-\ntures et al. [1,21].\nMost closely related to the present paper are efforts at\nexploiting spatial context as a source of free and plentiful\nlanguage processing: Deep neural networks with multitask\nlearning. In ICML, 2008. 3\n[6] C. Doersch, A. Gupta, and A. A. Efros. Context as supervi-\nsory signal: Discovering objects with predictable context. In\nECCV, 2014. 2"
    },
    {
        "title": "Learning to segment object candidates",
        "author": [
            "P.O. Pinheiro",
            "R. Collobert",
            "P. Dollar"
        ],
        "venue": "In NIPS,",
        "citeRegEx": "Pinheiro et al\\.,? \\Q2015\\E",
        "shortCiteRegEx": "Pinheiro et al\\.",
        "year": 2015,
        "abstract": "Recent object detection systems rely on two critical steps: (1) a set of\nobject proposals is predicted as efficiently as possible, and (2) this set of\ncandidate proposals is then passed to an object classifier. Such approaches\nhave been shown they can be fast, while achieving the state of the art in\ndetection performance. In this paper, we propose a new way to generate object\nproposals, introducing an approach based on a discriminative convolutional\nnetwork. Our model is trained jointly with two objectives: given an image\npatch, the first part of the system outputs a class-agnostic segmentation mask,\nwhile the second part of the system outputs the likelihood of the patch being\ncentered on a full object. At test time, the model is efficiently applied on\nthe whole test image and generates a set of segmentation masks, each of them\nbeing assigned with a corresponding object likelihood score. We show that our\nmodel yields significant improvements over state-of-the-art object proposal\nalgorithms. In particular, compared to previous approaches, our model obtains\nsubstantially higher object recall using fewer proposals. We also show that our\nmodel is able to generalize to unseen categories it has not seen during\ntraining. Unlike all previous approaches for generating object masks, we do not\nrely on edges, superpixels, or any other form of low-level segmentation.",
        "full_text": "Learning to Segment Object Candidates\nPedro O. Pinheiro\u2217\nRonan Collobert\nPiotr Doll\u00b4ar\npedro@opinheiro.com\nlocronan@fb.com\npdollar@fb.com\nFacebook AI Research\nAbstract\nRecent object detection systems rely on two critical steps: (1) a set of object pro-\nposals is predicted as ef\ufb01ciently as possible, and (2) this set of candidate proposals\nis then passed to an object classi\ufb01er. Such approaches have been shown they can\nbe fast, while achieving the state of the art in detection performance. In this pa-\nper, we propose a new way to generate object proposals, introducing an approach\nbased on a discriminative convolutional network. Our model is trained jointly\nwith two objectives: given an image patch, the \ufb01rst part of the system outputs a\nclass-agnostic segmentation mask, while the second part of the system outputs the\nlikelihood of the patch being centered on a full object. At test time, the model\nis ef\ufb01ciently applied on the whole test image and generates a set of segmenta-\ntion masks, each of them being assigned with a corresponding object likelihood\nscore. We show that our model yields signi\ufb01cant improvements over state-of-the-\nart object proposal algorithms. In particular, compared to previous approaches,\nour model obtains substantially higher object recall using fewer proposals. We\nalso show that our model is able to generalize to unseen categories it has not seen\nduring training. Unlike all previous approaches for generating object masks, we\ndo not rely on edges, superpixels, or any other form of low-level segmentation.\n1\nIntroduction\nObject detection is one of the most foundational tasks in computer vision [21]. Until recently, the\ndominant paradigm in object detection was the sliding window framework: a classi\ufb01er is applied at\nevery object location and scale [4, 8, 32]. More recently, Girshick et al. [10] proposed a two-phase\napproach. First, a rich set of object proposals (i.e., a set of image regions which are likely to contain\nan object) is generated using a fast (but possibly imprecise) algorithm. Second, a convolutional\nneural network classi\ufb01er is applied on each of the proposals. This approach provides a notable gain\nin object detection accuracy compared to classic sliding window approaches. Since then, most state-\nof-the-art object detectors for both the PASCAL VOC [7] and ImageNet [5] datasets rely on object\nproposals as a \ufb01rst preprocessing step [10, 15, 33].\nObject proposal algorithms aim to \ufb01nd diverse regions in an image which are likely to contain\nobjects. For ef\ufb01ciency and detection performance reasons, an ideal proposal method should possess\nthree key characteristics: (i) high recall (i.e., the proposed regions should contain the maximum\nnumber of possible objects), (ii) the high recall should be achieved with the minimum number of\nregions possible, and (iii) the proposed regions should match the objects as accurately as possible.\nIn this paper, we present an object proposal algorithm based on Convolutional Networks (Con-\nvNets) [20] that satis\ufb01es these constraints better than existing approaches. ConvNets are an im-\nportant class of algorithms which have been shown to be state of the art in many large scale object\nrecognition tasks. They can be seen as a hierarchy of trainable \ufb01lters, interleaved with non-linearities\n\u2217Pedro O. Pinheiro is with the Idiap Research Institute in Martigny, Switzerland and Ecole Polytechnique\nF\u00b4ed\u00b4erale de Lausanne (EPFL) in Lausanne, Switzerland. This work was done during an internship at FAIR.\n1\narXiv:1506.06204v2  [cs.CV]  1 Sep 2015\nand pooling. ConvNets saw a resurgence after Krizhevsky et al. [18] demonstrated that they per-\nform very well on the ImageNet classi\ufb01cation benchmark. Moreover, these models learn suf\ufb01ciently\ngeneral image features, which can be transferred to many different tasks [10, 11, 3, 22, 23].\nGiven an input image patch, our algorithm generates a class-agnostic mask and an associated score\nwhich estimates the likelihood of the patch fully containing a centered object (without any notion\nof an object category). The core of our model is a ConvNet which jointly predicts the mask and the\nobject score. A large part of the network is shared between those two tasks: only the last few network\nlayers are specialized for separately outputting a mask and score prediction. The model is trained by\noptimizing a cost function that targets both tasks simultaneously. We train on MS COCO [21] and\nevaluate the model on two object detection datasets, PASCAL VOC [7] and MS COCO.\nBy leveraging powerful ConvNet feature representations trained on ImageNet and adapted on the\nlarge amount of segmented training data available in COCO, we are able to beat the state of the art\nin object proposals generation under multiple scenarios. Our most notable achievement is that our\napproach beats other methods by a large margin while considering a smaller number of proposals.\nMoreover, we demonstrate the generalization capabilities of our model by testing it on object cate-\ngories not seen during training. Finally, unlike all previous approaches for generating segmentation\nproposals, we do not rely on edges, superpixels, or any other form of low-level segmentation. Our\napproach is the \ufb01rst to learn to generate segmentation proposals directly from raw image data.\nThe paper is organized as follows: \u00a72 presents related work, \u00a73 describes our architecture choices,\nand \u00a74 describes our experiments in different datasets. We conclude in \u00a75.\n2\nRelated Work\nIn recent years, ConvNets have been widely used in the context of object recognition. Notable\nsystems are AlexNet [18] and more recently GoogLeNet [29] and VGG [27], which perform ex-\nceptionally well on ImageNet. In the setting of object detection, Girshick et al. [10] proposed\nR-CNN, a ConvNet-based model that beats by a large margin models relying on hand-designed\nfeatures. Their approach can be divided into two steps: selection of a set of salient object propos-\nals [31], followed by a ConvNet classi\ufb01er [18, 27]. Currently, most state-of-the-art object detection\napproaches [30, 12, 9, 25] rely on this pipeline. Although they are slightly different in the classi\ufb01-\ncation step, they all share the \ufb01rst step, which consist of choosing a rich set of object proposals.\nMost object proposal approaches leverage low-level grouping and saliency cues. These approaches\nusually fall into three categories: (1) objectness scoring [1, 34], in which proposals are extracted by\nmeasuring the objectness score of bounding boxes, (2) seed segmentation [14, 16, 17], where models\nstart with multiple seed regions and generate separate foreground-background segmentation for each\nseed, and (3) superpixel merging [31, 24], where multiple over-segmentations are merged according\nto various heuristics. These models vary in terms of the type of proposal generated (bounding boxes\nor segmentation masks) and if the proposals are ranked or not. For a more complete survey of object\nproposal methods, we recommend the recent survey from Hosang et al. [13].\nAlthough our model shares high level similarities with these approaches (we generate a set of ranked\nsegmentation proposals), these results are achieved quite differently. All previous approaches for\ngenerating segmentation masks, including [17] which has a learning component, rely on low-level\nsegmentations such as superpixels or edges. Instead, we propose a data-driven discriminative ap-\nproach based on a deep-network architecture to obtain our segmentation proposals.\nMost closely related to our approach, Multibox [6, 30] proposed to train a ConvNet model to gen-\nerate bounding box object proposals. Their approach, similar to ours, generates a set of ranked\nclass-agnostic proposals. However, our model generates segmentation proposals instead of the less\ninformative bounding box proposals. Moreover, the model architectures, training scheme, etc., are\nquite different between our approach and [30]. More recently, Deepbox [19] proposed a ConvNet\nmodel that learns to rerank proposals generated by EdgeBox, a bottom-up method for bounding box\nproposals. This system shares some similarities to our scoring network. Our model, however, is\nable to generate the proposals and rank them in one shot from the test image, directly from the pixel\nspace. Finally, concurrently with this work, Ren et al. [25] proposed \u2018region proposal networks\u2019 for\ngenerating box proposals that shares similarities with our work. We emphasize, however, that unlike\nall these approaches our method generates segmentation masks instead of bounding boxes.\n2\nVGG#\n1x1#\nconv#\n2x2#\npool#\n#\nx:#3x224x224#\n512x14x14#\n512x7x7#\n512x1x1#\n1024x1x1#\nfsegm(x):#224x224#\nfscore(x):#1x1#\n512x14x14#\n512x1x1#\n56x56#\nFigure 1: (Top) Model architecture: the network is split into two branches after the shared feature\nextraction layers. The top branch predicts a segmentation mask for the the object located at the\ncenter while the bottom branch predicts an object score for the input patch. (Bottom) Examples\nof training triplets: input patch x, mask m and label y. Green patches contain objects that satisfy\nthe speci\ufb01ed constraints and therefore are assigned the label y = 1. Note that masks for negative\nexamples (shown in red) are not used and are shown for illustrative purposes only.\n3\nDeepMask Proposals\nOur object proposal method predicts a segmentation mask given an input patch, and assigns a score\ncorresponding to how likely the patch is to contain an object.\nBoth mask and score predictions are achieved with a single convolutional network. ConvNets are\n\ufb02exible models which can be applied to various computer vision tasks and they alleviate the need\nfor manually designed features. Their \ufb02exible nature allows us to design a model in which the two\ntasks (mask and score predictions) can share most of the layers of the network. Only the last layers\nare task-speci\ufb01c (see Figure 1). During training, the two tasks are learned jointly. Compared to a\nmodel which would have two distinct networks for the two tasks, this architecture choice reduces\nthe capacity of the model and increases the speed of full scene inference at test time.\nEach sample k in the training set is a triplet containing (1) the RGB input patch xk, (2) the binary\nmask corresponding to the input patch mk (with mij\nk \u2208{\u00b11}, where (i, j) corresponds to a pixel\nlocation on the input patch) and (3) a label yk \u2208{\u00b11} which speci\ufb01es whether the patch contains\nan object. Speci\ufb01cally, a patch xk is given label yk = 1 if it satis\ufb01es the following constraints:\n(i) the patch contains an object roughly centered in the input patch\n(ii) the object is fully contained in the patch and in a given scale range\nOtherwise, yk = \u22121, even if an object is partially present. The positional and scale tolerance used in\nour experiments are given shortly. Assuming yk = 1, the ground truth mask mk has positive values\nonly for the pixels that are part of the single object located in the center of the patch. If yk = \u22121 the\nmask is not used. Figure 1, bottom, shows examples of training triplets.\nFigure 1, top, illustrates an overall view of our model, which we call DeepMask. The top branch is\nresponsible for predicting a high quality object segmentation mask and the bottom branch predicts\nthe likelihood that an object is present and satis\ufb01es the above two constraints. We next describe in\ndetail each part of the architecture, the training procedure, and the fast inference procedure.\n3.1\nNetwork Architecture\nThe parameters for the layers shared between the mask prediction and the object score prediction are\ninitialized with a network that was pre-trained to perform classi\ufb01cation on the ImageNet dataset [5].\nThis model is then \ufb01ne-tuned for generating object proposals during training. We choose the VGG-\nA architecture [27] which consists of eight 3 \u00d7 3 convolutional layers (followed by ReLU non-\nlinearities) and \ufb01ve 2 \u00d7 2 max-pooling layers and has shown excellent performance.\n3\nAs we are interested in inferring segmentation masks, the spatial information provided in the con-\nvolutional feature maps is important. We therefore remove all the \ufb01nal fully connected layers of the\nVGG-A model. Additionally we also discard the last max-pooling layer. The output of the shared\nlayers has a downsampling factor of 16 due to the remaining four 2 \u00d7 2 max-pooling layers; given\nan input image of dimension 3 \u00d7 h \u00d7 w, the output is a feature map of dimensions 512 \u00d7 h\n16 \u00d7 w\n16.\nSegmentation: The branch of the network dedicated to segmentation is composed of a single 1 \u00d7 1\nconvolution layer (and ReLU non-linearity) followed by a classi\ufb01cation layer. The classi\ufb01cation\nlayer consists of h\u00d7w pixel classi\ufb01ers, each responsible for indicating whether a given pixel belongs\nto the object in the center of the patch. Note that each pixel classi\ufb01er in the output plane must be\nable to utilize information contained in the entire feature map, and thus have a complete view of the\nobject. This is critical because unlike in semantic segmentation, our network must output a mask for\na single object even when multiple objects are present (e.g., see the elephants in Fig. 1).\nFor the classi\ufb01cation layer one could use either locally or fully connected pixel classi\ufb01ers. Both\noptions have drawbacks: in the former each classi\ufb01er has only a partial view of the object while\nin the latter the classi\ufb01ers have a massive number of redundant parameters. Instead, we opt to\ndecompose the classi\ufb01cation layer into two linear layers with no non-linearity in between. This\ncan be viewed as a \u2018low-rank\u2019 variant of using fully connected linear classi\ufb01ers. Such an approach\nmassively reduces the number of network parameters while allowing each pixel classi\ufb01er to leverage\ninformation from the entire feature map. Its effectiveness is shown in the experiments. Finally, to\nfurther reduce model capacity, we set the output of the classi\ufb01cation layer to be ho\u00d7wo with ho < h\nand wo < w and upsample the output to h \u00d7 w to match the input dimensions.\nScoring: The second branch of the network is dedicated to predicting if an image patch satis\ufb01es\nconstraints (i) and (ii): that is if an object is centered in the patch and at the appropriate scale. It is\ncomposed of a 2 \u00d7 2 max-pooling layer, followed by two fully connected (plus ReLU non-linearity)\nlayers. The \ufb01nal output is a single \u2018objectness\u2019 score indicating the presence of an object in the\ncenter of the input patch (and at the appropriate scale).\n3.2\nJoint Learning\nGiven an input patch xk \u2208I, the model is trained to jointly infer a pixel-wise segmentation mask and\nan object score. The loss function is a sum of binary logistic regression losses, one for each location\nof the segmentation network and one for the object score, over all training triplets (xk, mk, yk):\nL(\u03b8) =\nX\nk\n\u0012\n1+yk\n2woho\nX\nij\nlog(1 + e\u2212mij\nk f ij\nsegm(xk)) + \u03bb log(1 + e\u2212ykfscore(xk))\n\u0013\n(1)\nHere \u03b8 is the set of parameters, f ij\nsegm(xk) is the prediction of the segmentation network at location\n(i, j), and fscore(xk) is the predicted object score. We alternate between backpropagating through\nthe segmentation branch and scoring branch (and set \u03bb =\n1\n32). For the scoring branch, the data is\nsampled such that the model is trained with an equal number of positive and negative samples.\nNote that the factor multiplying the \ufb01rst term of Equation 1 implies that we only backpropagate the\nerror over the segmentation branch if yk = 1. An alternative would be to train the segmentation\nbranch using negatives as well (setting mij\nk = 0 for all pixels if yk = 0). However, we found that\ntraining with positives only was critical for generalizing beyond the object categories seen during\ntraining and for achieving high object recall. This way, during inference the network attempts to\ngenerate a segmentation mask at every patch, even if no known object is present.\n3.3\nFull Scene Inference\nDuring full image inference, we apply the model densely at multiple locations and scales. This\nis necessary so that for each object in the image we test at least one patch that fully contains the\nobject (roughly centered and at the appropriate scale), satisfying the two assumptions made during\ntraining. This procedure gives a segmentation mask and object score at each image location. Figure 2\nillustrates the segmentation output when the model is applied densely to an image at a single scale.\nThe full image inference procedure is ef\ufb01cient since all computations can be computed convolution-\nally. The VGG features can be computed densely in a fraction of a second given a typical input\nimage. For the segmentation branch, the last fully connected layer can be computed via convolu-\ntions applied to the VGG features. The scores are likewise computed by convolutions on the VGG\nfeatures followed by two 1 \u00d7 1 convolutional layers. Exact runtimes are given in \u00a74.\n4\nFigure 2: Output of segmentation model applied densely to a full image with a 16 pixel stride (at a\nsingle scale at the central horizontal image region). Multiple locations give rise to good masks for\neach of the three monkeys (scores not shown). Note that no monkeys appeared in our training set.\nFinally, note that the scoring branch of the network has a downsampling factor 2\u00d7 larger than the\nsegmentation branch due to the additional max-pooling layer. Given an input test image of size\nht \u00d7 wt, the segmentation and object network generate outputs of dimension ht\n16 \u00d7 wt\n16 and ht\n32 \u00d7 wt\n32 ,\nrespectively. In order to achieve a one-to-one mapping between the mask prediction and object\nscore, we apply the interleaving trick right before the last max-pooling layer for the scoring branch\nto double its output resolution (we use exactly the implementation described in [26]).\n3.4\nImplementation Details\nDuring training, an input patch xk is considered to contain a \u2018canonical\u2019 positive example if an object\nis precisely centered in the patch and has maximal dimension equal to exactly 128 pixels. However,\nhaving some tolerance in the position of an object within a patch is critical as during full image\ninference most objects will be observed slightly offset from their canonical position. Therefore,\nduring training, we randomly jitter each \u2018canonical\u2019 positive example to increase the robustness of\nour model. Speci\ufb01cally, we consider translation shift (of \u00b116 pixels), scale deformation (of 2\u00b11/4),\nand also horizontal \ufb02ip. In all cases we apply the same transformation to both the image patch xk\nand the ground truth mask mk and assign the example a positive label yk = 1. Negative examples\n(yk = \u22121) are any patches at least \u00b132 pixels or 2\u00b11 in scale from any canonical positive example.\nDuring full image inference we apply the model densely at multiple locations (with a stride of 16\npixels) and scales (scales 2\u22122 to 21 with a step of 21/2). This ensures that there is at least one tested\nimage patch that fully contains each object in the image (within the tolerances used during training).\nAs in the original VGG-A network [27], our model is fed with RGB input patches of dimension\n3 \u00d7 224 \u00d7 224. Since we removed the \ufb01fth pooling layer, the common branch outputs a feature map\nof dimensions 512 \u00d7 14 \u00d7 14. The score branch of our network is composed of 2 \u00d7 2 max pooling\nfollowed by two fully connected layers (with 512 and 1024 hidden units, respectively). Both of these\nlayers are followed by ReLU non-linearity and a dropout [28] procedure with a rate of 0.5. A \ufb01nal\nlinear layer then generates the object score.\nThe segmentation branch begins with a single 1 \u00d7 1 convolutional layer with 512 units. This feature\nmap is then fully connected to a low dimensional output of size 512, which is further fully connected\nto each pixel classi\ufb01er to generate an output of dimension 56 \u00d7 56. As discussed, there is no non-\nlinearity between these two layers. In total, our model contains around 75M parameters.\nA \ufb01nal bilinear upsampling layer is added to transform the 56 \u00d7 56 output prediction to the full\n224 \u00d7 224 resolution of the ground-truth (directly predicting the full resolution output would have\nbeen much slower). We opted for a non-trainable layer as we observed that a trainable one simply\nlearned to bilinearly upsample. Alternatively, we tried downsampling the ground-truth instead of\nupsampling the network output; however, we found that doing so slightly reduced accuracy.\nDesign architecture and hyper-parameters were chosen using a subset of the MS COCO validation\ndata [21] (non-overlapping with the data we used for evaluation). We considered a learning rate\nof .001. We trained our model using stochastic gradient descent with a batch size of 32 examples,\nmomentum of .9, and weight decay of .00005. Aside from the pre-trained VGG features, weights\nare initialized randomly from a uniform distribution. Our model takes around 5 days to train on a\nNvidia Tesla K40m. To binarize predicted masks we simply threshold the continuous output (using\na threshold of .1 for PASCAL and .2 for COCO). All the experiments were conducted using Torch71.\n1http://torch.ch\n5\nFigure 3: DeepMask proposals with highest IoU to the ground truth on selected images from COCO.\nMissed objects (no matching proposals with IoU > 0.5) are marked with a red outline.\n4\nExperimental Results\nIn this section, we evaluate the performance of our approach on the PASCAL VOC 2007 test set [7]\nand on the \ufb01rst 5000 images of the MS COCO 2014 validation set [21]. Our model is trained on\nthe COCO training set which contains about 80,000 images and a total of nearly 500,000 segmented\nobjects. Although our model is trained to generate segmentation proposals, it can also be used to\nprovide box proposals by taking the bounding boxes enclosing the segmentation masks. Figures 3\nand 6 show examples of generated proposals with highest IoU to the ground truth on COCO.\nMetrics: We measure accuracy using the common Intersection over Union (IoU) metric. IoU is the\nintersection of a candidate proposal and ground-truth annotation divided by the area of their union.\nThis metric can be applied to both segmentation and box proposals. Following Hosang et al. [13],\nwe evaluate the performance of the proposal methods considering the average recall (AR) between\nIoU 0.5 and 1.0 for a \ufb01xed number of proposals. AR has been shown to correlate extremely well\nwith detector performance (recall at a single IoU threshold is far less predictive) [13].\nMethods: We compare to the current top-\ufb01ve publicly-available proposal methods including: Edge-\nBoxes [34], SelectiveSearch [31], Geodesic [16], Rigor [14], and MCG [24]. These methods achieve\ntop results on object detection (when coupled with R-CNNs [10]) and also obtain the best AR [13].\nResults: Figure 4 (a-c) compares the performance of our approach, DeepMask, to existing proposal\nmethods on PASCAL (using boxes) and COCO (using both boxes and segmentations). Shown is the\nAR of each method as a function of the number of generated proposals. Under all scenarios Deep-\nMask (and its variants) achieves substantially better AR for all numbers of proposals considered. AR\nat selected proposal counts and averaged across all counts (AUC) is reported in Tables 1 and 2 for\nCOCO and PASCAL, respectively. Notably, DeepMask achieves an order of magnitude reduction\nin the number of proposals necessary to reach a given AR under most scenarios. For example, with\n100 segmentation proposals DeepMask achieves an AR of .245 on COCO while competing methods\nrequire nearly 1000 segmentation proposals to achieve similar AR.\n6\n# proposals\n10 0\n10 1\n10 2\n10 3\naverage recall\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\nDeepMask\nMCG\nSelectiveSearch\nRigor\nGeodesic\nEdgeBoxes\n(a) Box proposals on PASCAL.\n# proposals\n10 0\n10 1\n10 2\n10 3\naverage recall\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\nDeepMask\nDeepMaskZoom\nMCG\nSelectiveSearch\nRigor\nGeodesic\nEdgeBoxes\n(b) Box proposals on COCO.\n# proposals\n10 0\n10 1\n10 2\n10 3\naverage recall\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\nDeepMask\nDeepMaskZoom\nMCG\nSelectiveSearch\nRigor\nGeodesic\n(c) Segm. proposals on COCO.\n# proposals\n10 0\n10 1\n10 2\n10 3\naverage recall\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\nDeepMask\nDeepMaskZoom\nMCG\nSelectiveSearch\nRigor\nGeodesic\n(d) Small objects (area< 322).\n# proposals\n10 0\n10 1\n10 2\n10 3\naverage recall\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\nDeepMask\nDeepMaskZoom\nMCG\nSelectiveSearch\nRigor\nGeodesic\n(e) Medium objects.\n# proposals\n10 0\n10 1\n10 2\n10 3\naverage recall\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\nDeepMask\nDeepMaskZoom\nMCG\nSelectiveSearch\nRigor\nGeodesic\n(f) Large objects (area> 962).\nIoU\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nrecall\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\nDeepMask\nDeepMaskZoom\nMCG\nSelectiveSearch\nRigor\nGeodesic\n(g) Recall with 10 proposals.\nIoU\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nrecall\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\nDeepMask\nDeepMaskZoom\nMCG\nSelectiveSearch\nRigor\nGeodesic\n(h) Recall with 100 proposals.\nIoU\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nrecall\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\nDeepMask\nDeepMaskZoom\nMCG\nSelectiveSearch\nRigor\nGeodesic\n(i) Recall with 1000 proposals.\nFigure 4: (a-c) Average recall versus number of box and segmentation proposals on various datasets.\n(d-f) AR versus number of proposals for different object scales on segmentation proposals in COCO.\n(g-h) Recall versus IoU threshold for different number of segmentation proposals in COCO.\nBox Proposals\nSegmentation Proposals\nAR@10\nAR@100\nAR@1000\nAUC\nAR@10\nAR@100\nAR@1000\nAUCS\nAUCM\nAUCL\nAUC\nEdgeBoxes [34]\n.074\n.178\n.338\n.139\n-\n-\n-\n-\n-\n-\n-\nGeodesic [16]\n.040\n.180\n.359\n.126\n.023\n.123\n.253\n.013\n.086\n.205\n.085\nRigor [14]\n-\n.133\n.337\n.101\n-\n.094\n.253\n.022\n.060\n.178\n.074\nSelectiveSearch [31]\n.052\n.163\n.357\n.126\n.025\n.095\n.230\n.006\n.055\n.214\n.074\nMCG [24]\n.101\n.246\n.398\n.180\n.077\n.186\n.299\n.031\n.129\n.324\n.137\nDeepMask20\n.139\n.286\n.431\n.217\n.109\n.215\n.314\n.020\n.227\n.317\n.164\nDeepMask20\u2217\n.152\n.306\n.432\n.228\n.123\n.233\n.314\n.020\n.257\n.321\n.175\nDeepMaskZoom\n.150\n.326\n.482\n.242\n.127\n.261\n.366\n.068\n.263\n.308\n.194\nDeepMaskFull\n.149\n.310\n.442\n.231\n.118\n.235\n.323\n.020\n.244\n.342\n.176\nDeepMask\n.153\n.313\n.446\n.233\n.126\n.245\n.331\n.023\n.266\n.336\n.183\nTable 1: Results on the MS COCO dataset for both bounding box and segmentation proposals. We\nreport AR at different number of proposals (10, 100 and 1000) and also AUC (AR averaged across\nall proposal counts). For segmentation proposals we report overall AUC and also AUC at different\nscales (small/medium/large objects indicated by superscripts S/M/L). See text for details.\nScale: The COCO dataset contains objects in a wide range of scales. In order to analyze performance\nin more detail, we divided the objects in the validation set into roughly equally sized sets according\nto object pixel area a: small (a < 322), medium (322 \u2264a \u2264962), and large (a > 962) objects.\nFigure 4 (d-f) shows performance at each scale; all models perform poorly on small objects. To\nimprove accuracy of DeepMask we apply it at an additional smaller scale (DeepMaskZoom). This\nboosts performance (especially for small objects) but at a cost of increased inference time.\n7\nPASCAL VOC07\nAR@10\nAR@100\nAR@1000\nAUC\nEdgeBoxes [34]\n.203\n.407\n.601\n.309\nGeodesic [16]\n.121\n.364\n.596\n.230\nRigor [14]\n.164\n.321\n.589\n.239\nSelectiveSearch [31]\n.085\n.347\n.618\n.241\nMCG [24]\n.232\n.462\n.634\n.344\nDeepMask\n.337\n.561\n.690\n.433\nTable 2: Results on PASCAL VOC 2007 test.\nFigure 5: Fast R-CNN results on PASCAL.\nLocalization: Figure 4 (g-i) shows the recall each model achieves as the IoU varies, shown for\ndifferent number of proposals per image. DeepMask achieves a higher recall in virtually every\nscenario, except at very high IoU, in which it falls slightly below other models. This is likely due\nto the fact that our method outputs a downsampled version of the mask at each location and scale; a\nmultiscale approach or skip connections could improve localization at very high IoU.\nGeneralization: To see if our approach can generalize to unseen classes [2, 19], we train two ad-\nditional versions of our model, DeepMask20 and DeepMask20\u2217. DeepMask20 is trained only with\nobjects belonging to one of the 20 PASCAL categories (subset of the full 80 COCO categories).\nDeepMask20\u2217is similar, except we use the scoring network from the original DeepMask. Results\nfor the two models when evaluated on all 80 COCO categories (as in all other experiments) are\nshown in Table 1. Compared to DeepMask, DeepMask20 exhibits a drop in AR (but still outper-\nforms all previous methods). DeepMask20\u2217, however, matches the performance of DeepMask. This\nsurprising result demonstrates that the drop in accuracy is due to the discriminatively trained scoring\nbranch (DeepMask20 is inadvertently trained to assign low scores to the other 60 categories); the\nsegmentation branch generalizes extremely well even when trained on a reduced set of categories.\nArchitecture: In the segmentation branch, the convolutional features are fully connected to a 512\n\u2018low-rank\u2019 layer which is in turn connected to the 56\u00d756 output (with no intermediate non-linearity),\nsee \u00a73. We also experimented with a \u2018full-rank\u2019 architecture (DeepMaskFull) with over 300M pa-\nrameters where each of the 56 \u00d7 56 outputs was directly connected to the convolutional features. As\ncan be seen in Table 1, DeepMaskFull is slightly inferior to our \ufb01nal model (and much slower).\nDetection: As a \ufb01nal validation, we evaluate how DeepMask performs when coupled with an object\ndetector on PASCAL VOC 2007 test. We re-train and evaluate the state-of-the-art Fast R-CNN [9]\nusing proposals generated by SelectiveSearch [31] and our method. Figure 5 shows the mean average\nprecision (mAP) for Fast R-CNN with varying number of proposals. Most notably, with just 100\nDeepMask proposals Fast R-CNN achieves mAP of 68.2% and outperforms the best results obtained\nwith 2000 SelectiveSearch proposals (mAP of 66.9%). We emphasize that with 20\u00d7 fewer proposals\nDeepMask outperforms SelectiveSearch (this is consistent with the AR numbers in Table 1). With\n500 DeepMask proposals, Fast R-CNN improves to 69.9% mAP, after which performance begins to\ndegrade (a similar effect was observed in [9]).\nSpeed: Inference takes an average of 1.6s per image in the COCO dataset (1.2s on the smaller\nPASCAL images). Our runtime is competitive with the fastest segmentation proposal methods\n(Geodesic [16] runs at \u223c1s per PASCAL image) and substantially faster than most (e.g., MCG [24]\ntakes \u223c30s). Inference time can further be dropped by \u223c30% by parallelizing all scales in a single\nbatch (eliminating GPU overhead). We do, however, require use of a GPU for ef\ufb01cient inference.\n5\nConclusion\nIn this paper, we propose an innovative framework to generate segmentation object proposals di-\nrectly from image pixels. At test time, the model is applied densely over the entire image at multiple\nscales and generates a set of ranked segmentation proposals. We show that learning features for\nobject proposal generation is not only feasible but effective. Our approach surpasses the previous\nstate of the art by a large margin in both box and segmentation proposal generation. In future work,\nwe plan on coupling our proposal method more closely with state-of-the-art detection approaches.\nAcknowledgements:\nWe would like to thank Ahmad Humayun and Tsung-Yi Lin for help with generat-\ning experimental results, Andrew Tulloch, Omry Yadan and Alexey Spiridonov for help with computational\ninfrastructure, and Rob Fergus, Yuandong Tian and Soumith Chintala for valuable discussions.\n8\nReferences\n[1] B. Alexe, T. Deselaers, and V. Ferrari. Measuring the objectness of image windows. PAMI, 2012. 2\n[2] N. Chavali, H. Agrawal, A. Mahendru, and D. Batra. Object-proposal evaluation protocol is \u2019gameable\u2019.\narXiv:1505.05836, 2015. 8\n[3] L. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and A. L. Yuille. Semantic image segmentation with\ndeep convolutional nets and fully connected crfs. ICLR, 2015. 2\n[4] N. Dalal and B. Triggs. Histograms of oriented gradients for human detection. In CVPR, 2005. 1\n[5] J. Deng, W. Dong, R. Socher, L. Li, K. Li, and L. Fei-Fei. Imagenet: A large-scale hierarchical image\ndatabase. In CVPR, 2009. 1, 3\n[6] D. Erhan, C. Szegedy, A. Toshev, and D. Anguelov. Scalable object detection using deep neural networks.\nIn CVPR, 2014. 2\n[7] M. Everingham, L. V. Gool, C. K. I. Williams, J. Winn, and A. Zisserman. The PASCAL visual object\nclasses (VOC) challenge. IJCV, 2010. 1, 2, 6\n[8] P. F. Felzenszwalb, R. B. Girshick, D. McAllester, and D. Ramanan. Object detection with discrimina-\ntively trained part-based models. PAMI, 2010. 1\n[9] R. Girshick. Fast R-CNN. arXiv:1504.08083, 2015. 2, 8\n[10] R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich feature hierarchies for accurate object detection\nand semantic segmentation. In CVPR, 2014. 1, 2, 6\n[11] B. Hariharan, P. Arbel\u00b4aez, R. Girshick, and J. Malik. Hypercolumns for object segmentation and \ufb01ne-\ngrained localization. In CVPR, 2015. 2\n[12] K. He, X. Zhang, S. Ren, and J. Sun. Spatial pyramid pooling in deep convolutional networks for visual\nrecognition. In ECCV, 2014. 2\n[13] J. Hosang, R. Benenson, P. Doll\u00b4ar, and B. Schiele.\nWhat makes for effective detection proposals?\narXiv:1502.05082, 2015. 2, 6\n[14] A. Humayun, F. Li, and J. M. Rehg. RIGOR: Reusing Inference in Graph Cuts for generating Object\nRegions. In CVPR, 2014. 2, 6, 7, 8\n[15] H. Kaiming, Z. Xiangyu, R. Shaoqing, and S. Jian. Spatial pyramid pooling in deep convolutional net-\nworks for visual recognition. In ECCV, 2014. 1\n[16] P. Kr\u00a8ahenb\u00a8uhl and V. Koltun. Geodesic object proposals. In ECCV, 2014. 2, 6, 7, 8\n[17] P. Kr\u00a8ahenb\u00a8uhl and V. Koltun. Learning to propose objects. In CVPR, 2015. 2\n[18] A. Krizhevsky, I. Sutskever, and G. Hinton. Imagenet classi\ufb01cation with deep convolutional neural net-\nworks. In NIPS, 2012. 2\n[19] W. Kuo, B. Hariharan, and J. Malik. Deepbox: Learning objectness with convolutional networks. In\narXiv:505.02146v1, 2015. 2, 8\n[20] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition.\nProceedings of the IEEE, 1998. 1\n[21] T.-Y. Lin, M. Maire, S. Belongie, L. Bourdev, R. Girshick, J. Hays, P. Perona, D. Ramanan, C. L. Zitnick,\nand P. Doll\u00b4ar. Microsoft COCO: Common objects in context. arXiv:1405.0312, 2015. 1, 2, 5, 6\n[22] M. Oquab, L. Bottou, I. Laptev, and J. Sivic. Is object localization for free? \u2013 Weakly-supervised learning\nwith convolutional neural networks. In CVPR, 2015. 2\n[23] P. O. Pinheiro and R. Collobert. Recurrent conv. neural networks for scene labeling. In ICML, 2014. 2\n[24] J. Pont-Tuset, P. Arbel\u00b4aez, J. Barron, F. Marques, and J. Malik. Multiscale combinatorial grouping for\nimage segmentation and object proposal generation. In arXiv:1503.00848, 2015. 2, 6, 7, 8\n[25] S. Ren, K. He, R. Girshick, and J. Sun. Faster r-cnn: Towards real-time object detection with region\nproposal networks. In arXiv:1506.01497, 2015. 2\n[26] P. Sermanet, D. Eigen, X. Zhang, M. Mathieu, R. Fergus, and Y. LeCun. Overfeat: Integrated recognition,\nlocalization and detection using convolutional networks. In ICLR, 2014. 5\n[27] K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition. In\nICLR, 2015. 2, 3, 5\n[28] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov. Dropout: A simple way to\nprevent neural networks from over\ufb01tting. JMLR, 2014. 5\n[29] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabi-\nnovich. Going deeper with convolutions. In CVPR, 2015. 2\n[30] C. Szegedy, S. Reed, D. Erhan, and D. Anguelov.\nScalable, high-quality object detection.\nIn\narXiv:1412.1441, 2014. 2\n[31] J. Uijlings, K. van de Sande, T. Gevers, and A. Smeulders. Selective search for object recog. IJCV, 2013.\n2, 6, 7, 8\n[32] P. Viola and M. J. Jones. Robust real-time face detection. IJCV, 2004. 1\n[33] Z. Y. Zhu, R. Urtasun, R. Salakhutdinov, and S. Fidler. segdeepm: Exploiting segmentation and context\nin deep neural networks for object detection. In CVPR, 2015. 1\n[34] C. L. Zitnick and P. Doll\u00b4ar. Edge boxes: Locating object proposals from edges. In ECCV, 2014. 2, 6, 7, 8\n9\nFigure 6: Additional DeepMask proposals with highest IoU to the ground truth on selected images\nfrom COCO. Missed objects (no matching proposals with IoU > 0.5) are marked with a red outline.\n10\n",
        "sentence": " , 2016), leading to important progress in a variety of tasks, like object detection (Girshick, 2015) or image segmentation (Pinheiro et al., 2015). As in Ranzato et al. (2007), we use image gradients instead of the images to avoid trivial solutions like clustering according to colors.",
        "context": "[4] N. Dalal and B. Triggs. Histograms of oriented gradients for human detection. In CVPR, 2005. 1\n[5] J. Deng, W. Dong, R. Socher, L. Li, K. Li, and L. Fei-Fei. Imagenet: A large-scale hierarchical image\ndatabase. In CVPR, 2009. 1, 3\nimage segmentation and object proposal generation. In arXiv:1503.00848, 2015. 2, 6, 7, 8\n[25] S. Ren, K. He, R. Girshick, and J. Sun. Faster r-cnn: Towards real-time object detection with region\nproposal networks. In arXiv:1506.01497, 2015. 2\nceptionally well on ImageNet. In the setting of object detection, Girshick et al. [10] proposed\nR-CNN, a ConvNet-based model that beats by a large margin models relying on hand-designed"
    },
    {
        "title": "Linking people in videos with their names using coreference resolution",
        "author": [
            "V. Ramanathan",
            "A. Joulin",
            "P. Liang",
            "L. Fei-Fei"
        ],
        "venue": "In ECCV,",
        "citeRegEx": "Ramanathan et al\\.,? \\Q2014\\E",
        "shortCiteRegEx": "Ramanathan et al\\.",
        "year": 2014,
        "abstract": "",
        "full_text": "",
        "sentence": " , 2014) or video/text alignment (Bojanowski et al., 2013; 2014; Ramanathan et al., 2014).",
        "context": null
    },
    {
        "title": "Unsupervised learning of invariant feature hierarchies with applications to object recognition",
        "author": [
            "M.A. Ranzato",
            "F.J. Huang",
            "Y.L. Boureau",
            "Y. LeCun"
        ],
        "venue": "In CVPR,",
        "citeRegEx": "Ranzato et al\\.,? \\Q2007\\E",
        "shortCiteRegEx": "Ranzato et al\\.",
        "year": 2007,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " Many have also used autoencoders with a reconstruction loss (Bengio et al., 2007; Ranzato et al., 2007; Masci et al., 2011). The decoder is often a fully connected network (Ranzato et al., 2007) or a deconvolutional network (Masci et al. During transfer learning, we consider the output of the last convolutional layer as our features as in Razavian et al. (2014). We use the same multi-layer perceptron (MLP) as in Krizhevsky et al.",
        "context": null
    },
    {
        "title": "CNN features off-the-shelf: an astounding baseline for recognition",
        "author": [
            "Razavian",
            "A. Sharif",
            "H. Azizpour",
            "J. Sullivan",
            "S. Carlsson"
        ],
        "venue": "In arXiv 1403.6382,",
        "citeRegEx": "Razavian et al\\.,? \\Q2014\\E",
        "shortCiteRegEx": "Razavian et al\\.",
        "year": 2014,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " Key to this success is their ability to produce features that easily transfer to new domains when trained on massive databases of labeled images (Razavian et al., 2014; Oquab et al., 2014) or weaklysupervised data (Joulin et al.",
        "context": null
    },
    {
        "title": "A metric for distributions with applications to image databases",
        "author": [
            "Y. Rubner",
            "C. Tomasi",
            "L.J. Guibas"
        ],
        "venue": "In ICCV,",
        "citeRegEx": "Rubner et al\\.,? \\Q1998\\E",
        "shortCiteRegEx": "Rubner et al\\.",
        "year": 1998,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " In some sense, we are optimizing a crude approximation of the earth mover\u2019s distance between the distribution of deep features and a given target distribution (Rubner et al., 1998).",
        "context": null
    },
    {
        "title": "Image classification with the fisher vector: Theory and practice",
        "author": [
            "J. S\u00e1nchez",
            "F. Perronnin",
            "T. Mensink",
            "J. Verbeek"
        ],
        "venue": null,
        "citeRegEx": "S\u00e1nchez et al\\.,? \\Q2013\\E",
        "shortCiteRegEx": "S\u00e1nchez et al\\.",
        "year": 2013,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " , SIFT with Fisher Vectors (SIFT+FV) (S\u00e1nchez et al., 2013). SIFT+FV (S\u00e1nchez et al., 2013) 55.",
        "context": null
    },
    {
        "title": "Colocalization in real-world images",
        "author": [
            "K. Tang",
            "A. Joulin",
            "Li",
            "L.-J",
            "L. Fei-Fei"
        ],
        "venue": "In CVPR,",
        "citeRegEx": "Tang et al\\.,? \\Q2014\\E",
        "shortCiteRegEx": "Tang et al\\.",
        "year": 2014,
        "abstract": "",
        "full_text": "",
        "sentence": " It has been successfully applied to several computer vision applications, like object discovery (Joulin et al., 2010; Tang et al., 2014) or video/text alignment (Bojanowski et al. We project the output of the network on the `2 sphere as in Tygert et al. (2017). The network is trained with SGD with a batch size of 256.",
        "context": null
    },
    {
        "title": "Scale-invariant learning and convolutional networks",
        "author": [
            "M. Tygert",
            "S. Chintala",
            "A. Szlam",
            "Y. Tian",
            "W. Zaremba"
        ],
        "venue": "Applied and Computational Harmonic Analysis,",
        "citeRegEx": "Tygert et al\\.,? \\Q2017\\E",
        "shortCiteRegEx": "Tygert et al\\.",
        "year": 2017,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " This is achieved by using a quadratic loss as in (Tygert et al., 2017) and a fast approximation of the Hungarian algorithm. As noted by Tygert et al. (2017), batch normalization plays a crucial role when optimizing the l2 loss, as it avoids exploding gradients. The features are unit normalized for the square loss (Tygert et al., 2017). As previously observed by Tygert et al. (2017), the performances are similar, hence validating our choice of loss function.",
        "context": null
    },
    {
        "title": "Conditional image generation with pixelcnn decoders",
        "author": [
            "A. van den Oord",
            "N. Kalchbrenner",
            "L. Espeholt",
            "O. Vinyals",
            "A. Graves"
        ],
        "venue": null,
        "citeRegEx": "Oord et al\\.,? \\Q2016\\E",
        "shortCiteRegEx": "Oord et al\\.",
        "year": 2016,
        "abstract": "This work explores conditional image generation with a new image density\nmodel based on the PixelCNN architecture. The model can be conditioned on any\nvector, including descriptive labels or tags, or latent embeddings created by\nother networks. When conditioned on class labels from the ImageNet database,\nthe model is able to generate diverse, realistic scenes representing distinct\nanimals, objects, landscapes and structures. When conditioned on an embedding\nproduced by a convolutional network given a single image of an unseen face, it\ngenerates a variety of new portraits of the same person with different facial\nexpressions, poses and lighting conditions. We also show that conditional\nPixelCNN can serve as a powerful decoder in an image autoencoder. Additionally,\nthe gated convolutional layers in the proposed model improve the log-likelihood\nof PixelCNN to match the state-of-the-art performance of PixelRNN on ImageNet,\nwith greatly reduced computational cost.",
        "full_text": "Conditional Image Generation with\nPixelCNN Decoders\nA\u00e4ron van den Oord\nGoogle DeepMind\navdnoord@google.com\nNal Kalchbrenner\nGoogle DeepMind\nnalk@google.com\nOriol Vinyals\nGoogle DeepMind\nvinyals@google.com\nLasse Espeholt\nGoogle DeepMind\nespeholt@google.com\nAlex Graves\nGoogle DeepMind\ngravesa@google.com\nKoray Kavukcuoglu\nGoogle DeepMind\nkorayk@google.com\nAbstract\nThis work explores conditional image generation with a new image density model\nbased on the PixelCNN architecture. The model can be conditioned on any vector,\nincluding descriptive labels or tags, or latent embeddings created by other networks.\nWhen conditioned on class labels from the ImageNet database, the model is able to\ngenerate diverse, realistic scenes representing distinct animals, objects, landscapes\nand structures. When conditioned on an embedding produced by a convolutional\nnetwork given a single image of an unseen face, it generates a variety of new\nportraits of the same person with different facial expressions, poses and lighting\nconditions. We also show that conditional PixelCNN can serve as a powerful\ndecoder in an image autoencoder. Additionally, the gated convolutional layers in\nthe proposed model improve the log-likelihood of PixelCNN to match the state-of-\nthe-art performance of PixelRNN on ImageNet, with greatly reduced computational\ncost.\n1\nIntroduction\nRecent advances in image modelling with neural networks [30, 26, 20, 10, 9, 28, 6] have made\nit feasible to generate diverse natural images that capture the high-level structure of the training\ndata. While such unconditional models are fascinating in their own right, many of the practical\napplications of image modelling require the model to be conditioned on prior information: for\nexample, an image model used for reinforcement learning planning in a visual environment would\nneed to predict future scenes given speci\ufb01c states and actions [17]. Similarly image processing\ntasks such as denoising, deblurring, inpainting, super-resolution and colorization rely on generating\nimproved images conditioned on noisy or incomplete data. Neural artwork [18, 5] and content\ngeneration represent potential future uses for conditional generation.\nThis paper explores the potential for conditional image modelling by adapting and improving a\nconvolutional variant of the PixelRNN architecture [30]. As well as providing excellent samples,\nthis network has the advantage of returning explicit probability densities (unlike alternatives such as\ngenerative adversarial networks [6, 3, 19]), making it straightforward to apply in domains such as\ncompression [32] and probabilistic planning and exploration [2]. The basic idea of the architecture\nis to use autoregressive connections to model images pixel by pixel, decomposing the joint image\ndistribution as a product of conditionals. Two variants were proposed in the original paper: PixelRNN,\nwhere the pixel distributions are modeled with two-dimensional LSTM [7, 26], and PixelCNN, where\nthey are modelled with convolutional networks. PixelRNNs generally give better performance, but\nPixelCNNs are much faster to train because convolutions are inherently easier to parallelize; given\nthe vast number of pixels present in large image datasets this is an important advantage. We aim to\narXiv:1606.05328v2  [cs.CV]  18 Jun 2016\n0\n255\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\nBlind spot\nHorizontal stack\nVertical stack\nFigure 1: Left: A visualization of the PixelCNN that maps a neighborhood of pixels to prediction for\nthe next pixel. To generate pixel xi the model can only condition on the previously generated pixels\nx1, . . . xi\u22121. Middle: an example matrix that is used to mask the 5x5 \ufb01lters to make sure the model\ncannot read pixels below (or strictly to the right) of the current pixel to make its predictions. Right:\nTop: PixelCNNs have a blind spot in the receptive \ufb01eld that can not be used to make predictions.\nBottom: Two convolutional stacks (blue and purple) allow to capture the whole receptive \ufb01eld.\ncombine the strengths of both models by introducing a gated variant of PixelCNN (Gated PixelCNN)\nthat matches the log-likelihood of PixelRNN on both CIFAR and ImageNet, while requiring less than\nhalf the training time.\nWe also introduce a conditional variant of the Gated PixelCNN (Conditional PixelCNN) that allows\nus to model the complex conditional distributions of natural images given a latent vector embedding.\nWe show that a single Conditional PixelCNN model can be used to generate images from diverse\nclasses such as dogs, lawn mowers and coral reefs, by simply conditioning on a one-hot encoding\nof the class. Similarly one can use embeddings that capture high level information of an image to\ngenerate a large variety of images with similar features. This gives us insight into the invariances\nencoded in the embeddings \u2014 e.g., we can generate different poses of the same person based on a\nsingle image. The same framework can also be used to analyse and interpret different layers and\nactivations in deep neural networks.\n2\nGated PixelCNN\nPixelCNNs (and PixelRNNs) [30] model the joint distribution of pixels over an image x as the\nfollowing product of conditional distributions, where xi is a single pixel:\np(x) =\nn2\nY\ni=1\np(xi|x1, ..., xi\u22121).\n(1)\nThe ordering of the pixel dependencies is in raster scan order: row by row and pixel by pixel within\nevery row. Every pixel therefore depends on all the pixels above and to the left of it, and not on any\nof other pixels. The dependency \ufb01eld of a pixel is visualized in Figure 1 (left).\nA similar setup has been used by other autoregressive models such as NADE [14] and RIDE [26].\nThe difference lies in the way the conditional distributions p(xi|x1, ..., xi\u22121) are constructed. In\nPixelCNN every conditional distribution is modelled by a convolutional neural network. To make\nsure the CNN can only use information about pixels above and to the left of the current pixel, the\n\ufb01lters of the convolution are masked as shown in Figure 1 (middle). For each pixel the three colour\nchannels (R, G, B) are modelled successively, with B conditioned on (R, G), and G conditioned on R.\nThis is achieved by splitting the feature maps at every layer of the network into three and adjusting the\ncentre values of the mask tensors. The 256 possible values for each colour channel are then modelled\nusing a softmax.\nPixelCNN typically consists of a stack of masked convolutional layers that takes an N x N x 3 image\nas input and produces N x N x 3 x 256 predictions as output. The use of convolutions allows the\npredictions for all the pixels to be made in parallel during training (all conditional distributions from\nEquation 1). During sampling the predictions are sequential: every time a pixel is predicted, it is\n2\nfed back into the network to predict the next pixel. This sequentiality is essential to generating high\nquality images, as it allows every pixel to depend in a highly non-linear and multimodal way on the\nprevious pixels.\n2.1\nGated Convolutional Layers\nPixelRNNs, which use spatial LSTM layers instead of convolutional stacks, have previously been\nshown to outperform PixelCNNs as generative models [30]. One possible reason for the advantage\nis that the recurrent connections in LSTM allow every layer in the network to access the entire\nneighbourhood of previous pixels, while the region of the neighbourhood available to pixelCNN\ngrows linearly with the depth of the convolutional stack. However this shortcoming can largely be\nalleviated by using suf\ufb01ciently many layers. Another potential advantage is that PixelRNNs contain\nmultiplicative units (in the form of the LSTM gates), which may help it to model more complex\ninteractions. To amend this we replaced the recti\ufb01ed linear units between the masked convolutions in\nthe original pixelCNN with the following gated activation unit:\ny = tanh(Wk,f \u2217x) \u2299\u03c3(Wk,g \u2217x),\n(2)\nwhere \u03c3 is the sigmoid non-linearity, k is the number of the layer, \u2299is the element-wise product\nand \u2217is the convolution operator. We call the resulting model the Gated PixelCNN. Feed-forward\nneural networks with gates have been explored in previous works, such as highway networks [25],\ngrid LSTM [13] and neural GPUs [12], and have generally proved bene\ufb01cial to performance.\n2.2\nBlind spot in the receptive \ufb01eld\nIn Figure 1 (top right), we show the progressive growth of the effective receptive \ufb01eld of a 3 \u00d7 3\nmasked \ufb01lter over the input image. Note that a signi\ufb01cant portion of the input image is ignored by the\nmasked convolutional architecture. This \u2018blind spot\u2019 can cover as much as a quarter of the potential\nreceptive \ufb01eld (e.g., when using 3x3 \ufb01lters), meaning that none of the content to the right of the\ncurrent pixel would be taken into account.\nIn this work, we remove the blind spot by combining two convolutional network stacks: one that\nconditions on the current row so far (horizontal stack) and one that conditions on all rows above\n(vertical stack). The arrangement is illustrated in Figure 1 (bottom right). The vertical stack, which\ndoes not have any masking, allows the receptive \ufb01eld to grow in a rectangular fashion without any\nblind spot, and we combine the outputs of the two stacks after each layer. Every layer in the horizontal\nstack takes as input the output of the previous layer as well as that of the vertical stack. If we had\nconnected the output of the horizontal stack into the vertical stack, it would be able to use information\nabout pixels that are below or to the right of the current pixel which would break the conditional\ndistribution.\nFigure 2 shows a single layer block of a Gated PixelCNN. We combine Wf and Wg in a single\n(masked) convolution to increase parallelization. As proposed in [30] we also use a residual connec-\ntion [11] in the horizontal stack. We have experimented with adding a residual connection in the\nvertical stack, but omitted it from the \ufb01nal model as it did not improve the results in our initial experi-\nments. Note that the (n \u00d7 1) and (n \u00d7 n) masked convolutions in Figure 2 can also be implemented\nby (\u2308n\n2 \u2309\u00d7 1) and (\u2308n\n2 \u2309\u00d7 n) convolutions followed by a shift in pixels by padding and cropping.\n2.3\nConditional PixelCNN\nGiven a high-level image description represented as a latent vector h, we seek to model the conditional\ndistribution p(x|h) of images suiting this description. Formally the conditional PixelCNN models\nthe following distribution:\np(x|h) =\nn2\nY\ni=1\np(xi|x1, ..., xi\u22121, h).\n(3)\nWe model the conditional distribution by adding terms that depend on h to the activations before the\nnonlinearities in Equation 2, which now becomes:\ny = tanh(Wk,f \u2217x + V T\nk,fh) \u2299\u03c3(Wk,g \u2217x + V T\nk,gh),\n(4)\n3\nn \u21e5n\n1 \u21e5n\ntanh\n\u03c3\n\u21e5\n+\n+\n1 \u21e51\n1 \u21e51\ntanh\n\u03c3\n\u21e5\n2p\np\np\np\np\np\np\n2p\np\nSplit feature maps\np = #feature maps\nFigure 2: A single layer in the Gated PixelCNN architecture. Convolution operations are shown in\ngreen, element-wise multiplications and additions are shown in red. The convolutions with Wf and\nWg from Equation 2 are combined into a single operation shown in blue, which splits the 2p features\nmaps into two groups of p.\nwhere k is the layer number. If h is a one-hot encoding that speci\ufb01es a class this is equivalent to\nadding a class dependent bias at every layer. Notice that the conditioning does not depend on the\nlocation of the pixel in the image; this is appropriate as long as h only contains information about\nwhat should be in the image and not where. For example we could specify that a certain animal or\nobject should appear, but may do so in different positions and poses and with different backgrounds.\nWe also developed a variant where the conditioning function was location dependent. This could\nbe useful for applications where we do have information about the location of certain structures\nin the image embedded in h. By mapping h to a spatial representation s = m (h) (which has the\nsame width and height as the image but may have an arbitrary number of feature maps) with a\ndeconvolutional neural network m(), we obtain a location dependent bias as follows:\ny = tanh(Wk,f \u2217x + Vk,f \u2217s) \u2299\u03c3(Wk,g \u2217x + Vk,g \u2217s).\n(5)\nwhere Vk,g \u2217s is an unmasked 1 \u00d7 1 convolution.\n2.4\nPixelCNN Auto-Encoders\nBecause conditional PixelCNNs have the capacity to model diverse, multimodal image distributions\np(x|h), it is possible to apply them as image decoders in existing neural architectures such as auto-\nencoders. An auto-encoder consists of two parts: an encoder that takes an input image x and maps it\nto a (usually) low-dimensional representation h, and a decoder that tries to reconstruct the original\nimage.\nStarting with a traditional convolutional auto-encoder architecture [16], we replace the deconvo-\nlutional decoder with a conditional PixelCNN and train the complete network end-to-end. Since\nPixelCNN has proved to be a strong unconditional generative model, we would expect this change to\nimprove the reconstructions. Perhaps more interestingly, we also expect it to change the representa-\ntions that the encoder will learn to extract from the data: since so much of the low level pixel statistics\ncan be handled by the PixelCNN, the encoder should be able to omit these from h and concentrate\ninstead on more high-level abstract information.\n3\nExperiments\n3.1\nUnconditional Modeling with Gated PixelCNN\nTable 1 compares Gated PixelCNN with published results on the CIFAR-10 dataset. These architec-\ntures were all optimized for the best possible validation score, meaning that models that get a lower\n4\nscore actually generalize better. Gated PixelCNN outperforms the PixelCNN by 0.11 bits/dim, which\nhas a very signi\ufb01cant effect on the visual quality of the samples produced, and which is close to the\nperformance of PixelRNN.\nModel\nNLL Test (Train)\nUniform Distribution: [30]\n8.00\nMultivariate Gaussian: [30]\n4.70\nNICE: [4]\n4.48\nDeep Diffusion: [24]\n4.20\nDRAW: [9]\n4.13\nDeep GMMs: [31, 29]\n4.00\nConv DRAW: [8]\n3.58 (3.57)\nRIDE: [26, 30]\n3.47\nPixelCNN: [30]\n3.14 (3.08)\nPixelRNN: [30]\n3.00 (2.93)\nGated PixelCNN:\n3.03 (2.90)\nTable 1: Test set performance of different models on CIFAR-10 in bits/dim (lower is better), training\nperformance in brackets.\nIn Table 2 we compare the performance of Gated PixelCNN with other models on the ImageNet\ndataset. Here Gated PixelCNN outperforms PixelRNN; we believe this is because the models are\nunder\ufb01tting, larger models perform better and the simpler PixelCNN model scales better. We were\nable to achieve similar performance to the PixelRNN (Row LSTM [30]) in less than half the training\ntime (60 hours using 32 GPUs). For the results in Table 2 we trained a larger model with 20 layers\n(Figure 2), each having 384 hidden units and \ufb01lter size of 5 \u00d7 5. We used 200K synchronous updates\nover 32 GPUs in TensorFlow [1] using a total batch size of 128.\n32x32\nModel\nNLL Test (Train)\nConv Draw: [8]\n4.40 (4.35)\nPixelRNN: [30]\n3.86 (3.83)\nGated PixelCNN:\n3.83 (3.77)\n64x64\nModel\nNLL Test (Train)\nConv Draw: [8]\n4.10 (4.04)\nPixelRNN: [30]\n3.63 (3.57)\nGated PixelCNN:\n3.57 (3.48)\nTable 2: Performance of different models on ImageNet in bits/dim (lower is better), training perfor-\nmance in brackets.\n3.2\nConditioning on ImageNet Classes\nFor our second experiment we explore class-conditional modelling of ImageNet images using Gated\nPixelCNNs. Given a one-hot encoding hi for the i-th class we model p(x|hi). The amount of\ninformation that the model receives is only log(1000) \u22480.003 bits/pixel (for a 32x32 image). Still,\none could expect that conditioning the image generation on class label could signi\ufb01cantly improve\nthe log-likelihood results, however we did not observe big differences. On the other hand, as noted\nin [27], we observed great improvements in the visual quality of the generated samples.\nIn Figure 3 we show samples from a single class-conditional model for 8 different classes. We see that\nthe generated classes are very distinct from one another, and that the corresponding objects, animals\nand backgrounds are clearly produced. Furthermore the images of a single class are very diverse: for\nexample the model was able to generate similar scenes from different angles and lightning conditions.\nIt is encouraging to see that given roughly 1000 images from every animal or object the model is able\nto generalize and produce new renderings.\n5\n3.3\nConditioning on Portrait Embeddings\nIn our next experiment we took the latent representations from the top layer of a convolutional\nnetwork trained on a large database of portraits automatically cropped from Flickr images using a\nface detector. The quality of images varied wildly, because a lot of the pictures were taken with\nmobile phones in bad lightning conditions.\nThe network was trained with a triplet loss function [23] that ensured that the embedding h produced\nfor an image x of a speci\ufb01c person was closer to the embeddings for all other images of the same\nperson than it was to any embedding of another person.\nAfter the supervised net was trained we took the (image=x, embedding=h) tuples and trained the\nConditional PixelCNN to model p(x|h). Given a new image of a person that was not in the training\nset we can compute h = f(x) and generate new portraits of the same person.\nSamples from the model are shown in Figure 4. We can see that the embeddings capture a lot of the\nfacial features of the source image and the generative model is able to produce a large variety of new\nfaces with these features in new poses, lighting conditions, etc.\nFinally, we experimented with reconstructions conditioned on linear interpolations between embed-\ndings of pairs of images. The results are shown in Figure 5. Every image in a single row used the\nsame random seed in the sampling which results in smooth transitions. The leftmost and rightmost\nimages are used to produce the end points of interpolation.\n3.4\nPixelCNN Auto Encoder\nThis experiment explores the possibility of training both the encoder and decoder (PixelCNN) end-\nto-end as an auto-encoder. We trained a PixelCNN auto-encoder on 32x32 ImageNet patches and\ncompared the results with those from a convolutional auto-encoder trained to optimize MSE. Both\nmodels used a 10 or 100 dimensional bottleneck.\nFigure 6 shows the reconstructions from both models. For the PixelCNN we sample multiple\nconditional reconstructions. These images support our prediction in Section 2.4 that the information\nencoded in the bottleneck representation h will be qualitatively different with a PixelCNN decoder\nthan with a more conventional decoder. For example, in the lowest row we can see that the model\ngenerates different but similar looking indoor scenes with people, instead of trying to exactly\nreconstruct the input.\n4\nConclusion\nThis work introduced the Gated PixelCNN, an improvement over the original PixelCNN that is able to\nmatch or outperform PixelRNN [30], and is computationally more ef\ufb01cient. In our new architecture,\nwe use two stacks of CNNs to deal with \u201cblind spots\u201d in the receptive \ufb01eld, which limited the original\nPixelCNN. Additionally, we use a gating mechanism which improves performance and convergence\nspeed. We have shown that the architecture gets similar performance to PixelRNN on CIFAR-10 and\nis now state-of-the-art on the ImageNet 32x32 and 64x64 datasets.\nFurthermore, using the Conditional PixelCNN we explored the conditional modelling of natural\nimages in three different settings. In class-conditional generation we showed that a single model is\nable to generate diverse and realistic looking images corresponding to different classes. On human\nportraits the model is capable of generating new images from the same person in different poses and\nlightning conditions from a single image. Finally, we demonstrated that the PixelCNN can be used as\na powerful image decoder in an autoencoder. In addition to achieving state of the art log-likelihood\nscores in all these datasets, the samples generated from our model are of very high visual quality\nshowing that the model captures natural variations of objects and lighting conditions.\nIn the future it might be interesting to try and generate new images with a certain animal or object\nsolely from a single example image [21, 22]. Another exciting direction would be to combine\nConditional PixelCNNs with variational inference to create a variational auto-encoder. In existing\nwork p(x|h) is typically modelled with a Gaussian with diagonal covariance and using a PixelCNN\ninstead could thus improve the decoder in VAEs. Another promising direction of this work would\nbe to model images based on an image caption instead of class label, as proposed by Mansimov et\n6\nAfrican elephant\nCoral Reef\nSandbar\nSorrel horse\nLhasa Apso (dog)\nLawn mower\nBrown bear\nRobin (bird)\nFigure 3: Class-Conditional samples from the Conditional PixelCNN.\nFigure 4: Left: source image. Right: new portraits generated from high-level latent representation.\nFigure 5: Linear interpolations in the embedding space decoded by the PixelCNN. Embeddings from\nleftmost and rightmost images are used for endpoints of the interpolation.\n7\nm = 10\nm = 100\nm = 10\nm = 100\nm = 10\nm = 100\nFigure 6: Left to right: original image, reconstruction by an auto-encoder trained with MSE,\nconditional samples from a PixelCNN auto-encoder. Both auto-encoders were trained end-to-end\nwith a m = 10-dimensional bottleneck and a m = 100 dimensional bottleneck.\nal. [15]. Because the alignDRAW model proposed by the authors tends to output blurry samples we\nbelieve that something akin to the Conditional PixelCNN could greatly improve those samples.\nReferences\n[1] Mart\u0131n Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S\nCorrado, Andy Davis, Jeffrey Dean, Matthieu Devin, et al. Tensor\ufb02ow: Large-scale machine learning on\nheterogeneous distributed systems. arXiv preprint arXiv:1603.04467, 2016.\n[2] Marc G Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom Schaul, David Saxton, and Remi Munos.\nUnifying count-based exploration and intrinsic motivation. arXiv preprint arXiv:1606.01868, 2016.\n[3] Emily L Denton, Soumith Chintala, Rob Fergus, et al. Deep generative image models using a laplacian\npyramid of adversarial networks. In Advances in Neural Information Processing Systems, pages 1486\u20131494,\n2015.\n[4] Laurent Dinh, David Krueger, and Yoshua Bengio. NICE: Non-linear independent components estimation.\narXiv preprint arXiv:1410.8516, 2014.\n[5] Leon A Gatys, Alexander S Ecker, and Matthias Bethge. A neural algorithm of artistic style. arXiv preprint\narXiv:1508.06576, 2015.\n[6] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron\nCourville, and Yoshua Bengio. Generative adversarial nets. In Advances in Neural Information Processing\nSystems, pages 2672\u20132680, 2014.\n[7] Alex Graves and J\u00fcrgen Schmidhuber. Of\ufb02ine handwriting recognition with multidimensional recurrent\nneural networks. In Advances in Neural Information Processing Systems, 2009.\n[8] Karol Gregor, Frederic Besse, Danilo J Rezende, Ivo Danihelka, and Daan Wierstra. Towards conceptual\ncompression. arXiv preprint arXiv:1601.06759, 2016.\n[9] Karol Gregor, Ivo Danihelka, Alex Graves, and Daan Wierstra. DRAW: A recurrent neural network for\nimage generation. Proceedings of the 32nd International Conference on Machine Learning, 2015.\n[10] Karol Gregor, Ivo Danihelka, Andriy Mnih, Charles Blundell, and Daan Wierstra. Deep autoregressive\nnetworks. In Proceedings of the 31st International Conference on Machine Learning, 2014.\n[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition.\narXiv preprint arXiv:1512.03385, 2015.\n[12] \u0141ukasz Kaiser and Ilya Sutskever. Neural gpus learn algorithms. arXiv preprint arXiv:1511.08228, 2015.\n[13] Nal Kalchbrenner, Ivo Danihelka, and Alex Graves. Grid long short-term memory. arXiv preprint\narXiv:1507.01526, 2015.\n8\n[14] Hugo Larochelle and Iain Murray. The neural autoregressive distribution estimator. The Journal of Machine\nLearning Research, 2011.\n[15] Elman Mansimov, Emilio Parisotto, Jimmy Lei Ba, and Ruslan Salakhutdinov. Generating images from\ncaptions with attention. arXiv preprint arXiv:1511.02793, 2015.\n[16] Jonathan Masci, Ueli Meier, Dan Cire\u00b8san, and J\u00fcrgen Schmidhuber. Stacked convolutional auto-encoders\nfor hierarchical feature extraction. In Arti\ufb01cial Neural Networks and Machine Learning\u2013ICANN 2011,\npages 52\u201359. Springer, 2011.\n[17] Junhyuk Oh, Xiaoxiao Guo, Honglak Lee, Richard L Lewis, and Satinder Singh. Action-conditional video\nprediction using deep networks in atari games. In Advances in Neural Information Processing Systems,\npages 2845\u20132853, 2015.\n[18] Christopher Olah and Mike Tyka. Inceptionism: Going deeper into neural networks. 2015.\n[19] Scott Reed, Zeynep Akata, Xinchen Yan, Lajanugen Logeswaran, Bernt Schiele, and Honglak Lee.\nGenerative adversarial text to image synthesis. arXiv preprint arXiv:1605.05396, 2016.\n[20] Danilo J Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and approximate\ninference in deep generative models. In Proceedings of the 31st International Conference on Machine\nLearning, 2014.\n[21] Danilo Jimenez Rezende, Shakir Mohamed, Ivo Danihelka, Karol Gregor, and Daan Wierstra. One-shot\ngeneralization in deep generative models. arXiv preprint arXiv:1603.05106, 2016.\n[22] Ruslan Salakhutdinov, Joshua B Tenenbaum, and Antonio Torralba. Learning with hierarchical-deep\nmodels. Pattern Analysis and Machine Intelligence, IEEE Transactions on, 35(8):1958\u20131971, 2013.\n[23] Florian Schroff, Dmitry Kalenichenko, and James Philbin. Facenet: A uni\ufb01ed embedding for face\nrecognition and clustering. In Proceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition, pages 815\u2013823, 2015.\n[24] Jascha Sohl-Dickstein, Eric A. Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised\nlearning using nonequilibrium thermodynamics. Proceedings of the 32nd International Conference on\nMachine Learning, 2015.\n[25] Rupesh K Srivastava, Klaus Greff, and J\u00fcrgen Schmidhuber. Training very deep networks. In Advances in\nNeural Information Processing Systems, pages 2368\u20132376, 2015.\n[26] Lucas Theis and Matthias Bethge. Generative image modeling using spatial LSTMs. In Advances in\nNeural Information Processing Systems, 2015.\n[27] Lucas Theis, Aaron van den Oord, and Matthias Bethge. A note on the evaluation of generative models.\narXiv preprint arXiv:1511.01844, 2015.\n[28] Benigno Uria, Marc-Alexandre C\u00f4t\u00e9, Karol Gregor, Iain Murray, and Hugo Larochelle. Neural autoregres-\nsive distribution estimation. arXiv preprint arXiv:1605.02226, 2016.\n[29] Aaron van den Oord and Joni Dambre. Locally-connected transformations for deep gmms. In International\nConference on Machine Learning (ICML) : Deep learning Workshop, Abstracts, pages 1\u20138, 2015.\n[30] Aaron van den Oord, Nal Kalchbrenner, and Koray Kavukcuoglu. Pixel recurrent neural networks. arXiv\npreprint arXiv:1601.06759, 2016.\n[31] A\u00e4ron van den Oord and Benjamin Schrauwen. Factoring variations in natural images with deep gaussian\nmixture models. In Advances in Neural Information Processing Systems, 2014.\n[32] Aaron van den Oord and Benjamin Schrauwen. The student-t mixture as a natural image patch prior with\napplication to image compression. The Journal of Machine Learning Research, 2014.\n9\nAppendix\nGeyser\nHartebeest\nGrey whale\nTiger\nEntleBucher (dog)\nYellow lady\u2019s slipper (\ufb02ower)\nFigure 7: Class-Conditional (multi-scale 64 \u00d7 64) samples from the Conditional Pixel CNN.\n10\nAfrican elephant\nCoral Reef\nSandbar\nSorrel horse\nLhasa Apso (dog)\nLawn mower\nBrown bear\nRobin (bird)\nFigure 8: Class-Conditional 32 \u00d7 32 samples from the Conditional Pixel CNN.\n11\nFigure 9: Left: source image. Right: new portraits generated from high-level latent representation.\nFigure 10: Linear interpolations in the embedding space decoded by the Pixel CNN. Embeddings\nfrom leftmost and rightmost images are used for endpoints of the interpolation.\n12\nm = 10\nm = 100\nm = 10\nm = 100\nm = 10\nm = 100\nm = 10\nm = 100\nm = 10\nm = 100\nm = 10\nm = 100\nm = 10\nm = 100\nFigure 11: Left to right: original image, reconstruction by an auto-encoder trained with MSE,\nconditional samples from a Pixel CNN auto-encoder. Both auto-encoders were trained end-to-end\nwith a m = 10-dimensional bottleneck and a m = 100 dimensional bottleneck.\n13\n",
        "sentence": "",
        "context": "[31] A\u00e4ron van den Oord and Benjamin Schrauwen. Factoring variations in natural images with deep gaussian\nmixture models. In Advances in Neural Information Processing Systems, 2014.\n[32] Aaron van den Oord and Benjamin Schrauwen. The student-t mixture as a natural image patch prior with\napplication to image compression. The Journal of Machine Learning Research, 2014.\n9\nAppendix\nGeyser\nHartebeest\nGrey whale\nTiger\nEntleBucher (dog)\nCorrado, Andy Davis, Jeffrey Dean, Matthieu Devin, et al. Tensor\ufb02ow: Large-scale machine learning on\nheterogeneous distributed systems. arXiv preprint arXiv:1603.04467, 2016."
    },
    {
        "title": "Clustering of the selforganizing",
        "author": [
            "J. Vesanto",
            "E. Alhoniemi"
        ],
        "venue": "map. Transactions on neural networks,",
        "citeRegEx": "Vesanto and Alhoniemi,? \\Q2000\\E",
        "shortCiteRegEx": "Vesanto and Alhoniemi",
        "year": 2000,
        "abstract": "",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion",
        "author": [
            "P. Vincent",
            "H. Larochelle",
            "I. Lajoie",
            "Y. Bengio",
            "Manzagol",
            "P.-A"
        ],
        "venue": "JMLR, 11(Dec):3371\u20133408,",
        "citeRegEx": "Vincent et al\\.,? \\Q2010\\E",
        "shortCiteRegEx": "Vincent et al\\.",
        "year": 2010,
        "abstract": "",
        "full_text": "",
        "sentence": " , 2014), and to a lesser extent, noisy autoencoders (Vincent et al., 2010).",
        "context": null
    },
    {
        "title": "Unsupervised learning of visual representations using videos",
        "author": [
            "X. Wang",
            "A. Gupta"
        ],
        "venue": "In ICCV,",
        "citeRegEx": "Wang and Gupta,? \\Q2015\\E",
        "shortCiteRegEx": "Wang and Gupta",
        "year": 2015,
        "abstract": "Is strong supervision necessary for learning a good visual representation? Do\nwe really need millions of semantically-labeled images to train a Convolutional\nNeural Network (CNN)? In this paper, we present a simple yet surprisingly\npowerful approach for unsupervised learning of CNN. Specifically, we use\nhundreds of thousands of unlabeled videos from the web to learn visual\nrepresentations. Our key idea is that visual tracking provides the supervision.\nThat is, two patches connected by a track should have similar visual\nrepresentation in deep feature space since they probably belong to the same\nobject or object part. We design a Siamese-triplet network with a ranking loss\nfunction to train this CNN representation. Without using a single image from\nImageNet, just using 100K unlabeled videos and the VOC 2012 dataset, we train\nan ensemble of unsupervised networks that achieves 52% mAP (no bounding box\nregression). This performance comes tantalizingly close to its\nImageNet-supervised counterpart, an ensemble which achieves a mAP of 54.4%. We\nalso show that our unsupervised network can perform competitively in other\ntasks such as surface-normal estimation.",
        "full_text": "Unsupervised Learning of Visual Representations using Videos\nXiaolong Wang, Abhinav Gupta\nRobotics Institute, Carnegie Mellon University\nAbstract\nIs strong supervision necessary for learning a good\nvisual representation?\nDo we really need millions of\nsemantically-labeled images to train a Convolutional Neu-\nral Network (CNN)? In this paper, we present a simple yet\nsurprisingly powerful approach for unsupervised learning\nof CNN. Speci\ufb01cally, we use hundreds of thousands of un-\nlabeled videos from the web to learn visual representations.\nOur key idea is that visual tracking provides the supervi-\nsion. That is, two patches connected by a track should have\nsimilar visual representation in deep feature space since\nthey probably belong to the same object or object part. We\ndesign a Siamese-triplet network with a ranking loss func-\ntion to train this CNN representation. Without using a sin-\ngle image from ImageNet, just using 100K unlabeled videos\nand the VOC 2012 dataset, we train an ensemble of un-\nsupervised networks that achieves 52% mAP (no bound-\ning box regression). This performance comes tantalizingly\nclose to its ImageNet-supervised counterpart, an ensemble\nwhich achieves a mAP of 54.4%. We also show that our\nunsupervised network can perform competitively in other\ntasks such as surface-normal estimation.\n1. Introduction\nWhat is a good visual representation and how can we\nlearn it? At the start of this decade, most computer vision\nresearch focused on \u201cwhat\u201d and used hand-de\ufb01ned features\nsuch as SIFT [32] and HOG [5] as the underlying visual\nrepresentation. Learning was often the last step where these\nlow-level feature representations were mapped to seman-\ntic/3D/functional categories. However, the last three years\nhave seen the resurgence of learning visual representations\ndirectly from pixels themselves using the deep learning\nand Convolutional Neural Networks (CNNs) [28, 24, 23].\nAt the heart of CNNs is a completely supervised learning\nparadigm. Often millions of examples are \ufb01rst labeled us-\ning Mechanical Turk followed by data augmentation to cre-\nate tens of millions of training instances. CNNs are then\ntrained using gradient descent and back propagation. But\none question still remains: is strong-supervision necessary\nfor training these CNNs? Do we really need millions of\nsemantically-labeled images to learn a good representation?\n\u2026 \n\u2026 \n\u2026 \n\u2026 \nLearning to Rank \nConv \nNet \nConv \nNet \nConv \nNet \nQuery \n(First Frame) \nTracked \n(Last Frame) \nNegative  \n(Random) \n(a) Unsupervised Tracking in Videos  \n\ud835\udc37 \n, \n\ud835\udc37 \n, \n\ud835\udc37 \n, \n\ud835\udc37 \n, \n\ud835\udc37: Distance in deep feature space \n(b) Siamese-triplet Network \n(c) Ranking Objective  \nFigure 1. Overview of our approach. (a) Given unlabeled videos,\nwe perform unsupervised tracking on the patches in them. (b)\nTriplets of patches including query patch in the initial frame of\ntracking, tracked patch in the last frame, and random patch from\nother videos are fed into our siamese-triplet network for train-\ning. (c) The learning objective: Distance between the query and\ntracked patch in feature space should be smaller than the distance\nbetween query and random patches.\nIt seems humans can learn visual representations using little\nor no semantic supervision but our approaches still remain\ncompletely supervised.\nIn this paper, we explore the alternative: how we can ex-\nploit the unlabeled visual data on the web to train CNNs\n(e.g. AlexNet [24])? In the past, there have been several at-\ntempts at unsupervised learning using millions of static im-\nages [26, 44] or frames extracted from videos [56, 48, 34].\nThe most common architecture used is an auto-encoder\nwhich learns representations based on its ability to recon-\nstruct the input images [35, 3, 49, 37]. While these ap-\nproaches have been able to automatically learn V1-like \ufb01l-\nters given unlabeled data, they are still far away from su-\npervised approaches on tasks such as object detection. So,\nwhat is the missing link? We argue that static images them-\nselves might not have enough information to learn a good\n1\narXiv:1505.00687v2  [cs.CV]  6 Oct 2015\nvisual representation. But what about videos? Do they have\nenough information to learn visual representations? In fact,\nhumans also learn their visual representations not from mil-\nlions of static images but years of dynamic sensory inputs.\nCan we have similar learning capabilities for CNNs?\nWe present a simple yet surprisingly powerful approach\nfor unsupervised learning of CNNs using hundreds of thou-\nsands of unlabeled videos from the web. Visual tracking is\none of the \ufb01rst capabilities that develops in infants and often\nbefore semantic representations are learned1. Taking a leaf\nfrom this observation, we propose to exploit visual track-\ning for learning CNNs in an unsupervised manner. Speci\ufb01-\ncally, we track millions of \u201cmoving\u201d patches in hundreds of\nthousands of videos. Our key idea is that two patches con-\nnected by a track should have similar visual representation\nin deep feature space since they probably belong to the same\nobject. We design a Siamese-triplet network with ranking\nloss function to train the CNN representation. This ranking\nloss function enforces that in the \ufb01nal deep feature space\nthe \ufb01rst frame patch should be much closer to the tracked\npatch than any other randomly sampled patch. We demon-\nstrate the strength of our learning algorithm using exten-\nsive experimental evaluation. Without using a single image\nfrom ImageNet, just 100K unlabeled videos and VOC 2012\ndataset, we train an ensemble of AlexNet networks that\nachieves 52% mAP (no bounding box regression). This per-\nformance is similar to its ImageNet-supervised counterpart,\nan ensemble which achieves 54.4% mAP. We also show that\nour network trained using unlabeled videos achieves simi-\nlar performance to its completely supervised counterpart on\nother tasks such as surface normal estimation. We believe\nthis is the \ufb01rst time an unsupervised-pretrained CNN has\nbeen shown so competitive; that too on varied datasets and\ntasks. Speci\ufb01cally for VOC, we would like to put our re-\nsults in context: this is the best results till-date by using\nonly PASCAL-provided annotations (next best is scratch at\n44%).\n2. Related Work\nUnsupervised learning of visual representations has a\nrich history starting from original auto-encoders work of\nOlhausen and Field [35]. Most of the work in this area\ncan be broadly divided into three categories.\nThe \ufb01rst\nclass of algorithms focus on learning generative models\nwith strong priors [20, 46]. These algorithms essentially\ncapture co-occurrence statistics of features.\nThe second\nclass of algorithms use manually de\ufb01ned features such as\nSIFT or HOG and perform clustering over training data\nto discover semantic classes [42, 38]. Some of these re-\ncent algorithms also focus on learning mid-level repre-\nsentations rather than discovering semantic classes them-\n1http://www.aoa.org/patients-and-public/good-vision-throughout-\nlife/childrens-vision/infant-vision-birth-to-24-months-of-age\nselves [41, 6, 7]. The third class of algorithms and more\nrelated to our paper is unsupervised learning of visual rep-\nresentations from the pixels themselves using deep learning\napproaches [21, 26, 44, 39, 29, 47, 9, 33, 2, 49, 8]. Starting\nfrom the seminal work of Olhausen and Field [35], the goal\nis to learn visual representations which are (a) sparse and\n(b) reconstructive. Olhausen and Field [35] showed that us-\ning this criteria they can learn V1-like \ufb01lters directly from\nthe data. However, this work only focused on learning a sin-\ngle layer. This idea was extended by Hinton and Salakhut-\ndinov [21] to train a deep belief network in an unsuper-\nvised manner via stacking layer-by-layer RBMs. Similar to\nthis, Bengio et al. [3] investigated stacking of both RBMs\nand autoencoders. As a next step, Le et al. [26] scaled up\nthe learning of multi-layer autoencoder on large-scale unla-\nbeled data. They demonstrated that although the network is\ntrained in an unsupervised manner, the neurons in high lay-\ners can still have high responses on semantic objects such\nas human heads and cat faces. Sermanet et al. [39] applied\nconvolutional sparse coding to pre-train the model layer-by-\nlayer in unsupervised manner. The model is then \ufb01ne-tuned\nfor pedestrian detection. In a contemporary work, Doersch\net al. [8] explored to use spatial context as a cue to perform\nunsupervised learning for CNNs.\nHowever, it is not clear if static images is the right way\nto learn visual representations. Therefore, researchers have\nstarted focusing on learning feature representations using\nvideos [11, 53, 27, 43, 56, 16, 48, 34, 45].\nEarly work\nsuch as [56] focused on inclusion of constraints via video\nto autoencoder framework. The most common constraint is\nenforcing learned representations to be temporally smooth.\nSimilar to this, Goroshin et al. [16] proposed to learn auto-\nencoders based on the slowness prior. Other approaches\nsuch as Taylor et al. [48] trained convolutional gated RBMs\nto learn latent representations from pairs of successive im-\nages. This was extended in a recent work by Srivastava et\nal. [43] where they proposed to learn a LSTM model in an\nunsupervised manner to predict future frames.\nFinally, our work is also related to metric learning via\ndeep networks [51, 31, 4, 17, 15, 22, 54]. For example,\nChopra et al. [4] proposed to learn convolutional networks\nin a siamese architecture for face veri\ufb01cation.\nWang et\nal. [51] introduced a deep triplet ranking network to learn\n\ufb01ne-grained image similarity. Zhang et al. [55] optimized\nthe max-margin loss on triplet units to learn deep hashing\nfunction for image retrieval. However, all these methods\nrequired labeled data.\nOur work is also related to [30],\nwhich used CNN pre-trained on ImageNet classi\ufb01cation\nand detection dataset as initialization, and performed semi-\nsupervised learning in videos to tackle object detection in\ntarget domain. However, in our work, we propose an unsu-\npervised approach instead of semi-supervised algorithm.\n3. Overview\nOur goal is to train convolutional neural networks using\nhundreds of thousands of unlabeled videos from the Inter-\nnet. We follow the AlexNet architecture to design our base\nnetwork. However, since we do not have labels, it is not\nclear what should be the loss function and how we should\noptimize it. But in case of videos, we have another supervi-\nsory information: time. For example, we all know that the\nscene does not change drastically within a short time in a\nvideo and same object instances appear in multiple frames\nof the video. So, how do we exploit this information to train\na CNN-based representation?\nWe sample millions of patches in these videos and track\nthem over time. Since we are tracking these patches, we\nknow that the \ufb01rst and last tracked frames correspond to the\nsame instance of the moving object or object part. There-\nfore, any visual representation that we learn should keep\nthese two data points close in the feature space. But just us-\ning this constraint is not suf\ufb01cient: all points can be mapped\nto a single point in feature space. Therefore, for training our\nCNN, we sample a third patch which creates a triplet. For\ntraining, we use a loss function [51] that enforces that the\n\ufb01rst two patches connected by tracking are closer in feature\nspace than the \ufb01rst one and a random one.\nTraining a network with such triplets converges fast since\nthe task is easy to over\ufb01t to. One way is to increase the\nnumber of training triplets. However, after initial conver-\ngence most triplets satisfy the loss function and therefore\nback-propagating gradients using such triplets is inef\ufb01cient.\nInstead, analogous to hard-negative mining, we select the\nthird patch from multiple patches that violates the constraint\n(loss is maximum).\nSelecting this patch leads to more\nmeaningful gradients for faster learning.\n4. Patch Mining in Videos\nGiven a video, we want to extract patches of interest\n(patches with motion in our case) and track these patches to\ncreate training instances. One obvious way to \ufb01nd patches\nof interest is to compute optical \ufb02ow and use the high mag-\nnitude \ufb02ow regions. However, since YouTube videos are\nnoisy with a lot of camera motion, it is hard to localize\nmoving objects using simple optical \ufb02ow magnitude vec-\ntors. Thus we follow a two-step approach: in the \ufb01rst step,\nwe obtain SURF [1] interest points and use Improved Dense\nTrajectories (IDT) [50] to obtain motion of each SURF\npoint. Note that since IDT applies a homography estimation\n(video stabilization) method, it reduces the problem caused\nby camera motion. Given the trajectories of SURF inter-\nest points, we classify these points as moving if the \ufb02ow\nmagnitude is more than 0.5 pixels. We also reject frames\nif (a) very few (< 25%) SURF interest points are classi\ufb01ed\nas moving because it might be just noise; (b) majority of\nSURF interest points (> 75%) are classi\ufb01ed as moving as\n\u2026 \n\u2026 \nQuery \n(First Frame) \nTracked \n(Last Frame) \nSliding Window Searching \nTracking \nSmall Motion \nCamera Motion \nFigure 2.\nGiven the video about buses (the \u201cbus\u201d label are not\nutilized), we perform IDT on it. red points represents the SURF\nfeature points, green represents the trajectories for the points. We\nreject the frames with small and large camera motions (top pairs).\nGiven the selected frame, we \ufb01nd the bounding box containing\nmost of the moving SURF points. We then perform tracking. The\n\ufb01rst and last frame of the track provide pair of patches for training\nCNN.\nit corresponds to moving camera. Once we have extracted\nmoving SURF interest points, in the second step, we \ufb01nd the\nbest bounding box such that it contains most of the moving\nSURF points. The size of the bounding box is set as h \u00d7 w,\nand we perform sliding window with it in the frame. We\ntake the bounding box which contains the most number of\nmoving SURF interest points as the interest bounding box.\nIn the experiment, we set h = 227, w = 227 in the frame\nwith size 448 \u00d7 600. Note that these patches might contain\nobjects or part of an object as shown in Figure 2.\nTracking.\nGiven the initial bounding box, we perform\ntracking using the KCF tracker [19]. After tracking along 30\nframes in the video, we obtain the second patch. This patch\nacts as the similar patch to the query patch in the triplet.\nNote that the KCF tracker does not use any supervised in-\nformation except for the initial bounding box.\n5. Learning Via Videos\nIn the previous section, we discussed how we can use\ntracking to generate pairs of patches. We use this procedure\nto generate millions of such pairs (See Figure 3 for exam-\nples of pairs of patches mined). We now describe how we\nuse these as training instances for our visual representation\nlearning.\n5.1. Siamese Triplet Network\nOur goal is to learn a feature space such that the query\npatch is closer to the tracked patch as compared to any other\nrandomly sampled patch. To learn this feature space we de-\nsign a Siamese-triplet network. A Siamese-triplet network\nconsist of three base networks which share the same param-\nQuery \n(First Frame) \nTracked \n(Last Frame) \nQuery \n(First Frame) \nTracked \n(Last Frame) \nPatch \nPairs \nPatch \nPairs \nFigure 3. Examples of patch pairs we obtain via patch mining in the videos.\neters (see Figure 4). For our experiments, we take the image\nwith size 227 \u00d7 227 as input. The base network is based\non the AlexNet architecture [24] for the convolutional lay-\ners. Then we stack two fully connected layers on the pool5\noutputs, whose neuron numbers are 4096 and 1024 respec-\ntively. Thus the \ufb01nal output of each single network is 1024\ndimensional feature space f(\u00b7). We de\ufb01ne the loss function\non this feature space.\n5.2. Ranking Loss Function\nGiven the set of patch pairs S sampled from the video,\nwe propose to learn an image similarity model in the form\nof CNN. Speci\ufb01cally, given an image X as an input for the\nnetwork, we can obtain its feature in the \ufb01nal layer as f(X).\nThen, we de\ufb01ne the distance of two image patches X1, X2\nbased on the cosine distance in the feature space as,\nD(X1, X2) = 1 \u2212\nf(X1) \u00b7 f(X2)\n\u2225f(X1)\u2225\u2225f(X2)\u2225.\n(1)\nWe want to train a CNN to obtain feature representation\nf(\u00b7), so that the distance between query image patch and the\ntracked patch is small and the distance between query patch\nand other random patches is encouraged to be larger. For-\nmally, given the patch set S, where Xi is the original query\npatch (\ufb01rst patch in tracked frames), X+\ni is the tracked patch\nand X\u2212\ni is a random patch from a different video, we want\nto enforce D(Xi, X\u2212\ni ) > D(Xi, X+\ni ). Therefore, the loss\nof our ranking model is de\ufb01ned by hinge loss as,\nL(Xi, X+\ni , X\u2212\ni ) = max{0, D(Xi, X+\ni ) \u2212D(Xi, X\u2212\ni ) + M},\n(2)\nwhere M represents the gap parameters between two dis-\ntances. We set M = 0.5 in the experiment. Then our objec-\ntive function for training can be represented as,\nmin\nW\n\u03bb\n2 \u2225W \u22252\n2 +\nN\nX\ni=1\nmax{0, D(Xi, X+\ni ) \u2212D(Xi, X\u2212\ni ) + M},\n(3)\nwhere W is the parameter weights of the network, i.e., pa-\nrameters for function f(\u00b7). N is the number of the triplets of\nsamples. \u03bb is a constant representing weight decay, which\nis set to \u03bb = 0.0005.\n5.3. Hard Negative Mining for Triplet Sampling\nOne non-trivial part for learning to rank is the process of\nselecting negative samples. Given a pair of similar images\nXi, X+\ni , how can we select the patch X\u2212\ni , which is a nega-\ntive match to Xi, from the large pool of patches? Here we\n\ufb01rst select the negative patches randomly, and then \ufb01nd hard\nexamples (in a process analogous to hard negative mining).\nRandom Selection:\nDuring learning, we perform\nmini-batch Stochastic Gradient Descent (SGD). For each\nXi, X+\ni , we randomly sample K negative matches in the\nsame batch B, thus we have K sets of triplet of samples.\nFor every triplet of samples, we calculate the gradients over\nthree of them respectively and perform back propagation.\nNote that we shuf\ufb02e all the images randomly after each\nepoch of training, thus the pair of patches Xi, X+\ni can look\nat different negative matches each time.\nHard Negative Mining: While one can continue to sam-\nple random patches for creating the triplets, it is more ef\ufb01-\ncient to search the negative patches smartly. After 10 epochs\nof training using negative data selected randomly, we want\nto make the problem harder to get more robust feature rep-\nresentations. Analogous to hard-negative mining procedure\nin SVM, where gradient descent learning is only performed\non hard-negatives (not all possible negative), we search for\n\ud835\udc4b\ud835\udc56\n+ \n\ud835\udc4b\ud835\udc56\n\u2212 \n\ud835\udc4b\ud835\udc56 \n\ud835\udc53(\ud835\udc4b\ud835\udc56\n+) \n\ud835\udc53(\ud835\udc4b\ud835\udc56\n\u2212) \n\ud835\udc53(\ud835\udc4b\ud835\udc56) \nRanking  \nLoss \nLayer \nShared Weights \nShared Weights \n96 256 384 \n384 \n256 \n4096 \n1024 \nFigure 4.\nSiamese-triplet network. Each base network in the\nSiamese-triplet network share the same architecture and parameter\nweights. The architecture is recti\ufb01ed from AlexNet by using only\ntwo fully connected layers. Given a triplet of training samples,\nwe obtain their features from the last layer by forward propagation\nand compute the ranking loss.\nFigure 5.\nTop response regions for the pool5 neurons of our\nunsupervised-CNN. Each row shows top response of one neuron.\nthe negative patch such that the loss is maximum and use\nthat patch to compute and back propagate gradients.\nSpeci\ufb01cally, the sampling of negative matches is similar\nas random selection before, except that this time we select\naccording to the loss(Eq. 2). For each pair Xi, X+\ni , we cal-\nculate the loss of all other negative matches in batch B, and\nselect the top K ones with highest losses. We apply the loss\non these K negative matches as our \ufb01nal loss and calculate\nthe gradients over them. Since the feature of each sample\nis already computed after the forward propagation, we only\nneed to calculate the loss over these features, thus the extra\ncomputation for hard negative mining is very small. For the\nexperiments, we use K = 4. Note that while some of the\nnegatives might be semantically similar patches, our em-\nbedding constraint only requires same instance examples to\nbe closer than category examples (which can be closer than\nother negatives in the space).\n5.4. Adapting for Supervised Tasks\nGiven the CNN learned by using unsupervised data, we\nwant to transfer the learned representations to the tasks with\nsupervised data. In our experiments, we apply our model\nto two different tasks including object detection and sur-\nface normal estimation. In both tasks we take the base net-\nwork from our Siamese-triplet network and adjust the fully\nconnected layers and outputs accordingly.\nWe introduce\ntwo ways to \ufb01ne-tune and transfer the information obtained\nfrom unsupervised data to supervised learning.\nOne straight forward approach is directly applying our\nranking model as a pre-trained network for the target task.\nMore speci\ufb01cally, we use the parameters of the convolu-\ntional layers in the base network of our triplet architecture\nas initialization for the target task. For the fully connected\nlayers, we initialize them randomly. This method of trans-\nferring feature representation is very similar to the approach\napplied in RCNN [14]. However, RCNN uses the network\npre-trained with ImageNet Classi\ufb01cation data. In our case,\nthe unsupervised ranking task is quite different from object\ndetection and surface normal estimation. Thus, we need\nto adapt the learning rate to the \ufb01ne-tuning procedure in-\ntroduced in RCNN. We start with the learning rate with\n\u03f5 = 0.01 instead of 0.001 and set the same learning rate\nfor all layers. This setting is crucial since we want the pre-\ntrained features to be used as initialization of supervised\nlearning, and adapting the features to the new task.\nIn this paper,\nwe explore one more approach to\ntransfer/\ufb01ne-tune the network. Speci\ufb01cally, we note that\nthere might be more juice left in the millions of unsuper-\nvised training data (which could not be captured in the ini-\ntial learning stage).\nTherefore, we use an iterative \ufb01ne-\ntuning scheme. Given the initial unsupervised network, we\n\ufb01rst \ufb01ne-tune using the PASCAL VOC data. Given the new\n\ufb01ne-tuned network, we use this network to re-adapt to rank-\ning triplet task. Here we again transfer convolutional pa-\nrameters for re-adapting. Finally, this re-adapted network is\n\ufb01ne-tuned on the VOC data yielding a better trained model.\nWe show in the experiment that this circular approach gives\nimprovement in performance. We also notice that after two\niterations of this approach the network converges.\n5.5. Model Ensemble\nWe proposed an approach to learn CNNs using unlabeled\nvideos. However, there is absolutely no limit to generating\ntraining instances and pairs of tracked patches (YouTube\nhas more than billions of videos). This opens up the possi-\nbility of training multiple CNNs using different sets of data.\nOnce we have trained these CNNs, we append the fc7 fea-\ntures from each of these CNNs to train the \ufb01nal SVM. Note\nthat the ImageNet trained models also provide initial boost\nfor adding more networks (See Table 1).\n5.6. Implementation Details\nWe apply mini-batch SGD in training. As the 3 networks\nshare the same parameters, instead of inputting 3 samples\nto the triplet network, we perform the forward propagation\nfor the whole batch by a single network and calculate the\nloss based on the output feature. Given a pair of patches\nXi, X+\ni , we randomly select another patch X\u2212\ni \u2208B which\nis extracted in a different video from Xi, X+\ni . Given their\nfeatures from forward propagation f(Xi), f(X+\ni ), f(X\u2212\ni ),\nwe can compute the loss as Eq. 2.\nFor unsupervised learning, we download 100K videos\nfrom YouTube using the URLs provided by [30]. [30] used\nthousands of keywords to retrieve videos from YouTube.\nNote we drop the labels associated with each video. By per-\nforming our patch mining method on the videos, we obtain\n8 million image patches. We train three different networks\nseparately using 1.5M, 5M and 8M training samples. We\nreport numbers based on these three networks. To train our\nsiamese-triplet networks, we set the batch size as |B| = 100,\n(a) Unsupervised Pre-trained \n(b) Fine-tuned  \nFigure 6.\nConv1 \ufb01lters visualization. (a) The \ufb01lters of the \ufb01rst\nconvolutional layer of the siamese-triplet network trained in unsu-\npervised manner. (b) By \ufb01ne-tuning the unsupervised pre-trained\nnetwork on PASCAL VOC 2012, we obtain sharper \ufb01lters.\nthe learning rate starting with \u03f50 = 0.001. We \ufb01rst train our\nnetwork with random negative samples at this learning rate\nfor 150K iterations, and then we apply hard negative min-\ning based on it. For training on 1.5M patches, we reduce\nthe learning rate by a factor of 10 at every 80K iterations\nand train for 240K iterations. For training on 5M and 8M\npatches, we reduce the learning rate by a factor of 10 at ev-\nery 120K iterations and train for 350K iterations.\n6. Experiments\nWe demonstrate the quality of our learned visual rep-\nresentations with qualitative and quantitative experiments.\nQualitatively, we show the convolutional \ufb01lters learned in\nlayer 1 (See Figure 6). Our learned \ufb01lters are similar to V1\nthough not as strong. However, after \ufb01ne-tuning on PAS-\nCAL VOC 2012, these \ufb01lters become quite strong. We also\nshow that the underlying representation learns a reasonable\nnearness metric by showing what the units in Pool5 layers\nrepresent (See Figure 5). Ignoring boundary effects, each\npool5 unit has a receptive \ufb01eld of 195 \u00d7 195 pixels in the\noriginal 227 \u00d7 227 pixel input. A central pool5 unit has a\nnearly global view, while one near the edge has a smaller,\nclipped support. Each row displays top 6 activations for a\npool5 unit. We have chosen 5 pool5 units for visualization.\nFor example, the \ufb01rst neuron represents animal heads, sec-\nond represents potted plant, etc. This visualization indicates\nthe nearness metric learned by the network since each row\ncorresponds to similar \ufb01ring patterns inside the CNN. Our\nunsupervised networks are available for download.\n6.1. Unsupervised CNNs without Fine-tuning\nFirst, we demonstrate that the unsupervised-CNN rep-\nresentation learned using videos (without \ufb01ne-tuning) is\nreasonable.\nWe perform Nearest Neighbors (NN) using\nground-truth (GT) windows in VOC 2012 val set as query.\nThe retrieval-database consists of all selective search win-\ndows (more than 0.5 overlap with GT windows) in VOC\n2012 train set. See Figure 7 for qualitative results. Our\nunsupervised-CNN is far superior to a random AlexNet ar-\nchitecture and the results are quite comparable to AlexNet\ntrained on ImageNet.\nQuantitatively, we measure the retrieval rate by counting\nnumber of correct retrievals in top-K (K=20) retrievals. A\nretrieval is correct if the semantic class for retrieved patch\nand query patch are the same. Using our unsupervised-CNN\n(Pool5 features) without \ufb01ne-tuning and cosine distance, we\nobtain 40% retrieval rate. Our performance is signi\ufb01cantly\nbetter as compared to 24% by ELDA [18] on HOG and\n19% by AlexNet with random parameters (our initializa-\ntion). This clearly demonstrates our unsupervised network\nlearns a good visual representation compared to a random\nparameter CNN. As a baseline, ImageNet CNN performs\n62% (but note it already learns on semantics).\nWe also evaluate our unsupervised-CNN without \ufb01ne-\ntuning for scene classi\ufb01cation task on MIT Indoor 67 [36].\nWe train a linear classi\ufb01er using softmax loss.\nUsing\npool5 features from unsupervised-CNN without \ufb01ne-tuning\ngives 41% classi\ufb01cation accuracy compared to 21% for\nGIST+SVM and 16% for random AlexNet.\nImageNet-\ntrained AlexNet has 54% accuracy. We also provide object\ndetection results without \ufb01ne-tuning in the next section.\n6.2. Unsupervised CNNs with Fine-tuning\nNext, we evaluate our approach by transferring the fea-\nture representation learned in unsupervised manner to the\ntasks with labeled data. We focus on two challenging prob-\nlems: object detection and surface normal estimation.\n6.2.1\nObject Detection\nFor object detection, we perform our experiments on PAS-\nCAL VOC 2012 dataset [10].\nWe follow the detection\npipeline introduced in RCNN [14], which borrowed the\nCNNs pre-trained on other datasets and \ufb01ne-tuned on it with\nthe VOC data. The \ufb01ne-tuned CNN was then used to extract\nfeatures followed by training SVMs for each object class.\nHowever, instead of using ImageNet pre-trained network as\ninitialization in RCNN, we use our unsupervised-CNN. We\n\ufb01ne-tune our network with the trainval set (11540 images)\nand train SVMs with them. Evaluation is performed in the\nstandard test set (10991 images).\nAt the \ufb01ne-tuning stage, we change the output to 21\nclasses and initialize the convolutional layers with our unsu-\npervised pre-trained network. To \ufb01ne-tune the network, we\nstart with learning rate as \u03f5 = 0.01 and reduce the learning\nrate by a factor of 10 at every 80K iterations. The network\nis \ufb01ne-tuned for 200K iterations. Note that for all the exper-\niments, no bounding box regression is performed.\nQuery \n(a) Random AlexNet \n(b) Imagenet AlexNet \n(c) Unsupervised AlexNet \nFigure 7. Nearest neighbors results. Given the query object from VOC 2012 val, we retrieve the NN from VOC 2012 train via calculating\nthe cosine distance on pool5 feature space. We compare the results of 3 different models: (a) AlexNet with random parameters; (b) AlexNet\ntrained with Imagenet data; (c) AlexNet trained using our unsupervised method on 8M data.\nTable 1. mean Average Precision (mAP) on VOC 2012. \u201cexternal\u201d column shows the number of patches used to pre-train unsupervised-CNN.\nVOC 2012 test\nexternal aero bike bird boat bottle\nbus\ncar\ncat\nchair cow table\ndog\nhorse mbike person plant sheep sofa train\ntv\nmAP\nscratch\n0\n66.1 58.1 32.7 23.0\n21.8\n54.5 56.4 50.8\n21.6\n42.2 31.8 49.2\n49.8\n61.6\n52.1\n25.1\n52.6\n31.3 50.0\n49.1\n44.0\nscratch (3 ensemble)\n0\n68.7 61.2 36.1 25.7\n24.3\n58.9 58.8 55.3\n24.4\n43.5 36.7 53.0\n53.8\n65.6\n54.3\n27.3\n53.5\n38.3 54.6\n51.8\n47.3\nunsup + ft\n1.5M\n68.8 62.1 34.7 25.3\n26.6\n57.7 59.6 56.3\n22.0\n42.6 33.8 52.3\n50.3\n65.6\n53.9\n25.8\n51.5\n32.3 51.7\n51.8\n46.2\nunsup + ft\n5M\n69.0 64.0 37.1 23.6\n24.6\n58.7 58.9 59.6\n22.3\n46.0 35.1 53.3\n53.7\n66.9\n54.1\n25.4\n52.9\n31.2 51.9\n51.8\n47.0\nunsup + ft\n8M\n67.6 63.4 37.3 27.6\n24.0\n58.7 59.9 59.5\n23.7\n46.3 37.6 54.8\n54.7\n66.4\n54.8\n25.8\n52.5\n31.2 52.6\n52.6\n47.5\nunsup + ft (2 ensemble)\n6.5M\n72.4 66.2 41.3 26.4\n26.8\n61.0 61.9 63.1\n25.3\n51.0 38.7 58.1\n58.3\n70.0\n56.2\n28.6\n56.1\n38.5 55.9\n54.3\n50.5\nunsup + ft (3 ensemble)\n8M\n73.4 67.3 44.1 30.4\n27.8\n63.3 62.6 64.2\n27.7\n51.1 40.6 60.8\n59.2\n71.2\n58.5\n28.2\n55.6\n39.4 58.0\n56.1\n52.0\nunsup + iterative ft\n5M\n67.7 64.0 41.3 25.3\n27.3\n58.8 60.3 60.2\n24.3\n46.7 34.4 53.6\n53.8\n68.2\n55.7\n26.4\n51.1\n34.3 53.4\n52.3\n48.0\nRCNN 70K\n72.7 62.9 49.3 31.1\n25.9\n56.2 53.0 70.0\n23.3\n49.0 38.0 69.5\n60.1\n68.2\n46.4\n17.5\n57.2\n46.2 50.8\n54.1\n50.1\nRCNN 70K (2 ensemble)\n75.3 68.3 53.1 35.2\n27.7\n59.6 54.7 73.4\n26.5\n53.0 42.2 73.1\n66.1\n71.0\n48.5\n21.7\n59.2\n50.8 55.2\n58.0\n53.6\nRCNN 70K (3 ensemble)\n74.6 68.7 54.9 35.7\n29.4\n61.0 54.4 74.0\n28.4\n53.6 43.0 74.0\n66.1\n72.8\n50.3\n20.5\n60.0\n51.2 57.9\n58.0\n54.4\nRCNN 200K (big stepsize)\n73.3 67.1 46.3 31.7\n30.6\n59.4 61.0 67.9\n27.3\n53.1 39.1 64.1\n60.5\n70.9\n57.2\n26.1\n59.0\n40.1 56.2\n54.9\n52.3\nWe compare our method with the model trained from\nscratch as well as using ImagNet pre-trained network. No-\ntice that the results for VOC 2012 reported in RCNN [14]\nare obtained by only \ufb01ne-tuning on the train set without\nusing the val set. For fair comparison, we \ufb01ne-tuned the\nImageNet pre-trained network with VOC 2012 trainval set.\nMoreover, as the step size of reducing learning rate in\nRCNN [14] is set to 20K and iterations for \ufb01ne-tuning is\n70K, we also try to enlarge the step size to 50K and \ufb01ne-\ntune the network for 200K iterations. We report the results\nfor both of these settings.\nSingle Model. We show the results in Table 1. As a\nbaseline, we train the network from scratch on VOC 2012\ndataset and obtain 44% mAP. Using our unsupervised net-\nwork pre-trained with 1.5M pair of patches and then \ufb01ne-\ntuned on VOC 2012, we obtain mAP of 46.2% (unsup+ft,\nexternal data = 1.5M). However, using more data, 5M\nand 8M patches in pre-training and then \ufb01ne-tune, we can\nachieve 47% and 47.5% mAP. These results indicate that\nour unsupervised network provides a signi\ufb01cant boost as\ncompared to the scratch network. More importantly, when\nmore unlabeled data is applied, we can get better perfor-\nmance ( 3.5% boost compared to training from scratch).\nModel Ensemble. We also try combining different mod-\nels using different sets of unlabeled data in pre-training. By\nensembling two \ufb01ne-tuned networks which are pre-trained\nusing 1.5M and 5M patches, we obtained a boost of 3.5%\ncomparing to the single model, which is 50.5%(unsup+ft\n(2 ensemble)). Finally, we ensemble all three different net-\nworks pre-trained with different sets of data, whose size are\n1.5M, 5M and 8M respectively. We get another boost and\nreach 52% mAP (unsup+ft (3 ensemble)).\nBaselines. We compare our approach with RCNN [14]\nwhich uses ImageNet pre-trained models. Following the\nprocedure in [14], we obtain 50.1% mAP (RCNN 70K) by\nsetting the step size to 20K and \ufb01ne-tuning for 70K itera-\ntions. To generate a model ensemble, the CNNs are \ufb01rst\ntrained on the ImageNet dataset separately, and then they\nare \ufb01ne-tuned with the VOC 2012 dataset. The result of\nensembling two of these networks is 53.6% mAP (RCNN\n70K (2 ensemble)). If we ensemble three networks, we get\na mAP of 54.4%. For fair of comparison, we also \ufb01ne-\ntuned the ImageNet pre-trained model with larger step size\n(50K) and more iterations (200K). The result is 52.3% mAP\n(RCNN 200K (big stepsize)). Note that while ImageNet\nnetwork shows diminishing returns with ensembling since\nthe training data remains similar, in our case since every\nnetwork in the ensemble looks at different sets of data, we\nget huge performance boosts.\nExploring a better way to transfer learned represen-\ntation. Given our \ufb01ne-tuned model using 5M patches in\npre-training (unsup+ft, external = 5M), we use it to re-learn\nand re-adapt to the unsupervised triplet task. After that, the\nnetwork is re-applied to \ufb01ne-tune on VOC 2012. The \ufb01nal\nTable 2. Results on NYU v2 for per-pixel surface normal estimation, eval-\nuated over valid pixels.\n(Lower Better)\n(Higher Better)\nMean\nMedian 11.25\u25e622.5\u25e630\u25e6\nscratch\n38.6\n26.5\n33.1\n46.8 52.5\nunsup + ft\n34.2\n21.9\n35.7\n50.6 57.0\nImageNet + ft\n33.3\n20.8\n36.7\n51.7 58.1\nUNFOLD [13]\n35.1\n19.2\n37.6\n53.3 58.9\nDiscr. [25]\n32.5\n22.4\n27.4\n50.2 60.2\n3DP (MW) [12]\n36.0\n20.5\n35.9\n52.0 57.8\nresult for this single model is 48% mAP (unsup + iterative\nft), which is 1% better than the initial \ufb01ne-tuned network.\nUnsupervised network without \ufb01ne-tuning: We also\nperform object detection without \ufb01ne-tuning on VOC 2012.\nWe extract pool5 features using our unsupervised-CNN and\ntrain SVM on top of it. We obtain mAP of 26.1% using our\nunsupervised network (training with 8M data). The ensem-\nble of two unsupervised-network (training with 5M and 8M\ndata) gets mAP of 28.2%. As a comparison, Imagenet pre-\ntrained network without \ufb01ne-tuning gets mAP of 40.4%.\n6.2.2\nSurface Normal Estimation\nTo illustrate that our unsupervised representation can be\ngeneralized to different tasks, we adapt the unsupervised\nCNN to the task of surface normal estimation from a RGB\nimage.\nIn this task, we want to estimate the orienta-\ntion of the pixels.\nWe perform our experiments on the\nNYUv2 dataset [40], which includes 795 images for train-\ning and 654 images for testing. Each image is has corre-\nsponding depth information which can be used to generate\ngroundtruth surface normals. For evaluation and generating\nthe groundtruth, we adopt the protocols introduced in [12]\nwhich is used by different methods [12, 25, 13] on this task.\nTo apply deep learning to this task, we followed the same\nform of outputs and loss function as the coarse network\nmentioned in [52]. Speci\ufb01cally, we \ufb01rst learn a codebook\nby performing k-means on surface normals and generate 20\ncodewords. Each codeword represents one class and thus\nwe transform the problem to 20-class classi\ufb01cation for each\npixel. Given a 227 \u00d7 227 image as input, our network gen-\nerates surface normals for the whole scene. The output of\nour network is 20 \u00d7 20 pixels, each of which is represented\nby a distribution over 20 codewords. Thus the dimension of\noutput is 20 \u00d7 20 \u00d7 20 = 8000.\nThe network architecture for this task is also based on\nthe AlexNet. To relieve over-\ufb01tting, we only stack two fully\nconnected layers with 4096 and 8000 neurons on the pool5\nlayer. During training, we initialize the network with the\nunsupervised pre-trained network (single network using 8M\nexternal data). We use the same learning rate 1.0 \u00d7 10\u22126\nas [52] and \ufb01ne-tune with 10K iterations given the small\nnumber of training data. Note that unlike [52], we do not\nutilize any data from the videos in NYU dataset for training.\nFigure 8. Surface normal estimation results on NYU dataset. For\nvisualization, we use green for horizontal surface, blue for facing\nright and red for facing left, i.e., blue \u2192X; green \u2192Y; red \u2192Z.\nFor comparison, we also trained networks from scratch\nas well as using ImageNet pre-trained. For evaluation, we\nreport mean and median error (in degrees). We also report\npercentage of pixels with less than 11.25, 22.5 and 30 de-\ngree errors. We show our qualitative results in in Figure 8.\nand quantitative results in Table 2. Our approach (unsup +\nft) is signi\ufb01cantly better than network trained from scratch\nand comes very close to Imagenet-pretrained CNN (\u223c1%).\n7. Discussion and Conclusion\nWe have presented an approach to train CNNs in an un-\nsupervised manner using videos. Speci\ufb01cally, we track mil-\nlions of patches and learn an embedding using CNN that\nkeeps patches from same track closer in the embedding\nspace as compared to any random third patch. Our unsuper-\nvised pre-trained CNN \ufb01ne-tuned using VOC training data\noutperforms CNN trained from scratch by 3.5%. An ensem-\nble version of our approach outperforms scratch by 4.7%\nand comes tantalizingly close to an Imagenet-pretrained\nCNN (within 2.5%). We believe this is an extremely sur-\nprising result since until recently semantic supervision was\nconsidered a strong requirement for training CNNs. We be-\nlieve our successful implementation opens up a new space\nfor designing unsupervised learning algorithms for CNN\ntraining.\nAcknowledgement: This work was partially supported by ONR MURI\nN000141010934 and NSF IIS 1320083. This material was also based on\nresearch partially sponsored by DARPA under agreement number FA8750-\n14-2-0244. The U.S. Government is authorized to reproduce and distribute\nreprints for Governmental purposes notwithstanding any copyright nota-\ntion thereon. The views and conclusions contained herein are those of the\nauthors and should not be interpreted as necessarily representing the of\ufb01-\ncial policies or endorsements, either expressed or implied, of DARPA or\nthe U.S. Government. The authors would like to thank Yahoo! and Nvidia\nfor the compute cluster and GPU donations respectively.\nReferences\n[1] H. Bay, T. Tuytelaars, and L. V. Gool.\nSurf: Speeded up robust\nfeatures. In ECCV, 2006. 3\n[2] Y. Bengio, A. Courville, and P. Vincent. Representation learning: A\nreview and new perspectives. TPAMI, 35(8):1798\u20131828, 2013. 2\n[3] Y. Bengio, P. Lamblin, D. Popovici, and H. Larochelle. Greedy layer-\nwise training of deep networks. In NIPS, 2007. 1, 2\n[4] S. Chopra, R. Hadsell, and Y. LeCun. Learning a similarity met-\nric discriminatively, with application to face veri\ufb01cation. In CVPR,\n2005. 2\n[5] N. Dalal and B. Triggs. Histograms of oriented gradients for human\ndetection. In CVPR, 2005. 1\n[6] C. Doersch, A. Gupta, and A. A. Efros. Mid-level visual element\ndiscovery as discriminative mode seeking. In NIPS, 2013. 2\n[7] C. Doersch, A. Gupta, and A. A. Efros. Context as supervisory sig-\nnal: Discovering objects with predictable context. In ECCV, 2014.\n2\n[8] C. Doersch, A. Gupta, and A. A. Efros. Unsupervised visual repre-\nsentation learning by context prediction. In ICCV, 2015. 2\n[9] S. M. A. Eslami, N. Heess, and J. Winn. The shape boltzmann ma-\nchine: a strong model of object shape. In CVPR, 2012. 2\n[10] M. Everingham, L. V. Gool, C. K. Williams, J. Winn, , and A. Zis-\nserman.\nThe pascal visual object classes (voc) challenge.\nIJCV,\n88(2):303\u2013338, 2010. 6\n[11] P. Foldiak. Learning invariance from transformation sequences. Neu-\nral Computation, 3(2):194\u2013200, 1991. 2\n[12] D. F. Fouhey, A. Gupta, and M. Hebert. Data-driven 3D primitives\nfor single image understanding. In ICCV, 2013. 8\n[13] D. F. Fouhey, A. Gupta, and M. Hebert. Unfolding an indoor origami\nworld. In ECCV, 2014. 8\n[14] R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich feature hier-\narchies for accurate object detection and semantic segmentation. In\nCVPR, 2014. 5, 6, 7\n[15] Y. Gong, Y. Jia, T. K. Leung, A. Toshev, and S. Ioffe. Deep con-\nvolutional ranking for multilabel image annotation. In ICLR, 2007.\n2\n[16] R. Goroshin, J. Bruna, J. Tompson, D. Eigen, and Y. LeCun. Un-\nsupervised learning of spatiotemporally coherent metrics.\nCoRR,\nabs/1412.6056, 2015. 2\n[17] R. Hadsell, S. Chopra, and Y. LeCun. Dimensionality reduction by\nlearning an invariant mapping. In CVPR, 2006. 2\n[18] B. Hariharan, J. Malik, and D. Ramanan. Discriminative decorrela-\ntion for clustering and classi\ufb01cation. In ECCV, 2012. 6\n[19] J. F. Henriques, R. Caseiro, P. Martins, and J. Batista. High-speed\ntracking with kernelized correlation \ufb01lters. TPAMI, 2015. 3\n[20] G. E. Hinton, P. Dayan, B. J. Frey, and R. M. Neal.\nThe\u201d\nwake-sleep\u201d algorithm for unsupervised neural networks. Science,\n268(5214):1158\u20131161, 1995. 2\n[21] G. E. Hinton and R. R. Salakhutdinov. Reducing the dimensionality\nof data with neural networks. Science, 313:504\u2013507, 2006. 2\n[22] E. Hoffer and N. Ailon. Deep metric learning using triplet network.\nCoRR, /abs/1412.6622, 2015. 2\n[23] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick,\nS. Guadarrama, and T. Darrell. Caffe: Convolutional architecture for\nfast feature embedding. CoRR, /abs/1408.5093, 2014. 1\n[24] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classi\ufb01ca-\ntion with deep convolutional neural networks. In NIPS, 2012. 1,\n4\n[25] L. Ladick\u00b4y, B. Zeisl, and M. Pollefeys.\nDiscriminatively trained\ndense surface normal estimation. In ECCV, 2014. 8\n[26] Q. V. Le, M. A. Ranzato, R. Monga, M. Devin, K. Chen, G. S. Cor-\nrado, J. Dean, and A. Y. Ng. Building high-level features using large\nscale unsupervised learning. In ICML, 2012. 1, 2\n[27] Q. V. Le, W. Y. Zou, S. Y. Yeung, and A. Y. Ng. Learning hierar-\nchical invariant spatio-temporal features for action recognition with\nindependent subspace analysis. In CVPR, 2011. 2\n[28] Y. LeCun, B. Boser, J. Denker, D. Henderson, R. E. Howard, W. Hub-\nbard, and L. D. Jackel. Handwritten digit recognition with a back-\npropagation network. In NIPS, 1990. 1\n[29] H. Lee, R. Grosse, R. Ranganath, and A. Y. Ng. Convolutional deep\nbelief networks for scalable unsupervised learning of hierarchical\nrepresentations. In ICML, 2009. 2\n[30] X. Liang, S. Liu, Y. Wei, L. Liu, L. Lin, and S. Yan. Computational\nbaby learning. CoRR, abs/1411.2861, 2014. 2, 5\n[31] S. Liu, X. Liang, L. Liu, X. Shen, J. Yang, C. Xu, X. Cao, and\nS. Yan. Matching-cnn meets knn: Quasi-parametric human parsing.\nIn CVPR, 2015. 2\n[32] D. Lowe.\nDistinctive Image Features from Scale-Invariant Key-\npoints. IJCV, 60(2):91\u2013110, 2004. 1\n[33] P. Luo, X. Wang, and X. Tang. Hierarchical face parsing via deep\nlearning. In CVPR, 2012. 2\n[34] H. Mobahi, R. Collobert, and J. Weston. Deep learning from tempo-\nral coherence in video. In ICML, 2009. 1, 2\n[35] B. A. Olshausen and D. J. Field. Sparse coding with an overcomplete\nbasis set: A strategy employed by v1? Vision research, 1997. 1, 2\n[36] A. Quattoni and A.Torralba. Recognizing indoor scenes. In CVPR,\n2009. 6\n[37] M. A. Ranzato, F. J. Huang, Y.-L. Boureau, and Y. LeCun. Unsu-\npervised learning of invariant feature hierarchies with applications to\nobject recognition. In CVPR, 2007. 1\n[38] B. C. Russell, A. A. Efros, J. Sivic, W. T. Freeman, and A. Zisserman.\nUsing multiple segmentations to discover objects and their extent in\nimage collections. In CVPR, 2006. 2\n[39] P. Sermanet, K. Kavukcuoglu, S. Chintala, and Y. LeCun. Pedestrian\ndetection with unsupervised multi-stage feature learning. In CVPR,\n2013. 2\n[40] N. Silberman, D. Hoiem, P. Kohli, and R. Fergus. Indoor segmen-\ntation and support inference from RGBD images. In ECCV, 2012.\n8\n[41] S. Singh, A. Gupta, and A. A. Efros. Unsupervised discovery of\nmid-level discriminative patches. In ECCV, 2012. 2\n[42] J. Sivic, B. C. Russell, A. A. Efros, A. Zisserman, and W. T. Freeman.\nDiscovering objects and their location in images. In ICCV, 2005. 2\n[43] N. Srivastava, E. Mansimov, and R. Salakhutdinov.\nUnsuper-\nvised learning of video representations using lstms.\nCoRR,\nabs/1502.04681, 2015. 2\n[44] N. Srivastava and R. R. Salakhutdinov. Multimodal learning with\ndeep boltzmann machines. In NIPS, 2012. 1, 2\n[45] D. Stavens and S. Thrun. Unsupervised learning of invariant features\nusing video. In CVPR, 2010. 2\n[46] E. B. Sudderth, A. Torralba, W. T. Freeman, and A. S. Willsky.\nDescribing visual scenes using transformed dirichlet processes. In\nNIPS, 2005. 2\n[47] Y. Tang, R. Salakhutdinov, and G. Hinton. Robust boltzmann ma-\nchines for recognition and denoising. In CVPR, 2012. 2\n[48] G. W. Taylor, R. Fergus, Y. LeCun, and C. Bregler. Convolutional\nlearning of spatio-temporal features. In ECCV, 2010. 1, 2\n[49] P. Vincent, H. Larochelle, Y. Bengio, and P. Manzagol.\nExtract-\ning and composing robust features with denoising autoencoders. In\nICML, 2008. 1, 2\n[50] H. Wang and C. Schmid. Action recognition with improved trajecto-\nries. In ICCV, 2013. 3\n[51] J. Wang, Y. Song, T. Leung, C. Rosenberg, J. Wang, J. Philbin,\nB. Chen, and Y. Wu. Learning \ufb01ne-grained image similarity with\ndeep ranking. In CVPR, 2014. 2, 3\n[52] X. Wang, D. F. Fouhey, and A. Gupta. Designing deep networks for\nsurface normal estimation. In CVPR, 2015. 8\n[53] L. Wiskott and T. J. Sejnowski. Slow feature analysis:unsupervised\nlearning of invariances. Neural Computation, 14:715\u2013770, 2002. 2\n[54] P. Wohlhart and V. Lepetit. Learning descriptors for object recogni-\ntion and 3d pose estimation. In CVPR, 2015. 2\n[55] R. Zhang, L. Lin, R. Zhang, W. Zuo, and L. Zhang. Bit-scalable deep\nhashing with regularized similarity learning for image retrieval and\nperson re-identi\ufb01cation. TIP, 24(12):4766\u20134779, 2015. 2\n[56] W. Y. Zou, S. Zhu, A. Y. Ng, and K. Yu. Deep learning of invariant\nfeatures via simulated \ufb01xations in video. In NIPS, 2012. 1, 2\n",
        "sentence": "",
        "context": "research partially sponsored by DARPA under agreement number FA8750-\n14-2-0244. The U.S. Government is authorized to reproduce and distribute\nreprints for Governmental purposes notwithstanding any copyright nota-\ntion thereon. The views and conclusions contained herein are those of the\nauthors and should not be interpreted as necessarily representing the of\ufb01-\ncial policies or endorsements, either expressed or implied, of DARPA or\ndirectly from pixels themselves using the deep learning\nand Convolutional Neural Networks (CNNs) [28, 24, 23].\nAt the heart of CNNs is a completely supervised learning\nparadigm. Often millions of examples are \ufb01rst labeled us-"
    },
    {
        "title": "Unsupervised deep embedding for clustering analysis",
        "author": [
            "J. Xie",
            "R. Girshick",
            "A. Farhadi"
        ],
        "venue": "In ICML,",
        "citeRegEx": "Xie et al\\.,? \\Q2016\\E",
        "shortCiteRegEx": "Xie et al\\.",
        "year": 2016,
        "abstract": "Clustering is central to many data-driven application domains and has been\nstudied extensively in terms of distance functions and grouping algorithms.\nRelatively little work has focused on learning representations for clustering.\nIn this paper, we propose Deep Embedded Clustering (DEC), a method that\nsimultaneously learns feature representations and cluster assignments using\ndeep neural networks. DEC learns a mapping from the data space to a\nlower-dimensional feature space in which it iteratively optimizes a clustering\nobjective. Our experimental evaluations on image and text corpora show\nsignificant improvement over state-of-the-art methods.",
        "full_text": "Unsupervised Deep Embedding for Clustering Analysis\nJunyuan Xie\nJXIE@CS.WASHINGTON.EDU\nUniversity of Washington\nRoss Girshick\nRBG@FB.COM\nFacebook AI Research (FAIR)\nAli Farhadi\nALI@CS.WASHINGTON.EDU\nUniversity of Washington\nAbstract\nClustering is central to many data-driven appli-\ncation domains and has been studied extensively\nin terms of distance functions and grouping al-\ngorithms. Relatively little work has focused on\nlearning representations for clustering.\nIn this\npaper, we propose Deep Embedded Clustering\n(DEC), a method that simultaneously learns fea-\nture representations and cluster assignments us-\ning deep neural networks. DEC learns a map-\nping from the data space to a lower-dimensional\nfeature space in which it iteratively optimizes a\nclustering objective. Our experimental evalua-\ntions on image and text corpora show signi\ufb01cant\nimprovement over state-of-the-art methods.\n1. Introduction\nClustering, an essential data analysis and visualization\ntool, has been studied extensively in unsupervised machine\nlearning from different perspectives: What de\ufb01nes a clus-\nter? What is the right distance metric? How to ef\ufb01ciently\ngroup instances into clusters? How to validate clusters?\nAnd so on. Numerous different distance functions and em-\nbedding methods have been explored in the literature. Rel-\natively little work has focused on the unsupervised learning\nof the feature space in which to perform clustering.\nA notion of distance or dissimilarity is central to data clus-\ntering algorithms. Distance, in turn, relies on represent-\ning the data in a feature space.\nThe k-means cluster-\ning algorithm (MacQueen et al., 1967), for example, uses\nthe Euclidean distance between points in a given feature\nspace, which for images might be raw pixels or gradient-\nProceedings of the 33 rd International Conference on Machine\nLearning, New York, NY, USA, 2016. JMLR: W&CP volume\n48. Copyright 2016 by the author(s).\norientation histograms. The choice of feature space is cus-\ntomarily left as an application-speci\ufb01c detail for the end-\nuser to determine. Yet it is clear that the choice of feature\nspace is crucial; for all but the simplest image datasets,\nclustering with Euclidean distance on raw pixels is com-\npletely ineffective. In this paper, we revisit cluster analysis\nand ask: Can we use a data driven approach to solve for\nthe feature space and cluster memberships jointly?\nWe take inspiration from recent work on deep learning for\ncomputer vision (Krizhevsky et al., 2012; Girshick et al.,\n2014; Zeiler & Fergus, 2014; Long et al., 2014), where\nclear gains on benchmark tasks have resulted from learn-\ning better features. These improvements, however, were\nobtained with supervised learning, whereas our goal is un-\nsupervised data clustering. To this end, we de\ufb01ne a pa-\nrameterized non-linear mapping from the data space X to\na lower-dimensional feature space Z, where we optimize\na clustering objective. Unlike previous work, which oper-\nates on the data space or a shallow linear embedded space,\nwe use stochastic gradient descent (SGD) via backpropaga-\ntion on a clustering objective to learn the mapping, which\nis parameterized by a deep neural network. We refer to\nthis clustering algorithm as Deep Embedded Clustering, or\nDEC.\nOptimizing DEC is challenging. We want to simultane-\nously solve for cluster assignment and the underlying fea-\nture representation. However, unlike in supervised learn-\ning, we cannot train our deep network with labeled data.\nInstead we propose to iteratively re\ufb01ne clusters with an\nauxiliary target distribution derived from the current soft\ncluster assignment. This process gradually improves the\nclustering as well as the feature representation.\nOur experiments show signi\ufb01cant improvements over state-\nof-the-art clustering methods in terms of both accuracy and\nrunning time on image and textual datasets. We evaluate\nDEC on MNIST (LeCun et al., 1998), STL (Coates et al.,\narXiv:1511.06335v2  [cs.LG]  24 May 2016\nUnsupervised Deep Embedding for Clustering Analysis\n2011), and REUTERS (Lewis et al., 2004), comparing it\nwith standard and state-of-the-art clustering methods (Nie\net al., 2011; Yang et al., 2010). In addition, our experiments\nshow that DEC is signi\ufb01cantly less sensitive to the choice\nof hyperparameters compared to state-of-the-art methods.\nThis robustness is an important property of our clustering\nalgorithm since, when applied to real data, supervision is\nnot available for hyperparameter cross-validation.\nOur contributions are: (a) joint optimization of deep em-\nbedding and clustering; (b) a novel iterative re\ufb01nement\nvia soft assignment; and (c) state-of-the-art clustering re-\nsults in terms of clustering accuracy and speed. Our Caffe-\nbased (Jia et al., 2014) implementation of DEC is available\nat https://github.com/piiswrong/dec.\n2. Related work\nClustering has been extensively studied in machine learn-\ning in terms of feature selection (Boutsidis et al., 2009; Liu\n& Yu, 2005; Alelyani et al., 2013), distance functions (Xing\net al., 2002; Xiang et al., 2008), grouping methods (Mac-\nQueen et al., 1967; Von Luxburg, 2007; Li et al., 2004),\nand cluster validation (Halkidi et al., 2001). Space does\nnot allow for a comprehensive literature study and we refer\nreaders to (Aggarwal & Reddy, 2013) for a survey.\nOne branch of popular methods for clustering is k-\nmeans (MacQueen et al., 1967) and Gaussian Mixture\nModels (GMM) (Bishop, 2006). These methods are fast\nand applicable to a wide range of problems. However, their\ndistance metrics are limited to the original data space and\nthey tend to be ineffective when input dimensionality is\nhigh (Steinbach et al., 2004).\nSeveral variants of k-means have been proposed to address\nissues with higher-dimensional input spaces. De la Torre &\nKanade (2006); Ye et al. (2008) perform joint dimension-\nality reduction and clustering by \ufb01rst clustering the data\nwith k-means and then projecting the data into a lower di-\nmensions where the inter-cluster variance is maximized.\nThis process is repeated in EM-style iterations until conver-\ngence. However, this framework is limited to linear embed-\nding; our method employs deep neural networks to perform\nnon-linear embedding that is necessary for more complex\ndata.\nSpectral clustering and its variants have gained popular-\nity recently (Von Luxburg, 2007). They allow more \ufb02ex-\nible distance metrics and generally perform better than k-\nmeans. Combining spectral clustering and embedding has\nbeen explored in Yang et al. (2010); Nie et al. (2011). Tian\net al. (2014) proposes an algorithm based on spectral clus-\ntering, but replaces eigenvalue decomposition with deep\nautoencoder, which improves performance but further in-\ncreases memory consumption.\nMost spectral clustering algorithms need to compute the\nfull graph Laplacian matrix and therefore have quadratic\nor super quadratic complexities in the number of data\npoints. This means they need specialized machines with\nlarge memory for any dataset larger than a few tens of\nthousands of points. In order to scale spectral clustering\nto large datasets, approximate algorithms were invented to\ntrade off performance for speed (Yan et al., 2009). Our\nmethod, however, is linear in the number of data points and\nscales gracefully to large datasets.\nMinimizing the Kullback-Leibler (KL) divergence be-\ntween a data distribution and an embedded distribution has\nbeen used for data visualization and dimensionality reduc-\ntion (van der Maaten & Hinton, 2008). T-SNE, for instance,\nis a non-parametric algorithm in this school and a paramet-\nric variant of t-SNE (van der Maaten, 2009) uses deep neu-\nral network to parametrize the embedding. The complexity\nof t-SNE is O(n2), where n is the number of data points,\nbut it can be approximated in O(n log n) (van Der Maaten,\n2014).\nWe take inspiration from parametric t-SNE. Instead of min-\nimizing KL divergence to produce an embedding that is\nfaithful to distances in the original data space, we de\ufb01ne\na centroid-based probability distribution and minimize its\nKL divergence to an auxiliary target distribution to simul-\ntaneously improve clustering assignment and feature repre-\nsentation. A centroid-based method also has the bene\ufb01t of\nreducing complexity to O(nk), where k is the number of\ncentroids.\n3. Deep embedded clustering\nConsider the problem of clustering a set of n points {xi \u2208\nX}n\ni=1 into k clusters, each represented by a centroid\n\u00b5j, j = 1, . . . , k. Instead of clustering directly in the data\nspace X, we propose to \ufb01rst transform the data with a non-\nlinear mapping f\u03b8 : X \u2192Z, where \u03b8 are learnable pa-\nrameters and Z is the latent feature space. The dimen-\nsionality of Z is typically much smaller than X in order\nto avoid the \u201ccurse of dimensionality\u201d (Bellman, 1961). To\nparametrize f\u03b8, deep neural networks (DNNs) are a natu-\nral choice due to their theoretical function approximation\nproperties (Hornik, 1991) and their demonstrated feature\nlearning capabilities (Bengio et al., 2013).\nThe proposed algorithm (DEC) clusters data by simultane-\nously learning a set of k cluster centers {\u00b5j \u2208Z}k\nj=1 in the\nfeature space Z and the parameters \u03b8 of the DNN that maps\ndata points into Z. DEC has two phases: (1) parameter ini-\ntialization with a deep autoencoder (Vincent et al., 2010)\nand (2) parameter optimization (i.e., clustering), where we\niterate between computing an auxiliary target distribution\nand minimizing the Kullback\u2013Leibler (KL) divergence to\nUnsupervised Deep Embedding for Clustering Analysis\nit. We start by describing phase (2) parameter optimiza-\ntion/clustering, given an initial estimate of \u03b8 and {\u00b5j}k\nj=1.\n3.1. Clustering with KL divergence\nGiven an initial estimate of the non-linear mapping f\u03b8 and\nthe initial cluster centroids {\u00b5j}k\nj=1, we propose to im-\nprove the clustering using an unsupervised algorithm that\nalternates between two steps. In the \ufb01rst step, we com-\npute a soft assignment between the embedded points and\nthe cluster centroids. In the second step, we update the\ndeep mapping f\u03b8 and re\ufb01ne the cluster centroids by learn-\ning from current high con\ufb01dence assignments using an aux-\niliary target distribution. This process is repeated until a\nconvergence criterion is met.\n3.1.1. SOFT ASSIGNMENT\nFollowing van der Maaten & Hinton (2008) we use the Stu-\ndent\u2019s t-distribution as a kernel to measure the similarity\nbetween embedded point zi and centroid \u00b5j:\nqij =\n(1 + \u2225zi \u2212\u00b5j\u22252/\u03b1)\u2212\u03b1+1\n2\nP\nj\u2032(1 + \u2225zi \u2212\u00b5j\u2032\u22252/\u03b1)\u2212\u03b1+1\n2 ,\n(1)\nwhere zi = f\u03b8(xi) \u2208Z corresponds to xi \u2208X after em-\nbedding, \u03b1 are the degrees of freedom of the Student\u2019s t-\ndistribution and qij can be interpreted as the probability\nof assigning sample i to cluster j (i.e., a soft assignment).\nSince we cannot cross-validate \u03b1 on a validation set in the\nunsupervised setting, and learning it is super\ufb02uous (van der\nMaaten, 2009), we let \u03b1 = 1 for all experiments.\n3.1.2. KL DIVERGENCE MINIMIZATION\nWe propose to iteratively re\ufb01ne the clusters by learning\nfrom their high con\ufb01dence assignments with the help of\nan auxiliary target distribution. Speci\ufb01cally, our model is\ntrained by matching the soft assignment to the target distri-\nbution. To this end, we de\ufb01ne our objective as a KL diver-\ngence loss between the soft assignments qi and the auxil-\niary distribution pi as follows:\nL = KL(P\u2225Q) =\nX\ni\nX\nj\npij log pij\nqij\n.\n(2)\nThe choice of target distributions P is crucial for DEC\u2019s\nperformance. A naive approach would be setting each pi to\na delta distribution (to the nearest centroid) for data points\nabove a con\ufb01dence threshold and ignore the rest. How-\never, because qi are soft assignments, it is more natural\nand \ufb02exible to use softer probabilistic targets. Speci\ufb01cally,\nwe would like our target distribution to have the following\nproperties: (1) strengthen predictions (i.e., improve clus-\nter purity), (2) put more emphasis on data points assigned\nwith high con\ufb01dence, and (3) normalize loss contribution\nof each centroid to prevent large clusters from distorting\nthe hidden feature space.\nIn our experiments, we compute pi by \ufb01rst raising qi to\nthe second power and then normalizing by frequency per\ncluster:\npij =\nq2\nij/fj\nP\nj\u2032 q2\nij\u2032/fj\u2032 ,\n(3)\nwhere fj = P\ni qij are soft cluster frequencies. Please\nrefer to section 5.1 for discussions on empirical properties\nof L and P.\nOur training strategy can be seen as a form of self-\ntraining (Nigam & Ghani, 2000). As in self-training, we\ntake an initial classi\ufb01er and an unlabeled dataset, then la-\nbel the dataset with the classi\ufb01er in order to train on its\nown high con\ufb01dence predictions. Indeed, in experiments\nwe observe that DEC improves upon the initial estimate\nin each iteration by learning from high con\ufb01dence predic-\ntions, which in turn helps to improve low con\ufb01dence ones.\n3.1.3. OPTIMIZATION\nWe jointly optimize the cluster centers {\u00b5j} and DNN pa-\nrameters \u03b8 using Stochastic Gradient Descent (SGD) with\nmomentum. The gradients of L with respect to feature-\nspace embedding of each data point zi and each cluster\ncentroid \u00b5j are computed as:\n\u2202L\n\u2202zi\n=\n\u03b1 + 1\n\u03b1\nX\nj\n(1 + \u2225zi \u2212\u00b5j\u22252\n\u03b1\n)\u22121\n(4)\n\u00d7(pij \u2212qij)(zi \u2212\u00b5j),\n\u2202L\n\u2202\u00b5j\n=\n\u2212\u03b1 + 1\n\u03b1\nX\ni\n(1 + \u2225zi \u2212\u00b5j\u22252\n\u03b1\n)\u22121\n(5)\n\u00d7(pij \u2212qij)(zi \u2212\u00b5j).\nThe gradients \u2202L/\u2202zi are then passed down to the DNN\nand used in standard backpropagation to compute the\nDNN\u2019s parameter gradient \u2202L/\u2202\u03b8. For the purpose of dis-\ncovering cluster assignments, we stop our procedure when\nless than tol% of points change cluster assignment between\ntwo consecutive iterations.\n3.2. Parameter initialization\nThus far we have discussed how DEC proceeds given ini-\ntial estimates of the DNN parameters \u03b8 and the cluster cen-\ntroids {\u00b5j}. Now we discuss how the parameters and cen-\ntroids are initialized.\nWe initialize DEC with a stacked autoencoder (SAE) be-\ncause recent research has shown that they consistently pro-\nduce semantically meaningful and well-separated represen-\ntations on real-world datasets (Vincent et al., 2010; Hin-\nton & Salakhutdinov, 2006; Le, 2013). Thus the unsuper-\nUnsupervised Deep Embedding for Clustering Analysis\nTable 1. Dataset statistics.\nDataset\n# Points\n# classes\nDimension\n% of largest class\nMNIST (LeCun et al., 1998)\n70000\n10\n784\n11%\nSTL-10 (Coates et al., 2011)\n13000\n10\n1428\n10%\nREUTERS-10K\n10000\n4\n2000\n43%\nREUTERS (Lewis et al., 2004)\n685071\n4\n2000\n43%\nP\nQ\nL = KL(P||Q)\nencoder\ndecoder\nDEC\ninput\nreconstruction\nfeature\nFigure 1. Network structure\nvised representation learned by SAE naturally facilitates\nthe learning of clustering representations with DEC.\nWe initialize the SAE network layer by layer with each\nlayer being a denoising autoencoder trained to reconstruct\nthe previous layer\u2019s output after random corruption (Vin-\ncent et al., 2010). A denoising autoencoder is a two layer\nneural network de\ufb01ned as:\n\u02dcx \u223cDropout(x)\n(6)\nh = g1(W1\u02dcx + b1)\n(7)\n\u02dch \u223cDropout(h)\n(8)\ny = g2(W2\u02dch + b2)\n(9)\nwhere Dropout(\u00b7) (Srivastava et al., 2014) is a stochastic\nmapping that randomly sets a portion of its input dimen-\nsions to 0, g1 and g2 are activation functions for encoding\nand decoding layer respectively, and \u03b8 = {W1, b1, W2, b2}\nare model parameters. Training is performed by minimiz-\ning the least-squares loss \u2225x \u2212y\u22252\n2. After training of one\nlayer, we use its output h as the input to train the next\nlayer. We use recti\ufb01ed linear units (ReLUs) (Nair & Hin-\nton, 2010) in all encoder/decoder pairs, except for g2 of the\n\ufb01rst pair (it needs to reconstruct input data that may have\npositive and negative values, such as zero-mean images)\nand g1 of the last pair (so the \ufb01nal data embedding retains\nfull information (Vincent et al., 2010)).\nAfter greedy layer-wise training, we concatenate all en-\ncoder layers followed by all decoder layers, in reverse\nlayer-wise training order, to form a deep autoencoder and\nthen \ufb01netune it to minimize reconstruction loss. The \ufb01nal\nresult is a multilayer deep autoencoder with a bottleneck\ncoding layer in the middle. We then discard the decoder\nlayers and use the encoder layers as our initial mapping be-\ntween the data space and the feature space, as shown in\nFig. 1.\nTo initialize the cluster centers, we pass the data through\nthe initialized DNN to get embedded data points and then\nperform standard k-means clustering in the feature space Z\nto obtain k initial centroids {\u00b5j}k\nj=1.\n4. Experiments\n4.1. Datasets\nWe evaluate the proposed method (DEC) on one text\ndataset and two image datasets and compare it against other\nalgorithms including k-means, LDGMI (Yang et al., 2010),\nand SEC (Nie et al., 2011). LDGMI and SEC are spec-\ntral clustering based algorithms that use a Laplacian matrix\nand various transformations to improve clustering perfor-\nmance. Empirical evidence reported in Yang et al. (2010);\nNie et al. (2011) shows that LDMGI and SEC outperform\ntraditional spectral clustering methods on a wide range of\ndatasets. We show qualitative and quantitative results that\ndemonstrate the bene\ufb01t of DEC compared to LDGMI and\nSEC.\nIn order to study the performance and generality of dif-\nferent algorithms, we perform experiment on two image\ndatasets and one text data set:\n\u2022 MNIST: The MNIST dataset consists of 70000 hand-\nwritten digits of 28-by-28 pixel size. The digits are\ncentered and size-normalized (LeCun et al., 1998).\n\u2022 STL-10: A dataset of 96-by-96 color images. There\nare 10 classes with 1300 examples each. It also con-\ntains 100000 unlabeled images of the same resolu-\ntion (Coates et al., 2011). We also used the unlabeled\nset when training our autoencoders. Similar to Doer-\nsch et al. (2012), we concatenated HOG feature and a\n8-by-8 color map to use as input to all algorithms.\n\u2022 REUTERS: Reuters contains about 810000 English\nnews stories labeled with a category tree (Lewis et al.,\n2004).\nWe used the four root categories:\ncorpo-\nrate/industrial, government/social, markets, and eco-\nnomics as labels and further pruned all documents that\nare labeled by multiple root categories to get 685071\nUnsupervised Deep Embedding for Clustering Analysis\n2\n4\n6\n8\n0.5\n0.55\n0.6\n0.65\n0.7\n0.75\n0.8\n0.85\n0.9\nAccuracy\nParameter Index\nMNIST\n \n \nDEC\nDEC w/o backprop\nKmeans\nLDGMI\nSEC\n2\n4\n6\n8\n0.2\n0.25\n0.3\n0.35\n0.4\nAccuracy\nParameter Index\nSTL\n2\n4\n6\n8\n0.35\n0.4\n0.45\n0.5\n0.55\n0.6\n0.65\n0.7\n0.75\nAccuracy\nParameter Index\nREUTERS\u221210K\n2\n4\n6\n8\n0.5\n0.55\n0.6\n0.65\n0.7\n0.75\n0.8\n0.85\nAccuracy\nParameter Index\nREUTERS\nFigure 2. Clustering accuracy for different hyperparameter choices for each algorithm. DEC outperforms other methods and is more\nrobust to hyperparameter changes compared to either LDGMI or SEC. Robustness is important because cross-validation is not possible\nin real-world applications of cluster analysis. This \ufb01gure is best viewed in color.\nTable 2. Comparison of clustering accuracy (Eq. 10) on four datasets.\nMethod\nMNIST\nSTL-HOG\nREUTERS-10k\nREUTERS\nk-means\n53.49%\n28.39%\n52.42%\n53.29%\nLDMGI\n84.09%\n33.08%\n43.84%\nN/A\nSEC\n80.37%\n30.75%\n60.08%\nN/A\nDEC w/o backprop\n79.82%\n34.06%\n70.05%\n69.62%\nDEC (ours)\n84.30%\n35.90%\n72.17%\n75.63%\narticles. We then computed tf-idf features on the 2000\nmost frequently occurring word stems. Since some\nalgorithms do not scale to the full Reuters dataset,\nwe also sampled a random subset of 10000 examples,\nwhich we call REUTERS-10k, for comparison pur-\nposes.\nA summary of dataset statistics is shown in Table 1. For\nall algorithms, we normalize all datasets so that 1\nd\u2225xi\u22252\n2 is\napproximately 1, where d is the dimensionality of the data\nspace point xi \u2208X.\n4.2. Evaluation Metric\nWe use the standard unsupervised evaluation metric and\nprotocols for evaluations and comparisons to other algo-\nrithms (Yang et al., 2010). For all algorithms we set the\nnumber of clusters to the number of ground-truth categories\nand evaluate performance with unsupervised clustering ac-\ncuracy (ACC):\nACC = max\nm\nPn\ni=1 1{li = m(ci)}\nn\n,\n(10)\nwhere li is the ground-truth label, ci is the cluster assign-\nment produced by the algorithm, and m ranges over all pos-\nsible one-to-one mappings between clusters and labels.\nIntuitively this metric takes a cluster assignment from an\nunsupervised algorithm and a ground truth assignment and\nthen \ufb01nds the best matching between them. The best map-\nping can be ef\ufb01ciently computed by the Hungarian algo-\nrithm (Kuhn, 1955).\n4.3. Implementation\nDetermining hyperparameters by cross-validation on a vali-\ndation set is not an option in unsupervised clustering. Thus\nwe use commonly used parameters for DNNs and avoid\ndataset speci\ufb01c tuning as much as possible. Speci\ufb01cally,\ninspired by van der Maaten (2009), we set network dimen-\nsions to d\u2013500\u2013500\u20132000\u201310 for all datasets, where d is\nthe data-space dimension, which varies between datasets.\nAll layers are densely (fully) connected.\nDuring greedy layer-wise pretraining we initialize the\nweights to random numbers drawn from a zero-mean Gaus-\nsian distribution with a standard deviation of 0.01. Each\nlayer is pretrained for 50000 iterations with a dropout rate\nof 20%. The entire deep autoencoder is further \ufb01netuned\nfor 100000 iterations without dropout. For both layer-wise\npretraining and end-to-end \ufb01netuning of the autoencoder\nthe minibatch size is set to 256, starting learning rate is\nset to 0.1, which is divided by 10 every 20000 iterations,\nand weight decay is set to 0.\nAll of the above param-\neters are set to achieve a reasonably good reconstruction\nloss and are held constant across all datasets.\nDataset-\nspeci\ufb01c settings of these parameters might improve perfor-\nmance on each dataset, but we refrain from this type of\nunrealistic parameter tuning. To initialize centroids, we\nrun k-means with 20 restarts and select the best solution.\nUnsupervised Deep Embedding for Clustering Analysis\n(a) MNIST\n(b) STL-10\nFigure 3. Each row contains the top 10 scoring elements from one cluster.\nIn the KL divergence minimization phase, we train with\na constant learning rate of 0.01. The convergence thresh-\nold is set to tol = 0.1%. Our implementation is based\non Python and Caffe (Jia et al., 2014) and is available at\nhttps://github.com/piiswrong/dec.\nFor all baseline algorithms, we perform 20 random restarts\nwhen initializing centroids and pick the result with the\nbest objective value.\nFor a fair comparison with previ-\nous work (Yang et al., 2010), we vary one hyperparameter\nfor each algorithm over 9 possible choices and report the\nbest accuracy in Table 2 and the range of accuracies in Fig.\n2. For LDGMI and SEC, we use the same parameter and\nrange as in their corresponding papers. For our proposed\nalgorithm, we vary \u03bb, the parameter that controls annealing\nspeed, over 2i \u00d7 10, i = 0, 1, ..., 8. Since k-means does not\nhave tunable hyperparameters (aside from k), we simply\nrun them 9 times. GMMs perform similarly to k-means so\nwe only report k-means results. Traditional spectral clus-\ntering performs worse than LDGMI and SEC so we only\nreport the latter (Yang et al., 2010; Nie et al., 2011).\n4.4. Experiment results\nWe evaluate the performance of our algorithm both quan-\ntitatively and qualitatively. In Table 2, we report the best\nperformance, over 9 hyperparameter settings, of each al-\ngorithm. Note that DEC outperforms all other methods,\nsometimes with a signi\ufb01cant margin. To demonstrate the\neffectiveness of end-to-end training, we also show the re-\nsults from freezing the non-linear mapping f\u03b8 during clus-\ntering. We \ufb01nd that this ablation (\u201cDEC w/o backprop\u201d)\ngenerally performs worse than DEC.\nIn order to investigate the effect of hyperparameters, we\nplot the accuracy of each method under all 9 settings (Fig.\n2). We observe that DEC is more consistent across hyper-\nparameter ranges compared to LDGMI and SEC. For DEC,\nhyperparameter \u03bb = 40 gives near optimal performance on\nall dataset, whereas for other algorithms the optimal hyper-\nparameter varies widely. Moreover, DEC can process the\nentire REUTERS dataset in half an hour with GPU acceler-\nation while the second best algorithms, LDGMI and SEC,\nwould need months of computation time and terabytes of\nmemory. We, indeed, could not run these methods on the\nfull REUTERS dataset and report N/A in Table 2 (GPU\nadaptation of these methods is non-trivial).\nIn Fig. 3 we show 10 top scoring images from each clus-\nter in MNIST and STL. Each row corresponds to a cluster\nand images are sorted from left to right based on their dis-\ntance to the cluster center. We observe that for MNIST,\nDEC\u2019s cluster assignment corresponds to natural clusters\nvery well, with the exception of confusing 4 and 9, while\nfor STL, DEC is mostly correct with airplanes, trucks and\ncars, but spends part of its attention on poses instead of\ncategories when it comes to animal classes.\n5. Discussion\n5.1. Assumptions and Objective\nThe underlying assumption of DEC is that the initial clas-\nsi\ufb01er\u2019s high con\ufb01dence predictions are mostly correct. To\nverify that this assumption holds for our task and that our\nchoice of P has the desired properties, we plot the mag-\nnitude of the gradient of L with respect to each embedded\npoint, |\u2202L/\u2202zi|, against its soft assignment, qij, to a ran-\nUnsupervised Deep Embedding for Clustering Analysis\n(a) Epoch 0\n(b) Epoch 3\n(c) Epoch 6\n(d) Epoch 9\n(e) Epoch 12\n0\n10\n20\n30\n0.79\n0.8\n0.81\n0.82\n0.83\n0.84\nAccuracy\nEpochs\nk-means initialization\n(f) Accuracy vs. epochs\nFigure 5. We visualize the latent representation as the KL divergence minimization phase proceeds on MNIST. Note the separation of\nclusters from epoch 0 to epoch 12. We also plot the accuracy of DEC at different epochs, showing that KL divergence minimization\nimproves clustering accuracy. This \ufb01gure is best viewed in color.\nTable 3. Comparison of clustering accuracy (Eq. 10) on autoencoder (AE) feature.\nMethod\nMNIST\nSTL-HOG\nREUTERS-10k\nREUTERS\nAE+k-means\n81.84%\n33.92%\n66.59%\n71.97%\nAE+LDMGI\n83.98%\n32.04%\n42.92%\nN/A\nAE+SEC\n81.56%\n32.29%\n61.86%\nN/A\nDEC (ours)\n84.30%\n35.90%\n72.17%\n75.63%\nFigure 4. Gradient visualization at the start of KL divergence min-\nimization. This plot shows the magnitude of the gradient of the\nloss L vs. the cluster soft assignment probability qij. See text for\ndiscussion.\ndomly chosen MNIST cluster j (Fig. 4).\nWe observe points that are closer to the cluster center (large\nqij) contribute more to the gradient. We also show the raw\nimages of 10 data points at each 10 percentile sorted by qij.\nInstances with higher similarity are more canonical exam-\nples of \u201c5\u201d. As con\ufb01dence decreases, instances become\nmore ambiguous and eventually turn into a mislabeled \u201c8\u201d\nsuggesting the soundness of our assumptions.\n5.2. Contribution of Iterative Optimization\nIn Fig. 5 we visualize the progression of the embedded rep-\nresentation of a random subset of MNIST during training.\nFor visualization we use t-SNE (van der Maaten & Hinton,\n2008) applied to the embedded points zi. It is clear that\nthe clusters are becoming increasingly well separated. Fig.\n5 (f) shows how accuracy correspondingly improves over\nSGD epochs.\n5.3. Contribution of Autoencoder Initialization\nTo better understand the contribution of each component,\nwe show the performance of all algorithms with autoen-\ncoder features in Table 3. We observe that SEC and LD-\nMGI\u2019s performance do not change signi\ufb01cantly with au-\nUnsupervised Deep Embedding for Clustering Analysis\nTable 4. Clustering accuracy (Eq. 10) on imbalanced subsample of MNIST.\nXXXXXXXXXX\nMethod\nrmin\n0.1\n0.3\n0.5\n0.7\n0.9\nk-means\n47.14%\n49.93%\n53.65%\n54.16%\n54.39%\nAE+k-means\n66.82%\n74.91%\n77.93%\n80.04%\n81.31%\nDEC\n70.10%\n80.92%\n82.68%\n84.69%\n85.41%\ntoencoder feature, while k-means improved but is still be-\nlow DEC. This demonstrates the power of deep embedding\nand the bene\ufb01t of \ufb01ne-tuning with the proposed KL diver-\ngence objective.\n5.4. Performance on Imbalanced Data\nIn order to study the effect of imbalanced data, we sample\nsubsets of MNIST with various retention rates. For mini-\nmum retention rate rmin, data points of class 0 will be kept\nwith probability rmin and class 9 with probability 1, with\nthe other classes linearly in between. As a result the largest\ncluster will be 1/rmin times as large as the smallest one.\nFrom Table 4 we can see that DEC is fairly robust against\ncluster size variation. We also observe that KL divergence\nminimization (DEC) consistently improves clustering ac-\ncuracy after autoencoder and k-means initialization (shown\nas AE+k-means).\n5.5. Number of Clusters\n0\n0.2\n0.4\n0.6\n0.8\n1\n3\n5\n7\n9\n11\n13\n15\n17\n19\nNumber\t\r \u00a0of\t\r \u00a0Clusters\nNMI\nGeneralizability\nFigure 6. Selection of the centroid count, k. This is a plot of Nor-\nmalized Mutual Information (NMI) and Generalizability vs. num-\nber of clusters. Note that there is a sharp drop of generalizability\nfrom 9 to 10 which means that 9 is the optimal number of clusters.\nIndeed, we observe that 9 gives the highest NMI.\nSo far we have assumed that the number of natural clusters\nis given to simplify comparison between algorithms. How-\never, in practice this quantity is often unknown. Therefore\na method for determining the optimal number of clusters is\nneeded. To this end, we de\ufb01ne two metrics: (1) the standard\nmetric, Normalized Mutual Information (NMI), for evalu-\nating clustering results with different cluster number:\nNMI (l, c) =\nI(l, c)\n1\n2[H(l) + H(c)],\nwhere I is the mutual information metric and H is entropy,\nand (2) generalizability (G) which is de\ufb01ned as the ratio\nbetween training and validation loss:\nG =\nLtrain\nLvalidation\n.\nG is small when training loss is lower than validation loss,\nwhich indicate a high degree of over\ufb01tting.\nFig. 6 shows a sharp drop in generalizability when cluster\nnumber increases from 9 to 10, which suggests that 9 is the\noptimal number of clusters. We indeed observe the highest\nNMI score at 9, which demonstrates that generalizability is\na good metric for selecting cluster number. NMI is highest\nat 9 instead 10 because 9 and 4 are similar in writing and\nDEC thinks that they should form a single cluster. This\ncorresponds well with our qualitative results in Fig. 3.\n6. Conclusion\nThis paper presents Deep Embedded Clustering, or DEC\u2014\nan algorithm that clusters a set of data points in a jointly op-\ntimized feature space. DEC works by iteratively optimiz-\ning a KL divergence based clustering objective with a self-\ntraining target distribution. Our method can be viewed as\nan unsupervised extension of semisupervised self-training.\nOur framework provide a way to learn a representation spe-\ncialized for clustering without groundtruth cluster member-\nship labels.\nEmpirical studies demonstrate the strength of our proposed\nalgorithm. DEC offers improved performance as well as\nrobustness with respect to hyperparameter settings, which\nis particularly important in unsupervised tasks since cross-\nvalidation is not possible. DEC also has the virtue of linear\ncomplexity in the number of data points which allows it to\nscale to large datasets.\n7. Acknowledgment\nThis work is in part supported by ONR N00014-13-1-0720,\nNSF IIS- 1338054, and Allen Distinguished Investigator\nAward.\nUnsupervised Deep Embedding for Clustering Analysis\nReferences\nAggarwal, Charu C and Reddy, Chandan K. Data cluster-\ning: algorithms and applications. CRC Press, 2013.\nAlelyani, Salem, Tang, Jiliang, and Liu, Huan.\nFeature\nselection for clustering: A review. Data Clustering: Al-\ngorithms and Applications, 2013.\nBellman, R. Adaptive Control Processes: A Guided Tour.\nPrinceton University Press, Princeton, New Jersey, 1961.\nBengio, Yoshua, Courville, Aaron, and Vincent, Pascal.\nRepresentation learning: A review and new perspectives.\n2013.\nBishop, Christopher M. Pattern recognition and machine\nlearning. springer New York, 2006.\nBoutsidis,\nChristos,\nDrineas,\nPetros,\nand Mahoney,\nMichael W. Unsupervised feature selection for the k-\nmeans clustering problem. In NIPS, 2009.\nCoates, Adam, Ng, Andrew Y, and Lee, Honglak.\nAn\nanalysis of single-layer networks in unsupervised feature\nlearning. In International Conference on Arti\ufb01cial Intel-\nligence and Statistics, pp. 215\u2013223, 2011.\nDe la Torre, Fernando and Kanade, Takeo. Discriminative\ncluster analysis. In ICML, 2006.\nDoersch, Carl, Singh, Saurabh, Gupta, Abhinav, Sivic,\nJosef, and Efros, Alexei. What makes paris look like\nparis? ACM Transactions on Graphics, 2012.\nGirshick, Ross, Donahue, Jeff, Darrell, Trevor, and Malik,\nJitendra. Rich feature hierarchies for accurate object de-\ntection and semantic segmentation. In CVPR, 2014.\nHalkidi, Maria, Batistakis, Yannis, and Vazirgiannis,\nMichalis. On clustering validation techniques. Journal\nof Intelligent Information Systems, 2001.\nHinton, Geoffrey E and Salakhutdinov, Ruslan R. Reduc-\ning the dimensionality of data with neural networks. Sci-\nence, 313(5786):504\u2013507, 2006.\nHornik, Kurt.\nApproximation capabilities of multilayer\nfeedforward networks. Neural networks, 4(2):251\u2013257,\n1991.\nJia, Yangqing, Shelhamer, Evan, Donahue, Jeff, Karayev,\nSergey, Long, Jonathan, Girshick, Ross, Guadarrama,\nSergio, and Darrell, Trevor.\nCaffe: Convolutional ar-\nchitecture for fast feature embedding.\narXiv preprint\narXiv:1408.5093, 2014.\nKrizhevsky, Alex, Sutskever, Ilya, and Hinton, Geoffrey E.\nImagenet classi\ufb01cation with deep convolutional neural\nnetworks. In NIPS, 2012.\nKuhn, Harold W. The hungarian method for the assignment\nproblem. Naval research logistics quarterly, 2(1-2):83\u2013\n97, 1955.\nLe, Quoc V. Building high-level features using large scale\nunsupervised learning. In Acoustics, Speech and Signal\nProcessing (ICASSP), 2013 IEEE International Confer-\nence on, pp. 8595\u20138598. IEEE, 2013.\nLeCun, Yann, Bottou, L\u00b4eon, Bengio, Yoshua, and Haffner,\nPatrick.\nGradient-based learning applied to document\nrecognition.\nProceedings of the IEEE, 86(11):2278\u2013\n2324, 1998.\nLewis, David D, Yang, Yiming, Rose, Tony G, and Li, Fan.\nRcv1: A new benchmark collection for text categoriza-\ntion research. JMLR, 2004.\nLi, Tao, Ma, Sheng, and Ogihara, Mitsunori.\nEntropy-\nbased criterion in categorical clustering. In ICML, 2004.\nLiu, Huan and Yu, Lei. Toward integrating feature selection\nalgorithms for classi\ufb01cation and clustering. IEEE Trans-\nactions on Knowledge and Data Engineering, 2005.\nLong, Jonathan, Shelhamer, Evan, and Darrell, Trevor.\nFully convolutional networks for semantic segmentation.\narXiv preprint arXiv:1411.4038, 2014.\nMacQueen, James et al. Some methods for classi\ufb01cation\nand analysis of multivariate observations. In Proceed-\nings of the \ufb01fth Berkeley symposium on mathematical\nstatistics and probability, pp. 281\u2013297, 1967.\nNair, Vinod and Hinton, Geoffrey E. Recti\ufb01ed linear units\nimprove restricted boltzmann machines. In ICML, 2010.\nNie, Feiping, Zeng, Zinan, Tsang, Ivor W, Xu, Dong,\nand Zhang, Changshui. Spectral embedded clustering:\nA framework for in-sample and out-of-sample spectral\nclustering.\nIEEE Transactions on Neural Networks,\n2011.\nNigam, Kamal and Ghani, Rayid. Analyzing the effective-\nness and applicability of co-training. In Proc. of the ninth\ninternational conference on Information and knowledge\nmanagement, 2000.\nSrivastava, Nitish, Hinton, Geoffrey, Krizhevsky, Alex,\nSutskever, Ilya, and Salakhutdinov, Ruslan. Dropout: A\nsimple way to prevent neural networks from over\ufb01tting.\nJMLR, 2014.\nSteinbach, Michael, Ert\u00a8oz, Levent, and Kumar, Vipin. The\nchallenges of clustering high dimensional data. In New\nDirections in Statistical Physics, pp. 273\u2013309. Springer,\n2004.\nUnsupervised Deep Embedding for Clustering Analysis\nTian, Fei, Gao, Bin, Cui, Qing, Chen, Enhong, and Liu,\nTie-Yan. Learning deep representations for graph clus-\ntering.\nIn AAAI Conference on Arti\ufb01cial Intelligence,\n2014.\nvan der Maaten, Laurens. Learning a parametric embed-\nding by preserving local structure. In International Con-\nference on Arti\ufb01cial Intelligence and Statistics, 2009.\nvan Der Maaten, Laurens. Accelerating t-SNE using tree-\nbased algorithms. JMLR, 2014.\nvan der Maaten, Laurens and Hinton, Geoffrey. Visualizing\ndata using t-SNE. JMLR, 2008.\nVincent, Pascal, Larochelle, Hugo, Lajoie, Isabelle, Ben-\ngio, Yoshua, and Manzagol, Pierre-Antoine. Stacked de-\nnoising autoencoders: Learning useful representations in\na deep network with a local denoising criterion. JMLR,\n2010.\nVon Luxburg, Ulrike.\nA tutorial on spectral clustering.\nStatistics and computing, 2007.\nXiang, Shiming, Nie, Feiping, and Zhang, Changshui.\nLearning a mahalanobis distance metric for data cluster-\ning and classi\ufb01cation. Pattern Recognition, 2008.\nXing, Eric P, Jordan, Michael I, Russell, Stuart, and Ng,\nAndrew Y. Distance metric learning with application to\nclustering with side-information. In NIPS, 2002.\nYan, Donghui, Huang, Ling, and Jordan, Michael I. Fast\napproximate spectral clustering.\nIn ACM SIGKDD,\n2009.\nYang, Yi, Xu, Dong, Nie, Feiping, Yan, Shuicheng, and\nZhuang, Yueting. Image clustering using local discrim-\ninant models and global integration. IEEE Transactions\non Image Processing, 2010.\nYe, Jieping, Zhao, Zheng, and Wu, Mingrui. Discrimina-\ntive k-means for clustering. In NIPS, 2008.\nZeiler, Matthew D and Fergus, Rob. Visualizing and un-\nderstanding convolutional networks. In ECCV. 2014.\n",
        "sentence": " Some of them are based on a clustering loss (Xie et al., 2016; Yang et al., 2016; Liao et al., 2016), but they are not tested at a scale comparable to that of supervised convnet training.",
        "context": "et al., 2002; Xiang et al., 2008), grouping methods (Mac-\nQueen et al., 1967; Von Luxburg, 2007; Li et al., 2004),\nand cluster validation (Halkidi et al., 2001). Space does\nnot allow for a comprehensive literature study and we refer\nand applicable to a wide range of problems. However, their\ndistance metrics are limited to the original data space and\nthey tend to be ineffective when input dimensionality is\nhigh (Steinbach et al., 2004).\non Image Processing, 2010.\nYe, Jieping, Zhao, Zheng, and Wu, Mingrui. Discrimina-\ntive k-means for clustering. In NIPS, 2008.\nZeiler, Matthew D and Fergus, Rob. Visualizing and un-\nderstanding convolutional networks. In ECCV. 2014."
    },
    {
        "title": "Maximum margin clustering",
        "author": [
            "L. Xu",
            "J. Neufeld",
            "B. Larson",
            "D. Schuurmans"
        ],
        "venue": "In NIPS,",
        "citeRegEx": "Xu et al\\.,? \\Q2004\\E",
        "shortCiteRegEx": "Xu et al\\.",
        "year": 2004,
        "abstract": "",
        "full_text": "",
        "sentence": " Many methods have been proposed to use discriminative losses for clustering (Xu et al., 2004; Bach & Harchaoui, 2007; Krause et al., 2010; Joulin & Bach, 2012). (2) would lead to a representation collapsing problem: all the images would be assigned to the same representation (Xu et al., 2004).",
        "context": null
    },
    {
        "title": "Joint unsupervised learning of deep representations and image clusters",
        "author": [
            "J. Yang",
            "D. Parikh",
            "D. Batra"
        ],
        "venue": null,
        "citeRegEx": "Yang et al\\.,? \\Q2016\\E",
        "shortCiteRegEx": "Yang et al\\.",
        "year": 2016,
        "abstract": "In this paper, we propose a recurrent framework for Joint Unsupervised\nLEarning (JULE) of deep representations and image clusters. In our framework,\nsuccessive operations in a clustering algorithm are expressed as steps in a\nrecurrent process, stacked on top of representations output by a Convolutional\nNeural Network (CNN). During training, image clusters and representations are\nupdated jointly: image clustering is conducted in the forward pass, while\nrepresentation learning in the backward pass. Our key idea behind this\nframework is that good representations are beneficial to image clustering and\nclustering results provide supervisory signals to representation learning. By\nintegrating two processes into a single model with a unified weighted triplet\nloss and optimizing it end-to-end, we can obtain not only more powerful\nrepresentations, but also more precise image clusters. Extensive experiments\nshow that our method outperforms the state-of-the-art on image clustering\nacross a variety of image datasets. Moreover, the learned representations\ngeneralize well when transferred to other tasks.",
        "full_text": "Joint Unsupervised Learning of Deep Representations and Image Clusters\nJianwei Yang, Devi Parikh, Dhruv Batra\nVirginia Tech\n{jw2yang, parikh, dbatra}@vt.edu\nAbstract\nIn this paper, we propose a recurrent framework for\nJoint Unsupervised LEarning (JULE) of deep represent-\nations and image clusters. In our framework, successive\noperations in a clustering algorithm are expressed as steps\nin a recurrent process, stacked on top of representations\noutput by a Convolutional Neural Network (CNN). Dur-\ning training, image clusters and representations are up-\ndated jointly: image clustering is conducted in the for-\nward pass, while representation learning in the backward\npass. Our key idea behind this framework is that good rep-\nresentations are bene\ufb01cial to image clustering and clus-\ntering results provide supervisory signals to representa-\ntion learning. By integrating two processes into a single\nmodel with a uni\ufb01ed weighted triplet loss and optimizing\nit end-to-end, we can obtain not only more powerful rep-\nresentations, but also more precise image clusters.\nEx-\ntensive experiments show that our method outperforms the\nstate-of-the-art on image clustering across a variety of im-\nage datasets. Moreover, the learned representations gen-\neralize well when transferred to other tasks. The source\ncode can be downloaded from https://github.com/\njwyang/joint-unsupervised-learning.\n1. Introduction\nWe are witnessing an explosion in visual content. Signi-\n\ufb01cant recent advances in machine learning and computer\nvision, especially via deep neural networks, have relied\non supervised learning and availability of copious annot-\nated data.\nHowever, manually labelling data is a time-\nconsuming, laborious, and often expensive process. In order\nto make better use of available unlabeled images, clustering\nand/or unsupervised learning is a promising direction.\nIn this work, we aim to address image clustering and rep-\nresentation learning on unlabeled images in a uni\ufb01ed frame-\nwork. It is a natural idea to leverage cluster ids of images as\nsupervisory signals to learn representations and in turn the\nrepresentations would be bene\ufb01cial to image clustering. At\na high-level view, given a collection of ns unlabeled images\n(a) Initial stage\n(b) Middle stage\n(c) Final stage\nFigure 1: Clustering outputs for MNIST [34] test set at dif-\nferent stages of the proposed method. We conduct PCA on\nthe image representations and then choose the \ufb01rst three di-\nmensions for visualization. Different colors correspond to\ndifferent clusters. Samples are grouped together gradually\nand more discriminative representations are obtained.\nI = {I1, ..., Ins}, the global objective function for learning\nimage representations and clusters can be written as:\nargmin\ny,\u03b8\nL(y, \u03b8|I)\n(1)\nwhere L(\u00b7) is a loss function, y denotes the cluster ids for all\nimages, and \u03b8 denotes the parameters for representations. If\nwe hold one in {y, \u03b8} to be \ufb01xed, the optimization can be\ndecomposed into two alternating steps:\nargmin\ny\nL(y|I, \u03b8)\n(2a)\nargmin\n\u03b8\nL(\u03b8|I, y)\n(2b)\nIntuitively, (2a) can be cast as a conventional cluster-\ning problem based on \ufb01xed representations, while (2b) is\na standard supervised representation learning process.\nIn this paper, we propose an approach that alternates\nbetween the two steps \u2013 updating the cluster ids given\nthe current representation parameters and updating the rep-\nresentation parameters given the current clustering res-\nult.\nSpeci\ufb01cally, we cluster images using agglomerative\nclustering[17] and represent images via activations of a\nConvolutional Neural Network (CNN).\nThe reason to choose agglomerative clustering is three-\nfold: 1) it begins with an over-clustering, which is more\nreliable in the beginning when a good representation has\nnot yet been learned. Intuitively, clustering with represent-\nations from a CNN initialized with random weights are not\n1\narXiv:1604.03628v3  [cs.CV]  20 Jun 2016\nreliable, but nearest neighbors and over-clusterings are of-\nten acceptable; 2) These over-clusterings can be merged as\nbetter representations are learned; 3) Agglomerative clus-\ntering is a recurrent process and can naturally be interpreted\nin a recurrent framework.\nOur \ufb01nal algorithm is farily intuitive.\nWe start with\nan intial over-clustering, update CNN parameters (2b) us-\ning image cluster labels as supervisory signals, then merge\nclusters (2a) and iterate until we reach a stopping criterion.\nAn outcome of the proposed framework is illustrated in\nFig. 1. Initially, there are 1,762 clusters for MNIST test\nset (10k samples), and the representations (image intensit-\nies) are not that discriminative. After several iterations, we\nobtain 17 clusters and more discriminative representations.\nFinally, we obtain 10 clusters which are well-separated\nby the learned representations and interestingly correspond\nprimarily to the groundtruth category labels in the dataset,\neven though the representation is learnt in an unsupervised\nmanner. To summarize, the major contributions of our work\nare:\n1 We propose a simple but effective end-to-end learning\nframework to jointly learn deep representations and\nimage clusters from an unlabeled image set;\n2 We formulate the joint learning in a recurrent frame-\nwork, where merging operations of agglomerative\nclustering are expressed as a forward pass, and rep-\nresentation learning of CNN as a backward pass;\n3 We derive a single loss function to guide agglomerat-\nive clustering and deep representation learning, which\nmakes optimization over the two tasks seamless;\n4 Our experimental results show that the proposed\nframework outperforms previous methods on image\nclustering and learns deep representations that can be\ntransferred to other tasks and datasets.\n2. Related Work\nClustering Clustering algorithms can be broadly categor-\nized into hierarchical and partitional approaches [25]. Ag-\nglomerative clustering is a hierarchical clustering algorithm\nthat begins with many small clusters, and then merges\nclusters gradually [17, 31, 13]. As for partitional cluster-\ning methods, the most well-known is K-means [39], which\nminimizes the sum of square errors between data points\nand their nearest cluster centers. Related ideas form the\nbasis of a number of methods, such as expectation max-\nimization (EM) [8, 40], spectral clustering [43, 67, 52], and\nnon-negative matrix factorization (NMF) based clustering\n[9, 1, 66].\nDeep Representation Learning Many works use raw im-\nage intensity or hand-crafted features [55, 10, 20, 19, 46, 24]\ncombined with conventional clustering methods. Recently,\nrepresentations learned using deep neural networks have\npresented signi\ufb01cant improvements over hand-designed\nfeatures on many computer vision tasks, such as image clas-\nsi\ufb01cation [30, 51, 54, 49], object detection [15, 14, 21, 47],\netc. However, these approaches rely on supervised learn-\ning with large amounts of labeled data to learn rich rep-\nresentations. A number of works have focused on learn-\ning representations from unlabled image data. One class\nof approaches cater to reconstruction tasks, such as auto-\nencoders [45, 22, 58, 29, 35], deep belief networks (DBN)\n[33], etc. Another group of techniques learn discriminative\nrepresentations after fabricating supervisory signals for im-\nages, and then \ufb01netune them supervisedly for downstream\napplications [12, 11, 60]. Unlike our approach, the fabric-\nated supervisory signal in these previous works is not up-\ndated during representation learning.\nCombination A number of works have explored combining\nimage clustering with representation learning. In [56], the\nauthors proposed to learn a non-linear embedding of the un-\ndirected af\ufb01nity graph using stacked autoencoder, and then\nran K-means in the embedding space to obtain clusters. In\n[57], a deep semi-NMF model was used to factorize the in-\nput into multiple stacking factors which are initialized and\nupdated layer by layer. Using the representations on the\ntop layer, K-means was implemented to get the \ufb01nal res-\nults. Unlike our work, they do not jointly optimize for the\nrepresentation learning and clustering.\nTo connect image clustering and representation learning\nmore closely, [64] conducted image clustering and code-\nbook learning iteratively. However, they learned codebook\nover SIFT feature [37], and did not learn deep representa-\ntions. Instead of using hand-crafted features, Chen [2] used\nDBN to learn representations, and then conducted a non-\nparametric maximum margin clustering upon the outputs\nof DBN. Afterwards, they \ufb01ne-tuned the top layer of DBN\nbased on clustering results. A more recent work on jointly\noptimizing two tasks is found in [61], where the authors\ntrained a task-speci\ufb01c deep architecture for clustering. The\ndeep architecture is composed of sparse coding modules\nwhich can be jointly trained through back propagation from\na cluster-oriented loss. However, they used sparse coding\nto extract representations for images, while we use a CNN.\nInstead of \ufb01xing the number of clusters to be the number of\ncategories and predicted labels based on softmax outputs,\nwe predict the labels using agglomerative clustering based\non the learned representations. In our experiments we show\nthat our approach outperforms [61].\n3. Approach\n3.1. Notation\nWe denote an image set with ns images by I\n=\n{I1, ..., Ins}.\nThe cluster labels for this image set are\ny = {y1, ..., yns}. \u03b8 are the CNN parameters, based on\nwhich we obtain deep representations X = {x1, ..., xns}\n2\nyt\ny t+ 1\nClustering\nClustering\nyT\nClustering\nXt\nXt+1\nXT\nht\nht+1\nhT\nh t\u00a11\nI\nCNN\nI\nI\nCNN\nCNN\nhT\u00a11\n(\u00b5t+1)\n(\u00b5t)\n(\u00b5T)\nFigure 2: Proposed recurrent framework for unsupervised\nlearning of deep representations and image clusters.\nfrom I. Given the predicted image cluster labels, we or-\nganize them into nc clusters C = {C1, ..., Cnc}, where\nCi = {xk|yk = i, \u2200k \u22081, ..., ns}.\nN Ks\ni\nare the Ks\nnearest neighbours of xi, and N Kc\nCi\nis the set of Kc nearest\nneighbour clusters of Ci. For convenience, we sort clusters\nin N Kc\nCi\nin descending order of af\ufb01nity with Ci so that the\nnearest neighbour argmaxC\u2208Ct A(Ci, C) is the \ufb01rst entry\nN Kc\nCi [1]. Here, A is a function to measure the af\ufb01nity (or\nsimilarity) between two clusters. We add a superscript t to\n{\u03b8, X, y, C} to refer to their states at timestep t. We use Y\nto denote the sequence {y1, ..., yT } with T timesteps.\n3.2. Agglomerative Clustering\nAs background, we \ufb01rst brie\ufb02y describe conventional ag-\nglomerative clustering [17, 31]. The core idea in agglomer-\native clustering is to merge two clusters at each step until\nsome stopping conditions. Mathematically, it tries to \ufb01nd\ntwo clusters Ca and Cb by\n{Ca, Cb} =\nargmax\nCi,Cj\u2208C,i\u0338=j\nA(Ci, Cj)\n(3)\nThere are many methods to compute the af\ufb01nity between\ntwo clusters [17, 31, 41, 70, 68]. More details can be found\nin [25]. We now describe how the af\ufb01nity is measured by A\nin our approach.\n3.3. Af\ufb01nity Measure\nFirst, we build a directed graph G =< V, E >, where V\nis the set of vertices corresponding to deep representations\nX for I, and E is the set of edges connecting vertices. We\nde\ufb01ne an af\ufb01nity matrix W \u2208Rns\u00d7ns corresponding to the\nedge set. The weight from vertex xi to xj is de\ufb01ned by\nW (i, j) =\n(\nexp(\u2212||xi\u2212xj||2\n2\n\u03c32\n),\nif xj \u2208N Ks\ni\n0,\notherwise\n(4)\nwhere \u03c32 =\na\nnsKs\nP\nxi\u2208X\nP\nxj\u2208N Ks\ni\n||xi \u2212xj||2\n2. This\nway to build up a directed graph can be found in many pre-\nvious works such as [70, 68]. Here, a and Ks are two pre-\nde\ufb01ned parameters (their values are listed in Table 2). After\nconstructing a directed graph for samples, we then adopt the\ngraph degree linkage in [68] to measure the af\ufb01nity between\ncluster Ci and Cj, denoted by A(Ci, Cj).\n3.4. A Recurrent Framework\nOur key insight is that agglomerative clustering can be\ninterpreted as a recurrent process in the sense that it merges\nclusters over multiple timesteps. Based on this insight, we\npropose a recurrent framework to combine the image clus-\ntering and representation learning processes.\nAs shown in Fig. 2, at the timestep t, images I are \ufb01rst\nfed into the CNN to get representations Xt and then used in\nconjunction with previous hidden state ht\u22121 to predict cur-\nrent hidden state ht, i.e, the image cluster labels at timestep\nt. In our context, the output at timestep t is yt = ht. Hence,\nat timestep t\nXt = fr(I|\u03b8t)\n(5a)\nht = fm(Xt, ht\u22121)\n(5b)\nyt = fo(ht) = ht\n(5c)\nwhere fr is a function to extract deep representations Xt\nfor input I using the CNN parameterized by \u03b8t, and fm is a\nmerging process for generating ht based on Xt and ht\u22121.\nIn a typical Recurrent Neural Network, one would un-\nroll all timesteps at each training iteration. In our case, that\nwould involve performing agglomerative clustering until we\nobtain the desired number of clusters, and then update the\nCNN parameters by back-propagation.\nIn this work, we introduce a partial unrolling strategy,\ni.e., we split the overall T timesteps into multiple periods,\nand unroll one period at a time. The intuitive reason we\nunroll partially is that the representation of the CNN at the\nbeginning is not reliable. We need to update CNN para-\nmeters to obtain more discriminative representations for the\nfollowing merging processes. In each period, we merge a\nnumber of clusters and update CNN parameters for a \ufb01xed\nnumber of iterations at the end of the period. An extreme\ncase would be one timestep per period, but it involves up-\ndating the CNN parameters too frequently and is thus time-\nconsuming. Therefore, the number of timesteps per period\n(and thus the number of clusters merged per period) is de-\ntermined by a parameter in our approach. We elaborate on\nthis more in Sec. 3.6.\n3.5. Objective Function\nIn our recurrent framework, we accumulate the losses\nfrom all timesteps, which is formulated as\nL({y1, ..., yT }, {\u03b81, ..., \u03b8T }|I) =\nT\nX\nt=1\nLt(yt, \u03b8t|yt\u22121, I) (6)\n3\nHere, y0 takes each image as a cluster. At timestep t, we\n\ufb01nd two clusters to merge given yt\u22121. In conventional ag-\nglomerative clustering, the two clusters are determined by\n\ufb01nding the maximal af\ufb01nity over all pairs of clusters. In\nthis paper, we introduce a criterion that considers not only\nthe af\ufb01nity between two clusters but also the local struc-\nture surrounding the clusters. Assume from yt\u22121 to yt, we\nmerged a cluster Ct\ni and its nearest neighbour. Then the loss\nat timestep t is a combination of negative af\ufb01nities, that is,\nLt(yt, \u03b8t|yt\u22121, I) = \u2212A(Ct\ni, N Kc\nCt\ni [1])\n(7a)\n\u2212\n\u03bb\n(Kc \u22121)\nKc\nX\nk=2\n\u0010\nA(Ct\ni, N Kc\nCt\ni [1]) \u2212A(Ct\ni, N Kc\nCt\ni [k])\n\u0011\n(7b)\nwhere \u03bb weighs (7a) and (7b). Note that yt, yt\u22121 and \u03b8t\nare not explicitly presented at the right side, but they de-\ntermine the loss via the image cluster labels and af\ufb01nities\namong clusters. On the right side of the above equation,\nthere are two terms: 1) (7a) measures the af\ufb01nity between\ncluster Ci and its nearest neighbour, which follows conven-\ntional agglomerative clustering; 2) (7b) measures the differ-\nence between af\ufb01nity of Ci to its nearest neighbour cluster\nand af\ufb01nities of Ci to its other neighbour clusters. This term\ntakes the local structure into account. See Sec. 3.5.1 for\ndetailed explanation.\nIt\nis\nhard\nto\nsimultaneously\nderive\nthe\noptimal\n{y1, ..., yT } and {\u03b81, ..., \u03b8T } that minimize the overall loss\nin Eq. (6). As aforementioned, we optimize iteratively in a\nrecurrent process. We divide T timesteps into P partially\nunrolled periods. In each period, we \ufb01x \u03b8 and search op-\ntimal y in the forward pass, and then in the backward pass\nwe derive optimal \u03b8 given the optimal y. Details will be\nexplained in the following sections.\n3.5.1\nForward Pass\nIn forward pass of the p-th (p \u2208{1, ..., P}) partially un-\nrolled period, we update the cluster labels with \u03b8 \ufb01xed to\n\u03b8p, and the overall loss in period p is\nLp(Yp|\u03b8p, I) =\nte\np\nX\nt=tsp\nLt(yt|\u03b8p, yt\u22121, I)\n(8)\nwhere Yp is the sequence of image labels in period p, and\n[ts\np, te\np] is the corresponding timesteps in period p. For op-\ntimization, we follow a greedy search similar to conven-\ntional agglomerative clustering. Starting from the time step\nts\np, it \ufb01nds one cluster and its nearest neighbour to merge so\nthat Lt is minimized over all possible cluster pairs.\nIn Fig. 3, we present a toy example to explain the reason\nwhy we employ the term (7b). As shown, it is often the\ncase that the clusters are densely populated in some re-\ngions while sparse in some other regions. In conventional\nb\nd\na\ne\nc\n(a)\nb\nd\na\ne\nc\n(b)\nFigure 3: A toy illustration of (a) conventional agglomerat-\nive clustering strategy and (b) the proposed one. For simpli-\n\ufb01cation, we use a single circle to represent a cluster/sample.\nIn conventional agglomerative clustering, node b and its\nnearest neighbour are chosen to merge because they are\nclosest to each other; while node e is chosen in our pro-\nposed strategy considering the local structure.\nagglomerative clustering, it will choose two clusters with\nlargest af\ufb01nity (or smallest loss) at each time no mater\nwhere the clusters are located. In this speci\ufb01c case, it will\nchoose cluster Cb and its nearest neighbour to merge. In\ncontrast, as shown in Fig. 3(b), our algorithm by adding\n(7b) will \ufb01nd cluster Ce, because it is not only close to it\nnearest neighbour, but also relatively far away from its other\nneighbours, i.e., the local structure is considered around one\ncluster. Another merit of introducing (7b) is that it will al-\nlow us to write the loss in terms of triplets as explained next.\n3.5.2\nBackward Pass\nIn forward pass of the p-th partially unrolled period, we\nhave merged a number of clusters.\nLet the sequence of\noptimal image cluster labels be given by Yp\n\u2217= {yt\n\u2217},\nand clusters merged in forward pass are denoted by\n{[Ct\n\u2217, N Kc\nCt\u2217[1]]}, t \u2208{ts\np, ..., te\np}. In the backward pass, we\naim to derive the optimal \u03b8 to minimize the losses generated\nin forward pass. Because the clustering in current period is\nconditioned on the clustering results of all previous periods,\nwe accumulate the losses of all p periods, i.e.,\nL(\u03b8|{Y1\n\u2217, ..., Yp\n\u2217}, I) =\np\nX\nk=1\nLk(\u03b8|Yk\n\u2217, I)\n(9)\nMinimizing (9) w.r.t \u03b8 leads to representation learning\non I supervised by {Y1\n\u2217, ..., Yp\n\u2217} or {y1\n\u2217, ..., y\nte\np\n\u2217}. Based\non (7a) and (7b), the loss in Eq. 9 is reformulated to\n\u2212\n\u03bb\nKc \u22121\nte\np\nX\nt=1\nKc\nX\nk=2\n\u0010\n\u03bb\u2032A(Ct\n\u2217, N Kc\nCt\u2217[1]) \u2212A(Ct\n\u2217, N Kc\nCt\u2217[k])\n\u0011\n(10)\nwhere \u03bb\u2032 = (1 + 1/\u03bb). (10) is a loss de\ufb01ned on clusters\nof points, which needs the entire dataset to estimate, mak-\ning it dif\ufb01cult to use batch-based optimization. However,\n4\nAlgorithm 1 Joint Optimization on y and \u03b8\nInput:\nI: = collection of image data;\nn\u2217\nc: = target number of clusters;\nOutput:\ny\u2217, \u03b8\u2217: = \ufb01nal image labels and CNN parameters;\n1: t \u21900; p \u21900\n2: Initialize \u03b8 and y\n3: repeat\n4:\nUpdate yt to yt+1 by merging two clusters\n5:\nif t = te\np then\n6:\nUpdate \u03b8p to \u03b8p+1 by training CNN\n7:\np \u2190(p + 1)\n8:\nend if\n9:\nt \u2190t + 1\n10: until Cluster number reaches n\u2217\nc\n11: y\u2217\u2190yt; \u03b8\u2217\u2190\u03b8p\nwe show that this loss can be approximated by a sample-\nbased loss, enabling us to compute unbiased estimators for\nthe gradients using batch-statistics.\nThe intuition behind reformulation of the loss is that\nagglomerative clustering starts with each datapoint as a\ncluster, and clusters at a higher level in the hierarchy are\nformed by merging lower level clusters.\nThus, af\ufb01nities\nbetween clusters can be expressed in terms of af\ufb01nities\nbetween datapoints. We show in the supplement that the\nloss in (10) can be approximately reformulated as\nL(\u03b8|y\nte\np\n\u2217, I) = \u2212\n\u03bb\nKc \u22121\nX\ni,j,k\n(\u03b3A(xi, xj) \u2212A(xi, xk))\n(11)\nwhere \u03b3 is a weight whose value depends on \u03bb\u2032 and how\nclusters are merged during the forward pass. xi and xj\nare from the same cluster, while xk is from the neighbour-\ning clusters, and their cluster labels are merely determined\nby the \ufb01nal clustering result y\nte\np\n\u2217. To further simplify the\noptimization, we instead search xk in at most Kc neigh-\nbour samples of xi from other clusters in a training batch.\nHence, the batch-wise optimization can be performed using\nconventional stochastic gradient descent method. Note that\nsuch triplet losses have appeared in other works [59, 50].\nBecause it is associated with a weight, we call (35) the\nweighted triplet loss.\n3.6. Optimization\nGiven an image dataset with ns samples, we assume the\nnumber of desired clusters n\u2217\nc is given to us as is standard in\nclustering. Then we can build up a recurrent process with\nT = ns \u2212n\u2217\nc timesteps, starting by regarding each sample\nas a cluster. However, such initialization makes the optim-\nization time-consuming, especially when datasets contain\na large number of samples. To address this problem, we\ncan \ufb01rst run a fast clustering algorithm to get the initial\nclusters. Here, we adopt the initialization algorithm pro-\nposed in [69] for fair comparison with their experiment res-\nults. Note that other kind of initializations can also be used,\ne.g. K-means. Based on the algorithm in [69], we obtain\na number of clusters which contain a few samples for each\n(average is about 4 in our experiments). Given these initial\nclusters, our optimization algorithm learns deep represent-\nations and clusters. The algorithm is outlined in Alg. 1.\nIn each partially unrolled period, we perform forward and\nbackward passes to update y and \u03b8, respectively. Speci\ufb01c-\nally, in the forward pass, we merge two clusters at each\ntimestep. In the backward pass, we run about 20 epochs\nto update \u03b8, and the af\ufb01nity matrix W is also updated based\non the new representation. The duration of the p-th period\nis np = ceil(\u03b7 \u00d7 ns\nc) timesteps, where ns\nc is the number\nof clusters at the beginning of current period, and \u03b7 is a\nparameter called unrolling rate to control the number of\ntimesteps. The less \u03b7 is, the more frequently we update \u03b8.\n4. Experiments\n4.1. Image Clustering\nWe compare our approach with 12 clustering algorithms,\nincluding K-means [39], NJW spectral clustering (SC-\nNJW) [43], self-tuning spectral clustering (SC-ST)[67],\nlarge-scale spectral clustering (SC-LS) [3], agglomerative\nclustering with average linkage (AC-Link)[25], Zeta func-\ntion based agglomerative clustering (AC-Zell) [70], graph\ndegree linkage-based agglomerative clustering (AC-GDL)\n[68], agglomerative clustering via path integral (AC-PIC)\n[69], normalized cuts (N-Cuts) [52], locality preserving\nnon-negative matrix factorization (NMF-LP) [1], NMF with\ndeep model (NMF-D) [57], task-speci\ufb01c clustering with\ndeep model (TSC-D) [61].\nFor evaluation, we use a commonly used metric: nor-\nmalized mutual information (NMI) [65]. It ranges in [0, 1].\nLarger value indicates more precise clustering results.\n4.1.1\nDatasets\nWe evaluate the clustering performance on two hand-\nwritten digit image datasets (MNIST [34] and USPS1), two\nmulti-view object image datasets (COIL20 and COIL100\n[42]), and four face image datasets (UMist [18], FRGC-\nv2.02, CMU-PIE [53], Youtube-Face (YTF)) [63].\nThe\nnumber of samples and categories, and image size are lis-\nted in Table 1. MNIST consists of training set (60,000) and\ntesting set (10,000). To compare with different approaches,\nwe experiment on the full set (MNIST-full) and testing set\n(MNIST-test), separately. For face image datasets such as\n1http://www.cs.nyu.edu/\u02dcroweis/data.html\n2http://www3.nd.edu/\u02dccvrl/CVRL/Data_Sets.html\n5\nTable 1: Datasets used in our experiments.\nDataset\nMNIST\nUSPS\nCOIL20\nCOIL100\nUMist\nFRGC-v2.0\nCMU-PIE\nYTF\n#Samples\n70000\n11000\n1440\n7200\n575\n2462\n2856\n10000\n#Categories\n10\n10\n20\n100\n20\n20\n68\n41\nImage Size\n28\u00d728\n16\u00d716\n128\u00d7128\n128\u00d7128\n112\u00d792\n32\u00d732\n32\u00d732\n55\u00d755\nTable 2: Hyper-parameters in our approach.\nHyper-parameter\nKs\na\nKc\n\u03bb\n\u03b3\n\u03b7\nValue\n20\n1.0\n5\n1.0\n2.0\n0.9 or 0.2\nUMist, CMU-PIE, we use the images provided as is without\nany changes. For FRGC-v2.0 and YTF datasets, we \ufb01rst\ncrop faces and then resize them to a constant size. In FRGC-\nv2.0 dataset, we randomly choose 20 subjects. As for YTF\ndataset, we choose the \ufb01rst 41 subjects which are sorted by\ntheir names in alphabet order.\n4.1.2\nExperimental Setup\nAll the hyper-parameters and their values for our approach\nare listed in Table 2. In our experiments, Ks is set to 20, the\nsame value to [68]. a and \u03bb are simply set to 1.0. We search\nthe values of Kc and \u03b3 for best performance on MNIST-test\nset. The unrolling rate \u03b7 for \ufb01rst four datasets is 0.9; and\n0.2 for face datasets. The target cluster number n\u2217\nc is set to\nbe the number of categories in each dataset.\nWe use Caffe [27] to implement our approach.\nWe\nstacked multiple combinations of convolutional layer, batch\nnormalization layer, ReLU layer and pooling layer. For all\nthe convolutional layers, the number of channels is 50, and\n\ufb01lter size is 5\u00d75 with stride = 1 and padding = 0. For pool-\ning layer, its kernel size is 2 and stride is 2. To deal with\nvarying image sizes across datasets, the number of stacked\nconvolutional layers for each dataset is chosen so that the\nsize of the output feature map is about 10\u00d710. On the top of\nall CNNs, we append an inner product (ip) layer whose di-\nmension is 160. ip layer is followed by a L2-normalization\nlayer before being fed to the weighted triplet loss layer or\nused for clustering. For each partially unrolled period, the\nbase learning rate is set to 0.01, momentum 0.9, and weight\ndecay 5 \u00d7 10\u22125. We use the inverse learning rate decay\npolicy, with Gamma=0.0001 and Power=0.75. Stochastic\ngradient descent (SGD) is adopted for optimization.\n4.1.3\nQuantitative Comparison\nWe report NMI for different methods on various datasets.\nResults are averaged from 3 runs. We report the results by\nre-running the code released by original papers. For those\nthat did not release the code, the corresponding results are\nborrowed from the papers. We \ufb01nd the results we obtain\nare somewhat different from the one reported in original\npapers. We suspect that these differences may be caused\nby the different experimental settings or the released code\nis changed from the one used in the original paper. For all\ntest algorithms, we conduct L2-normalization on the image\nintensities since it empirically improves the clustering per-\nformance. We report our own results in two cases: 1) the\nstraight-forward clustering results obtained when the recur-\nrent process \ufb01nish, denoted by OURS-SF; 2) the clustering\nresults obtained by re-running clustering algorithm after ob-\ntaining the \ufb01nal representation, denoted by OURS-RC. The\nquantitative results are shown in Table 3. In the table cells,\nthe value before \u2019/\u2019 is obtained by re-running code while the\nvalue after \u2019/\u2019 is that reported in previous papers.\nAs we can see from Table 3, both OURS-SF and OURS-\nRC outperform previous methods on all datasets with no-\nticeable margin. Interestingly, we achieved perfect results\n(NMI = 1) on COIL20 and CMU-PIE datasets, which means\nthat all samples in the same category are clustered into the\nsame group. The agglomerative clustering algorithms, such\nas AC-Zell, AC-GDL and AC-PIC perform better than other\nalgorithms generally. However, on MNIST-full test, they\nall perform poorly. The possible reason is that MNIST-\nfull has 70k samples, and these methods cannot cope with\nsuch large-scale dataset when using image intensity as rep-\nresentation.\nHowever, this problem is addressed by our\nlearned representation. We show that we achieved analog-\nous performance on MNIST-full to MNIST-test set. In most\ncases, we can \ufb01nd OURS-RC performs better on datasets\nthat have room for improvement. We believe the reason is\nthat OURS-RC uses the \ufb01nal learned representation over the\nentire clustering process, while OURS-SF starts with image\nintensity, which indicates that the learned representation is\nmore discriminative than image intensity. 3\n4.1.4\nGeneralization Across Clustering Algorithms\nWe now evaluate if the representations learned by our joint\nagglomerative clustering and representation learning ap-\nproach generalize to other clustering techniques. We re-run\nall the clustering algorithms without any changes of para-\nmeters, but using our learned deep representations as fea-\n3We experimented with hand-crafted features such as HOG, LBP, spa-\ntial pyramid on a subset of the datasets with some of the better clustering\nalgorithms from Table 3, and found that they performed worse.\n6\nTable 3: Quantitative clustering performance (NMI) for different algorithms using image intensities as input.\nDataset\nCOIL20\nCOIL100\nUSPS\nMNIST-test\nMNIST-full\nUMist\nFRGC\nCMU-PIE\nYTF\nK-means [39]\n0.775\n0.822\n0.447\n0.528\n0.500\n0.609\n0.389\n0.549\n0.761\nSC-NJW [43]\n0.860/0.889\n0.872/0.854\n0.409/0.690\n0.528/0.755\n0.476\n0.727\n0.186\n0.543\n0.752\nSC-ST [67]\n0.673/0.895\n0.706/0.858\n0.342/0.726\n0.445/0.756\n0.416\n0.611\n0.431\n0.581\n0.620\nSC-LS [3]\n0.877\n0.833\n0.681\n0.756\n0.706\n0.810\n0.550\n0.788\n0.759\nN-Cuts [52]\n0.768/0.884\n0.861/0.823\n0.382/0.675\n0.386/0.753\n0.411\n0.782\n0.285\n0.411\n0.742\nAC-Link [25]\n0.512\n0.711\n0.579\n0.662\n0.686\n0.643\n0.168\n0.545\n0.738\nAC-Zell [70]\n0.954/0.911\n0.963/0.913\n0.774/0.799\n0.810/0.768\n0.017\n0.755\n0.351\n0.910\n0.733\nAC-GDL [68]\n0.945/0.937\n0.954/0.929\n0.854/0.824\n0.864/0.844\n0.017\n0.755\n0.351\n0.934\n0.622\nAC-PIC [69]\n0.950\n0.964\n0.840\n0.853\n0.017\n0.750\n0.415\n0.902\n0.697\nNMF-LP [1]\n0.720\n0.783\n0.435\n0.467\n0.452\n0.560\n0.346\n0.491\n0.720\nNMF-D [57]\n0.692\n0.719\n0.286\n0.243\n0.148\n0.500\n0.258\n0.983/0.910\n0.569\nTSC-D [61]\n-/0.928\n-\n-\n-\n-/0.651\n-\n-\n-\n-\nOURS-SF\n1.000\n0.978\n0.858\n0.876\n0.906\n0.880\n0.566\n0.984\n0.848\nOURS-RC\n1.000\n0.985\n0.913\n0.915\n0.913\n0.877\n0.574\n1.00\n0.848\nTable 4: Quantitative clustering performance (NMI) for different algorithms using our learned representations as inputs.\nDataset\nCOIL20\nCOIL100\nUSPS\nMNIST-test\nMNIST-full\nUMist\nFRGC\nCMU-PIE\nYTF\nK-means [39]\n0.926\n0.919\n0.758\n0.908\n0.927\n0.871\n0.636\n0.956\n0.835\nSC-NJW [43]\n0.915\n0.898\n0.753\n0.878\n0.931\n0.833\n0.625\n0.957\n0.789\nSC-ST [67]\n0.959\n0.922\n0.741\n0.911\n0.906\n0.847\n0.651\n0.938\n0.741\nSC-LS [3]\n0.950\n0.905\n0.780\n0.912\n0.932\n0.879\n0.639\n0.950\n0.802\nN-Cuts [52]\n0.963\n0.900\n0.705\n0.910\n0.930\n0.877\n0.640\n0.995\n0.823\nAC-Link [25]\n0.896\n0.884\n0.783\n0.901\n0.918\n0.872\n0.621\n0.990\n0.803\nAC-Zell [70]\n1.000\n0.989\n0.910\n0.893\n0.919\n0.870\n0.551\n1.000\n0.821\nAC-GDL [68]\n1.000\n0.985\n0.913\n0.915\n0.913\n0.870\n0.574\n1.000\n0.842\nAC-PIC [69]\n1.000\n0.990\n0.914\n0.909\n0.907\n0.870\n0.553\n1.000\n0.829\nNMF-LP [1]\n0.855\n0.834\n0.729\n0.905\n0.926\n0.854\n0.575\n0.690\n0.788\ntures. The results are shown in Table 4. It can be seen that\nall clustering algorithms obtain more precise image clusters\nby using our learned representation. Some algorithms like\nK-means, AC-Link that performed very poorly with raw in-\ntensities perform much better with our learned representa-\ntions, and the variance in performance across all clustering\nalgorithms is much lower. These results clearly demonstrate\nthat our learned representation is not over-\ufb01tting to a single\nclustering algorithm, but generalizes well across various al-\ngorithms. Interestingly, using our learned representation,\nsome of the clustering algorithms perform even better than\nAC-GDL we build on in our approach.\n4.2. Transferring Learned Representation\n4.2.1\nCross-Dataset Clustering\nTable 5: NMI performance across COIL20 and COIL100.\nLayer\ndata\ntop(ip)\ntop-1\ntop-2\nCOIL20 \u2192COIL100\n0.924\n0.927\n0.939\n0.934\nCOIL100 \u2192COIL20\n0.944\n0.949\n0.957\n0.951\nTable 6: NMI performance across MNIST-test and USPS.\nLayer\ndata\ntop(ip)\ntop-1\ntop-2\nMNIST-test \u2192USPS\n0.874\n0.892\n0.907\n0.908\nUSPS \u2192MNIST-test\n0.872\n0.873\n0.886\n-\nIn this section, we study whether our learned represent-\nations generalize across datasets. We train a CNN based\non our approach on one dataset, and then cluster images\nfrom another (but related) dataset using the image fea-\ntures extracted via the CNN. Speci\ufb01cally, we experiment on\ntwo dataset pairs: 1) multi-view object datasets (COIL20\nand COIL100); 2) hand-written digit datasets (USPS and\nMNIST-test). We use the representation learned from one\ndataset to represent another dataset, followed by agglom-\nerative clustering.\nNote that because the image sizes or\nchannels are different across datasets, we resize the input\nimages and/or expand the channels before feeding them to\nCNN. The experimental results are shown in Table 5 and 6.\nWe use the representations from top ip layer and also the\nconvolutional or pooling layers (top-1, top-2) close to top\nlayer for image clustering. In two tables, compared with\n7\nTable 7: Face veri\ufb01cation results on LFW.\n#Samples\n10k\n20k\n30k\n50k\n100k\nSupervised\n0.737\n0.746\n0.748\n0.764\n0.770\nOURS\n0.728\n0.743\n0.750\n0.762\n0.767\ndirectly using raw image from the data layer, the cluster-\ning performance based on learned representations from all\nlayers improve, which indicates that the learned represent-\nations can be transferred across these datasets. As perhaps\nexpected, the performance on target datasets is worse com-\npared to learning on the target dataset directly. For COIL20\nand COIL100, a possible reason is that they have different\nimage categories. As for MNIST and USPS, the perform-\nance beats OURS-SF, but worse than OURS-RC. We \ufb01nd\ntransferring representation learned on MNIST-test to USPS\ngets close performance to OURS-RC learned on USPS.\n4.2.2\nFace Veri\ufb01cation\nWe now evaluate the performance of our approach by ap-\nplying it to face veri\ufb01cation. In particular, the represent-\nation is learned on Youtube-Face dataset and evaluated on\nLFW dataset [23] under the restricted protocol. For train-\ning, we randomly choose about 10k, 20k, 30k, 50k, 100k\nsamples from YTF dataset. All these subsets have 1446 cat-\negories. We implement our approach to train CNN model\nand cluster images on the training set. Then, we remove the\nL2-normalization layer and append a softmax layer to \ufb01ne-\ntune our unsupervised CNN model based on the predicted\nimage cluster labels. Using the same training samples and\nCNN architecture, we also train a CNN model with a soft-\nmax loss supervised by the groundtruth labels of the training\nset. According to the evaluation protocol in [23], we run 10-\nfold cross-validation. The cosine similarity is used to com-\npute the similarity between samples. In each of 10 cross-\nvalidations, nine folds are used to \ufb01nd the optimal threshold,\nand the remaining one fold is used for evaluation. The av-\nerage accuracy is reported in Table. 7. As shown, though\nno groundtruth labels are used for representation learning\nin our approach, we obtain analogous performance to the\nsupervised learning approach. Our approach even (slightly)\nbeats the supervised learning method in one case.\n4.3. Image Classi\ufb01cation\nRecently, unsupervised representation learning methods\nare starting to achieve promising results for a variety of re-\ncognition tasks [5, 4, 26, 36]. We are interested in know-\ning whether the proposed method can also learn useful rep-\nresentation for image classi\ufb01cation. We experiment with\nCIFAR-10 [28]. We follow the pipeline in [5], and base\nour experiments on their publicly available code. In this\npipeline, codebook with 1600 codes is build upon 6 \u00d7 6\nTable 8: Image classi\ufb01cation accuracy on CIFAR-10.\n#Samples\nK-means [5]\nconv1\nconv2\nconv1&2\n5k\n62.81%\n63.05%\n63.10%\n63.50%\n10k\n68.01%\n68.30%\n68.46%\n69.11%\n25k\n74.01%\n72.83%\n72.93%\n75.11%\n50k (full set)\n76.59%\n74.68%\n74.68%\n78.55%\nZCA-whitened image patches, and then used to code the\ntraining and testing samples by extracting 1,600-d feature\nfrom each of 4 image quadrants. Afterwards, a linear SVM\n[6] is applied for image classi\ufb01cation on 6,400-d feature. In\nour approach, the only difference is that we learn a new rep-\nresentation from 6 \u00d7 6 patches, and then use these new rep-\nresentations to build the codebook with 1,600 codes. The\nCNN architecture we use contains two convolutional lay-\ners, each of which is combined with a ReLu and a pooling\nlayer, followed by an inner product layer. Both convolu-\ntional layers have 50 3 \u00d7 3 \ufb01lters with pad = 1. The kernel\nsize of pooling layer is 2, and the stride is 2. To save on\ntraining time, 40k randomly extracted patches are extracted\nfrom 50k training set and used in all the experiments.\nClassi\ufb01cation accuracies on test set with different set-\ntings are shown in Table 8. We vary the number of train-\ning samples and evaluate the performance for representa-\ntions from different layers. As we can see, the combination\nof representations from the \ufb01rst and second convolutional\nlayer achieve the best performance. We also use the rep-\nresentation output by inner product layer to learn the code-\nbook. However, it performs poorly. A possible reason is\nthat it discards spatial information of image patches, which\nmay be important for learning a codebook. When using\n400k randomly extracted patches to learn the codebook, [5]\nachieved 77.9%. However, it is still lower than what we\nachieved. This performance also beats several other meth-\nods listed in [4, 16, 26, 36].\n5. Conclusion\nIn this paper, we have proposed an approach to jointly\nlearn deep representations and image clusters. In our ap-\nproach, we combined agglomerative clustering with CNNs\nand formulate them as a recurrent process. We used a par-\ntially unrolling strategy to divide the timesteps into mul-\ntiple periods. In each period, we merged clusters step by\nstep during the forward pass and learned representation in\nthe backward pass, which are guided by a single weighted\ntriplet-loss function. The extensive experiments on image\nclustering, deep representation transfer learning and im-\nage classi\ufb01cation demonstrate that our approach can obtain\nmore precise image clusters and discriminative representa-\ntions that generalize well across many datasets and tasks.\n8\n6. Acknowledgements\nThis work was supported in part by the Paul G. Al-\nlen Family Foundation, Google, and Institute for Critical\nTechnology and Applied Science (ICTAS) at Virginia Tech\nthrough awards to D. P.; and by a National Science Founda-\ntion CAREER award, an Army Research Of\ufb01ce YIP award,\nan Of\ufb01ce of Naval Research grant, an AWS in Education\nResearch Grant, and GPU support by NVIDIA to D. B. The\nviews and conclusions contained herein are those of the au-\nthors and should not be interpreted as necessarily represent-\ning the of\ufb01cial policies or endorsements, either expressed or\nimplied, of the U.S. Government or any sponsor.\nReferences\n[1] D. Cai, X. He, X. Wang, H. Bao, and J. Han.\nLocal-\nity preserving nonnegative matrix factorization. In IJCAI,\nvolume 9, pages 1010\u20131015, 2009. 2, 5, 7, 15\n[2] G. Chen. Deep learning with nonparametric clustering. arXiv\npreprint arXiv:1501.03084, 2015. 2\n[3] X. Chen and D. Cai. Large scale spectral clustering with\nlandmark-based representation. In AAAI, 2011. 5, 7, 14, 15,\n16\n[4] A. Coates and A. Y. Ng. Selecting receptive \ufb01elds in deep\nnetworks. In NIPS, pages 2528\u20132536, 2011. 8\n[5] A. Coates, A. Y. Ng, and H. Lee. An analysis of single-\nlayer networks in unsupervised feature learning. In Inter-\nnational conference on arti\ufb01cial intelligence and statistics,\npages 215\u2013223, 2011. 8\n[6] C. Cortes and V. Vapnik. Support-vector networks. Machine\nlearning, 20(3):273\u2013297, 1995. 8\n[7] N. Dalal and B. Triggs. Histograms of oriented gradients for\nhuman detection. In CVPR, volume 1, pages 886\u2013893. IEEE,\n2005. 14\n[8] A. P. Dempster, N. M. Laird, and D. B. Rubin.\nMax-\nimum likelihood from incomplete data via the em algorithm.\nJournal of the royal statistical society. Series B (methodolo-\ngical), pages 1\u201338, 1977. 2\n[9] C. Ding, T. Li, M. Jordan, et al.\nConvex and semi-\nnonnegative matrix factorizations. IEEE TPAMI, 32(1):45\u2013\n55, 2010. 2\n[10] C. Doersch, A. Gupta, and A. A. Efros. Mid-level visual\nelement discovery as discriminative mode seeking. In NIPS,\npages 494\u2013502, 2013. 2\n[11] C. Doersch, A. Gupta, and A. A. Efros. Unsupervised visual\nrepresentation learning by context prediction.\nIn ICCV,\npages 1422\u20131430, 2015. 2\n[12] A. Dosovitskiy, J. T. Springenberg, M. Riedmiller, and\nT. Brox. Discriminative unsupervised feature learning with\nconvolutional neural networks.\nIn NIPS, pages 766\u2013774,\n2014. 2\n[13] Y. Gdalyahu, D. Weinshall, and M. Werman.\nSelf-\norganization in vision: stochastic clustering for image seg-\nmentation, perceptual grouping, and image database organ-\nization. IEEE TPAMI, 23(10):1053\u20131074, 2001. 2\n[14] R. Girshick. Fast r-cnn. In ICCV, pages 1440\u20131448, 2015. 2\n[15] R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich fea-\nture hierarchies for accurate object detection and semantic\nsegmentation. In CVPR, pages 580\u2013587. IEEE, 2014. 2\n[16] I. J. Goodfellow, A. Courville, and Y. Bengio. Spike-and-\nslab sparse coding for unsupervised feature discovery. arXiv\npreprint arXiv:1201.3382, 2012. 8\n[17] K. C. Gowda and G. Krishna. Agglomerative clustering us-\ning the concept of mutual nearest neighbourhood. Pattern\nrecognition, 10(2):105\u2013112, 1978. 1, 2, 3\n[18] D. B. Graham and N. M. Allinson. Characterising virtual\neigensignatures for general purpose face recognition. In Face\nRecognition, pages 446\u2013456. Springer, 1998. 5\n[19] D. Han and J. Kim. Unsupervised simultaneous orthogonal\nbasis clustering feature selection.\nIn IEEE CVPR, pages\n5016\u20135023, 2015. 2\n[20] B. Hariharan, J. Malik, and D. Ramanan. Discriminative de-\ncorrelation for clustering and classi\ufb01cation. In ECCV, pages\n459\u2013472. Springer, 2012. 2\n[21] K. He, X. Zhang, S. Ren, and J. Sun. Spatial pyramid pooling\nin deep convolutional networks for visual recognition.\nIn\nECCV, pages 346\u2013361. Springer, 2014. 2\n[22] G. E. Hinton and R. R. Salakhutdinov.\nReducing the\ndimensionality of data with neural networks.\nScience,\n313(5786):504\u2013507, 2006. 2, 17\n[23] G. B. Huang, M. Ramesh, T. Berg, and E. Learned-Miller.\nLabeled faces in the wild: A database for studying face re-\ncognition in unconstrained environments. Technical report,\nTechnical Report 07-49, University of Massachusetts, Amh-\nerst, 2007. 8\n[24] H.-C. Huang, Y.-Y. Chuang, and C.-S. Chen. Af\ufb01nity ag-\ngregation for spectral clustering. In CVPR, pages 773\u2013780.\nIEEE, 2012. 2\n[25] A. K. Jain, M. N. Murty, and P. J. Flynn. Data clustering:\na review. ACM computing surveys (CSUR), 31(3):264\u2013323,\n1999. 2, 3, 5, 7, 15\n[26] Y. Jia, C. Huang, and T. Darrell. Beyond spatial pyramids:\nReceptive \ufb01eld learning for pooled image features. In CVPR,\npages 3370\u20133377. IEEE, 2012. 8\n[27] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Gir-\nshick, S. Guadarrama, and T. Darrell. Caffe: Convolutional\narchitecture for fast feature embedding. In Proceedings of\nthe ACM International Conference on Multimedia, pages\n675\u2013678. ACM, 2014. 6\n[28] A. Krizhevsky and G. Hinton. Learning multiple layers of\nfeatures from tiny images, 2009. 8\n[29] A. Krizhevsky and G. E. Hinton. Using very deep autoen-\ncoders for content-based image retrieval. In ESANN. Cite-\nseer, 2011. 2\n[30] A. Krizhevsky, I. Sutskever, and G. E. Hinton.\nImagenet\nclassi\ufb01cation with deep convolutional neural networks. In\nNIPS, pages 1097\u20131105, 2012. 2\n[31] T. Kurita. An ef\ufb01cient agglomerative clustering algorithm\nusing a heap. Pattern Recognition, 24(3):205\u2013209, 1991. 2,\n3\n[32] S. Lazebnik, C. Schmid, and J. Ponce.\nBeyond bags of\nfeatures: Spatial pyramid matching for recognizing natural\nscene categories.\nIn CVPR, volume 2, pages 2169\u20132178.\nIEEE, 2006. 14\n9\n[33] Q. V. Le. Building high-level features using large scale un-\nsupervised learning. In ICASSP, pages 8595\u20138598. IEEE,\n2013. 2\n[34] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-\nbased learning applied to document recognition. Proceed-\nings of the IEEE, 86(11):2278\u20132324, 1998. 1, 5\n[35] H. Lee, R. Grosse, R. Ranganath, and A. Y. Ng. Convolu-\ntional deep belief networks for scalable unsupervised learn-\ning of hierarchical representations. In ICML, pages 609\u2013616.\nACM, 2009. 2\n[36] T.-H. Lin and H. Kung. Stable and ef\ufb01cient representation\nlearning with nonnegativity constraints.\nIn ICML, pages\n1323\u20131331, 2014. 8\n[37] D. G. Lowe. Object recognition from local scale-invariant\nfeatures. In ICCV, volume 2, pages 1150\u20131157. IEEE, 1999.\n2\n[38] L. Maaten. Learning a parametric embedding by preserving\nlocal structure. In International Conference on Arti\ufb01cial In-\ntelligence and Statistics, pages 384\u2013391, 2009. 16, 17\n[39] J. MacQueen et al. Some methods for classi\ufb01cation and ana-\nlysis of multivariate observations. In Proceedings of the \ufb01fth\nBerkeley symposium on mathematical statistics and probab-\nility, volume 1, pages 281\u2013297. Oakland, CA, USA., 1967.\n2, 5, 7, 15\n[40] G. McLachlan and D. Peel. Finite mixture models. John\nWiley & Sons, 2004. 2\n[41] J. F. Navarro, C. S. Frenk, and S. D. White. A universal\ndensity pro\ufb01le from hierarchical clustering. The Astrophys-\nical Journal, 490(2):493, 1997. 3\n[42] S. A. Nene, S. K. Nayar, H. Murase, et al. Columbia object\nimage library (coil-20). Technical report, Technical Report\nCUCS-005-96, 1996. 5\n[43] A. Y. Ng, M. I. Jordan, Y. Weiss, et al. On spectral clustering:\nAnalysis and an algorithm. NIPS, 2:849\u2013856, 2002. 2, 5, 7,\n15\n[44] T. Ojala, M. Pietik\u00a8ainen, and D. Harwood. A comparative\nstudy of texture measures with classi\ufb01cation based on fea-\ntured distributions. Pattern recognition, 29(1):51\u201359, 1996.\n14\n[45] M. A. Ranzato, F. J. Huang, Y.-L. Boureau, and Y. LeCun.\nUnsupervised learning of invariant feature hierarchies with\napplications to object recognition.\nIn CVPR, pages 1\u20138.\nIEEE, 2007. 2\n[46] K. Rematas, B. Fernando, F. Dellaert, and T. Tuytelaars.\nDataset \ufb01ngerprints: Exploring image collections through\ndata mining. In IEEE CVPR, pages 4867\u20134875, 2015. 2\n[47] S. Ren, K. He, R. Girshick, and J. Sun. Faster r-cnn: Towards\nreal-time object detection with region proposal networks. In\nNIPS, pages 91\u201399, 2015. 2\n[48] S. Roweis, G. Hinton, and R. Salakhutdinov.\nNeighbour-\nhood component analysis. Advances in Neural Information\nProcessing Systems (NIPS), 17:513\u2013520, 2004. 16, 17\n[49] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh,\nS. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein,\net al.\nImagenet large scale visual recognition challenge.\nIJCV, pages 1\u201342, 2014. 2\n[50] F. Schroff, D. Kalenichenko, and J. Philbin. Facenet: A uni-\n\ufb01ed embedding for face recognition and clustering. In CVPR,\npages 815\u2013823, 2015. 5, 14\n[51] P. Sermanet, D. Eigen, X. Zhang, M. Mathieu, R. Fergus,\nand Y. LeCun. Overfeat: Integrated recognition, localization\nand detection using convolutional networks. arXiv preprint\narXiv:1312.6229, 2013. 2\n[52] J. Shi and J. Malik. Normalized cuts and image segmenta-\ntion. IEEE TPAMI, 22(8):888\u2013905, 2000. 2, 5, 7, 14, 15,\n16\n[53] T. Sim, S. Baker, and M. Bsat. The cmu pose, illumination,\nand expression (pie) database. In FG, pages 46\u201351. IEEE,\n2002. 5\n[54] K. Simonyan and A. Zisserman. Very deep convolutional\nnetworks for large-scale image recognition. arXiv preprint\narXiv:1409.1556, 2014. 2\n[55] S. Singh, A. Gupta, and A. Efros. Unsupervised discovery of\nmid-level discriminative patches. ECCV, pages 73\u201386, 2012.\n2\n[56] F. Tian, B. Gao, Q. Cui, E. Chen, and T.-Y. Liu. Learning\ndeep representations for graph clustering. In AAAI, pages\n1293\u20131299, 2014. 2\n[57] G. Trigeorgis, K. Bousmalis, S. Zafeiriou, and B. Schuller.\nA deep semi-nmf model for learning hidden representations.\nIn ICML, pages 1692\u20131700, 2014. 2, 5, 7\n[58] P. Vincent, H. Larochelle, Y. Bengio, and P.-A. Manzagol.\nExtracting and composing robust features with denoising au-\ntoencoders. In ICML, pages 1096\u20131103. ACM, 2008. 2\n[59] J. Wang, Y. Song, T. Leung, C. Rosenberg, J. Wang, J. Phil-\nbin, B. Chen, and Y. Wu. Learning \ufb01ne-grained image simil-\narity with deep ranking. In CVPR, pages 1386\u20131393. IEEE,\n2014. 5, 14\n[60] X. Wang and A. Gupta. Unsupervised learning of visual rep-\nresentations using videos. In ICCV, pages 2794\u20132802, 2015.\n2\n[61] Z. Wang, S. Chang, J. Zhou, and T. S. Huang. Learning a\ntask-speci\ufb01c deep architecture for clustering. In arXiv pre-\nprint arXiv:1509.00151, 2015. 2, 5, 7\n[62] S. Wold, K. Esbensen, and P. Geladi. Principal component\nanalysis. Chemometrics and intelligent laboratory systems,\n2(1-3):37\u201352, 1987. 16, 17\n[63] L. Wolf, T. Hassner, and I. Maoz. Face recognition in un-\nconstrained videos with matched background similarity. In\nCVPR, pages 529\u2013534. IEEE, 2011. 5\n[64] P. Xie and E. Xing. Integrating image clustering and code-\nbook learning. In AAAI, 2015. 2\n[65] W. Xu, X. Liu, and Y. Gong. Document clustering based\non non-negative matrix factorization. In Proceedings of the\n26th annual international ACM SIGIR conference on Re-\nsearch and development in informaion retrieval, pages 267\u2013\n273. ACM, 2003. 5\n[66] S. Zafeiriou and M. Petrou. Nonlinear non-negative compon-\nent analysis algorithms. IEEE TIP, 19(4):1050\u20131066, 2010.\n2\n[67] L. Zelnik-Manor and P. Perona. Self-tuning spectral cluster-\ning. In NIPS, pages 1601\u20131608, 2004. 2, 5, 7, 15\n10\n[68] W. Zhang, X. Wang, D. Zhao, and X. Tang. Graph degree\nlinkage: Agglomerative clustering on a directed graph. In\nECCV, pages 428\u2013441. Springer, 2012. 3, 5, 6, 7, 11, 15\n[69] W. Zhang, D. Zhao, and X. Wang. Agglomerative clustering\nvia maximum incremental path integral. Pattern Recogni-\ntion, 46(11):3056\u20133065, 2013. 5, 7, 14, 15, 16\n[70] D. Zhao and X. Tang. Cyclizing clusters via zeta function of\na graph. In NIPS, pages 1953\u20131960, 2009. 3, 5, 7, 15\nA. Appendix\nA.1. Af\ufb01nity Measure for Clusters\nIn this paper, we employ the af\ufb01nity measure in [68]\nA(Ci, Cj) = A(Cj \u2192Ci) + A(Ci \u2192Cj)\n=\n1\n|Ci|2 1T\n|Ci|WCi,CjWCj,Ci1|Ci|\n+\n1\n|Cj|2 1T\n|Cj|WCj,CiWCi,Cj1|Cj|\n(12)\nwhere W is the af\ufb01nity matrix for samples, and WCi,Cj \u2208\nR|Ci|\u00d7|Cj| is the submatrix in W pointing from samples in\nCi to samples in Cj, and WCj,Ci \u2208R|Cj|\u00d7|Ci| is the one\npointing from Cj to Ci. 1|Ci| and 1|Cj| are two vectors with\nall |Ci| and |Cj| elements be 1, respectively. Therefore, we\nhave A(Ci, Cj) = A(Cj, Ci).\nAccording to (12), we can derive\nA((Cm \u222aCn) \u2192Ci) = A(Cm \u2192Ci)+A(Cn \u2192Ci) (13)\nwhich has also been shown in [68]. Meanwhile,\nA(Ci \u2192(Cm \u222aCn))\n= \u03b21T\n|Cm|+|Cn|WCm\u222aCn,CiWCi,Cm\u222aCn1|Cm|+|Cn|\n= \u03b21T\n|Cm|WCm,CiWCi,Cm1|Cm| + \u03b21T\n|Cn|WCn,CiWCi,Cn1|Cn|\n+ \u03b21T\n|Cm|WCm,CiWCi,Cn1|Cn| + \u03b21T\n|Cn|WCn,CiWCi,Cm1|Cm|\n(14)\nwhere \u03b2 = 1/(|Cm| + |Cn|)2.\nA.2. Approximated Af\ufb01nity Measure\nDuring agglomerative clustering, we need to re-compute\nthe af\ufb01nity between the merged cluster to all other clusters\nbased on 13 and 14 repeatedly. It is simple to compute 13.\nHowever, to get A(Ci \u2192(Cm \u222aCn)), we need a lot of com-\nputations. These time costs become dominant and remark-\nable when we have a large-scale dataset. To accelerate the\ncomputations, we introduce an approximation method. At\nthe right side of (14), we assume samples in Cm and Cn have\nsimilar af\ufb01nities to Ci. This assumption is mild because the\ncondition to merge Cm and Cn is that they are similar to\neach other. In this case, the ratio between WCi,Cm1|Cm| and\nWCi,Cn1|Cn| is analogy to the ratio between the number of\nsamples in two set, i.e.,\nWCi,Cm1|Cm| = |Cm|\n|Cn| WCi,Cn1|Cn|\n(15)\nBased on (15), we have\n1T\n|Cm|WCm,CiWCi,Cn1|Cn| = |Cn|\n|Cm|1T\n|Cm|WCm,CiWCi,Cm1|Cm|\n(16a)\n1T\n|Cn|WCn,CiWCi,Cm1|Cm| = |Cm|\n|Cn| 1T\n|Cn|WCn,CiWCi,Cn1|Cn|\n(16b)\nAs a result, we can re-formulate (14) to\nA(Ci \u2192(Cm \u222aCn))\n=\n1\n(|Cm|2 + |Cm||Cn|)1T\n|Cm|WCm,CiWCi,Cm1|Cm|\n+\n1\n(|Cm||Cn| + |Cn|2)1T\n|Cn|WCn,CiWCi,Cn1|Cn|\n(17)\nTherefore, we have\nA(Ci \u2192(Cm \u222aCn)) =\n|Cm|\n|Cm| + |Cn|A(Ci \u2192Cm)\n+\n|Cn|\n|Cm| + |Cn|A(Ci \u2192Cn)\n(18)\nConsequently, we have\nA(Cm \u222aCn, Ci) = A(Cm \u2192Ci) + A(Cn \u2192Ci)\n+\n|Cm|\n|Cm| + |Cn|A(Ci \u2192Cm)\n+\n|Cn|\n|Cm| + |Cn|A(Ci \u2192Cn)\n(19)\nAbove approximation provides us a potential way to re-\nduce the computational complexity of agglomerative clus-\ntering. Though we computed A(Ci \u2192(C \u222aCn)) based\non Eq. (14) in all our experiments, we found the approx-\nimation version achieves analogy performance while costs\nmuch less time than the original one. We further simplify\nthe computation by assuming a constant ratio \u03b1 between the\nterms at the right side of Eq. (14):\n1T\n|Cm|WCm,CiWCi,Cn1|Cn| = \u03b11T\n|Cm|WCm,CiWCi,Cm1|Cm|\n(20a)\n1T\n|Cn|WCn,CiWCi,Cm1|Cm| = \u03b11T\n|Cn|WCn,CiWCi,Cn1|Cn|\n(20b)\nBased on above assumption,\nA(Cm \u222aCn, Ci) = A(Cm \u2192Ci) + A(Cn \u2192Ci)\n+ (1 + \u03b1)|Cm|2\n(|Cm| + |Cn|)2 A(Ci \u2192Cm)\n+ (1 + \u03b1)|Cn|2\n(|Cm| + |Cn|)2 A(Ci \u2192Cn)\n(21)\n11\nFigure 4: Performance of agglomerative clustering with approximations. Left one is NMI metric, and right one is AC metric.\nThe \ufb01rst column is without acceleration. For the other columns from left to right, \u03b1 = {\u22120.2, \u22120.1, 0, 0.1, 0.2, 0.3, 0.5}.\nFigure 5:\nTime cost for different values of \u03b1.\nThe\n\ufb01rst\ncolumn\nis\nthe\ntime\ncost\nwithout\nacceleration.\nFor\nthe\nother\ncolumns\nfrom\nleft\nto\nright,\n\u03b1\n=\n{\u22120.2, \u22120.1, 0, 0.1, 0.2, 0.3, 0.5}.\nWe\ntest\nvarious\nvalues\nfor\n\u03b1,\nwhich\nare\n{\u22120.2, \u22120.1, 0, 0.1, 0.2, 0.3, 0.5}. We show the quantitat-\nive comparison in Fig. 4. We use image intensities as input\nto rule out all random factors. The original AC-GDL al-\ngorithm is used as the baseline. By conducting experiments\non various datasets, we \ufb01nd a valid range [\u22120.2, 0.1] for \u03b1\nwhich helps achieve analogous or even better performance\nto the one without acceleration.\nThese results indicate\nthat we may do not need to compute the explicit value of\naf\ufb01nities to obtain equivalent level performance.\nAlso,\nto measure how much time we can save by using our\napproximation, we compare the time cost between original\nAC-GDL algorithm and accelerated one in Fig. 5.\nIt is\nclear that our approximation algorithm has much lower\ncomputational complexity.\nA.3. Cluster-based to Sample-based Loss\na\nb\nc\nd\ne\nf\ng\nh\ni\nj\nl\nn\nk\nm\nt=1\nt=2\nt=3\nt=4\nt=5\nt=6\nFigure 6: A illustration of agglomerative clustering.\nIn this part, we explain how to convert cluster-based loss\nto sample-based loss. Because it depends on speci\ufb01c ag-\nglomerative clustering processes, we use a toy example in\nFig. 6 for illustration. We set Kc be 2 for simplicity. In\nFig. 6, there are six time steps, and thus T = 6. We assume\nthey are in a single partial unrolled period. The leaf nodes\nrepresent single samples. For simplicity, we omit\n\u03bb\nKc\u22121 in\n(10), obtaining the overall loss\nL(\u03b8|Y\u2217, I) = \u2212\n6\nX\nt=1\n\u0010\n\u03bb\u2032A(Ct\n\u2217, N Kc\nCt\u2217[1]) \u2212A(Ct\n\u2217, N Kc\nCt\u2217[2])\n\u0011\n(22)\nGiven above loss function, we decompose it from \ufb01rst\ntime step (t = 1) to the most recent time step (t = 6):\n\u2022 t=1: C1\n\u2217= Ca, N 2\nC1\u2217[1] = Cb and N 2\nC1\u2217[2] = Cc. We have\nL(\u03b8|y1\n\u2217, I) = \u2212(\u03bb\u2032A(Ca, Cb) \u2212A(Ca, Cc))\n(23)\nClearly, above is sample-based weighted triplet loss\n12\nfunction, where samples Ca and Cb are positive pair\nand Ca and Cc are negative pair.\n\u2022 t=2: C2\n\u2217= Ci, N 2\nC2\u2217[1] = Cc and N 2\nC2\u2217[2] = Cd. We have\nL(\u03b8|{y1\n\u2217, y2\n\u2217}, I) = L(\u03b8|y1\n\u2217, I)\n\u2212(\u03bb\u2032A(Ci, Cc) \u2212A(Ci, Cd)) (24)\nSince Ci = Ca \u222aCb, we base on Eq. (19) for approxim-\nation\nA(Ci, Cc) = A(Ca \u2192Cc) + A(Cb \u2192Cc)\n+ 1\n2A(Cc \u2192Ca) + 1\n2A(Cc \u2192Cb)\n(25)\nA(Ci, Cd) = A(Ca \u2192Cd) + A(Cb \u2192Cd)\n+ 1\n2A(Cd \u2192Ca) + 1\n2A(Cd \u2192Cb)\n(26)\nThus,\nL(\u03b8|{y1\n\u2217, y2\n\u2217}, I)\n= \u2212\u03bb\u2032A(Ca, Cb) \u2212(\u03bb\u2032 \u22121)A(Ca \u2192Cc) \u2212\u03bb\u2032A(Cb \u2192Cc)\n\u2212(\u03bb\u2032\n2 \u22121)A(Cc \u2192Ca) \u2212\u03bb\u2032\n2 A(Cc \u2192Cb)\n+ A(Ca \u2192Cd) + A(Cb \u2192Cd)\n+ 1\n2A(Cd \u2192Ca) + 1\n2A(Cd \u2192Cb)\n(27)\nAt current time step, sample a, b and c belong to the\nsame cluster Cl, while sample d is from another cluster.\n(27) computes the sample-based weighted triplet loss\nfor samples in Cl and sample d. Except for Cl, the other\nclusters all have merely one sample. No need to com-\npute triplet loss for them. It should be pointed out that\n\u03bb\u2032 in above loss function should be not less than 2 so\nthat the af\ufb01nities for all pairs in Cl are enlarged.\n\u2022 t=3: C3\n\u2217= Cd, N 2\nC3\u2217[1] = Ce and N 2\nC3\u2217[2] = Cf. We\nhave\nL(\u03b8|{y1\n\u2217, y2\n\u2217, y3\n\u2217}, I) = L(\u03b8|{y1\n\u2217, y2\n\u2217}, I)\n\u2212\u03bb\u2032 (A(Cd, Ce) \u2212A(Cd, Cf))\n(28)\nBesides the loss L(\u03b8|{y1\n\u2217, y2\n\u2217}, I) for Cl, we also com-\npute the loss for Cj in (28) because it contains two\nsamples, d and e.\n\u2022 t=4: C4\n\u2217= Cf, N 2\nC4\u2217[1] = Cg and N 2\nC4\u2217[2] = Ch. We\nhave\nL(\u03b8|{y1\n\u2217, ..., y4\n\u2217}, I) = L(\u03b8|{y1\n\u2217, y2\n\u2217, y3\n\u2217}, I)\n\u2212(\u03bb\u2032A(Cf, Cg) \u2212A(Cf, Ch))\n(29)\nHere, we additionally compute the weighted triplet\nloss for cluster Ck since it contains two samples.\n\u2022 t=5: C5\n\u2217= Ck, N 2\nC5\u2217[1] = Ch and N 2\nC5\u2217[2] = Cj. We\nhave\nL(\u03b8|{y1\n\u2217, ..., y5\n\u2217}, I)\n= L(\u03b8|{y1\n\u2217, ..., y4\n\u2217}, I) \u2212(\u03bb\u2032A(Ck, Ch) \u2212A(Ck, Cj))\n(30)\nBecause Ck = Cf \u222aCg, we have\nA(Ck, Ch) = A(Cf \u2192Ch) + A(Cg \u2192Ch)\n+ 1\n2A(Ch \u2192Cf) + 1\n2A(Ch \u2192Cg)\n(31)\nA(Ck, Cj) = A(Cf \u2192Cj) + A(Cg \u2192Cj)\n+ 1\n2A(Cj \u2192Cf) + 1\n2A(Cj \u2192Cg)\n(32)\nSince Cj = Cd \u222aCe, we further transform above equa-\ntion to\nA(Ck, Cj) = 1\n2A(Cf \u2192Cd) + 1\n2A(Cf \u2192Ce)\n+ 1\n2A(Cg \u2192Cd) + 1\n2A(Cg \u2192Ce)\n+ 1\n2A(Cd \u2192Cf) + 1\n2A(Ce \u2192Cf)\n+ 1\n2A(Cd \u2192Cg) + 1\n2A(Ce \u2192Cg)\n(33)\nSimilar to the relation between sample a and c at time\nsteps t = 1, 2, sample f and h belong to the same\ncluster Cm at current time step while they are from dif-\nferent clusters at time step t = 4. Based on the approx-\nimation, the terms A(Cf \u2192Ch) and A(Ch \u2192Cf) in\ntwo time steps will be merged. As a result, the \ufb01nal\nloss is computed on intra-cluster pairs and inter-cluster\npairs sampled from three clusters Cl, Cj and Cm.\n\u2022 t=6: C6\n\u2217= Cl, N 2\nC6\u2217[1] = Cj and N 2\nC6\u2217[2] = Cm. Thus\nL(\u03b8|{y1\n\u2217, ..., y6\n\u2217}, I) = L(\u03b8|{y1\n\u2217, ..., y5\n\u2217}, I)\n\u2212(\u03bb\u2032A(Cl, Cj) \u2212A(Cl, Cm))\n(34)\nSimilar to the decomposition procedures above, both\nA(Cl, Cj) and A(Cl, Cm) can be transformed to\nsample-based af\ufb01nities.\nBecause Cl and Cj are re-\ngarded as different clusters previously, sample pairs\nfrom both of them are with positive weights in the loss\nfunction. However, it will be diminished by positive\npairs (with negative weights) at current time step.\nThough we use a toy example to show that the cluster-\nbased loss can be transformed to sample-based loss above,\nthe reduction is general to any possible agglomerative clus-\ntering processes because the loss for clusters at high-level\n13\ncan always be decomposed to the losses on clusters at low-\nlevel until it reaches to single samples.\nThe difference\namong various processes lies on the different weights as-\nsociated with sample-based af\ufb01nities.\nWe should know\nthat sample pairs from the same cluster may be with pos-\nitive weights.\nOne way to avoid this is increase \u03bb\u2032.\nIn\nour implementation, we aim to increase af\ufb01nities between\nsamples from the same clusters, while decrease the af\ufb01nities\nbetween samples from different clusters. And the clusters\nare determined by cluster ids at current step. Therefore, we\nassign a consistent weight \u03b3 to any af\ufb01nities from the same\ncluster and 1 to any af\ufb01nities from different clusters. Be-\ncause we use SGD for batch optimization, the scales for\naf\ufb01nities do not affect much on the performance. It is the\nsigns affect much. Accordingly, at any given time step T,\nthe overall loss is approximated to\nL(\u03b8|yT\n\u2217, I) = \u2212\n\u03bb\nKc \u22121\nX\ni,j,k\n(\u03b3A(xi, xj) \u2212A(xi, xk))\n(35)\nNote that we replace Y\u2217in (35) by yT\n\u2217in (35) be-\ncause it is merely determined by current yT , regardless\nof {y1\n\u2217, ..., yT \u22121\n\u2217\n}. As a result, we do not need to record\n{y1\n\u2217, ..., yT \u22121\n\u2217\n}. This simpli\ufb01es the batch optimization for\nCNN. Concretely, given a sample xi, we randomly select\na sample xj which belongs to the same cluster, while se-\nlect neighbours of xi that from other clusters to be xk. To\nomit the case that A(xi, xj) is much larger than A(xi, xk),\nwe also add a margin threshold like the triplet loss function\nused in [59, 50].\nA.4. Detailed CNN Architectures in our Paper\nIn this paper, the CNN architectures vary from dataset\nto dataset. As we mentioned in the main paper, we stacked\ndifferent number of layers for different datasets so that the\nsize of most top layer response map is about 10\u00d710. In\nTable 9, we list the architectures for the datasets used in our\npaper. \u201dconv\u201d means convolutional layer. \u201dbn\u201d means batch\nnormalization layer. \u201dwt-loss\u201d means weighted triplet loss\nlayer. \u2713means the layer is used, while \u2212means the layer\nis not used.\nA.5. Performance Evaluated by Accuracy\nIn this section, we evaluate the performance of differ-\nent algorithms based on clustering accuracy (AC) metric,\nas a supplement to the NMI metric used in our main pa-\nper. As we can see from table 10, the proposed method\noutperform other methods on all datasets, which has sim-\nilar trend as evaluated using NMI. Meanwhile, according\nto table 11, all other clustering algorithms are boosted after\nusing the learned representation as evaluated on AC. These\nresults further prove the proposed method is superior to\nother clustering algorithms and also learns powerful deep\nrepresentations that generalize well across different cluster-\ning algorithms.\nA.6. Robustness Analysis\nWe choose the two most important parameters: unfold-\ning rate \u03b7 and Ks for evaluating the robustness of our ap-\nproach to variations in these parameters. In these experi-\nments, we set all the other parameters except for the target\none to default values listed in Table 2 in the main paper. As\nwe can see from Fig. 7, when the unfolding rate increases,\nthe performance is not affected much for most of the data-\nsets. For Ks, the performance is stable when Ks <= 50\nfor all datasets. It drops with larger values of Ks for a few\ndatasets. Increasing Ks also result in similar degradation\nin the agglomerative clustering algorithms we compare to.\nThis suggests that Ks should not be set to very large value\nin general.\nA.7. Reliability Analysis\nWe evaluate the reliability by measuring the purity of\nsamples at the beginning of our algorithm. Because we use\nagglomerative clustering, there are very few samples in each\ncluster at the beginning (average is about 4 in our experi-\nments). Most samples in the same cluster tend to belong\nto the same category. Quantitatively, for each sample in a\ndataset, we count the number of samples (Km) that belong\nto the same category within its K nearest neighbours, and\nthen compute the precision Km/K for it. In Fig. 8, we re-\nport the average precision across all samples. As we can\nsee, based on raw image data, all datasets have high ratios\nwhen K is smaller, and the ratios increase further when us-\ning our learned deep representations. Consequently, when\nK is small, the pseudo-labels are reliable enough to learn\nplausible deep representations.\nA.8. Clustering based on Hand-Crafted Features\nWe also evaluate the performance of clustering based on\nimage features, instead of image intensities.\nWe choose\nthree different types of datasets for testing:\nCOIL100,\nMNIST-test and UMist, and three types of clustering al-\ngorithms including SC-LS [3], N-Cuts [52] and AC-PIC\n[69] for comparison since their better performance among\nall the algorithms. For these three datasets, we use spa-\ntial pyramid descriptor [32]4, histogram of oriented gradient\n(HOG) [7]5 and local binary pattern (LBP) [44] for repres-\nentation, respectively. We report the results in Table 12. \u2193\nmeans performance become worse, and \u2191means it become\nbetter.\nAlmost all algorithms perform worse than using\noriginal image as input. It indicates hand-crafted features\n4http://slazebni.cs.illinois.edu/research/\nSpatialPyramid.zip\n5http://www.robots.ox.ac.uk/\u02dcvgg/research/\ncaltech/phog.html\n14\nTable 9: CNN architectures for different datasets in our paper.\nDataset\nCOIL20\nCOIL100\nUSPS\nMNIST-test\nMNIST-full\nUMist\nFRGC\nCMU-PIE\nYTF\nconv1\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\nbn1\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\nrelu1\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\npool1\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\nconv2\n\u2713\n\u2713\n\u2212\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\nbn2\n\u2713\n\u2713\n\u2212\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\nrelu2\n\u2713\n\u2713\n\u2212\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\npool2\n\u2713\n\u2713\n\u2212\n\u2212\n\u2212\n\u2713\n\u2713\n\u2713\n\u2713\nconv3\n\u2713\n\u2713\n\u2212\n\u2212\n\u2212\n\u2713\n\u2212\n\u2212\n\u2212\nbn3\n\u2713\n\u2713\n\u2212\n\u2212\n\u2212\n\u2713\n\u2212\n\u2212\n\u2212\nrelu3\n\u2713\n\u2713\n\u2212\n\u2212\n\u2212\n\u2713\n\u2212\n\u2212\n\u2212\npool3\n\u2713\n\u2713\n\u2212\n\u2212\n\u2212\n\u2713\n\u2212\n\u2212\n\u2212\nconv4\n\u2713\n\u2713\n\u2212\n\u2212\n\u2212\n\u2212\n\u2212\n\u2212\n\u2212\nbn4\n\u2713\n\u2713\n\u2212\n\u2212\n\u2212\n\u2212\n\u2212\n\u2212\n\u2212\nrelu4\n\u2713\n\u2713\n\u2212\n\u2212\n\u2212\n\u2212\n\u2212\n\u2212\n\u2212\npool4\n\u2713\n\u2713\n\u2212\n\u2212\n\u2212\n\u2212\n\u2212\n\u2212\n\u2212\nip1\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\nl2-norm\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\nwt-loss\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\nTable 10: Quantitative clustering performance (AC) for different algorithms using image intensities as input.\nDataset\nCOIL20\nCOIL100\nUSPS\nMNIST-test\nMNIST-full\nUMist\nFRGC\nCMU-PIE\nYTF\nK-means [39]\n0.665\n0.580\n0.467\n0.560\n0.564\n0.419\n0.327\n0.246\n0.548\nSC-NJW [43]\n0.641\n0.544\n0.413\n0.220\n0.502\n0.551\n0.178\n0.255\n0.551\nSC-ST [67]\n0.417\n0.300\n0.308\n0.454\n0.311\n0.411\n0.358\n0.293\n0.290\nSC-LS [3]\n0.717\n0.609\n0.659\n0.740\n0.714\n0.568\n0.407\n0.549\n0.544\nN-Cuts [52]\n0.544\n0.577\n0.314\n0.304\n0.327\n0.550\n0.235\n0.155\n0.536\nAC-Link [25]\n0.251\n0.269\n0.421\n0.693\n0.657\n0.398\n0.175\n0.201\n0.547\nAC-Zell [70]\n0.867\n0.811\n0.575\n0.693\n0.112\n0.517\n0.266\n0.765\n0.519\nAC-GDL [68]\n0.865\n0.797\n0.867\n0.933\n0.113\n0.563\n0.266\n0.842\n0.430\nAC-PIC [69]\n0.855\n0.840\n0.855\n0.920\n0.115\n0.576\n0.320\n0.797\n0.472\nNMF-LP [1]\n0.621\n0.553\n0.522\n0.479\n0.471\n0.365\n0.259\n0.229\n0.546\nOURS-SF\n1.000\n0.894\n0.922\n0.940\n0.959\n0.809\n0.461\n0.980\n0.684\nOURS-RC\n1.000\n0.916\n0.950\n0.961\n0.964\n0.809\n0.461\n1.000\n0.684\nTable 11: Quantitative clustering performance (AC) for different algorithms using our learned representations as inputs.\nDataset\nCOIL20\nCOIL100\nUSPS\nMNIST-test\nMNIST-full\nUMist\nFRGC\nCMU-PIE\nYTF\nK-means [39]\n0.821\n0.751\n0.776\n0.957\n0.969\n0.761\n0.476\n0.834\n0.660\nSC-NJW [43]\n0.738\n0.659\n0.716\n0.868\n0.972\n0.707\n0.485\n0.776\n0.521\nSC-ST [67]\n0.851\n0.705\n0.661\n0.960\n0.958\n0.697\n0.496\n0.896\n0.575\nSC-LS [3]\n0.867\n0.735\n0.792\n0.960\n0.973\n0.733\n0.502\n0.802\n0.571\nN-Cuts [52]\n0.888\n0.626\n0.634\n0.959\n0.971\n0.798\n0.504\n0.981\n0.441\nAC-Link [25]\n0.678\n0.539\n0.773\n0.955\n0.964\n0.795\n0.495\n0.947\n0.602\nAC-Zell [70]\n1.000\n0.931\n0.879\n0.879\n0.969\n0.790\n0.449\n1.000\n0.644\nAC-GDL [68]\n1.000\n0.920\n0.949\n0.961\n0.878\n0.790\n0.461\n1.000\n0.677\nAC-PIC [69]\n1.000\n0.950\n0.955\n0.958\n0.882\n0.790\n0.438\n1.000\n0.652\nNMF-LP [1]\n0.769\n0.603\n0.778\n0.955\n0.970\n0.725\n0.481\n0.504\n0.575\n15\nFigure 7: Clustering performance (NMI) with different \u03b7 (left) and Ks (right).\nFigure 8: Average purity of K-nearest neighbour for varying values of K. Left is computed using raw image data, while right\nis computed using our learned representation.\nTable 12: Clustering performance (NMI) based on hand-\ncrafted features.\nDataset\nCOIL100\nMNIST-test\nUMist\nFRGC\nSC-LS [3]\n0.733\u2193\n0.625\u2193\n0.752\u2193\n0.338\u2193\nN-Cuts [52]\n0.722\u2193\n0.423\u2191\n0.420\u2193\n0.238\u2193\nAC-PIC [69]\n0.878\u2193\n0.735\u2193\n0.734\u2193\n0.322\u2193\nshould be designed dataset by dataset. In contrast, directly\nlearning from image intensities is more straightforward and\nalso achieves better performance.\nA.9. Visualizing Data in Low Dimension\nProjecting high-dimensional data into low-dimensional\nspace can help people to intuitively understand the data.\nThough the proposed method is aimed to learn deep rep-\nresentations and image clusters, we note that it can be nat-\nurally converted to a parametric visualization method for\nan image dataset by slightly alternating the objective. In-\nstead of updating the af\ufb01nities among samples based on the\nlearned representations gradually, we consistently use the\naf\ufb01nities among raw image data to perform the agglomer-\native cluster, which then guides representation learning in\nlow-dimensional space. By this way, we can obtain a low-\ndimensional space (2D or 3D) which can retain the structure\nof the original data.\nWe compare three dimension reduction techniques, prin-\nciple component analysis (PCA) [62], neighbourhood com-\n16\n(a) PCA.\n(b) Autoencoder.\n(c) Parametric t-SNE.\n(d) Visualization by our method.\nFigure 9: Visualization of 10,000 MNIST test samples in different embedding spaces.\nponents analysis (NCA) [48], and parametric t-SNE [38].\nThough both [38] and our visualization method are based\non neural networks, there are two main differences: 1) In\n[38], a Kullback-Leibler divergence between the joint dis-\ntributions of original data and the embedded data is con-\nsidered. However, in our method, we employ a weighted\ntriplet loss that directly takes the local structure of embed-\nded data into account; 2) In [38], the authors need to pre-\ntrain a stack of RBMs layer-by-layer, and then \ufb01ne-tune the\nneural network. Nevertheless, we directly train the neural\nnetwork from scratch end-to-end.\nWe perform experiments on MNIST dataset.\nIn\nMNIST, 60,000 training samples are used to learn the low-\ndimensional embedding space, and 10,000 test samples are\nused for evaluation. To train a D-dimensional embedding,\nwe \ufb01rst remove the normalization layer and then stack on\nthe top another linear layer whose dimension is D. To thor-\noughly explore the local structure in the original data, we\nmerge the clusters with a lower unfolding rate (\u03b7 = 0.2).\nThe learning process is stopped when the number of clusters\nreaches to 10. Though we stop the learning process as such,\nit should be noted that the stop criterion is not con\ufb01ned.\nFor quantitative analysis, we compute the nearest-neighbor\nclassi\ufb01cation error and trustworthiness as in [38].\nIn Table 13, we show the 1-nearest neighbour classi-\n\ufb01cation error on MNIST test dataset.\nWe copy the best\nresults of the compared methods from [38].\nAs we can\nsee, our method outperforms all three other methods across\nthree different embedding dimensions. These results illus-\ntrates that our method can obtain low-dimensional embed-\nding with better generalization ability.\nFor visualization, it is important to retain the original\ndata structure in the embedding space.\nFor quantitative\ncomparison, we report the trustworthiness of learned low-\ndimensional embedding in Table 14. Larger value means\nbetter preservation of original data structure. As we can\nsee, our method is not as good as parametric t-SNE. These\nresults are explainable. During training, we merely pay at-\ntention to the local structure among samples from different\nclusters, while omitting the relations among samples within\nTable 13: 1-nearest neighbor classi\ufb01cation error on low-\ndimensional embedding of MNIST dataset.\nMethod\n2D\n10D\n30D\nPCA [62]\n0.782\n0.430\n0.108\nNCA [48]\n0.568\n0.088\n0.073\nAutoencoder [22]\n0.668\n0.063\n0.027\nParam. t-SNE [38]\n0.099\n0.046\n0.027\nOURS\n0.067\n0.019\n0.027\nTable 14: Trustworthiness T(12) on low-dimensional em-\nbedding of MNIST dataset.\nMethod\n2D\n10D\n30D\nPCA [62]\n0.744\n0.991\n0.998\nNCA [48]\n0.721\n0.968\n0.971\nAutoencoder [22]\n0.729\n0.996\n0.999\nParam. t-SNE [38]\n0.927\n0.997\n0.999\nOurs\n0.768\n0.936\n0.975\none cluster.\nTherefore, the algorithm will learn embed-\ndings that discriminate clusters well but possibly disorder\nthe samples in each cluster. We believe this can be solved\nby introducing a loss to con\ufb01ne the within-cluster structure.\nWe leave this as a future work for limited space.\nA.10. Visualizing Learned Deep Representations\nWe show the \ufb01rst three principle components of learned\nrepresentations in Fig. 10 and Fig. 11 at different stages.\nFor comparison, we show the image intensities at the \ufb01rst\ncolumn. We use different colors for representing different\nclusters that we predict during the algorithm. At the bottom\nof each plot, we give the number of clusters at the corres-\nponding stage. At the \ufb01nal stage, the number of cluster is\nsame to the number of categories in the dataset. After a\nnumber of iterations, we can learn more discriminative rep-\nresentations for the datasets, and thus facilitate more precise\nclustering results.\n17\n(a) Initial stage (421)\n(b) Middle stage (42)\n(c) Final stage (20)\n(d) Initial stage (2162)\n(e) Middle stage (216)\n(f) Final stage (100)\n(g) Initial stage (2232)\n(h) Middle stage (22)\n(i) Final stage (10)\n(j) Initial stage (1762)\n(k) Middle stage (22)\n(l) Final stage (10)\n(m) Initial stage (11521)\n(n) Middle stage (115)\n(o) Final stage (10)\nFigure 10: Learned representations at different stages on \ufb01ve datasets. From top to bottom, they are COIL20, COIL100,\nUSPS and MNIST-test and MNIST-full. The \ufb01rst column are image intensities. For MNIST-test, we show another view point\ndifferent from Fig.1 in the main paper.\n18\n(a) Initial stage (188)\n(b) Middle stage (60)\n(c) Final stage (20)\n(d) Initial stage (775)\n(e) Middle stage (128)\n(f) Final stage (20)\n(g) Initial stage (775)\n(h) Middle stage (200)\n(i) Final stage (68)\n(j) Initial stage (2814)\n(k) Middle stage (300)\n(l) Final stage (41)\nFigure 11: Learned representations as different stages on four datasets. From top to bottom, they are UMist, FRGC, CMU-PIE\nand YTF. The \ufb01rst column are image intensities.\n19\n",
        "sentence": " , 2014) and clustering (Yang et al., 2016; Liao et al., 2016), but they are hard to scale and have only been tested on small datasets. Some of them are based on a clustering loss (Xie et al., 2016; Yang et al., 2016; Liao et al., 2016), but they are not tested at a scale comparable to that of supervised convnet training.",
        "context": "[35] H. Lee, R. Grosse, R. Ranganath, and A. Y. Ng. Convolu-\ntional deep belief networks for scalable unsupervised learn-\ning of hierarchical representations. In ICML, pages 609\u2013616.\nACM, 2009. 2\nimage cluster labels. Using the same training samples and\nCNN architecture, we also train a CNN model with a soft-\nmax loss supervised by the groundtruth labels of the training\nset. According to the evaluation protocol in [23], we run 10-\ntrained a task-speci\ufb01c deep architecture for clustering. The\ndeep architecture is composed of sparse coding modules\nwhich can be jointly trained through back propagation from\na cluster-oriented loss. However, they used sparse coding"
    },
    {
        "title": "Colorful image colorization",
        "author": [
            "R. Zhang",
            "P. Isola",
            "A. Efros"
        ],
        "venue": "In ECCV,",
        "citeRegEx": "Zhang et al\\.,? \\Q2016\\E",
        "shortCiteRegEx": "Zhang et al\\.",
        "year": 2016,
        "abstract": "Given a grayscale photograph as input, this paper attacks the problem of\nhallucinating a plausible color version of the photograph. This problem is\nclearly underconstrained, so previous approaches have either relied on\nsignificant user interaction or resulted in desaturated colorizations. We\npropose a fully automatic approach that produces vibrant and realistic\ncolorizations. We embrace the underlying uncertainty of the problem by posing\nit as a classification task and use class-rebalancing at training time to\nincrease the diversity of colors in the result. The system is implemented as a\nfeed-forward pass in a CNN at test time and is trained on over a million color\nimages. We evaluate our algorithm using a \"colorization Turing test,\" asking\nhuman participants to choose between a generated and ground truth color image.\nOur method successfully fools humans on 32% of the trials, significantly higher\nthan previous methods. Moreover, we show that colorization can be a powerful\npretext task for self-supervised feature learning, acting as a cross-channel\nencoder. This approach results in state-of-the-art performance on several\nfeature learning benchmarks.",
        "full_text": "Colorful Image Colorization\nRichard Zhang, Phillip Isola, Alexei A. Efros\n{rich.zhang,isola,efros}@eecs.berkeley.edu\nUniversity of California, Berkeley\nAbstract. Given a grayscale photograph as input, this paper attacks\nthe problem of hallucinating a plausible color version of the photograph.\nThis problem is clearly underconstrained, so previous approaches have\neither relied on signi\ufb01cant user interaction or resulted in desaturated col-\norizations. We propose a fully automatic approach that produces vibrant\nand realistic colorizations. We embrace the underlying uncertainty of the\nproblem by posing it as a classi\ufb01cation task and use class-rebalancing at\ntraining time to increase the diversity of colors in the result. The sys-\ntem is implemented as a feed-forward pass in a CNN at test time and is\ntrained on over a million color images. We evaluate our algorithm using a\n\u201ccolorization Turing test,\u201d asking human participants to choose between\na generated and ground truth color image. Our method successfully fools\nhumans on 32% of the trials, signi\ufb01cantly higher than previous methods.\nMoreover, we show that colorization can be a powerful pretext task for\nself-supervised feature learning, acting as a cross-channel encoder. This\napproach results in state-of-the-art performance on several feature learn-\ning benchmarks.\nKeywords: Colorization, Vision for Graphics, CNNs, Self-supervised\nlearning\n1\nIntroduction\nConsider the grayscale photographs in Figure 1. At \ufb01rst glance, hallucinating\ntheir colors seems daunting, since so much of the information (two out of the\nthree dimensions) has been lost. Looking more closely, however, one notices that\nin many cases, the semantics of the scene and its surface texture provide ample\ncues for many regions in each image: the grass is typically green, the sky is\ntypically blue, and the ladybug is most de\ufb01nitely red. Of course, these kinds of\nsemantic priors do not work for everything, e.g., the croquet balls on the grass\nmight not, in reality, be red, yellow, and purple (though it\u2019s a pretty good guess).\nHowever, for this paper, our goal is not necessarily to recover the actual ground\ntruth color, but rather to produce a plausible colorization that could potentially\nfool a human observer. Therefore, our task becomes much more achievable: to\nmodel enough of the statistical dependencies between the semantics and the\ntextures of grayscale images and their color versions in order to produce visually\ncompelling results.\nGiven the lightness channel L, our system predicts the corresponding a and\nb color channels of the image in the CIE Lab colorspace. To solve this problem,\narXiv:1603.08511v5  [cs.CV]  5 Oct 2016\n2\nZhang, Isola, Efros\nFig. 1. Example input grayscale photos and output colorizations from our algo-\nrithm. These examples are cases where our model works especially well. Please visit\nhttp://richzhang.github.io/colorization/ to see the full range of results and to\ntry our model and code. Best viewed in color (obviously).\nwe leverage large-scale data. Predicting color has the nice property that training\ndata is practically free: any color photo can be used as a training example, simply\nby taking the image\u2019s L channel as input and its ab channels as the supervisory\nsignal. Others have noted the easy availability of training data, and previous\nworks have trained convolutional neural networks (CNNs) to predict color on\nlarge datasets [1,2]. However, the results from these previous attempts tend to\nlook desaturated. One explanation is that [1,2] use loss functions that encourage\nconservative predictions. These losses are inherited from standard regression\nproblems, where the goal is to minimize Euclidean error between an estimate\nand the ground truth.\nWe instead utilize a loss tailored to the colorization problem. As pointed out\nby [3], color prediction is inherently multimodal \u2013 many objects can take on\nseveral plausible colorizations. For example, an apple is typically red, green, or\nyellow, but unlikely to be blue or orange. To appropriately model the multimodal\nnature of the problem, we predict a distribution of possible colors for each pixel.\nFurthermore, we re-weight the loss at training time to emphasize rare colors.\nThis encourages our model to exploit the full diversity of the large-scale data on\nwhich it is trained. Lastly, we produce a \ufb01nal colorization by taking the annealed-\nmean of the distribution. The end result is colorizations that are more vibrant\nand perceptually realistic than those of previous approaches.\nEvaluating synthesized images is notoriously di\ufb03cult [4]. Since our ultimate\ngoal is to make results that are compelling to a human observer, we introduce\na novel way of evaluating colorization results, directly testing their perceptual\nrealism. We set up a \u201ccolorization Turing test,\u201d in which we show participants\nreal and synthesized colors for an image, and ask them to identify the fake.\nIn this quite di\ufb03cult paradigm, we are able to fool participants on 32% of the\ninstances (ground truth colorizations would achieve 50% on this metric), signif-\nicantly higher than prior work [2]. This test demonstrates that in many cases,\nColorful Image Colorization\n3\nour algorithm is producing nearly photorealistic results (see Figure 1 for selected\nsuccessful examples from our algorithm). We also show that our system\u2019s col-\norizations are realistic enough to be useful for downstream tasks, in particular\nobject classi\ufb01cation, using an o\ufb00-the-shelf VGG network [5].\nWe additionally explore colorization as a form of self-supervised representa-\ntion learning, where raw data is used as its own source of supervision. The idea\nof learning feature representations in this way goes back at least to autoencoders\n[6]. More recent works have explored feature learning via data imputation, where\na held-out subset of the complete data is predicted (e.g., [7,8,9,10,11,12,13]).\nOur method follows in this line, and can be termed a cross-channel encoder.\nWe test how well our model performs in generalization tasks, compared to pre-\nvious [14,8,15,10] and concurrent [16] self-supervision algorithms, and \ufb01nd that\nour method performs surprisingly well, achieving state-of-the-art performance\non several metrics.\nOur contributions in this paper are in two areas. First, we make progress\non the graphics problem of automatic image colorization by (a) designing an\nappropriate objective function that handles the multimodal uncertainty of the\ncolorization problem and captures a wide diversity of colors, (b) introducing\na novel framework for testing colorization algorithms, potentially applicable to\nother image synthesis tasks, and (c) setting a new high-water mark on the task by\ntraining on a million color photos. Secondly, we introduce the colorization task\nas a competitive and straightforward method for self-supervised representation\nlearning, achieving state-of-the-art results on several benchmarks.\nPrior work on colorization Colorization algorithms mostly di\ufb00er in the\nways they obtain and treat the data for modeling the correspondence between\ngrayscale and color. Non-parametric methods, given an input grayscale image,\n\ufb01rst de\ufb01ne one or more color reference images (provided by a user or retrieved\nautomatically) to be used as source data. Then, following the Image Analogies\nframework [17], color is transferred onto the input image from analogous regions\nof the reference image(s) [18,19,20,21]. Parametric methods, on the other hand,\nlearn prediction functions from large datasets of color images at training time,\nposing the problem as either regression onto continuous color space [22,1,2] or\nclassi\ufb01cation of quantized color values [3]. Our method also learns to classify\ncolors, but does so with a larger model, trained on more data, and with several\ninnovations in the loss function and mapping to a \ufb01nal continuous output.\nConcurrent work on colorization Concurrently with our paper, Larsson\net al. [23] and Iizuka et al. [24] have developed similar systems, which leverage\nlarge-scale data and CNNs. The methods di\ufb00er in their CNN architectures and\nloss functions. While we use a classi\ufb01cation loss, with rebalanced rare classes,\nLarsson et al. use an un-rebalanced classi\ufb01cation loss, and Iizuka et al. use a\nregression loss. In Section 3.1, we compare the e\ufb00ect of each of these types\nof loss function in conjunction with our architecture. The CNN architectures\nare also somewhat di\ufb00erent: Larsson et al. use hypercolumns [25] on a VGG\nnetwork [5], Iizuka et al. use a two-stream architecture in which they fuse global\nand local features, and we use a single-stream, VGG-styled network with added\ndepth and dilated convolutions [26,27]. In addition, while we and Larsson et al.\ntrain our models on ImageNet [28], Iizuka et al. train their model on Places\n4\nZhang, Isola, Efros\nFig. 2. Our network architecture. Each conv layer refers to a block of 2 or 3 repeated\nconv and ReLU layers, followed by a BatchNorm [30] layer. The net has no pool layers.\nAll changes in resolution are achieved through spatial downsampling or upsampling\nbetween conv blocks.\n[29]. In Section 3.1, we provide quantitative comparisons to Larsson et al., and\nencourage interested readers to investigate both concurrent papers.\n2\nApproach\nWe train a CNN to map from a grayscale input to a distribution over quantized\ncolor value outputs using the architecture shown in Figure 2. Architectural de-\ntails are described in the supplementary materials on our project webpage1, and\nthe model is publicly available. In the following, we focus on the design of the\nobjective function, and our technique for inferring point estimates of color from\nthe predicted color distribution.\n2.1\nObjective Function\nGiven an input lightness channel X \u2208RH\u00d7W \u00d71, our objective is to learn a\nmapping bY = F(X) to the two associated color channels Y \u2208RH\u00d7W \u00d72, where\nH, W are image dimensions.\n(We denote predictions with a b\u00b7 symbol and ground truth without.) We per-\nform this task in CIE Lab color space. Because distances in this space model\nperceptual distance, a natural objective function, as used in [1,2], is the Eu-\nclidean loss L2(\u00b7, \u00b7) between predicted and ground truth colors:\nL2( bY, Y) = 1\n2\nX\nh,w\n\u2225Yh,w \u2212bYh,w\u22252\n2\n(1)\nHowever, this loss is not robust to the inherent ambiguity and multimodal\nnature of the colorization problem. If an object can take on a set of distinct\nab values, the optimal solution to the Euclidean loss will be the mean of the\nset. In color prediction, this averaging e\ufb00ect favors grayish, desaturated results.\nAdditionally, if the set of plausible colorizations is non-convex, the solution will\nin fact be out of the set, giving implausible results.\n1 http://richzhang.github.io/colorization/\nColorful Image Colorization\n5\nFig. 3. (a) Quantized ab color space with a grid size of 10. A total of 313 ab pairs are\nin gamut. (b) Empirical probability distribution of ab values, shown in log scale. (c)\nEmpirical probability distribution of ab values, conditioned on L, shown in log scale.\nInstead, we treat the problem as multinomial classi\ufb01cation. We quantize the\nab output space into bins with grid size 10 and keep the Q = 313 values which\nare in-gamut, as shown in Figure 3(a). For a given input X, we learn a mapping\nbZ = G(X) to a probability distribution over possible colors bZ \u2208[0, 1]H\u00d7W \u00d7Q,\nwhere Q is the number of quantized ab values.\nTo compare predicted bZ against ground truth, we de\ufb01ne function Z = H\u22121\ngt (Y),\nwhich converts ground truth color Y to vector Z, using a soft-encoding scheme2.\nWe then use multinomial cross entropy loss Lcl(\u00b7, \u00b7), de\ufb01ned as:\nLcl(bZ, Z) = \u2212\nX\nh,w\nv(Zh,w)\nX\nq\nZh,w,q log(bZh,w,q)\n(2)\nwhere v(\u00b7) is a weighting term that can be used to rebalance the loss based\non color-class rarity, as de\ufb01ned in Section 2.2 below. Finally, we map probability\ndistribution bZ to color values bY with function bY = H(bZ), which will be further\ndiscussed in Section 2.3.\n2.2\nClass rebalancing\nThe distribution of ab values in natural images is strongly biased towards val-\nues with low ab values, due to the appearance of backgrounds such as clouds,\npavement, dirt, and walls. Figure 3(b) shows the empirical distribution of pix-\nels in ab space, gathered from 1.3M training images in ImageNet [28]. Observe\nthat the number of pixels in natural images at desaturated values are orders of\nmagnitude higher than for saturated values. Without accounting for this, the\n2 Each ground truth value Yh,w can be encoded as a 1-hot vector Zh,w by searching for\nthe nearest quantized ab bin. However, we found that soft-encoding worked well for\ntraining, and allowed the network to quickly learn the relationship between elements\nin the output space [31]. We \ufb01nd the 5-nearest neighbors to Yh,w in the output\nspace and weight them proportionally to their distance from the ground truth using\na Gaussian kernel with \u03c3 = 5.\n6\nZhang, Isola, Efros\nloss function is dominated by desaturated ab values. We account for the class-\nimbalance problem by reweighting the loss of each pixel at train time based on\nthe pixel color rarity. This is asymptotically equivalent to the typical approach\nof resampling the training space [32]. Each pixel is weighed by factor w \u2208RQ,\nbased on its closest ab bin.\nv(Zh,w) = wq\u2217, where q\u2217= arg max\nq\nZh,w,q\n(3)\nw \u221d\n\u0010\n(1 \u2212\u03bb)ep + \u03bb\nQ\n\u0011\u22121\n,\nE[w] =\nX\nq\nepqwq = 1\n(4)\nTo obtain smoothed empirical distribution ep \u2208\u2206Q, we estimate the empirical\nprobability of colors in the quantized ab space p \u2208\u2206Q from the full ImageNet\ntraining set and smooth the distribution with a Gaussian kernel G\u03c3. We then\nmix the distribution with a uniform distribution with weight \u03bb \u2208[0, 1], take\nthe reciprocal, and normalize so the weighting factor is 1 on expectation. We\nfound that values of \u03bb = 1\n2 and \u03c3 = 5 worked well. We compare results with and\nwithout class rebalancing in Section 3.1.\n2.3\nClass Probabilities to Point Estimates\nFinally, we de\ufb01ne H, which maps the predicted distribution bZ to point estimate\nbY in ab space. One choice is to take the mode of the predicted distribution for\neach pixel, as shown in the right-most column of Figure 4 for two example im-\nages. This provides a vibrant but sometimes spatially inconsistent result, e.g.,\nthe red splotches on the bus. On the other hand, taking the mean of the predicted\ndistribution produces spatially consistent but desaturated results (left-most col-\numn of Figure 4), exhibiting an unnatural sepia tone. This is unsurprising, as\ntaking the mean after performing classi\ufb01cation su\ufb00ers from some of the same\nissues as optimizing for a Euclidean loss in a regression framework. To try to get\nthe best of both worlds, we interpolate by re-adjusting the temperature T of the\nsoftmax distribution, and taking the mean of the result. We draw inspiration\nfrom the simulated annealing technique [33], and thus refer to the operation as\ntaking the annealed-mean of the distribution:\nH(Zh,w) = E\n\u0002\nfT (Zh,w)\n\u0003\n,\nfT (z) =\nexp(log(z)/T)\nP\nq exp(log(zq)/T)\n(5)\nSetting T = 1 leaves the distribution unchanged, lowering the temperature\nT produces a more strongly peaked distribution, and setting T \u21920 results in a\n1-hot encoding at the distribution mode. We found that temperature T = 0.38,\nshown in the middle column of Figure 4, captures the vibrancy of the mode while\nmaintaining the spatial coherence of the mean.\nOur \ufb01nal system F is the composition of CNN G, which produces a predicted\ndistribution over all pixels, and the annealed-mean operation H, which produces\na \ufb01nal prediction. The system is not quite end-to-end trainable, but note that\nthe mapping H operates on each pixel independently, with a single parameter,\nand can be implemented as part of a feed-forward pass of the CNN.\nColorful Image Colorization\n7\nFig. 4. The e\ufb00ect of temperature parameter T on the annealed-mean output (Equation\n5). The left-most images show the means of the predicted color distributions and the\nright-most show the modes. We use T = 0.38 in our system.\n3\nExperiments\nIn Section 3.1, we assess the graphics aspect of our algorithm, evaluating the\nperceptual realism of our colorizations, along with other measures of accuracy.\nWe compare our full algorithm to several variants, along with recent [2] and\nconcurrent work [23]. In Section 3.2, we test colorization as a method for self-\nsupervised representation learning. Finally, in Section 10.1, we show qualitative\nexamples on legacy black and white images.\n3.1\nEvaluating colorization quality\nWe train our network on the 1.3M images from the ImageNet training set [28],\nvalidate on the \ufb01rst 10k images in the ImageNet validation set, and test on a\nseparate 10k images in the validation set, same as in [23]. We show quantitative\nresults in Table 1 on three metrics. A qualitative comparison for selected success\nand failure cases is shown in Figure 5. For a comparison on a full selection of\nrandom images, please see our project webpage.\nTo speci\ufb01cally test the e\ufb00ect of di\ufb00erent loss functions, we train our CNN\nwith various losses. We also compare to previous [2] and concurrent methods [23],\nwhich both use CNNs trained on ImageNet, along with naive baselines:\n1. Ours (full) Our full method, with classi\ufb01cation loss, de\ufb01ned in Equation 2,\nand class rebalancing, as described in Section 2.2. The network was trained\nfrom scratch with k-means initialization [36], using the ADAM solver for\napproximately 450k iterations3.\n2. Ours (class) Our network on classi\ufb01cation loss but no class rebalancing\n(\u03bb = 1 in Equation 4).\n3 \u03b21 = .9, \u03b22 = .99, and weight decay = 10\u22123. Initial learning rate was 3 \u00d7 10\u22125 and\ndropped to 10\u22125 and 3 \u00d7 10\u22126 when loss plateaued, at 200k and 375k iterations,\nrespectively. Other models trained from scratch followed similar training protocol.\n8\nZhang, Isola, Efros\nGround truth\nRegression\nClassi\ufb01cation \nw/ rebal\nInput\nClassi\ufb01cation\nSuccess cases\nFailure cases\nFig. 5. Example results from our ImageNet test set. Our classi\ufb01cation loss with re-\nbalancing produces more accurate and vibrant results than a regression loss or a clas-\nsi\ufb01cation loss without rebalancing. Successful colorizations are above the dotted line.\nCommon failures are below. These include failure to capture long-range consistency,\nfrequent confusions between red and blue, and a default sepia tone on complex indoor\nscenes. Please visit http://richzhang.github.io/colorization/ to see the full range\nof results.\nColorful Image Colorization\n9\nColorization Results on ImageNet\nModel\nAuC\nVGG Top-1\nAMT\nMethod\nParams Feats Runtime non-rebal rebal\nClass Acc\nLabeled\n(MB)\n(MB)\n(ms)\n(%)\n(%)\n(%)\nReal (%)\nGround Truth\n\u2013\n\u2013\n\u2013\n100\n100\n68.3\n50\nGray\n\u2013\n\u2013\n\u2013\n89.1\n58.0\n52.7\n\u2013\nRandom\n\u2013\n\u2013\n\u2013\n84.2\n57.3\n41.0\n13.0\u00b14.4\nDahl [2]\n\u2013\n\u2013\n\u2013\n90.4\n58.9\n48.7\n18.3\u00b12.8\nLarsson et al. [23]\n588\n495\n122.1\n91.7\n65.9\n59.4\n27.2\u00b12.7\nOurs (L2)\n129\n127\n17.8\n91.2\n64.4\n54.9\n21.2\u00b12.5\nOurs (L2, ft)\n129\n127\n17.8\n91.5\n66.2\n56.5\n23.9\u00b12.8\nOurs (class)\n129\n142\n22.1\n91.6\n65.1\n56.6\n25.2\u00b12.7\nOurs (full)\n129\n142\n22.1\n89.5\n67.3\n56.0\n32.3\u00b12.2\nTable 1. Colorization results on 10k images in the ImageNet validation set [28], as\nused in [23]. AuC refers to the area under the curve of the cumulative error distribution\nover ab space [22]. Results column 2 shows the class-balanced variant of this metric.\nColumn 3 is the classi\ufb01cation accuracy after colorization using the VGG-16 [5] network.\nColumn 4 shows results from our AMT real vs. fake test (with mean and standard error\nreported, estimated by bootstrap [34]). Note that an algorithm that produces ground\ntruth images would achieve 50% performance in expectation. Higher is better for all\nmetrics. Rows refer to di\ufb00erent algorithms; see text for a description of each. Parameter\nand feature memory, and runtime, were measured on a Titan X GPU using Ca\ufb00e [35].\n3. Ours (L2) Our network trained from scratch, with L2 regression loss, de-\nscribed in Equation 1, following the same training protocol.\n4. Ours (L2, ft) Our network trained with L2 regression loss, \ufb01ne-tuned from\nour full classi\ufb01cation with rebalancing network.\n5. Larsson et al. [23] A CNN method that also appears in these proceedings.\n6. Dahl [2] A previous model using a Laplacian pyramid on VGG features,\ntrained with L2 regression loss.\n7. Gray Colors every pixel gray, with (a, b) = 0.\n8. Random Copies the colors from a random image from the training set.\nEvaluating the quality of synthesized images is well-known to be a di\ufb03cult\ntask, as simple quantitative metrics, like RMS error on pixel values, often fail to\ncapture visual realism. To address the shortcomings of any individual evaluation,\nwe test three that measure di\ufb00erent senses of quality, shown in Table 1.\n1. Perceptual realism (AMT): For many applications, such as those in\ngraphics, the ultimate test of colorization is how compelling the colors look to a\nhuman observer. To test this, we ran a real vs. fake two-alternative forced choice\nexperiment on Amazon Mechanical Turk (AMT). Participants in the experiment\nwere shown a series of pairs of images. Each pair consisted of a color photo next\nto a re-colorized version, produced by either our algorithm or a baseline. Par-\nticipants were asked to click on the photo they believed contained fake colors\ngenerated by a computer program. Individual images of resolution 256\u00d7256 were\nshown for one second each, and after each pair, participants were given unlim-\nited time to respond. Each experimental session consisted of 10 practice trials\n10\nZhang, Isola, Efros\nGround truth\nOurs\n82%\n67%\n64%\n64%\n60%\n58%\n55%\n55%\n55%\n55%\n55%\n50%\n0%\n0%\n0%\n0%\nFooled more often\nFooled less often\nGround truth\nOurs\nGround truth\nOurs\nGround truth\nOurs\nFig. 6. Images sorted by how often AMT participants chose our algorithm\u2019s colorization\nover the ground truth. In all pairs to the left of the dotted line, participants believed\nour colorizations to be more real than the ground truth on \u226550% of the trials. In some\ncases, this may be due to poor white balancing in the ground truth image, corrected\nby our algorithm, which predicts a more prototypical appearance. Right of the dotted\nline are examples where participants were never fooled.\n(excluded from subsequent analysis), followed by 40 test pairs. On the practice\ntrials, participants were given feedback as to whether or not their answer was\ncorrect. No feedback was given during the 40 test pairs. Each session tested\nonly a single algorithm at a time, and participants were only allowed to com-\nplete at most one session. A total of 40 participants evaluated each algorithm.\nTo ensure that all algorithms were tested in equivalent conditions (i.e. time of\nday, demographics, etc.), all experiment sessions were posted simultaneously and\ndistributed to Turkers in an i.i.d. fashion.\nTo check that participants were competent at this task, 10% of the trials\npitted the ground truth image against the Random baseline described above.\nParticipants successfully identi\ufb01ed these random colorizations as fake 87% of\nthe time, indicating that they understood the task and were paying attention.\nFigure 6 gives a better sense of the participants\u2019 competency at detecting\nsubtle errors made by our algorithm. The far right column shows example pairs\nwhere participants identi\ufb01ed the fake image successfully in 100% of the trials.\nEach of these pairs was scored by at least 10 participants. Close inspection reveals\nthat on these images, our colorizations tend to have giveaway artifacts, such as\nthe yellow blotches on the two trucks, which ruin otherwise decent results.\nNonetheless, our full algorithm fooled participants on 32% of trials, as shown\nin Table 1. This number is signi\ufb01cantly higher than all compared algorithms\n(p < 0.05 in each case) except for Larsson et al., against which the di\ufb00erence\nwas not signi\ufb01cant (p = 0.10; all statistics estimated by bootstrap [34]). These\nresults validate the e\ufb00ectiveness of using both a classi\ufb01cation loss and class-\nrebalancing.\nColorful Image Colorization\n11\nNote that if our algorithm exactly reproduced the ground truth colors, the\nforced choice would be between two identical images, and participants would be\nfooled 50% of the time on expectation. Interestingly, we can identify cases where\nparticipants were fooled more often than 50% of the time, indicating our results\nwere deemed more realistic than the ground truth. Some examples are shown\nin the \ufb01rst three columns of Figure 6. In many case, the ground truth image\nis poorly white balanced or has unusual colors, whereas our system produces a\nmore prototypical appearance.\n2. Semantic interpretability (VGG classi\ufb01cation): Does our method\nproduce realistic enough colorizations to be interpretable to an o\ufb00-the-shelf ob-\nject classi\ufb01er? We tested this by feeding our fake colorized images to a VGG\nnetwork [5] that was trained to predict ImageNet classes from real color photos.\nIf the classi\ufb01er performs well, that means the colorizations are accurate enough\nto be informative about object class. Using an o\ufb00-the-shelf classi\ufb01er to assess\nthe realism of synthesized data has been previously suggested by [12].\nThe results are shown in the second column from the right of Table 1. Classi-\n\ufb01er performance drops from 68.3% to 52.7% after ablating colors from the input.\nAfter re-colorizing using our full method, the performance is improved to 56.0%\n(other variants of our method achieve slightly higher results). The Larsson et al.\n[23] method achieves the highest performance on this metric, reaching 59.4%. For\nreference, a VGG classi\ufb01cation network \ufb01ne-tuned on grayscale inputs reaches a\nperformance of 63.5%.\nIn addition to serving as a perceptual metric, this analysis demonstrates a\npractical use for our algorithm: without any additional training or \ufb01ne-tuning, we\ncan improve performance on grayscale image classi\ufb01cation, simply by colorizing\nimages with our algorithm and passing them to an o\ufb00-the-shelf classi\ufb01er.\n3. Raw accuracy (AuC): As a low-level test, we compute the percentage\nof predicted pixel colors within a thresholded L2 distance of the ground truth\nin ab color space. We then sweep across thresholds from 0 to 150 to produce\na cumulative mass function, as introduced in [22], integrate the area under the\ncurve (AuC), and normalize. Note that this AuC metric measures raw prediction\naccuracy, whereas our method aims for plausibility.\nOur network, trained on classi\ufb01cation without rebalancing, outperforms our\nL2 variant (when trained from scratch). When the L2 net is instead \ufb01ne-tuned\nfrom a color classi\ufb01cation network, it matches the performance of the classi\ufb01ca-\ntion network. This indicates that the L2 metric can achieve accurate coloriza-\ntions, but has di\ufb03culty in optimization from scratch. The Larsson et al. [23]\nmethod achieves slightly higher accuracy. Note that this metric is dominated by\ndesaturated pixels, due to the distribution of ab values in natural images (Figure\n3(b)). As a result, even predicting gray for every pixel does quite well, and our\nfull method with class rebalancing achieves approximately the same score.\nPerceptually interesting regions of images, on the other hand, tend to have a\ndistribution of ab values with higher values of saturation. As such, we compute\na class-balanced variant of the AuC metric by re-weighting the pixels inversely\nby color class probability (Equation 4, setting \u03bb = 0). Under this metric, our\nfull method outperforms all variants and compared algorithms, indicating that\nclass-rebalancing in the training objective achieved its desired e\ufb00ect.\n12\nZhang, Isola, Efros\nFig. 7. ImageNet Linear Classi\ufb01cation\nDataset and Task Generalization on PASCAL [37]\nClass.\nDet.\nSeg.\n(%mAP)\n(%mAP)\n(%mIU)\n\ufb01ne-tune layers\n[Ref] fc8 fc6-8 all [Ref] all [Ref] all\nImageNet [38]\n-\n76.8 78.9 79.9\n[36]\n56.8\n[42]\n48.0\nGaussian\n[10]\n\u2013\n\u2013\n53.3\n[10]\n43.4\n[10]\n19.8\nAutoencoder\n[16]\n24.8 16.0 53.8\n[10]\n41.9\n[10]\n25.2\nk-means [36]\n[16]\n32.0 39.2 56.6\n[36]\n45.6\n[16]\n32.6\nAgrawal et al. [8]\n[16]\n31.2 31.0 54.2\n[36]\n43.9\n\u2013\n\u2013\nWang & Gupta [15]\n\u2013\n28.1 52.2 58.7\n[36]\n47.4\n\u2013\n\u2013\n*Doersch et al. [14]\n[16]\n44.7 55.1 65.3 [36] 51.1\n\u2013\n\u2013\n*Pathak et al. [10]\n[10]\n\u2013\n\u2013\n56.5\n[10]\n44.5\n[10]\n29.7\n*Donahue et al. [16]\n\u2013\n38.2 50.2 58.6\n[16]\n46.2\n[16]\n34.9\nOurs (gray)\n\u2013\n52.4 61.5 65.9\n\u2013\n46.1\n\u2013\n35.0\nOurs (color)\n\u2013\n52.4 61.5 65.6\n\u2013\n46.9\n\u2013\n35.6\nTable 2. PASCAL Tests\nFig. 7. Task Generalization on ImageNet We freeze pre-trained networks and\nlearn linear classi\ufb01ers on internal layers for ImageNet [28] classi\ufb01cation. Features are\naverage-pooled, with equal kernel and stride sizes, until feature dimensionality is\nbelow 10k. ImageNet [38], k-means [36], and Gaussian initializations were run with\ngrayscale inputs, shown with dotted lines, as well as color inputs, shown with solid\nlines. Previous [14,10] and concurrent [16] self-supervision methods are shown.\nTab. 2. Task and Dataset Generalization on PASCAL Classi\ufb01cation and\ndetection on PASCAL VOC 2007 [39] and segmentation on PASCAL VOC 2012 [40],\nusing standard mean average precision (mAP) and mean intersection over union\n(mIU) metrics for each task. We \ufb01ne-tune our network with grayscale inputs (gray)\nand color inputs (color). Methods noted with a * only pre-trained a subset of the\nAlexNet layers. The remaining layers were initialized with [36]. Column Ref indicates\nthe source for a value obtained from a previous paper.\n3.2\nCross-Channel Encoding as Self-Supervised Feature Learning\nIn addition to making progress on the graphics task of colorization, we evaluate\nhow colorization can serve as a pretext task for representation learning. Our\nmodel is akin to an autoencoder, except that the input and output are di\ufb00erent\nimage channels, suggesting the term cross-channel encoder.\nTo evaluate the feature representation learned through this kind of cross-\nchannel encoding, we run two sets of tests on our network. First, we test the\ntask generalization capability of the features by \ufb01xing the learned representa-\ntion and training linear classi\ufb01ers to perform object classi\ufb01cation on already seen\ndata (Figure 7). Second, we \ufb01ne-tune the network on the PASCAL dataset [37]\nfor the tasks of classi\ufb01cation, detection, and segmentation. Here, in addition to\ntesting on held-out tasks, this group of experiments tests the learned represen-\ntation on dataset generalization. To fairly compare to previous feature learning\nalgorithms, we retrain an AlexNet [38] network on the colorization task, using\nour full method, for 450k iterations. We \ufb01nd that the resulting learned repre-\nsentation achieves higher performance on object classi\ufb01cation and segmentation\ntasks relative to previous methods tested (Table 2).\nImageNet classi\ufb01cation The network was pre-trained to colorize images\nfrom the ImageNet dataset, without semantic label information. We test how well\nColorful Image Colorization\n13\nthe learned features represent the object-level semantics. To do this, we freeze\nthe weights of the network, provide semantic labels, and train linear classi\ufb01ers\non each convolutional layer. The results are shown in Figure 7.\nAlexNet directly trained on ImageNet classi\ufb01cation achieves the highest per-\nformance, and serves as the ceiling for this test. Random initialization, with\nGaussian weights or the k-means scheme implemented in [36], peak in the mid-\ndle layers. Because our representation is learned on grayscale images, the network\nis handicapped at the input. To quantify the e\ufb00ect of this loss of information,\nwe \ufb01ne-tune AlexNet on grayscale image classi\ufb01cation, and also run the random\ninitialization schemes on grayscale images. Interestingly, for all three methods,\nthere is a 6% performance gap between color and grayscale inputs, which remains\napproximately constant throughout the network.\nWe compare our model to other recent self-supervised methods pre-trained on\nImageNet [14,10,16]. To begin, our conv1 representation results in worse linear\nclassi\ufb01cation performance than competiting methods [14,16], but is comparable\nto other methods which have a grayscale input. However, this performance gap\nis immediately bridged at conv2, and our network achieves competitive perfor-\nmance to [14,16] throughout the remainder of the network. This indicates that\ndespite the input handicap, solving the colorization task encourages representa-\ntions that linearly separate semantic classes in the trained data distribution.\nPASCAL classi\ufb01cation, detection, and segmentation We test our\nmodel on the commonly used self-supervision benchmarks on PASCAL classi\ufb01-\ncation, detection, and segmentation, introduced in [14,36,10]. Results are shown\nin Table 2. Our network achieves strong performance across all three tasks, and\nstate-of-the-art numbers in classi\ufb01cation and segmentation. We use the method\nfrom [36], which rescales the layers so they \u201clearn\u201d at the same rate. We test\nour model in two modes: (1) keeping the input grayscale by disregarding color\ninformation (Ours (gray)) and (2) modifying conv1 to receive a full 3-channel\nLab input, initializing the weights on the ab channels to be zero (Ours (color)).\nWe \ufb01rst test the network on PASCAL VOC 2007 [39] classi\ufb01cation, following\nthe protocol in [16]. The network is trained by freezing the representation up to\ncertain points, and \ufb01ne-tuning the remainder. Note that when conv1 is frozen,\nthe network is e\ufb00ectively only able to interpret grayscale images. Across all three\nclassi\ufb01cation tests, we achieve state-of-the-art accuracy.\nWe also test detection on PASCAL VOC 2007, using Fast R-CNN [41], fol-\nlowing the procedure in [36]. Doersch et al. [14] achieves 51.1%, while we reach\n46.9% and 47.9% with grayscale and color inputs, respectively. Our method is\nwell above the strong k-means [36] baseline of 45.6%, but all self-supervised meth-\nods still fall short of pre-training with ImageNet semantic supervision, which\nreaches 56.8%.\nFinally, we test semantic segmentation on PASCAL VOC 2012 [40], using\nthe FCN architecture of [42], following the protocol in [10]. Our colorization\ntask shares similarities to the semantic segmentation task, as both are per-pixel\nclassi\ufb01cation problems. Our grayscale \ufb01ne-tuned network achieves performance\nof 35.0%, approximately equal to Donahue et al. [16], and adding in color infor-\nmation increases performance to 35.6%, above other tested algorithms.\n14\nZhang, Isola, Efros\nFig. 8. Applying our method to legacy black and white photos. Left to right: photo\nby David Fleay of a Thylacine, now extinct, 1936; photo by Ansel Adams of Yosemite;\namateur family photo from 1956; Migrant Mother by Dorothea Lange, 1936.\n3.3\nLegacy Black and White Photos\nSince our model was trained using \u201cfake\u201d grayscale images generated by strip-\nping ab channels from color photos, we also ran our method on real legacy black\nand white photographs, as shown in Figure 8 (additional results can be viewed\non our project webpage). One can see that our model is still able to produce\ngood colorizations, even though the low-level image statistics of the legacy pho-\ntographs are quite di\ufb00erent from those of the modern-day photos on which it\nwas trained.\n4\nConclusion\nWhile image colorization is a boutique computer graphics task, it is also an in-\nstance of a di\ufb03cult pixel prediction problem in computer vision. Here we have\nshown that colorization with a deep CNN and a well-chosen objective function\ncan come closer to producing results indistinguishable from real color photos.\nOur method not only provides a useful graphics output, but can also be viewed\nas a pretext task for representation learning. Although only trained to color,\nour network learns a representation that is surprisingly useful for object clas-\nsi\ufb01cation, detection, and segmentation, performing strongly compared to other\nself-supervised pre-training methods.\nAcknowledgements\nThis research was supported, in part, by ONR MURI N000141010934, NSF SMA-\n1514512, an Intel research grant, and a hardware donation by NVIDIA Corp. We thank\nmembers of the Berkeley Vision Lab and Aditya Deshpande for helpful discussions,\nPhilipp Kr\u00a8ahenb\u00a8uhl and Je\ufb00Donahue for help with self-supervision experiments, and\nGustav Larsson for providing images for comparison to [23].\nColorful Image Colorization\n15\nReferences\n1. Cheng, Z., Yang, Q., Sheng, B.: Deep colorization. In: Proceedings of the IEEE\nInternational Conference on Computer Vision. (2015) 415\u2013423\n2. Dahl, R.: Automatic colorization. In: http://tinyclouds.org/colorize/. (2016)\n3. Charpiat, G., Hofmann, M., Sch\u00a8olkopf, B.: Automatic image colorization via mul-\ntimodal predictions. In: Computer Vision\u2013ECCV 2008. Springer (2008) 126\u2013139\n4. Ramanarayanan, G., Ferwerda, J., Walter, B., Bala, K.: Visual equivalence: to-\nwards a new standard for image \ufb01delity. ACM Transactions on Graphics (TOG)\n26(3) (2007) 76\n5. Simonyan, K., Zisserman, A.:\nVery deep convolutional networks for large-scale\nimage recognition. arXiv preprint arXiv:1409.1556 (2014)\n6. Bengio, Y., Courville, A., Vincent, P.: Representation learning: A review and new\nperspectives. IEEE transactions on pattern analysis and machine intelligence 35(8)\n(2013) 1798\u20131828\n7. Ngiam, J., Khosla, A., Kim, M., Nam, J., Lee, H., Ng, A.Y.: Multimodal deep\nlearning. In: Proceedings of the 28th international conference on machine learning\n(ICML-11). (2011) 689\u2013696\n8. Agrawal, P., Carreira, J., Malik, J.: Learning to see by moving. In: Proceedings of\nthe IEEE International Conference on Computer Vision. (2015) 37\u201345\n9. Jayaraman, D., Grauman, K.: Learning image representations tied to ego-motion.\nIn: Proceedings of the IEEE International Conference on Computer Vision. (2015)\n1413\u20131421\n10. Pathak, D., Kr\u00a8ahenb\u00a8uhl, P., Donahue, J., Darrell, T., Efros, A.: Context encoders:\nFeature learning by inpainting. In: CVPR. (2016)\n11. Lotter, W., Kreiman, G., Cox, D.:\nDeep predictive coding networks for video\nprediction and unsupervised learning. arXiv preprint arXiv:1605.08104 (2016)\n12. Owens, A., Isola, P., McDermott, J., Torralba, A., Adelson, E.H., Freeman, W.T.:\nVisually indicated sounds. CVPR (2016)\n13. Owens, A., Wu, J., McDermott, J.H., Freeman, W.T., Torralba, A.: Ambient sound\nprovides supervision for visual learning. In: ECCV. (2016)\n14. Doersch, C., Gupta, A., Efros, A.A.: Unsupervised visual representation learning\nby context prediction. In: Proceedings of the IEEE International Conference on\nComputer Vision. (2015) 1422\u20131430\n15. Wang, X., Gupta, A.: Unsupervised learning of visual representations using videos.\nIn: Proceedings of the IEEE International Conference on Computer Vision. (2015)\n2794\u20132802\n16. Donahue, J., Kr\u00a8ahenb\u00a8uhl, P., Darrell, T.:\nAdversarial feature learning.\narXiv\npreprint arXiv:1605.09782 (2016)\n17. Hertzmann, A., Jacobs, C.E., Oliver, N., Curless, B., Salesin, D.H.: Image analo-\ngies. In: Proceedings of the 28th annual conference on Computer graphics and\ninteractive techniques, ACM (2001) 327\u2013340\n18. Welsh, T., Ashikhmin, M., Mueller, K.: Transferring color to greyscale images.\nACM Transactions on Graphics (TOG) 21(3) (2002) 277\u2013280\n19. Gupta, R.K., Chia, A.Y.S., Rajan, D., Ng, E.S., Zhiyong, H.: Image colorization\nusing similar images. In: Proceedings of the 20th ACM international conference\non Multimedia, ACM (2012) 369\u2013378\n20. Liu, X., Wan, L., Qu, Y., Wong, T.T., Lin, S., Leung, C.S., Heng, P.A.: Intrinsic\ncolorization. In: ACM Transactions on Graphics (TOG). Volume 27., ACM (2008)\n152\n16\nZhang, Isola, Efros\n21. Chia, A.Y.S., Zhuo, S., Gupta, R.K., Tai, Y.W., Cho, S.Y., Tan, P., Lin, S.: Seman-\ntic colorization with internet images. In: ACM Transactions on Graphics (TOG).\nVolume 30., ACM (2011) 156\n22. Deshpande, A., Rock, J., Forsyth, D.: Learning large-scale automatic image col-\norization.\nIn: Proceedings of the IEEE International Conference on Computer\nVision. (2015) 567\u2013575\n23. Larsson, G., Maire, M., Shakhnarovich, G.: Learning representations for automatic\ncolorization. European Conference on Computer Vision (2016)\n24. Iizuka, S., Simo-Serra, E., Ishikawa, H.:\nLet there be Color!: Joint End-to-end\nLearning of Global and Local Image Priors for Automatic Image Colorization with\nSimultaneous Classi\ufb01cation. ACM Transactions on Graphics (Proc. of SIGGRAPH\n2016) 35(4) (2016)\n25. Hariharan, B., Arbel\u00b4aez, P., Girshick, R., Malik, J.:\nHypercolumns for object\nsegmentation and \ufb01ne-grained localization. In: Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition. (2015) 447\u2013456\n26. Chen, L.C., Papandreou, G., Kokkinos, I., Murphy, K., Yuille, A.L.:\nDeeplab:\nSemantic image segmentation with deep convolutional nets, atrous convolution,\nand fully connected crfs. arXiv preprint arXiv:1606.00915 (2016)\n27. Yu, F., Koltun, V.: Multi-scale context aggregation by dilated convolutions. In-\nternational Conference on Learning Representations (2016)\n28. Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z.,\nKarpathy, A., Khosla, A., Bernstein, M., et al.: Imagenet large scale visual recog-\nnition challenge. International Journal of Computer Vision 115(3) (2015) 211\u2013252\n29. Zhou, B., Lapedriza, A., Xiao, J., Torralba, A., Oliva, A.: Learning deep features\nfor scene recognition using places database. In: Advances in neural information\nprocessing systems. (2014) 487\u2013495\n30. Io\ufb00e, S., Szegedy, C.: Batch normalization: Accelerating deep network training by\nreducing internal covariate shift. arXiv preprint arXiv:1502.03167 (2015)\n31. Hinton, G., Vinyals, O., Dean, J.: Distilling the knowledge in a neural network.\narXiv preprint arXiv:1503.02531 (2015)\n32. Farabet, C., Couprie, C., Najman, L., LeCun, Y.: Learning hierarchical features\nfor scene labeling. Pattern Analysis and Machine Intelligence, IEEE Transactions\non 35(8) (2013) 1915\u20131929\n33. Kirkpatrick, S., Vecchi, M.P., et al.: Optimization by simmulated annealing. science\n220(4598) (1983) 671\u2013680\n34. Efron, B.: Bootstrap methods: another look at the jackknife. In: Breakthroughs\nin Statistics. Springer (1992) 569\u2013593\n35. Jia, Y., Shelhamer, E., Donahue, J., Karayev, S., Long, J., Girshick, R., Guadar-\nrama, S., Darrell, T.: Ca\ufb00e: Convolutional architecture for fast feature embedding.\nIn: Proceedings of the 22nd ACM international conference on Multimedia, ACM\n(2014) 675\u2013678\n36. Kr\u00a8ahenb\u00a8uhl, P., Doersch, C., Donahue, J., Darrell, T.:\nData-dependent initial-\nizations of convolutional neural networks. International Conference on Learning\nRepresentations (2016)\n37. Everingham, M., Van Gool, L., Williams, C.K., Winn, J., Zisserman, A.:\nThe\npascal visual object classes (voc) challenge.\nInternational journal of computer\nvision 88(2) (2010) 303\u2013338\n38. Krizhevsky, A., Sutskever, I., Hinton, G.E.: Imagenet classi\ufb01cation with deep con-\nvolutional neural networks. In: Advances in neural information processing systems.\n(2012) 1097\u20131105\n39. Everingham, M., Van Gool, L., Williams, C.K.I., Winn, J., Zisserman, A.:\nThe\nPASCAL\nVisual\nObject\nClasses\nChallenge\n2007\n(VOC2007)\nResults.\nhttp://www.pascal-network.org/challenges/VOC/voc2007/workshop/index.html\nColorful Image Colorization\n17\n40. Everingham, M., Van Gool, L., Williams, C.K.I., Winn, J., Zisserman, A.:\nThe\nPASCAL\nVisual\nObject\nClasses\nChallenge\n2012\n(VOC2012)\nResults.\nhttp://www.pascal-network.org/challenges/VOC/voc2012/workshop/index.html\n41. Girshick, R.: Fast r-cnn. In: Proceedings of the IEEE International Conference on\nComputer Vision. (2015) 1440\u20131448\n42. Long, J., Shelhamer, E., Darrell, T.: Fully convolutional networks for semantic\nsegmentation. In: Proceedings of the IEEE Conference on Computer Vision and\nPattern Recognition. (2015) 3431\u20133440\n43. Noroozi, M., Favaro, P.: Unsupervised learning of visual representations by solving\njigsaw puzzles. arXiv preprint arXiv:1603.09246 (2016)\n44. Chakrabarti, A.: Color constancy by learning to predict chromaticity from lumi-\nnance. In: Advances in Neural Information Processing Systems. (2015) 163\u2013171\n45. Xiao, J., Hays, J., Ehinger, K.A., Oliva, A., Torralba, A.: Sun database: Large-scale\nscene recognition from abbey to zoo. In: Computer vision and pattern recognition\n(CVPR), 2010 IEEE conference on, IEEE (2010) 3485\u20133492\n46. Ratli\ufb00, N.D., Silver, D., Bagnell, J.A.: Learning to search: Functional gradient\ntechniques for imitation learning. Autonomous Robots 27(1) (2009) 25\u201353\n47. Patterson, G., Hays, J.: Sun attribute database: Discovering, annotating, and rec-\nognizing scene attributes. In: Computer Vision and Pattern Recognition (CVPR),\n2012 IEEE Conference on, IEEE (2012) 2751\u20132758\nAppendix\nThe main paper is our ECCV 2016 camera ready submission. All networks were re-\ntrained from scratch, and are referred to as the v2 model. Due to space constraints, we\nwere unable to include many of the analyses presented in our original arXiv v1 paper.\nWe include these analyses in this Appendix, which were generated from a previous v1\nversion of the model. All models are publicly available on our website.\nSection 5 contains additional representation learning experiments. Section 6 inves-\ntigates additional analysis on the VGG semantic interpretability test. In Section 7, we\nexplore how low-level queues a\ufb00ect the output. Section 8 examines the multi-modality\nlearned in the network. Section 9 de\ufb01nes the network architecture used. In Section 10,\nwe compare our algorithm to previous approaches [22] and [1], and show additional\nexamples on legacy grayscale images.\n5\nCross-Channel Encoding as Self-Supervised Feature\nLearning (continued)\nIn Section 3.2, we discussed using colorization as a pretext task for representation learn-\ning. In addition to learning linear classi\ufb01ers on internal layers for ImageNet classi\ufb01ers,\nwe run the additional experiment of learning non-linear classi\ufb01ers, as proposed in [43].\nEach internal layer is frozen, along with all preceding layers, and the layers on top\nare randomly reinitialized and trained for classi\ufb01cation. Performance is summarized in\nTable 3. Of the unsupervised models, Noroozi et al. [43] have the highest performance\nacross all layers. The architectural modi\ufb01cations result in 5.6\u00d7 feature map size and\n7.35\u00d7 model run-time, up to the pool5 layer, relative to an unmodi\ufb01ed Alexnet. Of\nthe remaining methods, Donahue et al. [16] performs best at conv2 and Doersch et\nal. performs best at conv3 and conv4. Our method performs strongly throughout, and\nbest across methods at the conv5 layer.\n18\nZhang, Isola, Efros\nAuthor\nTraining Input\nModel\n[Ref]\nLayers\nParams Feats Runtime\nconv2 conv3 conv4 conv5\nKrizhevsky et al. [38]\nlabels\nrgb\n1.00\n1.00\n1.00\n\u2013\n56.5\n56.5\n56.5\n56.5\nKrizhevsky et al. [38]\nlabels\nL\n0.99\n1.00\n0.92\n\u2013\n50.5\n50.5\n50.5\n50.5\nNoroozi & Favaro [43] imagenet\nrgb\n1.00\n5.60\n7.35\n[43]\n56.0\n52.4\n48.3\n38.1\nGaussian\nimagenet\nrgb\n1.00\n1.00\n1.00\n[43]\n41.0\n34.8\n27.1\n12.0\nDoersch et al. [14]\nimagenet\nrgb\n1.61\n1.00\n2.82\n[43]\n47.6\n48.7\n45.6\n30.4\nWang & Gupta [15]\nvideos\nrgb\n1.00\n1.00\n1.00\n[43]\n46.9\n42.8\n38.8\n29.8\nDonahue et al. [16]\nimagenet\nrgb\n1.00\n0.87\n0.96\n[16]\n51.9\n47.3\n41.9\n31.1\nOurs\nimagenet\nL\n0.99\n0.87\n0.84\n\u2013\n46.6\n43.5\n40.7\n35.2\nTable 3. ImageNet classi\ufb01cation with nonlinear layers, as proposed in [43]. Note\nthat some models have architectural di\ufb00erences. We note the e\ufb00ect of these modi\ufb01ca-\ntions by the number of model parameters, number of features per image, and run-time,\nas a multiple of Alexnet [38] without modi\ufb01cations, up to the pool5 layer. Noroozi et al.\n[43] performs best on all layers, with denser feature maps due to smaller stride in conv1\nlayer, along with LRN and pool ordering switched. Doersch et al. [14] remove groups\nin conv layers. Donahue et al. [16] remove LRN layers and change ReLU to leakyReLU\nunits. Ours removes LRN and uses a single channel input. We also note the source of\nperformance numbers. Column Ref indicates the source for a value obtained from a\nprevious paper.\n6\nSemantic Interpretability of Colorizations\nIn Section 3.1, we investigated using the VGG classi\ufb01er to evaluate the semantic in-\nterpretability of our colorization results. In Section 6.1, we show the categories which\nperform well, and the ones which perform poorly, using this metric. In Section 6.2, we\nshow commonly confused categories after recolorization.\n6.1\nCategory Performance\nIn Figure 9, we show a selection of classes that have the most improvement in VGG\nclassi\ufb01cation with respect to grayscale, along with the classes for which our colorizations\nhurt the most. Interestingly, many of the top classes actually have a color in their\nname, such as the green snake, orange, and gold\ufb01nch. The bottom classes show some\ncommon errors of our system, such as coloring clothing incorrectly and inconsistently\nand coloring an animal with a plausible but incorrect color. This analysis was performed\nusing 48k images from the ImageNet validation set, and images in the top and bottom\n10 classes are provided on the website.\nOur process for sorting categories and images is described below. For each cat-\negory, we compute the top-5 classi\ufb01cation performance on grayscale and recolorized\nimages, agray, arecolor \u2208[0, 1]C, where C = 1000 categories. We sort the categories\nby arecolor \u2212agray. The re-colored vs grayscale performance per category is shown in\nFigure 11(a), with top and bottom 50 categories highlighted. For the top example cat-\negories, the individual images are sorted by ascending rank of the correct classi\ufb01cation\nof the recolorizeed image, with tiebreakers on descending rank of the correct classi\ufb01ca-\ntion of the grayscale image. For the bottom example categories, the images are sorted\nin reverse, in order to highlight the instances when recolorization results in an errant\nclassi\ufb01cation relative to the grayscale image.\nColorful Image Colorization\n19\nGreen  \nsnake (6)\nOrange (9)\nGold\ufb01nch (10)\nLorikeet (2)\nRapeseed (1)\nPomegranate (5)\nRock beauty (26)\nJelly\ufb01sh (43)\nMilitary (-1)\nModem (-6)\nGrey fox (-26)\nSweatshirt (-15)\nFig. 9. Images colorized by our algorithm from selected categories. Categories are\nsorted by VGG object classi\ufb01cation accuracy of our colorized images relative to ac-\ncuracy on gracyscale images. Top: example categories where our colorization helps the\nmost. Bottom: example categories where our colorization hurts the most. Number in\nparentheses indicates category rank amongst all 1000. Notice that the categories most\na\ufb00ected by colorization are those for which color information is highly diagnostic, such\nas birds and fruits. The bottom examples show several kinds of failures: 1) arti\ufb01cial\nobjects such as modems and clothes have ambiguous colors; color is not very infor-\nmative for classi\ufb01cation, and moreover, our algorithm tends to predict an incoherent\ndistribution of red and blue, 2) for certain categories, like the gray fox, our algorithm\nsystematically predicts the wrong color, confusing the species.\n6.2\nCommon Confusions\nTo further investigate the biases in our system, we look at the common classi\ufb01cation\nconfusions that often occur after image recolorization, but not with the original ground\ntruth image. Examples for some top confusions are shown in Figure 10. An image of a\n\u201cminibus\u201d is often colored yellow, leading to a misclassi\ufb01cation as \u201cschool bus\u201d. Animal\nclasses are sometimes colored di\ufb00erently than ground truth, leading to misclassi\ufb01cation\nto related species. Note that the colorizations are often visually realistic, even though\nthey lead to a misclassi\ufb01cation.\nTo \ufb01nd common confusions, we compute the rate of top-5 confusion Corig, Crecolor \u2208\n[0, 1]C\u00d7C, with ground truth colors and after recolorization. A value of Cc,d = 1 means\nthat every image in category c was classi\ufb01ed as category d in the top-5. We \ufb01nd the\nclass-confusion added after recolorization by computing A = Crecolor \u2212Corig, and\nsort the o\ufb00-diagonal entries. Figure 11(b) shows all C \u00d7 (C \u22121) o\ufb00-diagonal entries\nof Crecolor vs Corig, with the top 100 entries from A highlighted. For each category\npair (c, d), we extract the images that contained the confusion after recolorization, but\n20\nZhang, Isola, Efros\nnot with the original colorization. We then sort the images in descending order of the\nclassi\ufb01cation score of the confused category.\nFig. 10. Examples of some most-confused categories. Top rows show ground truth im-\nage. Bottom rows show recolorized images. Rank of common confusion in parentheses.\nGround truth and confused categories after recolorization are labeled.\nColorful Image Colorization\n21\nFig. 11. (a) Performance of VGG top-5 classi\ufb01cation on recolorized images vs grayscale\nimages per category (b) Top-5 confusion rates with recolorizations and original colors.\nTest was done on last 48,000 images in ImageNet validation set.\n7\nIs the network exploiting low-level cues?\nUnlike many computer vision tasks that can be roughly categorized as low, mid or\nhigh-level vision, color prediction requires understanding an image at both the pixel\nand the semantic-level. We have investigated how colorization generalizes to high-level\nsemantic tasks in Section 3.2. Studies of natural image statistics have shown that the\nlightness value of a single pixel can highly constrain the likely color of that pixel: darker\nlightness values tend to be correlated with more saturated colors [44].\nCould our network be exploiting a simple, low-level relationship like this, in order to\npredict color?4 We tested this hypothesis with the simple demonstration in Figure 12.\nGiven a grayscale Macbeth color chart as input, our network was unable to recover\nits colors. This is true, despite the fact that the lightness values vary considerably for\nthe di\ufb00erent color patches in this image. On the other hand, given two recognizable\nvegetables that are roughly isoluminant, the system is able to recover their color.\nIn Figure 12, we also demonstrate that the prediction is somewhat stable with\nrespect to low-level lightness and contrast changes. Blurring, on the other hand, has a\nbigger e\ufb00ect on the predictions in this example, possibly because the operation removes\nthe diagnostic texture pattern of the zucchini.\n8\nDoes our model learn multimodal color distributions?\nAs discussed in Section 2.1, formulating color prediction as a multinomial classi\ufb01cation\nproblem allows the system to predict multimodal distributions, and can capture the\ninherent ambiguity in the color of natural objects. In Figure 13, we illustrate the\n4 E.g., previous work showed that CNNs can learn to use chromatic aberration cues\nto predict, given an image patch, its (x,y) location within an image [14].\n22\nZhang, Isola, Efros\nFig. 12. Left: pixel lightness on its own does not reveal color, as shown by the color\nchart. In contrast, two vegetables that are nearly isoluminant are recognized as having\ndi\ufb00erent colors. Right: stability of the network predictions with respect to low-level\nimage transformations.\nprobability outputs bZ and demonstrate that the network does indeed learn multimodal\ndistributions. The system output bY is shown in the top-left of Figure 13. Each block\nillustrates the probability map bZq \u2208[0, 1]H,W given ab bin q in the output space. For\nclarity, we show a subsampling of the Q total output bins and coarsely quantize the\nprobability values. In Figure 13(a), the system clearly predicts a di\ufb00erent distribution\nfor the background vegetation and the foreground bird. The background is predicted\nto be green, yellow, or brown, while the foreground bird is predicted to be red or\nblue. Figure 13(b) shows that oranges can be predicted to be di\ufb00erent colors. Lastly,\nin Figure 13(c), the man\u2019s sarong is predicted to be either red, pink, or purple, while\nhis shirt is classi\ufb01ed as turquoise, cyan or light orange. Note that despite the multi-\nmodality of the prediction, taking the annealed-mean of the distribution produces a\nspatially consistent prediction.\n9\nNetwork architecture\nFigure 2 showed a diagram of our network architecture. Table 4 in this document thor-\noughly lists the layers used in our architecture during training time. During testing,\nthe temperature adjustment, softmax, mean, and bilinear upsampling are all imple-\nmented as subsequent layers in a feed-forward network. Note the column showing the\nColorful Image Colorization\n23\nFig. 13. The output probability distributions per image. The top-left image is \ufb01nal\nprediction of our system. The black sub-images are quantized blocks of the ab gamut.\nHigh probabilities are shown as higher luminance and are quantized for clarity. (a)\nBackground of bird is predicted to be green or brown. Foreground bird has distri-\nbution across blue and red colors. (b) Oranges are predicted to be di\ufb00erent colors.\n(c) The person\u2019s shirt and sarong has uncertainty across turqoise/cyan/orange and\nred/pink/purple colors, respectively. Note that despite the multimodality of the per-\npixel distributions, the results after taking the annealed-mean are typically spatially\nconsistent.\ne\ufb00ective dilation. The e\ufb00ective dilation is the spacing at which consecutive elements\nof the convolutional kernel are evaluated, relative to the input pixels, and is computed\nby the product of the accumulated stride and the layer dilation. Through each convo-\nlutional block from conv1 to conv5, the e\ufb00ective dilation of the convolutional kernel is\nincreased. From conv6 to conv8, the e\ufb00ective dilation is decreased.\n10\nColorization comparisons on held-out datasets\n10.1\nComparison to LEARCH [22]\nThough our model was trained on object-centric ImageNet dataset, we demonstrate\nthat it nonetheless remains e\ufb00ective for photos from the scene-centric SUN dataset [45]\nselected by Deshpande et al. [22]. Deshpande et al. recently established a benchmark\nfor colorization using a subset of the SUN dataset and reported top results using an\nalgorithm based on LEARCH [46]. Table 5 provides a quantitative comparison of our\nmethod to Deshpande et al.. For fair comparison, we use the same grayscale input\nas [22], which is\nR+G+B\n3\n. Note that this input space is non-linearly related to the\nL channel on which we trained. Despite di\ufb00erences in grayscale space and training\ndataset, our method outperforms Deshpande et al. in both the raw accuracy AuC\nCMF and perceptual realism AMT metrics. Figure 14 shows qualitative comparisons\nbetween our method and Deshpande et al., one from each of the six scene categories.\nA complete comparison on all 240 images are included in the supplementary material.\nOur results are able to fool participants in the real vs. fake task 17.2% of the time,\nsigni\ufb01cantly higher than Deshpande et al. at 9.8%.\n10.2\nComparison to Deep Colorization [1]\nWe provide qualitative comparisons to the 23 test images in [1] on the website, which\nwe obtained by manually cropping from the paper. Our results are about the same\n24\nZhang, Isola, Efros\nLayer\nX\nC\nS\nD\nSa\nDe\nBN\nL\ndata\n224\n3\n-\n-\n-\n-\n-\n-\nconv1 1\n224\n64\n1\n1\n1\n1\n-\n-\nconv1 2\n112\n64\n2\n1\n1\n1\n\u2713\n-\nconv2 1\n112\n128\n1\n1\n2\n2\n-\n-\nconv2 1\n56\n128\n2\n1\n2\n2\n\u2713\n-\nconv3 1\n56\n256\n1\n1\n4\n4\n-\n-\nconv3 2\n56\n256\n1\n1\n4\n4\n-\n-\nconv3 3\n28\n256\n2\n1\n4\n4\n\u2713\n-\nconv4 1\n28\n512\n1\n1\n8\n8\n-\n-\nconv4 2\n28\n512\n1\n1\n8\n8\n-\n-\nconv4 3\n28\n512\n1\n1\n8\n8\n\u2713\n-\nconv5 1\n28\n512\n1\n2\n8\n16\n-\n-\nconv5 2\n28\n512\n1\n2\n8\n16\n-\n-\nconv5 3\n28\n512\n1\n2\n8\n16\n\u2713\n-\nconv6 1\n28\n512\n1\n2\n8\n16\n-\n-\nconv6 2\n28\n512\n1\n2\n8\n16\n-\n-\nconv6 3\n28\n512\n1\n2\n8\n16\n\u2713\n-\nconv7 1\n28\n256\n1\n1\n8\n8\n-\n-\nconv7 2\n28\n256\n1\n1\n8\n8\n-\n-\nconv7 3\n28\n256\n1\n1\n8\n8\n\u2713\n-\nconv8 1\n56\n128\n.5\n1\n4\n4\n-\n-\nconv8 2\n56\n128\n1\n1\n4\n4\n-\n-\nconv8 3\n56\n128\n1\n1\n4\n4\n-\n\u2713\nTable 4. Our network architecture. X spatial resolution of output, C number of chan-\nnels of output; S computation stride, values greater than 1 indicate downsampling\nfollowing convolution, values less than 1 indicate upsampling preceding convolution;\nD kernel dilation; Sa accumulated stride across all preceding layers (product over all\nstrides in previous layers); De e\ufb00ective dilation of the layer with respect to the input\n(layer dilation times accumulated stride); BN whether BatchNorm layer was used after\nlayer; L whether a 1x1 conv and cross-entropy loss layer was imposed\nqualitative level as [1]. Note that Deep Colorization [1] has several advantages in this\nsetting: (1) the test images are from the SUN dataset [47], which we did not train on and\n(2) the 23 images were hand-selected from 1344 by the authors, and is not necessarily\nrepresentative of algorithm performance. We were unable to obtain the 1344 test set\nresults through correspondence with the authors.\nAdditionally, we compare the methods on several important dimensions in Table 6:\nalgorithm pipeline, learning, dataset, and run-time. Our method is faster, straightfor-\nward to train and understand, has fewer hand-tuned parameters and components, and\nhas been demonstrated on a broader and more diverse set of test images than Deep\nColorization [1].\n10.3\nAdditional Examples on Legacy Grayscale Images\nHere, we show additional qualitative examples of applying our model to legacy black\nand white photographs. Figures 16, 17, and 18 show examples including work of\nrenowned photographers, such as Ansel Adams and Henri Cartier-Bresson, photographs\nof politicians and celebrities, and old family photos. One can see that our model is often\nable to produce good colorizations, even though the low-level image statistics of old\nlegacy photographs are quite di\ufb00erent from those of modern-day photos.\nColorful Image Colorization\n25\nResults on LEARCH [22] dataset\nAuC\nAMT\nAlgorithm\nCMF\nLabeled\n(%)\nReal (%)\nOurs\n90.1 17.2\u00b11.9\nDeshpande et al. [22] 88.8\n9.8\u00b11.5\nGrayscale\n89.3\n\u2013\nGround Truth\n100\n50\nTable 5. Results on LEARCH [22] test set, containing 240 images from 6 categories\nbeach, outdoor, castle, bedroom, kitchen, and living room. Results column 1 shows the\nAuC of thresholded CMF over ab space. Results column 2 are from our AMT real vs.\nfake test.\nFig. 14. CMF on the LEARCH [22] test set\nDeep Colorization [1]\nOurs\n(1) Extract feature sets\n(a) 7x7 patch (b) DAISY\nAlgorithm\n(c) FCN on 47 categories\nFeed-forward CNN\n(2) 3-layer NN regressor\n(3) Joint-bilateral \ufb01lter\nExtract features. Train FCN [42]\nTrain CNN from pixels to\nLearning\non pre-de\ufb01ned categories.\ncolor distribution. Tune single\nTrain 3-layer NN regressor.\nparameter on validation.\n2688/1344 images from\n1.3M/10k images from\nDataset\nSUN [47] for train/test.\nImageNet [28] for train/test.\nLimited variety with\nBroad and diverse\nonly scenes.\nset of objects and scenes.\n4.9s/image on\n21.1ms/image in Ca\ufb00e\nRun-time\nMatlab implementation\non K40 GPU\nTable 6. Comparison to Deep Colorization [1]\n26\nZhang, Isola, Efros\nGround truth\nDeshpande  \net al. 2015\nOurs\nInput\nBeach\nBedroom\nCastle\nKitchen\nLiving room\nOutdoor\nFig. 15. Our model generalizes well to datasets on which it was not trained. Here\nwe show results on the dataset from [22], which consists of six scene categories from\nSUN [45]. Compared to the state of the art algorithm on this dataset [22], our method\nproduces more perceptually plausible colorization (see also Table 5 and Figure 14).\nPlease visit http://richzhang.github.io/colorization/ to see the results on all\n240 images.\nColorful Image Colorization\n27\nFig. 16. Applying our method to black and white photographs by Ansel Adams.\n28\nZhang, Isola, Efros\nFig. 17. Applying our method to black and white photographs by Henri Cartier-\nBresson.\nColorful Image Colorization\n29\nFig. 18. Applying our method to legacy black and white photographs. Top to bottom,\nleft to right: photo of Elvis Presley, photo of Migrant Mother by Dorothea Lange, photo\nof Marilyn Monroe, an amateur family photo, photo by Henri Cartier-Bresson, photo\nby Dr. David Fleay of Benjamin, the last captive thylacine which went extinct in 1936.\n",
        "sentence": " We compare our model with several self-supervised approaches (Wang & Gupta, 2015; Doersch et al., 2015; Zhang et al., 2016) and an unsupervised approach, i.",
        "context": "vious [14,8,15,10] and concurrent [16] self-supervision algorithms, and \ufb01nd that\nour method performs surprisingly well, achieving state-of-the-art performance\non several metrics.\nOur contributions in this paper are in two areas. First, we make progress\nlowing the procedure in [36]. Doersch et al. [14] achieves 51.1%, while we reach\n46.9% and 47.9% with grayscale and color inputs, respectively. Our method is\nwell above the strong k-means [36] baseline of 45.6%, but all self-supervised meth-\ndataset, our method outperforms Deshpande et al. in both the raw accuracy AuC\nCMF and perceptual realism AMT metrics. Figure 14 shows qualitative comparisons\nbetween our method and Deshpande et al., one from each of the six scene categories."
    },
    {
        "title": "Stacked What-Where Auto-encoders",
        "author": [
            "J. Zhao",
            "M. Mathieu",
            "R. Goroshin",
            "Y. LeCun"
        ],
        "venue": "In Workshop at ICLR,",
        "citeRegEx": "Zhao et al\\.,? \\Q2016\\E",
        "shortCiteRegEx": "Zhao et al\\.",
        "year": 2016,
        "abstract": "",
        "full_text": "",
        "sentence": " , 2007) or a deconvolutional network (Masci et al., 2011; Zhao et al., 2016) but can be more sophisticated, like a PixelCNN network (van den Oord et al.",
        "context": null
    }
]