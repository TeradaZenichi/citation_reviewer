[
    {
        "title": "Taming the monster: A fast and simple algorithm for contextual bandits",
        "author": [
            "Alekh Agarwal",
            "Daniel Hsu",
            "Satyen Kale",
            "John Langford",
            "Lihong Li",
            "Robert E Schapire"
        ],
        "venue": "In International Conference on Machine Learning,",
        "citeRegEx": "Agarwal et al\\.,? \\Q2014\\E",
        "shortCiteRegEx": "Agarwal et al\\.",
        "year": 2014,
        "abstract": "We present a new algorithm for the contextual bandit learning problem, where\nthe learner repeatedly takes one of $K$ actions in response to the observed\ncontext, and observes the reward only for that chosen action. Our method\nassumes access to an oracle for solving fully supervised cost-sensitive\nclassification problems and achieves the statistically optimal regret guarantee\nwith only $\\tilde{O}(\\sqrt{KT/\\log N})$ oracle calls across all $T$ rounds,\nwhere $N$ is the number of policies in the policy class we compete against. By\ndoing so, we obtain the most practical contextual bandit learning algorithm\namongst approaches that work for general policy classes. We further conduct a\nproof-of-concept experiment which demonstrates the excellent computational and\nprediction performance of (an online variant of) our algorithm relative to\nseveral baselines.",
        "full_text": "arXiv:1402.0555v2  [cs.LG]  14 Oct 2014\nTaming the Monster:\nA Fast and Simple Algorithm for Contextual Bandits\nAlekh Agarwal1, Daniel Hsu2, Satyen Kale3, John Langford1, Lihong Li1, and\nRobert E. Schapire1,4\n1Microsoft Research\n2Columbia University\n3Yahoo! Labs\n4Princeton University\nOctober 15, 2014\nAbstract\nWe present a new algorithm for the contextual bandit learning problem, where the learner repeat-\nedly takes one of K actions in response to the observed context, and observes the reward only for that\nchosen action. Our method assumes access to an oracle for solving fully supervised cost-sensitive clas-\nsi\ufb01cation problems and achieves the statistically optimal regret guarantee with only \u02dcO(\np\nKT/ log N)\noracle calls across all T rounds, where N is the number of policies in the policy class we compete\nagainst. By doing so, we obtain the most practical contextual bandit learning algorithm amongst\napproaches that work for general policy classes. We further conduct a proof-of-concept experiment\nwhich demonstrates the excellent computational and prediction performance of (an online variant of)\nour algorithm relative to several baselines.\n1\nIntroduction\nIn the contextual bandit problem, an agent collects rewards for actions taken over a sequence of rounds;\nin each round, the agent chooses an action to take on the basis of (i) context (or features) for the current\nround, as well as (ii) feedback, in the form of rewards, obtained in previous rounds. The feedback is\nincomplete: in any given round, the agent observes the reward only for the chosen action; the agent\ndoes not observe the reward for other actions. Contextual bandit problems are found in many important\napplications such as online recommendation and clinical trials, and represent a natural half-way point\nbetween supervised learning and reinforcement learning. The use of features to encode context is inherited\nfrom supervised machine learning, while exploration is necessary for good performance as in reinforcement\nlearning.\nThe choice of exploration distribution on actions is important. The strongest known results (Auer et al.,\n2002; McMahan and Streeter, 2009; Beygelzimer et al., 2011) provide algorithms that carefully control\nthe exploration distribution to achieve an optimal regret after T rounds of\nO\n\u0010p\nKT log(|\u03a0|/\u03b4)\n\u0011\n,\nwith probability at least 1 \u2212\u03b4, relative to a set of policies \u03a0 \u2286AX mapping contexts x \u2208X to actions\na \u2208A (where K is the number of actions). The regret is the di\ufb00erence between the cumulative reward of\nthe best policy in \u03a0 and the cumulative reward collected by the algorithm. Because the bound has a mild\nlogarithmic dependence on |\u03a0|, the algorithm can compete with very large policy classes that are likely\n1\nto yield high rewards, in which case the algorithm also earns high rewards. However, the computational\ncomplexity of the above algorithms is linear in |\u03a0|, making them tractable for only simple policy classes.\nA sub-linear in |\u03a0| running time is possible for policy classes that can be e\ufb03ciently searched. In\nthis work, we use the abstraction of an optimization oracle to capture this property: given a set of con-\ntext/reward vector pairs, the oracle returns a policy in \u03a0 with maximum total reward. Using such an ora-\ncle in an i.i.d. setting (formally de\ufb01ned in Section 2.1), it is possible to create \u01eb-greedy (Sutton and Barto,\n1998) or epoch-greedy (Langford and Zhang, 2007) algorithms that run in time O(log |\u03a0|) with only\na single call to the oracle per round.\nHowever, these algorithms have suboptimal regret bounds of\nO((K log |\u03a0|)1/3T 2/3) because the algorithms randomize uniformly over actions when they choose to\nexplore.\nThe Randomized UCB algorithm of Dud\u00b4\u0131k et al. (2011a) achieves the optimal regret bound (up to loga-\nrithmic factors) in the i.i.d. setting, and runs in time poly(T, log |\u03a0|) with \u02dcO(T 5) calls to the optimization\noracle per round. Naively this would amount to \u02dcO(T 6) calls to the oracle over T rounds, although a dou-\nbling trick from our analysis can be adapted to ensure only \u02dcO(T 5) calls to the oracle are needed over all\nT rounds in the Randomized UCB algorithm. This is a fascinating result because it shows that the oracle\ncan provide an exponential speed-up over previous algorithms with optimal regret bounds. However, the\nrunning time of this algorithm is still prohibitive for most natural problems owing to the \u02dcO(T 5) scaling.\nIn this work, we prove the following1:\nTheorem 1. There is an algorithm for the i.i.d. contextual bandit problem with an optimal regret bound\nrequiring \u02dcO\n\u0010q\nKT\nln(|\u03a0|/\u03b4)\n\u0011\ncalls to the optimization oracle over T rounds, with probability at least 1 \u2212\u03b4.\nConcretely, we make \u02dcO(\np\nKT/ ln(|\u03a0|/\u03b4)) calls to the oracle with a net running time of \u02dcO(T 1.5p\nK log |\u03a0|),\nvastly improving over the complexity of Randomized UCB. The major components of the new algorithm\nare (i) a new coordinate descent procedure for computing a very sparse distribution over policies which\ncan be e\ufb03ciently sampled from, and (ii) a new epoch structure which allows the distribution over poli-\ncies to be updated very infrequently. We consider variants of the epoch structure that make di\ufb00erent\ncomputational trade-o\ufb00s; on one extreme we concentrate the entire computational burden on O(log T )\nrounds with \u02dcO(\np\nKT/ ln(|\u03a0|/\u03b4)) oracle calls each time, while on the other we spread our computation\nover\n\u221a\nT rounds with \u02dcO(\np\nK/ ln(|\u03a0|/\u03b4)) oracle calls for each of these rounds. We stress that in either\ncase, the total number of calls to the oracle is only sublinear in T . Finally, we develop a more e\ufb03cient\nonline variant, and conduct a proof-of-concept experiment showing low computational complexity and\nhigh reward relative to several natural baselines.\nMotivation and related work.\nThe EXP4-family of algorithms (Auer et al., 2002; McMahan and Streeter,\n2009; Beygelzimer et al., 2011) solve the contextual bandit problem with optimal regret by updating\nweights (multiplicatively) over all policies in every round. Except for a few special cases (Helmbold and Schapire,\n1997; Beygelzimer et al., 2011), the running time of such measure-based algorithms is generally linear in\nthe number of policies.\nIn contrast, the Randomized UCB algorithm of Dud\u00b4\u0131k et al. (2011a) is based on a natural abstraction\nfrom supervised learning\u2014the ability to e\ufb03ciently \ufb01nd a function in a rich function class that minimizes\nthe loss on a training set.\nThis abstraction is encapsulated in the notion of an optimization oracle,\nwhich is also useful for \u01eb-greedy (Sutton and Barto, 1998) and epoch-greedy (Langford and Zhang, 2007)\nalgorithms. However, these latter algorithms have only suboptimal regret bounds.\nAnother class of approaches based on Bayesian updating is Thompson sampling (Thompson, 1933; Li,\n2013), which often enjoys strong theoretical guarantees in expectation over the prior and good empirical\nperformance (Chapelle and Li, 2011). Such algorithms, as well as the closely related upper-con\ufb01dence\nbound algorithms (Auer, 2002; Chu et al., 2011), are computationally tractable in cases where the pos-\nterior distribution over policies can be e\ufb03ciently maintained or approximated. In our experiments, we\ncompare to a strong baseline algorithm that uses this approach (Chu et al., 2011).\n1Throughout this paper, we use the \u02dcO notation to suppress dependence on logarithmic factors in T and K, as well as\nlog(|\u03a0|/\u03b4) (i.e. terms which are O(log log(|\u03a0|/\u03b4)).\n2\nTo circumvent the \u2126(|\u03a0|) running time barrier, we restrict attention to algorithms that only access the\npolicy class via the optimization oracle. Speci\ufb01cally, we use a cost-sensitive classi\ufb01cation oracle, and a key\nchallenge is to design good supervised learning problems for querying this oracle. The Randomized UCB\nalgorithm of Dud\u00b4\u0131k et al. (2011a) uses a similar oracle to construct a distribution over policies that solves\na certain convex program. However, the number of oracle calls in their work is prohibitively large, and\nthe statistical analysis is also rather complex.2\nMain contributions.\nIn this work, we present a new and simple algorithm for solving a similar convex\nprogram as that used by Randomized UCB. The new algorithm is based on coordinate descent: in each\niteration, the algorithm calls the optimization oracle to obtain a policy; the output is a sparse distribution\nover these policies. The number of iterations required to compute the distribution is small\u2014at most\n\u02dcO(\np\nKt/ ln(|\u03a0|/\u03b4)) in any round t. In fact, we present a more general scheme based on epochs and warm\nstart in which the total number of calls to the oracle is, with high probability, just \u02dcO(\np\nKT/ ln(|\u03a0|/\u03b4))\nover all T rounds; we prove that this is nearly optimal for a certain class of optimization-based algorithms.\nThe algorithm is natural and simple to implement, and we provide an arguably simpler analysis than that\nfor Randomized UCB. Finally, we report proof-of-concept experimental results using a variant algorithm\nshowing strong empirical performance.\n2\nPreliminaries\nIn this section, we recall the i.i.d. contextual bandit setting and some basic techniques used in previous\nworks (Auer et al., 2002; Beygelzimer et al., 2011; Dud\u00b4\u0131k et al., 2011a).\n2.1\nLearning Setting\nLet A be a \ufb01nite set of K actions, X be a space of possible contexts (e.g., a feature space), and \u03a0 \u2286AX\nbe a \ufb01nite set of policies that map contexts x \u2208X to actions a \u2208A.3 Let \u2206\u03a0 := {Q \u2208R\u03a0 : Q(\u03c0) \u2265\n0 \u2200\u03c0 \u2208\u03a0, P\n\u03c0\u2208\u03a0 Q(\u03c0) \u22641} be the set of non-negative weights over policies with total weight at most\none, and let RA\n+ := {r \u2208RA : r(a) \u22650 \u2200a \u2208A} be the set of non-negative reward vectors.\nLet D be a probability distribution over X \u00d7[0, 1]A, the joint space of contexts and reward vectors; we\nassume actions\u2019 rewards from D are always in the interval [0, 1]. Let DX denote the marginal distribution\nof D over X.\nIn the i.i.d. contextual bandit setting, the context/reward vector pairs (xt, rt) \u2208X \u00d7 [0, 1]A over all\nrounds t = 1, 2, . . . are randomly drawn independently from D. In round t, the agent \ufb01rst observes the\ncontext xt, then (randomly) chooses an action at \u2208A, and \ufb01nally receives the reward rt(at) \u2208[0, 1]\nfor the chosen action. The (observable) record of interaction resulting from round t is the quadruple\n(xt, at, rt(at), pt(at)) \u2208X \u00d7 A \u00d7 [0, 1] \u00d7 [0, 1]; here, pt(at) \u2208[0, 1] is the probability that the agent chose\naction at \u2208A. We let Ht \u2286X \u00d7 A \u00d7 [0, 1] \u00d7 [0, 1] denote the history (set) of interaction records in the\n\ufb01rst t rounds. We use the shorthand notation bEx\u223cHt[\u00b7] to denote expectation when a context x is chosen\nfrom the t contexts in Ht uniformly at random.\nLet R(\u03c0) := E(x,r)\u223cD[r(\u03c0(x))] denote the expected (instantaneous) reward of a policy \u03c0 \u2208\u03a0, and\nlet \u03c0\u22c6:= arg max\u03c0\u2208\u03a0 R(\u03c0) be a policy that maximizes the expected reward (the optimal policy). Let\nReg(\u03c0) := R(\u03c0\u22c6) \u2212R(\u03c0) denote the expected (instantaneous) regret of a policy \u03c0 \u2208\u03a0 relative to the\noptimal policy. Finally, the (empirical cumulative) regret of the agent after T rounds4 is de\ufb01ned as\nT\nX\nt=1\n\u0000rt(\u03c0\u22c6(xt)) \u2212rt(at)\n\u0001\n.\n2The paper of Dud\u00b4\u0131k et al. (2011a) is colloquially referred to, by its authors, as the \u201cmonster paper\u201d (Langford, 2014).\n3Extension to VC classes is simple using standard arguments.\n4We have de\ufb01ned empirical cumulative regret as being relative to \u03c0\u22c6, rather than to the empirical reward maximizer\narg max\u03c0\u2208\u03a0\nPT\nt=1 rt(\u03c0(xt)).\nHowever, in the i.i.d. setting, the two do not di\ufb00er by more than O(\np\nT ln(|\u03a0|/\u03b4)) with\nprobability at least 1 \u2212\u03b4.\n3\n2.2\nInverse Propensity Scoring\nAn unbiased estimate of a policy\u2019s reward may be obtained from a history of interaction records Ht using\ninverse propensity scoring (IPS; also called inverse probability weighting): the expected reward of policy\n\u03c0 \u2208\u03a0 is estimated as\nbRt(\u03c0) := 1\nt\nt\nX\ni=1\nri(ai) \u00b7 1{\u03c0(xi) = ai}\npi(ai)\n.\n(1)\nThis technique can be viewed as mapping Ht 7\u2192IPS(Ht) of interaction records (x, a, r(a), p(a)) to con-\ntext/reward vector pairs (x, \u02c6r), where \u02c6r \u2208RA\n+ is a \ufb01ctitious reward vector that assigns to the chosen\naction a a scaled reward r(a)/p(a) (possibly greater than one), and assigns to all other actions zero\nrewards. This transformation IPS(Ht) is detailed in Algorithm 3 (in Appendix A); we may equivalently\nde\ufb01ne bRt by bRt(\u03c0) := t\u22121 P\n(x,\u02c6r)\u2208IPS(Ht) \u02c6r(\u03c0(x)). It is easy to verify that E[\u02c6r(\u03c0(x))|(x, r)] = r(\u03c0(x)), as\np(a) is indeed the agent\u2019s probability (conditioned on (x, r)) of picking action a. This implies bRt(\u03c0) is\nan unbiased estimator for any history Ht.\nLet \u03c0t := arg max\u03c0\u2208\u03a0 bRt(\u03c0) denote a policy that maximizes the expected reward estimate based\non inverse propensity scoring with history Ht (\u03c00 can be arbitrary), and let d\nRegt(\u03c0) := bRt(\u03c0t) \u2212bRt(\u03c0)\ndenote estimated regret relative to \u03c0t. Note that d\nRegt(\u03c0) is generally not an unbiased estimate of Reg(\u03c0),\nbecause \u03c0t is not always \u03c0\u22c6.\n2.3\nOptimization Oracle\nOne natural mode for accessing the set of policies \u03a0 is enumeration, but this is impractical in general.\nIn this work, we instead only access \u03a0 via an optimization oracle which corresponds to a cost-sensitive\nlearner. Following Dud\u00b4\u0131k et al. (2011a), we call this oracle AMO5.\nDe\ufb01nition 1. For a set of policies \u03a0, the arg max oracle (AMO) is an algorithm, which for any sequence\nof context and reward vectors, (x1, r1), (x2, r2), . . . , (xt, rt) \u2208X \u00d7 RA\n+, returns\narg max\n\u03c0\u2208\u03a0\nt\nX\n\u03c4=1\nr\u03c4(\u03c0(x\u03c4)).\n2.4\nProjections and Smoothing\nIn each round, our algorithm chooses an action by randomly drawing a policy \u03c0 from a distribution over\n\u03a0, and then picking the action \u03c0(x) recommended by \u03c0 on the current context x. This is equivalent\nto drawing an action according to Q(a|x) := P\n\u03c0\u2208\u03a0:\u03c0(x)=a Q(\u03c0), \u2200a \u2208A. For keeping the variance of\nreward estimates from IPS in check, it is desirable to prevent the probability of any action from being\ntoo small.\nThus, as in previous work, we also use a smoothed projection Q\u00b5(\u00b7|x) for \u00b5 \u2208[0, 1/K],\nQ\u00b5(a|x) := (1 \u2212K\u00b5) P\n\u03c0\u2208\u03a0:\u03c0(x)=a Q(\u03c0) + \u00b5, \u2200a \u2208A.\nEvery action has probability at least \u00b5 under\nQ\u00b5(\u00b7|x).\nFor technical reasons, our algorithm maintains non-negative weights Q \u2208\u2206\u03a0 over policies that sum\nto at most one, but not necessarily equal to one; hence, we put any remaining mass on a default policy\n\u00af\u03c0 \u2208\u03a0 to obtain a legitimate probability distribution over policies \u02dcQ = Q +\n\u00001 \u2212P\n\u03c0\u2208\u03a0 Q(\u03c0)\n\u0001\n1\u00af\u03c0. We\nthen pick an action from the smoothed projection \u02dcQ\u00b5(\u00b7|x) of \u02dcQ as above.\nThis sampling procedure\nSample(x, Q, \u00af\u03c0, \u00b5) is detailed in Algorithm 4 (in Appendix A).\n3\nAlgorithm and Main Results\nOur algorithm (ILOVETOCONBANDITS) is an epoch-based variant of the Randomized UCB algorithm of\nDud\u00b4\u0131k et al. (2011a) and is given in Algorithm 1. Like Randomized UCB, ILOVETOCONBANDITS solves\n5Cost-sensitive learners often need a cost instead of reward, in which case we use ct = 1 \u2212rt.\n4\nan optimization problem (OP) to obtain a distribution over policies to sample from (Step 7), but does\nso on an epoch schedule, i.e., only on certain pre-speci\ufb01ed rounds \u03c41, \u03c42, . . .. The only requirement of\nthe epoch schedule is that the length of epoch m is bounded as \u03c4m+1 \u2212\u03c4m = O(\u03c4m). For simplicity, we\nassume \u03c4m+1 \u22642\u03c4m for m \u22651, and \u03c41 = O(1).\nThe crucial step here is solving (OP). Before stating the main result, let us get some intuition about\nthis problem. The \ufb01rst constraint, Eq. (2), requires the average estimated regret of the distribution Q\nover policies to be small, since b\u03c0 is a rescaled version of the estimated regret of policy \u03c0. This constraint\nskews our distribution to put more mass on \u201cgood policies\u201d (as judged by our current information), and\ncan be seen as the exploitation component of our algorithm. The second set of constraints, Eq. (3),\nrequires the distribution Q to place su\ufb03cient mass on the actions chosen by each policy \u03c0, in expectation\nover contexts. This can be thought of as the exploration constraint, since it requires the distribution to\nbe su\ufb03ciently diverse for most contexts. As we will see later, the left hand side of the constraint is a\nbound on the variance of our reward estimates for policy \u03c0, and the constraint requires the variance to\nbe controlled at the level of the estimated regret of \u03c0. That is, we require the reward estimates to be\nmore accurate for good policies than we do for bad ones, allowing for much more adaptive exploration\nthan the uniform exploration of \u01eb-greedy style algorithms.\nThis problem is very similar to the one in Dud\u00b4\u0131k et al. (2011a), and our coordinate descent algorithm\nin Section 3.1 gives a constructive proof that the problem is feasible. As in Dud\u00b4\u0131k et al. (2011a), we have\nthe following regret bound:\nTheorem 2. Assume the optimization problem ( OP) can be solved whenever required in Algorithm 1.\nWith probability at least 1 \u2212\u03b4, the regret of Algorithm 1 (ILOVETOCONBANDITS) after T rounds is\nO\n\u0010p\nKT ln(T |\u03a0|/\u03b4) + K ln(T |\u03a0|/\u03b4)\n\u0011\n.\nAlgorithm 1 Importance-weighted LOw-Variance Epoch-Timed Oracleized CONtextual BANDITS al-\ngorithm (ILOVETOCONBANDITS)\ninput Epoch schedule 0 = \u03c40 < \u03c41 < \u03c42 < \u00b7 \u00b7 \u00b7 , allowed failure probability \u03b4 \u2208(0, 1).\n1: Initial weights Q0 := 0 \u2208\u2206\u03a0, initial epoch m := 1.\nDe\ufb01ne \u00b5m := min{1/2K,\np\nln(16\u03c4 2m|\u03a0|/\u03b4)/(K\u03c4m)} for all m \u22650.\n2: for round t = 1, 2, . . . do\n3:\nObserve context xt \u2208X.\n4:\n(at, pt(at)) := Sample(xt, Qm\u22121, \u03c0\u03c4m\u22121, \u00b5m\u22121).\n5:\nSelect action at and observe reward rt(at) \u2208[0, 1].\n6:\nif t = \u03c4m then\n7:\nLet Qm be a solution to (OP) with history Ht and minimum probability \u00b5m.\n8:\nm := m + 1.\n9:\nend if\n10: end for\nOptimization Problem (OP)\nGiven a history Ht and minimum probability \u00b5m, de\ufb01ne b\u03c0 :=\nd\nRegt(\u03c0)\n\u03c8\u00b5m\nfor \u03c8 := 100, and \ufb01nd Q \u2208\u2206\u03a0\nsuch that\nX\n\u03c0\u2208\u03a0\nQ(\u03c0)b\u03c0 \u22642K\n(2)\n\u2200\u03c0 \u2208\u03a0 : bEx\u223cHt\n\u0014\n1\nQ\u00b5m(\u03c0(x)|x)\n\u0015\n\u22642K + b\u03c0.\n(3)\n5\n3.1\nSolving (OP) via Coordinate Descent\nWe now present a coordinate descent algorithm to solve (OP). The pseudocode is given in Algorithm 2.\nOur analysis, as well as the algorithm itself, are based on a potential function which we use to measure\nprogress. The algorithm can be viewed as a form of coordinate descent applied to this same potential\nfunction. The main idea of our analysis is to show that this function decreases substantially on every\niteration of this algorithm; since the function is nonnegative, this gives an upper bound on the total\nnumber of iterations as expressed in the following theorem.\nTheorem 3. Algorithm 2 (with Qinit := 0) halts in at most\n4 ln(1/(K\u00b5m))\n\u00b5m\niterations, and outputs a\nsolution Q to ( OP).\nAlgorithm 2 Coordinate Descent Algorithm\nRequire: History Ht, minimum probability \u00b5, initial weights Qinit \u2208\u2206\u03a0.\n1: Set Q := Qinit.\n2: loop\n3:\nDe\ufb01ne, for all \u03c0 \u2208\u03a0,\nV\u03c0(Q)\n=\nbEx\u223cHt[1/Q\u00b5(\u03c0(x)|x)]\nS\u03c0(Q)\n=\nbEx\u223cHt\n\u0002\n1/(Q\u00b5(\u03c0(x)|x))2\u0003\nD\u03c0(Q)\n=\nV\u03c0(Q) \u2212(2K + b\u03c0).\n4:\nif P\n\u03c0 Q(\u03c0)(2K + b\u03c0) > 2K then\n5:\nReplace Q by cQ, where\nc :=\n2K\nP\n\u03c0 Q(\u03c0)(2K + b\u03c0) < 1.\n(4)\n6:\nend if\n7:\nif there is a policy \u03c0 for which D\u03c0(Q) > 0 then\n8:\nAdd the (positive) quantity\n\u03b1\u03c0(Q) = V\u03c0(Q) + D\u03c0(Q)\n2(1 \u2212K\u00b5)S\u03c0(Q)\nto Q(\u03c0) and leave all other weights unchanged.\n9:\nelse\n10:\nHalt and output the current set of weights Q.\n11:\nend if\n12: end loop\n3.2\nUsing an Optimization Oracle\nWe now show how to implement Algorithm 2 via AMO (c.f. Section 2.3).\nLemma 1. Algorithm 2 can be implemented using one call to AMO before the loop is started, and one\ncall for each iteration of the loop thereafter.\nProof. At the very beginning, before the loop is started, we compute the best empirical policy so far, \u03c0t,\nby calling AMO on the sequence of historical contexts and estimated reward vectors; i.e., on (x\u03c4, \u02c6r\u03c4), for\n\u03c4 = 1, 2, . . . , t.\nNext, we show that each iteration in the loop of Algorithm 2 can be implemented via one call to AMO.\nGoing over the pseudocode, \ufb01rst note that operations involving Q in Step 4 can be performed e\ufb03ciently\nsince Q has sparse support. Note that the de\ufb01nitions in Step 3 don\u2019t actually need to be computed for\nall policies \u03c0 \u2208\u03a0, as long as we can identify a policy \u03c0 for which D\u03c0(Q) > 0. We can identify such a\npolicy using one call to AMO as follows.\n6\nFirst, note that for any policy \u03c0, we have\nV\u03c0(Q) = bEx\u223cHt\n\u0014\n1\nQ\u00b5(\u03c0(x)|x)\n\u0015\n= 1\nt\nt\nX\n\u03c4=1\n1\nQ\u00b5(\u03c0(x\u03c4)|x\u03c4),\nand\nb\u03c0 =\nd\nRegt(\u03c0)\n\u03c8\u00b5\n=\nbRt(\u03c0t)\n\u03c8\u00b5\n\u2212\n1\n\u03c8\u00b5t\nt\nX\n\u03c4=1\n\u02c6r\u03c4(\u03c0(x\u03c4)).\nNow consider the sequence of historical contexts and reward vectors, (x\u03c4, \u02dcr\u03c4) for \u03c4 = 1, 2, . . ., t, where\nfor any action a we de\ufb01ne\n\u02dcr\u03c4(a) := 1\nt\n\u0012\n\u03c8\u00b5\nQ\u00b5(a|x\u03c4) + \u02c6r\u03c4(a)\n\u0013\n.\n(5)\nIt is easy to check that\nD\u03c0(Q) = 1\n\u03c8\u00b5\nt\nX\n\u03c4=1\n\u02dcr\u03c4(\u03c0(x\u03c4)) \u2212\n \n2K +\nbRt(\u03c0t)\n\u03c8\u00b5\n!\n.\nSince 2K +\nb\nRt(\u03c0t)\n\u03c8\u00b5\nis a constant independent of \u03c0, we have\narg max\n\u03c0\u2208\u03a0 D\u03c0(Q) = arg max\n\u03c0\u2208\u03a0\nt\nX\n\u03c4=1\n\u02dcr\u03c4(\u03c0(x\u03c4)),\nand hence, calling AMO once on the sequence (x\u03c4, \u02dcr\u03c4) for \u03c4 = 1, 2, . . ., t, we obtain a policy that maximizes\nD\u03c0(Q), and thereby identify a policy for which D\u03c0(Q) > 0 whenever one exists.\n3.3\nEpoch Schedule\nRecalling the setting of \u00b5m in Algorithm 1, Theorem 3 shows that Algorithm 2 solves (OP) with\n\u02dcO(\np\nKt/ ln(|\u03a0|/\u03b4)) calls to AMO in round t. Thus, if we use the epoch schedule \u03c4m = m (i.e., run\nAlgorithm 2 in every round), then we get a total of \u02dcO(\np\nKT 3/ ln(|\u03a0|/\u03b4)) calls to AMO over all T rounds.\nThis number can be dramatically reduced using a more carefully chosen epoch schedule.\nLemma 2. For the epoch schedule \u03c4m := 2m\u22121, the total number of calls to AMO is \u02dcO(\np\nKT/ ln(|\u03a0|/\u03b4)).\nProof. The epoch schedule satis\ufb01es the requirement \u03c4m+1 \u22642\u03c4m. With this epoch schedule, Algorithm 2\nis run only O(log T ) times over T rounds, leading to \u02dcO(\np\nKT/ ln(|\u03a0|/\u03b4)) total calls to AMO over the\nentire period.\n3.4\nWarm Start\nWe now present a di\ufb00erent technique to reduce the number of calls to AMO.\nThis is based on the\nobservation that practically speaking, it seems terribly wasteful, at the start of a new epoch, to throw out\nthe results of all of the preceding computations and to begin yet again from nothing. Instead, intuitively,\nwe expect computations to be more moderate if we begin again where we left o\ufb00last, i.e., a \u201cwarm-start\u201d\napproach. Here, when Algorithm 2 is called at the end of epoch m, we use Qinit := Qm\u22121 (the previously\ncomputed weights) rather than 0.\nWe can combine warm-start with a di\ufb00erent epoch schedule to guarantee \u02dcO(\np\nKT/ ln(|\u03a0|/\u03b4)) total\ncalls to AMO, spread across O(\n\u221a\nT) calls to Algorithm 2.\nLemma 3. De\ufb01ne the epoch schedule (\u03c41, \u03c42) := (3, 5) and \u03c4m := m2 for m \u22653 (this satis\ufb01es \u03c4m+1 \u2264\n2\u03c4m). With high probability, the warm-start variant of Algorithm 1 makes \u02dcO(\np\nKT/ ln(|\u03a0|/\u03b4)) calls to\nAMO over T rounds and O(\n\u221a\nT) calls to Algorithm 2.\n7\n3.5\nComputational Complexity\nSo far, we have only considered computational complexity in terms of the number of oracle calls. However,\nthe reduction also involves the creation of cost-sensitive classi\ufb01cation examples, which must be accounted\nfor in the net computational cost. As observed in the proof of Lemma 1 (speci\ufb01cally Eq. (5)), this requires\nthe computation of the probabilities Q\u00b5(a|x\u03c4) for \u03c4 = 1, 2, . . . , t when the oracle has to be invoked at\nround t.\nAccording to Lemma 3, the support of the distribution Q at time t can be over at most\n\u02dcO(\np\nKt/ ln(|\u03a0|/\u03b4)) policies (same as the number of calls to AMO). This would suggest a computational\ncomplexity of \u02dcO(\np\nKt3/ ln(|\u03a0|/\u03b4)) for querying the oracle at time t, resulting in an overall computation\ncost scaling with T 2.\nWe can, however, do better with some natural bookkeeping. Observe that at the start of round t, the\nconditional distributions Q(a|xi) for i = 1, 2, . . . , t \u22121 can be represented as a table of size K \u00d7 (t \u22121),\nwhere rows and columns correspond to actions and contexts. Upon receiving the new example in round\nt, the corresponding t-th column can be added to this table in time K \u00b7|supp(Q)| = \u02dcO(K\np\nKt/ ln(|\u03a0|/\u03b4))\n(where supp(Q) \u2286\u03a0 denotes the support of Q), using the projection operation described in Section 2.4.\nHence the net cost of these updates, as a function of K and T , scales with as (KT )3/2. Furthermore,\nthe cost-sensitive examples needed for the AMO can be obtained by a simple table lookup now, since the\naction probabilities are directly available. This involves O(Kt) table lookups when the oracle is invoked\nat time t, and again results in an overall cost scaling as (KT )3/2. Finally, we have to update the table\nwhen the distribution Q is updated in Algorithm 2. If we \ufb01nd ourselves in the rescaling step 4, we can\nsimply store the constant c. When we enter step 8 of the algorithm, we can do a linear scan over the table,\nrescaling and incrementing the entries. This also resutls in a cost of O(Kt) when the update happens at\ntime t, resulting in a net scaling as (KT )3/2. Overall, we \ufb01nd that the computational complexity of our\nalgorithm, modulo the oracle running time, is \u02dcO(\np\n(KT )3/ ln(|\u03a0|/\u03b4)).\n3.6\nA Lower Bound on the Support Size\nAn attractive feature of the coordinate descent algorithm, Algorithm 2, is that the number of oracle calls\nis directly related to the number of policies in the support of Qm. Speci\ufb01cally, for the doubling schedule\nof Section 3.3, Theorem 3 implies that we never have non-zero weights for more than 4 ln(1/(K\u00b5m))\n\u00b5m\npolicies\nin epoch m. Similarly, the total number of oracle calls for the warm-start approach in Section 3.4 bounds\nthe total number of policies which ever have non-zero weight over all T rounds. The support size of the\ndistributions Qm in Algorithm 1 is crucial to the computational complexity of sampling an action (Step 4\nof Algorithm 1).\nIn this section, we demonstrate a lower bound showing that it is not possible to construct substantially\nsparser distributions that also satisfy the low-variance constraint (3) in the optimization problem (OP).\nTo formally de\ufb01ne the lower bound, \ufb01x an epoch schedule 0 = \u03c40 < \u03c41 < \u03c42 < \u00b7 \u00b7 \u00b7 and consider the\nfollowing set of non-negative vectors over policies:\nQm:={Q \u2208\u2206\u03a0 : Q satis\ufb01es Eq. (3) in round \u03c4m}.\n(The distribution Qm computed by Algorithm 1 is in Qm.) Recall that supp(Q) denotes the support of\nQ (the set of policies where Q puts non-zero entries). We have the following lower bound on |supp(Q)|.\nTheorem 4. For any epoch schedule 0 = \u03c40 < \u03c41 < \u03c42 < \u00b7 \u00b7 \u00b7 and any M \u2208N su\ufb03ciently large, there\nexists a distribution D over X \u00d7 [0, 1]A and a policy class \u03a0 such that, with probability at least 1 \u2212\u03b4,\ninf\nm\u2208N:\n\u03c4m\u2265\u03c4M/2\ninf\nQ\u2208Qm |supp(Q)| = \u2126\n s\nK\u03c4M\nln(|\u03a0|\u03c4M/\u03b4)\n!\n.\nThe proof of the theorem is deferred to Appendix E. In the context of our problem, this lower bound\nshows that the bounds in Lemma 2 and Lemma 3 are unimprovable, since the number of calls to AMO\nis at least the size of the support, given our mode of access to \u03a0.\n8\n4\nRegret Analysis\nIn this section, we outline the regret analysis for our algorithm ILOVETOCONBANDITS, with details\ndeferred to Appendix B and Appendix C.\nThe deviations of the policy reward estimates bRt(\u03c0) are controlled by (a bound on) the variance of\neach term in Eq. (1): essentially the left-hand side of Eq. (3) from (OP), except with bEx\u223cHt[\u00b7] replaced by\nEx\u223cDX[\u00b7]. Resolving this discrepancy is handled using deviation bounds, so Eq. (3) holds with Ex\u223cDX[\u00b7],\nwith worse right-hand side constants.\nThe rest of the analysis, which deviates from that of Randomized UCB, compares the expected regret\nReg(\u03c0) of any policy \u03c0 with the estimated regret d\nRegt(\u03c0) using the variance constraints Eq. (3):\nLemma 4 (Informally). With high probability, for each m such that \u03c4m \u2265\u02dcO(K log |\u03a0|), each round t in\nepoch m, and each \u03c0 \u2208\u03a0, Reg(\u03c0) \u22642d\nRegt(\u03c0) + O(K\u00b5m).\nThis lemma can easily be combined with the constraint Eq. (2) from (OP): since the weights Qm\u22121\nused in any round t in epoch m satisfy P\n\u03c0\u2208\u03a0 Qm\u22121(\u03c0)d\nReg\u03c4m\u22121(\u03c0) \u2264\u03c8 \u00b7 2K\u00b5\u03c4m\u22121, we obtain a bound\non the (conditionally) expected regret in round t using the above lemma: with high probability,\nX\n\u03c0\u2208\u03a0\neQm\u22121 Reg(\u03c0) \u2264O(K\u00b5m\u22121).\nSumming these terms up over all T rounds and applying martingale concentration gives the \ufb01nal regret\nbound in Theorem 2.\n5\nAnalysis of the Optimization Algorithm\nIn this section, we give a sketch of the analysis of our main optimization algorithm for computing weights\nQm on each epoch as in Algorithm 2. As mentioned in Section 3.1, this analysis is based on a potential\nfunction.\nSince our attention for now is on a single epoch m, here and in what follows, when clear from context,\nwe drop m from our notation and write simply \u03c4 = \u03c4m, \u00b5 = \u00b5m, etc. Let UA be the uniform distribution\nover the action set A. We de\ufb01ne the following potential function for use on epoch m:\n\u03a6m(Q) = \u03c4\u00b5\n bEx[RE (UA\u2225Q\u00b5(\u00b7 | x))]\n1 \u2212K\u00b5\n+\nP\n\u03c0\u2208\u03a0 Q(\u03c0)b\u03c0\n2K\n!\n.\n(6)\nThe function in Eq. (6) is de\ufb01ned for all vectors Q \u2208\u2206\u03a0. Also, RE (p\u2225q) denotes the unnormalized\nrelative entropy between two nonnegative vectors p and q over the action space (or any set) A:\nRE (p\u2225q) =\nX\na\u2208A\n(pa ln(pa/qa) + qa \u2212pa).\nThis number is always nonnegative. Here, Q\u00b5(\u00b7|x) denotes the \u201cdistribution\u201d (which might not sum to\n1) over A induced by Q\u00b5 for context x as given in Section 2.4. Thus, ignoring constants, this potential\nfunction is a combination of two terms: The \ufb01rst measures how far from uniform are the distributions\ninduced by Q\u00b5, and the second is an estimate of expected regret under Q since b\u03c0 is proportional to the\nempirical regret of \u03c0. Making \u03a6m small thus encourages Q to choose actions as uniformly as possible\nwhile also incurring low regret \u2014 exactly the aims of our algorithm. The constants that appear in this\nde\ufb01nition are for later mathematical convenience.\nFor further intuition, note that, by straightforward calculus, the partial derivative \u2202\u03a6m/\u2202Q(\u03c0) is\nroughly proportional to the variance constraint for \u03c0 given in Eq. (3) (up to a slight mismatch of con-\nstants). This shows that if this constraint is not satis\ufb01ed, then \u2202\u03a6m/\u2202Q(\u03c0) is likely to be negative,\nmeaning that \u03a6m can be decreased by increasing Q(\u03c0). Thus, the weight vector Q that minimizes \u03a6m\nsatis\ufb01es the variance constraint for every policy \u03c0. It turns out that this minimizing Q also satis\ufb01es the\n9\nlow regret constraint in Eq. (2), and also must sum to at most 1; in other words, it provides a complete\nsolution to our optimization problem. Algorithm 2 does not fully minimize \u03a6m, but it is based roughly\non coordinate descent. This is because in each iteration one of the weights (coordinate directions) Q(\u03c0)\nis increased. This weight is one whose corresponding partial derivative is large and negative.\nTo analyze the algorithm, we \ufb01rst argue that it is correct in the sense of satisfying the required\nconstraints, provided that it halts.\nLemma 5. If Algorithm 2 halts and outputs a weight vector Q, then the constraints Eq. (3) and Eq. (2)\nmust hold, and furthermore the sum of the weights Q(\u03c0) is at most 1.\nThe proof is rather straightforward: Following Step 4, Eq. (2) must hold, and also the weights must\nsum to 1. And if the algorithm halts, then D\u03c0(Q) \u22640 for all \u03c0, which is equivalent to Eq. (3).\nWhat remains is the more challenging task of bounding the number of iterations until the algorithm\ndoes halt. We do this by showing that signi\ufb01cant progress is made in reducing \u03a6m on every iteration. To\nbegin, we show that scaling Q as in Step 4 cannot cause \u03a6m to increase.\nLemma 6. Let Q be a weight vector such that P\n\u03c0 Q(\u03c0)(2K +b\u03c0) > 2K, and let c be as in Eq. (4). Then\n\u03a6m(cQ) \u2264\u03a6m(Q).\nProof sketch.\nWe consider \u03a6m(cQ) as a function of c, and argue that its derivative (with respect to\nc) at the value of c given in the lemma statement is always nonnegative. Therefore, by convexity, it is\nnondecreasing for all values exceeding c. Since c < 1, this proves the lemma.\nNext, we show that substantial progress will be made in reducing \u03a6m each time that Step 8 is executed.\nLemma 7. Let Q denote a set of weights and suppose, for some policy \u03c0, that D\u03c0(Q) > 0. Let Q\u2032 be\na new set of weights which is an exact copy of Q except that Q\u2032(\u03c0) = Q(\u03c0) + \u03b1 where \u03b1 = \u03b1\u03c0(Q) > 0.\nThen\n\u03a6m(Q) \u2212\u03a6m(Q\u2032) \u2265\n\u03c4\u00b52\n4(1 \u2212K\u00b5).\n(7)\nProof sketch.\nWe \ufb01rst compute exactly the change in potential for general \u03b1. Next, we apply a second-\norder Taylor approximation, which is maximized by the \u03b1 used in the algorithm. The Taylor approxi-\nmation, for this \u03b1, yields a lower bound which can be further simpli\ufb01ed using the fact that Q\u00b5(a|x) \u2265\u00b5\nalways, and our assumption that D\u03c0(Q) > 0. This gives the bound stated in the lemma.\nSo Step 4 does not cause \u03a6m to increase, and Step 8 causes \u03a6m to decrease by at least the amount\ngiven in Lemma 7. This immediately implies Theorem 3: for Qinit = 0, the initial potential is bounded by\n\u03c4\u00b5 ln(1/(K\u00b5))/(1 \u2212K\u00b5), and it is never negative, so the number of times Step 8 is executed is bounded\nby 4 ln(1/(K\u00b5))/\u00b5 as required.\n5.1\nEpoching and Warm Start\nAs shown in Section 2.3, the bound on the number of iterations of the algorithm from Theorem 3 also\ngives a bound on the number of times the oracle is called. To reduce the number of oracle calls, one\napproach is the \u201cdoubling trick\u201d of Section 3.3, which enables us to bound the total combined number of\niterations of Algorithm 2 in the \ufb01rst T rounds is only \u02dcO(\np\nKT/ ln(|\u03a0|/\u03b4)). This means that the average\nnumber of calls to the arg-max oracle is only \u02dcO(\np\nK/(T ln(|\u03a0|/\u03b4))) per round, meaning that the oracle\nis called far less than once per round, and in fact, at a vanishingly low rate.\nWe now turn to warm-start approach of Section 3.4, where in each epoch m + 1 we initialize the\ncoordinate descent algorithm with Qinit = Qm, i.e. the weights computed in the previous epoch m.\nTo analyze this, we bound how much the potential changes from \u03a6m(Qm) at the end of epoch m to\n\u03a6m+1(Qm) at the very start of epoch m + 1. This, combined with our earlier results regarding how\nquickly Algorithm 2 drives down the potential, we are able to get an overall bound on the total number\nof updates across T rounds.\n10\nTable 1: Progressive validation loss, best hyperparameter values, and running times of various algorithm\non RCV1.\nAlgorithm\n\u01eb-greedy\nExplore-\ufb01rst\nBagging\nLinUCB\nOnline Cover\nSupervised\nP.V. Loss\n0.148\n0.081\n0.059\n0.128\n0.053\n0.051\nSearched\n0.1 = \u01eb\n2 \u00d7 105 \ufb01rst\n16 bags\n103 dim, minibatch-10\ncover n = 1\nnothing\nSeconds\n17\n2.6\n275\n212 \u00d7 103\n12\n5.3\nLemma 8. Let M be the largest integer for which \u03c4M+1 \u2264T . With probability at least 1 \u22122\u03b4, for all T ,\nthe total epoch-to-epoch increase in potential is\nM\nX\nm=1\n(\u03a6m+1(Qm) \u2212\u03a6m(Qm)) \u2264\u02dcO\n r\nT ln(|\u03a0|/\u03b4)\nK\n!\n,\nwhere M is the largest integer for which \u03c4M+1 \u2264T .\nProof sketch.\nThe potential function, as written in Eq. (6), naturally breaks into two pieces whose\nepoch-to-epoch changes can be bounded separately. Changes a\ufb00ecting the relative entropy term on the\nleft can be bounded, regardless of Qm, by taking advantage of the manner in which these distributions\nare smoothed. For the other term on the right, it turns out that these epoch-to-epoch changes are related\nto statistical quantities which can be bounded with high probability. Speci\ufb01cally, the total change in this\nterm is related \ufb01rst to how the estimated reward of the empirically best policy compares to the expected\nreward of the optimal policy; and second, to how the reward received by our algorithm compares to that\nof the optimal reward. From our regret analysis, we are able to show that both of these quantities will\nbe small with high probability.\nThis lemma, along with Lemma 7 can be used to further establish Lemma 3. We only provide an\nintuitive sketch here, with the details deferred to the appendix. As we observe in Lemma 8, the total\namount that the potential increases across T rounds is at most \u02dcO(\np\nT ln(|\u03a0|/\u03b4)/K).\nOn the other\nhand, Lemma 7 shows that each time Q is updated by Algorithm 2 the potential decreases by at least\n\u02dc\u2126(ln(|\u03a0|/\u03b4)/K) (using our choice of \u00b5). Therefore, the total number of updates of the algorithm totaled\nover all T rounds is at most \u02dcO(\np\nKT/ ln(|\u03a0|/\u03b4)). For instance, if we use (\u03c41, \u03c42) := (3, 5) and \u03c4m := m2\nfor m \u22653, then the weight vector Q is only updated about\n\u221a\nT times in T rounds, and on each of those\nrounds, Algorithm 2 requires \u02dcO(\np\nK/ ln(|\u03a0|/\u03b4)) iterations, on average, giving the claim in Lemma 3.\n6\nExperimental Evaluation\nIn this section we evaluate a variant of Algorithm 1 against several baselines. While Algorithm 1 is\nsigni\ufb01cantly more e\ufb03cient than many previous approaches, the overall computational complexity is still\nat least \u02dcO((KT )1.5) plus the total cost of the oracle calls, as discussed in Section 3.5. This is markedly\nlarger than the complexity of an ordinary supervised learning problem where it is typically possible to\nperform an O(1)-complexity update upon receiving a fresh example using online algorithms.\nA natural solution is to use an online oracle that is stateful and accepts examples one by one. An\nonline cost-sensitive classi\ufb01cation (CSC) oracle takes as input a weighted example and returns a predicted\nclass (corresponding to one of K actions in our setting). Since the oracle is stateful, it remembers and\nuses examples from all previous calls in answering questions, thereby reducing the complexity of each\noracle invocation to O(1) as in supervised learning. Using several such oracles, we can e\ufb03ciently track a\ndistribution over good policies and sample from it. We detail this approach (which we call Online Cover)\nin the full version of the paper. The algorithm maintains a uniform distribution over a \ufb01xed number\nn of policies where n is a parameter of the algorithm. Upon receiving a fresh example, it updates all\nn policies with the suitable CSC examples (Eq. (5)). The speci\ufb01c CSC oracle we use is a reduction to\n11\nsquared-loss regression (Algorithms 4 and 5 of Beygelzimer and Langford (2009)) which is amenable to\nonline updates. Our implementation is included in Vowpal Wabbit.6\nDue to lack of public datasets for contextual bandit problems, we use a simple supervised-to-contextual-\nbandit transformation (Dud\u00b4\u0131k et al., 2011b) on the CCAT document classi\ufb01cation problem in RCV1 (Lewis et al.,\n2004). This dataset has 781265 examples and 47152 TF-IDF features. We treated the class labels as ac-\ntions, and one minus 0/1-loss as the reward. Our evaluation criteria is progressive validation (Blum et al.,\n1999) on 0/1 loss. We compare several baseline algorithms to Online Cover; all algorithms take advantage\nof linear representations which are known to work well on this dataset. For each algorithm, we report\nthe result for the best parameter settings (shown in Table 6).\n1. \u01eb-greedy (Sutton and Barto, 1998) explores randomly with probability \u01eb and otherwise exploits.\n2. Explore-\ufb01rst is a variant that begins with uniform exploration, then switches to an exploit-only\nphase.\n3. A less common but powerful baseline is based on bagging: multiple predictors (policies) are trained\nwith examples sampled with replacement. Given a context, these predictors yield a distribution\nover actions from which we can sample.\n4. LinUCB (Auer, 2002; Chu et al., 2011) has been quite e\ufb00ective in past evaluations (Li et al., 2010;\nChapelle and Li, 2011). It is impractical to run \u201cas is\u201d due to high-dimensional matrix inversions,\nso we report results for this algorithm after reducing to 1000 dimensions via random projections.\nStill, the algorithm required 59 hours7. An alternative is to use diagonal approximation to the\ncovariance, which runs substantially faster (\u22481 hour), but gives a worse error of 0.137.\n5. Finally, our algorithm achieves the best loss of 0.0530. Somewhat surprisingly, the minimum occurs\nfor us with a cover set of size 1\u2014apparently for this problem the small decaying amount of uniform\nrandom sampling imposed is adequate exploration. Prediction performance is similar with a larger\ncover set.\nAll baselines except for LinUCB are implemented as a simple modi\ufb01cation of Vowpal Wabbit. All\nreported results use default parameters where not otherwise speci\ufb01ed. The contextual bandit learning\nalgorithms all use a doubly robust reward estimator instead of the importance weighted estimators used\nin our analysis Dud\u00b4\u0131k et al. (2011b).\nBecause RCV1 is actually a fully supervised dataset, we can apply a fully supervised online multiclass\nalgorithm to solve it. We use a simple one-against-all implementation to reduce this to binary classi-\n\ufb01cation, yielding an error rate of 0.051 which is competitive with the best previously reported results.\nThis is e\ufb00ectively a lower bound on the loss we can hope to achieve with algorithms using only partial\ninformation. Our algorithm is less than 2.3 times slower and nearly achieves the bound. Hence on this\ndataset, very little further algorithmic improvement is possible.\n7\nConclusions\nIn this paper we have presented the \ufb01rst practical algorithm to our knowledge that attains the statistically\noptimal regret guarantee and is computationally e\ufb03cient in the setting of general policy classes.\nA\nremarkable feature of the algorithm is that the total number of oracle calls over all T rounds is sublinear\u2014\na remarkable improvement over previous works in this setting. We believe that the online variant of the\napproach which we implemented in our experiments has the right practical \ufb02avor for a scalable solution\nto the contextual bandit problem. In future work, it would be interesting to directly analyze the Online\nCover algorithm.\n6http://hunch.net/~vw. The implementation is in the \ufb01le cbify.cc and is enabled using --cover.\n7The linear algebra routines are based on Intel MKL package.\n12\nAcknowledgements\nWe thank Dean Foster and Matus Telgarsky for helpful discussions. Part of this work was completed\nwhile DH and RES were visiting Microsoft Research.\nReferences\nPeter Auer. Using con\ufb01dence bounds for exploitation-exploration trade-o\ufb00s. Journal of Machine Learning\nResearch, 3:397\u2013422, 2002.\nPeter Auer, Nicol`o Cesa-Bianchi, Yoav Freund, and Robert E. Schapire. The nonstochastic multiarmed\nbandit problem. SIAM Journal of Computing, 32(1):48\u201377, 2002.\nAlina Beygelzimer and John Langford. The o\ufb00set tree for learning with partial labels. In KDD, 2009.\nAlina Beygelzimer, John Langford, Lihong Li, Lev Reyzin, and Robert E. Schapire. Contextual bandit\nalgorithms with supervised learning guarantees. In AISTATS, 2011.\nAvrim Blum, Adam Kalai, and John Langford. Beating the holdout: Bounds for k-fold and progressive\ncross-validation. In COLT, 1999.\nOlivier Chapelle and Lihong Li. An empirical evaluation of Thompson sampling. In NIPS, 2011.\nWei Chu, Lihong Li, Lev Reyzin, and Robert E. Schapire. Contextual bandits with linear payo\ufb00functions.\nIn AISTATS, 2011.\nMiroslav Dud\u00b4\u0131k, Daniel Hsu, Satyen Kale, Nikos Karampatziakis, John Langford, Lev Reyzin, and Tong\nZhang. E\ufb03cient optimal learning for contextual bandits. In UAI, 2011a.\nMiroslav Dud\u00b4\u0131k, John Langford, and Lihong Li. Doubly robust policy evaluation and learning. In ICML,\n2011b.\nDavid P. Helmbold and Robert E. Schapire. Predicting nearly as well as the best pruning of a decision\ntree. Machine Learning, 27(1):51\u201368, 1997.\nJohn\nLangford.\nInteractive\nmachine\nlearning,\nJanuary\n2014.\nURL\nhttp://hunch.net/~jl/projects/interactive/index.html.\nJohn Langford and Tong Zhang. The epoch-greedy algorithm for contextual multi-armed bandits. In\nNIPS, 2007.\nDavid D Lewis, Yiming Yang, Tony G Rose, and Fan Li. Rcv1: A new benchmark collection for text\ncategorization research. The Journal of Machine Learning Research, 5:361\u2013397, 2004.\nLihong Li. Generalized Thompson sampling for contextual bandits. CoRR, abs/1310.7163, 2013.\nLihong Li, Wei Chu, John Langford, and Robert E. Schapire. A contextual-bandit approach to person-\nalized news article recommendation. In WWW, 2010.\nH. Brendan McMahan and Matthew Streeter. Tighter bounds for multi-armed bandits with expert advice.\nIn COLT, 2009.\nRichard S. Sutton and Andrew G. Barto. Reinforcement learning, an introduction. MIT Press, 1998.\nWilliam R. Thompson. On the likelihood that one unknown probability exceeds another in view of the\nevidence of two samples. Biometrika, 25(3\u20134):285\u2013294, 1933.\n13\nAlgorithm 3 IPS(H)\ninput History H \u2286X \u00d7 A \u00d7 [0, 1] \u00d7 [0, 1].\noutput Data set S \u2286X \u00d7 RA\n+.\n1: Initialize data set S := \u2205.\n2: for each (x, a, r(a), p(a)) \u2208H do\n3:\nCreate \ufb01ctitious rewards \u02c6r \u2208RA\n+ with \u02c6r(a) = r(a)/p(a) and \u02c6r(a\u2032) = 0 for all a\u2032 \u2208A \\ {a}.\n4:\nS := S \u222a{(x, \u02c6r)}.\n5: end for\n6: return S.\nAlgorithm 4 Sample(x, Q, \u00af\u03c0, \u00b5)\ninput Context x \u2208X, weights Q \u2208\u2206\u03a0, default policy \u00af\u03c0 \u2208\u03a0, minimum probability \u00b5 \u2208[0, 1/K].\noutput Selected action \u00afa \u2208A and probability \u00afp \u2208[\u00b5, 1].\n1: Let \u02dcQ := Q + (1 \u2212P\n\u03c0\u2208\u03a0 Q(\u03c0))1\u00af\u03c0\n(so P\n\u03c0\u2208\u03a0 \u02dcQ(\u03c0) = 1).\n2: Randomly draw action \u00afa \u2208A using the distribution\n\u02dcQ\u00b5(a|x) := (1 \u2212K\u00b5)\nX\n\u03c0\u2208\u03a0:\n\u03c0(x)=a\n\u02dcQ(\u03c0) + \u00b5,\n\u2200a \u2208A.\n3: Let \u00afp(\u00afa) := \u02dcQ\u00b5(\u00afa|x).\n4: return (\u00afa, \u00afp(\u00afa)).\nA\nOmitted Algorithm Details\nAlgorithm 3 and Algorithm 4 give the details of the inverse propensity scoring transformation IPS and\nthe action sampling procedure Sample.\nB\nDeviation Inequalities\nB.1\nFreedman\u2019s Inequality\nThe following form of Freedman\u2019s inequality for martingales is from Beygelzimer et al. (2011).\nLemma 9. Let X1, X2, . . . , XT be a sequence of real-valued random variables.\nAssume for all t \u2208\n{1, 2, . . ., T }, Xt \u2264R and E[Xt|X1, . . . , Xt\u22121] = 0. De\ufb01ne S := PT\nt=1 Xt and V := PT\nt=1 E[X2\nt |X1, . . . , Xt\u22121].\nFor any \u03b4 \u2208(0, 1) and \u03bb \u2208[0, 1/R], with probability at least 1 \u2212\u03b4,\nS \u2264(e \u22122)\u03bbV + ln(1/\u03b4)\n\u03bb\n.\nB.2\nVariance Bounds\nFix the epoch schedule 0 = \u03c40 < \u03c41 < \u03c42 < \u00b7 \u00b7 \u00b7 .\nDe\ufb01ne the following for any probability distribution P over \u03a0, \u03c0 \u2208\u03a0, and \u00b5 \u2208[0, 1/K]:\nV (P, \u03c0, \u00b5) := Ex\u223cDX\n\u0014\n1\nP \u00b5(\u03c0(x)|x)\n\u0015\n,\n(8)\nbVm(P, \u03c0, \u00b5) := bEx\u223cH\u03c4m\n\u0014\n1\nP \u00b5(\u03c0(x)|x)\n\u0015\n.\n(9)\nThe proof of the following lemma is essentially the same as that of Theorem 6 from Dud\u00b4\u0131k et al.\n(2011a).\n14\nLemma 10. Fix any \u00b5m \u2208[0, 1/K] for m \u2208N. For any \u03b4 \u2208(0, 1), with probability at least 1 \u2212\u03b4,\nV (P, \u03c0, \u00b5m) \u22646.4bVm(P, \u03c0, \u00b5m) + 75(1 \u2212K\u00b5m) ln |\u03a0|\n\u00b52m\u03c4m\n+ 6.3 ln(2|\u03a0|2m2/\u03b4)\n\u00b5m\u03c4m\nfor all probability distributions P over \u03a0, all \u03c0 \u2208\u03a0, and all m \u2208N. In particular, if\n\u00b5m \u2265\ns\nln(2|\u03a0|m2/\u03b4)\nK\u03c4m\n,\n\u03c4m \u22654K ln(2|\u03a0|m2/\u03b4),\nthen\nV (P, \u03c0, \u00b5m) \u22646.4bVm(P, \u03c0, \u00b5m) + 81.3K.\nProof sketch. By Bernstein\u2019s (or Freedman\u2019s) inequality and union bounds, for any choice of Nm \u2208N\nand \u03bbm \u2208[0, \u00b5m] for m \u2208N, the following holds with probability at least 1 \u2212\u03b4:\nV (P, \u03c0, \u00b5m) \u2212bVm(P, \u03c0, \u00b5m) \u2264(e \u22122)\u03bbmV (P, \u03c0, \u00b5m)\n\u00b5m\n+ ln(|\u03a0|Nm+12m2/\u03b4)\n\u03bbm\u03c4m\nall Nm-point distributions P over \u03a0, all \u03c0 \u2208\u03a0, and all m \u2208N. Here, an N-point distribution over \u03a0 is\na distribution of the form\n1\nN\nPN\ni=1 1\u03c0i for \u03c01, \u03c02, . . . , \u03c0N \u2208\u03a0. We henceforth condition on this \u22651 \u2212\u03b4\nprobability event (for choices of Nm and \u03bbm to be determined).\nUsing the probabilistic method (for more details, we refer the reader to the proof of Theorem 6 from\nDud\u00b4\u0131k et al. (2011a)), it can be shown that for any probability distribution P over \u03a0, any \u03c0 \u2208\u03a0, any\n\u00b5m \u2208[0, 1/K], and any cm > 0, there exists an Nm-point distribution eP over \u03a0 such that\n\u0000V (P, \u03c0, \u00b5m) \u2212V ( eP, \u03c0, \u00b5m)\n\u0001\n+ cm\n\u0000bVm( eP, \u03c0, \u00b5m) \u2212bVm(P, \u03c0, \u00b5m)\n\u0001\n\u2264\u03b3Nm,\u00b5m\n\u0000V (P, \u03c0, \u00b5m) + cm bVm(P, \u03c0, \u00b5m)\n\u0001\nwhere \u03b3N,\u00b5 :=\np\n(1 \u2212K\u00b5)/(N\u00b5) + 3(1 \u2212K\u00b5)/(N\u00b5).\nCombining the displayed inequalities (using cm := 1/(1 \u2212(e \u22122)\u03bbm/\u00b5m)) and rearranging gives\nV (P, \u03c0, \u00b5m) \u22641 + \u03b3Nm,\u00b5m\n1 \u2212\u03b3Nm,\u00b5m\n\u00b7\nbVm(P, \u03c0, \u00b5m)\n1 \u2212(e \u22122) \u03bbm\n\u00b5m\n+\n1\n1 \u2212\u03b3Nm,\u00b5m\n\u00b7\n1\n1 \u2212(e \u22122) \u03bbm\n\u00b5m\n\u00b7 ln(|\u03a0|Nm+12m2/\u03b4)\n\u03bbm\u03c4m\n.\nUsing Nm := \u230812(1 \u2212K\u00b5m)/\u00b5m\u2309and \u03bbm := 0.66\u00b5m for all m \u2208N gives the claimed inequalities.\nIf \u00b5m \u2265\np\nln(2|\u03a0|m2/\u03b4)/(K\u03c4m) and \u03c4m \u22654K ln(2|\u03a0|m2/\u03b4), then \u00b52\nm\u03c4m \u2265ln(|\u03a0|)/K and \u00b5m\u03c4m \u2265\nln(2|\u03a0|2m2/\u03b4), and hence\n75(1 \u2212K\u00b5m) ln |\u03a0|\n\u00b52m\u03c4m\n+ 6.3 ln(2|\u03a0|2m2/\u03b4)\n\u00b5m\u03c4m\n\u2264(75 + 6.3)K = 81.3K.\nB.3\nReward Estimates\nAgain, \ufb01x the epoch schedule 0 = \u03c40 < \u03c41 < \u03c42 < \u00b7 \u00b7 \u00b7 . Recall that for any epoch m \u2208N and round t in\nepoch m,\n\u2022 Qm\u22121 \u2208\u2206\u03a0 are the non-negative weights computed at the end of epoch m \u22121;\n\u2022 eQm\u22121 is the probability distribution over \u03a0 obtained from Qm\u22121 and the policy \u03c0m\u22121 with the\nhighest reward estimate through epoch m \u22121;\n\u2022 eQ\u00b5m\u22121\nm\u22121 (\u00b7|xt) is the probability distribution used to choose at.\n15\nLet\nm(t) := min{m \u2208N : t \u2264\u03c4m}\n(10)\nbe the index of the epoch containing round t \u2208N, and de\ufb01ne\nVt(\u03c0) :=\nmax\n0\u2264m\u2264m(t)\u22121{V ( eQm, \u03c0, \u00b5m)}\n(11)\nfor all t \u2208N and \u03c0 \u2208\u03a0.\nLemma 11. For any \u03b4 \u2208(0, 1) and any choices of \u03bbm\u22121 \u2208[0, \u00b5m\u22121] for m \u2208N, with probability at least\n1 \u2212\u03b4,\n| bRt(\u03c0) \u2212R(\u03c0)| \u2264Vt(\u03c0)\u03bbm\u22121 + ln(4t2|\u03a0|/\u03b4)\nt\u03bbm\u22121\nfor all policies \u03c0 \u2208\u03a0, all epochs m \u2208N, and all rounds t in epoch m.\nProof. Fix any policy \u03c0 \u2208\u03a0, epoch m \u2208N, and round t in epoch m. Then\nbRt(\u03c0) \u2212R(\u03c0) = 1\nt\nt\nX\ni=1\nZi\nwhere Zi := \u02c6ri(\u03c0(xi)) \u2212ri(\u03c0(xi)). Round i is in epoch m(i) \u2264m, so\n|Zi| \u2264\n1\neQ\n\u00b5m(i)\u22121\nm(i)\u22121 (\u03c0(xi)|xi)\n\u2264\n1\n\u00b5m(i)\u22121\nby the de\ufb01nition of the \ufb01ctitious rewards. Because the sequences \u00b51 \u2265\u00b52 \u2265\u00b7 \u00b7 \u00b7 and m(1) \u2264m(2) \u2264\u00b7 \u00b7 \u00b7\nare monotone, it follows that Zi \u22641/\u00b5m\u22121 for all 1 \u2264i \u2264t. Furthermore, E[Zi|Hi\u22121] = 0 and\nE[Z2\ni |Hi\u22121] \u2264E[\u02c6ri(\u03c0(xi))2|Hi\u22121]\n\u2264V ( eQm(i)\u22121, \u03c0, \u00b5m(i)\u22121) \u2264Vt(\u03c0)\nfor all 1 \u2264i \u2264t. The \ufb01rst inequality follows because for var(X) \u2264E(X2) for any random variable\nX; and the other inequalities follow from the de\ufb01nitions of the \ufb01ctitious rewards, V (\u00b7, \u00b7, \u00b7) in Eq. (8),\nand Vt(\u00b7) in Eq. (11). Applying Freedman\u2019s inequality and a union bound to the sums (1/t) Pt\ni=1 Zi and\n(1/t) Pt\ni=1(\u2212Zi) implies the following: for all \u03bbm\u22121 \u2208[0, \u00b5m\u22121], with probability at least 1\u22122\u00b7\u03b4/(4t2|\u03a0|),\n\f\f\f\f\f\n1\nt\nt\nX\ni=1\nZi\n\f\f\f\f\f \u2264(e \u22122)Vt(\u03c0)\u03bbm\u22121 + ln(4t2|\u03a0|/\u03b4)\nt\u03bbm\u22121\n.\nThe lemma now follows by applying a union bound for all choices of \u03c0 \u2208\u03a0 and t \u2208N, since\nX\n\u03c0\u2208\u03a0\nX\nt\u2208N\n\u03b4\n2t2|\u03a0| \u2264\u03b4.\nC\nRegret Analysis\nThroughout this section, we \ufb01x the allowed probability of failure \u03b4 \u2208(0, 1) provided as input to the\nalgorithm, as well as the epoch schedule 0 = \u03c40 < \u03c41 < \u03c42 < \u00b7 \u00b7 \u00b7 .\n16\nC.1\nDe\ufb01nitions\nDe\ufb01ne, for all t \u2208N,\ndt := ln(16t2|\u03a0|/\u03b4),\n(12)\nand recall that,\n\u00b5m = min\n(\n1\n2K ,\nr\nd\u03c4m\nK\u03c4m\n)\n.\nObserve that dt/t is non-increasing with t \u2208N, and \u00b5m is non-increasing with m \u2208N.\nLet\nm0 := min\n\u001a\nm \u2208N : d\u03c4m\n\u03c4m\n\u2264\n1\n4K\n\u001b\n.\nObserve that \u03c4m0 \u22652.\nDe\ufb01ne\n\u03c1 := sup\nm\u2265m0\n\u001ar \u03c4m\n\u03c4m\u22121\n\u001b\n.\nRecall that we assume \u03c4m+1 \u22642\u03c4m; thus \u03c1 \u2264\n\u221a\n2.\nC.2\nDeviation Control and Optimization Constraints\nLet E be the event in which the following statements hold:\nV (P, \u03c0, \u00b5m) \u22646.4bVm(P, \u03c0, \u00b5m) + 81.3K\n(13)\nfor all probability distributions P over \u03a0, all \u03c0 \u2208\u03a0, and all m \u2208N such that \u03c4m \u22654Kd\u03c4m (so \u00b5m =\np\nd\u03c4m/(K\u03c4m)); and\n| bRt(\u03c0) \u2212R(\u03c0)| \u2264\n\uf8f1\n\uf8f4\n\uf8f4\n\uf8f2\n\uf8f4\n\uf8f4\n\uf8f3\nmax\n(r\n3Vtdt\nt\n, 2Vtdt\nt\n)\nif m \u2264m0,\nVt(\u03c0)\u00b5m\u22121 +\ndt\nt\u00b5m\u22121\nif m > m0.\n(14)\nfor all all policies \u03c0 \u2208\u03a0, all epochs m \u2208N, and all rounds t in epoch m. By Lemma 10, Lemma 11, and\na union bound, Pr(E) \u22651 \u2212\u03b4/2.\nFor every epoch m \u2208N, the weights Qm computed at the end of the epoch (in round \u03c4m) as the\nsolution to (OP) satisfy the constraints Eq. (2) and Eq. (3): they are, respectively:\nX\n\u03c0\u2208\u03a0\nQm(\u03c0)d\nReg\u03c4m(\u03c0) \u2264\u03c8 \u00b7 2K\u00b5m\n(15)\nand, for all \u03c0 \u2208\u03a0,\nbVm(Qm, \u03c0, \u00b5m) \u22642K +\nd\nReg\u03c4m(\u03c0)\n\u03c8 \u00b7 \u00b5m\n.\n(16)\nRecall that \u03c8 = 100 (as de\ufb01ned in (OP), assuming \u03c1 \u2264\n\u221a\n2). De\ufb01ne \u03b81 := 94.1 and \u03b82 := \u03c8/6.4 (needed\nfor the next Lemma 12). With these settings, the proof of Lemma 13 will require that \u03b82 \u22658\u03c1, and hence\n\u03c8 \u22656.4 \u00b7 8\u03c1; this is true with our setting of \u03c8 since \u03c1 \u2264\n\u221a\n2.\n17\nC.3\nProof of Theorem 2\nWe now give the proof of Theorem 2, following the outline in Section 4.\nThe following lemma shows that if Vt(\u03c0) is large\u2014speci\ufb01cally, much larger than K\u2014then the estimated\nregret of \u03c0 was large in some previous round.\nLemma 12. Assume event E holds. Pick any round t \u2208N and any policy \u03c0 \u2208\u03a0, and let m \u2208N be the\nepoch achieving the max in the de\ufb01nition of Vt(\u03c0). Then\nVt(\u03c0) \u2264\n\uf8f1\n\uf8f4\n\uf8f2\n\uf8f4\n\uf8f3\n2K\nif \u00b5m = 1/(2K),\n\u03b81K +\nd\nReg\u03c4m(\u03c0)\n\u03b82\u00b5m\nif \u00b5m < 1/(2K).\nProof. Fix a round t \u2208N and policy \u03c0 \u2208\u03a0. Let m \u2264m(t) \u22121 be the epoch achieving the max in the\nde\ufb01nition of Vt(\u03c0) from Eq. (11), so Vt(\u03c0) = V ( eQm, \u03c0, \u00b5m). If \u00b5m = 1/(2K), then V ( eQm, \u03c0, \u00b5m) \u22642K.\nSo assume instead that 1/(2K) > \u00b5m =\np\nd\u03c4m/(K\u03c4m). This implies that \u03c4m > 4Kd\u03c4m. By Eq. (13),\nwhich holds in event E,\nV ( eQm, \u03c0, \u00b5m) \u22646.4bVm( eQm, \u03c0, \u00b5m) + 81.3K.\nThe probability distribution eQm satis\ufb01es the inequalities\nbVm( eQm, \u03c0, \u00b5m) \u2264bVm(Qm, \u03c0, \u00b5m) \u22642K +\nd\nReg\u03c4m(\u03c0)\n\u03c8\u00b5m\n.\nAbove, the \ufb01rst inequality follows because the value of bVm(Qm, \u03c0, \u00b5m) decreases as the value of Qm(\u03c0\u03c4m)\nincreases, as it does when going from Qm to eQm; the second inequality is the constraint Eq. (16) satis\ufb01ed\nby Qm. Combining the displayed inequalities from above proves the claim.\nIn the next lemma, we compare Reg(\u03c0) and d\nRegt(\u03c0) for any policy \u03c0 by using the deviation bounds\nfor estimated rewards together with the variance bounds from Lemma 12. De\ufb01ne t0 := min{t \u2208N :\ndt/t \u22641/(4K)}.\nLemma 13. Assume event E holds. Let c0 := 4\u03c1(1 + \u03b81). For all epochs m \u2265m0, all rounds t \u2265t0 in\nepoch m, and all policies \u03c0 \u2208\u03a0,\nReg(\u03c0) \u22642d\nRegt(\u03c0) + c0K\u00b5m;\nd\nRegt(\u03c0) \u22642 Reg(\u03c0) + c0K\u00b5m.\nProof. The proof is by induction on m. As the base case, consider m = m0 and t \u2265t0 in epoch m. By\nde\ufb01nition of m0, \u00b5m\u2032 = 1/(2K) for all m\u2032 < m0, so Vt(\u03c0) \u22642K for all \u03c0 \u2208\u03a0 by Lemma 12. By Eq. (14),\nwhich holds in event E, for all \u03c0 \u2208\u03a0,\n| bRt(\u03c0) \u2212R(\u03c0)| \u2264max\n(r\n6Kdt\nt\n, 4Kdt\nt\n)\n\u2264\nr\n6Kdt\nt\nwhere we use the fact that 4Kdt/t \u22641 for t \u2265t0. This implies\n|d\nRegt(\u03c0) \u2212Reg(\u03c0)| \u22642\nr\n6Kdt\nt\n.\nby the triangle inequality and optimality of \u03c0t and \u03c0\u22c6. Since t > \u03c4m0\u22121 and c0 \u22652\n\u221a\n6\u03c1, it follows that\n|d\nRegt(\u03c0) \u2212Reg(\u03c0)| \u22642\n\u221a\n6\u03c1K\u00b5m0 \u2264c0K\u00b5m0.\nFor the inductive step, \ufb01x some epoch m > m0. We assume as the inductive hypothesis that for all\nepochs m\u2032 < m, all rounds t\u2032 in epoch m\u2032, and all \u03c0 \u2208\u03a0,\nReg(\u03c0) \u22642d\nRegt\u2032(\u03c0) + c0K\u00b5m\u2032;\nd\nRegt\u2032(\u03c0) \u22642 Reg(\u03c0) + c0K\u00b5m\u2032.\n18\nWe \ufb01rst show that\nReg(\u03c0) \u22642d\nRegt(\u03c0) + c0K\u00b5m\n(17)\nfor all rounds t in epoch m and all \u03c0 \u2208\u03a0. So \ufb01x such a round t and policy \u03c0; by Eq. (14) (which holds\nin event E),\nReg(\u03c0) \u2212d\nRegt(\u03c0) =\n\u0000R(\u03c0\u22c6) \u2212R(\u03c0)\n\u0001\n\u2212\n\u0000 bRt(\u03c0t) \u2212bRt(\u03c0)\n\u0001\n\u2264\n\u0000R(\u03c0\u22c6) \u2212R(\u03c0)\n\u0001\n\u2212\n\u0000 bRt(\u03c0\u22c6) \u2212bRt(\u03c0)\n\u0001\n\u2264\n\u0000Vt(\u03c0) + Vt(\u03c0\u22c6)\n\u0001\n\u00b5m\u22121 +\n2dt\nt\u00b5m\u22121\n.\n(18)\nAbove, the \ufb01rst inequality follows from the optimality of \u03c0t. By Lemma 12, there exist epochs i, j < m\nsuch that\nVt(\u03c0) \u2264\u03b81K +\nd\nReg\u03c4i(\u03c0)\n\u03b82\u00b5i\n\u00b7 1{\u00b5i < 1/(2K)},\nVt(\u03c0\u22c6) \u2264\u03b81K +\nd\nReg\u03c4j(\u03c0\u22c6)\n\u03b82\u00b5j\n\u00b7 1{\u00b5j < 1/(2K)}.\nSuppose \u00b5i < 1/(2K), so m0 \u2264i < m: in this case, the inductive hypothesis implies\nd\nReg\u03c4i(\u03c0)\n\u03b82\u00b5i\n\u22642 Reg(\u03c0) + c0K\u00b5i\n\u03b82\u00b5i\n\u2264c0K\n\u03b82\n+ 2 Reg(\u03c0)\n\u03b82\u00b5m\u22121\nwhere the second inequality uses the fact that i \u2264m \u22121. Therefore,\nVt(\u03c0)\u00b5m\u22121 \u2264\n\u0012\n\u03b81 + c0\n\u03b82\n\u0013\nK\u00b5m\u22121 + 2\n\u03b82\nReg(\u03c0).\n(19)\nNow suppose \u00b5j < 1/(2K), so m0 \u2264j < m: as above, the inductive hypothesis implies\nd\nReg\u03c4j(\u03c0\u22c6)\n\u03b82\u00b5j\n\u22642 Reg(\u03c0\u22c6) + c0K\u00b5j\n\u03b82\u00b5j\n= c0\n\u03b82\nK\nsince Reg(\u03c0\u22c6) = 0. Therefore,\nVt(\u03c0\u22c6)\u00b5m\u22121 \u2264\n\u0012\n\u03b81 + c0\n\u03b82\n\u0013\nK\u00b5m\u22121.\n(20)\nCombining Eq. (18), Eq. (19), and Eq. (20), and rearranging gives\nReg(\u03c0) \u2264\n1\n1 \u22122\n\u03b82\n\u0012\nd\nRegt(\u03c0) + 2\n\u0012\n\u03b81 + c0\n\u03b82\n\u0013\nK\u00b5m\u22121 +\n2dt\nt\u00b5m\u22121\n\u0013\n.\nSince m \u2265m0+1, it follows that \u00b5m\u22121 \u2264\u03c1\u00b5m by de\ufb01nition of \u03c1. Moreover, since t > \u03c4m\u22121, (dt/t)/\u00b5m\u22121 \u2264\nK\u00b52\nm\u22121/\u00b5m\u22121 \u2264\u03c1K\u00b5m Applying these inequalities to the above display, and simplifying, yields Eq. (17)\nbecause c0 \u22654\u03c1(1 + \u03b81) and \u03b82 \u22658\u03c1.\nWe now show that\nd\nRegt(\u03c0) \u22642 Reg(\u03c0) + c0K\u00b5m\n(21)\nfor all \u03c0 \u2208\u03a0. Again, \ufb01x an arbitrary \u03c0 \u2208\u03a0, and by Eq. (14),\nd\nRegt(\u03c0) \u2212Reg(\u03c0) =\n\u0000 bRt(\u03c0t) \u2212bRt(\u03c0)\n\u0001\n\u2212\n\u0000R(\u03c0\u22c6) \u2212R(\u03c0)\n\u0001\n\u2264\n\u0000 bRt(\u03c0t) \u2212bRt(\u03c0)\n\u0001\n\u2212\n\u0000R(\u03c0t) \u2212R(\u03c0)\n\u0001\n\u2264\n\u0000Vt(\u03c0) + Vt(\u03c0t)\n\u0001\n\u00b5m\u22121 +\n2dt\nt\u00b5m\u22121\n(22)\n19\nwhere the \ufb01rst inequality follows from the optimality of \u03c0\u22c6. By Lemma 12, there exists an epoch j < m\nsuch\nVt(\u03c0t) \u2264\u03b81K +\nd\nReg\u03c4j(\u03c0t)\n\u03b82\u00b5j\n\u00b7 1{\u00b5j < 1/(2K)}.\nSuppose \u00b5j < 1/(2K), so m0 \u2264j < m: in this case the inductive hypothesis and Eq. (17) imply\nd\nReg\u03c4j(\u03c0t)\n\u03b82\u00b5j\n\u22642 Reg(\u03c0t) + c0K\u00b5j\n\u03b82\u00b5j\n\u2264\n2\n\u0010\n2d\nRegt(\u03c0t) + c0K\u00b5m\n\u0011\n+ c0K\u00b5j\n\u03b82\u00b5j\n= 3c0\n\u03b82\nK\n(the last equality follows because d\nRegt(\u03c0t) = 0). Thus\nVt(\u03c0t)\u00b5\u03c4(t)\u22121 \u2264\n\u0012\n\u03b81 + 3c0\n\u03b82\n\u0013\nK\u00b5m\u22121.\n(23)\nCombining Eq. (22), Eq. (23), and Eq. (19) gives\nd\nRegt(\u03c0) \u2264\n\u0012\n1 + 2\n\u03b82\n\u0013\nReg(\u03c0) +\n\u0012\n2\u03b81 + 4c0\n\u03b82\n\u0013\nK\u00b5m\u22121 +\n2dt\nt\u00b5m\u22121\n.\nAgain, applying the inequalities \u00b5m\u22121 \u2264\u03c1\u00b5m and (dt/t)/\u00b5m\u22121 \u2264K\u00b5m to the above display, and simpli-\nfying, yields Eq. (21) because c0 \u22654\u03c1(1 + \u03b81) and \u03b82 \u22658\u03c1. This completes the inductive step, and thus\nproves the overall claim.\nThe next lemma shows that the \u201clow estimated regret guarantee\u201d of Qt\u22121 (optimization constraint\nEq. (15)) also implies a \u201clow regret guarantee\u201d, via the comparison of d\nRegt(\u00b7) to Reg(\u00b7) from Lemma 13.\nLemma 14. Assume event E holds. For every epoch m \u2208N,\nX\n\u03c0\u2208\u03a0\neQm\u22121(\u03c0) Reg(\u03c0) \u2264(4\u03c8 + c0)K\u00b5m\u22121\nwhere c0 is de\ufb01ned in Lemma 13.\nProof. Fix any epoch m \u2208N. If m \u2264m0, then \u00b5m\u22121 = 1/(2K), in which case the claim is trivial.\nTherefore assume m \u2265m0 + 1. Then\nX\n\u03c0\u2208\u03a0\neQm\u22121(\u03c0) Reg(\u03c0) \u2264\nX\n\u03c0\u2208\u03a0\neQm\u22121(\u03c0)\n\u00002d\nReg\u03c4m\u22121(\u03c0) + c0K\u00b5m\u22121\n\u0001\n=\n\u0012\n2\nX\n\u03c0\u2208\u03a0\nQm\u22121(\u03c0)d\nReg\u03c4m\u22121(\u03c0)\n\u0013\n+ c0K\u00b5m\u22121\n\u2264\u03c8 \u00b7 4K\u00b5m\u22121 + c0K\u00b5m\u22121.\nThe \ufb01rst step follows from Lemma 13, as all rounds in an epoch m \u2265m0 + 1 satisfy t \u2265t0; the second\nstep follows from the fact that eQm\u22121 is a probability distribution, that eQm\u22121 = Qm\u22121 + \u03b11\u03c0\u03c4m\u22121 for\nsome \u03b1 \u22650, and that d\nReg\u03c4m\u22121(\u03c0\u03c4m\u22121) = 0; and the last step follows from the constraint Eq. (15) satis\ufb01ed\nby Qm\u22121.\nFinally, we straightforwardly translate the \u201clow regret guarantee\u201d from Lemma 14 to a bound on the\ncumulative regret of the algorithm. This involves summing the bound in Lemma 14 over all rounds t\n(Lemma 15 and Lemma 16) and applying a martingale concentration argument (Lemma 17).\nLemma 15. For any T \u2208N,\nT\nX\nt=1\n\u00b5m(t) \u22642\nr\nd\u03c4m(T )\u03c4m(T )\nK\n.\n20\nProof. We break the sum over rounds into the epochs, and bound the sum within each epoch:\nT\nX\nt=1\n\u00b5m(t) \u2264\nm(T )\nX\nm=1\n\u03c4m\nX\nt=\u03c4m\u22121+1\n\u00b5m\n\u2264\nm(T )\nX\nm=1\n\u03c4m\nX\nt=\u03c4m\u22121+1\nr\nd\u03c4m\nK\u03c4m\n\u2264\nr\nd\u03c4m(T )\nK\nm(T )\nX\nm=1\n\u03c4m \u2212\u03c4m\u22121\n\u221a\u03c4m\n\u2264\nr\nd\u03c4m(T )\nK\nm(T )\nX\nm=1\nZ \u03c4m\n\u03c4m\u22121\ndx\n\u221ax =\nr\nd\u03c4m(T )\nK\nZ \u03c4m(T )\n\u03c40\ndx\n\u221ax = 2\nr\nd\u03c4m(T )\nK\n\u221a\u03c4m(T ).\nAbove, the \ufb01rst step uses the fact that m(1) = 1 and \u03c4m(t)\u22121 + 1 \u2264t \u2264\u03c4m(t). The second step uses\nthe de\ufb01nition of \u00b5m. The third step simpli\ufb01es the sum over t and uses the bound d\u03c4m\u22121 \u2264d\u03c4m(T ). The\nremaining steps use an integral bound which is then directly evaluated (recalling that \u03c40 = 0).\nLemma 16. For any T \u2208N,\nT\nX\nt=1\n\u00b5m(t)\u22121 \u2264\u03c4m0\n2K +\nr\n8d\u03c4m(T )\u03c4m(T )\nK\n.\nProof. Under the epoch schedule condition \u03c4m+1 \u22642\u03c4m, we have \u00b5m(t)\u22121 \u2264\n\u221a\n2\u00b5m(t) whenever m(t) >\nm0; also, \u00b5m(t)\u22121 \u22641/(2K) whenever m(t) \u2264m0. The conclusion follows by applying Lemma 15.\nLemma 17. For any T \u2208N, with probability at least 1 \u2212\u03b4, the regret after T rounds is at most\nC0\n\u0012\n4Kd\u03c4m0\u22121 +\nq\n8Kd\u03c4m(T )\u03c4m(T )\n\u0013\n+\np\n8T log(2/\u03b4)\nwhere C0 := (4\u03c8 + c0) and c0 is de\ufb01ned in Lemma 13.\nProof. Fix T \u2208N. For each round t \u2208N, let Zt := rt(\u03c0\u22c6(xt)) \u2212rt(at) \u2212P\n\u03c0\u2208\u03a0 eQm(t)\u22121 Reg(\u03c0). Since\nE[rt(\u03c0\u22c6(xt)) \u2212rt(at)|Ht\u22121] = R(\u03c0\u22c6) \u2212\nX\n\u03c0\u2208\u03a0\neQm(t)\u22121(\u03c0)R(\u03c0) =\nX\n\u03c0\u2208\u03a0\neQm(t)\u22121 Reg(\u03c0),\nit follows that E[Zt|Ht\u22121] = 0. Since |Zt| \u22642, it follows by Azuma\u2019s inequality that\nT\nX\nt=1\nZt \u22642\np\n2T ln(2/\u03b4)\nwith probability at least 1 \u2212\u03b4/2. By Lemma 10, Lemma 11, and a union bound, the event E holds with\nprobability at least 1 \u2212\u03b4/2. Hence, by another union bound, with probability at least 1 \u2212\u03b4, event E\nholds and the regret of the algorithm is bounded by\nT\nX\nt=1\nX\n\u03c0\u2208\u03a0\neQm(t)\u22121(\u03c0) Reg(\u03c0) + 2\np\n2T ln(2/\u03b4).\nThe double summation above is bounded by Lemma 14 and Lemma 16:\nT\nX\nt=1\nX\n\u03c0\u2208\u03a0\neQm(t)\u22121(\u03c0) Reg(\u03c0) \u2264(4\u03c8 + c0)K\nT\nX\nt=1\n\u00b5m(t)\u22121 \u2264(4\u03c8 + c0)\n\u0012\u03c4m0\n2\n+\nq\n8Kd\u03c4m(T )\u03c4m(T )\n\u0013\n.\nBy the de\ufb01nition of m0, \u03c4m0\u22121 \u22644Kd\u03c4m0\u22121. Since \u03c4m0 \u22642\u03c4m0\u22121 by assumption, it follows that \u03c4m0 \u2264\n8Kd\u03c4m0\u22121.\n21\nTheorem 2 follows from Lemma 17 and the fact that \u03c4m(T ) \u22642(T \u22121) whenever \u03c4m(T )\u22121 \u22651.\nThere is one last result implied by Lemma 12 and Lemma 13 that is used elsewhere.\nLemma 18. Assume event E holds, and t is such that d\u03c4m(t)\u22121/\u03c4m(t)\u22121 \u22641/(4K). Then\nbRt(\u03c0t) \u2264R(\u03c0\u22c6) +\n\u0012\n\u03b81 + c0\n\u03b82\n+ c0 + 1\n\u0013\nK\u00b5m(t)\u22121.\nProof. Let m\u2032 < m(t) achieve the max in the de\ufb01nition of Vt(\u03c0\u22c6). If \u00b5m\u2032 < 1/(2K), then m\u2032 \u2265m0, and\nVt(\u03c0\u22c6) \u2264\u03b81K +\nd\nReg\u03c4m\u2032 (\u03c0\u22c6)\n\u03b82\u00b5m\u2032\n\u2264\u03b81K + 2 Reg(\u03c0\u22c6) + c0K\u00b5m\u2032\n\u03b82\u00b5m\u2032\n= cK\nfor c := \u03b81 + c0/\u03b82. Above, the second inequality follows by Lemma 13. If \u00b5m\u2032 = 1/(2K), then the same\nbound also holds. Using this bound, we obtain from Eq. (14),\nbRt(\u03c0\u22c6) \u2212R(\u03c0\u22c6) \u2264cK\u00b5m(t)\u22121 +\ndt\nt\u00b5m(t)\u22121\n.\nTo conclude,\nbRt(\u03c0\u03c4m) = R(\u03c0\u22c6) +\n\u0010\nbRt(\u03c0\u22c6) \u2212R(\u03c0\u22c6)\n\u0011\n+ d\nRegt(\u03c0\u22c6)\n\u2264R(\u03c0\u22c6) + cK\u00b5m(t)\u22121 +\ndt\nt\u00b5m(t)\u22121\n+ d\nRegt(\u03c0\u22c6)\n\u2264R(\u03c0\u22c6) + cK\u00b5m(t)\u22121 +\ndt\nt\u00b5m(t)\u22121\n+ c0K\u00b5m(t)\nwhere the last inequality follows from Lemma 13. The claim follows because dt/t \u2264d\u03c4m(t)\u22121/\u03c4m(t)\u22121 and\n\u00b5m(t) \u2264\u00b5m(t)\u22121.\nD\nDetails of Optimization Analysis\nD.1\nProof of Lemma 5\nFollowing the execution of Step 4, we must have\nX\n\u03c0\nQ(\u03c0)(2K + b\u03c0) \u22642K.\n(24)\nThis is because, if the condition in Step 7 does not hold, then Eq. (24) is already true. Otherwise, Q is\nreplaced by Q\u2032 = cQ, and for this set of weights, Eq. (24) in fact holds with equality. Note that, since all\nquantities are nonnegative, Eq. (24) immediately implies both Eq. (2), and that P\n\u03c0 Q(\u03c0) \u22641.\nFurthermore, at the point where the algorithm halts at Step 10, it must be that for all policies \u03c0,\nD\u03c0(Q) \u22640. However, unraveling de\ufb01nitions, we can see that this is exactly equivalent to Eq. (3).\nD.2\nProof of Lemma 6\nConsider the function\ng(c) = B0\u03a6m(cQ),\n22\nwhere, in this proof, B0 = 2K/(\u03c4\u00b5), where we recall that we drop the subscripts on \u03c4m and \u00b5m. Let\nQ\u00b5\nc (a|x) = (1 \u2212K\u00b5)cQ(a|x) + \u00b5. By the chain rule, the \ufb01rst derivative of g is:\ng\u2032(c)\n=\nB0\nX\n\u03c0\nQ(\u03c0)\u2202g(cQ)\n\u2202Q(\u03c0)\n=\nX\n\u03c0\nQ(\u03c0)\n\u0012\n(2K + b\u03c0) \u22122bEx\u223cHt\n\u0014\n1\nQ\u00b5\nc (\u03c0(x)|x)\n\u0015\u0013\n(25)\nTo handle the second term, note that\nX\n\u03c0\nQ(\u03c0)bEx\u223cHt\n\u0014\n1\nQ\u00b5\nc (\u03c0(x)|x)\n\u0015\n=\nX\n\u03c0\nQ(\u03c0)bEx\u223cHt\n\"X\na\u2208A\n1{\u03c0(x) = a}\nQ\u00b5\nc (a|x)\n#\n=\nbEx\u223cHt\n\"X\na\u2208A\nX\n\u03c0\nQ(\u03c0)1{\u03c0(x) = a}\nQ\u00b5\nc (a|x)\n#\n=\nbEx\u223cHt\n\"X\na\u2208A\nQ(a|x)\nQ\u00b5\nc (a|x)\n#\n=\n1\nc\nbEx\u223cHt\n\"X\na\u2208A\ncQ(a|x)\n(1 \u2212K\u00b5)cQ(a|x) + \u00b5\n#\n\u2264K\nc .\n(26)\nTo see the inequality in Eq. (26), let us \ufb01x x and de\ufb01ne qa = cQ(a|x). Then P\na qa = c P\n\u03c0 Q(\u03c0) \u22641 by\nEq. (4). Further, the expression inside the expectation in Eq. (26) is equal to\nX\na\nqa\n(1 \u2212K\u00b5)qa + \u00b5\n=\nK \u00b7 1\nK\nX\na\n1\n(1 \u2212K\u00b5) + \u00b5/qa\n\u2264\nK \u00b7\n1\n(1 \u2212K\u00b5) + K\u00b5/ P\na qa\n(27)\n\u2264\nK \u00b7\n1\n(1 \u2212K\u00b5) + K\u00b5 = K.\n(28)\nEq. (27) uses Jensen\u2019s inequality, combined with the fact that the function 1/(1 \u2212K\u00b5 + \u00b5/x) is concave\n(as a function of x). Eq. (28) uses the fact that the function 1/(1 \u2212K\u00b5 + K\u00b5/x) is nondecreasing (in x),\nand that the qa\u2019s sum to at most 1.\nThus, plugging Eq. (26) into Eq. (25) yields\ng\u2032(c) \u2265\nX\n\u03c0\nQ(\u03c0)(2K + b\u03c0) \u22122K\nc\n= 0\nby our de\ufb01nition of c. Since g is convex, this means that g is nondecreasing for all values exceeding c. In\nparticular, since c < 1, this gives\nB0\u03a6m(Q) = g(1) \u2265g(c) = B0\u03a6m(cQ),\nimplying the lemma since B0 > 0.\nD.3\nProof of Lemma 7\nWe \ufb01rst compute the change in potential for general \u03b1. Note that Q\u2032\u00b5(a|x) = Q\u00b5(a|x) if a \u0338= \u03c0(x), and\notherwise\nQ\u2032\u00b5(\u03c0(x)|x) = Q\u00b5(\u03c0(x)|x) + (1 \u2212K\u00b5)\u03b1.\n23\nThus, most of the terms de\ufb01ning \u03a6m(Q) are left unchanged by the update. In particular, by a direct\ncalculation:\n2K\n\u03c4\u00b5 (\u03a6m(Q) \u2212\u03a6m(Q\u2032))\n=\n2\n1 \u2212K\u00b5\nbEx\u223cHt\n\u0014\nln\n\u0012\n1 + \u03b1(1 \u2212K\u00b5)\nQ\u00b5(\u03c0(x)|x)\n\u0013\u0015\n\u2212\u03b1(2K + b\u03c0)\n\u2265\n2\n1 \u2212K\u00b5\nbEx\u223cHt\n\"\n\u03b1(1 \u2212K\u00b5)\nQ\u00b5(\u03c0(x)|x) \u22121\n2\n\u0012 \u03b1(1 \u2212K\u00b5)\nQ\u00b5(\u03c0(x)|x)\n\u00132#\n\u2212\u03b1(2K + b\u03c0)\n(29)\n=\n2\u03b1V\u03c0(Q) \u2212(1 \u2212K\u00b5)\u03b12S\u03c0(Q) \u2212\u03b1(2K + b\u03c0)\n=\n\u03b1(V\u03c0(Q) + D\u03c0(Q)) \u2212(1 \u2212K\u00b5)\u03b12S\u03c0(Q)\n(30)\n=\n(V\u03c0(Q) + D\u03c0(Q))2\n4(1 \u2212K\u00b5)S\u03c0(Q) .\n(31)\nEq. (29) uses the bound ln(1+x) \u2265x\u2212x2/2 which holds for x \u22650 (by Taylor\u2019s theorem). Eq. (31) holds\nby our choice of \u03b1 = \u03b1\u03c0(Q), which was chosen to maximize Eq. (30). By assumption, D\u03c0(Q) > 0, which\nimplies V\u03c0(Q) > 2K. Further, since Q\u00b5(a|x) \u2265\u00b5 always, we have\nS\u03c0(Q)\n=\nbEx\u223cHt\n\u0014\n1\nQ\u00b5(\u03c0(x) | x)2\n\u0015\n\u2264\n1\n\u00b5 \u00b7 bEx\u223cHt\n\u0014\n1\nQ\u00b5(\u03c0(x) | x)\n\u0015\n= V\u03c0(Q)\n\u00b5\n.\nThus,\n(V\u03c0(Q) + D\u03c0(Q))2\nS\u03c0(Q)\n\u2265V\u03c0(Q)2\nS\u03c0(Q) = V\u03c0(Q) \u00b7 V\u03c0(Q)\nS\u03c0(Q) \u22652K\u00b5.\nPlugging into Eq. (31) completes the lemma.\nD.4\nProof of Lemma 8\nWe break the potential of Eq. (6) into pieces and bound the total change in each separately. Speci\ufb01cally,\nby straightforward algebra, we can write\n\u03a6m(Q) = \u03c6a\nm(Q) + \u03c6b\nm + \u03c6c\nm(Q) + \u03c6d\nm(Q)\nwhere\n\u03c6a\nm(Q)\n=\n\u03c4m\u00b5m\nK(1 \u2212K\u00b5m)\nbEx\u223cHt\n\"\n\u2212\nX\na\nln Q\u00b5(a|x)\n#\n\u03c6b\nm\n=\n\u03c4m\u00b5m ln K\n1 \u2212K\u00b5m\n\u03c6c\nm(Q)\n=\n\u03c4m\u00b5m\n X\n\u03c0\nQ(\u03c0) \u22121\n!\n\u03c6d\nm(Q)\n=\n\u03c4m\u00b5m\n2K\nX\n\u03c0\nQ(\u03c0)b\u03c0.\nWe assume throughout that P\n\u03c0 Q(\u03c0) \u22641 as will always be the case for the vectors produced by\nAlgorithm 2. For such a vector Q,\n\u03c6c\nm+1(Q) \u2212\u03c6c\nm(Q)\n=\n(\u03c4m+1\u00b5m+1 \u2212\u03c4m\u00b5m)\n X\n\u03c0\nQ(\u03c0) \u22121\n!\n\u22640\n24\nsince \u03c4m\u00b5m is nondecreasing. This means we can essentially disregard the change in this term.\nAlso, note that \u03c6b\nm does not depend on Q. Therefore, for this term, we get a telescoping sum:\nM\nX\nm=1\n(\u03c6b\nm+1 \u2212\u03c6b\nm) = \u03c6b\nM+1 \u2212\u03c6b\n1 \u2264\u03c6b\nM+1 \u22642\nr\nT dT\nK\nln K\nsince K\u00b5M+1 \u22641/2, and where dT , used in the de\ufb01nition of \u00b5m, is de\ufb01ned in Eq. (12).\nNext, we tackle \u03c6a\nm:\nLemma 19.\nM\nX\nm=1\n(\u03c6a\nm+1(Qm) \u2212\u03c6a\nm(Qm)) \u22646\nr\nT dT\nK\nln(1/\u00b5M+1).\nProof. For the purposes of this proof, let\nCm =\n\u00b5m\n1 \u2212K\u00b5m\n.\nThen we can write\n\u03c6a\nm(Q) = \u2212Cm\nK\n\u03c4m\nX\nt=1\nX\na\nln Q\u00b5m(a|xt).\nNote that Cm \u2265Cm+1 since \u00b5m \u2265\u00b5m+1 and \u2212ln Q\u00b5m(a|xt) \u22650. Thus,\n\u03c6a\nm+1(Q) \u2212\u03c6a\nm(Q)\n\u2264\nCm+1\nK\n\u0014 \u03c4m\nX\nt=1\nX\na\nln Q\u00b5m(a|xt)\n\u2212\n\u03c4m+1\nX\nt=1\nX\na\nln Q\u00b5m+1(a|xt)\n\u0015\n=\nCm+1\nK\n\u0014 \u03c4m\nX\nt=1\nX\na\nln\n\u0012 Q\u00b5m(a|xt)\nQ\u00b5m+1(a|xt)\n\u0013\n\u2212\n\u03c4m+1\nX\nt=\u03c4m+1\nX\na\nln Q\u00b5m+1(a|xt).\n\u0015\n\u2264\nCm+1[\u03c4m ln(\u00b5m/\u00b5m+1) \u2212(\u03c4m+1 \u2212\u03c4m) ln \u00b5m+1].\n(32)\nEq. (32) uses Q\u00b5m+1(a|x) \u2265\u00b5m+1, and also\nQ\u00b5m(a|x)\nQ\u00b5m+1(a|x) =\n(1 \u2212K\u00b5m)Q(a|x) + \u00b5m\n(1 \u2212K\u00b5m+1)Q(a|x) + \u00b5m+1\n\u2264\n\u00b5m\n\u00b5m+1\n,\nusing \u00b5m+1 \u2264\u00b5m. A sum over the two terms appearing in Eq. (32) can now be bounded separately.\nStarting with the one on the left, since \u03c4m < \u03c4m+1 \u2264T and K\u00b5m \u22641/2, we have\nCm+1\u03c4m \u22642\u03c4m\u00b5m+1 \u22642\u03c4m+1\u00b5m+1 \u22642\nr\nT dT\nK .\nThus,\nM\nX\nm=1\nCm+1\u03c4m ln(\u00b5m/\u00b5m+1)\n\u2264\n2\nr\nT dT\nK\nM\nX\nm=1\nln(\u00b5m/\u00b5m+1)\n=\n2\nr\nT dT\nK\nln(\u00b51/\u00b5M+1))\n\u2264\n2\nr\nT dT\nK (\u2212ln(\u00b5M+1)).\n(33)\n25\nFor the second term in Eq. (32), using \u00b5m+1 \u2265\u00b5M+1 for m \u2264M, and de\ufb01nition of Cm, we have\nM\nX\nm=1\n\u2212Cm+1(\u03c4m+1 \u2212\u03c4m) ln \u00b5m+1\n\u2264\n\u22122(ln \u00b5M+1)\nM\nX\nm=1\n(\u03c4m+1 \u2212\u03c4m)\u00b5m+1\n\u2264\n\u22122(ln \u00b5M+1)\nT\nX\nt=1\n\u00b5m(t)\n\u2264\n\u22124\nr\nT dT\nK (ln \u00b5M+1)\n(34)\nby Lemma 15. Combining Eqs. (32), (33) and (34) gives the statement of the lemma.\nFinally, we come to \u03c6d\nm(Q), which, by de\ufb01nition of b\u03c0, can be rewritten as\n\u03c6d\nm(Q) = B1\u03c4m\nX\n\u03c0\nQ(\u03c0)d\nReg\u03c4m(\u03c0)\nwhere B1 = 1/(2K\u03c8) and \u03c8 is the same as appears in optimization problem (OP). Note that, conveniently,\n\u03c4m d\nReg\u03c4m(\u03c0) = bSm(\u03c0m) \u2212bSm(\u03c0),\nwhere bSm(\u03c0) is the cumulative empirical importance-weighted reward through round \u03c4m:\nbSm(\u03c0) =\n\u03c4m\nX\nt=1\n\u02c6rt(\u03c0(xt)) = \u03c4m bR\u03c4m(\u03c0).\nFrom the de\ufb01nition of \u02dcQ, we have that\n\u03c6d\nm( \u02dcQ)\n=\n\u03c6d\nm(Q)\n+B1\n \n1 \u2212\nX\n\u03c0\nQ(\u03c0)\n!\n\u03c4m d\nReg\u03c4m(\u03c0m)\n=\n\u03c6d\nm(Q)\nsince d\nReg\u03c4m(\u03c0m) = 0. And by a similar computation, \u03c6d\nm+1( \u02dcQ) \u2265\u03c6d\nm+1(Q) since d\nReg\u03c4m+1(\u03c0) is always\nnonnegative.\nTherefore,\n\u03c6d\nm+1(Qm) \u2212\u03c6d\nm(Qm)\n\u2264\n\u03c6d\nm+1( \u02dcQm) \u2212\u03c6d\nm( \u02dcQm)\n=\nB1\nX\n\u03c0\n\u02dcQm(\u03c0)\n\u0014\u0010\nbSm+1(\u03c0m+1) \u2212bSm+1(\u03c0)\n\u0011\n\u2212\n\u0010\nbSm(\u03c0m) \u2212bSm(\u03c0)\n\u0011\u0015\n=\nB1\n\u0010\nbSm+1(\u03c0m+1) \u2212bSm(\u03c0m)\n\u0011\n\u2212B1\n \u03c4m+1\nX\nt=\u03c4m+1\nX\n\u03c0\n\u02dcQm(\u03c0)\u02c6rt(\u03c0(xt))\n!\n.\n(35)\nWe separately bound the two parenthesized expressions in Eq. (35) when summed over all epochs.\nBeginning with the \ufb01rst one, we have\nM\nX\nm=1\n\u0010\nbSm+1(\u03c0m+1) \u2212bSm(\u03c0m)\n\u0011\n=\nbSM+1(\u03c0M+1) \u2212bS1(\u03c01) \u2264bSM+1(\u03c0M+1).\n26\nBut by Lemma 18 (and under the same assumptions),\nbSM+1(\u03c0M+1)\n=\n\u03c4M+1 bR\u03c4M+1(\u03c0M+1)\n\u2264\n\u03c4M+1(R(\u03c0\u22c6) + D0K\u00b5M)\n\u2264\n\u03c4M+1R(\u03c0\u22c6) + D0\np\nKT dT,\n(36)\nwhere D0 is the constant appearing in Lemma 18.\nFor the second parenthesized expression of Eq. (35), let us de\ufb01ne random variables\nZt =\nX\n\u03c0\n\u02dcQ\u03c4(t)(\u03c0)\u02c6rt(\u03c0(xt)).\nNote that Zt is nonnegative, and if m = \u03c4(t), then\nZt\n=\nX\n\u03c0\n\u02dcQm(\u03c0)\u02c6rt(\u03c0(xt))\n=\nX\na\n\u02dcQm(a|xt)\u02c6rt(a)\n=\nX\na\n\u02dcQm(a|xt)rt(a)1{a = at}\n\u02dcQ\u00b5m(a|xt)\n\u2264\nrt(at)\n1 \u2212K\u00b5m\n\u22642\nsince \u02dcQ\u00b5m(a|x) \u2265(1 \u2212K\u00b5m) \u02dcQm(a|x), and since rt(at) \u22641 and K\u00b5m \u22641/2. Therefore, by Azuma\u2019s\ninequality, with probability at least 1 \u2212\u03b4,\n\u03c4M+1\nX\nt=1\nZt \u2265\n\u03c4M+1\nX\nt=1\nE[Zt|Ht\u22121] \u2212\np\n2\u03c4M+1 ln(1/\u03b4).\nThe expectation that appears here can be computed to be\nE[Zt|Ht\u22121] =\nX\n\u03c0\n\u02dcQm(\u03c0)R(\u03c0)\nso\nR(\u03c0\u22c6) \u2212E[Zt|Ht\u22121]\n=\nX\n\u03c0\n\u02dcQm(\u03c0)(R(\u03c0\u22c6) \u2212R(\u03c0))\n=\nX\n\u03c0\n\u02dcQm(\u03c0) Reg(\u03c0)\n\u2264\n(4\u03c8 + c0)K\u00b5m\nby Lemma 14 (under the same assumptions, and using the same constants). Thus, with high probability,\n\u03c4M+1\nX\nt=1\n(R(\u03c0\u22c6) \u2212Zt)\n\u2264\n(4\u03c8 + c0)K\n\u03c4M+1\nX\nt=1\n\u00b5m(t) +\np\n2\u03c4M+1 ln(1/\u03b4)\n\u2264\n(4\u03c8 + c0)\np\n8KT dT +\np\n2T ln(1/\u03b4)\nby Lemma 16.\nCombining the above bound with our earlier inequality Eq. (36), and applying the union bound, we\n\ufb01nd that with probability at least 1 \u22122\u03b4, for all T (and corresponding M),\nM\nX\nm=1\n(\u03c6d\nm(Qm) \u2212\u03c6d\nm+1(Qm)) \u2264O\n r\nT\nK ln(T |\u03a0|/\u03b4)\n!\n.\nCombining the bounds on the separate pieces, we get the bound stated in the lemma.\n27\nD.5\nProof of Lemma 3\nWe \ufb01nally have all the pieces to establish our main bound on the oracle complexity with warm-start\npresented in Lemma 3. The proof is almost immediate, and largely follows the sketch in Section 3.4\napart from one missing bit of detail. Notice that we start Algorithm 1 with Q0 = 0, at which point\nthe objective \u03a60(Q0) = 0 since \u03c40 = 0. Initially, owing to the small values of \u03c4m, we might be in the\nregime where \u00b5 = 1/2K, where the decrease in the potential guaranteed by Lemma 7 is just \u02dcO(\u03c4/K2).\nHowever, in this regime, it is easy to check that Q0 = 0 remains a feasible solution to (OP). It clearly\nsatis\ufb01es the regret constraint Eq. (2), and \u00b5 = 1/(2K) ensures that the variance constraints Eq. (3) are\nalso met. Hence, we make no calls to the oracle in this initial regime and can focus our attention to \u03c4m\nlarge enough so that \u00b5m =\np\nd\u03c4m/(K\u03c4m).\nIn this regime, we observe that \u03c42\nm\u00b5m = d\u03c4m/K, so that Lemma 7 guarantees that we decreaes the\nobjective by at least d\u03c4m/(4K) (recalling K\u00b5m \u22650). Hence, the total decrease in our objective after\nN calls to the oracle is at least Nd\u03c4m/(4K), while the net increase is bounded by \u02dcO(\np\nT dT /K. Since\nthe potential is always positive, the number of oracle calls can be at most \u02dcO(\np\nT K/ ln(|\u03a0|/\u03b4)), which\ncompletes the proof.\nE\nProof of Theorem 4\nRecall the earlier de\ufb01nition of the low-variance distribution set\nQm = {Q \u2208\u2206\u03a0 : Q satis\ufb01es Eq. (3) in round \u03c4m}.\nFix \u03b4 \u2208(0, 1) and the epoch sequence, and assume M is large enough so \u00b5m =\np\nln(16\u03c4 2m|\u03a0|/\u03b4)/\u03c4m for\nall m \u2208N with \u03c4m \u2265\u03c4M/2. The low-variance constraint Eq. (3) gives, in round t = \u03c4m,\nbEx\u223cHt\n\u0014\n1\nQ\u00b5m(\u03c0(x)|x)\n\u0015\n\u22642K +\nd\nReg\u03c4m(\u03c0)\n\u03c8\u00b5m\n,\n\u2200\u03c0 \u2208\u03a0.\nBelow, we use a policy class \u03a0 where every policy \u03c0 \u2208\u03a0 has no regret (Reg(\u03c0) = 0), in which case\nLemma 13 implies\nbEx\u223cHt\n\u0014\n1\nQ\u00b5m(\u03c0(x)|x)\n\u0015\n\u22642K + c0K\u00b5m\n\u03c8\u00b5m\n= K\n\u0012\n2 + c0\n\u03c8\n\u0013\n,\n\u2200\u03c0 \u2208\u03a0.\nApplying Lemma 10 (and using our choice of \u00b5m) gives the following constraints: with probability at\nleast 1 \u2212\u03b4, for all m \u2208N with \u03c4m \u2265\u03c4M/2, for all \u03c0 \u2208\u03a0,\nEx\u223cDX\n\"\n1\neQ\u00b5m(\u03c0(x)|x)\n#\n\u226481.3K + 6.4K\n\u0012\n2 + c0\n\u03c8\n\u0013\n=: cK\n(37)\n(to make Q into a probability distribution eQ, the leftover mass can be put on any policy, say, already in\nthe support of Q). That is, with high probability, for every relevant epoch m, every Q \u2208Qm satis\ufb01es\nEq. (37) for all \u03c0 \u2208\u03a0.\nNext, we construct an instance with the property that these inequalities cannot be satis\ufb01ed by a very\nsparse Q. An instance is drawn uniformly at random from N di\ufb00erent contexts denoted as {1, 2, . . ., N}\n(where we set, with foresight, N := 1/(2\n\u221a\n2cK\u00b5M)).\nThe reward structure in the problem will be\nextremely simple, with action K always obtaining a reward of 1, while all the other actions obtain a\nreward of 0, independent of the context. The distribution D will be uniform over the contexts (with these\ndeterministic rewards). Our policy set \u03a0 will consist of (K \u22121)N separate policies, indexed by 1 \u2264i \u2264N\nand 1 \u2264j \u2264K \u22121. Policy \u03c0ij has the property that\n\u03c0ij(x) =\n(\nj\nif x = i,\nK\notherwise.\n28\nIn words, policy \u03c0ij takes action j on context i, and action K on all other contexts. Given the uniform\ndistribution over contexts and our reward structure, each policy obtains an identical reward\nR(\u03c0) =\n\u0012\n1 \u22121\nN\n\u0013\n\u00b7 1 + 1\nN \u00b7 0 = 1 \u22121\nN .\nIn particular, each policy has a zero expected regret as required.\nFinally, observe that on context i, \u03c0ij is the unique policy taking action j.\nHence we have that\neQ(j|i) = eQ(\u03c0ij) and eQ\u00b5m(j|i) = (1 \u2212K\u00b5m) eQ(\u03c0ij) + \u00b5m. Now, let us consider the constraint Eq. (37) for\nthe policy \u03c0ij. The left-hand side of this constraint can be simpli\ufb01ed as\nEx\u223cDX\n\"\n1\neQ\u00b5m(\u03c0(x)|x)\n#\n= 1\nN\nN\nX\nx=1\n1\neQ\u00b5m(\u03c0ij(x)|x)\n= 1\nN\nX\nx\u0338=i\n1\neQ\u00b5m(\u03c0ij(x)|x)\n+ 1\nN \u00b7\n1\neQ\u00b5m(j|i)\n\u22651\nN \u00b7\n1\neQ\u00b5m(j|i)\n.\nIf the distribution eQ does not put any support on the policy \u03c0ij, then eQ\u00b5m(j|i) = \u00b5m, and thus\nEx\u223cDX\n\"\n1\neQ\u00b5m(\u03c0(x)|x)\n#\n\u22651\nN \u00b7\n1\neQ\u00b5m(j|i)\n=\n1\nN\u00b5m\n\u2265\n1\n\u221a\n2N\u00b5M\n> cK\n(since N < 1/(\n\u221a\n2cK\u00b5M)). Such a distribution eQ violates Eq. (37), which means that every Q \u2208Qm\nmust have eQ(\u03c0ij) > 0. Since this is true for each policy \u03c0ij, we see that every Q \u2208Qm has\n|supp(Q)| \u2265(K \u22121)N =\nK \u22121\n2\n\u221a\n2cK\u00b5M\n= \u2126\n s\nK\u03c4M\nln(\u03c4M|\u03a0|/\u03b4)\n!\nwhich completes the proof.\nF\nOnline Cover algorithm\nThis section describes the pseudocode of the precise algorithm use in our experiments (Algorithm 5).\nThe minimum exploration probability \u00b5 was set as 0.05 min(1/K, 1/\n\u221a\ntK) for our evaluation.\nTwo additional details are important in Step 9:\n1. We pass a cost vector rather than a reward vector to the oracle since we have a loss minimization\nrather than a reward maximization oracle.\n2. We actually used a doubly robust estimate Dud\u00b4\u0131k et al. (2011b) with a linear reward function that\nwas trained in an online fashion.\n29\nAlgorithm 5 Online Cover\ninput Cover size n, minimum sampling probability \u00b5.\n1: Initialize online cost-sensitive minimization oracles O1, O2, . . . , On, each of which controls a policy\n\u03c0(1), \u03c0(2), . . . , \u03c0(n); U := uniform probability distribution over these policies.\n2: for round t = 1, 2, . . . do\n3:\nObserve context xt \u2208X.\n4:\n(at, pt(at)) := Sample(xt, U, \u2205, \u00b5).\n5:\nSelect action at and observe reward rt(at) \u2208[0, 1].\n6:\nfor each i = 1, 2, . . . , n do\n7:\nQi := (i \u22121)\u22121 P\nj<i 1\u03c0(j).\n8:\npi(a) := Q\u00b5(a|xt).\n9:\nCreate cost-sensitive example (xt, c) where c(a) = 1 \u2212rt(at)\npt(at)1{a = at} \u2212\n\u00b5\npi(a).\n10:\nUpdate \u03c0(i) = Oi(x, c)\n11:\nend for\n12: end for\n30\n",
        "sentence": " If the linear transformation is known, we analyze an algorithm that is structurally similar to the algorithm of Agarwal et al. [2014] and show that it enjoys a regret bound between \u00d5( \u221a KLT lnN) and \u00d5(L \u221a KT lnN), where K is the number of actions, L is the length of each action sequence, T is the number of rounds, and N is the number of policies. When no contextual information is present, this setting is referred to as semi-bandits [Audibert et al., 2014] or slate bandits [Kale et al., 2010] in the literature. Our goal is to design learning algorithms whose running time and statistical performance (measured by regret) scale with the number of simple actions rather than the number of composite actions. In the first part of the paper, we assume that the linear relationship between the reward and the feedback on the simple actions is known, and we derive a new algorithm for contextual semi-bandits that meets our goal. Our approach builds on the recent contextual bandit algorithms of Dud\u0131\u0301k et al. [2011] and Agarwal et al. [2011] and Agarwal et al. [2014] and enjoys a regret guarantee between \u00d5( \u221a KLT lnN) and \u00d5(L \u221a KT lnN), depending on the hardness of the problem, where K is the number of simple actions, each composite action consists of L simple actions, T is the number of rounds of the interaction and N is the size of a policy class that we are competing against2. [2011] and Agarwal et al. [2014] and enjoys a regret guarantee between \u00d5( \u221a KLT lnN) and \u00d5(L \u221a KT lnN), depending on the hardness of the problem, where K is the number of simple actions, each composite action consists of L simple actions, T is the number of rounds of the interaction and N is the size of a policy class that we are competing against2. The policy class \u03a0 is a set of functions mapping contexts into composite actions (e.g., linear learners, decision trees, or neural nets), which we access via an optimization oracle. We show that the algorithm makes \u00d5(T ) calls to the optimization oracle3, meaning that, given an efficient supervised learning algorithm, the algorithm has running time that is only logarithmic in |\u03a0|. This contrasts with the work of Kale et al. [2010] on contextual semi-bandits, which explicitly enumerates the policy class, and therefore has running time that is linear in |\u03a0|. , 2010, Audibert et al., 2011, 2014, Kveton et al., 2014, Chen et al., 2013, 2014, Qin et al., 2014, Kveton et al., 2015]. The majority of this research focuses on the non-contextual setting, and a typical result here is that \u00d5( \u221a KLT ) regret against the best fixed composite action is achievable. To our knowledge, only the work of Kale et al. [2010] and Qin et al. , 2010, Audibert et al., 2011, 2014, Kveton et al., 2014, Chen et al., 2013, 2014, Qin et al., 2014, Kveton et al., 2015]. The majority of this research focuses on the non-contextual setting, and a typical result here is that \u00d5( \u221a KLT ) regret against the best fixed composite action is achievable. To our knowledge, only the work of Kale et al. [2010] and Qin et al. [2014] consider the contextual setting. , 2010, Audibert et al., 2011, 2014, Kveton et al., 2014, Chen et al., 2013, 2014, Qin et al., 2014, Kveton et al., 2015]. The majority of this research focuses on the non-contextual setting, and a typical result here is that \u00d5( \u221a KLT ) regret against the best fixed composite action is achievable. To our knowledge, only the work of Kale et al. [2010] and Qin et al. [2014] consider the contextual setting. Kale et al. [2010] generalize the bandit algorithm of Auer et al. , 2010, Audibert et al., 2011, 2014, Kveton et al., 2014, Chen et al., 2013, 2014, Qin et al., 2014, Kveton et al., 2015]. The majority of this research focuses on the non-contextual setting, and a typical result here is that \u00d5( \u221a KLT ) regret against the best fixed composite action is achievable. To our knowledge, only the work of Kale et al. [2010] and Qin et al. [2014] consider the contextual setting. Kale et al. [2010] generalize the bandit algorithm of Auer et al. [2002] to the contextual semi-bandits, and achieve a \u00d5( \u221a KLT ) regret but require explicit enumeration of the policy class. , 2010, Audibert et al., 2011, 2014, Kveton et al., 2014, Chen et al., 2013, 2014, Qin et al., 2014, Kveton et al., 2015]. The majority of this research focuses on the non-contextual setting, and a typical result here is that \u00d5( \u221a KLT ) regret against the best fixed composite action is achievable. To our knowledge, only the work of Kale et al. [2010] and Qin et al. [2014] consider the contextual setting. Kale et al. [2010] generalize the bandit algorithm of Auer et al. [2002] to the contextual semi-bandits, and achieve a \u00d5( \u221a KLT ) regret but require explicit enumeration of the policy class. Qin et al. [2014] instead generalize the LinUCB algorithm of Chu et al. , 2010, Audibert et al., 2011, 2014, Kveton et al., 2014, Chen et al., 2013, 2014, Qin et al., 2014, Kveton et al., 2015]. The majority of this research focuses on the non-contextual setting, and a typical result here is that \u00d5( \u221a KLT ) regret against the best fixed composite action is achievable. To our knowledge, only the work of Kale et al. [2010] and Qin et al. [2014] consider the contextual setting. Kale et al. [2010] generalize the bandit algorithm of Auer et al. [2002] to the contextual semi-bandits, and achieve a \u00d5( \u221a KLT ) regret but require explicit enumeration of the policy class. Qin et al. [2014] instead generalize the LinUCB algorithm of Chu et al. [2011] to semi-bandits, imposing the assumption that the feedback on the simple actions is linearly related 2Extension to VC classes is straightforward using standard arguments. 3The dependence can be improved to \u00d5(T 1/2) using warm-start and epoching ideas identical to Agarwal et al. [2014]. When the weights in the linear transformation are known, we propose an algorithm that has a similar structure to a recent algorithm for the classical contextual bandit problem [Agarwal et al., 2014]. The main differences between SEMIBANDIT-VCEE and the algorithm of Agarwal et al. [2014] are in the OP and the definitions. This problem is similar to the one used by Agarwal et al. [2014] for classical contextual bandit learning, and following their approach, we provide a coordinate descent procedure in the policy space (See Algorithm 2). This problem is similar to the one used by Agarwal et al. [2014] for classical contextual bandit learning, and following their approach, we provide a coordinate descent procedure in the policy space (See Algorithm 2). There are two types of updates in the algorithm. If the weights Q are too large or the regret constraint in Equation 1 is violated, the algorithm multiplicatively shrinks all of the weights. Otherwise, if there is a policy that is found to violate the variance constraint in Equation 2, the algorithm adds weight to that policy, so that the constraint is no longer violated. We have the following computational guarantee on the coordinate descent algorithm: Theorem 2. For any history H and parameter \u03bc, Algorithm 2 halts and outputs a set of weights Q \u2208 \u2206|\u03a0| that is feasible for (OP). Moreover, Algorithm 2 halts in no more than 8 log(1/(K\u03bc)) \u03bcpmin iterations and each iteration can be implemented efficiently, with at most one call to AMO. Since the main ideas used to prove this theorem are borrowed from the proof of Agarwal et al. [2014], we only provide a sketch in Section 2. This problem is similar to the one used by Agarwal et al. [2014] for classical contextual bandit learning, and following their approach, we provide a coordinate descent procedure in the policy space (See Algorithm 2). There are two types of updates in the algorithm. If the weights Q are too large or the regret constraint in Equation 1 is violated, the algorithm multiplicatively shrinks all of the weights. Otherwise, if there is a policy that is found to violate the variance constraint in Equation 2, the algorithm adds weight to that policy, so that the constraint is no longer violated. We have the following computational guarantee on the coordinate descent algorithm: Theorem 2. For any history H and parameter \u03bc, Algorithm 2 halts and outputs a set of weights Q \u2208 \u2206|\u03a0| that is feasible for (OP). Moreover, Algorithm 2 halts in no more than 8 log(1/(K\u03bc)) \u03bcpmin iterations and each iteration can be implemented efficiently, with at most one call to AMO. Since the main ideas used to prove this theorem are borrowed from the proof of Agarwal et al. [2014], we only provide a sketch in Section 2.4. Equipped with this theorem, it is easy to see that the total number of calls to the AMO over the course of the execution of Algorithm 1 can be bounded as \u00d5 ( T 3/2 \u221a K pmin log(|\u03a0|/\u03b4) ) by the setting of \u03bct. Moreover, due to the nature of the coordinate descent algorithm, the weight vector Q remains sparse, so we can manipulate it efficiently and avoid running time that is linear in |\u03a0|. As mentioned, this contrasts with the exponential-weights style algorithm of Kale et al. [2010] which maintains a dense weight vector over \u2206|\u03a0|. This problem is similar to the one used by Agarwal et al. [2014] for classical contextual bandit learning, and following their approach, we provide a coordinate descent procedure in the policy space (See Algorithm 2). There are two types of updates in the algorithm. If the weights Q are too large or the regret constraint in Equation 1 is violated, the algorithm multiplicatively shrinks all of the weights. Otherwise, if there is a policy that is found to violate the variance constraint in Equation 2, the algorithm adds weight to that policy, so that the constraint is no longer violated. We have the following computational guarantee on the coordinate descent algorithm: Theorem 2. For any history H and parameter \u03bc, Algorithm 2 halts and outputs a set of weights Q \u2208 \u2206|\u03a0| that is feasible for (OP). Moreover, Algorithm 2 halts in no more than 8 log(1/(K\u03bc)) \u03bcpmin iterations and each iteration can be implemented efficiently, with at most one call to AMO. Since the main ideas used to prove this theorem are borrowed from the proof of Agarwal et al. [2014], we only provide a sketch in Section 2.4. Equipped with this theorem, it is easy to see that the total number of calls to the AMO over the course of the execution of Algorithm 1 can be bounded as \u00d5 ( T 3/2 \u221a K pmin log(|\u03a0|/\u03b4) ) by the setting of \u03bct. Moreover, due to the nature of the coordinate descent algorithm, the weight vector Q remains sparse, so we can manipulate it efficiently and avoid running time that is linear in |\u03a0|. As mentioned, this contrasts with the exponential-weights style algorithm of Kale et al. [2010] which maintains a dense weight vector over \u2206|\u03a0|. We mention in passing that Agarwal et al. [2014] also develop two improvements that lead to a more efficient algorithm. These are the analogs of Lemmas 6 and 7 in Agarwal et al. [2014]. The proof of the first is based on showing that the derivative of the function g(c) = \u03a6(cQ) is positive so that by convexity of \u03a6, shrinking the weights Q can only decrease the potential. If this linear relationship is known, we showed that an adaptation of the algorithm of Agarwal et al. [2014] achieves regret between \u00d5( \u221a KLT lnN) and \u00d5(L \u221a KT lnN), and can be implemented efficiently with access to an optimization oracle. The proof of this theorem is similar in spirit to a related theorem in Agarwal et al. [2014]. We first use Freedman\u2019s inequality (Lemma 18) to argue that for a fixed P, \u03c0, \u03bc, and t, the empirical version of the variance is close to the true variance. Our definition of \u03bct differs from Agarwal et al. [2014] only in the introduction of pmin, so by a straightforward adaptation we have: Lemma 10. The first three are fairly straightforward and the proof of the later two are based on the arguments of Agarwal et al. [2014]. For the first claim we have:",
        "context": "running time of this algorithm is still prohibitive for most natural problems owing to the \u02dcO(T 5) scaling.\nIn this work, we prove the following1:\nTheorem 1. There is an algorithm for the i.i.d. contextual bandit problem with an optimal regret bound\nHowever, these algorithms have suboptimal regret bounds of\nO((K log |\u03a0|)1/3T 2/3) because the algorithms randomize uniformly over actions when they choose to\nexplore.\non the (conditionally) expected regret in round t using the above lemma: with high probability,\nX\n\u03c0\u2208\u03a0\neQm\u22121 Reg(\u03c0) \u2264O(K\u00b5m\u22121).\nSumming these terms up over all T rounds and applying martingale concentration gives the \ufb01nal regret\nbound in Theorem 2.\n5"
    },
    {
        "title": "Minimax policies for combinatorial prediction",
        "author": [
            "Jean-Yves Audibert",
            "S\u00e9bastien Bubeck",
            "G\u00e1bor Lugosi"
        ],
        "venue": "games. arXiv:1105.4871,",
        "citeRegEx": "Audibert et al\\.,? \\Q2011\\E",
        "shortCiteRegEx": "Audibert et al\\.",
        "year": 2011,
        "abstract": "",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "Regret in online combinatorial optimization",
        "author": [
            "Jean-Yves Audibert",
            "S\u00e9bastien Bubeck",
            "G\u00e1bor Lugosi"
        ],
        "venue": "Mathematics of Operations Research,",
        "citeRegEx": "Audibert et al\\.,? \\Q2014\\E",
        "shortCiteRegEx": "Audibert et al\\.",
        "year": 2014,
        "abstract": "We address online linear optimization problems when the possible actions of\nthe decision maker are represented by binary vectors. The regret of the\ndecision maker is the difference between her realized loss and the best loss\nshe would have achieved by picking, in hindsight, the best possible action. Our\ngoal is to understand the magnitude of the best possible (minimax) regret. We\nstudy the problem under three different assumptions for the feedback the\ndecision maker receives: full information, and the partial information models\nof the so-called \"semi-bandit\" and \"bandit\" problems. Combining the Mirror\nDescent algorithm and the INF (Implicitely Normalized Forecaster) strategy, we\nare able to prove optimal bounds for the semi-bandit case. We also recover the\noptimal bounds for the full information setting. In the bandit case we discuss\nexisting results in light of a new lower bound, and suggest a conjecture on the\noptimal regret in that case. Finally we also prove that the standard\nexponentially weighted average forecaster is provably suboptimal in the setting\nof online combinatorial optimization.",
        "full_text": "arXiv:1204.4710v2  [cs.LG]  29 Mar 2013\nRegret in Online Combinatorial Optimization\nJean-Yves Audibert\nImagine, Universit\u00b4e Paris Est,\nand Sierra, CNRS/ENS/INRIA\naudibert@imagine.enpc.fr\nS\u00b4ebastien Bubeck\nDepartment of Operations Research and Financial Engineering,\nPrinceton University\nsbubeck@princeton.edu\nG\u00b4abor Lugosi\nICREA and Pompeu Fabra University\ngabor.lugosi@upf.edu\nApril 2, 2013\nAbstract\nWe address online linear optimization problems when the possible actions of the decision\nmaker are represented by binary vectors. The regret of the decision maker is the difference\nbetween her realized loss and the minimal loss she would have achieved by picking, in hind-\nsight, the best possible action. Our goal is to understand the magnitude of the best possible\n(minimax) regret. We study the problem under three different assumptions for the feedback\nthe decision maker receives: full information, and the partial information models of the so-\ncalled \u201csemi-bandit\u201d and \u201cbandit\u201d problems. In the full information case we show that the\nstandard exponentially weighted average forecaster is a provably suboptimal strategy. For the\nsemi-bandit model, by combining the Mirror Descent algorithm and the INF (Implicitely Nor-\nmalized Forecaster) strategy, we are able to prove the \ufb01rst optimal bounds. Finally, in the\nbandit case we discuss existing results in light of a new lower bound, and suggest a conjecture\non the optimal regret in that case.\n1\nIntroduction.\nIn this paper we consider the framework of online linear optimization. The setup may be described\nas a repeated game between a \u201cdecision maker\u201d (or simply \u201cplayer\u201d or \u201cforecaster\u201d) and an \u201cadver-\n1\nsary\u201d as follows: at each time instance t = 1, . . . , n, the player chooses, possibly in a randomized\nway, an action from a given \ufb01nite action set A \u2282Rd. The action chosen by the player at time t is\ndenoted by at \u2208A. Simultaneously to the player, the adversary chooses a loss vector zt \u2208Z \u2282Rd\nand the loss incurred by the forecaster is aT\nt zt. The goal of the player is to minimize the expected\ncumulative loss E Pn\nt=1 aT\nt zt where the expectation is taken with respect to the player\u2019s internal\nrandomization (and eventually the adversary\u2019s randomization).\nIn the basic \u201cfull-information\u201d version of this problem, the player observes the adversary\u2019s\nmove zt at the end of round t. Another important model for feedback is the so-called bandit\nproblem, in which the player only observes the incurred loss aT\nt zt. As a measure of performance\nwe de\ufb01ne the regret 1 of the player as\nRn = E\nn\nX\nt=1\naT\nt zt \u2212min\na\u2208A E\nn\nX\nt=1\naTzt .\nIn this paper we address a speci\ufb01c example of online linear optimization: we assume that the action\nset A is a subset of the d-dimensional hypercube {0, 1}d such that \u2200a \u2208A, ||a||1 = m, and the\nadversary has a bounded loss per coordinate, that is2 Z = [0, 1]d. We call this setting online com-\nbinatorial optimization. As we will see below, this restriction of the general framework contains\na rich class of problems. Indeed, in many interesting cases, actions are naturally represented by\nBoolean vectors.\nIn addition to the full information and bandit versions of online combinatorial optimization,\nwe also consider another type of feedback which makes sense only in this combinatorial setting.\nIn the semi-bandit version, we assume that the player observes only the coordinates of zt that\nwere played in at, that is the player observes the vector (at(1)zt(1), . . . , at(d)zt(d)). All three\nvariants of online combinatorial optimization are sketched in Figure 1. More rigorously, online\ncombinatorial optimization is de\ufb01ned as a repeated game between a \u201cplayer\u201d and an \u201cadversary.\u201d\nAt each round t = 1, . . . , n of the game, the player chooses a probability distribution pt over the\nset of actions A \u2282{0, 1}d and draws a random action at \u2208A according to pt. Simultaneously, the\nadversary chooses a vector zt \u2208[0, 1]d. More formally, zt is a measurable function of the \u201cpast\u201d\n(ps, as, zs)s=1,...,t\u22121. In the full information case, pt is a measurable function of (ps, as, zs)s=1,...,t\u22121.\nIn the semi-bandit case, pt is a measurable function of (ps, as, (as(i)zs(i))i=1,...,d)s=1,...,t\u22121 and in\nthe bandit problem it is a measurable function of (ps, as, (aT\ns zs))s=1,...,t\u22121.\n1.1\nMotivating examples.\nMany problems can be tackled under the online combinatorial optimization framework. We give\nhere three simple examples:\n\u2022 m-sets. In this example we consider the set A of all\n\u0000 d\nm\n\u0001\nBoolean vectors in dimension d\nwith exactly m ones. In other words, at every time step, the player selects m actions out of\n1In the full information version, it is straightforward to obtain upper bounds for the stronger notion of regret\nE Pn\nt=1 aT\nt zt \u2212E mina\u2208A\nPn\nt=1 aT zt which is always at least as large as Rn. However, for partial information\ngames, this requires more work. In this paper we only consider Rn as a measure of the regret.\n2Note that since all actions have the same size, i.e. ||a||1 = m, \u2200a \u2208A, one can reduce the case of Z = [\u03b1, \u03b2]d to\nZ = [0, 1]d via a simple renormalization.\n2\nParameters: set of actions A \u2282{0, 1}d; number of rounds n \u2208N.\nFor each round t = 1, 2, . . . , n;\n(1) the player chooses a probability distribution pt over A and draws a random action at \u2208A ac-\ncording to pt;\n(2) simultaneously, the adversary selects a loss vector zt \u2208[0, 1]d (without revealing it);\n(3) the player incurs the loss aT\nt zt. She observes\n\u2013 the loss vector zt in the full information setting,\n\u2013 the coordinates zt(i)at(i) in the semi-bandit setting,\n\u2013 the instantaneous loss aT\nt zt in the bandit setting.\nGoal: The player tries to minimize her cumulative loss Pn\nt=1 aT\nt zt.\nFigure 1: Online combinatorial optimization.\nd possibilities. When m = 1, the semi-bandit and bandit versions coincide and correspond\nto the standard (adversarial) multi-armed bandit problem.\n\u2022 Online shortest path problem. Consider a communication network represented by a graph\nin which one has to send a sequence of packets from one \ufb01xed vertex to another. For each\npacket one chooses a path through the graph and suffers a certain delay which is the sum of\nthe delays on the edges of the path. Depending on the traf\ufb01c, the delays on the edges may\nchange, and, at the end of each round, according to the assumed level of feedback, the player\nobserves either the delays of all edges, the delays of each edge on the chosen path, or only\nthe total delay of the chosen path. The player\u2019s objective is to minimize the total delay for\nthe sequence of packets.\nOne can represent the set of valid paths from the starting vertex to the end vertex as a set\nA \u2282{0, 1}d where d is the number of edges. If at time t, zt \u2208[0, 1]d is the vector of delays\non the edges, then the delay of a path a \u2208A is zT\nt a. Thus this problem is an instance of\nonline combinatorial optimization in dimension d, where d is the number of edges in the\ngraph. In this paper we assume, for simplicity, that all valid paths have the same length m.\n\u2022 Ranking. Consider the problem of selecting a ranking of m items out of M possible items.\nFor example a website could have a set of M ads, and it has to select a ranked list of m of\nthese ads to appear on the webpage. One can rephrase this problem as selecting a matching of\nsize m on the complete bipartite graph Km,M (with d = m\u00d7M edges). In the online learning\nversion of this problem, each day the website chooses one such list, and gains one dollar for\neach click on the ads. This problem can easily be formulated as an online combinatorial\noptimization problem.\nOur theory applies to many more examples, such as spanning trees (which can be useful in certain\ncommunication problems), or m-intervals.\n3\n1.2\nPrevious work.\n\u2022 Full Information. The full-information setting is now fairly well understood, and an op-\ntimal regret bound (in terms of m, d, n) was obtained by Koolen, Warmuth, and Kivinen\n[26]. Previous papers under full information feedback also include Gentile and Warmuth\n[14], Kivinen and Warmuth [25], Grove, Littlestone, and Schuurmans [15], Takimoto and\nWarmuth [34], Kalai and Vempala [22], Warmuth and Kuzmin [36], Herbster and Warmuth\n[19], and Hazan, Kale, and Warmuth [18].\n\u2022 Semi-bandit. The \ufb01rst paper on the adversarial multi-armed bandit problem (i.e., the special\ncase of m-sets with m = 1) is by Auer, Cesa-Bianchi, Freund, and Schapire [4] who derived\na regret bound of order \u221adn log d. This result was improved to\n\u221a\ndn by Audibert and Bubeck\n[2, 3]. Gy\u00a8orgy, Linder, Lugosi, and Ottucs\u00b4ak [16] consider the online shortest path problem\nand derive suboptimal regret bounds (in terms of the dependency on m and d). Uchiya,\nNakamura, and Kudo [35] (respectively Kale, Reyzin, and Schapire [23]) derived optimal\nregret bounds for the case of m-sets (respectively for the problem of ranking selection) up to\nlogarithmic factors.\n\u2022 Bandit. McMahan and Blum [27], and Awerbuch and Kleinberg [5] were the \ufb01rst to consider\nthis setting, and obtained suboptimal regret bounds (in terms of n). The \ufb01rst paper with\noptimal dependency in n was by Dani, Hayes, and Kakade [12]. The dependency on m and\nd was then improved in various ways by Abernethy, Hazan, and Rakhlin [1], Cesa-Bianchi,\nand Lugosi [11], and Bubeck, Cesa-Bianchi, and Kakade [9]. We discuss these bounds in\ndetail in Section 4. In particular, we argue that the optimal regret bound in terms of d (and\nm) is still an open problem.\nWe also refer the interested reader to the recent survey [8] for an overview of bandit problems in\nvarious other settings.\n1.3\nContribution and contents of the paper.\nIn this paper we are primarily interested in the optimal minimax regret in terms of m, d and n. More\nprecisely, our aim is to determine the order of magnitude of the following quantity: For a given\nfeedback assumption, write sup for the supremum over all adversaries and inf for the in\ufb01mum\nover all allowed strategies for the player under the feedback assumption. (Recall the de\ufb01nition of\n\u201cadversary\u201d and \u201cplayer\u201d from the introduction.) Then we are interested in\nmax\nA\u2282{0,1}d:\u2200a\u2208A,||a||1=m inf sup Rn.\nOur contribution to the study of this quantity is threefold. First, we unify the algorithms used\nin Abernethy, Hazan, and Rakhlin [1], Koolen, Warmuth, and Kivinen [26], Uchiya, Nakamura,\nand Kudo [35], and Kale, Reyzin, and Schapire [23] under the umbrella of mirror descent. The\nidea of mirror descent goes back to Nemirovski [28], Nemirovski and Yudin [29]. A somewhat\nsimilar concept was re-discovered in online learning by Herbster and Warmuth [20], Grove, Little-\nstone, and Schuurmans [15], Kivinen and Warmuth [25] under the name of potential-based gradient\n4\nFull Information\nSemi-Bandit\nBandit\nLower Bound\nm\nq\nn log d\nm\n\u221a\nmdn\nm\n\u221a\ndn\nUpper Bound\nm\nq\nn log d\nm\n\u221a\nmdn\nm3/2\nq\ndn log d\nm\nTable 1: Bounds on the minimax regret (up to constant factors). The new results are set in boldface. In this\npaper we also show that EXP2 in the full information case has a regret bounded below by d3/2\u221an (when m\nis of order d).\ndescent, see [10, Chapter 11]. Recently, these ideas have been \ufb02ourishing, see for instance Shalev-\nSchwartz [33], Rakhlin [30], Hazan [17], and Bubeck [7]. Our main theorem (Theorem 2) allows\none to recover almost all known regret bounds for online combinatorial optimization. This \ufb01rst\ncontribution leads to our second main result, the improvement of the known upper bounds for the\nsemi-bandit game. In particular, we propose a different proof of the minimax regret bound of the\norder of\n\u221a\nnd in the standard d-armed bandit game that is much simpler than the one provided\nin Audibert and Bubeck [3] (which also improves the constant factor). In addition to these upper\nbounds we prove two new lower bounds. First we answer a question of Koolen, Warmuth, and\nKivinen [26] by showing that the exponentially weighted average forecaster is provably subopti-\nmal for online combinatorial optimization. Our second lower bound is a minimax lower bound in\nthe bandit setting which improves known results by an order of magnitude. A summary of known\nbounds and the new bounds proved in this paper can be found in Table 1.\nThe paper is organized as follows. In Section 2 we introduce the two algorithms discussed\nin this paper. In particular in Section 2.1 we discuss the popular exponentially weighted average\nforecaster and we show that it is a provably suboptimal strategy. Then in Section 2.2 we describe\nour main algorithm, OSMD (Online Stochastic Mirror Descent), and prove a general regret bound in\nterms of the Bregman divergence of the Fenchel-Legendre dual of the Legendre function de\ufb01ning\nthe strategy. In Section 3 we derive upper bounds for the regret in the semi-bandit case for OSMD\nwith appropriately chosen Legendre functions. Finally in Section 4 we prove a new lower bound\nfor the bandit setting, and we formulate a conjecture on the correct order of magnitude of the regret\nfor that problem based on this new result and the regret bounds obtained in [1, 9].\n2\nAlgorithms.\nIn this section we discuss two classes of algorithms that have been proposed for online combina-\ntorial optimization.\n2.1\nExpanded Exponential weights (EXP2).\nThe simplest approach to online combinatorial optimization is to consider each action of A as\nan independent \u201cexpert,\u201d and then apply a generic regret minimizing strategy. Perhaps the most\npopular such strategy is the exponentially weighted average forecaster (see, e.g., [10]). (This\n5\nstrategy is sometimes called Hedge, see Freund and Schapire [13].) We call the resulting strategy\nfor the online combinatorial optimization problem EXP2, see Figure 2. In the full information\nsetting, EXP2 corresponds to \u201cExpanded Hedge,\u201d as de\ufb01ned in Koolen, Warmuth, and Kivinen\n[26]. In the semi-bandit case, EXP2 was studied by Gy\u00a8orgy, Linder, Lugosi, and Ottucs\u00b4ak [16]\nwhile in the bandit case in Dani, Hayes, and Kakade [12], Cesa-Bianchi and Lugosi [11], and\nBubeck, Cesa-Bianchi, and Kakade [9]. Note that in the bandit case, EXP2 is mixed with an\nexploration distribution, see Section 4 for more details.\nDespite strong interest in this strategy, no optimal regret bound has been derived for it in the\ncombinatorial setting. More precisely, the best bound (which can be derived from a standard\nargument, see for example [12] or [26]) is of order m3/2q\nn log\n\u0000 d\nm\n\u0001\n. On the other hand, in [26]\nthe authors showed that by using Mirror Descent (see next section) with the negative entropy, one\nobtains a regret bounded by m\nq\nn log\n\u0000 d\nm\n\u0001\n. Furthermore this latter bound is clearly optimal (up\nto a numerical constant), as one can see from the standard lower bound in prediction with expert\nadvice (consider the set A that corresponds to playing m expert problems in parallel with d/m\nexperts in each problem). In [26] the authors leave as an open question the problem of whether it\nwould be possible to improve the bound for EXP2 to obtain the optimal order of magnitude. The\nfollowing theorem shows that this is impossible, and that in fact EXP2 is a provably suboptimal\nstrategy.\nTheorem 1 Let n \u2265d. There exists a subset A \u2282{0, 1}d such that in the full information setting,\nthe regret of the EXP2 strategy (for any learning rate \u03b7), satis\ufb01es\nsup\nadversary\nRn \u22650.01 d3/2\u221an.\nThe proof is deferred to the Appendix.\n2.2\nOnline Stochastic Mirror Descent.\nIn this section we describe the main algorithm studied in this paper. We call it Online Stochastic\nMirror Descent (OSMD). Each term in this name refers to a part of the algorithm: Mirror Descent\noriginates in the work of Nemirovski and Yudin [29]. The idea of mirror descent is to perform a\ngradient descent, where the update with the gradient is performed in the dual space (de\ufb01ned by\nsome Legendre function F) rather than in the primal (see below for a precise formulation). The\nStochastic part takes its origin from Robbins and Monro [31] and from Kiefer and Wolfowitz [24].\nThe key idea is that it is enough to observe an unbiased estimate of the gradient rather than the true\ngradient in order to perform a gradient descent. Finally the Online part comes from Zinkevich [37].\nZinkevich derived the Online Gradient Descent (OGD) algorithm, which is a version of gradient\ndescent tailored to online optimization.\nTo properly describe the OSMD strategy, we recall a few concepts from convex analysis, see\nHiriart-Urruty and Lemar\u00b4echal [21] for a thorough treatment of this subject. Let D \u2282Rd be an\nopen convex set, and D the closure of D.\nDe\ufb01nition 1 We call Legendre any continuous function F : D \u2192R such that\n(i) F is strictly convex continuously differentiable on D,\n6\nEXP2:\nParameter: Learning rate \u03b7.\nLet p1 =\n\u0000 1\n|A|, . . . ,\n1\n|A|\n\u0001\n\u2208R|A|.\nFor each round t = 1, 2, . . . , n;\n(a) Play at \u223cpt and observe\n\u2013 the loss vector zt in the full information game,\n\u2013 the coordinates zt(i)1at(i)=1 in the semi-bandit game,\n\u2013 the instantaneous loss aT\nt zt in the bandit game.\n(b) Estimate the loss vector zt by ezt. For instance, one may take\n\u2013 ezt = zt in the full information game,\n\u2013 ezt(i) =\nzt(i)\nP\na\u2208A:a(i)=1 pt(a)at(i) in the semi-bandit game,\n\u2013 ezt = P +\nt ataT\nt zt, with Pt = Ea\u223cpt(aaT ) in the bandit game.\n(c) Update the probabilities, for all a \u2208A,\npt+1(a) =\nexp(\u2212\u03b7aT ezt)pt(a)\nP\nb\u2208A exp(\u2212\u03b7bT ezT\nt )pt(b).\nFigure 2: The EXP2 strategy. The notation Ea\u223cpt denotes expected value with respect to the random choice\nof a when it is distributed according to pt.\n(ii) limx\u2192D\\D ||\u2207F(x)|| = +\u221e.3\nThe Bregman divergence DF : D \u00d7 D associated to a Legendre function F is de\ufb01ned by\nDF(x, y) = F(x) \u2212F(y) \u2212(x \u2212y)T\u2207F(y).\nMoreover, we say that D\u2217= \u2207F(D) is the dual space of D under F. We also denote by F \u2217the\nLegendre-Fenchel transform of F de\ufb01ned by\nF \u2217(u) = sup\nx\u2208D\n\u0000xTu \u2212F(x)\n\u0001\n.\nLemma 1 Let F be a Legendre function. Then F \u2217\u2217= F and \u2207F \u2217= (\u2207F)\u22121 on the set D\u2217.\nMoreover, \u2200x, y \u2208D,\nDF(x, y) = DF \u2217(\u2207F(y), \u2207F(x)).\n(1)\n3By the equivalence of norms in Rd, this de\ufb01nition does not depend on the choice of the norm.\n7\nThe lemma above is the key to understanding how a Legendre function acts on the space. The\ngradient \u2207F maps D to the dual space D\u2217, and \u2207F \u2217is the inverse mapping from the dual space\nto the original (primal) space. Moreover, (1) shows that the Bregman divergence in the primal\nspace corresponds exactly to the Bregman divergence of the Legendre-Fenchel transform in the\ndual space. A proof of this result can be found, for example, in [Chapter 11, [10]].\nWe now have all ingredients to describe the OSMD strategy, see Figure 3 for the precise for-\nmulation. Note that step (d) is well de\ufb01ned if the following consistency condition is satis\ufb01ed:\n\u2207F(x) \u2212\u03b7ezt \u2208D\u2217, \u2200x \u2208Conv(A) \u2229D.\n(2)\nIn the full information setting, algorithms of this type were studied by Abernethy, Hazan, and\nRakhlin [1], Rakhlin [30], and Hazan [17]. In these papers the authors adopted the presenta-\ntion suggested by Beck and Teboulle [6], which corresponds to a Follow-the-Regularized-Leader\n(FTRL) type strategy. There the focus was on F being strongly convex with respect to some norm.\nMoreover, in [1] the authors also consider the bandit case, and switch to F being a self-concordant\nbarrier for the convex hull of A (see Section 4 for more details). Another line of work studied this\ntype of algorithms with F being the negative entropy, see Koolen, Warmuth, and Kivinen [26] for\nthe full information case and Uchiya, Nakamura, and Kudo [35], Kale, Reyzin, and Schapire [23]\nfor speci\ufb01c instances of the semi-bandit case. All these results are uni\ufb01ed and described in details\nin Bubeck [7]. In this paper we consider a new type of Legendre functions F inspired by Audibert\nand Bubeck [3], see Section 3.\nRegarding computational complexity, OSMD is ef\ufb01cient as soon as the polytope Conv(A) can\nbe described by a polynomial (in d) number of constraints. Indeed in that case steps (a)-(b) can be\nperformed ef\ufb01ciently jointly (one can get an algorithm by looking at the proof of Carath\u00b4eodory\u2019s\ntheorem), and step (d) is a convex program with a polynomial number of constraints. In many\ninteresting examples (such as m-sets, selection of rankings, spanning trees, paths in acyclic graphs)\none can describe the convex hull of A by a polynomial number of constraints, see Schrijver [32].\nOn the other hand, there also exist important examples where this is not the case (such as paths on\ngeneral graphs). Also note that for some speci\ufb01c examples it is possible to implement OSMD with\nimproved computational complexity, see Koolen, Warmuth, and Kivinen [26].\nIn this paper we restrict our attention to the combinatorial learning setting in which A is a\nsubset of {0, 1}d and the loss is linear. However, one should note that this speci\ufb01c form of A plays\nno role in the de\ufb01nition of OSMD. Moreover, if the loss is not linear, then one can modify OSMD\nby performing a gradient update with a gradient of the loss (rather than the loss vector zt). See\nBubeck [7] for more details on this approach.\nThe following result is at the basis of our improved regret bounds for OSMD in the semi-bandit\nsetting, see Section 3.\nTheorem 2 Suppose that (2) is satis\ufb01ed and the loss estimates are unbiased in the sense that\nEat\u223cptezt = zt. Then the regret of the OSMD strategy satis\ufb01es\nRn \u2264supa\u2208A F(a) \u2212F(x1)\n\u03b7\n+ 1\n\u03b7\nn\nX\nt=1\nEDF \u2217\n\u0012\n\u2207F(xt) \u2212\u03b7ezt, \u2207F(xt)\n\u0013\n.\n8\nOSMD:\nParameters:\n\u2022 learning rate \u03b7 > 0,\n\u2022 Legendre function F de\ufb01ned on D \u2283Conv(A).\nLet x1 \u2208argminx\u2208Conv(A) F(x).\nFor each round t = 1, 2, . . . , n;\n(a) Let pt be a distribution on the set A such that xt = Ea\u223cpta.\n(b) Draw a random action at according to the distribution pt and observe the feedback.\n(c) Based on the observed feedback, estimate the loss vector zt by ezt.\n(d) Let wt+1 \u2208D satisfy\n\u2207F(wt+1) = \u2207F(xt) \u2212\u03b7ezt.\n(3)\n(e) Project the weight vector wt+1 de\ufb01ned by (3) on the convex hull of A:\nxt+1 \u2208argmin\nx\u2208Conv(A)\nDF(x, wt+1).\n(4)\nFigure 3: Online Stochastic Mirror Descent (OSMD).\nProof Let a \u2208A. Using that at and ezt are unbiased estimates of xt and zt, we have\nE\nn\nX\nt=1\n(at \u2212a)Tzt = E\nn\nX\nt=1\n(xt \u2212a)T ezt.\nUsing (3), and applying the de\ufb01nition of the Bregman divergences, one obtains\n\u03b7ezT\nt (xt \u2212a) = (a \u2212xt)T\u0000\u2207F(wt+1) \u2212\u2207F(xt)\n\u0001\n= DF(a, xt) + DF(xt, wt+1) \u2212DF(a, wt+1).\nBy the Pythagorean theorem for Bregman divergences (see, e.g., Lemma 11.3 of [10]), we have\nDF(a, wt+1) \u2265DF(a, xt+1) + DF(xt+1, wt+1), hence\n\u03b7ezT\nt (xt \u2212a) \u2264DF(a, xt) + DF(xt, wt+1) \u2212DF(a, xt+1) \u2212DF(xt+1, wt+1) .\nSumming over t gives\nn\nX\nt=1\n\u03b7ezT\nt (xt \u2212a) \u2264DF(a, a1) \u2212DF(a, an+1) +\nn\nX\nt=1\n\u0000DF(xt, wt+1) \u2212DF(xt+1, wt+1)\n\u0001\n.\n9\nBy the nonnegativity of the Bregman divergences, we get\nn\nX\nt=1\n\u03b7ezT\nt (xt \u2212a) \u2264DF(a, a1) +\nn\nX\nt=1\nDF(xt, wt+1).\nFrom (1), one has DF(xt, wt+1) = DF \u2217\u0000\u2207F(xt) \u2212\u03b7ezt, \u2207F(xt)\n\u0001\n. Moreover, by writing the \ufb01rst-\norder optimality condition for x1, one directly obtains DF(a, x1) \u2264F(a)\u2212F(x1) which concludes\nthe proof.\nNote that, if F admits an Hessian, denoted \u22072F, that is always invertible, then one can prove\nthat, up to a third-order term\n\u0000in ezt\n\u0001\n, the regret bound can be written as\nRn \u2a85supa\u2208A F(a) \u2212F(x1)\n\u03b7\n+ \u03b7\n2\nn\nX\nt=1\nezT\nt\n\u0000\u22072F(xt)\n\u0001\u22121 ezt.\n(5)\nThe main technical dif\ufb01culty is to control the third-order error term in this inequality.\n3\nSemi-bandit feedback.\nIn this section we consider online combinatorial optimization with semi-bandit feedback. As we\nalready discussed, in the full information case Koolen, Warmuth, and Kivinen [26] proved that\nOSMD with the negative entropy is a minimax optimal strategy. We \ufb01rst prove a regret bound when\none uses this strategy with the following estimate for the loss vector:\nezt(i) = zt(i)at(i)\nxt(i)\n.\n(6)\nNote that this is a valid estimate since it makes only use of (zt(1)at(1), . . . , zt(d)at(d)). Moreover,\nit is unbiased with respect to the random draw of at from pt, since by de\ufb01nition, Eat\u223cptat(i) =\nxt(i). In other words, Eat\u223cptezt(i) = zt(i).\nTheorem 3 The regret of OSMD with F(x) = Pd\ni=1 xi log xi \u2212Pd\ni=1 xi (and D = (0, +\u221e)d) and\nany non-negative unbiased loss estimate ezt(i) \u22650 satis\ufb01es\nRn \u2264m log d\nm\n\u03b7\n+ \u03b7\n2\nn\nX\nt=1\nd\nX\ni=1\nxt(i)ezt(i)2.\nIn particular, with the estimate (6) and \u03b7 =\nq\n2 m log dm\nnd\n,\nRn \u2264\nr\n2mdn log d\nm.\n10\nProof One can easily see that for the negative entropy the dual space is D\u2217= Rd. Thus, (2) is\nveri\ufb01ed and OSMD is well de\ufb01ned. Moreover, again by straightforward computations, one can also\nsee that\nDF \u2217\n\u0012\n\u2207F(x), \u2207F(y)\n\u0013\n=\nd\nX\ni=1\ny(i) \u0398\n\u0012\n(\u2207F(x) \u2212\u2207F(y))(i)\n\u0013\n,\n(7)\nwhere \u0398(x) = exp(x) \u22121 \u2212x. Thus, using Theorem 2 and the facts that \u0398(x) \u2264x2\n2 for x \u22640\nand Pd\ni=1 xt(i) \u2264m, one obtains\nRn\n\u2264\nsupa\u2208A F(a) \u2212F(x1)\n\u03b7\n+ 1\n\u03b7\nn\nX\nt=1\nEDF \u2217\n\u0012\n\u2207F(xt) \u2212\u03b7ezt, \u2207F(xt)\n\u0013\n\u2264\nsupa\u2208A F(a) \u2212F(x1)\n\u03b7\n+ \u03b7\n2\nn\nX\nt=1\nd\nX\ni=1\nxt(i)ezt(i)2\nThe proof of the \ufb01rst inequality is concluded by noting that:\nF(a) \u2212F(x1) \u2264\nd\nX\ni=1\nx1(i) log\n1\nx1(i) \u2264m log\n \nd\nX\ni=1\nx1(i)\nm\n1\nx1(i)\n!\n= m log d\nm.\nThe second inequality follows from\nExt(i)ezt(i)2 \u2264Eat(i)\nxt(i) = 1.\nUsing the standard\n\u221a\ndn lower bound for the multi-armed bandit (which corresponds to the case\nwhere A is the canonical basis), see e.g., [Theorem 30, [3]], one can directly obtain a lower bound\nof order\n\u221a\nmdn for our setting. Thus the upper bound derived in Theorem 3 has an extraneous\nlogarithmic factor compared to the lower bound. This phenomenon already appeared in the basic\nmulti-armed bandit setting. In that case, the extra logarithmic factor was removed in Audibert and\nBubeck [2] by resorting to a new class of strategies for the expert problem, called INF (Implicitely\nNormalized Forecaster). Next we generalize this class of algorithms to the combinatorial setting,\nand thus remove the extra logarithmic factor. First we introduce the notion of a potential and the\nassociated Legendre function.\nDe\ufb01nition 2 Let \u03c9 \u22650. A function \u03c8 : (\u2212\u221e, a) \u2192R\u2217\n+ for some a \u2208R \u222a{+\u221e} is called an\n\u03c9-potential if it is convex, continuously differentiable, and satis\ufb01es\nlim\nx\u2192\u2212\u221e\u03c8(x) = \u03c9 ,\nlim\nx\u2192a \u03c8(x) = +\u221e,\n\u03c8\u2032 > 0 ,\nZ \u03c9+1\n\u03c9\n|\u03c8\u22121(s)|ds < +\u221e.\nFor every potential \u03c8 we associate the function F\u03c8 de\ufb01ned on D = (\u03c9, +\u221e)d by:\nF\u03c8(x) =\nd\nX\ni=1\nZ xi\n\u03c9\n\u03c8\u22121(s)ds.\n11\nIn this paper we restrict our attention to 0-potentials which we will simply call potentials. A\nnon-zero value of \u03c9 may be used to derive regret bounds that hold with high probability (instead\nof pseudo-regret bounds, see footnote 1).\nThe \ufb01rst order optimality condition for (4) implies that OSMD with F\u03c8 is a direct generalization\nof INF with potential \u03c8, in the sense that the two algorithms coincide when A is the canonical basis.\nNote, in particular, that with \u03c8(x) = exp(x) we recover the negative entropy for F\u03c8. In [3], the\nchoice of \u03c8(x) = (\u2212x)q with q > 1 was recommended. We show in Theorem 4 that here, again,\nthis choice gives a minimax optimal strategy.\nLemma 2 Let \u03c8 be a potential. Then F = F\u03c8 is Legendre and for all u, v \u2208D\u2217= (\u2212\u221e, a)d such\nthat ui \u2264vi, \u2200i \u2208{1, . . . , d},\nDF \u2217(u, v) \u22641\n2\nd\nX\ni=1\n\u03c8\u2032(vi)(ui \u2212vi)2.\nProof A direct examination shows that F = F\u03c8 is a Legendre function. Moreover, since \u2207F \u2217(u) =\n(\u2207F)\u22121(u) =\n\u0000\u03c8(u1), . . . , \u03c8(ud)\n\u0001\n, we obtain\nDF \u2217(u, v) =\nd\nX\ni=1\n\u0012 Z ui\nvi\n\u03c8(s)ds \u2212(ui \u2212vi)\u03c8(vi)\n\u0013\n.\nFrom a Taylor expansion, we get\nDF \u2217(u, v) \u2264\nd\nX\ni=1\nmax\ns\u2208[ui,vi]\n1\n2\u03c8\u2032(s)(ui \u2212vi)2.\nSince the function \u03c8 is convex, and ui \u2264vi, we have\nmax\ns\u2208[ui,vi] \u03c8\u2032(s) \u2264\u03c8\u2032\u0000max(ui, vi)\n\u0001\n\u2264\u03c8\u2032(vi),\nwhich gives the desired result.\nTheorem 4 Let \u03c8 be a potential. The regret of OSMD with F = F\u03c8 and any non-negative unbiased\nloss estimate ezt satis\ufb01es\nRn \u2264supa\u2208A F(a) \u2212F(x1)\n\u03b7\n+ \u03b7\n2\nn\nX\nt=1\nd\nX\ni=1\nE\nezt(i)2\n(\u03c8\u22121)\u2032(xt(i)).\nIn particular, with the estimate (6), \u03c8(x) = (\u2212x)\u2212q, q > 1,and \u03b7 =\nq\n2\nq\u22121\nm1\u22122/q\nd1\u22122/q\n1\nn,\nRn \u2264q\nr\n2\nq \u22121mdn .\nWith q = 2 this gives\nRn \u22642\n\u221a\n2mdn .\n12\nIn the case m = 1, the above theorem improves the bound Rn \u22648\n\u221a\nnd obtained in Theorem\n11 of [3].\nProof First note that since D\u2217= (\u2212\u221e, a)d and ezt has non-negative coordinates, OSMD is well\nde\ufb01ned (that is, (2) is satis\ufb01ed).\nThe \ufb01rst inequality follows from Theorem 2 and the fact that \u03c8\u2032(\u03c8\u22121(s)) =\n1\n(\u03c8\u22121)\u2032(s).\nLet \u03c8(x) = (\u2212x)\u2212q. Then \u03c8\u22121(x) = \u2212x\u22121/q and F(x) = \u2212\nq\nq\u22121\nPd\ni=1 x1\u22121/q\ni\n. In particular,\nnote that by H\u00a8older\u2019s inequality, since Pd\ni=1 x1(i) = m,\nF(a) \u2212F(x1) \u2264\nq\nq \u22121\nd\nX\ni=1\nx1(i)1\u22121/q \u2264\nq\nq \u22121m(q\u22121)/qd1/q.\nMoreover, note that (\u03c8\u22121)\u2032(x) = 1\nqx\u22121\u22121/q, and\nd\nX\ni=1\nE\nezt(i)2\n(\u03c8\u22121)\u2032(xt(i)) \u2264q\nd\nX\ni=1\nxt(i)1/q \u2264qm1/qd1\u22121/q,\nwhich concludes the proof.\n4\nBandit feedback.\nIn this section we consider online combinatorial optimization with bandit feedback. This setting is\nmuch more challenging than the semi-bandit case, and in order to obtain sublinear regret bounds all\nknown strategies add an exploration component to the algorithm. For example, in EXP2, instead\nof playing an action at random according to the exponentially weighted average distribution pt,\none draws a random action from pt with probability 1 \u2212\u03b3 and from some \ufb01xed \u201cexploration\u201d\ndistribution \u00b5 with probability \u03b3. On the other hand, in OSMD, one randomly perturbs xt to some\next, and then plays at random a point in A such that on average one plays ext.\nIn Bubeck, Cesa-Bianchi, and Kakade [9], the authors study the EXP2 strategy with the explo-\nration distribution \u00b5 supported on the contact points between the polytope Conv(A) and the John\nellipsoid of this polytope (i.e., the ellipsoid of minimal volume enclosing the polytope). Using this\nmethod they are able to prove the best known upper bound for online combinatorial optimization\nwith bandit feedback. They show that the regret of EXP2 mixed with John\u2019s exploration (and with\nthe estimate described in Figure 2) satis\ufb01es\nRn \u22642m3/2\nr\n3dn log ed\nm .\nOur next theorem shows that no strategy can achieve a regret less than a constant times m\n\u221a\ndn,\nleaving a gap of a factor of\nq\nm log d\nm. As we argue below, we conjecture that the lower bound is of\nthe correct order of magnitude. However, improving the upper bound seems to require some sub-\nstantially new ideas. Note that the following bound gives limitations that no strategy can surpass,\non the contrary to Theorem 1 which was dedicated to the EXP2 strategy.\n13\nTheorem 5 Let n \u2265d \u22652m. There exists a subset A \u2282{0, 1}d such that ||a||1 = m, \u2200a \u2208A,\nunder bandit feedback, one has\ninf\nstrategies\nsup\nadversaries\nRn \u22650.02m\n\u221a\ndn ,\n(8)\nwhere the in\ufb01mum and the supremum are taken over the class of strategies for the \u201cplayer\u201d and\nfor the \u201cadversary\u201d as de\ufb01ned in the introduction.\nNote that it should not come as a surprise that EXP2 (with John\u2019s exploration) is suboptimal,\nsince even in the full information case the basic EXP2 strategy was provably suboptimal, see Theo-\nrem 1. We conjecture that the correct order of magnitude for the minimax regret in the bandit case\nis m\n\u221a\ndn, as the above lower bound suggests.\nA promising approach to resolve this conjecture is to consider again the OSMD approach.\nHowever we believe that in the bandit case, one has to consider Legendre functions with non-\ndiagonal Hessian (on the contrary to the Legendre functions considered so far in this paper). Aber-\nnethy, Hazan, and Rakhlin [1] propose to use a self-concordant barrier function for the polytope\nConv(A). Then they randomly perturb the point xt given by OSMD using the eigenstructure of\nthe Hessian. This approach leads to a regret upper bound of order md\u221a\u03b8n log n for \u03b8 > 0 when\nConv(A) admits a \u03b8-self-concordant barrier function. Unfortunately, even when there exists a\nO(1)-self concordant barrier, this bound is still larger than the conjectured optimal bound by a\nfactor\n\u221a\nd. In fact, it was proved in [9] that in some cases there exist better choices for the Leg-\nendre function and the perturbation than those described in [1], even when there is a O(1)-self\nconcordant function for the action set. How to generalize this approach to the polytopes involved\nin online combinatorial optimization is a challenging open problem.\nA\nProof of Theorem 1.\nFor the sake of simplicity, we assume that d is a multiple of 4 and that n is even. We consider the\nfollowing subset of the hypercube:\nA =\n\u001a\na \u2208{0, 1}d :\nd/2\nX\ni=1\nai = d/4 and\n\u0012\nai = 1, \u2200i \u2208{d/2 + 1; . . . , d/2 + d/4}\n\u0013\nor\n\u0012\nai = 1, \u2200i \u2208{d/2 + d/4 + 1, . . . , d}\n\u0013\u001b\n.\nThat is, choosing a point in A corresponds to choosing a subset of d/4 elements among the \ufb01rst\nhalf of the coordinates, and choosing one of the two \ufb01rst disjoint intervals of size d/4 in the second\nhalf of the coordinates.\nWe prove that for any parameter \u03b7, there exists an adversary such that Exp2 (with parameter \u03b7)\nhas a regret of at least nd\n16 tanh\n\u0000 \u03b7d\n8\n\u0001\n, and that there exists another adversary such that its regret is at\nleast min\n\u0000d log 2\n12\u03b7 , nd\n12\n\u0001\n. As a consequence, we have\nsup Rn \u2265max\n\u0012nd\n16 tanh\n\u0010\u03b7d\n8\n\u0011\n, min\n\u0012d log 2\n12\u03b7 , nd\n12\n\u0013 \u0013\n\u2265min\n\u0012\nmax\n\u0012nd\n16 tanh\n\u0010\u03b7d\n8\n\u0011\n, d log 2\n12\u03b7\n\u0013\n, nd\n12\n\u0013\n\u2265min\n\u0010\nA, nd\n12\n\u0011\n,\n14\nwith\nA =\nmin\n\u03b7\u2208[0,+\u221e) max\n\u0012nd\n16 tanh\n\u0010\u03b7d\n8\n\u0011\n, d log 2\n12\u03b7\n\u0013\n\u2265min\n\u0012\nmin\n\u03b7d\u22658\nnd\n16 tanh\n\u0010\u03b7d\n8\n\u0011\n, min\n\u03b7d<8 max\n\u0012nd\n16 tanh\n\u0010\u03b7d\n8\n\u0011\n, d log 2\n12\u03b7\n\u0013 \u0013\n\u2265min\n\u0012nd\n16 tanh(1), min\n\u03b7d<8 max\n\u0012nd\n16\n\u03b7d\n8 tanh(1), d log 2\n12\u03b7\n\u0013 \u0013\n\u2265min\n \nnd\n16 tanh(1),\nr\nnd3 log 2 \u00b7 tanh(1)\n128 \u00b7 12\n!\n\u2265min\n\u00000.04 nd, 0.01 d3/2\u221an\n\u0001\n,\nwhere we used the fact that tanh is concave and increasing on R+. As n \u2265d, this implies the\nstated lower bound.\nFirst we prove the lower bound nd\n16 tanh\n\u0000 \u03b7d\n8\n\u0001\n. De\ufb01ne the following adversary:\nzt(i) =\n\uf8f1\n\uf8f2\n\uf8f3\n1\nif\ni \u2208{d/2 + 1; . . . , d/2 + d/4} and t odd,\n1\nif\ni \u2208{d/2 + d/4 + 1, . . . , d} and t even,\n0\notherwise.\nThis adversary always puts a zero loss on the \ufb01rst half of the coordinates, and alternates between\na loss of d/4 for choosing the \ufb01rst interval (in the second half of the coordinates) and the second\ninterval. At the beginning of odd rounds, any vertex a \u2208A has the same cumulative loss and\nthus Exp2 picks its expert uniformly at random, which yields an expected cumulative loss equal to\nnd/16. On the other hand, at even rounds the probability distribution to select the vertex a \u2208A is\nalways the same. More precisely, the probability of selecting a vertex which contains the interval\n{d/2 + d/4 + 1, . . . , d} (i.e, the interval with a d/4 loss at this round) is exactly\n1\n1+exp(\u2212\u03b7d/4). This\nadds an expected cumulative loss equal to nd\n8\n1\n1+exp(\u2212\u03b7d/4). Finally, note that the loss of any \ufb01xed\nvertex is nd/8. Thus, we obtain\nRn = nd\n16 + nd\n8\n1\n1 + exp(\u2212\u03b7d/4) \u2212nd\n8 = nd\n16 tanh\n\u0010\u03b7d\n8\n\u0011\n.\nIt remains to show a lower bound proportional to 1/\u03b7. To this end, we consider a different\nadversary de\ufb01ned by\nzt(i) =\n\uf8f1\n\uf8f2\n\uf8f3\n1 \u2212\u03b5\nif\ni \u2264d/4,\n1\nif\ni \u2208{d/4 + 1, . . . , d/2},\n0\notherwise,\nfor some \ufb01xed \u03b5 > 0.\nNote that against this adversary the choice of the interval (in the second half of the components)\ndoes not matter. Moreover, by symmetry, the weight of any coordinate in {d/4 + 1, . . . , d/2} is\nthe same (at any round). Finally, note that this weight is decreasing with t. Thus, we have the\nfollowing identities (in the big sums i represents the number of components selected in the \ufb01rst\n15\nd/4 components):\nRn = n\u03b5d\n4\nP\na\u2208A:ad/2=1 exp(\u2212\u03b7nzT\n1 a)\nP\na\u2208A exp(\u2212\u03b7nzT\n1 a)\n= n\u03b5d\n4\nPd/4\u22121\ni=0\n\u0000d/4\ni\n\u0001\u0000 d/4\u22121\nd/4\u2212i\u22121\n\u0001\nexp(\u2212\u03b7(nd/4 \u2212in\u03b5))\nPd/4\ni=0\n\u0000d/4\ni\n\u0001\u0000 d/4\nd/4\u2212i\n\u0001\nexp(\u2212\u03b7(nd/4 \u2212in\u03b5))\n= n\u03b5d\n4\nPd/4\u22121\ni=0\n\u0000d/4\ni\n\u0001\u0000 d/4\u22121\nd/4\u2212i\u22121\n\u0001\nexp(\u03b7in\u03b5)\nPd/4\ni=0\n\u0000d/4\ni\n\u0001\u0000 d/4\nd/4\u2212i\n\u0001\nexp(\u03b7in\u03b5)\n= n\u03b5d\n4\nPd/4\u22121\ni=0\n\u00001 \u22124i\nd\n\u0001\u0000d/4\ni\n\u0001\u0000 d/4\nd/4\u2212i\n\u0001\nexp(\u03b7in\u03b5)\nPd/4\ni=0\n\u0000d/4\ni\n\u0001\u0000 d/4\nd/4\u2212i\n\u0001\nexp(\u03b7in\u03b5)\nwhere we used\n\u0000 d/4\u22121\nd/4\u2212i\u22121\n\u0001\n=\n\u00001 \u22124i\nd\n\u0001\u0000 d/4\nd/4\u2212i\n\u0001\nin the last equality. Thus, taking \u03b5 = min\n\u0000 log 2\n\u03b7n , 1\n\u0001\nyields\nRn \u2265min\n\u0012d log 2\n4\u03b7\n, nd\n4\n\u0013 Pd/4\u22121\ni=0\n\u00001 \u22124i\nd\n\u0001\u0000d/4\ni\n\u00012 min(2, exp(\u03b7n))i\nPd/4\ni=0\n\u0000d/4\ni\n\u00012 min(2, exp(\u03b7n))i\n\u2265min\n\u0012d log 2\n12\u03b7 , nd\n12\n\u0013\n,\nwhere the last inequality follows from Lemma 3 in the appendix. This concludes the proof of the\nlower bound.\nB\nProof of Theorem 5\nThe structure of the proof is similar to that of [3, Theorem 30], which deals with the simple case\nwhere m = 1. The main important conceptual difference is contained in Lemma 4, which is at the\nheart of this new proof. The main argument follows the line of standard lower bounds for bandit\nproblems, see, e.g., [10]: The worst-case regret is bounded from below by by taking an average\nover a conveniently chosen class of strategies of the adversary. Then, by Pinsker\u2019s inequality, the\nproblem is reduced to computing the Kullback-Leibler divergence of certain distributions. The\nmain technical argument, given in Lemma 4, is for proving manageable bounds for the relevant\nKullback-Leibler divergence.\nFor the sake of simplifying notation, we assume that d is a multiple of m, and we identify\n{0, 1}d with the set of m \u00d7 (d/m) binary matrices {0, 1}m\u00d7 d\nm. We consider the following set of\nactions:\nA = {a \u2208{0, 1}m\u00d7 d\nm : \u2200i \u2208{1, . . . , m},\nd/m\nX\nj=1\na(i, j) = 1}.\nIn other words, the player is playing in parallel m \ufb01nite games with d/m actions.\nFrom step 1 to 3 we restrict our attention to the case of deterministic strategies for the player,\nand we show how to extend the results to arbitrary strategies in step 4.\nFirst step: de\ufb01nitions.\n16\nWe denote by Ii,t \u2208{1, . . . , m} the random variable such that at(i, Ii,t) = 1. That is, Ii,t is\nthe action chosen at time t in the ith game. Moreover, let \u03c4 be drawn uniformly at random from\n{1, . . . , n}.\nIn this proof we consider random adversaries indexed by A. More precisely, for \u03b1 \u2208A, we\nde\ufb01ne the \u03b1-adversary as follows: For any t \u2208{1, . . . , n}, zt(i, j) is drawn from a Bernoulli\ndistribution with parameter 1\n2 \u2212\u03b5\u03b1(i, j). In other words, against adversary \u03b1, in the ith game, the\naction j such that \u03b1(i, j) = 1 has a loss slightly smaller (in expectation) than the other actions. We\ndenote by E\u03b1 integration with respect to the loss generation process of the \u03b1-adversary. We write\nPi,\u03b1 for the probability distribution of \u03b1(i, Ii,\u03c4) when the player faces the \u03b1-adversary. Note that\nwe have Pi,\u03b1(1) = E\u03b1\n1\nn\nPn\nt=1\n1\u03b1(i,Ii,t)=1, hence, against the \u03b1-adversary, we have\nRn = E\u03b1\nn\nX\nt=1\nm\nX\ni=1\n\u03b5 1\u03b1(i,Ii,t)\u0338=1 = n\u03b5\nm\nX\ni=1\n(1 \u2212Pi,\u03b1(1)) ,\nwhich implies (since the maximum is larger than the mean)\nmax\n\u03b1\u2208A Rn \u2265n\u03b5\nm\nX\ni=1\n \n1 \u2212\n1\n(d/m)m\nX\n\u03b1\u2208A\nPi,\u03b1(1)\n!\n.\n(9)\nSecond step: information inequality.\nLet P\u2212i,\u03b1 be the probability distribution of \u03b1(i, Ii,\u03c4) against the adversary which plays like the\n\u03b1-adversary except that in the ith game, the losses of all coordinates are drawn from a Bernoulli\ndistribution of parameter 1/2. We call it the (\u2212i, \u03b1)-adversary and we denote by E(\u2212i,\u03b1) integration\nwith respect to its loss generation process. By Pinsker\u2019s inequality,\nPi,\u03b1(1) \u2264P\u2212i,\u03b1(1) +\nr\n1\n2KL(P\u2212i,\u03b1, Pi,\u03b1) ,\nwhere KL denotes the Kullback-Leibler divergence. Moreover, note that by symmetry of the\nadversaries (\u2212i, \u03b1),\n1\n(d/m)m\nX\n\u03b1\u2208A\nP\u2212i,\u03b1(1)\n=\n1\n(d/m)m\nX\n\u03b1\u2208A\nE(\u2212i,\u03b1)\u03b1(i, Ii,\u03c4)\n=\n1\n(d/m)m\nX\n\u03b2\u2208A\n1\nd/m\nX\n\u03b1:(\u2212i,\u03b1)=(\u2212i,\u03b2)\nE(\u2212i,\u03b1)\u03b1(i, Ii,\u03c4)\n=\n1\n(d/m)m\nX\n\u03b2\u2208A\n1\nd/mE(\u2212i,\u03b2)\nX\n\u03b1:(\u2212i,\u03b1)=(\u2212i,\u03b2)\n\u03b1(i, Ii,\u03c4)\n=\n1\n(d/m)m\nX\n\u03b2\u2208A\n1\nd/m\n=\nm\nd ,\n(10)\nand thus, thanks to the concavity of the square root,\n1\n(d/m)m\nX\n\u03b1\u2208A\nPi,\u03b1(1) \u2264m\nd +\ns\n1\n2(d/m)m\nX\n\u03b1\u2208A\nKL(P\u2212i,\u03b1, Pi,\u03b1).\n(11)\n17\nThird step: computation of KL(P\u2212i,\u03b1, Pi,\u03b1) with the chain rule.\nNote that since the forecaster is deterministic, the sequence of observed losses (up to time\nn) Wn \u2208{0, . . . , m}n uniquely determines the empirical distribution of plays, and, in particular,\nthe probability distribution of \u03b1(i, Ii,\u03c4) conditionally to Wn is the same for any adversary. Thus,\nif we denote by Pn\n\u03b1 (respectively Pn\n\u2212i,\u03b1) the probability distribution of Wn when the forecaster\nplays against the \u03b1-adversary (respectively the (\u2212i, \u03b1)-adversary), then one can easily prove that\nKL(P\u2212i,\u03b1, Pi,\u03b1) \u2264KL(Pn\n\u2212i,\u03b1, Pn\n\u03b1). Now we use the chain rule for Kullback-Leibler divergence\niteratively to introduce the probability distributions Pt\n\u03b1 of the observed losses Wt up to time t.\nMore precisely, we have,\nKL(Pn\n\u2212i,\u03b1, Pn\n\u03b1)\n= KL(P1\n\u2212i,\u03b1, P1\n\u03b1) +\nn\nX\nt=2\nX\nwt\u22121\u2208{0,...,m}t\u22121\nPt\u22121\n\u2212i,\u03b1(wt\u22121)KL(Pt\n\u2212i,\u03b1(.|wt\u22121), Pt\n\u03b1(.|wt\u22121))\n= KL (B\u2205, B\u2032\n\u2205)\n1\u03b1(i,Ii,1)=1 +\nn\nX\nt=2\nX\nwt\u22121:\u03b1(i,Ii,1)=1\nPt\u22121\n\u2212i,\u03b1(wt\u22121)KL\n\u0000Bwt\u22121, B\u2032\nwt\u22121\n\u0001\n,\nwhere Bwt\u22121 and B\u2032\nwt\u22121 are sums of m Bernoulli distributions with parameters in {1/2, 1/2 \u2212\u03b5}\nand such that the number of Bernoullis with parameter 1/2 in Bwt\u22121 is equal to the number of\nBernoullis with parameter 1/2 in B\u2032\nwt\u22121 plus one. Now using Lemma 4 (see below) we obtain,\nKL\n\u0000Bwt\u22121, B\u2032\nwt\u22121\n\u0001\n\u2264\n8 \u03b52\n(1 \u22124\u03b52)m.\nIn particular, this gives\nKL(Pn\n\u2212i,\u03b1, Pn\n\u03b1) \u2264\n8 \u03b52\n(1 \u22124\u03b52)mE\u2212i,\u03b1\nn\nX\nt=1\n1\u03b1(i,Ii,t)=1 =\n8 \u03b52n\n(1 \u22124\u03b52)mP\u2212i,\u03b1(1).\nSumming and plugging this into (11) we obtain (again thanks to (10)), for \u03b5 \u2264\n1\n\u221a\n8,\n1\n(d/m)m\nX\n\u03b1\u2208A\nPi,\u03b1(1) \u2264m\nd + \u03b5\nr\n8n\nd .\nTo conclude the proof of (8) for deterministic players one needs to plug this last equation in (9)\nalong with straightforward computations.\nFourth step: Fubini\u2019s theorem to handle non-deterministic players.\nConsider now a randomized player, and let Erand denote the expectation with respect to the\nrandomization of the player. Then one has (thanks to Fubini\u2019s theorem),\n1\n(d/m)m\nX\n\u03b1\u2208A\nE\nn\nX\nt=1\n(aT\nt zt \u2212\u03b1Tz) = Erand\n1\n(d/m)m\nX\n\u03b1\u2208A\nE\u03b1\nn\nX\nt=1\n(aT\nt zt \u2212\u03b1Tz).\nNow note that if we \ufb01x the realization of the forecaster\u2019s randomization then the results of the\nprevious steps apply and, in particular, one can lower bound\n1\n(d/m)m\nP\n\u03b1\u2208A E\u03b1\nPn\nt=1(aT\nt zt \u2212\u03b1Tz)\nas before (note that \u03b1 is the optimal action in expectation against the \u03b1-adversary).\n18\nC\nTechnical lemmas.\nLemma 3 For any k \u2208N\u2217, for any 1 \u2264c \u22642, we have\nPk\ni=0(1 \u2212i/k)\n\u0000k\ni\n\u00012ci\nPk\ni=0\n\u0000k\ni\n\u00012ci\n\u22651/3.\nProof Let f(c) denote the expression on the left-hand side of the inequality. Introduce the random\nvariable X, which is equal to i \u2208{0, . . . , k} with probability\n\u0000k\ni\n\u00012ci\u000e Pk\nj=0\n\u0000k\nj\n\u00012cj. We have\nf \u2032(c) = 1\ncE[X(1 \u2212X/k)] \u22121\ncE(X)E(1 \u2212X/k) = \u22121\nckVar X \u22640. So the function f is decreasing\non [1, 2], and therefore it suf\ufb01ces to consider c = 2. Numerator and denominator of the left-hand\nside differ only by the factor 1 \u2212i/k. A lower bound for the left-hand side can thus be obtained by\nshowing that the terms for i close to k are not essential to the value of the denominator. To prove\nthis, we may use Stirling\u2019s formula which implies that for any k \u22652 and i \u2208[1, k \u22121],\n\u0010k\ni\n\u0011i\u0010\nk\nk \u2212i\n\u0011k\u2212i\n\u221a\nk\np\n2\u03c0i(k \u2212i)\ne\u22121/6 <\n\u0012k\ni\n\u0013\n<\n\u0010k\ni\n\u0011i\u0010\nk\nk \u2212i\n\u0011k\u2212i\n\u221a\nk\np\n2\u03c0i(k \u2212i)\ne1/12,\nhence\n\u0010k\ni\n\u00112i\u0010\nk\nk \u2212i\n\u00112(k\u2212i)\nke\u22121/3\n2\u03c0i(k \u2212i) <\n\u0012k\ni\n\u00132\n<\n\u0010k\ni\n\u00112i\u0010\nk\nk \u2212i\n\u00112(k\u2212i)ke1/6\n2\u03c0i .\nIntroduce \u03bb = i/k and \u03c7(\u03bb) =\n2\u03bb\n\u03bb2\u03bb(1\u2212\u03bb)2(1\u2212\u03bb) . We have\n[\u03c7(\u03bb)]k 2e\u22121/3\n\u03c0k\n<\n\u0012k\ni\n\u00132\n2i < [\u03c7(\u03bb)]k e1/6\n2\u03c0\u03bb.\n(12)\nLemma 3 can be numerically veri\ufb01ed for k \u2264106.\nWe now consider k > 106.\nFor \u03bb \u2265\n0.666, since the function \u03c7 can be shown to be decreasing on [0.666, 1], the inequality\n\u0000k\ni\n\u000122i <\n[\u03c7(0.666)]k\ne1/6\n2\u00d70.666\u00d7\u03c0 holds. We have \u03c7(0.657)/\u03c7(0.666) > 1.0002. Consequently, for k > 106,\nwe have [\u03c7(0.666)]k < 0.001 \u00d7 [\u03c7(0.657)]k/k2. So for \u03bb \u22650.666 and k > 106, we have\n\u0012k\ni\n\u00132\n2i < 0.001 \u00d7 [\u03c7(0.657)]k\ne1/6\n2\u03c0 \u00d7 0.666 \u00d7 k2 < [\u03c7(0.657)]k 2e\u22121/3\n1000\u03c0k2\n=\nmin\n\u03bb\u2208[0.656,0.657][\u03c7(\u03bb)]k 2e\u22121/3\n1000\u03c0k2\n<\n1\n1000k\nmax\ni\u2208{1,...,k\u22121}\u2229[0,0.666k)\n\u0012k\ni\n\u00132\n2i ,\n(13)\nwhere the last inequality comes from (12) and the fact that there exists i \u2208{1, . . . , k \u22121} such that\ni/k \u2208[0.656, 0.657]. Inequality (13) implies that for any i \u2208{1, . . . , k}, we have\nX\n0.666k\u2264i\u2264k\n\u0012k\ni\n\u00132\n2i <\n1\n1000\nmax\ni\u2208{1,...,k\u22121}\u2229[0,0.666k)\n\u0012k\ni\n\u00132\n2i <\n1\n1000\nX\n0\u2264i<0.666k\n\u0012k\ni\n\u00132\n2i.\n19\nTo conclude, introducing A = P\n0\u2264i<0.666k\n\u0000k\ni\n\u000122i, we have\nPk\ni=0(1 \u2212i/k)\n\u0000k\ni\n\u000122i\nPk\ni=0\n\u0000k\ni\n\u0001\u0000 k\nk\u2212i\n\u0001\n2i\n> (1 \u22120.666)A\nA + 0.001A \u22651\n3.\nLemma 4 Let \u2113and n be integers with 1\n2 \u2264n\n2 \u2264\u2113\u2264n. Let p, p\u2032, q, p1, . . . , pn be real numbers in\n(0, 1) with q \u2208{p, p\u2032}, p1 = \u00b7 \u00b7 \u00b7 = p\u2113= q and p\u2113+1 = \u00b7 \u00b7 \u00b7 = pn. Let B (resp. B\u2032) be the sum of\nn + 1 independent Bernoulli distributions with parameters p, p1, . . . , pn (resp. p\u2032, p1, . . . , pn). We\nhave\nKL(B, B\u2032) \u2264\n2(p\u2032 \u2212p)2\n(1 \u2212p\u2032)(n + 2)q.\nProof Let Z, Z\u2032, Z1, . . . , Zn be independent Bernoulli distributions with parameters p, p\u2032, p1, . . . , pn.\nDe\ufb01ne S = P\u2113\ni=1 Zi, T = Pn\ni=\u2113+1 Zi and V = Z + S. By a slight and usual abuse of notation, we\nuse KL to denote Kullback-Leibler divergence of both probability distributions and random vari-\nables. Then we may write (the inequality is an easy consequence of the chain rule for Kullback-\nLeibler divergence)\nKL(B, B\u2032) = KL\n\u0000(Z + S) + T, (Z\u2032 + S) + T\n\u0001\n\u2264KL\n\u0000(Z + S, T), (Z\u2032 + S, T)\n\u0001\n= KL\n\u0000Z + S, Z\u2032 + S\n\u0001\n.\nLet sk = P(S = k) for k = \u22121, 0, . . . , \u2113+ 1. Using the equalities\nsk =\n\u0012\u2113\nk\n\u0013\nqk(1 \u2212q)\u2113\u2212k =\nq\n1 \u2212q\n\u2113\u2212k + 1\nk\n\u0012\n\u2113\nk \u22121\n\u0013\nqk\u22121(1 \u2212q)\u2113\u2212k+1 =\nq\n1 \u2212q\n\u2113\u2212k + 1\nk\nsk\u22121,\nwhich holds for 1 \u2264k \u2264\u2113+ 1, we obtain\nKL(Z + S, Z\u2032 + S) =\n\u2113+1\nX\nk=0\nP(V = k) log\n\u0012 P(Z + S = k)\nP(Z\u2032 + S = k)\n\u0013\n=\n\u2113+1\nX\nk=0\nP(V = k) log\n\u0012 psk\u22121 + (1 \u2212p)sk\np\u2032sk\u22121 + (1 \u2212p\u2032)sk\n\u0013\n=\n\u2113+1\nX\nk=0\nP(V = k) log\n\u0012 p 1\u2212q\nq k + (1 \u2212p)(\u2113\u2212k + 1)\np\u2032 1\u2212q\nq k + (1 \u2212p\u2032)(\u2113\u2212k + 1)\n\u0013\n= E log\n\u0012 (p \u2212q)V + (1 \u2212p)q(\u2113+ 1)\n(p\u2032 \u2212q)V + (1 \u2212p\u2032)q(\u2113+ 1)\n\u0013\n.\n(14)\n20\nFirst case: q = p\u2032.\nBy Jensen\u2019s inequality, using that EV = p\u2032(\u2113+ 1) + p \u2212p\u2032 in this case, we get\nKL(Z + S, Z\u2032 + S) \u2264log\n\u0012(p \u2212p\u2032)E(V ) + (1 \u2212p)p\u2032(\u2113+ 1)\n(1 \u2212p\u2032)p\u2032(\u2113+ 1)\n\u0013\n= log\n\u0012(p \u2212p\u2032)2 + (1 \u2212p\u2032)p\u2032(\u2113+ 1)\n(1 \u2212p\u2032)p\u2032(\u2113+ 1)\n\u0013\n= log\n\u0012\n1 +\n(p \u2212p\u2032)2\n(1 \u2212p\u2032)p\u2032(\u2113+ 1)\n\u0013\n\u2264\n(p \u2212p\u2032)2\n(1 \u2212p\u2032)p\u2032(\u2113+ 1) .\nSecond case: q = p.\nIn this case, V is a binomial distribution with parameters \u2113+ 1 and p. From (14), we have\nKL(Z + S, Z\u2032 + S) \u2264\u2212E log\n\u0012(p\u2032 \u2212p)V + (1 \u2212p\u2032)p(\u2113+ 1)\n(1 \u2212p)p(\u2113+ 1)\n\u0013\n\u2264\u2212E log\n\u0012\n1 + (p\u2032 \u2212p)(V \u2212EV )\n(1 \u2212p)p(\u2113+ 1)\n\u0013\n.\n(15)\nTo conclude, we will use the following lemma.\nLemma 5 The following inequality holds for any x \u2265x0 with x0 \u2208(0, 1):\n\u2212log(x) \u2264\u2212(x \u22121) + (x \u22121)2\n2x0\n.\nProof Introduce f(x) = \u2212(x \u22121) + (x\u22121)2\n2x0\n+ log(x). We have f \u2032(x) = \u22121 + x\u22121\nx0 + 1\nx, and\nf \u2032\u2032(x) =\n1\nx0 \u22121\nx2. From f \u2032(x0) = 0, we get that f \u2032 is negative on (x0, 1) and positive on (1, +\u221e).\nThis leads to f nonnegative on [x0, +\u221e).\nFinally, from Lemma 5 and (15), using x0 = 1\u2212p\u2032\n1\u2212p , we obtain\nKL(Z + S, Z\u2032 + S) \u2264\n\u0012\np\u2032 \u2212p\n(1 \u2212p)p(\u2113+ 1)\n\u00132E[(V \u2212EV )2]\n2x0\n=\n\u0012\np\u2032 \u2212p\n(1 \u2212p)p(\u2113+ 1)\n\u00132(\u2113+ 1)p(1 \u2212p)2\n2(1 \u2212p\u2032)\n=\n(p\u2032 \u2212p)2\n2(1 \u2212p\u2032)(\u2113+ 1)p.\nAcknowledgements\nG. Lugosi is supported by the Spanish Ministry of Science and Technology grant MTM2009-09063\nand PASCAL2 Network of Excellence under EC grant no. 216886.\n21\nReferences\n[1] J. Abernethy, E. Hazan, and A. Rakhlin, Competing in the dark: An ef\ufb01cient algorithm for\nbandit linear optimization, Proceedings of the 21st Annual Conference on Learning Theory\n(COLT), 2008, pp. 263\u2013274.\n[2] J.-Y. Audibert and S. Bubeck, Minimax policies for adversarial and stochastic bandits, Pro-\nceedings of the 22nd Annual Conference on Learning Theory (COLT), 2009.\n[3]\n, Regret bounds and minimax policies under partial monitoring, Journal of Machine\nLearning Research 11 (2010), 2635\u20132686.\n[4] P. Auer, N. Cesa-Bianchi, Y. Freund, and R. Schapire, The non-stochastic multi-armed bandit\nproblem, SIAM Journal on Computing 32 (2003), no. 1, 48\u201377.\n[5] B. Awerbuch and R. Kleinberg, Adaptive routing with end-to-end feedback: distributed learn-\ning and geometric approaches, STOC \u201904: Proceedings of the thirty-sixth annual ACM sym-\nposium on Theory of computing, 2004, pp. 45\u201353.\n[6] A. Beck and M. Teboulle, Mirror descent and nonlinear projected subgradient methods for\nconvex optimization, Operations Research Letters 31 (2003), no. 3, 167\u2013175.\n[7] S. Bubeck, Introduction to online optimization, Lecture Notes, 2011.\n[8] S. Bubeck and N. Cesa-Bianchi, Regret analysis of stochastic and nonstochastic multi-armed\nbandit problems, Foundations and Trends in Machine Learning 5 (2012), no. 1, 1\u2013122.\n[9] S. Bubeck, N. Cesa-Bianchi, and S. M. Kakade, Towards minimax policies for online linear\noptimization with bandit feedback, Arxiv preprint arXiv:1202.3079 (2012).\n[10] N. Cesa-Bianchi and G. Lugosi, Prediction, learning, and games, Cambridge University\nPress, 2006.\n[11]\n, Combinatorial bandits, Journal of Computer and System Sciences (2011), To ap-\npear.\n[12] V. Dani, T. Hayes, and S. Kakade, The price of bandit information for online optimization,\nAdvances in Neural Information Processing Systems (NIPS), vol. 20, 2008, pp. 345\u2013352.\n[13] Y. Freund and R. E. Schapire, A decision-theoretic generalization of on-line learning and an\napplication to boosting, Journal of Computer and System Sciences 55 (1997), 119\u2013139.\n[14] C. Gentile and M. Warmuth, Linear hinge loss and average margin, Advances in Neural\nInformation Processing Systems (NIPS), 1998.\n[15] A. Grove, N. Littlestone, and D. Schuurmans, General convergence results for linear dis-\ncriminant updates, Machine Learning 43 (2001), 173\u2013210.\n[16] A. Gy\u00a8orgy, T. Linder, G. Lugosi, and G. Ottucs\u00b4ak, The on-line shortest path problem under\npartial monitoring, Journal of Machine Learning Research 8 (2007), 2369\u20132403.\n22\n[17] E. Hazan, The convex optimization approach to regret minimization, Optimization for Ma-\nchine Learning (S. Sra, S. Nowozin, and S. Wright, eds.), MIT press, 2011, pp. 287\u2013303.\n[18] E. Hazan, S. Kale, and M. Warmuth, Learning rotations with little regret, Proceedings of the\n23rd Annual Conference on Learning Theory (COLT), 2010.\n[19] D. P. Helmbold and M. Warmuth, Learning permutations with exponential weights, Journal\nof Machine Learning Research 10 (2009), 1705\u20131736.\n[20] M. Herbster and M. Warmuth, Tracking the best expert, Machine Learning 32 (1998), 151\u2013\n178.\n[21] J.-B. Hiriart-Urruty and C. Lemar\u00b4echal, Fundamentals of convex analysis, Springer, 2001.\n[22] A. Kalai and S. Vempala, Ef\ufb01cient algorithms for online decision problems, Journal of Com-\nputer and System Sciences 71 (2005), 291\u2013307.\n[23] S. Kale, L. Reyzin, and R. Schapire, Non-stochastic bandit slate problems, Advances in\nNeural Information Processing Systems (NIPS), 2010, pp. 1054\u20131062.\n[24] J. Kiefer and J. Wolfowitz, Stochastic estimation of the maximum of a regression function,\nAnnals of Mathematical Statistics 23 (1952), 462\u2013466.\n[25] J. Kivinen and M. Warmuth, Relative loss bounds for multidimensional regression problems,\nMachine Learning 45 (2001), 301\u2013329.\n[26] W. Koolen, M. Warmuth, and J. Kivinen, Hedging structured concepts, Proceedings of the\n23rd Annual Conference on Learning Theory (COLT), 2010, pp. 93\u2013105.\n[27] H. McMahan and A. Blum, Online geometric optimization in the bandit setting against an\nadaptive adversary, In Proceedings of the 17th Annual Conference on Learning Theory\n(COLT), 2004, pp. 109\u2013123.\n[28] A. Nemirovski, Ef\ufb01cient methods for large-scale convex optimization problems, Ekonomika\ni Matematicheskie Metody 15 (1979), (In Russian).\n[29] A. Nemirovski and D. Yudin, Problem complexity and method ef\ufb01ciency in optimization,\nWiley Interscience, 1983.\n[30] A. Rakhlin, Lecture notes on online learning, 2009.\n[31] H. Robbins and S. Monro, A stochastic approximation method, Annals of Mathematical\nStatistics 22 (1951), 400\u2013407.\n[32] A. Schrijver, Combinatorial optimization, Springer, 2003.\n[33] S. Shalev-Shwartz, Online learning: Theory, algorithms, and applications, Ph.D. thesis, The\nHebrew University of Jerusalem, 2007.\n[34] E. Takimoto and M. Warmuth, Paths kernels and multiplicative updates, Journal of Machine\nLearning Research 4 (2003), 773\u2013818.\n23\n[35] T. Uchiya, A. Nakamura, and M. Kudo, Algorithms for adversarial bandit problems with\nmultiple plays, Proceedings of the 21st International Conference on Algorithmic Learning\nTheory (ALT), 2010.\n[36] M. Warmuth and D. Kuzmin, Randomized online pca algorithms with regret bounds that are\nlogarithmic in the dimension, Journal of Machine Learning Research 9 (2008), 2287\u20132320.\n[37] M. Zinkevich, Online convex programming and generalized in\ufb01nitesimal gradient ascent,\nProceedings of the Twentieth International Conference on Machine Learning (ICML), 2003.\n24\n",
        "sentence": " When no contextual information is present, this setting is referred to as semi-bandits [Audibert et al., 2014] or slate bandits [Kale et al.",
        "context": "called \u201csemi-bandit\u201d and \u201cbandit\u201d problems. In the full information case we show that the\nstandard exponentially weighted average forecaster is a provably suboptimal strategy. For the\n(2)\nIn the full information setting, algorithms of this type were studied by Abernethy, Hazan, and\nRakhlin [1], Rakhlin [30], and Hazan [17]. In these papers the authors adopted the presenta-\nm) is still an open problem.\nWe also refer the interested reader to the recent survey [8] for an overview of bandit problems in\nvarious other settings.\n1.3\nContribution and contents of the paper."
    },
    {
        "title": "The nonstochastic multiarmed bandit problem",
        "author": [
            "Peter Auer",
            "Nicol\u00f2 Cesa-Bianchi",
            "Yoav Freund",
            "Robert E. Schapire"
        ],
        "venue": "SIAM Journal on Computing,",
        "citeRegEx": "Auer et al\\.,? \\Q2002\\E",
        "shortCiteRegEx": "Auer et al\\.",
        "year": 2002,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "Contextual bandit algorithms with supervised learning guarantees",
        "author": [
            "Alina Beygelzimer",
            "John Langford",
            "Lihong Li",
            "Lev Reyzin",
            "Robert E Schapire"
        ],
        "venue": "In Artificial Intelligence and Statistics,",
        "citeRegEx": "Beygelzimer et al\\.,? \\Q2011\\E",
        "shortCiteRegEx": "Beygelzimer et al\\.",
        "year": 2011,
        "abstract": "We address the problem of learning in an online, bandit setting where the\nlearner must repeatedly select among $K$ actions, but only receives partial\nfeedback based on its choices. We establish two new facts: First, using a new\nalgorithm called Exp4.P, we show that it is possible to compete with the best\nin a set of $N$ experts with probability $1-\\delta$ while incurring regret at\nmost $O(\\sqrt{KT\\ln(N/\\delta)})$ over $T$ time steps. The new algorithm is\ntested empirically in a large-scale, real-world dataset. Second, we give a new\nalgorithm called VE that competes with a possibly infinite set of policies of\nVC-dimension $d$ while incurring regret at most $O(\\sqrt{T(d\\ln(T) + \\ln\n(1/\\delta))})$ with probability $1-\\delta$. These guarantees improve on those\nof all previous algorithms, whether in a stochastic or adversarial environment,\nand bring us closer to providing supervised learning type guarantees for the\ncontextual bandit setting.",
        "full_text": "arXiv:1002.4058v3  [cs.LG]  27 Oct 2011\nContextual Bandit Algorithms with Supervised Learning Guarantees\nAlina Beygelzimer\nJohn Langford\nLihong Li\nIBM Research\nHawthorne, NY\nbeygel@us.ibm.com\nYahoo! Research\nNew York, NY\njl@yahoo-inc.com\nYahoo! Research\nSanta Clara, CA\nlihong@yahoo-inc.com\nLev Reyzin\nRobert E. Schapire\nGeorgia Institute of Technology\nAtlanta, GA\nlreyzin@cc.gatech.edu\nPrinceton University\nPrinceton, NJ\nschapire@cs.princeton.edu\nAbstract\nWe address the problem of competing with\nany large set of N policies in the non-\nstochastic bandit setting, where the learner\nmust repeatedly select among K actions but\nobserves only the reward of the chosen action.\nWe present a modi\ufb01cation of the Exp4 algo-\nrithm of Auer et al. [2], called Exp4.P, which\nwith high probability incurs regret at most\nO(\n\u221a\nKT ln N). Such a bound does not hold\nfor Exp4 due to the large variance of the\nimportance-weighted estimates used in the\nalgorithm. The new algorithm is tested em-\npirically in a large-scale, real-world dataset.\nFor the stochastic version of the problem, we\ncan use Exp4.P as a subroutine to compete\nwith a possibly in\ufb01nite set of policies of VC-\ndimension d while incurring regret at most\nO(\n\u221a\nT d ln T) with high probability.\nThese guarantees improve on those of all pre-\nvious algorithms, whether in a stochastic or\nadversarial environment, and bring us closer\nto providing guarantees for this setting that\nare comparable to those in standard super-\nvised learning.\nAlso appeared in Proceedings of the 14th International Con-\nference on Arti\ufb01cial Intelligence and Statistics (AISTATS)\n2011, Fort Lauderdale, FL, USA.\nVolume 15 of JMLR:\nW&CP 15. Copyright 2011 by the authors.\n1\nINTRODUCTION\nA learning algorithm is often faced with the problem of\nacting given feedback only about the actions that it has\ntaken in the past, requiring the algorithm to explore.\nA canonical example is the problem of personalized\ncontent recommendation on web portals, where the\ngoal is to learn which items are of greatest interest\nto a user, given such observable context as the user\u2019s\nsearch queries or geolocation.\nFormally, we consider an online bandit setting where at\nevery step, the learner observes some contextual infor-\nmation and must choose one of K actions, each with\na potentially di\ufb00erent reward on every round. After\nthe decision is made, the reward of the chosen action\nis revealed.\nThe learner has access to a class of N\npolicies, each of which also maps context to actions;\nthe learner\u2019s performance is measured in terms of its\nregret to this class, de\ufb01ned as the di\ufb00erence between\nthe cumulative reward of the best policy in the class\nand the learner\u2019s reward.\nThis setting goes under di\ufb00erent names, including the\n\u201cpartial-label problem\u201d [11], the \u201cassociative bandit\nproblem\u201d [18], the \u201ccontextual bandit problem\u201d [13]\n(which is the name we use here), the \u201ck-armed (or\nmulti-armed) bandit problem with expert advice\u201d [2],\nand \u201cassociative reinforcement learning\u201d [9]. Policies\nare sometimes referred to as hypotheses or experts,\nand actions are referred to as arms.\nIf the total number of steps T (usually much larger\nthan K) is known in advance, and the contexts and\nrewards are sampled independently from a \ufb01xed but\nunknown joint distribution, a simple solution is to\n\ufb01rst choose actions uniformly at random for O(T 2/3)\nrounds, and from that point on use the policy that per-\nContextual Bandit Algorithms with Supervised Learning Guarantees\nformed best on these rounds. This approach, a variant\nof \u01eb-greedy (see [19]), sometimes called \u01eb-\ufb01rst, can be\nshown to have a regret bound of O\n\u0000T 2/3(K ln N)1/3\u0001\nwith high probability [13].\nIn the full-label setting,\nwhere the entire reward vector is revealed to the\nlearner at the end of each step, the standard ma-\nchinery of supervised learning gives a regret bound of\nO(\n\u221a\nT ln N) with high probability, using the algorithm\nthat predicts according to the policy with the currently\nlowest empirical error rate.\nThis paper presents the \ufb01rst algorithm, Exp4.P, that\nwith high probability achieves O(\n\u221a\nT K ln N) regret\nin the adversarial contextual bandit setting.\nThis\nimproves on the O(T 2/3(K ln N)1/3) high probability\nbound in the stochastic setting. Previously, this re-\nsult was known to hold in expectation for the algo-\nrithm Exp4 [2], but a high probability statement did\nnot hold for the same algorithm, as per-round regrets\non the order of O(T \u22121/4) were possible [2]. Succeed-\ning with high probability is important because reliably\nuseful methods are preferred in practice.\nThe Exp4.P analysis addresses competing with a \ufb01nite\n(but possibly exponential in T ) set of policies. In the\nstochastic case, \u01eb-greedy or epoch-greedy style algo-\nrithms [13] can compete with an in\ufb01nite set of policies\nwith a \ufb01nite VC-dimension, but the worst-case regret\ngrows as O(T 2/3) rather than O(T 1/2). We show how\nto use Exp4.P in a black-box fashion to guarantee a\nhigh probability regret bound of O(\n\u221a\nT d ln T) in this\ncase, where d is the VC-dimension.\nThere are sim-\nple examples showing that it is impossible to compete\nwith a VC-set with an online adaptive adversary, so\nsome stochastic assumption seems necessary here.\nThis paper advances a basic argument, namely, that\nsuch exploration problems are solvable in almost ex-\nactly the same sense as supervised learning problems,\nwith suitable modi\ufb01cations to existing learning algo-\nrithms. In particular, we show that learning to com-\npete with any set of strategies in the contextual ban-\ndit setting requires only a factor of K more experience\nthan for supervised learning (to achieve the same level\nof accuracy with the same con\ufb01dence).\nExp4.P does retain one limitation of its predecessors\u2014\nit requires keeping explicit weights over the experts, so\nin the case when N is too large, the algorithm becomes\nine\ufb03cient. On the other hand, Exp4.P provides a prac-\ntical framework for incorporating more expressive ex-\npert classes, and it is e\ufb03cient when N is polynomial\nin K and T . It may also be possible to run Exp4.P\ne\ufb03ciently in certain cases when working with a family\nof experts that is exponentially large, but well struc-\ntured, as in the case of experts corresponding to all\nprunings of a decision tree [8]. A concrete example of\nthis approach is given in Section 7, where an e\ufb03cient\nimplementation of Exp4.P is applied to a large-scale\nreal-world problem.\nRelated work:\nThe non-contextual K-armed ban-\ndit problem was introduced by Robbins [17], and an-\nalyzed by Lai and Robbins [12] in the i.i.d. case for\n\ufb01xed reward distributions.\nAn adversarial version of the bandit problem was in-\ntroduced by Auer et al. [2]. They gave an exponential-\nweight algorithm called Exp3 with expected cumula-\ntive regret of \u02dcO(\n\u221a\nKT) and also Exp3.P with a similar\nbound that holds with high probability.\nThey also\nshowed that these are essentially optimal by proving\na matching lower bound, which holds even in the i.i.d.\ncase. They were also the \ufb01rst to consider the K-armed\nbandit problem with expert advice, introducing the\nExp4 algorithm as discussed earlier.\nLater, McMa-\nhan and Streeter [16] designed a cleaner algorithm\nthat improves on their bounds when many irrelevant\nactions (that no expert recommends) exist. Further\nbackground on online bandit problems appears in [5].\nExp4.P is based on a careful composition of the Exp4\nand Exp3.P algorithms. We distill out the exact expo-\nnential moment method bound used in these results,\nproving an inequality for martingales (Theorem 1) to\nderive a sharper bound more directly.\nOur bound\nis a Freedman-style inequality for martingales [6],\nand a similar approach was taken in Lemma 2 of\nBartlett et al. [3]. Our bound, however, is more el-\nemental than Bartlett et al.\u2019s since our Theorem can\nbe used to prove (and even tighten) their Lemma, but\nnot vice versa.\nWith respect to competing with a VC-set, a claim\nsimilar to our Theorem 5 (Section 5) appears in a\nwork of Lazaric and Munos [14]. Although they in-\ncorrectly claimed that Exp4 can be analyzed to give a\nregret bound of \u02dcO(KT ln N) with high probability, one\ncan use Exp4.P in their proof instead. Besides being\ncorrect, our analysis is tighter, which is important in\nmany situations where such a risk-sensitive algorithm\nmight be applied.\nRelated to the bounded VC-dimension setting, Kakade\nand Kalai [10] give a O(T 3/4) regret guarantee for the\ntransductive online setting, where the learner can ob-\nserve the rewards of all actions, not only those it has\ntaken. In [4], Ben-David et al. consider agnostic online\nlearning for bounded Littlestone-dimension. However,\nas VC-dimension does not bound Littlestone dimen-\nsion, our work provides much tighter bounds in many\ncases.\nPossible Approaches for a High Probability Al-\ngorithm\nAlina Beygelzimer, John Langford, Lihong Li, Lev Reyzin, Robert E. Schapire\nTo develop a better intuition about the problem, we\ndescribe several naive strategies and illustrate why\nthey fail. These strategies fail even if the rewards of\neach arm are drawn independently from a \ufb01xed un-\nknown distribution, and thus certainly fail in the ad-\nversarial setting.\nStrategy 1: Use con\ufb01dence bounds to maintain a set\nof plausible experts, and randomize uniformly over the\nactions predicted by at least one expert in this set. To\nsee how this strategy fails, consider two arms, 1 and\n0, with respective deterministic rewards 1 and 0. The\nexpert set contains N experts. At every round, one\nof them is chosen uniformly at random to predict arm\n0, and the remaining N \u22121 predict arm 1. All of the\nexperts have small regret with high probability. The\nstrategy will randomize uniformly over both arms on\nevery round, incurring expected regret of nearly T/2.\nStrategy 2: Use con\ufb01dence bounds to maintain a set\nof plausible experts, and follow the prediction of an ex-\npert chosen uniformly at random from this set. To see\nhow this strategy fails, let the set consist of N > 2T ex-\nperts predicting in some set of arms, all with reward 0\nat every round, and let there be a good expert choosing\nanother arm, which always has reward 1. The prob-\nability we never choose the good arm is (1 \u22121/N)T .\nWe have \u2212T log(1 \u2212\n1\nN ) < T\n1\nN\n1\u22121\nN \u2264\n2T\nN\n< 1, using\nthe elementary inequality \u2212log(1 \u2212x) < x/(1 \u2212x) for\nx \u2208(0, 1]. Thus (1 \u22121/N)T > 1\n2, and the strategy in-\ncurs regret of T with probability greater than 1/2 (as\nit only observes 0 rewards and is unable to eliminate\nany of the bad experts).\n2\nPROBLEM SETTING AND\nNOTATION\nLet r(t) \u2208[0, 1]K be the vector of rewards, where rj(t)\nis the reward of arm j on round t. Let \u03bei(t) be the\nK-dimensional advice vector of expert i on round t.\nThis vector represents a probability distribution over\nthe arms, in which each entry \u03bei\nj(t) is the (expert\u2019s\nrecommendation for the) probability of choosing arm\nj. For readability, we always use i \u2208{1, . . . , N} to\nindex experts and j \u2208{1, . . ., K} to index arms.\nFor each policy \u03c0, the associated expert predicts ac-\ncording to \u03c0(xt), where xt is the context available in\nround t. As the context is only used in this fashion\nhere, we talk about expert predictions as described\nabove. For a deterministic \u03c0, the corresponding pre-\ndiction vector has a 1 in component \u03c0(xt) and 0 in the\nremaining components.\nOn each round t, the world commits to r(t) \u2208[0, 1]K.\nThen the N experts make their recommendations\n\u03be1(t), . . . , \u03beN(t), and the learning algorithm A (seeing\nthe recommendations but not the rewards) chooses ac-\ntion jt \u2208{1, . . . , K}. Finally, the world reveals reward\nrjt(t) to the learner, and this game proceeds to the\nnext round.\nWe de\ufb01ne the return (cumulative reward) of A as\nGA .= PT\nt=1 rjt(t). Letting yi(t) = \u03bei(t) \u00b7 r(t), we also\nde\ufb01ne the expected return of expert i,\nGi .=\nT\nX\nt=1\nyi(t),\nand Gmax = maxi Gi.\nThe expected regret of algo-\nrithm A is de\ufb01ned as\nGmax \u2212E[GA].\nWe can also think about bounds on the regret which\nhold with arbitrarily high probability. In that case, we\ncan say that the regret is bounded by \u01eb with probabil-\nity 1 \u2212\u03b4, if we have\nPr[Gmax \u2212GA > \u01eb] \u2264\u03b4.\nIn the de\ufb01nitions of expected regret and the high prob-\nability bound, the probabilities and expectations are\ntaken w.r.t. both the randomness in the rewards r(t)\nand the algorithm\u2019s random choices.\n3\nA GENERAL RESULT FOR\nMARTINGALES\nBefore proving our main result (Theorem 2), we prove\na general result for martingales in which the variance is\ntreated as a random variable. It is used in the proof of\nLemma 3 and may also be of independent interest. The\ntechnique is the standard one used to prove Bernstein\u2019s\ninequality for martingales [6].\nThe useful di\ufb00erence\nhere is that we prove the bound for any \ufb01xed estimate\nof the variance rather than any bound on the variance.\nLet X1, . . . , XT be a sequence of real-valued random\nvariables. Let Et [Y ] = E [Y |X1, . . . , Xt\u22121].\nTheorem 1. Assume, for all t, that Xt \u2264R and that\nEt [Xt] = 0. De\ufb01ne the random variables\nS .=\nT\nX\nt=1\nXt,\nV .=\nT\nX\nt=1\nEt\n\u0002\nX2\nt\n\u0003\n.\nThen for any \u03b4 > 0, with probability at least 1 \u2212\u03b4, we\nhave the following guarantee:\nFor any V \u2032 \u2208\nh\nR2 ln(1/\u03b4)\ne\u22122\n, \u221e\n\u0011\n,\nS \u2264\np\n(e \u22122) ln(1/\u03b4)\n\u0012 V\n\u221a\nV \u2032 +\n\u221a\nV \u2032\n\u0013\nContextual Bandit Algorithms with Supervised Learning Guarantees\nand for V \u2032 \u2208\nh\n0, R2 ln(1/\u03b4)\ne\u22122\ni\n,\nS \u2264R ln(1/\u03b4) + (e \u22122)V\nR.\nNote that a simple corollary of this theorem is the more\ntypical Freedman-style inequality, which depends on\nan a priori upper bound, which can be substituted for\nV \u2032 and V .\nProof. For a \ufb01xed \u03bb\n\u2208\n[0, 1/R], conditioning on\nX1, . . . , Xt\u22121 and computing expectations gives\nEt\n\u0002\ne\u03bbXt\u0003\n\u2264\nEt\n\u0002\n1 + \u03bbXt + (e \u22122)\u03bb2X2\nt\n\u0003\n(1)\n=\n1 + (e \u22122)\u03bb2Et\n\u0002\nX2\nt\n\u0003\n(2)\n\u2264\nexp\n\u0000(e \u22122)\u03bb2Et\n\u0002\nX2\nt\n\u0003\u0001\n.\n(3)\nEq. (1) uses the fact that ez \u22641 + z + (e \u22122)z2 for\nz \u22641. Eq. (2) uses Et [Xt] = 0. Eq. (3) uses 1+z \u2264ez\nfor all z.\nLet us de\ufb01ne random variables Z0 = 1 and, for t \u22651,\nZt = Zt\u22121 \u00b7 exp\n\u0000\u03bbXt \u2212(e \u22122)\u03bb2Et\n\u0002\nX2\nt\n\u0003\u0001\n.\nThen,\nEt [Zt]\n=\nZt\u22121 \u00b7 exp\n\u0000\u2212(e \u22122)\u03bb2Et\n\u0002\nX2\nt\n\u0003\u0001\n\u00b7 Et\n\u0002\ne\u03bbXt\u0003\n\u2264\nZt\u22121 \u00b7 exp\n\u0000\u2212(e \u22122)\u03bb2Et\n\u0002\nX2\nt\n\u0003\u0001\n\u00b7 exp\n\u0000(e \u22122)\u03bb2Et\n\u0002\nX2\nt\n\u0003\u0001\n= Zt\u22121.\nTherefore, taking expectation over all of the variables\nX1, . . . , XT gives\nE [ZT ] \u2264E [ZT \u22121] \u2264\u00b7 \u00b7 \u00b7 \u2264E [Z0] = 1.\nBy Markov\u2019s inequality, Pr [ZT \u22651/\u03b4] \u2264\u03b4. Since\nZT = exp\n\u0000\u03bbS \u2212(e \u22122)\u03bb2V\n\u0001\n,\nwe can substitute \u03bb = min\nn\n1\nR,\nq\nln(1/\u03b4)\n(e\u22122)V \u2032\no\nand apply\nalgebra to prove the theorem.\n4\nA HIGH PROBABILITY\nALGORITHM\nThe Exp4.P algorithm is given in Algorithm 1.\nIt\ncomes with the following guarantee.\nTheorem 2. Assume that ln(N/\u03b4) \u2264KT , and that\nthe set of experts includes one which, on each round,\nselects an action uniformly at random.\nThen, with\nprobability at least 1 \u2212\u03b4,\nGExp4.P \u2265Gmax \u22126\np\nKT ln(N/\u03b4).\nAlgorithm 1 Exp4.P\nparameters: \u03b4 > 0, pmin \u2208[0, 1/K]\n\u0012\nwe set pmin =\nq\nln N\nKT\n\u0013\ninitialization:\nSet wi(1) = 1 for i = 1, . . . , N.\nfor each t = 1, 2, . . .\n1. get advice vectors \u03be1(t), . . . , \u03beN(t)\n2. set Wt = PN\ni=1 wi(t) and for j = 1, . . . , K set\npj(t) = (1 \u2212Kpmin)\nN\nX\ni=1\nwi(t)\u03bei\nj(t)\nWt\n+ pmin\n3. draw action jt randomly according to the proba-\nbilities p1(t), . . . , pK(t).\n4. receive reward rjt(t) \u2208[0, 1].\n5. for j = 1, . . . , K set\n\u02c6rj(t) =\n(\nrj(t)/pj(t)\n0\nif j = jt\notherwise\n6. for i = 1, . . . , N set\n\u02c6yi(t)\n=\n\u03bei(t) \u00b7 \u02c6r(t)\n\u02c6vi(t)\n=\nX\nj\n\u03bei\nj(t)/pj(t)\nwi(t + 1)\n=\nwi(t)e\n\u0012\npmin\n2\n\u0012\n\u02c6yi(t)+\u02c6vi(t)\nq\nln(N/\u03b4)\nKT\n\u0013\u0013\nThe proof of this theorem relies on two lemmas. The\n\ufb01rst lemma gives an upper con\ufb01dence bound on the ex-\npected reward of an expert given the estimated reward\nof that expert.\nThe estimated reward of an expert is de\ufb01ned as\n\u02c6Gi .=\nT\nX\nt=1\n\u02c6yi(t).\nWe also de\ufb01ne\n\u02c6\u03c3i .=\n\u221a\nKT +\n1\n\u221a\nKT\nT\nX\nt=1\n\u02c6vi(t).\nLemma 3. Under the conditions of Theorem 2,\nPr\nh\n\u2203i : Gi \u2265\u02c6Gi +\np\nln(N/\u03b4)\u02c6\u03c3i\ni\n\u2264\u03b4.\nProof. Fix i. Recalling that yi(t) = \u03bei(t)\u00b7r(t) and the\nde\ufb01nition of \u02c6yi(t) in Algorithm 1, let us further de\ufb01ne\nAlina Beygelzimer, John Langford, Lihong Li, Lev Reyzin, Robert E. Schapire\nthe random variables Xt = yi(t) \u2212\u02c6yi(t) to which we\nwill apply Theorem 1. Then Et [\u02c6yi(t)] = yi(t) so that\nEt [Xt] = 0 and Xt \u22641. Further, we can compute\nEt\n\u0002\nX2\nt\n\u0003\n=\nEt\n\u0002\n(yi(t) \u2212\u02c6yi(t))2\u0003\n=\nEt\n\u0002\n\u02c6yi(t)2\u0003\n\u2212yi(t)2 \u2264Et[\u02c6yi(t)2]\n=\nEt\nh\u0000\u03bei(t) \u00b7 \u02c6r(t)\n\u00012i\n=\nX\nj\npj(t)\n\u0012\n\u03bei\nj(t) \u00b7 rj(t)\npj(t)\n\u00132\n\u2264\nX\nj\n\u03bei\nj(t)\npj(t)\n=\n\u02c6vi(t).\nNote that\nGi \u2212\u02c6Gi =\nT\nX\nt=1\nXt.\nUsing \u03b4/N instead of \u03b4, and setting V \u2032 = KT in The-\norem 1 gives us\nPr\nh\nGi \u2212\u02c6Gi \u2265\nq\n(e \u22122) ln\n\u0000 N\n\u03b4\n\u0001 \u0010 PT\nt=1 \u02c6vi(t)\n\u221a\nKT\n+\n\u221a\nKT\n\u0011i\n\u2264\n\u03b4/N\n.\nNoting that e\u22122 < 1, and applying a union bound over\nthe N experts gives the statement of the lemma.\nTo state the next lemma, de\ufb01ne\n\u02c6U = max\ni\n\u0010\n\u02c6Gi + \u02c6\u03c3i \u00b7\np\nln(N/\u03b4)\n\u0011\n.\nLemma 4. Under the conditions of Theorem 2,\nGExp4.P\n\u2265\n \n1 \u22122\nr\nK ln N\nT\n!\n\u02c6U \u22122\np\nKT ln(N/\u03b4)\n\u2212\n\u221a\nKT ln N \u2212ln(N/\u03b4).\nWe can now prove Theorem 2.\nProof. Taking the statement of Lemma 4 and applying\nthe result of Lemma 3, and we get, with probability at\nleast 1 \u2212\u03b4,\nGExp4.P\n\u2265\nGmax \u22122\nr\nK ln N\nT\nT \u2212ln(N/\u03b4) (4)\n\u2212\n\u221a\nKT ln N \u22122\np\nKT ln(N/\u03b4)\n\u2265\nGmax \u22126\np\nKT ln(N/\u03b4),\nwith Eq. (4) using Gmax \u2264T .\n5\nCOMPETING WITH SETS OF\nFINITE VC DIMENSION\nA standard VC-argument in the online setting can be\nused to apply Exp4.P to compete with an in\ufb01nite set\nof policies \u03a0 with a \ufb01nite VC dimension d, when the\ndata is drawn independently from a \ufb01xed, unknown\ndistribution. For simplicity, this section assumes that\nthere are only two actions (K = 2), as that is standard\nfor the de\ufb01nition of VC-dimension.\nThe algorithm VE chooses an action uniformly at ran-\ndom for the \ufb01rst \u03c4 =\nq\nT (2d ln eT\nd + ln 2\n\u03b4 ) rounds.\nThis step partitions \u03a0 into equivalence classes accord-\ning to the sequence of advice on the \ufb01rst \u03c4 rounds. The\nalgorithm constructs a \ufb01nite set of policies \u03a0\u2032 by tak-\ning one (arbitrary) policy from each equivalence class,\nand runs Exp4.P for the remaining T \u2212\u03c4 steps using\n\u03a0\u2032 as its set of experts.\nFor a set of policies \u03a0, de\ufb01ne Gmax(\u03a0) as the return of\nthe best policy in \u03a0 at time horizon T .\nTheorem 5. For all distributions D over contexts and\nrewards, for all sets of policies \u03a0 with VC dimension\nd, with probability 1 \u2212\u03b4,\nGVE \u2265Gmax(\u03a0) \u22129\ns\n2T\n\u0012\nd ln eT\nd + ln 2\n\u03b4\n\u0013\n.\nProof. The regret of the initial exploration is bounded\nby \u03c4. We \ufb01rst bound the regret of Exp4.P to \u03a0\u2032, and\nthe regret of \u03a0\u2032 to \u03a0. We then optimize with respect\nto \u03c4 to get the result.\nSauer\u2019s lemma implies that |\u03a0\u2032| \u2264\n\u0000 e\u03c4\nd\n\u0001d and hence\nwith probability 1\u2212\u03b4/2, we can bound GExp4.P(\u03a0\u2032, T \u2212\n\u03c4) from below by\nGmax(\u03a0\u2032) \u22126\np\n2(T \u2212\u03c4)(d ln(e\u03c4/d) + ln(2/\u03b4)).\nTo bound the regret of \u03a0\u2032 to \u03a0, pick any sequence of\nfeature observations x1, ..., xT . Sauer\u2019s Lemma implies\nthe number of unique functions on the observation se-\nquence in \u03a0 is bounded by\n\u0000 eT\nd\n\u0001d.\nFor a uniformly random subset S of size \u03c4 of the fea-\nture observations we bound the probability that two\nfunctions \u03c0, \u03c0\u2032 agree on the subset. Let n = n(\u03c0, \u03c0\u2032)\nbe the number of disagreements on the T -length se-\nquence. Then\nPrS [\u2200x \u2208S \u03c0(x) = \u03c0\u2032(x)] =\n\u0010\n1 \u2212n\nT\n\u0011\u03c4\n\u2264e\u2212n\u03c4\nT .\nThus for all \u03c0, \u03c0\u2032 \u2208\u03a0 with n(\u03c0, \u03c0\u2032) \u2265T\n\u03c4 ln 1/\u03b40, we\nhave PrS [\u2200x \u2208S \u03c0(x) = \u03c0\u2032(x)] \u2264\u03b40.\nContextual Bandit Algorithms with Supervised Learning Guarantees\nSetting \u03b40 = \u03b4\n2\n\u0000 d\neT\n\u00012d and using a union bound over\nevery pair of policies, we get\nPrS(\u2203\u03c0, \u03c0\u2032\ns.t. n(\u03c0, \u03c0\u2032) \u2265T\n\u03c4\n\u00002d ln eT\nd + ln 2\n\u03b4\n\u0001\ns.t. \u2200x \u2208S \u03c0(x) = \u03c0\u2032(x)) \u2264\u03b4/2.\nIn other words, for all sequences x1, ..., xT with prob-\nability 1 \u2212\u03b4/2 over a random subset of size \u03c4\nGmax(\u03a0\u2032) \u2265Gmax(\u03a0) \u2212T\n\u03c4\n\u0012\n2d ln eT\nd + ln 2\n\u03b4\n\u0013\n.\nBecause the above holds for any sequence x1, ..., xT , it\nholds in expectation over sequences drawn i.i.d. from\nD. Furthermore, we can regard the \ufb01rst \u03c4 samples as\nthe random draw of the subset since i.i.d. distributions\nare exchangeable.\nConsequently, with probability 1 \u2212\u03b4, we have\nGVE\n\u2265\nGmax(\u03a0) \u2212\u03c4 \u2212T\n\u03c4\n\u0012\n2d ln eT\nd + ln 2\n\u03b4\n\u0013\n\u22126\np\n2T (d ln(e\u03c4/d) + ln(2/\u03b4)).\nLetting \u03c4 =\nq\nT (2d ln eT\nd + ln 2\n\u03b4 ) and substituting T \u2265\n\u03c4 we get\nGVE \u2265Gmax(\u03a0) \u22129\nr\n2T (d ln eT\nd + ln 2\n\u03b4 ).\nThis theorem easily extends to more than two actions\n(K > 2) given generalizations of the VC-dimension to\nmulticlass classi\ufb01cation and of Sauer\u2019s lemma [7].\n6\nA PRACTICAL IMPROVEMENT\nTO EXP4.P\nHere we give a variant of Step 2 of Algorithm 1 for\nsetting the probabilities pj(t), in the style of [16]. For\nour analysis of Exp4.P, the two properties we need to\nensure in setting the probabilities pj(t) are\n1. pj(t) \u2248PN\ni=1\nwi(t)\u03bei\nj(t)\nWt\n.\n2. The value of each pj(t) is at least pmin.\nOne way to achieve this, as is done in Algorithm 1,\nis to mix in the uniform distribution over all arms.\nWhile this yields a simpler algorithm and achieves op-\ntimal regret up to a multiplicative constant, in general,\nthis technique can add unnecessary probability mass to\nbadly-performing arms; for example it can double the\nprobability of arms whose probability would already\nbe set to pmin.\nAlgorithm 2 An Alternate Method for Setting Prob-\nabilities in Step 2 of Algorithm 1\nparameters:\nw1(t), w2(t), . . . wN(t)\nand\n\u03be1(t), . . . , \u03beN(t) and pmin\nset\nWt =\nN\nX\ni=1\nwi(t)\nfor j = 1 to K set\npj(t) =\nN\nX\ni=1\nwi(t)\u03bei\nj(t)\nWt\nlet \u2206:= 0 and l := 1\nfor each action j in increasing order according to pj\n1. if pj (1 \u2212\u2206/l) \u2265pmin\nfor all actions j\u2032 with pj\u2032 \u2265pj\np\u2032\nj\u2032 = pj\u2032 (1 \u2212\u2206/l)\nreturn \u2200j p\u2032\nj\n2. else p\u2032\nj = pmin, \u2206:= \u2206+ p\u2032\nj \u2212pj, l := l \u2212pj.\nA \ufb01x to this, \ufb01rst suggested by [16], is to ensure the two\nrequirements via a di\ufb00erent mechanism. We present\na variant of their suggestion in Algorithm 2, which\ncan be used to make Exp4.P perform better in prac-\ntice with a computational complexity of O(K ln K) for\ncomputing the probabilities pj(t) per round. The ba-\nsic intuition of this algorithm is that it enforces the\nminimum probability in order from smallest to largest\naction probability, while otherwise minimizing the ra-\ntio of the initial to \ufb01nal action probability.\nThis technique ensures our needed properties, and it is\neasy to verify that by setting probabilities using Algo-\nrithm 2 the proof in Section 4 remains valid with little\nmodi\ufb01cation. We use this variant in the experiments\nin Section 7.\n7\nEXPERIMENTS\nIn this section, we applied Exp4.P with the improve-\nment in Section 6 to a large-scale contextual bandit\nproblem. The purpose of the experiments is two-fold:\nit gives a proof-of-concept demonstration of the per-\nformance of Exp4.P in a non-trivial problem, and also\nillustrates how the algorithm may be implemented ef-\n\ufb01ciently for special classes of experts.\nThe problem we study is personalized news article rec-\nommendation on the Yahoo! front page [1, 15]. Each\ntime a user visits the front page, a news article out of\nAlina Beygelzimer, John Langford, Lihong Li, Lev Reyzin, Robert E. Schapire\na small pool of hand-picked candidates is highlighted.\nThe goal is to highlight the most interesting articles to\nusers, or formally, maximize the total number of user\nclicks on the recommended articles. In this problem,\nwe treat articles as arms, and de\ufb01ne the payo\ufb00to be 1\nif the article is clicked on and 0 otherwise. Therefore,\nthe average per-trial payo\ufb00of an algorithm/policy is\nthe overall click-through rate (or CTR for short).\nFollowing [15], we created B = 5 user clusters and\nthus each user, based on normalized Euclidean dis-\ntance to the cluster centers, was associated with\na B-dimensional membership feature d whose (non-\nnegative) components always sum up to 1. Experts\nare designed as follows. Each expert is associated with\na mapping from user clusters to articles, that is, with\na vector a \u2208{1, . . ., K}B where ab is the article to be\ndisplayed for users from cluster b \u2208{1, . . . , B}. When\na user arrives with feature d, the prediction \u03bea of ex-\npert a is \u03bea\nj = P\nb:ab=j db. There are a total of KB\nexperts.\nNow we show how to implement Exp4.P e\ufb03ciently. Re-\nferring to the notation in Exp4.P, we have\n\u02c6ya(t)\n=\n\u03bea(t) \u00b7 \u02c6r(t) =\nX\nj\nX\nb:ab=j\ndb(t)\u02c6rj(t)\n=\nX\nb\ndb(t)\u02c6rab(t),\n\u02c6va(t)\n=\nX\nj\nX\nb:ab=j\ndb(t)\npj(t) =\nX\nb\ndb(t)\npab(t).\nThus,\nwa(t + 1)\n= wa(t) exp\n \npmin\n2\n \n\u02c6ya(t) + \u02c6va(t)\nr\nln(N/\u03b4)\nKT\n!!\n= wa(t) exp\n X\nb\ndb(t)fab(t)\n!\n,\nwhere\nfj(t) = pmin\n2\n \n\u02c6rj(t) +\n1\npj(t)\nr\nln(N/\u03b4)\nKT\n!\n.\nUnraveling the recurrence, we rewrite wa(t + 1) by\nwa(t + 1)\n=\nexp\n \nt\nX\n\u03c4=1\nX\nb\ndb(\u03c4)fab(\u03c4)\n!\n=\nexp\n X\nb\nt\nX\n\u03c4=1\ndb(\u03c4)fab(\u03c4)\n!\n=\nY\nb\ngb,ab(t),\nimplying\nthat\nwa(t + 1)\ncan\nbe\ncomputed\nim-\nplicitly\nby\nmaintaining\nthe\nquantity\ngb,j(t)\n=\nexp\n\u0010Pt\n\u03c4=1 db(\u03c4)fj(\u03c4)\n\u0011\nfor each b and j.\nNext,\nwe compute Wt as follows:\nWt\n=\nP\na wa(t)\n=\nP\na\nQ\nb gb,ab(t)\n=\nQ\nb\n\u0010P\nj gb,j(t)\n\u0011\n. Repeating the\nsame trick, we have\nX\na\nwa(t)\u03bea(t)\nWt\n=\nX\nb\ndb(t)gb,j(t)\nPK\nj\u2032=1 gb,j\u2032(t)\n,\nwhich are the inputs to Algorithm 2 to produce the\n\ufb01nal arm-selection probabilities, pj(t) for all j. There-\nfore, for this structured set of experts, the time com-\nplexity of Exp4.P is only linear in K and B despite\nthe exponentially large size of this set.\nTo compare algorithms, we collected historical user\nvisit events with a random policy that chose articles\nuniformly at random for a fraction of user visits on\nthe Yahoo! front page from May 1 to 9, 2009. This\ndata contains over 41M user visits, a total of 253 ar-\nticles, and about 21 candidate articles in the pool per\nuser visit.\n(The pool of candidate articles changes\nover time, requiring corresponding modi\ufb01cations to\nExp4.P1). With such random tra\ufb03c data, we were able\nto obtain an unbiased estimate of the CTR (called\neCTR) of a bandit algorithm as if it is run in the\nreal world [15].\nDue to practical concerns when applying a bandit algo-\nrithm, it is common to randomly assign each user visit\nto one of two \u201cbuckets\u201d: the learning bucket, where the\nbandit algorithm is run, and the deployment bucket,\nwhere the greedy policy (learned by the algorithm in\nthe learning bucket) is used to serve users without re-\nceiving payo\ufb00information. Note that since the ban-\ndit algorithm continues to re\ufb01ne its policy based on\npayo\ufb00feedback in the learning bucket, its greedy pol-\nicy may change over time. Its eCTR in the deploy-\nment bucket thus measures how good this greedy pol-\nicy is. And as the deployment bucket is usually much\nlarger than the learning bucket, the deployment eCTR\nis deemed a more important metric. Finally, to protect\nbusiness-sensitive information, we only report normal-\nized eCTRs, which are the actual eCTRs divided by\nthe random policy\u2019s eCTR.\nBased on estimates of T and K, we ran Exp4.P with\n\u03b4 = 0.01. The same estimates were used to set \u03b3 in\nExp4 to minimize the regret bound in Theorem 7.1 of\n[2]. Table 1 summarizes eCTRs of all three algorithms\nin the two buckets. All di\ufb00erences are signi\ufb01cant due\nto the large volume of data.\nFirst, Exp4.P\u2019s eCTR is slightly worse than Exp4 in\n1Our modi\ufb01cation ensured that a new article\u2019s initial\nscore was the average of all currently available ones\u2019.\nContextual Bandit Algorithms with Supervised Learning Guarantees\nExp4.P\nExp4\n\u01eb-greedy\nlearning CTR\n1.0525\n1.0988\n1.3827\ndeployment CTR\n1.6512\n1.5309\n1.4290\nTable 1: Overall click-through rates (eCTRs) of vari-\nous algorithms on the May 1\u20139 data set.\nthe learning bucket. This gap is probably due to the\nmore conservative nature of Exp4.P, as it uses the ad-\nditional \u02c6vi terms to control variance, which in turn\nencourages further exploration. In return for the more\nextensive exploration, Exp4.P gained the highest de-\nployment eCTR, implying its greedy policy is superior\nto Exp4.\nSecond, we note a similar comparison to the \u01eb-greedy\nvariant of Exp4.P. It was the most greedy among the\nthree algorithms and thus had the highest eCTR in the\nlearning bucket, but lowest eCTR in the deployment\nbucket. This fact also suggests the bene\ufb01ts of using\nthe somewhat more complicated soft-max exploration\nscheme in Exp4.P.\nAcknowledgments\nWe thank Wei Chu for assistance with the experiments\nand Kishore Papineni for helpful discussions.\nThis work was done while Lev Reyzin and Robert E.\nSchapire were at Yahoo! Research, NY. Lev Reyzin ac-\nknowledges this material is based upon work supported\nby the NSF under Grant #0937060 to the CRA for the\nComputing Innovation Fellowship program.\nReferences\n[1] Deepak Agarwal, Bee-Chung Chen, Pradheep\nElango, Nitin Motgi, Seung-Taek Park, Raghu\nRamakrishnan, Scott Roy, and Joe Zachariah.\nOnline models for content optimization. In Ad-\nvances in Neural Information Processing Systems\n21, pages 17\u201324, 2008.\n[2] Peter Auer, Nicol`o Cesa-Bianchi, Yoav Freund,\nand Robert E. Schapire. The nonstochastic mul-\ntiarmed bandit problem. SIAM Journal of Com-\nputing, 32(1):48\u201377, 2002.\n[3] Peter Bartlett, Varsha Dani, Thomas Hayes,\nSham Kakade, Alexander Rakhlin, and Ambuj\nTewari. High-probability regret bounds for ban-\ndit online linear optimization. In Conference on\nLearning Theory (COLT), 2008.\n[4] Shai Ben-david, D\u00b4avid P\u00b4al, and Shai Shalev-\nshwartz. Agnostic online learning. In Conference\non Learning Theory, 2009.\n[5] Nicol`o Cesa-Bianchi and Gabor Lugosi. Predic-\ntion, Learning, and Games. Cambridge Univer-\nsity Press, New York, NY, USA, 2006.\n[6] David A. Freedman.\nOn tail probabilities for\nmartingales. Annals of Probability, 3(1):100\u2013118,\n1975.\n[7] David Haussler and Philip M. Long. A general-\nization of Sauer\u2019s lemma. J. Comb. Theory, Ser.\nA, 71(2):219\u2013240, 1995.\n[8] David P. Helmbold and Robert E. Schapire. Pre-\ndicting nearly as well as the best pruning of a de-\ncision tree. Machine Learning, 27(1):51\u201368, 1997.\n[9] Leslie Pack Kaelbling. Associative reinforcement\nlearning: Functions in k-DNF. Machine Learning,\n15(3):279\u2013298, 1994.\n[10] Sham M. Kakade and Adam Kalai. From batch\nto transductive online learning. In NIPS, 2005.\n[11] Sham M. Kakade, Shai Shalev-Shwartz, and Am-\nbuj Tewari.\nE\ufb03cient bandit algorithms for on-\nline multiclass prediction. In Proceedings of the\nTwenty-Fifth International Conference on Ma-\nchine Learning (ICML), pages 440\u2013447, 2008.\n[12] Tze Leung Lai and Herbert Robbins. Asymptoti-\ncally e\ufb03cient adaptive allocation rules. Advances\nin Applied Mathematics, 6:4\u201322, 1985.\n[13] John Langford and Tong Zhang.\nThe epoch-\ngreedy algorithm for contextual multi-armed ban-\ndits. In Neural Information Processing Systems\n(NIPS), 2007.\n[14] Alessandro Lazaric and R\u00b4emi Munos.\nHybrid\nstochastic-adversarial on-line learning.\nIn Con-\nference on Learning Theory, 2009.\n[15] Lihong\nLi,\nWei\nChu,\nJohn\nLangford,\nand\nRobert E. Schapire.\nA contextual-bandit ap-\nproach to personalized news article recommenda-\ntion.\nIn Proceedings of the Nineteenth Interna-\ntional Conference on World Wide Web (WWW),\n2010.\n[16] Brendan\nMcMahan\nand\nMatthew\nStreeter.\nTighter bounds for multi-armed bandits with ex-\npert advice. In Conference on Learning Theory\n(COLT), 2009.\n[17] Herbert Robbins. Some aspects of the sequential\ndesign of experiments. Bulletin of the American\nMathematical Society, 58(5):527\u2013535, 1952.\n[18] Alexander\nL.\nStrehl,\nChris\nMesterharm,\nMichael\nL.\nLittman,\nand\nHaym\nHirsh.\nExperience-e\ufb03cient\nlearning\nin\nassociative\nbandit problems. In International Conference on\nMachine Learning (ICML), 2006.\n[19] Richard S. Sutton and Andrew G. Barto. Rein-\nforcement Learning: An Introduction. MIT Press,\n1998.\nAlina Beygelzimer, John Langford, Lihong Li, Lev Reyzin, Robert E. Schapire\nA\nPROOF OF LEMMA 4\nRecall that the estimated reward of expert i is de\ufb01ned\nas\n\u02c6Gi .=\nT\nX\nt=1\n\u02c6yi(t).\nAlso\n\u02c6\u03c3i .=\n\u221a\nKT +\n1\n\u221a\nKT\nT\nX\nt=1\n\u02c6vi(t)\nand that\n\u02c6U = max\ni\n\u0010\n\u02c6Gi + \u02c6\u03c3i \u00b7\np\nln(N/\u03b4)\n\u0011\n.\nLemma 4.\nUnder the conditions of Theorem 2,\nGExp4.P\n\u2265\n \n1 \u22122\nr\nK ln N\nT\n!\n\u02c6U \u22122\np\nKT ln(N/\u03b4)\n\u2212\n\u221a\nKT ln N \u2212ln(N/\u03b4).\nProof. For the proof, we use \u03b3 =\nq\nK ln N\nT\n.\nWe have\npj(t) \u2265pmin =\nr\nln N\nKT\nand\n\u02c6rj(t) \u22641/pmin\nso that\n\u02c6yi(t) \u22641/pmin\nand\n\u02c6vi(t) \u22641/pmin.\nThus,\npmin\n2\n \n\u02c6yi(t) +\nr\nln(N/\u03b4)\nKT\n\u02c6vi(t)\n!\n\u2264\npmin\n2\n(\u02c6yi(t) + \u02c6vi(t))\n\u2264\n1.\nLet \u00afwi(t) = wi(t)/Wt. We will need the following in-\nequality:\nInequality 1.\nPN\ni\n\u00afwi(t)\u02c6vi(t) \u2264\nK\n1\u2212\u03b3 .\nAs a corollary, we have\nN\nX\ni\n\u00afwi(t)\u02c6vi(t)2\n\u2264\nN\nX\ni\n\u00afwi(t)\u02c6vi(t)\n1\npmin\n\u2264\nr\nKT\nln N\nK\n1 \u2212\u03b3 .\nAlso, [2] (on p.67) prove the following two inequalities\n(with a typo). For completeness, the proofs of all three\ninequalities are given below this proof.\nInequality 2.\nPN\ni=1 \u00afwi(t)\u02c6yi(t) \u2264rjt(t)\n1\u2212\u03b3 .\nInequality 3.\nPN\ni=1 \u00afwi(t)\u02c6yi(t)2 \u2264\u02c6rjt(t)\n1\u2212\u03b3 .\nNow letting b = pmin\n2\nand c =\npmin\u221a\nln(N/\u03b4)\n2\n\u221a\nKT\nwe have\nWt+1\nWt\n=\nN\nX\ni=1\nwi(t + 1)\nWt\n=\nN\nX\ni=1\n\u00afwi(t) exp (b\u02c6yi(t) + c\u02c6vi(t))\n\u2264\nN\nX\ni=1\n\u00afwi(t) [1 + b\u02c6yi(t) + c\u02c6vi(t)]\n(5)\n+\nN\nX\ni=1\n\u00afwi(t)\n\u0002\n2b2\u02c6yi(t)2 + 2c2\u02c6vi(t)2\u0003\n=\n1 + b\nN\nX\ni=1\n\u00afwi(t)\u02c6yi(t) + c\nN\nX\ni=1\n\u00afwi(t)\u02c6vi(t)\n+2b2\nN\nX\ni=1\n\u00afwi(t)\u02c6yi(t)2 + 2c2\nN\nX\ni=1\n\u00afwi(t)\u02c6vi(t)2\n\u2264\n1 + brjt(t)\n1 \u2212\u03b3 + c\nK\n1 \u2212\u03b3 + 2b2 \u02c6rjt(t)\n1 \u2212\u03b3\n(6)\n+2c2\nr\nKT\nln N\nK\n1 \u2212\u03b3 .\nEq. (5) uses ea \u22641+a+(e\u22122)a2 for a \u22641, (a+b)2 \u2264\n2a2 + 2b2, and e \u22122 < 1. Eq. (6) uses inequalities 1\nthrough 3.\nNow take logarithms, use the inequality ln(1 + x) \u2264x,\nsum both sides over T , and we obtain\nln\n\u0012WT +1\nW1\n\u0013\n\u2264\nb\n1 \u2212\u03b3\nT\nX\nt=1\nrjt(t) + c KT\n1 \u2212\u03b3\n+ 2b2\n1 \u2212\u03b3\nT\nX\nt=1\n\u02c6rjt(t) + 2c2\nr\nKT\nln N\nKT\n1 \u2212\u03b3\n\u2264\nb\n1 \u2212\u03b3 GExp4.P + c KT\n1 \u2212\u03b3 +\n2b2\n1 \u2212\u03b3 K \u02c6U\n+2c2\nr\nKT\nln N\nKT\n1 \u2212\u03b3 .\nHere, we used\nGExp4.P =\nT\nX\nt=1\nrjt(t)\nand\nT\nX\nt=1\n\u02c6rjt(t) = K\nT\nX\nt=1\n1\nK\nK\nX\nj=1\n\u02c6rj(t) \u2264K \u02c6Guniform \u2264K \u02c6U.\nContextual Bandit Algorithms with Supervised Learning Guarantees\nbecause we assumed that the set of experts includes\none who always selects each action uniformly at ran-\ndom.\nWe also have ln(W1) = ln(N) and\nln(WT +1)\n\u2265\nmax\ni\n(ln wi(T + 1))\n=\nmax\ni\n \nb \u02c6Gi + c\nT\nX\nt=1\n\u02c6vi(t)\n!\n=\nb \u02c6U \u2212b\np\nKT ln(N/\u03b4).\nCombining then gives\nb \u02c6U \u2212b\np\nKT ln(N/\u03b4) \u2212ln N\n\u2264\nb\n1\u2212\u03b3 GExp4.P + c KT\n1\u2212\u03b3 + 2b2\n1\u2212\u03b3 K \u02c6U + 2c2\nq\nKT\nln N\nKT\n1\u2212\u03b3 .\nSolving for GExp4.P now gives\nGExp4.P\n\u2265\n(1 \u2212\u03b3 \u22122bK) \u02c6U \u2212\n\u00121 \u2212\u03b3\nb\n\u0013\nln N\n\u2212(1 \u2212\u03b3)\np\nKT ln(N/\u03b4) \u2212c\nbKT\n\u22122c2\nb\nr\nKT\nln N KT\n\u2265\n(1 \u2212\u03b3 \u22122bK) \u02c6U \u2212\np\nKT ln(N/\u03b4) (7)\n\u22121\nb ln N \u2212c\nbKT \u22122c2\nb\nr\nKT\nln N KT\n=\n \n1 \u22122\nr\nK ln N\nT\n!\n\u02c6U \u2212ln(N/\u03b4)\n(8)\n\u22122\n\u221a\nKT ln N \u2212\np\nKT ln(N/\u03b4),\nusing \u03b3 > 0 in Eq. (7) and plugging in the de\ufb01nition\nof \u03b3, b, c in Eq. (8).\nWe prove Inequalities 1 through 3 below.\nLet \u00afwi(t) = wi(t)/Wt.\nInequality 1.\nPN\ni\n\u00afwi(t)\u02c6vi(t) \u2264\nK\n1\u2212\u03b3 .\nProof.\nN\nX\ni\n\u00afwi(t)\u02c6vi(t)\n=\nN\nX\ni\n\u00afwi(t)\nK\nX\nj\n\u03bei\nj(t)\npj(t)\n=\nK\nX\nj=1\n1\npj(t)\nN\nX\ni\n\u00afwi(t)\u03bei\nj(t)\n=\nK\nX\nj=1\n1\npj(t)\n\u0012pj(t) \u2212pmin\n1 \u2212\u03b3\n\u0013\n\u2264\nK\nX\nj=1\n1\n1 \u2212\u03b3\n=\nK\n1 \u2212\u03b3 .\nInequality 2.\nPN\ni=1 \u00afwi(t)\u02c6yi(t) \u2264rjt(t)\n1\u2212\u03b3 .\nProof.\nN\nX\ni=1\n\u00afwi(t)\u02c6yi(t)\n=\nN\nX\ni=1\n\u00afwi(t)\n\uf8eb\n\uf8ed\nK\nX\nj=1\n\u03bei\nj(t)\u02c6rj(t)\n\uf8f6\n\uf8f8\n=\nK\nX\nj=1\n N\nX\ni=1\n\u00afwi(t)\u03bei\nj(t)\n!\n\u02c6rj(t)\n=\nK\nX\nj=1\n\u0012pj(t) \u2212pmin\n1 \u2212\u03b3\n\u0013\n\u02c6rj(t)\n\u2264\nrjt(t)\n1 \u2212\u03b3 .\nInequality 3.\nPN\ni=1 \u00afwi(t)\u02c6yi(t)2 \u2264\u02c6rjt(t)\n1\u2212\u03b3 .\nProof.\nN\nX\ni=1\n\u00afwi(t)\u02c6yi(t)2\n=\nN\nX\ni=1\n\u00afwi(t)\n\uf8eb\n\uf8ed\nK\nX\nj=1\n\u03bei\nj(t)\u02c6rj(t)\n\uf8f6\n\uf8f8\n2\n=\nN\nX\ni=1\n\u00afwi(t)\n\u0000\u03bei\njt(t)\u02c6rjt(t)\n\u00012\n\u2264\n\u0012pjt(t)\n1 \u2212\u03b3\n\u0013\n\u02c6rjt(t)2\n\u2264\n\u02c6rjt(t)\n1 \u2212\u03b3 .\n",
        "sentence": " The first inequality, which is a Bernstein-type deviation bound for martingales, is Freedman\u2019s inequality, which is from Beygelzimer et al. [2011] Lemma 18 (Freedman\u2019s Inequality).",
        "context": "Our bound\nis a Freedman-style inequality for martingales [6],\nand a similar approach was taken in Lemma 2 of\nBartlett et al. [3]. Our bound, however, is more el-\nemental than Bartlett et al.\u2019s since our Theorem can\ntypical Freedman-style inequality, which depends on\nan a priori upper bound, which can be substituted for\nV \u2032 and V .\nProof. For a \ufb01xed \u03bb\n\u2208\n[0, 1/R], conditioning on\nX1, . . . , Xt\u22121 and computing expectations gives\nEt\n\u0002\ne\u03bbXt\u0003\n\u2264\nEt\n\u0002\n1 + \u03bbXt + (e \u22122)\u03bb2X2\ntreated as a random variable. It is used in the proof of\nLemma 3 and may also be of independent interest. The\ntechnique is the standard one used to prove Bernstein\u2019s\ninequality for martingales [6].\nThe useful di\ufb00erence"
    },
    {
        "title": "Counterfactual reasoning and learning systems: The example of computational advertising",
        "author": [
            "L\u00e9on Bottou",
            "Jonas Peters",
            "Joaquin Qui\u00f1onero-Candela",
            "Denis Xavier Charles",
            "D. Max Chickering",
            "Elon Portugaly",
            "Dipankar Ray",
            "Patrice Simard",
            "Ed Snelson"
        ],
        "venue": "Journal of Machine Learning Research,",
        "citeRegEx": "Bottou et al\\.,? \\Q2013\\E",
        "shortCiteRegEx": "Bottou et al\\.",
        "year": 2013,
        "abstract": "",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "Regret analysis of stochastic and nonstochastic multi-armed bandit problems",
        "author": [
            "S\u00e9bastien Bubeck",
            "Nicol\u00f2 Cesa-Bianchi"
        ],
        "venue": "Foundations and Trends in Machine Learning,",
        "citeRegEx": "Bubeck and Cesa.Bianchi.,? \\Q2012\\E",
        "shortCiteRegEx": "Bubeck and Cesa.Bianchi.",
        "year": 2012,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " Learning from partial feedback (\u201cbandit\u201d feedback) is of great practical importance and has seen a recent surge of research interest [Bubeck and Cesa-Bianchi, 2012].",
        "context": null
    },
    {
        "title": "Combinatorial pure exploration of multi-armed bandits",
        "author": [
            "Shouyuan Chen",
            "Tian Lin",
            "Irwin King",
            "Michael R Lyu",
            "Wei Chen"
        ],
        "venue": "In Advances in Neural Information Processing Systems,",
        "citeRegEx": "Chen et al\\.,? \\Q2014\\E",
        "shortCiteRegEx": "Chen et al\\.",
        "year": 2014,
        "abstract": "",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "Combinatorial multi-armed bandit: General framework and applications",
        "author": [
            "Wei Chen",
            "Yajun Wang",
            "Yang Yuan"
        ],
        "venue": "In International Conference on Machine Learning,",
        "citeRegEx": "Chen et al\\.,? \\Q2013\\E",
        "shortCiteRegEx": "Chen et al\\.",
        "year": 2013,
        "abstract": "",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "Contextual bandits with linear payoff functions",
        "author": [
            "Wei Chu",
            "Lihong Li",
            "Lev Reyzin",
            "Robert E Schapire"
        ],
        "venue": "In Artificial Intelligence and Statistics,",
        "citeRegEx": "Chu et al\\.,? \\Q2011\\E",
        "shortCiteRegEx": "Chu et al\\.",
        "year": 2011,
        "abstract": "",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "Efficient optimal learning for contextual bandits",
        "author": [
            "Miroslav Dud\u0131\u0301k",
            "Daniel Hsu",
            "Satyen Kale",
            "Nikos Karampatziakis",
            "John Langford",
            "Lev Reyzin",
            "Tong Zhang"
        ],
        "venue": "In Uncertainty and Artificial Intelligence,",
        "citeRegEx": "Dud\u0131\u0301k et al\\.,? \\Q2011\\E",
        "shortCiteRegEx": "Dud\u0131\u0301k et al\\.",
        "year": 2011,
        "abstract": "We address the problem of learning in an online setting where the learner\nrepeatedly observes features, selects among a set of actions, and receives\nreward for the action taken. We provide the first efficient algorithm with an\noptimal regret. Our algorithm uses a cost sensitive classification learner as\nan oracle and has a running time $\\mathrm{polylog}(N)$, where $N$ is the number\nof classification rules among which the oracle might choose. This is\nexponentially faster than all previous algorithms that achieve optimal regret\nin this setting. Our formulation also enables us to create an algorithm with\nregret that is additive rather than multiplicative in feedback delay as in all\nprevious work.",
        "full_text": "arXiv:1106.2369v1  [cs.LG]  13 Jun 2011\nE\ufb03cient Optimal Learning for Contextual Bandits\nMiroslav Dudik\nmdudik@yahoo-inc.com\nDaniel Hsu\ndjhsu@rci.rutgers.edu\nSatyen Kale\nskale@yahoo-inc.com\nNikos Karampatziakis\nnk@cs.cornell.edu\nJohn Langford\njl@yahoo-inc.com\nLev Reyzin\nlreyzin@cc.gatech.edu\nTong Zhang\ntzhang@stat.rutgers.edu\nAbstract\nWe address the problem of learning in an on-\nline setting where the learner repeatedly ob-\nserves features, selects among a set of actions,\nand receives reward for the action taken. We\nprovide the \ufb01rst e\ufb03cient algorithm with an\noptimal regret.\nOur algorithm uses a cost\nsensitive classi\ufb01cation learner as an oracle\nand has a running time polylog(N), where N\nis the number of classi\ufb01cation rules among\nwhich the oracle might choose. This is expo-\nnentially faster than all previous algorithms\nthat achieve optimal regret in this setting.\nOur formulation also enables us to create an\nalgorithm with regret that is additive rather\nthan multiplicative in feedback delay as in all\nprevious work.\n1\nINTRODUCTION\nThe contextual bandit setting consists of the following\nloop repeated inde\ufb01nitely:\n1. The world presents context information as fea-\ntures x.\n2. The learning algorithm chooses an action a from\nK possible actions.\n3. The world presents a reward r for the action.\nThe key di\ufb00erence between the contextual bandit set-\nting and standard supervised learning is that only the\nreward of the chosen action is revealed. For example,\nafter always choosing the same action several times\nin a row, the feedback given provides almost no ba-\nsis to prefer the chosen action over another action.\nIn essence, the contextual bandit setting captures the\ndi\ufb03culty of exploration while avoiding the di\ufb03culty\nof credit assignment as in more general reinforcement\nlearning settings.\nThe contextual bandit setting is a half-way point be-\ntween standard supervised learning and full-scale re-\ninforcement learning where it appears possible to con-\nstruct algorithms with convergence rate guarantees\nsimilar to supervised learning. Many natural settings\nsatisfy this half-way point, motivating the investiga-\ntion of contextual bandit learning. For example, the\nproblem of choosing interesting news articles or ads for\nusers by internet companies can be naturally modeled\nas a contextual bandit setting. In the medical domain\nwhere discrete treatments are tested before approval,\nthe process of deciding which patients are eligible for\na treatment takes contexts into account. More gener-\nally, we can imagine that in a future with personalized\nmedicine, new treatments are essentially equivalent to\nnew actions in a contextual bandit setting.\nIn the i.i.d. setting, the world draws a pair (x,\u20d7r) con-\nsisting of a context and a reward vector from some\nunknown distribution D, revealing x in Step 1, but\nonly the reward r(a) of the chosen action a in Step 3.\nGiven a set of policies \u03a0 = {\u03c0 : X \u2192A}, the goal\nis to create an algorithm for Step 2 which competes\nwith the set of policies. We measure our success by\ncomparing the algorithm\u2019s cumulative reward to the\nexpected cumulative reward of the best policy in the\nset. The di\ufb00erence of the two is called regret.\nAll existing algorithms for this setting either achieve\na suboptimal regret (Langford and Zhang, 2007) or\nrequire computation linear in the number of poli-\ncies (Auer et al., 2002b; Beygelzimer et al., 2011). In\nunstructured policy spaces, this computational com-\nplexity is the best one can hope for.\nOn the other\nhand, in the case where the rewards of all actions are\nrevealed, the problem is equivalent to cost-sensitive\nclassi\ufb01cation, and we know of algorithms to e\ufb03ciently\nsearch the space of policies (classi\ufb01cation rules) such\nas cost-sensitive logistic regression and support vec-\ntor machines. In these cases, the space of classi\ufb01ca-\ntion rules is exponential in the number of features, but\nthese problems can be e\ufb03ciently solved using convex\noptimization.\nOur goal here is to e\ufb03ciently solve the contextual\nbandit problems for similarly large policy spaces.\nWe do this by reducing the contextual bandit prob-\nlem to cost-sensitive classi\ufb01cation.\nGiven a su-\npervised cost-sensitive learning algorithm as an or-\nacle (Beygelzimer et al., 2009), our algorithm runs\nin\ntime\nonly\npolylog(N)\nwhile\nachieving\nregret\nO(\n\u221a\nT K ln N), where N is the number of possible poli-\ncies (classi\ufb01cation rules), K is the number of actions\n(classes), and T is the number of time steps. This e\ufb03-\nciency is achieved in a modular way, so any future im-\nprovement in cost-sensitive learning immediately ap-\nplies here.\n1.1\nPREVIOUS WORK AND\nMOTIVATION\nAll previous regret-optimal approaches are measure\nbased\u2014they work by updating a measure over poli-\ncies, an operation which is linear in the number of\npolicies. In contrast, regret guarantees scale only log-\narithmically in the number of policies. If not for the\ncomputational bottleneck, these regret guarantees im-\nply that we could dramatically increase performance in\ncontextual bandit settings using more expressive poli-\ncies. We overcome the computational bottleneck using\nan algorithm which works by creating cost-sensitive\nclassi\ufb01cation instances and calling an oracle to choose\noptimal policies.\nActions are chosen based on the\npolicies returned by the oracle rather than accord-\ning to a measure over all policies. This is reminiscent\nof AdaBoost (Freund and Schapire, 1997), which cre-\nates weighted binary classi\ufb01cation instances and calls\na \u201cweak learner\u201d oracle to obtain classi\ufb01cation rules.\nThese classi\ufb01cation rules are then combined into a \ufb01-\nnal classi\ufb01er with boosted accuracy. Similarly as Ad-\naBoost converts a weak learner into a strong learner,\nour approach converts a cost-sensitive classi\ufb01cation\nlearner into an algorithm that solves the contextual\nbandit problem.\nIn a more di\ufb03cult version of contextual bandits, an ad-\nversary chooses (x,\u20d7r) given knowledge of the learning\nalgorithm (but not any random numbers). All known\nregret-optimal solutions in the adversarial setting are\nvariants of the EXP4 algorithm (Auer et al., 2002b).\nEXP4 achieves the same regret rate as our algorithm:\nO\n\u0010\u221a\nKT ln N\n\u0011\n, where T is the number of time steps,\nK is the number of actions available in each time step,\nand N is the number of policies.\nWhy not use EXP4 in the i.i.d. setting? For exam-\nple, it is known that the algorithm can be modi\ufb01ed\nto succeed with high probability (Beygelzimer et al.,\n2011), and also for VC classes when the adversary is\nconstrained to i.i.d. sampling. There are two central\nbene\ufb01ts that we hope to realize by directly assuming\ni.i.d. contexts and reward vectors.\n1. Computational Tractability. Even when the re-\nward vector is fully known, adversarial regrets\nscale as O\n\u0010\u221a\nln N\n\u0011\nwhile computation scales\nas O(N) in general.\nOne attempt to get\naround this is the follow-the-perturbed-leader al-\ngorithm (Kalai and Vempala, 2005) which pro-\nvides a computationally tractable solution in cer-\ntain special-case structures. This algorithm has\nno mechanism for e\ufb03cient application to arbitrary\npolicy spaces, even given an e\ufb03cient cost-sensitive\nclassi\ufb01cation oracle.\nAn e\ufb03cient cost-sensitive\nclassi\ufb01cation oracle has been shown e\ufb00ective in\ntransductive settings (Kakade and Kalai, 2005).\nAside from the drawback of requiring a transduc-\ntive setting, the regret achieved there is substan-\ntially worse than for EXP4.\n2. Improved Rates.\nWhen the world is not com-\npletely adversarial, it is possible to achieve sub-\nstantially lower regrets than are possible with al-\ngorithms optimized for the adversarial setting.\nFor example, in supervised learning, it is possible\nto obtain regrets scaling as O(log(T )) with a prob-\nlem dependent constant (Bartlett et al., 2007).\nWhen the feedback is delayed by \u03c4 rounds, lower\nbounds imply that the regret in the adversarial\nsetting increases by a multiplicative \u221a\u03c4 while in\nthe i.i.d. setting, it is possible to achieve an addi-\ntive regret of \u03c4 (Langford et al., 2009).\nIn\na\ndirect\ni.i.d.\nsetting,\nthe\nprevious-best\nap-\nproach using\na\ncost-sensitive classi\ufb01cation oracle\nwas\ngiven\nby\n\u01eb-greedy\nand\nepoch\ngreedy\nalgo-\nrithms (Langford and Zhang, 2007) which have a re-\ngret scaling as O(T 2/3) in the worst case.\nThere have also been many special-case analyses. For\nexample, theory of context-free setting is well un-\nderstood (Lai and Robbins, 1985; Auer et al., 2002a;\nEven-Dar et al., 2006). Similarly, good algorithms ex-\nist when rewards are linear functions of features (Auer,\n2002) or actions lie in a continuous space with the re-\nward function sampled according to a Gaussian pro-\ncess (Srinivas et al., 2010).\n1.2\nWHAT WE PROVE\nIn Section 3 we state the PolicyElimination algo-\nrithm, and prove the following regret bound for it.\nTheorem 4. For all distributions D over (x,\u20d7r) with\nK actions, for all sets of N policies \u03a0, with probabil-\nity at least 1 \u2212\u03b4, the regret of PolicyElimination\n(Algorithm 1) over T rounds is at most\n16\nr\n2T K ln 4T 2N\n\u03b4\n.\nThis result can be extended to deal with VC classes,\nas well as other special cases. It forms the simplest\nmethod we have of exhibiting the new analysis.\nThe new key element of this algorithm is identi\ufb01ca-\ntion of a distribution over actions which simultane-\nously achieves small expected regret and allows esti-\nmating value of every policy with small variance. The\nexistence of such a distribution is shown nonconstruc-\ntively by a minimax argument.\nPolicyElimination is computationally intractable\nand also requires exact knowledge of the context dis-\ntribution (but not the reward distribution!). We show\nhow to address these issues in Section 4 using an algo-\nrithm we call RandomizedUCB. Namely, we prove\nthe following theorem.\nTheorem 5. For all distributions D over (x,\u20d7r) with\nK actions, for all sets of N policies \u03a0, with proba-\nbility at least 1 \u2212\u03b4, the regret of RandomizedUCB\n(Algorithm 2) over T rounds is at most\nO\n\u0010p\nT K log (T N/\u03b4) + K log(NK/\u03b4)\n\u0011\n.\nRandomizedUCB\u2019s analysis is substantially more\ncomplex,\nwith\na\nkey\nsubroutine\nbeing\nan\nap-\nplication of the ellipsoid algorithm with a cost-\nsensitive classi\ufb01cation oracle (described in Section 5).\nRandomizedUCB does not assume knowledge of the\ncontext distribution, and instead works with the his-\ntory of contexts it has observed.\nModifying the\nproof for this empirical distribution requires a cov-\nering argument over the distributions over policies\nwhich uses the probabilistic method. The net result\nis an algorithm with a similar top-level analysis as\nPolicyElimination, but with the running time only\npoly-logarithmic in the number of policies given a cost-\nsensitive classi\ufb01cation oracle.\nTheorem 11. In each time step t, RandomizedUCB\nmakes at most O(poly(t, K, log(1/\u03b4), log N)) calls to\ncost-sensitive classi\ufb01cation oracle, and requires addi-\ntional O(poly(t, K, log N)) processing time.\nApart from a tractable algorithm, our analysis can be\nused to derive tighter regrets than would be possible in\nadversarial setting. For example, in Section 6, we con-\nsider a common setting where reward feedback is de-\nlayed by \u03c4 rounds. A straightforward modi\ufb01cation of\nPolicyElimination yields a regret with an additive\nterm proportional to \u03c4 compared with the delay-free\nsetting. Namely, we prove the following.\nTheorem 12. For all distributions D over (x,\u20d7r) with\nK actions, for all sets of N policies \u03a0, and all delay\nintervals \u03c4, with probability at least 1 \u2212\u03b4, the regret\nof DelayedPE (Algorithm 3) is at most\n16\nr\n2K ln 4T 2N\n\u03b4\n\u0010\n\u03c4 +\n\u221a\nT\n\u0011\n.\nWe start next with precise settings and de\ufb01nitions.\n2\nSETTING AND DEFINITIONS\n2.1\nTHE SETTING\nLet A be the set of K actions, let X be the domain of\ncontexts x, and let D be an arbitrary joint distribution\non (x,\u20d7r). We denote the marginal distribution of D\nover X by DX.\nWe denote \u03a0 to be a \ufb01nite set of policies {\u03c0 : X \u2192A},\nwhere each policy \u03c0, given a context xt in round t,\nchooses the action \u03c0(xt). The cardinality of \u03a0 is de-\nnoted by N. Let \u20d7rt \u2208[0, 1]K be the vector of rewards,\nwhere rt(a) is the reward of action a on round t.\nIn the i.i.d. setting, on each round t = 1 . . . T , the\nworld chooses (xt,\u20d7rt) i.i.d. according to D and reveals\nxt to the learner.\nThe learner, having access to \u03a0,\nchooses action at \u2208{1, . . ., K}. Then the world reveals\nreward rt(at) (which we call rt for short) to the learner,\nand the interaction proceeds to the next round.\nWe consider two modes of accessing the set of policies\n\u03a0. The \ufb01rst option is through the enumeration of all\npolicies.\nThis is impractical in general, but su\ufb03ces\nfor the illustrative purpose of our \ufb01rst algorithm. The\nsecond option is an oracle access, through an argmax\noracle, corresponding to a cost-sensitive learner:\nDe\ufb01nition 1. For a set of policies \u03a0, an argmax or-\nacle (AMO for short), is an algorithm, which for any\nsequence {(xt\u2032,\u20d7rt\u2032)}t\u2032=1...t, xt\u2032 \u2208X, \u20d7rt\u2032 \u2208RK, com-\nputes\narg max\n\u03c0\u2208\u03a0\nX\nt\u2032=1...t\nrt\u2032(\u03c0(xt\u2032)) .\nThe reason why the above can be viewed as a cost-\nsensitive classi\ufb01cation oracle is that vectors of rewards\n\u20d7rt\u2032 can be interpreted as negative costs and hence the\npolicy returned by AMO is the optimal cost-sensitive\nclassi\ufb01er on the given data.\n2.2\nEXPECTED AND EMPIRICAL\nREWARDS\nLet the expected instantaneous reward of a policy \u03c0 \u2208\n\u03a0 be denoted by\n\u03b7D(\u03c0) .=\nE\n(x,\u20d7r)\u223cD[r(\u03c0(x))] .\nThe best policy \u03c0max \u2208\u03a0 is that which maximizes\n\u03b7D(\u03c0). More formally,\n\u03c0max .= argmax\n\u03c0\u2208\u03a0\n\u03b7D(\u03c0) .\nWe de\ufb01ne ht to be the history at time t that the learner\nhas seen. Speci\ufb01cally\nht =\n[\nt\u2032=1...t\n(xt\u2032, at\u2032, rt\u2032, pt\u2032) ,\nwhere pt\u2032 is the probability of the algorithm choosing\naction at\u2032 at time t\u2032. Note that at\u2032 and pt\u2032 are produced\nby the learner while xt\u2032, rt\u2032 are produced by nature.\nWe write x \u223ch to denote choosing x uniformly at\nrandom from the x\u2019s in history h.\nUsing the history of past actions and probabilities with\nwhich they were taken, we can form an unbiased esti-\nmate of the policy value for any \u03c0 \u2208\u03a0:\n\u03b7t(\u03c0) .= 1\nt\nX\n(x,a,r,p)\u2208ht\nrI(\u03c0(x) = a)\np\n.\nThe unbiasedness follows, because Ea\u223cp\nrI(\u03c0(x)=a)\np(a)\n=\nP\na p(a) rI(\u03c0(x)=a)\np(a)\n= r(\u03c0(x)).\nThe empirically best\npolicy at time t is denoted\n\u03c0t .= argmax\n\u03c0\u2208\u03a0\n\u03b7t(\u03c0).\n2.3\nREGRET\nThe goal of this work is to obtain a learner that has\nsmall regret relative to the expected performance of\n\u03c0max over T rounds, which is\nX\nt=1...T\n(\u03b7D(\u03c0max) \u2212rt) .\n(2.1)\nWe say that the regret of the learner over T rounds is\nbounded by \u01eb with probability at least 1 \u2212\u03b4, if\nPr\n\" X\nt=1...T\n(\u03b7D(\u03c0max) \u2212rt) \u2264\u01eb\n#\n\u22651 \u2212\u03b4\nwhere the probability is taken with respect to the ran-\ndom pairs (xt,\u20d7rt) \u223cD for t = 1 . . . T , as well as any\ninternal randomness used by the learner.\nWe can also de\ufb01ne notions of regret and empirical re-\ngret for policies \u03c0. For all \u03c0 \u2208\u03a0, let\n\u2206D(\u03c0) = \u03b7D(\u03c0max) \u2212\u03b7D(\u03c0) ,\n\u2206t(\u03c0) = \u03b7t(\u03c0t) \u2212\u03b7t(\u03c0) .\nOur algorithms work by choosing distributions over\npolicies, which in turn then induce distributions over\nactions.\nFor any distribution P over policies \u03a0, let\nWP (x, a) denote the induced conditional distribution\nover actions a given the context x:\nWP (x, a) .=\nX\n\u03c0\u2208\u03a0:\u03c0(x)=a\nP(\u03c0) .\n(2.2)\nIn general, we shall use W, W \u2032 and Z as conditional\nprobability distributions over the actions A given con-\ntexts X, i.e., W : X \u00d7A \u2192[0, 1] such that W(x, \u00b7) is a\nprobability distribution over A (and similarly for W \u2032\nand Z). We shall think of W \u2032 as a smoothed version\nof W with a minimum action probability of \u00b5 (to be\nde\ufb01ned by the algorithm), such that\nW \u2032(x, a) = (1 \u2212K\u00b5)W(x, a) + \u00b5 .\nConditional distributions such as W (and W \u2032, Z, etc.)\ncorrespond to randomized policies. We de\ufb01ne notions\ntrue and empirical value and regret for them as follows:\n\u03b7D(W) .=\nE\n(x,\u20d7r)\u223cD[\u20d7r \u00b7 W(x)]\n\u03b7t(W) .= 1\nt\nX\n(x,a,r,p)\u2208ht\nrW(x, a)\np\n\u2206D(W) .= \u03b7D(\u03c0max) \u2212\u03b7D(W)\n\u2206t(W) .= \u03b7t(\u03c0t) \u2212\u03b7t(W) .\n3\nPOLICY ELIMINATION\nThe basic ideas behind our approach are demonstrated\nin our \ufb01rst algorithm:\nPolicyElimination (Algo-\nrithm 1).\nThe key step is Step 1, which \ufb01nds a distribution over\npolicies which induces low variance in the estimate of\nthe value of all policies. Below we use minimax the-\norem to show that such a distribution always exists.\nHow to \ufb01nd this distribution is not speci\ufb01ed here, but\nin Section 5 we develop a method based on the ellip-\nsoid algorithm. Step 2 then projects this distribution\nonto a distribution over actions and applies smoothing.\nFinally, Step 5 eliminates the policies that have been\ndetermined to be suboptimal (with high probability).\nALGORITHM ANALYSIS\nWe analyze PolicyElimination in several steps.\nFirst, we prove the existence of Pt in Step 1, provided\nAlgorithm 1 PolicyElimination(\u03a0,\u03b4,K,DX)\nLet \u03a00 = \u03a0 and history h0 = \u2205\nDe\ufb01ne: \u03b4t .= \u03b4 / 4Nt2\nDe\ufb01ne: bt .= 2\nr\n2K ln(1/\u03b4t)\nt\nDe\ufb01ne: \u00b5t .= min\n(\n1\n2K ,\nr\nln(1/\u03b4t)\n2Kt\n)\nFor each timestep t = 1 . . . T , observe xt and do:\n1. Choose distribution Pt over \u03a0t\u22121 s.t. \u2200\u03c0 \u2208\u03a0t\u22121:\nE\nx\u223cDX\n\u0014\n1\n(1 \u2212K\u00b5t)WPt(x, \u03c0(x)) + \u00b5t\n\u0015\n\u22642K\n2. Let W \u2032\nt(a) = (1\u2212K\u00b5t)WPt(xt, a)+\u00b5t for all a \u2208A\n3. Choose at \u223cW \u2032\nt\n4. Observe reward rt\n5. Let \u03a0t =\nn\n\u03c0 \u2208\u03a0t\u22121 :\n\u03b7t(\u03c0) \u2265\n\u0010\nmax\n\u03c0\u2032\u2208\u03a0t\u22121\u03b7t(\u03c0\u2032)\n\u0011\n\u22122bt\no\n6. Let ht = ht\u22121 \u222a(xt, at, rt, W \u2032\nt(at))\nthat \u03a0t\u22121 is non-empty. We recast the feasibility prob-\nlem in Step 1 as a game between two players: Prover,\nwho is trying to produce Pt, and Falsi\ufb01er, who is try-\ning to \ufb01nd \u03c0 violating the constraints. We give more\npower to Falsi\ufb01er and allow him to choose a distribu-\ntion over \u03c0 (i.e., a randomized policy) which would\nviolate the constraints.\nNote that any policy \u03c0 corresponds to a point in\nthe space of randomized policies (viewed as functions\nX \u00d7 A \u2192[0, 1]), with \u03c0(x, a) .= I(\u03c0(x) = a).\nFor\nany distribution P over policies in \u03a0t\u22121, the induced\nrandomized policy WP then corresponds to a point in\nthe convex hull of \u03a0t\u22121. Denoting the convex hull of\n\u03a0t\u22121 by C, Prover\u2019s choice by W and Falsi\ufb01er\u2019s choice\nby Z, the feasibility of Step 1 follows by the following\nlemma:\nLemma 1. Let C be a compact and convex set of ran-\ndomized policies. Let \u00b5 \u2208(0, 1/K] and for any W \u2208C,\nW \u2032(x, a) .= (1 \u2212K\u00b5)W(x, a) + \u00b5. Then for all distri-\nbutions D,\nmin\nW\u2208C max\nZ\u2208C\nE\nx\u223cDX\nE\na\u223cZ(x,\u00b7)\n\u0014\n1\nW \u2032(x, a)\n\u0015\n\u2264\nK\n1 \u2212K\u00b5 .\nProof. Let f(W, Z)\n.= Ex\u223cDX Ea\u223cZ(x,\u00b7)[1/W \u2032(x, a)]\ndenote the inner expression of the minimax problem.\nNote that f(W, Z) is:\n\u2022 everywhere de\ufb01ned: Since W \u2032(x, a) \u2265\u00b5, we ob-\ntain that 1/W \u2032(x, a) \u2208[0, 1/\u00b5], hence the expec-\ntations are de\ufb01ned for all W and Z.\n\u2022 linear in Z:\nLinearity follows from rewriting\nf(W, Z) as\nf(W, Z) =\nE\nx\u223cDX\nX\na\u2208A\n\u0014 Z(x, a)\nW \u2032(x, a)\n\u0015\n.\n\u2022 convex in W: Note that 1/W \u2032(x, a) is convex in\nW(x, a) by convexity of 1/(c1w+c2) in w \u22650, for\nc1 \u22650, c2 > 0. Convexity of f(W, Z) in W then\nfollows by taking expectations over x and a.\nHence, by Theorem 14 (in Appendix B), min and max\ncan be reversed without a\ufb00ecting the value:\nmin\nW\u2208C max\nZ\u2208C f(W, Z) = max\nZ\u2208C min\nW\u2208C f(W, Z) .\nThe right-hand side can be further upper-bounded by\nmaxZ\u2208C f(Z, Z), which is upper-bounded by\nf(Z, Z) =\nE\nx\u223cDX\nX\na\u2208A\n\u0014 Z(x, a)\nZ\u2032(x, a)\n\u0015\n\u2264\nE\nx\u223cDX\nX\na\u2208A:\nZ(x,a)>0\n\u0014\nZ(x, a)\n(1 \u2212K\u00b5)Z(x, a)\n\u0015\n=\nK\n1 \u2212K\u00b5 .\nCorollary 2. The set of distributions satisfying con-\nstraints of Step 1 is non-empty.\nGiven the existence of Pt, we will see below that the\nconstraints in Step 1 ensure low variance of the policy\nvalue estimator \u03b7t(\u03c0) for all \u03c0 \u2208\u03a0t\u22121. The small vari-\nance is used to ensure accuracy of policy elimination\nin Step 5 as quanti\ufb01ed in the following lemma:\nLemma 3. With probability at least 1 \u2212\u03b4, for all t:\n1. \u03c0max \u2208\u03a0t (i.e., \u03a0t is non-empty)\n2. \u03b7D(\u03c0max) \u2212\u03b7D(\u03c0) \u22644bt for all \u03c0 \u2208\u03a0t\nProof. We will show that for any policy \u03c0 \u2208\u03a0t\u22121, the\nprobability that \u03b7t(\u03c0) deviates from \u03b7D(\u03c0) by more\nthat bt is at most 2\u03b4t. Taking the union bound over all\npolicies and all time steps we \ufb01nd that with probability\nat least 1 \u2212\u03b4,\n|\u03b7t(\u03c0) \u2212\u03b7D(\u03c0)| \u2264bt\n(3.1)\nfor all t and all \u03c0 \u2208\u03a0t\u22121. Then:\n1. By the triangle inequality, in each time step,\n\u03b7t(\u03c0) \u2264\u03b7t(\u03c0max) + 2bt for all \u03c0 \u2208\u03a0t\u22121, yield-\ning the \ufb01rst part of the lemma.\n2. Also by the triangle inequality,\nif \u03b7D(\u03c0)\n<\n\u03b7D(\u03c0max) \u22124bt for \u03c0 \u2208\u03a0t\u22121, then \u03b7t(\u03c0) <\n\u03b7t(\u03c0max) \u22122bt. Hence the policy \u03c0 is eliminated\nin Step 5, yielding the second part of the lemma.\nIt remains to show Eq. (3.1). We \ufb01x the policy \u03c0 \u2208\u03a0\nand time t, and show that the deviation bound is vi-\nolated with probability at most 2\u03b4t.\nOur argument\nrests on Freedman\u2019s inequality (see Theorem 13 in Ap-\npendix A). Let\nyt = rtI(\u03c0(xt) = at)\nW \u2032\nt(at)\n,\ni.e., \u03b7t(\u03c0) = (Pt\nt\u2032=1 yt\u2032)/t.\nLet Et denote the con-\nditional expectation E[ \u00b7 | ht\u22121].\nTo use Freedman\u2019s\ninequality, we need to bound the range of yt and its\nconditional second moment Et[y2\nt ].\nSince rt \u2208[0, 1] and W \u2032\nt(at) \u2265\u00b5t, we have the bound\n0 \u2264yt \u22641/\u00b5t .= Rt .\nNext,\nEt[y2\nt ] =\nE\n(xt,\u20d7rt)\u223cD\nE\nat\u223cW \u2032\nt\n\u0002\ny2\nt\n\u0003\n=\nE\n(xt,\u20d7rt)\u223cD\nE\nat\u223cW \u2032\nt\n\u0014r2\nt I(\u03c0(xt) = at)\nW \u2032\nt(at)2\n\u0015\n\u2264\nE\n(xt,\u20d7rt)\u223cD\n\u0014 W \u2032\nt(\u03c0(xt))\nW \u2032\nt(\u03c0(xt))2\n\u0015\n(3.2)\n=\nE\nxt\u223cD\n\u0014\n1\nW \u2032\nt(\u03c0(xt))\n\u0015\n\u22642K .\n(3.3)\nwhere Eq. (3.2) follows by boundedness of rt and\nEq. (3.3) follows from the constraints in Step 1. Hence,\nX\nt\u2032=1...t\nEt\u2032[y2\nt\u2032] \u22642Kt .= Vt .\nSince (ln t)/t is decreasing for t \u22653, we obtain that \u00b5t\nis non-increasing (by separately analyzing t = 1, t = 2,\nt \u22653). Let t0 be the \ufb01rst t such that \u00b5t < 1/2K.\nNote that bt \u22654K\u00b5t, so for t < t0, we have bt \u22652 and\n\u03a0t = \u03a0. Hence, the deviation bound holds for t < t0.\nLet t \u2265t0. For t\u2032 \u2264t, by the monotonicity of \u00b5t\nRt\u2032 = 1/\u00b5t\u2032 \u22641/\u00b5t =\ns\n2Kt\nln(1/\u03b4t) =\ns\nVt\nln(1/\u03b4t) .\nHence, the assumptions of Theorem 13 are satis\ufb01ed,\nand\nPr [|\u03b7t(\u03c0) \u2212\u03b7D(\u03c0)| \u2265bt] \u22642\u03b4t .\nThe union bound over \u03c0 and t yields Eq. (3.1).\nThis immediately implies that the cumulative regret is\nbounded by\nX\nt=1...T\n(\u03b7D(\u03c0max) \u2212rt)\n\u2264\n8\nr\n2K ln 4NT 2\n\u03b4\nT\nX\nt=1\n1\n\u221a\nt\n\u2264\n16\nr\n2T K ln 4T 2N\n\u03b4\n(3.4)\nand gives us the following theorem.\nTheorem 4. For all distributions D over (x,\u20d7r) with\nK actions, for all sets of N policies \u03a0, with probabil-\nity at least 1 \u2212\u03b4, the regret of PolicyElimination\n(Algorithm 1) over T rounds is at most\n16\nr\n2T K ln 4T 2N\n\u03b4\n.\n4\nTHE RANDOMIZED UCB\nALGORITHM\nPolicyElimination is the simplest exhibition of the\nminimax argument, but it has some drawbacks:\n1. The algorithm keeps explicit track of the space\nof good policies (like a version space), which is\ndi\ufb03cult to implement e\ufb03ciently in general.\n2. If the optimal policy is mistakenly eliminated by\nchance, the algorithm can never recover.\n3. The algorithm requires perfect knowledge of the\ndistribution DX over contexts.\nThese di\ufb03culties are addressed by RandomizedUCB\n(or RUCB for short), an algorithm which we present\nand analyze in this section. Our approach is reminis-\ncent of the UCB algorithm (Auer et al., 2002a), devel-\noped for context-free setting, which keeps an upper-\ncon\ufb01dence bound on the expected reward for each ac-\ntion. However, instead of choosing the highest upper\ncon\ufb01dence bound, we randomize over choices accord-\ning to the value of their empirical performance. The\nalgorithm has the following properties:\n1. The optimization step required by the algorithm\nalways considers the full set of policies (i.e.,\nexplicit tracking of the set of good policies is\navoided), and thus it can be e\ufb03ciently imple-\nmented using an argmax oracle. We discuss this\nfurther in Section 5.\n2. Suboptimal policies are implicitly used with de-\ncreasing frequency by using a non-uniform vari-\nance constraint that depends on a policy\u2019s esti-\nmated regret. A consequence of this is a bound on\nthe value of the optimization, stated in Lemma 7\nbelow.\nAlgorithm 2 RandomizedUCB(\u03a0,\u03b4,K)\nLet h0 .= \u2205be the initial history.\nDe\ufb01ne the following quantities:\nCt .= 2 log\n\u0012Nt\n\u03b4\n\u0013\nand\n\u00b5t .= min\n(\n1\n2K ,\nr\nCt\n2Kt\n)\n.\nFor each timestep t = 1 . . . T , observe xt and do:\n1. Let Pt be a distribution over \u03a0 that approxi-\nmately solves the optimization problem\nmin\nP\nX\n\u03c0\u2208\u03a0\nP(\u03c0)\u2206t\u22121(\u03c0)\ns.t.\nfor all distributions Q over \u03a0 :\nE\n\u03c0\u223cQ\n\"\n1\nt \u22121\nt\u22121\nX\ni=1\n1\n(1 \u2212K\u00b5t)WP (xi, \u03c0(xi)) + \u00b5t\n#\n\u2264max\n\u001a\n4K, (t \u22121)\u2206t\u22121(WQ)2\n180Ct\u22121\n\u001b\n(4.1)\nso that the objective value at Pt is within \u03b5opt,t =\nO(\np\nKCt/t) of the optimal value, and so that\neach constraint is satis\ufb01ed with slack \u2264K.\n2. Let W \u2032\nt be the distribution over A given by\nW \u2032\nt(a) .= (1 \u2212K\u00b5t)WPt(xt, a) + \u00b5t\nfor all a \u2208A.\n3. Choose at \u223cW \u2032\nt.\n4. Observe reward rt.\n5. Let ht .= ht\u22121 \u222a(xt, at, rt, W \u2032\nt(at)).\n3. Instead of DX, the algorithm uses the history of\npreviously seen contexts. The e\ufb00ect of this ap-\nproximation is quanti\ufb01ed in Theorem 6 below.\nThe regret of RandomizedUCB is the following:\nTheorem 5. For all distributions D over (x,\u20d7r) with\nK actions, for all sets of N policies \u03a0, with proba-\nbility at least 1 \u2212\u03b4, the regret of RandomizedUCB\n(Algorithm 2) over T rounds is at most\nO\n\u0010p\nT K log (T N/\u03b4) + K log(NK/\u03b4)\n\u0011\n.\nThe proof is given in Appendix D.4. Here, we present\nan overview of the analysis.\n4.1\nEMPIRICAL VARIANCE ESTIMATES\nA key technical prerequisite for the regret analysis is\nthe accuracy of the empirical variance estimates. For\na distribution P over policies \u03a0 and a particular policy\n\u03c0 \u2208\u03a0, de\ufb01ne\nVP,\u03c0,t =\nE\nx\u223cDX\n\u0014\n1\n(1 \u2212K\u00b5t)WP (x, \u03c0(x)) + \u00b5t\n\u0015\nbVP,\u03c0,t =\n1\nt \u22121\nt\u22121\nX\ni=1\n1\n(1 \u2212K\u00b5t)WP (xi, \u03c0(xi)) + \u00b5t\n.\nThe \ufb01rst quantity VP,\u03c0,t is (a bound on) the vari-\nance incurred by an importance-weighted estimate of\nreward in round t using the action distribution in-\nduced by P, and the second quantity bVP,\u03c0,t is an\nempirical estimate of VP,\u03c0,t using the \ufb01nite sample\n{x1, . . . , xt\u22121} \u2286X drawn from DX. We show that\nfor all distributions P and all \u03c0 \u2208\u03a0, bVP,\u03c0,t is close to\nVP,\u03c0,t with high probability.\nTheorem 6. For any \u01eb \u2208(0, 1), with probability at\nleast 1 \u2212\u03b4,\nVP,\u03c0,t \u2264(1 + \u01eb) \u00b7 bVP,\u03c0,t + 7500\n\u01eb3\n\u00b7 K\nfor all distributions P over \u03a0, all \u03c0 \u2208\u03a0, and all t \u2265\n16K log(8KN/\u03b4).\nThe proof appears in Appendix C.\n4.2\nREGRET ANALYSIS\nCentral to the analysis is the following lemma that\nbounds the value of the optimization in each round. It\nis a direct corollary of Lemma 24 in Appendix D.4.\nLemma 7. If OPTt is the value of the optimization\nproblem (4.1) in round t, then\nOPTt \u2264O\n r\nKCt\u22121\nt \u22121\n!\n= O\n r\nK log(Nt/\u03b4)\nt\n!\n.\nThis lemma implies that the algorithm is always able\nto select a distribution over the policies that focuses\nmostly on the policies with low estimated regret.\nMoreover, the variance constraints ensure that good\npolicies never appear too bad, and that only bad poli-\ncies are allowed to incur high variance in their reward\nestimates. Hence, minimizing the objective in (4.1) is\nan e\ufb00ective surrogate for minimizing regret.\nThe bulk of the analysis consists of analyzing the\nvariance of the importance-weighted reward estimates\n\u03b7t(\u03c0), and showing how they relate to their actual ex-\npected rewards \u03b7D(\u03c0). The details are deferred to Ap-\npendix D.\n5\nUSING AN ARGMAX ORACLE\nIn this section, we show how to solve the optimization\nproblem (4.1) using the argmax oracle (AMO) for our\nset of policies. Namely, we describe an algorithm run-\nning in polynomial time independent1 of the number of\npolicies, which makes queries to AMO to compute a\ndistribution over policies suitable for the optimization\nstep of Algorithm 2.\nThis algorithm relies on the ellipsoid method. The el-\nlipsoid method is a general technique for solving con-\nvex programs equipped with a separation oracle.\nA\nseparation oracle is de\ufb01ned as follows:\nDe\ufb01nition 2. Let S be a convex set in Rn. A sepa-\nration oracle for S is an algorithm that, given a point\nx \u2208Rn, either declares correctly that x \u2208S, or pro-\nduces a hyperplane H such that x and S are on oppo-\nsite sides of H.\nWe do not describe the ellipsoid algorithm here (since\nit is standard), but only spell out its key properties in\nthe following lemma. For a point x \u2208Rn and r \u22650,\nwe use the notation B(x, r) to denote the \u21132 ball of\nradius r centered at x.\nLemma 8. Suppose we are required to decide whether\na convex set S \u2286Rn is empty or not. We are given\na separation oracle for S and two numbers R and r,\nsuch that S \u2208B(0, R) and if S is non-empty, then\nthere is a point x\u22c6such that S \u2287B(x\u22c6, r). The ellip-\nsoid algorithm decides correctly if S is empty or not,\nby executing at most O(n2 log( R\nr )) iterations, each in-\nvolving one call to the separation oracle and additional\nO(n2) processing time.\nWe now write a convex program whose solution is the\nrequired distribution, and show how to solve it using\nthe ellipsoid method by giving a separation oracle for\nits feasible set using AMO.\nFix a time period t. Let Xt\u22121 be the set of all con-\ntexts seen so far, i.e. Xt\u22121 = {x1, x2, . . . , xt\u22121}. We\nembed all policies \u03c0 \u2208\u03a0 in R(t\u22121)K, with coordinates\nidenti\ufb01ed with (x, a) \u2208Xt\u22121 \u00d7 A. With abuse of no-\ntation, a policy \u03c0 is represented by the vector \u03c0 with\ncoordinate \u03c0(x, a) = 1 if \u03c0(x) = a and 0 otherwise.\nLet C be the convex hull of all policy vectors \u03c0. Re-\ncall that a distribution P over policies corresponds to\na point inside C, i.e., WP (x, a) = P\n\u03c0:\u03c0(x)=a P(\u03c0), and\nthat W \u2032(x, a) = (1 \u2212\u00b5tK)W(x, a) + \u00b5t, where \u00b5t is as\nde\ufb01ned in Algorithm 2. Also de\ufb01ne \u03b2t =\nt\u22121\n180Ct\u22121 . In\nthe following, we use the notation x \u223cht\u22121 to denote\na context drawn uniformly at random from Xt\u22121.\n1Or rather dependent only on log N, the representation\nsize of a policy.\nConsider the following convex program:\nmin s s.t.\n\u2206t\u22121(W) \u2264s\n(5.1)\nW \u2208C\n(5.2)\n\u2200Z \u2208C :\nE\nx\u223cht\u22121\n\"X\na\nZ(x, a)\nW \u2032(x, a)\n#\n\u2264max{4K, \u03b2t\u2206t\u22121(Z)2} (5.3)\nWe claim that this program is equivalent to the RUCB\noptimization problem (4.1), up to \ufb01nding an explicit\ndistribution over policies which corresponds to the op-\ntimal solution. This can be seen as follows. Since we\nrequire W \u2208C, it can be interpreted as being equal\nto WP for some distribution over policies P. The con-\nstraints (5.3) are equivalent to (4.1) by substitution\nZ = WQ.\nThe above convex program can be solved by perform-\ning a binary search over s and testing feasibility of\nthe constraints. For a \ufb01xed value of s, the feasibility\nproblem de\ufb01ned by (5.1)\u2013(5.3) is denoted by A.\nWe now give a sketch of how we construct a separa-\ntion oracle for the feasible region of A. The details\nof the algorithm are a bit complicated due to the fact\nthat we need to ensure that the feasible region, when\nnon-empty, has a non-negligible volume (recall the re-\nquirements of Lemma 8). This necessitates having a\nsmall error in satisfying the constraints of the program.\nWe leave the details to Appendix E. Modulo these de-\ntails, the construction of the separation oracle essen-\ntially implies that we can solve A.\nBefore giving the construction of the separation ora-\ncle, we \ufb01rst show that AMO allows us to do linear\noptimization over C e\ufb03ciently:\nLemma 9. Given a vector w \u2208R(t\u22121)K, we can com-\npute arg maxZ\u2208C w \u00b7 Z using one invocation of AMO.\nProof. The sequence for AMO consists of xt\u2032 \u2208Xt\u22121\nand \u20d7rt\u2032(a) = w(xt\u2032, a). The lemma now follows since\nw \u00b7 \u03c0 = P\nx\u2208Xt\u22121 w(x, \u03c0(x)).\nWe need another simple technical lemma which ex-\nplains how to get a separating hyperplane for viola-\ntions of convex constraints:\nLemma 10. For x \u2208Rn, let f(x) be a convex function\nof x, and consider the convex set K de\ufb01ned by K =\n{x : f(x) \u22640}. Suppose we have a point y such that\nf(y) > 0. Let \u2207f(y) be a subgradient of f at y. Then\nthe hyperplane f(y) + \u2207f(y) \u00b7 (x \u2212y) = 0 separates y\nfrom K.\nProof. Let g(x) = f(y) + \u2207f(y) \u00b7 (x \u2212y).\nBy the\nconvexity of f, we have f(x) \u2265g(x) for all x. Thus,\nfor any x \u2208K, we have g(x) \u2264f(x) \u22640.\nSince\ng(y) = f(y) > 0, we conclude that g(x) = 0 separates\ny from K.\nNow given a candidate point W, a separation oracle\ncan be constructed as follows. We check whether W\nsatis\ufb01es the constraints of A. If any constraint is vi-\nolated, then we \ufb01nd a hyperplane separating W from\nall points satisfying the constraint.\n1. First, for constraint (5.1), note that \u03b7t\u22121(W) is\nlinear in W, and so we can compute max\u03c0 \u03b7t\u22121(\u03c0)\nvia AMO as in Lemma 9. We can then compute\n\u03b7t\u22121(W) and check if the constraint is satis\ufb01ed. If\nnot, then the constraint, being linear, automati-\ncally yields a separating hyperplane.\n2. Next, we consider constraint (5.2).\nTo check if\nW \u2208C, we use the perceptron algorithm.\nWe\nshift the origin to W, and run the perceptron al-\ngorithm with all points \u03c0 \u2208\u03a0 being positive ex-\namples. The perceptron algorithm aims to \ufb01nd a\nhyperplane putting all policies \u03c0 \u2208\u03a0 on one side.\nIn each iteration of the perceptron algorithm, we\nhave a candidate hyperplane (speci\ufb01ed by its nor-\nmal vector), and then if there is a policy \u03c0 that is\non the wrong side of the hyperplane, we can \ufb01nd\nit by running a linear optimization over C in the\nnegative normal vector direction as in Lemma 9.\nIf W /\u2208C, then in a bounded number of iterations\n(depending on the distance of W from C, and the\nmaximum magnitude \u2225\u03c0\u22252) we obtain a separat-\ning hyperplane. In passing we also note that if\nW \u2208C, the same technique allows us to explic-\nitly compute an approximate convex combination\nof policies in \u03a0 that yields W. This is done by\nrunning the perceptron algorithm as before and\nstopping after the bound on the number of iter-\nations has been reached. Then we collect all the\npolicies we have found in the run of the percep-\ntron algorithm, and we are guaranteed that W is\nclose in distance to their convex hull. We can then\n\ufb01nd the closest point in the convex hull of these\npolicies by solving a simple quadratic program.\n3. Finally, we consider constraint (5.3). We rewrite\n\u03b7t\u22121(W) as \u03b7t\u22121(W) = w \u00b7 W, where w(xt\u2032, a) =\nrt\u2032I(a = at\u2032)/W \u2032\nt\u2032(at\u2032). Thus, \u2206t\u22121(Z) = v\u2212w\u00b7Z,\nwhere v = max\u03c0\u2032 \u03b7t\u22121(\u03c0\u2032) = max\u03c0\u2032 w \u00b7 \u03c0\u2032, which\ncan be computed by using AMO once.\nNext, using the candidate point W, compute the\nvector u de\ufb01ned as u(x, a) =\nnx/t\nW \u2032(x,a), where nx\nis the number of times x appears in ht\u22121, so that\nEx\u223cht\u22121\nhP\na\nZ(x,a)\nW \u2032(x,a)\ni\n= u \u00b7 Z. Now, the problem\nreduces to \ufb01nding a policy Z \u2208C which violates\nthe constraint\nu \u00b7 Z \u2264max{4K, \u03b2t(w \u00b7 Z \u2212v)2}.\nDe\ufb01ne f(Z) = max{4K, \u03b2t(w\u00b7Z\u2212v)2}\u2212u\u00b7Z. Note\nthat f is a convex function of Z. Finding a point\nZ that violates the above constraint is equivalent\nto solving the following (convex) program:\nf(Z) \u22640\n(5.4)\nZ \u2208C\n(5.5)\nTo do this, we again apply the ellipsoid method.\nFor this, we need a separation oracle for the pro-\ngram. A separation oracle for the constraints (5.5)\ncan be constructed as in Step 2 above. For the\nconstraints (5.4), if the candidate solution Z has\nf(Z) > 0, then we can construct a separating hy-\nperplane as in Lemma 10.\nSuppose that after solving the program, we get\na point Z \u2208C such that f(Z) \u22640, i.e. W vio-\nlates the constraint (5.3) for Z. Then since con-\nstraint (5.3) is convex in W, we can construct a\nseparating hyperplane as in Lemma 10. This com-\npletes the description of the separation oracle.\nWorking out the details carefully yields the following\ntheorem, proved in Appendix E:\nTheorem 11. There is an iterative algorithm with\nO(t5K4 log2( tK\n\u03b4 )) iterations, each involving one call to\nAMO and O(t2K2) processing time, that either de-\nclares correctly that A is infeasible or outputs a distri-\nbution P over policies in \u03a0 such that WP satis\ufb01es\n\u2200Z \u2208C :\nE\nx\u223cht\u22121\n\"X\na\nZ(x, a)\nW \u2032\nP (x, a)\n#\n\u2264max{4K, \u03b2t\u2206t\u22121(Z)2} + 5\u01eb\n\u2206t\u22121(W) \u2264s + 2\u03b3,\nwhere \u01eb = 8\u03b4\n\u00b52\nt and \u03b3 =\n\u03b4\n\u00b5t .\n6\nDELAYED FEEDBACK\nIn a delayed feedback setting, we observe rewards with\na \u03c4 step delay according to:\n1. The world presents features xt.\n2. The learning algorithm chooses an action at \u2208\n{1, ..., K}.\n3. The world presents a reward rt\u2212\u03c4 for the action\nat\u2212\u03c4 given the features xt\u2212\u03c4.\nAlgorithm 3 DelayedPE(\u03a0,\u03b4,K,DX,\u03c4)\nLet \u03a00 = \u03a0 and history h0 = \u2205\nDe\ufb01ne: \u03b4t .= \u03b4 / 4Nt2 and bt .= 2\nr\n2K ln(1/\u03b4t)\nt\nDe\ufb01ne: \u00b5t .= min\n(\n1\n2K ,\nr\nln(1/\u03b4t)\n2Kt\n)\nFor each timestep t = 1 . . . T , observe xt and do:\n1. Let t\u2032 = max(t \u2212\u03c4, 1).\n2. Choose distribution Pt over \u03a0t\u22121 s.t. \u2200\u03c0 \u2208\u03a0t\u22121:\nE\nx\u223cDX\n\u0014\n1\n(1 \u2212K\u00b5t\u2032)WPt(x, \u03c0(x)) + \u00b5t\u2032\n\u0015\n\u22642K\n3. \u2200a \u2208A, Let W \u2032\nt(a) = (1 \u2212K\u00b5t\u2032)WPt(xt, a) + \u00b5t\u2032\n4. Choose at \u223cW \u2032\nt\n5. Observe reward rt.\n6. Let \u03a0t =\nn\n\u03c0 \u2208\u03a0t\u22121 :\n\u03b7h(\u03c0) \u2265\n\u0010\nmax\n\u03c0\u2032\u2208\u03a0t\u22121\u03b7h(\u03c0\u2032)\n\u0011\n\u22122bt\u2032\no\n7. Let ht = ht\u22121 \u222a(xt, at, rt, W \u2032\nt(at))\nWe deal with delay by suitably modifying Algorithm 1\nto incorporate the delay \u03c4, giving Algorithm 3.\nNow we can prove the following theorem, which shows\nthe delay has an additive e\ufb00ect on regret.\nTheorem 12. For all distributions D over (x,\u20d7r) with\nK actions, for all sets of N policies \u03a0, and all delay\nintervals \u03c4, with probability at least 1 \u2212\u03b4, the regret of\nDelayedPE (Algorithm 3) is at most\n16\nr\n2K ln 4T 2N\n\u03b4\n\u0010\n\u03c4 +\n\u221a\nT\n\u0011\n.\nProof. Essentially as Theorem 4. The variance bound\nis unchanged because it depends only on the context\ndistribution. Thus, it su\ufb03ces to replace PT\nt\u22121\n1\n\u221a\nt with\n\u03c4 + PT +\u03c4\nt=\u03c4+1\n1\n\u221at\u2212\u03c4 = \u03c4 + PT\nt=1\n1\n\u221a\nt in Eq. (3.4).\nAcknowledgements\nWe thank Alina Beygelzimer, who helped in several\nformative discussions.\nReferences\nPeter Auer.\nUsing con\ufb01dence bounds for exploitation-\nexploration trade-o\ufb00s. Journal of Machine Learning Re-\nsearch, 3:397\u2013422, 2002.\nPeter Auer, Nicol\u00f2 Cesa-Bianchi, and Paul Fischer. Finite-\ntime analysis of the multiarmed bandit problem. Ma-\nchine Learning, 47(2\u20133):235\u2013256, 2002a.\nPeter Auer,\nNicol\u00f2 Cesa-Bianchi,\nYoav Freund,\nand\nRobert E. Schapire. The nonstochastic multiarmed ban-\ndit problem. SIAM Journal of Computing, 32(1):48\u201377,\n2002b.\nP. L. Bartlett, E. Hazan, and A. Rakhlin. Adaptive online\ngradient descent. In NIPS, 2007.\nAlina Beygelzimer, John Langford, and Pradeep Raviku-\nmar. Error correcting tournaments. In ALT, 2009.\nAlina Beygelzimer, John Langford, Lihong Li, Lev Reyzin,\nand Robert E. Schapire. Contextual bandit algorithms\nwith supervised learning guarantees. In AISTATS, 2011.\nEyal Even-Dar, Shie Mannor, and Yishay Mansour. Action\nelimination and stopping conditions for the multi-armed\nbandit and reinforcement learning problems. Journal of\nMachine Learning Research, 7:1079\u20131105, 2006.\nDavid A. Freedman. On tail probabilities for martingales.\nAnnals of Probability, 3(1):100\u2013118, 1975.\nY. Freund and R. E. Schapire. A decision-theoretic gener-\nalization of on-line learning and an application to boost-\ning. Journal of Computer and System Sciences, 55(1):\n119\u2013139, 1997.\nSham M. Kakade and Adam Kalai. From batch to trans-\nductive online learning. In NIPS, 2005.\nAdam Tauman Kalai and Santosh Vempala. E\ufb03cient al-\ngorithms for online decision problems. J. Comput. Syst.\nSci., 71(3):291\u2013307, 2005.\nTze Leung Lai and Herbert Robbins. Asymptotically ef-\n\ufb01cient adaptive allocation rules.\nAdvances in Applied\nMathematics, 6:4\u201322, 1985.\nJ. Langford, A. Smola, and M. Zinkevich. Slow learners\nare fast. In NIPS, 2009.\nJohn Langford and Tong Zhang.\nThe epoch-greedy al-\ngorithm for contextual multi-armed bandits. In NIPS,\n2007.\nMaurice Sion. On general minimax theorems. Paci\ufb01c J.\nMath., 8(1):171\u2013176, 1958.\nNiranjan Srinivas, Andreas Krause, Sham Kakade, and\nMatthias Seeger. Gaussian process optimization in the\nbandit setting: No regret and experimental design. In\nICML, 2010.\nA\nConcentration Inequality\nThe following is an immediate corollary of Theorem\n1 of (Beygelzimer et al., 2011). It can be viewed as\na version of Freedman\u2019s Inequality (Freedman, 1975).\nLet y1, . . . , yT be a sequence of real-valued random\nvariables. Let Et denote the conditional expectation\nE[ \u00b7 | y1, . . . , yt\u22121] and Vt conditional variance.\nTheorem 13 (Freedman-style Inequality). Let V, R \u2208\nR such that PT\nt=1 Vt[yt] \u2264V , and for all t, yt \u2212\nEt[yt] \u2264R.\nThen for any \u03b4 > 0 such that R \u2264\np\nV/ ln(2/\u03b4), with probability at least 1 \u2212\u03b4,\n\f\f\f\f\f\nT\nX\nt=1\nyt \u2212\nT\nX\nt=1\nEt[yt]\n\f\f\f\f\f \u22642\np\nV ln(2/\u03b4) .\nB\nMinimax Theorem\nThe following is a continuous version of Sion\u2019s Mini-\nmax Theorem (Sion, 1958, Theorem 3.4).\nTheorem 14. Let W and Z be compact and convex\nsets, and f : W \u00d7 Z \u2192R a function which for all\nZ \u2208Z is convex and continuous in W and for all\nW \u2208W is concave and continuous in Z. Then\nmin\nW\u2208W max\nZ\u2208Z f(W, Z) = max\nZ\u2208Z min\nW\u2208W f(W, Z) .\nC\nEmpirical Variance Bounds\nIn this section we prove Theorem 6. We \ufb01rst show uni-\nform convergence for a certain class of policy distribu-\ntions (Lemma 15), and argue that each distribution P\nis close to some distribution eP from this class, in the\nsense that VP,\u03c0,t is close to V e\nP ,\u03c0,t and bVP,\u03c0,t is close\nto bV e\nP ,\u03c0,t (Lemma 16). Together, they imply the main\nuniform convergence result in Theorem 6.\nFor each positive integer m, let Sparse[m] be the set of\ndistributions eP over \u03a0 that can be written as\neP(\u03c0) = 1\nm\nm\nX\ni=1\nI(\u03c0 = \u03c0i)\n(i.e., the average of m delta functions) for some\n\u03c01, . . . , \u03c0m \u2208\u03a0.\nIn our analysis, we approximate\nan arbitrary distribution P over \u03a0 by a distribution\neP\n\u2208Sparse[m] chosen randomly by independently\ndrawing \u03c01, . . . , \u03c0m \u223cP; we denote this process by\neP \u223cP m.\nLemma 15. Fix positive integers (m1, m2, . . . ). With\nprobability at least 1 \u2212\u03b4 over the random samples\n(x1, x2, . . . ) from DX,\nV e\nP ,\u03c0,t \u2264(1 + \u03bb) \u00b7 bV e\nP ,\u03c0,t\n+\n\u0012\n5 + 1\n2\u03bb\n\u0013\n\u00b7 (mt + 1) log N + log 2t2\n\u03b4\n\u00b5t \u00b7 (t \u22121)\nfor all \u03bb > 0, all t \u22651, all \u03c0 \u2208\u03a0, and all distributions\neP \u2208Sparse[mt].\nProof. Let\nZ e\nP ,\u03c0,t(x) .=\n1\n(1 \u2212K\u00b5t)W e\nP (x, \u03c0(x)) + \u00b5t\nso V e\nP ,\u03c0,t = Ex\u223cDX[Z e\nP ,\u03c0,t(x)] and bV e\nP ,\u03c0,t = (t \u2212\n1)\u22121 Pt\u22121\ni=1 Z e\nP ,\u03c0,t(xi). Also let\n\u03b5t .= log(|Sparse[mt]|N2t2/\u03b4)\n\u00b5t \u00b7 (t \u22121)\n= ((mt + 1) log N + log 2t2\n\u03b4 )\n\u00b5t \u00b7 (t \u22121)\n.\nWe apply Bernstein\u2019s inequality and union bounds\nover eP \u2208Sparse[mt], \u03c0 \u2208\u03a0, and t \u22651 so that with\nprobability at least 1 \u2212\u03b4,\nV e\nP ,\u03c0,t \u2264bV e\nP ,\u03c0,t +\nq\n2V e\nP ,\u03c0,t\u03b5t + (2/3)\u03b5t\nall t \u22651, all \u03c0 \u2208\u03a0, and all distributions P\n\u2208\nSparse[mt].\nThe conclusion follows by solving the\nquadratic inequality for V e\nP ,\u03c0,t to get\nV e\nP ,\u03c0,t \u2264bV e\nP ,\u03c0,t +\nq\n2bV e\nP ,\u03c0,t\u03b5t + 5\u03b5t\nand then applying the AM/GM inequality.\nLemma 16. Fix any \u03b3 \u2208[0, 1], and any x \u2208X. For\nany distribution P over \u03a0 and any \u03c0 \u2208\u03a0, if\nm .=\n\u0018\n6\n\u03b32\u00b5t\n\u0019\n,\nthen\nE\ne\nP \u223cP m\n\f\f\f\f\f\n1\n(1 \u2212K\u00b5t)W e\nP (x, \u03c0(x)) + \u00b5t\n\u2212\n1\n(1 \u2212K\u00b5t)WP (x, \u03c0(x)) + \u00b5t\n\f\f\f\f\f\n\u2264\n\u03b3\n(1 \u2212K\u00b5t)WP (x, \u03c0(x)) + \u00b5t\n.\nThis implies that for all distributions P over \u03a0 and\nany \u03c0 \u2208\u03a0, there exists eP \u2208Sparse[m] such that for\nany \u03bb > 0,\n\u0010\nVP,\u03c0,t \u2212V e\nP ,\u03c0,t\n\u0011\n+ (1 + \u03bb)\n\u0010\nbV e\nP ,\u03c0,t \u2212bVP,\u03c0,t\n\u0011\n\u2264\u03b3(VP,\u03c0,t + (1 + \u03bb)bVP,\u03c0,t).\nProof. We randomly draw eP \u223cP m, with eP(\u03c0\u2032) .=\nm\u22121 Pm\ni=1 I(\u03c0\u2032 = \u03c0i), and then de\ufb01ne\nz .=\nX\n\u03c0\u2032\u2208\u03a0\nP(\u03c0\u2032) \u00b7 I(\u03c0\u2032(x) = \u03c0(x))\nand\n\u02c6z .=\nX\n\u03c0\u2032\u2208\u03a0\neP(\u03c0\u2032) \u00b7 I(\u03c0\u2032(x) = \u03c0(x)).\nWe have z\n=\nE\u03c0\u2032\u223cP [I(\u03c0\u2032(x)\n=\n\u03c0(x)] and \u02c6z\n=\nm\u22121 Pm\ni=1 I(\u03c0i(x) = \u03c0(x)). In other words, \u02c6z is the\naverage of m independent Bernoulli random variables,\neach with mean z. Thus, E e\nP \u223cP m[(\u02c6z\u2212z)2] = z(1\u2212z)/m\nand Pr e\nP \u223cP m[\u02c6z \u2264z/2] \u2264exp(\u2212mz/8) by a Cherno\ufb00\nbound. We have\nE\ne\nP \u223cP m\n\f\f\f\f\n1\n(1 \u2212K\u00b5t)\u02c6z + \u00b5t\n\u2212\n1\n(1 \u2212K\u00b5t)z + \u00b5t\n\f\f\f\f\n\u2264\nE\ne\nP \u223cP m\n(1 \u2212K\u00b5t)|\u02c6z \u2212z|\n[(1 \u2212K\u00b5t)\u02c6z + \u00b5t][(1 \u2212K\u00b5t)z + \u00b5t]\n\u2264\nE\ne\nP \u223cP m\n(1 \u2212K\u00b5t)|\u02c6z \u2212z|I(\u02c6z \u22650.5z)\n0.5[(1 \u2212K\u00b5t)z + \u00b5t]2\n+\nE\ne\nP \u223cP m\n(1 \u2212K\u00b5t)|\u02c6z \u2212z|I(\u02c6z \u22640.5z)\n\u00b5t[(1 \u2212K\u00b5t)z + \u00b5t]\n\u2264(1 \u2212K\u00b5t)\np\nE e\nP \u223cP m |\u02c6z \u2212z|2\n0.5[(1 \u2212K\u00b5t)z + \u00b5t]2\n+ (1 \u2212K\u00b5t)z Pr e\nP \u223cP m(\u02c6z \u22640.5z)\n\u00b5t[(1 \u2212K\u00b5t)z + \u00b5t]\n\u2264\n(1 \u2212K\u00b5t)\np\nz/m\n0.5[2\np\n(1 \u2212K\u00b5t)z\u00b5t][(1 \u2212K\u00b5t)z + \u00b5t]\n+ (1 \u2212K\u00b5t)z exp(\u2212mz/8)\n\u00b5t[(1 \u2212K\u00b5t)z + \u00b5t]\n\u2264\n\u03b3\u221a1 \u2212K\u00b5t\np\nz/m\np\nz(6/m)[(1 \u2212K\u00b5t)z + \u00b5t]\n+ (1 \u2212K\u00b5t)\u03b32mz exp(\u2212mz/8)\n6[(1 \u2212K\u00b5t)z + \u00b5t]\n,\nwhere the third inequality follows from Jensen\u2019s in-\nequality, and the fourth inequality uses the AM/GM\ninequality in the denominator of the \ufb01rst term and\nthe previous observations in the numerators. The \ufb01-\nnal expression simpli\ufb01es to the \ufb01rst desired displayed\ninequality by observing that mz exp(\u2212mz/8) \u22643 for\nall mz \u22650 (the maximum is achieved at mz = 8). The\nsecond displayed inequality follows from the following\nfacts:\nE\ne\nP \u223cP m |VP,\u03c0,t \u2212V e\nP ,\u03c0,t| \u2264\u03b3VP,\u03c0,t,\nE\ne\nP \u223cP m(1 + \u03bb)|bVP,\u03c0,t \u2212bV e\nP ,\u03c0,t| \u2264\u03b3(1 + \u03bb)bVP,\u03c0,t.\nBoth inequalities follow from the \ufb01rst displayed bound\nof the lemma, by taking expectation with respect to\nthe true (and empirical) distributions over x. The de-\nsired bound follows by adding the above two inequal-\nities, which implies that the bound holds in expecta-\ntion, and hence the existence of eP for which the bound\nholds.\nNow, we can prove Theorem 6.\nProof of Theorem 6. Let\nmt .=\n\u0018 6\n\u03bb2 \u00b7 1\n\u00b5t\n\u0019\n(for some \u03bb \u2208(0, 1/5) to be determined) and condition\non the \u22651 \u2212\u03b4 probability event from Lemma 15 that\nV e\nP ,\u03c0,t \u2212(1 + \u03bb)bV e\nP ,\u03c0,t\n\u2264K \u00b7\n\u0012\n5 + 1\n2\u03bb\n\u0013\n\u00b7 (mt + 1) log(N) + log(2t2/\u03b4)\nK\u00b5t \u00b7 (t \u22121)\n\u2264K \u00b7 5\n\u0012\n1 + 1\n\u03bb\n\u0013\n\u00b7 (mt + 1) log(N) + log(2t2/\u03b4)\nK\u00b5t \u00b7 t\nfor all t \u22652, all eP \u2208Sparse[mt], and all \u03c0 \u2208\u03a0. Using\nthe de\ufb01nitions of mt and \u00b5t, the second term is at most\n(40/\u03bb2)\u00b7(1+1/\u03bb)\u00b7K for all t \u226516K log(8KN/\u03b4): the\nkey here is that for t \u226516K log(8KN/\u03b4), we have\n\u00b5t =\np\nlog(Nt/\u03b4)/(Kt) \u22641/(2K) and therefore\nmt log(N)\nK\u00b5tt\n\u22646\n\u03bb2\nand\nlog(N) + log(2t2/\u03b4)\nK\u00b5tt\n\u22642.\nNow \ufb01x t \u226516K log(8KN/\u03b4), \u03c0 \u2208\u03a0, and a distribu-\ntion P over \u03a0. Let eP \u2208Sparse[mt] be the distribution\nguaranteed by Lemma 16 with \u03b3 = \u03bb satisfying\nVP,\u03c0,t \u2264\nV e\nP ,\u03c0,t \u2212(1 + \u03bb)bV e\nP ,\u03c0,t + (1 + \u03bb)2 bVP,\u03c0,t\n1 \u2212\u03bb\n.\nSubstituting the previous bound for V e\nP ,\u03c0,t \u2212(1 +\n\u03bb)bV e\nP ,\u03c0,t gives\nVP,\u03c0,t \u2264\n1\n1 \u2212\u03bb\n\u001240\n\u03bb2 (1 + 1/\u03bb)K + (1 + \u03bb)2 bVP,\u03c0,t\n\u0013\n.\nThis can be bounded as (1 + \u01eb) \u00b7 bVP,\u03c0,t + (7500/\u01eb3) \u00b7 K\nby setting \u03bb = \u01eb/5.\nD\nAnalysis of RandomizedUCB\nD.1\nPreliminaries\nFirst, we de\ufb01ne the following constants.\n\u2022 \u01eb \u2208(0, 1) is a \ufb01xed constant, and\n\u2022 \u03c1 .= 7500\n\u01eb3\nis the factor that appears in the bound\nfrom Theorem 6.\n\u2022 \u03b8 .= (\u03c1 + 1)/(1 \u2212(1 + \u01eb)/2) =\n2\n1\u2212\u01eb\n\u00001 + 7500\n\u01eb3\n\u0001\n\u22655\nis a constant central to Lemma 21, which bounds\nthe variance of the optimal policy\u2019s estimated re-\nwards.\nRecall the algorithm-speci\ufb01c quantities\nCt .= 2 log\n\u0012Nt\n\u03b4\n\u0013\n\u00b5t .= min\n(\n1\n2K ,\nr\nCt\n2Kt\n)\n.\nIt can be checked that \u00b5t is non-increasing. We de\ufb01ne\nthe following time indices:\n\u2022 t0 is the \ufb01rst round t in which \u00b5t =\np\nCt/(2Kt).\nNote that 8K \u2264t0 \u22648K log(NK/\u03b4).\n\u2022 t1 := \u230816K log(8KN/\u03b4)\u2309is the round given by\nTheorem 6 such that, with probability at least\n1 \u2212\u03b4,\nE\nxt\u223cDX\n\u0014\n1\nW \u2032\nt(\u03c0(xt))\n\u0015\n\u2264(1 + \u01eb)\nE\nx\u223cht\u22121\n\u0014\n1\nWPt,\u00b5t(x, \u03c0(x))\n\u0015\n+ \u03c1K\n(D.1)\nfor all \u03c0 \u2208\u03a0 and all t \u2265t1, where WP,\u00b5(x, \u00b7) is\nthe distribution over A given by\nWP,\u00b5(x, a) .= (1 \u2212K\u00b5)WP (x, a) + \u00b5,\nand the notation Ex\u223cht\u22121 denotes expectation\nwith respect to the empirical (uniform) distribu-\ntion over x1, . . . , xt\u22121.\nThe following lemma shows the e\ufb00ect of allowing slack\nin the optimization constraints.\nLemma 17. If P satis\ufb01es the constraints of the opti-\nmization problem (4.1) with slack K for each distribu-\ntion Q over \u03a0, i.e.,\nE\n\u03c0\u223cQ\nE\nx\u223cht\u22121\n\u0014\n1\n(1 \u2212K\u00b5t)WP (x, \u03c0(x)) + \u00b5t\n\u0015\n\u2264max\n\u001a\n4K, (t \u22121)\u2206t\u22121(WQ)2\n180Ct\u22121\n\u001b\n+ K\nfor all Q, then P satis\ufb01es\nE\n\u03c0\u223cQ\nE\nx\u223cht\u22121\n\u0014\n1\n(1 \u2212K\u00b5t)WP (x, \u03c0(x)) + \u00b5t\n\u0015\n\u2264max\n\u001a\n5K, (t \u22121)\u2206t\u22121(WQ)2\n144Ct\u22121\n\u001b\nfor all Q.\nProof. Let b .= max\nn\n4K, (t\u22121)\u2206t\u22121(\u03c0)2\n180Ct\u22121\no\n.\nNote that\nb\n4 \u2265K.\nHence b + K \u2264\n5b\n4 which gives the stated\nbound.\nNote that the allowance of slack K is somewhat arbi-\ntrary; any O(K) slack is tolerable provided that other\nconstants are adjusted appropriately.\nD.2\nDeviation Bound for \u03b7t(\u03c0)\nFor any policy \u03c0 \u2208\u03a0, de\ufb01ne, for 1 \u2264t \u2264t0,\n\u00afVt(\u03c0) .= K,\nand for t > t0,\n\u00afVt(\u03c0) .= K +\nE\nxt\u223cDX\n\u0014\n1\nW \u2032\nt(\u03c0(xt))\n\u0015\n.\nThe \u00afVt(\u03c0) bounds the variances of the terms in \u03b7t(\u03c0).\nLemma 18. Assume the bound in (D.1) holds for all\n\u03c0 \u2208\u03a0 and t \u2265t1. For all \u03c0 \u2208\u03a0:\n1. If t \u2264t1, then\nK \u2264\u00afVt(\u03c0) \u22644K.\n2. If t > t1, then\n\u00afVt(\u03c0)\n\u2264(1 + \u01eb)\nE\nx\u223cht\u22121\n\u0014\n1\n(1 \u2212K\u00b5t)WPt(x, \u03c0(x)) + \u00b5t\n\u0015\n+ (\u03c1 + 1)K.\nProof. For the \ufb01rst claim, note that if t < t0, then\n\u00afVt(\u03c0) = K, and if t0 \u2264t < t1, then\n\u00b5t =\nr\nlog(Nt/\u03b4)\nKt\n\u2265\ns\nlog(Nt0/\u03b4)\n16K2 log(8KN/\u03b4) \u2265\n1\n4K ;\nso W \u2032\nt(a) \u2265\u00b5t \u22651/(4K).\nFor the second claim, pick any t > t1, and note that\nby de\ufb01nition of t1, for any \u03c0 \u2208\u03a0 we have\nE\nxt\u223cDX\n\u0014\n1\nW \u2032\nt(\u03c0(xt))\n\u0015\n\u2264(1 + \u01eb)\nE\nx\u223cht\u22121\n\u0014\n1\n(1 \u2212K\u00b5t)WPt(x, \u03c0(x)) + \u00b5t\n\u0015\n+ \u03c1K.\nThe stated bound on \u00afVt(\u03c0) now follows from its de\ufb01-\nnition.\nLet\n\u00afVmax,t(\u03c0) .= max{ \u00afV\u03c4(\u03c0), \u03c4 = 1, 2, . . ., t}\nThe following lemma gives a deviation bound for \u03b7t(\u03c0)\nin terms of these quantities.\nLemma 19. Pick any \u03b4 \u2208(0, 1). With probability at\nleast 1 \u2212\u03b4, for all pairs \u03c0, \u03c0\u2032 \u2208\u03a0 and t \u2265t0, we have\n\f\f\f(\u03b7t(\u03c0) \u2212\u03b7t(\u03c0\u2032)) \u2212(\u03b7D(\u03c0) \u2212\u03b7D(\u03c0\u2032))\n\f\f\f\n\u22642\nr\n( \u00afVmax,t(\u03c0) + \u00afVmax,t(\u03c0\u2032)) \u00b7 Ct\nt\n.\n(D.2)\nProof. Fix any t \u2265t0 and \u03c0, \u03c0\u2032 \u2208\u03a0.\nLet \u03b4t :=\nexp(\u2212Ct). Pick any \u03c4 \u2264t. Let\nZ\u03c4(\u03c0) .= r\u03c4(a\u03c4)I(\u03c0(x\u03c4) = a\u03c4)\nW \u2032\u03c4(a\u03c4)\nso \u03b7t(\u03c0) = t\u22121 Pt\n\u03c4=1 Z\u03c4(\u03c0). It is easy to see that\nE\n(x\u03c4 ,\u20d7r\u03c4 )\u223cD,\na\u03c4 \u223cW \u2032\n\u03c4\n[Z\u03c4(\u03c0) \u2212Z\u03c4(\u03c0\u2032)] = \u03b7D(\u03c0) \u2212\u03b7D(\u03c0\u2032)\nand\nt\nX\n\u03c4=1\nE\n(x\u03c4,\u20d7r(\u03c4))\u223cD,\na\u03c4 \u223cW \u2032\n\u03c4\n\u0002\n(Z\u03c4(\u03c0) \u2212Z\u03c4(\u03c0\u2032))2\u0003\n\u2264\nt\nX\n\u03c4=1\nE\nx\u03c4 \u223cDX\n\u0014\n1\nW \u2032\u03c4(\u03c0(x\u03c4)) +\n1\nW \u2032\u03c4(\u03c0\u2032(x\u03c4))\n\u0015\n\u2264t \u00b7 ( \u00afVmax,t(\u03c0) + \u00afVmax,t(\u03c0\u2032)).\nMoreover, with probability 1,\n|Z\u03c4(\u03c0) \u2212Z\u03c4(\u03c0\u2032)| \u22641\n\u00b5\u03c4\n.\nNow, note that since t \u2265t0, \u00b5t =\nq\nCt\n2Kt, so that\nt =\nCt\n2K\u00b52\nt . Further, both \u00afVmax,t(\u03c0) and \u00afVmax,t(\u03c0\u2032) are\nat least K. Using these bounds we get\ns\n1\nlog(1/\u03b4t) \u00b7 t \u00b7 ( \u00afVmax,t(\u03c0) + \u00afVmax,t(\u03c0\u2032))\n\u2265\ns\n1\nCt\n\u00b7\nCt\n2K\u00b52\nt\n\u00b7 2K = 1\n\u00b5t\n\u22651\n\u00b5\u03c4\n,\nfor all \u03c4 \u2264t, since the \u00b5\u03c4\u2019s are non-increasing. There-\nfore, by Freedman\u2019s inequality (Theorem 13), we have\nPr\n\"\f\f\f(\u03b7t(\u03c0) \u2212\u03b7t(\u03c0\u2032)) \u2212(\u03b7D(\u03c0) \u2212\u03b7D(\u03c0\u2032))\n\f\f\f\n> 2\nr\n( \u00afVmax,t(\u03c0) + \u00afVmax,t(\u03c0\u2032)) \u00b7 log(1/\u03b4t)\nt\n#\n\u22642\u03b4t.\nThe conclusion follows by taking a union bound over\nt0 < t \u2264T and all pairs \u03c0, \u03c0\u2032 \u2208\u03a0.\nD.3\nVariance Analysis\nWe de\ufb01ne the following condition, which will be as-\nsumed by most of the subsequent lemmas in this sec-\ntion.\nCondition 1. The deviation bound (D.1) holds for\nall \u03c0 \u2208\u03a0 and t \u2265t1, and the deviation bound (D.2)\nholds for all pairs \u03c0, \u03c0\u2032 \u2208\u03a0 and t \u2265t0.\nThe next two lemmas relate the \u00afVt(\u03c0) to the \u2206t(\u03c0).\nLemma 20. Assume Condition 1. For any t \u2265t1 and\n\u03c0 \u2208\u03a0, if \u00afVt(\u03c0) > \u03b8K, then\n\u2206t\u22121(\u03c0) \u2265\ns\n72 \u00afVt(\u03c0)Ct\u22121\nt \u22121\n.\nProof. By Lemma 18, the fact \u00afVt(\u03c0) > \u03b8K implies\nthat\nE\nx\u223cht\u22121\n\u0014\n1\n(1 \u2212K\u00b5t)WPt(x, \u03c0(x)) + \u00b5t\n\u0015\n>\n1\n1 + \u01eb\n\u0012\n1 \u2212\u03c1 + 1\n\u03b8\n\u0013\n\u00afVt(\u03c0) \u22651\n2\n\u00afVt(\u03c0).\nSince \u00afVt(\u03c0) > \u03b8K \u22655K, Lemma 17 implies that in or-\nder for Pt to satisfy the optimization constraint in (4.1)\ncorresponding to \u03c0 (with slack \u2264K), it must be the\ncase that\n\u2206t\u22121(\u03c0)\n\u2265\ns\n144Ct\u22121\nt \u22121\n\u00b7\nE\nx\u223cht\u22121\n\u0014\n1\n(1 \u2212K\u00b5t)WPt(x, \u03c0(x)) + \u00b5t\n\u0015\n.\nCombining with the above, we obtain\n\u2206t\u22121(\u03c0) \u2265\ns\n72 \u00afVt(\u03c0)Ct\u22121\nt \u22121\n.\nLemma 21. Assume Condition 1.\nFor all t \u22651,\n\u00afVmax,t(\u03c0max) \u2264\u03b8K and \u00afVmax,t(\u03c0t) \u2264\u03b8K.\nProof. By induction on t. The claim for all t \u2264t1 fol-\nlows from Lemma 18. So take t > t1, and assume as\nthe (strong) inductive hypothesis that \u00afVmax,\u03c4(\u03c0max) \u2264\n\u03b8K and \u00afVmax,\u03c4(\u03c0\u03c4) \u2264\u03b8K for \u03c4 \u2208{1, . . . , t \u22121}. Sup-\npose for sake of contradiction that \u00afVt(\u03c0max) > \u03b8K. By\nLemma 20,\n\u2206t\u22121(\u03c0max) \u2265\ns\n72 \u00afVt(\u03c0max)Ct\u22121\nt \u22121\n.\nHowever, by the deviation bounds, we have\n\u2206t\u22121(\u03c0max) + \u2206D(\u03c0t\u22121)\n\u22642\ns\n( \u00afVmax,t\u22121(\u03c0t\u22121) + \u00afVmax,t\u22121(\u03c0max))Ct\u22121\nt \u22121\n\u22642\ns\n2 \u00afVt(\u03c0max)Ct\u22121\nt \u22121\n<\ns\n72 \u00afVt(\u03c0max)Ct\u22121\nt \u22121\n.\nThe second inequality follows from our assumption and\nthe induction hypothesis:\n\u00afVt(\u03c0max) > \u03b8K \u2265\u00afVmax,t\u22121(\u03c0t\u22121), \u00afVmax,t\u22121(\u03c0max).\nSince \u2206D(\u03c0t\u22121) \u22650, we have a contradiction, so\nit must be that \u00afVt(\u03c0max) \u2264\u03b8K.\nThis proves that\n\u00afVmax,t(\u03c0max) \u2264\u03b8K.\nIt remains to show that \u00afVmax,t(\u03c0t) \u2264\u03b8K.\nSo sup-\npose for sake of contradiction that the inequality fails,\nand let t1 < \u03c4 \u2264t be any round for which \u00afV\u03c4(\u03c0t) =\n\u00afVmax,t(\u03c0t) > \u03b8K. By Lemma 20,\n\u2206\u03c4\u22121(\u03c0t) \u2265\ns\n72 \u00afV\u03c4(\u03c0t)C\u03c4\u22121\n\u03c4 \u22121\n.\n(D.3)\nOn the other hand,\n\u2206\u03c4\u22121(\u03c0t) \u2264\u2206D(\u03c0\u03c4\u22121) + \u2206\u03c4\u22121(\u03c0t) + \u2206t(\u03c0max)\n=\n\u0010\n\u2206D(\u03c0\u03c4\u22121) + \u2206\u03c4\u22121(\u03c0max)\n\u0011\n+\n\u0010\n\u03b7\u03c4\u22121(\u03c0max) \u2212\u03b7\u03c4\u22121(\u03c0t) \u2212\u2206D(\u03c0t)\n\u0011\n+\n\u0010\n\u2206D(\u03c0t) + \u2206t(\u03c0max)\n\u0011\n.\nThe parenthesized terms can be bounded using the\ndeviation bounds, so we have\n\u2206\u03c4\u22121(\u03c0t)\n\u22642\ns\n( \u00afVmax,\u03c4\u22121(\u03c0\u03c4\u22121) + \u00afVmax,\u03c4\u22121(\u03c0max))C\u03c4\u22121\n\u03c4 \u22121\n+ 2\ns\n( \u00afVmax,\u03c4\u22121(\u03c0t) + \u00afVmax,\u03c4\u22121(\u03c0max))C\u03c4\u22121\n\u03c4 \u22121\n+ 2\nr\n( \u00afVmax,t(\u03c0t) + \u00afVmax,t(\u03c0max))Ct\nt\n\u22642\ns\n2 \u00afV\u03c4(\u03c0t)C\u03c4\u22121\n\u03c4 \u22121\n+ 2\ns\n2 \u00afV\u03c4(\u03c0t)C\u03c4\u22121\n\u03c4 \u22121\n+ 2\nr\n2 \u00afV\u03c4(\u03c0t)Ct\nt\n<\ns\n72 \u00afV\u03c4(\u03c0t)C\u03c4\u22121\n\u03c4 \u22121\nwhere the second inequality follows from the following\nfacts:\n1. By\ninduction\nhypothesis,\nwe\nhave\n\u00afVmax,\u03c4\u22121(\u03c0\u03c4\u22121), \u00afVmax,\u03c4\u22121(\u03c0max), \u00afVmax,t(\u03c0max)\n\u2264\n\u03b8K, and \u00afV\u03c4(\u03c0t) > \u03b8K,\n2. \u00afV\u03c4(\u03c0t) \u2265\u00afVmax,t(\u03c0t), and\n3. since \u03c4 is a round that achieves \u00afVmax,t(\u03c0t), we\nhave \u00afV\u03c4(\u03c0t) \u2265\u00afV\u03c4\u22121(\u03c0t).\nThis contradicts the inequality in (D.3), so it must be\nthat \u00afVmax,t(\u03c0t) \u2264\u03b8K.\nCorollary 22. Under the assumptions of Lemma 21,\n\u2206D(\u03c0t) + \u2206t(\u03c0max) \u22642\nr\n2\u03b8KCt\nt\nfor all t \u2265t0.\nProof. Immediate from Lemma 21 and the deviation\nbounds from (D.2).\nThe following lemma shows that if a policy \u03c0 has large\n\u2206\u03c4(\u03c0) in some round \u03c4, then \u2206t(\u03c0) remains large in\nlater rounds t > \u03c4.\nLemma 23. Assume Condition 1. Pick any \u03c0 \u2208\u03a0\nand t \u2265t1. If \u00afVmax,t(\u03c0) > \u03b8K, then\n\u2206t(\u03c0) > 2\nr\n2 \u00afVmax,t(\u03c0)Ct\nt\n.\nProof. Let \u03c4 \u2264t be any round in which \u00afV\u03c4(\u03c0) =\n\u00afVmax,t(\u03c0) > \u03b8K. We have\n\u2206t(\u03c0) \u2265\u2206t(\u03c0) \u2212\u2206t(\u03c0max) \u2212\u2206D(\u03c0\u03c4\u22121)\n= \u2206\u03c4\u22121(\u03c0) +\n\u0010\n\u03b7t(\u03c0max) \u2212\u03b7t(\u03c0) \u2212\u2206D(\u03c0)\n\u0011\n+\n\u0010\n\u03b7D(\u03c0\u03c4\u22121) \u2212\u03b7D(\u03c0) \u2212\u2206\u03c4\u22121(\u03c0)\n\u0011\n\u2265\ns\n72 \u00afV\u03c4(\u03c0)C\u03c4\u22121\n\u03c4 \u22121\n\u22122\nr\n( \u00afVmax,t(\u03c0) + \u00afVmax,t(\u03c0max))Ct\nt\n\u22122\ns\n( \u00afVmax,\u03c4\u22121(\u03c0) + \u00afVmax,\u03c4\u22121(\u03c0\u03c4\u22121))C\u03c4\u22121\n\u03c4 \u22121\n>\ns\n72 \u00afVmax,t(\u03c0)C\u03c4\u22121\n\u03c4 \u22121\n\u22122\nr\n2 \u00afVmax,t(\u03c0)Ct\nt\n\u22122\ns\n2 \u00afVmax,t(\u03c0)C\u03c4\u22121\n\u03c4 \u22121\n\u22652\ns\n2 \u00afVmax,t(\u03c0)C\u03c4\u22121\n\u03c4 \u22121\n\u22652\nr\n2 \u00afVmax,t(\u03c0)Ct\nt\nwhere the second inequality follows from Lemma 20\nand the deviation bounds, and the third inequality\nfollows from Lemma 21 and the facts that \u00afV\u03c4(\u03c0) =\n\u00afVmax,t(\u03c0) > \u03b8K \u2265\u00afVmax,t(\u03c0max), \u00afVmax,\u03c4\u22121(\u03c0\u03c4\u22121), and\n\u00afVmax,t(\u03c0) \u2265\u00afVmax,\u03c4\u22121(\u03c0).\nD.4\nRegret Analysis\nWe now bound the value of the optimization prob-\nlem (4.1), which then leads to our regret bound. The\nnext lemma shows the existence of a feasible solution\nwith a certain structure based on the non-uniform con-\nstraints. Recall from Section 5, that solving the opti-\nmization problem A, i.e. constraints (5.1, 5.2, 5.3), for\nthe smallest feasible value of s is equivalent to solving\nthe RUCB optimization problem (4.1).\nRecall that\n\u03b2t =\nt\u22121\n180Ct\u22121 .\nLemma 24. There is a point W \u2208R(t\u22121)K such that\n\u2206t\u22121(W) \u22644\ns\nK\n\u03b2t\nW \u2208C\n\u2200Z \u2208C : E\nx\u223cht\u22121\n\"X\na\nZ(x, a)\nW \u2032(x, a)\n#\n\u2264max{4K, \u03b2t\u2206t\u22121(Z)2}\nIn particular, the value of the optimization prob-\nlem (4.1), OPTt, is bounded by 8\nq\nK\n\u03b2t \u2264110\nq\nKCt\u22121\nt\u22121 .\nProof. De\ufb01ne the sets {Ci : i = 1, 2, . . .} such that\nCi := {Z \u2208C : 2i+1\u03ba \u2264\u2206t\u22121(Z) \u22642i+2\u03ba},\nwhere \u03ba =\nq\nK\n\u03b2t . Note that since \u2206t\u22121(Z) is a linear\nfunction of Z, each Ci is a closed, convex, compact\nset.\nAlso, de\ufb01ne C0 = {Z \u2208C :\n\u2206t\u22121(Z) \u22644\u03ba}.\nThis is also a closed, convex, compact set. Note that\nC = S\u221e\ni=0 Ci.\nLet I = {i : Ci \u0338= \u2205}.For i \u2208I \\ {0}, de\ufb01ne wi = 4\u2212i,\nand let w0 = 1 \u2212P\ni\u2208I\\{0} wi. Note that w0 \u22652/3.\nBy Lemma 1, for each i \u2208I, there is a point Wi \u2208Ci\nsuch that for all Z \u2208Ci, we have\nE\nx\u223cht\u22121\n\"X\na\nZ(x, a)\nW \u2032\ni(x, a)\n#\n\u22642K.\nHere we use the fact that K\u00b5t\n\u22641/2 to upper\nbound\nK\n1\u2212K\u00b5t by 2K. Now consider the point W =\nP\ni\u2208I wiWi. Since C is convex, W \u2208C.\nNow \ufb01x any i \u2208I. For any (x, a), we have W \u2032(x, a) \u2265\nwiW \u2032\ni(x, a), so that for all Z \u2208Ci, we have\nE\nx\u223cht\u22121\n\"X\na\nZ(x, a)\nW \u2032(x, a)\n#\n\u22641\nwi\n2K\n\u22644i+1K\n\u2264max{4K, \u03b2t\u2206t\u22121(Z)2},\nso the constraint for Z is satis\ufb01ed.\nFinally, since for all i \u2208I, we have wi \u22644\u2212i and\n\u2206t\u22121(Wi) \u22642i+2\u03ba, we get\n\u2206t\u22121(W) =\nX\ni\u2208I\nwi\u2206t\u22121(Wi) \u2264\n\u221e\nX\ni=0\n4\u2212i \u00b7 2i+2\u03ba \u22648\u03ba.\nThe value of the optimization problem (4.1) can be\nrelated to the expected instantaneous regret of policy\ndrawn randomly from the distribution Pt.\nLemma 25. Assume Condition 1. Then\nX\n\u03c0\u2208\u03a0\nPt(\u03c0)\u2206D(\u03c0) \u2264\n\u0010\n220 + 4\n\u221a\n2\u03b8\n\u0011\n\u00b7\nr\nKCt\u22121\nt \u22121 + 2\u03b5opt,t\nfor all t > t1.\nProof. Fix any \u03c0 \u2208\u03a0 and t > t1. By the deviation\nbounds, we have\n\u0010\n\u03b7D(\u03c0t\u22121) \u2212\u03b7D(\u03c0)\n\u0011\n\u2264\u2206t\u22121(\u03c0) + 2\ns\n( \u00afVmax,t\u22121(\u03c0) + \u00afVmax,t\u22121(\u03c0t\u22121))Ct\u22121\nt \u22121\n\u2264\u2206t\u22121(\u03c0) + 2\ns\u0000 \u00afVmax,t\u22121(\u03c0) + \u03b8K\n\u0001\nCt\u22121\nt \u22121\n,\nby Lemma 21. By Corollary 22 we have\n\u2206D(\u03c0t\u22121) \u22642\nr\n2\u03b8KCt\u22121\nt \u22121\nThus, we get\n\u2206D(\u03c0) \u2264\n\u0010\n\u03b7D(\u03c0t\u22121) \u2212\u03b7D(\u03c0)\n\u0011\n+ \u2206D(\u03c0t\u22121)\n\u2264\u2206t\u22121(\u03c0) + 2\ns\u0000 \u00afVmax,t\u22121(\u03c0) + \u03b8K\n\u0001\nCt\u22121\nt \u22121\n+ 2\nr\n2\u03b8KCt\u22121\nt \u22121\n.\nIf \u00afVmax,t\u22121(\u03c0) \u2264\u03b8K, then we have\n\u2206D(\u03c0) \u2264\u2206t\u22121(\u03c0) + 4\nr\n2\u03b8KCt\u22121\nt \u22121\n.\nOtherwise, Lemma 23 implies that\n\u00afVmax,t\u22121(\u03c0) \u2264(t \u22121) \u00b7 \u2206t\u22121(\u03c0)2\n8Ct\u22121\n,\nso\n\u2206D(\u03c0) \u2264\u2206t\u22121(\u03c0) + 2\nr\n\u2206t\u22121(\u03c0)2\n8\n+ \u03b8KCt\u22121\nt \u22121\n+ 2\nr\n2\u03b8KCt\u22121\nt \u22121\n\u22642\u2206t\u22121(\u03c0) + 4\nr\n2\u03b8KCt\u22121\nt \u22121\n.\nTherefore\nX\n\u03c0\u2208\u03a0\nPt(\u03c0)\u2206D(\u03c0)\n\u22642\nX\n\u03c0\u2208\u03a0\nPt(\u03c0)\u2206t\u22121(\u03c0) + 4\nr\n2\u03b8KCt\u22121\nt \u22121\n\u22642 (OPTt +\u03b5opt,t) + 4\nr\n2\u03b8KCt\u22121\nt \u22121\nwhere OPTt is the value of the optimization prob-\nlem (4.1). The conclusion follows from Lemma 24.\nWe can now \ufb01nally prove the main regret bound for\nRUCB.\nProof of Theorem 5. The regret through the \ufb01rst t1\nrounds is trivially bounded by t1. In the event that\nCondition 1 holds, we have for all t \u2265t1,\nX\na\u2208A\nWt(a)rt(a) \u2265\nX\na\u2208A\n(1 \u2212K\u00b5t)WPt(xt, a)rt(a)\n\u2265\nX\na\u2208A\nWPt(xt, a)rt(a) \u2212K\u00b5t\n=\nX\n\u03c0\u2208\u03a0\nPt(\u03c0)rt(\u03c0(xt)) \u2212K\u00b5t,\nand therefore\nE\n(xt,\u20d7r(t))\u223cD\nat\u223cW \u2032\nt\n[rt(at)]\n=\nE\n(xt,\u20d7r(t))\u223cD\n\"X\na\u2208A\nW \u2032\nt(a)rt(a)\n#\n\u2265\nX\n\u03c0\u2208\u03a0\nPt(\u03c0)\u03b7D(\u03c0) \u2212K\u00b5t\n\u2265\u03b7D(\u03c0max) \u2212O\n r\nKCt\u22121\nt \u22121\n+ \u03b5opt,t\n!\nwhere the last inequality follows from Lemma 25.\nSumming the bound from t = t1 + 1, . . . , T gives\nT\nX\nt=1\nE\n(xt,\u20d7r(t))\u223cD\nat\u223cW \u2032\nt\n[\u03b7D(\u03c0max) \u2212rt(at)]\n\u2264t1 + O\n\u0010p\nT K log (NT/\u03b4)\n\u0011\n.\nBy\nAzuma\u2019s\ninequality,\nthe\nprobability\nthat\nPT\nt=1 rt(at) deviates from its mean by more than\nO(\np\nT log(1/\u03b4)) is at most \u03b4. Finally, the probability\nthat Condition 1 does not hold is at most 2\u03b4 by\nLemma 19, Theorem 6, and a union bound.\nThe\nconclusion follows by a \ufb01nal union bound.\nE\nDetails of Oracle-based Algorithm\nWe show how to (approximately) solve A using the\nellipsoid algorithm with AMO. Fix a time period t.\nTo avoid clutter, (only) in this section we drop the\nsubscript t \u22121 from \u03b7t\u22121(\u00b7), \u2206t\u22121(\u00b7), and ht\u22121 so that\nthey becomes \u03b7(\u00b7), \u2206(\u00b7), and h respectively.\nIn order to use the ellipsoid algorithm, we need to\nrelax the program a little bit in order to ensure that\nthe feasible region has a non-negligible volume. To do\nthis, we need to obtain some perturbation bounds for\nthe constraints of A. The following lemma gives such\nbounds. For any \u03b4 > 0, we de\ufb01ne C\u03b4 to be the set of\nall points within a distance of \u03b4 from C.\nLemma 26. Let \u03b4 \u2264b/4 be a parameter. Let U, W \u2208\nC2\u03b4 be points such that \u2225U \u2212W\u2225\u2264\u03b4. Then we have\n|\u2206(U) \u2212\u2206(W)| \u2264\u03b3\n(E.1)\n\u2200Z \u2208C1 :\n\f\f\f\f\f E\nx\u223ch\n\"X\na\nZ(x, a)\nU \u2032(x, a)\n#\n\u2212E\nx\u223ch\n\"X\na\nZ(x, a)\nW \u2032(x, a)\n#\f\f\f\f\f \u2264\u01eb\n(E.2)\nwhere \u01eb = 8\u03b4\n\u00b52\nt and \u03b3 =\n\u03b4\n\u00b5t .\nProof. First, we have\n|\u03b7(U) \u2212\u03b7(W)| \u2264\n1\nt \u22121\nX\n(x,a,r,q)\u2208h\nr\np|U(x, a) \u2212W(x, a)|\n\u2264\n\u03b4\n\u00b5t\n= \u03b3,\nwhich implies (E.1).\nNext, for any Z \u2208C1, we have\n\f\f\f\f\f\nX\na\nZ(x, a)\nU \u2032(x, a) \u2212\nX\na\nZ(x, a)\nW \u2032(x, a)\n\f\f\f\f\f\n\u2264\nX\na\n|Z(x, a)||U \u2032(x, a) \u2212W \u2032(x, a)|\nU \u2032(x, a)W \u2032(x, a)\n\u22648\u03b4\n\u00b52\nt\n= \u01eb.\nIn the last inequality, we use the Cauchy-Schwarz in-\nequality, and use the following facts (here, Z(x, \u00b7) de-\nnotes the vector \u27e8Z(x, a)\u27e9a, etc.):\n1. \u2225Z(x, \u00b7)\u2225\u22642 since Z \u2208C1,\n2. \u2225U \u2032(x, \u00b7) \u2212W \u2032(x, \u00b7)\u2225\u2264\u2225U(x, \u00b7) \u2212W(x, \u00b7)\u2225\u2264\u03b4,\nand\n3. U \u2032(x, a) \u2265(1 \u2212bK) \u00b7 (\u22122\u03b4) + b \u2265b/2, for \u03b4 \u2264b/4,\nand similarly W \u2032(x, a) \u2265b/2.\nThis implies (E.2).\nWe now consider the following relaxed form of A.\nHere, \u03b4 \u2208(0, b/4) is a parameter. We want to \ufb01nd\na point W \u2208R(t\u22121)K such that\n\u2206(W) \u2264s + \u03b3\n(E.3)\nW \u2208C\u03b4\n(E.4)\n\u2200Z \u2208C2\u03b4 :\nE\nx\u223ch\n\"X\na\nZ(x, a)\nW \u2032(x, a)\n#\n\u2264max{4K, \u03b2t\u2206(Z)2} + \u01eb,\n(E.5)\nwhere \u01eb and \u03b3 are as de\ufb01ned in Lemma 26. Call this\nrelaxed program A\u2032.\nWe apply the ellipsoid method to A\u2032 rather than A.\nRecall the requirements of Lemma 8: we need an en-\nclosing ball of bounded radius for the feasible region,\nand the radius of an enclosed ball in the feasible region.\nThe following lemma gives this.\nLemma 27. The feasible region for A\u2032 is contained in\nB(0,\n\u221a\nt + \u03b4), and if A is feasible, then it contains a\nball of radius \u03b4.\nProof. Note that for any W \u2208C\u03b4, we have \u2225W\u2225\u2264\n\u221a\nt + \u03b4, so the feasible region lies in B(0,\n\u221a\nt + \u03b4).\nNext, if A is feasible, let W \u22c6\u2208C be any feasible solu-\ntion to A. Consider the ball B(W \u22c6, \u03b4). Let U be any\npoint in B(W \u22c6, \u03b4). Clearly U \u2208C\u03b4. By Lemma 26,\nassuming \u03b4 \u22641/2, we have for all Z \u2208C2\u03b4,\nE\nx\u223ch\n\"X\na\nZ(x, a)\nU \u2032(x, a)\n#\n\u2264E\nx\u223ch\n\"X\na\nZ(x, a)\nU \u2032(x, a)\n#\n+ \u01eb\n\u2264max{4K, \u03b2t\u2206(Z)2} + \u01eb.\nAlso\n\u2206(U) \u2264\u2206(W \u22c6) + \u03b3 \u2264s + \u03b3.\nThus, U is feasible for A\u2032, and hence the entire ball\nB(W \u22c6, \u03b4) is feasible for A\u2032.\nWe now give the construction of a separation oracle for\nthe feasible region of A\u2032 by checking for violations of\nthe constraints. In the following, we use the word \u201citer-\nation\u201d to indicate one step of either the ellipsoid algo-\nrithm or the perceptron algorithm. Each such iteration\ninvolves one call to AMO, and additional O(t2K2)\nprocessing time.\nLet W \u2208R(t\u22121)K be a candidate point that we want to\ncheck for feasibility for A\u2032. We can check for violation\nof the constraint (E.3) easily, and since it is a linear\nconstraint in W, it automatically yields a separating\nhyperplane if it is violated.\nThe harder constraints are (E.4) and (E.5).\nRecall\nthat Lemma 9 shows that that AMO allows us to do\nlinear optimization over C e\ufb03ciently.\nThis immedi-\nately gives us the following useful corollary:\nCorollary 28. Given a vector w \u2208R(t\u22121)K and \u03b4 > 0,\nwe can compute arg maxZ\u2208C\u03b4 w \u00b7 Z using one invoca-\ntion of AMO.\nProof. This follows directly from the following fact:\narg max\nZ\u2208C\u03b4 w \u00b7 Z =\n\u03b4\n\u2225w\u2225w + arg max\nZ\u2208C w \u00b7 Z.\nNow we show how to use AMO to check for constraint\n(E.4):\nLemma 29. Suppose we are given a point W. Then\nin O( t\n\u03b42 ) iterations, if W /\u2208C2\u03b4, we can construct a\nhyperplane separating W from C\u03b4. Otherwise, we de-\nclare correctly that W \u2208C2\u03b4. In the latter case, we can\n\ufb01nd an explicit distribution P over policies in \u03a0 such\nthat WP satis\ufb01es \u2225WP \u2212W\u2225\u22642\u03b4.\nProof. We run the perceptron algorithm with the ori-\ngin at W and all points in C\u03b4 being positive exam-\nples. The goal of the perceptron algorithm then is to\n\ufb01nd a hyperplane going through W that puts all of C\u03b4\n(strictly) on one side. In each iteration of the percep-\ntron algorithm, we have a weight vector w that is the\nnormal to a candidate hyperplane, and we need to \ufb01nd\na point Z \u2208C\u03b4 such that w \u00b7 (Z \u2212W) \u22640 (note that\nwe have shifted the origin to W). To do this, we use\nAMO as in Lemma 9 to \ufb01nd Z\u22c6= arg maxZ\u2208C\u03b4 \u2212w\u00b7Z.\nIf w \u00b7 (Z\u22c6\u2212W) \u22640, we use Z\u22c6to update w using the\nperceptron update rule, w \u2190w + (Z\u22c6\u2212W). Other-\nwise, we have w \u00b7 (Z \u2212W) > 0 for all W \u2208C\u03b4, and\nhence we have found our separating hyperplane.\nNow suppose that W /\u2208C2\u03b4, i.e. the distance of W\nfrom C\u03b4 is more than \u03b4.\nSince \u2225Z \u2212W\u2225\u22642\n\u221a\nt +\n3\u03b4 = O(\n\u221a\nt) for all W \u2208C\u03b4 (assuming \u03b4 = O(\n\u221a\nt)),\nthe perceptron convergence guarantee implies that in\nO( t\n\u03b42 ) iterations we \ufb01nd a separating hyperplane.\nIf in k = O( t\n\u03b42 ) iterations we haven\u2019t found a separat-\ning hyperplane, then W \u2208C2\u03b4. In fact the perceptron\nalgorithm gives a stronger guarantee: if the k poli-\ncies found in the run of the perceptron algorithm are\n\u03c01, \u03c02, . . . , \u03c0k \u2208\u03a0, then W is within a distance of 2\u03b4\nfrom their convex hull, C\u2032 = conv(\u03c01, \u03c02, . . . , \u03c0k). This\nis because a run of the perceptron algorithm on C\u2032\n2\u03b4\nwould be identical to that on C2\u03b4 for k steps. We can\nthen compute the explicit distribution over policies P\nby computing the Euclidean projection of W on C\u2032 in\npoly(k) time using a convex quadratic program:\nmin \u2225W\u2212Pk\ni=1Pi\u03c0i\u22252\nX\ni\nPi = 1\n\u2200i : Pi \u22650\nSolving this quadratic program, we get a distribution\nP over the policies {\u03c01, \u03c02, . . . , \u03c0k} such that \u2225WP \u2212\nW\u2225\u22642\u03b4.\nFinally, we show how to check constraint (E.5):\nLemma 30. Suppose we are given a point W.\nIn\nO( t3K2\n\u03b42\n\u00b7 log( t\n\u03b4)) iterations, we can either \ufb01nd a point\nZ \u2208C2\u03b4 such that\nE\nx\u223ch\n\"X\na\nZ(x, a)\nW \u2032(x, a)\n#\n\u2265max{4K, \u03b2t\u2206(Z)2} + 2\u01eb,\nor else we conclude correctly that for all Z \u2208C, we\nhave\nE\nx\u223ch\n\"X\na\nZ(x, a)\nW \u2032(x, a)\n#\n\u2264max{4K, \u03b2t\u2206(Z)2} + 3\u01eb.\nProof. We \ufb01rst rewrite \u03b7(W) as \u03b7(W) = w \u00b7 \u03c0, where\nw is a vector de\ufb01ned as\nw(x, a) =\n1\nt \u22121\nX\n(x\u2032,a\u2032,r,p)\u2208h: x\u2032=x,a\u2032=a\nr\np.\nThus, \u2206(Z) = v \u2212w \u00b7 Z, where v = max\u03c0\u2032 \u03b7(\u03c0\u2032) =\nmax\u03c0\u2032 w \u00b7 \u03c0\u2032 which can be computed by using AMO\nonce.\nNext, using the candidate point W, compute the\nvector u de\ufb01ned as u(x, a) =\nnx/t\nW \u2032(x,a), where nx\nis the number of times x appears in h, so that\nEx\u223ch\nhP\na\nZ(x,a)\nW \u2032(x,a)\ni\n= u \u00b7 Z. Now, the problem reduces\nto \ufb01nding a point R \u2208C which violates the constraint\nu \u00b7 Z \u2264max{4K, \u03b2t(w \u00b7 Z \u2212v)2} + 3\u01eb.\nDe\ufb01ne\nf(Z) = max{4K, \u03b2t(w \u00b7 Z \u2212v)2} + 3\u01eb \u2212u \u00b7 Z.\nNote that f is convex function of Z. Checking for vi-\nolation of the above constraint is equivalent to solving\nthe following (convex) program:\nf(Z) \u22640\n(E.6)\nZ \u2208C\n(E.7)\nTo do this, we again apply the ellipsoid method, but\non the relaxed program\nf(Z) \u2264\u01eb\n(E.8)\nZ \u2208C\u03b4\n(E.9)\nTo run the ellipsoid algorithm, we need a separation\noracle for the program. Given a candidate solution Z,\nwe run the algorithm of Lemma 29, and if Z /\u2208C2\u03b4, we\nconstruct a hyperplane separating Z from C\u03b4.\nNow suppose we conclude that Z \u2208C2\u03b4.\nThen we\nconstruct a separation oracle for (E.6) as follows. If\nf(Z) > \u01eb, then since f is a convex function of Z, we\ncan construct a separating hyperplane as in Lemma 10.\nNow we can run the ellipsoid algorithm with the\nstarting ellipsoid being B(0,\n\u221a\nt). If there is a point\nZ\u22c6\u2208C such that f(Z\u22c6) \u22640, then consider the ball\nB(Z\u22c6,\n4\u03b4\n5\n\u221a\ntK\u03b2t ). For any Y \u2208B(Z\u22c6,\n4\u03b4\n5\n\u221a\ntK\u03b2t ), we have\n|(u \u00b7 Z\u22c6) \u2212(u \u00b7 Y )| \u2264\u2225u\u2225\u2225Z\u22c6\u2212Y \u2225\u2264\u01eb\n2\nsince \u2225u\u2225\u2264\n\u221a\nK\n\u00b5t . Also,\n\u03b2t|(w \u00b7 Z\u22c6\u2212v)2 \u2212(w \u00b7 Y \u2212v)2|\n= \u03b2t|(w \u00b7 Z\u22c6\u2212w \u00b7 Y )(w \u00b7 Z\u22c6+ w \u00b7 Y \u22122v)|\n\u2264\u03b2t\u2225w\u2225\u2225Z\u22c6\u2212Y \u2225(\u2225w\u2225(\u2225Z\u22c6\u2225+ \u2225Y \u2225) + 2|v|) \u2264\u01eb\n2,\nsince \u2225w\u2225\u2264\n1\n\u00b5t , \u2225Z\u22c6\u2225\u2264\n\u221a\nt, \u2225Y \u2225\u2264\n\u221a\nt+\u03b4 \u22642\n\u221a\nt, and\n|v| \u2264\u2225w\u2225\u00b7\n\u221a\nt \u2264\n\u221a\nt\n\u00b5t .\nThus, f(Y ) \u2264f(Z\u22c6) + \u01eb \u2264\u01eb, so the entire ball\nB(Z\u22c6,\n4\u03b4\n5\n\u221a\ntK\u03b2t ) is feasible for the relaxed program.\nBy Lemma 8, in O(t2K2 \u00b7 log( tK\n\u03b4 )) iterations of the\nellipsoid algorithm, we obtain one of the following:\n1. we either \ufb01nd a point Z \u2208C2\u03b4 such that f(Z) \u2264\u01eb,\ni.e.\nE\nx\u223ch\n\"X\na\nZ(x, a)\nW \u2032(x, a)\n#\n\u2265max{4K, \u03b2t\u2206(Z)2} + 2\u01eb,\n2. or else we conclude that the original convex pro-\ngram (E.6,E.7) is infeasible, i.e. for all Z \u2208C, we\nhave\nE\nx\u223ch\n\"X\na\nZ(x, a)\nW \u2032(x, a)\n#\n\u2264max{4K, \u03b2t\u2206(Z)2} + 3\u01eb.\nThe total number of invocations of iterations is\nbounded by O(t2K2 \u00b7 log( tK\n\u03b4 )) \u00b7 O( t\n\u03b42 ) = O( t3K2\n\u03b42\n\u00b7\nlog( tK\n\u03b4 )).\nLemma 31. Suppose we are given a point Z \u2208C2\u03b4\nsuch that\nE\nx\u223ch\n\"X\na\nZ(x, a)\nW \u2032(x, a)\n#\n\u2265max{4K, \u03b2t\u2206(Z)2} + 2\u01eb.\nThen we can construct a hyperplane separating W\nfrom all feasible points for A\u2032.\nProof. For notational convenience, de\ufb01ne the function\nfZ(W) := E\nx\u223ch\n\"X\na\nZ(x, a)\nW \u2032(x, a)\n#\n\u2212max{4K, \u03b2t\u2206(Z)2}\u22122\u01eb.\nNote that it is a convex function of W. Note that for\nany point U that is feasible for A\u2032, we have fZ(U) \u2264\n\u2212\u01eb, whereas fZ(W) \u22650. Thus, by Lemma 10, we can\nconstruct the desired separating hyperplane.\nWe can \ufb01nally prove Theorem 11:\nProof. [Theorem 11.] We run the ellipsoid algorithm\nstarting with the ball B(0,\n\u221a\nt + \u03b4).\nAt each point,\nwe are given a candidate solution W for program A\u2032.\nWe check for violation of constraint (E.3) \ufb01rst.\nIf\nit is violated, the constraint, being linear, gives us\na separating hyperplane. Else, we use Lemma 29 to\ncheck for violation of constraint (E.4). If W /\u2208C2\u03b4,\nthen we can construct a separating hyperplane. Else,\nwe use Lemmas 30 and 31 to check for violation of\nconstraint (E.5).\nIf there is a Z \u2208C such that\nEx\u223ch\nhP\na\nZ(x,a)\nW \u2032(x,a)\ni\n\u2265max{4K, \u03b2t\u2206(Z)2} + 3\u01eb, then\nwe can \ufb01nd a separating hyperplane.\nElse, we con-\nclude that the current point W satis\ufb01es the following\nconstraints:\n\u2206(W) \u2264s + \u03b3\n\u2200Z \u2208C : E\nx\u223ch\n\"X\na\nZ(x, a)\nW \u2032(x, a)\n#\n\u2264max{4K, \u03b2t\u2206(Z)2} + 3\u01eb\nW \u2208C2\u03b4\nWe can then use the perceptron-based algorithm of\nLemma 29 to \u201cround\u201d W to an explicit distribution P\nover policies in \u03a0 such that WP satis\ufb01es \u2225WP \u2212W\u2225\u2264\n2\u03b4. Then Lemma 26 implies the stated bounds for WP .\nBy Lemma 8, in O(t2K2 log( t\n\u03b4 )) iterations of the el-\nlipsoid algorithm, we \ufb01nd the point W satisfying the\nconstraints given above, or declare correctly that A is\ninfeasible. In the worst case, we might have to run the\nalgorithm of Lemma 30 in every iteration, leading to an\nupper bound of O(t2K2 log( t\n\u03b4)) \u00d7 O( t3K2\n\u03b42\n\u00b7 log( tK\n\u03b4 )) =\nO(t5K4 log2( tK\n\u03b4 )) on the number of iterations.\n",
        "sentence": " To prove the variance deviation bound, we next use a discretization lemma from Dud\u0131\u0301k et al. [2011], which immediately implies that for any P , there exists a distribution P \u2032 supported on at most Nt policies such that for ct > 0, if Nt \u2265 6 \u03b32 t \u03bctpmin :",
        "context": "Z\u2208Z min\nW\u2208W f(W, Z) .\nC\nEmpirical Variance Bounds\nIn this section we prove Theorem 6. We \ufb01rst show uni-\nform convergence for a certain class of policy distribu-\ntions (Lemma 15), and argue that each distribution P\nTheorem 6 such that, with probability at least\n1 \u2212\u03b4,\nE\nxt\u223cDX\n\u0014\n1\nW \u2032\nt(\u03c0(xt))\n\u0015\n\u2264(1 + \u01eb)\nE\nx\u223cht\u22121\n\u0014\n1\nWPt,\u00b5t(x, \u03c0(x))\n\u0015\n+ \u03c1K\n(D.1)\nfor all \u03c0 \u2208\u03a0 and all t \u2265t1, where WP,\u00b5(x, \u00b7) is\nthe distribution over A given by\nWP,\u00b5(x, a) .= (1 \u2212K\u00b5)WP (x, a) + \u00b5,\nt \u22121\n!\n= O\n r\nK log(Nt/\u03b4)\nt\n!\n.\nThis lemma implies that the algorithm is always able\nto select a distribution over the policies that focuses\nmostly on the policies with low estimated regret.\nMoreover, the variance constraints ensure that good"
    },
    {
        "title": "Parametric bandits: The generalized linear case",
        "author": [
            "Sarah Filippi",
            "Olivier Cappe",
            "Aur\u00e9lien Garivier",
            "Csaba Szepesv\u00e1ri"
        ],
        "venue": "In Advances in Neural Information Processing Systems,",
        "citeRegEx": "Filippi et al\\.,? \\Q2010\\E",
        "shortCiteRegEx": "Filippi et al\\.",
        "year": 2010,
        "abstract": "",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "The on-line shortest path problem under partial monitoring",
        "author": [
            "Andr\u00e1s Gy\u00f6rgy",
            "Tam\u00e1s Linder",
            "G\u00e1bor Lugosi",
            "Gy\u00f6rgy Ottucs\u00e1k"
        ],
        "venue": "Journal of Machine Learning Research,",
        "citeRegEx": "Gy\u00f6rgy et al\\.,? \\Q2007\\E",
        "shortCiteRegEx": "Gy\u00f6rgy et al\\.",
        "year": 2007,
        "abstract": "",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "Non-stochastic bandit slate problems",
        "author": [
            "Satyen Kale",
            "Lev Reyzin",
            "Robert E Schapire"
        ],
        "venue": "In Advances in Neural Information Processing Systems,",
        "citeRegEx": "Kale et al\\.,? \\Q2010\\E",
        "shortCiteRegEx": "Kale et al\\.",
        "year": 2010,
        "abstract": "",
        "full_text": "",
        "sentence": " , 2014] or slate bandits [Kale et al., 2010] in the literature. In comparison with related work, our bound matches the result of Kale et al. [2010] on slate bandits, which falls into a special case of our setting. In comparison with related work, our bound matches the result of Kale et al. [2010] on slate bandits, which falls into a special case of our setting. Specifically, they assume that weights w = 1 and that uniform exploration is possible, and they obtain an \u00d5( \u221a KLT ln |\u03a0|) regret bound. Theorem 1 matches this bound, as \u2016w\u20162 = \u2016w\u20161 = L and pmin = L in this case. Our result improves on theirs in two directions: statistically we show how a non-uniform weight vector and restricted exploration distribution affects the regret and, computationally, our algorithm can be efficiently implemented with an optimization oracle while theirs cannot. When uniform exploration is not allowed, as considered by Kveton et al. [2015] in the non-contextual setting, we can set pmin = 1 and our bound is worse than theirs by a factor of \u221a L.",
        "context": null
    },
    {
        "title": "Matroid bandits: Fast combinatorial optimization with learning",
        "author": [
            "Branislav Kveton",
            "Zheng Wen",
            "Azin Ashkan",
            "Hoda Eydgahi",
            "Brian Eriksson"
        ],
        "venue": "In Uncertainty and Artificial Intelligence,",
        "citeRegEx": "Kveton et al\\.,? \\Q2014\\E",
        "shortCiteRegEx": "Kveton et al\\.",
        "year": 2014,
        "abstract": "A matroid is a notion of independence in combinatorial optimization which is\nclosely related to computational efficiency. In particular, it is well known\nthat the maximum of a constrained modular function can be found greedily if and\nonly if the constraints are associated with a matroid. In this paper, we bring\ntogether the ideas of bandits and matroids, and propose a new class of\ncombinatorial bandits, matroid bandits. The objective in these problems is to\nlearn how to maximize a modular function on a matroid. This function is\nstochastic and initially unknown. We propose a practical algorithm for solving\nour problem, Optimistic Matroid Maximization (OMM); and prove two upper bounds,\ngap-dependent and gap-free, on its regret. Both bounds are sublinear in time\nand at most linear in all other quantities of interest. The gap-dependent upper\nbound is tight and we prove a matching lower bound on a partition matroid\nbandit. Finally, we evaluate our method on three real-world problems and show\nthat it is practical.",
        "full_text": "Matroid Bandits: Fast Combinatorial Optimization with Learning\nBranislav Kveton, Zheng Wen, Azin Ashkan, Hoda Eydgahi, and Brian Eriksson\nTechnicolor Labs\nLos Altos, CA\n{branislav.kveton,zheng.wen,azin.ashkan,hoda.eydgahi,brian.eriksson}@technicolor.com\nAbstract\nA matroid is a notion of independence in combi-\nnatorial optimization which is closely related to\ncomputational ef\ufb01ciency. In particular, it is well\nknown that the maximum of a constrained mod-\nular function can be found greedily if and only if\nthe constraints are associated with a matroid. In\nthis paper, we bring together the ideas of bandits\nand matroids, and propose a new class of combi-\nnatorial bandits, matroid bandits. The objective\nin these problems is to learn how to maximize a\nmodular function on a matroid. This function is\nstochastic and initially unknown. We propose a\npractical algorithm for solving our problem, Op-\ntimistic Matroid Maximization (OMM); and prove\ntwo upper bounds, gap-dependent and gap-free,\non its regret. Both bounds are sublinear in time\nand at most linear in all other quantities of inter-\nest. The gap-dependent upper bound is tight and\nwe prove a matching lower bound on a partition\nmatroid bandit. Finally, we evaluate our method\non three real-world problems and show that it is\npractical.\n1\nIntroduction\nCombinatorial optimization is a well-established \ufb01eld that\nhas many practical applications, ranging from resource al-\nlocation [14] to designing network routing protocols [20].\nModern combinatorial optimization problems are often so\nmassive that even low-order polynomial-time solutions are\nnot practical. Fortunately, many important problems, such\nas \ufb01nding a minimum spanning tree, can be solved greed-\nily. Such problems can be often viewed as optimization on\na matroid [25], a notion of independence in combinatorial\noptimization which is closely related to computational ef-\n\ufb01ciency. In particular, it is well known that the maximum\nof a constrained modular function can be found greedily if\nand only if all feasible solutions to the problem are the in-\ndependent sets of a matroid [8]. Matroids are common in\npractice because they generalize many notions of indepen-\ndence, such as linear independence and forests in graphs.\nIn this paper, we propose an algorithm for learning how to\nmaximize a stochastic modular function on a matroid. The\nmodular function is represented as the sum of the weights\nof up to K items, which are chosen from the ground set E\nof a matroid, which has L items. The weights of the items\nare stochastic and represented as a vector w \u2208[0, 1]L. The\nvector w is drawn i.i.d. from a probability distribution P.\nThe distribution P is initially unknown and we learn it by\ninteracting repeatedly with the environment.\nMany real-world optimization problems can be formulated\nin our setting, such as building a spanning tree for network\nrouting [20]. When the delays on the links of the network\nare stochastic and their distribution is known, this problem\ncan be solved by \ufb01nding a minimum spanning tree. When\nthe distribution is unknown, it must be learned, perhaps by\nexploring routing networks that seem initially suboptimal.\nWe return to this problem in our experiments.\nThis paper makes three main contributions. First, we bring\ntogether the concepts of matroids [25] and bandits [15, 3],\nand propose a new class of combinatorial bandits, matroid\nbandits. On one hand, matroid bandits can be viewed as a\nnew learning framework for a broad and important class of\ncombinatorial optimization problems. On the other hand,\nmatroid bandits are a class of K-step bandit problems that\ncan be solved both computationally and sample ef\ufb01ciently.\nSecond, we propose a simple greedy algorithm for solving\nour problem, which explores based on the optimism in the\nface of uncertainty. We refer to our approach as Optimistic\nMatroid Maximization (OMM). OMM is both computationally\nand sample ef\ufb01cient. In particular, the time complexity of\nOMM is O(L log L) per episode, comparable to that of sort-\ning L numbers. Moreover, the expected cumulative regret\nof OMM is sublinear in the number of episodes, and at most\nlinear in the number of items L and the maximum number\nof chosen items K.\nFinally, we evaluate OMM on three real-world problems. In\narXiv:1403.5045v3  [cs.LG]  16 Jun 2014\nthe \ufb01rst problem, we learn routing networks. In the second\nproblem, we learn a policy for assigning loans in a micro-\n\ufb01nance network that maximizes chances that the loans are\nrepaid. In the third problem, we learn a movie recommen-\ndation policy. All three problems can be solved ef\ufb01ciently\nin our framework. This demonstrates that OMM is practical\nand can solve a wide range of real-world problems.\nWe adopt the following notation. We write A + e instead\nof A \u222a{e}, and A + B instead of A \u222aB. We also write\nA \u2212e instead of A \\ {e}, and A \u2212B instead of A \\ B.\n2\nMatroids\nA matroid is a pair M = (E, I), where E = {1, . . . , L} is\na set of L items, called the ground set, and I is a family of\nsubsets of E, called the independent sets. The family I is\nde\ufb01ned by the following properties. First, \u2205\u2208I. Second,\nevery subset of an independent set is independent. Finally,\nfor any X \u2208I and Y \u2208I such that |X| = |Y | + 1, there\nmust exist an item e \u2208X \u2212Y such that Y + e \u2208I. This\nis known as the augmentation property. We denote by:\nE(X) = {e : e \u2208E \u2212X, X + e \u2208I}\n(1)\nthe set of items that can be added to set X such that the set\nremains independent.\nA set is a basis of a matroid if it is a maximal independent\nset. All bases of a matroid have the same cardinality [25],\nwhich is known as the rank of a matroid. In this work, we\ndenote the rank by K.\nA weighted matroid is a matroid associated with a vector\nof non-negative weights w \u2208(R+)L. The e-th entry of w,\nw(e), is the weight of item e. We denote by:\nf(A, w) =\nX\ne\u2208A\nw(e)\n(2)\nthe sum of the weights of all items in set A. The problem\nof \ufb01nding a maximum-weight basis of a matroid:\nA\u2217= arg max\nA\u2208I\nf(A, w) = arg max\nA\u2208I\nX\ne\u2208A\nw(e)\n(3)\nis a well-known combinatorial optimization problem. This\nproblem can be solved greedily (Algorithm 1). The greedy\nalgorithm has two main stages. First, A\u2217is initialized to \u2205.\nSecond, all items in the ground set are sorted according to\ntheir weights, from the highest to the lowest, and greedily\nadded to A\u2217in this order. The item is added to the set A\u2217\nonly if it does not make the set dependent.\n3\nMatroid Bandits\nA minimum spanning tree is a maximum-weight basis of a\nmatroid. The ground set E of this matroid are the edges of\nAlgorithm 1 The greedy method for \ufb01nding a maximum-\nweight basis of a matroid [8].\nInput: Matroid M = (E, I), weights w\nA\u2217\u2190\u2205\nLet e1, . . . , eL be an ordering of items such that:\nw(e1) \u2265. . . \u2265w(eL)\nfor all i = 1, . . . , L do\nif (ei \u2208E(A\u2217)) then\nA\u2217\u2190A\u2217+ ei\nend if\nend for\na graph. A set of edges is considered to be independent if\nit does not contain a cycle. Each edge e is associated with\na weight w(e) = umax \u2212u(e), where umax = maxe u(e)\nand u(e) is the weight of edge e in the original graph.\nThe minimum spanning tree cannot be computed when the\nweights w(e) of the edges are unknown. This may happen\nin practice. For instance, consider the problem of building\na routing network, which is represented as a spanning tree,\nwhere the expected delays on the links of the network are\ninitially unknown. In this work, we study a variant of max-\nimizing a modular function on a matroid that can address\nthis kind of problems.\n3.1\nModel\nWe formalize our learning problem as a matroid bandit. A\nmatroid bandit is a pair (M, P), where M is a matroid and\nP is a probability distribution over the weights w \u2208RL of\nitems E in M. The e-th entry of w, w(e), is the weight of\nitem e. The weights w are stochastic and drawn i.i.d. from\nthe distribution P. We denote the expected weights of the\nitems by \u00afw = E[w] and assume that each of these weights\nis non-negative, \u00afw(e) \u22650 for all e \u2208E.\nEach item e is associated with an arm and we assume that\nmultiple arms can be pulled. A subset of arms A \u2286E can\nbe pulled if and only if A is an independent set. The return\nfor pulling arms A is f(A, w) (Equation 2), the sum of the\nweights of all items in A. After the arms A are pulled, we\nobserve the weight of each item in A, w(e) for all e \u2208A.\nThis model of feedback is known as semi-bandit [2].\nWe assume that the matroid M is known and that the dis-\ntribution P is unknown. Without loss of generality, we as-\nsume that the support of P is a bounded subset of [0, 1]L.\nWe would like to stress that we do not make any structural\nassumptions on P.\nThe optimal solution to our problem is a maximum-weight\nbasis in expectation:\nA\u2217= arg max\nA\u2208I\nEw[f(A, w)] = arg max\nA\u2208I\nX\ne\u2208A\n\u00afw(e). (4)\nAlgorithm 2 OMM: Optimistic matroid maximization.\nInput: Matroid M = (E, I)\n// Initialization\nObserve w0 \u223cP\n\u02c6we,1 \u2190w0(e)\n\u2200e \u2208E\nTe(0) \u21901\n\u2200e \u2208E\nfor all t = 1, . . . , n do\n// Compute UCBs\nUt(e) \u2190\u02c6we,Te(t\u22121) + ct\u22121,Te(t\u22121)\n\u2200e \u2208E\n// Find a maximum-weight basis with respect to Ut\nAt \u2190\u2205\nLet et\n1, . . . , et\nL be an ordering of items such that:\nUt(et\n1) \u2265. . . \u2265Ut(et\nL)\nfor all i = 1, . . . , L do\nif (et\ni \u2208E(At)) then\nAt \u2190At + et\ni\nend if\nend for\nObserve {wt(e) : e \u2208At}, where wt \u223cP\n// Update statistics\nTe(t) \u2190Te(t \u22121)\n\u2200e \u2208E\nTe(t) \u2190Te(t) + 1\n\u2200e \u2208At\n\u02c6we,Te(t) \u2190Te(t \u22121) \u02c6we,Te(t\u22121) + wt(e)\nTe(t)\n\u2200e \u2208At\nend for\nThe above optimization problem is equivalent to the prob-\nlem in Equation 3. Therefore, it can be solved greedily by\nAlgorithm 1 when the expected weights \u00afw are known.\nOur learning problem is episodic. In episode t, we choose\na basis At and gain f(At, wt), where wt is the realization\nof the stochastic weights in episode t. Our goal is to learn\na policy, a sequence of bases, that minimizes the expected\ncumulative regret in n episodes:\nR(n) = Ew1,...,wn\n\" n\nX\nt=1\nRt(wt)\n#\n,\n(5)\nwhere Rt(wt) = f(A\u2217, wt) \u2212f(At, wt) is the regret in\nepisode t.\n3.2\nAlgorithm\nOur solution is designed based on the optimism in the face\nof uncertainty principle [17]. In particular, it is a variant of\nthe greedy method for \ufb01nding a maximum-weight basis of\na matroid where the expected weight \u00afw(e) of each item e\nis substituted with its optimistic estimate Ut(e). Therefore,\nwe refer to our approach as Optimistic Matroid Maximiza-\ntion (OMM).\nThe pseudocode of our algorithm is given in Algorithm 2.\nThe algorithm can be summarized as follows. First, at the\nbeginning of each episode t, we compute the upper con\ufb01-\ndence bound (UCB) on the weight of each item e:\nUt(e) = \u02c6we,Te(t\u22121) + ct\u22121,Te(t\u22121),\n(6)\nwhere \u02c6we,Te(t\u22121) is our estimate of \u00afw(e) at the beginning\nof episode t, ct\u22121,Te(t\u22121) represents the radius of the con-\n\ufb01dence interval around this estimate, and Te(t \u22121) is the\nnumber of times that OMM chooses item e before episode t.\nSecond, we order all items e by their UCBs (Equation 6),\nfrom the highest to the lowest, and then add them greedily\nto At in this order. The item is added to the set At only if\nit does not make the set dependent. Finally, we choose the\nbasis At, observe the weights of all items in the basis, and\nupdate our model \u02c6w of the world.\nThe radius:\nct,s =\np\n2 log(t)/s\n(7)\nis de\ufb01ned such that each upper con\ufb01dence bound Ut(e) is\nwith high probability an upper bound on the weight \u00afw(e).\nThe role of the UCBs is to encourage exploration of items\nthat are not chosen very often. As the number of episodes\nincreases, the estimates of the weights \u00afw improve and OMM\nstarts exploiting best items. The log(t) term increases with\ntime t and enforces exploration, to avoid linear regret.\nOMM is a greedy algorithm and therefore is extremely com-\nputationally ef\ufb01cient. In particular, let the time complexity\nof checking for independence, et\ni \u2208E(At), be O(g(|At|)).\nThen the time complexity of OMM is O(L(log L + g(K)))\nper episode, comparable to that of sorting L numbers. The\ndesign of our algorithm is not surprising and is motivated\nby prior work [12]. The main challenge is to derive a tight\nupper bound on the regret of OMM, which would re\ufb02ect the\nstructure of our problem.\n4\nAnalysis\nIn this section, we analyze the regret of OMM. Our analysis\nis organized as follows. First, we introduce basic concepts\nand notation. Second, we show how to decompose the re-\ngret of OMM in a single episode. In particular, we partition\nthe regret of a suboptimal basis into the sum of the regrets\nof individual items. This part of the analysis relies heavily\non the structure of a matroid and is the most novel. Third,\nwe derive two upper bounds, gap-dependent and gap-free,\non the regret of OMM. Fourth, we prove a lower bound that\nmatches the gap-dependent upper bound. Finally, we sum-\nmarize the results of our analysis.\n4.1\nNotation\nBefore we present our results, we introduce notation used\nin our analysis. The optimal basis is A\u2217= {a\u2217\n1, . . . , a\u2217\nK}.\nWe assume that the items in A\u2217are ordered such that a\u2217\nk is\nthe k-th item with the highest expected weight. In episode\nt, OMM chooses a basis At = {at\n1, . . . , at\nK}, where at\nk is the\nk-th item chosen by OMM. We say that item e is suboptimal\nif it belongs to \u00afA\u2217= E \u2212A\u2217, the set of suboptimal items.\nFor any pair of suboptimal and optimal items, e \u2208\u00afA\u2217and\na\u2217\nk, we de\ufb01ne a gap:\n\u2206e,k = \u00afw(a\u2217\nk) \u2212\u00afw(e)\n(8)\nand use it as a measure of how dif\ufb01cult it is to discriminate\nthe items. For every item e \u2208\u00afA\u2217, we de\ufb01ne a set:\nOe = {k : \u2206e,k > 0} ,\n(9)\nthe indices of items in A\u2217whose expected weight is higher\nthan that of item e. The cardinality of Oe is Ke = |Oe|.\n4.2\nRegret Decomposition\nOur decomposition is motivated by the observation that all\nbases of a matroid are of the same cardinality. As a result,\nthe difference in the expected values of any two bases can\nbe always written as the sum of differences in the weights\nof their items. In particular:\nEw\n\u0002\nf(A\u2217, w) \u2212f(At, w)\n\u0003\n=\nK\nX\nk=1\n\u2206at\nk,\u03c0(k),\n(10)\nwhere \u03c0 : {1, . . . , K} \u2192{1, . . . , K} is an arbitrary bijec-\ntion from At to A\u2217such that \u03c0(k) is the index of the item\nin A\u2217that is paired with the k-th item in At. In this work,\nwe focus on one particular bijection.\nLemma 1. For any two matroid bases A\u2217and At, there\nexists a bijection \u03c0 : {1, . . . , K} \u2192{1, . . . , K} such that:\nn\nat\n1, . . . , at\nk\u22121, a\u2217\n\u03c0(k)\no\n\u2208I\n\u2200k = 1, . . . , K.\nIn addition, \u03c0(k) = i when at\nk = a\u2217\ni for some i.\nProof. The lemma is proved in Appendix.\nThe bijection \u03c0 in Lemma 1 has two important properties.\nFirst,\nn\nat\n1, . . . , at\nk\u22121, a\u2217\n\u03c0(k)\no\n\u2208I for all k. In other words,\nOMM can choose item a\u2217\n\u03c0(k) at step k. However, OMM selects\nitem at\nk. By the design of OMM, this can happen only when\nthe UCB of item at\nk is larger or equal to that of item a\u2217\n\u03c0(k).\nAs a result, we know that Ut(at\nk) \u2265Ut(a\u2217\n\u03c0(k)) in all steps\nk. Second, Lemma 1 guarantees that every optimal item in\nAt is paired with the same item in A\u2217.\nIn the rest of the paper, we represent the bijection \u03c0 using\nan indicator function. The indicator function:\n1e,k(t) = 1\n\b\n\u2203i : at\ni = e, \u03c0(i) = k\n\t\n(11)\nindicates the event that item e is chosen instead of item a\u2217\nk\nin episode t. Based on our new representation, we rewrite\nEquation 10 as:\nK\nX\nk=1\n\u2206at\nk,\u03c0(k) =\nX\ne\u2208\u00af\nA\u2217\nK\nX\nk=1\n\u2206e,k1e,k(t)\n\u2264\nX\ne\u2208\u00af\nA\u2217\nKe\nX\nk=1\n\u2206e,k1e,k(t)\n(12)\nand then bound it from above. The last inequality is due to\nneglecting the negative gaps.\nThe above analysis applies to any basis At in any episode\nt. The results of our analysis are summarized below.\nTheorem 1. The expected regret of choosing any basis At\nin episode t is bounded as:\nEw\n\u0002\nf(A\u2217, w) \u2212f(At, w)\n\u0003\n\u2264\nX\ne\u2208\u00af\nA\u2217\nKe\nX\nk=1\n\u2206e,k1e,k(t).\nThe indicator function 1e,k(t) indicates the event that item\ne is chosen instead of item a\u2217\nk in episode t. When the event\n1e,k(t) happens, Ut(e) \u2265Ut(a\u2217\nk). Moreover:\nX\ne\u2208\u00af\nA\u2217\nKe\nX\nk=1\n1e,k(t) \u2264K\n\u2200t\nKe\nX\nk=1\n1e,k(t) \u22641\n\u2200t, e \u2208\u00afA\u2217.\nThe last two inequalities follow from the fact that 1e,k(t)\nis a bijection from At to A\u2217, every item in the suboptimal\nbasis At is matched with one unique item in A\u2217.\nOne remarkable aspect of our regret decomposition is that\nthe exact form of the bijection is not required in the rest of\nour analysis. We only rely on the properties of 1e,k(t) that\nare stated in Theorem 1.\n4.3\nUpper Bounds\nOur \ufb01rst result is a gap-dependent bound.\nTheorem 2 (gap-dependent bound). The expected cumula-\ntive regret of OMM is bounded as:\nR(n) \u2264\nX\ne\u2208\u00af\nA\u2217\n16\n\u2206e,Ke\nlog n +\nX\ne\u2208\u00af\nA\u2217\nKe\nX\nk=1\n\u2206e,k\n4\n3\u03c02.\nProof. First, we bound the expected regret in episode t us-\ning Theorem 1:\nR(n) =\nn\nX\nt=1\nEw1,...,wt\u22121[Ewt[Rt(wt)]]\n\u2264\nn\nX\nt=1\nEw1,...,wt\u22121\n\uf8ee\n\uf8f0X\ne\u2208\u00af\nA\u2217\nKe\nX\nk=1\n\u2206e,k1e,k(t)\n\uf8f9\n\uf8fb\n=\nX\ne\u2208\u00af\nA\u2217\nKe\nX\nk=1\n\u2206e,kEw1,...,wn\n\" n\nX\nt=1\n1e,k(t)\n#\n.\n(13)\nSecond, we bound the expected cumulative regret associ-\nated with each item e \u2208\u00afA\u2217. The key idea of this step is to\ndecompose the indicator 1e,k(t) as:\n1e,k(t) = 1e,k(t)1{Te(t \u22121) \u2264\u2113e,k} +\n(14)\n1e,k(t)1{Te(t \u22121) > \u2113e,k}\nand choose \u2113e,k appropriately. By Lemma 2, the regret as-\nsociated with Te(t \u22121) > \u2113e,k is bounded as:\nKe\nX\nk=1\n\u2206e,kEw1,...,wn\n\" n\nX\nt=1\n1e,k(t)1{Te(t \u22121) > \u2113e,k}\n#\n\u2264\nKe\nX\nk=1\n\u2206e,k\n4\n3\u03c02\n(15)\nwhen \u2113e,k =\nj\n8\n\u22062\ne,k log n\nk\n. For the same value of \u2113e,k, the\nregret associated with Te(t \u22121) \u2264\u2113e,k is bounded as:\nKe\nX\nk=1\n\u2206e,kEw1,...,wn\n\" n\nX\nt=1\n1e,k(t)1{Te(t \u22121) \u2264\u2113e,k}\n#\n\u2264\nmax\nw1,...,wn\n\"\nn\nX\nt=1\nKe\nX\nk=1\n\u2206e,k1e,k(t) \u00d7\n(16)\n1\n(\nTe(t \u22121) \u2264\n8\n\u22062\ne,k\nlog n\n) #\n.\nThe next step of our proof is based on three observations.\nFirst, the gaps are ordered such that \u2206e,1 \u2265. . . \u2265\u2206e,Ke.\nSecond, by the design of OMM, the counter Te(t) increases\nwhen the event 1e,k(t) happens, for any k. Finally, by The-\norem 1, PKe\nk=1 1e,k(t) \u22641 for any given e and t. Based on\nthese facts, we bound Equation 16 from above by:\n\"\n\u2206e,1\n1\n\u22062\ne,1\n+\nKe\nX\nk=2\n\u2206e,k\n \n1\n\u22062\ne,k\n\u2212\n1\n\u22062\ne,k\u22121\n!#\n8 log n.\n(17)\nBy Lemma 3, the above term is bounded by\n16\n\u2206e,Ke\nlog n.\nFinally, we combine all of the above inequalities and get:\nKe\nX\nk=1\n\u2206e,kEw1,...,wn\n\" n\nX\nt=1\n1e,k(t)\n#\n\u2264\n16\n\u2206e,Ke\nlog n +\nKe\nX\nk=1\n\u2206e,k\n4\n3\u03c02.\n(18)\nOur main claim is obtained by summing over all subopti-\nmal items e \u2208\u00afA\u2217.\nWe also prove a gap-free bound.\nTheorem 3 (gap-free bound). The expected cumulative re-\ngret of OMM is bounded as:\nR(n) \u22648\np\nKLn log n + 4\n3\u03c02KL.\nProof. The key idea is to decompose the expected cumula-\ntive regret of OMM into two parts, where the gaps are larger\nthan \u03b5 and at most \u03b5. We analyze each part separately and\nthen set \u03b5 to get the desired result.\nLet Ke,\u03b5 be the number of optimal items whose expected\nweight is higher than that of item e by more than \u03b5 and:\nZe,k(n) = Ew1,...,wn\n\" n\nX\nt=1\n1e,k(t)\n#\n.\n(19)\nThen, based on Equation 13, the regret of OMM is bounded\nfor any \u03b5 as:\nR(n) \u2264\nX\ne\u2208\u00af\nA\u2217\nKe,\u03b5\nX\nk=1\n\u2206e,kZe,k(n) +\n(20)\nX\ne\u2208\u00af\nA\u2217\nKe\nX\nk=Ke,\u03b5+1\n\u2206e,kZe,k(n).\nThe \ufb01rst term can be bounded similarly to Equation 18:\nX\ne\u2208\u00af\nA\u2217\nKe,\u03b5\nX\nk=1\n\u2206e,kZe,k(n)\n\u2264\nX\ne\u2208\u00af\nA\u2217\n16\n\u2206e,Ke,\u03b5\nlog n +\nX\ne\u2208\u00af\nA\u2217\nKe,\u03b5\nX\nk=1\n\u2206e,k\n4\n3\u03c02\n\u226416\n\u03b5 L log n + 4\n3\u03c02KL.\n(21)\nThe second term is bounded trivially as:\nX\ne\u2208\u00af\nA\u2217\nKe\nX\nk=Ke,\u03b5+1\n\u2206e,kZe,k(n) \u2264\u03b5Kn\n(22)\nbecause all gaps \u2206e,k are bounded by \u03b5 and the maximum\nnumber of suboptimally chosen items in n episodes is Kn\n(Theorem 1). Based on our upper bounds, we get:\nR(n) \u226416\n\u03b5 L log n + \u03b5Kn + 4\n3\u03c02KL\n(23)\nand then set \u03b5 = 4\nr\nL log n\nKn\n. This concludes our proof.\n4.4\nLower Bounds\nWe derive an asymptotic lower bound on the expected cu-\nmulative regret R(n) that has the same dependence on the\ngap and n as the upper bound in Theorem 2. This bound is\nproved on a class of matroid bandits that are equivalent to\nK Bernoulli bandits.\nSpeci\ufb01cally, we prove the lower bound on a partition ma-\ntroid bandit, which is de\ufb01ned as follows. Let E be a set of\nL items and B1, . . . , BK be a partition of this set. Let the\nfamily of independent sets be de\ufb01ned as:\nI = {I \u2286E : (\u2200k : |I \u2229Bk| \u22641)} .\n(24)\nThen M = (E, I) is a partition matroid of rank K. Let P\nbe a probability distribution over the weights of the items,\nwhere the weight of each item is distributed independently\nof the other items. Let the weight of item e be drawn i.i.d.\nfrom a Bernoulli distribution with mean:\n\u00afw(e) =\n\u001a 0.5\ne = mini\u2208Bk i\n0.5 \u2212\u2206\notherwise,\n(25)\nwhere \u2206> 0. Then \u02dcB = (M, P) is our partition matroid\nbandit. The key property of \u02dcB is that it is equivalent to K\nindependent Bernoulli bandits, one for each partition. The\noptimal item in each partition is the item with the smallest\nindex, mini\u2208Bk i. All gaps are \u2206.\nTo formalize our result, we need to introduce the notion of\nconsistent algorithms. We say that the algorithm is consis-\ntent if for any matroid bandit, any suboptimal e \u2208\u00afA\u2217, and\nany \u03b1 > 0, E[Te(n)] = o(n\u03b1), where Te(n) is the number\nof times that item e is chosen in n episodes. In the rest of\nour analysis, we focus only on consistent algorithms. This\nis without loss of generality. In particular, by de\ufb01nition, an\ninconsistent algorithm performs poorly on some problems,\nand therefore extremely well on others. Because of this, it\nis dif\ufb01cult to prove good problem-dependent lower bounds\nfor inconsistent algorithms. Our main claim is below.\nTheorem 4. For any partition matroid bandit \u02dcB that is de-\n\ufb01ned in Equations 24 and 25, and parameterized by L, K,\nand 0 < \u2206< 0.5; the regret of any consistent algorithm is\nbounded from below as:\nlim inf\nn\u2192\u221e\nR(n)\nlog n \u2265L \u2212K\n4\u2206\n.\nProof. The theorem is proved as follows:\nlim inf\nn\u2192\u221e\nR(n)\nlog n \u2265\nK\nX\nk=1\nX\ne\u2208Bk\u2212A\u2217\n\u2206\nkl(0.5 \u2212\u2206, 0.5)\n=\n(L \u2212K)\u2206\nkl(0.5 \u2212\u2206, 0.5)\n\u2265L \u2212K\n4\u2206\n,\n(26)\nwhere kl(0.5 \u2212\u2206, 0.5) is the KL divergence between two\nBernoulli variables with means 0.5 \u2212\u2206and 0.5. The \ufb01rst\ninequality is due to Theorem 2.2 [4], which is applied sep-\narately to each partition Bk. The second inequality is due\nto kl(p, q) \u2264(p\u2212q)2\nq(1\u2212q), where p = 0.5 \u2212\u2206and q = 0.5.\n4.5\nSummary of Theoretical Results\nWe prove two upper bounds on the regret of OMM, one gap-\ndependent and one gap-free. These bounds can be summa-\nrized as:\nTheorem 2\nO(L(1/\u2206) log n)\nTheorem 3\nO(\np\nKLn log n),\n(27)\nwhere \u2206= min\ne\nmin\nk\u2208Oe \u2206e,k. Both bounds are sublinear in\nthe number of episodes n, and at most linear in the rank K\nof the matroid and the number of items L. In other words,\nthey scale favorably with all quantities of interest and as a\nresult we expect them to be practical.\nOur upper bounds are reasonably tight. More speci\ufb01cally,\nthe gap-dependent upper bound in Theorem 2 matches the\nlower bound in Theorem 4, which is proved on a partition\nmatroid bandit. Furthermore, the gap-free upper bound in\nTheorem 3 matches the lower bound of Audibert et al. [2]\nin adversarial combinatorial semi-bandits, up to a factor of\n\u221alog n.\nOur gap-dependent upper bound has the same form as the\nbound of Auer et al. [3] for multi-armed bandits. This ob-\nservation suggests that the sample complexity of learning a\nmaximum-weight basis of a matroid is comparable to that\nof the multi-armed bandit. The only major difference is in\nthe de\ufb01nitions of the gaps. We conclude that learning with\nmatroids is extremely sample ef\ufb01cient.\n5\nExperiments\nOur algorithm is evaluated on three matroid bandit prob-\nlems: graphic (Section 5.1), transversal (Section 5.2), and\nlinear (Section 5.3).\nAll experiments are episodic. In each episode, OMM selects\na basis At, observes the weights of the individual items in\nthat basis, and then updates its model of the environment.\nThe performance of OMM is measured by the expected per-\nstep return in n episodes:\n1\nnEw1,...,wn\n\" n\nX\nt=1\nf(At, wt)\n#\n,\n(28)\nthe expected cumulative return in n episodes divided by n.\nOMM is compared to two baselines. The \ufb01rst baseline is the\nmaximum-weight basis A\u2217in expectation. The basis A\u2217is\ncomputed as in Equation 4 and is our notion of optimality.\n200\n400\n600\n800\n1000\n304\n305\n306\n307\n308\n309\nISP network 1221\nEpisode n\nExpected per\u2212step cost\n200\n400\n600\n800\n1000\n620\n640\n660\n680\n700\nISP network 1239\nEpisode n\n200\n400\n600\n800\n1000\n190\n195\n200\n205\n210\nISP network 1755\nEpisode n\n \n \nOptimal policy\n\u03b5\u2212greedy policy\nOMM\nFigure 1: The expected per-step cost of building three minimum spanning trees in up to 103 episodes.\nISP\nNumber\nNumber\nMinimum\nMaximum\nAverage\nOptimal\n\u03b5-greedy\nnetwork\nof nodes\nof edges\nlatency\nlatency\nlatency\npolicy\npolicy\nOMM\n1221\n108\n153\n1\n17\n2.78\n305.00\n307.42 \u00b1 0.08\n305.49 \u00b1 0.10\n1239\n315\n972\n1\n64\n3.20\n629.88\n676.74 \u00b1 2.03\n641.17 \u00b1 0.18\n1755\n87\n161\n1\n31\n2.91\n192.81\n199.49 \u00b1 0.16\n194.88 \u00b1 0.11\n3257\n161\n328\n1\n47\n4.30\n550.85\n570.35 \u00b1 0.63\n559.80 \u00b1 0.10\n3967\n79\n147\n1\n44\n5.19\n306.80\n320.30 \u00b1 0.52\n308.54 \u00b1 0.08\n6461\n141\n374\n1\n45\n6.32\n376.27\n424.78 \u00b1 1.54\n381.48 \u00b1 0.07\nTable 1: The description of six ISP networks from our experiments and the expected per-step cost of building minimum\nspanning trees on these networks in 103 episodes. All latencies and costs are in milliseconds.\nThe second baseline is an \u03b5-greedy policy, where \u03b5 = 0.1.\nThis setting of \u03b5 is common in practice and corresponds to\n10% exploration.\n5.1\nGraphic Matroid\nIn the \ufb01rst experiment, we evaluate OMM on the problem of\nlearning a routing network for an Internet service provider\n(ISP). We make the assumption that the routing network is\na spanning tree. Our objective is to learn a tree that has the\nlowest expected latency on its edges.\nOur problem can be formulated as a graphic matroid ban-\ndit. The ground set E are the edges of a graph, which rep-\nresents the topology of a network. We experiment with six\nnetworks from the RocketFuel dataset [23], which contain\nup to 300 nodes and 103 edges (Table 1). A set of edges is\nconsidered independent if it does not contain a cycle. The\nlatency of edge e is w(e) = \u00afw(e) \u22121 + \u03b5, where \u00afw(e) is\nthe expected latency, which is recorded in our dataset; and\n\u03b5 \u223cExp(1) is exponential noise. The latency \u00afw(e) ranges\nfrom one to 64 milliseconds. Our noise model is motivated\nby the following observation. The latency in ISP networks\ncan be mostly explained by geographical distances [7], the\nexpected latency \u00afw(e). The noise tends to be small, on the\norder of a few hundred microseconds, and it is unlikely to\ncause high latency.\nIn Figure 1, we report our results from three ISP networks.\nWe observe the same trends on all networks. First, the ex-\npected cost of OMM approaches that of the optimal solution\nA\u2217as the number of episodes increases. Second, OMM out-\nperforms the \u03b5-greedy policy in less than 10 episodes. The\nexpected costs of all policies on all networks are reported\nin Table 1. We observe again that OMM consistently outper-\nforms the \u03b5-greedy policy, often by a large margin.\nOMM learns quickly because all of our networks are sparse.\nIn particular, the number of edges in each network is never\nmore than four times larger than the number of edges in its\nspanning tree. Therefore, at least in theory, each edge can\nbe observed at least once in four episodes and our method\ncan learn quickly the mean latency of each edge.\n5.2\nTransversal Matroid\nIn the second experiment, we study the assignment of lend-\ning institutions (known as partners) to lenders in a micro-\n\ufb01nance setting, such as Kiva [1]. This problem can be for-\nmulated under a family of matroids, called transversal ma-\ntroids [9]. The ground set E of a transversal matroid is the\nset of left vertices of the corresponding bipartite graph, and\nthe independence set I consists of the sets of left vertices\nthat belong to all possible matchings in the graph such that\nno two edges in a matching share an endpoint. The weight\n\u00afw(e) is the weight associated with the left vertices of the\nbipartite graph. The goal is to learn a transversal of the bi-\npartite graph that maximizes the overall weight of selected\n\u2022\u202f \t\r \u00a0\t\r \u00a0\n\u2022\u202f \t\r \u00a0\t\r \u00a0\n\u2022\u202f \t\r \u00a0\t\r \u00a0\nP232\t\r \u00a0\nLoans\t\r \u00a0\nX1\t\r \u00a0\n\u2022\u202f \t\r \u00a0\t\r \u00a0\n\u2022\u202f \t\r \u00a0\t\r \u00a0\n\u2022\u202f \t\r \u00a0\t\r \u00a0\nX2\t\r \u00a0\nX3\t\r \u00a0\nX4\t\r \u00a0\nX5\t\r \u00a0\nX2e5\t\r \u00a0\nX6\t\r \u00a0\nPartners\t\r \u00a0\nP1\t\r \u00a0\nP2\t\r \u00a0\nP3\t\r \u00a0\nLenders\t\r \u00a0\nL1\t\r \u00a0\n\u2022\u202f \t\r \u00a0\t\r \u00a0\n\u2022\u202f \t\r \u00a0\t\r \u00a0\n\u2022\u202f \t\r \u00a0\t\r \u00a0\nL2\t\r \u00a0\nL51\t\r \u00a0\nKiva\t\r \u00a0Bipar8te\t\r \u00a0Graph\t\r \u00a0\n3k\n6k\n9k\n12k\n15k\n0.76\n0.78\n0.8\n0.82\n0.84\n0.86\n0.88\n0.9\nEpisode n\nExpected per\u2212step return\n \n \nOptimal policy\n\u03b5\u2212greedy policy\nOMM\nPartner\n\u00afw(e)\nLender\nNum of\nAvg\nid\nid\npartners\nrate\n46\n1.0\n31\n200\n0.728\n70\n1.0\n2\n2\n0.924\n72\n1.0\n20\n195\n0.725\n88\n1.0\n23\n207\n0.724\n168\n0.983\n44\n49\n0.712\n231\n0.981\n48\n186\n0.723\n179\n0.970\n24\n149\n0.743\n157\n0.951\n10\n180\n0.735\n232\n0.940\n40\n10\n0.718\n123\n0.934\n42\n113\n0.721\n142\n0.925\n7\n168\n0.745\n130\n0.919\n32\n23\n0.690\n(a)\n(b)\n(c)\nFigure 2: (a) The Kiva dataset can be modeled as a bipartite graph connecting lenders to \ufb01eld partners, which, in turn, fund\nseveral loans in the region. (b) The expected per-step return of \ufb01nding maximum weight transversal in up to 15k episodes.\n(c) Top 12 selected partners assigned based on their mean success rate in the optimal solution A\u2217. The optimal solution\ninvolves 46 partner/lender assignments.\nleft vertices.\nWe used a sample of 194, 876 loans from the Kiva micro\ufb01-\nnance dataset [1], and created a bipartite graph. Every loan\nis handled by a partner (Figure 2-a). There are a total of\n232 partners in the dataset that represent the left vertices\nof the bipartite graph and therefore the ground set E of\nthe matroid. There are 286, 874 lenders in the dataset. We\ngrouped these lenders into 51 clusters according to the their\nlocation: 50 representing each individual state in United\nStates, and 1 representing all foreign lenders. These 51\nlender clusters constitute the right vertices of the bipartite\ngraph. There is an edge between a partner and a lender if\nthe lender is among the top 50% supporters of the partner,\nresulting in approximately 5k edges in the bipartite graph.\nThe weight \u00afw(e) is the probability that a loan handled by\npartner e will be paid back. We estimate it from the dataset\nas \u00afw(e) =\n1\nnl\nPnl\ni=1 wi(e), where nl is the number of loans\nhandled by this partner. We assume wi(e) is 0.7 if the loan\ni is in repayment, 1 if it is paid, and 0 otherwise. At the\nbeginning of each episode, we choose the loan i at random.\nThe optimal solution A\u2217is a transversal in the graph that\nmaximizes the overall success rate of the selected partners.\nTop twelve partners selected based on their mean success\nrate in the optimal solution are shown in Figure 2-c. For\neach partner, the id of the lender to which this partner was\nassigned along with the number of eligible partners of the\nlender and their average success rate are listed in the Table.\nThe objective of OMM and \u03b5-greedy policies is similar to the\noptimal policy with the difference that success rates (i.e.\nw(e)) are not known beforehand, and they must be learned\nby interacting repeatedly with the environment. Compari-\nson results of the three policies are reported in Figure 2-b.\nSimilar to the previous experiment, we observe the follow-\ning trends. First, the expected return of OMM approaches\nthat of the optimal solution A\u2217as the number of episodes\nincreases. Second, OMM outperforms the \u03b5-greedy policy.\n5.3\nLinear Matroid\nIn the last experiment, we evaluate OMM on the problem of\nlearning a set of diverse and popular movies. This kind of\nmovies is typically recommended by existing content rec-\nommender systems. The movies are popular, and therefore\nthe user is likely to choose them. The movies are diverse,\nand therefore cover many potential interests of the user.\nOur problem can be formulated as a linear matroid bandit.\nThe ground set E are movies from the MovieLens dataset\n[16], a dataset of 6 thousand people who rated one million\nmovies. We restrict our attention to 25 most rated movies\nand 75 movies that are not well known. So the cardinality\nof E is 100. For each movie e, we de\ufb01ne a feature vector\nue \u2208{0, 1}18, where ue(j) indicates that movie e belongs\nto genre j. A set of movies X is considered independent if\nfor any movie e \u2208X, the vector ue cannot be written as a\nlinear combination of the feature vectors of the remaining\nmovies in X. This is our notion of diversity. The expected\nweight \u00afw(e) is the probability that movie e is chosen. We\nestimate it as \u00afw(e) =\n1\nnp\nPnp\ni=1 wi(e), where wi(e) is the\nindicator that person i rated movie e and np is the number\nof people in our dataset. At the beginning of each episode,\nwe choose the person i at random.\nTwelve most popular movies from the optimal solution A\u2217\nare listed in Figure 3. These movies cover a wide range of\nmovie genres and appear to be diverse. This validates our\nassumption that linear independence is suitable for model-\ning diversity. The expected return of OMM is reported in the\nsame \ufb01gure. We observe the same trends as in the previous\nexperiments. More speci\ufb01cally, the expected return of OMM\nMovie title\n\u00afw(e)\nMovie genres\nAmerican Beauty\n0.568\nComedy Drama\nJurassic Park\n0.442\nAction Adventure Sci-Fi\nSaving Private Ryan\n0.439\nAction Drama War\nMatrix\n0.429\nAction Sci-Fi Thriller\nBack to the Future\n0.428\nComedy Sci-Fi\nSilence of the Lambs\n0.427\nDrama Thriller\nMen in Black\n0.420\nAction Adventure Comedy Sci-Fi\nFargo\n0.416\nCrime Drama Thriller\nShakespeare in Love\n0.392\nComedy Romance\nL.A. Con\ufb01dential\n0.379\nCrime Film-Noir Mystery Thriller\nE.T. the Extra-Terrestrial\n0.376\nChildren\u2019s Drama Fantasy Sci-Fi\nGhostbusters\n0.361\nComedy Horror\n10k\n20k\n30k\n40k\n50k\n4.2\n4.4\n4.6\n4.8\n5\n5.2\n5.4\n5.6\nEpisode n\nExpected per\u2212step return\n \n \nOptimal policy\n\u03b5\u2212greedy policy\nOMM\nFigure 3: Left. Twelve most popular movies in the optimal solution A\u2217. The optimal solution involves 17 movies. Right.\nThe expected per-step return of three movie recommendation policies in up to 50k episodes.\napproaches that of A\u2217as the number of episodes increases\nand OMM outperforms the \u03b5-greedy policy in 10k episodes.\n6\nRelated Work\nOur problem can be viewed as a stochastic combinatorial\nsemi-bandit [12], where all feasible solutions are the inde-\npendent sets of a matroid. Stochastic combinatorial semi-\nbandits were pioneered by Gai et al. [12], who proposed a\nUCB algorithm for solving these problems. Chen et al. [6]\nproved that the expected cumulative regret of this method\nis O(K2L(1/\u2206) log n). Our gap-dependent regret bound\nis O(L(1/\u2206) log n), a factor of K2 tighter than the bound\nof Chen et al. [6]. Our analysis relies heavily on the prop-\nerties of our problem and therefore we can derive a much\ntighter bound.\nCOMBAND [5], OSMD [2], and FPL [19] are algorithms\nfor adversarial combinatorial semi-bandits. The main limi-\ntation of COMBAND and OSMD is that they are not guar-\nanteed to be computationally ef\ufb01cient. More speci\ufb01cally,\nCOMBAND needs to sample from a distribution over ex-\nponentially many solutions and OSMD needs to project to\nthe convex hull of these solutions. FPL is computationally\nef\ufb01cient but not very practical because its time complexity\nincreases with time. On the other hand, OMM is guaranteed\nto be computationally ef\ufb01cient but can only solve a special\nclass of combinatorial bandits, matroid bandits.\nMatroids are a broad and important class of combinatorial\noptimization problems [21], which has been an active area\nof research for the past 80 years. This is the \ufb01rst paper that\nstudies a well-known matroid problem in the bandit setting\nand proposes a learning algorithm for solving it.\nOur work is also related to submodularity [18]. In particu-\nlar, let:\ng(X) =\nmax\nY :Y \u2286X,Y \u2208I f(Y, \u00afw)\n(29)\nbe the maximum weight of an independent set in X. Then\nit is easy to show that g(X) is submodular and monotonic\nin X, and that the maximum-weight basis of a matroid is a\nsolution to A\u2217= arg maxA:|A|=K g(A). Many algorithms\nfor learning how to maximize a submodular function have\nbeen proposed recently [13, 26, 10, 24, 11]. None of these\nalgorithms are suitable for solving our problem. There are\ntwo reasons. First, each algorithm is designed to maximize\na speci\ufb01c submodular function and our function g may not\nbe of that type. Second, the algorithms are only near opti-\nmal, learn a set A such that g(A) \u2265(1 \u22121/e)g(A\u2217). Note\nthat our method is guaranteed to be optimal and learn A\u2217.\n7\nConclusions\nThis is the \ufb01rst work that studies the problem of learning a\nmaximum-weight basis of a matroid, where the weights of\nthe items are initially unknown, and have to be learned by\ninteracting repeatedly with the environment. We propose a\npractical algorithm for solving this problem and bound its\nregret. The regret is sublinear in time and at most linear in\nall other quantities of interest. We evaluate our method on\nthree real-world problems and show that it is practical.\nOur regret bounds are \u2126(\n\u221a\nL). Therefore, OMM is not prac-\ntical when the number of items L is large. We believe that\nthese kinds of problems can be solved ef\ufb01ciently by intro-\nducing additional structure, such as linear generalization.\nIn this case, the weight of each item would be modeled as\na linear function of its features and the goal is to learn the\nparameters of this function.\nMany combinatorial optimization problems can be viewed\nas optimization on a matroid or its generalizations, such as\nmaximum-weight matching on a bipartite graph and mini-\nmum cost \ufb02ows. In a sense, these are the hardest problems\nin combinatorial optimization that can be solved optimally\nin polynomial time [22]. In this work, we show that one of\nthese problems is ef\ufb01ciently learnable. We believe that the\nkey ideas in our work are quite general and can be applied\nto other problems that involve matroids.\nReferences\n[1] KIVA. http://build.kiva.org/docs/data, 2013.\n[2] Jean-Yves Audibert, Sebastien Bubeck, and Gabor\nLugosi. Regret in online combinatorial optimization.\nMathematics of Operations Research, 39(1):31\u201345,\n2014.\n[3] Peter Auer, Nicolo Cesa-Bianchi, and Paul Fischer.\nFinite-time analysis of the multiarmed bandit prob-\nlem. Machine Learning, 47:235\u2013256, 2002.\n[4] S\u00b4ebastien Bubeck and Nicol`o Cesa-Bianchi. Regret\nanalysis of stochastic and nonstochastic multi-armed\nbandit problems. Foundations and Trends in Machine\nLearning, 2012.\n[5] Nicol`o Cesa-Bianchi and G\u00b4abor Lugosi. Combina-\ntorial bandits. Journal of Computer and System Sci-\nences, 78(5):1404\u20131422, 2012.\n[6] Wei Chen, Yajun Wang, and Yang Yuan. Combinato-\nrial multi-armed bandit: General framework and ap-\nplications. In Proceedings of the 30th International\nConference on Machine Learning, pages 151\u2013159,\n2013.\n[7] Baek-Young Choi, Sue Moon, Zhi-Li Zhang, Kon-\nstantina Papagiannaki, and Christophe Diot. Analysis\nof point-to-point packet delay in an operational net-\nwork. In Proceedings of the 23rd Annual Joint Con-\nference of the IEEE Computer and Communications\nSocieties, 2004.\n[8] Jack Edmonds. Matroids and the greedy algorithm.\nMathematical Programming, 1(1):127\u2013136, 1971.\n[9] Jack Edmonds and Delbert Ray Fulkerson. Transver-\nsals and matroid partition.\nJournal of Research of\nthe National Bureau of Standards, 69B(3):147\u2013153,\n1965.\n[10] Victor Gabillon, Branislav Kveton, Zheng Wen, Brian\nEriksson, and S. Muthukrishnan. Adaptive submod-\nular maximization in bandit setting.\nIn Advances\nin Neural Information Processing Systems 26, pages\n2697\u20132705, 2013.\n[11] Victor Gabillon, Branislav Kveton, Zheng Wen, Brian\nEriksson, and S. Muthukrishnan.\nLarge-scale op-\ntimistic adaptive submodularity.\nIn Proceedings of\nthe 28th AAAI Conference on Arti\ufb01cial Intelligence,\n2014.\n[12] Yi Gai, Bhaskar Krishnamachari, and Rahul Jain.\nCombinatorial network optimization with unknown\nvariables: Multi-armed bandits with linear rewards\nand individual observations. IEEE/ACM Transactions\non Networking, 20(5):1466\u20131478, 2012.\n[13] Andrew Guillory and Jeff Bilmes. Online submodular\nset cover, ranking, and repeated active learning. In\nAdvances in Neural Information Processing Systems\n24, pages 1107\u20131115, 2011.\n[14] Naoki Katoh. Combinatorial optimization algorithms\nin resource allocation problems. Encyclopedia of Op-\ntimization, pages 259\u2013264, 2001.\n[15] T. L. Lai and Herbert Robbins. Asymptotically ef\ufb01-\ncient adaptive allocation rules. Advances in Applied\nMathematics, 6(1):4\u201322, 1985.\n[16] Shyong Lam and Jon Herlocker.\nMovieLens 1M\nDataset. http://www.grouplens.org/node/12, 2012.\n[17] R\u00b4emi Munos.\nThe optimistic principle applied to\ngames, optimization, and planning: Towards founda-\ntions of Monte-Carlo tree search. Foundations and\nTrends in Machine Learning, 2012.\n[18] G. L. Nemhauser, L. A. Wolsey, and M. L. Fisher. An\nanalysis of approximations for maximizing submod-\nular set functions - I. Mathematical Programming,\n14(1):265\u2013294, 1978.\n[19] Gergely Neu and G\u00b4abor Bart\u00b4ok. An ef\ufb01cient algo-\nrithm for learning with semi-bandit feedback. In Pro-\nceedings of the 24th International Conference on Al-\ngorithmic Learning Theory, pages 234\u2013248, 2013.\n[20] Carlos Oliveira and Panos Pardalos. A survey of com-\nbinatorial optimization problems in multicast routing.\nComputers and Operations Research, 32(8):1953\u2013\n1981, 2005.\n[21] James Oxley.\nMatroid Theory.\nOxford University\nPress, New York, NY, 2011.\n[22] Christos Papadimitriou and Kenneth Steiglitz. Com-\nbinatorial Optimization. Dover Publications, Mine-\nola, NY, 1998.\n[23] Neil Spring, Ratul Mahajan, and David Wetherall.\nMeasuring ISP topologies with Rocketfuel. IEEE /\nACM Transactions on Networking, 12(1):2\u201316, 2004.\n[24] Zheng Wen, Branislav Kveton, Brian Eriksson, and\nSandilya Bhamidipati. Sequential Bayesian search.\nIn Proceedings of the 30th International Conference\non Machine Learning, pages 977\u2013983, 2013.\n[25] Hassler Whitney. On the abstract properties of lin-\near dependence. American Journal of Mathematics,\n57(3):509\u2013533, 1935.\n[26] Yisong Yue and Carlos Guestrin. Linear submodular\nbandits and their application to diversi\ufb01ed retrieval. In\nAdvances in Neural Information Processing Systems\n24, pages 2483\u20132491, 2011.\nA\nTechnical Lemmas\nLemma 1. For any two matroid bases A\u2217and At, there exists a bijection \u03c0 : {1, . . . , K} \u2192{1, . . . , K} such that:\nn\nat\n1, . . . , at\nk\u22121, a\u2217\n\u03c0(k)\no\n\u2208I\n\u2200k = 1, . . . , K.\nIn addition, \u03c0(k) = i when at\nk = a\u2217\ni for some i.\nProof. Our proof is constructive. The key idea is to exchange items in At for items in A\u2217in backward order, from at\nK to\nat\n1. For simplicity of exposition, we \ufb01rst assume that A\u2217\u2229At = \u2205.\nFirst, we exchange item at\nK. In particular, from the augmentation property of a matroid, we know that there exists an item\na\u2217\ni \u2208A\u2217\u2212(At \u2212at\nK) such that At \u2212at\nK + a\u2217\ni \u2208I. We choose any such item a\u2217\ni and exchange it for at\nK. The result is a\nbasis:\nBK\u22121 =\nn\nat\n1, . . . , at\nK\u22121, a\u2217\n\u03c0(K)\no\n\u2208I,\n(30)\nwhere \u03c0(K) = i. Second, we apply the same idea to item at\nK\u22121. In particular, from the augmentation property, we know\nthat there exists an item a\u2217\ni \u2208A\u2217\u2212(BK\u22121 \u2212at\nK\u22121) such that BK\u22121 \u2212at\nK\u22121 + a\u2217\ni \u2208I. We select any such item a\u2217\ni and\nexchange it for at\nK\u22121. The result is another basis:\nBK\u22122 =\nn\nat\n1, . . . , at\nK\u22122, a\u2217\n\u03c0(K\u22121), a\u2217\n\u03c0(K)\no\n\u2208I,\n(31)\nwhere \u03c0(K \u22121) = i. The same argument applies to item at\nK\u22122, all the way down to item at\n1. The result is a sequence of\nbases:\nBk\u22121 =\nn\nat\n1, . . . , at\nk\u22121, a\u2217\n\u03c0(k), . . . , a\u2217\n\u03c0(K)\no\n\u2208I\n\u2200k = 1, . . . , K.\n(32)\nOur main claim follows from the hereditary property of a matroid, any subset of an independent set is independent.\nFinally, suppose that A\u2217\u2229At \u0338= \u2205. Then our construction changes in only one step. In any step k, we set \u03c0(k) to i when\nat\nk = a\u2217\ni for some i. The items at\nk and a\u2217\ni can be always exchanged because a\u2217\ni /\u2208Bk \u2212at\nk. Otherwise, Bk would be a set\nwith two identical items, at\nk and a\u2217\ni , which contradicts to the fact that Bk is a basis.\nLemma 2. For any item e \u2208\u00afA\u2217and k \u2264Ke:\nEw1,...,wn\n\" n\nX\nt=1\n1e,k(t)1{Te(t \u22121) > \u2113}\n#\n\u22644\n3\u03c02\nwhen \u2113=\nj\n8\n\u22062\ne,k log n\nk\n.\nProof. First, note that the event 1e,k(t) implies Ut(e) \u2265Ut(a\u2217\nk) (Theorem 1). Second, by the design of OMM, the counter\nTe(t) increases when the event 1e,k(t) happens, for any k. Based on these facts, it follows that:\nn\nX\nt=1\n1e,k(t)1{Te(t \u22121) > \u2113} =\nn\nX\nt=\u2113+1\n1e,k(t)1{Te(t \u22121) > \u2113}\n\u2264\nn\nX\nt=\u2113+1\n1{Ut(e) \u2265Ut(a\u2217\nk), Te(t \u22121) > \u2113}\n\u2264\nn\nX\nt=\u2113+1\nt\nX\ns=1\nt\nX\nse=\u2113+1\n1\n\b\n\u02c6we,se + ct\u22121,se \u2265\u02c6wa\u2217\nk,s + ct\u22121,s\n\t\n=\nn\u22121\nX\nt=\u2113\nt+1\nX\ns=1\nt+1\nX\nse=\u2113+1\n1\n\b\n\u02c6we,se + ct,se \u2265\u02c6wa\u2217\nk,s + ct,s\n\t\n.\n(33)\nWhen \u02c6we,se + ct,se \u2265\u02c6wa\u2217\nk,s + ct,s, at least one of the following events must happen:\n\u02c6wa\u2217\nk,s \u2264\u00afw(a\u2217\nk) \u2212ct,s\n(34)\n\u02c6we,se \u2265\u00afw(e) + ct,se\n(35)\n\u00afw(a\u2217\nk) < \u00afw(e) + 2ct,se.\n(36)\nWe bound the probability of the \ufb01rst two events (Equations 34 and 35) using Hoeffding\u2019s inequality:\nP( \u02c6wa\u2217\nk,s \u2264\u00afw(a\u2217\nk) \u2212ct,s) \u2264exp[\u22124 log t] = t\u22124\n(37)\nP( \u02c6we,se \u2265\u00afw(e) + ct,se) \u2264exp[\u22124 log t] = t\u22124.\n(38)\nWhen se \u2265\n8\n\u22062\ne,k log n, the third event (Equation 36) cannot happen because:\n\u00afw(a\u2217\nk) \u2212\u00afw(e) \u22122ct,se = \u2206e,k \u22122\nr\n2 log t\nse\n\u22650.\n(39)\nThis is guaranteed when \u2113=\nj\n8\n\u22062\ne,k log n\nk\n. Finally, we combine all of our claims and get:\nEw1,...,wn\n\" n\nX\nt=1\n1e,k(t)1{Te(t \u22121) > \u2113}\n#\n\u2264\nn\u22121\nX\nt=\u2113\nt+1\nX\ns=1\nt+1\nX\nse=\u2113+1\n\u0002\nP( \u02c6wa\u2217\nk,s \u2264\u00afw(a\u2217\nk) \u2212ct,s) +\nP( \u02c6we,se \u2265\u00afw(e) + ct,se)\n\u0003\n\u2264\n\u221e\nX\nt=1\n2(t + 1)2t\u22124\n\u2264\n\u221e\nX\nt=1\n8t\u22122\n= 4\n3\u03c02.\n(40)\nThe last step is due to the fact that\n\u221e\nX\nt=1\nt\u22122 = \u03c02\n6 .\nLemma 3. Let \u22061 \u2265. . . \u2265\u2206K be a sequence of K positive numbers. Then:\n\"\n\u22061\n1\n\u22062\n1\n+\nK\nX\nk=2\n\u2206k\n\u0012 1\n\u22062\nk\n\u2212\n1\n\u22062\nk\u22121\n\u0013#\n\u2264\n2\n\u2206K\n.\nProof. First, we note that:\n\"\n\u22061\n1\n\u22062\n1\n+\nK\nX\nk=2\n\u2206k\n\u0012 1\n\u22062\nk\n\u2212\n1\n\u22062\nk\u22121\n\u0013#\n=\nK\u22121\nX\nk=1\n\u2206k \u2212\u2206k+1\n\u22062\nk\n+\n1\n\u2206K\n.\n(41)\nSecond, by our assumption, \u2206k \u2265\u2206k+1 for all k < K. Therefore:\nK\u22121\nX\nk=1\n\u2206k \u2212\u2206k+1\n\u22062\nk\n+\n1\n\u2206K\n\u2264\nK\u22121\nX\nk=1\n\u2206k \u2212\u2206k+1\n\u2206k\u2206k+1\n+\n1\n\u2206K\n=\nK\u22121\nX\nk=1\n\u0014\n1\n\u2206k+1\n\u22121\n\u2206k\n\u0015\n+\n1\n\u2206K\n=\n2\n\u2206K\n\u22121\n\u22061\n<\n2\n\u2206K\n.\n(42)\nThis concludes our proof.\n",
        "sentence": "",
        "context": "bandits and their application to diversi\ufb01ed retrieval. In\nAdvances in Neural Information Processing Systems\n24, pages 2483\u20132491, 2011.\nA\nTechnical Lemmas\nMatroid Theory.\nOxford University\nPress, New York, NY, 2011.\n[22] Christos Papadimitriou and Kenneth Steiglitz. Com-\nbinatorial Optimization. Dover Publications, Mine-\nola, NY, 1998.\n[23] Neil Spring, Ratul Mahajan, and David Wetherall.\nIn Advances\nin Neural Information Processing Systems 26, pages\n2697\u20132705, 2013.\n[11] Victor Gabillon, Branislav Kveton, Zheng Wen, Brian\nEriksson, and S. Muthukrishnan.\nLarge-scale op-\ntimistic adaptive submodularity.\nIn Proceedings of"
    },
    {
        "title": "Tight regret bounds for stochastic combinatorial semi-bandits",
        "author": [
            "Branislav Kveton",
            "Zheng Wen",
            "Azin Ashkan",
            "Csaba Szepesv\u00e1ri"
        ],
        "venue": "In Artificial Intelligence and Statistics,",
        "citeRegEx": "Kveton et al\\.,? \\Q2015\\E",
        "shortCiteRegEx": "Kveton et al\\.",
        "year": 2015,
        "abstract": "",
        "full_text": "",
        "sentence": " When the weights are known, can we obtain \u00d5( \u221a KLT ) regret even when the set of feasible actions are constrained, rather than \u00d5(L \u221a KT ) regret as in Theorem 1? The work on non-contextual combinatorial bandits suggests that the answer is yes [Kveton et al., 2015], but all algorithms for contextual bandit learning involve some degree of uniform exploration, which would prohibit such a regret bound.",
        "context": null
    },
    {
        "title": "The epoch-greedy algorithm for multi-armed bandits with side information",
        "author": [
            "John Langford",
            "Tong Zhang"
        ],
        "venue": "In Advances in neural information processing systems,",
        "citeRegEx": "Langford and Zhang.,? \\Q2008\\E",
        "shortCiteRegEx": "Langford and Zhang.",
        "year": 2008,
        "abstract": "",
        "full_text": "",
        "sentence": " Except for the work of Qin et al. [2014], all research on semi-bandits assumes that the reward for the composite action is the sum of the features for the simple actions. Except for the work of Qin et al. [2014], all research on semi-bandits assumes that the reward for the composite action is the sum of the features for the simple actions. Qin et al. [2014] generalize this slightly by assuming that the reward is a known function of the context and features. The algorithm requires knowledge of the time horizon T , which can be relaxed by variants of the Epoch-Greedy [Langford and Zhang, 2008] or -greedy approaches, although the analysis here is significantly simpler.",
        "context": null
    },
    {
        "title": "A contextual-bandit approach to personalized news article recommendation",
        "author": [
            "Lihong Li",
            "Wei Chu",
            "John Langford",
            "Robert E. Schapire"
        ],
        "venue": "In International Conference on World Wide Web,",
        "citeRegEx": "Li et al\\.,? \\Q2010\\E",
        "shortCiteRegEx": "Li et al\\.",
        "year": 2010,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "Contextual combinatorial bandit and its application on diversified online recommendation",
        "author": [
            "Lijing Qin",
            "Shouyuan Chen",
            "Xiaoyan Zhu"
        ],
        "venue": "In SIAM International Conference on Data Mining,",
        "citeRegEx": "Qin et al\\.,? \\Q2014\\E",
        "shortCiteRegEx": "Qin et al\\.",
        "year": 2014,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "The analysis of randomized and nonrandomized AIDS treatment trials using a new approach to causal inference in longitudinal studies",
        "author": [
            "J.M. Robins"
        ],
        "venue": "In Health Service Research Methodology: A Focus on AIDS,",
        "citeRegEx": "Robins.,? \\Q1989\\E",
        "shortCiteRegEx": "Robins.",
        "year": 1989,
        "abstract": "",
        "full_text": "",
        "sentence": " Motivating examples include healthcare [Robins, 1989] \u2013 where we only observe the result of the treatment prescribed to the patient, but obtain no information about how other treatments would have worked \u2013 or Internet applications [Li et al.",
        "context": null
    },
    {
        "title": "Hanson-wright inequality and sub-gaussian concentration",
        "author": [
            "Mark Rudelson",
            "Roman Vershynin"
        ],
        "venue": null,
        "citeRegEx": "Rudelson and Vershynin.,? \\Q2013\\E",
        "shortCiteRegEx": "Rudelson and Vershynin.",
        "year": 2013,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " Specifically, Lemma 20, due to Rudelson and Vershynin [2013], reveals that with probablity \u2265 1\u2212 \u03b4: \u2016U \u03be\u20162 \u2264 \u221a L+ \u221a c ln(2/\u03b4) We also make use of a vector-valued version of Hoeffding\u2019s inequality, due to Rudelson and Vershynin [2013]. Lemma 20 (Vector-valued subgaussian concentration).",
        "context": null
    },
    {
        "title": "Linearly parameterized bandits",
        "author": [
            "Paat Rusmevichientong",
            "John N Tsitsiklis"
        ],
        "venue": "Mathematics of Operations Research,",
        "citeRegEx": "Rusmevichientong and Tsitsiklis.,? \\Q2010\\E",
        "shortCiteRegEx": "Rusmevichientong and Tsitsiklis.",
        "year": 2010,
        "abstract": "We consider bandit problems involving a large (possibly infinite) collection\nof arms, in which the expected reward of each arm is a linear function of an\n$r$-dimensional random vector $\\mathbf{Z} \\in \\mathbb{R}^r$, where $r \\geq 2$.\nThe objective is to minimize the cumulative regret and Bayes risk. When the set\nof arms corresponds to the unit sphere, we prove that the regret and Bayes risk\nis of order $\\Theta(r \\sqrt{T})$, by establishing a lower bound for an\narbitrary policy, and showing that a matching upper bound is obtained through a\npolicy that alternates between exploration and exploitation phases. The\nphase-based policy is also shown to be effective if the set of arms satisfies a\nstrong convexity condition. For the case of a general set of arms, we describe\na near-optimal policy whose regret and Bayes risk admit upper bounds of the\nform $O(r \\sqrt{T} \\log^{3/2} T)$.",
        "full_text": "arXiv:0812.3465v2  [cs.LG]  24 Feb 2010\nLinearly Parameterized Bandits\nPaat Rusmevichientong\nJohn N. Tsitsiklis\nCornell University\nMIT\npaatrus@cornell.edu\njnt@mit.edu\nJanuary 18, 2010 @ 11:04pm\nAbstract\nWe consider bandit problems involving a large (possibly in\ufb01nite) collection of arms, in which\nthe expected reward of each arm is a linear function of an r-dimensional random vector Z \u2208Rr,\nwhere r \u22652. The objective is to minimize the cumulative regret and Bayes risk. When the set of\narms corresponds to the unit sphere, we prove that the regret and Bayes risk is of order \u0398(r\n\u221a\nT),\nby establishing a lower bound for an arbitrary policy, and showing that a matching upper bound is\nobtained through a policy that alternates between exploration and exploitation phases. The phase-\nbased policy is also shown to be e\ufb00ective if the set of arms satis\ufb01es a strong convexity condition.\nFor the case of a general set of arms, we describe a near-optimal policy whose regret and Bayes\nrisk admit upper bounds of the form O(r\n\u221a\nT log3/2 T).\n1.\nIntroduction\nSince its introduction by Thompson (1933), the multiarmed bandit problem has served as an\nimportant model for decision making under uncertainty. Given a set of arms with unknown reward\npro\ufb01les, the decision maker must choose a sequence of arms to maximize the expected total payo\ufb00,\nwhere the decision in each period may depend on the previously observed rewards. The multiarmed\nbandit problem elegantly captures the tradeo\ufb00between the need to exploit arms with high payo\ufb00\nand the incentive to explore previously untried arms for information gathering purposes.\nMuch of the previous work on the multiarmed bandit problem assumes that the rewards of the\narms are statistically independent (see, for example, Lai and Robbins (1985) and Lai (1987)). This\nassumption enables us to consider each arm separately, but it leads to policies whose regret scales\nlinearly with the number of arms. Most policies that assume independence require each arm to be\n1\ntried at least once, and are impractical in settings involving many arms. In such settings, we want\na policy whose regret is independent of the number of arms.\nWhen the mean rewards of the arms are assumed to be independent random variables, Lai and Robbins\n(1985) show that the regret under an arbitrary policy must increase linearly with the number of\narms. However, the assumption of independence is quite strong in practice. In many applications,\nthe information obtained from pulling one arm can change our understanding of other arms. For\ninstance, in marketing applications, we expect a priori that similar products should have similar\nsales. By exploiting the correlation among products/arms, we should be able to obtain a policy\nwhose regret scales more favorably than traditional bandit algorithms that ignore correlation and\nassume independence.\nMersereau et al. (2009) propose a simple model that demonstrates the bene\ufb01ts of exploiting\nthe underlying structure of the rewards.\nThey consider a bandit problem where the expected\nreward of each arm is a linear function of an unknown scalar, with a known prior distribution.\nSince the reward of each arm depends on a single random variable, the mean rewards are perfectly\ncorrelated. They prove that, under certain assumptions, the cumulative Bayes risk over T periods\n(de\ufb01ned below) under a greedy policy admits an O (log T) upper bound, independent of the number\nof arms.\nIn this paper, we extend the model of Mersereau et al. (2009) to the setting where the expected\nreward of each arm depends linearly on a multivariate random vector Z \u2208Rr. We concentrate on\nthe case where r \u22652, which is fundamentally di\ufb00erent from the previous model because the mean\nrewards now depend on more than one random variable, and thus, they are no longer perfectly\ncorrelated. The bounds on the regret and Bayes risk and the policies found in Mersereau et al.\n(2009) no longer apply. To give a \ufb02avor for the di\ufb00erences, we will show that, in our model, the\ncumulative Bayes risk under an arbitrary policy is at least \u2126(r\n\u221a\nT), which is signi\ufb01cantly higher\nthan the upper bound of O(log T) attainable when r = 1.\nThe linearly parameterized bandit is an important model that has been studied by many re-\nsearchers, including Ginebra and Clayton (1995), Abe and Long (1999), and Auer (2002). The re-\nsults in this paper complement and extend the earlier and independent work of Dani et al. (2008a)\nin a number of directions. We provide a detailed comparison between our work and the existing\nliterature in Sections 1.3 and 1.4.\n2\n1.1\nThe Model\nWe have a compact set Ur \u2282Rr that corresponds to the set of arms, where r \u22652. The reward Xu\nt\nof playing arm u \u2208Ur in period t is given by\nXu\nt = u\u2032Z + W u\nt ,\n(1)\nwhere u\u2032Z is the inner product between the vector u \u2208Ur and the random vector Z \u2208Rr. We\nassume that the random variables W u\nt\nare independent of each other and of Z. Moreover, for\neach u \u2208Ur, the random variables {W u\nt : t \u22651} are identically distributed, with E [W u\nt ] = 0 for\nall t and u. We allow the error random variables W u\nt to have unbounded support, provided that\ntheir moment generating functions satisfy certain conditions (given in Assumption 1). Each vector\nu \u2208Ur simultaneously represents an arm and determines the expected reward of that arm. So,\nwhen it is clear from the context, we will interchangeably refer to a u \u2208Ur as either a vector or an\narm.\nLet us introduce the following conventions and notation that will be used throughout the paper.\nWe denote vectors and matrices in bold. All vectors are column vectors. For any vector v \u2208Rr,\nits transpose is denoted by v\u2032, and is always a row vector.\nLet 0 denote the zero vector, and\nfor k = 1, . . . , r, let ek = (0, . . . , 1, . . . , 0) denote the standard unit vector in Rr, with a 1 in the\nkth component and a 0 elsewhere. Also, let Ik denote the k \u00d7 k identity matrix. We let A\u2032 and\ndet(A) denote the transpose and determinant of A, respectively. If A is a symmetric positive\nsemide\ufb01nite matrix, then \u03bbmin(A) and \u03bbmax(A) denote the smallest and the largest eigenvalues of\nA, respectively. We use A1/2 to denote its symmetric nonnegative de\ufb01nite square root, so that\nA1/2A1/2 = A. If A is also positive de\ufb01nite, we let A\u22121/2 =\n\u0000A\u22121\u00011/2. For any vector v, \u2225v\u2225=\n\u221a\nv\u2032v denotes the standard Euclidean norm, and for any positive de\ufb01nite matrix A, \u2225v\u2225A =\n\u221a\nv\u2032Av\ndenotes a corresponding weighted norm. For any two symmetric positive semide\ufb01nite matrices A\nand B, we write A \u2264B if the matrix B \u2212A is positive semide\ufb01nite. Also, all logarithms log(\u00b7)\nin the paper denote the natural log, with base e. A random variable is denoted by an uppercase\nletter while its realized values are denoted in lowercase.\nFor any t \u22651, let Ht\u22121 denote the set of possible histories until the end of period t \u22121. A\npolicy \u03c8 = (\u03c81, \u03c82, . . .) is a sequence of functions such that \u03c8t : Ht\u22121 \u2192Ur selects an arm in period\nt based on the history until the end of period t \u22121. For any policy \u03c8 and z \u2208Rr, the T-period\ncumulative regret under \u03c8 given Z = z, denoted by Regret(z, T, \u03c8), is de\ufb01ned by\nRegret(z, T, \u03c8) =\nT\nX\nt=1\nE\n\u0014\nmax\nv \u2208Ur v\u2032z \u2212U\u2032\ntz\n\f\f\f Z = z\n\u0015\n,\n3\nwhere for any t \u22651, Ut \u2208Ur is the arm chosen under \u03c8 in period t.\nSince Ur is compact,\nmaxv \u2208Ur v\u2032z is well de\ufb01ned for all z. The T-period cumulative Bayes risk under \u03c8 is de\ufb01ned by\nRisk(T, \u03c8) = E [Regret(Z, T, \u03c8)] ,\nwhere the expectation is taken with respect to the prior distribution of Z. We aim to develop a\npolicy that minimizes the cumulative regret and Bayes risk. We note that minimizing the T-period\ncumulative Bayes risk is equivalent to maximizing the expected total reward over T periods.\nTo facilitate exposition, when we discuss a particular policy, we will drop the superscript and\nwrite Xt and Wt to denote XUt\nt\nand W Ut\nt\n, respectively, where Ut is the arm chosen by the policy\nin period t. With this convention, the reward obtained in period t under a particular policy is\nsimply Xt = U\u2032\ntZ + Wt.\n1.2\nPotential Applications\nAlthough our paper focuses on a theoretical analysis, we mention brie\ufb02y potential applications\nto problems in marketing and revenue management. Suppose we have m arms indexed by Ur =\n{u1, u2, . . . , um} \u2282Rr.\nFor k = 1, . . . , r, let \u03c6k = (u1,k, u2,k, . . . , um,k) \u2208Rm denote an m-\ndimensional column vector consisting of the kth coordinates of the di\ufb00erent vectors u\u2113.\nLet\n\u00b5 = (\u00b51, . . . , \u00b5m) be the column vector consisting of expected rewards, where \u00b5\u2113denotes the\nexpected reward of arm u\u2113. Under our formulation, the vector \u00b5 lies in an r-dimensional subspace\nspanned by the vectors \u03c61, . . . , \u03c6r, that is, \u00b5 = Pr\nk=1 Zk\u03c6k, where Z = (Z1, . . . , Zr). If each arm\ncorresponds to a product to be o\ufb00ered to a customer, we can then interpret the vector \u03c6k as a fea-\nture vector or basis function, representing a particular characteristic of the products such as price\nor popularity. We can then interpret the random variables Z1, . . . , Zr as regression coe\ufb03cients,\nobtained from approximating the vector of expected rewards using the basis functions \u03c61, . . . , \u03c6r,\nor more intuitively, as the weights associated with the di\ufb00erent characteristics. Given a prior on the\ncoe\ufb03cients Zk, our goal is to choose a sequence of products that gives the maximum expected total\nreward. This representation suggests that our model might be applicable to problems where we\ncan approximate high-dimensional vectors using a linear combination of a few basis functions, an\napproach that has been successfully applied to high-dimensional dynamic programming problems\n(see Bertsekas and Tsitsiklis (1996) for an overview).\n1.3\nRelated Literature\nThe multiarmed bandit literature can be divided into two streams, depending on the objective\nfunction criteria: maximizing the total discounted reward over an in\ufb01nite horizon or minimizing\nthe cumulative regret and Bayes risk over a \ufb01nite horizon. Our paper focuses exclusively on the\n4\nsecond criterion. Much of the work in this area focuses on understanding the rate with which the\nregret and risk under various policies increase over time. In their pioneering work, Lai and Robbins\n(1985) establish an asymptotic lower bound of \u2126(m log T) for the T-period cumulative regret for\nbandit problems with m independent arms whose mean rewards are \u201cwell-separated,\u201d where the\ndi\ufb00erence between the expected reward of the best and second best arms is \ufb01xed and bounded\naway from zero. They further demonstrate a policy whose regret asymptotically matches the lower\nbound. In contrast, our paper focuses on problems where the number of arms is large (possibly\nin\ufb01nite), and where the gap between the maximum expected reward and the expected reward of\nthe second best arm can be arbitrarily small. Lai (1987) extends these results to a Bayesian setting,\nwith a prior distribution on the reward characteristics of each arm. He shows that when we have m\narms, the T-period cumulative Bayes risk is of order \u0398(m log2 T), when the prior distribution has a\ncontinuous density function satisfying certain properties (see Theorem 3 in Lai, 1987). Subsequent\npapers along this line include Agrawal et al. (1989), Agrawal (1995), and Auer et al. (2002).\nThere has been relatively little research, however, on policies that exploit the dependence\namong the arms. Thompson (1933) allows for correlation among arms in his initial formulation,\nthough he only analyzes a special case involving independent arms. Robbins (1952) formulates\na continuum-armed bandit regression problem, but does not provide an analysis of the regret or\nrisk. Berry and Fristedt (1985) allow for dependence among arms in their formulation in Chapter\n2, but mostly focus on the case of independent arms. Feldman (1962) and Keener (1985) consider\ntwo-armed bandit problems with two hidden states, where the rewards of each arm depend on the\nunderlying state that prevails. Pressman and Sonin (1990) formulate a general multiarmed bandit\nproblem with an arbitrary number of hidden states, and provide a detailed analysis for the case of\ntwo hidden states. Pandey et al. (2007) study bandit problems where the dependence of the arm\nrewards is represented by a hierarchical model.\nA somewhat related literature on bandits with dependent arms is the recent work by Wang et al.\n(2005a,b) and Goldenshluger and Zeevi (2008, 2009) who consider bandit problems with two arms,\nwhere the expected reward of each arm depends on an exogenous variable that represents side\ninformation. These models, however, di\ufb00er from ours because they assume that the side information\nvariables are independent and identically distributed over time, and moreover, these variables are\nperfectly observed before we choose which arm to play. In contrast, we assume that the underlying\nrandom vector Z is unknown and \ufb01xed over time, to be estimated based on past rewards and\ndecisions.\nOur formulation can be viewed as a sequential method for maximizing a linear function based\non noisy observations of the function values, and it is thus closely related to the \ufb01eld of stochas-\n5\ntic approximation, which was developed by Robbins and Monro (1951) and Kiefer and Wolfowitz\n(1952). We do not provide a comprehensive review of the literature here; interested readers are\nreferred to an excellent survey by Lai (2003). In stochastic approximation, we wish to \ufb01nd an\nadaptive sequence {Ut \u2208Rr : t \u22651} that converges to a maximizer u\u2217of a target function, and\nthe focus is on establishing the rate at which the mean squared error E\nh\n\u2225Ut \u2212u\u2217\u22252i\nconverges\nto zero (see, for example, Blum, 1954 and Cicek et al., 2009). In contrast, our cumulative regret\nand Bayes risk criteria take into account the cost associated with each observation. The di\ufb00erent\nperformance measures used in our formulation lead to entirely di\ufb00erent policies and performance\ncharacteristics.\nOur model generalizes the \u201cresponse surface bandits\u201d introduced by Ginebra and Clayton (1995),\nwho assume a normal prior on Z and provide a simple tunable heuristic, without any analysis on\nthe regret or risk.\nAbe and Long (1999), Auer (2002), and Dani et al. (2008a) all consider a\nspecial case of our model where the random vector Z and the error random variables W u\nt\nare\nbounded almost surely, and with the exception of the last paper, focus on the regret criterion.\nAbe and Long (1999) demonstrate a class of bandits where the dimension r is at least \u2126(\n\u221a\nT), and\nshow that the T-period regret under an arbitrary policy must be at least \u2126\n\u0000T 3/4\u0001\n. Auer (2002)\ndescribes an algorithm based on least squares estimation and con\ufb01dence bounds, and establishes an\nO\n\u0010\u221ar\n\u221a\nT log3/2 (T |Ur|)\n\u0011\nupper bound on the regret, for the case of \ufb01nitely many arms. Dani et al.\n(2008a) show that the policy of Auer (2002) can be extended to problems having an arbitrary com-\npact set of arms, and also make use of a barycentric spanner. They establish an O(r\n\u221a\nT log3/2 T)\nupper bound on the regret, and discuss a variation of the policy that is more computationally\ntractable (at the expense of higher regret). Dani et al. (2008a) also establish an \u2126(r\n\u221a\nT) lower\nbound on the Bayes risk when the set of arms is the Cartesian product of circles1. However, this\nleaves a O(log3/2 T) gap from the upper bound, leaving open the question of the exact order of\nregret and risk.\n1.4\nContributions and Organizations\nOne of our contributions is proving that the regret and Bayes risk for a broad class of linearly\nparameterized bandits is of order \u0398(r\n\u221a\nT). In Section 2, we establish an \u2126(r\n\u221a\nT) lower bound for\nan arbitrary policy, when the set of arms is the unit sphere in Rr. Then, in Section 3, we show that\na matching O(r\n\u221a\nT) upper bound can be achieved through a phase-based policy that alternates\nbetween exploration and exploitation phases. To the best of our knowledge, this is the \ufb01rst result\nthat establishes matching upper and lower bounds for a class of linearly parameterized bandits.\n1The original lower bound (Theorem 3 on page 360 of Dani et al., 2008a) was not entirely correct; a correct version\nwas provided later, in Dani et al. (2008b).\n6\nTable 1 summarizes our results and provides a comparison with the results in Mersereau et al.\n(2009) for the case r = 1. In the ensuing discussion of the bounds, we focus on the main parameters,\nr and T, with more precise statements given in the theorems.\nAlthough we obtain the same lower bound of \u2126(r\n\u221a\nT), our example and proof techniques are\nvery di\ufb00erent from Dani et al. (2008a). We consider the unit sphere, with a multivariate normal\nprior on Z, and standard normal errors. The analysis in Section 2 also illuminates the behavior of\nthe least mean squares estimator in this setting, and we believe that it provides an approach that\ncan be used to address more general classes of linear estimation and adaptive control problems.\nWe also prove that the phase-based policy remains e\ufb00ective (that is, admits an O(r\n\u221a\nT) upper\nbound) for a broad class of bandit problems in which the set of arms is strongly convex2 (de\ufb01ned\nin Section 3). To our knowledge, this is the \ufb01rst result that establishes the connection between\na geometrical property (strong convexity) of the underlying set of arms and the e\ufb00ectiveness of\nseparating exploration from exploitation.\nWe suspect that strong convexity may have similar\nimplications for other types of bandit and learning problems.\nWhen the set of arms is an arbitrary compact set, the separation of exploration and exploitation\nmay not be e\ufb00ective, and we consider in Section 4 an active exploration policy based on least squares\nestimation and con\ufb01dence regions. We prove that the regret and risk under this policy are bounded\nabove by O(r\n\u221a\nT log3/2 T), which is within a logarithmic factor of the lower bound. Our policy is\nclosely related to the one considered in Auer (2002) and further analyzed in Dani et al. (2008a),\nwith di\ufb00erences in a number of respects. First, our model allows the random vector Z and the\nerrors W u\nt\nto have unbounded support, which requires a somewhat more complicated analysis.\nSecond, our policy is an \u201canytime\u201d policy, in the sense that the policy does not depend on the\ntime horizon T of interest. In contrast, the policies of Auer (2002) and Dani et al. (2008a) involve\na certain parameter \u03b4 whose value must be set in advance as a function of the time horizon T in\norder to obtain the O\n\u0010\nr\n\u221a\nT log3/2 T\n\u0011\nregret bound.\nWe \ufb01nally comment on the case where the set of arms is \ufb01nite and \ufb01xed. We show that the\nregret and risk under our active exploration policy increase gracefully with time, as log T and log2 T,\nrespectively. These results show that our policy is within a constant factor of the asymptotic lower\nbounds established by Lai and Robbins (1985) and Lai (1987). In contrast, for the policies of Auer\n(2002) and Dani et al. (2008a), the available regret upper bounds grow over time as\n\u221a\nT log3/2 T\nand log3 T, respectively.\n2One can show that the Cartesian product of circles is not strongly convex, and thus, our phase-based policy cannot\nbe applied to give the matching upper bound for the example used in Dani et al. (2008a).\n7\nT -period Cumulative Regret\nT -period Cumulative Bayes Risk\nDimension\nSet of\n(r)\nArms (Ur)\nLower Bound\nUpper Bound\nLower Bound\nUpper Bound\nr = 1\nAny Compact Set\n\u2126\n\u0010\u221a\nT\n\u0011\nO\n\u0010\u221a\nT\n\u0011\n\u2126(log T )\nO (log T )\n(Mersereau et al., 2008)\nUnit Sphere\n\u2126\n\u0010\nr\n\u221a\nT\n\u0011\nO\n\u0010\nr\n\u221a\nT\n\u0011\n\u2126\n\u0010\nr\n\u221a\nT\n\u0011\nO\n\u0010\nr\n\u221a\nT\n\u0011\nr \u22652\n(Sections 2 and 3)\n(This Paper)\nAny Compact Set\n\u2126\n\u0010\nr\n\u221a\nT\n\u0011\nO\n\u0010\nr\n\u221a\nT log3/2 T\n\u0011\n\u2126\n\u0010\nr\n\u221a\nT\n\u0011\nO\n\u0010\nr\n\u221a\nT log3/2 T\n\u0011\n(Section 4)\nTable 1: Regret and risk bounds for various values of r and for di\ufb00erent collections of arms.\nWe note that the bounds on the cumulative Bayes risk given in Table 1 hold under certain\nassumptions on the prior distribution of the random vector Z. For r = 1, Z is assumed to be\na continuous random variable with a bounded density function (Theorem 3.2 in Mersereau et al.,\n2009). When the collection of arms is a unit sphere with r \u22652, we require that both E [\u2225Z\u2225] and\nE [1/ \u2225Z\u2225] are bounded (see Theorems 2.1 and 3.1, and Lemma 3.2). For general compact sets of\narms where our risk bound is not tight, we only require that \u2225Z\u2225has a bounded expectation.\n2.\nLower Bounds\nIn this section, we establish \u2126(r\n\u221a\nT) lower bounds on the regret and risk under an arbitrary policy\nwhen the set of arms is the unit sphere. This result is stated in the following theorem3\nTheorem 2.1 (Lower Bounds). Consider a bandit problem where the set of arms is the unit sphere\nin Rr, and W u\nt has a standard normal distribution with mean zero and variance one for all t and\nu. If Z has a multivariate normal distribution with mean 0 and covariance matrix Ir/r, then for\nall policies \u03c8 and every T \u2265r2,\nRisk (T, \u03c8) \u22650.006 r\n\u221a\nT .\nConsequently, for any policy \u03c8 and T \u2265r2, there exists z \u2208Rr such that\nRegret (z, T, \u03c8) \u22650.006 r\n\u221a\nT .\n3The result of Theorem 2.1 easily extends to the case where the covariance matrix is Ir, rather than Ir/r. The proof\nis essentially the same.\n8\nIt su\ufb03ces to establish the lower bound on the Bayes risk because the regret bound follows im-\nmediately. Throughout this section, we assume that Ur = {u \u2208Rr : \u2225u\u2225= 1}. We \ufb01x an arbitrary\npolicy \u03c8 and for any t \u22651, we let Ht = (U1, X1, U2, X2, . . . , Ut, Xt) be the history up to time t.\nWe also let bZt denote the least mean squares estimator of Z given the history Ht, that is,\nbZt = E\n\u0002\nZ\n\f\f Ht\n\u0003\n.\nLet S1\nt , . . . , Sr\u22121\nt\ndenote a collection of orthogonal unit vectors that are also orthogonal to bZt. Note\nthat bZt and S1\nt , . . . , Sr\u22121\nt\nare functions of Ht.\nSince Ur is the unit sphere, maxu\u2208Ur u\u2032z = (z\u2032z) / \u2225z\u2225= \u2225z\u2225, for all z \u2208Rr. Thus, the risk\nin period t is given by E [ \u2225Z\u2225\u2212U\u2032\ntZ ]. The following lemma establishes a lower bound on the\ncumulative risk in terms of the estimator error variance and the total amount of exploration along\nthe directions S1\nT , . . . , Sr\u22121\nT\n.\nLemma 2.2 (Risk Decomposition). For any T \u22651,\nRisk (T, \u03c8)\n\u22651\n2\nr\u22121\nX\nk=1\nE\n\"\n\u2225Z\u2225\nT\nX\nt=1\n\u0010\nU\u2032\ntSk\nT\n\u00112\n+\nT\n\u2225Z\u2225\n\u001a\u0010\nZ \u2212bZT\n\u0011\u2032\nSk\nT\n\u001b2#\n.\nProof. Using the fact that for any two unit vectors w and v, 1 \u2212w\u2032v = \u2225w \u2212v\u22252 /2, the instan-\ntaneous regret in period t satis\ufb01es\n\u2225Z\u2225\u2212U\u2032\ntZ = \u2225Z\u2225\n\u0012\n1 \u2212U\u2032\nt\nZ\n\u2225Z\u2225\n\u0013\n= \u2225Z\u2225\n2\n\r\r\r\rUt \u2212\nZ\n\u2225Z\u2225\n\r\r\r\r\n2\n\u2265\u2225Z\u2225\n2\nr\u22121\nX\nk=1\n\u001a\u0012\nUt \u2212\nZ\n\u2225Z\u2225\n\u0013\u2032\nSk\nT\n\u001b2\n,\nwhere the inequality follows from the fact that S1\nT , . . . , Sr\u22121\nT\nare orthogonal unit vectors. Therefore,\nthe cumulative conditional risk satis\ufb01es\n2\nT\nX\nt=1\nE\nh\n\u2225Z\u2225\u2212U\u2032\ntZ\n\f\f\f HT\ni\n\u2265\nT\nX\nt=1\nE\n\"\n\u2225Z\u2225\nr\u22121\nX\nk=1\n\u001a\u0012\nUt \u2212\nZ\n\u2225Z\u2225\n\u0013\u2032\nSk\nT\n\u001b2 \f\f\f HT\n#\n=\nT\nX\nt=1\nr\u22121\nX\nk=1\nE\n\"\n\u2225Z\u2225\n\u001a\u0012\nUt \u2212\nZ\n\u2225Z\u2225\n\u0013\u2032\nSk\nT\n\u001b2 \f\f\f HT\n#\n=\nT\nX\nt=1\nr\u22121\nX\nk=1\nE\n\"\n\u2225Z\u2225\n\u0010\nU\u2032\ntSk\nT\n\u00112\n\u22122\n\u0010\nU\u2032\ntSk\nT\n\u0011 \u0010\nZ\u2032Sk\nT\n\u0011\n+\n\u0000Z\u2032Sk\nT\n\u00012\n\u2225Z\u2225\n\f\f\f HT\n#\n,\nwith probability one. From the de\ufb01nition of Sk\nT , we have bZ\u2032\nT Sk\nT = 0 for k = 1 . . . , r \u22121. Therefore,\nfor t \u2264T,\nE\nh\u0010\nU\u2032\ntSk\nT\n\u0011 \u0010\nZ\u2032Sk\nT\n\u0011 \f\f\f HT\ni\n=\n\u0010\nU\u2032\ntSk\nT\n\u0011\nE\nh\nZ\u2032 \f\f\f HT\ni\nSk\nT =\n\u0010\nU\u2032\ntSk\nT\n\u0011\nbZ\u2032\nT Sk\nT = 0 ,\n9\nwhich eliminates the middle term in the summand above.\nFurthermore, we see that Z\u2032Sk\nT =\n\u0010\nZ \u2212bZT\n\u0011\u2032\nSk\nT for all k. Thus, with probability one,\nT\nX\nt=1\nE\nh\n\u2225Z\u2225\u2212U\u2032\ntZ\n\f\f\f HT\ni\n\u2265\n1\n2\nr\u22121\nX\nk=1\nE\n\"\n\u2225Z\u2225\nT\nX\nt=1\n\u0010\nU\u2032\ntSk\nT\n\u00112\n+\nT\n\u2225Z\u2225\n\u001a\u0010\nZ \u2212bZT\n\u0011\u2032\nSk\nT\n\u001b2 \f\f\f HT\n#\n,\nand the desired result follows by taking the expectation of both sides.\nSince Sk\nT is orthogonal to bZT , we can interpret PT\nt=1\n\u0000U\u2032\ntSk\nT\n\u00012 and\n\u001a\u0010\nZ \u2212bZT\n\u0011\u2032\nSk\nT\n\u001b2\nas the\ntotal amount of exploration over T periods and the squared estimation error, respectively, in the\ndirection Sk\nT . Thus, Lemma 2.2 tells us that the cumulative risk is bounded below by the sum\nof the squared estimation error and the total amount of exploration in the past T periods. This\nresult suggests an approach for establishing a lower bound on the risk. If the amount of exploration\nPT\nt=1\n\u0000U\u2032\ntSk\nT\n\u00012 is large, then the risk will be large. On the other hand, if the amount of exploration\nis small, we expect signi\ufb01cant estimation errors, which in turn imply large risk. This intuition\nis made precise in Lemma 2.3, which relates the squared estimation error and the amount of\nexploration.\nLemma 2.3 (Little Exploration Implies Large Estimation Errors). For any k and T \u22651,\nE\n\"\u001a\u0010\nZ \u2212bZT\n\u0011\u2032\nSk\nT\n\u001b2 \f\f\f\f HT\n#\n\u2265\n1\nr + PT\nt=1\n\u0000U\u2032\ntSk\nT\n\u00012 ,\nwith probability one.\nProof. Let QT = bZT\n\u000e\n\u2225bZT \u2225. For any t, we have that Ut = Pr\u22121\nk=1\n\u0000U\u2032\ntSk\nT\n\u0001\nSk\nT + (U\u2032\ntQT) QT. Let\nV =\n\u0002\nS1\nT S2\nT\n\u00b7 \u00b7 \u00b7 Sr\u22121\nT\nQT\n\u0003\nbe an r \u00d7 r orthonormal matrix whose columns are the vectors S1\nT , . . . , Sr\u22121\nT\n, and QT , respectively.\nThen, it is easy to verify that\nT\nX\nt=1\nUtU\u2032\nt = VAV\u2032 ,\nwhere A =\n\uf8eb\n\uf8ed\u03a3\nc\nc\u2032\na\n\uf8f6\n\uf8f8, is an r \u00d7 r matrix, with a = Q\u2032\nT\n\u0010PT\nt=1 UtU\u2032\nt\n\u0011\nQT , c is an (r \u22121)-\ndimensional column vector, and where \u03a3 is an (r\u22121)\u00d7(r\u22121) matrix with \u03a3k\u2113=\n\u0000Sk\nT\n\u0001\u2032 \u0010PT\nt=1 UtU\u2032\nt\n\u0011\nS\u2113\nT =\nPT\nt=1\n\u0000U\u2032\ntSk\nT\n\u0001 \u0000U\u2032\ntS\u2113\nT\n\u0001\nfor k, \u2113= 1, . . . , r \u22121,\nSince Z has a multivariate normal prior distribution with covariance matrix Ir/r, it is a standard\nresult (use, for example, Corollary E.3.5 in Appendix E in Bertsekas, 1995) that\nE\n\u0014\u0010\nZ \u2212bZT\n\u0011 \u0010\nZ \u2212bZT\n\u0011\u2032 \f\f\f\f HT\n\u0015\n=\n \nr Ir +\nT\nX\nt=1\nUtU\u2032\nt\n!\u22121\n= V (r Ir + A)\u22121 V\u2032 .\n10\nSince Sk\nT is a function of HT and V\u2032Sk\nT = ek, we have, for k \u2264r \u22121, that\nE\n\"\u001a\u0010\nZ \u2212bZT\n\u0011\u2032\nSk\nT\n\u001b2 \f\f\f\f HT\n#\n=\n\u0010\nV\u2032Sk\nT\n\u0011\u2032\n(rIr + A)\u22121 \u0010\nV\u2032Sk\nT\n\u0011\n=\nh\n(r Ir + A)\u22121i\nkk\n\u2265\n1\n(r Ir + A)kk\n=\n1\nr + PT\nt=1\n\u0000U\u2032\ntSk\nT\n\u00012 ,\nwhere the inequality follows from Fiedler\u2019s Inequality (see, for example, Theorem 2.1 in Fiedler and Pt\u00b4ak,\n1997), and the \ufb01nal equality follows from the de\ufb01nition of A.\nThe next lemma gives a lower bound on the probability that Z is bounded away from the origin.\nThe proof follows from simple calculations involving normal densities, and the details are given in\nAppendix A.1.\nLemma 2.4. For any \u03b8 \u22641/2 and \u03b2 > 0, Pr {\u03b8 \u2264\u2225Z\u2225\u2264\u03b2} \u22651 \u22124\u03b82 \u22121\n\u03b22 .\nThe last lemma establishes a lower bound on the sum of the total amount of exploration and\nthe squared estimation error, which is also the minimum cumulative Bayes risk along the direction\nSk\nT by Lemma 2.2.\nLemma 2.5 (Minimum Directional Risk). For k = 1, . . . , r \u22121, and T \u2265r2,\nE\n\"\n\u2225Z\u2225\nT\nX\nt=1\n\u0010\nU\u2032\ntSk\nT\n\u00112\n+\nT\n\u2225Z\u2225\n\u001a\u0010\nZ \u2212bZT\n\u0011\u2032\nSk\nT\n\u001b2#\n\u22650.027\n\u221a\nT .\nWe note that if \u2225Z\u2225were a constant, rather than a random variable, Lemma 2.5 would follow\nimmediately. Hence, most of the work in the proof below involves constraining \u2225Z\u2225to a certain\nrange [\u03b8, \u03b2].\nProof. Consider an arbitrary k, and let \u039e = PT\nt=1\n\u0000U\u2032\ntSk\nT\n\u00012, \u0393 =\n\u001a\u0010\nZ \u2212bZT\n\u0011\u2032\nSk\nT\n\u001b2\n. Our proof\nwill make use of positive constants \u03b8, \u03b2, and \u03b7, whose values will be chosen later. Note that\nE\n\u0014\n\u2225Z\u2225\u039e + T \u0393\n\u2225Z\u2225\n\f\f\f HT\n\u0015\n\u2265\nE\n\u0014\u0012\n\u2225Z\u2225\u039e + T \u0393\n\u2225Z\u2225\n\u0013\n1l{\u03b8 \u2264\u2225Z\u2225\u2264\u03b2}1l{\u039e \u2265\n\u221a\nT}\n\f\f\f HT\n\u0015\n+ E\n\u0014\u0012\n\u2225Z\u2225\u039e + T \u0393\n\u2225Z\u2225\n\u0013\n1l{\u03b8 \u2264\u2225Z\u2225\u2264\u03b2}1l{\u039e <\n\u221a\nT}\n\f\f\f HT\n\u0015\n\u2265\n\u03b8\n\u221a\nT 1l{\u039e \u2265\n\u221a\nT}E\n\u0002\n1l{\u03b8 \u2264\u2225Z\u2225\u2264\u03b2}\n\f\f HT\n\u0003\n+ T\n\u03b2 1l{\u039e <\n\u221a\nT}E\n\u0002\n\u0393 1l{\u03b8 \u2264\u2225Z\u2225\u2264\u03b2}\n\f\f HT\n\u0003\n,\nwhere we use the fact that \u039e is a function of HT in the \ufb01nal inequality. We will now lower bound\nthe last term on the right hand side of the above inequality. Let \u0398 = E\n\u0002\n\u0393\n\f\f HT\n\u0003\n. Since \u0398 is a\n11\nfunction of HT ,\nT\n\u03b2 1l{\u039e <\n\u221a\nT}E\n\u0002\n\u0393 1l{\u03b8 \u2264\u2225Z\u2225\u2264\u03b2}\n\f\f HT\n\u0003\n\u2265\nT\n\u03b2 1l{\u039e <\n\u221a\nT}E\n\u0002\n\u0393 1l{\u03b8 \u2264\u2225Z\u2225\u2264\u03b2}1l{\u0393 \u2265\u03b7 \u0398}\n\f\f HT\n\u0003\n\u2265\n\u03b7 T\n\u03b2 \u0398 1l{\u039e <\n\u221a\nT}E\nh\n1l{\u03b8 \u2264\u2225Z\u2225\u2264\u03b2}1l{\u0393 \u2265\u03b7 \u0398}\n\f\f\f HT\ni\n\u2265\n\u03b7\n\u221a\nT\n2\u03b2 1l{\u039e <\n\u221a\nT}E\nh\n1l{\u03b8 \u2264\u2225Z\u2225\u2264\u03b2}1l{\u0393 \u2265\u03b7 \u0398}\n\f\f\f HT\ni\n,\nwhere the last inequality follows from Lemma 2.3 which implies that, with probability one,\n\u03b7 T\n\u03b2 \u0398 1l{\u039e <\n\u221a\nT} \u2265\u03b7 T\n\u03b2 \u00b7\n1\nr + \u039e1l{\u039e <\n\u221a\nT} \u2265\u03b7 T\n\u03b2 \u00b7\n1\nr +\n\u221a\nT\n1l{\u039e <\n\u221a\nT} \u2265\u03b7\n\u221a\nT\n2\u03b2 1l{\u039e <\n\u221a\nT} ,\nand where the last inequality follows from the fact that T \u2265r2, and thus, 1/\n\u0010\nr +\n\u221a\nT\n\u0011\n\u22651/(2\n\u221a\nT).\nPutting everything together, we obtain\nE\n\u0014\n\u2225Z\u2225\u039e + T \u0393\n\u2225Z\u2225\n\f\f\f HT\n\u0015\n\u2265\n\u03b8\n\u221a\nT 1l{\u039e \u2265\n\u221a\nT}E\n\u0002\n1l{\u03b8 \u2264\u2225Z\u2225\u2264\u03b2}\n\f\f HT\n\u0003\n+ \u03b7\n\u221a\nT\n2\u03b2 1l{\u039e <\n\u221a\nT}E\nh\n1l{\u03b8 \u2264\u2225Z\u2225\u2264\u03b2}1l{\u0393 \u2265\u03b7 \u0398}\n\f\f\f HT\ni\n,\n\u2265\nmin\n\u001a\n\u03b8, \u03b7\n2\u03b2\n\u001b \u221a\nT E\nh\n1l{\u03b8 \u2264\u2225Z\u2225\u2264\u03b2}1l{\u0393 \u2265\u03b7 \u0398}\n\f\f\f HT\ni\n,\nwith probability one. By the Bonferroni Inequality, we have that\nE\n\u0002\n1l{\u03b8 \u2264\u2225Z\u2225\u2264\u03b2}1l{\u0393 \u2265\u03b7 \u0398}\n\f\f HT\n\u0003\n=\nPr\n\b\n\u03b8 \u2264\u2225Z\u2225\u2264\u03b2\nand\n\u0393 \u2265\u03b7 \u0398\n\f\f HT\n\t\n\u2265\nPr\n\b\n\u03b8 \u2264\u2225Z\u2225\u2264\u03b2\n\f\f HT\n\t\n+ Pr\n\b\n\u0393 \u2265\u03b7 \u0398\n\f\f HT\n\t\n\u22121 ,\nwith probability one. Conditioned on HT ,\n\u0010\nZ \u2212bZT\n\u0011\u2032\nSk\nT is normally distributed with mean zero\nand variance\nE\n\"\u001a\u0010\nZ \u2212bZT\n\u0011\u2032\nSk\nT\n\u001b2 \f\f\f HT\n#\n= E\nh\n\u0393\n\f\f\f HT\ni\n= \u0398 .\nLet \u03a6(\u00b7) be the cumulative distribution function of the standard normal random variable, that is,\n\u03a6(x) =\n1\n\u221a\n2\u03c0\nR x\n\u2212\u221ee\u2212u2/2 du. Then,\nPr\n\b\n\u0393 \u2265\u03b7 \u0398\n\f\f HT\n\t\n=\nPr\n\u001a\f\f\n\u0010\nZ \u2212bZT\n\u0011\u2032\nSk\nT\n\f\f \u2265\u221a\u03b7\n\u221a\n\u0398\n\f\f HT\n\u001b\n= 2 (1 \u2212\u03a6 (\u221a\u03b7)) ,\nfrom which it follows that, with probability one,\nE\nh\n1l{\u03b8 \u2264\u2225Z\u2225\u2264\u03b2}1l{\u0393 \u2265\u03b7 \u0398}\n\f\f\f HT\ni\n\u2265Pr\n\b\n\u03b8 \u2264\u2225Z\u2225\u2264\u03b2\n\f\f HT\n\t\n+ 2 (1 \u2212\u03a6 (\u221a\u03b7)) \u22121 .\nTherefore,\nE\n\u0014\n\u2225Z\u2225\u039e + T \u0393\n\u2225Z\u2225\n\f\f\f HT\n\u0015\n\u2265min\n\u001a\n\u03b8, \u03b7\n2\u03b2\n\u001b h\nPr\nn\n\u03b8 \u2264\u2225Z\u2225\u2264\u03b2\n\f\f\f HT\no\n+ 2 (1 \u2212\u03a6 (\u221a\u03b7)) \u22121\ni \u221a\nT ,\n12\nwith probability one, which implies that\nE\n\u0014\n\u2225Z\u2225\u039e + T \u0393\n\u2225Z\u2225\n\u0015\n\u2265\nmin\n\u001a\n\u03b8, \u03b7\n2\u03b2\n\u001b\n[Pr {\u03b8 \u2264\u2225Z\u2225\u2264\u03b2} + 2 (1 \u2212\u03a6 (\u221a\u03b7)) \u22121]\n\u221a\nT ,\n\u2265\nmin\n\u001a\n\u03b8, \u03b7\n2\u03b2\n\u001b \u0014\n2 (1 \u2212\u03a6 (\u221a\u03b7)) \u22121\n\u03b22 \u22124\u03b82\n\u0015 \u221a\nT ,\nwhere the last inequality follows from Lemma 2.4. Set \u03b8 = 0.09, \u03b2 = 3, and \u03b7 = 0.5, to obtain\nE\nh\n\u2225Z\u2225\u039e + T \u0393\n\u2225Z\u2225\ni\n\u22650.027\n\u221a\nT , which is the desired result.\nFinally, here is the proof of Theorem 2.1.\nProof. It follows from Lemmas 2.2 and 2.5 that\nRisk (T, \u03c8)\n\u2265\n1\n2\nr\u22121\nX\nk=1\nE\n\"\n\u2225Z\u2225\nT\nX\nt=1\n\u0010\nU\u2032\ntSk\nT\n\u00112\n+\nT\n\u2225Z\u2225\n\u001a\u0010\nZ \u2212bZT\n\u0011\u2032\nSk\nT\n\u001b2#\n\u2265\nr \u22121\n2\n\u00b7 0.027\n\u221a\nT \u2265r\n4 \u00b7 0.027\n\u221a\nT \u22650.006 r\n\u221a\nT ,\nwhere we have used the fact r \u22652, which implies that r \u22121 \u2265r/2.\n3.\nMatching Upper Bounds\nWe have established \u2126\n\u0010\nr\n\u221a\nT\n\u0011\nlower bounds when the set of arms Ur is the unit sphere. We now\nprove that a policy that alternates between exploration and exploitation phases yields matching\nupper bounds on the regret and risk, and is therefore optimal for this problem. Surprisingly, we\nwill see that the phase-based policy is e\ufb00ective for a large class of bandit problems, involving a\nstrongly convex set of arms. We introduce the following assumption on the tails of the error random\nvariables W u\nt and on the set of arms Ur, which will remain in e\ufb00ect throughout the rest of paper.\nAssumption 1.\n(a) There exists a positive constant \u03c30 such that for any r \u22652, u \u2208Ur, t \u22651, and x \u2208R, we\nhave E\n\u0002\nexW u\nt \u0003\n\u2264ex2\u03c32\n0/2 .\n(b) There exist positive constants \u00afu and \u03bb0 such that for any r \u22652,\nmax\nu \u2208Ur \u2225u\u2225\u2264\u00afu ,\nand the set of arms Ur \u2282Rr has r linearly independent elements b1, . . . , br such that\n\u03bbmin (Pr\nk=1 bkb\u2032\nk) \u2265\u03bb0.\nUnder Assumption 1(a), the tails of the distribution of the errors W u\nt\ndecay at least as fast as\nfor a normal random variable with variance \u03c32\n0. The \ufb01rst part of Assumption 1(b) ensures that\n13\nthe expected reward of the arms remain bounded as the dimension r increases, while the arms\nb1, . . . , br given in the second part of Assumption 1(b) will be used during the exploration phase\nof our policy.\nOur policy \u2013 which we refer to as the Phased Exploration and Greedy Exploitation\n(PEGE) \u2013 operates in cycles, and in each cycle, we alternate between exploration and exploitation\nphases. During the exploration phase of cycle c, we play the r linearly independent arms from\nAssumption 1(b). Using the rewards observed during the exploration phases in the past c cycles,\nwe compute an ordinary least squares (OLS) estimate bZ(c). In the exploitation phase of cycle c,\nwe use bZ(c) as a proxy for Z and compute a greedy decision G(c) \u2208Ur de\ufb01ned by:\nG(c) = arg max\nv \u2208Ur v\u2032bZ(c) ,\n(2)\nwhere we break ties arbitrarily. We then play the arm G(c) for an additional c periods to complete\ncycle c. Here is a formal description of the policy.\nPhased Exploration and Greedy Exploitation (PEGE)\nDescription: For each cycle c \u22651, complete the following two phases.\n1. Exploration (r periods): For k = 1, 2, . . . , r, play arm bk \u2208Ur given in Assumption 1(b),\nand observe the reward Xbk(c). Compute the OLS estimate bZ(c) \u2208Rr, given by\nbZ(c) = 1\nc\n r\nX\nk=1\nbkb\u2032\nk\n!\u22121\nc\nX\ns=1\nr\nX\nk=1\nbkXbk(s) = Z + 1\nc\n r\nX\nk=1\nbkb\u2032\nk\n!\u22121\nc\nX\ns=1\nr\nX\nk=1\nbkW bk(s) ,\nwhere for any k, Xbk(s) and W bk(s) denote the observed reward and the error random\nvariable associated with playing arm bk in cycle s. Note that the last equality follows from\nEquation (1) de\ufb01ning our model.\n2. Exploitation (c periods): Play the greedy arm G(c) = arg maxv\u2208Ur v\u2032bZ(c) for c periods.\nSince Ur is compact, for each z \u2208Rr, there is an optimal arm that gives the maximum expected\nreward. When this best arm varies smoothly with z, we will show that the T-period regret and\nrisk under the PEGE policy is bounded above by O(r\n\u221a\nT). More precisely, we say that a set of\narms Ur satis\ufb01es the smooth best arm response with parameter J (SBAR(J), for short) condition if\nfor any nonzero vector z \u2208Rr \\{0}, there is a unique best arm u\u2217(z) \u2208Ur that gives the maximum\nexpected reward, and for any two unit vectors z \u2208Rr an y \u2208Rr with \u2225z\u2225= \u2225y\u2225= 1, we have\n\u2225u\u2217(z) \u2212u\u2217(y)\u2225\u2264J \u2225z \u2212y\u2225.\n14\nEven though the SBAR condition appears to be an implicit one, it admits a simple interpreta-\ntion. According to Corollary 4 of Polovinkin (1996), a compact set Ur satis\ufb01es condition SBAR(J)\nif and only if it is strongly convex with parameter J, in the sense that the set Ur can be represented\nas the intersection of closed balls of radius J. Intuitively, the SBAR condition requires the bound-\nary of Ur to have a curvature that is bounded below by a positive constant. For some examples,\nthe unit ball satis\ufb01es the SBAR(1) condition. Furthermore, according to Theorem 3 of Polovinkin\n(1996), an ellipsoid of the form {u \u2208Rr : u\u2032Q\u22121u \u22641}, where Q is a symmetric positive de\ufb01nite\nmatrix, satis\ufb01es the condition SBAR\n\u0010\n\u03bbmax(Q)/\np\n\u03bbmin(Q)\n\u0011\n.\nThe main result of this section is stated in the following theorem. The proof is given in Sec-\ntion 3.1.\nTheorem 3.1 (Regret and Risk Under the Greedy Policy). Suppose that Assumption 1 holds and\nthat the sets Ur satisfy the SBAR(J) condition.\nThen, there exists a positive constant a1 that\ndepends only on \u03c30, \u00afu, \u03bb0, and J, such that for any z \u2208Rr \\ {0} and T \u2265r,\nRegret (z, T, PEGE) \u2264a1\n\u0012\n\u2225z\u2225+\n1\n\u2225z\u2225\n\u0013\nr\n\u221a\nT .\nSuppose in addition, that there exists a constant M > 0 such that for every r \u22652 we have E [ \u2225Z\u2225] \u2264\nM and E [ 1/ \u2225Z\u2225] \u2264M. Then, there exists a positive constant a2 that depends only on \u03c30, \u00afu, \u03bb0,\nJ, and M, such that for any T \u2265r,\nRisk (T, PEGE) \u2264a2 r\n\u221a\nT .\nDependence on \u2225z\u2225in the regret bound: By Assumption 1(b), for any z \u2208Rr, the instan-\ntaneous regret under arm v \u2208U is bounded by maxu\u2208U z\u2032(u\u2212v) \u22642\u00afu \u2225z\u2225. Thus, 2\u00afu \u2225z\u2225T provides\na trivial upper bound on the T-period cumulative regret under the PEGE policy. Combining this\nwith Theorem 3.1, we have that\nRegret(z, T, PEGE) \u2264max{a1, 2\u00afu} \u00b7 min\n\u001a\u0012\n\u2225z\u2225+\n1\n\u2225z\u2225\n\u0013\nr\n\u221a\nT, \u2225z\u2225T\n\u001b\n.\nThe above result shows that the performance of our policy does not deteriorate as the norm of z\napproaches zero.\nIntuitively, the requirement E [\u2225Z\u2225] \u2264M in Theorem 3.1 implies that, as r increases, the\nmaximum expected reward (over all arms) remains bounded. Moreover, the assumption on the\nboundedness of E [ 1/ \u2225Z\u2225] means that Z does not have too much mass near the origin.\nThe\nfollowing lemma provides conditions under which this assumption holds, and shows that the case\nof the multivariate normal distribution used in Theorem 2.1 is also covered. The proof is given in\nAppendix A.2.\n15\nLemma 3.2 (Small Mass Near the Origin).\n(a) Suppose that there exist constants M0 and \u03c1 \u2208(0, 1] such that for any r \u22652, the random\nvariable \u2225Z\u2225has a density function g : R+ \u2192R+ such that g(x) \u2264M0x\u03c1 for all x \u2208[0, \u03c1].\nThen, E [ 1/ \u2225Z\u2225] \u2264M, where M depends only on M0 and \u03c1.\n(b) Suppose that for any r \u22652, the random vector Z has a multivariate normal distribution with\nmean 0 \u2208Rr and covariance matrix Ir/r. Then, E [ \u2225Z\u2225] \u22641 and E [ 1/ \u2225Z\u2225] \u2264\u221a\u03c0.\nThe following corollary shows that the example in Section 2 admits tight matching upper bounds\non the regret and risk.\nCorollary 3.3 (Matching Upper Bounds). Consider a bandit problem where the set of arms is the\nunit sphere in Rr, and where W u\nt has a standard normal distribution with mean zero and variance\none for all t and u. Then, there exists an absolute constant a3 such that for any z \u2208Rr \\ {0} and\nT \u2265r,\nRegret (z, T, PEGE) \u2264a3\n\u0012\n\u2225z\u2225+\n1\n\u2225z\u2225\n\u0013\nr\n\u221a\nT .\nMoreover, if Z has a multivariate normal distribution with mean 0 and covariance matrix Ir/r,\nthen for all T \u2265r,\nRisk (T, PEGE) \u2264a3 r\n\u221a\nT .\nProof. Since the set of arms is the unit sphere and the errors are standard normal, Assumption\n1 is satis\ufb01ed with \u03c30 = \u00afu = \u03bb0 = 1. Moreover, as already discussed, the unit sphere satis\ufb01es\nthe SBAR(1) condition. Finally, By Lemma 3.2, the random vector Z satis\ufb01es the hypotheses of\nTheorem 3.1. The regret and risk bounds then follow immediately.\n3.1\nProof of Theorem 3.1\nThe proof of Theorem 3.1 relies on the following upper bound on the square of the norm di\ufb00erence\nbetween bZ(c) and Z.\nLemma 3.4 (Bound on Squared Norm Di\ufb00erence). Under Assumption 1, there exists a positive\nconstant h1 that depends only on \u03c30, \u00afu, and \u03bb0 such that for any z \u2208Rr and c \u22651,\nE\n\u0014\r\r\rbZ(c) \u2212z\n\r\r\r\n2 \f\f\f Z = z\n\u0015\n\u2264h1 r\nc\n.\nProof. Recall from the de\ufb01nition of the PEGE policy that the estimate bZ(c) at the end of the\nexploration phase of cycle c is given by\nbZ(c) = Z + 1\nc\n r\nX\nk=1\nbkb\u2032\nk\n!\u22121\nc\nX\ns=1\nr\nX\nk=1\nbkW bk(s) = Z + 1\nc\nc\nX\ns=1\nB V(s) ,\n16\nwhere B = (Pr\nk=1 bkb\u2032\nk)\u22121 and V(s) = Pr\nk=1 bkW bk(s). Note that the mean-zero random vari-\nables W bk(s) are independent of each other and their variance is bounded by some constant \u03b30\nthat depends only on \u03c30. Then, it follows from Assumption 1 that\nE\n\u0014\r\r\rbZ(c) \u2212z\n\r\r\r\n2 \f\f\f Z = z\n\u0015\n=\n1\nc2\nc\nX\ns=1\nE\n\u0002\nV(s)\u2032B2V(s)\n\u0003\n= 1\nc2\nc\nX\ns=1\nr\nX\nk=1\nE\n\u0014\u0010\nW bk(s)\n\u00112\u0015\nb\u2032\nkB2bk\n\u2264\n\u03b30\nc\nr\nX\nk=1\nb\u2032\nkB2bk \u2264\u03b30\nc\nr\nX\nk=1\n\u03bbmax\n\u0000B2\u0001\n\u2225bk\u22252 \u2264\u03b30 \u00afu2 r\n\u03bb2\n0 c\n,\nwhich is the desired result.\nThe next lemma gives an upper bound on the di\ufb00erence between two normalized vectors in\nterms of the di\ufb00erence of the original vectors.\nLemma 3.5 (Di\ufb00erence Between Normalized Vectors). For any z, w \u2208Rr, not both equal to zero,\n\r\r\r\r\nw\n\u2225w\u2225\u2212\nz\n\u2225z\u2225\n\r\r\r\r \u2264\n2 \u2225w \u2212z\u2225\nmax {\u2225z\u2225, \u2225w\u2225} ,\nwhere we de\ufb01ne 0/ \u22250\u2225to be some \ufb01xed unit vector.\nProof. The inequality is easily seen to hold if either w = 0 or z = 0. So, assume that both w and\nz are nonzero. Using the triangle inequality and the fact that\n\f\f \u2225w\u2225\u2212\u2225z\u2225\n\f\f \u2264\u2225w \u2212z\u2225, we have\nthat\n\r\r\r\r\nw\n\u2225w\u2225\u2212\nz\n\u2225z\u2225\n\r\r\r\r \u2264\n\r\r\r\r\nw\n\u2225w\u2225\u2212\nz\n\u2225w\u2225\n\r\r\r\r +\n\r\r\r\r\nz\n\u2225w\u2225\u2212\nz\n\u2225z\u2225\n\r\r\r\r = \u2225w \u2212z\u2225\n\u2225w\u2225\n+ \u2225z\u2225\n\f\f\f\f\n1\n\u2225w\u2225\u2212\n1\n\u2225z\u2225\n\f\f\f\f \u22642 \u2225w \u2212z\u2225\n\u2225w\u2225\n.\nBy symmetry, we also have\n\r\r\r w\n\u2225w\u2225\u2212\nz\n\u2225z\u2225\n\r\r\r \u22642\u2225w\u2212z\u2225\n\u2225z\u2225\n, which gives the desired result.\nThe following lemma gives an upper bound on the expected instantaneous regret under the\ngreedy decision G(c) during the exploitation phase of cycle c.\nLemma 3.6 (Regret Under the Greedy Decision). Suppose that Assumption 1 holds and the sets\nUr satisfy the SBAR(J) condition. Then, there exists a positive constant h2 that depends only on\n\u03c30, \u00afu, \u03bb0, and J, such that for any z \u2208Rr and c \u22651,\nE\n\u0014\nmax\nu\u2208Ur z\u2032 (u \u2212G(c))\n\f\f\f Z = z\n\u0015\n\u2264r h2\nc \u2225z\u2225,\nProof. The result is trivially true when z = 0. So, let us \ufb01x some z \u2208Rr \\ {0}. By comparing the\ngreedy decision G(c) with the best arm u\u2217(z), we see that the instantaneous regret satis\ufb01es\nz\u2032 (u\u2217(z) \u2212G(c))\n=\n\u0010\nz \u2212bZ(c)\n\u0011\u2032\nu\u2217(z) + (u\u2217(z) \u2212G(c))\u2032 bZ(c) +\n\u0010\nbZ(c) \u2212z\n\u0011\u2032\nG(c)\n\u2264\n\u0010\nz \u2212bZ(c)\n\u0011\u2032\nu\u2217(z) +\n\u0010\nbZ(c) \u2212z\n\u0011\u2032\nG(c)\n=\n\u0010\nbZ(c) \u2212z\n\u0011\u2032\n(G(c) \u2212u\u2217(z)) =\n\u0010\nbZ(c) \u2212z\n\u0011\u2032 \u0010\nu\u2217\u0010\nbZ(c)\n\u0011\n\u2212u\u2217(z)\n\u0011\n,\n17\nwhere the inequality follows from the de\ufb01nition of the greedy decision in Equation (2), and the\n\ufb01nal equality follows from the fact that G(c) = u\u2217\u0010\nbZ(c)\n\u0011\n. As a convention, we de\ufb01ne 0/ \u22250\u2225to\nsome \ufb01xed unit vector and set u\u2217(0) = u\u2217(0/ \u22250\u2225).\nIt then follows from the Cauchy-Schwarz Inequality that, with probability one,\nz\u2032 (u\u2217(z) \u2212G(c))\n\u2264\n\r\r\rbZ(c) \u2212z\n\r\r\r\n\r\r\ru\u2217\u0010\nbZ(c)\n\u0011\n\u2212u\u2217(z)\n\r\r\r\n=\n\r\r\rbZ(c) \u2212z\n\r\r\r\n\r\r\r\r\ru\u2217\n bZ(c)\n\u2225bZ(c)\u2225\n!\n\u2212u\u2217\n\u0012 z\n\u2225z\u2225\n\u0013\r\r\r\r\r\n\u2264\nJ\n\r\r\rbZ(c) \u2212z\n\r\r\r\n\r\r\r\r\r\nbZ(c)\n\u2225bZ(c)\u2225\n\u2212\nz\n\u2225z\u2225\n\r\r\r\r\r \u2264\n2J\n\r\r\rbZ(c) \u2212z\n\r\r\r\n2\n\u2225z\u2225\n,\nwhere the equality follows from the fact that u\u2217(z) = u\u2217(\u03bbz) for all \u03bb > 0. The second inequality\nfollows from condition SBAR(J), and the \ufb01nal inequality follows from Lemma 3.5. The desired\nresult follows by taking conditional expectations, given Z = z, and applying Lemma 3.4.\nWe can now complete the proof of Theorem 3.1, by adding the regret over the di\ufb00ernt times\nand cycles. By Assumption 1 and the Cauchy-Schwarz Inequality, the instantaneous regret from\nplaying any arm u \u2208Ur is bounded above by maxv\u2208Ur z\u2032 (v \u2212u) \u22642 \u00afu \u2225z\u2225. Consider an arbitrary\ncycle c.\nThen, the total regret incurred during the exploration phase (with r periods) in this\ncycle is bounded above by 2 \u00afu r \u2225z\u2225. During the exploitation phase of cycle c, we always play\nthe greedy arm G(c). The expected instantaneous regret in each period during the exploitation\nphase is bounded above by rh2/c \u2225z\u2225. So, the total regret during cycle c is bounded above by\n2 \u00afu r \u2225z\u2225+ h2 r/ \u2225z\u2225. Summing over K cycles, we obtain\nRegret\n \nz, rK +\nK\nX\nc=1\nc, PEGE\n!\n\u2264h3 r \u2225z\u2225K + h4\nK\nX\nc=1\nr\n\u2225z\u2225,\nfor some positive constants h3 and h4 that depend only on \u03c30, \u00afu, \u03bb0, and J.\nConsider an arbitrary time period T \u2265r and z \u2208Rr. Let K0 =\nl\u221a\n2T\nm\n. Note that the total\ntime periods after K0 cycles is at least T because rK0 + PK0\nc=1 c \u2265PK0\nc=1 c = K0(K0+1)\n2\n\u2265K2\n0\n2 \u2265T .\nSince the cumulative regret is nondecreasing over time, it follows that\nRegret (z, T, PEGE)\n\u2264\nRegret\n \nz, rK0 +\nK0\nX\nc=1\nc, PEGE\n!\n\u2264\nh3 r \u2225z\u2225K0 + h4\nrK0\n\u2225z\u2225\n\u22643 max{h3, h4}\n\u0012\n\u2225z\u2225+\n1\n\u2225z\u2225\n\u0013\nr\n\u221a\nT ,\nwhere the \ufb01nal inequality follows because K0 =\nl\u221a\n2T\nm\n\u22643\n\u221a\nT. The risk bound follows by taking\nexpectations and using the assumption on the boundedness of E[ \u2225Z\u2225] and E[ 1/ \u2225Z\u2225].\n18\n4.\nA Policy for General Bandits\nWe have shown that when a bandit has a smooth best arm response, the PEGE policy achieves\noptimal O(r\n\u221a\nT) regret and Bayes risk. The general idea is that when the estimation error is small,\nthe instantaneous regret of the greedy decision based on our estimate bZ(c) can be of the same order\nas \u2225Z\u2212bZ(c)\u2225. However, under the smoothness assumption, this upper bound on the instantaneous\nregret is improved to O\n\u0010\n\u2225Z \u2212bZ(c)\u22252\u0011\n, as shown in the proof of Lemma 3.6, and this enables us\nto separate exploration from exploitation.\nHowever, if the number of arms is \ufb01nite or if the collection of arms is an arbitrary compact\nset, then the PEGE policy may not be e\ufb00ective. This is because a small estimation error may\nhave a disproportionately large e\ufb00ect on the arm chosen by a greedy policy, leading to a large\ninstantaneous regret. In this section, we discuss a policy \u2013 which we refer to as the Uncertainty\nEllipsoid (UE) policy \u2013 that can be applied to any bandit problem, at the price of slightly higher\nregret and Bayes risk. In contrast to the PEGE policy, the UE policy combines active exploration\nand exploitation in every period.\nAs discussed in the introduction, the UE policy is closely related to the algorithms described\nin Auer (2002) and Dani et al. (2008a), but also has the \u201canytime\u201d property (the policy does not\nrequire prior knowledge of the time horizon T), and also allows the random vector Z and the errors\nW u\nt to be unbounded. For the sake of completeness, we give a detailed description of our policy and\nstate the regret and risk bounds that we obtain. The reader can \ufb01nd the proofs of these bounds in\nAppendix B.\nTo facilitate exposition, we introduce a constant that will appear in the description of the policy,\nnamely,\n\u03ba0 = 2\ns\n1 + log\n\u0012\n1 + 36 \u00afu2\n\u03bb0\n\u0013\n,\n(3)\nwhere the parameters \u00afu and \u03bb0 are given in Assumption 1. The UE policy maintains, at each time\nperiod t, the following two pieces of information.\n1. The ordinary least squares (OLS) estimate de\ufb01ned as follows: if U1, . . . , Ut are the arms\nchosen during the \ufb01rst t periods, then the OLS estimate bZt is given by4:\nCt =\n t\nX\ns=1\nUsU\u2032\ns\n!\u22121\n,\nMt =\nt\nX\ns=1\nUsWs ,\nand\nbZt = Ct\nt\nX\ns=1\nUsXs = Z + CtMt .\n(4)\n4Let us note that we are abusing notation here. Throughout this section bZt stands for the OLS estimate, which is\ndi\ufb00erent from the least mean squares estimator E\n\u0002\nZ\n\f\f Ht\n\u0003\nintroduced in Section 2.\n19\nIn contrast to the PEGE policy, whose estimates relied only on the rewards observed in the\nexploration phases, the estimate bZt incorporates all available information up to time t. We\ninitialize the policy by playing r linearly independent arms, so that Ct is positive de\ufb01nite for\nt \u2265r.\n2. An uncertainty ellipsoid Et \u2286Rr associated with the estimate bZt, de\ufb01ned by,\nEt =\n\u001a\nw \u2208Rr : w\u2032C\u22121\nt w \u2264\n\u0010\n\u03b1\np\nlog t\np\nmin{r log t , |Ur|}\n\u00112\u001b\nand\n\u03b1 = 4 \u03c30 \u03ba2\n0 ,\n(5)\nwhere the parameters \u03c30 and \u03ba0 are given in Assumption 1(a) and Equation (3). The uncer-\ntainty ellipsoid Et represents the set of likely \u201cerrors\u201d associated with the estimate bZt. We\nde\ufb01ne the uncertainty radius Ru\nt associated with each arm u as follows:\nRu\nt = max\nv\u2208Et v\u2032u = \u03b1\np\nlog t\np\nmin{r log t , |Ur|} \u2225u\u2225Ct .\n(6)\nA formal description of the policy is given below.\nUncertainty Ellipsoid (UE)\nInitialization: During the \ufb01rst r periods, play the r linearly independent arms b1, b2, . . . , br\ngiven in Assumption 1(b). Determine the OLS estimate bZr, the uncertainty ellipsoid Er, and the\nuncertainty radius associated with each arm.\nDescription: For t \u2265r + 1, do the following:\n(i) Let Ut \u2208Ur be an arm that gives the maximum estimated reward over the ellipsoid bZt\u22121+Et\u22121,\nthat is,\nUt = arg max\nv\u2208Ur\n\u001a\nv\u2032bZt\u22121 + max\nw\u2208Et\u22121 w\u2032v\n\u001b\n= arg max\nv\u2208Ur\nn\nv\u2032bZt\u22121 + Rv\nt\u22121\no\n,\n(7)\nwhere the uncertainty radius Rv\nt\u22121 is de\ufb01ned in Equation (6); ties are broken arbitrarily.\n(ii) Play arm Ut and observe the resulting reward Xt.\n(iii) Update the OLS estimate bZt, the uncertainty ellipsoid Et, and the uncertainty radius Ru\nt of\neach arm u, using the formulas in Equations (4), (5), and (6).\nBy choosing an arm that maximizes the estimated reward over the ellipsoid bZt + Et, our pol-\nicy involves simultaneous exploitation (via the term v\u2032bZt) and exploration (via the term Rv\nt =\nmaxw\u2208Et w\u2032v) in every period. The ellipsoid Et re\ufb02ects the uncertainty in our OLS estimate bZt.\nIt generalizes the classical upper con\ufb01dence index introduced by Lai and Robbins (1985), to ac-\ncount for correlations among the arm rewards. In the special case of r independent arms where\nUr = {e1, . . . , er}, it is easy to verify that for each arm e\u2113, the expression e\u2032\n\u2113bZt + Re\u2113\nt\ncoincides (up\n20\nto a scaling constant) with the upper con\ufb01dence bound used by Auer et al. (2002). Our de\ufb01nition\nof the uncertainty radius involves an extra factor of\np\nmin{r log t, |Ur|}, in order to handle the case\nwhere the arms are not standard unit vectors, and the rewards are correlated.\nThe main results of this section are given in the following two theorems. The \ufb01rst theorem\nestablishes upper bounds on the regret and risk when the set of arms is an arbitrary compact set.\nThis result shows that the UE policy is nearly optimal, admitting upper bounds that are within a\nlogarithmic factor of the \u2126(r\n\u221a\nT) lower bounds given in Theorem 2.1. Although the proof of this\ntheorem makes use of somewhat di\ufb00erent (and novel) large deviation inequalities for adaptive least\nsquares estimators, the argument shares similarities with the proofs given in Dani et al. (2008a),\nand we omit the details. The reader can \ufb01nd a complete proof in Appendix B.2.\nTheorem 4.1 (Bounds for General Compact Sets of Arms). Under Assumption 1, there exist\npositive constants a4 and a5 that depend only on the parameters \u03c30, \u00afu, and \u03bb0, such that for all\nT \u2265r + 1 and z \u2208Rr,\nRegret (z, T, UE)\n\u2264a4r \u2225z\u2225+ a5 r\n\u221a\nT log3/2 T ,\nand\nRisk (T, UE) \u2264a4r E [\u2225Z\u2225] + a5 r\n\u221a\nT log3/2 T .\nFor any arm u \u2208Ur and z \u2208Rr, let \u2206u (z) denote the di\ufb00erence between the maximum expected\nreward and the expected reward of arm u when Z = z, that is,\n\u2206u (z) = max\nv\u2208Ur v\u2032z \u2212u\u2032z .\nWhen the number of arms is \ufb01nite, it turns out that we can obtain bounds on regret and risk that\nscale more gracefully over time, growing as log T and log2 T, respectively. This result is stated in\nTheorem 4.2, which shows that, for a \ufb01xed set of arms, the UE policy is asymptotically optimal\nas a function time, within a constant factor of the lower bounds established by Lai and Robbins\n(1985) and Lai (1987).\nTheorem 4.2 (Bounds for Finitely Many Arms). Under Assumption 1, there exist positive con-\nstants a6 and a7 that depend only on the parameters \u03c30, \u00afu, and \u03bb0 such that for all T \u2265r + 1 and\nz \u2208Rr,\nRegret (z, T, UE)\n\u2264a6 |Ur| \u2225z\u2225+ a7 |Ur|\nX\nu\u2208Ur\nmin\n\u001a log T\n\u2206u (z) , T\u2206u(z)\n\u001b\n.\nMoreover, suppose that there exists a positive constant M0 such that, for all arms u, the distribution\nof the random variable \u2206u (Z) is described by a point mass at 0, and a density function that is\n21\nbounded above by M0 on R+. Then, there exist positive constants a8 and a9 that depend only on\nthe parameters \u03c30, \u00afu, \u03bb0, and M0, such that for all T \u2265r + 1,\nRisk (T, UE) \u2264a8 |Ur| E [\u2225Z\u2225] + a9 |Ur|2 log2 T .\nProof. For any arm u \u2208Ur and z \u2208Rr, let the random variable N u(z, T) denote the total number\nof times that the arm u is chosen during periods 1 through T, given that Z = z. Using an argument\nsimilar to the one in Auer et al. (2002), we can show that\nE [N u(z, T) | Z = z] \u22646 + 4\u03b12 |Ur| log T\n(\u2206u (z))2\n.\nThe reader can \ufb01nd a proof of this result in Appendix B.3.\nThe regret bound in Theorem 4.2 then follows immediately from the above upper bound and\nthe fact that N u(z, T) \u2264T with probability one, because\nRegret (z, T, UE)\n=\nX\nu\u2208Ur\n\u2206u (z) E [N u(z, T) | Z = z] \u2264\nX\nu\u2208Ur\n\u2206u (z) min\n\u001a\n6 + 4\u03b12 |Ur| log T\n(\u2206u (z))2\n, T\n\u001b\n\u2264\n6\nX\nu\u2208Ur\n\u2206u (z) + max{4\u03b12, 1} |Ur|\nX\nu\u2208Ur\nmin\n\u001a log T\n\u2206u (z) , T\u2206u (z)\n\u001b\n,\nand the desired result follows from the fact that \u2206u(z) = maxv\u2208Ur (v \u2212u)\u2032 z \u22642\u00afu \u2225z\u2225, by the\nCauchy-Schwarz Inequality.\nWe will now establish an upper bound on the Bayes risk. From the regret bound, it su\ufb03ces to\nshow that for any u \u2208Ur,\nE\n\u0014\nmin\n\u001a log T\n\u2206u (Z), T\u2206u (Z)\n\u001b\u0015\n\u2264(M0 + 1) log T + M0 log2 T .\nLet qu(\u00b7) denote the density function associated with the random variable \u2206u (Z). Then,\nE\n\u0014\nmin\n\u001a log T\n\u2206u (Z), T\u2206u (Z)\n\u001b\u0015\n=\nZ\nq\nlog T\nT\n0\nmin\n\u001alog T\nx\n, Tx\n\u001b\nqu(x)dx\n+\nZ 1\nq\nlog T\nT\nmin\n\u001alog T\nx\n, Tx\n\u001b\nqu(x)dx +\nZ \u221e\n1\nmin\n\u001alog T\nx\n, Tx\n\u001b\nqu(x)dx .\nWe will now proceed to bound each of the three terms on the right hand side of the above\nequality. Having assumed that qu(\u00b7) \u2264M0, the \ufb01rst term satis\ufb01es\nZ \u221a\n(log T)/T\n0\nmin\n\u001alog T\nx\n, Tx\n\u001b\nqu(x)dx \u2264M0\nZ \u221a\n(log T)/T\n0\nTx dx = M0T x2\n2\n\f\f\f\n\u221a\n(log T)/T\n0\n\u2264M0 log T .\nFor the second term, note that\nZ 1\n\u221a\n(log T)/T\nmin\n\u001alog T\nx\n, Tx\n\u001b\nqu(x)dx\n\u2264\nM0\nZ 1\n\u221a\n(log T)/T\nlog T\nx\ndx = M0 log T \u00b7\n\u0012\nlog x\n\f\f\f\n1\n\u221a\n(log T)/T\n\u0013\n=\nM0 (log T) \u00b7 log T \u2212log log T\n2\n\u2264M0 log2 T ,\n22\nwhere the last inequality follows from the fact that log T \u2212log log T \u22642 log T for all T \u22652. To\nevaluate the last term, note that log T\nx\n\u2264log T for all x \u22651, and thus,\nR \u221e\n1 min\nn\nlog T\nx , Tx\no\nqu(x)dx \u2264\nlog T\nR \u221e\n1\nqu(x) \u2264log T . Putting everything together, we have that E\nh\nmin\nn\nlog T\n\u2206u(Z) , T\u2206u (Z)\noi\n\u2264\n(M0 + 1) log T + M0 log2 T, which is the desired result.\nWe conclude this section by giving an example of a random vector Z that satis\ufb01es the condition\nin Theorem 4.2. A similar example also appears in Example 2 of Lai (1987).\nExample 4.3 (IID Random Variables). Suppose Ur = {e1, . . . , er} and Z = (Z1, . . . , Zr), where\nthe random variables Zk are independent and identically distributed with a common cumula-\ntive distribution function F and a density function f : R \u2192R which is bounded above by M.\nThen, for each k, the random variable \u2206ek (Z) is given by \u2206ek (Z) = (maxj=1,...,r Zj) \u2212Zk =\nmax {0 , maxj\u0338=k {Zj \u2212Zk}} . It is easy to verify that \u2206ek (Z) has a point mass at 0 and a con-\ntinuous density function qk(\u00b7) on R+ given by: for any x > 0,\nqk(x) = (r \u22121)\nZ\n{F(zk + x)}r\u22122 f(zk + x)f(zk)dzk \u2264(r \u22121)M .\n4.1\nRegret Bounds for Polyhedral Sets of Arms\nIn this section, we focus on the regret pro\ufb01les when the set of arms Ur is a polyhedral set. Let\nE(Ur) denote the set of extreme points of Ur. From a standard result in linear programming, for\nall z \u2208Rr,\nmax\nu\u2208Ur u\u2032z =\nmax\nu \u2208E(Ur) u\u2032z .\nSince a polyhedral set has a \ufb01nite number of extreme points (|E(Ur)| < \u221e), the parameterized\nbandit problem can be reduced to the standard multi-armed bandit problem, where each arm\ncorresponds to an extreme point of Ur.\nWe can thus apply the algorithm of Lai and Robbins\n(1985) and obtain the following upper bound on the T-period cumulative regret for polyhedra\nRegret (z, T, Lai\u2019s Algorithm) = O\n\u0012\n|E(Ur)| \u00b7 log T\nmin {\u2206u(z) : \u2206u(z) > 0}\n\u0013\n,\n(8)\nwhere the denominator corresponds to the di\ufb00erence between the expected reward of the optimal\nand the second best extreme points. The algorithm of Lai and Robbins (1985) is e\ufb00ective only when\nthe polyhedral set Ur has a small number of extreme points, as shown by the following examples.\nExample 4.4 (Simplex). Suppose Ur = {u \u2208Rr : Pr\ni=1 |ui| \u22641} is an r-dimensional unit simplex.\nThen, Ur has 2r extreme points, and Equation (8) gives an O(r log T) upper bound on the regret.\n23\nExample 4.5 (Linear Constraints). Suppose that Ur = {u \u2208Rr : Au \u2264b and u \u22650}, where A\nis a p \u00d7 r matrix with p \u2264r.\nIt follows from the standard linear programming theory that\nevery extreme point is a basic feasible solution, which has at most p nonzero coordinates (see, for\nexample, Bertsimas and Tsitsiklis, 1997). Thus, the number of extreme points is bounded above\nby\n\u0000r+p\np\n\u0001\n= O((2r)p), and Equation (8) gives an O((2r)p log T) upper bound on the regret.\nIn general, the number of extreme points of a polyhedron can be very large, rendering the\nbandit algorithm of Lai and Robbins (1985) ine\ufb00ective; consider, for example, the r-dimensional\ncube Ur = {u \u2208Rr : |ui| \u22641 for all i}, which has 2r extreme points. Moreover, we cannot apply\nthe results and algorithms from Section 3 to the convex hull of Ur. This is because the convex\nhull of a polyhedron is not strongly convex (it cannot be written as an intersection of Euclidean\nballs), and thus, it does not satisfy the required SBAR(\u00b7) condition in Theorem 3.1. The UE policy\nin the previous section gives O(r\n\u221a\nT log3/2 T) regret and risk upper bounds. However, \ufb01nding an\nalgorithm speci\ufb01cally for polyhedral sets that yields an O(r\n\u221a\nT) regret upper bound (without an\nadditional logarithmic factor) remains an open question.\n5.\nConclusion\nWe analyzed a class of multiarmed bandit problems where the expected reward of each arm depends\nlinearly on an unobserved random vector Z \u2208Rr, with r \u22652. Our model allows for correlations\namong the rewards of di\ufb00erent arms. When we have a smooth best arm response, we showed that\na policy that alternates between exploration and exploitation is optimal. For a general bandit, we\nproposed a near-optimal policy that performs active exploration in every period. For \ufb01nitely many\narms, our policy achieves asymptotically optimal regret and risk as a function of time, but scales\nwith the square of the number of arms. Improving the dependence on the number of arms remains\nan open question. It would also be interesting to study more general correlation structures. Our\nformulation assumes that the vector of expected rewards lies in an r-dimensional subspace spanned\nby a known set of basis functions that describe the characteristics of the arms. Extending our work\nto a setting where the basis functions are unknown has the potential to broaden the applicability\nof our model.\nAcknowledgement\nThe authors would like to thank Adrian Lewis and Mike Todd for helpful and stimulating discussions\non the structure of positive de\ufb01nite matrices and the eigenvalues associated with least squares\nestimators, Gena Samorodnitsky for sharing his deep insights on the application of large deviation\ntheory to this problem, Adam Mersereau for his contributions to the problem formulation, and\n24\nAssaf Zeevi for helpful suggestions and discussions during the \ufb01rst author\u2019s visit to Columbia\nGraduate School of Business. We also want to thank the Associate Editor and the referee for\ntheir helpful comments and suggestions on the paper. This research is supported in part by the\nNational Science Foundation through grants DMS-0732196, ECCS-0701623, CMMI-0856063, and\nCMMI-0855928.\nReferences\nAbe, N., and P. M. Long. 1999. Associative reinforcement learning using linear probabilistic concepts. In Proceedings\nof the 16th International Conference on Machine Learning, 3\u201311. San Francisco, CA: Morgan Kaufman.\nAgrawal, R. 1995.\nSample mean based index policies with O(log n) regret for the multi-armed bandit problem.\nAdvances in Applied Probability 27 (4): 1054\u20131078.\nAgrawal, R., D. Teneketzis, and V. Anantharam. 1989.\nAsymptotically e\ufb03cient adaptive allocation schemes for\ncontrolled I.I.D. processes: \ufb01nite parameter space. IEEE Transactions on Automatic Control 34 (3): 258\u2013267.\nAuer, P. 2002. Using con\ufb01dence bounds for exploitation-exploration trade-o\ufb00s. Journal of Machine Learning Re-\nsearch 3:397\u2013422.\nAuer, P., N. Cesa-Bianchi, and P. Fischer. 2002. Finite-time analysis of the multiarmed bandit problem. Machine\nLearning 47 (2): 235\u2013256.\nBerry, D., and B. Fristedt. 1985. Bandit Problems: Sequential Allocation of Experiments. London: Chapman and\nHall.\nBertsekas, D. 1995. Dynamic Programming and Optimal Control, Volume 1. Belmont, MA: Athena Scienti\ufb01c.\nBertsekas, D., and J. N. Tsitsiklis. 1996. Neuro-Dynamic Programming. Belmont, MA: Athena Scienti\ufb01c.\nBertsimas, D., and J. N. Tsitsiklis. 1997. Introduction to Linear Optimization. Belmont, MA: Athena Scienti\ufb01c.\nBhatia, R. 2007. Positive De\ufb01nite Matrices. Princeton, NJ: Princeton University Press.\nBlum, J. R. 1954. Multidimensional Stochastic Approximation Methods. Annals of Mathematical Statistics 25 (4):\n737\u2013744.\nCicek, D., M. Broadie, and A. Zeevi. 2009. General bounds and \ufb01nite-time performance improvement for the Kiefer-\nWolfowitz stochastic approximation algorithm. Working paper. Columbia Graduate School of Business.\nDani, V., T. P. Hayes, and S. M. Kakade. 2008a. Stochastic linear optimization under bandit feedback. In Proceedings\nof the 21th Annual Conference on Learning Theory (COLT 2008), 355\u2013366.\nDani, V., T. P. Hayes, and S. M. Kakade. December 2008b. Stochastic linear optimization under bandit feedback.\nWorking Paper. Available at http://ttic.uchicago.edu/~sham/papers/ml/bandit_linear_long.pdf.\nDe la Pe\u02dcna, V. H., M. J. Klass, and T. L. Lai. 2004. Self-normalized processes: exponential inequalities, moment\nbounds and iterated logarithm laws. Annals of Probability 32 (3A): 1902\u20131933.\nDudley, R. M. 1999. Uniform Central Limit Theorems. Cambridge: Cambridge University Press.\nFeldman, D. 1962. Contributions to the \u201ctwo-armed bandit\u201d problem. Annals of Mathematical Statistics 33 (3):\n847\u2013856.\n25\nFiedler, M., and V. Pt\u00b4ak. 1997. A new positive de\ufb01nite geometric mean of two positive de\ufb01ntie matrices. Linear\nAlgebra and Its Applications 251 (1): 1\u201320.\nGinebra, J., and M. K. Clayton. 1995. Response surface bandits. Journal of the Royal Statistical Society. Series B\n(Methodological) 57 (4): 771\u2013784.\nGoldenshluger, A., and A. Zeevi. 2008. Performance limitations in bandit problems with side observations. Working\npaper. Columbia Graduate School of Business.\nGoldenshluger, A., and A. Zeevi. 2009. Woodroofe\u2019s one-armed bandit problem revisited. Annals of Applied Proba-\nbility 19 (4): 1603\u20131633.\nKeener, R. 1985. Further contributions to the \u201ctwo-armed bandit\u201d problem. Annals of Statistics 13 (1): 418\u2013422.\nKiefer, J., and J. Wolfowitz. 1952.\nStochastic estimation of the maximum of a regression function.\nAnnals of\nMathematical Statistics 23 (3): 462\u2013466.\nLai, T. 2003. Stochastic Approximation (Invited Paper). The Annals of Statistics 31 (2): 391\u2013406.\nLai, T. L. 1987. Adaptive treatment allocation and the multi-armed bandit problem. Annals of Statistics 15 (3):\n1091\u20131114.\nLai, T. L., and H. Robbins. 1985. Asymptotically e\ufb03cient adaptive allocation rules. Advances in Applied Mathemat-\nics 6 (1): 4\u201322.\nMersereau, A. J., P. Rusmevichientong, and J. N. Tsitsiklis. 2009. A structured multiarmed bandit problem and the\ngreedy policy. IEEE Transactions on Automatic Control 54 (12): 2787\u20132802.\nPandey, S., D. Chakrabarti, and D. Agrawal. 2007. Multi-armed bandit problems with dependent arms. In Proceedings\nof the 24th International Conference on Machine Learning, 721\u2013728.\nPolovinkin, E. S. 1996. Strongly convex analysis. Sbornik: Mathematics 187 (2): 259\u2013286.\nPressman, E. L., and I. N. Sonin. 1990. Sequential Control With Incomplete Information. London: Academic Press.\nRobbins, H. 1952. Some aspects of the sequential design of experiments. Bulletin of the American Mathematical\nSociety 58 (5): 527\u2013535.\nRobbins, H., and S. Monro. 1951. A stochastic approximation method. Annals of Mathematical Statistics 22 (3):\n400\u2013407s.\nSherman, J., and W. J. Morrison. 1950. Adjustment of an inverse matrix corresponding to a change in one element\nof a given matrix. Annals of Mathematical Statistics 21 (1): 124\u2013127.\nThompson, W. R. 1933. On the likelihood that one unknown probability exceeds another in view of the evidence of\ntwo samples. Biometrika 25 (3): 285\u2013294.\nWang, C.-C., S. R. Kulkarni, and H. V. Poor. 2005a. Arbitrary side observations in bandit problems. Advances in\nApplied Mathematics 34 (4): 903\u2013938.\nWang, C.-C., S. R. Kulkarni, and H. V. Poor. 2005b. Bandit problems with side observations. IEEE Transactions\non Automatic Control 50 (3): 338\u2013355.\n26\nA.\nProperties of Normal Vectors\nIn this section, we prove that if Z has a multivariate normal distribution with mean 0 \u2208Rr and\ncovariance matrix Ir/r, then Z has the properties described in Lemmas 2.4 and 3.2.\nA.1\nProof of Lemma 2.4\nWe want to establish a lower bound on Pr {\u03b8 \u2264\u2225Z\u2225\u2264\u03b2}. Let Y = (Y1, . . . , Yr) denote the\nstandard multivariate normal random vector with mean 0 and identity covariance matrix Ir. By\nour hypothesis, Z has the same distribution as Y/\u221ar, which implies that\nPr {\u03b8 \u2264\u2225Z\u2225\u2264\u03b2} = Pr\n\b\n\u03b8\u221ar \u2264\u2225Y\u2225\u2264\u03b2\u221ar\n\t\n= 1 \u2212Pr\nn\n\u2225Y\u22252 < \u03b82r\no\n\u2212Pr\nn\n\u2225Y\u22252 > \u03b22r\no\n.\nBy de\ufb01nition, \u2225Y\u22252 = Y 2\n1 + \u00b7 \u00b7 \u00b7 + Y 2\nr has a chi-square distribution with r degrees of freedom. By\nthe Markov Inequality, Pr\nn\n\u2225Y\u22252 > \u03b22r\no\n\u2264E\nh\n\u2225Y\u22252i\n/(\u03b22r) = 1/\u03b22. We will now establish an\nupper bound on Pr\nn\n\u2225Y\u22252 < \u03b82r\no\n. Note that, for any \u03bb > 0,\nPr\nn\n\u2225Y\u22252 < \u03b82r\no\n= Pr\nn\ne\u2212\u03bb Pr\nk=1 Y 2\nk > e\u2212\u03bb\u03b82ro\n\u2264e\u03bb\u03b82r \u00b7 E\n\" r\nY\nk=1\ne\u2212\u03bbY 2\nk\n#\n=\n \ne\u03bb\u03b82\n\u221a\n1 + 2\u03bb\n!r\n,\nwhere last equality follows from the fact that Y1, . . . , Yr are independent standard normal random\nvariables and thus, E\nh\ne\u2212\u03bbY 2\nk\ni\n= 1/\n\u221a\n1 + 2\u03bb for \u03bb > 0. Set \u03bb = 1/\u03b82, and use the facts \u03b8 \u22641/2 \u2264\n\u221a\n2/e and r \u22652, to obtain\nPr\nn\n\u2225Y\u22252 < \u03b82r\no\n\u2264\n\u0012\ne\u03b8\n\u221a\n2 + \u03b82\n\u0013r\n\u2264\n\u0012 e\u03b8\n\u221a\n2\n\u0013r\n\u2264\n\u0012 e\u03b8\n\u221a\n2\n\u00132\n= e2\u03b82\n2\n\u22644\u03b82 ,\nwhich implies that Pr {\u03b8 \u2264\u2225Z\u2225\u2264\u03b2} \u22651 \u22121\n\u03b22 \u22124\u03b82, which is the desired result.\nA.2\nProof of Lemma 3.2\nFor part (a) of the lemma, we have\nE [ 1/ \u2225Z\u2225] =\nZ \u221e\n0\n1\nxg(x) dx \u2264M0\nZ \u03c1\n0\nx\u03c1\u22121 dx + 1\n\u03c1\nZ \u221e\n\u03c1\ng(x) dx \u2264M0\n\u03c1\u03c1\n\u03c1 + 1\n\u03c1.\nFor the proof of part (b), let Y = (Y1, . . . , Yr) be a standard multivariate normal random vector\nwith mean 0 and identity covariance matrix, Ir. Then, Z has the same distribution as Y/\u221ar. Note\nthat \u2225Y\u22252 has a chi-square distribution with r degrees of freedom. Thus,\nE[ \u2225Z\u2225] = 1\n\u221arE[ \u2225Y\u2225] \u22641\n\u221ar\nq\nE[ \u2225Y\u22252 ] = 1\n\u221ar\n\u221ar = 1 .\nWe will now establish an upper bound on E[ 1/ \u2225Z\u2225] = \u221ar E[ 1/ \u2225Y\u2225]. For r = 2, since \u2225Y\u2225\nhas a chi distribution with two degrees of freedom, we have that\nE[ 1/ \u2225Z\u2225] =\n\u221a\n2\nZ \u221e\n0\n1\nx \u00b7 xe\u2212x2/2 dx =\n\u221a\n2\nZ \u221e\n0\ne\u2212x2/2 dx = \u221a\u03c0 .\n27\nConsider the case where r \u22653. Then,\nE[ 1/ \u2225Z\u2225] = \u221ar E[ 1/ \u2225Y\u2225] \u2264\u221ar\nq\nE[ 1/ \u2225Y\u22252 ] .\nUsing the formula for the density of the chi-square distribution, we have\nE[ 1/ \u2225Y\u22252 ]\n=\nZ \u221e\n0\n1\nx \u00b7\n1\n2r/2\u0393(r/2)x(r/2)\u22121e\u2212x/2 dx\n=\n2(r/2)\u22121\n2r/2\n\u00b7 \u0393((r/2) \u22121)\n\u0393(r/2)\n\u00b7\nZ \u221e\n0\n1\n2(r\u22122)/2\u0393((r \u22122)/2)x((r\u22122)/2)\u22121e\u2212x/2 dx\n=\n1\n2((r/2) \u22121) =\n1\nr \u22122 \u22643\nr ,\nwhere the third equality follows from the fact that \u0393(r/2) = ((r/2) \u22121)\u00b7\u0393((r/2)\u22121) for r \u22653 and\nthe integrand is the density function of the chi-square distribution with r\u22122 degrees of freedom and\nevaluates to 1. The last inequality follows because r \u22653. Thus, we have E[ 1/ \u2225Z\u2225] \u2264\n\u221a\n3 \u2264\u221a\u03c0,\nwhich is the desired result.\n28\nB.\nProof of Theorems 4.1 and 4.2\nIn the next section, we establish large deviation inequalities for adaptive least squares estimators\n(with unbounded error random variables), which will be used in the proof of Theorems 4.1 and 4.2,\ngiven in Sections B.2 and B.3, respectively.\nB.1\nLarge Deviation Inequalities\nThe \ufb01rst result extend the standard Cherno\ufb00Inequality to our setting involving uncertainty ellip-\nsoids when we have \ufb01nitely many arms.\nTheorem B.1 (Cherno\ufb00Inequality for Uncertainty Ellipsoids with Finitely Many Arms). Under\nAssumption 1, for any t \u2265r, x \u2208Rr, z \u2208Rr, and \u03b6 > 0,\nPr\nn\nx\u2032 \u0010\nbZt \u2212z\n\u0011\n> \u03b6 \u03c30 \u2225x\u2225Ct\n\f\f Z = z\no\n\u2264t5|Ur|e\u2212\u03b62/2,\nand\nPr\nn\n(Ut+1 \u2212x)\u2032 \u0010\nbZt \u2212z\n\u0011\n> \u03b6 \u03c30 \u2225Ut+1 \u2212x\u2225Ct\n\f\f Z = z\no\n\u2264t5|Ur|e\u2212\u03b62/2 .\nProof. We will only prove the second inequality because the proof of the \ufb01rst one follows the same\nargument. If the sequence of arms U1, U2, . . . is deterministic (and thus, the matrix Ct is also\ndeterministic), then\n(Ut+1 \u2212x)\u2032 \u0010\nbZt \u2212z\n\u0011\n\u2225Ut+1 \u2212x\u2225Ct\n=\nt\nX\ns=1\n(Ut+1 \u2212x)\u2032 CtUs\n\u2225Ut+1 \u2212x\u2225Ct\nWs\nand\nt\nX\ns=1\n\u0012(Ut+1 \u2212x)\u2032 CtUs\n\u2225Ut+1 \u2212x\u2225Ct\n\u00132\n= (Ut+1 \u2212x)\u2032 Ct\n\u0000Pt\ns=1 UsU\u2032\ns\n\u0001\nCt (Ut+1 \u2212x)\n(Ut+1 \u2212x)\u2032 Ct (Ut+1 \u2212x)\n= 1 .\nThe classical Cherno\ufb00Inequality for the sum of independent random variables (see, for example,\nChapter 1 in Dudley, 1999) then yields\nPr\nn\n(Ut+1 \u2212x)\u2032 \u0010\nbZt \u2212z\n\u0011\n> \u03b6 \u03c30 \u2225Ut+1 \u2212x\u2225Ct\n\f\f Z = z\no\n\u2264\nexp\n(\n\u2212\n\u03b62\u03c32\n0\n2\u03c32\n0\nPt\ns=1\n\u0000(Ut+1 \u2212x)\u2032 CtUs\n\u000e\n\u2225Ut+1 \u2212x\u2225Ct\n\u00012\n)\n= e\u2212\u03b62/2 .\nIn our setting, however, the arms Ut are random variables that depend on the accumulated history,\nand we cannot apply the standard Cherno\ufb00inequality directly. If N u(z, t) denotes the total number\nof times that arm u has been chosen during the \ufb01rst t periods given that Z = z, then\nCt =\n t\nX\ns=1\nUsU\u2032\ns\n!\u22121\n=\n X\nu\u2208Ur\nN u(z, t) uu\u2032\n!\u22121\n,\n29\nwhich shows that the matrix Ct is completely determined by the nonnegative integer random vari-\nables N u(z, t). Since 0 \u2264N u(z, t) \u2264t, the number of possible values of the vector (N u(z, t) : u \u2208Ur)\nis at most t|Ur|.\nIt then follows easily that the number of di\ufb00erent values of the ordered pair\n(Ut+1, Ct) is at most |Ur| t|Ur| \u2264t5|Ur|. To get the desired result, we can then use the union bound\nand apply the classical Cherno\ufb00Inequality to each ordered pair.\nWhen the number of arms is in\ufb01nite, the bounds in Theorem B.1 are vacuous. The following\ntheorem provides an extension of the Cherno\ufb00inequality to the case of in\ufb01nitely many arms.\nTheorem B.2 (Cherno\ufb00Inequality for Uncertainty Ellipsoids with In\ufb01nitely Many Arms). Under\nAssumption 1, for any t \u2265r, x \u2208Rr, z \u2208Rr, and \u03b6 \u22652,\nPr\nn\nx\u2032 \u0010\nbZt \u2212z\n\u0011\n> \u03b6 \u03ba0 \u03c30\np\nlog t \u2225x\u2225Ct\n\f\f Z = z\no\n\u2264tr \u03ba2\n0 e\u2212\u03b62/4 ,\nand\nPr\nn\n(Ut+1 \u2212x)\u2032 \u0010\nbZt \u2212z\n\u0011\n> \u03b6 \u03ba0 \u03c30\np\nlog t \u2225Ut+1 \u2212x\u2225Ct\n\f\f Z = z\no\n\u2264tr \u03ba2\n0 e\u2212\u03b62/4 .\nThe proof of Theorem B.2 makes use of the following series of lemmas.\nThe \ufb01rst lemma\nestablishes a tail inequality for a ratio of two random variables. De la Pe\u02dcna et al. (2004) gave a\nproof of this result in Corollary 2.2 (page 1908) of their paper .\nLemma B.3 (Exponential Inequality for Ratios, De la Pe\u02dcna et al., 2004). Let A and B be two\nrandom variables such that B \u22650 with probability one and E\nh\ne\u03b3A\u2212(\u03b32B2/2)i\n\u22641 for all \u03b3 \u2208R.\nThen, for all \u03b6 \u2265\n\u221a\n2 and y > 0,\nPr\n(\n|A| \u2265\u03b6\ns\n(B2 + y)\n\u0012\n1 + 1\n2 log\n\u0012\n1 + B2\ny\n\u0013\u0013)\n\u2264e\u2212\u03b62/2\nRecall from Equation (4) that Mt = Pt\ns=1 UsWs is the martingale associated with the least\nsquares estimate bZt.\nThe next lemma establishes a martingale inequality associated with the\ninner product x\u2032Mt for an arbitrary vector x \u2208Rr.\nThis result is based on Lemma B.3 with\nA = x\u2032Mt\n\u03c30\n= Pt\ns=1\n(x\u2032Us)\n\u03c30\nWs and B = \u2225x\u2225C\u22121\nt\n=\nq\nx\u2032C\u22121\nt x =\nqPt\ns=1 (x\u2032Us)2. We then use upper\nand lower bounds on B2 to establish bounds on the term log\n\u0010\n1 + B2\ny\n\u0011\n, for a suitable choice of y,\ngiving us the desired result.\nLemma B.4 (Martingale Inequality). Under Assumption 1, for any x \u2208Rr, t \u22651, and \u03b6 \u2265\n\u221a\n2,\nPr\nn\f\fx\u2032Mt\n\f\f > \u03b6 \u03ba0 \u03c30\np\nlog t \u2225x\u2225C\u22121\nt\no\n= Pr\n\b\nx\u2032MtM\u2032\ntx > \u03b62 \u03ba2\n0 \u03c32\n0(log t)\n\u0000x\u2032C\u22121\nt x\n\u0001\t\n\u2264e\u2212\u03b62/2 .\n30\nProof. Let x \u2208Rr and t \u22651 be given. Without loss of generality, we can assume that \u2225x\u2225= 1.\nLet the random variables A and B be de\ufb01ned by\nA = x\u2032Mt\n\u03c30\n=\nt\nX\ns=1\n(x\u2032Us)\n\u03c30\nWs\nand\nB = \u2225x\u2225C\u22121\nt\n=\nq\nx\u2032C\u22121\nt x =\nv\nu\nu\nt\nt\nX\ns=1\n(x\u2032Us)2 .\nFor any s, let Hs = (U1, X1, W1, . . . , Us, Xs, Ws) the history until the end of period s. By de\ufb01nition,\nUs is a function of Hs\u22121, and it follows from Assumption 1(a) that for any \u03b3 \u2208R,\nE\n\"\ne\n\u03b3\n\u03c30 (x\u2032Us)Ws\u2212\n\u03b32(x\u2032Us)2\n2\n\f\f\f Hs\u22121\n#\n= e\u2212\n\u03b32(x\u2032Us)2\n2\nE\nh\ne\n\u03b3\n\u03c30 (x\u2032Us)Ws \f\f\f Hs\u22121\ni\n\u22641 .\nUsing a standard argument involving iterated expectations, we obtain\nE\nh\ne\u03b3A\u2212(\u03b32B2/2)i\n= E\n\uf8ee\n\uf8f0e\nPt\ns=1\n\u0012\n\u03b3\n\u03c30 (x\u2032Us)Ws\u2212\n\u03b32(x\u2032Us)2\n2\n\u0013\uf8f9\n\uf8fb= E\n\uf8ee\n\uf8f0\ntY\ns=1\ne\n\u0012\n\u03b3\n\u03c30 (x\u2032Us)Ws\u2212\n\u03b32(x\u2032Us)2\n2\n\u0013\uf8f9\n\uf8fb\u22641\nWe can thus apply Lemma B.3 to the random variables A and B. Moreover, it follows from the\nde\ufb01nition of \u00afu and \u03bb0 in Assumption 1(b) that, with probability one,\n\u03bb0 \u2264\u03bbmin\n t\nX\ns=1\nUsU\u2032\ns\n!\n\u2264x\u2032\n t\nX\ns=1\nUsU\u2032\ns\n!\nx = B2 =\nt\nX\ns=1\n\u0000x\u2032Us\n\u00012 \u2264t\u00afu2 .\nTherefore, B2 + \u03bb0 \u22642B2, and\n1 + 1\n2 log\n\u0012\n1 + B2\n\u03bb0\n\u0013\n\u2264\n1 + 1\n2 log\n\u0012\n1 + t\u00afu2\n\u03bb0\n\u0013\n\u22641\n2\n\u0012\nlog t + 2 + log\n\u0012\n1 + \u00afu2\n\u03bb0\n\u0013\u0013\n\u2264\nlog t\n2\n \n1 + 2 + log\n\u00001 + (\u00afu2/\u03bb0)\n\u0001\nlog t\n!\n\u2264\u03ba2\n0 log t\n2\n,\nwhere the last inequality follows from the de\ufb01nition of \u03ba0 and the fact that t \u2265r \u22652. These two\nupper bounds imply that\nr\n(B2 + \u03bb0)\n\u0010\n1 + 1\n2 log\n\u0010\n1 + B2\n\u03bb0\n\u0011\u0011\n\u2264\u03ba0\n\u221alog t B . Therefore,\nPr\nn\f\fx\u2032Mt\n\f\f > \u03b6 \u03ba0 \u03c30\np\nlog t \u2225x\u2225C\u22121\nt\no\n\u2264\nPr\n(\n|A| > \u03b6\ns\n(B2 + \u03bb0)\n\u0012\n1 + 1\n2 log\n\u0012\n1 + B2\n\u03bb0\n\u0013\u0013)\n,\nand the desired result then follows immediately from Lemma B.3.\nThe next and \ufb01nal lemma extends the previous result to show that the matrix \u03b62 \u03ba2\n0 \u03c32\n0 (log t) C\u22121\nt \u2212\nMtM\u2032\nt is positive semide\ufb01nite with a high probability. The proof of this result makes use of the\nfact that for the matrix \u03b62 \u03ba2\n0 \u03c32\n0 (log t) C\u22121\nt\n\u2212MtM\u2032\nt to be positive semide\ufb01nite, it su\ufb03ces for the\ninequality x\u2032MtM\u2032\ntx \u2264\u03b62 \u03ba2\n0 \u03c32\n0 (log t) x\u2032C\u22121\nt x to hold for vectors x in a su\ufb03ciently dense subset.\nWe can then apply Lemma B.4 for each such vector x and use the union bound.\n31\nLemma B.5. Under Assumption 1, for any t \u2265r and \u03b6 \u22652,\nPr\n\b\nMtM\u2032\nt \u2264\u03b62 \u03ba2\n0 \u03c32\n0 (log t) C\u22121\nt\n\t\n\u22651 \u2212tr \u03ba2\n0 e\u2212\u03b62/4 ,\nProof. Let Sr = {x \u2208Rr : \u2225x\u2225= 1} denote the unit sphere in Rr. Let \u03b4 > 0 be de\ufb01ned by:\n\u03b4 =\n\u03bb0\n9\u00afu2t ,\nwhere the constants \u03bb0 and \u00afu are given in Assumption 1(b). Without loss of generality, we can\nassume that \u03b4 \u22641/2 and that 1/\u03b4 is an integer. Let X r be a covering of Sr, that is, for any x \u2208Sr,\nthere exists y \u2208X r such that \u2225x \u2212y\u2225\u2264\u03b4. It is easy to verify that X r can be chosen to have a\ncardinality of at most (2\u221ar/\u03b4)r because we can consider a rectangular grid on [\u22121, 1]r with a grid\nspacing of \u03b4/\u221ar. Then, for any point x \u2208Sr, there is a point y on the rectangular grid such that\nthe magnitude of each component of x \u2212y is at most \u03b4/\u221ar, which implies that \u2225x \u2212y\u2225\u2264\u03b4.\nLet t \u2265r and \u03b6 \u22652 be given. To facilitate our exposition, let \u03b2 = \u03b62 \u03ba2\n0 \u03c32\n0 log t. Let G denote\nthe event that the following inequalities hold:\ne\u2032\niMtM\u2032\ntei \u2264\u03b2eiC\u22121\nt ei,\ni = 1, 2, . . . , r ,\nand\ny\u2032MtM\u2032\nty \u2264\u03b2\n2 y\u2032C\u22121\nt y ,\n\u2200y \u2208X r .\nUsing the union bound, it follows from Lemma B.4 that the event G happens with a probability at\nleast\n1 \u2212|X r| e\u2212\u03b62/4 \u2212re\u2212\u03b62/2\n\u2265\n1 \u2212\n\u00122\u221ar\n\u03b4\n\u0013r\ne\u2212\u03b62/4 \u2212re\u2212\u03b62/2 \u22651 \u2212\n\u0012\u00122\u221ar\n\u03b4\n\u0013r\n+ r\n\u0013\ne\u2212\u03b62/4\n\u2265\n1 \u2212\n\u00124\u221ar\n\u03b4\n\u0013r\ne\u2212\u03b62/4 \u22651 \u2212\n\u001236 \u00afu2 t2\n\u03bb0\n\u0013r\ne\u2212\u03b62/4 \u22651 \u2212tr \u03ba2\n0 e\u2212\u03b62/4 ,\nwhere we have used the fact that t \u2265\u221ar \u22652 in the penultimate inequality. The \ufb01nal inequality\nfollows from the de\ufb01nition of \u03ba0 in Equation (3), which implies that \u03ba2\n0 \u22654\n\u00001 + log(36\u00afu2/\u03bb0)\n\u0001\n\u22654,\nand thus, 36 \u00afu2 t2\n\u03bb0\n\u2264t2e\u03ba2\n0/4 \u2264t\u03ba2\n0/2 \u0000t2\u0001\u03ba2\n0/4 = t\u03ba2\n0 .\nTo complete the proof, it su\ufb03ces to show that when the event G occurs, we have that x\u2032MtM\u2032\ntx \u2264\n\u03b2 x\u2032C\u22121\nt x for all x \u2208Sr. Consider an arbitrary x \u2208Sr, and let y \u2208X r be such that \u2225x \u2212y\u2225\u2264\u03b4.\nThis implies that \u2225x + y\u2225\u2264\u22252x\u2225+ \u2225y \u2212x\u2225\u22642 + \u03b4 \u22643. Moreover, x\u2032MtM\u2032\ntx \u2212y\u2032MtM\u2032\nty =\n(x \u2212y)\u2032 MtM\u2032\nt (x + y) \u22643\u03b4 \u2225Mt\u22252 where we use the Cauchy-Schwarz for the last inequality.\nSimilarly, we can show that for all s, y\u2032UsU\u2032\nsy \u2264x\u2032UsU\u2032\nsx + 3\u03b4\u00afu2. Summing over all s, we\nobtain that y\u2032C\u22121\nt y \u2264xC\u22121\nt x + 3\u03b4t\u00afu2. Putting everything together, we have that\nx\u2032MtM\u2032\ntx\n\u2264\ny\u2032MtM\u2032\nty + 3\u03b4 \u2225Mt\u22252 \u2264\u03b2\n2 y\u2032C\u22121\nt y + 3\u03b4 \u2225Mt\u22252\n\u2264\n\u03b2\n2 x\u2032C\u22121\nt x + 3\u03b2\n2 \u03b4t\u00afu2 + 3\u03b4 \u2225Mt\u22252 \u2264\u03b2x\u2032C\u22121\nt x \u2212\u03b2\n2 \u03bb0 + 3\u03b2\n2 \u03b4t\u00afu2 + 3\u03b4 \u2225Mt\u22252 ,\n32\nwhere the last inequality follows from the fact that C\u22121\nt\n= Pt\ns=1 UsU\u2032\ns \u2265\u03bb0Ir from our de\ufb01nition\nof \u03bb0. Finally, note that under the event G,\n\u2225Mt\u22252\n=\nr\nX\ni=1\ne\u2032\niMtM\u2032\ntei \u2264\u03b2\nr\nX\ni=1\ne\u2032\niC\u22121\nt ei = \u03b2\nt\nX\ns=1\nr\nX\ni=1\ne\u2032\niUsU\u2032\nsei\n=\n\u03b2\nt\nX\ns=1\nr\nX\ni=1\n\f\fe\u2032\niUs\n\f\f2 = \u03b2\nt\nX\ns=1\n\u2225Us\u22252 \u2264\u03b2t\u00afu2 ,\nwhich implies that\n\u2212\u03b2\n2 \u03bb0 + 3\u03b2\n2 \u03b4t\u00afu2 + 3\u03b4 \u2225Mt\u22252 \u2264\u2212\u03b2\n2 \u03bb0 + 9\u03b2\n2 \u03b4t\u00afu2 = \u03b2\n2\n\u00009\u03b4t\u00afu2 \u2212\u03bb0\n\u0001\n= 0 ,\nwhere the last inequality follows from the de\ufb01nition of \u03b4. Thus, we have that x\u2032MtM\u2032\ntx \u2264\u03b2x\u2032C\u22121\nt x,\nwhich is the desired result.\nWe are now ready to give a proof of Theorem B.2.\nProof. It su\ufb03ces to establish the second inequality in Theorem B.2 because the proof for the \ufb01rst\ninequality follows the same argument. It follows from the Cauchy-Schwarz inequality that\n(Ut+1 \u2212x)\u2032 \u0010\nbZt \u2212z\n\u0011\n\u2225Ut+1 \u2212x\u2225Ct\n=\n(Ut+1 \u2212x)\u2032 C1/2\nt\nC\u22121/2\nt\n\u0010\nbZt \u2212z\n\u0011\n\r\r\rC1/2\nt\n(Ut+1 \u2212x)\n\r\r\r\n\u2264\n\r\r\rC\u22121/2\nt\n\u0010\nbZt \u2212z\n\u0011\r\r\r ,\nwith probability one. Therefore,\nPr\nn\n(Ut+1 \u2212x)\u2032 \u0010\nbZt \u2212z\n\u0011\n> \u03b6 \u03ba0 \u03c30\np\nlog t \u2225Ut+1 \u2212x\u2225Ct\n\f\f Z = z\no\n\u2264\nPr\nn\r\r\rC\u22121/2\nt\n\u0010\nbZt \u2212z\n\u0011\r\r\r > \u03b6 \u03ba0 \u03c30\np\nlog t\n\f\f Z = z\no\n=\nPr\n\u001a\u0010\nbZt \u2212z\n\u0011\u2032\nC\u22121\nt\n\u0010\nbZt \u2212z\n\u0011\n> \u03b62 \u03ba2\n0 \u03c32\n0 log t\n\f\f Z = z\n\u001b\n=\nPr\n\b\nM\u2032\ntCtMt > \u03b62 \u03ba2\n0 \u03c32\n0 log t\n\f\f Z = z\n\t\n,\nwhere the last equality follows from the de\ufb01nition of the least squares estimate bZt.\nIt is a well-known result in linear algebra (see, for example, Theorem 1.3.3 in Bhatia, 2007)\nthat if A and B are two symmetric positive de\ufb01nite matrices, then the block matrix\n\uf8eb\n\uf8edA\nX\nX\u2032\nB\n\uf8f6\n\uf8f8\nis positive semide\ufb01nite if and only if XB\u22121X\u2032 \u2264A. Applying this result to the two \u201cequivalent\u201d\n(r+1)\u00d7(r+1) matrices\n\uf8eb\n\uf8ed\u03b62 \u03ba2\n0 \u03c32\n0 log t\nM\u2032\nt\nMt\nC\u22121\nt\n\uf8f6\n\uf8f8\nand\n\uf8eb\n\uf8edC\u22121\nt\nMt\nM\u2032\nt\n\u03b62 \u03ba2\n0 \u03c32\n0 log t\n\uf8f6\n\uf8f8, we conclude\nthat M\u2032\ntCtMt \u2264\u03b62 \u03ba2\n0 \u03c32\n0 log t if and only if MtM\u2032\nt \u2264\u03b62 \u03ba2\n0 \u03c32\n0 (log t)C\u22121\nt . The desired result then\n33\nfollows from the fact that\nPr\n\b\nM\u2032\ntCtMt > \u03b62 \u03ba2\n0 \u03c32\n0 log t\n\f\f Z = z\n\t\n=\n1 \u2212Pr\n\b\nM\u2032\ntCtMt \u2264\u03b62 \u03ba2\n0 \u03c32\n0 log t\n\f\f Z = z\n\t\n=\n1 \u2212Pr\n\b\nMtM\u2032\nt \u2264\u03b62 \u03ba2\n0 \u03c32\n0 (log t)C\u22121\nt\n\f\f Z = z\n\t\n\u2264\ntr \u03ba2\n0 e\u2212\u03b62/4 ,\nwhere the last inequality follows from Lemma B.5.\nB.2\nBounds for General Compact Sets of Arms: Proof of Theorem 4.1\nThe proof of Theorem 4.1 makes use of a number of auxiliary results. The \ufb01rst result provides a\nmotivation for the choice of the parameter \u03b1 in Equation (5) and our de\ufb01nition of the uncertainty\nradius Ru\nt in Equation (6). They are chosen to keep the probability of overestimating the reward\nof an arm by more than Ru\nt bounded by 1/t2. This will limit the growth rate of the cumulative\nregret due to such overestimation.\nLemma B.6 (Large Deviation Inequalities for the Uncertainty Radius). Under Assumption 1, for\nany arm u \u2208Ur and t \u2265r,\nPr\nn\nu\u2032 \u0010\nbZt \u2212z\n\u0011\n> Ru\nt\n\f\f Z = z\no\n\u2264\n1\nt2 ,\nand for any x \u2208Rr,\nPr\nn\n(Ut+1 \u2212x)\u2032 \u0010\nbZt \u2212z\n\u0011\n> \u03b1\np\nlog t\np\nmin{r log t , |Ur|} \u2225Ut+1 \u2212x\u2225Ct\n\f\f Z = z\no\n\u2264\n1\nt2 ,\nwhere \u03b1 = 4\u03c30\u03ba2\n0.\nProof. It su\ufb03ces to establish the \ufb01rst inequality because the proof of the second one is exactly\nthe same.\nLet \u03b2t = 4\u03c30\u03ba2\n0\n\u221alog t\np\nmin{r log t , |Ur|}.\nRecall from Equations (5) and (6) that\nRu\nt = \u03b2t \u2225u\u2225Ct. By applying Theorem B.1 (with \u03b6 = 4\u03ba2\n0\n\u221alog t\np\nmin{r log t , |Ur|}) and Theorem\nB.2 (with \u03b6 = 4\u03ba0\np\nmin{r log t , |Ur|}), we obtain\nPr\nn\nu\u2032 \u0010\nbZt \u2212z\n\u0011\n> Ru\nt\n\f\f Z = z\no\n\u2264min\nn\nt5|Ur|e\u22128\u03ba4\n0(log t) min{r log t , |Ur|} , tr\u03ba2\n0e\u22124 \u03ba2\n0 min{r log t , |Ur|}o\n.\nThere are two cases to consider: r log t > |Ur| and r log t \u2264|Ur|. Suppose that r log t > |Ur|. Then,\nPr\nn\nu\u2032 \u0010\nbZt \u2212z\n\u0011\n> Ru\nt\n\f\f Z = z\no\n\u2264t5|Ur|e\u22128\u03ba4\n0(log t) min{r log t , |Ur|} = t5|Ur|e\u22128\u03ba4\n0(log t)|Ur| =\nt5|Ur|\nt8\u03ba4\n0|Ur| \u22641\nt2 ,\nwhere the last inequality follows from the fact that\n\u00008\u03ba4\n0 \u22125\n\u0001\n|Ur| \u22652. In the second case where\nr log t \u2264|Ur|, we have that\nPr\nn\nu\u2032 \u0010\nbZt \u2212z\n\u0011\n> Ru\nt\n\f\f Z = z\no\n\u2264tr\u03ba2\n0e\u22124 \u03ba2\n0 min{r log t,|Ur|} = tr\u03ba2\n0e\u22124 \u03ba2\n0r log t = tr\u03ba2\n0\nt4r\u03ba2\n0 =\n1\nt3r\u03ba2\n0 \u22641\nt2 ,\nwhere the last inequality follows from the fact that 3r\u03ba2\n0 \u22652. Since the probability is bounded by\n1/t2 in both cases, this gives the desired result.\n34\nFor any t \u22651, let the random variable Qt(z) denote the instantaneous regret in period t given\nthat Z = z, that is,\nQt(z)\n=\nmax\nv\u2208Ur v\u2032z \u2212U\u2032\ntz .\n(9)\nLemma B.6 shows that the probability of a large estimation error in period t is at most O\n\u00001/t2\u0001\n.\nConsequently, as shown in the following lemma, the probability of having a large instantaneous\nregret in period t is also small.\nLemma B.7 (Instantaneous Regret Bound). Under Assumption 1, for all t \u2265r and z \u2208Rr,\nPr\nn\nQt+1(z) > 2\u03b1\np\nlog t\np\nmin {r log t, |Ur|} \u2225Ut+1\u2225Ct\n\f\f Z = z\no\n\u22641\nt2 .\nProof. Let z \u2208Rr be given and let w denote an optimal arm, that is, maxv\u2208Ur v\u2032z = w\u2032z. To\nfacilitate our discussion, let \u03b2t = \u03b1 \u221alog t\np\nmin {r log t, |Ur|}. Then, it follows from the de\ufb01nition\nof the uncertainty radius in Equation (6) and the de\ufb01nition of Ut+1 in Equation (7) that\nU\u2032\nt+1bZt + \u03b2t \u2225Ut+1\u2225Ct \u2265w\u2032bZt + \u03b2t \u2225w\u2225Ct ,\nwhich implies that\n\u03b2t \u2225Ut+1\u2225Ct\n\u2265\n(w \u2212Ut+1)\u2032 bZt + \u03b2t \u2225w\u2225Ct\n=\n(w \u2212Ut+1)\u2032 z + (w \u2212Ut+1)\u2032 \u0010\nbZt \u2212z\n\u0011\n+ \u03b2t \u2225w\u2225Ct\n=\nQt+1(z) + (w \u2212Ut+1)\u2032 \u0010\nbZt \u2212z\n\u0011\n+ \u03b2t \u2225w\u2225Ct .\nSuppose that the event Qt+1(z) > 2\u03b2t \u2225Ut+1\u2225Ct occurs. Then, it follows that\n\u03b2t \u2225Ut+1\u2225Ct\n>\n2\u03b2t \u2225Ut+1\u2225Ct + (w \u2212Ut+1)\u2032 \u0010\nbZt \u2212z\n\u0011\n+ \u03b2t \u2225w\u2225Ct\nwhich implies that (Ut+1 \u2212w)\u2032 \u0010\nbZt \u2212z\n\u0011\n> \u03b2t\n\u0000\u2225Ut+1\u2225Ct + \u2225w\u2225Ct\n\u0001\n\u2265\u03b2t \u2225Ut+1 \u2212w\u2225Ct . Thus,\nPr\n\b\nQt+1(z) > 2\u03b2t \u2225Ut+1\u2225Ct\n\f\f Z = z\n\t\n\u2264\nPr\nn\n(Ut+1 \u2212w)\u2032 \u0010\nbZt \u2212z\n\u0011\n> \u03b2t \u2225Ut+1 \u2212w\u2225Ct\n\f\f Z = z\no\n\u22641\nt2 ,\nthe last inequality follows from Lemma B.6.\nLemma B.7 suggests the following approach for bounding the cumulative regret over T peri-\nods. In the \ufb01rst r periods (during the initialization), we incur a regret of O(r). For each time\nperiod between r + 1 and T, we consider the two cases: 1) where the instantaneous regret is large\nwith Qt+1(z) > 2\u03b1 \u221alog t\np\nmin {r log t, |Ur|} \u2225Ut+1\u2225Ct; and, 2) the instantaneous regret is small.\n35\nBy the above lemma, the contribution to the cumulative regret from the \ufb01rst case is bounded\nabove by O\n\u0000P\nt 1/t2\u0001\n, which is \ufb01nite.\nIn the second case, we have a simple upper bound of\n2\u03b1 \u221ar (log t) \u2225Ut+1\u2225Ct for the instantaneous regret. This argument leads to the following bound\non the cumulative regret over T periods.\nLemma B.8 (Regret Decomposition). Under Assumption 1, for all T \u2265r + 1 and z \u2208Rr,\nRegret (z, T, UE) \u22642 \u00afu(r + 2) \u2225z\u2225+ 2\u03b1 \u221ar (log T)\n\u221a\nT E\n\uf8ee\n\uf8f0\nv\nu\nu\nt\nT\u22121\nX\nt=r\n\u2225Ut+1\u22252\nCt\n\f\f\f\f\f Z = z\n\uf8f9\n\uf8fb.\nProof. Let z \u2208Rr be given. By the Cauchy-Schwarz Inequality and Assumption 1(b), we have the\nfollowing upper bound on the instantaneous regret for all t and z \u2208Rr: Qt(z) = maxv\u2208Ur (v \u2212Ut)\u2032 z \u2264\n2\u00afu \u2225z\u2225. Therefore,\nRegret (z, T, UE) \u22642 \u00afu r \u2225z\u2225+ E\n\"T\u22121\nX\nt=r\nQt+1(z)\n\f\f\f\f Z = z\n#\n.\nFor any t \u2265r, let the indicator random variable Gt+1(z) be de\ufb01ned by:\nGt+1(z) = 1l\nh\nQt+1(z) \u22642\u03b1\np\nlog t\np\nmin {r log t, |Ur|} \u2225Ut+1\u2225Ct\ni\n.\nThe contribution to the expected instantaneous regret E\n\u0002\nQt+1(z)\n\f\f Z = z\n\u0003\ncomes from two cases:\n1) when Gt+1(z) = 0 and 2) when Gt+1(z) = 1. We will upper bound each of these two contri-\nbutions separately. In the \ufb01rst case, we know from Lemma B.7 that Pr\n\b\nGt+1(z) = 0\n\f\f Z = z\n\t\n=\nPr\nn\nQt+1(z) > 2\u03b1 \u221alog t\np\nmin {r log t, |Ur|} \u2225Ut+1\u2225Ct\n\f\f Z = z\no\n\u22641/t2. Since P\u221e\nt=1 1/t2 \u22642, we\nhave that\nE\n\"T\u22121\nX\nt=r\n(1 \u2212Gt+1(z)) Qt+1(z)\n\f\f\f\f Z = z\n#\n\u22642\u00afu \u2225z\u2225\nT\u22121\nX\nt=r\nPr\nn\nGt+1(z) = 0\n\f\f\f Z = z\no\n\u22644\u00afu \u2225z\u2225.\nOn the other hand, when Gt+1(z) = 1, we have that Qt+1(z) \u22642\u03b1\u221ar(log t) \u2225Ut+1\u2225Ct. This implies\nthat, with probability one,\nT\u22121\nX\nt=r\nGt+1(z)Qt+1(z)\n\u2264\n2 \u03b1 \u221ar\nT\u22121\nX\nt=r\n(log t) \u2225Ut+1\u2225Ct\n\u22642 \u03b1 \u221ar\nv\nu\nu\nt\nT\u22121\nX\nt=r\nlog2 t \u00d7\nv\nu\nu\nt\nT\u22121\nX\nt=r\n\u2225Ut+1\u22252\nCt\n\u2264\n2\u03b1 \u221ar (log T)\n\u221a\nT\nv\nu\nu\nt\nT\u22121\nX\nt=r\n\u2225Ut+1\u22252\nCt ,\nwhere we use the Cauchy-Schwarz Inequality in the second inequality and the \ufb01nal inequality\nfollows from the fact that\nqPT\u22121\nt=r log2 t \u2264\nqPT\u22121\nt=1 log2 T \u2264(log T)\n\u221a\nT . Putting the two cases\ntogether gives the desired upper bound because\nE\n\"T\u22121\nX\nt=r\nQt+1(z)\n\f\f\f\f Z = z\n#\n\u22644\u00afu \u2225z\u2225+ 2\u03b1 \u221ar (log T)\n\u221a\nT E\n\uf8ee\n\uf8f0\nv\nu\nu\nt\nT\u22121\nX\nt=r\n\u2225Ut+1\u22252\nCt\n\f\f\f\f Z = z\n\uf8f9\n\uf8fb.\n36\nThe eigenvectors of the matrix Ct =\n\u0000Pt\ns=1 UsU\u2032\ns\n\u0001\u22121 re\ufb02ect the directions of the arms that are\nchosen during the \ufb01rst t periods. The corresponding eigenvalues then measure the frequency with\nwhich these directions are explored. Frequently explored directions will have small eigenvalues,\nwhile the eigenvalues for unexplored directions will be large. Thus, the weighted norm \u2225Ut+1\u2225Ct\nhas two interpretations. First, it measures the size of the regret in period t + 1. In addition, since\n\u2225Ut+1\u22252\nCt is a linear combination of the eigenvalues of Ct, it also re\ufb02ects the amount of exploration\nin period t + 1 in the unexplored directions.\nThe above interpretation suggests that if we incur large regrets in the past (equivalently, we have\ndone a lot of exploration), then the current regret should be small. Our intuition is con\ufb01rmed in\nthe following lemma that establishes a recursive relationship between the weighted norm \u2225Ut+1\u2225Ct\nin period t + 1 and the norms in the preceding periods.\nLemma B.9 (Large Past Regrets Imply Small Current Regret). Under Assumption 1, for all t \u2265r\nand z \u2208Rr, with probability one,\n0 \u2264\u2225Ut+1\u22252\nCt \u2264\u00afu2\n\u03bb0\nand\n\u2225Ut+1\u22252\nCt \u2264\n\b\n(\u00afu2/\u03bb0) \u00b7 (t + 1)\n\tr\nQt\u22121\ns=r\n\u0010\n1 + \u2225Us+1\u22252\nCs\n\u0011\n.\nProof. For any t \u2265r, let \u03a5t = (Ct)\u22121 = Pt\ns=1 UsU\u2032\ns. By the Rayleigh Principle,\n\u2225Ut+1\u22252\nCt = U\u2032\nt+1CtUt+1 \u2264\u03bbmax (Ct) \u2225Ut+1\u22252 = \u2225Ut+1\u22252\n\u03bbmin (\u03a5t) \u2264\u00afu2\n\u03bb0\n,\nwhere the last inequality follows from the de\ufb01nition of \u00afu and the fact that\n\u03bbmin (\u03a5t) = \u03bbmin\n \n\u03a5r +\nt\nX\ns=r+1\nUsU\u2032\ns\n!\n\u2265\u03bbmin (\u03a5r) \u2265\u03bb0 ,\nwhere the last equality follows from the fact that \u03a5r = Pr\nk=1 bkb\u2032\nk where the vectors b1, . . . , br\nare given in Assumption 1(b). This proves the claimed upper bound on \u2225Ut+1\u22252\nCt.\nWe will now establish the inequality that relates \u2225Ut+1\u22252\nCt to \u2225Us+1\u22252\nCs for s < t. Note that\n\u2225Ut+1\u22252\nCt\n=\nU\u2032\nt+1CtUt+1 \u22641 + U\u2032\nt+1CtUt+1\n=\ndet (\u03a5t) \u00b7\n\u00001 + U\u2032\nt+1CtUt+1\n\u0001\ndet (\u03a5t)\n= det\n\u0000\u03a5t + Ut+1U\u2032\nt+1\n\u0001\ndet (\u03a5t)\n= det (\u03a5t+1)\ndet (\u03a5t)\n,\n(10)\nwhere the second to last equality follows the matrix determinant lemma.\nWe will now establish bounds on the determinants det (\u03a5t+1) and det (\u03a5t). Note that\n\u03bbmax (\u03a5t+1) \u2264tr (\u03a5t+1) =\nt+1\nX\ns=1\ntr\n\u0000UsU\u2032\ns\n\u0001\n=\nt+1\nX\ns=1\n\u2225Us\u22252 \u2264(t + 1)\u00afu2 ,\n37\nwhere the last inequality follows from the de\ufb01nition of \u00afu . Therefore, det (\u03a5t+1) \u2264[\u03bbmax (\u03a5t+1)]r \u2264\n(t + 1)r\u00afu2r. Moreover, using Equation (10) repeatedly, we obtain\ndet (\u03a5t) = det(\u03a5r)\nt\u22121\nY\ns=r\n\u0010\n1 + \u2225Us+1\u22252\nCs\n\u0011\n\u2265\u03bbr\n0\nt\u22121\nY\ns=r\n\u0010\n1 + \u2225Us+1\u22252\nCs\n\u0011\n,\nwhere the last inequality follows from the fact that \u03a5r = Pr\nk=1 bkb\u2032\nk and det (\u03a5r) \u2265[\u03bbmin (\u03a5r)]r \u2265\n\u03bbr\n0, where the vectors b1, . . . , br and the parameter \u03bb0 are de\ufb01ned in Assumption 1(b).\nPutting everything together, we have that\n\u2225Ut+1\u22252\nCt \u2264det (\u03a5t+1)\ndet (\u03a5t)\n\u2264\n(t + 1)r\u00afu2r\n\u03bbr\n0\nQt\u22121\ns=r\n\u0010\n1 + \u2225Us+1\u22252\nCs\n\u0011 =\n\b\n(\u00afu2/\u03bb0) \u00b7 (t + 1)\n\tr\nQt\u22121\ns=r\n\u0010\n1 + \u2225Us+1\u22252\nCs\n\u0011 ,\nwhich is the desired result.\nThe above result shows that if the weighted norms in the preceding periods, as measured by\nQt\u22121\ns=r\n\u0010\n1 + \u2225Us+1\u22252\nCs\n\u0011\n, are large, then the weighted norm in the current period t + 1 will be small.\nMoreover, since the weighted norm in the current period depends on the product of the norms in\nthe past, we hope that the growth rate of the sum PT\u22121\nt=r \u2225Ut+1\u22252\nCt should be small. To formalize\nour conjecture, we introduce a related optimization problem. For any c \u22650 and t \u22651, let V \u2217(c, t)\nbe de\ufb01ned by:\nV \u2217(c, t)\n=\nmax\nt\nX\ns=1\nys\ns.t.\n0 \u2264ys \u2264c\nand\nys \u2264{c \u00b7 (r + s)}r\nQs\u22121\nq=1 (1 + yq)\n,\ns = 1, 2, . . . , t ,\nwhere we de\ufb01ne Q0\nq=1(1 + yq) = 1. The following lemma gives an upper bound in terms of the\nfunction V \u2217.\nLemma B.10 (Bounds on the Growth Rate of \u2225Ut+1\u22252\nCt). Under Assumption 1, for any T \u2265r+1\nand z \u2208Rr, with probability one, PT\u22121\nt=r \u2225Ut+1\u22252\nCt \u2264V \u2217\u0000\u00afu2/\u03bb0 , T \u2212r\n\u0001\n.\nProof. For all s \u22651, let ys = \u2225Ur+s\u22252\nCr+s\u22121.\nThen, PT\u22121\nt=r \u2225Ut+1\u22252\nCt = PT\u2212r\ns=1 ys.\nLet c0 =\n\u00afu2/\u03bb0. It follows from Lemma B.9 that for all s, with probability one, 0 \u2264ys \u2264c0 and ys \u2264\n{c0 (r + s)}r / Qs\u22121\nq=1 (1 + yq). Therefore, we have PT\u22121\nt=r \u2225Ut+1\u22252\nCt \u2264V \u2217(c0 , T \u2212r).\nIt follows from Lemma B.10 that it su\ufb03ces to develop an upper bound on V \u2217(c, t). This result\nis given in the following lemma.\nLemma B.11 (Optimization Bound). For all c \u22650, and t \u22651,\nV \u2217(c, t) \u22642 c0 (r log c0 + (r + 1) log(r + t + 1)) ,\nwhere c0 = max{1, c}.\n38\nProof. Any feasible solution {ys : s = 1, . . . , t} for the problem de\ufb01ning V \u2217(c, t), also satis\ufb01es the\nconstraints\n0 \u2264ys\n2c0\n\u2264ys\nc0\n\u22641\nand\nys\n2c0\n\u2264\n{c0 \u00b7 (r + s)}r\nQs\u22121\nq=1(1 + (yq/c0))\n\u2264{c0 \u00b7 (r + s)}re\u2212Ps\u22121\nq=1 yq/(2c0) ,\nwhere the last inequality follows from the fact that for any a \u2208[0, 1], we have 1 + a \u2265ea/2. Thus,\nby letting as = ys/2c0, we obtain V \u2217(c, t) \u22642c0W \u2217(c0, t), where W \u2217(c0, t) is the maximum possible\nvalue of Pt\ns=1 as, subject to\n0 \u2264as \u22641\nand\nas \u2264{c0 \u00b7 (r + s)}re\u2212Ps\u22121\nq=1 as .\nLet us introduce a continuous-time variable \u03c4, and de\ufb01ne a(\u03c4) = as, for \u03c4 \u2208[s \u22121, s). Let\nb(\u03c4) =\nR \u03c4\n0 a(\u03c4 \u2032) d\u03c4 \u2032, and note that b(s) = Ps\nq=1 aq. For any \u03c4 \u2208[s \u22121, s), we have\n\u02d9b(\u03c4) = as \u2264{c0 \u00b7 (r + s)}re\u2212Ps\u22121\nq=1 as \u2264{c0 \u00b7 (r + \u03c4 + 1)}reas\u2212Ps\nq=1 aq \u2264{c0 \u00b7 (r + \u03c4 + 1)}re\u2212b(\u03c4)+1.\nLet d(\u03c4) = eb(\u03c4). Then, for any \u03c4 \u22650,\n\u02d9d(\u03c4) = d(\u03c4)\u02d9b(\u03c4) \u2264eb(\u03c4){c0 \u00b7 (r + \u03c4 + 1)}re\u2212b(\u03c4)+1 = {c0 \u00b7 (r + \u03c4 + 1)}re.\nBy integrating both sides, we obtain d(t) \u2264e cr\n0 (r+t+1)r+1\nr+1\nfor all t \u22650. Since e/(r + 1) \u22641 because\nr \u22652, taking logarithms, we obtain\nt\nX\nq=1\nas = b(t) = log d(t) \u2264r log c0 + (r + 1) log(r + t + 1).\nThe right-hand side above is therefore an upper bound on W \u2217(c0, t), which leads to the upper\nbound on V \u2217(c, t), which gives the desired result.\nFinally, here is the proof of Theorem 4.1.\nProof. It su\ufb03ces to establish the regret bound because the risk bound follows immediately from\ntaking the expectation. Let A0 = max{1, \u00afu2/\u03bb0}. It follows from Lemmas B.8, B.10, and B.11 that\nRegret (z, T, UE)\n\u2264\n2 \u00afu(r + 2) \u2225z\u2225+ 2\u03b1 \u221ar (log T)\n\u221a\nT E\n\uf8ee\n\uf8f0\nv\nu\nu\nt\nT\u22121\nX\nt=r\n\u2225Ut+1\u22252\nCt\n\f\f\f\f\f Z = z\n\uf8f9\n\uf8fb\n\u2264\n2 \u00afu(r + 2) \u2225z\u2225+ 2\u03b1 \u221ar (log T)\n\u221a\nT\np\nV \u2217(\u00afu2/\u03bb0 , T \u2212r)\n\u2264\n2 \u00afu(r + 2) \u2225z\u2225+ 2\u03b1 \u221ar (log T)\n\u221a\nT\np\n2A0 {r log A0 + (r + 1) log(T + 1)}\n\u2264\na4 r \u2225z\u2225+ a5 r\n\u221a\nT log3/2 T ,\nfor some positive constants a4 and a5 that depend only on \u03c30, \u00afu, and \u03bb0.\n39\nB.3\nBounds for Finitely Many Arms: Proof of Theorem 4.2\nRecall that for any z \u2208Rr and u \u2208Ur, N u(z, T) is the number of times that arm u has been\nchosen during the \ufb01rst T periods. To complete the proof of Theorem 4.2, it su\ufb03ces to show that\nE [N u(z, T) | Z = z] \u22646 + 4\u03b12 |Ur| log T\n(\u2206u (z))2\n.\nLet us \ufb01x some z \u2208Rr and u \u2208Ur. Since N u(z, t) is nondecreasing in t, we can show that for any\npositive integer \u03b8, N u(z, T) \u2264\u03b8 + PT\u22121\nt=r 1l{Ut+1=u and Nu(z,t) \u2265\u03b8}. Suppose that w is the optimal\narm, that is, maxv\u2208Ur v\u2032z = w\u2032z. Then, we have that\n1l{Ut+1=u} \u22641l{u\u2032bZt+Ru\nt \u2265w\u2032bZt+Rw\nt } \u22641l{u\u2032(bZt\u2212z) > Ru\nt }+1l{w\u2032(bZt\u2212z) < \u2212Rw\nt }+1l{(w\u2212u)\u2032z \u22642Ru\nt } .\nSince Pr\nn\nu\u2032 \u0010\nbZt \u2212z\n\u0011\n> Ru\nt | Z = z\no\nand Pr\nn\nw\u2032 \u0010\nbZt \u2212z\n\u0011\n< \u2212Rw\nt | Z = z\no\nare bounded above by\n1/t2 by Lemma B.6, we can show that\nE [N u(z, T) | Z = z]\n\u2264\n\u03b8 + 2\n\u221e\nX\nt=1\n1\nt2 +\nT\u22121\nX\nt=r\nPr\n\b\n(w \u2212u)\u2032 z \u22642Ru\nt and N u(z, t) \u2265\u03b8 | Z = z\n\t\n,\n\u2264\n4 + \u03b8 +\nT\u22121\nX\nt=r\nPr\n\b\n(w \u2212u)\u2032 z \u22642Ru\nt and N u(z, t) \u2265\u03b8 | Z = z\n\t\n.\nLet H = P\nv\u2208Ur:v\u0338=u N v(z, t)vv\u2032.\nIt follows from Equation (4) and the Sherman-Morrison\nFormula (see Sherman and Morrison, 1950) that\nCt\n=\n\u0000H + N u(z, t)uu\u2032\u0001\u22121 = H\u22121 \u2212N u(z, t)H\u22121uu\u2032H\u22121\n1 + N u(z, t)u\u2032H\u22121u ,\nwhich implies that\n\u2225u\u22252\nCt = u\u2032Ctu = u\u2032H\u22121u \u2212N u(z, t)\n\u0000u\u2032H\u22121u\n\u00012\n1 + N u(z, t)u\u2032H\u22121u =\nu\u2032H\u22121u\n1 + N u(z, t)u\u2032H\u22121u \u2264\n1\nN u(z, t) ,\nand therefore, 2Ru\nt = 2\u03b1 \u221alog t\np\nmin {r log t, |Ur|} \u2225u\u2225Ct \u2264\n\u00002\u03b1\np\n|Ur| log t\n\u0001\n/\np\nN u(z, t).\nBy setting \u03b8 = 1 +\nl\n4\u03b12|Ur| log T\n(\u2206u(z))2\nm\n, we conclude that 2Ru\nt < \u2206u (z) = (w \u2212u)\u2032 z whenever\nN u(z, t) \u2265\u03b8. This implies that 1l{(w\u2212u)\u2032z \u22642Ru\nt and Nu(z,t)\u2265\u03b8} = 0 , and we have that\nE [N u(z, T) | Z = z] \u22644 + 1 +\n\u00184\u03b12 |Ur| log T\n(\u2206u (z))2\n\u0019\n\u22646 + 4\u03b12 |Ur| log T\n(\u2206u (z))2\n,\nwhich is the desired result.\n40\n",
        "sentence": "",
        "context": "theory to this problem, Adam Mersereau for his contributions to the problem formulation, and\n24\nAssaf Zeevi for helpful suggestions and discussions during the \ufb01rst author\u2019s visit to Columbia\nDe la Pe\u02dcna, V. H., M. J. Klass, and T. L. Lai. 2004. Self-normalized processes: exponential inequalities, moment\nbounds and iterated logarithm laws. Annals of Probability 32 (3A): 1902\u20131933.\nThompson, W. R. 1933. On the likelihood that one unknown probability exceeds another in view of the evidence of\ntwo samples. Biometrika 25 (3): 285\u2013294."
    },
    {
        "title": "User-Friendly Tail Bounds for Sums of Random Matrices",
        "author": [
            "Joel A. Tropp"
        ],
        "venue": "Foundations of Computational Mathematics,",
        "citeRegEx": "Tropp.,? \\Q2011\\E",
        "shortCiteRegEx": "Tropp.",
        "year": 2011,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " Finally, we use a well known matrix-valued version of Hoeffding\u2019s inequality, for example from Tropp [2011]. Lemma 21 (Matrix-Hoeffding).",
        "context": null
    },
    {
        "title": "Algorithms for adversarial bandit problems with multiple plays",
        "author": [
            "Taishi Uchiya",
            "Atsuyoshi Nakamura",
            "Mineichi Kudo"
        ],
        "venue": "In Algorithmic Learning Theory,",
        "citeRegEx": "Uchiya et al\\.,? \\Q2010\\E",
        "shortCiteRegEx": "Uchiya et al\\.",
        "year": 2010,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "The proof of this theorem is similar in spirit to a related theorem",
        "author": [
            "Agarwal"
        ],
        "venue": null,
        "citeRegEx": "Agarwal,? \\Q2014\\E",
        "shortCiteRegEx": "Agarwal",
        "year": 2014,
        "abstract": "",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "The first three are fairly straightforward and the proof of the later two are based on the arguments",
        "author": [
            "Agarwal"
        ],
        "venue": null,
        "citeRegEx": "Agarwal,? \\Q2014\\E",
        "shortCiteRegEx": "Agarwal",
        "year": 2014,
        "abstract": "",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "Freedman\u2019s Inequality)",
        "author": [
            "Beygelzimer"
        ],
        "venue": "Let X1,",
        "citeRegEx": "Beygelzimer,? \\Q2011\\E",
        "shortCiteRegEx": "Beygelzimer",
        "year": 2011,
        "abstract": "",
        "full_text": "",
        "sentence": "",
        "context": null
    }
]