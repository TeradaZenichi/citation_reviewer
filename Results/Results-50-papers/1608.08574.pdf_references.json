[
    {
        "title": "Artificial Intelligence: A Modern Approach, 3rd edition",
        "author": [
            "S. Russell",
            "P. Norvig"
        ],
        "venue": "Pearson Education Limited,",
        "citeRegEx": "1",
        "shortCiteRegEx": "1",
        "year": 2009,
        "abstract": "",
        "full_text": "",
        "sentence": " These learning systems as described by [1] are: This way, the computer learns from the mistakes it has previously made and from the reward it gets from achieving a particular goal [1].",
        "context": null
    },
    {
        "title": "On The Naive Bayes Model for Text Categorization",
        "author": [
            "S. Eyheramendy",
            "D.D. Lewis",
            "D. Madigan"
        ],
        "venue": "Proceedings Artificial Intelligence & Statistics, 2003, pp. 3\u201310.",
        "citeRegEx": "2",
        "shortCiteRegEx": null,
        "year": 2003,
        "abstract": "",
        "full_text": "",
        "sentence": " There is an increasing number of research using machine learning to classify text, sentiment analysis and documents with Na\u0131\u0308ve Bayes such as [2]\u2013[5].",
        "context": null
    },
    {
        "title": "Naive Bayes for Text Classification with Unbalanced Classes",
        "author": [
            "E. Frank",
            "R.R. Bouckaert"
        ],
        "venue": "PKDD\u201906 Proceedings of the 10th European conference on Principle and Practice of Knowledge Discovery in Databases, vol. 4213, Berlin, Germany, 2006, pp. 503\u2013510.",
        "citeRegEx": "3",
        "shortCiteRegEx": null,
        "year": 2006,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "Text Classification for Authorship Attribution Using Naive Bayes Classifier with Limited Training Data",
        "author": [
            "F. Howedi",
            "M. Mohd"
        ],
        "venue": "Computer Engineering and Intelligent Systems, vol. 5, no. 14, pp. 48\u201356, 2014.",
        "citeRegEx": "4",
        "shortCiteRegEx": null,
        "year": 2014,
        "abstract": "",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "Is Na\u0131\u0308ve bayes a good classifier for document classification?",
        "author": [
            "S.L. Ting",
            "W.H. Ip",
            "A.H.C. Tsang"
        ],
        "venue": "International Journal of Software Engineering and its Applications,",
        "citeRegEx": "5",
        "shortCiteRegEx": "5",
        "year": 2011,
        "abstract": "",
        "full_text": "",
        "sentence": " There is an increasing number of research using machine learning to classify text, sentiment analysis and documents with Na\u0131\u0308ve Bayes such as [2]\u2013[5]. businesses [5].",
        "context": null
    },
    {
        "title": "Fast and Accurate Sentiment Classification Using an Enhanced Naive Bayes Model",
        "author": [
            "V. Narayanan",
            "I. Arora",
            "A. Bhatia"
        ],
        "venue": "Intelligent Data Engineering and Automated Learning \u2013 IDEAL 2013. Springer Berlin Heidelberg, 2013, vol. 8206, pp. 194\u2013201.",
        "citeRegEx": "6",
        "shortCiteRegEx": null,
        "year": 2013,
        "abstract": "We have explored different methods of improving the accuracy of a Naive Bayes\nclassifier for sentiment analysis. We observed that a combination of methods\nlike negation handling, word n-grams and feature selection by mutual\ninformation results in a significant improvement in accuracy. This implies that\na highly accurate and fast sentiment classifier can be built using a simple\nNaive Bayes model that has linear training and testing time complexities. We\nachieved an accuracy of 88.80% on the popular IMDB movie reviews dataset.",
        "full_text": "Fast and accurate sentiment classification using an \nenhanced Naive Bayes model. \nVivek Narayanan1, Ishan Arora2, Arjun Bhatia3 \nDepartment of Electronics Engineering, \nIndian Institute of Technology (BHU), Varanasi, India \n1vivek.narayanan.ece09@iitbhu.ac.in \n2ishan.arora.ece09@iitbhu.ac.in \n3arjun.bhatia.ece09@iitbhu.ac.in\nAbstract. We have explored different methods of improving the accuracy of a \nNaive Bayes classifier for sentiment analysis. We observed that a combination \nof methods like effective negation handling, word n-grams and feature selection \nby mutual information results in a significant improvement in accuracy. This \nimplies that a highly accurate and fast sentiment classifier can be built using a \nsimple Naive Bayes model that has linear training and testing time complexi-\nties. We achieved an accuracy of 88.80% on the popular IMDB movie reviews \ndataset. The proposed method can be generalized to a number of text categori-\nzation problems for improving speed and accuracy. \n \nKeywords :- Sentiment classification, Negation Handling, Mutual Information, \nFeature Selection, n-grams \n1 \nIntroduction \nAmong the most researched topics of natural language processing is sentiment analy-\nsis. Sentiment analysis involves extraction of subjective information from documents \nlike online reviews to determine the polarity with respect to certain objects. It is use-\nful for identifying trends of public opinion in the social media, for the purpose of \nmarketing and consumer research. It has its uses in getting customer feedback about \nnew product launches, political campaigns and even in financial markets [14]. It aims \nto determine the attitude of a speaker or a writer with respect to some topic or simply \nthe contextual polarity of a document. Early work in this area was done by Turney and \nPang ([2], [7]) who applied different methods for detecting the polarity of product and \nmovie reviews. \n \nSentiment analysis is a complicated problem but experiments have been done using \nNaive Bayes, maximum entropy classifiers and support vector machines. Pang et al. \nfound the SVM to be the most accurate classifier in [2]. In this paper we present a \nsupervised sentiment classification model based on the Na\u00efve Bayes algorithm.  \nNa\u00efve Bayes is a very simple probabilistic model that tends to work well on text \nclassifications and usually takes orders of magnitude less time to train when com-\npared to models like support vector machines. We will show in this paper that a high \ndegree of accuracy can be obtained using Na\u00efve Bayes model, which is comparable to \nthe current state of the art models in sentiment classification. \n2 \nData \nWe used a publicly available dataset of movie reviews from the Internet Movie Data-\nbase (IMDb) [1] which was compiled by Andrew Maas et al. It is a set of 25,000 \nhighly polar movie reviews for training, and 25,000 for testing. Both the training and \ntest sets have an equal number of positive and negative reviews. We chose movie \nreviews as our data set because it covers a wide range of human emotions and cap-\ntures most of the adjectives relevant to sentiment classification. Also, most existing \nresearch on sentiment classification uses movie review data for benchmarking.  \nWe used the 25,000 documents in the training set to build our supervised learning \nmodel. The other 25,000 were used for evaluating the accuracy of our classifier. \n3 \nNa\u00efve Bayes Classifier \nA Naive bayes classifier is a simple probabilistic model based on the Bayes rule along \nwith a strong independence assumption. \n \nThe Na\u00efve Bayes model involves a simplifying conditional independence assumption. \nThat is given a class (positive or negative), the words are conditionally independent of \neach other. This assumption does not affect the accuracy in text classification by \nmuch but makes really fast classification algorithms applicable for the problem. Ren-\nnie et al discuss the performance of Na\u00efve Bayes on text classification tasks in their \n2003 paper. [6] \nIn our case, the maximum likelihood probability of a word belonging to a particu-\nlar class is given by the expression: \n\u0001\u0002\u0003\u0004| \u0007) =\n\n\u000b\f\r\u000e \u000b\u000f \u0003\u0004 \u0010\r \u0011\u000b\u0007\f\u0012\u0013\r\u000e\u0014 \u000b\u000f \u0007\u0015\u0016\u0014\u0014 \u0007 \n\u0017\u000b\u000e\u0016\u0015 \r\u000b \u000b\u000f \u0018\u000b\u0019\u0011\u0014 \u0010\r \u0011\u000b\u0007\f\u0012\u0013\r\u000e\u0014 \u000b\u000f \u0007\u0015\u0016\u0014\u0014 \u0007 \n                        \n \n(1) \nThe frequency counts of the words are stored in hash tables during the training \nphase. \nAccording to the Bayes Rule, the probability of a particular document belonging to \na class ci is given by, \n\u0001\u0002\u0007\u0004|\u0011) = \u0001\u0002\u0011 |\u0007\u0004) \u2217\u0001\u0002\u0007\u0004)\n\u0001\u0002\u0011)\n     \u00022) \n3 \n \nIf we use the simplifying conditional independence assumption, that given a class \n(positive or negative), the words are conditionally independent of each other. Due to \nthis simplifying assumption the model is termed as \u201cna\u00efve\u201d. \n\u0001\u0002\u0007\u0004|\u0011) = \u0002\u220f\u0001\u0002\u0003\u0004|\u0007\u001d)) \u2217\u0001\u0002\u0007\u001d)\n\u0001\u0002\u0011)\n    \u00023) \nHere the xi s are the individual words of the document. The classifier outputs the \nclass with the maximum posterior probability. \nWe also remove duplicate words from the document, they don\u2019t add any additional \ninformation; this type of na\u00efve bayes algorithm is called Bernoulli Na\u00efve Bayes. In-\ncluding just the presence of a word instead of the count has been found to improve \nperformance marginally, when there is a large number of training examples. \n4 \nLaplacian Smoothing \nIf the classifier encounters a word that has not been seen in the training set, the \nprobability of both the classes would become zero and there won\u2019t be anything to \ncompare between. This problem can be solved by Laplacian smoothing \n \n\u0001\u0002\u0003\u0004|\u0007\u001d) =\n\n\u000b\f\r\u000e\u0002\u0003\u0004) +   \n\u0002 + 1) \u2217\u0002\"\u000b \u000b\u000f \u0018\u000b\u0019\u0011\u0014 \u0010\r \u0007\u0015\u0016\u0014\u0014 \u0007\u001d)      \u00024) \n \nUsually, k is chosen as 1. This way, there is equal probability for the new word to \nbe in either class. Since Bernoulli Na\u00efve Bayes is used, the total number of words in a \nclass is computed differently. For the purpose of this calculation, each document is \nreduced to a set of unique words with no duplicates. \n5 \nNegation Handling \nNegation handling was one of the factors that contributed significantly to the accuracy \nof our classifier. A major problem faced during the task of sentiment classification is \nthat of handling negations. Since we are using each word as feature, the word \u201cgood\u201d \nin the phrase \u201cnot good\u201d will be contributing to positive sentiment rather that negative \nsentiment as the presence of \u201cnot\u201d before it is not taken into account.  \nTo solve this problem we devised a simple algorithm for handling negations using \nstate variables and bootstrapping. We built on the idea of using an alternate represen-\ntation of negated forms as shown by Das & Chen in [3]. Our algorithm uses a state \nvariable to store the negation state. It transforms a word followed by a not or n\u2019t into \n\u201cnot_\u201d + word.  Whenever the negation state variable is set, the words read are treated \nas \u201cnot_\u201d + word. The state variable is reset when a punctuation mark is encountered \nor when there is double negation. The pseudo code of the algorithm is described be-\nlow: \nPSEUDO CODE:.  \nnegated := False \nfor each word in document: \n   if negated = True: \n       Transform word to \u201cnot_\u201d + word. \n   if word is \u201cnot\u201d or \u201cn\u2019t\u201d: \n       negated := not negated \n   if a punctuation mark is encountered \n       negated := False. \nSince the number of negated forms might not be adequate for correct classifications. \nIt is possible that many words with strong sentiment occur only in their normal forms \nin the training set. But their negated forms would be of strong polarity. \nWe addressed this problem by adding negated forms to the opposite class along \nwith normal forms of all the features during the training phase. That is to say if we \nencounter the word \u201cgood\u201d in a positive document during the training phase, we in-\ncrement the count of \u201cgood\u201d in the positive class and also increment the count of \n\u201cnot_good\u201d for the negative class.  This is to ensure that the number of \u201cnot_\u201d forms \nare sufficient for classification. This modification resulted in a significant improve-\nment in classification accuracy (about 1%) due to bootstrapping of negated forms \nduring training. This form of negation handling can be applied to a variety of text \nrelated applications. \n6 \nn - grams \nGenerally, information about sentiment is conveyed by adjectives or more specifically \nby certain combinations of adjectives with other parts of speech.  \nThis information can be captured by adding features like consecutive pairs of \nwords (bigrams), or even triplets of words (trigrams). Words like \"very\" or \"definite-\nly\" don't provide much sentiment information on their own, but phrases like \"very \nbad\" or \"definitely recommended\" increase the probability of a document being nega-\ntively or positively biased. By including bigrams and trigrams, we were able to cap-\nture this information about adjectives and adverbs. Using bigrams and trigrams re-\nquire a substantial amount of data in the training set, but this is not a problem as our \ntraining set had 25,000 reviews. But the data may not be enough to add 4-grams, as \nthis may over-fit the training set. The counts of the n-grams were stored in a hash \ntable along with the counts of unigrams. \n7 \nFeature Selection \n \nFeature selection is the process of removing redundant features, while retaining those \nfeatures that have high disambiguation capabilities.  \n \n \nThe use of higher dimensional features like bigrams and trigrams pr\nlem, that of the number of feature\nMost of these features are redundant and noisy in nature. Including them would affect \nboth efficiency and accuracy. A basic filtering step of removing the features/terms \nwhich occur only once is performed. Now\n1,500,000 features. The features are then further filtered on the basis of mutual info\nmation [3]. \n7.1    Mutual Information\nMutual information is a quantity that measures the mutual dependence of the two \nrandom variables. Formally, the mutual information of two discrete random v\nriables X and Y can be defined as:\nWhere p(x,y) is the joint probability distribution function of \nand P(Y)  are the marginal probability distribution functions of \nHere X is an individual feature which can take two values, the feature is present or \nabsent and Y is the class, positive or negative. We selected the top \nmaximum mutual information. By plotting a graph between accuracy and number of \nfeatures, the optimal value for \n \nA plot of Accuracy versus Number of features\nFig. 1. Plot of Accuracy v/s No\n5 \nThe use of higher dimensional features like bigrams and trigrams presents a pro\nnumber of features increasing from 300,000 to about 11,000,000. \nMost of these features are redundant and noisy in nature. Including them would affect \nboth efficiency and accuracy. A basic filtering step of removing the features/terms \nwhich occur only once is performed. Now the number of features is reduced to about \n1,500,000 features. The features are then further filtered on the basis of mutual info\n.1    Mutual Information \nis a quantity that measures the mutual dependence of the two \nFormally, the mutual information of two discrete random v\ncan be defined as: \n(5) \n \n) is the joint probability distribution function of X and Y, and P(X)  \nand P(Y)  are the marginal probability distribution functions of X and Y respectively. \nHere X is an individual feature which can take two values, the feature is present or \nabsent and Y is the class, positive or negative. We selected the top k features with \nmaximum mutual information. By plotting a graph between accuracy and number of \nfeatures, the optimal value for k was found out to be 32,000.  \nAccuracy versus Number of features is shown in Fig 1: \n \nof Accuracy v/s No of features selected on Validation set \nsents a prob-\ns increasing from 300,000 to about 11,000,000. \nMost of these features are redundant and noisy in nature. Including them would affect \nboth efficiency and accuracy. A basic filtering step of removing the features/terms \nthe number of features is reduced to about \n1,500,000 features. The features are then further filtered on the basis of mutual infor-\nis a quantity that measures the mutual dependence of the two \nFormally, the mutual information of two discrete random va-\n, and P(X)  \nrespectively. \nHere X is an individual feature which can take two values, the feature is present or \nfeatures with \nmaximum mutual information. By plotting a graph between accuracy and number of \n8 \nResults \nWe implemented the classifier in Python using hash tables to store the counts of \nwords in their respective classes. The code is available at [13].  Training involved \npreprocessing data and applying negation handling before counting the words. Since \nwe were using Bernoulli Naive Bayes, each word is counted only once per document. \nOn a laptop running an Intel Core 2 Duo processor at 2.1 GHz, training took around 1 \nminute 30 seconds and used about 700 megabytes of memory. The memory usage \nstems largely from bigrams and trigrams prior to feature selection. \nThe optimal number of features was chosen by using a validation set of 1000 doc-\numents, the plot of accuracy v/s number of features is shown in Fig 1. Then the accu-\nracy was measured on the entire test set of 25000 documents.  The time for feature \nselection was about 3 minutes.  \n8.1   Performance and Comparison \nWe obtained an overall classification accuracy of 88.80% on the test set of 25000 \nmovie reviews. The running time of our algorithm is O(n + V lg V) for training and \nO(n) for testing, where n is the number of words in the documents (linear) and V the \nsize of the reduced vocabulary. It is much faster than other machine learning algo-\nrithms like Maxent classification or Support Vector Machines which take a long time \nto converge to the optimal set of weights.  The accuracy is comparable to that of the \ncurrent state-of-the-art algorithms used for sentiment classification on movie reviews. \nIt achieved a better or similar accuracy when compared to more complicated models \nlike SVMs, autoencoders, contextual valence shifters, matrix factorisation, appraisal \ngroups etc used in [2], [7], [8], [9], [10], [11] on the dataset of IMDb movie reviews. \n8.2   Results Timeline \nThe table and graph illustrate the evolution of the accuracy of our classifier and how \nthe inclusion of certain features helped. \nTable 1.RESULTS TIMELINE \n \nFeature Added \nAccuracy on test set \nOriginal Naive Bayes \nalgorithm with Laplacian \nSmoothing \n73.77% \nHandling negations \n82.80% \nBernoulli  Naive Bayes \n83.66% \nBigrams and trigrams \n85.20% \nFeature Selection \n88.80% \n \n7 \n \n \nFig. 2. Evolution of classification accuracy. \n9 \nConclusion \n \nOur results show that a simple Naive Bayes classifier can be enhanced to match the \nclassification accuracy of more complicated models for sentiment analysis by choos-\ning the right type of features and removing noise by appropriate feature selection. \nNaive Bayes classifiers due to their conditional independence assumptions are ex-\ntremely fast to train and can scale over large datasets. They are also robust to noise \nand less prone to overfitting. Ease of implementation is also a major advantage of \nNaive Bayes. They were thought to be less accurate than their more sophisticated \ncounterparts like support vector machines and logistic regression but we have shown \nthrough this paper that a significantly high accuracy can be achieved. The ideas used \nin this paper can also be applied to the more general domain of text classification.  \n \nAcknowledgement \n \nWe would like to express our sincere gratitude to our mentor, Professor R. R. Das \nfor sparing his valuable time and guiding us during the project. We are also thankful \nto Prof. P. K. Mukherjee for his support and encouragement. Lastly, we would like to \nthank all the faculty members and support staff of the Department of Electronics En-\ngineering, IIT (BHU). \n \nReferences \n1. \nLarge \nMovie \nReview \nDataset. \n(n.d.). \nRetrieved \nfrom \nhttp://ai.stanford.edu/~amaas/data/sentiment/ \n50\n60\n70\n80\n90\n100\nAccuracy\n1. Pang, Bo, Lillian Lee, and Shivakumar Vaithyanathan. \"Thumbs up?: sentiment classifica-\ntion using machine learning techniques.\" Proceedings of the ACL-02 conference on Empir-\nical methods in natural language processing-Volume 10. Association for Computational \nLinguistics, 2002. \n2.  Manning, Christopher D., Prabhakar Raghavan, and Hinrich Sch\u00fctze. Introduction to in-\nformation retrieval. Vol. 1. Cambridge: Cambridge University Press, 2008. \n3. Das, Sanjiv, and Mike Chen. \"Yahoo! for Amazon: Sentiment parsing from small talk on \nthe web.\" EFA 2001 Barcelona Meetings. 2001. \n4.  Pauls, Adam, and Dan Klein. \"Faster and smaller n-gram language models.\"Proceedings \nof the 49th annual meeting of the Association for Computational Linguistics: Human Lan-\nguage Technologies. Vol. 1. 2011. \n5.      Rennie, Jason D., et al. \"Tackling the poor assumptions of naive bayes text classifi-\ners.\" MACHINE LEARNING-INTERNATIONAL WORKSHOP THEN CONFERENCE-. \nVol. 20. No. 2. 2003. \n6. Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and \nChristopher Potts. (2011). Learning Word Vectors for Sentiment Analysis. The 49th An-\nnual Meeting of the Association for Computational Linguistics (ACL 2011). \n7.  Kennedy, Alistair, and Diana Inkpen. \"Sentiment classification of movie reviews using \ncontextual valence shifters.\" Computational Intelligence 22.2 (2006): 110-125. \n8.  Li, Tao, Yi Zhang, and Vikas Sindhwani. \"A non-negative matrix tri-factorization ap-\nproach to sentiment classification with lexical prior knowledge.\" Proceedings of the Joint \nConference of the 47th Annual Meeting of the ACL and the 4th International Joint Confe-\nrence on Natural Language Processing of the AFNLP: Volume 1-Volume 1. Association \nfor Computational Linguistics, 2009. \n9.  Matsumoto, S., Takamura, H., & Okumura, M. (2005). Sentiment classi\ufb01cation using \nword sub-sequences and dependency sub-trees. In PAKDD 2005 (pp. 301\u2013311). \n10. Springer-Verlag: Berlin, Heidelberg. \n11.  Whitelaw, Casey, Navendu Garg, and Shlomo Argamon. \"Using appraisal groups for sen-\ntiment analysis.\" Proceedings of the 14th ACM international conference on Information \nand knowledge management. ACM, 2005. \n12. Socher, Richard, et al. \"Semi-supervised recursive autoencoders for predicting sentiment \ndistributions.\" Proceedings of the Conference on Empirical Methods in Natural Language \nProcessing. Association for Computational Linguistics, 2011. \n13.  Source \ncode \nof \nclassifier \ndeveloped \nfor \nthis \npaper \n\u2013[Online] \nhttp://github.com/vivekn/sentiment \n14.  Devitt, Ann, and Khurshid Ahmad. \"Sentiment polarity identification in financial news: A \ncohesion-based \napproach.\" ANNUAL \nMEETING-ASSOCIATION \nFOR \nCOMPUTATIONAL LINGUISTICS. Vol. 45. No. 1. 2007. \n15. Peng, Fuchun, and Dale Schuurmans. \"Combining naive Bayes and n-gram language mod-\nels for text classification.\" Advances in Information Retrieval. Springer Berlin Heidelberg, \n2003. 335-350. \n",
        "sentence": " Since most machine learning classification algorithms are time-consuming and complicated, using Na\u0131\u0308ve Bayes classification provides a fast and simple way to classify data [6].",
        "context": "much but makes really fast classification algorithms applicable for the problem. Ren-\nnie et al discuss the performance of Na\u00efve Bayes on text classification tasks in their \n2003 paper. [6]\nNa\u00efve Bayes is a very simple probabilistic model that tends to work well on text \nclassifications and usually takes orders of magnitude less time to train when com-\npared to models like support vector machines. We will show in this paper that a high\n3 \nNa\u00efve Bayes Classifier \nA Naive bayes classifier is a simple probabilistic model based on the Bayes rule along \nwith a strong independence assumption. \n \nThe Na\u00efve Bayes model involves a simplifying conditional independence assumption."
    },
    {
        "title": "Estimation of prediction error by using K-fold crossvalidation",
        "author": [
            "T. Fushiki"
        ],
        "venue": "Statistics and Computing, vol. 21, no. 2, pp. 137\u2013146, 2011.",
        "citeRegEx": "7",
        "shortCiteRegEx": null,
        "year": 2011,
        "abstract": "",
        "full_text": "",
        "sentence": " These validation methods include the k-fold cross-validation [7], shuffle-split cross-validation used to generate a learning curve that determines the training and test scores for various training data size [8] and the recursive feature elimination for testing the number of features that produces the best results [9].",
        "context": null
    },
    {
        "title": "RANDOM PERMUTATION TESTING IN MULTIPLE LINEAR REGRESSION",
        "author": [
            "M.-H. Huh",
            "M. Jhun"
        ],
        "venue": "Communications in Statistics - Theory and Methods, vol. 30, no. 10, pp. 2023\u20132032, aug 2001.",
        "citeRegEx": "8",
        "shortCiteRegEx": null,
        "year": 2023,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " These validation methods include the k-fold cross-validation [7], shuffle-split cross-validation used to generate a learning curve that determines the training and test scores for various training data size [8] and the recursive feature elimination for testing the number of features that produces the best results [9].",
        "context": null
    },
    {
        "title": "Recursive feature elimination with random forest for PTR-MS analysis of agroindustrial products",
        "author": [
            "P.M. Granitto",
            "C. Furlanello",
            "F. Biasioli",
            "F. Gasperi"
        ],
        "venue": "Chemometrics and Intelligent Laboratory Systems, vol. 83, no. 2, pp. 83\u201390, 2006.",
        "citeRegEx": "9",
        "shortCiteRegEx": null,
        "year": 2006,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " These validation methods include the k-fold cross-validation [7], shuffle-split cross-validation used to generate a learning curve that determines the training and test scores for various training data size [8] and the recursive feature elimination for testing the number of features that produces the best results [9].",
        "context": null
    },
    {
        "title": "Introduction to Machine Learning",
        "author": [
            "E. Alpaydin"
        ],
        "venue": null,
        "citeRegEx": "10",
        "shortCiteRegEx": "10",
        "year": 2010,
        "abstract": "This book introduces the mathematical foundations and techniques that lead to\nthe development and analysis of many of the algorithms that are used in machine\nlearning. It starts with an introductory chapter that describes notation used\nthroughout the book and serve at a reminder of basic concepts in calculus,\nlinear algebra and probability and also introduces some measure theoretic\nterminology, which can be used as a reading guide for the sections that use\nthese tools. The introductory chapters also provide background material on\nmatrix analysis and optimization. The latter chapter provides theoretical\nsupport to many algorithms that are used in the book, including stochastic\ngradient descent, proximal methods, etc. After discussing basic concepts for\nstatistical prediction, the book includes an introduction to reproducing kernel\ntheory and Hilbert space techniques, which are used in many places, before\naddressing the description of various algorithms for supervised statistical\nlearning, including linear methods, support vector machines, decision trees,\nboosting, or neural networks. The subject then switches to generative methods,\nstarting with a chapter that presents sampling methods and an introduction to\nthe theory of Markov chains. The following chapter describe the theory of\ngraphical models, an introduction to variational methods for models with latent\nvariables, and to deep-learning based generative models. The next chapters\nfocus on unsupervised learning methods, for clustering, factor analysis and\nmanifold learning. The final chapter of the book is theory-oriented and\ndiscusses concentration inequalities and generalization bounds.",
        "full_text": "Introduction to Machine Learning\nLaurent Younes\nSeptember 5, 2024\narXiv:2409.02668v1  [stat.ML]  4 Sep 2024\n2\nContents\nPreface\n13\n1\nGeneral Notation and Background Material\n15\n1.1\nLinear algebra . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n15\n1.2\nTopology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n17\n1.3\nCalculus\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n18\n1.4\nProbability theory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n22\n2\nA Few Results in Matrix Analysis\n25\n2.1\nNotation and basic facts\n. . . . . . . . . . . . . . . . . . . . . . . . . .\n25\n2.2\nThe trace inequality . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n26\n2.3\nApplications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n30\n2.4\nSome matrix norms . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n32\n3\nIntroduction to Optimization\n37\n3.1\nBasic Terminology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n37\n3.2\nUnconstrained Optimization Problems . . . . . . . . . . . . . . . . . .\n39\n3.2.1\nConditions for optimality (general case) . . . . . . . . . . . . .\n39\n3.2.2\nConvex sets and functions . . . . . . . . . . . . . . . . . . . . .\n40\n3.2.3\nRelative interior . . . . . . . . . . . . . . . . . . . . . . . . . . .\n42\n3.2.4\nDerivatives of convex functions and optimality conditions . . .\n45\n3.2.5\nDirection of descent and steepest descent\n. . . . . . . . . . . .\n48\n3.2.6\nConvergence . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n49\n3.2.7\nLine search . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n52\n3.3\nStochastic gradient descent . . . . . . . . . . . . . . . . . . . . . . . . .\n56\n3.3.1\nStochastic approximation methods . . . . . . . . . . . . . . . .\n56\n3.3.2\nDeterministic approximation and convergence study . . . . . .\n56\n3.3.3\nThe ADAM algorithm . . . . . . . . . . . . . . . . . . . . . . . .\n62\n3.4\nConstrained optimization problems . . . . . . . . . . . . . . . . . . . .\n63\n3.4.1\nLagrange multipliers . . . . . . . . . . . . . . . . . . . . . . . .\n63\n3.4.2\nConvex constraints . . . . . . . . . . . . . . . . . . . . . . . . .\n65\n3.4.3\nApplications . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n67\n3.4.4\nProjected gradient descent . . . . . . . . . . . . . . . . . . . . .\n70\n3.5\nGeneral convex problems . . . . . . . . . . . . . . . . . . . . . . . . . .\n71\n3\n4\nCONTENTS\n3.5.1\nEpigraphs\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n71\n3.5.2\nSubgradients . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n72\n3.5.3\nDirectional derivatives . . . . . . . . . . . . . . . . . . . . . . .\n74\n3.5.4\nSubgradient descent\n. . . . . . . . . . . . . . . . . . . . . . . .\n76\n3.5.5\nProximal Methods . . . . . . . . . . . . . . . . . . . . . . . . . .\n78\n3.6\nDuality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n84\n3.6.1\nGeneralized KKT conditions . . . . . . . . . . . . . . . . . . . .\n84\n3.6.2\nDual problem . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n86\n3.6.3\nExample: Quadratic programming . . . . . . . . . . . . . . . .\n88\n3.6.4\nProximal iterations and augmented Lagrangian . . . . . . . . .\n89\n3.6.5\nAlternative direction method of multipliers . . . . . . . . . . .\n91\n3.7\nConvex separation theorems and additional proofs . . . . . . . . . . .\n92\n3.7.1\nProof of proposition 3.44 . . . . . . . . . . . . . . . . . . . . . .\n92\n3.7.2\nProof of theorem 3.45 . . . . . . . . . . . . . . . . . . . . . . . .\n94\n3.7.3\nProof of theorem 3.46 . . . . . . . . . . . . . . . . . . . . . . . .\n95\n4\nIntroduction: Bias and Variance\n97\n4.1\nParameter estimation and sieves . . . . . . . . . . . . . . . . . . . . . .\n97\n4.2\nKernel density estimation . . . . . . . . . . . . . . . . . . . . . . . . . . 101\n5\nPrediction: Basic Concepts\n107\n5.1\nGeneral Setting\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 107\n5.2\nConditional expectation\n. . . . . . . . . . . . . . . . . . . . . . . . . . 108\n5.3\nBayes predictor\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 111\n5.4\nExamples: model-based approach . . . . . . . . . . . . . . . . . . . . . 113\n5.4.1\nGaussian models and naive Bayes . . . . . . . . . . . . . . . . . 113\n5.4.2\nKernel regression . . . . . . . . . . . . . . . . . . . . . . . . . . 114\n5.4.3\nA classification example . . . . . . . . . . . . . . . . . . . . . . 115\n5.5\nEmpirical risk minimization . . . . . . . . . . . . . . . . . . . . . . . . 115\n5.5.1\nGeneral principles\n. . . . . . . . . . . . . . . . . . . . . . . . . 115\n5.5.2\nBias and variance . . . . . . . . . . . . . . . . . . . . . . . . . . 116\n5.6\nEvaluating the error . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 119\n5.6.1\nGeneralization error\n. . . . . . . . . . . . . . . . . . . . . . . . 119\n5.6.2\nCross validation . . . . . . . . . . . . . . . . . . . . . . . . . . . 120\n6\nInner Products and Reproducing Kernels\n123\n6.1\nIntroduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 123\n6.2\nBasic Definitions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 123\n6.2.1\nInner-product spaces . . . . . . . . . . . . . . . . . . . . . . . . 123\n6.2.2\nFeature spaces and kernels . . . . . . . . . . . . . . . . . . . . . 124\n6.3\nFirst examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 126\n6.3.1\nInner product . . . . . . . . . . . . . . . . . . . . . . . . . . . . 126\n6.3.2\nPolynomial Kernels . . . . . . . . . . . . . . . . . . . . . . . . . 126\nCONTENTS\n5\n6.3.3\nFunctional Features . . . . . . . . . . . . . . . . . . . . . . . . . 127\n6.3.4\nGeneral construction theorems\n. . . . . . . . . . . . . . . . . . 129\n6.3.5\nOperations on kernels\n. . . . . . . . . . . . . . . . . . . . . . . 130\n6.3.6\nCanonical Feature Spaces\n. . . . . . . . . . . . . . . . . . . . . 132\n6.4\nProjection on a finite-dimensional subspace . . . . . . . . . . . . . . . 134\n7\nLinear Regression\n137\n7.1\nLeast-Square Regression\n. . . . . . . . . . . . . . . . . . . . . . . . . . 137\n7.1.1\nNotation and Basic Estimator . . . . . . . . . . . . . . . . . . . 137\n7.1.2\nLimit behavior . . . . . . . . . . . . . . . . . . . . . . . . . . . . 140\n7.1.3\nGauss-Markov theorem . . . . . . . . . . . . . . . . . . . . . . . 141\n7.1.4\nKernel Version . . . . . . . . . . . . . . . . . . . . . . . . . . . . 142\n7.2\nRidge regression and Lasso . . . . . . . . . . . . . . . . . . . . . . . . . 143\n7.2.1\nRidge Regression\n. . . . . . . . . . . . . . . . . . . . . . . . . . 143\n7.2.2\nEquivalence of constrained and penalized formulations . . . . 147\n7.2.3\nLasso regression . . . . . . . . . . . . . . . . . . . . . . . . . . . 150\n7.3\nOther Sparsity Estimators\n. . . . . . . . . . . . . . . . . . . . . . . . . 155\n7.3.1\nLARS estimator . . . . . . . . . . . . . . . . . . . . . . . . . . . 155\n7.3.2\nThe Dantzig selector\n. . . . . . . . . . . . . . . . . . . . . . . . 157\n7.4\nSupport Vector Machines for regression\n. . . . . . . . . . . . . . . . . 161\n7.4.1\nLinear SVM\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 161\n7.4.2\nThe kernel trick and SVMs . . . . . . . . . . . . . . . . . . . . . 165\n8\nModels for linear classification\n167\n8.1\nLogistic regression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 167\n8.1.1\nGeneral Framework . . . . . . . . . . . . . . . . . . . . . . . . . 167\n8.1.2\nConditional log-likelihood . . . . . . . . . . . . . . . . . . . . . 168\n8.1.3\nTraining algorithm\n. . . . . . . . . . . . . . . . . . . . . . . . . 171\n8.1.4\nPenalized Logistic Regression . . . . . . . . . . . . . . . . . . . 173\n8.1.5\nKernel logistic regression . . . . . . . . . . . . . . . . . . . . . . 176\n8.2\nLinear Discriminant analysis . . . . . . . . . . . . . . . . . . . . . . . . 177\n8.2.1\nGenerative model in classification and LDA . . . . . . . . . . . 177\n8.2.2\nDimension reduction . . . . . . . . . . . . . . . . . . . . . . . . 179\n8.2.3\nFisher\u2019s LDA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 182\n8.2.4\nKernel LDA\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 182\n8.3\nOptimal Scoring . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 187\n8.3.1\nKernel optimal scoring . . . . . . . . . . . . . . . . . . . . . . . 192\n8.4\nSeparating hyperplanes and SVMs\n. . . . . . . . . . . . . . . . . . . . 194\n8.4.1\nOne-layer perceptron and margin . . . . . . . . . . . . . . . . . 194\n8.4.2\nMaximizing the margin . . . . . . . . . . . . . . . . . . . . . . . 195\n8.4.3\nKKT conditions and dual problem\n. . . . . . . . . . . . . . . . 197\n8.4.4\nKernel version . . . . . . . . . . . . . . . . . . . . . . . . . . . . 198\n6\nCONTENTS\n9\nNearest-Neighbor Methods\n201\n9.1\nNearest neighbors for regression . . . . . . . . . . . . . . . . . . . . . . 201\n9.1.1\nConsistency\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 201\n9.1.2\nOptimality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 207\n9.2\np-NN classification\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 208\n9.3\nDesigning the distance . . . . . . . . . . . . . . . . . . . . . . . . . . . 210\n10 Tree-based algorithms\n213\n10.1 Recursive Partitioning . . . . . . . . . . . . . . . . . . . . . . . . . . . . 213\n10.1.1 Binary prediction trees . . . . . . . . . . . . . . . . . . . . . . . 213\n10.1.2 Training algorithm\n. . . . . . . . . . . . . . . . . . . . . . . . . 214\n10.1.3 Resulting predictor . . . . . . . . . . . . . . . . . . . . . . . . . 215\n10.1.4 Stopping rule\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . 215\n10.1.5 Leaf predictors\n. . . . . . . . . . . . . . . . . . . . . . . . . . . 215\n10.1.6 Binary features\n. . . . . . . . . . . . . . . . . . . . . . . . . . . 215\n10.1.7 Pruning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 217\n10.2 Random Forests . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 218\n10.2.1 Bagging . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 218\n10.2.2 Feature randomization . . . . . . . . . . . . . . . . . . . . . . . 219\n10.3 Top-Scoring Pairs\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 220\n10.4 Adaboost . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 222\n10.4.1 General set-up . . . . . . . . . . . . . . . . . . . . . . . . . . . . 222\n10.4.2 The Adaboost algorithm . . . . . . . . . . . . . . . . . . . . . . 223\n10.4.3 Adaboost and greedy gradient descent . . . . . . . . . . . . . . 227\n10.5 Gradient boosting and regression . . . . . . . . . . . . . . . . . . . . . 228\n10.5.1 Notation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 228\n10.5.2 Translation-invariant loss\n. . . . . . . . . . . . . . . . . . . . . 229\n10.5.3 General loss functions\n. . . . . . . . . . . . . . . . . . . . . . . 230\n10.5.4 Return to classification . . . . . . . . . . . . . . . . . . . . . . . 232\n10.5.5 Gradient tree boosting . . . . . . . . . . . . . . . . . . . . . . . 233\n11 Neural Nets\n237\n11.1 First definitions\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 237\n11.2 Neural nets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 238\n11.2.1 Transitions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 238\n11.2.2 Output . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 238\n11.2.3 Image data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 239\n11.3 Geometry . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 239\n11.4 Objective function . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 241\n11.4.1 Definitions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 241\n11.4.2 Differential . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 241\n11.4.3 Complementary computations\n. . . . . . . . . . . . . . . . . . 244\n11.5 Stochastic Gradient Descent . . . . . . . . . . . . . . . . . . . . . . . . 245\nCONTENTS\n7\n11.5.1 Mini-batches . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 245\n11.5.2 Dropout\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 245\n11.6 Continuous time limit and dynamical systems . . . . . . . . . . . . . . 246\n11.6.1 Neural ODEs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 246\n11.6.2 Adding a running cost . . . . . . . . . . . . . . . . . . . . . . . 249\n12 Monte-Carlo Sampling\n255\n12.1 General sampling procedures\n. . . . . . . . . . . . . . . . . . . . . . . 255\n12.2 Rejection sampling\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 256\n12.3 Markov chain sampling . . . . . . . . . . . . . . . . . . . . . . . . . . . 257\n12.3.1 Definitions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 258\n12.3.2 Convergence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 259\n12.3.3 Invariance and reversibility\n. . . . . . . . . . . . . . . . . . . . 263\n12.3.4 Irreducibility and recurrence\n. . . . . . . . . . . . . . . . . . . 266\n12.3.5 Speed of convergence . . . . . . . . . . . . . . . . . . . . . . . . 268\n12.3.6 Models on finite state spaces . . . . . . . . . . . . . . . . . . . . 268\n12.3.7 Examples on Rd . . . . . . . . . . . . . . . . . . . . . . . . . . . 269\n12.4 Gibbs sampling\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 273\n12.4.1 Definition\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 273\n12.4.2 Example: Ising model . . . . . . . . . . . . . . . . . . . . . . . . 276\n12.5 Metropolis-Hastings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 277\n12.5.1 Definition\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 277\n12.5.2 Sampling methods for continuous variables . . . . . . . . . . . 278\n12.6 Perfect sampling methods\n. . . . . . . . . . . . . . . . . . . . . . . . . 282\n12.7 Markovian Stochastic Approximation . . . . . . . . . . . . . . . . . . . 286\n13 Markov Random Fields\n293\n13.1 Independence and conditional independence\n. . . . . . . . . . . . . . 293\n13.1.1 Definitions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 293\n13.1.2 Fundamental properties . . . . . . . . . . . . . . . . . . . . . . 295\n13.1.3 Mutual independence\n. . . . . . . . . . . . . . . . . . . . . . . 297\n13.1.4 Relation with Information Theory . . . . . . . . . . . . . . . . . 298\n13.2 Models on undirected graphs\n. . . . . . . . . . . . . . . . . . . . . . . 301\n13.2.1 Graphical representation of conditional independence . . . . . 301\n13.2.2 Reduction of the Markov property\n. . . . . . . . . . . . . . . . 304\n13.2.3 Restricted graph and partial evidence\n. . . . . . . . . . . . . . 307\n13.2.4 Marginal distributions . . . . . . . . . . . . . . . . . . . . . . . 308\n13.3 The Hammersley-Clifford theorem\n. . . . . . . . . . . . . . . . . . . . 309\n13.3.1 Families of local interactions . . . . . . . . . . . . . . . . . . . . 309\n13.3.2 Characterization of positive G-Markov processes . . . . . . . . 312\n13.4 Models on acyclic graphs . . . . . . . . . . . . . . . . . . . . . . . . . . 317\n13.4.1 Finite Markov chains . . . . . . . . . . . . . . . . . . . . . . . . 317\n13.4.2 Undirected acyclic graph models and trees\n. . . . . . . . . . . 319\n8\nCONTENTS\n13.5 Examples of general \u201cloopy\u201d Markov random fields . . . . . . . . . . . 324\n13.6 General state spaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 326\n14 Probabilistic Inference for MRF\n329\n14.1 Monte Carlo sampling\n. . . . . . . . . . . . . . . . . . . . . . . . . . . 330\n14.2 Inference with acyclic graphs\n. . . . . . . . . . . . . . . . . . . . . . . 335\n14.3 Belief propagation and free energy approximation\n. . . . . . . . . . . 341\n14.3.1 BP stationarity . . . . . . . . . . . . . . . . . . . . . . . . . . . . 341\n14.3.2 Free-energy approximations . . . . . . . . . . . . . . . . . . . . 342\n14.4 Computing the most likely configuration . . . . . . . . . . . . . . . . . 348\n14.5 General sum-prod and max-prod algorithms . . . . . . . . . . . . . . . 351\n14.5.1 Factor graphs\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . 351\n14.5.2 Junction trees\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . 356\n14.6 Building junction trees . . . . . . . . . . . . . . . . . . . . . . . . . . . 358\n14.6.1 Triangulated graphs\n. . . . . . . . . . . . . . . . . . . . . . . . 359\n14.6.2 Building triangulated graphs\n. . . . . . . . . . . . . . . . . . . 363\n14.6.3 Computing maximal cliques . . . . . . . . . . . . . . . . . . . . 366\n14.6.4 Characterization of junction trees . . . . . . . . . . . . . . . . . 367\n15 Bayesian Networks\n371\n15.1 Definitions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 371\n15.2 Conditional independence graph\n. . . . . . . . . . . . . . . . . . . . . 372\n15.2.1 Moral graph . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 372\n15.2.2 Reduction to d-separation . . . . . . . . . . . . . . . . . . . . . 374\n15.2.3 Chain-graph representation . . . . . . . . . . . . . . . . . . . . 376\n15.2.4 Markov equivalence . . . . . . . . . . . . . . . . . . . . . . . . . 378\n15.2.5 Probabilistic inference: Sum-prod algorithm\n. . . . . . . . . . 382\n15.2.6 Conditional probabilities and interventions . . . . . . . . . . . 387\n15.3 Structural equation models . . . . . . . . . . . . . . . . . . . . . . . . . 389\n16 Latent Variables and Variational Methods\n391\n16.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 391\n16.2 Variational principle\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . 391\n16.3 Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 393\n16.3.1 Mode approximation . . . . . . . . . . . . . . . . . . . . . . . . 393\n16.3.2 Gaussian approximation . . . . . . . . . . . . . . . . . . . . . . 394\n16.3.3 Mean-field approximation . . . . . . . . . . . . . . . . . . . . . 394\n16.4 Maximum likelihood estimation . . . . . . . . . . . . . . . . . . . . . . 397\n16.4.1 The EM algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . 397\n16.4.2 Application: Mixtures of Gaussian . . . . . . . . . . . . . . . . 399\n16.4.3 Stochastic approximation EM . . . . . . . . . . . . . . . . . . . 401\n16.4.4 Variational approximation . . . . . . . . . . . . . . . . . . . . . 403\n16.5 Remarks\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 405\nCONTENTS\n9\n16.5.1 Variations on the EM . . . . . . . . . . . . . . . . . . . . . . . . 405\n16.5.2 Direct minimization\n. . . . . . . . . . . . . . . . . . . . . . . . 406\n16.5.3 Product measure assumption\n. . . . . . . . . . . . . . . . . . . 407\n17 Learning Graphical Models\n409\n17.1 Learning Bayesian networks . . . . . . . . . . . . . . . . . . . . . . . . 409\n17.1.1 Learning a Single Probability\n. . . . . . . . . . . . . . . . . . . 409\n17.1.2 Learning a Finite Probability Distribution . . . . . . . . . . . . 411\n17.1.3 Conjugate Prior for Bayesian Networks . . . . . . . . . . . . . . 412\n17.1.4 Structure Scoring . . . . . . . . . . . . . . . . . . . . . . . . . . 413\n17.1.5 Reducing the Parametric Dimension . . . . . . . . . . . . . . . 414\n17.2 Learning Loopy Markov Random Fields\n. . . . . . . . . . . . . . . . . 415\n17.2.1 Maximum Likelihood with Exponential Models . . . . . . . . . 415\n17.2.2 Maximum likelihood with stochastic gradient ascent . . . . . . 417\n17.2.3 Relation with Maximum Entropy . . . . . . . . . . . . . . . . . 418\n17.2.4 Iterative Scaling . . . . . . . . . . . . . . . . . . . . . . . . . . . 420\n17.2.5 Pseudo likelihood . . . . . . . . . . . . . . . . . . . . . . . . . . 423\n17.2.6 Continuous variables and score matching . . . . . . . . . . . . 424\n17.3 Incomplete observations for graphical models . . . . . . . . . . . . . . 426\n17.3.1 The EM Algorithm\n. . . . . . . . . . . . . . . . . . . . . . . . . 426\n17.3.2 Stochastic gradient ascent . . . . . . . . . . . . . . . . . . . . . 427\n17.3.3 Pseudo-EM Algorithm\n. . . . . . . . . . . . . . . . . . . . . . . 428\n17.3.4 Partially-observed Bayesian networks on trees . . . . . . . . . . 429\n17.3.5 General Bayesian networks . . . . . . . . . . . . . . . . . . . . . 431\n18 Deep Generative Methods\n433\n18.1 Normalizing flows . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 433\n18.1.1 General concepts\n. . . . . . . . . . . . . . . . . . . . . . . . . . 433\n18.1.2 A greedy computation\n. . . . . . . . . . . . . . . . . . . . . . . 434\n18.1.3 Neural implementation . . . . . . . . . . . . . . . . . . . . . . . 435\n18.1.4 Time-continuous version . . . . . . . . . . . . . . . . . . . . . . 435\n18.2 Non-diffeomorphic models and variational autoencoders\n. . . . . . . 437\n18.2.1 General framework . . . . . . . . . . . . . . . . . . . . . . . . . 437\n18.2.2 Generative model for VAEs\n. . . . . . . . . . . . . . . . . . . . 437\n18.2.3 Discrete data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 439\n18.3 Generative Adversarial Networks (GAN) . . . . . . . . . . . . . . . . . 440\n18.3.1 Basic principles . . . . . . . . . . . . . . . . . . . . . . . . . . . 440\n18.3.2 Objective function\n. . . . . . . . . . . . . . . . . . . . . . . . . 440\n18.3.3 Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 441\n18.3.4 Associated probability metric and Wasserstein GANs\n. . . . . 442\n18.4 Reversed Markov chain models\n. . . . . . . . . . . . . . . . . . . . . . 444\n18.4.1 General principles\n. . . . . . . . . . . . . . . . . . . . . . . . . 444\n18.4.2 Binary model\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . 445\n10\nCONTENTS\n18.4.3 Model with continuous variables . . . . . . . . . . . . . . . . . 447\n18.4.4 Continuous-time limit . . . . . . . . . . . . . . . . . . . . . . . 449\n18.4.5 Differential of neural functions . . . . . . . . . . . . . . . . . . 449\n19 Clustering\n451\n19.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 451\n19.2 Hierarchical clustering and dendograms . . . . . . . . . . . . . . . . . 452\n19.2.1 Partition trees . . . . . . . . . . . . . . . . . . . . . . . . . . . . 452\n19.2.2 Bottom-up construction\n. . . . . . . . . . . . . . . . . . . . . . 453\n19.2.3 Top-down construction . . . . . . . . . . . . . . . . . . . . . . . 456\n19.2.4 Thresholding\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . 457\n19.3 K-medoids and K-mean . . . . . . . . . . . . . . . . . . . . . . . . . . . 457\n19.3.1 K-medoids . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 457\n19.3.2 Mixtures of Gaussian and deterministic annealing . . . . . . . 460\n19.3.3 Kernel (soft) K-means . . . . . . . . . . . . . . . . . . . . . . . . 462\n19.3.4 Convex relaxation . . . . . . . . . . . . . . . . . . . . . . . . . . 463\n19.4 Spectral clustering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 468\n19.4.1 Spectral approximation of minimum discrepancy . . . . . . . . 468\n19.5 Graph partitioning\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 470\n19.6 Deciding the number of clusters . . . . . . . . . . . . . . . . . . . . . . 472\n19.6.1 Detecting elbows\n. . . . . . . . . . . . . . . . . . . . . . . . . . 472\n19.6.2 The Cali\u00b4nski and Harabasz index . . . . . . . . . . . . . . . . . 473\n19.6.3 The \u201csilhouette\u201d index . . . . . . . . . . . . . . . . . . . . . . . 475\n19.6.4 Comparing to homogeneous data . . . . . . . . . . . . . . . . . 476\n19.7 Bayesian Clustering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 478\n19.7.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 478\n19.7.2 Model with a bounded number of clusters . . . . . . . . . . . . 482\n19.7.3 Non-parametric priors . . . . . . . . . . . . . . . . . . . . . . . 488\n20 Dimension Reduction and Factor Analysis\n497\n20.1 Principal component analysis\n. . . . . . . . . . . . . . . . . . . . . . . 497\n20.1.1 General Framework . . . . . . . . . . . . . . . . . . . . . . . . . 497\n20.1.2 Computation of the principal components . . . . . . . . . . . . 502\n20.2 Kernel PCA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 504\n20.3 Statistical interpretation and probabilistic PCA . . . . . . . . . . . . . 506\n20.4 Generalized PCA\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 509\n20.5 Nuclear norm minimization and robust PCA . . . . . . . . . . . . . . . 511\n20.5.1 Low-rank approximation . . . . . . . . . . . . . . . . . . . . . . 511\n20.5.2 The nuclear norm . . . . . . . . . . . . . . . . . . . . . . . . . . 513\n20.5.3 Robust PCA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 514\n20.6 Independent component analysis . . . . . . . . . . . . . . . . . . . . . 516\n20.6.1 Identifiability\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . 516\n20.6.2 Measuring independence and non-Gaussianity . . . . . . . . . 517\nCONTENTS\n11\n20.6.3 Maximization over orthogonal matrices\n. . . . . . . . . . . . . 523\n20.6.4 Parametric ICA\n. . . . . . . . . . . . . . . . . . . . . . . . . . . 524\n20.6.5 Probabilistic ICA\n. . . . . . . . . . . . . . . . . . . . . . . . . . 526\n20.7 Non-negative matrix factorization . . . . . . . . . . . . . . . . . . . . . 530\n20.8 Variational Autoencoders . . . . . . . . . . . . . . . . . . . . . . . . . . 535\n20.9 Bayesian factor analysis and Poisson point processes . . . . . . . . . . 535\n20.9.1 A feature selection model\n. . . . . . . . . . . . . . . . . . . . . 535\n20.9.2 Non-negative and count variables . . . . . . . . . . . . . . . . . 537\n20.9.3 Feature assignment model . . . . . . . . . . . . . . . . . . . . . 539\n20.10Point processes and random measures\n. . . . . . . . . . . . . . . . . . 542\n20.10.1Poisson processes . . . . . . . . . . . . . . . . . . . . . . . . . . 542\n20.10.2The gamma process . . . . . . . . . . . . . . . . . . . . . . . . . 545\n20.10.3The beta process . . . . . . . . . . . . . . . . . . . . . . . . . . . 546\n20.10.4Beta Process and feature selection . . . . . . . . . . . . . . . . . 547\n21 Data Visualization and Manifold Learning\n549\n21.1 Multidimensional scaling . . . . . . . . . . . . . . . . . . . . . . . . . . 549\n21.1.1 Similarity matching (Euclidean case) . . . . . . . . . . . . . . . 549\n21.1.2 Dissimilarity matching . . . . . . . . . . . . . . . . . . . . . . . 552\n21.2 Manifold learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 556\n21.2.1 Isomap . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 556\n21.2.2 Local Linear Embedding . . . . . . . . . . . . . . . . . . . . . . 558\n21.2.3 Graph Embedding\n. . . . . . . . . . . . . . . . . . . . . . . . . 561\n21.2.4 Stochastic neighbor embedding . . . . . . . . . . . . . . . . . . 565\n21.2.5 Uniform manifold approximation and projection (UMAP) . . . 568\n22 Generalization Bounds\n573\n22.1 Notation\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 573\n22.2 Penalty-based Methods and Minimum Description Length . . . . . . . 574\n22.2.1 Akaike\u2019s information criterion . . . . . . . . . . . . . . . . . . . 574\n22.2.2 Bayesian information criterion and minimum description length576\n22.3 Concentration inequalities . . . . . . . . . . . . . . . . . . . . . . . . . 581\n22.3.1 Cram\u00b4er\u2019s theorem . . . . . . . . . . . . . . . . . . . . . . . . . . 582\n22.3.2 Sub-Gaussian variables . . . . . . . . . . . . . . . . . . . . . . . 584\n22.3.3 Bennett\u2019s inequality . . . . . . . . . . . . . . . . . . . . . . . . . 587\n22.3.4 Hoeffding\u2019s inequality\n. . . . . . . . . . . . . . . . . . . . . . . 590\n22.3.5 McDiarmid\u2019s inequality\n. . . . . . . . . . . . . . . . . . . . . . 592\n22.3.6 Boucheron-Lugosi-Massart inequality\n. . . . . . . . . . . . . . 593\n22.4 Bounding the empirical error with the VC-dimension\n. . . . . . . . . 594\n22.4.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 594\n22.4.2 Vapnik\u2019s theorem . . . . . . . . . . . . . . . . . . . . . . . . . . 596\n22.4.3 VC dimension . . . . . . . . . . . . . . . . . . . . . . . . . . . . 598\n22.4.4 Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 601\n12\nCONTENTS\n22.4.5 Data-based estimates . . . . . . . . . . . . . . . . . . . . . . . . 602\n22.5 Covering numbers and chaining . . . . . . . . . . . . . . . . . . . . . . 604\n22.5.1 Covering, packing and entropy numbers . . . . . . . . . . . . . 605\n22.5.2 A first union bound . . . . . . . . . . . . . . . . . . . . . . . . . 605\n22.5.3 Evaluating covering numbers . . . . . . . . . . . . . . . . . . . 608\n22.5.4 Chaining . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 609\n22.5.5 Metric entropy . . . . . . . . . . . . . . . . . . . . . . . . . . . . 612\n22.5.6 Application\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 613\n22.6 Other complexity measures\n. . . . . . . . . . . . . . . . . . . . . . . . 615\n22.6.1 Fat-shattering and margins . . . . . . . . . . . . . . . . . . . . . 615\n22.6.2 Maximum discrepancy . . . . . . . . . . . . . . . . . . . . . . . 622\n22.6.3 Rademacher complexity . . . . . . . . . . . . . . . . . . . . . . 624\n22.6.4 Algorithmic Stability . . . . . . . . . . . . . . . . . . . . . . . . 627\n22.6.5 PAC-Bayesian bounds . . . . . . . . . . . . . . . . . . . . . . . . 629\n22.7 Application to model selection . . . . . . . . . . . . . . . . . . . . . . . 632\nPreface\nMachine learning addresses the issue of analyzing, reproducing and predicting var-\nious mechanisms and processes observable through experiments and data acquisi-\ntion. With the impetus of large technological companies in need of leveraging in-\nformation included in the gigantic datasets that they produced or obtained through\nuser data, with the development of new data acquisition techniques in biology, physics\nor astronomy, with the improvement of storage capacity and high-performance com-\nputing, this field has experienced an explosive growth over the past decades, in\nterms of scientific production and technological impact.\nWhile it is being recognized in some places as a scientific discipline in itself, ma-\nchine learning (which has received a few almost synonymic denominations across\ntime, including artificial intelligence, machine intelligence or statistical learning),\ncan also be seen as an interdisciplinary field interfacing techniques from traditional\ndomains such as computer science, applied mathematics, and statistics. From statis-\ntics, and more specially nonparametric statistics, it borrows its main formalism,\nasymptotic results and generalization bounds. It also builds on many classical meth-\nods that have been developed for estimation and prediction. From computer science,\nit involves the construction and implementation of efficient algorithms, program-\nming design and architecture. Finally, machine learning leverages classical methods\nfrom linear algebra and functional analysis, as well as from convex and nonlinear\noptimization, fields within which it had also provided new problems and discover-\nies. It forms a significant part of the larger field commonly called \u201cdata science,\u201d\nwhich includes methods for storing, sharing and managing data, the development\npowerful computer architectures for increasingly demanding algorithms, and, im-\nportantly, the definition of ethical limits and processes through which data should\nbe used in the modern world.\nThis book, which originates from lecture notes of a series of graduate course\ntaught in the Department of Applied Mathematics and Statistics at Johns Hopkins\nUniversity, adopts a viewpoint (or bias) mainly focused on the mathematical and sta-\ntistical aspects of the subject. Its goal is to introduce the mathematical foundations\nand techniques that lead to the development and analysis of many of the algorithms\nthat are used today. It is written with the hope to provide the reader with a deeper\n13\n14\nCONTENTS\nunderstanding of the algorithms made available to her in multiple machine learn-\ning packages and software, and that she will be able to assess their prerequisites and\nlimitations, and to extend them and develop new algorithms. Note that, while adopt-\ning a presentation with a strong mathematical flavor, we will still make explicit the\ndetails of many important machine learning algorithms.\nUnsurprisingly, the book will be more accessible to a reader with some back-\nground in mathematics and statistics. It assumes familiarity with basic concepts in\nlinear algebra and matrix analysis, in multivariate calculus and in probability and\nstatistics. We tried to place a limit at the use of measure theoretic tools, that are\navoided up to a few exceptions, which are be localized and be accompanied with\nalternative interpretations allowing for a reading at a more elementary level.\nThe book starts with an introductory chapter that describes notation used through-\nout the book and serve at a reminder of basic concepts in calculus, linear algebra and\nprobability. It also introduces some measure theoretic terminology, and can be used\nas a reading guide for the sections that use these tools. This chapter is followed by\ntwo chapters offering background material on matrix analysis and optimization. The\nlatter chapter, which is relatively long, provides necessary references to many algo-\nrithms that are used in the book, including stochastic gradient descent, proximal\nmethods, etc.\nChapter 4, which is also introductory, illustrates the bias-variance dilemma in\nmachine learning through the angle of density estimation and motivates chapter 5\nin which basic concepts for statistical prediction are provided. Chapter 6 provides\nan introduction to reproducing kernel theory and Hilbert space techniques that are\nused in many places, before tackling, with chapters 7 to 11, the description of vari-\nous algorithms for supervised statistical learning, including linear methods, support\nvector machines, decision trees, boosting, or neural networks.\nChapter 12, which presents sampling methods and an introduction to the theory\nof Markov chains, starts a series of chapters on generative models, and associated\nlearning algorithms. Graphical models and described in chapters 13 to 15. Chap-\nter 16 introduces variational methods for models with latent variables, with applica-\ntions to graphical models in chapter 17. Generative techniques using deep learning\nare presented in chapter 18.\nChapters 19 to 21 focus on unsupervised learning methods, for clustering, factor\nanalysis and manifold learning. The final chapter of the book is theory-oriented and\ndiscusses concentration inequalities and generalization bounds.\nChapter 1\nGeneral Notation and Background Material\n1.1\nLinear algebra\n1. The set of all subsets of a given set A is denoted P(A). If A and B are two sets, the\nnotation BA refers to the set of all functions f : A \u2192B. In particular, RA is the space\nof real-valued functions, and forms a vector space. When A is finite, this space is\nfinite dimensional and can be identified with R|A|, where |A| denotes the cardinality\n(number of elements) of A.\nThe indicator function of a subset C of A will be denoted 1C : A \u2192{0,1}, with 1C(x) =\n1 if x \u2208C and 0 otherwise. We will sometimes write 1x\u2208C for 1C(x).\n2. Elements of the d-dimensional Euclidean space Rd will be denoted with letters\nsuch as x,y,z, and their coordinates will be indexed as parenthesized exponents, so\nthat\nx =\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\nx(1)\n...\nx(d)\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\n(we will always identify element of Rd with column vectors). We will not distinguish\nin the notation between \u201cpoints\u201d in Rd, seen as an affine space, and \u201cvectors\u201d in Rd,\nseen as a vector space. The vectors 0d and 1d will denote the d-dimensional vectors\nwith all coordinates equal to 0 and 1, respectively. The identity matrix in Rd will be\ndenoted IdRd. The canonical basis of Rd, provided by the columns of IdRd will be\ndenoted e1,...,ed.\n3. The Euclidean norm of a vector x \u2208Rd is denoted |x| with\n|x| =\n\u0010\n(x(1))2 + \u00b7\u00b7\u00b7 + (x(d))2\u00111/2.\nIt will sometimes be denoted |x|2, identifying it as a member of the family of \u2113p\nnorms\n|x|p =\n\u0010\n(x(1))p + \u00b7\u00b7\u00b7 + (x(d))p\u00111/p\n(1.1)\n15\n16\nCHAPTER 1. GENERAL NOTATION AND BACKGROUND MATERIAL\nfor p \u22651. One can also define |x|p for 0 < p < 1, using (1.1), but in this case one\ndoes not get a norm because the triangle inequality |x + y|p \u2264|x|p + |y|p is not true\nin general. The family is interesting, however, because it approximates, in the limit\np \u21920, the number of non-zero components of x, denoted |x|0, which is a measure of\nsparsity. Note that we also use the notation |A| to denote the cardinality (number of\nelements) of a set A, hopefully without risk of confusion.\nWhile we use single bars (|x|) to represent norms of finite-dimensional vectors, we\nwill use double bars (\u2225h\u2225) for infinite-dimensional objects.\n4. The set of m \u00d7 d real matrices with real entries is denoted Mm,d(R), or simply\nMm,d (Md,d will also be denoted Md). The set of invertible d \u00d7 d matrices will be\ndenote GLd(R).\nGiven m column vectors x1,...,xm \u2208Rd, the notation [x1,...,xm] refers to the d by m\nmatrix with jth column equal to xj, so that, for example, IdRd = [e1,...,ed].\nEntry (i,j) in a matrix A \u2208Mm,d(R) will either be denoted A(i,j) or A(i)\nj . The rows of\nA will be denoted A(1),...,A(m) and the columns A1,...,Am.\nThe operator norm of a matrix A \u2208Mm,d is defined by\n|A|op = max{|Ax| : x \u2208Rd,|x| = 1}.\n5. The space of d \u00d7 d real symmetric matrices is denoted Sd, and its subsets con-\ntaining positive semi-definite (resp. positive definite) matrices is denoted S+\nd (resp.\nS++\nd ). If m \u2264d, Om,d denotes the set of m \u00d7 d matrices A such that AAT = IdRm, and\none writes Od for Od,d, the space of d-dimensional orthogonal matrices. Finally, SOd\nis the subset Od containing orthogonal matrices with determinant 1, i.e., rotation\nmatrices.\n6. A k-multilinear mapping is a function a : (x1,...,xk) 7\u2192a(x1,...,xk) defined on\n(Rd)k with values in Rq which is linear in each of its variables. The mapping is\nsymmetric if its value is unchanged after any permutation of the variables. If k =\n2 and q = 1, one also says that a is a bilinear form. The norm of a k-multilinear\nmapping is defined as\n|a| = max{a(x1,...,xk) : |xj| \u22641,j = 1,...,k}\nso that\n|a(x1,...,xk)| \u2264|a|\nk\nY\nj=1\n|xj|\nfor all x1,...,xk \u2208Rd.\nA symmetric bilinear form a is called positive semidefinite if a(x,x) \u22650 for all x \u2208Rd,\nand positive definite if it is positive semi-definite and a(x,x) = 0 if and only if x = 0.\n1.2. TOPOLOGY\n17\nSymmetric bilinear forms can always be expressed in the form a(x,y) = xT Ay for\nsome symmetric matrix A, and a is positive (semi-)definite if and only A is also.\nAnalogous statements hold for negative (semi-)definite forms and matrices. We will\nuse the notation A \u227b0 (resp. \u2ab00) to indicate that A is positive definite (resp. positive\nsemidefinite). Note that, if a(x,y) = xT Ay for A \u2208Sd, then |a| = |A|op.\n1.2\nTopology\n1. The open balls in Rd will be denoted\nB(x,r) = {y \u2208Rd : |y \u2212x| < r},\nwith x \u2208Rd and r > 0. The closed balls are denoted \u00afB(x,r) and contain all y\u2019s such\nthat |y \u2212x| \u2264r. A set U \u2282Rd is open if and only if for any x \u2208U, there exists r > 0\nsuch that B(x,r) \u2282U. A set \u0393 \u2282Rd is closed if its complement, denoted\n\u0393c = {x \u2208Rd : x < \u0393}\nis open. The topological interior of a set A \u2282Rd is the largest open set included in\nA. It will be denoted either by \u02daA or int(A). A point x belongs to \u02daA if and only if\nB(x,r) \u2282A for some r > 0.\n2. The closure of A is the smallest closed set that contains A and will be denoted\neither \u00afA or cl(A). A point x belongs to \u00afA if and only if B(x,r) \u2229A , \u2205for all r > 0.\nAlternatively, x belongs to \u00afA if and only if there exists a sequence (xk) that converges\nto x with xk \u2208A for all k.\n3. A compact set in Rd is a set \u0393 such that any sequence of points in \u0393 contains\na subsequence that converges to some point in \u0393. An alternate definition is that,\nwhenever \u0393 is covered by a collection of open sets, there exists a finite subcollection\nthat still covers \u0393.\nOne can show that compact subsets of Rd are exactly its bounded and closed subsets.\n4. A metric space is a space B equipped with a distance, i.e., a function \u03c1 : B \u00d7 B \u2192\n[0,+\u221e) that satisfies the following three properties.\n\u2200x,y \u2208B : \u03c1(x,y) = 0 \u21d4x = y,\n(1.2a)\n\u2200x,y \u2208B : \u03c1(x,y) = \u03c1(y,x),\n(1.2b)\n\u2200x,y,z \u2208B : \u03c1(x,z) \u2264\u03c1(x,y) + \u03c1(y,z).\n(1.2c)\nEquation (1.2c) is called the triangle inequality. The norm of the difference between\ntwo points: \u03c1(x,y) = |x \u2212y|, is a distance on Rd. The definition of open and closed\nsubsets in metric spaces is the same as above, with \u03c1(x,y) replacing |x \u2212y|, and one\nsays that (xn) converges to x if and only if \u03c1(xn,x) \u21920.\nCompact subsets are also defined in the same way, but are not necessarily character-\nized as bounded and closed.\n18\nCHAPTER 1. GENERAL NOTATION AND BACKGROUND MATERIAL\n1.3\nCalculus\n1. If x,y \u2208Rd, we will denote by [x,y] the closed segment delimited by x and y, i.e.,\nthe set of all points (1 \u2212t)x + ty for 0 \u2264t \u22641. One denotes by [x,y), (x,y] and (x,y)\nthe semi-open or open segments, with appropriate strict inequality for t. (Similarly\nto the notation for open intervals, whether (x,y) denotes an open segment or a pair\nof points will always be clear from the context.)\n2. The derivative of a differentiable function f : t 7\u2192f (t) from an interval I \u2282R to\nR will be denoted by \u2202f , or \u2202tf if the variable t is well identified. Its value at t0 \u2208I\nis denoted either as \u2202f (t0) or \u2202f |t=t0. Higher derivatives are denoted as \u2202kf , k \u22650,\nwith the usual convention \u22020f = f . Note that notation such as f \u2032,f \u2032\u2032,f (3) will never\nrefer to derivatives.\nIn the following, U is an open subset of Rd. If f is a function from U to Rm, we let\nf (i) denote the ith component of f , so that\nf (x) =\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\nf (1)(x)\n...\nf (m)(x)\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\nfor x \u2208U. If d = 1, and f is differentiable, the derivative of f at x is the column\nvector of the derivatives of its components,\n\u2202f (x) =\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\n\u2202f (1)(x)\n...\n\u2202f (m)(x)\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\nFor d \u22651 and j \u2208{1,...,d}, the jth partial derivative of f at x is\n\u2202jf (x) = \u2202(t 7\u2192f (x + tej))|t=0 \u2208Rm,\nwhere e1,...,ed form the canonical basis of Rd. If the notation for the variables on\nwhich f depends is well understood from the context, we will alternatively use \u2202xjf .\n(For example, if f : (\u03b1,\u03b2) 7\u2192f (\u03b1,\u03b2), we will prefer \u2202\u03b1f to \u22021f .) The differential of f\nat x is the linear mapping from Rd to Rm represented by the matrix\ndf (x) = [\u22021f (x),...,\u2202df (x)].\nIt is defined so that, for all h \u2208Rd\ndf (x)h = \u2202(t 7\u2192f (x + th))|t=0\nwhere the right-hand side is the directional derivative of f at x in the direction h.\nNote that, if f : Rd \u2192R (i.e., m = 1), df (x) is a row vector. If f is differentiable\n1.3. CALCULUS\n19\non U and df (x) is continuous as a function of x, one says that f is continuously\ndifferentiable, or C1.\nDifferentials obey the product rule and the chain rule. If f ,g : U \u2192R, then\nd(f g)(x) = f (x)dg(x) + g(x)df (x).\nIf f : U \u2192Rm, g : \u02dcU \u2282Rk \u2192U, then\nd(f \u25e6g)(x) = df (g(x))dg(x).\nIf d = m (so that df (x) is a square matrix), we let \u2207\u00b7f (x) = trace(df (x)), the divergence\nof f .\nThe Euclidean gradient of a differentiable function f : U \u2192R is \u2207f (x) = df (x)T .\nMore generally, one defines the gradient of f with respect to a tensor field x 7\u2192A(x)\ntaking values in S++\nd , as the vector \u2207Af (x) that satisfies the relation\ndf (x)h = \u2207Af (x)T A(x)h\nfor all h \u2208Rd, so that\n\u2207Af (x) = A(x)\u22121df (x)T .\n(1.3)\nIn particular, the Euclidean gradient is associated with A(x) = IdRd for all x. With\nsome abuse of notation, we will denote \u2207Af = A\u22121\u2207f when A is a fixed matrix,\ntherefore identified with the constant tensor field x 7\u2192A.\n3. We here compute, as an illustration and because they will be useful later, the\ndifferential of the determinant and the inversion in matrix spaces.\nRecall that, if A = [a1,...,ad] \u2208Md is a d by d matrix,, with a1,...,ad \u2208Rd, det(A) is a\nd-linear form \u03b4(a1,...,ad) which vanishes when two columns coincide and such that\n\u03b4(e1,...,ed) = 1. In particular \u03b4 changes signs when two of its columns are inverted.\nIt follows from this that\n\u2202aij det(A) = \u03b4(a1,...,ai\u22121,ej,aj+1,...,ad)\n= (\u22121)i\u22121\u03b4(ej,a1,...,ai\u22121,...,ad) = (\u22121)i+j detA(ij),\nwhere A(ij) is the matrix A with row i and column j removed. We therefore find that\nthe differential of A 7\u2192det(A) is the mapping\nH 7\u2192trace(cof(A)T H)\n(1.4)\nwhere cof(A) is the matrix composed of co-factors (\u22121)i+j detA(ij). As a consequence,\nif A is invertible, then the differential of log|det(A)| is the mapping\nH 7\u2192trace(det(A)\u22121cof(A)T H) = trace(A\u22121H)\n(1.5)\n20\nCHAPTER 1. GENERAL NOTATION AND BACKGROUND MATERIAL\nConsider now the function I(A) = A 7\u2192A\u22121 defined on GLd(R), which is an open\nsubset of Md(R). Using AI(A) = IdRd and the product rule, we get\nA(dI(A)H) + HI(A) = 0\nor\ndI(A)H = \u2212A\u22121HA\u22121.\n(1.6)\n4. Higher-order partial derivatives \u2202ik \u00b7\u00b7\u00b7\u2202i1f : U \u2192Rm are defined by iterating the\ndefinition of first-order derivatives, namely\n\u2202ik \u00b7\u00b7\u00b7\u2202i1f (x) = \u2202ik(\u2202ik\u22121 \u00b7\u00b7\u00b7\u2202i1f )(x)\nIf all order k partial derivatives of f exist and are continuous, one says that f is\nk-times continuously differentiable, or Ck and, when true, the order in which the\nderivatives are taken does not matter. In this case, one typically groups derivatives\nwith the same order using a power notation, writing, for example\n\u22021\u22022\u22021f = \u22022\n1\u22022f\nfor a C3 function.\nIf f is Ck, its kth differential at x is a symmetric k-multilinear map that can also be\niteratively defined by (for h1,...,hk \u2208Rd)\ndkf (x)(h1,...,hk) = d(dk\u22121f (x)(h1,...,hk\u22121))hk \u2208Rm.\nIt is related to partial derivatives through the relation:\ndkf (x)(h1,...,hk) =\nd\nX\ni1,...,ik=1\nh(i1)\n1\n\u00b7\u00b7\u00b7h(ik)\nk\n\u2202ik \u00b7\u00b7\u00b7\u2202i1f (x).\nWhen m = 1 and k = 2, one denotes by \u22072f (x) = (\u2202i\u2202jf (x),i,j = 1,...,n) the symmet-\nric matrix formed by partial derivatives of order 2 of f at x. It is called the Hessian\nof f at x and satisfies\nhT\n1 \u22072f (x)h2 = d2f (x)(h1,h2).\nThe Laplacian of f is the trace of \u22072f and denoted \u2206f .\n5. Taylor\u2019s theorem, in its integral form, generalizes the fundamental theorem of\ncalculus to higher derivatives. It expresses the fact that, if f is Ck on U and x,y \u2208U\nare such that the closed segment [x,y] is included in U, then, letting h = y \u2212x:\nf (x + h) = f (x) + df (x)h + 1\n2d2f (x)(h,h) + \u00b7\u00b7\u00b7 +\n1\n(k \u22121)!dk\u22121f (x)(h,...,h)\n+\n1\n(k \u22121)!\nZ 1\n0\n(1 \u2212t)k\u22121dkf (x + th)(h,...,h)dt\n(1.7)\n1.4. PROBABILITY THEORY\n21\nThe last term (remainder) can also be written as\n1\nk!\nR 1\n0 (1 \u2212t)k\u22121dkf (x + th)(h,...,h)dt\nR 1\n0 (1 \u2212t)k\u22121 dt\n.\nIf f takes scalar values, then dkf (x + th)(h,...,h) is real and the intermediate value\ntheorem implies that there exists some z in [x,y] such that\nf (x + h) = f (x) + df (x)h + 1\n2d2f (x)(h,h) + \u00b7\u00b7\u00b7 +\n1\n(k \u22121)!dk\u22121f (x)(h,...,h)\n+ 1\nk!dkf (z)(h,...,h).\n(1.8)\nThis is not true if f takes vector values. However, for any M such that |dkf (z)| \u2264M\nfor z \u2208[x,y] (such M\u2019s always exist because f is Ck), one has\n1\n(k \u22121)!\nZ 1\n0\n(1 \u2212t)k\u22121dkf (x + th)(h,...,h)dt \u2264M\nk! |h|k.\nEquation (1.7) can be written as\nf (x + h) = f (x) + df (x)h + 1\n2d2f (x)(h,h) + \u00b7\u00b7\u00b7 + 1\nk!dkf (x)(h,...,h)\n+\n1\n(k \u22121)!\nZ 1\n0\n(1 \u2212t)k\u22121(dkf (x + th)(h,...,h) \u2212dkf (x)(h,\u00b7\u00b7\u00b7 ,h))dt .\n(1.9)\nLet\n\u03f5x(r) = max\nn\n|dkf (x + h) \u2212dkf (x)| : |h| \u2264r\no\n.\nSince dkf is continuous, \u03f5x(r) tends to 0 when r \u21920 and we have\nZ 1\n0\n(1 \u2212t)k\u22121|dkf (x + th)(h,...,h) \u2212dkf (x)(h,\u00b7\u00b7\u00b7 ,h)|dt \u2264|h|k\nk \u03f5x(|h|).\nThis shows that (1.7) implies that\nf (x + h) = f (x) + df (x)h + 1\n2d2f (x)(h,h) + \u00b7\u00b7\u00b7 + 1\nk!dkf (x)(h,...,h) + |h|k\nk! \u03f5x(|h|)\n(1.10)\n= f (x) + df (x)h + 1\n2d2f (x)(h,h) + \u00b7\u00b7\u00b7 + 1\nk!dkf (x)(h,...,h) + o(|h|k)\n(1.11)\n22\nCHAPTER 1. GENERAL NOTATION AND BACKGROUND MATERIAL\n1.4\nProbability theory\n1. When discussing probabilistic concepts, we will make the convenient assump-\ntion that all random variables are defined on a fixed probability space (\u2126,P). This\nmeans that \u2126is large enough to include enough randomness to generate all required\nvariables (and implicitly enlarged when needed).\nWe assume that the reader is familiar with concepts related to discrete random vari-\nables or continuous variables (with values in Rd for some d) and their probability\ndensity functions, or p.d.f.\u2019s. In particular, X : \u2126\u2192Rd is a random variable with\np.d.f. f if and only if the expectation of \u03d5(X) is given by\nE(\u03d5(X)) =\nZ\nRd \u03d5(x)f (x)dx\nfor all bounded and continuous functions \u03d5 : Rd \u2192[0,+\u221e).\n2. With a few exceptions, we will use capital letters for random variables and small\nletters for scalars and vectors that represent realizations of these variables. One of\nthese exceptions will be our notation for training data, defined as an independent\nand identically distributed (i.i.d.) sample of a given random variable. A realization\nof such a sample will always be denoted T = (x1,...,xN), which is therefore a series\nof observations. We will use the notation T = (X1,...,XN) for the collection of i.i.d.\nrandom variables that generate the training set, so that T = (X1(\u03c9),...,XN(\u03c9)) = T (\u03c9)\nfor some \u03c9 \u2208\u2126. Another exception will apply to variables denoted using Greek\nletters, for which we will use boldface fonts (such as \u03b1,\u03b2,...).\nFor a random variable X, the notation [X = x], or [X \u2208A] refers to subsets of \u2126, for\nexample,\n[X = x] = {\u03c9 \u2208\u2126: X(\u03c9) = x}.\n3. As much as possible\u2014but not always\u2014we will avoid making explicit reference to\nmeasure theory, leaving to readers familiar with this theory the task to complete the\nnotation and sometimes assumption gaps in order to make some of our statements\nfully rigorous.\nHowever, there will be situations in which the flexibility of the measure-theoretic\nformalism is needed for the exposition. The following notions may help the reader\nnavigate through these situations (basic references in measure theory are Rudin\n[171], Dudley [66], Billingsley [32]).\nA measurable space is a pair (S,S) where S is a set and S \u2282P(S) contains S, is\nstable by complementation (if A \u2208S, then Ac = S \\ A \u2208S), by countable unions and\nintersections. Such an S is called a \u03c3-algebra and elements of S form the measurable\nsubsets of S (relative to the \u03c3-algebra).\nA (positive) measure \u00b5 on (S,S) in a mapping from S \u2192[0,+\u221e) that associates to\nA \u2208S its measure \u00b5(A), such that the measure of a countable union of disjoint sets\n1.4. PROBABILITY THEORY\n23\nis the countable sum of their measures. A function f : \u2126\u2192Rd is called measurable\nif the inverse images by f of open subsets of Rd are mesurable.\nA measurable set A (or event) is negligible (for P) if P(A) = 0 and events are said to\nhappen almost surely if their complements are negligible, i.e., P(Ac) = 0.\n4. The integral of a function f : \u2126\u2192Rd with respect to a measure (such as P) is\ndenoted\nR\nS f (x)\u00b5(dx). This integral is defined, using a limit argument, as a function\nwhich is linear in f and such that\nZ\nA\n\u00b5(dx) =\nZ\nS\n1A(x)\u00b5(dx) = \u00b5(A).\nThe Lebesgue measure, Ld, on Rd provides an important example. For this measure\nS is the \u03c3-algebra generated by open subsets,\nR\nRd f (x)Ld(dx) extends the Riemann\nintegral and is denoted\nR\nRd f (x)dx. Another important example, when S is finite or\ncountable, is the counting measure, denoted card, that return the number of ele-\nments of a set, so that card(A) = |A|. In this case, S = P(S) and the integral is simply\nthe sum:\nZ\nS\nf (x)card(dx) =\nX\nx\u2208F\nf (x).\n5. If \u00b5 and \u03bd are measures on (S,S), one says that \u03bd is absolutely continuous with\nrespect to \u00b5 and write \u03bd \u226a\u00b5 if,\n\u2200A \u2208S : \u00b5(A) = 0 \u21d2\u03bd(A) = 0.\n(1.12)\nThe Radon-Nikodym theorem states that \u03bd \u226a\u00b5 if and only if \u03bd has a density with\nrespect to \u00b5, i.e., there exists a measurable function \u03d5 : S \u2192[0,+\u221e) such that\nZ\nS\nf (x)\u03bd(dx) =\nZ\nS\nf (x)\u03d5(x)\u00b5(dx)\nfor all measurable f : S \u2192[0,+\u221e).\n6. If \u00b51 is a measure on (S1,S1) and \u00b52 a measure on (S2,S2), their tensor product is\ndenoted \u00b51 \u2297\u00b52. It is a measure on S1 \u00d7S2 defined by \u00b51 \u2297\u00b52(A1 \u00d7A2) = \u00b51(A1)\u00b52(A2)\nfor A1 \u2208S1 and A2 \u2208S2 (the \u03c3-algebra on S1 \u00d7S2 is the smallest one that contains all\nsets A1 \u00d7 A2, A1 \u2208S1, A2 \u2208S2).\nThe integral, with respect to the product measure, of a function f : S1 \u00d7 S2 \u2192Rd is\ndenoted\nZ\nS1\u00d7S2\nf (x1,x2)\u00b51(dx1)\u00b52(dx2) =\nZ\nS1\u00d7S2\nf (x1,x2)\u00b51 \u2297\u00b52(dx1,dx2).\nThe tensor product between more that two measures is defined similarly, with nota-\ntion\n\u00b51 \u2297\u00b7\u00b7\u00b7 \u2297\u00b5n =\nn\nO\nk=1\n\u00b5k.\n24\nCHAPTER 1. GENERAL NOTATION AND BACKGROUND MATERIAL\n7. When using measure-theoretic probability, we will therefore assume that the pair\n(\u2126,P) is completed to a triple (\u2126,A,P) where A is a \u03c3-algebra and P a probability\nmeasure, that is a positive measure on (\u2126,A) such that P(\u2126) = 1. This triple is called\na probability space.\nA random variable X must then also take values in a measurable space, say (S,S),\nand must be such that, for all C \u2208S, the set [X \u2208C] belongs to A. This justify the\ncomputation of P(X \u2208C), which will also be denoted PX(C).\nA random variable X taking values in Rd has a p.d.f. if and only if PX \u226aLd and\nthe p.d.f. is the density provided by the Radon-Nikodym theorem. For a discrete\nrandom variable (i.e., taking values in a finite or countable set), the p.m.f. of X is\nalso the density of PX with respect to the counting measure card.\nIf X is a random variable with values in Rd, the integral of X with respect to P is the\nexpectation of X, denoted E(X). More generally, if (S,S,P) is a probability space, we\nwill use the notation\nEP(f ) =\nZ\nS\nf (x)P(dx).\nIf P = PX for some random variable X : \u2126\u2192S, we will use EX rather than EPX.\n8. One more technical consideration. Whenever we will consider measurable spaces,\nand sometimes without additional mention, we will assume that these spaces are\ncomplete metric spaces that have a dense countable subset (i.e., that are separable).\nIf not specified otherwise, their \u03c3-algebras are given by the smallest ones containing\nall open sets (the Borel \u03c3-algebra).\nChapter 2\nA Few Results in Matrix Analysis\nThis chapter collects a few results in linear algebra that will be useful in the rest of\nthis book.\n2.1\nNotation and basic facts\nWe denote by Mn,d(R) the space of all n \u00d7 d matrices with real coefficients1. For a\nmatrix A \u2208Mn,d(R) and integer k \u2264n and l \u2264d, we let A\u2308kl\u2309\u2208Mk,l(R) denote the\nmatrix A restricted to its first k rows and first l columns. The i,j entry of A will be\ndenoted A(i,j) or A(ij).\nWe assume that the reader is familiar with elementary matrix analysis, including,\nin particular the fact that symmetric matrices are diagonalizable in an orthonormal\nbasis, i.e., if A \u2208Md,d(R) is a symmetric matrix (whose space is denoted Sd), there\nexists an orthogonal matrix U \u2208Od (i.e., satisfying UT U = UUT = IdRd) and a diag-\nonal matrix D \u2208Md,d(R) such that\nA = UDUT .\nThe identity AU = UD then implies that the columns of U form an orthonormal\nbasis of eigenvectors of A.\nIf A \u2208S+\nd is positive semi-definite (i.e., uT Au \u22650 for all u \u2208Rd), the entries of\nD in the decomposition A = UDUT are non-negative, and one can define the matrix\nsquare root of A as S = UD\u22991/2UT where D\u22991/2 is the diagonal matrix formed taking\nthe square roots of all coefficients of D. We will use the notation S = A1/2. Note that\nD1/2 = D\u22991/2 if D is diagonal and positive semi-definite.\nIf A \u2208S++\nd\nis positive definite (i.e., A is positive semi-definite and uT Au = 0 im-\nplies u = 0) and B is positive semi-definite, both being d\u00d7d matrices, the generalized\n1Unless mentioned otherwise, all matrices are assumed to be real.\n25\n26\nCHAPTER 2. A FEW RESULTS IN MATRIX ANALYSIS\neigenvalue problem associated with A and B consists in finding a diagonal matrix D\nand a matrix U such that BU = AUD and UT AU = IdRd. Letting \u02dcU = A1/2U, the\nproblem is equivalent to solving A\u22121/2BA\u22121/2 \u02dcU = \u02dcUD with \u02dcUT \u02dcU = IdRd, i.e., finding\nthe eigenvalue decomposition of the symmetric positive-definite matrix A\u22121/2BA\u22121/2.\nIf A \u2208Mn,d(R), it can be decomposed as\nA = UDV T\nwhere U \u2208On(R) and V \u2208Od(R)) are orthogonal matrices and D \u2208Mn,d(R) is diago-\nnal (i.e., such that D(i,j) = 0 whenever i , j) with non-negative diagonal coefficients.\nThese coefficients are called the singular values of A, and the procedure is called\na singular-value decomposition (SVD) of A. An equivalent formulation is that there\nexist orthonormal bases u1,...,un of Rn and v1,...,vd of Rd (forming the columns of\nU and V ) such that\nAvi = \u03bbiui\nfor i \u2264min(n,d), where \u03bb1,...,\u03bbmin(n,d) are the singular values. Of course, if A is\nsquare and symmetric positive semi-definite, an eigenvalue decomposition of A is\nalso a singular value decomposition (and the singular values coincide with the eigen-\nvalues). More generally, if A = UDV T , then AAT = UDDT UT and AT A = V DT DV T\nare eigenvalue decompositions of AAT and AT A. Singular values are uniquely de-\nfined, up to reordering. However, the matrices U and V are not unique up to column\nreordering in general.\nIf m = min(n,d), then, forming the matrices \u02dcU = U\u2308n,m\u2309(resp.\n\u02dcV = V\u2308d,m\u2309) by\nremoving from U (resp. V ) its last n \u2212m (resp. d \u2212m) columns , and \u02dcD = D\u2308m,m\u2309by\nremoving from D its n \u2212m rows and d \u2212m columns, one has\nA = \u02dcU \u02dcD \u02dcV T\nwith \u02dcU, \u02dcD and \u02dcV having respectively size n\u00d7m, m\u00d7m and m\u00d7d, \u02dcUT \u02dcU = \u02dcV T \u02dcV = IdRm\nand \u02dcD diagonal with non-negative coefficients. This representation provides a re-\nduced SVD of A and one can create a full SVD from a reduced one by completing the\nmissing rows of \u02dcU and \u02dcV to form orthogonal matrices, and by adding the required\nnumber of zeros to \u02dcD.\n2.2\nThe trace inequality\nWe now descibe Von Neumann\u2019s trace theorem. Its justification follows the proof\ngiven in Mirsky [137].\nTheorem 2.1 (Von Neumann) Let A,B \u2208Mn,d(R) have singular values (\u03bb1,...,\u03bbm) and\n(\u00b51,...,\u00b5m), respectively, where m = min(n,d). Assume that these eigenvalues are listed\n2.2. THE TRACE INEQUALITY\n27\nin decreasing order so that \u03bb1 \u2265\u00b7\u00b7\u00b7 \u2265\u03bbm and \u00b51 \u2265\u00b7\u00b7\u00b7 \u2265\u00b5m. Then,\ntrace(AT B) \u2264\nm\nX\ni=1\n\u03bbi\u00b5i .\n(2.1)\nMoreover, if trace(AT B) = Pm\ni=1 \u03bbi\u00b5i, then there exist n \u00d7 n and d \u00d7 d orthogonal ma-\ntrices U and V such that UT AV and UT BV are both diagonal, i.e., one can find SVDs of\nA and B in the same bases of Rn and Rd.\nProof We can assume without loss of generality that d \u2264n because, if the result\nholds for A and B, it also holds for AT and BT . Let A = U1\u039bV T\n1 and B = U2MV T\n2 be\nthe singular values decompositions of A and B (both \u039b and M are n \u00d7 d matrices).\nThen\ntrace(AT B) = trace(V1\u039bT UT\n1 U2MV2) = trace(\u039bT UMV T )\nwith U = UT\n1 U2 and V = V T\n1 V2. Let u(i,j),1 \u2264i,j \u2264n and v(i,j),1 \u2264i,j \u2264d be the\ncoefficients of the orthogonal matrices U and V . Then\ntrace(\u039bT UMV T ) =\nd\nX\ni,j=1\nu(i,j)v(i,j)\u03bbi\u00b5j \u22641\n2\nd\nX\ni,j=1\n\u03bbi\u00b5ju(i,j)2 + 1\n2\nd\nX\ni,j=1\n\u03bbi\u00b5jv(i,j)2 (2.2)\nLet us consider the first sum in the upper-bound. Let \u03bed = \u03bbd (resp. \u03b7d = \u00b5d) and\n\u03bei = \u03bbi \u2212\u03bbi+1 (resp. \u03b7i = \u00b5i \u2212\u00b5i+1) for i = 1,...,d \u22121. Since singular values are non-\nincreasing, we have \u03bei,\u03b7i \u22650 and\n\u03bbi =\nd\nX\nj=i\n\u03bej,\n\u00b5i =\nd\nX\nj=i\n\u03b7j\nfor i = 1,...,d. We have\nd\nX\ni,j=1\n\u03bbi\u00b5ju(i,j)2 =\nd\nX\ni,j=1\nd\nX\ni\u2032=i\n\u03bei\u2032\nd\nX\nj\u2032=j\n\u03b7j\u2032u(i,j)2 =\nd\nX\ni\u2032,j\u2032=1\n\u03bei\u2032\u03b7j\u2032\ni\u2032\nX\ni=1\nj\u2032\nX\nj=1\nu(i,j)2\n\u2264\nd\nX\ni\u2032,j\u2032=1\n\u03bei\u2032\u03b7j\u2032 min(i\u2032,j\u2032)\n(2.3)\nwhere we used the fact that U is orthogonal, which implies that Pj\u2032\nj=1 u(i,j)2 and\nPi\u2032\ni=1 u(i,j)2 are both less than 1. Notice also that, when u(i,j) = \u03b4ij (i.e., u(i,j) = 1 if\ni = j and zero otherwise), then\ni\u2032\nX\ni=1\nj\u2032\nX\nj=1\nu(i,j)2 = min(i\u2032,j\u2032),\n28\nCHAPTER 2. A FEW RESULTS IN MATRIX ANALYSIS\nso that the last inequality is an identity, and the chain of equalities leading to (2.3)\nimplies\nd\nX\ni\u2032,j\u2032=1\n\u03bei\u2032\u03b7j\u2032 min(i\u2032,j\u2032) =\nd\nX\ni=1\n\u03bbi\u00b5j .\nWe therefore obtain (for any U), the fact that\nd\nX\ni,j=1\n\u03bbi\u00b5ju(i,j)2 \u2264\nd\nX\ni=1\n\u03bbi\u00b5j.\nThe same identity obviously holds with v in place of u, and combining the two yields\n(2.1).\nWe now consider conditions for equality. Clearly, if one can find SVD decompo-\nsitions of A and B with U1 = U2 and V1 = V2, then U = IdRn, V = IdRd and (2.1) is an\nidentity. We want to prove the converse statement.\nFor (2.1) to be an equality, we first need (2.2) to be an identity, which requires that\nu(i,j) = v(i,j) as soon as \u03bbi\u00b5j > 0. We also need an equality in (2.3), which requires\ni\u2032\nX\ni=1\nj\u2032\nX\nj=1\nu(i,j)2 = min(i\u2032,j\u2032)\nas soon as \u03bbi\u2032 > \u03bbi\u2032+1 and \u00b5j\u2032 > \u00b5j\u2032+1. The same identity must be true with v(i,j)\nreplacing u(i,j)\nIn view of this, denote by i1 < \u00b7\u00b7\u00b7 < ip (resp. j1 < \u00b7\u00b7\u00b7 < jq) the indexes at which\nthe singular values of A (resp. B) differ form their successors, with the convention\n\u03bbd+1 = \u00b5d+1 = 0. Let, for k = 1,...,p and l = 1,...,q\nC(k,l) =\nik\nX\ni=1\njl\nX\nj=1\nu(i,j)2.\nThen, we must have C(k,l) = min(ik,jl) for all k,l and u(i,j) = v(i,j) for i = 1,...,ip\nand j = 1,...,jq.\nIf, for all i,j \u2264d, we let U\u2308ij\u2309be the matrix formed by the first i rows and j\ncolumns of U, the condition Ckl = min(ik,jl) requires that U\u2308ikjl\u2309UT\n\u2308ikjl\u2309= IdRik if ik \u2264jl\nand UT\n\u2308ikjl\u2309U\u2308ikjl\u2309= IdRjl if jl \u2264ik. This shows that, if ik \u2264jl, the rows of U\u2308ikjl\u2309form an\northonormal family, and necessarily, all elements u(i,j) for i \u2264ik and j > jl vanish.\nThe symmetric situation holds if jl \u2264ik.\n2.2. THE TRACE INEQUALITY\n29\nLet rk = ik \u2212ik\u22121 and sl = jl \u2212jl\u22121 (with i0 = j0 = 0). We now consider possible\nchanges in the SVDs of A and B. With our notation, the matrix \u039b takes the form\n\u039b =\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\n\u03bbi1IdRr1\n0\n0\n\u00b7\u00b7\u00b7\n0\n0\n...\n0\n0\n\u03bbi2IdRr2\n0\n\u00b7\u00b7\u00b7\n0\n0\n...\n0\n...\n...\n...\n...\n...\n0\n0\n...\n\u03bbipIdRrp\n0\n0\n...\n0\n0\n...\n0\n0\n...\n0\n...\n...\n...\n...\n0\n...\n0\n0\n...\n0\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\nLet W, \u02dcW be n \u00d7 n and d \u00d7 d orthogonal matrices taking the form\nW =\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\nW1\n0\n0\n\u00b7\u00b7\u00b7\n0\n0\nW2\n0\n\u00b7\u00b7\u00b7\n0\n...\n...\n...\n0\n0\n...\nWp\n0\n0\n...\nWp+1\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\n, \u02dcW =\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\nW1\n0\n0\n\u00b7\u00b7\u00b7\n0\n0\nW2\n0\n\u00b7\u00b7\u00b7\n0\n...\n...\n...\n0\n0\n...\nWp\n0\n0\n...\n\u02dcWp+1\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\nwhere W1,...,Wp are orthogonal with respective sizes r1,...,rp, Wp+1 is orthogonal\nwith size n \u2212ip and \u02dcWp+1 is orthogonal with size d \u2212ip. Then we have\nWD \u02dcW = D\nproving that U1 can be replaced by U1W provided that V1 is replaced by V1 \u02dcW. Sim-\nilar transformations can be made on U1 and V2, with U2 replaced by U2Z and V2 by\nV2 \u02dcZ with\nZ =\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\nZ1\n0\n0\n\u00b7\u00b7\u00b7\n0\n0\nZ2\n0\n\u00b7\u00b7\u00b7\n0\n...\n...\n...\n0\n0\n...\nZq\n0\n0\n...\nZq+1\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\n,\n\u02dcZ =\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\nZ1\n0\n0\n\u00b7\u00b7\u00b7\n0\n0\nZ2\n0\n\u00b7\u00b7\u00b7\n0\n...\n...\n...\n0\n0\n...\nZq\n0\n0\n...\n\u02dcZq+1\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\nwith a structure similar to W and \u02dcW, replacing r1,...,rp by s1,...,sq. As a conse-\nquence, U = UT\n1 U2 can be replaced by W T UZ and V by \u02dcW T V \u02dcZ. To complete the\nproof, we need to show that, when (2.1) is an equality, these matrices can be chosen\nso that W T UZ = IdRn and \u02dcW T V \u02dcZ = IdRd.\nLet us consider a first step in this direction, assuming that i1 \u2264j1 so that\nU[i1j1]UT\n\u2308i1j1\u2309= IdRi1.\n30\nCHAPTER 2. A FEW RESULTS IN MATRIX ANALYSIS\nComplete UT\n\u2308i1j1\u2309into a orthogonal matrix Z1 = [UT\n\u2308i1j1\u2309, \u02dcU]. Build a matrix Z as above\nby taking Z2, . . . , Zq+1 equal to the identity. Then UZ has a first i1 \u00d7i1 block equal to\nIdRi1, which implies that all coefficients on the right and below this block are zeros.\nIf j1 \u2264i1, a similar construction can be made on the other side, letting W1 = [U\u2308i1j1\u2309\u02dcU]\nwith the first j1 \u00d7 j1 block of the new matrix U equal to the identity. Note that, since\nV\u2308ipjq\u2309= U\u2308ipjq\u2309, the same result is obtained on V at the same time.\nPursuing this way (and skipping the formal induction argument, which is a bit\ntedious), we can progressively introduce identity blocks into U and V and transform\nthem into new matrices (that we still denote by U and V ) taking the form (letting\nk = min(ip,jq))\nU =\n \nIdRk\n0\n0\n\u00afU\n!\nand V =\n \nIdRk\n0\n0\n\u00afV\n!\nIf k = ip (resp. k = jq), the final reduction can be obtained by choosing Wp+1 = \u00afU\nand \u02dcWp+1 = \u00afV (resp. Zp+1 = \u00afUT and \u02dcZp+1 = \u00afV T ), leading to SVDs for A and B with\nidentical matrices U1 = U2 and V1 = V2.\n\u25a0\nRemark 2.2 Note that, since the singular values of \u2212A and of A coincide, theorem 2.1\nimplies\n\f\f\ftrace(AT B)\n\f\f\f \u2264\nm\nX\ni=1\n\u03bbi\u00b5i .\n(2.4)\nfor all matrices A and B, with equality if either A and B or \u2212A and B have an SVD\nusing the same bases.\n\u2666\n2.3\nApplications\nLet p and d be integers with p \u2264d. Let A \u2208Sd(R), B \u2208Sp(R) be symmetric ma-\ntrices. We consider the following optimization problem: maximize, over matrices\nU \u2208Md,p(R) such that UT U = IdRp, the function\nF(U) = trace(UT AUB) = trace(AUBUT ).\nWe first note that the singular values of UBUT , which is d \u00d7 d, are the same as the\neigenvalues of B completed with zeros. Letting \u03bb1 \u2265\u00b7\u00b7\u00b7 \u2265\u03bbd be the eigenvalues of A\nand \u00b51 \u2265\u00b7\u00b7\u00b7 \u2265\u00b5p those of B, we therefore have, from theorem 2.1,\nF(U) \u2264\np\nX\ni=1\n\u03bbi\u00b5i.\n2.3. APPLICATIONS\n31\nIntroduce the eigenvalue decompositions of A and B in the form A = V \u039bV T and\nB = WMW T . For F(U) to be equal to its upper-bound, we know that we must arrange\nUBUT to take the form\nUBUT = V\n \nM\n0\n0\n0\n!\nV T .\nUse, as before, the notation V\u2308dp\u2309to denote the matrix formed with the p first columns\nof V . Take U = V\u2308dp\u2309W T , which satisfies UT U = IdRp. We then have\nV\u2308dp\u2309W T BWV T\n\u2308dp\u2309= V\u2308dp\u2309MV T\n\u2308dp\u2309= V\n \nM\n0\n0\n0.\n!\nV T ,\nwhich shows that U is optimal. We summarize this discussion in the next theorem.\nTheorem 2.3 Let A \u2208Sd(R) and B \u2208Sp(R) be symmetric matrices, with p \u2264d. Let\neigenvalue decompositions of A and B be given by A = V \u039bV T and B = WMW T , where\nthe diagonal elements of \u039b (resp. M) are \u03bb1 \u2265\u00b7\u00b7\u00b7 \u2265\u03bbd (resp. \u00b51 \u2265\u00b7\u00b7\u00b7 \u2265\u00b5p).\nDefine F(U) = trace(AUBUT ), for U \u2208Md,p(R). Then,\nmax\nn\nF(U) : UT U = IdRp\no\n=\np\nX\ni=1\n\u03bbi\u00b5i.\nThis maximum is attained at\nU = V\u2308d,p\u2309W T .\nThe following corollary applies theorem 2.3 with B = diag(\u00b51,...,\u00b5p).\nCorollary 2.4 Let A \u2208Sd(R) be a symmetric matrix with eigenvalues \u03bb1 \u2265\u00b7\u00b7\u00b7 \u2265\u03bbd. For\np \u2264d, let \u00b51 \u2265\u00b7\u00b7\u00b7 \u2265\u00b5p > 0 and define\nF(e1,...,ep) =\np\nX\ni=1\n\u00b5ieT\ni Aei.\nThen, the maximum of F over all orthonormal families e1,...,ep in Rd is Pp\ni=1 \u03bbi\u00b5i and is\nattained when e1,...,ep are eigenvectors of A with eigenvalues \u03bb1,...,\u03bbp.\nThe minimum of F over all orthonormal families e1,...,ep in Rd is Pp\ni=1 \u03bbd\u2212i+1\u00b5i and\nis attained when e1,...,ep are eigenvectors of A with eigenvalues \u03bbd,...,\u03bbd\u2212p+1.\nProof The statement about the maximum is just a special case of theorem 2.3, with\nB = diag(\u00b51,...,\u00b5p), noting that the ith diagonal element of UT AU is eT\ni Aei where ei\nis the ith column of U.\nThe statement about the minimum is deduced by replacing A by \u2212A.\n\u25a0\n32\nCHAPTER 2. A FEW RESULTS IN MATRIX ANALYSIS\nApplying this corollary with p = 1, we retrieve the elementary result that \u03bb1 =\nmax{uT Au : |u| = 1} and \u03bbd = min{uT Au : |u| = 1}.\nTo complete this chapter, we quickly state and prove Rayleigh\u2019s theorem.\nTheorem 2.5 Let A \u2208Md,d(R) be a symmetric matrix with eigenvalues \u03bb1 \u2265\u00b7\u00b7\u00b7 \u2265\u03bbd.\nThen\n\u03bbk =\nmax\nV :dim(V )=k min{uT Au,u \u2208V ,|u| = 1} =\nmin\nV :dim(V )=d\u2212k+1max{uT Au,u \u2208V ,|u| = 1}\nwhere the min and max are taken over linear subspaces of Rd.\nProof Let e1,...,ed be an orthonormal basis of eigenvectors of A associated with\n\u03bb1,...,\u03bbd. Let, for k \u2264l, Wk,l = span(ek,...,el). Let V be a subspace of dimension k.\nThen V \u2229Wk,d , \u2205(because the sum of the dimensions of these two spaces is d + 1).\nTaking u0 with norm 1 in this intersection, we have\nmin{uT Au,u \u2208V ,|u| = 1} \u2264uT\n0 Au0 \u2264max{uT Au,u \u2208Wk,d,|u| = 1} = \u03bbk,\nwhere the last identity follows by considering the eigenvalues of A restricted to Wk,d.\nSo, the maximum of the right-hand side is indeed less than \u03bbk, and it is attained for\nV = W1,k. This proves the first identity, and the second one can be obtained by\napplying the first one to \u2212A.\n\u25a0\n2.4\nSome matrix norms\nThe operator norm of a matrix A \u2208Mn,d(R), is defined as\n|A|op = max{|Ax| : x \u2208Rd,|x| = 1}.\nIt is equal to the square root of the largest eigenvalue of AT A, i.e., to the largest\nsingular value of A.\nThe Frobenius norm of A is\n|A|F =\nq\ntrace(AT A) =\nv\nu\nu\nt\nd\nX\ni,j=1\nA(i,j)2,\nso that\n|A|F =\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\nm\nX\nk=1\n\u03c32\nk\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\n1/2\n2.4. SOME MATRIX NORMS\n33\nwhere \u03c31,...,\u03c3m are the singular values of A (and m = min(n,d)).\nThe nuclear norm of A is defined by\n|A|\u2217=\nd\nX\nk=1\n\u03c3k .\nOne can prove that this is a norm using an equivalent definition, provided by the\nfollowing proposition.\nProposition 2.6 Let A be an n by d matrix. Then\n|A|\u2217= max\n\u001a\ntrace(UAV T ) : U \u2208Mn,n and UT U = Id,V \u2208Md,d and V T V = Id\n\u001b\n.\nProof The fact that trace(UAV T ) \u2264|A|\u2217for any U and V is a consequence of the\ntrace inequality applied with B = [Id,0] or its transpose depending on whether n \u2264d\nor not. The upper-bound being attained when U and V are the matrices forming the\nsingular value decomposition of A, the proof is complete.\n\u25a0\nThe fact that |A|\u2217is a norm, for which the only non-trivial fact was the triangular\ninequality, now is an easy consequence of this proposition, because the maximum\nof the sum of two functions is always less than the sum of their maximums. More\nprecisely, we have\n|A + B|\u2217= max{trace(UAV T ) + trace(UBV T ) :\nUT U = Id,V T V = Id}\n\u2264max{trace(UAV T ) : UT U = Id,V T V = Id}\n+ max{trace(UBV T ) : UT U = Id,V T V = Id}\n= |A|\u2217+ |B|\u2217\nThe nuclear norms is also called the Ky Fan norm of order d. Ky Fan norms of\norder k (for 1 \u2264k \u2264d) associate to a matrix A the quantity\n|A|(k) = \u03bb1 + \u00b7\u00b7\u00b7 + \u03bbk,\ni.e., the sum of its k largest singular values. One has the following proposition.\nProposition 2.7 The Ky Fan norms satisfy the triangular inequality.\nProof We prove this following the argument suggested in Bhatia [28]. For A \u2208Md,d,\nand k = 1,...,d, let trace(k)(A) be the sum of the k largest diagonal elements of A. Let,\n34\nCHAPTER 2. A FEW RESULTS IN MATRIX ANALYSIS\nfor a symmetric matrix A, |A|\u2032\n(k) denote the sum of the k largest eigenvalues of A (it\nis equal to |A|(k) if A is positive definite, but can also include negative values).\nThen, for any symmetric matrix A \u2208Sd,\n|A|\u2032\n(k) = max\nn\ntrace(k)(UAUT ) : U \u2208COd\no\n.\n(2.5)\nTo show this, assume that V in Od diagonalizes A, so that D = V AV T is a diagonal\nmatrix. Assume, without loss of generality, that the coefficients \u03bbj = D(j,j) are non-\nincreasing. Fix U \u2208Od, let B = UAUT and W = V UT so that D = WBW T , or B =\nW T DW. Then, for any j \u2264d,\nB(j,j) =\nd\nX\ni=1\nW(i,j)2D(i,i).\nThen, for any 1 \u2264j1 < \u00b7\u00b7\u00b7 < jk \u2264d\nk\nX\nl=1\nB(jl,jl) =\nd\nX\ni=1\nD(i,i)\nk\nX\nl=1\nW(i,jl)2\n=\nk\nX\ni=1\nD(i,i) +\nk\nX\ni=1\nD(i,i)\n\u0010\nk\nX\nl=1\nW(i,jl)2 \u22121\n\u0011\n+\nd\nX\ni=k+1\nD(i,i)\nk\nX\nl=1\nW(i,jl)2\n=\nk\nX\ni=1\nD(i,i) +\nk\nX\ni=1\n(D(i,i) \u2212D(k,k))\n\u0010\nk\nX\nl=1\nW(i,jl)2 \u22121\n\u0011\n+\nd\nX\ni=k+1\n(D(i,i) \u2212D(k,k))\nk\nX\nl=1\nW(i,jl)2 + D(k,k)\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\nn\nX\ni=1\nk\nX\nj=1\nW(i,jl)2 \u2212k\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8.\nBecause W is orthogonal, we have Pk\nl=1 W(i,jl)2 \u22641 and\nn\nX\ni=1\nk\nX\nj=1\nW(i,jl)2 = k.\nThis shows that the terms after Pk\ni=1 D(i,i) in the upper bound are negative or zero,\nso that\nk\nX\nl=1\nB(jl,jl) \u2264\nk\nX\ni=1\nD(i,i).\nThe maximum of the left-hand side is trace(k)(B). Noting that we get an equality\nwhen choosing U = V , the proof of (2.5) is complete.\n2.4. SOME MATRIX NORMS\n35\nUsing the same argument as that made above for the nuclear norm, one deduces\nfrom this that\n|A + B|\u2032\n(k) \u2264|A|\u2032\n(k) + |B|\u2032\n(k)\nfor all A,B \u2208Sd and all k = 1,...,d.\nNow, let A \u2208Mn,d and consider the symmetric matrix\n\u02dcA =\n \n0\nAT\nA\n0\n!\n\u2208Sn+d.\nWrite a vector u \u2208Rn+d as u =\n \nu1\nu2\n!\nwith u1 \u2208Rd and u2 \u2208Rn. Then u is an eigen-\nvector of \u02dcA for an eigenvalue \u03bb if and only if AT u2 = \u03bbu1 and Au1 = \u03bbu2, which\nimplies that AT Au1 = \u03bb2u1 and \u03bb2 is a singular value of A. Conversely, if \u00b5 is a\nnonzero singular value of A, associated with eigenvector u1, then 1/\u221a\u00b5 and \u22121/\u221a\u00b5\nare eigenvalues of \u02dcA, associated with eigenvectors\n \nu1\n\u00b1Au1/\u221a\u00b5\n!\n. It follows from this\nthat |A|(k) = | \u02dcA|\u2032\n(k) for k \u2264min(n,d) and therefore satisfies the triangle inequality.\n\u25a0\nWe refer to [28] for more examples of matrix norms, including, in particular those\nprovided by taking pth powers in Ky Fan\u2019s norms, defining\n|A|(k,p) = (\u03bbp\n1 + \u00b7\u00b7\u00b7 + \u03bbp\nk)1/p.\n36\nCHAPTER 2. A FEW RESULTS IN MATRIX ANALYSIS\nChapter 3\nIntroduction to Optimization\nThis chapter summarizes some fundamental concepts in optimization that will be\nused later in the book. The reader is referred to textbooks, such as Beck [22], Eiselt\net al. [68], Nocedal and Wright [146], Boyd et al. [40] and many others for proofs and\ndeeper results.\n3.1\nBasic Terminology\n1. If I is a subset of R, a lower bound of I is an element u \u2208[\u2212\u221e,+\u221e] such that u \u2264x\nfor all x \u2208I. Among these lower bounds, there exists a largest element, denoted\ninfI \u2208[\u2212\u221e,+\u221e], called the infimum of I (by convention, the infimum of an empty\nset is +\u221e). Similarly, one defines the supremum of I, denoted supI, as the smallest\nupper bound of I (and the supremum of an empty set is \u2212\u221e). Every set in R has an\ninfimum and a supremum, but these numbers do not necessarily belong to I. When\nthey do, they are respectively called minimal and maximal elements of I, and are\ndenoted minI and maxI. So, the statement \u201cu = minI\u201d means u \u2208I and u \u2264v for all\nv \u2208I.\n2. If F : \u2126\u2192R is a real-valued function defined on a subset \u2126\u2282Rd, the infimum\nof F over \u2126is defined by\ninf\n\u2126F = inf{F(x) : x \u2208\u2126}\nand its supremum is\nsup\n\u2126\nF = sup{F(x) : x \u2208\u2126}.\nAs seen above both numbers are well defined, and can take infinite values. One says\nthat x \u2208\u2126is a (global) minimizer (resp. maximizer) of F if F(y) \u2265F(x) (resp. F(y) \u2264\nF(x)) for all y \u2208\u2126. One also says that F reaches its minimum (resp. maximum), or is\nminimized (resp. maximized) at x. Equivalently, x is a minimizer (resp. maximizer)\nof F if and only if x \u2208\u2126and\nF(x) = min{F(y) : y \u2208\u2126} (resp. max{F(y) : y \u2208\u2126}).\n37\n38\nCHAPTER 3. INTRODUCTION TO OPTIMIZATION\nIn such cases, one also writes F(x) = min\u2126F or F(x) = max\u2126F. In particular, the\nnotation u = min\u2126F indicates that u = inf\u2126F and that there exists an x in \u2126such\nthat F(x) = u (i.e., that the infimum of F over \u2126is realized at some x \u2208\u2126). Note\nthat the infimum of a function always exists, but not necessarily its minimum. Also\nnote that minimizers, when they exist, are not necessarily unique. We will denote\nby argmin\u2126F (resp. argmax\u2126F) the (possibly empty) set of minimizers (resp. maxi-\nmizers) of F\n3. One says that x is a local minimizer (resp. maximizer) of F on \u2126if there exists an\nopen ball B \u2282Rd such that x \u2208B and F(x) = min\u2126\u2229B F (resp. F(x) = max\u2126\u2229B F).\n4. An optimization problem consists in finding a minimizer or maximizer of an \u201cob-\njective function\u201d F. Focusing from now on on minimization problems (statements\nfor maximization problems are symmetric), we will always implicitly assume that\na minimizer exists. The following provides some general assumptions on F and \u2126\nthat ensure this fact.\nThe sublevel sets of F in \u2126are denoted [F \u2264u]\u2126(or simply [F \u2264u] when \u2126= Rd)\nfor u \u2208[\u2212\u221e,+\u221e] with\n[F \u2264u]\u2126= {x \u2208\u2126: F(x) \u2264u}.\nNote that\nargmin\n\u2126\nF =\n\\\nu>infF\n[F \u2264u]\u2126.\nA typical requirement for F is that its sublevel sets are closed in Rd, which means\nthat, if a sequence (xn) in \u2126satisfies, for some u \u2208R, F(xn) \u2264u for all n and converges\nto a limit x, then x \u2208\u2126and F(x) \u2264u. If this is true, one says that F is lower semi-\ncontinuous, or l.s.c, on \u2126. If, in addition to being closed, the sublevel sets of F are\nbounded (at least for u small enough\u2014larger than infF), then argmin\u2126F is an inter-\nsection of nested compact sets, and is therefore not empty (so that the optimization\nproblem has at least one solution).\n5. Different assumptions on F and \u2126lead to different types of minimization prob-\nlems, with specific underlying theory and algorithms.\n1. If F is C1 or smoother and \u2126= Rd, one speaks of an unconstrained smooth\noptimization problem.\n2. For constrained problems, \u2126is often specified by a finite number of inequali-\nties, i.e.,\n\u2126= {x \u2208Rd : \u03b3i(x) \u22640,i = 1,...,q}.\nIf F and all functions \u03b31,...,\u03b3q are C1 one speaks of smooth constrained problems.\n3. If \u2126is a convex set (i.e., x,y \u2208\u2126\u21d2[x,y] \u2208\u2126, where [x,y] is the closed line\nsegment connecting x and y) and F is a convex function (i.e., F((1 \u2212t)x + ty) \u2264(1 \u2212\nt)F(x) + tF(y) for all x,y \u2208\u2126), one speaks of a convex optimization problem.\n3.2. UNCONSTRAINED OPTIMIZATION PROBLEMS\n39\n4. Non-smooth problems are often considered in data science, and lead to inter-\nesting algorithms and solutions.\n5. When both F and \u03b31,...,\u03b3q are affine functions, one speaks of a linear program-\nming problem (or a linear program). (An affine function is a mapping x 7\u2192bT x + \u03b2,\nb \u2208Rd, \u03b2 \u2208R.)\nIf F is quadratic (F(x) = 1\n2xT Ax \u2212bT x), and all \u03b3i\u2019s are affine, one speaks of a\nquadratic programming problem.\n6. Finally, some machine learning problems are specified over discrete or finite\nsets \u2126(for example Zd, or {0,1}d), leading to combinatorial optimization problems.\n3.2\nUnconstrained Optimization Problems\n3.2.1\nConditions for optimality (general case)\nConsider a function F : \u2126\u2192R where \u2126is an open subset of Rd. We first discuss the\nunconstrained optimization problem of finding\nx\u2217\u2208argmin\n\u2126\nF.\n(3.1)\nThe following result summarizes (non-identical) necessary and sufficient conditions\nthat are applicable to such a solution.\nTheorem 3.1 Necessary conditions. Assume that F is differentiable over \u2126, and that\nx\u2217is a local minimum of F. Then \u2207F(x\u2217) = 0.\nIf F is C2, then, in addition, \u22072F(x\u2217) must be positive semidefinite.\nSufficient conditions. Assume that F \u2208C2(\u2126). If x\u2217\u2208\u2126is such that \u2207F(x\u2217) = 0 and\n\u22072F(x\u2217) is positive definite, then x\u2217is a local minimum of F.\nProof Necessary conditions: Since \u2126is open, it contains an open ball centered at\nx\u2217, with radius \u03f50 and therefore all segments [x\u2217,x\u2217+\u03f5h] for all \u03f5 \u2208[0,\u03f50] and all unit\nnorm vectors h. Since x\u2217is a local minimum, we can choose \u03f50 so that F(x\u2217+ \u03f5h) \u2265\nF(x\u2217) for all h with |h| = 1.\nUsing Taylor formula, we get (for \u03f5 \u2208[0,\u03f50], |h| = 1)\n0 \u2264F(x\u2217+ \u03f5h) \u2212f (x\u2217) = \u03f5\nZ 1\n0\ndF(x\u2217+ t\u03f5h)hdt .\nIf dF(x\u2217)h , 0 for some h, then, for small enough \u03f5, dF(x\u2217+ t\u03f5h)h cannot change sign\nfor t \u2208[0,1] and therefore\nR 1\n0 dF(x\u2217+ t\u03f5h)hdt has the same sign as dF(x\u2217)(h) which\nmust therefore be positive. But the same argument can be made with h replaced by\n40\nCHAPTER 3. INTRODUCTION TO OPTIMIZATION\n\u2212h, implying that dF(x\u2217)(\u2212h) = \u2212dF(x\u2217)h is also positive, and this gives a contradic-\ntion. We therefore have dF(x\u2217)(h) = 0 for all h, i.e., \u2207F(x\u2217) = 0.\nAssume that F is C2. Then, making a second-order Taylor expansion, one gets\n0 \u2264F(x\u2217+ \u03f5h) \u2212F(x\u2217) = \u03f52\nZ 1\n0\n(1 \u2212t)d2F(x\u2217+ t\u03f5h)(h,h)dt.\nThe same argument as above shows that, if d2F(x\u2217)(h,h) , 0, then it must be posi-\ntive. This shows that d2F(x\u2217)(h,h) \u22650 for all h and d2F(x\u2217) (or its associated matrix\n\u22072F(x\u2217)) is positive semidefinite.\nNow, assume that F is C2 and \u22072F(x\u2217) positive definite. One still has\nF(x\u2217+ \u03f5h) \u2212F(x\u2217) = \u03f52\nZ 1\n0\n(1 \u2212t)d2F(x\u2217+ t\u03f5h)(h,h)dt\nIf \u22072F(x\u2217) \u227b0, then \u22072F(x\u2217+ t\u03f5h) \u227b0 for small enough \u03f5, showing the the r.h.s. of\nthe identity is positive for h , 0, and that F(x\u2217+ \u03f5h) > F(x\u2217).\n\u25a0\nBecause maximizing F is the same as minimizing \u2212F, necessary (resp. sufficient)\nconditions for optimality in maximization problems are immediately deduced from\nthe above: it suffices to replace positive semidefinite (resp. positive definite) by\nnegative semidefinite (resp. negative definite).\n3.2.2\nConvex sets and functions\nDefinition 3.2 One says that a set \u2126\u2282Rd is convex if and only if, for all x,y \u2208\u2126, the\nclosed segment [x,y] also belongs to \u2126.\nA function F : Rd \u2192(\u2212\u221e,+\u221e] is convex if, for all \u03bb \u2208[0,1] and all x,y \u2208Rd, one has\nF((1 \u2212\u03bb)x + \u03bby) \u2264(1 \u2212\u03bb)F(x) + \u03bbF(y).\n(3.2)\nIf, whenever the lower bound is not infinite, the inequality above is strict for \u03bb \u2208(0,1),\none says that F is strictly convex.\nNote that, with our definition, convex functions can take the value +\u221ebut not\n\u2212\u221e. In order for the upper-bound to make sense when F takes infinite values, one\nmakes the following convention: a + (+\u221e) = +\u221efor any a \u2208(\u2212\u221e,+\u221e]; \u03bb \u00b7 (+\u221e) = +\u221e\nfor any \u03bb > 0; 0 \u00b7 (+\u221e) is not defined but 0 \u00b7 (+\u221e) + (+\u221e) = +\u221e.\n3.2. UNCONSTRAINED OPTIMIZATION PROBLEMS\n41\nDefinition 3.3 The domain of F, denoted dom(F) is the set of x \u2208Rd such that F(x) < \u221e.\nOne says that F is proper if dom(F) , \u2205.\nWe will only consider proper convex functions in our discussions, which will simply\nbe referred to as convex functions for brevity.\nProposition 3.4 If F is a convex function, then dom(F) is a convex subset of Rd. Con-\nversely, if \u2126is a convex set and F satisfies (3.2) for all x,y \u2208\u2126(i.e., F is convex on \u2126),\nthen the extension \u02c6F defined by \u02c6F(x) = F(x) if x \u2208\u2126and \u02c6F(x) = +\u221eis a convex function\ndefined on Rd (such that dom( \u02c6F) = \u2126).\nProof The first statement is a direct consequence of (3.2), which implies that F is\nfinite on [x,y] as soon as it is finite at x and at y. For the second statement, (3.2) for\n\u02c6F is true for x,y \u2208\u2126, since it is true for F, and the uper-bound is +\u221eotherwise.\n\u25a0\nThis proposition shows that there was no real loss of generality in requiring convex\nfunctions to be defined on the full space Rd. Note also that the upper bound in (3.2)\nis infinite unless both x and y belong to dom(F), so that the inequality only needs to\nbe checked in that case.\nOne says that a function F is concave if and only if \u2212F is convex. All definitions\nand properties made for convex functions then easily transcribe into similar state-\nments for concave functions. We say that a function f : I \u2192(\u2212\u221e,+\u221e] (where I is an\ninterval) is non-decreasing if, for all x,y \u2208I, x < y implies f (x) \u2264f (y). We say that f\nis increasing if if, for all x,y \u2208I, x < y implies f (x) < f (y) if f (x) < \u221eand f (y) = \u221e\notherwise.\nInequality (3.2) has important consequences on minimization problems. For ex-\nample, it implies the following proposition.\nProposition 3.5 Let F be a convex (resp. strictly convex) function on Rd. If x \u2208dom(F)\nand y \u2208Rd, the function\n\u03bb \u2208(0,1] 7\u21921\n\u03bb(F((1 \u2212\u03bb)x + \u03bby) \u2212F(x))\n(3.3)\nis non-decreasing (resp. increasing).\nConversely, let \u2126\u2282Rd be a convex set and F : \u2126\u2192(\u2212\u221e,+\u221e) be a function such that\nthe expression in (3.3) is non-decreasing (resp. increasing) for all x \u2208dom(F) and y \u2208Rd.\nThen, the extension \u02c6F of F defined in proposition 3.4 is convex (resp. strictly convex).\nProof Let f (\u03bb) = (F((1 \u2212\u03bb)x + \u03bby) \u2212F(x))/\u03bb. Let \u00b5 \u2264\u03bb denote z\u03bb = (1 \u2212\u03bb)x + \u03bby,\nz\u00b5 = (1 \u2212\u00b5)x + \u00b5y. One has z\u00b5 = (1 \u2212\u03bd)x + \u03bdz\u03bb, with \u03bd = \u00b5/\u03bb, so that\nF(z\u00b5) \u2264(1 \u2212\u00b5/\u03bb)F(x) + (\u00b5/\u03bb)F(z\u03bb).\n42\nCHAPTER 3. INTRODUCTION TO OPTIMIZATION\nSubtracting F(x) to both sides (which is allowed since F(x) < \u221e) and dividing by \u00b5\nyields\nf (\u00b5) \u2264f (\u03bb).\nIf F is strictly convex, then, either F(z\u00b5) = \u221e, in which case f (\u00b5) = f (\u03bb) = \u221e, or\nF(z\u00b5) < (1 \u2212\u00b5/\u03bb)F(x) + (\u00b5/\u03bb)F(z\u03bb).\nas soon as 0 < \u00b5 < \u03bb, yielding\nf (\u00b5) < f (\u03bb).\nNow consider the converse statement. By comparing the expression in (3.3) to\nthat obtained with \u03bb = 1, we find, for all x,y \u2208\u2126\n1\n\u03bb(F((1 \u2212\u03bb)x + \u03bby) \u2212F(x)) \u2264F(y) \u2212f (x)\nwhich is (3.2). Since \u02c6F satisfies (3.2) in its domain, it is convex. If the function in\n(3.3) is increasing, then the inequality is strict for 0 < \u03bb < 1 as soon as the lower\nbound is finite, and F is strictly convex.\n\u25a0\nCorollary 3.6 If F is convex, any local minimum of F is a global minimum.\nProof If x is a local minimum of F, then, obviously, x \u2208dom(F), and for any y \u2208Rd\nand small enough \u00b5 > 0, F(x) \u2264F((1 \u2212\u00b5)x + \u00b5y). Using the function in (3.3) for \u03bb = \u00b5\nand for \u03bb = 1, we get\n0 \u22641\n\u00b5(F((1 \u2212\u00b5)x + \u00b5y) \u2212F(x)) \u2264F(y) \u2212F(x)\nso that x is a global minimum.\n\u25a0\n3.2.3\nRelative interior\nIf \u2126is convex, then \u02da\u2126and \u00af\u2126(its topological interior and closure) are convex too (the\neasy proof is left to the reader). However, topological interiors of interesting convex\nsets are often empty, and a more adapted notion of relative interior is preferable.\nDefine the affine hull of a set \u2126, denoted aff(\u2126), as the smallest affine subset of\nRd that contains \u2126. The vector space parallel to aff(\u2126) (generated by all differences\nx \u2212y, x,y \u2208\u2126) will be denoted \u2212\u2212\u2192\naff (\u2126). Their dimension k, is the largest integer such\nthat there exist x0,x1,...,xk \u2208\u2126such that x1 \u2212x0,...,xk \u2212x0 are linearly indepen-\ndent. Moreover, given these points, elements of the affine hull are defined through\nbarycentric coordinates, yielding\naff(\u2126) = {x = \u03bb(0)x0 + \u00b7\u00b7\u00b7 + \u03bb(k)xk :,\u03bb(0) + \u00b7\u00b7\u00b7 + \u03bb(k) = 1}.\n3.2. UNCONSTRAINED OPTIMIZATION PROBLEMS\n43\nThe coordinates (\u03bb(0),...,\u03bb(k)) are uniquely associated to x \u2208aff(\u2126) and depend con-\ntinuously on x. They are indeed obtained by solving the linear system\nx \u2212x0 = \u03bb(1)(x1 \u2212x0) + \u00b7\u00b7\u00b7 + \u03bb(k)(xk \u2212x0)\nwhich has a unique solution for x \u2208aff(\u2126) by linear independence. To see continuity,\none can introduce the k \u00d7 k matrix G with entries G(ij) given by the inner products\n(xi \u2212x0)T (xj \u2212x0) and the vector h(x) \u2208Rk with entries h(j)(x) = (x \u2212x0)T (xj \u2212x0).\nContinuity is then clear since \u03bb = G\u22121h(x).\nDefinition 3.7 If \u2126is a convex set, then its relative interior, denoted relint(\u2126), is the\nset of all x \u2208\u2126such that there exists \u03f5 > 0 such that aff(\u2126) \u2229B(x,\u03f5) \u2282\u2126.\nWe have the following important property.\nProposition 3.8 Let \u2126be a nonempty convex set. If x \u2208relint(\u2126) and y \u2208\u2126, then\nx\u03bb = (1 \u2212\u03bb)x + \u03bby \u2208relint(\u2126) for all \u03bb \u2208[0,1).\nMoreover relint(\u2126) is a nonempty convex set.\nProof Take \u03f5 such that B(x,\u03f5) \u2229aff(\u2126) \u2282\u2126. Take any z \u2208B(x\u03bb,(1 \u2212\u03bb)\u03f5) \u2229aff(\u2126).\nDefine \u02dcz such that z = (1 \u2212\u03bb)\u02dcz + \u03bby, i.e.\n\u02dcz = z \u2212\u03bby\n1 \u2212\u03bb .\nThen \u02dcz \u2208aff(\u2126) and\n|\u02dcz \u2212x| = |z \u2212x\u03bb|\n1 \u2212\u03bb < \u03f5\nso that \u02dcz, and therefore z belongs to \u2126. This proves that B(x\u03bb,(1 \u2212\u03bb)\u03f5) \u2229aff(\u2126) \u2282\u2126\nso that x\u03bb \u2208relint(\u2126).\nIf both x and y belong to relint(\u2126), then x\u03bb \u2208relint(\u2126) for \u03bb \u2208[0,1], showing that\nthis set is convex.\nWe now show that relint(\u2126) , \u2205. Let k be the dimension of aff(\u2126), so that there\nexist x0,x1,...,xk \u2208\u2126such that x1 \u2212x0,...,xk \u2212x0 are linearly independent. Consider\nthe \u201csimplex\u201d\nS = {\u03bb(0)x0 + \u00b7\u00b7\u00b7 + \u03bb(k)xk :,\u03bb(0) + \u00b7\u00b7\u00b7 + \u03bb(k) = 1,\u03bb(j) \u22650,j = 0,...,k},\nwhich is included in \u2126.\nThen the average x = (x0 + \u00b7\u00b7\u00b7 + xk)/(k + 1) is such that\nB(x,\u03f5) \u2229aff(\u2126) \u2282S for small enough \u03f5. Otherwise, there would exist a sequence\ny(n) = \u03bb(0)(n)x0 + \u00b7\u00b7\u00b7 + \u03bb(k)(n)xk such that \u03bb(0)(n) + \u00b7\u00b7\u00b7 + \u03bb(k)(n) = 1 and at least one\n\u03bb(j)(n) < 0 that converges to x. Let yj be the set of elements in this sequence such\n44\nCHAPTER 3. INTRODUCTION TO OPTIMIZATION\nthat \u03bb(j)(n) < 0. This set is infinite for at least one j and provides a subsequence of\ny that also converges to x. But this would imply that the jth barycentric coordinate,\nwhich depends continuously on x, is non-positive, which is a contradiction.\nWe therefore have x \u2208relint(\u2126), which completes the proof.\n\u25a0\nThe following proposition provides an equivalent definition of the relative inte-\nrior.\nProposition 3.9 If \u2126is a convex set, then\nrelint(\u2126) = {x \u2208\u2126: \u2200y \u2208\u2126,\u2203\u03f5 > 0 such that x \u2212\u03f5(y \u2212x) \u2208\u2126}.\n(3.4)\nSo x belongs in the relative interior of \u2126if, for all y \u2208\u2126, the segment [x,y] can be\nextended on the x side and still remain included in \u2126.\nProof Let A be the set in the r.h.s. of (3.4). The proof that relint(\u2126) \u2282A is straight-\nforward and left to the reader. We consider the reverse inclusion.\nLet x \u2208A, and let y \u2208relint(\u2126), which is not empty. Then, for some \u03f5 > 0, we have\nz = x \u2212\u03f5(y \u2212x) \u2208\u2126.\nSince\nx =\n1\n1 + \u03f5(\u03f5y + z),\nproposition 3.8 implies that x \u2208relint(\u2126).\n\u25a0\nConvex functions have important regularity properties in the relative interior of\ntheir domain, that we will denote ridom(F). Importantly:\nridom(F) = relint(dom(F)) , int(dom(F)).\nA first such property is provided by the next proposition.\nProposition 3.10 Let F be a convex function. Then F is locally Lipschitz continuous on\nridom(F), i.e., for every compact subset C \u2282ridom(F), there exists a constant L > 0 such\nthat |F(x) \u2212F(y)| \u2264L|x \u2212y| for all x,y \u2208C.\nThis implies, in particular, that F is continuous on ridom(F).\nProof Take x \u2208ridom(F). Let K =\n\u001a\nh \u2208\u2212\u2212\u2192\naff (dom(F)),|h| = 1\n\u001b\n. Then, the segment\n[x \u2212ah,x + ah] is included in ridom(F) for small enough a and all h \u2208K. Since F is\nconvex, we have, for t \u2264a,\nF(x + th) \u2212F(x) \u2264t\na(F(x + ah) \u2212F(x))\n3.2. UNCONSTRAINED OPTIMIZATION PROBLEMS\n45\nWriting x = \u03bb(x \u2212ah) + (1 \u2212\u03bb)(x + th) with \u03bb = t/(t + a), we also have\nF(x) \u2264\nt\nt + a(F(x \u2212ah) +\na\nt + aF(x + th))\nwhich can be rewritten as\nF(x) \u2212F(x + th) \u2264t\na(F(x \u2212ah) \u2212F(x)).\nThese two inequalities show that F is continuous at x along any direction in \u2212\u2212\u2192\naff (dom(F)),\nwhich implies that F is continuous at x. Given this, the differences F(x+ah)\u2212F(x) are\nbounded over the compact set C, by some constant M and, the previous inequalities\nshow that\n|F(y) \u2212F(x)| \u2264M\na |x \u2212y|\nif y \u2208ridom(F), |y \u2212x| \u2264a.\n\u25a0\n3.2.4\nDerivatives of convex functions and optimality conditions\nThe following theorem provides a stronger version of optimality conditions for the\nminimization of differentiable convex functions. Note that we have only defined\ndifferentiability of functions defined over open sets.\nTheorem 3.11 Let F be a convex function, with int(dom(F)) , \u2205. Assume that x \u2208\nint(dom(F)) and that F is differentiable at x. Then, for all y \u2208Rd:\n\u2207F(x)T (y \u2212x) \u2264F(y) \u2212F(x).\n(3.5)\nIf F is strictly convex, the inequality is strict for y , x. In particular, \u2207F(x) = 0 implies\nthat x is a global minimizer of F. It is the unique minimizer if F is strictly convex.\nConversely, if F is C1 on an open convex set \u2126and satisfies (3.5) for all x,y \u2208\u2126, then\nF is convex.\nProof Equation (3.3) implies\n1\n\u03bb(F((1 \u2212\u03bb)x + \u03bby) \u2212F(x)) \u2264F(y) \u2212F(x),0 < \u03bb \u22641.\nTaking the limit of the lower bound for \u03bb \u21920, \u03bb > 0 yields (3.5). If F is strictly\nconvex, the inequality is strict for \u03bb < 1 and, since the l.h.s. is increasing in \u03bb, it\nremains strict when \u03bb \u21930.\nConversely, assuming (3.5) for all x,y \u2208\u2126, the derivative of \u03bb 7\u21921\n\u03bb(F((1 \u2212\u03bb)x +\n\u03bby) \u2212F(x)) is\n1\n\u03bb2(\u03bb\u2207F(x + \u03bbh)T h \u2212F(x + \u03bbh) + F(x))\nwith h = y \u2212x, which is non-negative by (3.5). This proves that F is convex. If (3.5)\nholds with a strict inequality, then the derivative is positive and 1\n\u03bb(F((1 \u2212\u03bb)x + \u03bby) \u2212\nF(x)) is increasing.\n\u25a0\n46\nCHAPTER 3. INTRODUCTION TO OPTIMIZATION\nThe next proposition describes C2 convex functions in terms of their second\nderivatives.\nProposition 3.12 Let F be convex and twice differentiable at x \u2208int(dom(F)). Then\n\u22072F(x) is positive semi-definite.\nConversely, assume that \u2126= dom(F) is an open set and that F is C2 on \u2126with a\npositive semi-definite second derivative. Then F (or, rather, its extension \u02c6F) is convex. If\nthe second derivative is everywhere positive definite, then F is strictly convex.\nProof Using Taylor formula (1.10) at order 2, we get, for any h \u2208Rd with |h| = 1,\n1\n2d2F(x)(h,h) = 1\n2t2d2F(x)(th,th) = 1\nt2(F(x + th) \u2212F(x) \u2212t\u2207F(x)T h) + \u03f5(t) \u2265\u03f5(t)\nwith \u03f5(t) \u21920 when t \u21920, the last inequality deriving from (3.5). This shows that\nd2F(x)(h,h) \u22650.\nTo prove the second statement, assume that F is C2 and \u22072F is positive semi-\ndefinite everywhere. Then (1.8) implies\nF(y) \u2212F(x) \u2212\u2207F(x)T (y \u2212x) = 1\n2(y \u2212x)T \u22072F(z)(y \u2212x)\nfor some z \u2208[x,y]. Since the r.h.s. is non-negative, (3.5) holds. If \u22072F is positive\ndefinite everywhere, then the r.h.s. is positive if y , x and (3.5) holds with a strict\ninequality.\n\u25a0\nIf F is C2 and \u22072F is positive definite and strictly convex, then (1.8) implies that,\nfor some z \u2208[x,y],\nF(y) \u2212F(x) \u2212\u2207F(x)T (y \u2212x) = 1\n2(y \u2212x)T \u22072F(z)(y \u2212x) \u2265\u03c1min(\u22072F(z))\n2\n|y \u2212x|2\nwhere \u03c1min(A) denotes the smallest eigenvalue of A. If this smallest eigenvalue is\nbounded from below away from zero, there exists a constant m > 0 such that\nF(y) \u2212F(x) \u2212\u2207F(x)T (y \u2212x) \u2212m\n2 |y \u2212x|2 \u22650.\n(3.6)\nThis property is captured by the following definition, which does not require F to be\nC2.\nDefinition 3.13 A C1 function F is strongly convex if\n1. int(dom(F)) , \u2205\n2. There exists m > 0 such that (3.6) holds for all x \u2208int(dom(F)) and y \u2208Rd.\n3.2. UNCONSTRAINED OPTIMIZATION PROBLEMS\n47\nWe have the following proposition.\nProposition 3.14 If F is strongly convex, then it is strictly convex, so that, in particular\nargminF has at most one element.\nIf dom(F) = Rd, then argminF is not empty.\nProof The first part is a direct consequence of (3.6) and theorem 3.11.\nFor the second part, (3.6) implies that\nF(x) \u2212F(0) \u2265\u2207F(0)T x + m\n2 |x|2 \u2265|x|\n\u0012m\n2 |x|2 \u2212|\u2207F(0)|\n\u0013\nThis shows that F(x) > F(0) if |x| > 2|\u2207F(0)|/m := r so that\nargminF = argmin\n\u00afB(0,r)\nF.\nThe set in the r.h.s. involves the minimization of a continuous function on a compact\nset, and is therefore not empty.\n\u25a0\nWe will use the following definition.\nDefinition 3.15 A function F : \u2126\u2192Rm is L-Ck, L being a positive number, if it is Ck\nand\n|dkF(x) \u2212dkF(y)| \u2264L|x \u2212y|.\nIf F is L-Ck, then Taylor formula ((1.9)) implies\n\f\f\f\f\ff (x + h) \u2212f (x) \u2212df (x)h \u22121\n2d2f (x)(h,h) \u2212\u00b7\u00b7\u00b7 \u22121\nk!dkf (x)(h,...,h)\n\f\f\f\f\f \u2264L|h|k+1\n(k + 1)!\n(3.7)\nfor which we used the fact that\nZ 1\n0\nt(1 \u2212t)k\u22121dt =\nZ 1\n0\n(1 \u2212t)k\u22121dt \u2212\nZ 1\n0\n(1 \u2212t)kdt = 1\nk \u2212\n1\nk + 1 =\n1\nk(k + 1).\nIf F is strongly convex and is, in addition, L-C1 for some L, then using (3.7), one\ngets the double inequality, for all x,y \u2208int(dom(F)):\nm\n2 |y \u2212x|2 \u2264F(y) \u2212F(x) \u2212\u2207F(x)T (y \u2212x) \u2264L\n2|y \u2212x|2.\n(3.8)\nThe following proposition will be used later.\n48\nCHAPTER 3. INTRODUCTION TO OPTIMIZATION\nProposition 3.16 Assume that F is strongly convex, satisfying (3.6), and that argminF =\n{x\u2217} with x\u2217\u2208int(dom(F)). Then, for all x \u2208int(dom(F)):\nm\n2 |x \u2212x\u2217|2 \u2264F(x) \u2212F(x\u2217) \u22641\n2m|\u2207F(x)|2\n(3.9)\nProof Since \u2207F(x\u2217) = 0, the first inequality is a consequence of (3.6) applied to x =\nx\u2217. Switching the role of x and x\u2217, we have\nF(x\u2217) \u2212F(x) \u2212\u2207F(x)T (x\u2217\u2212x) \u2265m\n2 |x \u2212x\u2217|2\nso that\n0 \u2264F(x) \u2212F(x\u2217) \u2264\u2212\u2207F(x)T (x\u2217\u2212x) \u2212m\n2 |x \u2212x\u2217|2 \u2264|\u2207F(x)||x \u2212x\u2217| \u2212m\n2 |x \u2212x\u2217|2\n(3.10)\nThe maximum of the r.h.s. with respect to |x \u2212x\u2217| is attained at |\u2207F(x)|/m, showing\nthat\nF(x) \u2212F(x\u2217) \u22641\n2m|\u2207F(x)|2,\nwhich is the second inequality.\n\u25a0\n3.2.5\nDirection of descent and steepest descent\nGradient-based algorithms for optimization iteratively update the variable x, creat-\ning a sequence governed by an equation taking the form xt+1 = xt + \u03b1tht with \u03b1t > 0\nand ht \u2208Rd. To ensure that the objective function F decreases at each step, ht is cho-\nsen to be a direction of descent for F at xt, a notion which, as seen below, is closely\nconnected with the direction of \u2207F(xt).\nDefinition 3.17 Let \u2126be open in Rd and F : \u2126\u2192R be a C1 function. A direction\nof descent for F at x \u2208\u2126is a vector h , 0 \u2208Rd such that there exists \u03f50 > 0 such that\nF(x + \u03f5h) < F(x) for all \u03f5 \u2208(0,\u03f50].\nProposition 3.18 Assume that F : \u2126\u2192R is C1 and take x \u2208\u2126. Then any direction h\nsuch that hT \u2207F(x) < 0 is a direction of descent for F at x. Conversely, if h is a direction of\ndescent, then hT \u2207F(x) \u22640.\nProof We have the first-order expansion F(x+\u03f5h)\u2212F(x) = \u03f5hT \u2207F(x)+o(\u03f5). If hT \u2207F(x) <\n0, the r.h.s. is negative for small enough \u03f5 and h is a direction of descent. Similarly,\nif hT \u2207F(x) > 0, the r.h.s. is positive for small enough \u03f5 and h cannot be a direction of\ndescent.\n\u25a0\n3.2. UNCONSTRAINED OPTIMIZATION PROBLEMS\n49\nIn particular, h = \u2212\u2207F(x) is always a direction of descent. It is called the steep-\nest descent direction because it minimizes h 7\u2192\u2202\u03b1F(x + \u03b1h)|\u03b1=0 over all h such that\n|h|2 = 1. However, this designation has a character of optimality that may be mis-\nleading, because using the Euclidean norm for the condition |h|2 = 1 is not neces-\nsarily adapted to the optimization problem at hand. In the absence of additional\ninformation on the problem, it does have a canonical nature, as it is (up to rescaling)\nthe only norm invariant to rotations (including permutations) of the coordinates.\nSuch invariance is not necessarily desirable when the variable x has a known struc-\nture (e.g., it is organized on a graph) which would be broken by permutation. Also,\nsteepest refers to a local \u201cgreedy\u201d evaluation, but may not be optimal from a global\nperspective. A simple example to illustrate this is the case of a quadratic function\nF(x) = 1\n2xT Ax \u2212bT x\nwhere A \u2208S++\nn\nis a positive definite symmetric matrix. Then \u2207F(x) = Ax \u2212b, but one\nmay argue that \u2207AF(x) = A\u22121\u2207F(x) (defined in (1.3)) is a better choice, because it\nallows the algorithm to reach the minimizer of F in one step, since x\u2212\u2207AF(x) = A\u22121b\n(this statement disregards the cost associated in solving the system Ax = b, which\ncan be an important factor in large dimension). Importantly, if F is any C1 function,\nand A \u2208S++\nn , the minimizer of h 7\u2192\u2202\u03b1F(x + \u03b1h)|\u03b1=0 over all h such that hT Ah = 1 is\ngiven by \u2212\u2207AF(x), i.e., \u2212\u2207AF(x) is the steepest descent for the norm associated with\nA. This yields a general version of steepest descent methods, iterating\nxt+1 = xt \u2212\u03b1t\u2207AtF(xt)\nwith \u03b1t > 0 and At \u2208S++\nn .\nOne can also notice that \u2207AF(x) is also a minimizer of\nF(x) + \u2207F(x)T h + 1\n2hT Ah.\nWhen \u22072F(x) is positive definite, it is then natural to choose it as the matrix A, there-\nfore taking h = \u2212\u22072F(x)\u22121\u2207F(x). This provides Newton\u2019s method for optimization.\nHowever, Newton method requires computing second derivatives of F, which can be\ncomputationally costly. It is, moreover, not a gradient-based method, which is the\nfocus of this discussion.\n3.2.6\nConvergence\nWe now consider a descent algorithm\nxt+1 = xt + \u03b1tht\n(3.11)\n50\nCHAPTER 3. INTRODUCTION TO OPTIMIZATION\nwhere ht is a direction of descent at xt for the objective function F. To ensure con-\nvergence, suitable choices for the direction of descent and the step must be made at\neach iteration, and some assumptions on the objective function are needed.\nRegarding the direction of descent, which must satisfy hT\nk \u2207F(xk) \u22640, we will as-\nsume a uniform control away from orthogonality to the gradient, with the condition\n\u2212hT\nt \u2207F(xt) \u2265\u03f5|ht||\u2207F(xt)|\n(3.12a)\nfor some fixed \u03f5 > 0. Without loss of generality (given that a multiplicative step \u03b1t\nmust also be chosen), we assume that ht is commensurable to the gradient, namely,\nthat\n\u03b31|\u2207F(xt)| \u2264|ht| \u2264\u03b32|\u2207F(xt)|\n(3.12b)\nfor fixed 0 < \u03b31 \u2264\u03b32. If ht = \u2207AtF, these assumptions are satisfied as soon as the\nsmallest and largest eigenvalues of At are controlled along the trajectory.\nWe have the following proposition.\nProposition 3.19 Assume that F is L-C1. Assume that xt satisfies (3.11) and that (3.12a)\nand (3.12b) hold. Then, there exist constants \u00af\u03b1 > 0 and C > 0 that depends on \u03b31,\u03b32 and\n\u03f5, such that, for \u03b1t \u2264\u00af\u03b1, one has\nF(xt+1) \u2212F(xt) \u2264\u2212C\u03b1t|\u2207F(xt)|2.\n(3.13)\nProof Applying (3.7) to xt and xt+1, we get\nF(xt+1) \u2212F(xt) \u2212\u03b1t\u2207F(xt)T ht \u2264L\n2\u03b12\nt |ht|2\nUsing (3.11) and (3.12a), this gives\nF(xt+1) \u2212F(xt) + \u03b1t\u03f5\u03b31|\u2207F(xt)|2 \u2264L\n2\u03b12\nt \u03b32\n2|\u2207F(xt)|2\nso that\nF(xt+1) \u2212F(xt) \u2264\u2212\u03b1t\n\u0010\n\u03f5\u03b31 \u2212\u03b1t\u03b32\n2L/2\n\u0011\n|\u2207F(xt)|2.\nIt suffices to take \u00af\u03b1 = \u03f5\u03b31/L\u03b32\n2 and C = \u03f5\u03b31/2 to obtain (3.13).\n\u25a0\nIterating (3.13) for t = 1 to t = T \u22121 yields\nT\nX\nt=1\n\u03b1t|\u2207F(xt)|2 \u22641\nC (F(x1) \u2212F(xT )).\nIf F is bounded from below, and one takes \u03b1t = \u00af\u03b1 for all t, one deduces that\nmin\nn\n|\u2207F(xt)|2 : t = 1,...,T\no\n\u2264F(x1) \u2212infF\nCT \u00af\u03b1\n.\n3.2. UNCONSTRAINED OPTIMIZATION PROBLEMS\n51\nWe can deduce from this, for example, that there exists a sequence t1 < \u00b7\u00b7\u00b7 < tn < \u00b7\u00b7\u00b7\nsuch that \u2207F(xtk) \u21920 when k \u2192\u221e. In particular, if one runs (3.11) until |\u2207F(xt)| is\nsmaller than a given tolerance level (which is standard), the procedure is guaranteed\nto terminate in a finite number of steps.\nStronger results may be obtained under stronger assumptions on F and on the\nalgorithm. The first assumption is an inequality similar to (3.13) and requires that,\nfor some constant C > 0,\nF(xt+1) \u2212F(xt) \u2264\u2212C|\u2207F(xt)|2.\n(3.14)\nSuch an inequality can be deduced from (3.13) under the additional assumption that\n\u03b1t is bounded from below and we will discuss later line search strategies that ensure\nits validity. The second assumption is that F is convex.\nTheorem 3.20 Assume that F is convex and finite and that its sub-level set [F \u2264F(x0)]\nis bounded. Assume that argminF is not empty and let x\u2217be a minimizer of F. If (3.14)\nis true, then\nF(xt) \u2212F(x\u2217) \u2264\nR2\nC(t + 1)\nwith R = max{|x \u2212x\u2217| : F(x) \u2264F(x0)}.\nProof Note that the algorithm never leaves [F \u2264F(x0)]. We have\nF(xt+1) \u2212F(x\u2217) \u2264F(xt) \u2212F(x\u2217) \u2212C|\u2207F(xt)|2.\nMoreover, by convexity, F(x\u2217) \u2212F(xt) \u2265\u2207F(xt)T (x\u2217\u2212xt), so that\nF(xt) \u2212F(x\u2217) \u2264\u2207F(xt)T (xt \u2212x\u2217) \u2264|\u2207F(xt)|R.\nCombining these two inequalities, we get\nF(xt+1) \u2212F(x\u2217) \u2264F(xt) \u2212F(x\u2217) \u2212C\nR2(F(xt) \u2212F(x\u2217))2.\nIntroducing \u03b4t = (C/R2)(F(xt) \u2212F(x\u2217)), this inequality implies\n\u03b4t+1 \u2264\u03b4t(1 \u2212\u03b4t).\nTaking inverses, we get\n1\n\u03b4t+1\n\u22651\n\u03b4t\n+\n1\n1 \u2212\u03b4t\n\u22651\n\u03b4t\n+ 1\nwhich implies 1\n\u03b4t \u2265t + 1 or \u03b4t \u22641/(t + 1), which in turn implies the statement of the\ntheorem.\n\u25a0\n52\nCHAPTER 3. INTRODUCTION TO OPTIMIZATION\nA faster convergence rate can be obtained if F is assumed to be strongly convex.\nIndeed, if (3.6) and (3.14) are satisfied, then (using proposition 3.16),\nF(xt+1) \u2212F(x\u2217) \u2264F(xt) \u2212F(x\u2217) \u2212C|\u2207F(xt)|2\n\u2264F(xt) \u2212F(x\u2217) \u22122Cm(F(xt) \u2212F(x\u2217))\n= (1 \u22122Cm)(F(xt) \u2212F(x\u2217)).\nWe therefore get the proposition:\nProposition 3.21 If F is finite and satisfies (3.6), and if the descent algorithm satisfies\n(3.14), then\nF(xt) \u2212F(x\u2217) \u2264(1 \u22122Cm)t(F(x0) \u2212F(x\u2217)).\n3.2.7\nLine search\nProposition 3.19 states that, to ensure that (3.14) holds, it suffices to take a small\nenough step parameter \u03b1. However, the values of \u03b1 that are acceptable depend on\nproperties of the objective function that are rarely known in practice. Moreover,\neven if a valid choice is determined (this can sometimes be done in practice by trial\nand error), setting a fixed value of \u03b1 for the whole algorithm is often too conserva-\ntive, as the best \u03b1 when starting the algorithm may be different from the best one\nclose to convergence.\nFor this reason, most gradient descent procedures select a parameter \u03b1t at each\nstep using a line search. Given a current position and direction of descent h, a line\nsearch explores the values of F(x + \u03b1h), \u03b1 \u2208(0,\u03b1max] in order to discover some \u03b1\u2217\nthat satisfies some desirable properties. We will assume in the following that x and\nh satisfy (3.12a) and (3.12b) for fixed \u03f5,\u03b31,\u03b32.\nOne possible strategy is to define \u03b1\u2217as a minimizer of the scalar function\nfh(\u03b1) = F(x + \u03b1h)\nover (0,\u03b1max] for a given upper-bound \u03f5max. This can be implemented using, e.g.,\nbinary or ternary search algorithms, but such algorithms would typically require a\nlarge number of number of evaluations of the function F, and would be too costly to\nbe run at each iteration of a gradient descent procedure.\nBased on the previous convergence study, we should be happy with a line search\nprocedure that ensures that (3.14) is satisfied for some fixed value of the constant C.\nOne such condition is the so-called Armijo rule that requires (with a fixed, typically\nsmall, value of c1 > 0):\nfh(\u03b1) \u2264fh(0) + c1\u03b1hT \u2207f (x).\n(3.15)\n3.2. UNCONSTRAINED OPTIMIZATION PROBLEMS\n53\nWe know that, under the assumptions of proposition 3.19, this condition can always\nbe satisfied with a small enough value of \u03b1. Such a value can be determined using a\n\u201cbacktracking procedure,\u201d which, given \u03b1max and \u03c1 \u2208(0,1), takes \u03b1 = \u03c1k\u03b1max where\nk is the smallest integer such that (3.15) is satisfied. This value of k is then deter-\nmined iteratively, trying \u03b1max, \u03c1\u03b1max, \u03c12\u03b1max,... until (3.15) is true (this provides\nthe \u201cbacktracking method\u201d).\nA stronger requirement in the line search is to ensure that \u2202fh(\u03b1) is not \u201ctoo\nnegative\u201d since one would otherwise be able to further reduce fh by taking a larger\nvalue of \u03b1. This leads to the weak Wolfe conditions, which combine the Armijo\u2019s\nrule in (3.15) and\n\u2202fh(\u03b1) = hT \u2207F(x + \u03b1h) \u2265c2hT \u2207F(x)\n(3.16a)\nfor some constant c2 \u2208(c1,1). The strong Wolfe conditions require (3.15) and\n|hT \u2207F(x + \u03b1h)| \u2264c2|hT \u2207F(x)|.\n(3.16b)\n(Since h is a direction of descent, (3.16b) requires (3.16a) and the fact that hT \u2207F(x +\n\u03b1h) does not take too large positive values.) If F is L-C1, these conditions, with\n(3.12a) and (3.12b), imply (3.14). Indeed, (3.16a) and the L-C1 condition imply\n\u2212(1 \u2212c2)hT \u2207F(x) \u2264hT (\u2207F(x + \u03b1h) \u2212\u2207F(x)) \u2264L\u03b1|h|2\nand (3.12a) and (3.12b) give\n(1 \u2212c2)\u03f5|\u2207F(x)|2 \u2264\u03b1L\u03b32\n2|\u2207F(x)|2\nshowing that \u03b1 \u2265(1 \u2212c2)\u03f5/(L\u03b32\n2). Moreover\nF(x + \u03b1h) \u2264F(x) + c1\u03b1hT \u2207f (x) \u2264F(x) \u2212c1\u03b1\u03f5|\u2207F(x)|2\nso that\nF(x + \u03b1h) \u2264F(x) \u2212c1(1 \u2212c2)\u03f52\nL\u03b32\n2\n|\u2207F(x)|2.\nWe have just proved the following proposition.\nProposition 3.22 Assume that F is L-C1 and that (3.12a), (3.12b), (3.15) and (3.16a)\nare satisfied. Then there exists C > 0, depending only of L,\u03f5,\u03b32,c1 and c2such that\nF(x + \u03b1h) \u2264F(x) \u2212C|\u2207F(x)|2.\nThe Wolfe conditions can always be satisfied by some \u03b1 as soon as F is C1 and\nbounded from below, and hT \u2207F(x) < 0. The next proposition shows this result for\nthe weak condition, while providing an algorithm finding an \u03b1 that satisfies it in a\nfinite number of steps.\n54\nCHAPTER 3. INTRODUCTION TO OPTIMIZATION\nProposition 3.23 Let f : \u03b1 7\u2192f (\u03b1) be a C1 function defined on [0,+\u221e) such that f is\nbounded from below and \u2202\u03b1f (0) < 0. Let 0 < c1 < c2 < 1.\nLet \u03b10,0 = \u03b10,1 = 0 and \u03b10 > 0. Define recursively sequences \u03b1n,0,\u03b1n,1 and \u03b1n as\nfollows.\n(i) If f (\u03b1n) \u2264f (0) + c1\u03b1n\u2202\u03b1f (0) and \u2202f (\u03b1n) \u2265c2\u2202\u03b1f (0) stop the construction.\n(ii) If f (\u03b1n) > f (0) + c1\u03b1n\u2202\u03b1f (0) let \u03b1n+1 = (\u03b1n + \u03b1n,0)/2, \u03b1n+1,1 = \u03b1n and \u03b1n+1,0 = \u03b1n,0.\n(iii) If f (\u03b1n) \u2264f (0) + c1\u03b1n\u2202\u03b1f (0) and \u2202f (\u03b1n) < c2\u2202\u03b1f (0):\n(a) If \u03b1n,1 = 0, let \u03b1n+1 = 2\u03b1n, \u03b1n+1,0 = \u03b1n and \u03b1n+1,1 = \u03b1n,1.\n(b) If \u03b1n,1 > 0, let \u03b1n+1 = (\u03b1n + \u03b1n,1)/2, \u03b1n+1,0 = \u03b1n and \u03b1n+1,1 = \u03b1n,1.\nThen the sequences are always finite, i.e., the algorithm terminates in a finite number of\nsteps.\nProof Assume, to get a contradiction, that the algorithm runs indefinitely, so that\ncase (i) never occurs. If case (ii) never occurs, then one runs step (iii-a) indefinitely,\nso that \u03b1n \u2192\u221ewith f (\u03b1n) \u2264f (0) + c1\u03b1n\u2202\u03b1f (0), and f cannot be bounded from\nbelow, yielding a contradiction. As soon as case (ii) occurs, we have, at every step,\n\u03b1n,0 \u2265\u03b1n\u22121,0, \u03b1n,1 \u2264\u03b1n\u22121,1, \u03b1n \u2208[\u03b1n,0,\u03b1n,1], f (\u03b1n,1) > f (0) + c1\u03b1n,1\u2202\u03b1f (0), f (\u03b1n,0) \u2264\nf (0) + c1\u03b1n,0\u2202\u03b1f (0) and \u2202f (\u03b1n,0) < c2\u2202\u03b1f (0). This implies that\nf (\u03b1n,1) \u2212f (\u03b1n,0) > c1(\u03b1n,1 \u2212\u03b1n,0)\u2202\u03b1f (0).\nMoreover, the updates imply that (\u03b1n+1,1 \u2212\u03b1n+1,0) = (\u03b1n,1 \u2212\u03b1n,0)/2. This requires that\nthe three sequences \u03b1n,\u03b1n,0 and \u03b1n,1 converge to the same limit, \u03b1. We have\n\u2202\u03b1f (\u03b1) = lim\nn\u2192\u221e\nf (\u03b1n,1) \u2212f (\u03b1n,0)\n\u03b1n,1 \u2212\u03b1n,0\n\u2265c1\u2202\u03b1f (0)\nand\n\u2202\u03b1f (\u03b1) = lim\nn\u2192\u221e\u2202\u03b1f (\u03b1n,0) \u2264c2\u2202\u03b1f (0)\nyielding c1\u2202\u03b1f (0) \u2264c2\u2202\u03b1f (0) which is impossible since c2 > c1 and \u2202\u03b1f (0) < 0.\n\u25a0\nThe existence of \u03b1 satisfying the strong Wolfe condition is a consequence of the\nfollowing proposition, which also provides an algorithm.\nProposition 3.24 Let f : \u03b1 7\u2192f (\u03b1) be a C1 function defined on [0,+\u221e) such that f is\nbounded from below and \u2202\u03b1f (0) < 0. Let 0 < c1 < c2 < 1.\nLet \u03b10,0 = \u03b10,1 = 0 and \u03b10 > 0. Define recursively sequences \u03b1n,0,\u03b1n,1 and \u03b1n as\nfollows.\n3.3. STOCHASTIC GRADIENT DESCENT\n55\n(i) If f (\u03b1n) \u2264f (0) + c1\u03b1n\u2202\u03b1f (0) and |\u2202\u03b1f (\u03b1n)| \u2264c2|\u2202\u03b1f (0)| stop the construction.\n(ii) If f (\u03b1n) > f (0) + c1\u03b1n\u2202\u03b1f (0) let \u03b1n+1 = (\u03b1n + \u03b1n,0)/2, \u03b1n+1,1 = \u03b1n and \u03b1n+1,0 = \u03b1n,0.\n(iii) If f (\u03b1n) \u2264f (0) + c1\u03b1n\u2202\u03b1f (0) and |\u2202\u03b1f (\u03b1n)| > c2|\u2202\u03b1f (0)|:\n(a) If \u03b1n,1 = 0 and \u2202\u03b1f (\u03b1n) > \u2212c2\u2202\u03b1f (0), let \u03b1n+1 = 2\u03b1n, \u03b1n+1,0 = \u03b1n,0 and \u03b1n+1,1 =\n\u03b1n,1.\n(b) If \u03b1n,1 = 0 and \u2202\u03b1f (\u03b1n) < c2\u2202\u03b1f (0), let \u03b1n+1 = 2\u03b1n, \u03b1n+1,0 = \u03b1n and \u03b1n+1,1 =\n\u03b1n,1.\n(c) If \u03b1n,1 > 0 and \u2202\u03b1f (\u03b1n) > \u2212c2\u2202\u03b1f (0), let \u03b1n+1 = (\u03b1n + \u03b1n,0)/2, \u03b1n+1,1 = \u03b1n and\n\u03b1n+1,0 = \u03b1n,0.\n(d) If \u03b1n,1 > 0 and \u2202\u03b1f (\u03b1n) < c2\u2202\u03b1f (0), let \u03b1n+1 = (\u03b1n + \u03b1n,1)/2, \u03b1n+1,0 = \u03b1n and\n\u03b1n+1,1 = \u03b1n,1.\nThen the sequences are always finite, i.e., the algorithm terminates in a finite number of\nsteps.\nProof Assume that the algorithm runs indefinitely in order to get a contradiction.\nIf the algorithm never enters case (ii), then \u03b1n,1 = 0 for all n, \u03b1n tends to infinity and\nf (\u03b1n) \u2264f (0) + c1\u03b1n\u2202\u03b1f (0), which contradicts the fact that f is bounded from below.\nAs soon as the algorithm enter (ii), we have, for all subsequent iterations: \u03b1n,0 \u2264\n\u03b1n \u2264\u03b1n,1, \u03b1n+1,0 \u2265\u03b1n,0, \u03b1n+1,1 \u2264\u03b1n,1 and \u03b1n+1,1 \u2212\u03b1n+1,0 = (\u03b1n,1 \u2212\u03b1n,0)/2. This implies\nthat both \u03b1n,0 and \u03b1n,1 converge to the same limit \u03b1.\nMoreover, we have, at each step:\nf (\u03b1n,1) > f (0) + c1\u03b1n,1\u2202\u03b1f (0) or \u2202\u03b1f (\u03b1n,1) > \u2212c2\u2202\u03b1f (0)\nand\nf (\u03b1n,0) \u2264f (0) + c1\u03b1n,0\u2202\u03b1f (0) and \u2202\u03b1f (\u03b1n,0) \u2264c2\u2202\u03b1f (0).\nThis implies that, at each step:\nf (\u03b1n,1) \u2212f (\u03b1n,0)\n\u03b1n,1 \u2212\u03b1n,0\n> c1\u2202\u03b1f (0) or \u2202\u03b1f (\u03b1n,1) > \u2212c2\u2202\u03b1f (0)\nand\n\u2202\u03b1f (\u03b1n,0) \u2264c2\u2202\u03b1f (0).\nThere inequalities remain satisfied at the limit, and we must have\n\u2202\u03b1f (\u03b1) > c1\u2202\u03b1f (0) or \u2202\u03b1f (\u03b1) > \u2212c2\u2202\u03b1f (0)\nand\n\u2202\u03b1f (\u03b1) \u2264c2\u2202\u03b1f (0),\nwhich is a contradiction since c2 > c1 and \u2202\u03b1f (0) < 0.\n\u25a0\n56\nCHAPTER 3. INTRODUCTION TO OPTIMIZATION\n3.3\nStochastic gradient descent\n3.3.1\nStochastic approximation methods\nIn some situations, the computation of \u2207F can be too costly, if not intractable, to run\ngradient descent updates while a low-cost stochastic approximation is available. For\nexample, if F is an average of a sum of many terms, the approximation may simply\nbe based on averaging over a randomly selected subset of the terms. This leads\nto a stochastic approximation algorithm [163, 113, 25, 67] called stochastic gradient\ndescent (SGD).\nA general stochastic approximation algorithm of the Robbins-Monro type up-\ndates a parameter, denoted x \u2208Rd, using stochastic rules. One associates to each x a\nprobability distribution (\u03c0x) on some set S, and, for some function H : Rd \u00d7S \u2192Rd,\nconsiders the sequence of random iterations:\n(\u03bet+1 \u223c\u03c0Xt\nXt+1 = Xt + \u03b1t+1H(Xt,\u03bet+1)\n(3.17)\nwhere \u03bet+1 is a random variable and the notation \u03bet+1 \u223c\u03c0Xt should be interpreted\nas the more precise statement that the conditional distribution of \u03bet+1 given all past\nrandom variables Ut = (\u03be1,X1,...,\u03bet,Xt) only depends on Xt and is given by \u03c0Xt.\nIt is sometimes assumed in the literature that \u03c0x does not depend on x. This is\nno real loss of generality because under mild assumptions, a random variable \u03be fol-\nlowing \u03c0x can be generated as function U(x, \u02dc\u03be) where \u02dc\u03be follows a fixed distribution\n(such as that of a family of independent uniformly distributed variables) and one\ncan replace H(x,\u03be) by H(x,U(x, \u02dc\u03be)). On the other hand, allowing \u03c0 to depend on x\nbrings little additional complication in the notation, and corresponds to the natural\nform of many applications.\nMore complex situations can also be considered, in which \u03bet+1 is not condition-\nally independent of the past variables given Xt. For example, the conditional distri-\nbution of \u03bet+1 given the past may also depends on \u03bet, which allows for the combina-\ntion of stochastic gradient methods with Markov chain Monte-Carlo methods. This\nsituation is studied, for example, in M\u00b4etivier and Priouret [138], Benveniste et al.\n[25], and we will discuss an example in section 17.2.2.\n3.3.2\nDeterministic approximation and convergence study\nIntroduce the function\n\u00afH(x) = E\u03c0x(H(x,\u00b7))\n3.3. STOCHASTIC GRADIENT DESCENT\n57\nand write\nXt+1 = Xt + \u03b1t+1 \u00afH(Xt) + \u03b1t+1\u03b7t+1\nwith \u03b7t+1 = H(Xt,\u03bet+1) \u2212\u00afH(Xt) in order to represent the evolution of Xt in (3.17) as\na perturbation of the deterministic algorithm\n\u00afxt+1 = \u00afxt + \u03b1n+1 \u00afH( \u00afxt)\n(3.18)\nby the \u201cnoise term\u201d \u03b1t+1\u03b7t+1. In many cases, the deterministic algorithm provides\nthe limit behavior of the stochastic sequence, and one should ensure that this limit\nis as desired. By definition, the conditional expectation of \u03b7t+1 given Ut (the past) is\nzero and one says that \u03b1t+1\u03b7t+1 is a \u201cmartingale increment.\u201d Then,\nMT =\nT\nX\nt=0\n\u03b1t+1\u03b7t+1\n(3.19)\nis called a \u201cmartingale.\u201d The theory of martingales offers numerous tools for con-\ntrolling the size of MT and is often a key element in proving the convergence of the\nmethod.\nMany convergence results have been provided in the literature and can be found\nin textbooks or lecture notes such as Bena\u00a8\u0131m [23], Kushner and Yin [113], Benveniste\net al. [25]. These results rely on some smoothness and growth assumptions made on\nthe function H, and on the dynamics of the deterministic equation (3.18). Depend-\ning on these assumptions, proofs may become quite technical. We will here restrict\nto a reasonably simple context and assume that\n(H1) There exists a constant C such that, for all x \u2208Rd,\nE\u03c0x(|H(x,\u00b7)|2) \u2264C(1 + |x|2).\n(H2) There exists x\u2217\u2208Rd and \u00b5 > 0 such that, for all x \u2208Rd\n(x \u2212x\u2217)T \u00afH(x) \u2264\u2212\u00b5|x \u2212x\u2217|2.\nAssuming this, let At = |Xt \u2212x\u2217|2 and at = E(At). Then, using (3.17),\nAt+1 = At + 2\u03b1t+1(Xt \u2212x\u2217)T H(Xt,\u03bet+1) + \u03b12\nt+1|H(Xt,\u03bet+1)|2 .\nTaking the conditional expectation given past variables yields\nE(At+1 | Ut) = At + 2\u03b1t+1(Xt \u2212x\u2217)T \u00afH(Xt) + \u03b12\nt+1E\u03c0xt (|H(Xt,\u00b7)|2)\n\u2264At \u22122\u03b1t+1\u00b5At + \u03b12\nt+1C(1 + |Xt|2)\n\u2264(1 \u22122\u03b1t+1\u00b5 + C\u03b12\nt+1)At + \u03b12\nt+1 \u02dcC\n58\nCHAPTER 3. INTRODUCTION TO OPTIMIZATION\nwith \u02dcC = 1 + |x\u2217|2. Taking expectations on both sides yields\nat+1 \u2264(1 \u22122\u03b1t+1\u00b5 + C\u03b12\nt+1)at + \u03b12\nt+1 \u02dcC.\n(3.20)\nWe state the next step in the computation as a lemma.\nLemma 3.25 Assume that the sequence at satisfies the recursive inequality\nat+1 \u2264(1 \u2212\u03b4t)at + \u03f5t\n(3.21)\nwith 0 \u2264\u03b4t \u22641. Let vk,t = Qt\nj=k+1(1 \u2212\u03b4j). Then\nat \u2264a0v0,t +\nt\nX\nk=1\n\u03f5kvk,t.\n(3.22)\nProof Letting bt = at/v0,t, we get\nbt+1 \u2264bt + \u03f5t+1\nv0,t+1\nso that\nbt \u2264b0 +\nt\nX\nk=1\n\u03f5k\nv0,k\n,\nand\nat \u2264a0v0,t +\nt\nX\nk=1\n\u03f5kvk,t.\n\u25a0\nUsing (3.20), we can apply this lemma with \u03f5t = \u02dcC\u03b12\nt and \u03b4t = 2\u03b1t\u00b5\u2212C\u03b12\nt , making the\nadditional assumption that, for all t, \u03b1t < min( 1\n2\u00b5, 2\u00b5\nC ), which ensures that 0 < \u03b4t < 1.\nStarting with a simple case, assume that the steps \u03b3t are constant, equal to some\nvalue \u03b3 (yielding also constant \u03b4 and \u03f5). Then, (3.22) gives\nat \u2264a0(1 \u2212\u03b4)t + \u03f5\nt\nX\nk=1\n(1 \u2212\u03b4)t\u2212k\u22121 \u2264a0(1 \u2212\u03b4)t + \u03f5\n\u03b4.\n(3.23)\nReturning to the expression of \u03b4 and \u03f5 as functions of \u03b1, this gives\nat \u2264a0(1 \u22122\u03b1\u00b5 + \u03b12C)t +\n\u03b1 \u02dcC\n2\u00b5 \u2212\u03b1C .\nThis shows that limsupat = O(\u03b1).\nReturn to the general case in which the steps depend on t, we will use the follow-\ning simple result, that we state as a lemma for future reference.\n3.3. STOCHASTIC GRADIENT DESCENT\n59\nLemma 3.26 Assume that the double indexed sequence wst, s \u2264t of non-negative num-\nbers is bounded and such that, for all s, limt\u2192\u221ewst = 0. Let \u03b21,\u03b22,... be such that\n\u221e\nX\nt=1\n|\u03b2t| < \u221e.\nThen\nlim\nt\u2192\u221e\nt\nX\ns=1\n\u03b2swst = 0.\nProof For any t0, we have\n\f\f\f\f\f\f\f\nt\nX\ns=1\n\u03b2swst\n\f\f\f\f\f\f\f\n\u2264max\ns\n|\u03b2s|\nt0\nX\ns=1\nwst + max\ns,t |wst|\nX\ns=t0+1\n|\u03b2s|\nso that\nlimsup\nt\u2192\u221e\n\f\f\f\f\f\f\f\nt\nX\ns=1\n\u03b2swst\n\f\f\f\f\f\f\f\n\u2264max\ns,t |wst|\nX\ns=t0+1\n|\u03b2s|\nand since this upper bound can be made arbitrarily small, the result follows.\n\u25a0\nLemma 3.25 implies that\nat \u2264a0v0,t + \u02dcC\nt\nX\ns=1\n\u03b12\ns+1vs,t.\nAssume that\n(H3) P\u221e\nk=1 \u03b1k = \u221eand P\u221e\nk=1 \u03b12\nk < \u221e,\nThen limt\u2192\u221evst = 0 for all s and lemma 3.26 implies that at tends to zero. So, we\nhave just proved that, if (H1), (H2) and (H3) are true, the sequence Xt converges in\nthe L2 sense to x\u2217. Actually, under these conditions, one can show that Xt converge\nto x\u2217almost surely, and we refer to Benveniste et al. [25], Chapter 5, for a proof (the\nargument above for an L2 convergence follows the one given in Nemirovski et al.\n[145]).\nUnder (H3), one can say much more on the asymptotic behavior of the algorithm\nby comparing it with an ordinary differential equation. The \u201cODE method,\u201d intro-\nduced in Ljung [120], is indeed a fundamental tool for the analysis of stochastic\napproximation algorithms. The correspondence between discrete and continuous\n60\nCHAPTER 3. INTRODUCTION TO OPTIMIZATION\ntimes is provided by the sequence \u03b1t. More precisely, let \u03c40 = 0 and \u03c4t = \u03c4t\u22121 + \u03b1t,\nt \u22651. From (H3), \u03c4t \u2192\u221ewhen t \u2192\u221e. Define the piecewise linear interpolation\nx\u2113(\u03c1) of the sequence xt by\nX\u2113(\u03c1) = Xt + \u03c1 \u2212\u03c4t\n\u03b1t+1\n(Xt+1 \u2212Xt),\n\u03c1 \u2208[\u03c4t,\u03c4t+1).\nSwitching to continuous time allows us to interpret the average iteration \u00afxt+1 = \u00afxt +\n\u03b1t+1 \u00afH( \u00afxt) as an Euler discretization scheme for the ordinary differential equation\n(ODE)\n\u2202\u03c1 \u00afx = \u00afH( \u00afx).\n(3.24)\nMost of the insight on long-term behavior of stochastic approximations results\nfrom the fact that the random process x behaves asymptotically like solutions of\nthis ODE. One has, for example, the following result, for which we introduce some\nadditional notation.\nAssume that (3.24) has unique solutions for given initial conditions on any fi-\nnite interval, and denote by \u03d5(\u03c1,\u03c9) its solution at time \u03c1 initialized with \u00afx(0) = \u03c9.\nLet \u03b1c(\u03c1) and \u03b7c(\u03c1) be piecewise constant interpolations of (\u03b1t) and (\u03b7t) defined by\n\u03b1c(\u03c1) = \u03b1t+1 and \u03b7c(\u03c1) = \u03b7t+1 on the interval [\u03c4t,\u03c4t+1). Finally, let\n\u2206(\u03c1,T ) =\nmax\ns\u2208[\u03c1,\u03c1+T ]\n\f\f\f\f\f\f\nZ s\n\u03c1\n\u03b7c(u)du\n\f\f\f\f\f\f.\nThe following proposition (see [23]) compares the tails of the process x\u2113(i.e., the\nfunctions x\u2113(\u03c1 + s), s \u22650) with the solutions of the ODE over finite intervals.\nProposition 3.27 (Benaim) Assume that \u00afH is Lipschitz and bounded. Then, for some\nconstant C(T) that only depends on T and \u00afH, one has, for all \u03c1 \u22650\nsup\nh\u2208[0,T]\n|X\u2113(\u03c1 + h) \u2212\u03d5(h,X\u2113(\u03c1))| \u2264C(T )\n \n\u2206(\u03c1 \u22121,T + 1) +\nmax\ns\u2208[\u03c1,\u03c1+T]\u03b1c(s)\n!\n.\n(3.25)\nRecall that \u00afH being Lipschitz means that there exists a constant C such that\n| \u00afH(w) \u2212\u00afH(w\u2032)| \u2264C|w \u2212w\u2032|\nfor all w,w\u2032 \u2208Rp.\nIn the upper-bound in (3.25), the term \u2206(\u03c1 \u22121,T + 1) is a random variable. It can\nbe related to the variations\n\u2206\u2032(t,N) = max\nk=0,...,N |Mt+k \u2212Mt|,\n3.3. STOCHASTIC GRADIENT DESCENT\n61\nwhere M is defined in (3.19), because, if m(\u03c1) is the largest integer t such that \u03c4t \u2264\u03c1,\nthen\n\u2206\u2032(m(\u03c1) + 1,m(\u03c1 + T) \u2212m(\u03c1)) \u2264\u2206(\u03c1,T ) \u2264\u2206\u2032(m(\u03c1),m(\u03c1 + T) \u2212m(t) + 1).\nIn the case we are considering, one can use martingale inequalities (called Doob\u2019s\ninequalities) to control \u2206\u2032. One has, for example,\nP\n\u0012\nmax\n0\u2264k\u2264N |Mt+k \u2212Mt| > \u03bb\n\u0013\n\u2264E(|Mt+N \u2212Mt|2)\n\u03bb2\n.\n(3.26)\nFurthermore, using the fact that E(\u03b7k+1\u03b7l+1) = 0 if k , l, one has\nE(|Mt+N \u2212Mt|2) =\nt+N\nX\nk=t\n\u03b12\nk+1E(|\u03b7t+1|2).\nIf we assume (to simplify) that H is bounded and P\u221e\nk=1 \u03b12\nk < \u221ethen, for some con-\nstant C, we have\nE(|Mt+N \u2212Mt|2) \u2264C\n\u221e\nX\nk=t\n\u03b12\nk+1 \u21920\nand inequality (3.26) can then be used in (3.25) to control the probability of devia-\ntion of the stochastic approximation from the solution of the ODE over finite inter-\nvals (a little more work is required under weaker assumptions on H, such as (H1)).\nProposition 3.27 cannot be used with T = \u221ebecause the constant C(T ) typically\ngrows exponentially with T. In order to draw conclusions on the limit of the process\nW, one needs additional assumptions on the stability of the ODE. We refer to [23]\nfor a collection of results on the relationship between invariant sets and attractors of\nthe ODE and limit trajectories of the stochastic approximation. We here quote one\nof these results which is especially relevant for SGD.\nProposition 3.28 Assume that \u00afH = \u2212\u2207E is the gradient of a function E and that \u2207E only\nvanishes at a finite number of points. Assume also that Xt is bounded. Then Xt converges\nto a point x\u2217such that \u2207E(x\u2217) = 0.\nSome additional conditions on \u00afH can ensure that stochastic approximation tra-\njectories remain bounded. The simplest one assumes the existence of a \u201cLyapunov\nfunction\u201d that controls the ODE at infinity. The following result is a simplified ver-\nsion of Theorem 17 in Benveniste et al. [25].\n62\nCHAPTER 3. INTRODUCTION TO OPTIMIZATION\nTheorem 3.29 In addition to the hypotheses previously made, assume that there exists a\nC2 function U with bounded second derivatives and K0 > 0 such that, for allx such that\n|x| \u2265K0,\n\u2207U(x)T \u00afH(x) \u22640,\nU(x) \u2265\u03b3|x|2,\u03b3 > 0.\nThen, the trajectories X\u2113(\u03c1) are almost surely bounded.\nNote that hypothesis (H2) above implies the theorem\u2019s assumptions.\n3.3.3\nThe ADAM algorithm\nADAM (for adaptive moment estimation [102]) is a popular variant of stochastic\ngradient descent. When dealing with high-dimensional vectors W, using a single\n\u201cgain\u201d parameter (\u03b3n+1 in (11.2)) is a limiting assumption since all parameters do\nnot need to scale the same way. This can sometimes be handled by reweighting the\ncomponents of H, i.e., using iterations\nXt+1 = Xt + \u03b1tDtH(Xt,\u03bet+1)\nwhere Dt is a (typically diagonal) matrix. The previous theory can be applied to\nsituations in which D may be random, provided it converges almost surely to a fixed\nmatrix.\nThe ADAM algorithm provides such a construction (without the theoretical guar-\nantees) in which Dt is computed using past iterations of the algorithm. It requires\nseveral parameters, namely: \u03b1: the algorithm gain, taken as constant (e.g., \u03b1 =\n0.001); Two parameters \u03b21 and \u03b22 for moment estimates (e.g. \u03b21 = 0.9 and \u03b22 =\n0.999); A small number \u03f5 (e.g., \u03f5 = 10\u22128) to avoid divisions by 0. In addition, ADAM\ndefines two vectors: a mean m and a second moment v, respectively initialized at\n0 and 1. The ADAM iterations are given below, in which g\u22972 denotes the vector\nobtained by squaring each coefficient of a vector g.\nAlgorithm 3.1 (ADAM)\n1. Let Xt be the current state, mt and vt the current mean and variance.\n2. Generate \u03bet+1 and let gt+1 = H(Xt,\u03bet+1).\n3. Update mt+1 = \u03b21mt + (1 \u2212\u03b21)gt+1.\n4. Update vt+1 = \u03b22vt + (1 \u2212\u03b22)g\u22992\nt+1.\n5. Let \u02c6mt+1 = mt+1/(1 \u2212\u03b2t+1\n1\n) and \u02c6vt+1 = vt+1/(1 \u2212\u03b2t+1\n2\n)\n3.4. CONSTRAINED OPTIMIZATION PROBLEMS\n63\n6. Set\nXt+1 = Xt \u2212\u03b1\n\u02c6mt+1\n\u221a\u02c6vt+1 + \u03f5\nNote that the iteration on mt and vt correspond to defining\n\u02c6mt =\n\u03b21\n1 \u2212\u03b2t\n1\nt\nX\nk=0\n(1 \u2212\u03b21)t\u2212kgk\nand\n\u02c6vt =\n\u03b22\n1 \u2212\u03b2t\n2\nt\nX\nk=0\n(1 \u2212\u03b22)t\u2212kg\u22992\nk .\n3.4\nConstrained optimization problems\n3.4.1\nLagrange multipliers\nA constrained optimization problem minimizes a function F over a closed subset\n\u2126of Rd, with \u2126, Rd. This restriction invalidates, in a large part, the optimality\nconditions discussed in section 3.2. These conditions indeed apply to minimizers\nbelonging to the interior of \u2126, and therefore do not hold when they lie at its bound-\nary, which is a very common situation in practice (\u2126often has an empty interior).\nIn this section, which follows the discussion given in Wright and Recht [204], we\nreview conditions for optimality for constrained minimization of smooth functions,\nin two cases. The first one, discussed in this section, is when \u2126is defined by a finite\nnumber of smooth constraints, leading, under some assumptions, to the Karush-\nKuhn-Tucker (or KKT) conditions. The second one, in the next section, specializes\nto closed convex \u2126.\nKKT conditions\nWe introduce some notation. Let \u03b3i, for i \u2208C, be C1 functions \u03b3i : Rd \u2192R, where C\nis a finite set of indices. We assume that C is divided into two non-intersecting parts,\nC = E \u222aI and consider minimization problems searching for\nx\u2217\u2208argmin\n\u2126\nF\n(3.27)\nwhere\n\u2126= {x \u2208Rd : \u03b3i(x) = 0,i \u2208E and \u03b3i(x) \u22640,i \u2208I}.\n(3.28)\n64\nCHAPTER 3. INTRODUCTION TO OPTIMIZATION\nThe set \u2126of all x that satisfy the constraints is called the feasible set for the consid-\nered problem. We will always assume that it is non-empty. If x \u2208\u2126, one defines the\nset A(x) of active constraints at x to be\nA(x) = {i \u2208C : \u03b3i(x) = 0}.\nOne obviously has E \u2282A(x) for x \u2208\u2126.\nTo be valid, the KKT conditions require some additional assumptions on poten-\ntial minimizers, called \u201cconstraint qualifications.\u201d An instance of such assumptions\nis provided by the next definition.\nDefinition 3.30 A point x \u2208\u2126satisfies the Mangasarian-Fromovitz constraint qualifi-\ncations (MF-CQ) if the following two conditions are satisfied.\n(MF1) The vectors (\u2207\u03b3i(x),i \u2208E) are linearly independent.\n(MF2) There exists a vector h \u2208Rd such that hT \u2207\u03b3i(x) = 0 for all i \u2208E and hT \u2207\u03b3i(x) < 0\nfor all i \u2208A(x) \u2229I.\nA sufficient (and easier to check) condition for x to satisfy these constraints is when\nthe vectors (\u2207\u03b3i(x),i \u2208A(x)) are linearly independent [37]. Indeed, if the latter \u201cLI-\nCQ\u201d condition is true, then any set of values can be assigned to hT \u2207\u03b3i(x) with the\nexistence of a vector h that achieves them.\nWe introduce the Lagrangian\nL(x,\u03bb) = F(x) +\nX\ni\u2208C\n\u03bbi\u03b3i(x)\n(3.29)\nwhere the real numbers \u03bbi, i \u2208C are called Lagrange multipliers. The following the-\norem (stated without proof, see, e.g., [146, 35]) provides necessary conditions satis-\nfied by solutions of the constrained minimization problem that satisfy the constraint\nqualifications.\nTheorem 3.31 Assume x\u2217\u2208\u2126is a solution of (3.27), and that x\u2217satisfies the MF-CQ\nconditions. Then there exist Lagrange multipliers \u03bbi, i \u2208C, such that\n(\u2202xL(x\u2217,\u03bb) = 0\n\u03bbi \u22650 if i \u2208I, with \u03bbi = 0 when i < A(x\u2217)\n(3.30)\nConditions (3.30) are the KKT conditions for the constrained optimization problem.\nThe second set of conditions is often called the complementary slackness conditions\nand state that \u03bbi = 0 for an inequality constraint unless this constraint is satisfied\nwith an equality. The next section provides examples in which the MF-CQ condi-\ntions are not satisfied and Theorem 3.31 does not hold. However, these conditions\nare not needed in the special case when the constraints are affine.\n3.4. CONSTRAINED OPTIMIZATION PROBLEMS\n65\nTheorem 3.32 Assume that for all i \u2208A(x\u2217), the functions \u03b3i are affine, i.e., \u03b3i(x) =\nbT\ni x + \u03b2i for some b \u2208Rd and \u03b2 \u2208R. Then (3.30) holds at any solution of (3.27).\nRemark 3.33 We have taken the convention to express the inequality constraints as\n\u03b3i(x) \u22640, i \u2208I. With the reverse convention, i.e., \u03b3i(x) \u22650, i \u2208I, one generally\ndefines the Lagrangian as\nL(x,\u03bb) = F(x) \u2212\nX\ni\u2208C\n\u03bbi\u03b3i(x)\nand the KKT conditions remain unchanged.\n\u2666\nExamples.\nConstraint qualifications are important to ensure the validity of the the-\norem. Consider a problem with equality constraints only, and replace it by\nx\u2217\u2208argmin\n\u2126\nF\nsubject to \u02dc\u03b3i(x) = 0, i \u2208E, with \u02dc\u03b3i = \u03b32\ni . We clearly did not change the problem.\nHowever, the previous theorem applied to the Lagrangian\nL(x,\u03bb) = F(x) +\nX\ni\u2208C\n\u03bbi \u02dc\u03b3i(x)\nwould require an optimal solution to satisfy \u2207F(x) = 0, because \u2207\u02dc\u03b3i(x) = 2\u03b3i(x)\u2207\u03b3i(x) =\n0 for any feasible solution. Minimizers of constrained problems do not necessarily\nsatisfy \u2207F(x) = 0, however. This is no contradiction with the theorem since \u2207\u02dc\u03b3i(x) = 0\nfor all i shows that no feasible point satisfies the MF-CQ.\nTo take a more specific example, still with equality constraints, let d = 3, C = {1,2}\nwith F(x,y,z) = x/2+y and \u03b31(x,y,z) = x2\u2212y2,\u03b32(x,y,z) = y\u2212z2. Note that \u03b31 = \u03b32 = 0\nimplies that y = |x|, so that, for a feasible point, F(x,y,z) = |x| + x/2 \u22650 and vanishes\nonly when x = y = 0, in which case z = 0 also. So (0,0,0) is a global minimizer.\nWe have dF(0) = (1/2,1,0), d\u03b31(0) = (0,0,0) and d\u03b32(0) = (0,1,0) so that 0 does not\nsatisfy the MF-CQ. The equation\ndF(0) + \u03bb1d\u03b31(0) + \u03bb2d\u03b32(0) = 0\nhas no solution (\u03bb1,\u03bb2), so that the conclusion of the theorem does not hold.\n3.4.2\nConvex constraints\nWe now consider the case in which \u2126is a closed convex set. To specify the optimality\nconditions in this case, we need the following definition.\n66\nCHAPTER 3. INTRODUCTION TO OPTIMIZATION\nDefinition 3.34 Let \u2126\u2282Rd be convex and let x \u2208\u2126. The normal cone to \u2126at x is the\nset\nN\u2126(x) = {h \u2208Rd : hT (y \u2212x) \u22640 for all y \u2208\u2126}\n(3.31)\nThe normal cone is an example of convex cone. (A convex subset \u0393 of Rd is called\na convex cone, if it is such that \u03bbx \u2208\u0393 for all x \u2208\u0393 and \u03bb \u22650, a property obviously\nsatisfied by N\u2126(x).) It should also be clear from the definition that non-zero vectors\nin N\u2126(x) always point outside \u2126, i.e., x + h < \u2126if h \u2208N\u2126(x), h , 0. Here are some\nexamples.\n\u2022 If x is in the interior of \u2126, then N\u2126(x) = {0}.\n\u2022 Assume that \u2126is a half space, i.e., \u2126= {x : bT x + \u03b2 \u22640} with |b| = 1, and take\nx \u2208\u2202\u2126, i.e., bT x + \u03b2 = 0. Then\nN\u2126(x) = {h = \u00b5b : \u00b5 \u22650}.\nIndeed, any element of Rd can be written as y = x+\u03bbb+q with qT b = 0, and y \u2208\u2126\nif and only if \u03bb \u22640. Fix such a y and take h \u2208Rd, decomposed as h = \u00b5b + r, with\nrT b = 0. We have hT (y \u2212x) = \u03bb\u00b5 + rT q. Clearly, if \u00b5 < 0, or if r , 0, one can find \u03bb \u22640\nand q \u22a5b such that hT (y \u2212x) > 0. One the other hand, if \u00b5 \u22640 and r = 0, we have\nhT (y \u2212x) \u22640 for all y \u2208\u2126, which proves the above statement.\n\u2022 With a similar argument, if \u2126= {x : bT x +\u03b2 = 0} is a hyperplane, one finds that\nN\u2126(x) = {h = \u03bbb : \u03bb \u2208R}.\nOne can build normal cones to domains associated with multiple inequalities or\nequalities based on the following theorem.\nTheorem 3.35 Let \u21261 and \u21262 be two convex sets with relint(\u21261)\u2229relint(\u21262) , \u2205. Then,\nif x \u2208\u21261 \u2229\u21262\nN\u21261\u2229\u21262(x) = N\u21261(x) + N\u21262(x)\nHere, the addition is the standard sum between sets in a vector space:\nA + B = {x + y : x \u2208A,y \u2208B}.\nFinally, we note that, if x \u2208relint(\u2126), then\nN\u2126(x) = {h \u2208Rd : hT (y \u2212x) = 0,y \u2208\u2126}.\n(3.32)\nIndeed, if y \u2208\u2126, then x + \u03f5(y \u2212x) \u2208\u2126for small enough \u03f5 (positive or negative). For\nh \u2208N\u2126(x), the condition \u03f5hT (y \u2212x) \u22640 for small enough \u03f5 requires that hT (y \u2212x) = 0.\nWith this definition in hand, we have the following theorem.\n3.4. CONSTRAINED OPTIMIZATION PROBLEMS\n67\nTheorem 3.36 Let F be a C1 function and \u2126a closed convex set. If x\u2217\u2208argmin\u2126F, then\n\u2212\u2207F(x\u2217) \u2208N\u2126(x\u2217).\n(3.33)\nIf F is convex and (3.33) holds, then x\u2217\u2208argmin\u2126F.\nProof Assume that x\u2217\u2208argmin\u2126F. If y \u2208\u2126, then x\u2217+ t(y \u2212x\u2217) \u2208\u2126for all t \u2208[0,1]\nand the function f (t) = F(x + t(y \u2212x\u2217)) is C1 on [0,1], with a minimum at t = 0. This\nrequires that \u2202tf (0) = \u2207F(x\u2217)T (y \u2212x\u2217) \u22650, because, if \u2202tf (0) < 0, a Taylor expansion\nwould show that f (t) < f (0) for small enough t > 0.\nIf F is convex and (3.33) holds, we have F(y) \u2265F(x\u2217)+\u2207F(x\u2217)T (y\u2212x\u2217) by convexity,\nso that\nF(x\u2217) \u2264F(y) + (\u2212\u2207F(x\u2217))T (y \u2212x\u2217) \u2264F(y).\n\u25a0\n3.4.3\nApplications\nLagrange multipliers revisited. Consider \u2126defined by (3.28), with the additional\nassumptions that \u03b3i(x) = bT\ni x + \u03b2i for i \u2208E and \u03b3i is convex for i \u2208I, which ensure\nthat \u2126is convex. Define\nN \u2032\n\u03b3(x) =\n\uf8f1\uf8f4\uf8f4\uf8f4\uf8f2\uf8f4\uf8f4\uf8f4\uf8f3\ng =\nX\ni\u2208A(x)\n\u03bbi\u2207\u03b3i(x) : \u03bbi \u22650,i \u2208A(x) \u2229I\n\uf8fc\uf8f4\uf8f4\uf8f4\uf8fd\uf8f4\uf8f4\uf8f4\uf8fe\n.\nThen, the KKT conditions in (3.30) can be rewritten as\n\u2212\u2207F(x\u2217) \u2208N \u2032\n\u03b3(x\u2217).\nNote that one always have N \u2032\n\u03b3(x) \u2282N\u2126(x) since, for g = P\ni\u2208A(x) \u03bbi\u2207\u03b3i(x) \u2208N \u2032\n\u03b3(x), one\nhas, for y \u2208\u2126,\ngT (y \u2212x) =\nX\ni\u2208A(x)\n\u03bbi\u2207\u03b3i(x)T (y \u2212x)\n=\nX\ni\u2208E\n\u03bbi(aT\ni y \u2212aT\ni x) +\nX\ni\u2208A(x)\u2229I\n\u03bbi(\u03b3i(x) + \u2207\u03b3i(x)T (y \u2212x))\n=\nX\ni\u2208A(x)\u2229I\n\u03bbi(\u03b3i(x) + \u2207\u03b3i(x)T (y \u2212x))\n\u2264\u03bbi\u03b3i(y) \u22640,\nin which the have used the facts that aT\ni x = aT\ni y = \u2212\u03b2i for x,y \u2208\u2126, i \u2208E, \u03b3i(x) = 0 for\ni \u2208A(x) and the convexity of \u03b3i. Constraint qualifications such as those considered\nabove are sufficient conditions that ensure the identity between the two sets.\n68\nCHAPTER 3. INTRODUCTION TO OPTIMIZATION\nConsider now the situation of theorem 3.32, and assume that all constraints are\naffine inequalities, \u03b3i(x) = bT\ni x + \u03b2 \u22640,i \u2208I. Then, the statement N\u2126(x) \u2282N \u2032\n\u03b3(x) can\nbe reexpressed as follows. All h \u2208Rd such that\nhT (y \u2212x) \u22640\nas soon as bT\ni (y \u2212x) \u22640 for all i \u2208A(x) must take the form\nh =\nX\ni\u2208A(x)\n\u03bbibi\nwith \u03bb(i) \u22650. This property is called Farkas\u2019s lemma (see, e.g. [167]). Note that affine\nequalities bT\ni x + \u03b2 = 0 can be included as two inequalities bT\ni x + \u03b2 \u22640, \u2212bT\ni x \u2212\u03b2 \u22640,\nwhich removes the sign constraint on the corresponding \u03bb(i) and therefore yields\ntheorem 3.32.\nPositive semi-definite matrices. We now take an example in which theorem 3.32\ndoes not apply directly. Let \u2126= S+\nn be the space of positive semidefinite n \u00d7 n ma-\ntrices, considered as a subset of the space Mn of n \u00d7n matrices, itself identified with\nRn2. With this identification, the Euclidean inner product between two matrices can\nbe expressed as (A,B) 7\u2192trace(AT B).\nWe have A \u2208S+\nn if and only if, for all u \u2208Rd, uT Au \u22650, which provides an infinity\nof linear inequality constraints on A. Elements of NS+(A) are matrices H \u2208Mn such\nthat\ntrace(HT (B \u2212A)) \u22640\nfor all B \u2208S+\nn , and we want to make this normal cone explicit. We first note that,\nevery square matrix H can be decomposed as the sum of a symmetric matrix, Hs and\nof a skew symmetric one, Ha (namely, Hs = (H + HT )/2 and Ha = (H \u2212HT )/2). We\nhave moreover trace(HT\na (B \u2212A)) = 0, so the condition is only on the symmetric part\nof H.\nFor any u \u2208Rd, one can take B = A+uuT , which belongs to S+\nn , with trace(HT\ns (B\u2212\nA)) = uT Hsu. This shows that, for H to belong to NS+n (A), one needs Hs \u2aaf0.\nNow, take an eigenvector u of A with eigenvalue \u03c1 > 0. Then B = A \u2212\u03b1uuT is\nalso in S+\nn as soon as 0 \u2264\u03b1 \u2264\u03c1, and trace(HT\ns (B \u2212A)) = \u2212\u03b1uT Hsu. So, if H \u2208NS+n (A),\nwe have uT Hsu \u22650, and since Hs \u2aaf0, this gives uT Hsu = 0. Still because Hs is\nnegative semi-definite, this implies Hsu = 0. (This can be shown, for example, using\nSchwarz\u2019s inequality which says that (uT Hsv)2 \u2264(uT Hsu)(vT Hsv) for all v \u2208Rd.)\nDecomposing A with respect to its non-zero eigenvectors, i.e., writing\nA =\np\nX\nk=1\n\u03c1kukuT\nk\n3.4. CONSTRAINED OPTIMIZATION PROBLEMS\n69\nwhere p = rank(A), we get AHs = HsA = 0. We therefore obtained the proposition\nProposition 3.37 Let A \u2208S+\nn . Then H \u2208Mn belongs to NS+n (A) if and only if \u2212Hs \u2208S+\nn\nand HsA = 0, where Hs = (H + HT )/2.\nNow, if one wants to minimize a function F over positive semidefinite matrices,\nand A\u2217is a minimizer, we get the necessary condition that A\u2217(\u2207F(A\u2217))s = 0 with\n(\u2207F(A\u2217))s positive semidefinite. These conditions are sufficient if F is convex.\nFor example, take\nF(A) = 1\n2trace(A2) \u2212trace(BA)\n(3.34)\nwith B \u2208Sn. Then (\u2207F(A))s = A \u2212B and the condition is A(A \u2212B) = 0 with A \u2ab0B. If\nB is diagonalized in the form B = UT DU, with U orthogonal and D diagonal, then\nthe solution is A\u2217= UT D+U where D+ is deduced from U by replacing non-negative\nentries by zeros.\nProjection. Let \u2126be closed convex, x0 \u2208Rd and F(x) = 1\n2|x \u2212x0|2. We have\nmin\n\u2126F =\nmin\n\u2126\u2229\u00afB(0,R)F\nfor large enough R (e.g., larger than F(x) for any fixed point in \u2126), and since the\nlatter minimization is over a compact set, argmin\u2126F is not empty. The function F\nbeing strongly convex, its minimizer over \u2126is unique and called the projection of\nx0 on \u2126, denoted proj\u2126(x0).\nSince \u2207F(x) = x \u2212x0, theorem 3.36 implies that proj\u2126(x0) is characterized by\nproj\u2126(x0) \u2208\u2126and\nx0 \u2212proj\u2126(x0) \u2208N\u2126(proj\u2126(x0))\n(3.35)\nor\n(x0 \u2212proj\u2126(x0))T (y \u2212proj\u2126(x0)) \u22640 for all y \u2208\u2126.\n(3.36)\nIf x0 < \u2126, then proj\u2126(x0) \u2208\u2202\u2126, since otherwise we would have N\u2126(proj\u2126(x0)) = {0}\nand x0 = proj\u2126(x0), a contradiction. Of course, if x0 \u2208\u2126, then proj\u2126(x0) = x0.\nHere are some important examples.\n1. Let \u2126= z0+V , where z0 \u2208Rd and V is a linear space (i.e., \u2126is an affine subset\nof Rd). Then N\u2126(x) = z0 + V \u22a5= x + V \u22a5for all x \u2208\u2126, where V \u22a5is the vector space of\nvectors orthogonal to V , and proj\u2126(x0) is characterized by proj\u2126(x0) \u2208\u2126and\n(x0 \u2212proj\u2126(x0)) \u2208V \u22a5\nwhich is the usual characterization of the orthogonal projection on an affine space\n(compare to section 6.4).\n70\nCHAPTER 3. INTRODUCTION TO OPTIMIZATION\n2. If \u2126= \u00afB(0,1), the closed unit sphere, then N\u2126(x) = R+x for x \u2208\u2202\u2126(i.e., |x| = 1).\nOne can indeed note that, if h , 0 in normal to \u2126at x, then h/|h| \u2208\u2126so that\nhT\n h\n|h| \u2212x\n!\n\u22640\nwhich yields |h| \u2264hT x. The Cauchy-Schwartz inequality implying that hT x \u2264|h||x| =\n|h|, we must have equality, hT x = |h||x|, which is only possible when x and h are\ncollinear.\nGiven x0 \u2208Rd with x0 \u22651, we see that proj\u2126(x0) must satisfy the conditions\n|proj\u2126(x0)| = 1 (to be in \u2202\u2126) and x0 \u2212proj\u2126(x0) = \u03bbx0 for some \u03bb \u22650, which gives\nproj\u2126(x0) = x0/|x0|.\n3. If \u2126= S+\nn and B (taking the role of x0) is a symmetric matrix, then proj\u2126(B) was\nfound in the previous section, and is given by A = UT D+U where UT DU provides a\ndiagonalization of B.\nThe projection has the important property of being 1-Lipschitz.\nProposition 3.38 Let \u2126be a closed convex subset of Rd. Then, for all x,y \u2208Rd\n|proj\u2126(x) \u2212proj\u2126(y)| \u2264|x \u2212y|.\n(3.37)\nProof This proposition is a special case of proposition 3.55 below.\n\u25a0\n3.4.4\nProjected gradient descent\nThe projected gradient descent algorithm minimizes F over \u2126by iterating\nxt+1 = proj\u2126(xt \u2212\u03b1t\u2207F(xt)),\n(3.38)\nwhich provides a feasible method when proj\u2126is easy to compute. An equivalent\nformulation is\nxt+1 = argmin\n\u2126\nF(xt) + \u2207F(xt)T (x \u2212xt) + 1\n2\u03b1t\n|x \u2212xt|2.\n(3.39)\nTo justify this last statement it suffices to notice that the function in the r.h.s. can be\nwritten as\n1\n2\u03b1t\n|x \u2212xt + \u03b1t\u2207F(xt)|2 \u2212\u03b1t\n2 |\u2207F(xt)|2 + F(xt)\nand apply the definition of the projection.\nThe convergence properties of this algorithm will be discussed in section 3.5.5,\nin a more general context.\n3.5. GENERAL CONVEX PROBLEMS\n71\n3.5\nGeneral convex problems\n3.5.1\nEpigraphs\nDefinition 3.39 Let F be a convex function. The epigraph of F is the set\nepi(F) =\nn\n(x,a) \u2208Rd \u00d7 R : F(x) \u2264a\no\n.\n(3.40)\nOne says that F is closed if epi(F) is a closed subset of Rd \u00d7 R, that is: if x = limn xn and\na = limn an with F(xn) \u2264an, then F(x) \u2264a.\nClearly, if (x,a) \u2208epi(F), then x \u2208dom(F). It should also be clear that epi(F) is always\nconvex when F is convex: If (x,a),(y,b) \u2208epi(F), then\nF((1 \u2212t)x + ty) \u2264(1 \u2212t)F(x) + tF(y) \u2264(1 \u2212t)a + tb\nso that (1 \u2212t)(x,a) + t(y,b) \u2208epi(F).\nTo illustrate the definition, consider a simple example. Let F be the function\ndefined on R by F(x) = |x| if |x| < 1 and F(x) = +\u221eotherwise. It is convex, but not\nclosed, as can be seen by taking the sequence (1 \u22121/n,1) \u2208epi(F), with, at the limit,\nF(1) = +\u221e> 1. In contrast, the function defined by \u02dcF(x) = |x| if |x| \u22641 and \u02dcF(x) = +\u221e\notherwise is convex and closed.\nWe have the following proposition.\nProposition 3.40 A convex function F is closed if and only if all its sub-level sets\n\u039ba(F) =\nn\nx \u2208Rd : F(x) \u2264a\no\nare closed subsets of Rd.\nProof If F is closed, then \u039ba(F) is the intersection of the set {(x,a) : x \u2208Rd}, which is\nobviously closed, and of epi(F). It is therefore a closed set.\nConversely, assume that all \u039ba(F) are closed and take a sequence (xn,an) in epi(F)\nthat converges to (x,a). Then, fixing \u03f5 > 0, xn \u2208\u039ba+\u03f5 for large enough n, and since\nthis set is closed, F(x) \u2264a + \u03f5. Since this is true for all \u03f5 > 0, we have F(x) \u2264a and\n(x,a) \u2208epi(F).\n\u25a0\nNote that, if F is continuous, then it is closed, so that closedness generalizes con-\ntinuity for convex functions, but it also applies to the non-smooth case.\nIf \u2126is a convex subset of Rd, its indicator function \u03c3\u2126(such that \u03c3\u2126(x) = 0 for\nx \u2208\u2126and \u03c3\u2126(x) = +\u221eotherwise) is closed if and only if \u2126is a closed subset of Rd.\nThis is obvious since \u039ba(\u03c3\u2126) = \u2126if a \u22650 and \u2205otherwise.\n72\nCHAPTER 3. INTRODUCTION TO OPTIMIZATION\n3.5.2\nSubgradients\nSeveral machine learning problems involve convex functions that are not C1, requir-\ning a generalization of the notion of derivative provided by the following definition.\nDefinition 3.41 If F is a convex function and x \u2208dom(F), a vector g \u2208Rd such that\nF(x) + gT (y \u2212x) \u2264F(y)\n(3.41)\nfor all y \u2208Rd is called a subgradient of F at x.\nThe set of subgradients of F at x is denoted \u2202F(x) and called the subdifferential of F\nat x.\nIf x \u2208int(dom(F)) and F is differentiable at x, (3.5) implies that \u2207F(x) \u2208\u2202F(x).\nThis is in this case the only element of \u2202F(x).\nProposition 3.42 If F is differentiable at x \u2208int(dom(F)), then \u2202F(x) = {\u2207F(x)}.\nProof We need to prove that there is no other subgradient. Assume that \u2207F(x) exists\nand take y = x + \u03f5u in (3.41) (u \u2208Rd). One gets, for g \u2208\u2202F(x),\n\u03f5gT u \u2264F(x + \u03f5u) \u2212F(x) = \u03f5\u2207F(x)T u + o(\u03f5)\nDividing by |\u03f5| and letting \u03f5 \u21920 gives (depending on the sign of \u03f5)\ngT u \u2264\u2207F(x)T u and \u2212gT u \u2264\u2212\u2207F(x)T u\nThis is only possible if gT u = \u2207F(x)T u for all u \u2208Rd, which itself implies g = \u2207F.\n\u25a0\nThe next theorem, which is an obvious consequence of definition 3.41, character-\nizes minimizers of convex functions in the general case.\nTheorem 3.43 Let F : Rd \u2192R be convex. Then x is a (global) minimizer of F if and only\nif 0 \u2208\u2202F(x).\nThe following result shows that subgradients exist under generic conditions. We\nnote that g \u2208\u2202F(x) if and only if proj\u2212\u2212\u2192\naff (dom(F))(g) \u2208\u2202F, because (3.41) is trivial if\nF(y) = +\u221e. So \u2202F cannot be bounded unless aff(dom(D)) = Rd. However, it is the\npart of this set that is included in the \u2212\u2212\u2192\naff (dom(F)) that is of interest.\nProposition 3.44 For all x \u2208Rd, \u2202F(x) is a closed convex set (possibly empty, in par-\nticular for x < dom(F)). If x \u2208ridom(F), then \u2202F(x) , \u2205and \u2202F(x) \u2229\u2212\u2212\u2192\naff (dom(F)) is\ncompact.\n3.5. GENERAL CONVEX PROBLEMS\n73\nProof The convexity and closedness of \u2202F(x) is clear from the definition. If x \u2208\nridom(F), there exists \u03f5 > 0 such that x + \u03f5h \u2208ridom(F) for all h \u2208\u2212\u2212\u2192\naff (dom(F)) with\n|h| = 1. For all g \u2208\u2202F(x) \u2229\u2212\u2212\u2192\naff (dom(F)), one has\n|g| = max{gT h : h \u2208\u2212\u2212\u2192\naff (dom(F)),|h| = 1}\n\u2264max((F(x + \u03f5h) \u2212F(x))/\u03f5 : h \u2208\u2212\u2212\u2192\naff (dom(F)),|h| = 1)\nand the upper bound is finite because it is the maximum of a continuous function\nover a bounded set. This shows that \u2202F(x) is bounded. We defer the proof that\n\u2202F(x) , \u2205to section 3.7.\n\u25a0\nSubdifferentials are, under mild conditions, additive. More precisely, we have\nthe following proposition.\nTheorem 3.45 Let F1 and F2 be convex functions such that\nridom(F1) \u2229ridom(F2) , \u2205.\nThen, for all x \u2208Rd, \u2202(F1 + F2)(x) = \u2202F1(x) + \u2202F2(x).\nNote that the inclusion\n\u2202F1(x) + \u2202F2(x) \u2282\u2202(F1 + F2)(x)\nas can be immediately checked by summing the inequalities satisfied by subgradi-\nents. The reverse inclusion requires the use of separation theorems for convex sets\n(see section 3.7).\nAnother important point is how the chain rule works with compositions with\naffine functions.\nTheorem 3.46 Let F be a convex function on Rd, A a d \u00d7 m matrix and b \u2208Rd. Let\nG(x) = F(Ax + b), x \u2208Rm. Assume that there exists x0 \u2208Rm such that Ax0 \u2208ridom(F).\nThen, for all x \u2208Rm,\n\u2202G(x) = AT \u2202F(Ax + b).\nOne direction is straightforward and does not require the condition on ridom(F). If\ng \u2208\u2202F(Ax + b), then\nF(z) \u2212F(Ax + b) \u2265gT (z \u2212Ax \u2212b),z \u2208Rd\nand applying this inequality to z = Ay + b for y \u2208Rm yields\nG(y) \u2212G(x) \u2265gT A(y \u2212x)\nso that AT g \u2208\u2202G and AT \u2202F \u2282\u2202G. The reverse inclusion is proved in section 3.7.\nSubdifferentials can be seen as generalizations of normal cones.\n74\nCHAPTER 3. INTRODUCTION TO OPTIMIZATION\nProposition 3.47 Assume that \u2126is a closed convex subset of Rd. Then \u03c3\u2126(the indicator\nfunction of \u2126) has a subdifferential everywhere on \u2126with\n\u2202\u03c3\u2126(x) = N\u2126(x), x \u2208\u2126\nProof For x \u2208\u2126, (3.41) is\ngT (y \u2212x) \u2264\u03c3\u2126(y)\nfor y \u2208Rd, but since \u03c3\u2126(y) = +\u221eoutside of \u2126, g \u2208\u2202\u03c3\u2126(x) is equivalent to\ngT (y \u2212x) \u22640\nfor y \u2208\u2126, which is exactly the definition of the normal cone.\n\u25a0\nGiven this proposition, it is also clear (after noting that \u03c3\u21261 + \u03c3\u21262 = \u03c3\u21261\u2229\u21262) that\ntheorem 3.45 is a generalization of theorem 3.35.\n3.5.3\nDirectional derivatives\nFrom proposition 3.5, applied with y = x + h, we see that\nt 7\u21921\nt (F(x + th) \u2212F(x))\nis increasing as a function of t. This property allows us to define directional deriva-\ntives of F at x.\nDefinition 3.48 Let F be convex and x \u2208dom(F). The directional derivative of F at x in\nthe direction h \u2208Rd is defined by\ndF(x,h) = lim\nt\u21930\n1\nt (F(x + th) \u2212F(x)),\n(3.42)\nand belong to [\u2212\u221e,+\u221e].\nNote that, still from proposition 3.5, one has, for all x \u2208dom(F) and y \u2208Rd:\nF(y) \u2265F(x) + dF(x,y \u2212x)\n(3.43)\nWe have the proposition:\nProposition 3.49 If F is convex, then x\u2217\u2208argmin(F) if and only if dF(x\u2217,h) \u22650 for all\nh \u2208Rd.\n3.5. GENERAL CONVEX PROBLEMS\n75\nProof If dF(x\u2217,h) \u22650, then F(x\u2217+th)\u2212F(x\u2217) \u22650 for all t > 0 and this being true for all\nh implies that x\u2217is a minimizer. Conversely, if x\u2217is a minimizer, dF(x\u2217,h) is a limit\nof non-negative numbers and is therefore non-negative.\n\u25a0\nProposition 3.50 If F is convex and x \u2208dom(F), then dF(x,h) is positively homogeneous\nand subadditive (hence convex) as a function of h, namely\ndF(x,\u03bbh) = \u03bbdF(x,h),\u03bb > 0\nand\ndF(x,h1 + h2) \u2264dF(x,h1) + dF(x,h2).\nProof Positive homogeneity is straightforward and left to the reader. For the second\none, we can write\nF(x + th1 + th2) \u22641\n2(F(x + th1/2) + F(x + th2/2))\nby convexity so that\n1\nt (F(x + th1 + th2) \u2212F(x)) \u22641\n2\n\u00121\nt (F(x + th1/2) \u2212F(x)) + 1\nt (F(x + th2/2) \u2212F(x))\n\u0013\n.\nTaking t \u21930,\ndF(x;h1 + h2) \u22641\n2(dF(x;h1/2) + dF(x,h2/2)) = dF(x,h1) + dF(x,h2).\n\u25a0\nProposition 3.51 If F is convex and x \u2208dom(F), then\ndF(x,h) \u2265sup{gT h,g \u2208\u2202F(x)}.\nIf x \u2208ridom(F), then\ndF(x,h) = max{gT h,g \u2208\u2202F(x)}.\nProof If g \u2208\u2202F(x), then for all t > 0\nF(x + th) \u2212F(x) \u2265tgT h.\nDividing by t and passing to the limit yields dF(x,h) \u2265gT h.\nWe prove that the maximum is attained at some g \u2208\u2202F(x) when x \u2208ridom(F).\nIn this case, the domain of the convex function G : \u02dch 7\u2192dF(x, \u02dch) is the vector space\nparallel to aff(dom(F)), namely\ndom(G) = {h : x + h \u2208aff(dom(F))}.\n76\nCHAPTER 3. INTRODUCTION TO OPTIMIZATION\nIndeed, for any h in this set, there exists \u03f5 > 0 such that x + th \u2208dom(F) for 0 < t < \u03f5\nand dF(x,h) \u2264(F(x +th)\u2212F(x))/t < \u221e. Conversely, if h \u2208dom(G), then F(x +th)\u2212F(x)\nmust be finite for small enough t, so that x + th \u2208dom(F) and x + h \u2208aff(dom(F)).\nAs a consequence, for any h \u2208aff(dom(F)), there exists \u02c6g \u2208\u2202G(h), which therefore\nsatisfies\ndF(x, \u02dch) \u2265dF(x,h) + \u02c6gT (\u02dch \u2212h)\nfor any \u02dch \u2208Rd (the upper bound is infinite if \u02dch < dom(G)). Letting \u02dch \u21920, we get\ndF(x,h) \u2264\u02c6gT h.\nAlso, by positive homogeneity, we have\ntdF(x, \u02dch) \u2265dF(x,h) + \u02c6gT (t\u02dch \u2212h)\nfor all t > 0, which requires dF(x, \u02dch) \u2265\u02c6gT \u02dch for all \u02dch, and in particular dF(x,h) = \u02c6gT h.\nSince\nF(x + \u02dch) \u2212F(x) \u2265dF(x, \u02dch) \u2265\u02c6gT \u02dch\nwe see that \u02c6g \u2208\u2202F(x), with \u02c6gT h = dF(x,h), which concludes the proof.\n\u25a0\nThe next proposition gives a criterion for a vector g to belong to \u2202F(x) based on\ndirectional derivatives.\nProposition 3.52 Assume that x \u2208dom(F) where F is convex. If g \u2208Rd is such that\ndF(x,h) \u2265gT h\nfor all h \u2208Rd, then g \u2208\u2202F(x).\nProof Just use the fact that dF(x,h) \u2264F(x + h) \u2212F(x).\n\u25a0\n3.5.4\nSubgradient descent\nWhen F is a non-differentiable a convex function, directions g such that \u2212g \u2208\u2202F(x)\ndo not always provide directions of descent. Indeed, g \u2208\u2202F(x) implies\nF(x \u2212\u03b1g) \u2265F(x) \u2212\u03b1|g|2\nbut the inequality goes in the \u201cwrong direction.\u201d However, we know that, for any\nh \u2208Rd, there exists gh \u2208\u2202F(x) such that\ndF(x,\u2212h) = \u2212gT\nh h \u2265\u2212gT h\n3.5. GENERAL CONVEX PROBLEMS\n77\nfor all g \u2208\u2202F(x). As a consequence, any non-vanishing solution of the equation\nh = gh will provide a direction of descent. This suggests looking for h \u2208\u2202F(x) such\nthat h , 0 and |h|2 \u2264gT h for all g \u2208\u2202F(x). Since gT h \u2264|g||h|, this requires that |h| \u2264|g|\nfor all g \u2208\u2202F(x), i.e.,\nh = argmin\n\u2202F(x)\n(g 7\u2192|g|).\n(3.44)\nConversely, if h is the minimal-norm element of \u2202F(x) (which is necessarily unique\nsince the norm is strictly convex and \u2202F(x) is convex and compact), then |h|2 \u2264|h +\nt(g \u2212h)|2 for all g \u2208\u2202F(x) and t \u2208[0,1], and taking the difference yields\n2thT (g \u2212h) + t2|g \u2212h|2 \u22650.\nThe fact that this holds for all t \u22650 requires that hT (g \u2212h) \u22650 as required. We have\ntherefore proved that h defined by (3.44) is a descent direction for F at x (it is actually\nthe steepest descent direction: see [204] for a proof), justifying the algorithm\nxt+1 = xt \u2212\u03b1t argmin\n\u2202F(x)\n(g 7\u2192|g|)\nas subgradient descent iterations.\nExample. Consider the minimization of\nF(x) = \u03c8(x) + \u03bb\nn\nX\ni=1\n|x(i)|\nwhere \u03c8 is a C1 convex function on Rd. Let A(x) = {i : x(i) = 0}. Then\n\u2202F(x) =\n\uf8f1\uf8f4\uf8f4\uf8f4\uf8f2\uf8f4\uf8f4\uf8f4\uf8f3\n\u2207\u03c8(x) + \u03bb\nX\ni<A(x)\nsign(x(i)) + \u03bb\nX\ni\u2208A(x)\n\u03c1iei : |\u03c1i| \u22641,i \u2208A(x)\n\uf8fc\uf8f4\uf8f4\uf8f4\uf8fd\uf8f4\uf8f4\uf8f4\uf8fe\nwhere ei is the ith vector of the canonical basis of Rd.\nFor g = \u2207\u03c8(x) + \u03bbP\ni<A(x) sign(x(i)) + \u03bbP\ni\u2208A(x) \u03c1iei, we have\n|g|2 =\nX\ni<A(x)\n(\u2202iF(x) + \u03bbsign(x(i)))2 +\nX\ni\u2208A(x)\n(\u2202i\u03c8(x) \u2212\u03bb\u03c1i)2.\nDefine\ns(t) = sign(t)min(|t|,1).\nThen h satisfying (3.44) is given by\nh(i) =\n(\u2202i\u03c8(x) \u2212\u03bbsign(x(i)) if i < A(x)\n\u03bb s(\u2202i\u03c8(x)/\u03bb) if i \u2208A(x).\n78\nCHAPTER 3. INTRODUCTION TO OPTIMIZATION\nIn more complex situations, the extra minimization step at each iteration of the\nalgorithm can be challenging computationally. The following subgradient method\nuses an averaging approach to minimize F without requiring finding subgradients\nwith minimal norms. It simply defines\nxt+1 = xt \u2212\u03b1tgt,\ngt \u2208\u2202F(xt)\nand computes\n\u00afxt =\nPt\nj=1 \u03b1jxj\nPt\nj=1 \u03b1j\n.\nWe refer to [204] for a proof of convergence of this method.\n3.5.5\nProximal Methods\nProximal operator. We start with a few simple facts. Let F be a closed convex\nfunction and \u03c8 be convex and differentiable, with dom(\u03c8) = Rd. Let G = F + \u03c8.\nThen G is a closed convex function. Indeed, consider the sub-level set \u039ba(G) = {x :\nG(x) \u2264a} and assume that xn \u2192x with xn \u2208\u039ba(g). Then \u03c8(xn) \u2192\u03c8(x) by continuity,\nand for all \u03f5 > 0, we have, for large enough n, F(xn) \u2264a \u2212\u03c8(x) + \u03f5. This inequality\nremains true at the limit because F is closed, yielding G(x) \u2264a + \u03f5 for all \u03f5 > 0, so\nthat x \u2208\u039ba(G).\nWe have ridom(F) \u2229ridom(\u03c8) , \u2205so that (by theorem 3.45 and proposition 3.42)\n\u2202G(x) = \u2207\u03c8(x) + \u2202F(x). In particular, x\u2217is a minimizer of G if and only if \u2212\u2207\u03c8(x\u2217) \u2208\n\u2202F(x\u2217).\nIt one assumes that \u03c8 is strongly convex, so that there exists m and L such that\nm\n2 |y \u2212x|2 \u2264\u03c8(y) \u2212\u03c8(x) \u2212\u2207\u03c8(x)T (y \u2212x) \u2264L\n2|y \u2212x|2\nfor all x,y \u2208Rd, then a minimizer of G exists and is unique. To see this, fix x0 \u2208\nridom(F) and consider the closed convex set\n\u21260 = \u039bG(x0)(G) = {x : G(x) \u2264G(x0)}.\nAny minimizer of G must clearly belong to \u21260. If x \u2208\u21260, we have\nF(x) + \u03c8(x0) + \u2207\u03c8(x0)T (x \u2212x0) + m\n2 |x \u2212x0|2 \u2264G(x) \u2264G(x0).\nMoreover, there exists (from proposition 3.44) an element g \u2208\u2202F(x0) so that F(x) \u2265\nF(x0) + gT (x \u2212x0) for all x \u2208Rd. We therefore get\nF(x0) + \u03c8(x0) + (g + \u2207\u03c8(x0))T (x \u2212x0) + m\n2 |x \u2212x0|2 \u2264G(x0).\n3.5. GENERAL CONVEX PROBLEMS\n79\nfor all x \u2208\u21260, which shows that \u21260 must be bounded and therefore compact. There\nexists a minimizer x\u2217of G on \u21260, and therefore on all Rd. This minimizer is unique,\nsince the sum of a convex function and a strictly convex function is strictly convex.\nIn particular, for any closed convex F, we can apply the previous remarks to\nG : v 7\u2192F(v) + 1\n2|x \u2212v|2\nwhere x \u2208Rd is fixed. The function \u03c8 : v 7\u2192|v\u2212x|2/2 is strongly convex (with L = m =\n1) and G therefore has a unique minimizer v\u2217. This is summarized in the following\ndefinition.\nDefinition 3.53 Let F be a closed convex function. The proximal operator associated to\nF is the mapping proxF : Rd \u2192dom(F) defined by\nproxF(x) = argmin\nRd\n(v 7\u2192F(v) + 1\n2|x \u2212v|2).\n(3.45)\nFrom the previous discussion, we also deduce\nProposition 3.54 Let F be a closed convex function and \u03b1 > 0. We have x\u2032 = prox\u03b1F(x)\nif and only if x \u2208x\u2032 + \u03b1\u2202F(x\u2032). In particular, x\u2217is a minimizer of F if and only if x\u2217=\nprox\u03b1F(x\u2217)\nLet us take a few examples.\n\u2022 Let F(x) = \u03bb|x|, x \u2208Rd, for some \u03bb > 0. Then F is differentiable everywhere except\nat x = 0 and dom(F) = Rd. We have \u2202F(x) = \u03bbx/|x| for x , 0. A vector g belongs to\n\u2202F(0) if and only if\ngT x \u2264\u03bb|x|\nfor all x \u2208Rd, which is equivalent to |g| \u2264\u03bb so that \u2202F(0) = \u00afB(0,\u03bb).\nWe have x\u2032 = proxF(x) if and only if x\u2032 , 0 and x = x\u2032 + \u03bbx\u2032/|x\u2032| or x\u2032 = 0 and |x| \u2264\u03bb.\nFor |x| > \u03bb, the equation x = x\u2032 + \u03bbx\u2032/|x\u2032| is solved by\nx\u2032 = |x| \u2212\u03bb\n|x|\nx\nyielding\nproxF(x) =\n\uf8f1\uf8f4\uf8f4\uf8f4\uf8f2\uf8f4\uf8f4\uf8f4\uf8f3\n|x| \u2212\u03bb\n|x|\nx if |x| \u2265\u03bb\n0 otherwise\n(3.46)\n\u2022 Let \u2126be a closed convex set. Then prox\u03c3\u2126= proj\u2126, the projection operator on \u2126,\nas directly deduced from the definition.\n80\nCHAPTER 3. INTRODUCTION TO OPTIMIZATION\nThe following proposition can then be compared to proposition 3.38.\nProposition 3.55 Let F be a closed convex function. Then proxF is 1-Lipschitz: for all\nx,y \u2208Rd,\n|proxF(x) \u2212proxF(y)| \u2264|x \u2212y|.\n(3.47)\nProof Let x\u2032 = proxF(x) and y\u2032 = proxF(y). Then, there exists g \u2208\u2202F(x\u2032) and h \u2208\n\u2202F(y\u2032) such that x = x\u2032 + g and y = y\u2032 + h. Moreover, we have\nF(y\u2032) \u2212F(x\u2032) \u2265gT (y\u2032 \u2212x\u2032)\nF(x\u2032) \u2212F(y\u2032) \u2265hT (x\u2032 \u2212y\u2032)\nfrom which we deduce gT (y\u2032 \u2212x\u2032) \u2264hT (y\u2032 \u2212x\u2032) or (h \u2212g)T (x\u2032 \u2212y\u2032) \u22650. Expressing g,h\nin terms or x,x\u2032,y,y\u2032, we get (y \u2212x \u2212y\u2032 + x\u2032)T (y\u2032 \u2212x\u2032) \u22650 or\n|y\u2032 \u2212x\u2032|2 \u2264(y \u2212x)T (y\u2032 \u2212x\u2032) \u2264|y \u2212x||y\u2032 \u2212x\u2032|\nwhich is only possible if |y\u2032 \u2212x\u2032| \u2264[y \u2212x|.\n\u25a0\nIf F is differentiable, then x\u2032 = prox\u03b1F(x) satisfies\nx\u2032 = x \u2212\u03b1\u2207F(x\u2032)\nso that x 7\u2192prox\u03b1F(x) can be interpreted as an implicit version of the standard gra-\ndient step x 7\u2192x\u2212\u03b1\u2207F(x). The iterations x(t+1) = prox\u03b1tF(x(t)) provide an algorithm\nthat converges to a minimizer of F (this will be justified below). This algorithm is\nrarely practical, however, since the minimization required at each step is not nec-\nessarily much easier to perform than minimizing F itself. The proximal operator,\nhowever, is especially useful when combined with splitting methods.\nProximal gradient descent. Assume that the objective function F takes the form\nF(x) = G(x) + H(x)\n(3.48)\nwhere G is C1 on Rd and H is a closed convex function. We first note that\ndF(x,h) = lim\nt\u21930\nF(x + th) \u2212F(x)\nt\nis well defined (even if G is not convex, because it is smooth), with\ndF(x,h) = \u2207G(x)T h + dH(x,h)\n3.5. GENERAL CONVEX PROBLEMS\n81\nIn particular, if x\u2217be a minimizer of F, then dF(x,h) \u22650 for all h so that dH(x,h) \u2265\n\u2212\u2207G(x)T h for all h. Using proposition 3.52, this shows that \u2212\u2207G(x) \u2208\u2202H(x), which\nis a necessary condition for optimality for F (which is sufficient if G is convex).\nProximal gradient descent implements the algorithm\nxt+1 = prox\u03b1tH(xt \u2212\u03b1t\u2207G(xt)).\n(3.49)\nWe note that a stationary point of this algorithm, i.e. a point x such that x = prox\u03b1tH(x\u2212\n\u03b1t\u2207G(x)) must be such that x \u2212\u03b1t\u2207G(x) \u2208x + \u03b1t\u2202H(x), so that \u2212\u2207G(x) \u2208\u2202H(x). This\nshows that the property of being stationary does not depend on \u03b1t > 0, and is equiv-\nalent to the necessary optimality condition that was just discussed.\nWe first study this algorithm under the assumption that G is L-C1, which implies\nthat, for all x,y \u2208Rd.\nG(y) \u2264G(x) + \u2207G(x)T (y \u2212x) + L\n2|x \u2212y|2.\nAt iteration t, we have\nxt \u2212\u03b1t\u2207G(xt) \u2208xt+1 + \u03b1t\u2202H(xt+1)\nwhich implies, in particular\n\u03b1tH(xt)\u2212\u03b1tH(xt+1) \u2265(xt\u2212xt+1)T (xt\u2212xt+1\u2212\u03b1t\u2207G(xt)) = |xt\u2212xt+1|2+\u03b1t\u2207G(xt)T (xt+1\u2212xt)\nDividing by \u03b1t and adding G(xt) \u2212G(xt+1), we get\nF(xt) \u2212F(xt+1) \u22651\n\u03b1t\n|xt \u2212xt+1|2 + G(xt) + \u2207G(xt)T (xt+1 \u2212xt) \u2212G(xt+1)\n\u2265\n 1\n\u03b1t\n\u2212L\n2\n!\n|xt \u2212xt+1|2\n(3.50)\nso that proximal gradient descent iterations reduce the objective function as soon as\n\u03b1t \u22642/L.\nAssuming that \u03b1t < 2/L, (3.50) can be rewritten as\n\f\f\f\f\f\nxt+1 \u2212xt\n\u03b1t\n\f\f\f\f\f\n2\n\u2264\n2\n\u03b1t(2 \u2212\u03b1tL)(F(xt) \u2212F(xt+1).\nThis inequality should be compared to (3.11) in the unconstrained case. It yields, in\nparticular, the inequality\nmin\n(\f\f\f\f\f\nxt+1 \u2212xt\n\u03b1t\n\f\f\f\f\f : t \u2264T\n)\n\u2264\nF(x0) \u2212minF\n2T min{\u03b1t(2 \u2212\u03b1tL),t \u2264T} .\n(3.51)\n82\nCHAPTER 3. INTRODUCTION TO OPTIMIZATION\nAs a consequence, if one runs proximal gradient descent until |xt+1 \u2212xt|/\u03b1t is small\nenough, the algorithm will terminate in finite time as soon as \u03b1t is bounded from\nbelow (and, in particular, if \u03b1t is constant).\nIf we assume that G is convex, in addition to being L-C1, then we have a stronger\nresult. Let x\u2217be a minimizer of F. Then, using again xt\u2212\u03b1t\u2207G(xt) \u2208xt+1+\u03b1t\u2202H(xt+1),\nwe have\n\u03b1tH(x\u2217) \u2212\u03b1tH(xt+1) \u2265(x\u2217\u2212xt+1)T (xt \u2212xt+1 \u2212\u03b1t\u2207G(xt))\nand\n\u03b1tF(x\u2217) \u2212\u03b1tF(xt+1) \u2265(x\u2217\u2212xt+1)T (xt \u2212xt+1) \u2212\u03b1t(x\u2217\u2212xt+1)T \u2207G(xt)) + \u03b1tG(x\u2217) \u2212\u03b1tG(xt+1)\n\u2265(x\u2217\u2212xt+1)T (xt \u2212xt+1) \u2212\u03b1t(x\u2217\u2212xt)T \u2207G(xt)) + \u03b1tG(x\u2217)\n+ \u03b1t(xt+1 \u2212xt)T \u2207G(xt) \u2212\u03b1tG(xt+1)\n\u2265(x\u2217\u2212xt+1)T (xt \u2212xt+1) \u2212\u03b1tL\n2 |xt \u2212xt+1|2\nAssuming that \u03b1tL \u22641, then\n\u03b1tF(x\u2217) \u2212\u03b1tF(xt+1) \u2265(x\u2217\u2212xt+1)T (xt \u2212xt+1) \u22121\n2|xt \u2212xt+1|2 = 1\n2(|xt+1 \u2212x\u2217|2 \u2212|xt \u2212x\u2217|2),\nwhich we rewrite as\n\u03b1t(F(xt+1) \u2212F(x\u2217)) \u22641\n2(|xt \u2212x\u2217|2 \u2212|xt+1 \u2212x\u2217|2)\nNote that, from (3.50), we also have\nF(xt+1) \u2264F(xt) \u22121\n2\u03b1t\n|xt \u2212xt+1|2\nwhen \u03b1tL \u22641, which shows, in particular that F(xt) is decreasing. Fixing a time T,\nwe have, from these two observations\n\u03b1t(F(xT ) \u2212F(x\u2217)) \u22641\n2(|xt \u2212x\u2217|2 \u2212|xt+1 \u2212x\u2217|2)\nfor all t \u2264T \u22121, and summing over T ,\n(F(xT ) \u2212F(x\u2217))\nT \u22121\nX\nt=0\n\u03b1t \u22641\n2(|x0 \u2212x\u2217|2 \u2212|xT \u2212x\u2217|2)\nyielding\nF(xT ) \u2212F(x\u2217) \u2264|x0 \u2212x\u2217|2\n2PT\u22121\nt=0 \u03b1t\n.\n(3.52)\nWe summarize this in the following theorem, specializing to the case of constant\nstep \u03b1t.\n3.6. DUALITY\n83\nTheorem 3.56 Let G be am L-C1 function defined on Rd and H be closed convex. As-\nsume that F = G+H has a minimizer x\u2217. Then the algorithm (3.49) run with \u03b1t = \u03b1 \u22641/L\nfor all t is such that, for all T > 0,\nF(xT ) \u2212F(x\u2217) \u2264|x0 \u2212x\u2217|2\n2\u03b1T\n.\n(3.53)\nAlso, when G = 0, F = H, we retrieve the proximal iterations algorithm\nxt+1 = prox\u03b1F(xt),\n(3.54)\nand we have just proved that it converges for any \u03b1 > 0 as soon as F is a closed convex\nfunction.\nOne gets a stronger result under the assumption that G is C2, and is such that the\neigenvalues of \u22072G(x) are included in a fixed interval [m,L] for all x \u2208Rd with m > 0.\nSuch a G is strongly convex, which implies that F has a unique minimizer. We have\n|xt+1 \u2212x\u2217| =\n\f\f\fprox\u03b1tH(xt \u2212\u03b1t\u2207G(xt)) \u2212prox\u03b1tH(x\u2217\u2212\u03b1t\u2207G(x\u2217)\n\f\f\f\n\u2264|xt \u2212x\u2217\u2212\u03b1t(\u2207G(xt)) \u2212\u2207G(x\u2217))|.\nWrite\n|xt \u2212x\u2217\u2212\u03b1t(\u2207G(xt)) \u2212\u2207G(x\u2217)| =\n\f\f\f\f\f\f\nZ 1\n0\n(IdRn \u2212\u03b1t\u22072G(x\u2217+ t(xt \u2212x\u2217)))(xt \u2212x\u2217)dt\n\f\f\f\f\f\f\n\u2264\nZ 1\n0\n\f\f\f(IdRn \u2212\u03b1t\u22072G(x\u2217+ t(xt \u2212x\u2217)))(xt \u2212x\u2217)\n\f\f\fdt\n\u2264max(|1 \u2212\u03b1tm|,|1 \u2212\u03b1tL|)|xt \u2212x\u2217|\nwhere we have use the fact that the eigenvalues of IdRn \u2212\u03b1t\u22072G(x) are included in\n[1\u2212\u03b1tL,1\u2212\u03b1tm] for all x \u2208Rd. If one assumes that \u03b1t \u22641/L, so that max(|1\u2212\u03b1tm|,|1\u2212\n\u03b1tL|) \u22641 \u2212\u03b1tm, one gets\n|xt+1 \u2212x\u2217| \u2264(1 \u2212\u03b1tm)|xt \u2212x\u2217|.\nIterating this inequality, we get the theorem that we state for constant \u03b1t.\nTheorem 3.57 Let F = G + H where G is a C2 convex function and H is a closed convex\nfunction. Assume that the eigenvalues of \u22072G are uniformly included in [m,L] with m > 0.\nLet x\u2217argminF.\nLet (xt) satisfy (3.49) with constant \u03b1t = \u03b1 < 1/L. Then\n|xt \u2212x\u2217| \u2264(1 \u2212\u03b1m)t|x0 \u2212x\u2217|.\nNote that these results also apply to projected gradient descent (section 3.4.4),\nwhich is a special case (taking G = \u03c3\u2126).\n84\nCHAPTER 3. INTRODUCTION TO OPTIMIZATION\n3.6\nDuality\n3.6.1\nGeneralized KKT conditions\nA constrained convex minimization problem consists in the minimization of a closed\nconvex function F over a closed convex set \u2126\u2282ridom(F). We have seen in theo-\nrem 3.36 that, for smooth F, any solution x\u2217of this problem had to satisfy \u2212\u2207F(x\u2217) \u2208\nN\u2126(x) where\nN\u2126(x) = {h : hT (y \u2212x) \u22640 for all y \u2208\u2126}.\nThe next theorem generalizes this property to the non-smooth convex case, for\nwhich the necessary optimality condition is also sufficient.\nTheorem 3.58 Let F be a closed convex function, \u2126\u2282ridom(F) a nonempty closed con-\nvex set. Then x\u2217\u2208argmin\u2126F if and only if\n0 \u2208\u2202F(x\u2217) + N\u2126(x\u2217)\nProof Introduce the indicator function \u03c3\u2126. Then minimizing F over \u2126is the same\nas minimizing G = F+\u03c3\u2126over Rd. The assumptions imply that ridom(\u03c3\u2126) = relint(\u2126) \u2282\nridom(F) and therefore\n\u2202G(x) = \u2202F(x) + \u2202\u03c3\u2126(x)\nfor all x \u2208\u2126. Since\n\u2202\u03c3\u2126(x) = N\u2126(x)\nthe result follows for the characterization of minimum of convex functions.\n\u25a0\nIn the following, we will restrict to the situation in which F is finite (i.e., dom(F) =\nRd) and \u2126is defined through a finite number of equalities and inequalities, taking\nthe form\n\u2126=\nn\nx \u2208Rd : \u03b3i(x) = 0,i \u2208E and \u03b3i(x) \u22640,i \u2208I\no\nfor functions (\u03b3i,i \u2208C = E\u222aI) such that \u03b3i : x 7\u2192bT\ni x+\u03b2t is affine for all i \u2208E and \u03b3i is\nclosed convex for all i \u2208I. This is similar to the situation considered in section 3.4.1,\nwith additional convexity assumptions, but without assuming smoothness. We re-\ncall the definition of active constraints from section 3.4.1, namely, for x \u2208\u2126,\nA(x) = {i \u2208C : \u03b3i(x) = 0}.\nFollowing the discussion in the smooth case, define the set N \u2032\n\u03b3(x) \u2282Rd by\nN \u2032\n\u03b3(x) =\n\uf8f1\uf8f4\uf8f4\uf8f4\uf8f2\uf8f4\uf8f4\uf8f4\uf8f3\nX\ni\u2208A(x)\n\u03bbisi : si \u2208\u2202\u03b3i(x),i \u2208A(x),\u03bbi \u22650,i \u2208A(x) \u2229I\n\uf8fc\uf8f4\uf8f4\uf8f4\uf8fd\uf8f4\uf8f4\uf8f4\uf8fe\n.\n3.6. DUALITY\n85\nThe property 0 \u2208\u2202F(x\u2217) + N\u03b3(x\u2217) is the expression of the KKT conditions in the non-\nsmooth case. It holds for x\u2217\u2208argmin\u2126F as soon as N\u2126(x\u2217) = N \u2032\n\u03b3(x\u2217), which is true\nunder appropriate constraint qualifications. We here replace the MF-CQ in defini-\ntion 3.30 by the following conditions that do not involve gradients.\nDefinition 3.59 Let (\u03b3i,i \u2208C = E \u222aI) be a set of equality and inequality constraints,\nwith \u03b3i : x 7\u2192bT\ni x + \u03b2i, i,\u2208E and \u03b3i closed convex, i \u2208I. One says that these constraints\nsatisfy the Slater constraint qualifications (Sl-CQ) if and only if:\n(Sl 1) The vectors (bi,i \u2208E) are linearly independent.\n(Sl 2) There exists x \u2208Rd such that \u03b3i(x) = 0 for i \u2208E and \u03b3i(x) < 0 for i \u2208I.\nThe first constraint is a very mild condition. When it is not satisfied, this means that\nsome bi\u2019s are linear combinations of others, and equality constraints for the latter\nimplies equality constraints for the former. These redundancies can therefore be\nremoved without changing the problem.\nNote that (Sl2) can be replaced by the apparently weaker condition that, for all\ni \u2208I, there exists xi \u2208Rd satisfying all the constraints and \u03b3i(xi) < 0. Indeed, if\nthis is true, then the average, \u00afx, of (xi,i \u2208I) also satisfies the equality constraints by\nlinearity, and if i \u2208I,\n\u03b3i( \u00afx) \u22641\n|I|\nX\nj\u2208I\n\u03b3i(x(j)) \u22641\n|I|\u03b3i(x(i)) < 0.\nThe following proposition makes a connection between the Slater conditions and\nthe MF-CQ in definition 3.30.\nProposition 3.60 Assume that \u03b3i, i \u2208I are convex C1 functions. Then, if there exists a\nfeasible point x\u2217that satisfies the MF-CQ, there exists another point x satisfying the Sl-\nCQ. Conversely, if there exists x satisfying the Sl-CQ, then every feasible point x\u2217satisfies\nthe MF-CQ.\nProof The linear independence conditions on equality constraints are the same in\nMF-CQ and Sl-CQ, so that we only need to consider inequality constraints.\nLet x\u2217satisfy MF-CQ, and take h , 0 such that bT\ni h = 0 for all i \u2208E, and \u2207\u03b3i(x\u2217)T h <\n0, i \u2208A(x) \u2229I. Then x\u2217+ th satisfies the equality constraints for all t \u2208R. If i \u2208I is\nnot active, then \u03b3i(x\u2217) < 0 and this will remain true at x\u2217+ th for small t by continu-\nity. If i \u2208A(x) \u2229I, then a first order expansion gives \u03b3i(x\u2217+ th) = t\u2207\u03b3i(x\u2217)T h + o(|h|),\nwhich is also negative for small enough t > 0. So, x\u2217+th satisfies the Sl-CQ for small\nenough t > 0.\n86\nCHAPTER 3. INTRODUCTION TO OPTIMIZATION\nConversely, let x satisfy the Sl-CQ. Take a feasible point x\u2217. If x\u2217= x, then there\nis no active inequality constraint and x\u2217satisfies MF-CQ. Assume x\u2217, x and let\nh = x \u2212x\u2217. Then bT\ni h = 0 for all i \u2208E, and if i \u2208I \u2229A(x\u2217),\n0 > \u03b3i(x) = \u03b3i(x\u2217+ h) \u2265\u03b3i(x\u2217) + \u2207\u03b3i(x\u2217)T h = \u2207\u03b3i(x\u2217)T h\nso that x\u2217satisfies MF-CQ.\n\u25a0\nThe following theorem, that we give without proof, states that the Slater condi-\ntions implies that the KKT conditions are satisfied for a minimizer.\nTheorem 3.61 Assume that all the constraints are affine, or that they satisfy the Sl-CQ\nin definition 3.59. Let x\u2217\u2208argmin\u2126F. Then N\u2126(x\u2217) = N \u2032\n\u03b3(x\u2217), so that there exist s0 \u2208\n\u2202F(x\u2217), si \u2208\u2202\u03b3i(x\u2217), i \u2208A(x\u2217), (\u03bbi,i \u2208A(x\u2217)) with \u03bbi \u22650 if i \u2208I \u2229A(x\u2217), such that\ns0 +\nX\ni\u2208A(x)\n\u03bbisi = 0\n(3.55)\n3.6.2\nDual problem\nConsider the Lagrangian\nL(x,\u03bb) = F(x) +\nX\ni\u2208C\n\u03bbi\u03b3i(x)\ndefined in (3.29) and let D = {\u03bb : \u03bbi \u22650,i \u2208I}. Because the functions \u03b3i are non-\npositive on \u2126, we have\nL(x,\u03bb) \u2264F(x)\nfor all x \u2208\u2126and \u03bb \u2208D, which implies that\nL\u2217(\u03bb) = inf{L(x,\u03bb) : x \u2208Rd}\nis such that L\u2217(\u03bb) \u2264F(x) for all \u03bb \u2208D and x \u2208\u2126. Define\n\u02c6d = sup{L\u2217(\u03bb) : \u03bb \u2208D}\nand\n\u02c6p = inf{F(x) : x \u2208\u2126},\nwhose computations respectively represent the dual and primal problems. Then, we\nhave \u02c6d \u2264\u02c6p.\nWe did not need much of our assumptions (not even F to be convex) to reach\nthis conclusion. When the converse inequality is true (so that the duality gap \u02c6p \u2212\u02c6d\nvanishes), the dual problem provides important insights on the primal problem, as\nwell as alternative ways to solve it. This is true under the Slater conditions.\n3.6. DUALITY\n87\nTheorem 3.62 The duality gap vanishes when the constraints are all affine, or when they\nsatisfy the Sl-CQ in definition 3.59. In this case, any solution \u03bb\u2217of the dual problem\nprovides Lagrange multipliers in theorem 3.61 and conversely.\nWe justify this statement, as a consequence of theorem 3.61 and the following\nanalysis. The Lagrangian L(x,\u03bb) is linear in \u03bb, and when \u03bb \u2208D, is a convex func-\ntion of x. Moreover, one can use subdifferential calculus (theorem 3.45) to con-\nclude that, for any \u03bb \u2208D, (3.55) expresses the fact that 0 \u2208\u2202xL(x\u2217,\u03bb), i.e., that\nx\u2217\u2208argminRd L(\u00b7,\u03bb).\nFixing x \u2208Rd, one can also consider the maximization of L in \u03bb \u2208D. Clearly, if\nx < \u2126, so that \u03b3i(x) , 0 for some i \u2208E or \u03b3i(x) > 0 for some i \u2208I, then maxD L(x,\u03bb) =\n+\u221e. If x \u2208\u2126, then the slackness conditions, which require \u03bb(i)\u03b3i(x) = 0, i \u2208I, ensure\nthat \u03bb \u2208argmaxD L(x,\u00b7).\nAs a consequence, any pair x\u2217\u2208\u2126, \u03bb\u2217\u2208D satisfying the KKT conditions is such\nthat\nL(x\u2217,\u03bb) \u2264L(x\u2217,\u03bb\u2217) \u2264L(x,\u03bb\u2217)\n(3.56)\nfor all x \u2208Rd and \u03bb \u2208D. Such a pair (x\u2217,\u03bb\u2217) is called a saddle point of the function L.\nConversely, any saddle point of L, i.e., any (x\u2217,\u03bb\u2217) \u2208Rd \u00d7D satisfying (3.56), must be\nsuch that x\u2217\u2208\u2126(to ensure that L(x\u2217,\u00b7) is bounded), and satisfies the KKT conditions.\nWe therefore obtain the equivalence of the two properties, for (x\u2217,\u03bb\u2217) \u2208Rd \u00d7 D:\n(i) x\u2217\u2208\u2126and (x\u2217,\u03bb\u2217) satisfies the KKT conditions.\n(ii) Equation (3.56) holds for all (x,\u03bb) \u2208Rd \u00d7 D.\nConsider now the additional condition that\n(iii) x\u2217\u2208argmin\u2126F and \u03bb\u2217\u2208argmaxD L\u2217.\nWe already know that, if (x\u2217,\u03bb\u2217) satisfy the KKT conditions, then x\u2217\u2208argmin\u2126F\n(because N \u2032\n\u03b3(x\u2217) \u2282N\u2126(x\u2217)). Moreover, if (3.56) holds, then the inequality L(x\u2217,\u03bb) \u2264\nL(x\u2217,\u03bb\u2217) implies that L\u2217(\u03bb) \u2264L(x\u2217,\u03bb\u2217) for all \u03bb \u2208D. The inequality L(x\u2217,\u03bb\u2217) \u2264L(x,\u03bb\u2217)\nfor all x implies that L(x\u2217,\u03bb\u2217) \u2264L\u2217(\u03bb\u2217). We therefore obtain the fact that \u03bb\u2217\u2208argmaxL\u2217(\u03bb).\nTo summarize, we have\n(i) \u21d4(ii) \u21d2(iii).\nTo obtain the final equivalence, we need to assume constraints qualifications,\nsuch as Slater\u2019s conditions, to ensure that N \u2032\n\u03b3(x\u2217) = N\u2126(x\u2217). If this holds, then (iii)\nimplies (via theorem 3.61) that there exists \u02dc\u03bb such that (i) and (ii) are satisfied for\n88\nCHAPTER 3. INTRODUCTION TO OPTIMIZATION\n(x\u2217, \u02dc\u03bb), with L(x\u2217, \u02dc\u03bb) = L\u2217( \u02dc\u03bb) and \u02dc\u03bb \u2208argminD L\u2217. This shows that L\u2217( \u02dc\u03bb) = L\u2217(\u03bb\u2217). More-\nover, from (3.56), we have\nL(x\u2217,\u03bb\u2217) \u2264L(x\u2217, \u02dc\u03bb) = L\u2217( \u02dc\u03bb),\nand, by definition of L\u2217, L(x\u2217,\u03bb\u2217) \u2265L\u2217(\u03bb\u2217). This shows that L(x\u2217,\u03bb\u2217) = L(x\u2217, \u02dc\u03bb). As a\nconsequence, for all (x,\u03bb) \u2208Rd \u00d7 D:\nL(x\u2217,\u03bb) \u2264L(x\u2217, \u02dc\u03bb) = L(x\u2217,\u03bb\u2217) = L\u2217(\u03bb\u2217) = inf\nRd L(\u00b7,\u03bb\u2217) \u2264L(x,\u03bb\u2217)\nso that (x\u2217,\u03bb\u2217) satisfies (ii).\n3.6.3\nExample: Quadratic programming\nQuadratic programming problems minimize F(x) = 1\n2xT Ax \u2212bT x, where A is a pos-\nitive semidefinite matrix and b \u2208Rd, subject to affine constraints cT\ni x \u2212di = 0, i \u2208E\nand cT\ni x \u2212di \u22640, i \u2208I.\nWe here consider the following objective function. Introduce variables x \u2208Rd,\nx0 \u2208R and \u03be \u2208RN and minimize, for a fixed parameter \u03b3,\nF(x,x0,\u03be) = 1\n2|x|2 + \u03b3\nN\nX\nk=1\n\u03be(k)\nsubject to constraints, for k = 1,...,N \u03be(k) \u22650 and\nbk(x0 + xT ak) + \u03be(k) \u22651\nwhere bk \u2208{\u22121,1} and ak \u2208Rn respectively denote the kth output and input train-\ning sample. This algorithm minimizes a quadratic function of the input variables\n(x,x0,\u03be) subject to linear constraints, and is an instance of a quadratic program-\nming problem (this is actually the support vector machine problem for classification,\nwhich will be described in section 8.4.1).\nIntroduce Lagrange multipliers \u03b7k for the constraint \u03be(k) \u22650 and \u03b1k for bk(x0 +\nxT ak) + \u03be(k) \u22651. The Lagrangian then takes the form\nL(x,x0,\u03be,\u03b1,\u03b7) = 1\n2|x|2 + \u03b3\nN\nX\nk=1\n\u03be(k) \u2212\nN\nX\ni=1\n\u03b7k\u03be(k) \u2212\nN\nX\nk=1\n\u03b1k(bk(x0 + xT ak) + \u03be(k) \u22121)\n= 1\n2|x|2 +\nN\nX\nk=1\n(\u03b3 \u2212\u03b7k \u2212\u03b1k)\u03be(k) \u2212x0\nN\nX\nk=1\n\u03b1kbk \u2212xT\nN\nX\nk=1\n\u03b1kbkak +\nN\nX\nk=1\n\u03b1k.\n3.6. DUALITY\n89\nWe compute the dual Lagrangian L\u2217by minimizing with respect to the primal vari-\nables. We note that L\u2217(\u03b1,\u03b7) = \u2212\u221ewhen PN\nk=1\nalphakbk , 0, so that PN\nk=1 \u03b1kbk = 0 is a constraint for the dual problem. The min-\nimization in \u03be(k) also gives \u2212\u221eunless \u03b3 \u2212\u03b7k \u2212\u03b1k = 0, which is therefore another\nconstraint. Finally, the optimal values of x is\nx =\nN\nX\nk=1\n\u03b1kbkak\nand we obtain the expression of the dual problem, which maximizes\n\u22121\n2\nN\nX\nk,l=1\n\u03b1k\u03b1lbkblaT\nk al +\nN\nX\nk=1\n\u03b1k\nsubject to \u03b7k,\u03b1k \u22650, \u03b3 \u2212\u03b7k \u2212\u03b1k = 0 and PN\nk=1 \u03b1kbk = 0. The conditions on \u03b7k and \u03b1k\ncan be rewritten as 0 \u2264\u03b1k \u2264\u03b3, \u03b7k = \u03b3 \u2212\u03b1k, and since the rest of the problem does\nnot depends on \u03b7, the dual problem can be reduced to maximizing\nL\u2217(\u03b1) = \u22121\n2\nN\nX\nk,l=1\n\u03b1k\u03b1laT\nk al +\nN\nX\nk=1\n\u03b1k\nsubject to 0 \u2264\u03b1k \u2264\u03b3 and PN\nk=1 \u03b1kbk = 0.\n3.6.4\nProximal iterations and augmented Lagrangian\nThe concave function L\u2217can be maximized by minimizing \u2212L\u2217using proximal itera-\ntions ((3.54)):\n\u03bb(t + 1) = prox\u2212\u03b1tL\u2217(\u03bb(t)) = argmax\nD\n(\u03bb 7\u2192L\u2217(\u03bb) \u22121\n2\u03b1t\n|\u03bb \u2212\u03bb(t)|2).\nIntroduce the function\n\u03d5(x,\u03bb) = F(x) +\nX\ni\u2208C\n\u03bb(i)\u03b3i(x) \u22121\n2\u03b1t\n|\u03bb \u2212\u03bb(t)|2\nso that\n\u03bb(t + 1) = argmax\n\u00b5\u2208D\ninf\nx\u2208Rn \u03d5(x,\u00b5).\nThe function \u03d5 is convex in x and strongly concave in \u00b5. Results in \u201cminimax\ntheory\u201d [27] implies that one has the equality\nmax\n\u00b5\u2208D inf\nx\u2208Rn \u03d5(x,\u00b5) = inf\nx\u2208Rd sup\n\u00b5\u2208D\n\u03d5(x,\u00b5)\n(3.57)\n90\nCHAPTER 3. INTRODUCTION TO OPTIMIZATION\n(Note that the left-hand side of this equation is never larger than the right-hand\nside, but their equality requires additional hypotheses\u2014which are satisfied in our\ncontext\u2014in order to hold.)\nImportantly, the maximization in \u00b5 in the right-hand side has a closed form so-\nlution. It requires to maximize\nX\ni\u2208C\n\u0012\n\u00b5i\u03b3i(x) \u22121\n2\u03b1t\n(\u00b5i \u2212\u03bbi(t))2\u0013\nsubject to \u00b5i \u22650 for i \u2208I, and each \u00b5i can be computed separately. For i \u2208E, there is\nno constraint on \u00b5i, and one finds\n\u00b5i = \u03bbi(t) + \u03b1t\u03b3i(x),\nand\n\u00b5i\u03b3i(x) \u22121\n2\u03b1t\n(\u00b5i \u2212\u03bbi(t))2 = \u03bbi(t)\u03b3i + \u03b1t\n2 \u03b3i(x)2 =\n1\n2\u03b1t\n(\u03bbi(t) + \u03b1t\u03b3i(x))2 \u2212\u03bbi(t)2\n2\u03b1t\n.\nFor i \u2208I, the solution is\n\u00b5i = max(0,\u03bbi(t) + \u03b1t\u03b3i(x))\nand one can check that, in this case:\n\u00b5i\u03b3i(x) \u22121\n2\u03b1t\n(\u00b5i \u2212\u03bbi(t))2 =\n1\n2\u03b1t\nmax(0,\u03bbi(t) + \u03b1t\u03b3i(x))2 \u2212\u03bbi(t)2\n2\u03b1t\nAs a consequence, the right-hand side of (3.57) requires to minimize\nG(x) = F(x) + 1\n2\u03b1t\nX\ni\u2208E\n(\u03bbi(t) + \u03b1t\u03b3i(x))2 + 1\n2\u03b1t\nX\ni\u2208I\nmax(0,\u03bbi(t) + \u03b1t\u03b3i(x)))2\n\u22121\n2\u03b1t\nX\ni\u2208C\n\u03bbi(t)2.\nIf we assume that the sub-level sets {x \u2208\u2126: F(x) \u2264\u03c1} are bounded (or empty) for any\n\u03c1 \u2208R, then so are the sets {x \u2208Rn : G(x) \u2264\u03c1}, and this is a sufficient condition for the\nexistence of a saddle point for \u03d5, which is a pair (x\u2217,\u03bb\u2217) such that, for all (x,\u03bb) \u2208Rn\u00d7D,\n\u03d5(x\u2217,\u03bb) \u2264\u03d5(x\u2217,\u03bb\u2217) \u2264\u03d5(x,\u03bb\u2217).\nOne can then check that this implies that x\u2217\u2208argminRn G while \u03bb\u2217= \u03bb(t + 1), so that\n3.6. DUALITY\n91\nthe latter can be computed as follows:\n\uf8f1\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f2\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f3\nx(t) = argmin\nx\u2208Rn\n(\nF(x) + 1\n2\u03b1t\nX\ni\u2208E\n(\u03bbi(t) + \u03b1t\u03b3i(x))2\n+ 1\n2\u03b1t\nX\ni\u2208I\nmax(0,\u03bbi(t) + \u03b1t\u03b3i(x)))2\n)\n\u03bbi(t + 1) = \u03bbi(t) + \u03b1t\u03b3i(x(t)), i \u2208E\n\u03bbi(t + 1) = max(0,\u03bbi(t) + \u03b1t\u03b3i(x(t))), i \u2208I\n(3.58)\nThese iterations define the augmented Lagrangian algorithm. Starting this algorithm\nwith some \u03bb(0) \u2208R|C|, and constant \u03b1, \u03bb(t) will converge to a solution \u02c6\u03bb of the dual\nproblem. The last two iterations stabilizing imply that \u03b3i(x(t)) converges to 0 for\ni \u2208E, and also for i \u2208I such that \u02c6\u03bbi > 0, and that limsup\u03b3i(x(t)) = 0 otherwise. This\nshows that, if x(t) converges to a limit \u02dcx, then G( \u02dcx) = F( \u02dcx). However, for any x \u2208\u2126,\nwe have\nG(x(t)) \u2264G(x) \u2264F(x)\n(the proof being left to the reader), showing that \u02dcx \u2208argmin\u2126F.\nNote that the augmented Lagrangian method can also be used in non-convex\noptimization problems [146], requiring in that case that \u03b1 is small enough.\n3.6.5\nAlternative direction method of multipliers\nWe return to a situation considered in section 3.5.5 where the function to minimize\ntakes the form F(x) = G(x) + H(x). Here, we do not assume that G or H is smooth,\nbut we will need their respective proximal operators to be easy to compute.\nThe problem can be reformulated as a minimization with equality constraints,\nnamely that of minimizing \u02dcF(x,z) = G(x) + F(z) subject to x = z. We will actually\nconsider a more general situation, namely the problem minimizing a function \u02dcF(x,z)\nsubject to constraints Ax + Bz = c where A and B are respectively d \u00d7 n and d \u00d7 m\nmatrices, x \u2208Rn, z \u2208mRm, c \u2208Rd. The augmented Lagrangian algorithm applied to\nthis problem leads to iterate (with only equality constraints)\n\uf8f1\uf8f4\uf8f4\uf8f4\uf8f2\uf8f4\uf8f4\uf8f4\uf8f3\nxt,zt = argmin{G(x) + F(z) + 1\n2\u03b1t\n|\u03bbt + \u03b1t(Ax + Bz \u2212c)|2}\n\u03bbt+1 = \u03bbt + \u03b1t(Axt + Bzt \u2212c)\nwith \u03bbt \u2208Rd.\n92\nCHAPTER 3. INTRODUCTION TO OPTIMIZATION\nOne can now consider splitting the first step in two and iterate:\n\uf8f1\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f2\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f3\nxt = argmin{G(x) + F(zt\u22121) + 1\n2\u03b1t\n|\u03bbt + \u03b1t(Ax + Bzt\u22121 \u2212c)|2}\nzt = argmin{G(xt) + F(z) + 1\n2\u03b1t\n|\u03bbt + \u03b1t(Axt + Bz \u2212c)|2}\n\u03bbt+1 = \u03bbt + \u03b1t(Axt + Bzt \u2212c)\n(3.59)\nThese iterations constitute the \u201calternative direction method of multipliers,\u201d or ADMM\n(it is also sometimes called Douglas-Rachford splitting). It is not equivalent to the\naugmented Lagrangian algorithm (one would need to iterate a large number of times\nover the first two steps before applying the third one for this), but still satisfies good\nconvergence properties. The reader can refer to Boyd et al. [40] for a relatively el-\nementary proof that shows that this algorithm converges as soon as, in addition to\nthe hypotheses that were already made, the Lagrangian\nL(x,z,\u03bb) = G(x) + H(z) + \u03bbT (Ax + Bz \u2212c)\nhas a saddle point: there exists x\u2217,z\u2217,y\u2217such that\nmax\ny\nL(x\u2217,z\u2217,\u03bb) = L(x\u2217,z\u2217,\u03bb\u2217) = min\nx,z L(x,z,\u03bb\u2217).\n3.7\nConvex separation theorems and additional proofs\nWe conclude this chapter by completing some of the proofs left aside when discus-\nsion convex functions. These proofs use convex separation theorems, stated below\n(without proof).\nTheorem 3.63 (c.f., Rockafellar [167]) Let \u21261 and \u21262 be two nonempty convex sets\nwith relint(\u21261) \u2229relint(\u21262) = \u2205. Then there exists b \u2208Rd and \u03b2 \u2208R such that b , 0,\nbT x \u2264\u03b2 for all x \u2208\u21261 and bT x \u2265\u03b2 for all x \u2208\u21262, with a strict inequality for at least one\nx \u2208\u21261 \u222a\u21262.\nTheorem 3.64 Let \u21261 and \u21262 be two nonempty convex sets with \u21261 \u2229\u21262 = \u2205and \u21261\ncompact. Then there exists b \u2208Rn, \u03b2 \u2208R and \u03f5 < 0 such that bT x \u2264\u03b2 \u2212\u03f5 for all x \u2208\u21261\nand bT x \u2265\u03b2 + \u03f5 for all x \u2208\u21262.\n3.7.1\nProof of proposition 3.44\nWe start with a few general remarks. If x \u2208Rd, the set {x} is convex and relint({x}) =\n{x}. If \u2126is any convex set such that x < relint(\u2126), then theorem 3.63 implies that\nthere exist b \u2208Rd and \u03b2 \u2208R such that bT y \u2265\u03b2 \u2265bT x for all y \u2208\u2126(with bT y > bT x for\n3.7. CONVEX SEPARATION THEOREMS AND ADDITIONAL PROOFS\n93\nat least one y). If x is in \u2126\\ (relint(\u2126)) (so that x is a point on the relative boundary\nof \u2126), then, necessarily bT x = \u03b2 and we can write\nbT y \u2265bT x\nfor all y \u2208\u2126with a strict inequality for some y \u2208\u2126. One says that b and \u03b2 provide a\nsupporting hyperplane for \u2126at x.\nNow, if F is a convex function, with\nepi(F) = {(y,a) \u2208Rd \u00d7 R : F(y) \u2264a}\nthen\nrelint(epi(F)) = {(y,a) \u2208ridom(F) \u00d7 R : F(y) < a}\n(this simple fact is proved in lemma 3.65 below). In particular, if x \u2208dom(F), then\n(x,F(x)) must be in the relative boundary of epi(F). This implies that there exists\n(b,b0) , (0,0) \u2208Rd \u00d7 R such that, for all (y,a) \u2208epi(F):\nbT y + b0a \u2265bT x + b0F(x).\nIf one assumes that x \u2208ridom(F), then, necessarily, b0 , 0. To show this, assume\notherwise, so that bT y \u2265bT x for all y \u2208dom(F), with b , 0. We get a contradiction\nusing the fact that, for some \u03f5 > 0, [y,x\u2212\u03f5(y\u2212x)] belongs to dom(\u2126), because bT (y\u2212x)\ncannot have a constant sign on this segment.\nSo b0 , 0 and necessarily b0 > 0 to ensure that bT y + b0a is bounded from below\nfor all a \u2265F(y). Without loss of generality, we can assume b0 = 1 and we get, for all\ny \u2208dom(F)\nF(y) + bT y \u2265F(x) + bT x\nwhich shows that \u2212b \u2208\u2202F(x), justifying the fact that \u2202F(x) , \u2205for x \u2208ridom(F).\nWe now state and prove the result announced above on the relative interior of\nthe epigraph of a convex function.\nLemma 3.65 Let F be a convex function with epigraph\nepi(F) = {(y,a) : y \u2208dom(F),F(y) \u2264a}.\nThen\nrelint(epi(G)) = {(y,a) : y \u2208ridom(F),F(y) < a}.\nProof Let \u0393 = {(y,a) : y \u2208ridom(F),F(y) < a}. Assume that (y,a) \u2208relint(epi(F)).\nThen (y,b) \u2208epi(F) for all b > a and there exists \u03f5 > 0 such that (y,a) \u2212\u03f5((y,b) \u2212\n(y,a)) \u2208epi(F) which requires that F(y) \u2264a \u2212\u03f5(b \u22121) < a. Now, take x \u2208dom(F).\n94\nCHAPTER 3. INTRODUCTION TO OPTIMIZATION\nThen, (x,F(x)) \u2208epi(dom(F)) and (y,a) \u2212\u03f5((x,F(x)) \u2212(y,a)) \u2208epi(F) for small enough\n\u03f5, showing that F(y \u2212\u03f5(x \u2212y)) \u2264(1+\u03f5)a\u2212\u03f5F(x) and y \u2212\u03f5(x \u2212y) \u2208dom(F). This proves\nthat y \u2208ridom(F) and the fact that relint(epi(F)) \u2282\u0393.\nTake (y,a) \u2208\u0393, and (x,b) \u2208epi(F). We need to show that (y \u2212\u03f5(x \u2212y),a \u2212\u03f5(b \u2212a)) \u2208\nepi(F) for small enough \u03f5, i.e., that\nF(y \u2212\u03f5(x \u2212y)) \u2264a \u2212\u03f5(b \u2212a)\nfor small enough \u03f5. But this is an immediate consequence of the facts that F is\ncontinuous at y \u2208ridom(G) and F(y) < a.\n\u25a0\n3.7.2\nProof of theorem 3.45\nAssume that there exists \u00afx \u2208ridom(F1)\u2229ridom(F2). Take x \u2208dom(F1)\u2229dom(F2) and\ng \u2208\u2202(F1 + F2)(x). We want to show that g = g1 + g2 with g1 \u2208\u2202F1(x) and g2 \u2208\u2202F2(x).\nBy definition, we have\nF1(y) + F2(y) \u2265F1(x) + F2(x) + gT (y \u2212x)\nfor all y. We want to decompose g as g = g1 + g2 with g1 \u2208\u2202F1(x) and g2 \u2208\u2202F2(x).\nEquivalently, we want to find g2 \u2208Rd such that, for all y \u2208Rd,\nF1(y) \u2265F1(x) + (g \u2212g2)T (y \u2212x)\nF2(y) \u2265F2(x) + gT\n2 (y \u2212x)\nFirst note that we can replace F1 by y 7\u2192F1(y) \u2212F1(x) \u2212gT (y \u2212x) and F2 by y 7\u2192\nF2(y) \u2212F2(x) and assume with loss of generality that F1(x) = F2(x) = 0 and g = 0.\nMaking this assumption, we need to find g2 such that\nF1(y) \u2265\u2212gT\n2 (y \u2212x)\nF2(y) \u2265gT\n2 (y \u2212x)\nfor all y \u2208Rd and some g2 \u2208Rd, under the assumption that F1(y) + F2(y) \u22650 for all\ny. Introduce the two convex sets in Rd \u00d7 R\n\u21261 = epi(F1) = {(y,a) \u2208Rd \u00d7 R : F1(y) \u2264a}\n\u21262 = {(y,a) \u2208Rd \u00d7 R : F2(y) \u2264\u2212a}.\nThe set \u21262 is the image of epi(F2) by the transformation (y,a) 7\u2192(y,\u2212a). We have\nrelint(\u21261) = epi(F1) = {(y,a) \u2208ridom(F1) \u00d7 R : F1(y) < a}\nrelint(\u21262) = {(y,a) \u2208ridom(F2) \u00d7 R : F2(y) < \u2212a}.\n3.7. CONVEX SEPARATION THEOREMS AND ADDITIONAL PROOFS\n95\nSince F1 + F2 \u22650, \u21261 and \u21262 have non-intersecting relative interiors. We can apply\nthe first separation theorem, providing \u00afb = (b,b0) \u2208Rd \u00d7 R and \u03b2 \u2208R such that\n\u00afb , (0,0), bT y + b0a \u2212\u03b2 \u22640 for (y,a) \u2208\u21261 and bT y + b0a \u2212\u03b2 \u22650 for (y,b) \u2208\u21262, with a\nstrict inequality for at least one point in \u21261 \u222a\u21262. We therefore obtain the fact that,\nfor all y and a,\nF1(y) \u2264a \u21d2bT y + b0a \u2212\u03b2 \u22640\nF2(y) \u2264\u2212a \u21d2bT y + b0a \u2212\u03b2 \u22650.\nWe claim that b0 , 0. Indeed, if b0 = 0, the statement for F1 would imply that bT y \u2212\n\u03b2 \u22640 for all y \u2208dom(F1) and the one on F2 that bT y \u2212\u03b2 \u22650 for y \u2208dom(F2). The\npoint \u00afx \u2208relint(\u21261) \u2229relint(\u21262) should then satisfy bT \u00afx \u2212\u03b2 = 0. We know that there\nexists a point y \u2208\u21261 \u222a\u21262 such that bT y , \u03b2. Assume that y \u2208\u21261, so that bT y \u2212\u03b2 < 0\nand take \u03f5 > 0 such that \u02dcy = \u00afx \u2212\u03f5(y \u2212\u00afx) \u2208\u21261. Then\nbT \u02dcy \u2212\u03b2 = \u2212\u03f5(bT y \u2212\u03b2) < 0,\nwhich is a contradiction. A similar contradiction is obtained when y belongs to \u21262,\nyielding the fact that b0 cannot vanish.\nMoreover, we clearly need b0 < 0 to ensure that bT y + b0a \u2212\u03b2 \u22640 for all large\nenough a if y \u2208dom(\u21261). There is then no loss of generality in assuming b0 = \u22121 and\nwe get\nF1(y) \u2264a \u21d2bT y \u2212\u03b2 \u2264a\nF2(y) \u2264\u2212a \u21d2bT y \u2212\u03b2 \u2265a,\nwhich is equivalent to\n\u2212F2(y) \u2264bT y \u2212\u03b2 \u2264F1(y)\nTaking y = x gives \u03b2 = bT x and we get the desired inequality with g2 = \u2212b.\n3.7.3\nProof of theorem 3.46\nLet \u00afx \u2208Rm such that A \u00afx \u2208ridom(F). We need to prove that \u2202G(x) \u2282AT \u2202F(Ax + b)\nwhen G(x) = F(Ax + b). We assume in the following that b = 0, since the theorem\nwith G(x) = F(x + b) is obvious. If g \u2208\u2202G(x), we have\nF(Ay) \u2265F(Ax) + gT (y \u2212x)\nfor all y \u2208Rm. We want to show that there exists h \u2208Rd such that g = AT h and, for\nall z \u2208Rd,\nF(z) \u2265F(Ax) + hT (z \u2212Ax) = F(Ax) + hT z \u2212gT x.\n96\nCHAPTER 3. INTRODUCTION TO OPTIMIZATION\nLet \u21261 = epi(F) = {(z,a) :,z \u2208Rd,F(z) \u2264a} and\n\u21262 = {(Ay,a) : y \u2208Rm,a = gT (y \u2212x) + G(x)} \u2282Rd \u00d7 R.\nNote that \u21262 is an affine space with relint(\u21262) = \u21262. If (z,a) \u2208relint(\u21261) \u2229\u21262, then\nz = Ay for some y \u2208Rm and gT (y \u2212x) + G(x) > F(z) = G(y). This contradicts the fact\nthat g \u2208\u2202G(x) and shows that relint(\u21261) \u2229\u21262 = \u2205. As a consequence, there exist\n(b,b0) , (0,0) and \u03b2 such that\nF(z) \u2264a \u21d2bT z + b0a \u2264\u03b2\nz = Ay,a = gT (y \u2212x) + G(x) \u21d2bT z + b0a \u2265\u03b2\nAssume, to get a contradiction, that b0 = 0 (so that b , 0). Then bT Ay \u2265\u03b2 for all y,\nwhich is only possible if b is perpendicular to the range of A and \u03b2 \u22640. On the other\nhand, F(A \u00afx) < \u221eimplies that 0 = bT A \u00afx + b0F(A \u00afx) \u2264\u03b2, so that \u03b2 = 0. Furthermore,\nwe know that one of the inequalities above has to be strict for at least one element\nof \u21261 \u222a\u21262, but this cannot be true on \u21262, so there exists z \u2208dom(F) such that\nbT z < 0. Since bT A \u00afx = 0 and A \u00afx \u2208ridom(F), we have A \u00afx \u2212\u03f5(z \u2212A \u00afx) \u2208dom(F), so that\nbT (\u2212\u03f5z) \u22640, yielding a contradiction.\nSo, we need b0 , 0, and the first pair of inequalities clearly requires b0 < 0, so that\nwe can take b0 = \u22121. This shows that\nbT z \u2212\u03b2 \u2264F(z)\nfor all z and\nbT Ay \u2212\u03b2 \u2265gT (y \u2212x) + F(Ax)\nfor all y. Taking y = x, z = Ax, we find that \u03b2 = bT Ax \u2212F(Ax) yielding\nF(z) \u2212F(Ax) \u2265bT (z \u2212x)\nfor all z and bT A(y \u2212x) \u2265gT (y \u2212x) for all y. This last inequality implies that g = AT b\nand the first one that b \u2208\u2202F(Ax), therefore concluding the proof.\nChapter 4\nIntroduction: Bias, Variance and Density Esti-\nmation\nIn this chapter, we illustrate the bias variance dilemma in the context of density es-\ntimation, in which problems are similar to those encountered in classical parametric\nor non-parametric statistics [159, 60, 154].\nFor density estimation, one assumes that a random variable X is given with un-\nknown p.d.f. f and we want to build an estimator, i.e., a mapping (x,T ) 7\u2192\u02c6f (x;T)\nthat provides an estimation of f (x) based on a training set T = (x1,...,xN) containing\nN i.i.d. realizations of X (i.e., T is a realization of T = (X1,...,XN), N independent\ncopies of X). Alternatively, we will say that the mapping T 7\u2192\u02c6f (\u00b7;T) is an estimator\nof the full density f . Note that, to further illustrate our notation, \u02c6f (x;T ) is a number\nwhile \u02c6f (x;T ) is a random variable.\n4.1\nParameter estimation and sieves\nParameter estimation is the most common density estimation method, in which one\nrestrict \u02c6f to belong to a finite-dimensional parametric class, denoted (f\u03b8,\u03b8 \u2208\u0398), with\n\u0398 \u2282Rp. For example, f\u03b8 can be a family of Gaussian distributions on Rd. With our\nnotation, a parametric model provides estimators taking the form\n\u02c6f (x;T ) = f \u02c6\u03b8(T )(x)\nand the problem becomes the estimation of the parameter \u02c6\u03b8.\nThere are several, well-known methods for parameter estimation, and, since this\nis not the focus of the book, we only consider the most common one, maximum\n97\n98\nCHAPTER 4. INTRODUCTION: BIAS AND VARIANCE\nlikelihood, which consists in computing \u02c6\u03b8 that maximizes the log-likelihood\nC(\u03b8) = 1\nN\nN\nX\nk=1\nlogf\u03b8(xk).\n(4.1)\nThe resulting \u02c6\u03b8 (when it exists) is called the maximum likelihood estimator of \u03b8, or\nm.l.e.\nIf the true f belongs to the parametric class, so that f = f\u03b8\u2217for some \u03b8\u2217\u2208\u0398, stan-\ndard results in mathematical statistics [29, 118] provide sufficient conditions for \u02c6\u03b8\nto converge to \u03b8\u2217when N tends to infinity. However, the fact that the true p.d.f. be-\nlongs to the finite dimensional class (f\u03b8) is an optimistic assumption that is generally\nfalse. In this regard, the standard theorems in parametric statistics may be regarded\nas analyzing a \u201cbest case scenario,\u201d or as performing a \u201csanity check,\u201d in which one\nasks whether, in the ideal situation in which f actually belongs to the parametric\nclass, the designed estimator has a proper behavior. In non-parametric statistics, a\nparametric model can still be a plausible approach in order to approximate the true\nf , but the relevant question should then be whether \u02c6f provides (asymptotically), the\nbest approximation to f among all f\u03b8, \u03b8 \u2208\u0398. The maximum likelihood estimator can\nbe analyzed from this viewpoint, if one measures the difference between two density\nfunctions by the Kullback-Liebler divergence (also called differential entropy):\nKL(f \u2225f\u03b8) =\nZ\nRd log f (x)\nf\u03b8(x)f (x)dx\n(4.2)\nwhich is positive unless f = f\u03b8 (and may be equal to +\u221e).\nThis expression of the divergence is a simplification of its general measure-theo-\nretic definition, that we now provide for completeness\u2014and future use. Let \u00b5 and\n\u03bd be two probability measures on a set e\u2126. One says that \u00b5 is absolutely continuous\nwith respect to \u03bd, with notation \u00b5 \u226a\u03bd, if, for every (measurable) subset A \u2282e\u2126,\n\u03bd(A) = 0 implies \u00b5(A) = 0. The Radon-Nikodym theorem [31] states that \u00b5 \u226a\u03bd is\nand only if there exists a non-negative function g defined on e\u2126such that\n\u00b5(A) =\nZ\nA\ng(x)d\u03bd(x).\nIn terms of random variables, this says that, if X : \u2126\u2192e\u2126and Y : \u2126\u2192e\u2126are two ran-\ndom variables with respective distributions \u00b5 and \u03bd, and \u03d5 : e\u2126\u2192R is measurable,\nthen E(\u03d5(X)) = E(g(Y)\u03d5(Y)). The function g is called the Radon-Nikodym derivative\nof \u00b5 with respect to \u03bd and is denoted d\u00b5/d\u03bd (it is defined up to a modification on a\nset of \u03bd-probability zero). The general definition of the Kullback-Liebler divergence\n4.1. PARAMETER ESTIMATION AND SIEVES\n99\nbetween \u00b5 and \u03bd is then:\nKL(\u00b5\u2225\u03bd) =\n\uf8f1\uf8f4\uf8f4\uf8f4\uf8f4\uf8f2\uf8f4\uf8f4\uf8f4\uf8f4\uf8f3\nZ\n\u02dc\u2126\n \nlog d\u00b5\nd\u03bd\n! d\u00b5\nd\u03bd d\u03bd\nif \u00b5 \u226a\u03bd\n+ \u221e\notherwise\n(4.3)\nIn the case when \u00b5 = f dx and \u03bd = \u02dcf dx are both probability measures on Rd with\nrespective p.d.f.\u2019s f and \u02dcf , \u00b5 \u226a\u03bd means that f / \u02dcf is well defined everywhere except\non a set of \u03bd-probability zero. It is then equal to d\u00b5/d\u03bd. If \u00b5 \u226a\u03bd, we can therefore\nwrite\nKL(\u00b5\u2225\u03bd) =\nZ\nRd\nf (x)\n\u02dcf (x)\nlog\n f (x)\n\u02dcf (x)\n!\n\u02dcf (x)dx =\nZ\nRd log\n f (x)\n\u02dcf (x)\n!\nf (x)dx\nand we will make the abuse of notation of writing KL(f \u2225\u02dcf ) for KL(f dx\u2225\u02dcf dx), which\ngives the expression provided in (4.2).\nThe general definition also gives a simple expression when e\u2126is a finite set, with\nKL(\u00b5\u2225\u03bd) =\nX\nx\u2208e\u2126\nlog \u00b5(x)\n\u03bd(x)\u00b5(x),\nthat we will use later in these notes (if there exists x such that \u00b5(x) > 0 and \u03bd(x) =\n0, then KL(\u00b5\u2225\u03bd) = \u221e). The most important property for us is that the Kullback-\nLiebler divergence can be used as a measure of discrepancy between two probability\ndistribution, based on the following proposition.\nProposition 4.1 Let \u00b5 and \u03bd be two probability measures on e\u2126. Then KL(\u00b5\u2225\u03bd) \u22650 and\nvanishes if and only if \u00b5 = \u03bd.\nProof Assume that \u00b5 \u226a\u03bd since the statement is obvious otherwise and let g =\nd\u00b5/d\u03bd. We have\nR\ne\u2126gd\u03bd = 1 (since, by definition, it is equal to \u00b5(e\u2126)) so that\nKL(\u00b5\u2225\u03bd) =\nZ\ne\u2126\n(g logg + 1 \u2212g)d\u03bd.\nWe have t logt + 1 \u2212t \u22650 with equality if and only t = 1 (the proof being left to the\nreader) so that KL(\u00b5\u2225\u03bd) = 0 if and only if g = 1 with \u03bd-probability one, i.e., if and\nonly if \u00b5 = \u03bd.\n\u25a0\nMinimizing KL(f \u2225f\u03b8) with respect to \u03b8 is equivalent to maximizing\nEf (logf\u03b8) =\nZ\nRd logf\u03b8(x)f (x)dx,\n100\nCHAPTER 4. INTRODUCTION: BIAS AND VARIANCE\nand an empirical evaluation of this expectation is 1\nN\nPN\nk=1 logf\u03b8(xk), which provides\nthe maximum likelihood method. Seen in this context, consistency of the maximum\nlikelihood estimator states that this estimator almost surely converges to a best ap-\nproximator of the true f in the class (f\u03b8,\u03b8 \u2208\u0398). More precisely, if one assumes that\nthe function \u03b8 7\u2192logf\u03b8(x) is continuous1 in \u03b8 for almost all x and that, for all \u03b8 \u2208\u0398,\nthere exists a small enough \u03b4 > 0 such that\nZ\nRd\n\u0012\nsup\n|\u03b8\u2032\u2212\u03b8|<\u03b4\nlogf\u03b8\u2032(x)\n\u0013\nf (x)dx < \u221e\nthen, letting \u0398\u2217denote the set of maximizers of Ef (logf\u03b8), and assuming that it is\nnot empty, the maximum likelihood estimator \u02c6\u03b8N is such that, for all \u03f5 > 0 and all\ncompact subsets K \u2282\u0398,\nlim\nN\u2192\u221eP\n\u0010\nd( \u02c6\u03b8N,\u0398\u2217) > \u03f5 and \u02c6\u03b8N \u2208K\n\u0011\n\u21920\nwhere d( \u02c6\u03b8N,\u0398\u2217) is the Euclidean distance between \u02c6\u03b8N and the set \u0398\u2217. The interested\nreader can refer to Van der Vaart [194], Theorem 5.14, for a proof of this statement.\nNote that this assertion does not exclude the situation in which \u02c6\u03b8N goes to infinity\n(i.e., steps out of ever compact subset K in \u0398), and the boundedness of the m.l.e. is\neither asserted from additional properties of the likelihood, or by simply restricting\n\u0398 to be a compact set.\nIf \u0398\u2217= {\u03b8\u2217} and the m.l.e. almost surely converges to \u03b8\u2217, the speed of conver-\ngence can also be quantified by a central limit theorem (see Van der Vaart [194],\nTheorem 5.23) ensuring that, in standard cases\n\u221a\nN( \u02c6\u03b8N \u2212\u03b8\u2217) converges to a normal\ndistribution.\nEven though these results relate our present subject to classical parametric statis-\ntics, they are not sufficient for our purpose, because, when f , f\u03b8\u2217, the convergence\nof the m.l.e. to the best approximator in \u0398 still leaves a gap in the estimation of f .\nThis gap is often called the bias of the class (f\u03b8,\u03b8 \u2208\u0398). One can reduce it by con-\nsidering larger classes (e.g., with more dimensions), but the larger the class, the less\naccurate the estimation of the best approximator becomes for a fixed sample size\n(the estimator has a larger variance). This issue is known as the \u201cbias vs. variance\ndilemma,\u201d and to address it, it is necessary to adjust the class \u0398 to the sample size\nin order to optimally balance the two types of error (and all non-parametric estima-\ntion methods have at least one mechanism that allows for this). When the \u201ctuning\nparameter\u201d is the dimension of \u0398, the overall approach is often referred to as the\nmethod of sieves [83, 80], in which the dimension of \u0398 is increased as a function of N\nin a suitable way.\n1Upper-semi continuous is sufficient.\n4.2. KERNEL DENSITY ESTIMATION\n101\nGaussian mixture models provide one of the most popular choices with the me-\nthod of sieves. Modeling in this setting typically follows some variation of the fol-\nlowing construction. Fix a sequence (mN,N \u22651) and let\n\u0398N =\n\u001a\nf : f (x) =\nmN\nX\nj=1\n\u03b1j\ne\u2212|x\u2212\u00b5j|2/2\u03c32\n(2\u03c0\u03c32)d/2 ,\n\u00b51,...,\u00b5mN \u2208Rd,\u03b11 + \u00b7\u00b7\u00b7 + \u03b1mN = 1,\u03b11,...,\u03b1mN \u2208[0,+\u221e),\u03c3 > 0\n\u001b\n.\n(4.4)\nThere are therefore (d + 1)mN free parameters in \u0398N. The integer mN allows one\nto adjust the dimension of \u0398N and therefore controls the bias-variance trade-off. If\nmN tends to infinity \u201cslowly enough,\u201d the m.l.e. will converges (almost surely) to\nthe true p.d.f. f [80]. However, determining optimal sequences N \u2192mN remains a\nchallenging and largely unsolved problem.\nIn practice the computation of the m.l.e. in this context uses an algorithm called\nEM, for expectation-maximization. This algorithm will be described later in chap-\nter 16.\n4.2\nKernel density estimation\nKernel density estimators [150, 177, 178] provide alternatives to the method of\nsieves. They also lend themselves to some analytical developments that provide\nelementary illustrations of the bias-variance dilemma.\nDefine a kernel function as a function K : Rd \u2192[0,+\u221e) such that\nZ\nRd K(x)dx = 1,\nZ\nRd |x|K(x)dx < \u221e,\nZ\nRd xK(x)dx = 0.\n(4.5)\nNote that the third equation is satisfied, in particular, when K is an even function,\ni.e., K(\u2212x) = K(x).\nGiven K and a scalar \u03c3 > 0, the rescaled kernel is defined by\nK\u03c3(x) = 1\n\u03c3d K\n\u0012 x\n\u03c3\n\u0013\n.\nUsing the change of variable y = x/\u03c3 (so that dy = dx/\u03c3d) one sees that K\u03c3 satisfies\n(4.5) as soon as K does.\nBased on a training set T = (x1,...,xN), the kernel density estimator defines the\nfamily of densities\n\u02c6f\u03c3(x;T) = 1\nN\nN\nX\nk=1\nK\u03c3(x \u2212xk)\n102\nCHAPTER 4. INTRODUCTION: BIAS AND VARIANCE\nOne has\nZ\nRd K\u03c3(x \u2212xk)dx = 1\nso that it is clear that \u02c6f\u03c3 is a p.d.f. In addition,\nZ\nRd xK\u03c3(x \u2212xk)dx =\nZ\nRd(y + xk)K\u03c3(y)dy = xk\nso that\nZ\nRd x \u02c6f\u03c3(x;T)dx = \u00afx\nwhere \u00afx = (x1 + \u00b7\u00b7\u00b7 + xN)/N.\nA typical choice for K is a Gaussian kernel, K(y) = e\u2212|y|2/2/(2\u03c0)d/2. In this case, the\nestimated density is a sum of bumps centered at the data points x1,...,xN. The width\nof the bumps is controlled by the parameter \u03c3. A small \u03c3 implies less rigidity in the\nmodel, which will therefore be more affected by changes in the data: the estimated\ndensity will have a larger variance. The converse is true for large \u03c3, at the cost of\nbeing less able to adapt to variations in the true density: the model has a larger bias\n(see fig. 4.1 and fig. 4.2).\nAs we now show, in order to get a consistent estimator, one needs to let \u03c3 = \u03c3N\ndepend on the size of the training set. We have, taking expectations with respect to\ntraining data,\nE( \u02c6f\u03c3(x;T ))\n=\n1\nN\u03c3d\nN\nX\nk=1\nE\n\u0010\nK((x \u2212Xk)/\u03c3)\n\u0011\n=\n1\n\u03c3d\nZ\nRd K((x \u2212y)/\u03c3)f (y)dy\n=\nZ\nRd K(z)f (x \u2212\u03c3z)dz\nThe bias of the estimator, i.e., the average difference between \u02c6f\u03c3(x;T ) and f (x) is\ntherefore given by\nE( \u02c6f\u03c3(x;T )) \u2212f (x) =\nZ\nRd K(z)(f (x \u2212\u03c3z) \u2212f (x))dz.\nInterestingly, this bias does not depend on N, but only on \u03c3, and it is clear that,\nunder mild continuity assumptions on f , it will go to zero with \u03c3.\nThe variance of \u02c6f\u03c3(x;T ) is given by\nvar( \u02c6f\u03c3(x;T )) =\n1\nN\u03c32d var(K((x \u2212X)/\u03c3))\n4.2. KERNEL DENSITY ESTIMATION\n103\n\u03c3 = 0.1\n\u03c3 = 0.25\n\u03c3 = 0.5\n\u03c3 = 1.0\nFigure 4.1: Kernel density estimators using a Gaussian kernel and various values of \u03c3 when\nthe true distribution of the data is a standard Gaussian (Orange: true density; Blue: esti-\nmated density, Red dots: training data).\n104\nCHAPTER 4. INTRODUCTION: BIAS AND VARIANCE\n\u03c3 = 0.1\n\u03c3 = 0.25\n\u03c3 = 0.5\n\u03c3 = 1.0\nFigure 4.2: Kernel density estimators using a Gaussian kernel and various values of \u03c3 when\nthe true distribution of the data is a Gamma distribution with parameter 2 (Orange: true\ndensity; Blue: estimated density, Red dots: training data).\n4.2. KERNEL DENSITY ESTIMATION\n105\nwith\n1\nN\u03c32d var(K((x \u2212X)/\u03c3))\n=\n1\nN\u03c32d\nZ d\nR\nK((x \u2212y)/\u03c3)2f (y)dy\n\u2212\n1\nN\u03c32d\n Z\nRd K((x \u2212y)/\u03c3)f (y)dy\n!2\n=\n1\nN\u03c3d\nZ\nRd K(z)2f (x \u2212\u03c3z)dz \u22121\nN\n Z\nRd K(z)f (x \u2212\u03c3z)dz\n!2\nThe total mean-square error of the estimator is\nE(( \u02c6f\u03c3(x) \u2212f (x))2) = var( \u02c6f\u03c3(x)) + (E( \u02c6f\u03c3(x)) \u2212f (x))2.\nClearly, this error cannot go to zero unless we allow \u03c3 = \u03c3N to depend on N. For the\nbias term to go to zero, we know that we need \u03c3N \u21920, in which case we can expect\nthe second term in the variance to decrease like 1/N, while, for the first term to go to\nzero, we need N\u03c3d\nN to go to infinity. This illustrates the bias-variance dilemma: \u03c3N\nmust go to zero in order to cancel the bias, but not too fast in order to also cancel the\nvariance. There is, for each N, an optimal value of \u03c3 that minimizes the error, and\nwe now proceed to a more detailed analysis and make this statement a little more\nprecise.\nLet us make a Taylor expansion of both bias and variance, assuming that f has at\nleast three bounded derivatives and that\nR\nRd |x|3K(x)dx < \u221e. We can write\nf (x \u2212\u03c3z) = f (x) \u2212\u03c3zT \u2207f (x) + \u03c32\n2 zT \u22072f (x)z + O(\u03c33|z|3),\nwhere \u22072f (x) denotes the matrix of second derivatives of f at x. Since\nR\nzK(z)dz = 0,\nthis gives\nE( \u02c6f\u03c3(x;T )) \u2212f (x) = \u03c32\n2 Mf (x) + o(\u03c32)\nwith Mf =\nR\nK(z)zT \u22072f (x)zdz. Similarly, letting S =\nR\nK2(z)dz,\nvar( \u02c6f\u03c3(x)) =\n1\nN\u03c3d\n\u0010\nSf (x) + o(\u03c3d + \u03c32)\n\u0011\n.\nAssuming that f (x) > 0, we can obtain an asymptotically optimal value for \u03c3 by\nminimizing the leading terms of the mean square error, namely\n\u03c34\n4 M2\nf +\nS\nN\u03c3d f (x)\nwhich yields \u03c3N = O(N \u22121/(d+4)) and\nE(( \u02c6f\u03c3N(x;T ) \u2212f (x))2) = O(N \u22124/(d+4)).\n106\nCHAPTER 4. INTRODUCTION: BIAS AND VARIANCE\nIf f has r + 1 derivatives, and K has r \u22121 vanishing moments (this excludes the\nGaussian kernel) one can reduce this error to N \u22122r\n2r+d . These rates can be shown to\nbe \u201coptimal,\u201d in the \u201cmin-max\u201d sense, which roughly expresses the fact that, for any\nother estimator, there exists a function f for which the convergence speed is at least\nas \u201cbad\u201d as the one obtained for kernel density estimation.\nThis result says that, in order to obtain a given accuracy \u03f5 in the worst case sce-\nnario, N should be chosen of order (1/\u03f5)1+(d/2r) which grows exponentially fast with\nthe dimension. This is the curse of dimensionality which essentially states that the\nissue of density estimation may be intractable in large dimensions. The same state-\nment is true also for most other types of machine learning problems. Since machine\nlearning essentially deals with high-dimensional data, this issue can be problematic.\nObviously, because the min-max theory is a worst-case analysis, not all situations\nwill be intractable for a given estimator, and some cases that are challenging for one\nof them may be quite simple for others: even though all estimators are \u201ccursed,\u201d the\nway each of them is cursed differs. Moreover, while many estimators are optimal\nin the min-max sense, this theory does not give any information on \u201chow often\u201d an\nestimator performs better than its worst case, or how it will perform on a given class\nof problems. (For kernel density estimation, however, what we found was almost\nuniversal with respect to the unknown density f , which indicates that this estimator\nis not a good choice in large dimensions.)\nAnother important point with this curse of dimensionality is that data may very\noften appear to be high dimensional while it has a simple, low-dimensional struc-\nture, maybe because many dimensions are irrelevant to the problem (they contain,\nfor example, just random noise), or because the data is supported by a non-linear\nlow-dimensional space, such as a curve or a surface. This information is, of course,\nnot available to the analysis, but can sometimes be inferred using some of the dimen-\nsion reduction methods that will be discussed later in chapter 20. Sometimes, and\nthis is also important, information on the data structure can be provided by domain\nknowledge, that is, by elements, provided by experts, that specify how the data has\nbeen generated (such as underlying equations) and reasonable hypotheses that are\nmade in the field. This source of information should never be ignored in practice.\nChapter 5\nPrediction: Basic Concepts\n5.1\nGeneral Setting\nThe goal of prediction is to learn, based on training data, an input-output relation-\nship between two random variables X and Y, in the sense of finding, for a specified\ncriterion, the best function of the input X that predicts the output Y. (In statistics, Y\nis often called the dependent variable, and X the independent variable.) We will, as\nalways, assume that all the variables mentioned in this chapter are defined on a fixed\nprobability space (\u2126,P). We assume that X : \u2126\u2192RX, where RX is the input space,\nand Y : \u2126\u2192RY, where RY is the output space. The input-output relationship is\ntherefore captured by an unknown function f : RX \u2192RY, the predictor.\nThe following two subclasses of prediction problems are important enough to\nhave learned their own names and specific literature.\n\u2022 Quantitative output: RY = Rq (often with q = 1). One then speaks of a regres-\nsion problem.\n\u2022 Categorical output: RY = {g1,...,gq} is a finite set. One then speaks of a classi-\nfication problem.\nIn most cases, the input space is Euclidean, i.e., RX = Rd. Note also that, in clas-\nsification, instead of a function f : R \u2192RY, one sometimes estimates a function\nf : RX \u2192\u03a0(RY), where \u03a0(RY) is the space of probability distributions on RY. We\nwill return to this in remark 5.4.\nThe quality of a prediction is assessed through the definition of a risk function.\nSuch a function, denoted r, is defined on RY \u00d7RY, takes values in [0,+\u221e) and should\nbe understood as\nr(True output,Predicted output),\n(5.1)\n107\n108\nCHAPTER 5. PREDICTION: BASIC CONCEPTS\nso that r(y,y\u2032) assigns a cost to the situation in which a true y is predicted by y\u2032.\nNote that this definition is asymmetric, and there is no requirement that r(y,y\u2032) =\nr(y\u2032,y). It is important to remember our convention that the first variable is the true\nobservation and the second one is a place-holder for a prediction. Risk functions\nare also called loss functions, or simply cost functions and we will use these terms as\nsynonyms.\nThe goal in prediction is to minimize the expected risk, also called the generaliza-\ntion error:\nR(f ) = E(r(Y,f (X))).\nWe will prove that an optimal f can be easily described based on the joint distri-\nbution of X and Y (which is, unfortunately, never available). We will need for this\nto use conditional expectations and conditional probabilities and proceed first to a\nreminder of their definitions and properties.\n5.2\nConditional expectation\nIf \u03be : \u2126\u2192R\u03be and \u03b7 : \u2126\u2192R\u03b7 \u2282Rd are discrete random variables, then\nP(\u03b7 = \u03b7 | \u03be = \u03be) = P(\u03b7 = \u03b7,\u03be = \u03be)/P(\u03be = \u03be)\nif P(\u03be = \u03be) > 0 and is undefined otherwise. Then, if \u03b7 is real-valued and discrete,\none defines the conditional expectation of \u03b7 given \u03be, denoted E(\u03b7 | \u03be), by\nE(\u03b7 | \u03be)(\u03c9) =\nX\n\u03b7\u2208R\u03b7\n\u03b7P(\u03b7 = \u03b7 | \u03be = \u03be(\u03c9))\nfor all \u03c9 such that P(\u03be = \u03be(\u03c9)) > 0. Note that E(\u03b7 | \u03be) is a random variable, defined\nover \u2126. It however only depends on the values of \u03be, in the sense that E(\u03b7 | \u03be)(\u03c9) =\nE(\u03b7 | \u03be)(\u03c9\u2032) if \u03be(\u03c9) = \u03be(\u03c9\u2032). We will use the notation\nE(\u03b7 | \u03be = \u03be) =\nX\n\u03b7\u2208R\u03b7\n\u03b7P(\u03b7 = \u03b7 | \u03be = \u03be),\nwhich is now a function defined on R\u03be. One has E(\u03b7 | \u03be)(\u03c9) = E(\u03b7 | \u03be = \u03be(\u03c9)).\nOne can characterize E(\u03b7 | \u03be) by the properties\n\uf8f1\uf8f4\uf8f4\uf8f2\uf8f4\uf8f4\uf8f3\nE(\u03b7 | \u03be) is a function of \u03be\n\u2200f : R\u03b7 \u2192R,E(E(\u03b7 | \u03be)f (\u03be)) = E(\u03b7f (\u03be)).\n(5.2)\nThe proof that our definition of E(\u03b7 | \u03be) for discrete random variables is the only\none satisfying these properties is left to the reader. The interest of reformulating the\n5.2. CONDITIONAL EXPECTATION\n109\ndefinition of the conditional expectation via (5.2) is that this provides a definition\nthat works for general random variables (with the additional assumption that f is\nmeasurable), not only for discrete ones. We assume below that (R\u03be,S\u03be) and (R\u03b7,S\u03b7)\nare measurable spaces.\nDefinition 5.1 Assume that R\u03b7 = Rd. Let \u03be : \u2126\u2192R\u03be and \u03b7 : \u2126\u2192R\u03b7 be two random\nvariables with E(|\u03b7|) < \u221e. The conditional expectation of \u03b7 given \u03be is a random variable\n\u03b6 : \u2126\u2192R\u03b7 such that\n(i) There exists a function h : R\u03be \u2192R such that \u03b6 = h \u25e6\u03be almost surely.\n(ii) For any measurable function g : R\u03be \u2192[0,+\u221e), one has\nE(\u03b7g \u25e6\u03be) = E(\u03b6g \u25e6\u03be).\nThe variable \u03b6 is then denoted E(\u03b7|\u03be) and the function h in (i) is denoted E(\u03b7|\u03be = \u00b7).\nImportantly, functions \u03b6 satisfying conditions (i) and (ii) always exists and are\nalmost surely unique, in the sense that if another function \u03b6\u2032 satisfies these condi-\ntions, then \u03b6 = \u03b6\u2032 with probability one. One obtains an equivalent definition if one\nrestricts functions g in (ii) to indicators of measurable sets, yielding the condition\nthat, if A \u2282R\u03be is measurable,\nE(\u03b71\u03be\u2208B) = E(\u03b61\u03be\u2208B).\nTaking g(\u03be) = 1 for all \u03be \u2208R\u03be in condition (ii), one gets the well-known identity\nE(E(\u03b7|\u03be)) = E(\u03b7).\nMoreover, for any function g defined on R\u03be we have E(\u03b7g \u25e6\u03be|\u03be) = (g \u25e6\u03be)E(\u03b7|\u03be),\nwhich can be checked by proving that the right-hand side satisfies the conditions (i)\nand (ii).\nConditional expectations share many of the properties of simple expectations.\nFor example, if \u03b7 \u2264\u03b7\u2032, both taking scalar values, then E(\u03b7 | \u03be) \u2264E(\u03b7\u2032 | \u03be) almost\nsurely. Jensen\u2019s inequality also holds: if \u03b3 : Rd \u2192R is convex and \u03b3 \u25e6\u03b7 is integrable,\nthen\n\u03b3 \u25e6E(\u03b7 | \u03be) \u2264E(\u03b3 \u25e6\u03b7 | \u03be).\nWe will discuss convex functions in chapter 3, but two important examples for this\nsection are \u03b3(\u03b7) = |\u03b7| and \u03b3(\u03b7) = |\u03b7|2. The first one implies that |E(\u03b7 | \u03be)| \u2264E(|\u03b7| |\nbf xi) and, taking expectations on both sides: E(|E(\u03b7 | \u03be)|) \u2264E(|\u03b7|), the upper bound\nbeing finite by assumption. For the square norm, we find that, if \u03b7 is square inte-\ngrable, then so is E(\u03b7 | \u03be) and\nE(|E(\u03b7 | \u03be)|2) \u2264E(|\u03b7|2).\n110\nCHAPTER 5. PREDICTION: BASIC CONCEPTS\nIf \u03b7 is square integrable, then this inequality shows that E(\u03b7 | \u03be) minimizes\nE[|\u03b7 \u2212\u03b6|2] among all square integrable functions \u03b6 : \u2126\u2192R\u03b7 that satisfy (i). In\nother terms, the conditional expectation is the optimal least-square approximation\nof \u03b7 by a function of \u03be. To see this, just write\nE[|\u03b7 \u2212\u03b6|2 | \u03be] = E[|\u03b7|2 | \u03be] \u22122E[\u03b7T \u03b6 | \u03be] + |\u03b6|2\n= E[|\u03b7|2 | \u03be] \u22122E(\u03b7 | \u03be)T \u03b6 + |\u03b6|2\n= E[|\u03b7|2 | \u03be] \u2212|E(\u03b7 | \u03be)|2 + |E(\u03b7 | \u03be) \u2212\u03b6|2\n= E[|\u03b7 \u2212E(\u03b7 | \u03be)|2 | \u03be] + |E(\u03b7 | \u03be) \u2212\u03b6|2\n\u2265E[|\u03b7 \u2212E(\u03b7 | \u03be)|2 | \u03be]\nand taking expectations on both sides yields the desired result.\nIf A is a measurable subset of R\u03b7, the conditional expectation E(1A | \u03be) (resp.\nE(1A | \u03be = \u03be)) is denoted P(\u03b7 \u2208A | \u03be) (resp. P(\u03b7 \u2208A | \u03be = \u03be)), or P\u03b7(A | \u03be) (resp.\nP\u03b7(A | \u03be = \u03be)). While these functions are defined separately up to modifications on\nsets of probability zero. Under general assumptions on the set R\u03b7 and its \u03c3-algebra\n(always satisfied in our discussions), these conditional probabilities can be defined\ntogether so that, for all \u03c9 \u2208\u2126A 7\u2192P\u03b7(A | \u03be)(\u03c9) is a probability distribution on R\u03b7\nsuch that\nE(\u03b7 | \u03be) =\nZ\nR\u03b7\n\u03b7P(d\u03b7 | \u03be).\nAssume that the the sets R\u03be and R\u03b7 are equipped with measures, say \u00b5\u03be and\n\u00b5\u03b7 such that the joint distribution of (\u03be,\u03b7) is absolutely continuous with respect to\n\u00b5\u03be \u2297\u00b5\u03b7, so that there exists a function \u03d5 : R\u03be \u00d7 R\u03b7 \u2192R (the p.d.f. of (\u03be,\u03b7) with\nrespect to \u00b5\u03be \u2297\u00b5\u03b7) such that\nP(\u03be \u2208A,\u03b7 \u2208B) =\nZ\nA\u00d7B\n\u03d5(\u03be,\u03b7)\u00b5\u03be \u2297\u00b5\u03b7(dx,d\u03b7).\nThen P\u03b7(\u00b7 | \u03be) is absolutely continuous with respect to \u00b5\u03b7, with density given by the\nconditional p.d.f. of \u03b7 given \u03be, namely,\n\u03d5(\u00b7 | \u03be) : (\u03b7,\u03c9) 7\u2192\n\u03d5(\u03b7,\u03be(\u03c9))\nR\nR\u03b7 \u03d5(\u03b7\u2032,\u03be(\u03c9))\u00b5\u03b7(d\u03b7\u2032)\n= \u03d5(\u03b7 | \u03be = \u03be(\u03c9)).\n(5.3)\nNote that\nP\n\uf8f1\uf8f4\uf8f4\uf8f2\uf8f4\uf8f4\uf8f3\u03c9 :\nZ\nR\u03b7\n\u03d5(\u03b7\u2032,\u03be(\u03c9))\u00b5\u03b7(d\u03b7\u2032) = 0\n\uf8fc\uf8f4\uf8f4\uf8fd\uf8f4\uf8f4\uf8fe= 0\n5.3. BAYES PREDICTOR\n111\nso that the conditional density can be defined arbitrarily when the numerator van-\nishes1.\nThe most common example is when R\u03be and R\u03b7 are Euclidean spaces and \u00b5\u03be, \u00b5\u03b7\nare Lebesgue\u2019s measures, in which case (5.3) is the usual definition of conditional\np.d.f.\u2019s. Note also that, for discrete random variables, (5.3) coincides with the defi-\nnition of conditional probabilities P(\u03b7 = \u03b7 | \u03be = \u03be(\u03c9)) when \u00b5x and \u00b5\u03b7 are counting\nmeasures. As a last example, if R\u03be = Rd, \u00b5\u03be is Lebesgue\u2019s measure and \u03b7 is discrete,\nthen\n\u03d5(\u03b7 | \u03be = \u03be(\u03c9)) =\n\u03d5(\u03b7,\u03be(\u03c9))\nP\n\u03b7\u2032\u2208R\u03b7 \u03d5(\u03b7\u2032,\u03be(\u03c9)).\n5.3\nBayes predictor\nRecall that r : (y,y\u2032) 7\u2192r(y,y\u2032) denotes the risk function and that we want to minimize\nR(f ) = E(r(Y,f (X)) over all possible predictors f .\nDefinition 5.2 A Bayes predictor is a measurable function f : RX \u2192RY such that, for\nall x \u2208RX,\nE\n\u0010\nr(Y,f (x)) | X = x\n\u0011\n= min\nn\nE\n\u0010\nr(Y,y\u2032) | X = x\n\u0011\n: y\u2032 \u2208RY\no\nThere can be multiple Bayes predictors if the minimum in the proposition is not\nuniquely attained. Note that, if f \u2217is a Bayes predictor and \u02c6f any other predictor, we\nhave, by definition\nE\n\u0010\nr(Y,f \u2217(X)) | X\n\u0011\n\u2264E\n\u0010\nr(Y, \u02c6f (X)) | X\n\u0011\n.\nPassing to expectations, this implies R(f \u2217) \u2264R( \u02c6f ). We therefore have the following\nresult:\nTheorem 5.3 Any Bayes predictor f \u2217is optimal, in the sense that it minimizes the gen-\neralization error R.\nExample 1. Regression with mean-square error. When RX = Rd and RY = Rq, the\nmost common risk function is the squared norm r(y,y\u2032) = |y \u2212y\u2032|2. The resulting\ngeneralization error is called the MSE (mean square error) and given by R(f ) = E(|Y \u2212\nf (X)|2). The Bayes predictor is such that f \u2217(x) minimizes\nt 7\u2192E(|Y \u2212t|2 | X = x).\n1Letting \u03d5\u03be(\u03be) =\nR\nR\u03b7 \u03d5(\u03b7\u2032,\u03be)\u00b5\u03b7(d\u03b7\u2032), which is the marginal p.d.f. of \u03be with respect to \u00b5\u03be, we have\nP(\u03d5\u03be(\u03be) = 0) =\nZ\nR\u03be\n1\u03d5\u03be(\u03be)=0\u03d5\u03be(\u03be)\u00b5\u03be(d\u03be) = 0.\n112\nCHAPTER 5. PREDICTION: BASIC CONCEPTS\nLet f \u2217(x) = E(Y | X = x) and write\nE(|Y \u2212t|2 | X = x) =E(|Y \u2212f \u2217(x)|2 | X = x) + 2E((Y \u2212f \u2217(x))T (f \u2217(x) \u2212t) | X = x)\n+ |f \u2217(x) \u2212t|2\n=E(|Y \u2212f \u2217(x)|2 | X = x) + 2E((Y \u2212f \u2217(x))T | X = x)(f \u2217(x) \u2212t)\n+ |f \u2217(x) \u2212t|2\n=E(|Y \u2212f \u2217(x)|2 | X = x) + |f \u2217(x) \u2212t|2.\nThis proves that E(Y | X = x) is the unique Bayes classifier (up to a modification on\na set of probability 0).\nExample 2. Classification with zero-one loss. Let RX = Rd and RY be a finite set. The\nzero-one loss function is defined by r(y,y\u2032) = 1 if y , y\u2032 and 0 otherwise. From\nthis, it results that the generalization error is the probability of misclassification\nR(f ) = P(Y , f (X)) (also called the misclassification error).\nThe Bayes predictor is such that f \u2217(x) minimizes\ng 7\u2192P(Y , g | X = x) = 1 \u2212P(Y = g | X = x).\nIt is therefore given by the so-called posterior mode:\nf \u2217(x) = argmaxgP(Y = g | X = x).\nRemark 5.4 As mentioned at the beginning of the chapter, one sometimes replaces a\npointwise prediction of the output by a probabilistic one, so that f (x) is a probability\ndistribution on RY. If A is a (measurable) subset of RY, we will write f (x,A) rather\nthan f (x)(A).\nIn such a case, the loss function, r, is defined on RY \u00d7 \u03a0(RY), and the expected\nrisk is still defined by E(r(Y,f (X))).\nIt is quite natural to require that \u03c0 7\u2192r(y,\u03c0) is minimized. For classification\nproblems, where RY is finite, one can choose\nr(y,\u03c0) = \u2212log\u03c0(y)\n(5.4)\nThe Bayes estimator is then a minimizer of \u03c0 7\u2192\u2212E(log\u03c0(Y) | X = x). The solution is\n(unsurprisingly) f (x,y) = P(Y = y | X = x) since we always have\n\u2212E(log\u03c0(Y) | X = x) = \u2212\nX\ny\u2208RY\nlog\u03c0(y)f (x,y) \u2265\u2212\nX\ny\u2208RY\nlogf (x,y)f (x,y).\nThe difference between these terms is indeed\nX\ny\u2208RY\nlog f (x,y)\n\u03c0(y) f (x,y) = KL(f (x,\u00b7),\u03c0) \u22650.\n5.4. EXAMPLES: MODEL-BASED APPROACH\n113\nFor regression problems, with RY = Rq, one can choose\nr(y,\u03c0) =\nZ\nRq |z \u2212y|2\u03c0(dz)\nwhich is indeed minimum when \u03c0 is concentrated on y. Here, the Bayes estimator\nminimizes (with respect to \u03c0)\nZ\nRq\nZ\nRq |z \u2212y|2\u03c0(dz)PY(x,dy) =\nZ\nRq\n Z\nRq |z \u2212y|2PY(x,dy)\n!\n\u03c0(dz)\nwhere PY(x,\u00b7) is the conditional distribution of Y given X = x. For any z, one has\nZ\nRq |y \u2212z|2f (x,dy) \u2265\nZ\nRq |y \u2212E(Y | X = x)|2f (x,dy)\n\u2666\nwhich shows that the Bayes estimator is, in this case, the Dirac measure concentrated\nat E(Y | X = x).\n5.4\nExamples: model-based approach\nBayes predictors are never available in practice, because the true distribution of\n(X,Y), or that of Y given X, are unknown. These distributions can only be inferred\nfrom observations, i.e., from a training set: T = (x1,y1,...,xN,yN).\nThis is the approach followed by model-based, or generative methods, namely us-\ning training data to approximate the joint distribution of X and Y with a statistical\nmodel estimated from data before using the Bayes estimator derived from this model\nfor prediction. We now illustrate this approach with a few examples.\n5.4.1\nGaussian models and naive Bayes\nConsider a regression problem with RY = R, and model the joint distribution of\n(X,Y) as a (d + 1)-dimensional Gaussian distribution with mean \u00b5 and covariance\nmatrix \u03a3, which must be estimated from data. Write \u00b5 =\n \nm\n\u00b50\n!\n, with \u00b50 \u2208R, m \u2208Rd\nand \u03a3 in the form, for some symmetric matrix S and d-dimensional vector u\n\u03a3 =\n \nS\nu\nuT\n\u03c32\n00\n!\n.\nThen, letting \u2206= \u03c32\n00 \u2212uT S\u22121u,\n\u03a3\u22121 = 1\n\u2206\n \n\u2206S\u22121 + S\u22121uuT S\u22121\n\u2212S\u22121u\n\u2212uT S\u22121\n1\n!\n.\n114\nCHAPTER 5. PREDICTION: BASIC CONCEPTS\nThis shows that the joint p.d.f. of (X,Y) is proportional to\nexp\n\u0012\n\u2212\n1\n2\u2206\n\u0010\n(y \u2212\u00b50)2 \u22122uT S\u22121(x \u2212m)(y \u2212\u00b50) + (terms not depending on y)\n\u0011\u0013\n.\nIn particular\nE(Y|X) = \u00b50 + uT S\u22121(x \u2212m),\nwhich provides the least-square linear regression predictor. (In this expression, u is\nthe covariance between X and Y and S is the covariance matrix of X.)\nIf one restricts the model to having a diagonal covariance matrix S, then\nE(Y|X) = \u00b50 +\nd\nX\nj=1\nu(j)\nsjj\n(x(j) \u2212m(j)).\nThis predictor is often called the naive Bayes predictor for regression.\n5.4.2\nKernel regression\nLet RX = Rd and RY = R. Let K1 : Rd \u2192R and K2 : R \u2192R be two kernels, therefore\nsatisfying\nZ\nRd K1(x)dx =\nZ\nR\nK2(x)dx = 1;\nZ\nRd xK1(x)dx =\nZ\nR\nyK2(y)dy = 0.\nLet K(x,y) = K1(x)K2(y) so that\nZ\nRd+1 K(x,y)dydx = 1\nZ\nRd+1 yK(y,x)dydx = 0\nZ\nRd+1 xK(y,x)dydx = 0.\nThe kernel estimator of the joint p.d.f., \u03d5, of (X,Y) at scale \u03c3 is, in this case:\n\u02c6\u03d5(x,y) = 1\nN\nN\nX\nk=1\n1\n\u03c3d+1K1\n\u0012x \u2212xk\n\u03c3\n\u0013\nK2\n\u0012y \u2212yk\n\u03c3\n\u0013\n.\nBased on \u02c6\u03d5, the conditional expectation of Y given X is\n\u02c6f (x) =\n1\nN\nPN\nk=1\n1\n\u03c3d+1\nR\nR yK1\n\u0010x\u2212xk\n\u03c3\n\u0011\nK2\n\u0010y\u2212yk\n\u03c3\n\u0011\ndy\n1\nN\nPN\nk=1\n1\n\u03c3d+1\nR\nR K1\n\u0010x\u2212xk\n\u03c3\n\u0011\nK2\n\u0010y\u2212yk\n\u03c3\n\u0011\ndy\n.\n5.5. EMPIRICAL RISK MINIMIZATION\n115\nUsing the fact that \u03c3\u22121 R\nR yK2\n\u0010y\u2212yk\n\u03c3\n\u0011\ndy = yk, we can simplify this expression to\nobtain\n\u02c6f (x) =\nPN\nk=1 ykK1\n\u0010x\u2212xk\n\u03c3\n\u0011\nPN\nk=1 K1\n\u0010x\u2212xk\n\u03c3\n\u0011 .\nThis the kernel-density regression estimator [139, 202].\n5.4.3\nA classification example\nLet RY = {0,1} and assume RX = N = {0,1,2,...}. Let p = P(Y = 1) and assume that\nconditionally to Y = g, X follows a Poisson distribution with mean \u00b5g. Assume that\n\u00b50 < \u00b51.\nThe posterior distribution of Y given X = x is 2\nP(Y = g | X = x) \u221d\n((1 \u2212p)\u00b5x\n0e\u2212\u00b50 if g = 0\np\u00b5x\n1e\u2212\u00b51 if g = 1\nA Bayes classifier is then provided by taking f (x) = 1 if\nlogp + xlog\u00b51 \u2212\u00b51 \u2265log(1 \u2212p) + xlog\u00b50 \u2212\u00b50\nthat is:\nxlog \u00b51\n\u00b50\n\u2265log 1 \u2212p\np\n+ \u00b51 \u2212\u00b50\nSince we are assuming that \u00b51 > \u00b50, we find that f (x) = 1 if 3\nx \u2265\n&log((1 \u2212p)/p) + \u00b51 \u2212\u00b50\nlog(\u00b51/\u00b50)\n'\nand 0 otherwise.\n5.5\nEmpirical risk minimization\n5.5.1\nGeneral principles\nModel-based approaches for prediction are based on the estimation of the joint dis-\ntribution of the input and output variables, which is arguably a harder problem\nthan prediction [196].\nSince the goal is to find f minimizing the expected risk\n2\u221dis the notation for \u201cproportional to\u201d\n3\u2308x\u2309is the smallest integer larger than x (ceiling).\n116\nCHAPTER 5. PREDICTION: BASIC CONCEPTS\nR(f ) = E(r(Y,f (X)), one may prefer a direct approach and consider the minimization\nof an empirical estimate of this risk, based on training data T = (x1,y1,...,xN,yN),\nnamely\n\u02c6R(f ) = 1\nN\nN\nX\nk=1\nr(yk,f (xk)).\nThis strategy is called empirical risk minimization.\nImportantly, \u02c6R must be minimized over a restricted class, F , of predictors to\navoid overfitting. For example, with RY = R and R = Rd, one can take\nF =\n\uf8f1\uf8f4\uf8f4\uf8f2\uf8f4\uf8f4\uf8f3f : f (x) = \u03b20 +\nd\nX\ni=1\nb(i)x(i) : \u03b20,b(1) ...,b(d) \u2208R\n\uf8fc\uf8f4\uf8f4\uf8fd\uf8f4\uf8f4\uf8fe.\nMinimizing the empirical mean-square error\n\u02c6R(f ) = 1\nN\nN\nX\nk=1\n(yk \u2212f (xk))2\nover f \u2208F leads to the standard least-square regression estimator.\nAs another example, consider\nF =\n\uf8f1\uf8f4\uf8f4\uf8f4\uf8f2\uf8f4\uf8f4\uf8f4\uf8f3\nf : f (x) =\np\nX\nj=1\nwj\u03c8\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\u03b2j0 +\nd\nX\ni=1\n\u03b2jix(i)\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8,wj,\u03b2ji \u2208R\n\uf8fc\uf8f4\uf8f4\uf8f4\uf8fd\uf8f4\uf8f4\uf8f4\uf8fe\n.\nwith a fixed function \u03c8. This corresponds to a two-layer perceptron model.\nAs a last example for now (we will see many others in the rest of this book),\ntaking d = 1, the set\nF =\n(\nf :\nZ\nR\nf \u2032\u2032(x)2dx < \u00b5\n)\n(with \u00b5 > 0) provides an infinite dimensional space of predictors, which leads to\nspline regression.\n5.5.2\nBias and variance\nWe give a further illustration of the bias-variance dilemma in the regression case,\nusing the mean-square error and taking q = 1 to simplify. Denote the Bayes predictor\nby f \u2217(x) = E(Y | X = x).\n5.6. EVALUATING THE ERROR\n117\nFix a function space F , and let \u02c6f \u2217be the optimal predictor in F , in the sense that\nit minimizes E(|Y \u2212f (X)|2) over f \u2208F . Then, letting \u02c6fN \u2208F denote an estimated\npredictor,\nR( \u02c6fN)\n= E(|Y \u2212\u02c6fN(X)|2)\n= E(|Y \u2212\u02c6f \u2217(X)|2) + E(| \u02c6fN(X) \u2212\u02c6f \u2217(X)|2)\n+2E((Y \u2212\u02c6f \u2217(X))( \u02c6f \u2217(X) \u2212\u02c6fN(X))\nLet us make the assumption that there exists \u03f5 > 0 such that f\u03bb = \u02c6f \u2217+ \u03bb( \u02c6fN \u2212\u02c6f \u2217)\nbelongs to F for \u03bb \u2208[\u2212\u03f5,\u03f5]. This happens when F is a linear space, or more generally\nwhen F is convex and \u02c6f \u2217is in its relative interior (see chapter 3). Let \u03c8 : \u03bb 7\u2192\nE(|Y \u2212f\u03bb(X)|2), which is minimal at \u03bb = 0. We have\n\u03c8(\u03bb) =E(|Y \u2212\u02c6f \u2217(X) \u2212\u03bb( \u02c6fN(X) \u2212\u02c6f \u2217(X))|2)\n=E(|Y \u2212\u02c6f \u2217(X)|2) \u22122\u03bbE((Y \u2212\u02c6f \u2217(X))( \u02c6fN(X) \u2212\u02c6f \u2217(X))) + \u03bb2E(| \u02c6fN(X) \u2212\u02c6f \u2217(X))|2)\nand\n0 = \u03c8\u2032(0) = 2E((Y \u2212\u02c6f \u2217(X))( \u02c6f \u2217(X) \u2212\u02c6fN(X)))\nWe therefore get the identity\nR( \u02c6fN) = E(|Y \u2212\u02c6f \u2217(X)|2) + E(| \u02c6fN(X) \u2212\u02c6f \u2217(X)|2) = \u201cBias\u201d + \u201cVariance\u201d.\nThe bias can be further decomposed as\nE(|Y \u2212\u02c6f \u2217(X)|2) = E(|Y \u2212f \u2217(X)|2) + E(|f \u2217(X) \u2212\u02c6f \u2217(X)|2)\nbecause f \u2217is the conditional expectation. As a result, we obtain an expression the\ngeneralization error with three contributions, namely,\nR( \u02c6fN) \u2264E(|Y \u2212f \u2217(X)|2) + E(|f \u2217\u2212\u02c6f \u2217(X)|2) + E(| \u02c6fN(X) \u2212\u02c6f \u2217(X)|2).\nThe first term is the Bayes error. It is fixed by the joint distribution of X and Y\nand measures how well Y can be approximated by a function of X. The second term\ncompares f \u2217to its best approximation in F , and is therefore reduced by taking larger\nmodel spaces. The last term is the error caused by using the data to estimate \u02c6f \u2217. It\nincreases with the size of F . This is illustrated in Figure 5.1.\nRemark 5.5 If the assumption made on \u02c6f \u2217is not valid, one can write\nR( \u02c6fN) = E(|Y \u2212\u02c6fN(X)|2) \u22642\n\u0010\nE(|Y \u2212\u02c6f \u2217(X)|2) + E(| \u02c6fN(X) \u2212\u02c6f \u2217(X)|2)\n\u0011\nand still obtain a control (as an inequality) of the generalization error by a bias-plus-\nvariance sum.\n\u2666\n118\nCHAPTER 5. PREDICTION: BASIC CONCEPTS\n\u02c6P\nP\u2217\nF\n\u02c6f\n\u02c6f \u2217\nf \u2217\nProbability space\nPredictor space\nFigure 5.1: Sources of errors in statistical Learning: When P\u2217is the distribution of the data,\nthe optimal predictor f \u2217minimizes the expected loss function. Based on data Z1,...,ZN,\nthe sample-based distribution is \u02c6P = (\u03b4Z1 + \u00b7\u00b7\u00b7 + \u03b4ZN )/N and the empirical loss is minimized\nover a subset S of the space of all possible estimators. The expected discrepancy between\nthe resulting estimator and the one minimizing the true expected loss on the subspace is the\n\u201cvariance\u201d of the method, and the expected discrepancy between this subspace-constrained\nestimator and and the optimal one is the \u201cbias.\u201d\n5.6. EVALUATING THE ERROR\n119\n5.6\nEvaluating the error\n5.6.1\nGeneralization error\nGiven input and output variables X : \u2126\u2192RX and Y : \u2126\u2192RY and a risk function\nr : RY \u00d7 RY \u2192[0. + \u221e), we have defined the generalization (or prediction) error as\nR(f ) = E(r(Y,f (X))).\nRecall that a training set T = ((x1,y1),...,(xN,yN)) is a realization T = T (\u03c9) of the\nrandom variable T = ((X1,Y1),...,(XN,YN)), an i.i.d. sample of the joint distribution\nof (X,Y). A learning algorithm is a function T 7\u2192\u02c6fT defined on the set of training\nsets, namely, S\u221e\nN=1(R \u00d7 RY)N and taking values in F .\nFor a given T and a specific algorithm, one is primarily interested in evaluating\nR( \u02c6fT ), the generalization error of the predictor estimated from observed data. To\nemphasize the fact that the training set is fixed in this expression, one often writes:\nR( \u02c6fT ) = E(r(Y, \u02c6f T (X))|T = T)\nIf we also take the expectation with respect to T (for fixed N), we obtain the\naveraged generalization risk as\nE(R( \u02c6f T )) = E(r(Y, \u02c6f T (X))),\nwhich provides an evaluation of the average quality of the algorithm when evaluated\non random training sets of size N. If A : T 7\u2192\u02c6fT denotes the learning algorithm, we\nwill denote RN(A) = E(R( \u02c6f T )).\nSince their computation requires the knowledge of the joint distribution of X and\nY, these errors are not available in practice. Given a training set T and a predictor\nf , one can compute the empirical error\n\u02c6RT (f ) = 1\nN\nN\nX\nk=1\nr(yk,f (xk)).\nUnder the usual moment conditions, the law of large numbers implies that \u02c6RT (f ) \u2192\nR(f ) with probability one for any given predictor f . However, the law of large num-\nbers cannot be applied to assess whether the in-sample error,\nET\n\u2206= \u02c6RT ( \u02c6fT ) = 1\nN\nN\nX\nk=1\nr(yk, \u02c6fT (xk)),\n120\nCHAPTER 5. PREDICTION: BASIC CONCEPTS\nis a good approximation of the generalization error R( \u02c6fT ). This is because each term\nin the sum depends on the full data set, so that ET is not a sum of independent terms.\nThe in-sample error typically under-estimates the generalization error, sometimes\nwith a large discrepancy.\nWhen one has enough data, however, it is possible to set some of it aside to form\na test set. Formally, a test set is a collection T \u2032 = (x\u2032\n1,y\u2032\n1,...,x\u2032\nN\u2032,y\u2032\nN\u2032) considered as a\nrealization of an i.i.d. sample of (X,Y), T \u2032 = (X\u2032\n1,Y \u2032\n1,...,X\u2032\nN\u2032,Y \u2032\nN\u2032), independent of T .\nThe test set error is then given by\nET,T \u2032 = \u02c6RT \u2032( \u02c6fT ) = 1\nN \u2032\nN\u2032\nX\nk=1\nr(y\u2032\nk, \u02c6fT (x\u2032\nk)).\nThe law of large numbers (applied conditionally to T = T) implies that ET,T \u2032 con-\nverges to R( \u02c6fT ) with probability one when N \u2032 \u2192\u221e.\nHowever, in many applications, data acquisition is difficult or expensive (e.g., in\nthe medical field) and sparing a part of it in order to form a test set is not a reasonable\noption. In such situations, cross-validation is generally a preferred alternative.\n5.6.2\nCross validation\nCross-validation error\nThe n-fold cross-validation method (see, e.g., Stone [184]) separates the training set\ninto n non-overlapping sets of equal sizes, and estimates n predictors by leaving out\none of these subsets as a temporary test set. A generalization error is estimated from\neach test set and averaged over the n results.\nLet us formalize this computation after introducing some notation. We represent\ntraining data in the form T = (z1,...,zN), a sample of a random variable Z. With\nthis notation, we can include supervised problems, such as prediction (taking Z =\n(X,Y)) and unsupervised ones such as density estimation (taking Z = X). One tries\nto estimate a function f within a given class (e.g., a predictor, or a density) and\none has a measure of \u201closs\u201d, denoted \u2113(f ,z) \u22650 measuring how badly f performs\non the data z. For prediction, one takes \u2113(f ,z) = r(y,f (x)) with z = (x,y) and for\ndensity estimation, e.g., \u2113(f ,z) = \u2212logf (z), the negative log-likelihood. One then\nlets R(f ) = E(\u2113(f ,Z)). For an algorithm A : T 7\u2192\u02c6fT , the loss \u00afR(A) is the quantity of\ninterest.\n5.6. EVALUATING THE ERROR\n121\nGiven another set T \u2032 = (z\u2032\n1,...,z\u2032\nN\u2032), the empirical loss is\n\u02c6RT \u2032(f ) = 1\nN \u2032\nN\u2032\nX\nk=1\n\u2113(f ,z\u2032\nk)\nand, using T as a training set and T \u2032 as a test set, we let\nET,T \u2032 = \u02c6RT \u2032( \u02c6fT ).\nTo define an n-fold cross-validation estimator of the error, one assumes that the\ntraining set T is partitioned into n subsets of equal sizes (up to one element if N is\nnot a multiple of n), T1, . . . , Tn, so that Ti and Tj are non-intersecting if i , j, and\nT = Sn\ni=1 Ti. For each i, let T (i) = T \\ Ti, which provides the training data with the\nelements of Ti removed. Then, the n-fold cross-validation error is defined by\nECV(T) = 1\nn\nn\nX\ni=1\nET (i),Ti .\nAssuming, to simplify, that N is a multiple of n, the expectation of the cross-\nvalidation error is E(R( \u02c6fTN\u2032)), where the average is made over training sets TN\u2032 of\nsize N \u2032 = N \u2212N/n. Note that the cross-validation error is an estimate of the average\nerror of the algorithm over random training sets, not necessarily that of the current\nestimator \u02c6fT . It returns an evaluation of the algorithm A : T 7\u2192\u02c6fT . When needed,\none can emphasize this and write \u00afRCV,T (A).\nThe limit case when n = N is called leave-one-out (LOO) cross validation. In this\ncase ECV is an almost unbiased estimator of E(R( \u02c6fT )), but, because it is an average of\nfunctions of the training set that are quite similar (and that will therefore be posi-\ntively correlated), its variance (as a function of T) may be quite large. Conversely,\nsmaller values of n will have smaller variances, but larger biases. In practice, it is dif-\nficult to assess which choice of n is optimal, although 5- or 10-fold cross-validation\nis quite popular. LOO cross-validation is also often used, especially when N is small.\nModel selection using cross validation\nBecause it evaluates the quality of an algorithm, cross-validation is often used to per-\nform model selection. Indeed, many learning algorithms depends on a parameter,\nthat we will denote \u03bb. In kernel density estimation, for example, \u03bb = \u03c3 is the ker-\nnel width. For mixtures of Gaussian, \u03bb = m is the number of Gaussian terms in the\nmixtures. Formally, this means that one has, for every \u03bb, an algorithm A\u03bb : T 7\u2192\u02c6fT ,\u03bb.\n122\nCHAPTER 5. PREDICTION: BASIC CONCEPTS\nFixing a training set T , one can compute, for each \u03bb, the cross-validation error\neT (\u03bb) = \u00afRCV,T (A\u03bb). Model selection is then performed by finding\n\u03bb\u2217(T) = argmin\n\u03bb\neT (\u03bb).\nOnce this \u03bb\u2217is obtained, the final estimator is \u02c6fT,\u03bb\u2217(T ), obtained by rerunning the\nalgorithm one more time on the full training set.\nThis defines a new training algorithm, A\u2217: T 7\u2192\u02c6fT,\u03bb\u2217(T ). It is a common mistake\nto consider that the cross-validation error associated to this algorithm is still given\nby e(\u03bb\u2217(T)). This is false, because the computation of \u03bb\u2217uses the full training set.\nTo compute the cross-validation error of A\u2217, one needs to encapsulate this model\nselection procedure in an other cross-validation loop. So, one needs to compute,\nusing the previous notation,\nE\u2217\nCV(T) = 1\nn\nn\nX\ni=1\n\u02c6RTi( \u02c6fT (i),\u03bb\u2217(T (i)))\nwhere each \u02c6fT (i),\u03bb\u2217(T (i)) is computed by running a cross-validated model selection pro-\ncedure restricted to T (i). This is often called a double-loop cross-validation proce-\ndure (the number of folds in the inner and outer loops do not have to coincide). Note\nthat each \u03bb\u2217(T (i)) that does not necessarily coincide with the optimal \u03bb\u2217(T) obtained\nwith the full training set.\nChapter 6\nInner Products and Reproducing Kernels\n6.1\nIntroduction\nWe will discuss later in this book various methods that specify the prediction is as a\nlinear function of the input. These methods are often applied after taking transfor-\nmations of the original variables, in the form x 7\u2192h(x) (i.e., the prediction algorithm\nis applied to h(x) instead of x). We will refer to h as a \u201cfeature function,\u201d which typi-\ncally maps the initial data x \u2208R to a vector space, sometimes of infinite dimensions,\nthat we will denote H (the \u201cfeature space\u201d).\nThe present chapter provides a formal description of this framework, focusing,\nin particular, on situations in which H has an inner product, as this inner product\nis often instrumental in the design of linear methods on H. Many machine learning\nmethods can indeed be expressed either as functions of the coordinates of the input\ndata in some space, or as functions of the inner products between the input samples.\nSuch methods can bypass the difficulty of using high-dimensional features with the\nhelp of the theory of \u201creproducing kernels,\u201d [12, 201] which ensures that the inner\nproduct between special classes of feature functions h(x) and h(x\u2032) can be explicitly\ncomputed as a function of x and x\u2032.\n6.2\nBasic Definitions\n6.2.1\nInner-product spaces\nWe recall that a real vector space 1 is a set, H, on which an addition and a scalar\nproduct are defined, namely (h,h\u2032) \u2208H \u00d7 H 7\u2192h + h\u2032 \u2208H and (\u03bb,h) \u2208R \u00d7 H 7\u2192\u03bbh \u2208\nH, and we assume that the reader is familiar with the theory of finite-dimensional\n1All vector spaces in these notes will be real, and will therefore only be referred as vector spaces.\n123\n124\nCHAPTER 6. INNER PRODUCTS AND REPRODUCING KERNELS\nspaces.\nAn inner product on a vector space H is a bilinear function, typically denoted\n(\u03be,\u03b7) 7\u2192\u27e8\u03be , \u03b7\u27e9such that \u27e8\u03be , \u03be\u27e9\u22650 with \u27e8\u03be , \u03be\u27e9= 0 if and only if \u03be = 0. A vector\nspace equipped with an inner product is called an inner-product space. We will\noften denote the inner product with a subscript referring to the space (e.g., \u27e8\u00b7 , \u00b7\u27e9H).\nGiven such a product, the function\n\u03be 7\u2192\u2225\u03be\u2225H =\np\n\u27e8\u03be , \u03be\u27e9H\nis a norm, so that H is also a normed space (but not all normed spaces are inner-\nproduct spaces) 2.\nWhen a normed space is complete with respect to the topology induced by its\nnorm, it is called a Banach space, or a Hilbert space when the norm is associated with\nan inner product. Completeness means that Cauchy sequences in this space always\nhave a limit, i.e., if the sequence (\u03ben) is such that, for any \u03f5 > 0, there exists n0 > 0\nsuch that \u2225\u03ben \u2212\u03bem\u2225H < \u03f5 for all n,m \u2265n0, then there exists \u03be such that \u2225\u03ben \u2212\u03be\u2225H \u21920.\nCompleteness is a very natural property. It allows, for example, for the definition of\nintegrals such as\nR\nh(t)dt as limits of Riemann sums for suitable functions h : R \u2192H,\nleading (with more general notions of integrals) to proper definitions of expectations\nof H-valued random variables. Using a standard (abstract) construction, one can\nprove that any normed space (resp. inner-product) can be extended to a Banach\n(resp. Hilbert) space within which it is dense.\nNote that finite-dimensional normed spaces are always complete.\n6.2.2\nFeature spaces and kernels\nNow, consider an input set, say R, and a mapping h from R to H, where H is an inner\nproduct space. For us, R is the set over which the original input data is observed,\ntypically Rd, and H is the feature space. One can define the function Kh : R\u00d7R \u2192R\nby\nKh(x,y) = \u27e8h(x) , h(y)\u27e9H.\nThe function Kh satisfies the following two properties.\n[K1] Kh is symmetric, namely Kh(x,y) = Kh(y,x) for all x and y in R.\n[K2] For any n > 0, for any choice of scalars \u03bb1,...,\u03bbn \u2208R and any x1,...,xn \u2208R, one\nhas\nn\nX\ni,j=1\n\u03bbi\u03bbjKh(xi,xj) \u22650.\n(6.1)\n2Note that we are using double bars for the norm in H, which, in most applications, is infinite\ndimensional\n6.2. BASIC DEFINITIONS\n125\nThe first property is obvious, and the second one results from the fact that one can\nwrite\nn\nX\ni,j=1\n\u03bbi\u03bbjKh(xi,xj) =\nn\nX\ni,j=1\n\u03bbi\u03bbj\u27e8h(xi) , h(xj)\u27e9H =\n\r\r\r\r\nn\nX\ni=1\n\u03bbih(xi)\n\r\r\r\r\n2\nH \u22650.\n(6.2)\nThis leads us to the following definition.\nDefinition 6.1 A function K : R \u00d7 R 7\u2192R satisfying properties [K1] and [K2] is called\na positive kernel.\nOne says that the kernel is positive definite if the sum in (6.1) cannot vanish unless (i)\n\u03bb1 = \u00b7\u00b7\u00b7 = \u03bbn = 0 or (ii) xi = xj for some i , j.\nAn equivalent definition of positive kernels can be given using kernel matrices,\nfor which we introduce a notation.\nDefinition 6.2 If K : R \u00d7 R 7\u2192R is given, we define, for every x1,...,xn \u2208R, the kernel\nmatrix KK(x1,...,xn) with entries K(xi,xj), for i,j = 1,...,n. (If K is understood from the\ncontext, we will simply write K(x1,...,xn) instead of KK(x1,...,xn).)\nGiven this notation, it is clear that K is a positive kernel if and only if for all x1,...,xn \u2208\nR, the matrix KK(x1,...,xn) is symmetric, positive semidefinite. It is a positive def-\ninite kernel if KK(x1,...,xn) is positive definite as soon as all xj\u2019s are distinct. This\nlatter condition is obviously needed since, if xi = xj, the ith and jth columns of K\ncoincide and this matrix cannot be full-rank.\nRemark 6.3 It is important to point out that K being a positive kernel does not require\nthat K(x,y) \u22650 for all x,y \u2208R (see examples in the next section). However, it does\nimply that K(x,x) \u22650 for all x \u2208R, since diagonal elements of positive semi-definite\nmatrices are non-negative.\n\u2666\nThe function Kh defined above is therefore always a positive kernel, but not al-\nways positive definite, as seen below. We will also see later that the converse state-\nment is true: any positive kernel K : R \u00d7 R 7\u2192R can be expressed as Kh for some\nfeature function h between R and some feature space H.\nGiven a feature function h : R \u2192H, we will denote by Vh = span(h(x),x \u2208R) the\nvector space generated by the features, which, by definition, is the space of all linear\ncombinations\n\u03be =\nn\nX\ni=1\n\u03bbih(xi)\n126\nCHAPTER 6. INNER PRODUCTS AND REPRODUCING KERNELS\nwith \u03bb1,...,\u03bbm \u2208R, x1,...,xn \u2208R and n \u22650 (by convention, \u03be = 0 if n = 0). Then Kh is\npositive definite if and only if any family (h(x1),...,h(xn)) with distinct xi\u2019s is linearly\nindependent. This is a direct consequence of (6.2).\nn\nX\ni,j=1\n\u03bbi\u03bbjKh(xi,xj) =\n\r\r\r\r\r\r\r\nn\nX\ni=1\n\u03bbih(xi)\n\r\r\r\r\r\r\r\n2\nH\n.\nThis implies in particular that positive-definite kernels over infinite input spaces R\ncan only be associated to infinite-dimensional spaces H, since Vh \u2282H.\n6.3\nFirst examples\n6.3.1\nInner product\nClearly, if R is an inner product space, it has an associated reproducing kernel, de-\nfined by\nK(x,y) = \u27e8x , y\u27e9R .\nThis kernel is equal to Kh with H = R and h = id (the identity mapping). In par-\nticular K(x,y) = xT y is a positive kernel if R = Rd. This kernel can obviously take\npositive and negative values.\nNotice that this kernel is not positive definite, because the rank of K(x1,...,xn) is\nequal to the dimension of span(x1,...,xn), which can be less than n even when the\nxi\u2019s are distinct.\n6.3.2\nPolynomial Kernels\nConsider R = Rd and define\nh(x) = (x(i1) ...x(ik),1 \u2264i1,...,ik \u2264d),\nwhich contains all products of degree k formed from variables x(1),...,x(d), i.e., all\nmonomials of degree k in x. This function takes its values in the space H = RNk,\nwhere Nk = dk. Using, in H, the inner product \u27e8\u03be , \u03b7\u27e9H = \u03beT \u03b7, we have\nKh(x,y)\n=\nX\n1\u2264i1,...,ik\u2264d\n(x(i1)y(i1))\u00b7\u00b7\u00b7(x(ik)y(ik))\n=\n(xT y)k.\nThis provides the homogeneous polynomial kernel of order k.\n6.3. FIRST EXAMPLES\n127\nIf one now takes all monomials of order less than or equal to k, i.e.,\nh(x) = (x(i1) ...x(il),1 \u2264i1,...,il \u2264d,0 \u2264l \u2264k),\nwhich now takes values in a space of dimension 1 + d + \u00b7\u00b7\u00b7 + dk, the corresponding\nkernel is\nKh(x,y) = 1 + (xT y) + \u00b7\u00b7\u00b7 + (xT y)k = (xT y)k+1 \u22121\nxT y \u22121\n.\nThis provides a polynomial kernel of order k. It is important to notice here that, even\nthough the dimension of the feature space increases exponentially in k, so that the\ncomputation of the feature function rapidly becomes intractable, the computation\nof the kernel itself remains a relatively mild operation.\nOne can make variations on this construction. For example, choosing any family\nc0,c1,...,ck of positive numbers, one can take\nh(x) = (clx(i1) ...x(il),1 \u2264i1,...,il \u2264d,0 \u2264l \u2264k)\nyielding\nKh(x,y) = c2\n0 + c2\n1(xT y) + \u00b7\u00b7\u00b7 + c2\nk(xT y)k.\nTaking cl =\n \nk\nl\n!1/2\n\u03b1l for some \u03b1 > 0, we get another form of polynomial kernel,\nnamely,\nKh(x,y) = (1 + \u03b12xT y)k.\n6.3.3\nFunctional Features\nWe now consider an example in which H is infinite dimensional. Let R = Rd. We as-\nsume that a function s : Rd \u2192R is chosen, such that s is both (absolutely) integrable\nand square integrable. We also fix a scaling parameter \u03c1 > 0. Associate to x \u2208Rd the\nfunction\n\u03bex : y 7\u2192s((y \u2212x)/\u03c1),\nwhich is also square integrable (as a function of y). We define the feature function\nh : x 7\u2192\u03bex from Rd to H = L2(Rd), the space of square integrable functions on Rd\nwith inner product\n\u27e8\u03be , \u03b7\u27e9H =\nZ\nRd \u03be(z)\u03b7(z)dz.\nThe resulting kernel is\nKh(x,y) =\nZ\nRd s(z/\u03c1 \u2212x)s(z/\u03c1 \u2212y)dz = \u03c1d\nZ\nRd s(z)s(z \u2212(y \u2212x)/\u03c1)dz.\n128\nCHAPTER 6. INNER PRODUCTS AND REPRODUCING KERNELS\nNote that Kh(x,y) is \u201ctranslation-invariant,\u201d which means that it only depends on\nx \u2212y. It takes the form Kh(x,y) = \u03c1d\u0393((y \u2212x)/\u03c1) where\n\u0393(u) =\nZ\nRd s(z)s(z \u2212u)dz.\nis the convolution3 of s with \u02dcs : z 7\u2192s(\u2212z).\nLet \u03c3 be the Fourier transform of s, i.e.,\n\u03c3(\u03c9) =\nZ\nRd e\u22122i\u03c0\u03c9T us(u)du.\nBecause s is real-valued, we have \u03c3(\u2212\u03c9) = \u00af\u03c3(\u03c9), the complex conjugate of \u03c3. More-\nover, \u00af\u03c3 is also the Fourier transform of \u02dcs. Using the fact that the Fourier transform of\nthe convolution of two functions is the product of their Fourier transforms, we see\nthat the Fourier transform of \u0393 = s\u2217\u02dcs is equal to |\u03c3|2. Applying the inverse transform,\nwe find\n\u0393(u) =\nZ\nRd e2i\u03c0\u03c9T u|\u03c3(\u03c9)|2 d\u03c9 =\nZ\nRd e\u22122i\u03c0\u03c9T u|\u03c3(\u2212\u03c9)|2 d\u03c9.\nThis form is (almost) characteristic of translation-invariant kernels.\nLet us consider a few examples of kernels that can be obtained in this way.\n(1) Take d = 1 and let s be the indicator function of the interval [\u22121\n2, 1\n2]. Then, one\nfinds\n\u0393(t) = max(1 \u2212|t|,0).\nIn this case, the space Vh is the space of all functions expressed as finite sums\nz 7\u2192\nn\nX\nj=1\n\u03bbj1[xj\u2212\u03c1/2,xj+\u03c1/2](z),\nand therefore is a space of compactly-supported piecewise constant functions. Such\na function computed with distinct xj\u2019s cannot vanish everywhere unless all \u03bbj\u2019s van-\nish, so that Kh is positive definite. Indeed, let\nf (z) =\nn\nX\nj=1\n\u03bbj1[xj\u2212\u03c1/2,xj+\u03c1/2](z)\nand assume without loss of generality that x1 < x2 < \u00b7\u00b7\u00b7 < xn and let xn+1 = \u221e. Let i0\nbe the smallest index j such that \u03bbj , 0, assuming that such an index exists. Then\nf (z) = \u03bbi0 > 0 for all z \u2208[xi0 \u2212\u03c1/2,xi0+1 \u2212\u03c1/2) which is a non-empty interval. So, if f\nvanishes almost everywhere, we must have \u03bbj = 0 for all j = 1,...,n.\n3The convolution between two absolutely integrable functions f and g is defined by f \u2217g(u) =\nR\nRd f (z)g(u \u2212z)dz\n6.3. FIRST EXAMPLES\n129\n(2) Still with d = 1, let s(z) = e\u2212|z|. Then, for t > 0,\n\u0393(t) =\nZ \u221e\n\u2212\u221e\ne\u2212|z|e\u2212|z\u2212t| dz\n=\nZ 0\n\u2212\u221e\nezez\u2212t dz +\nZ t\n0\ne\u2212zez\u2212t dz +\nZ \u221e\nt\ne\u2212ze\u2212z+t dz\n= e\u2212t\n2 + te\u2212t + e\u2212t\n2\n= (1 + t)e\u2212t\nUsing the fact that \u0393(\u2212t) = \u0393(t) (make the change of variable z \u2192\u2212z in the integral),\nwe get\n\u0393(t) = (1 + |t|)e\u2212|t|.\nfor all t. This shows that\nK(x,y) = (1 + |x \u2212y|)e\u2212|x\u2212y|\nis a positive kernel on Rd.\n(3) Take s(z) = e\u2212|z|2/2, z \u2208Rd. Then\n\u0393(u) =\nZ\nRd e\u2212|z|2+|u\u2212z|2\n2\ndz = e\u2212|u|2\n4\nZ\nRd e\u2212|z\u2212u/2|2 dz\n= (4\u03c0)d/2e\u2212|u|2\n4 .\nThis provides a special case of Gaussian kernel.\n6.3.4\nGeneral construction theorems\nTranslation invariance\nAs introduced above, a kernel K is translation invariant if it takes the form K(x,y) =\n\u0393(x\u2212y) for some continuous function \u0393 defined on Rd. Bochner\u2019s theorem [33] states\nthat such a K is a positive kernel if and only if \u0393 is the Fourier transform of a positive\nmeasure, namely,\n\u0393(x) =\nZ\nRd e\u22122i\u03c0\u27e8x,\u03c9\u27e9d\u00b5(\u03c9)\nwhere \u00b5 is a positive and symmetric (invariant by sign change) measure on Rd. For\nexample one can take d\u00b5(\u03c9) = \u03bd(\u03c9)d\u03c9, where \u03bd is a integrable, positive and even\nfunction.\nThis theorem provides an at least numerical, and sometimes analytical, method\nfor constructing kernels. The previous section exhibited a special case of translation-\ninvariant kernel for which \u03bd = |\u03c3|2.\n130\nCHAPTER 6. INNER PRODUCTS AND REPRODUCING KERNELS\nRadial kernels\nA radial kernel takes the form K(x,y) = \u03b3(|x \u2212y|2), for some continuous function\n\u03b3 defined on [0,+\u221e). Shoenberg\u2019s theorem [173] states that, if this function \u03b3 is\nuniversally valid, i.e., K is a kernel for all dimensions d, then, it must take the form\n\u03b3(t) =\nZ \u221e\n0\ne\u2212\u03bbtd\u00b5(\u03bb)\nfor some positive finite measure \u00b5 on [0,+\u221e).\nFor example, when \u00b5 is a Dirac measure, i.e., \u00b5 = \u03b4(2a)\u22121 for some a > 0, then\nK(x,y) = exp(\u2212|x \u2212y|2/2a), which is the Gaussian kernel. Taking d\u00b5 = e\u2212a\u03bbd\u03bb yields\n\u03b3(t) = 1/(t + a), and d\u00b5 = \u03bbe\u2212a\u03bbd\u03bb yields \u03b3(t) = 1/(a + t)2.\nThere is also, in Schoenberg [173], a characterization of radial kernels for a fixed\ndimension d. Such kernels must take the form\n\u03b3(t) =\nZ +\u221e\n0\n\u2126d(t\u03bb)d\u00b5(\u03bb)\nwith \u2126d(t) = \u0393(d/2)(2/t)(d\u22122)/2J(d\u22122)/2(t) where J(d\u22122)/2 is Bessel\u2019s function of the first\nkind.\n6.3.5\nOperations on kernels\nKernels can be combined in several ways as described in the next proposition.\nProposition 6.4 Let K1 : R \u00d7 R \u2192R and K2 : R \u00d7 R \u2192R be positive kernels. Then the\nfollowing assertions hold.\n(i) If \u03bb1,\u03bb2 > 0, \u03bb1K1 + \u03bb2K2 is a positive kernel. It is positive definite as soon as either\nK1 or K2 is positive definite.\n(ii) For any function f : R\u2032 \u2192R, K\u2032\n1(x\u2032,y\u2032)\n\u2206= K1(f (x\u2032),f (y\u2032)) is a positive kernel. It is\npositive definite as soon as K1 is positive definite and f is one-to-one.\n(iii) K(x,y) = K1(x,y)K2(x,y) is a positive kernel. It is positive definite as soon as K1 and\nK2 are positive definite.\n(iv) Let K1 and K2 be translation-invariant with R = Rd, taking the form Ki(x,y) =\n\u0393i(x \u2212y), where \u0393i is continuous ( i = 1,2). Assume that one of the two functions\n\u03931,\u03932 is integrable on Rd. Then\nK(x,y) =\nZ\nRd K1(x,z)K2(z,y)dz\nis also a positive kernel.\n6.3. FIRST EXAMPLES\n131\nProof Point (i) is obvious. Point (ii) is almost as simple, because, for any \u03bb1,...,\u03bbn \u2208\nR and x\u2032\n1,...,x\u2032\nn \u2208R\u2032,\nn\nX\ni,j=1\n\u03bbi\u03bbjK\u2032\n1(x\u2032\ni,x\u2032\nj) =\nn\nX\ni,j=1\n\u03bbi\u03bbjK1(f (x\u2032\ni),f (x\u2032\nj)) \u22650.\nIf K1 is positive definite, then the latter sum can only vanish if all \u03bbi are zero, or\nsome of the points in (f (x\u2032\n1),...,f (x\u2032\nn)) coincide. If, in addition, f is one-to-one, then\nthis is equivalent to all \u03bbi are zero, or some of the points in (x\u2032\n1,...,x\u2032\nn) coincide, so\nthat K\u2032\n1 is positive definite.\nTo prove point (iii), take x1,...,xN \u2208Rd and form the matrices Ki = Ki(x1,...,xN),\ni = 1,2, which are, by assumption positive semi-definite. The matrix K = K(x1,...,xN)\nis the element-wise (or Hadamard) product of K1 and K2, and the conclusion fol-\nlows from the linear algebra result stating that the Hadamard product of two pos-\nitive semi-definite (resp. positive definite) matrices A = (a(i,j),1 \u2264i,j \u2264N) and\nB = (b(i,j),1 \u2264i,j \u2264N) is positive semi-definite (resp. positive definite). This is\nproved by diagonalizing, say, A in an orthonormal basis u1,...,uN, with eigenvalues\n\u03bb1,...,\u03bbN and writing\nN\nX\ni,j=1\n\u03b1(i)a(i,j)b(i,j)\u03b1(j) =\nN\nX\ni,j,k=1\n\u03b1(i)u(k)\ni u(k)\nj \u03bbkb(i,j)\u03b1(j)\n=\nN\nX\nk=1\n\u03bbk\nN\nX\ni,j=1\n(\u03b1(i)u(k)\ni )(\u03b1(j)u(k)\nj ))b(i,j) \u22650\nIf B is positive definite, then the sum above can be zero only if, for each k, either\n\u03bbk = 0 or \u03b1(i)u(k)\ni\n= 0 for all i. If A is also positive definite, then the only possibility\nis \u03b1(i)u(k)\ni\n= 0 for all i and k, which implies \u03b1(i) = 0 for all i since ui , 0.\nTo prove point (iv) 4, we first note that a translation invariant kernel K\u2032(x,y) =\n\u0393\u2032(x \u2212y) is always bounded. Indeed, the matrix K\u2032(x,0) is positive semi-definite,\nwith determinant \u0393\u2032(0)2 \u2212\u0393\u2032(x)2 > 0, showing that |\u0393\u2032(x)| < \u0393\u2032(0). This shows that\nthe integral defining K(x,y) converges as soon as one of the two functions \u03931 or \u03932 is\nintegrable. Moreover, we have K(x,y) = \u0393(x \u2212y) with\n\u0393(x) =\nZ\nRd \u03931(x \u2212z)\u03932(z)dz =\nZ\nRd \u03931(x \u2212u)\u03932(u \u2212y)du\nUsing the fact that both \u03931 and \u03932 are even, and making the change of variable z 7\u2192\u2212z,\none easily shows that \u0393(x) = \u0393(\u2212x), which implies that K is symmetric.\n4This part of the proof uses some measure theory.\n132\nCHAPTER 6. INNER PRODUCTS AND REPRODUCING KERNELS\nWe proceed with the assumption that \u03932 is integrable and use Bochner\u2019s theorem\nto write\n\u03931(y) =\nZ\nRd e\u2212i\u03beT yd\u00b51(\u03be)\nfor some positive finite measure \u00b51. Then\n\u0393(x) =\nZ\nRd\n Z\nRd e\u22122i\u03c0\u03beT (x\u2212z)d\u00b51(\u03be)\n!\n\u03932(z)dz\n=\nZ\nRd e\u22122i\u03c0\u03beT x\n Z\nRd e2i\u03c0\u03beT z\u03932(z)dz\n!\nd\u00b51(\u03be)\nThe shift in the order of the variables \u03be and z uses Fubini\u2019s theorem. The function\n\u03c8(\u03be) =\nZ\nRd e2i\u03c0\u03beT z\u03932(z)dz\nis the inverse Fourier transform of \u03932. Because \u03932 is bounded and integrable, it is also\nsquare integrable, which implies that its inverse Fourier transform is also a square\nintegrable function. Since Bochner\u2019s theorem implies that \u03932 is the Fourier transform\nof a positive measure \u00b52, we find, using the injectivity of the Fourier transform, that\n\u03c8 is non-negative. So \u0393 is the Fourier transform of the finite positive measure \u03c8d\u00b51,\nwhich implies that K is a positive kernel.\n\u25a0\nPoint (iv) can be related to the following discrete statement on symmetric ma-\ntrices: assume that A and B are positive semi-definite and that they commute, so\nthat AB = BA: then AB is positive semi-definite (see ??). In the case of kernels, one\nmay consider the symmetric linear operators Ki : f 7\u2192\nR\nRd Ki(\u00b7,y)f (y)dy which maps\nthe space of square integrable functions into itself. Then K1 and K2 commute and\nK = K1K2.\n6.3.6\nCanonical Feature Spaces\nLet K be a positive kernel on a set R. The following construction, which is fun-\ndamental, shows that K can always be associated with a feature function h taking\nvalues in a suitably chosen inner-product space H.\nAssociate to each x \u2208R the function \u03bex : y 7\u2192K(y,x) (we will also write \u03bex =\nK(\u00b7,x)), and let HK = span(\u03bex,x \u2208R), a subspace of the vector space of all functions\nfrom R to R. Define the feature function h : x 7\u2192\u03bex from R to HK. There is a unique\ninner product on HK such that K = Kh. Indeed, by definition, this requires\n\u27e8K(\u00b7,x) , K(\u00b7,y)\u27e9HK = K(x,y).\n(6.3)\n6.4. PROJECTION ON A FINITE-DIMENSIONAL SUBSPACE\n133\nMoreover, by linearity, for any \u03be = Pn\ni=1 \u03bbiK(\u00b7,xi) and \u03b7 = Pm\ni=1 \u00b5iK(\u00b7,yi), one needs\n\u27e8\u03be , \u03b7\u27e9HK =\nn\nX\ni=1\nm\nX\nj=1\n\u03bbi\u00b5jK(xi,yj),\nso that the inner product is uniquely specified on HK. To make sure that this inner-\nproduct is well defined, we must check that there is no ambiguity, in the sense that,\nif \u03be has an alternative decomposition \u03be = Pn\u2032\ni=1 \u03bb\u2032\niK(\u00b7,x\u2032\ni), then, the value of \u27e8\u03be , \u03b7\u27e9HK\nremains unchanged. But this is clear, because one can also write\n\u27e8\u03be , \u03b7\u27e9HK =\nm\nX\nj=1\n\u00b5j\u03be(yj),\nwhich only depends on \u03be and not on its decomposition. The linearity of the product\nwith respect to \u03be is also clear from this expression, and the bilinearity by symmetry.\nThe Schwartz inequality implies that\n|\u27e8\u03be , \u03b7\u27e9HK| \u2264\u2225\u03be\u2225HK \u2225\u03b7\u2225HK\nFrom which we deduce that \u2225\u03be\u2225HK = 0 implies that \u27e8\u03be , \u03b7\u27e9HK = 0 for \u03b7 \u2208HK. Since\n\u27e8\u03be , K(\u00b7,y)\u27e9HK = \u03be(y) for all y, this also implies that \u03be = 0, completing the proof that\nHK is an inner-product space.\nEquation (6.3) is the \u201creproducing property\u201d of the kernel for the inner-product\non HK. In functional analysis, the completion, \u02c6HK, of HK for the topology associated\nto its norm is then a Hilbert space, and is referred to as a \u201creproducing kernel Hilbert\nspace,\u201d or RKHS.\nMore generally, an inner-product space H of functions h : R \u2192R is a reproducing\nkernel Hilbert space if H is a complete space (which makes it Hilbert) and there\nexists a positive kernel K such that,\n[RKHS1] For all x \u2208R, K(\u00b7,x) belongs to H,\n[RKHS2] For all h \u2208H and x \u2208R,\n\u27e8h , K(\u00b7,x)\u27e9H = h(x).\nReturning to the example of functional features in section 6.3.3, we have two dif-\nferent representations of the kernel in feature space, namely in H = L2(Rd), or in HK,\nwith a different inner product. There is not a contradiction, and simply shows that\nthe representation of a positive kernel in terms of a feature function is not unique.\n134\nCHAPTER 6. INNER PRODUCTS AND REPRODUCING KERNELS\n6.4\nProjection on a finite-dimensional subspace\nIf H is an inner-product space and V is a subspace of H, one defines the orthogonal\nprojection of an element \u03be \u2208H on V as its closest point in V , that is, the element\n\u03b7\u2217of V minimizing the function F : \u03b7 7\u2192\u2225\u03b7 \u2212\u03be\u22252\nH over all \u03b7 \u2208V . This closest point\ndoes not always exist, but it does in the special case in which V is finite dimensional\n(or, more generally, when V is a closed subspace of H; see Yosida [205]). We state,\nwithout proof, some of the properties of this operation.\nAssuming that V is closed, this minimizer is unique and will be denoted \u03b7\u2217=\n\u03c0V (\u03be). Moreover, \u03c0V is a linear transformation from H to V , and \u03b7\u2217is characterized\nby the properties\n(\u03b7\u2217\u2208V\n\u03be \u2212\u03b7\u2217\u22a5V ,\nthe last condition meaning that \u27e8\u03be \u2212\u03b7\u2217, \u03b7\u27e9H = 0 for all \u03b7 \u2208V .\nBecause \u2225\u03be\u22252\nH = \u2225\u03c0V (\u03be)\u22252\nH + \u2225\u03be \u2212\u03c0V (\u03be)\u22252\nH, one always has \u2225\u03c0V (\u03be)\u2225H \u2264\u2225\u03be\u2225H, with\ninequality if and only if \u03c0V (\u03be) = \u03be, i.e., if and only if \u03be \u2208V .\nIf V is finite-dimensional and \u03b71,...,\u03b7n is a basis of V , then \u03c0V (\u03be) is given by\n\u03c0V (\u03be) =\nn\nX\ni=1\n\u03b1(i)\u03b7i\nwith \u03b1 (considered as a column vector in Rn) given by\n\u03b1 = Gram(\u03b71,...,\u03b7n)\u22121\u03bb,\nwhere \u03bb \u2208Rn is the vector with coordinates \u03bb(i) = \u27e8\u03be , \u03b7i\u27e9H, i = 1,...,n. The Gram ma-\ntrix of \u03b71,...,\u03b7n, denoted Gram(\u03b71,...,\u03b7n), is the n by n matrix with entries \u27e8\u03b7i , \u03b7j\u27e9H\nfor i,j = 1,...,n.\nIf A is a subset of H, the set A\u22a5consists of all vectors perpendicular to A, namely\nA\u22a5=\nn\nh \u2208H : \u27e8h , \u02dch\u27e9H = 0 for all \u02dch \u2208A\no\n.\nIf V is a finite-dimensional (or, more generally, closed) subspace of H, then any point\nin h is decomposed as h = \u03c0V (h)+h\u2212\u03c0V (h) with h\u2212\u03c0V (h) \u2208V \u22a5. This shows that \u03c0V \u22a5\nis well defined and equal to idH \u2212\u03c0V .\nOrthogonal projections can be applied to function interpolation in an RKHS. In-\ndeed, assuming that H is an RKHS, as described at the end of the previous sec-\ntion, with a positive-definite kernel. Given distinct points x1,...,xN \u2208R and values\n6.4. PROJECTION ON A FINITE-DIMENSIONAL SUBSPACE\n135\n\u03b11,...,\u03b1N \u2208R, the interpolation problem consists in finding h \u2208H with minimal\nnorm satisfying h(xk) = \u03b1k, k = 1,...,N. Consider the finite dimensional space\nV = span{K(\u00b7,xk),k = 1,...N}.\nThen there exists an element h0 \u2208V that satisfies the constraints. Indeed, looking\nfor h0 in the form\nh0(x) =\nN\nX\nl=1\nK(x,xl)\u03bbl\none has\nh0(xk) =\nN\nX\nl=1\nK(xk,xl)\u03bbl\nso that\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\n\u03bb1...\n\u03bbN\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\n= K(x1,...,xN)\u22121\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\n\u03b11...\n\u03b1N\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\nAny other function h satisfying the constraints satisfies h(xk) \u2212h0(xk) = 0, which,\nusing RKHS2, is equivalent to \u27e8h \u2212h0 , K(\u00b7,xk)\u27e9H = 0, i.e., to h \u2212h0 \u2208V \u22a5. This shows\nthat h0 = \u03c0V (h), so that \u2225h\u2225H \u2265\u2225h0\u2225H and h0 provides the optimal interpolation. We\nsummarize this in the proposition:\nProposition 6.5 Let H is an RKHS with a positive-definite kernel. Let x1,...,xN \u2208R be\ndistinct points and \u03b11,...,\u03b1N \u2208R. Then the function h \u2208H with minimal norm satisfying\nh(xk) = \u03b1k, k = 1,...,N takes the form\nh(xk) =\nN\nX\nl=1\nK(xk,xl)\u03bbl\n(6.4a)\nwith\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\n\u03bb1...\n\u03bbN\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\n= K(x1,...,xN)\u22121\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\n\u03b11...\n\u03b1N\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\n.\n(6.4b)\nA variation of this problem replaces the constraint by a penalty that complete\nthe minimization associated with the orthogonal projection, namely, minimizing (in\nh \u2208H)\n\u2225h\u22252\nH + \u03c32\nN\nX\nk=1\n|h(xk) \u2212\u03b1k|2.\n136\nCHAPTER 6. INNER PRODUCTS AND REPRODUCING KERNELS\nLetting h0 = \u03c0V (h), so that h0(xk) = h(xk) for all k, this expression can be rewritten as\n\u2225h0\u22252\nH + \u2225h \u2212h0\u22252\nH + \u03c32\nN\nX\nk=1\n|h0(xk) \u2212\u03b1k|2.\nThis shows that the optimal h must coincide with its projection on V , and therefore\nbelong to that subspace. Looking for h in the form\nh(\u00b7) =\nN\nX\nl=1\nK(\u00b7,xl)\u03bbl,\nthe objective function is rewritten as\nN\nX\nk,l=1\nK(xk,xl)\u03bbk\u03bbl + \u03c32\nN\nX\nk=1\n\f\f\f\f\f\f\f\nN\nX\nl=1\nK(xk,xl)\u03bbl \u2212\u03b1k\n\f\f\f\f\f\f\f\n2\n,\nwhich, in vector notation gives, writing \u03bb =\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\n\u03bb1...\n\u03bbN\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\nand \u03b1 =\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\n\u03b11...\n\u03b1N\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\n,\n\u03bbT K(x1,...,xN)\u03bb + \u03c32 (K(x1,...,xN)\u03bb \u2212\u03b1)T (K(x1,...,xN)\u03bb \u2212\u03b1).\nThe differential of this expression in \u03bb is\nK(x1,...,xN)\u03bb + 2\u03c32K(x1,...,xN)(K(x1,...,xN)\u03bb \u2212\u03b1).\nAssuming that x1,...,xN are distinct, this vanishes if and only if\n\u03bb = (K(x1,...,xN) + (1/\u03c32)IdRN)\u22121\u03b1.\nWe have just proved the proposition:\nProposition 6.6 Let H is an RKHS with a positive-definite kernel. Let x1,...,xN \u2208R be\ndistinct points and \u03b11,...,\u03b1N \u2208R. Then the unique minimizer of\nh 7\u2192\u2225h\u22252\nH + \u03c32\nN\nX\nk=1\n|h(xk) \u2212\u03b1k|2\non H is given by\nh(xk) =\nN\nX\nl=1\nK(xk,xl)\u03bbl\n(6.5a)\nwith\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\n\u03bb1...\n\u03bbN\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\n= (K(x1,...,xN) + (1/\u03c32)IdRN)\u22121\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\n\u03b11...\n\u03b1N\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\n.\n(6.5b)\nChapter 7\nLinear Models for Regression\nIn regression, linear models refer to situations in which one tries to predict the de-\npendent variable Y \u2208RY = Rq by a function \u02c6f (X) of the dependent variable X \u2208RX,\nwhere \u02c6f is optimized over a linear space F . The most common situation is the \u201cstan-\ndard linear model,\u201d for which RX = Rd and\nF = {f (x) = a0 + bT x : a0 \u2208Rq,b \u2208Md,q(R)}.\n(7.1)\nMore generally, with q = 1, given a mapping h : R \u2192H, where H is an inner-\nproduct space, one can take:\nF = {f (x) = a0 + \u27e8b , h(x)\u27e9H : a0 \u2208R,b \u2208H}.\n(7.2)\nNote that h can be nonlinear, and F can be infinite dimensional. Such sets corre-\nsponds to linear models using feature functions, and will be addressed using kernel\nmethods in this chapter.\nNote also that, even if the model is linear, the associated training algorithms\ncan be nonlinear, and we will review in fact several situations in which solving the\nestimation problem requires nonlinear optimization methods.\n7.1\nLeast-Square Regression\n7.1.1\nNotation and Basic Estimator\nWe denote by Y and X the dependent and independent variables of the regression\nproblem. We will assume that Y takes values in Rq and that X takes values in a set\nRX, which will, by default, be equal to Rd, except when discussing kernel methods,\nfor which this set can be arbitrary (provided that there is a mapping h from RX to\nan inner product space H with an easily computable kernel).\n137\n138\nCHAPTER 7. LINEAR REGRESSION\nLeast-square regression uses the risk function r(y,y\u2032) = |y \u2212y\u2032|2. The prediction\nerror is then R(f ) = E(|Y \u2212f (X)|2) for any predictor f and the Bayes predictor is the\nconditional expectation x 7\u2192E(Y | X = x) (see item Example 1. in section 5.3). We\nalso start with the standard setting where RX = Rd and F given by (7.1).\nWe will use the following notation, which sometimes simplifies the computation.\nIf x \u2208Rd, we let \u02dcx =\n \n1\nx\n!\n, which belongs to Rd+1. The linear predictor f (x) = a0 +\nbT x with a0 \u2208Rq,b \u2208Md,q(R) can then be written as f (x) = \u03b2T \u02dcx with \u03b2 =\n \naT\n0\nb\n!\n\u2208\nMd+1,q(R).\nIn a model-based approach, the linear model is a Bayes predictor under the\ngenerative assumption that Y = a0 + bT X + \u03f5 where \u03f5 is a residual noise satisfying\nE(\u03f5 | X) = 0, which is true, for example, when \u03f5 is centered and independent of\nX. If one further specifies the model so that \u03f5 is Gaussian, centered and indepen-\ndent of X, and one assumes that the distribution of X does not depend on a0 and b,\nthen the maximum likelihood estimator of these parameters based on a training set\nT = ((x1,y1),...,(xN,yN)) must minimize the \u201cresidual sum of squares:\u201d\nRSS(\u03b2)\n\u2206= N \u02c6R(f )\n\u2206=\nN\nX\nk=1\n|yk \u2212f (xk)|2 =\nN\nX\nk=1\n|yk \u2212\u03b2T \u02dcxk|2 .\nIn other terms, the model-based approach is identical, under these (standard) as-\nsumptions, to empirical risk minimization (section 5.5), on which we now focus.\n(Recall that, even when using a model-based approach, one does not make assump-\ntions on the true distribution of X and Y; one rather treats the model as an approxi-\nmation of these distributions, estimated by maximum likelihood, and uses the Bayes\npredictor for the estimated model.)\nThe computation of the optimal regression parameters is made easier by the in-\ntroduction of the following matrices. Introduce the N \u00d7 (d + 1) matrix X with rows\n\u02dcxT\n1 ,..., \u02dcxT\nN and the N \u00d7 q matrix Y with rows yT\n1 ,...,yT\nN, that is:\nX =\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\n1\nx(1)\n1\n\u00b7\u00b7\u00b7\nx(d)\n1\n...\n...\n...\n1\nx(1)\nN\n\u00b7\u00b7\u00b7\nx(d)\nN\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\n,\nY =\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\ny(1)\n1\n\u00b7\u00b7\u00b7\ny(q)\n1\n...\n...\ny(1)\nN\n\u00b7\u00b7\u00b7\ny(q)\nN\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\n.\nWith this notation, we have\nRSS(\u03b2) = |Y \u2212X \u03b2|2\n2.\nwith |A|2\n2 = trace(AT A) for a rectangular matrix A. The solution of the problem is\nthen provided by the following theorem.\n7.1. LEAST-SQUARE REGRESSION\n139\nTheorem 7.1 Assume that the matrix X has rank d + 1. Then the RSS is minimized for\n\u02c6\u03b2 = (X T X )\u22121X T Y\nProof We provide two possible proofs of this elementary problem. The first one is\nan optimization argument noting that F(\u03b2)\n\u2206= RSS(\u03b2) is a convex function defined\non Md+1,q(R) and with values in R. Since F is quadratic, we have, for any matrix\nh \u2208Md+1,q(R),\ndF(\u03b2)h = \u2202\u03f5F(\u03b2 + \u03f5h)|\u03f5=0 = \u22122trace(hT X T (Y \u2212X \u03b2))\nand\ndF(\u03b2) = 0 \u21d4X T (Y \u2212X \u03b2) = 0 \u21d4\u03b2 = \u02c6\u03b2.\nOne can alternatively proceed with a direct computation. We have\nRSS(\u03b2) = |Y|2\n2 \u22122trace(\u03b2T X T Y) + trace(\u03b2T X T X \u03b2)\n= |Y|2\n2 \u22122trace(\u03b2T X T X \u02c6\u03b2) + trace(\u03b2T X T X \u03b2).\nReplacing \u03b2 by \u02c6\u03b2 and simplifying yields\nRSS( \u02c6\u03b2) = |Y|2\n2 \u2212trace( \u02c6\u03b2T X T X \u02c6\u03b2)\nIt follows that\nRSS(\u03b2) =RSS( \u02c6\u03b2) + trace( \u02c6\u03b2T X T X \u02c6\u03b2) \u22122trace(\u03b2T X T X \u02c6\u03b2) + trace(\u03b2T X T X \u03b2)\n=RSS( \u02c6\u03b2) + |X ( \u02c6\u03b2 \u2212\u03b2)|2\n2\nso that the left-hand side is minimized at \u03b2 = \u02c6\u03b2.\n\u25a0\nRemark 7.2 If X does not have rank d + 1, then optimal solutions exist, but they\nare not unique. By convexity, the solutions are exactly the vectors \u03b2 at which the\ngradient vanishes, i.e., those that satisfy X T X \u03b2 = X T Y. The set of solutions can be\nobtained by introducing the SVD of X in the form X = UDV T and letting \u03b3 = V T \u03b2\nand Z = UT Y. Then\nX T X \u03b2 = X T Y \u21d4DT D\u03b3 = DT Z.\nLetting d(1),...,d(m) denote the nonzero diagonal entries of D (so that m \u2264d + 1), we\nfind \u03b3(i) = z(i)/d(i) for i \u2264m (the other equalities being 0 = 0). So, the d + 1 \u2212m last\nentries of \u03b3 can be chosen arbitrarily (and \u03b2 = V \u03b3).\n\u2666\nAn alternate representation of the solution use a two-step computation that esti-\nmates b first, then a0. Indeed, for fixed \u02c6b, the minimum of\nN\nX\nk=1\n|yk \u2212a0 \u2212xT\nk \u02c6b|2\n140\nCHAPTER 7. LINEAR REGRESSION\nis attained at \u02c6a0 = \u00afy \u2212\u00afxT \u02c6b with the usual definitions\n\u00afy = 1\nN\nN\nX\nk=1\nyk and \u00afx = 1\nN\nN\nX\nk=1\nxk.\nThis shows that \u02c6b itself must be a minimizer of\nN\nX\nk=1\n|yk \u2212\u00afy \u2212(xk \u2212\u00afx)T b|2.\nDenote by Yc and Xc the matrices\nXc =\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\nx(1)\n1 \u2212x(1)\n\u00b7\u00b7\u00b7\nx(d)\n1 \u2212x(d)\n...\n...\nx(1)\nN \u2212x(1)\n\u00b7\u00b7\u00b7\nx(d)\nN \u2212x(d)\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\n, Yc =\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\ny(1)\n1 \u2212y(1)\n\u00b7\u00b7\u00b7\ny(q)\n1 \u2212y(q)\n...\n...\ny(1)\nN \u2212y(1)\n\u00b7\u00b7\u00b7\ny(q)\nN \u2212y(q)\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\n.\nThen \u02c6b must minimize |Yc \u2212Xcb|2, yielding\n\u02c6b = (X T\nc Xc)\u22121X T\nc Yc,\n\u02c6a0 = \u00afy \u2212\u00afxT \u02c6b.\nThe reader may want to double-check that this solution coincides with the one pro-\nvided in theorem 7.1.\n7.1.2\nLimit behavior\nThe matrix\n\u02c6\u03a3XX = 1\nN X T\nc Xc = 1\nN\nN\nX\nk=1\n(xk \u2212x)(xk \u2212x)T\nis a sample estimate of the covariance matrix of X, that we will denote \u03a3XX. Simi-\nlarly, \u02c6\u03a3XY = X T\nc Yc/N is a sample estimate of \u03a3XY, the covariance between X and Y.\nWith this notation, we have\n\u02c6b = \u02c6\u03a3\u22121\nXX \u02c6\u03a3XY,\nwhich, by the law of large numbers, converges to b\u2217= \u03a3\u22121\nXX\u03a3XY.\nLet a\u2217\n0 = mY\u2212mT\nXb\u2217. Then f \u2217(x) = a\u2217\n0+(b\u2217)T x is the least-square optimal approxima-\ntion of Y by a linear function of X, and the linear predictor \u02c6f (x) = \u02c6a0 + \u02c6bT x converges\na.s. to f \u2217(x). Of course, f \u2217generally differs from f : x 7\u2192E(Y | X = x), which is the\nleast-square optimal approximation of Y by any (square-integrable) function of X,\nso that the linear estimator will have a residual bias.\n7.1. LEAST-SQUARE REGRESSION\n141\n7.1.3\nGauss-Markov theorem\nIf one makes the (unlikely) assumption that the linear model is exact, i.e., f (x) =\nf \u2217(x), one has:\nE( \u02c6\u03b2) = E(E( \u02c6\u03b2 | X )) = E((X T X )\u22121X TE(Y | X )) = E((X T X )\u22121X T X \u03b2) = \u03b2\nand the estimator is \u201cunbiased.\u201d Under this parametric assumption, many other\nproperties of linear estimators can be proved, among which the well-known Gauss-\nMarkov theorem on the optimality of least-square estimation that we now state and\nprove. For this theorem, for which we take (for simplicity) q = 1, we also assume\nthat var(Y | X = x), the variance of Y for its conditional distribution given X does\nnot depend on x, and denote it by \u03c32. This typically correspond to the standard\nregression model in which one assumes that Y = f (X) + \u03f5 where \u03f5 is independent of\nX with variance \u03c32.\nRecall that a symmetric matrix A is said to be larger than or equal to another\nsymmetric matrix, B, writing A \u2ab0B, if and only if A \u2212B is positive semi-definite.\nTheorem 7.3 (Gauss-Markov) Assume that an estimator \u02dc\u03b2 takes the form \u02dc\u03b2 = A(X )Y\n(it is linear) and is unbiased conditionally to X : E\u03b2( \u02dc\u03b2 | X ) = \u03b2 (for all \u03b2). Then (under\nthe assumptions above) the covariance matrix of \u02dc\u03b2 cannot be smaller than that of the least\nsquare estimate, \u02c6\u03b2.\nProof We write A = A(X ) for short. The condition that E(AY | X ) = \u03b2 for all \u03b2 yields\nAX \u03b2 = \u03b2 for all \u03b2, or AX = IdRd+1 (A is a (d + 1) \u00d7 N matrix). Since \u02dc\u03b2 is unbiased, its\ncovariance matrix is\nE(AYY T AT ) \u2212\u03b2\u03b2T\nand\nE(AYY T AT ) = E(E(AYY T AT | X )) = \u03c32E(AAT ).\nFor \u02dc\u03b2 = \u02c6\u03b2, for which A = (X T X )\u22121X T , we get E(AYY T AT ) = \u03c32E((X T X )\u22121). We\ntherefore need to show that E(AAT ) \u2ab0E(X T X ), i.e., that for any u \u2208Rd+1,\nuTE(AAT )u \u2265uTE((X T X )\u22121)u\nas soon as AX = IdRd+1. We in fact have the stronger result (without expectations):\nAX = IdRd+1 \u21d2AAT \u2ab0(X T X )\u22121.\nTo see this, fix u and consider the problem of minimizing Fu(A) = A 7\u2192uT AAT u\nsubject to the linear constraint AX = IdRd+1. The Lagrange multipliers for this affine\nconstraint can be organized in a matrix C and the Lagrangian is\nuT AAT u + trace(CT (AX \u2212IdRd+1)).\n142\nCHAPTER 7. LINEAR REGRESSION\nTaking the derivative in A, we find that optimal solutions must satisfy\n2uT AHT u + trace(CT HX ) = 0\nfor all H, which yields trace(HT (2uuT A + CX T )) = 0 for all H. This is only possible\nwhen 2uuT A + CX T = 0, which in turn implies that 2uuT AX = \u2212CX T X . Using the\nconstraint, we get\nC = \u22122uuT (X T X )\u22121\nso that uuT A = uuT (X T X )\u22121X T . This implies that A = (X T X )\u22121X T (the least-square\nestimator) is a minimizer of Fu(A) for all u.\nAny other solution that satisfies uuT A = uuT (X T X )\u22121X T for all u. Taking u = ei\nand summing over i (with Pd+1\ni=1 eieT\ni = IdRd+1) yields A = (X T X )\u22121X T .\n\u25a0\n7.1.4\nKernel Version\nWe now assume that X takes its values in an arbitrary set RX, with a representation\nh : RX \u2192H into an inner-product space. This representation does not need to be\nexplicit or computable, but the associated kernel K(x,y) = \u27e8h(x) , h(y)\u27e9H is assumed\nto be known and easy to compute. (Recall that, from chapter 6, a positive kernel is\nalways associated with an inner-product space.) In particular, any algorithm in this\ncontext should only rely on the kernel, and the function h only has a conceptual role.\nAssume that q = 1 to lighten the notation, so that the dependent variable is scalar-\nvalued. We here let the space of predictors be\nF = {f (x) = a0 + \u27e8b , h(x)\u27e9H : a0 \u2208R,b \u2208H}.\nThe residual sum of squares associated with this function space is\nRSS(a0,b) =\nN\nX\nk=1\n(yk \u2212a0 \u2212\u27e8b , h(xk)\u27e9)2.\nThe following result (or results similar to it) is a key step in almost all kernel\nmethods in machine learning.\nProposition 7.4 Let V = span(h(x1),...,h(xN)) be the finite-dimensional subspace of H\ngenerated by the feature functions evaluated on training input data. Then\nRSS(a0,b) = RSS(a0,\u03c0V (b)).\nwhere \u03c0V is the orthogonal projection on V .\n7.2. RIDGE REGRESSION AND LASSO\n143\nProof The justification is immediate: since h(xk) \u2208V , we have\n\u27e8b , h(xk)\u27e9H = \u27e8\u03c0V (b) , h(xk)\u27e9H\nfor all b \u2208H.\n\u25a0\nThis shows that there is no loss of generality in restricting the minimization of\nthe residual sum of squares to b \u2208V . Such a b takes the form\nb =\nN\nX\nk=1\n\u03b1kh(xk)\n(7.3)\nand the regression problem can be reformulated as a function of the coefficients\n\u03b11,...,\u03b1N \u2208R, with\nf (x) = a0 +\nN\nX\nk=1\n\u03b1k\u27e8h(x) , h(xk)\u27e9H = a0 +\nN\nX\nk=1\n\u03b1kK(x,xk),\nwhich only depends on the kernel. (This reduction is often referred to as the \u201ckernel\ntrick.\u201d)\nHowever, the solution of the problem is, in this context, not very interesting.\nIndeed, assume that K is positive definite and that all observations in the training set\nare distinct. Then the matrix K(x1,...,xN) formed by the kernel evaluations K(xi,xj)\nis invertible, and one can solve exactly the equations\nyk =\nN\nX\nj=1\n\u03b1jK(xk,xj),\nk = 1,...,N\nto get a zero RSS with a0 = 0. Unless there is no noise, such a solution will certainly\noverfit the data. If K is not positive definite, and the dimension of V is less than\nN (since this would place us in the previous situation otherwise), then it is more\nefficient to work directly in a basis of V rather than using the over-parametrized ker-\nnel representation. We will see however, starting with the next section, that kernel\nmethods become highly relevant as soon as the regression is estimated with some\ncontrol on the size of the regression coefficients, b.\n7.2\nRidge regression and Lasso\n7.2.1\nRidge Regression\nMethod.\nWhen the set F of possible predictors is too large, some additional com-\nplexity control is needed to reduce the estimation variance. One simple approach\n144\nCHAPTER 7. LINEAR REGRESSION\nis to limit the number of parameters to be estimated, which, for regression, corre-\nsponds to limiting the number of possible predictors. This is related to the methods\nof Sieves mentioned in section 4.1. In contrast, ridge regression and lasso control the\nsize of the parameters, as captured by their norm.\nIn both cases, one assigns a measure of complexity, denoted f 7\u2192\u03b3(f ) \u22650, to each\nelement f \u2208F . Given \u03b3, one can either optimize this predictor (using, for example,\nthe RSS) with the constraint that \u03b3(f ) \u2264C for some constant C, or add a penalty\n\u03bb\u03b3(f ) to the objective function for some \u03bb > 0. In general, the two approaches (con-\nstraint or penalty) are equivalent.\nIn linear spaces, complexity measures are often associated with a norm, and ridge\nregression uses the sum of squares of coefficients of the prediction matrix b, mini-\nmizing\nN\nX\nk=1\n|yk \u2212a0 \u2212bT xk|2 + \u03bbtrace(bT b),\n(7.4)\nwhich can be written in vector form as\n|Y \u2212X \u03b2|2\n2 + \u03bbtrace(\u03b2T \u2206\u03b2),\nwhere \u2206= diag(0,1,...,1). In the following, we will work with an unspecified (d +\n1) \u00d7 (d + 1) symmetric positive semi-definite matrix \u2206. Various choices are indeed\npossible, for example, \u2206= diag(0, \u02c6\u03c32(1),..., \u02c6\u03c32(d)), where \u02c6\u03c32(i) is the empirical vari-\nance of the ith coordinate of X in the training set. This last choice is quite natural,\nbecause it ensures that, whenever one of the variable X(i) is rescaled by a factor c,\nthe corresponding optimal ith row of bT is rescaled by 1/c, leaving the predictor\nunchanged.\nUnder this assumption, the optimal parameter is\n\u02c6\u03b2\u03bb = (X T X + \u03bb\u2206)\u22121X T Y ,\nwith a proof similar to that made for least-square regression. We obviously retrieve\nthe original formula for regression when \u03bb = 0.\nAlternatively, assuming that \u2206=\n \n0\n0\n0\n\u2206\u2032\n!\n, so that no penalty is imposed on the\nintercept, we have\n\u02c6b\u03bb = (X T\nc Xc + \u03bb\u2206\u2032)\u22121X T\nc Yc\n(7.5)\nand \u02c6a0\u03bb = \u00afy \u2212(\u02c6b\u03bb)T \u00afx. The proof of these statements is left to the reader.\n7.2. RIDGE REGRESSION AND LASSO\n145\nAnalysis in a special case\nTo illustrate the impact of the penalty term on balancing\nbias and variance, we now make a computation in the special case when Y = \u02dcX\u03b2 + \u03f5,\nwhere var(\u03f5) = \u03c32 and \u03f5 is independent of X. In the following computation, we as-\nsume that the training set is fixed (or rather, compute probabilities and expectations\nconditionally to it). Also, to simplify notation, we denote\nS\u03bb = X T X + \u03bb\u2206=\nN\nX\nk=1\n\u02dcxT\nk \u02dcxk + \u03bb\u2206\nand \u03a3 = E( \u02dcXT \u02dcX) for a single realization of X. Finally, we assume that q = 1, also to\nsimplify the discussion.\nThe mean-square prediction error is\nR(\u03bb)\n=\nE((Y \u2212\u02dcXT \u02c6\u03b2\u03bb)2)\n=\nE(( \u02dcXT (\u03b2 \u2212\u02c6\u03b2\u03bb) + \u03f5)2)\n=\n( \u02c6\u03b2\u03bb \u2212\u03b2)T \u03a3( \u02c6\u03b2\u03bb \u2212\u03b2) + \u03c32.\nDenote by \u03f5k the (true) residual \u03f5k = yk \u2212\u02dcxT\nk \u03b2 on training data and by \u03f5 the vector\nstacking these residuals. We have, writing S0 = S\u03bb \u2212\u03bb\u2206,\n\u02c6\u03b2\u03bb\n=\nS\u22121\n\u03bb X T Y\n=\nS\u22121\n\u03bb S0\u03b2 + S\u22121\n\u03bb X T \u03f5\n=\n\u03b2 \u2212\u03bbS\u22121\n\u03bb \u2206\u03b2 + S\u22121\n\u03bb X T \u03f5\nSo we can rewrite\nR(\u03bb) = \u03bb2\u03b2T \u2206S\u22121\n\u03bb \u03a3S\u22121\n\u03bb \u2206\u03b2 \u22122\u03bb\u03f5T X S\u22121\n\u03bb \u03a3S\u22121\n\u03bb \u2206\u03b2 + \u03f5T X S\u22121\n\u03bb \u03a3S\u22121\n\u03bb X T \u03f5 + \u03c32.\nLet us analyze the quantities that depend on the training set in this expression. The\nfirst one is S\u03bb = S0 + \u03bb\u2206. From the law of large numbers, S0/N \u2192\u03a3 when N tends\nto infinity, so that, assuming in addition that \u03bb = \u03bbN = O(N), we have S\u22121\n\u03bb = O(1/N).\nThe second one is\n\u03f5T X =\nN\nX\nk=1\n\u03f5k \u02dcxk\nwhich, according to the central limit theorem, is such that\nN \u22121/2\u03f5T X \u223cN (0,\u03c32Var( \u02dcX))\nwhen N \u2192\u221e. So, we can expect the coefficient of \u03bb2 in R(\u03bb) to have order N \u22122, the\ncoefficient of \u03bb to have order N \u22123/2 and the constant coefficient of have order N \u22121.\nThis suggests taking \u03bb = \u00b5\n\u221a\nN so that all coefficients have roughly the same order\nwhen expanding in powers of \u00b5.\n146\nCHAPTER 7. LINEAR REGRESSION\nThis gives S\u03bb = N(S0/N +\u00b5\u2206/\n\u221a\nN) \u2243N\u03a3 and we make the approximation, letting\n\u03be = N \u22121/2\u03c3\u22121/2\u03f5X T and \u03b3 = \u03a3\u22121/2\u2206\u03b2, that\nN(R(\u03bb) \u2212\u03c32) \u2243\u00b52|\u03b3|2 \u22122\u00b5\u03beT \u03b3 + \u03beT \u03be.\nWith this approximation, the optimal \u00b5 should be\n\u00b5 = \u03beT \u03b3\n|\u03b3|2 .\nOf course, this \u00b5 cannot be computed from data, but we can see that, since \u03be con-\nverges to a centered Gaussian random variable, its value cannot be too large. It is\ntherefore natural to choose \u00b5 to be constant and use ridge regression in the form\nN\nX\nk=1\n(yk \u2212\u02dcxT\nk \u03b2)2 +\n\u221a\nN\u00b5\u03b2T \u2206\u03b2.\nIn all cases, the mere fact that we find that the optimal \u00b5 is not 0 shows that, under\nthe simplifying (and optimistic) assumptions that we made for this computation,\nallowing for a penalty term always reduces the prediction error. In other terms,\nintroducing some estimation bias in order to reduce the variance is beneficial.\nKernel Ridge Regression\nWe now return to the feature-space situation and take h :\nRX \u2192H with associated kernel K. We still take q = 1 for simplicity. One formulates\nthe ridge regression problem in this context as the minimization of\nN\nX\nk=1\n(yl \u2212a0 \u2212\u27e8b , h(xl)\u27e9H)2 + \u03bb\u2225b\u22252\nH\nwith respect to \u03b2 = (a0,b). Introducing the space V generated by the feature function\nevaluated on the training set, we know from proposition 7.4 that replacing b by\n\u03c0V (b) leaves the residual sum of squares invariant. Moreover, one has \u2225\u03c0V (b)\u22252\nH \u2264\n\u2225b\u22252\nH with equality if and only if b \u2208V . This shows that the solution b must belong\nto V and therefore take the form (7.3).\nUsing this expression, one finds that the problem is reduced to finding the mini-\nmum of\nN\nX\nk=1\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8edyk \u2212a0 \u2212\nN\nX\nl=1\nK(xl,xk)\u03b1l\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\n2\n+ \u03bb\nN\nX\nk,l=1\n\u03b1k\u03b1lK(xk,xl)\nwith respect to a0,\u03b11,...,\u03b1N. Recall that we have denoted by K = K(x1,...,xN) the\nkernel matrix with entries K(xi,xj), i,j = 1,...,N. We will assume in the following\nthat K is invertible.\n7.2. RIDGE REGRESSION AND LASSO\n147\nIntroduce the vector 1N \u2208RN with all coordinates equal to one. Let\n\u02dcK =\n\u0010\n1N\nK\n\u0011\nand K\u2032 =\n \n0\n0\n0\nK\n!\n.\nLet \u03b1 \u2208RN be the vector with coefficients \u03b11,...,\u03b1N and \u02dc\u03b1 =\n \na0\n\u03b1\n!\n. With this\nnotation, the function to minimize is\nF(\u03b1) = |Y \u2212\u02dcK \u02dc\u03b1|2 + \u03bb \u02dc\u03b1T K\u2032 \u02dc\u03b1.\nThis takes the same form as standard ridge regression, replacing \u03b2 by \u02dc\u03b1, X by \u02dcK and\n\u2206by K\u2032. The solution therefore is\n\u02dc\u03b1\u03bb = ( \u02dcKT \u02dcK + \u03bbK\u2032)\u22121 \u02dcKT Y.\nNote that K being invertible implies that \u02dcKT \u02dcK + \u03bbK\u2032 is invertible. 1\nTo write the equivalent of (7.5), we need to use the equivalent of the matrix Xc,\nthat is, the matrix K with the average of the jth column subtracted to each (i,j) entry,\ngiven by:\nKc = K \u22121\nN 1N1T\nNK.\nIntroduce the matrix P = Id \u22121N1T\nN/N. It is easily checked that P2 = P (P is a pro-\njection matrix). Since Kc = PK, we have KT\nc Kc = KPK. One deduces from this the\nexpression of the optimal vector \u03b1\u03bb, namely,\n\u03b1\u03bb = (KPK + \u03bbK)\u22121KPYc = (PK + \u03bbIdRN)\u22121Yc\nwhere we have, in addition, used the fact that PYc = Yc. Finally, the intercept is\ngiven by\na0 = y \u22121\nN (\u03b1\u03bb)T K1N.\n7.2.2\nEquivalence of constrained and penalized formulations\nCase of ridge regression. Returning to the basic case (without feature space), we\nnow introduce an alternate formulation of ridge regression.\nLet ridge(\u03bb) denote\nthe ridge regression problem that we have considered so far, for some parameter\n1Indeed, let u =\n \nw0\nw\n!\nwith w0 \u2208R and w \u2208RN be such that uT ( \u02dcKT \u02dcK + \u03bbK\u2032)u = 0. This requires\n\u02dcKu = 0 and uT K\u2032u = 0. The latter quantity is wT Kw, which shows that w = 0 since K has rank N.\nThen \u02dcK = 1Nw0 so that w0 = 0 also.\n148\nCHAPTER 7. LINEAR REGRESSION\n\u03bb. Consider now the following problem, which will be called ridge\u2032(C): minimize\nPN\nk=1 |yk \u2212\u02dcxT\nk \u03b2|2 subject to the constraint \u03b2T \u2206\u03b2 \u2264C. We claim that this problem is\nequivalent to the ridge regression problem, in the following sense: for any C, there\nexists a \u03bb such that the solution of ridge\u2032(C) coincides with the solution of ridge(\u03bb)\nand vice-versa.\nIndeed, fix a C > 0. Consider an optimal \u03b2 for ridge\u2032(C). Assuming as above that \u2206\nis symmetric positive semi-definite, we let V be its null space and PV the orthogonal\nprojection on V . Write \u03b2 = \u03b21 + \u03b22 with \u03b21 = PV \u03b2. Let d1 and d2 be the respective\ndimensions of V and V \u22a5so that d1 + d2 = d. Identifying Rd with the product space\nV \u00d7V \u22a5(i.e., making a linear change of coordinates), the problem can be rewritten as\nthe minimization of\n|Y \u2212X1\u03b21 \u2212X2\u03b22|2\nsubject to \u03b2T\n2 \u2206\u03b22 \u2264C, where X1 (resp. X2) is N \u00d7 d1 (resp. N \u00d7 d2).\nThe gradient of the constraint \u03b3(\u03b22) = \u03b2T\n2 \u2206\u03b22 \u2212C is \u2207\u03b3(\u03b22) = 2\u2206\u03b22. Assume first\nthat \u2206\u03b22 , 0. Then the solution must satisfy the KKT conditions, which require that\nthere exists \u00b5 \u22650 such that \u03b2 is a stationary point of the Lagrangian\n|Y \u2212X1\u03b21 \u2212X2\u03b22|2 + \u00b5\u03b2T\n2 \u2206\u03b22,\nwith \u00b5 > 0 only possible if \u03b2T \u2206\u03b2 = C. This requires that\nX T\n1 X1\u03b21 + X T\n1 X2\u03b22 = X T Y,\nX T\n2 X1\u03b21 + X T\n2 X2\u03b22 + \u00b5\u2206\u03b22 = X T Y.\nSince \u2206\u03b21 = 0, and using X = (X1,X2), we have\n\u03b2 = (X T X + \u00b5\u2206)\u22121X T Y,\nwhich is the only solution of ridge(\u00b5).\nIf \u2206\u03b22 = 0, then, necessarily, \u03b22 = 0. Since C > 0, \u03b2 must then be the solution of\nthe unconstrained problem, which is ridge(0).\nConversely, any solution \u03b2 of ridge(\u03bb) satisfies the first-order optimality condi-\ntions for ridge\u2032(C) for C = \u03b2T \u2206\u03b2 (or any C \u2265\u03b2T \u2206\u03b2 if \u03bb = 0). This shows the equiva-\nlence of the two problems.\nGeneral case.\nWe now consider this equivalence in a more general setting. Con-\nsider a penalized optimization problem, denoted var(\u03bb) which consists in minimiz-\ning in \u03b2 some objective function of the form U(\u03b2) + \u03bb\u03d5(\u03b2),\u03bb \u22650. Consider also\nthe family of problems var\u2032(C), with C > inf(\u03d5), which minimize U(\u03b2) subject to\n\u03d5(\u03b2) \u2264C.\n7.2. RIDGE REGRESSION AND LASSO\n149\nWe make the following assumptions.\n(i) U and \u03d5 are continuous functions from Rn to R.\n(ii) \u03d5(\u03b2) \u2192\u221ewhen \u03b2 \u2192\u221e.\n(iii) For any \u03bb \u22650, there is a unique solution of var(\u03bb), denoted \u03b2\u03bb.\n(iv) For any C, there is a unique solution of var\u2032(C). denoted \u03b2\u2032\nC.\nAssumptions (ii) and (iv) are true, in particular, when U is strictly convex, \u03d5 is\nconvex and U has compact level sets. We show that, with these assumptions, the\ntwo families of problems are equivalent.\nWe first discuss the penalized problems and prove the following proposition,\nwhich has its own interest.\nProposition 7.5 The function \u03bb 7\u2192U(\u03b2\u03bb) is nondecreasing, and \u03bb 7\u2192\u03d5(\u03b2\u03bb) is non-\nincreasing, with\nlim\n\u03bb\u2192\u221e\u03d5(\u03b2\u03bb) = inf(\u03d5).\nMoreover, \u03b2\u03bb varies continuously as a function of \u03bb.\nProof Consider two parameters \u03bb and \u03bb\u2032. We have\nU(\u03b2\u03bb) + \u03bb\u03d5(\u03b2\u03bb)\n\u2264\nU(\u03b2\u03bb\u2032) + \u03bb\u03d5(\u03b2\u03bb\u2032)\nand U(\u03b2\u03bb\u2032) + \u03bb\u2032\u03d5(\u03b2\u03bb\u2032)\n\u2264\nU(\u03b2\u03bb) + \u03bb\u2032\u03d5(\u03b2\u03bb)\nsince both left-hand sides are minimizers. This implies\n\u03bb(\u03d5(\u03b2\u03bb) \u2212\u03d5(\u03b2\u03bb\u2032)) \u2264U(\u03b2\u03bb\u2032) \u2212U(\u03b2\u03bb) \u2264\u03bb\u2032(\u03d5(\u03b2\u03bb) \u2212\u03d5(\u03b2\u03bb\u2032)).\n(7.6)\nIn particular: (\u03bb\u2032 \u2212\u03bb)(\u03d5(\u03b2\u03bb) \u2212\u03d5(\u03b2\u03bb\u2032)) \u22650. Assume that \u03bb < \u03bb\u2032. Then this last\ninequality implies \u03d5(\u03b2\u03bb) \u2265\u03d5(\u03b2\u03bb\u2032) and (7.6) then implies that U(\u03b2\u03bb) \u2264U(\u03b2\u03bb\u2032), which\nproves the first part of the proposition.\nNow assume that there exists \u03f5 > 0 such that \u03d5(\u03b2\u03bb) > inf\u03d5 + \u03f5 for all \u03bb \u22650. Take\n\u02dc\u03b2 such that \u03d5( \u02dc\u03b2) \u2264inf\u03d5 + \u03f5/2. For any \u03bb > 0, we have\nU(\u03b2\u03bb) + \u03bb\u03d5(\u03b2\u03bb) \u2264U( \u02dc\u03b2) + \u03bb\u03d5( \u02dc\u03b2)\nso that U(\u03b2\u03bb) < U( \u02dc\u03b2) \u2212\u03bb\u03f5/2. Since U(\u03b2\u03bb) \u2265U(a0), we get U(a0) = \u2212\u221e, which is a\ncontradiction. This shows that \u03d5(\u03b2\u03bb) tends to inf(\u03d5) when \u03bb tends to infinity.\nWe now prove that \u03bb 7\u2192\u03b2\u03bb is continuous. Define G(\u03b2,\u03bb) = U(\u03b2)+\u03bb\u03d5(\u03b2). Since we\nassume that \u03d5(\u03b2) \u2192\u221ewhen \u03b2 \u2192\u221e, and we have just proved that \u03d5(\u03b2\u03bb) \u2264\u03d5(a0) for\nany \u03bb, we obtain the fact that the set (|\u03b2\u03bb|,\u03bb \u22650) is bounded, say by a constant B \u22650.\n150\nCHAPTER 7. LINEAR REGRESSION\nConsider a sequence \u03bbn that converges to \u03bb. We want to prove that \u03b2\u03bbn \u2192\u03b2\u03bb, for\nwhich (because \u03b2\u03bb is bounded) it suffices to show that if any subsequence of (\u03b2\u03bbn)\nconverges to some \u02dc\u03b2, then \u02dc\u03b2 = \u03b2\u03bb.\nSo, consider such a converging subsequence, that we will still denote by \u03b2\u03bbn for\nconvenience. Since G is continuous, one has G(\u03b2\u03bbn,\u03bbn) \u2192G( \u02dc\u03b2,\u03bb) when n tends to\ninfinity. Let us prove that G(\u03b2\u03bb,\u03bb) is continuous in \u03bb. For any pair \u03bb,\u03bb\u2032 and any \u03b2,\nwe have\nG(\u03b2\u03bb\u2032,\u03bb\u2032) \u2264G(\u03b2\u03bb,\u03bb\u2032) = G(\u03b2\u03bb,\u03bb) + (\u03bb\u2032 \u2212\u03bb)\u03d5(\u03b2\u03bb) \u2264G(\u03b2\u03bb,\u03bb) + |\u03bb\u2032 \u2212\u03bb|\u03d5(a0).\nThis yields, by symmetry, |G(\u03b2\u03bb\u2032,\u03bb\u2032)\u2212G(\u03b2\u03bb,\u03bb)| \u2264\u03d5(a0)|\u03bb\u2212\u03bb\u2032|, proving the continuity\nin \u03bb.\nSo we must have G( \u02dc\u03b2,\u03bb) = G(\u03b2\u03bb,\u03bb). This implies that both \u02dc\u03b2 and \u03b2\u03bb are solutions\nof var(\u03bb), so that \u03b2\u03bb = \u02dc\u03b2 because we assume that the solution is unique.\n\u25a0\nWe now prove that the classes of problems var(\u03bb) and var\u2032(C) are equivalent.\nFirst, \u03b2\u03bb is a minimizer of U(\u03b2) subject to the constraint \u03d5(\u03b2) \u2264C, with C = \u03d5(\u03b2\u03bb).\nIndeed, if U(\u03b2) < U(\u03b2\u03bb) for some \u03b2 with \u03d5(\u03b2) \u2264\u03d5(\u03b2\u03bb), then U(\u03b2) + \u03bb\u03d5(\u03b2) < U(\u03b2\u03bb) +\n\u03bb\u03d5(\u03b2\u03bb) which is a contradiction. So \u03b2\u03bb = \u03b2\u2032\n\u03d5(\u03b2\u03bb). Using the continuity of \u03b2\u03bb and \u03d5,\nthis proves the equivalence of the problems when C is in the interval (a,\u03d5(a0)) where\na = lim\u03bb\u2192\u221e\u03d5(\u03b2\u03bb) = inf(\u03d5).\nSo, it remains to consider the case C > \u03d5(a0). For such a C, the solution of var\u2032(C)\nmust be a0 since it is a solution of the unconstrained problem, and satisfies the con-\nstraint.\n7.2.3\nLasso regression\nProblem statement\nAssume that the output variable is scalar, i.e., q = 1. Let \u02c6\u03c32(i)\nbe the empirical variance of the ith variable X(i). Then, the lasso estimator is defined\nas a minimizer of PN\nk=1(yk \u2212\u02dcxT\nk \u03b2)2 subject to the constraint Pd\ni=1 \u02c6\u03c3(i)|\u03b2(i)| \u2264C. Com-\npared to ridge regression, the sum of squares for \u03b2 is simply replaced by a weighted\nsum of absolute values, but we will see that this change may significantly affect the\nnature of the solutions.\nAs we have just seen, the penalized formulation, minimizing\nN\nX\nk=1\n(yk \u2212\u02dcxT\nk \u03b2)2 + \u03bb\nd\nX\ni=1\n\u02c6\u03c3(i)|\u03b2(i)|\nprovides an equivalent family of problems, on which we will focus (because it is\neasier to analyze). Since one uses a non-Euclidean norm in the penalty, there is no\n7.2. RIDGE REGRESSION AND LASSO\n151\nkernel version of the lasso and we only discuss the method in the original input\nspace R = Rd.\nFor a vector a \u2208Rk, we let |a|1 = |a(1)| + \u00b7\u00b7\u00b7 + |a(k)|, the \u21131 norm of a. Using the\nprevious notation for Y and X , the quantity to minimize can be rewritten as\n|Y \u2212X \u03b2|2 + \u03bb|D\u03b2|1\nwhere D is the d \u00d7 (d + 1) matrix with d(i,i + 1) = \u02c6\u03c3(i) for i = 1,...,d and all other\ncoefficients equal to 0. This is a convex optimization problem which, unlike ridge\nregression, does not have a closed form solution.\nADMM.\nThe alternating direction method of multipliers (ADMM) that was de-\nscribed in section 3.6, (3.59) is one of the state-of-the-art algorithm to solve the lasso\nproblem, especially in large dimensions. Other iterative methods include subgradi-\nent descent (see the example in section 3.5.4) and proximal gradient descent. Since\nx has a different meaning here, we change the notation in (3.59) by replacing x,z,u\nby \u03b2,\u03b3,\u03c4, and rewrite the lasso problem as the minimization of\n|Y \u2212X \u03b2|2 + \u03bb|\u03b3|1\nsubject to D\u03b2 \u2212\u03b3 = 0. Applying (3.59) with A = D, B = \u2212Id and c = 0, the ADMM\niterations are\n\uf8f1\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f2\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f3\n\u03b2(n + 1)\n= argmin\u03b2\n \n|Y \u2212X \u03b2|2 + 1\n2\u03c1|D\u03b2 \u2212\u03b3(n) + \u03c4(n)|2\n!\n\u03b3(i)(n + 1)\n= argmint\n \n\u03bb|t| + 1\n2\u03c1(t \u2212D\u03b2(i)(n + 1) \u2212\u03c4(i)(n))2\n!\n, i = 1,...,d\n\u03c4(n + 1)\n= \u03c4(n) + D\u03b2(n + 1) \u2212\u03b3(n + 1)\nThe solutions of both minimization problems are explicit, yielding the following\nalgorithm, which converges to a solution if \u03c1 is small enough.\nAlgorithm 7.1 (ADMM for lasso)\nLet \u03c1 > 0 be chosen. Starting with initial values \u03b2(0), \u03b3(0) and \u03c4(0), the ADMM algo-\nrithm for lasso iterates:\n\uf8f1\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f2\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f3\n\u03b2(n + 1)\n=\n \nX T X + DT D\n2\u03c1\n!\u22121  \nX T Y + DT\n2\u03c1 (\u03b3(n) \u2212\u03c4(n))\n!\n\u03b3(i)(n + 1)\n= S\u03bb\u03c1\n\u0010\nD\u03b2(i)(n + 1) + \u03c4(i)(n)\n\u0011\n, i = 1,...,d\n\u03c4(n + 1)\n= \u03c4(n) + D\u03b2(n + 1) \u2212\u03b3(n + 1)\n152\nCHAPTER 7. LINEAR REGRESSION\nuntil the difference between the variables at steps n and n + 1 is below a small toler-\nance level. Here, S\u03bb\u03c1 is the so-called shrinkage operator\nS\u03bb\u03c1(v) =\n\uf8f1\uf8f4\uf8f4\uf8f4\uf8f4\uf8f2\uf8f4\uf8f4\uf8f4\uf8f4\uf8f3\nv \u2212\u03bb\u03c1\nif v \u2265\u03bb\u03c1\n0\nif |v| \u2264\u03bb\u03c1\nv + \u03bb\u03c1\nif v \u2264\u2212\u03bb\u03c1\nNote that the ADMM algorithm makes an iterative approximation of the constraints,\nso that they are only satisfied at some precision level when the algorithm is stopped.\nExact computation.\nWe now provide a more detailed characterization of the solu-\ntion of the lasso problem and analyze, in particular, how this solution changes when\n\u03bb (or C) varies. To simplify the exposition, and without loss of generality, we will as-\nsume that the variables have been normalized so that \u02c6\u03c3(i) = 1 and the penalty simply\nis the sum of absolute values. Let\nG\u03bb(\u03b2) =\nN\nX\nk=1\n(yk \u2212a0 \u2212xT\nk b)2 + \u03bb\nd\nX\ni=1\n|b(i)|.\nThe following proposition, in which we let\nrb = 1\nN\nN\nX\nk=1\n(yk \u2212a0 \u2212xT\nk b)xk,\ncharacterizes the solution of the lasso.\nProposition 7.6 The pair (a0,b) is the optimal solution of the lasso problem with param-\neter \u03bb if and only if a0 = \u00afy \u2212\u00afxT b and, for all i = 1,...,d,\n|r(i)\nb | \u2264\u03bb\n2N\n(7.7)\nwith\nr(i)\nb = sign(b(i)) \u03bb\n2N if b(i) , 0.\n(7.8)\nIn particular |r(i)\nb | < \u03bb/(2N) implies b(i) = 0.\nProof Using the subdifferential calculus in theorem 3.45, one can compute the sub-\ngradients of G by adding the subdifferentials of the terms that compose it. All these\nterms are differentiable except |b(i)| when b(i) = 0, and the subdifferential of t 7\u2192|t| at\nt = 0 is the interval [\u22121,1].\n7.2. RIDGE REGRESSION AND LASSO\n153\nThis shows that g \u2208\u2202G\u03bb(\u03b2) if and only if\ng = \u22122Nrb + \u03bbz\nwith z(i) = sign(b(i)) if b(i) , 0 and |z(i)| \u22641 otherwise. Proposition 7.6 immediately\nfollows by taking g = 0.\n\u25a0\nLet \u03b6 = sign(b), the vector formed by the signs of the coordinates of b, with\nsign(0) = 0. Then proposition 7.6 uniquely specifies a0 and b once \u03bb and \u03b6 are\nknown. Indeed, let J = J\u03b6 denote the ordered subset of indices j \u2208{1,...,d} such\nthat \u03b6(j) , 0, and let b(J), xk(J), \u03b6(J), etc., denote the restrictions of vectors to these\nindices. Equation (7.8) can be rewritten as (after replacing a0 by its optimal value)\nXc(J)T Xc(J)b(J) = Xc(J)T Yc \u2212\u03bb\n2\u03b6(J)\nwhere\nXc(J) =\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\n(x1(J) \u2212x(J))T\n...\n(xN(J) \u2212x(J))T\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\n.\nThis yields\nb(J) = (Xc(J)T Xc(J))\u22121\u0010\nXc(J)T Yc \u2212\u03bb\n2\u03b6(J)\n\u0011\n,\n(7.9)\nwhich fully determine b since b(j) = 0 if j < J, by definition.\nFor given \u03bb, only one sign configuration \u03b6 will provide the correct solution, with\ncorrect signs for nonzero values of b above, and correct inequalities on rb. Call-\ning this configuration \u03b6\u03bb, one can note that if \u03b6\u03bb is known for a given value of \u03bb,\nit remains valid if we increase or decrease \u03bb until one of the optimality conditions\nchanges, i.e., either one of the coordinates b(i),i \u2208J\u03b6\u03bb, vanishes, or one of the inequal-\nities for i < J\u03b6\u03bb becomes an equality. Moreover, proposition 7.6 shows that between\nthese events both b and therefore rb depend linearly on \u03bb, which makes easy the\ntask of determining maximal intervals around a given \u03bb over which \u03b6 remains un-\nchanged.\nNote that solutions are known for \u03bb = 0 (standard least squares) and for \u03bb large\nenough (for which b = 0). Indeed, for b = 0 to be a solution, it suffices that\n\u03bb > \u03bb0\n\u2206= 2max\ni\n\f\f\f\f\f\f\f\nN\nX\nk=1\n(yk \u2212y)(x(i)\nk \u2212x(i))\n\f\f\f\f\f\f\f\n.\nThese remarks set the stage for an algorithm computing the optimal solution of\nthe lasso problem for all values of \u03bb, starting either from \u03bb = 0 or \u03bb > \u03bb0. We will de-\nscribe this algorithm starting for \u03bb > \u03bb0, which has the merit to avoid complications\n154\nCHAPTER 7. LINEAR REGRESSION\ndue to underconstrained least squares when d is large. For this purpose, we need a\nlittle more notation. For a given \u03b6, let\nb\u03b6 = (Xc(J\u03b6)T Xc(J\u03b6))\u22121Xc(J\u03b6)T Yc\nand\nu\u03b6 = 1\n2(Xc(J\u03b6)T Xc(J\u03b6))\u22121\u03b6(J\u03b6),\nso that b(J\u03b6) = b\u03b6 \u2212\u03bbu\u03b6. The residuals then take the form\nr(i)\nb = 1\nN\nN\nX\nk=1\n(yk \u2212a0 \u2212bT\n\u03b6 xk)x(i)\nk + \u03bb\nN\nN\nX\nk=1\n(xT\nk u\u03b6)(x(i)\nk \u2212x(i))\n= \u03c1(i)\n\u03b6 + \u03bbd(i)\n\u03b6 ,\nwhere the last equation defines \u03c1\u03b6 and d\u03b6.\nAssume that one wants to minimize G\u03bb\u2217for some \u03bb\u2217> 0. We need to describe\nthe sequence of changes to the minimizers of G\u03bb when \u03bb decreases from some value\nlarger than \u03bb0 to the value \u03bb\u2217.\nIf \u03bb\u2217\u2265\u03bb0, then the optimal solution is b = 0, so we can assume that \u03bb\u2217< \u03bb0.\nWhen \u03bb is slightly smaller than \u03bb0, one needs to introduce some non-zero values in\n\u03b6. Those values are at the indexes i such that\n\u03bb0 = 2\n\f\f\f\f\f\f\nN\nX\nk=1\n(yk \u2212y)(x(i)\nk \u2212x(i))\n\f\f\f\f\f\f\nThe sign of \u03b6(i) is also determined since sign(b(i)) = sign(r(i)\nb ) when b(i) , 0.\nThe algorithm will then continue by progressively adding non-zero entries to \u03b6\nwhen the covariance between some unused variables and the residual becomes too\nlarge, or by removing non-zero values when the optimal b crosses a zero. We now\ndescribe it in detail.\nAlgorithm 7.2 (Exact minimization for lasso)\n1. Initialization: let \u03bb(0) = 1 + \u03bb0, \u03c3(0) = 0 and the corresponding values a0(0) = y\nand b(0) = 0.\n2. Assume that the algorithm has reached step n with current variables \u03bb(n), \u03c3(n),\na0(n) and b(n).\n3. Determine the first \u03bb\u2032 < \u03bb(n) for which either\n(i) For some i, \u03b6(i)(n) , 0 and b(i)\n\u03b6(n) \u2212\u03bb\u2032u(i)\n\u03b6(n) = 0.\n7.3. OTHER SPARSITY ESTIMATORS\n155\n(ii) For some i, \u03b6(i)(n) = 0 and (1 \u22122Nd(i)\n\u03b6(n))\u03bb\u2032 \u22122N\u03c1\u03b6(n) = 0.\n(iii) For some i, \u03b6(i)(n) = 0 and (1 + 2Nd(i)\n\u03b6(n))\u03bb\u2032 + 2N\u03c1\u03b6(n) = 0.\n4. Then, there are two cases:\n(a) If \u03bb\u2032 \u2265\u03bb\u2217, set \u03bb(n +1) = \u03bb\u2032. Let \u03b6(i)(n +1) = \u03b6(i)(n) if i does not satisfy (i), (ii) or\n(iii). If i is in case (i), set \u03b6(i)(n+1) = 0. For i in case (ii) (resp. (iii)), set \u03b6(i)(n+1) = 1\n(resp. \u22121).\n(b) If \u03bb\u2032 < \u03bb\u2217, terminate the algorithm without updating \u03b6 and set\nb(i) = b(i)\n\u03b6(n) \u2212\u03bb\u2217u(i)\n\u03b6(n),\n\u03b6(i)(n) , 0\nand a0 = \u00afy \u2212bT \u00afx to obtain the final solution.\n7.3\nOther Sparsity Estimators\n7.3.1\nLARS estimator\nAlgorithm.\nThe LARS algorithm can be seen as a simplification of the previous\nlasso algorithm in which one always adds active variables at each step. We assume\nas above that input variables are normalized such that \u02c6\u03c3(i) = 1.\nGiven a current set J of selected variables, the algorithm will decide either to stop\nor to add a new variable to J according to a criterion that depends on a parameter\n\u03bb > 0. Let b(J) \u2208R|J| be the least-square estimator based on variables in J\nb(J) = (Xc(J)T Xc(J))\u22121Xc(J)T Yc.\nLet bJ \u2208Rd such that b(i)\nJ\n= b(i)\n(J) for i \u2208J and 0 otherwise. The covariances between\nthe remaining variables and the residuals are given by\nr(i)\nJ\n= 1\nN\nN\nX\nk=1\n(yk \u2212y \u2212(xk \u2212x)T bJ)(x(i)\nk \u2212x(i)),\ni < J.\nIf, for all i \u2208J, |r(i)\nJ | \u2264\n\u221a\n\u03bb/N, the procedure is stopped. Otherwise, one adds to J the\nvariable i such that |r(i)\nJ | is largest and continues.\n156\nCHAPTER 7. LINEAR REGRESSION\nJustification.\nRecall the notation |b|0 for the number of non-zero entries of b. Con-\nsider the objective function\nL(b) = |Yc \u2212Xcb|2 + \u03bb|b|0.\nLet J be the set currently selected by the algorithm, and bJ defined as above. We\nconsider the problem of adding one non-zero entry to b. Fix i < J, and let \u02dcb \u2208Rd\nhave all coordinates equalt to those of bJ for all except the ith one, which is therefore\nallowed to be non-zero. Then\nL(\u02dcb) =\nN\nX\nk=1\n\u0012\nyk \u2212y \u2212\nX\nj\u2208J\n(x(j)\nk \u2212x)b(j) \u2212(x(i)\nk \u2212x)\u02dcb(i)\u00132\n+ \u03bb|J| + \u03bb,\nso that (using \u02c6\u03c3(i) = 1)\nL(\u02dcb) = L(bJ) \u22122Nr(i)\nJ \u02dcb(i) + N(\u02dcb(i))2 + \u03bb\nNow, L(\u02dcb) is an upper-bound for L(bJ\u222a{i}), and so is its minimum with respect to \u02dcb(i).\nThis yields:\nL(bJ\u222a{i}) \u2264L(bJ) \u2212N(r(i)\nJ )2 + \u03bb\nThe LARS algorithm therefore finds the value of i that minimizes this upper-bound,\nprovided that the resulting minimum is less that L(bJ).\nVariant.\nThe same argument can be made with |b|0 replaced by |b|1 and one gets\nL(\u02dcb) = L(bJ) \u22122Nr(i)\nJ \u02dcb(i) + N(\u02dcb(i))2 + \u03bb|\u02dcb(i)|\nMinimizing this expression with respect to \u02dcb(i) yields the upper bound:\nL(bJ\u222a{i}) \u2264\n\uf8f1\uf8f4\uf8f4\uf8f4\uf8f4\uf8f2\uf8f4\uf8f4\uf8f4\uf8f4\uf8f3\nL(bJ) \u2212N\n\u0010\n|r(i)\nJ | \u2212\u03bb\n2N\n\u00112\nif |r(i)\nJ | \u2265\u03bb\n2N\nL(bJ)\nif |r(i)\nJ | \u2264\u03bb\n2N\nThis leads to the following alternate form of LARS. Given a current set J of se-\nlected variables, compute\nr(i)\nJ\n= 1\nN\nN\nX\nk=1\n(yk \u2212y \u2212(xk \u2212x)T bJ)(x(i)\nk \u2212x(i)),\ni < J .\nIf, for all i < J, |r(i)\nJ | \u2264\u03bb/2N, stop the procedure. Otherwise, add to J the variable i\nsuch that |r(i)\nJ | is largest and continue. This form tends to add more variables since\nthe stopping criterion decreases in 1/N instead of 1/\n\u221a\nN.\n7.3. OTHER SPARSITY ESTIMATORS\n157\nWhy \u201cleast angle\u201d?\nLet \u00b5J,k = yk\u2212y\u2212(xk\u2212x)T bJ denote the residual after regression.\nThe empirical correlation between \u00b5 and x(i) is equal to the cosine of the angle, say\n\u03b8(i)\nJ\nbetween \u00b5J \u2208RN and x(i)\u2212x both considered as vectors in RN. This cosine is also\nequal to\ncos\u03b8(i)\nJ\n=\n\u00b5T\nJ (x(i) \u2212x(i))\n|x(i) \u2212x(i)||\u00b5J|\n=\n\u221a\nN\nr(i)\nJ\n|\u00b5J|\nwhere we have used the fact that |x(i) \u2212x(i)|/\n\u221a\nN = \u02c6\u03c3(i) = 1. Since |\u00b5J| does not depend\non i, looking for the largest value of |r(i)\nJ | is equivalent to looking for the smallest\nvalue of |\u03b8(i)\nJ |, so that we are looking for the unselected variable for which the angle\nwith the current residual is minimal.\n7.3.2\nThe Dantzig selector\nNoise-free case.\nAssume that one wants to solve the equation X \u03b2 = Y when the\ndimension, N, of Y is small compared to number of columns, d, in X . Since the\nsystem is under-determined, one needs additional constraints on \u03b2 and a natural one\nis to look for sparse solutions, i.e., find solutions with a maximum number of zero\ncoefficients. However, this is numerically challenging, and it is easier to minimize\nthe \u21131 norm of \u03b2 instead (as seen when discussing the lasso, using this norm often\nprovides sparse solutions). In the following, we assume that the empirical variance\nof each variable is normalized, so that, denoting X (i) the ith column of X , we have\n|X (i)| = 1.\nThe Dantzig selector [47] minimizes\nd\nX\ni=1\n|\u03b2(i)|\nsubject to the constraint X \u03b2 = Y. This results in a linear program (therefore easy to\nimplement). More precisely, introducing slack variables, it is indeed equivalent to\nminimize\nd\nX\ni=1\n\u03be(i) +\nd\nX\ni=1\n\u03be\u2217(i)\nsubject to constraints \u03be(i) \u2265\u03b2(i), \u03be(i)\u2217\u2265\u2212\u03b2(i), \u03be(i) \u22650, \u03be\u2217(i) \u22650 and X \u03b2 = Y.\nSparsity recovery\nUnder some assumptions, this method does recover sparse solu-\ntions when they exist. More precisely, let \u02c6\u03b2 be the solution of the linear programming\nproblem above. Assume that there is a set J\u2217\u2282{1,...,d} such that X \u03b2 = Y for some\n158\nCHAPTER 7. LINEAR REGRESSION\n\u03b2 \u2208Rd with \u03b2(i) = 0 if i < J\u2217. Conditions under which \u02c6\u03b2 is equal to \u03b2 are provided\nin Candes and Tao [47] and involve the correlations between pairs of columns of X ,\nand the size of J.\nThat the size of J\u2217must be a factor is clear, since, for the statement to make sense,\nthere cannot exist two \u03b2\u2019s satisfying X \u03b2 = Y and \u03b2(i) = 0 for i < J\u2217. Uniqueness is\nobviously not true if |J| > N, because, even if one knew J, the condition would be\nunder-constrained for \u03b2. Since the set J\u2217is not known, and we also want to avoid\nany other solution associated to a set of same size. So, there cannot exist \u03b2 and \u02dc\u03b2\nrespectively vanishing outside of J\u2217and \u02dcJ\u2217, where J\u2217and \u02dcJ\u2217have same cardinality,\nsuch that X \u03b2 = Y = X \u02dc\u03b2. The equation X (\u03b2 \u2212\u02dc\u03b2) = 0 would be under-constrained as\nsoon as the number of non-zero coefficients of \u03b2 \u2212\u02dc\u03b2 is larger than N, and since this\nnumber can be as large as |J\u2217| + |\u02dcJ\u2217| = 2|J\u2217|, we see that one should impose at least\n|J\u2217| \u2264N/2.\nGiven this restriction, another obvious remark is that, if the set J on which \u03b2 does\nnot vanish is known, with |J| small enough, then X \u03b2 = Y is over-constrained and any\nsolution is (typically) unique. So the issue really is whether the set J\u03b2 listing the\nnon-zero indexes of a solution \u03b2 is equal to y J\u2217.\nAs often, precious insight on the solution of this minimization problem is ob-\ntained by considering the dual problem. Introducing Lagrange multipliers \u03bb(i) \u2265\n0,i = 1,...,d for the constraints \u03be(i) \u2212\u03b2(i) \u22650, \u03bb\u2217(i) \u22650, i = 1,...,d for \u03be\u2217(i) + \u03b2(i) \u22650,\n\u03b3(i),\u03b3\u2217(i) \u22650 for \u03be(i) \u22650 and \u03be\u2217(i) \u22650, and \u03b1 \u2208RN for X \u03b2 = Y, the Lagrangian is\nL(\u03b2,\u03be,\u03bb,\u03bb\u2217,\u03b1) = (1d \u2212\u03bb \u2212\u03b3)T \u03be + (1d \u2212\u03bb\u2217\u2212\u03b3\u2217)T \u03be\u2217+ (\u03bb \u2212\u03bb\u2217+ X T \u03b1)T \u03b2 \u2212\u03b1T Y.\nThe KKT conditions require \u03b3 = 1d \u2212\u03bb, \u03b3\u2217= 1d \u2212\u03bb\u2217, X \u03b1 = \u03bb\u2217\u2212\u03bb and the comple-\nmentary slackness conditions give (1 \u2212\u03bb(i))\u03be(i) = (1 \u2212\u03bb\u2217\ni)\u03be\u2217(i) = 0, \u03bb(i)(\u03b2(i) \u2212\u03be(i)) =\n\u03bb\u2217(i)(\u03b2(i) + \u03be\u2217(i)) = 0.\nThe dual problem requires to minimize \u03b1T Y subject to the constraints X T \u03b1 =\n\u03bb\u2217\u2212\u03bb and 0 \u2264\u03bb(i),\u03bb\u2217(i) \u22641. Assume that (\u03b1,\u03bb,\u03bb\u2217) is a solution of this dual problem.\nOne has the following cases.\n(1) If \u03bb(i) \u2208(0,1), then \u03be(i) = \u03b2(i) \u2212\u03be(i) = 0, which implies \u03be(i) = \u03b2(i) = 0, and, as a\nconsequence (1 \u2212\u03bb\u2217(i))\u03be\u2217(i) = \u03bb\u2217(i)\u03be\u2217(i) = 0, so that also \u03be\u2217(i) = 0 .\n(2) Similarly, \u03bb\u2217(i) \u2208(0,1) implies \u03be(i) = \u03be\u2217(i) = \u03b2(i) = 0.\n(3) If \u03bb(i) = \u03bb\u2217(i) = 1, then \u03b2(i) \u2212\u03be(i) = \u03b2(i) + \u03be(i) = 0 with \u03be(i),\u03be\u2217(i) \u22650, so that also\n\u03be(i) = \u03be\u2217(i) = \u03b2(i) = 0.\n(4) If \u03bb(i) = \u03bb\u2217(i) = 0, then \u03be(i) = \u03be\u2217(i) = 0 and since \u03b2(i) \u2264\u03be(i) and \u03b2(i) \u2264\u2212\u03be\u2217(i), we\nget \u03b2(i) = 0.\n7.3. OTHER SPARSITY ESTIMATORS\n159\n(5) The only remaining situation, in which \u03b2(i) can be non-zero, is when \u03bb(i) = 1 \u2212\n\u03bb\u2217(i) \u2208{0,1}, or, equivalently, when |\u03bb(i) \u2212\u03bb\u2217(i)| = 1.\nThis discussion allows one to reconstruct the set J\u03b2 associated with the primal prob-\nlem given the solution of the dual problem. Note that |\u03bb(i) \u2212\u03bb\u2217(i)| = |\u03b1T X (i)|, so that\nthe set of indexes with |\u03bb(i) \u2212\u03bb\u2217(i)| = 1 is also\nI\u03b1\n\u2206=\nn\ni : |\u03b1T X (i)| = 1\no\n.\nOne has\n\u03b1T Y = \u03b1T X \u03b2 =\nd\nX\ni=1\n\u03b2(i)\u03b1T X (i) \u2264\nX\ni\u2208J\u03b2\n|\u03b2(i)||\u03b1T X (i)| \u2264\nX\ni\u2208J\u03b2\n|\u03b2(i)|.\nThe upper-bound is achieved when \u03b1T X (i) = sign(\u03b2(i)) for i \u2208J\u03b2. So, if a vector \u03b1 can\nbe found such that\n(i) \u03b1T X (i) = sign(\u03b2(i)) for i \u2208J\u2217,\n(ii) |\u03b1T X (j)| < 1 for j < J\u2217,\nthen it is a solution of the dual problem with J\u03b1 = J\u2217.\nLet sJ = (s(j),j \u2208J) be defined by s(j) = sign(\u03b2(j)). One can always decompose\n\u03b1 \u2208RN in the form\n\u03b1 = XJ\u2217\u03c1 + w\nwhere \u03c1 \u2208R|J\u2217| and w \u2208RN is perpendicular to the columns of XJ\u2217. From X T\nJ\u2217\u03b1 = sJ,\nwe get\n\u03c1 = (X T\nJ\u2217XJ\u2217)\u22121sJ\u2217.\nLetting \u03b1J\u2217be the solution with w = 0, the question is therefore whether one can find\nw such that\n(\nwT X (j) = 0,\nj \u2208J\u2217\n|\u03b1T\nJ X (k) + wT X (k)| < 1,\nk < J\u2217\nDenote for short \u03a3JJ\u2032 = X T\nJ XJ\u2032. One can show that such a solution exists when\nthe matrices \u03a3JJ are close to the identity as soon as |J| is small enough [47]. More\nprecisely, denote, for q \u2264d\n\u03b4(q) = max\n|J|\u2264q max(\u2225\u03a3JJ\u2225,\u2225\u03a3\u22121\nJJ \u2225\u22121) \u22121,\n160\nCHAPTER 7. LINEAR REGRESSION\nin which one uses the operator norm on matrices, and\n\u03b8(q,q\u2032) = max\nn\nzT \u03a3TT \u2032z\u2032 : |J|,|J\u2032| \u2264q.J \u2229J\u2032 = \u2205,|z| = |z\u2032| = 1\no\n.\nThen, the following proposition is true.\nProposition 7.7 (Candes-Tao) Let q = |J\u2217| and s = (s(j),j \u2208J\u2217) \u2208Rq.\nAssume that\n\u03b4(2q) + \u03b8(q,2q) < 1. Then there exists \u03b1 \u2208RN such that \u03b1T X (j) = s(j) for j \u2208J\u2217and\n|\u03b1T X (j)| \u2264\n\u03b8(q,q)\n1 \u2212\u03b4(2q) \u2212\u03b8(q,2q) if j < J\u2217.\nSo \u03b1 has the desired property as soon as \u03b4(2q) + \u03b8(q,q) + \u03b8(q,2q) \u22641. to control\nsubsets of variables of size less than 3q to obtain the conclusion, which is important,\nof course, when q is small compared to d.\nNoisy case\nConsider now the noisy case. We here again introduce quantities that\nwere pivotal for the lasso and LARS estimators, namely, the covariances between the\nvariables and the residual error. So, we define, for a given \u03b2\nr(i)\n\u03b2 = X (i)T (Y \u2212X \u03b2)\nwhich depends linearly on \u03b2. Then, the Dantzig selector is defined by the linear\nprogram: Minimize:\nd\nX\nj=1\n|\u03b2(j)|\nsubject to the constraint:\nmax\nj=1,...,d |r(j)\n\u03b2 | \u2264C.\nThe explicit expression of this problem as a linear program is obtained as before by\nintroducing slack variables \u03be(j),\u03be\u2217(j),j = 1,...,d and minimizing\nd\nX\nj=1\n\u03be(j) +\nd\nX\nj=1\n\u03be\u2217(j)\nwith constraints \u03be(j),\u03be\u2217(j) \u22650, \u03be \u2265\u03b2, \u03be\u2217\u2265\u2212\u03b2, max\nj=1,...,d |r(j)\n\u03b2 | \u2264C.\nSimilar to the noise-free case, the Dantzig selector can identify sparse solutions\n(up to a small error) if the columns of X are nearly orthogonal, with the same type of\nconditions [48]. Interestingly enough, the accuracy of this algorithm can be proved\nto be comparable to that of the lasso in the presence of a sparse solution [30].\n7.4. SUPPORT VECTOR MACHINES FOR REGRESSION\n161\n7.4\nSupport Vector Machines for regression\n7.4.1\nLinear SVM\nProblem formulation\nWe start by discussing support vector machines (SVM) [196,\n197] with RX = Rd equipped with the standard inner product (generally referred to\nas linear SVM) and will extend the theory to kernel methods in the next section.\nSVMs solve a linear regression problem, but replace the least-squares loss function\nby (y,y\u2032) 7\u2192V (y \u2212y\u2032) with\nV (t) =\n(0\nif |t| < \u03f5\n|t| \u2212\u03f5\nif |t| \u2265\u03f5\n(7.10)\nFigure 7.1: The function V defining the SVM risk function.\nA plot of the function V is provided in fig. 7.1. This function is an example\nof what is often called a robust loss function. The quadratic error used in linear\nregression had the advantage of providing closed form expressions for the solution,\nbut is quite sensitive to outliers. For robustness, it is preferable to use loss functions\nthat, like V , increase at most linearly at infinity. One sometimes choose them as\nsmooth convex functions, for example V (t) = (1 \u2212cos\u03b3t)/(1 \u2212cos\u03b3\u03f5) for |t| < \u03f5 and\nf (t) = |t| for t \u2265\u03f5, where \u03b3 is chosen so that \u03b3\u03f5sin\u03b3\u03f5/(1 \u2212cos\u03b3\u03f5) = 1. In such a case,\nminimizing\nF(\u03b2) =\nN\nX\nk=1\nV (yk \u2212a0 \u2212xT\nk b)\ncan be done using gradient descent methods. Using V in (7.10) will require a little\nmore work, as we see now.\n162\nCHAPTER 7. LINEAR REGRESSION\nThe SVM regression problem is generally formulated as the minimization of\nN\nX\nk=1\nV (yk \u2212a0 \u2212xT\nk b) + \u03bb|b|2 ,\nand we will study a slightly more general problem, minimizing\nF(a0,b) =\nN\nX\nk=1\nV (yk \u2212a0 \u2212xT\nk b) + \u03bbbT \u2206b,\nwhere \u2206is a symmetric positive-definite matrix. This objective function exhibits the\nfollowing features:\n\u2022 A penalty on the coefficients of b, similar to ridge regression.\n\u2022 A linear penalty (instead of quadratic) for large errors in the prediction.\n\u2022 An \u03f5-tolerance for small errors, often referred to as the margin of the regression\nSVM.\nWe now describe the various steps in the analysis and reduction of the problem.\nThey will lead to simple minimization algorithms, and possible extensions to non-\nlinear problems.\nReduction to a quadratic programming problem\nIntroduce slack variables \u03bek,\u03be\u2217\nk,k =\n1,...,N. The original problem is equivalent to the minimization, with respect to\n(a0,b,\u03be,\u03be\u2217), of\nN\nX\nk=1\n(\u03bek + \u03be\u2217\nk) + \u03bbbT \u2206b\nunder the constraints:\n\uf8f1\uf8f4\uf8f4\uf8f4\uf8f4\uf8f2\uf8f4\uf8f4\uf8f4\uf8f4\uf8f3\n\u03bek,\u03be\u2217\nk \u22650\n\u03bek \u2212yk + a0 + xT\nk b + \u03f5 \u22650\n\u03be\u2217\nk + yk \u2212a0 \u2212xT\nk b + \u03f5 \u22650\n(7.11)\nThe simple proof of this equivalence, which results in a quadratic programming\nproblem, is left to the reader. As often, one gains additional insight by studying the\ndual problem.\nDual problem\nIntroduce 4N non-negative Lagrange multipliers for the 4N con-\nstraints in the problem, namely, \u03b7k,\u03b7\u2217\nk \u22650 for the positivity constraints, and \u03b1k,\u03b1\u2217\nk \u2265\n7.4. SUPPORT VECTOR MACHINES FOR REGRESSION\n163\n0 for the last two in (7.11). The resulting Lagrangian is\nL(a0,b,\u03be,\u03be\u2217,\u03b1,\u03b1\u2217,\u03b7,\u03b7\u2217) =\nN\nX\nk=1\n(\u03bek + \u03be\u2217\nk) + \u03bbbT \u2206b \u2212\nN\nX\nk=1\n(\u03b7k\u03bek + \u03b7\u2217\nk\u03be\u2217\nk)\n\u2212\nN\nX\nk=1\n\u03b1k(\u03bek \u2212yk + a0 + xT\nk b + \u03f5) \u2212\nN\nX\nk=1\n\u03b1\u2217\nk(\u03be\u2217\nk + yk \u2212a0 \u2212xT\nk b + \u03f5).\nIn this formulation, (a0,b,\u03be,\u03be\u2217) are the primal variables, and \u03b1,\u03b1\u2217,\u03b7,\u03b7\u2217the dual vari-\nables.\nThe KKT conditions are provided by the system:\n\uf8f1\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f2\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f3\nN\nX\nk=1\n(\u03b1k \u2212\u03b1\u2217\nk) = 0\n2\u03bb\u2206b \u2212\nN\nX\nk=1\n(\u03b1k \u2212\u03b1\u2217\nk)xk = 0\n1 \u2212\u03b7k \u2212\u03b1k = 0\n1 \u2212\u03b7\u2217\nk \u2212\u03b1\u2217\nk = 0\n\u03b1k(\u03f5 + \u03bek \u2212yk + a0 + xT\nk b) = 0\n\u03b1\u2217\nk(\u03f5 + \u03be\u2217\nk + yk \u2212a0 \u2212xT\nk b) = 0\n\u03b7k\u03bek = \u03b7\u2217\nk\u03be\u2217\nk = 0\n(7.12)\nThe first four equations are the derivatives of the Lagrangian with respect to a0,b,\u03bek,\u03be\u2217\nk\nin this order and the last three are the complementary slackness conditions.\nThe dual problem maximizes the function\nL\u2217(\u03b1,\u03b1\u2217,\u03b7,\u03b7\u2217) = inf\n\u03b2,\u03be,\u03be\u2217L.\nunder the previous positivity constraints. Since the Lagrangian is linear in a0, \u03bek\nand \u03be\u2217\nk, its minimum is \u2212\u221eunless the coefficients vanish. The linear terms must\ntherefore vanish for L\u2217to be finite. With these conditions plus the fact that \u2202bL = 0,\nwe retrieve the first four equations of system (7.12). Using \u03b7k = 1 \u2212\u03b1k, \u03b7\u2217\nk = 1 \u2212\u03b1\u2217\nk\nand\nb = 1\n2\u03bb\nN\nX\nk=1\n(\u03b1k \u2212\u03b1\u2217\nk)\u2206\u22121xk\n(7.13)\none can express L\u2217uniquely as a function of \u03b1,\u03b1\u2217, yielding\nL\u2217(\u03b1,\u03b1\u2217) = \u22121\n4\u03bb\nN\nX\nk,l=1\n(\u03b1k \u2212\u03b1\u2217\nk)(\u03b1l \u2212\u03b1\u2217\nl )xT\nk \u2206\u22121xl \u2212\u03f5\nN\nX\nk=1\n(\u03b1k + \u03b1\u2217\nk) +\nN\nX\nk=1\n(\u03b1k \u2212\u03b1\u2217\nk)yk .\n164\nCHAPTER 7. LINEAR REGRESSION\nThis quantity must be maximized subject to the constraints 0 \u2264\u03b1k,\u03b1\u2217\nk \u22641 and\nPN\nk=1(\u03b1k \u2212\u03b1\u2217\nk) = 0. This still is a quadratic programming problem, but it now has\nnice additional features and interpretations.\nStep 3: Analysis of the dual problem\nThe dual problem only depends on the xk\u2019s\nthrough the matrix with coefficients xT\nk \u2206\u22121xl, which is the Gram matrix of x1,...,xN\nfor the inner product associated with \u2206\u22121. This property will lead to the the kernel\nversion of SVMs discussed in the next section. The obtained predictor can also be\nexpressed as a function of these products, since\ny = a0 + xT b = a0 + 1\n2\u03bb\nN\nX\nk=1\n(\u03b1k \u2212\u03b1\u2217\nk)(xT\nk \u2206\u22121x).\nMoreover, the dimension of the dual problem is 2N, which allows the method to be\nused in large (possibly infinite) dimensions with a bounded cost.\nWe now analyze the solutions \u03b1,\u03b1\u2217of the dual problem. The complementary\nslackness conditions reduce to:\n\uf8f1\uf8f4\uf8f4\uf8f4\uf8f4\uf8f2\uf8f4\uf8f4\uf8f4\uf8f4\uf8f3\n\u03b1k(\u03f5 + \u03bek \u2212yk + a0 + xT\nk b) = 0\n\u03b1\u2217\nk(\u03f5 + \u03be\u2217\nk + yk \u2212a0 \u2212xT\nk b) = 0\n(1 \u2212\u03b1k)\u03bek = (1 \u2212\u03b1\u2217\nk)\u03be\u2217\nk = 0\n(7.14)\nThese conditions have the following consequences, based on the prediction error\nmade for each training sample.\n(i) First consider indexes k such that the error is strictly within the tolerance mar-\ngin \u03f5: |yk \u2212a0 \u2212xT\nk b| < \u03f5. Then the terms between parentheses in first two equations\nof (7.14) are strictly positive, which implies that \u03b1k = \u03b1\u2217\nk = 0. The last two equations\nin (7.14) then imply \u03bek = \u03be\u2217\nk = 0.\n(ii) Consider now the case when the prediction is strictly less accurate than the\ntolerance margin. Assume that yk \u2212a0 \u2212xT\nk b > \u03f5. The second and third equations in\n(7.14) imply that \u03b1\u2217\nk = \u03be\u2217\nk = 0. The assumption also implies that\n\u03bek = yk \u2212a0 \u2212xT\nk b \u2212\u03f5 > 0\nand \u03b1k = 1. The case yk \u2212a0 \u2212xT\nk b < \u2212\u03f5 is symmetric and provides \u03b1k = \u03bek = 0, \u03be\u2217\nk > 0\nand \u03b1\u2217\nk = 1.\n(iii) Finally, consider samples for which the prediction error is exactly at the toler-\nance margin. If yk \u2212a0 \u2212xT\nk b = \u03f5, we have \u03b1\u2217\nk = \u03bek = \u03be\u2217\nk = 0. The fact that \u03b1\u2217\nk = \u03be\u2217\nk = 0 is\nclear. To prove that \u03bek = 0, we note that would have otherwise \u03bek\u2212yk+a0+xT\nk b+\u03f5 > 0,\nwhich would imply that \u03b1k = 0 and we reach a contradiction with (1\u2212\u03b1k)\u03bek = 0. Sim-\nilarly, yk \u2212a0 \u2212xT\nk b = \u2212\u03f5 implies that \u03b1k = \u03bek = \u03be\u2217\nk = 0.\nThe points for which |yk \u2212a0 \u2212xT\nk b| = \u03f5 are called support vectors.\n7.4. SUPPORT VECTOR MACHINES FOR REGRESSION\n165\nOne important information deriving from this discussion is that the variables\n(\u03b1k,\u03b1\u2217\nk) have prescribed values as long as the error yk \u2212a0 \u2212xT\nk b is not exactly \u03f5 in\nabsolute value: (1,0) if the error is larger than \u03f5, (0,0) if it is strictly between \u2212\u03f5 and \u03f5\nand (0,1) if it is less than \u2212\u03f5. Also in all cases, at least one of \u03b1k and \u03b1\u2217\nk must vanish.\nOnly in the case of support vectors does the previous discussion fail to provide a\nvalue for one of these variables.\nNow, we want to reverse the discussion and assume that the dual problem is\nsolved to see how the variables a0 and b of the primal problem can be retrieved. For\nb, this is easy, thanks to (7.13). For a0 a direct computation can be made if a support\nvector is identified, either because 0 < \u03b1k < 1, which implies that a0 = yk \u2212xT\nk b \u2212\u03f5, or\nbecause 0 < \u03b1\u2217\nk < 1, which yields a0 = yk \u2212xT\nk b + \u03f5.\nIf no support vector can be identified, a0 is not uniquely determined (note that\nthe objective function is not strictly convex in a0). However, the coefficients \u03b1k,\u03b1\u2217\nk\nprovide some information on this intercept, in the form of inequalities. More pre-\ncisely, let J+ = {k : \u03b1k = 1}, J\u2212= {k : \u03b1\u2217\nk = 1} and J0 = {k : \u03b1k = \u03b1\u2217\nk = 0}. Then k \u2208J+\nimplies that yk \u2212a0 \u2212bT xk \u2265\u03f5, so that a0 \u2264yk \u2212bT xk \u2212\u03f5. Similarly, k \u2208J\u2212implies that\na0 \u2265yk \u2212bT xk + \u03f5. Finally, k \u2208J0 implies that a0 \u2265yk \u2212bT xk \u2212\u03f5 and a0 \u2264yk \u2212bT xk + \u03f5.\nAs a consequence, one can take a0 to be any point in the interval [a0\u2212,a0+], where\na0\n\u2212= max\n \nmax\nk\u2208J\u2212(yk \u2212xT\nk b + \u03f5),max\nk\u2208J0\n(yk \u2212xT\nk b \u2212\u03f5)\n!\na0\n+ = min\n \nmin\nk\u2208J+ (yk \u2212xT\nk b \u2212\u03f5),min\nk\u2208J0\n(yk \u2212xT\nk b + \u03f5)\n!\n.\n7.4.2\nThe kernel trick and SVMs\nReturning to our feature space notation, let X take values in RX and h : RX \u2192H be\na feature function with values in an inner-product space H with associated kernel K.\nSVMs in feature space must minimize, with a0 \u2208R and b \u2208H\nF(a0,b) =\nN\nX\nk=1\nV\n\u0010\nyk \u2212a0 \u2212\u27e8h(xk) , b\u27e9H\n\u0011\n+ \u03bb\u2225b\u22252\nH .\nLetting as before V = span(h(x1),...,h(xN)), the same argument as that made for\nridge regression works, namely that the first term in F is unchanged if b is replaced\nby \u03c0V (b) and the second one is strictly reduced unless b \u2208V , leading to a finite-\ndimensional formulation in which\nb =\nN\nX\nk=1\nckh(xk)\n166\nCHAPTER 7. LINEAR REGRESSION\nand one minimizes\nF(a0,c) =\nN\nX\nk=1\nV\n\u0012\nyk \u2212a0 \u2212\nN\nX\nl=1\nK(xk,xl)cl\n\u0013\n+ \u03bb\nN\nX\nk,l=1\nK(xk,xl)ckcl .\nThis function has the same form as the one studied in the linear case with b replaced\nby c \u2208RN, xk replaced by the vector with coefficients K(xk,xl),l = 1,...,N, that we\nwill denote K(k) and \u2206= K = K(x1,...,xN). Note that K(k) is the kth column of K, so\nthat\n\u0010\nK(k)\u0011T K\u22121K(l) = K(xk,xl).\nUsing this, we find that the dual problem requires to maximize\nL\u2217(\u03b1,\u03b1\u2217) = \u22121\n4\u03bb\nN\nX\nk,l=1\n(\u03b1k \u2212\u03b1\u2217\nk)(\u03b1l \u2212\u03b1\u2217\nl )K(xk,xl) \u2212\u03f5\nN\nX\nk=1\n(\u03b1k + \u03b1\u2217\nk) +\nN\nX\nk=1\n(\u03b1k \u2212\u03b1\u2217\nk)yk .\nwith\n\uf8f1\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f2\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f3\n0 \u2264\u03b1k \u22641\n0 \u2264\u03b1\u2217\nk \u22641\nN\nX\nk=1\n(\u03b1k \u2212\u03b1\u2217\nk) = 0\nThe associated vector c satisfies\n2\u03bbc =\nN\nX\nk=1\n(\u03b1k \u2212\u03b1\u2217\nk)K\u22121K(k) = \u03b1 \u2212\u03b1\u2217.\nand the regression function is\nf (x) = a0 + \u27e8b , h(x)\u27e9H = a0 + 1\n2\u03bb\nN\nX\nk=1\n(\u03b1k \u2212\u03b1\u2217\nk)K(x,xk).\nFinally, the discussions on the values of \u03b1,\u03b1\u2217and on the computation of a0 remain\nunchanged.\nChapter 8\nModels for linear classification\nIn this chapter, Y is categorical and takes values in the finite set RY =\nn\ng1,...,gq\no\n.\nThe goal is to predict this class variable from the input X, taking values in a set RX.\nUsing the same progression as in the regression case, we will first discuss basic linear\nmethods, for which RX = Rd before extending them, whenever possible, to kernel\nmethods, for which RX can be arbitrary as soon as a feature space representation is\navailable.\nClassifiers will be based on a training set T = ((x1,y1),...,(xN,yN)) with xk \u2208RX\nand yk \u2208RY for k = 1,...,N. For g \u2208RY, we will also let Ng denote the number of\nsamples in the training set such that yk = g, i.e.,\nNg =\n\f\f\f{k : yk = g}\n\f\f\f =\nN\nX\nk=1\n1yk=g.\n8.1\nLogistic regression\n8.1.1\nGeneral Framework\nLogistic regression uses the fact that, in order to apply Bayes\u2019s rule, only the condi-\ntional distribution of the class variables Y given X is needed, and trains a parametric\nmodel of this distribution. More precisely, if one denotes by p(g|x) the probability\nthat Y = g conditional to X = x, logistic regression assumes that, for some parameters\n(a0(g),b(g),g \u2208RY) with a0(g) \u2208R and b(g) \u2208Rd, one has p = pa0,b with\nlogpa0,b(g | x) = a0(g) + xT b(g) \u2212log(C(a0,b,x)),\nwhere C(a0,b,x) = P\ng\u2208RY exp(a0(g) + xT b(g)).\n167\n168\nCHAPTER 8. MODELS FOR LINEAR CLASSIFICATION\nIntroduce the functions, defined over mappings \u00b5 : RY \u2192R (which can be iden-\ntified with vectors in Rq)\nFg(\u00b5) = \u00b5(g) \u2212log\nX\ng\u2032\u2208RY\ne\u00b5(g\u2032) .\n(8.1)\nWith this notation, letting \u03b2(g) =\n \na0(g)\nb(g)\n!\nand \u02dcx =\n \n1\nx\n!\n, one has logp\u03b2(g|x) = Fg(\u03b2T \u02dcx),\nwhere \u03b2T \u02dcx is the function (g\u2032 7\u2192\u03b2(g\u2032)T \u02dcx).\nFor any constant function (g 7\u2192\u00b50 \u2208R) one has\nFg(\u00b5 + \u00b50) = \u00b5(g) + \u00b50 \u2212log\nX\ng\u2032\u2208RY\ne\u00b5(g\u2032)+\u00b50 = \u00b5(g) + \u00b50 \u2212\u00b50 \u2212log\nX\ng\u2032\u2208RY\ne\u00b5(g\u2032) = Fg(\u00b5).\nAs a consequence, if one replaces, for all g, \u03b2(g) by \u02dc\u03b2(g) = \u03b2(g) + \u03b3, with \u03b3 \u2208Rd+1,\nthen \u02dc\u03b2T \u02dcx = \u03b2T \u02dcx + \u03b3T \u02dcx and\nlogp \u02dc\u03b2(g | x) = logp\u03b2(g | x).\nThis shows that the model is over-parametrized.\nOne therefore needs a (d + 1)-\ndimensional constraint to ensure uniqueness, and we will enforce a linear constraint\nin the form\nX\ng\u2208RY\n\u03c1g\u03b2(g) = c\nwith P\ng \u03c1g , 0.\n8.1.2\nConditional log-likelihood\nThe conditional log-likelihood computed from the training set is:\n\u2113(\u03b2) =\nN\nX\nk=1\nlogp\u03b2(yk | xk).\nLogistic regression computes a maximizer \u02c6\u03b2 of this log-likelihood. The classification\nrule given a new input x then chooses the class g for which p \u02c6\u03b2(g | x) is largest, or,\nequivalently, the class g that maximizes \u02dcxT \u03b2(g).\nProposition 8.1 Let \u02dcmg = P\nk:yg=k xk/Ng. The conditional log-likelihood \u2113is concave\nwith first derivative\n\u2202\u03b2(g)\u2113= Ng \u02dcmT\ng \u2212\nN\nX\nk=1\n\u02dcxT\nk p\u03b2(g|xk)\n(8.2)\n8.1. LOGISTIC REGRESSION\n169\nand negative semi-definite second derivative\n\u2202\u03b2(g)\u2202\u03b2(g\u2032)\u2113= \u22121[g=g\u2032]\nN\nX\nk=1\n\u02dcxk \u02dcxT\nk p\u03b2(g|xk) +\nN\nX\nk=1\n\u02dcxk \u02dcxT\nk p\u03b2(g|xk)p\u03b2(g\u2032|xk).\n(8.3)\nRemark 8.2 In this discussion, we consider \u2113as a function defined over collections\n(\u03b2(g),g \u2208RY), or, if one prefers, on the q(d + 1)-dimensional linear space, F , of\nfunctions \u03b2 : RY \u2192Rq+1. With this in mind, the differential d\u2113(\u03b2) is a linear form\nfrom F to R, therefore associating to any family u = (u(g),g \u2208RY), the expression\nd\u2113(\u03b2)u =\nX\ng\u2208RY\n\u2202\u03b2(g)\u2113u(g).\nSimilarly, the second derivative is the bilinear form\nd2\u2113(\u03b2)(u,u\u2032) =\nX\ng,g\u2032\u2208RY\nu(g)T \u2202\u03b2(g)\u2202\u03b2(g\u2032)\u2113u(g\u2032).\nThe last statement in the proposition expresses the fact that d2\u2113(\u03b2)(u,u) \u22640 for all\nu \u2208F .\n\u2666\nProof First consider the function Fg in (8.1), so that\n\u2113(\u03b2) =\nN\nX\nk=1\nFyk(\u03b2T \u02dcxk).\nWe have, for \u03b6 : RY \u2192R,\ndFg(\u00b5)\u03b6 = \u03b6(g) \u2212\nP\ng\u2032\u2208RY e\u00b5(g\u2032)\u03b6(g\u2032)\nP\ng\u2032\u2208RY e\u00b5(g\u2032)\nas can be easily computed by evaluating the derivative of F(\u00b5 + \u03f5u) at \u03f5 = 0. Intro-\nducing the notation\nq\u00b5(g) =\ne\u00b5(g)\nP\ng\u2208RY e\u00b5(g)\nand\n\u27e8\u03b6\u27e9\u00b5 =\nX\ng\u2208RY\n\u03b6(g)q\u00b5(g),\nwe have dFg(\u00b5)\u03b6 = \u03b6(g) \u2212\u27e8\u03b6\u27e9\u00b5. Evaluating the derivative of dFg(\u00b5 + \u03f5u\u2032)(\u03b6) at \u03f5 = 0,\none gets (the computation being left to the reader)\nd2Fg(\u00b5)(\u03b6,\u03b6\u2032) = \u2212\u27e8\u03b6\u03b6\u2032\u27e9\u00b5 + \u27e8\u03b6\u27e9\u00b5 \u27e8\u03b6\u2032\u27e9\u00b5.\n(8.4)\n170\nCHAPTER 8. MODELS FOR LINEAR CLASSIFICATION\nNote that \u2212d2Fg(\u00b5)(\u03b6,\u03b6) is the variance of \u03b6 for the probability mass function q\u00b5 and\nis therefore non-negative (so that F\u00b5 is concave). This immediately shows that \u2113is\nconcave as a sum of concave functions.\nUsing the chain rule, we have, for u : RY \u2192Rq,\nd\u2113(\u03b2)u =\nN\nX\nk=1\ndFyk(\u03b2T \u02dcxk) \u02dcxT\nk u(\u00b7) =\nN\nX\nk=1\n\u02dcxT\nk u(yk) \u2212\nN\nX\nk=1\n\u27e8\u02dcxT\nk u(\u00b7)\u27e9\u03b2T \u02dcxk.\nReordering the first sum in the right-hand side according to the values of yk gives\nN\nX\nk=1\nu(yk)T \u02dcxk =\nX\ng\u2208RY\nNgu(g)T \u02dcmg.\nNoting that q\u03b2T \u02dcx = p\u03b2(\u00b7|x), we find\nd\u2113(\u03b2)(u) =\nX\ng\u2208RY\nNg \u02dcmT\ng u(g) \u2212\nX\ng\u2208RY\n\u02dcxT\nk u(g)p\u03b2(g|xk),\nyielding (8.2). Applying the chain rule again, we have\nd2\u2113(\u03b2)(u,u\u2032) =\nN\nX\nk=1\nd2Fyk(\u03b2T \u02dcxk)( \u02dcxT\nk u(\u00b7), \u02dcxT\nk u\u2032(\u00b7))\n(8.5)\nwith\nd2Fyk(\u03b2T \u02dcxk)( \u02dcxT\nk u(\u00b7), \u02dcxT\nk u\u2032(\u00b7)) = \u2212\u27e8u(\u00b7)T \u02dcxk \u02dcxT\nk u\u2032(\u00b7)\u27e9\u03b2T \u02dcxk + \u27e8\u02dcxT\nk u(\u00b7)\u27e9\u03b2T \u02dcxk \u27e8\u02dcxT\nk u\u2032(\u00b7)\u27e9\u03b2T \u02dcxk\n= \u2212\nX\ng\u2208RY\nu(g)T \u02dcxk \u02dcxT\nk u\u2032(g)p\u03b2(g|xk)\n+\nX\ng,g\u2032\u2208RY\nu(g)T \u02dcxk \u02dcxT\nk u\u2032(g\u2032)p\u03b2(g|xk)p\u03b2(g\u2032|xk)\nfrom which (8.3) follows.\n\u25a0\nRemark 8.3 From Fg(\u00b5+\u00b50) = Fg(\u00b5) when \u00b50 is constant on RY, one deduces (taking\nthe derivative at \u00b50 = 0) that dFg(\u00b5)1 = 0 for all \u00b5, where 1 denotes the constant\nfunction equal to 1 on RY. For h \u2208Rd+1, let ch denote the constant function ch(g) = h,\ng \u2208RY. We have\nd\u2113(\u03b2)ch =\nN\nX\nk=1\ndFyk(\u03b2T \u02dcxk) \u02dcxT\nk ch =\nN\nX\nk=1\ndFyk(\u03b2T \u02dcxk)( \u02dcxT\nk h1) =\nN\nX\nk=1\n( \u02dcxT\nk h)dFyk(\u03b2T \u02dcxk)1 = 0.\nTaking one extra derivative we see that\nd\u2113(\u03b2)(ch,u) = 0\nfor all functions u : RY \u2192Rq.\n\u2666\n8.1. LOGISTIC REGRESSION\n171\nWe now discuss whether there are other elements in the null space of the second\nderivative of \u2113. We will use notation introduced in the proof of proposition 8.1.\nFrom (8.4), we have d2Fg(\u00b5)(\u03b6,\u03b6) = 0 if and only if the variance of \u03b6 for q\u00b5 vanishes,\nwhich, since q\u00b5 > 0, is equivalent to \u03b6 being constant. So, the null space of d2Fg(\u00b5)\nis one-dimensional, and composed of scalar multiples of 1. Using (8.5), we see that\nd2\u2113(u,u) = 0 if and only if , for all k = 1,...,N, (g 7\u2192\u02dcxT\nk u(g)) is a constant function.\nAssume that this is true. Then, letting \u00afu = 1\nq\nP\ng\u2208RY u(g), one has, for all g \u2208RY\nand k = 1,...,N,\n\u02dcxT\nk u(g) = \u02dcxT\nk \u00afu\nso that u(g) \u2212\u00afu is in the null space of the matrix X . This leads to the following\nproposition.\nProposition 8.4 Assume that X has rank d + 1. Then the null space of d2\u2113(\u03b2) is the set\nof all vectors u = ch for h \u2208Rd+1. In particular, for any c \u2208Rd+1, the function \u2113restricted\nto the space\nM =\n\uf8f1\uf8f4\uf8f4\uf8f4\uf8f2\uf8f4\uf8f4\uf8f4\uf8f3\n\u03b2 :\nX\ng\u2208RY\n\u03c1g\u03b2(g) = c\n\uf8fc\uf8f4\uf8f4\uf8f4\uf8fd\uf8f4\uf8f4\uf8f4\uf8fe\nis strictly concave as soon as the scalar coefficients (\u03c1g,g \u2208RY) are such that P\ng\u2208RY \u03c1g , 0.\nProof From the discussion before the proposition, u \u2208Null(d2\u2113) implies that X (u(g)\u2212\n\u00afu) = 0 for all g, and since we assume that X has rand d +1, this requires that u(g) = \u00afu\nfor all g, i.e., u = c \u00afu. This proves the first point.\nIf one restricts \u2113to M, then we must restrict d2\u2113(\u03b2) to those u\u2019s such that P\ng\u2208RY \u03c1gu(g) =\n0. But if d2\u2113(\u03b2)(u,u) = 0 for such an u, then u = c \u00afu and\nX\ng\u2208RY\n\u03c1gu(g) =\n\u0012 X\ng\u2208RY\n\u03c1g\n\u0013\n\u00afu.\nSince we assume that P\ng\u2208RY \u03c1g , 0, this requires \u00afu = 0, and therefore u = 0.\nThis shows that the second derivative of the restriction of \u2113to M is negative\ndefinite, so this restriction is strictly concave.\n\u25a0\n8.1.3\nTraining algorithm\nGiven that we have expressed the first and second derivatives of \u2113in closed form1,\nwe can use Newton-Raphson gradient ascent to maximize \u2113over the affine space:\nM =\n\uf8f1\uf8f4\uf8f4\uf8f4\uf8f2\uf8f4\uf8f4\uf8f4\uf8f3\n\u03b2 :\nX\ng\u2208RY\n\u03c1g\u03b2(g) = c\n\uf8fc\uf8f4\uf8f4\uf8f4\uf8fd\uf8f4\uf8f4\uf8f4\uf8fe\n1Their computation is feasible unless N is very large, and the matrix inversion in Newton\u2019s itera-\ntion also requires d to be not too large.\n172\nCHAPTER 8. MODELS FOR LINEAR CLASSIFICATION\nwith P\ng\u2208RY \u03c1g , 0. We assume in the following that the matrix X has rand d + 1 so\nthat proposition 8.4 applies. Since the constraint is affine, it is easy to express one of\nthe parameters \u03b2(g) as a function of the others and solve the strictly concave problem\nas a function of the remaining variables. It is not much harder, and arguably more\nelegant to solve the problem without breaking its symmetry with respect to the class\nindexes, as described below.\nLet\nM0 =\n\uf8f1\uf8f4\uf8f4\uf8f4\uf8f2\uf8f4\uf8f4\uf8f4\uf8f3\n\u03b2 :\nX\ng\u2208RY\n\u03c1g\u03b2(g) = 0\n\uf8fc\uf8f4\uf8f4\uf8f4\uf8fd\uf8f4\uf8f4\uf8f4\uf8fe\n.\nWe still have the second order expansion\n\u2113(\u03b2 + u) = \u2113(\u03b2) + d\u2113(\u03b2)u + 1\n2d2\u2113(\u03b2)(u,u) + o(|u|2)\nand we consider the maximization of the first three terms, simply restricting to vec-\ntors u \u2208M0. To allow for matrix computation, we use our ordering RY = (g1,...,gq)\nand identify a with the column vector\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\nu(g1)\n...\nu(gq)\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\n\u2208Rq(d+1)\nSimilarly, we let\n\u2207\u2113(\u03b2) =\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\n\u2202\u03b2(g1)\u2113\n...\n\u2202\u03b2(gq)\u2113\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\nand let \u22072(\u2113)(\u03b2) be the block matrix with i,j block given by \u2202\u03b2(gi)\u2202\u03b2(gj)\u2113(\u03b2). We let \u02c6\u03c1\nbe the (d + 1) \u00d7 q(d + 1) row block matrix\n\u0010\n\u03c1(g1)IdRd+1\n\u00b7\u00b7\u00b7\n\u03c1(gq)IdRd+1\n\u0011\nso that u \u2208M0 is just \u02c6\u03c1u = 0 in vector notation. Given this we have\n\u2113(\u03b2 + u) = \u2113(\u03b2) + \u2207\u2113(\u03b2)T u + 1\n2uT \u22072(\u2113)(\u03b2)u + o(|u|2).\nThe maximum of \u2113(\u03b2) + uT \u2207\u2113(\u03b2) + 1\n2uT \u22072(\u2113)(\u03b2)u subject to \u02c6\u03c1u = 0 is a stationary\npoint of the Lagrangian\nL = \u2113(\u03b2) + uT \u2207\u2113(\u03b2) + 1\n2uT \u22072(\u2113)(\u03b2)u + \u03bbT \u02c6\u03c1u\n8.1. LOGISTIC REGRESSION\n173\nfor some \u03bb \u2208Rd+1 and is characterized by\n(\u22072(\u2113)(\u03b2)u + \u2207\u2113(\u03b2) + \u02c6\u03c1T \u03bb = 0\n\u02c6\u03c1u = 0\nThis shows that the Newton-Raphson iterations can be implemented as\n\u03b2n+1 = \u03b2n \u2212\u03f5n+1un+1\n(8.6)\nwith\n \nun+1\n\u03bb\n!\n=\n \n\u22072(\u2113)(\u03b2n)\n\u02c6\u03c1T\n\u02c6\u03c1\n0\n!\u22121  \n\u2207\u2113(\u03b2n)\n0\n!\n.\n(8.7)\nWe summarize this discussion in the following algorithm.\nAlgorithm 8.1 (Logistic regression with Newton\u2019s gradient ascent)\n(1) Input: (i) training data (x1,y1,...,xN,yN) with xi \u2208Rd and yi \u2208RY; (ii) coefficients\n\u03c1g,g \u2208RY with non-zero sum and target value c \u2208R; (iii) algorithm step \u03f5 small\nenough.\n(2) Initialize the algorithm with \u03b20 such that P\ng \u03c1g\u03b20(g) = c.\n(3) At iteration n, compute \u2207\u2113(\u03b2n) and \u22072(\u2113)(\u03b2n) as provided by proposition 8.1.\n(4) Update \u03b2n using (8.6) and (8.7), with \u03f5n+1 = \u03f5. Alternatively, optimize \u03f5n+1 using\na line search.\n(5) Stop the procedure if the change in the parameter is below a small tolerance\nlevel. Otherwise, return to step 2.\n8.1.4\nPenalized Logistic Regression\nLogistic regression can be combined with a penalty term, e.g., maximizing\n\u21132(\u03b2) = \u2113(\u03b2) \u2212\u03bb\nd\nX\ni=1\n|b(i)|2\n(8.8)\nor\n\u21131(\u03b2) = \u2113(\u03b2) \u2212\u03bb\nd\nX\ni=1\n|b(i)|\n(8.9)\nwhere b(i) is the q-dimensional vector formed with the ith coefficients of b(g) for\ng \u2208RY. Similarly to penalized regression, one generally normalizes the x variables\nto have unit standard deviation before applying the method.\n174\nCHAPTER 8. MODELS FOR LINEAR CLASSIFICATION\nMaximization with the \u21132 norm\nThe problem in (8.8) relates to ridge regression\nand can be solved using a Newton-Raphson method (Algorithm 8.1) with minor\nchanges. More precisely, letting\n\u2206=\n \n0\n0\n0\nIdRd\n!\nwe have, considering \u03b2 as a d + 1 by q matrix,\n\u21132(\u03b2) = \u2113(\u03b2) \u2212\u03bbtrace(\u03b2T \u2206\u03b2)\nand\nd\u21132(\u03b2)u = d\u2113(\u03b2)u \u22122\u03bbtrace(\u03b2T \u2206u),\nd2\u21132(\u03b2)(u,u\u2032) = d\u2113(\u03b2)(u,u\u2032) \u22122\u03bbtrace(uT \u2206u\u2032).\nIn addition, when \u03bb > 0, the problem is over-parametrized only up to the addition\nof a constant to (g 7\u2192a0(g)), so that one only needs a single constraint P\ng \u03c1ga0(g) = 0\nand the Lagrange coefficient in (8.7) is one dimensional.\nMaximization with the \u21131 norm\nThe maximization in (8.9) can be run using prox-\nimal gradient ascent (section 3.5.5), writing the objective function in the form\n\u21131(a0,b) = \u2113(a0,b) \u2212\u03bb\u03b3(a0,b)\nwith\n\u03b3(a0,b) =\nd\nX\ni=1\ns X\ng\u2208RY\nb(i)(g)2.\nHere, \u2113is concave and \u03b3 is convex and the proximal gradient iterations are\n\u03b2n+1 = prox\u03f5\u03bb\u03b3(\u03b2n + \u03f5\u2207\u2113(\u03b2n))\n(8.10)\nwhere \u2207\u2113is the gradient of \u2113projected on the set of functions u : RY \u2192Rd+1 satis-\nfying\nX\ng\u2208RY\n\u03c1gu(0)(g) = 0\nwhere u(0)(g) is the first coordinate of u(g). This projection can be computed by\nsubtracting\nP\ng\u2032\u2208RY \u03c1(g\u2032)\u2202a0(g\u2032)\u2113\nP\ng\u2032\u2208RY \u03c1(g\u2032)2\n\u03c1(g)\nto \u2202a0(g)\u2113. This algorithm will converge for small enough \u03f5.\n8.1. LOGISTIC REGRESSION\n175\nWe already know the gradient of \u2113, so it only remains to determine the proximal\noperator of \u03b3 to make (8.10) explicit. Let us denote the coordinates of a function\nu : RY \u2192Rd+1 as u(i)(g) for i = 0,...,d and g \u2208RY.\nprox\u03f5\u03bb\u03b3(u) = argmin\n\u02dcu\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\u03b3( \u02dcu) +\n1\n2\u03bb\u03f5\nd\nX\ni=0\nX\ng\u2208RY\n(u(i)(g) \u2212\u02dcu(i)(g))2\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8.\nSince \u03b3 does not depend on \u02dcu(0)(\u00b7), the optimal \u02dcu(0)(\u02d9) is \u02dcu(0)(\u00b7) = u(0)(\u00b7). One can\noptimize separately each group ( \u02dcu(i)(g),g \u2208RY), which must minimize\ns X\ng\u2208RY\n\u02dcu(i)(g)2 +\n1\n2\u03bb\u03f5\nX\ng\u2208RY\n(u(i)(g) \u2212\u02dcu(i)(g))2.\nThe function t 7\u2192\n\u221a\nt being differentiable everywhere except at 0, we first search for\na solution for which at least one \u02dcu(i)(g) does not vanish. If such a solution exists, it\nmust satisfy, for all g \u2208RY\n\u02dcu(i)(g)\nqP\ng\u2032\u2208RY \u02dcu(i)(g\u2032)2\n+ 1\n\u03bb\u03f5( \u02dcu(i)(g) \u2212u(i)(g)) = 0\nLetting | \u02dcu(i)(\u00b7)| =\nqP\ng\u2208RY \u02dcu(i)(g)2 we get\n\u02dcu(i)(\u00b7)(| \u02dcu(i)(\u00b7)| + \u03bb\u03f5) = u(i)(\u00b7)| \u02dcu(i)(\u00b7)|\nTaking the norm on both sides and dividing by | \u02dcu(i,\u00b7)| (which is assumed not to\nvanish) yields\n| \u02dcu(i)(\u00b7)| + \u03bb\u03f5 = |u(i)(\u00b7)|,\nwhich has a positive solution only if |u(i)(\u00b7)| > \u03bb\u03f5, and gives in that case\n\u02dcu(i)(\u00b7) = |u(i)(\u00b7)| \u2212\u03f5\u03bb\n|u(i)(\u00b7)|\nu(i)(\u00b7)\nIf |u(i)(\u00b7)| \u2264\u03bb\u03f5, then we must take \u02dcu(i)(\u00b7) = 0. We have therefore obtained:\nprox\u03f5\u03bbg(a) = \u02dcu\nwith \u02dcu(0)(\u00b7) = u(0)(\u00b7) and\n\u02dcu(i)(\u00b7) = max\n\u0012|u(i)(\u00b7)| \u2212\u03f5\u03bb\n|u(i)(\u00b7)|\n,0\n\u0013\nu(i)(\u00b7)\nfor i \u22651. We summarize this discussion in the next algorithm.\n176\nCHAPTER 8. MODELS FOR LINEAR CLASSIFICATION\nAlgorithm 8.2 (Logistic lasso)\n(1) Input: (i) training data (x1,y1,...,xN,yN) with xi \u2208Rd and yi \u2208RY; (ii) coefficients\n\u03c1g,g \u2208RY with non-zero sum and target value c \u2208R; (iii) algorithm step \u03f5; (iv)\npenalty coefficient \u03bb.\n(2) Initialize the algorithm with \u03b20 = (a00,b0) such that P\ng \u03c1ga00(g) = c.\n(3) At iteration n, compute u = \u03b2n + \u03f5\u2207\u2113(\u03b2n), with \u03b2n = (a0,n,bn).\n(4) Let an+1,0(\u00b7) = u(0)(\u00b7) and for i \u22651,\nb(i)\nn+1(\u00b7) = max\n\u0012|u(i)(\u00b7)| \u2212\u03f5\u03bb\n|u(i)(\u00b7)|\n,0\n\u0013\nu(i)(\u00b7)\n(5) Stop the procedure if the change in the parameter is below a small tolerance\nlevel. Otherwise, return to step 2.\n8.1.5\nKernel logistic regression\nLet h : RX \u2192H be a feature function with values in a Hilbert space H with K(x,x\u2032) =\n\u27e8h(x) , h(x\u2032)\u27e9H. The kernel version of logistic regression uses the model:\nlogpa0,b(g | x) = a0(g) + \u27e8h(x) , b(g)\u27e9H \u2212log\nX\n\u02dcg\u2208RY\nexp(a0(\u02dcg) + \u27e8h(x) , b(\u02dcg)\u27e9H)\nwith b(g) \u2208H for g \u2208RY.\nUsing the usual kernel argument, one sees that, when maximizing the log-likelihood,\nthere is no loss of generality is assuming that each b(g) belongs to V = span(h(x1),...,h(xN)).\nTaking\nb(g) =\nN\nX\nk=1\n\u03b1k(g)h(xk),\nwe have\nlogp\u03b1(g | x) = a0(g) +\nN\nX\nk=1\n\u03b1k(g)K(x,xk) \u2212log\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\nX\n\u02dcg\u2208RY\nexp(a0(\u02dcg) +\nN\nX\nk=1\n\u03b1k(\u02dcg)K(x,xk))\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8.\nTo avoid overfitting, one must include a penalty term in the likelihood, and (in order\nto take advantage of the kernel), one can take this term proportional to P\ng \u2225b(g)\u22252\nH.\nThe complete learning procedure then requires to maximize the concave penalized\nlikelihood\n\u2113(\u03b1) =\nN\nX\nk=1\nlogp\u03b1(yk | xk) \u2212\u03bb\nX\ng\u2208RY\nN\nX\nk,l=1\n\u03b1k(g)\u03b1l(g)K(xk,xl).\n8.2. LINEAR DISCRIMINANT ANALYSIS\n177\nThe computation of the first and second derivatives of this function is similar to that\nfor the original version, and we skip the details.\n8.2\nLinear Discriminant analysis\n8.2.1\nGenerative model in classification and LDA\nGenerative model\nIn classification, the class variable Y generally has a causal role\nupon which the variable X is produced. Prediction can therefore be seen as an in-\nverse problem where the cause is deduced from the result. In terms of generative\nmodeling, one should therefore model the distribution of Y, followed by the the\nconditional distribution of X given Y.\nTaking RX = Rd, denote by fg the conditional p.d.f. of X given Y = g and let \u03c0g =\nP(Y = g). The Bayes estimator for the 0\u20131 loss maximizes the posterior probability\nP(Y = g | X = x) =\n\u03c0gfg(x)\nP\ng\u2032\u2208RY \u03c0g\u2032fg\u2032(x) .\nSince the denominator does not depend on g the Bayes estimator equivalently max-\nimizes (taking logarithms)\nlogfg(x) + log\u03c0g .\nOne generally speaks of a linear classification method when the prediction is\nbased on the maximization in g of a function U(g,x) where U is affine in x. In this\nsense, logistic regression is linear, and kernel logistic regression is linear in feature\nspace. For the generative approach, this occurs when one uses the following model,\nwhich provides the generative form of linear discriminant analysis (LDA). Assume\nthat the distributions fg are all Gaussian with mean mg and common variance S, so\nthat\nfg(x) =\n1\np\n(2\u03c0)d det\u03a3\ne\u22121\n2(x\u2212mg)T S\u22121(x\u2212mg).\n(8.11)\nIn this case, the optimal predictor must maximize (in g)\n\u22121\n2(x \u2212mg)T S\u22121(x \u2212mg) + log\u03c0g .\nIntroduce m = E(X) = P\ng\u2208RY \u03c0gmg. Then the optimal classifier must maximize\n\u22121\n2(x \u2212m)T S\u22121(x \u2212m) + (x \u2212m)T S\u22121(mg \u2212m) \u22121\n2(mg \u2212m)T S\u22121(mg \u2212m) + log\u03c0g.\nSince the first term does not depend on g, it is equivalent to maximize\n(x \u2212m)T S\u22121(mg \u2212m) \u22121\n2(mg \u2212m)T S\u22121(mg \u2212m) + log\u03c0g\n(8.12)\nwith respect to the class g, which provides an affine function of x.\n178\nCHAPTER 8. MODELS FOR LINEAR CLASSIFICATION\nTraining\nTraining for LDA simply consists in estimating the class means and com-\nmon variance in (8.11) from data. We introduce some notation for this purpose (this\nnotation will be reused through the rest of this chapter).\nRecall that Ng,g \u2208RY denotes the number of samples with class g in the train-\ning set T = (x1,y1,...,xN,yN). We let cg = Ng/N and C be the diagonal matrix with\ndiagonal coefficients cg1,...,cgq. We also let \u03b6 \u2208Rq denote the vector with the same\ncoordinates. For g \u2208RY, \u00b5g denotes the class average\n\u00b5g = 1\nNg\nn\nX\nk=1\nxk1yk=g\nand \u00b5 the global average\n\u00b5 = 1\nN\nN\nX\nk=1\nxk =\nX\ng\u2208RY\ncg\u00b5g.\nLet \u03a3g denote the sample covariance matrix in class g, defined by\n\u03a3g = 1\nNg\nN\nX\nk=1\n(xk \u2212\u00b5g)(xk \u2212\u00b5g)T 1yk=g,\nand \u03a3w the pooled class covariance (also called within-class covariance) defined by\n\u03a3w = 1\nN\nN\nX\nk=1\n(xk \u2212\u00b5yk)(xk \u2212\u00b5yk)T =\nX\ng\u2208RY\ncg\u03a3g.\nLet, in addition, \u03a3b denotes the \u201cbetween-class\u201d covariance matrix, given by\n\u03a3b =\nX\ng\u2208RY\ncg(\u00b5g \u2212\u00b5)(\u00b5g \u2212\u00b5)T\nThe global covariance matrix, given by,\n\u03a3XX = 1\nN\nN\nX\nk=1\n(xk \u2212\u00b5)(xk \u2212\u00b5)T\nsatisfies \u03a3XX = \u03a3w + \u03a3b. This identity is proved by noting that, for any g \u2208RY,\n1\nNg\nN\nX\nk=1\n(xk \u2212\u00b5)(xk \u2212\u00b5)T 1yk=g = \u03a3g + (\u00b5g \u2212\u00b5)(\u00b5g \u2212\u00b5)T .\n8.2. LINEAR DISCRIMINANT ANALYSIS\n179\nWe will finally denote by M the matrix\nM =\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\n(\u00b5g1 \u2212\u00b5)T\n...\n(\u00b5gq \u2212\u00b5)T\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\n.\nNote that \u03a3b = MT CM.\nGiven this notation, one can in particular take \u02c6mg = \u00b5g, m = \u00b5 and \u02c6S = \u03a3w in\n(8.12). The class probabilities, \u03c0g, can be deduced from the normalized frequencies\nof y1,...,yN. However, in many applications, one prefers to simply fix \u03c0g = 1/q, in\norder to balance the importance of each class.\nRemark 8.5 If one relaxes the assumption of common class variances, one needs to\nuse \u03a3g in place of \u03a3w for class g. The decision boundaries are not linear in this\ncase, but provided by quadratic equations (and the resulting method if often called\nquadratic discriminant analysis, or QDA). QDA requires the estimation of qd(d +\n3)/2 coefficients, which may be overly ambitious when the sample size is not large\ncompared to the dimension, in which case QDA is prone to overfitting. (Even LDA,\nwhich involves qd +d(d +1)/2 parameters, may be unrealistic in some cases.) We also\nnote a variant of QDA that uses class covariance matrices given by\n\u02dc\u03a3g = \u03b1\u03a3w + (1 \u2212\u03b1)\u03a3g.\n8.2.2\nDimension reduction\nOne of the interests of LDA is that it can be combined with a rank reduction proce-\ndure. LDA with q classes can always be seen as a (q \u22121)-dimensional problem after\nsuitable projection on a data-dependent affine space. Recall that the classification\nrule after training requires to maximize w.r.t. g \u2208RY the function\n(x \u2212\u00b5)T \u03a3\u22121\nw (\u00b5g \u2212\u00b5) \u22121\n2(\u00b5g \u2212\u00b5)T \u03a3\u22121\nw (\u00b5g \u2212\u00b5) + log\u03c0g.\nDefine the \u201cspherized\u201d data 2 by \u02dcxk = \u03a3\u22121/2\nw\n(xk \u2212\u00b5), where \u03a31/2\nw\nis the positive sym-\nmetric square root of \u03a3w. Also let \u02dc\u00b5g = \u03a3\u22121/2\nw\n(\u00b5g \u2212\u00b5).\nWith this notation, the predictor chooses the class g that maximizes\n\u02dcxT \u02dc\u00b5g \u22121\n2| \u02dc\u00b5g|2 + log\u03c0g\nwith \u02dcx = \u03a3\u22121/2\nw\n(x \u2212\u00af\u00b5).\n2In this section only, the notation \u02dcx does not refer to (1,xT )T .\n180\nCHAPTER 8. MODELS FOR LINEAR CLASSIFICATION\nNow, let V = span{ \u02dc\u00b5g,g \u2208RY}. Since P\ng cg \u02dc\u00b5g = 0, this space is at most (q \u22121)-\ndimensional. Let PV denote the orthogonal projection on V . We have \u02dcxT z = (PV \u02dcx)T z\nfor any z \u2208V and \u02dcx \u2208Rd.\nThe classification rule can then be replaced by maximizing\n(PV \u02dcx)T \u02dc\u00b5g \u22121\n2| \u02dc\u00b5g|2 + log\u03c0g\nwith \u02dcx = \u03a3\u22121/2\nw\n(x \u2212\u00af\u00b5).\nRecall that M =\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\n(\u00b5g1 \u2212\u00b5)T\n...\n(\u00b5gq \u2212\u00b5)T\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\nand let e\nM =\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\n\u02dc\u00b5T\ng1...\n\u02dc\u00b5T\ngq\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\n. The dimension, denoted r, of V is\nequal to the rank of e\nM. Let (\u02dce1,..., \u02dcer) be an orthonormal basis of V . One has\nPV \u02dcx =\nr\nX\nj=1\n( \u02dcxT \u02dcej) \u02dcej.\nGiven an input x, one must therefore compute the \u201cscores\u201d \u03b3j(x) = \u02dcxT \u02dcej and maxi-\nmize\nr\nX\nj=1\n\u03b3j(x)\u03b3j(\u00b5g) \u22121\n2\nr\nX\nj=1\n\u03b3j(\u00b5g)2 + log\u03c0g .\nThe following proposition is key to the practical implementation of LDA with\ndimension reduction.\nProposition 8.6 An orthonormal basis of V = span( \u02dc\u00b5g,g \u2208CG) is provided by the the\nfirst r eigenvectors of e\nMT C e\nM associated with eigenvalues \u03bb1 \u2265\u00b7\u00b7\u00b7 \u2265\u03bbr > 0 (all other\neigenvalues being zero).\nProof Indeed, if \u02dcx is perpendicular to V , we have\n\u02dcMT C \u02dcM \u02dcx =\nX\ng\u2208RY\ncg( \u02dc\u00b5T\ng \u02dcx) \u02dc\u00b5g = 0\nso that V \u22a5\u2282Null( \u02dcMT C \u02dcM), and both spaces coincide because they have the same\ndimension (d \u2212r). This shows that V = Null( e\nMT C e\nM)\u22a5= Range( \u02dcMT C e\nM). Since\ne\nMT C e\nM is symmetric, Null( e\nMT C e\nM)\u22a5is generated by eigenvectors with non-zero\neigenvalues.\n\u25a0\nReturning to the original variables, we have e\nM = M\u03a3\u22121/2\nw\nand MT CM = \u03a3b, the\nbetween class covariance matrix. This implies that e\nMT C e\nM = \u03a3\u22121/2\nw\n\u03a3b\u03a3\u22121/2\nw\nand each\n8.2. LINEAR DISCRIMINANT ANALYSIS\n181\nFigure 8.1: Left: Original (training) data with three classes. Right: LDA scores, where the x\naxis provides \u03b31 and the y axis \u03b32.\neigenvector \u02dcej therefore satisfies\n\u03a3b\u03a3\u22121/2\nw\n\u02dcej = \u03bbj\u03a31/2\nw \u02dcej = \u03bbj\u03a3w(\u03a3\u22121/2\nw\n\u02dcej).\nTherefore, letting ej = \u03a3\u22121/2\nw\n\u02dcej, (e1,...,er) are the solutions of the generalized eigen-\nvalue problem \u03a3be = \u03bb\u03a3we that are associated with non-zero eigenvalues (they are\nhowever normalized so that eT\nj \u03a3wej = 1). Moreover, the scores are given by\n\u03b3j(x) = \u02dcxT \u02dcej = (x \u2212\u00b5)T \u03a3\u22121/2\nw\n\u02dcej = (x \u2212\u00b5)T ej\nand can therefore be computed directly from the original data and the vectors e1,...,er.\nAn example of training data and its representation in the LDA space (associated with\nthe scores) in provided in fig. 8.1.\nWe can now describe the LDA learning algorithm with dimension reduction.\nAlgorithm 8.3 (LDA with dimension reduction)\n1. Compute \u00b5g,g \u2208RY, \u03a3w and \u03a3b from training data.\n2. Estimate (if needed) \u03c0g,g \u2208RY\n3. Solve the generalized eigenvalue problem \u03a3be = \u03bb\u03a3we.\nLet e1,...,er be the\neigenvectors associated with non-zero eigenvalues, normalized so that eT\nj \u03a3wej = 1.\n4. Choose a reduced dimension r0 \u2264r.\n5. Precompute mean scores \u03b3j(\u00b5g) = (\u00b5g \u2212\u00b5)T ej, g \u2208RY,j = 1,...,r0.\n6. To classify a new example x, compute \u03b3j(x) = (x \u2212\u00b5)T ej and choose the class\nthat maximizes\nr0\nX\nj=1\n\u03b3j(x)\u03b3j(\u00b5g) \u22121\n2\nr0\nX\nj=1\n\u03b3j(\u00b5g)2 + log\u03c0g .\n182\nCHAPTER 8. MODELS FOR LINEAR CLASSIFICATION\n8.2.3\nFisher\u2019s LDA\nThis characterization leads to the discriminative interpretation of LDA, also called\nFisher\u2019s LDA. Indeed, the generalized eigenvalue problem \u03a3be = \u03bb\u03a3we is directly\nrelated to the maximization of the ratio eT \u03a3be subject to eT \u03a3we = 1, which provides\ndirections that have a large between-class variance for within class variance equal to\n1. More precisely, e1 is the direction that achieves the maximum; e2 is the second best\ndirection, constrained to being perpendicular to e1, and so on until er which is the\noptimal constrained to be perpendicular to (e1,...,er\u22121). We are therefore looking\nfor directions that have the largest ratio of between-class variance to within-class\nvariance.\n8.2.4\nKernel LDA\nMean and covariance in feature space\nWe assume the usual construction where h :\nRX \u2192H is a feature function, H a Hilbert space with kernel K(x,x\u2032) = \u27e8h(x) , h(x\u2032)\u27e9H.\n(The assumption that H is a complete space is here required for the expectations\nbelow to be meaningful.)\nWe now discuss the kernel version of LDA by plugging the feature space repre-\nsentation directly in the classification rule. So, consider h : R \u2192H. Let X : \u2126\u2192R be\na random variable such that E(\u2225h(X)\u22252\nH) < \u221e. Then, its mean feature m = E(h(X)) is\nwell defined as an element of H , and so are the class averages, mg = E(h(X) | Y = g).\nIn this possibly infinite-dimensional setting, the covariance \u201cmatrix\u201d is defined\nas a linear operator S : H \u2192H such that, for all \u03be,\u03b7 \u2208H:\n\u27e8\u03be , S\u03b7\u27e9H = E\n\u0010\n\u27e8h(X) \u2212m , \u03be\u27e9H\u27e8h(X) \u2212m , \u03b7\u27e9H\n\u0011\n,\n(8.13)\nwhich is equivalent to defining\nS\u03b7 = E(\u27e8h(X) \u2212m , \u03b7\u27e9H(h(X) \u2212m))\nfor \u03b7 \u2208H. This definition generalizes the identity for a random variable U : \u2126\u2192Rd:\nSUw = E((U \u2212E(U))(U \u2212E(U))T )w = E(((U \u2212E(U))T w)(U \u2212E(U)))\nOne can similarly define the covariance matrix in class g, Sg, by conditioning the\nright-hand side in (8.13) by Y = g and replacing m by mg.\nLDA in feature space\nFollowing the LDA model, we assume that the operators Sg are all equal to a fixed\noperator, the within-class covariance operator denoted S.\n8.2. LINEAR DISCRIMINANT ANALYSIS\n183\nAssuming that S is invertible, one can generalize the LDA classification rule to\ndata represented in feature space by classifying a new input x in class g when\n\u27e8h(x) \u2212m , S\u22121(mg \u2212m)\u27e9H \u22121\n2\u27e8mg \u2212m , S\u22121(mg \u2212m)\u27e9H + log\u03c0g\n(8.14)\nis maximal over all classes. Notice that this is a transcription of the finite-dimensional\nBayes rule, but cannot be derived from a generative model, because the assumption\nthat h(X) is Gaussian is not valid in general. (It would require that h takes values\nin a d-dimensional linear space, which would eliminate all interesting kernel repre-\nsentations.)\nLet, as before, T = (x1,y1,...,xN,yN) be the training set, Ng denote the number\nof examples in class g and cg = Ng/N. When h is known (which, we recall, is not a\npractical assumption, but we will fix this later), one can estimate the class averages\nfrom training data by\n\u00b5g = 1\nNg\nN\nX\nk=1\nh(xk)1yk=g\nand the within-class covariance operator by\n\u27e8\u03be , \u03a3w\u03b7\u27e9H = 1\nN\nN\nX\nk=1\n\u27e8h(xk) \u2212\u00b5yk , \u03be\u27e9H\u27e8h(xk) \u2212\u00b5yk , \u03b7\u27e9H.\nUnfortunately, the resulting variance estimator cannot be directly used in (8.14),\nbecause it is not invertible if dim(H) > N. Indeed, one has \u03a3w\u03b7 = 0 as soon as \u03b7 is\nperpendicular to V\n\u2206= span(h(x1),...,h(xN)).\nOne way to address the degeneracy of the estimated covariance operator is to\nadd to \u03a3w a small multiple of the identity, say \u03c1IdH,3 and let the classification rule\nmaximize in g:\n\u27e8h(x) \u2212\u00b5 , (\u03a3w + \u03c1IdH)\u22121(\u00b5g \u2212\u00b5)\u27e9H \u22121\n2\u27e8\u00b5g \u2212\u00b5 , (\u03a3w + \u03c1IdH)\u22121(\u00b5g \u2212\u00b5)\u27e9H + log\u03c0g .\n(8.15)\nwhere \u00b5 is the average of h(x1),...,h(xN). Taking this option, we still need to make\nthis expression computable and remove the dependency in the feature function h.\nReduction\nWe have \u00b5g \u2208V for all g \u2208RY and, since\n\u03a3w\u03b7 = 1\nN\nN\nX\nk=1\n\u27e8h(xk) \u2212\u00b5yk , \u03b7\u27e9H(h(xk) \u2212\u00b5yk),\n3The operator A + \u03c1IdH is invertible as soon as A is symmetric positive semi-definite.\n184\nCHAPTER 8. MODELS FOR LINEAR CLASSIFICATION\nthis operator maps H to V , which implies that \u03a3w + \u03c1IdH maps V into itself. More-\nover, this mapping is onto: If v \u2208V and u = (\u03a3w + \u03c1IdH)\u22121v, then, u \u2208V . Indeed,\nfor any z \u22a5V , we have \u27e8z , \u03a3wu + \u03c1u\u27e9H = \u27e8z , v\u27e9H. We have \u27e8z , \u03a3wu\u27e9H = 0 (because\n\u03a3w maps H to V ) and \u27e8z , v\u27e9H = 0 (because v \u2208V ), so that we can conclude that\n\u27e8z , u\u27e9H = 0. Since this is true for all z \u22a5V , this requires that u \u2208V .4\nWe now express the classification rule in (8.15) as a function of the kernel asso-\nciated with the feature-space representation. Denote, for any vector u \u2208RN,\n\u03be(u) =\nN\nX\nk=1\nu(k)h(xk),\ntherefore defining a mapping \u03be from RN onto V . Letting as usual K = K(x1,...,xN)\nbe the matrix formed by pairwise evaluations of K on training inputs, we have the\nidentity\n\u27e8\u03be(u) , \u03be(u\u2032)\u27e9H = uT Ku\u2032.\nfor all u,u\u2032 \u2208RN. For simplicity, we will assume in the rest of the discussion that K\nis invertible.\nWe have \u00b5g = \u03be(1g/Ng), where 1g \u2208RN is the vector with kth coordinate equal\nto 1 if yk = g and 0 otherwise. Also \u00b5 = \u03be(1/N) (recall that 1 is the vector with all\ncoordinates equal to 1).\nFor u \u2208RN, we want to characterize v \u2208RN such that \u03a3w\u03be(u) = \u03be(v). Let \u03b4k\ndenote the vector with 1 at the kth entry and 0 otherwise. We have\n\u03a3w\u03be(u) = 1\nN\nN\nX\nk=1\n\u27e8\u03be(u) , h(xk) \u2212\u00b5yk\u27e9H(h(xk) \u2212\u00b5yk)\n= 1\nN\nN\nX\nk=1\n\u27e8\u03be(u) , \u03be(\u03b4k \u22121yk/Nyk)\u27e9H\u03be(\u03b4k \u22121yk/Nyk)\n= 1\nN\nN\nX\nk=1\n((\u03b4k \u22121yk/Nyk)T Ku)\u03be(\u03b4k \u22121yk/Nyk)\n= \u03be\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\n1\nN\nN\nX\nk=1\n((\u03b4k \u22121yk/Nyk)T Ku)(\u03b4k \u22121yk/Nyk)\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\nso that \u03a3w\u03be(u) = \u03be(PKu) with\nP = 1\nN\nN\nX\nk=1\n(\u03b4k \u22121yk/Nyk)(\u03b4k \u22121yk/Nyk)T\n4One has (V \u22a5)\u22a5= V for finite-dimensional\u2014or more generally closed\u2014subspaces of H\n8.2. LINEAR DISCRIMINANT ANALYSIS\n185\nNote that one has\n\u2022\nN\nX\nk=1\n\u03b4k\u03b4T\nk = IdRN,\n\u2022\nN\nX\nk=1\n 1yk\nNyk\n!\n\u03b4T\nk =\nX\ng\u2208RY\n1g\nNg\nX\nk:yk=g\n\u03b4T\nk =\nX\ng\u2208RY\n1g1T\ng\nNg\n=\nN\nX\nk=1\n\u03b4k\n 1yk\nNyk\n!T\n,\n\u2022\nN\nX\nk=1\n 1yk\nNyk\n! 1yk\nNyk\n!T\n=\nX\ng\u2208RY\n1g1T\ng\nNg\n.\nThis shows that P can be expressed as\nP = 1\nN\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8edIdRN \u2212\nX\ng\u2208RY\n1g1T\ng /Ng\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8.\nWe have therefore proved that\n\u2022\n(\u03a3w + \u03c1IdH)\u03be(u) = \u03be ((PK + \u03c1IdRN)u)\n\u2022\n(\u03a3w + \u03c1IdH)\u22121\u03be( \u02dcu) = \u03be\n\u0010\n(PK + \u03c1IdRN)\u22121 \u02dcu\n\u0011\n.\nRecall that the feature-space LDA classification rule maximizes\n\u27e8h(x) \u2212\u00b5 , (\u03a3w + \u03c1IdH)\u22121(\u00b5g \u2212\u00b5)\u27e9H \u22121\n2\u27e8\u00b5g \u2212\u00b5 , (\u03a3w + \u03c1IdH)\u22121(\u00b5g \u2212\u00b5)\u27e9H + log\u03c0g .\nAll terms belong to V , except h(x), but this term can be replaced by its orthogonal\nprojection on V without changing the result. This projection can be made explicit\nin terms of the representation \u03be as follows. For x \u2208R, let \u03be(\u03c8(x)) denote the orthog-\nonal projection of h(x) on V (this defines the function \u03c8). If v(x) denotes the vector\nwith coordinates K(x,xk), k = 1,...,N, then \u03c8(x) = K\u22121v(x), as can be obtained by\nidentifying the inner products \u27e8h(x) , h(xk)\u27e9H and \u27e8\u03be(\u03c8(x)) , h(xk)\u27e9H.\nWe are now ready to rewrite the kernel LDA classification rule in terms of quan-\ntities that only involve K. We have\n\u27e8h(x) \u2212\u00b5 , (\u03a3w + \u03c1IdH)\u22121(\u00b5g \u2212\u00b5)\u27e9H\n= \u27e8\u03be(\u03c8(x) \u22121/N) , \u03be((PK + \u03c1IdRN)\u22121(1g/Ng \u22121/N))\u27e9H\n= (\u03c8(x) \u22121/N)T K(PK + \u03c1IdRN)\u22121(1g/Ng \u22121/N)\nGiven this, the classification rule must maximize\n(\u03c8(x) \u22121/N)T K(PK + \u03c1IdRN)\u22121(1g/Ng \u22121/N)\n\u22121\n2(1g/Ng \u22121/N)T K(PK + \u03c1IdRN)\u22121(1g/Ng \u22121/N) + log\u03c0g.\n(8.16)\n186\nCHAPTER 8. MODELS FOR LINEAR CLASSIFICATION\nDimension reduction\nNote that K(PK+\u03c1IdRN)\u22121 = K(KPK+\u03c1K)\u22121K is a symmet-\nric matrix. So, the expression in (8.16) can be written as\n(v(x) \u2212\u03bd)T R\u22121(\u03bdg \u2212\u03bd) \u22121\n2(\u03bdg \u2212\u03bd)T R\u22121(\u03bdg \u2212\u03bd) + log\u03c0g.\nwith R = KPK + \u03c1K, \u03bdg = K1g/Ng and \u00af\u03bd = K1/N. Clearly, if v1,...,vN are the column\nvectors K, we have\n\u03bdg = 1\nNg\nN\nX\nk=1\nvk1yk=g,\n\u00af\u03bd = 1\nN\nN\nX\nk=1\nvk .\nWe therefore retrieve an expression similar to finite-dimensional LDA, provided\nthat one replaces x by v(x), xk by vk and \u03a3w by R. Letting\nQ = 1\nN\nX\ng\u2208RY\nNg(\u03bdg \u2212\u00af\u03bd)(\u03bdg \u2212\u00af\u03bd)T\nbe the between-class covariance matrix, the discriminant directions are therefore\nsolutions of the generalized eigenvalue problem\nQfj = \u03bbjRfj\nwith f T\nj Rfj = 1 with R = (KPK + \u03c1K). Note that\nKPK = 1\nN\nN\nX\nk=1\n(vk \u2212\u00af\u03bdyk)(vk \u2212\u00af\u03bdyk)T\nis the within-class covariance matrix for the training data (v1,y1,...,vN,yN).\nThe following summarizes the kernel LDA classification algorithm.\nAlgorithm 8.4 (Kernel LDA)\n(1) Select a positive kernel K and a coefficient \u03c1 > 0.\n(2) Given T = (x1,y1,...,xN,yN) and a kernel K, compute the kernel matrix K =\nK(x1,...,xN) and the matrix R = KPK+\u03c1K. Let v1,...,vN be the column vectors of K.\n(3) Compute, for g \u2208RY,\n\u03bdg = 1\nNg\nN\nX\nk=1\nvk1yk=g ,\n\u00af\u03bd = 1\nN\nN\nX\nk=1\nvk\nand let Q = 1\nN\nP\ng\u2208RY Ng(\u03bdg \u2212\u00af\u03bd)(\u03bdg \u2212\u00af\u03bd)T .\n8.3. OPTIMAL SCORING\n187\n(4) Fix r0 \u2264q\u22121 and compute the eigenvectors f1,...,fr0 associated with the r0 largest\neigenvalues for the generalized eigenvalue problem Qf = \u03bbRf , normalized such that\nf T\nj Rfj = 1.\n(5) Compute the scores \u03b3jg = (\u03bdg \u2212\u00af\u03bd)T fj.\n(6) Given a new observation x, let v(x) be the vector with coordinates K(x,xk), k =\n1,...,N. Compute the scores \u03b3j(x) = (v(x) \u2212\u00af\u03bd)T fj, j = 1,...,r0. Classify x in the class\ng maximizing\nr\nX\ni=1\n\u03b3i(x)\u03b3ig \u22121\n2\nr\nX\ni=1\n\u03b32\nig + log\u03c0g.\n(8.17)\n8.3\nOptimal Scoring\nIt is possible to apply linear regression (chapter 7) to solve a classification problem\nby mapping the set RY to a collection of r-dimensional row vectors, or \u201cscores.\u201d\nThese scores (which have a different meaning from the LDA scores) will be repre-\nsented by a function \u03b8 : RY 7\u2192Rr. As an example, one can take r = q and\n\u03b8(g1) =\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\n1\n0\n0\n...\n0\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\n, \u03b8(g2) =\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\n0\n1\n0\n...\n0\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\n, ... , \u03b8(gq) =\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\n0\n0\n...\n0\n1\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\n.\nGiven a training set T = (x1,y1,...,xN,yN) and a score function \u03b8, a linear model can\nthen be estimated from data by minimizing\nN\nX\nk=1\n|\u03b8yk \u2212a0 \u2212bT xk|2\nwhere b is a d\u00d7q matrix and a0 \u2208Rq. Letting as before \u03b2 be the matrix with aT\n0 added\nas first row to b and X the matrix with first row containing only ones and subsequent\nrows given by xT\n1 ,...,xT\nN, one gets the least square estimator \u02c6\u03b2 = (X T X )\u22121X T Y, where\nY is the N \u00d7 q matrix of stacked \u03b8T\nyk row vectors.\nGiven an input vector x, the row vector \u02dcxT \u03b2 will generally not coincide with\none of the score vectors. Assignment to a class can then be made by minimizing\n|a0 + bT x \u2212\u03b8g| over all g in RY.\nSince the scores \u03b8 are free to choose, one may also try to optimize them, resulting\nin the optimal scoring algorithm. To describe it, we will need the notation already\n188\nCHAPTER 8. MODELS FOR LINEAR CLASSIFICATION\nintroduced for LDA, plus the following. We will write, for short, \u03b8j = \u03b8(gj) and\nintroduce the q \u00d7 r matrix \u0398 =\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\n\u03b8T\n1...\n\u03b8T\nq\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\n. We also denote by \u03c11,...,\u03c1r the column vectors\nof \u0398, so that \u0398 = [\u03c11,...,\u03c1r]. Let ugi, for i = 1,...,q, denote the q-dimensional vector\nwith ith coordinate equal to 1 and all others equal to 0. As before, Ng denote the class\nsizes, cg = Ng/N, C is the diagonal matrix with coefficients cg1,...,cgq and \u03b6 =\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\ncg1...\ncgq\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\n.\nThe goal of optimal scoring is to minimize, now with respect to \u03b8, a0 and b, the\nfunction\nF(\u03b8,a0,b) =\nN\nX\nk=1\n|\u03b8(yk) \u2212a0 \u2212bT xk|2 .\nSome normalizing conditions are clearly needed, because this problem is under-\nconstrained. (In the form above, the optimal choice is to take all free parameters\nequal to 0.) We now discuss the various indeterminacies and redundancies in the\nmodel,\n(a) If R is an r \u00d7 r orthogonal matrix, then F(R\u03b8,Ra0,bRT ) = F(\u03b8,a0,b), yielding an\ninfinity of possible equivalent solutions (that all lead to the same classification rule).\nThis implies that there is no loss of generality in assuming that \u0398T C\u0398 is diagonal\n(introducing C here will turn out to be convenient). Indeed, given any (\u03b8,a0,b), one\ncan just take R such that R\u0398T C\u0398RT is diagonal and replace \u0398 by R\u0398, a0 by Ra0 and\nb by bRT to get an equivalent solution satisfying the constraint.\n(b) Let D be an r by r diagonal matrix with positive entries. Replace \u03b8, a0 and b\nrespectively by D\u03b8, Da0 and bD. The resulting objective function is\nF(D\u03b8,Da0,bDT ) =\nN\nX\nk=1\n|D\u03b8(yk) \u2212Da0 \u2212DbT xk|2\n=\nr\nX\nj=1\nN\nX\nk=1\nd2\njj\n\u0012\n\u03b8(yk,j) \u2212a0(j) \u2212\nd\nX\ni=1\nb(i,j)xk(i)\n\u00132\nIf the coefficient djj is free to chose, then the objective function can always be re-\nduced by letting djj \u21920, which removes one of the dimensions in \u03b8. In order to\navoid this, one needs to fix the diagonal values of \u0398T C\u0398, and, by symmetry, it is\nnatural to require \u0398T C\u0398 = IdRr.\n(c) Given any \u03b4 \u2208Rr, one has F(\u03b8,a0,b) = F(\u03b8 \u2212\u03b4,a0 + \u03b4,b), with identical classifica-\ntion rule. One can therefore without loss of generality introduce r linear constraints,\n8.3. OPTIMAL SCORING\n189\nand a convenient choice is\n\u0398T \u03b6 =\nX\ng\u2208RY\ncg\u03b8g = 0.\nGiven this reduction, we can now describe the optimal scoring problem as the\nminimization of\nN\nX\nk=1\n|\u03b8yk \u2212a0 \u2212bT xk|2\nsubject to \u0398T C\u0398 = IdRr and \u0398T \u03b6 = 0.\nThe optimal a0 is given by\n\u02c6a0 = 1\nN\nN\nX\nk=1\n\u03b8yk \u2212bT \u00b5 = \u2212bT \u00b5,\nso that the problem is reduced to minimizing\nN\nX\nk=1\n|\u03b8yk \u2212bT (xk \u2212\u00b5)|2\nsubject to the same constraints. Using the facts that \u03b8yk = \u0398T uyk, that\nN\nX\nk=1\nuykuT\nyk =\nX\ng\u2208RY\nNguguT\ng = NC\nand that\nN\nX\nk=1\nuyk(xk \u2212\u00b5)T =\nN\nX\ng\u2208RY\nug\nX\nk:yk=g\n(xk \u2212\u00b5)T =\nN\nX\ng\u2208RY\nugNg(\u00b5g \u2212\u00b5)T = NCM,\none can write\nN\nX\nk=1\n|\u03b8yk \u2212bT (xk \u2212\u00b5)|2 =\nN\nX\nk=1\n|\u0398T uyk \u2212bT (xk \u2212\u00b5)|2\n=\nN\nX\nk=1\nuT\nyk\u0398\u0398T uyk \u22122\nN\nX\nk=1\n(xk \u2212\u00b5)T b\u0398T uyk +\nN\nX\nk=1\n(xk \u2212\u00b5)T bbT (xk \u2212\u00b5)\n=\nN\nX\nk=1\ntrace(\u0398T uykuT\nyk\u0398) \u22122\nN\nX\nk=1\ntrace(\u0398T uyk(xk \u2212\u00b5)T b)\n+\nN\nX\nk=1\ntrace(bT (xk \u2212\u00b5)(xk \u2212\u00b5)T b)\n= Ntrace(\u0398T C\u0398) \u22122Ntrace(\u0398T CMb) + Ntrace(bT \u03a3XXb).\n190\nCHAPTER 8. MODELS FOR LINEAR CLASSIFICATION\nNote that, since \u0398T C\u0398 = IdRr, then trace(\u0398T C\u0398) = r. We therefore obtain a\nconcise form of the optimal scoring problem: minimize\n\u22122trace(\u0398T CMb) + trace(bT \u03a3XXb).\nsubject to \u0398T C\u0398 = IdRr and \u0398T \u03b6 = 0.\nGiven \u0398, the optimal b is \u03a3\u22121\nXXMT C\u0398, and replacing it in the objective function,\none finds that \u0398 must minimize\n\u22122trace(\u0398T CM\u03a3\u22121\nXXMT C\u0398) + trace(\u0398T CM\u03a3\u22121\nXXMT C\u0398)\ni.e., maximize\ntrace(\u0398T CM\u03a3\u22121\nXXMT C\u0398)\nsubject to \u0398T C\u0398 = IdRr and \u0398T \u03b6 = 0. We now recall the following linear algebra\nresult (see chapter 2).\nProposition 8.7 Let A and B be respectively positive definite and non-negative semi-\ndefinite symmetric q by q matrices. Then, the maximum, over all q by r matrices S such\nthat trace(ST AS) = IdRr, of trace(ST BS) is attained at S = [\u03c31,...,\u03c3r], where the columns\nvectors \u03c31,...,\u03c3r are the solutions of the generalized eigenvalue problem\nB\u03c3 = \u03bbA\u03c3\nassociated with the largest eigenvalues, normalized so that \u03c3T\ni A\u03c3i = 1 for i = 1,...,r..\nGiven this proposition, let \u03c11,...,\u03c1r be the r first eigenvectors for the problem\nCM\u03a3\u22121\nXXMT C\u03c1 = \u03bbC\u03c1.\n(8.18)\nAssume that r is small enough so that the associated eigenvalues are not zero. Let\n\u0398 = [\u03c11,...,\u03c1r]. We now prove that \u0398 is indeed a solution of the optimal scoring\nproblem, and the only point to show to complete the statement is that this \u0398 satisfies\nthe constraints \u0398T \u03b6 = 0. But we have\nMT C1q =\nX\ng\ncg(\u00b5g \u2212\u00af\u00b5) = 0,\nwhich implies that 1q is a solution of the generalized eigenvalue problem associated\nwith \u03bb = 0. This in turn implies that 1T\nq C\u03c1i = \u03b6T \u03c1i = 0, which is exactly \u0398T \u03b6 = 0.\nTo summarize, we have found that the solution \u03b8,b minimizing\n\u22122trace(\u0398T CMb) + trace(bT \u03a3XXb)\nsubject to \u0398T C\u0398 = IdRr and \u0398T \u03b6 = 0 is given by\n8.3. OPTIMAL SCORING\n191\n(i) \u0398 = [\u03c11,...,\u03c1r] where \u03c11,...,\u03c1r are the eigenvectors for the problem\nCM\u03a3\u22121\nXXMT C\u03c1 = \u03bbC\u03c1\nassociated with the r largest eigenvalues, normalized so that \u03c1T C\u03c1 = 1.\n(ii) b = \u03a3\u22121\nXXMT C\u0398.\nThe computation can, however, be further simplified. Let \u03bb1,...,\u03bbr be the eigen-\nvalues associated with \u03c11,...,\u03c1r. Letting D be the associated diagonal matrix, one\ncan write\nCM\u03a3\u22121\nXXMT C\u0398 = C\u0398D.\nThis yields\n\u0398 = M\u03a3\u22121\nXXMT C\u0398D\u22121 = MbD\u22121,\nfrom which we deduce that \u03b8g = \u0398T ug = D\u22121bT (\u00b5g \u2212\u00af\u00b5). So, given a new input vector\nx, the decision rule is to assign it to the class g for which\n|\u03b8g \u2212bT (x \u2212\u00af\u00b5)|2 = |\u0398T ug \u2212bT (x \u2212\u00af\u00b5)|2 = |D\u22121bT (\u00b5g \u2212\u00af\u00b5) \u2212bT (x \u2212\u00af\u00b5)|2\nis minimal. Letting b1,...,br denote the r columns of b, this is equivalent to mini-\nmizing, in g\nr\nX\nj=1\n(bT\nj (\u00b5g \u2212\u00af\u00b5))2/\u03bb2\nj \u22122\nr\nX\nj=1\n(bT\nj (x \u2212\u00af\u00b5))(bT\nj (\u00b5g \u2212\u00af\u00b5))/\u03bbj.\n(8.19)\nFrom b = \u03a3\u22121\nXXMT C\u0398 and \u0398 = MbD\u22121 we see that\nbD = \u03a3\u22121\nXXMT CMb,\nso that \u03a3bb = \u03a3XXbD. This shows that the columns of b are solution of the eigenvalue\nproblem \u03a3bu = \u03bb\u03a3XXu. Moreover, from \u0398T C\u0398 = IdRr, we get bT \u03a3bb = D2. Since\nbT \u03a3bb = bT \u03a3XXbD, we get that b must be normalized to that bT \u03a3XXb = D.\nThis shows that the solution of the optimal scoring problem can be reformulated\nuniquely in terms of b: if b1,...,br are the r principal solutions of the eigenvalue\nproblem \u03a3bu = \u03bb\u03a3XXu, normalized so that uT \u03a3XXu = \u03bb, a new input is classified\ninto the class g minimizing\nr\nX\nj=1\n\u03b3j(\u00b5g)2/\u03bb2\nj \u22122\nr\nX\nj=1\n\u03b3j(x)\u03b3j(\u00b5g)/\u03bbj.\nwith \u03b3j(x) = bT\nj (x \u2212\u00af\u00b5).\n192\nCHAPTER 8. MODELS FOR LINEAR CLASSIFICATION\nRemark 8.8 The following computation shows that optimal scoring is closely re-\nlated to LDA. Recall the identity \u03a3XX = \u03a3w + \u03a3b. It implies that a solution of \u03a3bu =\n\u03bb\u03a3XXu is also a solution of \u03a3bu = \u02dc\u03bb\u03a3wu with \u02dc\u03bb = \u03bb/(1 \u2212\u03bb). If uT \u03a3XXu = \u03bb, then\nuT \u03a3wu = \u03bb \u2212uT \u03a3bu = \u03bb \u2212\u03bb2 =\n\u02dc\u03bb\n(1 + \u02dc\u03bb)2,\nwhich shows that\n\u02dcu = 1 + \u02dc\u03bb\n\u221a\u02dc\u03bb\nu\nsatisfies \u02dcuT \u03a3w \u02dcu = 1. So,\nej =\n1 + \u02dc\u03bbj\nq\n\u02dc\u03bbj\nbj\ncoincide with the LDA directions. We have, letting \u02dc\u03b3j(x) = eT\nj (x \u2212\u00af\u00b5) =\nq\n\u02dc\u03bbj\u03b3j(x)/(1 +\n\u02dc\u03bbj):\nr\nX\nj=1\n\u03b3j(\u00b5g)2/\u03bb2\nj \u22122\nr\nX\nj=1\n\u03b3j(x)\u03b3j(\u00b5g)/\u03bbj =\nr\nX\nj=1\n\u02dc\u03b3j(\u00b5g)2/ \u02dc\u03bbj \u22122\nr\nX\nj=1\n\u03b3j(x)\u03b3j(\u00b5g)/(1 + \u02dc\u03bbj)\nwhich relates the classification rules for the two methods.\n\u2666\nRemark 8.9 Optimal scoring can be modified by adding a penalty in the form\n\u03b3\nr\nX\ni=1\nbT\ni \u2126bi = \u03b3trace(bT \u2126b)\n(8.20)\nwhere \u2126is a weight matrix. This only modifies the previous discussion by adding\n\u03b3\u2126/N to both \u03a3XX and \u03a3w.\n\u2666\n8.3.1\nKernel optimal scoring\nLet h : RX \u2192H be the feature function and K the associated kernel, as usual. Opti-\nmal scoring in feature space requires to minimize\nN\nX\nk=1\n|\u03b8yk \u2212a0 \u2212b(h(xk))|2 + \u03b3\u2225b\u22252\nH,\n8.3. OPTIMAL SCORING\n193\nwhere we have introduced a penalty on b. Here, b is a linear operator from H to Rr,\ntherefore taking the form\nb(h) =\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\n\u27e8b1 , h\u27e9H\n...\n\u27e8br , h\u27e9H\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\nwith b1,...,br \u2208H, and we take\n\u2225b\u22252\nH =\nr\nX\ni=1\n\u2225bi\u22252\nH.\nIt is once again clear (and the argument is left to the reader) that the problem\ncan be reduced to the finite dimensional space V = span(h(x1),...,h(xN)), and that\nthe optimal b1,...,br must take the form\nbj =\nN\nX\nl=1\n\u03b1lih(xl).\nIntroduce the kernel matrix K = K(x1,...,xN) with kth column denoted K(k). Let \u03b1\nbe the N by r matrix with entries \u03b1kj, k = 1,...,N,j = 1,...,r. Then b(h(xk)), which is\nthe vector with coordinates\n\u27e8bj , h(xk)\u27e9=\nN\nX\nl=1\n\u03b1liK(xk,xl),\nis equal to \u03b1T K(k). Moreover\n\u2225b\u22252\nH =\nr\nX\nj=1\nN\nX\nk,l=1\n\u03b1kjK(xk,xl)\u03b1lj = trace(\u03b1T K\u03b1).\nWe therefore need to minimize\nN\nX\nk=1\n|\u03b8yk \u2212a0 \u2212\u03b1T K(k)|2 + \u03b3trace(\u03b1T K\u03b1),\nso that the problem is reduced to penalized optimal scoring, with xk replaced by K(k),\nb replaced by \u03b1 and the matrix \u2126in (8.20) replaced by K. Introducing the matrix P =\nIdRN \u221211T /N and Kc = PK, the covariance matrix \u03a3XX becomes KT\nc Kc/N = KPK/N.\nThe class averages \u00b5g are equal to K1(g)/Ng while \u00b5 = K1/N, so that the matrix\nM is equal to\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\n1(g1)T /Ng1 \u22121T /N\n...\n1(gq)T /Ngq \u22121T /N\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\nK\n194\nCHAPTER 8. MODELS FOR LINEAR CLASSIFICATION\nwhich gives \u03a3b = MT CM = KQK, where Q is\nQ = PCP =\nX\ng\u2208RY\nNg\nN\n 1(g)\nNg\n\u22121\nN\n! 1(g)\nNg\n\u22121\nN\n!T\nSo, the columns of \u03b1 are the r principal eigenvectors \u03c11,...,\u03c1r of the problem\nKQK\u03c1 = 1\nN (KPK + \u03b3K)\u03c1.\nGiven \u03b1, one then has, for any x \u2208Rd,\n\u27e8bi , h(x)\u27e9H =\nN\nX\nk=1\n\u03b1kiK(x,xk)\nand\na0\n(i) = 1\nN\nN\nX\nk,l=1\n\u03b1kiK(xl,xk).\n8.4\nSeparating hyperplanes and SVMs\n8.4.1\nOne-layer perceptron and margin\nIn this whole section, we restrict to two-class problems, and let RY = {\u22121,1}. Given\na0 \u2208R and b , 0 \u2208Rd, the equation a0 + bT x = 0 defines a hyperplane in Rd. The\nfunction f (x) = sign(a0 + xT b) defines a classifier that attributes a class \u00b11 to x ac-\ncording to which side of the hyperplane it belongs (we ignore the ambiguity when\nx is on the hyperplane). With this notation, a pair (x,y), where y is the true class, is\ncorrectly classified if and only if y(a0 + xT b) > 0.\nLet T = (x1,y1),...,(xN,yN) denote, as usual, the training data. A hyperplane,\nrepresented by the parameters (a0,b) is separating for T if it correctly classifies all its\nsamples, i.e., if yk(a0 + xT\nk b) > 0 for k = 1,...,N. If such a hyperplane exists, one says\nthat T is linearly separable.\nThe perceptron algorithm computes a0 and b by minimizing\nL(\u03b2) =\nN\nX\nk=1\n[yk(a0 + xT\nk b)]\u2212\n8.4. SEPARATING HYPERPLANES AND SVMS\n195\nwith u\u2212= max(0,\u2212u), , or more precisely, fixing a small positive number \u03b4:\nL(\u03b2) =\nN\nX\nk=1\n[\u03b4 \u2212yk(a0 + xT\nk b)]+ .\nThe problem can be recast as a linear program, i.e., minimize\nN\nX\nk=1\n\u03bek\nsubject to \u03bek \u22650,\u03bek + yk(a0 + xT\nk b) \u2212\u03b4 \u22650 for k = 1,...,N.\nHowever, when T is linearly separable, separating hyperplanes are not uniquely\ndefined, and there is in general (depending on the choice made for \u03b4) an infinity\nof solutions to the perceptron problem. Intuitively, one should prefers a solution\nthat classifies the training data with some large margin, rather than one for which\ntraining points may be very close to the separating boundary (see fig. 8.2).\nFigure 8.2: The green line is preferable to the purple one in order to separate the data.\nThis leads to the maximum margin separating hyperplane classifier, also called\nlinear SVM, introduced by Vapnik and Chervonenkis [196, 197].\n8.4.2\nMaximizing the margin\nWe will use the following result.\nProposition 8.10 The distance of a point x \u2208Rd to the hyperplane M : a0 + bT x = 0 is\ngiven by |a0 + xT b|/|b|.\n196\nCHAPTER 8. MODELS FOR LINEAR CLASSIFICATION\nProof By definition, distance(x,M) = |x \u2212\u03c0M(x)| where \u03c0M is the orthogonal projec-\ntion on M. Since b is normal to M and letting h = \u03c0M(x), we have x = \u03bbb + h so that\n|\u03bbb| = distance(x,M). Writing a0 +bT h = 0 in this equation implies a0 +bT x = \u03bb|b|2 so\nthat |\u03bb||b| = |a0 + xT b|/|b|.\n\u25a0\nAssume that T is linearly separable and let M : a0 + bT x = 0 be a separating\nhyperplane. The classification margin is defined as the minimal distance of the input\nvectors x1,...,xN to this hyperplane, i.e.,\nm(a0,b) = min{|a0 + xT\nk b|/|b| : k = 1,...,N}.\nBecause the hyperplane is separating, we have yk(a0 + xT\nk b) = |a0 + xT\nk b| for all k, so\nthat we also have\nm(a0,b) = min{yk(a0 + xT\nk b)/|b| : k = 1,...,N}.\nWe want to maximize this margin among all separating hyperplanes. This can be\nexpressed as maximizing, with respect to a0,b, the quantity\nmin{yk(a0 + xT\nk b)/|b| : k = 1,...,N}\nsubject to the constraint that the hyperplane is separating, namely\nyk(a0 + xT\nk b) \u22650, k = 1,...,N.\nIntroducing a new variable C representing the margin, the previous problem is\nequivalent to maximizing C subject to\nyk(a0 + xT\nk b) \u2265C|b|, k = 1,...,N.\nThe problem is now overparametrized, and there is no loss of generality in en-\nforcing the additional constraint C|b| = 1. Noting that maximizing C is the same as\nminimizing |b|2, we can now reformulate the maximum margin hyperplane problem\nas minimizing |b|2/2 subject to\nyk(a0 + xT\nk b) \u22651, k = 1,...,N,\nwith C (the margin) given by C = 1/|b|. This results in a quadratic programming\nproblem.\nIf the data is not separable, there is no feasible point for this problem. To also\naccount for this situation (which is common), we can replace the constraint by a\npenalty and minimize, with respect to a0 and b:\n|b|2\n2 + \u03b3\nN\nX\nk=1\n(1 \u2212yk(a0 + xT\nk b))+\n8.4. SEPARATING HYPERPLANES AND SVMS\n197\nfor some \u03b3 > 0. (Recall that x+ = max(x,0).) This is equivalent to minimizing the\nperceptron objective function, with \u03b4 = 1, and with an additional penalty term equal\nto |b|2/(2\u03b3). This minimization problem is equivalent to a quadratic programming\nproblem obtained by introducing slack variables \u03bek, k = 1,...,N and minimizing\n1\n2|b|2 + \u03b3\nN\nX\nk=1\n\u03bek ,\nsubject to the constraints \u03bek \u22650, yk(a0 + xT\nk b) + \u03bek \u22651, for k = 1,...,N.\n8.4.3\nKKT conditions and dual problem\nIntroduce Lagrange multipliers \u03b7k \u22650 for \u03bek \u22650 and \u03b1k \u22650 for yk(a0 + xT\nk b) + \u03bek \u22651.\nThe Lagrangian is then given by\nL = 1\n2|b|2 + \u03b3\nN\nX\nk=1\n\u03bek \u2212\nN\nX\nk=1\n\u03b7k\u03bek \u2212\nN\nX\nk=1\n\u03b1k\n\u0010\nyk(a0 + xT\nk b) + \u03bek \u22121\n\u0011\n.\nThe KKT conditions are\n\uf8f1\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f2\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f3\nb \u2212\nN\nX\nk=1\n\u03b1kykxk = 0\nN\nX\nk=1\n\u03b1kyk = 0\n\u03b3 \u2212\u03b7k \u2212\u03b1k = 0,\nk = 1,...,N\n\u03bek\u03b7k = 0,\nk = 1,...,N\n\u03b1k\n\u0010\nyk(a0 + xT\nk b) + \u03bek \u22121\n\u0011\n= 0,\nk = 1,...,N\n(8.21)\nMinimizing L with respect to a0, b and \u03be1,...,\u03beN and ensuring that the minimum\nis finite provides the first three KKT conditions. The resulting dual formulation\ntherefore requires to maximize\nN\nX\nk=1\n\u03b1k \u22121\n2\nN\nX\nk,l=1\n\u03b1k\u03b1lykylxT\nk xl\nsubject to the constraints 0 \u2264\u03b1k \u2264\u03b3, PN\nk=1 \u03b1kyk = 0.\nWe now discuss the consequences of the complementary slackness conditions\nbased on the position of training sample relative to the separating hyperplane.\n198\nCHAPTER 8. MODELS FOR LINEAR CLASSIFICATION\n(i) First consider indices k such that (xk,yk) is correctly classified beyond the margin,\ni.e., yk(a0+xT\nk b) > 1. The last KKT condition and the constraint \u03bek \u22650 require \u03b1k = 0,\nand the third one then gives \u03bek = 0.\n(ii) For samples that are misclassified or correctly classified below the margin 5, i,e.,\nyk(a0 +xT\nk b) < 1, the constraint yk(a0 +xT\nk b)+\u03bek \u22651 implies \u03bek > 0, so that \u03b1k = \u03b3 and\nyk(a0 + xT\nk b) + \u03bek = 1.\n(iii) If (xk,yk) is correctly classified exactly at the margin, then \u03bek = 0 and there is\nno constrain on \u03b1k beside belonging to [0,\u03b3]. Training samples that lie exactly at the\nmargin are called support vectors.\nGiven a solution \u03b11,...,\u03b1N of the dual problem, one immediately recovers b via\nthe first equation in (8.21). For a0, one must, similarly to the regression case, rely on\nsupport vectors, which can be identified when 0 < \u03b1k < \u03b3. In this case, one can take\na0 = yk \u2212xT\nk b.\nIf no support vector is found, then a0 is not uniquely determined, and can be any\nvalue such that yk(a0 + bT xk) \u22651 if \u03b1k = 0 and yk(a0 + bT xk) \u22641 if \u03b1k = \u03b3. This shows\nthat a0 can be any point in the interval [\u03b2\u2212\n0,\u03b2+\n0 ] with\na0\n\u2212= max{yk \u2212xT\nk b : (yk = 1 and \u03b1k = 0) or (yk = \u22121 and \u03b1k = \u03b3)}\na0\n+ = min{yk \u2212xT\nk b : (yk = \u22121 and \u03b1k = 0) or (yk = 1 and \u03b1k = \u03b3)}.\n8.4.4\nKernel version\nWe make the usual assumptions: h : RX \u2192H is a feature map with values in an\ninner-product space with K(x,y) = \u27e8h(x) , h(y)\u27e9H. The predictors take the form f (x) =\nsign(a0 + \u27e8b , h(x)\u27e9H), a0 \u2208R and b \u2208H, and the goal is to minimize\n1\n2\u2225b\u22252\nH + \u03b3\nN\nX\nk=1\n\u03bek ,\nsubject to \u03bek \u22650, yk(a0 + \u27e8h(xk) , b\u27e9H) + \u03bek \u22651, k = 1,...,N.\nLet V = span(h(x1),...,h(xN)). The usual projection argument implies that the\noptimal b must belong to V and therefore take the form\nb =\nN\nX\nk=1\nukh(xk).\n5Note that, even if the training data is linearly separable, there are generally samples that are on\nthe right side of the hyperplane, but at a distance to the hyperplane strictly lower that the \u201cnominal\nmargin\u201d C = 1/|b|. This is due to our relaxation of the original problem of finding a separating\nhyperplane with maximal margin.\n8.4. SEPARATING HYPERPLANES AND SVMS\n199\nWe therefore need to minimize\n1\n2\nN\nX\nk,l=1\nukulK(xk,xl) + \u03b3\nN\nX\nk=1\n\u03bek ,\nsubject to\nyk\n\u0012\na0 +\nN\nX\nl=1\nK(xk,xl)al\n\u0013\n+ \u03bek \u22651\nfor k = 1,...,N. Introducing the same Lagrange multipliers as before, the Lagrangian\nis\nL = 1\n2\nN\nX\nk,l=1\nukulK(xk,xl) + \u03b3\nN\nX\nk=1\n\u03bek\n\u2212\nN\nX\nk=1\n\u03b7k\u03bek \u2212\nN\nX\nk=1\n\u03b1k\n\u0012\nyk\n\u0012\na0 +\nN\nX\nl=1\nK(xk,xl)ul\n\u0013\n+ \u03bek \u22121\n\u0013\n.\nUsing vector notation, we have\nL = 1\n2uT Ku + \u03beT (\u03b31 \u2212\u03b7 \u2212\u03b1) \u2212a0\u03b1T y \u2212(\u03b1 \u2299y)T Ku + \u03b1T1\nwhere y \u2299\u03b1 is the vector with coordinates yk\u03b1k. The infimum of L is \u2212\u221eunless\n\u03b31\u2212\u03b7 \u2212\u03b1 = 0 and \u03b1T y = 0. If these identities are true, then the optimal u is u = \u03b1 \u2299y\nand the minimum of L is\n\u22121\n2(\u03b1 \u2299y)T K(\u03b1 \u2299y) + \u03b1T1\nThe dual problem therefore requires to minimize\n1\n2(\u03b1 \u2299y)T K(\u03b1 \u2299y) \u2212\u03b1T1 = \u03b1T (K \u2299yyT )\u03b1 \u2212\u03b1T1\nsubject to \u03b31 \u2212\u03b7 \u2212\u03b1 = 0 and \u03b1T y = 0.\nThis is exactly the same problem as the one we obtained in the linear case, up\nto the replacement of the Euclidean inner products xT\nk xl by the kernel evaluations\nK(xk,xl). Given the solution of the dual problem, the optimal b is\nb =\nX\nk\nukh(xk) =\nN\nX\nk=1\n\u03b1kykh(xk).\n200\nCHAPTER 8. MODELS FOR LINEAR CLASSIFICATION\nIt is no computable, but the classification rule is explicit and given by\nf (x) = sign\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8eda0 +\nN\nX\nk=1\n\u03b1kykK(xk,x)\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8.\nSimilarly to the linear case, the coefficient a0 can be identified using a support\nvector, or is otherwise not uniquely determined. More precisely, if one of the \u03b1k\u2019s is\nstrictly between 0 and \u03b3, then a0 is given by a0 = yk \u2212P\nl \u03b1lylK(xk,xl). Otherwise, a0\nis any number between\n\u03b2\u2212\n0 = max\n\uf8f1\uf8f4\uf8f4\uf8f2\uf8f4\uf8f4\uf8f3yk \u2212\nX\nl\n\u03b1lylK(xk,xl) : (yk = 1 and \u03b1k = 0) or (yk = \u22121 and \u03b1k = \u03b3)\n\uf8fc\uf8f4\uf8f4\uf8fd\uf8f4\uf8f4\uf8fe\nand\n\u03b2+\n0 = min\n\uf8f1\uf8f4\uf8f4\uf8f2\uf8f4\uf8f4\uf8f3yk \u2212\nX\nl\n\u03b1lylK(xk,xl) : (yk = \u22121 and \u03b1k = 0) or (yk = 1 and \u03b1k = \u03b3)\n\uf8fc\uf8f4\uf8f4\uf8fd\uf8f4\uf8f4\uf8fe.\nChapter 9\nNearest-Neighbor Methods\nUnlike linear models, nearest-neighbor methods are completely non-parametric and\nassume no regularity on the decision rule or the regression function. In their sim-\nplest version, they require no training and rely on the proximity of a new observa-\ntion to those that belong to the training set. We will discuss in this chapter how\nthese methods are used for regression and classification, and study some of their\ntheoretical properties.\n9.1\nNearest neighbors for regression\n9.1.1\nConsistency\nWe let RX denote the input space, and RY = Rq be the output space. We assume that\na distance, denoted dist is defined on RX. This means that dist : RX \u00d7 RX \u2192[0,+\u221e]\n(we allow for infinite values) is a symmetric function such that dist(x,x\u2032) = 0 if and\nonly if x = x\u2032 and, for all x,x\u2032,x\u2032\u2032 \u2208RX\ndist(x,x\u2032) \u2264dist(x,x\u2032\u2032) + dist(x\u2032\u2032,x\u2032),\nwhich is the triangle inequality.\nLet T = (x1,y1,...,xN,yN) be the training set. For x \u2208RX, let\nDT (x) = (dist(x,xk),k = 1,...,N)\nbe the collection of all distances between x and the inputs in the training set. We\nconsider regression estimators taking the form\n\u02c6f (x) =\nN\nX\nk=1\nWk(x)yk\n(9.1)\n201\n202\nCHAPTER 9. NEAREST-NEIGHBOR METHODS\nwhere W1(x),...,WN(x) is a family of coefficients, or weights, that only depends on\nDT (x).\nWe will, more precisely, use the following construction [183]. Assume that a\nfamily of numbers w1 \u2265w2 \u2265\u00b7\u00b7\u00b7 \u2265wN \u22650 is chosen, with PN\nj=1 wj = 1. Given x \u2208Rd\nand k \u2208{1,...,N}, we let r+\nk (x) denote the number of indexes k\u2032 such that dist(x,xk\u2032) \u2264\ndist(x,xk) and r\u2212\nk (x) the number of such indexes such that d(x,xk\u2032) < d(x,xk). The\ncoefficients defining \u02c6f in (9.1) are then chosen as:\nWk(x) =\nPr+\nk (x)\nk\u2032=r\u2212\nk (x)+1 wk\u2032\nr+\nk (x) \u2212r\u2212\nk (x) .\n(9.2)\nTo emphasize the role of (w1,...,wN) is this definition, we will denote the resulting\nestimation as \u02c6fw. If there is no tie in the sequence of distances between x and ele-\nments of the training set, then r+\nk (x) = r\u2212\nk (x)+1 is the rank of xk when training data is\nordered according to their proximity to x, and Wk(x) = wr+\nk (x). In this case, defining\nl1,...,lN such that d(x,xl1) < \u00b7\u00b7\u00b7 < d(x,xlN), we have\n\u02c6fw(x) =\nN\nX\nj=1\nwjylj.\nIn the general case, the weights wj associated with tied observations are averaged.\nIf p is an integer, the p-nearest neighbor (p-NN) estimator (that we will denote\n\u02c6fp) is associated to the weights wj = 1/p for j = 1,...,p and 0 otherwise. If there is no\ntie for the definition of the pth nearest neighbor of x, Wk(x) = 1/p if k is among the\np nearest-neighbors and Wk(x) = 0 otherwise, so that \u02c6fp is the average of the output\nvalues over these p nearest neighbors. If the pth nearest neighbors are tied, their\noutput value is averaged before being used in the sum. For example, assume that\nN = 5 and p = 2 and let the distances between x and xk for k = 1,...,5 be respectively\n9,3,2,4,6. Then \u02c6f2(x) = (y2 + y3)/2. If the distances were 9,3,2,3,6, then we would\nhave \u02c6f2(x) = (y2 + y4)/4 + y3/2.\nWhen RX = Rd and d(x,x\u2032) = |x \u2212x\u2032|, the following result is true.\nTheorem 9.1 ([183]) Assume that E(Y 2) < \u221e. Assume that, for each N, a sequence\nw(N) = w(N)\n1\n\u2265\u00b7\u00b7\u00b7 \u2265w(N)\nN\n\u22650 is chosen with PN\nj=1 w(N)\nj\n= 1. Assume, in addition, that\n(i) limN\u2192\u221ew(N)\n1\n= 0\n(ii) limN\u2192\u221e\nP\nj\u2265\u03b1N w(N)\nj\n\u21920, for some \u03b1 \u2208(0,1).\n9.1. NEAREST NEIGHBORS FOR REGRESSION\n203\nThen the corresponding classifier \u02c6fw(N) converges in the L2 norm to E(Y | X):\nE\n\u0010\n| \u02c6fw(N)(X) \u2212E(Y | X)|2\u0011\n\u21920.\nFor nearest-neighbor regression, (i) and (ii) mean that the number of nearest neighbors\npN must be chosen such that pN \u2192\u221eand pN/N \u21920.\nProof We give a proof under the assumption that f : x 7\u2192E(Y | X = x) is uniformly\ncontinuous and bounded (one can, in fact, prove that it is always possible to reduce\nto this case).\nTo lighten the notation, we will not make explicit the dependency on N in of\nquantities such as W or w. One has\n\u02c6fw(X) \u2212E(Y | X) =\nN\nX\nk=1\nWk(X)(f (Xk) \u2212f (X)) +\nN\nX\nk=1\nWk(X)(Yk \u2212f (Xk))\n(9.3)\nand the two sums can be addressed separately.\nWe start with the first sum and write, by Schwartz\u2019s inequality:\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\nX\nk\nWk(X)(f (Xk) \u2212f (X))\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\n2\n\u2264\nX\nk\nWk(X)(f (Xk) \u2212f (X))2.\nIt therefore suffices to study the limit of E(P\nk Wk(X)(f (Xk) \u2212f (X))2. Fix \u03f5 > 0. By\nassumption, there exists M,a > 0 such that |f (x)| \u2264M for all x and |x \u2212y| \u2264a \u21d2\n|f (x) \u2212f (y)|2 \u2264\u03f5. Then\nE\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\nX\nk\nWk(X)(f (Xk) \u2212f (X))2\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8=E\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\nX\nk\nWk(X)(f (Xk) \u2212f (X))2 1|Xk\u2212X|\u2264a\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\n+ E\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\nX\nk\nWk(X)(f (Xk) \u2212f (X))2 1|Xk\u2212X|>a\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\n\u2264\u03f52 + 4M2E\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\nX\nk\nWk(X)1|Xk\u2212X|>a\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8.\nSince \u03f5 can be made arbitrarily small, we need to show that, for any positive a, the\nsecond term in the upper-bound tends to 0 when N \u2192\u221e. We will use the follow-\ning fact, which requires some minor measure theory argument to prove rigorously.\nDefine\nS = {x : \u2200\u03b4 > 0,P(|X \u2212x| < \u03b4) > 0}.\n204\nCHAPTER 9. NEAREST-NEIGHBOR METHODS\nThis set is called the support of X. Then, one can show that P(X \u2208S) = 1. This\nmeans that, if \u02dcX is independent from X with the same distribution, then, for any\n\u03b4 > 0, P(|X \u2212\u02dcX| < \u03b4|X) > 0 with probability one. 1\nLet Na(x) = |{k : |Xk \u2212x| \u2264a}|. We have, for all x \u2208S and a > 0, and using the law\nof large numbers,\nNa(x)\nN\n= 1\nN\nN\nX\nk=1\n1|Xk\u2212x|\u2264a \u2192P(|X \u2212x| \u2264a) > 0.\nIf |X \u2212Xk| > a, then r\u2212\nk (X) > Na(x) so that\nX\nk\nWk(X)1|Xk\u2212X|>a \u2264\nX\nj\u2265Na(X)\nwj,\nand we have, taking 0 < \u03b1 < P(|X \u2212x| \u2264a),\nE\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\nX\nj\u2265Na(X)\nwj\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\u2264\nX\nj\u2265\u03b1N\nwj + P(Na(X) < \u03b1N)\nand both terms in the upper bound converge to 0. This shows that the first sum in\n(9.3) tends to 0.\nWe now consider the second sum in (9.3). Let Zk = Yk \u2212E(Y | Xk). We have\nE(Zk | Xk) = 0 and E(Z2\nk ) < \u221e. We can write\nE\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\n\f\f\f\f\f\f\f\nN\nX\nk=1\nWk(X)Zk\n\f\f\f\f\f\f\f\n2\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\n= E\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8edE\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\n\f\f\f\f\f\f\f\nN\nX\nk=1\nWk(X)Zk\n\f\f\f\f\f\f\f\n2 \f\f\f\fX,X1,...,XN\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\n= E\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\nN\nX\nk=1\nWk(X)2E(Z2\nk | Xk)\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\n+\nN\nX\nk,l=1\nE(Wk(X)Wl(X)E(ZkZl | Xi,Xj))\n1This statement is proved as follows (with the assumption that X is Borel measurable). Let Sc\ndenote the complement of S. Then Sc is open. Indeed if x < S, there exists \u03b4x > 0 such that, letting\nB(x,\u03b4x) denote the open ball with radius \u03b4x, P(X \u2208B(x,\u03b4x)) = 0. Then P(X \u2208B(x\u2032,\u03b4x/3)) = 0 as soon as\n|x \u2212x\u2032| < \u03b4x/3, so that B(x,\u03b4x/3) \u2282Sc.\nIf K \u2282Sc is compact, then K \u2282S\nx\u2208K B(x,\u03b4x) and one can find a finite subset M \u2282K such that\nK \u2282S\nx\u2208M B(x,\u03b4x), which proves that P(X \u2208K) = 0. Since P(X \u2208Sc) = maxK P(X \u2208K) where the\nmaximum is over all compact subsets of Sc, we find P(X \u2208Sc) = 0 as required.\n9.1. NEAREST NEIGHBORS FOR REGRESSION\n205\nThe cross products in the last term vanish because E(Zk | Xk) = 0 and the samples\nare independent. So it only remains to consider\nE\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\nN\nX\nk=1\nWk(X)2E(Z2\nk | Xk)\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\nThe random variable E(Zk | Xk) = E(Y 2\nk | Xk) \u2212E(Yk | Xk)2 is a fixed non-negative\nfunction of Xk, that we will denote h(Xk). We have\nE\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\nN\nX\nk=1\nWk(X)2h(Xk)\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\u2264w1E\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\nN\nX\nk=1\nWk(X)h(Xi)\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\nwith w1 \u21920 and the proof is concluded by showing that E\n\u0010PN\nk=1 Wk(X)h(Xk)\n\u0011\nis\nbounded.\nRecall that the weights Wk are functions of X and of the whole training set,\nand we will need to make this dependency explicit and write Wi(X,TX) where TX =\n(X1,...,XN). Similarly, the ranks in (9.2) will be written r+\nj (X,TX) and r\u2212\nj (X,TX).\nBecause X,X1,...,XN are i.i.d., we can switch the role of X and Xk in the kth term\nof the sum, yielding\nE\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\nN\nX\nk=1\nWk(X,TX)h(Xk)\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8= E\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\nN\nX\ni=1\nWk(Xk,T (k)\nX )\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8h(X)\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\nwith T (k)\nX = (X1,...,Xk\u22121,X,Xk+1,...,XN). We now show that PN\nk=1 Wk(Xk,T (k)\nX ) is bounded\nindependently of X,X1,...,XN.\nFor this purpose, we group X1,...,XN according to approximate alignment with\nX. For u \u2208Rd with |u| = 1 and for \u03b4 \u2208(0,\u03c0/4), denote by \u0393(u,\u03b4) the cone formed by\nall vectors v in Rd such that \u27e8v , u\u27e9> |v|cos\u03b4 (i.e., the angle between v and u is less\nthan \u03b4). Notice that if v,v\u2032 \u2208\u0393(u,\u03b4), then \u27e8v , v\u2032\u27e9\u2265cos(2\u03b4)|v||v\u2032| and if |v\u2032| \u2264|v|, then\n|v|2 \u2212|v \u2212v\u2032|2 = |v\u2032|(2|v|cos(2\u03b4) \u2212|v\u2032|) > 0\n(9.4)\nbecause cos(2\u03b4) > 1/2.\nFixing \u03b4, let Cd(\u03b4) be the minimal number of such cones needed to cover Rd.\nChoosing such a covering \u0393(u1,\u03b4),...,\u0393(uM,\u03b4) where M = Cd(\u03b4), we define the fol-\nlowing subsets of {1,...,M}:\nI0 = {k : Xk = X}\nIq =\nn\nk < I0 : Xk \u2212X \u2208\u0393(uq,\u03b4)\no\n,\nq = 1,...,M\n206\nCHAPTER 9. NEAREST-NEIGHBOR METHODS\n(these sets may overlap). We have\nN\nX\nk=1\nWk(Xk,T (k)\nX ) \u2264\nM\nX\nq=0\nX\nk\u2208Iq\nWk(Xk,T (k)\nX )\nIf k \u2208I0, then r\u2212\nk (Xk,T (k)\nX ) = 0 and r+\nk (Xk,T (k)\nX ) = c with c = |I0|. This implies that, for\nk \u2208I0, we have Wk(Xk,T (k)\nX ) = Pc\nj=1 wj/c and\nX\nk\u2208I0\nWk(Xk,T (k)\nX ) =\nc\nX\nj=1\nwj.\nWe now consider Iq with q \u22651. Write Iq = {i1,...,ir} ordered so that |Xij \u2212X| is\nnon-decreasing. If j\u2032 < j, we have (using (9.4)) |Xij \u2212Xij\u2032| < |X \u2212Xij|. This implies that\nr\u2212\nij(Xij,T\n(ij)\nX ) \u2265j \u22121 and r+\nij (Xij,T\n(ij)\nX ) \u2212r\u2212ij(Xij,T\n(ij)\nX ) \u2265c + 1. Therefore,\nWij(Xij,T\n(ij)\nX ) \u2264\n1\nc + 1\nc+j\nX\nj\u2032=j\nwj\u2032\nand\nX\nk\u2208Iq\nWk(Xk,T (k)\nX ) \u2264\n1\nc + 1\nN\nX\nj=1\nc+j\nX\nj\u2032=j\nwj\u2032 =\n1\nc + 1\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\nc\nX\nj\u2032=1\nj\u2032wj\u2032 + (c + 1)\nN\nX\nj\u2032=c+1\nwj\u2032\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8.\nThis yields\nN\nX\nk=1\nWk(Xk,T (k)\nX ) \u2264\nc\nX\nj=1\nwj + Cd(\u03b4)\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\n1\nc + 1\nc\nX\nj\u2032=1\nj\u2032wj\u2032 +\nN\nX\nj\u2032=c+1\nwj\u2032\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\u2264Cd(\u03b4) + 1.\nWe therefore have\nE\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\nN\nX\nk=1\nWk(X)2E(Z2\nk | Xk)\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\u2264w1(Cd(\u03b4) + 1)E(h(X)) \u21920,\nwhich concludes the proof.\n\u25a0\nTheorem 9.1 is proved in Stone [183] with weaker hypotheses allowing for more\nflexibility in the computation of distances, in which, for example, differences X \u2212Xi\ncan be normalized by dividing them by a factor \u03c3i that may depend on the training\nset. These relaxed assumptions slightly complicate the proof, and we refer the reader\nto Stone [183] for a complete exposition.\n9.1. NEAREST NEIGHBORS FOR REGRESSION\n207\n9.1.2\nOptimality\nThe NN method can be shown to be optimal over some classes of functions. Opti-\nmality is in the min-max sense, and works as follows. We assume that the regression\nfunction f (x) = E(Y | X = x) belongs to some set F of real-valued functions on Rd.\nMost of the time, the estimation methods must be adapted to a given choice of F ,\nand various choices have arisen in the literature: classes of functions with r bounded\nderivatives, Sobolev or related spaces, functions whose Fourier transforms has given\nproperties, etc.\nConsider now an estimator of f , denoted \u02c6fN, based on a training set of size N.\nWe can measure the error by, say:\n\r\r\r \u02c6fN \u2212f\n\r\r\r2 =\n Z\nRd( \u02c6fN(x) \u2212f (x))2dx\n!1/2\nSince \u02c6fN is computed from a random sample, this error is a random variable. One\ncan study, when bN \u21920, the probability\nPf\n\u0010\n\u2225\u02c6fN \u2212f \u22252\n2 \u2265cbN\n\u0011\nfor some constant c and, for example, for the model: Y = f (X) + noise. Here, the\nnotation Pf refers to the model assumption indicating the unobserved function f .\nThe min-max method considers the worst case and computes\nMN(c) = sup\nf \u2208F\nPf\n\u0010\n\u2225\u02c6fN \u2212f \u22252\n2 \u2265cbN\n\u0011\n.\nThis quantity now only depends on the estimation algorithm. One defines the no-\ntion of \u201clower convergence rate\u201d as a sequence bN such that, for any choice of the\nestimation algorithm, MN(c) can be found arbitrarily close to 1 (i.e., \u2225\u02c6fN \u2212f \u22252\n2 \u2265cbN\nwith arbitrarily high probability for all f \u2208F ), for arbitrarily large N (and for some\nchoice of c). The mathematical statement is\n\u2203c > 0 : liminf\nN\u2192\u221eMN(c) = 1.\nSo, if bN is a lower convergence rate, then, for every estimator, there exists a constant\nc such that the accuracy cbN cannot be achieved.\nOn the other hand, one says that bN is an achievable rate of convergence if there\nexists an estimator such that, for some c\u2032,\nlimsup\nN\u2192\u221e\nMN(c\u2032) = 0.\n208\nCHAPTER 9. NEAREST-NEIGHBOR METHODS\nThis says that for large N, and for some c\u2032, the accuracy is higher than c\u2032bN for the\ngiven estimator. Notice the difference: a lower rate holds for all estimators, and an\nachievable rate for at least one estimator.\nThe final definition of a min-max optimal rate is that it is both a lower rate and\nan achievable rate (obviously for different constants c and c\u2032). And an estimator is\noptimal in the min-max sense if it achieves an optimal rate.\nOne can show that the p-NN estimator is optimal (under some assumptions on\nthe ratio pN/N) when F is the class of Lipschitz functions on Rd, i.e., the class of\nfunctions such that there exists a constant K with\n|f (x) \u2212f (y)| \u2264K|x \u2212y|\nfor all x,y \u2208Rd. In this case, the optimal rate is bN = N \u22121/(2+d) (notice again the\n\u201ccurse of dimensionality\u201d: to achieve a given accuracy in the worst case, the number\nof data points must grow exponentially with the dimension).\nIf the function class consists of smoother functions (for example, several deriva-\ntives), the p-NN method is not optimal. This is because the local averaging method\nis too crude when one knows already that the function is smooth. But it can be\nmodified (for example by fitting, using least squares, a polynomial of some degree\ninstead of computing an average) in order to obtain an optimal rate.\n9.2\np-NN classification\nLet (x1,y1,...,xN,yN) be the training set, with xi \u2208Rd and yi \u2208RY where RY is a\nfinite set of classes. Using the same notation as in the previous section, define\nb\u03c0w(y|x) =\nN\nX\nk=1\nWk(x)1yk=y.\nLet the corresponding classifier be\n\u02c6fw(x) = argmax\ny\u2208RY\nb\u03c0w(y|x).\nTheorem 9.1 may be applied, for y \u2208RY, to the function fy(x) = \u03c0(y | x) = E(1Y=y |\nX = x), which allows one to interpret the estimator b\u03c0(y | x) as a nearest-neighbor\npredictor of the random variable 1Y=y as a function of X. We therefore obtain the\nconsistency of the estimated posteriors when N \u2192\u221eunder the same assumption as\nthose of theorem 9.1. This implies that, for large N, the classification will be close to\nBayes\u2019s rule.\n9.2. p-NN CLASSIFICATION\n209\nAn asymptotic comparison with Bayes\u2019s rule can already be made with p = 1. Let\n\u02c6yN(x) be the 1-NN estimator of Y given x and a training set of size N, and let \u02c6y(x) be\nthe Bayes estimator. We can compute the Bayes error by\nP( \u02c6y(X) , Y)\n=\n1 \u2212P( \u02c6y(X) = Y)\n=\n1 \u2212E(P( \u02c6y(X) = Y|X))\n=\n1 \u2212E(max\ny\u2208RY\n\u03c0(y|X))\nFor the 1-NN rule, we have\nP( \u02c6yN(X) , Y)\n=\n1 \u2212P( \u02c6yN(X) = Y)\n=\n1 \u2212E(P( \u02c6yN(X) = Y|X))\nLet us make the assumption that nearest neighbors are not tied (with probability\none). Let k\u2217(x,T ) denote the index of the nearest neighbor to x in the training set T.\nWe have\nP( \u02c6yN(X) = Y | X)\n= E(P( \u02c6yN(X) = Y | X,T ))\n= E\n\u0012 N\nX\nk=1\nP(Y = Yk | X,T )1k\u2217(X,T )=k\n\u0013\n= E\n\u0012 N\nX\nk=1\nP(Y = Yk | X,Xk)1k\u2217(X,T )=k\n\u0013\n= E\n\u0012 N\nX\nk=1\nX\ng\u2208RY\nP(Y = g,Yk = g | X,Xk)1k\u2217(X,T )=k\n\u0013\n= E\n\u0012 N\nX\nk=1\nX\ng\u2208RY\n\u03c0(g | X)\u03c0(g | Xk)\u03c7k\u2217(X,T )=k\n\u0013\n= E\n\u0012 X\ng\u2208RY\n\u03c0(g | X)\u03c0(g | Xk\u2217(X,T ))\n\u0013\nNow, assume the continuity of x 7\u2192\u03c0(g | x) (although the result can be proved\nwithout this simplifying assumption). We know that Xk\u2217(X,T ) \u2192X when N \u2192\u221e(see\nthe proof of theorem 9.1), which implies that \u03c0(g | Xk\u2217(X,T )) \u2192\u03c0(x | X) and at the\nlimit\nP( \u02c6yN(X) = Y | X) \u2192\nX\ng\u2208RY\n\u03c0(g | X)2.\n210\nCHAPTER 9. NEAREST-NEIGHBOR METHODS\nThis implies that the asymptotic 1-NN misclassification error is always smaller\nthan 2 times the Bayes error, that is\n1 \u2212E\n\u0012 X\ng\u2208RY\n\u03c0(g | X)2\u0013\n\u22642(1 \u2212E(max\ng\n\u03c0(g | X)))\nIndeed, the left-hand term is smaller than 1 \u2212E(maxg \u03c0(g|x)2) and the result comes\nfrom the fact that for any t \u2208R. 1 \u2212t2 \u22642 \u22122t.\nRemark 9.2 Nearest neighbor methods may require large computation time, since,\nfor a given x, the number of comparisons which are needed is the size of the training\nset. However, efficient (tree-based) search algorithms can be used in many cases to\nreduce it to a logarithm in the size of the database, which is acceptable. A reduction\nof the size of the training set by clustering also is a possibility for improving the\nefficiency.\nThe computation time is also generally proportional to the dimension d of the\ninput x. When d is large, a reduction of dimension is often a good idea. Principal\ncomponents (see chapter 20), or LDA directions (see chapter 8) can be used for this\npurpose.\n\u2666\n9.3\nDesigning the distance\nLDA-based distance\nThe most important factor in the design of a NN procedure\nprobably is the choice of the distance, something we have not discussed so far. In-\ntuitively, the distance should increase fast in the directions \u201cperpendicular\u201d to the\nregions of constancy of the class variables, and slowly (ideally not at all) within these\nregions. The following construction uses discriminant analysis [87].\nFor g \u2208RY, let \u03a3g be the covariance matrix in class g, and \u03a3w = P\ng\u2208RY \u03c0g\u03a3g be\nthe within-class variance, where \u03c0g is the frequency of class g. Let \u03a3b denote the\nbetween-class covariance matrix (see section 8.2).\nFor x \u2208Rd, define the spherized vector x\u2217= \u03a3\u22121/2\nw\nx. The between-class variance\ncomputed for spherized data is \u03a3\u2217\nb = \u03a3\u22121/2\nw\n\u03a3b\u03a3\u22121/2\nw\n. A direction is discriminant if it is\nclose to the principal eigenvectors of \u03a3\u2217\nb. This suggests the introduction of the norm\n|x|2\n\u2217= (x\u2217)T \u03a3\u2217\nbx\u2217= xT \u03a3\u22121/2\nw\n(\u03a3\u22121/2\nw\n\u03a3b\u03a3\u22121/2\nw\n)\u03a3\u22121/2\nw\nx = xT \u03a3\u22121\nw \u03a3b\u03a3\u22121\nw x.\nThis replaces the standard Euclidean norm (the method can be made more robust\nby adding \u03f5IdRd to \u03a3\u2217\nb.)\n9.3. DESIGNING THE DISTANCE\n211\nTangent distance\nDesigning the distance, however, can sometimes be based on a\npriori knowledge on some invariance properties associated with the classes. A suc-\ncessful example comes from character recognition, where it is known that trans-\nforming images by slightly rotating, scaling, or translating the character should not\nchange its class. This corresponds to the following general framework.\nFor each input x \u2208Rd, assume that one can make small transformations without\nchanging the class of x. We model these transformations as parametrized functions\nx 7\u2192x\u03b8 = \u03d5(x,\u03b8) \u2208Rd, such that \u03d5(x,0) = x and \u03d5 is smooth in \u03b8, which is a q-\ndimensional parameter. The assumption is that \u03d5(x,\u03b8) and x should be from the\nsame class, at least for small \u03b8. This will be used to improve on the Euclidean dis-\ntance on Rd.\nTake x,x\u2032 \u2208Rd. Ideally, one would like to use the distance D(x,x\u2032) = inf\u03b8,\u03b8\u2032 dist(x\u03b8,x\u03b8\u2032)\nwhere \u03b8 and \u03b8\u2032 are restricted to a small neighborhood of 0. A more tractable expres-\nsion can be based on first-order approximations\nx\u03b8 \u2243x + \u2207\u03b8\u03d5(x,0)u = x +\nq\nX\ni=1\nui\u2202\u03b8i\u03d5(x,0)\nand\nx\u2032\n\u03b8 \u2243x\u2032 + \u2207\u03b8\u03d5(x\u2032,0)u\u2032 = x\u2032 +\nq\nX\ni=1\nu\u2032\ni\u2202\u03b8i\u03d5(x\u2032,0)\nyielding the approximation (also called the tangent distance)\nD(x,x\u2032)2 \u2243\ninf\nu,u\u2032\u2208Rq\n\r\r\rx \u2212x\u2032 + \u2207\u03b8\u03d5(x,0)u \u2212\u2207\u03b8\u03d5(x\u2032,0)u\u2032\r\r\r2 .\nThe computation now is a simple least-squares problem, for which the solution is\ngiven by the system\n \n\u2207\u03b8\u03d5(x,0)T \u2207\u03b8\u03d5(x,0)\n\u2212\u2207\u03b8\u03d5(x,0)T \u2207\u03b8\u03d5(x\u2032,0)\n\u2212\u2207\u03b8\u03d5(x\u2032,0)T \u2207\u03b8\u03d5(x,0)\n\u2207\u03b8\u03d5(x\u2032,0)T \u2207\u03b8\u03d5(x\u2032,0)\n! \nu\nv\n!\n=\n \n\u2207\u03b8\u03d5(x,0)T (x\u2032 \u2212x)\n\u2207\u03b8\u03d5(x\u2032,0)T (x \u2212x\u2032)\n!\n.\nA slight modification, to ensure that the norms of u and u\u2032 are not too large, is to\nadd a penalty \u03bb(|u|2 + |u\u2032|2), which results in adding \u03bbIdRq to the diagonal blocs of\nthe above matrix.\n212\nCHAPTER 9. NEAREST-NEIGHBOR METHODS\nChapter 10\nTree-based Algorithms, Randomization and Boost-\ning\n10.1\nRecursive Partitioning\nRecursive partitioning methods implement a \u201cdivide and conquer\u201d strategy to ad-\ndress the prediction problem. They separate the input space RX into small regions\non which prediction is \u201ceasy,\u201d i.e., such that the observed values of the output vari-\nable are (almost) constant for input values in these regions. The regions are esti-\nmated by recursive divisions until they become either too small or homogeneous.\nThese divisions are conveniently represented in the form of binary trees.\n10.1.1\nBinary prediction trees\nDefine a binary node to be a structure \u03bd that contains the following information (note\nthat the definition is recursive):\n\u2022 A label L(\u03bd) that uniquely identifies the node.\n\u2022 A set of children, C(\u03bd), which is either empty or a pair of nodes (l(\u03bd),r(\u03bd)).\n\u2022 A binary feature, i.e., a function \u03b3\u03bd : RX \u2192{0,1}, which is \u201cNone\u201d (i.e., irrelevant)\nif the node has no children.\n\u2022 A predictor, f\u03bd : RX \u2192RY, which is \u201cNone\u201d if the node has children.\nA node without children is called a terminal node, or a leaf.\nA binary prediction tree T is a finite set of nodes, with the following properties:\n(i) Only one node has no parent (the root, denoted \u03c1 or \u03c1T);\n213\n214\nCHAPTER 10. TREE-BASED ALGORITHMS\n(ii) Each other node has exactly one parent;\n(iii) No node is a descendent of itself.\n10.1.2\nTraining algorithm\nAssume that a family \u0393 of binary features \u03b3 : RX \u2192{0,1} is chosen, together with a\nfamily F of predictors f : RX \u2192RY. Assume also the existence of two \u201calgorithms\u201d\nas follows:\n\u2022 Feature selection: Given the feature set \u0393 and a training set T, return an optimized\nbinary feature b\n\u03b3T,\u0393 \u2208\u0393.\n\u2022 Predictor optimization: Given the predictor set F and a training set T, return an\noptimized predictor \u02c6fT,F \u2208F .\nFinally, assume that a stopping rule is defined, as a function of training sets \u03c3 : T 7\u2192\n\u03c3(T ) \u2208{0,1}, where 0 means \u201ccontinue\u201d, and 1 means \u201cstop\u201d.\nGiven a training set T0, the algorithm builds a binary tree T using a recursive\nconstruction. Each node \u03bd \u2208T will be associated to a subset of T0, denoted T\u03bd. We\ndefine below a recursive operation, denoted Node(T ,j) that adds a node \u03bd to a tree\nT given a subset T of T0 and a label j. Starting with T = \u2205, calling Node(T0,0) will\nthen create the desired tree.\nAlgorithm 10.1 (Node insertion: Node(T,j))\n(a) Given T and j, let T\u03bd = T and L(\u03bd) = j.\n(b) If \u03c3(T) = 1, let C(\u03bd) = \u2205, \u03b3\u03bd = \u201cNone\u201d and f\u03bd = \u02c6fT,F .\n(c) If \u03c3(T ) = 0, let f\u03bd = \u201cNone\u201d, \u03b3\u03bd = b\n\u03b3T,\u0393 and C(\u03bd) = (l(\u03bd),r(\u03bd)) with\nl(\u03bd) = Node(Tl,2j + 1),\nr(\u03bd) = Node(Tr,2j + 2)\nwhere\nTl = {(x,y) \u2208T : \u03b3\u03bd(x) = 0},\nTr = {(x,y) \u2208T : \u03b3\u03bd(x) = 1}\n(d) Add \u03bd to T and return.\nRemark 10.1 Note that, even though the learning algorithm for prediction trees can\nbe very conveniently described in recursive form as above, efficient computer im-\nplementations should avoid recursive calls, which may be inefficient and memory\ndemanding. Moreover, for large trees, it is likely that recursive implementations\nwill reach the maximal number of recursive calls imposed by compilers.\n\u2666\n10.1. RECURSIVE PARTITIONING\n215\n10.1.3\nResulting predictor\nOnce the tree is built, the predictor x 7\u2192\u02c6fT(x) is recursively defined as follows.\n(a) Initialize the computation with \u03bd = \u03c1.\n(b) At a given step of the algorithm, let \u03bd be the current node.\n\u2022 If \u03bd has no children: then let \u02c6fT(x) = f\u03bd(x).\n\u2022 Otherwise: replace \u03bd by l(\u03bd) if \u03b3\u03bd(x) = 0 and by r(\u03bd) if \u03b3\u03bd(x) = 1 and go back to\n(b).\n10.1.4\nStopping rule\nThe function \u03c3, which decides whether a node is terminal or not is generally defined\nbased on very simple rules. Typically, \u03c3(T) = 1 when one the following conditions is\nsatisfied:\n\u2022 The number of training examples in T is small (e.g., less than 5).\n\u2022 The values yk in T have a small variance (regression) or are constant (classifica-\ntion).\n10.1.5\nLeaf predictors\nWhen one reaches a terminal node \u03bd (so that \u03c3(T\u03bd) = 1), a predictor f\u03bd must be\ndetermined. This function can be optimized within any set F of predictors, using\nany learning algorithm, but in practice, one usually makes this fairly simple and\ndefines F to be the family of constant functions taking values in RY. The function\n\u02c6fT ,F is then defined as:\n\u2022 the average of the values of yk, for (xk,yk) \u2208T (regression);\n\u2022 the mode of the distribution of yk, for (xk,yk) \u2208T (classification).\n10.1.6\nBinary features\nThe space \u0393 of possible binary features must be specified in order to partition non-\nterminal nodes. A standard choice, used in the CART model [43] with RX = Rd,\nis\n\u0393 =\nn\n\u03b3(x) = 1[x(i)\u2265\u03b8],i = 1,...,d,\u03b8 \u2208R\no\n(10.1)\nwhere x(i) is the ith coordinate of x. This corresponds to splitting the space using a\nhyperplane parallel to one of the coordinate axes.\n216\nCHAPTER 10. TREE-BASED ALGORITHMS\nThe binary function b\n\u03b3T,\u0393 can be optimized over \u0393 using a greedy evaluation of\nthe risk, assuming that the prediction is based on the two nodes resulting from the\nsplit. For \u03b3 \u2208\u0393, f0,f1 \u2208F , define\nF\u03b3,f0,f1(x) =\n(f0(x) if \u03b3(x) = 0\nf1(x) if \u03b3(x) = 1\nGiven a risk function r, one then evaluates\nET (\u03b3) = min\nf0,f1\u2208F\nX\n(x,y)\u2208T\nr(y,F\u03b3,f0,f1(x))\nOne then chooses b\n\u03b3T,\u0393 = argmin\u03b3\u2208\u0393(ET (\u0393)).\nExample 10.2 (Regression) Consider the regression case, taking squared differences\nas risk and letting F contain only constant functions. Then\nET (\u03b3) = min\nm0,m1\nX\n(x,y)\u2208T\n\u0010\n(y \u2212m0)21\u03b3(x)=0 + (y \u2212m1)21\u03b3(x)=1\n\u0011\n.\nObviously, the optimal m0 and m1 are the averages of the output values, y, in each\nof the subdomains defined by \u03b3. For CART (see (10.1)), this cost must be minimized\nover all choices (i,\u03b8) with i = 1,...,d and \u03b8 \u2208R where \u03b3i,\u03b8(x) = 1 if x(i) > \u03b8 and 0\notherwise.\n\u2666\nExample 10.3 (Classification.) For classification, one can apply the same method,\nwith the 0/1 loss, letting\nET (\u03b3) = min\ng0,g1\nX\n(x,y)\u2208T\n\u0010\n1y,g01\u03b3(x)=0 + 1y,g11\u03b3(x)=1\n\u0011\n.\nThe optimal g0 and g1 are the majority classes in T \u2229{\u03b3 = 0} and T \u2229{\u03b3 = 1}.\n\u2666\nExample 10.4 (Entropy selection for classification) For classification trees, other\nsplitting criteria may be used based on the empirical probability pT on the set T,\ndefined as\npT (A) = 1\nN |{k : (xk,yk) \u2208A}|\nfor A \u2282RX \u00d7 RY. The previous criterion, ET (\u03b3), is proportional to\npT (\u03b3 = 0)(1 \u2212max\ng\npT (g | \u03b3 = 0)) + pT (\u03b3 = 1)(1 \u2212max\ng\npT (g | \u03b3 = 1)).\nOne can define alternative objectives in the form\npT (\u03b3 = 0)H(pT (g | \u03b3 = 0)) + pT (\u03b3 = 1)H(pT (g | \u03b3 = 1))\n10.1. RECURSIVE PARTITIONING\n217\nwhere \u03c0 \u2192H(\u03c0) associates to a probability distribution \u03c0 a \u201ccomplexity measure\u201d\nthat is minimal when \u03c0 is concentrated on a single class (which is the case for \u03c0 7\u2192\n1 \u2212maxg \u03c0(g)).\nMany such measures exists, and many of them are defined as various forms of\nentropy designed in information theory. The most celebrated is Shannon\u2019s entropy\n[176], defined by\nH(p) = \u2212\nX\ng\u2208RY\np(g)logp(g).\nIt is always positive, and minimal when the distribution is concentrated on a single\nclass. Other entropy measures include:\n\u2022 The Tsallis entropy: H(p) =\n1\n1\u2212q\nP\ng\u2208RY (p(g)q\u22121), for q , 1. (Tsallis entropy for q = 2\nis sometimes called the Gini impurity index.)\n\u2022 The Renyi entropy: H(p) =\n1\n1\u2212q logP\ng\u2208RY p(g)q, for q \u22650,q , 1.\n\u2666\n10.1.7\nPruning\nGrowing a decision tree to its maximal depth (given the amount of available data)\ngenerally leads to predictors that overfit the data. The training algorithm is usually\nfollowed by a pruning step that removes some some nodes based on a complexity\npenalty.\nLetting \u03c4(T) denote the set of terminal nodes in the tree T and \u02c6fT the associated\npredictor, pruning is represented as an optimization problem, where one minimizes,\ngiven the training set T,\nU\u03bb(T,T ) = \u02c6RT ( \u02c6fT) + \u03bb|\u03c4(T)|\nwhere \u02c6RT is as usual the in-sample error measured on the training set T.\nTo prune a tree, one selects one or more internal nodes and remove all their\ndescendants (so that these nodes become terminal). Associate to each node \u03bd in T its\nlocal in-sample error ET\u03bd equal to the error made by the optimal classifier estimated\nfrom the training data associated with \u03bd. Then,\nU\u03bb(T,T ) =\nX\n\u03bd\u2208\u03c4(T)\n|T\u03bd|\n|T | ET\u03bd + \u03bb|\u03c4(T)|\nIf \u03bd is a node in T (internal or terminal), let T\u03bd be the subtree of T containing\n\u03bd as a root and all its descendants. Let T(\u03bd) be the tree T will all descendants of \u03bd\n218\nCHAPTER 10. TREE-BASED ALGORITHMS\nremoved (keeping \u03bd). Then\nU\u03bb(T,T ) = U0(T(\u03bd),T ) \u2212|T\u03bd|\n|T | (ET\u03bd \u2212U0(T\u03bd,T\u03bd)) + \u03bb(|\u03c4(T\u03bd)| \u22121).\nNote also that, if \u03bd is internal, and \u03bd\u2032, \u03bd\u2032\u2032 are its children, then\nU0(T\u03bd,T\u03bd) = |T\u03bd\u2032|\n|T\u03bd| U0(T\u03bd\u2032,T\u03bd\u2032) + |T\u03bd\u2032\u2032|\n|T\u03bd| U0(T\u03bd\u2032\u2032,T\u03bd\u2032\u2032)\nThis formula can be used to compute U0(T\u03bd) recursively for all nodes, starting with\nleaves for which U0(T\u03bd) = E(T\u03bd). (We also have |\u03c4(T\u03bd)| = |\u03c4(T\u03bd\u2032)| + |\u03c4(T\u03bd\u2032\u2032)|.) The\nfollowing algorithm converges to a global minimizer of U\u03bb.\nAlgorithm 10.2 (Pruning)\n(1) Start with a complete tree T(0) built without penalty.\n(2) Compute, for all nodes U0(T\u03bd) and |\u03c4(T\u03bd)|. Let\n\u03c8\u03bd = |T\u03bd|\n|T| (ET\u03bd \u2212U0(T\u03bd)) \u2212\u03bb(|\u03c4(T\u03bd)| \u22121).\n(3) Iterate the following steps.\n\u2022 If \u03c8\u03bd < 0 for all internal nodes \u03bd, exit the program and return the current T(n).\n\u2022 Otherwise choose an internal node \u03bd such that \u03c8\u03bd is largest.\n\u2022 Let T(n + 1) = T(\u03bd)(n). Subtract \u03bb(|\u03c4(T\u03bd(n))| \u22121) to \u03c1\u03bd\u2032 for all \u03bd\u2032 ancestor of \u03bd.\n10.2\nRandom Forests\n10.2.1\nBagging\nA random forest [7, 42] is a special case of composite predictors (we will see other\nexamples later in this chapter when describing boosting methods) that train mul-\ntiple individual predictors under various conditions and combine them, through\naveraging, or majority voting. With random forests, one generates individual trees\nby randomizing the parameters of the learning process. One way to achieve this is\nto randomly sample from the training set before running the training algorithm.\nLetting as before T0 = (x1,y1,...,xN,yN) denote the original set, with size N, one\ncan create \u201cnew\u201d training data by sampling with replacement from T0. More pre-\ncisely, consider the family of independent random variables \u03be = (\u03be1,...,\u03beN), with\n10.2. RANDOM FORESTS\n219\neach \u03bej following a uniform distribution over {1,...,N}. One can then form the ran-\ndom training set\nT0(\u03be) = (x\u03be1,y\u03be1,...,x\u03beN,y\u03beN).\nRunning the training algorithm using T0(\u03be) then provides a random tree, denoted\nT(\u03be). Now, by sampling K realizations of \u03be, say \u03be(1),...,\u03be(K), one obtains a collection\nof K random trees (a random forest) T\u2217= (T1,...,TK), with Tj = T(\u03be(j)) that can\nbe combined to provide a final predictor. The simplest way to combine them is to\naverage the predictors returned by each tree (assuming, for classification, that this\npredictor is a probability distribution on classes), so that\nfT\u2217(x) = 1\nK\nK\nX\nj=1\nfTj(x).\n(10.2)\nFor classification, one can alternatively let each individual tree \u201cvote\u201d for their most\nlikely class.\nObviously, randomizing training data and averaging the predictors is a general\napproach that can be applied to any prediction algorithm, not only to decision trees.\nIn the literature, the approach described above has been called bagging [41], which is\nan acronym for \u201cbootstrap aggregating\u201d (bootstrap itself being a general resampling\nmethod in statistics that samples training data with replacement to determine some\nproperties of estimators). Another way to randomize predictors (especially when\nd, the input dimension is large), is to randomize input data by randomly removing\nsome of the coordinates, leading to a similar construction.\nWith decision trees one can in addition randomize the binary features use to\nsplit nodes, as described next. While bagging may provide some enhancement to\npredictors, feature randomization for decision trees often significantly improves the\nperformance, and is the typical randomization method used for random forests.\n10.2.2\nFeature randomization\nWhen one decides to split a node during the construction of a prediction tree, one\ncan optimize the binary feature \u03b3 over a random subset of \u0393 rather than exploring\nthe whole set. For CART, for example, one can select a small number of dimensions\ni1,...,iq \u2208{1,...,d} with q \u226ad, and optimize \u03b3 by thresholding one of the coordi-\nnates x(ij) for j \u2208{1,...,q}. This results in a randomized version of the node insertion\nfunction.\nAlgorithm 10.3 (Randomized node insertion: RNode(T,j))\n(a) Given T and j, let T\u03bd = T and L(\u03bd) = j.\n(b) If \u03c3(T) = 1, let C(\u03bd) = \u2205, \u03b3\u03bd = \u201cNone\u201d and f\u03bd = \u02c6fT,CF.\n220\nCHAPTER 10. TREE-BASED ALGORITHMS\n(c) If \u03c3(T \u2032) = 0, sample (e.g., uniformly without replacement) a subset \u0393\u03bd of \u0393 and let\nf\u03bd = \u201cNone\u201d, \u03b3\u03bd = \u02c6\u03b3T,\u0393\u03bd and C(\u03bd) = (l(\u03bd),r(\u03bd)) with\nl(\u03bd) = Node(Tl,2j + 1)\nr(\u03bd) = Node(Tr,2j + 2)\nwhere\nTl = {(x,y) \u2208T : \u03b3\u03bd(x) = 0}\nTr = {(x,y) \u2208T : \u03b3\u03bd(x) = 1}\n(d) Add \u03bd to T and return.\nNow, each time the function RNode(T0,0) is run, it returns a different, random,\ntree. If it is called K times, this results in a random forest T\u2217= (T1,...TK), with a\npredictor FT\u2217given by (10.2). Note that trees in random forests are generally not\npruned, since this operation has been observed to bring no improvement in the con-\ntext of randomized tress.\n10.3\nTop-Scoring Pairs\nTop-Scoring Pair (TSP) classifiers were introduced in Geman et al. [78] and can be\nseen as forests formed with depth-one classification trees in which splitting rules are\nbased on the comparison of pairs of variables. More precisely, define\n\u03b3ij(x) = 1x(i)>x(j).\nA decision tree based on these rules only relies on the order between the features,\nand is therefore well adapted to situations in which the observations are subject to\nincreasing transformations, i.e., when the observed variable X is such that X(j) =\n\u03d5(Z(j)), where \u03d5 : R \u2192R is random and increasing and Z is a latent (unobserved)\nvariable. Obviously, in such a case, order-based splitting rules do not depend on \u03d5.\nSuch an assumption is relevant, for example, when experimental conditions (such\nas temperature) may affect the actual data collection, without changing their order,\nwhich is the case when measuring high-throughput biological data, such as microar-\nrays, for which the approach was introduced.\nAssuming two classes, a depth-one tree in this context is simply the classifier\nfij = \u03b3ij. Given a training set, the associated empirical error is\nEij = 1\nN\nN\nX\nk=1\n1\u03b3ij(xk),yk = 1\nN\nN\nX\nk=1\n|yk \u2212\u03b3ij(xk)|\n10.4. ADABOOST\n221\nand the balanced error (better adapted to situations in which one class is observed\nmore often than the other) is\nEb\nij =\nN\nX\nk=1\nwk|yk \u2212\u03b3ij(xk)|\nwith wk = 1/(2Nyk), where N0, N1 are the number of observations with yk = 0, yk = 1.\nPairs (i,j) with small errors are those for which the order between the features switch\nwith high probability when passing from class 0 to class 1.\nIn its simplest form, the TSP classifier defines the set\nP = argmin\nij\nEb\nij\nof global minimizers of the empirical error (which may just be a singleton) and pre-\ndicts the class based on a majority vote among the family of predictors (fij,(i,j) \u2208P).\nEquivalently, selected variables maximize the score \u2206ij = 1 \u2212Eb\nij, leading to the\nmethod\u2019s name.\nSuch classifiers, which are remarkably simple, have been found to be competitive\namong a wide range of \u201cadvanced\u201d classification algorithms for large-dimensional\nproblems in computational biology. The method has been refined in Tan et al. [190],\nleading to the k-TSP classifier, which addresses the following remarks. First, when\nj,j\u2032 are highly correlated, and (i,j) is a high-scoring pair, then (i,j\u2032) is likely to be\none too, and their associated decision rules will be redundant. Such cases should\npreferably be pruned from the classification rules, especially if one wants to select\na small number of pairs. Second, among pairs of features that switch with the same\nprobability, it is natural to prefer those for which the magnitude of the switch is\nlargest, e.g., when the pair of variables switches from a regime in which one of them\nis very low and the other very high to the opposite. In Tan et al. [190], a rank-based\ntie-breaker is introduced, defined as\n\u03c1ij =\nN\nX\nk=1\nwk(Rk(i) \u2212Rk(j))(2yk \u22121),\nwhere Rk(i) denotes the rank of x(i)\nk\nin x(1)\nk ,...,x(d)\nk . One can now order pairs (i,j)\nand (i\u2032,j\u2032) by stating that the former scores higher if (i) \u2206ij > \u2206i\u2032j\u2032, or (ii) \u2206ij = \u2206i\u2032j\u2032\nand \u03c1ij > \u03c1i\u2032j\u2032. The k-TSP classifier is formed by selecting pairs, starting from the\nhighest scoring one, and use as lth pair (for l \u2264k) the highest scoring ones among all\nthose that do not overlap with the previously selected ones. In [190], the value of k\nis optimized using cross-validation.\n222\nCHAPTER 10. TREE-BASED ALGORITHMS\n10.4\nAdaboost\nBoosting methods refer to algorithms in which classifiers are enhanced by recur-\nsively making them focus on harder data. We first address the issue of classification,\nand describe one of the earliest algorithms (Adaboost). We will then interpret it\nas a greedy gradient descent algorithm, as this interpretation will lead to further\nextensions.\n10.4.1\nGeneral set-up\nWe first consider binary classification problems, with RY = {\u22121,1}. We want to de-\nsign a function x 7\u2192F(x) \u2208{\u22121,1} on the basis of a training set T = (x1,y1,...,xN,yN).\nWith the 0-1 loss, minimizing the empirical error is equivalent to maximizing\nET (F) = 1\nN\nN\nX\nk=1\nykF(xk).\nBoosting algorithms build the function F as a linear combination of \u201cbase classi-\nfiers,\u201d f1,...,fM, taking\nF(x) = sign\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\nM\nX\nj=1\n\u03b1jfj(x)\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8.\nWe assume that each base classifier, fj, takes values in [\u22121,1] (the interval).\nThe sequence of base classifiers is learned by progressively focusing on the hard-\nest examples. We will therefore assume that the training algorithm for base clas-\nsifiers takes as input the training set T as well a family of positive weights W =\n(w1,...,wN). More precisely, letting\npW(k) =\nwk\nPN\nk=1 wk\n,\nthe weighted algorithm should implement (explicitly or implicitly) the equivalent\nof an unweighted algorithm on a simulated training set obtained by sampling with\nreplacement K \u226bN elements of T according to pW (ideally letting K \u2192\u221e). Let us\ntake a few examples.\n\u2022 Weighted LDA: one can use LDA as described in section 8.2 with\ncg =\nX\nk:yk=g\npW(k),\n\u00b5g = 1\ncg\nX\nk:yk=g\npW(k)xk,\n\u00b5 =\nX\ng\u2208RY\ncg\u00b5g\n10.4. ADABOOST\n223\nand the covariance matrices:\n\u03a3w =\nN\nX\nk=1\npW(k)(xk \u2212\u00b5yk)(xk \u2212\u00b5yk)T ,\n\u03a3b =\nX\ng\u2208RY\ncg(\u00b5g \u2212\u00af\u00b5)(\u00b5g \u2212\u00af\u00b5)T .\n\u2022 Weighted logistic regression: just maximize\nN\nX\nk=1\npW(k)log\u03c0\u03b8(yk|xk)\nwhere \u03c0\u03b8 is given by the logistic model.\n\u2022 Empirical risk minimization algorithms can be modified in order to minimize\n\u02c6RT,W(f ) =\nN\nX\nk=1\nwkr(yk,f (xk)).\n\u2022 Of course, any algorithm can be run on a training set resampled using pW.\n10.4.2\nThe Adaboost algorithm\nBoosting algorithms keep track of a family of weights and modify it after the jth\nclassifier fj is computed, increasing the importance of misclassified examples, before\ncomputing the next classifier. The following algorithm, called Adaboost [172, 73],\ndescribes one such approach.\nAlgorithm 10.4 (Adaboost)\n\u2022 Start with uniform weights, letting W(1) = (w1(1),...,wN(1)) with wk(1) = 1/N,\nk = 1,...,N. Fix a number \u03c1 \u2208(0,1] and an integer M > 0.\n\u2022 Iterate, for j = 1,...,M:\n(1) Fit a base classifier fj using the weights W(j) = (w1(j),...,wN(j)). Let\nS+\nw(j) =\nN\nX\nk=1\nwk(j)(2 \u2212|yk \u2212fj(xk)|)\n(10.3a)\nS\u2212\nw(j) =\nN\nX\nk=1\nwk(j)|yk \u2212fj(xk)|\n(10.3b)\nand define \u03b1j = \u03c1log\n\u0010\nS+\nw(j)/S\u2212\nw(j)\n\u0011\n(2) Update the weights by\nwk(j + 1) = wk(j)exp\n\u0010\n\u03b1j|yk \u2212fj(xk)|/2\n\u0011\n.\n224\nCHAPTER 10. TREE-BASED ALGORITHMS\n\u2022 Return the classifier:\nF(x) = sign\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\nM\nX\nj=1\n\u03b1jfj(x)\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8.\nIf fj is binary, i.e., fj(x) \u2208{\u22121,1}, then |yk \u2212fj(xk)| = 21yk,fj(xk), so that S+\nW/2 is the\nweighted number of correct classifications and S\u2212\nW/2 is the weighted number of in-\ncorrect ones.\nFor \u03b1j to be positive, the jth classifier must do better than pure chance on the\nweighted training set. If not, taking \u03b1j \u22640 reflects the fact that, in that case, \u2212fj has\nbetter performance on training data.\nAlgorithms that do slightly better than chance with high probability are called\n\u201cweak learners\u201d [172]. The following proposition [73] shows that, if the base clas-\nsifiers reliably perform strictly better than chance (by a fixed, but not necessarily\nlarge, margin), then the boosting algorithm can make the training-set error arbitrar-\nily close to 0.\nProposition 10.5 Let ET be the training set error of the estimator F returned by Algo-\nrithm 10.4, i.e.,\nET = 1\nN\nN\nX\nk=1\n1yk,F(xk).\nThen\nET \u2264\nM\nY\nj=1\n\u0012\n\u03f5\u03c1\nj (1 \u2212\u03f5j)1\u2212\u03c1 + \u03f51\u2212\u03c1\nj\n(1 \u2212\u03f5j)\u03c1\u0013\nwhere\n\u03f5j =\nS\u2212\nW(j)\nS+\nW(j) + S\u2212\nW(j).\nProof We note that example k is misclassified by the final classifier if and only if\nM\nX\nj=1\n\u03b1jykfj(xk) \u22640\nor\nM\nY\nj=1\ne\u2212\u03b1jykfj(xk)/2 \u22651\n10.4. ADABOOST\n225\nNoting that |yk \u2212fj(xk)| = 1 \u2212ykfj(xk), we see that example k is misclassified when\nM\nY\nj=1\ne\u03b1j|yk\u2212fj(xk)|/2 \u2265\nM\nY\nj=1\ne\u03b1j/2.\nThis shows that\nET = 1\nN\nN\nX\nk=1\n1yk,F(xk)\n= 1\nN\nN\nX\nk=1\n1QM\nj=1 e\u03b1j |yk\u2212fj (xk)|/2\u2265QM\nj=1 e\u03b1j /2\n\u22641\nN\nN\nX\nk=1\nM\nY\nj=1\ne\u03b1j|yk\u2212fj(xk)|/2\nM\nY\nj=1\ne\u2212\u03b1j/2.\nLet, for q \u2264M,\nUq = 1\nN\nN\nX\nk=1\nq\nY\nj=1\ne\u03b1j|yk\u2212fj(xk)|/2.\nSince\nwk(q) = 1\nN\nq\u22121\nY\nj=1\ne\u03b1j|yk\u2212fj(xk)|/2,\nwe also have Uq = PN\nk=1 wk(q + 1) = (S+\nW(q + 1) + S\u2212\nW(q + 1))/2.\nWe will use the inequality 1\ne\u03b1t \u22641 \u2212(1 \u2212e\u03b1)t,\n1This inequality is clear for \u03b1 = 0. Assuming \u03b1 , 0, the difference between the upper and lower\nbound is\nq(t) = 1 \u2212e\u03b1t \u2212(1 \u2212e\u03b1)t.\nThe function q is concave (its second derivative is \u2212\u03b12e\u03b1t) with q(0) = q(1) = 0 and therefore non-\nnegative over [0,1].\n226\nCHAPTER 10. TREE-BASED ALGORITHMS\nwhich is true for all \u03b1 \u2208R and t \u2208[0,1], to write\nUq \u22641\nN\nN\nX\nk=1\nq\u22121\nY\nj=1\ne\u03b1j|yk\u2212fj(xk)|/2(1 \u2212(1 \u2212e\u03b1q)|yk \u2212fq(xk)|/2)\n=\nN\nX\nk=1\nwk(q)(1 \u2212(1 \u2212e\u03b1q)|yk \u2212fq(xk)|/2)\n=\nN\nX\nk=1\nwk(q) \u2212(1 \u2212e\u03b1q)\nN\nX\nk=1\nwk(q)|yk \u2212fq(xk)|/2\n= Uq\u22121(1 \u2212(1 \u2212e\u03b1q)\u03f5q)\nThis gives (using U0 = 1)\nUM \u2264\nM\nY\nj=1\n\u0012\n1 \u2212(1 \u2212e\u03b1j\n\u0013\n\u03f5j)\nand\nET \u2264\nM\nY\nj=1\n\u0012\n1 \u2212(1 \u2212e\u03b1j)\u03f5j\n\u0013\ne\u2212\u03b1j/2.\nIt now suffices to replace e\u03b1j by (1 \u2212\u03f5j)\u03c1\u03f5\u2212\u03c1\nj\nand note that\n\u0012\n1 \u2212(1 \u2212(1 \u2212\u03f5j)\u03c1\u03f5\u2212\u03c1\nj )\u03f5j\n\u0013\n(1 \u2212\u03f5j)\u2212\u03c1/2\u03f5\u03c1/2\nj\n= \u03f5\u03c1\nj (1 \u2212\u03f5j)1\u2212\u03c1 + \u03f51\u2212\u03c1\nj\n(1 \u2212\u03f5j)\u03c1\nto conclude the proof.\n\u25a0\nFor \u03f5 \u2208[0,1], one has\n\u03f5\u03c1(1 \u2212\u03f5)1\u2212\u03c1 + \u03f51\u2212\u03c1(1 \u2212\u03f5)\u03c1 = 1 \u2212(\u03f5\u03c1 \u2212(1 \u2212\u03f5)\u03c1)(\u03f51\u2212\u03c1 \u2212(1 \u2212\u03f5)\u22121\u2212\u03c1) \u22641\nwith equality if and only if \u03f5 = 1/2, so that each term in the upper-bound reduces\nthe error unless the corresponding base classifier does not perform better than pure\nchance. The parameter \u03c1 determines the level at which one increases the importance\nof misclassified examples for the next step. Let \u02dcS+\nW(j) and \u02dcS\u2212\nW(j) denote the expres-\nsions in (10.3a) and (10.3b) with wk(j) replaced by wk(j + 1). Then, in the case when\nthe base classifiers are binary, ensuring that |yk \u2212fj(xk)|/2 = 1yk,fj(xk), one can easily\ncheck that \u02dcS+\nW(j)/ \u02dcS\u2212\nW(j) = (S+\nW(j)/S\u2212\nW(j))1\u2212\u03c1. So, the ratio is (of course) unchanged if\n\u03c1 = 0, and pushed to a pure chance level if \u03c1 = 1. We provide below an interpretation\nof boosting as a greedy optimization procedure that will lead to the value \u03c1 = 1/2.\n10.4. ADABOOST\n227\n10.4.3\nAdaboost and greedy gradient descent\nWe here restrict to the case of binary base classifiers and denote their linear combi-\nnation by\nh(x) =\nM\nX\nj=1\n\u03b1jfj(x).\nWhether an observation x is correctly classified in the true class y is associated to\nthe sign of the product yh(x), but the value of this product also has an important\ninterpretation, since, when it is positive, it can be thought of as a margin with which\nx is correctly classified.\nAssume that the function F is evaluated, not only on the basis of its classification\nerror, but also based on this margin, using a loss function of the kind\n\u03a8(h) =\nN\nX\nk=1\n\u03c8(ykh(xk))\n(10.4)\nwhere \u03c8 is decreasing. The boosting algorithm can then be interpreted as an classi-\nfier which incrementally improves this objective function.\nLet, for j < M,\nh(j) =\nj\nX\nq=1\n\u03b1qfq .\nThe next combination h(j+1) is equal to h(j) +\u03b1j+1fj+1, and we now consider the prob-\nlem of minimizing, with respect to fj+1 and \u03b1j+1, the function \u03a8(h(j+1)), without\nmodifying the previous classifiers (i.e., performing a greedy optimization). So, we\nwant to minimize, with respect to the base classifier \u02dcf and to \u03b1 \u22650, the function\nU(\u03b1, \u02dcf ) =\nN\nX\nk=1\n\u03c8\n\u0010\nykh(j)(xk) + \u03b1yk \u02dcf (xk)\n\u0011\nUsing the fact that \u02dcf is a binary classifier, this can be written\nU(\u03b1, \u02dcf ) =\nN\nX\nk=1\n\u03c8(ykh(j)(xk) + \u03b1)1yk= \u02dcf (xk) +\nN\nX\nk=1\n\u03c8(ykh(j)(xk) \u2212\u03b1)1yk, \u02dcf (xk)\n(10.5)\n=\nN\nX\nk=1\n(\u03c8(ykh(j)(xk) \u2212\u03b1) \u2212\u03c8(ykh(j)(xk) + \u03b1))1yk, \u02dcf (xk)\n+\nN\nX\nk=1\n\u03c8(ykh(j)(xk) + \u03b1).\n228\nCHAPTER 10. TREE-BASED ALGORITHMS\nThis shows that \u03b1 and \u02dcf have inter-dependent optimality conditions. For a given\n\u03b1, the best classifier \u02dcf must minimize a weighted empirical error with non-negative\nweights (since \u03c8 is decreasing)\nwk = \u03c8(ykh(j)(xk) \u2212\u03b1) \u2212\u03c8(ykh(j)(xk) + \u03b1).\nGiven \u02dcf , \u03b1 must minimize the expression in (10.5). One can use an alternative min-\nimization procedure to optimize both \u02dcf (as a weighted basic classifier) and \u03b1. How-\never, for the special choice \u03c8(t) = e\u2212t, this optimization turns out to only require one\nstep.\nIn this case, we have\nU(\u03b1, \u02dcf ) =\nN\nX\nk=1\n(e\u03b1 \u2212e\u2212\u03b1)e\u2212ykh(j)(xk)1yk, \u02dcf (xk) + e\u2212\u03b1\nN\nX\nk=1\ne\u2212ykh(j)(xk)\n= e\u2212\u03b1(j)(e\u03b1 \u2212e\u2212\u03b1)\nN\nX\nk=1\nwk(j)1yk, \u02dcf (xk) + e\u2212\u03b1(j)e\u2212\u03b1\nN\nX\nk=1\nwk(j)\nwith wk(j+1) = e\u03b1(j)\u2212ykh(j)(xk) and \u03b1(j) = \u03b11+\u00b7\u00b7\u00b7+\u03b1j. This shows that \u02dcf should minimize\nN\nX\nk=1\nwk(j + 1)1yk, \u02dcf (xk).\nWe note that\nwk(j + 1) = wk(j)e\u03b1j(1\u2212ykfj(xk)) = wk(j)e\u03b1j|yk\u2212fk(xk)|,\nwhich is identical to the weight updates in algorithm Algorithm 10.4 (this is the\nreason why the term \u03b1(j) was introduced in the computation). The new value of \u03b1\nmust minimize (using the notation of Algorithm 10.4)\ne\u2212\u03b1S+\nW(j) + e\u03b1S\u2212\nW(j),\nwhich yields \u03b1 = 1\n2 logS+\nW(j)/S\u2212\nW(j). This is the value \u03b1j+1 in Algorithm 10.4 with\n\u03c1 = 1/2.\n10.5\nGradient boosting and regression\n10.5.1\nNotation\nThe boosting idea, and in particular its interpretation as a greedy gradient proce-\ndure, can be extended to non-linear regression problems [75]. Let us denote by F0\n10.5. GRADIENT BOOSTING AND REGRESSION\n229\nthe set of base predictors, therefore functions from RX = Rd to RY = Rq, since we\nare considering regression problems. The final predictor is a linear combination\nF(x) =\nM\nX\nj=1\n\u03b1jfj(x)\nwith \u03b11,...,\u03b1M \u2208R and f1,...,fM \u2208F0. Note that the the coefficients \u03b1j are redundant\nwhen the class F0 is invariant by multiplication by a scalar. Replacing if needed F0 by\n{f = \u03b1g,\u03b1 \u2208R,g \u2208F0}, we will assume that this property holds and therefore remove\nthe coefficients \u03b1j from the problem.\nIn accordance with the principle of performing greedy searches, we let\nF(j)(x) =\nj\nX\nq=1\nfq(x),\nand consider the problem of minimizing over f \u2208F0,\nU(f ) =\nN\nX\nk=1\nr(yk,F(j)(xk) + f (xk)),\nwhere T = (x1,y1,...,xN,yN) is the training data and r is the loss function.\n10.5.2\nTranslation-invariant loss\nIn the case, which is frequent in regression, when r(y,y\u2032) only depends on y \u2212y\u2032, the\nproblem is equivalent to minimizing\nU(f ) =\nN\nX\nk=1\nr(yk \u2212F(j)(xk),f (xk)),\ni.e., to let fj+1 be the optimal predictor (in F0 and for the loss r) of the residuals\ny(j)\nk = yk \u2212F(j)(xk). In this case, this provides a conceptually very simple algorithm.\nAlgorithm 10.5 (Gradient boosting for regression with translation-invariant loss)\n\u2022 Let T = (x1,y1,...,xN,yN) be a training set and r a loss function such that r(y,y\u2032)\nonly depends on y \u2212y\u2032.\n\u2022 Let F0 be a function class such that f \u2208F0 \u21d2\u03b1f \u2208F0 for all \u03b1 \u2208R.\n\u2022 Select an integer M > 0 and let F(0) = 0, y(0)\nk\n= yk, k = 1,...,N.\n\u2022 For j = 1,...,M:\n230\nCHAPTER 10. TREE-BASED ALGORITHMS\n(1) Find the optimal predictor fj \u2208F0 for the training set (x1,y(j\u22121)\n1\n,...,xN,y(j\u22121)\nN\n).\n(2) Let y(j)\nk = y(j\u22121)\nk\n\u2212fj(xk)\n\u2022 Return F = PM\nk=1 fj.\nRemark 10.6 Obviously, the class F0 should not be a linear class for the boosting\nalgorithm to have any effect. Indeed, if f ,f \u2032 \u2208F0 implies f +f \u2032 \u2208F0, no improvement\ncould be made to the predictor after the first step.\n\u2666\nA successful example of this algorithm uses regression trees as base predictors.\nRecall that the functions output by such trees take the form\nf (x) =\nX\nA\u2208C\nwA1x\u2208A\nwhere C is a finite partition of Rd. Each set in the partition is specified by the value\ntaken by a finite number of binary features (denoted by \u03b3 in our discussion of pre-\ndiction trees) and the maximal number of such features is the depth of the tree. We\nassume that the set \u0393 of binary features is shared by all regression trees in F0, and\nthat the depth of these trees is bounded by a fixed constant. These restrictions pre-\nvent F0 from forming a linear class.2 Note that the maximal depth of tree learnable\nfrom a finite training set is always bounded, since such trees cannot have more nodes\nthan the size of the training set (but one may want to restrict the maximal depth of\nbase predictors to be way less than N).\n10.5.3\nGeneral loss functions\nWe now consider situations in which the loss function is not necessarily a function\nof the difference between true and predicted output. We are still interested in the\nproblem of minimizing U(f ), but we now approximate this problem using the first-\norder expansion\nU(f ) =\nN\nX\nk=1\nr(yk,F(j)(xk)) +\nN\nX\nk=1\n\u22022r(yk,F(j)(xk))T f (xk) + o(f ),\nwhere \u22022r denotes the derivative of r with respect to its second variable. This sug-\ngests (similarly to gradient descent) to choose f such that f (xk) = \u2212\u03b1\u22022r(yk,F(j)(xk))\n2If f and g are representable as trees, f + g can be represented as a tree whose depth is the sum as\nthose of the original trees, simply by inserting copies of g below each leaf of f .\n10.5. GRADIENT BOOSTING AND REGRESSION\n231\nfor some \u03b1 > 0 and all k = 1,...,N. However, such an f may not exist in the class F0,\nand the next best choice is to pick f = \u03b1 \u02dcf with \u02dcf minimizing\nN\nX\nk=1\n| \u02dcf (xk) + \u22022r(yk,F(j)(xk))|2\nover all \u02dcf \u2208F0. This is similar to projected gradient descent in optimization, and \u03b1\nsuch that f = \u03b1 \u02dcf should minimize\nN\nX\nk=1\nr(yk,F(j)(xk) + \u03b1 \u02dcf (xk)).\nThis provides a generic \u201cgradient boosting\u201d algorithm [75], summarized below.\nAlgorithm 10.6 (Gradient boosting)\n\u2022 Let T = (x1,y1,...,xN,yN) be a training set and r a differentiable loss function.\n\u2022 Let F0 be a function class such that f \u2208F0 \u21d2\u03b1f \u2208F0 for all \u03b1 \u2208R.\n\u2022 Select an integer M > 0 and let F(0) = 0.\n\u2022 For j = 1,...,M:\n(1) Find \u02dcfj \u2208F0 minimizing\nN\nX\nk=1\n| \u02dcf (xk) + \u22022r(yk,F(j\u22121)(xk))|2\nover all \u02dcf \u2208F0.\n(2) Let fj = \u03b1j \u02dcfj where \u03b1j minimizes\nN\nX\nk=1\nr(yk,F(j\u22121)(xk) + \u03b1 \u02dcfj(xk)).\n(3) Let F(j) = F(j\u22121) + fj.\n\u2022 Return F = F(M).\nRemark 10.7 Importantly, the fact that F0 is stable by scalar multiplication implies\nthat the function \u02dcfj satisfies\nN\nX\nk=1\n\u02dcf (xk)T \u22022r(yk,F(j\u22121)(xk)) \u22640,\n\u2666\nthat is, excepted in the unlikely case in which the above sum is zero, it is a direction\nof descent for the function U (because one could otherwise replace \u02dcfj by \u2212\u02dcfj and\nimprove the approximation of the gradient).\n232\nCHAPTER 10. TREE-BASED ALGORITHMS\n10.5.4\nReturn to classification\nA slight modification of this algorithm may also be applied to classification, pro-\nvided that the classifier f is obtained by learning the conditional distribution, de-\nnoted g 7\u2192p(g|x), of the output variable (assumed to take values in a finite set RY)\ngiven the input (assumed to take values in RX = Rd).\nOur goal is to estimate an unknown target conditional distribution, \u00b5, therefore\ntaking the form \u00b5(g|x) for g \u2208RY and x \u2208Rd. We assume that a family \u00b5k,k =\n1,...,N of distributions on the set RY is observed, where each \u00b5k is assumed to be\nan approximation of the unknown \u00b5(\u00b7|xk) (typically, \u00b5k(g) = 1g=yk, i.e., \u00b5k = \u03b4yk). The\nrisk function must take the form r(\u00b5,\u00b5\u2032) where \u00b5,\u00b5\u2032 \u2208S(RY), the set of probability\ndistributions on RY. We will work with\nr(\u00b5,\u00b5\u2032) = \u2212\nX\ng\u2208RY\n\u00b5(g)log\u00b5\u2032(g).\nOne can note that\nr(\u00b5,\u00b5\u2032) = KL(\u00b5\u2225\u00b5\u2032) + r(\u00b5,\u00b5),\nwhich is therefore minimal when \u00b5\u2032 = \u00b5. Moreover, in the special case \u00b5k = \u03b4yk, the\nempirical risk is\n\u02c6R(p) =\nN\nX\nk=1\nr(\u00b5k,p(\u00b7|xk)) = \u2212\nN\nX\nk=1\nlogp(yk|xk),\nso that minimizing it is equivalent to maximizing the conditional likelihood that was\nused for logistic regression.\nBefore applying the previous algorithm, one must address the issue that prob-\nability distributions do not form a vector space, and cannot be added to form new\nprobability distributions. In Friedman [75], Hastie et al. [87], it is suggested to use\nthe representation, which can be associated with any function F : (g,x) 7\u2192F(g|x) \u2208R,\npF(g|x) =\neF(g|x)\nP\nh\u2208RY eF(h|x).\nBecause the representation if not unique (pF = pF\u2032 if F \u2212F\u2032 only depends on x), we\nwill require in addition that\nX\nh\u2208RY\nF(h|x) = 0\nfor all x \u2208Rd. The space formed by such functions F is now linear, and we can\nconsider the empirical risk\n\u02c6R(F) = \u2212\nN\nX\nk=1\nX\ng\u2208RY\n\u00b5k(g)logpF(g|xk) = \u2212\nN\nX\nk=1\nX\ng\u2208RY\n\u00b5k(g)F(g|xk) +\nN\nX\nk=1\nlog\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\nX\ng\u2208RY\neF(g|xk)\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8.\n10.5. GRADIENT BOOSTING AND REGRESSION\n233\nOne can evaluate the derivative of this risk with respect to a change on F(g|xk),\nand a short computation gives\n\u2202R\n\u2202F(g|xk) = \u2212\nN\nX\nk=1\n(\u00b5k(g) \u2212pF(g|xk)).\nNow assume that a basic space F0 of functions f : (g,x) 7\u2192f (g|x) is chosen, such\nthat all function in F0 satisfy\nX\ng\u2208RY\nf (g|x) = 0\nfor all x \u2208Rd. The gradient boosting algorithm then requires to minimize (in Step\n(1)):\nN\nX\nk=1\nX\ng\u2208RY\n(\u00b5k(g) \u2212pF(j\u22121)(g|xk) \u2212\u02dcf (g|xk))2\nwith respect to all functions \u02dcf \u2208F0. Given the optimal \u02dcfj, the next step requires to\nminimize, with respect to \u03b1 \u2208R:\n\u2212\u03b1\nN\nX\nk=1\nX\ng\u2208RY\n\u00b5k(g) \u02dcfj(g|xk) +\nN\nX\nk=1\nlog\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\nX\ng\u2208RY\neF(j\u22121)(g|xk)+\u03b1 \u02dcfj(g|xk)\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8.\nThis is a scalar convex problem that can be solved, e.g., using gradient descent.\n10.5.5\nGradient tree boosting\nWe now specialize to the situation in which the set F0 contains regression trees. In\nthis situation, the general algorithm can be improved by taking advantage of the fact\nthat the predictors returned by such trees are piecewise constant functions, where\nthe regions of constancy are associated with partitions C of Rd defined by the leaves\nof the trees. In particular, \u02dcfj(x) in Step (1) takes the form\n\u02dcfj(g|x) =\nJ\nX\nA\u2208C\n\u02dcfj,A(g)1x\u2208A.\nThe final f at Step (2) should therefore take the form\nX\nA\u2208C\n\u03b1 \u02dcfj,A(g)1x\u2208A\n234\nCHAPTER 10. TREE-BASED ALGORITHMS\nbut not much additional complexity is introduced by freely optimizing the values of\nfj on A, that is, by looking at f in the form\nX\nA\u2208C\nfj,A(g)1x\u2208A\nwhere the values fj,A(g) optimize the empirical risk. This risk becomes\n\u2212\nN\nX\nk=1\nX\nA\u2208C\nX\ng\u2208RY\n\u00b5k(g)fj,A(g)1xk\u2208A +\nN\nX\nk=1\nX\nA\u2208C\nlog\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\nX\ng\u2208RY\neF(j\u22121)(g|xk)+fj,A(g)\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f81xk\u2208A.\nThe values fj,A(g),g \u2208RY can therefore be optimized separately, minimizing\n\u2212\nX\nk=1:xk\u2208A\nX\ng\u2208RY\n\u00b5k(g)fj,A(g) +\nX\nk:xk\u2208A\nlog\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\nX\ng\u2208RY\neF(j\u22121)(g|xk)+fj,A(g)\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f81xk\u2208A.\nThis is still a convex program, which has to be run at every leaf of the optimized\ntree. If computing time is limited (or for large-scale problems), the determination of\nfj,A(g) may be restricted to one step of gradient descent starting at fj,A = 0. A simple\ncomputation indeed shows that the first derivative of the function above with respect\nto fj,A(g) is\naA(g) = \u2212\nX\nk:xk\u2208A\n(\u00b5k(g) \u2212pF(g|xk)).\nThe derivative of this expression with respect to fj,A(g) (for the same g) is\nbA(g) =\nX\nk:xk\u2208A\npF(g|xk)(1 \u2212pF(g|xk)).\nThe off-diagonal terms in the second derivative are, for g , h,\n\u2212\nX\nk:xk\u2208A\npF(g|xk)pF(h|xk).\nIn Friedman et al. [74], it is suggested to use an approximate Newton step, where\nthe off-diagonal terms in the second derivative are neglected. This corresponds to\nminimizing\nX\ng\u2208RY\naA(g)fj,A(g) + 1\n2\nX\ng\u2208RY\nbA(g)fj,A(g)2.\nThe solution is (introducing a Lagrange multiplier for the constraint P\ng fj,A(g) = 0)\nfj,A(g) = \u2212aA(g) \u2212\u03bb\nbA(g)\n10.5. GRADIENT BOOSTING AND REGRESSION\n235\nwith\n\u03bb =\nP\ng\u2208RY aA(g)/bA(g)\nP\ng\u2208RY 1/bA(g)\n.\nA small value \u03f5 can be added to bA to avoid divisions by zero. We refer the reader\nto Friedman et al. [74], Friedman [75], Hastie et al. [87] for several variations on this\nbasic idea. Note that an approximate but highly efficient implementation of boosted\ntrees, called XGBoost, has been developed in Chen and Guestrin [53].\n236\nCHAPTER 10. TREE-BASED ALGORITHMS\nChapter 11\nIterated Compositions of Functions and Neural\nNets\n11.1\nFirst definitions\nWe now discuss a class of methods in which the predictor f is built using iterated\ncompositions, with a main application to neural nets. We will structure these mod-\nels using directed acyclic graphs (DAG). These graphs are composed with a set of\nvertexes (or nodes) V = {0,...,m + 1} and a collection L of directed edges i \u2192j be-\ntween some vertexes. If an edge exists between i and j, one says that i is a parent of j\nand j a child of i and will use the notation pa(i) (resp. ch(i)) denote the set of parents\n(resp. children) of i. The graphs we consider must satisfy the following conditions:\n(i) No index is a descendant of itself, i.e., that the graph is acyclic.\n(ii) The only index without parent is i = 0 and the only one without children in\ni = m + 1.\nTo each node i in the graph, one associates a dimension di and a variable zi \u2208Rdi.\nThe root node variable, z0 = x, is the input and zm+1 is the output. One also associates\nto each node i , 0 a function \u03c8i defined on the product space\nN\nj\u2208pa(i)Rdj and taking\nvalues in Rdi. The input-output relation is then defined by the family of equations:\nzi = \u03c8i(zpa(i))\nwhere zpa(i) = (zj,j \u2208pa(i)). Since there is only one root and one terminal node, these\niterations implement a relationship y = zm+1 = f (x), with z0 = x. We will refer to the\nz1,...,zm as the latent variables of the network.\nEach function \u03c8i is furthermore parametrized by an si-dimensional vector wi \u2208\nRsi, so that we will write\nzi = \u03c8i(zpa(i);wi).\n237\n238\nCHAPTER 11. NEURAL NETS\nWe let W denote the vector containing all parameters w1,...,wm+1, which therefore\nhas dimension s = s1 + \u00b7\u00b7\u00b7 + sm+1. The network function f is then parametrized by W\nand we will write y = f (x;W).\n11.2\nNeural nets\n11.2.1\nTransitions\nMost neural networks iterate functions taking the form\n\u03c8i(z;w) = \u03c1(bz + \u03b20),z \u2208Rdj\nwhere b is a di \u00d7 (P\nj\u2208pa(i) dj) matrix and \u03b20 \u2208Rdi (so that w = (b,\u03b20) is si = di(1 +\nP\nj\u2208pa(i) dj)-dimensional); \u03c1 is defined on and takes values in R, and we make the\nabuse of notation, for any d and u \u2208Rd\n\u03c1(u) =\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\n\u03c1(u(1))\n...\n\u03c1(u(d))\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\n.\nThe most popular choice for \u03c1 is the positive part, or ReLU (for rectified linear\nunit), given by \u03c1(t) = max(t,0). Other common choices are \u03c1(t) = 1/(1+e\u2212t) (sigmoid\nfunction), or \u03c1(t) = tanh(z).\nResidual neural networks (or ResNets [89]) are discussed in section 11.6. They\niterate transitions between inputs and outputs of same dimension, taking\nzi+1 = zi + \u03c8(zi;w).\n(11.1)\n11.2.2\nOutput\nThe last node of the graph provides the prediction, y. Its expression depends on the\ntype of predictor that is learned\n\u2022 For regression, y can be chosen as an affine function of is its parents: zm+1 =\nbzpa(m+1) + a0.\n\u2022 For classification, one can also use a linear model zm+1 = bzpa(m+1) + a0 where\nzm+1 is q-dimensional and let the classification be argmax(z(i)\nm+1,i = 1,...,q). Alterna-\ntively, one uses \u201csoftmax\u201d transformation, with\nz(i)\nm+1 =\ne\u03b6(i)\nm+1\nPq\nj=1 e\u03b6(j)\nm+1\nwith \u03b6m+1 = bzpa(m+1) + a0.\n11.3. GEOMETRY\n239\n11.2.3\nImage data\nNeural networks have achieved top performance when working with organized struc-\ntures such as images. A typical problem in this setting is to categorize the content of\nthe image, i.e., return a categorical variable naming its principal element(s). Other\napplications include facial recognition or identification. In this case, the transition\nfunction can take advantage of the 2D structure, with some special terminology.\nInstead of speaking of the total dimension, say, d, of the considered variables,\nwriting z = (z(1),...,z(d)), images are better represented with three indices z(u,v,\u03bb)\nwhere u = 1,...,U and U is the width of the image, v = 1,...,V and V is the height\nof the image, \u03bb = 1,...,\u039b and \u039b is the depth of the image. (With this notation\nd = UV \u039b.) Typical images have length and width of one or two hundred pixels,\nand depth \u039b = 3 for the three color channels. This three-dimensional structure is\nconserved also for latent variables, with different dimensions. Deep neural networks\noften combine compression in width and height with expansion in depth while tran-\nsitioning from input to output.\nThe linear transformation b mapping one layer with dimensions Ui,Vi,\u039bi to an-\nother with dimensions Ui+1,Vi+1,\u039bi+1 is then preferably seen as a collection of num-\nbers: b(u\u2032,v\u2032,\u03bb\u2032,u,v,\u039b) so that the transition from zi to zi+1 is given by\nzi+1(u\u2032,v\u2032,\u03bb\u2032) = \u03c1\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\u03b20(u\u2032,v\u2032,\u03bb\u2032) +\nUi\nX\nu=1\nVi\nX\nv=1\n\u039bi\nX\n\u03bb=1\nb(u\u2032,v\u2032,\u03bb\u2032,u,v,\u03bb)zi(u,v,\u03bb)\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8.\nFor images, it is often preferable to use convolutional transitions, providing con-\nvolutional neural networks ([116, 115], or CNNs. If Ui = Ui+1 and Vi = Vi+1, such\na transition requires that b(u\u2032,v\u2032,\u03bb\u2032,u,v,\u03bb) only depends on \u03bb, \u03bb\u2032 and on the differ-\nences u\u2032 \u2212u and v \u2212v\u2032. In general, one also requires that b(u\u2032,v\u2032,\u03bb\u2032,u,v,\u03bb) is non-zero\nonly if |u\u2032\u2212u| and |v\u2032\u2212v| are both less than a constant, typically a small number. Also,\nthere is generally little computation across depths: each output at depth \u03bb\u2032 only uses\nvalues from a single input depth. These restrictions obviously reduce dramatically\nthe number of free parameters involved in the transition.\nAfter one or a few convolutions, the dimension is often reduced by a \u201cpooling\u201d\noperation, dividing the image into small non-overlapping windows and replacing\neach such window by a single value, either the max (max-pooling) or the average.\n11.3\nGeometry\nIn addition to the transitions between latent variables and resulting changes of di-\nmension, the structure of the DAG defining the network is an important element in\n240\nCHAPTER 11. NEURAL NETS\nx\nz1\n...\nzm\ny\nFigure 11.1: Linear net with increasing layer depths and decreasing layer width.\nx\nz1\nzm\nz2m\u22121\ny\nFigure 11.2: A sketch of the U-net architecture designed for image segmentation [168].\nthe design of a neural net. The simplest choice is a purely linear structure (as shown\nin Figure 11.1), as was, for example, used for image categorization in [110].\nMore complex architectures have been introduced in recent years. Their design\nis in a large part heuristic and based on an analysis of the kind of computation\nthat should be done in the network to perform a particular task. For example, an\narchitecture used for image segmentation in summarized in fig. 11.2.\nAn important feature of neural nets is their modularity, since \u201csimple\u201d architec-\ntures can be combined (e.g., by placing the output of a network as input of another\none) and form a more complex network that still follows the basic structure defined\nabove. One example of such a building block is the \u201cattention module,\u201d which take\nas input three vectors Q,K,V (for query, key, and value) and return\nsoftmax(QKT )V .\nThese modules are fundamental elements of \u201ctransformer networks\u201d [198], that are\nused, among other tasks, for automatic translation.\n11.4. OBJECTIVE FUNCTION\n241\n11.4\nObjective function\n11.4.1\nDefinitions\nWe now return to the general form of the problem, with variables z0,...,zm+1 satis-\nfying\nzi = \u03c8i(zpa(i);wi)\nLet T = (x1,y1,...,xN,yN) denote the training data.\nFor regression problems, the objective function minimized by the algorithm is\ntypically the empirical risk, the simplest choice being the mean square error, which\ngives\nF(W) = 1\nN\nN\nX\nk=1\n|yk \u2212zk,m+1(W)|2.\nwith zk;m+1(W) = f (xk;W).\nFor classification, with the dimension of the output variable equal to the number\nof classes and the decision based on the largest coordinate, one can take (letting\nzk,m+1(i;W) denote the ith coordinate of zk,m+1(W)):\nF(W) = 1\nN\nN\nX\nk=1\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\u2212zk,m+1(yk;W) + log\n\u0012\nq\nX\ni=1\nexp(zk,m+1(i;W))\n\u0013\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8.\nThis objective function is similar to that minimized in logistic regression.\n11.4.2\nDifferential\nGeneral computation.\nThe computation of the differential of F with respect to W\nmay look daunting, but it has actually a simple structure captured by the back-\npropagation algorithm. Even if programming this algorithm can often be avoided\nby using an automatic differentiation software, it is important to understand how it\nworks, and why the implementation of gradient-descent algorithms remains feasi-\nble.\nConsider the general situation of minimizing a function G(W,z) over W \u2208Rs\nand z \u2208Rr, subject to a constraint \u03b3(W,z) = 0 where \u03b3 is defined on Rs \u00d7 Rr and\ntakes values in Rr (here, it is important that the number of constraints is equal to\nthe dimension of z). We will denote below by \u2202W and \u2202z the derivatives of these\nfunctions with respect to the multi-dimensional variables W and z. We make the\nassumptions that \u2202z\u03b3, which is an r \u00d7r matrix, is invertible, and that the constraints\ncan be solved to express z as a function of W, that we will denote Z(W).\n242\nCHAPTER 11. NEURAL NETS\nThis allows us to define the function F(W) = G(W,Z(W)) and we want to com-\npute the gradient of F. (Clearly, the function F in the previous section satisfies these\nassumptions). Taking h \u2208Rs, we have\ndF(W)h = \u2202WGh + \u2202zGdZh.\nMoreover, since \u03b3(W,Z) = 0 by definition of Z, we have\n\u2202W\u03b3h + \u2202z\u03b3 dZh = 0,\nso that\ndF(W)h = \u2202WGh \u2212\u2202zG\u2202z\u03b3\u22121 \u2202W\u03b3h.\nLet p \u2208Rr be the solution of the linear system\n\u2202z\u03b3T p = \u2202zGT .\nThen,\ndF(W)h = (\u2202WG \u2212pT \u2202W\u03b3)h\nor\n\u2207F = \u2202WGT \u2212\u2202W\u03b3T p.\nNote that, introducing the \u201cHamiltonian\u201d\nH(p,z,W) = pT \u03b3(W,z) \u2212G(W,z),\none can summarize the previous computation with the system\n\uf8f1\uf8f4\uf8f4\uf8f4\uf8f4\uf8f2\uf8f4\uf8f4\uf8f4\uf8f4\uf8f3\n\u2202pH = 0\n\u2202zH = 0\n\u2207F = \u2212\u2202WHT .\nApplication: back-propagation.\nIn our case, we are minimizing a function of the\nform\nG(W,z1,...,zN) = 1\nN\nN\nX\nk=1\nr(yk,zk,m+1)\nsubject to constraints zk,i+1 = \u03c8i(zk,pa(i);wi), i = 0,...,m, zk,0 = xk. We focus on one\nof the terms in the sum, therefore fixing k, that we will temporarily drop from the\nnotation.\n11.4. OBJECTIVE FUNCTION\n243\nSo, we evaluate the gradient of G(W,z) = r(y,zm+1) with zi+1 = \u03c8i(zpa(i);wi), i =\n0,...,m, z0 = x. With the notation of the previous paragraph, we take \u03b3 = (\u03b31,...,\u03b3m+1)\nwith\n\u03b3i(W,z) = \u03c8i(zpa(i);wi) \u2212zi\nThese constraints uniquely define z as a function of W, which was one of our as-\nsumptions. For the derivative, we have, for u = (u1,...,um+1) \u2208Rr (with r = d1 + \u00b7\u00b7\u00b7 +\ndm+1, ui \u2208Rdi), and for i = 1,...,m + 1\n\u2202z\u03b3iu =\nX\nj\u2208pa(i)\n\u2202zj\u03c8i(zpa(i);wi)uj \u2212ui\nTaking p = (p1,...,pm+1) \u2208Rr, we get\npT \u2202z\u03b3u =\nm+1\nX\ni=1\nX\nj\u2208pa(i)\npT\ni \u2202zj\u03c8i(zpa(i);wi)uj \u2212\nm+1\nX\ni=1\npT\ni ui\n=\nm+1\nX\nj=1\nX\ni\u2208ch(j)\npT\ni \u2202zj\u03c8i(zpa(i);wi)uj \u2212\nm+1\nX\nj=1\npT\nj uj\nThis allows us to identify \u2202z\u03b3T p as the vector g = (g1,...,gm+1) with\ngj =\nX\ni\u2208ch(j)\n\u2202zj\u03c8i(zpa(i);wi)T pi \u2212pj.\nFor j = m + 1 (which has no children), we get gm+1 = \u2212pm+1, so that the equation\n\u2202z\u03b3T p = g can be solved recursively by taking pm+1 = \u2212gm+1 and propagating back-\nward, with\npj = \u2212gj +\nX\ni\u2208ch(j)\n\u2202zj\u03c8i(zpa(i);wi)T pi\nfor j = m,...,1.\nTo compute the gradient of G, the propagation has to be applied to g = \u2202zG. Since\nG only depends on zm+1, we have gm+1 = \u2202zm+1r(y,zm+1) and gj = 0 for j = 1,...,m.\nMoreover, G does not depend on W, so that \u2202WG = 0. We have\n\u2202W\u03b3i = \u2202wi\u03c8i(zpa(i),wi)\nyielding \u2202W\u03b3T p = (\u03b61,...,\u03b6m) with\n\u03b6j = \u2202wj\u03c8j(zpa(j),wj)T pj.\nWe can now formulate an algorithm that computes the gradient of F with respect\nto W, reintroducing training data indexes in the notation.\n244\nCHAPTER 11. NEURAL NETS\nAlgorithm 11.1 (Back-propagation)\nLet (x1,y1,...,xN,yN) be the training set and Rk(z) = r(yk,z) so that\nF(W) = 1\nN\nN\nX\nk=1\nRk(zk,m+1(W))\nwith zk,m+1(W) = f (xk,W). Let W be a family of weights. The following steps com-\npute \u2207F(W).\n1. For all k = 1,...,N and all i = 1,...,m + 1, compute zk,i(W) (forward computa-\ntion through the network).\n2. Initialize variables pk,m+1 = \u2212\u2207Rk(zk,m+1(W)), k = 1,...,N.\n3. For all k = 1,...,N and all j = 1,...,m, compute pk,j using iterations\npk,j =\nX\ni\u2208ch(j)\n\u2202zj\u03c8i(zk,pa(i),wi)T pk,i.\n4. Let\n\u2207F(W) = \u22121\nN\nN\nX\nk=1\nm+1\nX\ni=1\nDT\ni \u2202wi\u03c8i(zk,pa(i),wi)T pk,i,\nwhere Di is the si \u00d7 s matrix such that Dih = hi.\n11.4.3\nComplementary computations\nThe back-propagation algorithm requires the computation of the gradient of the\ncosts Rk and of the derivatives of the functions \u03c8i, and this can generally be done in\nclosed form, with relatively simple expressions.\n\u2022 If Rk(z) = |yk\u2212z|2 (which is the typical choice for regression models) then \u2207Rk(z) =\n2(z \u2212yk).\n\u2022 In classification, with Rk(z) = \u2212z(yk) + log\n\u0012Pq\ni=1 exp(z(i))\n\u0013\n, one has\n\u2207Rk(z) = \u2212uyk +\nexp(z)\nPq\ni=1 exp(z(i))\nwhere uyk \u2208Rd is the vector with 1 at position yk and zero elsewhere, and exp(z) is\nthe vector with coordinates exp(z(i)), i = 1,...,d.\n11.5. STOCHASTIC GRADIENT DESCENT\n245\n\u2022 For dense transition functions in the form \u03c8(z;w) = \u03c1(bz + \u03b20) with w = (\u03b20,b),\nthen \u2202z\u03c8(z,w) = diag(\u03c1\u2032(\u03b20 + bz))b so that\n\u2202z\u03c8(z,w)T p = bT diag(\u03c1\u2032(\u03b20 + bz))p\n\u2022 Similarly\n\u2202w\u03c8(z,w)T p =\nh\ndiag(\u03c1\u2032(\u03b20 + bz))p,diag(\u03c1\u2032(\u03b20 + bz))pzT i\n.\nNote that neural network packages implement these functions (and more) automat-\nically.\n11.5\nStochastic Gradient Descent\n11.5.1\nMini-batches\nFix \u2113\u226aN. Consider the set of B\u2113of binary sequences \u03be = (\u03be1,...,\u03beN) such that\n\u03bek \u2208{0,1} and PN\nk=1 \u03bek = \u2113. Define\nH(W,\u03be) = \u2207W\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\n1\n\u2113\nN\nX\nk=1\n\u03bekr(yk,f (xk,W))\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8= 1\n\u2113\nN\nX\nk=1\n\u03bek\u2207W r(yk,f (xk,W))\nwhere \u03be follows the uniform distribution on B\u2113. Consider the stochastic approxima-\ntion algorithm:\nWn+1 = Wn \u2212\u03b3n+1H(Wn,\u03ben+1).\n(11.2)\nBecause E(\u03bek) = \u2113/N, we have E(H(W,\u03be)) = \u2207WET (f (\u00b7,W)) and (11.2) provides a\nstochastic gradient descent algorithm to which the discussion in section 3.3 applies.\nSuch an approach is often referred to as \u201cmini-batch\u201d selection in the deep-learning\nliterature, since it correspond to sampling \u2113examples from the training set without\nreplacement and only computing the gradient of the empirical loss computed from\nthese examples.\n11.5.2\nDropout\nIntroduced for deep learning in Srivastava et al. [181], \u201cdropout\u201d is a learning paradigm\nthat brings additional robustness (and, maybe, reduces overfitting risks) to mas-\nsively parametrized predictors.\nAssume that a random perturbation mechanism of the model parameters has\nbeen designed. We will represent it using a random variable \u03b7 (interpreted as noise)\nand a transformation W \u2032 = \u03d5(W,\u03b7) describing how \u03b7 affects a given weight con-\nfiguration W to form a perturbed one W \u2032. In order to shorten notation, we will\n246\nCHAPTER 11. NEURAL NETS\nwrite \u03d5(W,\u03b7) = \u03b7 \u00b7 W, borrowing the notation for a group action from group the-\nory. As a typical example, \u03b7 can be chosen as a vector of Bernoulli random variables\n(therefore taking values in {0,1}), with same dimension as W and one can simply let\n\u03b7 \u00b7W = \u03b7 \u2299W be the pointwise multiplication of the two vectors. This corresponds to\nreplacing some of the parameters by zero (\u201cdropping them out\u201d) while keeping the\nothers unchanged. One generally preserves the parameters of the final layer (gm), so\nthat the corresponding \u03b7\u2019s are equal to one, and let the other ones be independent,\nwith some probability p of being one, say, p = 1/2.\nReturning to the general case, in which \u03b7 is simply assumed to be a random\nvariable with known probability distribution, the dropout method replaces the ob-\njective function F(W) = ET (f (\u00b7,W)) by its expectation over perturbed predictors\nG(W) = E(ET (f (\u00b7,\u03b7 \u00b7 W))) where the expectation is taken with respect to the random\nvariable \u03b7. While this expectation cannot be computed explicitly, its minimization\ncan be performed using stochastic gradient descent, with\nWn+1 = Wn \u2212\u03b3n+1L(Wn,\u03b7n+1),\nwhere \u03b71,\u03b72,... is a sequence of independent realizations of \u03b7 and\nL(W,\u03b7) = \u2207W (ET (f (\u00b7,\u03b7 \u00b7 W))) .\nThen, averaging in \u03b7\n\u00afL(W) = E(\u2207WF(\u03b7 \u2299W)) = \u2207G(W).\nIn the special case where \u03b7 \u00b7 W is just pointwise multiplication, then\nL(W,\u03b7) = \u03b7 \u2299\u2207F(\u03b7 \u2299W).\nSo this quantity can be evaluated by using back-propagation to compute \u2207F(\u03b7 \u00b7 W)\nand multiplying the result by \u03b7 pointwise. Obviously, random weight perturba-\ntion can be combined with mini-batch selection in a hybrid stochastic gradient de-\nscent algorithm, the specification of which being left to the reader. We also note\nthat stochastic gradient descent in neural networks is often implemented using the\nADAM algorithm (section 3.3.3).\n11.6\nContinuous time limit and dynamical systems\n11.6.1\nNeural ODEs\nEquation (11.1) expresses the difference of the input and output of a neural transi-\ntion as a non-linear function f (z;w) of the input. This strongly suggests passing to\n11.6. CONTINUOUS TIME LIMIT AND DYNAMICAL SYSTEMS\n247\ncontinuous time and replacing the difference by a derivative, i.e., replacing the neu-\nral network by a high-dimensional parametrized dynamical system. The continuous\nmodel then takes the form [52]\n\u2202tz(t) = \u03c8(z(t);w(t))\n(11.3)\nwhere t varies in a a fixed interval, say, [0,T ]. The whole process is parametrized by\nW = (w(t),t \u2208[0,T ]). We need to assume existence and uniqueness of solutions of\n(11.3), which usually restricts the domain of admissibility of parameters W.\nTypical neural transition functions are Lipschitz functions whose constant de-\npend on the weight magnitude, i.e., are such that\n|\u03c8(z,w) \u2212\u03c8(z\u2032,w)| \u2264C(w)|z \u2212z\u2032|\n(11.4)\nwhere C is a continuous function of W. For example, for \u03c8(z,w) = \u03c1(bz + \u03b20), w =\n(b,\u03b20), one can take C(w) = C\u03c1|b|op. The Caratheodory theorem [17] implies that\nsolutions are well-defined as soon as\nZ T\n0\nC(w(t))dt < \u221e.\n(11.5)\nThis is a relatively mild requirement, on which we will return later. Assuming this,\nwe can consider z(T) as a function of the initial value, z(0) = x and of the parameters,\nwriting z(T ) = f (x,W).\nGiven a training set, we consider the problem of minimizing\nF(W) = 1\nN\nN\nX\nk=1\nr(yk,f (xk,W)).\n(11.6)\nThe discussion in section section 11.4.2 applies\u2014formally, at least\u2014to this continu-\nous case, and we can consider the equivalent problem of minimizing\nG(W,z1,...,zN) = 1\nN\nN\nX\nk=1\nr(yk,zk(T))\nwith \u2202tzk(t) = \u03c8(zk(t);w(t)), zk(0) = xk. Once again, we consider each k separately,\nwhich boils down to considering N = 1 and we drop the index k from the notation,\nletting F(W) = r(y,f (x,W)) G(W,z) = r(y,z(T )).\nWe define \u03b3(W,z) to return the function\nt 7\u2192\u03b3(W,z)(t) = \u03c8(z(t);w(t)) \u2212\u2202tz(t).\n248\nCHAPTER 11. NEURAL NETS\nLet p : [0,T ] \u2192Rd. We want to determine the expression of u = \u2202z\u03b3T p, which\nsatisfies\nZ T\n0\nu(t)T \u03b4z(t)dt =\nZ T\n0\np(t)T (\u2202z\u03c8(z(t),w(t))\u03b4z(t) \u2212\u2202t\u03b4z(t))dt\nAfter an integration by parts, the r.h.s. becomes\n\u2212p(T)T \u03b4z(T) +\nZ T\n0\n\u2202tp(t)T \u03b4z(t)dt +\nZ T\n0\np(t)T \u2202z\u03c8(z(t),w(t))\u03b4z(t))dt\nwhich gives\nu(t) = \u2212p(T )\u03b4T + \u2202tp(t) + \u2202z\u03c8(z(t),w(t))T p(t).\nThe equation \u2202z\u03b3T p = \u2202zGT therefore gives\n\u2212p(T )\u03b4T + \u2202tp(t) + \u2202z\u03c8(z(t),w(t))T p(t) = \u22022r(y,z(T ))\u03b4T ,\nso that p satisfies p(T) = \u2212\u22022r(y,z(T)) and\n\u2202tp(t) = \u2212\u2202z\u03c8(z(t),w(t))T p(t).\n(11.7)\nWe have \u2202WG = 0 and v = \u2202W\u03b3T p satisfies\nZ T\n0\nv(t)T \u03b4w(t)dt =\nZ T\n0\np(t)T \u2202w\u03c8(z(t),w(t))\u03b4w(t)dt\nso that\n\u2207F(W) = (t 7\u2192\u2212\u2202w\u03c8(z(t),w(t))T p(t)).\nThis informal derivation (more work is needed to justify the existence of various\ndifferentials in appropriate function spaces) provides the continuous-time version\nof the back-propagation algorithm, which is also known as the adjoint method in\nthe optimal control literature [91, 123]. In that context, z represents the state of\nthe control system, w is the control and p is called the costate, or covector. We\nsummarize the gradient computation algorithm, reintroducing N training samples.\nAlgorithm 11.2 (Adjoint method for neural ODE)\nLet (x1,y1,...,xN,yN) be the training set and Rk(z) = r(yk,z) so that\nF(W) = 1\nN\nN\nX\nk=1\nRk(zk(T ,W))\nwith \u2202tzk = \u03c8(zk,W), zk(0) = xk. Let W be a family of weights. The following steps\ncompute \u2207F(W).\n11.6. CONTINUOUS TIME LIMIT AND DYNAMICAL SYSTEMS\n249\n1. For all k = 1,...,N and all t \u2208[0,T ], compute zk(t,W) (forward computation\nthrough the dynamical system).\n2. Initialize variables pk(T) = \u2212\u2207Rk(zk(T ,W))/N, k = 1,...,N.\n3. For all k = 1,...,N and all j = 1,...,m, compute pk(t) by solving (backwards in\ntime)\n\u2202tpk(t) = \u2212\u2202z\u03c8(zk(t),w(t))T pk(t).\n4. Let, for t \u2208[0,T ],\n\u2207F(W)(t) = \u2212\nN\nX\nk=1\n\u2202w\u03c8(zk(t),w(t))T pk(t).\nOf course, in numerical applications, the forward and backward dynamical sys-\ntems need to be discretized, in time, resulting in a finite number of computation\nsteps. This can be done explicitly (for example using basic Euler schemes), or using\nODE solvers [52] available in every numerical software.\n11.6.2\nAdding a running cost\nOptimal control problems are usually formulated with a \u201crunning cost\u201d that penal-\nizes the magnitude of the control, which in our case is provided by the function\nW : t 7\u2192w(t). Penalties on network weights are rarely imposed with discrete neural\nnetworks, but, as discussed above, in the continuous setting, some assumptions on\nthe function W, such as (11.5), are needed to ensure that the problem is well defined.\nIt is therefore natural to modify the objective function in (11.6) by adding a\npenalty term ensuring the finiteness of the integral in (11.5), taking, for example,\nfor some \u03bb > 0,\nF(W) = \u03bb\nZ T\n0\nC(w(t))2dt +\nN\nX\nk=1\nr(yk,f (xk,W)).\n(11.8)\nThe finiteness of the integral of the squared C(w)2 implies, by Cauchy-Schwartz, the\nintegrability of C(w) itself, and usually leads to simpler computations.\nIf C(w) is known explicitly and is differentiable, the previous discussion and the\nback-propagation algorithm can be adapted with minor modifications for the mini-\nmization of (11.8). The only difference appears in Step 4 of Algorithm 11.2, with\n\u2207F(W)(t) = 2\u03bb\u2207C(w(t)) \u22121\nN\nN\nX\nk=1\n\u2202w\u03c8(zk(t),w(t))T pk(t).\n250\nCHAPTER 11. NEURAL NETS\nComputationally, one should still ensure that C and its gradient are not too costly to\ncompute. If \u03c8(z,w) = \u03c1(bz +\u03b20), w = (b,\u03b20), the choice C(w) = C\u03c1|b|op is valid, but not\ncomputationally friendly. The simpler choice C(w) = C\u03c1|b|2 is also valid, but cruder\nas an upper-bound of the Lipschitz constant. It leads however to straightforward\ncomputations.\nThe addition of a running cost to the objective is important to ensure that any\npotential solution of the problem leads to a solvable ODE. It does not guarantee that\nan optimal solution exists, which is a trickier issue in the continuous setting than\nin the discrete setting. This is an important theoretical issue, since it is needed, for\nexample, to ensure that various numerical discretization schemes lead to consistent\napproximations of a limit continuous problem. The existence of minimizers is not\nknown in general for ODE networks. It does hold, however, in the following non-\nparametric (i.e., weight-free) context that we now describe.\nThe function \u03c8 in the r.h.s. of (11.3), is, for any fixed w, a function that maps\nz \u2208Rd to a vector \u03c8(z,w) \u2208Rd. Such functions are called vector fields on Rd, and the\ncollection \u03c8(\u00b7,w),w \u2208Rs is a parametrized family of vector fields.\nThe non-parametric approach replaces this family of functions by a general vec-\ntor field, v so that the time-indexed parametrized family of vector fields (t 7\u2192\u03c8(\u00b7,w(t)))\nbecomes an unconstrained family (t 7\u2192f (t,\u00b7)). Following the general non-parametric\nframework in statistics, one needs to define a suitable function space for the vector\nfields, and use a penalty in the objective function.\nWe will assume that, at each time, f (t,\u00b7) belongs to a reproducing kernel Hilbert\nspace (RKHS), as introduced in chapter 6. However, because we are considering a\nspace of vector fields rather than scalar-valued functions, we need work with matrix-\nvalued kernels [5], for which we give a definition that generalizes definition 6.1\n(which corresponds to q = 1 below).\nDefinition 11.1 A function K : Rd \u00d7 Rd 7\u2192Mq(R) satisfying\n[K1-vec] K is symmetric, namely K(x,y) = K(y,x)T for all x and y in Rd.\n[K2-vec] For any n > 0, for any choice of vectors \u03bb1,...,\u03bbn \u2208Rq and any x1,...,xn \u2208Rd,\none has\nn\nX\ni,j=1\n\u03bbT\ni K(xi,xj)\u03bbj \u22650.\n(11.9)\nis called a positive (matrix-valued) kernel.\nOne says that the kernel is positive definite if the sum in (6.1) cannot vanish, unless\n(i) \u03bb1 = \u00b7\u00b7\u00b7 = \u03bbn = 0 or (ii) xi = xj for some i , j.\n11.6. CONTINUOUS TIME LIMIT AND DYNAMICAL SYSTEMS\n251\nIf \u03ba is a \u201cscalar kernel\u201d (satisfying definition 6.1), then K(x,y) = \u03ba(x,y)IdRq is a\nmatrix-valued kernel.\nA reproducing kernel Hilbert space of vector-valued functions is a Hilbert space\nH of functions from Rd to Rq such that there exists a reproducing kernel K : Rd \u00d7\nRd 7\u2192Mq(R) with the following properties\n[RKHS1] For all x \u2208Rd and \u03bb \u2208Rq, K(\u00b7,x)\u03bb belongs to H,\n[RKHS2] For all h \u2208H, x \u2208Rd and \u03bb \u2208Rq,\n\u27e8h , K(\u00b7,x)\u03bb\u27e9H = \u03bbT h(x).\nProposition 6.5 remains valid in the for vector-valued RKHS, with the following\nmodifications: \u03bb1,...,\u03bbN and \u03b11,...,\u03b1N are q-dimensional vectors and the matrix\nK(x1,...,xN) is now an Nq \u00d7 Nq block matrix, with q \u00d7 q blocks given by K(xk,xl),\nk,l = 1,...,N.\nReturning to the specification of the nonparametric control problem, we will as-\nsume that a vector-valued RKHS, H, has been chosen, with q = d in definition 11.1.\nWe further assume that elements of H are Lipschitz continuous, with\n|v(z) \u2212v(\u02dcz)| \u2264C\u2225v\u2225H|z \u2212\u02dcz|\n(11.10)\nfor some constant C and all v \u2208H. We note that, for every \u03bb \u2208Rd,\n|\u03bbT (v(z) \u2212v(\u02dcz))|2 = |\u27e8v , K(\u00b7,z)\u03bb \u2212K(\u00b7, \u02dcz)\u03bb\u27e9H|2\n\u2264\u2225v\u22252\nH \u2225K(\u00b7,z)\u03bb \u2212K(\u00b7, \u02dcz)\u03bb\u22252\nH\n= \u2225v\u22252\nH (\u03bbT K(z,z)\u03bb \u22122\u03bbT K(z, \u02dcz)\u03bb + \u03bbT K(\u02dcz, \u02dcz))\n\u2264|\u03bb|2\u2225v\u22252\nH |K(z,z) \u22122K(z, \u02dcz) + K(\u02dcz, \u02dcz)|.\nThis shows that (11.10) can be derived from regularity properties of the kernel,\nnamely, that\n|K(z,z) \u22122K(z, \u02dcz) + K(\u02dcz, \u02dcz)| \u2264C|z \u2212\u02dcz|2\nfor some constant C and all z, \u02dcz \u2208Rd. This property is satisfied by most of the kernels\nthat are used in practice.\nLet \u03b7 : t 7\u2192\u03b7(t) be a function from [0,1] to H. This means that, for each t, \u03b7(t) is\na vector field x 7\u2192\u03b7(t)(x) on Rd, and we will write indifferently \u03b7(t) and \u03b7(t,\u00b7), with\na preference for \u03b7(t,x) rather than \u03b7(t)(x). We consider the objective function\n\u00afF(f ) = \u03bb\nZ T\n0\n\u2225\u03b7(t)\u22252\nHdt + 1\nN\nN\nX\nk=1\nr(yk,zk(1)),\n(11.11)\n252\nCHAPTER 11. NEURAL NETS\nwith \u2202tzk(t) = \u03b7(t,zk(t)), zk(0) = xk. To compare with (11.8), the finite-dimensional\nw \u2208Rs is now replaced with an infinite-dimensional parameter, \u03b7, and the transition\n\u03c8(z,w) becomes \u03b7(z).\nUsing the vector version of proposition 6.5 (or the kernel trick used several times\nin chapters 7 and 8), one sees that there is no loss of generality in replacing \u03b7(t) by\nits projection onto the vector space\nV (t) =\n\uf8f1\uf8f4\uf8f4\uf8f2\uf8f4\uf8f4\uf8f3\nN\nX\nl=1\nK(\u00b7,zl(t))wl : w1,...,wN \u2208Rd\n\uf8fc\uf8f4\uf8f4\uf8fd\uf8f4\uf8f4\uf8fe.\nNoting that, if \u03b7(t) takes the form\n\u03b7(t) =\nN\nX\nl=1\nK(\u00b7,zl(t))wl(t),\nthen\n\u2225\u03b7(t)\u22252\nH =\nN\nX\nk,l=1\nwk(t)T K(zk(t),zl(t))wl(t).\nThis allows us to replace the infinite-dimensional parameter \u03b7 by a family W =\n(w(t),t \u2208[0,T ] with w(t) = (wk(t),k = 1,...,N). The minimization of \u00afF in (11.11) can\nbe replaced by that of\nF(W) = \u03bb\nZ T\n0\nN\nX\nk,l=1\nwk(t)T K(zk(t),zl(t))wl(t)dt + 1\nN\nN\nX\nk=1\nr(yk,zk(1)),\n(11.12)\nwith\n\u2202tzk(t) =\nN\nX\nl=1\nK(zk(t),zl(t))wl(t).\nThis optimal control problem has a similar form to that considered in (11.8),\nwhere the running cost C(w)2 is replaced by a cost that depends on the control (still\ndenoted w) and the state z. The discussion in section section 11.6.1 can be applied\nwith some modifications. Let K(z) be the dN \u00d7 dN matrix formed with d \u00d7 d blocks\nK(zk(t),zl(t)) and w(t) the dN-dimensional vector formed by stacking w1,...,wN. Let\nG(W,z) = \u03bb\nZ T\n0\nw(t)T K(z(t))w(t)dt + 1\nN\nN\nX\nk=1\nr(yk,zk(1))\nand\n\u03b3(W,z)(t) = K(z(t))w(t) \u2212\u2202tz(t).\n11.6. CONTINUOUS TIME LIMIT AND DYNAMICAL SYSTEMS\n253\nThe backward ODE in step 3. of Algorithm 11.2 now becomes\n\u2202tpk(t) = \u2212\u2202zk(w(t)T K(z(t))p(t)) + \u03bb\u2202zk(w(t)T K(z(t))w(t))\nfor k = 1,...,N. Step 4. becomes (for t \u2208[0,T ]),\n\u2207F(W)(t) = K(z(t))(2\u03bb \u2212p(t)).\nThe resulting algorithm was introduced in [209]. It has the interesting prop-\nerty (shared with neural ODE models with smooth controlled transitions) to de-\ntermine an implicit diffeomorphic transformation of the space, i.e., the function\nx 7\u2192f (x;W,z) = \u02dcz(T) which returns the solution at time T of the ODE\n\u2202t \u02dcz(t) =\nN\nX\nl=1\nK(\u02dcz(t),zl(t))wl(t)\n(or \u2202\u02dcz(t) = \u03c8(\u02dcz(t);w(t)) for neural ODEs) is smooth, invertible, with a smooth inverse.\n254\nCHAPTER 11. NEURAL NETS\nChapter 12\nMonte-Carlo Sampling\nThe goal of this section is to describe how, from a basic random number generator\nthat provides samples from a uniform distribution on [0,1], one can generate sam-\nples that follow, or approximately follow, complex probability distributions on finite\nor general spaces. This, combined with the law of large numbers, permits to approx-\nimate probabilities or expectations by empirical averages over a large collection of\ngenerated samples.\nWe assume that as many as needed independent samples of the uniform dis-\ntribution are available, which is only an approximation of the truth. In practice,\ncomputer programs are only able to generate pseudo-random numbers, which are\nhighly chaotic recursive sequences, but still deterministic. Also, these numbers are\ngenerated as integers, which only provide, after normalization, a distribution on a\nfinite discretization of the unit interval. We will neglect these facts, however, and\nwork as if the output of the function random (or any similar name) in a computer\nprogram is a true realization of the uniform distribution.\n12.1\nGeneral sampling procedures\nReal-valued variables.\nWe will use the following notation for the left limit of a\nfunction F at a given point z\nF(z \u22120) =\nlim\ny\u2192z,y<zF(y)\nassuming, of course that this limit exists (which is always true, for example when F is\nnon-decreasing). Recall that F is left continuous if and only if F = F(\u00b7 \u22120). Moreover,\nit is easy to see that F(\u00b7 \u22120) is left-continuous1. Note also that, if F is non-decreasing,\n1For every z and every \u03f5 > 0, there exists z\u2032 < z such that for all z\u2032\u2032 \u2208[z\u2032,z), |F(z\u2032\u2032) \u2212F(z \u22120)| < \u03f5.\nMoreover, taking any y \u2208(z\u2032,z), there exists y\u2032 < y such that for all y\u2032\u2032 \u2208[y\u2032,y), |F(y\u2032\u2032) \u2212F(y \u22120)| < \u03f5.\n255\n256\nCHAPTER 12. MONTE-CARLO SAMPLING\none always has F(z) \u2264F(y \u22120) whenever z < y. The following proposition provides a\nbasic mechanism for Monte-Carlo sampling.\nProposition 12.1 Let Z be a real-valued random variable with c.d.f. FZ. For u \u2208[0,1],\ndefine\nF\u2212\nX(u) = max{z : FZ(z \u22120) \u2264u}.\nLet U be uniformly distributed over [0,1]. Then F\u2212\nZ(U) has the same distribution as Z.\nProof Let Az = {u \u2208[0,1] : F\u2212\nZ(u) \u2264z}. Assume first that u < FZ(z). Then FZ(y\u22120) \u2264u\nimplies that y \u2264z, since y > z would imply that FZ(z) \u2264FZ(y \u22120). This shows that\nsup{z\u2032 : FZ(z\u2032 \u22120) \u2264u} \u2264z, i.e., u \u2208Az.\nNow, take u > FZ(z). Because c.d.f.\u2019s are right continuous, there exists y > z such\nthat u > FZ(y), which implies that F\u2212\nZ(u) \u2265y and u < Az.\nWe have therefore shown that [0,FZ(z)) \u2282Az \u2282[0,FZ(z)]. If U is uniformly dis-\ntributed on [0,1], then P(U < FZ(z)) = P(U \u2264FZ(z)) = FZ(z), showing that\nP(F\u2212\nZ(U) \u2264z) = P(U \u2208Az) = FZ(z).\n\u25a0\nThis proposition shows that one can generate random samples of a real-valued\nrandom variable Z as soon as one can compute F\u2212\nZ and generate uniformly dis-\ntributed variables. Note that, if FZ is strictly increasing, then F\u2212\nZ = F\u22121\nZ , the usual\nfunction inverse.\nThe proposition also shows how to sample from random variables taking values\nin finite sets. Indeed, if Z takes values in e\u2126Z = {z1,...,zn} with pi = P(Z = zi), sam-\npling from Z is equivalent to sampling from the integer valued random variable eZ\nwith P(eZ = i) = pi. For this variable, F\u2212\n\u02dcZ(u) is the largest i such that p1 + \u00b7\u00b7\u00b7 + pi\u22121 \u2264u\n(this sum being zero if i = 1), which provides the standard sampling scheme for\ndiscrete probability distributions.\n12.2\nRejection sampling\nWhile the previous approach can be generalized to multivariate distributions, it\nquickly becomes unfeasible when the dimension gets large, excepting simple cases\nin which the variables are independent, or, say, Gaussian. Rejection sampling is a\nsimple algorithm that allows, in some cases, for the generation of samples from a\ncomplicated distribution based on repeated sampling of a simpler one.\nWithout loss of generality, we can assume that y\u2032 \u2265z\u2032, yielding |F(z \u22120) \u2212F(y \u22120)| \u22642\u03f5, showing the\nleft continuity of F(\u00b7 \u22120).\n12.3. MARKOV CHAIN SAMPLING\n257\nLet us assume that we want to sample from a variable Z taking values RZ, and\nthat there exists a measure \u00b5 on RZ with respect to which the distribution of Z is\nabsolutely continuous, i.e., so that this distribution has a density fZ with respect\nto \u00b5. For example, RZ = Rd, and fZ is the p.d.f. of Z with respect to Lebesgue\u2019s\nmeasire. Assume that g is another density functions (with respect to \u00b5) from which\nit is \u201ceasy\u201d to sample. Consider the following algorithm, which includes a function\na : z 7\u2192a(z) \u2208[0,1] that will be specified later.\nAlgorithm 12.1 (Rejection sampling with acceptance function a and base p.d.f. g)\n(1) Sample a realization z of a random variable with p.d.f. g.\n(2) Generate b \u2208{0,1} with P(b = 1) = a(z).\n(3) If b = 1, return Z = z and exit.\n(4) Otherwise, return to step 1.\nThe probability of exiting at step 3 is \u03c1 =\nR\nRd g(z)a(z)\u00b5(dz). So, the algorithm\nsimulates a random variable with p.d.f.\n\u02dcf (z) = g(z)a(z)(1 + (1 \u2212\u03c1) + (1 \u2212\u03c1)2 + \u00b7\u00b7\u00b7) = g(z)a(z)\n\u03c1\n.\nAs a consequence, in order to simulate fZ, one must choose a so that fZ(z) is pro-\nportional to g(z)a(z), which, (assuming that g(z) > 0 whenever fZ(z) > 0), requires\nthat a(z) is proportional to fZ(z)/g(z). Since a(z) must take values in [0,1], but should\notherwise be chosen as large as possible to ensure that fewer iterations are needed,\none should take\na(z) = fZ(z)\ncg(z)\nwhere c = max{fZ(z)/g(z) : z \u2208Rd}, which must therefore be finite. This fully specifies\na rejection sampling algorithm for fZ. Note that g is free to choose (with the restric-\ntion that fZ(z)/g(z) must be bounded), and should be selected so that sampling from\nit is easy, and the coefficient c above is not too large.\n12.3\nMarkov chain sampling\nWhen dealing with high-dimensional distributions, the constant c in the previous\nprocedure is typically extremely large, and the rejection-sampling algorithm be-\ncomes unfeasible, because it keeps rejecting samples for very long times. In such\ncases, one can use alternative simulation methods that iteratively updates the vari-\nable Z by making small changes at each step, resulting in a procedure that asymp-\ntotically converges to a sample of the target distribution. Such sampling schemes\n258\nCHAPTER 12. MONTE-CARLO SAMPLING\nare usually described as Markov chains, leading to the name Markov-chain Monte\nCarlo (or MCMC) sampling.\nAssume that we want to sample from a random variable that takes values in\nsome (measurable) set B = RX.2 A Markov chain is the probabilistic analogous of a\nrecursive sequence Xn+1 = \u03a6(Xn), which is fully defined by the function \u03a6 : B \u2192B\nand the initial value X0 \u2208B.\n12.3.1\nDefinitions\nFor Markov chains, X0 is a random variable, which therefore does not have a fixed\nvalue, but follows a probability distribution that we will generally denote \u00b50: P0(x) =\nP(X0 = x). The computation of Xn+1 given Xn is not deterministic either, but given\nthe conditional probabilities\nPn,n+1(x,A) = P(Xn+1 \u2208A | Xn = x).\nwhere A \u2282B is measurable. The left-hand side of this equation, Pn,n+1 is called a\ntransition probability, according to the following definition.\nDefinition 12.2 Let F1 and F2 be two sets equipped with \u03c3-algebras A1 and A2. A\ntransition probability from F1 to F2 is a function p : F1 \u00d7 A2 \u2192[0,1] such that, for all\nx \u2208F1, the function A 7\u2192p(x,A) is a probability on F2 and for all A \u2208A2, the function\nx 7\u2192p(x,A), x \u2208F1, is measurable.\nWhen F2 is discrete, the probabilities are fully specified by their values on singleton\nsets, and we will write p(x,y) for p(x,{y}).\nWhen Pn,n+1(x,\u00b7) does not depend on n, the Markov chain is said to be homoge-\nneous. To simplify notation, we will restrict to homogeneous chains (and therefore\nonly write P(x,A)), although some of the chains used in MCMC sampling may be\ninhomogeneous. This is not a very strong loss of generality, however, because in-\nhomogeneous Markov chains can be considered as homogeneous by extending the\nspace \u2126on which they are defined to \u2126\u00d7 N, and defining the transition probability\n\u02dcp\n\u0010\n(x,n),A \u00d7 {r}\n\u0011\n= 1r=n+1pn,n+1(x,A).\nAn important special case is when B is countable, in which case one only needs\nto specify transition probabilities for singletons A = {y}, and we will write\np(x,y) = P(x,{y}) = P(Xn+1 = y | Xn = x)\n2We will assume in this chapter that B is a complete metric space with a dense countable subset,\nwith the associated Borel \u03c3-algebra.\n12.3. MARKOV CHAIN SAMPLING\n259\nfor the p.m.f. associated with P(x,\u00b7).\nAnother simple situation is when B = Rd and each P(x,\u00b7) has a p.d.f. that we will\nalso denote as p(x,\u00b7). In this latter case, assuming that P0 also have a p.d.f. that we\nwill denote by \u00b50, the joint p.d.f. of (X0,...,Xn) on (Rd)n+1 is given by\nf (x0,x1,...,xn) = \u00b50(x0)p(x0,x1)\u00b7\u00b7\u00b7p(xn\u22121,xn).\n(12.1)\nThe same expression holds for the joint p.m.f. in the discrete case.\nIn the general case (invoking measure theory), the joint distribution is also deter-\nmined by the transition probabilities, and we leave the derivation of the expression\nto the reader. An important point is that, in both special cases considered above,\nand under some very mild assumptions in the general case , these transition proba-\nbilities also uniquely define the joint distribution of the infinite process (X0,X1,...)\non B\u221e, which gives theoretical support to the consideration of asymptotic proper-\nties of Markov chains. In this discussion, we are interested in conditions ensuring\nthat the chain asymptotically samples from a target probability distribution Q, i.e.,\nwhether P(Xn \u2208A) converges to Q(A) (one says that Xn converges in distribution to\nQ). In practice, Q is given or modeled, and the goal is to determine the transition\nprobabilities. Note that the marginal distribution of Xn is computed by integrating\n(or summing) (12.1) with respect to x0,...,xn\u22121, which is generally computationally\nchallenging.\nGiven a transition probability P on B, we will use the notation, for a function\nf : B \u2192R:\nPf (x) =\nZ\nB\nf (y)P(x,dy).\nIf Q is a probability distribution on B, it will also be convenient to write\nQf (x) =\nZ\nB\nf (y)Q(dy).\n12.3.2\nConvergence\nWe will denote Px(\u00b7) the conditional distribution P(\u00b7 | X0 = x) and Pn(x,A) = Px(Xn \u2208\nA), which is a probability distribution on B. The goal of Markov Chain Monte Carlo\nsampling is to design the transition probabilities such that Pn\nx (A) converges to Q(A)\nwhen n tends to infinity. One furthermore wants to complete this convergence with\na law of large numbers, ensuring that\n1\nn\nn\nX\nk=1\nf (Xk) \u2192\nZ\nB\nf (x)Q(dx)\n260\nCHAPTER 12. MONTE-CARLO SAMPLING\nwhen n \u2192\u221e, where Xn is the generated Markov chain and f is Q-integrable.\nIntroduce the total variation distance between two probability measures on a\ngiven probability space,\nDvar(\u00b51,\u00b52) = sup\nA\n(\u00b51(A) \u2212\u00b52(A)).\n(12.2)\nwhere the supremum is taken over all measurable sets A.\nWe will say that the\nMarkov chain with transition P asymptotically samples from Q if\nlim\nn\u2192\u221eDvar(Pn(x,\u00b7),Q) = 0\n(12.3)\nfor Q-almost all x \u2208B. The chain must satisfy specific conditions for this to be\nguaranteed.\nWe now discuss some properties of the total variation distance that will be useful\nlater. First, we note that the supremum in the r.h.s. of (12.2) is achieved. Indeed,\nthere exists a set A0 such that, for all measurable sets A, \u00b51(A \u2229A0) \u2265\u00b52(A \u2229A0) and\n\u00b51(A\u2229Ac\n0) \u2264\u00b52(A\u2229Ac\n0). If B is a finite set, it suffices to let A0 = {x \u2208B : \u00b51(x) \u2265\u00b52(x)};\nif both \u00b51 and \u00b52 have p.d.f.\u2019s \u03d51 and \u03d52 with respect to Lebesgue\u2019s measure (with\nB = Rd), then one can take A0 = {x \u2208B : \u03d51(x) \u2265\u03d52(x)}. In the general case, one\ncan take \u00b5 = \u00b51 + \u00b52 so that \u00b51,\u00b52 \u226a\u00b5, and letting \u03d5i = d\u00b5i/d\u00b5, also take A0 = {x \u2208\nB : \u03d51(x) \u2265\u03d52(x)}. (This is also a special case of the Hahn-Jordan decomposition of\nsigned measures [66]).\nNow, it is clear that, for any A\n\u00b51(A) \u2212\u00b52(A) = \u00b51(A \u2229A0) \u2212\u00b52(A \u2229A0) + \u00b51(A \u2229Ac\n0) \u2212\u00b52(A \u2229Ac\n0)\n\u2264\u00b51(A \u2229A0) \u2212\u00b52(A \u2229A0)\n\u2264\u00b51(A \u2229A0) \u2212\u00b52(A \u2229A0) + \u00b51(Ac \u2229A0) \u2212\u00b52(Ac \u2229A0)\n= \u00b51(A0) \u2212\u00b52(A0)\nshowing that\nDvar(\u00b51,\u00b52) = \u00b51(A0) \u2212\u00b52(A0).\nThe following proposition lists additional properties.\nProposition 12.3 (i) If \u00b51,\u00b52 have a densities \u03d51,\u03d52 with respect to some positive mea-\nsure \u00b5 (such as \u00b51 + \u00b52), then\nDvar(\u00b51,\u00b52) = 1\n2\nZ\nB\n|\u03d51(x) \u2212\u03d52(x)|\u00b5(dx).\nIn particular, if B is finite\nDvar(\u00b51,\u00b52) = 1\n2\nX\nx\u2208B\n|\u00b51(x) \u2212\u00b52(x)|.\n12.3. MARKOV CHAIN SAMPLING\n261\n(ii) For general B,\nDvar(\u00b51,\u00b52) = sup\nf\n Z\nB\nf (x)\u00b51(dx) \u2212\nZ\nB\nf (x)\u00b52(dx)\n!\n.\n(12.4)\nwhere the supremum is taken over all measurable functions f taking values in [0,1].\n(iii) If f : B \u2192R is bounded, define the maximal oscillation of f by\nosc(f ) = sup{f (x) \u2212f (y) : x,y \u2208B}.\nThen\nDvar(\u00b51,\u00b52) = sup\n(Z\nB\nf (x)\u00b51(dx) \u2212\nZ\nB\nf (x)\u00b52(dx) : osc(f ) \u22641\n)\n(iv) Conversely, for any bounded measurable f : B \u2192R,\nosc(f ) = sup\n(Z\nB\nf (x)\u00b51(dx) \u2212\nZ\nB\nf (x)\u00b52(dx) : Dvar(\u00b51,\u00b52) \u22641\n)\nProof If one takes A0 = {x \u2208B : \u03d51(x) \u2265\u03d52(x)}, then\nDvar(\u00b51,\u00b52) =\nZ\nA0\n(\u03d51(x) \u2212\u03d52(x))\u00b5(dx) =\nZ\nA0\n|\u03d51(x) \u2212\u03d52(x)|\u00b5(dx).\nBut, because both \u00b51 and \u00b52 are probability measures\nZ\nB\n(\u03d51(x) \u2212\u03d52(x))\u00b5(dx) = 0\nso that\nZ\nAc\n0\n(\u03d51(x) \u2212\u03d52(x))\u00b5(dx) = \u2212\nZ\nA0\n(\u03d51(x) \u2212\u03d52(x))\u00b5(dx).\nHowever, the l.h.s. is also equal to\n\u2212\nZ\nAc\n0\n|\u03d51(x) \u2212\u03d52(x)|\u00b5(dx)\nso that\nZ\nB\n|\u03d51(x) \u2212\u03d52(x)|\u00b5(dx) = 2\nZ\nA0\n(\u03d51(x) \u2212\u03d52(x)) = 2Dvar(\u00b51,\u00b52),\nwhich proves (i).\nTo prove (ii), first notice that, for all A,\n\u00b51(A) \u2212\u00b52(A) =\nZ\nB\nf (x)\u00b51(dx) \u2212\nZ\nB\nf (x)\u00b52(dx)\n262\nCHAPTER 12. MONTE-CARLO SAMPLING\nfor f = 1A, so that\nDvar(\u00b51,\u00b52) \u2264sup\nf\n Z\nB\nf (x)\u00b51(dx) \u2212\nZ\nB\nf (x)\u00b52(dx)\n!\n.\nConversely, using A0 as above, and taking f with values in [0,1]\nZ\nB\nf (x)\u00b51(dx) \u2212\nZ\nB\nf (x)\u00b52(dx) =\nZ\nA0\nf (x)(\u00b51 \u2212\u00b52)(dx) +\nZ\nAc\n0\nf (x)(\u00b51 \u2212\u00b52)(dx)\n\u2264\nZ\nA0\nf (x)(\u00b51 \u2212\u00b52)(dx)\n\u2264\nZ\nA0\n(\u00b51 \u2212\u00b52)(dx) = Dvar(\u00b51,\u00b52)\nThis shows (ii). For (iii), one can note that, if f takes values in [0,1], then osc(f ) \u2264\n1 so that\nDvar(\u00b51,\u00b52) \u2264sup\n(Z\nB\nf (x)\u00b51(dx) \u2212\nZ\nB\nf (x)\u00b52(dx) : osc(f ) \u22641\n)\nConversely, take f such that osc(f ) \u22641, \u03f5 > 0 and y such that f (y) \u2265infxf (x) + \u03f5. Let\nf\u03f5(x) = (f (x) \u2212f (y) + \u03f5)/(1 + \u03f5), which takes values in [0,1]. Then\nDvar(\u00b51,\u00b52) \u2265\nZ\nB\nf\u03f5(x)\u00b51(dx) \u2212\nZ\nB\nf\u03f5(x)\u00b52(dx)\n=\n1\n1 + \u03f5\n Z\nB\nf (x)\u00b51(dx) \u2212\nZ\nB\nf (x)\u00b52(dx)\n!\nand since this is true for all \u03f5 > 0, we get\nZ\nB\nf (x)\u00b51(dx) \u2212\nZ\nB\nf (x)\u00b52(dx) \u2264Dvar(\u00b51,\u00b52)\nwhich completes the proof of (iii).\nUsing (iii), we find, for any \u00b51,\u00b52 and any bounded f\nZ\nB\nf (x)\u00b51(dx) \u2212\nZ\nB\nf (x)\u00b52(dx) \u2264Dvar(\u00b51,\u00b52)osc(f )\nwhich shows that\nsup\n(Z\nB\nf (x)\u00b51(dx) \u2212\nZ\nB\nf (x)\u00b52(dx) : Dvar(\u00b51,\u00b52) \u22641\n)\n\u2264osc(f ).\n12.3. MARKOV CHAIN SAMPLING\n263\nHowever, taking \u00b51 = \u03b4x and \u00b52 = \u03b4y, so that Dvar(\u00b51,\u00b52) = 0 is x = y and 1 otherwise,\nwe get\nf (x) \u2212f (y) =\nZ\nB\nf (x)\u00b51(dx) \u2212\nZ\nB\nf (x)\u00b52(dx)\n\u2264sup\n(Z\nB\nf (x)\u00b51(dx) \u2212\nZ\nB\nf (x)\u00b52(dx) : Dvar(\u00b51,\u00b52) \u22641\n)\nwhich yields (iv) after taking the supremum with respect to x and y.\n\u25a0\n12.3.3\nInvariance and reversibility\nIf a Markov chain converges to Q, then Q must be an \u201cinvariant distribution,\u201d in the\nsense that, if Xn \u223cQ for some n, then so does Xn+1 and as a consequences all Xm for\nn \u2265m. This can be seen by writing\nPn+1(x,A) = Px(Xn+1 \u2208A) = Ex(P(Xn,A)) = EPn(x,\u00b7)(P(\u00b7,A))\nIf Pn(x,\u00b7) (and therefore also Pn+1(x,\u00b7)) converges to Q, then passing to the limit\nabove yields\nQ(A) = EQ(P(\u00b7,A))\nand this states that, if Xn \u223cQ, then so does Xn+1. If Q has a p.d.f. (resp. p.m.f.), say,\nq, this gives\nq(y) =\nZ\nRd p(x,y)q(x)dx,(resp. q(y) =\nX\nx\u2208B\np(x,y)q(x)).\nSo, if one designs a Markov chain with a target asymptotic distribution Q, the first\nthing to ensure is that Q is invariant.\nWhile invariance leads to an integral equation for q, a stronger condition, called\nreversibility is easier to assess.\nAssume that Q is invariant by P. Make the assumption that P(x,\u00b7) has a density\np\u2217with respect to Q (this is, essentially, no loss of generality, see argument below),\nso that\nP(x,A) =\nZ\nA\np\u2217(x,y)Q(dy).\nTaking A = B above, we have\nZ\nB\np\u2217(x,y)Q(dy) = P(x,B) = 1\nbut we also have, because Q is invariant, that\nZ\nB\np\u2217(x,y)Q(dx) = Q(B) = 1.\n264\nCHAPTER 12. MONTE-CARLO SAMPLING\nOne says that the density is \u201cdoubly stochastic\u201d with respect to Q.\nConversely, if a transition probability P has a doubly stochastic density p\u2217with\nrespect to some probability Q on B, then Q is invariant by P, since\nZ\nB\nP(x,A)Q(dx) =\nZ\nB\nZ\nA\np\u2217(x,y)Q(dy)Q(dx)\n=\nZ\nA\nZ\nB\np\u2217(x,y)Q(dx)Q(dy) =\nZ\nA\nQ(dy) = Q(A).\nThe property of being doubly stochastic can be reinterpreted in terms of time\nreversal for Markov chains. Let Q0 be an initial distribution for a Markov chain\nwith transition P (not necessarily invariant) so that, for any n \u22650, the distribution\nof Xn is Qn = Q0Pn. Fixing any m > 0, we are interested in the reversed process\n\u02dcXk = Xm\u2212k. We first notice that the conditional distribution of Xn given its future\nXn+1,...,Xm (with n < m) only depends on Xn+1, so that the reversed process is also\nMarkov. Indeed, for any positive function f : B \u2192R, g : Bm\u2212n \u2192R, one has, using\nthe fundamental properties of conditional expectations and the fact that (Xn) is a\nMarkov chain,\nE(f (Xn)g(Xn+1,...,Xm)) = E(E(f (Xn)g(Xn+1,...,Xm) | Xn,Xn+1))\n= E(f (Xn)E(g(Xn+1,...,Xm) | Xn,Xn+1))\n= E(f (Xn)E(g(Xn+1,...,Xm) | Xn+1))\n= E(E(f (Xn) | Xn+1)E(g(Xn+1,...,Xm) | Xn+1))\n= E(E(f (Xn) | Xn+1)g(Xn+1,...,Xm)).\nThis shows that\nE(f (Xn) | Xn+1,...,Xm) = E(f (Xn) | Xn+1),\nwhich is what we wanted. To identify the conditional distribution of Xn given Xn+1,\nwe note that for any x \u2208B, the transition probability P(x,\u00b7) is absolutely continuous\nwith respect to Qn+1 since\nQn+1(A) =\nZ\nB\nP(x,A)Qn(dx)\nand the r.h.s. is zero only if P(x,A) = 0 Qn-almost everywhere 3. This shows that\nthere exists a function rn+1 : B \u00d7 B \u2192R such that, for all A\nP(x,A) =\nZ\nA\nrn+1(x,y)Qn+1(dy).\n3The \u201calmost everywhere\u201d statement a priori depends on A, but can be made independent of it\nunder the mild assumption (that we will always make) that B has a countable basis of open sets.\n12.3. MARKOV CHAIN SAMPLING\n265\nGiven this point, one can write\nE(f (Xn)g(Xn+1) =\nZ\nB2 f (xn)g(xn+1)P(xn,dxn+1)Qn(dxn)\n=\nZ\nB2 f (xn)g(xn+1)rn+1(xn,xn+1)Qn+1(dxn+1)Qn(dxn)\n=\nZ\nB\n Z\nB\nf (xn)rn+1(xn,xn+1)Qn(dxn)\n!\ng(xn+1)Qn+1(dxn+1)\nwhich shows that the conditional distribution of Xn given Xn+1 = xn+1 has density\nxn 7\u2192rn+1(xn,xn+1) relatively to Qn. Note that, for discrete probabilities, one has\nrn+1(x,y) = P(x,y)\nQn+1(y)\nand\nP(Xn = x | Xn+1 = y) = Qn(x)P(x,y)\nQn+1(y)\n.\n(12.5)\nThe formula is identical if both Q0 and P(x,\u00b7) have p.d.f.\u2019s with respect to a fixed\nreference measure \u00b5 on B (for example, Lebesgue\u2019s measure when B = Rd), denoting\nthese p.d.f\u2019s by q0 and p(x,\u00b7). Then, the p.d.f. of the distribution of Xn given Xn+1 = y\nis\n\u02dcpn(y,x) = qn(x)p(x,y)\nqn+1(y)\n(12.6)\nwhere qn is the p.d.f. of Qn. Note that the transition probabilities of the reversed\nMarkov chain depend on n, i.e., the reversed chain is non-homogeneous in general.\nHowever, if one assumes that Q0 = Q is invariant by P, then Qn = Q for all n and\ntherefore rn(x,y) = p\u2217(x,y), using the previous notation. In this case, the reversed\nchain has transitions independent of n and its transition probability has density\n\u02dcp\u2217(x,y) = p\u2217(y,x)\nwith respect to Q. In the discrete case, letting p(x,y) = P(Xn+1 = y | Xn = x), we have\np\u2217(x,y) = p(x,y)/Q(y), so that the reversed transition (call it \u02dcp) is such that\n\u02dcp(x,y)\nQ(y) = p(y,x)\nQ(x) ,\ni.e.,\nQ(y)p(y,x) = Q(x) \u02dcp(x,y).\n(12.7)\nOne retrieves easily the fact that if p is such that there exists Q and \u02dcp such that (12.7)\nis satisfied, then (summing the equation over y) Q is an invariant probability for p.\n266\nCHAPTER 12. MONTE-CARLO SAMPLING\nLet Q be a probability on B. One says that the Markov chain (or the transition\nprobability p) is Q-reversible if and only if p(x,\u00b7) has a density p\u2217(x,\u00b7) with respect\nto Q such that p\u2217(x,y) = p\u2217(y,x) for all x,y \u2208B. Since such a density is necessarily\ndoubly stochastic, Q is then invariant by p. Reversibility is equivalent to the prop-\nerty that, whenever Xn \u223cQ, the joint distribution of (Xn,Xn+1) coincides with that of\n(Xn+1,Xn). Alternatively, Q-reversibility requires that for all A,B \u2282B,\nZ\nA\nP(z,B)dQ(z) =\nZ\nB\nP(z,A)dQ(z).\n(12.8)\nIn the discrete case, (12.8) is equivalent to the \u201cdetailed balance\u201d condition:\nQ(y)p(y,x) = Q(x)p(x,y).\n(12.9)\nWhile Q can be an invariant distribution for a Markov chain without that chain\nbeing Q-reversible, the latter property is easier to ensure when designing transition\nprobabilities, and most sampling algorithms are indeed reversible with respect to\ntheir target distribution.\nRemark 12.4 A simple example of non-reversible Markov chain with invariant prob-\nability Q is often obtained in practice by alternating two or more Q-reversible transi-\ntion probabilities. Assume, to simplify, that B is discrete and that p1 and p2 are tran-\nsition probabilities that satisfy (12.9). Consider a composite Markov chain for which\nthe transition from Xn to Xn+1 consists in generating first Yn according to p1(Xn,\u00b7)\nand then Xn+1 according to p2(Yn,\u00b7). The resulting composite transition probability\nis\np(x,y) =\nX\nz\u2208B\np1(x,z)p2(z,y).\nTrivially, Q is invariant by p, since it is invariant by p1 and p2, but p is not Q-\nreversible. Indeed, p satisfies (12.7) with\n\u02dcp(x,y) =\nX\nz\u2208B\np2(x,z)p1(z,y).\n\u2666\n12.3.4\nIrreducibility and recurrence\nWhile necessary, invariance is not sufficient for a Markov chain to converge to Q\nin distribution. However, it simplifies the general ergodicity conditions compared\nto the general theory of Markov chains [147, 160], as summarized below, following\n[192] (see also [13]). We therefore assume that the transition probability P is such\nthat Q is P-invariant.\n12.3. MARKOV CHAIN SAMPLING\n267\nOne says that the Markov chain is Q-irreducible (or, simply, irreducible in what\nfollows) if and only if, for all z \u2208B and all (measurable) B \u2282B such that Q(B) > 0,\nthere exists n > 0 with Pz(Xn \u2208B) > 0. (Irreducibility implies that Q is the only\ninvariant probability of the Markov chain.)\nA Markov chain is called periodic if there exists m > 1 such that B can be covered\nby disjoint subsets B0,...,Bm\u22121 that satisfy P(x,Bj) = 1 for all x \u2208Bj\u22121 if j \u22651 and\nall x \u2208Bm\u22121 if j = 0. In other terms, the chain loops between the sets B0,...,Bm\u22121. If\nsuch a decomposition does not exists, the chain is called aperiodic.\nA periodic chain cannot satisfy (12.3). Indeed, periodicity implies that Px(Xn \u2208\nBi) = 0 for all x \u2208Bi unless i = 0 (mod d). Since the sets Bi cover B, (12.3) is only pos-\nsible with Q = 0. Irreducibility and aperiodicity are therefore necessary conditions\nfor ergodicity. Combined with the fact that Q is an invariant probability distribu-\ntion, these conditions are also sufficient, in the sense that (12.3) is true for Q-almost\nall x. (See [192] for a proof.)\nWithout the knowledge that the chain has an invariant probability, showing con-\nvergence usually requires showing that the chain is recurrent, which means that, for\nany set B such that Q(B) > 0, the probability that, starting from x, Xn \u2208B for an in-\nfinite number of n, written as Px(Xn \u2208B i.o.) (for infinitely often) is positive for all\nx \u2208E and equal to 1 Q-almost surely. The fact that irreducibility and aperiodicity\ncombined with Q-invariance imply recurrence (or, more precisely, Q-positive recur-\nrent [147]) is an important remark that significantly simplifies the theory for MCMC\nsimulation. Note that, by restricting B to a suitable set of Q-probability 1, one can\nassume that Px(Xn \u2208B i.o.) = 1 for all x \u2208B, which is called Harris recurrence. It the\nchain is Harris recurrent, then (12.3) holds with \u00b50 = \u03b4x for all x \u2208B. 4\nOne says that C \u2282B is a \u201csmall\u201d set if Q(C) > 0 and there exists a triple (m0,\u03f5,\u03bd),\nwith \u03f5 > 0 and \u03bd a probability distribution on B, such that\nPm0(x,\u00b7) \u2265\u03f5\u03bd(\u00b7)\nfor all x \u2208C. A slightly different result, proved in [13], replaces irreducibility by the\nproperty that there exists a small set C \u2282B such that\nPx(\u2203n : Xn \u2208C) > 0\n4Harris recurrence is also associated with the uniqueness of right eigenvectors of P, that is func-\ntions h : B \u2192R such that\nPh(x)\n\u2206=\nZ\nB\nP(x,dy)h(y) = h(x).\nSuch functions are also called harmonic for P. Because P is a transition probability, constant functions\nare always harmonic. Harris recurrence, in the current context, is equivalent to the fact that every\nbounded harmonic function is constant.\n268\nCHAPTER 12. MONTE-CARLO SAMPLING\nfor Q-almost all x \u2208B. One then replaces aperiodicity by the similar condition that\nthe greatest common divisor of the set of integers m such that there exists \u03f5m with\nPm(x,\u00b7) \u2265\u03f5m\u03bd(\u00b7) for all x \u2208C is equal to 1. These two conditions combined with\nQ-invariance also imply that (12.3) holds for Q-almost all x \u2208B.\n12.3.5\nSpeed of convergence\nIt is also important to quantify the speed of convergence in (12.3). Efficient algo-\nrithms typically have a geometric convergence speed, namely\nDvar(Pn\nx ,Q) \u2264M(x)rn\n(12.10)\nfor some 0 \u2264r < 1 and some function M(x), or uniformly geometric convergence\nspeed, for which the function M is bounded (or, equivalently, constant).\nA sufficient condition for geometric ergodicity is provided in Nummelin [147,\nProposition 5.21]. Assume that the chain is Harris recurrent and that there exist\nr > 1, a small set C and a \u201cdrift function\u201d h with\nsup\nx<C\n(rE(h(Xn+1) | Xn = x) \u2212h(x)) < 0\n(12.11a)\nand\nsup\nx\u2208C\nE(h(Xn+1)1Xn+1<C | Xn = x) < \u221e.\n(12.11b)\nThen the Markov chain is geometrically ergodic. Note that E(h(Xn+1) | Xn = x) =\nPh(x). Equations (12.11a) and (12.11b) can be summarized in a single equation\n[136], namely\nPh(x) \u2264\u03b2h(x) + M1C(x)\n(12.12)\nwith \u03b2 = 1/r < 1 and M \u22650.\n12.3.6\nModels on finite state spaces\nUniform geometric ergodicity is implied by the simple condition that the whole set\nB is small, requiring in a uniform lower bound, for some probability distribution \u03bd,\nPm0(x,\u00b7) \u2265\u03f5\u03bd(\u00b7)\n(12.13)\nfor all x \u2208B.\nSuch uniform conditions usually require strong restrictions on the space B, such\nas compactness or finiteness. To illustrate this consider the case in which the set B\nis finite. Assume, to simplify, that Q(x) > 0 for all x \u2208B (one can restrict the Markov\nchain to such x\u2019s otherwise). Arbitrarily labeling elements of B as B = {x1,...,xN},\n12.3. MARKOV CHAIN SAMPLING\n269\nwe can consider p(x,y) as the coefficients of a matrix P = (p(xk,xl),k,l = 1,...,N).\nSuch a matrix, which has non-negative entries and row sums equal to 1, is called a\nstochastic matrix.\nWe will denote the nth power of P as Pn = (p(n)(xk,xl),k,l = 1,...,N). One im-\nmediately sees that irreducibility is equivalent to the fact that, for all x,y \u2208B, there\nexists m (that may depend of x and y) such that p(m)(x,y) > 0. One can furthermore\nshow that the chain is irreducible and aperiodic if one can choose m independent of\nx and y above, that is, if there exists m such that Pm has positive coefficients. This\ncondition clearly implies uniformly geometric ergodicity, which is therefore valid\nfor all irreducible and aperiodic Markov chains on finite sets.\nThis result can also be deduced from properties of matrices with non-negative\nor positive coefficients. The Perron-Frobenius theorem [93] states that the eigen-\nvalue 1 (associated with the eigenvector 1) is the largest, in modulus, eigenvalue of\na stochastic matrix \u02dcP with positive entries, that it has multiplicity one and that all\nother eigenvalues have a modulus strictly smaller that one. If Pm has positive en-\ntries, this implies that all the eigenvalues of (Pm \u22121Q) (where Q is considered as a\nrow vector) have modulus strictly less than one. This fact can then be used to prove\nuniformly geometric ergodicity.\n12.3.7\nExamples on Rd\nTo take a geometrically ergodic example that is not uniform, consider the simple\nrandom walk provided by the iterations\nXn+1 = \u03c1Xn + \u03c42\u03f5n\nwhere \u03f5n \u223cN (0,IdRd), \u03c42 > 0 and 0 < \u03c1 < 1. One shows easily by induction that\nthe conditional distribution of Xn given X0 = x is Gaussian with mean mn = \u03c1nx and\ncovariance matrix \u03c32\nnIdRd with\n\u03c32\nn = 1 \u2212\u03c12n\n1 \u2212\u03c12 \u03c42.\nIn particular, the distribution Q = N (0,\u03c32\n\u221eIdRd), with \u03c32\n\u221e= \u03c42/(1 \u2212\u03c12), is invariant.\nEstimates on the variational distances between Gaussian distributions, such as those\nprovided in Devroye et al. [61], can then be used to show that\nDvar(Pn\nx ,Q) \u2264M(x)\u03c1n\nwhere M grows linearly in x but is not bounded.\nSituations in which on can, as above, compute the probability distributions of Xn\nare rare, however, and proving geometric convergence is significantly more difficult\n270\nCHAPTER 12. MONTE-CARLO SAMPLING\nthan for finite-state chains. For chains on Rd (or, more generally, locally compact\nmetric spaces), the drift function criterion (12.12) can be used. Assume that Ph(\u00b7),\ngiven by\nPh(x) = E(h(Xn+1) | Xn = x) =\nZ\nRd h(y)P(x,dy)\nis continuous as soon as the function h : Rd \u2192R is continuous (one says that the\nchain is weak Feller). This true, for example, if P(x,\u00b7) has a p.d.f. with respect\nto Lebesgue\u2019s measure which is continuous in x. In such a situation, one can see\nthat compact sets are small sets, and (12.12) can be restated as the existence if a\npositive function h with compact sub-level sets and such that h(x) \u22651, of a compact\nset C \u2282Rd and of positive constants \u03b2 < 1 and b such that, for all x \u2208Rd,\nPh(x) \u2264\u03b2h(x) + b1C(x).\n(12.14)\nAs an example, consider the Markov chain defined by\nXn+1 = Xn \u2212\u03b4\u2207H(Xn) + \u03c4\u03f5n+1\nwhere \u03f52,\u03f52,... are i.i.d. standard d-dimensional Gaussian variables and H : Rd \u2192R\nis C2. This chain is clearly irreducible (with respect to Lebesgue\u2019s measure). One has\nPh(x) =\n1\n(2\u03c0\u03c42)d/2\nZ\nRd h(y)e\u22121\n2\u03c42 |y\u2212x+\u03b4\u2207H(x)|2\ndy =\n1\n(2\u03c0)d/2\nZ\nRd h(x\u2212\u03b4\u2207H(x)+\u03c4u)e\u2212|u|2\n2 dy.\nLet us make the assumption that H is L-C1 for some L > 0 (c.f. definition 3.15)\nand furthermore assume that |\u2207H(x)| tends to infinity when x tends to infinity, en-\nsuring the fact that the sets {x : |\u2207H(x)| \u2264c} are compact for c > 0. We want to show\nthat, if \u03b4 is small enough, (12.14) holds for h(x) = exp(mH(x)) and m small enough.\nWe first compute an upper bound of\ng(x,u) = mH(x \u2212\u03b4\u2207H(x) + \u03c4u) \u2212|u|2\n2 .\nUsing the L-C1 property, we have\ng(x,u) \u2264mH(x) + m(\u2212\u03b4\u2207H(x) + \u03c4u)T \u2207H(x) + mL\n2 |\u03b4\u2207H(x) \u2212\u03c4u|2 \u2212|u|2\n2\n= mH(x) \u2212m\u03b4(1 \u2212\u03b4L/2)|\u2207H(x)|2 + m\u03c4(1 \u2212\u03c4L)\u2207H(x)T u \u22121 \u2212mL\u03c42\n2\n|u|2\n= mH(x) \u22121 \u2212mL\u03c42\n2\n\f\f\f\f\fu \u2212m\u03c4(1 \u2212\u03c4L)\n1 \u2212mL\u03c42 \u2207H(x)\n\f\f\f\f\f\n2\n\u2212m\n \n\u03b4(1 \u2212\u03b4L/2) \u2212m\u03c42(1 \u2212\u03c4L)2\n2(1 \u2212mL\u03c42)\n!\n|\u2207H(x)|2\n12.3. MARKOV CHAIN SAMPLING\n271\nAssume that mL\u03c42 \u22641. It follows that\nPh(x) =\n1\n(2\u03c0)d/2\nZ\nRd eg(x,u) du \u2264\nh(x)\n(1 \u2212mL\u03c42)d/2\nexp\n \n\u2212m\n \n\u03b4(1 \u2212\u03b4L/2) \u2212m\u03c42(1 \u2212\u03c4L)2\n2(1 \u2212mL\u03c42)\n!\n|\u2207H(x)|2\n!\nUsing this upper bound, we see that (12.14) will hold if one first chooses \u03b4 such that\n\u03b4L < 2, then m such that mL\u03c42 < 1 and\nm\u03c42(1 \u2212\u03c4L)2\n2(1 \u2212mL\u03c42) < \u03b4(1 \u2212\u03b4L/2)\nand finally choose C = {x : |\u2207H(x)| \u2264c} where c is large enough so that\n1\n(1 \u2212mL\u03c42)d/2 exp\n \n\u2212m\n \n\u03b4(1 \u2212\u03b4L/2) \u2212m\u03c42(1 \u2212\u03c4L)2\n2(1 \u2212mL\u03c42)\n!\nc2\n!\n< 1.\nNote that this Markov chain is not in detailed balance. Since P(x,\u00b7) has a p.d.f.,\nbeing in detailed balance requires the ratio p(x,y)/p(y,x) to simplify as a ratio q(y)/q(x)\nfor some function q, which does not hold. However, we can identify the invariant\ndistribution approximately with small \u03b4 and \u03c4, that we will assume to satisfy \u03c4 = a\n\u221a\n\u03b4\nfor a fixed a > 0, with \u03b4 a small number.\nWe can write\np(x,y) =\n1\n(2\u03c0\u03c42)d/2 exp\n\u0012\n\u22121\n2\u03c42|y \u2212x + \u03b4\u2207H(x)|2\u0013\n=\n1\n(2\u03c0\u03c42)d/2 exp\n \n\u22121\n2\u03c42|y \u2212x|2 \u2212\u03b4\n\u03c42(y \u2212x)T \u2207H(x) \u2212\u03b42\n2\u03c42|\u2207H(x)|2\n!\n.\nIf q is a density, we have\nqP(y) =\nZ\nRd q(x)p(x,y)dx\n=\n1\n(2\u03c0)d/2\nZ\nRd q(y + a\n\u221a\n\u03b4u)exp\n \n\u22121\n2|u|2 +\n\u221a\n\u03b4\na uT \u2207H(y + a\n\u221a\n\u03b4u) \u2212\u03b4\n2a2|\u2207H(y + a\n\u221a\n\u03b4u)|2\n!\ndu\nMake the expansions:\nq(y + a\n\u221a\n\u03b4u) = q(y) + a\n\u221a\n\u03b4\u2207q(y)T u + a2\u03b4\n2 uT \u22072q(y)u + o(\u03b4|u|2)\n272\nCHAPTER 12. MONTE-CARLO SAMPLING\nand\nexp\n \u221a\n\u03b4\na uT \u2207H(y + a\n\u221a\n\u03b4u) \u2212\u03b4\n2a2|\u2207H(y + a\n\u221a\n\u03b4u)|2\n!\n= 1 +\n\u221a\n\u03b4\na uT \u2207H(y) \u2212\u03b4\n2a2|\u2207H(y)|2 + \u03b4uT \u22072H(u)u + \u03b4\n2a2(uT \u2207H(y))2 + o(\u03b4|u|2).\nTaking the product and using the fact that (2\u03c0)\u2212d/2 R\nRd u exp(\u2212|u|2/2)du = 0 and\nthat (2\u03c0)\u2212d/2 R\nRd uT Au exp(\u2212|u|2/2)du = trace(A) for any symmetric matrix A, we can\nwrite, taking the product:\nqP(y) = q(y) + \u03b4\n a2\n2 \u2206q(y) + \u2207H(y)T \u2207q(y) + q(y)\u2206H(y)\n!\n+ o(\u03b4)\nThis indicates that, if q is invariant by P, it should satisfy\na2\n2 \u2206q(y) + \u2207H(y)T \u2207q(y) + q(y)\u2206H(y) = o(1).\nThe partial differential equation\na2\n2 \u2206q(y) + \u2207H(y)T \u2207q(y) + q(y)\u2206H(y) = 0\n(12.15)\nis satisfied by the function y 7\u2192e\u22122H(y)\na2 . Assuming that this function is integrable, this\ncomputation suggests that, for small \u03b4, the Markov chain approximately samples\nfrom the probability distribution\nq0 = 1\nZ e\u22122H(x)\na2 .\nThis is further discussed in the next remark that involves stochastic differential\nequations. We will also present a correction of this Markov chain that samples from\nq0 for all \u03b4 in section 12.5.2.\nRemark 12.5 (Langevin equation) This chain is indeed the Euler discretization [106]\nof the stochastic differential equation,\ndxt = \u2212\u2207H(xt)dt + adwt\n(12.16)\nwhere wt is a Brownian motion. Under general hypotheses, this stochastic diffusion\nequation, called a Langevin equation, indeed converges in distribution to q0(x). 5\n5Providing a rigorous account of the theory of stochastic differential equations is beyond our\nscope, and we refer the reader to the many textbooks on the subject, such as McKean [131], Ikeda\nand Watanabe [96], Ethier and Kurtz [69] (see also Berglund [26] for a short introduction).\n12.4. GIBBS SAMPLING\n273\nSuch diffusions are continuous-time Markov processes (Xt,t \u22650), which means\nthat the probability distribution of Xt+s given all events before and including time s\nonly depends on Xs and is provided by a transition probability Pt, with\nP(Xt+1 \u2208A | Xs = x) = Pt(x,A).\nSimilarly to deterministic ordinary differential equations, one shows that under suf-\nficient regularity conditions (e.g., \u2207H is C1), equations such as (12.16) have solutions\nup to some positive (random) explosion time, and that this explosion time is finite\nunder additional conditions that ensure that |\u2207H(x)| does not grow too fast when x\ntends to infinity.\nIf \u03d5 is a smooth enough function (say, C2, with compact support), the function\n(t,x) 7\u2192Pt\u03d5(x) satisfies the partial differential equation, called Kolmogorov\u2019s back-\nward equation,\n\u2202tPt\u03d5(x) = \u2212\u2207H(x)T \u2207Pt\u03d5(x) + a2\n2 \u2206Pt\u03d5(x)\nwith initial condition P0\u03d5(x) = \u03d5(x). If Pt(x,\u00b7) has at all times t a p.d.f. pt(x,\u00b7), then\nthis p.d.f. must satisfy the forward Kolmogorov equation:\n\u2202tpt(x,y) = \u22072 \u00b7 (\u2207H(y)pt(x,y)) + a2\n2 \u22062pt(x,y)\nwhere \u22072 and \u22062 indicate differentiation with respect to the second variable (y). (Re-\ncall that \u03b4f denotes the Laplacian of f .) Moreover, if Q is an invariant distribution\nwith p.d.f. q, it satisfies the equation\n\u2207\u00b7 (q\u2207H) + a2\n2 \u2206q(y) = 0.\nNoting that \u2207\u00b7 (q\u2207H) = \u2207qT \u2207H + q\u2206H, we retrieve (12.15). Convergence proper-\nties (and, in particular, geometric convergence) of the Langevin equation to its limit\ndistribution are studied in Roberts and Tweedie [166], using methods introduced in\nMeyn and Tweedie [134, 135, 136]\n\u2666\n12.4\nGibbs sampling\n12.4.1\nDefinition\nThe Gibbs sampling algorithm [79] was introduced to sample from a distribution\non large sets for which direct sampling is intractable and rejection samping is inef-\nficient. It generates a Markov chain that converges (under some hypotheses) in dis-\ntribution to this target probability. A general version of this algorithm is described\nbelow.\n274\nCHAPTER 12. MONTE-CARLO SAMPLING\nLet Q be a probability distribution on B. Consider a finite family U1,...,UK of\nrandom variables defined on B with values in measurable spaces B\u2032\n1,...,B\u2032\nK. Let\nQi = QUi denote the image of Q by Ui, defined by Qi(Bi) = Q(Ui \u2208Bi) for Bi \u2282Bi.\nAlso, assume that there exists, for all i, a regular family of conditional probabilities\nfor Q given Ui, defined as a collection of transition probabilities (ui,A) 7\u2192Qi(ui,A)\nfor ui \u2208Bi and A \u2282B, that satisfy\nZ\nA\ng(Ui(x))Q(dx) =\nZ\nBi\nQi(ui,A)g(ui)Qi(dui)\nfor all nonnegative measurable functions g : Bi \u2192R. In simpler terms, Qi(ui,A)\ndetermine a consistent set of conditional probabilities for Q(A | Ui = ui). For dis-\ncrete random variables (resp. variables with p.d.f.\u2019s on Rd), they are just elementary\nconditional probabilities.\nWe then consider the following algorithm.\nAlgorithm 12.2 (Gibbs sampling)\nInitialize the algorithm with some z(0) = z0 \u2208B and iterate the following two update\nsteps given a current z(n) \u2208B:\n(1) Select j \u2208{1,...,K} according to some pre-defined scheme, i.e., at random ac-\ncording to a probability distribution \u03c0(n) on the set {1,...,K}.\n(2) Sample a new value z(n+1) according to the probability distribution Qj(Uj(z(n)),\u00b7).\nOne typically chooses the probability distribution in Step 1 equal to the uniform\ndistribution on {1,...,K} (in which case it is independent on n), or to \u03c0(n) = \u03b4jn where\njn = 1+(n (mod) K) (periodic scheme). Strictly speaking, Gibbs sampling is a Markov\nchain if \u03c0(n) does not depend on n, and we will make this simplifying assumption in\nthe rest of our discussion (therefore replacing \u03c0(n) by \u03c0). One obvious requirement\nfor the feasibility of the method is that step (2) can be performed efficiently since it\nmust be repeated a very large number of times.\nOne can see that the Markov chain generated by this algorithm is Q-reversible.\nIndeed, assume that Xn \u223cQ. For any (measurable) subsets A and B in B, one has,\nusing the definition of conditional expectations,\nP(Xn \u2208A,Xn+1 \u2208B) =\nK\nX\ni=1\nE\n\u0010\n1Z\u2208AQi(Ui(Z),B)\n\u0011\n\u03c0(i).\n(12.17)\n12.4. GIBBS SAMPLING\n275\nNow, for any i\nE\n\u0010\n1Z\u2208AQi(Ui(Z),B)\n\u0011\n=\nZ\nA\nQi(Ui(z),B)Q(dz)\n=\nZ\nBi\nQi(ui,A)Qi(ui,B)Qi(dui)\nwhich is symmetric in A and B.\nNote that, in the discrete case\nP(z, \u02dcz) =\nn\nX\ni=1\n\u03c0(i)\nQ(\u02dcz)1Ui(\u02dcz)=Ui(z)\nP\nz\u2032:Ui(z\u2032)=Ui(z) Q(z\u2032)\n(12.18)\nand the relation Q(z)P(z, \u02dcz) = Q(\u02dcz)P(\u02dcz,z) is obvious.\nThe conditioning variables U1,...,UK should ensure, at least, that the associated\nMarkov chain is irreducible and aperiodic. For irreducibility, this requires that Z\ncan visits Q-almost all elements of B by a sequence of steps that lead one of the Ui\u2019s\ninvariant.\nRemark 12.6 In the standard version of Gibbs sampling, B is a product space B1 \u00d7\n\u00b7\u00b7\u00b7 \u00d7 BK, and\nB\u2032\nj = B1 \u00d7 \u00b7\u00b7\u00b7 \u00d7 Bj\u22121 \u00d7 Bj+1 \u00d7 \u00b7\u00b7\u00b7 \u00d7 BK.\nOne then takes Uj(z(1),...,z(K)) = (z(1),...,z(i\u22121),z(i+1),...,z(K)). In other terms, step 2\nin the algorithm replaces the current value of z(j)(n) by a new one sampled from the\nconditional distribution of Z(j) given the current values of z(i)(n),i , j.\n\u2666\nRemark 12.7 We have considered a fixed number of conditioning variables, U1,...,UK,\nfor simplicity, but the same analysis can be carried on if one replaces Uj by a func-\ntion U : (x,\u03b8) 7\u2192U\u03b8(x) defined on a product space B \u00d7 \u0398, taking values in some\nspace \u02dcB, where \u0398 is a probability space equipped with a probability distribution\n\u03c0 and B is measurable. The previous discussion corresponds to \u0398 = {1,...,K} and\nB = SK\ni=1{i} \u00d7 Bi (so that Ui(x) is replaced by (i,Ui(x))).\nOne may then define Q\u03b8 as the image of Q by U\u03b8 and let Q\u03b8(u,A) provide a\nversion of Q(A | U\u03b8 = u). The only change in the previous discussion (besides using\n\u03b8 in index) is that (12.17) becomes\nP(Xn \u2208A,Xn+1 \u2208B) =\nZ\n\u0398\nE\n\u0010\n1Z\u2208AQ\u03b8(U\u03b8(Z),B)\n\u0011\n\u03c0(d\u03b8).\n\u2666\n276\nCHAPTER 12. MONTE-CARLO SAMPLING\nRemark 12.8 Using notation from the previous remark, and allowing \u03c0 = \u03c0(n) to\ndepend on n, it is possible to allow \u03c0(n) to depend on the current state z(n) using the\nfollowing construction.\nFor every step n, assume that there exists a subset \u0398n of \u0398 such that \u03c0(n)(z,\u0398n) = 1\nand that, for all \u03b8 \u2208\u0398n, \u03c0(n) can be expressed in the form\n\u03c0(n)(z,\u00b7) = \u03c8(n)\n\u03b8 (U\u03b8(z),\u00b7)\nfor some transition probability \u03c8(n)\n\u03b8\nfrom B\u03b8 to \u0398n. The resulting chain remains\nQ-reversible, since\nP(Xn \u2208A,Xn+1 \u2208B) =\nZ\nB\nZ\n\u0398n\n1z\u2208AQ\u03b8(U\u03b8(z),B)\u03c0(n)(z,d\u03b8)Q(dz)\n=\nZ\n\u0398n\nZ\nB\n1z\u2208AQ\u03b8(U\u03b8(z),B)\u03c8(n)\n\u03b8 (U\u03b8(z),d\u03b8)Q(dz)\n=\nZ\n\u0398n\nZ\n\u02dcB\nQ\u03b8(u,A)Q\u03b8(u,B)\u03c8(n)\n\u03b8 (u,d\u03b8)Q\u03b8(du).\n\u2666\n12.4.2\nExample: Ising model\nWe will see several examples of applications of Gibbs sampling in the next few chap-\nters. Here, we consider a special instance of Markov random field (see chapter 13)\ncalled the Ising model. For this example, B = {0,1}L, and\nq(z) = 1\nC exp\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\nL\nX\nj=1\n\u03b1z(j) +\nL\nX\ni,j=1,i<j\n\u03b2ijz(i)z(j)\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8.\nNote that, although B is a finite set, its cardinality, 2L, is too large for the enumerative\nprocedure described in section 12.1 to be applicable as soon as L is, say, larger than\n30. In practical applications of this model, L is orders of magnitude larger, typically\nin the thousands or tens of thousands.\nWe here apply standard Gibbs sampling, as described in remark 12.6, defining\nBj = {0,1} and\nUi(z(1),...,z(L)) = (z(1),...,z(i\u22121),z(i+1),...,z(L)).\nThe conditional distribution of Z(j) given Uj(z) is a Bernoulli distribution with pa-\nrameter\nqZ(j)(1|Uj(z)) =\nexp(\u03b1 + PL\nj\u2032=1,j\u2032,j \u03b2jj\u2032z(j\u2032))\n1 + exp(\u03b1 + PL\nj\u2032=1,j\u2032,j \u03b2ijz(j))\n12.5. METROPOLIS-HASTINGS\n277\n(taking \u03b2jj\u2032 = \u03b2j\u2032j for j > j\u2032). Gibbs sampling for this model will generate a sequence\nof variables Z(0),Z(1),... by fixing Z(0) arbitrarily and, given Z(n) = z, applying the\ntwo steps:\n1. Select j \u2208{1,...,L} at random according to a probability distribution \u03c0(n) on the\nset {1,...,L}.\n2. Sample a new value \u03b6 \u2208{0,1} according to the Bernoulli distribution with pa-\nrameter qZ(j)\nn (1|Uj(z)), and set Z(j)(n + 1) = \u03b6 and Z(j\u2032)(n + 1) = Z(j\u2032)(n) for j\u2032 , j.\nLet us now consider the Ising model with fixed total activation, namely the pre-\nvious distribution conditional to S(z)\n\u2206= z(1) + \u00b7\u00b7\u00b7 + z(L) = h where 0 < h < L. The\ndistribution one wants to sample from now is\nqh(z) = 1\nCh\nexp\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\nL\nX\nj=1\n\u03b1z(j) +\nL\nX\ni,j=1,i<j\n\u03b2ijz(i)z(j)\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f81S(z)=h.\nIn that case, the previous choice for the one-step transitions does not work, because\nfixing all but one coordinate of z also fixes the last one (so that the chain would not\nmove from its initial value and would certainly not be irreducible). One can however\nfix all but two coordinates, therefore defining\nUij(z(1),...,z(L)) = (z(1),...,z(i\u22121),z(i+1),...,z(j\u22121),z(j+1),...,z(L))\nand Bij = {0,1}2. If Uij(z) is fixed, the only acceptable configurations are z and the\nconfiguration z\u2032 deduced from z by switching the value of z(i) and z(j). Thus, there\nis no possible change is z(i) = z(j). If z(i) , z(j), then the probability of flipping the\nvalues of z(i) and z(j) is qh(z\u2032)/(qh(z) + qh(z\u2032)).\n12.5\nMetropolis-Hastings\n12.5.1\nDefinition\nGibbs sampling is a special case of a generic MCMC algorithm called Metropolis-\nHastings that is defined as follows [133, 88]. Assume that the distribution Q has a\ndensity q with respect to a measure \u00b5 on B. Specify a transition probability on B,\nrepresented by a family of density functions with respect to \u00b5, (g(z,\u00b7),z \u2208B), and a\nfamily of acceptance functions (z,z\u2032) 7\u2192a(z,z\u2032) \u2208[0,1]. Two basic examples are when\nB is finite, \u00b5 is the counting measure, and q and g are probability mass functions, and\nwhen B = Rd, \u00b5 is Lebesgue\u2019s measure and q and g are probability density functions.\nThe sampling algorithm is then defined as follows. It invokes a function a that\nwill be specified below.\n278\nCHAPTER 12. MONTE-CARLO SAMPLING\nAlgorithm 12.3 (Metropolis-Hastings)\nInitialize the algorithm with Z(0) = z(0) \u2208B. At step n, the current value Z(n) = z is\nthen updated as follows.\n\u2022 \u201cPropose\u201d a new configuration z\u2032 drawn according to g(z,\u00b7).\n\u2022 \u201cAccept\u201d z\u2032 (i.e., set Z(n + 1) = z\u2032) with probability a(z,z\u2032). If the new value is\nrejected, keep the current one, i.e., let Z(n + 1) = z.\nThe transition probabilities for this process are p(x,y) = g(x,y)a(x,y) if x , y and\np(x,x) = 1 \u2212P\ny,x p(x,y). The chain is Q-reversible is the detailed balance equation\nq(z)g(z,z\u2032)a(z,z\u2032) = q(z\u2032)g(z\u2032,z)a(z\u2032,z)\n(12.19)\nis satisfied. The functions g and a are part of the design of the algorithm, but (12.19)\nsuggest that g should satisfy the \u201cweak symmetry\u201d condition:\n\u2200x,y \u2208\u2126: g(x,y) = 0 \u21d4g(y,x) = 0.\n(12.20)\nNote that this condition is necessary to ensure (12.19) if q(z) > 0 for all z. If q(z) > 0,\nthe fact that acceptance probabilities are less than 1 requires that\na(z,z\u2032) \u2264min\n \n1, q(z\u2032)g(z\u2032,z)\nq(z)g(z,z\u2032)\n!\n.\nIf one takes a(z,z\u2032) equal to the r.h.s., so that\na(z,z\u2032) = min\n \n1, q(z\u2032)g(z\u2032,z)\nq(z)g(z,z\u2032)\n!\n,\n(12.21)\nthen (12.19) is satisfied as soon as q(z) > 0. If q(z) = 0, then this definition ensures\nthat a(z\u2032,z) = 0 and (12.19) is also satisfied. Note also that the case g(z,z\u2032) = 0 is not\nrelevant, since z\u2032 is not attainable from z in one step in this case. This shows that\n(12.21) provides a Q-reversible chain. Obviously, if g already satisfies q(z)g(z,z\u2032) =\nq(z\u2032)g(z\u2032,z), which is the case for Gibbs sampling, then one should take a(z,z\u2032) = 1 for\nall z and z\u2032.\n12.5.2\nSampling methods for continuous variables\nWhile the Gibbs sampling and Metropolis-Hastings methods can be (and were) for-\nmulated for general variables and probability distributions, proving that the re-\nlated chains are ergodic, and checking conditions for geometric convergence speed\nis much harder when dealing with general state spaces than with finite or com-\npact spaces (see, e.g., [164, 132, 6, 165]). On the other hand, interesting choices\n12.5. METROPOLIS-HASTINGS\n279\nof proposal transitions for Metropolis-Hastings are available when B = Rd and \u00b5 is\nLebesgue\u2019s measure, taking advantage, in particular, of differential calculus. More\nprecisely, assume that q takes the form\nq(z) = 1\nC exp(\u2212H(z))\nfor some smooth function H (at least C1), such that exp(\u2212H) is integrable. We saw\nin section 12.3.7 that, under suitable assumptions, the Markov chain\nXn+1 = Xn \u2212\u03b4\n2\u2207H(Xn) +\n\u221a\n\u03b4\u03f5n+1\n(12.22)\nwith \u03f5n+1 \u223cN (0,IdRd) has q as invariant distribution in the limit \u03b4 \u21920. Its transition\nprobability, such that g(z,\u00b7) is the p.d.f. of N (z\u2212\u03b4\n2\u2207H(z),\u03b4IdRd), is therefore a natural\nchoice for a proposal distribution in the Metropolis-Hastings algorithm. In addition\nto converging from the exact target distribution, this \u201cMetropolis Adjusted Langevin\nAlgorithm\u201d (or MALA) can also be proved to satisfy geometric convergence under\nless restrictive hypotheses than (12.22) [166].\nAnother approach, similar to MALA is the Hamiltonian Monte-Carlo methods\n(or hybrid Monte-Carlo) [65, 142]. Inspired by physics, the method introduces a\nnew variable, p \u2208Rd, called \u201cmomentum,\u201d and defines the \u201cHamiltonian:\u201d\nH(z,m) = H(z) + 1\n2|m|2.\nFix a time \u03b8 > 0. The proposal transition g(z,\u00b7) is then defined as the value \u03b6(\u03b8) that\nis obtained by solving the Hamiltonian dynamical system\n(\u2202t\u03b6(t) = \u2202pH(\u03b6(t),\u00b5(t)) = \u00b5(t)\n\u2202t\u00b5(t) = \u2212\u2202zH(\u03b6(t),\u00b5(t)) = \u2212\u2207H(\u03b6(t))\n(12.23)\nwith \u03b6(0) = z and \u00b5(0) \u223cN (0,IdRd). One can easily see that \u2202tH(\u03b6(t),\u00b5(t)) = 0, which\nimplies that\nH(\u03b6(t)) + 1\n2|\u00b5(t)|2 = H(z) + 1\n2|\u00b5(0)|2\nat all times t, or, denoting by \u03d5N the p.d.f. of the d-dimensional standard Gaussian,\nq(\u03b6(t))\u03d5N (\u00b5(t)) = q(\u03b6(0))\u03d5N (\u00b5(0)).\nMoreover, if one denotes by \u03a6t(z,m) = (zt(z,m),mt(z,m)) the solution (\u03b6(t),\u00b5(t)) of\nthe system started with \u03b6(0) = z and \u00b5(0) = m, one can also see that det(d\u03a6t(z,m)) = 1\nat all times. Indeed, applying (1.5) and the chain rule, we have\n\u2202t logdet(d\u03a6t(z,m)) = trace(d\u03a6t(z,m)\u22121\u2202td\u03a6t(t,m)).\n280\nCHAPTER 12. MONTE-CARLO SAMPLING\nFrom\n(\u2202tzt(z,m) = mt(z,p)\n\u2202tpt(z,m) = \u2212\u2207H(zt(z,m))\nwe get\n\u2202td\u03a6t(z,m) =\n \n\u2202zmt(z,m)\n\u2202mmt(z,m)\n\u2212\u22072H(zt(z,m))\u2202zzt(z,m)\n\u2212\u22072H(zt(z,m))\u2202mzt(z,m)\n!\n=\n \n0\nIdRd\n\u2212\u22072H(zt(z,m))\n0\n!\nd\u03a6t(z,m).\nWe therefore get\n\u2202t logdet(d\u03a6t(z,m)) = trace\n \n0\nIdRd\n\u2212\u22072H(zt(z,m))\n0\n!\n= 0\nshowing that the determinant is constant. Since \u03a60(z,m) = (z,m) by definition, we\nget det(d\u03a6t(z,m)) = 1 at all times.\nLet \u00afqt denote the p.d.f. of \u03a6t(z,m) and assume that \u00afq0(z,m) = q(z)\u03d5N (m). We\nhave, using the change of variable formula\n\u00afqt(\u03a6t(z,m))|detd\u03a6t(z,m)| = q(z)\u03d5N (m)\nbut the r.h.s. is, from the remarks above also equal to\nq(zt(z,m))\u03d5N (mt(z,m))|detd\u03a6t(z,m)|\nyielding the identification\n\u00afqt(z\u2032,m\u2032) = q(z\u2032)\u03d5N (m\u2032)\nThis shows that Q (with p.d.f. q) is left invariant by this Markov chain. One can ac-\ntually show that chain is in detailed balance for the joint density \u00afq(z,m) = q(z)\u03d5N (m).\nThis is due to the fact that the system (12.23) is reversible, in the sense that\n\u03a6t(zt(z,m),\u2212mt(z,m)) = (z,\u2212m),\ni.e., the system solved from its end point after changing the sign of the momentum\nreturns to its initial state after changing the sign of the momentum a second time.\nIn other terms, letting J(z,m) = (z,\u2212m), we have \u03a6\u22121\nt\n= J\u03a6t \u25e6J. So, consider a function\nf : (Rd \u00d7Rd)2 \u2192R. Denoting the Markov chain by (Zn,Mn), we assume that the next\npair Zn+1,Mn+1 is computed by (i) sampling M\u2032\nn \u223cN (0,IdRd); (ii) solving (12.23),\nwith initial conditions \u03b6(0) = Zn and \u00b5(0) = M\u2032\nn; (iii) taking Zn+1 = \u03b6(\u03b8) and sampling\nMn+1 \u223cN (0,IdRd).\nWe have\nE(f (Zn,Mn,Zn+1,Mn+1)) =\nZ\nf (z, \u02dcm,z(z,m), \u00afm)\u03d5N (m)\u03d5N ( \u00afm)\u03d5N ( \u02dcm)q(z)dmd \u00afmd \u02dcmdz.\n12.5. METROPOLIS-HASTINGS\n281\nMake the change of variables z\u2032 = z(z,m), m\u2032 = m(z,m), which has Jacobian determi-\nnant 1, and is such that z = z(z\u2032,\u2212m\u2032), m = \u2212m(z\u2032,\u2212m\u2032). We get\nE(f (Zn,Mn,Zn+1,Mn+1))\n=\nZ\nf (z(z\u2032,\u2212m\u2032), \u02dcm,z\u2032, \u00afm)\u03d5N (\u2212m(z\u2032,\u2212m\u2032))\u03d5N ( \u00afm)\u03d5N ( \u02dcm)q(z(z\u2032,\u2212m\u2032))dm\u2032d \u00afmd \u02dcmdz\u2032\n=\nZ\nf (z(z\u2032,\u2212m\u2032), \u02dcm,z\u2032, \u00afm)\u03d5N (m(z\u2032,\u2212m\u2032))\u03d5N ( \u00afm)\u03d5N ( \u02dcm)q(z(z\u2032,\u2212m\u2032))dm\u2032d \u00afmd \u02dcmdz\u2032\n=\nZ\nf (z(z\u2032,\u2212m\u2032), \u02dcm,z\u2032, \u00afm)\u03d5N (\u2212m\u2032)\u03d5N ( \u00afm)\u03d5N ( \u02dcm)q(z\u2032)dm\u2032d \u00afmd \u02dcmdz\u2032,\nusing the conservation of H. Making the change of variables m\u2032 \u2192\u2212m\u2032, we get\nE(f (Zn,Mn,Zn+1,Mn+1)\n=\nZ\nf (z(z\u2032,m\u2032), \u02dcm,z\u2032, \u00afm)\u03d5N (m\u2032)\u03d5N ( \u00afm)\u03d5N ( \u02dcm)q(z\u2032)dm\u2032d \u00afmd \u02dcmdz\u2032\nwhich is equal to E(f (Zn+1,Mn+1,Zn,Mn)) showing the reversibility of the chain.\nThis simulation scheme can potentially make large moves in the current configu-\nration z while maintaining detailed balance (therefore not requiring an accept/reject\nstep). However, practical implementations require discretizing (12.23), which breaks\nthe conservation properties that were used in the argument above, therefore requir-\ning a Metropolis-Hastings correction. For example, a second-order Runge Kutta\n(RK2) scheme with time step \u03b1 gives\n\uf8f1\uf8f4\uf8f4\uf8f4\uf8f4\uf8f2\uf8f4\uf8f4\uf8f4\uf8f4\uf8f3\nZn+1 = Zn + \u03b1Mn \u2212\u03b12\n2 \u2207H(Zn)\nMn+1 = Mn \u2212\u03b1\n2 (\u2207H(Zn) + \u2207H(Zn + hMn))\nOnly the update for Zn matters, however, since Mn+1 is discarded and resampled at\neach step. Importantly, if we let \u03b4 = \u221a\u03b1 the first equation in the system becomes\nZn+1 = Zn \u2212\u03b4\n2\u2207H(Zn) + \u03b4Mn\nwith Mn \u223cN (0,1), which is exactly (12.22). Note that one can, in principle, solve\n(12.23) for more that one discretization step (the continuous equation can be solved\nfor an arbitrary time), but one must then face the challenge of computing the Metropo-\nlis correction since the Hamiltonian is not conserved at each step.\nOne can however use schemes that are more adapted to solving Hamiltonian\n282\nCHAPTER 12. MONTE-CARLO SAMPLING\nsystems [119], such as the St\u00a8ormer-Verlet scheme, which is\n\uf8f1\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f2\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f3\nMn+1/2 = Mn \u2212\u03b1\n2 \u2207H(Zn)\nZn+1 = Zn + \u03b1Mn+1/2\nMn+1 = Mn+1/2 \u2212\u03b1\n2 \u2207H(Zn+1)\nThis scheme computes \u03c81 \u25e6\u03c82 \u25e6\u03c81(z,m) with \u03c81(z,m) = (z,m \u2212(\u03b1/2)\u2207H(z)) and\n\u03c82(z,m) = (z + \u03b1m,m). Because both \u03c81 and \u03c82 have a Jacobian determinant equal to\n1, so does their composition. This scheme is also reversible, since we have\n\uf8f1\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f2\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f3\n\u2212Mn+1/2 = \u2212Mn+1 \u2212\u03b1\n2 \u2207H(Zn+1)\nZn = Zn+1 \u2212\u03b1Mn+1/2\n\u2212Mn = \u2212Mn+1/2 \u2212\u03b1\n2 \u2207H(Zn)\nThese properties are conserved if one applies the St\u00a8ormer-Verlet scheme more than\nonce at each iteration, that is, fixing some N > 0 and letting \u03a6(z,m) = (\u03c81\u25e6\u03c82\u25e6\u03c81)\u25e6N,\nthen \u03a6\u22121 = J\u03a6 \u25e6J, with J(z,m) = (z,\u2212m) with detd\u03a6 = 1. Considering again the aug-\nmented chain which, starting from (Zn,Mn), samples \u02dcM \u223cN (0,IdRd), then computes\n(Z\u2032, \u02dcM\u2032) = \u03a6(Zn, \u02dcM) and finally samples M\u2032 \u223cN (0,IdRd) as a Metropolis-Hastings\nproposal to sample from (z,m) 7\u2192q(z)\u03d5N (m), then, assuming that (Z,M) follows this\ntarget distribution and letting (Z\u2032,M\u2032) be the result of the proposal distribution, we\nhave, as computed above\nE(f (Z,M,Z\u2032,M\u2032))\n=\nZ\nf (z, \u02dcm,z(z,m), \u00afm)\u03d5N (m)\u03d5N ( \u00afm)\u03d5N ( \u02dcm)q(z)dmd \u00afmd \u02dcmdz\n=\nZ\nf (z(z\u2032,m\u2032), \u02dcm,z\u2032, \u00afm)\u03d5N (m(z\u2032,m\u2032))\u03d5N ( \u00afm)\u03d5N ( \u02dcm)q(z(z\u2032,m\u2032))dm\u2032d \u00afmd \u02dcmdz\u2032\nThis shows that the acceptance probability in the Metropolis step is\na(z,m,z\u2032,m\u2032) = min\n \n1, \u03d5N (m(z\u2032,m\u2032))q(z(z\u2032,m\u2032))\n\u03d5N (m)q(z)\n!\n= exp(\u2212max(H(z(z\u2032,m\u2032),m(z\u2032,m\u2032)) \u2212H(z,m)),0)\nWhile the Hamiltonian is not kept invariant by the St\u00a8ormer-Verlet scheme, so that an\naccept-reject step is needed, it is usually quite stable over extended periods of time\nso that the acceptance probability is generally close to one.\n12.6\nPerfect sampling methods\nThe Markov chain simulation methods, provided in the previous sections do not\nprovide exact samples from the distribution q, but only increasingly accurate ap-\n12.6. PERFECT SAMPLING METHODS\n283\nproximations. Perfect sampling algorithms [156, 157, 71] use Markov chains \u201cback-\nwards\u201d to generate exact samples. To describe them, it is easier to describe a Markov\nchain as a stochastic recursive equation of the form\nXn+1 = f (Xn,Un+1)\n(12.24)\nwhere Un+1 is independent of Xn,Xn\u22121,..., and the Uk\u2019s are identically distributed. In\nthe discrete case (assumed in this section), and given a stochastic matrix P, one can\ntake Un to be the uniformly distributed variable used to sample from (p(Xn,x),x \u2208B).\nConversely, the transition probability associated to (12.24) is p(x,y) = P(f (x,U) = y).\nIt will be convenient to consider negative times also. For n > 0, recursively define\nF\u2212n(x,u\u2212n+1,...,u0) by\nF\u2212n\u22121(x,u\u2212n,...,u0) = F\u2212n(f (x,u\u2212n),u\u2212n+1,...,u0)\nand F\u22121(x,u0) = f (x,u0). Denote, for short, U0\n\u2212n = (U\u2212n,...,U0). The function F\u2212n(x,u0\n\u2212n+1)\nprovides the value of X0 when X\u2212n = x and U0\n\u2212n+1 = u0\n\u2212n+1.\nFor an infinite past sequence, u0\n\u2212\u221e, let \u03bd(u0\n\u2212\u221e) denote the first integer n such that\nF\u2212n(x,u0\n\u2212n+1) does not depend on x (the function \u201ccoalesces\u201d). Then, the following\ntheorem is true:\nTheorem 12.9 Assume that the chain defined by (12.24) is ergodic, with invariant dis-\ntribution Q. Then \u03bd = \u03bd(U0\n\u2212\u221e) is finite with probability 1, and\nX\u2217:= F\u2212\u03bd(x,U0\n\u2212\u03bd+1)\n(12.25)\n(which is independent of x) has distribution Q.\nProof Because the chain is ergodic, we know that there exists an integer N such\nthat one can pass from any state to any other with positive probability. So the chain\ncan, starting from anywhere, coalesce with positive probability in N steps; \u03bd being\ninfinite would imply that this event never occurs in an infinite number of trials, and\nthis has probability 0.\nFor any k > 0 and any x \u2208B, we have\nX\u2217= F\u2212\u03bd(f\u2212k(x,U\u2212\u03bd\n\u2212\u03bd\u2212k+1),U0\n\u2212\u03bd+1) = F\u2212\u03bd\u2212k(x,U0\n\u2212\u03bd\u2212k+1).\n(12.26)\nBut, because the chain is ergodic, we have, for any x \u2208B\nlim\nk\u2192\u221eP(F\u2212k(x,U0\n\u2212k+1) = y) = Q(y).\nWe can write\nP(F\u2212k(x,U0\n\u2212k+1) = y) = P(F\u2212k(x,U0\n\u2212k+1) = y,\u03bd \u2264k) + P(F\u2212k(x,U0\n\u2212k+1) = y,\u03bd > k)\n= P(X\u2217= y,\u03bd \u2264k) + P(F\u2212k(x,U0\n\u2212k+1) = y,\u03bd > k)\n284\nCHAPTER 12. MONTE-CARLO SAMPLING\nThe right-hand side tends to P(X\u2217= y) when k tends to infinity (because P(\u03bd > k)\ntends to 0), and the left-hand side tends to Q(y), which gives the second part of the\ntheorem.\n\u25a0\nFrom (12.26), which is the key step in proving that X\u2217follows the invariant distri-\nbution, one can see why it is important to consider sampling that expands backward\nin time rather than forward. More specifically, consider the coalescence time for the\nforward chain, letting \u02dc\u03bd(u\u221e\n0 ) be the first index for which\n\u02dcX\u2217:= F \u02dc\u03bd(x,u \u02dc\u03bd\n0)\nis independent from the starting point, x. For any k \u22650, one still has the fact that\nF \u02dc\u03bd+k(x,u \u02dc\u03bd+k\n0\n) does not depend on x, but its value depends on k and will not be equal\nto \u02dcX\u2217anymore, which prevents the rest of the proof of theorem 12.9 to carry on.\nAn equivalent algorithm is described in the next proposition (the proof is easy\nand left to the reader).\nProposition 12.10 Using the same notation as above, the following algorithm generates\na perfect sample, \u03be\u2217, of the invariant distribution of an ergodic Markov chain.\nAssume that an infinite sample u0\n\u2212\u221eof U is available. Given this sequence, the algo-\nrithm, starting with t0 = 2, is:\n1. For all x \u2208B, define \u03bex\n\u2212t,t = \u2212t0,...,0 by \u03bex\n\u2212t0 = x and \u03bex\n\u2212t+1 = f (\u03bex\n\u2212t,u\u2212t+1).\n2. If \u03bex\n0 is constant (independent of x), let \u03be\u2217be equal to this constant value and stop.\nOtherwise, return to step 1 replacing t0 with 2t0.\nIn practice, the u\u2212k\u2019s are only generated when they are needed. But it is important to\nconsider the sequence as fixed: once u\u2212k is generated, it must be stored (or identically\nregenerated, using the same seed) for further use. It is important to strengthen the\nfact that this algorithm works backward in time, in the sense that the first states of\nthe sequence are not identical at each iteration, because they are generated using\nrandom numbers with indexes further in the past.\nSuch an algorithm is not feasible when |B| is too large, since one would have to\nconsider an intractable number of Markov chains (one for each x \u2208B). However\nthere are cases in which the constancy of \u03bex\n0 over all B can be decided from its con-\nstancy over a small subset of B.\nOne situation in which this is true is when the Markov chain is monotone, ac-\ncording to the following definition. Assume that B can be partially ordered, and\nthat f in (12.24) is increasing in x, i.e.,\nx \u2264x\u2032 \u21d2\u2200u,f (x,u) \u2264f (x\u2032,u).\n(12.27)\n12.6. PERFECT SAMPLING METHODS\n285\nLet Bmin and Bmax be the set of minimal and maximal elements in B. Then the\nsequence coalesces for the algorithm above if and only if it coalesces over Bmin \u222a\nBmax. Indeed, any x \u2208B is smaller than some maximal element, and larger than\nsome minimal element in B. By (12.27), these inequalities remain true at each step\nof the sampling process, which implies that when chains initialized with extremal\nelements coalesce, so do the other ones. Therefore, it suffices to run the algorithm\nwith extremal configurations only.\nOne can rewrite (12.27) in terms of transition probabilities p(x,y), assuming that\nU follows a uniform distribution on [0,1] and, for all x \u2208B, there exists a partition\n(Ixy,y \u2208B) of B, such that\nf (x,u) = y \u21d4u \u2208Ix,y\nand Ixy is an interval with length pxy. Condition (12.27) is then equivalent to\nx \u2264x\u2032 \u21d2\u2200y \u2208B,Ixy \u2282\n[\ny\u2032\u2265y\nIx\u2032y\u2032.\nThis requires in particular that P\ny\u2265y0 p(x,y) \u2264P\ny\u2265y0 p(x\u2032,y) whenever x \u2264x\u2032 (one\nsays that p(x,\u00b7) is stochastically smaller than p(x\u2032,\u00b7)).\nOne example in which this reduction works is with the ferromagnetic Ising model,\nfor which B = {\u22121,1}L and\nq(x) = 1\nC exp\n\u0010\nL\nX\ns,t=1,s<t\n\u03b2stx(s)x(t)\u0011\nwith \u03b2st \u22650 for all {s,t}. Then, the Gibbs sampling algorithm iterates the follow-\ning steps: take a random s \u2208{1,...,L} and update x(s) according to the conditional\ndistribution\ngs(y(s) | x(sc)) =\ney(s)vs(x)\ne\u2212vs(x) + evs(x)\nwith vs(x) = P\nt,s \u03b2stx(t). Order B so that x \u2264\u02dcx if and only if x(s) \u2264\u02dcx(s) for all s =\n1,...,L. The minimal and maximal elements are unique in this case, with x(s)\nmin \u2261\u22121\nand x(s)\nmax \u22611. Moreover, because all \u03b2st are non-negative, vs is an increasing function\nof x so that, if x \u2264\u02dcx, then gs(1 | x(s)) \u2264gs(1 | \u02dcx(s)).\nTo define the stochastic iterations, first introduce\nfs(x,u) =\n\uf8f1\uf8f4\uf8f4\uf8f2\uf8f4\uf8f4\uf8f3\n1(s) \u2227x(sc) if u \u2264qs(1 | x(s))\n(\u22121)(s) \u2227x(sc) if u > qs(1 | x(s)),\nwhich satisfies (12.27). The whole updating scheme can then be implemented with\nthe function\nf (x,(u, \u02dcu)) =\nL\nX\ns=1\n\u03b4Is( \u02dcu)fs(x,u)\n286\nCHAPTER 12. MONTE-CARLO SAMPLING\nwhere (Is,s \u2208V ) is any partition of [0,1] in intervals of length 1/L. This is still mono-\ntonic. The algorithm described in proposition 12.10 can therefore be applied to\nsample exactly, in finite time, from the ferromagnetic Ising model.\n12.7\nApplication: Stochastic approximation with Markovian tran-\nsitions\nUsing the material developed in this chapter, we now discuss the convergence of\nstochastic approximation methods (such as stochastic gradient descent) when the\nrandom random variable in the update term follows Markovian transitions. In sec-\ntion 3.3, we considered algorithms in the form\n(\u03bet+1 \u223c\u03c0Xt\nXt+1 = Xt + \u03b1t+1H(Xt,\u03bet+1)\nwhere \u03bet : \u2126\u2192R\u03be is a random variable. We now want to addres situations in which\nthe random variable \u03bet+1 is obtained through a transition probability, therefore con-\nsidering the algorithm\n(\u03bet+1 \u223cPXt(\u03bet,\u00b7)\nXt+1 = Xt + \u03b1t+1H(Xt,\u03bet+1)\n(12.28)\nHere Px is, for all x, a transition probability from R\u03be to R\u03be. We will assume that,\nfor all x \u2208Rd, the Markov chain with transition Px is geometrically ergodic, and we\ndenote by \u03c0x its invariant distribution. We let, as in section 3.3, \u00afH(x) = E\u03c0x(H(x,\u00b7)).\nWe will use the notation for a function f : Rd \u00d7 R\u03be \u2192R\nPxf : (x\u2032,\u03be) \u2208Rd \u00d7 R\u03be 7\u2192Pxf (x\u2032,\u03be) =\nZ\nR\u03be\nf (x\u2032,\u03be\u2032)Px(\u03be,d\u03be\u2032)\nand\n\u03c0xf : x\u2032 \u2208Rd 7\u2192\u03c0xf (x\u2032) =\nZ\nR\u03be\nf (x\u2032,\u03be)\u03c0x(d\u03be).\nIn particular, \u00afH(x) = \u03c0xH(x). We also define h(x,\u03be) = H(x,\u03be) \u2212\u00afH(x) and \u02dch(x,\u03be) =\nPxh(x,\u03be). We make the following assumptions.\n(H1) There exists constants C0,C1,c2 such that, for all x,y \u2208Rd,\nsup\n\u03be\u2208R\u03be\n|H(x,\u03be)| \u2264C0,\n(12.29a)\nsup\n\u03be\u2208R\u03be\n|\u02dch(x,\u03be)| \u2264C1,\n(12.29b)\nsup\n\u03be\u2208R\u03be\n|\u02dch(x,\u03be) \u2212\u02dch(y,\u03be)| \u2264C1|x \u2212y|,\n(12.29c)\nDvar(\u03c0x,\u03c0y) \u2264C2|x \u2212y|\n(12.29d)\n12.7. MARKOVIAN STOCHASTIC APPROXIMATION\n287\n(H2) There exists x\u2217\u2208Rd and \u00b5 > 0 such that, for all x \u2208Rd\n(x \u2212x\u2217)T \u00afH(x) \u2264\u2212\u00b5|x \u2212x\u2217|2.\n(12.30)\n(H3) We assume that there exists a constant M and a non-decreasing function \u03c1 :\n[0,+\u221e) \u2192[0,1) such that, for all probability distributions Q and Q\u2032 on R\u03be,\nDvar(QPn\nx ,Q\u2032Pn\nx ) \u2264M\u03c1(|x|)nDvar(Q,Q\u2032).\n(12.31)\n(H4) The sequence \u03b11,\u03b12,... is non-increasing, with\n\u221e\nX\nt=1\n\u03b1t = +\u221e\nand\n\u221e\nX\nt=1\n\u03b12\nt < +\u221e.\n(12.32a)\nLet \u03c3t = Pt\ns=1 \u03b1s. If C1 > 0, we also require that\nlim\nt\u2192\u221e\u03b1t\u03c3t(1 \u2212\u03c1(\u03c3t))\u22121 = 0\n(12.32b)\nand\nt\nX\ns=2\n\u03b12\ns \u03c3s(1 \u2212\u03c1(\u03c3s))\u22122 < \u221e.\n(12.32c)\nGiven this, the following theorem holds.\nTheorem 12.11 Assuming (H1) to (H4), the sequence defined by (12.28) is such that\nlim\nt\u2192\u221eE(|Xt \u2212x\u2217|2) = 0\nRemark 12.12 Condition (H1) assumes that H is bounded and uniformly Lipschitz\nin x, which is more restrictive than what was assumed in section 3.3.2, but applies,\nfor example, to situations considered in Younes [206] and later in this book in sec-\ntion 17.2.2.\nCondition (H3) implies that the Markov chain with transition Px is uniformly\ngeometrically ergodic, but the ergodicity rate may depend on x and in particular\nconverge to 1 when x tends to \u221e, which is the situation targeted in this theorem.\nThe reader may refer to [208] for a general discussion of this problem with re-\nlaxed hypotheses and almost sure convergence, at the expense of significantly longer\nproofs.\n\u2666\nProof We note that, from (12.29a), one has\n|Xt \u2212x\u2217| \u2264C0\u03c3t|X0 \u2212x\u2217|.\n(12.33)\n288\nCHAPTER 12. MONTE-CARLO SAMPLING\nSimilarly to section 3.3.2, we let At = |Xt \u2212x\u2217|2 and at = E(At). One can then write\nAt+1 = At+2\u03b1t+1(Xt\u2212x\u2217)T \u00afH(Xt)+2\u03b1t+1(Xt\u2212x\u2217)T (H(Xt,\u03bet+1)\u2212\u00afH(Xt))+\u03b12\nt+1|H(Xt,\u03bet+1)|2\nbut we do not have\nE((Xt \u2212x\u2217)T (H(Xt,\u03bet+1) \u2212\u00afH(Xt)) | Ut) = 0\nanymore, where Ut is the \u03c3-algebra of all past events up to time t (all events depend-\ning of Xs,\u03bes, s \u2264t). Indeed the Markovian assumption implies that\nE((Xt \u2212x\u2217)T (H(Xt,\u03bet+1) \u2212\u00afH(Xt)) | Ut) = (Xt \u2212x\u2217)T\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\nZ\nR\u03be\nH(Xt,\u03be)PXt(\u03bet,d\u03be) \u2212\u00afH(Xt)\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\n= (Xt \u2212x\u2217)T ((PXtH(Xt,\u00b7))(\u03bet) \u2212\u00afH(Xt)),\nwhich does not vanish in general. Following Benveniste et al. [25], this can be ad-\ndressed by introducing the solution g(x,\u00b7) of the \u201cPoisson equation\u201d\ng(x,\u00b7) \u2212Pxg(x,\u00b7) = h(x,\u00b7).\n(12.34)\n(Recall that h(x,\u03be) = H(x,\u03be) \u2212\u00afH(x).) One can then write\n(Xt \u2212x\u2217)T h(Xt,\u03bet+1) = (Xt \u2212x\u2217)T (g(Xt,\u03bet+1) \u2212PXtg(Xt,\u03bet+1)\nand\nAt+1 \u2264(1 \u22122\u03b1t+1\u00b5)At + 2\u03b1t+1(Xt \u2212x\u2217)T (g(Xt,\u03bet+1) \u2212PXtg(Xt,\u03bet)))\n+ 2\u03b1t+1(Xt \u2212x\u2217)T PXtg(Xt,\u03bet) \u22122\u03b1t+1(Xt \u2212x\u2217)T PXtg(Xt,\u03bet+1) + \u03b12\nt+1|H(Xt,\u03bet+1)|2\nIntroducing the notation\n\u03b7st = E((Xs \u2212x\u2217)T PXsg(Xs,\u03bet)).\nUsing the fact that\nE\n\u0010\n(Xt \u2212x\u2217)T (g(Xt,\u03bet+1) \u2212PXtg(Xt,\u03bet))) | Ut\n\u0011\n= 0\nand and noting that |H(Xt,\u03bet+1)|2 \u2264C2\n0, this gives, after taking expectations,\nat+1 \u2264(1 \u22122\u03b1t+1\u00b5)at + 2\u03b1t+1\u03b7tt \u22122\u03b1t+1\u03b7t,t+1 + \u03b12\nt+1C2\n0.\nApplying lemma 3.25, and letting vs,t = Qt\nj=s+1(1 \u22122\u03b1j+1\u00b5), we get\nat \u2264a0v0,t + 2\nt\nX\ns=1\nvs,t\u03b1s+1(\u03b7ss \u2212\u03b7s,s+1) + C2\n0\nt\nX\ns=1\nvs,t\u03b12\ns+1.\n12.7. MARKOVIAN STOCHASTIC APPROXIMATION\n289\nWe now want to ensure that each term in the upper bound converges to 0. Simi-\nlarly to section 3.3.2, (12.32a) implies that this holds the first and last terms and we\ntherefore focus on the middle one, writing\nt\nX\ns=1\nvs,t\u03b1s+1(\u03b7ss \u2212\u03b7s,s+1) = v1,t\u03b12\u03b711 \u2212\u03b1t+1\u03b7t,t+1 +\nt\nX\ns=2\n(vs,t\u03b1s+1 \u2212vs\u22121,t\u03b1s)\u03b7ss\n(12.35)\n+\nt\nX\ns=2\nvs\u22121,t\u03b1s(\u03b7ss \u2212\u03b7s\u22121,s)\nWe will need the following estimates on the function g in (12.34), which is de-\nfined by\ng(x,\u03be) =\n\u221e\nX\nn=0\nPn\nx h(x,\u03be) = h(x,\u03be) +\n\u221e\nX\nn=0\nPn\nx \u02dch(x,\u03be).\nLemma 12.13 We have\n|g(x,\u00b7)| \u2264C0 + 2C1M(1 \u2212\u03c1(x))\u22121,\n(12.36a)\n|Pxg(x,\u00b7)| \u22642C1M(1 \u2212\u03c1(x))\u22121.\n(12.36b)\nand, for all x,y \u2208Rd and \u03be \u2208R\u03be\n|Pxg(x,\u03be) \u2212Py(g(y,\u03be)| = M2C1C2(1 \u2212\u00af\u03c1)\u22122 + MC1(1 + C2)(1 \u2212\u00af\u03c1)\u22121.\n(12.37)\nwith \u00af\u03c1 = max(\u03c1(|x|),\u03c1(|y|)).\nUsing lemma lemma 12.13 (which is proved at the end of the section), we can\ncontrol the terms intervening in (12.35). Note that the first term, v1t\u03b12\u03b711, converges\nto 0 since (12.32a) implies that v1t converges to 0.\nWe have,\n\u03b1t+1|E((Xt \u2212x\u2217)T PXtg(Xt,\u03bet+1))| \u22642MC1\u03b1t+1\u03c3t(1 \u2212\u03c1(\u03c3t))\u22121,\nso that (12.32b) implies that \u03b1t+1\u03b7t,t+1 \u21920.\nand since \u03b1s+1 \u2264\u03b1s, we have\n\f\f\f\f\f\f\f\nt\nX\ns=2\n(vs,t\u03b1s+1 \u2212vs\u22121,t\u03b1s)\u03b7ss\n\f\f\f\f\f\f\f\n\u2264\nt\nX\ns=2\n|vst\u03b1s \u2212vs,t\u03b1s+1||\u03b7ss|\n\u2264MC1\nt\nX\ns=2\n|vs\u22121,t\u03b1s \u2212vs,t\u03b1s+1|\u03b1s+1\u03c3s(1 \u2212\u03c1(\u03c3s))\u22121\n\u2264C\nt\nX\ns=2\n|vs\u22121,t\u03b1s \u2212vs,t\u03b1s+1|\n290\nCHAPTER 12. MONTE-CARLO SAMPLING\nfor some constant C, since \u03b1s+1\u03c3s(1 \u2212\u03c1(\u03c3s))\u22121 is bounded. Writing\nvs,t\u03b1s+1 \u2212vs\u22121,t\u03b1s = vst(\u03b1s+1 \u2212\u03b1s + 2\u00b5\u03b12\ns ),\nwe get (using \u03b1s+1 \u2264\u03b1s)\nt\nX\ns=2\n|vs\u22121,t\u03b1s \u2212vs,t\u03b1s+1| \u2264\nt\nX\ns=2\nvst(\u03b1s \u2212\u03b1s+1) +\nt\nX\ns=2\nvst2\u00b5\u03b12\ns .\nSince both P\ns(\u03b1s \u2212\u03b1s+1) and Pt\ns=2 \u03b12\ns converge (the former is just \u03b11), lemma 3.26\nimplies that\nt\nX\ns=2\n(vs,t\u03b1s+1 \u2212vs\u22121,t\u03b1s)\u03b7ss\ntends to zero. The last term to consider is\nt\nX\ns=2\nvs\u22121,t\u03b1s(\u03b7ss \u2212\u03b7s\u22121,s) =\nt\nX\ns=2\nvs\u22121,t\u03b1sE((Xs \u2212Xs\u22121)T PXsg(Xs,\u03bes))\n+\nt\nX\ns=2\nvs\u22121,t\u03b1sE((Xs\u22121 \u2212x\u2217)T (PXsg(Xs,\u03bes)) \u2212PXs\u22121g(Xs\u22121,\u03bes))).\nWe have\n\f\f\f\f\f\f\f\nt\nX\ns=2\nvs\u22121,t\u03b1sE((Xs \u2212Xs\u22121)T PXsg(Xs,\u03bes))\n\f\f\f\f\f\f\f\n\u22642C0C1M\nt\nX\ns=2\nvs\u22121,t\u03b12\ns (1 \u2212\u03c1(\u03c3s))\u22121\nand\n\f\f\f\f\f\f\f\nt\nX\ns=2\nvs\u22121,t\u03b1sE((Xs\u22121 \u2212x\u2217)T (PXsg(Xs,\u03bes)) \u2212PXs\u22121g(Xs\u22121,\u03bes)))\n\f\f\f\f\f\f\f\n\u22642M2C0C1(1 + C2)|X0 \u2212x\u2217|\nt\nX\ns=2\nvs\u22121,t\u03b12\ns \u03c3s(1 \u2212\u03c1(\u03c3s))\u22122\nand lemma 3.26 implies that both terms vanish at infinity. This concludes the proof\nof theorem 12.11.\n\u25a0\nProof (Proof of lemma 12.13) Condition (H3) and proposition 12.3 and imply that\n(since \u03c0x\u02dch = 0)\n|Pn\nx \u02dch(x,\u03be)| \u2264Dvar(Pn\nx (\u03be,\u00b7),\u03c0x)osc(\u02dch(x,\u00b7)) \u22642C1M\u03c1(x)n\nso that g is well defined with\n|g(x,\u00b7)| \u2264C0 + 2C1M(1 \u2212\u03c1(x))\u22121,\n|Pxg(x,\u00b7)| \u22642C1M(1 \u2212\u03c1(x))\u22121.\n12.7. MARKOVIAN STOCHASTIC APPROXIMATION\n291\nWe will also need to control differences of the kind\nPxg(x,\u03be) \u2212Pyg(y,\u03be).\nWe consider the nth term in the series, writing\nPn\nx \u02dch(x,\u03be) \u2212Pn\ny \u02dch(y,\u03be) =\nn\u22121\nX\nk=0\n(Pn\u2212k\nx\nPk\ny \u02dch(y,\u03be) \u2212(Pn\u2212k\u22121\nx\nPk+1\ny\n\u02dch(y,\u03be))\n+ Pn\nx \u02dch(x,\u03be) \u2212Pn\nx \u02dch(y,\u03be).\nThis gives\nPn\nx \u02dch(x,\u03be) \u2212Pn\ny \u02dch(y,\u03be) =\nn\u22121\nX\nk=0\nPn\u2212k\u22121\nx\n(PxPk\ny \u02dch(y,\u03be) \u2212Pk+1\ny\n\u02dch(y,\u03be) \u2212\u03c0xPk\ny \u02dch(y) + \u03c0xPk+1\ny\n\u02dch(y))\n+\nn\u22121\nX\nk=0\n(\u03c0xPk\ny \u02dch(y) \u2212\u03c0xPk+1\ny\n\u02dch(y)) + Pn\nx \u02dch(x,\u03be) \u2212Pn\nx \u02dch(y,\u03be)\n=\nn\u22121\nX\nk=0\nPn\u2212k\u22121\nx\n(PxPk\ny \u02dch(y,\u03be) \u2212Pk+1\ny\n\u02dch(y,\u03be) \u2212\u03c0xPk\ny \u02dch(y) + \u03c0xPk+1\ny\n\u02dch(y))\n+ \u03c0x\u02dch(y) \u2212\u03c0xPn\ny \u02dch(y) + Pn\nx \u02dch(x,\u03be) \u2212Pn\nx \u02dch(y,\u03be)\nFinally\nPn\nx h(x,\u03be) \u2212Pn\ny h(y,\u03be) =\nn\u22121\nX\nk=0\nPn\u2212k\u22121\nx\n(PxPk\ny \u02dch(y,\u03be) \u2212Pk+1\ny\n\u02dch(y,\u03be) \u2212\u03c0xPk\ny \u02dch(y) + \u03c0xPk+1\ny\n\u02dch(y))\n+ Pn\nx (\u02dch(x,\u03be) \u2212\u02dch(y,\u03be) + \u03c0x\u02dch(y)) \u2212(\u03c0x \u2212\u03c0y)Pn\ny \u02dch(y)\nUsing proposition 12.3, we can write, letting \u00af\u03c1 = max(\u03c1(|x|),\u03c1(|y|)),\n|Pn\u2212k\u22121\nx\n(PxPk\ny \u02dch(y,\u03be) \u2212Pk+1\ny\n\u02dch(y,\u03be) \u2212\u03c0xPk\ny \u02dch(y,\u03be) + \u03c0xPk+1\ny\n\u02dch(y,\u03be))|\n\u2264M \u00af\u03c1n\u2212k\u22121osc(PxPk\ny \u02dch(y,\u03be) \u2212Pk+1\ny\n\u02dch(y,\u03be))\n\u2264C2M \u00af\u03c1n\u2212k\u22121|x \u2212y|osc(Pk\ny \u02dch(y,\u03be))\n\u2264C2C1M2 \u00af\u03c1n\u22121|x \u2212y|\nWe also have\n|Pn\nx (\u02dch(x,\u03be) \u2212\u02dch(y,\u03be) + \u03c0x\u02dch(y,\u03be))| \u2264MC1 \u00af\u03c1n|x \u2212y|\nand\n|(\u03c0x \u2212\u03c0y)Pn\ny \u02dch(y,\u03be)| \u2264MC2C1 \u00af\u03c1n|x \u2212y|\n292\nCHAPTER 12. MONTE-CARLO SAMPLING\nso that\n|Pn\nx h(x,\u03be) \u2212Pn\ny h(y,\u03be)| \u2264MC1 \u00af\u03c1n\u22121(nMC2 + (1 + C2) \u00af\u03c1)|x \u2212y|\nFrom this, it follows that\n|Pxg(x,\u03be) \u2212Py(g(y,\u03be)| \u2264MC1\n\u221e\nX\nn=1\n\u00af\u03c1n\u22121(nMC2 + (1 + C2)|x \u2212y|\n= M2C1C2(1 \u2212\u00af\u03c1)\u22122 + MC1(1 + C2)(1 \u2212\u00af\u03c1)\u22121.\n\u25a0\nChapter 13\nMarkov Random Fields\nWith this chapter, we start a discussion of large-scale statistical models in data sci-\nence, starting with graphical models (Markov random fields and Bayesian networks)\nbefore discussing more recent approaches using, notably, deep learning. Impor-\ntant textbook references for the present chapter include Pearl [151], Ancona et al.\n[8], Winkler [203], Lauritzen [114], Cowell et al. [56], Koller and Friedman [108].\n13.1\nIndependence and conditional independence\n13.1.1\nDefinitions\nWe consider random variables X,Y,Z ..., and denote by RX,RY,RZ ... the sets in\nwhich they take their values. We discuss in this section concepts of independence\nand conditional independence between random variables. To simplify the exposi-\ntion, we will work (unless mentioned otherwise) with discrete random variables (X\nis discrete if RX is finite or countable)1. We start with a basic definition.\nDefinition 13.1 Two discrete random variables X : \u2126\u2192RX and Y : \u2126\u2192RY are inde-\npendent if and only if\n\u2200x \u2208RX,\u2200y \u2208RY : P(X = x,Y = y) = P(X = x)P(Y = y).\nThe general definition for arbitrary r.v.\u2019s is that\nE(f (X)g(Y)) = E(f (X))E(g(Y))\nfor any pair of (measurable) non-negative functions f : RX \u2192[0,+\u221e) and g : RY \u2192\n[0,+\u221e).\n1In the general case, RX,RY,... are metric spaces with a countable dense subset with \u03c3-algebras\nSX,SY,...\n293\n294\nCHAPTER 13. MARKOV RANDOM FIELDS\nOne can easily check that X and Y are independent if and only if, for any non-\nnegative function g : RY \u2192R, one has\nE(g(Y) | X) = E(g(Y)).\nNotation 13.2 Independence is a property that involves two variables X and Y and\nan underlying probability distribution P. Independence of X and Y relative to P will\nbe denoted (XyY)P. However we will only write XyY when there is no ambiguity\non P.\n\u2666\nMore than independence, the concept of conditional independence will be fun-\ndamental in this chapter. It requires three variables, say X,Y,Z. Returning to the\ndiscrete case, one says that X and Y are conditionally independent given Z is, for\nany x \u2208RX, y \u2208RY and z \u2208RZ such that P(Z = z) > 0,\nP(X = x,Y = y | Z = z) = P(X = x | Z = z)P(Y = y | Z = z).\n(13.1)\nAn equivalent statement is that, for any z such that P(Z = z) , 0, X and Y are inde-\npendent when P is replaced by the conditional distribution P(\u00b7 | Z = z).\nIn the general case conditional independence means that, for any pair of non-\nnegative measurable functions f and g,\nE(f (X)g(Y) | Z) = E(f (X) | Z)E(g(Y) | Z).\n(13.2)\nFrom now, we restrict our discussion to discrete random variables.\nMultiplying both terms in (13.1) by P(Z = z)2, we get the equivalent statement:\nX and Y are conditionally independent given Z if and only if,\n\u2200x,y,z : P(X = x,Y = y,Z = z)P(Z = z) = P(X = x,Z = z)P(Y = y,Z = z).\n(13.3)\nNote that the identity is meaningful, and always true, for P(Z = z) = 0, so that this\ncase does not need to be excluded anymore.\nConditional independence can be interpreted by the statement that X brings no\nmore information on Y than what is already provided by Z: one has\nP(Y = y | X = x,Z = z) = P(Y = y,X = x,Z = z)\nP(X = x,Z = z)\n= P(Y = y,Z = z)\nP(Z = z)\nas directly deduced from (13.3). (This computation being valid as soon as P(X =\nx,Z = z) > 0.)\nNotation 13.3 To indicate that X and Y are conditionally independent given Z for\nthe distribution P, we will write (XyY | Z)P or simply (XyY | Z).\n\u2666\n13.1. INDEPENDENCE AND CONDITIONAL INDEPENDENCE\n295\nSo we have the equivalence:\n(XyY | Z)P \u21d4\n\u0010\n\u2200z : P(Z = z) > 0 \u21d2(XyY)P(|Z=z)\n\u0011\n.\nAbsolute independence is like \u201cindependence conditional to no variable\u201d, and we\nwill use the notation \u2205for the \u201cempty\u201d random variable that contains no information\n(for example, a set-valued random variable that always returns the empty set, or any\nconstant variable). So we have the tautology\nXyY \u21d4(XyY | \u2205).\nNote that, dealing with discrete variables, all previous definitions automatically\nextend to groups of variables: for example, if Z1, Z2 are two discrete variables, so\nis Z = (Z1,Z2) and we immediately obtain a definition for the conditional indepen-\ndence of X and Y given Z1 and Z2, denoted (XyY | Z1,Z2).\n13.1.2\nFundamental properties\nProposition 13.5 below lists important properties of conditional independence that\nwill be used repeatedly in this chapter. Before stating this proposition, we need the\nfollowing definition.\nDefinition 13.4 One says that the joint distribution of the random variables (X1,...,XN)\nis positive if there exists subsets \u02dcRk \u2282RXk, k = 1,...,N such that P(Xk \u2208\u02dcRk) = 1 and:\nP(X1 = x1,...,XN = xN) > 0\nif xk \u2208\u02dcRk, k = 1,...,N.\nNote that the condition implies P(Xk = xk) > 0 for all xk \u2208\u02dcRk, so that \u02dcRk = {xk \u2208\nRXk : P(Xk = xk) > 0}, i.e., \u02dcRk is the support of PXk. One can interpret the definition\nas expressing the fact that any conjunction of events for different Xk\u2019s has positive\nprobability, as soon as each of them has positive probability (if all events may occur,\nthen they may occur together).\nNote that the sets \u02dcRk depend on X1,...,XN. However, if this family of variables\nis fixed, there is no loss in generality in restricting the space RXk to \u02dcRk and there for\nassume that P(X1 = x1,...,XN = xN) > 0 everywhere.\nProposition 13.5 Let X,Y,Z and W be random variables. The following properties are\ntrue.\n(CI1) Symmetry: (XyY | Z) \u21d2(YyX | Z).\n296\nCHAPTER 13. MARKOV RANDOM FIELDS\n(CI2) Decomposition: (Xy(Y,W) | Z) \u21d2(XyY | Z).\n(CI3) Weak union: (Xy(Y,W) | Z) \u21d2(XyY | (Z,W)).\n(CI4) Contraction: (XyY | Z) and (XyW | (Z,Y)) \u21d2(Xy(Y,W) | Z).\n(CI5) Intersection: assume that the joint distribution of W,Y and Z is positive. Then\n(XyW | (Z,Y)) and (XyY | (Z,W)) \u21d2(Xy(Y,W) | Z).\nProof Properties (CI1) and (CI2) are easily deduced from (13.3) and left to the\nreader. To prove the last three, we will use the notation P(x),P(x,y) etc. instead\nof P(X = x),P(X = x,Y = y), etc. to save space. Identities are assumed to hold for all\nx,y,z,w unless stated otherwise.\nFor (CI3), we must prove, according to (13.3), that\nP(x,y,z,w)P(z,w) = P(x,z,w)P(y,z,w)\n(13.4)\nwhenever P(x,y,z,w)P(z) = P(x,z)P(y,z,w). Summing this last equation over y (or\napplying (CI2)) yields P(x,z,w)P(z) = P(x,z)P(z,w). We can note that all terms in\n(13.4) vanish when P(z) = 0, so that the identity is true in this case. When P(z) , 0,\nthe right-hand side of (13.4) becomes\n(P(x,z)P(z,w)/P(z))P(y,z,w) = (P(x,z)P(y,z,w)/P(z))P(z,w) = P(x,y,z,w)P(z,w),\nusing once again the hypothesis. This proves (CI3).\nFor (CI4), the hypotheses are\n\uf8f1\uf8f4\uf8f4\uf8f2\uf8f4\uf8f4\uf8f3\nP(x,y,z)P(z) = P(x,z)P(y,z)\nP(x,y,z,w)P(y,z) = P(x,y,z)P(y,z,w)\nand the conclusion must be\nP(x,y,z,w)P(z) = P(x,z)P(y,z,w).\n(13.5)\nSince (13.5) is true when P(y,z) = 0, we assume that this probability does not vanish\nand write\nP(x,y,z,w)P(z)\n=\nP(x,y,z)P(z)P(y,z,w)/P(y,z)\n=\nP(x,z)P(y,z)P(y,z,w)/P(y,z)\n=\nP(x,z)P(y,z,w)\nyielding (13.5).\n13.1. INDEPENDENCE AND CONDITIONAL INDEPENDENCE\n297\nFor (CI5), assuming\n\uf8f1\uf8f4\uf8f4\uf8f2\uf8f4\uf8f4\uf8f3\nP(x,y,z,w)P(y,z) = P(x,y,z)P(y,z,w)\nP(x,y,z,w)P(z,w) = P(x,z,w)P(y,z,w),\n(13.6)\nwe want to show that\nP(x,y,z,w)P(z) = P(x,z)P(y,z,w).\nSince this identity is true when any of the events W = w,Y = y or Z = z has zero\nprobability, we can assume that their probabilities are positive, which, by assump-\ntion, also implies that all joint probabilities are positive. From the two identities, we\nget\nP(x,y,z,w)/P(y,z,w) = P(x,y,z)/P(y,z) = P(x,z,w)/P(z,w)\nThis implies\nP(x,y,z) = P(y,z)P(x,z,w)/P(z,w)\nthat we can sum over y to obtain\nP(x,z) = P(z)P(x,z,w)/P(z,w)\nWe therefore get\nP(x,y,z,w)/P(y,z,w) = P(x,z,w)/P(z,w) = P(x,z)/P(z),\nwhich is what we wanted.\n\u25a0\nA counter-example of (CI5) when the positivity assumption is not satisfied can be\nbuilt as follows: let X be a Bernoulli random variable, and let Y = W = X. Let Z be\nany Bernoulli random variable, independent from X. Given Z and W, X and Y are\nconstant and therefore independent. Similarly, given Z and Y, X and W are constant\nand therefore independent. However, given Z, X and (Y,W) are not independent\n(they are equal and non constant).\n13.1.3\nMutual independence\nAnother concept of interest is the mutual (conditional) independence of more than\ntwo random variables. The random variables (X1,...,Xn) are mutually conditionally\nindependent given Z if and only if\nE(f1(X1)\u00b7\u00b7\u00b7fn(Xn) | Z) = E(f1(X1) | Z)\u00b7\u00b7\u00b7E(fn(Xn) | Z)\nfor any non-negative measurable functions f1,...,fn. In terms of discrete probabili-\nties, this can be written as\nP(X1 = x1,...,Xn = xn,Z = z)P(Z = z)n\u22121 =\nP(X1 = x1,Z = z)\u00b7\u00b7\u00b7P(Xn = xn,Z = z).\n298\nCHAPTER 13. MARKOV RANDOM FIELDS\nThis will be summarized with the notation\n(X1y\u00b7\u00b7\u00b7yXn | Z).\nWe have the proposition\nProposition 13.6 For variables X1,...,Xn and Z, the following properties are equivalent.\n(i) (X1y\u00b7\u00b7\u00b7yXn | Z);\n(ii) For all S,T \u2282{1,...,n} with S \u2229T = \u2205, we have: ((Xi,i \u2208S)y(Xj,j \u2208T) | Z);\n(iii) For all s \u2208{1,...,n}, we have: (Xsy(Xt,t , s) | Z);\n(iv) For all s \u2208{2,...,n}, we have: (Xsy(X1,...,Xs\u22121) | Z).\nProof It is clear that (i) \u21d2\u00b7\u00b7\u00b7 \u21d2(iv) so it suffices to prove that (iv) \u21d2(i). For this,\nsimply write (applying (iv) repeatedly to s = n \u22121,n \u22122,...)\nE(f1(X1)\u00b7\u00b7\u00b7fn(Xn) | Z)\n= E(f1(X1)\u00b7\u00b7\u00b7fn\u22121(Xn\u22121) | Z)E(fn(Xn) | Z)\n= E(f1(X1)\u00b7\u00b7\u00b7fn\u22122(Xn\u22122) | Z)E(fn\u22121(Xn\u22121) | Z)\nE(fn(Xn) | Z)\n...\n= E(f1(X1) | Z)\u00b7\u00b7\u00b7E(fn(Xn) | Z).\n13.1.4\nRelation with Information Theory\nSeveral concepts in information theory are directly related to independence between\nrandom variables. Recall that the (Shannon) entropy of a discrete probability distri-\nbution over a finite set R is defined by\nH(P) = \u2212\nX\n\u03c9\u2208R\nlogP(\u03c9)P(\u03c9).\n(13.7)\nSimilarly, the entropy of a random variable X : \u2126\u2192RX is defined by\nH(X)\n\u2206= H(PX) = \u2212\nX\nx\u2208RX\nlogP(X = x)P(X = x).\n(13.8)\nThe entropy is always non-negative, and provides a measure of the uncertainty as-\nsociated to P. For a given finite set R, it is maximal when P is uniform over R, and\nminimal (and vanishes) when P is supported by a single \u03c9 \u2208R (i.e. P(\u03c9) = 1).\n13.1. INDEPENDENCE AND CONDITIONAL INDEPENDENCE\n299\nOne defines the entropy of two or more random variables as the entropy of their\njoint distribution, so that, for example,\nH(X,Y) = \u2212\nX\n(x,y)\u2208RX\u00d7RY\nlogP(X = x,Y = y)P(X = x,Y = y).\nWe have the proposition:\nProposition 13.7 For random variables X1,...,Xn, one has\nH(X1,...,Xn) \u2264H(X1) + \u00b7\u00b7\u00b7 + H(Xn)\nwith equality if and only if (X1,...,Xn) are mutually independent.\nProof The proof of this proposition uses properties of the Kullback-Leibler diver-\ngence (c.f. (4.3)), given by, for two probability distributions \u03c0 and \u03c0\u2032 on a finite set\nB,\nKL(\u03c0\u2225\u03c0\u2032) =\nX\n\u03c9\u2208B\n\u03c0(\u03c9)log \u03c0(\u03c9)\n\u03c0\u2032(\u03c9).\nwith the convention \u03c0log(\u03c0/\u03c0\u2032) = 0 if \u03c0 = 0 and = \u221eif \u03c0 > 0 and \u03c0\u2032 = 0. Returning to\nproposition 13.7, a straightforward computation (which is left to the reader) shows\nthat\nH(X1) + \u00b7\u00b7\u00b7 + H(Xn) \u2212H(X1,...,Xn) = KL(\u03c0\u2225\u03c0\u2032)\nwith \u03c0(x1,...,xn) = P(X1 = x1,...,Xn = xn) and \u03c0\u2032(x1,...,xn) = Qn\nk=1P(Xk = xk). This\nmakes proposition 13.7 a direct consequence of proposition 4.1.\n\u25a0\nThe mutual information between two random variables X and Y is defined by\nI(X,Y) = H(X) + H(Y) \u2212H(X,Y).\n(13.9)\nFrom proposition 13.7, I(X,Y) is nonnegative and vanishes if and only if X and\nY are independent. Also from the proof of proposition 13.7, I(X,Y) is equal to\nKL(P(X,Y)\u2225PX \u2297PY) where the first probability is the joint distribution of X and Y and\nthe second one the product of the marginals of X and Y, which coincides with PX,Y\nif and only if X and Y are independent.\nIf X and Y are two random variables, and y \u2208RY with P(Y = y) > 0, the entropy\nof the conditional probability x 7\u2192P(X = x | Y = y) is denoted H(X | Y = y), and\nis a function of y. The conditional entropy of X given Y, denoted H(X | Y) is the\nexpectation of H(X | Y = y) for the distribution of Y, i.e.,\nH(X | Y) =\nX\ny\u2208RY\nH(X | Y = y)P(Y = y)\n= \u2212\nX\nx\u2208RX\nX\ny\u2208RY\nlogP(X = x | Y = y)P(X = x,Y = y).\nSo, we have (with a straightforward proof)\n300\nCHAPTER 13. MARKOV RANDOM FIELDS\nProposition 13.8 Given two random variables X and Y, we have\nH(X | Y)\n=\n\u2212EX,Y(logP(X = \u00b7 | Y = \u00b7))\n(13.10)\n=\nH(X,Y) \u2212H(Y)\nThis proposition also immediately yields:\nI(X,Y) = H(X) \u2212H(X | Y) = H(Y) \u2212H(Y | X).\n(13.11)\nThe identity H(X,Y) = H(X | Y)+H(Y) that is deduced from proposition 13.8 can be\ngeneralized to more than two random variables (the proof being left to the reader),\nyielding, if X1,...,Xn are random variables:\nH(X1,...,Xn) =\nn\nX\nk=1\nH(Xk | X1,...,Xk\u22121).\n(13.12)\nIf Z is an additional random variable, the following identity is obtained by ap-\nplying the previous one to conditional distributions given Z = z and taking averages\nover z:\nH(X1,...,Xn | Z) =\nn\nX\nk=1\nH(Xk | X1,...,Xk\u22121,Z).\n(13.13)\nThe following proposition characterizes conditional independence in terms of\nentropy.\nProposition 13.9 Let X,Y and Z be three random variables. The following statements\nare equivalent.\n(i) X and Y are conditionally independent given Z.\n(ii) H(X,Y | Z) = H(X | Z) + H(Y | Z)\n(iii) H(X | Y,Z) = H(X | Y)\nMoreover, when (i) to (iii) are satisfied, we have:\n(iv) I(X,Y) \u2264min(I(X,Z),I(Y,Z)).\nProof From proposition 13.7, we have, for any three random variables X,Y,Z, and\nany z such that P(Z = z) > 0,\nH(X,Y | Z = z) \u2264H(X | Z = z) + H(Y | Z = z).\n13.2. MODELS ON UNDIRECTED GRAPHS\n301\nTaking expectations on both sides implies the important inequality\nH(X,Y | Z) \u2264H(X | Z) + H(Y | Z)\n(13.14)\nand equality occurs if and only if P(X = x,Y = y | Z = z) = P(X = x | Z = z)P(Y =\ny | Z = z) whenever P(Z = z) > 0, that is, if and only if X and Y are conditionally\nindependent given Z. This proves that (i) and (ii) are equivalent. The fact that\n(ii) and (iii) are equivalent comes from (13.13), which gives, for any three random\nvariables\nH(X,Y | Z) = H(X | Y,Z) + H(Y | Z).\n(13.15)\nTo prove that (i)-(iii) implies (iv), we note that (13.14) and (13.15) imply that, for\nany three random variables:\nH(X | Y,Z) \u2264H(X | Y).\nIf X and Y are conditionally independent given Z, then the right-hand side is equal\nto H(X | Z) and this yields\nI(X,Y) = H(X) \u2212H(X | Y) \u2264H(X) \u2212H(X | Z) = I(X,Z).\nBy symmetry, we must also have I(X,Y) \u2264I(Y,Z) so that (iv) is true.\n\u25a0\nStatement (iv) is often called the data-processing inequality, and has been used to\ninfer conditional independence within gene networks [125].\n13.2\nModels on undirected graphs\n13.2.1\nGraphical representation of conditional independence\nAn undirected graph is a collection of vertexes and edges, in which edges link pairs\nof vertexes without order. Edges can therefore be identified to subsets of cardinality\ntwo of the set of vertexes, V . This yields the definition:\nDefinition 13.10 An undirected graph G is a pair G = (V ,E) where V is a finite set of\nvertexes and elements e \u2208E are subsets e = {s,t} \u2282V .\nNote that edges in undirected graphs are defined as sets, i.e., unordered pairs, which\nare delimited with braces in these notes. Later on, we will use parentheses to repre-\nsent ordered pairs, (s,t) , (t,s). We will write s \u223cG t, or simply s \u223ct to indicate that s\nand t are connected by an edge in G (we also say that s and t are neighbors in G).\n302\nCHAPTER 13. MARKOV RANDOM FIELDS\nDefinition 13.11 A path in an undirected graph G = (V ,E) is a finite sequence (s0,...,sN)\nof vertexes such that sk\u22121 \u223csk \u2208E. (A sequence, (s0), of length 1 is also a path by exten-\nsion.)\nWe say that s and t are connected by a path if either s = t or there exists a path\n(s0,...,sN) such that s0 = s and sN = t.\nA subset S \u2282G is connected if any pair of elements in S can be connected by a path.\nA subset T \u2282G separates two other subsets S and S\u2032 if all paths between S and S\u2032\nmust pass in T. We will write (SyS\u2032 | T) in such a case.\nOne of the goals of this chapter is to relate the notion of conditional indepen-\ndence within a set of variables to separation in a suitably chosen undirected graph\nwith vertexes in one-to-one correspondence with the variables. This will also justi-\nfies the similarity of notation used for separation and conditional independence.\nWe have the following simple fact:\nLemma 13.12 Let G = (V ,E) be an undirected graph, and S,S\u2032,T \u2282V . Then\n(SyS\u2032 | T) \u21d2S \u2229S\u2032 \u2282T .\nIndeed, if (SyS\u2032 | T) and s0 \u2208S \u2229S\u2032, the path (s0) links S and S\u2032 and therefore must\npass in T .\nProposition 13.5 translates into similar properties for separation:\nProposition 13.13 Let (V ,E) be an undirected graph and S,T ,U,R be subsets of V . The\nfollowing properties hold\n(i) (SyT |U) \u21d4(T yS|U).\n(ii) (SyT \u222aR|U) \u21d2(SyT|U).\n(iii) (SyT \u222aR|U) \u21d2(SyT |U \u222aR).\n(iv) (SyT | U) and (SyR | U \u222aT) \u21d4(SyT \u222aR | U).\n(v) U \u2229R = \u2205,(SyR | U \u222aT) and (SyU | T \u222aR) \u21d2(SyU \u222aR | T).\nProof (i) is obvious, and for (ii) (and (iii)), if any path between S and T \u222aR must\npass by U, the same is obviously true for a path between S and T.\nFor the \u21d2part of (iv), if a path links S and T \u222aR, then it either links S and T\nand must pass through U by the first assumption, or link S and R and therefore pass\nthrough U or T by the second assumption. But if the path passes through T, it must\n13.2. MODELS ON UNDIRECTED GRAPHS\n303\nalso pass through U before by the first assumption. In all cases, the path passes\nthrough U. The \u21d0part of (iv) is obvious.\nFinally, consider (v) and take a path between two distinct elements in S and U\u222aR.\nConsider the first time the path hits U or R, and assumes that it hits U (the other\ncase being treated similarly by symmetry). Notice that the path cannot hit both U\nand R at the same point since U \u2229R = \u2205. From the assumptions, the path must hit\nT \u222aR before passing by U, and the intersection cannot be in R, so it is in T , which is\nthe conclusion we wanted.\n\u25a0\nTo make a connection between separation in graphs and conditional indepen-\ndence between random variables, we consider a graph G = (V ,E) and a family of\nrandom variables (X(s),s \u2208V ) indexed by V . Each variable is assumed to take values\nin a set Fs = RX(s). The collection of values taken by the random variables will be\ncalled configurations, and the sets Fs,s \u2208V are called the state spaces.\nLetting F denote the collection (Fs,s \u2208V ), we will denote the set of such configu-\nrations as F (V ,F ). Then F is clear from the context, we will just write F (V ). If S \u2282V\nand x \u2208F (V ,F ), the restriction of x to S is denoted x(S) = (x(s),s \u2208S). The set formed\nby those restrictions will be denoted F (S,F ) (or just F (S)).\nRemark 13.14 Some care needs to be given to the definition of the space of con-\nfigurations, to avoid ambiguities when two sets Fs coincide. The configuration x =\n(x(s),s \u2208V ) should be understood, in an emphatic way, as the collection \u02c6x = ((s,x(s)),s \u2208\nV ), which makes explicit the fact that x(s) is the value observed at vertex s. Similarly\nthe emphatic notation for x(S) \u2208F (V ,F ) is \u02c6x(S) = ((s,x(s)),s \u2208S).\nIn the following, we will not use the emphatic notation to avoid overly heavy\nexpressions, but its relevance should be clear with the following simple example.\nTake V = {1,2,3} and F1 = F2 = F3 = {0,1}. Let x(1) = 0, x(2) = 0 and x(3) = 1. Then\nthe sub-configurations x({1,3}) and x({2,3}) both corresponds to values (0,1), but we\nconsider them as distinct. In the same spirit, x(1) = x(2), but x({1}) , x({2})\n\u2666\nIf S,T \u2282V with S \u2229T = \u2205, x(S) \u2208F (S,F ), y(T ) \u2208F (T ,F ), we will denote their\nconcatenation by x(S) \u2227y(T ), which is the configuration z = (zs,s \u2208S \u222aT) \u2208F (T \u222aS,F )\nsuch that z(s) = x(s) if s \u2208S and z(s) = y(s) if s \u2208T.\nWe define a random field over V as a random configuration X : \u2126\u2192F (V ,F ), that\nwe will denote for short X = (X(s),s \u2208V ). If S \u2282V , the restriction X(S) will also be\ndenoted (X(s),s \u2208S).\nWe can now write the definition:\n304\nCHAPTER 13. MARKOV RANDOM FIELDS\nDefinition 13.15 Let G = (V ,E) be an undirected graph and X = (X(s),s \u2208V ) a random\nfield over V . We say that X is Markov (or has the Markov property) relative to G (or is\nG-Markov, or is a Markov random field on G) if and only if, for all S,T ,U \u2282V :\n(SyT | U) \u21d2(X(S)yX(T) | X(U)).\n(13.16)\nLetting the observation over an empty set S be empty, i.e., X\u2205= \u2205, this definition in-\ncludes the statement that, if S and T are disconnected (i.e., there is no path between\nthem: they are separated by the empty set), then (X(S)yX(T) | \u2205): X(S) and X(T ) are\nindependent.\nWe will say that a probability distribution \u03c0 on F (V ) is G-Markov if its associated\ncanonical random field X = (X(s),s \u2208V ) defined on \u02dc\u2126= F (V ) by X(s)(x) = x(s) is G-\nMarkov.\n13.2.2\nReduction of the Markov property\nWe now proceed, in a series of steps, to a simplification of definition 13.15 in order\nto obtain a minimal number of conditional independence statements. Note that, in\nits current form, definition 13.15 requires to check (13.16) for any three subsets of\nV , which provides a huge number of conditions. Fortunately, as we will see, these\nconditions are not independent, and checking a much smaller number of them will\nensure that all of them are true.\nThe first step for our reduction is provided by the following lemma.\nLemma 13.16 Let G = (V ,E) be an undirected graph and X = (Xs,s \u2208V ) a set of random\nvariables indexed by V . Then X is G-Markov if and only if, for S,T ,U \u2282V ,\nS \u2229U = T \u2229U = \u2205and (SyT | U) \u21d2(X(S)yX(T) | X(U)).\n(13.17)\nProof Assume that (13.17) is true, and take any S,T ,U with (SyT | U). Let A =\nS \u2229U, B = T \u2229U and C = A \u222aB. Partition S in S = S1 \u222aA, T in T1 \u222aB and U in\nU1 \u222aC. From (SyT | U), we get (S1yT1 | U). Since S1 \u2229U = T1 \u2229U = \u2205, this implies\n(X(S1)yX(T1) | X(U)). But this implies ((X(S1),X(A))y(X(T1),X(B)) | X(U)). Indeed, this\nproperty requires\nPX(x(S1) \u2227x(A) \u2227x(T1) \u2227x(B) \u2227x(U1) \u2227y(C))PX(x(U1) \u2227y(C))\n= PX(x(S1) \u2227x(A) \u2227x(U1) \u2227y(C))PX(x(T1) \u2227x(B) \u2227x(U1) \u2227y(C))\nIf the configurations x(A),x(B),y(C) are not consistent (i.e., x(t) , y(t) for some t \u2208C),\nthen both sides vanish. So we can assume x(C) = y(C) and remove x(A) and x(B) from\nthe expression, since they are redundant. The resulting identity is true since it ex-\nactly states that (X(S1)yX(T1) | X(U)).\n\u25a0\n13.2. MODELS ON UNDIRECTED GRAPHS\n305\nDefine the set of neighbors of s \u2208V (relative to the graph G) as the set of t , s\nsuch that {s,t} \u2208E and denote this set by Vs. For S \u2282V define also\nVS = Sc \u2229\n[\ns\u2208S\nVs\nwhich is the set of neighbors of all vertexes in S that do not belong to S. (Here Sc\ndenotes the complementary set of S, Sc = V \\ S.) Finally, let WS denote the vertexes\nthat are \u201cremote\u201d from S, WS = (S \u222aVS)c.\nWe have the following important reduction of the condition in definition 13.15.\nProposition 13.17 X is Markov relative to G if and only if, for any S \u2282V ,\n(X(S)yX(WS) | X(VS)).\n(13.18)\nThis says that\nP(X(S) = x(S) | X(Sc) = x(Sc))\nonly depends (when defined) on variables x(t) for t \u2208S \u222aVS.\nProof First note that (SyWS | VS) is always true, since any path reaching S from Sc\nmust pass through VS. This immediately proves the \u201conly if\u201d part of the proposition.\nConsider now the \u201cif\u201d part. Take S,T ,U such that (SyT | U). We want to prove\nthat (XSyXT | XU). According to lemma 13.16, we can assume, without loss of gen-\nerality, that S \u2229U = T \u2229U = \u2205.\nDefine R as the set of vertexes v in V such that there exists a path between S and\nv that does not pass in U. Then:\n1. S \u2282R: the path (s) for s \u2208S does not pass in U since S \u2229U = \u2205.\n2. U \u2229R = \u2205by definition.\n3. VR \u2282U: assume that there exists a point r in VR which is not in U. Then r has a\nneighbor, say r\u2032 in R. By definition of R, there exists a path from S to r\u2032 that does not\nhit U, and this path can obviously be extended by adding r at the end to obtain a\npath that still does not hit U. But this implies that r \u2208R, which contradicts the fact\nthat VR \u2229R = \u2205.\n4. T \u2229(R \u222aVR) = \u2205: if t \u2208T, then t < R from (SyT | U) and t < VR from T \u2229U = \u2205.\nWe can then write (each decomposition being a partition, implicitly defining the\nsets A, B and C, see Fig. 13.1) R = S \u222aA, U = VR \u222aC, (R \u222aVR)c = T \u222aC \u222aB, and from\n(X(R)yX(WR) | X(VR)), we get\n((X(S),X(A))y(X(T),X(C),X(B)) | X(VR))\n306\nCHAPTER 13. MARKOV RANDOM FIELDS\nFigure 13.1: See proof of proposition 13.17 for details\nwhich implies\n((X(S),X(A))y(X(T),X(B)) | X(U))\nby (CI3), which finally implies (X(S)yX(T ) | X(U)) by (CI2).\n\u25a0\nFor positive probabilities, it suffices to consider singletons in proposition 13.17.\nProposition 13.18 If the joint distribution of (X(s),s \u2208V ) is positive and, for any s \u2208V ,\n(X(s)yX(Ws) | X(Vs)),\n(13.19)\nthen X is Markov relative to G. The converse statement is true without the positivity\nassumption.\nProof It suffices to prove that, if (13.18) is true for S and T \u2282V , with T \u2229S = \u2205, it is\nalso true for S \u222aT. The result will then follow by induction.\nSo, let U = VS\u222aT and R = WS\u222aT = V \\ (S \u222aT \u222aU). Then, we have\n(X(S)yX(WS) | X(VS)) \u21d2(X(S)yX(R) | (X(U),X(T)))\nbecause R \u2282WS (if s \u2208VS, then it is either in U or in T and therefore cannot be in\nR). Similarly, (X(T )yX(R) | (X(U),X(S))), and (CI5) (for which we need P positive) now\nimplies ((X(T ),X(S))yX(R) | X(U)).\n\u25a0\nTo see that the positivity assumption is needed, consider the following example with\nsix variables X(1),...,X(6), and a graph linking consecutive integers and closing with\n13.2. MODELS ON UNDIRECTED GRAPHS\n307\nan edge between 1 and 6. Assume that X(1) = X(2) = X(4) = X(5), and that X(1),X(3)\nand X(6) are independent. Then (13.19) is true, since, for k = 1,2,4,5, X(k) is constant\ngiven its neighbors, and X(3) (resp. X(6)) is independent of the rest of the variables.\nBut (X(1),X(2)) is not independent of (X(4),X(5)) given the neighbors X(3),X(6).\nFinally, another statement equivalent to proposition 13.18 is the following:\nProposition 13.19 If the joint distribution of (X(s),s \u2208V ) is positive and, for any s,t \u2208V ,\ns \u2241G t \u21d2(X(s)yX(t) | X(V \\{s,t})),\nthen X is Markov relative to G. The converse statement is true without the positivity\nassumption.\nProof Fix s \u2208V and assume that (X(s)yX(R) | X(V \\R)) for any R \u2282Ws with cardinality\nat most k (the statement is true for k = 1 by assumption). Consider a set \u02dcR \u2282Ws of\ncardinality k + 1, that we decompose into R \u222a{t} for some t \u2208\u02dcR. We have (X(s)yX(t) |\nX(V \\ \u02dcR),XR) from the initial hypothesis and (X(s)yX(R) | X(V \\ \u02dcR),Xt) from the induction\nhypothesis. Using property (CI5), this yields (X(s)yX( \u02dcR) | X(V \\ \u02dcR)). This proves the\nproposition by induction.\n\u25a0\nRemark 13.20 It is obvious from the definition of a G-Markov process that, if X is\nMarkov for a graph G = (V ,E), it is automatically Markov for any richer graph, i.e.,\nany graph \u02dcG = (V , \u02dcE) with E \u2282\u02dcE. This is because separation in \u02dcG implies separation\nin G. Moreover, any X is G-Markov for the complete graph on V , for which s \u223ct for\nall s , t \u2208V . This is because no pair of sets can be separated in a complete graph.\nAny graph with respect to which X is Markov must be richer than the graph\nGX = (V ,EX) defined by s \u2241GX t if and only (X(s)yX(t) | X({s,t}c)). This is true because,\nfor any graph G for which X is Markov, we have\ns \u2241G t \u21d2(X(s)yX(t) | X({s,t}c)) \u21d2s \u2241GX t.\nInterestingly, proposition 13.19 states that X is GX-Markov as soon as its joint dis-\ntribution is positive. This implies that GX is the minimal graph over which X is\nMarkov in this case.\n\u2666\n13.2.3\nRestricted graph and partial evidence\nAssume that some variables X(T) = (X(t),t \u2208T) (with T \u2282V ) have been observed,\nwith observed values x(T ) = (x(t),t \u2208T ). One would like to use this partial evidence\nto get additional information on the remaining variables, X(S) where S = V \\T. From\nthe probabilistic point of view, this means computing the conditional distribution of\nX(S) given X(T) = x(T ).\n308\nCHAPTER 13. MARKOV RANDOM FIELDS\nOne important property of G-Markov models is that the Markov property is es-\nsentially conserved when passing to conditional distributions. We introduce for this\nthe following definitions.\nDefinition 13.21 If G = (V ,E) is an undirected graph, a subgraph of G is a graph G\u2032 =\n(V \u2032,E\u2032) with V \u2032 \u2282V and E\u2032 \u2282E.\nIf S \u2282V , the restricted graph, GS, of G to S is defined by\nGS = (S,ES) with ES = {e = {s,t} : s,t \u2208S and e \u2208E}.\n(13.20)\nWe have the following proposition.\nProposition 13.22 Let G = (V ,E) be an undirected graph and X be G-Markov. Let S \u2282V\nand T = Sc. Given a partial evidence x(T ) such that P(X(T) = x(T )) > 0, X(S), conditionally\nto X(T ) = x(T ), is GS-Markov.\nProof The proof is straightforward once it is noticed that\n(AyB | C)GS \u21d2(AyB | C \u222aT )G\n\u25a0\nso that\n(AyB | C)GS\n\u21d2\n(X(A)yX(B) | X(C),X(T ))P\n\u21d2\n(X(A)yX(B) | X(C))P(\u00b7|X(T )=x(T ))\n13.2.4\nMarginal distributions\nThe effect of taking marginal distributions for a G-Markov model is, unfortunately,\nnot as much a mild operation as computing conditional distributions, in the sense\nthat the conditional independence structure of the marginal distribution may be\nmuch more complex than the original one.\nLet G = (V ,E) be an undirected graph, and let S be a subset of V . Define the\ngraph GS = (S,ES) by {s,t} \u2208ES if and only if {s,t} \u2208E or there exist u,u\u2032 \u2208Sc such\nthat {s,u} \u2208E, {t,u\u2032} \u2208E and u and u\u2032 are connected by a path in Sc. In other terms\nES links all s,t \u2208S that can be connected by a path, all but the extremities of which\nare included in Sc. With this notation, the following proposition holds.\nProposition 13.23 Let G = (V ,E) be an undirected graph, and S \u2282V . Assume that X =\n(X(s),s \u2208V ) is a family of random variables which is G-Markov. Then X(S) = (x(s),s \u2208S)\nis GS-Markov.\n13.3. THE HAMMERSLEY-CLIFFORD THEOREM\n309\nProof It suffices to prove that, for A,B,C \u2282S,\n(AyB | C)GS \u21d2(AyB | C)G.\nSo, assume that A and B are separated by C in GS. If a path connects A and B in G,\nwe can, by definition of ES, remove from this path any portion that passes in Sc and\nobtain a valid path in GS. By assumption, this path must pass in C, and therefore so\ndoes the original path.\n\u25a0\nThe graph GS can be much more complex than the restricted graph GS intro-\nduced in the previous section (note that, by definition, GS is richer than GS). Take,\nfor example, the graph that corresponds to \u201chidden Markov models,\u201d for which (cf.\nfig. 13.2)\nV = {1,...,N} \u00d7 {0,1}\nand edges {s,t} \u2208E have either s = (k,0) and t = (l,0) with |k \u2212l| = 1, or s = (k,0) and\nt = (k,1). Let S = {1,...,N} \u00d7 {1}. Then, GS is totally disconnected (ES = \u2205), since no\nedge in G links two elements of S. In contrast, any pair of elements in S is connected\nby a path in Sc, so that GS is a complete graph.\nFigure 13.2: In this graph, variables in the lower row are conditionally independent given\nthe first row, while their marginal distribution requires a completely connected graph.\n13.3\nThe Hammersley-Clifford theorem\nThe Hammersley-Clifford theorem, which will be proved in this section, gives a com-\nplete description of positive Markov processes relative to a given graph, G. It states\nthat positive G-Markov models are associated to families of positive local interac-\ntions indexed by cliques in the graph. We now introduce each of these concepts.\n13.3.1\nFamilies of local interactions\nDefinition 13.24 Let V be a set of vertexes and (Fs,s \u2208V ) a collection of state spaces.\nA family of local interactions is a collection of non-negative functions \u03a6 = (\u03d5C,C \u2208C)\nindexed over some subset C of P(V ), such that each \u03d5C only depends on configurations\n310\nCHAPTER 13. MARKOV RANDOM FIELDS\nrestricted to C (i.e., it is defined on F (C)), with values in [0,+\u221e). (Recall that P(V ) is\nthe set of all subsets of V .)\nSuch a family has order p if no C \u2208C has cardinality larger than p. A family of local\ninteractions of order 2 is also called a family of pair interactions.\nSuch a family is said to be consistent, if there exists an x \u2208F (V ) such that\nY\nC\u2208C\n\u03d5C(x(C)) , 0.\nTo a consistent family of local interactions, one associates the probability distribution \u03c0\u03a6\non F (V ) defined by\n\u03c0\u03a6(x) = 1\nZ\u03a6\nY\nC\u2208C\n\u03d5C(x(C))\n(13.21)\nfor all x \u2208F (V ), where Z\u03a6 is a normalizing constant.\nGiven C \u2282P(V ), define the graph GC = (V ,EC) by letting {s,t} \u2208EC if and only if\nthere exists C \u2208C such that {s,t} \u2208C. We then have the following proposition.\nProposition 13.25 Let \u03a6 = (\u03d5C,C \u2282C) be a consistent family of local interactions, asso-\nciated to some C \u2282P(V ). Then the associated distribution \u03c0\u03a6 is GC-Markov.\nProof Let X be a random field associated with \u03c0 = \u03c0\u03a6.\nAccording to proposi-\ntion 13.17, we must show that, for any S \u2282V , one has\n(X(S)yX(WS) | X(VS))\nwhere VS is the set of neighbors of S in GC and WS = V \\ (VS \u222aS). Define the set US\nby\nUS =\n[\nC\u2208C,S\u2229C,\u2205\nC\nso that VS = US \\S and WS = V \\US. To prove conditional independence, we need to\nprove that, for any x \u2208F:\n\u03c0(x)\u03c0VS(x(VS)) = \u03c0US(x(US))\u03c0V \\S(x(V \\S))\n(13.22)\n(where we denote \u03c0A the marginal distribution of \u03c0 on F (A).)\nFrom the definition of \u03c0, we have\n\u03c0(x)\n=\n1\nZ\nY\nC\u2208C\n\u03d5C(x(C))\n=\n1\nZ\nY\nC:C\u2229S,\u2205\n\u03d5C(x(C))\nY\nC:C\u2229S=\u2205\n\u03d5C(x(C)).\n13.3. THE HAMMERSLEY-CLIFFORD THEOREM\n311\nThe first term in the last product only depends on x(US), and the second one only on\nx(V \\S). Introduce the notation\n\uf8f1\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f2\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f3\n\u00b51(x(VS)) =\nX\ny(US ):y(VS )=x(VS )\nY\nC:C\u2229S,\u2205\n\u03d5C(x(C))\n\u00b52(x(VS)) =\nX\ny(V \\S):y(VS )=x(VS )\nY\nC:C\u2229S=\u2205\n\u03d5C(x(C)).\nWith this notation, we have:\n\uf8f1\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f2\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f3\n\u03c0US(x(US)) = (\u00b52(x(VS))/Z)\nY\nC:C\u2229S,\u2205\n\u03d5C(x(C))\n\u03c0V \\S(x(V \\S)) = (\u00b51(x(VS))/Z)\nY\nC:C\u2229S=\u2205\n\u03d5C(x(C))\n\u03c0VS(x(VS)) = \u00b51(x(VS))\u00b52(x(VS))/Z\nfrom which (13.22) can be easily obtained.\n\u25a0\nWe now discuss conditional distributions and marginals for processes associated\nwith local interactions. If T \u2282V , we let \u03c0T = \u03c0\u03a6\nT denote the marginal distribution of\n\u03c0 on T.\nWe start with a discussion of conditionals. Let \u03c0 be associated with \u03a6, and let\nS \u2282V and T = V \\S. Assume that a configuration y(T) is given, such that \u03c0T (y(T)) > 0,\nand consider the conditional distribution\n\u03c0S|T (x(S) | y(T)) = \u03c0(x(S) \u2227y(T))/\u03c0T (y(T )).\n(13.23)\nWe have the following proposition.\nProposition 13.26 With the notation above, \u03c0S|T (\u00b7 | y(T )) is associated to the family of\nlocal interactions \u03a6|yT = (\u03d5 \u02dcC|y(T ), \u02dcC \u2208CS) with\nCS =\nn \u02dcC : \u02dcC \u2282S,\u2203C \u2208C : \u02dcC = C \u2229S\no\nand\n\u03d5 \u02dcC|y(T )(x( \u02dcC)) =\nY\nC\u2208C:C\u2229S= \u02dcC\n\u03d5C(x( \u02dcC) \u2227y(C\u2229T)).\nProof From (13.23) and the definition of \u03c0, it is easy to sees that\n\u03c0S|T (x(S) | y(T)) =\n1\nZ(y(T ))\nY\nC:C\u2229S,\u2205\n\u03d5C(x(C\u2229S) \u2227y(C\u2229T)),\nwhere Z(y(T )) is a constant that only depends on y(T ). The fact that \u03c0S|T (\u00b7 | y(T )) is\nassociated to \u03a6|y(T) is then obtained by reorganizing the product over distinct S \u2229\nC\u2019s.\n\u25a0\n312\nCHAPTER 13. MARKOV RANDOM FIELDS\nThis result, combined with proposition 13.25, is consistent with proposition 13.22,\nin the sense that the restriction to GC to S coincides with the graph GCS. The easy\nproof is left to the reader.\nWe now consider marginals, and more specifically marginals when only one node\nis removed, which provides the basis for \u201cnode elimination.\u201d\nProposition 13.27 Let \u03c0 be associated to \u03a6 = (\u03d5C,C \u2208C) as above. Let t \u2208V and\nS = V \\ {t}. Define Ct \u2208P(V ) as the set\nCt = {C \u2208C : t < C} \u222a{ \u02dcCt}\nwith\n\u02dcCt =\n[\nC\u2208C:t\u2208C\nC \\ {t}.\nDefine a family of local interactions \u03a6t = ( \u02dc\u03d5 \u02dcC, \u02dcC \u2208Ct) by \u02dc\u03d5 \u02dcC = \u03d5 \u02dcC if \u02dcC , \u02dcCt and:\n\u2022 If \u02dcCt < C:\n\u02dc\u03d5 \u02dcCt(x( \u02dcCt)) =\nX\ny(t)\u2208Ft\nY\nC\u2208C,t\u2208C\n\u03d5C(x( \u02dcCt) \u2227y(t)).\n\u2022 If \u02dcCt \u2208C:\n\u02dc\u03d5 \u02dcCt(x( \u02dcCt)) = \u03d5Ct(x( \u02dcCt))\nX\ny(t)\u2208Ft\nY\nC\u2208C,t\u2208C\n\u03d5C(x(Ct) \u2227y(t))\nThen the marginal, \u03c0S, of \u03c0 over S is the distribution associated to \u03a6t.\nThe proof is almost straightforward by summing over possible values of yt in the\nexpression of \u03c0 and left to the reader.\n13.3.2\nCharacterization of positive G-Markov processes\nUsing families of local interactions is a typical way to build graphical models in\napplications. The previous section describes a graph with respect to which the ob-\ntained process is Markov. Conversely, given a graph G, the Hammersley-Clifford\ntheorems states that families of local interactions over the cliques of G are the only\nways to build positive graphical models, which reinforces the importance of this\nconstruction. We now pass to the statement and proof of this theorem, starting with\nthe following definition.\nDefinition 13.28 Let G = (V ,E) be an undirected graph. A clique in G is a nonempty\nsubset C \u2282V such that s \u223cG t whenever s,t \u2208C, s , t. (In particular, subsets of cardinality\none are always cliques.) Cliques therefore form complete subgraphs of G.\n13.3. THE HAMMERSLEY-CLIFFORD THEOREM\n313\nThe set of cliques of a graph G will be denoted CG.\nA clique that cannot be strictly included in any other clique is called a maximal clique,\nand their set denoted C\u2217\nG.\n(Note that some authors call cliques what we refer to as maximal cliques.)\nGiven G = (V ,E), consider a random field X = (X(s),s \u2208V ). We assume that X(s)\ntakes values in a finite set Fs with P(X(s) = a) > 0 for any a \u2208Fs (this is no loss of\ngenerality since one can always restrict Fs to such a\u2019s). If S \u2282V , we denote as before\nF (S) the set of restrictions of configurations to S. With this notation, X is positive,\naccording to definition 13.4, if and only if P(X = x) > 0 for all x \u2208F (V ). We will let\n\u03c0 = PX be the probability distribution of X, so that \u03c0(x) = P(X = x) and use as above\nthe notation: for S,T \u2282V\n\uf8f1\uf8f4\uf8f4\uf8f2\uf8f4\uf8f4\uf8f3\n\u03c0S(x(S)) = P(X(S) = x(S))\n\u03c0S|T (x(S) | x(T )) = P(X(S) = x(s) | X(T) = x(T )).\n(13.24)\n(For the first notation, we will simply write \u03c0 if S = V .)\nWe will also need to fix a reference, or \u201czero,\u201d configuration in F (V ) that we will\ndenote 0 = (0(s),s \u2208V ), with 0(s) \u2208Fs for all s. We can choose it arbitrarily. Given this,\nwe have the theorem:\nTheorem 13.29 (Hammersley-Clifford) With the previous notation, X is a positive G-\nMarkov process if and only if its distribution, \u03c0, is associated to a family of local interac-\ntions \u03a6 = (\u03d5C,C \u2282CG) such that \u03d5C(x(C)) > 0 for all x(C) \u2208F (C).\nMoreover, \u03a6 is uniquely characterized by the additional constraint: \u03d5C(x(C)) = 1 as\nsoon as there exists s \u2208C such that x(s) = 0(s).\nLetting \u03bbC = \u2212log\u03d5C, we get an equivalent formulation of the theorem in terms\nof potentials, where a potential is defined as a family of functions\n\u039b = (\u03bbC,C \u2208C)\nindexed by a subset C of P(V ), such that \u03bbC only depends on x(C). The distribution\nassociated to \u039b is\n\u03c0(x) = 1\nZ\u039b\nexp\n\u0012\n\u2212\nX\nC\u2208C\n\u03bbC(x(C))\n\u0013\n.\n(13.25)\nWith this terminology, we trivially have an equivalent formulation:\n314\nCHAPTER 13. MARKOV RANDOM FIELDS\nTheorem 13.30 X is a positive G-Markov process if and only if its distribution, \u03c0, is\nassociated to a potential \u039b = (\u03bbC,C \u2282CG).\nMoreover, \u039b is uniquely characterized by the additional constraint: \u03bbC(x(C)) = 0 as\nsoon as there exists s \u2208C such that x(s) = 0(s).\nWe now prove this theorem.\nProof Let us start with the \u201cif\u201d part. If \u03c0 is associated to a potential over CG, we\nhave already proved that \u03c0 is GCG-Markov, so that it suffices to prove that GCG = G,\nwhich is almost obvious: If s \u223cG t, then {s,t} \u2208CG and s \u223cGCG t by definition of GCG.\nConversely, if s \u223cGCG t, there exists C \u2208CG such that {s,t} \u2282C, which implies that\ns \u223cG t, by definition of a clique.\nWe now prove the \u201conly if\u201d part, which relies on a combinatorial lemma, which\nis one of M\u00a8obius\u2019s inversion formulas.\nLemma 13.31 Let A be a finite set and f : P(A) \u2192R, B 7\u2192fB. Then, there is a unique\nfunction \u03bb : P(A) \u2192R such that\n\u2200B \u2282A, fB =\nX\nC\u2282B\n\u03bbC,\n(13.26)\nand \u03bb is given by\n\u03bbC =\nX\nB\u2282C\n(\u22121)|C|\u2212|B|fB.\n(13.27)\nTo prove the lemma, first notice that the space F of functions f : P(A) \u2192R is a vector\nspace of dimension 2|A| and that the transformation \u03d5 : \u03bb 7\u2192f with fB = P\nC\u2282B \u03bbC\nis linear. It therefore suffices to prove that, given any f , the function \u03bb given in\n(13.27) satisfies \u03d5(\u03bb) = f , since this proves that \u03d5 is onto from F to F and therefore\nnecessarily one to one.\nSo consider f and \u03bb given by (13.27). Then\n\u03d5(\u03bb)(B) =\nX\nC\u2282B\n\u03bbC =\nX\nC\u2282B\nX\n\u02dcB\u2282C\n(\u22121)|C|\u2212| \u02dcB|f \u02dcB =\nX\n\u02dcB\u2282B\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\nX\nC\u2283\u02dcB,C\u2282B\n(\u22121)|C|\u2212| \u02dcB|\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8f \u02dcB = fB\nThe last identity comes from the fact that, for any finite set \u02dcB \u2282B, \u02dcB , B, we have\nX\nC\u2283\u02dcB,C\u2282B\n(\u22121)|C|\u2212| \u02dcB| = 0\n13.3. THE HAMMERSLEY-CLIFFORD THEOREM\n315\n(for \u02dcB = B, the sum is obviously equal to 1). Indeed, if s \u2208B, s < \u02dcB, we have\nX\nC\u2283\u02dcB,C\u2282B\n(\u22121)|C|\u2212| \u02dcB|\n=\nX\nC\u2283\u02dcB,C\u2282B,s\u2208C\n(\u22121)|C|\u2212| \u02dcB| +\nX\nC\u2283\u02dcB,C\u2282B,s<C\n(\u22121)|C|\u2212| \u02dcB|\n=\nX\nC\u2283\u02dcB,C\u2282B,s<C\n((\u22121)|C\u222a{s}|\u2212| \u02dcB| + (\u22121)|C|\u2212| \u02dcB|)\n=\n0.\nSo the lemma is proved. We now proceed to proving the existence and unique-\nness statements in theorem 13.30. Assume that X is G-Markov and positive. Fix\nx \u2208F (V ) and consider the function, defined on P(V ) by\nfB(x(B)) = \u2212log \u03c0(x(B) \u22270(Bc))\n\u03c0(0)\n.\nThen, letting\n\u03bbC(x(C)) =\nX\nB\u2282C\n(\u22121)|C|\u2212|B|fB(x(B)),\nwe have fB(x(B)) = P\nC\u2282B \u03bbC(x(C)). In particular, for B = V , this gives\n\u03c0(x) = 1\nZ exp\n\u0012\n\u2212\nX\nC\u2282V\n\u03bbC(x(C))\n\u0013\nwith Z = P(0). We now prove that \u03bbC(x(C)) = 0 if x(s) = 0(s) for some s \u2208V or if C < CG.\nThis will prove (13.25) and the existence statement in theorem 13.30.\nSo, assume x(s) = 0(s). Then, for any B such that s < B, we have fB(x(B)) = f{s}\u222aB(x({s}\u222aB)).\nNow take C with s \u2208C. We have\n\u03bbC(x(C))\n=\nX\nB\u2282C,s\u2208B\n(\u22121)|C|\u2212|B|fB(x(B)) +\nX\nB\u2282C,s<B\n(\u22121)|C|\u2212|B|fB(x(B))\n=\nX\nB\u2282C,s<B\n(\u22121)|C|\u2212|B\u222a{s}|fB\u222a{s}(x(B\u222a{s})) +\nX\nB\u2282C,s<B\n(\u22121)|C|\u2212|B|fB(x(B))\n=\nX\nB\u2282C,s<B\n((\u22121)|C|\u2212|B\u222a{s}| + (\u22121)|C|\u2212|B|)fB(x(B))\n= 0.\nNow assume that C is not a clique, and let s , t \u2208C such that s \u2241t. We can write,\nusing decompositions similar to the above,\n\u03bbC(x(C)) =\nX\nB\u2282C\\{s,t}\n(\u22121)|C|\u2212|B| \u0010\nfB\u222a{s,t}(x(B\u222a{s,t})) \u2212fB\u222a{s}(x(B\u222a{s})) \u2212fB\u222a{t}(x(B\u222a{t})) + fB(x(B))\n\u0011\n.\n316\nCHAPTER 13. MARKOV RANDOM FIELDS\nBut, for B \u2282C \\ {s,t}, we have\nfB\u222a{s,t}(x(B\u222a{s,t})) \u2212fB\u222a{s}(x(B\u222a{s}))\n=\n\u2212log \u03c0(x(B\u222a{s,t}) \u22270(Bc\\{s,t}))\n\u03c0(x(B\u222a{s}) \u22270(Bc\\{s}))\n=\nlog \u03c0t(x(t) | x(B\u222a{s}) \u22270(Bc\\{s,t}))\n\u03c0t(0(t) | x(B\u222a{s}) \u22270(Bc\\{s,t}))\nand\nfB\u222a{t}(x(B\u222a{t})) \u2212fB(x(B))\n=\n\u2212log \u03c0(x(B\u222a{t}) \u22270(Bc\\{t}))\n\u03c0(x(B) \u22270(Bc))\n=\nlog \u03c0t(x(t) | x(B) \u22270(Bc\\{t}))\n\u03c0t(0(t) | x(B) \u22270(Bc\\{t})).\nSo, we can write\n\u03bbC(x(C)) =\nX\nB\u2282C\\{s,t}\n(\u22121)|C|\u2212|B| log \u03c0t(x(t) | x(B\u222a{s}) \u22270(Bc\\{s,t}))\u03c0t(0(t) | x(B) \u22270(Bc\\{t}))\n\u03c0t(0(t) | xB\u222a{s} \u22270(Bc\\{s,t}))\u03c0t(x(t) | x(B) \u22270(Bc\\{t}))\nwhich vanishes, because\n\u03c0t(x(t) | x(B\u222a{s}) \u22270(Bc\\{s,t})) = \u03c0t(x(t) | x(B) \u22270(Bc\\{t}))\nwhen s \u2241t.\nTo prove uniqueness, note that, for any zero-normalized \u039b satisfying (13.25), we\nmust have \u03c0(0) = 1/Z and therefore, for any x,\n\u2212log \u03c0(x(B) \u22270(Bc))\n\u03c0(0)\n=\nX\nC\u2282B\n\u03bbC(x(C))\n(extending \u039b so that \u03bbC = 0 for C < CG). But, from lemma 13.31, this uniquely\ndefines \u039b.\n\u25a0\nThe exponential form of the distribution in the Hammersley-Clifford theorem is\nrelated to what is called a Gibbs distribution in statistical mechanics. More precisely:\nDefinition 13.32 Let F be a finite set and W : F \u2192R be a scalar function. The Gibbs\ndistribution with energy W at temperature T > 0 is defined by\n\u03c0(x) = 1\nZT\ne\u2212W(x)\nT , x \u2208F\nThe normalizing constant ZT = P\ny\u2208F exp(\u2212W(y)/T) is called the partition function.\nIf \u039b = (\u03bbC,C \u2282V ) is a potential then its associated energy is\nW(x) =\nX\nC\u2282V\n\u03bbC(x(C)).\n13.4. MODELS ON ACYCLIC GRAPHS\n317\nSo the Hammersley-Clifford theorem implies that any positive G-Markov model is\nassociated to a unique zero-normalized potential defined over the cliques of G. This\nrepresentation can also be used to provide an alternate proof of proposition 13.19,\nwhich is left to the reader. Finally, one can restate proposition 13.26 in terms of\npotentials, yielding:\nProposition 13.33 Let P be a Gibbs distribution associated with a zero-normalized po-\ntential \u03bb = (\u03bbC,C \u2282V ). Let S \u2282V and T = Sc. Then the conditional distribution of X(S)\ngiven X(T ) = x(T ) is the Gibbs distribution associated with the zero-normalized potential\n\u02dc\u03bb = ( \u02dc\u03bbC,C \u2282S) where\n\u02dc\u03bbC(y(C)) =\nX\nC\u2032\u2282V ,C\u2032\u2229S=C\n\u03bbC\u2032(y(C) \u2227x(T \u2229C\u2032)).\n13.4\nModels on acyclic graphs\n13.4.1\nFinite Markov chains\nWe now review a few important examples of Markov processes X associated to spe-\ncific graphs G = (V ,E). We will always denote by Fs the space in which X(s) takes his\nvalues, for s \u2208V .\nThe simplest example of G-Markov process (for any graph G) is the case when\nX = (X(s),s \u2208V ) is a collection of independent random variables. In this case, we can\ntake GX = (V ,\u2205), the totally disconnected graph on V . Another simple fact is that, as\nalready remarked, any X is Markov for the complete graph (V ,P2(V )) where P2(V )\ncontains all subsets of V with cardinality 2.\nBeyond these trivial (but nonetheless important) cases, the simplest graph-Markov\nprocesses are those associated with linear graphs, providing finite Markov chains.\nFor this, we let V be a finite ordered set, say,\nV = {0,...,N}.\nWe say that X is a finite Markov chain if, for any k = 1,...,N\n(X(k)y(X(0),...,X(k\u22122)) | X(k\u22121)).\nSo we have the identity\nP(X(0) = x(0),...,X(k) = x(k))P(X(k\u22121) = x(k\u22121))\n= P(X(0) = x(0),...,X(k\u22121) = x(k\u22121))P(X(k\u22121) = x(k\u22121),X(k) = x(k)).\n318\nCHAPTER 13. MARKOV RANDOM FIELDS\nThe distribution of a Markov chain is therefore fully specified by P(X(0) = x(0)),x0 \u2208\nF0 (the initial distribution) and the conditional probabilities\npk(x(k\u22121),x(k)) = P(X(k) = x(k) | X(k\u22121) = x(k\u22121))\n(13.28)\n(with an arbitrary choice when P(X(k\u22121) = x(k\u22121)) = 0). Indeed, assume that P(X(0) =\nx(0),...,X(k\u22121) = x(k\u22121)) is known (for all x(0),...,x(k\u22121)). Then, either:\n(i) P(X(0) = x(0),...,X(k\u22121) = x(k\u22121)) = 0, in which case\nP(X(0) = x(0),...,X(k) = x(k)) = 0\nfor any x(k), or:\n(ii) P(X(0) = x(0),...,X(k\u22121) = x(k\u22121)) > 0, in which case, necessarily, P(X(k\u22121) = x(k\u22121)) >\n0, and\nP(X(0) = x(0),...,X(k) = x(k)) = pk(x(k\u22121),x(k))P(X(0) = x(0),...,X(k\u22121) = x(k\u22121)).\nNote that pk in (13.28) is a transition probability (according to definition 12.2) be-\ntween Fk\u22121 and Fk.\nWe have the following identification of a finite Markov chain with a graph-Markov\nprocess:\nProposition 13.34 Let X = (X(0),...,X(N)) be a finite Markov chain, such that X is posi-\ntive. Then X is G-Markov for the linear graph G = (V ,E) with\nV\n=\n{1,...,N}\nE\n=\n{{1,2},...,{N \u22121,N}}.\nThe converse is true without the positivity assumption: a G-Markov process for the graph\nabove is always a finite Markov chain.\nProof We prove the direct statement (the converse one being obvious). Let s and t\nbe nonconsecutive distinct integers, with, say, s < t. From the Markov chain assump-\ntion, we have\n(X(t)y(X(s),X({1,t\u22122}\\{s,})) | X(t\u22121)),\nwhich, using (CI3), yields (X(t)yX(s) | X({1,...,t\u22121}\\{s})). Define Y (u) = X({1,...,u}\\{s,t}): what\nwe have proved is (X(t)yX(s) | Y (t)).\nWe now proceed by induction and assume that (X(t)yX(s) | Y (u)) for some u \u2265t.\nThen, we have (X(u+1)y(X(s),X(t),Y (u\u22121)) | X(u)), which implies (from (CI3)) (X(u+1)yX(t) |\nX(s),Y (u)). Applying (CI4) to (X(t)yX(s) | Y (u)) and (X(t)yX(u+1) | X(s),Y (u)), we obtain\n(X(t)y(X(s),X(u+1)) | Y (u)) and finally, (X(t)yX(s) | Y (u+1)). By induction, this gives\n(X(t)yX(s) | Y (N)) and therefore proposition 13.19 now implies that X is G-Markov.\n(The proposition can also be proved as a consequence of the decomposition\nP(X(0) = x(0),...,X(N) = x(N)) = P(X(0) = x(0))p1(x(0),x(1))...pN(x(N\u22121),x(N)).)\n\u25a0\n13.4. MODELS ON ACYCLIC GRAPHS\n319\n13.4.2\nUndirected acyclic graph models and trees\nThe situation with acyclic graphs is only slightly more complex than with linear\ngraphs, but will require a few new definitions, including those of directed graphs\nand trees.\nThe difference between directed and undirected graphs is that the edges of the\nformer are ordered pairs, namely:\nDefinition 13.35 A (finite) directed graph G is a pair G = (V ,E) where V is a finite set\nof vertexes and E is a subset of\nV \u00d7 V \\ {(s,s),s \u2208V },\nwhich satisfies, in addition,\n(s,t) \u2208E \u21d2(t,s) < E.\nSo, for directed graphs, edges (s,t) and (t,s) have different meanings, and we allow\nat most one of them in E. We say that the edge e = (s,t) stems from s and points to t.\nThe parents of a vertex s are the vertexes t such that (t,s) \u2208E, and its children are the\nvertexes t such that (s,t) \u2208E. We will also use the notation s \u2192G t to indicate that\n(s,t) \u2208E (compare to s \u223cG t for undirected graphs).\nDefinition 13.36 A path in a directed graph G = (V ,E) is a sequence (s0,...,sN) such\nthat, for all k = 1,...,N, sk \u2192G sk+1 (this includes the \u201ctrivial\u201d, one-vertex, paths (s0)).\n(The definition was the same for undirected graph, replacing sk \u2192G sk+1 by sk \u223cG sk+1.)\nFor both directed and undirected cases, one says that a path is closed if s0 = sN.\nIn an undirected graph, a path is folded if it can be written as (s0,...,sN\u22121,sN,sN\u22121,...,s0).\nIf G = (V ,E) is directed, one says that t \u2208V is a descendant of s \u2208V (or that s is an\nancestor of t) if there exists a path starting at s and ending at t. In particular, every vertex\nis both a descendant and an ancestor of itself.\nWe finally define acyclic graphs.\nDefinition 13.37 A loop in a directed (resp. an undirected) graph G is a path (s0,s1,...,sN),\nwith N \u22653, such that sN = s0, which passes only once through s0,...,sN\u22121 (no self-\nintersection except at the end).\nA (directed or undirected) graph G is acyclic if it contains no loop.\nThe following property will be useful.\n320\nCHAPTER 13. MARKOV RANDOM FIELDS\nProposition 13.38 In a directed graph, any non-trivial closed path contains a loop (i.e.,\none can delete vertexes from it to finally obtain a loop.)\nIn an undirected graph, any non-trivial closed path which is not a union of folded\npaths contains a loop.\nProof Take \u03b3 = (s0,s1,...,sN) with sN = s0. The path being non-trivial means N > 1.\nFirst take the case of a directed graph. Clearly, N \u22653 since a two-vertex path\ncannot be closed in an directed graph. Consider the first occurrence of a repetition,\ni.e., the first index for which\nsj \u2208{s0,...,sj\u22121}.\nThen there is a unique j\u2032 \u2208{0,...,j \u22121} such that sj\u2032 = sj, and the path (sj\u2032,...,sj\u22121)\nmust be a loop (any repetition in the sequence would contradict the fact that j was\nthe first occurrence. This proves the result in the directed case.\nConsider now an undirected graph. We can recursively remove all folded sub-\npaths, by keeping everything but their initial point, since each such operation still\nprovide a path at the end. Assume that this is done, still denoting the remaining\npath (s0,s1,...,sN), which therefore has no folded subpath. We must have N \u22653\nsince N = 1 implies that the original path was a union of folded paths, and N = 2\nprovides a folded path. Let, 0 \u2264j\u2032 < j be as in the directed case. Note that one must\nhave j\u2032 < j \u22122, since j\u2032 = j \u22121 would imply an edge between j and itself and j\u2032 = j \u22122\ninduces a folded subpath. But this implies that (sj\u2032,...,sj\u22121) is a loop.\n\u25a0\nDirected acyclic graphs (DAG) will be important for us, because they are associ-\nated with Bayesian networks that we will discuss later. For now, we are interested\nwith undirected acyclic graphs and their relation to trees, which form a subclass of\ndirected acyclic graphs, defined as follows.\nDefinition 13.39 A forest is a directed acyclic graph with the additional requirement\nthat each of its vertexes has at most one parent.\nA root in a forest is a vertex that has no parent. A forest with a single root is called a\ntree.\nIt is clear that a forest has at least one root, since one could otherwise describe\na nontrivial loop by starting from a any vertex and passing to its parent until the\nsequence self-intersects (which must happen since V is finite). We will use the fol-\nlowing definition.\nDefinition 13.40 If G = (V ,E) is a directed graph, its flattened graph, denoted G\u266d=\n(V ,E\u266d) is the undirected graph obtained by forgetting the edge ordering, namely\n{s,t} \u2208E\u266d\u21d4(s,t) \u2208E or (t,s) \u2208E.\n13.4. MODELS ON ACYCLIC GRAPHS\n321\nThe following proposition relates forests and undirected acyclic graphs.\nProposition 13.41 If G is a forest, then G\u266dis an undirected acyclic graph.\nConversely, if G is an undirected acyclic graph, there exists a forest \u02dcG such that \u02dcG\u266d= G.\nProof Let G = (V ,E) be a forest and, in order to reach a contradiction, assume that\nG\u266dhas a loop, s0,...,sN\u22121,sN = s0. Assume that (s0,s1) \u2208E; then, also (s1,s2) \u2208E\n(otherwise s1 would have two parents), and this propagates to all (sk,sk+1) for k =\n0,...,N \u22121. But, since sN = s0, this provides a loop in G which is not possible. This\nproves thet G\u266dhas no loop since the case (s1,s0) \u2208E is treated similarly.\nNow, let G be an undirected acyclic graph. Fix a vertex s \u2208V and consider the\nfollowing procedure, in which we recursively define sets Sk of processed vertexes,\nand \u02dcEk of oriented edges, k \u22650, initialized with S0 = {s} and \u02dcE0 = \u2205.\n\u2013 At step k of the procedure, assume that vertexes in Sk have been processed and\nedges in \u02dcEk have been oriented so that (Sk, \u02dcEk) is a forest, and that \u02dcE\u266d\nk is the set of edges\n{s,t} \u2208E such that s,t \u2208Sk (so, oriented edges at step k can only involve processed\nvertexes).\n\u2013 If Sk = V : stop, the proposition is proved.\n\u2013 Otherwise, apply the following construction. Let Fk be the set of edges in E that\ncontain exactly one element of Sk.\n(1) If Fk = \u2205, take any s \u2208V \\ Sk as a new root and let Sk+1 = Sk \u222a{s}, \u02dcEk+1 = \u02dcEk.\n(2) Otherwise, add to \u02dcEk the oriented edges (s,t) such that s \u2208Sk and {s,t} \u2208Fk,\nyielding \u02dcEk+1, and add to Sk the corresponding children (t\u2019s) yielding Sk+1.\nWe need to justify the fact that \u02dcGk+1 = (Sk+1, \u02dcEk+1) above is still a forest. This is\nobvious after Case (1), so consider Case (2). First \u02dcGk+1 is acyclic, since any oriented\nloop is a fortiori an unoriented loop and G is acyclic. So we need to prove that no\nvertex in Sk+1 has two parents. Since we did not add any parent to the vertexes in Sk\nand, by assumption, (Sk, \u02dcEk) is a forest, the only possibility for a vertex to have two\nparents in Sk+1 is the existence of t such that there exists s,s\u2032 \u2208Sk with {s,t} and {s\u2032,t}\nin E. But, since s and s\u2032 have unaccounted edges containing them, they cannot have\nbeen introduced in Sk before the previously introduced root has been added, so they\nare both connected to this root: but the two connections to t would create a loop in\nG which is impossible.\nSo the procedure carries on, and must end with Sk = V at some point since we\nkeep adding points to Sk at each step.\n\u25a0\nNote that the previous proof shows there is more than one possible orientation\nof a connected undirected tree into a tree is not unique, although uniquely specified\n322\nCHAPTER 13. MARKOV RANDOM FIELDS\nonce a root is chosen. The proof is constructive, and provides an algorithm building\na forest from an undirected acyclic graph.\nWe now define graphical models supported by trees, which constitute our first\nMarkov models associated with directed graphs. Define the depth of a vertex in a\ntree G = (V ,E) to be the number of edges in the unique path that links it to the\nroot. We will denote by Gd the set of vertexes in G that are at depth d, so that G0\ncontains only the root, G1 the children of the root and so on. Using this, we have the\ndefinition:\nDefinition 13.42 Let G = (V ,E) be a tree. A process X = (X(s),s \u2208V ) is G-Markov if\nand only, for each d \u22651, and for each s \u2208Gd, we have\n(X(s)y(X(Gd\\{s}),X(Gq\\{pa(s)}),q < d) | X(pa(s)))\n(13.29)\nwhere pa(s) is the parent of s.\nSo, conditional to its parent, X(s) is independent from all other variables at depth\nsmaller or equal to the depth of s.\nNote that, from (CI3), definition 13.42 implies that, for all s \u2208Gd,\n(X(s)yX(Gd\\{s}) | X(Gq),q < d),\nwhich, using proposition 13.6, implies that the variables (X(s),s \u2208Gd) are mutually\nindependent given X(Gq),q < d. This implies that, for d = 1 (letting s0 denote the root\nin G):\nP(X(G1) = x(G1),X(s0) = x(s0)) = P(X(s0) = x(s0))\nY\ns\u2208G1\nP(X(s) = x(s) | X(s0) = x(s0)).\n(If P(X(s0) = x(s0)) = 0, the choice for the conditional probabilities can be made ar-\nbitrarily without changing the left-hand side which vanishes.) More generally, we\nhave, letting G<d = G0 \u222a\u00b7\u00b7\u00b7 \u222aGd\u22121,\nP(X(G\u2264d) = x(G\u2264d)) =\nY\ns\u2208Gd\nP(X(s) = x(s) | X(pa(s)) = x(pa(s)))P(X(G<d) = x(G<d))\n(with again an arbitrary choice for conditional probabilities that are not defined) so\nthat, we obtain, by induction, for x \u2208F (V )\nP(X = x) = P(X(s0) = x(s0))\nY\ns,s0\nps(x(pa(s)),x(s))\n(13.30)\nwhere ps(x(pa(s)),x(s))\n\u2206= P(X(s) = x(s) | X(pa(s)) = x(pa(s))) are the tree transition probabil-\nities between a parent and a child. So we have the following proposition.\n13.4. MODELS ON ACYCLIC GRAPHS\n323\nProposition 13.43 A process X is Markov relative to a tree G = (V ,E) if and only if there\nexists a probability distribution p0 on Fs0 and a family (pst,(s,t) \u2208E) such that pst is a\ntransition probability from Fs to Ft and\nPX(x) = p0(xs0)\nY\n(s,t)\u2208E\npst(x(s),x(t)), x \u2208F (V ).\n(13.31)\nWe only have proved the \u201conly if\u201d part, but the \u201cif\u201d part is obvious from (13.31).\nAnother property that becomes obvious with this expression is the first part of the\nfollowing proposition.\nProposition 13.44 If a process X is Markov relative to a tree G = (V ,E) then it is G\u266d\nMarkov. Conversely, if G = (V ,E) is an undirected acyclic graph and X is G-Markov, then\nX is Markov relative to any tree \u02dcG such that \u02dcG\u266d= G.\nProof To prove the converse part, assume that G = (V ,E) is undirected acyclic and\nthat X is G-Markov. Take \u02dcG such that \u02dcG\u266d= G. For s \u2208V and its parent pa(s) in \u02dcG, the\nsets {s} and \u02dcG\u2264d \\ {s,pa(s)} are separated by pa(s) in G. To see this, assume that there\nexists a t \u2208\u02dcG\u2264d \\ {s,pa(s)} with a path from t to s that does not pass through pa(s).\nThen we can complete this path with the path from t to the first common ancestor\n(in \u02dcG) of t and s and back to s to create a path from s to s that passes only once\nthrough {pa(s),s} and therefore contains a loop by proposition 13.38.\nThe G-Markov property now implies\n(X(s)y(X( \u02dcGd\\{s}),X( \u02dcGq\\{pa(s)}),q < d) | X(pa(s)))\nwhich proves that X is \u02dcG-Markov.\n\u25a0\nRemark 13.45 We see that there is no real gain in generality with passing from undi-\nrected to directed graphs when working with trees. This is an important remark,\nbecause directionality in graphs is often interpreted as causality. For example, there\nis a natural causal order in the statements\n(it rains) \u2192(car windshields get wet) \u2192(car wipers are on)\nin the sense that each event can be seen as a logical precursor to the next one. How-\never, because one can pass from this directed chain to an equivalent undirected chain\nand then back to a equivalent directed tree by choosing any of the three variables as\nroots, there is no way to infer, from the observation of the joint distribution of the\nthree events (it rains, car windshields get wet, wipers are on), any causal relationship\nbetween them: the joint distribution cannot resolve whether wipers are on because\n324\nCHAPTER 13. MARKOV RANDOM FIELDS\nit rains, or whether turning wipers on automatically wets windshields which in turn\ntriggers a shower !\nTo infer causal relationships, one needs a different kind of observation, that\nwould modify the distribution of the system. Such an operation (called an inter-\nvention), can be done, for example, by preventing the windshields from being wet\n(doing, for example, the observation in a parking garage), or forcing them to be wet\n(using a hose). Then, one can compare observations made with these new condi-\ntions, and those made with the original system, and check, for example, whether\nthey modified the probability that rain occurs outside. The answer (likely to be\nnegative !) would refute any causal relationship from \u201cwindshields are wet\u201d to \u201cit\nrains.\u201d On the other hand, the intervention might modify how wipers are used,\nwhich would indicate a possible causal relationship from \u201cwindshields are wet\u201d to\n\u201cwipers are on.\u201d\n\u2666\n13.5\nExamples of general \u201cloopy\u201d Markov random fields\nWe will see that acyclic models have very nice computational properties that make\nthem attractive in designing distributions. However, the absence of loops is a very\nrestrictive constraint, which is not realistic in many practical situations. Feedback\neffects are often needed, for example. Most models in statistical physics are sup-\nported by a lattice, in which natural translation/rotation invariance relations forbid\nusing any non-trivial acyclic model. As an example, we now consider the 2D Ising\nmodel on a finite grid, which is a model for (anti)-ferromagnetic interaction in a spin\nsystem.\nLet G = (V ,E). A (positive) G-Markov model is said to have only pair interactions\nif and only if can be written in the form\n\u03c0(x) = 1\nZ exp\n\u0012\n\u2212\nX\ns\u2208G\nhs(x(s)) \u2212\nX\n{s,t}\u2208E\nh{s,t}(x(s,t))\n\u0013\n.\nRelating to theorem 13.30, this says that \u03c0 is associated to a potential involving\ncliques of order 2 at most (note that this does not mean that the cliques of the asso-\nciated graph have order 2 at most; there can be higher-order cliques, which would\nthen have a zero potential). The functions in the potential are indexed by sets, as\nthey should be from the general definition. However, models with pair interactions\nare often written in the form\n\u03c0(x) = 1\nZ exp\n\u0012\n\u2212\nX\ns\u2208G\nhs(x(s)) \u2212\nX\n{s,t}\u2208E\n\u02dchst(x(s),x(t))\n\u0013\nwith \u02dchst(\u03bb,\u00b5) = \u02dchts(\u00b5,\u03bb) (which is equivalent, taking \u02dch = h/2).\n13.5. EXAMPLES OF GENERAL \u201cLOOPY\u201d MARKOV RANDOM FIELDS\n325\nThe Ising model is a special case of models with pair interactions, for which the\nstate space, Fs, is equal to {\u22121,1} for all s and\nhs(x(s)) = \u03b1sx(s), h{s,t}(x(s),x(t)) = \u03b2stx(s)x(t).\nIn fact, for binary variables, this is the most general pair interaction model.\nFigure 13.3: Graph forming a two-dimensional regular grid.\nThe Ising model is moreover usually defined on a regular lattice, which, in two\ndimensions, implies that V is a finite rectangle in Z2, for example V = {\u2212N,...,N}2.\nThe simplest choice of a translation- and 90-degree rotation-invariant graph is the\nnearest-neighbor graph for which {(i,j),(i\u2032,j\u2032)} \u2208E if and only if |i \u2212i\u2032| + |j \u2212j\u2032| = 1\n(see fig. 13.3). With this graph, one can furthermore simplify the model to obtain\nthe isotropic Ising model given by\n\u03c0(x) = 1\nZ exp\n\u0012\n\u2212\u03b1\nX\ns\u2208V\nx(s) \u2212\u03b2\nX\ns\u223ct\nx(s)x(t)\u0013\n.\nWhen \u03b2 < 0, the model is ferromagnetic: each pair of neighbors with identical signs\nbrings a negative contribution to the energy, making the configuration more likely\n(since lower energy implies higher probability).\n326\nCHAPTER 13. MARKOV RANDOM FIELDS\nThe Potts model generalizes the Ising model to finite, but non-necessarily binary,\nstate spaces, say, Fs = F = {1,...,n}. Define the function \u03b4(\u03bb,\u00b5) = 1 if \u03bb = \u00b5 and (\u22121)\notherwise. Then the Potts model is given by\n\u03c0(x) = 1\nZ exp\n\u0012\n\u2212\u03b1\nX\ns\u2208V\nh(x(s)) \u2212\u03b2\nX\ns\u223ct\n\u03b4(x(s),x(t))\n\u0013\n(13.32)\nfor some function h defined on F.\n13.6\nGeneral state spaces\nOur discussion of Markov random fields on graphs was done under the assumption\nof finite state spaces, which notably simplifies many of the arguments and avoids\nrelying too much on measure theory. While this situation does cover a large range of\napplication, there are cases in which one wants to consider variables taking values\nin continuous spaces, or in countable (infinite) spaces.\nThe results obtained for discrete variables can most of the time be extended to\nvariables whose distribution has a p.d.f. with respect to a product of measures on\nthe sets in which they take their values. For example, let X,Y,Z takes values in\nRX,RY,RZ, equipped with \u03c3-algebras SX, SY, SZ and measures \u00b5X, \u00b5Y, \u00b5Z. As-\nsume that PX,Y,Z is absolutely continuous with respect to \u00b5X \u2297\u00b5Y \u2297\u00b5Z, with density\n\u03d5XYZ. In such a situation, (13.3) remains valid, in that X is conditionally indepen-\ndent of Y given Z if and only if\n\u03d5XYZ(x,y,z)\u03d5Z(z) = \u03d5XZ(x,z)\u03d5YZ(y,z)\n(13.33)\nalmost everywhere (relative to \u00b5X \u2297\u00b5Y \u2297\u00b5Z). Here, \u03d5XZ,\u03d5YZ,\u03d5Z are marginal densi-\nties of the indexed random variables. The only difficulty in the argument, provided\nbelow for the interested reader, is dealing properly with sets of measure zero.\nProof (Proof of (13.33)) Introduce the conditional densities\n\u03d5XY|Z(x,y | z) = \u03d5XYZ(x,y,z)\n\u03d5Z(z)\nand similarly \u03d5X|Z and \u03d5Y|Z, which are defined when z < MZ = {z \u2208RZ : \u03d5Z(z) = 0}.\nBy definition of conditional independence, we have, for all A \u2208SX, B \u2208SX\nZ\nA\u00d7B\n\u03d5XY|Z(x,y | z)\u00b5X(dx)\u00b5Y(dy) =\nZ\nA\u00d7B\n\u03d5X|Z(x | z)\u03d5Y|Z(y | z)\u00b5X(dx)\u00b5Y(dy)\nfor all z < MZ, which implies that, for all z < MZ, there exists a set Nz \u2282RX \u00d7RY such\nthat \u00b5X \u00d7 \u00b5Y(Nz) = 0 and\n\u03d5XY|Z(x,y | z) = \u03d5X|Z(x | z)\u03d5Y|Z(y | z)\n13.6. GENERAL STATE SPACES\n327\nfor all z < MZ and (x,y) < Nz. This immediately implies (13.33) for those (x,y,z).\nIf z \u2208MZ, then\n0 = \u03d5Z(z) =\nZ\nRX\n\u03d5XZ(x,z)\u00b5X(dx) =\nZ\nRY\n\u03d5YZ(x,z)\u00b5Y(dy)\nimplying that \u03d5XZ(x,z) = \u03d5YZ(y,z) = 0 excepted on some set Nz such that \u00b5X \u2297\n\u00b5Y(Nz) = 0, and (13.33) is therefore also true outside of this set. Now, letting N =\n{(x,y,z) : (x,y) \u2208Nz}, we find that (13.33) is true for all (x,y,z) < N and\n\u00b5X\u2297\u00b5Y\u2297\u00b5Z(N) =\nZ\nRX\u00d7RY \u00d7RZ\n1(x,y)\u2208Nz\u00b5X(dx)\u00b5Y(dy)\u00b5Z(dz) =\nZ\nRZ\n\u00b5X\u2297\u00b5Y(Nz)\u00b5Z(dz) = 0.\n(This argument involves Fubini\u2019s theorem [171].)\n\u25a0\nWith this definition, the proof of proposition 13.5 can be caried on without change,\nwith the positivity condition expressing the fact that there exists \u02dcRX \u2282RX, \u02dcRY \u2282RY\nand \u02dcRZ \u2282RX such that \u03d5XYZ(x,y,z) > 0 for all x,y,z \u2208\u02dcRX \u00d7 \u02dcRY \u00d7 \u02dcRZ. (This proposition\nis actually valid in full generality, with a proper definition of positivity.)\nWhen considering random fields with general state spaces, we will restrict to\nthe similar situation in which each state space Fs is equipped with a \u03c3-algebra Ss\nand a measure \u00b5s, and the joint distribution, PX of the random field X = (Xs,s \u2208V ) is\nabsolutely continuous with respect to \u00b5\n\u2206=\nN\ns\u2208V \u00b5s, denoting by \u03c0 the corresponding\np.d.f. We will says that \u03c0 is positive if there exists \u02dcF = ( \u02dcFs,s \u2208V ) with measurable\n\u02dcFs \u2282Fs such that \u03c0(x) > 0 for all x \u2208F (V , \u02dcF). Without loss of generality unless one\nconsiders multiple random fields with different supports, we will assume that \u02dcFs = Fs\nfor all s.\nThe definition of consistent families of local interactions (definition 13.24) must\nbe modified by adding the condition that\nZ\nF (V )\nY\nC\u2208C\n\u03d5C(x(C))\u00b5(dx) < \u221e.\n(13.34)\nThis requirement is obviously needed to ensure that the normalizing constant in\n(13.21) is finite. Proposition 13.25 is then true (with sums replaced by integrals in\nthe proof) and so are propositions 13.26 and 13.27. Finally, the Hammersley-Clifford\ntheorem (theorem 13.29) extends to this context.\nEven though it is a natural requirement, condition (13.34) may be hard to assess\nwith general families of local interactions. In the case of Gaussian distributions,\nhowever, one can provide relatively simple conditions. Assume that Fs = R for all\n328\nCHAPTER 13. MARKOV RANDOM FIELDS\ns \u2208V , and condider a potential \u039b = (\u03bbC,C \u2208C) with only univariate and bivariate\ninteractions, such that, for some vector a \u2208Rd (with d = |V |) and symmetric matrix\nb \u2208Sd,\n\uf8f1\uf8f4\uf8f4\uf8f4\uf8f2\uf8f4\uf8f4\uf8f4\uf8f3\n\u03bb{s}(x({s})) = \u2212a(s)x(s) + 1\n2bss(x(s))2\n\u03bb{s,t}(x({s,t})) = bstx(s)x(t)\nThen, considering x \u2208F (V ) as a d-dimensional vector, we have\n\u03c0(x) = 1\nZ exp\n\u0012\naT x \u22121\n2xT bx\n\u0013\n,\nwith the integrability requirement that b \u227b0 (positive definite). The random field\nthen follows a Gaussian distribution with mean m = b\u22121a and covariance matrix\n\u03a3 = b\u22121. The normalizing constant, Z, is given by\nZ = e\u22121\n2aT ba(2\u03c0)d/2\n\u221a\ndetb\n.\nThis Markov random field parametrization of Gaussian distributions emphasizes\nthe conditional structure of the variables rather than their covariances. It is useful\nwhen the associated graph, represented by the matrix b is sparse. In particular,\nthe conditional distribution of X(s) given the other variables is Gaussian, with mean\n(a(s) \u2212P\nt,s bstx(t))/bss and variance 1/bss.\nChapter 14\nProbabilistic Inference for Random Fields\nOnce the joint distribution of a family of variables has been modeled as a random\nfield, this model can be used to estimate the probabilities of specific events, or the\nexpectations of random variables of interest. For example, if the modeled variables\nrelate to a medical condition, in which variables such as diagnosis, age, gender, clin-\nical evidence can interact, one may want to compute, say, the probability of someone\nhaving a disease given other observable factors. Note that, being able to compute\nexpectations of the modeled variables for G-Markov processes also ensures that one\ncan compute conditional expectations of some modeled variables given others, since,\nby proposition 13.22, conditional G-Markov distributions are Markov over restricted\ngraphs.\nWe assume that X is G-Markov for a graph G = (V ,E) and restrict (unless spec-\nified otherwise) to finite state spaces. We condider the basic problem to compute\nP(X(S) = x(S)) when S \u2282V , starting with one-vertex marginals, P(X(s) = x(s)).\nThe Hammersley-Clifford theorem provides a generic form for general positive\nG-Markov processes, in the form\nP(X = x) = \u03c0(x) = 1\nZ exp\n\u0012\n\u2212\nX\nC\u2208CG\nhC(x(C))\n\u0013\n.\n(14.1)\nSo, formally, marginal distributions are given by the ratio\nP(X(S) = x(S)) =\nP\ny\u2208F (V ),y(S)=x(S) exp\n\u0012\n\u2212P\nC\u2208CG hC(y(C))\n\u0013\nP\ny\u2208F (V ) exp\n\u0012\n\u2212P\nC\u2208CG hC(y(C))\n\u0013\n.\nThe problem is that the sums involved in this ratio involve a number of terms that\ngrows exponentially with the size of V . Unless V is very small, a direct computation\nof these sums is intractable. An exception to this is the case of acyclic graphs, as\n329\n330\nCHAPTER 14. PROBABILISTIC INFERENCE FOR MRF\nwe will see in section 14.2. But for general, loopy, graphs, the sums can only be\napproximated, using, for example, Monte-Carlo sampling, as described in the next\nsection.\n14.1\nMonte Carlo sampling\nMarkov chain Monte Carlo methods are well adapted to sampling from Markov ran-\ndom fields, because conditional distributions used in Gibbs sampling, or, more gen-\nerally, ratios of probabilities used in the Metropolis-Hastings algorithm do not in\nrequire the computation of the normalizing constant Z in (14.1). The simplest use\nof Gibbs sampling generalizes the Ising model example of section 12.4.2. Using the\nnotation of Algorithm 12.2, one lets B\u2032\ns = F (sc) (with the notation sc = V \\ {s}) and\nUs(x) = x(sc). The conditional distribution given Us is\nQs(Us(x),y) = P(X(s) = y(s) | X(sc) = x(sc))1y(sc)=x(sc).\nThe conditional probability in the r.h.s. of this equation takes the form\n\u03c0s(y(s) | x(sc))\n\u2206= P(X(s) = y(s) | X(sc) = x(sc)) =\n1\nZs(x(sc)) exp\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\u2212\nX\nC\u2208C,s\u2208\u2208C\nhC(y(s) \u2227x(C\u2229sc))\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\nwith\nZs(x(sc)) =\nX\nz(s)\u2208Fs\nexp\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\u2212\nX\nC\u2208C,s\u2208\u2208C\nhC(z(s) \u2227x(C\u2229sc))\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8.\nThe Gibbs sampling algorithm samples from Qs by visiting all s \u2208V infinitely of-\nten, as described in Algorithm 12.2. Metropolis-Hastings schemes are implemented\nsimilarly, the most common choice using a local update scheme in Algorithm 12.3\nsuch that g(x,\u00b7) only changes one coordinate, chosen at random, so that\ng(x,y) = 1\n|V |\nX\ns\u2208V\n1y(sc)=x(sc)gs(y(s))\nwhere gs is some probability distribution on Fs. The acceptance probability a(x,y) is\nequal to 1 when y = x. If y , x and g(x,y) > 0, there is a unique s for which y(sc) = x(sc)\nand\na(x,y) = min\n \n1, \u03c0(y)g(y,x)\n\u03c0(x)g(x,y)\n!\nwith\n\u03c0(y)g(y,x)\n\u03c0(x)g(x,y) = \u03c0s(y(s) | x(sc))gs(x(s))\n\u03c0s(x(s) | x(sc))gs(y(s)).\n14.1. MONTE CARLO SAMPLING\n331\nNote that the latter equation avoids the computation of the local normalizing con-\nstant Zs(x(sc)), which simplifies in the ratio.\nBoth algorithms have a transition probability P that satisfies Pm(x,y) > 0 for all\nx,y \u2208F (V ), with m = |V | (for Metropolis-Hastings, one must assume that gs(y(s)) > 0\nfor all y(s) \u2208Fs. This ensures that the chain is uniformly geometrically ergodic, i.e.,\n(12.10) is satisfied with a constant M and some \u03c1 < 1. However, in many practical\ncases (especially for strongly structured distributions and large sets V ), the conver-\ngence rate, \u03c1 can be very close to 1, resulting in a slow convergence.\nAcceleration strategies have been designed to address this issue, which is often\ndue to the existence of multiple configurations that are local modes of the probabil-\nity \u03c0. Such configurations are isolated from other high-probability configurations\nbecause local updating schemes need to make multiple low-probability changes to\naccess them from the local mode. The following two approaches provide examples\ndesigned to address this issue.\na. Cluster sampling. To facilitate escaping from such local modes, it is sometimes\npossible to augment the state space by introducing a new configuration space, with\nvariable denoted \u03be, and designing a joint distributions \u02c6\u03c0(\u03be,x) such that the marginal\ndistribution on F (V ) (summing over \u03be) is the targeted \u03c0. The additional variable\ncan create high-probability bridges between local modes for \u03c0, and accelerate con-\nvergence.\nTo take an example, assume that all sets Fs are identical (letting F = Fs, s \u2208V ) and\nthat the auxiliary variable \u03be takes values in the set of functions from E to {0,1}, that\nwe will denote B(E), i.e., that it takes the form (\u03be(st),{s,t} \u2208E), with \u03be(st) \u2208{0,1}. For\nx \u2208F (V ), introduce the set Bx containing all \u03be \u2208B(E), such that for all {s,t} \u2208E,\nx(s) , x(t) \u21d2\u03be(st) = 1.\nAssume that the conditional distribution of \u03be given x is supported by Bx, such that,\nfor \u03be \u2208Bx\nP(\u03be = \u03be | X = x) = \u02c6\u03c0(\u03be | x) =\n1\n\u03b6(x) exp\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\u2212\nX\n{s,t}\u2208E\n\u00b5st\u03be(st)\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8.\nThe coefficients \u00b5st are free to choose (and one possible choice is to take \u00b5st = 0 for\nall {s,t} \u2208E). For this distribution, all \u03be(st) are independent conditionally to X = x,\nwith \u03be(st) = 1 with probability 1 if x(s) , x(t), and\nP(\u03be(st) = 1 | X = x) =\ne\u2212\u00b5st\n1 + e\u2212\u00b5st\n(14.2)\nif x(s) = x(t). This conditional distribution is, as a consequence, very easy to sample\n332\nCHAPTER 14. PROBABILISTIC INFERENCE FOR MRF\nfrom. Moreover, the normalizing constant \u03b6(x) has closed form and is given by\n\u03b6(x) =\nY\n{s,t}\u2208E\n(1x(s)=x(t) + e\u2212\u00b5st) = exp\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\nX\n{s,t}\u2208E\nlog(1 + e\u2212\u00b5st) +\nX\n{s,t}\u2208E\nlog(1 + e\u00b5st)1x(s),x(t)\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8.\nNow consider the conditional probability that X = x given \u03be = \u03be. For this distribu-\ntion, one has, with probability 1, X(s) = X(t) when \u03be(st) = 0. This implies that X is con-\nstant on the connected components of the subgraph (V ,E\u03be) of (V ,E), where {s,t} \u2208E\u03be\nif and only if \u03be(st) = 0. Let V1,...,Vm denote these connected components (these com-\nponents and their number depend on \u03be). The conditional distribution of X given \u03be\nis therefore supported by the configurations such that there exists c1,...,cm \u2208F such\nthat x(s) = cj if and only if s \u2208Vj, that we will denote, with some abuse of notation:\nc(V1)\n1\n\u2227\u00b7\u00b7\u00b7 \u2227c(Vm)\nm\n.\nGiven this remark, the conditional distribution of X given \u03be = \u03be is equivalent to a\ndistribution on Fm, which may be feasible to sample from directly if |F| and m are not\ntoo large. To sample from \u03c0, one now needs to alternate between sampling \u03be given\nX and the converse, yielding the following first version of cluster-based sampling.\nAlgorithm 14.1 (Cluster-based sampling: Version 1)\nThis algorithm samples from (14.1).\n(1) Initialize the algorithm some configuration x \u2208F (V ).\n(2) Loop over the following steps:\na.\nGenerate a configuration \u03be \u2208Bx such that \u03be(st) = 1 with probability given by\n(14.2) when x(s) = x(t).\nb.\nDetermine the connected components, V1,...,Vm, of the graph G\u03be = (V ,E\u03be)\nwith edges given by pairs {s,t} such that \u03be(st) = 1.\nc.\nSample values c1,...,cm \u2208F according to the distribution\nq(c1,...,cm) \u221d\u03c0(c(V1)\n1\n\u2227\u00b7\u00b7\u00b7 \u2227c(Vm)\nm\n)\n\u03b6(c(V1)\n1\n\u2227\u00b7\u00b7\u00b7 \u2227c(Vm)\nm\n)\n.\nd. Set x = c(V1)\n1\n\u2227\u00b7\u00b7\u00b7 \u2227c(Vm)\nm\n.\nStep (2.c) takes a simple form in the special case when \u03c0 is a non-homogeneous Potts\nmodel ((13.32)) with positive interactions, that we will write as\n\u03c0(x) = exp\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\u2212\nX\ns\u2208V\n\u03b1sx(s) \u2212\nX\n{s,t}\u2208E\n\u03b2st1x(s),x(t)\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\n14.1. MONTE CARLO SAMPLING\n333\nwith \u03b2st \u22650. Then\n\u03c0(x)\n\u03b6(x) \u221dexp\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\u2212\nX\ns\u2208V\n\u03b1sx(s) \u2212\nX\n{s,t}\u2208E\n(\u03b2st \u2212\u03b2\u2032\nst)1xs,st\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\nwith \u03b2\u2032\nst = log(1 + e\u00b5st). If one chooses \u00b5st such that \u03b2\u2032\nst = \u03b2st (which is possible since\n\u03b2st \u22650), then the interaction term disappears and the probability q in (2.c) is propor-\ntional to\nm\nY\nj=1\nexp\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\u2212\nX\ns\u2208Vj\n\u03b1s\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\nso that c1,...,cm can be generated independently. The resulting algorithm is the\nSwendsen-Wang sampling algorithm for the Potts model [186]. The presentation\ngiven here adapts the one introduced in Barbu and Zhu [16].\nFor more general models, step (2.c) can be computationally costly, especially if the\nnumber of connected components is large. In this case, this step can be replaced by\na Gibbs sampling step for one of the c\u2032\njs conditional to the others (and \u03be) that we\nsummarize in the following variation of Algorithm 14.1.\nAlgorithm 14.2 (Cluster-based sampling: Version 2)\nThis algorithm samples from (14.1).\n(1) Initialize the algorithm some configuration x \u2208F (V ).\n(2) Loop over the following steps:\na.\nGenerate a configuration \u03be \u2208Bx such that \u03be(st) = 1 with probability given by\n(14.2) when x(s) = x(t).\nb.\nDetermine the connected components, V1,...,Vm, of the graph G\u03be = (V ,E\u03be)\nwith edges given by pairs {s,t} such that \u03be(st) = 1. Note that x is constant on\neach of these connected components, i.e., there exists c1,...,cm \u2208F such that\nx = c(V1)\n1\n\u2227\u00b7\u00b7\u00b7 \u2227c(Vm)\nm\n.\nc.\nSelect at random one of the components, say, j0 \u2208{1,...,m}.\nd. Sample the value \u02dccj0 \u2208F according to the distribution\nq(\u02dccj0) \u221d\u03c0(\u02dcc(V1)\n1\n\u2227\u00b7\u00b7\u00b7 \u2227\u02dcc(Vm)\nm\n)\n\u03b6(\u02dcc(V1)\n1\n\u2227\u00b7\u00b7\u00b7 \u2227\u02dcc(Vm)\nm\n)\n.\nwith \u02dccj = cj if j , j0.\ne.\nSet x(s) = \u02dccj0 for s \u2208Vj0.\nUnlike single-variable updating schemes, these algorithms can update large chunks\nof the configurations at each step, and may result in significantly faster convergence\n334\nCHAPTER 14. PROBABILISTIC INFERENCE FOR MRF\nof the sampling procedure. Note that step (2.d) in Algorithm 14.2 can be replaced\nby a Metropolis-Hastings update with a proper choice of proposal probability [16].\nb. Parallel tempering. We now consider a different kind of extension in which we\nallow \u03c0 depends continuously on a parameter \u03b2 > 0, writing \u03c0\u03b2 and, the goal is to\nsample from \u03c01. For example, one can extend (14.1) by the family of probability\ndistributions\n\u03c0\u03b2(x) = 1\nZ\u03b2\nexp\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\u2212\u03b2\nX\nC\u2208CG\nhC(x(C))\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\nfor \u03b2 \u22650. For small \u03b2, \u03c0\u03b2 gets close to the uniform distribution on F (V ) (achieved for\n\u03b2 = 0), so that it becomes easier to move from local mode to local mode. This implies\nthat sampling with small \u03b2 is more efficient and the associated Markov chain moves\nmore rapidly in the configuration space.\nAssume given, for all \u03b2, two ergodic transition probabilities on F (V ), q\u03b2 and \u02dcq\u03b2 such\nthat (12.7) is satisfied with \u03c0\u03b2 as invariant probability, namely\n\u03c0\u03b2(y)q\u03b2(y,x) = \u03c0\u03b2(x)\u02dcq\u03b2(x,y)\n(14.3)\nfor all x,y \u2208F (V ) (as seen in (12.7), \u02dcq\u03b2 is the transition probability for the reversed\nchain). The basic idea is that q\u03b2 provides a Markov chain that converges rapidly\nfor small \u03b2 and slowly when \u03b2 is closer to 1. Parallel tempering (this algorithm\nwas introduced in Neal [140] based on ideas developed in Marinari and Parisi [126])\nleverages this fact (and the continuity of \u03c0\u03b2 in \u03b2) to accelerate the simulation of \u03c01\nby introducing intermediate steps sampling at low \u03b2 values.\nThe algorithm specifies a sequence of parameters 0 \u2264\u03b21 \u2264\u00b7\u00b7\u00b7 \u2264\u03b2m = 1. One simula-\ntion steps goes down, then up this scale, as described in the following algorithm.\nAlgorithm 14.3 (Parallel Tempering)\nStart with an initial configuration x0 \u2208F (V ). This configuration is then updated at\neach step, using the following sequence of operations.\n(1) For j = 1,...,m, generate a configuration xj according to \u02dcq\u03b2j(xj\u22121,\u00b7).\n(2) Generate a configuration zm\u22121 according to q\u03b2m(xm,\u00b7).\n(3) For j = m \u22121,...,1, generate a configuration zj\u22121 according to q\u03b2j(zj,\u00b7).\n(4) Set x0 = z0 with probability\nmin\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed1,\n\u03c0\u03b20(z0)\n\u03c0\u03b20(x0)\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\nm\u22121\nY\nj=1\n\u03c0\u03b2j(xj\u22121)\n\u03c0\u03b2j(xj)\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\n\u03c0\u03b2m(xm\u22121)\n\u03c0\u03b2m(zm\u22121)\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\nm\u22121\nY\nj=1\n\u03c0\u03b2j(zj)\n\u03c0\u03b2j(zj\u22121)\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8.\n(Otherwise, keep x0 unchanged).\n14.2. INFERENCE WITH ACYCLIC GRAPHS\n335\nImportantly, the acceptance probability at step (4) only involves ratios of \u03c0\u2032\n\u03b2s and\ntherefore no normalizing constant. We now show that this algorithm is \u03c0\u03b20-reversible.\nLet p(\u00b7,\u00b7) denote the transition probability of the chain. If z0 , x0, p(x0,z0) corre-\nsponds to steps (1) to (3), with acceptance at step(4), and is therefore given by the\nsum, over all x1,...,xm and z1,...,zm, of products\n\u02dcq\u03b21(x0,x1)\u00b7\u00b7\u00b7 \u02dcq\u03b2m(xm\u22121,xm)q\u03b2m(xm,zm\u22121)\u00b7\u00b7\u00b7q\u03b21(z1,z0)\nmin\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed1,\n\u03c0\u03b20(z0)\n\u03c0\u03b20(x0)\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\nm\u22121\nY\nj=1\n\u03c0\u03b2j(xj\u22121)\n\u03c0\u03b2j(xj)\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\n\u03c0\u03b2m(xm\u22121)\n\u03c0\u03b2m(zm\u22121)\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\nm\u22121\nY\nj=1\n\u03c0\u03b2j(zj)\n\u03c0\u03b2j(zj\u22121)\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\nApplying (14.3), this is equal to\nmin\n\u0012\n\u02dcq\u03b21(x0,x1)\u00b7\u00b7\u00b7 \u02dcq\u03b2m(xm\u22121,xm)q\u03b2m(xm,zm\u22121)\u00b7\u00b7\u00b7q\u03b21(z1,z0),\n\u03c0\u03b20(z0)\n\u03c0\u03b20(x0)q\u03b21(x0,x1)\u00b7\u00b7\u00b7q\u03b2m(xm\u22121,xm)\u02dcq\u03b2m(xm,zn\u22121)\u00b7\u00b7\u00b7 \u02dcq\u03b21(z1,z0)\n\u0013\nSo,\n\u03c0\u03b20(x0)p(x0,z0) =\nX\nmin\n\u0012\n\u03c0\u03b20(x0)\u02dcq\u03b21(x0,x1)\u00b7\u00b7\u00b7 \u02dcq\u03b2m(xm\u22121,xm)q\u03b2m(xm,zm\u22121)\u00b7\u00b7\u00b7q\u03b21(z1,z0),\n\u03c0\u03b20(z0)q\u03b21(x1,x0)\u00b7\u00b7\u00b7q\u03b2m(xm,xm\u22121)\u02dcq\u03b2m(zm\u22121,xm)\u00b7\u00b7\u00b7 \u02dcq\u03b21(z0,z1)\n\u0013\nwhere the sum is over all x1,...,xm,z1,...,zm\u22121 \u2208F (V ). The sum is, of course, un-\nchanged if one renames x1,...,xm,z1,...,zm\u22121 to z1,...,zm,x1,...,xm\u22121, but doing so\nprovides the expression of \u03c0\u03b20(z0)p(z0,x0), proving the reversibility of the chain with\nrespect to \u03c0\u03b20.\n14.2\nInference with acyclic graphs\nWe now switch to deterministic methods to compute, or approximate, marginal\nprobabilities of Markov random fields. In this section, we consider a directed acyclic\ngraph G = (V ,E). As we have seen, Markov processes for acyclic graphs are also\nMarkov for any tree structure associated with the graph. Introducing such a tree,\n\u02dcG = (V , \u02dcE) with \u02dcG\u266d= G, we know that a Markov process on G can be written in the\nform (letting s0 denote the root in \u02dcG):\n\u03c0(x) = ps0(x(s0))\nY\n(s,t)\u2208\u02dcE\npst(x(s),x(t))\n(14.4)\n336\nCHAPTER 14. PROBABILISTIC INFERENCE FOR MRF\nwhere ps0 is a probability and pst a transition probability.\nWe now show how to compute marginal probabilities of configurations x(S), de-\nnoted \u03c0S(x(S)), for a set S \u2282V , starting with singletons S = {s}. The computation\ncan be done by propagating down the tree as follows. For s = s0, the probability is\nknown, with \u03c0s0 = ps0. Now take an arbitrary s , s0 and let pa(s) be its parent. Then\n\u03c0s(x(s)) = P(X(s) = x(s)) =\nX\ny(pa(s))\u2208Fpa(s)\nP(X(s) = x(s) | X(pa(s)) = y(pa(s)))P(x(pa(s)) = y(pa(s)))\n=\nX\ny(pa(s))\u2208Fpa(s)\n\u03c0pa(s)(y(pa(s)))ppa(s)(ypa(s),x(s))\nso that the marginal probability at any s , s0 can be computed given the marginal\nprobability of its parent. We can propagate the computation down the tree, with a\ntotal cost for computing \u03c0s proportional to Pn\nk=1 |Ftk\u22121||Ftk| where t0 = s0,t1,...,tn = s\nis the unique path between s0 and s. This is linear in the depth of the tree, and\nquadratic (not exponential) in the sizes of the state spaces. The computation of all\nsingleton marginals requires an order of P\n(s,t)\u2208E |Fs||Ft| operations.\nNow, assume that probabilities of singletons have been computed and consider\nan arbitrary set S \u2282V . Let s \u2208V be an ancestor of every vertex in S, maximal in the\nsense that none of its children also satisfy this property. Consider the subtrees of\n\u02dcG starting from each of the children of s, denoted \u02dcG1,..., \u02dcGn with \u02dcGk = (Vk, \u02dcEk). Let\nSk = S \u2229Vk. From the conditional independence,\n\u03c0S(x(S))\n=\nX\ny(s)\u2208Fs\nP(X(S\\{s}) = x(S\\{s}) | X(s) = y(s))\u03c0s(y(s))\n=\nX\ny(s)\u2208Fs\nn\nY\nk=1,Sk,\u2205\nP(X(Sk) = x(Sk) | X(s) = y(s))\u03c0s(ys)\nNow, for all k = 1,...,n, we have |Sk| < |S|: this is obvious if S is not completely\nincluded in one of the Vk\u2019s. But if S \u2282Vk then the root, sk, of Vk is an ancestor of\nall the elements in S and is a child of s, which contradicts the assumption that s is\nmaximal. So we have reduced the computation of \u03c0S(xS) to the computations of n\nprobabilities of smaller sets, namely P(X(Sk) = x(Sk) | X(s) = y(s)) for Sk , \u2205. Because\nthe distribution of X(Vk) conditioned at s is a \u02dcGk-Markov model, we can reiterate\nthe procedure until only sets of cardinality one remain, for which we know how to\nexplicitly compute probabilities.\nThis provides a feasible algorithm to compute marginal probabilities with trees,\nat least when its distribution is given in tree-form, like in (14.4). We now address\n14.2. INFERENCE WITH ACYCLIC GRAPHS\n337\nthe situation in which one starts with a probability distribution associated with pair\ninteractions (cf. definition 13.24) over the acyclic graph G\n\u03c0(x) = 1\nZ\nY\ns\u2208V\n\u03d5s(x(s))\nY\n{s,t}\u2208E\n\u03d5st(x(s),x(t)).\n(14.5)\nWe assume these local interactions to be consistent, still allowing for some vanishing\n\u03d5st(x(s),x(t)).\nPutting \u03c0 in the form (14.4) is equivalent to computing all joint probability dis-\ntributions \u03c0st(x(s),x(t)) for {s,t} \u2208E, and we now describe this computation. Denote\nU(x) =\nY\ns\u2208V\n\u03d5s(x(s))\nY\n{s,t}\u2208E\n\u03d5st(x(s),x(t))\nso that Z = P\ny\u2208F (V ) U(y). For the tree \u02dcG = (V , \u02dcE), and t \u2208V , we let \u02dcGt = (Vt, \u02dcEt) be the\nsubtree of G rooted at t (containing t and all its descendants). For S \u2282V , define\nUS(x(S)) =\nY\ns\u2208S\n\u03d5s(x(s))\nY\n{s,s\u2032}\u2208E,s,s\u2032\u2208D\n\u03d5ss\u2032(x(s),x(s\u2032))\nand\nZt(x(t)) =\nX\ny(V \u2217t )\u2208F (V \u2217\nt )\nUVt(x(t) \u2227y(V \u2217\nt )).\nwith V \u2217\nt = Vt \\ {t}.\nLemma 14.1 Let G = (V ,E) be a directed acyclic graph and \u03c0 = PX be the G-Markov\ndistribution given by (14.5). With the notation above, we have\n\u03c0s0(x(s0)) =\nZs0(x(s0))\nP\ny(s0)\u2208Fs0 Zs0(y(s0))\n(14.6)\nand, for (s,t) \u2208\u02dcE,\npst(x(s),x(t)) = P(X(t) = x(t) | X(s) = x(s)) =\n\u03d5st(x(s),x(t))Zt(x(t))\nP\ny(t)\u2208Ft \u03d5st(x(s),y(t))Zt(y(t))\n(14.7)\nProof Let Wt = V \\Vt. Clearly, Z = P\nx(0)\u2208Fs0 Zs0(x(0)) and \u03c0s0(x(0)) = Zs0(x(0))/Z which\ngives (14.6). Moreover, if s \u2208V , we have\nP(X(V \u2217s ) = x(V \u2217s ) | X(s) = x(s)) =\nP\ny(Ws) U(x(Vs) \u2227y(Ws))\nP\ny(V \u2217s ),y(Ws) U(x(s) \u2227y(V \u2217s ) \u2227y(Ws)).\n338\nCHAPTER 14. PROBABILISTIC INFERENCE FOR MRF\nWe can write\nU(x(s) \u2227y(V \u2217s ) \u2227y(Ws)) = UVs(x(s) \u2227y(V \u2217s ))U{s}\u222aWs(x(s) \u2227y(Ws))\u03d5s(x(s))\u22121\nyielding the simplified expression\nP(X(V \u2217s ) = x(V \u2217s ) | X(s) = x(s)) =\nUVs(x(Vs))\u03d5s(x(s))\u22121 P\nyWs U{s}\u222aWs(x(s) \u2227y(Ws))\n\u03d5s(x(s))\u22121\u0010P\ny(V \u2217s ) UVs(x(s) \u2227y(V \u2217s ))\n\u0011\u0010P\ny(Ws) U{s}\u222aWs(x(s) \u2227y(Ws))\n\u0011\n= UVs(x(Vs))\nZs(x(s))\nNow, if t1,...,tn are the children of s, we have\nUVs(x(Vs)) = \u03d5s(x(s))\nn\nY\nk=1\n\u03d5stk(x(s),x(tk))\nn\nY\nk=1\nUVtk (x(Vtk )),\nso that\nP(X(tk) = x(tk),k = 1,...,n | X(s) = x(s))\n=\n1\nZs(x(s))\nX\ny\n(V \u2217tk ),k=1,...,n\n\u03d5s(x(s))\nn\nY\nk=1\n\u03d5stk(x(s),x(tk))\nn\nY\nk=1\nUVtk (x(tk) \u2227y(V \u2217\ntk ))\n= \u03d5s(x(s))Qn\nk=1 \u03d5stk(x(s),x(tk))Qn\nk=1 Ztk(x(tk))\nZs(x(s))\nThis implies that the transition probability needed for the tree model, pst1(x(s),x(t1)),\nmust be proportional to \u03d5st1(x(s),x(t1))Zt1(x(t1)) which proves the lemma.\n\u25a0\nThis lemma reduces the computation of the transition probabilities to the com-\nputation of Zs(x(s)), for s \u2208V . This can be done efficiently, going upward in the\ntree (from terminal vertexes to the root). Indeed, if s is terminal, then Vs = {s} and\nZs(x(s)) = \u03d5s(x(s)). Now, if s is non-terminal and t1,...,tn are its children, then, it is\neasy to see that\nZs(x(s)) = \u03d5s(x(s))\nX\nx(t1)\u2208Ft1,...,x(tn)\u2208Ftn\nn\nY\nk=1\n\u03d5stk(x(s),x(tk))Ztk(x(tk))\n= \u03d5s(x(s))\nn\nY\nk=1\n\u0012\nX\nx(tk)\u2208Ftk\n\u03d5stk(x(s),x(tk))Ztk(x(tk))\n\u0013\n(14.8)\nSo, Zs(x(s)) can be easily computed once the Zt(x(t))\u2019s are known for the children of s.\n14.2. INFERENCE WITH ACYCLIC GRAPHS\n339\nEquations (14.6) to (14.8) therefore provide the necessary relations in order to\ncompute the singleton and edge marginal probabilities on the tree. It is important\nto note that these relations are valid for any tree structure consistent with the acyclic\ngraph we started with. We now rephrase them with notation that only depend on\nthis graph and not on the selected orientation.\nLet {s,t} be an edge in E. Then s separates the graph G \\ {s} into two components.\nLet Vst be the component that contains t, and V \u2217\nst = Vst \\ t. Define\nZst(xt) =\nX\ny(V \u2217st)\u2208F (V \u2217\nst)\nUVst(x(t) \u2227y(V \u2217\nst)).\nThis Zst coincides with the previously introduced Zt, computed with any tree in\nwhich the edge {s,t} is oriented from s to t. Equation (14.8) can be rewritten with\nthis new notation in the form:\nZst(x(t)) = \u03d5t(x(t))\nY\nt\u2032\u2208Vt\\{s}\n\u0012 X\nx(t\u2032)\u2208Ft\u2032\n\u03d5tt\u2032(x(t),x(t\u2032))Ztt\u2032(x(t\u2032))\n\u0013\n.\n(14.9)\nThis equation is usually written in terms of \u201cmessages\u201d defined by\nmts(x(s)) =\nX\nx(t)\u2208Ft\n\u03d5st(x(s),x(t))Zst(x(t))\nwhich yields\nZst(x(t)) = \u03d5t(x(t))\nY\nt\u2032\u2208Vt\\{s}\nmt\u2032t(x(t))\nand the message consistency relation\nmts(x(s)) =\nX\nx(t)\u2208Ft\n\u03d5st(x(s),x(t))\u03d5t(x(t))\nY\nt\u2032\u2208Vt\\{s}\nmt\u2032t(x(t)).\n(14.10)\nAlso, because one can start building a tree from G\u266dusing any vertex as a root,\n(14.6) is valid for any s \u2208V , in the form (applying (14.8) to the root)\n\u03c0s(x(s)) = 1\n\u03b6s\n\u03d5s(x(s))\nY\nt\u2208Vs\nmts(x(s))\n(14.11)\nwhere \u03b6s is chosen to ensure that the sum of probabilities is 1. (In fact, looking at\nlemma 14.1, we have Zs = Z, independent of s.)\nSimilarly, (14.7) can be written\npst(x(s),x(t)) = mts(x(s))\u22121\u03d5st(x(s),x(t))\u03d5t(x(t))\nY\nt\u2032\u2208Vt\\{s}\nmt\u2032t(x(t))\n(14.12)\n340\nCHAPTER 14. PROBABILISTIC INFERENCE FOR MRF\nwhich provides the edge transition probabilities. Combining this with (14.11), we\nget the edge marginal probabilities:\n\u03c0st(x(s),x(t)) = 1\n\u03b6 \u03d5st(x(s),x(t))\u03d5s(x(s))\u03d5t(x(t))\nY\nt\u2032\u2208Vt\\{s}\nmt\u2032t(x(t))\nY\ns\u2032\u2208Vs\\{t}\nms\u2032s(x(s)).\n(14.13)\nRemark 14.2 We can modify (14.10) by multiplying the right-hand side by an ar-\nbitrary constant qts without changing the resulting estimation of probabilities: this\nonly multiplies the messages by a constant, which cancels after normalization. This\nremark can be useful in particular to avoid numerical overflow; one can, for exam-\nple, define qts = 1/ P\nxs\u2208Fs mts(xs) so that the messages always sum to 1. This is also\nuseful when applying belief propagation (see next section) to loopy networks, for\nwhich (14.10) may diverge while the normalized version converges.\n\u2666\nThe following summarizes this message passing algorithm.\nAlgorithm 14.4 (Belief propagation on acyclic graphs)\nGiven a family of interactions \u03d5s : Fs \u2192[0,+\u221e), \u03d5st : Fs \u00d7 Ft \u2192[0,+\u221e),\n(1) Initialize functions (messages) mts : Fs \u2192R, e.g., taking mts(x(s)) = 1/|Fs|.\n(2) Compute unnormalized messages\n\u02dcmts(\u00b7) =\nX\nx(t)\u2208Ft\n\u03d5st(\u00b7,x(t))\u03d5t(x(t))\nY\nt\u2032\u2208Vt\\{s}\nmt\u2032t(x(t))\nand let mts(\u00b7) = qts \u02dcmts(\u00b7), for some choice of constant qts, which must be a fixed func-\ntion of \u02dcmts(\u00b7), such as\nqts =\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\nX\nx(s)\u2208Fs\n\u02dcmts(x(s))\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\n\u22121\n.\n(3) Stop the algorithm when the messages stabilize (which happens after a finite\nnumber of updates). Compute the edge marginal distributions using (14.13).\nIt should be clear, from the previous analysis that messages stabilize in finite time,\nstarting from the outskirts of the acyclic graph. Indeed, messages starting from a\nterminal t (a vertex with only one neighbor) are automatically set to their correct\nvalue in (14.10),\nmts(xs) =\nX\nxt\u2208Ft\n\u03d5st(xs,xt)\u03d5t(xt),\nat the first update. These values then propagate to provide messages that satisfy\n(14.10) starting from the next-to-terminal vertexes (those that have only one neigh-\nbor left when the terminals are removed) and so on.\n14.3. BELIEF PROPAGATION AND FREE ENERGY APPROXIMATION\n341\n14.3\nBelief propagation and free energy approximation\n14.3.1\nBP stationarity\nIt is possible to run Algorithm 14.4 on graphs that are not acyclic, since nothing in\nits formulation requires this property. However, while the method stabilizes in finite\ntime for acyclic graphs, this property, or even the convergence of the messages is not\nguaranteed for general, loopy, graphs. Convergence, however, has been observed in\na large number of applications, sometimes with very good approximations of the\ntrue marginal distributions.\nWe will refer to stable solutions of Algorithm 14.4 as BP-stationary points, as\nformally stated in the next definition, which allows for a possible normalization of\nmessages, which is particularly important with loopy networks.\nDefinition 14.3 Let G = (V ,E) be an undirected graph and \u03a6 = (\u03d5st,{s,t} \u2208E,\u03d5s,s \u2208\nV ) a consistent family of pair interactions. We say that a family of joint probability\ndistributions (\u03c0\u2032\nst,{s,t} \u2208E) is BP-stationary for (G,\u03a6) if there exists messages xt \u2208Ft 7\u2192\nmst(xt), constants \u03b6st for t \u223cs and \u03b1s for s \u2208V satisfying\nmts(x(s)) = \u03b1s\n\u03b6ts\nX\nx(t)\u2208Ft\n\u03d5st(x(s),x(t))\u03d5t(x(t))\nY\nt\u2032\u2208Vt\\{s}\nmt\u2032t(x(t))\n(14.14)\nsuch that\n\u03c0\u2032\nst(x(s),x(t)) = 1\n\u03b6st\n\u03d5st(x(s),x(t))\u03d5s(x(s))\u03d5t(x(t))\nY\nt\u2032\u2208Vt\\{s}\nmt\u2032t(x(t))\nY\ns\u2032\u2208Vs\\{t}\nms\u2032s(x(s)). (14.15)\nThere is no loss of generality in the specific form chosen for the normalizing con-\nstants in (14.14) and (14.15), in the sense that, if the messages satisfy (14.15) and\nmts(x(s)) = qts\nX\nx(t)\u2208Ft\n\u03d5st(x(s),x(t))\u03d5t(x(t))\nY\nt\u2032\u2208Vt\\{s}\nmt\u2032t(x(t))\nfor some constants qts, then\n\u03b6st\n=\nX\nx(s)\u2208Fs,x(t)\u2208Ft\n\u03d5st(x(s),x(t))\u03d5s(x(s))\u03d5t(x(t))\nY\nt\u2032\u2208Vt\\{s}\nmt\u2032t(x(t))\nY\ns\u2032\u2208Vs\\{t}\nms\u2032s(x(s))\n=\n1\nqts\nX\nx(s)\u2208Fs\n\u03d5s(x(s))\nY\ns\u2032\u2208Vs\nms\u2032s(x(s))\nso that \u03b6stqts (which has been denoted \u03b1s) does not depend on t. Of course, the rele-\nvant questions regarding BP-stationarity is whether the collection of pairwise prob-\nability \u03c0\u2032\nst exists, how to compute them, and whether \u03c0\u2032\nst(x(s),x(t)) provides a good\n342\nCHAPTER 14. PROBABILISTIC INFERENCE FOR MRF\napproximation of the marginals of the probability distribution \u03c0 that is associated\nto \u03a6, namely\n\u03c0(x) = 1\nZ\nY\ns\u2208V\n\u03d5s(x(s))\nY\n{s,t}\u2208E\n\u03d5st(x(s),x(t)).\nA reassuring statement for BP-stationarity is that it is not affected when the func-\ntions in \u03a6 are multiplied by constants, which does not affect the underlying proba-\nbility \u03c0. This is stated in the next proposition.\nProposition 14.4 Let \u03a6 be as above a family of edge and vertex interactions. Let cst,{s,t} \u2208\nE, cs,s \u2208V be families of positive constants, and define \u02dc\u03a6 = ( \u02dc\u03d5st, \u02dc\u03d5s) by \u02dc\u03d5st = cst\u03d5st and\n\u02dc\u03d5s = cs\u03d5s. Then,\n\u03c0\u2032 is BP-stationary for (G,\u03a6) \u21d4\u03c0\u2032 is BP-stationary for (G, \u02dc\u03a6).\nProof Indeed, if (14.14) and (14.15) are true for (G,\u03a6), it suffices to replace \u03b1s by\n\u03b1scs and \u03b6st by \u03b6stcstct to obtain (14.14) and (14.15) for (G, \u02dc\u03a6).\n\u25a0\nIt is also important to notice that, if G is acyclic, definition 14.3 is no more general\nthan the message-passing rule we had considered earlier. More precisely, we have\n(see remark 14.2),\nProposition 14.5 Let G = (V ,E) be undirected acyclic and \u03a6 = (\u03d5st,{s,t} \u2208E,\u03d5s,s \u2208V )\na consistent family of pair interactions. Then, the only BP-stationary distributions are the\nmarginals of the distribution \u03c0 associated to \u03a6.\n14.3.2\nFree-energy approximations\nA partial justification of the good behavior of BP with general graphs has been pro-\nvided in terms of a quantity introduced in statistical mechanics, called the Bethe free\nenergy. We let G = (V ,E) be an undirected graph and assume that a consistent family\nof pair interactions is given (denoted \u03a6 = (\u03d5s,s \u2208V ,\u03d5st,{s,t} \u2208E)) and consider the\nassociated distribution, \u03c0, on F (V ), given by\n\u03c0(x) = 1\nZ\nY\ns\u2208V\n\u03d5s(x(s))\nY\n{s,t}\u2208E\n\u03d5st(x(s),x(t)).\n(14.16)\nIt will also be convenient to use the function\n\u03c8st(x(s),x(t)) = \u03d5s(x(s))\u03d5t(x(t))\u03d5st(x(s),x(t))\nsuch that\n\u03c0(x) = 1\nZ\nY\ns\u2208V\n\u03d5s(x(s))1\u2212|Vs| Y\n{s,t}\u2208E\n\u03c8st(x(s),x(t)).\n(14.17)\n14.3. BELIEF PROPAGATION AND FREE ENERGY APPROXIMATION\n343\nWe will consider approximations \u03c0\u2032 of \u03c0 that minimize the Kullback-Leibler di-\nvergence, KL(\u03c0\u2032\u2225\u03c0) (see (4.3)), subject to some constraints. We can write\nKL(\u03c0\u2032\u2225\u03c0)\n=\n\u2212E\u03c0\u2032(log\u03c0) \u2212H(\u03c0\u2032)\n=\n\u2212logZ \u2212\nX\ns\u2208V\n(1 \u2212|Vs|)E\u03c0\u2032(log\u03d5s) \u2212\nX\n{s,t}\u2208E\nE\u03c0\u2032(log\u03c8st) \u2212H(\u03c0\u2032)\n(where H(\u03c0\u2032) is the entropy of \u03c0\u2032). Introduce the one- and two-dimensional marginals\nof \u03c0\u2032, denoted \u03c0\u2032\ns ad \u03c0\u2032\nst. Then\nKL(\u03c0\u2032\u2225\u03c0) = \u2212logZ \u2212\nX\ns\u2208V\n(1 \u2212|Vs|)E\u03c0\u2032(log \u03d5s\n\u03c0\u2032s\n) \u2212\nX\n{s,t}\u2208E\nE\u03c0\u2032(log \u03c8st\n\u03c0\u2032\nst\n)\n+\nX\ns\u2208V\n(1 \u2212|Vs|)H(\u03c0\u2032\ns) +\nX\n{s,t}\u2208E\nH(\u03c0\u2032\nst) \u2212H(\u03c0\u2032).\nThe Bethe free energy is the function F\u03b2 defined by\nF\u03b2(\u03c0\u2032) = \u2212\nX\ns\u2208V\n(1 \u2212|Vs|)E\u03c0\u2032(log \u03d5s\n\u03c0\u2032s\n) \u2212\nX\n{s,t}\u2208E\nE\u03c0\u2032(log \u03c8st\n\u03c0\u2032\nst\n);\n(14.18)\nso that\nKL(\u03c0\u2032\u2225\u03c0) = F\u03b2(\u03c0\u2032) \u2212logZ + \u2206G(\u03c0\u2032)\nwith\n\u2206G(\u03c0\u2032) =\nX\ns\u2208V\n(1 \u2212|Vs|)H(\u03c0\u2032\ns) +\nX\n{s,t}\u2208E\nH(\u03c0\u2032\nst) \u2212H(\u03c0\u2032).\nUsing this computation, one can consider the approximation problem: find \u02c6\u03c0\u2032\nthat minimizes KL(\u03c0\u2032\u2225\u03c0) over a class of distributions \u03c0\u2032 for which the computation\nof the first and second order marginals is easy. This problem has an explicit solution\nwhen the distribution \u03c0\u2032 is such that all variables are independent, leading to what\nis called the mean-field approximation of \u03c0. Indeed, in this case, we have\n\u2206G(\u03c0\u2032) =\nX\n{s,t}\u2208G\n(H(\u03c0\u2032\ns) + H(\u03c0\u2032\nt)) +\nX\ns\u2208S\n(1 \u2212|Vs|)H(\u03c0\u2032\ns) \u2212\nX\ns\u2208S\nH(\u03c0\u2032\ns) = 0\nand\nF\u03b2(\u03c0\u2032) = \u2212\nX\ns\u2208V\n(1 \u2212|Vs|)E\u03c0\u2032(log \u03d5s\n\u03c0\u2032s\n) \u2212\nX\n{s,t}\u2208E\nE\u03c0\u2032(log \u03c8st\n\u03c0\u2032s\u03c0\u2032\nt\n).\nF\u03b2 must be minimized with respect to the variables \u03c0\u2032\ns(x(s)),s \u2208S,xs \u2208FS subject to\nthe constraints P\nxs\u2208Fs \u03c0\u2032\ns(x(s)) = 1. The corresponding necessary optimality condi-\ntions equations provide the mean-field consistency equations, described in the fol-\nlowing definition.\n344\nCHAPTER 14. PROBABILISTIC INFERENCE FOR MRF\nProposition 14.6 A local minimum of F\u03b2(\u03c0\u2032) over all probability distributions \u03c0\u2032 of the\nform\n\u03c0\u2032(x) =\nY\ns\u2208V\n\u03c0\u2032\ns(x(s))\nmust satisfy the mean field consistency equations:\n\u03c0s(x(s)) = 1\nZs\n\u03d5s(x(s))1\u2212|Vs| Y\nt\u223cs\nexp\n\u0010\nE\u03c0t(log\u03c8st(x(,).))\n\u0011\n.\n(14.19)\nProof Since all constraints are affine, we can use Lagrange multipliers, denoted\n(\u03bbs,s \u2208S) for each of the constraints, to obtain necessary conditions for a minimizer,\nyielding\n\u2202F\u03b2\n\u2202\u03c0s(xs) \u2212\u03bbs = 0, s \u2208S,xs \u2208Fs.\nThis gives:\n\u2212(1 \u2212|Vs|)\n \nlog \u03d5s(xs)\n\u03c0s(xs) \u22121\n!\n\u2212\nX\nt\u223cs\nX\nxt\u2208Ft\n \nlog \u03c8st(xs,xt)\n\u03c0s(xs)\u03c0t(xt) \u22121\n!\n\u03c0t(xt) = \u03bbs.\nSolving this with respect to \u03c0s(xs) and regrouping all constant terms (independent\nfrom xs) in the normalizing constant Zs yields (14.19).\n\u25a0\nThe mean field consistency equations can be solved using a root-finding algo-\nrithm or by directly solving the minimization problem. We will retrieve this method,\nwith more details, in our discussion of variational approximations in chapter 16.\nIn the particular case in which G is acyclic and the approximation is made by\nG-Markov processes, the Kullback-Leibler distance is minimized with \u03c0\u2032 = \u03c0 (since\n\u03c0 belongs to the approximating class). A slightly non-trivial remark is that \u03c0 is\noptimal also for the minimization of the Bethe free energy F\u03b2, because this energy\ncoincides, up to the constant term logZ, with the Kullback-Leibler divergence, as\nproved by the following proposition.\nProposition 14.7 If G is acyclic and \u03c0\u2032 is G-Markov, then \u2206G(\u03c0\u2032) = 0.\nThis proposition is a consequence of the following lemma that has its own interest:\nLemma 14.8 If G is acyclic and \u03c0 is a G-Markov distribution, then\n\u03c0(x) =\nY\ns\u2208V\n\u03c0s(x(s))1\u2212|Vs| Y\n{s,t}\u2208E\n\u03c0st(x(s),x(t)).\n(14.20)\n14.3. BELIEF PROPAGATION AND FREE ENERGY APPROXIMATION\n345\nProof (of lemma 14.8) We know that, if \u02dcG = (V , \u02dcE) is a tree such that \u02dcG\u266d= G, we\nhave, letting s0 be the root in \u02dcG\n\u03c0(x)\n=\n\u03c0s0(x(s0))\nY\n(s,t)\u2208\u02dcE\npst(x(s),x(t))\n=\n\u03c0s0(x(s0))\nY\n(s,t)\u2208\u02dcE\n(\u03c0st(x(s),x(t))\u03c0(x(s))\u22121).\nEach vertex s in V has |Vs|\u22121 children in \u02dcG, except s0 which has |Vs0| children. Using\nthis, we get\n\u03c0(x)\n=\n\u03c0s0(x(s0))\u03c0s0(x(s0))\u2212|Vs0|\nY\ns\u2208V \\{s0}\n\u03c0s(x(s))1\u2212|Vs| Y\n(s,t)\u2208\u02dcE\n\u03c0st(x(s),x(t))\n=\nY\ns\u2208V\n\u03c0s(x(s))1\u2212|Vs| Y\n{s,t}\u2208E\n\u03c0st(x(s),x(t)).\nProof (of proposition 14.7) If \u03c0\u2032 is given by (14.20), then\nH(\u03c0\u2032)\n=\n\u2212E\u03c0\u2032 log\u03c0\u2032\n=\n\u2212\nX\ns\u2208V\n(1 \u2212|Vs|)E\u03c0\u2032 log\u03c0\u2032\ns \u2212\nX\n{s,t}\u2208E\nE\u03c0\u2032 log\u03c0\u2032\nst\n=\nX\ns\u2208V\n(1 \u2212|Vs|)H(\u03c0\u2032\ns) +\nX\n{s,t}\u2208E\nH(\u03c0\u2032\nst)\nwhich proves that \u2206G(\u03c0\u2032) = 0.\n\u25a0\nIn view of this, it is tempting to \u201cgeneralize\u201d the mean field optimization proce-\ndure and minimize F\u03b2(\u03c0\u2032) over all possible consistent singletons and pair marginals\n(\u03c0\u2032\ns and \u03c0\u2032\nst), then use the optimal ones as an approximation of \u03c0s and \u03c0st. What\nwe have just proved is that this procedure provides the exact expression of the\nmarginals when G is acyclic. For loopy graphs, however, it is not justified, and is\nat best an approximation. A very interesting fact is that this procedure provides the\nsame consistency equations as belief propagation. To see this, we first start with the\ncharacterization of minimizers of F\u03b2.\nProposition 14.9 Let G = (V ,E) be an undirected graph and \u03c0 be given by (14.16).\nConsider the problem of minimizing the Bethe free energy F\u03b2 in (14.18) with respect to all\npossible choices of probability distributions (\u03c0\u2032\nst,{s,t} \u2208E), (\u03c0\u2032\ns,s \u2208V ) with the constraints\n\u03c0\u2032\ns(x(s)) =\nX\nx(t)\u2208Ft\n\u03c0\u2032\nst(x(s),x(t)),\u2200x(s) \u2208Fs and t \u223cs.\n346\nCHAPTER 14. PROBABILISTIC INFERENCE FOR MRF\nThen a local minimum of this problem must take the form\n\u03c0\u2032\nst(x(s),x(t)) = 1\nZst\n\u03c8st(x(s),x(t))\u00b5st(x(t))\u00b5ts(x(s))\n(14.21)\nwhere the functions \u00b5st : Ft \u2192[0,+\u221e) are defined for all (s,t) such that {s,t} \u2208E and\nsatisfy the consistency conditions:\n\u00b5ts(x(s))\u2212(|Vs|\u22121) Y\ns\u2032\u223cs\n\u00b5s\u2032s(x(s)) =\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\ne\nZst\nX\nx(t)\u2208Ft\n\u03c8st(x(s),x(t))\u03d5t(x(t))\u00b5st(x(t))\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\n|Vs|\u22121\n.\n(14.22)\nProof We introduce Lagrange multipliers: \u03bbts(x(s)) for the constraint\n\u03c0\u2032\ns(x(s)) =\nX\nx(t)\u2208Ft\n\u03c0\u2032\nst(x(s),x(t))\nand \u03b3st for\nX\nx(s),x(t)\n\u03c0\u2032\nst(x(s),x(t)) = 1,\nwhich covers all constraints associated to the minimization problem. The associated\nLagrangian is\nF\u03b2(\u03c0\u2032) \u2212\nX\ns\u2208V\nX\nx(s)\u2208Fs\nX\nt\u223cs\n\u03bbts(x(s))\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\nX\nx(t)\u2208Ft\n\u03c0\u2032\nst(x(s),x(t)) \u2212\u03c0\u2032\ns(x(s))\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\n\u2212\nX\n{s,t}\u2208E\n\u03b3st\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\nX\nx(s)\u2208Fs,x(t)\u2208Ft\n\u03c0\u2032\nst(x(s),x(t)) \u22121\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8.\nThe derivative with respect to \u03c0\u2032\nst(x(s),x(t)) yields the condition\nlog\u03c0\u2032\nst(x(s),x(t)) \u2212log\u03c8st(x(s),x(t)) + 1 \u2212\u03bbts(x(s)) \u2212\u03bbst(x(t)) \u2212\u03b3st = 0.\nwhich implies\n\u03c0\u2032\nst(x(s),x(t)) = \u03d5st(x(s),x(t))exp(\u03b3st \u22121)exp(\u03bbts(x(s)) + \u03bbst(x(t))).\nWe let Zst = exp(1 \u2212\u03b3st), with \u03b3st chosen so that \u03c0\u2032\nst is a probability. The derivative\nwith respect to \u03c0\u2032\ns(x(s)) gives\n(1 \u2212|Vs|)(log\u03c0\u2032\ns(x(s)) \u2212log\u03d5s(x(s)) + 1) +\nX\nt\u223cs\n\u03bbts(x(s)) = 0.\n14.3. BELIEF PROPAGATION AND FREE ENERGY APPROXIMATION\n347\nCombining this with the expression just obtained for \u03c0\u2032\nst, we get, for t \u223cs,\n(1 \u2212|Vs|)log\nX\nx(t)\u2208Ft\n\u03c8st(x(s),x(t))e\u03bbst(x(t)) + (1 \u2212|Vs|)\u03bbts(x(s))\n+ (1 \u2212|Vs|)(1 \u2212logZst \u2212log\u03d5s(x(s))) +\nX\ns\u2032\u223cs\n\u03bbs\u2032s(x(s)) = 0,\nwhich gives (14.22) with \u00b5st = exp(\u03bbst).\n\u25a0\nA family \u03c0\u2032\nst satisfying conditions (14.21) and (14.22) of proposition 14.9 will be\ncalled Bethe-consistent. A very interesting remark states that Bethe-consistency is\nequivalent to BP-stationarity, as stated below.\nProposition 14.10 Let G = (V ,E) be an undirected graph and \u03a6 = (\u03d5st,{s,t} \u2208E,\u03d5s,s \u2208\nV ) a consistent family of pair interactions. Then a family \u03c0\u2032 of joint probability distribu-\ntions is BP-stationary if and only if it is Bethe-consistent.\nProof First assume that \u03c0\u2032 is BP-stationary with messages mst, so that (14.14) and (14.15)\nare satisfied. Take\n\u00b5st = at\nY\nt\u2032\u2208Vt,t\u2032,s\nmt\u2032t(x(t))\nfor some constant at that will be determined later. Then, the left-hand side of (14.22)\nis\n\u00b5ts(x(s))\u2212(|Vs|\u22121) Y\ns\u2032\u2208Vs\n\u00b5s\u2032s(x(s)) = as\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\nY\ns\u2032\u2208Vs,s\u2032,t\nms\u2032s(x(s))\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\n\u2212(|Vs|\u22121) Y\ns\u2032\u2208Vs\nY\ns\u2032\u2032\u2208Vs,s\u2032\u2032,s\u2032\nms\u2032\u2032s(x(s))\n= asmts(x(s))|Vs|\u22121.\nThe right-hand side is equal to (using (14.14))\n eat\u03b6st\nZst\u03b1s\nmts(x(s))\n!|Vs|\u22121\n,\nso that we need to have\nas =\n eat\u03b6st\nZst\u03b1s\n!|Vs|\u22121\n.\nWe also need\nZst =\nX\nx(s),x(t)\n\u03c8st(x(s),x(t))\u00b5st(x(t))\u00b5ts(x(s)) = asat\u03b6st.\nSolving these equations, we find that (14.21) and (14.22) are satisfied with\n\uf8f1\uf8f4\uf8f4\uf8f2\uf8f4\uf8f4\uf8f3\nas = (e/\u03b1s)(|Vs|\u22121)/|Vs|\nZst = \u03b6stasat\n348\nCHAPTER 14. PROBABILISTIC INFERENCE FOR MRF\nwhich proves that \u03c0\u2032 is Bethe-consistent.\nConversely, take a Bethe-consistent \u03c0\u2032, and \u00b5st,Zst satisfying (14.21) and (14.22).\nFor s such that |Vs| > 1, define, for t \u2208Vs,\nmts(x(s)) = \u00b5ts(x(s))\u22121 Y\ns\u2032\u223cs\n\u00b5s\u2032s(x(s))1/(|Vs|\u22121).\n(14.23)\nDefine also, for |Vs| > 1,\n\u03c1ts(x(s)) =\nY\ns\u2032\u2208Vs,s\u2032,t\nms\u2032s(x(s)).\n(If |Vs| = 1, take \u03c1ts \u22611.) Using (14.23), we find \u03c1ts = \u00b5ts when |Vs| > 1, and this\nidentity is still valid when |Vs| = 1, since in this case, (14.22) implies that \u00b5ts(x(s)) = 1.\nWe need to find constants \u03b1t and \u03b6st such that (14.14) and (14.15) are satisfied.\nBut (14.15) implies\n\u03b6ts =\nX\nxt,xs\n\u03c8st(x(s),x(t))\u03c1st(x(t))\u03c1ts(x(s))\nand (14.21) implies \u03b6ts = Zts.\nWe now consider (14.14), which requires\nmts(x(s)) = \u03b1s\n\u03b6st\nX\nx(t)\n\u03d5st(x(s),x(t))\u03d5t(x(t))\u03c1st(x(t)).\nIt is now easy to see that this identity to the power |Vs| \u22121 coincides with (14.22) as\nsoon as one takes \u03b1s = e.\n\u25a0\n14.4\nComputing the most likely configuration\nWe now address the problem of finding a configuration that maximizes \u03c0(x) (mode\ndetermination). This problem turns out to be very similar to the computation of\nmarginals, that we have considered so far, and we will obtain similar algorithms.\nAssume that G is undirected and acyclic and that \u03c0 can be written as\n\u03c0(x) = 1\nZ\nY\n{s,t}\u2208E\n\u03d5st(x(s),x(t))\nY\ns\u2208V\n\u03d5s(x(s)).\nMaximizing \u03c0(x) is equivalent to maximizing\nU(x) =\nY\n{s,t}\u2208E\n\u03d5st(x(s),x(t))\nY\ns\u2208V\n\u03d5s(x(s)).\n(14.24)\n14.4. COMPUTING THE MOST LIKELY CONFIGURATION\n349\nAssume that a root has been chosen in G, with the resulting edge orientation yielding\na tree \u02dcG = (V , \u02dcE) such that \u02dcG\u266d= G. We partially order the vertexes according to \u02dcG,\nwriting s \u2264t if there exists a path from s to t in \u02dcG (s is an ancestor of t). Let V +\ns\ncontain all t \u2208V with t \u2265s, and define\nUs(x(V +\ns )) =\nY\n{t,u}\u2208EV +\ns\n\u03d5tu(x(t),x(u))\nY\nt>s\n\u03d5t(x(t))\nand\nU\u2217\ns (x(s)) = max\nn\nUs(y(V +\ns )),y(s) = x(s)o\n.\n(14.25)\nSince we can write\nUs(x(V +\ns )) =\nY\nt\u2208s+\n\u03d5st(x(s),x(t))\u03d5t(x(t))Ut(x(V +\nt )),\n(14.26)\nwe have\nU\u2217\ns (x(s))\n=\nmax\nx(t),t\u2208s+\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\nY\nt\u2208s+\n\u03d5t(x(t))\u03d5st(x(s),x(t))U\u2217\nt (x(t))\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\n=\nY\nt\u2208s+\nmax\nxt\u2208Ft\n(\u03d5t(x(t))\u03d5st(x(s),x(t))U\u2217\nt (x(t))).\n(14.27)\nThis provides a method to compute U\u2217\ns (x(s)) for all s, starting with the leaves and\nprogressively updating the parents. (When s is a leaf, U\u2217\ns (x(s)) = 1, by definition.)\nOnce all U\u2217\ns (x(s)) have been computed, it is possible to obtain a configuration x\u2217\nthat maximizes \u03c0. This is because an optimal configuration must satisfy U\u2217\ns (x(s)\n\u2217) =\nUs(x(V +\ns )\n\u2217\n) for all s \u2208V , i.e., x(V +\ns \\{s})\n\u2217\nmust solve the maximization problem in (14.25).\nBut because of (14.26), we can separate this problem over the children of s and obtain\nthe fact that, it t \u2208s+,\nx(t)\n\u2217= argmax\nx(t)\n\u0012\n\u03d5t(x(t))\u03d5st(x(s)\n\u2217,x(t))U\u2217\nt (x(t))\n\u0013\n.\nThis procedure can be rewritten in a slightly different form using messages sim-\nilar to the belief propagation algorithm. It s \u2208t+, define\n\u00b5st(x(t)) = max\nxs\u2208Fs\n(\u03d5t(x(t))\u03d5ts(x(t),x(s))U\u2217\ns (x(s)))\nand\n\u03best(x(t)) = argmax\nx(s)\u2208Fs\n(\u03d5t(x(t))\u03d5ts(x(t),x(s))U\u2217\ns (x(s))).\n350\nCHAPTER 14. PROBABILISTIC INFERENCE FOR MRF\nUsing section 14.4, we get\n\u00b5st(x(t))\n=\nmax\nx(s)\u2208Fs\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\u03d5ts(x(t),x(s))\u03d5s(x(s))\nY\nu\u2208s+\n\u00b5us(x(s))\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8,\n\u03best(xt)\n=\nargmax\nx(s)\u2208Fs\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\u03d5ts(x(t),x(s))\u03d5s(x(s))\nY\nu\u2208s+\n\u00b5us(x(s))\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8.\nAn optimal configuration can now be computed using x(t)\n\u2217= \u03bets(x(s)\n\u2217), with s \u2208pa(t).\nThis resulting algorithm therefore first operates upwards on the tree (from leaves\nto root) to compute the \u00b5st\u2019s and \u03best\u2019s, then downwards to compute x\u2217. This is sum-\nmarized in the following algorithm.\nAlgorithm 14.5\nA most likely configuration for\n\u03c0(x) = 1\nZ\nY\n{s,t}\u2208E\n\u03d5st(xs,xt)\nY\ns\u2208V\n\u03d5s(xs).\ncan be computed after iterating the following updates, based on any acyclic orienta-\ntion of G:\n(1) Compute, from leaves to root:\n\u00b5st(x(t)) = max\nx(s)\u2208Fs\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\u03d5ts(x(t),x(s))\u03d5s(x(s))\nY\nu\u2208s+\n\u00b5us(x(s))\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\nand \u03best(x(t)) = argmax\nx(s)\u2208Fs\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\u03d5ts(x(t),x(s))\u03d5s(x(s))\nY\nu\u2208s+\n\u00b5us(x(s))\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8.\n(2) Compute, from root to leaves: x(t)\n\u2217= \u03bets(x(s)\n\u2217), with s = pa(t).\nSimilar to the computation of marginals, this algorithm can be rewritten in an\norientation-independent form. The main remark is that the value of \u00b5st(x(t)) does\nnot depend on the tree orientation, as long as it is chosen such that s \u2208t+, i.e., the\nedge {s,t} is oriented from t to s. This is because such a choice uniquely prescribes\nthe orientation of the edges of the descendants of s for any such tree, and \u00b5st only\ndepends on this structure. Since the same remark holds for \u03best, this provides a def-\ninition of these two quantities for any pair s,t such that {s,t} \u2208E. The updating rule\n14.5. GENERAL SUM-PROD AND MAX-PROD ALGORITHMS\n351\nnow becomes\n\u00b5st(x(t))\n=\nmax\nx(s)\u2208Fs\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\u03d5ts(x(t),x(s))\u03d5s(x(s))\nY\nu\u2208Vs\\{t}\n\u00b5us(x(s))\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8,\n(14.28)\n\u03best(x(t))\n=\nargmax\nx(s)\u2208Fs\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\u03d5ts(x(t),x(s))\u03d5s(x(s))\nY\nu\u2208Vs\\{t}\n\u00b5us(x(s))\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\n(14.29)\nwith x(t)\n\u2217\n= \u03bets(x(s)\n\u2217) for any pair s \u223ct. Like with the mts in the previous section,\nlooping over updating all \u00b5ts in any order will finally stabilize to their correct values,\nalthough, if an orientation is given, going from leaves to roots is obviously more\nefficient.\nThe previous analysis is not valid for loopy graphs but section 14.4 and sec-\ntion 14.4 provide well defined iterations when G is an arbitrary undirected graph,\nand can therefore be used as such, without any guaranteed behavior.\n14.5\nGeneral sum-prod and max-prod algorithms\n14.5.1\nFactor graphs\nThe expressions we obtained for message updating with belief propagation and with\nmode determination respectively took the form\nmts(x(s)) \u2190\nX\nx(t)\u2208Ft\n\u03d5st(x(s),x(t))\u03d5t(x(t))\nY\nt\u2032\u2208Vt\\{s}\nmt\u2032t(x(t))\nand\n\u00b5ts(x(s)) \u2190max\nx(t)\u2208Ft\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\u03d5st(x(s),x(t))\u03d5t(x(t))\nY\nt\u2032\u2208Vt\\{s}\n\u00b5t\u2032t(x(t))\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8.\nThey first one is often referred to as the \u201csum-prod\u201d update rule, and the second\nas the \u201cmax-prod\u201d. In our construction, the sum-prod algorithm provided us with a\nmethod computing\n\u03c3s(x(s)) =\nX\ny(V \\{s})\nU(x(s) \u2227y(V \\{s}))\nwith\nU(x) =\nY\ns\n\u03d5s(x(s))\nY\n{s,t}\u2208E\n\u03d5st(x(s),x(t)).\n352\nCHAPTER 14. PROBABILISTIC INFERENCE FOR MRF\nIndeed, we have, according to (14.11)\n\u03c3s(x(s)) = \u03d5s(x(s))\nY\nt\u2208Vs\nmts(x(s)).\nSimilarly, the max-prod algorithm computes\n\u03c1s(x(s)) = max\nyV \\{s} U(x(s) \u2227y(V \\{s}))\nvia the relation\n\u03c1s(x(s)) = \u03d5s(x(s))\nY\nt\u2208Vs\n\u00b5ts(x(s)).\nWe now discuss generalizations of these algorithms to situations in which the\nfunction U does not decompose as a product of bivariate functions. More precisely,\nlet S be a subset of P(V ), and assume the decomposition\nU(x) =\nY\nC\u2282S\n\u03d5C(xC).\nThe previous algorithms can be generalized using the concept of factor graphs asso-\nciated with the decomposition. The vertexes of this graph are either indexes s \u2208V or\nsets C \u2208S, and the only edges link indexes and sets that contain them. The formal\ndefinition is as follows.\nDefinition 14.11 Let V be a finite set of indexes and S a subset of P(V ). The factor\ngraph associated to V and S is the graph G = (V \u222aS,E), E being constituted of all pairs\n{s,C} with C \u2208S and s \u2208C.\nWe assign the variable x(s) to a vertex s \u2208V of the factor graph, and the function \u03d5C\nto C \u2208S. With this in mind, the sum-prod and max-prod algorithms are extended\nto factor graphs as follows.\nDefinition 14.12 Let G = (V \u222aS,E) be a factor graph, with associated functions \u03d5C(xC).\nThe sum-prod algorithm on G updates messages msC(xs) and mCs(xs) according to the\nrules\n\uf8f1\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f2\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f3\nmsC(x(s)) \u2190\nY\n\u02dcC,s\u2208\u02dcC, \u02dcC,C\nm \u02dcCs(x(s))\nmCs(x(s)) \u2190\nX\nyC:y(s)=x(s)\n\u03d5C(y(C))\nY\nt\u2208C\\{s}\nmtC(y(t))\n(14.30)\n14.5. GENERAL SUM-PROD AND MAX-PROD ALGORITHMS\n353\nSimilarly, the max-prod algorithm iterates\n\uf8f1\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f2\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f3\n\u00b5sC(x(s)) \u2190\nY\n\u02dcC,s\u2208\u02dcC, \u02dcC,C\n\u00b5 \u02dcCs(x(s))\n\u00b5Cs(x(s)) \u2190\nmax\ny(C):y(s)=x(s) \u03d5C(y(C))\nY\nt\u2208C\\{s}\n\u00b5tC(y(t))\n(14.31)\nThese algorithms reduce to the original ones when only single vertex and pair in-\nteractions exist. Let us check this with sum-prod. In this case, the set S contains\nall singletons C = {s}, with associated function \u03d5s, and all edges {s,t} with associated\nfunction \u03d5st. We have links between s and {s} and s and {s,t} \u2208E. For singletons, we\nhave\nms{s}(x(s)) \u2190\nY\nt\u223cs\nms{s,t}(x(s)) and m{s}s(x(s)) \u2190\u03d5s(x(s)).\nFor pairs,\nms{s,t}(x(s)) \u2190\u03d5s(x(s))\nY\n\u02dct\u2208Vs\\{t}\nm{s,\u02dct}s(x(s))\nand\nm{s,t}s(x(s)) \u2190\nX\ny(t)\n\u03d5st(x(s),y(t))mt{s,t}(y(t))\nand, combining the last two assignments, it becomes clear that we retrieve the initial\nalgorithm with m{s,t}s taking the role of what we previously denoted mts.\nThe important question, obviously, is whether the algorithms converge. The fol-\nlowing result shows that this is true when the factor graph is acyclic.\nProposition 14.13 Let G = (V \u222aS,E) be a factor graph with associated functions \u03d5C.\nAssume that G is acyclic. Then the sum-prod and max-prod algorithms converge in finite\ntime.\nAfter convergence, we have \u03c3s(x(s)) = Q\nC,s\u2208C mCs(x(s)) and \u03c1s(x(s)) = Q\nC,s\u2208C \u00b5Cs(x(s)).\nProof Let us assume that G is connected, which is without loss of generality, since\nthe following argument can be applied to each component of G separately. Since G\nis acyclic, we can arbitrarily select one of its vertexes as a root to form a tree. This\nbeing done, we can see that the messages going upward in the tree (from children\nto parent) progressively stabilize, starting with leaves. Leaves in the factor graph\nindeed are either singletons, C = {s}, or vertexes s \u2208V that belong to only one set\nC \u2208S. In the first case, the algorithm imposes (taking, for example, the sum-prod\ncase) m{s}s(x(s)) = \u03d5s(x(s)), and in the second case msC(x(s)) = 1. So the messages sent\nupward by the leaves are set at the first step. Since the messages going from a child\nto its parents only depend on the messages that it received from its other neighbors\n354\nCHAPTER 14. PROBABILISTIC INFERENCE FOR MRF\nin the acyclic graph, which are its children in the tree, it is clear that all upward\nmessages progressively stabilize until the root is reached. Once this is done, mes-\nsages propagate downward from each parent to its children. This stabilizes as soon\nas all incoming messages to the parent are stabilized, since outgoing messages only\ndepend on those. At the end of the upward phase, this is true for the root, which\ncan then send its stable message to its children. These children now have all their\nincoming messages and can now send their messages to their own children and so\non down to the leaves.\nWe now consider the second statement, proceeding by induction, assuming that\nthe result is true for any smaller graph than the one considered. Let s0 be the selected\nroot, and consider all vertexes s , s0 such that there exists Cs \u2208S such that s0 and s\nboth belong to Cs. Given s, there cannot be more than one such Cs since this would\ncreate a loop in the graph. For each such s, consider the part Gs of G containing all\ndescendants of s. Let Vs be the set of vertexes among the descendants of s and Cs the\nset of C\u2019s below s. Define\nUs(x(Vs)) =\nY\nC\u2208Cs\n\u03d5C(x(C)).\nSince the upward phase of the algorithm does not depend on the ancestors of s,\nthe messages incoming to s for the sum-prod algorithm restricted to Gs are the same\nas with the general algorithm, so that, using the induction hypothesis\nX\ny(Vs),y(s)=x(s)\nUs(y(Vs)) =\nY\nC\u2208Cs,s\u2208C\nmCs(x(s)) = msCs(x(s)).\nNow let C1,...,Cn list all the sets in C that contain s0, which must be non-intersecting\n(excepted at {s0}), again not to create loops. Write\nC1 \u222a\u00b7\u00b7\u00b7 \u222aCn = {s0,s1,...,sq}.\nThen, we have\nU(x) =\nn\nY\nj=1\n\u03d5Cj(x(Cj))\nq\nY\ni=1\nUsi(x(Vsi ))\n14.5. GENERAL SUM-PROD AND MAX-PROD ALGORITHMS\n355\nand letting S = Sn\nj=1 Cj \\ {s0},\n\u03c3s0(x(s0))\n=\nX\ny(V ):y(s0)=x(s0)\nn\nY\nj=1\n\u03d5Cj(y(Cj))\nq\nY\ni=1\nUsi(y(Vsi ))\n=\nX\ny()S:y(s0)=x(s0)\nn\nY\nj=1\n\u03d5Cj(y(Cj))\nq\nY\ni=1\nmsiCsi (y(si))\n=\nn\nY\nj=1\nX\ny()Cj:y(s0)=x(s0)\n\u03d5Cj(y(Cj))\nY\ns\u2208Cj\\{s0}\nmsCs(y(s))\n=\nn\nY\nj=1\nmCjs0(x(s0))\nwhich proves the required result (note that, when factorizing the sum, we have used\nthe fact that the sets Cj \\ {s0} are non intersecting). An almost identical argument\nholds for the max-prod algorithm.\n\u25a0\nRemark 14.14 Note that these algorithms are not always feasible. For example, it is\nalways possible to represent a function U on F (V ) with the trivial factor graph in\nwhich S = {V } and E contains all {s,V },s \u2208V (using \u03d5V = U), but computing mV s\nis identical to directly computing \u03c3s with a sum over all configurations on V \\ {s}\nwhich grows exponentially. In fact, the complexity of the sum-prod and max-prod\nalgorithms is exponential in the size of the largest C in S which should therefore\nremain small.\n\u2666\nRemark 14.15 It is not always possible to decompose a function so that the resulting\nfactor graph is acyclic with small degree (maximum number of edges per vertex).\nSum-prod and max-prod can still be used with loopy networks, sometimes with\nexcellent results, but without theoretical support.\n\u2666\nRemark 14.16 One can sometimes transform a given factor graph into an acyclic\none by grouping vertexes. Assume that the set S \u2282P(V ) is given. We will say that\na partition \u2206= (D1,...Dk) of V is S-admissible if, for any C \u2208S and any j \u2208{1,...,k},\none has either Dj \u2229C = \u2205or Dj \u2282C.\nIf \u2206is S-admissible, one can define a new factor graph \u02dcG as follows. We first let\n\u02dcV = {1,...,k}. To define \u02dcS \u2282P( \u02dcV ) assign to each C \u2208S the set JC of indexes j such\nthat Dj \u2282C. From the admissibility assumption,\nC =\n[\nj\u2208JC\nDj,\n(14.32)\n356\nCHAPTER 14. PROBABILISTIC INFERENCE FOR MRF\nso that C 7\u2192JC is one-to-one. Let \u02dcS = {JC,C \u2208S}. Group variables using \u02dcx(k) = x(Dk),\nso that \u02dcFk = F (Dk). Define \u02dc\u03a6 = ( \u02dc\u03d5 \u02dcC, \u02dcC \u2208\u02dcS) by \u02dc\u03d5 \u02dcC = \u03d5C where C is given by (14.32).\nIn other terms, one groups variables (x(s),s \u2208V ) into clusters, to create a simpler\nfactor graph, which may be acyclic even if the original one was not. For example, if\nV = {a,b,c,d}, S = {A,B} with A = {a,b,c} and B = {b,c,d}, then (A,c,B,b) is a cycle in\nthe associated factor graph. If, however, one takes D1 = {a}, D2 = {b,c} and D3 = {d},\nthen (D1,D2,D3) is S-admissible and the associated factor graph is acyclic. In fact, in\nsuch a case, the resulting factor graph, considered as a graph with vertexes given by\nsubsets of V , is a special case of a junction tree, which is defined in the next section.\u2666\n14.5.2\nJunction trees\nDefinition 14.17 Let V be a finite set. A junction tree on V is an undirected acyclic\ngraph G = (S,E) where S \u2282P(V ) is a family of subsets of V that satisfy the following\nproperty, called the running intersection constraint: if C,C\u2032 \u2208S and s \u2208C \u2229C\u2032, then all\nsets C\u2032\u2032 in the (unique) path connecting C and C\u2032 in G must also contain s.\nRemark 14.18 Let us check that the clustered factor graph \u02dcG defined in remark 14.16\nis equivalent to a junction tree when acyclic.\nUsing the same notation, let \u02c6S =\n{D1,...,Dk} \u222aS, removing if needed sets C \u2208S that coincide with one of the Dj\u2019s.\nPlace an edge between Dj and C if and only if Dj \u2282C.\nLet (C1,Di1,...,Din\u22121,Cn) be a path in that graph. Assume that s \u2208C1 \u2229C2. Let Din\nbe the unique Dj that contains s. It is such that from the the admissibility assump-\ntion, Din \u2282C1 and Din \u2282Cn, which implies that (C1,Di1,...,Cn,Din,C1) is a path in \u02dcG.\nSince \u02dcG is acyclic, this path must be a union of folded paths. But it is easy to see that\nany folded path satisfies the running intersection constraint. (Note that there was\nno loss of generality in assuming that the path started and ended with a \u201cC\u201d, since\nany \u201cD\u201d must be contained in the C that follows or precedes it.)\n\u2666\nWe now consider a probability distribution written in the form\n\u03c0(x) = 1\nZ\nY\nC\u2208S\n\u03d5C(x(C))\nand we make the assumption that S can be organized as a junction tree.\nBelief propagation can be extended to junction trees. Fixing a root C0 \u2208S, we\nfirst choose an orientation on G, which induces as usual a partial order on S. For\nC \u2208S, define S+\nC as the set of all B \u2208S such that B > C. Define also\nV +\nC =\n[\nB\u2208S+\nC\nB.\n14.5. GENERAL SUM-PROD AND MAX-PROD ALGORITHMS\n357\nWe want to compute sums\n\u03c3C(x(C)) =\nX\ny(V \\C)\nU(x(C) \u2227y(V \\C)),\nwhere U(x) = Q\nC\u2208S \u03d5C(x(C)). We have\n\u03c3C(x(C)) =\nX\ny(V \\C)\n\u03d5C(x(C))\nY\nB\u2208S\\{C}\n\u03d5B(x(B\u2229C) \u2227y(B\\C)).\nDefine\n\u03c3+\nC(x(C)) =\nX\ny(V +\nC \\C)\nY\nB>C\n\u03d5B(x(B\u2229C) \u2227y(B\\C)).\nNote that we have \u03c3C0 = \u03d5C0\u03c3+\nC0 at the root. We have the recursion formula\n\u03c3+\nC(x(C))\n=\nX\ny(V +\nC \\C)\nY\nC\u2192B\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\u03d5B(x(B\u2229C) \u2227y(B\\C))\nY\nB\u2032>B\n\u03d5B\u2032(x(B\u2032\u2229C) \u2227y(B\u2032\\C))\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\n=\nY\nC\u2192B\nX\ny(B\u222aV +\nB \\C)\n\u03d5B(x(B\u2229C) \u2227y(B\\C))\nY\nB\u2032>B\n\u03d5B\u2032(x(B\u2032\u2229C) \u2227y(B\u2032\\C))\n=\nY\nC\u2192B\nX\ny(B\\C)\n\u03d5B(x(B\u2229C) \u2227y(B\\C))\u03c3+\nB (x(B\u2229C) \u2227y(B\\C)).\nThe inversion between the sum and product in the second equation above was pos-\nsible because the sets B \u222aV +\nB \\ C, C \u2192B are disjoint. Indeed, if there existed B,B\u2032\nsuch that C \u2192B and C \u2192B\u2032, and descendants C\u2032 of B\u2032 and C\u2032\u2032 of B\u2032\u2032 with a non-\nempty intersection, then this intersection would have to be included in every set in\nthe (non-oriented) path connecting C\u2032 and C\u2032\u2032 in G. Since this path contains C, the\nintersection must also be included in C, so that the sets B \u222aV +\nB \\ C, with C \u2192B are\ndisjoint.\nIntroduce messages\nm+\nB(x(C)) =\nX\ny(B\\C)\n\u03d5B(x(B\u2229C) \u2227y(B\\C))\u03c3+\nB (x(B\u2229C) \u2227y(B\\C))\nwhere C is the parent of B. Then\nm+\nB(x(C)) =\nX\ny(B\\C)\n\u03d5B(x(B\u2229C) \u2227y(B\\C))\nY\nB\u2192B\u2032\nm+\nB\u2032(x(B\u2229C) \u2227y(B\\C))\nwith\n\u03c3+\nC(x(C)) =\nY\nC\u2192B\nm+\nB(x(C))\n358\nCHAPTER 14. PROBABILISTIC INFERENCE FOR MRF\nwhich provides \u03c3C at the root. Reinterpreting this discussion in terms of the undi-\nrected graph, we are led to introducing messages mBC(x(C)) for B \u223cC in G, with the\nmessage-passing rule\nmBC(x(C)) =\nX\ny(B\\C)\n\u03d5B(x(B\u2229C) \u2227y(B\\C))\nY\nB\u2032\u223cB,B\u2032,C\nmB\u2032B(x(B\u2229C) \u2227y(B\\C)).\n(14.33)\nMessages progressively stabilize when applied in G, and at convergence, we have\n\u03c3C(x(C)) = \u03d5C(x(C))\nY\nB\u223cC\nmBC(x(C)).\n(14.34)\nNote that the complexity of the junction tree algorithm is exponential in the car-\ndinality of the largest C \u2208S. This algorithm will therefore be unfeasible if S contains\nsets that are too large.\n14.6\nBuilding junction trees\nThere is more than one family of set interactions with respect to which a given prob-\nability \u03c0 can be decomposed (notice that, unlike in the Hammersley-Clifford The-\norem, we do not assume that the interactions are normalized), and not all of them\ncan be organized as a junction tree. One can however extend any given family into a\nnew one on which one can build a junction tree.\nDefinition 14.19 Let V be a set of vertexes, and S0 \u2282P(V ). We say that a set S \u2282P(V )\nis an extension of S0 if, for any C0 \u2208S0, there exists a C \u2208S such that C0 \u2282C.\nA tree G = (S,E) is a junction-tree extension of S0 if S is an extension of S0 and G is\na junction tree.\nIf \u03a60 = (\u03d50\nC,C \u2208S0) is a consistent family of set interactions, and S is an extension\nof S0, one can build a new family, \u03a6 = (\u03d5C,C \u2208S), of set interactions which yields\nthe same probability distribution, i.e., such that, for all x \u2208F (V ),\nY\nC\u2208S\n\u03d5C(x(C)) \u221d\nY\nC0\u2208S0\n\u03d50\nC0(x(C0)).\nFor this, it suffices to build a mapping say T : S0 \u2192S such that C0 \u2282T(C0) for\nall C0 \u2208S0, which is always possible since S is an extension of S0 (for example,\narbitrarily order the elements of S and let T(S0) be the first element of S, according\nto this order, that contains C0). One can then define\n\u03d5C(x(C)) =\nY\nC0:T (C0)=C\n\u03d50\nC0(x(C0)).\n14.6. BUILDING JUNCTION TREES\n359\nGiven \u03a60, our goal is to design a junction-tree extension which is as feasible\nas possible. So, we are not interested by the trivial extension G = (V ,\u2205), since the\nresulting junction-tree algorithm is unfeasible as soon as V is large. Theorem 14.24\nin the next section will be the first step in the design of an algorithm that computes\njunction trees on a given graph.\n14.6.1\nTriangulated graphs\nDefinition 14.20 Let G = (V ,E) be an undirected graph. Let (s1,s2,...,sn) be a path in\nG. One says that this path has a chord at sj, with j \u2208{2,...,n} , if sj\u22121 \u223csj+1, and we will\nrefer to (sj\u22121,sj,sj+1) as a chordal triangle. A path in G is achordal if it has no chord.\nOne says that G is triangulated (or chordal) if it has no achordal loop.\nDefinition 14.21 The graph G is decomposable if it satisfies the following recursive con-\ndition: it is either complete, or there exists disjoint subsets (A,B,C) of V such that\n\u2022 V = A \u222aB \u222aC,\n\u2022 A and B are not empty,\n\u2022 C is clique in G, C separates A and B,\n\u2022 the restricted graphs, GA\u222aC and GB\u222aC are decomposable.\nThese definitions are in fact equivalent, as stated in the following proposition.\nProposition 14.22 An undirected graph is triangulated if and only if it is decomposable\nProof To prove the \u201cif\u201d part, we proceed by induction on n = |V |. Note that every\ngraph for n \u22643 is both decomposable and triangulated (we leave the verification\nto the reader). Assume that the statement \u201cdecomposable \u21d2triangulated\u201d holds\nfor graphs with less than n vertexes, and take G with n vertexes. Assume that G is\ndecomposable. If it is complete, it is obviously triangulated. Otherwise, there exists\nA,B,C such that V = A \u222aB \u222aC, with A and B non-empty such that GA\u222aC and GB\u222aC\nare decomposable, hence triangulated from the induction hypothesis, and such that\nC is a clique which separates A and B. Assume that \u03b3 is an achordal loop in G. Since\nit cannot be included in A \u222aC or B \u222aC, \u03b3 must go from A to B and back, which\nimplies that it passes at least twice in C. Since C is complete, the original loop can\nbe shortcut to form subloops in A \u222aC and B \u222aC. If one of (or both) these loops has\ncardinality 3, this would provide \u03b3 with a chord, which contradicts the assumption.\nOtherwise, the following lemma also provides a contradiction, since one of the two\nchords that it implies must also be a chord in the original \u03b3.\nLemma 14.23 Let (s1,...,sn,sn+1 = s1) be a loop in a triangulated graph, with n \u22654.\nThen the path has a chord at two non-contiguous vertexes at least.\n360\nCHAPTER 14. PROBABILISTIC INFERENCE FOR MRF\nTo prove the lemma, assume the contrary and let (s1,...,sn,sn+1 = s1) be a loop that\ndoes not satisfy the condition, with n as small as possible. If n > 4, the loop must\nhave a chord, say at sj, and one can remove sj from the loop to still obtain a smaller\nloop that must satisfy the condition in the lemma, since n was as small as possible.\nOne of the two chords must be at a vertex other than the two neighbors of sj, and\nthus provide a second chord in the original loop, which is a contradiction. Thus\nn = 4, but G being triangulated implies that this 4-point loop has a diagonal, so that\nthe condition in the lemma also holds, which provides a contradiction.\nFor the \u201conly if\u201d part of proposition 14.22, assume that G is triangulated. We\nprove that the graph is decomposable by induction on |G|. The induction will work if\nwe can show that, if G is triangulated, it is either complete or there exists a clique in\nG such that V \\C is disconnected, i.e., there exist two elements a,b \u2208V \\C which are\nrelated by no path in V \\C. Indeed, we will then be able to decompose V = A\u222aB\u222aC,\nwhere A and B are unions of (distinct) connected components of V \\ C. Take, for\nexample, A to be the set of vertexes connected to A in G \\ C, and B = V \\ (A \u222aC),\nwhich is not empty since it contains b. Note that restricted graphs from triangulated\ngraphs are triangulated too.\nSo, assume that G is triangulated, and not complete. Let C be a subset of V that\nsatisfies the property that V \\ C is disconnected, and take C minimal, so that V \\ C\u2032\nis connected for any C\u2032 \u2282C, C\u2032 , C. We want to show that C is a clique, so take s and\nt in C and assume that they are not neighbors to reach a contradiction.\nLet A and B be two connected components of V \\ C. For any a \u2208A, b \u2208B, and\ns,t \u2208C, we know that there exists a path between a and b in V \\ C \u222a{s} and another\none in V \\C\u222a{t}, the first one passing by s (because it would otherwise connect a and\nb in V \\ C) and the second one passing by t. Any point before s (or t) in these paths\nmust belong to A, and any point after them must belong to B. Concatenating these\ntwo paths, and removing multiple points if needed, we obtain a loop passing in A,\nthen by s, then in B, then by t. We can recursively remove all points at which these\npaths have a chord. We can also notice that we cannot remove s nor t in this process,\nsince this would imply an edge between A and B, and that we must leave at least one\nelement in A and one in B because removing the last one would require s \u223ct. So, at\nthe end, we obtain an achordal loop with at least four points, which contradicts the\nfact that G is triangulated.\n\u25a0\nWe can now characterize graphs that admit junction trees over the set of their\nmaximal cliques.\nTheorem 14.24 Let G = (V ,E) be an undirected graph, and C\u2217\nG be the set of all maximum\ncliques in G. The following two properties are equivalent.\n(i) There exists a junction tree over C\u2217\nG.\n14.6. BUILDING JUNCTION TREES\n361\n(ii) G is triangulated/decomposable.\nProof The proof works by induction on the number of maximal cliques, |C\u2217\nG|. If G\nhas only one maximal clique, then G is complete, because any point not included\nin this clique will have to be included in another maximal clique, which leads to a\ncontradiction. So G is decomposable, and, since any single node obviously provides\na junction tree, (i) is true also.\nNow, fix G and assume that the theorem is true for any graph with fewer maximal\ncliques. First assume that C\u2217\nG has a junction tree, T . Let C1 be a leaf in T , connected,\nsay, to C2, and let T2 be T restricted to C2 = C\u2217\nG\\{C1}. Let V2 be the unions of maximal\ncliques from nodes in T2. A maximal clique C in GV2 is a clique in GV and therefore\nincluded in some maximal clique C\u2032 \u2208CV . If C\u2032 \u2208C2, then C\u2032 is also a clique in GV2,\nand for C to be maximal, we need C = C\u2032. If C\u2032 = C1, we note that we must also have\nC =\n[\n\u02dcC\u2208C2\nC \u2229\u02dcC\nand whenever C\u2229\u02dcC is not empty, this set must be included in any node in the path in\nT that links \u02dcC to C1. Since this path contains C2, we have C \u2229\u02dcC \u2282C2 so that C \u2282C2,\nbut, since C is maximal, this would imply that C = C2 = C1 which is impossible.\nThis shows that C\u2217\nG2 = C2. This also shows that T2 is a junction tree over C2. So,\nby the induction hypothesis, GV2 is decomposable. If s \u2208V2 \u2229C1, then s also belongs\nto some clique C\u2032 \u2208C2, and therefore belongs to any clique in the path between\nC\u2032 and C1, which includes C2. So s \u2208C1 \u2229C2 and C1 \u2229V2 = C1 \u2229C2. So, letting\nA = C1 \\ (C1 \u2229C2), B = V1 \\ (C1 \u2229C2), S = C1 \u2229C2, we know that GA\u222aS and GB\u222aS are\ndecomposable (the first one being complete), and that S is a clique. To show that G\nis decomposable, it remains to show that S separates A from B.\nIf a path connects A to B in G, it must contain an edge, say {s,t}, with s \u2208V \\S and\nt \u2208S; {s,t} must be included in a maximal clique in G. If this clique is C1, we have\ns \u2208C1 \u2229V2 = S. The same argument shows that this is the only possibility, because,\nif {s,t} is included in some maximal clique in C2, then we would find t \u2208C1 \u2229C2. So\nS separates A and B in G.\nLet us now prove the converse statement, and assume that G is decomposable.\nIf G is complete, it has only one maximal clique and we are done. Otherwise, there\nexists a partition V = A \u222aB \u222aS such that GA\u222aS and GB\u222aS are decomposable, A and B\nseparated by S which is complete. Let C\u2217\nA be the maximal cliques in GA\u222aS and C\u2217\nB the\nmaximal cliques in GB\u222aS. By hypothesis, there exist junction trees TA and TB over C\u2217\nA\nand C\u2217\nB.\nLet C be a maximal clique in GA\u222aS. Assume that C intersect A; C can be extended\nto a maximal clique, C\u2032, in G, but C\u2032 cannot intersect B (since this would imply a\n362\nCHAPTER 14. PROBABILISTIC INFERENCE FOR MRF\ndirect edge between A and B) and is therefore included in A \u222aS, so that C = C\u2032.\nSimilarly, all maximal cliques in GB\u222aS that intersect B also are maximal cliques in G.\nThe clique S is included in some maximal clique S\u2217\nA \u2208C\u2217\nA. From the previous\ndiscussion, we have either S\u2217\nA = S or S\u2217\nA \u2208C\u2217\nG. Similarly, S can be extended to a\nmaximal clique S\u2217\nB \u2208C\u2217\nB, with S\u2217\nB = S or S\u2217\nB \u2208C\u2217\nG. Notice also that at least one of S\u2217\nA\nor S\u2217\nB must be a maximal clique in G: indeed, assume that both sets are equal to S,\nwhich, as a clique, can extended to a maximal clique S\u2217in G; S\u2217must be included\neither in A \u222aS or in B \u222aS, and therefore be a maximal clique in the corresponding\ngraph which yields S\u2217= S. Reversing the notation if needed, we will assume that\nS\u2217\nA \u2208C\u2217\nG.\nAll elements of C\u2217\nG must belong either to C\u2217\nA or C\u2217\nB since any maximal clique, say C,\nin G must be included in either A \u222aS or B \u222aS, and therefore also provide a maximal\nclique in the related graph. So the nodes in TA and TB enumerate all maximal cliques\nin G, and we can build a tree T over C\u2217\nG by identifying S\u2217\nA and S\u2217\nB to S\u2217and merging\nthe two trees at this node. To conclude our proof, it only remains to show that the\nrunning intersection property is satisfied. So consider two nodes C,C\u2032 in T and\ntake s \u2208C \u2229C\u2032. If the path between these nodes remain in C\u2217\nA, or in C\u2217\nB, then s will\nbelong to any set along that path, since the running intersection is true on TA and\nTB. Otherwise, we must have s \u2208S, and the path must contain S\u2217to switch trees,\nand s must still belong to any clique in the path (applying the running intersection\nproperty between the beginning of the path and S\u2217, and between S\u2217and the end of\nthe path).\n\u25a0\nThis theorem delineates a strategy in order to build a junction tree that is adapted\nto a given family of local interactions \u03a6 = (\u03d5C,C \u2208C). Letting G be the graph in-\nduced by these interactions, i.e., s \u223cG t if and only if there exists C \u2208C such that\n{s,t} \u2282C, the method proceeds as follows.\n(JT1) Extend G by adding edges to obtain a triangulated graph G\u2217.\n(JT2) Compute the set C\u2217of maximal cliques in G\u2217, which therefore extend C.\n(JT3) Build a junction tree over C\u2217.\n(JT4) Assign interaction \u03d5C to a clique C\u2217\u2208C\u2217such that C \u2282C\u2217.\n(JT5) Run the junction-tree belief propagation algorithm to compute the marginal\nof \u03c0 (associated to \u03a6) over each set C\u2217\u2208C\u2217.\nSteps (JT4) and (JT5) have already been discussed, and we now explain how the first\nthree steps can be implemented.\n14.6. BUILDING JUNCTION TREES\n363\n14.6.2\nBuilding triangulated graphs\nFirst consider step (JT1). To triangulate a graph G = (V ,E), it suffices to order its\nvertexes so that V = {s1,...,sn}, and then run the following algorithm.\nAlgorithm 14.6 (Graph triangulation)\nInitialize the algorithm with k = n and Ek = E. Given Ek, determine Ek\u22121 as follows:\n\u2022 Add an edge to any pair of neighbors of sk (unless, of course, they are already\nlinked).\n\u2022 Let Ek\u22121 be the new set of edges.\nThen the graph G\u2217= (V ,E0) is triangulated. Indeed taking any achordal loop,\nand selecting the vertex with highest index in the loop, say sk, brings a contradiction,\nsince the neighbors of sk have been linked when building Ek\u22121.\nHowever, the quality of the triangulation, which can be measured by the number\nof added edges, or by the size of the maximal cliques, highly depends on the way ver-\ntexes have been numbered. Take the simple example of the linear graph with three\nvertexes A \u223cB \u223cC. If the point of highest index is B, then the previous algorithm\nwill return the three-point loop A \u223cB \u223cC \u223cA. Any other ordering will leave the\nlinear graph, which is already triangulated, invariant.\nSo, one must be careful about the order with which nodes will be processed.\nFinding an optimal ordering for a given global cost is an NP-complete problem.\nHowever, a very simple modification of the previous algorithm, which starts with\nsn having the minimal number of neighbors, and at each step defines sk to be the\none with fewest neighbors that haven\u2019t been visited yet, provides an efficient way\nfor building triangulations. (It has the merit of leaving G invariant if it is a tree,\nfor example). Another criterion may be preferred to the number of neighbors (for\nexample, the number of new edges that would be needed if s is added).\nIf G is triangulated, there exists an ordering of V such that the algorithm above\nleaves G invariant. We now proceed to a proof of this statement and also show that\nsuch an ordering can be computed using an algorithm called maximum cardinality\nsearch, which, in addition, allows one to decide whether a graph is triangulated. We\nstart with a definition that formalizes the sequence of operations in the triangulation\nalgorithm.\nDefinition 14.25 Let G = (V ,E) be an undirected graph. A node elimination consists in\nselecting a vertex s \u2208V and building the graph G(s) = (V (s),E(s)) with V (s) = V \\ {s}, and\nE(s) containing all pairs {t,t\u2032} \u2282V (s) such that either {t,t\u2032} \u2208E or {t,t\u2032} \u2282Vs.\n364\nCHAPTER 14. PROBABILISTIC INFERENCE FOR MRF\nG(s) is called the s-elimination graph of G. The set of added edges, namely E(s)\\(E\u2229E(s))\nis called the deficiency set of s and denoted D(s) (or DG(s)).\nSo, the triangulation algorithm implements a sequence of node eliminations, suc-\ncessively applied to sn,sn\u22121, etc. One says that such an elimination process is perfect\nif, for all k = 1,...,n, the deficiency set of sk in the graph obtained after elimination\nof sn,...,sk+1 is empty (so that no edge is added during the process). We will also say\nthat (s1,...,sn) provides a perfect ordering for G.\nTheorem 14.26 An undirected graph G = (V ,E) admits a perfect ordering if and only if\nit is triangulated.\nProof The \u201conly if\u201d part is obvious, since, the triangulation algorithm following\na perfect ordering does not add any edge to G, which must therefore have been\ntriangulated to start with.\nWe now proceed to the \u201cif\u201d part. For this it suffices to prove that for any trian-\ngulated graph, there exists a vertex s such that DG(s) = \u2205. One can then easily prove\nthe result by induction, since, after removing this s, the remaining graph G(s) is still\ntriangulated and would admit (by induction) a perfect ordering that completes this\nfirst step.\nTo prove that such an s exists, we take a decomposition V = A \u222aS \u222aB, in which\nS is complete and separates A and B, such that |A \u222aS| is minimal (or |B| maximal).\nWe claim that A \u222aS must be complete. Otherwise, since A \u222aS is still triangulated,\nThere exists a similar decomposition A \u222aS = A\u2032 \u222aS\u2032 \u222aB\u2032. One cannot have S \u2229A\u2032\nand S \u2229B\u2032 non empty simultaneously, since this would imply a direct edge from A\u2032\nto B\u2032 (S is complete). Say that S \u2229A\u2032 = \u2205, so that A\u2032 \u2282A. Then the decomposition\nV = A\u2032 \u222aS\u2032 \u222a(B\u2032 \u222aB) is such that S\u2032 separates A\u2032 from B \u222aB\u2032. Indeed, a path from A\u2032\nto b \u2208B \u222aB\u2032 must pass in S\u2032 if b \u2208B\u2032, and, if b \u2208B, it must pass in S (since it links\nA and B). But S \u2282S\u2032 \u222aB\u2032 so that the path must intersect S\u2032. We therefore obtain\na decomposition that enlarges B, which is a contradiction and shows that A \u222aS is\ncomplete. Given this, any element s \u2208A can only have neighbors in A \u222aS and is\ntherefore such that DG(s) = \u2205, which concludes the proof.\n\u25a0\nIf a graph is triangulated, there is more than one perfect ordering of its vertexes.\nOne of these orderings is provided the maximum cardinality search algorithm, which\nalso allows one to decide whether the graph is triangulated. We start with a defini-\ntion/notation.\nDefinition 14.27 If G = (V ,E) is an undirected graph, with |V | = n, any ordering V =\n(s1,...,sn) can be identified with the bijection \u03b1 : V \u2192{1,...,n} defined by \u03b1(sk) = k. In\nother terms, \u03b1(s) is the rank of s in the ordering. We will refer to \u03b1 as an ordering, too.\n14.6. BUILDING JUNCTION TREES\n365\nGiven an ordering \u03b1, we define incremental neighborhoods V \u03b1,k\ns\n, for s \u2208V and k =\n1,...,n to be the intersections of Vs with the sets \u03b1\u22121({1,...,k}), i.e.,\nV \u03b1,k\ns\n= {t \u2208V ,t \u223cs,\u03b1(t) \u2264k}.\nOne says that \u03b1 satisfies the maximum cardinality property if, for all k = {2,...,n}\n|V \u03b1,k\u22121\nsk\n| = max\n\u03b1(s)\u2265k |V \u03b1,k\u22121\ns\n|.\n(14.35)\nwhere sk = \u03b1\u22121(k).\nGiven this, we have the proposition:\nProposition 14.28 If G = (V ,E) is triangulated, then any ordering that satisfies the max-\nimum cardinality property is perfect.\nEquation (14.35) immediately provides an algorithm that constructs an order-\ning satisfying the maximum cardinality property given a graph G. From proposi-\ntion 14.28, we see that, if for some k, the largest set V \u03b1,k\u22121\nsk\nis not a clique, then G is\nnot triangulated. We now proceed to the proof of this proposition.\nProof Let G be triangulated, and assume that \u03b1 is an ordering that satisfies (14.35).\nAssume that \u03b1 is not proper in order to reach a contradiction.\nLet k be the first index for which V \u03b1,k\u22121\nsk\nis not a clique, so that sk has two neigh-\nbors, say t and u, such that \u03b1(t) < k, \u03b1(u) < k and t \u2241u. Assume that \u03b1(t) > \u03b1(u).\nThen t must have a neighbor that is not neighbor of s, say t\u2032, such that \u03b1(t\u2032) < \u03b1(t)\n(otherwise, s would have more neighbors than t at order less than \u03b1(t), which con-\ntradicts the maximum cardinality property). The sequence t\u2032,t,s,u forms a path that\nis such that \u03b1 increases from t\u2032 to s, then decreases from s to u, and contains no\nchord. Moreover, t\u2032 and u cannot be neighbors, since this would yield an achordal\nloop and a contradiction. The proof of proposition 14.28 consists in showing that\nthis construction can be iterated until a contradiction is reached.\nMore precisely, assume that an achordal path s1,...,sk has been obtained, such\nthat \u03b1(s) is first increasing, then decreasing along the path, and such that, at extrem-\nities one either has \u03b1(s1) < \u03b1(sk) < \u03b1(s2) or \u03b1(sk) < \u03b1(s1) < \u03b1(sk\u22121). In fact, one can\nswitch between these last two cases by reordering the path backwards. Both paths\n(u,s,t) and (u,s,t,t\u2032) in the discussion above satisfy this property.\n\u2022 Assume, without loss of generality, that \u03b1(s1) < \u03b1(sk) < \u03b1(s2) and note that, in the\nconsidered path, s1 and sk cannot be neighbors (for, if j is the last index smaller than\nk \u22121 such that sj and sk are neighbors, then j must also be smaller than k \u22122 and the\nloop sj,...,sk\u22121,sk would be achordal).\n366\nCHAPTER 14. PROBABILISTIC INFERENCE FOR MRF\n\u2022 Since \u03b1(s2) > \u03b1(sk), and s1 and s2 are neighbors, sk must have a neighbor, say s\u2032\nk,\nsuch that s\u2032\nk is not neighbor of s2 and \u03b1(s\u2032\nk) < \u03b1(sk).\n\u2022 Select the first index j > 2 such that sj \u223cs\u2032\nk, and consider the path (s1,...,sj,s\u2032\nk).\nThis path is achordal, by construction, and one cannot have s1 \u223cs\u2032\nk since this would\ncreate an achordal loop. Let us show that \u03b1 first increases and then decreases along\nthis path. Since s2 is in the path, \u03b1 must first increase, and it suffices to show that\n\u03b1(s\u2032\nk) < \u03b1(sj). If \u03b1 increases from s1 to sj, then \u03b1(sj) > \u03b1(s2) > \u03b1(sk) > \u03b1(s\u2032\nk). If \u03b1\nstarted decreasing at some point before sj, then \u03b1(sj) > \u03b1(sk) > \u03b1(s\u2032\nk).\n\u2022 Finally, we need to show that the \u03b1-value at one extremity is between the first two\n\u03b1-values on the other end of the path. If \u03b1(s\u2032\nk) < \u03b1(s1), and since we have just seen\nthat \u03b1(sj) > \u03b1(sk) > \u03b1(s1), we do get \u03b1(s\u2032\nk) < \u03b1(s1) < \u03b1(sj). If \u03b1(s\u2032\nk) > \u03b1(s1), then, since\nby construction \u03b1(s2) > \u03b1(sk) > \u03b1(s\u2032\nk), we have \u03b1(s2) > \u03b1(s\u2032\nk) > \u03b1(s1).\n\u2022 So, we have obtained a new path that satisfies the same property that the one we\nstarted with, but with a maximum value at end points smaller than the initial one,\ni.e.,\nmax(\u03b1(s1),\u03b1(s\u2032\nk)) < max(\u03b1(s1),\u03b1(sk)).\nSince \u03b1 takes a finite number of values, this process cannot be iterated indefinitely,\nwhich yields our contradiction.\n\u25a0\n14.6.3\nComputing maximal cliques\nAt this point, we know that a graph must be triangulated for its maximal cliques\nto admit junction trees, and we have an algorithm to decide whether a graph is\ntriangulated, and extend it into a triangulated one if needed. This provides the first\nstep, (JT1), of our description of the junction tree algorithm. The next step, (JT2),\nrequires computing a list of maximal cliques. Computing maximal cliques in general\ngraph is an NP complete problem, for which a large number of algorithms has been\ndeveloped (see, for example, [149] for a review). For graphs with a perfect ordering,\nhowever, this problem can always be solved in a polynomial time.\nIndeed, assume that a perfect ordering is given for G = (V ,E), so that V = {s1,...,sn}\nis such that, for all k, V \u2032\nsk := Vsk \u2229{s1,...,sk\u22121} is a clique. Let Gk be G restricted to\n{s1,...,sk} and C\u2217\nk be the set of maximal cliques in Gk. Then the set Ck := {sk} \u222aV \u2032\nsk is\nthe only maximal clique in Gk that contains sk: it is a clique because the ordering is\nperfect, and any clique that contains sk must be included in it (because its elements\nare either sk or neighbors of sk). It follows from this that the set C\u2217\nk can be deduced\nfrom C\u2217\nk\u22121 by\n\uf8f1\uf8f4\uf8f4\uf8f2\uf8f4\uf8f4\uf8f3\nC\u2217\nk = C\u2217\nk\u22121 \u222a{Ck} if V \u2032\nk < C\u2217\nk\u22121\nC\u2217\nk = (C\u2217\nk\u22121 \u222a{Ck}) \\ {V \u2032\nk} if V \u2032\nk \u2208C\u2217\nk\u22121\nThis allows one to enumerate all elements in C\u2217\nG = C\u2217\nn, starting with C\u2217\n1 = {{s1}}.\n14.6. BUILDING JUNCTION TREES\n367\n14.6.4\nCharacterization of junction trees\nWe now discuss the last remaining point, (JT3). For this, we need to form the clique\ngraph of G, which is the undirected graph G = (C\u2217\nG,E) defined by (C,C\u2032) \u2208E if and\nonly if C \u2229C\u2032 , \u2205. We then have the following fact:\nProposition 14.29 The clique graph G of a connected triangulated undirected graph G\nis connected.\nProof We proceed by induction, and assume that the result is true if |V | = n \u22121\n(the proposition obviously holds if |V | = 1). Assume that a perfect order on G has\nbeen chosen, say V = {s1,...,sn}. Let G\u2032 be G restricted to {s1,...,sn\u22121}, and G\u2032 the\nassociated clique graph. Because {sn} \u222aVsn is a clique, any path in G provides a valid\npath in G\u2032 after removing all occurrences of sn (because any two neighbors of sn\nare linked). The induction hypothesis also implies that G\u2032 is connected. Since G is\nconnected, Vsn is not empty. Moreover, C := {sn} \u222aVsn must be a maximal clique in\nG (since we assume that the order is perfect) and it is the only maximal clique in G\nthat contains sn (all other maximal cliques in G therefore are maximal cliques in G\u2032\nalso). To prove that G is connected, it suffices to prove that C is connected to any\nother maximal clique, C\u2032, in G by a path in G. If t \u2208C, t , sn, there exists a maximal\nclique, say C\u2032\u2032, in G\u2032 that contains t, and, since G\u2032 is connected, there exists a path\n(C1 = C\u2032,...,Cq = C\u2032\u2032) connecting C\u2032 to C\u2032\u2032 in G\u2032. Let j be the first integer such that\nCj = Vn (take j = q + 1 if this never happens). Then (C1,...,Cj\u22121,C) is a path linking\nC\u2032 and C in G.\n\u25a0\nWe hereafter assume that G, and hence G, is connected. This is not real loss of\ngenerality because connected components in undirected graphs yields independent\nprocesses that can be handled separately. We assign weights to edges of the clique\ngraph of G by defining w(C,C\u2032) = |C \u2229C\u2032|. A subgraph \u02dcT of any given graph \u02dcG is\ncalled a spanning tree if \u02dcT is a tree with set of vertexes equal to the set of vertexes of\n\u02dcG. If T = (C\u2217\nG,E\u2032) is a spaning tree of G, we define the total weight\nw(T ) =\nX\n{C,C\u2032}\u2208E\u2032\nw(C,C\u2032).\nWe then have the proposition:\nProposition 14.30 [99] If G is a connected triangulated graph, the set of junction trees\nover C\u2217\nG coincides with the set of maximizers of w(T ) over all spanning trees of G.\n(Notice that G being connected implies that spanning trees over G exist.)\n368\nCHAPTER 14. PROBABILISTIC INFERENCE FOR MRF\nBefore proving this proposition, we discuss some properties related to maximal\n(or maximum-weight) spanning trees over an undirected graph. For this discussion,\nwe let G = (V ,E) be any undirected graph with weight (w(e),e \u2208E). We will then\napply these results to a clique graph when will switch back to the general notation\nof this section. Maximal spanning trees can be computed using the so-called Prim\u2019s\nalgorithm [98, 155, 63].\nAlgorithm 14.7 (Prim\u2019s algorithm)\nInitialize the algorithm with a single-node tree T1 = ({s1},\u2205), for some arbitrary s1 \u2208\nV . Let Tk\u22121 = (Vk\u22121,Ek\u22121) be the tree obtained at step k \u22121 of the algorithm. If k \u2264n,\nthe next tree is built as follows.\n(1) Let\nVk = {sk} \u222aVk\u22121 (sk < Vk\u22121.)\n(2) Let Ek = {ek} \u222aEk\u22121, such that ek = {sk,s} for some s \u2208Vk\u22121 satisfying\nw(ek) = max\n\u0012\nw({t,t\u2032}),{t,t\u2032} \u2208E,t < Vk\u22121,t\u2032 \u2208Vk\u22121\n\u0013\n.\n(14.36)\nThe ability of this algorithm to always build a maximal spanning tree is summa-\nrized in the following proposition [81, 129].\nProposition 14.31 If G = (V ,E,w) is a weighted, connected undirected graph, Prim\u2019s\nalgorithm, as described above, provides a sequence Tk = (Vk,Ek), for k = 1,...,n of subtrees\nof G such that Vn = V and, for all k, Tk is a maximal spanning tree for the restriction GVk\nof G to Vk.\nMoreover, any maximal spanning tree of G, can be realized as Tn, where (T1,...,Tn) is\na sequence provided by Prim\u2019s algorithm.\nProof We first prove that, for all k, Tk is a maximal spanning tree on the graph GVk.\nWe will prove a slightly stronger statement, namely, that, for all k, Tk can be\nextended to form a maximal spanning tree of G. This is stronger, because, if Tk =\n(Vk,Ek) can be extended to a maximal spanning tree T = (V ,E), and if T \u2032\nk = (Vk,E\u2032\nk) is\na spanning tree for GVk such that w(Tk) < w(T \u2032\nk), then the graph T \u2032 = (V ,E\u2032) with\nE\u2032 = (E \\ Ek) \u222aE\u2032\nk\nwould be a spanning tree for G with w(T) < w(T \u2032), which is impossible. (To see that\nT \u2032 is a tree, notice that paths in T \u2032 are in one-to-one correspondence with paths in T\nby replacing any subpath within T \u2032\nk by the unique subpath in Tk that has the same\nextremities.)\n14.6. BUILDING JUNCTION TREES\n369\nClearly, T1, which only has one vertex, can be extended to a maximal spanning\ntree. Let k \u22651 be the last integer for which this property is true for all j = 1,...,k.\nIf k = n, we are done. Otherwise, take a maximum spanning tree, T , that extends\nTk. This tree cannot contain the new edge added when building Tk+1, namely ek+1 =\n{sk+1,s} as defined in Prim\u2019s algorithm, since it would otherwise also extend Tk+1.\nConsider the path \u03b3 in T that links s to sk. This path must have an edge e = {t,t\u2032}\nsuch that t \u2208Vk and t\u2032 < Vk, and by definition of ek+1, we must have w(e) \u2264w(ek+1).\nNotice that e is uniquely defined, because a path leaving Vk cannot return in this set,\nsince one would be otherwise able to close it into a loop by inserting the only path\nin Tk that connects its extremities.\nReplace e by ek+1 in T. The resulting graph, say T \u2032, is still a spanning tree for\nG. From any path in T, one can create a path in T \u2032 with the same extremities by\nreplacing any occurrence of the edge, e, by the concatenation of the unique path in\nT going from t to s, followed by (s,sk+1), followed by the unique path in T going\nfrom sk+1 to t\u2032. This implies that T \u2032 is connected. It is also acyclic, since any loop\nin T would have to contain ek+1 (since T is acyclic), but there is no other path than\n(s,sk+1) in T \u2032 that links s and sk, because this path would have to be in T, and we\nhave removed the only possible one from T by deleting the edge e.\nAs a conclusion, T \u2032 is an extension of Tk+1, and a spanning tree with total weight\nlarger or equal to the one of T , and must therefore be optimal, too. But this contra-\ndicts the fact that Tk+1 cannot be extended to a maximal tree, so that k = n and the\nsequence of trees provided by Prim\u2019s algorithm is optimal.\nTo prove the second statement, let T be an optimal spanning tree. Let k be the\nlargest integer such that there exists a sequence (T1,...,Tk) generated by Prim\u2019s algo-\nrithm, such that, for all j = 1,...,k, Tj is a subtree of T. One necessarily has j \u22651,\nsince T extends any one-vertex tree. If k = n, we are done. Assuming otherwise,\nlet Tk = (Vk,Ek) and make one more step of Prim\u2019s algorithm, selecting an edge\nek+1 = (sk+1,s) satisfying (14.36). By assumption, ek+1 is not in T. Take as before\nthe unique path linking s and sk+1 in T and let e be the unique edge at which this\npath leaves Vk. Replacing e by ek+1 in T provides a new spanning tree, T \u2032. One\nmust have w(e) \u2265w(ek+1) because T is optimal, and w(ek+1) \u2265w(e) by (14.36). So\nw(e) = w(ek+1), and one can use e instead of ek+1 for the (k + 1)th step of Prim\u2019s al-\ngorithm. But this contradicts the fact that k was the largest integer in a sequence of\nsubtrees of T that is generated by Prim\u2019s algorithm, and one therefore has k = n.\n\u25a0\nThe proof of proposition 14.30, that we provide now, uses very similar \u201cedge-\nswitching\u201d arguments.\nProof (Proof of proposition 14.30) Let us start with a maximum weight spanning\ntree for G, say T , and show that it is a junction tree. Since T has maximum weight, we\n370\nCHAPTER 14. PROBABILISTIC INFERENCE FOR MRF\nknow that it can be obtained via Prim\u2019s algorithm, and that there exists a sequence\nT1,...,Tn = T of trees constructed by this algorithm. Let Tk = (Ck,Ek).\nWe proceed by contradiction. Let k be the largest index such that Tk can be ex-\ntended to a junction tree for C\u2217\nG, and let T \u2032 be a junction tree extension of Tk. Assume\nthat k < n, and let ek+1 = (Ck+1,C\u2032) be the edge that has been added when building\nTk+1, with Ck+1 = {Ck+1} \u222aCk. This edge is not in T \u2032, so that there exists a unique\nedge e = (B,B\u2032) in the path between Ck and C\u2032 in T \u2032 such that B \u2208Ck and B\u2032 < Ck. We\nmust have w(e) = |B \u2229B\u2032| \u2264w(ek+1) = |Ck+1 \u2229C\u2032|. But, since the running intersection\nproperty is true for T \u2032, both B and B\u2032 must contain Ck+1\u2229C\u2032 so that B\u2229B\u2032 = Ck+1\u2229C\u2032.\nThis implies that, if one modifies T \u2032 by replacing edge e by edge ek+1, yielding a new\nspanning tree T \u2032\u2032, the running intersection property is still satisfied in T \u2032. Indeed if\na vertex s \u2208V belongs to both extremities of a path containing B and B\u2032 in T \u2032, then\nit must belong to B \u2229B\u2032, and hence to Ck+1 \u2229C\u2032, and therefore to any set in the path\nin T \u2032 that linked Ck+1 and C\u2032. So we found a junction tree extension of Tk+1, which\ncontradicts our assumption that k was the largest. We must therefore have k = n and\nT is a junction tree.\nLet us now consider the converse statement and assume that T is a junction tree.\nLet k be the largest integer such that there exists a sequence of subgraphs of T that\nis provided by Prim\u2019s algorithm. Denote such a sequence by (T1,...,Tk), with Tj =\n(Cj,Ej). Assume (to get a contradiction) that k < n, and consider a new step for\nPrim\u2019s algorithm, adding a new edge ek+1 = {Ck+1,C\u2032} to Tk. Take as before the path\nin T linking C\u2032 to Ck+1 in T , and select the edge e at which this path leaves Ck. If e =\n(B,B\u2032), we must have w(e) = |B\u2229B\u2032| \u2264w(ek) = |Ck+1\u2229C\u2032|, and the running intersection\nproperty in T implies that Ck+1 \u2229C\u2032 \u2282B \u2229B\u2032, which implies that w(e) = w(ek+1).\nThis implies that adding e instead of ek+1 at step k + 1 is a valid choice for Prim\u2019s\nalgorithm, and contradicts the fact that k was the largest number of such steps that\ncould provide a subtree of T . So k = n and T is maximal.\n\u25a0\nChapter 15\nBayesian Networks\n15.1\nDefinitions\nBayesian networks are graphical models supported by directed acyclic graphs (DAG),\nwhich provide them with an ordered organization (directed graphs were introduced\nin definition 13.35).\nWe first introduce some notation. Let G = (V ,E) be a directed acyclic graph. The\nparents of s \u2208V are vertexes t such that (t,s) \u2208E, and its children are t\u2019s such that\n(s,t) \u2208E. The set of parents of s is denoted pa(s), and the set of its children is ch(s),\nwith Vs = ch(s) \u222apa(s).\nSimilarly to trees, the vertexes of G can be partially ordered by s \u2264G t if and only\nif there exists a path going from s to t. Unlike trees, however, there can be more\nthan one minimal element in V , and we still call roots vertexes that have no parent,\ndenoting\nV0 = {s \u2208V : pa(s) = \u2205}.\nWe also call leaves, or terminal nodes, vertexes that have no children. Unless other-\nwise specified, we assume that all graphs are connected.\nBayesian networks over G are defined as follows. We use the same notation as\nwith Markov random fields to represent the set of configurations F (V ) that contains\ncollections x = (xs,s \u2208V ) with xs \u2208Fs.\nDefinition 15.1 A random variable X with values in F (V ) is a Bayesian network over a\nDAG G = (V ,E) if and only if its distribution can be written in the form\nPX(x) =\nY\ns\u2208V0\nps(x(s))\nY\ns\u2208V \\V0\nps(x(pa(s)),x(s))\n(15.1)\nwhere ps is, for all s \u2208V , a probability distribution with respect to x(s).\n371\n372\nCHAPTER 15. BAYESIAN NETWORKS\nUsing the convention that conditional distributions given the empty set are just ab-\nsolute distributions, we can rewrite (15.1) as\nPX(x) =\nY\ns\u2208V\nps(x(pa(s)),x(s)).\n(15.2)\nOne can verify that P\nx\u2208\u2126PX(x) = 1. Indeed, when summing over x, we can start\nsumming over all x(s) with ch(s) = \u2205(the leaves). Such x(s)\u2019s only appear in the corre-\nsponding ps\u2019s, which disappear since they sum to 1. What remains is the sum of the\nproduct over V minus the leaves, and the argument can be iterated until the remain-\ning sum is 1 (alternatively, work by induction on |V |). This fact is also a consequence\nof proposition 15.5 below, applied with A = \u2205.\n15.2\nConditional independence graph\n15.2.1\nMoral graph\nBayesian networks have a conditional independence structure which is not exactly\ngiven by G, but can be deduced from it. Indeed, fixing S \u2282V , we can see, when\ncomputing the probability of X(S) = x(s) given X(Sc) = x(Sc), which is\nP(X(S) = x(S) | X(Sc) = x(Sc)) =\n1\nZ(x(Sc))\nY\ns\u2208V\nps(x(pa(s)),x(s)),\nthat the only variables x(t),t < S that can be factorized in the normalizing constant\nare those that are neither parent nor children of vertexes in S, and do not share a\nchild with a vertex in S (i.e., they intervene in no ps(x(pa(s)),x(s)) that involve elements\nof S). This suggests the following definition.\nDefinition 15.2 Let G be a directed acyclic graph. We denote G\u266f= (V ,E\u266f) the undirected\ngraph on V such that {s,t} \u2208E\u266fif one of the following conditions is satisfied\n\u2022 Either (s,t) \u2208E or (t,s) \u2208E.\n\u2022 There exists u \u2208V such that (s,u) \u2208E and (t,u) \u2208E.\nG\u266fis sometimes called the moral graph of G (because it forces parents to marry !). A\npath in G\u266fcan be visualized as a path in G\u266d(the undirected graph associated with\nG) which is allowed to jump between parents of the same vertex even if they were\nnot connected originally.\nThe previous discussion implies:\n15.2. CONDITIONAL INDEPENDENCE GRAPH\n373\nProposition 15.3 Let X be a Bayesian network on G. We have\n(SyT | U)G\u266f\u21d2(X(S)yX(T ) | X(U)),\ni.e., X is G\u266f-Markov.\nThis proposition can be refined by noticing that the joint distribution of X(S),\nX(T ) and X(U) can be deduced from a Bayesian network on a graph restricted to the\nancestors of S \u222aT \u222aU. Definition 13.21 for restricted graphs extends without change\nto directed graphs, and we repeat it below for convenience.\nDefinition 15.4 Let G = (V ,E) be a graph (directed or undirected), and A \u2282V . The\nrestricted graph GA = (A,EA) is such that the elements of EA are the edges (s,t) (or {s,t})\nin E such that both s and t belong to A.\nMoreover, for a directed acyclic graph G and s \u2208V , we define the set of ancestors of\ns by\nAs = {t \u2208V ,t \u2264G s}\n(15.3)\nfor the partial order on V induced by G.\nIf S \u2282V , we denote AS = S\ns\u2208S As. Note that, by definition, S \u2282AS. The following\nproposition is true.\nProposition 15.5 Let X be a Bayesian network on G = (V ,E) with distribution given by\n(15.2). Let S \u2282V and A = AS. Then the distribution of X(A) is a Bayesian network over\nGA given by\nP(X(A) = x(A)) =\nY\ns\u2208A\nps(x(pa(s)),x(s)).\n(15.4)\nThere is no ambiguity in the notation pa(s), since the parents of s \u2208A are the same in\nGA as in G.\nProof One needs to show that\nY\ns\u2208A\nps(x(pa(s)),x(s)) =\nX\nxAc\nY\ns\u2208V\nps(x(pa(s)),x(s)).\nThis can be done by induction on the cardinality of V . Assume that the result is true\nfor graphs of size n, and let |V | = n + 1 (the result is obvious for graphs of size 1).\nIf A = V , there is nothing to prove, so assume that Ac is not empty. Then Ac must\ncontain a leaf in G, since otherwise, A would contain all leaves and their ancestors\nwhich would imply that A = V .\nIf s \u2208Ac is a leaf in G, one can remove the variable x(s) from the sum, since it\nonly appear in ps and transition probabilities sum to one. But one can now apply\nthe induction assumption to the restriction of G to V \\ {s}.\n\u25a0\n374\nCHAPTER 15. BAYESIAN NETWORKS\nGiven proposition 15.5, proposition 15.3 can therefore be refined as follows.\nProposition 15.6 Let X be a Bayesian network on G. We have\n(SyT | U)(GAS\u222aT \u222aU )\u266f\u21d2(X(S)yX(T) | X(U)).\nProposition 15.5 is also used in the proof of the following proposition.\nProposition 15.7 Let G = (V ,E) be a directed acyclic graph, and X be a Bayesian net-\nwork over G. Then, for all s \u2208S\nP(X(s) = x(s) | X(As\\{s}) = x(As\\{s})) = P(X(s) = x(s) | X(pa(s)) = x(s\u2212)) = ps(x(pa(s)),x(s)).\nProof By proposition 15.5, we can without loss of generality assume that V = As.\nThen\nP(X(s) = x(s) | X(As\\{s}) = x(As\\{s}))\n\u221dP(X(As) = x(As))\n=\nps(x(pa(s)),x(s))Z(x(As\\{s}))\nwhere\nZ(x(As\\{s})) =\nY\nt\u2208As\\{s}\npt(x(pa(t)),x(t))\ndisappears when the conditional probability is normalized.\n\u25a0\n15.2.2\nReduction to d-separation\nWe now want to reformulate proposition 15.6 in terms of the unoriented graph G\u266d\nand specific features in G called v-junctions, that we now define.\nDefinition 15.8 Let G = (V ,E) be a directed graph. A v-junction is a triple of distinct\nvertexes, (s,t,u) \u2208V \u00d7 V \u00d7 V such that {s,u} \u2282pa(t) (i.e., s and u are parents of t).\nWe will say that a path (s1,...,sN) in G\u266dpasses at s = sk with a v-junction if (sk\u22121,sk,sk+1)\nis a v-junction in G.\nWe have the lemma:\nLemma 15.9 Two vertexes s and t in G are separated by a set U in (GA{s,t}\u222aU)\u266fif and only\nif any path between s and t in G\u266dmust either\n(1) Pass at a vertex in U without a v-junction.\n(2) Pass in V \\ A{s,t}\u222aU at a v-junction.\n15.2. CONDITIONAL INDEPENDENCE GRAPH\n375\nProof\nStep 1. We first note that the v-junction clause is redundant in (2). It can be removed\nwithout affecting the condition. Indeed, if a path in G\u266dpasses in V \\A{s,t}\u222aU one can\nfollow this path downward (i.e., following the orientation in G) until a v-junction is\nmet. This has to happen before reaching the extremities of the path, since u would\nbe an ancestor of s or t otherwise. We can therefore work with the weaker condition\n(that we will denote (2)\u2019) in the rest of proof.\nStep 2. Assume that U separates s and t in (GA{s,t}\u222aU)\u266f. Take a path \u03b3 between s and\nt in G\u266d. We need to show that the path satisfies (1) or (2)\u2019. So assume that (2)\u2019 is\nfalse (otherwise we are done) so that \u03b3 is included in A{s,t}\u222aU. We can modify \u03b3 by\nremoving all the central nodes in v-junctions and still keep a valid path in (GA{s,t}\u222aU)\u266f\n(since parents are connected in the moral graph). The remaining path must intersect\nU by assumption, and this cannot be at a v-junction in \u03b3 since we have removed\nthem. So (1) is true.\nStep 3. Conversely, assume that (1) or (2) is true for any path in G\u266d. Consider a path\n\u03b3 in (GA{s,t}\u222aU)\u266fbetween s and t. Any edge in \u03b3 that is not in G\u266dmust involve parents\nof a common child in A{s,t}\u222aU. Insert this child between the parents every time this\noccurs, resulting in a v-junction added to \u03b3. Since the added vertexes are still in\nA{s,t}\u222aU, the new path still has no intersection with V \\ A{s,t}\u222aU and must therefore\nsatisfy (1). So there must be an intersection with U without a v-junction, and since\nthe new additions are all at v-junctions, the intersection must have been originally\nin \u03b3, which therefore passes in U. This shows that U separates s and t in (GA{s,t}\u222aU)\u266f.\u25a0\nCondition (2) can be further restricted to provide the notion of d-separation.\nDefinition 15.10 One says that two vertexes s and t in G are d-separated by a set U if\nand only if any path between s and t in G\u266dmust either\n(D1) Pass at a vertex in U without a v-junction.\n(D2) Pass in V \\ AU with a v-junction.\nThen we have:\nTheorem 15.11 Two vertexes s and t in G are separated by a set U in (GA{s,t}\u222aU)\u266fif and\nonly if they are d-separated by U.\nProof It suffices to show that if condition ((D1) or (D2)) holds for any path between\ns and t in G\u266d, then so does ((1) or (2)). So take a path between s and t: if (D1) is true\n376\nCHAPTER 15. BAYESIAN NETWORKS\nfor this path, the conclusion is obvious, since (D1) and (1) are the same. So assume\nthat (D1) (and therefore (1)) is false and that (D2) is true. Let u be a vertex in V \\AU\nat which \u03b3 passes with a v-junction.\nAssume that (2) is false. Then u must be an ancestor of either s or t. Say it is an\nancestor of s: there is a path in G going from u to s without passing by U (otherwise\nu would be an ancestor of U); one can replace the portion of the old path between\ns and u by this new one, which does not pass by u with a v-junction anymore. So\nthe new path still does not satisfy (D1) and must satisfy (D2). Keep on removing\nall intersections with ancestors of s and t that have v-junctions to finally obtain a\npath that satisfies neither (D1) or (D2) and a contradiction to the fact that s and t are\nd-separated by U.\n\u25a0\n15.2.3\nChain-graph representation\nThe d-separability property involves both unoriented and oriented edges. It is in\nfact a property of the hybrid graph in which the orientation is removed from the\nedges that are not involved in a v-junction, and retained otherwise. Such graphs are\nparticular instances of chain graphs.\nDefinition 15.12 A chain graph G = (V ,E, \u02dcE) is composed with a finite set V of vertexes,\na set E \u2282P2(V ) of unoriented edges and a set \u02dcE \u2282E \u00d7 E \\ {(t,t),t \u2208E} of oriented edges\nwith the property that E \u2229\u02dcE\u266d= \u2205, i.e., two vertexes cannot be linked by both an oriented\nand an unoriented edge.\nA path in a chain graph is a a sequence of vertexes s0,...,sN such that for all k \u22651,\nsk\u22121 and sk form an edge, which means that either {sk\u22121,sk} \u2208E or (sk\u22121,sk) \u2208\u02dcE.\nA chain graph is acyclic if it contains no loop. It is semi-acyclic if it contains no loop\ncontaining oriented edges.\nWe start with the following equivalence relation within vertexes in a semi-acyclic\nchain graph.\nProposition 15.13 Let G = (V ,E, \u02dcE) be a semi-acyclic chain graph. Define the relation\nsRt if and only if there exists a path in the unoriented subgraph (V ,E) that links s and t.\nThen R is an equivalence relation.\nThe proposition is obvious. This relation partitions V in equivalence classes, the\nset of which being denoted VR. If S \u2208VR, then any pair s,t in S is related by an\nunoriented path, and if S , S\u2032 \u2208VR, no elements s \u2208S and t \u2208S\u2032 can be related by\nsuch a path.\n15.2. CONDITIONAL INDEPENDENCE GRAPH\n377\nMoreover, no path in G between two elements of S \u2208VR, can contain a directed\nedge, since these elements must also be related by an undirected path, and this\nwould create a loop in G containing an undirected edge. So the restriction of G\nto S is an undirected graph.\nOne can define a directed graph over equivalence classes as follows. Let GR =\n(VR,ER) be such that (S,S\u2032) \u2208ER if and only if there exists s \u2208S and t \u2208S\u2032 such\nthat (s,t) \u2208\u02dcE. The graph GR is acyclic: any loop in GR would induce a loop in G\ncontaining at least one oriented edge.\nWe now can formally define a probability distribution on a semi-acyclic chain\ngraph.\nDefinition 15.14 Let G = (V ,E, \u02dcE) be a semi-acyclic chain graph. One says that a ran-\ndom variable X decomposes on G if and only if: (X(S),S \u2208VR) is a Bayesian network on\nGR and the conditional distribution of X(S) given X(S\u2032),S\u2032 \u2208pa(S) is GS-Markov, such\nthat, for s \u2208S, P(X(s) = x(s) | X(t),t \u2208S,XS\u2032,S\u2032 \u2208pa(S)) only depends on x(t) with {s,t} \u2208E\nor (t,s) \u2208\u02dcE.\nReturning to our discussion on Bayesian networks, we have the following. Asso-\nciate to a DAG G = (V ,E) the chain graph G\u2020 = (V ,E\u2020, \u02dcE\u2020) defined by: {s,t} \u2208E\u2020 if and\nonly if (s,t) or (t,s) \u2208E and is not involved in a v-junction, and (s,t) \u2208\u02dcE\u2020 if (s,t) \u2208E\nand is involved in a v-junction. This graph is acyclic; indeed, take any loop in G\u2020:\nwhen its edges are given their original orientations in E, the sequence cannot contain\na v-junction, since the orientation in v-junctions are kept in G\u2020; the path therefore\nconstitutes a loop in G which is a contradiction.\nAll, excepted at most one, vertexes in an equivalence class S \u2208G\u2020\nR have all their\nparents in S. Indeed, assume that two vertexes, s and t, in S have parents outside of\nS. There exists an unoriented path, s0 = s,s1,...,sN = t, in G\u2020 connecting them, since\nthey belong to the same equivalence class. The edge at s must be oriented from s to\ns1 in G, since otherwise s1 would be a second parent to s in G, creating a v-junction,\nand the edge would have remained oriented in G\u2020. Similarly, the last edge in the\npath must be oriented from t to sN\u22121 in G. But this implies that there exists a v-\njunction in the original orientation along the path, which cannot be constituted with\nonly unoriented edges in G\u2020. So we get a contradiction.\nThus, random variables that decompose on G\u2020 are \u201cBayesian networks\u201d of acyclic\ngraphs, or trees since we know these are equivalent. The root of each tree must have\nmultiple (vertex) parents in the parent tree in GR. The following theorem states that\nall Bayesian networks are equivalent to such a process.\nTheorem 15.15 Let G = (V ,E) be a DAG. The random variable X is a Bayesian network\non G if and only if it decomposes over G\u2020.\n378\nCHAPTER 15. BAYESIAN NETWORKS\nProof Assume that X is a Bayesian network on G. We can obviously rewrite the\nprobability distribution of X in the form\n\u03c0(x) =\nY\nS\u2208G\u2020\nR\nY\ns\u2208S\nps(x(pa(s)),x(s)).\nSince every vertex in S has its parents in S or in S\nT\u2208pa(S) T , this a fortiori takes the\nform\n\u03c0(x) =\nY\nS\u2208G\u2020\nR\npS((x(T),T \u2208S\u2212),x(s)).\nSo X(S),S \u2208VR is a Bayesian network. Moreover,\npS((x(T ),T \u2208S\u2212),x(s)) =\nY\ns\u2208S\nps(x(pa(s)),x(s))\nis a tree distribution with the required form of the individual conditional distribu-\ntions.\nNow assume that X decomposes on G\u2020. Then the conditional distribution of\nX(S) given X(T),T \u2208pa(S) is Markov for the acyclic undirected graph GS, and can\ntherefore be expressed as a tree distribution consistent with the orientation of G.\n\u25a0\n15.2.4\nMarkov equivalence\nWhile the previous discussion provides a rather simple description of Bayesian net-\nworks in terms of chain graphs, it does not go all the way in reducing the number\nof oriented edges in the definition of a Bayesian network. The issue is, in some way,\naddressed by the notion of Markov equivalence, which is defined as follows.\nDefinition 15.16 Two directed acyclic graphs on the same set of vertexes G = (V ,E) and\n\u02dcG = (V , \u02dcE) are Markov-equivalent if any family of random variables that decomposes as a\n(positive) Bayesian network over one of them also decomposes as a Bayesian network over\nthe other.\nThe notion of Markov equivalence is exactly described by d-separation. This\nis stated in the following theorem, due to Geiger and Pearl [77, 76], that we state\nwithout proof.\nTheorem 15.17 G and \u02dcG are Markov equivalent if and only if, whenever two vertexes\nare d-separated by a set in one of them, the same separation is true with the other.\nThis property can be expressed in a strikingly simple condition. One says that a\nv-junction (s,t,u) in a DAG is unlinked if s and u are not neighbors.\n15.2. CONDITIONAL INDEPENDENCE GRAPH\n379\nTheorem 15.18 G and \u02dcG are Markov equivalent if and only if G\u266d= \u02dcG\u266dand G and \u02dcG have\nthe same unlinked v-junctions.\nProof Step 1. We first show that a given pair of vertexes in a DAG is unlinked if\nand only if it can be d-separated by some set in the graph. Clearly, if they are linked,\nthey cannot be d-separated (which is the \u201cif\u201d part), so what really needs to be proved\nis that unlinked vertexes can be d-separated. Let s and t be these vertexes and let\nU = A{s,t} \\ {s,t}. Then U d-separates s and t since any path between s and t in\n(GA{s,t}\u222aU)\u266f= (GA{s,t})\u266fmust obviously pass in U.\nStep 2. We now prove the only-if part of theorem 15.18 and therefore assume that\nG and \u02dcG are Markov equivalent, or, as stated in theorem 15.17, that d-separation\ncoincides in G and \u02dcG. We want to prove that G\u266d= \u02dcG\u266dand unlinked v-junctions are\nthe same.\nStep 2.1. The first statement is obvious from Step 1: d-separation determines the\nexistence of a link, so if d-separation coincides in the two graphs, then the same\nholds for links and G\u266d= \u02dcG\u266d.\nStep 2.2. So let us proceed to the second statement and let (s,t,u) be an unlinked v-\njunction in G. We want to show that it is also a v-junction in \u02dcG (obviously unlinked\nsince links coincide).\nWe will denote by \u02dc\nAS the ancestors of some set S \u2282V in \u02dcG (while AS still denotes\nits ancestors in G). Let U = A{s,u} \\ {s,u}. Then, as we have shown in Step 1, U\nd-separates s and u in G, so that, by assumption it also d-separates them in \u02dcG.\nWe know that t < U, because it cannot be both a child and an ancestor of {s,u} in G\n(this would induce a loop). The path (s,t,u) links s and u and does not pass in U,\nwhich is only possible (since U d-separates s and t in \u02dcG) if it passes in V \u2212\u02dc\nAU at a\nv-junction: so (s,t,u) is a v-junction in \u02dcG, which is what we wanted to prove.\nStep 3. We now consider the converse statement and assume that G\u266d= \u02dcG\u266dand un-\nlinked v-junctions coincide. We want to show that d-separation is the same in G and\n\u02dcG. So, we assume that U d-separates s and t in G, and we want to show that the same\nis true in \u02dcG. Thus, what we need to prove is:\nClaim 1. Consider a path \u03b3 between s and t in \u02dcG\u266d= G\u266d. Then \u03b3 either (D1) passes in\nU without a v-junction in \u02dcG, or (D2) in V \\ \u02dc\nAU with a v-junction in \u02dcG.\nWe will prove Claim 1 using a series of lemmas. We say that \u03b3 has a three-point loop\nat u if (v,u,w) are three consecutive points in \u03b3 such that v and w are linked. So\n(v,u,w,v) forms a loop in the undirected graph.\nLemma 15.19 If \u03b3 is a path between s and t that does not satisfy (D2) for G and passes\nin U without three-point loops, then \u03b3 satisfies (D1) for \u02dcG.\nThe proof is easy: since \u03b3 does not satisfy (D2) in G, it satisfies (D1) and passes in\nU without a v-junction in G. But this intersection cannot be a v-junction in \u02dcG since\n380\nCHAPTER 15. BAYESIAN NETWORKS\nit would otherwise have to be linked and constitute a three-point loop in \u03b3, which\nproves that (D1) is true for \u03b3 in \u02dcG.\nThe next step is to remove the three-point loop condition in lemma 15.19. This will\nbe done using the next two results.\nLemma 15.20 Let \u03b3 be a path with a three-point loop at u \u2208U for G. Assume that \u03b3 \\ u\n(which is a valid path in G\u266d) satisfies (D1) or (D2) in \u02dcG. Then \u03b3 satisfies (D1) or (D2) in\n\u02dcG.\nTo prove the lemma, let v and w be the predecessor and successor of u in \u03b3. First\nassume that \u03b3 \\ u satisfies (D1) in \u02dcG. If this does not happen at v or at w, then this\nwill apply also to \u03b3 and we are done, so let us assume that v \u2208U and that (v\u2032,v,w) is\nnot a v-junction in \u02dcG, where v\u2032 is the predecessor of v. If (v\u2032,v,u) is not a v-junction\nin \u02dcG, then (D1) is true for \u03b3 in \u02dcG. If it is a v-junction, then (v,u,w) is not and (D1) is\ntrue too.\nAssume now that (D2) is true for \u03b3 \\u in \u02dcG. Again, there is no problem if (D2) occurs\nfor some point other than v or w, so let us consider the case for which it happens at\nv. This means that v < \u02dc\nAU and (v\u2032,v,w) is a v-junction. But, since u \u2208U, the link\nbetween u and v must be from u to v in \u02dcG so that there is no v-junction at u and (D1)\nis true in \u02dcG. This proves lemma 15.20.\nLemma 15.21 Let \u03b3 be a path with a three-point loop at u \u2208U for G. Assume that \u03b3\ndoes not satisfy (D2) in G. Then \u03b3 \\ u does not satisfy this property either.\nLet us assume that \u03b3 \\ u satisfies (D2) and reach a contradiction. Letting (v,u,w) be\nthe three-point loop, (D2) can only happen in \u03b3 \\ u at v or w, and let us assume that\nthis happens at v, so that, v\u2032 being the predecessor of v, (v\u2032,v,w) is a v-junction in G\nwith v < AU. Since v < AU, the link between u and v in G must be from u to v, but\nthis implies that (v\u2032,v,u) is a v-junction in G with v < AU which is a contradiction:\nthis proves lemma 15.21.\nThe previous three lemmas directly imply the next one.\nLemma 15.22 If \u03b3 is a path between s and t that does not satisfy (D2) for G, then \u03b3\nsatisfies (D1) or (D2) for \u02dcG.\nIndeed, if we start with \u03b3 that does not satisfy (D2) for G, lemma 15.21 allows us\nto progressively remove three-point loops from \u03b3 until none remains with a final\npath that satisfies the assumptions of lemma 15.19 and therefore satisfies (D1) in \u02dcG,\nand lemma 15.20 allows us to add the points that we have removed in reverse order\nwhile always satisfying (D1) or (D2) in \u02dcG.\nWe now partially relax the hypothesis that (D2) is not satisfied with the next lemma.\nLemma 15.23 If \u03b3 is a path between s and t that does not pass in V \\ AU at a linked\nv-junction for G, then \u03b3 satisfies (D1) or (D2) for \u02dcG.\n15.2. CONDITIONAL INDEPENDENCE GRAPH\n381\nAssume that \u03b3 does not satisfy (D2) for \u02dcG (otherwise the result is proved).\nBy\nlemma 15.22, \u03b3 must satisfy (D2) for G. So, take an intersection of \u03b3 with V \\AU that\noccurs at a v-junction in G, that we will denote (v,u,w). This is still a v-junction in\n\u02dcG since we assume it to be unlinked. Since (D2) is false in \u02dcG, we must have u \u2208\u02dc\nAU,\nand there is an oriented path, \u03c4, from u to U in \u02dcG.\nWe can assume that \u03c4 has no v-junction in G. If a v-junction exists in \u03c4, then this\nv-junction must be linked (otherwise this would also be a v-junction in \u02dcG and con-\ntradict the fact that \u03c4 is consistently oriented in \u02dcG), and this link must be oriented\nfrom u to U in \u02dcG to avoid creating a loop in this graph. This implies that we can\nbypass the v-junction while keeping a consistently oriented path in \u02dcG, and iterate\nthis until \u03c4 has no v-junction in G. But this implies that \u03c4 is consistently oriented in\nG, necessarily from U to u since u < AU.\nDenote \u03c4 = (u0 = u,v1,...,un \u2208U). We now prove by induction that each (v,uk,w) is\nan unlinked v-junction. This is true when k = 0, and let us assume that it is true for\nk \u22121. Then (uk,uk\u22121,v) is a v-junction in G but not in \u02dcG: so it must be linked and\nthere exists an edge between v and uk. In \u02dcG, this edge must be oriented from v to uk,\nsince (v,uk\u22121,uk,v) would form a loop otherwise. For the same reason, there must be\nan edge in \u02dcG from w to uk so that (v,uk,w) is an unlinked v-junction.\nSince this is true for k = n, we can replace u by un in \u03b3 and still obtain a valid path.\nThis can be done for all intersections of \u03b3 with V \\AU that occur at v-junctions. This\nfinally yields a path (denote it \u00af\u03b3) which does not satisfy (D2) in G anymore, and\ntherefore satisfies (D1) or (D2) in \u02dcG: so \u00af\u03b3 must either pass in U without a v-junction\nor in V \\ \u02dcAU at a v-junction. None of the nodes that were modified can satisfy any of\nthese conditions, since they were all in U with a v-junction, so that the result is true\nfor the original \u03b3 also. This proves lemma 15.23.\nSo the only unsolved case is when \u03b3 is allowed to pass in V \\AU at linked v-junctions.\nWe define an algorithm that removes them as follows. Let \u03b30 = \u03b3 and let \u03b3k be the\npath after step k of the algorithm. One passes from \u03b3k to \u03b3k+1 as follows.\n\u2022 If \u03b3k has no linked v-junctions in V \\ AU for G, stop.\n\u2022 Otherwise, pick such a v-junction and let (v,u,w) be the three nodes involved in\nit.\n(i) If v \u2208U,v\u2032 < U and (v\u2032,v,u) is a v-junction in \u02dcG, remove v from \u03b3k to define\n\u03b3k+1.\n(ii) Otherwise, if w \u2208U,w\u2032 < U and (u,w,w\u2032) is a v-junction in \u02dcG, remove w from\n\u03b3k to define \u03b3k+1.\n(iii) Otherwise, remove u from \u03b3k to define \u03b3k+1.\nNone of the considered cases can disconnect the path. This is clear for case (iii) since\nv and w are linked. For case (i), note that, in G, (v\u2032,v,u) cannot be a v-junction since\n(v,u,w) is one. This implies that the v-junction in \u02dcG must be linked and that v\u2032 and\nu are connected.\n382\nCHAPTER 15. BAYESIAN NETWORKS\nThe algorithm will stop at some point with some \u03b3n that does not have any linked\nv-junction in V \\AU anymore, which implies that (D1) or (D2) is true in \u02dcG for \u03b3n. To\nprove that this statement holds for \u03b3, it suffices to show that if (D1) or (D2) is true\nin \u02dcG with \u03b3k+1, it must have been true with \u03b3k at each step of the algorithm. So let\u2019s\nassume that \u03b3k+1 satisfies (D1) or (D2) in \u02dcG.\nFirst assume that we passed from \u03b3k to \u03b3k+1 via case (iii). Assume that (D2) is true\nfor \u03b3k+1, with as usual the only interesting case being when this occurs at v or w.\nAssume it occurs at v so that (v\u2032,v,w) is a v-junction and v < \u02dc\nAU. If (v\u2032,v,u) is a\nv-junction, then (D2) is true with \u03b3k. Otherwise, there is an edge from v to u in \u02dcG\nwhich also implies an edge from w to u since (v,u,w,v) would be a loop otherwise.\nSo (v,u,w) is a v-junction in \u02dcG, and u cannot be in \u02dc\nAU since its parent, v would be in\nthat set also. So (D2) is true in \u02dcG. Now, assume that (D1) is true at v, so that (v\u2032,v,w)\nis not a v-junction and v \u2208U. If (v\u2032,v,u) is not a v-junction either, we are done, so\nassume the contrary. If v\u2032 \u2208U, then we cannot have a v-junction at v\u2032 and (D1) is\ntrue. But v\u2032 < U is not possible since this leads to case (i).\nNow assume that we passed from \u03b3k to \u03b3k+1 via case (i). Assume that (D1) is true for\n\u03b3k: this cannot be at v\u2032 since v\u2032 < U, neither at u since u < AU, so it will also be true\nfor \u03b3k+1. The same statement holds with (D2) since (v\u2032,v,u) is a v-junction in \u02dcG with\nv \u2208U which implies that both v\u2032 and u are in \u02dc\nAU. Case (ii) is obviously addressed\nsimilarly.\nWith this, the proof of theorem 15.18 is complete.\n\u25a0\n15.2.5\nProbabilistic inference: Sum-prod algorithm\nWe now discuss the issue of using the sum-prod algorithm to compute marginal\nprobabilities, P(X(s) = x(s)) for s \u2208V when X is a Bayesian network on G = (V ,E). By\ndefinition, P(X = x) can be written in the form\nP(X = x) =\nY\nC\u2208C\n\u03d5C(x(C))\nwhere C contains all subsets Cs := {s}\u222apa(s), s \u2208V . Marginal probabilities can there-\nfore be computed easily when the factor graph associated to C is acyclic, according\nto proposition 14.13. However, because of the specific form of the \u03d5C\u2019s (they are\nconditional probabilities), the sum-prod algorithm can be analyzed in more detail,\nand provide correct results even when the factor graph is not acyclic.\nThe general rules for the sum-prod algorithm are\n\uf8f1\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f2\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f3\nmsC(x(s)) \u2190\nY\n\u02dcC,s\u2208\u02dcC, \u02dcC,C\nm \u02dcCs(x(s))\nmCs(x(s)) \u2190\nX\ny(C):y(s)=x(s)\n\u03d5C(y(C))\nY\nt\u2208C\\{s}\nmtC(y(t))\n15.2. CONDITIONAL INDEPENDENCE GRAPH\n383\nThey take a particular form for Bayesian networks, using the fact that a vertex s\nbelongs to Cs, and to all Ct for t \u2208ch(s).\nmsCs(x(s))\n\u2190\nY\nt\u2208ch(s)\nmCts(x(s)),\nmsCt(x(s))\n\u2190\nmCss(x(s))\nY\nu\u2208ch(s),u,t\nmCus(x(s)), for t \u2208ch(s),\nmCss(x(s))\n\u2190\nX\ny(Cs),y(s)=x(s)\nps(y(pa(s)),x(s))\nY\nt\u2208pa(s)\nmtCs(y(t)),\nmCts(x(s))\n\u2190\nX\ny(Ct),y(s)=x(s)\npt(x(s) \u2227y(pa(s)\\{t}),y(t))mtCt(y(t))\nY\nu\u2208pa(s),u,t\nmuCt(y(u)),\nfor t \u2208ch(s).\nThese relations imply that, if pa(s) = \u2205(s is a root), then mCss = ps(x(s)). Also, if\nch(s) = \u2205(s is a leaf) then msCs = 1. The following proposition shows that many of the\nmessages become constant over time.\nProposition 15.24 All upward messages, msCs and mCts with t \u2208ch(s) become constant\n(independent from x(s)) in finite time.\nProof This can be shown recursively as follows. Assume that, for a given s, mtCt is\nconstant for all t \u2208ch(s) (this is true if s is a leaf). Then,\nmCts(x(s))\n\u2190\nX\npeyCt,y(s)=x(s)\npt(x(s) \u2227y(pa(s)\\{t}),y(t))mtCt(y(t))\nY\nu\u2208pa(s),u,t\nmuCt(y(u)),\n=\nmtCt\nX\ny(Ct),y(s)=x(s)\npt(x(s) \u2227y(pa(s)\\{t}),y(t))\nY\nu\u2208pa(s),u,t\nmuCt(y(u))\n=\nmtCt\nX\ny(Ct\\{t}),y(s)=x(s)\nY\nu\u2208pa(s),u,t\nmuCt(y(u))\n=\nmtCt\nY\nu\u2208pa(s),u,t\nX\ny(u)\nmuCt(y(u))\nwhich is constant. Now\nmsCs(x(s)) \u2190\nY\nt\u2208ch(s)\nmCts(x(s))\nis also constant. This proves that all msCs progressively become constant, and, as we\nhave just seen, this implies the same property for mCts, t \u2208ch(s).\n\u25a0\n384\nCHAPTER 15. BAYESIAN NETWORKS\nThis proposition implies that, if initialized with constant messages (or after a\nfinite time), the sum-prod algorithm iterates\nmsCs\n\u2190\nY\nt\u2208ch(s)\nmCts\nmCss(x(s))\n\u2190\nX\ny(Cs),y(s)=x(s)\nps(y()pa(s),x(s))\nY\nt\u2208pa(s)\nmtCs(y(t))\nmsCt(x(s))\n\u2190\nmCss(x(s))\nY\nu\u2208ch(s),u,t\nmCus, t \u2208ch(s)\nmCts\n\u2190\nmsCs\nY\nu\u2208pa(t),u,s\nX\ny(u)\nmuCs(y(u)), t \u2208ch(s).\nFrom this expression, we can conclude\nProposition 15.25 If the previous algorithm is first initialized with upward messages,\nmsCs = mCts all equal to 1, and if downward messages are computed top down from the\nroots to the leaves, the obtained configuration of messages is invariant for the sum-prod\nalgorithm.\nProof If all upward messages are equal to 1, then clearly, the downward messages\nsum to 1 once they are updated from roots to leaves, and this implies that the upward\nmessages will remain equal to 1 for the next round. The obtained configuration is\ninvariant since the downward messages are recursively uniquely defined by their\nvalue at the roots.\n\u25a0\nThe downward messages, under the previous assumptions, satisfy msCt(x(s)) = mCss(x(s))\nfor all t \u2208ch(s) and therefore\nmCss(x(s)) =\nX\ny(Cs),y(s)=x(s)\n\u03c0(y(pa(s)),x(s))\nY\nt\u2208pa(s)\nmCtt(y(t)).\n(15.5)\nNote that the associated \u201cmarginals\u201d inferred by the sum-prod algorithm are\n\u03c3s(x(s)) =\nY\nC,s\u2208C\nmCs(x(s)) = mCss(x(s))\nsince mCts(x(s)) = 1 when t \u2208ch(s).\nAlthough the sum-prod algorithm initialized with unit messages converges to a\nstable configuration if run top-down, the obtained \u03c3s\u2019s do not necessarily provide\nthe correct single site marginals. There is a situation for which this is true, however,\nwhich is when the initial directed graph is singly connected, as we will see below.\n15.2. CONDITIONAL INDEPENDENCE GRAPH\n385\nBefore this, let us analyze the complexity resulting from an iterative computation of\nthe marginal probabilities, similar to what we have done with trees.\nWe define the depth of a vertex in G as follows.\nDefinition 15.26 Let G = (V ,E) be a DAG. The depth of a vertex s in V is defined recur-\nsively by\n- depth(s) = 0 if s has no parent.\n- depth(s) = 1 + max\n\u0010\ndepth(t),t \u2208pa(s)\n\u0011\notherwise.\nThe recursive computation of marginal distributions is made possible (although\nnot always feasible) with the following remark.\nLemma 15.27 Let X be a Bayesian network on the DAG G = (V ,E), and S \u2282V , such\nthat all elements in S have the same depth. Let pa(S) be the set of parents of elements\nin S, and T = depth\u2212(S) the set of vertexes in V with depth strictly smaller than the\ndepth of S. Then (X(S)yX(T\\pa(S)) | X(pa(S))) and the variables X(s),s \u2208S are conditionally\nindependent given X(pa(S)).\nProof It suffices to show that vertexes in S are separated from T \\ pa(S) and from\nother elements of S by pa(S) for the graph (GS\u222aT )\u266f. Any path starting at s \u2208S must\neither pass by a parent of s (which is what we want), or by one of its children, or\nby another vertex that shares a child with s in GS\u222aT . But s cannot have any child\nin GS\u222aT , since this child cannot have a smaller depth than s, and it cannot be in S\neither since all elements in S have the same depth.\n\u25a0\nThis lemma allows us to work recursively as follows. Assume that we can compute\nmarginal distributions over sets S with maximal depth no larger than d. Take a set\nS of maximal depth d +1, and let S0 be the set of elements of depth d +1 in S. Then,\nletting T = depth\u2212(S) = depth\u2212(S0), and S1 = S \\ S0,\nP(X(S) = x(S))\n=\nX\ny(T \\S1)\nP(X(S0) = x(S0) | X(T ) = y(T\\S1) \u2227x(S1))P(X(T \u222aS1) = y(T \\S1) \u2227x(S1))\n=\nX\ny(pa(S)\\S1)\nY\ns\u2208S0\nps((y \u2227x)(pa(s)) \u2227x(S1),x(s))P(X(pa(S0)\u222aS1) = y(pa(S0)\\S1) \u2227x(S1))\n(15.6)\nSince pa(S)\u222aS1 has maximal depth strictly smaller than the maximal depth of S, this\nindeed provides a recursive formula for the computation of marginal over subsets\nof V with increasing maximal depths. However, because one needs to add parents\nto the considered set when reducing the depth, one may end up having to compute\nmarginals over very large sets, which becomes intractable without further assump-\ntions.\n386\nCHAPTER 15. BAYESIAN NETWORKS\nA way to reduce the complexity is to assume that the graph G is singly connected,\nas defined below.\nDefinition 15.28 A DAG G is singly connected if there exists at most one path in G that\nconnects any two vertexes.\nSuch a property is true for a tree, but also holds for some networks with multiple\nparents. We have the following nice property in this case.\nProposition 15.29 Let G be a singly connected DAG and X a Bayesian network on G. If\ns is a vertex in G, the variables (X(t),t \u2208pa(s)) are mutually independent.\nProof We have, using proposition 15.5,\nP(X(pa(s)) = x(pa(s))) =\nX\ny(Apa(s)),y(pa(s))=x(pa(s))\nY\nu\u2208Apa(s)\npu(y(pa(u)),y(u)).\nBecause the graph is singly connected, two parents of s cannot have a common an-\ncestor (since there would then be two paths from this ancestor to S). So Apa(s) is the\ndisjoint union of the At\u2019s for t \u2208pa(s) and we can write\nP(X(pa(s)) = x(pa(s)))\n=\nX\ny(Apa(s)),y(pa(s))=x(pa(s))\nY\nt\u2208pa(s)\nY\nu\u2208At\npu(y(pa(u)),y(u))\n=\nY\nt\u2208pa(s)\nX\ny(At),y(t)=x(t)\nY\nu\u2208At\npu(y(pa(u)),y(u))\n=\nY\nt\u2208pa(s)\nP(X(t) = x(t))\nThis proves the lemma.\n\u25a0\nSection 15.2.5 can be simplified under the assumption of a singly connected\ngraph, at least for the computation of single vertex marginals; we have, if s \u2208V\nand G is singly connected\nP(X(s) = x(s)) =\nX\ny(pa(s))\nps(y(pa(s)),x(s))\nY\nt\u2208pa(s)\nP(X(t) = y(t)).\n(15.7)\nThis is now recursive in single vertex marginal probabilities. It moreover coincides\nwith the recursive equation that defines the messages mCss in (15.5), which shows\nthat the sum-prod algorithm provides the correct answer in this case.\n15.2. CONDITIONAL INDEPENDENCE GRAPH\n387\n15.2.6\nConditional probabilities and interventions\nOne of the main interests of graphical models is to provide an ability to infer the be-\nhavior of hidden variables of interest given other, observed, variables. When dealing\nwith oriented graphs the way this should be analyzed is, however, ambiguous.\nLet\u2019s consider an example, provided by the graph in fig. 15.1. The Bayesian net-\nNo school\nBad weather\nBroken HVAC\nFigure 15.1: Example of causal graph.\nwork interpretation of this graph is that both events (which may be true or false)\n\u201cBad weather\u201d and \u201cBroken HVAC\u201d happen first, and that they are independent.\nThen, given their observation, the \u201cNo school\u201d event may occur, probably more\nlikely if the weather is bad or the HVAC is broken or snow, and even more likely\nif both happened at the same time.\nNow consider the following passive observation: you wake up, you haven\u2019t checked\nthe weather yet or the news yet, and someone tells you that there is no school today.\nThen you may infer that there is more chances than usual for bad weather or the\nHVAC broken at school. Conditionally to this information, these two events become\ncorrelated, even if they were initially independent. So, even if the \u201cNo school\u201d event\nis considered as a probabilistic consequence of its parents, observing it influences\nour knowledge on them.\nNow, here is an intervention, or manipulation: the school superintendent has\ndeclared that he has given enough snow days for the year and declared that there\nwould be school today whatever happens. So you know that the \u201cno-school\u201d event\nwill not happen. Does it change the risk of bad weather of broken HVAC? Obviously\nnot: an intervention on a node does not affect the distribution of the parents.\nManipulation and passive observation are two very different ways of affecting\nunobserved variables in Bayesian networks. Both of them may be relevant in appli-\ncations. Of the two, the simplest to analyze is intervention, since it merely consists\nin clamping one of the variables while letting the rest of the network dynamics un-\nchanged. This leads to the following formal definition of manipulation.\nDefinition 15.30 Let G = (V ,E) be a directed acyclic graph and X a Bayesian network on\n388\nCHAPTER 15. BAYESIAN NETWORKS\nG. Let S be a subset of G and x(S) \u2208FS a given configuration on S. Then the manipulated\ndistribution of X with fixed values x(S) on S is the Bayesian network on the restricted\ngraph GS, with the same conditional probabilities, using the value x(s) every time a vertex\ns \u2208S is a parent of t \u2208V \\ S in G.\nSo, if the distribution of X is given by (15.2), then its distribution after manipulation\non S is\n\u02dc\u03c0(y(V \\S)) =\nY\nt\u2208V \\S\npt(y(pa(t)),y(t))\nwhere pa(t) is the set of parents of t in G, and y(s) = x(s) whenever s \u2208pa(t) \u2229S.\nThe distribution of a Bayesian network X after passive observation X(S) = x(S) is\nnot so easily described. It is obviously the conditional distribution P(X(V \\S) = y(V \\S) |\nX(S) = x(S)) and therefore requires using the conditional dependency structure, in-\nvolving the moral graph and/or d-separation.\nLet us discuss this first in the simpler case of trees, for which the moral graph is\nthe undirected acyclic graph underlying the tree, and d-separation is simple separa-\ntion on this acyclic graph. We can then use proposition 13.22 to understand the new\nstructure after conditioning: it is a G\u266d\nV \\S-Markov random field, and, for t \u2208V \\S, the\nconditional distribution of X(t) = y(t) given its neighbors is the same as before, using\nthe value x(s) when s \u2208S. But note that when doing this (passing to G\u266d), we broke the\ncausality relation between the variables. We can however always go back to a tree (or\nforest, since connectedness may have been broken) with the same edge orientation as\nthey initially were, but this requires reconstituting the edge joint probabilities from\nthe new acyclic graph, and therefore using (acyclic) belief propagation.\nWith general Bayesian networks, we know that the moral graph can be loopy and\ntherefore a source of difficulties. The following proposition states that the damage\nis circumscribed to the ancestors of S.\nProposition 15.31 Let G = (V ,E) be a directed acyclic graph, X a Bayesian network\non G, S \u2282V and x(AS) \u2208F (AS). Then the conditional distribution of X(Ac\nS) given by\nX(AS) = x(AS) coincides with the manipulated distribution in definition 15.30.\nProof The conditional distribution is proportional to\nY\ns\u2208V\np(y(pa(s)),y(s))\nwith y(t) = x(t) if t \u2208AS. Since s \u2208AS implies pa(s) \u2282AS, all terms with s \u2208AS are\nconstant in the sum and can be factored out after normalization. So the conditional\n15.3. STRUCTURAL EQUATION MODELS\n389\ndistribution is proportional to\nY\ns\u2208Ac\nS\np(y(pa(s)),y(s))\nwith y(t) = x(t) if t \u2208AS. But we know that such products sum to 1, so that the\nconditional distribution is equal to this expression and therefore provides a Bayesian\nnetwork on GAc\nS.\n\u25a0\n15.3\nStructural equation models\nStructural equation models (SEM\u2019s) provides an alternative (and essentially equiva-\nlent) formulation of Bayesian networks, which may be more convenient to use, espe-\ncially when dealing with variables taking values in general state spaces.\nLet G = (V ,E) be a directed acyclic graph. SEMs are associated to families of\nfunctions \u03a6s : F (pa(s)) \u00d7 Bs \u2192Fs and random variables \u03bes : \u2126\u2192Bs (where Bs is\nsome measurable set), for s \u2208V . The random field X : \u2126\u2192F (V ) associated to the\nSEM satisfies the equations\nXs = \u03a6(s)(X(s\u2212),\u03be(s)).\n(15.8)\nBecause of the DAG structure, these equations uniquely define X once \u03be is specified.\nAs a consequence, there exists a function \u03a8 such that X = \u03a8(\u03be).\nThe model is therefore fully specified by the functions \u03a6(s) and the probability\ndistributions of the variables \u03be(s). We will assume that they have a density, denoted\ng(s),s \u2208V , with respect to some measure \u00b5s on Bs. They are typically chosen as\nuniform distributions on Bs (continuous and compact, or discrete) or as standard\nGaussian when Bs = Rds for some ds. One also generally assumes that the variables\n(\u03be(s),s \u2208V ) are jointly independent, and we make this assumption below.\nLet Vk, k \u22650, be the set of vertexes in V with depth k (c.f. definition 15.26) and\nV<k = V0\u222a\u00b7\u00b7\u00b7\u222aVk\u22121. Then (using the independence of (\u03be(s),s \u2208V ), for s \u2208Vk, the con-\nditional distribution of X(s) given X(V<k) = x(V<k) is the distribution of \u03a6(s)(x(s\u2212),\u03be(s)).\nFormally this is given by\n\u03a6(s)(x(s\u2212),\u00b7)\u266f(g(s)\u00b5s),\nthe pushforward of the distribution of \u03be(s) by \u03a6(s)(x(s\u2212),\u00b7).\nMore concretely, assume that \u03bes follows a uniform distribution on Bs = [0,1]h for\nsome h, and assume that Fs is finite for all s. Then,\nP(X(s) = x(s) | X(V<k) = x(V<k)) = Volume(Us(x(pa(s)),x(s)))\n\u2206= ps(x(pa(s)),x(s))\n390\nCHAPTER 15. BAYESIAN NETWORKS\nwhere\nUs(x(pa(s)),x(s)) =\nn\n\u03be \u2208[0,1]h : \u03a6(s)(x(s\u2212),\u03be) = x(s)o\n.\nSince variables X(s), s \u2208Vk are conditionally independent given X(V <k), we find that\nX decomposes as a Bayesian network over G,\nP(X = x) =\nY\ns\u2208V\nps(x(pa(s)),x(s)).\nSimilarly, if Fs = Bs = Rds, \u03be(s) \u223cN (0,IdRds), and \u03be(s) 7\u2192\u03a6(s)\n\u03b8 (x(pa(s)),\u03be(s)) is invertible,\nwith C1 inverse x(s) 7\u2192\u03a8(s)\n\u03b8 (x(pa(s)),x(s)), then X is a Bayesian network, with continu-\nous variables, and, using the change of variable formula, the conditional distribution\nof X(s) given X(pa(s)) = x(s\u2212) has p.d.f.\nps(x(pa(s)),x(s)) =\n1\n(2\u03c0)ds/2 exp\n\u0012\n\u22121\n2|xs \u2212\u03a8(s)\n\u03b8 (x(pa(s)),x(s))|2\u0013\f\f\f\fdet(\u2202x(s)\u03a8(s)\n\u03b8 (x(pa(s)),x(s)))\n\f\f\f\f.\nA simple and commonly used special case for this example are linear SEMs, with\nX(s) = as + bT\ns X(s) + \u03c3s\u03be(s).\nIn this case, the inverse mapping is immediate and the Jacobian determinant in the\nchange of variables is 1/\u03c3ds\ns .\nChapter 16\nLatent Variables and Variational Methods\n16.1\nIntroduction\nWe will describe, in the next chapters, methods that fit a parametric model to the\nobservation while introducing unobserved, or \u201clatent,\u201d components in their models,\nwhose inference typically attaches interpretable information or structure to the data.\nWe have seen one such example in the form of the mixture of Gaussian in chapter 4,\nthat we will revisit in chapter 19. We now provide a presentation of the variational\nBayes paradigm that provides a general strategy to address latent variable problems\n[143, 97, 14, 100].\nThe general framework is as follows. Variables in the model are divided in two\ngroups: the observable part, that we denote X, and the latent part, denoted Z. In\nmany models Z represents some unobservable structure, such that X conditional to\nZ has some relatively simple distribution (in a Bayesian estimation context, Z often\ncontains model parameters). The quantity of interest, however, is the conditional\ndistribution of Z given X (also called the \u201cposterior distribution\u201d), which allows one\nto infer the latent structure from the observations, and will also have an important\nrole in maximum likelihood parametric estimation, as we will see below. This condi-\ntional distribution is not always easy to compute or simulate, and variational Bayes\nprovides a framework under which it can be approximated.\n16.2\nVariational principle\nWe consider a pair of random variables X and Z, where X is considered as \u201cob-\nserved\u201d and Z is hidden, or \u201clatent\u201d. We will use U = (X,Z) to denote the two\nvariables taken together. We denote as usual by PU the probability law of U, defined\non RU = RX \u00d7 RZ by PU(A) = P(U \u2208A). We will also assume that there exists a mea-\nsure \u00b5 on RU that decomposes as a product measure \u00b5 = \u00b5X \u00d7 \u00b5Z (where \u00b5X and \u00b5Z\n391\n392\nCHAPTER 16. LATENT VARIABLES AND VARIATIONAL METHODS\nare measures on RX and RZ), such that PU \u226a\u00b5 (\u03c0U is absolutely continuous with\nrespect to \u00b5). This implies that PU has a density with respect to \u00b5 that we will denote\nfU. If both RX and RZ are discrete, \u00b5 is typically the counting measure, and if they\nare both Euclidean space, \u00b5 can be the Lebesgue measure on the product.1\nThe variables X and Z then have probability density functions with respect to \u00b5X\nad \u00b5Z, given by\nfX(x) =\nZ\nRZ\nfU(x,z)\u00b5Z(dz)\nand\nfZ(z) =\nZ\nRX\nfU(x,z)\u00b5X(dx).\nThe conditional distribution of X given Z = z, denoted PX(\u00b7 | Z = z), has density\nfX(x | z) = fU(x,z)/fZ(z) with respect to \u00b5X and that of Z given X = x, denoted PZ(\u00b7 |\nX = x), has density fZ(z | x) = fU(x,z)/fX(x) with respect to \u00b5Z. We will be mainly\ninterested by approximations of PZ(\u00b7 | X = x), assuming that PZ and PX(\u00b7 | Z = z) (and\nhence PU) are easy to compute or simulate.\nWe will use the Kullback-Liebler divergence to quantify the accuracy of the ap-\nproximation. As stated in proposition 4.1, we have\nPZ(\u00b7 | X = x) = argmin\n\u03bd\u2208M1(RZ)\nKL(\u03bd \u2225PZ(\u00b7 | X = x))\nwhere M1(RZ) denotes the set of all probability distributions on RZ. Note that all\ndistributions \u03bd for which KL(\u03bd \u2225\u03c0Z(\u00b7|X = x)) is finite must be absolutely continuous\nwith respect to \u00b5Z and therefore take the form \u03bd = g\u00b5Z. One has\nKL(gd\u00b5Z\u2225PZ(\u00b7|X = x)) =\nZ\nRZ\nlog g(z)\nfZ(z|x)g(z)\u00b5Z(dz)\n=\nZ\nRZ\nlog\ng(z)\nfU(x,z)g(z)\u00b5Z(dz) + logfX(x).\n(16.1)\nWe will denote by P(\u00b5Z), or just P when there is no ambiguity, the set of all p.d.f.\u2019s g\nwith respect to \u00b5Z, i.e., the set of all non-negative measurable functions on RZ with\nR\nRZ g(z)\u00b5Z(dz) = 1.\nThe basic principle of variational Bayes methods is to replace P by a subset b\nP\nand to define the approximation\nbPZ(\u00b7|X = x) = argmin\ng\u2208b\nP\nKL(g\u00b5Z\u2225PZ(\u00b7|X = x)).\n1The reader unfamiliar with measure theory may want to read this discussion by replacing d\u00b5X\nby dx, d\u00b5Z by dz and d\u00b5U by dxdz, i.e., in the context of continuous probability distributions having\np.d.f.\u2019s with respect to the Lebesgue\u2019s measure.\n16.3. EXAMPLES\n393\nFor the approximation to be practical, the set b\nP must obviously be chosen so that\nthe computation of bPZ(\u00b7|X = x) is computationally feasible. We now review a few\nexamples, before passing to the EM algorithm and its approximations.\n16.3\nExamples\n16.3.1\nMode approximation\nAssume that RZ is discrete and \u00b5Z is the counting measure so that\nKL(gd\u00b5Z \u2225PZ(\u00b7 | X = x)) \u2212logfX(x) =\nX\nz\u2208RZ\nlog\ng(z)\nfU(x,z)g(z),\nthe sum being infinite if there exists z such that \u03bd(z) > 0 and fU(x,z) = 0. Take\nb\nP = {1z : z \u2208RZ},\nthe family of all Dirac functions on RZ. Then,\nKL(1z \u2225PZ(\u00b7|X = x)) \u2212logfX(x) = \u2212logfU(x,z).\nThe variational approximation of PZ(\u00b7 | X = x) over b\nP therefore is the Dirac measure\nat point(s) z \u2208RZ at which fU(x,z) is largest, i.e., the mode(s) of the posterior distri-\nbution. This approximation is often called the MAP approximation (for maximum a\nposteriori).\nIf RZ is, say, Rq and \u00b5Z = dz is Lebesgue\u2019s measure, then the previous construc-\ntion does not work because 1z is not a p.d.f. with respect to \u00b5Z. In place of Dirac\nfunctions, one can use constant functions on small balls. Let B(z,\u03f5) denote the open\nball with radius \u03f5, and let |B(z,\u03f5)| denote its volume. Let uz,\u03f5 = 1B(z,\u03f5)/|B(z,\u03f5)|. Fixing\n\u03f5, we can consider the set\nb\nP = \buz,\u03f5 : z \u2208Rq\t.\nNow, one has (leaving the computation to the reader)\nKL(uz,\u03f5dz\u2225PZ(\u00b7|X = x)) \u2212logfX(x) = \u2212log\n \n1\n|B(z,\u03f5)\nZ\nB(z,\u03f5)\nfU(x,z\u2032)dz\u2032\n!\n.\nThe limit for small \u03f5 (assuming that fU(x,\u00b7) is continuous at z, or defining the limit\nup to sets of measure zero) is \u2212logfU(x,z), justifying again choosing the mode of the\nposterior distribution of Z for the approximation.\nThe mode approximation has some limitations. First, it is in general a very crude\napproximation of the posterior distribution. Second, even with the assumption that\nfU has closed form, this p.d.f. is often difficult to maximize (for example when defin-\ning models over large discrete sets). In such cases, the mode approximation has\nlimited practical use.\n394\nCHAPTER 16. LATENT VARIABLES AND VARIATIONAL METHODS\n16.3.2\nGaussian approximation\nLet us still assume that RZ = Rq and that \u00b5Z = dz. Let b\nP be the family of all Gaussian\ndistributions N (m,\u03a3) on Rq. Then, denoting by \u03d5(\u00b7;m,\u03a3) the density of N (m,\u03a3),\nKL(\u03d5(\u00b7; m,\u03a3)\u2225PZ(\u00b7 | X = x)) \u2212logfX(x) = \u2212q\n2 log2\u03c0 \u2212q\n2 \u22121\n2 logdet(\u03a3)\n\u2212\nZ\nRq logfU(x,z)\u03d5(z;m,\u03a3)dz.\nIn order to provide the best approximation, m and \u03a3 must therefore maximize\nZ\nRq logfU(x,z)\u03d5(z;m,\u03a3)dz + 1\n2 logdet(\u03a3).\n(16.2)\nThe resulting optimization problem does not have a closed form solution in general\n(see section 18.2.2 for an example in which stochastic gradient methods are used to\nsolve this problem). Another approach that is commonly used in practice is to push\nthe approximation further by replacing logfU(x,z) by its second order expansion\naround its maximum as a function of z. Let m(x) be the posterior mode, i.e., the\nvalue of z at which x 7\u2192logfU(x,z) is maximal, that we will assume to be unique.\nLet H(x) denote the q \u00d7 q Hessian matrix formed by the second partial derivatives\nof \u2212logfU(x,z) (with respect to z) at z = m(x). This matrix is positive semidefinite\naccording to the choice made for m(x), and we will assume that it is positive definite.\nSince the first derivatives of logfU(x,z) at m(x) must vanish, we have the expansion:\nlogfU(x,z) = logfU(x,m(x)) \u22121\n2(z \u2212m(x))T H(x)(z \u2212m(x)) + \u00b7\u00b7\u00b7\nPlugging the expansion into the integral in (16.2) yields\n\u22121\n2trace(H(x)\u03a3) \u22121\n2(m \u2212m(x))T H(x)(m \u2212m(x)) + 1\n2 logdet\u03a3.\nTo maximize this expression, one must clearly take m = m(x). Moreover,\n\u2202\u03a3 (\u2212trace(H(x)\u03a3) + logdet\u03a3) = \u2212H(x)T + (\u03a3T )\u22121 = \u2212H(x) + \u03a3\u22121,\nand we see that one must take \u03a3 = H(x)\u22121. This provides the Laplace approxima-\ntion [62] of the posterior, N (m(x),H(x)\u22121), which is practical when the mode and\ncorresponding second derivatives are feasible to compute.\n16.3.3\nMean-field approximation\nThis section generalizes the approach discussed in proposition 14.6 for Markov ran-\ndom fields. Assume that RZ can be decomposed into several components R[1]\nZ ,...,R[K]\nZ ,\n16.3. EXAMPLES\n395\nwriting z = (z[1],...,z[K]) (for example, taking K = q and z[i] = z(i), the ith coordinate\nof z if RZ = Rq). Also assume that \u00b5Z splits into a product measure \u00b5[1]\nZ \u2297\u00b7\u00b7\u00b7 \u2297\u00b5[K]\nZ .\nMean-field approximation consists in assuming that probabilities \u03bd in b\nP split into\nindependent components, i.e., their densities g take the form:\ng(z) = g[1](z[1])\u00b7\u00b7\u00b7g[K](z[K]).\nThen,\nKL(\u03bd \u2225PZ(\u00b7 | X = x)) \u2212logfX(x) =\nK\nX\nj=1\nZ\nR[j]\nZ\nlogg[j](z[j])g[j](z[j])\u00b5[j]\nZ (dz[j])\n\u2212\nZ\nRZ\nlogfU(x,z)\nq\nY\nj=1\ng[j](z[j])\u00b5Z(dz).\n(16.3)\nThe mean-field approximation may be feasible when logfU(x,z) can be written as a\nsum of products of functions of each z[j]. Indeed, assume that\nlogfU(x,z) =\nX\n\u03b1\u2208A\nK\nY\nj=1\n\u03c8\u03b1,j(z[j],x)\n(16.4)\nwhere A is a finite set. To shorten notation, let us denote by \u27e8\u03c8\u27e9the expectation of a\nfunction \u03c8 with respect to the product p.d.f. g. Then, (16.3) can be written as\nKL(\u03bd\u2225PZ(\u00b7 | X = x)) \u2212logfX(x) =\nK\nX\nj=1\n\u27e8logg(j)(z[j])\u27e9\u2212\nX\n\u03b1\u2208A\nK\nY\nj=1\n\u27e8\u03c8\u03b1,j(z[j],x)\u27e9.\nThe following lemma will allow us to identify the form taken by the optimal\np.d.f. g[j].\nLemma 16.1 Let Q be a set equipped with a positive measure \u00b5. Let \u03c8 : Q \u2192R be a\nmeasurable function such that\nC\u03c8\n\u2206=\nZ\nQ\nexp(\u03c8(q))\u00b5(dq) < \u221e.\nLet\ng\u03c8(q) = 1\nC\u03c8\nexp(\u03c8(q)).\nLet g be any p.d.f. with respect to \u00b5, and define\nF(g) =\nZ\nQ\n(logg(q) \u2212\u03c8(q))g(q)\u00b5(dq).\nThen F(g\u03c8) \u2264F(g).\n396\nCHAPTER 16. LATENT VARIABLES AND VARIATIONAL METHODS\nProof We note that g\u03c8 > 0, and that\nKL(g\u2225g\u03c8) = F(g) + logC\u03c8 = F(g) \u2212F(g\u03c8),\nwhich proves the result, since KL divergences are always non-negative.\n\u25a0\nApplying this lemma separately to each function g[j] implies that any optimal g\nmust be such that\ng[j](z[j]) \u221dexp\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\nX\n\u03b1\u2208A\nM\u03b1,j\u03c8\u03b1,j(z[j],x)\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\nwith\nM\u03b1,j =\nK\nY\nj\u2032=1,j\u2032,j\n\u27e8\u03c8\u03b1,j\u2032(z[j\u2032],x)\u27e9.\nWe therefore have\n\u27e8\u03c8\u03b1,j(z[j],x)\u27e9=\nR\nR[j]\nZ \u03c8\u03b1,j(z[j],x)exp\n\u0010P\n\u03b1\u2032\u2208A M\u03b1\u2032,j\u03c8\u03b1\u2032,j(z[j],x)\n\u0011\n\u00b5[j]\nZ (dz[j])\nR\nR[j]\nZ exp\n\u0010P\n\u03b1\u2032\u2208A M\u03b1\u2032,j\u03c8\u03b1\u2032,j(z[j],x)\n\u0011\n\u00b5[j]\nZ (dz[j])\n(16.5)\nThis specifies a relationship expressing \u27e8\u03c8\u03b1,j(z[j],x)\u27e9as a function of the other\nexpectations \u27e8\u03c8\u03b1\u2032,j\u2032(z(j\u2032),x)\u27e9for j , j\u2032. These equations put together are called the\nmean-field consistency equations. When these equations can be written explicitly, i.e.,\nwhen the integrals in (16.5) can be evaluated analytically (which is generally the case\nwhen the p.d.f.\u2019s g[j] can be associated with standard distributions), one obtains an\nalgorithm that iterates (16.5) over all \u03b1 and j until stabilization (each step reducing\nthe objective function in (16.3)).\nLet us retrieve the result obtained in proposition 14.6 using the current formal-\nism. Assume that RX finite and RZ = {0,1}L, where L can be a large number, with\nfU(x,z) = 1\nC exp\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\nL\nX\nj=1\n\u03b1j(x)z(j) +\nL\nX\ni,j=1,i<j\n\u03b2ij(x)z(i)z(j)\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8.\nTake K = L, z[j] = z(j). Applying the previous discussion, we see that g[j] must take\nthe form\ng[j](z(j)) =\nexp\n\u0010\n\u03b1j(x)z(j) + P\ni,j \u03b2ij(x)\u27e8z(i)\u27e9z(j)\u0011\n1 + exp\n\u0010\n\u03b1j(x) + P\ni,j \u03b2ij(x)\u27e8z(i)\u27e9\n\u0011\nIn particular\n\u27e8z(j)\u27e9=\nexp\n\u0010\n\u03b1j(x) + P\ni,j \u03b2ij(x)\u27e8z(i)\u27e9\n\u0011\n1 + exp\n\u0010\n\u03b1j(x) + P\ni,j \u03b2ij(x)\u27e8z(i)\u27e9\n\u0011\n16.4. MAXIMUM LIKELIHOOD ESTIMATION\n397\nproviding the mean-field consistency equations.\nIn this special case, it is also possible to express the objective function as a simple\nfunction of the expectations \u27e8z(j)\u27e9\u2019s. We indeed have, letting \u03c1j = \u27e8z(j)\u27e9,\nX\nz\u2208RZ\nlogfU(x,z)\nL\nY\nj=1\ng[j](z(j)) = \u2212logC +\nL\nX\nj=1\n\u03b1j(x)\u03c1j +\nL\nX\ni,j=1,i<j\n\u03b2ij(x)\u03c1i\u03c1j.\nThe values of \u03c11,...,\u03c1L are then obtained by maximizing\nL\nX\nj=1\n\u03b1j(x)\u03c1j +\nL\nX\ni,j=1,i<j\n\u03b2ij(x)\u03c1i\u03c1j \u2212\nL\nX\nj=1\n\u0010\n\u03c1j log\u03c1j + (1 \u2212\u03c1j)log(1 \u2212\u03c1j)\n\u0011\n.\nThe consistency equations express the fact that the derivatives of this expression\nwith respect to each \u03c1j vanish.\n16.4\nMaximum likelihood estimation\n16.4.1\nThe EM algorithm\nWe now consider maximum likelihood estimation with latent variables and use the\nnotation of section 16.2. The main tool is the following obvious consequence of\n(16.1).\nProposition 16.2 One has\nlogfX(x) = max\ng\u2208P(\u00b5Z)\nZ\nRZ\nlog\n fU(x,z)\ng(z)\n!\ng(z)d\u00b5Z(z)\nand the maximum is achieved for g(z) = fZ(z | x), the conditional p.d.f. of Z given X = x.\nProof Equation (16.1) implies that\nZ\nRZ\nlog\n fU(x,z)\ng(z)\n!\ng(z)d\u00b5Z(z) = logfX(x) \u2212KL(g \u00b5Z\u2225PZ(\u00b7|X = x))\nand the r.h.s. is indeed maximum when the Kullback-Liebler divergence vanishes,\nthat is, when g is the p.d.f. of PZ(\u00b7 | X = x).\n\u25a0\nWe will use this proposition for the derivation of the expectation-maximization\n(or EM) algorithm for maximum likelihood with latent variables. We now assume\nthat PU, and therefore fU, is parametrized by \u03b8 \u2208\u0398, and that a training set T =\n398\nCHAPTER 16. LATENT VARIABLES AND VARIATIONAL METHODS\n(x1,...,xN) of X is observed. To indicate the dependence in \u03b8, we will write fU(x,z; \u03b8),\nor fZ(z | x; \u03b8). The maximum likelihood estimator (m.l.e.) then maximizes\n\u2113(\u03b8) =\nX\nx\u2208T\nlogfX(x; \u03b8).\nThe EM algorithm is useful when the computation of the m.l.e. for complete obser-\nvations, i.e., the maximization of\nlogfU(x,z; \u03b8)\nwhen both x and z are given, is easy, whereas the same problem with the marginal\ndistribution is hard.\nFrom the proposition, we have:\nX\nx\u2208T\nlogfX(x; \u03b8) =\nX\nx\u2208T\nmax\ngx\u2208P(\u00b5Z)\nZ\nRZ\nlog\n fU(x,z; \u03b8)\ngx(z)\n!\ngx(z)\u00b5Z(dz)\nTherefore the maximum likelihood requires to compute\nmax\n\u03b8,gx,x\u2208T\nX\nx\u2208T\nZ\nRZ\nlog\n fU(x,z; \u03b8)\ngx(z)\n!\ngx(z)\u00b5Z(dz).\n(16.6)\nThe maximization can therefore be done by iterating the following two steps.\n1. Given \u03b8n, compute\nargmax\ngx,x\u2208T\nX\nx\u2208T\nZ\nRZ\nlog\n fU(x,z; \u03b8)\ngx(z)\n!\ngx(z)\u00b5Z(dz).\n2. Given g1,...,gN, compute\nargmax\n\u03b8\nX\nx\u2208T\nZ\nRZ\nlog\n fU(x,z; \u03b8)\ngx(z)\n!\ngx(z)\u00b5Z(dz)\n= argmax\n\u03b8\nX\nx\u2208T\nZ\nRZ\nlog(fU(x,z; \u03b8))gx(z)\u00b5Z(dz).\nStep 1. is explicit and its solution is gx(z) = fZ(z | x; \u03b8). Using this, both steps can\nbe grouped together, yielding the EM algorithm.\n16.4. MAXIMUM LIKELIHOOD ESTIMATION\n399\nAlgorithm 16.1 (EM algorithm)\nLet a statistical model with density fU(x,z; \u03b8) modeling an observable variable X\nand a latent variable Z be given, and a training set T = (x1,...,xN). Starting with an\ninitial guess of the parameter, \u03b8(0), the EM algorithm iterate the following equation\nuntil numerical stabilization, .\n\u03b8n+1 = argmax\n\u03b8\u2032\nX\nx\u2208T\nZ\nRZ\nlog(fU(x,z; \u03b8\u2032))fZ(z | x; \u03b8n)\u00b5Z(dz).\n(16.7)\nEquation (16.7) maximizes (in \u03b8\u2032) a function defined as an expectation (for \u03b8n), jus-\ntifying the name \u201dExpectation-Maximization.\u201d\n16.4.2\nApplication: Mixtures of Gaussian\nA mixture of Gaussian (MoG) model was introduced in chapter 4 ((4.4)). We now\nreinterpret it (in a slightly generalized version) as a model with partial observations\nand show how the EM algorithm can be applied. Let \u03d5(x; m,\u03a3) denote the p.d.f. of\nthe d-dimensional multivariate Gaussian distribution with mean m and covariance\nmatrix \u03a3. We model fX(x; \u03b8) as\nfX(x; \u03b8) =\np\nX\nj=1\n\u03b1j\u03d5(x,; cj,\u03a3j).\nHere, \u03b8 contains all sequences \u03b11,...,\u03b1p (non-negative numbers that sum to one),\nc1,...,cp \u2208Rd and \u03a31,...,\u03a3p (d \u00d7 d positive definite matrices).\nUsing the previous notation, we therefore have RX = Rd, and \u00b5X the Lebesgue\nmeasure on that space. The variable Z will take values in RZ = {1,...,p}, with \u00b5Z\nbeing the counting measure. We model the joint density function for (X,Z) as\nfU(x,z; \u03b8) = \u03b1z\u03d5(x; cz,\u03a3z).\n(16.8)\nClearly fX is the marginal p.d.f. of fU. One can therefore consider Z as a latent\nvariable, and therefore estimate \u03b8 using the EM algorithm.\nWe now make (16.7) explicit for mixtures of Gaussian. For given \u03b8 and \u03b8\u2032 and\nx \u2208R, let\nUx(\u03b8,\u03b8\u2032) = d\n2 log2\u03c0 +\nZ\nRZ\nlog(fU(x,z; \u03b8\u2032))fZ(z|x; \u03b8)d\u00b5Z(z)\n=\np\nX\nz=1\n\u0012\nlog\u03b1\u2032\nz \u22121\n2 logdet\u03a3\u2032\nz \u22121\n2(x \u2212c\u2032\nz)T \u03a3\u2032\nz\n\u22121(x \u2212c\u2032\nz)\n\u0013\nfZ(z|x; \u03b8)\n400\nCHAPTER 16. LATENT VARIABLES AND VARIATIONAL METHODS\nwith\nfZ(z | x; \u03b8) =\n(det\u03a3z)\u22121\n2\u03b1ze\u22121\n2(x\u2212cz)T \u03a3\u22121\nz (x\u2212cz)\nPp\nj=1(det\u03a3j)\u22121\n2\u03b1je\u22121\n2(x\u2212cj)T \u03a3\u22121\nj (x\u2212cj).\nIf \u03b8n is the current parameter in the EM, the next one, \u03b8n+1 must maximize\nPN\nx\u2208T Ux(\u03b8n,\u03b8\u2032). This can be solved in closed form. To compute \u03b1\u2032\n1,...,\u03b1\u2032\np, one must\nmaximize\nX\nx\u2208T\np\nX\nz=1\n(log\u03b1\u2032\nz)fZ(z|x; \u03b8)\nsubject to the constraint that P\nz \u03b1\u2032\nz = 1. This yields\n\u03b1\u2032\nz =\nX\nx\u2208T\nfZ(z|x; \u03b8)\n.\np\nX\nj=1\nX\nx\u2208T\nfZ(j|x; \u03b8) = \u03b6z / N\nwith \u03b6z = P\nx\u2208T fZ(z|x; \u03b8).\nThe centers c\u2032\n1,...,c\u2032\np must minimize P\nx\u2208T (x \u2212c\u2032\nz)T \u03a3\u2032\nz\n\u22121(x \u2212c\u2032\nz)fZ(z|x; \u03b8), which\nyields\nc\u2032\nz = 1\n\u03b6z\nX\nx\u2208T\nxfZ(z|x; \u03b8).\nFinally, \u03a3\u2032\nz must minimize\n\u03b6z\n2 logdet\u03a3\u2032\nz + 1\n2\nX\nx\u2208T\n(x \u2212c\u2032\nz)T \u03a3\u2032\nz\n\u22121(x \u2212c\u2032\nz)fZ(z|x; \u03b8),\nwhich yields\n\u03a3\u2032\nz = 1\n\u03b6z\nX\nx\u2208T\n(x \u2212c\u2032\nz)(x \u2212c\u2032\nz)T fZ(z|x; \u03b8).\nWe can now summarize the algorithm.\nAlgorithm 16.2 (EM for Mixture of Gaussian distributions)\n1. Initialize the parameter \u03b8(0) = (\u03b1(0),c(0),\u03a3(0)). Choose a small constant \u03f5 and\na maximal number of iterations M.\n2. At step n of the algorithm, let \u03b8 = \u03b8(n) be the current parameter, writing for\nshort \u03b8 = (\u03b1,c,\u03a3).\n3. Compute, for x \u2208T and i = 1,...,p\nfZ(i | x; \u03b8) =\n(det\u03a3i)\u22121\n2\u03b1ie\u22121\n2(x\u2212ci)T \u03a3\u22121\ni (x\u2212ci)\nPp\nj=1(det\u03a3j)\u22121\n2\u03b1je\u22121\n2(x\u2212cj)T \u03a3\u22121\nj (x\u2212cj)\nand let \u03b6i = P\nx\u2208T fZ(i|x; \u03b8), i = 1,...,p.\n16.4. MAXIMUM LIKELIHOOD ESTIMATION\n401\n4. Let \u03b1\u2032\ni = \u03b6i/N.\n5. For i = 1,...,p, let\nc\u2032\ni = 1\n\u03b6i\nX\nx\u2208T\nxfZ(i | x; \u03b8).\n6. For i = 1,...,p, let\n\u03a3\u2032\ni = 1\n\u03b6i\nX\nx\u2208T\n(x \u2212c\u2032\ni)(x \u2212c\u2032\ni)T fZ(i | x; \u03b8).\n7. Let \u03b8\u2032 = (\u00b5\u2032,c\u2032,\u03a3\u2032). If |\u03b8\u2032 \u2212\u03b8| < \u03f5 or n + 1 = M: return \u03b8\u2032 and exit the algorithm.\n8. Set \u03b8(n + 1) = \u03b8\u2032 and return to step 2.\nRemark 16.3 Algorithm 16.2 can be simplified by making restrictions on the model.\nHere are some examples.\n(i) One may restrict to \u03a3i = \u03c32\ni IdRd to reduce the number of free parameters. Then,\nStep 7 of the algorithm needs to be replaced by:\n(\u03c3\u2032\ni )2 = 1\nd\u03b6i\nX\nx\u2208T\n|x \u2212c\u2032\ni|2fZ(i | x; \u03b8).\n(ii) Alternatively, the model may be simplified by assuming that all covariance ma-\ntrices coincide: \u03a3i = \u03a3 for i = 1,...,p. Then, Step 7 becomes\n\u03a3\u2032\ni = 1\nN\np\nX\ni=1\nX\nx\u2208T\n(x \u2212c\u2032\ni)(x \u2212c\u2032\ni)T fZ(i | x; \u03b8).\n\u2666\n(iii) Finally, one may assume that \u03a3 is known and fixed in the algorithm (usually in\nthe form \u03a3 = \u03c32IdRd for some \u03c3 > 0) so that Step 7 of the algorithm can be removed.\n(iv) One may also assume also that the (prior) class probabilities are known, typi-\ncally set to \u03b1i = 1/p for all i, so that Step 4 can be skipped.\n16.4.3\nStochastic approximation EM\nThe stochastic approximation EM (or SAEM) algorithm has been proposed by De-\nlyon et al. [58] (see this reference for convergence results) to address the situation in\nwhich the expectations for the posterior distribution cannot be computed in closed\nform, but can be estimated using Monte-Carlo simulations. SAEM uses a special\n402\nCHAPTER 16. LATENT VARIABLES AND VARIATIONAL METHODS\nform of stochastic approximation, different from the SGD algorithm described in\nsection 3.3. It updates, at each step n, an approximate objective function that we\nwill denote \u03bbn and a current parameter \u03b8(n). It implements the following iterations:\n\uf8f1\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f2\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f3\n\u03be(x)\nn+1 \u223cPZ(\u00b7 | X = x; \u03b8n),\nx \u2208T\n\u03bbn+1(\u03b8\u2032) =\n\u0012\n1 \u2212\n1\nn + 1\n\u0013\n\u03bbn(\u03b8\u2032) +\n1\nn + 1\n\u0012X\nx\u2208T\nlogfU(x,\u03be(x)\nn+1 ; \u03b8\u2032) \u2212\u03bbn(\u03b8\u2032)\n\u0013\n, \u03b8\u2032 \u2208\u0398\n\u03b8n+1 = argmax\n\u03b8\u2032\n\u03bbn+1(\u03b8\u2032)\n(16.9)\nThe second step means that\n\u03bbn(\u03b8\u2032) =\nN\nX\nx\u2208T\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\n1\nn\nn\nX\nj=1\nlogfU(x,\u03be(x)\nj\n; \u03b8\u2032)\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8.\nGiven that \u03be(x)\nn+1 \u223cPZ(\u00b7 | X = x; \u03b8n), one expects this expression to approximate\nX\nx\u2208T\nZ\nRZ\nlog(fU(x,z; \u03b8\u2032))fZ(z | x; \u03b8)d\u00b5Z(z)\nso that the third step of (16.9) can be seen as an approximation of (16.7). Suffi-\ncient conditions under which this actually happens (and \u03b8(n) converges to a local\nmaximizer of the likelihood) are provided in Delyon et al. [58] (see also Kuhn and\nLavielle [112] for a convergence result under more general hypotheses on how \u03be is\nsimulated).\nTo be able to run this algorithm efficiently, one needs the simulation of the pos-\nterior distribution to be feasible. Importantly, one also needs to be able to update\nefficiently the function \u03bbn. This can be achieved when the considered model belongs\nto an exponential family, which corresponds to assuming that the p.d.f. of U takes\nthe form\nfU(x,z; \u03b8) =\n1\nC(\u03b8) exp\n\u0010\n\u03c8(\u03b8)T H(x,z)\n\u0011\nfor some functions \u03c8 and H. For example, the MoG model of equation (4.4) takes\n16.4. MAXIMUM LIKELIHOOD ESTIMATION\n403\nthis form, with\n\u03c8(\u03b8)T =\n\u0012\nlog\u03b11 \u22121\n2mT\n1 \u03a3\u22121\n1 m1 \u22121\n2 logdet\u03a31,...,log\u03b1p \u22121\n2mT\np \u03a3\u22121\np mp \u22121\n2 logdet\u03a3p,\n\u03a3\u22121\n1 m1,...,\u03a3\u22121\np mp,\n\u03a3\u22121\n1 ,...,\u03a3\u22121\np\n\u0013\n,\nH(x,z)T =\n\u0012\n1z=1,...,1z=p,\nx1z=1,...,x1z=p,\n\u22121\n2xxT 1z=1,...,\u22121\n2xxT 1z=p\n\u0013\nand C(\u03b8) = (2\u03c0)pd/2.\nFor such a model, we can replace the algorithm in (16.9) by the more manageable\none:\n\uf8f1\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f2\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f3\n\u03be(x)\nn+1 \u223cPZ(\u00b7 | X = x; \u03b8n), x \u2208T\n\u03b7(x)\nn+1 =\n\u0012\n1 \u2212\n1\nn + 1\n\u0013\n\u03b7(x)\nn +\n1\nn + 1(H(x,\u03be(x)\nn+1) \u2212\u03b7(x)\nn )\n\u03bbn+1(\u03b8\u2032) = \u03c8(\u03b8\u2032)T \u0012X\nx\u2208T\n\u03b7(x)\nn+1\n\u0013\n\u2212logC(\u03b8\u2032)\n\u03b8n+1 = argmax\n\u03b8\u2032\n\u03bbn+1(\u03b8\u2032)\n(16.10)\nWe leave as an exercise the computation leading to the implementation of this algo-\nrithm for mixtures of Gaussian.\n16.4.4\nVariational approximation\nReturning to proposition 16.2 and (16.6), we see that one can make a variational\napproximation of the maximum likelihood by computing\nmax\n\u03b8\u2208\u0398,gx\u2208b\nP,x\u2208T\nX\nx\u2208T\nZ\nRZ\nlog\n fU(x,z; \u03b8)\ngx(z)\n!\ngx(z)\u00b5Z(dz),\n(16.11)\nwhere b\nP \u2282P is a class of p.d.f. with respect to \u00b5Z. The resulting algorithm is then\nimplemented by iterating the computation of gx, x \u2208T, using approximations sim-\nilar to those provided in section 16.3, and maximization in \u03b8 for given gx, x \u2208T.\nThis variational approximation of the maximum likelihood estimator is therefore\nprovided by the following algorithm.\n404\nCHAPTER 16. LATENT VARIABLES AND VARIATIONAL METHODS\nAlgorithm 16.3 (Variational Bayes approximation of the m.l.e.)\nLet a statistical model with density fU(x,z; \u03b8) modeling an observable variable X\nand a latent variable Z be given, and a training set T = (x1,...,xN) be observed. Let\nb\nP be a set of p.d.f. on RZ and define\nbg(\u00b7; x,\u03b8) = argmin\ng\u2208b\nP\nZ\nRZ\nlog\n \ng(z)\nfU(x,z; \u03b8)\n!\ng(z)\u00b5Z(dz)\n(assuming that this minimizer is uniquely defined).\nStarting with an initial guess of the parameter, \u03b80, iterate the following equation\nuntil numerical stabilization:\n\u03b8(n + 1) = argmax\n\u03b8\u2032\nX\nx\u2208T\nZ\nRZ\nlog(fU(x,z; \u03b8\u2032))bg(z|x; \u03b8(n))\u00b5Z(dz).\n(16.12)\nAssume that the distributions in b\nP are also parametrized, denoting their param-\neter by \u03b7, belonging to some Euclidean domain H. Let g(\u00b7;\u03b7) denote the p.d.f. in\nb\nP with parameter \u03b7. Letting \u03b7 = (\u03b7x,x \u2208T) denote an element of HT (parameters\nin H indexed by elements of the training set), (16.11) can then be written as the\nmaximization of\nF(\u03b8,\u03b7) =\nX\nx\u2208T\nZ\nRZ\nlog\n fU(x,z; \u03b8)\ng(z;\u03b7x)\n!\ng(z;\u03b7x)\u00b5Z(dz).\n(16.13)\nThis expression is amenable to a stochastic gradient ascent implementation. We\nhave\n\u2202\u03b8\nZ\nRZ\nlog\n fU(x,z; \u03b8)\ng(z;\u03b7x)\n!\ng(z;\u03b7x)\u00b5Z(dz) =\nZ\nRZ\n\u2202\u03b8 logfU(x,z; \u03b8)g(z;\u03b7x)\u00b5Z(dz)\nand\n\u2202\u03b7x\nZ\nRZ\nlog\n fU(x,z; \u03b8)\ng(z;\u03b7x)\n!\ng(z;\u03b7x)\u00b5Z(dz)\n=\nZ\nRZ\n \n\u2212\u2202\u03b7 logg(z;\u03b7x)g(z;\u03b7x) + log\n fU(x,z; \u03b8)\ng(z;\u03b7x)\n!\n\u2202\u03b7g(z;\u03b7x)\n!\n\u00b5Z(dz)\n=\nZ\nRZ\nlog\n fU(x,z; \u03b8)\ng(z;\u03b7x)\n!\n\u2202\u03b7 logg(z;\u03b7x)g(z;\u03b7x)\u00b5Z(dz)\nHere, we have used the fact that, for all \u03b7,\nZ\nRZ\n\u2202\u03b7 logg(z;\u03b7)g(z;\u03b7)\u00b5Z(dz) =\nZ\nRZ\n\u2202\u03b7g(z;\u03b7)\u00b5Z(dz) = 0\n16.5. REMARKS\n405\nsince\nR\nRZ g(x,\u03b7)\u00b5Z(dz) = 1.\nDenote by \u03c0\u03b7 the probability distribution of the random variable Z taking val-\nues in R|T|\nZ obtained by sampling Z = (Zx,x \u2208T ) such that the components Zx are\nindependent and with p.d.f. g(\u00b7;\u03b7x) with respect to \u00b5Z. Define\n\u03a61(\u03b8,z) =\nX\nx\u2208T\n\u2202\u03b8 logfU(x,zx ; \u03b8)\nand\n\u03a62(theta,\u03b7,z) =\nX\nx\u2208T\nlog\n fU(x,z; \u03b8)\ng(z;\u03b7x)\n!\n\u2202\u03b7 logg(zx;\u03b7x).\nThen, following section 3.3, one can maximize (16.13) using the algorithm\n(\u03b8n+1 = \u03b8n + \u03b3n+1\u03a61(\u03b8n,Z n+1)\n\u03b7n+1 = \u03b7n + \u03b3n+1\u03a62(\u03b8n,\u03b7n,Z n+1)\n(16.14)\nwhere Z n+1 \u223c\u03c0\u03b7n.\nAlternatively (for example when T is large), one can also sample from x \u2208T at\neach update. This would require defining \u03c0\u03b7 as the distribution on T \u00d7RZ with p.d.f.\n\u03d5\u03b7(x,z) = g(z;\u03b7x)/N, where N = |T|. One can now use\n\u03a61(\u03b8,x,z) = \u2202\u03b8 logfU(x,z; \u03b8)\nand\n\u03a62(\u03b8,\u03b7,z) = log\n fU(x,z; \u03b8)\ng(z;\u03b7)\n!\n\u2202\u03b7 logg(z;\u03b7),\none can use\n\uf8f1\uf8f4\uf8f4\uf8f4\uf8f4\uf8f2\uf8f4\uf8f4\uf8f4\uf8f4\uf8f3\n\u03b8n+1 = \u03b8n + \u03b3n+1\u2202\u03b8 logfU(Xn+1,Zn+1 ; \u03b8n)\n\u03b7n+1,Xn+1 = \u03b7n,Xn+1 + \u03b3n+1 log\n fU(Xn+1,Zn+1 ; \u03b8n)\ng(Zn+1;\u03b7n,Xn+1)\n!\n\u2202\u03b7 logg(Zn+1;\u03b7n,Xn+1)\n(16.15)\nwith (Xn+1,Zn+1) \u223c\u03c0\u03b7n. Sampling from a single training sample at each step can be\nreplaced by sampling from a minibatch with obvious modifications.\n16.5\nRemarks\n16.5.1\nVariations on the EM\nBased on the formulation of the EM as the solution of (16.6), it should be clear that\nsolving (16.7) at each step can be replaced by any update of the parameter that in-\ncreases (16.6). For example, (16.7) can be replaced by a partial run of a gradient\n406\nCHAPTER 16. LATENT VARIABLES AND VARIATIONAL METHODS\nascent algorithm, stopped before convergence. One can also use a coordinate as-\ncent strategy. Assume that \u03b8 can be split into several components, say two, so that\n\u03b8 = (\u03b8(1),\u03b8(2)). Then, (16.7) may then be split into\n\u03b8(1)\nn+1 = argmax\n\u03b8(1)\nX\nx\u2208T\nZ\nRZ\nlog\n\u0012\nfU(x,z; \u03b8(1),\u03b8(2)\nn )\n\u0013\nfZ(z | x; \u03b8(n))\u00b5Z(dz)\n\u03b8(2)\nn+1 = argmax\n\u03b8(2)\nX\nx\u2208T\nZ\nRZ\nlog\n\u0012\nfU(x,z; \u03b8(1)\nn+1,\u03b8(2))\n\u0013\nfZ(z | x; \u03b8(n))\u00b5Z(dz).\nDoing so is, in particular, useful when both these steps are explicit, but not (16.7).\n16.5.2\nDirect minimization\nWhile the EM algorithm is widely used in the context of partial observations, it is\nalso possible to make explicit the derivative of\nlogfX(x; \u03b8) = log\nZ\nRZ\nfU(x,z; \u03b8)\u00b5Z(dz)\nwith respect to the parameter \u03b8. Indeed, differentiating the integral and writing\n\u2202\u03b8fU = fU\u2202\u03b8 logfU, we have\n\u2202\u03b8 logfX(x; \u03b8) =\nZ\nRZ\n\u2202\u03b8 logfU(x,z; \u03b8)fU(x,z; \u03b8)\nfX(x; \u03b8) \u00b5Z(dz)\n=\nZ\nRZ\n\u2202\u03b8 logfU(x,z; \u03b8)fZ(z|x,\u03b8)\u00b5Z(dz).\nIn other terms, the derivative of the log-likelihood of the observed data is the con-\nditional expectation of the derivative of the log-likelihood of the full data given\nthe observed data. When computable, this expression can be used with standard\ngradient-based optimization methods, such as those described in chapter 3. This\nexpression is also amenable to a stochastic gradient ascent algorithm, namely\n\u03b8n+1 = \u03b8n + \u03b3n+1\nX\nx\u2208T\n\u2202\u03b8fU(x,Zn+1,x,\u03b8n)\n(16.16)\nwhere Zn+1,x follows the distribution with density fZ(\u00b7|x,\u03b8n) with respect to \u00b5Z. An\nalternative SGA implementation can use the discussion in section 16.4.4, with the\ndensity g\u03b7x replaces by fZ(\u00b7|x,\u03b7x), which leads to\n\uf8f1\uf8f4\uf8f4\uf8f4\uf8f2\uf8f4\uf8f4\uf8f4\uf8f3\n\u03b8n+1 = \u03b8n + \u03b3n+1\nX\nx\u2208T\n\u2202\u03b8 logfU(x,Zn+1,x,\u03b8n)\n\u03b7n+1,x = \u03b7n,x \u2212\u03b3n+1\u2202\u03b7x logfZ(Zn+1,x |x,\u03b7x),\nx \u2208T\nwhere Zn+1,x follows the distribution with density fZ(\u00b7|x,\u03b7n,x).\n16.5. REMARKS\n407\n16.5.3\nProduct measure assumption\nWe have worked, in this chapter, under the assumption that \u03c0U was absolutely con-\ntinuous with respect to a product measure \u00b5U = \u00b5X \u2297\u00b5Z. This is not a mild as-\nsumption, as it fails to include some important cases, for example when X and Z\nhave some deterministic relationship, the simplest instance being when X = F(Z)\nfor some function F. In many cases, however, one can make simple transformations\non the model that will make it satisfy this working assumption. For example, if\nX = F(Z), one can generally split Z into Z = (Z(1),Z(2)) so that the equation X = F(Z)\nis equivalent to Z(2) = G(X,Z(1)) for some function G. One can then apply the dis-\ncussion above to U = (X,Z(1)) instead of U = (X,Z).\nIf one is ready to step further into measure theoretic concepts, however, one can\nsee that this product decomposition assumption was in fact unnecessary. Indeed,\none can assume that the measure \u00b5U can \u201cdisintegrate\u201d in the following sense: there\nexists, a measure \u00b5X on RX and, for all x \u2208RX, a measure \u00b5Z(\u00b7|x) on RZ such that,\nfor all functions \u03c8 defined on RU,\nZ\nRU\n\u03c8(x,z)\u00b5U(dx,dz) =\nZ\nRX\nZ\nRZ\n\u03c8(x,z)\u00b5Z(dz|x)\u00b5X(dx).\nThis is in fact a fairly general situation [34] as soon as one assumes that \u00b5U(R) is\nfinite (which is not a real loss of generality as one can reduce to this case by replacing\nif needed \u00b5U by an equivalent probability distribution).\nWith this assumption, the marginal distribution of X had a p.d.f. with respect to\n\u00b5X given by\nfX(x) =\nZ\nRZ\nfU(x,z)\u00b5Z(dz|x)\nand the conditional distributions PZ(\u00b7 | x) have a p.d.f. relative to \u00b5Z(\u00b7 | x) given by\nfZ(z | x) = fU(x,z)\nfX(x) .\nThe computations and approximations made earlier in this chapter can then be ap-\nplied with essentially no modification.\n408\nCHAPTER 16. LATENT VARIABLES AND VARIATIONAL METHODS\nChapter 17\nLearning Graphical Models\nWe discuss, in this chapter, several methods designed to learn parameters of graph-\nical models, starting with the somewhat simpler case of Bayesian networks, than\npassing to Markov random fields on loopy graphs.\n17.1\nLearning Bayesian networks\n17.1.1\nLearning a Single Probability\nSince Bayesian networks are specified by probabilities and conditional probabilities\nof configurations of variables, we start with a discussion of the basic problem of\nestimating discrete probability distributions.\nThe obvious way to estimate the probability of an event A based on a series of N\nindependent experiments is by using relative frequencies\nfA = #{A occurs}\nN\n.\nThis estimation is unbiased (E(fA) = P(A)) and its variance is P(A)(1\u2212P(A))/N. This\nimplies that the relative error \u03b4A = fA/P(A) \u22121 has zero mean and variance\n\u03c32 = 1 \u2212P(A)\nNP(A) .\nThis number can clearly become very large when P(A) \u22430. In particular, when\nP(A) is small compared to 1/N, the relative frequency will often be fA = 0, leading\nto the false conclusion that A is not just rare, but impossible. If there are reasons to\nexpect beforehand that A is indeed possible, it is important to inject this prior belief\nin the procedure, which suggest using Bayesian estimation methods.\n409\n410\nCHAPTER 17. LEARNING GRAPHICAL MODELS\nThe main assumption for these methods is to consider the unknown probability,\np = P(A), as a random variable, yielding a generative process in which a random\nprobability is first obtained, and then N instances of A or not-A are generated using\nthis probability.\nAssume that the \u201cprior distribution\u201d of p (which determines a prior belief) has\na p.d.f. q (with respect to Lebesgue\u2019s measure) on the unit interval. Given on N in-\ndependent observations of occurrences of A, each following a Bernoulli distribution\nb(p), the joint likelihood of all involved variables is given by\n N\nk\n!\npk(1 \u2212p)N\u2212kq(p),\nwhere k is the number of times the event A has been observed.\nThe conditional density of p given the observation (k occurrences of A) is called\nthe posterior distribution. Here, it is given by\nq(p | k) = q(p)\nCk\npk(1 \u2212p)N\u2212k\nwhere Ck is a normalizing constant. If there was no specific prior knowledge on p\n(so that q(p) = 1), the resulting distribution is a beta distribution with parameters\nk + 1 and N \u2212k + 1, the beta distribution being defined as follows.\nDefinition 17.1 The beta distribution with parameters a and b (abbreviated \u03b2(a,b)) has\ndensity with respect to Lebesgue\u2019s measure\n\u03c1(t) = \u0393(a + b)\n\u0393(a)\u0393(b)ta\u22121(1 \u2212t)b\u22121 if t \u2208[0,1]\nand \u03c1(t) = 0 otherwise, with\n\u0393(x) =\nZ \u221e\n0\ntx\u22121e\u2212tdt.\nFrom the definition of a beta distribution, it is clear also that, if we choose the\nprior to be \u03b2(a + 1,\u03bd \u2212a + 1) then the posterior is \u03b2(k + a + 1,N + \u03bd \u2212(k + a) + 1).\nThe posterior therefore belongs to the same family of distributions as the prior, and\none says that the beta distribution is a conjugate prior for the binomial distribution.\nThe mode of the posterior distribution (which is the maximum a posteriori (MAP)\nestimator) is given by\n\u02c6p = k + a\nN + \u03bd .\nThis estimator now provides a positive value even if k = 0. By selecting a and \u03bd,\none therefore includes the prior belief that p is positive.\n17.1. LEARNING BAYESIAN NETWORKS\n411\n17.1.2\nLearning a Finite Probability Distribution\nNow assume that F is a finite space and that we want to estimate a probability distri-\nbution p = (p(x),x \u2208F) using a Bayesian approach as above. We cannot use the previ-\nous approach to estimate each p(x) separately, since these probabilities are linked by\nthe fact that they sum to 1. We can however come up with a good (conjugate) prior,\nidentified, as done above, by computing the posterior associated to a uniform prior\ndistribution.\nLetting Nx be the number of times x \u2208F is observed among N independent sam-\nples of a random variable X with distribution PX(\u00b7) = p(\u00b7), the joint distribution of\n(Nx,x \u2208F) is multinomial, given by\nP(Nx,x \u2208F | p(\u00b7)) =\nN!\nQ\nx\u2208F Nx!\nY\nx\u2208F\np(x)Nx.\nThe posterior distribution of p(\u00b7) given the observations with a uniform prior is pro-\nportional to Q\nx\u2208F p(x)Nx. It belongs to the family of Dirichlet distributions, described\nin the following definition.\nDefinition 17.2 Let F be a finite set and SF be the simplex defined by\nSF =\n\uf8f1\uf8f4\uf8f4\uf8f2\uf8f4\uf8f4\uf8f3(p(x),x \u2208F) : p(x) \u22650,x \u2208F and\nX\nx\u2208F\np(x) = 1\n\uf8fc\uf8f4\uf8f4\uf8fd\uf8f4\uf8f4\uf8fe.\nThe Dirichlet distribution with parameters a = (a(x),x \u2208F) (abbreviated Dir(a)) has den-\nsity\n\u03c1(p(\u00b7)) =\n\u0393(\u03bd)\nQ\nx\u2208F \u0393(a(x))\nY\nx\u2208F\np(x)a(x)\u22121, if x \u2208SF\nand 0 otherwise, with \u03bd = P\nx\u2208F a(x).\nNote that, if F has cardinality 2, the Dirichlet distribution coincides with the beta\ndistribution. Similarly to the beta for the binomial, and almost by construction, the\nDirichlet distribution is a conjugate prior for the multinomial. More precisely, if the\nprior distribution for p(\u00b7) is Dir(1+a(x),x \u2208F), then the posterior after N observations\nof X is Dir(1 + Nx + a(x),x \u2208F), and the MAP estimator is given by\n\u02c6p(x) = Nx + a(x)\nN + \u03bd\nwith \u03bd = P\nx\u2208F a(x).\n412\nCHAPTER 17. LEARNING GRAPHICAL MODELS\n17.1.3\nConjugate Prior for Bayesian Networks\nWe now consider a Bayesian network on the set F (V ) containing configurations x =\n(x(s),s \u2208V ) with x(s) \u2208Fs. We want to estimate the conditional probabilities in the\nrepresentation\nP(X = x) =\nY\ns\u2208V\nps(x(pa(s)),x(s)).\nAssume that N independent observations of X have been made. Define the counts\nNs(x(s),x(pa(s))) to be the number of times the observation x({s}\u222apa(s)) has been made.\nThen, it is straightforward to see that, assuming a uniform prior for the ps, their\nposterior distribution is proportional to\nY\ns\u2208V\nY\nx(pa(s))\u2208Fpa(s)\nY\nx(s)\u2208Fs\nps(x(pa(s)),x(s))Ns(x(s),x(s\u2212)).\nThis implies that, for the posterior distribution, the conditional probabilities\nps(x(pa(s)),\u00b7) are independent and follow a Dirichlet distribution with parameters\n1 + Ns(x(s),x(pa(s))), x(s) \u2208Fs.\nSo, independent Dirichlet distributions indexed by configurations of parents of\nnodes provide a conjugate prior for the general Bayesian network model. This prior\nis specified by a family of positive numbers\n\u0010\nas(x(s),x(pa(s))),s \u2208V ,x(s) \u2208Fs,x(pa(s)) \u2208F (pa(s))\n\u0011\n,\n(17.1)\nyielding a prior probability proportional to\nY\ns\u2208V\nY\nx(pa(s))\u2208Fpa(s)\nY\nx(s)\u2208Fs\nps(x(pa(s)),x(s))as(x(s),x(s\u2212))\u22121.\nand a MAP estimator\n\u02c6ps(x(pa(s)),x(s)) = Ns(x(s),x(pa(s))) + as(x(s),x(s\u2212))\nNs(x(s\u2212)) + \u03bds(x(s\u2212))\n(17.2)\nwhere Ns(x(pa(s))) = P\nx(s)\u2208Fs Ns(x(s),x(pa(s))) and \u03bds(x(pa(s))) = P\nx(s)\u2208Fs as(x(s),x(pa(s))).\nOne can restrict the huge class of coefficients described by (17.1) to a smaller\nclass by imposing the following condition.\nDefinition 17.3 One says that the family of coefficients\na = (as(x(s),x(pa(s))),s \u2208V ,x(s) \u2208Fs,x(pa(s)) \u2208F (pa(s))),\nis consistent if there exists a positive scalar \u03bd and a probability distribution P\u2032 on F (V )\nsuch that\nas(x(s),x(pa(s))) = \u03bdP\u2032\n{s}\u222apa(s)(x({s}\u222apa(s))).\n17.1. LEARNING BAYESIAN NETWORKS\n413\nThe class of products of Dirichlet distributions with consistent families of coeffi-\ncients still provides a conjugate prior for Bayesian networks (the proof being left to\nthe reader). Within this class, the simplest choice (and most natural in the absence\nof additional information) is to assume that P\u2032 is uniform, so that\nas(x(s),x(pa(s))) =\n\u03bd\u2032\n|F ({s} \u222apa(s))|.\n(17.3)\nWith this choice, \u03bd\u2032 is the only parameter that needs to be specified. It is often called\nthe equivalent sample size for the prior distribution.\nWe can see from (17.2) that using a prior distribution is quite important for\nBayesian networks, since, when the number of parents increases, some configura-\ntions on F (pa(s)) may not be observed, resulting in an undetermined value for the\nratio\nNs(x(s),x(pa(s)))/Ns(x(s\u2212)),\neven though, for the estimated model, the probability of observing x(pa(s)) may not\nbe zero.\n17.1.4\nStructure Scoring\nGiven a prior defined as a family of Dirichlet distributions associated to a = (as(x(s),x(pa(s)))\nfor s \u2208V ,x(s) \u2208Fs,x(pa(s)) \u2208F (pa(s)), the joint density of the observations and param-\neters is given by\nP(x,\u03b8) =\nY\ns,x(pa(s))\nD(as(\u00b7,x(pa(s))))\nY\ns,x(s),x(pa(s))\np(x(pa(s)),x(s))Ns(x(s),x(pa(s)))+as(x(s),x(pa(s)))\u22121\nwith\nD(a(\u03bb),\u03bb \u2208F) =\n\u0393(\u03bd)\nQ\n\u03bb \u0393(a(\u03bb))\nand \u03bd = P\n\u03bb a(\u03bb). Here, \u03b8 represents the parameters of the model, i.e., the conditional\ndistributions that specify the Bayesian network. Note that P(x,\u03b8) is a density over\nthe product space F (V ) \u00d7 \u0398 where \u0398 is the space of all these conditional distribu-\ntions. The marginal of this likelihood over all possible parameters, i.e.,\nP(x) =\nZ\nP(x,\u03b8)d\u03b8\nprovides the expected likelihood of the sample relative to the distribution of the pa-\nrameters, and only depends on the structure of the network. In our case, integrating\nwith respect to \u03b8 yields\nlogP(x) =\nX\ns,xpa(s)\nlog\nD(as(\u00b7,x(pa(s))))\nD(as(\u00b7,x(pa(s))) + Ns(\u00b7,x(pa(s)))).\n414\nCHAPTER 17. LEARNING GRAPHICAL MODELS\nLetting\n\u03b3(s,pa(s)) =\nX\nx(pa(s))\nlog\nD(as(\u00b7,x(pa(s))))\nD(as(\u00b7,x(pa(s))) + Ns(\u00b7,x(pa(s)))),\nthe decomposition\nlogP(x) =\nX\ns\u2208V\n\u03b3(s,pa(s))\nexpresses this likelihood as a sum of \u201cscores\u201d (associated to each node and its par-\nents), which depends on the observed sample. The scores that are computed above\nare often called Bayesian scores because they derive from a Bayesian construction.\nOne can also consider simpler scores, such as penalized likelihood:\n\u03b3(s,pa(s)) = \u2212\nX\nx(pa(s))\n\u02c6H(X(s) | X(pa(s)))|F (pa(s))| \u2212\u03c1|pa(s)|,\nwhere \u02c6H is the conditional entropy for the empirical distribution based on observed\nsamples. Structure learning algorithms [144, 108] are designed to optimize such\nscores.\n17.1.5\nReducing the Parametric Dimension\nIn the previous section, we estimated all conditional probabilities intervening in the\nnetwork. This is obviously a lot of parameters and, even with a regularizing prior,\nthe estimated values are likely to be be inaccurate for small sample sizes. It then\nbecomes desirable to simplify the parametric complexity of the model.\nWhen the sets Fs are not too large, which is common in practice, the paramet-\nric explosion is due to the multiplicity of parents, since the number of conditional\nprobabilities ps(x(pa(s)),\u00b7) grows exponentially with |pa(s)|. One way to simplify this\nis to assume that the conditional probability at s only depends on x(pa(s)) via some\n\u201cglobal-effect\u201d statistic gs(x(pa(s))). The idea, of course, is that the number of values\ntaken by gs should remain small, even if the number of parents is large.\nExamples of some functions gs can be max(x(t),t \u2208pa(s)), or the min, or some\nsimple (quantized) function of the sum. With binary variables (Fs = {0,1}), logical\noperators are also available (\u201cand\u201d, \u201cor\u201d, \u201cxor\u201d), as well as combinations of them.\nThe choice made for the functions gs is part of building the model, and would rely on\nthe specific context and prior information on the process, which is always important\nto account for, in any statistical problem.\nOnce the gs\u2019s are fixed, learning the network distribution, which is now given by\n\u03c0(x) =\nY\ns\u2208V\nps(gs(x(pa(s))),x(s))\n17.2. LEARNING LOOPY MARKOV RANDOM FIELDS\n415\ncan be done exactly as before, the parameters being all ps(w,\u03bb),\u03bb \u2208Fs,w \u2208Ws, where\nWs is the range of gs, and Dirichlet priors can be associated to each ps(w,\u00b7) for s \u2208V\nand w \u2208Ws. The counts provided in (17.3) now can be chosen as\nas(xs,w) =\n\u03bd\u2032\n|F||g\u22121\ns (w)|\n.\n(17.4)\n17.2\nLearning Loopy Markov Random Fields\nLike everything else, parameter estimation for loopy networks is much harder than\nwith trees or Bayesian networks. There is usually no closed form expression for\nthe estimators, and their computation relies on more or less tractable numerical\nprocedures.\n17.2.1\nMaximum Likelihood with Exponential Models\nIn this section, we consider a parametrized model for a Gibbs distribution\n\u03c0\u03b8(x) = 1\nZ\u03b8\nexp(\u2212\u03b8T U(x))\n(17.5)\nwhere \u03b8 is a d-dimensional parameter and U is a function from F (V ) to Rd. For\nexample, if \u03c0 is an Ising model with\n\u03c0(x) = 1\nZ exp\n\u0010\n\u03b1\nX\ns\u2208V\nx(s) + \u03b2\nX\ns\u223ct\nx(s)x(t)\u0011\n,\nthen \u03b8 = (\u03b1,\u03b2) and U(x) = \u2212(P\ns x(s),P\ns\u223ct x(s)x(t)). Most of the Markov random fields\nmodels that are used in practice can be put in this form. The constant Z\u03b8 in (17.5) is\nZ\u03b8 =\nX\nx\u2208F (V )\nexp(\u2212\u03b8T U(x))\nand is usually not computable.\nNow, assume that an N-sample, x1,...,xN, is observed for this distribution. The\nmaximum likelihood estimator maximizes\n\u2113(\u03b8) = 1\nN\nN\nX\nk=1\nlog\u03c0\u03b8(xk) = \u2212\u03b8T \u00afUN \u2212logZ\u03b8\nwith \u00afUN = (U(x1) + \u00b7\u00b7\u00b7 + U(xN))/N.\nWe have the following proposition, which is a well-known property of exponen-\ntial families of probabilities.\n416\nCHAPTER 17. LEARNING GRAPHICAL MODELS\nProposition 17.4 The log-likelihood, \u2113, is a concave function of \u03b8, with\n\u2207\u2113(\u03b8) = E\u03b8(U) \u2212\u00afUN\n(17.6)\nand\n\u22072\u2113(\u03b8) = \u2212Var\u03b8(U)\n(17.7)\nwhere E\u03b8 denotes the expectation with respect to \u03c0\u03b8 and Var\u03b8 the covariance matrix under\nthe same distribution.\nWe skip the proof, which is just computation. This proposition implies that a\nlocal maximum of \u03b8 7\u2192\u2113(\u03b8) must also be global. Any such maximum must be a\nsolution of\nE\u03b8(U) = \u00afUN(x0)\nand conversely. There are some situations in which the maximum does not exist, or\nis not unique. Let us first discuss the second case.\nIf several solutions exist, the log-likelihood cannot be strictly concave: there must\nexist at least one \u03b8 for which Var\u03b8(U) is not definite. This implies that there exists a\nnonzero vector u such that var\u03b8(uT U) = uT Var\u03b8(U)u = 0. This is only possible when\nuT U(x) = cst for all x \u2208FV . Conversely, if this is true, Var\u03b8(U) is degenerate for all \u03b8.\nSo, the non-uniqueness of the solutions is only possible when a deterministic\naffine relation exists between the components of U, i.e., when the model is over-\ndimensioned. Such situations are usually easily dealt with by removing some pa-\nrameters. In all other cases, there exists at most one maximum.\nFor a concave function like \u2113to have no maximum, there must exist what is called\na direction of recession [167], which is a direction \u03b1 \u2208Rd such that, for all \u03b8, the\nfunction t 7\u2192\u2113(\u03b8 + t\u03b1) is increasing. In this case the maximum is attained \u201cat infin-\nity\u201d. Denoting U\u03b1(x) = \u03b1T U(x), the derivative in t of \u2113(\u03b8 + t\u03b1) is\nE\u03b8+t\u03b1(U\u03b1) \u2212\u00afU\u03b1\nwhere \u00afU\u03b1 = \u03b1T \u00afUN. This derivative is positive for all t if and only if\n\u00afU\u03b1 = U\u2217\n\u03b1 := min{U\u03b1(x),x \u2208F (V )}\n(17.8)\nand U\u03b1 is not constant. To prove this, assume that the derivative is positive. Then\nU\u03b1 is not constant (otherwise, the derivative would be zero). Let F \u2217\n\u03b1 \u2282F (V ) be the\n17.2. LEARNING LOOPY MARKOV RANDOM FIELDS\n417\nset of configurations x for which U\u03b1(x) = U\u2217\n\u03b1. Then\nE\u03b8+t\u03b1(U\u03b1)\n=\nP\nx\u2208F (V ) U\u03b1(x)exp(\u2212\u03b8T U(x) \u2212tU\u03b1(x))\nP\nx\u2208F (V ) exp(\u2212\u03b8T U(x) \u2212tU\u03b1(x))\n=\nP\nx\u2208F (V ) U\u03b1(x)exp(\u2212\u03b8T U(x) \u2212t(U\u03b1(x) \u2212U\u2217\n\u03b1))\nP\nx\u2208F (V ) exp(\u2212\u03b8T U(x) \u2212t(U\u03b1(x) \u2212U\u2217\u03b1))\n=\nU\u2217\n\u03b1\nP\nx\u2208F \u2217\u03b1 exp(\u2212\u03b8T U(x)) + P\nx<F \u2217\u03b1 U\u03b1(x)exp(\u2212\u03b8T U(x) \u2212t(U\u03b1(x) \u2212U\u2217\n\u03b1))\nP\nx\u2208F \u2217\u03b1 exp(\u2212\u03b8T U(x)) + P\nx<F \u2217\u03b1 exp(\u2212\u03b8T U(x) \u2212t(U\u03b1(x) \u2212U\u2217\u03b1))\n.\nWhen t tends to +\u221e, the sums over x < F \u2217\n\u03b1 tend to 0, which implies that E\u03b8+t\u03b1(U\u03b1)\ntends to U\u2217\n\u03b1. So, if E\u03b8+t\u03b1(U\u03b1) \u2212\u00afU\u03b1 > 0 for all t, then \u00afU\u03b1 = U\u2217\n\u03b1 and U\u03b1 is not constant.\nThe converse statement is obvious.\nAs a conclusion, the function \u2113has a finite maximum if and only if there is no\ndirection \u03b1 \u2208Rd such that \u03b1T (U(x)\u2212\u00afUN) \u22640 for all x \u2208F (V ). Equivalently, \u00afUN must\nbelong to the interior of the convex hull of the finite set\n{U(x),x \u2208F (V )} \u2282Rd.\nIn such a case, that we hereafter assume, computing the maximum likelihood\nestimator boils down to solving the equation\nE\u03b8(U) = \u00afUN.\nBecause the maximization problem is concave, we know that numerical algorithms\nsuch as gradient ascent,\n\u03b8(t + 1) = \u03b8(t) + \u03f5(E\u03b8(t)(U) \u2212\u00afUN),\n(17.9)\nconverge to the optimal parameter. Unfortunately, the computation of the expec-\ntations and covariance matrices can only be made explicitly for acyclic models, for\nwhich parameter estimation is not a problem anyway. For general loopy graphical\nmodels, the expectation can be estimated iteratively using Monte-Carlo methods. It\nturns out that this estimation can be synchronized with gradient descent to obtain a\nconsistent algorithm, which is described in the next section.\n17.2.2\nMaximum likelihood with stochastic gradient ascent\nAs remarked above, for fixed \u03b8, we have designed, in chapter 14, Markov chain\nMonte Carlo algorithms that asymptotically sample form \u03c0\u03b8. Select one of these\nalgorithms, and let p\u03b8 be the corresponding transition probabilities for a given \u03b8, so\n418\nCHAPTER 17. LEARNING GRAPHICAL MODELS\nthat p\u03b8(x,y) = P(Xn+1 = y | Xn = x) for the sampling chain. Then, define the iterative\nalgorithm, initialized with arbitrary \u03b80 and x0 \u2208F (V ), that loops over the following\ntwo steps.\n(SG1) Sample from the distribution p\u03b8t(xt,\u00b7) to obtain a new configuration xt+1.\n(SG2) Update the parameter using\n\u03b8t+1 = \u03b8t + \u03b3t+1(U(xt+1) \u2212\u00afUN).\n(17.10)\nThis algorithm differs from the situation considered in section 3.3 in that the\ndistribution of the sampled variable xt+1 depends on both the current parameter \u03b8t\nand on the current variable xt. Convergence requires additional constraints on the\nsize of the gains \u03b3(t) and we have the following theorem [206].\nTheorem 17.5 If p\u03b8 corresponds to the Gibbs sampler or Metropolis algorithm, and \u03b3t+1 =\n\u03f5/(t+1) for small enough \u03f5, the algorithm that iterates (SG1) and (SG2) converges almost\nsurely to the maximum likelihood estimator.\nThe speed of convergence of such algorithms depends both on the speed of con-\nvergence of the Monte-Carlo sampling and of the original gradient ascent. The latter\ncan be improved somewhat with variants similar to those discussed in section 3.3,\nfor example by choosing data-adaptive gains as in the ADAM algorithm.\n17.2.3\nRelation with Maximum Entropy\nThe maximum likelihood estimator is closely related to what is called the maximum\nentropy extension of a set of constraints. Let the function U from F (V ) to Rd be\ngiven. An element u \u2208Rd is said to be a consistent assignment for U if there exists\na probability distribution \u03c0 on F (V ) such that E\u03c0(U) = u. An example of consistent\nassignment is any empirical average \u00afU based on a sample (x(1),...,x(N)), since \u00afU =\nE\u03c0(U) for\n\u03c0 = 1\nN\nN\nX\nk=1\n\u03b4x(k).\nGiven U and a consistent assignment, u, the associated maximum entropy exten-\nsion is defined as a probability distribution \u03c0 maximizing the entropy, H(\u03c0), subject\nto the constraint E\u03c0(U) = u. This is a convex optimization problem, with constraints\n\uf8f1\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f2\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f3\nX\nx\u2208F (V )\n\u03c0(x) = 1\nX\nx\u2208F (V )\nUj(x)\u03c0(x) = uj,j = 1,...,d\n\u03c0(x) \u22650,x \u2208F (V )\n(17.11)\n17.2. LEARNING LOOPY MARKOV RANDOM FIELDS\n419\nBecause the entropy is strictly convex, there is a unique solution to this problem.\nWe first discuss non-positive solutions, i.e., solutions for which \u03c0(x) = 0 for some x.\nAn important fact is that, if, for a given x, there exists \u03c01 such that E\u03c01(U) = u and\n\u03c01(x) > 0, then the optimal \u03c0 must also satisfy \u03c0(x) > 0. This is because, if \u03c0(x) = 0,\nthen, letting \u03c0\u03f5 = (1 \u2212\u03f5)\u03c0 + \u03f5\u03c01, we have E\u03c0\u03f5(U) = u since this constraint is linear,\n\u03c0\u03f5(x) > 0 and\nH(\u03c0\u03f5) \u2212H(\u03c0)\n=\n\u2212\nX\ny,\u03c0(y)>0\n(\u03c0\u03f5(y)log\u03c0\u03f5(y) \u2212\u03c0(y)log\u03c0(y))\n\u2212\nX\ny,\u03c0(y)=0\n\u03f5\u03c01(y)(log(\u03f5) + log\u03c01(y))\n=\n\u2212\u03f5log\u03f5\nX\ny,\u03c0(y)=0\n\u03c01(y) + O(\u03f5)\nwhich is positive for small enough \u03f5, contradicting the fact that \u03c0 is a maximizer.\nIntroduce the set Nu containing all configurations x \u2208F (V ) such that \u03c0(x) = 0\nfor all \u03c0 such that E\u03c0(U) = u. Then we know that the maximum entropy extension\nsatisfies \u03c0(x) > 0 if x < Nu. Introduce Lagrange multipliers \u03b80,\u03b81,...,\u03b8d for the d + 1\nequality constraints in (17.11), and the Lagrangian\nL = H(\u03c0) +\nX\nx\u2208F (V )\\Nu\n(\u03b80 + \u03b8T U(x))\u03c0(x)\nin which we have set \u03b8 = (\u03b81,...,\u03b8d), we find that the optimal \u03c0 must satisfy\n\uf8f1\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f2\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f3\nlog\u03c0(x) = \u2212\u03b80 \u22121 \u2212\u03b8T U(x)\nX\nx\n\u03c0(x) = 1\nE\u03c0(U) = \u00afu\nIn other terms, the maximum entropy extension is characterized by\n\u03c0(x) = 1\nZ\u03b8\nexp(\u2212\u03b8T U(x))1N cu(x)\nand E\u03c0(U) = u.\nIn particular, if Nu = \u2205, then the maximum entropy extension is positive. If,\nin addition, u = \u00afU for some observed sample, then it coincides with the maximum\nlikelihood estimator for (17.5). Notice that, in this case, the condition Nu , \u2205coin-\ncide with the condition that there exists \u03b1 such that \u03b1T U(x) \u2265\u03b1T u for all x, with\n\u03b1T U(x) not constant. Indeed, assume that the latter condition is true. Then, if\nE\u03c0(U) = u, then E\u03c0(\u03b1T U) = \u03b1T u, which is only possible if \u03c0(x) = 0 for all x such\n420\nCHAPTER 17. LEARNING GRAPHICAL MODELS\nthat \u03b1T U(x) < \u03b1T u. Such x\u2019s exist by assumption, and therefore Nu , \u2205. Conversely,\nassume Nu , \u2205. If condition (17.8) is not satisfied, then we have shown when dis-\ncussing maximum likelihood that an optimal parameter for the exponential model\nwould exist, leading to a positive distribution for which E\u03c0(U) = u, which is a con-\ntradiction.\n17.2.4\nIterative Scaling\nIterative scaling is a method that is well-adapted to learning distributions given by\n(17.5), when U can be interpreted as a random histogram, or a collection of them.\nMore precisely, assume that for all x \u2208F (V ), one has\nU(x) = (U1(x),...,Uq(x))\nwith\nq\nX\nj=1\nUj(x) = 1 and Uj(x) \u22650.\nLet the parameter be given by \u03b8 = (\u03b81,...,\u03b8q). Assume that x1,...,xN have been\nobserved, and let u \u2208Rd be a consistent assignment for U, with uj > 0 for j = 1,...,d\nand such that Nu = \u2205. Iterative scaling computes the maximum entropy extension of\nE\u03c0(U) = u, that we will denote \u03c0\u2217. It is supported by the following lemma.\nLemma 17.6 Let \u03c0 be a probability on F (V ) with \u03c0 > 0 and define\n\u03c0\u2032(x) = \u03c0(x)\n\u03b6\nd\nY\nj=1\n \nuj\nE\u03c0(Uj)\n!Uj(x)\nwhere \u03b6 is chosen so that \u03c0\u2032 is a probability. Then \u03c0\u2032 > 0 and\nKL(\u03c0\u2217\u2225\u03c0\u2032) \u2212KL(\u03c0\u2217\u2225\u03c0) \u2264\u2212KL(u\u2225E\u03c0(U)) \u22640\n(17.12)\nProof Note that, since \u03c0 > 0, E\u03c0(Uj) must also be positive for all j, since E\u03c0(Uj) = 0\nwould otherwise imply Uj = 0 and uj = 0 for u to be consistent. So, \u03c0\u2032 is well defined\nand obviously positive.\nWe have\nKL(\u03c0\u2217\u2225\u03c0\u2032) \u2212KL(\u03c0\u2217\u2225\u03c0)\n=\nlog\u03b6 \u2212\nX\nx\u2208F (V )\n\u03c0\u2217(x)\nd\nX\nj=1\nUj(x)log\nuj\nE\u03c0(Uj)\n=\nlog\u03b6 \u2212\nd\nX\nj=1\nuj log\nuj\nE\u03c0(Uj)\n=\nlog\u03b6 \u2212KL(u\u2225E\u03c0(U)).\n17.2. LEARNING LOOPY MARKOV RANDOM FIELDS\n421\n(We have used the identity E\u03c0\u2217(U) = u.) So it suffices to prove that \u03b6 \u22641. We have\n\u03b6\n=\nX\nx\u2208F (V )\n\u03c0(x)\nd\nY\nj=1\n \nuj\nE\u03c0(Uj)\n!Uj(x)\n\u2264\nd\nX\nj=1\nX\nx\u2208F (V )\n\u03c0(x)Uj(x)\nuj\nE\u03c0(Uj)\n=\nd\nX\nj=1\nE\u03c0(Uj)\nuj\nE\u03c0(Uj) = 1,\nwhich proves the lemma (we have used the fact that, for xi, wi positive numbers with\nP\ni wi = 1, one has Q\ni xwi\ni\n\u2264P\ni wixi, which is a consequence of the concavity of the\nlogarithm).\n\u25a0\nConsider the iterative algorithm\n\u03c0n+1(x) = \u03c0n(x)\n\u03b6n\nd\nY\nj=1\n \nuj\nE\u03c0n(Uj)\n!Uj(x)\ninitialized with a uniform distribution. Equivalently, using the exponential formu-\nlation, define, for j = 1,...,d,\n\u03b8n+1,j = \u03b8n,j + log\nE\u03b8n(Uj)\nuj\n+ KL(u\u2225E\u03b8n(U)),\n(17.13)\nwith \u03c0\u03b8 given by (17.5), initialized with \u03b80 = 0. Note that adding a term that is\nindependent of j to \u03b8 does not change the value of \u03c0\u03b8, because the Uj\u2019s sum to 1.\nThe model is in fact overparametrized, and the addition of the KL divergence in\n(17.13) ensures that Pd\ni=1 ui\u03b8i = 0 at all steps.\nThis algorithm always reduces the Kullback-Leibler distance to the maximum en-\ntropy extension. This distance being always positive, it therefore converges to a limit,\nwhich, still according to lemma 17.6, is only possible if KL(u\u2225E\u03c0n(U)) also tends to\n0, that is E\u03c0n(U) \u2192u. Since the space of probability distributions is compact, the\nHeine-Borel theorem implies that the sequence \u03c0\u03b8n has at least one accumulation\npoint, that we now identify. If \u03c0 is such a point, one must have E\u03c0(U) = u. More-\nover, we have \u03c0 > 0, since otherwise KL(\u03c0\u2217\u2225\u03c0) = +\u221e. To prove that \u03c0 = \u03c0\u2217(and\ntherefore the limit of the sequence), it remains to show that it can be put in the form\n(17.5). For this, define the vector space V of functions v : F (V ) \u2192R which can be\nwritten in the form\nv(x) = \u03b10 +\ng\nX\nj=1\n\u03b1jUj(x).\n422\nCHAPTER 17. LEARNING GRAPHICAL MODELS\nSince log\u03c0\u03b8n \u2208V for all n, so is its limit, and this proves that log\u03c0 belongs to V. We\nhave obtained the following proposition.\nProposition 17.7 Assume that for all x \u2208F (V ), one has U(x) = (U1(x),...,Ud(x)) with\nd\nX\nj=1\nUj(x) = 1 and Uj(x) \u22650.\nLet u be a consistent assignment for the expectation of U such thatNu = \u2205. Then, the\nalgorithm described in (17.13) converges to the maximum entropy extension of u.\nThis is the iterative scaling algorithm. This method can be extended in a straight-\nforward way to handle the maximum entropy extension for a family of functions\nU(1),...,U(K), such that, for all x and for all k, U(k)(x) is a dk-dimensional vector such\nthat\ndk\nX\nj=1\nU(k)\nj (x) = 1.\nThe maximum entropy extension takes the form\n\u03c0\u03b8(x) = 1\nZ\u03b8\nexp\n\u0012\n\u2212\nK\nX\nk=1\n(\u03b8(k))T U(k)(x)\n\u0013\n,\nwhere \u03b8(k) is dk-dimensional, and iterative scaling can then be implemented by up-\ndating only one of these vectors at a time, using (17.13) with U = U(k).\nThe restriction to U(x) providing a discrete probability distribution for all x is,\nin fact, no loss of generality. This is because adding a constant to U does not change\nthe resulting exponential model in (17.5), and multiplying U by a constant can be\nalso compensated by dividing \u03b8 by the same constant in the same model. So, if u\u2212is\na lower bound for minj,x Uj(x), one can replace U by (U \u2212u\u2212), and therefore assume\nthat U \u22650, and if u+ is an upper bound for P\nj Uj(x), we can replace U by U/u+ and\ntherefore assume that P\nj Uj(x) \u22641. Define\nUd+1(x) = 1 \u2212\nd\nX\nj=1\nUj(x) \u22650.\nThen, the maximum entropy extension for (U1,...,Ud) with assignment (u1,...,ud) is\nobviously also the extension for (U1,...,Ud+1), with assignment (u1,...,ud+1), where\nud+1 = 1 \u2212\nd\nX\nj=1\nuj,\n17.2. LEARNING LOOPY MARKOV RANDOM FIELDS\n423\nand the latter is in the form required in proposition 17.7. Note that iterative scaling\nrequires to compute the expectation of U1,...,Ud before each update. These are not\nnecessarily available in closed form and may have to be estimated using Monte-Carlo\nsampling.\n17.2.5\nPseudo likelihood\nMaximum likelihood estimation is a special case of minimal contrast estimators. These\nestimators are based on the definition of a measure of dissimilarity, say C(\u03c0\u2225\u02dc\u03c0), be-\ntween two probability distributions \u03c0 and \u02dc\u03c0. The usual assumptions on C are that\nC(\u03c0\u2225\u02dc\u03c0) \u22650, with equality if and only if \u03c0 = \u02dc\u03c0, and that C is \u2014 at least \u2014 continuous\nin \u03c0 and \u02dc\u03c0. Minimal contrast estimators approximate the problem of minimizing\n\u03b8 7\u2192C(\u03c0true\u2225\u03c0\u03b8) over a parameter \u03b8 \u2208\u0398, (which is not feasible, since \u03c0true, the true\ndistribution of the data, is unknown) by the minimization of \u03b8 7\u2192C( \u02c6\u03c0\u2225\u03c0\u03b8) where\n\u02c6\u03c0 is the empirical distribution computed from observed data. Under mild condi-\ntions on C, these estimators are generally consistent when N tends to infinity, which\nmeans that the estimated parameter asymptotically (in the sample size N) provides\nthe best (according to C) approximation of \u03c0true by the family \u03c0\u03b8,\u03b8 \u2208\u0398.\nThe contrast that is associated with maximum likelihood is the Kullback-Leibler\ndivergence. Indeed, given a sample x1,...,xN, we have\nKL( \u02c6\u03c0\u2225\u03c0\u03b8)\n=\nE \u02c6\u03c0 log \u02c6\u03c0 \u2212E \u02c6\u03c0 log\u03c0\u03b8\n=\nE \u02c6\u03c0 log \u02c6\u03c0 \u2212\nN\nX\nk=1\nlog\u03c0\u03b8(xk).\nSince E \u02c6\u03c0 log \u02c6\u03c0 does not depend on \u03b8, minimizing KL( \u02c6\u03c0\u2225\u03c0\u03b8) is equivalent to maxi-\nmizing PN\nk=1 log\u03c0\u03b8(xk) which is the log-likelihood.\nMaximum pseudo-likelihood estimators form another class of minimal contrast\nestimators for graphical models. Given a distribution \u03c0 on F (V ), define the local\nspecifications \u03c0s(x(s) | x(t),t , s) to be conditional distributions at one vertex given\nthe others, and the contrast\nC(\u03c0\u2225\u02dc\u03c0) =\nX\ns\u2208V\nE\u03c0(log \u03c0s\n\u02dc\u03c0s\n).\nBecause we can write, using standard properties of conditional expectations,\nC(\u03c0\u2225\u02dc\u03c0) =\nX\ns\u2208V\nE\u03c0\n \nE\u03c0s(log \u03c0s\n\u02dc\u03c0s\n)\n!\n=\nX\ns\u2208V\nE(KL(\u03c0s(\u00b7 | X(t),t , s)\u2225\u02dc\u03c0s(\u00b7 | X(t),t , s)),\nwe see that C(\u03c0, \u02dc\u03c0) is always positive, and vanishes (under the assumption of positive\n\u03c0) only if all the local specifications for \u03c0 and \u02dc\u03c0 coincide, and this can be shown\n424\nCHAPTER 17. LEARNING GRAPHICAL MODELS\nto imply that \u03c0 = \u02dc\u03c0. Indeed, for any x,y \u2208F (V ), and choosing some order V =\n{s1,...,sn} on V , one can write\n\u03c0(x)\n\u03c0(y) =\nn\nY\nk=1\n\u03c0(x(sk)|x(s1),...,x(sk\u22121),y(sk+1),...,y(sn))\n\u03c0(x(sk)|x(s1),...,x(sk\u22121),y(sk+1),...,y(sn))\nand the ratios \u03c0(x)/\u03c0(y), for x \u2208F (V ), combined with the constraint that P\nx \u03c0(x) = 1\nuniquely define \u03c0.\nSo C is a valid contrast and\nC( \u02c6\u03c0\u2225\u03c0\u03b8) =\nX\ns\u2208V\nE \u02c6\u03c0 log \u02c6\u03c0s \u2212\nX\ns\u2208V\nN\nX\nk=1\nlog\u03c0\u03b8,s(x(s)\nk |x(t)\nk ,t , s).\nThis yields the maximum pseudo-likelihood estimator (or pseudo maximum likeli-\nhood) defined as a maximizer of the function (called log-pseudo-likelihood)\n\u03b8 7\u2192\nX\ns\u2208V\nN\nX\nk=1\nlog\u03c0\u03b8,s(x(s)\nk |x(s)\nk ,t , s).\nAlthough maximum likelihood is known to provide the most accurate approxi-\nmations in many cases, maximum of pseudo likelihood has the important advantage\nto be, most of the time, computationally feasible. This is because, for a model like\n(17.5), local specifications are given by\n\u03c0\u03b8,s(x(s) | x(t),t , s) =\nexp(\u2212\u03b8T U(x))\nP\ny(s)\u2208Fs exp(\u2212\u03b8T U(y(s) \u2227x(V \\s))).\nand therefore include no intractable normalizing constant. Maximum of pseudo-\nlikelihood estimators can be computed using standard maximization algorithms.\nFor exponential models such as (17.5), the log-pseudo-likelihood is, like the log-\nlikelihood, a concave function.\n17.2.6\nContinuous variables and score matching\nThe methods that were presented so far for discrete variables formally generalize to\nmore general state spaces, even though consistency or convergence issues in non-\ncompact cases can be significantly harder to address. Score matching is a parameter\nestimation method that was introduced in [95] and was designed, in its original\nversion, to estimate parameters for statistical models taking the form\n\u03c0\u03b8(x) =\n1\nC(\u03b8) exp(\u2212F(x,\u03b8))\n17.2. LEARNING LOOPY MARKOV RANDOM FIELDS\n425\nwith x \u2208Rd. We assume below suitable integrability and differentiability conditions,\nin order to justify differentiation under integrals whenever they are needed. The\n\u201cscore function\u201d is defined as\ns(x,\u03b8) = \u2212\u2207x log\u03c0\u03b8(x) = \u2207xF(x,\u03b8)\nwhere \u2207x denotes the gradient with respect to the x variable. Letting \u03c0true denote\nthe p.d.f. of the true data distribution (not necessarily part of the statistical model),\nscore matching minimizes\nf (\u03b8) =\nZ\nRd |s(x,\u03b8) \u2212strue(x)|2\u03c0true(x)dx\nwhere strue = \u2212\u2207log\u03c0true. This integral can be restricted to the support of \u03c0true, if\nwe don\u2019t want to assume that \u03c0true is non-vanishing. Note, however that f (\u03b8) = 0\nimplies that log\u03c0\u03b8(\u00b7,\u03b8) = log\u03c0true \u03c0true-almost everywhere, so that \u03c0\u03b8(x) = c\u03c0true(x)\nfor some constant c and x in the support of \u03c0true. Only if \u03c0true(x) > 0 for all x \u2208Rd,\ncan we conclude that this requires \u03c0\u03b8 = \u03c0true.\nExpanding the squared norm and applying the divergence theorem yield\nf (\u03b8) =\nZ\nRd |\u2207x log\u03c0\u03b8(x)|2\u03c0true(x)dx \u22122\nZ\nRd \u2207x log\u03c0\u03b8(x)T \u2207\u03c0true(x)dx\n+\nZ\nRd |strue(x)|2\u03c0true(x)dx\n=\nZ\nRd |\u2207x log\u03c0\u03b8(x)|2\u03c0true(x)dx + 2\nZ\nRd \u2206log\u03c0\u03b8(x)T \u03c0true(x)dx +\nZ\nRd |strue(x)|2dx\nTo justify the use of the divergence theorem, one needs to assume two derivatives in\nthe log-likelihoods with sufficient decay at infinity (see Hyv\u00a8arinen and Dayan [95]\nfor details). This shows that minimizing f is equivalent to minimizing\ng(\u03b8) =\nZ\nRd |\u2207x log\u03c0\u03b8(x)|2\u03c0true(x)dx + 2\nZ\nRd \u2206log\u03c0\u03b8(x)T \u03c0true(x)dx\n= E(|\u2207x log\u03c0\u03b8(X)|2 + 2\u2206log\u03c0\u03b8(X)).\nIn this form, the objective function can be approximated by a sample average, so\nthat, given observed data x1,...,xN, one can define the score-matching estimator as\na minimizer of\nN\nX\nk=1\n\u0010\n|\u2207x log\u03c0\u03b8(xk)|2 + 2\u2206log\u03c0\u03b8(xk)\n\u0011\n.\n(17.14)\nRemark 17.8 The method can be adapted to deal with discrete variables replacing\nderivatives with differences. Let X take values in a finite set, RX, on which a graph\n426\nCHAPTER 17. LEARNING GRAPHICAL MODELS\nstructure can be defined, writing x \u223cy if x and y are connected by an edge. For\nexample, if X is itself a Markov random field on a graph G = (V ,E), so that RX =\nF (V ), one can define x \u223cy if and only if x(s) = y(s) for all but one s \u2208V . One can then\ndefine the score function\ns\u03b8(x,y) = 1 \u2212\u03c0\u03b8(y)\n\u03c0\u03b8(x)\ndefined over all x,y \u2208RX such that x \u223cy. Now the score matching functional is\nf (\u03b8) =\nX\nx\u2208RX\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\nX\ny\u223cx\n|s\u03b8(x,y) \u2212s\u2217(x,y)|2\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\u03c0\u2217(x),\nwhose minimization is, after reordering terms, equivalent to that of\ng(\u03b8) =\nX\nx\u2208RX\nX\ny\u223cx\n\f\f\f\f\f1 \u2212\u03c0\u03b8(y)\n\u03c0\u03b8(x)\n\f\f\f\f\f\n2\n\u03c0\u2217(x) + 2\nX\nx\u2208RX\nX\ny\u223cx\n \u03c0\u03b8(x)\n\u03c0\u03b8(y) \u2212\u03c0\u03b8(y)\n\u03c0\u03b8(x)\n!\n\u03c0\u2217(x).\nBased on training data, a discrete score matching estimator is a minimizer of\nN\nX\nk=1\nX\ny\u223cxk\n\f\f\f\f\f1 \u2212\u03c0\u03b8(y)\n\u03c0\u03b8(xk)\n\f\f\f\f\f\n2\n+ 2\nN\nX\nk=1\nX\ny\u223cxk\n \u03c0\u03b8(xk)\n\u03c0\u03b8(y) \u2212\u03c0\u03b8(y)\n\u03c0\u03b8(xk)\n!\n.\n(17.15)\n\u2666\n17.3\nIncomplete observations for graphical models\n17.3.1\nThe EM Algorithm\nMissing variable sin the context of graphical models may correspond to real pro-\ncesses that cannot be measured, which is common, for example, with biological data.\nThey may be more conceptual objects that are interpretable but are not parts of the\ndata acquisition process, like phonemes in speech recognition, or edges and labels\nin image processing and object recognition. They may also be variables that have\nbeen added to the model to increase its parametric dimension without increasing\nthe complexity of the graph. However, as we will see, dealing with incomplete or\nimperfect observations brings the parameter estimation problem to a new level of\ndifficulty.\nSince it is the most common approach to address incomplete or noisy observa-\ntions, we start with a description of how the EM algorithm (Algorithm 16.1) applies\nto graphical models, and of its limitations. We assume a graphical model on an\nundirected graph G = (V ,E), in which we assume that V is separated in two non-\nintersecting subsets, V = S \u222aH. Letting X be a G-Markov random field, the part X(S)\nis assumed to be observable, and X(H) is hidden.\n17.3. INCOMPLETE OBSERVATIONS FOR GRAPHICAL MODELS\n427\nWe assume that X takes values in F (V ), where we still denote by Fs the sets in\nwhich Xs takes values for s \u2208V . We let the model distribution belong to an expo-\nnential family, with\n\u03c0\u03b8(x) =\n1\nZ(\u03b8) exp\n\u0010\n\u2212\u03b8T U(x)\n\u0011\n, x \u2208F (V ).\n(17.16)\nAssume that an N-sample x(S)\n1 ,...,x(S)\nN is observed over S. Since\nlog\u03c0\u03b8(x) = \u2212logZ(\u03b8) \u2212\u03b8T U(x),\nthe transition from \u03b8n to \u03b8n+1 in Algorithm 16.1 is done by maximizing\n\u2212logZ(\u03b8) \u2212\u03b8T \u00afUn\n(17.17)\nwhere\n\u00afUn = 1\nN\nN\nX\nk=1\nE\u03b8n(U(X) | X(S) = x(S)\nk ).\n(17.18)\nSo, the M-step of the EM, which maximizes (17.17), coincides with the complete-\ndata maximum-likelihood problem for which the empirical average of U is replaced\nby the average of its conditional expectations given the observations, as given in\n(17.18), which constitutes the E-step. As a consequence, a strict application of the\nEM algorithm for graphical models is unfeasible, since each step requires running\nan algorithm of similar complexity maximum likelihood for complete data, that we\nalready identified as a challenging, computationally costly problem. The same re-\nmark holds for the SAEM algorithm of section 16.4.3, which also requires solving a\nmaximum likelihood problem at each iteration.\n17.3.2\nStochastic gradient ascent\nThe stochastic gradient ascent described in section 17.2.2 can be extended to partial\nobservations [207], even though it loses the global convergence guarantee that re-\nsulted from the concavity of the log-likelihood for complete observations. Indeed,\napplying the computation of section 16.5.2, to a model given by (17.16), we get using\nproposition 17.4,\n\u2202\u03b8 log\u03c8\u03b8 = E\u03b8(E\u03b8(U) \u2212U | X(S) = x(S)) = E\u03b8(U) \u2212E\u03b8(U | X(S) = x(S))\nwhere we \u03c8\u03b8(x(S)) denotes the marginal distribution of \u03c0\u03b8 on S.\nLet \u03c0\u03b8(x(H) | x(S)) denotes the conditional probability P(X(H) = x(H) | X(S) = s(S))\nfor the distribution \u03c0\u03b8, therefore taking the form\n\u03c0\u03b8(x(H) | x(S)) =\n1\n\u02dcZ(\u03b8,x(S)) exp\n\u0010\n\u2212\u03b8T U(x(S) \u2227x(H))\n\u0011\n.\n428\nCHAPTER 17. LEARNING GRAPHICAL MODELS\nAssume given an ergodic transition probability p\u03b8 on F (V ), and a family of ergodic\ntransition probabilities px(S)\n\u03b8\n, x(S) \u2208F (S), such that the invariant distribution of p\u03b8 is\n\u03c0\u03b8, and the one of px(S)\n\u03b8\nis \u03c0\u03b8(\u00b7 | x(S)). Then the following SGA algorithm can be used\nto estimate \u03b8\nAlgorithm 17.1\nStart the algorithm with an initial parameter \u03b8(0) and initial configurations x(0) and\nx(H)\nk\n(0), k = 1,...,N. Then, at step n,\n(SGH1) Sample from the distribution p\u03b8(n)(x(n),\u00b7) to obtain new configurations x(n+\n1) \u2208F (V ).\n(SGH2) For k = 1,...,N, sample from the distribution p\nx(S)\nk\n\u03b8(n)(x(H)\nk\n(n),\u00b7) to obtain a new\nconfiguration x()\nk H(n + 1) over the hidden vertexes.\n(SGH3) Update the parameter using\n\u03b8(n + 1) = \u03b8(n) + \u03b3(n + 1)\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8edU(x(n + 1)) \u22121\nN\nN\nX\nk=1\nU(x(S)\nk\n\u2227x(H)\nk\n(n + 1))\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8.\n(17.19)\n17.3.3\nPseudo-EM Algorithm\nThe EM update\n\u03b8n+1 = argmax\n\u03b8\n\u0012 N\nX\nk=1\nE\u03b8n\n\u0012\nlog\u03c0\u03b8(X) | X(S) = x(S)\nk\n\u0013\u0013\n.\nbeing challenging for Markov random fields, it is tempting to replace the log-likelihood\nin the expectation by an other contrast, such as the log-pseudo-likelihood. A simi-\nlar approach to that described here was introduced in Chalmond [51], for situations\nwhen the conditional distribution of X(S) given X(H) is \u201csimple enough\u201d (for exam-\nple, if the variables Xs,s \u2208S are conditionally independent given X(H)) and when the\ncardinality of the sets Fs, s \u2208H is small (binary, or ternary, variables).\nThe algorithm has the following variational interpretation. Fix x(S) \u2208F (S) and\ns \u2208H. Also denote \u00b5s = 1/|F (H \\ {s})|. If q is a transition probability from F (H \\ {s})\nto Fs, let\n\u2206(s)\n\u03b8 (q,x(S)) =\nX\ny\u2208F (H)\n \nlog\n \u03c0\u03b8,s(y(s) \u2227x(S) | y(H\\{s}))\nq(y(H\\{s}),y(s))\u00b5s\n!\nq(y(H\\{s}),y(s))\u00b5s\n!\n.\n(17.20)\n17.3. INCOMPLETE OBSERVATIONS FOR GRAPHICAL MODELS\n429\nThis function is concave in q, since its first partial derivative with respect to q(y(H\\{s}),y(s))\n(for each y \u2208F (H)) is given by\n\u00b5s log\u03c0\u03b8,s(y(s) \u2227x(S) | y(H\\{s}))\u00b5s(y(H\\{s})) \u2212\u00b5s log(q(y(H\\{s}),y(s))\u00b5s) \u2212\u00b5s\nso that its Hessian is the diagonal matrix with negative entries \u2212\u00b5s/q(y(H\\{s}),y(s)).\nUsing Lagrange multipliers to express the constraints P\ny(s)\u2208Fs q(y(H\\{s}),y(s)) = 1 for\nall y(H\\{s}), we find that \u2206(s)\n\u03b8 (q,x(S)) is maximized when q(y(H\\{s}),y(s)) is proportional\nto \u03c0\u03b8,s(y(s) \u2227x(S) | y(H\\{s})), yielding\nq(y(H\\{s}),y(s)) = \u03c0\u03b8,s(y(s) | x(S) \u2227y(H\\{s})).\nNow, consider the problem of maximizing\nN\nX\nk=1\nX\ns\u2208H\n\u2206(n)\n\u03b8 (q(s)\nk ,x(s)\nk )\n(17.21)\nwith respect to \u03b8 and q(s)\nk , k = 1,...,N, s \u2208H. Consider an iterative maximization\nscheme in which, from a current parameter \u03b8n, one first, maximizes (17.21) with\nrespect to transition probabilities q(s)\nk , then with respect to \u03b8 to obtain \u03b8n+1. This\nscheme provides the iteration\n\u03b8n+1 =\nargmax\n\u03b8\nN\nX\nk=1\nX\ns\u2208H\nX\ny\u2208F (H)\n\u0012\nlog\u03c0\u03b8,s(y(s) \u2227x(S)\nk\n| y(H\\{s}))\n\u0013\n\u03c0\u03b8n,s(y(s) | x(S)\nk\n\u2227y(H\\{s}))\u00b5s.\n17.3.4\nPartially-observed Bayesian networks on trees\nWe now consider the situation in which the joint distribution of X = X(S) \u2227X(H) is a\nBayesian network over a directed acyclic graph G = (V ,E).\nAssume that x(S)\n1 ,...,x(S)\nN are observed. The parameter \u03b8 is the collection of all\np(x(pa(s)),x(s)) for s \u2208V . Define the random variables Is,x(y) equal to one if y({s}\u222apa(s)) =\nx({s}\u222apa(s)) and zero otherwise. We can write\nlog\u03c0(y) =\nX\ns\u2208S\nlogps(y(pa(s)),y(s)) =\nX\ns\u2208S\nX\nx({s}\u222apa(s))\u2208F ({s}\u222apa(s))\nlogps(x(pa(s)),x(s))Is,x(y)\n430\nCHAPTER 17. LEARNING GRAPHICAL MODELS\nThis implies that\nN\nX\nk=1\nE\u03b8n\n\u0012\nlog\u03c0(x(S)\nk ,X(H)) | X(S) = x(S)\nk\n\u0013\n=\nX\nx({s}\u222apa(s))\u2208F ({s}\u222apa(s))\nlogps(x(pa(s)),x(s))\nN\nX\nk=1\nE\u03b8n(Is,x(X) | X(S) = x(S)\nk )\n=\nX\nx({s}\u222apa(s))\u2208F ({s}\u222apa(s))\nlogps(x(pa(s)),x(s))\nN\nX\nk=1\n\u03c0\u03b8n(x({s}\u222apa(s)) | X(S) = x(S)\nk ).\nThe EM iteration at step n then is\np(n+1)\ns\n(x(pa(s)),x(s)) =\n1\nZs(x(s\u2212))\nN\nX\nk=1\n\u03c0\u03b8n(x({s}\u222apa(s)) | X(S) = x(S)\nk )\nwith\n\u03c0\u03b8n(x) =\nY\ns\u2208V\np(n)(x(pa(s)),x(s)),\nZs being a normalization constant.\nIf the estimation is solved with a Dirichlet prior Dir(1+as(x(s),x(pa(s)))), the update\nformula becomes\np(n+1)\ns\n(x(pa(s)),x(s)) =\n1\nZs(x(s\u2212))\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8edas(x(s),x(pa(s))) +\nN\nX\nk=1\n\u03c0\u03b8n(x({s}\u222apa(s)) | X(S) = x(S)\nk )\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8.\n(17.22)\nThis algorithm is very simple when the conditional distributions \u03c0\u03b8n(x(s\u222apa(s)) |\nX(S) = x(S)\nk ) can be easily computed, which is not always the case for a general\nBayesian network, since conditional distributions do not always have a structure of\nBayesian network. The computation is simple enough for trees, however, since con-\nditional tree distributions are still trees (or forests). More precisely, the conditional\ndistribution given the observed variables can be written in the form\n\u03c0(y(H) | x(S)) =\n1\nZ(x(S))\nY\ns\u2208H\n\u03d5s,x(y(s))\nY\nt\u223cs,{s,t}\u2282H\n\u03d5st(y(s),y(t))\nwith \u03d5s,pa(s)(y(s),y(pa(s))) = ps(y(pa(s)),y(s)) and, letting \u03d5s(y(s)) = ps(y(s)) if pa(s) = \u2205and\n1 otherwise,\n\u03d5s,x(y(s)) = \u03d5s(y(s))\nY\nt\u223cs,t\u2208S\n\u03d5st(y(s),x(t)).\n17.3. INCOMPLETE OBSERVATIONS FOR GRAPHICAL MODELS\n431\nSo, the marginal joint distribution of a vertex and its parents are directly given by\nbelief propagation, using the just defined interactions. This training algorithm is\nsummarized below.\nAlgorithm 17.2 (Learning tree distributions with hidden variables)\nStart with some initial guess of the conditional probabilities (for example, those\ngiven by the prior). The iterate the following two steps providing the transition\nfrom \u03b8n to step \u03b8n+1.\n(1) For k = 1,...,N, use belief propagation (or sum-prod) to compute all \u03c0\u03b8n(x({s}\u222apa(s)) |\nX(S) = x(S)\nk ). Note that these probabilities can be 0 or 1 when s \u2208S and/or pa(s) \u2282S.\n(2) Use (17.22) to compute the next set of parameters.\nThe tree case includes the important example of hidden Markov models, which\nare defined as follows. S and H are ordered, with same cardinality, say S = {s1,...,sq}\nand H = {h1,...,hq}. Edges are (h1,h2),...,(hq\u22121,hq) and (h1,s1),...,(hq,sq). The in-\nterpretation generally is that the hidden variables, hs, are the variables of interest,\nand behave like a Markov chain, and that the observations, xs, are either noisy or\ntransformed versions of them. A major application is in speech recognition, where\nthe hs\u2019s are labels that represent specific phonemes (little pieces of spoken words)\nand the xs\u2019s are measured signals. The transitions between hidden variables then\ndescribe how phonemes are likely to appear in sequence for a given language, and\nthose between hidden and observed variables describe how each phoneme is likely\nto be pronounced and heard.\n17.3.5\nGeneral Bayesian networks\nThe algorithm in the general case can move from tractable to intractable depending\non the situation. This must generally be handled in a case by case basis, by analyzing\nthe conditional structure, for a given model, knowing the observations.\nIn practice, it is always possible to use loopy belief propagation to obtain some\napproximation of the conditional probabilities, even if it is not sure that the algo-\nrithm will converge to the correct marginals. When feasible, junction trees can be\nused, too. Monte-Carlo sampling is also an option, although quite computational.\n432\nCHAPTER 17. LEARNING GRAPHICAL MODELS\nChapter 18\nDeep Generative Methods\n18.1\nNormalizing flows\n18.1.1\nGeneral concepts\nWe develop, in this chapter, methods that model stochastic processes using a feed-\nforward approach that generates complex random variables using non-linear trans-\nformations of simpler ones. Many of these methods can be seen as instances of struc-\ntural equation models (SEMs), described in section 15.3, with, for deep-learning im-\nplementations, high-dimensional parametrizations of (15.8).\nWith start with the formally simple case where the modeled variable takes values\nin Rd and is modeled as\nX = g(Z)\nwhere Z also takes values in Rd, with a known distribution and g is C1, invertible,\nwith a C1 inverse on Rd, i.e., is a diffeomorphism of Rd. Let us denote by h the inverse\nof g.\nIf Z has a p.d.f. fZ with respect to Lebesgue\u2019s measure, then, using the change of\nvariable formula, the p.d.f. of X is\nfX(x) = fZ(h(x)) |det\u2202xh(x)|.\nNow, given a training set T = (x1,...,xN), the log-likelihood, considered as a func-\ntion of h, is given by\n\u2113(h) =\nN\nX\nk=1\nlogfZ(h(xk)) +\nN\nX\nk=1\nlog|det\u2202xh(xk)|.\n(18.1)\nThis expression should then be maximized with respect to h, subject to some restric-\ntions or constraints to avoid overfitting.\n433\n434\nCHAPTER 18. DEEP GENERATIVE METHODS\n18.1.2\nA greedy computation\nOne can define a rich class of diffeomorphisms through iterative compositions of\nsimple transformations. This framework was introduced in [187], where a greedy\napproach was suggested to build such compositions. The method was termed \u201cnor-\nmalizing flows,\u201d since it create a discrete flow of diffeomorphisms that transform the\ndata into a sample of a normal distribution.\nWe quickly describe the basic principles of the algorithm. One starts with a\nparametrized family, say (\u03c8\u03b1,\u03b1 \u2208A) of diffeomorphisms of R. Such families are\nrelatively easy to design, one example proposed in [187] being a smoothed version\nof the piecewise linear function\nu 7\u2192v0 + (1 \u2212\u03c3)u + \u03b3|(1 \u2212\u03c3)u \u2212u0|\nwhich is increasing as soon as 0 \u2264max(\u03c3,\u03b3) < 1. The smoothed version has an\nadditional parameter, \u03f5, and takes the form\nu 7\u2192v0 + (1 \u2212\u03c3)u + \u03b3\nq\n\u03f52 + ((1 \u2212\u03c3)u \u2212u0)2.\nThis transformation is parametrized by \u03b1 = (v0,\u03c3,\u03b3,u0,\u03f5). Other families of parametrized\ntransformations can be designed. A multivariate transformation \u03d5\u03b1,U : Rd \u2192Rd can\nthen be associated to families \u03b1 = (\u03b11,...,\u03b1d) and orthogonal matrices U by taking\n\u03d5\u03b1,U(x) =\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\n\u03c8\u03b11(y(1))\n...\n\u03c8\u03b1d(y(d))\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\nwith y = Ux.\nThe algorithm in [187] is initialized with h0 = id[d] and update the transforma-\ntion at step n according to\nhn = \u03d5\u03b1n,Un \u25e6hn\u22121.\nIn this update, Un is generated as a random rotation matrix, and \u03b1n is determined\nas a gradient ascent update (starting from \u03b1 = 0) for the maximization of\n\u03b1 7\u2192\u2113(\u03d5\u03b1,Un \u25e6hn\u22121).\n(Here, the current value hn\u22121 is not revisited, therefore providing a \u201cgreedy\u201d opti-\nmization method.)\n18.1. NORMALIZING FLOWS\n435\nLetting zn,k = hn(xk), the chain rule implies that\n\u2113(\u03d5\u03b1,Un \u25e6hn\u22121)) =\nN\nX\nk=1\nlogfZ(\u03d5\u03b1,Un(zn\u22121,k)) +\nN\nX\nk=1\nlog|det\u03d5\u03b1,Un(zn\u22121,k)|\n+\nN\nX\nk=1\nlog|det\u2202xhn\u22121(xk)|.\nSince the last term does not depend on \u03b1, we see that it suffices to keep track of the\n\u201cparticle\u201d locations, zn\u22121,k to be able to compute \u03b1n. Note also that these locations\nare easily updated with zn,k = \u03d5\u03b1n,Un(zn\u22121,k).\n18.1.3\nNeural implementation\nThis iterated composition of diffeomorphisms obviously provides a neural architec-\nture similar to those discussed in chapter 11. Fixing the number of iterations to be,\nsay, m, one can consider families of diffeomorphisms (\u03d5\u03b8) indexed by a parameter w\n(we had w = (\u03b1,U) in the previous discussion), and optimize (18.1) over all functions\nh taking the form h = \u03d5wm \u25e6\u00b7\u00b7\u00b7 \u25e6\u03d5w1. Letting zj,k = \u03d5wj \u25e6\u00b7\u00b7\u00b7 \u25e6\u03d5w1(xk) for j \u2264m (with\nz0,k = xk), we can write\n\u2113(h) =\nN\nX\nk=1\nlogfZ(zm,k) +\nN\nX\nk=1\nm\nX\nj=1\nlog|det\u2202x\u03d5wj(zj\u22121,k)|.\nNormalizing flows in this form are described in [161, 107, 148]. The gradient of \u2113\nwith respect to the parameters w1,...,wm can be computed by backpropagation. We\nnote however that, unlike typical neural implementations, the parameters may come\nwith specific constraints, such as U \u2208Od(R) when w = (\u03b1,U), so that the gradient\nand associated displacement may have to be adapted compared to standard gradient\nascent implementations (see section 20.6.3 for a discussion of first-order implemen-\ntations of gradient methods for functions of orthogonal matrices, and [1] for more\ngeneral methods on optimization over matrix groups).\n18.1.4\nTime-continuous version\nIn section 11.6, we described how diffeomorphisms could be generated as flows of\ndifferential equations, and this remark can be used to provide a time-continuous\nversion of normalizing flows. Using (11.3), one generates trajectories z(\u00b7) by solving\nover, say, [0,T ]\n\u2202tz(t) = \u03c8w(t)(z(t))\n436\nCHAPTER 18. DEEP GENERATIVE METHODS\nwith z(0) = x for some function w : t 7\u2192w(t). Letting z(t) = hw(t,x) (which defines\nhw), we know that, under suitable assumptions on \u03c8, the mapping x 7\u2192hw(t,x) is a\ndiffeomorphism of Rd. One can then maximize\n\u2113(hw(T,\u00b7)) =\nN\nX\nk=1\nlogfZ(hw(T ,xk)) +\nN\nX\nk=1\nlog|det\u2202xhw(T ,xk)|\nwith respect to the function w. Let zk(t) = hw(t,xk) and Jk(t) = log|det\u2202xhw(t,xk)|. We\nhave, by definition\n\u2202tzk(t) = \u03c8w(t)(zk(t))\nwith zk(0) = xk. One can also show that\n\u2202tJk(t) = \u2207\u00b7 \u03c8w(t)(zk(t))\nwith Jk(0) = 0, where the r.h.s. is the divergence of \u03c8w(t) evaluated at zk(t). We\nprovide a quick (and formal) justification of this fact. First note that differentiating\n\u2202thw(t,x) = \u03c8w(t)(hw(t,x)) with respect to x yields\n\u2202t\u2202xhw(t,x) = \u2202x\u03c8w(t)(hw(t,x))\u2202xhw(t,x).\nThe mapping J : A 7\u2192log|det(A)| is differentiable on the set of invertible matrices\nand is such that dJ (A)H = trace(A\u22121H). Applying the chain rule, we find\n\u2202t log|det\u2202xhw(t,x)| = trace(\u2202xhw(t,x)\u22121\u2202x\u03c8w(t)(hw(t,x))\u2202xhw(t,x))\n= trace(\u2202x\u03c8w(t)(hw(t,x))) = \u2207\u00b7 \u03c8w(t)(hw(t,x)).\nFrom this, it follows that the time-continuous normalizing flow problem can be\nreformulated as maximizing\nN\nX\nk=1\nlogfZ(zk(T )) +\nN\nX\nk=1\nJk(T)\nsubject to \u2202tzk(t) = \u03c8w(t)(zk(t)), \u2202tJk(t) = \u2207\u00b7 \u03c8w(t)(zk(t)), zk(0) = xk, Jk(0) = 0. This is\nan optimal control problem, whose analysis can be done similarly to that made in\nsection 11.6.1, provided that \u2207\u00b7 \u03c8w(t) can be expressed in closed form.\nNote that the inverse of hw(T,\u00b7), which provides the generative model going from\nZ to X can also be obtained as the solution of an ODE. Namely, if one solves the\ndifferential equation\n\u2202tx(t) = \u2212\u03c8w(T\u2212t)(x(t))\nwith initial condition x(0) = z, then x(T ) solves the equation hw(T ,\u00b7) = z.\n18.2. NON-DIFFEOMORPHIC MODELS AND VARIATIONAL AUTOENCODERS437\n18.2\nNon-diffeomorphic models and variational autoencoders\n18.2.1\nGeneral framework\nThe previous discussion addressed the situation X = g(Z) when g is a diffeomor-\nphism, which required, in particular, that X and Z are real vectors with identical di-\nmensions. This may not always be desirable, as one may prefer a small-dimensional\nvariable Z (in the spirit of the factor analysis methods discussed in chapter 20), or\na high-dimensional Z to increase, for example the modeling power. In addition, the\nobservation variables may be discrete, which precludes the use of the change of vari-\nables formula. In such cases, Z has to be treated as a hidden variable using one of\nthe methods discussed in chapter 16.\nIt will convenient to model the generative process in the form of a conditional\ndistribution of X given Z rather than a deterministic function. We place ourselves\nin the framework of chapter 16 (with slightly modified notation) and let RX and RZ\ndenote the measured spaces over where X and Z take their values, with measures\n\u00b5X and \u00b5Z, and assume that the conditional distribution of X given Z = z has den-\nsity fX(x | z,\u03b8) with respect to \u00b5X, for some parameter \u03b8. We also assume that Z\nhas a distribution with density fZ with respect to \u00b5Z, that we assume given and un-\nparametrized. One can then directly apply the algorithms provided in chapter 16,\nand in particular the variational methods described in section 16.4.4 with an appro-\npriate definition of the approximation of the conditional density of Z given X. An\nimportant example in this context is provided by variational autoencoders (VAEs)\nthat we now present.\n18.2.2\nGenerative model for VAEs\nVAEs [103, 104] model X \u2208Rd as X = g(Z,\u03b8)+\u03f5 where \u03f5 is a centered Gaussian noise\nwith covariance matrix Q. The function g is typically non-linear, and VAEs have\nbeen introduced with this function modeled as a deep neural network (see chap-\nter 11). Letting \u03d5N (\u00b7; 0,Q) denote the p.d.f. of the Gaussian distribution N (0,Q),\nthe conditional distribution of X given Z = z has density\nfX(x | z,\u03b8) = \u03d5N (x \u2212g(z,\u03b8)); 0,Q)\nwith respect to Lebesgue\u2019s measure on Rd.\nFollowing the procedure in section 16.4.4, we define an approximation of the\nconditional distribution of Z given X. Assuming that Z \u2208Rp, we let this distri-\nbution be N (\u00b5(x,w),\u03a3(x,w)) for some functions \u00b5 and \u03a3, w being a parameter. To\nensure that \u03a3 \u2ab00, we will represent it in the form \u03a3(x,w) = S(x,w)2 where S is a sym-\nmetric matrix. In [103], both functions \u00b5 and S are represented as neural networks\n438\nCHAPTER 18. DEEP GENERATIVE METHODS\nparametrized by w. The joint density of X and Z is such that\nlogfX,Z(x,z; \u03b8,Q) = log\u03d5N (x \u2212g(z,\u03b8)); 0,Q) + logfZ(z)\n= \u22121\n2(x \u2212g(z,\u03b8))T Q\u22121(x \u2212g(z,\u03b8)) \u22121\n2 logdetQ \u2212d\n2 log2\u03c0 + logfZ(z)\nWe also have\nlog\u03d5N (z; \u00b5(x,w),S(x,w)2) = \u22121\n2(z\u2212\u00b5(x,w))T S(x,w)\u22122(z\u2212\u00b5(x,w))\u2212logdetS(x,w)\u2212p\n2 log2\u03c0.\nWe can then rewrite the algorithm in (16.15) as\n\uf8f1\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f2\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f3\n\u03b8n+1 = \u03b8n + \u03b3n+1\u2202\u03b8 logfX,Z(Xn+1,Zn+1;\u03b8n,Qn)\nQn+1 = Qn + \u03b3n+1\u2202Q logfX,Z(Xn+1,Zn+1;\u03b8n,Qn)\nwn+1 = wn + \u03b3n+1 log\n \nfX,Z(Xn+1,Zn+1;\u03b8n,Qn)\n\u03d5N (Xn+1 ; \u00b5(Xn+1,wn),S(Xn+1,wn)2)\n!\n\u00d7 \u2202w log\u03d5N (Xn+1 ; \u00b5(Xn+1,wn),S(Xn+1,wn)2)\n(18.2)\nwhere Xn+1 is drawn uniformly from the training data and\nZn+1 \u223cN (\u00b5(Xn+1,wn),S(Xn+1,wn)2).\nThe derivatives in this system can be computed from those of g,\u00b5 and S (typically\ninvolving back-propagation) and the expression of the derivatives of the determinant\nand inverse of a matrix provided in (1.4) and (1.6).\nThe computation can be simplified if one assumes that fZ is the p.d.f. of a stan-\ndard Gaussian, i.e., fZ = \u03d5N (\u00b7;0,IdRp). Indeed, in that case, the integral in (16.11),\nwhich is, using the current notation\nZ\nRp log \u03d5N (x \u2212g(z,\u03b8); 0,Q)\u03d5N (z; 0,IdRp)\n\u03d5N (z; \u00b5(x,w),S(x,w)2)\n\u03d5N (z; \u00b5(x,w),S(x,w)2)dz,\n(18.3)\ncan be partially computed. For any two p-dimensional Gaussian p.d.f.\u2019s, one has\nZ\nRp log\u03d5N (z; \u00b51,\u03a31) \u03d5N (z; \u00b52,\u03a32)dz = \u22121\n2trace(\u03a3\u22121\n1 \u03a32) \u22121\n2(\u00b52 \u2212\u00b51)T \u03a3\u22121\n1 (\u00b52 \u2212\u00b51)\n\u22121\n2 logdet(\u03a31) \u2212p\n2 log(2\u03c0).\n(18.4)\nAs a consequence, (18.3) becomes\n\u22121\n2Ew\n\u0010\n(X \u2212g(Z,\u03b8))T Q\u22121(X \u2212g(Z,\u03b8))\n\u0011\n\u22121\n2 logdetQ \u2212d\n2 log2\u03c0\n\u2212Ew\n\u00121\n2trace(S(X,w)2) + 1\n2|\u00b5(X,w)|2 \u2212logdet(S(X,w))\n\u0013\n+ p\n2,\n(18.5)\n18.2. NON-DIFFEOMORPHIC MODELS AND VARIATIONAL AUTOENCODERS439\nwhere Ew denotes the expectation for the random variable (X,Z) where X follows a\nuniform distribution over training data and the conditional distribution of Z given\nX = x is N (\u00b5(x,w), S(x,w)2).\nThe algorithm proposed in Kingma and Welling [103] introduces a change of\nvariable Z = \u00b5(X,w) + S(X,w)U where U \u223cN (0,IdRp), rewriting (18.5) as\n\u22121\n2E\n\u0010\n(X \u2212g(\u00b5(X,w) + S(X,w)U,\u03b8))T Q\u22121(X \u2212g(\u00b5(X,w) + S(X,w)U,\u03b8))\n\u0011\n\u2212Ew\n\u00121\n2trace(S(x,w)2) + 1\n2|\u00b5(X,w)|2 \u2212logdet(S(X,w))\n\u0013\n\u22121\n2 logdetQ \u2212d\n2 log2\u03c0 + p\n2,\n(18.6)\nwith a modified version of (18.2). Letting\nF(\u03b8,Q,w,x,u) = \u22121\n2(x \u2212g(\u00b5(x,w) \u2212S(x,w)U,\u03b8))T Q\u22121(x \u2212g(\u00b5(x,w) \u2212S(x,w)U,\u03b8))\n\u22121\n2 logdetQ \u22121\n2trace(S(x,w)2) \u22121\n2|\u00b5(x,w)|2 + logdet(S(x,w))\nthe resulting algorithm is\n\uf8f1\uf8f4\uf8f4\uf8f4\uf8f4\uf8f2\uf8f4\uf8f4\uf8f4\uf8f4\uf8f3\n\u03b8n+1 = \u03b8n + \u03b3n+1\u2202\u03b8F(\u03b8n,Qn,wn,Xn+1,Un+1)\nQn+1 = Qn + \u03b3n+1\u2202QF(\u03b8n,Qn,wn,Xn+1,Un+1)\nwn+1 = wn \u2212\u03b3n+1\u2202wF(\u03b8n,Qn,wn,Xn+1,Un+1)\n(18.7)\nwhere Xn+1 is drawn uniformly from the training data and Un+1 \u223cN (0,IdRp).\n18.2.3\nDiscrete data\nThis framework can be easily adapted to situations in which the observations are\ndiscrete. Consider, as an example, the situation in which X takes values in {0,1}V ,\nwhere V is a set of vertexes, i.e., X is a binary Markov random field on V . Assume,\nas a generative model, that conditionally to the latent variable Z \u2208Rp, the variables\nX(s),s \u2208V are independent and X(s) follows a Bernoulli distribution with parame-\nter gs(z,\u03b8), where g : Rp \u2192[0,1]V . Assume also that Z \u223cN (0,IdRp), and define, as\nabove, an approximation of the conditional distribution of Z given X = x as a Gaus-\nsian with mean \u00b5(x,w) and covariance matrix S(x,w)2. Then, the joint density of X\nand Z (with respect to the product of the counting measure on {0,1}V and Lebesgue\u2019s\nmeasure on Rp) is\nlogfX,Z(x,z; \u03b8) = xlogg(z,\u03b8) + (1 \u2212x)log(1 \u2212g(z,\u03b8)) + log\u03d5N (z; 0,IdRp)\n440\nCHAPTER 18. DEEP GENERATIVE METHODS\nand (18.2) becomes\n\uf8f1\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f2\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f3\n\u03b8n+1 = \u03b8n + \u03b3n+1\u2202\u03b8 logfX,Z(Xn+1,Zn+1;\u03b8n,Qn)\nwn+1 = wn + \u03b3n+1 log\n \nfX,Z(Xn+1,Zn+1;\u03b8n)\n\u03d5N (Xn+1 ; \u00b5(Xn+1,wn),S(Xn+1,wn)2)\n!\n\u00d7 \u2202w log\u03d5N (Xn+1 ; \u00b5(Xn+1,wn),S(Xn+1,wn)2)\n(18.8)\n18.3\nGenerative Adversarial Networks (GAN)\n18.3.1\nBasic principles\nSimilarly to the methods discussed so far, GANs [82], use a one-step nonlinear gen-\nerator X = g(Z,\u03b8), with \u03b8 \u2208RK, to model observed data (we here switch back to a\ndeterministic relation), where Z has a known distribution, with p.d.f. fZ, for exam-\nple Z \u223cN (0,IdRp). However, unlike the exact or approximate likelihood maximiza-\ntion that were discussed in sections 18.1 and 18.2, GANs us a different criterion for\nestimating the parameter \u03b8 by minimizing metrics that can be approximated by opti-\nmizing a classifier. The classifier is a function x 7\u2192f (x,w), parametrized by w \u2208RM,\nwhose goal is to separate simulated samples from real ones: it takes values in [0,1]\nand estimates the (posterior) probability that its input x is real. GANs\u2019 adversarial\nparadigm consists in estimating \u03b8 and w together so that generated data, using \u03b8,\nare indistinguishable from real ones using the optimal w. Their basic structure is\nsummarized in Figure 18.1.\nClassifier\nPrediction\nW\nData\nSimulation\nGenerator\n\u03b8\nNoise\nFigure 18.1: Basic structure of GANs: W is optimized to improve the prediction problem:\n\u201creal data\u201d vs. \u201csimulation\u201d. Given W, \u03b8 is optimized to worsen the prediction.\n18.3.2\nObjective function\nLet P\u03b8 denote the distribution of g(Z,\u03b8), and Ptrue the target distribution of \u201creal\ndata.\u201d One can formalize the \u201creal data\u201d vs. \u201csimulation\u201d problem with a pair of\n18.3. GENERATIVE ADVERSARIAL NETWORKS (GAN)\n441\nrandom variables (X,Y) where Y follows a Bernoulli distribution with parameter\n1/2, and the conditional distribution of X given Y is Ptrue when Y = 1 and P\u03b8 when\nY = 0. Given a loss function r : {0,1} \u00d7 [0,1] \u2192[0,+\u221e), one can define\nU(\u03b8,w) = E\u03b8(r(Y,f (X,w)))\nand\nU\u2217(\u03b8) = min\nw\u2208RM U(\u03b8,w).\nWe want to maximize U\u2217or, equivalently, solve the optimization problem\n\u03b8\u2217= argmax\u03b8 min\nw\u2208RM U(\u03b8,w).\nNote that\n2U(\u03b8,w) = Etrue(r(1,f (X,w))) + E\u03b8(r(0,f (X,w)))\nso that choosing the cost requires to specify the two functions t 7\u2192r(1,t) and t 7\u2192\nr(0,t). In Goodfellow et al. [82], they are:\nr(1,t) = \u2212logt\nr(0,t) = \u2212log(1 \u2212t).\n(18.9)\n18.3.3\nAlgorithm\nUsing costs in (18.9), one must compute\n\u03b8\u2217= argmin max\nw\u2208RM\n\u0012\nEtrue(logf (X,w)) + E\u03b8(log(1 \u2212f (X,w)))\n\u0013\n= argmin max\nw\u2208RM\n\u0012\nEtrue(logf (X,w)) + E(log(1 \u2212f (g(Z,\u03b8),w)))\n\u0013\n.\nSuch min-max, or saddle-point problem are numerically challenging. The fol-\nlowing algorithm was proposed in Goodfellow et al. [82], and also includes a stochas-\ntic approximation component. Indeed, in practice, Etrue is only known through the\nobservation of training data, say x1,...,xN. Moreover, E\u03b8 is only accessible through\nMonte-Carlo simulation, so that both expectations can only be approximated through\nfinite-sample averaging.\nAlgorithm 18.1 (GAN training algorithm)\n1. Extract a batch of m examples from training data, simulate m samples accord-\ning to P\u03b8 and run a few (stochastic) gradient ascent steps with fixed \u03b8 to update w,\nreplacing expectations by averages.\n2. Generate m new samples of Z and update \u03b8 with fixed w by iterating a few\nsteps of (stochastic) gradient descent.\n442\nCHAPTER 18. DEEP GENERATIVE METHODS\n18.3.4\nAssociated probability metric and Wasserstein GANs\nLet F be the family of all measurable functions: f : Rd \u2192[0,1]. Given two possi-\nble probability distributions P1,P2 (with associated expectations denoted E1,E2) of a\nrandom variable X taking values in Rd, consider the function\nD(P1,P2) = 2log2 + max\nf \u2208F\n\u0012\nE1(logf (X)) + E2(log(1 \u2212f (X)))\n\u0013\nAssume that X under P1 (resp. under P2) has p.d.f. g1 (resp. g2) with respect to\nLebesgue\u2019s measure (this assumption is not needed for the following to hold, but\nmakes the discussion more elementary). Then\nE1(logf (X)) + E2(log(1 \u2212f (X))) =\nZ\nRd(g1 logf + g2 log(1 \u2212f ))dx\nwhich is maximal at f \u2217= g1/(g1 + g2). For this f \u2217,\n2log2 + E1(logf \u2217(X)) + E2(log(1 \u2212f \u2217(X))) =\nZ\nRd g1 log\n2g1\ng1 + g2\ndx +\nZ\nRd g2 log\n2g2\ng1 + g2\ndx\n= KL\n\u0012g1 + g2\n2\n,g1\n\u0013\n+ KL\n\u0012g1 + g2\n2\n,g2\n\u0013\nThis expression is called the Jensen-Shannon divergence between g1 and g2. It is al-\nways non-negative, and vanishes only when g1 = g2.\nSo, D : (P1,P2) 7\u2192D(P1,P2) can be interpreted as a way to evaluate the difference\nbetween two probability distributions on Rd. One can then define\n\u02c6D(P1,P2) = max\nw\u2208RM\n\u0012\nE1(logf (X,w)) + E2(log(1 \u2212f (X,w)))\n\u0013\nas an approximation of D in which the set of all possible functions with values in\n[0,1] is replaced by those arising from the GAN classification network, parametrized\nby w. This approximation is useful when g1,g2 are only observable through random\nsampling or simulation. With this interpretation, GANs minimize \u02c6D(Ptrue,P\u03b8).\nThis discussion suggests that new types of GAN may be designed using other\ndiscrepancy functions between probability distributions, provided they can be ex-\npressed in terms of the maximization of some quantity over some space of functions.\nConsider, for example the norm in total variation, defined by (for discrete distribu-\ntions)\nDvar(P1,P2) = 1\n2\nX\nx\n|P1(x) \u2212P2(x)|.\nor, in the general case Dvar(P1,P2) = maxA(P1(A) \u2212P2(A)).\n18.3. GENERATIVE ADVERSARIAL NETWORKS (GAN)\n443\nIf F is the space of continuous functions f : Rd \u2192[0,1], then we also have (under\nmild assumptions on P1 and P2)\nDvar(P1,P2) = max\nf \u2208F (E1(f ) \u2212E2(f )).\nSince neural nets typically generate continuous functions with values in [0,1], one\ncould train GANs by maximizing\n\u02c6Dvar(P1,P2) = max\nw\u2208RM\n\u0012\nE1(f (X,w)) \u2212E2(f (X,w))\n\u0013\nHowever, the total variation distance is too crude to allow for meaningful compar-\nisons between distributions. For example, the distance between two Dirac distri-\nbutions at, say, x1 and x2 in Rd is always 1, whatever the distance between x1 and\nx2, unless x1 = x2. A more sensitive distance can be defined based on the notion of\noptimal transport.\nThe Monge-Kantorovich, also called Wasserstein, and sometimes also called \u201cearth-\nmover\u201d, distance evaluates the minimal total distance along which \u201cmass\u201d needs to\nbe transported to transform a distribution, P1, into another, P2. Its mathematical\ndefinition is\nDw(P1,P2) = inf\nQ\nZ\nRd\u00d7Rd |x1 \u2212x2|Q(dx1,dx2)\nwhere the inf is computed over all joint distributions on Rd\u00d7Rd whose first marginal\nis P1 and second marginal P2. Note that the distance Dw between \u03b4x1 and \u03b4x2 now is\n|x1 \u2212x2|.\nThe Wasserstein distance can also be defined by\nDw(P1,P2) = max\nf \u2208F (E1(f ) \u2212E2(f ))\nwhere F is now the space of contractive (or 1-Lipschitz) functions, i.e., f \u2208F if and\nonly if, for all x1,x2 \u2208Rd, |f (x1) \u2212f (x2)| \u2264|x1 \u2212x2|.\nUsing the fact that a neural network with all weights bounded by a constant K\ngenerates a function whose Lipschitz constant is controlled solely by K, one can then\napproximate (up to a multiplicative constant) the Wasserstein distance by\n\u02c6Dw(P1,P2) = max\nw\u2208W\n\u0012\nE1(f (X,w)) \u2212E2(f (X,w))\n\u0013\nwhere W is the set of all weights bounded by a fixed constant. Given the distribution\nPtrue and the model P\u03b8, Wasserstein GANs (WGANs [11]) must then solve the saddle-\npoint problem\nU(\u03b8,w) = max\nw\u2208W\n\u0012\nEtrue(f (X,w)) \u2212E\u03b8(f (X,w))\n\u0013\n444\nCHAPTER 18. DEEP GENERATIVE METHODS\nand\nU\u2217(\u03b8) = min\nw\u2208RM U(\u03b8,w),\nwith an algorithm similar to that described earlier.\nAs a final reference, we note the improved WGAN algorithm introduced in Gul-\nrajani et al. [84] in which the boundedness constraint in the weights is replaced by\nan explicit control of the derivative in x of the function f . More precisely, introduce\na random variable Z with distribution \u02dcP\u03b8 equal (1 \u2212U)X + UX\u2032 where U is uni-\nformly distributed over [0,1] and X and X\u2032 are independent respectively following\nthe distribution Ptrue and P\u03b8. Then, the following approximation of the Wasserstein\ndistance between Ptrue and P\u03b8 that is used in Gulrajani et al. [84]:\n\u02c6Dw(Ptrue,P\u03b8) = max\nw\u2208W\n\u0012\nEtrue(f (X,w)) \u2212E\u03b8(f (X,w)) \u2212\u02dcE\u03b8((|\u2202zf (Z,w)| \u22121)2)\n\u0013\n.\n18.4\nReversed Markov chain models\n18.4.1\nGeneral principles\nThe discussions in sections 18.2 and 18.3 can be applied to sequences of structural\nequations (describing finite Markov chains) in the form\n\uf8f1\uf8f4\uf8f4\uf8f4\uf8f4\uf8f2\uf8f4\uf8f4\uf8f4\uf8f4\uf8f3\nZ0 = \u03be0\nZk+1 = g(Zk,\u03bek;\u03b8k), k = 0,...,m \u22121\nX = Zm\nwhere \u03be0,...,\u03bem\u22121 are random variables with fixed distribution.\nIndeed, letting \u02dcZ = (\u03be0,...,\u03ben\u22121) and \u02dc\u03b8 = (\u03b80,...,\u03b8m\u22121) the whole system can be\nconsidered as a function X = G( \u02dcZ, \u02dc\u03b8) as considered in these sections. This repre-\nsentation, however, includes a large number of hidden variables, and it is unclear\nwhether much improvement can be added to the case m = 1 to justify the additional\ncomputational load.\nReversed Markov chain models use a different generative approach in that they\nfirst model a forward Markov chain Zn,n \u22650 which is ergodic with known (and\neasy to sample from) limit distribution Q\u221e, and initial distribution Qtrue, the true\ndistribution of the data. If one fixes a large enough number of steps, say, \u03c4, then it is\nreasonable to assume that Z\u03c4 approximately follows the limit distribution, Q\u221e. One\ncan then (approximately) sample from Qtrue by sampling \u02dcZ0 according to Q\u221eand\nthen applying \u03c4 steps of the time-reversed Markov chain.\n18.4. REVERSED MARKOV CHAIN MODELS\n445\nReversed chains were discussed in section 12.3.3. Assuming that Qtrue and P(z,\u00b7)\nhave a density with respect to a fixed measure \u00b5 on RZ, we found that \u02dcZk = Z\u03c4\u2212k is a\nnon-homogeneous Markov chain whose transition probability \u02dcPk(x,A) = P( \u02dcZk+1 \u2208A |\n\u02dcZk = x) has density\n\u02dcpk(x,y) = p(y,x)q\u03c4\u2212k\u22121(y)\nq\u03c4\u2212k(x)\nwith respect to \u00b5, where qn is the p.d.f. of Qn = QtruePn, the distribution of Zn.\nThe distributions Qn,n \u22650 are unknown, since they depend on the data dis-\ntribution Ptrue, and the transition probabilities above must be estimated from data\nto provide a sampling algorithm from the reversed Markov chain. While, at first\nglance, this does not seem like a simplification of the problem, because one now has\nto sample from a potentially large number (\u03c4) of distributions instead of one, this\nleads, with proper modeling and some intensive learning, to efficient and accurate\nsampling algorithms.\nSeveral factors can indeed make this approach achievable. First, the forward\nchain should be making small changes to the current configuration at each step (e.g.,\nadding a small amount of noise). This ensures that the reversed transition probabili-\nties \u02dcpk(x,\u00b7) are close to Dirac distributions and are therefore likely to be well approx-\nimated by simple unimodal distributions such as Gaussians. Second, the estimation\nproblem does not have hidden data: given an observed sample, one can simulate \u03c4\nsteps of the forward chain to obtain, after reversing the order, a full observation of\nthe reversed chain. Third, in some cases, analytical considerations can lead to partial\ncomputations that facilitate the modeling of the reversed transitions.\n18.4.2\nBinary model\nWe now take some examples, starting with a discrete one. Let Qtrue be the distribu-\ntion of a binary random field with state space {0,1} over a set of vertexes V , i.e., with\nthe notation of section 13.2, RX = F (V ) with F = {0,1}. Fix a small \u03f5 > 0 and define\nthe transition probability p(x,y) for x,y \u2208F (V ) by\np(x,y) =\nY\ns\u2208V\n\u0010\n(1 \u2212\u03f5)1y(s)=x(s) + \u03f51y(s)=1\u2212x(s)\n\u0011\n.\nSince p(x,y) > 0 for all x and y, the chain converges (uniformly geometrically) to its\ninvariant probability Q\u221eand one easily checks that this probability is such that all\nvariables are independent Bernoulli random variables with success probability 1/2.\nAssuming that \u03c4 is large enough so that Q\u03c4 \u2243Q\u221e, the sampling algorithm initializes\nthe reversed chain as independent Bernoulli(1/2) variables and runs \u03c4 steps using\nthe transitions \u02dcpk which must be learned from data.\n446\nCHAPTER 18. DEEP GENERATIVE METHODS\nFor this model, we have\nqk(x) =\nX\ny\u2208F (V )\nqk\u22121(y)p(y,x).\nFor this transition, the probabililty of flipping two or more values of y is\n1 \u2212(1 \u2212\u03f5)N \u2212N\u03f5(1 \u2212\u03f5)N\u22121 = N(N \u22121)\n2\n\u03f52 + o(\u03f52)\nwith N = |V |. We will write x \u223cs y if y(s) = 1 \u2212x(s) and y(t) = x(t) for s , t, and we will\nwrite x \u223cy if x \u223cs y for some s. With this notation, we have\nqk(x) = (1 \u2212N\u03f5)qk\u22121(x) + \u03f5\nX\ny:y\u223cx\nqk\u22121(y) + O(\u03f52)\nSince it implies that qk(x) = qk\u22121(x) + o(\u03f5), this expression can be reversed as\nqk\u22121(y) = (1 + N\u03f5)qk(y) \u2212\u03f5\nX\nx:x\u223cy\nqk(x) + O(\u03f52)\nSimilarly, we have\np(y,x) = (1 \u2212N\u03f5)1x=y + \u03f51x\u223cy + O(\u03f52).\nThis gives\np(y,x)qk\u22121(y) = qk(x)1x=y \u2212\u03f51x=y\nX\nx\u2032:x\u2032\u223cy\nqk(x\u2032) + \u03f5qk(y)1x\u223cy + O(\u03f52),\nand we finally get\n\u02dcpk(x,y) =\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed1 \u2212\u03f5\nX\nx\u2032:x\u2032\u223cy\nq\u03c4\u2212k(x\u2032)\nq\u03c4\u2212k(x)\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f81x=y + \u03f5q\u03c4\u2212k(y)\nq\u03c4\u2212k(x)1x\u223cy + O(\u03f52)\nIf one lets \u03c3(s)\nk (x) = q\u03c4\u2212k(y)\nq\u03c4\u2212k(x) with y \u223cs x, and defines\n\u02c6pk(x,y) =\nY\ns\u2208V\n\u0012\n(1 \u2212\u03f5\u03c3(s)\nk (x))1y(s)=x(s) + \u03f5\u03c3(s)\nk (x)1y(s)=1\u2212x(s)\n\u0013\n,\none checks easily that \u02c6pk(x,y) = \u02dcpk(x,y)+O(\u03f52). This suggests modeling the reversed\nchain using transitions \u02c6pk, for which the mapping x 7\u2192(\u03c3(s)\nk (x),s \u2208V ) needs to be\nlearned from data (for example using a deep neural network). Note that 1 \u2212\u03c3k(x) is\nprecisely the score function introduced for discrete distributions in remark 17.8.\n18.4. REVERSED MARKOV CHAIN MODELS\n447\n18.4.3\nModel with continuous variables\nWe now switch to an example with vector-valued variables, RX = Rd, and assume\nthat the forward Markov chain is such that, conditionally to Xn = x,\nXn+1 \u223cN (x + hf (x),\n\u221a\nhIdRd),\nwhere f is C1. We saw in section 12.3.7 that, when f = \u2212\u2207H/2 for a C2 function\nH such that exp(\u2212H) is integrable, this chain converges (approximately for small h)\nto a limit distribution with p.d.f. (with respect to Lebesgue\u2019s measure) proportional\nto exp(\u2212H). In the linear case, in which f (x) = \u2212Ax/2 for some positive-definite\nsymmetric matrix A, so that H(x) = 1\n2xT Ax, the limit distribution can be identified\nexactly as N (0,\u03a3h) where \u03a3h satisfies the equation\nA\u03a3h + \u03a3hA \u2212h\n2A2 \u22122IdRd = 0\nwhose solution is \u03a3h = (A \u2212hA2/4)\u22121 (details being left to the reader). This implies\nthat this limit distribution can be easily sampled from for any choice of A.\nWe now return to general f \u2019s and make, like in the discrete case, a first-order\nidentification of the reversed chain. We note that, for any smooth function \u03b3,\nE(\u03b3(Xn+1) | Xn = x) = E(\u03b3(x + hf (x) +\n\u221a\nhU))\nwhere U \u223cN (0,IdRd). Making the second order expansion\n\u03b3(x + hf (x) + hU) = \u03b3(x) +\n\u221a\nh\u2207\u03b3(x)T U + h\u2207\u03b3(x)T f (x) + h\n2UT \u22072\u03b3(x)U + o(h)\nand taking the expectation gives\nE(\u03b3(Xn+1) | Xn = x) = \u03b3(x) + h\u2207\u03b3(x)T f (x) + h\n2\u2206\u03b3(x) + o(h).\n(18.10)\nConsidering the reversed chain, and letting qk denote the p.d.f. of Xk for the\nforward chain, we have\nE(\u03b3(Xk\u22121) | Xk = x) =\nZ\nRd \u03b3(y) \u02dcpk(x,y)dy\n=\nZ\nRd \u03b3(y)p(y,x)qk\u22121(y)\nqk(x) dy\n=\n1\n(2\u03c0h)d/2\nZ\nRd \u03b3(y)qk\u22121(y)\nqk(x) e\u22121\n2h|x\u2212y\u2212hf (y)|2dy\n=\n1\n(2\u03c0)d/2\nZ\nRd \u03b3(x \u2212\n\u221a\nhu)qk\u22121(x \u2212\n\u221a\nhu)\nqk(x)\ne\u22121\n2|u\u2212\n\u221a\nhf (x\u2212\n\u221a\nhu)|2dy,\n448\nCHAPTER 18. DEEP GENERATIVE METHODS\nwith the change of variable u = (x \u2212y)/\n\u221a\nh. We make a first-order expansion of the\nterms in this integral, with\n\u03b3(x \u2212\n\u221a\nhu)qk\u22121(x \u2212\n\u221a\nhu) = \u03b3(x)qk\u22121(x) \u2212\n\u221a\nh\u2207(\u03b3qk\u22121)(x)T u + h\n2uT \u22072(\u03b3qk\u22121)(x)u + o(h)\nand\ne\u22121\n2|u\u2212\n\u221a\nhf (x\u2212\n\u221a\nhu)|2 = e\u22121\n2|u|2e\n\u221a\nhuT f (x)\u2212huT df (x)u\u22121\n2|f (x)|2+o(h)\n= e\u22121\n2|u|2  \n1 +\n\u221a\nhuT f (x) \u2212huT df (x)u \u2212h\n2|f (x)|2 + h\n2|uT f (x)|2 + o(h)\n!\n.\nTaking products\n\u03b3(x \u2212\n\u221a\nhu)qk\u22121(x \u2212\n\u221a\nhu)e\u22121\n2|u\u2212\n\u221a\nhf (x\u2212\n\u221a\nhu)|2\n= e\u22121\n2|u|2\u03b3(x)qk\u22121(x)\n \n1 +\n\u221a\nhuT f (x) \u2212huT df (x)u \u2212h\n2|f (x)|2 + h\n2|uT f (x)|2\n!\n+ e\u22121\n2|u|2  \n\u2212\n\u221a\nh\u2207(\u03b3qk\u22121)(x)T u \u2212h(\u2207(\u03b3qk\u22121)(x)T u)(f (x)T u) + h\n2uT \u22072(\u03b3qk\u22121)(x)u\n!\n+ o(h)\nWe now take the integral with respect to u (recall that E(UT AU) = trace(A) if A is\nany square matrix and U is standard Gaussian), so that\n1\n(2\u03c0)d/2\nZ\nRd \u03b3(x \u2212\n\u221a\nhu)qk\u22121(x \u2212\n\u221a\nhu)e\u22121\n2|u\u2212\n\u221a\nhf (x\u2212\n\u221a\nhu)|2du\n= \u03b3(x)qk\u22121(x) + h\n\u0012\n\u2212\u03b3(x)qk\u22121(x)\u2207\u00b7 f (x) \u2212\u2207(\u03b3qk\u22121)(x)T f (x) + 1\n2\u2206(\u03b3qk\u22121)(x)\n\u0013\n+ o(h)\n= qk\u22121(x)\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\u03b3(x) + h\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\u2212\u03b3(x)\u2207\u00b7 f (x) \u2212\n \u2207(\u03b3qk\u22121)(x)\nqk\u22121(x)\n!T\nf (x) + 1\n2\n\u2206(\u03b3qk\u22121)(x)\nqk\u22121(x)\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8+ o(h)\nTo compute an expansion of qk(x), it suffices to take \u03b3 = 1 above, so that\nqk(x) = qk\u22121(x)\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed1 + h\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\u2212\u2207\u00b7 f (x) \u2212\n \u2207qk\u22121(x)\nqk\u22121(x)\n!T\nf (x) + 1\n2\n\u2206qk\u22121(x)\nqk\u22121(x)\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8+ o(h).\nWe now take the first-order expansion of the ratio, removing terms that cancel, and\nget\nE(\u03b3(Xk\u22121) | Xk = x) = \u03b3(x) \u2212h\u2207\u03b3(x)T f (x) + h\u2207\u03b3(x)T\n \u2207qk\u22121(x)\nqk\u22121(x)\n!\n+ h\n2\u2206\u03b3(x) + o(h)\nComparing with (18.10), we find that \u02dcXk = X\u03c4\u2212k behaves, for small h, like the\nnon-homogeneous Markov chain such that the conditional distribution of \u02dcXk+1 given\n\u02dcXk = x is N (x \u2212hf (x) \u2212hs\u03c4\u2212k\u22121(x),\n\u221a\nhIdRd), with s\u03c4\u2212k\u22121(x) = \u2212\u2207logq\u03c4\u2212k\u22121, the score\nfunction introduced in section 17.2.6, and score-matching methods from that section\ncan be used to estimate it from observations of the forward chain initialized with\ntraining data.\n18.4. REVERSED MARKOV CHAIN MODELS\n449\n18.4.4\nContinuous-time limit\nThe forward schemes described in the previous examples can be interpreted as con-\ntinuous time processes over discrete or continuous variables. In the latter case, the\nexample Xk+1 \u223cN (x+hf (x),\n\u221a\nhIdRd) conditionally to Xk = x is a discretization of the\nstochastic differential equation\ndxt = f (xt)dt + dwt\n(see remark 12.5), where wt is a Brownian motion and the diffusion is initialized\nwith Qtrue. We found that going backward meant (at first order and conditionally to\nXk = x)\nXk\u22121 \u223cN (x \u2212hf (x) \u2212hsk\u22121(x),\n\u221a\nhId)\nthat we can rewrite as\nx\u03c4 \u2212Xk\u22121 \u223cN (x\u03c4 \u2212x + hf (x) + hsk\u22121(x),\n\u221a\nhId).\nFollowing the definition in Anderson [9], this corresponds to a first-order discretiza-\ntion of the reverse diffusion\ndxt = (f (xt) + st(xt))dt + d \u02dcwt, t \u2264\u03c4\nwhere \u02dcwt is also a Brownian motion. This reverse diffusion with X\u03c4 \u223cQ\u221ewill there-\nfore approximately sample from Qtrue. (With this terminology, forward and reverse\ndiffusions have similar differential notation, but mean different things.) Note that,\nin the continuous-time limit, the reverse Markov process follows the distribution of\nthe reversed diffusion exactly.\n18.4.5\nDifferential of neural functions\nAs we have seen in the previous two examples, estimating the reversed Markov chain\nrequires computing the score functions of the forward probabilities. In the case\nof continuous variables, this score function is typically parametrized as a neural\nnetwork, so that the function sk(x) = \u2212\u2207logqk(x) is computed as sk(x) = F(x;Wk),\nwith the usual definition F(x,Wk) = zm+1 with zj+1 = \u03d5j(zj,wjk), z0 = x and Wk =\n(w0k,...,wmk).\nAssume that a training set T is observed. Running the forward Markov chain\ninitialized with elements of T generates a new training step at each time step, that\nwe will denote Tk at step k. We have seen in section 17.2.6 that the score function sk\ncould be estimated by minimizing, with respect to W\nX\nx\u2208Tk\n\u0010\n|F(x,W)|2 \u22122\u2207\u00b7 F(x,W)\n\u0011\n.\n450\nCHAPTER 18. DEEP GENERATIVE METHODS\nThis term involves the differential of F, which is defined recursively by (simply tak-\ning the derivative at each step)\ndF(x,W) = \u03b6m+1, \u03b6j+1 = d\u03d5j(zj,wj)\u03b6j,\nwith \u03b60 = IdRd. From this recursive definition, back-propagation can be applied, in\nprinciple, to compute the derivative of dF(x,W) with respect to W. The feasibility of\nthis computation, however, is limited when d is large (d could be tens of thousands\nif one models images) computing the d \u00d7 d matrix dF(x,W) is intractable.\nWe can note that, for any h \u2208Rd, the vector dF(x,W)h also satisfies the recursion\ndF(x,W)h = \u03b6m+1h, \u03b6j+1h = d\u03d5j(zj,wj)\u03b6jh,\nwith \u03b60h = h and\n\u2207\u00b7 F(x,W) =\nd\nX\ni=1\neT\ni dF(x,W)ei\nwhere e1,...,ed is the canonical basis of Rd. Putting the divergence of F in this\nform does not reduce the computation cost (which is, roughly d2m, assuming that\nall zj\u2019s have the same dimension), but expresses the divergence term in a form that is\namenable to stochastic gradient descent (which is typically already used to approx-\nimate the sum over x). Indeed, if U follows any distribution with zero mean and\ncovariance matrix equal to the identity (such as a standard Gaussian, or the uniform\ndistribution on the unit sphere), then\n\u2207\u00b7 F(x,W) = E(UT dF(x,W)U)\nso that U can be sampled from in minibatches in SGD implementations (see [180],\nwhere this approach is called \u201csliced score matching\u201d).\nChapter 19\nClustering\n19.1\nIntroduction\nWe now describe a collection of methods designed to divide a training set into ho-\nmogeneous subsets, or clusters. This grouping operation is a key problem in many\napplications for which it is important to categorize the data in order to obtain im-\nproved understanding of the sampled phenomenon, and sometimes to be able to\napply a different approach to subsequent processing or analysis adapted to each\ncluster.\nWe will assume that the variables of interest belong a set R = RX where R is\nequipped with a discrepancy function \u03b1 : R \u00d7 R \u2192[0,+\u221e). Often, \u03b1 is derived from\na distance \u03c1 on R, but this is not always the case. We will assume that the data results\nfrom a training set T = (x1,...,xN). However, it may happen that only the discrep-\nancy matrix A = (\u03b1(x,y),x,y \u2208T) is observed, while a coordinate representation of\nthe elements of T is not available.\nLet us consider a few examples.\n(i) The simplest case is when R = Rd with the standard Euclidean metric. Slightly\nmore generally, a metric may be defined by \u03c12(x,y) = \u2225h(x) \u2212h(y)\u22252\nH, where H is an\ninner-product space and the feature function h : R 7\u2192H may be unknown, while its\nassociated \u201ckernel\u201d, K(x,y) = \u27e8h(x) , h(y)\u27e9H is known (this is a metric if h is one-to-\none). In this case\n\u03c12(x,y) = K(x,x) \u22122K(x,y) + K(y,y).\nTypically, one then takes \u03b1 = \u03c1 or \u03b1 = \u03c12.\n(ii) Very often, however, the data is not Euclidean, and the distance does not cor-\nrespond to a feature space representation. This is the case, for example, for data be-\nlonging to \u201ccurved spaces\u201d (manifolds), for which one may use the intrinsic distance\n451\n452\nCHAPTER 19. CLUSTERING\nprovided by the length of shortest paths linking two points (assuming of course that\nthis notion can be given a rigorous meaning). The simplest example is data on the\nunit sphere, where the distance \u03c1(x,y) between two points x and y is the length of\nthe shortest large circle that connects them, satisfying\n|x \u2212y|2 = 2 \u22122cos\u03c1(x,y).\nOnce again, \u03b1 = \u03c1 or \u03c12 is a typical choice.\n(iii) A more complex example is provided by R being the space of symmetric\npositive-definite matrices on Rd, for which one defines the length of a differentiable\ncurve (S(t),t \u2208[a,b]) in this space by\nZ b\na\nq\ntrace((S(t)\u22121\u2202tS)(S(t)\u22121\u2202tS)T )dt\nand for which\n\u03c12(S1,S2) =\nd\nX\ni=1\n(log\u03bbi)2\nwhere \u03bb1,...,\u03bbd are the eigenvalues of S\u22121/2\n1\nS2S\u22121/2\n1\nor, equivalently, solutions of the\ngeneralized eigenvalue problem S2u = \u03bbS1u (see, for example, [72]).\n(iv) Another common assumption is that the elements of R are vertices of a weighted\ngraph of which T is a subgraph; \u03c1 may then be, e.g., the geodesic distance on the\ngraph.\n19.2\nHierarchical clustering and dendograms\n19.2.1\nPartition trees\nThis method builds clusters by organizing them in a binary hierarchy in which the\ndata is divided into subsets, starting with the full training set, and iteratively split-\nting each subset into two parts until reaching singletons. This results in a binary\ntree structure, called a dendogram, or partition tree, which is defined as follows.\nDefinition 19.1 A partition tree of a finite set A is a finite collection of nodes T with the\nfollowing properties.\n(i) Each node has either zero or exactly two children. (We will use the notation v \u2192v\u2032\nto indicate that v\u2032 is a child of v.\n(ii) All nodes but one have exactly one parent. The node without parent is the root of\nthe tree.\n(iii) To each node v \u2208T is associated a subset Av \u2282A.\n19.2. HIERARCHICAL CLUSTERING AND DENDOGRAMS\n453\n1: {a,b,c,d,e,f }\n2: {a,c,f }\n3: {b,d,e}\n4: {a,f }\n5: {c}\n6: {d}\n7: {b,e}\n8: {a}\n9: {f }\n10: {b}\n11: {e}\nFigure 19.1: A partition tree of the set {a,b,c,d,e,f }.\n(iv) If v\u2032 and v\u2032\u2032 are the children of v, then (Av\u2032,Av\u2032\u2032) forms a partition of Av.\nNodes without children are called leaves, or terminal nodes. We will say that the hierarchy\nis complete if Av = A if v is the root, and |Av| = 1 for all terminal nodes.\nAn example of partition tree is provided in fig. 19.1.\nThe construction of the tree can follow two directions, the first one being bottom-\nup, or agglomerative, in which the algorithm starts with the collection of all single-\ntons and merges subsets one pair at a time until everything is merged into the full\ndataset. The second approach is top-down, or divisive, and initializes the algorithm\nwith the full training set which is recursively split until singletons are reached. The\nfirst approach, on which we now focus, is more common, and computationally sim-\npler.\nWe let T denote the training set and assume that a matrix of dissimilarities\n(\u03b1(x,y), x,y \u2208T)\nis given. We will make the abuse of notation of considering that T is a set even\nthough some of its elements may be repeated. This is no loss of generality, since\nT = (x1,...,xN) can always be replaced by the subset {(k,xk),k = 1,...,N} of N \u00d7 R.\n19.2.2\nBottom-up construction\nWe will extend \u03b1 to a dissimilarity measure between subsets A,A\u2032 \u2282T that we will\ndenote (A,A\u2032) 7\u2192\u03d5(A,A\u2032). Once \u03d5 is defined, agglomeration works along the follow-\ning algorithm.\nAlgorithm 19.1\n1. Start with the collection T1,...,TN of all single-node trees associated to each\nelement of T . Let n = 0 and m = N.\n454\nCHAPTER 19. CLUSTERING\n2. Assume that, at step n of the algorithm, one has a collection of partition trees\nT1,...,Tm with root nodes r1,...,rm associated with subsets Ar1,...,Arm of T. Let the\ntotal collection of nodes be indexed as Vn = {v1,...,vN+n}, so that {r1,...,rm} \u2282Vn.\n3. If m = 1, stop the algorithm.\n4. Select indices i,j \u2208{1,...,m} such that \u03d5(Ari,Arj) is minimal, and merge the\ncorresponding trees by creating a new node vn+1+N with the root nodes of Ti and Tj\nas children (so that vn+1+N is associated with Ari \u222aArj). Add vn+1+N to the collection\nof root nodes, and remove ri and rj.\n5. Set n \u2192n + 1 and m \u2192m \u22121 and return to step 2.\nClearly, the specification of the extended dissimilarity measure (\u03d5) is a key ele-\nment of the method. Some of most commonly used extensions are:\n\u2022 Minimum gap: \u03d5min(A,A\u2032) = min(\u03b1(x,x\u2032) : x \u2208A,x\u2032 \u2208A\u2032).\n\u2022 Maximum dissimilarity: \u03d5max(A,A\u2032) = max(\u03b1(x,x\u2032) : x \u2208A,x\u2032 \u2208A\u2032).\n\u2022 Sum of dissimilarities:\n\u03d5sum(A,A\u2032) =\nX\nx\u2208A\nX\nx\u2032\u2208A\u2032\n\u03b1(x,x\u2032)\n\u2022 Average dissimilarity:\n\u03d5avg(A,A\u2032) =\n1\n|A||A\u2032|\nX\nx\u2208A\nX\nx\u2032\u2208A\u2032\n\u03b1(x,x\u2032).\nAs shown in the next two propositions, the maximum distance favors clusters\nwith small diameters, while using minimum gaps tends to favor connected clusters.\nProposition 19.2 Let diam(A) = max(\u03b1(x,y),x,y \u2208A). The agglomerative algorithm\nusing \u03d5max is identical to that using \u03d5(A,A\u2032) = diam(A \u222aA\u2032).\nProof Call Algorithm 1 the agglomerative algorithm using \u03d5max, and Algorithm 2\nthe one using \u03d5. At initialization, we have (because all sets are singletons),\n\u03d5max(Ak,Al) = diam(Ak \u222aAl) for all 1 \u2264k , l \u2264m.\n(19.1)\nWe show that this property remains true at all steps of the algorithms. Pro-\nceeding by induction, assume that, up to the step n, Algorithms 1 and 2 have been\nidentical and result in sets (A1,...,Am) satisfy (19.1). Then the next steps of the\ntwo algorithms coincide and assume, without loss of generality, that this next step\n19.2. HIERARCHICAL CLUSTERING AND DENDOGRAMS\n455\nmerges Am\u22121 with Am. Let A\u2032\nm\u22121 = Am\u22121 \u222aAm so that diam(A\u2032\nm\u22121) \u2264diam(Ai \u222aAj) for\nall 1 \u2264i , j \u2264m.\nWe need to show that the new partition satisfies (19.1), which requires that\n\u03d5max(A\u2032\nm\u22121,Ak) = diam(A\u2032\nm\u22121 \u222aAk)\nfor k = 1,...,m \u22122.\nWe have\ndiam(A\u2032\nm\u22121 \u222aAk) = max(diam(A\u2032\nm\u22121),diam(Ak),\u03d5max(A\u2032\nm\u22121,Ak)),\nso that we must show that\nmax(diam(A\u2032\nm\u22121),diam(Ak)) \u2264\u03d5max(A\u2032\nm\u22121,Ak).\nWrite\n\u03d5max(A\u2032\nm\u22121,Ak) = max(\u03d5max(Am,Ak),\u03d5max(Am\u22121,Ak))\n= max(diam(Am \u222aAk),diam(Am\u22121 \u222aAk))\nwhere the last identity results from the induction hypothesis.\nThe fact that\ndiam(Ak) \u2264max(diam(Am \u222aAk),diam(Am\u22121 \u222aAk))\nis obvious, and the inequality\ndiam(A\u2032\nm\u22121) \u2264max(diam(Am \u222aAk),diam(Am\u22121 \u222aAk))\nresults from the fact that Am and Am\u22121 was an optimal pair. This shows that the\ninduction hypothesis remains true at the next step and concludes the proof of the\nproposition.\n\u25a0\nWe now analyze \u03d5min and, more specifically, the equivalence between the result-\ning algorithm and the one using the following measure of connectedness. For a given\nset A and x,y \u2208A, let\n\u02dc\u03b1A(x,y) = inf\nn\n\u03f5 : \u2203n > 0,\u2203(x = x0,x1,...,xn\u22121,xn = y) \u2208An+1 :\n\u03b1(xi,xi\u22121) \u2264\u03f5 for 1 \u2264i \u2264n\no\n.\nSo \u02dc\u03b1A is the smallest \u03f5 such that there exists a sequence of steps of size less than \u03f5 in\nA going from x to y. The function\nconn(A) = max{ \u02dc\u03b1A(x,y) : x,y \u2208A}\nmeasures how well the set A is connected relative to the dissimilarity measure \u03b1.\nand we have:\n456\nCHAPTER 19. CLUSTERING\nProposition 19.3 The agglomerative algorithm using \u03d5min is identical to that using\n\u03d5(A,A\u2032) = conn(A \u222aA\u2032).\nProof The proof is similar to that of proposition 19.2. Indeed one can note that\nconn(A \u222aA\u2032) = max(conn(A),conn(A\u2032),\u03d5min(A,A\u2032)).\nGiven this we can proceed by induction and prove that, if the current decomposi-\ntion is A1,...,Am such that \u03c8(Ak \u222aAl) = \u03d5min(Ak,Al) for all 1 \u2264k , l \u2264m, then this\nproperty is still true after merging using \u03d5min and \u03d5.\nAssuming again that Am\u22121 and Am are merged, and letting A\u2032\nm\u22121 = Am \u222aAm\u22121, we\nneed to show that conn(Ak \u222aA\u2032\nm\u22121) = \u03d5min(Ak,A\u2032\nm\u22121) for all k = 1,...,m \u22122, which is\nthe same as showing that:\nmax(conn(Ak),conn(A\u2032\nm\u22121)) \u2264\u03d5min(Ak,A\u2032\nm\u22121) = min(\u03d5min(Ak,Am\u22121),\u03d5min(Ak,Am)).\nFrom the induction hypothesis, we have\nmin(\u03d5min(Ak,Am\u22121),\u03d5min(Ak,Am)) = min(conn(Ak \u222aAm\u22121),conn(Ak \u222aAm))\nand both terms in the right-hand side are larger than conn(Ak) and also larger than\nconn(A\u2032\nm\u22121) which was a minimizer.\n\u25a0\n19.2.3\nTop-down construction\nThe agglomerative method is the most common way to build dendograms, mostly\nbecause of the simplicity of the construction algorithm. The divisive approach is\nmore complex, because the division step, which requires, given a set A, to optimize\na splitting criterion over all two-partitions of A, may be significantly more expensive\nthan the merging steps in the agglomerative algorithm. The top-down construction\ntherefore requires the specification of a \u201csplitting algorithm\u201d \u03c3 : A 7\u2192(A\u2032,A\u2032\u2032) such\nthat (A\u2032,A\u2032\u2032) is a partition of A. We assume that, if |A| > 1, then the partition A,A\u2032\u2032 is\nnot trivial, i.e., neither set is empty.\nGiven \u03c3, the top-down construction is as follows.\nAlgorithm 19.2\n1. Start with the one-node partition tree T0 = (T ).\n2. Assume that at a given step of the algorithm, the current partition is T .\n3. If T is complete, stop the algorithm.\n4. For each terminal node v in T such that |Av| > 1, compute (A\u2032\nv,A\u2032\u2032\nv ) = \u03c3(Av) and\nadd two children v\u2032 and v\u2032\u2032 to v with Av\u2032 = A\u2032\nv and Av\u2032\u2032 = A\u2032\u2032\nv .\n5. Return to step 2.\nThe division of a set into two parts is itself a clustering algorithm, and one may apply\nany of those described in the rest of this chapter.\n19.3. K-MEDOIDS AND K-MEAN\n457\n19.2.4\nThresholding\nOnce a complete hierarchy is built, it provides a complete binary partition tree T .\nThis tree provides in turn a collection of partitions of V, each of them obtained\nthrough pruning. We now formalize this operation.\nLet VT denote the set of terminal nodes in T and V0 = V \\ VT contain the interior\nnodes. Define a pruning set to be a subset D \u2282V0 that contains no pair of nodes v,v\u2032\nsuch that v\u2032 is a descendant of v. To any pruning set D, one can associate the pruned\nsubtree T (D) of T consisting of T from which all the vertices that are descendants\nof elements of D are removed. From any such pruned subtree, one obtain a partition\nS(D) of T formed by the collection of sets Av for v in the terminal nodes of T (D).\nBetween the extreme case S(v0) = {V} (where v0 is the root of T ) and S(\u2205) = ({x},x \u2208\nVT ), there exists a huge number of possible partitions obtained in this way.\nIt is often convenient to organize these partitions according to the level sets of\na well-chosen score function v 7\u2192h(v) defined over V0. For D \u2282V, we denote by\nmax(D) the set of its deepest elements, i.e., the set formed by those v \u2208D that have\nno descendant in D. Then, for any \u03bb \u2208R, one can define D+\n\u03bb = max{v : h(v) \u2265\u03bb} (resp.\nD\u2212\n\u03bb = max{v : h(v) \u2264\u03bb}) and the associated partition S(D+\n\u03bb) (resp. S(D\u2212\n\u03bb)). The score\nfunction h can be linked to the construction algorithm. For example, if one uses a\nbottom-up construction using an extended dissimilarity \u03d5, one can associate to each\nnode v with v \u2208V0 the value of \u03d5(Av\u2032,Av\u2032\u2032) where v\u2032 and v\u2032\u2032 are the children of v.\nAnother way to define such scores functions is by assigning weights to edges in\nT . Indeed, given a collection w of positive numbers w(v,v\u2032) for v \u2192v\u2032 in T , one can\ndefine a score hw recursively by letting hw(v0) = 0 and hw(v\u2032) = hw(v)+w(v,v\u2032) if v\u2032 is\na child of v. The choice w(v,v\u2032) = 1 for all v,v\u2032 provide the usual notion of depth in\nthe tree.\nScores can also be built bottom-up, letting h(v) = 0 for terminal nodes and, for\nv \u2208V0,\nhw(v) = max(hw(v\u2032) + w(v,v\u2032),hw(v\u2032\u2032) + w(v,v\u2032\u2032))\nwhere v\u2032,v\u2032\u2032 are the children of v Here, taking w = 1 provides the height of each\nnode.\n19.3\nK-medoids and K-mean\n19.3.1\nK-medoids\nOne of the limitation of hierarchical clustering is that it is a greedy approach that\ndoes not optimize a global quality measure associated to the partition. Such qual-\n458\nCHAPTER 19. CLUSTERING\nity measures can indeed be defined based on the heuristic that clusters should be\nhomogeneous (for some criterion) and far apart from each other.\nIn centroid-based methods, the homogeneity criterion is the minimum, over all\npossible points in R, of the sum of dissimilarities between elements of the cluster\nand that point. More precisely, for any A \u2282R, and any dissimilarity measure \u03b1,\ndefine the central dispersion index\nV\u03b1(A) = inf\n\uf8f1\uf8f4\uf8f4\uf8f2\uf8f4\uf8f4\uf8f3\nX\nx\u2208A\n\u03b1(x,c) : c \u2208R\n\uf8fc\uf8f4\uf8f4\uf8fd\uf8f4\uf8f4\uf8fe.\n(19.2)\nIf c achieves the minimum in the definition of V\u03b1, it is called a centroid of A for the\ndissimilarity \u03b1.\nThe most common choice is \u03b1 = \u03c12, where \u03c1 is a metric on R, and in this case,\nwe will just use V in place of V\u03c12. Note also that it is always possible to limit R to\nthe training set T, in which case the optimization in (19.2) is over a finite number of\ncenters. This makes centroid-based methods also applicable to the situation when\nthe matrix of dissimilarities is the only input provided to the algorithm, or when the\nset R and the function \u03b1 are too complex for the optimization in (19.2) to be feasible.\nA centroid, c, in (19.2) may not always exists, and when it exists it may not always\nbe unique. For \u03b1 = \u03c12, a point c such that\nV (A) =\nX\nx\u2208A\n\u03c12(x,c)\nis called a Fr\u00b4echet mean of the set A. Returning to the examples provided in the\nbeginning of this chapter, two antipodal points on the sphere (whose distance is \u03c0)\nhave an infinity of Fr\u00b4echet means (or midpoints in this case) provided by every point\nin the equator between them. In contrast, the example provided with symmetric\nmatrices provides a so-called Hadamard space [44] and the Fr\u00b4echet mean in that\ncase is unique. Of course, for Euclidean metrics, the Fr\u00b4echet mean is just the usual\none.\nReturning to our general discussion, the K-medoids method optimizes the sum\nof central dispersions with a fixed number of clusters. Note that the letter K in K-\nmedoids originally refers to this number of clusters, but this notation conflicts with\nother notation in this book (e.g., reproducing kernels) and we shall denote by p this\n19.3. K-MEDOIDS AND K-MEAN\n459\ntarget number1. So the K-medoids method minimizes\nW\u03b1(A1,...,Ap) =\np\nX\ni=1\nV\u03b1(Ai)\nover all partitions A1,...,Ap of the training set T. Equivalently, it minimizes\nW\u03b1(A1,...,Ap,c1,...,cp) =\np\nX\ni=1\nX\nx\u2208Ai\n\u03b1(x,ci)\n(19.3)\nover all partitions of T and c1,...,cp \u2208R. Finally, taking first the minimum with\nrespect to Ai, which corresponds to associating each x to the subset with closest\ncenter, K-medoids, an equivalent formulation minimizes\n\u02dcW\u03b1(c1,...,cp) =\nX\nx\u2208T\nmin\nn\n\u03b1(x,ci),i = 1,...,p\no\n.\nThe standard implementation of K-medoids solves this problem using an alter-\nnate minimization, as defined in the following algorithm.\nAlgorithm 19.3 (K-medoids)\nLet T \u2282R be the training set. Start with an initial choice of c1,...,cp \u2208R and iterate\nover the following two steps until stabilization:\n(1) For i = 1,...,p, let Ai contain points x \u2208T such that \u03b1(x,ci) = min{\u03b1(x,cj),j =\n1,...,p}. In case of a tie in this minimum, assign x to only one of the tied sets\n(e.g., at random) to ensure that A1,...,Ap is a partition.\n(2) For i = 1,...,p, let ci be a minimizer of P\nx\u2208Ai \u03b1(x,ci) if Ai is not empty, or ci be a\nrandom point in T otherwise.\nIt should be clear that each step reduces the total cost W\u03b1 and that this cost\nshould stabilize at some point (which provides the stopping criterion) because there\nis only a finite number of possible partitions of T. However, there can be many\npossible limit points that are stable under the previous iterations, and some may\ncorrespond to poor \u201clocal minima\u201d of the objective function. Since the end-point of\nthe algorithm depends on the initialization, this step requires extra care. One may\ndesign ad-hoc heuristics in order to start the algorithm with a good initial point that\nis likely to provide a good solution at the end. These heuristics may depend on the\n1We still call the method K-medoids rather than p-medoids, to keep the name universally used in\nthe literature.\n460\nCHAPTER 19. CLUSTERING\nproblem at hand, or use a generic strategy. As a common example of the latter, one\nmay ensure that the initial centers are sufficiently far apart by picking c1 at random,\nc2 as far as possible from c1, c3 maximizing the sum of distances to c1 and c2 etc.\nOne also typically runs the algorithm several times with random initial conditions\nand select the best solution over these multiple runs.\nThe second step of Algorithm 19.3 can be computationally challenging depend-\ning on the set R and the dissimilarity measure \u03b1. When R = Rd and \u03b1 = \u03c12 is the\nsquare Euclidean distance, the solution is explicit and ci is simply the average of all\npoints in Ai. The resulting algorithm is the original incarnation of K-medoids, and\ncalled K-means [182, 121, 124]. K-means is probably the most popular clustering\nmethod and is often a step in more advanced approaches, as we will discuss later.\nThe two steps of Algorithm 19.3 are then simplified as follows.\nAlgorithm 19.4 (K-means)\nLet T \u2282Rd be the training set. Start with an initial choice of c1,...,cp \u2208Rd and iterate\nover the following two steps until stabilization:\n(1) For i = 1,...,p, let Ai contain points x \u2208T such that |x \u2212ci|2 = min{|x \u2212cj|2,j =\n1,...,p}. In case of tie in this minimum, assign x to only one of the tied sets\n(e.g., at random) to ensure that A1,...,Ap is a partition.\n(2) For i = 1,...,p, let\nci = 1\n|Ai|\nX\nx\u2208Ai\nx\nif Ai is not empty, or ci be a random point in T otherwise.\n19.3.2\nMixtures of Gaussian and deterministic annealing\nMixtures of Gaussian (MoG) were discusssed in chapter 16 and in Algorithm 16.2.\nRecall that they model the observed data X together with a latent class variable\nZ \u2208{1,...,p} with joint distribution\nf (x,z;\u03b8) = (2\u03c0)\u2212d\n2 (det\u03a3z)\u22121\n2\u03b1ze\u22121\n2(x\u2212cz)T \u03a3\u22121\nz (x\u2212cz)\nwhere \u03b8 contains the weights, \u03b11,...,\u03b1p, the means, c1,...,cp and the covariance ma-\ntrices \u03a31,...,\u03a3p (we create, hopefully without risk of confusion, a short-lived conflict\nof notation between the weights and the dissimilarity function). The posterior class\nprobabilities\nfZ(i|x; \u03b8) =\n(det\u03a3i)\u22121\n2\u03b1ie\u22121\n2(x\u2212ci)T \u03a3\u22121\ni (x\u2212ci)\nPp\nj=1(det\u03a3j)\u22121\n2\u03b1je\u22121\n2(x\u2212cj)T \u03a3\u22121\nj (x\u2212cj),\ni = 1,...,p,\n19.3. K-MEDOIDS AND K-MEAN\n461\nwhich are computed in step 3 of Algorithm 16.2 can be interpreted as a likelihood\nthat observation x belongs to group i. As a consequence, the mixture of Gaussian\nalgorithm can also be seen as a clustering method, in which one assigns each x \u2208T\nto cluster i when i = argmax{fZ(j|x,\u03b8) : j = 1,...,p}, making an arbitrary decision in\ncase of a tie.\nIn the special case in which all variances are fixed and equal to \u03c32IdRd, and all\nprior class probabilities are equal to 1/p (see remark 16.3), the EM algorithm for mix-\ntures of Gaussian is also called \u201csoft K-means\u201d, because it replaces the \u201chard\u201d cluster\nassignments in K-means by \u201csoft\u201d ones represented by the update of the posterior\ndistribution. We repeat its definition here for completeness (where \u03b8 = (c1,...,cp)).\nAlgorithm 19.5 (Soft K-means)\n1. Choose a number \u03c32 > 0, a small constant \u03f5 and a maximal number of itera-\ntions M. Initialize the centers c = (c1,...,cp).\n2. At step n of the algorithm, let c be the current centers.\n3. Compute, for x \u2208T and i = 1,...,p\nfZ(i|x,\u03b8) =\ne\u2212\n1\n2\u03c32 |x\u2212ci|2\nPp\nj=1 e\u2212\n1\n2\u03c32 |x\u2212cj|2\nand let \u03b6i = PN\nk=1 fZ(i|x,\u03b8), i = 1,...,p.\n4. For i = 1,...,p, let\nc\u2032\ni = 1\n\u03b6i\nX\nx\u2208T\nxfZ(i|x,\u03b8).\n5. If |c\u2032 \u2212c| < \u03f5 or n = M: stop the algorithm.\n6. Replace c by c\u2032 and n by n + 1 and return to step 2.\nWhen \u03c32 \u21920, fZ(\u00b7|xk,\u03b8) converges to the uniform probability on indexes j such\nthat cj is closest to xk, which is a Dirac measure unless there are ties. Class allo-\ncation and center updating become then asymptotically identical to the K-means\nalgorithm. A variant of soft K-means, called deterministic annealing [169], applies\nAlgorithm 19.5 while letting \u03c3 slowly tend to 0. This new algorithm is experimen-\ntally more robust than K-means, in that it is less likely to be trapped in bad local\nminimums.\nRemark 19.4 The soft K-means algorithm can also be defined directly as an alternate\nminimization method for the objective function\nF(c,fZ) = 1\n2\nX\nx\u2208T\np\nX\nj=1\nfZ(j|x)|x \u2212cj|2 + \u03c32 X\nx\u2208T\np\nX\nj=1\nfZ(j|x)logfZ(j|x),\n462\nCHAPTER 19. CLUSTERING\nwith the constraints fZ(j|x) \u22650 for all j and x and Pp\nj=1 fZ(j|x) = 1. One can check\n(we leave this as an exercise) that Step 3 in Algorithm 19.5 provides the optimal fZ\nfor F when c is fixed, and that Step 4 gives the optimal c when fZ is fixed (see ??). \u2666\nRemark 19.5 We note that, if a K-means, soft K-means or MoG algorithm has been\ntrained on a training set T, it is then easy to assign a new sample \u02dcx to one of the\nclusters. Indeed, for K-means, it suffices to determine the center closest to \u02dcx, and\nfor the other methods to maximize fZ(j| \u02dcx,\u03b8), which is computable given the model\nparameters. In contrast, there was no direct way to do so using hierarchical cluster-\ning.\n\u2666\n19.3.3\nKernel (soft) K-means\nWe now consider the soft K-means algorithm in feature space, and introduce fea-\ntures hk = h(xk) in an inner product space H such that \u27e8hk , hl\u27e9H = K(xk,xl) for some\npositive definite kernel. As usual, the underlying assumption is that the computa-\ntion of h(x) does not need to be feasible, while evaluations of K(x,y) are easy. Let us\nconsider the minimization of\n1\n2\nX\nx\u2208T\np\nX\nj=1\nfZ(j|x)\u2225h(x) \u2212cj\u22252\nH + \u03c32 X\nx\u2208T\np\nX\nj=1\nfZ(j|x)logfZ(j|x)\nfor some \u03c32 > 0 (kernel K-means corresponds to taking the limit \u03c32 \u21920). Given fZ,\nthe optimal centers are\ncj = 1\n\u03b6 j\nX\nx\u2208T\nfZ(j|x)h(x)\nwith \u03b6 = P\nx\u2208T fZ(j|x). They belong to the feature space, H, and are therefore not\ncomputable in general. However, the distance between them and a point h(y) \u2208H is\nexplicit and given by\n\u2225h(y) \u2212cj\u22252\nH = K(y,y) \u22122\n\u03b6j\nX\nx\u2208T\nfZ(j|x)K(y,x) + 1\n\u03b62\nj\nX\nx,x\u2032\u2208T\nfZ(j|x)fZ(j|x\u2032)K(x,x\u2032).\nThe class probabilities at each iteration can therefore be updated using\nfZ(j|x) =\ne\u2212\u2225h(x)\u2212cj\u22252\nH / 2\u03c32\nPp\nj\u2032=1 e\u2212\u2225h(y)\u2212cj\u2032\u22252\nH / 2\u03c32 .\nThis yields the soft kernel K-means algorithm, that we repeat below.\n19.3. K-MEDOIDS AND K-MEAN\n463\nAlgorithm 19.6 (Kernel soft K-means)\nLet T \u2282Rd be the training set. Initialize the algorithm with some choice for fZ(j|x),\nj = 1,...,p, x \u2208T (for example: fZ(j|x) = 1/p for all j and x).\n(1) For j = 1,...,p and x \u2208T compute\n\u2225h(x) \u2212cj\u22252\nH = K(x,x) \u22122\n\u03b6j\nX\nx\u2032\u2208T\nfZ(j|x\u2032)K(x,x\u2032) + 1\n\u03b62\nj\nX\nx\u2032,x\u2032\u2032\u2208T\nfZ(j|x\u2032)fZ(j|x\u2032\u2032)K(x\u2032,x\u2032\u2032)\nwith \u03b6j = P\nx\u2032\u2208T fZ(j|x\u2032).\n(2) Compute, for x \u2208T and j = 1,...,p,\nfZ(j|x) =\ne\u2212\u2225h(x)\u2212cj\u22252\nH/2\u03c32\nPp\nj\u2032=1 e\u2212\u2225h(y)\u2212cj\u2032\u22252\nH/2\u03c32 .\n(3) If the variation of fZ compared to the previous iteration is small, or if a maximum\nnumber of iterations has been reached, exit the algorithm.\n(4) Return to step 1.\nAfter convergence, the clusters are computed by assigning x to Ai when i = argmax{fZ(j|x) :\nj = 1,...,p}, making an arbitrary decision in case of a tie.\nFor \u201chard\u201d K-means (with \u03c32 \u21920), step 2 simply updates fZ(j|x) as the uniform\nprobability on the set of indexes j at which \u2225h(x) \u2212cj\u22252\nH is minimal.\n19.3.4\nConvex relaxation\nWe return to the initial formulation of K-means for Euclidean data, as a minimiza-\ntion, over all partitions A = {A1,...,AK} of {1,...,N} of\nW(A) =\nK\nX\nj=1\nX\nk\u2208Aj\n|xk \u2212cj|2\nwhere cj is the average of the points xj such that j \u2208Aj. We start with a simple\ntransformation expressing this function in terms of the matrix S\u03b1 of square distances\n464\nCHAPTER 19. CLUSTERING\n\u03b1(xk,xl) = |xk \u2212xl|2. Indeed, we have\nX\nk\u2208Aj\n|xk \u2212cj|2 =\nX\nk\u2208Aj\n|xk|2 \u22121\n|A|\n\f\f\f\f\f\f\f\f\nX\nk\u2208Aj\nxk\n\f\f\f\f\f\f\f\f\n=\nX\nk\u2208Aj\n|xk|2 \u22121\n|A|\nX\nk,l\u2208Aj\nxT\nk xl\n=\n1\n2|Aj|\nX\nk,l\u2208Aj\n(|xk|2 + |xl|2 \u22122xT\nk xl)\n=\n1\n2|Aj|\nX\nk,l\u2208Aj\n|xk \u2212xl|2\nIntroduce the vector uj \u2208RN with coordinates u(k)\nj\n= 1/\nq\n|Aj| for k \u2208Aj and 0 other-\nwise. Then\n1\n2|Aj|\nX\nk,l\u2208Aj\n|xk \u2212xl|2 = 1\n2uT\nj S\u03b1uJ = 1\n2trace(S\u03b1ujuT\nj ).\n(19.4)\nLet\nZ(A) =\np\nX\nj=1\nujuT\nj ,\nso that Z(A) has entries Z(k,l)(A) = 1/|Aj| for k,l \u2208Aj, j = 1,...p and 0 for all other\nk,l. Summing (19.4) over j, we get\nW(A) = 1\n2trace(S\u03b1Z(A)).\nThe matrix Z(A) is symmetric, has non-negative entries. It moreover satisfies\nZ(A)1N = 1N and Z(A)2 = Z(A). Interestingly, these properties characterize matri-\nces Z associated with partitions, as stated in the next proposition [153, 152].\nProposition 19.6 Let Z \u2208MN(R) be a symmetric matrix with non-negative entries sat-\nisfying Z1N = 1N and Z2 = Z. The there exists a partition A of {1,...,N} such that\nZ = Z(A).\nProof Note that Z being symmetric and satisfying Z2 = Z imply that it is an orthog-\nonal projection with eigenvalues 0 and 1. In particular Z is positive semidefinite.\nThis implies that, for all i,j \u2208{1,...,N}, one has\nZ(i,j)2 \u2264Z(i,i),Z(j,j).\nThis inequality combined with PN\nj=1 Z(k,j) = 1 (expressing Z1N = 1N) shows that all\ndiagonal entries of Z are positive.\n19.3. K-MEDOIDS AND K-MEAN\n465\nDefine on {1,...,N} the relation k \u223cj if and only if Z(j,k) > 0. The relation is\nsymmetric and we just checked that k \u223ck for all k. It is also transitive, from the\nrelation (deriving from Z2 = Z)\nZ(k,j) =\nN\nX\ni=1\nZ(k,i)Z(i,j)\nwhich shows (since all terms in the sum are non-negative) that k \u223ci and j \u223ci imply\nk \u223cj.\nLet A = {A1,...,Aq} be the partition of {1,...,N} formed by the equivalence classes\nfor this relation. We now show that Z = Z(A).\nWe have, for all k,j \u2208{1,...,N}\nN\nX\ni=1\nZ(k,i)(Z(k,j) \u2212Z(i,j)) = Z(k,j)\nN\nX\ni=1\nZ(k,i) \u2212\nN\nX\ni=1\nZ(k,i)Z(i,j)\n= Z(k,j) \u2212\nN\nX\ni=1\nZ(k,i)Z(i,j) = 0\nNow, if k,j \u2208As for some s, the identity reduces to\nX\ni\u2208As\nZ(k,i)(Z(k,j) \u2212Z(i,j)) = 0.\n(19.5)\nChoose k such that Z(k,k) = max{Z(i,i) : i \u2208As}. Then, for all i,j \u2208As, Z(i,j) \u2264\np\nZ(i,i)Z(j,j) \u2264Z(k,k) and (19.5) for j = k yields\nX\ni\u2208As\nZ(k,i)(Z(k,k) \u2212Z(k,i)) = 0,\nwhich is only possible (since all Z(k,i) are positive) if Z(k,i) = Z(k,k) for all i \u2208As.\nFrom Z(k,i) \u2264\np\nZ(i,i)Z(k,k), we get Z(i,i) = Z(k,k) for all i, and therefore (reapply-\ning what we just found to i insteand of k) Z(i,j) = Z(i,i) = Z(k,k) for all i,j \u2208As.\nFinally, we have\n1 =\nX\ni\u2208As\nZ(k,i) = |As|Z(k,k)\nshowing that Z(k,k) = 1/|As| and completing the proof that Z = Z(A).\n\u25a0\nNote that the number of clusters, |A| is equal to the trace of Z(A). This shows that\nminimizing W(A) over partitions with p clusters is equivalent to the constrained\noptimization problem minimizing\nG(Z) = trace(S\u03b1Z)\n(19.6)\n466\nCHAPTER 19. CLUSTERING\nover all matrices Z such that Z \u22650, ZT = Z, Z1N = 1N, trace(Z) = p and Z2 = Z.\nThis is still a difficult problem, since it is equivalent to K-means, which is NP hard.\nSeeing the problem in this form, however, is more amenable to approximations and,\nin particular, convex relaxations.\nIn [152], it is proposed to use a semidefinite program (SDP) as a relaxation. The\nconditions Z = ZT and Z2 = Z require that all eigenvalues of Z are either 0 or 1, and a\ndirect relaxation is to replace these constraints by ZT = Z and 0 \u2aafZ \u2aafIdRN. The last\ninequality is however redundant if we add the conditions 2 Z \u22650 and Z1 = 1. This\nis a consequence of the Perron-Frobenius theorem which states that a matrix \u02dcZ with\npositive entries has a largest (in modulus) real eigenvalue, which has multiplicity\none and is associated with an eigenvector with positive coordinates, the latter eigen-\nvector being (up to multiplication by a constant) the unique eigenvector of \u02dcZ with\npositive coordinates. So, if a matrix \u02dcZ is symmetric, satisfies \u02dcZ > 0 and \u02dcZ1N = 1N,\nthen \u02dcZ \u2aafIdRN. Applying this result to \u02dcZ = (1 \u2212\u03f5)Z + (\u03f5/N)1N1T\nN and letting \u03f5 tend\nto 0 shows that any matrix Z with non-negative entries satisfying Z1N = 1N also\nsatisfies Z \u2aafIdRN.\nThis provides the following SDP relaxation of K-means [152]: minimize\nG(Z) = trace(S\u03b1Z)\n(19.7)\nsubject to ZT = Z, Z1N = 1N, trace(Z) = p, Z \u22650, Z \u2ab00.\nClusters can be immediately inferred from the columns of the matrix Z(A), since\nthey are identical for two indices in the same cluster, and orthogonal to each other\nfor two indices in different clusters. Let z1(A),...,zN(A) denote the columns of Z(A)\nand \u00afzk(A) = zk(A)/|zk(A)|. One has |\u00afzk(A) \u2212\u00afzl(A)| = 0 if k and l belong to the same\ncluster and\n\u221a\n2 otherwise.\nThese properties will not necessarily be satisfied by a solution, say, Z\u2217, of the\nSDP relaxation, but, assuming that the approximation is good enough, one may still\nconsider the normalized columns of Z\u2217and expect them to be similar for indices in\nthe same cluster, and away from each other otherwise. Denoting by \u00afz\u2217\n1,..., \u00afz\u2217\nN these\nnormalized columns, one can then run on them the standard K-means algorithm, or\na spectral clustering method such as those described in the next sections, to infer\nclusters.\nRemark 19.7 Clearly, one can use any symmetric matrix S in the definition of G in\n(19.6) and (19.7). The method is equivalent to, or to a relaxation of, K-means only\nwhen S is formed with squared norms in inner-product spaces, which does include\nkernel K-means, for which\n\u03b1(xk,xl) = K(xk,xk) \u22122K(xk,xl) + K(xl,xl).\n2Recall that Z \u2ab00 means that Z is positive definite, while Z \u22650 indicates that all its entries are\nnon-negative.\n19.4. SPECTRAL CLUSTERING\n467\nIf \u03b1 is an arbitrary discrepancy measure, the minimization of G(Z) still makes sense,\nsince it is equivalent to minimizing\nG(Z(A)) =\np\nX\nj=1\nD\u03b1(Aj).\nwhere\nD\u03b1(A) = 1\n|A|\nX\nx,y\u2208A\n\u03b1(x,y).\n(19.8)\nis a (normalized) measure of size, that we will call the \u03b1-dispersion of a finite set A.\u2666\nRemark 19.8 Instead of using dissimilarities, some algorithms are more naturally\ndefined in terms of similarities. Given such a similarity measure, say, \u03b2, one must\nmaximize rather than minimize the index \u2206\u03b2 (which becomes, rather than a measure\nof dispersion, a measure of concentration).\nOne passes from a dissimilarity \u03b1 to a similarity \u03b2 by applying a decreasing func-\ntion to the former, a common choice being\n\u03b2(x,x\u2032) = exp(\u2212\u03b1(x,x\u2032)/\u03c4)\nfor some \u03c4 > 0.\nAlternatively, one can fix an element x0 \u2208R and let\n\u03b2(x,y) = \u03b1(x,x0) + \u03b1(y,x0) \u2212\u03b1(x,y) \u2212\u03b1(x0,x0),\n(note that the last term, \u03b1(x0,x0) is generally equal to 0). For example, if \u03b1(x,y) =\n|x \u2212y|2, then \u03b2(x,y) = 2(x \u2212x0)T (y \u2212x0) (for which it is natural to take x0 = 0). If \u03b1 is a\ndistance (not squared!), then \u03b2 \u22650 by the triangular inequality. In this case, we have\n\u2206\u03b2(A1,...,Ap) =\nn\nX\nk=1\nD\u03b2(Ak)\n=\np\nX\nk=1\n1\n|Ap|\nX\nx,y\u2208Ak\n\u03b1(x,x0) +\np\nX\nk=1\n1\n|Ap|\nX\nx,y\u2208Ak\n\u03b1(y,x0)\n\u2212\np\nX\nk=1\n1\n|Ap|\nX\nx,y\u2208Ak\n\u03b1(x0,x0) \u2212\np\nX\nk=1\n1\n|Ap|\nX\nx,y\u2208Ak\n\u03b1(x,x0)\n= 2\np\nX\nk=1\nX\nx\u2208Ak\n\u03b1(x,x0) \u2212\np\nX\nk=1\n|Ak|\u03b1(x0,x0) \u2212\u2206\u03b1(A1,...,Ap)\n= 2\nX\nx\u2208T\n\u03b1(x,x0) \u2212|T |\u03b1(x0,x0) \u2212\u2206\u03b1(A1,...,Ap)\n\u2666\nso that minimizing \u2206\u03b1 is equivalent to maximizing \u2206\u03b2.\n468\nCHAPTER 19. CLUSTERING\n19.4\nSpectral clustering\n19.4.1\nSpectral approximation of minimum discrepancy\nOne refers to spectral methods algorithms that rely on computing eigenvectors and\neigenvalues (the spectrum) of data-dependent matrices. In the case of minimizing\ndiscrepancies, they can be obtained by further simplifying (19.7), essentially by re-\nmoving constraints.\nOne indeed gets a simpler problem if the non-negativity constraint, Z \u22650, is\nremoved. Doing so, one cannot guarantee anymore that Z \u2aafIdRN, so we need to\nreinstate this constraint. We will first make the further simplification to remove\nthe constraint Z1N = 1N, the problem becoming minimizing trace(S\u03b1Z) over all Z \u2208\nS+\nN(R) such that 0 \u2aafZ \u2aafIdRN and trace(Z) = p. Decomposing Z in an eigenbasis,\ni.e., looking for it in the form\nZ =\nN\nX\nj=1\n\u03bejejeT\nj ,\nthis is equivalent to minimizing\nN\nX\nj=1\n\u03bejeT\nj S\u03b1ej\n(19.9)\nsubject to 0 \u2264\u03bej \u22641, PN\nj=1 \u03bej = p and u1,...,uN orthonormal basis of RN. First con-\nsider minimization with respect to the basis, fixing \u03be. There is obviously no loss of\ngenerality in requiring that \u03be1 \u2264\u03be2 \u2264\u00b7\u00b7\u00b7 \u2264\u03beN, and using corollary 2.4 (adapted\nto minimizing (19.9) rather than maximizing it) we know that an optimal basis\nis given by the eigenvectors of S\u03b1, ordered with non-decreasing eigenvalues. Let-\nting \u03bb1 \u2264\u00b7\u00b7\u00b7 \u2264\u03bbN denote these eigenvalues, we find that \u03be1,...,xN must be a non-\ndecreasing sequence minimizing\nN\nX\nj=1\n\u03bbj\u03bej\nsubject to 0 \u2264\u03bek \u22641 and PN\nj=1 \u03bej = p. The optimal solution is obtained by taking\n19.4. SPECTRAL CLUSTERING\n469\n\u03be1 = \u00b7\u00b7\u00b7 = \u03bep = 1, since, for any other solution\nN\nX\nj=1\n\u03bbj\u03bej \u2212\np\nX\nj=1\n\u03bbj \u2265\u03bbp+1\nN\nX\nj=p+1\n\u03bej +\np\nX\nj=1\n\u03bbj(\u03bej \u22121)\n= \u03bbp+1\np\nX\nj=1\n(1 \u2212\u03bej) +\np\nX\nj=1\n\u03bbj(\u03bek \u22121)\n=\np\nX\nj=1\n(\u03bbp+1 \u2212\u03bbj)(1 \u2212\u03bej)\n\u22650.\nThe following algorithm (similar to that discussed in [64]) summarizes this dis-\ncussion.\nAlgorithm 19.7 (Spectral clustering: version 1)\nLet S\u03b1 be an N \u00d7 N discrepancy matrix. Let p denote the number of clusters.\n(1) Compute the eigenvectors of S\u03b1 associated with the p smallest eigenvalues.\n(2) Denoting these eigenvectors by e1,...,ep, define y1,...,yN \u2208Rp by y(j)\nk = e(k)\nj .\n(3) Run K-means on (y1,...,yN) to determine a partition.\nThis algorithm needs to be slightly modified if one also wants Z to satisfy Z1 = 1.\nIn that case, 1 is one of the eigenvectors (with eigenvalue 1), and the others are\northogonal to it. As a consequence, one now looks for Z in the form\nZ =\nN\u22121\nX\nk=1\n\u03bejejeT\nj + 1\nN 11T\nleading to the minimization of\nN\u22121\nX\nj=1\n\u03bejeT\nj S\u03b1ej + 1\nN 1T S\u03b11\nover all \u03be1,...,\u03beN\u22121 such that 0 \u2264\u03bej \u22641 and PN\nj=1 \u03bej = p \u22121, and over all e1,...,eN\u22121\nsuch that e1,...,eN\u22121,1/\n\u221a\nN form an orthonormal basis. The main difference with the\nprevious problem is that we now need to ensure that all ej are perpendicular to 1.\n470\nCHAPTER 19. CLUSTERING\nTo achieve this, introduce the projection matrix P = IdRN \u221211T /N and let \u02dcS\u03b1 =\nPS\u03b1P. Then, since uT1 = 0 implies uT \u02dcS\u03b1u = uT S\u03b1u, it is equivalent to minimize\nN\u22121\nX\nj=1\n\u03bejeT\nj \u02dcS\u03b1ej\nover all \u03be1,...,\u03beN\u22121 such that 0 \u2264\u03bej \u22641 and PN\nj=1 \u03bej = p \u22121, and over all e1,...,eN\u22121\nsuch that e1,...,eN\u22121,1/\n\u221a\nN form an orthonormal basis. Because \u02dcS\u03b11 = 0, we know\nthat \u02dcS\u03b1 can be diagonalized in an orthonormal basis (e1,...,eN\u22121,1/\n\u221a\nN), and we ob-\ntain an optimal solution by selecting the p\u22121 vectors associated with smallest eigen-\nvalues, with associated \u03bej = 1. We therefore get a modified version of the spectral\nclustering algorithm.\nAlgorithm 19.8 (Spectral clustering: version 2)\nLet S\u03b1 be an N \u00d7 N discrepancy matrix. Let p denote the number of clusters. Let\nP = IdRN \u22121N1T\nN/N.\n(1) Compute \u02dcS\u03b1 = PS\u03b1P\n(2) Compute the eigenvectors of \u02dcS\u03b1 associated with the p \u22121 smallest eigenvalues.\n(3) Denoting these eigenvectors by e1,...,ep\u22121, define y1,...,yN \u2208Rp\u22121 by y(j)\nk = e(k)\nj .\n(4) Run K-means on (y1,...,yN) to determine a partition.\n19.5\nGraph partitioning\nSimilarity measures are often associated with graph structures, with a goal of finding\na partition of their set of vertices. So, let T denote the set of these vertices and\nassume that to all pairs x,y \u2208T, one attribute a weight given by \u03b2(x,y), where \u03b2 is\nassumed to be non-negative. We define \u03b2 for all x,y \u2208T, but we interpret \u03b2(x,y) = 0\nas marking the absence of an edge between x and y.\nLet V denote the vector space of all functions f : T \u2192R (we have dim(V ) = |T|).\nThis space can be equipped with the standard Euclidean norm, that we will call\nin this section the L2 norm (by analogy with general spaces of square integrable\nfunctions), letting,\n|f |2\n2 =\nX\nx\u2208T\nf (x)2.\nOne can also associate a measure of smoothness for a function f \u2208V by computing\nthe discrete \u201cH1\u201d semi-norm,\n|f |2\nH1 =\nX\nx,y\u2208T\n\u03b2(x,y)(f (x) \u2212f (y))2.\n19.5. GRAPH PARTITIONING\n471\nWith this definition, \u201csmooth functions\u201d tend to have similar values at points x,y\nin T such that \u03b2(x,y) is large while there is less constraint when \u03b2(x,y) is small. In\nparticular, |f |H1 = 0 if and only if f is constant on connected components of the\ngraph.3\nThe notion of connected components, combined with thresholding, can be used\nto build a hierarchical family of partitions of the graph. Define, for all t > 0, the\nthresholded weights \u03b2(t)(x,y) = max(\u03b2(x,y) \u2212t,0). The set of connected components\nassociated with the pair (V ,\u03b2(t)) forms a partition, say, A(t), of T . The resulting set\nof partitions is nested in the sense that, if s < t, the sets forming the partition A(s) are\nunions of sets forming A(t). This thresholding procedure is not always satisfactory,\nhowever, because there does not always exist a fixed value of t that produces a good\nquality cluster decomposition.\nIf there exists p connected components, then the subspace of all functions f \u2208V\nsuch that |f |H1 = 0 has dimension p. If C1,...,Cp are the connected components,\nthis space is generated by the functions \u03b4Ck, k = 1,...,p, with \u03b4Ck(x) = 1 if x \u2208Ck\nand 0 otherwise. These functions form, in addition, an orthogonal system for the\nEuclidean inner product: \u27e8\u03b4Ck , \u03b4Cl\u27e92 = 0 if k , l.\nOne can write 1\n2|f |2\nH1 = f T Lf where L, called the Laplacian operator associated to\nthe considered graph, is defined by\nLf (x) =\nX\ny\u2208T\nL(x,y)f (y)\nand\nL(x,y) =\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\nX\nz\u2208T\n\u03b2(x,z)\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f81x=y \u2212\u03b2(x,y).\n(19.10)\nThe vectors \u03b4Ck, k = 1,...,p are then an orthogonal basis of the null space of L. Con-\nversely, let (e1,...,ep) be any basis of this null space. Then, there exists an invertible\nmatrix A = (aij,i,j = 1,...,p) such that\nei(x) =\np\nX\nj=1\naij\u03b4Cj(x).\nAssociate to each x \u2208T the vector e(x) =\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\ne1(x)\n...\nep(x)\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\n\u2208Rp. Then, for any x,y \u2208T, we have\ne(x) = e(y) if and only if \u03b4Cj(x) = \u03b4Cj(y) for all j = 1,...,p (because A is invertible),\n3Two nodes x and y are connected in the graph if there is a sequence z0,...,zn in T such that z0 = x,\nzn = y and \u03b2(zi,zi\u22121) > 0 for i = 1,...,n. This provides an equivalence relation and equivalent classes\nare called connected components.\n472\nCHAPTER 19. CLUSTERING\nthat it, if and only if x and y belong to the same connected component. So, given\nany basis of the null space of L, the function x 7\u2192e(x) determines these connected\ncomponents. So, a\u2014not very efficient\u2014way of determining the connected compo-\nnents of the graph can be to diagonalize the operator L (written as an N by N matrix,\nwhere N = |T|), extract the p eigenvectors e1,...,ep associated with eigenvalue zero\nand deduce from the function e(x) above the set of connected components.\nNow, in practice, the graph associated to T and \u03b2 will not separate nicely into\nconnected components in order to cluster the training set. Most of the time, because\nof noise or some weak connections, there will be only one such component, or in any\ncase much less than what one would expect when clustering the data. The previous\ndiscussion suggests, however, that in the presence of moderate noise in the con-\nnection weights, one may expect that the eigenvectors associated to the p smallest\neigenvalues of L provide vectors e(x),x \u2208T such that e(x) and e(y) have similar values\nif x and y belong to the same cluster (see 19.2). In such cases, these clusters should\nbe easy to determine using, say, K-means on the transformed dataset \u02dcT = (e(x),x \u2208T ).\nThis is summarized in the following algorithm.\nAlgorithm 19.9 (Spectral Graph Partitioning)\nLet T \u2282R be the training set and (x,y) 7\u2192\u03b2(x,y) a similarity measure defined on\nT \u00d7 T . Let p be the desired number of clusters.\n(1) Form the Laplacian operator described in (19.10) and let e1,...,ep be its eigen-\nvectors associated to the p lowest eigenvalues. For x \u2208T, let e(x) \u2208Rp be given\nby\ne(x) = (e1(x),...,ep(x))T \u2208Rp.\n(2) Apply the K-means algorithm (or one of its variants) with p clusters to \u02dcT =\n(e(x),x \u2208T).\n19.6\nDeciding the number of clusters\n19.6.1\nDetecting elbows\nThe number, p, of subsets with respect to which the population should be parti-\ntioned is rarely known a priori, and several methods have been introduced in the\nliterature in order to assess the ideal number of clusters. We now review some of\nthese methods, and denote, for this purpose, by L\u2217(p) the minimized cost function\nobtained with p clusters, e.g., using (19.3),\nL\u2217(p) = min{W\u03b1(A1,...,Ap,c1,...,cp) : A1,...,Ap partition of T ,c1,...,cp \u2208R},\n19.6. DECIDING THE NUMBER OF CLUSTERS\n473\nFigure 19.2:\nExample of data transformed using the eigenvectors of the graph Laplacian.\nLeft: Original data. Center: Result of a Kmeans algorithm with three clusters applied to the\ntransformed data (2D projection). Right: Visualization of the cluster labels on the original\ndata.\nin the case of K-medoids (this definition is algorithm dependent). It is clear that L\u2217\nis a decreasing function of p. It is also natural to expect that L\u2217should decrease\nsignificantly when p is smaller than the correct number of clusters, while the varia-\ntion should be more marginal when p is overestimated, because the cost in putting\ntogether two sets of points that are far apart (which happens when p is too small) is\ntypically larger than the gain in splitting a homogeneous region in two.\nThe simplest approach in this context is to visualize L\u2217(p) as a function of p and\ntry to locate at which value the resulting curve makes an \u201celbow,\u201d i.e., switches from\na sharply decreasing slope to a milder one. Figure 19.3 provides an illustration of\nthis visualization when the true number of clusters is three (the data in each cluster\nfollowing a normal distribution). When the clusters are well separated, an elbow\nclearly appears on the graph of \u0393\u2217\n\u03b1, but this situation is harder to observe when clus-\nters overlap with each other.\nOne can measure the \u201ccurvature\u201d at the elbow using the distance between each\npoint in the graph of (p,W \u2217\n\u03b1(p)) and the line between its predecessor and successor.\nThe result gives the criterion\nC(p) = L\u2217(p + 1) + L\u2217(p \u22121) \u22122L\u2217(p)\np\n(L\u2217(p + 1) \u2212L\u2217(p \u22121))2 + 4\n,\nspecifying the elbow point as the value of p at which C attains its maximum. For\nboth examples in fig. 19.3, this method returns the correct number of clusters (3).\n19.6.2\nThe Cali\u00b4nski and Harabasz index\nSeveral other criteria have been introduced in the literature. Cali\u00b4nski and Harabasz\n[46] propose to minimize the ratio of normalized between-group and within-groups\nsums of squares associated with K-means. For a given p, let c1,...,cp denote the\noptimal centers, and A1,...,Ap the optimal partition, with Nk = |Ak|. The normalized\n474\nCHAPTER 19. CLUSTERING\nFigure 19.3: Elbow graphs for K-means clustering for two populations generated as mixtures\nof Gaussian.\nbetween-group sum of squares is\nh\u03b1(p) =\n1\np \u22121\np\nX\nk=1\nNk|ck \u2212x|2\nand the normalized within-group sum of squares is\nw\u03b1(p) =\n1\nN \u2212p\np\nX\nk=1\nX\nx\u2208Ak\n|x \u2212ck|2\nCali\u00b4nski and Harabasz [46] suggest to maximize \u03b3CH(p) = h\u03b1(p)/w\u03b1(p).\nThis criterion can be extended to other types of cluster analysis. We have seen in\nsection 19.4 that, when \u03b1(x,y) = |x \u2212y|2,\n1\n2\np\nX\nk=1\nX\nx,y\u2208Ak\n\u03b1(x,y)/Nk =\np\nX\nk=1\nX\nx\u2208Ak\n|x \u2212ck|2.\nWe also have\nX\nx\u2208T\n|x \u2212x|2 =\np\nX\nk=1\nX\nx\u2208Ak\n|x \u2212ck|2 +\np\nX\nk=1\nNk|ck \u2212x|2\n19.6. DECIDING THE NUMBER OF CLUSTERS\n475\nand the left-hand side is also equal to\n1\n2N\nX\nx,y\u2208T\n\u03b1(x,y).\nIt follows that, when \u03b1(x,y) = |x \u2212y|2,\nh\u03b1(p) =\n1\n2(p \u22121)\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\n1\nN\nX\nx,y\u2208T\n\u03b1(x,y) \u2212\np\nX\nk=1\nX\nx,y\u2208Ak\n\u03b1(x,y)/Nk\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\nand\nw\u03b1(p) =\n1\n2(N \u2212p)\np\nX\nk=1\nX\nx,y\u2208Ak\n\u03b1(x,y)/Nk.\nThese expressions can obviously be applied to any dissimilarity measure, extending\n\u03b3CH to general clustering problems.\n19.6.3\nThe \u201csilhouette\u201d index\nFor x \u2208T , let\nd\u03b1(x,Ak) = 1\nNk\nX\ny\u2208Ak\n\u03b1(x,y).\nLet a\u03b1(x,p) = d\u03b1(x,A(x)) and b(x,p) = min{d\u03b1(x,Ak) : Ak , A(x)}. Define the silhouette\nindex of x in the segmentation [170]by\ns\u03b1(x,p) =\nb\u03b1(x,p) \u2212a\u03b1(x,p)\nmax(b\u03b1(x,p),a\u03b1(x,p)) \u2208[\u22121,1].\nThis index measures how well x is classified in the partitioning. It is large when\nthe mean distance between x and other objects in its class is small compared to the\nminimum mean distance between x and any other class. In order to estimate the best\nnumber of clusters with this criterion, one then can maximize the average index:\n\u03b3R(p) = 1\nN\nX\nx\u2208T\ns\u03b1(x,p).\nRemark 19.9 One can rewrite the Cali\u00b4nski and Harabasz index using the notation\nintroduced for the silhouette index. Indeed, let A(x) be the cluster Ak to which x\nbelongs. Then\nh\u03b1(p) =\n1\n2(p \u22121)\nX\nx\u2208T\np\nX\nk=1\nNk\nN (d\u03b1(x,Ak) \u2212d\u03b1(x,A(x)))\nand\nw\u03b1(p) =\n1\n2(N \u2212p)\np\nX\nk=1\nX\nx\u2208Ak\nd\u03b1(x,Ak).\n\u2666\n476\nCHAPTER 19. CLUSTERING\nFigure 19.4: Division of the unit square into clusters for uniformly distributed data.\n19.6.4\nComparing to homogeneous data\nSeveral selection methods choose p based on the comparison of the data to a \u201cnull\nhypothesis\u201d of no cluster. For example, assume that K-means is applied to a training\nset T where samples are drawn uniformly according to the uniform distribution on\n[0,1]d. Given centers, c1,...,cp, let \u00afAk be the set of points in [0,1]d that are closer\nto ck than to any other point. Then the segmentation of T is formed by the sets\nAk = {x \u2208T : x \u2208\u00afAk} and, for large enough N, we can approximate |Ak|/N (by the\nLaw of Large Numbers) by the volume of the set \u00afAk, that we will denote by vol( \u00afAk).\nLet us assume that c1,...,cp are uniformly spaced, so that the sets \u00afAk have similar\nvolumes (close to 1/p) and have roughly spherical shapes (see fig. 19.4). This implies\nthat\nZ\n\u00afAk\n|x \u2212ck|2dx \u2243vol(Ak)\nr2\npd\nd + 2\nwhere rp is the radius of a sphere of volume 1/p, i.e., prd\np \u2243d/\u0393d\u22121 where \u0393d\u22121 is the\nsurface area of the unit sphere in Rd. So, we should have, for some constant C that\nonly depends on d,\nX\nx\u2208Ak\n|x \u2212ck|2 \u2243Nk\nZ\n\u00afAk\n|x \u2212ck|2dx \u2243C(d)(pN)p\u22122/d\u22121 = C(d)Np\u22122/d.\nThis suggests that, for fixed N and d, p2/dL\u2217(p) should vary slowly when p overesti-\n19.6. DECIDING THE NUMBER OF CLUSTERS\n477\nmate the number of clusters (assuming that this operation divides an homogeneous\ncluster). Based on this analysis, Krzanowski and Lai [111] introduced the difference-\nratio criterion, namely,\n\u03b3KL(p) =\n\f\f\f\f\f\f\f\n(p \u22121)\n2\nd L\u2217(p \u22121) \u2212p\n2\nd L\u2217(p)\np\n2\nd L\u2217(p) \u2212(p + 1)\n2\nd L\u2217(p + 1)\n\f\f\f\f\f\f\f\n,\nand estimate the number of clusters by taking p maximizing \u03b3KL.\nAnother similar approach, introduced by Sugar and James [185], is based on an\nanalysis of mixtures of Gaussian, namely assuming an underlying model with p0\ngroups, where data in group k follow a Gaussian distribution N (\u00b5k,Id) (possibly\nafter standardizing the covariance matrix). In that work, the authors show that,\nif d (the dimension) tends to infinity, with the minimal distance between centers\ngrowing proportionally to\n\u221a\nd, then L\u2217(p)/d tends to infinity when p < p0. They also\nshow that, with similar assumptions, L\u2217(p)/d behaves like p\u22122/d for p \u2265p0, still for\nlarge dimensions. Based on this, they suggest using the criterion\n\u03b3SJ(p) =\n L\u2217(p)\nd\n!\u2212\u03bd\n\u2212\n L\u2217(p \u22121)\nd\n!\u2212\u03bd\n(with the convention that L\u2217(0) = 0) for some positive number \u03bd and select the value\nof p that maximizes \u03b3SJ. Indeed, in the case of Gaussian mixtures, the choice \u03bd = d/2\nensures that, in large dimensions, \u03b3SJ(p) is small for p < p0, that it is close to 1 for\np > p0 and close to p0 for p = p0.\nA more computational approach, based on Monte-Carlo simulations has been\nintroduced in Tibshirani et al. [191], defining the gap index\n\u03b3TWH(p) = E(L\u2217(p,T \u266f)) \u2212L\u2217(p,T )\nwhere the L\u2217(p,T ) denotes the optimal value of the optimized cost with p clusters\nfor a training set T. The notation T \u266frepresent a random training set, with same\nsize and dimension as T , generated using an unclustered probability distribution\nused as a reference. In Tibshirani et al. [191], this distribution is taken as uniform\n(over the smallest hypercube containing the observed data), or uniform on the co-\nefficients of a principal component decomposition of the data (see chapter 20). The\nexpectation E(L\u2217(p,T \u266f)) is computed by Monte-Carlo simulation, by sampling many\nrealizations of the training set T , running the clustering algorithm for each of them\nand averaging the optimal costs.\nOne can expect L\u2217(p,T ) (for observed data) to decrease much faster (when adding\na cluster) than its expectation for homogeneous data when p < p0, and the decrease of\n478\nCHAPTER 19. CLUSTERING\nboth terms to be comparable when p \u2265p0. So the number of clusters can in principle\nbe estimated by detecting an elbow in the graph of \u03b3TWH(p) as a function of p. The\nprocedure suggested in Tibshirani et al. [191] in order to detect this elbow if to look\nfor the first index p such that\n\u03b3TWH(p + 1) \u2264\u03b3TWH(p) + \u03c3(p + 1)\nwhere \u03c3(p + 1) is the standard deviation of L\u2217(p + 1,T \u266f) for homogeneous data, also\nestimated via Monte-Carlo simulation.\nFigures figs. 19.5 to 19.7 provide a comparative illustration of some of these in-\ndexes.\n19.7\nBayesian Clustering\n19.7.1\nIntroduction\nWe have seen an example of model-based clustering with mixtures of Gaussian dis-\ntributions. The main parameters in this model were the number of classes, p, and\nthe probabilities \u03b1j associated to each cluster, and the parameter of the conditional\ndistribution (e.g., N (cj,\u03c32IdRd)) of X conditionally to being in the jth cluster. In the\napproach we described, these parameters were estimated from data using maximum\nlikelihood (through the EM algorithm) and probabilities fZ(j|x) were then estimated\nin order to compute the most likely clustering.We interpreted fZ(j|x) as the condi-\ntional probability P(Z = z|X = x), where Z \u2208{1,...,p} represents the group variable.\nThe natural generative order is Z \u2192X: first decide to which group the observation\nbelongs to, then sample the value of X conditional to this group. Clustering is in this\ncase reversing the order, i.e., computing the posterior distribution of Z given X.\nIn a Bayesian approach, the parameters p,\u03b1,c and \u03c32 are also considered as ran-\ndom variables, so that (letting \u03b8 denote the vector formed by these parameters), the\ngenerative random sequence becomes \u03b8 \u2192Z \u2192X. Importantly, \u03b8 is assumed to\nbe generated once for all, even if several samples of X are observed, yielding the\ngenerative sequence for an N-sample,\n\u03b8 \u2192(Z1,...,ZN) \u2192(X1,...,XN).\nWe use below underlined letters to denote configurations of points, Z = (Z1,...,ZN),\nX = (X1,...,XN), etc. We also use capital letters or boldface letters (for Greek sym-\nbols) to differentiate random variable from realizations.\nClusters are still evaluated based on the conditional distribution of Z given X,\nbut this distribution must be evaluated by averaging the conditional distribution of\n19.7. BAYESIAN CLUSTERING\n479\nFigure 19.5: Comparison of cluster indices for Gaussian clusters. First row: original data\nand ground truth. Second panel: plots of four indices as functions of p (Elbow; Cali\u00b4nski and\nHarabasz; silhouette; Sugar and James)\n480\nCHAPTER 19. CLUSTERING\nFigure 19.6: Comparison of cluster indices for Gaussian clusters. First row: original data\nand ground truth. Second panel: plots of four indices as functions of p (Elbow; Cali\u00b4nski and\nHarabasz; silhouette; Sugar and James).\n19.7. BAYESIAN CLUSTERING\n481\nFigure 19.7: Comparison of cluster indices for Gaussian clusters. First row: original data\nand ground truth. Second panel: plots of four indices as functions of p (Elbow; Cali\u00b4nski and\nHarabasz; silhouette; Sugar and James).\n482\nCHAPTER 19. CLUSTERING\nZ and \u03b8 given X with respect to \u03b8, formally4,\nP(z|x) =\nZ\nP(z,\u03b8|x)P(\u03b8)d\u03b8\n\u221d\nZ\nN\nY\nk=1\nP(xk|zk,\u03b8)P(zk|\u03b8)P(\u03b8)d\u03b8.\nIn this expression, P(\u03b8)d\u03b8 implies an integration with respect to the prior distribu-\ntion of the parameters. This distribution is part of the design of the method, but one\nusually chooses it so that it leads to simple computations, using so-called conjugate\npriors, which are such that posterior distributions belong to the same parametric\nfamily as the prior. For example, the conjugate prior for the mean of a Gaussian\ndistribution (such as ci in our model) is also a Gaussian distribution. The conjugate\nprior for a scalar variance is the inverse gamma distribution, with density\nvu\n\u0393(u)s\u2212u\u22121 exp(\u2212v/s)\nfor some parameters u,v. A conjugate prior for the class probabilities \u03b1 = (\u03b11,...,\u03b1p)\nis the Dirichlet distribution, with density\nD(\u03b11,...,\u03b1p) =\n\u0393(a1 + \u00b7\u00b7\u00b7 + ap)\n\u0393(a1)\u00b7\u00b7\u00b7\u0393(ap)\np\nY\nj=1\n\u03b1\naj\u22121\nj\non the simplex\nSp = {(\u03b11,...,\u03b1p) \u2208Rp : \u03b1i \u22650,\u03b11 + \u00b7\u00b7\u00b7 + \u03b1p = 1}.\nNote that these conjugate priors have the same form (up to normalization) as the\nparametric model densities when considered as functions of the parameters.\n19.7.2\nModel with a bounded number of clusters\nWe first discuss the Bayesian approach assuming that the number of clusters is\nsmaller than a fixed number, p. In this example, we assume that c1,...,cp are mod-\neled as independent Gaussian variables N (0,\u03c42IdRd), \u03c32 with an inverse gamma\ndistribution with parameters u and v and (\u03b11,...,\u03b1p) using a Dirichlet distribution\nwith parameters (a,...,a).\n4The symbol \u221dmeans \u201cequal up to a multiplicative constant\u201d.\n19.7. BAYESIAN CLUSTERING\n483\nAnalytical example.\nThe joint probability density of (X,Z) and \u03b8 is proportional\nto\n(\u03c32)\u2212u\u22121e\u2212v/\u03c32e\u2212Pp\nj=1 |cj|2/2\u03c42\np\nY\nj=1\n\u03b1a\u22121\nj\nN\nY\nk=1\ne\u2212|xk\u2212czk |2/2\u03c32\n(\u03c32)d/2\nN\nY\nk=1\n\u03b1zk\n= (\u03c32)\u2212u\u2212dN/2\u22121 exp\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\u2212(v + 1\n2\nN\nX\nk=1\n|xk \u2212czk|2)/\u03c32\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\np\nY\nj=1\n\u03b1\na+Nj\u22121\nj\n.\nOne can explicitly integrate this last expression with respect to \u03c32 and \u03b1, using\nthe expressions of the normalizing constants in the inverse gamma and Dirichlet\ndistributions, yielding (after integration and ignoring constant terms)\n\u0393(a + N1)\u00b7\u00b7\u00b7\u0393(a + Np)\n(v + 1\n2\nPN\nk=1 |xk \u2212czk|2)u+dN/2 exp\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\u2212\np\nX\nj=1\n|cj|2/2\u03c42\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\n=\n\u0393(a + N1)\u00b7\u00b7\u00b7\u0393(a + Np)\n(v + 1\n2Sw + 1\n2\nPp\nj=1 Nj|cj \u2212\u00afxj|2)u+dN/2 exp\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\u2212\np\nX\nj=1\n|cj|2/2\u03c42\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\nwhere Sw = PN\nk=1 |xk \u2212\u00afxzk|2 is the within group sum of squares. Note that this sum of\nsquares depends on x and z, and that (N1,...,Np), the group sizes, depend on z.\nLet us assume a \u201cnon-informative prior\u201d on the centers, which corresponds to\nletting \u03c4 tend to infinity and neglecting the last exponential. The remaining expres-\nsion can now be integrated with respect to c1,...,cp by making a change of variables\n\u00b5j =\nq\nNj/(2v + Sk)(cj \u2212xj) and using the fact that\nZ\n(Rd)p\ndc1 ...dcp\n(v + 1\n2Sw + 1\n2\nPp\nj=1 Nj|cj \u2212\u00afxj|2)u+dN/2 =\n(2v + Sw)(p\u2212N)d/2\u2212u)\np\nY\nj=1\nN \u2212d/2\nj\nZ\n(Rd)p\nd\u00b51 ...d\u00b5p\n(1\n2 + 1\n2\nPp\nj=1 |\u00b5j|2)u+dN/2\nand the final integral does not depend on x or z. It follows from this that the condi-\ntional distribution of Z given x takes the form\nP(z|x) = C(x)\nQp\nj=1 \u0393(a + Nj)\n(2v + Sw)(N\u2212p)d/2+u) Qp\nj=1 N d/2\nj\nwhere C(x) is a normalization constant ensuring that the right-hand side is a proba-\nbility distribution over configurations z = (z1,...,zN) \u2208{1,...,p}N. In order to obtain\n484\nCHAPTER 19. CLUSTERING\nthe most likely configuration for this posterior distribution, one should therefore\nminimize in z the function\n((N \u2212p)d\n2 + u)log(2v + Sw) + d\n2\np\nX\nj=1\nlogNj \u2212\np\nX\nj=1\nlog\u0393(a + Nj).\nThis final optimization problem cannot be solved in closed form, but this can be\nperformed numerically. One can simplify it a little by only keeping the main order\nterms in the last two sums (using Stirling formula for the Gamma function) and\nminimize\n((N \u2212p)d\n2 + u)log(2v + Sw) \u2212\np\nX\nj=1\n(a + Nj)log(a + Nj).\nThis expression has a nice interpretation, since the first term minimizes the within-\ngroup sum of squares, the same objective function as in K-means, and the second\none is an entropy term that favors clusters with similar sizes.\nMonte-Carlo simulation.\nAn alternative to this analytical approach is to use Monte-\nCarlo simulations to estimate some properties of the posterior distribution numeri-\ncally. While they are often computationally demanding, Monte-Carlo methods are\nmore flexible and can be used in situations when analytic computations are intrac-\ntable. In order to sample from the distribution of Z given x, it is actually easier to\nsample from the joint distribution of (Z,\u03b8) given x, because this distribution has a\nsimpler form. Of course, if the pair (Z,\u03b8) is sampled from the conditional distri-\nbution given x, the first component, Z will follow the posterior distribution we are\ninterested in.\nIn the context of the discussed example, this reduces to sampling from a distri-\nbution proportional to\n(\u03c32)\u2212u\u22121e\u2212v/\u03c32e\u2212Pp\nj=1 |cj|2/2\u03c42\np\nY\nj=1\n\u03b1a\u22121\nj\nN\nY\nk=1\ne\u2212|xk\u2212czk |2/2\u03c32\n(\u03c32)d/2\nN\nY\nk=1\n\u03b1zk .\n(19.11)\nSampling from all these variables at once is not tractable, but it is easy to sample\nfrom them in sub-groups, conditionally to the rest of the variables. We can, for\nexample, deduce from the expression above the following conditional distributions.\n(i) Given (\u03b1,c,z), \u03c32 follows an inverse gamma distribution with parameters u +\ndN/2 and v + 1\n2\nPN\nk=1 |xk \u2212czk|2.\n(ii) Given (z,z,\u03c32), \u03b1 follows a Dirichlet distribution with parameters a+N1,...,a+\nNp.\n19.7. BAYESIAN CLUSTERING\n485\n(iii) Given (z,\u03c32,\u03b1), c1,...,cp are independent and follow a Gaussian distribution,\nrespectively with mean (1 + \u03c32/(Nj\u03c42))\u22121 \u00afxj and variance (Nj/\u03c32 + 1/\u03c42)\u22121.\n(iv) Given (\u03c32,\u03b1,c), z1,...,zN are independent and\nP(zk = j|\u03c32,\u03b1,c,x) \u221d\u03b1je\u2212|xk\u2212cj|2/2\u03c32.\nAlgorithm 19.10 (Gibbs sampling for mixture of Gaussian (Bayesian case))\n(1) Initialize with variables \u03b1,c,\u03c3 and z, for example generated according to the\nprior distribution.\n(2) Loop a large number of times over the following steps.\n(i) Simulate a new value of \u03c32 according to an inverse gamma distribution with\nparameters u + dN/2 and v + 1\n2\nPN\nk=1 |xk \u2212czk|2.\n(ii) Simulate new values for \u03b11,...,\u03b1p according to a Dirichlet distribution with\nparameters a + N1,...,a + Np.\n(iii) Simulate new values for c1,...,cp independently, sampling ci according to\na Gaussian distribution with mean (1+\u03c32/(Nj\u03c42))\u22121 \u00afxj and variance (Nj/\u03c32 +1/\u03c42)\u22121.\n(iv) Simulate new values of z1,...,zN independently such that\nP(zk = j|\u03c32,\u03b1,c,x) \u221d\u03b1je\u2212|xk\u2212cj|2/2\u03c32.\nNote that this algorithm is only asymptotically providing a sample of the poste-\nrior distribution (it has to be stopped at some point, of course). Note also that, at\neach step, the labels z1,...,zN provide a random partition of the set {1,...,N}, and\nthis partition changes at every step.\nTo estimate one single partition out of this simulation, several strategies are pos-\nsible. Using the simulation, one can estimate the probability wkl that xk and xl be-\nlong to the same cluster. This can be dome by averaging the number of times that\nzk = zl was observed along the Gibbs sampling iterations (from which one usually\nexcludes a few early \u201cburn-in\u201d iterations). These weights, wkl can then be used as\nsimilarity measures in a clustering algorithm.\nAlternatively, one can average for each k, the values of the class center czk associ-\nated to k, still along the Gibbs sampling iterations. These average values can then be\nused as input of, say, a K-means algorithm to estimate final clusters.\n486\nCHAPTER 19. CLUSTERING\nMean-field approximation.\nWe conclude this section with a variational Bayes ap-\nproximation of the posterior distribution. We will make a mean-field approxima-\ntion, in which all parameters and latent variables are independent, therefore ap-\nproximating the distribution in (19.11) by a product distribution taking the form\ng(\u03c32,\u03b1,c,z) = g(\u03c32)(\u03c32)g(\u03b1)(\u03b1)\np\nY\nj=1\ng(c)\nj (cj)\nN\nY\nk=1\ng(z)\nk (zk).\nHere c = (c1,...,cp), z = (z1,...,zN) and \u03b1 = (\u03b11,...,\u03b1p). We have \u03c32 \u2208(0,+\u221e), c \u2208\n(Rd)p, \u03b1 \u2208S, the set of all non-negative \u03b11,...,\u03b1p that sum to one, and z \u2208{1,...,p}N\n(so that g(x)\nk\nis a p.m.f. on {1,...,p}. We will use the discussion in section 16.3.3 and\nlemma 16.1, and use the notation introduced in that section to denote as \n\u03d5\u000b the\nexpectation a variable \u03d5 of the variables above for the p.d.f. g.\nThe log-likelihood for a mixture of Gaussian takes the form (ignoring contant\nterms)\n\u2113(\u03c32,\u03b1,c,z) = \u2212(u + 1)log\u03c32 \u2212v\u03c3\u22122 \u22121\n2\u03c42\np\nX\nk=1\n|cj|2 +\np\nX\nj=1\n(a \u22121)log\u03b1j\n\u2212Nd\n2 log\u03c32 \u22121\n2\u03c3\u22122\nN\nX\nk=1\n|xk \u2212czk|2 +\nN\nX\nk=1\nlog\u03b1zk\n= \u2212(u + 1)log\u03c32 \u2212v\u03c3\u22122 \u22121\n2\u03c42\np\nX\nk=1\n|cj|2 +\np\nX\nk=1\n(a \u22121)log\u03b1j\n\u2212Nd\n2 log\u03c32 \u22121\n2\u03c3\u22122\nN\nX\nk=1\np\nX\nj=1\n|xk \u2212cj|21zk=j +\nN\nX\nk=1\np\nX\nj=1\nlog\u03b1j1zj=k\nand can therefore be decomposed as a sum of products of functions of single vari-\nables, as assumed in section 16.3.3. Using lemma 16.1, we can identify each of the\ndistributions composing g, namely:\n\u2022 g(\u03c32) is the p.d.f. of an inverse gamma with parameters \u02dcu = u + Nd/2 and\n\u02dcv = \u03bd + 1\n2\nN\nX\nk=1\np\nX\nj=1\nD\n|xk \u2212Cj|2E \nZk = j\u000b.\n\u2022 g(c)\nj\nis the p.d.f. of a Gaussian, with parameters N ( \u02dcmj, \u02dc\u03c32\nj IdRd), with, letting\n\u02dc\u03b6(j) =\nN\nX\nk=1\n\nZk = j\u000b =\nN\nX\nk=1\ng(z)\nk (j),\n19.7. BAYESIAN CLUSTERING\n487\n\u02dc\u03c32\nj =\n\u0010 1\n\u03c42 +\nD\n\u03c3\u22122E \u02dc\u03b6(j)\n\u0011\u22121 and \u02dcmi =\nD\n\u03c3\u22122E\n\u02dc\u03c32\nj\nPN\nk=1\n\nZk = j\u000bxk.\n\u2022 g(\u03b1) of a Dirichlet distribution, with parameters \u02dca1,..., \u02dcak, with \u02dcai = a + \u02dc\u03b6(j).\n\u2022 Finally g(z)\nk\nis a p.m.f. on {1,...,p} with\ng(z)\nk (j) \u221dexp\n\u0012\n\u22121\n2\nD\n\u03c3\u22122ED\n|xk \u2212Cj|2E\n+\nD\nlog\u03b1j\nE\u0013\n.\nTo complete the consistency equations, it now suffices to evaluate the expecta-\ntions in the formula above as functions of the other parameters. We leave to the\nreader the verification of the following statements.\n\u2022 If \u03c32 follows an inverse gamma distribution with parameters \u02dcu and \u02dcv, then\nD\n\u03c3\u22122E\n=\n\u02dcu/ \u02dcv.\n\u2022 If Cj \u223cN ( \u02dcmj, \u02dc\u03c32\nj IdRd), then\nD\n|xk \u2212Cj|2E\n= |xk \u2212\u02dcmj|2 + d \u02dc\u03c32\nj .\n\u2022 If \u03b1 follows a Dirichlet distribution with parameters \u02dca1,..., \u02dcap, then\nD\nlog\u03b1j\nE\n=\n\u03c8(\u02dcaj) \u2212\u03c8(\u02dca1 + \u00b7\u00b7\u00b7 + \u02dcap) where \u03c8 is the digamma function (derivative of the logarithm\nof the gamma function).\nCombining these facts with the expression of the mean-field parameters, we can\nnow formulate a mean-field estimation algorithm for mixtures of Gaussian that iter-\natively applies the consistency equations.\nAlgorithm 19.11 (Mean-field algorithm for mixtures of Gaussian)\n(1) : Input: training set (x1,...,xN), number of clusters p, prior parameters u,v,\u03c42\nand a .\n(2) Initialize variables \u02dc\u03c32\n1 ,..., \u02dc\u03c32\np , \u02dcm1,..., \u02dcmp, \u02dca1,..., \u02dcap, \u02dcgk(j), k = 1,...,N, j = 1,...,p.\n(3) Let \u02dc\u03b6(j) = PN\nk=1 \u02dcgk(j), j = 1,...,p.\n(4) Let\n\u02dc\u03c12 =\n1\nu + Nd/2\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8edv + 1\n2\nN\nX\nk=1\np\nX\nj=1\n\u02dcgk(j)|xk \u2212\u02dcmj|2 + d\n2\np\nX\nj=1\n\u02dc\u03c32\nj \u02dc\u03b6(j)\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8.\n(5) For j = 1,...,p, let \u02dc\u03c32\ni =\n\u0012\n1\n\u03c42 +\n\u02dc\u03b6(j)\n\u02dc\u03c12\n\u0013\u22121\nand \u02dcmi =\n\u02dc\u03c32\nj\n\u02dc\u03c12\nPN\nk=1 \u02dcgk(j)xk.\n(6) Let \u02dcai = a + \u02dc\u03b6(j), j = 1,...,p.\n(7) For k = 1,...,N, j = 1,...,p, let\n\u02dcgk(j) \u221dexp\n \n\u22121\n2 \u02dc\u03c12\n\u0010\n|xk \u2212\u02dcmj|2 + d \u02dc\u03c32\nj\n\u0011\n+ \u03c8(\u02dcaj)\n!\n.\n488\nCHAPTER 19. CLUSTERING\n(8) Compare the updated variables with their previous values and stop if the dif-\nference is below a tolerance level. Otherwise, return to (3).\nAfter convergence g(z)\nk\nprovides the mean-field approximation of the posterior prob-\nability of classes for observation k and can be used to determine clusters.\n19.7.3\nNon-parametric priors\nThe Polya urn\nIn the previous model with p clusters or less, the joint distribution\nof Z1,...,ZN is given by\n\u03c0(z1,...,zN) = \u0393(pa)\n\u0393(a)p\nZ\nSp\np\nY\nj=1\n\u03b1\na+Nj\u22121\nj\nd\u03b1 =\n\u0393(pa)\n\u0393(pa + N)\np\nY\nj=1\n\u0393(a + Nj)\n\u0393(a)\n.\nConditional to z1,...,zN, the data model was completed by sampling p sets of pa-\nrameters, say, \u03b81,...,\u03b8p, each belonging to a parameter space \u0398 and following a prior\nprobability distribution with density, say, \u03c8 and variables X1,...,XN, where Xk \u2208R\nwas drawn according to a law dependent on its cluster, that we will denote \u03d5(\u00b7 | \u03b8zk).\nThe complete likelihood of the data is now\nL(z,\u03b8,x) =\n\u0393(pa)\n\u0393(pa + N)\np\nY\nj=1\n\u0393(a + Nj)\n\u0393(a)\np\nY\nj=1\n\u03c8(\u03b8j)\nN\nY\nk=1\n\u03d5(xk|\u03b8zk).\nNote that the right-hand side does not change if one relabels the values of z1,...,zN,\ni.e., if one replaces each zk by s(zk) where s is a permutation of {1,...,p}, creating a\nnew configuration denoted s \u00b7 z. Let [z] denote the equivalence class of z, containing\nall z\u2032 = s \u00b7 z,s \u2208SN: all the labelings in [z] provide the same partition of {1,...,N}\nand can therefore be identified. One defines a probability distribution \u00af\u03c0 over these\nequivalence classes by letting\n\u00af\u03c0([z]) = |[z]|\n\u0393(pa)\n\u0393(pa + N)\np\nY\nj=1\n\u0393(a + Nj)\n\u0393(a)\n.\nThe first term on the right-hand side is the number of elements in the equivalence\nclass of [z]. To compute it, let p0 = p0(z) denote the number of different values\ntaken by z1,...,zN, i.e., the \u201ctrue\u201d number of clusters (ignoring the empty ones),\nwhich now is a function of z. Let A1,...,Ap0 denote the partition associated with z.\nNew labelings equivalent to z can be obtained by assigning any index i1 \u2208{1,...,p}\n19.7. BAYESIAN CLUSTERING\n489\nto elements of A1, then any index i2 , i1 to elements of A2, etc., so that there are\n|[z]| = p!/(p \u2212p0)! choices. We therefore find:\n\u00af\u03c0([z]) =\np!\n(p \u2212p0)!\n\u0393(pa)\n\u0393(pa + N)\np\nY\nj=1\n\u0393(a + Nj)\n\u0393(a)\n.\nLetting \u03bb = pa and using the formula \u0393(x + 1) = x\u0393(x), this can be rewritten as\n\u00af\u03c0([z]) = p(p \u22121)\u00b7\u00b7\u00b7(p \u2212p0 + 1)\n\u03bb(\u03bb + 1)...(\u03bb + N \u22121)\np\nY\nj=1\nNj\u22121\nY\ni=0\n(\u03bb/p + i).\nNow, the class [z] contains exactly one element \u02c6z with the following properties\n\u2022 \u02c6z1 = 1,\n\u2022 \u02c6zk \u2264max(zj,j < k) + 1 for all k > 1.\nThis means that the kth label is either one of those already appearing in (\u02c6z1,..., \u02c6zk\u22121)\nor the next integer in the enumeration. We will call such a \u02c6z admissible. If we assume\nthat z is admissible in the expression of \u00af\u03c0, we can write\n\u00af\u03c0([z]) =\nQp0\nj=1\n\u0012\n\u03bb(1 \u2212j/p)QNj\u22121\ni=1 (\u03bb/p + i)\n\u0013\n\u03bb(\u03bb + 1)...(\u03bb + N \u22121)\n.\nIf one takes the limit p \u2192\u221ein this expression, one still gets a probability distribu-\ntion on admissible labelings, namely\n\u00af\u03c0([z]) =\n\u03bbp0 Qp0\nj=1(Nj \u22121)!\n\u03bb(\u03bb + 1)...(\u03bb + N \u22121).\n(19.12)\nRecall that, in this equation, p0 is a function of z, equal, for admissible labelings, to\nthe largest j such that Nj > 0.\nThe probability \u00af\u03c0 is generated by the following sampling scheme, called the\nPolya urn process simulating admissible labelings.\nAlgorithm 19.12 (Polya Urn)\n1 Initialize k = 1, z1 = 1, j = 1. Let N1 = 1\n2 At step k, assume that z1,...,zk have been generated, with associated number of\nclusters equal to j and N1,...,Nj elements per cluster. Generate zk+1 such that\nzk+1 =\n\uf8f1\uf8f4\uf8f4\uf8f4\uf8f4\uf8f2\uf8f4\uf8f4\uf8f4\uf8f4\uf8f3\ni\nwith probability\nNi\n\u03bb + k , for i = 1,...,j\nj + 1\nwith probability\n\u03bb\n\u03bb + k\n(19.13)\n490\nCHAPTER 19. CLUSTERING\n3 If zk+1 = i \u2264j, then replace Ni by Ni + 1, k by k + 1.\n4 If zk+1 = j + 1, let Nj+1 = 1, replace j by j + 1 and k by k + 1.\n5 If k < N, return to step 2, otherwise, stop.\nUsing this prior, the complete model for the distribution of the observed data is\nL(z,\u03b8,x) =\n\u03bbp0 Qp0\nj=1(Nj \u22121)!\n\u03bb(\u03bb + 1)...(\u03bb + N \u22121)\np0\nY\nj=1\n\u03c8(\u03b8j)\nN\nY\nk=1\n\u03d5(xk|\u03b8zk)\nRecall that, in this expression, z is restricted to the set of admissible labelings. We\nalso note that admissible labelings are in one-to-one correspondence with the par-\ntitions of {1,...,N}, so that the latent variable z in this expression can also be inter-\npreted as representing a random partition of this set.\nDirichlet processes.\nAs we will see later, the expression of the global likelihood\nand the Polya urn model will suffice for us to develop non-parametric clustering\nmethods for a set of observations x1,...,xN. However, this model is also associated\nto an important class of random probability distributions (i.e., random variables\ntaking values in some set of probability distributions) called Dirichlet processes for\nwhich we provide a brief description.\nThe distribution in (19.12) was obtained by passing to the limit from a model\nthat first generates p numbers \u03b11,...,\u03b1p, then generates the labels z1,...,zN \u2208{1,...,p}\nidentified modulo relabeling. This distribution can also be defined directly, by first\ndefining an infinity of positive numbers (\u03b1j,j \u22651) such that P\u221e\ni=1 \u03b1i = 1, followed by\nthe generation of random labels Z1,...,ZN such that P(Zk = j) = \u03b1j, followed once\nagain with an identification up to relabeling.\nThe distribution of \u03b1 that leads to the Polya urn is called the stick breaking process.\nThis process is such that\n\u03b1j = Uj\nj\u22121\nY\ni=1\n(1 \u2212Ui)\nwhere U1,U2,... is a sequence of i.i.d. variables following a Beta(1,\u03bb) distribution,\ni.e., with p.d.f. \u03bb(1 \u2212u)\u03bb\u22121 for u \u2208[0,1]. The stick breaking interpretation comes\nfrom the way \u03b11,\u03b12,... can be simulated: let \u03b11 \u223cBeta(1,\u03bb); given \u03b11,...,\u03b1j\u22121, let\n\u03b1j = (1 \u2212\u03b11 \u2212\u00b7\u00b7\u00b7 \u2212\u03b1j\u22121)Uj where Uj \u223cBeta(1,\u03bb) and is independent from the past.\nEach step can be thought of as breaking the remaining length, (1 \u2212\u03b11 \u2212\u00b7\u00b7\u00b7 \u2212\u03b1j\u22121),\nof an original stick of length 1 using a beta-distributed variable, Uj. This process\nleads to the distribution (19.12) over admissible distributions, i.e., if \u03b1 is generated\naccording to the stick breaking process, and Z1,...,ZN are independent, each such\n19.7. BAYESIAN CLUSTERING\n491\nthat P(Zk = j) = \u03b1j, then the probability that (Z1,...,ZN) is identical, after relabeling,\nto the admissible configuration z is given by (19.12). (We skip the proof of this result,\nwhich is not straightforward.)\nNow, take a realization \u03b1 = (\u03b11,\u03b12,...) of the stick-breaking process, and inde-\npendent realizations \u03b7 = (\u03b71,\u03b71,...) drawn according to the p.d.f. \u03c8. Define\n\u03c1 =\n\u221e\nX\nj=1\n\u03b1j\u03b4\u03b7j .\n(19.14)\nFor any realization of \u03b1 and of \u03b7, \u03c1 is a probability distribution on the parameter\nspace \u0398 (in which one chooses \u03b7i with probability \u03b1i). Since \u03b1 and \u03b7 are both random\nvariables, this defines a random variable \u03c1 with values in the space of probability\nmeasures on \u0398.\nThis process has the following characteristic property. For any family V1,...,Vk \u2282\n\u0398 forming a partition of that set, the random variable (\u03c1(U1),...,\u03c1(Uk)) follows a\nDirichlet distribution with parameters\n \n\u03bb\nZ\nU1\n\u03c8 d\u03b7,...,\u03bb\nZ\nU1\n\u03c8 d\u03b7\n!\n.\nThis is the definition of a Dirichlet process with parameters (\u03bb,\u03c8), or, simply, with\nparameter \u03bb\u03c8. Conversely, one can also show that any Dirichlet process can be de-\ncomposed as in (19.14) where \u03b1 is a stick-breaking process and \u03b7 independent real-\nizations of \u03c8.\nMonte-Carlo simulation.\nThe joint distribution of labels, parameters and observed\nvariables can also be deduced from (19.12), with a joint p.d.f. given by\n\u03bbp0\u22121 Qp0\nj=1(Nj \u22121)!\n(\u03bb + 1)\u00b7\u00b7\u00b7(\u03bb + N \u22121)\np0\nY\nj=1\n\u03c8(\u03b7j)\nN\nY\nk=1\n\u03d5(xk|\u03b7zk).\n(19.15)\nThe forward simulation of this distribution is a straightforward extension of Algo-\nrithm 19.12, namely:\nAlgorithm 19.13\n1 Initialize k = 1, z1 = 1, j = 1. Let N1 = 1.\n2 Sample \u03b71 \u223c\u03c8 and x1 \u223c\u03d5(\u00b7|\u03b71).\n3 At step k, assume that z1,...,zk has been generated, with associated number of\nclusters equal to j and N1,...,Nj elements per cluster. Generate zk+1 such that\nzk+1 =\n\uf8f1\uf8f4\uf8f4\uf8f4\uf8f4\uf8f2\uf8f4\uf8f4\uf8f4\uf8f4\uf8f3\ni\nwith probability\nNi\n\u03bb + k , for i = 1,...,j\nj + 1\nwith probability\n\u03bb\n\u03bb + k\n492\nCHAPTER 19. CLUSTERING\n4 If zk+1 = i \u2264j, sample xk+1 \u223c\u03d5(\u00b7|\u03b7i). Replace Ni by Ni + 1, k by k + 1.\n5 If zk+1 = j + 1, let Nj+1 = 1, sample \u03b7j+1 \u223c\u03c8 and xk+1 \u223c\u03d5(\u00b7|\u03b7j+1). Replace j by\nj + 1 and k by k + 1.\n6 If k < N, return to step 2, otherwise, stop.\nThis algorithm cannot be used, of course, to sample from the conditional distri-\nbution of Z and \u03b7 given X = x, and Markov-chain Monte-Carlo must be used for this\npurpose. In order to describe how Gibbs sampling may be applied to this problem,\nwe use the fact that, as previously remarked, using admissible labelings z is equiv-\nalent to using partitions A = (A1,...,Ap0) of {1,...,N}, and we will use the latter\nformalism to describe the algorithm. We will also use the notation \u03b7A to denote the\nparameter associated to A \u2208A so our new notation for the variables is (A,\u03b7) where\nA is a partition of {1,...,N} and \u03b7 is a collection (\u03b7A,A \u2208A) with \u03b7A \u2208\u0398. Given this,\nwe want to sample from a conditional p.d.f.\n\u03a6(A,\u03b7|x) \u221d\u03bb|A|\u22121 Q\nA\u2208A(|A| \u22121)!\n(\u03bb + 1)\u00b7\u00b7\u00b7(\u03bb + N \u22121)\nY\nA\u2208A\n\u03c8(\u03b7A)\nY\nk\u2208A\n\u03d5(xk|\u03b7A).\n(19.16)\nAs an additional notation, given a partition A and an index k \u2208{1....,N}, we let Ak\ndenote the set A in A that contains k.\nThe following points are relevant for the design of the sampling algorithm.\n(1) The conditional distribution of \u03b7 given A and the training data is proportional\nto\nY\nA\u2208A\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\u03c8(\u03b7A)\nY\nk\u2208A\n\u03d5(xk|\u03b7A)\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\nThis shows that the parameters \u03b7A,A \u2208A are independent of each other, with \u03b7A\nfollowing a distribution proportional to\n\u03b7 7\u2192\u03c8(\u03b7)\nY\nk\u2208Aj\n\u03d5(xk|\u03b7).\nSampling from this distribution generally offers no special difficulty, especially if\nthe prior \u03c8 is conjugate to \u03d5. Importantly, one does not need to sample exactly from\n\u03b7A, and it is often more convenient to separate \u03b7A into several components (such as\nmean and variance for mixtures of Gaussian) and sample from them alternatively,\ncreating another level of Gibbs sampling.\n(2) We now consider the issue of updating A. We will use for this purpose the\nformalism of Algorithm 12.2. In particular, for each k \u2208{1,...,N}, we associate to\n19.7. BAYESIAN CLUSTERING\n493\nthe variable (A,\u03b7) the pair (A(k),\u03b7(k)), where A(k) is the partition of {1,...,N} \\ {k}\nformed by the sets A(k) = A \\ {k} and \u03b7(k)\nA = \u03b7A, unless A = {k}, in which case the set\nand the corresponding \u03b7A are dropped.\nWe can write \u03a6(A,\u03b7|x) in the form\n\u03a6(A,\u03b7|x) \u221dq(Ak,\u03b7Ak)\u03d5(xk|\u03b7Ak)\u03bb|A(k)|\u22121 Q\nB\u2208A(k)(|B| \u22121)!\n(\u03bb + 1)\u00b7\u00b7\u00b7(\u03bb + N \u22121)\nY\nB\u2208A(k)\n\u03c8(\u03b7B)\nY\nl\u2208B\n\u03d5(xl|\u03b7B)\n(19.17)\nwith\nq(A,\u03b8) =\nX\nB\u2208A(k)\n|B|1A=B\u222a{k} + \u03bb\u03c8(\u03b8)1A={k}\nPartitions A\u2032 that are consistent with A(k) allocate k to one of the clusters in A(k) or\ncreate a new cluster with a new parameter \u03b7\u2032\nk. If one replaces (A,\u03b7) by (A\u2032,\u03b7\u2032), only\nthe first two terms in (19.17) will be affected, so that the conditional probability of\nA\u2032 given A(k) is proportional to q(A\u2032\nk,\u03b7A\u2032\nk)\u03d5(xk|\u03b7A\u2032\nk) and given by\n\uf8f1\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f2\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f3\n|B|\u03d5(xk|\u03b7B)\nC1 + \u03bbC2\nif A\u2032\nk = B \u222a{k},\u03b7\u2032\nB = \u03b7B,B \u2208A(k)\n\u03bb\u03d5(xk|\u03b7\u2032\nk)\u03c8(\u03b7\u2032\nk)\nC1 + \u03bbC2\nif A\u2032\nk = {k},\nwhere\nC1 =\nX\nB\u2208Ak\n|B|\u03d5(xk|\u03b7B)\nand\nC2 =\nZ\n\u0398\n\u03d5(xk|\u03b8)\u03c8(\u03b8)d\u03b8.\nConcretely, this means that one first decides to allocate k to a set B in A(k) with\nprobability |B|\u03d5(xk|\u03b7B)/(C1+\u03bbC2) and to create a new set with probability \u03bbC2/(C1+\n\u03bbC2). If a new set is created, then the associated parameter \u03b7\u2032\n{k} is sampled according\nto the p.d.f. \u03d5(xk|\u03b8)\u03c8(\u03b8/C2.\n(3) However, sampling using this conditional probability requires the computa-\ntion of the integral C2, which can represent a significant computational burden,\nsince this has to be done many times in a Gibbs sampling algorithm. A modification\nof this algorithm, introduced in Neal [141], avoids this computation by adding new\nauxiliary variables at each step of the computation. These variables are m parameters\n\u03b7\u2217\n1,...,\u03b7\u2217\nm \u2208\u0398 where m is a fixed integer. To define the joint distribution of A,\u03b7,\u03b7\u2217,\none lets the marginal distribution of (A,\u03b7) be given by (19.16) and conditionally to\nA,\u03b7, let \u03b7\u2217\n1,...,\u03b7\u2217\nm be:\n(i) independent with density \u03c8 if |Ak| > 1;\n(ii) such that \u03b7\u2217\nj = \u03b7Ak and the other m \u22121 starred parameters are independent\nwith distribution \u03c8, where j is randomly chosen in {1,...,m} if Ak = {k}.\n494\nCHAPTER 19. CLUSTERING\nWith this definition, the joint conditional distribution of (A,\u03b7,\u03b7\u2217) takes the form\nb\u03a6(A,\u03b7,\u03b7\u2217|x) \u221d\u02c6q(Ak,\u03b7Ak,\u03b7\u2217)\u03d5(xk|\u03b7Ak)\n\u03bb|A(k)|\u22121 Q\nB\u2208A(k)(|B| \u22121)!\n(\u03bb + 1)\u00b7\u00b7\u00b7(\u03bb + N \u22121)\nY\nB\u2208A(k)\n\u03c8(\u03b7B)\nY\nl\u2208B\n\u03d5(xl|\u03b7B)\n(19.18)\nwith\n\u02c6q(A,\u03b8,\u03b7\u2217\n1,...,\u03b7\u2217\nm) =\nX\nB\u2208A(k)\n|B|1\u03b8=\u03b7B,A=B\u222a{k}\nm\nY\nj=1\n\u03c8(\u03b7\u2217\nj) + \u03bb\nm\nm\nX\nj=1\n1\u03b8=\u03b7\u2217\nj ,A={k}\u03c8(\u03b8)\nm\nY\ni=1,i,j\n\u03c8(\u03b7\u2217\ni ).\nNote that b\u03a6 depends on k, so that the definition of the auxiliary variables will change\nat each step of Gibbs sampling. The conditional distribution, for b\u03a6, of A\u2032,\u03b7\u2032 given\nA(k),\u03b7(k),\u03b7\u2217is such that\n\u2022 A\u2032\nk = B \u222a{k} and \u03b7\u2032\nA\u2032\nk = \u03b7B with probability |B|\u03d5(xk|\u03b7B)/C, for B \u2208A(k).\n\u2022 A\u2032\nk = {k} and \u03b7A\u2032\nk = \u03b7\u2217\nj with probability (\u03bb/m)\u03d5(xk|\u03b7\u2217\nj)/C, j = 1,...,m.\nThe constant C is given by\nC =\nX\nB\u2208Ak\n|B|\u03d5(xk|\u03b7B) + \u03bb\nm\nm\nX\nj=1\n\u03d5(xk|\u03b7\u2217\nj)\nand is therefore easy to compute.\nWe can now summarize this discussion with Neal\u2019s version of the Gibbs sampling\nalgorithm.\nAlgorithm 19.14 (Neal)\nInitialize the algorithm with some arbitrary partition and parameters (A,\u03b7) (for ex-\nample, generated using the Dirichlet prior). Use the same notation to denote these\nvariables at the end of the previous iteration of the algorithm. The next iteration is\nthen run as follows.\n(1) For k = 1,...,N, reallocate k to a cluster as follows.\n(i) Form the new family of sets A(k) and labels \u03b7(k) by removing k from the parti-\ntion A.\n(ii) If |Ak| > 1, generate m variables \u03b7\u2217\n1,...,\u03b7\u2217\nm according to \u03c8. If Ak = {k}, generate\nonly m \u22121 such variables and let the last one be equal to \u03b7Ak.\n19.7. BAYESIAN CLUSTERING\n495\n(iii) Allocate k to a new cluster A\u2032 with parameter \u03b7\u2032\nA\u2032 according to probabilities\nproportional to\n\uf8f1\uf8f4\uf8f4\uf8f4\uf8f2\uf8f4\uf8f4\uf8f4\uf8f3\n|B|\u03d5(xk|\u03b7(k)\nB ) if A\u2032 = B \u222a{k} and \u03b7\u2032\nA\u2032 = \u03b7(k)\nB\n\u03bb\nm\u03d5(xk|\u03b7\u2217\nj) if A = {k} and \u03b7\u2032\nA\u2032 = \u03b7\u2217\nj,j = 1,...,m\n(2) For A \u2208A, update \u03b7A,A \u2208A according to the distribution proportional to\n\u03c8(\u03b7)\nY\nk\u2208A\n\u03d5(xk|\u03b7)\neither directly, or via one step of Gibbs sampling visiting each of the variables that\nconstitute \u03b7A.\n(3) Loop a sufficient number of times over the previous two steps.\nAfter running this algorithm, the set of clusters should be finalized by using\nstatistics computed along the simulation, as discussed after Algorithm 19.10.\nFull example: Mixture of Gaussian.\nTo conclude this section, we summarize the\nMonte-Carlo sampling algorithm for mixtures of Gaussian using a non-parametric\nBayesian prior. Here, \u03b7 \u2208\u0398 is the center c \u2208Rd, with prior distribution \u03c8 = N (0,\u03c42IdRd).\nThe previous algorithm must be modified because an additional parameter \u03c32 is\nshared by all classes, with prior given by an inverse gamma distribution with pa-\nrameters u and v. The conditional distribution of the data is \u03d5(x|c,\u03c3) \u223cN (c,\u03c32IdRd).\nAlgorithm 19.15 (Gibbs sampling for non-parametric mixture of Gaussian)\n(1) Initialize the algorithm with some arbitrary partition and parameters (A,\u03b7).\n(2) For k = 1,...,N, reallocate k to a cluster as follows.\n(i) Form the new family of sets A(k) and labels \u03b7(k) by removing k from the parti-\ntion A.\n(ii) If |Ak| > 1, generate m variables c\u2217\ni, i = 1,...,m independently with c\u2217\ni \u223cN (0,\u03c42IdRd).\nIf Ak = {k}, generate only m \u22121 such pairs of variables and let the last one be equal\nto cAk.\n(iii) Allocate k to a new cluster A\u2032 with parameter c\u2032\nA\u2032 according to probabilities\nproportional to\n\uf8f1\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f2\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f3\n|B|exp\n\u0010\n\u2212|xk \u2212c(k)\nB |\n2\u03c32\n\u0011\nif A\u2032 = B \u222a{k} and c\u2032\nA\u2032 = c(k)\nB\n\u03bb\nm exp\n\u0010\n\u2212|xk \u2212c\u2217\nB|\n2\u03c32\n\u0011\nif A = {k} and c\u2032\nA\u2032 = c\u2217\nj,j = 1,...,m.\n496\nCHAPTER 19. CLUSTERING\n(3) Simulate a new value of \u03c32 according to an inverse gamma distribution with\nparameters u + dN/2 and v + 1\n2\nPN\nk=1 |xk \u2212cAk|2.\n(4) Simulate new values for cA,A \u2208A independently, sampling cA according to a\nGaussian distribution with mean (1 + \u03c32/(Nj\u03c42))\u22121 \u00afxA and variance (|A|/\u03c32 + 1/\u03c42)\u22121,\nwhere\n\u00afxA = 1\n|A|\nX\nk\u2208A\nxk.\nChapter 20\nDimension Reduction and Factor Analysis\n20.1\nPrincipal component analysis\n20.1.1\nGeneral Framework\nFactor analysis aims at representing potentially high-dimensional data as functions\nof a (generally) small number of \u201cfactors,\u201d with a representation taking the general\nform\nX = \u03a6(Y,\u03b8) + residual,\n(20.1)\nwhere X is the observation, Y provide the factors and \u03a6 is a function parametrized\nby \u03b8. A factor analysis model must therefore specify \u03a6 (often, a linear function of Y),\nadd hypotheses on Y (such as its dimension, or properties of its distribution) and on\nthe residuals. The transformation \u03a6 is estimated from training data, but, ideally, the\nmethod should also provide an algorithm that infers Y from a new observation of X.\nMost of the time, Y is small dimensional so that the model also implies a reduction\nof dimension.\nWe start our discussion with principal component analysis (or PCA). This meth-\nods can be characterized in multiple ways, and we introducing through the angle of\ndata approximation. In the following, the random variable X takes values in a finite-\nor infinite-dimensional inner-product space H. We will denote, as usual, by \u27e8. , .\u27e9H\nthe product in this space.\nAssume that N independent realization of X, denoted x1,...,xN, are observed,\nforming our training set T. Our goal is to obtain a small-dimensional representation\nof these data, while loosing a minimal amount of relevant information. PCA, is the\nsimplest and most commonly used approach developed for this purpose.\nIf V is a finite-dimensional subspace of H, we denote by PV (y) the orthogonal\nprojection of y \u2208H on V , i.e., the element \u03be \u2208V such that \u2225y \u2212\u03be\u22252\nH is minimal\n497\n498\nCHAPTER 20. DIMENSION REDUCTION AND FACTOR ANALYSIS\n(see section 6.4). Recall that this orthogonal projection if characterized by the two\nproperties: (i) PV (y) \u2208V and (ii) (y \u2212PV (y)) \u22a5V .\nGiven a target dimension p, PCA determines a p-dimensional subspace of H, say,\nV and a point c \u2208H, such that, letting\nRk = xk \u2212c \u2212PV (xk \u2212c)\nfor k = 1,...,N, the residual sum of squares\nS =\nN\nX\nk=1\n\u2225Rk\u22252\nH\n(20.2)\nis as small as possible.\nAn optimal choice for c is c = x = PN\nk=1 xk/N. Indeed, using the linearity of the\northogonal projection, we have\nS =\nN\nX\nk=1\n\u2225xk \u2212PV (xk) \u2212(c \u2212PV (c))\u22252\nH\n=\nN\nX\nk=1\n\u2225xk \u2212PV (xk) \u2212(x \u2212PV (x))\u22252\nH + N\u2225x \u2212PV (x) \u2212(c \u2212PV (c))\u22252\nH.\nGiven this, there would be no loss of generality in assuming that all xk\u2019s have been\nreplaced by xk \u2212x and taking c = 0. While this is often done in the literature, there\nare some advantages (especially when discussing kernel methods) in keeping the\naverage explicit in the notation, as we will continue to do.\nIntroducing an orthonormal basis (e1,...,ep) of V , one has\nPV (xk \u2212x) =\np\nX\ni=1\n\u03c1k(i)ei\nwith \u03c1ki = \u27e8xk \u2212x , ei\u27e9H. One can then reformulate the problem in terms of (e1,...,ep),\nwhich must minimize\nS\n=\nN\nX\nk=1\n\u2225xk \u2212x \u2212\np\nX\ni=1\n\u27e8xk \u2212x , ei\u27e9ei\u22252\nH\n=\nN\nX\nk=1\n\u2225xk \u2212x\u22252\nH \u2212\np\nX\ni=1\nN\nX\nk=1\n\u27e8xk \u2212x , ei\u27e92\nH.\n20.1. PRINCIPAL COMPONENT ANALYSIS\n499\nFor u,v \u2208H, define\n\u27e8u , v\u27e9T = 1\nN\nN\nX\nk=1\n\u27e8xk \u2212x , u\u27e9H\u27e8xk \u2212x , v\u27e9H\nand \u2225u\u2225T = \u27e8u , u\u27e91/2\nT\n(the index T refers to the fact that this norm is associated with\nthe training set). This provides a new quadratic form on H. The formula above\nshows that minimizing S is equivalent to maximizing\np\nX\ni=1\n\u2225ei\u22252\nT\nsubject to the constraint that (e1,...,ep) is orthonormal in H.\nLet us consider a slightly more general problem. If H is a separable Hilbert\nspace1 and \u00b5 is a square-integrable probability measure on H, such that\nZ\nH\n\u2225x\u22252\nH d\u00b5(x) < \u221e,\none can define m =\nR\nH xd\u00b5(x) and \u03c32\n\u00b5 =\nR\nH \u2225x \u2212m\u22252\nHd\u00b5. One can then define the\ncovariance bilinear form\n\u0393\u00b5(u,v) =\nZ\nH\n\u27e8u , x \u2212m\u27e9H \u27e8v , x \u2212m\u27e9H d\u00b5(x),\nwhich satisfies \u0393\u00b5(u,v) \u2264\u03c32\n\u00b5\u2225u\u2225H \u2225v\u2225H.\nWith this notation, we have\n\u27e8u , v\u27e9T = \u0393\u02c6\u00b5T (u,v),\nwhere \u02c6\u00b5T = (1/N)PN\nk=1 \u03b4xk is the empirical measure (and in that case m = \u00afx). We can\ntherefore generalize the PCA problem by considering the maximization of\np\nX\nk=1\n\u0393\u00b5(ek,ek)\n(20.3)\nover all orthonormal families (e1,...,ep) in H.\nWhen \u00b5 is square integrable, the associated operator, A\u00b5 defined by\n\u27e8u , A\u00b5v\u27e9H = \u0393\u00b5(u,v)\n(20.4)\n1A Hilbert space is an inner-product space which is complete for its norm. A separable Hilbert\nspace must have a dense countable subset, which, in particular, implies that it has orthonormal bases.\n500\nCHAPTER 20. DIMENSION REDUCTION AND FACTOR ANALYSIS\nfor all u,v \u2208H, is a Hilbert-Schmidt operator [205]. Such an operator can, in partic-\nular, be diagonalized in an orthonormal basis of H, i.e., there exists an orthonormal\nbasis, (f1,f2,...) of H such that A\u00b5fi = \u03bb2\ni fi for a non-increasing sequence of eigenval-\nues (with \u03bb1 \u2265\u03bb2 \u2265\u00b7\u00b7\u00b7 \u22650) such that\n\u03c32\n\u00b5 =\n\u221e\nX\nk=1\n\u03bb2\ni .\nThe main statement of the following result is in finite dimensions, a simple ap-\nplication of corollary 2.4. We here give a direct proof that also works in infinite\ndimensions.\nTheorem 20.1 Let (f1,f2,...) be an orthonormal basis of eigenvectors of A\u00b5 with associ-\nated eigenvalues \u03bb2\n1 \u2265\u03bb2\n2 \u2265\u00b7\u00b7\u00b7 \u22650. Then an orthonormal family (e1,...,ep) in H maxi-\nmizes (20.3) if and only if,\nspan(fj : \u03bb2\nj > \u03bb2\np) \u2282span(e1,...,ep) \u2282span(fj : \u03bb2\nj \u2265\u03bb2\np).\n(20.5)\nIn particular f1,...,fp always provide a solution and span(e1,...,ep) = span(f1,...,fp) for\nany other solution as soon as \u03bb2\np > \u03bb2\np+1.\nDefinition 20.2 When \u00b5 = \u02c6\u00b5T , the vectors (f1,...,fp) are called (with some abuse when\neigenvalues coincide) the first p principal components of the training set (x1,...,xN).\nProof If (e1,...,ep) is an orthonormal family in H, let\nF(e1,...,ep) =\np\nX\nk=1\n\u0393\u00b5(ek,ek).\nNote that F(f1,...,fp) = \u03bb2\n1 + \u00b7\u00b7\u00b7 + \u03bb2\np. Write ek = P\u221e\nj=1 \u03b1(j)\nk fj (so that \u03b1(j)\nk\n= \u27e8fj , ek\u27e9H).\nThese coefficients satisfy P\u221e\nj=1 \u03b1(j)\nk \u03b1(j)\nl\n= 1 if k = l and 0 otherwise. Then\n\u0393\u00b5(ek,ek) =\n\u221e\nX\nj=1\n\u03bb2\nj (\u03b1(j)\nk )2.\n20.1. PRINCIPAL COMPONENT ANALYSIS\n501\nWe have\nF(e1,...,ep) =\np\nX\nk=1\n\u221e\nX\nj=1\n\u03bb2\nj (\u03b1(j)\nk )2\n=\np\nX\nk=1\np\nX\nj=1\n\u03bb2\nj (\u03b1(j)\nk )2 +\np\nX\nk=1\n\u221e\nX\nj=p+1\n\u03bb2\nj (\u03b1(j)\nk )2\n\u2264\np\nX\nk=1\np\nX\nj=1\n\u03bb2\nj (\u03b1(j)\nk )2 +\np\nX\nk=1\n\u221e\nX\nj=p+1\n\u03bb2\np+1(\u03b1(j)\nk )2\n=\np\nX\nj=1\n(\u03bb2\nj \u2212\u03bb2\np+1)\np\nX\nk=1\n(\u03b1(j)\nk )2 + p\u03bb2\np+1.\nLet P denote the orthogonal projection operator from H to span(e1,...,ep). We have,\nfor any h \u2208H, \u2225Ph\u22252\nH \u2264\u2225h\u22252\nH with equality if and only if h \u2208span(e1,...,ep). Applying\nthis to h = fj, with P(fj) = Pp\nk=1 \u03b1(j)\nk ek, we get Pp\nk=1(\u03b1(j)\nk )2 \u22641 with equality if and only\nif fj \u2208span(e1,...,ep).\nAs a consequence, the previous upper bound on F(e1,...,ep) implies\nF(e1,...,ep) \u2264\np\nX\nj=1\n\u03bb2\nj .\nThis upper bound is attained at (e1,...,ep) = (f1,...,fp), which is therefore a maxi-\nmizer. Also, inspecting the argument above, we see that F(e1,...,ep) < \u03bb2\n1 + \u00b7\u00b7\u00b7 + \u03bb2\np\nunless\n(a) for all k \u2264p and j \u2265p + 1: \u03b1(j)\nk = 0 if \u03bb2\nj > \u03bb2\np+1, and\n(b) for all j \u2264p: Pp\nk=1(\u03b1(j)\nk )2 = 1 unless \u03bb2\nj = \u03bb2\np+1.\nCondition (a) implies that span(e1,...,ep) \u2282span(fj : \u03bb2\nj \u2264\u03bb2\np+1). If \u03bb2\np = \u03bb2\np+1, the\ninclusion span(e1,...,ep) \u2282span(fj : \u03bb2\nj \u2264\u03bb2\np) therefore holds. If \u03bb2\np < \u03bb2\np+1, condition\n(b) requires Pp\nk=1(\u03b1(j)\nk )2 = 1 for all j \u2264p, which implies fj \u2208span(e1,...,ep) for j \u2264p,\nso that span(e1,...,ep) = span(f1,...,fp) and the inclusion also hold.\nCondition (b) always requires Pp\nk=1(\u03b1(j)\nk )2 = 1, hence fj \u2208span(f1,...,fp), when\n\u03bbj < \u03bbp, showing that span(fj : \u03bb2\nj < \u03bb2\np) \u2282span(e1,...,ep). Equation (20.5) therefore\nalways holds for (e1,...,ep) such that F(e1,...,ep) = \u03bb2\n1 + \u00b7\u00b7\u00b7 + \u03bb2\np. Furthermore, condi-\ntions (a) and (b) always hold for any orthonormal family that satisfy (20.5), showing\nthat any such solution is optimal.\n\u25a0\n502\nCHAPTER 20. DIMENSION REDUCTION AND FACTOR ANALYSIS\nNotice that the optimal S in (20.2) is such that\nS = N\nX\ni>p\n\u03bb2\ni .\nRemark 20.3 The interest of discussing PCA associated with a covariance operator\nfor a square integrable measure (in which case it is often called a Karhunen-Loeve\n(KL) expansion) is that this setting is often important when discussing infinite-\ndimensional random processes (such as Gaussian random fields). Moreover, these\noperators quite naturally provide asymptotic versions of sample-based PCA. In-\nteresting issues, that are part of functional data analysis [158], address the design\nof proper estimation procedures to obtain converging estimators of KL expansions\nbased on finite samples for stochastic processes in infinite-dimensional spaces.\n\u2666\n20.1.2\nComputation of the principal components\nSmall dimension.\nAssume that H has finite dimension, d, i.e., H = Rd, and repre-\nsent x1,...,xN \u2208Rd as column vectors. Let the inner product on H be associated to a\npositive-definite symmetric matrix Q:\n\u27e8u , v\u27e9H = uT Qv.\nIntroduce the covariance matrix of the data\n\u03a3T = 1\nN\nN\nX\nk=1\n(xk \u2212x)(xk \u2212x)T ,\nWrite AT = A \u02c6\u00b5T , for short, in (20.4). We have:\n\u27e8u , AT v\u27e9H = 1\nN\nN\nX\nk=1\n(uT Q(xk \u2212x))(vT Q(xk \u2212x))\n= 1\nN\nN\nX\nk=1\nuT Q(xk \u2212x)(xk \u2212x)T Qv\n= \u27e8u , \u03a3T Qv\u27e9H ,\nso that AT = \u03a3T Q.\nThe eigenvectors, f , of AT are such that Q1/2f are eigenvectors of the symmetric\nmatrix Q1/2\u03a3T Q1/2, which shows that they form an orthogonal system in H, which\nwill be orthonormal if the eigenvectors are normalized so that f T Qf = 1. Equiva-\nlently, they solve the generalized eigenvalue problem Q\u03a3T Qf = \u03bb2Qf , which may\nbe preferred numerically to diagonalizing the non-symmetric matrix \u03a3T Q.\n20.1. PRINCIPAL COMPONENT ANALYSIS\n503\nRemark 20.4 Sometimes, the metric is specified by giving Q\u22121 instead of Q (or Q\u22121\nis easy to compute). Then, one can directly solve the generalized eigenvalue problem\n\u03a3T \u02dcf = \u03bb2Q\u22121 \u02dcf and set f = Q\u22121 \u02dcf . The normalization f T Qf = 1 is then obtained by\nnormalizing \u02dcf so that \u02dcf T Q\u22121 \u02dcf = 1.\n\u2666\nRemark 20.5 The \u201cstandard\u201d version of PCA applies this computation using the Eu-\nclidean inner product, with Q = IdRd, and the principal components are the eigen-\nvectors of the covariance matrix of T associated with the largest eigenvalues.\n\u2666\nLarge dimension.\nIt often happens that the dimension of H is much larger than the\nnumber of observations, N. In such a case, the previous approach is quite inefficient\n(especially when the dimension of H is infinite!) and one should proceed as follows.\nReturning to the original problem, one can remark that there is no loss of gener-\nality in assuming that V is a subspace of W := span{x1 \u2212x,...,xN \u2212x}. Indeed, letting\nV \u2032 = PW(V ) (the projection of V on W), we have, for \u03be \u2208W,\n\u2225\u03be \u2212PV \u03be\u22252\nH = \u2225\u03be\u22252\nH \u22122\u27e8\u03be , PV \u03be\u27e9H + \u2225PV \u03be\u22252\nH\n= \u2225\u03be\u22252\nH \u22122\u27e8PW\u03be , PV \u03be\u27e9H + \u2225PV \u03be\u22252\nH\n= \u2225\u03be\u22252\nH \u22122\u27e8\u03be , PWPV \u03be\u27e9H + \u2225PV \u03be\u22252\nH\n\u2265\u2225\u03be\u22252\nH \u22122\u27e8\u03be , PWPV x\u27e9H + \u2225PWPV \u03be\u22252\nH\n= \u2225\u03be \u2212PWPV \u03be\u22252\nH\n\u2265\u2225\u03be \u2212PV \u2032\u03be\u22252\nH .\nIn this computation, we have used the facts that PW\u03be = \u03be (since \u03be \u2208W), that \u2225PWPV \u03be\u2225H \u2264\n\u2225PV \u03be\u2225H, that PWPV \u03be \u2208V \u2032 and that PV \u2032(\u03be) is the best approximation of \u03be by an ele-\nment of V \u2032. This shows that (since xk \u2212x \u2208W for all k)\nN\nX\nk=1\n\u2225xk \u2212x \u2212PV (xk \u2212x)\u22252\nH \u2265\nN\nX\nk=1\n\u2225xk \u2212x \u2212PV \u2032(xk \u2212x)\u22252\nH\nwith V \u2032 a subspace of W of dimension less than p, proving the result. This computa-\ntion also shows that no improvement in PCA can be obtained by looking for spaces\nof dimension p \u2265dim(W) (with dim(W) \u2264N \u22121 because the data is centered).\nIt therefore suffices to look for f1,...,fp in the form\nfi =\nN\nX\nk=1\n\u03b1(i)\nk (xk \u2212x).\nfor some \u03b1(i)\nk , 1 \u2264k \u2264N,1 \u2264i \u2264p.\n504\nCHAPTER 20. DIMENSION REDUCTION AND FACTOR ANALYSIS\nWith this notation, we have \u27e8fi , fj\u27e9H = PN\nk,l=1 \u03b1(i)\nk \u03b1(j)\nl \u27e8xk \u2212x , xl \u2212x\u27e9H and\n\u27e8fi , fj\u27e9T = 1\nN\nN\nX\nl=1\n\u27e8fi , xl \u2212x\u27e9H\u27e8fj , xl \u2212x\u27e9H\n= 1\nN\nN\nX\nk,k\u2032=1\n\u03b1(i)\nk \u03b1(j)\nk\u2032\nN\nX\nl=1\n\u27e8xk \u2212x , xl \u2212x\u27e9H\u27e8xk\u2032 \u2212x , xl \u2212x\u27e9H.\nLet S be the Gram matrix of the centered data, formed by the inner products \u27e8xk \u2212x , xl \u2212x\u27e9H,\nfor k,l = 1,...,N. Let \u03b1(i) be the column vector with coordinates \u03b1(i)\nk , k = 1,...,N. We\nhave \u27e8fi , fj\u27e9H = (\u03b1(i))T S\u03b1(j) and \u27e8fi , fj\u27e9T = (\u03b1(i))T S2\u03b1(j)/N, which implies that, in this\nrepresentation, the operator AT is given by S/N. Thus, the previous simultaneous\northogonalization problem can be solved in terms of the \u03b1\u2019s by diagonalizing S and\ntaking the first eigenvectors, normalized so that (\u03b1(i))T S\u03b1(i) = 1. Let \u03bb2\nj , j = 1,...,N\nbe the eigenvalues of S/N (of which only the first min(d,N \u22121) may be non-zero).\nIn this representation, the decomposition of the projection of xk on the PCA basis is\ngiven by\nxk =\np\nX\nj=1\n\u03b2(j)\nk fj\nwith\n\u03b2(j)\nk = \u27e8xk \u2212x , fj\u27e9H =\nN\nX\nl=1\n\u03b1(j)\nl \u27e8xl \u2212x , xk \u2212x\u27e9H = N\u03bb2\nj \u03b1(j)\nk .\n20.2\nKernel PCA\nSince the previous computation only depended on the inner products \u27e8xk \u2212x , xl \u2212x\u27e9H,\nPCA can be performed in reproducing kernel Hilbert spaces, and the resulting method\nis called kernel PCA. In this framework, X may take values in any set R with a rep-\nresentation h : R \u2192H. The associated kernel, K(x,x\u2032) = \u27e8h(x) , h(x\u2032)\u27e9H, provides a\nclosed form expression of the inner products in terms of the original variables. The\nfeature function itself is most of the time unnecessary.\nThe kernel version of PCA consists in replacing xk \u2212x with h(xk) \u2212\u00afh where \u00afh is\nthe average feature. This leads to defining a \u201ccentered kernel:\u201d\nKc(x,x\u2032)\n=\n\u27e8h(x) \u2212\u00afh , h(x\u2032) \u2212\u00afh\u27e9H\n=\n\u27e8h(x) , h(x\u2032)\u27e9H \u2212\u27e8h(x) + h(x\u2032) , \u00afh\u27e9+\n\r\r\r\u00afh\n\r\r\r2\nH\n=\nK(xk,xl) \u22121\nN\nN\nX\nk=1\n(K(x,xk) + K(x\u2032,xk)) + 1\nN 2\nN\nX\nk,l=1\nK(xk,xl).\n20.2. KERNEL PCA\n505\nThen the Gram matrix in feature space is S with skl = Kc(xk,xl) and the computation\ndescribed in the previous section can be applied. Note that, if one denotes, as usual\nK = K(x1,...,xN) the matrix formed by kernel evaluations K(xk,xl), and if one lets\nP = IdRN \u22121N1N/N, then we have the simple matrix expression S = PKP.\nLetting \u03b1(1),...,\u03b1(p) \u2208RN be the first p eigenvectors of S, normalized so that\n(\u03b1(i))T S\u03b1(i) = 1, the principal directions are vectors in feature space given by (us-\ning the notation in the previous section in which the kth coordinate of \u03b1(i) is \u03b1(i)\nk )\nfi =\nN\nX\nk=1\n\u03b1(i)\nk (h(xk) \u2212\u00afh),\nand they are not computable when the features not known explicitly. However, a\nfew geometric features associated with these directions can be characterized using\nthe kernel only.\nConsider the line in feature space Di =\nn\u00afh + \u03bbfi,\u03bb \u2208R\no\n. Let \u2126i denote the points\nx \u2208R such that h(x) \u2208Di. Then x \u2208\u2126i if and only if h(x) coincides with its orthogonal\nprojection on Di, which is equivalent to\n\u27e8h(x) \u2212\u00afh , fi\u27e92\nH =\n\r\r\rh(x) \u2212\u00afh\n\r\r\r2\nH ,\nwhich can be expressed with the kernel as\nKc(x,x) \u2212\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\nN\nX\nk=1\n\u03b1(i)\nk Kc(x,xk)\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\n2\n= 0.\n(20.6)\nThis provides a nonlinear equation in x. In particular, \u2126i is generally nonlinear,\npossibly with several connected components. Note that, by definition, the difference\nin (20.6) is always non-negative, so that a way to visualize \u2126i is to compute its sub-\nlevel sets, i.e., the set of all x such that\nKc(x,x) \u2212\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\nN\nX\nk=1\n\u03b1(i)\nk Kc(x,xk)\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\n2\n\u2264\u03f5\nfor small \u03f5.\nSimilarly, the feature vector h(x) \u2212\u00afh belongs to the space generated by the first p\ncomponents if and only if\np\nX\ni=1\n\u27e8h(x) \u2212\u00afh , fi\u27e92\nH =\n\r\r\rh(x) \u2212\u00afh\n\r\r\r2\nH\n506\nCHAPTER 20. DIMENSION REDUCTION AND FACTOR ANALYSIS\ni.e.,\np\nX\ni=1\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\nN\nX\nk=1\n\u03b1(i)\nk Kc(x,xk)\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\n2\n= Kc(x,x).\nOne can also compute the finite-dimensional coordinates of h(x) in the PCA basis,\nand this computation is easier. The representation is\nx 7\u2192(u1(x),...,up(x))\nwith\nui = \u27e8h(x) \u2212\u00afh , fi\u27e9H =\nN\nX\nk=1\n\u03b1(i)\nk Kc(x,xk).\nThis provides an explicit nonlinear transformation that maps each data point x into\na p-dimensional point. This representation allows one to easily exploit the reduction\nof dimension.\n20.3\nStatistical interpretation and probabilistic PCA\nThere is a simple probabilistic interpretation of linear PCA. Assume that H = Rd\nwith the standard inner product and that X is a centered random vector with covari-\nance matrix \u03a3. Consider the problem that consists in finding a factor decomposition\nX =\np\nX\ni=1\nY (i)ei + R\nwhere Y = (Y (1),...,Y (p))T forms a p-dimensional centered vector, e1,...,ep is an or-\nthonormal system, and R is a random vector, independent of Y and as small as pos-\nsible, in the sense that E(|R|2) is minimal.\nOne can see that, in an optimal decomposition, one needs RT ei = 0 for all i,\nbecause one can always write\np\nX\ni=1\nY (i)ei + R =\np\nX\ni=1\n(Y (i) + RT ei)ei + R \u2212\np\nX\ni=1\nRT eiei .\nIf R is centered, then so is R \u2212Pp\ni=1 RT eiei and the latter provides a better solution\nsince |R \u2212Pp\ni=1 RT eiei| \u2264|R|. Also, there is no loss of generality in requiring that\n(Y (1),...,Y (p)) are uncorrelated, as this can always be obtained after a change of basis\nin span(e1,...,ep).\n20.3. STATISTICAL INTERPRETATION AND PROBABILISTIC PCA\n507\nAssuming this, we can write\nE(|X|2) =\np\nX\ni=1\nE((Y (i))2) + E(|R|2)\nwith Y (i) = eT\ni X. So, to minimize E(|R|2), one needs to maximize\np\nX\ni=1\nE((eT\ni X)2)\nwhich is equal to (letting \u03a3 be the covariance matrix of X)\np\nX\ni=1\neT\ni \u03a3ei.\nThe solution of this problem is given by the first p eigenvectors of \u03a3. PCA (with a\nEuclidean metric) exactly applies this procedure, with \u03a3 replaced by the empirical\ncovariance.\n\u201cProbabilistic PCA\u201d is based on a slightly different statistical model in which it is\nassumed that X can be decomposed as\nX =\np\nX\ni=1\n\u03bbiY (i)ei + \u03c3R,\nwhere R is a d dimensional standard Gaussian vector and Y = (Y (1),...,Y (p))T a p-\ndimensional standard Gaussian vector, independent of R. The main difference with\nstandard PCA is that the total variance of the residual, here d\u03c32, is a model param-\neter and not a quantity to minimize.\nIn addition to \u03c32, the model is parametrized by the coordinates of e1,...,ep and\nthe values of \u03bb1,...,\u03bbp. Introduce the d \u00d7 p matrix\nW = [\u03bb1e1,...,\u03bbpep].\nWe can rewrite this model in the form\nX = WY + \u03c32R\nwhere the parameters are W and \u03c32, with the constraint that W T W is a diagonal\nmatrix. As a linear combination of independent Gaussian random variables, X is\n508\nCHAPTER 20. DIMENSION REDUCTION AND FACTOR ANALYSIS\nGaussian with covariance matrix WW T + \u03c32Id. The log-likelihood of the observa-\ntions x1,...,xN therefore is\nL(W,\u03c3) = \u2212N\n2\n\u0012\nd log2\u03c0 + logdet(WW T + \u03c32Id) + trace((WW T + \u03c32Id)\u22121\u03a3T )\n\u0013\n(20.7)\nwhere \u03a3Y is the empirical covariance matrix of x1,...,xN. This function can be max-\nimized explicitly in W and \u03c3, as stated in the following proposition.\nProposition 20.6 Assume that the matrix \u03a3T is invertible. The log-likelihood in (20.7)\nis maximized by taking\n(i) W = [\u03bb1e1,...,\u03bbpep] where e1,...,ep are the eigenvectors of \u03a3T associated to the p\nlargest eigenvalues, and \u03bbi =\nq\n\u03b42\ni \u2212\u03c32, where \u03b42\ni is the eigenvalue of \u03a3 associated to ei;\n(ii) and\n\u03c32 =\n1\nd \u2212p\nd\nX\ni=p+1\n\u03b42\ni .\nProof We make the following change of variables: let \u03c12 = 1/\u03c32 and\n\u00b52\ni = 1\n\u03c32 \u2212\n1\n\u03bb2\ni + \u03c32.\nLet Q = [\u00b51e1,...,\u00b5pep]. We have\n(WW T + \u03c32Id)\u22121 = \u03c12Id \u2212QQT .\nTo see this, complete (e1,...,ep) into an orthonormal basis of Rd, letting ep+1,...,ed\ndenote the added vectors. Then\nWW T + \u03c32Id =\np\nX\ni=1\n(\u03bb2\ni + \u03c32)eieT\ni +\nd\nX\ni=p+1\n\u03c32eieT\ni\nso that\n(WW T + \u03c32Id)\u22121 =\np\nX\ni=1\n(\u03bb2\ni + \u03c32)\u22121eieT\ni +\nd\nX\ni=p+1\n\u03c3\u22122eieT\ni = \u03c12Id \u2212QQT .\nUsing these variables, we can reformulate the problem as the minimization of\n\u2212\np\nX\ni=1\nlog(\u03c12 \u2212\u00b52\ni ) \u2212(d \u2212p)log\u03c12 + \u03c12trace(\u03a3) \u2212\np\nX\nj=1\n\u00b52\nj eT\nj \u03a3ej.\n20.4. GENERALIZED PCA\n509\nFrom theorem 2.3, we have\np\nX\nj=1\n\u00b52\nj eT\nj \u03a3ej \u2264\np\nX\nj=1\n\u00b52\nj \u03b42\nj\nand this upper bound is attained by letting e1,...,ep be the first p eigenvectors of \u03a3.\nUsing this, we see that \u03c32,\u00b52\n1,...,\u00b52\np must minimize\n\u2212\np\nX\ni=1\nlog(\u03c12 \u2212\u00b52\ni ) \u2212(d \u2212p)log\u03c12 + \u03c12\nd\nX\nj=1\n\u03b42\nj \u2212\np\nX\nj=1\n\u00b52\nj \u03b42\nj .\nComputing the solution is elementary and left to the reader, and yields, when ex-\npressed as functions of \u03c32,\u03bb2\n1,...,\u03bb2\np, the expressions given in the statement of the\ntheorem.\n\u25a0\n20.4\nGeneralized PCA\nWe now discuss a dimension reduction method called generalized PCA (GPCA) [200]\nthat, instead of looking for the best linear approximation of the training set by one\nspecific subspace, provides an approximation by a finite union of such spaces.\nAs a motivation, consider the situation in fig. 20.1 in which part of the data\nis aligned along one direction in space, and another part along another direction.\nThen, the only information that PCA can retrieve (provided that the two directions\nintersect) is the plane generated by the two directions, which will be captured by\nthe two principal components. PCA will not be able to determine the individual\ndirections. GPCA addresses this type of situation as follows.\nFigure 20.1: PCA cannot distinguish between the situations depicted in the two datasets.\n510\nCHAPTER 20. DIMENSION REDUCTION AND FACTOR ANALYSIS\nFor simplicity, assume that we are trying to decompose the data along unions of\nhyperplanes in Rd. Such hyperplanes have equations of the form uT \u02dcx = 0 where \u02dcx is\nour notation for the vector (1,xT )T . If we have two hyperplanes, specified by u1 and\nu2 and all the training samples approximately belong to one of them, then one has,\nfor all k = 1,...,N:\n(uT\n1 \u02dcxk)(uT\n2 \u02dcxk) = \u02dcxT\nk u1uT\n2 \u02dcxk \u22430.\nSimilarly, for n hyperplanes, the identity is, for k = 1,...,N:\nn\nY\nj=1\n(uT\nj \u02dcxk) \u22430.\nWrite\nn\nY\nj=1\n(uT\nj x) =\nX\n1\u2264i1,...,in\u2264d\nu1(i1)\u00b7\u00b7\u00b7un(in)x(i1) \u00b7\u00b7\u00b7x(in)\nin the form (by regrouping the terms associated with the same powers of x)\nF(x) =\nX\np1+...+pd=n\nqp1...pd (x(1))p1 ...(x(d))pd .\n(20.8)\nThe collection of \u0000n+d\u22121\nn\n\u0001 numbers Q = (qp1...pn,p1 + \u00b7\u00b7\u00b7 + pd = n) takes a specific form\n(that we will not need to make explicit) as a function of the unknown u1,...,un, but\nthe first step of GPCA ignores this constraint and estimates Q minimizing\nN\nX\nk=1\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\nX\np1+...+pd=n\nqp1...pd (x(1)\nk )p1 ...(x(d)\nk )pd\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\n2\nunder the constraint Pq2\np1...pn = 1 (to avoid trivial solutions). Choosing an ordering\non the set of indices (p1,...,pd) such that p1 +\u00b7\u00b7\u00b7pd = n, one can stack the coefficients\nin Q and the monomials (x(1)\nk )p1 ...(x(d)\nk )pd to form two vectors denoted Q (with some\nabuse of notation) and V (xk). One can then rewrite the problem of determining Q\nas minimizing QT \u03a3Q subject to |Q|2 = 1, where\n\u03a3 =\nN\nX\nk=1\nV (xk)V (xk)T .\nThe solution is given by the eigenvector associated with the smallest eigenvalue of \u03a3.\nIf the model is exact, this eigenvalue should be zero, and if only one decomposition\nof the data in a set of distinct hyperplanes exists (i.e., if n is not chosen too large),\nthen Q is the unique solution up to a multiplicative constant.\n20.5. NUCLEAR NORM MINIMIZATION AND ROBUST PCA\n511\nOnce Q is found, it remains to identify the vectors u1,...,un. This identification\ncan be obtained by inspecting the gradient of F on the union of hyperplanes. Indeed,\none has, for x \u2208Rd,\n\u2207F(x) =\nn\nX\nj=1\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\nY\nj\u2032,j\nuT\nj\u2032 x\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8uj\nHowever, if x belong in one and only one of the hyperplanes, say xT uj = 0, then all\nterms in the sum vanish but one and \u2207F(x) is proportional to uj. So, if the model is\nexact, one has, for each k = 1,...,N, either \u2207F(xk) = 0 (if xk belongs to the intersection\nof two hyperplanes) or \u2207F(xk)/|\u2207F(xk)| = \u00b1uj for some j, and the sign ambiguity can\nbe removed by ensuring, for example, that the first non-vanishing coordinate of uj is\npositive. (The gradient of F can be computed from Q using (20.8).) The computation\nof \u2207F on training data therefore allows for an exact computation of the hyperplanes.\nIn practice, when noise is present, one cannot expect this computation to be\nexact. The vectors u1,...,un can be estimated by clustering the collection of non-\nvanishing gradients \u2207F(xk), k = 1,...,N. For example, one can compute a dissimi-\nlarity matrix such as dkl = 1 \u2212cos2(\u03b8kl), where \u03b8kl is the angle between \u2207F(xk) and\n\u2207F(xl), and apply one of the methds discussed in section 19.4.1.\nThis analysis provides a decomposition of the training set into n (or fewer) hyper-\nplanes. The computation can then be recursively refined in order to obtain smaller\ndimensional subspaces by applying the same method separately to each hyperplane.\n20.5\nNuclear norm minimization and robust PCA\n20.5.1\nLow-rank approximation\nOne can also interpret PCA in terms of low-rank matrix approximations. Let Xc be\nthe N by d matrix (x1 \u2212x,...,xN \u2212x)T , which, in generic situations, has rank d \u22121.\nThen PCA with p components is equivalent to minimizing, over all N by d matrices\nZ of rank p, the norm of the difference\n|Xc \u2212Z|2 = trace((Xc \u2212Z)T (Xc \u2212Z)).\n(20.9)\nThe quantity |A|2 = trace(AT A) is the sum of square of the entries of A, which is often\nreferred to as the (squared) Frobenius norm. We have\n|A|2 =\nd\nX\nk=1\n\u03c32\nk\nwhere \u03c31,...,\u03c3d are the singular values of A, i.e., the square roots of the eigenvalues\nof AT A.\n512\nCHAPTER 20. DIMENSION REDUCTION AND FACTOR ANALYSIS\nWe first note the following characterization of rank-p matrices.\nProposition 20.7 A matrix Z has rank p if and only if it can be written in the form\nZ = AW T where A is N \u00d7p, and W is d \u00d7p with W T W = IdRp, i.e., W = [e1,...,ep] where\nthe columns form an orthonormal family of Rd.\nProof The \u201cif\u201d part is obvious and we prove the \u201conly if\u201d part. Assume that Z has\nrank p. Take W = [e1,...,ep], where (e1,...,ep) is an orthonormal family in Null(Z)\u22a5.\nLetting ep+1,...,ed denote an orthonormal basis of Null(Z), we have Pd\ni=1 eieT\ni = IdRd\nand\nZ = Z\nd\nX\ni=1\neieT\ni = Z\np\nX\ni=1\neieT\ni = ZWW T\nso that one can take A = ZW.\n\u25a0\nUsing this representation and letting zT\nk be the kth row vector of Z, we have\n|Xx \u2212Z|2 =\nN\nX\nk=1\n|xk \u2212x \u2212zk|2 =\nN\nX\nk=1\n\f\f\f\fxk \u2212x \u2212\np\nX\nj=1\na(j)\nk ej\n\f\f\f\f\n2\n.\nWith fixed e1,...,ep, the optimal matrix A has coefficients a(j)\nk = (xk \u2212x)T ej. In matrix\nform, this is:\nZ = Xc\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\np\nX\nj=1\nejeT\nj\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8.\nWe therefore retrieve the PCA formulation that we gave in section 20.1, in the\nspecial case of H = Rd with the standard Euclidean product.\nThe lowest value\nachieved by the PCA solution is\n|Xc \u2212Z|2 = N\nd\nX\nk=p+1\n\u03bb2\nk\nwhere \u03bb2\n1,...,\u03bb2\nd are the eigenvalues of the covariance matrix computed from x1,...,xN,\nwho are also the squared singular values of the matrix Xc divided by N.\nIn this section, we will explore variations on PCA in which the minimization\nof |Xc \u2212Z|2 is completed with a penalty that depends on the singular values of the\nmatrix Z. As a first example, one can modify PCA by adding a penalty on the rank\n(i.e., on the number of non-zero singular values), minimizing:\n\u03b3|Xc \u2212Z|2 + rank(Z)\n20.5. NUCLEAR NORM MINIMIZATION AND ROBUST PCA\n513\nfor some parameter \u03b3 > 0. However, the solution to this problem is a small variation\nof that of standard PCA. It is indeed given by standard PCA with p components\nwhere p minimizes\nN\u03b3\nd\nX\nk=p+1\n\u03bb2\nk + p = N\u03b3\nd\nX\nk=p+1\n(\u03bb2\nk \u2212(N\u03b3)\u22121) + d,\ni.e., p is the index of the last eigenvalue that is larger than (N\u03b3)\u22121.\n20.5.2\nThe nuclear norm\nBased on the fact that rank(Z) is the number of non-zero singular values of Z, one\ncan use the same heuristic as in the development of the lasso, and replace counting\nthe non-zero values by the sum of the absolute values of the singular values, which\nis just the sum of singular values since they are non-negative. This provides the\nnuclear norm of A, defined in section 2.4 by\n|A|\u2217=\nd\nX\nk=1\n\u03c3k\nwhere \u03c31,...,\u03c3d are the singular values of A. We will consider below the problem of\nminimizing\n\u03b3|Xc \u2212Z|2 + |Z|\u2217\n(20.10)\nand show that its solution is once again similar to PCA.\nWe recall the characterization of the nuclear norm proposition 2.6. If A is an N\nby d matrix,\n|A|\u2217= max\n\u001a\ntrace(UAV T ) : U is N \u00d7 N and UT U = Id,V is d \u00d7 d and V T V = Id\n\u001b\n.\nIn Cai et al. [45], the authors consider the minimization of (20.10) and prove\nthe following result. Recall that we have defined the shrinkage function S\u03c4 : t 7\u2192\nsign(t)max(|t|\u2212\u03c4,0) (with \u03c4 \u22650), using the same notation S\u03c4(X) when applying S\u03c4 to\nevery entry of a vector or matrix X. Following Cai et al. [45], we define the singular\nvalue thresholding operator A 7\u2192S\u03c4(A), where A is any rectangular matrix, by\nS\u03c4(A) = US\u03c4(\u2206)V T\nwhen A = U\u2206V T is a singular value decomposition of A.\nProposition 20.8 Let us assume without loss of generality that N \u2265d. The function\nZ 7\u2192\u03b3|Xc \u2212Z|2 + |Z|\u2217is minimized by Z = S1/2\u03b3(X ).\n514\nCHAPTER 20. DIMENSION REDUCTION AND FACTOR ANALYSIS\nProof Representing Z by its singular value decomposition, we have the equivalent\nformulation of minimizing\nF(U,V ,D) = \u03b3|Xc \u2212UDV T |2 + |D|\u2217\n= \u03b3|Xc|2 \u22122\u03b3trace(X T\nc UDV T ) + \u03b3|D|2 + |D|\u2217\nover all orthonormal matrices U and V and diagonal matrices with non-negative\ncoefficients D. From theorem 2.1, we know that trace(X T\nc UDV T ) is less than the\nsum of the products of the non-increasingly ordered singular values of Xc and D\nand this upper bound is attained by taking U = \u00afU and V = \u00afV where \u00afU and \u00afV are\nthe matrices providing the SVD of Xc, i.e., such that Xc = \u00afU\u2206\u00afV T where \u2206is diagonal\nwith non-decreasing coefficients along the diagonal. So, letting \u03bb1 \u2265\u00b7\u00b7\u00b7 \u2265\u03bbd \u22650 and\n\u00b51 \u2265\u00b7\u00b7\u00b7 \u2265\u00b5d \u22650 be the singular values of Xc and Z, we have just proved that, for any\nD,\nF(U,V ,D) \u2265F( \u00afU, \u00afV ,D) = \u22122\u03b3\nd\nX\ni=1\n\u00b5i\u03bbi + \u03b3\nd\nX\ni=1\n\u00b52\ni +\nd\nX\ni=1\n\u00b5i .\nThe lower bound is minimized when \u00b5i = max(\u03bbi \u22121/2\u03b3,0). This proves the propo-\nsition.\n\u25a0\n20.5.3\nRobust PCA\nAs a consequence, the nuclear norm penalty provides the same principal directions\n(after replacing \u03b3 by 2\u03b3) as the rank penalty, but applies a shrinking operation rather\nthan thresholding on the singular values. The difference is however more fundamen-\ntal if, in addition to using the nuclear norm as a penalty, on replaces the squared\nFrobenius norm on the approximation error by the \u21131 norm, where, for an n by m\nmatrix A with coefficients (a(i,j)),\n|A|\u21131 =\nX\ni,j\n|a(i,j)|.\nThis is the formulation of robust PCA [49], which minimizes\n\u03b3|Xc \u2212Z|\u21131 + |Z|\u2217\n(20.11)\nwith respect to Z.\nRobust PCA (which was initially named Principal Component Pursuit by the au-\nthors in Cand`es et al. [49]) is designed for situations in which Xc can be decomposed\nas the sum of a low-rank matrix Z and of a sparse residual S. Some theoretical justi-\nfication was provided in the original paper, stating that if Xc = Z+S, with Z = UDV T\n(its singular value decomposition) such that U and V are sufficiently \u201cdiffuse\u201d and\n20.6. INDEPENDENT COMPONENT ANALYSIS\n515\nrank(Z) is small enough, with the residual\u2019s sparsity pattern taken uniformly at ran-\ndom over the subsets of entries of S with a sufficiently small cardinality, then robust\nPCA is able to reconstruct the decomposition exactly with high probability (relative\nto the random selection of the sparsity pattern of S). We refer to Cand`es et al. [49]\nfor the long proof that justifies this statement.\nRobust PCA can be solved using the ADMM algorithm (section 3.5.5) after refor-\nmulating the problem as the minimization of\n\u03b3|R|\u21131 + |Z|\u2217\nsubject to R + Z = Xc. The algorithm therefore iterates over the following steps.\n\uf8f1\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f2\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f3\nZ(k+1) = argmin\nZ\n\u0012\n|Z|\u2217+ 1\n2\u03b1 |Z + R(k) \u2212Xx + U(k)|2\u0013\nR(k+1) = argmin\nR\n\u0012\n\u03b3|R|\u21131 + 1\n2\u03b1|Z(k+1) + R \u2212Xc + U(k)|2\u0013\nU(k+1) = U(k) + Z(k+1) + R(k+1) \u2212Xc\n(20.12)\nThe first minimization is covered by proposition 2.6 and yields\nZ(k+1) = S\u03b1(Xc \u2212R(k) \u2212U(k)).\nThe second minimization is solved by a standard shrinking operation, i.e.,\nR(k+1) = S\u03b3\u03b1(Xc \u2212Z(k+1) \u2212U(k)).\nUsing this, we can rewrite the robust PCA algorithm as the sequence of fairly simple\niterations.\nAlgorithm 20.1\n(1) Choose a small enough constant \u03b1 and a very small tolerance level \u03f5.\n(2) Initialize the algorithm with N by d matrices R(0) and U(0) (e.g., equal to zero).\n(3) At step n, apply the iteration:\n\uf8f1\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f2\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f3\nZ(k+1) = S\u03b1(Xc \u2212R(k) \u2212U(k))\nR(k+1) = S\u03b3\u03b1(Xc \u2212Z(k+1) \u2212U(k))\nU(k+1) = U(k) + Z(k+1) + R(k+1) \u2212Xc\n(20.13)\n(4) Stop the algorithm is the variation compared to variables at the previous step is\nbelow the tolerance level. Otherwise, apply step n + 1.\n516\nCHAPTER 20. DIMENSION REDUCTION AND FACTOR ANALYSIS\n20.6\nIndependent component analysis\nIndependent component analysis (ICA) is a factor analysis method that represents a\nd-dimensional random variable X in the form X = AY where A is a fixed d \u00d7d invert-\nible matrix and Y is a d-dimensional random vector with independent components.\nThere are two main approaches in this setting. The first one optimizes the matrix\nW = A\u22121 so that the components of WX are \u201cas independent as possible\u201d according\nto a suitable criterion. The second one is model-based, where a statistical model is\nassumed for Y, and its parameters, together with the entries of the matrix A, are es-\ntimated via maximum likelihood. Before describing each of these methods, we first\ndiscuss the extent to which the coefficients of A are identifiable.\n20.6.1\nIdentifiability\nA statistical model is identifiable if its parameters (which could be finite- of infinite-\ndimensional) are uniquely defined by the distribution of the observable variables. In\nthe case of ICA, this question boils down to deciding whether AY \u223cA\u2032Y \u2032 (i.e., they\nhave the same probability distribution) implies that A = A\u2032 (where Y and Y \u2032 are two\nrandom vectors with independent components).\nIt should be clear that the answer to this question is negative, because there are\ntrivial transformations of the matrix A that do not break the ICA model. One can,\nfor example, take any invertible diagonal matrix, D, and let A\u2032 = AD\u22121 and Y \u2032 = DY.\nThe same statement can be made if D is replaced by a permutation matrix, P, which\nreorders the components of Y. So we know that AY \u223cA\u2032Y \u2032 is possible already when\nA\u2032 = ADP where D is diagonal and invertible and P is a permutation matrix. Note\nthat iterating such matrices (i.e., letting A\u2032 = ADPD\u2032P\u2032) does not extend the class\nof transformations because one has DP = PP\u22121DP and one can easily check that\nP\u22121DP is diagonal, so that one can rewrite any product of permutations and diagonal\nmatrices as a single diagonal matrix multiplied by a single permutation.\nIt is interesting, and fundamental for the well-posedness of ICA, that, under one\nimportant additional assumption, the indeterminacy in the identification of A stops\nat these transformations. The additional assumption is that at most one of the com-\nponents of Y follows a Gaussian distribution. That such a restriction is needed is\nclear from the fact that one can transform any Gaussian vector Y with independent\ncomponents into another, BY, one as soon as BBT is diagonal. If two or more com-\nponents of Y are Gaussian, one can restrict these matrices B to only affect those\ncomponents. If only one of them is Gaussian, such an operation has no effect.\nThe following theorem is formally stated in Comon [54], and is a rephrasing\nof the Darmois-Skitovitch theorem [57, 179]. The proof of this theorem relies on\ncomplex analysis arguments on characteristic functions and is beyond the scope of\n20.6. INDEPENDENT COMPONENT ANALYSIS\n517\nthese notes (see Kagan et al. [101] for more details).\nTheorem 20.9 Assume that Y is a random vector with independent components, such\nthat at most one of its components is Gaussian. Let A be an invertible linear transforma-\ntion and \u02dcY = CY. Then the following statements are independent.\n(i) For all i , j, the components \u02dcY (i), \u02dcY (j) are independent.\n(ii) \u02dcY (1),..., \u02dcY (d) are mutually independent.\n(iii) C = DP is the product on a diagonal matrix and of a permutation.\nThe equivalence of (ii) and (iii) implies that the ICA model is identifiable up\nto multiplication on the right by a permutation and a diagonal matrix. Indeed, if\nX = AY = A\u2032Y \u2032 are two decompositions, then it suffices to apply the theorem to\nC = (A\u2032)\u22121A to conclude. The equivalence of (i) and (ii) is striking, and has the\nimportant consequence that, if the data satisfies the ICA model, then, in order to\nidentify A (up to the listed indeterminacy), it suffices to look for Y = A\u22121X with\npairwise independent components, which is a much lesser constraint than full mu-\ntual independence.\nAs a final remark on the Gaussian indeterminacy, we point out that, if the mean\n(m) and covariance matrix (\u03a3) of X are known (or estimated from data), the ICA\nproblem can be reduced to looking for orthogonal transformations A. Indeed, as-\nsuming X = AY and letting \u02dcX = \u03a3\u22121/2(X \u2212m) and \u02dcY = D\u22121/2(Y \u2212A\u22121m), where D is\nthe (diagonal) covariance matrix of Y, we have\n\u02dcX = \u03a3\u22121/2(AY \u2212m) = \u03a3\u22121/2AD1/2 \u02dcY.\nLetting \u02dcA = \u03a3\u22121/2AD1/2, we have IdRd = E( \u02dcX \u02dcXT ) = \u02dcA \u02dcAT so that \u02dcA is orthogonal.\nThis shows that the ICA problem for \u02dcX in the form \u02dcX = \u02dcA \u02dcY with the restriction\nthat \u02dcA is orthogonal has a solution, and also provides a solution of the original ICA\nproblem by letting A = \u03a31/2 \u02dcA and Y = \u02dcY \u2212\u02dcA\u22121\u03a3\u22121/2m. Therefore, the indeterminacy\nassociated with Gaussian vectors is as general as possible up to a normalization of\nfirst and second moments.\n20.6.2\nMeasuring independence and non-Gaussianity\nIndependence between d variables is a very strong property and its complete char-\nacterization is computationally challenging. The fact that the joint p.d.f.of the d\nvariables (we will restrict, to simplify our discussion, to variables that are absolutely\ncontinuous) factorizes into the product of the marginal p.d.f.\u2019s of each variable can\nbe measured by computing the mutual information between the variables, defined\n518\nCHAPTER 20. DIMENSION REDUCTION AND FACTOR ANALYSIS\nby (letting \u03d5Z denote the p.d.f. of a variable Z)\nI(Y) =\nZ\n\u03d5Y(y)\nQd\ni=1 \u03d5Y (i)(y(i))\n\u03d5Y(y)dy.\nThe mutual information is always non-negative and vanishes only if the components\nof Y are mutually independent. Therefore, one can represent ICA as an optimization\nproblem minimizing I(WX) with respect to all invertible matrices W (so that W =\nA\u22121). Letting\nh(Y) = \u2212\nZ\nlog\u03d5Y(y)\u03d5Y(y)dy\ndenote the \u201cdifferential entropy\u201d of Y, we can write\nI(Y) =\nd\nX\ni=1\nh(Y (i)) \u2212h(Y).\nIf Z = WX, then \u03d5Z(z) = \u03d5X(W \u22121x)|det(W)|\u22121. Using this expression in h(Z) and\nmaking a change of variables yields h(WZ) = h(X) + log|detW| and\nI(WX) =\nd\nX\ni=1\nh(Z(i)) \u2212log|det(W)| \u2212h(X).\nThis shows that the optimal W can be obtained by minimizing\nF(W) =\nd\nX\ni=1\nh(W (i)X) \u2212log|det(W)|\nwhere W (i) is the ith row of W. This brings a notable simplification, since this ex-\npression only involves differential entropies of scalar variables, but still remains a\nchallenging problem.\nIn Comon [54], it is proposed to use cumulant expansions of the entropy around\nthat of a Gaussian with identical mean and variance to approximate the differential\nentropy. If \u03be \u223cN (m,\u03c32) , then\nh(\u03be) = 1\n2 + 1\n2 log(2\u03c0\u03c32).\nDefine, for a general random variable U with standard deviation \u03c3U, the non-Gaussian\nentropy, or negentropy, defined by\n\u03bd(U) = 1\n2 + 1\n2 log(2\u03c0\u03c32\nU) \u2212h(U).\n20.6. INDEPENDENT COMPONENT ANALYSIS\n519\nOne can shows that \u03bd(U) \u22650 and is equal to 0 if and only if U is Gaussian. One can\nrewrite F(W) as\nF(W) = d\n2 + d\n2 log(2\u03c0) +\nd\nX\ni=1\nlog(\u03c32\nW (i)X) \u2212\nd\nX\ni=1\n\u03bd(W (i)X) \u2212log|det(W)|\nAs we remarked earlier, if we replace X by \u03a3\u22121/2(X \u2212m) (after estimating the\ncovariance matrix of X), there is no loss of generality in requiring that W is an or-\nthogonal matrix, in which case both \u03c32\nW (i)X and |detW| are equal to 1. Assuming\nsuch a reduction is done, we see that the problem now requires to maximize\nd\nX\ni=1\n\u03bd(W (i)X)\n(20.14)\namong all orthogonal matrices W. Still in Comon [54], an approximation of the\nnegentropy \u03bd(U) is provided as a function of the third and fourth cumulants of the\ndistribution of U. These are given by\n\u03ba3 = E((U \u2212E(U))3)\nand\n\u03ba4 = E((U \u2212E(U))4) \u22123\u03c34\nU.\nIn particular, when U is normalized, i.e., E(U) = 0 and \u03c32\nU = 1, we have \u03ba3 = E(U3)\nand \u03ba4 = E(U4)\u22123. Under the same assumption, it is proposed in Comon [54] to use\nthe approximation\n\u03bd(U) \u223c\u03ba2\n3\n12 + \u03ba2\n4\n48 + 7\u03ba4\n3\n48 \u2212\u03ba2\n3\u03ba4\n8\n.\nThis approximation was derived from an Edgeworth expansion of the p.d.f. of U,\nwhich can be seen as a Taylor expansion around a Gaussian distribution. Plugging\nthis expression into (20.14) provides an expression that can be maximized in W\nwhere the cumulants are replaced by their sample estimates. However, the maxi-\nmized function involves high-degree polynomials in the unknown coefficients of W,\nand this simplified problem still presents numerical challenges.\nAn alternative approximation of the negentropy has been proposed in Hyv\u00a8arinen\n[94] relying on the maximum entropy principle, described in the following theorem.\nAssociate to any random variable Y : G \u2192R the differential entropy\nh\u00b5(Y) = \u2212\nZ\nG\nlog\u03d5Y(x)\u03d5Y(x)d\u00b5(x)\n520\nCHAPTER 20. DIMENSION REDUCTION AND FACTOR ANALYSIS\nif the distribution of Y has a density, denoted \u03d5Y, with respect to \u00b5 and h\u00b5(Y) = \u2212\u221e\notherwise. Use also the same notation\nh\u00b5(\u03d5) = \u2212\nZ\nG\nlog\u03d5(x)\u03d5(x)d\u00b5(x)\nfor a p.d.f. \u03d5 with respect to \u00b5 (i.e., such that \u03d5 is non-negative and has integral 1).\nThen, the following is true.\nTheorem 20.10 Let g = (g(1),...,g(p))T be a function defined on a measurable space G,\ntaking values in Rp, and let \u00b5 be a measure on G. Let \u0393\u00b5 be the set of all \u03bb = (\u03bb(1),...,\u03bb(p)) \u2208\nRp such that\nZ\nG\nexp\n\u0010\n\u03bbT g(y)\n\u0011\nd\u00b5(y) < \u221e.\n(20.15)\nThen\nh\u00b5(Y) \u2264inf\n(\n\u2212\u03bbT E(g(Y)) + log\nZ\nG\nexp\n\u0010\n\u03bbT g(y)\n\u0011\nd\u00b5(y) : \u03bb \u2208\u0393\u00b5\n)\n.\n(20.16)\nDefine, for \u03bb \u2208\u0393\u00b5,\n\u03c8\u03bb(x) =\nexp\n\u0010\n\u03bbT g(x)\n\u0011\nd\u00b5(x)\nR\nG exp\n\u0010\n\u03bbT g(y)\n\u0011\nd\u00b5(y)\n.\n(20.17)\nAssume that the infimum in (20.16) is attained at an interior point \u03bb\u2217of \u0393\u00b5. Then\nh(\u03d5\u03bb\u2217) = max{h( \u02dcY) : E \u02dcY(g) = EY(g),i = 1,...,p}.\n(20.18)\nProof Let Y be a random variable with p.d.f. \u03d5Y with respect to \u00b5 (otherwise the\nlower bound in (20.16) is \u2212\u221e). Then\nh\u00b5(Y) + \u03bbE(g(Y)) \u2212log\nZ\nexp(\u03bbg(y))d\u00b5(y) = \u2212\nZ\nG\n\u03d5Y(x)log \u03d5Y(x)\n\u03c8\u03bb(x)d\u00b5(x) \u22640\nsince\nR\nG \u03d5Y(x)log \u03d5Y (x)\n\u03c8\u03bb(x)d\u00b5(x) is a KL divergence and is always non-negative.\nAssume that \u03bb is in \u02da\u0393\u00b5. Then, there exists \u03f5 > 0 such that, for any u \u2208Rp, |u| = 1,\n\u03bb + \u03f5u \u2208\u0393\u00b5. Using the fact that e\u03b2 \u2265e\u03b1 + (\u03b2 \u2212\u03b1)e\u03b1, we can write\n\u03f5uT ge\u03bbT g \u2264e(\u03bb+\u03f5u)T g \u2212e\u03bbT g\n\u2212\u03f5uT ge\u03bbT g \u2264e(\u03bb\u2212\u03f5u)T g \u2212e\u03bbT g\nyielding\n\u03f5|uT g|e\u03bbT g \u2264max(e(\u03bb+\u03f5u)T g,e(\u03bb\u2212\u03f5u)T g) \u2212e\u03bbT g \u2264e(\u03bb+\u03f5u)T g + e(\u03bb\u2212\u03f5u)T g \u2212e\u03bbT g.\n20.6. INDEPENDENT COMPONENT ANALYSIS\n521\nSince the upper-bound is integrable with respect to \u00b5, so is the lower bound, showing\nthat (taking u in the canonical basis of Rp)\nZ\nG\n|g(i)(y)|e\u03bbT g(y)d\u00b5(y) < \u221e\nfor all i, or\nZ\nG\n|g(y)|e\u03bbT g(y)d\u00b5(y) < \u221e.\nLet c = E(g(Y)) and define\n\u03a8c(\u03bb) = \u2212cT \u03bb + log\nZ\nexp(\u03bbT g(y))dy.\n(20.19)\nThen\n\u2202\u03bb\u03a8c = \u2212cT +\nR\ng(x)T exp(\u03bbT g(x))dx\nR\nexp(\u03bbT g(y))dy\n= \u2212cT +\nZ\nG\ng(x)T \u03c8\u03bb(x)dx.\nSince \u03bb\u2217is a minimizer, we find that, if \u02dcY is a random variable with p.d.f. \u03bb\u2217, then\nE( \u02dcY) = c = E(Y). In that case, the upper-bound in (20.16) is h\u00b5( \u02dcY), proving (20.18). \u25a0\nRemark 20.11 The previous theorem is typically applied with \u00b5 equal to Lebesgue\u2019s\nmeasure on G = Rd or to a counting measure with G finite. To rewrite the statement\nof theorem 20.10 in those cases, it suffices to replace d\u00b5(x) by dx for the former, and\nintegrals by sums over G for the latter. In the rest of the discussion, we restrict to the\ncase when \u00b5 is Lebesgue\u2019s measure, using h(Y) instead of h\u00b5(Y).\n\u2666\nRemark 20.12 This principle justifies, in particular, that the negentropy is always\nnon-negative since it implies that a distribution that maximizes the entropy given\nits first and second moments must be Gaussian.\n\u2666\nThe right-hand side of (20.16) provides a variational approximation of the en-\ntropy. If one uses this approximation when minimizing h(W (1)X) + \u00b7\u00b7\u00b7 + h(W (d)X),\nthe resulting problem can be expressed as a minimization, with respect to W and\n\u03bb1,...,\u03bbd \u2208Rp of\n\u2212\nd\nX\nj=1\n\u03bbT E(g(W (j)X)) +\nd\nX\nj=1\nlog\nZ\nexp\n\u0010\n\u03bbT g(y)\n\u0011\ndy .\nWhile it would be possible to solve this optimization problem directly, a further\napproximation of the upper bound can be developed leading to a simpler procedure.\n522\nCHAPTER 20. DIMENSION REDUCTION AND FACTOR ANALYSIS\nWe have seen in the previous proof that, defining \u03a8c by (20.19) and denoting by E\u03bb\nthe expectation with respect to \u03d5\u03bb, one has\n\u2207\u03a8c(\u03bb) = \u2212c + E\u03bb(g).\nTaking the second derivative, one finds\n\u22072\u03a8c(\u03bb) = E\u03bb((g \u2212E\u03bb(g))(g \u2212E\u03bb(g))T ).\nNow choose c0 such that a maximizer of \u03a8c0(\u03bb), say, \u03bbc0, is known. If c is close to c0,\na first order expansion indicates that, for \u03bbc maximizing \u03a8c, one should have\n\u03bbc \u2243\u03bbc0 + \u22072\u03a8c(\u03bbc0)\u22121(c \u2212c0)\nwith\n\u03a8c(\u03bbc) \u2243\u03a8c(\u03bbc0) \u2212(c \u2212c0)T \u22072\u03a8c(\u03bbc0)\u22121(c \u2212c0).\nOne can then use the right-hand side as an approximation of the optimal entropy.\nThis leads to simple computations under the following assumptions. First, as-\nsume that the first two functions g(1) and g(2) are u and u2/\n\u221a\n3. Let \u03d50 be the p.d.f.\nof a standard Gaussian. Assume that the functions g(j) are chosen so that\nZ\ng(i)(u)g(j)(u)\u03d50(y)dy = \u03b4ij\nfor i,j = 1,...,p and such that\nR\ng(i)(u)\u03d50(y)dy = 0 for i , 2. Take\nc0 =\nZ\ng\u03d50(u)du\nso that c(1)\n0 = 0, c(2)\n0 = 1/\n\u221a\n3 and c(i)\n0 = 0 for i \u22652.\nThen \u03bbc0 provides, by construction, the distribution \u03d50 and for any c, \u22072\u03a8c(\u03bbc0) =\nIdRp. With these assumptions, the approximation is\n\u03a8c(\u03bb) = h(\u03d50) \u2212|c \u2212c0|2\n= 1\n2(1 + log2\u03c0) \u2212\nX\nj\u22653\n(c(j))2\n(assuming that the data is centered and normalized so that c(1) = 0 and c(2) = 1/\n\u221a\n3).\nThe ICA problem can then be solved by maximizing\nd\nX\nj=1\np\nX\ni=1\nE(g(i)(W (j)X))2\n(20.20)\nover orthogonal matrices W.\n20.6. INDEPENDENT COMPONENT ANALYSIS\n523\nRemark 20.13 Without the assumption made on the functions g(j), one needs to\ncompute S = Cov(g(U))\u22121 where U \u223cN (0,1) and maximize\nd\nX\nj=1\n(E(g(W (j)X)) \u2212E(g(U)))T S(E(g(W (j)X)) \u2212E(g(U))).\nClearly, this expression can be reduced to (20.20) by replacing g by S\u22121/2(g\u2212E(g(U))).\nNote also that we retrieve here a similar idea to the negentropy, maximizing a devi-\nation to a Gaussian.\n\u2666\n20.6.3\nMaximization over orthogonal matrices\nIn the previous discussion, we reached a few times a formulation of ICA which re-\nquired optimizing a function W 7\u2192F(W) over all orthogonal matrices. We now dis-\ncuss how such a problem may be implemented.\nIn all the examples that were considered, there would have been no loss of gen-\nerality in requiring that W is a rotation, i.e., det(W) = 1. This is because one can\nchange the sign of this determinant by simply changing the sign of one of the in-\ndependent components, which is always possible. (In fact, the indeterminacy in W\nis by right multiplication by the product of a permutation matrix and a diagonal\nmatrix with \u00b11 entries.)\nLet us assume that F(W) is actually defined and differentiable over all invertible\nmatrices, which form an open subset of the linear space Md(R) of d by d matrices.\nOur optimization problem can therefore be considered as the minimization of F with\nthe constraint that WW T = IdRd.\nGradient descent derives from the analysis that a direction of descent should be\na matrix H such that F(W + \u03f5H) < F(W) for small enough \u03f5 > 0 and on the remark\nthat H = \u2212\u2207F(W) provides such a direction. This analysis does not apply to the con-\nstrained optimization setting because, unless the constraints are linear, W + \u03f5H will\ngenerally stop to satisfy the constraint when \u03f5 > 0, requiring the use of more complex\nprocedures. In our case, however, one can take advantage of the fact that orthogo-\nnal matrices form a group to replace the perturbation W 7\u2192W + \u03f5H by W 7\u2192We\u03f5H\n(using the matrix exponential) where H is moreover required to be skew symmetric\n(H + HT = 0), which guarantees that e\u03f5H is an orthogonal matrix with determinant\n1. Now, using the fact that e\u03f5H = Id + \u03f5H + o(\u03f5), we can write\nF(We\u03f5H) = F(W) + \u03f5trace(\u2207F(W)T WH) + o(\u03f5).\nLet \u2207sF(W) be the skew symmetric part of W T \u2207F(W), i.e.,\n\u2207sF(W) = 1\n2(W T \u2207F(W) \u2212\u2207F(W)T W).\n524\nCHAPTER 20. DIMENSION REDUCTION AND FACTOR ANALYSIS\nThen, if H is skew symmetric,\ntrace(\u2207sF(W)T H) = 1\n2trace(\u2207F(W)T WH) \u22121\n2trace(W T \u2207F(W)H)\n= 1\n2trace(\u2207F(W)T WH) + 1\n2trace(W T \u2207F(W)HT )\n= trace(\u2207F(W)T WH)\nso that\nF(We\u03f5H) = F(W) + \u03f5trace(\u2207sF(W)T H) + o(\u03f5).\nThis show that H = \u2212\u2207sF(W) provides a direction of descent in the orthogonal group,\nin the sense that, if \u2207sF(W) , 0,\nF(We\u2212\u03f5\u2207sF(W)) < F(W)\nfor small enough \u03f5 > 0. As a consequence, the algorithm\nWn+1 = Wne\u2212\u03f5n\u2207sF(Wn)\ncombined with a line search for \u03f5n implements gradient descent in the group of\northogonal matrices, and therefore converges to a local minimizer of F.\nIf one linearizes the r.h.s. as a function of \u03f5, one gets\nWne\u2212\u03f5n\u2207sF(Wn) = Wn + \u03f5n\n2 Wn((Wn)T \u2207F(Wn) \u2212\u2207F(Wn)T Wn) + o(\u03f5)\n= Wn + \u03f5n\n2 (\u2207F(Wn) \u2212Wn\u2207F(Wn)T Wn) + o(\u03f5).\nAs already argued, this linearized version cannot be used when optimizing over the\northogonal group. However, if one denotes by \u03c9(A) the unitary part of the polar\ndecomposition of A, i.e., \u03c9(A) = (AAT )\u22121/2A, then the algorithm\nWn+1 = \u03c9\n\u0012\nWn + \u03f5n\n2 (\u2207F(Wn) \u2212Wn\u2207F(Wn)T Wn)\n\u0013\nalso provides a valid gradient descent algorithm.\n20.6.4\nParametric ICA\nWe now describe a parametric version of ICA in which a model is chosen for the in-\ndependent components of Y. The simplest version of to assume that all Y (j) are i.i.d.\nwith some prescribed p.d.f., say, \u03c8. A typical example for \u03c8 is a logistic distribution\nwith\n\u03c8(t) =\n2\n(et + e\u2212t)2.\n20.6. INDEPENDENT COMPONENT ANALYSIS\n525\nIf y is a vector in Rd, we will use, as usual, the notation \u03c8(y) = (\u03c8(y(1)),...,\u03c8(y(d)))T\nfor \u03c8 applied to each component of y.\nThe model parameter is then the matrix A, or preferably W = A\u22121, and it may be\nestimated using maximum likelihood. Indeed, the p.d.f. of X is\nfX(x) = |detW|\nd\nY\nj=1\n\u03c8(W (j)x)\nwhere W (j) is the jth row of W, so that W can be estimated by maximizing\n\u2113(W) = N log|det(W)| +\nN\nX\nk=1\nd\nX\nj=1\nlog\u03c8(W (j)xk).\nIf we denote by \u0393(W) the matrix with coefficients\n\u03b3ij(W) =\nN\nX\nk=1\nxk(i)\u03c8\u2032(W (j)xk)\n\u03c8(W (j)xk)\nand use the fact that the gradient of W 7\u2192log|detW| is W \u2212T (the inverse transpose\nof W), we can write\n\u2207\u2113(W) = NW \u2212T + \u0393(W).\nWe need however the maximization to operate on sets of invertible matrices, and\nit is more natural to move in this set through multiplication than through addition,\nbecause the product of two invertible matrices is always invertible, but not necessar-\nily their sum. So, similarly to the previous section, we will look for small variations\nin the form W 7\u2192We\u03f5H, or simply, in this case, W 7\u2192W(IdRd +\u03f5H). In both case, the\nfirst order expansion of the log-likelihood gives\n\u2113(W) + \u03f5trace((NW \u2212T + \u0393(W))T WH)\nwhich suggests taking\nH = W T (NW \u2212T + \u0393(W)) = NId + W T \u0393(W).\nDividing H by N, we obtain the following variant of gradient ascent for maxi-\nmum likelihood\nWn+1 = (1 + \u03f5n)Wn + \u03f5nWnW T\nn \u0393(Wn).\nThis algorithm numerically performs much better than standard gradient ascent. It\nmoreover presents the advantage of avoiding computing the inverse of W at each\nstep.\n526\nCHAPTER 20. DIMENSION REDUCTION AND FACTOR ANALYSIS\n20.6.5\nProbabilistic ICA\nNote that the algorithms that we discussed concerning ICA were all formulated in\nterms of the matrix W = A\u22121, which \u201cfilters\u201d the data into independent components.\nAs a result, ICA requires as many independent components as the dimension of X.\nMoreover, because the components are typically normalized to have equal variance,\nthere is no obvious way to perform dimension reduction using this method. Indeed,\nICA is typically run after the data is preprocessed using PCA, this preprocessing\nstep providing the reduction of dimension.\nIt is however possible to define a model similar to probabilistic PCA, assuming a\nlimited number of components to which a Gaussian noise is added, in the form\nX =\np\nX\nj=1\najY (j) + \u03c3R\nwith p < d, a1,...,ap \u2208Rd, Y (1),...,Y (p) independent variables as before, and R \u223c\nN (0,IdRd). This model is identifiable (up to permutation and scalar multiplication\nof the components) as soon as none of the variables Y (j) is Gaussian.\nLet us assume a parametric setting similar to that of the previous section, so that\nY (1),...,Y (p) are explicitly modeled as independent variables with p.d.f. \u03c8. Introduce\nthe matrix A = [a1,...,ap], so that the model can also be written X = AY + \u03c3R, where\nA and \u03c32 are unknown model parameters.\nThe p.d.f. of X is now given by\nfX(x;A,\u03c32) =\n1\n(2\u03c0\u03c32)d/2\nZ\nRp e\u2212|x\u2212Ay|2\n2\u03c32\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\np\nY\ni=1\n\u03c8(y(i))\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8dy(1) ...dy(p),\nwhich is definitely not a closed form. Since we are in a situation in which the pair of\nrandom variables is imperfectly observed through X, using the EM algorithm (chap-\nter 16) is an option, but it may, as we shall see below, lead to heavy computation. The\nbasic step of the EM is, given current parameters A0,\u03c30, to maximize the conditional\nexpectation (knowing X, for the current parameters) of the joint log-likelihood of\n(X,Y) with respect to the new parameters. In this context, the joint distribution of\n(X,Y) has density\nfX,Y(x,y;A,\u03c32) =\n1\n(2\u03c0\u03c32)d/2e\u2212|x\u2212Ay|2\n2\u03c32\np\nY\ni=1\n\u03c8(y(i))\n20.6. INDEPENDENT COMPONENT ANALYSIS\n527\nso that, the conditional joint likelihood over the training set is\n\u2212Nd\n2 log(2\u03c0\u03c32)\u22121\n2\u03c32\nN\nX\nk=1\nEA0,\u03c30(|xk \u2212AY|2 |X = xk)\u2212\nN\nX\nk=1\np\nX\nj=1\nEA0,\u03c32\n0 (log\u03c8(Y (j))|X = xk).\nNotice that the last term does not depend on A,\u03c32, and that, given A, the optimal\nvalue of \u03c32 is given by\n\u03c32 = 1\nNd\nN\nX\nk=1\nEA0,\u03c30(|xk \u2212AY|2 |X = xk)\nThe minimization of\nN\nX\nk=1\nEA0,\u03c30(|xk \u2212AY|2 |X = xk)\nwith respect to A is a least square problem. Let b(j)\nk = EA0,\u03c30(Y (j)|X = xk) and sk(i,j) =\nEA0,\u03c30(Y (i)Y (j)|X = xk): the gradient of the previous term is\n\u22122\nN\nX\nk=1\nEA0,\u03c32\n0 ((xk \u2212AY)Y T |X = xk) = \u22122\nN\nX\nk=1\n(xkbT\nk \u2212ASk),\nbk being the column vector with coefficients b(j)\nk\nand Sk the matrix with coefficients\nsk(i,j). The result therefore is\nA =\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\nN\nX\nk=1\nxkbT\nk\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\nN\nX\nk=1\nSk\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\n\u22121\n.\nUnfortunately, the computation of the moments of the conditional distribution\nof Y given xk (needed in bk and Sk) is a difficult task. The conditional density of Y\ngiven X = xk is\ng(y|xk) = \u03c8(y)e\n\u2212|A0y\u2212x|2\n2\u03c32\n0\n/Z(A0,\u03c30)\nfrom which moments cannot be computed analytically in general. Monte-Carlo sam-\npling algorithms can be used however to approximate these moments, but they are\ncomputationally demanding. And they must be run at every step of the EM.\nIn place of the exact EM, one may use a mode approximation (section 16.3.1),\nwhich replaces the conditional likelihood of Y given X = xk by a Dirac distribution\n528\nCHAPTER 20. DIMENSION REDUCTION AND FACTOR ANALYSIS\nat the mode:\n\u02c6yA0,\u03c30(xk) = argmaxy\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\u03c8(y)e\n\u2212|A0y\u2212xk|2\n2\u03c32\n0\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8.\nThe maximization step then reduces to maximizing in A,\u03c32\n\u2212Nd\n2 log(2\u03c0\u03c32) \u2212\n1\n2\u03c32\nN\nX\nk=1\n\f\f\fxk \u2212A \u02c6yA0,\u03c30(xk)\n\f\f\f2 .\n(20.21)\nThis therefore provides a two-step procedure.\nAlgorithm 20.2 (Probabilistic ICA: mode approximation)\n(1) Initialize the algorithm with A0,\u03c30.\n(2) At step n:\n(i) For k = 1,...,N, maximize Qp\ni=1 \u03c8(y(i))e\n\u2212|Any\u2212xk|2\n2\u03c32n\nto obtain \u02c6yAn,\u03c3n(xk). This\nrequires a numerical optimization procedure, such as gradient ascent. The problem\nis concave when log\u03c8 is concave.\n(ii) Minimize (20.21) with respect to A,\u03c32, yielding\nAn+1 =\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\nN\nX\nk=1\nxkbT\nk\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\nN\nX\nk=1\nSk\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\n\u22121\nwith bk = \u02c6yAn,\u03c3n(xk), Sk = \u02c6yAn,\u03c3n(xk) \u02c6yAn,\u03c3n(xk)T , and\n\u03c32\nn+1 = 1\nNd\nN\nX\nk=1\n\f\f\fxk \u2212A \u02c6yAn,\u03c3n\n\f\f\f2 .\n(3) Stop if the variation of the parameter is below a tolerance level. Otherwise,\niterate to the next step.\nOnce A and \u03c32 have been estimated, the y components associated to a new obser-\nvation x can be estimated by \u02c6yA,\u03c3(x), therefore minimizing\n1\n2\u03c32\n\f\f\fxk \u2212Ay\n\f\f\f2 +\np\nX\nj=1\nlog\u03c8(y(j)),\nyielding the map estimate, the same convex optimization problem as in step (1)\nabove. Now we can see how the method takes from both PCA and ICA: the columns\n20.6. INDEPENDENT COMPONENT ANALYSIS\n529\nof A, a1,...,ap can be considered as p principal directions, and are fixed after learn-\ning; they are not orthonormal, and do not satisfy the nesting properties of PCA (that\nthose p contain those for p \u22121). The coordinates of x with respect to this basis is not\na projection, as would be provided by PCA, but the result of a penalized estimation\nproblem. The penalty associated to the logistic case is\nlog\u03c8(y(j)) = log2 \u22122log(ey(j) + e\u2212y(j)).\nThis distribution with \u201cexponential tails\u201d has the interest of allowing large values of\ny(j), which generally entails sparse decompositions, in which y has a few large coeffi-\ncients, and many zeros.\nAs an alternative to the mode approximation of the EM, which may lead to bi-\nased estimators, one may use the SAEM algorithm (section 16.4.3), as proposed in\nAllassonniere and Younes [3]. Recall that the EM algorithm replaces the parameters\nA0,\u03c32\n0 by minimizers of\nNd\n2 log(\u03c32) +\n1\n2\u03c32\nN\nX\nk=1\nEA0,\u03c30(|xk \u2212AY|2 |X = xk)\n= Nd\n2 log(\u03c32) +\n1\n2\u03c32\nN\nX\nk=1\n|xk|2 \u22121\n\u03c32\nN\nX\nk=1\nxT\nk Abk +\n1\n2\u03c32\nN\nX\nk=1\ntrace(AT ASk),\nwhere the computation of b(j)\nk = EA0,\u03c30(Y (j)|X = xk) and sk(i,j) = EA0,\u03c30(Y (i)Y (j)|X = xk)\nwas the challenging issue. In the SAEM algorithm, the statistics bk and Sk are part of\na stochastic approximation scheme, and are estimated in parallel with EM updates\nas follows.\nAlgorithm 20.3 (SAEM for probabilistic ICA)\nInitialize the algorithm with parameters A, \u03c32. Define a sequence of decreasing\nsteps, \u03b3t.\nLet, for k = 1,...,N, bk = 0 and Sk = Id. Iterate the following steps.\n(1) For k = 1,...,N, sample yk according to the conditional distribution of Y given\nX = xk, using the current parameters A and \u03c32.\n(2) Update bk and Sk, letting (assuming step t of the algorithm)\n(bk \u2192bk + \u03b3t(Yk \u2212bk)\nSk \u2192Sk + \u03b3t(YkY T\nk \u2212Sk)\n530\nCHAPTER 20. DIMENSION REDUCTION AND FACTOR ANALYSIS\n(3) Replace A and \u03c32 by\nA =\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\nN\nX\nk=1\nxkbT\nk\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\nN\nX\nk=1\nSk\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\n\u22121\nand\n\u03c32 = 1\nNd\nN\nX\nk=1\n\f\f\fxk \u2212A \u02c6yA0,\u03c30\n\f\f\f2 .\nThe parameter \u03b3t should be decreasing with t, typically so that P\nt \u03b3t = +\u221eand\nP\nt \u03b32\nt < \u221e(e.g., \u03b3t \u221d1/t). One way to sample from Yk is to uses a rejection scheme,\niterating the procedure which samples y according to the prior and accepts the result\nwith probability M exp(\u2212|xk \u2212Ay|2/2\u03c32) until acceptance. Here M must be chosen so\nthat M maxy exp(\u2212|xk \u2212Ay|2/2\u03c32) \u22641 (e.g., M = 1).\nThis method will work for small p, but for large p, the probability of acceptance\nmay be very small. In such cases, Yk can be sampled changing one component at a\ntime using a Metropolis-Hastings scheme. If component j is updated, this scheme\nsamples a new value of y (call it y\u2032) by changing only y(j) according to the prior\ndistribution \u03c8 and accept the change with probability\nmin\n \n1, exp(\u2212|xk \u2212Ay\u2032|2/2\u03c32)\nexp(\u2212|xk \u2212Ay|2/2\u03c32)\n!\n.\n20.7\nNon-negative matrix factorization\nIn this section, we consider factor analysis methods that approximate a random vari-\nable X in the form X = Pp\nj=1 a(j)Y (j) with the constraint that the scalars a(1), . . . ,\na(p) \u2208R and the vectors Y (1),...,Y (p) \u2208Rd are respectively non-negative and with\nnon-negative entries. This model makes sense, for example, when X represents the\ntotal multivariate production (e.g., in terms of number of molecules of various types)\nresulting of several chemical reactions that operate together. Another application\nis when X is a list of preference scores associated with a person for, say, books or\nmovies, and each person is modeled as a positive linear combination of p \u201ctypical\nscorers,\u201d represented by the vector Y (j) for j = 1,...,p.\nWhen training data (x1,...,xN) is observed and stacked in an N by d matrix X ,\nthe decomposition can be summarized for all observations together in the matrix\nform\nX = AY T\n20.7. NON-NEGATIVE MATRIX FACTORIZATION\n531\nwhere A is N by p and provides the coefficients a(j)\nk associated with each observation\nand Y = [y(1),...,y(p)] is d by p and provides the p typical profiles. The matrices A\nand Y are unknown and their estimation subject to the constraint of having non-\nnegative components represent the non-negative matrix factorization (NMF) prob-\nlem.\nNMF is often implemented by solving the constrained optimization problem of\nminimizing |X \u2212AY T |2 subject to A and Y having non-negative entries. This problem\nis non-convex in general but the sub-problems of optimizing either A or Y when the\nother matrix is fixed are simple quadratic programs.\nThis suggests using an alternating minimization method, iterating steps in which\nA is updated with Y fixed, followed by an update of Y with A fixed. However,\nsolving a full quadratic program at each step would be computationally prohibitive\nwith large datasets, and simpler update rules have been suggested, updating each\nmatrix in turn with a guarantee of reducing the objective function.\nIf Y is considered as fixed and A is the free variable, we have\n|X \u2212AY T |2 = |X |2 \u22122trace(X T AY T ) + trace(AY T YAT )\n= trace(AT AY T Y) \u22122trace(AT (X Y)) + |X |2 .\nThe next lemma will provide update steps for A.\nLemma 20.14 Let M be an n by n symmetric matrix and b \u2208Rn, both assumed to have\nnon-negative entries. Let u \u2208Rn, also with non-negative coefficients, and let\nv(i) = u(i)\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\nb(i)\nPd\nj=1 m(i,j)u(j)\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8.\nThen\nvT Mv \u22122bT v \u2264uT Mu \u22122bT u .\nMoreover, v = u if and only if u minimizes uT Mu \u22122bT u subject to u(i) = 0, i = 1,...,n.\nProof Let F(u) = uT Mu \u22122bT u. We look for v(i) = \u03b2(i)u(i) with \u03b2(i) \u22650 such that\nF(v) \u2264F(u). We have\nF(v) =\nn\nX\ni,j=1\n\u03b2(i)\u03b2(j)u(i)u(j)m(i,j) \u22122\nn\nX\ni=1\nb(i)\u03b2(i)\n\u22641\n2\nn\nX\ni,j=1\n((\u03b2(i))2 + (\u03b2(j))2)u(i)u(j)m(i,j) \u22122\nn\nX\ni=1\nb(i)\u03b2(i)u(i)\n=\nn\nX\ni,j=1\n(\u03b2(i))2u(i)u(j)m(i,j) \u22122\nn\nX\ni=1\nb(i)\u03b2(i)u(i)\n532\nCHAPTER 20. DIMENSION REDUCTION AND FACTOR ANALYSIS\nWhen \u03b2 = 1n, this upper-bound is equal to F(u). So, if we choose \u03b2 minimizing the\nupper-bound, we will indeed find v such that F(v) \u2264F(u). Rewriting the upper-\nbound as\nn\nX\ni=1\nu(i)\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed(\u03b2(i))2\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\nn\nX\nj=1\nm(i,j)u(j)\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\u22122b(i)\u03b2(i)\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\nwe see that \u03b2(i) = b(i)/ Pn\nj=1 m(i,j)u(j) provides such a minimizer, which proves the\nfirst statement of the lemma. For the second statement, we have v(i) = u(i) if and\nonly if u(i) = 0 or Pn\nj=1 m(i,j)u(j) = b(i), and one directly checks that these are exactly\nthe KKT conditions for a minimizer of F over vectors with non-negative entries.\n\u25a0\nTo apply the lemma to the minimization in A, let M : A 7\u2192AY T Y and b = X Y\n(we are working in the linear space of N by p matrices). Then the update\na(i)\nk 7\u2192a(i)\nk\n(X Y)(i,k)\n(AY T Y)(i.k)\ndecreases the objective function.\nSimilarly, applying the lemma with the operator Y 7\u2192YAT A and b = X T A gives\nthe update for Y, namely\ny(i)\nj\n7\u2192y(i)\nj\n(X T A)(i,j)\n(YAT A)(i,j).\nWe have therefore obtained the following algorithm.\nAlgorithm 20.4 (NMF, quadratic cost)\n1. Fix p > 0 and let X be the N by d matrix containing the observed data. Initialize\nthe procedure with matrices A and Y, respectively of size N by p and d by p, with\npositive coefficients.\n2. At a given stage of the algorithm, let A and Y be the current matrices providing\nan approximate decomposition of X .\n3. For the next step, let \u02dc\nA be the matrix with coefficients\n\u02dca(i)\nk = a(i)\nk\n(X Y)(i,k)\n(AY T Y)(i,k)\nand \u02dcY the matrix with coefficients\n\u02dcy(i)\nj\n= y(i)\nj\n(X T \u02dc\nA)(i,j)\n(Y \u02dc\nAT \u02dc\nA)(i,j)\n.\n20.7. NON-NEGATIVE MATRIX FACTORIZATION\n533\n4. Replace A by \u02dc\nA and Y by \u02dcY, iterating until numerical convergence.\nAn alternative version of the method has been proposed, where the objective\nfunction is \u03a6(AY T ), where, for an N by d matrix Z = [z1,...,zN]T ,\n\u03a6(Z) =\nN\nX\nk=1\nd\nX\ni=1\n(z(i)\nk \u2212x(i)\nk logz(i)\nk )\nwhich is indeed minimal for Z = X . We state and prove a second lemma that will\nallow us to address this problem.\nLemma 20.15 Let M be an n by q matrix and x \u2208Rn, b \u2208Rq, all assumed to have positive\nentries. For u \u2208(0,+\u221e)q, define\nF(u) =\nq\nX\nj=1\nb(j)u(j) \u2212\nn\nX\ni=1\nx(i) log\nq\nX\nj=1\nm(i,j)u(j).\nDefine v \u2208(0,+\u221e)q by\nv(j) = u(j)\n Pn\ni=1 m(i,j)x(i)/\u03b1(i)\nb(j)\n!\nwith \u03b1(i) = Pq\nk=1 m(i,k)u(k). Then F(v) \u2264F(u). Moreover, v = u if and only if u minimizes\nF subject to u(i) \u22650,i = 1,...,n.\nProof Introduce a variable \u03b2(j) > 0 for j = 1,...,q an let w(j) = u(j)\u03b2(j). Then\nF(w) =\nq\nX\nj=1\nb(j)u(j)\u03b2(j) \u2212\nn\nX\ni=1\nx(i) log\nq\nX\nj=1\nm(i,j)u(j)\u03b2(j)\n=\nq\nX\nj=1\nb(j)u(j)\u03b2(j) \u2212\nn\nX\ni=1\nx(i) log\nPq\nj=1 m(i,j)u(j)\u03b2(j)\nPq\nj=1 m(i,j)u(j)\n\u2212\nn\nX\ni=1\nx(i) log\nq\nX\nj=1\nm(i,j)u(j)\nLet \u03c1(i,j) = m(i,j)u(j)/\u03b1(i). Since the logarithm is concave, we have\nlog\nq\nX\nj=1\n\u03c1(i,j)\u03b2(j) \u2265\nq\nX\nj=1\n\u03c1(i,j)log\u03b2(j)\nso that\nF(w) \u2264\nq\nX\nj=1\nb(j)u(()j\u03b2(j) \u2212\nn\nX\ni=1\nq\nX\nj=1\nx(i)\u03c1(i,j)log\u03b2(j) \u2212\nn\nX\ni=1\nx(i) log\nq\nX\nj=1\nm(i,j)u(j).\n534\nCHAPTER 20. DIMENSION REDUCTION AND FACTOR ANALYSIS\nThe upper bound with \u03b2(j) \u22611 gives F(u), so minimizing this expression in \u03b2 will\ngive F(w) \u2264F(u). This minimization is straightforward and gives\n\u03b2(j) =\nPn\ni=1 x(i)\u03c1(i,j)\nb(j)u(j)\n=\nPn\ni=1 m(i,j)x(i)/\u03b1(i)\nb(j)\nand the optimal w is the vector v provided in the lemma. Finally, one checks that\nv = u if and only if u satisfies the KKT conditions for the considered problem.\n\u25a0\nWe can now apply this lemma to derive update rules for Y and A, where the\nobjective is\nN\nX\nk=1\nd\nX\ni=1\np\nX\nj=1\ny(i)\nj a(j)\nk \u2212\nN\nX\nk=1\nd\nX\ni=1\nx(i)\nk log\np\nX\nj=1\ny(i)\nj a(j)\nk .\nStarting with the minimization in A, we apply the lemma to each index k separately,\ntaking n = d and q = p, with b(j) = Pd\ni=1 y(i)\nj\nand m(i,j) = y(j)\ni . Then the update is\nak(j) 7\u2192ak(j)\nPd\ni=1 x(i)\nk y(i)\nj /\u03b1(i)\nk\nPd\ni=1 y(i)\nj\nwith \u03b1(i)\nk = Pp\nj=1 y(i)\nj a(j)\nk .\nFor Y, we can work with fixed i and apply the lemma with n = N, q = p, b(j) =\nPN\nk=1 a(j)\nk and m(k,j) = a(j)\nk . This gives the update:\ny(i)\nj\n7\u2192y(i)\nj\nPN\nk=1 x(i)\nk a(j)\nk /\u03b1(i)\nk\nPN\nk=1 a(j)\nk\n,\nstill with \u03b1(i)\nk = Pp\nj=1 y(i)\nj a(j)\nk .\nWe summarize this in our second algorithm for NMF.\nAlgorithm 20.5 (NMF, logarithmic cost)\n1. Fix p > 0 and let X be the N by d matrix containing the observed data.\n2. Initialize the procedure with matrices Y and A, respectively of size N by p and\nd by p, with positive coefficients.\n3. At a given stage of the algorithm, let A and Y be the current matrices decom-\nposing X .\n20.8. VARIATIONAL AUTOENCODERS\n535\n4. Let \u02dc\nA be the matrix with coefficients\n\u02dca(j)\nk = a(j)\nk\nPd\ni=1 x(i)\nk y(i)\nj /\u03b1(i)\nk\nPd\ni=1 y(i)\nj\nwith \u03b1(i)\nk = Pp\nj=1 y(i)\nj a(j)\nk .\n5. Let \u02dcY the matrix with coefficients\n\u02dcy(i)\nj\n= y(i)\nj\nPN\nk=1 x(i)\nk \u02dca(j)\nk / \u02dc\u03b1(i)\nk\nPp\nj=1 \u02dca(j)\nk\nwith \u02dc\u03b1(i)\nk = Pp\nj=1 y(i)\nj \u02dca(j)\nk .\n6. Replace A by \u02dc\nA and Y by \u02dcY, iterating until numerical convergence.\n20.8\nVariational Autoencoders\nVariational autoencoders, which were described in section 18.2.2, can be ineter-\npreted as a non-linear factor model in which X = g(\u03b8,Y) + \u03f5 where \u03f5 is a centered\nGaussian noise with covariance matrix Q and Y \u2208Rp has a known probability dis-\ntribution, such as Y \u223cN (0,IdRp). In this framework, the conditional distribution of\nY given X = x was approximated as a Gaussian distribution with mean \u00b5(x,w) and\ncovariance matrix S(x,w)2. The implementation in Kingma and Welling [103, 104]\nuse neural networks for the three functions g, \u00b5 and S.\n20.9\nBayesian factor analysis and Poisson point processes\n20.9.1\nA feature selection model\nThe expectation in many factor models is that individual observations are obtained\nby mixing pure categories, or topics, and represented as a weighted sum or linear\ncombination of a small number of uncorrelated or independent variables. Denote p\nthe number of possible categories, which, in this section, can be assumed to be quite\nlarge.\nWe will assume that each observation randomly selects a small number among\nthese categories before combining them. Let us consider (as an example) the follow-\ning model.\n536\nCHAPTER 20. DIMENSION REDUCTION AND FACTOR ANALYSIS\n\u2022 The observations X1,...,XN take the form of a probabilistic ICA model\nXk =\np\nX\nj=1\nak(j)bk(j)Y (j) + \u03c3Rk,\nwhere:\n\u2022 Rk follows a standard Gaussian distribution,\n\u2022 ak(1),...,ak(p) are independent with ak(j) \u223cN (mj,\u03c42\nj ),\n\u2022 bk(1),...,bk(p) are independent and follow a Bernoulli distribution with param-\neter \u03c0j,\n\u2022 Y (1),...,Y (p) are independent standard Gaussian random variables.\n\u2022 \u03c32 follows an inverse gamma distribution with parameters \u03b10,\u03b20.\n\u2022 \u03c42\n1,...,\u03c42\np follow independent inverse gamma distributions with parameters \u03b11,\u03b21.\n\u2022 mj follow a Gaussian N (0,\u03c12) and,\n\u2022 \u03c0j follow a beta distribution with parameters (u,v).\nThe priors are, as usual, chosen so that the computation of posterior distributions\nis easy, i.e., they are conjugate priors. The observed data is therefore obtained by\nselecting components Yj with probability \u03c0j and weighted with a Gaussian random\ncoefficient, then added before introducing noise.\nLet nj = PN\nk=1 bk(j). Ignoring constant factors, the joint likelihood of all variables\ntogether is proportional to:\nL \u221d\u03c3\u2212Nd exp\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\u22121\n2\u03c32\nN\nX\nk=1\n|Xk \u2212\np\nX\nj=1\nak(j)bk(j)Y (j)|2\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\np\nY\nj=1\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\u03c4\u2212N\nj\nexp\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\u22121\n2\u03c42\nj\nN\nX\nk=1\n(ak(j) \u2212mj)2\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8exp\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\u22121\n2\u03c12\np\nX\nj=1\nm2\nj\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\np\nY\nj=1\n\u0010\n\u03c0\nnj\nj (1 \u2212\u03c0j)N\u2212nj\u0011\np\nY\nj=1\n\u0010\n(\u03c42\nj )\u2212\u03b11\u22121 exp(\u2212\u03b21/\u03c42\nj )\n\u0011\n(\u03c32)\u03b10\u22121 exp(\u2212\u03b20/\u03c32)\np\nY\nj=1\n\u0010\n\u03c0u\u22121\nj\n(1 \u2212\u03c0j)v\u22121\u0011\nexp\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\u22121\n2\np\nX\ni=1\n|Y (i)|2\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\nIn spite of the complexity of this expression, it is relatively straightforward (by\nconsidering each variable in isolation) to see that\n20.9. BAYESIAN FACTOR ANALYSIS AND POISSON POINT PROCESSES\n537\n\u2022 The conditional distribution of \u03c32,\u03c42\n1,...,\u03c42\np given all other variables remains a\nproduct of inverse gamma distributions.\n\u2022 The conditional distribution of Y (1),...,Y (p) given the other variables is Gaus-\nsian.\n\u2022 The conditional distribution of \u03c01,...,\u03c0p given the other variables is a product\nof beta distributions.\n\u2022 The conditional distribution of m1,...,mp given the other variables remain in-\ndependent Gaussian.\n\u2022 The posterior distribution of a1,...,aN (considered as p-dimensional vectors)\ngiven the other variables is a product of independent Gaussian (but the components\nak(j), j = 1,...,p are correlated).\n\u2022 For the posterior distribution given the other variables, b1,...,bN (considered\nas p-dimensional vectors) are independent. The components of each bk are not inde-\npendent but each bk(j) being a binary variable follows a Bernoulli distribution given\nthe other ones.\nThese remarks provide the basis of a Gibbs sampling algorithm for the simulation\nof the posterior distribution of all unobserved variables (the computation of the pa-\nrameters of each of the conditional distribution above requires some work, of course,\nand these details are left to the reader). This simulation does not explicitly provide a\nmatrix factorization of the data (in the sense of a single matrix A such that X = AY,\nas considered in the previous section), but a probability distribution on such matri-\nces, expressed as A(k,j) = ak(j)bk(j). One can however use the average of the matri-\nces obtained through the simulation for this purpose. Additional information can\nbe obtained through this simulation. For example, the expectation of bk(j) provides\na measure of proximity of observation k to category j.\n20.9.2\nNon-negative and count variables\nPoisson factor analysis.\nMany variations can be made on the previous construc-\ntion. When the observations are non-negative, for example, an additive Gaussian\nnoise may not be well adapted. Alternative models should model the conditional\ndistribution of X given a, b and Y as a distribution over non-negative numbers with\nmean (a \u2299b)T Y (for example a gamma distribution with appropriate parameters).\nThe posterior sampling generally is more challenging in this case because simple\nconjugate priors are not always available.\nAn important special case is when X is a count variable taking values in the set of\nnon-negative integers. In this case (starting with a model without feature selection),\nmodeling X as a Poisson variable with mean a(1)Y (1) +\u00b7\u00b7\u00b7+a(p)Y (p) leads to tractable\ncomputations, once it is noticed that X can be seen as a sum of random variables\n538\nCHAPTER 20. DIMENSION REDUCTION AND FACTOR ANALYSIS\nZ[1],...,Z[p] where Z[i] follows a Poisson distribution with parameter a(i)Y (i). This\nsuggests introducing new latent variables (Z[1],...,Z[p]), which are not observed but\nfollow, conditionally to their sum, which is X and is observed, a multinomial distri-\nbution with parameters X,q1,...,qp, with qi = a(i)Y (i)/(Pp\nj=1 a(j)Y (j)).\nThis provides what is referred to as a Poisson factor analysis (PFA). As an exam-\nple, consider a Bayesian approach where, for the prior distribution, a(1),...,a(p) are\nindependent and follow as a gamma distribution with parameters \u03b10 and \u03b20, and\nY (1),...,Y (p) are independent, exponentially distributed with parameter 1. The joint\nlikelihood of all data then is (up to constant factors):\nL \u221dexp\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\u2212\nN\nX\nk=1\n(ak(1)Y (1) + \u00b7\u00b7\u00b7 + ak(p)Y (p))\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\nN\nY\nk=1\np\nY\ni=1\n(ak(i)Y (i))z[i]\nk\nz[i]\nk !\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\nN\nY\nk=1\np\nY\ni=1\nak(i)\u03b1\u22121\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8exp\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\u2212\u03b2\nN\nX\nk=1\np\nX\ni=1\nak(i)\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8exp\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\u2212\np\nX\ni=1\nY (i)\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8.\nThis is the GaP (For Gamma-Poisson) model introduced in Canny [50]. The condi-\ntional distribution of the variables (ak(i)) given (Z[i]\nk ) and (Yi) are independent and\ngamma-distributed, and so are (Y (i)) given the other variables. Finally, for each k, the\nfamily (Z[1]\nk ,...,Z[p]\nk ) follows a multinomial distribution conditionally to their sum,\nXk, and the rest of the variables, and these variables are conditionally independent\nacross k.\nGaP with feature selection\nOne can include a feature selection step in this model\nby introducing binary variables b(1),...,b(p), with selection probabilities \u03c01,...,\u03c0p,\nwith a Beta(u,v) prior distribution on \u03c0i. Doing so, the likelihood of the extended\nmodel is:\nL \u221dexp\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\u2212\nN\nX\nk=1\n(ak(1)bk(1)Y (1) + \u00b7\u00b7\u00b7 + ak(p)bk(p)Y (p))\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\nN\nY\nk=1\np\nY\ni=1\n(ak(i)bk(i)Y (i))z[i]\nk\nz[i]\nk !\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\nN\nY\nk=1\np\nY\ni=1\nak(i)\u03b1\u22121\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8exp\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\u2212\u03b2\nN\nX\nk=1\np\nX\ni=1\nak(i)\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8exp\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\u2212\np\nX\ni=1\nY (i)\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\np\nY\nj=1\n\u0010\n\u03c0\nnj\nj (1 \u2212\u03c0j)N\u2212nj\u0011\np\nY\nj=1\n\u0010\n\u03c0u\u22121\nj\n(1 \u2212\u03c0j)v\u22121\u0011\n.\nwhere, as before, nj = PN\nk=1 bk(j). The conditional distribution of \u03c01,...,\u03c0p given\nthe other variables is therefore still that of a family of independent beta-distributed\nvariables. The binary variables bk(1),...,bk(p) are also conditionally independent\ngiven the other variables, with bk(i) = 1 with probability one if z[i]\nk\n> 0 and with\nprobability \u03c0j exp(\u2212ak(j)Y (j)) if z[j]\nk = 0.\n20.9. BAYESIAN FACTOR ANALYSIS AND POISSON POINT PROCESSES\n539\n20.9.3\nFeature assignment model\nThe previous models assumed that p features were available, modeled as p random\nvariables with some prior distribution, and that each observation picks a subset of\nthem, drawing feature j with probability \u03c0j. We denoted by bk(j) the binary variable\nindicating whether feature j was selected for observation k, and nj was the number\nof times that feature was selected. Finally, we modeled \u03c0j as a beta variable with\nparameters u and v.\nOne can compute, using this model, the probability distribution of of the feature\nselection variables, b = (bk(j),j = 1,...,p,k = 1,...,N). From the model definition, the\nprobability of observing such a configuration is given by\nQ(b) = \u0393(u + v)p\n\u0393(u)p\u0393(v)p\nZ\np\nY\nj=1\n\u03c0\nnj+u\u22121\nj\n(1 \u2212\u03c0j)N\u2212nj+v\u22121d\u03c01 ...d\u03c0p\n=\np\nY\nj=1\n\u0393(u + v)\u0393(u + nj)\u0393(v + N \u2212nj)\n\u0393(u)\u0393(v)\u0393(u + v + N)\n=\np\nY\nj=1\nu(u + 1)\u00b7\u00b7\u00b7(u + nj \u22121)v(v + 1)\u00b7\u00b7\u00b7(v + N \u2212nj \u22121)\n(u + v)(u + v + 1)\u00b7\u00b7\u00b7(u + v + N \u22121)\nDenote by njk = Pk\u22121\nl=1 bl(j) the number of observations with index less than k that\npick feature j. Using this notation, we can write, using the fact that\nu(u + 1)\u00b7\u00b7\u00b7(u + nj \u22121) =\nN\nY\nk=1\n(u + njk)bk(j)\nand a similar identity for v(v + 1)\u00b7\u00b7\u00b7(v + N \u2212nj \u22121),\nQ(b) =\np\nY\nj=1\nN\nY\nk=1\n(u + njk)bk(j)(v + k \u22121 \u2212njk)1\u2212bk(j)\nu + v + k \u22121\n=\nN\nY\nk=1\np\nY\nj=1\n\u0012\nu + njk\nu + v + k \u22121\n\u0013bk(j)  v + k \u22121 \u2212njk\nu + v + k \u22121\n!1\u2212bk(j)\n.\nUsing this last equation, we can interpret the probability Q as resulting from a pro-\ngressive feature assignment process. The first observation, k = 1, for which njk = 0\nfor all j, chooses each feature with probability u/(u +v). When reaching observation\nk, feature j is chosen with probability (u + njk)/(u + v + k \u22121). At all steps, features\nare chosen independently from each other.\n540\nCHAPTER 20. DIMENSION REDUCTION AND FACTOR ANALYSIS\nLet Fk be the set of features assigned to observation k, i.e., Fk = {j : bk(j) = 1} and\nGk = Fk \\\nk\u22121\n[\nl=1\nFl\nbe the set of features used in observation k but in no previous observation. Let\nCk = Fk \\ Gk and Uk = G1 \u2229\u00b7\u00b7\u00b7 \u2229Gk\u22121. Instead of considering configurations b =\n(bk(j),i = 1,...,d,k = 1,...,N) we may alternatively consider the family of sets S =\n(Gk,Ck,1 \u2264k \u2264N). Such a family must satisfy the property that the sets Gk and\nCk are non-intersecting, Ck \u2282Uk and Gl \u2229Gk = \u2205for l < k. It provide a unique\nconfiguration b by letting bk(j) = 1 if and only if j \u2208Gk \u222aCk. We will let, in the\nfollowing, qk = |Gk| and pk = |Uk|. The probability Q(b) can be re-expressed in terms\nof S, letting (with some abuse of notation)\nQ(S) =\nN\nY\nk=1\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\n\u0012\nu\nu + v + k \u22121\n\u0013qk  \nv + k \u22121\nu + v + k \u22121\n!p\u2212pk+1\nY\nj\u2208Uk\n\u0012\nu + njk\nu + v + k \u22121\n\u00131j\u2208Ck  v + k \u22121 \u2212njk\nu + v + k \u22121\n!1j<Ck \uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8.\nLet Sk = (Gl,Cl,l \u2264k). Then the expression of Q shows that, conditionally to Sk\u22121, Gk\nand Ck are independent. Elements in Ck are chosen independently for each feature\nj \u2208Uk with probability (u +njk)/(u +v +k \u22121). Moreover, the conditional distribution\nof qk given Sk\u22121 is proportional to\n\u0012\nu\nu + v + k \u22121\n\u0013qk  \nv + k \u22121\nu + v + k \u22121\n!p\u2212pk\u2212qk\ni.e., it is a binomial distribution with parameters p \u2212pk and u/(u + v + k \u22121). Finally,\ngiven sk\u22121 and qk, the distribution of Gk is uniform among all \u0000p\u2212pk\nqk\n\u0001 subsets of\n{1,...,p} \\ (G1 \u222a\u00b7\u00b7\u00b7 \u222aGk\u22121)\nwith cardinality qk.\nIf there is no special meaning in the feature label, which is the case in our discus-\nsion of prior models in which all features are sampled independently with the same\ndistribution, we may identify configurations that can be deduced from each other by\nrelabeling (note that relabeling features does not change the value of Q).\nCall a configuration normal if Gk = {pk + 1,...,pk+1}. Given S, it is always pos-\nsible to relabel the features with a permutation \u03c3 so that, for each k, \u03c3(Gk) = {pk +\n1,...,pk+1}. There are, in fact, q1!...qN! such permutations. We can complete the pro-\ncess generating S by adding at the end a transformation into a normal configuration\n20.9. BAYESIAN FACTOR ANALYSIS AND POISSON POINT PROCESSES\n541\n(picking uniformly at random one of the possible ones). The probability of a normal\nconfiguration S obtained through this process is (using a simple counting argument)\nQ(S) =\nN\nY\nk=1\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\n p \u2212pk\nqk\n!\u0012\nu\nu + v + k \u22121\n\u0013qk  \nv + k \u22121\nu + v + k \u22121\n!p\u2212pk+1\nY\nj\u2208Uk\n\u0012\nu + njk\nu + v + k \u22121\n\u00131j\u2208Ck  v + k \u22121 \u2212njk\nu + v + k \u22121\n!1j<Ck \uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8,\nThis provides a new incremental procedure that directly samples normalized as-\nsignments. First let q1 follow a binomial distribution bin(p,u/(u + v)) and assign the\nfirst observation to features 1 to q1. Assume that pk labels have been created before\nstep k. Then select for observation k some of the already labeled features, label j\nbeing selected with probability (u + njk)/(u + v + k \u22121) as above. Finally, add qk new\nfeatures where qk follows a binomial distribution bin(p \u2212pk,u/(u + v + k \u22121)).\nThis discussion is clearly reminiscent of the one that was made in section 19.7.3\nleading to the Polya urn process, and we want here also to let p tend to infinity\n(with fixed N) with proper choices of u and v as functions of p in the expression\nabove. Choose two positive numbers c and \u03b3 and let u = c\u03b3/p and v = c \u2212u. Note\nthat, with the incremental simulation process that we just described, the conditional\nexpectation of the next number of labels, pk+1 given the current one, pk is\nE(pk+1|pk) =\n(p \u2212pk)u\nu + v + k \u22121 + pk =\nc\u03b3\nc + k \u22121 +\n \n1 \u2212\nc\u03b3\np(c + k \u22121)\n!\npk \u2264\nc\u03b3\nc + k \u22121 + pk\nTaking expectations on both sides, we get\nE(pk+1) \u2264\nk\nX\nl=1\nc\u03b3\nc + l \u22121 \u2264\nN\nX\nl=1\nc\u03b3\nc + l \u22121\nso that this expectation is bounded independently of k. This shows in particular\nthat pk/p tends to 0 in probability (just applying Markov\u2019s inequality) and that the\nbinomial distribution bin(p \u2212pk,u/(u + v + k \u22121)) can be approximated by a Poisson\ndistribution with parameter c\u03b3/(c + k \u22121).\nSo, when p \u2192\u221e, we obtain the following incremental simulation process for the\nfeature labels, that we combine with the actual simulation of the features, assumed\nto follow a prior distribution with p.d.f. \u03c8. This process is called the Indian buffet\nprocess in the literature, the analogy being that a buffet offers an infinite variety of\ndishes, and each observation is a customer who tastes a finite number of them.\n542\nCHAPTER 20. DIMENSION REDUCTION AND FACTOR ANALYSIS\nAlgorithm 20.6 (Indian buffet process)\n1. Initialization:\n(i) Sample an integer q1 according to a Poisson distribution with parameter \u03b3.\n(ii) Sample features y(1),...,y(q1) according to \u03c8.\n(iii) Assign these features to observation 1, and let n2,j = 1 for j = 1,...,q1.\n2. Assume that observations 1 to k\u22121 have been obtained, with pk features y(1),...,y(pk)\nsuch that the jth feature has been chosen nk,j times.\n(i) For j = 1,...,pk, assign feature j to sample k with probability\nnk,j\nc+k\u22121. If j is\nselected, let nk+1,j = nk,j + 1, otherwise let nk+1,j = nk,j.\n(ii) Sample an integer qk according to a Poisson distribution with parameter\nc\u03b3\nc+k\u22121 and let pk+1 = pk + qk.\n(iii) Sample features y(pk+1),...,y(pk+1) according to \u03c8.\n(iv) Assign these features to observation k, and let nk+1,j = 1 for j = pk+1,...,pk.\n3. If k = N, stop, otherwise replace k by k + 1 and return to Step 2.\n20.10\nPoint processes and random measures\nThis section assumes that the reader is familiar with measure theory. It can however safely\nbe skipped as it is not reused in the rest of the book.\n20.10.1\nPoisson processes\nIf Z is a set, we will denote by Pc(Z) the set composed with all finite or countable\nsubsets of Z. A point process over Z is a random variable S : \u2126\u2192Pc(Z), i.e., a\nvariable that provides a countable random subset of Z. If B \u2282Z one can then define\nthe counting function \u03bdS(B) = |S \u2229B| \u2208Z \u222a{+\u221e}.\nA proper definition of such point processes requires some measure theory. Equip\nZ with a \u03c3-algebra A and consider the set N0 of integer-valued measures \u00b5 on (Z,A)\nsuch that \u00b5(Z) < \u221e. Let N be the set formed with all countable sums of measures\nin N0. Then a general point process is a mapping \u03bd : \u2126\u2192N such that for all\nk \u2208N \u222a{+\u221e} and all B \u2208A, the event {\u03bd(B) = k} is measurable. Recall that, for each\nB \u2208A, \u03bd(B) is itself a random variable, that we may denote \u03c9 7\u2192\u03bd\u03c9(B). One then\ndefine the intensity of the process as the the function \u00b5 : B 7\u2192E(\u03bd(B)).\nThe following proposition provides an important identity satisfied by such mod-\nels.\n20.10. POINT PROCESSES AND RANDOM MEASURES\n543\nTheorem 20.16 (Campbell identity) Let \u03bd be a point process with intensity \u00b5. For\n\u03c9 \u2208\u2126, let X\u03c9 : \u2126\u2032 \u2192Z be a random variable with distribution \u03bd\u03c9 (defined, if needed, on\na different probability space (\u2126\u2032,P\u2032)). Then, for any \u00b5-integrable function f :\nE(f (X)) =\nZ\nZ\nf (z)d\u00b5(z).\n(20.22)\nHere, the expectation of f (X) is over both spaces \u2126and \u2126\u2032 and corresponds to the\naverage of f . The identity is an immediate consequence of Fubini\u2019s theorem.\nWe will be mainly interested in the family of Poisson point processes. These\nprocesses are themselves parametrized by a measure, say \u00b5, on Z such that \u00b5 is \u03c3-\nfinite and \u00b5(B) = 0 if B is a singleton. A Poisson process with intensity measure \u00b5 is\na point process \u03bd such that:\n(i) If B1,...,Bn are non-intersecting pairwise, then \u03bd(B1),...,\u03bd(Bn) are mutually in-\ndependent.\n(ii) for all B, \u03bd(B) \u223cPoisson(\u00b5(B)).\nWe take the convention that \u03bd(B) = 0 (resp. = \u221e) almost surely if \u00b5(B) = 0 (resp.\n= \u221e). Note that property (i) also implies that if g1,...,gn are measurable functions\nfrom Z to (0,+\u221e) such that gigj = 0 for i , j, then the variables \u03bd(gi) =\nR\nZ gi(z)d\u03bd(z)\nare independent.\nIf \u00b5(Z) < \u221e(i.e., \u00b5 is finite), one can represent the distribution of a Poisson point\nprocess as follows:\n\u03bd =\n\u03bd(Z)\nX\nk=1\n\u03b4Xk\nwith \u03bd(Z) \u223cPoisson(\u00b5(Z)) and, conditional to \u03bd(Z) = N, X1,...,XN are i.i.d. and fol-\nlow the probability distribution \u00af\u00b5 = \u00b5/\u00b5(Z). This measure can also be identified with\nthe random set S = {X1,...,X\u03bd(Z)}. The assumption that \u00b5({z}) = 0 for any singleton\nimplies that \u03bd({z}) = 0 almost surely. It also ensures that the points X1,...,XN are\ndistinct with probability one.\nIf \u00b5 is \u03c3-finite, then (by definition), it is a countable sum of finite measures\n\u00b51,\u00b52,.... Then \u03bd can be generated as the sum of independent \u03bd1,\u03bd2,..., where \u03bdi\nis a Poisson process with intensity \u00b5i. It can moreover be identified with the count-\nable random set S = S\u221e\ni=1 Si, where Si is the random set associated with \u03bdi. Note that,\nin this construction, one can always assume that the measures \u00b51,\u00b52,... are mutually\nsingular (i.e., \u00b5i(B) > 0 for some i implies that \u00b5j(B) = 0 for j , i).\n544\nCHAPTER 20. DIMENSION REDUCTION AND FACTOR ANALYSIS\nIf we consider a Poisson process on (0,+\u221e) \u00d7 Z, we can define weighted random\nmeasures. Indeed, such a point process takes values in the collection of all sets of the\nform {(wk,zk),k \u2208I} where I is finite or countable. These subsets can be represented\nas the sum of weighted Dirac masses,\n\u03be =\nX\nk\u2208I\nwk\u03b4zk .\nTo ensure that the points (zk,k \u2208I) generated by this process are all different, we need\nto assume that the intensity \u00b5 of this random process is such that \u00b5((0,+\u221e) \u00d7 {z}) = 0\nfor all z \u2208Z. We will refer to \u03be as a weighted Poisson process.\nIn the following, we will consider this class of random measures, with the small\naddition of allowing for an extra term including a measure supported by a fixed set.\nMore precisely, given a (deterministic) countable subset I \u2282Z, a family of indepen-\ndent random variables (\u03c1z,z \u2208I) and a \u03c3-finite measure \u00b5o such that \u00b5o((0,+\u221e) \u00d7\n{z}) = 0 for all z \u2208Z, we can define the random measure\n\u03be = \u03bef + \u03beo\nwhere \u03beo is a weighted Poisson process with intensity \u00b5o, assumed independent of\n(\u03c1z,z \u2208I) and\n\u03bef =\nX\nz\u2208I\n\u03c1z\u03b4z.\nThe subscripts o and f come from the terminology introduced in Kingman [105],\nwhich studies \u201ccompletely random measures,\u201d which are a random measures that\nsatisfy point (i) in the definition of a Poisson process. Under mild assumptions, such\nmeasures can be decomposed as a sum of a weighted Poisson process (here, \u03beo, the\nordinary part), of a process with fixed support, (here, \u03bef , the fixed part) and of a\ndeterministic measure (which is here taken to be 0).\nLet us rapidly check that \u03be satisfies property (i). Let B1,...,Bn be non-overlapping\nelements of A. Get gi(w,z) = w1Bi(z). Then\n\u03be(Bi) = \u03bef (Bi) + \u03bdo(gi)\nwhere \u03bdo is a Poisson process with intensity \u00b5o. Since the sets do not overlap, the\nvariables (\u03bef (Bi),i = 1,...,n) are independent, and so are (\u03bdo(gi),i = 1,...,n) since\ngigj = 0 for i , j. Since \u03bef and \u03bdo are, in addition independent, we see that (\u03be(Bi),i =\n1,...,n) are independent.\nThe intensity measure of such a process is still defined by\n\u03b7(B) = E(\u03be(B)) =\nX\nz\u2208I\nP((\u03c1z,z) \u2208B) +\nZ\n(0,+\u221e)\u00d7B\nwd\u00b5o(w,z)\nwhere the last term is an application of Campbell\u2019s inequality to the Poisson process\n\u03bdo and the function g(w,x) = w1B(x).\n20.10. POINT PROCESSES AND RANDOM MEASURES\n545\n20.10.2\nThe gamma process\nThe main example of such processes in factor analysis is the beta process that will be\ndiscussed in the next section. We start, however, with a first example that is closely\nrelated with the Dirichlet process, called the gamma process.\nIn this process, one fixes a finite measure \u03c00 on Z and defines \u00b5 on (0,+\u221e)\u00d7Z by\n\u00b5(dw,dz) = cw\u22121e\u2212cw\u03c00(dz)dw.\nBecause \u00b5 is \u03c3-finite but not finite (the integral over t diverges at t = 0), every real-\nization of \u03be is an infinite sum\n\u03be =\n\u221e\nX\nk=1\nwk\u03b4zk.\nThe intensity measure of \u03be is\n\u03b7(B) = c\u03c00(B)\nZ +\u221e\n0\ne\u2212cwdw = \u03c00(B).\nIn particular,\n\u221e\nX\nk=1\nwk = \u03b7(Z) = \u03c00(Z) < \u221e.\nFor fixed B, the variable \u03be(B) follows a Gamma distribution. This can be proved\nby computing the Laplace transform of \u03be, E(e\u2212\u03bb\u03be(B)), and identify it to that of a\nGamma. To make this computation, consider the point process \u03bdJ restricted to a\ninterval J \u2282(0,+\u221e) with min(J) > 0, and \u03beJ the corresponding weighted process.\nLet mJ(t) =\nR\nJ w\u22121ce\u2212(c+t)w dw. Then a realization of \u03bdJ can be obtained by first sam-\npling N from a Poisson distribution with parameter \u00b5(J \u00d7 Z) = mJ(0)\u03c00(Z) and then\nsampling N points (wi,zi) independently from the distribution \u00b5/(mJ(0)\u03c00(Z)). This\nimplies that\nE(e\u2212t\u03beJ(B)) =\n\u221e\nX\nn=0\ne\u2212mJ(0)\u03c00(Z)(mJ(0)\u03c00(Z))n\nn!\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\nR \u221e\n0 e\u2212tw1B(z)w\u22121ce\u2212cwdwd\u03c00(z)\nmJ(0)\u03c00(Z)\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\nn\n=\n\u221e\nX\nn=0\ne\u2212mJ(0)\u03c00(Z)\nn!\n\u0010\n\u03c00(B)mJ(t) + (\u03c00(Z) \u2212\u03c00(B))mJ(0)\n\u0011n\n= e\u03c00(B)(mJ(t)\u2212mJ(0)) .\nNow,\nmJ(t) \u2212mJ(0) = c\nZ\nJ\necw e\u2212tw \u22121\nw\ndw\n546\nCHAPTER 20. DIMENSION REDUCTION AND FACTOR ANALYSIS\nis finite even when J = (0,+\u221e). With a little more work justifying passing to the\nlimit, one finds that, for J = (0,+\u221e),\nE(e\u2212t\u03beJ(B)) = exp\n \n\u03c00(B)\nZ +\u221e\n0\ne\u2212cw e\u2212tw \u22121\nw\ndw\n!\n.\nFinally, write\nc\nZ +\u221e\n0\ne\u2212cw e\u2212tw \u22121\nw\ndw = \u2212c\nZ +\u221e\n0\ne\u2212cw\nZ t\n0\ne\u2212swdsdw\n= \u2212c\nZ t\n0\nZ +\u221e\n0\ne\u2212(s+c)wdwds\n= \u2212\nZ t\n0\nc(s + c)\u22121ds = \u2212clog(1 + t\nc).\nThis shows that\nE(e\u2212t\u03beJ(B)) =\n\u0012\n1 + t\nc\n\u0013\u2212c\u03c00(B)\nwhich is the Laplace transform of a Gamma distribution with parameters c\u03c00(B) and\nc, i.e., with density proportional to wc\u03c00(B)\u22121e\u2212cw.\nAs a consequence, the normalized process \u03b4 = \u03be/\u03be(Z) is a Dirichlet process with\nintensity c\u03c00. Indeed, if B1,...,Bn is a partition of Z the family (\u03b4(B1),...,\u03b4(Bn)) is\nthe ratio of n independent gamma variables to their sum, which provides a Dirichlet\ndistribution, and this property characterizes Dirichlet processes.\n20.10.3\nThe beta process\nThe definition of the beta process parallels that of the gamma process, with weights\ntaking this time values in (0,1). Fix again a finite measure \u03c00 on Z and let \u00b5o on\n(0,+\u221e) \u00d7 Z be defined by\n\u00b5o(dw,dz) = cw\u22121(1 \u2212w)c\u22121\u03c00(dz)dw.\nThe associated weighted Poisson process can therefore be represented as a sum\n\u03beo =\n\u221e\nX\nk=1\nwk\u03b4zk,\nand its intensity measure is\n\u03b7o(B) = c\u03c00(B)\nZ 1\n0\n(1 \u2212t)c\u22121dw = \u03c00(B).\n20.10. POINT PROCESSES AND RANDOM MEASURES\n547\nIn particular, since \u03c00 is finite, we have P\u221e\nk=1 wk < \u221ealmost surely. A beta process is\nthe sum of the process \u03beo and of a fixed set process\n\u03bef =\nX\nz\u2208I\nwz\u03b4z\nwhere I is a fixed finite set and (wz,z \u2208I) are independent and follow a beta distri-\nbution with parameters (a(z),b(z)).\nIf Z is a space of features, such a process provides a prior distribution on fea-\nture selections. It indeed provides, in addition to the deterministic set I, a random\ncountable set J \u2282Z, with a set of random weights wz,z \u2208F := I \u222aJ . Given this,\none defines the feature process as the selection of a subset A \u2282F where each feature\nz is selected with probability wz. Because E(|A|) = P\nz\u2208F wz is finite, A is finite with\nprobability 1.\nIn the same way the Polya urn could be used to sample from a realization of a\nDirichlet process without actually sampling the whole process, there exists an algo-\nrithm that samples a sequence of feature sets (A1,...,An) from this feature selection\nprocess without needing the infinite collection of weights and features associated\nwith a beta process. We assume in the following that the prior process has an empty\nfixed set. (Non-empty fixed sets will appear in the posterior.)\nThe first set of features, A1, is obtained as follows according to a Poisson process\nwith intensity \u03c00: choose the number N of features in A1 according to a Poisson dis-\ntribution with parameter \u03c00(Z). Then sample N features independently according\nto the distribution \u03c00/\u03c00(Z).\nNow assume that n\u22121 sets of features A1,...,An have been obtained and we want\nto sample a new set An+1 conditionally to their observation. Let Jn be the union of\nall random features obtained up to this point and n(z), for z \u2208Jn the number of times\nthis feature was observed in A1,...,An. Then the conditional distribution of the beta\nprocess \u03be given this observation is still a beta process, with fixed set given by I = Jn,\n(a(z),b(z)) = (n(z),c + n \u2212n(z)) for z \u2208Jn\u22121 and base measure \u03c0n = c\u03c00/(c + n). This\nimplies that the next set An+1 can be obtained by sampling from the associated fea-\nture process. To do this, one first selects features z \u2208Jn with probability n(z)/(c + n),\nthen selects additional features z1,...,zN independently with distribution \u03c00/\u03c00(Z)\nwhere N follows a Poisson distribution with parameter c\u03c00(Z)/(c + n). This is the\nIndian buffet process, described in Algorithm 20.6 (taking \u03c00 = \u03b3\u03c8).\n20.10.4\nBeta Process and feature selection\nThe beta process can be used as a prior for feature selection within a factor analysis\nmodel, as described in the previous paragraph. It is however easier to approximate\n548\nCHAPTER 20. DIMENSION REDUCTION AND FACTOR ANALYSIS\nit with a model with almost surely finite support. Indeed, letting, for \u03f5 > 0\n\u00b5(dw,dz) =\n\u0393(c + 1)\n\u0393(\u03f5 + 1)\u0393(c \u2212\u03f5)w\u03f5\u22121(1 \u2212w)c\u2212\u03f5\u03c00(dz)dw,\none obtains a finite measure since\nZ +\u221e\n0\nZ\nZ\n\u00b5o(dw,dz) = c\u03b3\n\u03f5\nwhere \u03b3 = \u03c00(Z). Note that \u00b5 is normalized so that E(\u03be(B)) = \u03c00(B) for B \u2282Z.\nIn this case, the prior generates features by first sampling their number, p, ran-\ndomly according to a Poisson distribution with mean c\u03b3/\u03f5, then select p probabilities\nw1,...,wp independently using a beta distribution with parameters \u03f5 and c \u2212\u03f5, and\nfinally attach to each i a feature zi with distribution \u03c00/\u03b3. The features associated\nwith a given sample are then obtained by selecting each zi with probability wi.\nWe note also that the model described in section 20.9.3 provides an approxima-\ntion of this prior using a finite number of features. With our notation here, this\ncorresponds to taking p \u226b1 and \u03f5 = c\u03b3/p.\nChapter 21\nData Visualization and Manifold Learning\n21.1\nMultidimensional scaling\nThe methods described in this chapter aim at representing a dataset in low dimen-\nsion, allowing for its visual exploration by summarizing its structure in a user-\naccessible interface. Unlike factor analysis methods, they do not necessarily attempt\nat providing a causal model expressing the data as a function of a small number of\nsources, and generally do not provide a direct mechanism for adding new data to the\nrepresentation. In addition, all these methods take as input similarity dissimilarity\nmatrices between data points and do not require, say, Euclidean coordinates.\nAssuming that a dissimilarity matrix D = (dkl, k,l = 1,...,N) is given, the goals of\nmultidimensional scaling (or MDS) is to determine a small-dimensional Euclidean\nrepresentation, say y1,...,yN \u2208Rp, such that\n\f\f\fyk \u2212yl\n\f\f\f2 \u2243d2\nkl. We review below two\nversions of this algorithm, referred to as \u201csimilarity\u201d and \u201cdissimilarity\u201d matching.\n21.1.1\nSimilarity matching (Euclidean case)\nWe start with the standard hypotheses of MDS, assuming that the distances dkl de-\nrive from a representation in feature space, so that d2\nkl = \u2225hk \u2212hl\u22252\nH for some inner-\nproduct space H and (possibly unknown) features h1,...,hN. Note that, since the\nEuclidean distance is invariant by translation, there is no loss of generality in as-\nsuming h1 + \u00b7\u00b7\u00b7 + hN = 0, which will be done in the following.\nWe look for a p-dimensional representation in the form yk = \u03a6hk where \u03a6 is a lin-\near transformation (and we want yk to be computable directly from dissimilarities,\nsince we do not assume that hk is known). Since we are only interested in a trans-\nformation of the h1,...,hN, it suffices to compute \u03a6 in the vector space generated by\n549\n550\nCHAPTER 21. DATA VISUALIZATION AND MANIFOLD LEARNING\nthem, so that we let\n\u03a6 : span(h1,...,hN) \u2192Rp ,\nand we want \u03a6 to (approximately) conserve the norm, i.e., be close to being an isom-\netry.\nBecause isometries are one-to-one and onto, the existence of an exact isometry\nwould require V\n\u2206= span(h1,...,hN) to be p-dimensional. The mapping \u03a6 could then\nbe defined as \u03a6(h) = (\u27e8h , e1\u27e9H,...,\u27e8h , ep\u27e9H) where e1,...,ep is any orthonormal basis\nof V . In the general case, however, where V is not p-dimensional or less, one can\nreplace it by a best p-dimensional approximation of the training data, leading to a\nproblem similar to PCA in feature space.\nIndeed, as we have seen in section 20.1.2, this best approximation can be ob-\ntained by diagonalizing the Gram matrix S of h1,...,hN, which is such that skl =\n\u27e8hk , hl\u27e9H. (Recall that we assume that \u00afh = 0, so we do not center the data here.) Us-\ning the notation in section 20.1.2, let \u03b1(1),...,\u03b1(p) denote the eigenvectors associated\nwith the p largest eigenvalues, normalized so that (\u03b1(i))T S\u03b1(i) = 1 for i = 1,...,p. One\ncan then take\nei =\nN\nX\nl=1\n\u03b1(i)\nl hl\nand, for k = 1,...,N, j = 1,...,p:\ny(i)\nk = \u03bb2\ni \u03b1(i)\nk\n(21.1)\nwhere \u03bb2\ni is the eigenvalue associated with \u03b1(i).\nThis does not entirely address the original problem, since the inner products skl\nare not given, but only the distances dkl, which satisfy\nd2\nkl = \u22122skl + skk + sll .\n(21.2)\nThis provides a linear system of equations in the unknown skl. This system is under-\ndetermined, because D is invariant by any transformation hk 7\u2192hk + h0 (for a fixed\nh0), and S is not. However, the assumption h1 + \u00b7\u00b7\u00b7 + hN = 0 provides the additional\nconstraint needed to provide a unique solution.\nSumming (21.2) over l, we then get\nN\nX\nl=1\nd2\nkl = Nskk +\nN\nX\nl=1\nsll .\n(21.3)\nSumming this equation over k, we find\nN\nX\nk,l=1\nd2\nkl = 2N\nN\nX\nl=1\nsll.\n21.1. MULTIDIMENSIONAL SCALING\n551\nUsing this in (21.3), we get\nskk = 1\nN\nN\nX\nl=1\nd2\nkl \u2212\n1\n2N 2\nN\nX\nk,l=1\nd2\nkl,\nand, from (21.2)\nskl = \u22121\n2\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8edd2\nkl \u22121\nN\nN\nX\nk\u2032=1\nd2\nk\u2032l \u22121\nN\nN\nX\nl\u2032=1\nd2\nkl\u2032 + 1\nN 2\nN\nX\nk\u2032,l\u2032=1\nd2\nk\u2032l\u2032\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8.\nIf we denote by D\u22992 the matrix formed with the squared distances d2\nkl, this identity\ncan we rewritten in the simpler form\nS = \u22121\n2PD\u22992P\n(21.4)\nwith P = IdRN \u22121N1T\nN/N.\nWe now show that this PCA approach to MDS is equivalent to the problem of\nminimizing\nF(y) =\nN\nX\nk,l=1\n(yT\nk yl \u2212skl)2\n(21.5)\nover all y1,...,yN \u2208Rp such that y1+\u00b7\u00b7\u00b7+yN = 0, which can be interpreted as matching\n\u201csimilarities\u201d skl rather than distances. Indeed, letting Y denote the N by p matrix\nwith rows yT\n1 ,...,yT\nN, we have\nF(y) = trace((YY T \u2212S)2).\nFinding Y is equivalent to finding a symmetric matrix M of rank p minimizing\ntrace((M \u2212S)2). We have, using the trace inequality (theorem 2.1), and letting \u03bb2\n1 \u2265\n\u00b7\u00b7\u00b7 \u2265\u03bb2\nN (resp. \u00b52\n1 \u2265\u00b7\u00b7\u00b7 \u2265\u00b52\np) denote the eigenvalues of S (resp. M)\ntrace((M \u2212S)2) = trace(M2) \u22122trace(MS) + trace(S2)\n=\np\nX\nk=1\n\u00b54\nk \u22122trace(MS) +\nN\nX\nk=1\n\u03bb4\nk\n\u2265\np\nX\nk=1\n\u00b54\nk \u22122\np\nX\nk=1\n\u03bb2\nk\u00b52\nk +\nN\nX\nk=1\n\u03bb2\nk\n=\np\nX\nk=1\n(\u03bb2\nk \u2212\u00b52\nk)2 +\nN\nX\nk=p+1\n\u03bb4\nk\n\u2265\nN\nX\nk=p+1\n\u03bb4\nk\n552\nCHAPTER 21. DATA VISUALIZATION AND MANIFOLD LEARNING\nThis lower bound is attained when M and S can be diagonalized in the same or-\nthonormal basis with \u03bb2\nk = \u00b52\nk for k = 1,...,p. So, letting S = UDUT , where U is\northogonal and D is diagonal with decreasing numbers on the diagonal, an optimal\nM is given by M = UpDpUT\np , where Up is formed with the first p columns of A and Dp\nis the first p \u00d7 p block of D. This shows that the matrix Y = UpD1/2 provides a min-\nimizer of F. The matrix U = [u(1),...,u(N)] differs from the matrix A = [\u03b1(1),...,\u03b1(N)]\nabove through the normalization of its column vectors: we have S\u03b1(i) = \u03bb2\ni \u03b1(i) with\n(\u03b1(i))T S\u03b1(i) = 1 while Su(i) = \u03bb2\ni u(i) with (u(i))T Su(i) = \u03bb2\ni showing that \u03b1(i) = \u03bb\u22121\ni u(i).\nThis shows that Ap = UpD\u22121/2\np\nso that Y can also be rewritten as Y = ApDp, i.e.,\ny(i)\nk = \u03bb2\ni \u03b1(i)\nk , the same expression that was obtained before.\nThe minimization of F is called similarity matching. Clearly, this method can\nbe applied when one starts directly with a matrix of dissimilarities S, provided it\nsatisfies PN\nl=1 skl = 0 for all k. If this is not the case, then interpreting skl as an inner\nproduct hT\nk hl, it is natural to replace skl by what would give (hk \u2212\u00afh)T (hl \u2212\u00afh), namely,\nby\ns\u2032\nkl = skl \u22121\nN\nN\nX\nl\u2032=1\nskl\u2032 \u22121\nN\nN\nX\nk\u2032=1\nsk\u2032l + 1\nN 2\nN\nX\nk\u2032,l\u2032=1\nsk\u2032l\u2032.\nInterestingly, this discussion provides us with yet another interpretation of PCA.\n21.1.2\nDissimilarity matching\nWhile the minimization of (21.5) did not provide us with a new way of analyzing the\ndata (since it was equivalent to PCA), the direct comparison of dissimilarities, that\nis, the minimization of\nG(y) =\nN\nX\nk,l=1\n(|yk \u2212yl| \u2212dkl)2\nover y1,...,yN \u2208Rp, provides a different approach. Since this may be useful in prac-\ntice and does not bring in much additional difficulty, we will allow for the possibility\nof weighting the differences in G and consider the minimization of\nG(y) =\nN\nX\nk,l=1\nwkl(|yk \u2212yl| \u2212dkl)2\nwhere W = (wkl) is a symmetric matrix of non-negative weights. The only additional\ncomplexity resulting by adding weights is that the indeterminacy on y1,...,yN is that\nG(y) = G(y\u2032) as soon as y \u2212y\u2032 is constant on every connected component of the graph\nassociated with the weight matrix W, so that the constraint on y should be replaced\nby\nX\nk\u2208\u0393\nyk = 0\n21.1. MULTIDIMENSIONAL SCALING\n553\nfor any connected component \u0393 of this graph. (If all weights are positive, then the\nonly non-empty connected component is {1,...,N} and we retrieve our previous con-\nstraint PN\nk=1 yk = 0.)\nStandard nonlinear optimization methods, such as projected gradient descent,\nmay be used to minimize G, but the preferred algorithm for MDS uses a stepwise\nprocedure resulting from the addition of an auxiliary variable. Rewrite\nG(y) =\nN\nX\nk,l=1\nwkl|yk \u2212yl|2 \u22122\nN\nX\nk,l=1\nwkldkl|yk \u2212yl| +\nN\nX\nk,l=1\nd2\nkl .\nWe have, for u \u2208Rp:\n|u| = max{zT u : z \u2208Rp,|z| = 1u,0}.\nUsing this identity, we can introduce auxiliary variables zkl, k,l = 1,...,N in Rp, with\n|zkl| = 1 if yk , yl and define\n\u02c6G(y,z) =\nN\nX\nk,l=1\nwkl|yk \u2212yl|2 \u22122\nN\nX\nk,l=1\nwkldkl(yk \u2212yl)T zkl +\nN\nX\nk,l=1\nd2\nkl .\nWe then have\nG(y) =\nmin\nz:|zkl|=1 if yk,yk\n\u02c6G(y,z).\nAs a consequence, minimizing G in y can be achieved by minimizing \u02c6G in y and\nz and discarding z when this is done. One can minimize \u02c6G iteratively, alternating\nminimization in y given z and in z given y, both steps being elementary. In order to\ndescribe these steps, introduce some matrix notation.\nLet L denote the Laplacian matrix of the weighted graph on {1,...,N} associated\nwith the weight matrix W, namely L = (\u2113kl,k,l = 1,...,N) with \u2113kk = PN\nk=1 wkl \u2212wkk\nand \u2113kl = \u2212wkl when k , l. Then,\nN\nX\nk,l=1\nwkl|yk \u2212yl|2 = 2trace(Y T LY).\nDefining uk \u2208Rp by\nuk =\nN\nX\nl=1\nwkldkl(zkl \u2212zlk),\nand U =\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\nuT\n1...\nuT\nN\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\n, we have\nN\nX\nk,l=1\nwkldkl(yk \u2212yl)T zkl = trace(U T Y).\n554\nCHAPTER 21. DATA VISUALIZATION AND MANIFOLD LEARNING\nWith this notation, the optimal matrix Y must minimize\n2trace(Y T LY) \u22122trace(U T Y).\nLet m be the number of connected components of the weighted graph. Recall that\nthe matrix L is positive semi-definite and that an orthonormal basis of its null space\nis provided by vectors, say e1,...,em, that are constant on each of the m connected\ncomponents of the graph, so that the constraint on Y can be written as eT\nj Y = 0 for\nj = 1,...,m. Introduce the matrix\n\u02c6L = L +\nm\nX\nk=1\nekeT\nk\nwhich is positive definite. Our minimization problem is then equivalent to minimiz-\ning\n2trace(Y T \u02c6LY) \u22122trace(U T Y),\nsubject to eT\nj Y = 0 for j = 1,...,m. The derivative of this function is\n4\u02c6LY \u22122U\nso that an optimal Y must satisfy\n4\u02c6LY \u22122U +\nm\nX\nj=1\nej\u00b5T\nj = 0\nfor Lagrange multipliers \u00b51,...,\u00b5m \u2208Rp. This shows that\nY = 1\n2\n\u02c6L\u22121\u0012\nU \u22121\n2\nm\nX\nj=1\nej\u00b5T\nj\n\u0013\n= 1\n2\n\u02c6L\u22121U \u22121\n4\nm\nX\nj=1\nej\u00b5T\nj\nwhere we have used the fact that \u02c6L\u22121ej = ej. We can now identify \u00b5j since\n0 = eT\nj Y = 1\n2eT\nj \u02c6L\u22121U \u22121\n4\nm\nX\nj\u2032=1\neT\nj ej\u2032\u00b5T\nj = 1\n2eT\nj U \u22121\n4\u00b5T\nj\nso that \u00b5T\nj = 2eT\nj U and the optimal Y is\nY = 1\n2\n\u02c6L\u22121U \u22121\n2\nm\nX\nj=1\nejeT\nj U.\n21.2. MANIFOLD LEARNING\n555\nNote that this expression can be rewritten as\nY = 1\n2PL \u02c6L\u22121U\nwhere PL = IdRN \u2212PN\nk=1 ejeT\nj is the projection onto the space perpendicular to the null\nspace of L (i.e., the range of L). In the case where the graph has a single connected\ncomponent, one has m = 1 and e1 = 1N/\n\u221a\nN yielding\nPL = IdRN \u22121\nN 1N1T\nN.\nThe minimization in z given y is straightforward: if yk , yk, then zkl = (yk \u2212\nyl)/|yk \u2212yl|. If yk = yl, then one can take any value for zkl and the simplest if of course\nzkl = 0. Using the previous computation, we can summarize a training algorithm for\nmulti-dimensional scaling, called SMACOF for \u201cScaling by Maximizing a Convex\nFunction\u201d (see, e.g., Borg and Groenen [36] for more details and references).\nAlgorithm 21.1 (SMACOF)\nAssume that a symmetric matrix of dissimilarities (dkl,k,l = 1,...,N) is given, to-\ngether with a matrix of weights (wkl,k,l = 1,...,N). Fix a target dimension, p. Fix a\ntolerance constant \u03f5.\n1. Compute the Laplacian matrix L of the graph associated with the weights, the\nprojection matrix PL onto the range of L and the matrix M = (L + IdRN \u2212PL)\u22121.\n2. Initialize the algorithm with some family y1,...,yN \u2208Rp and let Y =\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\nyT\n1...\nyT\nN\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\n.\n3. At a given step of the algorithm, let Y be the current solution and compute, for\nk = 1,...,N:\nuk = 2\nN\nX\nl=1\nwkldkl\nyk \u2212yl\n|yk \u2212yl|1yk,yl\nto form the matrix U =\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\nuT\n1...\nuT\nN\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\n.\n4. Compute Y \u2032 = 1\n2PLMU.\n5. If |Y \u2212Y \u2032| \u2264\u03f5, exit and return Y \u2032.\n6. Return to step 3.\n556\nCHAPTER 21. DATA VISUALIZATION AND MANIFOLD LEARNING\nFigure 21.1:\nLeft: Multidimensional scaling applied to a 3D curve embedded in a 10-\ndimensional space retrieves the Euclidean structure. Right: Isomap, in contrasts, identifies\nthe one-dimensional nature of the data.\n21.2\nManifold learning\nThe goal of MDS is to map a full matrix of distances into a low-dimensional Eu-\nclidean space. Such a representation, however, cannot address the possibility that\nthe data is supported by a low-dimensional, albeit nonlinear, space. For example,\npeople leaving on Earth live, for all purposes, on a two-dimensional structure (a\nsphere), but any faithful Euclidean representation of the world population needs\nto use the three spatial dimensions. One may also argue that the relevant distance\nbetween points on Earth is not the Euclidean one either (because one would never\ntravel through Earth to go from one place to another), but the distance associated to\nthe shortest path on the sphere, which is measured along great circles.\nTo take another example, the left panel in fig. 21.1 provides the result of applying\nMDS to a ten-dimensional dataset obtained by applying a random ten-dimensional\nrotation to a curve supported by a three-dimensional torus. MDS indeed retrieves\nthe correct curve structure in space, which is three dimensional. However, for a\nperson \u201cliving\u201d on the curve, the data is one-dimensional, a fact that is captured by\nthe Isomap method that we now describe.\n21.2.1\nIsomap\nLet us return to the example of people living on the spherical Earth. One can de-\nfine the distance between two points on Earth either as the shortest length a person\nwould have to travel (say, by plane) to go from one point to the other (that we can\ncall the intrinsic distance), or simply the chordal distance in 3D space between the two\npoints. The first one is obviously the most relevant to the spherical structure of the\nEarth, but the second one is easier to compute given the locations of the points in\n21.2. MANIFOLD LEARNING\n557\nspace.\nFor typical datasets, the geometric structure of the data (e.g., that it is supported\nby a sphere) is unknown, and the only information that is available is their chordal\ndistance in an ambient space (which can be very large). An important remark, how-\never is that, when the points are close to each other, the two distances can be ex-\npected to be similar, if we assume that the geometry of the set supporting the data is\nlocally linear (e.g., that it is, like the sphere, a \u201csubmanifold\u201d of the ambient space,\nwith small neighborhoods of any data point well approximated, at first order, by\npoints on a tangent space). Isomap uses this property, only trusting small distances\nin the matrix D, and infers large distances by adding the costs resulting from travel-\ning from data points to nearby data points.\nFix an integer c. Given D, the c-nearest neighbor graph on V = {1,...,N} places an\nedge between k and l if and only if dk,l is among the c smallest values in {dkl\u2032,l\u2032 , k}\nneighbors or xl among the c smallest values in {dk\u2032l,k\u2032 , l}. We will write k \u223cc l to\nindicate that there exists an edge between k and l in this graph. One then defines\nthe geodesic distance on the graph as\nd(\u2217)\nkl = min\n\uf8f1\uf8f4\uf8f4\uf8f4\uf8f2\uf8f4\uf8f4\uf8f4\uf8f3\nm\nX\nj=1\ndkj\u22121kj : k0,...,km \u2208{1,...,N},k0 = k \u223cc k1 \u223cc \u00b7\u00b7\u00b7 \u223cc km\u22121 \u223cc km = l,m \u22650\n\uf8fc\uf8f4\uf8f4\uf8f4\uf8fd\uf8f4\uf8f4\uf8f4\uf8fe\n.\nThis geodesic distance can be computed incrementally as follows. First define\nd(1)\nkl = |xk \u2212xl| if k \u223cc l and d(1)\nkl = +\u221eotherwise (and also let d(1)\nkk = 0). Then, given\nd(n\u22121), define\nd(n)\nkl = min\n\u001a\nd(n\u22121)\nkl\u2032\n+ d(1)\nll\u2032 l\u2032 = 1,...,N\n\u001b\nuntil the entries stabilize, i.e., d(n+1) = d(n), in which case one has d(\u2217) = d(n). The\nvalidity of the statement can be easily proved by checking that\nd(n)\nkl = min\n\uf8f1\uf8f4\uf8f4\uf8f4\uf8f2\uf8f4\uf8f4\uf8f4\uf8f3\nn\nX\nj=1\nd(1)\nkj\u22121kj : k0,...,kn \u2208{1,...,N},k0 = k,kn = l\n\uf8fc\uf8f4\uf8f4\uf8f4\uf8fd\uf8f4\uf8f4\uf8f4\uf8fe\n,\nwhich can be done by induction, the details being left to the reader. It should also\nbe clear that the procedure will stabilize after no more than N steps.\nOnce the distance is computed, Isomap then applies standard MDS, resulting\nin a straightened representation of the data like in fig. 21.1. Another example is\nprovided in fig. 21.2, where, this time, the input curve is closed and cannot therefore\nbe represented as a one-dimensional structure. One can note, however, that, even in\nthis case, Isomap still provides some simplification of the initial shape of the data.\n558\nCHAPTER 21. DATA VISUALIZATION AND MANIFOLD LEARNING\nFigure 21.2:\nLeft: Multidimensional scaling applied to a 3D curve embedded in a 10-\ndimensional space retrieves the Euclidean structure. Right: Isomap, in contrasts, identifies\nthe one-dimensional nature of the data.\n21.2.2\nLocal Linear Embedding\nLocal linear embedding (LLE) exploits in a different way the fact that manifolds are\nlocally well approximated by linear spaces. Like Isomap, it starts also with build-\ning a c-nearest-neighbor graph on {1,...,k}. Assume, for the sake of the discussion,\nthat the distance matrix is computed for possibly unobserved data T = (x1,...,xN).\nLetting Nk denote the indices of the nearest neighbors of k (excluding k itself), the\nbasic assumption is that xk should approximately lie in the affine space generated by\nxl,l \u2208Nk. Expressed in barycentric coordinates, this space is defined by\nTk =\n\uf8f1\uf8f4\uf8f4\uf8f4\uf8f2\uf8f4\uf8f4\uf8f4\uf8f3\nX\nl\u2208Nk\n\u03c1(l)xl : \u03c1 \u2208RNk,\nX\nl\u2208Nk\n\u03c1(l) = 1\n\uf8fc\uf8f4\uf8f4\uf8f4\uf8fd\uf8f4\uf8f4\uf8f4\uf8fe\n,\nand Tk can be interpreted as an approximation of the tangent space at xk to the data\nmanifold. Optimal coefficients (\u03c1()kl,k = 1,...,N,l \u2208Nk) providing the representa-\ntion of xk in that space can be estimated by minimizing, for all k\n\f\f\f\f\f\f\f\f\nxk \u2212\nX\nl\u2208Nk\n\u03c1(l)\nk xl\n\f\f\f\f\f\f\f\f\n2\nsubject to P\nl\u2208Nk \u03c1(l)\nk = 1. This is a simple least-square program. Let ck = |Nk| (ck = c\nin the absence of ties). Order the elements of Nk to represent \u03c1(l)\nk ,l \u2208Nk as a vector\ndenoted \u03c1k \u2208Rck. Similarly, let Sk be the Gram matrix associated with xl,l \u2208Nk\nformed with all inner products xT\nl\u2032 xl, l,l\u2032 = 1,...,N and let rk be the vector composed\nwith products xT\nk xl,l \u2208Nk. Assume that Sk is invertible, which is generally true if\n21.2. MANIFOLD LEARNING\n559\nc < d, unless the neighbors are exactly linearly aligned. Then, the optimal \u03c1k and the\nLagrange multiplier \u03bb for the constraint are given by\n \n\u03c1k\n\u03bb\n!\n=\n \nSk\n1ck\n1T\nck\n0\n!\u22121  \nrk\n1\n!\n.\n(21.6)\nIf Sk is not invertible, the problem is under-constrained and one of its solutions can\nbe obtained by replacing the inverse above by a pseudo-inverse.\nThe low-dimensional representation of the data, still denoted (y1,...,yN) with\nyk \u2208Rp is then estimated so that the relative position of yk to its neighbors is the\nsame as that of xk, i.e., so that\nyk \u2243\nX\nl\u2208Nk\n\u03c1(l)\nk yl.\nThese vectors are estimated by minimizing\nF(y) =\nN\nX\nk=1\n\f\f\f\f\f\f\f\f\nyk \u2212\nX\nl\u2208Nk\n\u03c1(l)\nk yl\n\f\f\f\f\f\f\f\f\n2\n.\nObviously, some additional constraints are needed to avoid the trivial solution yk = 0\nfor all k. Also, replacing all yk\u2019s by y\u2032\nk = Ryk +b where R is an orthogonal transforma-\ntion in Rp and b is a translation does not change the value of F, so there is no loss of\ngenerality in assuming that PN\nk=1 yk = 0 and that PN\nk=1 ykyT\nk = D0, a diagonal matrix.\nHowever, if one lets y\u2032\nk = Dyk where D is diagonal, then\nF(y) =\np\nX\ni=1\nD2\nii\nN\nX\nk=1\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8edy(i)\nk \u2212\nX\nl\u2208Nk\n\u03c1(l)\nk y(i)\nl\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\n2\n.\nThis shows that one should not allow the diagonal coefficients of D0 to be chosen\nfreely, since otherwise the optimal solution would require to take this coefficient to 0.\nSo D0 should be a fixed matrix, and by symmetry, it is natural to take D0 = IdRp. (Any\nother solution\u2014for a different D0\u2014can then be obtained by rescaling independently\nthe coordinates of y1,...,yN.)\nExtend \u03c1(l)\nk to an N-dimensional vector by taking \u03c1(k)\nk\n= \u22121 and \u03c1(l)\nk = 0 if l , k and\nl < Nk. We can write\nF(y) =\nN\nX\nk=1\n\f\f\f\f\f\f\f\nN\nX\nl=1\n\u03c1(l)\nk yl\n\f\f\f\f\f\f\f\n2\n.\nExpanding the square, this is\nF(y) =\nN\nX\nl,l\u2032=1\nwll\u2032yT\nl yl\u2032\n560\nCHAPTER 21. DATA VISUALIZATION AND MANIFOLD LEARNING\nwith wll\u2032 = PN\nk=1 \u03c1(l)\nk \u03c1(l\u2032)\nk . Introducing the matrix W with entries wkl and the N \u00d7 p\nmatrix Y =\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\nyT\n1...\nyT\nN\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\n, we have the simple expression\nF(y) = trace(Y T WY).\nNote that the constraints are Y T Y = IdRp and Y T1N = 0. Without this last constraint,\nwe know that an optimal solution is provided by Y = [e1,...,ep] where e1,...,ep pro-\nvide an orthonormal family of eigenvectors associated to the p smallest eigenvalues\nof W (this is a consequence of corollary 2.4). To handle the additional constraint, it\nsuffices to note that W1N = 0, so that 1N is a zero eigenvector. Given this, it suffices\nto compute p + 1 eigenvectors associated to smallest eigenvalues of W, e1,...,ep+1,\nwith the condition that e1 = \u00b11N/\n\u221a\nN (which is automatically satisfied unless 0 is a\nmultiple eigenvalue of W) and let\nY = [e2,...,ep+1].\nNote that e2,...,ep+1 are also the p smallest eigenvectors of W + \u03bb11T for any large\nenough \u03bb, e.g., \u03bb > trace(W)/N.\nLLE is summarized in the following algorithm.\nAlgorithm 21.2 (Local linear embedding)\nThe input of the algorithm is\n(i) Either a training set T = (x1,...,xN), or its Gram matrix S containing all\ninner products xT\nk xl (or more generally inner products in feature space), or a dissim-\nilarity matrix D = (dkl).\n(ii) An integer c for the graph construction.\n(iii) An integer p for the target dimension.\n(1) If not provided in input, compute the Gram matrix S and distance matrix D\n(using (21.2) and (21.4)).\n(2) Build the c-nearest-neighbor graph associated with the distances. Let Nk be the\nset of neighbors of k, with ck = |Nk|.\n(3) For k = 1,...,N, let Sk be the sub-matrix of S matrix associated with xl,l \u2208Nk\nand compute coefficients \u03c1(l)\nk ,l \u2208Nk stacked in a vector \u03c1k \u2208Rck by solving (21.6).\n(4) Form the matrix W with entries wll\u2032 = PN\nk=1 \u03c1(l)\nk \u03c1(l\u2032)\nk\nwith \u03c1 extended so that \u03c1(k)\nk\n=\n\u22121 and \u03c1(l)\nk = 0 if l , k and l < Nk.\n21.2. MANIFOLD LEARNING\n561\nFigure 21.3: Local linear embedding with target dimension 3 applied to the data in fig. 21.1\nand fig. 21.2.\n(5) Compute the first p + 1 eigenvectors, e1,...,ep+1, of W (associated with smallest\neigenvalues) arranging for e1 to be proportional to 1N.\n(6) Set y(i)\nk = e(k)\ni+1 for i = 1,...,p and k = 1,...,N.\nThe results of LLE applied to the datasets described in fig. 21.1 and fig. 21.2 are\nprovided in fig. 21.3.\nRemark 21.1 We note that, for both Isomap and LLE, the c-nearest-neighbors graph\ncan be replaced by the graph formed with edges between all pairs of points that are\nat distance less than \u03f5 from each other, for a chosen \u03f5 > 0, with no change in the\nalgorithms.\nThese parameters (c or \u03f5) must be chosen carefully and may have an important\nimpact on the output of the algorithm. Choosing them too small would not allow\nfor a correct estimation of distances in Isomap (with possibly some of them being\ninfinite if the graph has more than one connected component), or of the linear ap-\nproximations in LLE. However, choosing them too large may break the basic hypoth-\nesis that the data is locally Euclidean or linear that form the basic principles of these\nalgorithms.\n\u2666\n21.2.3\nGraph Embedding\nBoth Isomap and LLE are based on the construction of a nearest-neighbor graph\nbased on dissimilarity data and the conservation of some of its geometric features\nwhen deriving a small-dimensional representation. For LLE, a weight matrix W was\nfirst estimated based on optimal linear approximations of xk by its neighbors, and\n562\nCHAPTER 21. DATA VISUALIZATION AND MANIFOLD LEARNING\nthe representation was computed by estimating the eigenvectors associated with the\nsmallest eigenvalues of W (excluding the eigenvector proportional to 1). However,\nboth methods were motivated by the intuition that the dataset was supported by\na continuous small-dimensional manifold. We now discuss methods that are solely\nmotivated by the discrete geometry of a graph, for which we use tools that are similar\nto our discussion of graph clustering in section 19.5.\nAdapting the notation in that section to the present one, we start with a graph\nwith N vertices and weights \u03b2kl between these vertices (such that \u03b2ll = 0) and we\nform the Laplacian operator defined by, for any vector u \u2208RN:\n1\n2\u2225u\u2225H1 = 1\n2\nN\nX\nl,l\u2032=1\n\u03b2ll\u2032(u(l) \u2212u(l\u2032))2 = uT Lu,\nso that L is identified as the matrix with coefficients \u2113ll\u2032 = \u2212\u03b2ll\u2032 for l , l\u2032 and \u2113ll =\nPN\nl\u2032=1 \u03b2ll\u2032. The matrix W that was obtained for LLE coincides with this graph Lapla-\ncian if one lets \u03b2ll\u2032 = \u2212wll\u2032 for l , l\u2032, since we have PN\nl\u2032=1 wll\u2032 = 0. The usual require-\nment that weights are non-negative is no real loss of generality, because in LLE (and\nin the Graph embedding method above), one is only interested in eigenvectors of W\n(or L below) that are perpendicular to 1, and those remain the same if one replaces\nW by\nW \u2212a1N1T\nN + NaIdRN\nwhich has negative off-diagonal coefficients \u02dcwll\u2032 = wll\u2032 \u2212a for large enough a.\nIn graph (or Laplacian) embedding, the starting point is a weighted graph on\n{1,...,N} with edge weights \u03b2ll\u2032 interpreted as similarities between vertexes. These\nweights may or may not be deduced from measures of dissimilarity (dll\u2032,k,l = 1,...,N)\nwhich themselves may or may not be computed as distances between training data\nx1,...,xN. If one starts with dissimilarities, it is typical to use simple transformations\nto compute edge weights, and one the most commonly used is\n\u03b2ll\u2032 = exp(\u2212d2\nll\u2032/2\u03c42)\nfor some constant \u03c4. These weights are usually truncated, replacing small values\nby zeros (or the computation is restricted to nearest neighbors), to ensure that the\nresulting graph is sparse, which speeds up the computation of eigenvectors for large\ndatasets.\nGiven a target dimension p, the graph is then represented as a collection of points\ny1,...,yN \u2208Rp, where yk is associated to vertex k. For this purpose, one needs to com-\npute the first p +1 eigenvectors, e1,...,ep+1, of the graph Laplacian, with the require-\nment that e1 = \u00b11N/\n\u221a\nN. (This is always possible and can be achieved numerically\nby computing eigenvectors of L+c11T for large enough c.) The graph representation\n21.2. MANIFOLD LEARNING\n563\nis then given by y(|)\nk i = e(k)\ni+1 for i = 1,...,p and k = 1,...,N. Note that these are exactly\nthe same operations as those described in steps 4 and 5 of the LLE algorithm.\nOne way to interpret this construction is that e2,...,ep+1 (the coordinate functions\nfor the representation y1,...,yN) minimize\np\nX\nj=1\n\u2225ei\u22252\nH1\nsubject to e2,...,ep+1 being perpendicular to each other and perpendicular to the\nconstant functions (these constraints being justified for the same reasons as those\ndiscussed for LLE). Small H1 semi-norms being associated with smoothness on the\ngraph, we see that we are looking for the smoothest zero-mean representation of the\ndata.\nBased on our discussion of LLE, we can make an alternative interpretation by\nintroducing a symmetric square root R of the Laplacian matrix L or any matrix such\nthat RRT = L. Writing R = [\u03c11,...,\u03c1N], one has\nL =\nN\nX\nk=1\n\u03c1k\u03c1T\nk\nand PN\nk=1 \u03c1k = 0. With this notation, we can interpret Laplacian embedding as the\nminimization of\nN\nX\nk=1\n\f\f\f\f\f\f\f\nN\nX\nl=1\n\u03c1(l)\nk yl\n\f\f\f\f\f\f\f\n2\n(subject to previous orthogonality constraints). In other terms, y1,...,yN are deter-\nmined so that the linear relationships\n\u03c1kkyk = \u2212\nX\nl,k\n\u03c1(l)\nk yl\nare satisfied, which is similar to the LLE condition, without the requirement that\n\u03c1k(k) = 1.\nAn alternate requirement that could have been made for LLE is that PN\nl=1(\u03c1(l)\nk )2 =\n1 for all k. Instead of having to solve a linear system in step 2 of Algorithm 21.2, one\nwould then compute an eigenvector with smallest eigenvalue of Sk. For graph em-\nbedding, this constraint can be enforced by modifying the Laplacian matrix, since\nPN\nl=1(\u03c1(l)\nk )2 is just the (k,k) coefficient of RRT . Given this, let D be the diagonal matrix\nformed by the diagonal elements of L, and define the so-called \u201csymmetric Lapla-\ncian\u201d \u02dcL = D\u22121/2LD\u22121/2. One obtain an alternative, and popular, graph embedding\nmethod by replacing e1,...,ep+1 above by the first p eigenvectors of \u02dcL.\n564\nCHAPTER 21. DATA VISUALIZATION AND MANIFOLD LEARNING\nAnother interpretation of this representation can be based on the random walk\nassociated with the graph structure. Consider the random process t 7\u2192q(t) defined\nas follows. The initial position, q(0) is selected according to some arbitrary dis-\ntribution, say \u03c00. Conditional to q(t) = k, the next position is determined by set-\nting random waiting times \u03c4kl, each distributed as an exponential distribution with\nrate \u03b2kl (or expectation 1/\u03b2kl), and the process moves to the position l for which\n\u03c4kl is smallest after waiting for that time. Let P(t) be the matrix with coefficients\nP(t,k,l) = P(q(t + s) = l | q(s) = k). Then, one has\nP(t) = e\u2212tL\nwhere the right-hand side is the matrix exponential. If \u03bb1 = 0 \u2264\u03bb2 \u2264\u00b7\u00b7\u00b7 \u2264\u03bbN are the\neigenvalues of L with corresponding eigenvectors e1,...,eN, then\nP(t) =\nN\nX\ni=1\ne\u2212t\u03bbieieT\ni\nIn particular, restricting the first eigenvectors of L provides an approximation of this\nstochastic process, i.e.,\nP(t) \u224311T\nN +\np\nX\ni=1\ne\u2212t\u03bbi+1y(i)y(i)T .\nWe could also have considered the discrete-time version of the walk, for which,\nconsidering integer times t \u2208N,\nP(q(t + 1) = l | q(t) = k) =\n\uf8f1\uf8f4\uf8f4\uf8f4\uf8f4\uf8f2\uf8f4\uf8f4\uf8f4\uf8f4\uf8f3\n\u03b2kl\nPN\nl\u2032=1,l\u2032,k \u03b2kl\u2032\nif l , k\n0 if l = k\nIntroducing the matrix B of similarities \u03b2kl (with zero on the diagonal) and the diag-\nonal matrix D with coefficients dkk = PN\nl=1,l,k \u03b2kl, the r.h.s. of the previous equation\nis the k,l entry of the matrix \u02dcP = D\u22121B. Then, for any integer s, P(q(t+s) = l | q(t) = k)\nis the k,l entry if \u02dcPs = D\u22121/2(D\u22121/2BD\u22121/2)sD1/2.\nThe Laplacian matrix L is given by L = D \u2212B. The normalized Laplacian is\n\u00afL = D\u22121/2LD\u22121/2 = IdRN \u2212D\u22121/2BD\u22121/2\nso that\n\u02dcPs = D\u22121/2(IdRN \u2212\u00afL)sD1/2.\n21.2. MANIFOLD LEARNING\n565\nIf one introduces the eigenvectors \u00afe1,..., \u00afeN of the normalized Laplacian, still asso-\nciated with non-decreasing eigenvalues \u00af\u03bb1 = 0,..., \u00af\u03bbN, and arranges without loss of\ngenerality that \u00afe1 \u221dD1/21N, then\n\u02dcPs = D\u22121/2\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\nN\nX\ni=1\n(1 \u2212\u00af\u03bbi)s \u00afei \u00afeT\ni\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8D1/2.\nThis shows that, for s large enough, the transitions of this Markov chain are well ap-\nproximated by its first terms, suggesting using the alternative representation based\non the normalized Laplacian:\n\u00afyk(i) = \u00afei+1(k).\nBoth representations (using normalized or un-normalized Laplacians) are commonly\nused in practice.\n21.2.4\nStochastic neighbor embedding\nGeneral algorithm\nStochastic neighbor embedding (SNE, Hinton and Roweis [90]), and its variant (t-\nSNE, Maaten and Hinton [122]) have become a popular tool for the visualization\nof high-dimensional data based on dissimilarity matrices. One of the key contri-\nbutions of this algorithm is to introduce a local data rescaling step, that allows for\nvisualization of more homogeneous point clouds.\nAssume that dissimilarities D = (dkl,k,l = 1,...,N) are observed. The basic prin-\nciple in SNE is to deduce from the dissimilarities a family of N probability distribu-\ntions on {1,...,N}, that we will denote \u03c0k, k = 1,...,N, with the property that \u03c0k(k) =\n0. The computation of these probabilities include the local normalization step, and\nwe will return to this later. Given the \u03c0k\u2019s, one then estimate low-dimensional rep-\nresentations y = (y1,...,yN) such that \u03c0k \u2243\u03c8k where \u03c8k is given by\n\u03c8k(l;y) =\nexp\n\u0010\n\u2212\u03b2\n\u0010\n|yk \u2212yl|2\u0011\u0011\nPN\nl\u2032=1,l\u2032,k exp\n\u0010\n\u2212\u03b2\n\u0010\n|yk \u2212yl\u2032|2\u0011\u00111l,k.\nHere, \u03b2 : [0,+\u221e) \u2192[0,+\u221e) is an increasing differentiable function that tends to +\u221e\nat infinity. The derivative is denoted \u2202\u03b2. The original version of SNE [90] uses\n\u03b2(t) = t and t-SNE [122] takes \u03b2(t) = log(1 + t).\nThe determination of the representation can then be performed by minimizing a\nmeasure of discrepancy between the probabilities \u03c0k and \u03c8k. In Hinton and Roweis\n[90], it is suggested to minimize the sum of Kullback-Liebler divergences, namely\nN\nX\nk=1\nKL(\u03c0k\u2225\u03c8k(\u00b7;y))\n566\nCHAPTER 21. DATA VISUALIZATION AND MANIFOLD LEARNING\nor, equivalently, to maximize\nF(y) =\nN\nX\nk,l=1\n\u03c0k(l)log\u03c8k(l;y)\n= \u2212\nN\nX\nk,l=1\n\u03b2(|yk \u2212yl|2)\u03c0k(l) +\nN\nX\nk=1\nlog\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\nN\nX\nl=1,l,k\nexp(\u2212\u03b2(|yk \u2212yl|2))\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\nThe gradient of this function can be computed by evaluating the derivative at \u03f5 = 0\nof f : \u03f5 7\u2192F(y + \u03f5h). This computation gives\nf \u2032(0) = \u22122\nN\nX\nk,l=1\n\u2202\u03b2(|yk \u2212yl|2)(yk \u2212yl)T (hk \u2212hl)\u03c0k(l)\n+ 2\nN\nX\nk=1\nN\nX\nl=1\n\u2202\u03b2(|yk \u2212yl|2)(yk \u2212yl)T (hk \u2212hl)\u03c8k(l;y)\n= \u22122\nN\nX\nk=1\nhT\nk\nN\nX\nl=1\n\u2202\u03b2(|yk \u2212yl|2)(yk \u2212yl)(\u03c0k(l) + \u03c0l(k) \u2212\u03c8k(l;y) \u2212\u03c8l(k;y))\nThis shows that\n\u2202ykF(y) = \u22122\nN\nX\nl=1\n\u03b2(|yk \u2212yl|2)(yk \u2212yl)(\u03c0k(l) + \u03c0l(k) \u2212\u03c8k(l;y) \u2212\u03c8l(k;y)).\nThis is a rather simple expression that can be used with any first-order opti-\nmization algorithm to maximize F. The algorithm in Hinton and Roweis [90] uses\ngradient ascent with momentum, namely iterating\ny(n+1) = y(n) + \u03b3\u2207F(y(n)) + \u03b1(n)(y(n) \u2212y(n\u22121))\nChoosing \u03b1(n) = 0 provides standard gradient ascent with fixed gain \u03b3 (of course,\nother optimization methods may be used). The momentum can be interpreted, in a\nloose sense, as a \u201cfriction term\u201d.\nA variant of the algorithm replaces the node-dependent probabilities \u03c0k by a sin-\ngle, symmetric, joint distribution \u00af\u03c0 on {1,...,N}2, (k,l) 7\u2192\u00af\u03c0(k,l), satisfying \u00af\u03c0(k,k) = 0\nand \u00af\u03c0(k,l) = \u00af\u03c0(l,k). The target distribution \u00af\u03c8 then becomes\n\u00af\u03c8(k,l;y) =\nexp(\u2212\u03b2(|yk \u2212yl|2))\nPN\nk\u2032,l\u2032=1 exp(\u2212\u03b2(|yk\u2032 \u2212yl\u2032|2))\n.\n21.2. MANIFOLD LEARNING\n567\nWith such a choice, the objective function has a simpler form, namely minimizing\nKL( \u00af\u03c0\u2225\u00af\u03c8(\u00b7,y)) or maximizing the expected likelihood\n\u00afF(y) =\nN\nX\nk,l=1\n\u00af\u03c0(k,l)log \u00af\u03c8(k,l;y) = \u2212\nN\nX\nk,l=1\n\u03b2(|yk \u2212yl|2) \u00af\u03c0(k,l) + log\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\nN\nX\nk,l=1\nexp(\u2212\u03b2(|yk \u2212yl|2))\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8.\nThe gradient of this symmetric version of F can be computed similarly to the previ-\nous one and is given by\n\u2202yk \u00afF(y) = \u22124\nN\nX\nl=1\n\u2202\u03b2(|yk \u2212yl|2)(yk \u2212yl)( \u00af\u03c0(k,l) \u2212\u00af\u03c8(k,l;y)).\nSetting initial probabilities\nThe probabilities \u03c0k(l) or \u00af\u03c0(k,l) are deduced from the dissimilarities as\n\u03c0k(l) =\ne\u2212d2\nkl/2\u03c32\nk\nPN\nl\u2032=1,l\u2032,k e\u2212d2\nkl\u2032/2\u03c32\nk\nfor l , k and\n\u00af\u03c0(k,l) = \u03c0k(l) + \u03c0l(k)\n2n\n.\nThe coefficients \u03c32\nk , k = 1,...,N operate the local normalization, justifying, in\nparticular, the parameter-free expression chosen for \u03c8 and \u00af\u03c8. These coefficients are\nestimated so as to adjust the entropies of all \u03c0k to a fixed value, which is a parameter\nof the algorithm. Note that, letting t = 1/2\u03c32\nk and H(\u03c0k) = \u2212PN\nl=1 \u03c0k(l)log\u03c0k(l),\n\u2202tH(\u03c0k) = \u2212\nN\nX\nl=1\n\u2202t\u03c0k(l)log\u03c0k(l) \u2212\nN\nX\nl=1\n\u2202t\u03c0k(l)\n= \u2212\nN\nX\nl=1\n\u2202t\u03c0k(l)log\u03c0k(l)\nNow\n\u2202t log\u03c0k(l) = \u2212d2\nkl + \u00afd2\nk\nwith \u00afd2\nk = PN\nl\u2032=1 d2\nkl\u2032\u03c0k(l\u2032). Writing \u2202t\u03c0k(l) = \u03c0k(l)\u2202t log\u03c0k(l), we have\n\u2202tH(\u03c0k) =\nN\nX\nl=1\n(dkl log\u03c0k(l))\u03c0k(l) \u2212\u00afdk\nN\nX\nl=1\n\u03c0k(l)log\u03c0k(l).\n568\nCHAPTER 21. DATA VISUALIZATION AND MANIFOLD LEARNING\nUsing Schwartz inequality, we see that \u2202tH(\u03c0k) \u22640 so that H(\u03c0k) is decreasing as\na function of t, i.e., increasing as a function of \u03c32\nk . When \u03c32\nk \u21920, \u03c0k converges to\nthe uniform distribution on the set of nearest neighbors of k (the indexes l , k such\nthat d2\nkl is minimal) and, letting \u03bdk denote their number, which is typically equal to\n1, H(\u03c0k) converges to log\u03bdk. When \u03c32\nk tends to infinity, \u03c0k converges to the uniform\ndistribution over indexes l , k, whose entropy is log(N \u22121). This shows that eH(\u03c0k),\nwhich is called the perplexity of \u03c0k can take any value between \u03bdk and N \u22121. The\ncommon target value of the perplexity can therefore be taken anywhere between\nmaxk \u03bdk and N \u22121. In Maaten and Hinton [122], it is recommended to choose a value\nbetween 5 and 50.\nRemark 21.2 The complexity of the computation of the gradient of the objective\nfunction (either F or \u00afF ) scales like the square of the size of the training set, which\nmay be prohibitive when N is large. In Van Der Maaten [193], an accelerated proce-\ndure, that involves an approximation of the gradient is proposed. (This procedure is\nhowever limited to representations in dimensions 2 or 3.)\n\u2666\n21.2.5\nUniform manifold approximation and projection (UMAP)\nUMAP is similar in spirit to t-SNE, with a few important differences that result in\na simpler optimization problem and faster algorithms. Like Isomap, the approach\nis based on matching distances between the high-dimensional data and the low-\ndimensional representation. But while Isomap estimates a unique distance on the\nwhole training set (the geodesic distance on the nearest-neighbor graph), UMAP\nestimates as many \u201clocal distances\u201d as observations before \u201cpatching\u201d them to form\nthe final representation.\nThe goal of transporting possibly non-homogeneous locally defined objects on\ninitial data to a homogeneous low-dimensional visualization is what makes UMAP\nsimilar to t-SNE. The difference is that t-SNE transports local probability distri-\nbutions, while UMAP transports metric spaces.\nMore precisely, given distances\n(dkl,k,l = 1,...,N) and an integer m provided as input, the algorithm builds, for\neach k = 1,...,N a (pseudo-)metric \u03b4k on the associated data graph by letting\n\u03b4(k)(k,l) = \u03b4(k)(l,k) = 1\n\u03c3k\n\u0012\ndkl \u2212min\nl\u2032,k dkl\u2032\n\u0013\nif l is among the m nearest neighbors of k, where m is a parameter of the algorithm,\nwith all other distances being infinite. The normalization parameter \u03c3k has a role\nsimilar to that of the same parameter in t-SNE in that it tends to make the represen-\ntation homogeneous. Here, it is computed such that\nX\nl\nexp(\u2212\u03b4(k)(l,l\u2032)) = log2 m.\n21.2. MANIFOLD LEARNING\n569\nEach such metric provides a weighted graph structure on {1,...,N} by defining\nweights w(k)\nll\u2032 = exp(\u2212\u03b4(k)(l,l\u2032)). In UMAP, these weights are interpreted in the frame-\nwork of fuzzy sets, where a fuzzy set is defined by a pair (A,\u00b5) where A is a set and\n\u00b5 a function \u00b5 : A \u2192[0,1] [210]. The function \u00b5 is called the membership func-\ntion and \u00b5(x) for x \u2208A is the membership strength of x to A. Letting V = {1,...,N}\nand E = V \u00d7 V, one then interprets the weights as defining the membership strength\nof edges to the graph, i.e., one defines the \u201cfuzzy graph\u201d G(k) = (V,E,\u00b5(k)) where\n\u00b5(k)(l,l\u2032) = w(k)\nll\u2032 is the membership strength of edge (l,l\u2032) to G(k).\nThis is, of course, just a reinterpretation of weighted graphs in terms of fuzzy\nsets, but it allows one to combine the collection (G(k),k = 1,...,N) using simple fuzzy\nsets operations, namely, defining the combined (fuzzy) graph G = (V,E,\u00b5) with\n(E,\u00b5) =\nN\n[\nk=1\n(E,\u00b5(k))\nbeing the fuzzy union of the edge sets. There are, in fuzzy logic, multiple ways\nto define set unions [85], and the one selected for UMAP define (A,\u00b5) \u222a(A\u2032,\u00b5\u2032) =\n(A\u222aA\u2032,\u03bd) with \u03bd(x) = \u00b5(x)+\u00b5\u2032(x)\u2212\u00b5(x)\u00b5\u2032(x) (\u00b5(x) and \u00b5\u2032(x) being defined as 0 is x < A\nor x < A\u2032 respectively). In UMAP, each edge \u00b5(k)(l,l\u2032) is non-zero only is k = l or l\u2032 so\nthat\n\u00b5(l,l\u2032) = w(l)\nll\u2032 + w(l\u2032)\nll\u2032 \u2212w(l)\nll\u2032w(l\u2032)\nll\u2032 .\nThis defines an input fuzzy graph structure on {1,...,N} that serves as target for\nan optimized similar structured associated with the representation y = (y1,...,yN).\nThis representation, since it is designed as a homogeneous representation of the\ndata, provides a unique fuzzy graph H(y) = (V,E,\u03bd(\u00b7;y)) and the edge membership\nfunction is defined by \u03bd(l,l\u2032;y) = \u03d5a,b(yl,yl\u2032) with\n\u03d5a,b(y,y\u2032) =\n1\n1 + a|y \u2212y\u2032|b .\nThe parameters a and b are adjusted so that \u03d5a,b provides a differentiable approxi-\nmation of the function\n\u03c8\u03c10(y,y\u2032) = exp(\u2212max(0,|y \u2212y\u2032| \u2212\u03c10))\nwhere \u03c10 is an input parameter of the algorithm. This function \u03c8\u03c10 takes the same\nform as the membership function defined for local graphs G(k), and its replacement\nby \u03d5a,b makes possible the use of gradient-based methods for the determination of\nthe optimal y (\u03c8\u03c10 is not differentiable everywhere).\nThe representation y is optimized by minimizing the \u201cfuzzy set cross-entropy\u201d\nC(\u00b5\u2225\u03bd(\u00b7,y)) =\nX\n(k,l)\u2208E\n \n\u00b5(k,l)log \u00b5(k,l)\n\u03bd(k,l|y) + (1 \u2212\u00b5(k,l))log 1 \u2212\u00b5(k,l)\n1 \u2212\u03bd(k,l|y)\n!\n570\nCHAPTER 21. DATA VISUALIZATION AND MANIFOLD LEARNING\nor, equivalently, maximizing (using, for short, \u03d5 = \u03d5a,b)\nF(y) =\nX\n(k,l)\u2208E\n(\u00b5(k,l)log\u03bd(k,l|y) + (1 \u2212\u00b5(k,l))log(1 \u2212\u03bd(k,l|y)))\n=\nX\n(k,l)\u2208E\n(\u00b5(k,l)log\u03d5(yk,yl) + (1 \u2212\u00b5(k,l))log(1 \u2212\u03d5(yk,yl)))\nNote the important simplification compared to the similar function F is t-SNE,\nin that the logarithm of a potentially large sum is avoided. We have\n\u2202ykF(y) =2\nN\nX\nl=1\n\u00b5(k,l)\u2202yk log\u03d5(yk,yl) + 2\nN\nX\nl=1\n(1 \u2212\u00b5(k,l))\u2202yk log(1 \u2212\u03d5(yk,yl))\n=2\nN\nX\nl=1\n\u00b5(k,l)\u2202yk log\n\u03d5(yk,yl)\n1 \u2212\u03d5(yk,yl) + 2\nN\nX\nl=1\n\u2202yk log(1 \u2212\u03d5(yk,yl)).\nThe optimization can be implemented using stochastic gradient ascent. Introduce\nrandom variables \u03bekl and \u03be\u2032\nkl both taking value in {0,1}, all independent of each\nother and such that P(\u03bekl = 1) = \u00b5kl and P(\u03be\u2032\nkl = 1) = \u03f5. Define\nHk(y,\u03be,\u03be\u2032) = 2\nN\nX\nl=1\n\u03bekl\u2202yk log\n\u03d5(yk,yl)\n1 \u2212\u03d5(yk,yl) + 2ck\nN\nX\nl=1\nN\nX\nl\u2032=1\n\u03bekl\u03be\u2032\nkl\u2032\u2202yk log(1 \u2212\u03d5(yk,yl\u2032)).\nThen, if one takes ck = 1/(\u03f5P\nl \u00b5(k,l)) one has\nE(Hk(y,\u03be,\u03be\u2032)) = \u2202ykF(y).\nThis corresponds to SGA iterations in which:\n(1) Each edge (k,l) is selected with probability \u00b5(k,l) (which are zero for unless k\nand l are neighbors);\n(2) If (k,l) is selected, one selects an additional edges (k,l\u2032) each with probability \u03f5.\nLetting l1,...,lm be the number of edges selected, yk is updated according to\nyk \u2190yk + 2\u03b3\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\u2202yk log\n\u03d5(yk,yl)\n1 \u2212\u03d5(yk,yl) + ck\nm\nX\nj=1\n\u2202yk log(1 \u2212\u03d5(yk,yl\u2032))\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8.\nRemark 21.3 If one prefers using probability rather than fuzzy set theory, the graphs\nG(k) may also be interpreted as random graphs in which edges are added indepen-\ndently from each other and each edge (l,l\u2032) is drawn with probability \u00b5(k)(l,l\u2032). The\n21.2. MANIFOLD LEARNING\n571\ncombined graph G is then the random graph in which (l,l\u2032) is present if and only\nif it is in at least one of the G(k) and the objective function C coincides with the KL\ndivergence between this random graph and the random graph similarly defined for\ny.\nHowever, this fuzzy/random graph formulation of UMAP\u2014which corresponds\nto current practical implementations\u2014is only a special case of the theoretical con-\nstruction made in McInnes et al. [130] which builds on the theory of (fuzzy) simpli-\ncial sets and their representation of metric spaces. We refer the interested reader to\nthis reference, which requires a mathematical background beyond the scope of these\nnotes.\n\u2666\n572\nCHAPTER 21. DATA VISUALIZATION AND MANIFOLD LEARNING\nChapter 22\nGeneralization Bounds\nWe provide, in this chapter, an introduction to some theoretical aspects of statistical\n(or machine) learning, mostly focusing on the derivation of \u201cgeneralization bounds\u201d\nthat provide high-probability guarantees on the generalization error of predictors\nusing training data. While these bounds are not always of practical use, because\nmaking them small in realistic situations would require an enormous amount of\ntraining data, their derivations and the form they take for specific model classes\nbring important insight on the structure of the learning problem, and help under-\nstand why some methods may perform well while others do not.\n22.1\nNotation\nWe here recall some notation introduced in chapter 5. We consider a pair of random\nvariables (X,Y), with X : \u2126\u2192RX and Y : \u2126\u2192G. Regression problems correspond\nto RY = R (or Rq if multivariate) and classification to RY being a finite set. A pre-\ndictor is a function f : RX \u2192RY. The general prediction problem is to find such\na predictor within a class of functions, denoted F , minimizing the prediction (or\ngeneralization error)\nR(f ) = E(r(Y,f (X)))\nwhere r : RY \u00d7 RY \u2192[0. + \u221e) is a risk function.\nA training set is a family T = ((x1,y1),...,(xN,yN)) \u2208(RX \u00d7 RY)N, the set T of all\npossible training sets therefore being the set of all finite sequences in RX \u00d7 RY. A\ntraining algorithm can then be seen as a function A : T \u2192F which associates to\neach training set T a function A(T) = \u02c6fT .\n573\n574\nCHAPTER 22. GENERALIZATION BOUNDS\nGiven T \u2208T , The training set error associated to a function f \u2208F is\n\u02c6RT (f ) = 1\n|T |\nX\n(x,y)\u2208T\nr(y,f (x)))\nand the in-sample error associated to a learning algorithm is the function T 7\u2192ET\n\u2206=\n\u02c6RT ( \u02c6fT ). Fixing the size (N) of T, one also considers the random variable T with\nvalues in T distributed as an N-sample of the distribution of (X,Y).\nA good learning algorithm should be such that the generalization error R( \u02c6fT ) is\nsmall, at least in average (i.e., E(R( \u02c6fT )) is small). Our main goal in this chapter is to\ndescribe generalization bounds trying to find upper-bounds for R( \u02c6fT ) based on ET\nand properties of the function class F . These bounds will reflect the bias-variance\ntrade-off, in that, even though large function classes provide smaller in-sample er-\nrors, they will also induce a large additive term in the upper-bound, accounting for\nthe \u201cvariance\u201d associated to the class.\nRemark 22.1 Both variables X and Y are assumed to be random in the previous\nsetting, but there are often situations when one of them is \u201cmore random\u201d than the\nother. Randomness in Y is associated to measurement errors, or ambiguity in the\ndecision. Randomness in X more generally relates to the issue of sampling a dataset\nin a large dimensional space. In some cases, Y is not random at all: for example,\nin object recognition, the question of assigning categories for images such as those\ndepicted in fig. 22.1 has a quasi-deterministic answer. Sometimes, it is X who is\nnot random, for example when observing noisy signals where X is a deterministic\ndiscretization of a time interval and Y is some function of X perturbed by noise.\n\u2666\n22.2\nPenalty-based Methods and Minimum Description Length\n22.2.1\nAkaike\u2019s information criterion\nWe make a computation under the following assumptions. We assume a regression\nmodel Y = f\u03b8(X) + \u03f5 where \u03f5 \u223cN (0,\u03c32) and f is some function parametrized by\n\u03b8 \u2208Rm. We also assume that the true distribution is actually covered by this model\nand represented by a parameter \u03b80. Let \u02c6\u03b8T denote the parameter estimated by least\nsquares using a training set T, and denote for short \u02c6fT = f \u02c6\u03b8T .\nThe in-sample error is\nET = 1\nN\nN\nX\nk=1\n(yk \u2212\u02c6fT (xk))2.\n22.2. PENALTY-BASED METHODS AND MINIMUM DESCRIPTION LENGTH575\nFigure 22.1: Images extracted from the PASCAL challenge 2007 dataset [70], in which cate-\ngories must be associated with images. There is little ambiguity on correct answers based on\nobserving the image, i.e., little randomness in the variable Y.\nWe want to compare the training-set-averaged prediction error and the average in-\nsample error, namely compute the error bias\n\u2206N = E(R(fT )) \u2212E(ET ).\nWrite\n\u2206N = E(R(fT )) \u2212R(f\u03b80) + R(f\u03b80) \u2212E(ET ).\nWe make a heuristic argument to evaluate \u2206N. We can use the fact that \u02c6\u03b8T mini-\nmizes the empirical error and write\n1\nN\nN\nX\nk=1\n(Yk \u2212f\u03b80(Xk))2 = ET + \u03c32( \u02c6\u03b8T \u2212\u03b80)T JT ( \u02c6\u03b8T \u2212\u03b80) + o(| \u02c6\u03b8T \u2212\u03b80|2)\nwith\nJT =\n1\n2\u03c32N\nN\nX\nk=1\n\u22022\n\u03b8((yk \u2212f\u03b8(xk))2)|\u03b8\u2212\u02c6\u03b8T ,\nwhich is an m by m symmetric matrix.\nNow, using the fact that \u03b80 minimizes the mean square error (since f\u03b80(x) =\nE(Y|X = x)), we can write, for any T :\nR(fT ) = R(f\u03b80) + \u03c32( \u02c6\u03b8T \u2212\u03b80)T I( \u02c6\u03b8T \u2212\u03b80) + o(| \u02c6\u03b8T \u2212\u03b80|2)\n576\nCHAPTER 22. GENERALIZATION BOUNDS\nwith\nI =\n1\n2\u03c32E(\u22022\n\u03b8(Y \u2212f\u03b8(X))2\n|\u03b8\u2212\u03b80).\nAs a consequence, we can write (taking expectations in both Taylor expansions)\n\u2206N = \u03c32E\n\u0010\n( \u02c6\u03b8T \u2212\u03b80)T JT ( \u02c6\u03b8T \u2212\u03b80)\n\u0011\n+ \u03c32E\n\u0010\n( \u02c6\u03b8T \u2212\u03b80)T I( \u02c6\u03b8T \u2212\u03b80)\n\u0011\n+ o(E(| \u02c6\u03b8T \u2212\u03b80|2)).\n(We skip hypotheses and justification for the analysis of the residual term.)\nWe now note that, because we are assuming a Gaussian noise, and that the true\ndata distribution belongs to the parametrized family, the least-square estimator is\nalso a maximum likelihood estimator. Indeed, the likelihood of the data is\n1\n(2\u03c0\u03c32)N/2 exp\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\u22121\n2\u03c32\nN\nX\nk=1\n(Yk \u2212f\u03b8(Xk))2\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\nN\nY\nk=1\n\u03d5X(Xk)\nwhere \u03d5X is the p.d.f. of X and does not depend on the unknown parameter.\nWe can therefore apply classical results from mathematical statistics [194]. Un-\nder some mild smoothness assumptions on the mapping \u03b8 7\u2192f\u03b8, \u02c6\u03b8T converges to\n\u03b80 in probability when N tends to infinity, the matrix JT converges to I, which\nis the model\u2019s Fisher information matrix, and\n\u221a\nN( \u02c6\u03b8T \u2212\u03b80) converges in distribu-\ntion to a Gaussian N (0,I\u22121) . This implies that both N( \u02c6\u03b8T \u2212\u03b80)T JT ( \u02c6\u03b8T \u2212\u03b80) and\nN( \u02c6\u03b8T \u2212\u03b80)T I( \u02c6\u03b8T \u2212\u03b80) converge to a chi-square distribution with m degrees of free-\ndom, whose expectation is m, which indicates that \u2206N has order 2\u03c32m/N.\nThis analysis can be used to develop model selection rules, in which one chooses\nbetween models of dimensions k1 < k2 < \u00b7\u00b7\u00b7 < kq = m (e.g., by truncating the last\ncoordinates of X). The rule suggested by the previous computation is to select j\nminimizing\nE(j)\nT ( \u02c6fT ) +\n2\u03c32kj\nN\n,\nwhere E(j) is the in-sample error computed using the kj-dimensional model. This\nis an example of a penalty-based method, using the so-called Akaike\u2019s information\ncriterion (AIC) [2].\n22.2.2\nBayesian information criterion and minimum description length\nOther penalty-based methods are more size-averse and replace the constant, 2, in\nAIC by a function of N = |T |, for example logN. Such a change can be justified by a\nBayesian analysis, yielding the Bayesian information criterion (BIC) [174]. The ap-\nproach in this case is not based on an evaluation of the error, but on an asymptotic\n22.2. PENALTY-BASED METHODS AND MINIMUM DESCRIPTION LENGTH577\nestimation of the posterior distribution resulting from a Bayesian model selection\nprinciple. Like in the previous section, we content ourselves with a heuristic discus-\nsion.\nLet us consider a statistical model parametrized by \u03b8 \u2208\u0398, where \u0398 is an open\nconvex subset of Rm with p.d.f. given by\nf (z;\u03b8) = exp(\u03b8T U(z) \u2212C(\u03b8)),\nwith U : Rd \u2192Rmand z = (x,y). We are given a family of sub-models represented\nby M1,...,Mq, where, for each j, Mj is the intersection of \u0398 with a kj-dimensional\naffine subspace of Rm. We are also given a prior distribution for \u03b8 in which a sub-\nmodel is first chosen, with probabilities \u03b11,...,\u03b1q, and given that, say, Mj is se-\nlected, \u03b8 \u2208Mj is chosen with a probability distribution with density \u03d5j with respect\nto Lebesgue\u2019s measure on Mj (denoted dmj). Given training data T = (z1,...,zN),\nBayesian model selection consists in choosing the model Mj where j maximizes the\nposterior log-likelihood\n\u00b5(Mj|T ) = log\nZ\nRm \u03b1jeN(\u03b8T \u00afUT \u2212C(\u03b8))\u03d5jdmj(\u03b8)\nwhere \u00afUT = (U(z1) + \u00b7\u00b7\u00b7 + U(zN))/N.\nConsider the maximum likelihood estimator \u02c6\u03b8j within Mj, maximizing \u2113(\u03b8, \u00afUT ) =\n\u03b8T \u00afUT \u2212C(\u03b8) over Mj. Then one has\n\u2113(\u03b8, \u00afUT ) = \u2113( \u02c6\u03b8j, \u00afUT ) + 1\n2(\u03b8 \u2212\u02c6\u03b8j)T \u22022\n\u03b8\u2113( \u02c6\u03b8j, \u00afUT )(\u03b8 \u2212\u02c6\u03b8j) + Rj(\u03b8, \u02c6\u03b8j)|\u03b8 \u2212\u02c6\u03b8j|3\nNote that the first derivative of \u2113is \u2202\u03b8\u2113= \u00afU \u2212E\u03b8(U) where E\u03b8 is the expectation for\nf (\u00b7,\u03b8). The second derivative is \u2212var\u03b8(U) (showing that \u2113is concave) and the third\nderivative involves third-order moments of U for E\u03b8 and (like the second derivative)\ndoes not depend on \u00afUT . In particular, we can assume that, for any M > 0, there exists\na constant CM such that whenever max(|\u03b8|,| \u02c6\u03b8j|) \u2264M, we have Rj(\u03b8, \u02c6\u03b8j) \u2264CM.\nThe law of large numbers implies that \u00afUT converges to a limit when N tends to\ninfinity, and our assumptions imply that \u02c6\u03b8j converges to the parameter providing\nthe best approximation of the distribution of Z for the Kullback-Leibler divergence.\nIn particular, with probability 1, there exists an N such that \u02c6\u03b8j belongs to any large\nenough, but fixed, compact set. Moreover, the second derivative \u2113( \u02c6\u03b8j) will also con-\nverge to a limit, \u2212\u03a3j.\n578\nCHAPTER 22. GENERALIZATION BOUNDS\nFor any \u03f5 > 0, write\nZ\nRm \u03b1jeN(\u03b8T \u00afUT \u2212C(\u03b8))\u03d5jdmj(\u03b8)\n=\nZ\n|\u03b8\u2212\u02c6\u03b8j|\u2264\u03f5\neN(\u03b8T \u00afUT \u2212C(\u03b8))\u03d5jdmj(\u03b8) +\nZ\n|\u03b8\u2212\u02c6\u03b8j|\u2265\u03f5\neN(\u03b8T \u00afUT \u2212C(\u03b8))\u03d5jdmj(\u03b8).\nThe second integral converges to 0 exponentially fast when N tends to \u221e. The first\none behaves essentially like\nZ\nMj\ne\u22121\n2N(\u03b8\u2212\u02c6\u03b8j)T \u03a3\u22121\nj (\u03b8\u2212\u02c6\u03b8j)+log\u03d5j(\u03b8)dmj(\u03b8).\nNeglecting log\u03d5j(\u03b8), this integral behaves like (2\u03c0det(\u03a3j/N))\u22121/2, whose logarithm\nis (\u2212kj(logN)/2) plus constant terms. As a consequence, we find that\n\u00b5(Mj | T ) = max\n\u03b8\u2208Mj\n\u2113(\u03b8) \u2212\nkj\n2 logN + bounded terms.\nConsider, as an example, linear regression with Y = \u03b20 + bT x + \u03c32\u03bd where \u03bd is a\nstandard Gaussian random variable. Assume that the distribution of X is known, or,\npreferably, make the previous discussion conditional to X1,...,XN. Let sub-models\nMj correspond to the assumption that all but the first kj \u22121 coefficients of b van-\nish. Then, up to bounded terms, the Bayesian estimator must minimize (over such\nparameters b)\n1\n2\u03c32\nN\nX\nk=1\n(yk \u2212\u03b20 \u2212bT xk)2 +\nkj\n2 logN .\nor\nE(j)\nT +\nkj\u03c32\nN\nlogN .\nWe now turn to another interesting point of view, which provides the same penalty,\nbased on maximum description length principle (MDL; Rissanen [162]) measuring\nthe coding efficiency of a model.\nLet us fix some notation. We assume that one has q competing models for pre-\ndicting Y from X, for example, linear regression models based on different subsets\nof the explanatory variables. Denote these models M1,...,Mq. Each model will be\nseen, not as an assumption on the true joint distribution of X and Y, but rather as\na tool to efficiently encode the training set ((x1,y1),...,(xN,yN)). To describe MDL,\n22.2. PENALTY-BASED METHODS AND MINIMUM DESCRIPTION LENGTH579\nwhich selects the model that provides the most efficient code, we need to reintroduce\na few basic concepts of information theory.\nThe entropy of a discrete probability P over a set \u2126is\nH2(P) = \u2212\nX\nx\u2208\u2126\npx log2 px.\n(The logarithm in base 2 is used because of the tradition of coding with bits in infor-\nmation theory.)\nFor a discrete random variable X, the entropy H2(X) is H2(PX) where PX is the\nprobability distribution of X. The relation between the entropy and coding theory\nis as follows: a code is a function which associates to any element \u03c9 \u2208\u2126a string of\nbits c(\u03c9). The associated code-length is denoted lc(\u03c9), which is simply the number\nof bits in c(\u03c9). When P is a probability on \u2126, the efficiency of a code is measured by\nthe average code-length:\nEP(lc) =\nX\n\u03c9\u2208\u2126\nlc(\u03c9)P(\u03c9).\nShannon\u2019s theorem [175, 55] states that, under some conditions on the code (en-\nsuring that any sequence of words can be recognized as soon as it is observed:\none says that it is instantaneously decodable) the average code length can never\nbe larger than the entropy of P.\nMoreover, it states that there exists codes that\nachieve this lower bound with no more than one bit loss, such that for all \u03c9, lc(\u03c9) \u2264\n\u2212log2(P(\u03c9))+1. These optimal codes, such as the Huffman code [55], can completely\nbe determined from the knowledge of P. This allows one to interpret a probability P\non \u2126as a tool for designing codes with code-lengths essentially equal to (\u2212log2 P).\nThis statement can be generalized to continuous random variables (replacing the\ndiscrete probability P by a probability density function, say \u03d5) if one introduces a\ncoding precision level, denoted \u03b40, meaning that the decoded values may differ by\nno more than \u03b40 from the encoded ones. The result is that the optimal code-length\nat precision \u03b40 can be estimated (up to one extra bit) by \u2212log2 \u03d5 \u2212log2 \u03b40.\nIn our context, each model of the conditional distribution of Y given X, with\nconditional density \u03d5(y|x), provides a way to encode the training set with a total\ncode length, for (y1,...,yN), of\n\u2212\nN\nX\nk=1\nlog2 \u03d5(yk | xk) \u2212N log2 \u03b40\n(working, as before, conditionally to x1,...,xN). We assume that the precision at\nwhich the data is encoded is fixed, which implies that the last term does not affect the\n580\nCHAPTER 22. GENERALIZATION BOUNDS\nmodel choice. Now, assume a sequence of m parametrized model classes, M1,...,Mm\nand let \u03d5(y | x,\u03b8,Mj) denote the conditional distribution with parameter \u03b8 in the\nclass Mj. Within model Mj, the optimal code length corresponds to the maximum\nlikelihood:\n\u2212\nN\nX\nk=1\nlog2 \u03d5(x,y | \u02c6\u03b8j,Mj) = \u2212max\n\u03b8\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\nN\nX\nk=1\nlog2 \u03d5(x,y;\u03b8,Mj)\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8.\nIf the models are nested, which is often the case, the most efficient will always be\nthe largest model, since the maximization is on a larger set. However, the minimum\ndescription length (MDL) principle uses the fact that, in order to decode the com-\npressed data, the model, including its optimal parameters, has to be known, so that\nthe complete code needs to include a model description. The decoding algorithm\nwill then be: decode the model, then use it to decode the data.\nSo assume that a model (one of the Mj\u2019s) has a kj-dimensional parameter \u03b8. Also\nassume that a probability distribution, \u03c0(\u03b8 | Mj), is used to encode \u03b8. Also choose a\nprecision level, \u03b4ij, for each coordinate in \u03b8, i = 1,...,kj. (Previously, we could con-\nsider the precision of the yk, \u03b40, as fixed, but now, the precision level for parameters\nis a variable that will be optimized.) The total description length using this model\nnow becomes\n\u2212\nN\nX\nk=1\nlog2 \u03d5(yk | xk;\u03b8,Mj) \u2212log2 \u03c0(\u03b8 | Mj) \u2212\nkj\nX\ni=1\nlog2(\u03b4ij).\nLet \u02c6\u03b8(j) be the parameter that maximizes\nL(\u03b8 | Mj) =\nN\nX\nk=1\nlog2 \u03d5(yk | xk;\u03b8,Mj) + log2 \u03c0(\u03b8 | Mj)\nIf \u03c0 is interpreted as a prior distribution of the parameters, \u02c6\u03b8(j) is the maximum\na posteriori Bayes estimator. We now take the correction caused by (\u03b4ij,i = 1,...,kj)\ninto account, by assuming that the ith coordinate in \u02c6\u03b8(j) is truncated to \u2212log2 \u03b4i bits.\nLet \u03b8\n(j) denote this approximation. A second-order expansion of L(\u03b8|Mj) around\n\u02c6\u03b8(j) yields (assuming sufficient differentiability)\nL(\u03b8\n(j) | Mj) = L( \u02c6\u03b8(j) | Mj) + 1\n2(\u03b8\n(j) \u2212\u02c6\u03b8(j))T S \u02c6\u03b8(j)(\u03b8\n(j) \u2212\u02c6\u03b8(j)) + o(|\u03b8\n(j) \u2212\u02c6\u03b8(j)|2)\nwhere S\u03b8 is the matrix of second derivatives of L(\u00b7 | Mj) at \u03b8. Approximating \u03b8\n(j)\u2212\u02c6\u03b8(j)\nby \u03b4(j) (the kj-dimensional vector with coordinates \u03b4ij, i = 1,...,kj), we see that the\n22.3. CONCENTRATION INEQUALITIES\n581\nprecision should maximize\n1\n2(\u03b4(j))T S \u02c6\u03b8(j)\u03b4(j) +\nkj\nX\ni=1\nlog2 \u03b4ij.\nNote that S \u02c6\u03b8(j) must be negative semi-definite, since \u02c6\u03b8 is a local maximum. Assuming\nit is non-singular, the previous expression can be maximized and yields\nS \u02c6\u03b8(j)\u03b4(j) = \u2212\n1\nlog2\n1\n\u03b4(j)\n(22.1)\nwhere 1/\u03b4(j) is the vector with coordinates (1/\u03b4ij).\nLet us now make an asymptotic evaluation. Because L(\u03b8 | Mj) includes a sum\nover N independent terms, it is reasonable to assume that S \u02c6\u03b8(j) has order N, and\nmore precisely, that S \u02c6\u03b8(j)/N has a limit. Rewrite (22.1) as\nS \u02c6\u03b8(j)\nN\n\u221a\nN\u03b4(j) = \u2212\n1\nlog2\n1\n\u221a\nN\u03b4(j) .\nThis implies that\n\u221a\nN\u03b4(j) is the solution of an equation which stabilizes with N, and\nit is therefore reasonable to assume that the optimal \u03b4ij takes the form \u03b4ij = ci(N |\nMj)/\n\u221a\nN, with ci(N | Mj) converging to some limit when N tends to infinity. The\ntotal cost can therefore be estimated by\n\u2212L( \u02c6\u03b8(j) | Mj) +\nkj\n2 log2 N \u2212\nkj\n2 \u2212\nkj\nX\ni=1\nlog2 ci(N | Mj)\nThe last two terms are O(1), and can be neglected, at least when N is large compared\nto kj. The final criterion becomes the penalized likelihood\nld(\u03b8 | Mj) = L(\u03b8|Mj) \u2212\nkj\n2 log2 N\nin which we see that the dimension of the model appears with a factor log2 N as\nannounced (one needs to normalize both terms by N to compare with the previous\nparagraph).\n22.3\nConcentration inequalities\nThe discussion of the AIC was a first attempt at evaluating a prediction error. It was\nhowever done under very specific parametric assumptions, including the fact that\nthe true distribution of the data was within the considered model class. It was, in\n582\nCHAPTER 22. GENERALIZATION BOUNDS\naddition, a bias evaluation, i.e., we estimated how much, in average, the in-sample\nerror was less than the generalization error. We would like to obtain upper bounds to\nthe generalization error that hold with high probability, and rely as little as possible\non assumptions on the true data distribution.\nOne of the main tools used in this context are concentration inequalities, which\nprovide upper bounds on the various probabilities of events involving a large num-\nber of random variables. The current section provides a review of some of these\ninequalities.\n22.3.1\nCram\u00b4er\u2019s theorem\nIf X1,X2,... are independent, integrable random variables with identical distribu-\ntions (to that of a random variable X), the law of large numbers tells us that the\nempirical mean \u00afXN = (X1 + \u00b7\u00b7\u00b7 + XN)/N converges with probability one to m = E(X).\nWhen the variables are square integrable, Chebychev\u2019s inequality provides an easy\nproof of the weak law of large numbers. Indeed,\nP\n\u0010\n| \u00afXn \u2212m| > \u03f5\n\u0011\n\u22641\n\u03f52E\n\u0010\n( \u00afXN \u2212m)21| \u00afXn\u2212m|>\u03f5\n\u0011\n\u2264var( \u00afXN)\n\u03f52\n= var(X)\nN\u03f52 .\nA stronger assumption on the moments of X yields a stronger inequality. One\nsays that X has exponential moments if there exists \u03bb0 > 0 such that E(e\u03bb0|X|) < \u221e. In\nthis case, the cumulant-generating function, defined, for \u03bb \u2208R, by\nMX(\u03bb) = logE(e\u03bbX) \u2208[0,+\u221e],\n(22.2)\nis finite for \u03bb \u2208[\u2212\u03bb0,\u03bb0].\nHere are a few straightforward properties of the cumulant-generating function.\n(i) One has MX(0) = 0.\n(ii) For any a \u2208R, one has MaX(\u03bb) = MX(a\u03bb).\n(iii) If X1 and X2 are independent variables, one also has\nMX1+X2(\u03bb) = MX1(\u03bb) + MX2(\u03bb).\nIn particular, MX+a(\u03bb) = MX(\u03bb) + \u03bba, so that MX\u2212E(X)(\u03bb) = MX(\u03bb) \u2212\u03bbE(X).\n(iv) Finally, Markov\u2019s inequality (which states that, for any non-negative variable Y,\nP(Y > t) \u2264E(Y)/t) applied to Y = e\u03bbX for \u03bb > 0 yields\nP(X > t) = P(e\u03bbX > e\u03bbt) \u2264eMX(\u03bb)\u2212\u03bbt .\n(22.3)\n(Note that this inequality is trivially true for \u03bb = 0.)\n22.3. CONCENTRATION INEQUALITIES\n583\nFrom these properties, one can easily derive a concentration inequality for the\nmean of independent random variables. We have M \u00afXN(\u03bb) = NMX(\u03bb/N) and apply-\ning (22.3) we get, for any \u03bb \u22650 and t > 0\nP( \u00afXN \u2212m > t) \u2264e\u2212\u03bb(m+t)+M \u00afXN (\u03bb) = e\u2212N\n\u0010 \u03bb(m+t)\nN\n\u2212MX( \u03bb\nN )\n\u0011\nwhere the right-hand side may be infinite. Because this inequality is true for any \u03bb,\nwe have\nP( \u00afXN \u2212m > t) \u2264e\u2212NM\u2217\nX,+(m+t)\nwhere M\u2217\nX,+(u) = sup\u03bb\u22650(\u03bbu \u2212MX(\u03bb)), which is non-negative since the maximized\nquantity vanishes for \u03bb = 0. A symmetric computation yields\nP( \u00afXN \u2212m < \u2212t) \u2264e\u2212NM\u2217\nX,\u2212(m\u2212t)\nwhere M\u2217\nX,\u2212(t) = sup\u03bb\u22640(\u03bbt \u2212MX(\u03bb)), which is also non-negative.\nLet\nM\u2217\nX(t) = sup\n\u03bb\u2208R\n(\u03bbt \u2212MX(\u03bb)) \u22650\n(22.4)\n(this is the Fenchel-Legendre transform of the cumulant generating function, some-\ntimes called the Cram\u00b4er transform of X). One has M\u2217\nX(m + t) = M\u2217\nX,+(m + t) for t > 0.\nIndeed, because x 7\u2192e\u03bbx is convex, Jensen\u2019s inequality implies that\nE(e\u03bbX) \u2265e\u03bbm\nso that \u03bb(m +t)\u2212MX(\u03bb) \u2264\u03bbt < 0 if \u03bb < 0. Similarly, M\u2217\nX(m \u2212t) = M\u2217\nX,\u2212(m \u2212t) for t > 0.\nWe therefore have the following result.\nTheorem 22.2 Let X1,...,XN be independent and identically distributed random vari-\nables. Assume that these variables are integrable and let m = E(X1). Then, for all t > 0,\nP( \u00afXN \u2212m > t) \u2264e\u2212NM\u2217\nX(m+t)\nand\nP(| \u00afXN \u2212m| > t) \u22642e\u2212N min(M\u2217\nX(m+t),M\u2217\nX(m\u2212t))\nThe last inequality derives from\nP(| \u00afXN \u2212m| > t) = P( \u00afXN \u2212m > t) + uP( \u00afXN \u2212m < \u2212t).\nThis is our first example of concentration inequality that shows that, when\nmin(M\u2217\nX(m + t),M\u2217\nX(m \u2212t)) > 0,\n584\nCHAPTER 22. GENERALIZATION BOUNDS\nthe probability of a deviation by t at least of \u00afXn from its mean decays exponen-\ntially fast. The derivation of the inequality above was quite easy: apply Markov\u2019s\ninequality in a parametrized form and optimize over the parameter. It is therefore\nsurprising that this inequality is sharp, in the sense that a similar lower bound also\nholds. Even though we are not going to use it in the rest of this chapter, it is worth\nsketching the argument leading to this lower bound, which involves an interesting\nstep making a change of measure.\nAssume (without loss of generality) that m = 0 and consider P( \u00afXn > t). Assume,\nto simplify the discussion, that the supremum of \u03bb 7\u2192\u03f5\u03bb\u2212MX(\u03bb) is attained at some\n\u03bbt. We have\n\u2202\u03bbMX(\u03bb) = E(Xe\u03bbX)\nE(e\u03bbX) .\nLet q\u03bb(x) =\ne\u03bbx\nE(e\u03bbX) and P\u03bb (with expectation E\u03bb) the probability distribution on \u2126\nwith density q\u03bb(X) with respect to P, so that \u2202\u03bbMX(\u03bb) = E\u03bb(X). We have, since \u03bbt is\na maximizer, E\u03bbt(X) = t. Moreover, fixing \u03b4 > 0,\nP( \u00afXN > t) = E(1 \u00afXN>t)\n\u2265E(1| \u00afXn\u2212t\u2212\u03b4|<\u03b4)\n\u2265E\n\u0010\n1| \u00afXn\u2212t\u2212\u03b4|<\u03b4eN\u03bb \u00afXN\u2212Nt\u22122N\u03b4\u0011\n= e\u2212N(t+2\u03b4)MX(\u03bb)NP\u03bb(| \u00afXN \u2212t \u2212\u03b4| < \u03b4)\nIf one takes \u03bb = \u03bbt+\u03b4, this implies that\nP( \u00afXN > t) \u2265e\u2212NM\u2217\nX(t+\u03b4)e\u2212N\u03b4P\u03bbt+\u03b4(| \u00afXN \u2212t \u2212\u03b4| < \u03b4).\nBy the law of large numbers (applied to P\u03bbt+\u03b4), P\u03bbt+\u03b4(| \u00afXN \u2212t \u2212\u03b4| < \u03b4) tends to 1 when\nN tends to infinity. This implies that the logarithmic rate of convergence to 0 of\nP( \u00afXN > t) is larger than N(M\u2217\nX(t + \u03b4) + \u03b4), for any \u03b4 > 0, to be compared with the\nrate NM\u2217\nX(t) for the upper bound. In Large Deviation theory, the upper and lower\nbounds are often simplified by considering the limit of logP( \u00afXN > t)/N, which, in\nthis case, is M\u2217\nX(t) (and this result is called Cram\u00b4er\u2019s therorem).\nWhile Cram\u00b4er\u2019s upper bound is sharp, its computation requires an exact knowl-\nedge of the distribution of X, which is not a common situation. The following sec-\ntions optimize the upper bound in situations where only partial information on the\nvariable is known, such as its moments or its range. As a first example, we consider\nconcentration of the mean for sub-Gaussian variables.\n22.3.2\nSub-Gaussian variables\nIf X has exponential moments, then, (applying again Markov\u2019s inequality)\nP(|X| > x) \u2264Ce\u2212\u03bbx\n22.3. CONCENTRATION INEQUALITIES\n585\nfor some positive constants C and \u03bb. Reducing if needed the value of \u03bb, one can\nassume that C takes some predetermined (larger than 1) value, say, C = 2, the simple\nargument being left to the reader. A random variable such that, for some \u03bb > 0\nP(|X| > x) \u22642e\u2212\u03bbx\nis called sub-exponential (and this property is equivalent to X having exponential\nmoments). Similarly, one says that X is sub-Gaussian if, some \u03c3 > 0,\nP(|X| > x) \u22642e\u2212x2\n2\u03c32 .\n(22.5)\nSub-Gaussian random variables are such that M(\u03bb) < \u221efor all \u03bb \u2208R. Indeed, for\n\u03bb > 0\nE(e\u03bb|X|) =\nZ \u221e\n0\nP(e\u03bb|X| > z)dz\n= 1 +\nZ \u221e\n1\nP(|X| > \u03bb\u22121 logz)dz\n\u22641 + 2\nZ \u221e\n1\ne\u2212(logz)2\n2\u03c32\u03bb2 dz\n\u22641 + 2\nZ \u221e\n1\nex\u2212\nx2\n2\u03bb2\u03c32 dx\n\u22641 + 2\n\u221a\n2\u03c0\u03bb\u03c3e\n\u03bb2\u03c32\n2\n.\nProposition 22.3 Assume that X is sub-Gaussian, so that (22.5) holds for some \u03c32 > 0.\nThen, for any t > 0, we have\nP( \u00afXn \u2212E(X) > t) \u2264\n \n1 + 4t2\n\u03c32\n!N\ne\u2212Nt2\n2\u03c32 .\nProof Let us assume, without loss of generality, that E(X) = 0. For \u03bb > 0, we then\nhave\nE(e\u03bbX) = 1 + E(e\u03bbX \u2212\u03bbX \u22121).\nLet \u03d5(t) = et \u2212t \u22121. We have \u03d5(t) \u22650 for all t, \u03d5(0) = 0 and, for z > 0, the equation\nz = \u03d5(t) has two solutions, one positive and one negative that we will denote g+(z) >\n0 > g\u2212(z). We have\nE(\u03d5(\u03bbX)) =\nZ \u221e\n0\nP(\u03d5(\u03bbX) > z)dz\n=\nZ \u221e\n0\nP(\u03bbX > g+(z))dz +\nZ \u221e\n0\nP(\u03bbX < g\u2212(z))dz\n586\nCHAPTER 22. GENERALIZATION BOUNDS\nThe change of variable u = g+(z) in the first integral is equivalent to u > 0, \u03d5(u) = z\nwith dz = (eu\u22121)du. Similarly, u = \u2212g\u2212(z) in the second integral gives u > 0, \u03d5(\u2212u) = z\nand dz = (1 \u2212e\u2212u)du so that\nE(\u03d5(\u03bbX)) =\nZ \u221e\n0\nP(\u03bbX > u)(eu \u22121)du +\nZ \u221e\n0\nP(\u03bbX < \u2212u)(1 \u2212e\u2212u)du\n\u2264\nZ \u221e\n0\nP(\u03bb|X| > u)(eu \u2212e\u2212u)du.\n(Using the fact that max(P(\u03bbX > u),P(\u03bbX < \u2212u)) \u2264P(\u03bb|X| > u).) We have\nZ \u221e\n0\nP(\u03bb|X| > u)(eu \u2212e\u2212u)du \u22642\nZ +\u221e\n0\n(eu \u2212e\u2212u)e\u2212\nu2\n2\u03bb2\u03c32 du\n= 2\u03bb\u03c3\nZ +\u221e\n0\n(e\u03bb\u03c3v \u2212e\u2212\u03bb\u03c3v)e\u2212v2\n2 dv\n= 2\u03bb\u03c3e\n\u03bb2\u03c32\n2 \u221a\n2\u03c0(\u03a6(\u2212\u03c3\u03bb) \u2212\u03a6(\u03c3\u03bb))\n\u22644\u03bb2\u03c32e\n\u03bb2\u03c32\n2\nwhere \u03a6 is the cumulative distribution function of the standard Gaussian and we\nhave used \u03a6(\u2212t) \u2212\u03a6(t) \u22642t/\n\u221a\n2\u03c0. We therefore have\nMX(\u03bb) \u2264log\n\u0012\n1 + 4\u03bb2\u03c32e\n\u03bb2\u03c32\n2\n\u0013\n\u2264\u03bb2\u03c32\n2\n+ log(1 + 4\u03bb2\u03c32).\nThis implies\nM\u2217\nX(t) = sup\n\u03bb>0\n(\u03bbt \u2212MX(\u03bb)) \u2265t2\n\u03c32 \u2212MX(t/\u03c32) \u2265t2\n2\u03c32 \u2212log(1 + 4t2\n\u03c32 )\nso that\nP( \u00afXn > t) \u2264\n \n1 + 4t2\n\u03c32\n!N\ne\u2212Nt2\n2\u03c32 .\n\u25a0\nThe following result allows one to control the expectation of a non-negative sub-\nGaussian random variable.\nProposition 22.4 Let X be a non-negative random variable such that\nP(X > t) \u2264Ce\u2212t2/2\u03c32\nfor some constants C and \u03c32. Then,\nE(X) \u22643\u03c3\np\nlogC.\n22.3. CONCENTRATION INEQUALITIES\n587\nProof For any \u03b1 \u2208(1,C], one has\nmin(1,Ce\u2212t2/2\u03c32) \u2264\u03b1e\n\u2212t2 log\u03b1\n2\u03c32 logC ,\nwhich implies that\nE(X) =\nZ +\u221e\n0\nP(X > t)dt \u2264\n\u03b1\n2log\u03b1\n\u221a\n2\u03c0\u03c3\np\nlogC\nTaking \u03b1 = \u221ae gives\nE(X) \u2264\n\u221a\n\u03c0e\u03c3\np\nlogC \u22643\u03c3\np\nlogC.\n\u25a0\n22.3.3\nBennett\u2019s inequality\nThe following proposition (see [24]) provides an upper bound for MX(\u03bb) as a func-\ntion of E(X) and var(X) under the additional assumption that X is bounded from\nabove.\nProposition 22.5 Let m = E(X) and assume that for some constant b, one has X \u2264b with\nprobability one. Then, for any \u03c32 > 0 such that var(X) \u2264\u03c32, one has\nE(e\u03bbX) \u2264e\u03bbm\n \n(b \u2212m)2\n(b \u2212m)2 + \u03c32e\u2212\u03bb\u03c32\n(b\u2212m) +\n\u03c32\n(b \u2212m)2 + \u03c32e\u03bb(b\u2212m)\n!\n(22.6)\nfor any \u03bb \u22650.\nProof There is no loss of generality in assuming that m = 0 and \u03bb = 1, in which case\none must show that\nE(eX) \u2264\nb2\nb2 + \u03c32e\u2212\u03c32\nb +\n\u03c32\nb2 + \u03c32eb\n(22.7)\nif X < b and E(X2) \u2264\u03c32. Indeed, if this inequality is true for m = 0 and \u03bb = 1, (22.6)\nin the general case will result from letting X = Y/\u03bb+m and applying the special case\nto Y.\nThe right-hand side of (22.7) is exactly E(eX) when X follows the discrete distri-\nbution P0 supported by two points x0 and b, and such that E(X) = 0 and E(X2) = \u03c32,\nwhich requires x0 = \u2212\u03c32/b and P(X = x0) = b2/(\u03c32 + b2).\nNow consider the quadratic function v(x) = \u03b1x2 + \u03b2x + \u03b3 which intersects x 7\u2192ex\nat x = x0 and x = b, and is tangent to it at x = x0, i.e., v(b) = eb and v(x0) = v\u2032(x0) = ex0\n(this uniquely defines v). Then ex \u2264v(x) for x < b, yielding\nE(eX) \u2264\u03b1\u03c32 + \u03b3.\nHowever, since v(X) = eX almost surely when X \u223cP0, this upper bound is attained\nand equal to that provided in (22.7).\n\u25a0\n588\nCHAPTER 22. GENERALIZATION BOUNDS\nIf F(\u03bb) denotes the right-hand side of (22.6), we have, for m \u2264u < b,\nM\u2217\nX(t) \u2265sup\n\u03bb\u22650\n(\u03bbu \u2212logF(\u03bb))\nand we now estimate this lower bound. Maximizing \u03bby \u2212logF(\u03bb) is equivalent to\nminimizing\n\u03bb 7\u2192(b \u2212m)2e\u2212\u03bb(\u03c32+(u\u2212m))\nb\u2212m\n+ \u03c32e\u03bb(b\u2212u)\n(b \u2212m)2 + \u03c32\n.\nIntroduce the notation \u03c1 = \u03c32/(b \u2212m)2, \u00b5 = \u03bb(b \u2212m) and x = (u \u2212m)/(b \u2212m), so that\nthe function to minimize is\n\u00b5 7\u2192e\u2212\u00b5(\u03c1+x) + \u03c1e\u00b5(1\u2212x)\n1 + \u03c1\n.\nComputing the derivative in \u00b5 and equating it to 0 gives\n\u00b5 =\n1\n1 + \u03c1 log\n\u03c1 + x\n\u03c1(1 \u2212x),\nwhich is non-negative since \u03c1 + x \u2212\u03c1(1 \u2212x) = (1 + \u03c1)x. For this value of \u00b5, we have\ne\u2212\u00b5(\u03c1+x) + \u03c1e\u00b5(1\u2212x)\n\u03c1 + 1\n= e\u2212\u00b5(\u03c1+x)1 + \u03c1e\u00b5(1+\u03c1)\n\u03c1 + 1\n= e\u2212\u00b5(\u03c1+x)1 + \u03c1 \u03c1+x\n\u03c1(1\u2212x)\n\u03c1 + 1\n= e\u2212\u00b5(\u03c1+x)\n1 \u2212x\nand\n\u2212log e\u2212\u00b5(\u03c1+x) + \u03c1e\u00b5(1\u2212x)\n\u03c1 + 1\n= \u00b5(\u03c1 + x) + log(1 \u2212x)\n= \u03c1 + x\n1 + \u03c1 log\n\u03c1 + x\n\u03c1(1 \u2212x) + log(1 \u2212x)\n= \u03c1 + x\n1 + \u03c1 log \u03c1 + x\n\u03c1\n+ 1 \u2212x\n1 + \u03c1 log(1 \u2212x).\nThis provides a lower bound for M\u2217\nX(m+(b\u2212m)x), and yields the following corollary.\nCorollary 22.6 Assume that X satisfy the conditions of proposition 22.5. Then\nP( \u00afXN > m + t) \u2264exp\n \n\u2212N\n \u03c1 + x\n1 + \u03c1 log \u03c1 + x\n\u03c1\n+ 1 \u2212x\n1 + \u03c1 log(1 \u2212x)\n!!\n(22.8)\nwith x = t/(b \u2212m) and \u03c1 = \u03c32/(b \u2212m)2.\n22.3. CONCENTRATION INEQUALITIES\n589\nBennett\u2019s inequality is sometimes stated in a slightly weaker, but simpler form\n[127]. Returning to the proof of proposition 22.5 and using the fact that logu \u2264u\u22121,\nequation (22.7) implies\nlogE(eX) \u2264\nb2\nb2 + \u03c32e\u2212\u03c32\nb +\n\u03c32\nb2 + \u03c32eb \u22121\n=\nb2\nb2 + \u03c32(e\u2212\u03c32\nb + \u03c32\nb \u22121) +\n\u03c32\nb2 + \u03c32(eb \u2212b \u22121).\nWe will use the following lemma.\nLemma 22.7 The function \u03d5 : u 7\u2192(eu \u2212u \u22121)/u2 is non-decreasing.\nProof We have \u03d5\u2032(u) = \u03c8(u)/u3 where \u03c8(u) = ueu \u22122eu + u + 2, yielding \u03c8\u2032(u) =\nueu \u2212eu + 1, \u03c8\u2032\u2032(u) = ueu. Therefore, \u03c8\u2032 is has its minimum at u = 0 with \u03c8\u2032(0) = 0 so\nthat \u03c8 is increasing. Since \u03c8(0) = 0, we have \u03c8(u)/u3 \u22650.\n\u25a0\nWe therefore have\nlogE(eX) \u2264\nb2\nb2 + \u03c32(e\u2212\u03c32\nb + \u03c32\nb \u22121) +\n\u03c32\nb2 + \u03c32(eb \u2212b \u22121)\n=\nb2\nb2 + \u03c32\n\u03c34\nb2 \u03d5(\u2212\u03c32/b) +\n\u03c32\nb2 + \u03c32b2\u03d5(b)\n\u2264\n \n\u03c34\nb2 + \u03c32 + \u03c32b2\nb2 + \u03c32\n!\n\u03d5(b)\n= \u03c32\nb2 (eb \u2212b \u22121)\nThis shows that\nlogE(e\u03bbX) \u2264\u03c32\nb2 (e\u03bbb \u2212\u03bbb \u22121)\nand\nM\u2217\nX(t) \u2265\u03c32\nb2 max\n\u03bb (\u03bbb2t/\u03c32 \u2212e\u03bbb + \u03bbb + 1) = \u03c32\nb2 h(bt/\u03c32)\nwhere h(u) = (1 + u)log(1 + u) \u2212u.\nWe summarize this in the following corollary.\nCorollary 22.8 Assume that X satisfy the conditions of proposition 22.5. Then, for t > 0,\nP( \u00afXN > m + t) \u2264exp\n \n\u2212\nN\u03c32\n(b \u2212m)2h\n\u0012(b \u2212m)t\n\u03c32\n\u0013!\n(22.9)\nwhere h(u) = (1 + u)log(1 + u) \u2212u.\n590\nCHAPTER 22. GENERALIZATION BOUNDS\nThis estimate can be further simplified as follows. Let g be such that g\u2032\u2032(u) =\n(1+u/3)\u22123 and g(0) = g\u2032(0) = 0, which gives g(u) = u2/(2+2u/3). Noting that h\u2032\u2032(u) =\n(1 + u)\u22121 and that (1 + u)\u22121 \u2265(1 + u/3)\u22123, for u \u22650 we find, integrating twice, that\nh(u) \u2265g(u) for u \u22650. This shows that the following upper-bound is also true:\nP( \u00afXN > m + t) \u2264exp\n \n\u2212\nNt2\n2\u03c32 + 2t(b \u2212m)/3\n!\n.\n(22.10)\nThis upper bound is known as Bernstein\u2019s inequality.\nRemark 22.9 It should be clear that, in the previous discussion, one may relax the\nassumption that X1,...,XN are identically distributed as long as there is a common\nfunction M such that MXk(\u03bb) \u2264mk + M(\u03bb) for all k, with mk = E(Xk). We have in this\ncase\nP( \u00afXN > \u00afmN + t) \u2264exp(\u2212NM\u2217(t))\nwith \u00afmN = (m1 + \u00b7\u00b7\u00b7 + mN)/N and M\u2217(t) = sup\u03bb(\u03bbt \u2212M(\u03bb)). This remark can be,\nin particular, applied to the situation in which X1,...,XN satisfy the conditions of\nproposition 22.5 with the same constants b and \u03c32, yielding the same upper bound\nas in equation (22.8).\n\u2666\n22.3.4\nHoeffding\u2019s inequality\nWe now consider the case in which the random variables X1,...,XN are bounded\nfrom above and from below, and start with the following consequence of proposi-\ntion 22.5.\nProposition 22.10 Let X be a random variable taking values in the interval [a,b]. Let\nm = E(X). Then\nE(e\u03bbX) \u2264b \u2212m\nb \u2212a e\u03bba + m \u2212a\nb \u2212a e\u03bbb \u2264e\u03bbme\n\u03bb2(b\u2212a)2\n8\n(22.11)\nfor all \u03bb \u2208R.\nProof We first note that, if X takes values in [a,b], then var(X) \u2264(b\u2212m)(m\u2212a) (using\n\u03c32 = (b \u2212m)(m \u2212a) in (22.6)). To prove the upper bound on the variance, introduce\nthe function g(x) = (x \u2212a)(x \u2212b) so that g(x) \u22640 on [a,b]. Noting that one can write\ng(x) = (x \u2212m)2 + (2m \u2212a \u2212b)(x \u2212m) + (a \u2212m)(b \u2212m), we have\nE(g(X)) = var(X) \u2212(b \u2212m)(m \u2212a) \u22640,\nwhich proves the inequality.\nThis shows that, if \u03bb \u22650, we can apply proposition 22.5 with \u03c32 = (b \u2212m)(m \u2212a),\nwhich provides the first inequality in (22.11). To handle the case \u03bb \u22640, it suffices to\napply this inequality with \u02dc\u03bb = \u2212\u03bb, \u02dcX = \u2212X, \u02dca = \u2212b, \u02dcb = \u2212a and \u02dcm = \u2212m.\n22.3. CONCENTRATION INEQUALITIES\n591\nThe second inequality, namely\n b \u2212m\nb \u2212a e\u03bba + m \u2212a\nb \u2212a e\u03bbb\n!\n\u2264e\u03bbme\n\u03bb2(b\u2212a)2\n8\nrequires a little additional work. Letting u = (m \u2212a)/(b \u2212a), \u03b1 = \u03bb(b \u2212a) and taking\nlogarithms, we need to prove that\nlog(1 \u2212u + ue\u03b1) \u2212u\u03b1 \u2264\u03b12\n8\nLet f (\u03b1) denote the difference between the right-hand side and left-hand side. Then\nf (0) = 0,\nf \u2032(\u03b1) = \u03b1\n4 \u2212\nue\u03b1\n1 \u2212u + ue\u03b1 + u,\n(so that f \u2032(0) = 0) and\nf \u2032\u2032(\u03b1) = 1\n4 \u2212\nu(1 \u2212u)e\u03b1\n(1 \u2212u + ue\u03b1)2 .\nFor positive numbers x = 1 \u2212u and y = ue\u03b1, one has (x + y)2 \u22654xy, which shows that\nf \u2032\u2032(\u03b1) \u22650. This proves that f \u2032 is non-decreasing with f \u2032(0) = 0, proving that f is\nminimized at \u03b1 = 0, so that f (\u03b1) \u22650 as needed.\n\u25a0\nWe can then deduce the following theorem [92].\nCorollary 22.11 (Hoeffding Inequality) If X1,...,XN are independent, taking values,\nrespectively, in intervals of length, c1,...,cN and Y = X1 + \u00b7\u00b7\u00b7 + XN, then\nP(Y > E(Y) + t) \u2264exp\n \n\u22122t2\n|c|2\n!\n(22.12)\nand\nP(Y < E(Y) \u2212t) \u2264exp\n \n\u22122t2\n|c|2\n!\n(22.13)\nwhere |c|2 = PN\nk=1 c2\nk.\nProof We have, by proposition 22.10, for any \u03bb > 0\nP(Y > E(Y) + t) \u2264e\n\u2212\n\u0010\n\u03bbt\u2212PN\nk=1 MXk (\u03bb)\n\u0011\n\u2264e\u2212(\u03bbt\u2212\u03bb2\n8 |c|2)\nThe upper bound is minimized for \u03bb = 4t/|c|2, yielding (22.12). Equation (22.13) is\nobtained by applying (22.12) to \u2212X.\n\u25a0\n592\nCHAPTER 22. GENERALIZATION BOUNDS\nAn important special case of this inequality is when X1,...,XN are i.i.d. taking\nvalues in an interval of length \u03b4. Then\nP( \u00afXN > E(X) + t) \u2264exp\n \n\u22122Nt2\n\u03b42\n!\n.\n(22.14)\nThis inequality is obtained after applying Hoeffding\u2019s inequality to X1/N,...,XN/N,\ntherefore taking c1 = \u00b7\u00b7\u00b7 = cN = \u03b4/N and |c|2 = \u03b42/N.\n22.3.5\nMcDiarmid\u2019s inequality\nOne can relax the assumption that the random variables X1,...,XN are independent\nand only assume that these variables behave like \u201cmartingale increments,\u201d as stated\nin the following proposition [59].\nProposition 22.12 Let X1,...,XN, Z1,...,ZN be two sequences of N random variables\nsuch that\nE(Zk | X1,Z1,...,Xk\u22121,Zk\u22121) = mk\nis constant and |Zk \u2212mk| \u2264ck for some constants c1,...,cN. Then\nP(Y > E(Y) + t) \u2264e\u22122t2/|c|2\nwith Y = Z1 + \u00b7\u00b7\u00b7 + ZN and |c|2 = PN\nk=1 c2\nk.\nProof Proposition 22.10 applied to the conditional distribution implies that, for\n\u03bb \u22650:\nlogE(e\u03bb(Zk\u2212mk) | X1,Z1,...,Xk\u22121,Zk\u22121) \u2264logE(e\u03bb|Zk\u2212mk| | X1,Z1 ...,Xk\u22121,Zk\u22121) \u2264\u03bb2c2\nk\n8\n.\nLet Sk = Pk\nj=1(Zj \u2212mj). Then\nE(e\u03bbSk) = E(e\u03bbSk\u22121E(e\u03bb(Zk\u2212mk) | X1,Z1,...,Xk\u22121,Zk\u22121)) \u2264e\n\u03bb2c2\nk\n8 E(e\u03bbSk\u22121)\nso that\nE(e\u03bbSN) \u2264e\n\u03bb2\n8\nPN\nk=1 c2\nk\nand the result follows from Markov\u2019s inequality optimized over \u03bb.\n\u25a0\nWe will use this proposition to prove the \u201cbounded difference,\u201d or McDiarmid\u2019s\ninequality.\n22.3. CONCENTRATION INEQUALITIES\n593\nTheorem 22.13 (McDiarmid\u2019s inequality) Let X1,...,XN be independent random vari-\nables and g : RN \u2192R a function such that there exists c1,...,cN such that\n|g(x1,...,xk\u22121,xk,xk+1,...,xN) \u2212g(x1,...,xk\u22121, \u02dcxk,xk+1,...,xN)| \u2264ck\n(22.15)\nfor all k = 1,...,N and x1,...,xk\u22121,xk, \u02dcxk,xk+1,...,xN. Then\nP(g(X1,...,XN) > E(g(X1,...,XN)) + t) \u2264e\u22122t2/|c|2\nwith |c|2 = c2\n1 + \u00b7\u00b7\u00b7 + c2\nN.\nProof Let m = E(g(X1,...,XN)). Let Z0 = 0,\nYk = E(g(X1,...,XN) | X1,...,Xk) \u2212m\nand Zk = Yk \u2212Yk\u22121. Note that Zk is a function of X1,...,Xk and can therefore be\nomitted from the conditional expectation given (X1,Z1,...,Xk\u22121,Zk\u22121).\nWe have E(Yk) = 0 and E(Yk | X1,...,Xk\u22121) = Yk\u22121 so that E(Zk | X1,...,Xk\u22121) = 0.\nBecause the variables are independent, we have, letting \u02dcX1,..., \u02dcXN be independent\ncopies of X1,...,XN,\nZk = E(g(X1,...,Xk\u22121,Xk, \u02dcXk+1,..., \u02dcXN) | X1,...,Xk)\n\u2212E(g(X1,...,Xk\u22122, \u02dcXk\u22121, \u02dcXk,..., \u02dcXN) | X1,...,Xk\u22121).\nFor fixed X1,...,Xk\u22121, (22.15) implies that Zk varies in an interval of length ck at most\n(whose bounds depend on X1,...,Xk\u22121) so that |Zk \u2212E(Zk)| \u2264ck. Proposition 22.12\nimplies that\nP(Z1 + \u00b7\u00b7\u00b7 + ZN \u2265t) \u2264e\u22122t2/|c|2,\nwhich concludes the proof since\nZ1 + \u00b7\u00b7\u00b7 + ZN = g(X1,...,XN) \u2212E(g(X1,...,XN)).\n\u25a0\n22.3.6\nBoucheron-Lugosi-Massart inequality\nThe following result [38], that we state without proof, extends on the same idea.\nTheorem 22.14 Let X1,...,XN be independent random variables. Let\nZ = g(X1,...,XN)\nwith g : RN \u2192[0,+\u221e) and for k = 1,...,N,\nZk = gk(X1,...,Xk\u22121,Xk+1,...,XN)\n594\nCHAPTER 22. GENERALIZATION BOUNDS\nwith gk : RN\u22121 \u2192R. Assume that, for all k = 1,...,N, one has 0 \u2264Z \u2212Zk \u22641 and that\nN\nX\nk=1\n(Z \u2212Zk) \u2264Z.\nThen\nP(Z \u2212E(Z) > t) \u2264exp(\u2212E(Z)h(t/E(Z))) \u2264exp\n \n\u2212\nt2\n2E(Z) + 2t/3\n!\nwhere h(u) = (1 + u)log(1 + u) \u2212u. Moreover, for t < E(Z),\nP(Z \u2212E(Z) < \u2212t) \u2264exp(\u2212E(Z)h(\u2212t/E(Z))) \u2264exp(\u2212t2/2E(Z)).\nFinally, for all \u03bb \u2208R\nlogE(e\u03bb(Z\u2212E(Z))) \u2264E(Z)(e\u03bb \u2212\u03bb \u22121).\n(22.16)\n22.4\nBounding the empirical error with the VC-dimension\n22.4.1\nIntroduction\nSection 22.3 provides some of the most important inequalities used to evaluate the\ndeviation of various combinations of independent random variables (e.g., their em-\npirical mean) from their expectations (the reader may refer to Ledoux and Talagrand\n[117], Devroye et al. [60], Talagrand [188], Dembo and Zeitouni [59], Vershynin [199]\nand other textbooks on the subject for further developments).\nWe now return to the problem of estimating the generalization error based on\ntraining data. For a given predictor f , concentration bounds allow us to control the\nprobability\nP(R(f ) \u2212\u02c6RT (f ) > t)\nwhere\nR(f ) = E(r(Y,f (X))\nand\n\u02c6RT (f ) = 1\nN\nN\nX\nk=1\nr(yk,f (xk))\nfor a training set T = (x1,y1,...,xN,yN).\nIf this probability is small, then R(f ) \u2264\u02c6RT (f )+t with high probability, providing\na likely upper bound to the generalization error of f . For example, if r is the 0\u20131\n22.4. BOUNDING THE EMPIRICAL ERROR WITH THE VC-DIMENSION\n595\nloss in a classification problem, Hoeffding\u2019s inequality implies, for training sets of\nsize N,\nP(R(f ) \u2212\u02c6RT (f ) > t) \u2264e\u22122Nt2 .\nNow corollary 22.11 does not hold if we replace f by \u02c6fT , i.e., if f is estimated\nfrom the training set T , which is, unfortunately, the situation we are interested in.\nBefore addressing this problem, we point out that this inequality does apply to the\ncase in which f = \u02c6fT0 where T0 is another training set, independent from T , so that\nP(R( \u02c6fT0) \u2212\u02c6RT ( \u02c6fT0) > t) \u2264e\u22122Nt2 ,\nwhich is proved by writing\nP(R( \u02c6fT0) \u2212\u02c6RT ( \u02c6fT0) > t) = E(P(R( \u02c6fT ) \u2212\u02c6RT ( \u02c6fT ) |> t|T0 = T )).\nIn this situation, the empirical risk is computed on a test or validation set (T ) inde-\npendent of the set used to estimate f (T0).\nIf one does not have a test set, and \u02c6fT is optimized over a set F of possible pre-\ndictors, one can rarely do much better than starting from a variation of the trivial\nupper bound\nP(R( \u02c6fT ) \u2212ET > t) \u2264P\n\u0012\nsup\nf \u2208F\n(R(f ) \u2212ET (f )) > t\n\u0013\n(with ET = \u02c6RT ( \u02c6fT )) and the concentration inequalities discussed in section 22.3 need\nto be extended to provide upper bounds to the right-hand side.\nRemark 22.15 Computing supremums of functions over non countable sets may\nbring some issues regarding measurability. To avoid complications, we will always\nassume, when computing supremums over infinite sets, that such supremums can\nbe reduced to maximizations over finite sets, i.e., when considering supf \u2208F \u03a6(f ) for\nsome function \u03a6, we will assume that there exists a nested sequence of finite subsets\nFn \u2282F such that\nsup{\u03a6(f ) : f \u2208F } = lim\nn\u2192\u221esup{\u03a6(f ) : f \u2208Fn}.\n(22.17)\nThis is true, for example, when F has a topology that admits a countable dense\nsubset, with respect to which \u03a6 is continuous.\n\u2666\nWhen F is a finite set, one can use a \u201dunion bound\u201d with\nP(sup\nf \u2208F\n(R(f ) \u2212ET (f )) > t) \u2264\nX\nf \u2208F\nP(R(f ) \u2212ET (f ) > t) \u2264|F |max\nf \u2208F P(R(f ) \u2212ET (f ) > t).\n596\nCHAPTER 22. GENERALIZATION BOUNDS\nSuch bounds cannot be applied to the typical case in which F is infinite, and is\nlikely to provide very poor estimates even when F is finite, but |F | is large. How-\never, all proofs of concentration inequalities applied to such supremums require us-\ning a union bound at some point, often after considerable preparatory work. Union\nbounds will in particular appear in conjunction with the Vapnik-Chervonenkis di-\nmension that we now discuss.\n22.4.2\nVapnik\u2019s theorem\nWe consider a classification problem with two classes, 0 and 1, and therefore let F\nbe a set of binary functions, i.e., taking values in {0,1}. We also assume that the risk\nfunction r takes values in the interval [0,1] (using, for example, the 0\u20131 loss). Let\nU(t) = P\n\u0012\nsup\nf \u2208F\n(R(f ) \u2212ET (f )) > t\n\u0013\n.\n(22.18)\nA fundamental theorem of Vapnik provides an estimate of U(t) based on the\nnumber of possible ways to split a training set of 2N points into two classes using\nfunctions in F . The rest of this section is devoted to a discussion of this result and\nrelated notions.\nIf A is a finite subset of R, we let F (A) denote the set {f|A : f \u2208F } of restrictions\nof elements of F to the set A. As a convention, we let F (\u2205) = {f\u2205}, containing the so-\ncalled empty function. Since F only contains binary functions, we have |F (A)| \u22642|A|.\nIf x1,...,xM \u2208R, we let, with a slight abuse of notation,\nF (x1,...,xM) = F (A)\nwhere A = {xi,i = 1,...,M}. This provides the number of possible splits of a training\nset T = (x1,...,xM) using classifiers in F . Fixing in this section a random variable X,\nwe let\nSF (M) = E(|F (X1,...,XM)|)\nwhere the expectation is taken over all M i.i.d. realizations from X. We also let\nS\u2217\nF (M) = max{|F (A)| : A \u2282R,|A| \u2264M}.\nThe following theorem controls U in (22.18) in terms of SF .\nTheorem 22.16 (Vapnik) With the notation above, one has, for t \u2265\n\u221a\n2/N:\nP\n\u0012\nsup\nf \u2208F\n(R(f ) \u2212ET (f )) > t\n\u0013\n\u22642SF (2N)e\u2212Nt2/8,\n(22.19)\n22.4. BOUNDING THE EMPIRICAL ERROR WITH THE VC-DIMENSION\n597\nwhich implies that, with probability at least 1 \u2212\u03b4, we have\n\u2200f \u2208F : R(f ) \u2264ET (f )) +\nr\n8\nN\n\u0012\nlogSF (N) + log 2\n\u03b4\n\u0013\n(22.20)\n(The requirement that t \u2265\n\u221a\n2/N does not really reduce the range of applicability\nof (22.19), since, for t \u2264\n\u221a\n2/N, the upper bound in that equation is typically much\nlarger than 1.)\nProof We first show that the problem can be symmetrized with the inequality, valid\nif Nt2 \u22652,\nP\n\u0012\nsup\nf \u2208F\n(R(f ) \u2212ET (f )) \u2265t\n\u0013\n\u22642P\n\u0012\nsup\nf \u2208F\n(ET \u2032(f ) \u2212ET (f )) \u2265t\n2\n\u0013\n(22.21)\nin which T \u2032 is a second training set (independent of T ) with N samples also. In view\nof assumption (22.17), there is no loss of generality in assuming that F is finite.\nAssociate to any training set T , a classifier fT \u2208F maximizing R(fT ) \u2212E(fT ). One\nthen has\nP\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8edsup\nf \u2208F\n(ET \u2032(f ) \u2212ET (f )) \u2265t\n2\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\u2265P\n\u0012\n(ET \u2032(fT ) \u2212ET (fT )) \u2265t\n2\n\u0013\n\u2265P\n\u0012\n(R(fT ) \u2212ET \u2032(fT ) \u2264t\n2 and R(fT ) \u2212ET (fT )) \u2265t\n\u0013\n= E\n\u0012\n1R(fT )\u2212ET (fT ))\u2265tP\n\u0012\nR(fT ) \u2212ET \u2032(fT ) \u2264t\n2\n\f\f\f T\n\u0013\u0013\nConditional to T , ET \u2032(fT ) is the average of M i.i.d. Bernoulli random variables, with\nvariance bounded from above by 1/4 and\nP\n\u0012\nR(fT ) \u2212ET \u2032(fT ) \u2264t\n2\n\f\f\f T\n\u0013\n\u22651 \u2212\n1/4\nNt2/4 \u22651\n2 .\nIt follows that\nP\n\u0012\nsup\nf \u2208F\n(ET \u2032(f ) \u2212ET (f )) > t\n2\n\u0013\n\u22651\n2P\n\u0012\nR(fT ) \u2212ET (fT )) \u2265t\n\u0013\n= 1\n2P\n\u0012\nsup\nf \u2208F\n(R(f ) \u2212ET (f )) \u2265t\n\u0013\n.\nThis justifies (22.21).\nNow consider a family of independent Rademacher random variables \u03be1,...,\u03beN,\nalso independent of T and T \u2032, taking values \u22121 and +1 with equal probability. By\nsymmetry,\nsup\nf \u2208F\n(ET \u2032(f ) \u2212ET (f )) = sup\nf \u2208F\nN\nX\nk=1\n(r(Yk,f (Xk)) \u2212r(Y \u2032\nk,f (X\u2032\nk)))/N\n598\nCHAPTER 22. GENERALIZATION BOUNDS\nhas the same distribution as\nsup\nf \u2208F\nN\nX\nk=1\n\u03bek(r(Yk,f (Xk)) \u2212r(Y \u2032\nk,f (X\u2032\nk)))/N .\nNow, there are at most |F (X1,...,XN,X\u2032\n1,...,X\u2032\nN)| different sets of coefficients in front\nof \u03be1,...,\u03beN in the above sum when f varies in F , so that, conditioning on T ,T \u2032 and\ntaking a union bound , we have\nP\n\u0012\nsup\nf \u2208F\nN\nX\nk=1\n\u03bek(r(Yk,f (Xk)) \u2212r(Y \u2032\nk,f (X\u2032\nk)))/N \u2265t/2\n\f\f\f\f T ,T \u2032\u0013\n\u2264|F (X1,...,XN,X\u2032\n1,...,X\u2032\nN)|\nsup\nf \u2208F\nP\n\u0012 N\nX\nk=1\n\u03bek(r(Yk,f (Xk)) \u2212r(Y \u2032\nk,f (X\u2032\nk)))/N \u2265t/2\n\f\f\f\f T ,T \u2032\u0013\nThe variables \u03bek(r(Yk,f (Xk))\u2212r(Y \u2032\nk,f (X\u2032\nk)) are centered and belong to the interval\n[\u22121,1], which has length 2, so that Hoeffding\u2019s inequality implies\nP\n\u0012 N\nX\nk=1\n\u03bek(r(Yk,f (Xk)) \u2212r(Y \u2032\nk,f (X\u2032\nk)))/N \u2265t/2\n\f\f\f\f T ,T \u2032\u0013\n\u2264e\u22122N(t/2)2/4 = e\u2212Nt2/8\nand taking expectation over T and T \u2032 yields\nP\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8edsup\nf \u2208F\n(R(f ) \u2212ET (f )) \u2265t\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8= 2SF (2N)e\u2212Nt2/8.\nEquation (22.20) is then obtained from letting \u03b4 = 2SF (2N)e\u2212Nt2/8 so that t =\nq\n8\nN log 2SF (2N)\n\u03b4\nwith R(f ) \u2264ET (f ) + t for all f with probability 1 \u2212\u03b4 or more.\n\u25a0\n22.4.3\nVC dimension\nTo obtain a practical bound, the quantity SF (2N), or its upper-bound S\u2217\nF (2N), needs\nto be estimated. We prove below an important property of S\u2217\nF , namely that, either\nS\u2217\nF (M) = 2M for all M, or there exists an M0 for which S\u2217\nF (M0) < 2M0, and taking\nM0 to be the largest one for which an equality occurs, S\u2217\nF (M) has order MM0 for all\nM \u2265M0. This motivates the following definition of the VC-dimension of the model\nclass.\n22.4. BOUNDING THE EMPIRICAL ERROR WITH THE VC-DIMENSION\n599\nDefinition 22.17 The Vapnik-Chervonenkis dimension (or VC dimension) of the model\nclass F is\nVC-dim(F ) = max{M : S\u2217\nF (M) = 2M}.\n(where the infimum of an empty set is +\u221e).\nRemark 22.18 If, for a finite set A \u2282R, one has |F (A)| = 2|A|, one says that A is\nshattered by F . So VC-dim(F ) is the largest integer M such that there exists a set of\ncardinality M in R that is shattered by F .\n\u2666\nWe now evaluate the growth of S\u2217\nF (M) in terms of the VC-dimension, starting\nwith the following lemma, which states that, if A is a finite subset of R, there are at\nleast |F (A)| subsets of A that are shattered by F .\nLemma 22.19 (Pajor) Let A be a finite subset of R. Then\n|F (A)| \u2264|{B \u2282A : |F (B)| = 2B}|.\nProof The statement holds for A = \u2205, for which |F\u2205| = 1 = 20. For |A| = 1, the upper-\nbound is either 1 if |F (A)| = 1, or 2 if |F (A)| = 2, and the collection of sets B \u2282A such\nthat |F (B)| = 2B is {\u2205} in the first case and {\u2205,A} in the second one. So, the statement\nis true for |A| = 0 or 1.\nProceeding by induction, assume that the result is true if |A| \u2264N, and consider a\nset A\u2032 with |A\u2032| = N +1. Assume that |F (A\u2032)| \u22652 (otherwise there is nothing to prove),\nwhich implies that there exists x \u2208A\u2032 such that |F (x)| = 2. Take such an x and write\nA\u2032 = A \u222a{x} with x < A. Let\nF0 = {f \u2208F : f (x) = 0} and F1 = {f \u2208F : f (x) = 1}.\nSince F0 \u2229F1 = \u2205, we have\n|F (A\u2032)| = |F0(A\u2032)| + |F1(A\u2032)|.\nSince f (x) is constant on F0 (resp. F1), we have |F0(A\u2032)| = |F0(A)| (resp. |F1(A\u2032)| =\n|F1(A)|), and the induction hypothesis implies\n|F (A\u2032)| \u2264|{B \u2282A : |F0(B)| = 2B}| + |{B \u2282A : |F1(B)| = 2B}|\n= |{B \u2282A : |F0(B)| = 2B or |F0(B)| = 2B}|\n+ |{B \u2282A : |F0(B)| = |F1(B)| = 2B}|.\nIf B \u2282A is shattered by F0 or F1, it is obviously shattered by F . Moreover, if B is\nshattered by both, then B \u222a{x} is shattered by F . The upper bound in the equation\nabove is therefore less than the total number of sets shattered by F , which proves\nthe lemma.\n\u25a0\n600\nCHAPTER 22. GENERALIZATION BOUNDS\nFrom this lemma, it results that if VC-dim(F ) = D < \u221e, then S\u2217\nF (M) is bounded\nby the total number of subsets of cardinality D or less in a set of cardinality M. This\nprovides the following result, which implies that the term in front of the exponential\nin (22.18) grows polynomially in N if F have finite VC-dimension.\nProposition 22.20 (Sauer-Shelah\u2019s lemma) If D is the VC-dimension of F , then, for\nN \u2265D,\nS\u2217\nF (N) \u2264\n\u0012eN\nD\n\u0013D\n.\nProof Pajor\u2019s lemma implies that\nS\u2217\nF (N) \u2264\nD\nX\nk=0\n N\nk\n!\nand the statement of the proposition derives from the standard upper bound\nD\nX\nk=0\n N\nk\n!\n\u2264\n\u0012eN\nD\n\u0013D\nthat we now justify for completeness. We have\n N\nk\n!\n=\nN!\n(N \u2212k)!k! \u2264N k\nk! \u2264N D\nDD\nDk\nk!\nif k \u2264D \u2264N. This yields\nD\nX\nk=0\n N\nk\n!\n\u2264N D\nDD\nD\nX\nk=0\nDk\nk! \u2264N DeD\nDD\nas required.\n\u25a0\nWe can therefore state a corollary to theorem 22.16 for model classes with finite\nVC-dimension.\nCorollary 22.21 Assume that VC-dim(F ) = D < \u221e. Then, for t \u2265\n\u221a\n2/N and N \u2265D,\nP\n\u0012\nsup\nf \u2208F\n(R(f ) \u2212ET (f )) > t\n\u0013\n\u22642\n\u00122eN\nD\n\u0013D\ne\u2212Nt2/8.\n(22.22)\nand\nP\n\u0012\nsup\nf \u2208F\n(R(f ) \u2212ET (f )) \u2264\nr\n8\nN\nr\nD log eN\nD + log 2\n\u03b4\n\u0013\n\u22651 \u2212\u03b4.\n(22.23)\n22.4. BOUNDING THE EMPIRICAL ERROR WITH THE VC-DIMENSION\n601\n22.4.4\nExamples\nThe following result provides the VC-dimension of the collection of linear classifiers.\nProposition 22.22 Let R = Rd and F =\nn\nx 7\u2192sign(a0 + bT x) : \u03b20 \u2208R,b \u2208Rdo\n. Then\nVC-dim(F ) = d + 1.\nProof Let us show that no set of d+2 points can be shattered by F . Use the notation\n\u02dcx = (1,xT )T and \u03b2 = (a0,bT )T , and consider d + 2 points x1,...,xd+2. Then \u02dcx1,..., \u02dcxd+2\nare linearly dependent and one of them, say, \u02dcxd+2 can be expressed as a linear com-\nbination of the others. Write\n\u02dcxd+2 =\nd+1\nX\nk=1\n\u03b1k \u02dcxk .\nThen there is no function f \u2208F (taking the form \u02dcx 7\u2192sign(\u03b2 \u02dcx)) that maps (x1,...,xd+2)\nto (sign(\u03b11),...,sign(\u03b1d+1),\u22121) (where the definition of sign(0) = \u00b11 is indifferent),\nsince any such function satisfies\n\u03b2T \u02dcxd+2 =\nd+1\nX\nk=1\n\u03b1k\u03b2T \u02dcxk > 0.\nThis proves VC-dim(F ) < d +2. To prove that VC-dim(F ) = d +1, it suffices to exhibit\na set of d + 1 vectors in Rd that can be shattered by F . Choose x1,...,xd+1 such\nthat \u02dcx1,..., \u02dcxd+1 are linearly independent (for example xi = Pi\u22121\nk=1 ei, where (e1,...,ed)\nis the canonical basis of Rd). This linear independence implies that, for any vector\n\u03b1 = (\u03b11,...,\u03b1d+1)T \u2208Rd+1, there exists a vector \u03b2 \u2208Rd+1 such that \u02dcxT\ni \u03b2 = \u03b1i for all\ni = 1,...,d + 1. This shows that any combination of signs for \u02dcxT\ni \u03b2 can be achieved, so\nthat (x1,...,xd+1) is shattered.\n\u25a0\nUpper-bounds on VC dimensions of more complex models have also been pro-\nposed in the literature. As an example, the following theorem, that we provide with-\nout proof, considers feed-forward neural networks with piecewise linear units (such\nas ReLU, see chapter 11). This theorem is a special case of Theorem 7 in Bartlett et al.\n[21], in which the more general case of networks with piecewise polynomial units is\nprovided. Given integers L, U1,...,UL and W1,...,WL, define the function class\nF (L,(Ui),(Wi),p)\nthat consists of feed-forward neural networks with L layers, Ui piecewise linear com-\nputational units with less than p pieces in the ith layer, and such that the total num-\nber of parameters involved in layers 1,2,...,j is less than Wj.\n602\nCHAPTER 22. GENERALIZATION BOUNDS\nTheorem 22.23\nVC-dim(F (L,(Ui),(Wi),p)) = O(\u00afLWL log(pU)).\nwhere U = U1 + \u00b7\u00b7\u00b7 + UL and\n\u00afL = 1\nWL\nL\nX\nj=1\nWj.\nNote that p = 2 for ReLU networks. Theorem 7 in Bartlett et al. [21] also provides a\nmore explicit upper bound, namely\nVC-dim(F (L,(Ui),(W),p)) \u2264L + \u00afLWL log2\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed4ep\nL\nX\ni=1\niUi log2\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\nL\nX\ni=1\n(2epiUi)\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8.\n22.4.5\nData-based estimates\nApproximations of the shattering numbers can be computed using training data.\nOne can, in particular, prove a concentration inequality [38] on logSF (X1,...,XN),\nwhich may in turn be used to estimate log(SF (2N)). In the following, we let HVC(N,F )\ndenote the expectation of logSF (X1,...,XN). It is often referred to as the VC entropy\nof F .\nTheorem 22.24 One has, letting HVC = HVC(N,F ):\nP(logSF (X1,...,XN) \u2265HVC + t) \u2264exp\n \n\u2212\nt2\n2HVC + 2t/3\n!\nand\nP(logSF (X1,...,XN) \u2264HVC \u2212t) \u2264exp\n \n\u2212\nt2\n2HVC\n!\nProof We show that the random variable Z = log2 SF (X1,...,XN) satisfies the as-\nsumptions of theorem 22.14, with\nZk = log2 SF (X1,...,Xk\u22121,Xk+1,...,XN).\nClearly, 0 \u2264Z, 0 \u2264Z \u2212Zk \u22641, because one can do no more than double SF by adding\none point. We need to show that\nN\nX\nk=1\n(Z \u2212Zk) \u2264Z.\n(22.24)\nNote that Z is the base-two entropy of the uniform distribution, \u03c0, on the set\nF (X1,...,XN) \u2282{\u22121,1}N.\nWe will use the following lemma.\n22.4. BOUNDING THE EMPIRICAL ERROR WITH THE VC-DIMENSION\n603\nLemma 22.25 Let A be a finite set and \u03c8 a probability distribution on AN. Let \u03c8k be its\nmarginal when the kth variable is removed. Then:\nN\nX\nk=1\nH2(\u03c8k) \u2212(N \u22121)H2(\u03c8) \u22650.\n(22.25)\nThis lemma is a special case of a collection of results on non-negative entropy mea-\nsures developed in Han [86], and we provide a direct proof below for completeness.\nGiven the lemma, let \u03c0k denote the marginal distribution of \u03c0 when the kth\nvariable is removed, i.e.,\n\u03c0k(\u03f51,...\u03f5k\u22121,\u03f5k+1,...,\u03f5N)\n= \u03c0(\u03f51,...\u03f5k\u22121,\u22121,\u03f5k+1,...,\u03f5N) + \u03c0(\u03f51,...\u03f5k\u22121,1,\u03f5k+1,...,\u03f5N).\nWe have:\nN\nX\nk=1\n(H2(\u03c0) \u2212H2(\u03c0k)) \u2264H(\u03c0)\nfrom which (22.24) derives since Z = H2(\u03c0) and Zk \u2265H2(\u03c0k). The result then follows\nfrom theorem 22.14.\nWe now prove lemma 22.25 by induction (this proof requires some basic notions\nof information theory). For convenience, introduce random variables (\u03be1,...,\u03beN)\nsuch that \u03bek \u2208A, with joint probability distribution given by \u03c8. Let Y = (\u03be1,...,\u03beN),\nY (k) the (N \u22121)-tuple formed from Y by removing \u03bek, Y (k,l) the (N \u22122)-tuple obtained\nby removing \u03bek and \u03bel, etc. Inequality (22.25) can then be rewritten\nN\nX\nk=1\nH2(Y (k)) \u2212(N \u22121)H2(Y) \u22650.\nThis inequality is obviously true for N = 1, and it is true also for N = 2 since it gives\nin this case the well-known inequality H2(Y1,Y2) \u2264H2(Y1) + H2(Y2). Fix M > 2 and\nassume that the lemma is true for any N < M. To prove the statement for N = M,\nwe will use the following inequality, which holds for any three random variables\nU1,U2,U3:\nH2(U1,U3) + H2(U2,U3) \u2265H2(U1,U2,U3) + H2(U3).\nThis inequality is equivalent to the statement on conditional entropies that H2(U1,U2 |\nU3) \u2264H2(U1 | U3) + H2(U2 | U3). We apply it, for given k , l, to U1 = Yl, U2 = Yk,\nU3 = Y (k,l), yielding\nH2(Y (k)) + H2(Y (l)) \u2265H2(Y) + H2(Y (k,l)).\n604\nCHAPTER 22. GENERALIZATION BOUNDS\nWe now sum over all pairs k , l, yielding\n2(N \u22121)\nN\nX\nk=1\nH2(Y (k)) \u2265N(N \u22121)H2(Y) +\nX\nk,l\nH2(Y (k,l)).\nWe finally use the induction hypothesis to write that, for all k\nX\nl,k\nH2(Y (k,l)) \u2265(N \u22122)H2(Y (k))\nand obtain\n2(N \u22121)\nN\nX\nk=1\nH2(Y (k)) \u2265N(N \u22121)H2(Y) + (N \u22122)\nN\nX\nk=1\nH2(Y (k)),\nwhich provides the desired result after rearranging the terms.\n\u25a0\nNote that theorem 22.16 involves SF (2N), with:\nlog2(SF (2N)) = log2E(SF (X1,...,X2N)) \u2265HVC(2N,F )\nfrom Jensen\u2019s inequality. This implies that the high-probability upper bound on\nHVC(2N,F ) that results from the previous theorem is not necessarily an upper bound\non log(SF (2N)). It is however proved in Boucheron et al. [38] that\nlog2E(SF (X1,...,X2N)) \u2264\n1\nlog2HVC(2N,F )\nalso holds (as a consequence of (22.16)). A little more work (see Boucheron et al. [38])\ncombining theorem 22.16 and theorem 22.24 implies the following bound, which\nholds with probability 1 \u2212\u03b4 at least:\n\u2200f \u2208F : R(f ) \u2264E(f ) +\nr\n6logSF (X1,...,XN)\nN\n+ 4\nr\nlog(2/\u03b4)\nN\n.\n22.5\nCovering numbers and chaining\nThe upper bounds using the VC dimension relied on the number of different values\ntaken by a set of functions when evaluated on a finite set, this number being used to\napply a union bound. A different point of view may be applied when one relies on\nsome notion of continuity of the family of functions on which a uniform concentra-\ntion bound is needed, with respect to a given metric. This viewpoint is furthermore\napplicable when the sets F (X1,...,XN) are infinite. To develop these tools, we will\nneed some new concepts measuring the size of sets in a metric space.\n22.5. COVERING NUMBERS AND CHAINING\n605\n22.5.1\nCovering, packing and entropy numbers\nDefinition 22.26 Let (G,\u03c1) be a metric space and let \u03f5 > 0. The \u03f5-covering number of\n(G,\u03c1). denoted N (G,\u03c1,\u03f5), is the smallest integer n such that there exists a subset G \u2282G\nsuch that |G| = n and maxg\u2208G \u03c1(g,G) \u2264\u03f5.\nLet \u03b3 > 0. The \u03b3-packing number M(G,\u03c1,\u03b3), is the largest number n such that there\nexists a subset A \u2282G with cardinality n such that any two distinct elements of A are at\ndistance strictly larger than \u03b3 (such sets are called \u03b3-nets).\nWhen G and \u03c1 are well understood from the context, we will write simply N (\u03f5) and\nM(\u03b3).\nProposition 22.27 One has, for any \u03b3 > 0:\nM(G,\u03c1,2\u03b3) \u2264N (G,\u03c1,\u03b3) \u2264M(G,\u03c1,\u03b3).\nProof Let A be a maximal \u03b3-net. Then, for all x \u2208G, there exists y \u2208A such that\n\u03c1(x,y) \u2264\u03b3: otherwise A\u222a{x} would also be a \u03b3 \u2212net. This shows that max(\u03c1(x,A),x \u2208\nG) \u2264\u03b3 and N (G,\u03c1,\u03b3) \u2264|A|.\nConversely, let A be a 2\u03b3-net. Let G be an optimal \u03b3-covering. Associate to each\ny \u2208A a point x \u2208G at distance less than \u03b3: at least one exists because G is a covering.\nThis defines a function f : A \u2192G, which is necessarily one-to-one, because if two\npoints in A map to the same point in G, the distance between these two points would\nbe less than or equal to 2\u03b3. This shows that M(G,\u03c1,2\u03b3) \u2264N (G,\u03c1,\u03b3).\n\u25a0\nThe entropy numbers of (G,\u03c1), denoted, for an integer N, e(G,\u03c1,N) (or just e(N))\nrepresent the best accuracy that can be achieved by subsets of G of size N, namely\ne(G,\u03c1,N) =\nmin\nG\u2282G,|G|=N max{\u03c1(g,G) : g \u2208G}.\n(22.26)\nWe have:\ne(G,\u03c1,N) = inf{\u03f5 : N(G,\u03c1,\u03f5) \u2264N}\n(22.27a)\nand\nN(G,\u03c1,\u03f5) = min{N : e(G,\u03c1,N) \u2264\u03f5}.\n(22.27b)\n22.5.2\nA first union bound\nLet Z be a random variable Z : \u2126\u2192Z. We will consider a space G of functions\ng : Z \u2192R, such that (to simplify the discussion) E(g(Z)) = 0 for all g \u2208G. In this\nsection, we assume that functions in G are bounded and let\n\u03c1\u221e(g,g\u2032) = sup\nz\u2208Z\n|g(z) \u2212g\u2032(z)|.\n606\nCHAPTER 22. GENERALIZATION BOUNDS\nAssume that N (G,\u03c1\u221e,\u03f5) < \u221e, for all \u03f5 > 0 (which requires the set G to be pre-\ncompact for the \u03c1\u221emetric). Take t > 0, 0 < \u03f5 < t and choose a set G \u2282G such that\n|G| = N (G,\u03c1\u221e,\u03f5). Then, using a union bound,\nP(sup\ng\u2208G\ng(Z) \u2265t) \u2264P(sup\ng\u2208G\ng(Z) \u2265t \u2212\u03f5)\n(22.28)\n\u2264N (G,\u03c1\u221e,\u03f5) sup\ng\u2208G\nP(g(Z) \u2265t \u2212\u03f5).\nNow, if each function in G satisfies a concentration inequality, say,\nP(g(Z) \u2265u) \u2264e\u2212u2\n2\u00b5(g)\nfor some \u00b5(g) > 0, then, assuming that \u00b5(G)\n\u2206= maxg\u2208G \u00b5(g) is finite, we find that, for\n0 < \u03f5 < t,\nP(sup\ng\u2208G\ng(Z) \u2265t) \u2264N (G,\u03c1\u221e,\u03f5)e\u2212(t\u2212\u03f5)2\n2\u00b5(G) .\nWe now apply this inequality to the case of binary classification, where a binary\nvariable Y is predicted by an input variable X, with a model class of classifiers F\nand the 0\u20131 loss function. If A is a finite family of elements of R, we define, for\nf ,f \u2032 \u2208F\n\u03c1A(f ,f \u2032) = 1\n|A|\nX\nx\u2208A\n1f (x),f \u2032(x).\nLet\n\u00af\nN (F ,\u03f5,N) = E\n\u0010\nN (F ,\u03c1{X1,...,XN},\u03f5)\n\u0011\nwhere X1,...,XN is an i.i.d. sample of X. We then have the following proposition.\nProposition 22.28 For all \u03f5 > 0, one has\nP\n\u0012\nsup\nf \u2208F\n(R(f ) \u2212ET (f )) \u2265t\n\u0013\n\u22642 \u00af\nN (F ,\u03f5/2,N)e\u2212N(t/2\u2212\u03f5)2\n4\n.\n(22.29)\nProof A key step in the proof of theorem 22.16, was to show that\nP\n\u0012\nsup\nf \u2208F\n(R(f )\u2212ET (f )) \u2265t\n\u0013\n\u22642P\n\u0012\nsup\nf \u2208F\nN\nX\nk=1\n\u03bek(r(Y \u2032\nk,f (X\u2032\nk))\u2212r(Yk,f (Xk))) \u2265Nt/2\n\u0013\n. (22.30)\nwhere \u03be1,...,\u03beN are Rademacher random variables and T ,T \u2032 are two independent\ntraining sets of size N. We start from this inequality and bound the conditional\nexpectation\nP\n\u0012\nsup\nf \u2208F\nN\nX\nk=1\n\u03bek(r(Y \u2032\nk,f (X\u2032\nk)) \u2212r(Yk,f (Xk))) \u2265Nt/2\n\f\f\f\f T ,T \u2032\u0013\n(22.31)\n22.5. COVERING NUMBERS AND CHAINING\n607\nand therefore consider r(Y \u2032\nk,f (X\u2032\nk)) \u2212r(Yk,f (Xk)) as constants that we will denote\nck(f ). Since we are using a 0\u20131 loss, we have ck(f ) \u2208{\u22121,0,1} and, for f ,f \u2032 \u2208F ,\n|ck(f ) \u2212ck(f \u2032)| \u22641f (Xk),f \u2032(Xk) + 1f (X\u2032\nk),f \u2032(X\u2032\nk) .\n(22.32)\nConsider the random variable Z = (\u03be1,...,\u03beN), and let\nG =\nn\ngf ,f \u2208F\no\nwith\ngf (\u03be1,...,\u03beN) = 1\nN\nN\nX\nk=1\nck(f )\u03bek .\nWe have\n\u03c1\u221e(gf ,gf \u2032) = 1\nN\nN\nX\nk=1\n|ck(f ) \u2212ck(f \u2032)|.\nApplying Hoeffding\u2019s inequality, we have, for u > 0 and using the fact that ck \u2208[\u22121,1]\nP(gf (Z) > u | T ,T \u2032) \u2264e\u22122Nu2\n4\n= e\u2212Nu2\n2\nand the discussion preceding the theorem yields the fact that, for any \u03f5 > 0:\nP(sup\nf \u2208F\ngf (Z) > t/2 | T ,T \u2032) \u2264N (G,\u03f5,\u03c1\u221e)e\u2212N(t/2\u2212\u03f5)2\n2\n.\n(22.33)\nLet A = (X1,...,XN,X\u2032\n1,...,X\u2032\nN) so that\n\u03c1A(f ,f \u2032) = 1\n2N\nN\nX\nk=1\n\u0010\n1f (Xk),f \u2032(Xk) + 1f (X\u2032\nk),f \u2032(X\u2032\nk)\n\u0011\n.\nUsing (22.32), we have \u03c1\u221e(gf ,gf \u2032) \u22642\u03c1A(f ,f \u2032), which implies\nN (G,\u03f5,\u03c1\u221e) \u2264N (F ,\u03f5/2,\u03c1A).\nUsing this in (22.33) and taking the expectation in (22.31), we get\nP\n\u0012\nsup\nf \u2208F\n(R(f ) \u2212ET (f )) \u2265t\n\u0013\n\u22642 \u00af\nN (F ,\u03f5/2,N)e\u2212N(t/2\u2212\u03f5)2\n2\n(22.34)\nwhich is valid for all \u03f5 > 0.\n\u25a0\nOne can retrieve the bound obtained in theorem 22.16 using the obvious fact that\nN (F ,\u03f5,\u03c1A) \u2264|F (A)|,\n608\nCHAPTER 22. GENERALIZATION BOUNDS\nfor any A \u2282R, so that\nP\n\u0012\nsup\nf \u2208F\n(R(f ) \u2212ET (f )) \u2265t\n\u0013\n\u22642S(F ,2N)e\u2212N(t/2\u2212\u03f5)2\n2\nfor any \u03f5 > 0, and letting \u03f5 go to zero,\nP\n\u0012\nsup\nf \u2208F\n(R(f ) \u2212ET (f )) \u2265t\n\u0013\n\u22642S(F ,2N)e\u2212Nt2\n8 .\nSo (22.29) provides a family of equations that depend on a parameter \u03f5 which, in\nthe limit \u03f5 \u21920, includes theorem 22.16 as a particular case. For a given N, optimiz-\ning (22.29) over \u03f5 may give a better upper bound, provided one has a good way to\nestimate \u00af\nN (F ,\u03f5/2,N) (which is, of course, far from obvious).\n22.5.3\nEvaluating covering numbers\nCovering numbers can be evaluated in some simple situations. The following propo-\nsition provides an example in finite dimensions.\nProposition 22.29 Assume that G is a parametric family of functions, so that G = {g\u03b8,\u03b8 \u2208\u0398}\nwhere \u0398 \u2282Rm. Assume also that, for some constant C, \u03c1\u221e(g\u03b8,g\u03b8\u2032) \u2264C|\u03b8 \u2212\u03b8\u2032| for all\n\u03b8,\u03b8\u2032 \u2208\u0398. Let G(M) = {g\u03b8 : \u03b8 \u2208\u0398,|\u03b8| \u2264M}. Then\nN (G,\u03c1\u221e,\u03f5) \u2264\n\u0012\n1 + 2CM\n\u03f5\n\u0013m\nProof Letting \u03c1 denote the Euclidean distance in Rm, our hypotheses imply that\nN (G(M),\u03c1\u221e,\u03f5) is bounded by N (BM,\u03c1,\u03f5/C) where BM is the ball with radius M in\nRm. Now, if \u03b81,...,\u03b8n is an \u03b1-covering of BM, then \u03b81/M,...,\u03b8n/M is an (\u03b1/M)-\ncovering of B1, which shows (together with a symmetric argument) that N (BM,\u03c1,\u03b1) =\nN (B1,\u03c1,\u03b1/M) and we get\nN (G(M),\u03c1\u221e,\u03f5) \u2264N (B1,\u03c1,\u03f5/MC)\nand we only need to evaluate N (B1,\u03c1,\u03b1) for \u03b1 > 0. Using proposition 22.27, one can\ninstead evaluate M(B1,\u03c1,\u03b1). So let A be an \u03b1-net in B1. Then\n[\nx\u2208A\nB\u03c1(x,\u03b1/2) \u2282B\u03c1(0,1 + \u03b1/2)\nand, since the sets in the union are disjoint,\nX\nx\u2208A\nvolume(B\u03c1(x,\u03b1/2)) = |A|volume(B\u03c1(0,\u03b1/2)) \u2264volume(B\u03c1(0,1 + \u03b1/2)).\n22.5. COVERING NUMBERS AND CHAINING\n609\nLetting Cm denote the volume of the unit ball in Rm, this shows\n|A|Cm\n\u0012\u03b1\n2\n\u0013m\n\u2264Cm\n\u0012\n1 + \u03b1\n2\n\u0013m\nand\n|A| \u2264\n\u0012\n1 + 2\n\u03b1\n\u0013m\n,\nwhich concludes the proof.\n\u25a0\nOne can also obtain entropy number estimates in infinite dimensions. Here, we\nquote a result applicable to spaces of smooth functions, referring to Van der Vaart\nand Wellner [195] for a proof.\nTheorem 22.30 Let Z be a bounded convex subset of Rd with non-empty interior. For\np \u22651 and f \u2208Cp(Z), let\n\u2225f \u2225p,\u221e= max\nn\n|Dk(f (x)| : k = 0,...,p,x \u2208Z\no\n.\nLet G be the unit ball for this norm,\nG =\nn\nf \u2208Cp(Z) : \u2225f \u2225p,\u221e\u22641\no\n.\nLet Z(1) be the set of all x \u2208Rd at distance less than 1 from R.\nThen there exists a constant K depending only on p and d such that\nlogN (\u03f5,G,\u03c1\u221e) \u2264Kvolume(Z(1))\n\u00121\n\u03f5\n\u0013d/p\n22.5.4\nChaining\nThe distance \u03c1\u221emay not always be the best one to analyze the set of functions, G. For\nexample, if G is a class of functions with values in {\u22121,1}, then \u03c1\u221e(g,g\u2032) = 2 unless\ng = g\u2032. In such contexts, it is often preferable to use distances that compute average\ndiscrepancies, such as\n\u03c1p(g,g\u2032) = E(|g(Z) \u2212g\u2032(Z)|p)1/p ,\n(22.35)\nfor some random variable Z. Such distances, by definition, do not provide uniform\nbounds on differences between functions (that we used to write (22.28)), but can\nrather be used in upper-bounds on the probabilities of deviations from zero, which\nhave to be handled somewhat differently. We here summarize a general approach\ncalled \u201cchaining,\u201d following for this purpose the presentation made in Talagrand\n[189] (see also Audibert and Bousquet [15]). From now on, we assume that (G,\u03c1) is a\n610\nCHAPTER 22. GENERALIZATION BOUNDS\n(pseudo-)metric space of functions g : Z \u2192R and Z a random variable taking values\nin Z. We will make the basic assumption that, for all g,g\u2032 \u2208G and t > 0,\nP(|g(Z) \u2212g\u2032(Z)| > t) \u22642e\n\u2212\nt2\n2\u03c1(g,g\u2032)2 .\nNote that this assumption includes cases in which\nP(|g(Z) \u2212g\u2032(Z)| > t) \u22642e\n\u2212\nt2\n2\u03c1(g,g\u2032)\u03b1 .\nfor some \u03b1 \u2208(0,2], because, if \u03c1 is a distance, then so is \u03c1\u03b1/2 if \u03b1 \u22642. We will also\nassume that E(g(Z)) = 0 in order to avoid centering the variables at every step.\nWe are interested in upper bounds for P(supg\u2208G g(Z) > t). To build a chaining\nargument, consider a family (G0,G1,...) of subsets of G. Assume that |Gk| \u2264Nk with\nNk chosen, for future simplicity, so that Nk\u22121Nk \u2264Nk+1. For g \u2208G, let \u03c0k(g) denote a\nclosest point to g in Gk. Also assume that G0 = {g0} is a singleton, so that \u03c00(g) = g0\nfor all g \u2208G. (One can generally assume without harm that 0 \u2208G, in which case one\nshould choose g0 = 0 in the following discussion.) For g \u2208Gn, we therefore have\ng \u2212g0 =\nn\nX\nk=1\n(\u03c0k(g) \u2212\u03c0k\u22121(g)).\nLet (t1,t2,...) be a sequence of numbers that will be determined later. Let\nSn = max\ng\u2208Gn\nn\nX\nk=1\ntk\u03c1(\u03c0k(g),\u03c0k\u22121(g)).\n(22.36)\nThen, for any t,\nP(sup\ng\u2208Gn\ng(Z) \u2212g0(Z) > tSn)\n\u2264P(\u2203g \u2208Gn,\u2203k \u2264n : \u03c0k(g)(Z) \u2212\u03c0k\u22121(g)(Z) > ttk\u03c1(\u03c0k(g),\u03c0k\u22121(g)))\n\u2264P(\u2203k \u2264n,\u2203g \u2208Gk,g\u2032 \u2208Gk\u22121 : g(Z) \u2212g\u2032(Z) > ttk\u03c1(g,g\u2032))\n\u2264\nn\nX\nk=1\nNkNk\u22121\nsup\ng\u2208Gk,g\u2032\u2208Gk\u22121\nP(g(Z) \u2212g\u2032(Z) > ttk\u03c1(g,g\u2032))\n\u22642\nn\nX\nk=1\nNk+1e\u2212\nt2t2\nk\n2\nIf one takes Nk = 22k, which satisfies NkNk\u22121 = 22k+2k\u22121 \u2264Nk+1, and tk = 2k/2, one\nfinds that\nP(sup\ng\u2208Gn\ng(Z) \u2212g0(Z) > tSn) \u22642\nn\nX\nk=1\n22k+1e\u22122k\u22121t2.\n22.5. COVERING NUMBERS AND CHAINING\n611\nThe upper bound converges (as a function of n) as soon as t > 2\np\nlog2. Moreover,\none has\n2\nn\nX\nk=1\n22k+1e\u22122k\u22121t2 = 2e\u2212t2\n2\nn\nX\nk=1\ne\u22122k\u22122(t2\u22128log2) \u22642e\u2212t2\n2\n\u221e\nX\nk=1\ne\u22122k\u22122\nwhen t >\np\n1 + 8log2. This provides a concentration bound for P(supg\u2208Gn g(Z) \u2212\ng0(Z) > tSn), that we may rewrite as\nP(sup\ng\u2208Gn\ng(Z) \u2212g0(Z) > t) \u2264Ce\n\u2212t2\n2S2n\n(22.37)\nfor t > 2Sn\np\nlog2, C = 2P\u221e\nk=1 e\u22122k\u22122 and Sn given by (22.36), with tk = 2k/2. Moreover,\nwe have\nSn = max\ng\u2208Gn\nn\nX\nk=1\n2k/2\u03c1(\u03c0k(g),\u03c0k\u22121(g))\n\u2264max\ng\u2208Gn\nn\nX\nk=1\n2k/2(\u03c1(g,Gk) + \u03c1(g,Gk\u22121))\n\u22642max\ng\u2208Gn\nn\nX\nk=0\n2k/2\u03c1(g,Gk)\nand this simpler upper bound can be used in (22.37).\nWe haven\u2019t made many assumptions so far on the sequence G0,G1,..., beyond\nbounding their cardinality, but it is natural to require that they are built in order to\nbehave like a dense subset of G, so that\nlim\nn\u2192\u221emax\ng\u2208G \u03c1(x,Gn) = 0.\n(22.38)\nNote that this requires that the set G is precompact for the distance \u03c1. We will also\nassume that\nlim\nn\u2192\u221esup\ng\u2208Gn\ng(x) = sup\ng\u2208G\ng(x).\n(22.39)\nThen, we have proved the following result [188].\nTheorem 22.31 Let G0,G1,... be a family of subsets of G satisfying (22.38) and (22.39)\nand such that G0 = {g0} and |Gn| \u226422n for n \u22650. Let\nS = 2sup\ng\u2208G\n\u221e\nX\nn=0\n2n/2\u03c1(g,Gn)\n(22.40)\n612\nCHAPTER 22. GENERALIZATION BOUNDS\nThen, for t > S\np\n1 + 8log2,\nP(sup\ng\u2208G\ng(Z) \u2212g0(Z) > t) \u2264Ce\u2212t2\n2S2\n(22.41)\nwith C = 2P\u221e\nk=1 e\u22122k\u22122.\nThe exponential rate of convergence in the right-hand side of (22.41) is the quan-\ntity S, and the upper bound will be improved when building the sequence (G0,G1,...)\nso that S is as small as possible. Such an optimization for a given family of functions\nis however a formidable problem. It is however interesting to see (still following\n[188]) that theorem 22.31 implies a classical inequality in terms of what is called the\nmetric entropy of the metric space (G,\u03c1).\n22.5.5\nMetric entropy\nIf S is given by (22.40), we have\nS = 2sup\n\u0012 \u221e\nX\nn=0\n2n/2\u03c1(g,Gn) : g \u2208G\n\u0013\n\u22642\n\u221e\nX\nn=0\n2n/2 sup{\u03c1(g,Gn) : g \u2208G}\nTake Gn achieving the minimum in the entropy number e(G,\u03c1,22n). Then, (22.41)\nholds with S replaced by\n\u02c6S = 2\n\u221e\nX\nn=0\n2n/2e(G,\u03c1,22n).\nConsider the function\nh(G,\u03c1) =\nZ \u221e\n0\nq\nlogN (G,\u03c1,\u03f5)d\u03f5,\n(22.42)\nwhich is known as Dudley\u2019s metric entropy of the space (G,\u03c1). We have\nh(G,\u03c1) =\nZ e(2)\n0\nq\nlogN (\u03f5)d\u03f5 +\n\u221e\nX\nn=1\nZ e(22n)\ne(22n\u22121)\nq\nlogN (\u03f5)d\u03f5.\nIf \u03f5 \u2208[e(22n\u22121),e(22n)), we have N (\u03f5) > 22n so that\nh(G,\u03c1) \u2265e(2)\np\nlog3 +\n\u221e\nX\nn=1\n2n/2(e(22n) \u2212e(22n\u22121))\n\u2265\n\u0010\n1 \u2212\n\u221a\n2\n2\n\u0011 \u221e\nX\nn=1\n2n/2e(22n).\n22.5. COVERING NUMBERS AND CHAINING\n613\nTherefore,\n\u02c6S \u2264\n4\n2 \u2212\n\u221a\n2\nh(G,\u03c1) \u22647h(G,\u03c1)\nand this upper bound can also be used to obtain a simpler (but weaker) form of\ntheorem 22.31.\nRemark 22.32 The covering numbers of a class G of binary functions g with values\nin {\u22121,1} can be controlled by the VC dimension of the class. Here, we consider\n\u03c1(g,g\u2032) = P(g , g\u2032) = \u03c11(g,g\u2032)/2. Then, the following theorem holds.\nTheorem 22.33 Let G be a class of binary functions such that D = VC-dim(G) < \u221e.\nThen, there is a universal constant K such that, for any \u03f5 \u2208(0,1),\nN (G,\u03c1,\u03f5) \u2264KD(4e)D \u00121\n\u03f5\n\u0013D\u22121\nwith \u03c1(g,g\u2032) = P(g , g\u2032).\nWe refer to Van der Vaart and Wellner [195], Theorem 2.6.4 for a proof, which is\nrather long and technical.\n\u2666\n22.5.6\nApplication\nWe quickly show how this discussion can be turned into results applicable to the\nclassification problem. If F is a function class of binary classifiers and r is the risk\nfunction, one can consider the class\nG = {(x,y) 7\u2192r(y,f (x)) : f \u2208F }.\nIf r is the 0\u20131 loss, we have VC-dim(G) \u2264VC-dim(F ). Indeed, if one considers N\npoints in R \u00d7 {\u22121,1}, say (x1,y1,...,xN,yN), then\nG(x1,y1,...,xN,yN)\n= {r(1,f (xk)) : k = 1,...,N,yk = 1} \u222a{r(\u22121,f (xk)) : k = 1,...,N,yk = \u22121}.\nIf the two sets in the right-hand side are not empty, i.e., the numbers N(1) and N(\u22121)\nof k\u2019s such that yk = 1 or yk = \u22121 are not zero, then\n|G(x1,y1,...,xN,yN)| \u22642N(1) + 2N(\u22121),\nwhich is less that 2N as soon as N > 2. So, taking N > 2, for (x1,y1,...,xN,yN) to be\nshattered by G, we need N(1) = N or N(\u22121) = N and in this case, the inequality:\n|G(x1,y1,...,xN,yN)| \u2264|F (x1,...,xN)|\n614\nCHAPTER 22. GENERALIZATION BOUNDS\nis obvious. The same inequality will be true for some x1,...,xN with N = 2, except in\nthe uninteresting case where f (x) = 1 (or \u22121) for every x \u2208R.\nA similar inequality holds for entropy numbers with the \u03c11 distance (cf. (22.35))\nbecause\nE(|r(Y,f (X)) \u2212r(Y,f \u2032(X))|) \u2264P(f (X) , f \u2032(X))\nwhenever r takes values in [0,1], which implies that\nN (G,\u03c11,\u03f5) \u2264N (F ,\u03c11,\u03f5)\nfor all \u03f5 > 0. Note however that evaluating this upper bound may still be challenging\nand would rely on strong assumptions on the distribution of X allowing to control\nP(f (X) , f \u2032(X)).\nWe now assume that functions in F define \u201cposterior probabilities\u201d on G. More\nprecisely, given \u03bb \u2208R we can define the probability \u03c0\u03bb on {\u22121,1} by\n\u03c0\u03bb(y) =\ne\u03bby\ne\u2212\u03bb + e\u03bb.\nNow, if F is a class of real-valued functions, we can define the risk function\nr(y,f (x)) = log\n1\n\u03c0f (x)(y) .\nSince |\u2202\u03bb log\u03c0\u03bb(y)| = |y \u2212tanh\u03bb| \u22642 for y \u2208{\u22121,1}, we have\n|r(y,f (x)) \u2212r(y,f \u2032(x))| \u22642|f (x) \u2212f \u2032(x)|\nso that entropy numbers in G can be estimated from entropy numbers in F . As an\nexample, let F be a space of affine functions x 7\u2192a0 + bT x, x \u2208Rd. Assume that the\nrandom variable X is bounded, so that one can take R to be an open ball centered at\n0 with radius, say, U. For M > 0, let\nFM = {f : x 7\u2192a0 + bT x : |b| \u2264M,|a0| \u2264UM}.\nThe restriction |b| \u2264M is equivalent to using a penalty method, such as, for example,\nridge logistic regression. Moreover, if |b| \u2264M, it is natural to assume that |a0| \u2264UM\nbecause otherwise f would have a constant sign on R. In this case, we get\n\u03c1\u221e(r(y,f (x)),r(y,f \u2032(x))) \u2264|a0 \u2212a\u2032\n0| + U|b \u2212b\u2032|\nand a small modification of the proof of proposition 22.29 shows that\nN (F ,\u03c1\u221e,\u03f5) \u2264\n\u0012\n1 + 4CU\n\u03f5\n\u0013d+1\n22.6. OTHER COMPLEXITY MEASURES\n615\n22.6\nOther complexity measures\n22.6.1\nFat-shattering and margins\nVC-dimension and metric entropy are measures that control the complexity of a\nmodel class, and can therefore be evaluated a priori without observing any data.\nThese bounds can be improved, in general, by using information derived from the\ntraining set, and, particular the classification margin that has been obtained [18].\nFor this discussion, we need to return to the definition of covering numbers. If F\nis a function class, \u03c1\u221ethe supremum metric on F , \u03f5 > 0 and N is an integer, we let\nN (F ,\u03c1\u221e,\u03f5,N) = max{N (F (A),\u03c1\u221e,\u03f5) : A \u2282R,|A| = N}\nthat we will abbreviate in N\u221e(\u03f5,N) when F is known from the context. We will\nassume that functions in F take values values in [\u22121,1], and we define for \u03b3 \u22650,\ny \u2208{0,1}, u \u2208R:\nr\u03b3(y,u) =\n\uf8f1\uf8f4\uf8f4\uf8f4\uf8f4\uf8f2\uf8f4\uf8f4\uf8f4\uf8f4\uf8f3\n0 if u < \u2212\u03b3 and y = 0\n0 if u > \u03b3 and y = 1\n1 otherwise\nSo, r\u03b3(y,f (x)) is equal to 0 if f (x) correctly predicts y with margin \u03b3 and to 1 other-\nwise. We then define the classification error with margin \u03b3 as\nR\u03b3(f ) = E(r\u03b3(Y,f (X)))\nand, given a training set T of size N\nE\u03b3,T = 1\nN\nN\nX\nk=1\nr\u03b3(yk,f (xk)).\nWe then have the following theorem [10].\nTheorem 22.34 If t \u2265\n\u221a\n2/N\nP(sup\nf \u2208F\n(R0(f ) \u2212E\u03b3,T (f )) > t) \u22642N\u221e(\u03b3/2,2N)e\u2212Nt2/8,\n(22.43)\nor, equivalently, with probability larger than 1 \u2212\u03b4, one has, for all f \u2208F ,\nR0(f ) \u2212E\u03b3,T (f )) \u2264\nr\n8\nN\n\u0012\nlogN\u221e(\u03b3/2,2N) + log 2\n\u03b4\n\u0013\n.\n(22.44)\nProof We first note that, for Nt2 > 2,\nP\n\u0012\nsup\nf \u2208F\n(R0(f ) \u2212E\u03b3,T (f )) > t\n\u0013\n\u22642P\n\u0012\nsup\nf \u2208F\n(ET \u2032(f ) \u2212E\u03b3,T (f )) > \u03f5\n2\n\u0013\n,\n616\nCHAPTER 22. GENERALIZATION BOUNDS\nwhich is proved exactly the same way as (22.21) in theorem 22.16, and we skip the\nargument.\nWe have\nET \u2032(f ) \u2212E\u03b3,T (f ) = 1\nN\nN\nX\nk=1\n(r0(Y \u2032\nk,f (X\u2032\nk)) \u2212r\u03b3(Yk,f (Xk)))\nand because (Xk,Yk) and (X\u2032\nk,Y \u2032\nk) have the same distribution, supf \u2208F (ET \u2032(f )\u2212E\u03b3,T (f ))\nhas the same distribution as\n\u2206T ,T \u2032(\u03be1,...,\u03beN) =\nsup\nf \u2208F\n1\nN\nN\nX\nk=1\n\u0010\n(r0(Y \u2032\nk,f (X\u2032\nk)) \u2212r\u03b3(Yk,f (Xk)))\u03bek + (r0(Yk,f (Xk)) \u2212r\u03b3(Y \u2032\nk,f (X\u2032\nk)))(1 \u2212\u03bek)\n\u0011\nwhere \u03be1,...,\u03beN is a sequence of Bernoulli random variables with parameter 1/2.\nWe now estimate P(\u2206T ,T \u2032(\u03be1,...,\u03beN) > t/2 | T ,T \u2032) and we therefore consider T and\nT \u2032 as fixed. Let F be a subset of F , with cardinality N\u221e(\u03b3/2,2N), such that for all f \u2208\nF there exists an f \u2032 \u2208F such that |f (x)\u2212f \u2032(x)| \u2264\u03b3/2 for all x \u2208{X1,...,XN,X\u2032\n1,...,X\u2032\nN}.\nThen we claim that\n\u2206T ,T \u2032(\u03be1,...,\u03beN) \u2264\u2206\u2032\nT ,T \u2032(\u03be1,...,\u03beN)\nwhere\n\u2206\u2032\nT ,T \u2032(\u03be1,...,\u03beN) = max\nf \u2208F\n1\nN\nN\nX\nk=1\n(2\u03bek \u22121)\n\u0010\nr \u03b3\n2 (Y \u2032\nk,f (X\u2032\nk)) \u2212r \u03b3\n2 (Yk,f (Xk))\n\u0011\n.\nThis is because, for any (x,y) \u2208R \u00d7 {0,1}, and f ,f \u2032 such that |f (x) \u2212f \u2032(x)| < \u03b3/2, we\nhave r0(y,f (x)) \u2264r\u03b3/2(y,f \u2032(x)) and r\u03b3/2(y,f \u2032(x)) \u2264r\u03b3(y,f (x)): if an example is misclas-\nsified by f (resp. f \u2032) at a given margin, it must be misclassified by f \u2032 (resp. f ) at this\nmargin plus \u03b3/2.\nNow,\nP(\u2206\u2032\nT ,T \u2032(\u03be1,...,\u03beN) > t\n2)\n\u2264|F|max\nf \u2208F P\n\u0012 1\nN\nN\nX\nk=1\n(2\u03bek \u22121)(r \u03b3\n2 (Y \u2032\nk,f (X\u2032\nk)) \u2212r \u03b3\n2 (Yk,f (Xk))) > t\n2\n\u0013\nto which we can apply Hoeffding\u2019s inequality, yielding\nP\n\u0012\n\u2206\u2032\nT ,T \u2032(\u03be1,...,\u03beN) > t\n2\n\u0013\n\u2264|F|e\u2212Nt2/8,\n\u25a0\nwhich concludes the proof, since, by proposition 22.27, |F| \u2264N\u221e(\u03b3/2,2N).\n22.6. OTHER COMPLEXITY MEASURES\n617\nIn order to evaluate the covering numbers N\u221e(\u03f5,N) using quantities similar\nto VC-dimensions, a different type of set decomposition and shattering has been\nproposed. Following Alon et al. [4], we introduce the following notions. Recall\nthat a family of functions F : R \u2192{0,1} shatters a finite set A \u2282R if and only if\n|F (A)| = 2|A|. The following definitions are adapted to functions taking values in a\ncontinuous set.\nDefinition 22.35 Let F be a family of functions f : R \u2192[\u22121,1] and A a finite subset of\nR.\n(i) One says that F P-shatters A if there exists a function gA : R \u2192R such that, for each\nB \u2282A, there exists a function f \u2208F such that f (x) \u2265gA(x) if x \u2208B and f (x) < gA(x) if\nx \u2208A \\ B.\n(ii) Let \u03b3 be a positive number. One says that F P\u03b3-shatters A if there exists a function\ngA : R \u2192R such that, for each B \u2282A, there exists a function f \u2208F such that f (x) \u2265\ngA(x) + \u03b3 if x \u2208B and f (x) \u2264gA(x) \u2212\u03b3 if x \u2208A \\ B.\nNote that only the restriction of gA to A matters in this definition. This function\nacts as a threshold for binary classification. More precisely, given a function g : A \u2192\nR, one can associate to every f \u2208F the binary function fg with fg(x) equal to 1 if\nf (x) \u2265g(x) and to 0 otherwise. Letting Fg = {fg : f \u2208F } we see that F P-shatters A\nif there exists a function gA such that FgA shatters A. The definition of P\u03b3-shattering\nintroduces a margin in the definition of fg (with fg(x) equal to 1 if f (x) \u2265g(x) + \u03b3, to\n0 if f (x) \u2264g(x) \u2212\u03b3 and is ambiguous otherwise), and A is P\u03b3-shattered by F if, for\nsome gA, the corresponding FgA shatters A without ambiguities.\nDefinition 22.36 One then defines the P-dimension of F by\nP-dim(F ) = max{|A| : A \u2282R,F P-shatters A},\nand similarly the P\u03b3-dimension of F is\nP\u03b3-dim(F ) = max{|A| : A \u2282R,F P\u03b3-shatters A}.\nThe P\u03b3-dimension of F will replace the VC-dimension in order to control the\ncovering numbers. More precisely, we have the following theorem [4].\nTheorem 22.37 Let \u03b3 > 0 and assume that F has P\u03b3/4-dimension D < \u221e. Then,\nN\u221e(\u03b3,N) \u22642\n 16N\n\u03b32\n!D log(4eN/(D\u03b3))\n.\n618\nCHAPTER 22. GENERALIZATION BOUNDS\nProof The proof is quite technical and relies on a combinatorial argument in which\nF is first assumed to take integer values before addressing the continuous case.\nStep 1. We first assume that functions in F take values in the finite set {1,...,r}\nwhere r is an integer. For the time of this proof, we introduce yet another notion of\nshattering called S-shattering (for strong shattering) which is essentially the same\nas P1-shattering, except that functions g are restricted to take values in {1,...,r}. Let\nA be a finite subset of R. Given a function g : R \u2192{1,...,r}, we say that (F ,g) S-\nshatters A if, for any B \u2282A, there exist f \u2208F satisfying f (x) \u2265g(x) + 1 for x \u2208B and\nf (x) \u2264g(x) \u22121 if x \u2208A \\ B. We say that F S-shatters A if (F ,g) S-shatters A for some\ng. The S-dimension of F is the cardinality of the largest subset of R that can be\nS-shattered and will be denoted S-dim(F ). The first, and most difficult, part of the\nproof is to show that, if S-dim(F ) = D, then\nM(F (A),\u03c1\u221e,2) \u22642(|A|r2)\u2308log2 y\u2309\nwith\ny =\nD\nX\nk=1\n |A|\nk\n!\nrk\nand \u2308i\u2309denotes the smallest integer larger than u \u2208R. Here, M is the packing\nnumber defined in section 22.5.1.\nTo prove this, we can assume that r \u22653, since, for r \u22642, M(F (A),\u03c1\u221e,2) = 1 (the\ndiameter of F for the \u03c1\u221edistance is 0 or 1). Let G(A) = {1,...,r}A be the set of all\nfunctions f : A \u2192{1,...,r} and let\nUA = \bF \u2282G(A) : \u2200f ,f \u2032 \u2208F,\u2203x \u2208A with |f (x) \u2212f \u2032(x)| \u22652\t .\nFor F \u2208UA, let\nSA(F) = {(B,g) : B \u2282A,B , \u2205,g : B \u2192{1,...,r},(F,g) S-shatters B}.\nLet tA(h) = min{|SA(F)| : F \u2208UA,|F| = h} (where the minimum of the empty set is +\u221e).\nSince we are considering in UA all possible functions from A to {1,...,r}, it is clear\nthat tA(h) only depends on |A|, and we will also denote it by t(h,|A|).\nNote that, by definition, if (B,g) \u2208SA(F), and F \u2282F , then |B| \u2264D. So, the number\nof elements in SA(F) for such an F is less or equal than the number of possible such\npairs (B,g), which is strictly less than y = PD\nk=1\n\u0000|A|\nk\n\u0001rk. So, if t(h,|A|) \u2265y, then there\ncannot be any F \u2282F in the set UA and M(F (A),\u03c1\u221e,2) < h. The rest of the proof\nconsists in showing that t(h,|A|) \u2265y.\nFor any n \u22651, we have t(2,n) = 1: fix x \u2208A, and F = {f1,f2} \u2208G such that f1(x) = 1,\nf2(x) = 3 and f1(y) = f2(y) if y , x. Then only ({x},g) is S-shattered by F, with g such\nthat g(x) = 2.\n22.6. OTHER COMPLEXITY MEASURES\n619\nNow, assume that, for some integer m, t(2mnr2,n) < \u221e, so that there exists F \u2208UA\nsuch that |F| = 2mnr2. Arrange the elements of F into mnr2 pairs {fi,f \u2032\ni }. For each\nsuch pair, there exists xi \u2208A such that |fi(xi) \u2212f \u2032\ni (xi)| > 1. Since there are at most n\nselected xi, one of them must be appearing at least mr2 times. Call it x and keep (and\nreindex) the corresponding mr2 pairs, still denoted {fi,f \u2032\ni }. Now, there are at most\nr(r \u22121)/2 possible distinct values for the unordered pairs {fi(x),f \u2032\ni (x)}, so that one of\nthem must be appearing at least 2mr2/r(r \u22121) > 2m times. Select these functions,\nreindex them and exchange the role of fi and f \u2032\ni if needed to obtain 2m pairs {fi,f \u2032\ni }\nsuch that fi(x) = k and f \u2032\ni (x) = l for all i and fixed k,l \u2208{1,...,r} such that k + 1 < l.\nLet F1 = {f1,...,f2m} and F\u2032\n1 = {f \u2032\n1,...,f \u2032\n2m}. Let A\u2032 = A \\ {x}. Then both F1 and F\u2032\n1\nbelong to UA\u2032, which implies that both SA\u2032(F1) and SA\u2032(F\u2032\n1) have cardinality at least\nt(2m,n\u22121). Moreover, both sets are included in SA(F), and if (B,g) \u2208SA\u2032(F1)\u2229SA\u2032(F\u2032\n1),\nthen (B \u222a{x},g\u2032) \u2208SA(F), with g\u2032(y) = g(y) for y \u2208B and g\u2032(x) = k + 1. This provides\n2t(2m,n\u22121) elements in SA(F) and shows the key inequality (which is obviously true\nwhen the left-hand side is infinite)\nt(2mnr2,n) \u22652t(2m,n \u22121).\nThis inequality can now be used to prove by induction that for all 0 \u2264k < n, one\nhas t(2(nr2)k,n) \u22652k, since\nt(2((n + 1)r2)k+1,n + 1) \u22652t(2((n + 1)r2)k,n) \u22652t(2(nr2)k,n).\nFor k \u2265n, one has 2(nr2)k > rn, where rn is the number of functions in G(A), so\nthat t(2(nr2)k,n) = +\u221e. So, t(2(nr2)k,n) \u22652k is valid for all k and it suffices to take\nk = \u2308log2 y\u2309to obtain the desired result.\nStep 2. The next step uses a discretization scheme to extend the previous result to\nfunctions with values in [\u22121,1]. More precisely, given f : R \u2192[0,1], and \u03b7 > 0, let\nf \u03b7(x) = max{k \u2208N : 2k\u03b7 \u22121 \u2264f (x)}\nwhich takes values in {0,...,r} for r = \u230a\u03b7\u22121\u230b. If F is a class of functions with values\nin [\u22121,1], define F \u03b7 = {f \u03b7 : f \u2208F }. With this notation, the following holds.\n(a) For all \u03b3 \u2264\u03b7: S-dim(F \u03b7) \u2264P\u03b3-dim(F )\n(b) For all \u03f5 \u22654\u03b7 and A \u2282R: M(F (A),\u03c1\u221e,\u03f5) \u2264M\u221e(F \u03b7(A),\u03c1\u221e,2).\nTo prove (a), assume that F \u03b7 S-shatters A, so that there exists g such that, for all\nB \u2282A, there exists f \u2208F such that f \u03b7(x) \u2265g(x) + 1 for x \u2208B and f \u03b7(x) \u2264g(x) \u22121\nfor x \u2208A \\ B. Using the fact that 2\u03b7f \u03b7(x) \u22121 \u2264f (x) < 2\u03b7f \u03b7(x) + 2\u03b7 \u22121, we get f (x) \u2265\n2\u03b7g(x)+2\u03b7\u22121 for x \u2208B and f (x) \u22642\u03b7g(x)\u22121 for x \u2208A\\B. So taking \u02dcg(x) = 2\u03b7g(x)+\u03b7\u22121\n620\nCHAPTER 22. GENERALIZATION BOUNDS\nas threshold function (which does not depend on B), we see that F P\u03b3-shatters A if\n\u03b3 \u2264\u03b7.\nFor (b), we deduce from the definition of f \u03b7 that |f \u03b7(x) \u2212\u02dcf \u03b7(x)| > (2\u03b7)\u22121|f (x) \u2212\n\u02dcf (x)| \u22121 so that, if \u03f5 = 4\u03b7, |f (x) \u2212\u02dcf (x)| \u2265\u03f5 implies |f \u03b7(x) \u2212\u02dcf \u03b7(x)| > 1, or, equivalently\n|f \u03b7(x) \u2212\u02dcf \u03b7(x)| \u22652.\nStep 3. We can now conclude. Taking \u03b3 > 0, we have, if |A| = N\nN (F (A),\u03c1\u221e,\u03b3) \u2264M(F (A),\u03c1\u221e,\u03b3) \u2264M(F \u03b3/4(A),\u03c1\u221e,2) \u22642\n 16N\n\u03b32\n!\u2308logy\u2309\nwith\ny =\nD\nX\nk=1\n N\nk\n!\n(\u03b3/4)\u2212k \u2264(\u03b3/4)\u2212D\nD\nX\nk=1\n N\nk\n!\n\u2264\n 4Ne\nD\u03b3\n!D\n.\nSince the maximum of N (F (A),\u03c1\u221e,\u03b3) over A with cardinality N is N\u221e(\u03b3,N), the\nproof is complete.\n\u25a0\nOne can use this result to evaluate margin bounds on linear classifiers with\nbounded data. Let R be the ball with radius \u039b in Rd and consider the model class\ncontaining all functions f (x) = a0 + bT x with a0 \u2208[\u2212\u039b,\u039b] and b \u2208Rd, |b| \u22641. Let\nA = {x1,...,xN} be a finite subset of R. Then, F P\u03b3-shatters A if and only if there\nexists g1,...,gN \u2208R such that, for any sequences \u03be = (\u03be1,...,\u03beN) \u2208{\u22121,1}N , there\nexists a\u03be\n0 \u2208[\u2212\u039b,\u039b] and b\u03be \u2208Rd, |b\u03be| \u22641 with \u03bek(a\u03be\n0 + (b\u03be)T xk \u2212gk) \u2265\u03b3 for k = 1,...,N.\nSumming over N, we find that\nN\u03b3 +\nN\nX\nk=1\ngk\u03bek \u2264a\u03be\n0\nN\nX\nk=1\n\u03bek + (b\u03be)T\nN\nX\nk=1\n\u03bekxk .\nThis shows that, for any sequence \u03be1,...,\u03beN,\nN\u03b3 +\nN\nX\nk=1\ngk\u03bek \u2264\u039b\n\f\f\f\f\nN\nX\nk=1\n\u03bek\n\f\f\f\f +\n\f\f\f\f\nN\nX\nk=1\n\u03bekxk\n\f\f\f\f\nApplying the same inequality after changing the signs of \u03be1,...,\u03beN yields\nN\u03b3 \u2264N\u03b3 +\n\f\f\f\f\nN\nX\nk=1\ngk\u03bek\n\f\f\f\f \u2264\u039b\n\f\f\f\f\nN\nX\nk=1\n\u03bek\n\f\f\f\f +\n\f\f\f\f\nN\nX\nk=1\n\u03bekxk\n\f\f\f\f.\nThis shows, in particular, that (letting \u03be1,...,\u03beN be independent Rademacher ran-\ndom variables)\nP\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\u039b\n\f\f\f\f\nN\nX\nk=1\n\u03bek\n\f\f\f\f +\n\f\f\f\f\nN\nX\nk=1\n\u03bekxk\n\f\f\f\f \u2265N\u03b3\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8= 1.\n22.6. OTHER COMPLEXITY MEASURES\n621\nHowever, using the identity (A + B)2 \u22642A2 + 2B2, we have\nP\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\u039b\n\f\f\f\f\nN\nX\nk=1\n\u03bek\n\f\f\f\f +\n\f\f\f\f\nN\nX\nk=1\n\u03bekxk\n\f\f\f\f \u2265N\u03b3\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\u2264P\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed2\u039b2\f\f\f\f\nN\nX\nk=1\n\u03bek\n\f\f\f\f\n2\n+ 2\n\f\f\f\f\nN\nX\nk=1\n\u03bekxk\n\f\f\f\f\n2\n\u2265N 2\u03b32\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8.\nSince\nE\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed2\u039b2\f\f\f\f\nN\nX\nk=1\n\u03bek\n\f\f\f\f\n2\n+ 2\n\f\f\f\f\nN\nX\nk=1\n\u03bekxk\n\f\f\f\f\n2\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8= 2N\u039b2 + 2\nn\nX\nk=1\n|xk|2 \u22644N\u039b2,\nMarkov\u2019s inequality implies\nP\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed2\u039b2\f\f\f\f\nN\nX\nk=1\n\u03bek\n\f\f\f\f\n2\n+ 2\n\f\f\f\f\nN\nX\nk=1\n\u03bekxk\n\f\f\f\f\n2\n\u2265N 2\u03b32\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\u22644\u039b2\nN\u03b32 .\nWe get a contradiction unless N \u22644\u039b2/\u03b32, which shows that P\u03b3-dim(F ) \u22644\u039b2/\u03b32.\nTheorem 22.37 then implies that\nN\u221e(\u03b3,N) \u22642\n 16N\n\u03b32\n! 63\u039b2\n\u03b32 log\n\u0010 16eN\u03b3\n\u039b2\n\u0011\nand this upper bound can then be plugged into equations (22.43) or (22.44) to esti-\nmate the generalization error.\nBeyond the explicit expression of the upper bound, the important point in the\nprevious argument is that the P\u03b3-dimension is bounded independently from the di-\nmension d of X (and therefore also applies in the infinite-dimensional case). This\nshould be compared to what we found for the VC-dimension of separating hyper-\nplanes, which was d + 1 (cf. proposition 22.22).\nRemark 22.38 Note that the upper-bound obtained in theorem 22.34 depends on a\nparameter (\u03b3) and the result is true for any choice of this parameter. It is tempting at\nthis point to optimize the bound with respect to \u03b3, but this would be a mistake since\na family of events being likely does not imply that their intersection is likely too.\nHowever, with a little work, one can ensure that an intersection of slightly weaker\ninequalities holds. Indeed, assume that an estimate similar to (22.43) holds, in the\nform\nP(R0( \u02c6fT ) > UT (\u03b3) + t) \u2264C(\u03b3)e\u2212mt2/2\nor, equivalently\nP\n\u0012\nR0( \u02c6fT ) > UT (\u03b3) +\nq\nt2 + 2logC(\u03b3)\n\u0013\n\u2264e\u2212mt2/2 ,\n622\nCHAPTER 22. GENERALIZATION BOUNDS\nwhere UT (\u03b3) depends on data and is increasing (as a function of \u03b3), and C(\u03b3) is a\ndecreasing function of \u03b3. Consider a decreasing sequence (\u03b3k) that converges to 0\n(for example \u03b3k = L2\u2212k). Choose also an increasing function \u03f5(\u03b3). Then\nP\n\u0012\nR0( \u02c6fT ) > min{UT (\u03b3) +\nq\nt2 + 2logC(\u03b3) + \u03f52(\u03b3) : 0 \u2264\u03b3 \u2264L}\n\u0013\n\u2264P\n\u0012\nR0( \u02c6fT ) > min{UT (\u03b3k) +\nq\nt2 + 2logC(\u03b3k\u22121) + \u03f52(\u03b3k) : k \u22651}\n\u0013\n.\nMoreover\nP\n\u0012\nR0( \u02c6fT ) > min{UT (\u03b3k) +\nq\nt2 + 2logC(\u03b3k\u22121) + \u03f52(\u03b3k) : k \u22651}\n\u0013\n\u2264\n\u221e\nX\nk=0\nP\n\u0012\nR0( \u02c6fT ) > UT (\u03b3k) +\nq\nt2 + 2logC(\u03b3k\u22121) + \u03f5(\u03b3k)\n\u0013\n\u2264\n\u221e\nX\nk=0\nC(\u03b3k)\nC(\u03b3k\u22121)e\u2212m\u03f52(\u03b3k)/2\u2212mt2/2 .\nSo, it suffices to choose \u03f5(\u03b3) so that\nC0 =\n\u221e\nX\nk=1\nC(\u03b3k)\nC(\u03b3k\u22121)e\u2212m\u03f52(\u03b3k)/2 < \u221e\nto ensure that\nP\n\u0012\nR0( \u02c6fT ) > min{UT (\u03b3) +\nq\nt2 + 2logC(\u03b3) + \u03f52(\u03b3) : \u03b30 \u2264\u03b3 \u2264L}\n\u0013\n\u2264C0e\u2212mt2/2.\nFor example, if \u03b3k = L2\u2212k, one can take\n\u03f5(\u03b3) =\ns\n2\nm\n \nlog C(\u03b3)\nC(\u03b3/2) + log\u03b3\u22121\n!\nwhich yields C0 \u2264L.\n\u2666\n22.6.2\nMaximum discrepancy\nLet T be a training set and let T1 and T2 form a fixed partition of the training set\nin two equal parts. Assume, for simplicity, that N is even and that the method\nfor selecting the two parts is deterministic, e.g., place the first half of T in T1 and\nsecond one in T2. Following Bartlett et al. [20], one can then define the maximum\ndiscrepancy on T by\nCT = sup\nf \u2208F\n(ET1(f ) \u2212ET2(f ))\n22.6. OTHER COMPLEXITY MEASURES\n623\nThis discrepancy measures the extent to which estimators may differ when trained\non two independent half-sized training sets. For a binary classification problem, the\nestimation of CT can be made with the same algorithm as the initial classifier, since\nET1(f ) \u2212ET2(f ) is, up to a constant, exactly the classification error for the training set\nin which the class labels are flipped for the data in T2.\nFollowing [20], we now discuss concentration bounds that rely on CT and start\nwith the following Lemma.\nLemma 22.39 Introduce the function\n\u03a6(T) = sup\nf \u2208F\n(R(f ) \u2212ET (f )) \u2212sup\nf \u2208F\n(ET1(f ) \u2212ET2(f )).\nThen E(\u03a6(T )) \u22640.\nProof Note that, if T \u2032 is a training set, independent of T with identical distribution,\nthen, for any f0 \u2208F ,\nR(f0) \u2212ET (f0) = E(ET \u2032(f0) \u2212ET (f0) | T ) \u2264E(sup\nf \u2208F\n(ET \u2032(f ) \u2212ET (f )) | T )\nso that\nE(sup\nf \u2208F\n(R(f ) \u2212ET (f ))) \u2264E(sup\nf \u2208F\n(ET \u2032(f ) \u2212ET (f ))).\nNow, for a given f , we have ET (f ) = 1\n2(ET1(f ) + ET2(f )) and splitting T \u2032 the same way,\nwe have ET \u2032(f ) = 1\n2(ET \u2032\n1(f ) + ET \u2032\n2(f )).\nWe can therefore write\nE(sup\nf \u2208F\n(R(f ) \u2212ET (f ))) \u22641\n2E(sup\nf \u2208F\n(ET \u2032\n1(f ) \u2212ET1(f )) + (ET \u2032\n2(f ) \u2212ET2(f )))\n\u22641\n2\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8edE(sup\nf \u2208F\n(ET \u2032\n1(f ) \u2212ET1(f ))) + E(sup\nf \u2208F\n(ET \u2032\n2(f ) \u2212ET2(f ))\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\n= E(sup\nf \u2208F\n(ET1(f ) \u2212ET2(f )))\nwhere we have used the fact that both (T \u2032\n1,T1) and (T \u2032\n2,T2) form random training sets\nwith identical distribution to (T1,T2).\nThis proves that E(\u03a6(T )) \u22640.\n\u25a0\nUsing the lemma, one can write\nP(sup\nf \u2208F\n(R(f ) \u2212ET (f )) \u2265CT + \u03f5) = P(\u03a6(T ) \u2265\u03f5) \u2264P(\u03a6(T ) \u2212E(\u03a6(T )) \u2265\u03f5).\n624\nCHAPTER 22. GENERALIZATION BOUNDS\nOne can then use McDiarmid\u2019s inequality (theorem 22.13) after noticing that, letting\nzk = (xk,yk) for k = 1,...,N,\nmax\nz1,...,zN,z\u2032\nk\n\f\f\f\u03a6(z1,...,zN) \u2212\u03a6(z1,...,zk\u22121,z\u2032\nk,zk+1,...,zN)\n\f\f\f \u22643\nN\nyielding\nP(sup\nf \u2208F\n(R(f ) \u2212ET (f )) \u2265CT + \u03f5) \u2264e\u22122N\u03f52\n9 .\n22.6.3\nRademacher complexity\nWe now extend the previous definition by computing discrepancies over random\ntwo-set partitions of the training set, which have equal size in average. This leads\nto the empirical Rademacher complexity of the function class. Let \u03be1,...,\u03beN be a\nsequence of Rademacher random variables (equal to -1 and +1 with equal probabil-\nity 1/2). Then, the (empirical) Rademacher complexity of the training set T for the\nmodel class F is\nrad(T) = E\n\u0012\nsup\nf \u2208F\n1\nN\nN\nX\nk=1\n\u03bekr(Yk,f (Xk))\n\f\f\f\f T = T\n\u0013\n.\nThe mean Rademacher complexity is then the expectation of this quantity over\nthe training set distribution. The Rademacher complexity can be computed with\na\u2014costly\u2014Monte-Carlo simulation, in which the best estimator is computed with\nrandomly flipped labels corresponding to the values of k such that \u03bek = \u22121.\nThis measure of complexity was introduced to the machine learning framework\nin Koltchinskii and Panchenko [109], Bartlett and Mendelson [19], and Rademacher\nsums have been extensively studied in relation to empirical processes (cf. Ledoux\nand Talagrand [117], chapter 4).\nOne can bound the Rademacher complexity in terms of VC dimension.\nProposition 22.40 Let F be a function class such that D = VC-dim(F ) < \u221e. Then\nrad(T) \u2264\n3\n\u221a\nN\np\n2D log(eN/D).\nProof One has, using Hoeffding\u2019s inequality\nP\n\u0012\nsup\nf \u2208F\n1\nN\nN\nX\nk=1\n\u03bekr(yk,fk) > t\n\u0013\n\u2264|F (T )|sup\nf \u2208F\nP\n\u0012 1\nN\nN\nX\nk=1\n\u03bekr(yk,fk) > t\n\u0013\n\u2264|F (T )|e\u2212Nt2/2.\n22.6. OTHER COMPLEXITY MEASURES\n625\nThis implies that\nP\n\u0012\nsup\nf \u2208F\n\f\f\f\f\n1\nN\nN\nX\nk=1\n\u03bekr(yk,fk)\n\f\f\f\f > t\n\u0013\n\u22642|F (T )|e\u2212Nt2/2\nand proposition 22.4 implies\nrad(T) \u22643\np\n2|F (T)|\n\u221a\nN\n.\nTherefore if D = VC-dim(F ) < \u221e, proposition 22.20 implies\nrad(T ) \u2264\n3\n\u221a\nN\np\n2D log(eN/D).\n\u25a0\nWe now discuss generalization bounds using Rademacher\u2019s complexity. While we\nstill consider binary classification problems (with RY = {\u22121,1}), we will assume that\nF contains functions that can take arbitrary scalar values, and the 0\u20131 loss function\nbecomes r(y,y\u2032) = 1yy\u2032\u22640 with y \u2208{\u22121,1} and y\u2032 \u2208R. We will also consider functions\nthat dominate this loss, i.e., functions \u03c1 : RY \u00d7 R \u2192[0,1] such that\nr(y,y\u2032) \u2264\u03c1(y,y\u2032)\nfor all y \u2208RY, y\u2032 \u2208R. Some examples are the margin loss \u03c1\u2217\nh(y,y\u2032) = 1yy\u2032\u2264h for h \u22650,\nor the piecewise linear function\n\u03c1h(y,y\u2032) =\n\uf8f1\uf8f4\uf8f4\uf8f4\uf8f4\uf8f2\uf8f4\uf8f4\uf8f4\uf8f4\uf8f3\n1\nif yy\u2032 \u22640\n1 \u2212yy\u2032/h\nif 0 \u2264yy\u2032 \u2264h\n0\nif yy\u2032 \u2265h\nIf G is a class of functions g : Z \u2192R, we will denote\nRadG(z1,...,zN) = 1\nN E\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8edsup\ng\u2208G\nN\nX\nk=1\n\u03bekg(zk)\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\nand\nRadG(N) = E\n\u0010\nRadG(Z1,...,ZN)\n\u0011\n.\nOur previous notation can then be rewritten as rad(T ) = RadG(z1,...,zn) where zi =\n(xi,yi) and G is the space of functions: g : (x,y) 7\u2192r(y,f (x)) for f \u2208F . The following\ntheorem is proved in Koltchinskii and Panchenko [109], Bartlett and Mendelson [19].\n626\nCHAPTER 22. GENERALIZATION BOUNDS\nTheorem 22.41 Let \u03c1 be a function dominating the risk function r(y,y\u2032) = 1yy\u2032\u22640. Let\nG\u03c1 = {(x,y) 7\u2192\u03c1(y,f (x)) \u22121 : f \u2208F }\nand\nE\u03c1\nT (f ) = 1\nN\nN\nX\nk=1\n\u03c1(yk,f (xk)).\nThen\nP(sup\nf \u2208F\nR(f ) \u2265E\u03c1\nT (f ) + 2RadG\u03c1(N) + t) \u2264e\u2212Nt2/2\nProof For f \u2208F , we have\nR(f ) \u2212E\u03c1(f ) \u2264E(\u03c1(Y,f (X))) \u2212E\u03c1(f ) \u2264\u03a6(Z1,...,ZN)\nwhere\n\u03a6(Z1,...,ZN) = sup\ng\u2208G\u03c1\n\u0012\nE(g(Z)) \u22121\nN\nN\nX\nk=1\ng(Zk)\n\u0013\n.\nSince changing one variable among Z1,...,ZN changes \u03a6 by at most 2/N, McDi-\narmid\u2019s inequality implies that\nP(\u03a6(Z1,...,ZN) \u2212E(\u03a6(Z1,...,ZN)) \u2265t) \u2264e\u2212Nt2/2.\nNow we have\nE(\u03a6(Z1,...,ZN)) = E\n\u0012\nsup\ng\u2208G\u03c1 E\n\u0012 1\nN\nN\nX\nk=1\ng(Z\u2032\nk) \u22121\nN\nN\nX\nk=1\ng(Zk)\n\f\f\f\fZ1,...,ZN\n\u0013\u0013\n\u2264E\n\u0012\nsup\ng\u2208G\u03c1\n\u0012 1\nN\nN\nX\nk=1\ng(Z\u2032\nk) \u22121\nN\nN\nX\nk=1\ng(Zk)\n\u0013\u0013\n\u2264E\n\u0012\nE\n\u0012\nsup\ng\u2208G\u03c1\n\u0012 1\nN\nN\nX\nk=1\n\u03bek(g(Z\u2032\nk) \u2212g(Zk))\n\u0013\f\f\fZ,Z\u2032\u0013\u0013\n\u22642E\n\u0012\nE\n\u0012\nsup\ng\u2208G\u03c1\n\u0012 1\nN\nN\nX\nk=1\n\u03bekg(Zk)\n\u0013\f\f\fZ\n\u0013\u0013\n\u22642RadG\u03c1(N),\nof which the statement of the theorem is a direct consequence.\n\u25a0\n22.6. OTHER COMPLEXITY MEASURES\n627\n22.6.4\nAlgorithmic Stability\nAnother result using McDiarmid\u2019s inequality is proved in Bousquet and Elisseeff\n[39], and is based on the stability of a classifier when one removes a single example\nfrom the training set. As before, we consider training sets T of size N, where T is a\nrandom variable.\nFor k \u2208{1,...,N}, and a training set T = (x1,y1,...,xN,yN), we let T (k) be the train-\ning set with sample (xk,yk) removed. One says that the predictor (T 7\u2192\u02c6fT ) has uni-\nform stability \u03b2N for the loss function r if, for all T of size N, all k \u2208{1,...,N}, and\nall x,y:\n|r( \u02c6fT (x),y) \u2212r( \u02c6fT (k)(x),y)| \u2264\u03b2N .\n(22.45)\nWith this definition, the following theorem holds.\nTheorem 22.42 (Bousquet and Elisseeff [39]) Assume that \u02c6fT has uniform stability\n\u03b2N for training sets of size N and that the loss function r(Y,f (X)) is almost surely\nbounded by M > 0. Then, for all \u03f5 > 2\u03b2N, one has\nP(R( \u02c6fT ) \u2265ET ( \u02c6fT ) + \u03f5) \u2264e\u22122N\n\u0010\n\u03f5\u22122\u03b2N\n4N\u03b2N +M\n\u00112\n.\nOf course, this theorem is interesting only when \u03b2N is small as a function of N, i.e.,\nwhen N\u03b2N is bounded.\nProof Let Zi = (Xi,Yi) and F(Z1,...,ZN) = R( \u02c6fT ) \u2212ET ( \u02c6fT ). We want to apply McDi-\narmid inequality (theorem 22.13) to F, and therefore estimate\n\u03b4k(F)\n\u2206=\nmax\nz1,...,zN,z\u2032\nk\n\f\f\fF(z1,...,zN) \u2212F(z1,...,zk\u22121,z\u2032\nk,zk+1,...,zN)\n\f\f\f.\nIntroduce a training set \u02dcTk in which the variable zk is replaced by z\u2032\nk = (x\u2032\nk,y\u2032\nk).\nBecause \u02dcT (k)\nk\n= T (k), we have\n|R( \u02c6fT ) \u2212R( \u02c6f \u02dcTk)|\n\u2264\nE(|r(Y, \u02c6fT (X)) \u2212r(Y, \u02c6f \u02dcTk(X)))|\n\u2264\nE(|r(Y, \u02c6fT (X)) \u2212r(Y, \u02c6fT (k)(X))|)\n+E(|r(Y, \u02c6f \u02dcTk(X)) \u2212E(r(Y, \u02c6f \u02dcT (k)\nk (X)))|)\n\u2264\n2\u03b2N\n628\nCHAPTER 22. GENERALIZATION BOUNDS\nSimilarly, we have\n|ET ( \u02c6fT ) \u2212E \u02dcTk)( \u02c6f \u02dcTk)|\n\u2264\n1\nN\nX\nl,k\n|r(yl, \u02c6fT (xl),) \u2212r(yl, \u02c6f \u02dcTk(xl))|\n+ 1\nN |r(yk, \u02c6fT (xk)) \u2212r(y\u2032\nk, \u02c6f \u02dcTk(x\u2032\nk))|\n\u2264\n1\nN\nX\nl,k\n|r(yl, \u02c6fT (xl)) \u2212r(yl, \u02c6fT (k)(xl))|\n+ 1\nN\nX\nl,k\n|r(yl, \u02c6f \u02dcTk(xl)) \u2212r(yl, \u02c6f \u02dcT (k)\nk (xl))| + M\nN\n\u2264\n2\u03b2N + M\nN\nCollecting these results, we find that \u03b4k(F) \u22644\u03b2N + M\nN , so that, by theorem 22.13,\nP\n\u0010\nR( \u02c6fT ) \u2265ET ( \u02c6fT ) + E(R( \u02c6fT ) \u2212ET ( \u02c6fT ))\n\u0011\n+ \u03f5) \u2264exp\n \n\u2212\n2N\u03f52\n(4N\u03b2N + M)2\n!\n.\nIt remains to evaluate the expectation in this formula. Introducing as above vari-\nables Z\u2032\n1,...,Z\u2032\nN and using the same notation for \u02dcTk, we have\nE(R( \u02c6fT )) = E(r(Y \u2032\nk,fT (X\u2032\nk))) = E(r(Yk,f \u02dcTk(Xk))).\nUsing this, we have\nE(R( \u02c6fT ) \u2212ET ( \u02c6fT ))\n=\n1\nN\nN\nX\nk=1\nE(r(Yk,f \u02dcTk(Xk)) \u2212r(Yk,fT (Xk)))\n=\n1\nN\nN\nX\nk=1\nE(r(Yk,f \u02dcTk(Xk)) \u2212r(Yk,f \u02dcT (k)\nk (Xk)))\n+ 1\nN\nN\nX\nk=1\nE(r(Yk,fT (k)\nk (Xk)) \u2212r(Yk,fT (Xk)))\nfrom which one deduces that\n|E(R( \u02c6fT ) \u2212ET ( \u02c6fT ))| \u22642\u03b2N.\nWe therefore obtain\nP\n\u0010\nR( \u02c6fT ) \u2265ET ( \u02c6fT ) + \u03f5 + 2\u03b2N\n\u0011\n\u2264exp\n \n\u2212\n2N\u03f52\n(4N\u03b2N + M)2\n!\n.\nas required.\n\u25a0\n22.6. OTHER COMPLEXITY MEASURES\n629\n22.6.5\nPAC-Bayesian bounds\nOur final discussion of concentration bounds for the empirical error uses a slightly\ndifferent paradigm from that discussed so far. The main difference is that, instead of\ncomputing one predictor \u02c6fT from a training set T, it would return a random variable\nwith values in F , or, equivalently, a probability distribution on F (therefore assum-\ning that this space is measurable) that we will denote \u02c6\u00b5T . The training set error is\nnow defined by:\n\u00afET (\u00b5) =\nZ\nET (f )d\u00b5(f ),\nfor any probability distribution \u00b5 on F , while the generalization error is:\n\u00afR(\u00b5) =\nZ\nF\nR(f )d\u00b5(f ).\nOur goal is to obtain upper bounds on \u00afR(\u00b5T )\u2212\u00afET (\u00b5T ) that hold with high probability.\nIn this framework, we have the following result, in which Q denotes the space of\nprobability distributions on F .\nAssume that the loss function r takes its values in [0,1]. Recall that KL(\u00b5\u2225\u03c0) is\nthe Kullback-Leibler divergence from \u00b5 to \u03c0, defined by\nKL(\u00b5\u2225\u03c0) =\nZ\nF\nlog(\u03d5(f ))\u03d5(f )d\u03c0(f )\nif \u00b5 has a density \u03d5 with respect to \u03c0 and +\u221eotherwise. Then, the following theorem\nholds.\nTheorem 22.43 (McAllester [128]) With the notation above, for any fixed probability\ndistribution \u03c0 \u2208Q,\nP\n\u0012\nsup\n\u00b5\u2208Q\n( \u00afR(\u00b5) \u2212\u00afET (\u00b5)) >\nr\nt + KL(\u00b5\u2225\u03c0)\n2N\n\u0013\n\u22642Ne\u2212Nt.\n(22.46)\nTaking t = log(2N/\u03b4)/2N, the theorem is equivalent to the statement that, with prob-\nability 1 \u2212\u03b4, one has\n\u00afR(\u00b5) \u2212\u00afET (\u00b5) \u2264\nr\nlog2N/\u03b4 + KL(\u00b5\u2225\u03c0)\n2N\n.\n(22.47)\nProof We first show that, for any probability distributions \u03c0,\u00b5 on F , and any func-\ntion H on F ,\nZ\nF\nH(f )d\u00b5 \u2212log\nZ\nF\neH(f )d\u03c0 \u2264KL(\u00b5\u2225\u03c0).\n630\nCHAPTER 22. GENERALIZATION BOUNDS\nIndeed, assume that \u00b5 has a density \u03d5 with respect to \u03c0 (otherwise the upper bound\nis infinite) and let\n\u03d5H =\neH\nR\nF eH(f )d\u03c0\n.\nThen\nKL(\u00b5\u2225\u03c0) \u2212\nZ\nF\nH(f )d\u00b5 + log\nZ\nF\neH(f )d\u03c0 =\nZ\nF\n\u03d5 log\u03d5d\u03c0 \u2212\nZ\nF\n\u03d5 log\u03d5Hd\u03c0\n=\nZ\nF\n\u03d5\n\u03d5H\n \nlog \u03d5\n\u03d5H\n!\n\u03d5Hd\u03c0\n= KL(\u00b5\u2225\u03d5H\u03c0) \u22650,\nwhich proves the result (and also shows that one can only have equality when \u03d5 =\n\u03d5H \u03c0-almost surely.)\nLet \u03c7(u) = max(u,0)2. We can use this inequality to show that, for any probability\nQ \u2208Q and \u03bb > 0,\n\u03bb\u03c7( \u00afR(\u00b5) \u2212\u00afET (\u00b5)) \u2264\u03bb\nZ\nF\n\u03c7(R(f ) \u2212ET (f ))d\u00b5(f ) \u2264KL(\u00b5\u2225\u03c0) + log\nZ\nF\ne\u03bb\u03c7(R(f )\u2212ET (f ))d\u03c0\nwhere we have applied Jensen\u2019s inequality to the convex function \u03c7. This yields\ne\u03bb\u03c7( \u00afR(\u00b5)\u2212\u00afET (Q)) \u2264eKL(\u00b5\u2225\u03c0)\nZ\nF\ne\u03bb\u03c7(R(f )\u2212ET (f ))d\u03c0.\nHoeffding\u2019s inequality implies that, for all f \u2208F and t \u22650\nP(\u03c7(R(f ) \u2212ET (f )) > t) = P(R(f ) \u2212ET (f ) >\n\u221a\nt) \u2264e\u22122Nt\nso that\nE\n\u0010\ne\u03bb\u03c7(R(f )\u2212ET (f ))\u0011\n=\nZ \u221e\n0\nP(\u03bb\u03c7(R(f ) \u2212ET (f )) > logt)dt\n\u22641 +\nZ e\u03bb\n1\ne\u22122N logt\n\u03bb\ndt\n= 1 +\nZ \u03bb\n0\ne\u22122Nu\n\u03bb +udu\n= 1 + \u03bbe\u03bb\u22122N \u22121\n\u03bb \u22122N\n.\nFrom this and Markov\u2019s inequality, we get, for any \u03bb > 0:\nP(sup\n\u00b5\u2208Q\n\u03c7( \u00afR(\u00b5) \u2212\u00afET (\u00b5)) > t + KL(\u00b5\u2225\u03c0)/\u03bb) \u2264e\u2212\u03bbt\n \n1 + \u03bbe\u03bb\u22122N \u22121\n\u03bb \u22122N\n!\n.\n22.7. APPLICATION TO MODEL SELECTION\n631\nTaking \u03bb = 2N yields\nP(sup\n\u00b5\u2208Q\n\u03c7( \u00afR(\u00b5) \u2212\u00afET (\u00b5)) > t + KL(\u00b5\u2225\u03c0)/2N) \u22642Ne\u22122Nt,\nwhich implies\nP\n\u0012\nsup\n\u00b5\u2208Q\n\u00afR(\u00b5) \u2212\u00afET (\u00b5) >\nq\nt + KL(\u00b5\u2225\u03c0)/2N\n\u0013\n\u22642Ne\u22122Nt,\nconcluding the proof.\n\u25a0\nRemark 22.44 Note that the proof, which follows that given in Audibert and Bous-\nquet [15], provides a family of inequalities obtained by taking \u03bb = 2N/c in the final\nstep, with c > 1. In this case\n1 + \u03bbe\u03bb\u22122N \u22121\n\u03bb \u22122N\n\u22641 +\n\u03bb\n2N \u2212\u03bb =\nc\nc \u22121\nand one gets\nP\n\u0012\nsup\n\u00b5\u2208Q\n\u00afR(\u00b5) \u2212\u00afET (\u00b5) >\nq\nt + cKL(\u00b5\u2225\u03c0)/2N\n\u0013\n\u2264\nc\nc \u221212Ne\u22122Nt.\n\u2666\nRemark 22.45 One special case of theorem 22.43 is when \u03c0 is a discrete probability\nmeasure supported by a subset F0 of F and \u00b5 corresponds to a deterministic pre-\ndictor optimized over F0, and is therefore a Dirac measure on some element f \u2208F0.\nBecause \u03b4f has density \u03d5(g) = 1/\u03c0(g) if g = f and 0 otherwise with respect to \u03c0, we\nhave KL(\u03b4f \u2225\u03c0) = \u2212log\u03c0(f ) and theorem 22.43 implies that, with probability larger\nthan 1 \u2212\u03b4,\nR(f ) \u2212ET (f ) \u2264\nr\nlog2N/\u03b4 \u2212log\u03c0(f )\n2N\n.\nThe term log2N is however superfluous in this simple context, because one can\nwrite, for any t > 0\nP\n\u0012\nsup\nf \u2208F0\nR(f ) \u2212ET (f ) \u2265\nr\nt \u2212log(\u03c0(f ))\n2N\n\u0013\n\u2264\nX\nf \u2208F0\ne\u22122N(t log(\u03c0(f ))\n2N\n) = e\u22122Nt\nso that, with probability 1 \u2212\u03b4 (letting t = log(1/\u03b4)/2N), for all f \u2208F0:\nR(f ) \u2212ET (f ) \u2264\nr\n\u2212log\u03b4 \u2212log\u03c0(f )\n2N\n.\n\u2666\n632\nCHAPTER 22. GENERALIZATION BOUNDS\n22.7\nApplication to model selection\nWe now describe how the previous results can, in principle, be applied to model\nselection [20]. We assume that we have a countable family of nested models classes\n(F (j),j \u2208J ). Denote, as usual, by ET (f ) the empirical prediction error in the training\nset for a given function f . We will denote by \u02c6f (j)\nT\na minimizer of the in-sample error\nfor F (j), such that\nET ( \u02c6f (j)\nT ) = min\nf \u2208F (j) ET (f ).\nIn the model selection problem, one would like to determine the best model class,\nj = j(T ), such that the prediction error R( \u02c6f (j)\nT ) is minimal, or, more realistically, de-\ntermine j\u2217such that R( \u02c6f (j\u2217)\nT\n) is not too far from the optimal one.\nWe will consider penalty-based methods in which one minimizes \u02dcET (f ) = ET (f )+\nCT (j) to determine j(T ). The penalty, CT , may also be data-dependent, and will\ntherefore be a random variable. The previous concentration inequalities provided\nhighly probable upper-bounds for R( \u02c6f (j)\nT ), each exhibiting a random variable \u0393(j)\nT\nthat\nis larger than R( \u02c6f (j)\nT ) with probability close to one. More precisely, we obtained in-\nequalities taking the form (when applied to F (j))\nP(RT ( \u02c6f (j)) \u2265\u0393(j)\nT + t) \u2264cje\u2212mt2\n(22.48)\nfor some known constants cj and m. For example, the VC-dimension bounds have\n\u0393(j)\nT\n= ET ( \u02c6f (j)\nT ), cj = 2SF (j)(2N) and m = N/8.\nGiven such inequalities, one can develop a model selection strategy that relies on\na priori weights, provided by a sequence \u03c0j of positive numbers such that P\nj\u2208J \u03c0j =\n1. Define\n\u02dc\u03c0j =\n\u03c0j/cj\nP\u221e\nj\u2032=1 \u03c0j\u2032/cj\u2032 ,\nand let\nC(j)\nT = \u0393(j)\nT \u2212ET ( \u02c6f (j)\nT ) +\nr\n\u2212\nlog \u02dc\u03c0j\nm\nyielding a penalty-based method that requires the minimization of\n\u02dcET (f ) = (ET (f ) \u2212ET ( \u02c6f (j)\nT )) + \u0393(j)\nT +\nr\n\u2212\nlog \u02dc\u03c0j\nm\n.\nThe selected model class is then F (j\u2217) where j\u2217minimizes \u0393(j)\nT +\nq\n\u2212\nlog \u02dc\u03c0j\n2m .\n22.7. APPLICATION TO MODEL SELECTION\n633\nThe same proof as that provided at the end of section 22.6.5 justifies this proce-\ndure. Indeed, for t > 0,\nP\n\u0010\nR( \u02c6fT ) \u2212\u02dcET ( \u02c6fT ) \u2265t\n\u0011\n\u2264P\n \nmax\nj\n(R( \u02c6f (j)\nT ) \u2212\u02dcET ( \u02c6f (j)\nT )) \u2265t\n!\n\u2264P\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8edmax\nj\n(R( \u02c6f (j)\nT ) \u2265R\u2217\nj + t +\nr\n\u2212\nlog \u02dc\u03c0j\nm\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\n\u2264\u02dcc\nX\nj\n\u03c0je\u2212mt2\n\u2264\u02dcce\u2212mt2\nwith \u02dcc = P\u221e\nj=1 \u03c0j/cj.\n634\nCHAPTER 22. GENERALIZATION BOUNDS\nBibliography\n[1] Pierre-Antoine Absil, Robert Mahony, and Rodolphe Sepulchre. Optimization\nalgorithms on matrix manifolds. Princeton University Press, 2008.\n[2] Hirotugu Akaike. Information theory and an extension of the maximum like-\nlihood principle. In 2nd International Symposium on Information Theory, 1973.\nAkademiai Kaido, 1973.\n[3] St\u00b4ephanie Allassonniere and Laurent Younes. A stochastic algorithm for prob-\nabilistic independent component analysis. The Annals of Applied Statistics, 6\n(1):125\u2013160, 2012.\n[4] Noga Alon, Shai Ben-David, Nicolo Cesa-Bianchi, and David Haussler. Scale-\nsensitive dimensions, uniform convergence, and learnability. Journal of the\nACM (JACM), 44(4):615\u2013631, 1997.\n[5] Mauricio A. \u00b4Alvarez, Lorenzo Rosasco, and Neil D. Lawrence. Kernels for\nvector-valued functions: A review. Foundations and Trends in Machine Learn-\ning, 4(3):195\u2013266, 2012. ISSN 1935-8237. doi: 10.1561/2200000036.\n[6] Yali Amit. Convergence properties of the gibbs sampler for perturbations of\ngaussians. The Annals of Statistics, 24(1):122\u2013140, 1996.\n[7] Yali Amit and Donald Geman. Shape quantization and recognition with ran-\ndomized trees. Neural computation, 9(7):1545\u20131588, 1997.\n[8] Alano Ancona, Donald Geman, Nobuyuki Ikeda, and D Geman.\nRandom\nfields and inverse problems in imaging. In Ecole d\u2019ete de Probabilites de Saint-\nFlour XVIII-1988, pages 115\u2013193. Springer, 1990.\n[9] Brian D.O. Anderson. Reverse-time diffusion equation models. Stochastic Pro-\ncesses and their Applications, 12(3):313\u2013326, 1982.\n[10] Martin Anthony and Peter L. Bartlett.\nNeural network learning: Theoretical\nfoundations. cambridge university press, 2009.\n635\n636\nBIBLIOGRAPHY\n[11] Martin Arjovsky, Soumith Chintala, and L\u00b4eon Bottou. Wasserstein Genera-\ntive Adversarial Networks. In Proceedings of the 34th International Conference\non Machine Learning - Volume 70, ICML\u201917, pages 214\u2013223. JMLR.org, 2017.\nevent-place: Sydney, NSW, Australia.\n[12] Nachman Aronszajn. Theory of Reproducing Kernels. Trans. Am. Math. Soc.,\n68:337\u2013404, 1950.\n[13] Krishna B. Athreya, Hani Doss, and Jayaram Sethuraman. On the convergence\nof the markov chain simulation method. The Annals of Statistics, 24(1):69\u2013100,\n1996.\n[14] Hagai Attias.\nA Variational Baysian Framework for Graphical Models.\nIn\nNIPS, volume 12. Citeseer, 1999.\n[15] Jean-Yves Audibert and Olivier Bousquet.\nCombining pac-bayesian and\ngeneric chaining bounds. Journal of Machine Learning Research, 8(Apr):863\u2013\n889, 2007.\n[16] Adrian Barbu and Song-Chun Zhu. Generalizing swendsen-wang to sampling\narbitrary posterior probabilities.\nIEEE Transactions on Pattern Analysis and\nMachine Intelligence, 27(8):1239\u20131253, 2005.\n[17] Viorel Barbu. Differential equations. Springer, 2016.\n[18] Peter Bartlett and John Shawe-Taylor. Generalization performance of support\nvector machines and other pattern classifiers. Advances in Kernel methods\u2014\nsupport vector learning, pages 43\u201354, 1999.\n[19] Peter L. Bartlett and Shahar Mendelson. Rademacher and gaussian complexi-\nties: Risk bounds and structural results. Journal of Machine Learning Research,\n3(Nov):463\u2013482, 2002.\n[20] Peter L. Bartlett, St\u00b4ephane Boucheron, and G\u00b4abor Lugosi. Model selection and\nerror estimation. Machine Learning, 48:85\u2013113, 2002.\n[21] Peter L. Bartlett, Nick Harvey, Christopher Liaw, and Abbas Mehrabian.\nNearly-tight vc-dimension and pseudodimension bounds for piecewise linear\nneural networks. Journal of Machine Learning Research, 20(63):1\u201317, 2019.\n[22] Amir Beck. Introduction to nonlinear optimization: Theory, algorithms, and ap-\nplications with MATLAB. SIAM, 2014.\n[23] Michel Bena\u00a8\u0131m. Dynamics of stochastic approximation algorithms. In Semi-\nnaire de probabilites XXXIII, pages 1\u201368. Springer, 1999.\n[24] George Bennett. Probability inequalities for the sum of independent random\nvariables. Journal of the American Statistical Association, 57(297):33\u201345, 1962.\nBIBLIOGRAPHY\n637\n[25] Albert Benveniste, Michel M\u00b4etivier, and Pierre Priouret. Adaptive algorithms\nand stochastic approximations, volume 22. Springer Science & Business Media,\n2012.\n[26] Nils Berglund. Long-time dynamics of stochastic differential equations. arXiv\npreprint arXiv:2106.12998, 2021.\n[27] Dimitri Bertsekas. Convex optimization theory, volume 1. Athena Scientific,\n2009.\n[28] Rajendra Bhatia. Matrix analysis, volume 169. Springer Science & Business\nMedia, 2013.\n[29] Peter J. Bickel and Kjell A. Doksum. Mathematical statistics: basic ideas and\nselected topics, volume I, volume 117. CRC Press, 2015.\n[30] Peter J. Bickel, Ya\u2019acov Ritov, and Alexandre B. Tsybakov. Simultaneous anal-\nysis of lasso and dantzig selector. 2009.\n[31] Patrick Billingsley. Probability and measure. John Wiley & Sons, 2008.\n[32] Patrick Billingsley. Convergence of probability measures. John Wiley & Sons,\n2013.\n[33] Salomon Bochner. Vorlesungen \u00a8uber fouriersche integrale. Bull Amer Math\nSoc, 39:184, 1933.\n[34] Vladimir I. Bogachev. Measure Theory. Springer, 2007.\n[35] Joseph-Fr\u00b4ed\u00b4eric Bonnans, Jean Charles Gilbert, Claude Lemar\u00b4echal, and Clau-\ndia A. Sagastiz\u00b4abal. Numerical optimization: theoretical and practical aspects.\nSpringer Science & Business Media, 2006.\n[36] Ingwer Borg and Patrick J.F. Groenen. Modern multidimensional scaling: Theory\nand applications. Springer Science & Business Media, 2005.\n[37] Jonathan Borwein and Adrian S. Lewis. Convex analysis and nonlinear opti-\nmization: theory and examples. Springer Science & Business Media, 2010.\n[38] St\u00b4ephane Boucheron, G\u00b4abor Lugosi, and Pascal Massart. A sharp concentra-\ntion inequality with applications. Random Structures & Algorithms, 16(3):277\u2013\n292, 2000.\n[39] Olivier Bousquet and Andr\u00b4e Elisseeff. Stability and generalization. Journal of\nmachine learning research, 2(Mar):499\u2013526, 2002.\n638\nBIBLIOGRAPHY\n[40] Stephen Boyd, Neal Parikh, Eric Chu, Borja Peleato, and Jonathan Eckstein.\nDistributed optimization and statistical learning via the alternating direction\nmethod of multipliers. Foundations and Trends\u00ae in Machine learning, 3(1):1\u2013\n122, 2011.\n[41] Leo Breiman. Bagging predictors. Machine learning, 24(2):123\u2013140, 1996.\n[42] Leo Breiman. Random forests. Machine learning, 45(1):5\u201332, 2001.\n[43] Leo Breiman, Jerome Friedman, Charles J Stone, and Richard A. Olshen. Clas-\nsification and regression trees. CRC press, 1984.\n[44] Dmitri Burago, Iu D. Burago, Yuri Burago, Sergei A. Ivanov, and Sergei Ivanov.\nA course in metric geometry, volume 33. American Mathematical Soc., 2001.\n[45] Jian-Feng Cai, Emmanuel J. Cand`es, and Zuowei Shen.\nA singular value\nthresholding algorithm for matrix completion. SIAM Journal on optimization,\n20(4):1956\u20131982, 2010.\n[46] Tadeusz Cali\u00b4nski and Jerzy Harabasz. A dendrite method for cluster analysis.\nCommunications in Statistics-theory and Methods, 3(1):1\u201327, 1974.\n[47] Emmanuel J. Candes and Terence Tao. Decoding by linear programming. IEEE\nTrans. information theory, 51(12):4203\u20134215, 2005.\n[48] Emmanuel J. Candes and Terence Tao. The dantzig selector: statistical esti-\nmation when p is much larget. Annals of statistics, 35, 2007.\n[49] Emmanuel J. Cand`es, Xiaodong Li, Yi Ma, and John Wright. Robust principal\ncomponent analysis? Journal of the ACM (JACM), 58(3):11, 2011.\n[50] John Canny. Gap: a factor model for discrete data. In Proceedings of the 27th\nannual international ACM SIGIR conference on Research and development in in-\nformation retrieval, pages 122\u2013129, 2004.\n[51] B. Chalmond. An iterative Gibbsian technique for reconstruction of m-ary\nimages. Pattern recognition, 22(6):747\u2013761, 1989. ISSN 0031-3203.\n[52] Ricky T. Q. Chen, Yulia Rubanova, Jesse Bettencourt, and David Duve-\nnaud.\nNeural ordinary differential equations.\nIn S. Bengio, H. Wallach,\nH. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, Advances\nin Neural Information Processing Systems 31, pages 6571\u20136583. Curran Asso-\nciates, Inc., 2018.\n[53] Tianqi Chen and Carlos Guestrin. Xgboost: A scalable tree boosting system.\nIn Proceedings of the 22nd acm sigkdd international conference on knowledge dis-\ncovery and data mining, pages 785\u2013794, 2016.\nBIBLIOGRAPHY\n639\n[54] Pierre Comon. Independent component analysis, a new concept? Signal pro-\ncessing, 36(3):287\u2013314, 1994.\n[55] Thomas M. Cover and Joy A. Thomas. Elements of information theory. John\nWiley & Sons, 2012.\n[56] Robert G. Cowell, A. Philip Dawid, Steffen L. Lauritzen, and David J. Spiegel-\nhalter. Probabilistic networks and expert systems. Springer, 2007.\n[57] George Darmois. Analyse g\u00b4en\u00b4erale des liaisons stochastiques: etude partic-\nuli`ere de l\u2019analyse factorielle lin\u00b4eaire. Revue de l\u2019Institut international de statis-\ntique, pages 2\u20138, 1953.\n[58] Bernard Delyon, Marc Lavielle, and Eric Moulines. Convergence of a stochas-\ntic approximation version of the em algorithm. Annals of statistics, pages 94\u2013\n128, 1999.\n[59] Amir Dembo and Ofer Zeitouni. Large deviations techniques and applica-\ntions. 1998. Applications of Mathematics, 38, 2011.\n[60] Luc Devroye, L\u00b4azl\u00b4o Gy\u00a8orfi, and G\u00b4abor Lugosi. A Probabilistic Theory of Pattern\nRecognition. Springer, 1996.\n[61] Luc Devroye, Abbas Mehrabian, and Tommy Reddad. The total variation dis-\ntance between high-dimensional gaussians with the same mean. arXiv preprint\narXiv:1810.08693, 2018.\n[62] Jean Dieudonn\u00b4e. Infinitesimal Calculus. Houghton Mifflin, 1971.\n[63] Edsger W. Dijkstra. A note on two problems in connexion with graphs. Nu-\nmerische mathematik, 1(1):269\u2013271, 1959. ISSN 0029-599X.\n[64] Petros Drineas, Alan Frieze, Ravi Kannan, Santosh Vempala, and Vish-\nwanathan Vinay. Clustering large graphs via the singular value decomposi-\ntion. Machine learning, 56:9\u201333, 2004.\n[65] Simon Duane, Anthony D. Kennedy, Brian J. Pendleton, and Duncan Roweth.\nHybrid monte carlo. Physics letters B, 195(2):216\u2013222, 1987.\n[66] Richard M. Dudley. Real analysis and probability. Chapman and Hall/CRC,\n2018.\n[67] Marie Duflo. Random iterative models, volume 34. Springer Science & Business\nMedia, 2013.\n[68] HA Eiselt, Carl-Louis Sandblom, et al. Nonlinear optimization: Methods and\napplications. Springer, 2019.\n640\nBIBLIOGRAPHY\n[69] Stewart N. Ethier and Thomas G. Kurtz. Markov processes: Characterization\nand convergence. 1986.\n[70] Mark Everingham, Luc Van Gool, Christopher KI Williams, John Winn, and\nAndrew Zisserman. The pascal visual object classes (voc) challenge. Interna-\ntional journal of computer vision, 88(2):303\u2013338, 2010.\n[71] James A. Fill. An interruptible algorithm for perfect sampling via Markov\nchains. The Annals of Applied Probability, 8(1):131\u2013162, 1998.\n[72] P. Thomas Fletcher and Sarang Joshi. Principal geodesic analysis on symmetric\nspaces: Statistics of diffusion tensors. In Computer vision and mathematical\nmethods in medical and biomedical image analysis, pages 87\u201398. Springer, 2004.\n[73] Yoav Freund and Robert E. Schapire. A decision-theoretic generalization of\non-line learning and an application to boosting. Journal of computer and system\nsciences, 55(1):119\u2013139, 1997. Publisher: Elsevier.\n[74] Jerome Friedman, Trevor Hastie, and Robert Tibshirani. Additive logistic re-\ngression: a statistical view of boosting (with discussion and a rejoinder by the\nauthors). The annals of statistics, 28(2):337\u2013407, 2000.\n[75] Jerome H. Friedman. Greedy function approximation: a gradient boosting\nmachine. Annals of statistics, pages 1189\u20131232, 2001. Publisher: JSTOR.\n[76] Dan Geiger and Judea Pearl. On the logic of causal models. In Machine intel-\nligence and pattern recognition, volume 9, pages 3\u201314. Elsevier, 1990.\n[77] Dan Geiger, Thomas Verma, and Judea Pearl. Identifying independence in\nbayesian networks. Networks, 20(5):507\u2013534, August 1990. ISSN 00283045.\ndoi: 10.1002/net.3230200504.\n[78] Donald Geman, Christian d\u2019Avignon, Daniel Q. Naiman, and Raimond L\nWinslow. Classifying gene expression profiles from pairwise mrna compar-\nisons. Statistical applications in genetics and molecular biology, 3(1):1\u201319, 2004.\n[79] Stuart Geman and Donald Geman. Stochastic relaxation, gibbs distributions,\nand the bayesian restoration of images. IEEE Transactions on pattern analysis\nand machine intelligence, (6):721\u2013741, 1984.\n[80] Stuart Geman and Chii-Ruey Hwang. Nonparametric maximum likelihood\nestimation by the method of sieves. The Annals of Statistics, pages 401\u2013414,\n1982.\n[81] M. Gondran and M. Minoux. Graphs and algorithms. John Wiley & Sons, 1983.\nBIBLIOGRAPHY\n641\n[82] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-\nFarley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adver-\nsarial nets. In Advances in neural information processing systems, pages 2672\u2013\n2680, 2014.\n[83] Ulf Grenander. Abstract Inference. Wiley, 1981.\n[84] Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and\nAaron C. Courville. Improved training of Wasserstein GANs. In Advances\nin neural information processing systems, pages 5767\u20135777, 2017.\n[85] Madan M. Gupta and J. Qi. Theory of T-norms and fuzzy inference methods.\nFuzzy Sets and Systems, 40(3):431\u2013450, April 1991. ISSN 0165-0114.\n[86] Te Sun Han. Nonnegative entropy measures of multivariate symmetric corre-\nlations. Information and Control, 36:133\u2013156, 1978.\n[87] Trevor Hastie, Robert Tibshirani, and Jerome H. Friedman. The elements of\nstatistical learning. Springer, 2003.\n[88] W. Keith Hastings. Monte carlo sampling methods using markov chains and\ntheir applications. 1970.\n[89] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual\nlearning for image recognition. In Proceedings of the IEEE Conference on Com-\nputer Vision and Pattern Recognition, pages 770\u2013778. IEEE, 2016.\n[90] Geoffrey E. Hinton and Sam Roweis. Stochastic neighbor embedding. Ad-\nvances in neural information processing systems, 15:857\u2013864, 2002.\n[91] Leslie M. Hocking. Optimal Control: An Introduction to the Theory with Appli-\ncations. Oxford University Press, 1991.\n[92] Wassily Hoeffding. Probability inequalities for sums of bounded random vari-\nables. In The Collected Works of Wassily Hoeffding, pages 409\u2013426. Springer,\n1994.\n[93] Roger A. Horn and Charles R. Johnson. Matrix analysis. Cambridge university\npress, 2012.\n[94] Aapo Hyv\u00a8arinen. New approximations of differential entropy for independent\ncomponent analysis and projection pursuit. In Advances in neural information\nprocessing systems, pages 273\u2013279, 1998.\n[95] Aapo Hyv\u00a8arinen and Peter Dayan. Estimation of non-normalized statistical\nmodels by score matching. Journal of Machine Learning Research, 6(4), 2005.\n642\nBIBLIOGRAPHY\n[96] Nobuyuki Ikeda and Shinzo Watanabe.\nStochastic differential equations and\ndiffusion processes. Elsevier, 1981.\n[97] Tommi Sakari Jaakkola.\nVariational methods for inference and estimation in\ngraphical models. PhD Thesis, Massachusetts Institute of Technology, 1997.\n[98] Vojtech Jarnik. O jistem problemu minimalnim (about a certain minimal prob-\nlem). Prace Moravske Prirodovedecke Spolecnosti, 6:57\u201363, 1930.\n[99] Finn Jensen and Frank Jensen. Optimal junction trees. In Proceedings of the\nTenth Conference on Uncertainty in Artificial Intelligence, pages 360\u2013366, 1994.\n[100] Michael I. Jordan, Zoubin Ghahramani, Tommi S. Jaakkola, and Lawrence K.\nSaul. An introduction to variational methods for graphical models. Machine\nlearning, 37(2):183\u2013233, 1999.\n[101] Abram M. Kagan, Calyampudi Radhakrishna Rao, and Yurij Vladimirovich\nLinnik. Characterization problems in mathematical statistics. 1973.\n[102] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimiza-\ntion. arXiv preprint arXiv:1412.6980, 2014.\n[103] Diederik P. Kingma and Max Welling. Auto-Encoding Variational Bayes. 2014.\n[104] Diederik P. Kingma and Max Welling. An Introduction to Variational Autoen-\ncoders. Foundations and Trends\u00ae in Machine Learning, 12(4):307\u2013392, 2019.\nPublisher: Now Publishers, Inc.\n[105] John Kingman. Completely random measures. Pacific Journal of Mathematics,\n21(1):59\u201378, 1967.\n[106] Peter E. Kloeden and Eckhard Platen. Numerical solutions of stochastic differen-\ntial equations. Springer, 1992.\n[107] Ivan Kobyzev, Simon J.D. Prince, and Marcus A. Brubaker. Normalizing flows:\nAn introduction and review of current methods. IEEE transactions on pattern\nanalysis and machine intelligence, 43(11):3964\u20133979, 2020.\n[108] Daphne Koller and Nir Friedman. Probabilistic graphical models: principles and\ntechniques. The MIT Press, 2009.\n[109] Vladimir Koltchinskii and Dmitry Panchenko. Empirical margin distributions\nand bounding the generalization error of combined classifiers. The Annals of\nStatistics, 30(1):1\u201350, 2002.\n[110] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. Imagenet classifica-\ntion with deep convolutional neural networks. Communications of the ACM,\n60(6):84\u201390, 2017.\nBIBLIOGRAPHY\n643\n[111] Wojtek J. Krzanowski and Y.T. Lai. A criterion for determining the number of\ngroups in a data set using sum-of-squares clustering. Biometrics, pages 23\u201334,\n1988.\n[112] Estelle Kuhn and Marc Lavielle. Coupling a stochastic approximation version\nof EM with an MCMC procedure. ESAIM: Probability and Statistics, 8:115\u2013131,\n2004. Publisher: EDP Sciences.\n[113] Harold Kushner and G. George Yin.\nStochastic approximation and recursive\nalgorithms and applications, volume 35. Springer Science & Business Media,\n2003.\n[114] Steffen L Lauritzen. Graphical models, volume 17. Clarendon Press, 1996.\n[115] Yann LeCun and Yoshua Bengio. Convolutional networks for images, speech,\nand time series. The handbook of brain theory and neural networks, 3361(10):\n1995, 1995.\n[116] Yann LeCun, Bernhard Boser, John S. Denker, Donnie Henderson, Richard E.\nHoward, Wayne Hubbard, and Lawrence D. Jackel. Backpropagation applied\nto handwritten zip code recognition. Neural computation, 1(4):541\u2013551, 1989.\n[117] Michel Ledoux and Michel Talagrand. Probability in Banach Spaces: isoperime-\ntry and processes. Springer Science & Business Media, 1991.\n[118] Erich L. Lehmann and George Casella. Theory of point estimation. Springer\nScience & Business Media, 2006.\n[119] Benedict Leimkuhler and Sebastian Reich. Simulating Hamiltonian Dynamics.\nCambridge Monographs on Applied and Computational Mathematics. Cam-\nbridge University Press, 2005.\n[120] Lennart Ljung. Analysis of recursive stochastic algorithms. IEEE Transactions\non Automatic Control, 22(4):551\u2013575, August 1977. ISSN 1558-2523. Confer-\nence Name: IEEE Transactions on Automatic Control.\n[121] Stuart Lloyd. Least squares quantization in pcm. IEEE transactions on infor-\nmation theory, 28(2):129\u2013137, 1982.\n[122] Laurens van der Maaten and Geoffrey Hinton. Visualizing data using t-SNE.\nJournal of machine learning research, 9(Nov):2579\u20132605, 2008.\n[123] Jack Macki and Aaron Strauss.\nIntroduction to Optimal Control Theory.\nSpringer Science & Business Media, 2012.\n[124] James MacQueen. Some methods for classification and analysis of multivari-\nate observations. In Proceedings of the fifth Berkeley symposium on mathematical\nstatistics and probability, volume 1, pages 281\u2013297. Oakland, CA, USA, 1967.\n644\nBIBLIOGRAPHY\n[125] Adam a Margolin, Ilya Nemenman, Katia Basso, Chris Wiggins, Gustavo\nStolovitzky, Riccardo Dalla Favera, and Andrea Califano. ARACNE: an al-\ngorithm for the reconstruction of gene regulatory networks in a mammalian\ncellular context. BMC bioinformatics, 7 Suppl 1:S7, January 2006. ISSN 1471-\n2105. doi: 10.1186/1471-2105-7-S1-S7.\n[126] Enzo Marinari and Giorgio Parisi. Simulated tempering: a new monte carlo\nscheme. Europhysics letters, 19(6):451, 1992.\n[127] Pascal Massart.\nConcentration inequalities and model selection, volume 6.\nSpringer, 2007.\n[128] David A. McAllester. Pac-bayesian model averaging. In COLT, volume 99,\npages 164\u2013170. Citeseer, 1999.\n[129] James A. McHugh. Algorithmic graph theory. New Jersey: Prentice-Hall Inc,\n1990.\n[130] Leland McInnes, John Healy, and James Melville. UMAP: Uniform Manifold\nApproximation and Projection for Dimension Reduction. arXiv:1802.03426\n[cs, stat], September 2020. arXiv: 1802.03426.\n[131] Henry P. McKean. Stochastic integrals, volume 353. American Mathematical\nSociety, 1969.\n[132] Kerrie L. Mengersen and Richard L. Tweedie.\nRates of convergence of the\nhastings and metropolis algorithms. The annals of Statistics, 24(1):101\u2013121,\n1996.\n[133] Nicholas Metropolis, Arianna W. Rosenbluth, Marshall N. Rosenbluth, Au-\ngusta H. Teller, and Edward Teller. Equation of state calculations by fast com-\nputing machines. The journal of chemical physics, 21(6):1087\u20131092, 1953.\n[134] Sean P. Meyn and Richard L. Tweedie. Stability of markovian processes ii:\nContinuous-time processes and sampled chains. Advances in Applied Probabil-\nity, 25(3):487\u2013517, 1993.\n[135] Sean P. Meyn and Richard L. Tweedie. Stability of markovian processes iii:\nFoster\u2013lyapunov criteria for continuous-time processes. Advances in Applied\nProbability, 25(3):518\u2013548, 1993.\n[136] Sean P. Meyn and Richard L. Tweedie. Markov chains and stochastic stability.\nSpringer Science & Business Media, 2012.\n[137] Leon Mirsky. A trace inequality of john von neumann. Monatshefte f\u00a8ur mathe-\nmatik, 79(4):303\u2013306, 1975.\nBIBLIOGRAPHY\n645\n[138] Michel M\u00b4etivier and Pierre Priouret. Th\u00b4eor`emes de convergence presque sure\npour une classe d\u2019algorithmes stochastiques `a pas d\u00b4ecroissant. Probability The-\nory and related fields, 74(3):403\u2013428, 1987. Publisher: Springer.\n[139] Elizbar A. Nadaraya. On estimating regression. Theory of Probability & Its\nApplications, 9(1):141\u2013142, 1964.\n[140] Radford M. Neal. Sampling from multimodal distributions using tempered\ntransitions. Statistics and computing, 6:353\u2013366, 1996.\n[141] Radford M. Neal. Markov chain sampling methods for dirichlet process mix-\nture models. Journal of computational and graphical statistics, 9(2):249\u2013265,\n2000.\n[142] Radford M. Neal.\nMcmc using hamiltonian dynamics.\narXiv preprint\narXiv:1206.1901, 2012.\n[143] Radford M. Neal and Geoffrey E. Hinton. A view of the EM algorithm that jus-\ntifies incremental, sparse, and other variants. In Learning in graphical models,\npages 355\u2013368. Springer, 1998.\n[144] R. E. Neapolitan. Learning Bayesian networks. Prentice Hall, 2004.\n[145] Arkadi Nemirovski, Anatoli Juditsky, Guanghui Lan, and Alexander Shapiro.\nRobust Stochastic Approximation Approach to Stochastic Programming.\nSIAM Journal on Optimization, 19(4):1574\u20131609, January 2009. ISSN 1052-\n6234. Publisher: Society for Industrial and Applied Mathematics.\n[146] Jorge Nocedal and Stephen J. Wright. Nonlinear Equations. Springer, 2006.\n[147] Esa Nummelin. General irreducible Markov chains and non-negative operators.\nNumber 83. Cambridge University Press, 2004.\n[148] George Papamakarios, Eric Nalisnick, Danilo Jimenez Rezende, Shakir Mo-\nhamed, and Balaji Lakshminarayanan. Normalizing flows for probabilistic\nmodeling and inference. Journal of Machine Learning Research, 22(57):1\u201364,\n2021.\n[149] Panos M. Pardalos and Jue Xue. The maximum clique problem. Journal of\nGlobal Optimization, 4(3):301\u2013328, 1994. ISSN 0925-5001.\n[150] Emanuel Parzen. On estimation of a probability density function and mode.\nThe annals of mathematical statistics, 33(3):1065\u20131076, 1962.\n[151] Judea Pearl. Probabilistic reasoning in intelligent systems. Morgan Kaufmann,\n1988, 2012.\n646\nBIBLIOGRAPHY\n[152] Jiming Peng and Yu Wei. Approximating k-means-type clustering via semidef-\ninite programming. SIAM journal on optimization, 18(1):186\u2013205, 2007.\n[153] Jiming Peng and Yu Xia. A new theoretical framework for k-means-type clus-\ntering. Foundations and advances in data mining, pages 79\u201396, 2005. Publisher:\nSpringer.\n[154] Odile Pons. Functional estimation for density, regression models and processes.\nWorld scientific, 2011.\n[155] Robert C. Prim. Shortest connection networks and some generalizations. Bell\nsystem technical journal, 36(6):1389\u20131401, 1957.\n[156] James G. Propp and David B. Wilson. Exact sampling with coupled Markov\nchains and applications to statistical mechanics. Random Structures and Algo-\nrithms, 9(1&2):223\u2013252, 1996.\n[157] James G. Propp and David B. Wilson. How to get a perfectly random sam-\nple from a generic Markov chain and generate a random spanning tree of a\ndirected graph. Journal of Algorithms, 27:170\u2013217, 1998.\n[158] Jim O. Ramsay and Bernard W. Silverman. Functional Data Analysis. Springer-\nVerlag, 1997.\n[159] BLS Prakasa Rao. Nonparametric functional estimation. Academic press, 1983.\n[160] Daniel Revuz. Markov chains. Elsevier, 2008.\n[161] Danilo Rezende and Shakir Mohamed. Variational inference with normalizing\nflows. In International conference on machine learning, pages 1530\u20131538. PMLR,\n2015.\n[162] Jorma Rissanen. Stochastic complexity in statistical inquiry. World Scientific,\n1989.\n[163] Herbert Robbins and Sutton Monro. A stochastic approximation method. In\nHerbert Robbins Selected Papers, pages 102\u2013109. Springer, 1985.\n[164] Gareth O. Roberts and Nicholas G. Polson. On the geometric convergence of\nthe gibbs sampler. Journal of the Royal Statistical Society Series B: Statistical\nMethodology, 56(2):377\u2013384, 1994.\n[165] Gareth O. Roberts and Jeffrey S. Rosenthal. General state space markov chains\nand mcmc algorithms. Probability Surveys, 1:20\u201371, 2004.\n[166] Gareth O. Roberts and Richard L. Tweedie.\nExponential convergence of\nlangevin distributions and their discrete approximations. Bernoulli, 2(4):341\u2013\n363, 1996.\nBIBLIOGRAPHY\n647\n[167] R. Tyrrell Rockafellar. Convex analysis, volume 18. Princeton university press,\n1970.\n[168] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-Net: Convolutional\nNetworks for Biomedical Image Segmentation.\nIn Nassir Navab, Joachim\nHornegger, William M. Wells, and Alejandro F. Frangi, editors, Medical Im-\nage Computing and Computer-Assisted Intervention \u2013 MICCAI 2015, Lecture\nNotes in Computer Science, pages 234\u2013241, Cham, 2015. Springer Interna-\ntional Publishing. ISBN 978-3-319-24574-4.\n[169] Kenneth Rose, Eitan Gurewitz, and Geoffrey Fox. A deterministic annealing\napproach to clustering. Pattern Recognition Letters, 11(9):589\u2013594, 1990.\n[170] Peter J. Rousseeuw. Silhouettes: a graphical aid to the interpretation and val-\nidation of cluster analysis. Journal of computational and applied mathematics,\n20:53\u201365, 1987.\n[171] Walter Rudin. Real and Complex Analysis. Tata McGraw Hill, 1966.\n[172] Robert E. Schapire. The strength of weak learnability. Machine learning, 5(2):\n197\u2013227, 1990.\n[173] Isaac J. Schoenberg. Metric spaces and completely monotone functions. An-\nnals of Mathematics, pages 811\u2013841, 1938.\n[174] Gideon Schwarz. Estimating the dimension of a model. The annals of statistics,\n6(2):461\u2013464, 1978.\n[175] Claude E. Shannon. A mathematical theory of communication. The Bell system\ntechnical journal, 27(3):379\u2013423, 1948.\n[176] Claude E. Shannon. Communication in the presence of noise. Proc. Institute\nof Radio Engineers, 37(1):10\u201321, 1949.\n[177] Simon J. Sheather and Michael C. Jones.\nA reliable data-based bandwidth\nselection method for kernel density estimation. Journal of the Royal Statistical\nSociety: Series B (Methodological), 53(3):683\u2013690, 1991.\n[178] Bernard W. Silverman. Density estimation for statistics and data analysis. Chap-\nman et Hall, 1998.\n[179] Viktor Pavlovich Skitovich. Linear forms of independent random variables\nand the normal distribution law. Izvestiya Rossiiskoi Akademii Nauk. Seriya\nMatematicheskaya, 18(2):185\u2013200, 1954.\n[180] Yang Song, Sahaj Garg, Jiaxin Shi, and Stefano Ermon. Sliced score match-\ning: A scalable approach to density and score estimation. In Uncertainty in\nArtificial Intelligence, pages 574\u2013584. PMLR, 2020.\n648\nBIBLIOGRAPHY\n[181] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Rus-\nlan Salakhutdinov. Dropout: a simple way to prevent neural networks from\noverfitting. The Journal of Machine Learning Research, 15(1):1929\u20131958, 2014.\n[182] Hugo Steinhaus. Sur la division des corp materiels en parties. Bull. Acad.\nPolon. Sci, 1(804):801, 1956.\n[183] Charles J. Stone. Consistent nonparametric regression. The annals of statistics,\npages 595\u2013620, 1977.\n[184] Mervyn Stone. Cross-validatory choice and assessment of statistical predic-\ntions. Journal of the royal statistical society: Series B (Methodological), 36(2):\n111\u2013133, 1974.\n[185] Catherine A. Sugar and Gareth M. James. Finding the number of clusters in a\ndataset: An information-theoretic approach. Journal of the American Statistical\nAssociation, 98(463):750\u2013763, 2003.\n[186] Robert H. Swendsen and Jian-Sheng Wang. Nonuniversal critical dynamics in\nmonte carlo simulations. Physical review letters, 58(2):86, 1987.\n[187] Esteban G. Tabak and Eric Vanden-Eijnden. Density estimation by dual ascent\nof the log-likelihood. 2010.\n[188] Michel Talagrand. The generic chaining: upper and lower bounds of stochastic\nprocesses. Springer Science & Business Media, 2006.\n[189] Michel Talagrand. Upper and lower bounds for stochastic processes: modern meth-\nods and classical problems, volume 60.\nSpringer Science & Business Media,\n2014.\n[190] Aik Choon Tan, Daniel Q. Naiman, Lei Xu, Raimond L. Winslow, and Don-\nald Geman. Simple decision rules for classifying human cancers from gene\nexpression profiles. Bioinformatics, 21(20):3896\u20133904, 2005.\n[191] Robert Tibshirani, Guenther Walther, and Trevor Hastie. Estimating the num-\nber of clusters in a data set via the gap statistic. Journal of the Royal Statistical\nSociety: Series B (Statistical Methodology), 63(2):411\u2013423, 2001.\n[192] Luke Tierney. Markov Chains for Exploring Posterior Distributions. Annals\nof Statistics, 22(4):1701\u20131728, December 1994. ISSN 0090-5364, 2168-8966.\nPublisher: Institute of Mathematical Statistics.\n[193] Laurens Van Der Maaten. Accelerating t-sne using tree-based algorithms. The\nJournal of Machine Learning Research, 15(1):3221\u20133245, 2014.\n[194] Aad W. Van der Vaart. Asymptotic statistics, volume 3. Cambridge university\npress, 2000.\nBIBLIOGRAPHY\n649\n[195] Aad W. Van der Vaart and John A. Wellner. Weak convergence and empirical\nprocesses with applications to statistics. Springer, 1996.\n[196] Vladimir Vapnik. Statistical learning theory. 1998. Wiley, New York, 1998.\n[197] Vladimir Vapnik. The nature of statistical learning theory. Springer science &\nbusiness media, 2013.\n[198] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\nAidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you\nneed. In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vish-\nwanathan, and R. Garnett, editors, Advances in Neural Information Processing\nSystems, volume 30. Curran Associates, Inc., 2017.\n[199] Roman Vershynin. High-dimensional probability: An introduction with applica-\ntions in data science, volume 47. Cambridge University Press, 2018.\n[200] Rene Vidal, Yi Ma, and Shankar Sastry.\nGeneralized principal component\nanalysis (gpca). IEEE transactions on pattern analysis and machine intelligence,\n27(12):1945\u20131959, 2005.\n[201] Grace Wahba. Spline Models for Observational Data. SIAM, 1990.\n[202] Geoffrey S. Watson. Smooth regression analysis. Sankhy\u00afa: The Indian Journal\nof Statistics, Series A, pages 359\u2013372, 1964.\n[203] Gerhard Winkler. Image analysis, random fields and Markov chain Monte Carlo\nmethods. Springer, 1995,2003.\n[204] Stephen J. Wright and Benjamin Recht. Optimization for data analysis. Cam-\nbridge University Press, 2022.\n[205] K\u02c6osaku Yosida. Functional Analysis. Springer, 1970.\n[206] Laurent Younes. Estimation and annealing for gibbsian fields. Ann. de l\u2019Inst.\nHenri Poincar\u00b4e, 2, 1988.\n[207] Laurent Younes. Parametric inference for imperfectly observed gibbsian fields.\nProb. Thry. Rel. Fields, 82:625\u2013645, 1989.\n[208] Laurent Younes. On the convergence of markovian stochastic algorithms with\nrapidly decreasing ergodicity rates.\nStochastics: An International Journal of\nProbability and Stochastic Processes, 65(3-4):177\u2013228, 1999.\n[209] Laurent Younes. Diffeomorphic learning. Journal of Machine Learning Research,\n21:1 \u2013 28, 2020.\n[210] Lotfi A. Zadeh. Fuzzy sets. In Fuzzy sets, fuzzy logic, and fuzzy systems: selected\npapers by Lotfi A Zadeh, pages 394\u2013432. World Scientific, 1996.\n",
        "sentence": " As described by [10] classification is an example of supervised learning, where a",
        "context": "10.5.4\nReturn to classification\nA slight modification of this algorithm may also be applied to classification, pro-\nvided that the classifier f is obtained by learning the conditional distribution, de-\nsively making them focus on harder data. We first address the issue of classification,\nand describe one of the earliest algorithms (Adaboost). We will then interpret it\nas a greedy gradient descent algorithm, as this interpretation will lead to further\n8.2.1\nGenerative model in classification and LDA\nGenerative model\nIn classification, the class variable Y generally has a causal role\nupon which the variable X is produced. Prediction can therefore be seen as an in-"
    },
    {
        "title": "Can structural MRI aid in clinical classification? A machine learning study in two independent samples of patients with schizophrenia, bipolar disorder and healthy subjects",
        "author": [
            "H.G. Schnack",
            "M. Nieuwenhuis",
            "N.E. van Haren",
            "L. Abramovic",
            "T.W. Scheewe",
            "R.M. Brouwer",
            "H.E. Hulshoff Pol",
            "R.S. Kahn"
        ],
        "venue": "NeuroImage, vol. 84, pp. 299\u2013306, jan 2014.",
        "citeRegEx": "11",
        "shortCiteRegEx": null,
        "year": 2014,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " [11] using machine learning to classify patients with schizophrenia, bipolar disorder and healthy subjects with their structural MRI scans.",
        "context": null
    },
    {
        "title": "Using supervised learning to classify clothing brand styles",
        "author": [
            "C. David Kreyenhagen",
            "T.I. Aleshin",
            "J.E. Bouchard",
            "A.M.I. Wise",
            "R.K. Zalegowski"
        ],
        "venue": "2014 Systems and Information Engineering Design Symposium (SIEDS). Charlottesville, VA, USA: IEEE, apr 2014, pp. 239\u2013243.",
        "citeRegEx": "12",
        "shortCiteRegEx": null,
        "year": 2014,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " There has also been similar research in product classification using SVM, for example [12] shows that SVM adds value to the classification of fashion brands in their research thereby making it easy for users to narrow down their searches when looking for a particular product.",
        "context": null
    },
    {
        "title": "Sentiment Analysis on Unstructured Review",
        "author": [
            "R. Nithya",
            "D. Maheswari"
        ],
        "venue": "2014 International Conference on Intelligent Computing Applications. Coimbatore, India: IEEE, mar 2014, pp. 367\u2013371.",
        "citeRegEx": "13",
        "shortCiteRegEx": null,
        "year": 2014,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " Other research such as sentiment analysis done by [13] used Na\u0131\u0308ve Bayes algorithm to classify the most identified features in an unstructured review and determine polarity distribution in terms of being positive, negative and neutral.",
        "context": null
    },
    {
        "title": "Automatic Indexing: An Experimental Inquiry",
        "author": [
            "M.E. Maron"
        ],
        "venue": "Journal of the ACM, vol. 8, no. 3, pp. 404\u2013417, jul 1961.",
        "citeRegEx": "14",
        "shortCiteRegEx": null,
        "year": 1961,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " The Naive Bayes machine learning model [14] is a popular statistical learning system that has been successful in many applications where features are independent of each other.",
        "context": null
    },
    {
        "title": "Machine learning for information extraction in informal domains",
        "author": [
            "D. Freitag"
        ],
        "venue": "Machine learning, vol. 39, no. 2-3, pp. 169\u2013202, 2000.",
        "citeRegEx": "15",
        "shortCiteRegEx": null,
        "year": 2000,
        "abstract": "",
        "full_text": "",
        "sentence": " One of the earliest application of this model to Information Extraction was done by [15].",
        "context": null
    },
    {
        "title": "Naive Bayes Modeling with Proper Smoothing for Information Extraction",
        "author": [
            "Zhenmei Gu",
            "N. Cercone"
        ],
        "venue": "2006 IEEE International Conference on Fuzzy Systems. Vancouver, BC, Canada: IEEE, 2006, pp. 393\u2013400.",
        "citeRegEx": "16",
        "shortCiteRegEx": null,
        "year": 2006,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " [16] proposed a smoothing strategy using Na\u0131\u0308ve Bayes for Information Extraction.",
        "context": null
    },
    {
        "title": "Medical Data Classification with Naive Bayes Approach",
        "author": [
            "K. Al-Aidaroo",
            "A. Bakar",
            "Z. Othman"
        ],
        "venue": "Information Technology Journal, vol. 11, no. 9, pp. 1166\u20131174, sep 2012.",
        "citeRegEx": "17",
        "shortCiteRegEx": null,
        "year": 2012,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " [17] compared Na\u0131\u0308ve Bayes with other classification algorithms for a medical dataset.",
        "context": null
    },
    {
        "title": "A lexicon pool augmented Naive Bayes Classifier for Nepali Text",
        "author": [
            "S.K. Thakur",
            "V.K. Singh"
        ],
        "venue": "2014 Seventh International Conference on Contemporary Computing (IC3). Noida, India: IEEE, aug 2014, pp. 542\u2013546.",
        "citeRegEx": "18",
        "shortCiteRegEx": null,
        "year": 2014,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " An example of this is the modified Na\u0131\u0308ve Bayes algorithm proposed by [18] to improve the classification of Nepali texts.",
        "context": null
    },
    {
        "title": "An Effective Algorithm for Improving the Performance of Naive Bayes for Text Classification",
        "author": [
            "Guo Qiang"
        ],
        "venue": "2010 Second International Conference on Computer Research and Development, no. 1. Kuala Lumpur, Malaysia: IEEE, 2010, pp. 699\u2013701.",
        "citeRegEx": "19",
        "shortCiteRegEx": null,
        "year": 2010,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " Another example of improving the Naive Bayes algorithm is the improved Naive Bayes probabilistic model-Multinomial Event Model for text classification by [19]. independently even though multiple occurrences of the same word in a document are not necessarily independent [19] and Multinomial Na\u0131\u0308ve Bayes does not account for this occurrence. Although the authors did not use Na\u0131\u0308ve Bayes for analyzing the reviews, research such as [19], [21] and [23] show that machine learning using Na\u0131\u0308ve Bayes can also be applied in classifying customer reviews on Google Play Store.",
        "context": null
    },
    {
        "title": "Low Cost Portability for Statistical Machine Translation based on N-gram Frequency and TF-IDF",
        "author": [
            "M. Eck",
            "S. Vogel",
            "A. Waibel"
        ],
        "venue": "IInternational Workshop on Spoken Language Translation, IWSLT 2005, Pittsburgh, PA, USA, 2005, pp. 61\u201367.",
        "citeRegEx": "20",
        "shortCiteRegEx": null,
        "year": 2005,
        "abstract": "",
        "full_text": "",
        "sentence": " Other weighting schemes in text classification involve the use of N-grams, which is a sequence of n-items from a given document or text and (Term Frequency-Inverse Document Frequency) TF-IDF, which shows how important a word is in a document or a given set of documents [20]. The formula used to calculate TF-IDF is given as (6) as described in [20].",
        "context": null
    },
    {
        "title": "An Approach to Spam Detection by Naive Bayes Ensemble Based on Decision Induction",
        "author": [
            "Z. Yang",
            "X. Nie",
            "W. Xu",
            "J. Guo"
        ],
        "venue": "Sixth International Conference on Intelligent Systems Design and Applications, vol. 2. Jinan, China: IEEE, oct 2006, pp. 861\u2013866.",
        "citeRegEx": "21",
        "shortCiteRegEx": null,
        "year": 2006,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " [21] proposed a Na\u0131\u0308ve Bayes spam detection method based on decision trees, they also presented an improved method based on classifier error weight. Their experimentation shows that the implementation is valid, but there are not many solutions for valid incremental decision tree induction algorithm as they described [21]. Although the authors did not use Na\u0131\u0308ve Bayes for analyzing the reviews, research such as [19], [21] and [23] show that machine learning using Na\u0131\u0308ve Bayes can also be applied in classifying customer reviews on Google Play Store.",
        "context": null
    },
    {
        "title": "Learning Weighted Naive Bayes with Accurate Ranking",
        "author": [
            "H. Zhang",
            "Shengli Sheng"
        ],
        "venue": "Fourth IEEE International Conference on Data Mining (ICDM\u201904). IEEE, 2004, pp. 567\u2013570.",
        "citeRegEx": "22",
        "shortCiteRegEx": null,
        "year": 2004,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " Another use of Na\u0131\u0308ve Bayes is that proposed by [22] for ranking. Their results show that the weighted Na\u0131\u0308ve Bayes outperforms the standard Na\u0131\u0308ve Bayes, and both the weighted Na\u0131\u0308ve Bayes and the standard Na\u0131\u0308ve Bayes are better in performance than the decision tree algorithm [22].",
        "context": null
    },
    {
        "title": "Bug report, feature request, or simply praise? On automatically classifying app reviews",
        "author": [
            "W. Maalej",
            "H. Nabil"
        ],
        "venue": "2015 IEEE 23rd International Requirements Engineering Conference (RE). Ottawa, ON: IEEE, aug 2015, pp. 116\u2013125.",
        "citeRegEx": "23",
        "shortCiteRegEx": null,
        "year": 2015,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " There has been a few research done on Google Play Store using sentiment analysis applied to customer reviews on mobile apps to determine their polarity such as [23], where app reviews were automatically classified and result compared with other classification methods. Although the authors did not use Na\u0131\u0308ve Bayes for analyzing the reviews, research such as [19], [21] and [23] show that machine learning using Na\u0131\u0308ve Bayes can also be applied in classifying customer reviews on Google Play Store.",
        "context": null
    },
    {
        "title": "Numeric rating of Apps on Google Play Store by sentiment analysis on user reviews",
        "author": [
            "M.R. Islam"
        ],
        "venue": "2014 International Conference on Electrical Engineering and Information & Communication Technology. Dhaka, Bangladesh: IEEE, apr 2014, pp. 1\u20134.",
        "citeRegEx": "24",
        "shortCiteRegEx": null,
        "year": 2014,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " Additionally, [24] used sentiment analysis on customer reviews on the store.",
        "context": null
    },
    {
        "title": "Comparing naive Bayes, decision trees, and SVM with AUC and accuracy",
        "author": [
            "J. Huang",
            "J. Lu",
            "C. Ling"
        ],
        "venue": "Third IEEE International Conference on Data Mining. IEEE Comput. Soc, 2003, pp. 553\u2013556.",
        "citeRegEx": "25",
        "shortCiteRegEx": null,
        "year": 2003,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " Since the classification of apps will be done on a regular computer, Na\u0131\u0308ve Bayes is most efficient in terms of CPU and memory consumption as described in [25].",
        "context": null
    }
]