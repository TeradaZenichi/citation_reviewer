[
    {
        "title": "Interpretability of sudden concept drift in medical informatics domain",
        "author": [
            "G. Stiglic",
            "P. Kokol"
        ],
        "venue": "2011 IEEE 11th International Conference on Data Mining Workshops, Dec 2011, pp. 609\u2013613.",
        "citeRegEx": "1",
        "shortCiteRegEx": null,
        "year": 2011,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " , medical informatics [1], financial data analysis [2], social networks [3], et al.",
        "context": null
    },
    {
        "title": "Concept drift-oriented adaptive and dynamic support vector machine ensemble with time window in corporate financial risk prediction",
        "author": [
            "J. Sun",
            "H. Li",
            "H. Adeli"
        ],
        "venue": "IEEE Trans. Syst., Man, and Cybern.: Syst.,, vol. 43, pp. 801\u2013813, July 2013.",
        "citeRegEx": "2",
        "shortCiteRegEx": null,
        "year": 2013,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " , medical informatics [1], financial data analysis [2], social networks [3], et al.",
        "context": null
    },
    {
        "title": "Online ensemble learning of data streams with gradually evolved classes",
        "author": [
            "Y. Sun",
            "K. Tang",
            "L.L. Minku",
            "S. Wang",
            "X. Yao"
        ],
        "venue": "IEEE Transactions on Knowledge and Data Engineering, vol. 28, no. 6, pp. 1532\u2013 1545, June 2016.",
        "citeRegEx": "3",
        "shortCiteRegEx": null,
        "year": 2016,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " , medical informatics [1], financial data analysis [2], social networks [3], et al.",
        "context": null
    },
    {
        "title": "Learn++: an incremental learning algorithm for supervised neural networks",
        "author": [
            "R. Polikar",
            "L. Upda",
            "S. Upda",
            "V. Honavar"
        ],
        "venue": "IEEE Trans. Syst., Man, and Cybern. C, vol. 31, no. 4, pp. 497\u2013508, Nov 2001.",
        "citeRegEx": "4",
        "shortCiteRegEx": null,
        "year": 2001,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " In particular, the learning machines should be updated without access to previous data, such that there is no need to store or re-process the previous data [4], [5]. Ensemble methods [4], [6] offer a natural approach to of Classifiers (DIC) approach [19], the Learn++ algorithm [4] in Non-Stationary Environments (Learn++.",
        "context": null
    },
    {
        "title": "Incremental learning from stream data",
        "author": [
            "H. He",
            "S. Chen",
            "K. Li",
            "X. Xu"
        ],
        "venue": "IEEE Trans. Neural Netw., vol. 22, no. 12, pp. 1901\u20131914, Dec 2011.",
        "citeRegEx": "5",
        "shortCiteRegEx": null,
        "year": 1901,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " In particular, the learning machines should be updated without access to previous data, such that there is no need to store or re-process the previous data [4], [5]. In the experiment, all of the data locate in the area of [-5, 5] for both dimensions.",
        "context": null
    },
    {
        "title": "A streaming ensemble algorithm (sea) for large-scale classification",
        "author": [
            "W.N. Street",
            "Y. Kim"
        ],
        "venue": "KDD. New York, NY, USA: ACM, 2001, pp. 377\u2013382.",
        "citeRegEx": "6",
        "shortCiteRegEx": null,
        "year": 2001,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " Ensemble methods [4], [6] offer a natural approach to For such reasons, ensemble methods, which preserve historical models, are gaining more popularity in recent years [6], [7], [17]. Typical examples of such approaches include the Streaming Ensemble Algorithm (SEA) [6], the Temporal Inductive Transfer (TIX) approach [18], the Dynamic Integration The following representative algorithms are compared in the experiments: SEA [6], Learn++. According to the suggestion in [6], the ensemble size is set to 25 for the compared algorithms, unless mentioned otherwise. Based on the previous research [6], [7], [14], [17], [27], [28], five widely used synthetic concept drifts are employed in our experiment, described as follows. \u2022 SEA moving hyperplane concepts (SEA) [6] involves 3 features with a value between 0 and 10.",
        "context": null
    },
    {
        "title": "Reacting to different types of concept drift: The accuracy updated ensemble algorithm",
        "author": [
            "D. Brzezinski",
            "J. Stefanowski"
        ],
        "venue": "IEEE Trans. Neural Netw. Learning Syst., vol. 25, no. 1, pp. 81\u201394, Jan 2014.",
        "citeRegEx": "7",
        "shortCiteRegEx": null,
        "year": 2014,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " It can be observed from the literature [7] that ensemble methods have been used frequently in many advanced incremental learning algorithms and have achieved great successes. For such reasons, ensemble methods, which preserve historical models, are gaining more popularity in recent years [6], [7], [17]. NSE [17]) and Accuracy Updated Ensemble (AUE2) [7]. Among the combination schemes described in Section II, the weighted voting scheme used by AUE2 is employed because AUE2 showed the best overall performance among ensemble methods for incremental learning [7]. NSE [17], AUE2 [7], and TIX [18]. Based on the previous research [6], [7], [14], [17], [27], [28], five widely used synthetic concept drifts are employed in our experiment, described as follows. \u2022 Rotating concepts (ROT) [7], [17] rotates the decision boundary or data points to simulate the change of data distribution. \u2022 Electricity, a widely used dataset [14], [7], is collected from the New South Wales Electricity Market in Australia, containing 45,312 instances dated from 7 May 1996 to 5 December 1998.",
        "context": null
    },
    {
        "title": "A survey on concept drift adaptation",
        "author": [
            "J. a. Gama",
            "I. \u017dliobait\u0117",
            "A. Bifet",
            "M. Pechenizkiy",
            "A. Bouchachia"
        ],
        "venue": "ACM Comput. Surv., vol. 46, no. 4, pp. 44:1\u201344:37, Mar. 2014.",
        "citeRegEx": "8",
        "shortCiteRegEx": null,
        "year": 2014,
        "abstract": "Concept drift primarily refers to an online supervised learning scenario when the relation between the input data and the target variable changes over time. Assuming a general knowledge of supervised learning in this article, we characterize adaptive learning processes; categorize existing strategies for handling concept drift; overview the most representative, distinct, and popular techniques and algorithms; discuss evaluation methodology of adaptive algorithms; and present a set of illustrative applications. The survey covers the different facets of concept drift in an integrated way to reflect on the existing scattered state of the art. Thus, it aims at providing a comprehensive introduction to the concept drift adaptation for researchers, industry analysts, and practitioners.",
        "full_text": "",
        "sentence": " This phenomenon, referred to as concept drift, is one of the key challenges that incremental learning approaches [8] [9], including those based on ensembles, need to deal with.",
        "context": null
    },
    {
        "title": "An overview of concept drift applications",
        "author": [
            "I. \u017dliobait\u0117",
            "M. Pechenizkiy",
            "J. a. Gama"
        ],
        "venue": "Big Data Analysis: New Algorithms for a New Society, ser. Studies in Big Data, N. Japkowicz and J. Stefanowski, Eds. Springer International Publishing, 2016, vol. 16, pp. 91\u2013114.",
        "citeRegEx": "9",
        "shortCiteRegEx": null,
        "year": 2016,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " This phenomenon, referred to as concept drift, is one of the key challenges that incremental learning approaches [8] [9], including those based on ensembles, need to deal with.",
        "context": null
    },
    {
        "title": "Resampling-based ensemble methods for online class imbalance learning",
        "author": [
            "S. Wang",
            "L.L. Minku",
            "X. Yao"
        ],
        "venue": "IEEE Transactions on Knowledge and Data Engineering, vol. 27, no. 5, pp. 1356\u20131368, May 2015.  11",
        "citeRegEx": "10",
        "shortCiteRegEx": null,
        "year": 2015,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " It should be noted that a special case of a data chunk is a single data example, which is more often referred to as online learning [10].",
        "context": null
    },
    {
        "title": "Learning in the presence of concept drift and hidden contexts",
        "author": [
            "G. Widmer",
            "M. Kubat"
        ],
        "venue": "Machine Learning, vol. 23, no. 1, pp. 69\u2013101, 1996.",
        "citeRegEx": "11",
        "shortCiteRegEx": null,
        "year": 1996,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " The sliding window methods [11], [12], [13], which are mainly applied in the online learning scenario, preserve part of the most recently arrived data and update the current model with both the preserved data and the newly arrived training example.",
        "context": null
    },
    {
        "title": "Mining time-changing data streams",
        "author": [
            "G. Hulten",
            "L. Spencer",
            "P. Domingos"
        ],
        "venue": "KDD. New York, NY, USA: ACM, 2001, pp. 97\u2013106.",
        "citeRegEx": "12",
        "shortCiteRegEx": null,
        "year": 2001,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " The sliding window methods [11], [12], [13], which are mainly applied in the online learning scenario, preserve part of the most recently arrived data and update the current model with both the preserved data and the newly arrived training example.",
        "context": null
    },
    {
        "title": "Learning from time-changing data with adaptive windowing",
        "author": [
            "R.G. Albert Bifet"
        ],
        "venue": "SDM, 2006.",
        "citeRegEx": "13",
        "shortCiteRegEx": null,
        "year": 2006,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " The sliding window methods [11], [12], [13], which are mainly applied in the online learning scenario, preserve part of the most recently arrived data and update the current model with both the preserved data and the newly arrived training example.",
        "context": null
    },
    {
        "title": "Learning with drift detection",
        "author": [
            "J. a. Gama",
            "P. Medas",
            "G. Castillo",
            "P. Rodrigues"
        ],
        "venue": "SBIA 2004, ser. LNCS. Springer Berlin Heidelberg, 2004, vol. 3171, pp. 286\u2013295.",
        "citeRegEx": "14",
        "shortCiteRegEx": null,
        "year": 2004,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " Some other methods [14], [15], [16] explicitly involve a concept drift detection module in the learning algorithm. Otherwise, the current model, which may be either a single learner [14] or an ensemble [16], is discarded and a new model is built from scratch. Based on the previous research [6], [7], [14], [17], [27], [28], five widely used synthetic concept drifts are employed in our experiment, described as follows. \u2022 Circle concepts (CIR) [14], [27] applies a circle as the decision boundary in a 2-dimensional feature space and simulates the concept drift by changing the radius of the circle, i. \u2022 Sine concepts (SIN) [14], [27] determines the label of data by a sine curve in a 2-dimensional feature space, which is defined as follow. \u2022 Electricity, a widely used dataset [14], [7], is collected from the New South Wales Electricity Market in Australia, containing 45,312 instances dated from 7 May 1996 to 5 December 1998.",
        "context": null
    },
    {
        "title": "Early drift detection method",
        "author": [
            "A. Bifet"
        ],
        "venue": "Fourth Int\u2019l Workshop on Knowledge Discovery from Data Streams, vol. 6, 2006, pp. 77\u201386.",
        "citeRegEx": "15",
        "shortCiteRegEx": null,
        "year": 2006,
        "abstract": "",
        "full_text": "",
        "sentence": " Some other methods [14], [15], [16] explicitly involve a concept drift detection module in the learning algorithm.",
        "context": null
    },
    {
        "title": "Ddd: A new ensemble approach for dealing with concept drift",
        "author": [
            "L. Minku",
            "X. Yao"
        ],
        "venue": "IEEE Trans. Knowl. Data Eng., vol. 24, no. 4, pp. 619\u2013633, April 2012.",
        "citeRegEx": "16",
        "shortCiteRegEx": null,
        "year": 2012,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " Some other methods [14], [15], [16] explicitly involve a concept drift detection module in the learning algorithm. Otherwise, the current model, which may be either a single learner [14] or an ensemble [16], is discarded and a new model is built from scratch. Although the idea of ensembles is also adopted in the Diversity for Dealing with Drifts (DDD) method [16], we distinguish it from the above-mentioned ensemble methods as DDD does not preserve historical models and the ensembles used in that context could be regarded as a single model for time step t.",
        "context": null
    },
    {
        "title": "Incremental learning of concept drift in nonstationary environments",
        "author": [
            "R. Elwell",
            "R. Polikar"
        ],
        "venue": "IEEE Trans. Neural Netw., vol. 22, no. 10, pp. 1517\u20131531, Oct 2011.",
        "citeRegEx": "17",
        "shortCiteRegEx": null,
        "year": 2011,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " For such reasons, ensemble methods, which preserve historical models, are gaining more popularity in recent years [6], [7], [17]. NSE [17]) and Accuracy Updated Ensemble (AUE2) [7]. NSE [17], AUE2 [7], and TIX [18]. Based on the previous research [6], [7], [14], [17], [27], [28], five widely used synthetic concept drifts are employed in our experiment, described as follows. \u2022 Rotating concepts (ROT) [7], [17] rotates the decision boundary or data points to simulate the change of data distribution.",
        "context": null
    },
    {
        "title": "Tackling concept drift by temporal inductive transfer",
        "author": [
            "G. Forman"
        ],
        "venue": "SIGIR. New York, NY, USA: ACM, 2006, pp. 252\u2013259.",
        "citeRegEx": "18",
        "shortCiteRegEx": null,
        "year": 2006,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " Typical examples of such approaches include the Streaming Ensemble Algorithm (SEA) [6], the Temporal Inductive Transfer (TIX) approach [18], the Dynamic Integration NSE [17], AUE2 [7], and TIX [18].",
        "context": null
    },
    {
        "title": "Dynamic integration of classifiers for handling concept drift",
        "author": [
            "A. Tsymbal",
            "M. Pechenizkiy",
            "P. Cunningham",
            "S. Puuronen"
        ],
        "venue": "Information Fusion, vol. 9, no. 1, pp. 56 \u2013 68, 2008.",
        "citeRegEx": "19",
        "shortCiteRegEx": null,
        "year": 2008,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " of Classifiers (DIC) approach [19], the Learn++ algorithm [4] in Non-Stationary Environments (Learn++.",
        "context": null
    },
    {
        "title": "Diversity creation methods: a survey and categorisation",
        "author": [
            "G. Brown",
            "J. Wyatt",
            "R. Harris",
            "X. Yao"
        ],
        "venue": "Information Fusion, vol. 6, no. 1, pp. 5 \u2013 20, 2005, diversity in Multiple Classifier Systems.",
        "citeRegEx": "20",
        "shortCiteRegEx": null,
        "year": 2005,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " None of the existing ensemble methods for incremental learning has considered ensemble diversity explicitly, although diversity has been shown to play a crucial role in ensembles [20], [21]. It is well acknowledged in the ensemble learning literature [20], [21] that, with an appropriate combination scheme, diversity among individual models are essential. Diversity between individual models should be encouraged, which could be implemented by diverse training data, diverse initial models, different learning algorithms [20].",
        "context": null
    },
    {
        "title": "An analysis of diversity measures",
        "author": [
            "E.K. Tang",
            "P.N. Suganthan",
            "X. Yao"
        ],
        "venue": "Machine Learning, vol. 65, no. 1, pp. 247\u2013271, 2006.",
        "citeRegEx": "21",
        "shortCiteRegEx": null,
        "year": 2006,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " None of the existing ensemble methods for incremental learning has considered ensemble diversity explicitly, although diversity has been shown to play a crucial role in ensembles [20], [21]. It is well acknowledged in the ensemble learning literature [20], [21] that, with an appropriate combination scheme, diversity among individual models are essential. In general, any diversity measure [21] proposed for ensemble learning could be used for this purpose.",
        "context": null
    },
    {
        "title": "A survey on transfer learning",
        "author": [
            "S.J. Pan",
            "Q. Yang"
        ],
        "venue": "IEEE Transactions on Knowledge and Data Engineering, vol. 22, no. 10, pp. 1345\u2013 1359, Oct 2010.",
        "citeRegEx": "22",
        "shortCiteRegEx": null,
        "year": 2010,
        "abstract": "",
        "full_text": "",
        "sentence": " However, viewing concepts C1 and C2 from the same incremental learning task as the source and target domains of transfer learning [22], it is reasonable to assume that C1 and C2 are correlated with each other.",
        "context": null
    },
    {
        "title": "On the association of attributes in statistics: With illustrations from the material of the childhood society, &c",
        "author": [
            "G.U. Yule"
        ],
        "venue": "Philosophical Transactions of the Royal Society of London A: Mathematical, Physical and Engineering Sciences, vol. 194, no. 252-261, pp. 257\u2013319, 1900.",
        "citeRegEx": "23",
        "shortCiteRegEx": null,
        "year": 1900,
        "abstract": "",
        "full_text": "",
        "sentence": " In this work, the Yules Q-statistic [23] is employed since it is one of the most popular diversity measures in the literature.",
        "context": null
    },
    {
        "title": "Classification and regression trees",
        "author": [
            "L. Breiman",
            "J. Friedman",
            "C.J. Stone",
            "R.A. Olshen"
        ],
        "venue": "Monterey, CA: Wadsworth & Brooks/Cole Advanced Books & Software,",
        "citeRegEx": "25",
        "shortCiteRegEx": "25",
        "year": 1984,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " The classification and regression tree (CART) [25], is employed as the base learner in DTEL. Specifically, the traditional decision tree method CART [25] is applied in SEA, Learn++.",
        "context": null
    },
    {
        "title": "Mining high-speed data streams",
        "author": [
            "P. Domingos",
            "G. Hulten"
        ],
        "venue": "KDD. New York, NY, USA: ACM, 2000, pp. 71\u201380.",
        "citeRegEx": "26",
        "shortCiteRegEx": null,
        "year": 2000,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " Since AUE2 needs to use an on-line model as the base learner, Hoeffding tree [26], an online decision tree method, was applied.",
        "context": null
    },
    {
        "title": "The impact of diversity on online ensemble learning in the presence of concept drift",
        "author": [
            "L. Minku",
            "A. White",
            "X. Yao"
        ],
        "venue": "IEEE Trans. Knowl. Data Eng., vol. 22, no. 5, pp. 730\u2013742, May 2010.",
        "citeRegEx": "27",
        "shortCiteRegEx": null,
        "year": 2010,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " Based on the previous research [6], [7], [14], [17], [27], [28], five widely used synthetic concept drifts are employed in our experiment, described as follows. \u2022 Circle concepts (CIR) [14], [27] applies a circle as the decision boundary in a 2-dimensional feature space and simulates the concept drift by changing the radius of the circle, i. \u2022 Sine concepts (SIN) [14], [27] determines the label of data by a sine curve in a 2-dimensional feature space, which is defined as follow. \u2022 STAGGER Boolean concepts (STA) [27], [28] generates the data with categorical features using a set of rules to determine the class label. According to [27] and [28], the features and values are color \u2208 {red(R), blue(B), green(G)}, shape \u2208 {circle(C), square(S), triangle(T)}, and size \u2208 {small(S), medium(M), large(L)}.",
        "context": null
    },
    {
        "title": "Incremental learning from noisy data",
        "author": [
            "J.C. Schlimmer",
            "R.H. Granger",
            "Jr."
        ],
        "venue": "Mach. Learn., vol. 1, no. 3, pp. 317\u2013354, Mar. 1986. [Online]. Available: http://dx.doi.org/10.1023/A:1022810614389",
        "citeRegEx": "28",
        "shortCiteRegEx": null,
        "year": 1986,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " Based on the previous research [6], [7], [14], [17], [27], [28], five widely used synthetic concept drifts are employed in our experiment, described as follows. \u2022 STAGGER Boolean concepts (STA) [27], [28] generates the data with categorical features using a set of rules to determine the class label. According to [27] and [28], the features and values are color \u2208 {red(R), blue(B), green(G)}, shape \u2208 {circle(C), square(S), triangle(T)}, and size \u2208 {small(S), medium(M), large(L)}.",
        "context": null
    },
    {
        "title": "UCI machine learning repository",
        "author": [
            "K. Bache",
            "M. Lichman"
        ],
        "venue": "2013. [Online]. Available: http://archive.ics.uci.edu/ml",
        "citeRegEx": "29",
        "shortCiteRegEx": null,
        "year": 2013,
        "abstract": "",
        "full_text": "",
        "sentence": " \u2022 Covertype [29] is a real-world dataset for describing the observation of a forest area with 51 cartographic variables. \u2022 PokerHand [29] describes the suits and ranks of a hand of five playing cards.",
        "context": null
    },
    {
        "title": "Statistical comparisons of classifiers over multiple data sets",
        "author": [
            "J. Dem\u0161ar"
        ],
        "venue": "J. Mach. Learn. Res., vol. 7, pp. 1\u201330, Dec. 2006.",
        "citeRegEx": "30",
        "shortCiteRegEx": null,
        "year": 2006,
        "abstract": "",
        "full_text": "",
        "sentence": " To make a comprehensive comparison, a Friedman test [30] is conducted based on the average accuracy results on both synthetic data streams (Table III) and real-world data streams (Table V), as shown in Table VI.",
        "context": null
    }
]