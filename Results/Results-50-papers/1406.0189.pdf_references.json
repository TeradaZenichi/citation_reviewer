[
    {
        "title": "Solution of the matrix equation AX+",
        "author": [
            "R.H. Bartels",
            "G.W. Stewart"
        ],
        "venue": "XB = C. Communications of the ACM,",
        "citeRegEx": "Bartels and Stewart,? \\Q1972\\E",
        "shortCiteRegEx": "Bartels and Stewart",
        "year": 1972,
        "abstract": "",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "A global solution for the structured total least squares problem with block circulant matrices",
        "author": [
            "A. Beck",
            "A. Ben-Tal"
        ],
        "venue": "SIAM Journal on Matrix Analysis and Applic.,",
        "citeRegEx": "Beck and Ben.Tal,? \\Q2005\\E",
        "shortCiteRegEx": "Beck and Ben.Tal",
        "year": 2005,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "Constrained Optim. and Lagrange Multiplier Methods",
        "author": [
            "D.P. Bertsekas"
        ],
        "venue": null,
        "citeRegEx": "Bertsekas,? \\Q1982\\E",
        "shortCiteRegEx": "Bertsekas",
        "year": 1982,
        "abstract": "",
        "full_text": "",
        "sentence": " We suggest fast first-order methods based on Augmented Lagrangian multipliers (Bertsekas, 1982) to compute the STLS solution. In the next section we focus on augmented Lagrangian methods (ALM) (Bertsekas, 1982) which allow fast convergence without using computationally expensive second-order information. We develop an effective first-order approach for STLS based on the augmented Lagrangian multiplier (ALM) method (Bertsekas, 1982; Lin et al., 2010). When f and h are both continuously differentiable, if \u03bck is an increasing sequence, the solution converges Q-linearly to the optimal one (Bertsekas, 1982). Finally, instead of relaxing the constraint L(E) = b, we keep the constrained form, and follow each step by a projection (Bertsekas, 1982).",
        "context": null
    },
    {
        "title": "Distributed optimization and statistical learning via the alternating direction method of multipliers",
        "author": [
            "S. Boyd",
            "N. Parikh",
            "E. Chu",
            "B. Peleato",
            "J. Eckstein"
        ],
        "venue": "Foundations and Trends in Machine Learning,",
        "citeRegEx": "Boyd et al\\.,? \\Q2011\\E",
        "shortCiteRegEx": "Boyd et al\\.",
        "year": 2011,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " 3This is closely related to the popular alternating direction of multipliers methods (Boyd et al., 2011).",
        "context": null
    },
    {
        "title": "A singular value thresholding algorithm for matrix completion",
        "author": [
            "J. Cai",
            "E.J. Candes",
            "Z. Shen"
        ],
        "venue": "SIAM Journal on Optim.,",
        "citeRegEx": "Cai et al\\.,? \\Q1956\\E",
        "shortCiteRegEx": "Cai et al\\.",
        "year": 1956,
        "abstract": "This paper introduces a novel algorithm to approximate the matrix with\nminimum nuclear norm among all matrices obeying a set of convex constraints.\nThis problem may be understood as the convex relaxation of a rank minimization\nproblem, and arises in many important applications as in the task of recovering\na large matrix from a small subset of its entries (the famous Netflix problem).\nOff-the-shelf algorithms such as interior point methods are not directly\namenable to large problems of this kind with over a million unknown entries.\n  This paper develops a simple first-order and easy-to-implement algorithm that\nis extremely efficient at addressing problems in which the optimal solution has\nlow rank. The algorithm is iterative and produces a sequence of matrices (X^k,\nY^k) and at each step, mainly performs a soft-thresholding operation on the\nsingular values of the matrix Y^k. There are two remarkable features making\nthis attractive for low-rank matrix completion problems. The first is that the\nsoft-thresholding operation is applied to a sparse matrix; the second is that\nthe rank of the iterates X^k is empirically nondecreasing. Both these facts\nallow the algorithm to make use of very minimal storage space and keep the\ncomputational cost of each iteration low. We provide numerical examples in\nwhich 1,000 by 1,000 matrices are recovered in less than a minute on a modest\ndesktop computer. We also demonstrate that our approach is amenable to very\nlarge scale problems by recovering matrices of rank about 10 with nearly a\nbillion unknowns from just about 0.4% of their sampled entries. Our methods are\nconnected with linearized Bregman iterations for l1 minimization, and we\ndevelop a framework in which one can understand these algorithms in terms of\nwell-known Lagrange multiplier algorithms.",
        "full_text": "arXiv:0810.3286v1  [math.OC]  18 Oct 2008\nA Singular Value Thresholding Algorithm for Matrix Completion\nJian-Feng Cai\u2020\nEmmanuel J. Cand`es\u266f\nZuowei Shen\u00a7\n\u2020 Temasek Laboratories, National University of Singapore, Singapore 117543\n\u266fApplied and Computational Mathematics, Caltech, Pasadena, CA 91125\n\u00a7 Department of Mathematics, National University of Singapore, Singapore 117543\nSeptember 2008\nAbstract\nThis paper introduces a novel algorithm to approximate the matrix with minimum nuclear\nnorm among all matrices obeying a set of convex constraints. This problem may be understood as\nthe convex relaxation of a rank minimization problem, and arises in many important applications\nas in the task of recovering a large matrix from a small subset of its entries (the famous Net\ufb02ix\nproblem). O\ufb00-the-shelf algorithms such as interior point methods are not directly amenable to\nlarge problems of this kind with over a million unknown entries.\nThis paper develops a simple \ufb01rst-order and easy-to-implement algorithm that is extremely\ne\ufb03cient at addressing problems in which the optimal solution has low rank. The algorithm is\niterative and produces a sequence of matrices {Xk, Y k} and at each step, mainly performs a\nsoft-thresholding operation on the singular values of the matrix Y k. There are two remarkable\nfeatures making this attractive for low-rank matrix completion problems.\nThe \ufb01rst is that\nthe soft-thresholding operation is applied to a sparse matrix; the second is that the rank of\nthe iterates {Xk} is empirically nondecreasing. Both these facts allow the algorithm to make\nuse of very minimal storage space and keep the computational cost of each iteration low. On\nthe theoretical side, we provide a convergence analysis showing that the sequence of iterates\nconverges. On the practical side, we provide numerical examples in which 1, 000\u00d71, 000 matrices\nare recovered in less than a minute on a modest desktop computer. We also demonstrate that\nour approach is amenable to very large scale problems by recovering matrices of rank about 10\nwith nearly a billion unknowns from just about 0.4% of their sampled entries. Our methods are\nconnected with the recent literature on linearized Bregman iterations for \u21131 minimization, and\nwe develop a framework in which one can understand these algorithms in terms of well-known\nLagrange multiplier algorithms.\nKeywords. Nuclear norm minimization, matrix completion, singular value thresholding, La-\ngrange dual function, Uzawa\u2019s algorithm.\n1\nIntroduction\n1.1\nMotivation\nThere is a rapidly growing interest in the recovery of an unknown low-rank or approximately low-\nrank matrix from very limited information. This problem occurs in many areas of engineering and\n1\napplied science such as machine learning [1, 3, 4], control [42] and computer vision, see [48]. As\na motivating example, consider the problem of recovering a data matrix from a sampling of its\nentries. This routinely comes up whenever one collects partially \ufb01lled out surveys, and one would\nlike to infer the many missing entries. In the area of recommender systems, users submit ratings\non a subset of entries in a database, and the vendor provides recommendations based on the user\u2019s\npreferences. Because users only rate a few items, one would like to infer their preference for unrated\nitems; this is the famous Net\ufb02ix problem [2]. Recovering a rectangular matrix from a sampling of\nits entries is known as the matrix completion problem. The issue is of course that this problem is\nextraordinarily ill posed since with fewer samples than entries, we have in\ufb01nitely many completions.\nTherefore, it is apparently impossible to identify which of these candidate solutions is indeed the\n\u201ccorrect\u201d one without some additional information.\nIn many instances, however, the matrix we wish to recover has low rank or approximately low\nrank.\nFor instance, the Net\ufb02ix data matrix of all user-ratings may be approximately low-rank\nbecause it is commonly believed that only a few factors contribute to anyone\u2019s taste or preference.\nIn computer vision, inferring scene geometry and camera motion from a sequence of images is a well-\nstudied problem known as the structure-from-motion problem. This is an ill-conditioned problem\nfor objects may be distant with respect to their size, or especially for \u201cmissing data\u201d which occur\nbecause of occlusion or tracking failures. However, when properly stacked and indexed, these images\nform a matrix which has very low rank (e.g. rank 3 under orthography) [21, 48]. Other examples\nof low-rank matrix \ufb01tting abound; e.g. in control (system identi\ufb01cation), machine learning (multi-\nclass learning) and so on. Having said this, the premise that the unknown has (approximately) low\nrank radically changes the problem, making the search for solutions feasible since the lowest-rank\nsolution now tends to be the right one.\nIn a recent paper [13], Cand`es and Recht showed that matrix completion is not as ill-posed as\npeople thought. Indeed, they proved that most low-rank matrices can be recovered exactly from\nmost sets of sampled entries even though these sets have surprisingly small cardinality, and more\nimportantly, they proved that this can be done by solving a simple convex optimization problem.\nTo state their results, suppose to simplify that the unknown matrix M \u2208Rn\u00d7n is square, and that\none has available m sampled entries {Mij : (i, j) \u2208\u2126} where \u2126is a random subset of cardinality\nm. Then [13] proves that most matrices M of rank r can be perfectly recovered by solving the\noptimization problem\nminimize\n\u2225X\u2225\u2217\nsubject to\nXij = Mij,\n(i, j) \u2208\u2126,\n(1.1)\nprovided that the number of samples obeys\nm \u2265Cn6/5r log n\n(1.2)\nfor some positive numerical constant C.1 In (1.1), the functional \u2225X\u2225\u2217is the nuclear norm of the\nmatrix M, which is the sum of its singular values. The optimization problem (1.1) is convex and\ncan be recast as a semide\ufb01nite program [29,30]. In some sense, this is the tightest convex relaxation\nof the NP-hard rank minimization problem\nminimize\nrank(X)\nsubject to\nXij = Mij,\n(i, j) \u2208\u2126,\n(1.3)\n1Note that an n \u00d7 n matrix of rank r depends upon r(2n \u2212r) degrees of freedom.\n2\nsince the nuclear ball {X : \u2225X\u2225\u2217\u22641} is the convex hull of the set of rank-one matrices with\nspectral norm bounded by one. Another interpretation of Cand`es and Recht\u2019s result is that under\nsuitable conditions, the rank minimization program (1.3) and the convex program (1.1) are formally\nequivalent in the sense that they have exactly the same unique solution.\n1.2\nAlgorithm outline\nBecause minimizing the nuclear norm both provably recovers the lowest-rank matrix subject to\nconstraints (see [45] for related results) and gives generally good empirical results in a variety of\nsituations, it is understandably of great interest to develop numerical methods for solving (1.1). In\n[13], this optimization problem was solved using one of the most advanced semide\ufb01nite programming\nsolvers, namely, SDPT3 [47].\nThis solver and others like SeDuMi are based on interior-point\nmethods, and are problematic when the size of the matrix is large because they need to solve huge\nsystems of linear equations to compute the Newton direction. In fact, SDPT3 can only handle\nn \u00d7 n matrices with n \u2264100. Presumably, one could resort to iterative solvers such as the method\nof conjugate gradients to solve for the Newton step but this is problematic as well since it is well\nknown that the condition number of the Newton system increases rapidly as one gets closer to the\nsolution. In addition, none of these general purpose solvers use the fact that the solution may have\nlow rank. We refer the reader to [40] for some recent progress on interior-point methods concerning\nsome special nuclear norm-minimization problems.\nThis paper develops the singular value thresholding algorithm for approximately solving the\nnuclear norm minimization problem (1.1) and by extension, problems of the form\nminimize\n\u2225X\u2225\u2217\nsubject to\nA(X) = b,\n(1.4)\nwhere A is a linear operator acting on the space of n1 \u00d7 n2 matrices and b \u2208Rm. This algorithm is\na simple \ufb01rst-order method, and is especially well suited for problems of very large sizes in which\nthe solution has low rank. We sketch this algorithm in the special matrix completion setting and\nlet P\u2126be the orthogonal projector onto the span of matrices vanishing outside of \u2126so that the\n(i, j)th component of P\u2126(X) is equal to Xij if (i, j) \u2208\u2126and zero otherwise. Our problem may be\nexpressed as\nminimize\n\u2225X\u2225\u2217\nsubject to\nP\u2126(X) = P\u2126(M),\n(1.5)\nwith optimization variable X \u2208Rn1\u00d7n2. Fix \u03c4 > 0 and a sequence {\u03b4k}k\u22651 of scalar step sizes.\nThen starting with Y 0 = 0 \u2208Rn1\u00d7n2, the algorithm inductively de\ufb01nes\n(\nXk = shrink(Y k\u22121, \u03c4),\nY k = Y k\u22121 + \u03b4kP\u2126(M \u2212Xk)\n(1.6)\nuntil a stopping criterion is reached. In (1.6), shrink(Y , \u03c4) is a nonlinear function which applies a\nsoft-thresholding rule at level \u03c4 to the singular values of the input matrix, see Section 2 for details.\nThe key property here is that for large values of \u03c4, the sequence {Xk} converges to a solution which\nvery nearly minimizes (1.5). Hence, at each step, one only needs to compute at most one singular\nvalue decomposition and perform a few elementary matrix additions. Two important remarks are\nin order:\n3\n1. Sparsity. For each k \u22650, Y k vanishes outside of \u2126and is, therefore, sparse, a fact which can\nbe used to evaluate the shrink function rapidly.\n2. Low-rank property. The matrices Xk turn out to have low rank, and hence the algorithm has\nminimum storage requirement since we only need to keep principal factors in memory.\nOur numerical experiments demonstrate that the proposed algorithm can solve problems, in\nMatlab, involving matrices of size 30, 000\u00d730, 000 having close to a billion unknowns in 17 minutes\non a standard desktop computer with a 1.86 GHz CPU (dual core with Matlab\u2019s multithreading\noption enabled) and 3 GB of memory. As a consequence, the singular value thresholding algorithm\nmay become a rather powerful computational tool for large scale matrix completion.\n1.3\nGeneral formulation\nThe singular value thresholding algorithm can be adapted to deal with other types of convex\nconstraints. For instance, it may address problems of the form\nminimize\n\u2225X\u2225\u2217\nsubject to\nfi(X) \u22640,\ni = 1, . . . , m,\n(1.7)\nwhere each fi is a Lipschitz convex function (note that one can handle linear equality constraints by\nconsidering pairs of a\ufb03ne functionals). In the simpler case where the fi\u2019s are a\ufb03ne functionals, the\ngeneral algorithm goes through a sequence of iterations which greatly resemble (1.6). This is useful\nbecause this enables the development of numerical algorithms which are e\ufb00ective for recovering\nmatrices from a small subset of sampled entries possibly contaminated with noise.\n1.4\nContents and notations\nThe rest of the paper is organized as follows. In Section 2, we derive the singular value threshold-\ning (SVT) algorithm for the matrix completion problem, and recasts it in terms of a well-known\nLagrange multiplier algorithm. In Section 3, we extend the SVT algorithm and formulate a gen-\neral iteration which is applicable to general convex constraints.\nIn Section 4, we establish the\nconvergence results for the iterations given in Sections 2 and 3. We demonstrate the performance\nand e\ufb00ectiveness of the algorithm through numerical examples in Section 5, and review additional\nimplementation details. Finally, we conclude the paper with a short discussion in Section 6.\nBefore continuing, we provide here a brief summary of the notations used throughout the\npaper. Matrices are bold capital, vectors are bold lowercase and scalars or entries are not bold.\nFor instance, X is a matrix and Xij its (i, j)th entry.\nLikewise, x is a vector and xi its ith\ncomponent. The nuclear norm of a matrix is denoted by \u2225X\u2225\u2217, the Frobenius norm by \u2225X\u2225F\nand the spectral norm by \u2225X\u22252; note that these are respectively the 1-norm, the 2-norm and the\nsup-norm of the vector of singular values.\nThe adjoint of a matrix X is X\u2217and similarly for\nvectors. The notation diag(x), where x is a vector, stands for the diagonal matrix with {xi} as\ndiagonal elements. We denote by \u27e8X, Y \u27e9= trace(X\u2217Y ) the standard inner product between two\nmatrices (\u2225X\u22252\nF = \u27e8X, X\u27e9). The Cauchy-Schwarz inequality gives \u27e8X, Y \u27e9\u2264\u2225X\u2225F \u2225Y \u2225F and it is\nwell known that we also have \u27e8X, Y \u27e9\u2264\u2225X\u2225\u2217\u2225Y \u22252 (the spectral and nuclear norms are dual from\none another), see e.g. [13,45].\n4\n2\nThe Singular Value Thresholding Algorithm\nThis section introduces the singular value thresholding algorithm and discusses some of its ba-\nsic properties. We begin with the de\ufb01nition of a key building block, namely, the singular value\nthresholding operator.\n2.1\nThe singular value shrinkage operator\nConsider the singular value decomposition (SVD) of a matrix X \u2208Rn1\u00d7n2 of rank r\nX = U\u03a3V \u2217,\n\u03a3 = diag({\u03c3i}1\u2264i\u2264r),\n(2.1)\nwhere U and V are respectively n1 \u00d7 r and n2 \u00d7 r matrices with orthonormal columns, and the\nsingular values \u03c3i are positive (unless speci\ufb01ed otherwise, we will always assume that the SVD of\na matrix is given in the reduced form above). For each \u03c4 \u22650, we introduce the soft-thresholding\noperator D\u03c4 de\ufb01ned as follows:\nD\u03c4(X) := UD\u03c4(\u03a3)V \u2217,\nD\u03c4(\u03a3) = diag({\u03c3i \u2212\u03c4)+}),\n(2.2)\nwhere t+ is the positive part of t, namely, t+ = max(0, t). In words, this operator simply applies a\nsoft-thresholding rule to the singular values of X, e\ufb00ectively shrinking these towards zero. This is\nthe reason why we will also refer to this transformation as the singular value shrinkage operator.\nEven though the SVD may not be unique, it is easy to see that the singular value shrinkage operator\nis well de\ufb01ned and we do not elaborate further on this issue. In some sense, this shrinkage operator\nis a straightforward extension of the soft-thresholding rule for scalars and vectors. In particular,\nnote that if many of the singular values of X are below the threshold \u03c4, the rank of D\u03c4(X) may\nbe considerably lower than that of X, just like the soft-thresholding rule applied to vectors leads\nto sparser outputs whenever some entries of the input are below threshold.\nThe singular value thresholding operator is the proximity operator associated with the nuclear\nnorm. Details about the proximity operator can be found in e.g. [35].\nTheorem 2.1 For each \u03c4 \u22650 and Y \u2208Rn1\u00d7n2, the singular value shrinkage operator (2.2) obeys\nD\u03c4(Y ) = arg min\nX\n\u001a1\n2\u2225X \u2212Y \u22252\nF + \u03c4\u2225X\u2225\u2217\n\u001b\n.\n(2.3)\nProof. Since the function h0(X) := \u03c4\u2225X\u2225\u2217+ 1\n2\u2225X \u2212Y \u22252\nF is strictly convex, it is easy to see that\nthere exists a unique minimizer, and we thus need to prove that it is equal to D\u03c4(Y ). To do this,\nrecall the de\ufb01nition of a subgradient of a convex function f : Rn1\u00d7n2 \u2192R. We say that Z is a\nsubgradient of f at X0, denoted Z \u2208\u2202f(X0), if\nf(X) \u2265f(X0) + \u27e8Z, X \u2212X0\u27e9\n(2.4)\nfor all X. Now \u02c6\nX minimizes h0 if and only if 0 is a subgradient of the functional h0 at the point\n\u02c6\nX, i.e.\n0 \u2208\u02c6\nX \u2212Y + \u03c4\u2202\u2225\u02c6\nX\u2225\u2217,\n(2.5)\nwhere \u2202\u2225\u02c6\nX\u2225\u2217is the set of subgradients of the nuclear norm. Let X \u2208Rn1\u00d7n2 be an arbitrary\nmatrix and U\u03a3V \u2217be its SVD. It is known [13,37,49] that\n\u2202\u2225X\u2225\u2217=\n\b\nUV \u2217+ W : W \u2208Rn1\u00d7n2,\nU \u2217W = 0,\nW V = 0,\n\u2225W \u22252 \u22641\n\t\n.\n(2.6)\n5\nSet \u02c6\nX := D\u03c4(Y ) for short. In order to show that \u02c6\nX obeys (2.5), decompose the SVD of Y as\nY = U0\u03a30V \u2217\n0 + U1\u03a31V \u2217\n1 ,\nwhere U0, V0 (resp. U1, V1) are the singular vectors associated with singular values greater than \u03c4\n(resp. smaller than or equal to \u03c4). With these notations, we have\n\u02c6\nX = U0(\u03a30 \u2212\u03c4I)V \u2217\n0\nand, therefore,\nY \u2212\u02c6\nX = \u03c4(U0V \u2217\n0 + W ),\nW = \u03c4 \u22121U1\u03a31V \u2217\n1 .\nBy de\ufb01nition, U \u2217\n0 W = 0, W V0 = 0 and since the diagonal elements of \u03a31 have magnitudes\nbounded by \u03c4, we also have \u2225W \u22252 \u22641. Hence Y \u2212\u02c6\nX \u2208\u03c4\u2202\u2225\u02c6\nX\u2225\u2217, which concludes the proof.\n2.2\nShrinkage iterations\nWe are now in the position to introduce the singular value thresholding algorithm. Fix \u03c4 > 0 and\na sequence {\u03b4k} of positive step sizes. Starting with Y0, inductively de\ufb01ne for k = 1, 2, . . .,\n(\nXk = D\u03c4(Y k\u22121),\nY k = Y k\u22121 + \u03b4kP\u2126(M \u2212Xk)\n(2.7)\nuntil a stopping criterion is reached (we postpone the discussion this stopping criterion and of the\nchoice of step sizes). This shrinkage iteration is very simple to implement. At each step, we only\nneed to compute an SVD and perform elementary matrix operations. With the help of a standard\nnumerical linear algebra package, the whole algorithm can be coded in just a few lines.\nBefore addressing further computational issues, we would like to make explicit the relationship\nbetween this iteration and the original problem (1.1). In Section 4, we will show that the sequence\n{Xk} converges to the unique solution of an optimization problem closely related to (1.1), namely,\nminimize\n\u03c4\u2225X\u2225\u2217+ 1\n2\u2225X\u22252\nF\nsubject to\nP\u2126(X) = P\u2126(M).\n(2.8)\nFurthermore, it is intuitive that the solution to this modi\ufb01ed problem converges to that of (1.5) as\n\u03c4 \u2192\u221eas shown in Section 3. Thus by selecting a large value of the parameter \u03c4, the sequence of\niterates converges to a matrix which nearly minimizes (1.1).\nAs mentioned earlier, there are two crucial properties which make this algorithm ideally suited\nfor matrix completion.\n\u2022 Low-rank property. A remarkable empirical fact is that the matrices in the sequence {Xk}\nhave low rank (provided, of course, that the solution to (2.8) has low rank). We use the word\n\u201cempirical\u201d because all of our numerical experiments have produced low-rank sequences but\nwe cannot rigorously prove that this is true in general. The reason for this phenomenon is,\nhowever, simple: because we are interested in large values of \u03c4 (as to better approximate the\nsolution to (1.1)), the thresholding step happens to \u2018kill\u2019 most of the small singular values\nand produces a low-rank output. In fact, our numerical results show that the rank of Xk is\nnondecreasing with k, and the maximum rank is reached in the last steps of the algorithm,\nsee Section 5.\n6\nThus, when the rank of the solution is substantially smaller than either dimension of the\nmatrix, the storage requirement is low since we could store each Xk in its SVD form (note\nthat we only need to keep the current iterate and may discard earlier values).\n\u2022 Sparsity. Another important property of the SVT algorithm is that the iteration matrix Y k\nis sparse. Since Y 0 = 0, we have by induction that Y k vanishes outside of \u2126. The fewer\nentries available, the sparser Y k. Because the sparsity pattern \u2126is \ufb01xed throughout, one can\nthen apply sparse matrix techniques to save storage. Also, if |\u2126| = m, the computational cost\nof updating Y k is of order m. Moreover, we can call subroutines supporting sparse matrix\ncomputations, which can further reduce computational costs.\nOne such subroutine is the SVD. However, note that we do not need to compute the entire\nSVD of Y k to apply the singular value thresholding operator. Only the part corresponding\nto singular values greater than \u03c4 is needed. Hence, a good strategy is to apply the iterative\nLanczos algorithm to compute the \ufb01rst few singular values and singular vectors. Because\nY k is sparse, Y k can be applied to arbitrary vectors rapidly, and this procedure o\ufb00ers a\nconsiderable speedup over naive methods.\n2.3\nRelation with other works\nOur algorithm is inspired by recent work in the area of \u21131 minimization, and especially by the work\non linearized Bregman iterations for compressed sensing, see [9\u201311,23,44,51] for linearized Bregman\niterations and [14\u201317,26] for some information about the \ufb01eld of compressed sensing. In this line\nof work, linearized Bregman iterations are used to \ufb01nd the solution to an underdetermined system\nof linear equations with minimum \u21131 norm. In fact, Theorem 2.1 asserts that the singular value\nthresholding algorithm can be formulated as a linearized Bregman iteration. Bregman iterations\nwere \ufb01rst introduced in [43] as a convenient tool for solving computational problems in the imaging\nsciences, and a later paper [51] showed that they were useful for solving \u21131-norm minimization\nproblems in the area of compressed sensing. Linearized Bregman iterations were proposed in [23]\nto improve performance of plain Bregman iterations, see also [51].\nAdditional details together\nwith a technique for improving the speed of convergence called kicking are described in [44]. On\nthe practical side, the paper [11] applied Bregman iterations to solve a deblurring problem while\non the theoretical side, the references [9, 10] gave a rigorous analysis of the convergence of such\niterations. New developments keep on coming out at a rapid pace and recently, [32] introduced a\nnew iteration, the split Bregman iteration, to extend Bregman-type iterations (such as linearized\nBregman iterations) to problems involving the minimization of \u21131-like functionals such as total-\nvariation norms, Besov norms, and so forth.\nWhen applied to \u21131-minimization problems, linearized Bregman iterations are sequences of\nsoft-thresholding rules operating on vectors. Iterative soft-thresholding algorithms in connection\nwith \u21131 or total-variation minimization have quite a bit of history in signal and image processing\nand we would like to mention the works [12, 39] for total-variation minimization, [24, 25, 31] for\n\u21131 minimization, and [5, 7, 8, 19, 20, 27, 28, 46] for some recent applications in the area of image\ninpainting and image restoration. Just as iterative soft-thresholding methods are designed to \ufb01nd\nsparse solutions, our iterative singular value thresholding scheme is designed to \ufb01nd a sparse vector\nof singular values. In classical problems arising in the areas of compressed sensing, and signal or\nimage processing, the sparsity is expressed in a known transformed domain and soft-thresholding is\napplied to transformed coe\ufb03cients. In contrast, the shrinkage operator D\u03c4 is adaptive. The SVT not\n7\nonly discovers a sparse singular vector but also the bases in which we have a sparse representation.\nIn this sense, the SVT algorithm is an extension of earlier iterative soft-thresholding schemes.\nFinally, we would like to contrast the SVT iteration (2.7) with the popular iterative soft-\nthresholding algorithm used in many papers in imaging processing and perhaps best known under\nthe name of Proximal Forward-Backward Splitting method (PFBS), see [8,22,24,31,33] for example.\nThe constrained minimization problem (1.5) may be relaxed into\nminimize\n\u03bb\u2225X\u2225\u2217+ 1\n2\u2225P\u2126(X) \u2212P\u2126(M)\u22252\nF\n(2.9)\nfor some \u03bb > 0. Theorem 2.1 asserts that D\u03bb is the proximity operator of \u03bb\u2225X\u2225\u2217and Proposition\n3.1(iii) in [22] gives that the solution to this unconstrained problem is characterized by the \ufb01xed\npoint equation X = D\u03bb\u03b4(X + \u03b4P\u2126(M \u2212X)) for each \u03b4 > 0. One can then apply a simpli\ufb01ed\nversion of the PFBS method (see (3.6) in [22]) to obtain iterations of the form\nXk = D\u03bb\u03b4k\u22121(Xk\u22121 + \u03b4k\u22121P\u2126(M \u2212Xk\u22121)).\nIntroducing an intermediate matrix Y k, this algorithm may be expressed as\n(\nXk = D\u03bb\u03b4k\u22121(Y k\u22121),\nY k = Xk + \u03b4kP\u2126(M \u2212Xk).\n(2.10)\nThe di\ufb00erence with (2.7) may seem subtle at \ufb01rst\u2014replacing Xk in (2.10) with Y k\u22121 and setting\n\u03b4k = \u03b4 gives (2.7) with \u03c4 = \u03bb\u03b4\u2014but has enormous consequences as this gives entirely di\ufb00erent\nalgorithms. First, they have di\ufb00erent limits: while (2.7) converges to the solution of the constrained\nminimization (2.8), (2.10) converges to the solution of (2.9) provided that the sequence of step sizes\nis appropriately selected. Second, selecting a large \u03bb (or a large value of \u03c4 = \u03bb\u03b4) in (2.10) gives\na low-rank sequence of iterates and a limit with small nuclear norm. The limit, however, does\nnot \ufb01t the data and this is why one has to choose a small or moderate value of \u03bb (or of \u03c4 = \u03bb\u03b4).\nHowever, when \u03bb is not su\ufb03ciently large, the Xk may not have low rank even though the solution\nhas low rank (and one may need to compute many singular vectors), and Y k is not su\ufb03ciently\nsparse to make the algorithm computationally attractive. Moreover, the limit does not necessary\nhave a small nuclear norm. These are reasons why (2.10) is not suitable for matrix completion.\n2.4\nInterpretation as a Lagrange multiplier method\nIn this section, we recast the SVT algorithm as a type of Lagrange multiplier algorithm known as\nUzawa\u2019s algorithm. An important consequence is that this will allow us to extend the SVT algorithm\nto other problems involving the minimization of the nuclear norm under convex constraints, see\nSection 3. Further, another contribution of this paper is that this framework actually recasts linear\nBregman iterations as a very special form of Uzawa\u2019s algorithm, hence providing fresh and clear\ninsights about these iterations.\nIn what follows, we set f\u03c4(X) = \u03c4\u2225X\u2225\u2217+ 1\n2\u2225X\u22252\nF for some \ufb01xed \u03c4 > 0 and recall that we wish\nto solve (2.8)\nminimize\nf\u03c4(X)\nsubject to\nP\u2126(X) = P\u2126(M).\nThe Lagrangian for this problem is given by\nL(X, Y ) = f\u03c4(X) + \u27e8Y , P\u2126(M \u2212X)\u27e9,\n8\nwhere Y \u2208Rn1\u00d7n2. Strong duality holds and X\u22c6and Y \u22c6are primal-dual optimal if (X\u22c6, Y \u22c6) is a\nsaddlepoint of the Lagrangian L(X, Y ), i.e. a pair obeying\nsup\nY\ninf\nX L(X, Y ) = L(X\u22c6, Y \u22c6) = inf\nX sup\nY\nL(X, Y ).\n(2.11)\n(The function g0(Y ) = infX L(X, Y ) is called the dual function.) Uzawa\u2019s algorithm approaches\nthe problem of \ufb01nding a saddlepoint with an iterative procedure. From Y0 = 0, say, inductively\nde\ufb01ne\n(\nL(Xk, Y k\u22121) = minX L(X, Y k\u22121)\nY k = Y k\u22121 + \u03b4kP\u2126(M \u2212Xk),\n(2.12)\nwhere {\u03b4k}k\u22651 is a sequence of positive step sizes. Uzawa\u2019s algorithm is, in fact, a subgradient\nmethod applied to the dual problem, where each step moves the current iterate in the direction of\nthe gradient or of a subgradient. Indeed, observe that\n\u2202Y g0(Y ) = \u2202Y L( \u02dc\nX, Y ) = P\u2126(M \u2212\u02dc\nX),\n(2.13)\nwhere \u02dc\nX is the minimizer of the Lagrangian for that value of Y so that a gradient descent update\nfor Y is of the form\nY k = Y k\u22121 + \u03b4k\u2202Y g0(Y k\u22121) = Y k\u22121 + \u03b4kP\u2126(M \u2212Xk).\nIt remains to compute the minimizer of the Lagrangian (2.12), and note that\narg min f\u03c4(X) + \u27e8Y , P\u2126(M \u2212X)\u27e9= arg min \u03c4\u2225X\u2225\u2217+ 1\n2\u2225X \u2212P\u2126Y \u22252\nF .\n(2.14)\nHowever, we know that the minimizer is given by D\u03c4(P\u2126(Y )) and since Y k = P\u2126(Y k) for all k \u22650,\nUzawa\u2019s algorithm takes the form\n(\nXk = D\u03c4(Y k\u22121)\nY k = Y k\u22121 + \u03b4kP\u2126(M \u2212Xk),\nwhich is exactly the update (2.7). This point of view brings to bear many di\ufb00erent mathemat-\nical tools for proving the convergence of the singular value thresholding iterations. For an early\nuse of Uzawa\u2019s algorithm minimizing an \u21131-like functional, the total-variation norm, under linear\ninequality constraints, see [12].\n3\nGeneral Formulation\nThis section presents a general formulation of the SVT algorithm for approximately minimizing the\nnuclear norm of a matrix under convex constraints.\n3.1\nLinear equality constraints\nSet the objective functional f\u03c4(X) = \u03c4\u2225X\u2225\u2217+ 1\n2\u2225X\u22252\nF for some \ufb01xed \u03c4 > 0, and consider the\nfollowing optimization problem:\nminimize\nf\u03c4(X)\nsubject to\nA(X) = b,\n(3.1)\n9\nwhere A is a linear transformation mapping n1 \u00d7n2 matrices into Rm (A\u2217is the adjoint of A). This\nmore general formulation is considered in [13] and [45] as an extension of the matrix completion\nproblem. Then the Lagrangian for this problem is of the form\nL(X, y) = f\u03c4(X) + \u27e8y, b \u2212A(X)\u27e9,\n(3.2)\nwhere X \u2208Rn1\u00d7n2 and y \u2208Rm, and starting with y0 = 0, Uzawa\u2019s iteration is given by\n(\nXk = D\u03c4(A\u2217(yk\u22121)),\nyk = yk\u22121 + \u03b4k(b \u2212A(Xk)).\n(3.3)\nThe iteration (3.3) is of course the same as (2.7) in the case where A is a sampling operator\nextracting m entries with indices in \u2126out of an n1 \u00d7 n2 matrix. To verify this claim, observe\nthat in this situation, A\u2217A = P\u2126, and let M be any matrix obeying A(M) = b. Then de\ufb01ning\nY k = A\u2217(yk) and substituting this expression in (3.3) gives (2.7).\n3.2\nGeneral convex constraints\nOne can also adapt the algorithm to handle general convex constraints.\nSuppose we wish to\nminimize f\u03c4(X) de\ufb01ned as before over a convex set X \u2208C. To simplify, we will assume that this\nconvex set is given by\nC = {X : fi(X) \u22640, \u2200i = 1, . . . , m},\nwhere the fi\u2019s are convex functionals (note that one can handle linear equality constraints by\nconsidering pairs of a\ufb03ne functionals). The problem of interest is then of the form\nminimize\nf\u03c4(X)\nsubject to\nfi(X) \u22640,\ni = 1, . . . , m.\n(3.4)\nJust as before, it is intuitive that as \u03c4 \u2192\u221e, the solution to this problem converges to a minimizer\nof the nuclear norm under the same constraints (1.7) as shown in Theorem 3.1 at the end of this\nsection.\nPut F(X) := (f1(X), . . . , fm(X)) for short. Then the Lagrangian for (3.4) is equal to\nL(X, y) = f\u03c4(X) + \u27e8y, F(X)\u27e9,\nwhere X \u2208Rn1\u00d7n2 and y \u2208Rm is now a vector with nonnegative components denoted, as usual,\nby y \u22650. One can apply Uzawa\u2019s method just as before with the only modi\ufb01cation that we will\nuse a subgradient method with projection to maximize the dual function since we need to make\nsure that the successive updates yk belong to the nonnegative orthant. This gives\n(\nXk = arg min {f\u03c4(X) + \u27e8yk\u22121, F(X)\u27e9},\nyk = [yk\u22121 + \u03b4kF(Xk)]+.\n(3.5)\nAbove, x+ is of course the vector with entries equal to max(xi, 0). When F is an a\ufb03ne mapping\nof the form b \u2212A(X) so that one solves\nminimize\nf\u03c4(X)\nsubject to\nA(X) \u2265b,\n10\nthis simpli\ufb01es to\n(\nXk = D\u03c4(A\u2217(yk\u22121)),\nyk = [yk\u22121 + \u03b4k(b \u2212A(Xk))]+,\n(3.6)\nand thus the extension to linear inequality constraints is straightforward.\n3.3\nExample\nAn interesting example concerns the extension of the Dantzig selector [18] to matrix problems.\nSuppose we have available linear measurements about a matrix M of interest\nb = A(M) + z,\n(3.7)\nwhere z \u2208Rm is a noise vector. Then under these circumstances, one might want to \ufb01nd the\nmatrix which minimizes the nuclear norm among all matrices which are consistent with the data b.\nInspired by the work on the Dantzig selector which was originally developed for estimating sparse\nparameter vectors from noisy data, one could approach this problem by solving\nminimize\n\u2225X\u2225\u2217\nsubject to\n|vec(A\u2217(r))| \u2264vec(E),\nr := b \u2212A(X),\n(3.8)\nwhere E is an array of tolerances, which is adjusted to \ufb01t the noise statistics [18]. Above, vec(A) \u2264\nvec(B), for any two matrices A and B, means componentwise inequalities; that is, Aij \u2264Bij for all\nindices i, j. We use this notation as not to confuse the reader with the positive semide\ufb01nite ordering.\nIn the case of the matrix completion problem where A extracts sampled entries indexed by \u2126, one\ncan always see the data vector as the sampled entries of some matrix B obeying P\u2126(B) = A\u2217(b);\nthe constraint is then natural for it may be expressed as\n|Bij \u2212Xij| \u2264Eij,\n(i, j) \u2208\u2126,\nIf z is white noise with standard deviation \u03c3, one may want to use a multiple of \u03c3 for Eij. In\nwords, we are looking for a matrix with minimum nuclear norm under the constraint that all of its\nsampled entries do not deviate too much from what has been observed.\nLet Y+ \u2208Rn1\u00d7n2 (resp. Y\u2212\u2208Rn1\u00d7n2) be the Lagrange multiplier associated with the compo-\nnentwise linear inequality constraints vec(A\u2217(r)) \u2264vec(E) (resp. \u2212vec(A\u2217(r)) \u2264vec(E)). Then\nstarting with Y 0\n\u00b1 = 0, the SVT iteration for this problem is of the form\n(\nXk = D\u03c4(A\u2217A(Y k\u22121\n+\n\u2212Y k\u22121\n\u2212\n)),\nY k\n\u00b1 = [Y k\u22121\n\u00b1\n+ \u03b4k(\u00b1A\u2217(rk) \u2212E)]+,\nrk = bk \u2212A(Xk),\n(3.9)\nwhere again [\u00b7]+ is applied componentwise.\nWe conclude by noting that in the matrix completion problem where A\u2217A = P\u2126and one\nobserves P\u2126(B), one can check that this iteration simpli\ufb01es to\n(\nXk = D\u03c4(Y k\u22121\n+\n\u2212Y k\u22121\n\u2212\n),\nY k\n\u00b1 = [Y k\u22121\n\u00b1\n+ \u03b4kP\u2126(\u00b1(B \u2212Xk) \u2212E)]+.\n(3.10)\nAgain, this is easy to implement and whenever the solution has low rank, the iterates Xk have low\nrank as well.\n11\n3.4\nWhen the proximal problem gets close\nWe now show that minimizing the proximal objective f\u03c4(X) = \u03c4\u2225X\u2225\u2217+ 1\n2\u2225X\u22252\nF is the same as\nminimizing the nuclear norm in the limit of large \u03c4\u2019s. The theorem below is general and covers the\nspecial case of linear equality constraints as in (2.8).\nTheorem 3.1 Let X\u22c6\n\u03c4 be the solution to (3.4) and X\u221ebe the minimum Frobenius-norm solution\nto (1.7) de\ufb01ned as\nX\u221e:= arg min\nX {\u2225X\u22252\nF\n: X is a solution of (1.7)}.\n(3.11)\nAssume that the fi(X)\u2019s, 1 \u2264i \u2264m, are convex and lower semi-continuous. Then\nlim\n\u03c4\u2192\u221e\u2225X\u22c6\n\u03c4 \u2212X\u221e\u2225F = 0.\n(3.12)\nProof. It follows from the de\ufb01nition of X\u22c6\n\u03c4 and X\u221ethat\n\u2225X\u22c6\n\u03c4 \u2225\u2217+ 1\n2\u03c4 \u2225X\u22c6\n\u03c4 \u22252\nF \u2264\u2225X\u221e\u2225\u2217+ 1\n2\u03c4 \u2225X\u221e\u22252\nF ,\nand\n\u2225X\u221e\u2225\u2217\u2264\u2225X\u22c6\n\u03c4 \u2225\u2217.\n(3.13)\nSumming these two inequalities gives\n\u2225X\u22c6\n\u03c4 \u22252\nF \u2264\u2225X\u221e\u22252\nF ,\n(3.14)\nwhich implies that \u2225X\u22c6\n\u03c4 \u22252\nF is bounded uniformly in \u03c4. Thus, we would prove the theorem if we\ncould establish that any convergent subsequence {X\u22c6\n\u03c4k}k\u22651 must converge to X\u221e.\nConsider an arbitrary converging subsequence {X\u22c6\n\u03c4k} and set Xc := limk\u2192\u221eX\u22c6\n\u03c4k. Since for\neach 1 \u2264i \u2264m, fi(X\u22c6\n\u03c4k) \u22640 and fi is lower semi-continuous, Xc obeys\nfi(Xc) \u22640,\ni = 1, . . . , m.\n(3.15)\nFurthermore, since \u2225X\u22c6\n\u03c4 \u22252\nF is bounded, (3.13) yields\nlim sup\n\u03c4\u2192\u221e\n\u2225X\u22c6\n\u03c4 \u2225\u2217\u2264\u2225X\u221e\u2225\u2217,\n\u2225X\u221e\u2225\u2217\u2264lim inf\n\u03c4\u2192\u221e\u2225X\u22c6\n\u03c4 \u2225\u2217.\nAn immediate consequence is lim\u03c4\u2192\u221e\u2225X\u22c6\n\u03c4 \u2225\u2217= \u2225X\u221e\u2225\u2217and, therefore, \u2225Xc\u2225\u2217= \u2225X\u221e\u2225\u2217. This\nshows that Xc is a solution to (1.1). Now it follows from the de\ufb01nition of X\u221ethat \u2225Xc\u2225F \u2265\n\u2225X\u221e\u2225F , while we also have \u2225Xc\u2225F \u2264\u2225X\u221e\u2225F because of (3.14). We conclude that \u2225Xc\u2225F =\n\u2225X\u221e\u2225F and thus Xc = X\u221esince X\u221eis unique.\n4\nConvergence Analysis\nThis section establishes the convergence of the SVT iterations. We begin with the simpler proof\nof the convergence of (2.7) in the special case of the matrix completion problem, and then present\nthe argument for the more general constraints (3.5). We hope that this progression will make the\nsecond and more general proof more transparent.\n12\n4.1\nConvergence for matrix completion\nWe begin by recording a lemma which establishes the strong convexity of the objective f\u03c4.\nLemma 4.1 Let Z \u2208\u2202f\u03c4(X) and Z\u2032 \u2208\u2202f\u03c4(X\u2032). Then\n\u27e8Z \u2212Z\u2032, X \u2212X\u2032\u27e9\u2265\u2225X \u2212X\u2032\u22252\nF .\n(4.1)\nProof. An element Z of \u2202f\u03c4(X) is of the form Z = \u03c4Z0 + X, where Z0 \u2208\u2202\u2225X\u2225\u2217, and similarly\nfor Z\u2032. This gives\n\u27e8Z \u2212Z\u2032, X \u2212X\u2032\u27e9= \u03c4 \u27e8Z0 \u2212Z\u2032\n0, X \u2212X\u2032\u27e9+ \u2225X \u2212X\u2032\u22252\nF\nand it thus su\ufb03ces to show that the \ufb01rst term of the right-hand side is nonnegative. From (2.6),\nwe have that any subgradient of the nuclear norm at X obeys \u2225Z0\u22252 \u22641 and \u27e8Z0, X\u27e9= \u2225X\u2225\u2217. In\nparticular, this gives\n|\u27e8Z0, X\u2032\u27e9| \u2264\u2225Z0\u22252\u2225X\u2032\u2225\u2217\u2264\u2225X\u2032\u2225\u2217,\n|\u27e8Z\u2032\n0, X\u27e9| \u2264\u2225Z\u2032\n0\u22252\u2225X\u2225\u2217\u2264\u2225X\u2225\u2217.\nWhence,\n\u27e8Z0 \u2212Z\u2032\n0, X \u2212X\u2032\u27e9= \u27e8Z0, X\u27e9+ \u27e8Z\u2032\n0, X\u2032\u27e9\u2212\u27e8Z0, X\u2032\u27e9\u2212\u27e8Z\u2032\n0, X\u27e9\n= \u2225X\u2225\u2217+ \u2225X\u2032\u2225\u2217\u2212\u27e8Z0, X\u2032\u27e9\u2212\u27e8Z\u2032\n0, X\u27e9\u22650,\nwhich proves the lemma.\nThis lemma is key in showing that the SVT algorithm (2.7) converges.\nTheorem 4.2 Suppose that the sequence of step sizes obeys 0 < inf \u03b4k \u2264sup \u03b4k < 2. Then the\nsequence {Xk} obtained via (2.7) converges to the unique solution of (2.8).\nProof. Let (X\u22c6, Y \u22c6) be primal-dual optimal for the problem (2.8). The optimality conditions give\n0 = Zk \u2212P\u2126(Y k\u22121)\n0 = Z\u22c6\u2212P\u2126(Y \u22c6),\nfor some Zk \u2208\u2202f\u03c4(Xk) and some Z\u22c6\u2208\u2202f\u03c4(X\u22c6). We then deduce that\n(Zk \u2212Z\u22c6) \u2212P\u2126(Y k\u22121 \u2212Y \u22c6) = 0\nand, therefore, it follows from Lemma 4.1 that\n\u27e8Xk \u2212X\u22c6, P\u2126(Y k\u22121 \u2212Y \u22c6)\u27e9= \u27e8Zk \u2212Z\u22c6, Xk \u2212X\u22c6\u27e9\u2265\u2225Xk \u2212X\u22c6\u22252\nF .\n(4.2)\nWe continue and observe that because P\u2126X\u22c6= P\u2126M,\n\u2225P\u2126(Y k \u2212Y \u22c6)\u2225F = \u2225P\u2126(Y k\u22121 \u2212Y \u22c6) + \u03b4kP\u2126(X\u22c6\u2212Xk)\u2225F .\nTherefore, setting rk = \u2225P\u2126(Y k \u2212Y \u22c6)\u2225F ,\nr2\nk = r2\nk\u22121 \u22122\u03b4k\u27e8P\u2126(Y k\u22121 \u2212Y \u22c6), Xk \u2212X\u22c6\u27e9+ \u03b42\nk\u2225P\u2126(X\u22c6\u2212Xk)\u22252\nF\n\u2264r2\nk\u22121 \u22122\u03b4k\u2225Xk \u2212X\u22c6\u22252\nF + \u03b42\nk\u2225Xk \u2212X\u22c6\u22252\nF\n(4.3)\nsince for any matrix X, \u2225P\u2126(X)\u2225F \u2264\u2225X\u2225F . Under our assumptions about the size of \u03b4k, we have\n2\u03b4k \u2212\u03b42\nk \u2265\u03b2 for all k \u22651 and some \u03b2 > 0 and thus\nr2\nk \u2264r2\nk\u22121 \u2212\u03b2\u2225Xk \u2212X\u22c6\u22252\nF .\n(4.4)\nTwo properties follow from this:\n13\n1. The sequence {\u2225P\u2126(Y k \u2212Y \u22c6)\u2225F } is nonincreasing and, therefore, converges to a limit.\n2. As a consequence, \u2225Xk \u2212X\u22c6\u22252\nF \u21920 as k \u2192\u221e.\nThe theorem is established.\n4.2\nGeneral convergence theorem\nOur second result is more general and establishes the convergence of the SVT iterations to the\nsolution of (3.4) under general convex constraints. From now now, we will only assume that the\nfunction F(X) is Lipschitz in the sense that\n\u2225F(X) \u2212F(Y \u2225\u2264L(F)\u2225X \u2212Y \u2225F ,\n(4.5)\nfor some nonnegative constant L(F).\nNote that if F is a\ufb03ne, F(X) = b \u2212A(X), we have\nL(F) = \u2225A\u22252 where \u2225A\u22252 is the spectrum norm of the linear transformation A de\ufb01ned as \u2225A\u22252 :=\nsup{\u2225A(X)\u2225\u21132 : \u2225X\u2225F = 1}. We also recall that F(X) = (f1(X), . . . , fm(X)) where each fi is\nconvex, and that the Lagrangian for the problem (3.4) is given by\nL(X, y) = f\u03c4(X) + \u27e8y, F(X)\u27e9,\ny \u22650.\nWe will assume to simplify that strong duality holds which is automatically true if the constraints\nobey constraint quali\ufb01cations such as Slater\u2019s condition [6].\nWe \ufb01rst establish the following preparatory lemma.\nLemma 4.3 Let (X\u22c6, y\u22c6) be a primal-dual optimal pair for (3.4). Then for each \u03b4 > 0, y\u22c6obeys\ny\u22c6= [y\u22c6+ \u03b4F(X\u22c6)]+.\n(4.6)\nProof. Recall that the projection x0 of a point x onto a convex set C is characterized by\n(\nx0 \u2208C,\n\u27e8y \u2212x0, x \u2212x0\u27e9\u22640, \u2200y \u2208C.\nIn the case where C = Rm\n+ = {x \u2208Rm : x \u22650}, this condition becomes x0 \u22650 and\n\u27e8y \u2212x0, x \u2212x0\u27e9\u22640, \u2200y \u22650.\nNow because y\u22c6is dual optimal we have\nL(X\u22c6, y\u22c6) \u2265L(X\u22c6, y),\n\u2200y \u22650.\nSubstituting the expression for the Lagrangian, this is equivalent to\n\u27e8y \u2212y\u22c6, F(X\u22c6)\u27e9\u22640,\n\u2200y \u22650,\nwhich is the same as\n\u27e8y \u2212y\u22c6, y\u22c6+ \u03c1F(X\u22c6) \u2212y\u22c6\u27e9\u22640,\n\u2200y \u22650, \u2200\u03c1 \u22650.\nHence it follows that y\u22c6must be the projection of y\u22c6+ \u03c1F(X\u22c6) onto the nonnegative orthant Rm\n+.\nSince the projection of an arbitrary vector x onto Rm\n+ is given by x+, our claim follows.\nWe are now in the position to state our general convergence result.\n14\nTheorem 4.4 Suppose that the sequence of step sizes obeys 0 < inf \u03b4k \u2264sup \u03b4k < 2/\u2225L(F)\u22252,\nwhere L(F) is the Lipschitz constant in (4.5). Then assuming strong duality, the sequence {Xk}\nobtained via (3.5) converges to the unique solution of (3.4).\nProof. Let (X\u22c6, y\u22c6) be primal-dual optimal for the problem (3.4). We claim that the optimality\nconditions give that for all X\n\u27e8Zk, X \u2212Xk\u27e9+ \u27e8yk\u22121, F(X) \u2212F(Xk)\u27e9\u22650,\n\u27e8Z\u22c6, X \u2212X\u22c6\u27e9+ \u27e8y\u22c6, F(X) \u2212F(X\u22c6)\u27e9\u22650,\n(4.7)\nfor some Zk \u2208\u2202f\u03c4(Xk) and some Z\u22c6\u2208\u2202f\u03c4(X\u22c6). We justify this assertion by proving one of the\ntwo inequalities since the other is exactly similar. For the \ufb01rst, Xk minimizes L(X, yk\u22121) over all\nX and, therefore, there exist Zk \u2208\u2202f\u03c4(Xk) and Zk\ni \u2208\u2202fi(Xk), 1 \u2264i \u2264m, such that\nZk +\nm\nX\ni=1\nyk\u22121\ni\nZk\ni = 0.\nNow because each fi is convex,\nfi(X) \u2212fi(Xk) \u2265\u27e8Zk\ni , X \u2212Xk\u27e9\nand, therefore,\n\u27e8Zk, X \u2212Xk\u27e9+\nm\nX\ni=1\nyk\u22121\ni\n(fi(X) \u2212fi(Xk)) \u2265\u27e8Zk +\nm\nX\ni=1\nyk\u22121\ni\nZk\ni , X \u2212Xk\u27e9= 0.\nThis is (4.7).\nNow write the \ufb01rst inequality in (4.7) for X\u22c6, the second for Xk and sum the two inequalities.\nThis gives\n\u27e8Zk \u2212Z\u22c6, Xk \u2212X\u22c6\u27e9+ \u27e8yk\u22121 \u2212y\u22c6, F(Xk) \u2212F(X\u22c6)\u27e9\u22640.\nThe rest of the proof is essentially the same as that of Theorem 4.5. It follows from Lemma 4.1\nthat\n\u27e8yk\u22121 \u2212y\u22c6, F(Xk) \u2212F(X\u22c6)\u27e9\u2264\u2212\u27e8Zk \u2212Z\u22c6, Xk \u2212X\u22c6\u27e9\u2264\u2212\u2225Xk \u2212X\u22c6\u22252\nF .\n(4.8)\nWe continue and observe that because y\u22c6= [y\u22c6+ \u03b4kF(X)]+ by Lemma 4.3, we have\n\u2225yk \u2212y\u22c6\u2225= \u2225[yk\u22121 + \u03b4kF(Xk)]+ \u2212[y\u22c6+ \u03b4kF(X\u22c6)]+\u2225\n\u2264\u2225yk\u22121 \u2212y\u22c6+ \u03b4k(F(Xk) \u2212F(X\u22c6))\u2225\nsince the projection onto the convex set Rm\n+ is a contraction. Therefore,\n\u2225yk \u2212y\u22c6\u22252 = \u2225yk\u22121 \u2212y\u22c6\u22252 + 2\u03b4k \u27e8yk\u22121 \u2212y\u22c6, F(Xk) \u2212F(X\u22c6)\u27e9+ \u03b42\nk\u2225F(Xk) \u2212F(X\u22c6)\u22252\n\u2264\u2225yk\u22121 \u2212y\u22c6\u22252 \u22122\u03b4k\u2225Xk \u2212X\u22c6\u22252\nF + \u03b42\nkL2 \u2225Xk \u2212X\u22c6\u22252\nF ,\nwhere we have put L instead of L(F) for short. Under our assumptions about the size of \u03b4k, we\nhave 2\u03b4k \u2212\u03b42\nkL2 \u2265\u03b2 for all k \u22651 and some \u03b2 > 0. Then\n\u2225yk \u2212y\u22c6\u22252 \u2264\u2225yk\u22121 \u2212y\u22c6\u22252 \u2212\u03b2\u2225Xk \u2212X\u22c6\u22252\nF ,\n(4.9)\n15\nand the conclusion is as before.\nThe problem (3.1) with linear constraints can be reduced to (3.4) by choosing\nF(X) =\n\u0014 b\n\u2212b\n\u0015\n\u2212\n\u0014 A\n\u2212A\n\u0015\nX,\nand we have the following corollary:\nCorollary 4.5 Suppose that the sequence of step sizes obeys 0 < inf \u03b4k \u2264sup \u03b4k < 2/\u2225A\u22252\n2. Then\nthe sequence {Xk} obtained via (3.3) converges to the unique solution of (3.1).\nLet \u2225A\u22252 := sup{\u2225A(X)\u2225F : \u2225X\u2225F = 1}. With F(X) given as above, we have |L(F)|2 = 2\u2225A\u22252\n2\nand thus, Theorem 4.4 guarantees convergence as long as 0 < inf \u03b4k \u2264sup \u03b4k < 1/\u2225A\u22252\n2. However,\nan argument identical to the proof of Theorem 4.2 would remove the extra factor of two. We omit\nthe details.\n5\nImplementation and Numerical Results\nThis section provides implementation details of the SVT algorithm\u2014as to make it practically\ne\ufb00ective for matrix completion\u2014such as the numerical evaluation of the singular value thresholding\noperator, the selection of the step size \u03b4k, the selection of a stopping criterion, and so on. This\nsection also introduces several numerical simulation results which demonstrate the performance\nand e\ufb00ectiveness of the SVT algorithm. We show that 30, 000 \u00d7 30, 000 matrices of rank 10 are\nrecovered from just about 0.4% of their sampled entries in a matter of a few minutes on a modest\ndesktop computer with a 1.86 GHz CPU (dual core with Matlab\u2019s multithreading option enabled)\nand 3 GB of memory.\n5.1\nImplementation details\n5.1.1\nEvaluation of the singular value thresholding operator\nTo apply the singular value tresholding operator at level \u03c4 to an input matrix, it su\ufb03ces to know\nthose singular values and corresponding singular vectors above the threshold \u03c4.\nIn the matrix\ncompletion problem, the singular value thresholding operator is applied to sparse matrices {Y k}\nsince the number of sampled entries is typically much lower than the number of entries in the\nunknown matrix M, and we are hence interested in numerical methods for computing the dominant\nsingular values and singular vectors of large sparse matrices. The development of such methods is\na relatively mature area in scienti\ufb01c computing and numerical linear algebra in particular. In fact,\nmany high-quality packages are readily available. Our implementation uses PROPACK, see [36]\nfor documentation and availability. One reason for this choice is convenience: PROPACK comes\nin a Matlab and a Fortran version, and we \ufb01nd it convenient to use the well-documented Matlab\nversion. More importantly, PROPACK uses the iterative Lanczos algorithm to compute the singular\nvalues and singular vectors directly, by using the Lanczos bidiagonalization algorithm with partial\nreorthogonalization. In particular, PROPACK does not compute the eigenvalues and eigenvectors\nof (Y k)\u2217Y k and Y k(Y k)\u2217, or of an augmented matrix as in the Matlab built-in function \u2018svds\u2019 for\nexample. Consequently, PROPACK is an e\ufb03cient\u2014both in terms of number of \ufb02ops and storage\nrequirement\u2014and stable package for computing the dominant singular values and singular vectors\n16\nof a large sparse matrix.\nFor information, the available documentation [36] reports a speedup\nfactor of about ten over Matlab\u2019s \u2018svds\u2019. Furthermore, the Fortran version of PROPACK is about\n3\u20134 times faster than the Matlab version. Despite this signi\ufb01cant speedup, we have only used the\nMatlab version but since the singular value shrinkage operator is by-and-large the dominant cost in\nthe SVT algorithm, we expect that a Fortran implementation would run about 3 to 4 times faster.\nAs for most SVD packages, though one can specify the number of singular values to compute,\nPROPACK can not automatically compute only those singular values exceeding the threshold \u03c4.\nOne must instead specify the number s of singular values ahead of time, and the software will\ncompute the s largest singular values and corresponding singular vectors. To use this package, we\nmust then determine the number sk of singular values of Y k\u22121 to be computed at the kth iteration.\nWe use the following simple method. Let rk\u22121 = rank(Xk\u22121) be the number of nonzero singular\nvalues of Xk\u22121 at the previous iteration. Set sk = rk\u22121 +1 and compute the \ufb01rst sk singular values\nof Y k\u22121. If some of the computed singular values are already smaller than \u03c4, then sk is a right\nchoice. Otherwise, increment sk by a prede\ufb01ned integer \u2113repeatedly until some of the singular\nvalues fall below \u03c4. In the experiments, we choose \u2113= 5. Another rule might be to repeatedly\nmultiply sk by a positive number\u2014e.g. 2\u2014until our criterion is met. Incrementing sk by a \ufb01xed\ninteger works very well in practice; in our experiments, we very rarely need more than one update.\nWe note that it is not necessary to rerun the Lanczos iterations for the \ufb01rst sk vectors since they\nhave been already computed; only a few new singular values (\u2113of them) need to be numerically\nevaluated. This can be done by modifying the PROPACK routines. We have not yet modi\ufb01ed\nPROPACK, however. Had we done so, our run times would be decreased.\n5.1.2\nStep sizes\nThere is a large literature on ways of selecting a step size but for simplicity, we shall use step sizes\nthat are independent of the iteration count; that is \u03b4k = \u03b4 for k = 1, 2, . . .. From Theorem 4.2,\nconvergence for the completion problem is guaranteed (2.7) provided that 0 < \u03b4 < 2. This choice\nis, however, too conservative and the convergence is typically slow. In our experiments, we use\ninstead\n\u03b4 = 1.2 n1n2\nm ,\n(5.1)\ni.e. 1.2 times the undersampling ratio. We give a heuristic justi\ufb01cation below.\nConsider a \ufb01xed matrix A \u2208Rn1\u00d7n2. Under the assumption that the column and row spaces of\nA are not well aligned with the vectors taken from the canonical basis of Rn1 and Rn2 respectively\u2014\nthe incoherence assumption in [13]\u2014then with very large probability over the choices of \u2126, we have\n(1 \u2212\u01eb)p \u2225A\u22252\nF \u2264\u2225P\u2126(A)\u22252\nF \u2264(1 + \u01eb)p \u2225A\u22252\nF ,\np := m/(n1n2),\n(5.2)\nprovided that the rank of A is not too large. The probability model is that \u2126is a set of sampled\nentries of cardinality m sampled uniformly at random so that all the choices are equally likely. In\n(5.2), we want to think of \u01eb as a small constant, e.g. smaller than 1/2. In other words, the \u2018energy\u2019\nof A on \u2126(the set of sampled entries) is just about proportional to the size of \u2126. The near isometry\n(5.2) is a consequence of Theorem 4.1 in [13], and we omit the details.\nNow returning to the proof of Theorem 4.2, we see that a su\ufb03cient condition for the convergence\nof (2.7) is\n\u2203\u03b2 > 0,\n\u22122\u03b4\u2225X\u22c6\u2212Xk\u22252\nF + \u03b42\u2225P\u2126(X\u22c6\u2212Xk)\u22252\nF \u2264\u2212\u03b2\u2225X\u22c6\u2212Xk\u22252\nF ,\n17\ncompare (4.4), which is equivalent to\n0 < \u03b4 < 2\n\u2225X\u22c6\u2212Xk\u22252\nF\n\u2225P\u2126(X\u22c6\u2212Xk)\u22252\nF\n.\nSince \u2225P\u2126(X)\u2225F \u2264\u2225X\u2225F for any matrix X \u2208Rn1\u00d7n2, it is safe to select \u03b4 < 2. But suppose that\nwe could apply (5.2) to the matrix A = X\u22c6\u2212Xk. Then we could take \u03b4 inversely proportional\nto p; e.g. with \u01eb = 1/4, we could take \u03b4 \u22641.6p\u22121. Below, we shall use the value \u03b4 = 1.2p\u22121 which\nallows us to take large steps and still provides convergence, at least empirically.\nThe reason why this is not a rigorous argument is that (5.2) cannot be applied to A = X\u22c6\u2212Xk\neven though this matrix di\ufb00erence may obey the incoherence assumption. The issue here is that\nX\u22c6\u2212Xk is not a \ufb01xed matrix, but rather depends on \u2126since the iterates {Xk} are computed\nwith the knowledge of the sampled set.\n5.1.3\nInitial steps\nThe SVT algorithm starts with Y 0 = 0, and we want to choose a large \u03c4 to make sure that the\nsolution of (2.8) is close enough to a solution of (1.1). De\ufb01ne k0 as that integer obeying\n\u03c4\n\u03b4\u2225P\u2126(M)\u22252\n\u2208(k0 \u22121, k0].\n(5.3)\nSince Y 0 = 0, it is not di\ufb03cult to see that\nXk = 0,\nY k = k\u03b4 P\u2126(M),\nk = 1, . . . , k0.\nTo save work, we may simply skip the computations of X1, . . . , Xk0, and start the iteration by\ncomputing Xk0+1 from Y k0.\nThis strategy is a special case of a kicking device introduced in [44]; the main idea of such\na kicking scheme is that one can \u2018jump over\u2019 a few steps whenever possible.\nJust like in the\naforementioned reference, we can develop similar kicking strategies here as well. Because in our\nnumerical experiments the kicking is rarely triggered, we forgo the description of such strategies.\n5.1.4\nStopping criteria\nHere, we discuss stopping criteria for the sequence of SVT iterations (2.7), and present two possi-\nbilities.\nThe \ufb01rst is motivated by the \ufb01rst-order optimality conditions or KKT conditions tailored to the\nminimization problem (2.8). By (2.14) and letting \u2202Y g0(Y ) = 0 in (2.13), we see that the solution\nX\u22c6\n\u03c4 to (2.8) must also verify\n(\nX = D\u03c4(Y ),\nP\u2126(X \u2212M) = 0,\n(5.4)\nwhere Y is a matrix vanishing outside of \u2126c. Therefore, to make sure that Xk is close to X\u22c6\n\u03c4 , it\nis su\ufb03cient to check how close (Xk, Y k\u22121) is to obeying (5.4). By de\ufb01nition, the \ufb01rst equation in\n(5.4) is always true. Therefore, it is natural to stop (2.7) when the error in the second equation is\nbelow a speci\ufb01ed tolerance. We suggest stopping the algorithm when\n\u2225P\u2126(Xk \u2212M)\u2225F\n\u2225P\u2126(M)\u2225F\n\u2264\u01eb,\n(5.5)\n18\nwhere \u01eb is a \ufb01xed tolerance, e.g. 10\u22124. We provide a short heuristic argument justifying this choice\nbelow.\nIn the matrix completion problem, we know that under suitable assumptions\n\u2225P\u2126(M)\u22252\nF \u224dp \u2225M\u22252\nF ,\nwhich is just (5.2) applied to the \ufb01xed matrix M (the symbol \u224dhere means that there is a constant\n\u01eb as in (5.2)). Suppose we could also apply (5.2) to the matrix Xk \u2212M (which we rigorously cannot\nsince Xk depends on \u2126), then we would have\n\u2225P\u2126(Xk \u2212M)\u22252\nF \u224dp \u2225Xk \u2212M\u22252\nF ,\n(5.6)\nand thus\n\u2225P\u2126(Xk \u2212M)\u2225F\n\u2225P\u2126(M)\u2225F\n\u224d\u2225Xk \u2212M\u2225F\n\u2225M\u2225F\n.\nIn words, one would control the relative reconstruction error by controlling the relative error on\nthe set of sampled locations.\nA second stopping criterion comes from duality theory. Firstly, the iterates Xk are generally\nnot feasible for (2.8) although they become asymptotically feasible. One can construct a feasible\npoint from Xk by projecting it onto the a\ufb03ne space {X : P\u2126(X) = P\u2126(M)} as follows:\n\u02dc\nXk = Xk + P\u2126(M \u2212Xk).\nAs usual let f\u03c4(X) = \u03c4\u2225X\u2225\u2217+ 1\n2\u2225X\u22252\nF and denote by p\u22c6the optimal value of (2.8). Since \u02dc\nXk is\nfeasible, we have\np\u22c6\u2264f\u03c4( \u02dc\nXk) := bk.\nSecondly, using the notations of Section 2.4, duality theory gives that\nak := g0(Y k\u22121) = L(Xk, Y k\u22121) \u2264p\u22c6.\nTherefore, bk \u2212ak is an upper bound on the duality gap and one can stop the algorithm when this\nquantity falls below a given tolerance.\nFor very large problems in which one holds Xk in reduced SVD form, one may not want to\ncompute the projection \u02dc\nXk since this matrix would not have low rank and would require signi\ufb01-\ncant storage space (presumably, one would not want to spend much time computing this projection\neither). Hence, the second method only makes practical sense when the dimensions are not pro-\nhibitively large, or when the iterates do not have low rank.\n5.1.5\nAlgorithm\nWe conclude this section by summarizing the implementation details and give the SVT algorithm\nfor matrix completion below (Algorithm 1). Of course, one would obtain a very similar structure\nfor the more general problems of the form (3.1) and (3.4) with linear inequality constraints. For\nconvenience, de\ufb01ne for each nonnegative integer s \u2264min{n1, n2},\n[U k, \u03a3k, V k]s,\nk = 1, 2, . . . ,\nwhere U k = [uk\n1, . . . , uk\ns] and V k = [vk\n1, . . . , vk\ns] are the \ufb01rst s singular vectors of the matrix Y k,\nand \u03a3k is a diagonal matrix with the \ufb01rst s singular values \u03c3k\n1, . . . , \u03c3k\ns on the diagonal.\n19\nAlgorithm 1: Singular Value Thresholding (SVT) Algorithm\nInput: sampled set \u2126and sampled entries P\u2126(M), step size \u03b4, tolerance \u01eb, parameter\n\u03c4, increment \u2113, and maximum iteration count kmax\nOutput: Xopt\nDescription: Recover a low-rank matrix M from a subset of sampled entries\n1\nSet Y 0 = k0\u03b4 P\u2126(M) (k0 is de\ufb01ned in (5.3))\n2\nSet r0 = 0\n3\nfor k = 1 to kmax\n4\nSet sk = rk\u22121 + 1\n5\nrepeat\n6\nCompute [U k\u22121, \u03a3k\u22121, V k\u22121]sk\n7\nSet sk = sk + \u2113\n8\nuntil \u03c3k\u22121\nsk\u2212\u2113\u2264\u03c4\n9\nSet rk = max{j : \u03c3k\u22121\nj\n> \u03c4}\n10\nSet Xk = Prk\nj=1(\u03c3k\u22121\nj\n\u2212\u03c4)uk\u22121\nj\nvk\u22121\nj\n11\nif \u2225P\u2126(Xk \u2212M)\u2225F /\u2225P\u2126M\u2225F \u2264\u01eb then break\n12\nSet Y k\nij =\n(\n0\nif (i, j) \u0338\u2208\u2126,\nY k\u22121\nij\n+ \u03b4(Mij \u2212Xk\nij)\nif (i, j) \u2208\u2126\n13\nend for k\n14\nSet Xopt = Xk\n5.2\nNumerical results\n5.2.1\nLinear equality constraints\nOur implementation is in Matlab and all the computational results we are about to report were\nobtained on a desktop computer with a 1.86 GHz CPU (dual core with Matlab\u2019s multithreading\noption enabled) and 3 GB of memory. In our simulations, we generate n \u00d7 n matrices of rank r\nby sampling two n \u00d7 r factors ML and MR independently, each having i.i.d. Gaussian entries, and\nsetting M = MLM \u2217\nR as it is suggested in [13]. The set of observed entries \u2126is sampled uniformly\nat random among all sets of cardinality m.\nThe recovery is performed via the SVT algorithm (Algorithm 1), and we use\n\u2225P\u2126(Xk \u2212M)\u2225F /\u2225P\u2126M\u2225F < 10\u22124\n(5.7)\nas a stopping criterion. As discussed earlier, the step sizes are constant and we set \u03b4 = 1.2p\u22121.\nThroughout this section, we denote the output of the SVT algorithm by Xopt. The parameter \u03c4\nis chosen empirically and set to \u03c4 = 5n. A heuristic argument is as follows. Clearly, we would like\nthe term \u03c4\u2225M\u2225\u2217to dominate the other, namely, 1\n2\u2225M\u22252\nF . For products of Gaussian matrices as\nabove, standard random matrix theory asserts that the Frobenius norm of M concentrates around\nn\u221ar, and that the nuclear norm concentrates around about nr (this should be clear in the simple\ncase where r = 1 and is generally valid). The value \u03c4 = 5n makes sure that on the average, the\n20\nUnknown M\nComputational results\nsize (n \u00d7 n)\nrank (r)\nm/dr\nm/n2\ntime(s)\n# iters\nrelative error\n10\n6\n0.12\n23\n117\n1.64 \u00d7 10\u22124\n1, 000 \u00d7 1, 000\n50\n4\n0.39\n196\n114\n1.59 \u00d7 10\u22124\n100\n3\n0.57\n501\n129\n1.68 \u00d7 10\u22124\n10\n6\n0.024\n147\n123\n1.73 \u00d7 10\u22124\n5, 000 \u00d7 5, 000\n50\n5\n0.10\n950\n108\n1.61 \u00d7 10\u22124\n100\n4\n0.158\n3,339\n123\n1.72 \u00d7 10\u22124\n10\n6\n0.012\n281\n123\n1.73 \u00d7 10\u22124\n10, 000 \u00d7 10, 000\n50\n5\n0.050\n2,096\n110\n1.65 \u00d7 10\u22124\n100\n4\n0.080\n7,059\n127\n1.79 \u00d7 10\u22124\n10\n6\n0.006\n588\n124\n1.73 \u00d7 10\u22124\n20, 000 \u00d7 20, 000\n50\n5\n0.025\n4,581\n111\n1.66 \u00d7 10\u22124\n30, 000 \u00d7 30, 000\n10\n6\n0.004\n1,030\n125\n1.73 \u00d7 10\u22124\nTable 1: Experimental results for matrix completion. The rank r is the rank of the unknown\nmatrix M, m/dr is the ratio between the number of sampled entries and the number of\ndegrees of freedom in an n \u00d7 n matrix of rank r (oversampling ratio), and m/n2 is the fraction\nof observed entries. All the computational results on the right are averaged over \ufb01ve runs.\nvalue of \u03c4\u2225M\u2225\u2217is about 10 times that of 1\n2\u2225M\u22252\nF as long as the rank is bounded away from the\ndimension n.\nOur computational results are displayed in Table 1. There, we report the run time in seconds, the\nnumber of iterations it takes to reach convergence (5.7), and the relative error of the reconstruction\nrelative error = \u2225Xopt \u2212M\u2225F /\u2225M\u2225F ,\n(5.8)\nwhere M is the real unknown matrix. All of these quantities are averaged over \ufb01ve runs. The table\nalso gives the percentage of entries that are observed, namely, m/n2 together with a quantity that\nwe may want to think as the information oversampling ratio. Recall that an n \u00d7 n matrix of rank\nr depends upon dr := r(2n \u2212r) degrees of freedom. Then m/dr is the ratio between the number of\nsampled entries and the \u2018true dimensionality\u2019 of an n \u00d7 n matrix of rank r.\nThe \ufb01rst observation is that the SVT algorithm performs extremely well in these experiments.\nIn all of our experiments, it takes fewer than 200 SVT iterations to reach convergence.\nAs a\nconsequence, the run times are short.\nAs indicated in the table, we note that one recovers a\n1, 000\u00d71, 000 matrix of rank 10 in less than a minute. The algorithm also recovers 30, 000\u00d730, 000\nmatrices of rank 10 from about 0.4% of their sampled entries in just about 17 minutes. In addition,\nhigher-rank matrices are also e\ufb03ciently completed: for example, it takes between one and two\nhours to recover 10, 000\u00d710, 000 matrices of rank 100 and 20, 000\u00d720, 000 matrices of rank 50. We\nwould like to stress that these numbers were obtained on a modest CPU (1.86GHz). Furthermore,\na Fortran implementation is likely to cut down on these numbers by a multiplicative factor typically\nbetween three and four.\nWe also check the validity of the stopping criterion (5.7) by inspecting the relative error de\ufb01ned\nin (5.8). The table shows that the heuristic and nonrigorous analysis of Section 5.1 holds in practice\nsince the relative reconstruction error is of the same order as \u2225P\u2126(Xopt \u2212M)\u2225F /\u2225P\u2126M\u2225F \u223c10\u22124.\nIndeed, the overall relative errors reported in Table 1 are all less than 2 \u00d7 10\u22124.\n21\nWe emphasized all along an important feature of the SVT algorithm, which is that the matrices\nXk have low rank.\nWe demonstrate this fact empirically in Figure 1, which plots the rank of\nXk versus the iteration count k, and does this for unknown matrices of size 5, 000 \u00d7 5, 000 with\ndi\ufb00erent ranks. The plots reveal an interesting phenomenon: in our experiments, the rank of Xk\nis nondecreasing so that the maximum rank is reached in the \ufb01nal steps of the algorithm. In fact,\nthe rank of the iterates quickly reaches the value r of the true rank. After these few initial steps,\nthe SVT iterations search for that matrix with rank r minimizing the objective functional. As\nmentioned earlier, the low-rank property is crucial for making the algorithm run fast.\n0\n50\n100\n150\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\nItertion Step k\nRank of Xk\n0\n20\n40\n60\n80\n100\n120\n140\n0\n10\n20\n30\n40\n50\n60\nIteration step k\nRank of Xk\n0\n50\n100\n150\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90\n100\n110\nIteration step k\nRank of Xk\nr = 10\nr = 50\nr = 100\nFigure 1: Rank of Xk as a function k when the unknown matrix M is of size 5, 000 \u00d7 5, 000\nand of rank r.\nFinally, we demonstrate the results of the SVT algorithm for matrix completion from noisy\nsampled entries. Suppose we observe data from the model\nBij = Mij + Zij,\n(i, j) \u2208\u2126,\n(5.9)\nwhere Z is a zero-mean Gaussian white noise with standard deviation \u03c3. We run the SVT algorithm\nbut stop early, as soon as Xk is consistent with the data and obeys\n\u2225P\u2126(Xk \u2212B)\u22252\nF \u2264(1 + \u01eb) m\u03c32,\n(5.10)\nwhere \u01eb is a small parameter. Our reconstruction\n\u02c6\nM is the \ufb01rst Xk obeying (5.10). The results\nare shown in Table 2 (the quantities are averages of 5 runs). De\ufb01ne the noise ratio as\n\u2225P\u2126(Z)\u2225F /\u2225P\u2126(M)\u2225F ,\nand the relative error by (5.8). From Table 2, we see that the SVT algorithm works well as the\nrelative error between the recovered and the true data matrix is just about equal to the noise ratio.\nThe theory of low-rank matrix recovery from noisy data is nonexistent at the moment, and is\nobviously beyond the scope of this paper. Having said this, we would like to conclude this section\nwith an intuitive and nonrigorous discussion, which may explain why the observed recovery error\nis within the noise level. Suppose again that\n\u02c6\nM obeys (5.6), namely,\n\u2225P\u2126( \u02c6\nM \u2212M)\u22252\nF \u224dp\u2225\u02c6\nM \u2212M\u22252\nF .\n(5.11)\n22\nUnknown matrix M\nComputational results\nnoise ratio\nsize (n \u00d7 n)\nrank (r)\nm/dr\nm/n2\ntime(s)\n# iters\nrelative error\n10\n6\n0.12\n10.8\n51\n0.78 \u00d7 10\u22122\n10\u22122\n1, 000 \u00d7 1, 000\n50\n4\n0.39\n87.7\n48\n0.95 \u00d7 10\u22122\n100\n3\n0.57\n216\n50\n1.13 \u00d7 10\u22122\n10\n6\n0.12\n4.0\n19\n0.72 \u00d7 10\u22121\n10\u22121\n1, 000 \u00d7 1, 000\n50\n4\n0.39\n33.2\n17\n0.89 \u00d7 10\u22121\n100\n3\n0.57\n85.2\n17\n1.01 \u00d7 10\u22121\n10\n6\n0.12\n0.9\n3\n0.52\n1\n1, 000 \u00d7 1, 000\n50\n4\n0.39\n7.8\n3\n0.63\n100\n3\n0.57\n34.8\n3\n0.69\nTable 2: Simulation results for noisy data. The computational results are averaged over \ufb01ve\nruns.\nAs mentioned earlier, one condition for this to happen is that M and\n\u02c6\nM have low rank. This is\nthe reason why it is important to stop the algorithm early as we hope to obtain a solution which\nis both consistent with the data and has low rank (the limit of the SVT iterations, limk\u2192\u221eXk,\nwill not generally have low rank since there may be no low-rank matrix matching the noisy data).\nFrom\n\u2225P\u2126( \u02c6\nM \u2212M)\u2225F \u2264\u2225P\u2126( \u02c6\nM \u2212B)\u2225F + \u2225P\u2126(B \u2212M)\u2225F ,\nand the fact that both terms on the right-hand side are on the order of\n\u221a\nm\u03c32, we would have\np\u2225\u02c6\nM \u2212M\u22252\nF = O(m\u03c32) by (5.11). In particular, this would give that the relative reconstruction\nerror is on the order of the noise ratio since \u2225P\u2126(M)\u22252\nF \u224dp\u2225M\u22252\nF \u2014as observed experimentally.\n5.2.2\nLinear inequality constraints\nWe now examine the speed at which one can solve similar problems with linear inequality constraints\ninstead of linear equality constraints. We assume the model (5.9), where the matrix M of rank\nr is sampled as before, and solve the problem (3.8) by using (3.10). We formulate the inequality\nconstraints in (3.8) with Eij = \u03c3 so that one searches for a solution\n\u02c6\nM with minimum nuclear\nnorm among all those matrices whose sampled entries deviate from the observed ones by at most\nthe noise level \u03c3.2 In this experiment, we adjust \u03c3 to be one tenth of a typical absolute entry of\nM, i.e. \u03c3 = 0.1 P\nij\u2208\u2126|Mij|/m, and the noise ratio as de\ufb01ned earlier is 0.780. We set n = 1, 000,\nr = 10, and the number m of sampled entries is \ufb01ve times the number of degrees of freedom,\ni.e. m = 5dr. Just as before, we set \u03c4 = 5n, and choose a constant step size \u03b4 = 1.2p\u22121.\nThe results, reported in Figure 2, show that the algorithm behaves just as well with linear\ninequality constraints.\nTo make this point, we compare our results with those obtained from\nnoiseless data (same unknown matrix and sampled locations). In the noiseless case, it takes about\n150 iterations to reach the tolerance \u01eb = 10\u22124 whereas in the noisy case, convergence occurs in\nabout 200 iterations (Figure 2(a)). In addition, just as in the noiseless problem, the rank of the\niterates is nondecreasing and quickly reaches the true value r of the rank of the unknown matrix\n2This may not be conservative enough from a statistical viewpoint but this works well in this case, and our\nemphasis here is on computational rather than statistical issues.\n23\n0\n20\n40\n60\n80\n100\n120\n140\n160\n180\n200\n10\n\u22125\n10\n\u22124\n10\n\u22123\n10\n\u22122\n10\n\u22121\n10\n0\nRelative errors\n \n \nError from noisy data\nResidual error from noisy data\nError from noiseless data\nResidual error from noiseless data\n0\n20\n40\n60\n80\n100\n120\n140\n160\n180\n200\n0\n5\n10\n15\nRank\n \n \nNoisy data\nNoiseless data\n(a)\n(b)\n0\n20\n40\n60\n80\n100\n120\n140\n160\n180\n200\n0\n0.05\n0.1\n0.15\n0.2\n0.25\n0.3\n0.35\n0.4\nsvd time\n \n \nNoisy data\nNoiseless data\n(c)\nFigure 2: Computational results of the algorithm applied to noisy (linear inequality con-\nstraints as in (3.8)) and noiseless data (equality constraints). The blue (resp. red) color is\nused for the noisy (resp. noiseless) experiment.\n(a) Plot of the reconstruction errors from\nnoisy and noiseless data as a function of the iteration count. The thin line is the residual\nrelative error \u2225P\u2126(Xk \u2212M)\u2225F /\u2225P\u2126(M)\u2225F and the thick line is the overall relative error\n\u2225Xk \u2212M\u2225F /\u2225M\u2225F. (b) Rank of the iterates as a function of the iteration count. (c) Time it\ntakes to compute the singular value thresholding operation as a function of the iteration count.\nThe computer here is a single-core 3.00GHz Pentium 4 running Matlab 7.2.0.\nM we wish to recover (Figure 2(b)). As a consequence the SVT iterations take about the same\namount of time as in the noiseless case (Figure 2(c)) so that the total running time of the algorithm\ndoes not appear to be substantially di\ufb00erent from that in the noiseless case.\nWe close by pointing out that from a statistical point of view, the recovery of the matrix M\nfrom undersampled and noisy entries by the matrix equivalent of the Dantzig selector appears to\nbe accurate since the relative error obeys \u2225\u02c6\nM \u2212M\u2225F /\u2225M\u2225F = 0.0769 (recall that the noise ratio\nis about 0.08).\n6\nDiscussion\nThis paper introduced a novel algorithm, namely, the singular value thresholding algorithm for\nmatrix completion and related nuclear norm minimization problems. This algorithm is easy to\nimplement and surprisingly e\ufb00ective both in terms of computational cost and storage requirement\n24\nwhen the minimum nuclear-norm solution is also the lowest-rank solution. We would like to close\nthis paper by discussing a few open problems and research directions related to this work.\nOur algorithm exploits the fact that the sequence of iterates {Xk} have low rank when the\nminimum nuclear solution has low rank. An interesting question is whether one can prove (or\ndisprove) that in a majority of the cases, this is indeed the case.\nIt would be interesting to explore other ways of computing D\u03c4(Y )\u2014in words, the action of\nthe singular value shrinkage operator. Our approach uses the Lanczos bidiagonalization algorithm\nwith partial reorthogonalization which takes advantages of sparse inputs but other approaches are\npossible. We mention two of them.\n1. A series of papers have proposed the use of randomized procedures for the approximation\nof a matrix Y with a matrix Z of rank r [38, 41]. When this approximation consists of the\ntruncated SVD retaining the part of the expansion corresponding to singular values greater\nthan \u03c4, this can be used to evaluate D\u03c4(Y ). Some of these algorithms are e\ufb03cient when the\ninput Y is sparse [41], and it would be interesting to know whether these methods are fast\nand accurate enough to be used in the SVT iteration (2.7).\n2. A wide range of iterative methods for computing matrix functions of the general form f(Y )\nare available today, see [34] for a survey.\nA valuable research direction is to investigate\nwhether some of these iterative methods, or other to be developed, would provide powerful\nways for computing D\u03c4(Y ).\nIn practice, one would like to solve (2.8) for large values of \u03c4. However, a larger value of \u03c4\ngenerally means a slower rate of convergence. A good strategy might be to start with a value of\n\u03c4, which is large enough so that (2.8) admits a low-rank solution, and at the same time for which\nthe algorithm converges rapidly. One could then use a continuation method as in [50] to increase\nthe value of \u03c4 sequentially according to a schedule \u03c40, \u03c41, . . ., and use the solution to the previous\nproblem with \u03c4 = \u03c4i\u22121 as an initial guess for the solution to the current problem with \u03c4 = \u03c4i (warm\nstarting). We hope to report on this in a separate paper.\nAcknowledgments\nJ-F. C. is supported by the Wavelets and Information Processing Programme under a grant from DSTA,\nSingapore. E. C. is partially supported by the Waterman Award from the National Science Foundation\nand by an ONR grant N00014-08-1-0749. Z. S. is supported in part by Grant R-146-000-113-112 from the\nNational University of Singapore. E. C. would like to thank Benjamin Recht and Joel Tropp for fruitful\nconversations related to this project, and Stephen Becker for his help in preparing the computational results\nof Section 5.2.2.\nReferences\n[1] J. Abernethy, F. Bach, T. Evgeniou, and J.-P. Vert. Low-rank matrix factorization with attributes.\nTechnical Report N24/06/MM, Ecole des Mines de Paris, 2006.\n[2] ACM SIGKDD and Net\ufb02ix. Proceedings of KDD Cup and Workshop, 2007. Proceedings available online\nat http://www.cs.uic.edu/\u223cliub/KDD-cup-2007/proceedings.html.\n[3] Y. Amit, M. Fink, N. Srebro, and S. Ullman. Uncovering shared structures in multiclass classi\ufb01cation.\nIn Proceedings of the Twenty-fourth International Conference on Machine Learning, 2007.\n25\n[4] A. Argyriou, T. Evgeniou, and M. Pontil. Multi-task feature learning. In Neural Information Processing\nSystems, 2007.\n[5] J. Bect, L. Blanc-F\u00b4eraud, G. Aubert, and A. Chambolle, A \u21131 uni\ufb01ed variational framework for image\nrestoration, in Proc. Eighth Europ. Conf. Comput. Vision, 2004.\n[6] S. Boyd, and L. Vandenberghe. Convex Optimization. Cambridge University Press, 2004.\n[7] J.-F. Cai, R. Chan, L. Shen, and Z. Shen. Restoration of chopped and nodded images by framelets.\nSIAM J. Sci. Comput., 30(3):1205\u20131227, 2008.\n[8] J.-F. Cai, R. H. Chan, and Z. Shen.\nA framelet-based image inpainting algorithm.\nAppl. Comput.\nHarmon. Anal., 24(2):131\u2013149, 2008.\n[9] J.-F. Cai, S. Osher, and Z. Shen. Convergence of the Linearized Bregman Iteration for \u21131-norm Mini-\nmization, 2008. UCLA CAM Report (08-52).\n[10] J.-F. Cai, S. Osher, and Z. Shen. Linearized Bregman Iterations for Compressed Sensing, 2008. Math.\nComp., to appear; see also UCLA CAM Report (08-06).\n[11] J.-F. Cai, S. Osher, and Z. Shen. Linearized Bregman Iterations for Frame-Based Image Deblurring,\n2008. preprint.\n[12] E. J. Cand`es, and F. Guo. New multiscale transforms, minimum total variation synthesis: Applications\nto edge-preserving image reconstruction. Signal Processing, 82:1519\u20131543, 2002.\n[13] E. J. Cand`es and B. Recht. Exact Matrix Completion via Convex Optimization, 2008.\n[14] E. J. Cand`es and J. Romberg. Sparsity and incoherence in compressive sampling. Inverse Problems,\n23(3):969\u2013985, 2007.\n[15] E. J. Cand`es, J. Romberg, and T. Tao. Robust uncertainty principles: exact signal reconstruction from\nhighly incomplete frequency information. IEEE Trans. Inform. Theory, 52(2):489\u2013509, 2006.\n[16] E. J. Cand`es and T. Tao. Decoding by linear programming. IEEE Trans. Inform. Theory, 51(12):4203\u2013\n4215, 2005.\n[17] E. J. Cand`es and T. Tao. Near-optimal signal recovery from random projections: universal encoding\nstrategies? IEEE Trans. Inform. Theory, 52(12):5406\u20135425, 2006.\n[18] E. J. Cand`es and T. Tao. The Dantzig selector: statistical estimation when p is much larger than n.\nAnnals of Statistics 35:2313\u20132351, 2007.\n[19] A. Chai and Z. Shen. Deconvolution: A wavelet frame approach. Numer. Math., 106(4):529\u2013587, 2007.\n[20] R. H. Chan, T. F. Chan, L. Shen, and Z. Shen. Wavelet algorithms for high-resolution image recon-\nstruction. SIAM J. Sci. Comput., 24(4):1408\u20131432 (electronic), 2003.\n[21] P. Chen, and D. Suter. Recovering the missing components in a large noisy low-rank matrix: application\nto SFM source. IEEE Transactions on Pattern Analysis and Machine Intelligence, 26(8):1051-1063, 2004.\n[22] P. L. Combettes and V. R. Wajs. Signal recovery by proximal forward-backward splitting. Multiscale\nModel. Simul., 4(4):1168\u20131200 (electronic), 2005.\n[23] J. Darbon and S. Osher. Fast discrete optimization for sparse approximations and deconvolutions, 2007.\npreprint.\n[24] I. Daubechies, M. Defrise, and C. De Mol. An iterative thresholding algorithm for linear inverse problems\nwith a sparsity constraint. Comm. Pure Appl. Math., 57(11):1413\u20131457, 2004.\n[25] I. Daubechies, G. Teschke, and L. Vese. Iteratively solving linear inverse problems under general convex\nconstraints. Inverse Probl. Imaging, 1(1):29\u201346, 2007.\n26\n[26] D. L. Donoho. Compressed sensing. IEEE Trans. Inform. Theory, 52(4):1289\u20131306, 2006.\n[27] M. Elad, J.-L. Starck, P. Querre, and D. L. Donoho. Simultaneous cartoon and texture image inpainting\nusing morphological component analysis (MCA). Appl. Comput. Harmon. Anal., 19(3):340\u2013358, 2005.\n[28] M. J. Fadili, J.-L. Starck, and F. Murtagh. Inpainting and zooming using sparse representations. The\nComputer Journal, to appear.\n[29] M. Fazel. Matrix Rank Minimization with Applications. PhD thesis, Stanford University, 2002.\n[30] M. Fazel, H. Hindi, and S. Boyd, Log-det heuristic for matrix rank minimization with applications to\nHankel and Euclidean distance matrices. in Proc. Am. Control Conf., June 2003.\n[31] M. Figueiredo, and R. Nowak, An EM algorithm for wavelet-based image restoration. IEEE Transactions\non Image Processing, 12(8):906\u2013916, 2003.\n[32] T. Goldstein and S. Osher. The Split Bregman Algorithm for L1 Regularized Problems, 2008. UCLA\nCAM Reprots (08-29).\n[33] E. T. Hale, W. Yin, and Y. Zhang. Fixed-point continuation for l1-minimization: methodology and\nconvergence. 2008. preprint.\n[34] N. J. Higham. Functions of Matrices: Theory and Computation. Society for Industrial and Applied\nMathematics, Philadelphia, PA, USA, 2008.\n[35] J.-B. Hiriart-Urruty and C. Lemar\u00b4echal. Convex analysis and minimization algorithms. I, volume 305\nof Grundlehren der Mathematischen Wissenschaften [Fundamental Principles of Mathematical Sciences].\nSpringer-Verlag, Berlin, 1993. Fundamentals.\n[36] R. M. Larsen, PROPACK \u2013 Software for large and sparse SVD calculations, Available from http:\n//sun.stanford.edu/\u223crmunk/PROPACK/.\n[37] A. S. Lewis. The mathematics of eigenvalue optimization. Math. Program., 97(1-2, Ser. B):155\u2013176,\n2003. ISMP, 2003 (Copenhagen).\n[38] E. Liberty, F. Woolfe, P.-G. Martinsson, V. Rokhlin, and M. Tygert. Randomized algorithms for the\nlow-rank approximation of matrices. Proc. Natl. Acad. Sci. USA, 104(51): 20167\u201320172, 2007.\n[39] S. Lintner, and F. Malgouyres. Solving a variational image restoration model which involves \u2113\u221econ-\nstraints. Inverse Problems, 20:815\u2013831, 2004.\n[40] Z. Liu, and L. Vandenberghe. Interior-point method for nuclear norm approximation with application\nto system identi\ufb01cation. submitted to Mathematical Programming, 2008.\n[41] P.-G. Martinsson, V. Rokhlin, and M. Tygert.\nA randomized algorithm for the approximation of\nmatrices Department of Computer Science, Yale University, New Haven, CT, Technical Report 1361,\n2006.\n[42] M. Mesbahi and G. P. Papavassilopoulos. On the rank minimization problem over a positive semide\ufb01nite\nlinear matrix inequality. IEEE Transactions on Automatic Control, 42(2):239\u2013243, 1997.\n[43] S. Osher, M. Burger, D. Goldfarb, J. Xu, and W. Yin. An iterative regularization method for total\nvariation-based image restoration. Multiscale Model. Simul., 4(2):460\u2013489 (electronic), 2005.\n[44] S. Osher, Y. Mao, B. Dong, and W. Yin. Fast Linearized Bregman Iteration for Compressed Sensing\nand Sparse Denoising, 2008. UCLA CAM Reprots (08-37).\n[45] B. Recht, M. Fazel, and P. Parrilo. Guaranteed minimum rank solutions of matrix equations via nuclear\nnorm minimization. 2007. Submitted to SIAM Review.\n[46] J.-L. Starck, D. L. Donoho, and E. J. Cand`es,\nAstronomical image representation by the curvelet\ntransform. Astronom. and Astrophys., 398:785\u2013800, 2003.\n27\n[47] K. C. Toh, M. J. Todd, and R. H. T\u00a8ut\u00a8unc\u00a8u. SDPT3 \u2013 a MATLAB software package for semide\ufb01nite-\nquadratic-linear programming, Available from http://www.math.nus.edu.sg/\u223cmattohkc/sdpt3.html.\n[48] C. Tomasi and T. Kanade. Shape and motion from image streams under orthography: a factorization\nmethod. International Journal of Computer Vision, 9(2):137\u2013154, 1992.\n[49] G. A. Watson. Characterization of the subdi\ufb00erential of some matrix norms. Linear Algebra Appl.,\n170:33\u201345, 1992.\n[50] S. J. Wright, R. Nowak, and M. Figueiredo. Sparse reconstruction by separable approximation. Sub-\nmitted for publication, 2007.\n[51] W. Yin, S. Osher, D. Goldfarb, and J. Darbon. Bregman iterative algorithms for \u21131-minimization with\napplications to compressed sensing. SIAM J. Imaging Sci., 1(1):143\u2013168, 2008.\n28\n",
        "sentence": "",
        "context": "National University of Singapore. E. C. would like to thank Benjamin Recht and Joel Tropp for fruitful\nconversations related to this project, and Stephen Becker for his help in preparing the computational results\nof Section 5.2.2.\nReferences\napplications to compressed sensing. SIAM J. Imaging Sci., 1(1):143\u2013168, 2008.\n28\n6\n0.12\n4.0\n19\n0.72 \u00d7 10\u22121\n10\u22121\n1, 000 \u00d7 1, 000\n50\n4\n0.39\n33.2\n17\n0.89 \u00d7 10\u22121\n100\n3\n0.57\n85.2\n17\n1.01 \u00d7 10\u22121\n10\n6\n0.12\n0.9\n3\n0.52\n1\n1, 000 \u00d7 1, 000\n50\n4\n0.39\n7.8\n3\n0.63\n100\n3\n0.57\n34.8\n3\n0.69"
    },
    {
        "title": "Enhancing sparsity by reweighted l1 minimization",
        "author": [
            "E.J. Candes",
            "M.B. Wakin",
            "S.P. Boyd"
        ],
        "venue": "J. of Fourier Analysis and Applic.,",
        "citeRegEx": "Candes et al\\.,? \\Q2008\\E",
        "shortCiteRegEx": "Candes et al\\.",
        "year": 2008,
        "abstract": "It is now well understood that (1) it is possible to reconstruct sparse\nsignals exactly from what appear to be highly incomplete sets of linear\nmeasurements and (2) that this can be done by constrained L1 minimization. In\nthis paper, we study a novel method for sparse signal recovery that in many\nsituations outperforms L1 minimization in the sense that substantially fewer\nmeasurements are needed for exact recovery. The algorithm consists of solving a\nsequence of weighted L1-minimization problems where the weights used for the\nnext iteration are computed from the value of the current solution. We present\na series of experiments demonstrating the remarkable performance and broad\napplicability of this algorithm in the areas of sparse signal recovery,\nstatistical estimation, error correction and image processing. Interestingly,\nsuperior gains are also achieved when our method is applied to recover signals\nwith assumed near-sparsity in overcomplete representations--not by reweighting\nthe L1 norm of the coefficient sequence as is common, but by reweighting the L1\nnorm of the transformed object. An immediate consequence is the possibility of\nhighly efficient data acquisition protocols by improving on a technique known\nas compressed sensing.",
        "full_text": "arXiv:0711.1612v1  [stat.ME]  10 Nov 2007\nEnhancing Sparsity by Reweighted \u21131 Minimization\nEmmanuel J. Cand`es\u2020\nMichael B. Wakin\u266f\nStephen P. Boyd\u00a7\n\u2020 Applied and Computational Mathematics, Caltech, Pasadena, CA 91125\n\u266fElectrical Engineering & Computer Science, University of Michigan, Ann Arbor, MI, 48109\n\u00a7 Department of Electrical Engineering, Stanford University, Stanford, CA 94305\nOctober 2007\nAbstract\nIt is now well understood that (1) it is possible to reconstruct sparse signals exactly from\nwhat appear to be highly incomplete sets of linear measurements and (2) that this can be\ndone by constrained \u21131 minimization. In this paper, we study a novel method for sparse signal\nrecovery that in many situations outperforms \u21131 minimization in the sense that substantially\nfewer measurements are needed for exact recovery. The algorithm consists of solving a sequence\nof weighted \u21131-minimization problems where the weights used for the next iteration are computed\nfrom the value of the current solution. We present a series of experiments demonstrating the\nremarkable performance and broad applicability of this algorithm in the areas of sparse signal\nrecovery, statistical estimation, error correction and image processing. Interestingly, superior\ngains are also achieved when our method is applied to recover signals with assumed near-sparsity\nin overcomplete representations\u2014not by reweighting the \u21131 norm of the coe\ufb03cient sequence as is\ncommon, but by reweighting the \u21131 norm of the transformed object. An immediate consequence\nis the possibility of highly e\ufb03cient data acquisition protocols by improving on a technique known\nas compressed sensing.\nKeywords. \u21131-minimization, iterative reweighting, underdetermined systems of linear equa-\ntions, compressed sensing, the Dantzig selector, sparsity, FOCUSS.\n1\nIntroduction\nWhat makes some scienti\ufb01c or engineering problems at once interesting and challenging is that\noften, one has fewer equations than unknowns. When the equations are linear, one would like to\ndetermine an object x0 \u2208Rn from data y = \u03a6x0, where \u03a6 is an m \u00d7 n matrix with fewer rows than\ncolumns; i.e., m < n. The problem is of course that a system with fewer equations than unknowns\nusually has in\ufb01nitely many solutions and thus, it is apparently impossible to identify which of these\ncandidate solutions is indeed the \u201ccorrect\u201d one without some additional information.\nIn many instances, however, the object we wish to recover is known to be structured in the\nsense that it is sparse or compressible.\nThis means that the unknown object depends upon a\nsmaller number of unknown parameters. In a biological experiment, one could measure changes of\n1\nexpression in 30,000 genes and expect at most a couple hundred genes with a di\ufb00erent expression\nlevel. In signal processing, one could sample or sense signals which are known to be sparse (or\napproximately so) when expressed in the correct basis. This premise radically changes the problem,\nmaking the search for solutions feasible since the simplest solution now tends to be the right one.\nMathematically speaking and under sparsity assumptions, one would want to recover a sig-\nnal x0 \u2208Rn, e.g., the coe\ufb03cient sequence of the signal in the appropriate basis, by solving the\ncombinatorial optimization problem\n(P0)\nmin\nx\u2208Rn \u2225x\u2225\u21130\nsubject to\ny = \u03a6x,\n(1)\nwhere \u2225x\u2225\u21130 = |{i : xi \u0338= 0}|. This is a common sense approach which simply seeks the simplest\nexplanation \ufb01tting the data. In fact, this method can recover sparse solutions even in situations in\nwhich m \u226an. Suppose for example that all sets of m columns of \u03a6 are in general position. Then\nthe program (P0) perfectly recovers all sparse signals x0 obeying \u2225x0\u2225\u21130 \u2264m/2. This is of little\npractical use, however, since the optimization problem (1) is nonconvex and generally impossible\nto solve as its solution usually requires an intractable combinatorial search.\nA common alternative is to consider the convex problem\n(P1)\nmin\nx\u2208Rn \u2225x\u2225\u21131\nsubject to\ny = \u03a6x,\n(2)\nwhere \u2225x\u2225\u21131 = Pn\ni=1 |xi|. Unlike (P0), this problem is convex\u2014it can actually be recast as a linear\nprogram\u2014and is solved e\ufb03ciently [1]. The programs (P0) and (P1) di\ufb00er only in the choice of\nobjective function, with the latter using an \u21131 norm as a proxy for the literal \u21130 sparsity count. As\nsummarized below, a recent body of work has shown that perhaps surprisingly, there are conditions\nguaranteeing a formal equivalence between the combinatorial problem (P0) and its relaxation (P1).\nThe use of the \u21131 norm as a sparsity-promoting functional traces back several decades. A leading\nearly application was re\ufb02ection seismology, in which a sparse re\ufb02ection function (indicating mean-\ningful changes between subsurface layers) was sought from bandlimited data. In 1973, Claerbout\nand Muir [2] \ufb01rst proposed the use of \u21131 to deconvolve seismic traces. Over the next decade this\nidea was re\ufb01ned to better handle observation noise [3,4], and the sparsity-promoting nature of \u21131\nminimization was empirically con\ufb01rmed. Rigorous results began to appear in the late-1980\u2019s, with\nDonoho and Stark [5] and Donoho and Logan [6] quantifying the ability to recover sparse re\ufb02ec-\ntivity functions. The application areas for \u21131 minimization began to broaden in the mid-1990\u2019s, as\nthe LASSO algorithm [7] was proposed as a method in statistics for sparse model selection, Basis\nPursuit [8] was proposed in computational harmonic analysis for extracting a sparse signal repre-\nsentation from highly overcomplete dictionaries, and a related technique known as total variation\nminimization was proposed in image processing [9,10].\nSome examples of \u21131 type methods for sparse design in engineering include Vandenberghe et\nal. [11, 12] for designing sparse interconnect wiring, and Hassibi et al. [13] for designing sparse\ncontrol system feedback gains. In [14], Dahleh and Diaz-Bobillo solve controller synthesis problems\nwith an \u21131 criterion, and observe that the optimal closed-loop responses are sparse.\nLobo et\nal. used \u21131 techniques to \ufb01nd sparse trades in portfolio optimization with \ufb01xed transaction costs\nin [15]. In [16], Ghosh and Boyd used \u21131 methods to design well connected sparse graphs; in [17],\nSun et al. observe that optimizing the rates of a Markov process on a graph leads to sparsity.\nIn [1, \u00a76.5.4, \u00a711.4.1], Boyd and Vandenberghe describe several problems involving \u21131 methods for\nsparse solutions, including \ufb01nding small subsets of mutually infeasible inequalities, and points that\n2\nviolate few constraints. In a recent paper, Koh et al. used these ideas to carry out piecewise-linear\ntrend analysis [18].\nOver the last decade, the applications and understanding of \u21131 minimization have continued to\nincrease dramatically. Donoho and Huo [19] provided a more rigorous analysis of Basis Pursuit, and\nthis work was extended and re\ufb01ned in subsequent years, see [20\u201322]. Much of the recent focus on\n\u21131 minimization, however, has come in the emerging \ufb01eld of Compressive Sensing [23\u201325]. This is a\nsetting where one wishes to recover a signal x0 from a small number of compressive measurements\ny = \u03a6x0. It has been shown that \u21131 minimization allows recovery of sparse signals from remarkably\nfew measurements [26,27]: supposing \u03a6 is chosen randomly from a suitable distribution, then with\nvery high probability, all sparse signals x0 for which \u2225x0\u2225\u21130 \u2264m/\u03b1 with \u03b1 = O(log(n/m)) can be\nperfectly recovered by using (P1). Moreover, it has been established [27] that Compressive Sensing\nis robust in the sense that \u21131 minimization can deal very e\ufb00ectively (a) with only approximately\nsparse signals and (b) with measurement noise.\nThe implications of these facts are quite far-\nreaching, with potential applications in data compression [24,28], digital photography [29], medical\nimaging [23,30], error correction [31,32], analog-to-digital conversion [33], sensor networks [34,35],\nand so on. (We will touch on some more concrete examples in Section 3.)\nThe use of \u21131 regularization has become so widespread that it could arguably be considered the\n\u201cmodern least squares\u201d. This raises the question of whether we can improve upon \u21131 minimization?\nIt is natural to ask, for example, whether a di\ufb00erent (but perhaps again convex) alternative to \u21130\nminimization might also \ufb01nd the correct solution, but with a lower measurement requirement than\n\u21131 minimization.\nIn this paper, we consider one such alternative, which aims to help rectify a key di\ufb00erence\nbetween the \u21131 and \u21130 norms, namely, the dependence on magnitude: larger coe\ufb03cients are penalized\nmore heavily in the \u21131 norm than smaller coe\ufb03cients, unlike the more democratic penalization of the\n\u21130 norm. To address this imbalance, we propose a weighted formulation of \u21131 minimization designed\nto more democratically penalize nonzero coe\ufb03cients. In Section 2, we discuss an iterative algorithm\nfor constructing the appropriate weights, in which each iteration of the algorithm solves a convex\noptimization problem, whereas the overall algorithm does not. Instead, this iterative algorithm\nattempts to \ufb01nd a local minimum of a concave penalty function that more closely resembles the\n\u21130 norm. Finally, we would like to draw attention to the fact that each iteration of this algorithm\nsimply requires solving one \u21131 minimization problem, and so the method can be implemented readily\nusing existing software.\nIn Section 3, we present a series of experiments demonstrating the superior performance and\nbroad applicability of this algorithm, not only for recovery of sparse signals, but also pertaining\nto compressible signals, noisy measurements, error correction, and image processing. This section\ndoubles as a brief tour of the applications of Compressive Sensing. In Section 4, we demonstrate\nthe promise of this method for e\ufb03cient data acquisition. Finally, we conclude in Section 5 with a\n\ufb01nal discussion of related work and future directions.\n3\n2\nAn iterative algorithm for reweighted \u21131 minimization\n2.1\nWeighted \u21131 minimization\nConsider the \u201cweighted\u201d \u21131 minimization problem\n(WP1)\nmin\nx\u2208Rn\nX\ni=1\nwi|xi|\nsubject to\ny = \u03a6x,\n(3)\nwhere w1, w2, . . . , wn are positive weights. Just like its \u201cunweighted\u201d counterpart (P1), this convex\nproblem can be recast as a linear program.\nIn the sequel, it will be convenient to denote the\nobjective functional by \u2225Wx\u2225\u21131 where W is the diagonal matrix with w1, . . . , wn on the diagonal\nand zeros elsewhere.\nThe weighted \u21131 minimization (WP1) can be viewed as a relaxation of a weighted \u21130 minimization\nproblem\n(WP0)\nmin\nx\u2208Rn \u2225Wx\u2225\u21130\nsubject to\ny = \u03a6x.\n(4)\nWhenever the solution to (P0) is unique, it is also the unique solution to (WP0) provided that\nthe weights do not vanish. However, the corresponding \u21131 relaxations (P1) and (WP1) will have\ndi\ufb00erent solutions in general. Hence, one may think of the weights (wi) as free parameters in the\nconvex relaxation, whose values\u2014if set wisely\u2014could improve the signal reconstruction.\nThis raises the immediate question: what values for the weights will improve signal reconstruc-\ntion? One possible use for the weights could be to counteract the in\ufb02uence of the signal magnitude\non the \u21131 penalty function. Suppose, for example, that the weights were inversely proportional to\nthe true signal magnitude, i.e., that\nwi =\n(\n1\n|x0,i|,\nx0,i \u0338= 0,\n\u221e,\nx0,i = 0.\n(5)\nIf the true signal x0 is k-sparse, i.e., obeys \u2225x0\u2225\u21130 \u2264k, then (WP1) is guaranteed to \ufb01nd the correct\nsolution with this choice of weights, assuming only that m \u2265k and that just as before, the columns\nof \u03a6 are in general position.\nThe large (actually in\ufb01nite) entries in wi force the solution x to\nconcentrate on the indices where wi is small (actually \ufb01nite), and by construction these correspond\nprecisely to the indices where x0 is nonzero. It is of course impossible to construct the precise\nweights (5) without knowing the signal x0 itself, but this suggests more generally that large weights\ncould be used to discourage nonzero entries in the recovered signal, while small weights could be\nused to encourage nonzero entries.\nFor the sake of illustration, consider the simple 3-D example in Figure 1, where x0 = [0 1 0]T\nand\n\u03a6 =\n\u0014 2\n1\n1\n1\n1\n2\n\u0015\n.\nWe wish to recover x0 from y = \u03a6x0 = [1 1]T . Figure 1(a) shows the original signal x0, the set\nof points x \u2208R3 obeying \u03a6x = \u03a6x0 = y, and the \u21131 ball of radius 1 centered at the origin. The\ninterior of the \u21131 ball intersects the feasible set \u03a6x = y, and thus (P1) \ufb01nds an incorrect solution,\nnamely, x\u22c6= [1/3 0 1/3]T \u0338= x0 (see Figure 1(b)).\nConsider now a hypothetical weighting matrix W = diag([3 1 3]T ).\nFigure 1(c) shows the\n\u201cweighted \u21131 ball\u201d of radius \u2225Wx\u2225\u21131 = 1 centered at the origin. Compared to the unweighted \u21131\n4\n(a)\n(b)\n(c)\nFigure 1: Weighting \u21131 minimization to improve sparse signal recovery. (a) Sparse signal\nx0, feasible set \u03a6x = y, and \u21131 ball of radius \u2225x0\u2225\u21131. (b) There exists an x \u0338= x0 for which\n\u2225x\u2225\u21131 < \u2225x0\u2225\u21131. (c) Weighted \u21131 ball. There exists no x \u0338= x0 for which \u2225Wx\u2225\u21131 \u2264\u2225Wx0\u2225\u21131.\nball (Figure 1(a)), this ball has been sharply pinched at x0. As a result, the interior of the weighted\n\u21131 ball does not intersect the feasible set, and consequently, (WP1) will \ufb01nd the correct solution\nx\u22c6= x0. Indeed, it is not di\ufb03cult to show that the same statements would hold true for any\npositive weighting matrix for which w2 < (w1 + w3)/3. Hence there is a range of valid weights for\nwhich (WP1) will \ufb01nd the correct solution. As a rough rule of thumb, the weights should relate\ninversely to the true signal magnitudes.\n2.2\nAn iterative algorithm\nThe question remains of how a valid set of weights may be obtained without \ufb01rst knowing x0. As\nFigure 1 shows, there may exist a range of favorable weighting matrices W for each \ufb01xed x0, which\nsuggests the possibility of constructing a favorable set of weights based solely on an approximation\nx to x0 or on other side information about the vector magnitudes.\nWe propose a simple iterative algorithm that alternates between estimating x0 and rede\ufb01ning\nthe weights. The algorithm is as follows:\n1. Set the iteration count \u2113to zero and w(0)\ni\n= 1, i = 1, . . . , n.\n2. Solve the weighted \u21131 minimization problem\nx(\u2113) = arg min \u2225W (\u2113)x\u2225\u21131\nsubject to\ny = \u03a6x.\n3. Update the weights: for each i = 1, . . . , n,\nw(\u2113+1)\ni\n=\n1\n|x(\u2113)\ni | + \u01eb\n.\n(6)\n4. Terminate on convergence or when \u2113attains a speci\ufb01ed maximum number of iterations \u2113max.\nOtherwise, increment \u2113and go to step 2.\n5\nWe introduce the parameter \u01eb > 0 in step 3 in order to provide stability and to ensure that a\nzero-valued component in x(\u2113) does not strictly prohibit a nonzero estimate at the next step. As\nempirically demonstrated in Section 3, \u01eb should be set slightly smaller than the expected nonzero\nmagnitudes of x0. In general, the recovery process tends to be reasonably robust to the choice of \u01eb.\nUsing an iterative algorithm to construct the weights (wi) tends to allow for successively better\nestimation of the nonzero coe\ufb03cient locations. Even though the early iterations may \ufb01nd inaccurate\nsignal estimates, the largest signal coe\ufb03cients are most likely to be identi\ufb01ed as nonzero. Once\nthese locations are identi\ufb01ed, their in\ufb02uence is downweighted in order to allow more sensitivity for\nidentifying the remaining small but nonzero signal coe\ufb03cients.\nFigure 2 illustrates this dynamic by means of an example in sparse signal recovery. Figure 2(a)\nshows the original signal of length n = 512, which contains 130 nonzero spikes. We collect m = 256\nmeasurements where the matrix \u03a6 has independent standard normal entries.\nWe set \u01eb = 0.1\nand \u2113max = 2. Figures 2(b)-(d) show scatter plots, coe\ufb03cient-by-coe\ufb03cient, of the original signal\ncoe\ufb03cient x0 versus its reconstruction x(\u2113).\nIn the unweighted iteration (Figure 2(b)), we see\nthat all large coe\ufb03cients in x0 are properly identi\ufb01ed as nonzero (with the correct sign), and that\n\u2225x0 \u2212x(0)\u2225\u2113\u221e= 0.4857. In this \ufb01rst iteration, \u2225x(0)\u2225\u21130 = 256 = m, with 15 nonzero spikes in\nx0 reconstructed as zeros and 141 zeros in x0 reconstructed as nonzeros. These numbers improve\nafter one reweighted iteration (Figure 2(c)) with now \u2225x \u2212x(1)\u2225\u2113\u221e= 0.2407, \u2225x(1)\u2225\u21130 = 256 =\nm, 6 nonzero spikes in x0 reconstructed as zeros and 132 zeros in x0 reconstructed as nonzeros.\nThis improved signal estimate is then su\ufb03cient to allow perfect recovery in the second reweighted\niteration (Figure 2(d)).\n2.3\nAnalytical justi\ufb01cation\nThe iterative reweighted algorithm falls in the general class of MM algorithms, see [36] and ref-\nerences therein. In a nutshell, MM algorithms are more general than EM algorithms, and work\nby iteratively minimizing a simple surrogate function majorizing a given objective function. To\nestablish this connection, consider the problem\nmin\nx\u2208Rn\nn\nX\ni=1\nlog(|xi| + \u01eb)\nsubject to\ny = \u03a6x,\n(7)\nwhich is equivalent to\nmin\nx,u\u2208Rn\nn\nX\ni=1\nlog(ui + \u01eb)\nsubject to\ny = \u03a6x,\n|xi| \u2264ui, i = 1, . . . , n.\n(8)\nThe equivalence means that if x\u22c6is a solution to (7), then (x\u22c6, |x\u22c6|) is a solution to (8). And\nconversely, if (x\u22c6, u\u22c6) is a solution to (8), then x\u22c6is a solution to (7).\nProblem (8) is of the general form\nmin\nv\ng(v)\nsubject to\nv \u2208C,\nwhere C is a convex set. In (8), the function g is concave and, therefore, below its tangent. Thus,\none can improve on a guess v at the solution by minimizing a linearization of g around v. This\nsimple observation yields the following MM algorithm: starting with v(0) \u2208C, inductively de\ufb01ne\nv(\u2113+1) = arg min g(v(\u2113)) + \u2207g(v(\u2113)) \u00b7 (v \u2212v(\u2113))\nsubject to\nv \u2208C.\n6\n0\n100\n200\n300\n400\n500\n\u22122\n\u22121\n0\n1\n2\ni\nx0,i\n\u22122\n\u22121\n0\n1\n2\n\u22122\n\u22121\n0\n1\n2\nx0\nx(0)\n(a)\n(b)\n\u22122\n\u22121\n0\n1\n2\n\u22122\n\u22121\n0\n1\n2\nx0\nx(1)\n\u22122\n\u22121\n0\n1\n2\n\u22122\n\u22121\n0\n1\n2\nx0\nx(2)\n(c)\n(d)\nFigure 2: Sparse signal recovery through reweighted \u21131 iterations. (a) Original length n =\n512 signal x0 with 130 spikes.\n(b) Scatter plot, coe\ufb03cient-by-coe\ufb03cient, of x0 versus its\nreconstruction x(0) using unweighted \u21131 minimization. (c) Reconstruction x(1) after the \ufb01rst\nreweighted iteration. (d) Reconstruction x(2) after the second reweighted iteration.\n7\nEach iterate is now the solution to a convex optimization problem. In the case (8) of interest, this\ngives\n(x(\u2113+1), u(\u2113+1)) = arg min\nn\nX\ni=1\nui\nu(\u2113)\ni\n+ \u01eb\nsubject to\ny = \u03a6x,\n|xi| \u2264ui, i = 1, . . . , n,\nwhich is of course equivalent to\nx(\u2113+1) = arg min\nn\nX\ni=1\n|xi|\n|x(\u2113)\ni | + \u01eb\nsubject to\ny = \u03a6x.\nOne now recognizes our iterative algorithm.\nIn two papers [37,38], Fazel et al. have considered the same reweighted \u21131 minimization algorithm\nas in Section 2.2, \ufb01rst as a heuristic algorithm for applications in portfolio optimization [37], and\nsecond as a special case of an iterative algorithm for minimizing the rank of a matrix subject to\nconvex constraints [38]. Using general theory, they argue that Pn\ni=1 log(|x(\u2113)\ni | + \u01eb) converges to\na local minimum of g(x) = Pn\ni=1 log(|xi| + \u01eb) (note that this not saying that the sequence (x(\u2113))\nconverges). Because the log-sum penalty function is concave, one cannot expect this algorithm to\nalways \ufb01nd a global minimum. As a result, it is important to choose a suitable starting point for\nthe algorithm. Like [38], we have suggested initializing with the solution to (P1), the unweighted\n\u21131 minimization. In practice we have found this to be an e\ufb00ective strategy. Further connections\nbetween our work and FOCUSS strategies are discussed at the end of the paper.\nThe connection with the log-sum penalty function provides a basis for understanding why\nreweighted \u21131 minimization can improve the recovery of sparse signals.\nIn particular, the log-\nsum penalty function has the potential to be much more sparsity-encouraging than the \u21131 norm.\nConsider, for example, three potential penalty functions for scalar magnitudes t:\nf0(t) = 1{t\u0338=0},\nf1(t) = |t|,\nand\nflog,\u01eb(t) \u221dlog(1 + |t|/\u01eb),\nwhere the constant of proportionality is set such that flog,\u01eb(1) = 1 = f0(1) = f1(1), see Figure 3.\nThe \ufb01rst (\u21130-like) penalty function f0 has in\ufb01nite slope at t = 0, while its convex (\u21131-like) relaxation\nf1 has unit slope at the origin. The concave penalty function flog,\u01eb, however, has slope at the origin\nthat grows roughly as 1/\u01eb when \u01eb \u21920. Like the \u21130 norm, this allows a relatively large penalty to\nbe placed on small nonzero coe\ufb03cients and more strongly encourages them to be set to zero. In\nfact, flog,\u01eb(t) tends to f0(t) as \u01eb \u21920. Following this argument, it would appear that \u01eb should be set\narbitrarily small, to most closely make the log-sum penalty resemble the \u21130 norm. Unfortunately,\nas \u01eb \u21920, it becomes more likely that the iterative reweighted \u21131 algorithm will get stuck in an\nundesirable local minimum. As shown in Section 3, a cautious choice of \u01eb (slightly smaller than\nthe expected nonzero magnitudes of x) provides the stability necessary to correct for inaccurate\ncoe\ufb03cient estimates while still improving upon the unweighted \u21131 algorithm for sparse recovery.\n2.4\nVariations\nOne could imagine a variety of possible reweighting functions in place of (6). We have experimented\nwith alternatives, including a binary (large/small) setting of wi depending on the current guess.\nThough such alternatives occasionally provide superior reconstruction of sparse signals, we have\nfound the rule (6) to perform well in a variety of experiments and applications.\n8\nFigure 3: At the origin, the canonical \u21130 sparsity count f0(t) is better approximated by the\nlog-sum penalty function flog,\u01eb(t) than by the traditional convex \u21131 relaxation f1(t).\nAlternatively, one can attempt to minimize a concave function other than the log-sum penalty.\nFor instance, we may consider\ng(x) =\nn\nX\ni=1\natan(|xi|/\u01eb)\nin lieu of Pn\ni=1 log(1 + |xi|/\u01eb). The function atan is bounded above and \u21130-like. If x is the current\nguess, this proposal updates the sequence of weights as wi = 1/(x2\ni + \u01eb2). There are of course\nmany possibilities of this nature and they tend to work well (sometimes better than the log-sum\npenalty). Because of space limitations, however, we will limit ourselves to empirical studies of the\nperformance of the log-sum penalty, and leave the choice of other penalties for further research.\n2.5\nHistorical progression\nThe development of the reweighted \u21131 algorithm has an interesting historical parallel with the use\nof Iteratively Reweighted Least Squares (IRLS) for robust statistical estimation [39\u201341]. Consider\na regression problem Ax = b where the observation matrix A is overdetermined. It was noticed that\nstandard least squares regression, in which one minimizes \u2225r\u22252 where r = Ax \u2212b is the residual\nvector, lacked robustness vis a vis outliers.\nTo defend against this, IRLS was proposed as an\niterative method to minimize instead the objective\nmin\nx\nX\ni\n\u03c1(ri(x)),\nwhere \u03c1(\u00b7) is a penalty function such as the \u21131 norm [39,42]. This minimization can be accomplished\nby solving a sequence of weighted least-squares problems where the weights {wi} depend on the\nprevious residual wi = \u03c1\u2032(ri)/ri.\nFor typical choices of \u03c1 this dependence is in fact inversely\nproportional\u2014large residuals will be penalized less in the subsequent iteration and vice versa\u2014\nas is the case with our reweighted \u21131 algorithm. Interestingly, just as IRLS involved iteratively\nreweighting the \u21132-norm in order to better approximate an \u21131-like criterion, our algorithm involves\niteratively reweighting the \u21131-norm in order to better approximate an \u21130-like criterion.\n3\nNumerical experiments\nWe present a series of experiments demonstrating the bene\ufb01ts of reweighting the \u21131 penalty. We\nwill see that the requisite number of measurements to recover or approximate a signal is typi-\n9\ncally reduced, in some cases by a substantial amount. We also demonstrate that the reweighting\napproach is robust and broadly applicable, providing examples of sparse and compressible signal\nrecovery, noise-aware recovery, model selection, error correction, and 2-dimensional total-variation\nminimization. Meanwhile, we address important issues such as how one can choose \u01eb wisely and\nhow robust is the algorithm to this choice, and how many reweighting iterations are needed for\nconvergence.\n3.1\nSparse signal recovery\nThe purpose of this \ufb01rst experiment is to demonstrate (1) that reweighting reduces the necessary\nsampling rate for sparse signals (2) that this recovery is robust with respect to the choice of \u01eb\nand (3) that few reweighting iterations are typically needed in practice. The setup for each trial\nis as follows. We select a sparse signal x0 of length n = 256 with \u2225x0\u2225\u21130 = k. The k nonzero\nspike positions are chosen randomly, and the nonzero values are chosen randomly from a zero-mean\nunit-variance Gaussian distribution. We set m = 100 and sample a random m \u00d7 n matrix \u03a6 with\ni.i.d. Gaussian entries, giving the data y = \u03a6x0. To recover the signal, we run several reweighting\niterations with equality constraints (see Section 2.2). The parameter \u01eb remains \ufb01xed during these\niterations. Finally, we run 500 trials for various \ufb01xed combinations of k and \u01eb.\nFigure 4(a) compares the performance of unweighted \u21131 to reweighted \u21131 for various values of\nthe parameter \u01eb. The solid line plots the probability of perfect signal recovery (declared when\n\u2225x0 \u2212x\u2225\u2113\u221e\u226410\u22123) for the unweighted \u21131 algorithm as a function of the sparsity level k. The\ndashed curves represent the performance after 4 reweighted iterations for several di\ufb00erent values of\nthe parameter \u01eb. We see a marked improvement over the unweighted \u21131 algorithm; with the proper\nchoice of \u01eb, the requisite oversampling factor m/k for perfect signal recovery has dropped from\napproximately 100/25 = 4 to approximately 100/33 \u22483. This improvement is also fairly robust\nwith respect to the choice of \u01eb, with a suitable rule being about 10% of the standard deviation of\nthe nonzero signal coe\ufb03cients.\nFigure 4(b) shows the performance, with a \ufb01xed value of \u01eb = 0.1, of the reweighting algorithm\nfor various numbers of reweighted iterations. We see that much of the bene\ufb01t comes from the \ufb01rst\nfew reweighting iterations, and so the added computational cost for improved signal recovery is\nquite moderate.\n3.2\nSparse and compressible signal recovery with adaptive choice of \u01eb\nWe would like to con\ufb01rm the bene\ufb01ts of reweighted \u21131 minimization for compressible signal recovery\nand consider the situation when the parameter \u01eb is not provided in advance and must be estimated\nduring reconstruction. We propose an experiment in which each trial is designed as follows. We\nsample a signal of length n = 256 from one of three types of distribution: (1) k-sparse with\ni.i.d. Gaussian entries, (2) k-sparse with i.i.d. symmetric Bernoulli \u00b11 entries, or (3) compressible,\nconstructed by randomly permuting the sequence {i\u22121/p}n\ni=1 for a \ufb01xed p, applying random sign\n\ufb02ips, and normalizing so that \u2225x0\u2225\u2113\u221e= 1. We set m = 128 and sample a random m \u00d7 n matrix\n\u03a6 with i.i.d. Gaussian entries. To recover the signal, we again solve a reweighted \u21131 minimization\nwith equality constraints y = \u03a6x0 = \u03a6x. In this case, however, we adapt \u01eb at each iteration as a\nfunction of the current guess x(\u2113); step 3 of the algorithm is modi\ufb01ed as follows:\n10\n10\n20\n30\n40\n50\n60\n0\n0.2\n0.4\n0.6\n0.8\n1\nSparsity k\nPr(recovery)\n4 Reweighting Iterations\n \n \nUnwght. L1\n\u03b5 = 0.001\n\u03b5 = 0.01\n\u03b5 = 0.1\n\u03b5 = 1\n\u03b5 = 10\n10\n20\n30\n40\n50\n60\n0\n0.2\n0.4\n0.6\n0.8\n1\nSparsity k\nPr(recovery)\nReweighting, \u03b5 = 0.1\n \n \nUnwght. L1\n1 iterations\n2 iterations\n4 iterations\n8 iterations\n(a)\n(b)\nFigure 4: Sparse signal recovery from m = 100 random measurements of a length n = 256\nsignal. The probability of successful recovery depends on the sparsity level k. The dashed\ncurves represent a reweighted \u21131 algorithm that outperforms the traditional unweighted \u21131\napproach (solid curve). (a) Performance after 4 reweighting iterations as a function of \u01eb. (b)\nPerformance with \ufb01xed \u01eb = 0.1 as a function of the number of reweighting iterations.\n3. Let (|x|(i)) denote a reordering of (|xi|) in decreasing order of magnitude. Set\n\u01eb = max\nn\n|x(\u2113)|(i0), 10\u22123o\n,\nwhere i0 = m/[4 log(n/m)]. De\ufb01ne w(\u2113+1) as in (6).\nOur motivation for choosing this value for \u01eb is based on the anticipated accuracy of \u21131 minimization\nfor arbitrary signal recovery. In general, the reconstruction quality a\ufb00orded by \u21131 minimization is\ncomparable (approximately) to the best i0-term approximation to x0, and so we expect approxi-\nmately this many signal components to be approximately correct. Choosing the smallest of these\ngives us a rule of thumb for choosing \u01eb.\nWe run 100 trials of the above experiment for each signal type. The results for the k-sparse\nexperiments are shown in Figure 5(a). The solid black line indicates the performance of unweighted\n\u21131 recovery (success is declared when \u2225x0 \u2212x\u2225\u2113\u221e\u226410\u22123). This curve is the same for both the\nGaussian and Bernoulli coe\ufb03cients, as the success or failure of unweighted \u21131 minimization depends\nonly on the support and sign pattern of the original sparse signal. The dashed curves indicate the\nperformance of reweighted \u21131 minimization for Gaussian coe\ufb03cients (blue curve) and Bernoulli\ncoe\ufb03cients (red curve) with \u2113max = 4. We see a substantial improvement for recovering sparse\nsignals with Gaussian coe\ufb03cients, yet we see only very slight improvement for recovering sparse\nsignals with Bernoulli coe\ufb03cients. This discrepancy likely occurs because the decay in the sparse\nGaussian coe\ufb03cients allows large coe\ufb03cients to be easily identi\ufb01ed and signi\ufb01cantly downweighted\nearly in the reweighting algorithm. With Bernoulli coe\ufb03cients there is no such \u201clow-hanging fruit\u201d.\nThe results for compressible signals are shown in Figure 5(b),(c). Each plot represents a his-\ntogram, over 100 trials, of the \u21132 reconstruction error improvement a\ufb00orded by reweighting, namely,\n\u2225x0\u2212x(4)\u2225\u21132/\u2225x0\u2212x(0)\u2225\u21132. We see the greatest improvements for smaller p corresponding to sparser\n11\n20\n40\n60\n80\n100\n0\n0.2\n0.4\n0.6\n0.8\n1\nSparsity k\nPr(recovery)\nSparse signal recovery\n \n \nUnw. L1\nGaus. RW\nBern. RW\n0\n0.5\n1\n1.5\nCompressible signal, p=0.4\n0.8\n1\n1.2\n1.4\nCompressible signal, p=0.7\n(a)\n(b)\n(c)\nFigure 5: (a) Improvements in sparse signal recovery from reweighted \u21131 minimization when\ncompared to unweighted \u21131 minimization (solid black curve). The dashed blue curve corre-\nsponds to sparse signals with Gaussian coe\ufb03cients; the dashed red curve corresponds to sparse\nsignals with Bernoulli coe\ufb03cients. (b),(c) Improvements in compressible signal recovery from\nreweighted \u21131 minimization when compared to unweighted \u21131 minimization; signal coe\ufb03cients\ndecay as n\u22121/p with (b) p = 0.4 and (c) p = 0.7. Histograms indicate the \u21132 reconstruction\nerror improvements a\ufb00orded by the reweighted algorithm.\nsignals, with reductions in \u21132 reconstruction error up to 50% or more. As p \u21921, the improvements\ndiminish.\n3.3\nRecovery from noisy measurements\nReweighting can be applied to a noise-aware version of \u21131 minimization, further improving the\nrecovery of signals from noisy data. We observe y = \u03a6x0 +z, where z is a noise term which is either\nstochastic or deterministic. To recover x0, we adapt quadratically-constrained \u21131 minimization [7,\n27], and modify step 2 of the reweighted \u21131 algorithm with equality constraints (see Section 2.2) as\nx(\u2113) = arg min \u2225W (\u2113)x\u2225\u21131\nsubject to\n\u2225y \u2212\u03a6x\u2225\u21132 \u2264\u03b4.\n(9)\nThe parameter \u03b4 is adjusted so that the true vector x0 be feasible (resp. feasible with high proba-\nbility) for (9) in the case where z is deterministic (resp. stochastic).\nTo demonstrate how this proposal improves on plain \u21131 minimization, we sample a vector of\nlength n = 256 from one of three types of distribution: (1) k-sparse with k = 38 and i.i.d. Gaussian\nentries, (2) k-sparse with k = 38 and i.i.d. symmetric Bernoulli \u00b11 entries, or (3) compressible,\nconstructed by randomly permuting the sequence {i\u22121/p}n\ni=1 for a \ufb01xed p, applying random sign\n\ufb02ips, and normalizing so that \u2225x0\u2225\u2113\u221e= 1. The matrix \u03a6 is 128 \u00d7 256 with i.i.d. Gaussian entries\nwhose columns are subsequently normalized, and the noise vector z is drawn from an i.i.d. Gaussian\nzero-mean distribution properly rescaled so that \u2225z\u2225\u21132 = \u03b2\u2225\u03a6x\u2225\u21132 with \u03b2 = 0.2; i.e., z = \u03c3z0 where\nz0 is standard white noise and \u03c3 = \u03b2\u2225\u03a6x\u2225\u21132/\u2225z0\u2225\u21132. The parameter \u03b4 is set to \u03b42 = \u03c32(m + 2\n\u221a\n2m)\nas this provides a likely upper bound on \u2225z\u2225\u21132. We set \u01eb to be the empirical maximum value of\n\u2225\u03a6\u2217\u03be\u2225\u2113\u221eover several realizations of a random vector \u03be \u223cN(0, \u03c32Im). (This gives a rough estimate\nfor the noise amplitude in the signal domain, and hence, a baseline above which signi\ufb01cant signal\ncomponents could be identi\ufb01ed.)\nWe run 100 trials for each signal type. Figure 6 shows histograms of the \u21132 reconstruction error\nimprovement a\ufb00orded by 9 iterations, i.e., each histogram documents \u2225x0 \u2212x(9)\u2225\u21132/\u2225x0 \u2212x(0)\u2225\u21132\n12\n0.4\n0.6\n0.8\n1\nGaussian coeffs, k = 38\n0\n0.5\n1\n1.5\nBernoulli coeffs, k = 38\n0.6\n0.8\n1\nCompressible signal, p=0.7\n(a)\n(b)\n(c)\nFigure 6:\nSparse and compressible signal reconstruction from noisy measurements.\nHis-\ntograms indicate the \u21132 reconstruction error improvements a\ufb00orded by the reweighted\nquadratically-constrained \u21131 minimization for various signal types.\nover 100 trials.\nWe see in these experiments that the reweighted quadratically-constrained \u21131\nminimization typically o\ufb00ers improvements \u2225x0 \u2212x(9)\u2225\u21132/\u2225x0 \u2212x(0)\u2225\u21132 in the range 0.5\u22121 in many\nexamples. The results for sparse Gaussian spikes are slightly better than for sparse Bernoulli spikes,\nthough both are generally favorable. Similar behavior holds for compressible signals, and we have\nobserved that smaller values of p (sparser signals) allow the most improvement.\n3.4\nStatistical estimation\nReweighting also enhances statistical estimation as well. Suppose we observe y = \u03a6x0 + z, where\n\u03a6 is m \u00d7 n with m \u2264n, and z is a noise vector z \u223cN(0, \u03c32Im) drawn from an i.i.d. Gaussian\nzero-mean distribution, say. To estimate x0, we adapt the Dantzig selector [43] and modify step 2\nof the reweighted \u21131 algorithm as\nx(\u2113) = arg min \u2225W (\u2113)x\u2225\u21131\nsubject to\n\u2225\u03a6\u2217(y \u2212\u03a6x)\u2225\u2113\u221e\u2264\u03b4.\n(10)\nAgain \u03b4 is a parameter making sure that the true unknown vector is feasible with high probability.\nTo judge this proposal, we consider a sequence of experiments in which x0 is of length n = 256\nwith k = 8 nonzero entries in random positions. The nonzero entries of x0 have i.i.d. entries ac-\ncording to the model xi = si(1+|ai|) where the sign si = \u00b11 with probability 1/2 and ai \u223cN(0, 1).\nThe matrix \u03a6 is 72 \u00d7 256 with i.i.d. Gaussian entries whose columns are subsequently normalized\njust as before. The noise vector (zi) has i.i.d. N(0, \u03c32) components with \u03c3 = 1/3\np\nk/m \u22480.11.\nThe parameter \u03b4 is set to be the empirical maximum value of \u2225\u03a6\u2217z\u2225\u2113\u221eover several realizations of\na random vector z \u223cN(0, \u03c32Im). We set \u01eb = 0.1.\nAfter each iteration of the reweighted Dantzig selector, we also re\ufb01ne our estimate x(\u2113) using\nthe Gauss-Dantzig technique to correct for a systematic bias [43]. Let I = {i : |x(\u2113)\ni | > \u03b1 \u00b7 \u03c3} with\n\u03b1 = 1/4. Then one substitutes x(\u2113) with the least squares estimate which solves\nmin\nx\u2208Rn \u2225y \u2212\u03a6x\u2225\u21132\nsubject to\nxi = 0, i /\u2208I;\nthat is, by regressing y onto the subset of columns indexed by I.\nWe \ufb01rst report on one trial with \u2113max = 4. Figure 7(a) shows the original signal x0 along with the\nrecovery x(0) using the \ufb01rst (unweighted) Dantzig selector iteration; the error is \u2225x0\u2212x(0)\u2225\u21132 = 1.46.\n13\nUnweighted\nReweighted\nGauss-Dantzig\nGauss-Dantzig\nMedian error ratio \u03c12\n2.43\n5.63\nMean error ratio \u03c12\n6.12\n1.21\nAvg. false positives\n3.25\n0.50\nAvg. correct detections\n7.86\n7.80\nTable 1: Model selection results for unweighted and reweighted versions of the Gauss-Dantzig\nestimator. In each of 5000 trials the true sparse model contains k = 8 nonzero entries.\nFigure 7(b) shows the Dantzig selector recovery after 4 iterations; the error has decreased to\n\u2225x0 \u2212x(4)\u2225\u21132 = 1.25. Figure 7(c) shows the Gauss-Dantzig estimate x(0) obtained from the \ufb01rst\n(unweighted) Dantzig selector iteration; this decreases the error to \u2225x0 \u2212x(0)\u2225\u21132 = 0.57.\nThe\nestimator correctly includes all 8 positions at which x0 is nonzero, but also incorrectly includes 4\npositions at which x0 should be zero. In Figure 7(d) we see, however, that all of these mistakes\nare recti\ufb01ed in the Gauss-Dantzig estimate x(4) obtained from the reweighted Dantzig selector; the\ntotal error also decreases to \u2225x0 \u2212x(4)\u2225\u21132 = 0.29.\nWe repeat the above experiment across 5000 trials. Figure 8 shows a histogram of the ratio \u03c12\nbetween the squared error loss of some estimate x and the ideal squared error\n\u03c12 :=\nPn\ni=1(xi \u2212x0,i)2\nPn\ni=1 min(x2\n0,i, \u03c32)\nfor both the unweighted and reweighted Gauss-Dantzig estimators. (The results are also summa-\nrized in Table 1.) For an interpretation of the denominator, the ideal squared error P min(x2\n0,i, \u03c32)\nis roughly the mean-squared error one could achieve if one had available an oracle supplying perfect\ninformation about which coordinates of x0 are nonzero, and which are actually worth estimating.\nWe see again a signi\ufb01cant reduction in reconstruction error; the median value of \u03c12 decreases from\n2.43 to 1.21. As pointed out, a primary reason for this improvement comes from a more accurate\nidenti\ufb01cation of signi\ufb01cant coe\ufb03cients: on average the unweighted Gauss-Dantzig estimator in-\ncludes 3.2 \u201cfalse positives,\u201d while the reweighted Gauss-Dantzig estimator includes only 0.5. Both\nalgorithms correctly include all 8 nonzero positions in a large majority of trials.\n14\n0\n100\n200\n300\n\u22123\n\u22122\n\u22121\n0\n1\n2\n3\nUnweighted Dantzig Selector\n \n \ntrue value\nestimate\n0\n100\n200\n300\n\u22123\n\u22122\n\u22121\n0\n1\n2\n3\nReweighted Dantzig Selector\n \n \ntrue value\nestimate\n(a)\n(b)\n0\n100\n200\n300\n\u22123\n\u22122\n\u22121\n0\n1\n2\n3\nUnweighted Gauss\u2212Dantzig\n \n \ntrue value\nestimate\n0\n100\n200\n300\n\u22123\n\u22122\n\u22121\n0\n1\n2\n3\nReweighted Gauss\u2212Dantzig\n \n \ntrue value\nestimate\n(c)\n(d)\nFigure 7: Reweighting the Dantzig selector. Blue asterisks indicate the original signal x0;\nred circles indicate the recovered estimate. (a) Unweighted Dantzig selector. (b) Reweighted\nDantzig selector.\n(c) Unweighted Gauss-Dantzig estimate.\n(d) Reweighted Gauss-Dantzig\nestimate.\n15\n0\n10\n20\n30\n0\n200\n400\n600\n800\n1000\n0\n10\n20\n30\n0\n200\n400\n600\n800\n1000\n(a)\n(b)\nFigure 8: Histogram of the ratio \u03c12 between the squared error loss and the ideal squared\nerror for (a) unweighted Gauss-Dantzig estimator and (b) reweighted Gauss-Dantzig estimator.\nApproximately 5% of the tail of each histogram has been truncated for display; across 5000\ntrials the maximum value observed was \u03c12 \u2248165.\n3.5\nError correction\nSuppose we wish to transmit a real-valued signal x0 \u2208Rn, a block of n pieces of information, to\na remote receiver. The vector x0 is arbitrary and in particular, nonsparse. The di\ufb03culty is that\nerrors occur upon transmission so that a fraction of the transmitted codeword may be corrupted\nin a completely arbitrary and unknown fashion. In this setup, the authors in [31] showed that\none could transmit n pieces of information reliably by encoding the information as \u03a6x0 where\n\u03a6 \u2208Rm\u00d7n, m \u2265n, is a suitable coding matrix, and by solving\nmin\nx\u2208Rn \u2225y \u2212\u03a6x\u2225\u21131\n(11)\nupon receiving the corrupted codeword y = \u03a6x0 + e; here, e is the unknown but sparse corruption\npattern. The conclusion of [31] is then that the solution to this program recovers x0 exactly provided\nthat the fraction of errors is not too large. Continuing on our theme, one can also enhance the\nperformance of this error-correction strategy, further increasing the number of corrupted entries\nthat can be overcome.\nSelect a vector of length n = 128 with elements drawn from a zero-mean unit-variance Gaussian\ndistribution, and sample an m\u00d7n coding matrix \u03a6 with i.i.d. Gaussian entries yielding the codeword\n\u03a6x. For this experiment, m = 4n = 512, and k random entries of the codeword are corrupted with\na sign \ufb02ip. For the recovery, we simply use a reweighted version of (11). Our algorithm is as follows:\n1. Set \u2113= 0 and w(0)\ni\n= 1 for i = 1, 2, . . . , m.\n2. Solve the weighted \u21131 minimization problem\nx(\u2113) = arg min \u2225W (\u2113)(y \u2212\u03a6x)\u2225\u21131.\n(12)\n16\n0.25\n0.3\n0.35\n0.4\n0.45\n0\n0.2\n0.4\n0.6\n0.8\n1\nError proportion k/m\nPr(recovery)\n \n \nUnwt. L1\n\u03b2 = 0.01\n\u03b2 = 0.1\n\u03b2 = 1\n\u03b2 = 10\nFigure 9: Unweighted (solid curve) and reweighted (dashed curve) \u21131 signal recovery from\ncorrupted measurements y = \u03a6x0 + e. The signal x0 has length n = 128, the codeword y has\nsize m = 4n = 512, and the number of corrupted entries \u2225e\u2225\u21130 = k.\n3. Update the weights; let r(\u2113) = y \u2212\u03a6x(\u2113) and for each i = 1, . . . , m, de\ufb01ne\nw(\u2113+1)\ni\n=\n1\n|r(\u2113)\ni | + \u01eb\n.\n(13)\n4. Terminate on convergence or when \u2113attains a speci\ufb01ed maximum number of iterations \u2113max.\nOtherwise, increment \u2113and go to step 2.\nWe set \u01eb to be some factor \u03b2 times the standard deviation of the corrupted codeword y. We run\n100 trials for several values of \u03b2 and of the size k of the corruption pattern.\nFigure 9 shows the probability of perfect signal recovery (declared when \u2225x0 \u2212x\u2225\u2113\u221e\u226410\u22123)\nfor both the unweighted \u21131 decoding algorithm and the reweighted versions for various values of \u03b2\n(with \u2113max = 4). Across a wide range of values \u03b2 (and hence \u01eb), we see that reweighting increases\nthe number of corrupted entries (as a percentage of the codeword size m) that can be overcome,\nfrom approximately 28% to 35%.\n3.6\nTotal variation minimization for sparse image gradients\nIn a di\ufb00erent direction, reweighting can also boost the performance of total-variation (TV) min-\nimization for recovering images with sparse gradients.\nRecall the total-variation norm of a 2-\ndimensional array (xi,j), 1 \u2264i, j \u2264n, de\ufb01ned as the \u21131 norm of the magnitudes of the discrete\ngradient,\n\u2225x\u2225TV =\nX\n1\u2264i,j\u2264n\u22121\n\u2225(Dx)i,j\u2225,\nwhere (Dx)i,j is the 2-dimensional vector of forward di\ufb00erences (Dx)i,j = (xi+1,j \u2212xi,j, xi,j+1\u2212xi,j).\nBecause many natural images have a sparse or nearly sparse gradient, it makes sense to search for\nthe reconstruction with minimal TV norm, i.e.,\nmin \u2225x\u2225TV\nsubject to\ny = \u03a6x;\n(14)\n17\nsee [9, 10], for example.\nIt turns out that this problem can be recast as a second-order cone\nprogram [44], and thus solved e\ufb03ciently.\nWe adapt (14) by minimizing a sequence of weighted TV norms as follows:\n1. Set \u2113= 0 and w(0)\ni,j = 1, 1 \u2264i, j \u2264n \u22121.\n2. Solve the weighted TV minimization problem\nx(\u2113) = arg min\nX\n1\u2264i,j\u2264n\u22121\nw(\u2113)\ni,j \u2225(Dx)i,j\u2225,\nsubject to\ny = \u03a6x.\n3. Update the weights; for each (i, j), 1 \u2264i, j \u2264n \u22121,\nw(\u2113+1)\ni,j\n=\n1\n\u2225(Dx(\u2113))i,j\u2225+ \u01eb.\n(15)\n4. Terminate on convergence or when \u2113attains a speci\ufb01ed maximum number of iterations \u2113max.\nOtherwise, increment \u2113and go to step 2.\nNaturally, this iterative algorithm corresponds to minimizing a sequence of linearizations of the\nlog-sum function P\n1\u2264i,j\u2264n\u22121 log(\u2225(Dx)i,j\u2225+ \u01eb) around the previous signal estimate.\nTo show how this can enhance the performance of the recovery, consider the following exper-\niment.\nOur test image is the Shepp-Logan phantom of size n = 256 \u00d7 256 (see Figure 10(a)).\nThe pixels take values between 0 and 1, and the image has a nonzero gradient at 2184 pixels. We\nmeasure y by sampling the discrete Fourier transform of the phantom along 10 pseudo-radial lines\n(see Figure 10(b)). That is, y = \u03a6x0, where \u03a6 represents a subset of the Fourier coe\ufb03cients of x0.\nIn total, we take m = 2521 real-valued measurements.\nFigure 10(c) shows the result of the classical TV minimization, which gives a relative error\nequal to \u2225x0 \u2212x(0)\u2225\u21132/\u2225x0\u2225\u21132 \u22480.43. As shown in Figure 10(d), however, we see a substantial\nimprovement after 6 iterations of reweighted TV minimization (we used 0.1 for the value of \u01eb). The\nrecovery is near-perfect, with a relative error obeying \u2225x0 \u2212x(6)\u2225\u21132/\u2225x0\u2225\u21132 \u22482 \u00d7 10\u22123.\nFor point of comparison it takes approximately 17 radial lines (m = 4257 real-valued measure-\nments) to perfectly recover the phantom using unweighted TV minimization. Hence, with respect to\nthe sparsity of the image gradient, we have reduced the requisite oversampling factor signi\ufb01cantly,\nfrom 4257\n2184 \u22481.95 down to 2521\n2184 \u22481.15. It is worth noting that comparable reconstruction perfor-\nmance on the phantom image has also been recently achieved by directly minimizing a nonconvex\n\u2113p norm, p < 1, of the image gradient [45]; we discuss this approach further in Section 5.1.\n4\nReweighted \u21131 analysis\nIn many problems, a signal may assume sparsity in a possibly overcomplete representation. To\nmake things concrete, suppose we are given a dictionary \u03a8 of waveforms (\u03c8j)j\u2208J (the columns\nof \u03a8) which allows representing any signal as x = \u03a8\u03b1. The representation \u03b1 is deemed sparse\nwhen the vector of coe\ufb03cients \u03b1 has comparably few signi\ufb01cant terms. In some applications, it\nmay be natural to choose \u03a8 as an orthonormal basis but in others, a sparse representation of the\nsignal x may only become possible when \u03a8 is a redundant dictionary; that is, it has more columns\nthan rows. A good example is provided by an audio signal which often is sparsely represented as\n18\n(a)\n(b)\n(c)\n(d)\nFigure 10: Image recovery from reweighted TV minimization. (a) Original 256\u00d7256 phantom\nimage. (b) Fourier-domain sampling pattern. (c) Minimum-TV reconstruction; total variation\n= 1336. (d) Reweighted TV reconstruction; total variation (unweighted) = 1464.\na superposition of waveforms of the general shape \u03c3\u22121/2g((t \u2212t0)/\u03c3)ei\u03c9t, where t0, \u03c9, and \u03c3 are\ndiscrete shift, modulation and scale parameters.\nIn this setting, the common approach for sparsity-based recovery from linear measurements\ngoes by the name of Basis Pursuit [8] and is of the form\nmin \u2225\u03b1\u2225\u21131\nsubject to\ny = \u03a6\u03a8\u03b1;\n(16)\nthat is, we seek a sparse set of coe\ufb03cients \u03b1 that synthesize the signal x = \u03a8\u03b1. We call this\nsynthesis-based \u21131 recovery. A far less common approach, however, seeks a signal x whose coe\ufb03cients\n\u03b1 = \u03a8\u2217x (when x is analyzed in the dictionary \u03a8) are sparse\nmin \u2225\u03a8\u2217x\u2225\u21131\nsubject to\ny = \u03a6x.\n(17)\nWe call this analysis-based \u21131 recovery. When \u03a8 is an orthonormal basis, these two programs are\nidentical, but in general they \ufb01nd di\ufb00erent solutions. When \u03a8 is redundant, (17) involves fewer\nunknowns than (16) and may be computationally simpler to solve [46]. Moreover, in some cases\nthe analysis-based reconstruction may in fact be superior, a phenomenon which is not very well\nunderstood; see [47] for some insights.\nBoth programs are amenable to reweighting but what is interesting is the combination of\nanalysis-based \u21131 recovery and iterative reweighting which seems especially powerful. This section\nprovides two typical examples. For completeness, the iterative reweighted \u21131-analysis algorithm is\nas follows:\n1. Set \u2113= 0 and w(\u2113)\nj\n= 1, j \u2208J (J indexes the dictionary).\n2. Solve the weighted \u21131 minimization problem\nx(\u2113) = arg min \u2225W (\u2113)\u03a8\u2217x\u2225\u21131\nsubject to\ny = \u03a6x.\n3. Put \u03b1(\u2113) = \u03a8\u2217x(\u2113) and de\ufb01ne\nw(\u2113+1)\nj\n=\n1\n|\u03b1(\u2113)\nj | + \u01eb\n,\nj \u2208J.\n4. Terminate on convergence or when \u2113attains a speci\ufb01ed maximum number of iterations \u2113max.\nOtherwise, increment \u2113and go to step 2.\n19\nIteration count \u2113\n0\n1\n2\n3\n4\n5\n6\n7\nError \u2225x0 \u2212x(\u2113)\u2225\u21132/\u2225x\u2225\u21132\n0.460\n0.101\n0.038\n0.024\n0.022\n0.022\n0.022\n0.022\nTable 2: Relative \u21132 reconstruction error as a function of reweighting iteration for two-pulse\nsignal reconstruction.\n4.1\nIncoherent sampling of radar pulses\nOur \ufb01rst example is motivated by our own research focused on advancing devices for analog-\nto-digital conversion of high-bandwidth signals. To cut a long story short, standard analog-to-\ndigital converter (ADC) technology implements the usual quantized Shannon representation; that\nis, the signal is uniformly sampled at or above the Nyquist rate.\nThe hardware brick wall is\nthat conventional analog-to-digital conversion technology is currently limited to sample rates on\nthe order of 1GHz, and hardware implementations of high precision Shannon-based conversion\nat substantially higher rates seem out of sight for decades to come. This is where the theory of\ncompressive sensing becomes relevant.\nWhereas it may not be possible to digitize an analog signal at a very high rate rate, it may\nbe quite possible to change its polarity at a high rate. The idea is then to multiply the signal by\na pseudo-random sequence of plus and minus ones, integrate the product over time windows, and\ndigitize the integral at the end of each time interval. This is a parallel architecture and one has\nseveral of these random multiplier-integrator pairs running in parallel using distinct or event nearly\nindependent pseudo-random sign sequences.\nTo show the promise of this approach, we take x0 to be a 1-D signal of length n = 512 which\nis a superposition of two modulated pulses (see Figure 11(a)). From this signal, we collect m = 30\nmeasurements using an m \u00d7 n matrix \u03a6 populated with i.i.d. Bernoulli \u00b11 entries.\nThis is an\nunreasonably small amount of data corresponding to an undersampling factor exceeding 17. For\nreconstruction we consider a time-frequency Gabor dictionary that consists of a variety of sine\nwaves modulated by Gaussian windows, with di\ufb00erent locations and scales. Overall the dictionary\nis approximately 43\u00d7 overcomplete and does not contain the two pulses that comprise x0.\nFigure 11(b) shows the result of minimizing \u21131 synthesis (16) in this redundant dictionary.\nThe reconstruction shows pronounced artifacts and \u2225x0 \u2212x\u2225\u21132/\u2225x\u2225\u21132 \u22480.67. These artifacts are\nsomewhat reduced by analysis-based \u21131 recovery (17), as demonstrated in Figure 11(c); here, see\n\u2225x0 \u2212x\u2225\u21132/\u2225x\u2225\u21132 \u22480.46. However, reweighting the \u21131 analysis problem o\ufb00ers a very substantial\nimprovement. Figure 11(d) shows the result after four iterations; \u2225x0 \u2212x(4)\u2225\u21132/\u2225x\u2225\u21132 is now about\n0.022. Further, Table 2 shows the relative reconstruction error \u2225x0 \u2212x(\u2113)\u2225\u21132/\u2225x\u2225\u21132 as a function of\nthe iteration count \u2113. Massive gains are achieved after just 4 iterations.\n4.2\nFrequency sampling of biomedical images\nCompressed sensing can help reduce the scan time in Magnetic Resonance Imaging (MRI) and o\ufb00er\nsharper images of living tissues. This is especially important because time consuming MRI scans\nhave traditionally limited the use of this sensing modality in important applications. Simply put,\nfaster imaging here means novel applications. In MR, one collects information about an object by\nmeasuring its Fourier coe\ufb03cients and faster acquisition here means fewer measurements.\nWe mimic an MR experiment by taking our unknown image x0 to be the n = 256\u00d7256 = 65536\n20\n0\n100\n200\n300\n400\n500\n\u22121\n\u22120.5\n0\n0.5\n1\n0\n100\n200\n300\n400\n500\n\u22121\n\u22120.5\n0\n0.5\n1\n(a)\n(b)\n0\n100\n200\n300\n400\n500\n\u22121\n\u22120.5\n0\n0.5\n1\n0\n100\n200\n300\n400\n500\n\u22121\n\u22120.5\n0\n0.5\n1\n(c)\n(d)\nFigure 11: (a) Original two-pulse signal (blue) and reconstructions (red) via (b) \u21131 synthesis,\n(c) \u21131 analysis, (d) reweighted \u21131 analysis. (e) Relative \u21132 reconstruction error as a function of\nreweighting iteration.\n21\npixel MR angiogram image shown in Figure 12(a). We sample the image along 80 lines in the Fourier\ndomain (see Figure 12(b)), e\ufb00ectively taking m = 18737 real-valued measurements y = \u03a6x0. In\nplain terms, we undersample by a factor of about 3.\nFigure 12(c) shows the minimum energy reconstruction which solves\nmin \u2225x\u2225\u21132\nsubject to\ny = \u03a6x.\n(18)\nFigure 12(d) shows the result of TV minimization. The minimum \u21131-analysis (17) solution where\n\u03a8 is a three-scale redundant D4 wavelet dictionary that is 10 times overcomplete, is shown on\nFigure 12(e). Figure 12(f) shows the result of reweighting the \u21131 analysis with \u2113max = 4 and \u01eb\nset to 100. For a point of comparison, the maximum wavelet coe\ufb03cient has amplitude 4020, and\napproximately 108000 coe\ufb03cients (out of 655360) have amplitude greater than 100.\nWe can reinterpret these results by comparing the reconstruction quality to the best k-term\napproximation to the image x0 in a nonredundant wavelet dictionary.\nFor example, an \u21132 re-\nconstruction error equivalent to the \u21132 reconstruction of Figure 12(c) would require keeping the\nk = 1905 \u2248m/9.84 largest wavelet coe\ufb03cients from the orthogonal wavelet transform of our test\nimage. In this sense, the requisite oversampling factor can be thought of as being 9.84. Of course\nthis can be substantially improved by encouraging sparsity, and the factor is reduced to 3.33 using\nTV minimization, to 3.25 using \u21131 analysis, and to 3.01 using reweighted \u21131 analysis.\nWe would like to be clear about what this means. Consider the image in Figure 12(a) and\nits best k-term wavelet approximation with k = 6225; that is, the approximation obtained by\ncomputing all the D4 wavelet coe\ufb03cients and retaining the k largest in the expansion of the object\n(and throwing out the others). Then we have shown that the image obtained by measuring 3k\nreal-valued Fourier measurements and solving the iterative reweighted \u21131 analysis has just about\nthe same accuracy. That is, the oversampling factor needed to obtain an image of the same quality\nas if one knew ahead of time the locations of the k most signi\ufb01cant pieces of information and their\nvalue, is just 3.\n5\nDiscussion\nIn summary, reweighted \u21131 minimization outperforms plain \u21131 minimization in a variety of setups.\nTherefore, this technique might be of interest to researchers in the \ufb01eld of compressed sensing\nand/or statistical estimation as it might help to improve the quality of reconstructions and/or\nestimations. Further, this technique is easy to deploy as (1) it can be built on top of existing \u21131\nsolvers and (2) the number of iterations is typically very low so that the additional computational\ncost is not prohibitive. We conclude this paper by discussing related work and possible future\ndirections.\n5.1\nRelated work\nWhereas we have focused on modifying the \u21131 norm, a number of algorithms been have proposed that\ninvolve successively reweighting alternative penalty functions. In addition to IRLS (see Section 2.5),\nseveral such algorithms deserve mention.\nGorodnitsky and Rao [48] propose FOCUSS as an iterative method for \ufb01nding sparse solutions\nto underdetermined systems. At each iteration, FOCUSS solves a reweighted \u21132 minimization with\n22\n(a)\n(b)\n(c)\n(d)\n(e)\n(f)\nFigure 12: (a) Original MR angiogram. (b) Fourier sampling pattern. (c) Backprojection,\nPSNR = 29.00dB. (d) Minimum TV reconstruction, PSNR = 34.23dB. (e) \u21131 analysis recon-\nstruction, PSNR = 34.37dB. (f) Reweighted \u21131 analysis reconstruction, PSNR = 34.78dB.\n23\nweights\nw(\u2113)\ni\n=\n1\nx(\u2113\u22121)\ni\n(19)\nfor i = 1, 2, . . . , n. For nonzero signal coe\ufb03cients, it is shown that each step of FOCUSS is equivalent\nto a step of the modi\ufb01ed Newton\u2019s method for minimizing the function\nX\ni:xi\u0338=0\nlog |xi|\nsubject to y = \u03a6x. As the iterations proceed, it is suggested to identify those coe\ufb03cients apparently\nconverging to zero, remove them from subsequent iterations, and constrain them instead to be\nidentically zero.\nIn a small series of experiments, we have observed that reweighted \u21131 minimization recovers\nsparse signals with lower error (or from fewer measurements) than the FOCUSS algorithm. We\nattribute this fact, for one, to the natural tendency of unweighted \u21131 minimization to encourage\nsparsity (while unweighted \u21132 minimization does not).\nWe have also experimented with an \u01eb-\nregularization to the reweighting function (19) that is analogous to (6). However we have found that\nthis formulation fails to encourage strictly sparse solutions. (Sparse solutions can be encouraged by\nletting \u01eb \u21920 as the iterations proceed, but the overall performance remains inferior to reweighted\n\u21131 minimization with \ufb01xed \u01eb.)\nHarikumar and Bresler [49] propose an iterative algorithm that can be viewed as a generalization\nof FOCUSS. At each stage, the algorithm solves a convex optimization problem with a reweighted\n\u21132 cost function that encourages sparse solutions. The algorithm allows for di\ufb00erent reweighting\nrules; for a given choice of reweighting rule, the algorithm converges to a local minimum of some\nconcave objective function (analogous to the log-sum penalty function in (7)). These methods build\nupon \u21132 minimization rather than \u21131 minimization.\nDelaney and Bresler [50] also propose a general algorithm for minimizing functionals having\nconcave regularization penalties, again by solving a sequence of reweighted convex optimization\nproblems (though not necessarily \u21132 problems) with weights that decrease as a function of the prior\nestimate. With the particular choice of a log-sum regularization penalty, the algorithm resembles\nthe noise-aware reweighted \u21131 minimization discussed in Section 3.3.\nFinally, in a slightly di\ufb00erent vein, Chartrand [45] has recently proposed an iterative algorithm\nto minimize the concave objective \u2225x\u2225\u2113p with p < 1. (The algorithm alternates between gradient\ndescent and projection onto the constraint set y = \u03a6x.)\nWhile a global optimum cannot be\nguaranteed, experiments suggests that a local minimum may be found\u2014when initializing with the\nminimum \u21132 solution\u2014that is often quite sparse. This algorithm seems to outperform (P1) in a\nnumber of instances and o\ufb00ers further support for the utility of nonconvex penalties in sparse signal\nrecovery. To reiterate, a major advantage of reweighted \u21131 minimization in this thrust is that (1)\nit can be implemented in a variety of settings (see Sections 3 and 4) on top of existing and mature\nlinear programming solvers and (2) it typically converges in very few steps. The log-sum penalty\nis also more \u21130-like and as we discuss in Section 2.4, additional concave penalty functions can be\nconsidered simply by adapting the reweighting rule.\n5.2\nFuture directions\nIn light of the promise of reweighted \u21131 minimization, it seems desirable to further investigate the\nproperties of this algorithm.\n24\n\u2022 Under what conditions does the algorithm converge? That is, when do the successive iterates\nx(\u2113) have a limit x(\u221e)?\n\u2022 As shown in Section 2, when there is a sparse solution and the reweighted algorithm \ufb01nds\nit, convergence may occur in just very few steps. It would be of interest to understand this\nphenomenon more precisely.\n\u2022 What are smart and robust rules for selecting the parameter \u01eb? That is, rules that would\nautomatically adapt to the dynamic range and the sparsity of the object under study as to\nensure reliable performance across a broad array of signals. Of interest are ways of updating\n\u01eb as the algorithm progresses towards a solution. Of course, \u01eb does not need to be uniform\nacross all coordinates.\n\u2022 We mentioned the use of other functionals and reweighting rules. How do they compare?\n\u2022 Finally, any result quantifying the improvement of the reweighted algorithm for special classes\nof sparse or nearly sparse signals would be signi\ufb01cant.\nAcknowledgments\nE. C. was partially supported by a National Science Foundation grant CCF-515362, by the 2006\nWaterman Award (NSF) and by a grant from DARPA. This work was performed while M. W. was an\nNSF Postdoctoral Fellow (NSF DMS-0603606) in the Department of Applied and Computational\nMathematics at Caltech.\nS. B. was partially supported by NSF award 0529426, NASA award\nNNX07AEIIA, and AFOSR awards FA9550-06-1-0514 and FA9550-06-1-0312. We would like to\nthank Nathaniel Braun and Peter Stobbe for fruitful discussions about this project. Parts of this\nwork were presented at the Fourth IEEE International Symposium on Biomedical Imaging (ISBI\n\u201807) held April 12\u201315, 2007 and at the Von Neumann Symposium on Sparse Representation and\nHigh-Dimensional Geometry held July 8\u201312, 2007. Related work was \ufb01rst developed as lecture notes\nfor the course EE364b: Convex Optimization II, given at Stanford Winter quarter 2006-07 [51].\n25\nReferences\n[1] S. Boyd and L. Vandenberghe, Convex Optimization, Cambridge University Press, 2004.\n[2] J. F. Claerbout and F. Muir, \u201cRobust modeling with erratic data,\u201d Geophysics, vol. 38, no. 5, pp.\n826\u2013844, Oct. 1973.\n[3] H. L. Taylor, S. C. Banks, and J. F. McCoy, \u201cDeconvolution with the \u21131 norm,\u201d Geophysics, vol. 44,\nno. 1, pp. 39\u201352, Jan. 1979.\n[4] F. Santosa and W. W. Symes, \u201cLinear inversion of band-limited re\ufb02ection seismograms,\u201d SIAM J. Sci.\nStat. Comput., vol. 7, no. 4, pp. 1307\u20131330, 1986.\n[5] D. L. Donoho and P. B. Stark, \u201cUncertainty principles and signal recovery,\u201d SIAM J. Appl. Math., vol.\n49, no. 3, pp. 906\u2013931, June 1989.\n[6] D. L. Donoho and B. F. Logan, \u201cSignal recovery and the large sieve,\u201d SIAM J. Appl. Math., vol. 52,\nno. 2, pp. 577\u2013591, Apr. 1992.\n[7] R. Tibshirani, \u201cRegression shrinkage and selection via the lasso,\u201d J. Royal. Statist. Soc B., vol. 58, no.\n1, pp. 267\u2013288, 1996.\n[8] S. Chen, D. Donoho, and M. Saunders, \u201cAtomic decomposition by basis pursuit,\u201d SIAM J. on Sci.\nComp., vol. 20, no. 1, pp. 33\u201361, 1998.\n[9] L. I. Rudin, S. Osher, and E. Fatemi, \u201cNonlinear total variation based noise removal algorithms,\u201d Phys.\nD, vol. 60, no. 1-4, pp. 259\u2013268, 1992.\n[10] P. Blomgren and T. F. Chan,\n\u201cColor TV: total variation methods for restoration of vector-valued\nimages,\u201d IEEE Trans. Image Processing, vol. 7, pp. 304\u2013309, March 1998.\n[11] L. Vandenberghe, S. Boyd, and A. El Gamal, \u201cOptimal wire and transistor sizing for circuits with non-\ntree topology,\u201d in Proceedings of the 1997 IEEE/ACM International Conference on Computer Aided\nDesign, 1997, pp. 252\u2013259.\n[12] L. Vandenberghe, S. Boyd, and A. El Gamal, \u201cOptimizing dominant time constant in RC circuits,\u201d\nIEEE Transactions on Computer-Aided Design, vol. 2, no. 2, pp. 110\u2013125, Feb. 1998.\n[13] A. Hassibi, J. How, and S. Boyd, \u201cLow-authority controller design via convex optimization,\u201d AIAA\nJournal of Guidance, Control, and Dynamics, vol. 22, no. 6, pp. 862\u2013872, November-December 1999.\n[14] M. Dahleh and I. Diaz-Bobillo,\nControl of Uncertain Systems: A Linear Programming Approach,\nPrentice-Hall, 1995.\n[15] M. Lobo, M. Fazel, and S. Boyd,\n\u201cPortfolio optimization with linear and \ufb01xed transaction costs,\u201d\nAnnals of Operations Research, vol. 152, no. 1, pp. 341\u2013365, 2006.\n[16] A. Ghosh and S. Boyd, \u201cGrowing well-connected graphs,\u201d in Proceedings of the 45th IEEE Conference\non Decision and Control, December 2006, pp. 6605\u20136611.\n[17] J. Sun, S. Boyd, L. Xiao, and P. Diaconis, \u201cThe fastest mixing Markov process on a graph and a\nconnection to a maximum variance unfolding problem,\u201d SIAM Review, vol. 48, no. 4, pp. 681\u2013699,\n2006.\n[18] S.-J. Kim, K. Koh S. Boyd, and D. Gorinevsky,\n\u201c\u21131 trend \ufb01ltering,\u201d 2007,\nAvailable at\nwww.stanford.edu/~boyd/l1_trend_filter.html.\n[19] D. L. Donoho and X. Huo, \u201cUncertainty principles and ideal atomic decomposition,\u201d IEEE Trans.\nInform. Theory, vol. 47, no. 7, pp. 2845\u20132862, Nov. 2001.\n[20] M. Elad and A. M. Bruckstein, \u201cA generalized uncertainty principle and sparse representation in pairs\nof bases,\u201d IEEE Trans. Inform. Theory, vol. 48, no. 9, pp. 2558\u20132567, 2002.\n26\n[21] R. Gribonval and M. Nielsen, \u201cSparse representations in unions of bases,\u201d IEEE Trans. Inform. Theory,\nvol. 49, no. 12, pp. 3320\u20133325, 2003.\n[22] J. A. Tropp, \u201cJust relax: Just relax: convex programming methods for identifying sparse signals in\nnoise,\u201d IEEE Trans. Inform. Theory, vol. 52, pp. 1030\u20131051, 2006.\n[23] E. J. Cand`es, J. Romberg, and T. Tao, \u201cRobust uncertainty principles: Exact signal reconstruction from\nhighly incomplete frequency information,\u201d IEEE Trans. Inform. Theory, vol. 52, no. 2, pp. 489\u2013509,\nFeb. 2006.\n[24] E. J. Cand`es and T. Tao, \u201cNear optimal signal recovery from random projections: Universal encoding\nstrategies?,\u201d IEEE Trans. Inform. Theory, vol. 52, no. 12, pp. 5406\u20135425, Dec. 2006.\n[25] D. Donoho, \u201cCompressed sensing,\u201d IEEE Trans. Inform. Theory, vol. 52, no. 4, Apr. 2006.\n[26] D. L. Donoho and J. Tanner, \u201cCounting faces of randomly-projected polytopes when then projection\nradically lowers dimension,\u201d Tech. Rep. 2006-11, Stanford University Department of Statistics, 2006.\n[27] E. J. Cand`es, J. Romberg, and T. Tao, \u201cStable signal recovery from incomplete and inaccurate mea-\nsurements,\u201d Comm. Pure Appl. Math., vol. 59, no. 8, pp. 1207\u20131223, Aug. 2006.\n[28] D. Donoho and Y. Tsaig, \u201cExtensions of compressed sensing,\u201d Signal Processing, vol. 86, no. 3, pp.\n533\u2013548, Mar. 2006.\n[29] D. Takhar, V. Bansal, M. Wakin, M. Duarte, D. Baron, K. F. Kelly, and R. G. Baraniuk, \u201cA compressed\nsensing camera: New theory and an implementation using digital micromirrors,\u201d in Proc. Comp. Imaging\nIV at SPIE Electronic Imaging, San Jose, California, January 2006.\n[30] M. Lustig, D. Donoho, and J. M. Pauly, \u201cSparse MRI: The application of compressed sensing for rapid\nMR imaging,\u201d 2007, Preprint.\n[31] E. J. Cand`es and T. Tao, \u201cDecoding by linear programming,\u201d IEEE Trans. Inform. Theory, vol. 51,\nno. 12, Dec. 2005.\n[32] E. J. Cand`es and P. A. Randall, \u201cHighly robust error correction by convex programming,\u201d Available\non the ArXiV preprint server (cs/0612124), 2006.\n[33] D. L. Healy (Program Manager),\n\u201cAnalog-to-Information (A-to-I),\u201d\nDARPA/MTO Broad Agency\nAnnouncement BAA 05-35, July 2005.\n[34] W. Bajwa, J. Haupt, A. Sayeed, and R. Nowak, \u201cCompressive wireless sensing,\u201d in Proc. Fifth Int.\nConf. on Information processing in sensor networks, 2006, pp. 134\u2013142.\n[35] D. Baron, M. B. Wakin, M. F. Duarte, S. Sarvotham, and R. G. Baraniuk, \u201cDistributed compressed\nsensing,\u201d 2005, Preprint.\n[36] K. Lange, Optimization, Springer Texts in Statistics. Springer-Verlag, New York, 2004.\n[37] M. S. Lobo, M. Fazel, and S. Boyd, \u201cPortfolio optimization with linear and \ufb01xed transaction costs,\u201d\nAnn. Oper. Res., vol. 152, no. 1, pp. 341\u2013365, July 2007.\n[38] M. Fazel, H. Hindi, and S. Boyd, \u201cLog-det heuristic for matrix rank minimization with applications to\nHankel and Euclidean distance matrices,\u201d in Proc. Am. Control Conf., June 2003.\n[39] E. J. Schlossmacher, \u201cAn iterative technique for absolute deviations curve \ufb01tting,\u201d J. Amer. Statist.\nAssoc., vol. 68, no. 344, pp. 857\u2013859, Dec. 1973.\n[40] P. Holland and R. Welsch, \u201cRobust regression using iteratively reweighted least-squares,\u201d Commun.\nStat. Theoret. Meth., vol. A6, 1977.\n[41] P. J. Huber, Robust Statistics, Wiley-Interscience, 1981.\n27\n[42] R. Yarlagadda, J. B. Bednar, and T. L. Watt, \u201cFast algorithms for \u2113p deconvolution,\u201d IEEE Trans.\nAcoust., Speech, Signal Processing, vol. 33, no. 1, pp. 174\u2013182, Feb. 1985.\n[43] E. J. Cand`es and T. Tao, \u201cThe Dantzig selector: Statistical estimation when p is much larger than n,\u201d\nAnn. Statist., 2006, To appear.\n[44] D. Goldfarb and W. Yin, \u201cSecond-order cone programming methods for total variation-based image\nrestoration,\u201d SIAM J. Scienti\ufb01c Comput., vol. 27, no. 2, pp. 622\u2013645, 2005.\n[45] R. Chartrand, \u201cExact reconstruction of sparse signals via nonconvex minimization,\u201d Signal Process.\nLett., vol. 14, no. 10, pp. 707\u2013710, 2007.\n[46] J.-L. Starck, M. Elad, and D. L. Donoho, \u201cRedundant multiscale transforms and their application for\nmorphological component analysis,\u201d Adv. Imaging and Electron Phys., vol. 132, 2004.\n[47] M. Elad, P. Milanfar, and R. Rubinstein, \u201cAnalysis versus synthesis in signal priors,\u201d Inverse Problems,\nvol. 23, 2007.\n[48] I. F. Gorodnitsky and B. D. Rao, \u201cSparse signal reconstruction from limited data using FOCUSS: A\nre-weighted minimum norm algorithm,\u201d IEEE Trans. Signal Processing, vol. 45, no. 3, pp. 600\u2013616,\nMar. 1997.\n[49] G. Harikumar and Y. Bresler,\n\u201cA new algorithm for computing sparse solutions to linear inverse\nproblems,\u201d in Proc. Int. Conf. Acoustics, Speech, Signal Processing (ICASSP). May 1996, IEEE.\n[50] A. H. Delaney and Y. Bresler, \u201cGlobally convergent edge-preserving regularized reconstruction: An\napplication to limited-angle tomography,\u201d IEEE Trans. Image Processing, vol. 7, no. 2, pp. 204\u2013221,\nFeb. 1998.\n[51] S.\nBoyd,\n\u201cLecture\nnotes\nfor\nEE364B:\nConvex\nOptimization\nII,\u201d\n2007,\nAvailable\nat\nwww.stanford.edu/class/ee364b/.\n28\n",
        "sentence": " \u2211 i wi|xi| with suitable positive weights wi (Candes et al., 2008) instead of a plain `1-norm. This iterative approach can be seen as an iterative local linearization of the concave log-penalty for sparsity, \u2211 i log(\u03b4+ |xi|) (Fazel et al., 2001; Candes et al., 2008).",
        "context": "Naturally, this iterative algorithm corresponds to minimizing a sequence of linearizations of the\nlog-sum function P\n1\u2264i,j\u2264n\u22121 log(\u2225(Dx)i,j\u2225+ \u01eb) around the previous signal estimate.\nconvex constraints [38]. Using general theory, they argue that Pn\ni=1 log(|x(\u2113)\ni | + \u01eb) converges to\na local minimum of g(x) = Pn\ni=1 log(|xi| + \u01eb) (note that this not saying that the sequence (x(\u2113))\non the \u21131 penalty function. Suppose, for example, that the weights were inversely proportional to\nthe true signal magnitude, i.e., that\nwi =\n(\n1\n|x0,i|,\nx0,i \u0338= 0,\n\u221e,\nx0,i = 0.\n(5)"
    },
    {
        "title": "Rank-sparsity incoherence for matrix decomposition",
        "author": [
            "V. Chandrasekaran",
            "S. Sanghavi",
            "P.A. Parrilo",
            "A.S. Willsky"
        ],
        "venue": "SIAM Journal on Optim.,",
        "citeRegEx": "Chandrasekaran et al\\.,? \\Q2011\\E",
        "shortCiteRegEx": "Chandrasekaran et al\\.",
        "year": 2011,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " The nuclear norm relaxation has been successfully used for a range of machine learning problems involving rank constraints, including low-rank matrix completion, low-order system approximation, and robust PCA (Cai et al., 2010; Chandrasekaran et al., 2011). In context of matrix completion and robust PCA, the nuclear norm relaxation has strong theoretical accuracy guarantees (Recht et al., 2010; Chandrasekaran et al., 2011).",
        "context": null
    },
    {
        "title": "Low-rank structure learning via log-sum heuristic recovery",
        "author": [
            "Y. Deng",
            "Q. Dai",
            "R. Liu",
            "Z. Zhang",
            "S. Hu"
        ],
        "venue": "arXiv preprint arXiv:1012.1919,",
        "citeRegEx": "Deng et al\\.,? \\Q2012\\E",
        "shortCiteRegEx": "Deng et al\\.",
        "year": 2012,
        "abstract": "",
        "full_text": "",
        "sentence": " Note that (Deng et al., 2012) considered a strategy for minimizing re-weighted nuclear norms for matrix completion, but instead of using exact minimization over A, they took a step in the gradient direction.",
        "context": null
    },
    {
        "title": "Robust solutions to leastsquares problems with uncertain data",
        "author": [
            "L. El Ghaoui",
            "H. Lebret"
        ],
        "venue": "SIAM J. on Matrix Analysis and Applic.,",
        "citeRegEx": "Ghaoui and Lebret,? \\Q1997\\E",
        "shortCiteRegEx": "Ghaoui and Lebret",
        "year": 1997,
        "abstract": "",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "A rank minimization heuristic with application to minimum order system approximation",
        "author": [
            "M. Fazel",
            "H. Hindi",
            "S.P. Boyd"
        ],
        "venue": "In IEEE American Control Conference,",
        "citeRegEx": "Fazel et al\\.,? \\Q2001\\E",
        "shortCiteRegEx": "Fazel et al\\.",
        "year": 2001,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " Our approach uses the re-weighted nuclear norm relaxation (Fazel et al., 2001) and is highly flexible: it can handle very general linear structure on errors, including arbitrary weights (changing noise for different entries), patterns of observed and unobserved errors, Toeplitz and Hankel structures, and even norms other than the Frobenius norm. A very effective improvement of the nuclear norm comes from re-weighting it (Fazel et al., 2001; Mohan & Fazel, 2010) based on the log-determinant heuristic for rank. This iterative approach can be seen as an iterative local linearization of the concave log-penalty for sparsity, \u2211 i log(\u03b4+ |xi|) (Fazel et al., 2001; Candes et al., 2008). has a semi-definite programming (SDP) representation (Fazel et al., 2001).",
        "context": null
    },
    {
        "title": "An analysis of the total least squares problem",
        "author": [
            "G.H. Golub",
            "C.F. Van Loan"
        ],
        "venue": "SIAM Journal on Numerical Analysis,",
        "citeRegEx": "Golub and Loan,? \\Q1980\\E",
        "shortCiteRegEx": "Golub and Loan",
        "year": 1980,
        "abstract": "",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "Weighted `1 minimization for sparse recovery with prior information",
        "author": [
            "A. Khajehnejad",
            "W. Xu",
            "S. Avestimehr",
            "B. Hassibi"
        ],
        "venue": "In IEEE Int. Symposium on Inf. Theory,",
        "citeRegEx": "Khajehnejad et al\\.,? \\Q2009\\E",
        "shortCiteRegEx": "Khajehnejad et al\\.",
        "year": 2009,
        "abstract": "",
        "full_text": "",
        "sentence": " In both empirical and emerging theoretical studies(Needell, 2009; Khajehnejad et al., 2009)",
        "context": null
    },
    {
        "title": "The augmented Lagrange multiplier method for exact recovery of corrupted low-rank matrices",
        "author": [
            "Z. Lin",
            "M. Chen",
            "Y. Ma"
        ],
        "venue": "arXiv preprint arXiv:1009.5055,",
        "citeRegEx": "Lin et al\\.,? \\Q2010\\E",
        "shortCiteRegEx": "Lin et al\\.",
        "year": 2010,
        "abstract": "",
        "full_text": "",
        "sentence": " We develop an effective first-order approach for STLS based on the augmented Lagrangian multiplier (ALM) method (Bertsekas, 1982; Lin et al., 2010). The work of (Lin et al., 2010) extended the analysis to allow objective functions involving nuclear-norm terms. We do not wait for the coordinate descent to converge at each ALM step, but rather update \u039b and \u03bc after a single iteration, following the inexact ALM algorithm in (Lin et al., 2010).",
        "context": null
    },
    {
        "title": "Robust recovery of subspace structures by low-rank representation",
        "author": [
            "G. Liu",
            "Z. Lin",
            "S. Yan",
            "J. Sun",
            "Y. Yu",
            "Y. Ma"
        ],
        "venue": "arXiv preprint arXiv:1010.2955,",
        "citeRegEx": "Liu et al\\.,? \\Q2010\\E",
        "shortCiteRegEx": "Liu et al\\.",
        "year": 2010,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " There is no known analytic thresholding solution for the weighted nuclear norm, so instead we follow (Liu et al., 2010) to create a new variable D = W1AW2 and add this definition as an additional linear constraint:",
        "context": null
    },
    {
        "title": "Software for weighted structured low-rank approximation",
        "author": [
            "I. Markovsky",
            "K. Usevich"
        ],
        "venue": "J. Comput. Appl. Math.,",
        "citeRegEx": "Markovsky and Usevich,? \\Q2014\\E",
        "shortCiteRegEx": "Markovsky and Usevich",
        "year": 2014,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "Overview of total least-squares methods",
        "author": [
            "I. Markovsky",
            "S. Van Huffel"
        ],
        "venue": "Signal processing,",
        "citeRegEx": "Markovsky and Huffel,? \\Q2007\\E",
        "shortCiteRegEx": "Markovsky and Huffel",
        "year": 2007,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "Application of structured total least squares for system identification and model reduction",
        "author": [
            "I. Markovsky",
            "J.C. Willems",
            "S. Van Huffel",
            "B. De Moor",
            "R. Pintelon"
        ],
        "venue": "Automatic Control, IEEE Trans. on,",
        "citeRegEx": "Markovsky et al\\.,? \\Q2005\\E",
        "shortCiteRegEx": "Markovsky et al\\.",
        "year": 2005,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " block-diagonal, Toeplitz, or Hankel in system identification literature (Markovsky et al., 2005).",
        "context": null
    },
    {
        "title": "Reweighted nuclear norm minimization with application to system identification",
        "author": [
            "K. Mohan",
            "M. Fazel"
        ],
        "venue": "In American Control Conference,",
        "citeRegEx": "Mohan and Fazel,? \\Q2010\\E",
        "shortCiteRegEx": "Mohan and Fazel",
        "year": 2010,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "Noisy signal recovery via iterative reweighted l1-minimization",
        "author": [
            "D. Needell"
        ],
        "venue": "In Forty-Third Asilomar Conference on Signals, Systems and Computers,",
        "citeRegEx": "Needell,? \\Q2009\\E",
        "shortCiteRegEx": "Needell",
        "year": 2009,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " In both empirical and emerging theoretical studies(Needell, 2009; Khajehnejad et al., 2009)",
        "context": null
    },
    {
        "title": "Guaranteed minimum-rank solutions of linear matrix equations via nuclear norm minimization",
        "author": [
            "B. Recht",
            "M. Fazel",
            "P.A. Parrilo"
        ],
        "venue": "SIAM Review,",
        "citeRegEx": "Recht et al\\.,? \\Q2010\\E",
        "shortCiteRegEx": "Recht et al\\.",
        "year": 2010,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " In context of matrix completion and robust PCA, the nuclear norm relaxation has strong theoretical accuracy guarantees (Recht et al., 2010; Chandrasekaran et al., 2011).",
        "context": null
    },
    {
        "title": "Coupling among growth rate response, metabolic cycle, and cell division cycle in yeast",
        "author": [
            "N. Slavov",
            "D. Botstein"
        ],
        "venue": "Molecular bio. of the cell,",
        "citeRegEx": "Slavov and Botstein,? \\Q2011\\E",
        "shortCiteRegEx": "Slavov and Botstein",
        "year": 2011,
        "abstract": " We studied the steady-state responses to changes in growth rate of yeast when ethanol is the sole source of carbon and energy. Analysis of these data, together with data from studies where glucose was the carbon source, allowed us to distinguish a \u201cuniversal\u201d growth rate response (GRR) common to all media studied from a GRR specific to the carbon source. Genes with positive universal GRR include ribosomal, translation, and mitochondrial genes, and those with negative GRR include autophagy, vacuolar, and stress response genes. The carbon source\u2013specific GRR genes control mitochondrial function, peroxisomes, and synthesis of vitamins and cofactors, suggesting this response may reflect the intensity of oxidative metabolism. All genes with universal GRR, which comprise 25% of the genome, are expressed periodically in the yeast metabolic cycle (YMC). We propose that the universal GRR may be accounted for by changes in the relative durations of the YMC phases. This idea is supported by oxygen consumption data from metabolically synchronized cultures with doubling times ranging from 5 to 14 h. We found that the high oxygen consumption phase of the YMC can coincide exactly with the S phase of the cell division cycle, suggesting that oxidative metabolism and DNA replication are not incompatible. ",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "Metabolic cycling without cell division cycling in respiring yeast",
        "author": [
            "N. Slavov",
            "J. Macinskas",
            "A. Caudy",
            "D. Botstein"
        ],
        "venue": "Proceedings of the National Academy of Sciences,",
        "citeRegEx": "Slavov et al\\.,? \\Q2011\\E",
        "shortCiteRegEx": "Slavov et al\\.",
        "year": 2011,
        "abstract": "Despite rapid progress in characterizing the yeast metabolic cycle, its connection to the cell division cycle (CDC) has remained unclear. We discovered that a prototrophic batch culture of budding yeast, growing in a phosphate-limited ethanol medium, synchronizes spontaneously and goes through multiple metabolic cycles, whereas the fraction of cells in the G1/G0 phase of the CDC increases monotonically from 90 to 99%. This demonstrates that metabolic cycling does not require cell division cycling and that metabolic synchrony does not require carbon-source limitation. More than 3,000 genes, including most genes annotated to the CDC, were expressed periodically in our batch culture, albeit a mere 10% of the cells divided asynchronously; only a smaller subset of CDC genes correlated with cell division. These results suggest that the yeast metabolic cycle reflects a growth cycle during G1/G0 and explains our previous puzzling observation that genes annotated to the CDC increase in expression at slow growth.",
        "full_text": "",
        "sentence": " We consider a cell culture containing cells in K distinct physiological states, such as phases of cell growth or division cycles (Slavov et al., 2011; 2012). Our algorithm infers the fraction of cells in HOC and in LOC phase, up to a scalar factor, in close agreement with expectations from physical measurements in synchronized cultures (Slavov et al., 2011; Slavov & Botstein, 2011).",
        "context": null
    },
    {
        "title": "A conserved cell growth cycle can account for the environmental stress responses of divergent eukaryotes",
        "author": [
            "Slavov",
            "Nikolai",
            "Airoldi",
            "Edoardo M",
            "van Oudenaarden",
            "Alexander",
            "Botstein",
            "David"
        ],
        "venue": "Molecular Biology of the Cell,",
        "citeRegEx": "Slavov et al\\.,? \\Q1986\\E",
        "shortCiteRegEx": "Slavov et al\\.",
        "year": 1986,
        "abstract": "The respiratory metabolic cycle in budding yeast (Saccharomyces cerevisiae) consists of two phases that are most simply defined phenomenologically: low oxygen consumption (LOC) and high oxygen consumption (HOC). Each phase is associated with the periodic expression of thousands of genes, producing oscillating patterns of gene expression found in synchronized cultures and in single cells of slowly growing unsynchronized cultures. Systematic variation in the durations of the HOC and LOC phases can account quantitatively for well-studied transcriptional responses to growth rate differences. Here we show that a similar mechanism\u2014transitions from the HOC phase to the LOC phase\u2014can account for much of the common environmental stress response (ESR) and for the cross-protection by a preliminary heat stress (or slow growth rate) to subsequent lethal heat stress. Similar to the budding yeast metabolic cycle, we suggest that a metabolic cycle, coupled in a similar way to the ESR, in the distantly related fission yeast, Schizosaccharomyces pombe, and in humans can explain gene expression and respiratory patterns observed in these eukaryotes. Although metabolic cycling is associated with the G0/G1 phase of the cell division cycle of slowly growing budding yeast, transcriptional cycling was detected in the G2 phase of the division cycle in fission yeast, consistent with the idea that respiratory metabolic cycling occurs during the phases of the cell division cycle associated with mass accumulation in these divergent eukaryotes.",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "Weighted low-rank approximations",
        "author": [
            "N. Srebro",
            "T. Jaakkola"
        ],
        "venue": "In Int. Conf. Machine Learning (ICML),",
        "citeRegEx": "Srebro and Jaakkola,? \\Q2003\\E",
        "shortCiteRegEx": "Srebro and Jaakkola",
        "year": 2003,
        "abstract": "",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "SDPT3 \u2013 a Matlab software package for semidefinite programming, version 1.3",
        "author": [
            "K.C. Toh",
            "M.J. Todd",
            "R.H. T\u00fct\u00fcnc\u00fc"
        ],
        "venue": "Optim. Method. Softw.,",
        "citeRegEx": "Toh et al\\.,? \\Q1999\\E",
        "shortCiteRegEx": "Toh et al\\.",
        "year": 1999,
        "abstract": "",
        "full_text": "",
        "sentence": " There are various ways to solve the plain and weighted nuclear norm STLS formulations, including interiorpoint methods (Toh et al., 1999) and iterative thresholding (Cai et al.",
        "context": null
    },
    {
        "title": "Weighted and structured sparse total least-squares for perturbed compressive sampling",
        "author": [
            "H. Zhu",
            "G.B. Giannakis",
            "G. Leus"
        ],
        "venue": "In IEEE Int. Conf. Acoustics, Speech and Signal Proc.,",
        "citeRegEx": "Zhu et al\\.,? \\Q2011\\E",
        "shortCiteRegEx": "Zhu et al\\.",
        "year": 2011,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " by local optimization methods (Markovsky & Usevich, 2014; Zhu et al., 2011; Srebro & Jaakkola, 2003).",
        "context": null
    }
]