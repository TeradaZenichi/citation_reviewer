[
    {
        "title": "Learning from noisy examples",
        "author": [
            "D. Angluin",
            "P.D. Laird"
        ],
        "venue": "Machine Learning 2, 4, 343\u2013 370.",
        "citeRegEx": "Angluin and Laird,? 1987",
        "shortCiteRegEx": "Angluin and Laird",
        "year": 1987,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " Informally, our results imply that for every concept class C known to be PAC learnable with classification noise (Angluin and Laird, 1987), and almost every class P known to be PAC learnable in the distributional sense of Kearns et al. Informally, our results imply that for every concept class C known to be PAC learnable with classification noise (Angluin and Laird, 1987), and almost every class P known to be PAC learnable in the distributional sense of Kearns et al. (1994), PwD problems given by (C,P ) are learnable in our framework. First, our work is related to the results in PAC learning under classification noise (Angluin and Laird, 1987; Decatur, 1997; Kearns, 1998), and makes use of a result by Ralaivola et al. At the highest level, our model falls under the framework of Haussler (1992), which gives a decision-theoretic treatment of PAC-style learning (Valiant, 1984) for very general loss functions; our model can be viewed as a special case in which the loss function is conditional log-loss given the value of a classifier. At the highest level, our model falls under the framework of Haussler (1992), which gives a decision-theoretic treatment of PAC-style learning (Valiant, 1984) for very general loss functions; our model can be viewed as a special case in which the loss function is conditional log-loss given the value of a classifier. Whereas Haussler (1992) is primarily concerned with sample complexity, our focus here is on computational complexity and composition of learning models. First, our work is related to the results in PAC learning under classification noise (Angluin and Laird, 1987; Decatur, 1997; Kearns, 1998), and makes use of a result by Ralaivola et al. (2006) that established the equivalence of learning under (standard) classification noise (CN) and under class-conditional classification noise (CCCN). First, our work is related to the results in PAC learning under classification noise (Angluin and Laird, 1987; Decatur, 1997; Kearns, 1998), and makes use of a result by Ralaivola et al. (2006) that established the equivalence of learning under (standard) classification noise (CN) and under class-conditional classification noise (CCCN). Our work also relies on the PAC model for distribution learning (Kearns et al., 1994), including a long line of works on learning mixtures of distributions (see e.g. Dasgupta (1999); Arora and Kannan (2001); Vempala and Wang (2004); Feldman et al. First, our work is related to the results in PAC learning under classification noise (Angluin and Laird, 1987; Decatur, 1997; Kearns, 1998), and makes use of a result by Ralaivola et al. (2006) that established the equivalence of learning under (standard) classification noise (CN) and under class-conditional classification noise (CCCN). Our work also relies on the PAC model for distribution learning (Kearns et al., 1994), including a long line of works on learning mixtures of distributions (see e.g. Dasgupta (1999); Arora and Kannan (2001); Vempala and Wang (2004); Feldman et al. First, our work is related to the results in PAC learning under classification noise (Angluin and Laird, 1987; Decatur, 1997; Kearns, 1998), and makes use of a result by Ralaivola et al. (2006) that established the equivalence of learning under (standard) classification noise (CN) and under class-conditional classification noise (CCCN). Our work also relies on the PAC model for distribution learning (Kearns et al., 1994), including a long line of works on learning mixtures of distributions (see e.g. Dasgupta (1999); Arora and Kannan (2001); Vempala and Wang (2004); Feldman et al. First, our work is related to the results in PAC learning under classification noise (Angluin and Laird, 1987; Decatur, 1997; Kearns, 1998), and makes use of a result by Ralaivola et al. (2006) that established the equivalence of learning under (standard) classification noise (CN) and under class-conditional classification noise (CCCN). Our work also relies on the PAC model for distribution learning (Kearns et al., 1994), including a long line of works on learning mixtures of distributions (see e.g. Dasgupta (1999); Arora and Kannan (2001); Vempala and Wang (2004); Feldman et al. (2008)). CNLearning Wefirst introduce PAC learning under classification noise (CN) (Angluin and Laird, 1987).",
        "context": null
    },
    {
        "title": "Learning mixtures of arbitrary gaussians",
        "author": [
            "S. Arora",
            "R. Kannan"
        ],
        "venue": "Proceedings of the Thirty-third Annual ACM Symposium on Theory of Computing. STOC \u201901. ACM, New York, NY, USA, 247\u2013257.",
        "citeRegEx": "Arora and Kannan,? 2001",
        "shortCiteRegEx": "Arora and Kannan",
        "year": 2001,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " 8 However, we now describe a different, \u201creverse\u201d reduction that instead assumes learnability of mixtures, and thus is applicable to more general Gaussians via known mixture learning algorithms (Dasgupta, 1999; Arora and Kannan, 2001; Feldman et al., 2006).",
        "context": null
    },
    {
        "title": "Combining labeled and unlabeled data with co-training",
        "author": [
            "A. Blum",
            "T.M. Mitchell"
        ],
        "venue": "Proceedings of the Eleventh Annual Conference on Computational Learning Theory, COLT 1998, Madison, Wisconsin, USA, July 24-26, 1998. 92\u2013100.",
        "citeRegEx": "Blum and Mitchell,? 1998",
        "shortCiteRegEx": "Blum and Mitchell",
        "year": 1998,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " Our model is also technically related to the one of co-training (Blum and Mitchell, 1998) in that the input x and the output y give two different views on the data, and they are conditionally independent given the unknown label z = c(x), which is also a crucial assumption for co-training (as well as various other latent variable models for inference and learning). 3In the work of Blum and Mitchell (1998), the authors showed that any CN learnable class is also learnable when the class-conditional noise rates satisfy \u03b70 + \u03b71 < 1.",
        "context": null
    },
    {
        "title": "Learning mixtures of gaussians",
        "author": [
            "S. Dasgupta"
        ],
        "venue": "40th Annual Symposium on Foundations of Computer Science, FOCS \u201999, 17-18 October, 1999, New York, NY, USA. 634\u2013644.",
        "citeRegEx": "Dasgupta,? 1999",
        "shortCiteRegEx": "Dasgupta",
        "year": 1999,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " 8 However, we now describe a different, \u201creverse\u201d reduction that instead assumes learnability of mixtures, and thus is applicable to more general Gaussians via known mixture learning algorithms (Dasgupta, 1999; Arora and Kannan, 2001; Feldman et al., 2006). Dasgupta (1999); Feldman et al. Dasgupta (1999); Feldman et al. (2006, 2008); Hsu and Kakade (2013)).",
        "context": null
    },
    {
        "title": "PAC learning with constant-partition classification noise and applications to decision tree induction",
        "author": [
            "S.E. Decatur"
        ],
        "venue": "Proceedings of the Fourteenth International Conference on Machine Learning. ICML \u201997. Morgan Kaufmann Publishers Inc., San Francisco, CA, USA, 83\u201391.",
        "citeRegEx": "Decatur,? 1997",
        "shortCiteRegEx": "Decatur",
        "year": 1997,
        "abstract": "",
        "full_text": "",
        "sentence": " First, our work is related to the results in PAC learning under classification noise (Angluin and Laird, 1987; Decatur, 1997; Kearns, 1998), and makes use of a result by Ralaivola et al.",
        "context": null
    },
    {
        "title": "Learningmixtures of product distributions over discrete domains",
        "author": [
            "J. Feldman",
            "R. O\u2019Donnell",
            "R.A. Servedio"
        ],
        "venue": "SIAM J. Comput",
        "citeRegEx": "Feldman et al\\.,? \\Q2008\\E",
        "shortCiteRegEx": "Feldman et al\\.",
        "year": 2008,
        "abstract": "",
        "full_text": "",
        "sentence": " Gaussians), it can be obtained using standard procedures (for instance, by truncating, or mixing with a small amount of the uniform distribution; see Feldman et al. (2006) for an example).",
        "context": null
    },
    {
        "title": "PAC learning axis-aligned mixtures of Gaussians with no separation assumption",
        "author": [
            "J. Feldman",
            "R.A. Servedio",
            "R. O\u2019Donnell"
        ],
        "venue": "In Learning Theory, 19th Annual Conference on Learning Theory, COLT",
        "citeRegEx": "Feldman et al\\.,? \\Q2006\\E",
        "shortCiteRegEx": "Feldman et al\\.",
        "year": 2006,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " 8 However, we now describe a different, \u201creverse\u201d reduction that instead assumes learnability of mixtures, and thus is applicable to more general Gaussians via known mixture learning algorithms (Dasgupta, 1999; Arora and Kannan, 2001; Feldman et al., 2006).",
        "context": null
    },
    {
        "title": "Decision theoretic generalizations of the PAC model for neural net and other learning applications",
        "author": [
            "D. Haussler"
        ],
        "venue": "Inf. Comput. 100, 1, 78\u2013150.",
        "citeRegEx": "Haussler,? 1992",
        "shortCiteRegEx": "Haussler",
        "year": 1992,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "Learning mixtures of spherical gaussians: moment methods and spectral decompositions",
        "author": [
            "D.J. Hsu",
            "S.M. Kakade"
        ],
        "venue": "Innovations in Theoretical Computer Science, ITCS \u201913, Berkeley, CA, USA, January 9-12, 2013. 11\u201320.",
        "citeRegEx": "Hsu and Kakade,? 2013",
        "shortCiteRegEx": "Hsu and Kakade",
        "year": 2013,
        "abstract": "This work provides a computationally efficient and statistically consistent\nmoment-based estimator for mixtures of spherical Gaussians. Under the condition\nthat component means are in general position, a simple spectral decomposition\ntechnique yields consistent parameter estimates from low-order observable\nmoments, without additional minimum separation assumptions needed by previous\ncomputationally efficient estimation procedures. Thus computational and\ninformation-theoretic barriers to efficient estimation in mixture models are\nprecluded when the mixture components have means in general position and\nspherical covariances. Some connections are made to estimation problems related\nto independent component analysis.",
        "full_text": "arXiv:1206.5766v4  [cs.LG]  28 Oct 2012\nLearning mixtures of spherical Gaussians:\nmoment methods and spectral decompositions\nDaniel Hsu and Sham M. Kakade\nMicrosoft Research New England\nOctober 30, 2012\nAbstract\nThis work provides a computationally e\ufb03cient and statistically consistent moment-based\nestimator for mixtures of spherical Gaussians. Under the condition that component means are\nin general position, a simple spectral decomposition technique yields consistent parameter esti-\nmates from low-order observable moments, without additional minimum separation assumptions\nneeded by previous computationally e\ufb03cient estimation procedures. Thus computational and\ninformation-theoretic barriers to e\ufb03cient estimation in mixture models are precluded when the\nmixture components have means in general position and spherical covariances. Some connections\nare made to estimation problems related to independent component analysis.\n1\nIntroduction\nThe Gaussian mixture model (Pearson, 1894; Titterington et al., 1985) is one of the most well-\nstudied and widely-used models in applied statistics and machine learning. An important special\ncase of this model (the primary focus of this work) restricts the Gaussian components to have\nspherical covariance matrices; this probabilistic model is closely related to the (non-probabilistic)\nk-means clustering problem (MacQueen, 1967).\nThe mixture of spherical Gaussians model is speci\ufb01ed as follows. Let wi be the probability of\nchoosing component i \u2208[k] := {1, 2, . . . , k}, let \u00b51, \u00b52, . . . , \u00b5k \u2208Rd be the component mean vectors,\nand let \u03c32\n1, \u03c32\n2, . . . , \u03c32\nk \u22650 be the component variances. De\ufb01ne\nw := [w1, w2, . . . , wk]\u22a4\u2208Rk,\nA := [\u00b51|\u00b52| \u00b7 \u00b7 \u00b7 |\u00b5k] \u2208Rd\u00d7k;\nso w is a probability vector, and A is the matrix whose columns are the component means. Let\nx \u2208Rk be the (observed) random vector given by\nx := \u00b5h + z,\nwhere h is the discrete random variable with Pr(h = i) = wi for i \u2208[k], and z is a random vector\nwhose conditional distribution given h = i (for some i \u2208[k]) is the multivariate Gaussian N(0, \u03c32\ni I)\nwith mean zero and covariance \u03c32\ni I.\nE-mail: {dahsu,skakade}@microsoft.com\n1\nThe estimation task is to accurately recover the model parameters (component means, variances,\nand mixing weights) {(\u00b5i, \u03c32\ni , wi) : i \u2208[k]} from independent copies of x.\nThis work gives a procedure for e\ufb03ciently and exactly recovering the parameters using a simple\nspectral decomposition of low-order moments of x, under the following condition.\nCondition 1 (Non-degeneracy). The component means span a k-dimensional subspace (i.e., the\nmatrix A has column rank k), and the vector w has strictly positive entries.\nThe proposed estimator is based on a spectral decomposition technique (Chang, 1996; Mossel and Roch,\n2006; Anandkumar et al., 2012b), and is easily stated in terms of exact population moments of the\nobserved x. With \ufb01nite samples, one can use a plug-in estimator based on empirical moments\nof x in place of exact moments.\nThese empirical moments converge to the exact moments at\na rate of O(n\u22121/2), where n is the sample size.\nAs discussed in Section 3, sample complexity\nbounds for accurate parameter estimation can be derived using matrix perturbation arguments\n(Anandkumar et al., 2012b). Since only low-order moments are required by the plug-in estimator,\nthe sample complexity is polynomial in the relevant parameters of the estimation problem.\nRelated work.\nThe \ufb01rst estimators for the Gaussian mixture models were based on the method-\nof-moments, as introduced by Pearson (1894) (see also Lindsay and Basak, 1993, and the refer-\nences therein). Roughly speaking, these estimators are based on \ufb01nding parameters under which\nthe Gaussian mixture distribution has moments approximately matching the observed empirical\nmoments. Finding these parameters typically involves solving systems of multivariate polynomial\nequations, which is typically computationally challenging. Besides this, the order of the moments\nof some of the early moment-based estimators were either growing with the dimension d or the\nnumber of components k, which is undesirable because the empirical estimates of such high-order\nmoments may only be reliable when the sample size is exponential in d or k. Both the compu-\ntational and sample complexity issues have been addressed in recent years, at least under various\nrestrictions.\nFor instance, several distance-based estimators require that the component means\nbe well-separated in Euclidean space, by at least some large factor times the directional stan-\ndard deviation of the individual component distributions (Dasgupta, 1999; Arora and Kannan,\n2001; Dasgupta and Schulman, 2007; Vempala and Wang, 2002; Chaudhuri and Rao, 2008), but\notherwise have polynomial computational and sample complexity. Some recent moment-based es-\ntimators avoid the minimum separation condition of distance-based estimators by requiring either\ncomputational or data resources exponential in the number of mixing components k (but not the\ndimension d) (Belkin and Sinha, 2010; Kalai et al., 2010; Moitra and Valiant, 2010) or by making\na non-degenerate multi-view assumption (Anandkumar et al., 2012b).\nBy contrast, the moment-based estimator described in this work does not require a minimum\nseparation condition, exponential computational or data resources, or non-degenerate multiple\nviews. Instead, it relies only on the non-degeneracy condition discussed above together with a\nspherical noise condition. The non-degeneracy condition is much weaker than an explicit minimum\nseparation condition because the parameters can be arbitrarily close to being degenerate, as long\nas the sample size grows polynomially with a natural quantity measuring this closeness to degen-\neracy (akin to a condition number). Like other moment-based estimators, the proposed estimator\nis based on solving multivariate polynomial equations, although these solutions can be found e\ufb03-\nciently because the problems are cast as eigenvalue decompositions of symmetric matrices, which\nare e\ufb03cient to compute.\n2\nRecent work of Moitra and Valiant (2010) demonstrates an information-theoretic barrier to\nestimation for general Gaussian mixture models.\nMore precisely, they construct a pair of one-\ndimensional mixtures of Gaussians (with separated component means) such that the statistical\ndistance between the two mixture distributions is exponentially small in the number of components.\nThis implies that in the worst case, the sample size required to obtain accurate parameter estimates\nmust grow exponentially with the number of components, even when the component distributions\nare non-negligibly separated. A consequence of the present work is that natural non-degeneracy\nconditions preclude these worst case scenarios. The non-degeneracy condition in this work is similar\nto one used for bypassing computational (cryptographic) barriers to estimation for hidden Markov\nmodels (Chang, 1996; Mossel and Roch, 2006; Hsu et al., 2012a; Anandkumar et al., 2012b).\nFinally, it is interesting to note that similar algebraic techniques have been developed for cer-\ntain models in independent component analysis (ICA) (Comon, 1994; Cardoso and Comon, 1996;\nHyv\u00a8arinen and Oja, 2000; Comon and Jutten, 2010; Arora et al., 2012) and other closely related\nproblems (Frieze et al., 1996; Nguyen and Regev, 2009). In contrast to the ICA setting, handling\nnon-spherical Gaussian noise for mixture models appears to be a more delicate issue. These con-\nnections and open problems are further discussed in Section 3.\n2\nMoment-based estimation\nThis section describes a method-of-moments estimator for the spherical Gaussian mixture model.\nThe following theorem is the main structural result that relates the model parameters to ob-\nservable moments.\nTheorem 1 (Observable moment structure). Assume Condition 1 holds. The average variance\n\u00af\u03c32 := Pk\ni=1 wi\u03c32\ni is the smallest eigenvalue of the covariance matrix E[(x \u2212E[x])(x \u2212E[x])\u22a4]. Let\nv \u2208Rd be any unit norm eigenvector corresponding to the eigenvalue \u00af\u03c32. De\ufb01ne\nM1 := E[x(v\u22a4(x \u2212E[x]))2] \u2208Rd,\nM2 := E[x \u2297x] \u2212\u00af\u03c32I \u2208Rd\u00d7d,\nM3 := E[x \u2297x \u2297x] \u2212\nd\nX\ni=1\n\u0000M1 \u2297ei \u2297ei + ei \u2297M1 \u2297ei + ei \u2297ei \u2297M1\n\u0001\n\u2208Rd\u00d7d\u00d7d\n(where \u2297denotes tensor product, and {e1, e2, . . . , ed} is the coordinate basis for Rd). Then\nM1 =\nk\nX\ni=1\nwi \u03c32\ni \u00b5i,\nM2 =\nk\nX\ni=1\nwi \u00b5i \u2297\u00b5i,\nM3 =\nk\nX\ni=1\nwi \u00b5i \u2297\u00b5i \u2297\u00b5i.\nRemark 1. We note that in the special case where \u03c32\n1 = \u03c32\n2 = \u00b7 \u00b7 \u00b7 = \u03c32\nk = \u03c32 (i.e., the mixture\ncomponents share a common spherical covariance matrix), the average variance \u00af\u03c32 is simply \u03c32,\nand M3 has a simpler form:\nM3 = E[x \u2297x \u2297x] \u2212\u03c32\nd\nX\ni=1\n\u0000E[x] \u2297ei \u2297ei + ei \u2297E[x] \u2297ei + ei \u2297ei \u2297E[x]\n\u0001\n\u2208Rd\u00d7d\u00d7d.\nThere is no need to refer to the eigenvectors of the covariance matrix or M1.\n3\nProof of Theorem 1. We \ufb01rst characterize the smallest eigenvalue of the covariance matrix of x,\nas well as all corresponding eigenvectors v. Let \u00af\u00b5 := E[x] = E[\u00b5h] = Pk\ni=1 wi\u00b5i. The covariance\nmatrix of x is\nE[(x \u2212\u00af\u00b5) \u2297(x \u2212\u00af\u00b5)] =\nk\nX\ni=1\nwi\n\u0012\n(\u00b5i \u2212\u00af\u00b5) \u2297(\u00b5i \u2212\u00af\u00b5) + \u03c32\ni I\n\u0013\n=\nk\nX\ni=1\nwi (\u00b5i \u2212\u00af\u00b5) \u2297(\u00b5i \u2212\u00af\u00b5) + \u00af\u03c32I.\nSince the vectors \u00b5i \u2212\u00af\u00b5 for i \u2208[k] are linearly dependent (Pk\ni=1 wi(\u00b5i \u2212\u00af\u00b5) = 0), the positive\nsemide\ufb01nite matrix Pk\ni=1 wi(\u00b5i\u2212\u00af\u00b5)\u2297(\u00b5i\u2212\u00af\u00b5) has rank r \u2264k\u22121. Thus, the d\u2212r smallest eigenvalues\nare exactly \u00af\u03c32, while all other eigenvalues are strictly larger than \u00af\u03c32. The strict separation of\neigenvalues implies that every eigenvector corresponding to \u00af\u03c32 is in the null space of Pk\ni=1 wi(\u00b5i \u2212\n\u00af\u00b5) \u2297(\u00b5i \u2212\u00af\u00b5); thus v\u22a4(\u00b5i \u2212\u00af\u00b5) = 0 for all i \u2208[k].\nNow we can express M1, M2, and M3 in terms of the parameters wi, \u00b5i, and \u03c32\ni . First,\nM1 = E[x(v\u22a4(x \u2212E[x]))2] = E[(\u00b5h + z)(v\u22a4(\u00b5h \u2212\u00af\u00b5 + z))2] = E[(\u00b5h + z)(v\u22a4z)2] = E[\u00b5h\u03c32\nh],\nwhere the last step uses the fact that z|h \u223cN(0, \u03c32\nhI), which implies that conditioned on h,\nE[(v\u22a4z)2|h] = \u03c32\nh and E[z(v\u22a4z)2|h] = 0. Next, observe that E[z \u2297z] = Pk\ni=1 wi\u03c32\ni I = \u00af\u03c32I, so\nM2 = E[x \u2297x] \u2212\u00af\u03c32I = E[\u00b5h \u2297\u00b5h] + E[z \u2297z] \u2212\u00af\u03c32I = E[\u00b5h \u2297\u00b5h] =\nk\nX\ni=1\nwi \u00b5i \u2297\u00b5i.\nFinally, for M3, we \ufb01rst observe that\nE[x \u2297x \u2297x] = E[\u00b5h \u2297\u00b5h \u2297\u00b5h] + E[\u00b5h \u2297z \u2297z] + E[z \u2297\u00b5h \u2297z] + E[z \u2297z \u2297\u00b5h]\n(terms such as E[\u00b5h \u2297\u00b5h \u2297z] and E[z \u2297z \u2297z] vanish because z|h \u223cN(0, \u03c32\nhI)). We now claim\nthat E[\u00b5h \u2297z \u2297z] = Pd\ni=1 M1 \u2297ei \u2297ei. This holds because\nE[\u00b5h \u2297z \u2297z] = E\n\u0002\nE[\u00b5h \u2297z \u2297z|h]\n\u0003\n= E\n\u0014\nE\n\u0014\nd\nX\ni,j=1\nzizj \u00b5h \u2297ei \u2297ej\n\f\f\fh\n\u0015\u0015\n= E\n\u0014 d\nX\ni=1\n\u03c32\nh \u00b5h \u2297ei \u2297ei\n\u0015\n=\nd\nX\ni=1\nM1 \u2297ei \u2297ei,\ncrucially using the fact that E[zizj|h] = 0 for i \u0338= j and E[z2\ni |h] = \u03c32\nh. By the same derivation, we\nhave E[z \u2297\u00b5h \u2297z] = Pd\ni=1 ei \u2297M1 \u2297ei and E[z \u2297z \u2297\u00b5h] = Pd\ni=1 ei \u2297ei \u2297M1. Therefore,\nM3 = E[x\u2297x\u2297x]\u2212\n\u0000E[\u00b5h\u2297z\u2297z]+E[z\u2297\u00b5h\u2297z]+E[z\u2297z\u2297\u00b5h]\n\u0001\n= E[\u00b5h\u2297\u00b5h\u2297\u00b5h] =\nk\nX\ni=1\nwi\u00b5i\u2297\u00b5i\u2297\u00b5i\nas claimed.\nTheorem 1 shows the relationship between (some functions of) the observable moments and the\ndesired parameters. A simple estimator based on this moment structure is given in the following the-\norem. For a third-order tensor T \u2208Rd\u00d7d\u00d7d, we de\ufb01ne T(\u03b7) := Pd\ni1=1\nPd\ni2=1\nPd\ni3=1 Ti1,i2,i3\u03b7i3 ei1 \u2297\nei2 \u2208Rd\u00d7d for any vector \u03b7 \u2208Rd.\n4\nTheorem 2 (Moment-based estimator). The following can be added to the results of Theorem 1.\nSuppose \u03b7\u22a4\u00b51, \u03b7\u22a4\u00b52, . . . , \u03b7\u22a4\u00b5k are distinct and non-zero (which is satis\ufb01ed almost surely, for in-\nstance, if \u03b7 is chosen uniformly at random from the unit sphere in Rd). Then the matrix\nMGMM(\u03b7) := M\u20201/2\n2\nM3(\u03b7)M\u20201/2\n2\nis diagonalizable (where \u2020 denotes the Moore-Penrose pseudoinverse); its non-zero eigenvalue /\neigenvector pairs (\u03bb1, v1), (\u03bb2, v2), . . . , (\u03bbk, vk) satisfy \u03bbi = \u03b7\u22a4\u00b5\u03c0(i) and M1/2\n2\nvi = si\u221aw\u03c0(i)\u00b5\u03c0(i) for\nsome permutation \u03c0 on [k] and signs s1, s2, . . . , sk \u2208{\u00b11}. The \u00b5i, \u03c32\ni , and wi are recovered (up\nto permutation) with\n\u00b5\u03c0(i) =\n\u03bbi\n\u03b7\u22a4M1/2\n2\nvi\nM1/2\n2\nvi,\n\u03c32\ni = 1\nwi\ne\u22a4\ni A\u2020M1,\nwi = e\u22a4\ni A\u2020E[x].\nProof. By Theorem 1,\nM1 = A diag(\u03c32\n1, \u03c32\n2, . . . , \u03c32\nk)w,\nM2 = A diag(w)A\u22a4,\nM3(\u03b7) = A diag(w)D1(\u03b7)A\u22a4,\nwhere D1(\u03b7) := diag(\u03b7\u22a4\u00b51, \u03b7\u22a4\u00b52, . . . , \u03b7\u22a4\u00b5k).\nLet USR\u22a4be the thin SVD of A diag(w)1/2 (U \u2208Rd\u00d7k, S \u2208Rk\u00d7k, and R \u2208Rk\u00d7k), so M2 =\nUS2U \u22a4and M\u20201/2\n2\n= US\u22121U \u22a4since A diag(w)1/2 has rank k by assumption. Also by assumption,\nthe diagonal entries of D1(\u03b7) are distinct and non-zero. Therefore, every non-zero eigenvalue of\nthe symmetric matrix MGMM(\u03b7) = UR\u22a4D1(\u03b7)RU \u22a4has geometric multiplicity one. Indeed, these\nnon-zero eigenvalues \u03bbi are the diagonal entries of D1(\u03b7) (up to some permutation \u03c0 on [k]), and\nthe corresponding eigenvectors vi are the columns of UR\u22a4up to signs:\n\u03bbi = \u03b7\u22a4\u00b5\u03c0(i)\nand\nvi = siUR\u22a4e\u03c0(i).\nNow, since\nM1/2\n2\nvi = si\npw\u03c0(i)\u00b5\u03c0(i),\n\u03bbi\n\u03b7\u22a4M1/2\n2\nvi\n=\n\u03b7\u22a4\u00b5\u03c0(i)\nsi\u221aw\u03c0(i)\u03b7\u22a4\u00b5\u03c0(i)\n=\n1\nsi\u221aw\u03c0(i)\n,\nit follows that\n\u00b5\u03c0(i) =\n\u03bbi\n\u03b7\u22a4M1/2\n2\nvi\nM1/2\n2\nvi,\ni \u2208[k].\nThe claims regarding \u03c32\ni and wi are also evident from the structure of M1 and E[x] = Aw.\nAn e\ufb03ciently computable plug-in estimator can be derived from Theorem 2. We state one such\nalgorithm (called LearnGMM) in Appendix C; for simplicity, we restrict to the case where the\ncomponents share the same common spherical covariance, i.e., \u03c32\n1 = \u03c32\n2 = \u00b7 \u00b7 \u00b7 = \u03c32\nk = \u03c32. The\nfollowing theorem provides a sample complexity bound for accurate estimation of the component\nmeans. Since only low-order moments are used, the sample complexity is polynomial in the relevant\nparameters of the estimation problem (in particular, the dimension d and the number of mixing\ncomponents k). It is worth noting that the polynomial is quadratic in the inverse accuracy param-\neter 1/\u03b5; this owes to the fact that the empirical moments converge to the population moments at\nthe usual n\u22121/2 rate as per the central limit theorem.\n5\nTheorem 3 (Finite sample bound). There exists a polynomial poly(\u00b7) such that the following holds.\nLet M2 be the matrix de\ufb01ned in Theorem 2, and \u03c2t[M2] be its t-th largest singular value (for t \u2208[k]).\nLet bmax := maxi\u2208[k] \u2225\u00b5i\u22252 and wmin := mini\u2208[k] wi. Pick any \u03b5, \u03b4 \u2208(0, 1). Suppose the sample size\nn satis\ufb01es\nn \u2265poly\n\u0010\nd, k, 1/\u03b5, log(1/\u03b4), 1/wmin, \u03c21[M2]/\u03c2k[M2], b2\nmax/\u03c2k[M2], \u03c32/\u03c2k[M2],\n\u0011\n.\nThen with probability at least 1 \u2212\u03b4 over the random sample and the internal randomness of the\nalgorithm, there exists a permutation \u03c0 on [k] such that the {\u02c6\u00b5i : i \u2208[k]} returned by LearnGMM\nsatisfy\n\u2225\u02c6\u00b5\u03c0(i) \u2212\u00b5i\u22252 \u2264\n\u0010\n\u2225\u00b5i\u22252 +\np\n\u03c21[M2]\n\u0011\n\u03b5\nfor all i \u2208[k].\nThe proof of Theorem 3 is given in Appendix C. It is also easy to obtain accuracy guarantees for\nestimating \u03c32 and w. The role of Condition 1 enters by observing that \u03c2k[M2] = 0 if either rank(A) <\nk or wmin = 0, as M2 = A diag(w)A\u22a4. The sample complexity bound then becomes trivial in this\ncase, as the bound grows with 1/\u03c2k[M2] and 1/wmin. Finally, we also note that LearnGMM is just\none (easy to state) way to obtain an e\ufb03cient algorithm based on the structure in Theorem 1. It is\nalso possible to use, for instance, simultaneous diagonalization techniques (Bunse-Gerstner et al.,\n1993) or orthogonal tensor decompositions (Anandkumar et al., 2012a) to extract the parameters\nfrom (estimates of) M2 and M3; these alternative methods are more robust to sampling error, and\nare therefore recommended for practical implementation.\n3\nDiscussion\nMulti-view methods and a simpler algorithm in higher dimensions.\nSome previous work\nof the authors on moment-based estimators for the Gaussian mixture model relies on a non-\ndegenerate multi-view assumption (Anandkumar et al., 2012b).\nIn this work, it is shown that\nif each mixture component i has an axis-aligned covariance \u03a3i := diag(\u03c32\n1,i, \u03c32\n2,i, . . . , \u03c32\nd,i), then un-\nder some additional mild assumptions (which ultimately require d > k), a moment-based method\ncan be used to estimate the model parameters. The idea is to partition the coordinates [d] into\nthree groups, inducing multiple \u201cviews\u201d x = (x1, x2, x3) with each xt \u2208Rdt for some dt \u2265k such\nthat x1, x2, and x3 are conditionally independent given h. When the matrix of conditional means\nAt := [E[xt|h = 1]|E[xt|h = 2]| \u00b7 \u00b7 \u00b7 |E[xt|h = k]] \u2208Rdt\u00d7k for each view t \u2208{1, 2, 3} has rank k,\nthen an e\ufb03cient technique similar to that described in Theorem 2 will recover the parameters.\nTherefore, the problem is reduced to partitioning the coordinates so that the resulting matrices At\nhave rank k.\nIn the case where each component covariance is spherical (\u03a3i = \u03c32\ni I), we may simply apply\na random rotation to x before (arbitrarily) splitting into the three views.\nLet \u02dcx := \u0398x for a\nrandom orthogonal matrix \u0398 \u2208Rd\u00d7d, and partition the coordinates so that \u02dcx = (\u02dcx1, \u02dcx2, \u02dcx3) with\n\u02dcxt \u2208Rdt and dt \u2265k. By the rotational invariance of the multivariate Gaussian distribution, the\ndistribution of \u02dcx is still a mixture of spherical Gaussians, and moreover, the matrix of conditional\nmeans \u02dcAt := [E[\u02dcxt|h = 1]|E[\u02dcxt|h = 2]| \u00b7 \u00b7 \u00b7 |E[\u02dcxt|h = k]] \u2208Rdt\u00d7k for each view \u02dcxt has rank k with\nprobability 1. To see this, observe that a random rotation in Rd followed by a restriction to dt\n6\ncoordinates is simply a random projection from Rd to Rdt, and that a random projection of a linear\nsubspace of dimension k (in particular, the range of A) to Rdt is almost surely injective as long as\ndt \u2265k. Therefore it is su\ufb03cient to require d \u22653k so that it is possible to split \u02dcx into three views,\neach of dimension dt \u2265k. To guarantee that the k-th largest singular value of each \u02dcAt is bounded\nbelow in terms of the k-th largest singular value of A (with high probability), we may require d to\nbe somewhat larger: O(k log k) certainly works (see Appendix B), and we conjecture c \u00b7 k for some\nc > 3 is in fact su\ufb03cient.\nSpectral decomposition approaches for ICA.\nThe Gaussian mixture model shares some simi-\nlarities to a standard model for independent component analysis (ICA) (Comon, 1994; Cardoso and Comon,\n1996; Hyv\u00a8arinen and Oja, 2000; Comon and Jutten, 2010). Here, let h \u2208Rk be a random vector\nwith independent entries, and let z \u2208Rk be multivariate Gaussian random vector. We think of h\nas an unobserved signal and z as noise. The observed random vector is\nx := Ah + z\nfor some A \u2208Rk\u00d7k, where h and z are assumed to be independent. (For simplicity, we only consider\nsquare A, although it is easy to generalize to A \u2208Rd\u00d7k for d \u2265k.)\nIn contrast to this ICA model, the spherical Gaussian mixture model is one where h would take\nvalues in {e1, e2, . . . , ek}, and the covariance of z (given h) is spherical.\nFor ICA, a spectral decomposition approach related to the one described in Theorem 2 can be\nused to estimate the columns of A (up to scale), without knowing the noise covariance E[zz\u22a4]. Such\nan estimator can be obtained from Theorem 4 using techniques commonplace in the ICA literature;\nits proof is given in Appendix A for completeness.\nTheorem 4. In the ICA model described above, assume E[hi] = 0, E[h2\ni ] = 1, and \u03bai := E[h4\ni ]\u22123 \u0338=\n0 ( i.e., the excess kurtosis is non-zero), and that A is non-singular. De\ufb01ne f : Rk \u2192R by\nf(\u03b7) := 12\u22121\u0000m4(\u03b7) \u22123m2(\u03b7)2\u0001\nwhere mp(\u03b7) := E[(\u03b7\u22a4x)p]. Suppose \u03c6 \u2208Rk and \u03c8 \u2208Rk are such that (\u03c6\u22a4\u00b51)2\n(\u03c8\u22a4\u00b51)2 , (\u03c6\u22a4\u00b52)2\n(\u03c8\u22a4\u00b52)2 , . . . , (\u03c6\u22a4\u00b5k)2\n(\u03c8\u22a4\u00b5k)2 \u2208\nR are distinct. Then the matrix\nMICA(\u03c6, \u03c8) :=\n\u0000\u22072f(\u03c6)\n\u0001\u0000\u22072f(\u03c8)\n\u0001\u22121\nis diagonalizable; the eigenvalues are (\u03c6\u22a4\u00b51)2\n(\u03c8\u22a4\u00b51)2 , (\u03c6\u22a4\u00b52)2\n(\u03c8\u22a4\u00b52)2 , . . . , (\u03c6\u22a4\u00b5k)2\n(\u03c8\u22a4\u00b5k)2 and each have geometric multi-\nplicity one, and the corresponding eigenvectors are \u00b51, \u00b52, . . . , \u00b5k (up to scaling and permutation).\nAgain, choosing \u03c6 and \u03c8 as random unit vectors ensures the distinctness assumption is satis\ufb01ed\nalmost surely, and a \ufb01nite sample analysis can be given using standard matrix perturbation tech-\nniques (Anandkumar et al., 2012b). A number of related deterministic algorithms based on alge-\nbraic techniques are discussed in the text of Comon and Jutten (2010). Recent work of Arora et al.\n(2012) provides a \ufb01nite sample complexity analysis for an e\ufb03cient estimator based on local search.\nNon-degeneracy.\nThe non-degeneracy assumption (Condition 1) is quite natural, and its has\nthe virtue of permitting tractable and consistent estimators. Although previous work has typically\ntied it with additional assumptions, this work shows that they are largely unnecessary.\n7\nOne drawback of Condition 1 is that it prevents the straightforward application of these tech-\nniques to certain problem domains (e.g., automatic speech recognition (ASR), where the number\nof mixture components is typically enormous, but the dimension of observations is relatively small;\nalternatively, the span of the means has dimension < k). To compensate, one may require mul-\ntiple views, which are granted by a number of models, including hidden Markov models used in\nASR (Hsu et al., 2012a; Anandkumar et al., 2012b), and combining these views in a tensor prod-\nuct fashion (Allman et al., 2009). This increases the complexity of the estimator, but that may\nbe inevitable as estimation for certain non-singular models is conjectured to be computationally\nintractable (Mossel and Roch, 2006).\nAcknowledgements\nWe thank Dean Foster and Anima Anandkumar for helpful insights. We also thank Rong Ge and\nSanjeev Arora for discussions regarding their recent work on ICA.\nReferences\nE. S. Allman, C. Matias, and J. A. Rhodes. Identi\ufb01ability of parameters in latent structure models\nwith many observed variables. The Annals of Statistics, 37(6A):3099\u20133132, 2009.\nA. Anandkumar, R. Ge, D. Hsu, S. M. Kakade, and M. Telgarsky.\nTensor decompositions for\nlearning latent variable models, 2012a. Manuscript.\nA. Anandkumar, D. Hsu, and S. M. Kakade. A method of moments for mixture models and hidden\nMarkov models. In COLT, 2012b.\nS. Arora and R. Kannan. Learning mixtures of arbitrary Gaussians. In STOC, 2001.\nS. Arora, R. Ge, A. Moitra, and S. Sachdeva. Provable ICA with unknown Gaussian noise, and\nimplications for Gaussian mixtures and autoencoders. In NIPS, 2012.\nM. Belkin and K. Sinha. Polynomial learning of distribution families. In FOCS, 2010.\nA. Bunse-Gerstner, R. Byers, and V. Mehrmann. Numerical methods for simultaneous diagonal-\nization. SIAM Journal on Matrix Analysis and Applications, 14(4):927\u2013949, 1993.\nJ.-F. Cardoso and P. Comon. Independent component analysis, a survey of some algebraic methods.\nIn IEEE International Symposium on Circuits and Systems Circuits and Systems Connecting the\nWorld, 1996.\nJ. T. Chang.\nFull reconstruction of Markov models on evolutionary trees: Identi\ufb01ability and\nconsistency. Mathematical Biosciences, 137:51\u201373, 1996.\nK. Chaudhuri and S. Rao.\nLearning mixtures of product distributions using correlations and\nindependence. In COLT, 2008.\nP. Comon. Independent component analysis, a new concept?\nSignal Processing, 36(3):287\u2013314,\n1994.\n8\nP. Comon and C. Jutten. Handbook of Blind Source Separation: Independent Component Analysis\nand Applications. Academic Press. Elsevier, 2010.\nS. Dasgupta. Learning mixutres of Gaussians. In FOCS, 1999.\nS. Dasgupta and A. Gupta.\nAn elementary proof of a theorem of Johnson and Lindenstrauss.\nRandom Structures and Algorithms, 22(1):60\u201365, 2003.\nS. Dasgupta and L. Schulman. A probabilistic analysis of EM for mixtures of separated, spherical\nGaussians. Journal of Machine Learning Research, 8(Feb):203\u2013226, 2007.\nA. M. Frieze, M. Jerrum, and R. Kannan. Learning linear transformations. In FOCS, 1996.\nD. Hsu, S. M. Kakade, and T. Zhang.\nAn analysis of random design linear regression, 2011.\narXiv:1106.2363.\nD. Hsu, S. M. Kakade, and T. Zhang. A spectral algorithm for learning hidden Markov models.\nJournal of Computer and System Sciences, 78(5):1460\u20131480, 2012a.\nD. Hsu, S. M. Kakade, and T. Zhang. Tail inequalities for sums of random matrices that depend\non the intrinsic dimension. Electronic Communications in Probability, 17(14):1\u201313, 2012b.\nA. Hyv\u00a8arinen and E. Oja. Independent component analysis: algorithms and applications. Neural\nNetworks, 13(4\u20135):411\u2013430, 2000.\nA. T. Kalai, A. Moitra, and G. Valiant. E\ufb03ciently learning mixtures of two Gaussians. In STOC,\n2010.\nB. Laurent and P. Massart. Adaptive estimation of a quadratic functional by model selection. The\nAnnals of Statistics, 28(5):1302\u20131338, 2000.\nB. G. Lindsay and P. Basak. Multivariate normal mixtures: a fast consistent method. Journal of\nthe American Statistical Association, 88(422):468\u2013476, 1993.\nA. Litvak, A. Pajor, M. Rudelson, and N. Tomczak-Jaegermann. Smallest singular values of random\nmatrices and geometry of random polytopes. Advances in Mathematics, 195:491\u2013523, 2005.\nJ. B. MacQueen. Some methods for classi\ufb01cation and analysis of multivariate observations. In\nProceedings of the \ufb01fth Berkeley Symposium on Mathematical Statistics and Probability, volume 1,\npages 281\u2013297. University of California Press, 1967.\nA. Moitra and G. Valiant. Settling the polynomial learnability of mixtures of Gaussians. In FOCS,\n2010.\nE. Mossel and S. Roch. Learning nonsingular phylogenies and hidden Markov models. Annals of\nApplied Probability, 16(2):583\u2013614, 2006.\nP. Q. Nguyen and O. Regev. Learning a parallelepiped: Cryptanalysis of GGH and NTRU signa-\ntures. Journal of Cryptology, 22(2):139\u2013160, 2009.\nK. Pearson. Contributions to the mathematical theory of evolution. Philosophical Transactions of\nthe Royal Society, London, A., page 71, 1894.\n9\nG. Pisier. The volume of convex bodies and Banach space geometry. Cambridge University Press,\n1989.\nG. W. Stewart and Ji-Guang Sun. Matrix Perturbation Theory. Academic Press, 1990.\nD. M. Titterington, A. F. M. Smith, and U. E. Makov. Statistical analysis of \ufb01nite mixture distri-\nbutions. Wiley, 1985.\nS. Vempala and G. Wang. A spectral algorithm for learning mixtures of distributions. In FOCS,\n2002.\nA\nConnection to independent component analysis\nProof of Theorem 4. It can be shown that\nm2(\u03b7) = E[(\u03b7\u22a4Ah)2] + E[(\u03b7\u22a4z)2],\nm4(\u03b7) = E[(\u03b7\u22a4Ah)4] \u22123E[(\u03b7\u22a4Ah)2]2 + 3m2(\u03b7)2.\nBy the assumptions,\nE[(\u03b7\u22a4Ah)4] =\nk\nX\ni=1\n(\u03b7\u22a4\u00b5i)4E[h4\ni ] + 3\nX\ni\u0338=j\n(\u03b7\u22a4\u00b5i)2(\u03b7\u22a4\u00b5j)2\n=\nk\nX\ni=1\n\u03bai(\u03b7\u22a4\u00b5i)4 + 3\nX\ni,j\n(\u03b7\u22a4\u00b5i)2(\u03b7\u22a4\u00b5j)2\n=\nk\nX\ni=1\n\u03bai(\u03b7\u22a4\u00b5i)4 + 3E[(\u03b7\u22a4Ah)2]2,\nand therefore\nf(\u03b7) = 12\u22121\u0000E[(\u03b7\u22a4Ah)4] \u22123E[(\u03b7\u22a4Ah)2]2\u0001\n= 12\u22121\nk\nX\ni=1\n\u03bai(\u03b7\u22a4\u00b5i)4.\nThe Hessian of f is given by\n\u22072f(\u03b7) =\nk\nX\ni=1\n\u03bai(\u03b7\u22a4\u00b5i)2\u00b5i\u00b5\u22a4\ni .\nDe\ufb01ne the diagonal matrices\nK := diag(\u03ba1, \u03ba2, . . . , \u03bak),\nD2(\u03b7) := diag((\u03b7\u22a4\u00b51)2, (\u03b7\u22a4\u00b52)2, . . . , (\u03b7\u22a4\u00b5k)2)\nand observe that\n\u22072f(\u03b7) = AKD2(\u03b7)A\u22a4.\nBy assumption, the diagonal entries of D2(\u03c6)D2(\u03c8)\u22121 are distinct, and therefore\nMICA(\u03c6, \u03c8) =\n\u0000\u22072f(\u03c6)\n\u0001\u0000\u22072f(\u03c8)\n\u0001\u22121 = AD2(\u03c6)D2(\u03c8)\u22121A\u22121\nis diagonalizable, and every eigenvalue has geometric multiplicity one.\n10\nB\nIncoherence and random rotations\nThe multi-view technique of Anandkumar et al. (2012b) can be used to estimate mixtures of product\ndistributions, which include, as special cases, mixtures of Gaussians with axis-aligned covariances\n\u03a3i = diag(\u03c32\n1,i, \u03c32\n2,i, . . . , \u03c32\nd,i). Spherical covariances \u03a3i = \u03c32\ni I are, of course, also axis-aligned. The\nidea is to randomly partition the coordinates [d] into three groups, inducing multiple \u201cviews\u201d x =\n(x1, x2, x3) with each xt \u2208Rdt for some dt \u2265k such that x1, x2, and x3 are conditionally independent\ngiven h. When the matrix of conditional means At := [E[xt|h = 1]|E[xt|h = 2]| \u00b7 \u00b7 \u00b7 |E[xt|h = k]] \u2208\nRdt\u00d7k for each view t \u2208{1, 2, 3} has rank k, then an e\ufb03cient technique similar to that described in\nTheorem 2 will recover the parameters (for details, see Anandkumar et al., 2012b,a).\nAnandkumar et al. (2012b) show that if A has rank k and also satis\ufb01es a mild incoherence\ncondition, then a random partitioning guarantees that each At has rank k, and lower-bounds the\nk-th largest singular value of each At by that of A.\nThe condition is similar to the spreading\ncondition of Chaudhuri and Rao (2008).\nDe\ufb01ne coherence(A) := maxi\u2208[d]{e\u22a4\ni \u03a0Aei} to be the largest diagonal entry of the ortho-projector\n\u03a0A to the range of A. When A has rank k, we have coherence(A) \u2208[k/d, 1]; it is maximized when\nrange(A) = span{e1, e2, . . . , ek} and minimized when the range is spanned by a subset of the\nHadamard basis of cardinality k. Roughly speaking, if the matrix of conditional means has low\ncoherence, then its full-rank property is witnessed by many partitions of [d]; this is made formal in\nthe following lemma.\nLemma 1. Assume A has rank k and that coherence(A) \u2264(\u03b52/6)/ ln(3k/\u03b4) for some \u03b5, \u03b4 \u2208(0, 1).\nWith probability at least 1 \u2212\u03b4, a random partitioning of the dimensions [d] into three groups (for\neach i \u2208[d], independently pick t \u2208{1, 2, 3} uniformly at random and put i in group t) has the\nfollowing property. For each t \u2208{1, 2, 3}, the matrix At obtained by selecting the rows of A in group\nt has full column rank, and the k-th largest singular value of At is at least\np\n(1 \u2212\u03b5)/3 times that\nof A.\nFor a mixture of spherical Gaussians, one can randomly rotate x before applying the random\ncoordinate partitioning. This is because if \u0398 \u2208Rd\u00d7d is an orthogonal matrix, then the distribution\nof \u02dcx := \u0398x is also a mixture of spherical Gaussians. Its matrix of conditional means is given by\n\u02dcA := \u0398A. The following lemma implies that multiplying a tall matrix A by a random rotation \u0398\ncauses the product to have low coherence.\nLemma 2 (Hsu et al., 2011). Let A \u2208Rd\u00d7k be a \ufb01xed matrix with rank k, and let \u0398 \u2208Rd\u00d7d\nbe chosen uniformly at random among all orthogonal d \u00d7 d matrices. For any \u03b7 \u2208(0, 1), with\nprobability at least 1 \u2212\u03b7, the matrix \u02dcA := \u0398A satis\ufb01es\ncoherence( \u02dcA) \u2264k +\np\n2k ln(d/\u03b7) + 2 ln(d/\u03b7)\nd\n\u00001 \u22121/(4d) \u22121/(360d3)\n\u00012 .\nTake \u03b7 from Lemma 2 and \u03b5, \u03b4 from Lemma 1 to be constants. Then the incoherence condition\nof Lemma 1 is satsi\ufb01ed provided that d \u2265c \u00b7 (k log k) for some positive constant c.\nC\nLearning algorithm and \ufb01nite sample analysis\nIn this section, we state and analyze a learning algorithm based on the estimator from Theorem 2,\nwhich assumed availability of exact moments of x. The proposed algorithm only uses a \ufb01nite sample\n11\nto estimate moments, and also explicitly deals with the eigenvalue separation condition assumed\nin Theorem 2 via internal randomization.\nC.1\nNotation\nFor a matrix X \u2208Rm\u00d7m, we use \u03c2t[X] to denote the t-th largest singular value of a matrix X, and\n\u2225X\u22252 to denote its spectral norm (so \u2225X\u22252 = \u03c21[X]).\nFor a third-order tensor Y \u2208Rm\u00d7m\u00d7m and U, V, W \u2208Rm\u00d7n, we use the notation Y [U, V, W] \u2208\nRn\u00d7n\u00d7n to denote the third-order tensor given by\nY [U, V, W]j1,j2,j3 =\nX\n1\u2264i1,i2,i3\u2264m\nUi1,j1Vi2,j2Wi3,j3Yi1,i2,i3,\n\u2200j1, j2, j3 \u2208[n].\nNote that this is the analogue of U \u22a4XV \u2208Rn\u00d7n for a matrix X \u2208Rm\u00d7m and U, V \u2208Rm\u00d7n. For\nY \u2208Rm\u00d7m\u00d7m, we use \u2225Y \u22252 to denote its operator (or supremum) norm \u2225Y \u22252 := sup{|Y [u, v, w]| :\nu, v, w \u2208Rm, \u2225u\u22252 = \u2225v\u22252 = \u2225w\u22252 = 1}.\nC.2\nAlgorithm\nThe proposed algorithm, called LearnGMM, is described in Figure 1. The algorithm essentially\nimplements the decomposition strategy in Theorem 2 using plug-in moments.\nTo simplify the\nanalysis, we split our sample (say, initially of size 2n) in two: we use the \ufb01rst half for empirical\nmoments (\u02c6\u00b5 and c\nM2) used in constructing \u02c6\u03c32, c\nM2, c\nW, and bB; and we use the second half for\nempirical moments (c\nW \u22a4\u02c6\u00b5 and c\nM3[c\nW, c\nW, c\nW] used in constructing c\nM3[c\nW, c\nW, c\nW]. Observe that\nthis ensures c\nM3 is independent of c\nW.\nLet {(xi, hi) : i \u2208[n]} be n i.i.d. copies of (x, h), and write S := {x1, x2, . . . , xn}. Let S be an\nindependent copy of S. Furthermore, de\ufb01ne the following moments and empirical moments:\n\u00b5 := E[x],\nM2 := E[xx\u22a4],\nM3 := E[x \u2297x \u2297x],\n\u02c6\u00b5 := 1\n|S|\nX\nx\u2208S\nx,\nc\nM2 := 1\n|S|\nX\nx\u2208S\nxx\u22a4,\nc\nM3 := 1\n|S|\nX\nx\u2208S\nx \u2297x \u2297x,\n\u02c6\u00b5 := 1\n|S|\nX\nx\u2208S\nx.\nSo S represents the \ufb01rst half of the sample, and S represents the second half of the sample.\nC.3\nStructure of the moments\nWe \ufb01rst recall the basic structure of the moments \u00b5, M2, and M3 as established in Theorem 2; for\nsimplicity, we restrict to the special case where \u03c32\n1 = \u03c32\n2 = \u00b7 \u00b7 \u00b7 = \u03c32\nk = \u03c32.\nLemma 3 (Structure of moments).\n\u00b5 =\nk\nX\ni=1\nwi\u00b5i,\nM2 =\nk\nX\ni=1\nwi\u00b5i\u00b5\u22a4\ni + \u03c32I,\nM3 =\nk\nX\ni=1\nwi\u00b5i \u2297\u00b5i \u2297\u00b5i + \u03c32\nd\nX\nj=1\n\u0010\n\u00b5 \u2297ej \u2297ej + ej \u2297\u00b5 \u2297ej + ej \u2297ej \u2297\u00b5\n\u0011\n.\n12\nLearnGMM\n1. Using the \ufb01rst half of the sample, compute empirical mean \u02c6\u00b5 and empirical second-order\nmoments c\nM2.\n2. Let \u02c6\u03c32 be the k-th largest eigenvalue of the empirical covariance matrix c\nM2 \u2212\u02c6\u00b5\u02c6\u00b5\u22a4.\n3. Let c\nM2 be the best rank-k approximation to c\nM2 \u2212\u02c6\u03c32I\nc\nM2 := arg\nmin\nX\u2208Rd\u00d7d:rank(X)\u2264k \u2225( c\nM2 \u2212\u02c6\u03c32I) \u2212X\u22252\nwhich can be obtained via the singular value decomposition.\n4. Let bU \u2208Rd\u00d7k be the matrix of left orthonormal singular vectors of c\nM2.\n5. Let c\nW := bU(bU \u22a4c\nM2 bU)\u20201/2, where X\u2020 denotes the Moore-Penrose pseudoinverse of a\nmatrix X.\nAlso de\ufb01ne bB := bU(bU \u22a4c\nM2 bU)1/2.\n6. Using the second half of the sample, compute whitened empirical averages c\nW \u22a4\u02c6\u00b5 and\nthird-order moments c\nM3[c\nW, c\nW, c\nW].\n7. Let c\nM3[c\nW, c\nW, c\nW] := c\nM3[c\nW, c\nW, c\nW] \u2212\u02c6\u03c32 Pd\ni=1\n\u0000(c\nW \u22a4\u02c6\u00b5) \u2297(c\nW \u22a4ei) \u2297(c\nW \u22a4ei) + (c\nW \u22a4ei) \u2297\n(c\nW \u22a4\u02c6\u00b5) \u2297(c\nW \u22a4ei) + (c\nW \u22a4ei) \u2297(c\nW \u22a4ei) \u2297(c\nW \u22a4\u02c6\u00b5)\n\u0001\n.\n8. Repeat the following steps t times (where t := \u2308log2(1/\u03b4)\u2309for con\ufb01dence 1 \u2212\u03b4):\n(a) Choose \u03b8 \u2208Rk uniformly at random from the unit sphere in Rk.\n(b) Let {(\u02c6vi, \u02c6\u03bbi) : i \u2208[k]} be the eigenvector/eigenvalue pairs of c\nM3[c\nW, c\nW, c\nW\u03b8].\nRetain the results for which min\n\u0000{|\u02c6\u03bbi \u2212\u02c6\u03bbj| : i \u0338= j} \u222a{|\u02c6\u03bbi| : i \u2208[k]}\n\u0001\nis largest.\n9. Return the parameter estimates \u02c6\u03c32,\n\u02c6\u00b5i :=\n\u02c6\u03bbi\n\u03b8\u22a4\u02c6vi\nbB\u02c6vi,\ni \u2208[k],\n\u02c6w := [\u02c6\u00b51|\u02c6\u00b52| \u00b7 \u00b7 \u00b7 |\u02c6\u00b5k]\u2020\u02c6\u00b5.\nFigure 1: Algorithm for learning mixtures of Gaussians with common spherical covariance.\n13\nC.4\nConcentration behavior of empirical quantities\nIn this subsection, we prove concentration properties of empirical quantities based on S; clearly\nthe same properties hold for S.\nLet Si := {xj \u2208S : hj = i} and \u02c6wi := |Si|/|S| for i \u2208[k]. Also, de\ufb01ne the following (empirical)\nconditional moments:\n\u00b5i := E[x|h = i],\nM2,i := E[xx\u22a4|h = i],\nM3,i := E[x \u2297x \u2297x|h = i],\n\u02c6\u00b5i :=\n1\n|Si|\nX\nx\u2208Si\nx,\nc\nM2,i :=\n1\n|Si|\nX\nx\u2208Si\nxx\u22a4,\nc\nM3,i :=\n1\n|Si|\nX\nx\u2208Si\nx \u2297x \u2297x.\nLemma 4 (Concentration of proportions). Pick any \u03b4 \u2208(0, 1/2). With probability at least 1 \u22122\u03b4,\n| \u02c6wi \u2212wi| \u2264\nr\n2wi(1 \u2212wi) ln(2k/\u03b4)\nn\n+ 2 ln(2k/\u03b4)\n3n\n,\n\u2200i \u2208[k];\n\u0012 k\nX\ni=1\n( \u02c6wi \u2212wi)2\n\u00131/2\n\u22641 +\np\nln(1/\u03b4)\n\u221an\n.\nProof. The \ufb01rst inequality follows from Bernstein\u2019s inequality and a union bound. The second in-\nequality follows from a simple application of McDiarmid\u2019s inequality (see Hsu et al., 2012a, Propo-\nsition 19).\nLemma 5 (Concentration of per-component empirical moments). Pick any \u03b4 \u2208(0, 1) and any\nmatrix R \u2208Rd\u00d7r of rank r.\n1. First-order moments: with probability at least 1 \u2212\u03b4,\n\u2225R\u22a4(\u02c6\u00b5i \u2212\u00b5i)\u22252 \u2264\u03c3\u2225R\u22252\ns\nr + 2\np\nr ln(k/\u03b4) + 2 ln(k/\u03b4)\n\u02c6win\n,\n\u2200i \u2208[k].\n2. Second-order moments: with probability at least 1 \u2212\u03b4,\n\u2225R\u22a4( c\nM2,i \u2212M2,i)R\u22252 \u2264\u03c32\u2225R\u22252\n2\n s\n128(r ln 9 + ln(2k/\u03b4))\n\u02c6win\n+ 4(r ln 9 + ln(2k/\u03b4))\n\u02c6win\n!\n+ 2\u03c3\u2225R\u22a4\u00b5i\u22252\u2225R\u22252\ns\nr + 2\np\nr ln(2k/\u03b4) + 2 ln(2k/\u03b4)\n\u02c6win\n,\n\u2200i \u2208[k].\n3. Third-order moments: with probability at least 1 \u2212\u03b4,\n\u2225( c\nM3,i \u2212M3,i)[R, R, R]\u22252 \u2264\u03c33\u2225R\u22253\n2\ns\n108e3\u2308r ln 13 + ln(3k/\u03b4)\u23093\n\u02c6win\n+ 3\u03c32\u2225R\u22a4\u00b5i\u22252\u2225R\u22252\n2\n s\n128(r ln 9 + ln(3k/\u03b4))\n\u02c6win\n+ 4(r ln 9 + ln(3k/\u03b4))\n\u02c6win\n!\n+ 3\u03c3\u2225R\u22a4\u00b5i\u22252\n2\u2225R\u22252\ns\nr + 2\np\nr ln(3k/\u03b4) + 2 ln(3k/\u03b4)\n\u02c6win\n,\n\u2200i \u2208[k].\n14\nProof. We separately consider \ufb01rst-, second-, and third-order moments. Throughout, we let the thin\nSVD of R be given by R = USV \u22a4, where U \u2208Rd\u00d7r has orthonormal columns, and \u2225V S\u22252 = \u2225R\u22252.\nFirst-order moments. Observe that ( \u02c6win/\u03c32)\u2225U \u22a4(\u02c6\u00b5i \u2212\u00b5i)\u22252\n2 is distributed as the sum of r indepen-\ndent \u03c72 random variables, each with one degree of freedom. Thus, Lemma 18 and union bounds\nimply\nPr\n\"\n\u2203i \u2208[k] \u0005 \u2225U \u22a4(\u02c6\u00b5i \u2212\u00b5i)\u22252\n2 > \u03c32\n\u0012r + 2\np\nr ln(k/\u03b4) + 2 ln(k/\u03b4)\n\u02c6win\n\u0013#\n\u2264\u03b4.\nSecond-order moments. Since M2,i = \u03c32I + \u00b5i\u00b5\u22a4\ni , it follows by the triangle and Cauchy-Schwarz\ninequalities that\n\u2225R\u22a4( c\nM2,i \u2212M2,i)R\u22252 \u2264\u2225R\u22252\n2\n\r\r\r\r\n1\n\u02c6win\nX\nj\u2208[n]:xj\u2208Si\n\u0010\nU \u22a4(xj \u2212\u00b5i)(xj \u2212\u00b5i)\u22a4U \u2212\u03c32I\n\u0011\r\r\r\r\n2\n+ 2\u2225R\u22a4\u00b5i\u22252\u2225R\u22a4(\u02c6\u00b5i \u2212\u00b5i)\u22252.\nA tail bound for the \ufb01rst term follows from Lemma 19, combined with a union bound:\nPr\n\"\n\u2203i \u2208[k] \u0005\n\r\r\r\r\n1\n\u02c6win\nX\nj\u2208[n]:xj\u2208Si\n\u0010\nU \u22a4(xj \u2212\u00b5i)(xj \u2212\u00b5i)\u22a4U \u2212\u03c32I\n\u0011\r\r\r\r\n2\n> \u03c32\n s\n128(r ln 9 + ln(k/\u03b4))\n\u02c6win\n+ 4(r ln 9 + ln(k/\u03b4))\n\u02c6win\n!#\n\u2264\u03b4.\nThe second term is handled as above.\nThird-order moments. It can be checked that\nM3,i = \u00b5i \u2297\u00b5i \u2297\u00b5i + \u03c32\nd\nX\n\u03b9=1\n\u0012\n\u00b5i \u2297e\u03b9 \u2297e\u03b9 + e\u03b9 \u2297\u00b5i \u2297e\u03b9 + e\u03b9 \u2297e\u03b9 \u2297\u00b5i\n\u0013\n(similar to Lemma 3) and\nc\nM3,i \u2212M3,i\n=\n1\n\u02c6win\n \nX\nj\u2208[n]:xj\u2208Si\n(xj \u2212\u00b5i) \u2297(xj \u2212\u00b5i) \u2297(xj \u2212\u00b5i)\n+\nX\nj\u2208[n]:xj\u2208Si\n\u0010\n\u00b5i \u2297(xj \u2212\u00b5i) \u2297(xj \u2212\u00b5i) \u2212\u03c32\nd\nX\n\u03b9=1\n\u00b5i \u2297e\u03b9 \u2297e\u03b9\n\u0011\n+\nX\nj\u2208[n]:xj\u2208Si\n\u0010\n(xj \u2212\u00b5i) \u2297\u00b5i \u2297(xj \u2212\u00b5i) \u2212\u03c32\nd\nX\n\u03b9=1\ne\u03b9 \u2297\u00b5i \u2297e\u03b9\n\u0011\n+\nX\nj\u2208[n]:xj\u2208Si\n\u0010\n(xj \u2212\u00b5i) \u2297(xj \u2212\u00b5i) \u2297\u00b5i \u2212\u03c32\nd\nX\n\u03b9=1\ne\u03b9 \u2297e\u03b9 \u2297\u00b5i\n\u0011\n+\nX\nj\u2208[n]:xj\u2208Si\n\u00b5i \u2297\u00b5i \u2297(xj \u2212\u00b5i) +\nX\nj\u2208[n]:xj\u2208Si\n\u00b5i \u2297(xj \u2212\u00b5i) \u2297\u00b5i +\nX\nj\u2208[n]:xj\u2208Si\n(xj \u2212\u00b5i) \u2297\u00b5i \u2297\u00b5i\n!\n.\n15\nTherefore, by the triangle and Cauchy-Schwarz inequalities,\n\u2225( c\nM3,i \u2212M3,i)[R, R, R]\u22252 \u2264\u2225R\u22253\n2\n\r\r\r\r\n1\n\u02c6win\nX\nj\u2208[n]:xj\u2208Si\nU \u22a4(xj \u2212\u00b5i) \u2297U \u22a4(xj \u2212\u00b5i) \u2297U \u22a4(xj \u2212\u00b5i)\n\r\r\r\r\n2\n+ 3\u2225R\u22a4\u00b5i\u22252\n\r\r\r\r\n1\n\u02c6win\nX\nj\u2208[n]:xj\u2208Si\nR\u22a4\u0010\n(xj \u2212\u00b5i)(xj \u2212\u00b5i)\u22a4\u2212\u03c32I\n\u0011\nR\n\r\r\r\r\n2\n+ 3\u2225R\u22a4\u00b5i\u22252\n2\u2225R\u22a4(\u02c6\u00b5i \u2212\u00b5i)\u22252.\nA tail bound for the \ufb01rst term is given by Lemma 21, combined with a union bound:\nPr\n\"\n\u2203i \u2208[k] \u0005\n\r\r\r\r\n1\n\u02c6win\nX\nj\u2208[n]:xj\u2208Si\n1\n\u03c33 U \u22a4(xj \u2212\u00b5i) \u2297U \u22a4(xj \u2212\u00b5i) \u2297U \u22a4(xj \u2212\u00b5i)\n\r\r\r\r\n2\n> \u03c33\ns\n108e3\u2308r ln 13 + ln(k/\u03b4)\u23093\n\u02c6win\n#\n\u2264\u03b4.\nThe other terms are handled as per above.\nLemma 6 (Accuracy of empirical moments). Fix a matrix R \u2208Rd\u00d7r. De\ufb01ne B1,R := maxi\u2208[k] \u2225R\u22a4\u00b5i\u22252,\nB2,R := maxi\u2208[k] \u2225R\u22a4M2,iR\u22252, B3,R := maxi\u2208[k] \u2225M3,i[R, R, R]\u22252, E1,R := maxi\u2208[k] \u2225R\u22a4(\u02c6\u00b5i \u2212\u00b5i)\u22252,\nE2,R := maxi\u2208[k] \u2225R\u22a4( c\nM2,i \u2212M2,i)R\u22252, E3,R := maxi\u2208[k] \u2225c\nM3,i[R, R, R] \u2212M3,i[R, R, R]\u22252, and\nEw := (Pk\ni=1( \u02c6wi \u2212wi)2)1/2. Then\n\u2225R\u22a4(\u02c6\u00b5 \u2212\u00b5)\u22252 \u2264(1 +\n\u221a\nkEw)E1,R +\n\u221a\nkB1,REw;\n\u2225R\u22a4( c\nM2 \u2212M2)R\u22252 \u2264(1 +\n\u221a\nkEw)E2,R +\n\u221a\nkB2,REw;\n\u2225( c\nM3 \u2212M3)[R, R, R]\u22252 \u2264(1 +\n\u221a\nkEw)E3,R +\n\u221a\nkB3,REw.\nProof. We just show the third claimed inequalitiy, as the others are similar. Write as shorthand\nbTi := c\nM3,i[R, R, R] and Ti := M3,i[R, R, R]. Then\n\r\r\r\r\nk\nX\ni=1\n\u02c6wi bTi \u2212\nk\nX\ni=1\nwiTi\n\r\r\r\r\n2\n\u2264\n\r\r\r\r\nk\nX\ni=1\nwi( bTi \u2212Ti)\n\r\r\r\r\n2\n+\n\r\r\r\r\nk\nX\ni=1\n( \u02c6wi \u2212wi)Ti\n\r\r\r\r\n2\n+\n\r\r\r\r\nk\nX\ni=1\n( \u02c6wi \u2212wi)( bTi \u2212Ti)\n\r\r\r\r\n2\n\u2264\nk\nX\ni=1\nwi\u2225bTi \u2212Ti\u22252 +\nk\nX\ni=1\n| \u02c6wi \u2212wi|\u2225Ti\u22252 +\nk\nX\ni=1\n| \u02c6wi \u2212wi|\u2225bTi \u2212Ti\u22252\n\u2264max\ni\u2208[k] \u2225bTi \u2212Ti\u22252 +\n\u221a\nk\u2225\u02c6w \u2212w\u22252 max\ni\u2208[k] \u2225Ti\u22252 +\n\u221a\nk\u2225\u02c6w \u2212w\u22252 max\ni\u2208[k] \u2225bTi \u2212Ti\u22252\n= E3,R +\n\u221a\nkEwB3,R +\n\u221a\nkEwE3,R\nwhere the \ufb01rst and second steps use the triangle inequality, and the second step uses H\u00a8older\u2019s\ninequality.\n16\nC.5\nEstimation of \u03c32, M2, and M3\nThe covariance matrix can be written as M2 \u2212\u00b5\u00b5\u22a4, and the empirical covariance matrix can be\nwritten as c\nM2 \u2212\u02c6\u00b5\u02c6\u00b5\u22a4. Recall that the estimate of \u03c32, denoted by \u02c6\u03c32, is given by the k-th largest\neigenvalue of the empirical covariance matrix c\nM2 \u2212\u02c6\u00b5\u02c6\u00b5\u22a4; and that the estimate of M2, denoted by\nc\nM2, is the best rank-k approximation to c\nM2 \u2212\u02c6\u03c32I. Of course, the singular values of a positive\nsemi-de\ufb01nite matrix are the same as its eigenvalues; in particular, \u02c6\u03c32 = \u03c2k[ c\nM2 \u2212\u02c6\u00b5\u02c6\u00b5\u22a4].\nLemma 7 (Accuracy of \u02c6\u03c32 and c\nM2).\n1. |\u02c6\u03c32 \u2212\u03c32| \u2264\u2225c\nM2 \u2212M2\u22252 + 2\u2225\u00b5\u22252\u2225\u02c6\u00b5 \u2212\u00b5\u22252 + \u2225\u02c6\u00b5 \u2212\u00b5\u22252\n2.\n2. \u2225c\nM2 \u2212M2\u22252 \u22644\u2225c\nM2 \u2212M2\u22252 + 4\u2225\u00b5\u22252\u2225\u02c6\u00b5 \u2212\u00b5\u22252 + 2\u2225\u02c6\u00b5 \u2212\u00b5\u22252\n2.\nProof. Using Weyl\u2019s inequality (Stewart and Sun, 1990, Theorem 4.11, p. 204), we obtain |\u03c2k[ c\nM2 \u2212\n\u02c6\u00b5\u02c6\u00b5\u22a4]\u2212\u03c2k[M2 \u2212\u00b5\u00b5\u22a4]| \u2264\u2225( c\nM2 \u2212\u02c6\u00b5\u02c6\u00b5\u22a4)\u2212(M2 \u2212\u00b5\u00b5\u22a4)\u22252 \u2264\u2225c\nM2 \u2212M2\u22252 +2\u2225\u00b5\u22252\u2225\u02c6\u00b5\u2212\u00b5\u22252 +\u2225\u02c6\u00b5\u2212\u00b5\u22252\n2.\nThe \ufb01rst claim then follows by observing that \u03c2k[M2 \u2212\u00b5\u00b5\u22a4] = \u03c32 as per Theorem 1.\nFor the second claim, observe that \u03c2k+1(M2 \u2212\u03c32I) = 0 as M2 \u2212\u03c32I has rank k. Therefore\n\u03c2k+1( c\nM2 \u2212\u02c6\u03c32I) = |\u03c2k+1( c\nM2 \u2212\u02c6\u03c32I) \u2212\u03c2k+1(M2 \u2212\u03c32I)| \u2264\u2225( c\nM2 \u2212\u02c6\u03c32I) \u2212(M2 \u2212\u03c32I)\u22252, again\nusing Weyl\u2019s inequality. Since c\nM2 is the best rank-k approximation to c\nM2 \u2212\u02c6\u03c32I, it follows that\n\u2225c\nM2 \u2212( c\nM2 \u2212\u02c6\u03c32I)\u22252 \u2264\u03c2k+1( c\nM2 \u2212\u02c6\u03c32I) \u2264\u2225( c\nM2 \u2212\u02c6\u03c32I) \u2212(M2 \u2212\u03c32I)\u22252. Therefore \u2225c\nM2 \u2212M2\u22252 \u2264\n\u2225( c\nM2 \u2212\u02c6\u03c32I) \u2212(M2 \u2212\u03c32I)\u22252 + \u2225c\nM2 \u2212( c\nM2 \u2212\u02c6\u03c32I)\u22252 \u22642\u2225( c\nM2 \u2212\u02c6\u03c32I) \u2212(M2 \u2212\u03c32I)\u22252 \u22642\u2225c\nM2 \u2212\nM2\u22252 + 2|\u02c6\u03c32 \u2212\u03c32| \u22644\u2225c\nM2 \u2212M2\u22252 + 4\u2225\u00b5\u22252\u2225\u02c6\u00b5 \u2212\u00b5\u22252 + 2\u2225\u02c6\u00b5 \u2212\u00b5\u22252\n2.\nRecall that the estimate of M3, denoted by c\nM3, is given by c\nM3 \u2212\u02c6\u03c32 Pd\ni=1(\u02c6\u00b5 \u2297ei \u2297ei + ei \u2297\n\u02c6\u00b5 \u2297ei + ei \u2297ei \u2297\u02c6\u00b5).\nLemma 8 (Accuracy of c\nM3). For any matrix R \u2208Rd\u00d7r,\n\u2225c\nM3[R, R, R] \u2212M3[R, R, R]\u22252 \u2264\u2225c\nM3[R, R, R] \u2212M3[R, R, R]\u22252\n+ 3\u2225R\u22252\n2(\u2225R\u22a4(\u02c6\u00b5 \u2212\u00b5)\u22252 + \u2225R\u22a4\u00b5\u22252)\n(\u2225c\nM2 \u2212M2\u22252 + 2\u2225\u00b5\u22252\u2225\u02c6\u00b5 \u2212\u00b5\u22252 + \u2225\u02c6\u00b5 \u2212\u00b5\u22252\n2)\n+ \u03c32\u2225R\u22252\n2\u2225R\u22a4(\u02c6\u00b5 \u2212\u00b5)\u22252.\nProof. Let bG := Pd\ni=1(\u02c6\u00b5 \u2297ei \u2297ei + ei \u2297\u02c6\u00b5 \u2297ei + ei \u2297ei \u2297\u02c6\u00b5 and G := Pd\ni=1(\u00b5 \u2297ei \u2297ei + ei \u2297\u00b5 \u2297\nei + ei \u2297ei \u2297\u00b5). Then\n\u2225(c\nM3 \u2212M3)[R, R, R]\u22252\n= \u2225( c\nM3 \u2212M3)[R, R, R] \u2212(\u02c6\u03c32 \u2212\u03c32)( bG \u2212G)[R, R, R] \u2212(\u02c6\u03c32 \u2212\u03c32)G[R, R, R] \u2212\u03c32( bG \u2212G)[R, R, R]\u22252\n\u2264\u2225( c\nM3 \u2212M3)[R, R, R]\u22252 + |\u02c6\u03c32 \u2212\u03c32|\u2225( bG \u2212G)[R, R, R]\u22252 + |\u02c6\u03c32 \u2212\u03c32|\u2225G[R, R, R]\u22252\n+ \u03c32\u2225( bG \u2212G)[R, R, R]\u22252.\nObserve that by the triangle inequality, \u2225G[R, R, R]\u22252 \u22643\u2225R\u22252\n2\u2225R\u22a4\u00b5\u22252 and \u2225( bG \u2212G)[R, R, R]\u22252 \u2264\n3\u2225R\u22252\n2\u2225R\u22a4(\u02c6\u00b5 \u2212\u00b5)\u22252. Furthermore, by Lemma 7, we have |\u02c6\u03c32 \u2212\u03c32| \u2264\u2225c\nM2 \u2212M2\u22252 + 2\u2225\u00b5\u22252\u2225\u02c6\u00b5 \u2212\n\u00b5\u22252 + \u2225\u02c6\u00b5 \u2212\u00b5\u22252\n2. Therefore the claim follows.\n17\nC.6\nProperties of projection and whitening operators\nRecall that bU \u2208Rd\u00d7k is the matrix of left orthonormal singular vectors of c\nM2, and let bS \u2208Rk\u00d7k be\nthe diagonal matrix of corresponding singular values. Analogously de\ufb01ne U and S relative to M2.\nDe\ufb01ne EM2 := \u2225c\nM2 \u2212M2\u22252/\u03c2k[M2].\nLemma 9 (Properties of projection operators). Assume EM2 \u22641/3. Then\n1. (1 + EM2)S \u2ab0bU \u22a4c\nM2 bU = bS \u2ab0(1 \u2212EM2)S \u227b0.\n2. \u03c2k[bU \u22a4U] \u2265\nq\n1 \u2212(9/4)E2\nM2 > 0.\n3. \u03c2k[bU \u22a4M2 bU] \u2265(1 \u2212(9/4)E2\nM2)\u03c2k[M2] > 0.\n4. \u2225(I \u2212bU bU \u22a4)UU \u22a4\u22252 \u2264(3/2)EM2.\nProof. By the assumptions that EM2 \u22641/3 and M2 is symmetric positive de\ufb01nite, we have\n|\u03c2t[c\nM2] \u2212\u03c2t[M2]| \u2264\u2225c\nM2 \u2212M2\u22252 \u2264EM2\u03c2k[M2],\n\u2200t \u2208[k]\nby Weyl\u2019s inequality. Therefore c\nM2 is symmetric positive de\ufb01nite, and\n(1 + EM2)S \u2ab0bU \u22a4c\nM2 bU = bS \u2ab0(1 \u2212EM2)S,\nwhich proves the \ufb01rst claim.\nNow let bU\u22a5\u2208Rd\u00d7(d\u2212k) be a matrix with orthonormal columns spanning the orthogonal com-\nplement of the range of c\nM2. By Wedin\u2019s theorem (Stewart and Sun, 1990, Theorem 4.4, p. 262)\nand the assumption that EM2 \u22641/3,\n\u2225bU \u22a4\n\u22a5U\u22252 \u2264\u2225c\nM2 \u2212M2\u22252\n\u03c2k[c\nM2]\n\u2264\nEM2\n1 \u2212EM2\n\u22643\n2EM2 \u22641\n2.\nTherefore bU \u22a4U is non-singular, and for any v \u2208Rk, \u2225bU \u22a4Uv\u22252\n2 = 1 \u2212\u2225bU \u22a4\n\u22a5Uv\u22252\n2 \u22651 \u2212(9/4)E2\nM2,\nwhich in turn implies the second claim.\nThe second claim then implies that \u03c2k[bU \u22a4M2 bU] \u2265\n\u03c2k[bU \u22a4U]2\u03c2k[M2] \u2265(1 \u2212(9/4)E2\nM2)\u03c2k[M2], which gives the third claim.\nFor the \ufb01nal claim, ob-\nserve that \u2225(I \u2212bU bU \u22a4)UU \u22a4\u22252 = \u2225bU\u22a5bU \u22a4\n\u22a5UU \u22a4\u22252 = \u2225bU \u22a4\n\u22a5U\u22252 \u2264(3/2)EM2, using the fact that bU\u22a5and\nU have orthonormal columns, and the above displayed inequality.\nRecall that c\nW = bU(bU \u22a4c\nM2 bU)\u20201/2.\nLemma 10 (Properties of whitening operators). De\ufb01ne W := c\nW(c\nW \u22a4M2c\nW)\u20201/2. Assume EM2 \u2264\n1/3. Then\n1. c\nW \u22a4M2c\nW is symmetric positive de\ufb01nite, W \u22a4M2W = I, and W \u22a4A diag(w)1/2 is orthogonal.\n2. \u2225c\nW\u22252 \u2264\n1\n\u221a\n(1\u2212EM2)\u03c2k[M2].\n18\n3. \u2225(c\nW \u22a4M2c\nW)1/2 \u2212I\u22252 \u2264(3/2)EM2,\n\u2225(c\nW \u22a4M2c\nW)\u22121/2 \u2212I\u22252 \u2264(3/2)EM2,\n\u2225c\nW \u22a4A diag(w)1/2\u22252 \u2264\np\n1 + (3/2)EM2,\n\u2225(c\nW \u2212W)\u22a4A diag(w)1/2\u22252 \u2264(3/2)\np\n1 + (3/2)EM2EM2.\nProof. By Lemma 9 (\ufb01rst and third claims), the matrices bU \u22a4c\nM2 bU and bU \u22a4M2 bU are symmetric\npositive de\ufb01nite. Therefore\nc\nW \u22a4M2c\nW = (bU \u22a4c\nM2 bU)\u22121/2(bU \u22a4M2 bU)(bU \u22a4c\nM2 bU)\u22121/2 \u227b0,\nW = c\nW(c\nW \u22a4M2c\nW)\u22121/2,\nW \u22a4M2W = (c\nW \u22a4M2c\nW)\u22121/2(c\nW \u22a4M2c\nW)(c\nW \u22a4M2c\nW)\u22121/2 = I.\nSince M2 = A diag(w)A\u22a4, it follows from the third equation above that W \u22a4A diag(w)1/2 is orthog-\nonal. Thus the \ufb01rst claim is established.\nFor the second claim, note that\n\u2225c\nW\u22252 \u2264\u2225(bU \u22a4c\nM2 bU)\u22121/2\u22252 = \u03c2k[bU \u22a4c\nM2 bU]\u22121/2 \u2264((1 \u2212EM2)\u03c2k[M2])\u22121/2\nwhere the last inequality follows from Lemma 9 (\ufb01rst claim).\nTo show the third claim, we \ufb01rst bound \u2225c\nW \u22a4M2c\nW \u2212I\u22252 as\n\u2225c\nW \u22a4M2c\nW \u2212I\u22252 = \u2225c\nW \u22a4(M2 \u2212c\nM2)c\nW\u22252\n\u2264\u2225c\nW\u22252\n2\u2225M2 \u2212c\nM2\u22252\n\u2264\nEM2\n1 \u2212EM2\n\u22643\n2EM2 \u22641\n2\nwhere the second inequality follows from the \ufb01rst claim.\nThis implies that every eigenvalue of\nc\nW \u22a4M2c\nW is contained in the interval of radius (3/2)EM2 around 1. Because |(1 + x)\u22121/2 \u22121| \u2264|x|\nfor all |x| \u22641/2, the same is true of the eigenvalues of (c\nW \u22a4M2c\nW)\u22121/2:\n\u2225(c\nW \u22a4M2c\nW)\u22121/2 \u2212I\u22252 \u22643\n2EM2.\nFurthermore,\n\u2225c\nW \u22a4A diag(w)1/2\u22252\n2 = \u2225c\nW \u22a4M2c\nW\u22252\n= \u2225I + c\nW \u22a4M2c\nW \u2212I\u22252\n\u22641 + \u2225c\nW \u22a4M2c\nW \u2212I\u22252 \u22641 + 3\n2EM2,\nso\n\u2225(c\nW \u2212W)\u22a4A diag(w)1/2\u22252 = \u2225(I \u2212(c\nW \u22a4M2c\nW)\u22121/2)c\nW \u22a4A diag(w)1/2\u22252\n\u2264\u2225I \u2212(c\nW \u22a4M2c\nW)\u22121/2\u22252\u2225c\nW \u22a4A diag(w)1/2\u22252 \u22643\n2EM2\nr\n1 + 3\n2EM2.\nThis establishes the third claim.\n19\nDe\ufb01ne bT := c\nM3[c\nW, c\nW, c\nW] and T := M3[W, W, W], both symmetric tensors in Rk\u00d7k\u00d7k. Also,\nde\ufb01ne bT[u] := c\nM3[c\nW, c\nW, c\nWu] and T[u] := M3[W, W, Wu], both symmetric matrices in Rk\u00d7k.\nLemma 11 (Tensor structure). The tensor T can be written as\nT =\nk\nX\ni=1\n1\n\u221awi\n(W \u22a4A diag(w)1/2ei) \u2297(W \u22a4A diag(w)1/2ei) \u2297(W \u22a4A diag(w)1/2ei)\nwhere the vectors {W \u22a4A diag(w)1/2ei : i \u2208[k]} are orthonormal. Furthermore, the eigenvectors of\nT[u] are {W \u22a4A diag(w)1/2ei : i \u2208[k]} and the corresponding eigenvalues are {u\u22a4W \u22a4\u00b5i : i \u2208[k]}.\nProof. The structure of T follows from Lemma 3, and the orthogonality of {W \u22a4A diag(w)1/2ei :\ni \u2208[k]} follows from Lemma 10 (\ufb01rst claim). The eigendecomposition of T[u] is then readily seen\nfrom the structure of T.\nLemma 12 (Tensor accuracy). Assume EM2 \u22641/3. Then\n\u2225bT \u2212T\u22252 \u2264\u2225c\nM3[c\nW, c\nW, c\nW] \u2212M3[c\nW, c\nW, c\nW]\u22252 +\n6\n\u221awmin\nEM2.\nProof. By Lemma 11, T = Pk\ni=1 w\u22121/2\ni\nvi \u2297vi \u2297vi for some orthonormal vectors {vi : i \u2208[k]}, so\n\u2225T\u22252 \u22641/\u221awmin. By Lemma 10 (\ufb01rst and third claims), c\nW = W(c\nW \u22a4M2c\nW)1/2 and \u2225(c\nW \u22a4M2c\nW)1/2\u2212\nI\u22252 \u2264(3/2)EM2. Therefore\n\u2225M3[c\nW, c\nW, c\nW] \u2212M3[W, W, W]\u22252\n\u2264\u2225M3[c\nW \u2212W, c\nW , c\nW]\u22252 + \u2225M3[W, c\nW \u2212W, c\nW ]\u22252 + \u2225M3[W, W, c\nW \u2212W]\u22252\n\u2264\u2225M3[W, W, W]\u22252\n\u0012\n\u2225(c\nW \u22a4M2c\nW)1/2 \u2212I\u22252\u2225(c\nW \u22a4M2c\nW)1/2\u22252\n2\n+ \u2225(c\nW \u22a4M2c\nW)1/2 \u2212I\u22252\u2225(c\nW \u22a4M2c\nW)1/2\u22252 + \u2225(c\nW \u22a4M2c\nW)1/2 \u2212I\u22252\n\u0013\n\u2264\u2225M3[W, W, W]\u22252\n\u0012\n(1 + (3/2)EM2)2(3/2)EM2 + (1 + (3/2)EM2)(3/2)EM2 + (3/2)EM2\n\u0013\n\u22646\u2225M3[W, W, W]\u22252EM2 \u2264\n6\n\u221awmin\nEM2.\nThus we can bound \u2225bT \u2212T\u22252 using the triangle inequality and the above bound:\n\u2225bT \u2212T\u22252 = \u2225c\nM3[c\nW, c\nW, c\nW] \u2212M3[W, W, W]\u22252\n\u2264\u2225c\nM3[c\nW, c\nW, c\nW] \u2212M3[c\nW, c\nW, c\nW]\u22252 + \u2225M3[c\nW, c\nW, c\nW] \u2212M3[W, W, W]\u22252\n\u2264\u2225c\nM3[c\nW, c\nW, c\nW] \u2212M3[c\nW, c\nW, c\nW]\u22252 +\n6\n\u221awmin\nEM2.\n20\nC.7\nEigendecomposition analysis\nDe\ufb01ne\n\u03b3 :=\n1\n2\u221awmax\n\u221a\nek\n\u0000k+1\n2\n\u0001\n(1)\nwhere wmax := maxi\u2208[k] wi.\nLemma 13 (Random separation). Let \u03b8 \u2208Rk be a random vector distributed uniformly over the\nunit sphere in Rk. Let Q := {ei \u2212ej : {i, j} \u2208\n\u0000k\n2\n\u0001\n} \u222a{ei : i \u2208[k]}. Then\nPr\nh\nmin\nq\u2208Q |\u03b8\u22a4W \u22a4Aq| > \u03b3\ni\n\u22651\n2\nwhere the probability is taken with respect to the distribution of \u03b8.\nProof. By Lemma 17, with probability at least 1/2,\nmin\nq\u2208Q |\u03b8\u22a4W \u22a4Aq| > minq\u2208Q \u2225W \u22a4Aq\u22252\n2\n\u221a\nek\n\u0000k+1\n2\n\u0001\n.\nNow \ufb01x any i \u0338= j. By Lemma 10 (\ufb01rst claim), W \u22a4A diag(w)1/2 is orthogonal, so \u2225W \u22a4A(ei\u2212ej)\u22252 =\n\u2225diag(w)\u22121/2(ei \u2212ej)\u22252 = \u2225ei/\u221awi \u2212ej/\u221awj\u22252 =\np\n1/wi + 1/wj.\nSimilarly, for any i \u2208[k],\n\u2225W \u22a4Aei\u22252 =\np\n1/wi. Therefore minq\u2208Q \u2225W \u22a4Aq\u22252 = mini\u2208[k]\np\n1/wi.\nLet ET := \u2225bT \u2212T\u22252/\u03b3.\nLet \u03b81, \u03b82, . . . , \u03b8t be the random unit vectors in Rk drawn by the\nalgorithm.\nDe\ufb01ne bT[\u03b8t\u2032] := c\nM3[c\nW, c\nW, c\nW\u03b8t\u2032] and T[\u03b8t\u2032] := M3[W, W, W\u03b8t\u2032].\nAlso, let \u2206(t\u2032) :=\nmin{|\u03bbi \u2212\u03bbj| : i \u0338= j} \u222a{|\u03bbi| : i \u2208[k]} for the eigenvalues {\u03bbi : i \u2208[k]} of T[\u03b8t\u2032], and let\nb\u2206(t\u2032) := min{|\u02c6\u03bbi \u2212\u02c6\u03bbj| : i \u0338= j} \u222a{|\u03bbi| : i \u2208[k]} for the eigenvalues {\u02c6\u03bbi : i \u2208[k]} of bT[\u03b8t\u2032].\nLemma 14 (Eigenvalue gap). Pick any \u03b4 \u2208(0, 1). If t \u2265log2(1/\u03b4), then with probability at least\n1 \u2212\u03b4, the trial \u02c6\u03c4 := arg maxt\u2032\u2208[t] b\u2206(t\u2032) satis\ufb01es\nb\u2206(\u02c6\u03c4) \u2265\u03b3 \u22122ET \u03b3.\nProof. For each t\u2032 \u2208[t], the eigenvalues of \u02c6\u03bb1, \u02c6\u03bb2, \u00b7 \u00b7 \u00b7 , \u02c6\u03bbk of bT[\u03b8t\u2032] (arranged in non-increasing order)\nsatisfy\n|\u02c6\u03bbi \u2212\u02c6\u03bbj| \u2265|\u03bbi \u2212\u03bbj| \u22122\u2225bT [\u03b8t\u2032] \u2212T[\u03b8t\u2032]\u22252 \u2265|\u03bbi \u2212\u03bbj| \u22122ET \u03b3,\ni \u0338= j\n(2)\nwhere the second inequality follows from Weyl\u2019s inequality. Similarly,\n|\u02c6\u03bbi| \u2265|\u03bbi| \u2212|\u02c6\u03bbi \u2212\u03bbi| \u2265|\u03bbi| \u2212ET \u03b3.\n(3)\nBy Lemma 11, the eigenvalues of T[v] are v\u22a4W \u22a4Aei for i \u2208[k]. Thus, by Lemma 13, the probability\nthat some \u03c4 \u2208[t] has \u2206(\u03c4) > \u03b3 is at least 1 \u2212\u03b4.\nIn this event, (2) implies that trial \u03c4 has\nb\u2206(\u03c4) \u2265\u2206(\u03c4) \u22122ET \u03b3, and hence b\u2206(\u02c6\u03c4) = maxt\u2032\u2208[t] b\u2206(t\u2032) \u2265\u03b3 \u22122ET \u03b3.\nWe now just consider the trial \u02c6\u03c4 retained by the algorithm.\nLet {(vi, \u03bbi) : i \u2208[k]} be the\neigenvector/eigenvalue pairs of T[\u03b8\u02c6\u03c4], and let {(\u02c6vi, \u02c6\u03bbi) : i \u2208[k]} be the eigenvector/eigenvalue pairs\nof bT[\u03b8\u02c6\u03c4].\n21\nLemma 15 (Accuracy of eigendecomposition). Assume the 1 \u2212\u03b4 probability event in Lemma 14\nholds, and also assume that ET \u22641/4.\nThen there exists a permutation \u03c0 on [k] and signs\ns1, s2, . . . , sk \u2208{\u00b11} such that, for all i \u2208[k],\n\u2225vi \u2212si\u02c6v\u03c0(i)\u22252 \u22644\n\u221a\n2ET\n|\u03bbi \u2212\u02c6\u03bb\u03c0(i)| \u2264ET \u03b3.\nProof. To simplify notation, assume the eigenvalues of bT[\u03b8] and T[\u03b8] are already sorted in non-\nincreasing order. Observe that for all i \u0338= j,\n|\u02c6\u03bbi \u2212\u03bbj| = |\u02c6\u03bbi \u2212\u02c6\u03bbj + \u02c6\u03bbj \u2212\u03bbj|\n\u2265|\u02c6\u03bbi \u2212\u02c6\u03bbj| \u2212|\u02c6\u03bbj \u2212\u03bbj|\n\u2265(\u03b3 \u22122ET \u03b3) \u2212ET \u03b3\n\u2265\u03b3/4\nwhere the second-to-last inequality follows by the assumption b\u2206(\u02c6\u03c4) \u2265\u03b3 \u22122ET \u03b3 and by Weyl\u2019s\ninequality. Therefore, the interval of radius \u03b3/4 surronding each eigenvalue \u02c6\u03bbi of bT[\u03b8\u02c6\u03c4] contains\nonly one eigenvalue \u03bbi of T[\u03b8\u02c6\u03c4].\nBy the Davis-Kahan sin(\u0398) theorem (Stewart and Sun, 1990,\nTheorem 3.4, p. 250), we have that\nq\n1 \u2212(v\u22a4\ni \u02c6vi)2 \u22644ET .\nTherefore, for si := sign(v\u22a4\ni \u02c6vi),\n\u2225vi \u2212si\u02c6vi\u22252\n2 = 2\n\u00001 \u2212siv\u22a4\ni \u02c6vi\n\u0001\n= 2\n\u00001 \u2212|v\u22a4\ni \u02c6vi|\n\u0001\n\u22642\n\u00001 \u2212\np\n1 \u2212(4ET )2\u0001\n\u226432E2\nT .\nThe bound |\u03bbi \u2212\u02c6\u03bbi| \u2264ET \u03b3 follows simply from Weyl\u2019s inequality.\nC.8\nOverall error analysis\nDe\ufb01ne\n\u03ba[M2] := \u03c21[M2]/\u03c2k[M2],\n\u01eb0 :=\n\u0012\n5.5EM2 + 7ET\n\u0013\n/\u221awmin,\n\u01eb1 :=\n\u00121.25\u2225M2\u22251/2\n2\n\u01eb0/\u221awmin\n\u03c2k[M2]1/2\n+ 2EM2 + \u03b3\u221awminET\n\u0013\n/\n\u0000\u03b3\u221awmin\n\u0001\n=\n\u0012\u0010\n6.875\u03ba[M2]1/2 + 2\n\u0011\nEM2 +\n\u0010\n8.75\u03ba[M2]1/2 + \u03b3\u221awmin\n\u0011\nET\n\u0013\n/\n\u0000\u03b3\u221awmin\n\u0001\n.\nLemma 16 (Error bound). Assume the 1\u2212\u03b4 probability event of Lemma 14 holds, and also assume\nthat EM2 \u22641/3, ET \u22641/4, and \u01eb1 \u22641/3. Then there exists a permutation \u03c0 on [k] such that\n\u2225\u02c6\u00b5\u03c0(i) \u2212\u00b5i\u22252 \u22643\u2225\u00b5i\u22252\u01eb1 + 2\u2225M2\u22251/2\n2\n\u01eb0,\ni \u2208[k].\n22\nProof. To simplify notation, we assume throughout that the permutation \u03c0 from Lemma 15 is the\nidentity permutation. Let V := [v1|v2| \u00b7 \u00b7 \u00b7 |vk]. We \ufb01rst bound bBsi\u02c6vi \u2212\u221awi\u00b5i. This quantity can\nbe split into two parts: the part in the range of bU, and the rest. The part in the range of bU is\nbounded as\n\u2225bBsi\u02c6vi \u2212bU bU \u22a4\u221awi\u00b5i\u22252 = \u2225(bU \u22a4c\nM2 bU)1/2si\u02c6vi \u2212bU \u22a4A diag(w)1/2ei\u22252\n= \u2225(bU \u22a4c\nM2 bU)1/2(si\u02c6vi \u2212vi) + ((bU \u22a4c\nM2 bU)1/2 \u2212bU \u22a4A diag(w)1/2V \u22a4)vi\u22252\n\u2264\u2225(bU \u22a4c\nM2 bU)1/2\u22252\u2225si\u02c6vi \u2212vi\u22252 + \u2225(bU \u22a4c\nM2 bU)1/2 \u2212bU \u22a4A diag(w)1/2V \u22a4\u22252\n\u2264\u2225c\nM2\u22251/2\n2\n\u2225si\u02c6vi \u2212vi\u22252 + \u2225(bU \u22a4c\nM2 bU)1/2 \u2212bU \u22a4A diag(w)1/2V \u22a4\u22252\n\u2264\n\u0010\n(1 + EM2)\u2225M2\u22252\n\u00111/2\n4\n\u221a\n2ET + \u2225(bU \u22a4c\nM2 bU)1/2 \u2212bU \u22a4A diag(w)1/2V \u22a4\u22252\nwhere the third inequality follows from Lemma 9 and Lemma 15. To bound the second term in the\nlast step, recall that W = c\nW(c\nW \u22a4M2c\nW)\u22121/2 (using Lemma 10 to guarantee the positive de\ufb01niteness\nof c\nW \u22a4M2c\nW), so we may write bU \u22a4AV \u22a4as\nbU \u22a4A diag(w)1/2V \u22a4= bU \u22a4A diag(w)1/2(W \u22a4A diag(w)1/2)\u22a4\n= bU \u22a4M2c\nW(c\nW \u22a4M2c\nW)\u22121/2\n=\n\u0010\n(bU \u22a4M2 bU)1/2 + bU \u22a4(M2 \u2212c\nM2)bU(bU \u22a4c\nM2 bU)\u22121/2\u0011\n(c\nW \u22a4M2c\nW)\u22121/2.\nTherefore\n\u2225(bU \u22a4c\nM2 bU)1/2 \u2212bU \u22a4A diag(w)1/2V \u22a4\u22252\n\u2264\u2225(bU \u22a4c\nM2 bU)1/2\u0010\nI \u2212(c\nW \u22a4M2c\nW)\u22121/2\u0011\n\u22252 + \u2225bU \u22a4(M2 \u2212c\nM2)bU\u22252\u2225(bU \u22a4c\nM2 bU)\u22121/2\u22252\u2225(c\nW \u22a4M2c\nW)\u22121/2\u22252\n\u2264\u2225c\nM2\u22251/2\n2\n\u2225I \u2212(c\nW \u22a4M2c\nW)\u22121/2\u22252 +\n1\n\u03c2k[bU \u22a4c\nM2 bU]1/2 \u2225M2 \u2212c\nM2\u22252\n\u0010\n1 + \u2225I \u2212(c\nW \u22a4M2c\nW)\u22121/2\u22252\n\u0011\n\u2264(1 + EM2)1/2\u2225M2\u22251/2\n2\n(3/2)EM2 + (1 + (3/2)EM2)EM2\u03c2k[M2]\n(1 \u2212EM2)1/2\u03c2k[M2]1/2\n= (1 + EM2)1/2\u2225M2\u22251/2\n2\n(3/2)EM2 + (1 + (3/2)EM2)\n(1 \u2212EM2)1/2 \u03c2k[M2]1/2EM2\nwhere the last inequality uses Lemma 9 and Lemma 10. Thus\n\u2225bBsi\u02c6vi \u2212bU bU \u22a4\u221awi\u00b5i\u22252 \u2264(1 + EM2)1/2\u2225M2\u22251/2\n2\n4\n\u221a\n2ET\n+ (1 + EM2)1/2\u2225M2\u22251/2\n2\n(3/2)EM2 + (1 + (3/2)EM2)\n(1 \u2212EM2)1/2 \u03c2k[M2]1/2EM2.\nNow consider the part not in the range of bU. This is simply bounded as\n\u2225(I \u2212bU bU \u22a4)\u221awiUU \u22a4\u00b5i\u22252 \u2264\u2225I \u2212bU bU \u22a4UU \u22a4\u22252\n\u221awi\u2225\u00b5i\u22252\n\u2264(3/2)EM2\n\u221awi\u2225\u00b5i\u22252\n23\nusing Lemma 9. Therefore, overall, we have\n\u2225bB\u02c6vi \u2212si\n\u221awi\u00b5i\u22252 = \u2225bBsi\u02c6vi \u2212\u221awi\u00b5i\u22252\n\u2264(1 + EM2)1/2\u2225M2\u22251/2\n2\n4\n\u221a\n2ET + (1 + EM2)1/2\u2225M2\u22251/2\n2\n(3/2)EM2\n+ (1 + (3/2)EM2)\n(1 \u2212EM2)1/2 \u03c2k[M2]1/2EM2 + (3/2)EM2\n\u221awi\u2225\u00b5i\u22252 \u2264\u2225M2\u22251/2\n2\n\u01eb0\n\u221awmin.\nSince the actual estimate of \u00b5i is (\u02c6\u03bbi/\u03b8\u22a4\u02c6vi) bB\u02c6vi, we need to show that \u03b8\u22a4\u02c6vi is approximately\nsi\n\u221awi\u02c6\u03bbi. Indeed,\n|\u03b8\u22a4\u02c6vi \u2212si\n\u221awi\u02c6\u03bbi| = |\u03b8\u22a4c\nW \u22a4( \u02c6B\u02c6vi \u2212si\n\u221awi\u00b5i) + si\n\u221awi\u03b8\u22a4(c\nW \u2212W)\u22a4\u00b5i + si\n\u221awi(\u03bbi \u2212\u02c6\u03bbi)|\n\u2264\u2225c\nW\u03b8\u22252\u2225bB\u02c6vi \u2212si\n\u221awi\u00b5i\u22252 + \u2225(c\nW \u2212W)\u22a4A diag(w)1/2ei\u22252 + \u221awi|\u03bbi \u2212\u02c6\u03bbi|\n\u2264\n\u2225M2\u22251/2\n2\n\u01eb0\u221awmin\n(1 \u2212EM2)1/2\u03c2k[M2]1/2 + (3/2)\np\n1 + (3/2)EM2EM2 + \u221awiET \u03b3 \u2264\u01eb1\u03b3\u221awmin\nwhere the last inequality uses Lemma 10 and Lemma 15. Therefore\n\u221awi\u2225(\u02c6\u03bbi/\u03b8\u22a4\u02c6vi) bB\u02c6vi \u2212\u00b5i\u22252\n= \u2225(\u02c6\u03bbi/\u03b8\u22a4\u02c6vi)si\n\u221awi bB\u02c6vi \u2212si\n\u221awi\u00b5i\u22252\n\u2264|(\u02c6\u03bbi/\u03b8\u22a4\u02c6vi)si\n\u221awi \u22121|\u2225bB\u02c6vi\u22252 + \u2225bB\u02c6vi \u2212si\n\u221awi\u00b5i\u22252\n\u2264|(\u02c6\u03bbisi\n\u221awi/\u03b8\u22a4\u02c6vi) \u22121|(\u221awi\u2225\u00b5i\u22252 + \u2225M2\u22251/2\n2\n\u01eb0\n\u221awmin) + \u2225M2\u22251/2\n2\n\u01eb0\n\u221awmin\n\u2264\n|\u02c6\u03bbisi\u221awi \u2212\u03b8\u22a4\u02c6vi|\n|\u02c6\u03bbi\u221awi| \u2212|\u03b8\u22a4\u02c6vi \u2212\u02c6\u03bbisi\u221awi|\n(\u221awi\u2225\u00b5i\u22252 + \u2225M2\u22251/2\n2\n\u01eb0\n\u221awmin) + \u2225M2\u22251/2\n2\n\u01eb0\n\u221awmin\n\u2264\n\u01eb1\u03b3\u221awmin\n|\u03bbi\u221awi| \u2212|\u02c6\u03bbi \u2212\u03bbi|\u221awi \u2212\u01eb1\u03b3\u221awmin\n(\u221awi\u2225\u00b5i\u22252 + \u2225M2\u22251/2\n2\n\u01eb0\n\u221awmin) + \u2225M2\u22251/2\n2\n\u01eb0\n\u221awmin\n\u2264\n\u01eb1\u03b3\u221awmin\n\u03b3\u221awi(1 \u2212ET ) \u2212\u01eb1\u03b3\u221awmin\n(\u221awi\u2225\u00b5i\u22252 + \u2225M2\u22251/2\n2\n\u01eb0\n\u221awmin) + \u2225M2\u22251/2\n2\n\u01eb0\n\u221awmin\n=\n\u01eb1\u03b3\n\u03b3(1 \u2212ET ) \u2212\u01eb1\u03b3 (\u221awi\u2225\u00b5i\u22252 + \u2225M2\u22251/2\n2\n\u01eb0\n\u221awmin) + \u2225M2\u22251/2\n2\n\u01eb0\n\u221awmin\nwhere the fourth inequality uses Lemma 15. We thus conclude that\n\u2225\u02c6\u00b5i \u2212\u00b5i\u22252 = \u2225(\u02c6\u03bbi/\u03b8\u22a4\u02c6vi) bB\u02c6vi \u2212\u00b5i\u22252 \u2264\n\u01eb1\u03b3\n\u03b3(1 \u2212ET ) \u2212\u01eb1\u03b3 (\u2225\u00b5i\u22252 + \u2225M2\u22251/2\n2\n\u01eb0) + \u2225M2\u22251/2\n2\n\u01eb0\n\u22643\u01eb1(\u2225\u00b5i\u22252 + \u2225M2\u22251/2\n2\n\u01eb0) + \u2225M2\u22251/2\n2\n\u01eb0\n\u22643\u2225\u00b5i\u22252\u01eb1 + 2\u2225M2\u22251/2\n2\n\u01eb0.\nWe can now prove Theorem 3, stated below with the explicit polynomial sample complexity\nbound (up to constants).\nTheorem 3 restated (Finite sample bound). There exists a constant C > 0 such that the following\n24\nholds. Let bmax := maxi\u2208[k] \u2225\u00b5i\u22252. Pick any \u03b5, \u03b4 \u2208(0, 1). Suppose the sample size 2n satis\ufb01es\nn \u2265C \u00b7 d + log(k/\u03b4)\nwmin\n\u00b7\n \u0014\u03ba[M2]1/2(\u03c32 + b2\nmax)\n\u03b32wmin\u03c2k[M2]\u03b5\n\u00152\n+\n\u0014\u03ba[M2]1/2(\u03c32 + b2\nmax)\n\u03b32wmin\u03c2k[M2]\u03b5\n\u0015!\n+ C \u00b7 (k + log(k/\u03b4))3\nwmin\n\u00b7\n\u0014\n\u03ba[M2]1/2\u03c33\n\u03b32\u221awmin\u03c2k[M2]3/2\u03b5\n\u00152\n+ C \u00b7 k + log(k/\u03b4)\nwmin\n\u00b7\n \u0014\n\u03ba[M2]1/2\u03c32\n\u03b32wmin\u03c2k[M2]\u03b5\n\u00152\n+\n\u0014\n\u03ba[M2]1/2\u03c32\n\u03b32wmin\u03c2k[M2]\u03b5\n\u0015\n+\n\u0014\n\u03ba[M2]1/2\u03c3\n\u03b32w3/2\nmin\u03c2k[M2]1/2\u03b5\n\u00152\n+\n\u0014 \u03ba[M2]1/2\n\u03b32\u221awmin\u03b5 \u00b7\n\u03c32\n\u03c2k[M2]1/2 \u00b7 max\nn\n1, \u03c32/\u03c2k[M2]\no\u00152!\n+ C \u00b7 k log(1/\u03b4)\nwmin\n \u0014 \u03ba[M2]1/2\n\u03b32\u221awmin\u03b5 \u00b7 max\nn\n1, \u03c32/\u03c2k[M2]\no\u00152\n+\n\u0014\u03ba[M2]1/2\n\u03b32w2\nmin\u03b5\n\u00152\n+\n\u0014\n\u03ba[M2]1/2\u03c32\n\u03b32wmin\u03c2k[M2]\u03b5\n\u00152!\nwhere\n\u03b3 =\n1\n2\u221awmax\n\u221a\nek\n\u0000k+1\n2\n\u0001\n(as de\ufb01ned in Lemma 13). Then with probability at least 1 \u22123\u03b4 over the random sample and the\ninternal randomness of the algorithm, there exists a permutation \u03c0 on [k] such that\n\u2225\u02c6\u00b5\u03c0(i) \u2212\u00b5i\u22252 \u2264\n\u0010\n\u2225\u00b5i\u22252 + \u2225M2\u22251/2\n2\n\u0011\n\u03b5\nfor all i \u2208[k].\nProof. Throughout, C1, c1, C2, c2, . . . will represent absolute positive constants. First, observe that\nthe sample size bound\nn \u2265C \u00b7 k log(1/\u03b4)\nand Lemma 4 ensure that Ew \u22641 (where Ew is de\ufb01ned in Lemma 6). Therefore, from Lemma 5\nand Lemma 6 (together with union bounds), with probability at least 1 \u2212\u03b4,\n\u2225\u02c6\u00b5 \u2212\u00b5\u22252 \u2264C1\u03c3\ns\nd + log(k/\u03b4)\nwminn\n+ C1bmax\nr\nk log(1/\u03b4)\nn\n,\n\u2225c\nM2 \u2212M2\u22252 \u2264C1\n \n\u03c32\ns\nd + log(k/\u03b4)\nwminn\n+ \u03c32 d + log(k/\u03b4)\nwminn\n+ \u03c3bmax\ns\nd + log(k/\u03b4)\nwminn\n!\n+ C1\n\u0010\nb2\nmax + \u03c32\u0011r\nk log(1/\u03b4)\nn\n\u2264C1\n \n1.7\n\u0010\n\u03c32 + b2\nmax\n\u0011s\nd + log(k/\u03b4)\nwminn\n+ \u03c32 d + log(k/\u03b4)\nwminn\n!\n.\n25\nTherefore, by Lemma 7,\nmax{|\u02c6\u03c32 \u2212\u03c32|, \u2225c\nM2 \u2212M2\u22252} \u22644C1\n \n1.7\n\u0010\n\u03c32 + b2\nmax\n\u0011s\nd + log(k/\u03b4)\nwminn\n+ \u03c32 d + log(k/\u03b4)\nwminn\n!\n+ 4C1\u2225\u00b5\u22252\n \n\u03c3\ns\nd + log(k/\u03b4)\nwminn\n+ bmax\nr\nk log(1/\u03b4)\nn\n!\n+ 2C2\n1\n \n\u03c3\ns\nd + log(k/\u03b4)\nwminn\n+ bmax\nr\nk log(1/\u03b4)\nn\n!2\n\u2264C2(\u03c32 + b2\nmax)\n s\nd + log(k/\u03b4)\nwminn\n+ d + log(k/\u03b4)\nwminn\n!\n.\nThe sample size bound\nn \u2265C \u00b7 d + log(k/\u03b4)\nwmin\n\u00b7\n \u0014\u03ba[M2]1/2(\u03c32 + b2\nmax)\n\u03b32wmin\u03c2k[M2]\u03b5\n\u00152\n+\n\u0014\u03ba[M2]1/2(\u03c32 + b2\nmax)\n\u03b32wmin\u03c2k[M2]\u03b5\n\u0015!\n,\nensures that\nmax\nn|\u02c6\u03c32 \u2212\u03c32|\n\u03c2k[M2] , EM2\no\n\u2264c1\n\u03b32wmin\n\u03ba[M2]1/2 \u03b5 \u22641/3.\n(4)\nNow condition on the above event and take c\nW as given. By Lemma 10,\n\u2225c\nW\u22252 \u2264\np\n1.5/\u03c2k[M2],\nmax\ni\u2208[k] \u2225c\nW \u22a4\u00b5i\u22252 \u2264\u2225c\nW \u22a4A diag(w)1/2\u22252/\u221awmin \u2264\np\n1.5/wmin,\nmax\ni\u2208[k] \u2225c\nW \u22a4M2,ic\nW\u22252 \u22641.5/\u03c2k[M2] + 1.5/wmin,\nmax\ni\u2208[k] \u2225M3,i[c\nW, c\nW, c\nW]\u22252 \u2264(1.5/wmin)3/2 + 3\u03c32p\n1.5/wmin1.5/\u03c2k[M2].\nTherefore, Lemma 5 and Lemma 6 imply that with probability at least 1 \u2212\u03b4,\n\u2225c\nW \u22a4(\u02c6\u00b5 \u2212\u00b5)\u22252 \u2264C3\n\u03c3\n\u03c2k[M2]1/2\ns\nk + log(k/\u03b4)\nwminn\n+ C3\n1\n\u221awmin\nr\nk log(1/\u03b4)\nn\n,\n\u2225c\nM3[c\nW, c\nW, c\nW] \u2212M3[c\nW, c\nW, c\nW]\u22252 \u2264C3\n\u03c33\n\u03c2k[M2]3/2\ns\n(k + log(k/\u03b4))3\nwminn\n+ C3\n\u03c32\n\u221awmin\u03c2k[M2]\n\"s\nk + log(k/\u03b4)\nwminn\n+ k + log(k/\u03b4)\nwminn\n#\n+ C3\n\u03c3\nwmin\u03c2k[M2]1/2\ns\nk + log(k/\u03b4)\nwminn\n+ C3\n \n1\nw3/2\nmin\n+\n\u03c32\n\u221awmin\u03c2k[M2]\n!r\nk log(1/\u03b4)\nn\n.\n26\nTherefore, by Lemma 8 and Lemma 12,\n\u2225bT \u2212T\u22252 \u2264\u2225c\nM3[c\nW, c\nW, c\nW] \u2212M3[c\nW, c\nW, c\nW]\u22252\n+\n4.5\n\u03c2k[M2]\n\u0010\n\u2225c\nW \u22a4(\u02c6\u00b5 \u2212\u00b5)\u22252 +\np\n1.5/wmin\n\u0011\n|\u02c6\u03c32 \u2212\u03c3| + 1.5\u03c32\n\u03c2k[M2]\u2225c\nW \u22a4(\u02c6\u00b5 \u2212\u00b5)\u22252.\n(5)\nThe sample size bound\nn \u2265C \u00b7 k + log(k/\u03b4)\nwmin\n\u00b7\n\u0014 \u03ba[M2]1/2\n\u03b32\u221awmin\u03b5 \u00b7\n\u03c32\n\u03c2k[M2]1/2 \u00b7 max\nn\n1, \u03c32/\u03c2k[M2]\no\u00152\n+ C \u00b7 k log(1/\u03b4)\nwmin\n\u00b7\n\u0014 \u03ba[M2]1/2\n\u03b32\u221awmin\u03b5 \u00b7 max\nn\n1, \u03c32/\u03c2k[M2]\no\u00152\nensures\nmax\nn\n1, \u03c32/\u03c2k[M2]1/2o\u2225c\nW \u22a4(\u02c6\u00b5 \u2212\u00b5)\u22252\n\u03b3\n\u2264c2\n\u03b3\u221awmin\n\u03ba[M2]1/2 \u03b5 \u22641.\n(6)\nFurthermore, the sample size bound\nn \u2265C \u00b7 (k + log(k/\u03b4))3\nwmin\n\u00b7\n\u0014\n\u03ba[M2]1/2\u03c33\n\u03b32\u221awmin\u03c2k[M2]3/2\u03b5\n\u00152\n+ C \u00b7 k + log(k/\u03b4)\nwmin\n\u00b7\n \u0014\n\u03ba[M2]1/2\u03c32\n\u03b32wmin\u03c2k[M2]\u03b5\n\u00152\n+\n\u0014\n\u03ba[M2]1/2\u03c32\n\u03b32wmin\u03c2k[M2]\u03b5\n\u0015!\n+ C \u00b7 k + log(k/\u03b4)\nwmin\n\u00b7\n\u0014\n\u03ba[M2]1/2\u03c3\n\u03b32w3/2\nmin\u03c2k[M2]1/2\u03b5\n\u00152\n+ C \u00b7 k log(1/\u03b4) \u00b7\n \u0014\u03ba[M2]1/2\n\u03b32w2\nmin\u03b5\n\u00152\n+\n\u0014\n\u03ba[M2]1/2\u03c32\n\u03b32wmin\u03c2k[M2]\u03b5\n\u00152!\nensures\n\u2225c\nM3[c\nW, c\nW, c\nW] \u2212M3[c\nW, c\nW, c\nW]\u22252\n\u03b3\n\u2264c2\n\u03b3\u221awmin\n\u03ba[M2]1/2 \u03b5.\n(7)\nUsing the inequalities (6), (7), and (4) with (5) gives\nET = \u2225bT \u2212T\u22252\n\u03b3\n\u2264c3\n\u03b3\u221awmin\n\u03ba[M2]1/2 \u03b5.\n(8)\nSince t = log2(1/\u03b4), the inequality from Lemma 14 holds with probability at least 1 \u2212\u03b4 over the\ninternal randomness of the algorithm. Therefore, using (4) and (8) with Lemma 16 gives in this\nevent:\n\u2225\u02c6\u00b5\u03c0(i) \u2212\u00b5i\u22252 \u2264C4 \u00b7 \u2225\u00b5i\u22252 \u00b7\n\u0010\nEM2 + ET\n\u0011\n\u00b7 \u03ba[M2]1/2\n\u03b3\u221awmin\n+ C4 \u00b7 \u2225M2\u22251/2\n2\n\u00b7\n\u0010\nEM2 + ET\n\u0011\n\u00b7\n1\n\u221awmin\n\u2264\n\u0010\n\u2225\u00b5i\u22252 + \u2225M2\u22251/2\n2\n\u0011\n\u03b5\nfor all i \u2208[k], for some permutation \u03c0 on [k].\n27\nD\nProbability tail inequalities\nWe recall and derive some probability tail inequalities used in the analysis.\nLemma 17 (Dasgupta and Gupta, 2003; Anandkumar et al., 2012b). Pick any \u03b4 \u2208(0, 1), matrix\nX \u2208Rp\u00d7p, and \ufb01nite subset Q \u2286Rp. If \u03b8 \u2208Rp be a random vector distributed uniformly over the\nunit sphere in Rp, then\nPr\n\u0014\nmin\nq\u2208Q |\u03b8\u22a4Xq| > minq\u2208Q \u2225Xq\u22252 \u00b7 \u03b4\n\u221aep|Q|\n\u0015\n\u22651 \u2212\u03b4.\nLemma 18 (Laurent and Massart, 2000). Let z2\n1, z2\n2, . . . , z2\nm be i.i.d. \u03c72 random variables, each\nwith one degree of freedom. Then for any \u03b4 \u2208(0, 1),\nPr\n\u0014 m\nX\ni=1\nz2\ni > m + 2\np\nm ln(1/\u03b4) + 2 ln(1/\u03b4)\n\u0015\n\u2264\u03b4.\nLemma 19 (Litvak et al., 2005; Hsu et al., 2012b). Let y1, y2, . . . , ym be i.i.d. N(0, I) random\nvectors in Rp. Then for any \u01eb0 \u2208(1, 1/2) and \u03b4 \u2208(0, 1),\nPr\n\"\r\r\r\r\n1\nm\nm\nX\ni=1\nyiy\u22a4\ni \u2212I\n\r\r\r\r\n2\n>\n1\n1 \u22122\u01eb0\n r\n32 ln((1 + 2/\u01eb0)p/\u03b4)\nm\n+ 2 ln((1 + 2/\u01eb0)p/\u03b4)\nm\n!#\n\u2264\u03b4.\nLemma 20 (Sums of cubes of normal random variables). Let z1, z2, . . . , zm be i.i.d. N(0, 1) random\nvariables. Then for any \u03b4 \u2208(0, 1),\nPr\n\u0014\f\f\f\f\nm\nX\ni=1\nz3\ni\n\f\f\f\f >\np\n27e3m\u2308ln(1/\u03b4)\u23093\n\u0015\n\u2264\u03b4.\nProof. We use Markov\u2019s inequality via the p-th moment to derive the tail inequality. Pick some\neven integer p \u22652, and observe that\nE\n\u0014\u0012 m\nX\ni=1\nz3\ni\n\u0013p\u0015\n=\nX\ni1,i2,...,ip\u2208[m]\nE\n\u0014 p\nY\nj=1\nz3\nij\n\u0015\n.\nBy the independence of the zi\u2019s, a term in the summation is zero if any index i \u2208[m] is selected\nan odd number of times (i.e., |{j \u2208[p] : ij = i}| is odd, for any i \u2208[m]). Therefore the summation\ncan be written as\nX\np1+\u00b7\u00b7\u00b7+pm=p/2\nE\n\u0014 m\nY\ni=1\nz6pi\ni\n\u0015\n=\nX\np1+\u00b7\u00b7\u00b7+pm=p/2\nm\nY\ni=1\nE\n\u0002\nz6pi\ni\n\u0003\n=\nX\np1+\u00b7\u00b7\u00b7+pm=p/2\nm\nY\ni=1\n(6pi \u22121)!!\nwhere the summations are over non-negative integers p1, p2, . . . , pm that sum to p/2, and n!! is\nthe product of all odd integers between 1 and n; the last step uses the well-known fact that\nE[zk] = (k \u22121)!! for a standard normal random variable z. As pi \u2264p/2 for each i \u2208[m], the\nproduct can be crudely bounded by (3p)3p/2, and hence the sum is bounded by\n(3p)3p/2\n\u0012p/2 + m \u22121\np/2\n\u0013\n\u2264(27p3)p/2\n\u0012(p/2 + m \u22121)e\np/2\n\u0013p/2\n\u2264(27ep3m)p/2.\n28\nBy Markov\u2019s inequality, for any t > 0,\nPr\n\u0014\f\f\f\f\nm\nX\ni=1\nz3\ni\n\f\f\f\f > t\n\u0015\n\u2264t\u2212pE\n\u0014\u0012 m\nX\ni=1\nz3\ni\n\u0013p\u0015\n\u2264\n\u0012p\n27ep3m\nt\n\u0013p\n.\nThe bound is at most \u03b4 for t := e\np\n27em\u2308ln(1/\u03b4)\u23093 and p := \u2308ln(1/\u03b4)\u2309.\nLemma 21 (Third-order tensor of normal random vectors). Let y1, y2, . . . , ym be i.i.d. N(0, I)\nrandom vectors in Rp. Then for any \u01eb0 \u2208(1, 1/3) and \u03b4 \u2208(0, 1),\nPr\n\"\r\r\r\r\n1\nm\nm\nX\ni=1\nyi \u2297yi \u2297yi\n\r\r\r\r\n2\n>\n1\n1 \u22123\u01eb0\nr\n27e3\u2308ln((1 + 2/\u01eb0)p/\u03b4)\u23093\nm\n#\n\u2264\u03b4.\nProof. We follow the covering approach of Litvak et al. (2005). Let Q \u2286{x \u2208Rp : \u2225x\u22252 = 1}\nbe an \u01eb0-cover of the unit sphere in Rp of cardinality at most (1 + 2/\u01eb0)p, which can be shown\nto exist by a standard volume argument (Pisier, 1989).\nLet Y := m\u22121 Pm\ni=1 yi \u2297yi \u2297yi and\n\u01eb := (27e3\u2308ln(|Q|/\u03b4)\u23093/m)1/2. Since y\u22a4\ni q is distributed as N(0, 1) for any q \u2208Q, it follows from\nLemma 20 and a union bound that Pr[\u2203q \u2208Q \u0005 |Y [q, q, q]| > \u01eb] \u2264\u03b4.\nHenceforth we assume\n\u2200q \u2208Q \u0005 |Y [q, q, q] \u2264\u01eb.\nNow pick any unit vector u such that |Y [u, u, u]| is maximized (i.e.,\n\u2225Y \u22252 = |Y [u, u, u]|), choose q \u2208Q such that \u2225q \u2212u\u22252 \u2264\u01eb0, and set \u2206:= u \u2212q and \u00af\u2206:= \u2206/\u2225\u2206\u22252.\nThen\n\u2225Y \u22252 = |Y [u, u, u]|\n= |Y [\u2206, u, u] + Y [q, \u2206, u] + Y [q, q, \u2206] + Y [q, q, q]|\n\u2264\u01eb0(Y [ \u00af\u2206, u, u] + Y [q, \u00af\u2206, u] + Y [q, q, \u00af\u2206]) + \u01eb\nby the triangle inequality and facts \u2225\u2206\u22252 \u2264\u01eb0 and |Y [q, q, q]| \u2264\u01eb. Since Y has the form Y =\nPm\nj=1 \u02dcyj \u2297\u02dcyj \u2297\u02dcyj for vectors \u02dcyj := m\u22121/3yj \u2208Rm, it follows that\nsup\n\u2225u\u22252=\u2225v\u22252=\u2225w\u22252=1\n|Y [u, v, w]| =\nsup\n\u2225u\u22252=\u2225v\u22252=\u2225w\u22252=1\n\f\f\f\f\nr\nX\nj=1\n(u\u22a4\u02dcyj)(v\u22a4\u02dcyj)(w\u22a4\u02dcyj)\n\f\f\f\f\n=\nsup\n\u2225u\u22252=\u2225v\u22252=\u2225w\u22252=1\n\f\f\fu\u22a4\u02dcY diag(w\u22a4\u02dcY ) \u02dcY \u22a4v\n\f\f\f\n=\nsup\n\u2225w\u22252=1\n\u2225\u02dcY diag(w\u22a4\u02dcY ) \u02dcY \u22a4\u22252\n=\nsup\n\u2225u\u22252=\u2225w\u22252=1\n|Y [u, u, w]|\nwhere \u02dcY = [\u02dcy1|\u02dcy2| \u00b7 \u00b7 \u00b7 |\u02dcym] \u2208Rp\u00d7m\u2014i.e., we can take the unit vectors u and v achieving |Y [u, v, w]| =\n\u2225Y \u22252 to be the same. By symmetry, sup\u2225u\u22252=1 |Y [u, u, u]| = \u2225Y \u22252. Therefore\n\u2225Y \u22252 \u2264\u01eb0(Y [ \u00af\u2206, u, u] + Y [q, \u00af\u2206, u] + Y [q, q, \u00af\u2206]) + \u01eb \u22643\u01eb0\u2225Y \u22252 + \u01eb\nwhich implies \u2225Y \u22252 \u2264\u01eb/(1 \u22123\u01eb0). This proves that\nPr[\u2225Y \u22252 \u2264\u01eb/(1 \u22123\u01eb0)] \u2265Pr[\u2200q \u2208Q \u0005 |Y [q, q, q]| \u2264\u01eb] \u22651 \u2212\u03b4\nas required.\n29\n",
        "sentence": "",
        "context": "P. Q. Nguyen and O. Regev. Learning a parallelepiped: Cryptanalysis of GGH and NTRU signa-\ntures. Journal of Cryptology, 22(2):139\u2013160, 2009.\nK. Pearson. Contributions to the mathematical theory of evolution. Philosophical Transactions of\np\nm ln(1/\u03b4) + 2 ln(1/\u03b4)\n\u0015\n\u2264\u03b4.\nLemma 19 (Litvak et al., 2005; Hsu et al., 2012b). Let y1, y2, . . . , ym be i.i.d. N(0, I) random\nvectors in Rp. Then for any \u01eb0 \u2208(1, 1/2) and \u03b4 \u2208(0, 1),\nPr\n\"\r\r\r\r\n1\nm\nm\nX\ni=1\nyiy\u22a4\ni \u2212I\n\r\r\r\r\n2\n>\n1\n1 \u22122\u01eb0\n r\nthe Royal Society, London, A., page 71, 1894.\n9\nG. Pisier. The volume of convex bodies and Banach space geometry. Cambridge University Press,\n1989.\nG. W. Stewart and Ji-Guang Sun. Matrix Perturbation Theory. Academic Press, 1990."
    },
    {
        "title": "Learning boolean formulas",
        "author": [
            "M. Kearns",
            "M. Li",
            "L. Valiant"
        ],
        "venue": "J. ACM 41, 6 (Nov.), 1298\u2013 1328.",
        "citeRegEx": "Kearns et al\\.,? 1994",
        "shortCiteRegEx": "Kearns et al\\.",
        "year": 1994,
        "abstract": "\n            Efficient distribution-free learning of Boolean formulas from positive and negative examples is considered. It is shown that classes of formulas that are efficiently learnable from only positive examples or only negative examples have certain closure properties. A new substitution technique is used to show that in the distribution-free case learning DNF (disjunctive normal form formulas) is no harder than learning monotone DNF. We prove that monomials cannot be efficiently learned from negative examples alone, even if the negative examples are uniformly distributed. It is also shown that, if the examples are drawn from uniform distributions, then the class of DNF in which each variable occurs at most once is efficiently\n            weakly learnable\n            (i.e., individual examples are  correctly classified with a probability larger than 1/2 + 1/\n            p\n            , where\n            p\n            is a polynomial in the relevant parameters of the learning problem). We then show an equivalence between the notion of weak learning and the notion of\n            group learning\n            , where a group of examples of polynomial size, either all positive or all negative, must be correctly classified with high probability.\n          ",
        "full_text": "",
        "sentence": " Within the standard PAC (classification) model, there is a rich theory of reducibility between specific learning problems (Pitt and Warmuth, 1990; Kearns and Valiant, 1994), between classes of learning problems (Schapire, 1990; Kearns, 1998), as well as composition theorems allowing the creation of more complex learning algorithm from simpler ones (Kearns et al., 1994). A natural starting point for such an investigation is with the standard PAC supervised learning model, and its distributional analogue (Kearns et al., 1994), since these models are each already populated with a number of algorithms with strong theoretical guarantees. Our work also relies on the PAC model for distribution learning (Kearns et al., 1994), including a long line of works on learning mixtures of distributions (see e. Distribution Learning We also make use of results from for PAC learning probability distributions (Kearns et al., 1994). Distribution Learning We also make use of results from for PAC learning probability distributions (Kearns et al., 1994). A distribution class P is efficiently learnable if there exists a polynomialtime algorithm that, given sample access to an unknown target distribution P, outputs an accurate distribution P\u0302 such that KL(P ||P\u0302) \u2264 \u03b5 for some target accuracy \u03b5. For any distribution P \u2208 P and any point y \u2208 Y , we assume that we can evaluate the probability (density) of y assigned by P (referred to as learning with an evaluator in Kearns et al. (1994); see the appendix for the formal",
        "context": null
    },
    {
        "title": "Cryptographic limitations on learning boolean formulae and finite automata",
        "author": [
            "M. Kearns",
            "L. Valiant"
        ],
        "venue": "J. ACM 41, 1 (Jan.), 67\u201395.",
        "citeRegEx": "Kearns and Valiant,? 1994",
        "shortCiteRegEx": "Kearns and Valiant",
        "year": 1994,
        "abstract": "\n            In this paper, we prove the intractability of learning several classes of Boolean functions in the distribution-free model (also called the Probably Approximately Correct or PAC model) of learning from examples. These results are\n            representation independent\n            , in that they hold regardless of the syntactic form in which the learner chooses to represent its hypotheses.\n          \n          Our methods reduce the problems of cracking a number of well-known public-key cryptosystems to the learning problems. We prove that a polynomial-time learning algorithm for Boolean formulae, deterministic finite automata or constant-depth threshold circuits would have dramatic consequences for cryptography and number theory. In particular, such an algorithm could be used to break the RSA cryptosystem, factor Blum integers (composite numbers equivalent to 3 modulo 4), and detect quadratic residues. The results hold even if the learning algorithm is only required to obtain a slight advantage in prediction over random guessing. The techniques used demonstrate an interesting duality between learning and cryptography.\n          We also apply our results to obtain strong intractability results for approximating a generalization of graph coloring.",
        "full_text": "",
        "sentence": " Within the standard PAC (classification) model, there is a rich theory of reducibility between specific learning problems (Pitt and Warmuth, 1990; Kearns and Valiant, 1994), between classes of learning problems (Schapire, 1990; Kearns, 1998), as well as composition theorems allowing the creation of more complex learning algorithm from simpler ones (Kearns et al.",
        "context": null
    },
    {
        "title": "Efficient noise-tolerant learning from statistical queries",
        "author": [
            "M.J. Kearns"
        ],
        "venue": "J. ACM 45, 6, 983\u2013 1006.",
        "citeRegEx": "Kearns,? 1998",
        "shortCiteRegEx": "Kearns",
        "year": 1998,
        "abstract": "\n            In this paper, we study the problem of learning in the presence of classification noise in the probabilistic learning model of Valiant and its variants. In order to identify the class of \u201crobust\u201d learning algorithms in the most general way, we formalize a new but related model of learning from\n            statistical queries\n            . Intuitively, in this model a learning algorithm is forbidden to examine individual examples of the unknown target function, but is given acess to an oracle providing estimates of probabilities over the sample space of random examples.\n          \n          One of our main results shows that any class of functions learnable from statistical queries is in fact learnable with classification noise in Valiant's model, with a  noise rate approaching the  information-theoretic barrier of 1/2. We then demonstrate the generality of the statistical query model, showing that practically every class learnable in Valiant's model and its variants can also be learned in the new model (and thus can be learned in the presence of noise). A notable exception to this statement is the class of parity functions, which we prove is not learnable from statistical queries, and for which no noise-tolerant algorithm is known.",
        "full_text": "",
        "sentence": " Within the standard PAC (classification) model, there is a rich theory of reducibility between specific learning problems (Pitt and Warmuth, 1990; Kearns and Valiant, 1994), between classes of learning problems (Schapire, 1990; Kearns, 1998), as well as composition theorems allowing the creation of more complex learning algorithm from simpler ones (Kearns et al. Since practically every C known to be PAC learnable can also be learned with classification noise (either directly or via the statistical query framework (Kearns, 1998), with parity-based constructions being the only known exceptions), and the distribution classes P known to be PAC learnable have small sets of distinguishing events (such as product distributions), and/or have mixture learning algorithms (such as Gaussians), our results yield efficient PwD algorithms for almost all combinations of PAC classification and distribution learning algorithms known to date. First, our work is related to the results in PAC learning under classification noise (Angluin and Laird, 1987; Decatur, 1997; Kearns, 1998), and makes use of a result by Ralaivola et al.",
        "context": null
    },
    {
        "title": "On the learnability of discrete distributions",
        "author": [
            "M.J. Kearns",
            "Y. Mansour",
            "D. Ron",
            "R. Rubinfeld",
            "R.E. Schapire",
            "L. Sellie"
        ],
        "venue": "Proceedings of the Twenty-Sixth Annual ACM Symposium on Theory of Computing, 23-25 May 1994, Montr\u00e9al, Qu\u00e9bec, Canada. 273\u2013282.",
        "citeRegEx": "Kearns et al\\.,? 1994",
        "shortCiteRegEx": "Kearns et al\\.",
        "year": 1994,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "Asymptotic methods in statistical decision theory",
        "author": [
            "L.M. Le Cam"
        ],
        "venue": "Springer series in statistics. Springer-Verlag, New York.",
        "citeRegEx": "Cam,? 1986",
        "shortCiteRegEx": "Cam",
        "year": 1986,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " Our result relies on the well-known Le Cam\u2019s method, which is a powerful tool for giving lower bounds in hypothesis testing. We state the following version for our purpose.6 Lemma 5. [Le Cam\u2019s method (see e.g. Le Cam (1986); Yu (1997))] Let Q0 and Q1 be two probability distributions over Y , and let A : Ym \u2192 {0,1} be a mapping from m observations in Y to either 0 or 1. Our result relies on the well-known Le Cam\u2019s method, which is a powerful tool for giving lower bounds in hypothesis testing. We state the following version for our purpose.6 Lemma 5. [Le Cam\u2019s method (see e.g. Le Cam (1986); Yu (1997))] Let Q0 and Q1 be two probability distributions over Y , and let A : Ym \u2192 {0,1} be a mapping from m observations in Y to either 0 or 1.",
        "context": null
    },
    {
        "title": "Prediction-preserving reducibility",
        "author": [
            "L. Pitt",
            "M.K. Warmuth"
        ],
        "venue": "J. Comput. Syst. Sci. 41, 3 (Dec.), 430\u2013467.",
        "citeRegEx": "Pitt and Warmuth,? 1990",
        "shortCiteRegEx": "Pitt and Warmuth",
        "year": 1990,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " Within the standard PAC (classification) model, there is a rich theory of reducibility between specific learning problems (Pitt and Warmuth, 1990; Kearns and Valiant, 1994), between classes of learning problems (Schapire, 1990; Kearns, 1998), as well as composition theorems allowing the creation of more complex learning algorithm from simpler ones (Kearns et al.",
        "context": null
    },
    {
        "title": "CN = CPCN",
        "author": [
            "L. Ralaivola",
            "F. Denis",
            "C.N. Magnan"
        ],
        "venue": "InMachine Learning, Proceedings of the Twenty-Third International Conference (ICML 2006), Pittsburgh, Pennsylvania, USA, June 25-29, 2006. 721\u2013728.",
        "citeRegEx": "Ralaivola et al\\.,? 2006",
        "shortCiteRegEx": "Ralaivola et al\\.",
        "year": 2006,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " CCCNLearning In amore general noisemodel calledClass-Conditional Classification Noise (CCCN) proposed by Ralaivola et al. (2006), the example oracle EX \u03b7 CCCN has class-dependent noise rates\u2014 that is, the noise rate \u03b70 for the negative examples (c(x) = 0) and the noise rate \u03b71 for the positive examples (c(x) = 1) may be different, and both below 1/2. CCCNLearning In amore general noisemodel calledClass-Conditional Classification Noise (CCCN) proposed by Ralaivola et al. (2006), the example oracle EX \u03b7 CCCN has class-dependent noise rates\u2014 that is, the noise rate \u03b70 for the negative examples (c(x) = 0) and the noise rate \u03b71 for the positive examples (c(x) = 1) may be different, and both below 1/2. Moreover, Ralaivola et al. (2006) show that any class that is learnable under CN is also learnable under CCCN. Thus, whenever the class C is learnable under CN (and hence learnable under CCCN by Ralaivola et al. (2006)), we can learn the target concept c under the PwD model using a distinguishing event.",
        "context": null
    },
    {
        "title": "The strength of weak learnability",
        "author": [
            "R.E. Schapire"
        ],
        "venue": "Mach. Learn. 5, 2 (July), 197\u2013227.",
        "citeRegEx": "Schapire,? 1990",
        "shortCiteRegEx": "Schapire",
        "year": 1990,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " Within the standard PAC (classification) model, there is a rich theory of reducibility between specific learning problems (Pitt and Warmuth, 1990; Kearns and Valiant, 1994), between classes of learning problems (Schapire, 1990; Kearns, 1998), as well as composition theorems allowing the creation of more complex learning algorithm from simpler ones (Kearns et al.",
        "context": null
    },
    {
        "title": "A theory of the learnable",
        "author": [
            "L.G. Valiant"
        ],
        "venue": "Proceedings of the 16th Annual ACM Symposium on Theory of Computing, April 30 - May 2, 1984, Washington, DC, USA. 436\u2013445.",
        "citeRegEx": "Valiant,? 1984",
        "shortCiteRegEx": "Valiant",
        "year": 1984,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " At the highest level, our model falls under the framework of Haussler (1992), which gives a decision-theoretic treatment of PAC-style learning (Valiant, 1984) for very general loss functions; our model can be viewed as a special case in which the loss function is conditional log-loss given the value of a classifier.",
        "context": null
    },
    {
        "title": "A spectral algorithm for learning mixture models",
        "author": [
            "S. Vempala",
            "G. Wang"
        ],
        "venue": "J. Comput. Syst. Sci. 68, 4, 841\u2013860.",
        "citeRegEx": "Vempala and Wang,? 2004",
        "shortCiteRegEx": "Vempala and Wang",
        "year": 2004,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "Assouad, fano, and le cam",
        "author": [
            "B. Yu"
        ],
        "venue": "Festschrift for Lucien Le Cam. Springer New York, 423\u2013435. 15",
        "citeRegEx": "Yu,? 1997",
        "shortCiteRegEx": "Yu",
        "year": 1997,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "We say that a probability model T is good if err(T ) \u2264 4\u03b5, and bad otherwise. We know that T is guaranteed to contain at least one good model",
        "author": [
            "Feldman"
        ],
        "venue": null,
        "citeRegEx": "Feldman,? \\Q2008\\E",
        "shortCiteRegEx": "Feldman",
        "year": 2008,
        "abstract": "",
        "full_text": "",
        "sentence": "",
        "context": null
    }
]