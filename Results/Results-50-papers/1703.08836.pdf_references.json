[
    {
        "title": "Computational stereo",
        "author": [
            "S.T. Barnard",
            "M.A. Fischler"
        ],
        "venue": "ACM Computing Surveys, 14(4):553\u2013572",
        "citeRegEx": "1",
        "shortCiteRegEx": null,
        "year": 1982,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " Much of the early work addressed the minimal two-view case [1].",
        "context": null
    },
    {
        "title": "PatchMatch: a randomized correspondence algorithm for structural image editing",
        "author": [
            "C. Barnes",
            "E. Shechtman",
            "A. Finkelstein",
            "D. Goldman"
        ],
        "venue": "ACM Transactions on Graphics, 28(3)",
        "citeRegEx": "2",
        "shortCiteRegEx": null,
        "year": 2009,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " The other avoids exhaustive testing and instead relies on efficient random sampling and propagation schemes like PatchMatch [2] to find a good depth estimate at every position, e.",
        "context": null
    },
    {
        "title": "3D- R2N2: A unified approach for single and multi-view 3d object reconstruction",
        "author": [
            "C.B. Choy",
            "D. Xu",
            "J. Gwak",
            "K. Chen",
            "S. Savarese"
        ],
        "venue": null,
        "citeRegEx": "3",
        "shortCiteRegEx": "3",
        "year": 2016,
        "abstract": "",
        "full_text": "",
        "sentence": " Recently, bolder ideas have emerged, all the way to learning an end-to-end mapping from images to (volumetric, low-resolution) 3D models [3].",
        "context": null
    },
    {
        "title": "A space-sweep approach to true multi-image matching",
        "author": [
            "R.T. Collins"
        ],
        "venue": null,
        "citeRegEx": "4",
        "shortCiteRegEx": "4",
        "year": 1996,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " To test our similarity measure, we embed it into a standard depth estimation setup, namely multi-view plane-sweeping [4]: for each pixel in an image we compute similarity scores for a range of depths along the ray, and pick the depth with the highest similarity. One exhaustively tests for all possible depths, often by \u201csweeping\u201d a fronto-parallel plane through the 3D object space along the depth axis [4] or in multiple directions [10] to sample different depth values efficiently.",
        "context": null
    },
    {
        "title": "Minimizing the multi-view stereo reprojection error for triangular surface meshes",
        "author": [
            "A. Delaunoy",
            "E. Prados",
            "P.G.I. Pirac\u00e9s",
            "J.-P. Pons",
            "P. Sturm"
        ],
        "venue": null,
        "citeRegEx": "5",
        "shortCiteRegEx": "5",
        "year": 2008,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " We note that the same is true for \u201cmulti-view stereo\u201d methods that reconstruct implicit [25] or explicit [5] surfaces. We note that there is also a large body of work termed \u201cmulti-view reconstruction\u201d that in fact computes depth maps from two views and focuses on integrating the corresponding 3D point clouds into implicit [7, 25] or explicit [5] surface representations.",
        "context": null
    },
    {
        "title": "Massively parallel multiview stereopsis by surface normal diffusion",
        "author": [
            "S. Galliani",
            "K. Lasinger",
            "K. Schindler"
        ],
        "venue": null,
        "citeRegEx": "6",
        "shortCiteRegEx": "6",
        "year": 2015,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " [6].",
        "context": null
    },
    {
        "title": "Just look at the image: viewpoint-specific surface normal prediction for improved multi-view reconstruction",
        "author": [
            "S. Galliani",
            "K. Schindler"
        ],
        "venue": null,
        "citeRegEx": "7",
        "shortCiteRegEx": "7",
        "year": 2016,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " We note that there is also a large body of work termed \u201cmulti-view reconstruction\u201d that in fact computes depth maps from two views and focuses on integrating the corresponding 3D point clouds into implicit [7, 25] or explicit [5] surface representations.",
        "context": null
    },
    {
        "title": "Are we ready for autonomous driving? the KITTI vision benchmark suite",
        "author": [
            "A. Geiger",
            "P. Lenz",
            "R. Urtasun"
        ],
        "venue": null,
        "citeRegEx": "8",
        "shortCiteRegEx": "8",
        "year": 2012,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " A discussion of the complete twoview stereo literature is beyond the scope of this paper, for an overview and further reading please refer to benchmarks like the Middlebury Stereo page [17] or KITTI [8].",
        "context": null
    },
    {
        "title": "MatchNet: Unifying feature and metric learning for patchbased matching",
        "author": [
            "X. Han",
            "T. Leung",
            "Y. Jia",
            "R. Sukthankar",
            "A.C. Berg"
        ],
        "venue": null,
        "citeRegEx": "9",
        "shortCiteRegEx": "9",
        "year": 2015,
        "abstract": "",
        "full_text": "",
        "sentence": " Closely related work started from separate steps for descriptor learning and metric learning, and unified them to effectively obtain a direct similarity prediction from raw image data [9] as well.",
        "context": null
    },
    {
        "title": "Real-time direct dense matching on fisheye images using plane-sweeping stereo",
        "author": [
            "C. H\u00e4ne",
            "L. Heng",
            "G.H. Lee",
            "A. Sizov",
            "M. Pollefeys"
        ],
        "venue": null,
        "citeRegEx": "10",
        "shortCiteRegEx": "10",
        "year": 2014,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " One exhaustively tests for all possible depths, often by \u201csweeping\u201d a fronto-parallel plane through the 3D object space along the depth axis [4] or in multiple directions [10] to sample different depth values efficiently. Having introduced the multi-view similarity computation, we use it as a building block in a fairly standard multiview stereo pipeline [10], assuming known camera poses (e. These five images and their camera poses are fed to a plane-sweeping routine \u2013 we use the publicly available code of [10]. The plane-sweeping library [10] offers two such options: (i) interpolation of the discrete depth levels to sub-pixel accuracy, and (ii) box filtering to account for correlations between nearby depth values.",
        "context": null
    },
    {
        "title": "Computer Matching of Areas in Stereo Images",
        "author": [
            "M.J. Hannah"
        ],
        "venue": "PhD thesis, Stanford University",
        "citeRegEx": "11",
        "shortCiteRegEx": null,
        "year": 1974,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " Zero-mean normalized cross correlation (ZNCC) [11] is another popular similarity measure, which features invariance against linear brightness changes.",
        "context": null
    },
    {
        "title": "Large scale multi-view stereopsis evaluation",
        "author": [
            "R. Jensen",
            "A. Dahl",
            "G. Vogiatzis",
            "E. Tola",
            "H. Aans"
        ],
        "venue": null,
        "citeRegEx": "12",
        "shortCiteRegEx": "12",
        "year": 2014,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " A host of rather successful multi-view stereo methods exist (see benchmark results such as [12, 18, 19, 22]). The network is trained on a portion of the public DTU multi-view dataset [12], and evaluated on the remaining part of it, as well as on an unrelated public dataset. Again, we refer the reader to benchmark datasets such as [19] and [12] for an overview of contemporary multi-view stereo algorithms. As training data, we sample image patches from 49 scenes of the DTU dataset [12] (Fig. In the first experiment, we demonstrate the performance of our approach using the DTU [12] evaluation framework and compare it to four existing baseline methods.",
        "context": null
    },
    {
        "title": "Caffe: Convolutional architecture for fast feature embedding",
        "author": [
            "Y. Jia",
            "E. Shelhamer",
            "J. Donahue",
            "S. Karayev",
            "J. Long",
            "R. Girshick",
            "S. Guadarrama",
            "T. Darrell"
        ],
        "venue": "arXiv preprint arXiv:1408.5093",
        "citeRegEx": "13",
        "shortCiteRegEx": null,
        "year": 2014,
        "abstract": "Caffe provides multimedia scientists and practitioners with a clean and\nmodifiable framework for state-of-the-art deep learning algorithms and a\ncollection of reference models. The framework is a BSD-licensed C++ library\nwith Python and MATLAB bindings for training and deploying general-purpose\nconvolutional neural networks and other deep models efficiently on commodity\narchitectures. Caffe fits industry and internet-scale media needs by CUDA GPU\ncomputation, processing over 40 million images a day on a single K40 or Titan\nGPU ($\\approx$ 2.5 ms per image). By separating model representation from\nactual implementation, Caffe allows experimentation and seamless switching\namong platforms for ease of development and deployment from prototyping\nmachines to cloud environments. Caffe is maintained and developed by the\nBerkeley Vision and Learning Center (BVLC) with the help of an active community\nof contributors on GitHub. It powers ongoing research projects, large-scale\nindustrial applications, and startup prototypes in vision, speech, and\nmultimedia.",
        "full_text": "Caffe: Convolutional Architecture\nfor Fast Feature Embedding\n\u2217\nYangqing Jia\u2217, Evan Shelhamer\u2217, Jeff Donahue, Sergey Karayev,\nJonathan Long, Ross Girshick, Sergio Guadarrama, Trevor Darrell\nUC Berkeley EECS, Berkeley, CA 94702\n{jiayq,shelhamer,jdonahue,sergeyk,jonlong,rbg,sguada,trevor}@eecs.berkeley.edu\nABSTRACT\nCa\ufb00e provides multimedia scientists and practitioners with\na clean and modi\ufb01able framework for state-of-the-art deep\nlearning algorithms and a collection of reference models.\nThe framework is a BSD-licensed C++ library with Python\nand MATLAB bindings for training and deploying general-\npurpose convolutional neural networks and other deep mod-\nels e\ufb03ciently on commodity architectures. Ca\ufb00e \ufb01ts indus-\ntry and internet-scale media needs by CUDA GPU computa-\ntion, processing over 40 million images a day on a single K40\nor Titan GPU (\u22482.5 ms per image). By separating model\nrepresentation from actual implementation, Ca\ufb00e allows ex-\nperimentation and seamless switching among platforms for\nease of development and deployment from prototyping ma-\nchines to cloud environments.\nCa\ufb00e is maintained and developed by the Berkeley Vi-\nsion and Learning Center (BVLC) with the help of an ac-\ntive community of contributors on GitHub. It powers on-\ngoing research projects, large-scale industrial applications,\nand startup prototypes in vision, speech, and multimedia.\nCategories and Subject Descriptors\nI.5.1 [Pattern Recognition]: [Applications\u2013Computer vi-\nsion]; D.2.2 [Software Engineering]: [Design Tools and\nTechniques\u2013Software libraries]; I.5.1 [Pattern Recognition]:\n[Models\u2013Neural Nets]\nGeneral Terms\nAlgorithms, Design, Experimentation\nKeywords\nOpen Source, Computer Vision, Neural Networks, Parallel\nComputation, Machine Learning\n\u2217Corresponding Authors.\nThe work was done while\nYangqing Jia was a graduate student at Berkeley.\nHe is\ncurrently a research scientist at Google, 1600 Amphitheater\nPkwy, Mountain View, CA 94043.\n.\n1.\nINTRODUCTION\nA key problem in multimedia data analysis is discovery of\ne\ufb00ective representations for sensory inputs\u2014images, sound-\nwaves, haptics, etc.\nWhile performance of conventional,\nhandcrafted features has plateaued in recent years, new de-\nvelopments in deep compositional architectures have kept\nperformance levels rising [8].\nDeep models have outper-\nformed hand-engineered feature representations in many do-\nmains, and made learning possible in domains where engi-\nneered features were lacking entirely.\nWe are particularly motivated by large-scale visual recog-\nnition, where a speci\ufb01c type of deep architecture has achieved\na commanding lead on the state-of-the-art.\nThese Con-\nvolutional Neural Networks, or CNNs, are discriminatively\ntrained via back-propagation through layers of convolutional\n\ufb01lters and other operations such as recti\ufb01cation and pooling.\nFollowing the early success of digit classi\ufb01cation in the 90\u2019s,\nthese models have recently surpassed all known methods for\nlarge-scale visual recognition, and have been adopted by in-\ndustry heavyweights such as Google, Facebook, and Baidu\nfor image understanding and search.\nWhile deep neural networks have attracted enthusiastic\ninterest within computer vision and beyond, replication of\npublished results can involve months of work by a researcher\nor engineer. Sometimes researchers deem it worthwhile to\nrelease trained models along with the paper advertising their\nperformance. But trained models alone are not su\ufb03cient for\nrapid research progress and emerging commercial applica-\ntions, and few toolboxes o\ufb00er truly o\ufb00-the-shelf deployment\nof state-of-the-art models\u2014and those that do are often not\ncomputationally e\ufb03cient and thus unsuitable for commercial\ndeployment.\nTo address such problems, we present Ca\ufb00e, a fully open-\nsource framework that a\ufb00ords clear access to deep architec-\ntures.\nThe code is written in clean, e\ufb03cient C++, with\nCUDA used for GPU computation, and nearly complete,\nwell-supported bindings to Python/Numpy and MATLAB.\nCa\ufb00e adheres to software engineering best practices, pro-\nviding unit tests for correctness and experimental rigor and\nspeed for deployment. It is also well-suited for research use,\ndue to the careful modularity of the code, and the clean sep-\naration of network de\ufb01nition (usually the novel part of deep\nlearning research) from actual implementation.\nIn Ca\ufb00e, multimedia scientists and practitioners have an\norderly and extensible toolkit for state-of-the-art deep learn-\ning algorithms, with reference models provided out of the\nbox. Fast CUDA code and GPU computation \ufb01t industry\nneeds by achieving processing speeds of more than 40 mil-\narXiv:1408.5093v1  [cs.CV]  20 Jun 2014\nCore\nOpen\nPretrained\nFramework\nLicense\nlanguage\nBinding(s)\nCPU\nGPU\nsource\nTraining\nmodels\nDevelopment\nCa\ufb00e\nBSD\nC++\nPython,\ndistributed\nMATLAB\ncuda-convnet [7]\nunspeci\ufb01ed\nC++\nPython\ndiscontinued\nDecaf [2]\nBSD\nPython\ndiscontinued\nOverFeat [9]\nunspeci\ufb01ed\nLua\nC++,Python\ncentralized\nTheano/Pylearn2 [4]\nBSD\nPython\ndistributed\nTorch7 [1]\nBSD\nLua\ndistributed\nTable 1: Comparison of popular deep learning frameworks. Core language is the main library language, while\nbindings have an o\ufb03cially supported library interface for feature extraction, training, etc. CPU indicates\navailability of host-only computation, no GPU usage (e.g., for cluster deployment); GPU indicates the GPU\ncomputation capability essential for training modern CNNs.\nlion images per day on a single K40 or Titan GPU. The\nsame models can be run in CPU or GPU mode on a vari-\nety of hardware: Ca\ufb00e separates the representation from the\nactual implementation, and seamless switching between het-\nerogeneous platforms furthers development and deployment\u2014\nCa\ufb00e can even be run in the cloud.\nWhile Ca\ufb00e was \ufb01rst designed for vision, it has been adopted\nand improved by users in speech recognition, robotics, neu-\nroscience, and astronomy. We hope to see this trend con-\ntinue so that further sciences and industries can take advan-\ntage of deep learning.\nCa\ufb00e is maintained and developed by the BVLC with the\nactive e\ufb00orts of several graduate students, and welcomes\nopen-source contributions at http://github.com/BVLC/caffe.\nWe thank all of our contributors for their work!\n2.\nHIGHLIGHTS OF CAFFE\nCa\ufb00e provides a complete toolkit for training, testing,\n\ufb01netuning, and deploying models, with well-documented ex-\namples for all of these tasks. As such, it\u2019s an ideal starting\npoint for researchers and other developers looking to jump\ninto state-of-the-art machine learning. At the same time,\nit\u2019s likely the fastest available implementation of these algo-\nrithms, making it immediately useful for industrial deploy-\nment.\nModularity. The software is designed from the begin-\nning to be as modular as possible, allowing easy extension to\nnew data formats, network layers, and loss functions. Lots\nof layers and loss functions are already implemented, and\nplentiful examples show how these are composed into train-\nable recognition systems for various tasks.\nSeparation of representation and implementation.\nCa\ufb00e model de\ufb01nitions are written as con\ufb01g \ufb01les using the\nProtocol Bu\ufb00er language.\nCa\ufb00e supports network archi-\ntectures in the form of arbitrary directed acyclic graphs.\nUpon instantiation, Ca\ufb00e reserves exactly as much memory\nas needed for the network, and abstracts from its underly-\ning location in host or GPU. Switching between a CPU and\nGPU implementation is exactly one function call.\nTest coverage. Every single module in Ca\ufb00e has a test,\nand no new code is accepted into the project without corre-\nsponding tests. This allows rapid improvements and refac-\ntoring of the codebase, and imparts a welcome feeling of\npeacefulness to the researchers using the code.\nPython and MATLAB bindings.\nFor rapid proto-\ntyping and interfacing with existing research code, Ca\ufb00e\nprovides Python and MATLAB bindings. Both languages\nmay be used to construct networks and classify inputs. The\nPython bindings also expose the solver module for easy pro-\ntotyping of new training procedures.\nPre-trained reference models. Ca\ufb00e provides (for aca-\ndemic and non-commercial use\u2014not BSD license) reference\nmodels for visual tasks, including the landmark \u201cAlexNet\u201d\nImageNet model [8] with variations and the R-CNN detec-\ntion model [3].\nMore are scheduled for release.\nWe are\nstrong proponents of reproducible research: we hope that\na common software substrate will foster quick progress in\nthe search over network architectures and applications.\n2.1\nComparison to related software\nWe summarize the landscape of convolutional neural net-\nwork software used in recent publications in Table 1. While\nour list is incomplete, we have included the toolkits that are\nmost notable to the best of our knowledge. Ca\ufb00e di\ufb00ers from\nother contemporary CNN frameworks in two major ways:\n(1) The implementation is completely C++ based, which\neases integration into existing C++ systems and interfaces\ncommon in industry. The CPU mode removes the barrier of\nspecialized hardware for deployment and experiments once\na model is trained.\n(2) Reference models are provided o\ufb00-the-shelf for quick\nexperimentation with state-of-the-art results, without the\nneed for costly re-learning. By \ufb01netuning for related tasks,\nsuch as those explored by [2], these models provide a warm-\nstart to new research and applications. Crucially, we publish\nnot only the trained models but also the recipes and code\nto reproduce them.\n3.\nARCHITECTURE\n3.1\nData Storage\nCa\ufb00e stores and communicates data in 4-dimensional ar-\nrays called blobs.\nBlobs provide a uni\ufb01ed memory interface, holding batches\nof images (or other data), parameters, or parameter updates.\nBlobs conceal the computational and mental overhead of\nmixed CPU/GPU operation by synchronizing from the CPU\nhost to the GPU device as needed. In practice, one loads\ndata from the disk to a blob in CPU code, calls a CUDA\nkernel to do GPU computation, and ferries the blob o\ufb00to\nthe next layer, ignoring low-level details while maintaining\na high level of performance. Memory on the host and device\nis allocated on demand (lazily) for e\ufb03cient memory usage.\n\u0001\u0002\u0003\u0004\u0005\u0001\u0006\u0006\u0007\b\u0002\b\t\n\u000b\f\r\u000e\n\u0001\u0002\u0003\n\f\t\u0006\u000f\u0003\u0004\u0005\f\t\u0006\u000f\u000e\n\f\t\u0006\u000f\u0003\n\u0002\t\t\u0010\u0011\n\n\u0012\r\u0012\n\f\t\u0006\u000f\u0011\u0004\u0005\f\t\u0006\u000f\u000e\n\u0002\t\t\u0010\u0003\n\u0001\u0002\u0011\u0004\u0005\u0001\u0006\u0006\u0007\b\u0002\b\t\n\u000b\f\r\u000e\n\u0010\t\u0013\u0013\u0004\u0005\u0013\t\u0014\r\u0015\u0012\u0016\u0017\u0010\t\u0013\u0013\u000e\n\u0001\u0002\u0011\n\u0015\u0006\u0001\u0013\r\u0018\u0001\u0006\u0002\u000b\r\u0004\u0005\n\u0012\r\u0012\u000e\n\u0010\u0012\u0019\u0007\u0010\n\f\t\u0006\u000f\u0011\n\u0002\t\t\u0010\u0011\u0004\u0005\u0002\t\t\u0010\u000e\n\u0002\t\t\u0010\u0003\u0004\u0005\u0002\t\t\u0010\u000e\n\b\u0007\u0010\u000b\u0011\u0004\u0005\b\u0007\u0010\u000b\u000e\n\b\u0007\u0010\u000b\u0011\nFigure 1: An MNIST digit classi\ufb01cation example of a Ca\ufb00e network, where blue boxes represent layers and\nyellow octagons represent data blobs produced by or fed into the layers.\nModels are saved to disk as Google Protocol Bu\ufb00ers1,\nwhich have several important features: minimal-size binary\nstrings when serialized, e\ufb03cient serialization, a human-readable\ntext format compatible with the binary version, and e\ufb03-\ncient interface implementations in multiple languages, most\nnotably C++ and Python.\nLarge-scale data is stored in LevelDB2 databases. In our\ntest program, LevelDB and Protocol Bu\ufb00ers provide a through-\nput of 150MB/s on commodity machines with minimal CPU\nimpact. Thanks to layer-wise design (discussed below) and\ncode modularity, we have recently added support for other\ndata sources, including some contributed by the open source\ncommunity.\n3.2\nLayers\nA Ca\ufb00e layer is the essence of a neural network layer: it\ntakes one or more blobs as input, and yields one or more\nblobs as output. Layers have two key responsibilities for the\noperation of the network as a whole: a forward pass that\ntakes the inputs and produces the outputs, and a backward\npass that takes the gradient with respect to the output, and\ncomputes the gradients with respect to the parameters and\nto the inputs, which are in turn back-propagated to earlier\nlayers.\nCa\ufb00e provides a complete set of layer types including: con-\nvolution, pooling, inner products, nonlinearities like recti\ufb01ed\nlinear and logistic, local response normalization, element-\nwise operations, and losses like softmax and hinge. These are\nall the types needed for state-of-the-art visual tasks. Coding\ncustom layers requires minimal e\ufb00ort due to the composi-\ntional construction of networks.\n3.3\nNetworks and Run Mode\nCa\ufb00e does all the bookkeeping for any directed acyclic\ngraph of layers, ensuring correctness of the forward and\nbackward passes. Ca\ufb00e models are end-to-end machine learn-\ning systems. A typical network begins with a data layer that\nloads from disk and ends with a loss layer that computes the\nobjective for a task such as classi\ufb01cation or reconstruction.\nThe network is run on CPU or GPU by setting a single\nswitch.\nLayers come with corresponding CPU and GPU\nroutines that produce identical results (with tests to prove\nit). The CPU/GPU switch is seamless and independent of\nthe model de\ufb01nition.\n3.4\nTraining A Network\nCa\ufb00e trains models by the fast and standard stochastic\ngradient descent algorithm.\nFigure 1 shows a typical ex-\nample of a Ca\ufb00e network (for MNIST digit classi\ufb01cation)\nduring training: a data layer fetches the images and labels\n1https://code.google.com/p/protobuf/\n2https://code.google.com/p/leveldb/\nFigure 2: An example of the Ca\ufb00e object classi\ufb01ca-\ntion demo. Try it out yourself online!\nfrom disk, passes it through multiple layers such as con-\nvolution, pooling and recti\ufb01ed linear transforms, and feeds\nthe \ufb01nal prediction into a classi\ufb01cation loss layer that pro-\nduces the loss and gradients which train the whole network.\nThis example is found in the Ca\ufb00e source code at exam-\nples/lenet/lenet_train.prototxt. Data are processed in\nmini-batches that pass through the network sequentially. Vi-\ntal to training are learning rate decay schedules, momentum,\nand snapshots for stopping and resuming, all of which are\nimplemented and documented.\nFinetuning, the adaptation of an existing model to new\narchitectures or data, is a standard method in Ca\ufb00e. From\na snapshot of an existing network and a model de\ufb01nition for\nthe new network, Ca\ufb00e \ufb01netunes the old model weights for\nthe new task and initializes new weights as needed. This\ncapability is essential for tasks such as knowledge transfer\n[2], object detection [3], and object retrieval [5].\n4.\nAPPLICATIONS AND EXAMPLES\nIn its \ufb01rst six months since public release, Ca\ufb00e has al-\nready been used in a large number of research projects at\nUC Berkeley and other universities, achieving state-of-the-\nart performance on a number of tasks. Members of Berkeley\nEECS have also collaborated with several industry partners\nsuch as Facebook [11] and Adobe [6], using Ca\ufb00e or its direct\nprecursor (Decaf) to obtain state-of-the-art results.\nObject Classi\ufb01cation Ca\ufb00e has an online demo3 show-\ning state-of-the-art object classi\ufb01cation on images provided\nby the users, including via mobile phone. The demo takes\nthe image and tries to categorize it into one of the 1,000 Im-\nageNet categories4. A typical classi\ufb01cation result is shown\nin Figure 2.\nFurthermore, we have successfully trained a model with\nall 10,000 categories of the full ImageNet dataset by \ufb01ne-\ntuning this network. The resulting network has been applied\nto open vocabulary object retrieval [5].\n3http://demo.caffe.berkeleyvision.org/\n4http://www.image-net.org/challenges/LSVRC/2013/\nFigure 3:\nFeatures extracted from a deep network,\nvisualized in a 2-dimensional space. Note the clear\nseparation between categories, indicative of a suc-\ncessful embedding.\nLearning Semantic Features In addition to end-to-end\ntraining, Ca\ufb00e can also be used to extract semantic features\nfrom images using a pre-trained network.\nThese features\ncan be used \u201cdownstream\u201d in other vision tasks with great\nsuccess [2]. Figure 3 shows a two-dimensional embedding\nof all the ImageNet validation images, colored by a coarse\ncategory that they come from. The nice separation testi\ufb01es\nto a successful semantic embedding.\nIntriguingly, this learned feature is useful for a lot more\nthan object categories. For example, Karayev et al. have\nshown promising results \ufb01nding images of di\ufb00erent styles\nsuch as \u201cVintage\u201d and \u201cRomantic\u201d using Ca\ufb00e features (Fig-\nure 4) [6].\nEthereal\nHDR\nMelancholy\nMinimal\nFigure 4:\nTop three most-con\ufb01dent positive pre-\ndictions on the Flickr Style dataset, using a Ca\ufb00e-\ntrained classi\ufb01er.\nObject Detection Most notably, Ca\ufb00e has enabled us\nto obtain by far the best performance on object detection,\nevaluated on the hardest academic datasets: the PASCAL\nVOC 2007-2012 and the ImageNet 2013 Detection challenge\n[3].\nGirshick et al. [3] have combined Ca\ufb00e together with tech-\nniques such as Selective Search [10] to e\ufb00ectively perform\nsimultaneous localization and recognition in natural images.\nFigure 5 shows a sketch of their approach.\nBeginners\u2019 Guides To help users get started with in-\nstalling, using, and modifying Ca\ufb00e, we have provided in-\nstructions and tutorials on the Ca\ufb00e webpage. The tuto-\nrials range from small demos (MNIST digit recognition) to\nserious deployments (end-to-end learning on ImageNet).\nAlthough these tutorials serve as e\ufb00ective documentation\nof the functionality of Ca\ufb00e, the Ca\ufb00e source code addition-\nally provides detailed inline documentation on all modules.\n1. Input \nimage\n2. Extract region \nproposals (~2k)\n3. Compute \nCNN features\naeroplane? no.\n...\nperson? yes.\ntvmonitor? no.\n4. Classify \nregions\nwarped region\n...\nCNN\nR-CNN: Regions with CNN features\nFigure 5: The R-CNN pipeline that uses Ca\ufb00e for\nobject detection.\nThis documentation will be exposed in a standalone web\ninterface in the near future.\n5.\nAVAILABILITY\nSource code is published BSD-licensed on GitHub.5 Project\ndetails, step-wise tutorials, and pre-trained models are on\nthe homepage.6 Development is done in Linux and OS X,\nand users have reported Windows builds.\nA public Ca\ufb00e\nAmazon EC2 instance is coming soon.\n6.\nACKNOWLEDGEMENTS\nWe would like to thank NVIDIA for GPU donation, the\nBVLC sponsors (http://bvlc.eecs.berkeley.edu/), and\nour open source community.\n7.\nREFERENCES\n[1] R. Collobert, K. Kavukcuoglu, and C. Farabet. Torch7: A\nMATLAB-like environment for machine learning. In\nBigLearn, NIPS Workshop, 2011.\n[2] J. Donahue, Y. Jia, O. Vinyals, J. Ho\ufb00man, N. Zhang,\nE. Tzeng, and T. Darrell. Decaf: A deep convolutional\nactivation feature for generic visual recognition. In ICML,\n2014.\n[3] R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich\nfeature hierarchies for accurate object detection and\nsemantic segmentation. In CVPR, 2014.\n[4] I. Goodfellow, D. Warde-Farley, P. Lamblin, V. Dumoulin,\nM. Mirza, R. Pascanu, J. Bergstra, F. Bastien, and\nY. Bengio. Pylearn2: a machine learning research library.\narXiv preprint 1308.4214, 2013.\n[5] S. Guadarrama, E. Rodner, K. Saenko, N. Zhang,\nR. Farrell, J. Donahue, and T. Darrell. Open-vocabulary\nobject retrieval. In RSS, 2014.\n[6] S. Karayev, M. Trentacoste, H. Han, A. Agarwala,\nT. Darrell, A. Hertzmann, and H. Winnemoeller.\nRecognizing image style. arXiv preprint 1311.3715, 2013.\n[7] A. Krizhevsky. cuda-convnet.\nhttps://code.google.com/p/cuda-convnet/, 2012.\n[8] A. Krizhevsky, I. Sutskever, and G. Hinton. ImageNet\nclassi\ufb01cation with deep convolutional neural networks. In\nNIPS, 2012.\n[9] P. Sermanet, D. Eigen, X. Zhang, M. Mathieu, R. Fergus,\nand Y. LeCun. Overfeat: Integrated recognition,\nlocalization and detection using convolutional networks. In\nICLR, 2014.\n[10] J. Uijlings, K. van de Sande, T. Gevers, and A. Smeulders.\nSelective search for object recognition. IJCV, 2013.\n[11] N. Zhang, M. Paluri, M. Ranzato, T. Darrell, and\nL. Bourdev. Panda: Pose aligned networks for deep\nattribute modeling. In CVPR, 2014.\n5https://github.com/BVLC/caffe/\n6http://caffe.berkeleyvision.org/\n",
        "sentence": " Our network is implemented in Caffe [13], and learned from scratch.",
        "context": "our list is incomplete, we have included the toolkits that are\nmost notable to the best of our knowledge. Ca\ufb00e di\ufb00ers from\nother contemporary CNN frameworks in two major ways:\n(1) The implementation is completely C++ based, which\n3.4\nTraining A Network\nCa\ufb00e trains models by the fast and standard stochastic\ngradient descent algorithm.\nFigure 1 shows a typical ex-\nample of a Ca\ufb00e network (for MNIST digit classi\ufb01cation)\nduring training: a data layer fetches the images and labels\nWe thank all of our contributors for their work!\n2.\nHIGHLIGHTS OF CAFFE\nCa\ufb00e provides a complete toolkit for training, testing,\n\ufb01netuning, and deploying models, with well-documented ex-\namples for all of these tasks. As such, it\u2019s an ideal starting"
    },
    {
        "title": "A stereo machine for video-rate dense depth mapping and its new applications",
        "author": [
            "T. Kanade",
            "A. Yoshida",
            "K. Oda",
            "H. Kano",
            "M. Tanaka"
        ],
        "venue": null,
        "citeRegEx": "14",
        "shortCiteRegEx": "14",
        "year": 1996,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " For disparity map computation a commonly used measure is the sum of absolute differences (SAD) [14].",
        "context": null
    },
    {
        "title": "Direct visibility of point sets",
        "author": [
            "S. Katz",
            "A. Tal",
            "R. Basri"
        ],
        "venue": null,
        "citeRegEx": "15",
        "shortCiteRegEx": "15",
        "year": 2007,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " The ground truth 3D point cloud is processed using the visibility check of [15].",
        "context": null
    },
    {
        "title": "Distinctive image features from scale-invariant keypoints",
        "author": [
            "D. Lowe"
        ],
        "venue": "IJCV, 60(2):91\u2013110",
        "citeRegEx": "16",
        "shortCiteRegEx": null,
        "year": 2004,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " That work also showed that CNN-based similarities outperform both classical descriptor spaces like SIFT [16] and other learned descriptors such as [21]. The learned output is a 128-dimensional descriptor vector which corresponds to the size of the SIFT descriptor vector [16] so that LIFT can serve as a drop-in replacement for SIFT or similar handcoded descriptors in existing matching pipelines. For the other compared descriptors, SIFT [16] and LIFT [20, 23], we compute similarity via the pairwise (Euclidean) descriptor distances from the warped images, and feed them to the same decision mechanism to select the best per-pixel depth estimate.",
        "context": null
    },
    {
        "title": "A taxonomy and evaluation of dense two-frame stereo correspondence algorithms",
        "author": [
            "D. Scharstein",
            "R. Szeliski"
        ],
        "venue": "International Journal of Computer Vision, 47(1-3):7\u201342",
        "citeRegEx": "17",
        "shortCiteRegEx": null,
        "year": 2002,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " A discussion of the complete twoview stereo literature is beyond the scope of this paper, for an overview and further reading please refer to benchmarks like the Middlebury Stereo page [17] or KITTI [8].",
        "context": null
    },
    {
        "title": "A multi-view stereo benchmark with high-resolution images and multi-camera videos",
        "author": [
            "T. Sch\u00f6ps",
            "J. Schnberger",
            "S. Galliani",
            "T. Sattler",
            "K. Schindler",
            "M. Pollefeys",
            "A. Geiger"
        ],
        "venue": null,
        "citeRegEx": "18",
        "shortCiteRegEx": "18",
        "year": 2017,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " A host of rather successful multi-view stereo methods exist (see benchmark results such as [12, 18, 19, 22]).",
        "context": null
    },
    {
        "title": "A comparison and evaluation of multi-view stereo reconstruction algorithms",
        "author": [
            "S.M. Seitz",
            "B. Curless",
            "J. Diebel",
            "D. Scharstein",
            "R. Szeliski"
        ],
        "venue": null,
        "citeRegEx": "19",
        "shortCiteRegEx": "19",
        "year": 2006,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " A host of rather successful multi-view stereo methods exist (see benchmark results such as [12, 18, 19, 22]). Again, we refer the reader to benchmark datasets such as [19] and [12] for an overview of contemporary multi-view stereo algorithms.",
        "context": null
    },
    {
        "title": "Discriminative learning of deep convolutional feature point descriptors",
        "author": [
            "E. Simo-Serra",
            "E. Trulls",
            "L. Ferraz",
            "I. Kokkinos",
            "P. Fua",
            "F. Moreno-Noguer"
        ],
        "venue": null,
        "citeRegEx": "20",
        "shortCiteRegEx": "20",
        "year": 2015,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " The recently proposed LIFT descriptor [20, 23] is learned with the help of a Siamese network, using a loss function that ensures that descriptors of matching patches end up having low Euclidean distance, whereas non-matching descriptors have not. For the other compared descriptors, SIFT [16] and LIFT [20, 23], we compute similarity via the pairwise (Euclidean) descriptor distances from the warped images, and feed them to the same decision mechanism to select the best per-pixel depth estimate.",
        "context": null
    },
    {
        "title": "Learning local feature descriptors using convex optimisation",
        "author": [
            "K. Simonyan",
            "A. Vedaldi",
            "A. Zisserman"
        ],
        "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,",
        "citeRegEx": "21",
        "shortCiteRegEx": "21",
        "year": 2014,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " That work also showed that CNN-based similarities outperform both classical descriptor spaces like SIFT [16] and other learned descriptors such as [21].",
        "context": null
    },
    {
        "title": "On benchmarking camera calibration and multi-view stereo for high resolution imagery",
        "author": [
            "C. Strecha",
            "W.V. Hansen",
            "L.J.V. Gool",
            "P. Fua",
            "U. Thoennessen"
        ],
        "venue": null,
        "citeRegEx": "22",
        "shortCiteRegEx": "22",
        "year": 2008,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " A host of rather successful multi-view stereo methods exist (see benchmark results such as [12, 18, 19, 22]). Therefore, we apply the learned multi-view similarity measure also to the well-known Fountain dataset [22], without retraining it.",
        "context": null
    },
    {
        "title": "LIFT: learned invariant feature transform",
        "author": [
            "K.M. Yi",
            "E. Trulls",
            "V. Lepetit",
            "P. Fua"
        ],
        "venue": "CoRR, abs/1603.09114",
        "citeRegEx": "23",
        "shortCiteRegEx": null,
        "year": 2016,
        "abstract": "We introduce a novel Deep Network architecture that implements the full\nfeature point handling pipeline, that is, detection, orientation estimation,\nand feature description. While previous works have successfully tackled each\none of these problems individually, we show how to learn to do all three in a\nunified manner while preserving end-to-end differentiability. We then\ndemonstrate that our Deep pipeline outperforms state-of-the-art methods on a\nnumber of benchmark datasets, without the need of retraining.",
        "full_text": "LIFT: Learned Invariant Feature Transform\nKwang Moo Yi\u2217,1, Eduard Trulls\u2217,1, Vincent Lepetit2, Pascal Fua1\n1Computer Vision Laboratory, Ecole Polytechnique F\u00b4ed\u00b4erale de Lausanne (EPFL)\n2Institute for Computer Graphics and Vision, Graz University of Technology\n{kwang.yi, eduard.trulls, pascal.fua}@epfl.ch, lepetit@icg.tugraz.at\nAbstract. We introduce a novel Deep Network architecture that imple-\nments the full feature point handling pipeline, that is, detection, orienta-\ntion estimation, and feature description. While previous works have suc-\ncessfully tackled each one of these problems individually, we show how to\nlearn to do all three in a uni\ufb01ed manner while preserving end-to-end dif-\nferentiability. We then demonstrate that our Deep pipeline outperforms\nstate-of-the-art methods on a number of benchmark datasets, without\nthe need of retraining.\nKeywords: Local Features, Feature Descriptors, Deep Learning\n1\nIntroduction\nLocal features play a key role in many Computer Vision applications. Find-\ning and matching them across images has been the subject of vast amounts of\nresearch. Until recently, the best techniques relied on carefully hand-crafted fea-\ntures [1,2,3,4,5]. Over the past few years, as in many areas of Computer Vision,\nmethods based in Machine Learning, and more speci\ufb01cally Deep Learning, have\nstarted to outperform these traditional methods [6,7,8,9,10].\nThese new algorithms, however, address only a single step in the complete\nprocessing chain, which includes detecting the features, computing their orienta-\ntion, and extracting robust representations that allow us to match them across\nimages. In this paper we introduce a novel Deep architecture that performs all\nthree steps together. We demonstrate that it achieves better overall performance\nthan the state-of-the-art methods, in large part because it allows these individual\nsteps to be optimized to perform well in conjunction with each other.\nOur architecture, which we refer to as LIFT for Learned Invariant Feature\nTransform, is depicted by Fig. 1. It consists of three components that feed into\neach other: the Detector, the Orientation Estimator, and the Descriptor. Each\none is based on Convolutional Neural Networks (CNNs), and patterned after\nrecent ones [6,9,10] that have been shown to perform these individual functions\nwell. To mesh them together we use Spatial Transformers [11] to rectify the\n\u2217First two authors contributed equally.\nThis work was supported in part by the EU FP7 project MAGELLAN under grant\nnumber ICT-FP7-611526.\narXiv:1603.09114v2  [cs.CV]  29 Jul 2016\n2\nK. M. Yi, E. Trulls, V. Lepetit, P. Fua\nDET\nCrop\nORI\nRot\nDESC\nLIFT pipeline\nSCORE MAP\nsoftargmax\ndescription \nvector\nFig. 1. Our integrated feature extraction pipeline. Our pipeline consists of three major\ncomponents: the Detector, the Orientation Estimator, and the Descriptor. They are\ntied together with di\ufb00erentiable operations to preserve end-to-end di\ufb00erentiability.1\nimage patches given the output of the Detector and the Orientation Estimator.\nWe also replace the traditional approaches to non-local maximum suppression\n(NMS) by the soft argmax function [12]. This allows us to preserve end-to-end\ndi\ufb00erentiability, and results in a full network that can still be trained with back-\npropagation, which is not the case of any other architecture we know of.\nAlso, we show how to learn such a pipeline in an e\ufb00ective manner. To this\nend, we build a Siamese network and train it using the feature points produced\nby a Structure-from-Motion (SfM) algorithm that we ran on images of a scene\ncaptured under di\ufb00erent viewpoints and lighting conditions, to learn its weights.\nWe formulate this training problem on image patches extracted at di\ufb00erent scales\nto make the optimization tractable. In practice, we found it impossible to train\nthe full architecture from scratch, because the individual components try to op-\ntimize for di\ufb00erent objectives. Instead, we introduce a problem-speci\ufb01c learning\napproach to overcome this problem. It involves training the Descriptor \ufb01rst,\nwhich is then used to train the Orientation Estimator, and \ufb01nally the Detector,\nbased on the already learned Descriptor and Orientation Estimator, di\ufb00erenti-\nating through the entire network. At test time, we decouple the Detector, which\nruns over the whole image in scale space, from the Orientation Estimator and\nDescriptor, which process only the keypoints.\nIn the next section we brie\ufb02y discuss earlier approaches. We then present our\napproach in detail and show that it outperforms many state-of-the-art methods.\n2\nRelated work\nThe amount of literature relating to local features is immense, but it always\nrevolves about \ufb01nding feature points, computing their orientation, and matching\nthem. In this section, we will therefore discuss these three elements separately.\n2.1\nFeature Point Detectors\nResearch on feature point detection has focused mostly on \ufb01nding distinctive lo-\ncations whose scale and rotation can be reliably estimated. Early works [13,14]\n1 Figures are best viewed in color.\nLIFT: Learned Invariant Feature Transform\n3\nused \ufb01rst-order approximations of the image signal to \ufb01nd corner points in im-\nages. FAST [15] used Machine Learning techniques but only to speed up the\nprocess of \ufb01nding corners. Other than corner points, SIFT [1] detect blobs in\nscale-space; SURF [2] use Haar \ufb01lters to speed up the process; Maximally Sta-\nble Extremal Regions (MSER) [16] detect regions; [17] detect a\ufb03ne regions.\nSFOP [18] use junctions and blobs, and Edge Foci [19] use edges for robustness\nto illumination changes. More recently, feature points based on more sophisti-\ncated and carefully designed \ufb01lter responses [5,20] have also been proposed to\nfurther enhance the performance of feature point detectors.\nIn contrast to these approaches that focus on better engineering, and follow-\ning the early attempts in learning detectors [21,22], [6] showed that a detector\ncould be learned to deliver signi\ufb01cantly better performance than the state-of-\nthe-art. In this work, piecewise-linear convolutional \ufb01lters are learned to robustly\ndetect feature points in spite of lighting and seasonal changes. Unfortunately, this\nwas done only for a single scale and from a dataset without viewpoint changes.\nWe therefore took our inspiration from it but had to extend it substantially to\nincorporate it into our pipeline.\n2.2\nOrientation Estimation\nDespite the fact that it plays a critical role in matching feature points, the\nproblem of estimating a discriminative orientation has received noticeably less\nattention than detection or feature description. As a result, the method intro-\nduced by SIFT [1] remains the de facto standard up to small improvements, such\nas the fact that it can be sped-up by using the intensity centroid, as in ORB [4].\nA departure from this can be found in a recent paper [9] that introduced a\nDeep Learning-based approach to predicting stable orientations. This resulted\nin signi\ufb01cant gains over the state-of-the-art. We incorporate this architecture\ninto our pipeline and show how to train it using our problem-speci\ufb01c training\nstrategy, given our learned descriptors.\n2.3\nFeature Descriptors\nFeature descriptors are designed to provide discriminative representations of\nsalient image patches, while being robust to transformations such as viewpoint\nor illumination changes. The \ufb01eld reached maturity with the introduction of\nSIFT [1], which is computed from local histograms of gradient orientations, and\nSURF [2], which uses integral image representations to speed up the computa-\ntion. Along similar lines, DAISY [3] relies on convolved maps of oriented gra-\ndients to approximate the histograms, which yields large computational gains\nwhen extracting dense descriptors.\nEven though they have been extremely successful, these hand-crafted de-\nscriptors can now be outperformed by newer ones that have been learned. These\nrange from unsupervised hashing to supervised learning techniques based on\nlinear discriminant analysis [23,24], genetic algorithm [25], and convex optimiza-\ntion [26]. An even more recent trend is to extract features directly from raw image\n4\nK. M. Yi, E. Trulls, V. Lepetit, P. Fua\nDET\nCrop\nDET\nDET\nW\nDET\nW\nW\nORI\nW\nORI\nORI\nW\nDESC\nDESC\nDESC\nW\nW\nCrop\nCrop\nRot\nRot\nRot\nsoftargmax\nsoftargmax\nsoftargmax\nP1\nd1\nd2\nd3\np3\n\u2713\np2\n\u2713\np1\n\u2713\np1\np2\np3\nP3\nP2\nP4\nS3\nS2\nS1\nx1\nx2\nx3\n\u27133\n\u27132\n\u27131\nLEARNING\nFig. 2. Our Siamese training architecture with four branches, which takes as input\na quadruplet of patches: Patches P1 and P2 (blue) correspond to di\ufb00erent views of\nthe same physical point, and are used as positive examples to train the Descriptor;\nP3 (green) shows a di\ufb00erent 3D point, which serves as a negative example for the\nDescriptor; and P4 (red) contains no distinctive feature points and is only used as a\nnegative example to train the Detector. Given a patch P, the Detector, the softargmax,\nand the Spatial Transformer layer Crop provide all together a smaller patch p inside P.\np is then fed to the Orientation Estimator, which along with the Spatial Transformer\nlayer Rot, provides the rotated patch p\u03b8 that is processed by the Descriptor to obtain\nthe \ufb01nal description vector d.\npatches with CNNs trained on large volumes of data. For example, MatchNet [7]\ntrained a Siamese CNN for feature representation, followed by a fully-connected\nnetwork to learn the comparison metric. DeepCompare [8] showed that a net-\nwork that focuses on the center of the image can increase performance. The\napproach of [27] relied on a similar architecture to obtain state-of-the-art results\nfor narrow-baseline stereo. In [10], hard negative mining was used to learn com-\npact descriptors that use on the Euclidean distance to measure similarity. The\nalgorithm of [28] relied on sample triplets to mine hard negatives.\nIn this work, we rely on the architecture of [10] because the corresponding\ndescriptors are trained and compared with the Euclidean distance, which has a\nwider range of applicability than descriptors that require a learned metric.\n3\nMethod\nIn this section, we \ufb01rst formulate the entire feature detection and description\npipeline in terms of the Siamese architecture depicted by Fig. 2. Next, we discuss\nthe type of data we need to train our networks and how to collect it. We then\ndescribe the training procedure in detail.\n3.1\nProblem formulation\nWe use image patches as input, rather than full images. This makes the learning\nscalable without loss of information, as most image regions do not contain key-\npoints. The patches are extracted from the keypoints used by a SfM pipeline, as\nwill be discussed in Section 3.2. We take them to be small enough that we can\nLIFT: Learned Invariant Feature Transform\n5\nassume they contain only one dominant local feature at the given scale, which\nreduces the learning process to \ufb01nding the most distinctive point in the patch.\nTo train our network we create the four-branch Siamese architecture pictured\nin Fig. 2. Each branch contains three distinct CNNs, a Detector, an Orientation\nEstimator, and a Descriptor. For training purposes, we use quadruplets of image\npatches. Each one includes two image patches P1 and P2, that correspond to\ndi\ufb00erent views of the same 3D point, one image patch P3, that contains the pro-\njection of a di\ufb00erent 3D point, and one image patch P4 that does not contain any\ndistinctive feature point. During training, the i-th patch Pi of each quadruplet\nwill go through the i-th branch.\nTo achieve end-to-end di\ufb00erentiability, the components of each branch are\nconnected as follows:\n1. Given an input image patch P, the Detector provides a score map S.\n2. We perform a soft argmax [12] on the score map S and return the location\nx of a single potential feature point.\n3. We extract a smaller patch p centered on x with the Spatial Transformer\nlayer Crop (Fig. 2). This serves as the input to the Orientation Estimator.\n4. The Orientation Estimator predicts a patch orientation \u03b8.\n5. We rotate p according to this orientation using a second Spatial Transformer\nlayer, labeled as Rot in Fig. 2, to produce p\u03b8.\n6. p\u03b8 is fed to the Descriptor network, which computes a feature vector d.\nNote that the Spatial Transformer layers are used only to manipulate the\nimage patches while preserving di\ufb00erentiability. They are not learned modules.\nAlso, both the location x proposed by the Detector and the orientation \u03b8 for\nthe patch proposal are treated implicitly, meaning that we let the entire network\ndiscover distinctive locations and stable orientations while learning.\nSince our network consists of components with di\ufb00erent purposes, learning\nthe weights is non-trivial. Our early attempts at training the network as a whole\nfrom scratch were unsuccessful. We therefore designed a problem-speci\ufb01c learn-\ning approach that involves learning \ufb01rst the Descriptor, then the Orientation\nEstimator given the learned descriptor, and \ufb01nally the Detector, conditioned on\nthe other two. This allows us to tune the Orientation Estimator for the Descrip-\ntor, and the Detector for the other two components.\nWe will elaborate on this learning strategy in Secs. 3.3 (Descriptor), 3.4 (Ori-\nentation Estimator), and 3.5 (Detector), that is, in the order they are learned.\n3.2\nCreating the Training Dataset\nThere are datasets that can be used to train feature descriptors [24] and orien-\ntation estimators [9]. However it is not so clear how to train a keypoint detec-\ntor, and the vast majority of techniques still rely on hand-crafted features. The\nTILDE detector [6] is an exception, but the training dataset does not exhibit\nany viewpoint changes.\nTo achieve invariance we need images that capture views of the same scene\nunder di\ufb00erent illumination conditions and seen from di\ufb00erent perspectives. We\n6\nK. M. Yi, E. Trulls, V. Lepetit, P. Fua\nFig. 3. Sample images and patches from Piccadilly (left) and Roman-Forum (right).\nKeypoints that survive the SfM pipeline are drawn in blue, and the rest in red.\nthus turned to photo-tourism image sets. We used the collections from Piccadilly\nCircus in London and the Roman Forum in Rome from [29] to reconstruct the\n3D using VisualSFM [30], which relies of SIFT features. Piccadilly contains 3384\nimages, and the reconstruction has 59k unique points with an average of 6.5 ob-\nservations for each. Roman-Forum contains 1658 images and 51k unique points,\nwith an average of 5.2 observations for each. Fig. 3 shows some examples.\nWe split the data into training and validation sets, discarding views of train-\ning points on the validation set and vice-versa. To build the positive training\nsamples we consider only the feature points that survive the SfM reconstruction\nprocess. To extract patches that do not contain any distinctive feature point,\nas required by our training method, we randomly sample image regions that\ncontain no SIFT features, including those that were not used by SfM.\nWe extract grayscale training patches according to the scale \u03c3 of the point,\nfor both feature and non-feature point image regions. Patches P are extracted\nfrom a 24\u03c3 \u00d7 24\u03c3 support region at these locations, and standardized into S \u00d7 S\npixels where S = 128. The smaller patches p and p\u03b8 that serve as input to the\nOrientation Estimator and the Descriptor, are cropped and rotated versions of\nthese patches, each having size s\u00d7s, where s = 64. The smaller patches e\ufb00ectively\ncorrespond to the SIFT descriptor support region size of 12\u03c3. To avoid biasing\nthe data, we apply uniform random perturbations to the patch location with a\nrange of 20% (4.8\u03c3). Finally, we normalize the patches with the grayscale mean\nand standard deviation of the entire training set.\n3.3\nDescriptor\nLearning feature descriptors from raw image patches has been extensively re-\nsearched during the past year [7,8,10,27,28,31], with multiple works reporting\nimpressive results on patch retrieval, narrow baseline stereo, and matching non-\nrigid deformations. Here we rely on the relatively simple networks of [10], with\nthree convolutional layers followed by hyperbolic tangent units, l2 pooling [32]\nand local subtractive normalization, as they do not require learning a metric.\nThe Descriptor can be formalized simply as\nd = h\u03c1(p\u03b8) ,\n(1)\nLIFT: Learned Invariant Feature Transform\n7\nwhere h(.) denotes the Descriptor CNN, \u03c1 its parameters, and p\u03b8 is the rotated\npatch from the Orientation Estimator. When training the Descriptor, we do not\nyet have the Detector and the Orientation Estimator trained. We therefore use\nthe image locations and orientations of the feature points used by the SfM to\ngenerate image patches p\u03b8.\nWe train the Descriptor by minimizing the sum of the loss for pairs of cor-\nresponding patches\n\u0000p1\n\u03b8, p2\n\u03b8\n\u0001\nand the loss for pairs of non-corresponding patches\n\u0000p1\n\u03b8, p3\n\u03b8\n\u0001\n. The loss for pair\n\u0000pk\n\u03b8, pl\n\u03b8\n\u0001\nis de\ufb01ned as the hinge embedding loss of the\nEuclidean distance between their description vectors. We write\nLdesc(pk\n\u03b8, pl\n\u03b8) =\n(\r\rh\u03c1\n\u0000pk\n\u03b8\n\u0001\n\u2212h\u03c1\n\u0000pl\n\u03b8\n\u0001\r\r\n2\nfor positive pairs, and\nmax\n\u00000, C \u2212\n\r\rh\u03c1\n\u0000pk\n\u03b8\n\u0001\n\u2212h\u03c1\n\u0000pl\n\u03b8\n\u0001\r\r\n2\n\u0001\nfor negative pairs ,\n(2)\nwhere positive and negative samples are pairs of patches that do or do not\ncorrespond to the same physical 3D points, \u2225\u00b7\u22252 is the Euclidean distance, and\nC = 4 is the margin for embedding.\nWe use hard mining during training, which was shown in [10] to be critical for\ndescriptor performance. Following this methodology, we forward Kf sample pairs\nand use only the Kb pairs with the highest training loss for back-propagation,\nwhere r = Kf/Kb \u22651 is the \u2018mining ratio\u2019. In [10] the network was pre-trained\nwithout mining and then \ufb01ne-tuned with r = 8. Here, we use an increasing\nmining scheme where we start with r = 1 and double the mining ratio every\n5000 batches. We use balanced batches with 128 positive pairs and 128 negative\npairs, mining each separately.\n3.4\nOrientation Estimator\nOur Orientation Estimator is inspired by that of [9]. However, this speci\ufb01c one\nrequires pre-computations of description vectors for multiple orientations to com-\npute numerically the Jacobian of the method parameters with respect to orien-\ntations. This is a critical limitation for us because we treat the output of the\ndetector component implicitly throughout the pipeline and it is thus not possible\nto pre-compute the description vectors.\nWe therefore propose to use Spatial Transformers [11] instead to learn the\norientations. Given a patch p from the region proposed by the detector, the\nOrientation Estimator predicts an orientation\n\u03b8 = g\u03c6(p) ,\n(3)\nwhere g denotes the Orientation Estimator CNN, and \u03c6 its parameters.\nTogether with the location x from the Detector and P the original image\npatch, \u03b8 is then used by the second Spatial Transformer Layer Rot(.) to provide\na patch p\u03b8 = Rot (P, x, \u03b8), which is the rotated version of patch p.\nWe train the Orientation Estimator to provide the orientations that minimize\nthe distances between description vectors for di\ufb00erent views of the same 3D\npoints. We use the already trained Descriptor to compute the description vectors,\n8\nK. M. Yi, E. Trulls, V. Lepetit, P. Fua\nand as the Detector is still not trained, we use the image locations from SfM.\nMore formally, we minimize the loss for pairs of corresponding patches, de\ufb01ned\nas the Euclidean distance between their description vectors\nLorientation(P1, x1, P2, x2) =\n\r\rh\u03c1(G(P1, x1)) \u2212h\u03c1(G(P2, x2))\n\r\r\n2 ,\n(4)\nwhere G(P, x) is the patch centered on x after orientation correction: G(P, x) =\nRot(P, x, g\u03c6(Crop(P, x))). This complex notation is necessary to properly han-\ndle the cropping of the image patches. Recall that pairs (P1, P2) comprise image\npatches containing the projections of the same 3D point, and locations x1 and\nx2 denote the reprojections of these 3D points. As in [9], we do not use pairs\nthat correspond to di\ufb00erent physical points whose orientations are not related.\n3.5\nDetector\nThe Detector takes an image patch as input, and returns a score map. We imple-\nment it as a convolution layer followed by piecewise linear activation functions,\nas in TILDE [6]. More precisely, the score map S for patch P is computed as:\nS = f\u00b5(P) =\nN\nX\nn\n\u03b4n\nM\nmax\nm (Wmn \u2217P + bmn) ,\n(5)\nwhere f\u00b5(P) denotes the Detector itself with parameters \u00b5, \u03b4n is +1 if n is odd\nand \u22121 otherwise, \u00b5 is made of the \ufb01lters Wmn and biases bmn of the convolution\nlayer to learn, \u2217denotes the convolution operation, and N and M are hyper-\nparameters controlling the complexity of the piecewise linear activation function.\nThe main di\ufb00erence with TILDE lies in the way we train this layer. To let\nS have maxima in places other than a \ufb01xed location retrieved by SfM, we treat\nthis location implicitly, as a latent variable. Our method can potentially discover\npoints that are more reliable and easier to learn, whereas [6] cannot. Incidentally,\nin our early experiments, we noticed that it was harmful to force the Detector\nto optimize directly for SfM locations.\nFrom the score map S, we obtain the location x of a feature point as\nx = softargmax (S) ,\n(6)\nwhere softargmax is a function which computes the Center of Mass with the\nweights being the output of a standard softmax function [12]. We write\nsoftargmax (S) =\nP\ny exp(\u03b2S(y))y\nP\ny exp(\u03b2S(y)) ,\n(7)\nwhere y are locations in S, and \u03b2 = 10 is a hyper-parameter controlling the\nsmoothness of the softargmax. This softargmax function acts as a di\ufb00erentiable\nversion of non-maximum suppression. x is given to the \ufb01rst Spatial Trans-\nformer Layer Crop(.) together with the patch P to extract a smaller patch\np = Crop (P, x) used as input to the Orientation Estimator.\nLIFT: Learned Invariant Feature Transform\n9\nAs the Orientation Estimator and the Descriptor have been learned by this\npoint, we can train the Detector given the full pipeline. To optimize over the\nparameters \u00b5, we minimize the distances between description vectors for the\npairs of patches that correspond to the same physical points, while maximizing\nthe classi\ufb01cation score for patches not corresponding to the same physical points.\nMore exactly, given training quadruplets\n\u0000P1, P2, P3, P4\u0001\n, where P1 and P2\ncorrespond to the same physical point, P1 and P3 correspond to di\ufb00erent SfM\npoints, and P4 to a non-feature point location, we minimize the sum of their\nloss functions\nLdetector(P1, P2, P3, P4) = \u03b3Lclass(P1, P2, P3, P4) + Lpair(P1, P2) ,\n(8)\nwhere \u03b3 is a hyper-parameter balancing the two terms in this summation\nLclass(P1, P2, P3, P4) =\n4\nX\ni=1\n\u03b1i max\n\u00000,\n\u00001 \u2212softmax\n\u0000f\u00b5\n\u0000Pi\u0001\u0001\nyi\n\u0001\u00012 ,\n(9)\nwith yi = \u22121 and \u03b1i = 3/6 if i = 4, and yi = +1 and \u03b1i = 1/6 otherwise to\nbalance the positives and negatives. softmax is the log-mean-exponential softmax\nfunction. We write\nLpair(P1, P2) = \u2225h\u03c1(G(P1, softargmax(f\u00b5(P1)))) \u2212\nh\u03c1(G(P2, softargmax(f\u00b5(P2))))\n\u22252 .\n(10)\nNote that the locations of the detected feature points x appear only implicitly\nand are discovered during training. Furthermore, all three components are tied\nin with the Detector learning. As with the Descriptor we use a hard mining\nstrategy, in this case with a \ufb01xed mining ratio of r = 4.\nIn practice, as the Descriptor already learns some invariance, it can be hard\nfor the Detector to \ufb01nd new points to learn implicitly. To let the Detector start\nwith an idea of the regions it should \ufb01nd, we \ufb01rst constrain the patch proposals\np = Crop(P, softargmax(f\u00b5(P))) that correspond to the same physical points\nto overlap. We then continue training the Detector without this constraint.\nSpeci\ufb01cally, when pre-training the Detector, we replace Lpair in Eq. (8) with\n\u02dcLpair, where \u02dcLpair is equal to 0 when the patch proposals overlap exactly, and\nincreases with the distance between them otherwise. We therefore write\n\u02dcLpair(P1, P2) = 1 \u2212p1 \u2229p2\np1 \u222ap2 + max\n\u00000,\n\r\rx1 \u2212x2\r\r\n1 \u22122s\n\u0001\np\np1 \u222ap2\n,\n(11)\nwhere xj = softargmax(f\u00b5(Pj)), pj = Crop(Pj, xj), \u2225\u00b7\u22251 is the l1 norm. Recall\nthat s = 64 pixels is the width and height of the patch proposals.\n3.6\nRuntime pipeline\nThe pipeline used at run-time is shown in Fig. 4. As our method is trained on\npatches, simply applying it over the image would require the network to be tested\n10\nK. M. Yi, E. Trulls, V. Lepetit, P. Fua\nSCALE-SPACE IMAGE\nSCORE PYRAMID\nDET\nNMS\nKEYPOINTS\nCrop\nRot\nORI\n\u2026\n\u2026\nDESC\nd1\nd2\n\u2026\nScale-space Detection\ndN\nFig. 4. An overview of our runtime architecture. As the Orientation Estimator and the\nDescriptor only require evaluation at local maxima, we decouple the Detector and run it\nin scale space with traditional NMS to obtain proposals for the two other components.\nwith a sliding window scheme over the whole image. In practice, this would be\ntoo expensive. Fortunately, as the Orientation Estimator and the Descriptor only\nneed to be run at local maxima, we can simply decouple the detector from the\nrest to apply it to the full image, and replace the softargmax function by NMS,\nas outlined in red in Fig. 4. We then apply the Orientation Estimator and the\nDescriptor only to the patches centered on local maxima.\nMore exactly, we apply the Detector independently to the image at di\ufb00erent\nresolutions to obtain score maps in scale space. We then apply a traditional NMS\nscheme similar to that of [1] to detect feature point locations.\n4\nExperimental validation\nIn this section, we \ufb01rst present the datasets and metrics we used. We then present\nqualitative results, followed by a thorough quantitative comparison against a\nnumber of state-of-the-art baselines, which we consistently outperform.\nFinally, to better understand what elements of our approach most contribute\nto this result, we study the importance of the pre-training of the Detector com-\nponent, discussed in Section 3.5, and analyze the performance gains attributable\nto each component.\n4.1\nDataset and Experimental Setup\nWe evaluate our pipeline on three standard datasets:\n\u2013 The Strecha dataset [33], which contains 19 images of two scenes seen from\nincreasingly di\ufb00erent viewpoints.\n\u2013 The DTU dataset [34], which contains 60 sequences of objects with di\ufb00erent\nviewpoints and illumination settings. We use this dataset to evaluate our\nmethod under viewpoint changes.\n\u2013 The Webcam dataset [6], which contains 710 images of 6 scenes with strong\nillumination changes but seen from the same viewpoint. We use this dataset\nto evaluate our method under natural illumination changes.\nLIFT: Learned Invariant Feature Transform\n11\nFor Strecha and DTU we use the provided ground truth to establish corre-\nspondences across viewpoints. We use a maximum of 1000 keypoints per image,\nand follow the standard evaluation protocol of [35] on the common viewpoint\nregion. This lets us evaluate the following metrics.\n\u2013 Repeatability (Rep.): Repeatability of feature points, expressed as a ratio.\nThis metric captures the performance of the feature point detector by report-\ning the ratio of keypoints that are found consistently in the shared region.\n\u2013 Nearest Neighbor mean Average Precision (NN mAP): Area Under Curve\n(AUC) of the Precision-Recall curve, using the Nearest Neighbor match-\ning strategy. This metric captures how discriminating the descriptor is by\nevaluating it at multiple descriptor distance thresholds.\n\u2013 Matching Score (M. Score): The ratio of ground truth correspondences that\ncan be recovered by the whole pipeline over the number of features proposed\nby the pipeline in the shared viewpoint region. This metric measures the\noverall performance of the pipeline.\nWe compare our method on the three datasets to the following combination of\nfeature point detectors and descriptors, as reported by the authors of the corre-\nsponding papers: SIFT [1], SURF [2], KAZE [36], ORB [4], Daisy [37] with SIFT\ndetector, sGLOH [38] with Harris-a\ufb03ne detector [39], MROGH [40] with Harris-\na\ufb03ne detector, LIOP [41] with Harris-a\ufb03ne detector, BiCE [42] with Edge Foci\ndetector [19], BRISK [43], FREAK [44] with BRISK detector, VGG [26] with\nSIFT detector, DeepDesc [10] with SIFT detector, PN-Net [28] with SIFT detec-\ntor, and MatchNet [7] with SIFT detector. We also consider SIFT with Hessian-\nA\ufb03ne keypoints [17]. For the learned descriptors VGG, DeepDesc, PN-Net and\nMatchNet we use SIFT keypoints because they are trained using a dataset cre-\nated with Di\ufb00erence-of-Gaussians, which is essentially the same as SIFT. In the\ncase of Daisy, which was not developed for a speci\ufb01c detector, we also use SIFT\nkeypoints. To make our results reproducible, we provide additional implementa-\ntion details for LIFT and the baselines in the supplementary material.2\n4.2\nQualitative Examples\nFig. 5 shows image matching results with 500 feature points, for both SIFT\nand our LIFT pipeline trained with Piccadilly. As expected, LIFT returns more\ncorrect correspondences across the two images. One thing to note is that the two\nDTU scenes in the bottom two rows are completely di\ufb00erent from the photo-\ntourism datasets we used for training. Given that the two datasets are very\ndi\ufb00erent, this shows good generalization properties.\n4.3\nQuantitative Evaluation of the Full Pipeline\nFig. 6 shows the average matching score for all three datasets, and Table 1\nprovides the exact numbers for the two LIFT variants. LIFT (pic) is trained\n2 Source and models will be available at https://github.com/cvlab-epfl/LIFT.\n12\nK. M. Yi, E. Trulls, V. Lepetit, P. Fua\nFig. 5. Qualitative local feature matching examples of left: SIFT and right: our\nmethod LIFT. Correct matches recovered by each method are shown in green lines and\nthe descriptor support regions with red circles. Top row: Herz-Jesu-P8 of Strecha,\nsecond row: Frankfurt of Webcam, third row: Scene 7 of DTU and bottom row:\nScene 19 of DTU. Note that the images are very di\ufb00erent from one another.\nwith Piccadilly and LIFT (rf) with Roman-Forum. Both of our learned models\nsigni\ufb01cantly outperform the state-of-the-art on Strecha and DTU and achieve\nstate-of-the-art on Webcam. Note that KAZE, which is the best performing\ncompetitor on Webcam, performs poorly on the other two datasets. As discussed\nabove, Piccadilly and Roman-Forum are very di\ufb00erent from the datasets used\nfor testing. This underlines the strong generalization capability of our approach,\nwhich is not always in evidence with learning-based methods.\nInterestingly, on DTU, SIFT is still the best performing method among the\ncompetitors, even compared to methods that rely on Deep Learning, such as\nDeepDesc and PN-Net. Also, the gap between SIFT and the learning-based\nVGG, DeepDesc, and PN-Net is not large for the Strecha dataset.\nThese results show that although a component may outperform another\nmethod when evaluated individually, they may fail to deliver their full potential\nwhen integrated into the full pipeline, which is what really matters. In other\nwords, it is important to learn the components together, as we do, and to con-\nsider the whole pipeline when evaluating feature point detectors and descriptors.\n4.4\nPerformance of Individual Components\nFine-tuning the Detector. Recall that we pre-train the detector and then\n\ufb01nalize the training with the Orientation Estimator and the Descriptor, as dis-\nLIFT: Learned Invariant Feature Transform\n13\nTable 1. Average matching score for all baselines.\nSIFT\nSIFT-HesA\ufb00SURF\nORB\nDaisy\nsGLOH MROGH\nLIOP\nBiCE\nStrecha\n.283\n.314\n.208\n.157\n.272\n.207\n.239\n.211\n.270\nDTU\n.272\n.274\n.244\n.127\n.262\n.187\n.223\n.189\n.242\nWebcam\n.128\n.164\n.117\n.120\n.120\n.113\n.125\n.086\n.166\nBRISK\nFREAK\nVGG\nMatchNet DeepDesc PN-Net\nKAZE\nLIFT-pic LIFT-rf\nStrecha\n.208\n.183\n.300\n.223\n.298\n.300\n.250\n.374\n.369\nDTU\n.193\n.186\n.271\n.198\n.257\n.267\n.213\n.317\n.308\nWebcam\n.118\n.116\n.118\n.101\n.116\n.114\n.195\n.196\n.202\n0\n0.1\n0.2\n0.3\n0.4\nAvg. matching score on \u2018Strecha\u2019\nSIFT\nSIFT-HesAf\nSURF\nORB\nDaisy\nsGLOH\nMROGH\nLIOP\nBiCE\nBRISK\nFREAK\nVGG\nDeepDesc\nPN-Net\nMatchNet\nKAZE\nLIFT (pic)\nLIFT (rf)\n0\n0.1\n0.2\n0.3\n0.4\nAvg. matching score on \u2018Strecha\u2019\n0\n0.08\n0.16\n0.24\n0.32\nAvg. matching score on \u2018DTU\u2019\n0\n0.055\n0.11\n0.165\n0.22\nAvg. matching score on \u2018Webcam\u2019\nFig. 6. Average matching score for all baselines.\ncussed in Section 3.5. It is therefore interesting to see the e\ufb00ect of this \ufb01nalizing\nstage. In Table 2 we evaluate the entire pipeline with the pre-trained Detector\nand the \ufb01nal Detector. As the pair-wise loss term \u02dcLpair of Eq. (11) is designed\nto emulate the behavior of an ideal descriptor, the pre-trained Detector already\nperforms well. However, the full training pushes the performance slightly higher.\nA closer look at Table 2 reveals that gains are larger overall for Piccadilly\nthan for Roman-Forum. This is probably due to the fact that Roman-Forum\ndoes not have many non-feature point regions. In fact, the network started to\nover-\ufb01t quickly after a few iterations on this dataset. The same happened when\nwe further tried to \ufb01ne-tune the full pipeline as a whole, suggesting that our\nlearning strategy is already providing a good global solution.\nPerformance of individual components. To understand the in\ufb02uence of\neach component on the overall performance, we exchange them with their SIFT\ncounterparts, for both LIFT (pic) and LIFT (rf), on Strecha. We report the\nresults in Table 3. In short, each time we exchange to SIFT, we decrease per-\nformance, thus showing that each element of the pipeline plays and important\nrole. Our Detector gives higher repeatability for both models. Having better ori-\nentations also helps whichever detector or descriptor is being used, and also the\nDeep Descriptors perform better than SIFT.\nOne thing to note is that our Detector is not only better in terms of repeata-\nbility, but generally better in terms of both the NN mAP, which captures the\n14\nK. M. Yi, E. Trulls, V. Lepetit, P. Fua\nTable 2. Results on Strecha for both LIFT models trained on Piccadilly and Roman-\nForum, with the pre-trained and fully-trained Detector.\nTrained on Piccadilly\nTrained on Roman-Forum\nRep.\nM.Score\nRep.\nM.Score\nPre-trained\n.436\n.367\n.447\n.368\nFully-trained\n.446\n.374\n.447\n.369\nTable 3. Results on Strecha for both LIFT models trained on Piccadilly and Roman-\nForum, interchanging our components with their SIFT counterparts.\nTrained on Piccadilly\nTrained on Roman-Forum\nDet.\nOri.\nDesc.\nRep.\nNN mAP\nM.Score\nRep.\nNN mAP\nM.Score\nSIFT\nSIFT\nSIFT\n.428\n.517\n.282\n.428\n.517\n.282\nSIFT\nOurs\nSIFT\n.671\n.341\n.662\n.338\nSIFT\nSIFT\nOurs\n.568\n.290\n.581\n.295\nSIFT\nOurs\nOurs\n.685\n.344\n.688\n.342\nOurs\nSIFT\nSIFT\n.446\n.540\n.325\n.447\n.545\n.319\nOurs\nOurs\nSIFT\n.644\n.372\n.630\n.360\nOurs\nSIFT\nOurs\n.629\n.339\n.644\n.337\nOurs\nOurs\nOurs\n.446\n.686\n.374\n.447\n.683\n.369\ndescriptor performance, and in terms of matching score, which evaluates the full\npipeline. This shows that our Detector learns to \ufb01nd not only points that can be\nfound often but also points that can be matched easily, indicating that training\nthe pipeline as a whole is important for optimal performance.\n5\nConclusion\nWe have introduced a novel Deep Network architecture that combines the three\ncomponents of standard pipelines for local feature detection and description\ninto a single di\ufb00erentiable network. We used Spatial Transformers together with\nthe softargmax function to mesh them together into a uni\ufb01ed network that\ncan be trained end-to-end with back-propagation. While this makes learning\nthe network from scratch theoretically possible, it is not practical. We therefore\nproposed an e\ufb00ective strategy to train it.\nOur experimental results demonstrate that our integrated approach outper-\nforms the state-of-the-art. To further improve performance, we will look into\nstrategies that allow us to take advantage even more e\ufb00ectively of our ability to\ntrain the network as a whole. In particular, we will look into using hard negative\nmining strategies over the whole image [45] instead of relying on pre-extracted\npatches. This has the potential of producing more discriminative \ufb01lters and,\nconsequently, better descriptors.\nLIFT: Learned Invariant Feature Transform\n15\nReferences\n1. Lowe, D.: Distinctive Image Features from Scale-Invariant Keypoints. IJCV 20(2)\n(2004)\n2. Bay, H., Ess, A., Tuytelaars, T., Van Gool, L.: SURF: Speeded Up Robust Features.\nCVIU 10(3) (2008) 346\u2013359\n3. Tola, E., Lepetit, V., Fua, P.: A Fast Local Descriptor for Dense Matching. In:\nCVPR. (2008)\n4. Rublee, E., Rabaud, V., Konolidge, K., Bradski, G.: ORB: An E\ufb03cient Alternative\nto SIFT or SURF. In: ICCV. (2011)\n5. Mainali, P., Lafruit, G., Tack, K., Van Gool, L., Lauwereins, R.: Derivative-Based\nScale Invariant Image Feature Detector with Error Resilience. TIP 23(5) (2014)\n2380\u20132391\n6. Verdie, Y., Yi, K.M., Fua, P., Lepetit, V.: TILDE: A Temporally Invariant Learned\nDEtector. In: CVPR. (2015)\n7. Han, X., Leung, T., Jia, Y., Sukthankar, R., Berg, A.C.:\nMatchNet: Unifying\nFeature and Metric Learning for Patch-Based Matching. In: CVPR. (2015)\n8. Zagoruyko, S., Komodakis, N.: Learning to Compare Image Patches via Convolu-\ntional Neural Networks. In: CVPR. (2015)\n9. Yi, K., Verdie, Y., Lepetit, V., Fua, P.: Learning to Assign Orientations to Feature\nPoints. In: CVPR. (2016)\n10. Simo-Serra, E., Trulls, E., Ferraz, L., Kokkinos, I., Fua, P., Moreno-Noguer, F.:\nDiscriminative Learning of Deep Convolutional Feature Point Descriptors.\nIn:\nICCV. (2015)\n11. Jaderberg, M., Simonyan, K., Zisserman, A., Kavukcuoglu, K.: Spatial Transformer\nNetworks. In: NIPS. (2015)\n12. Chapelle, O., Wu, M.: Gradient Descent Optimization of Smoothed Information\nRetrieval Metrics. Information Retrieval 13(3) (2009) 216\u2013235\n13. Harris, C., Stephens, M.:\nA Combined Corner and Edge Detector.\nIn: Fourth\nAlvey Vision Conference. (1988)\n14. Moravec, H.: Obstacle Avoidance and Navigation in the Real World by a Seeing\nRobot Rover.\nIn: tech. report CMU-RI-TR-80-03, Robotics Institute, Carnegie\nMellon University, Stanford University. (September 1980)\n15. Rosten, E., Drummond, T.: Machine Learning for High-Speed Corner Detection.\nIn: ECCV. (2006)\n16. Matas, J., Chum, O., Martin, U., Pajdla, T.: Robust Wide Baseline Stereo from\nMaximally Stable Extremal Regions. In: BMVC. (September 2002) 384\u2013393\n17. Mikolajczyk, K., Schmid, C.:\nAn A\ufb03ne Invariant Interest Point Detector.\nIn:\nECCV. (2002) 128\u2013142\n18. F\u00a8orstner, W., Dickscheid, T., Schindler, F.: Detecting Interpretable and Accurate\nScale-Invariant Keypoints. In: ICCV. (September 2009)\n19. Zitnick, C., Ramnath, K.: Edge Foci Interest Points. In: ICCV. (2011)\n20. Mainali, P., Lafruit, G., Yang, Q., Geelen, B., Van Gool, L., Lauwereins, R.: SIFER:\nScale-Invariant Feature Detector with Error Resilience. IJCV 104(2) (2013) 172\u2013\n197\n21. Sochman, J., Matas, J.: Learning a Fast Emulator of a Binary Decision Process.\nIn: ACCV. (2007) 236\u2013245\n22. Trujillo, L., Olague, G.: Using Evolution to Learn How to Perform Interest Point\nDetection. In: ICPR. (2006) 211\u2013214\n16\nK. M. Yi, E. Trulls, V. Lepetit, P. Fua\n23. Strecha, C., Bronstein, A., Bronstein, M., Fua, P.: LDAHash: Improved Matching\nwith Smaller Descriptors. PAMI 34(1) (January 2012)\n24. Winder, S., Brown, M.: Learning Local Image Descriptors. In: CVPR. (June 2007)\n25. Perez, C., Olague, G.:\nGenetic Programming As Strategy for Learning Image\nDescriptor Operators. Intelligent Data Analysis 17 (2013) 561\u2013583\n26. Simonyan, K., Vedaldi, A., Zisserman, A.:\nLearning Local Feature Descriptors\nUsing Convex Optimisation. PAMI (2014)\n27. Zbontar, J., LeCun, Y.: Computing the Stereo Matching Cost with a Convolutional\nNeural Network. In: CVPR. (2015)\n28. Balntas, V., Johns, E., Tang, L., Mikolajczyk, K.: PN-Net: Conjoined Triple Deep\nNetwork for Learning Local Image Descriptors. In: arXiv Preprint. (2016)\n29. Wilson, K., Snavely, N.:\nRobust Global Translations with 1DSfM.\nIn: ECCV.\n(2014)\n30. Wu, C.: Towards Linear-Time Incremental Structure from Motion. In: 3DV. (2013)\n31. Paulin, M., Douze, M., Harchaoui, Z., Mairal, J., Perronnin, F., Schmid, C.: Local\nConvolutional Features with Unsupervised Training for Image Retrieval. In: ICCV.\n(2015)\n32. Sermanet, P., Chintala, S., LeCun, Y.: Convolutional Neural Networks Applied to\nHouse Numbers Digit Classi\ufb01cation. In: ICPR. (2012)\n33. Strecha, C., Hansen, W., Van Gool, L., Fua, P., Thoennessen, U.: On Benchmark-\ning Camera Calibration and Multi-View Stereo for High Resolution Imagery. In:\nCVPR. (2008)\n34. Aanaes, H., Dahl, A.L., Pedersen, K.S.: Interesting Interest Points. IJCV 97 (2012)\n18\u201335\n35. Mikolajczyk, K., Schmid, C.: A Performance Evaluation of Local Descriptors. In:\nCVPR. (June 2003) 257\u2013263\n36. Alcantarilla, P., Fern\u00b4andez, P., Bartoli, A., Davidson, A.J.: KAZE Features. In:\nECCV. (2012)\n37. Tola, E., Lepetit, V., Fua, P.: Daisy: An E\ufb03cient Dense Descriptor Applied to\nWide Baseline Stereo. PAMI 32(5) (2010) 815\u2013830\n38. Bellavia, F., Tegolo, D.: Improving Sift-Based Descriptors Stability to Rotations.\nIn: ICPR. (2010)\n39. Mikolajczyk, K., Schmid, C.: Scale and A\ufb03ne Invariant Interest Point Detectors.\nIJCV 60 (2004) 63\u201386\n40. Fan, B., Wu, F., Hu, Z.: Aggregating Gradient Distributions into Intensity Orders:\nA Novel Local Image Descriptor. In: CVPR. (2011)\n41. Wang, Z., Fan, B., Wu, F.: Local Intensity Order Pattern for Feature Description.\nIn: ICCV. (2011)\n42. Zitnick, C.: Binary Coherent Edge Descriptors. In: ECCV. (2010)\n43. Leutenegger, S., Chli, M., Siegwart, R.: BRISK: Binary Robust Invariant Scalable\nKeypoints. In: ICCV. (2011)\n44. Alahi, A., Ortiz, R., Vandergheynst, P.: FREAK: Fast Retina Keypoint. In: CVPR.\n(2012)\n45. Felzenszwalb, P., Girshick, R., McAllester, D., Ramanan, D.: Object Detection\nwith Discriminatively Trained Part Based Models. PAMI (2010)\n",
        "sentence": " An alternative view of our work is as a multi-view extension of learning-based stereo correspondence [23, 26, 27] to more than two views. , Euclidean distance) and learn to map image patches to \u201cdescriptor vectors\u201d that, according to that metric, have small distance for matching patches and large distance for non-matching patches [23]. The recently proposed LIFT descriptor [20, 23] is learned with the help of a Siamese network, using a loss function that ensures that descriptors of matching patches end up having low Euclidean distance, whereas non-matching descriptors have not. For the other compared descriptors, SIFT [16] and LIFT [20, 23], we compute similarity via the pairwise (Euclidean) descriptor distances from the warped images, and feed them to the same decision mechanism to select the best per-pixel depth estimate.",
        "context": "patches. This has the potential of producing more discriminative \ufb01lters and,\nconsequently, better descriptors.\nLIFT: Learned Invariant Feature Transform\n15\nReferences\n1. Lowe, D.: Distinctive Image Features from Scale-Invariant Keypoints. IJCV 20(2)\n3.3\nDescriptor\nLearning feature descriptors from raw image patches has been extensively re-\nsearched during the past year [7,8,10,27,28,31], with multiple works reporting\nimpressive results on patch retrieval, narrow baseline stereo, and matching non-\nsalient image patches, while being robust to transformations such as viewpoint\nor illumination changes. The \ufb01eld reached maturity with the introduction of\nSIFT [1], which is computed from local histograms of gradient orientations, and"
    },
    {
        "title": "Toward robust distance metric analysis for similarity estimation",
        "author": [
            "J. Yu",
            "Q. Tian",
            "J. Amores",
            "N. Sebe"
        ],
        "venue": null,
        "citeRegEx": "24",
        "shortCiteRegEx": "24",
        "year": 2006,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " SIFT, and train a similarity/dissimilarity metric between them [24]. SIFT, the learning served to \u201cdistort\u201d the descriptor space so that nearby false matches get pushed apart and the distance becomes more discriminative [24].",
        "context": null
    },
    {
        "title": "Fast and high quality fusion of depth maps",
        "author": [
            "C. Zach"
        ],
        "venue": null,
        "citeRegEx": "25",
        "shortCiteRegEx": "25",
        "year": 2008,
        "abstract": "",
        "full_text": "",
        "sentence": " We note that the same is true for \u201cmulti-view stereo\u201d methods that reconstruct implicit [25] or explicit [5] surfaces. We note that there is also a large body of work termed \u201cmulti-view reconstruction\u201d that in fact computes depth maps from two views and focuses on integrating the corresponding 3D point clouds into implicit [7, 25] or explicit [5] surface representations.",
        "context": null
    },
    {
        "title": "Learning to compare image patches via convolutional neural networks",
        "author": [
            "S. Zagoruyko",
            "N. Komodakis"
        ],
        "venue": null,
        "citeRegEx": "26",
        "shortCiteRegEx": "26",
        "year": 2015,
        "abstract": "In this paper we show how to learn directly from image data (i.e., without\nresorting to manually-designed features) a general similarity function for\ncomparing image patches, which is a task of fundamental importance for many\ncomputer vision problems. To encode such a function, we opt for a CNN-based\nmodel that is trained to account for a wide variety of changes in image\nappearance. To that end, we explore and study multiple neural network\narchitectures, which are specifically adapted to this task. We show that such\nan approach can significantly outperform the state-of-the-art on several\nproblems and benchmark datasets.",
        "full_text": "Learning to Compare Image Patches via Convolutional Neural Networks\nSergey Zagoruyko\nUniversite Paris Est, Ecole des Ponts ParisTech\nsergey.zagoruyko@imagine.enpc.fr\nNikos Komodakis\nUniversite Paris Est, Ecole des Ponts ParisTech\nnikos.komodakis@enpc.fr\nAbstract\nIn this paper we show how to learn directly from image\ndata (i.e., without resorting to manually-designed features)\na general similarity function for comparing image patches,\nwhich is a task of fundamental importance for many com-\nputer vision problems. To encode such a function, we opt\nfor a CNN-based model that is trained to account for a\nwide variety of changes in image appearance. To that end,\nwe explore and study multiple neural network architectures,\nwhich are speci\ufb01cally adapted to this task. We show that\nsuch an approach can signi\ufb01cantly outperform the state-of-\nthe-art on several problems and benchmark datasets.\n1. Introduction\nComparing patches across images is probably one of the\nmost fundamental tasks in computer vision and image anal-\nysis. It is often used as a subroutine that plays an important\nrole in a wide variety of vision tasks. These can range from\nlow-level tasks such as structure from motion, wide baseline\nmatching, building panoramas, and image super-resolution,\nup to higher-level tasks such as object recognition, image\nretrieval, and classi\ufb01cation of object categories, to mention\na few characteristic examples.\nOf course, the problem of deciding if two patches corre-\nspond to each other or not is quite challenging as there exist\nfar too many factors that affect the \ufb01nal appearance of an\nimage [17]. These can include changes in viewpoint, varia-\ntions in the overall illumination of a scene, occlusions, shad-\ning, differences in camera settings, etc. In fact, this need\nof comparing patches has given rise to the development of\nmany hand-designed feature descriptors over the past years,\nincluding SIFT [15], that had a huge impact in the com-\nputer vision community. Yet, such manually designed de-\nscriptors may be unable to take into account in an optimal\nmanner all of the aforementioned factors that determine the\nappearance of a patch. On the other hand, nowadays one\ncan easily gain access to (or even generate using available\nSource code and trained models are available online at http:\n//imagine.enpc.fr/\u02dczagoruys/deepcompare.html\n(work\nsupported by EC project FP7-ICT-611145 ROBOSPECT).\nConvNet\nsimilarity\npatch 1\npatch 2\ndecision network\nFigure 1. Our goal is to learn a general similarity function for im-\nage patches. To encode such a function, here we make use of and\nexplore convolutional neural network architectures.\nsoftware) large datasets that contain patch correspondences\nbetween images [22]. This begs the following question: can\nwe make proper use of such datasets to automatically learn\na similarity function for image patches ?\nThe goal of this paper is to af\ufb01rmatively address the\nabove question. Our aim is thus to be able to generate a\npatch similarity function from scratch, i.e., without attempt-\ning to use any manually designed features but instead di-\nrectly learn this function from annotated pairs of raw image\npatches. To that end, inspired also by the recent advances in\nneural architectures and deep learning, we choose to repre-\nsent such a function in terms of a deep convolutional neural\nnetwork [14, 13] (Fig. 1). In doing so, we are also interested\nin addressing the issue of what network architecture should\nbe best used in a task like this. We thus explore and propose\nvarious types of networks, having architectures that exhibit\ndifferent trade-offs and advantages. In all cases, to train\nthese networks, we are using as sole input a large database\nthat contains pairs of raw image patches (both matching\nand non-matching). This allows to further improve the per-\nformance of our method simply by enriching this database\nwith more samples (as software for automatically generat-\ning such samples is readily available [21]).\nTo conclude this section, the paper\u2019s main contributions\nare as follows: (i) We learn directly from image data (i.e.,\nwithout any manually-designed features) a general similar-\n1\narXiv:1504.03641v1  [cs.CV]  14 Apr 2015\nity function for patches that can implicitly take into ac-\ncount various types of transformations and effects (due to\ne.g., a wide baseline, illumination, etc.). (ii) We explore\nand propose a variety of different neural network models\nadapted for representing such a function, highlighting at the\nsame time network architectures that offer improved per-\nformance. as in [19]. (iii) We apply our approach on sev-\neral problems and benchmark datasets, showing that it sig-\nni\ufb01cantly outperforms the state-of-the-art and that it leads\nto feature descriptors with much better performance than\nmanually designed descriptors (e.g., SIFT, DAISY) or other\nlearnt descriptors as in [19]. Importantly, due to their con-\nvolutional nature, the resulting descriptors are very ef\ufb01cient\nto compute even in a dense manner.\n2. Related work\nThe conventional approach to compare patches is to use\ndescriptors and a squared euclidean distance. Most feature\ndescriptors are hand-crafted as SIFT [15] or DAISY [26].\nRecently, methods for learning a descriptor have been pro-\nposed [27] (e.g., DAISY-like descriptors learn pooling re-\ngions and dimensionality reduction [3]). Simonyan et al.\n[19] proposed a convex procedure for training on both tasks.\nOur approach, however, is inspired by the recent success\nof convolutional neural networks [18, 25, 24, 9]. Although\nthese models involve a highly non-convex objective func-\ntion during training, they have shown outstanding results in\nvarious tasks [18]. Fischer et al. [10] analysed the perfor-\nmance of convolutional descriptors from AlexNet network\n(that was trained on Imagenet dataset [13]) on the well-\nknown Mikolajczyk dataset [16] and showed that these con-\nvolutional descriptors outperform SIFT in most cases ex-\ncept blur. They also proposed an unsupervised training ap-\nproach for deriving descriptors that outperform both SIFT\nand Imagenet trained network.\nZbontar and LeCun in [28] have recently proposed a\nCNN-based approach to compare patches for computing\ncost in small baseline stereo problem and shown the best\nperformance in KITTI dataset. However, the focus of that\nwork was only on comparing pairs that consist of very small\npatches like the ones in narrow baseline stereo. In contrast,\nhere we aim for a similarity function that can account for\na broader set of appearance changes and can be used in a\nmuch wider and more challenging set of applications, in-\ncluding, e.g., wide baseline stereo, feature matching and\nimage retrieval.\n3. Architectures\nAs already mentioned, the input to the neural network\nis considered to be a pair of image patches. Our models\ndo not impose any limitations with respect to the number\nof channels in the input patches, i.e., given a dataset with\ncolour patches the networks could be trained to further in-\ncrease performance. However, to be able to compare our\napproach with state-of-the-art methods on existing datasets,\nwe chose to use only grayscale patches during training. Fur-\nthermore, with the exception of the SPP model described in\nsection 3.2, in all other cases the patches given as input to\nthe network are assumed to have a \ufb01xed size of 64 \u00d7 64\n(this means that original patches may need to be resized to\nthe above spatial dimensions).\nThere are several ways in which patch pairs can be pro-\ncessed by the network and how the information sharing can\ntake place in this case. For this reason, we explored and\ntested a variety of models. We start in section 3.1 by de-\nscribing the three basic neural network architectures that\nwe studied, i.e., 2-channel, Siamese, Pseudo-siamese (see\nFig. 2), which offer different trade-offs in terms of speed\nand accuracy (note that, as usually, applied patch-matching\ntechniques imply testing a patch against a big number of\nother patches, and so re-using computed information is al-\nways useful). Essentially these architectures stem from the\ndifferent way that each of them attempts to address the fol-\nlowing question: when composing a similarity function for\ncomparing image patches, do we \ufb01rst choose to compute\na descriptor for each patch and then create a similarity on\ntop of these descriptors or do we perhaps choose to skip\nthe part related to the descriptor computation and directly\nproceed with the similarity estimation?\nIn addition to the above basic models, we also describe\nin section 3.2 some extra variations concerning the network\narchitecture. These variations, which are not mutually ex-\nclusive to each other, can be used in conjunction with any\nof the basic models described in section 3.1. Overall, this\nleads to a variety of models that is possible to be used for\nthe task of comparing image patches.\n3.1. Basic models\nSiamese: This type of network resembles the idea of\nhaving a descriptor [2, 6]. There are two branches in the net-\nwork that share exactly the same architecture and the same\nset of weights. Each branch takes as input one of the two\npatches and then applies a series of convolutional, ReLU\nand max-pooling layers. Branch outputs are concatenated\nand given to a top network that consists of linear fully con-\nnected and ReLU layers. In our tests we used a top network\nconsisting of 2 linear fully connected layers (each with 512\nhidden units) that are separated by a ReLU activation layer.\nBranches of the siamese network can be viewed as de-\nscriptor computation modules and the top network - as a\nsimilarity function. For the task of matching two sets of\npatches at test time, descriptors can \ufb01rst be computed inde-\npendently using the branches and then matched with the top\nnetwork (or even with a distance function like l2).\nPseudo-siamese: In terms of complexity, this architec-\nbranch network 1\nbranch network 2\ndecision network\npatch 1\npatch 2\n2-channel network\ndecision layer\npatch 1\npatch 2\nshared\n(siamese)\nunshared\n(pseudo-\nsiamese)\nFigure 2. Three basic network architectures: 2-channel on the left,\nsiamese and pseudo-siamese on the right (the difference between\nsiamese and pseudo-siamese is that the latter does not have shared\nbranches). Color code used: cyan = Conv+ReLU, purple = max\npooling, yellow = fully connected layer (ReLU exists between\nfully connected layers as well).\nture can be considered as being in-between the siamese\nand the 2-channel networks. More speci\ufb01cally, it has the\nstructure of the siamese net described above except that\nthe weights of the two branches are uncoupled, i.e., not\nshared. This increases the number of parameters that can\nbe adjusted during training and provides more \ufb02exibility\nthan a restricted siamese network, but not as much as the\n2-channel network described next. On the other hand, it\nmaintains the ef\ufb01ciency of siamese network at test time.\n2-channel: unlike the previous models, here there is no\ndirect notion of descriptor in the architecture. We simply\nconsider the two patches of an input pair as a 2-channel\nimage, which is directly fed to the \ufb01rst convolutional layer\nof the network. In this case, the bottom part of the net-\nwork consists of a series of convolutional, ReLU and max-\npooling layers. The output of this part is then given as input\nto a top module that consists simply of a fully connected\nlinear decision layer with 1 output. This network provides\ngreater \ufb02exibility compared to the above models as it starts\nby processing the two patches jointly. Furthermore, it is\nfast to train, but in general at test time it is more expensive\nas it requires all combinations of patches to be tested against\neach other in a brute-force manner.\n3.2. Additional models\nDeep network. We apply the technique proposed by Si-\nmonyan and Zisserman in [20] advising to break up bigger\nconvolutional layers into smaller 3x3 kernels, separated by\nReLU activations, which is supposed to increase the non-\nlinearities inside the network and make the decision func-\ntion more discriminative. They also report that it might be\ndif\ufb01cult to initialise such a network, we, however, do not\nobserve this behavior and train the network from scratch\nas usual. In our case, when applying this technique to our\nsurround stream\n(2 shared branches)\ndecision network\npatch 1\ncentral stream\n(2 shared branches)\npatch 2\nFigure 3. A central-surround two-stream network that uses a\nsiamese-type architecture to process each stream. This results in\n4 branches in total that are given as input to the top decision layer\n(the two branches in each stream are shared in this case).\nmodel, the convolutional part of the \ufb01nal architecture turns\nout to consist of one convolutional 4x4 layer and 6 convo-\nlutional layers with 3x3 layers, separated by ReLU activa-\ntions. As we shall also see later in the experimental results,\nsuch a change in the network architecture can contribute in\nfurther improving performance, which is in accordance with\nanalogous observations made in [20].\nCentral-surround two-stream network. As its name\nsuggests, the proposed architecture consists of two separate\nstreams, central and surround, which enable a processing in\nthe spatial domain that takes place over two different resolu-\ntions. More speci\ufb01cally, the central high-resolution stream\nreceives as input two 32 \u00d7 32 patches that are generetad\nby cropping (at the original resolution) the central 32 \u00d7 32\npart of each input 64\u00d7 64 patch. Furthermore, the surround\nlow-resolution stream receives as input two 32\u00d732 patches,\nwhich are generated by downsampling at half the original\npair of input patches. The resulting two streams can then be\nprocessed by using any of the basic architectures described\nin section 3.1 (see Fig. 3 for an example that uses a siamese\narchitecture for each stream).\nOne reason to make use of such a two-stream architec-\nture is because multi-resolution information is known to be\nimportant in improving the performance of image match-\ning. Furthermore, by considering the central part of a patch\ntwice (i.e., in both the high-resolution and low-resolution\nstreams) we implicitly put more focus on the pixels closer\nto the center of a patch and less focus on the pixels in the pe-\nriphery, which can also help for improving the precision of\nmatching (essentially, since pooling is applied to the down-\nsampled image, pixels in the periphery are allowed to have\nmore variance during matching). Note that the total input\ndimenionality is reduced by a factor of two in this case. As\na result, training proceeds faster, which is also one other\nbranch network 1\nbranch network 2\ndecision network\npatch 1\npatch 2\nSPP\nSPP\nFigure 4. SPP network for a siamese architecture: SPP layers (or-\nange) are inserted immediately after the 2 branches of the network\nso that the top decision layer has an input of \ufb01xed dimensionality\nfor any size of the input patches.\npractical advantage.\nSpatial pyramid pooling (SPP) network for compar-\ning patches. Up to this point we have been assuming that\nthe network requires the input patches to have a \ufb01xed size\nof 64 \u00d7 64. This requirement comes from the fact that the\noutput of the last convolutional layer of the network needs\nto have a prede\ufb01ned dimensionality. Therefore, when we\nneed to compare patches of arbitrary sizes, this means that\nwe \ufb01rst have to resize them to the above spatial dimensions.\nHowever, if we look at the example of descriptors like SIFT,\nfor instance, we can see that another possible way to deal\nwith patches of arbitrary sizes is via adjusting the size of\nthe spatial pooling regions to be proportional to the size of\nthe input patch so that we can still maintain the required\n\ufb01xed output dimensionality for the last convolutional layer\nwithout deteriorating the resolution of the input patches.\nThis is also the idea behind the recently proposed SPP-\nnet architecture [11], which essentially amounts to inserting\na spatial pyramid pooling layer between the convolutional\nlayers and the fully-connected layers of the network. Such a\nlayer aggregates the features of the last convolutional layer\nthrough spatial pooling, where the size of the pooling re-\ngions is dependent on the size of the input. Inspired by this,\nwe propose to also consider adapting the network models of\nsection 3.1 according to the above SPP-architecture. This\ncan be easily achieved for all the considered models (e.g.,\nsee Fig. 4 for an example with a siamese model).\n4. Learning\nOptimization. We train all models in strongly super-\nvised manner. We use a hinge-based loss term and squared\nl2-norm regularization that leads to the following learning\nobjective function\nmin\nw\n\u03bb\n2 \u2225w\u22252 +\nN\nX\ni=1\nmax(0, 1 \u2212yionet\ni\n) ,\n(1)\nwhere w are the weights of the neural network, onet\ni\nis\nthe network output for the i-th training sample, and yi \u2208\n{\u22121, 1} the corresponding label (with \u22121 and 1 denoting a\nnon-matching and a matching pair, respectively).\nASGD with constant learning rate 1.0, momentum 0.9\nand weight decay \u03bb = 0.0005 is used to train the models.\nTraining is done in mini-batches of size 128. Weights are\ninitialised randomly and all models are trained from scratch.\nData Augmentation and preprocessing.\nTo com-\nbat over\ufb01tting we augment training data by \ufb02ipping both\npatches in pairs horizontally and vertically and rotating to\n90, 180, 270 degrees. As we don\u2019t notice over\ufb01tting while\ntraining in such manner we train models for a certain num-\nber of iterations, usually for 2 days, and then test perfor-\nmance on test set.\nTraining dataset size allows us to store all the images di-\nrectly in GPU memory and very ef\ufb01ciently retrieve patch\npairs during training. Images are augmented \u201don-the \ufb02y\u201d.\nWe use Titan GPU in Torch [7] and convolution routines\nare taken from Nvidia cuDNN library [5]. Our siamese de-\nscriptors on GPU are just 2 times slower than computing\nSIFT descriptors on CPU and 2 times faster than Imagenet\ndescriptors on GPU according to [10].\n5. Experiments\nWe applied our models to a variety of problems and\ndatasets. In the following we report results, and also pro-\nvide comparisons with the state-of-the-art.\n5.1. Local image patches benchmark\nFor a \ufb01rst evaluation of our models, we used the standard\nbenchmark dataset from [3] that consists of three subsets,\nYosemite, Notre Dame, and Liberty, each of which contains\nmore than 450,000 image patches (64 x 64 pixels) sampled\naround Difference of Gaussians feature points. The patches\nare scale and orientation normalized. Each of the subsets\nwas generated using actual 3D correspondences obtained\nvia multi-view stereo depth maps. These maps were used to\nproduce 500,000 ground-truth feature pairs for each dataset,\nwith equal number of positive (correct) and negative (incor-\nrect) matches.\nFor evaluating our models, we use the evaluation pro-\ntocol of [4] and generate ROC curves by thresholding the\ndistance between feature pairs in the descriptor space. We\nreport the false positive rate at 95% recall (FPR95) on each\nof the six combinations of training and test sets, as well as\nthe mean across all combinations. We also report the mean,\ndenoted as mean(1, 4), for only those 4 combinations that\nwere used in [1], [3] (in which case training takes place on\nYosemite or Notre Dame, but not Liberty).\nTable 1 reports the performance of several models, and\nalso details their architecture (we have also experimented\nwith smaller kernels, less max-pooling layers, as well as\nadding normalisations, without noticing any signi\ufb01cant im-\nprovement in performance). We brie\ufb02y summarize some of\nthe conclusions that can be drawn from this table. A \ufb01rst\nimportant conclusion is that 2-channel-based architectures\n(e.g., 2ch, 2ch-deep, 2ch-2stream) exhibit clearly\nthe best performance among all models. This is something\nthat indicates that it is important to jointly use information\nfrom both patches right from the \ufb01rst layer of the network.\n2ch-2stram network was the top-performing network\non this dataset, with 2ch-deep following closely (this ver-\ni\ufb01es the importance of multi-resolution information during\nmatching and that also increasing the network depth helps).\nIn fact, 2ch-2stream managed to outperform the previ-\nous state-of-the-art by a large margin, achieving 2.45 times\nbetter score than [19]! The difference with SIFT was even\nlarger, with our model giving 6.65 times better score in this\ncase (SIFT score on mean(1,4) was 31.2 according to\n[3]).\nRegarding siamese-based architectures, these too man-\nage to achieve better performance than existing state-of-\nthe-art systems.\nThis is quite interesting because, e.g.,\nnone of these siamese networks tries to learn the shape,\nsize or placement of the pooling regions (like, e.g., [19, 3]\ndo), but instead utilizes just standard max-pooling lay-\ners. Among the siamese models, the two-stream network\n(siam-2stream) had the best performance, verifying\nonce more the importance of multi-resolution information\nwhen it comes to comparing image patches. Furthermore,\nthe pseudo-siamese network (pseudo-siam) was better\nthan the corresponding siamese one (siam).\nWe also conducted additional experiments, in which\nwe tested the performance of siamese models when their\ntop decision layer is replaced with the l2 Euclidean dis-\ntance of the two convolutional descriptors produced by\nthe two branches of the network (denoted with the suf-\n\ufb01x l2 in the name).\nIn this case, prior to applying the\nEuclidean distance, the descriptors are l2-normalized (we\nalso tested l1 normalization). For pseudo-siamese only one\nbranch was used to extract descriptors.\nAs expected, in\nthis case the two-stream network (siam-2stream-l2)\ncomputes better distances than the siamese network\n(siam-l2), which, in turn, computes better distances than\nthe pseudo-siamese model (pseudo-siam-l2). In fact,\nthe siam-2stream-l2 network manages to outperform\neven the previous state-of-the-art descriptor [19], which is\nquite surprising given that these siamese models have never\nbeen trained using l2 distances.\nFor a more detailed comparison of the various models,\nconv3(3456)\nconv4(3456)\nconv5(2304)\nNotredame\n12.22\n9.64\n19.384\nLiberty\n16.25\n14.26\n21.592\nYosemite\n33.25\n30.22\n43.262\nmean\n20.57\n17.98\n28.08\nTable 2. FPR95 for imagenet-trained features (dimensionality of\neach feature appears as subscript).\nwe provide the corresponding ROC curves in Fig. 5. Fur-\nthermore, we show in Table 2 the performance of imagenet-\ntrained CNN features (these were l2-normalized to improve\nresults). Among these, conv4 gives the best FPR95 score,\nwhich is equal to 17.98. This makes it better than SIFT but\nstill much worse than our models.\n(a)\n(b)\nFigure 6. (a) Filters of the \ufb01rst convolutional layer of siam net-\nwork. (b) Rows correspond to \ufb01rst layer \ufb01lters from 2ch network\n(only a subset shown), depicting left and right part of each \ufb01lter.\n(a) true positives\n(b) false negatives\n(c) true negatives\n(d) false positives\nFigure 7. Top-ranking false and true matches by 2ch-deep.\nFig. 6(a) displays the \ufb01lters of the \ufb01rst convolutional\nlayer learnt by the siamese network. Furthermore, Fig. 6(b)\nshows the left and right parts for a subset of the \ufb01rst layer\n\ufb01lters of the 2-channel network 2ch. It is worth mention-\ning that corresponding left and right parts look like being\nnegative to each other, which basically means that the net-\nwork has learned to compute differences of features be-\ntween the two patches (note, though, that not all \ufb01rst layer\n\ufb01lters of 2ch look like this). Last, we show in Fig. 7 some\ntop ranking false and correct matches as computed by the\n2ch-deep network. We observe that false matches could\nbe easily mistaken even by a human (notice, for instance,\nhow similar the two patches in false positive examples look\nlike).\nFor the rest of the experiments, we note that we use mod-\nels trained on the Liberty dataset.\nTrain\nTest\n2ch-2stream\n2ch-deep\n2ch\nsiam\nsiam-l2\npseudo-siam\npseudo-siam-l2\nsiam-2stream\nsiam-2stream-l2\n[19]\nYos\nND\n2.11\n2.52\n3.05\n5.75\n8.38\n5.44\n8.95\n5.29\n5.58\n6.82\nYos\nLib\n7.2\n7.4\n8.59\n13.48\n17.25\n10.35\n18.37\n11.51\n12.84\n14.58\nND\nYos\n4.1\n4.38\n6.04\n13.23\n15.89\n12.64\n15.62\n10.44\n13.02\n10.08\nND\nLib\n4.85\n4.55\n6.05\n8.77\n13.24\n12.87\n16.58\n6.45\n8.79\n12.42\nLib\nYos\n5\n4.75\n7\n14.89\n19.91\n12.5\n17.83\n9.02\n13.24\n11.18\nLib\nND\n1.9\n2.01\n3.03\n4.33\n6.01\n3.93\n6.58\n3.05\n4.54\n7.22\nmean\n4.19\n4.27\n5.63\n10.07\n13.45\n9.62\n13.99\n7.63\n9.67\n10.38\nmean(1,4)\n4.56\n4.71\n5.93\n10.31\n13.69\n10.33\n14.88\n8.42\n10.06\n10.98\nTable 1. Performance of several models on the \u201clocal image patches\u201d benchmark. The models architecture is as follows: (i) 2ch-2stream\nconsists of two branches C(95, 5, 1)-ReLU-P(2, 2)-C(96, 3, 1)-ReLU-P(2, 2)-C(192, 3, 1)-ReLU-C(192, 3, 1)-ReLU, one for cen-\ntral and one for surround parts, followed by F(768)-ReLU-F(1) (ii) 2ch-deep = C(96, 4, 3)-Stack(96)-P(2, 2)-Stack(192)-F(1),\nwhere Stack(n) = C(n, 3, 1)-ReLU-C(n, 3, 1)-ReLU-C(n, 3, 1)-ReLU. (iii) 2ch = C(96, 7, 3)-ReLU-P(2, 2)-C(192, 5, 1)-ReLU-\nP(2, 2)-C(256, 3, 1)-ReLU-F(256)-ReLU-F(1) (iv) siam has two branches C(96, 7, 3)-ReLU-P(2, 2)-C(192, 5, 1)-ReLU-P(2, 2)-\nC(256, 3, 1)-ReLU and decision layer F(512)-ReLU-F(1) (v) siam-l2 reduces to a single branch of siam (vi) pseudo-siam is uncoupled\nversion of siam (vii) pseudo-siam-l2 reduces to a single branch of pseudo-siam (viii) siam-2stream has 4 branches C(96, 4, 2)-ReLU-\nP(2, 2)-C(192, 3, 1)-ReLU-C(256, 3, 1)-ReLU-C(256, 3, 1)-ReLU (coupled in pairs for central and surround streams), and decision\nlayer F(512)-ReLU-F(1) (ix) siam-2stream-l2 consists of one central and one surround branch of siam-2stream. The shorthand no-\ntation used was the following: C(n, k, s) is a convolutional layer with n \ufb01lters of spatial size k \u00d7 k applied with stride s, P(k, s) is a\nmax-pooling layer of size k \u00d7 k applied with stride s, and F(n) denotes a fully connected linear layer with n output units.\n0\n0.05\n0.1\n0.15\n0.2\n0.25\n0.3\n0.7\n0.75\n0.8\n0.85\n0.9\n0.95\n1\nyosemite \u2212> notredame\nFalse positive rate\nTrue positive rate\nSimonyan etal 6.82%\n2ch\u22122stream 2.11%\n2ch\u2212deep 2.52%\nsiam 5.75%\n2ch 3.05%\nsiam\u22122stream 5.29%\n0\n0.05\n0.1\n0.15\n0.2\n0.25\n0.3\n0.7\n0.75\n0.8\n0.85\n0.9\n0.95\n1\nyosemite \u2212> liberty\nFalse positive rate\nTrue positive rate\nSimonyan etal 14.58%\n2ch\u22122stream 7.20%\n2ch\u2212deep 7.40%\nsiam 13.48%\n2ch 8.59%\nsiam\u22122stream 11.51%\n0\n0.05\n0.1\n0.15\n0.2\n0.25\n0.3\n0.7\n0.75\n0.8\n0.85\n0.9\n0.95\n1\nnotredame \u2212> yosemite\nFalse positive rate\nTrue positive rate\nSimonyan etal 10.08%\n2ch\u22122stream 4.09%\n2ch\u2212deep 4.38%\nsiam 13.23%\n2ch 6.04%\nsiam\u22122stream 10.44%\n0\n0.05\n0.1\n0.15\n0.2\n0.25\n0.3\n0.7\n0.75\n0.8\n0.85\n0.9\n0.95\n1\nnotredame \u2212> liberty\nFalse positive rate\nTrue positive rate\nSimonyan etal 12.42%\n2ch\u22122stream 4.85%\n2ch\u2212deep 4.56%\nsiam 8.77%\n2ch 6.04%\nsiam\u22122stream 6.45%\n0\n0.05\n0.1\n0.15\n0.2\n0.25\n0.3\n0.7\n0.75\n0.8\n0.85\n0.9\n0.95\n1\nliberty \u2212> yosemite\nFalse positive rate\nTrue positive rate\nSimonyan etal 11.18%\n2ch\u22122stream 5.00%\n2ch\u2212deep 4.75%\nsiam 14.89%\n2ch 7.00%\nsiam\u22122stream 9.02%\n0\n0.05\n0.1\n0.15\n0.2\n0.25\n0.3\n0.7\n0.75\n0.8\n0.85\n0.9\n0.95\n1\nliberty \u2212> notredame\nFalse positive rate\nTrue positive rate\nSimonyan etal 7.22%\n2ch\u22122stream 1.90%\n2ch\u2212deep 2.01%\nsiam 4.33%\n2ch 3.03%\nsiam\u22122stream 3.05%\nFigure 5. ROC curves for various models (including the state-of-the-art descriptor [19]) on the local image patches benchmark. Numbers\nin the legends are corresponding FPR95 values\n5.2. Wide baseline stereo evaluation\nFor this evaluation we chose the dataset by Strecha et al.\n[23], which contains several image sequences with ground\ntruth homographies and laser-scanned depthmaps. We used\n\u201cfountain\u201d and \u201cherzjesu\u201d sequences to produce 6 and 5 rec-\nti\ufb01ed stereo pairs respectively. Baselines in both sequences\nwe chose are increasing with each image making matching\nmore dif\ufb01cult. Our goal was to show that a photometric cost\ncomputed with neural network competes favorably against\ncosts produced by a state-ot-the-art hand-crafted feature de-\nscriptor, so we chose to compare with DAISY [26].\nSince our focus was not on ef\ufb01ciency, we used an un-\noptimized pipeline for computing the photometric costs.\nMore speci\ufb01cally, for 2-channel networks we used a brute-\nforce approach, where we extract patches on corresponding\nepipolar lines with subpixel estimation, construct batches\n(containing a patch from the left image I1 and all patches\non the corresponding epipolar line from the right image I2)\nand compute network outputs, resulting in the cost:\nC(p, d) = \u2212onet(I1(p), I2(p + d))\n(2)\nHere, I(p) denotes a neighbourhood intensity matrix\naround a pixel p, onet(P1, P2) is the output of the neural\nnetwork given a pair of patches P1 and P2, and d is the dis-\ntance between points on epipolar line.\nFor siamese-type networks, we compute descriptors for\neach pixel in both images once and then match them with\ndecision top layer or l2 distance. In the \ufb01rst case the formula\nfor photometric cost is the following:\nC(p, d) = \u2212otop(D1(I1(p)), D2(I2(p + d)))\n(3)\nwhere otop is output of the top decision layer, and D1, D2\nare outputs of branches of the siamese or pseudo-siamese\nnetwork, i.e. descriptors (in case of siamese network D1 =\nD2). For l2 matching, it holds:\nC(p, d) = \u2225D1(I1(p)) \u2212D2(I2(p + d))\u22252\n(4)\nIt is worth noting that all costs above can be computed a\nlot more ef\ufb01ciently using speed optimizations similar with\n[28]. This essentially means treating all fully connected lay-\ners as 1 \u00d7 1 convolutions, computing branches of siamese\nnetwork only once, and furthermore computing the outputs\nof these branches as well as the \ufb01nal outputs of the network\nat all locations using a number of forward passes on full im-\nages (e.g., for a 2-channel architecture such an approach of\ncomputing the photometric costs would only require feed-\ning the network with s2 \u00b7 dmax full 2-channel images of size\nequal to the input image pair, where s is the stride at the \ufb01rst\nlayer of the network and dmax is the maximum disparity).\nOnce computed, the photometric costs are subsequently\nused as unary terms in the following pairwise MRF energy\nE({dp})=\nX\np\nC(p, dp)+\nX\n(p,q)\u2208E\n(\u03bb1+\u03bb2e\u2212\u2225\u2207I1(p)\u22252\n\u03c32\n)\u00b7|dp\u2212dq| ,\nminimized using algorithm [8] based on FastPD [12] (we\nset \u03bb1 =0.01, \u03bb2 =0.2, \u03c3=7 and E is a 4-connected grid).\nWe show in Fig. 9 some qualitative results in terms of\ncomputed depth maps (with and without global optimiza-\ntion) for the \u201cfountain\u201d image set (results for \u201cherzjesu\u201d ap-\npear in supp. material due to lack of space). Global MRF\noptimization results visually verify that photometric cost\ncomputed with neural network is much more robust than\nwith hand-crafted features, as well as the high quality of the\ndepth maps produced by 2-channel architectures. Results\nwithout global optimization also show that the estimated\ndepth maps contain much more \ufb01ne details than DAISY.\nThey may exhibit a very sparse set of errors for the case of\nsiamese-based networks, but these errors can be very easily\neliminated during global optimization.\nFig. 8 also shows a quantitative comparison, focusing in\nthis case on siamese-based models as they are more ef\ufb01-\ncient. The \ufb01rst plot of that \ufb01gure shows (for a single stereo\npair) the distribution of deviations from the ground truth\nacross all range of error thresholds (expressed here as a\nfraction of the scene\u2019s depth range). Furthermore, the other\nplots of the same \ufb01gure summarize the corresponding distri-\nbutions of errors for the six stereo pairs of increasing base-\nline (in this case we also show separately the error distribu-\ntions when only unoccluded pixels are taken into account).\nThe error thresholds were set to 3 and 5 pixels in these plots\n(note that the maximum disparity is around 500 pixels in the\nlargest baseline). As can be seen, all siamese models per-\nform much better than DAISY across all error thresholds\nand all baseline distances (e.g., notice the difference in the\ncurves of the corresponding plots).\n5.3. Local descriptors performance evaluation\nWe also test our networks on Mikolajczyk dataset for lo-\ncal descriptors evaluation [16]. The dataset consists of 48\nimages in 6 sequences with camera viewpoint changes, blur,\ncompression, lighting changes and zoom with gradually in-\ncreasing amount of transfomation. There are known ground\ntruth homographies between the \ufb01rst and each other image\nin sequence.\nTesting technique is the same as in [16]. Brie\ufb02y, to test\na pair of images, detectors are applied to both images to\nextract keypoints. Following [10], we use MSER detec-\ntor. The ellipses provided by detector are used to exctract\npatches from input images. Ellipse size is magni\ufb01ed by a\nfactor of 3 to include more context. Then, depending on\nthe type of network, either descriptors, meaning outputs of\nsiamese or pseudo-siamese branches, are extracted, or all\npatch pairs are given to 2-channel network to assign a score.\nA quantitative comparison on this dataset is shown for\nseveral models in Fig. 10.\nHere we also test the CNN\nnetwork siam-SPP-l2, which is an SPP-based siamese\narchitecture (note that siam-SPP is same as siam but\nwith the addition of two SPP layers - see also Fig. 4). We\nused an inserted SPP layer that had a spatial dimension of\n4 \u00d7 4. As can be seen, this provides a big boost in match-\ning performance, suggesting the great utility of such an ar-\nchitecture when comparing image patches. Regarding the\nrest of the models, the observed results in Fig. 10 recon-\n\ufb01rm the conclusions already drawn from previous experi-\nments. We simply note again the very good performance\nof siam-2stream-l2, which (although not trained with\n0\n20\n40\n50\n60\n70\n80\n90\n100\nError %\nCorrect depth %\n2ch\nsiam-2stream-l 2\nsiam\nDA I SY\n1\n2\n3\n4\n5\n0\n20\n40\n60\n80\n100\nTransformation magnitude\nMRF 3\u2212pixel error (non occl. pixels)\n1\n2\n3\n4\n5\n0\n20\n40\n60\n80\n100\nTransformation magnitude\nMRF 1\u2212pixel error\n1\n2\n3\n4\n5\n0\n20\n40\n60\n80\n100\nTransformation magnitude\nMRF 1\u2212pixel error (non occl. pixels)\n1\n2\n3\n4\n5\n0\n20\n40\n60\n80\n100\nMRF 3\u2212pixel error\nTransformation magnitude\nFigure 8. Quantitative comparison for wide-baseline stereo on \u201cfountain\u201d dataset. (Leftmost plot) Distribution of deviations from ground\ntruth, expressed as a fraction of scene\u2019s depth range. (Other plots) Distribution of errors for stereo pairs of increasing baseline (horizontal\naxis) both with and without taking into account occluded pixels (error thresholds were set equal to 1 and 3 pixels in these plots - maximum\ndisparity is around 500 pixels).\nFigure 9. Wide baseline stereo evaluation. From left to right: DAISY, siam-2stream-l2, siam, 2ch. First row - \u201cwinner takes all\u201d\ndepthmaps, second row - depthmaps after MRF optimization.\nl2 distances) is able to signi\ufb01cantly outperform SIFT and\nto also match the performance of imagenet-trained features\n(using, though, a much lower dimensionality of 512).\n6. Conclusions\nIn this paper we showed how to learn directly from\nraw image pixels a general similarity function for patches,\nwhich is encoded in the form of a CNN model. To that\nend, we studied several neural network architecures that\nare speci\ufb01cally adapted to this task, and showed that they\nexhibit extremely good performance, signi\ufb01cantly outper-\nforming the state-of-the-art on several problems and bench-\nmark datasets.\nAmong these architectures, we note that 2-channel-based\nones were clearly the superior in terms of results. It is,\ntherefore, worth investigating how to further accelerate\nthe evaluation of these networks in the future.\nRegard-\ning siamese-based architectures, 2-stream multi-resolution\nmodels turned out to be extremely strong, providing always\na signi\ufb01cant boost in performance and verifying the im-\nportance of multi-resolution information when comparing\npatches. The same conclusion applies to SPP-based siamese\nnetworks, which also consistently improved the quality of\nresults1.\n1In fact, SPP performance can improve even further, as no multiple\naspect ratio patches were used during the training of SPP models (such\npatches appear only at test time).\n1\n2\n3\n4\n5\n0\n10\n20\n30\n40\n50\n60\n70\n80\nMatching mAP\nTransformation Magnitude\nAverage of all sequences\n \n \nMSER SIFT\nMSER siam-2stream-l2\nMSER Imagenet\nMSER siam-SPP-l2\nMSER 2ch-deep\nMSER 2ch-2stream\nFigure 10. Evaluation on the Mikolajczyk dataset [16] showing the\nmean average precision (mAP) averaged over all types of transfor-\nmations in the dataset (as usual, the mAP score measures the area\nunder the precision-recall curve). More detailed plots are provided\nin the supplemental material due to lack of space.\nLast, we should note that simply the use of a larger train-\ning set can potentially bene\ufb01t and improve the overall per-\nformance of our approach even further (as the training set\nthat was used in the present experiments can actually be\nconsidered rather small by today\u2019s standards).\nReferences\n[1] X. Boix, M. Gygli, G. Roig, and L. Van Gool. Sparse quan-\ntization for patch description. In CVPR, 2013. 5\n[2] J. Bromley, I. Guyon, Y. Lecun, E. Sckinger, and R. Shah.\nSignature veri\ufb01cation using a \u201dsiamese\u201d time delay neural\nnetwork. In NIPS, 1994. 2\n[3] M. Brown, G. Hua, and S. Winder. Discriminative learning\nof local image descriptors. IEEE Transactions on Pattern\nAnalysis and Machine Intelligence, 2010. 2, 4, 5\n[4] M. Brown, G. Hua, and S. Winder. Discriminative learning\nof local image descriptors. Pattern Analysis and Machine\nIntelligence, IEEE Transactions on, 33(1):43\u201357, Jan 2011.\n4\n[5] S. Chetlur, C. Woolley, P. Vandermersch, J. Cohen, J. Tran,\nB. Catanzaro, and E. Shelhamer. cudnn: Ef\ufb01cient primitives\nfor deep learning. CoRR, abs/1410.0759, 2014. 4\n[6] S. Chopra, R. Hadsell, and Y. LeCun. Learning a similarity\nmetric discriminatively, with application to face veri\ufb01cation.\nIn CVPR, 2005. 2\n[7] R. Collobert, K. Kavukcuoglu, and C. Farabet. Torch7: A\nmatlab-like environment for machine learning. In BigLearn,\nNIPS Workshop, 2011. 4\n[8] B. Conejo, N. Komodakis, S. Leprince, and J.-P. Avouac.\nInference by learning: Speeding-up graphical model opti-\nmization via a coarse-to-\ufb01ne cascade of pruning classi\ufb01er.\nIn NIPS, 2014. 7\n[9] D. Eigen, C. Puhrsch, and R. Fergus. Depth map prediction\nfrom a single image using a multi-scale deep network. In\nNIPS, 2014. 2\n[10] P. Fischer, A. Dosovitskiy, and T. Brox. Descriptor matching\nwith convolutional neural networks: a comparison to SIFT.\nCoRR, abs/1405.5769, 2014. 2, 4, 7\n[11] K. He, X. Zhang, S. Ren, and J. Sun. Spatial pyramid pooling\nin deep convolutional networks for visual recognition.\nIn\nECCV14, pages III: 346\u2013361, 2014. 4\n[12] N. Komodakis, G. Tziritas, and N. Paragios. Fast, approxi-\nmately optimal solutions for single and dynamic MRFs. In\nCVPR, 2007. 7\n[13] A. Krizhevsky, I. Sutskever, and G. E. Hinton.\nImagenet\nclassi\ufb01cation with deep convolutional neural networks. In\nF. Pereira, C. Burges, L. Bottou, and K. Weinberger, edi-\ntors, Advances in Neural Information Processing Systems 25,\npages 1097\u20131105. Curran Associates, Inc., 2012. 1, 2\n[14] Y. LeCun. A theoretical framework for back-propagation.\nIn Proceedings of the 1988 Connectionist Models Summer\nSchool, pages 21\u201328, 1988. 1\n[15] D. G. Lowe. Distinctive image features from scale-invariant\nkeypoints. International Journal of Computer Vision, 60:91\u2013\n110, 2004. 1, 2\n[16] K. Mikolajczyk and C. Schmid. A performance evaluation\nof local descriptors. IEEE Transactions on Pattern Analysis\n& Machine Intelligence, 27(10):1615\u20131630, 2005. 2, 7, 8\n[17] E. Nowak and F. Jurie.\nLearning Visual Similarity Mea-\nsures for Comparing Never Seen Objects. In CPVR 2007\n- IEEE Conference on Computer Vision and Pattern Recog-\nnition, pages 1\u20138, Minneapolis, United States, June 2007.\nIEEE Computer society. 1\n[18] A. S. Razavian, H. Azizpour, J. Sullivan, and S. Carls-\nson.\nCNN features off-the-shelf: An astounding baseline\nfor recognition. In IEEE Conference on Computer Vision\nand Pattern Recognition, CVPR Workshops 2014, Columbus,\nOH, USA, June 23-28, 2014, pages 512\u2013519, 2014. 2\n[19] K. Simonyan, A. Vedaldi, and A. Zisserman. Learning local\nfeature descriptors using convex optimisation. IEEE Trans-\nactions on Pattern Analysis and Machine Intelligence, 2014.\n2, 5, 6\n[20] K. Simonyan and A. Zisserman.\nVery deep convolu-\ntional networks for large-scale image recognition.\nCoRR,\nabs/1409.1556, 2014. 3\n[21] N. Snavely, S. M. Seitz, and R. Szeliski.\nPhoto tourism:\nExploring photo collections in 3d.\nACM Trans. Graph.,\n25(3):835\u2013846, July 2006. 1\n[22] N. Snavely, S. M. Seitz, and R. Szeliski.\nModeling the\nworld from internet photo collections. Int. J. Comput. Vision,\n80(2):189\u2013210, Nov. 2008. 1\n[23] C. Strecha, W. von Hansen, L. J. V. Gool, P. Fua, and\nU. Thoennessen. On benchmarking camera calibration and\nmulti-view stereo for high resolution imagery.\nIn CVPR.\nIEEE Computer Society, 2008. 6\n[24] C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan,\nI. J. Goodfellow, and R. Fergus. Intriguing properties of neu-\nral networks. CoRR, abs/1312.6199, 2013. 2\n[25] Y. Taigman, M. Yang, M. Ranzato, and L. Wolf. Deepface:\nClosing the gap to human-level performance in face veri\ufb01ca-\ntion. In Conference on Computer Vision and Pattern Recog-\nnition (CVPR), 2014. 2\n[26] E. Tola, V.Lepetit, and P. Fua. A Fast Local Descriptor for\nDense Matching. In Proceedings of Computer Vision and\nPattern Recognition, Alaska, USA, 2008. 2, 6\n[27] T. Trzcinski, M. Christoudias, V. Lepetit, and P. Fua. Learn-\ning Image Descriptors with the Boosting-Trick.\nIn NIPS,\n2012. 2\n[28] J. Zbontar and Y. LeCun.\nComputing the stereo match-\ning cost with a convolutional neural network.\nCoRR,\nabs/1409.4326, 2014. 2, 7\n",
        "sentence": " An alternative view of our work is as a multi-view extension of learning-based stereo correspondence [23, 26, 27] to more than two views. An extensive study of similarity measures based on different CNN architectures is presented in [26].",
        "context": "multi-view stereo for high resolution imagery.\nIn CVPR.\nIEEE Computer Society, 2008. 6\n[24] C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan,\nI. J. Goodfellow, and R. Fergus. Intriguing properties of neu-\nral networks. CoRR, abs/1312.6199, 2013. 2\ngions and dimensionality reduction [3]). Simonyan et al.\n[19] proposed a convex procedure for training on both tasks.\nOur approach, however, is inspired by the recent success\nof convolutional neural networks [18, 25, 24, 9]. Although\na broader set of appearance changes and can be used in a\nmuch wider and more challenging set of applications, in-\ncluding, e.g., wide baseline stereo, feature matching and\nimage retrieval.\n3. Architectures"
    },
    {
        "title": "Computing the stereo matching cost with a convolutional neural network",
        "author": [
            "J. Zbontar",
            "Y. LeCun"
        ],
        "venue": null,
        "citeRegEx": "27",
        "shortCiteRegEx": "27",
        "year": 2015,
        "abstract": "We present a method for extracting depth information from a rectified image\npair. We train a convolutional neural network to predict how well two image\npatches match and use it to compute the stereo matching cost. The cost is\nrefined by cross-based cost aggregation and semiglobal matching, followed by a\nleft-right consistency check to eliminate errors in the occluded regions. Our\nstereo method achieves an error rate of 2.61 % on the KITTI stereo dataset and\nis currently (August 2014) the top performing method on this dataset.",
        "full_text": "Computing the Stereo Matching Cost with a Convolutional Neural Network\nJure \u02c7Zbontar\nUniversity of Ljubljana\njure.zbontar@fri.uni-lj.si\nYann LeCun\nNew York University\nyann@cs.nyu.edu\nAbstract\nWe present a method for extracting depth information\nfrom a recti\ufb01ed image pair. We train a convolutional neu-\nral network to predict how well two image patches match\nand use it to compute the stereo matching cost. The cost\nis re\ufb01ned by cross-based cost aggregation and semiglobal\nmatching, followed by a left-right consistency check to elim-\ninate errors in the occluded regions. Our stereo method\nachieves an error rate of 2.61 % on the KITTI stereo dataset\nand is currently (August 2014) the top performing method\non this dataset.\n1. Introduction\nConsider the following problem: given two images taken\nfrom cameras at different horizontal positions, the goal is\nto compute the disparity d for each pixel in the left image.\nDisparity refers to the difference in horizontal location of\nan object in the left and right image\u2014an object at position\n(x, y) in the left image will appear at position (x \u2212d, y) in\nthe right image. Knowing the disparity d of an object, we\ncan compute its depth z (i.e. the distance from the object to\nthe camera) by using the following relation:\nz = fB\nd ,\n(1)\nwhere f is the focal length of the camera and B is the dis-\ntance between the camera centers.\nThe described problem is a subproblem of stereo recon-\nstruction, where the goal is to extract 3D shape from one\nor more images. According to the taxonomy of Scharstein\nand Szeliski [14], a typical stereo algorithm consists of four\nsteps: (1) matching cost computation, (2) cost aggregation,\n(3) optimization, and (4) disparity re\ufb01nement. Following\nHirschmuller and Scharstein [5], we refer to steps (1) and\n(2) as computing the matching cost and steps (3) and (4) as\nthe stereo method.\nWe propose training a convolutional neural network [9]\non pairs of small image patches where the true disparity is\nknown (e.g. obtained by LIDAR). The output of the net-\nwork is used to initialize the matching cost between a pair\nof patches. Matching costs are combined between neighbor-\ning pixels with similar image intensities using cross-based\ncost aggregation. Smoothness constraints are enforced by\nsemiglobal matching and a left-right consistency check is\nused to detect and eliminate errors in occluded regions. We\nperform subpixel enhancement and apply a median \ufb01lter\nand a bilateral \ufb01lter to obtain the \ufb01nal disparity map. Fig-\nure 1 depicts the inputs to and the output from our method.\nThe two contributions of this paper are:\n\u2022 We describe how a convolutional neural network can\nbe used to compute the stereo matching cost.\n\u2022 We achieve an error rate of 2.61 % on the KITTI\nstereo dataset, improving on the previous best result\nof 2.83 %.\n2. Related work\nBefore the introduction of large stereo datasets [2, 13],\nrelatively few stereo algorithms used ground-truth informa-\ntion to learn parameters of their models; in this section, we\nreview the ones that did. For a general overview of stereo\nalgorithms see [14].\nKong and Tao [6] used sum of squared distances to com-\npute an initial matching cost. They trained a model to pre-\ndict the probability distribution over three classes: the ini-\ntial disparity is correct, the initial disparity is incorrect due\nto fattening of a foreground object, and the initial disparity\nis incorrect due to other reasons. The predicted probabil-\nities were used to adjust the initial matching cost. Kong\nand Tao [7] later extend their work by combining predic-\ntions obtained by computing normalized cross-correlation\nover different window sizes and centers. Peris et al. [12]\ninitialized the matching cost with AD-Census [11] and used\nmulticlass linear discriminant analysis to learn a mapping\nfrom the computed matching cost to the \ufb01nal disparity.\nGround-truth data was also used to learn parameters of\ngraphical models. Zhang and Seitz [22] used an alterna-\ntive optimization algorithm to estimate optimal values of\nMarkov random \ufb01eld hyperparameters. Scharstein and Pal\n1\narXiv:1409.4326v2  [cs.CV]  20 Oct 2015\nLeft input image\nRight input image\nOutput disparity map\n1.7 m\n90 m\n20 m\nFigure 1. The input is a pair of images from the left and right camera. The two input images differ mostly in horizontal locations of objects.\nNote that objects closer to the camera have larger disparities than objects farther away. The output is a dense disparity map shown on the\nright, with warmer colors representing larger values of disparity (and smaller values of depth).\n[13] constructed a new dataset of 30 stereo pairs and used\nit to learn parameters of a conditional random \ufb01eld.\nLi\nand Huttenlocher [10] presented a conditional random \ufb01eld\nmodel with a non-parametric cost function and used a struc-\ntured support vector machine to learn the model parameters.\nRecent work [3, 15] focused on estimating the con\ufb01-\ndence of the computed matching cost. Haeusler et al. [3]\nused a random forest classi\ufb01er to combine several con\ufb01-\ndence measures. Similarly, Spyropoulos et al. [15] trained\na random forest classi\ufb01er to predict the con\ufb01dence of the\nmatching cost and used the predictions as soft constraints\nin a Markov random \ufb01eld to decrease the error of the stereo\nmethod.\n3. Computing the matching cost\nA typical stereo algorithm begins by computing a match-\ning cost C(p, d) at each position p for all disparities d under\nconsideration. A simple example is the sum of absolute dif-\nferences:\nCAD(p, d) =\nX\nq\u2208Np\n|IL(q) \u2212IR(qd)|,\n(2)\nwhere IL(p) and IR(p) are image intensities at position p\nof the left and right image and Np is the set of locations\nwithin a \ufb01xed rectangular window centered at p. We use\nbold lowercase letters (p, q, and r) to denote pairs of real\nnumbers. Appending a lowercase d has the following mean-\ning: if p = (x, y) then pd = (x \u2212d, y).\nEquation (2) can be interpreted as measuring the cost as-\nsociated with matching a patch from the left image, centered\nat position p, with a patch from the right image, centered at\nposition pd. Since examples of good and bad matches can\nbe obtained from publicly available datasets, e.g. KITTI [2]\nand Middlebury [14], we can attempt to solve the matching\nproblem by a supervised learning approach. Inspired by the\nsuccessful applications of convolutional neural networks to\nvision problems [8], we used them to evaluate how well two\nsmall image patches match.\n3.1. Creating the dataset\nA training example comprises two patches, one from the\nleft and one from the right image:\n< PL\n9\u00d79(p), PR\n9\u00d79(q) >,\n(3)\nwhere PL\n9\u00d79(p) denotes a 9 \u00d7 9 patch from the left image,\ncentered at p = (x, y). For each location where the true\ndisparity d is known, we extract one negative and one posi-\ntive example. A negative example is obtained by setting the\ncenter of the right patch q to\nq = (x \u2212d + oneg, y),\n(4)\nwhere oneg is an offset corrupting the match, chosen ran-\ndomly from the set {\u2212Nhi, . . . , \u2212Nlo, Nlo, . . . , Nhi}. Simi-\nlarly, a positive example is derived by setting\nq = (x \u2212d + opos, y),\n(5)\nwhere\nopos\nis\nchosen\nrandomly\nfrom\nthe\nset\n{\u2212Phi, . . . , Phi}.\nThe reason for including opos, in-\nstead of setting it to zero, has to do with the stereo method\nused later on. In particular, we found that cross-based cost\naggregation performs better when the network assigns low\nmatching costs to good matches as well as near matches.\nNlo, Nhi, Phi, and the size of the image patches n are\nhyperparameters of the method.\n3.2. Network architecture\nThe architecture we used is depicted in Figure 2. The\nnetwork consists of eight layers, L1 through L8.\nThe\n\ufb01rst layer is convolutional, while all other layers are fully-\nconnected. The inputs to the network are two 9 \u00d7 9 gray\nimage patches. The \ufb01rst convolutional layer consists of 32\nkernels of size 5 \u00d7 5 \u00d7 1. Layers L2 and L3 are fully-\nconnected with 200 neurons each. After L3 the two 200 di-\nmensional vectors are concatenated into a 400 dimensional\nvector and passed through four fully-connected layers, L4\n9\n9\n5\n5\n5\n5\n32\n200\n200\n400\n300\n300\n300\n300\n2\n9\n9\n5\n5\n5\n5\n32\n200\n200\nLeft image patch\nRight image patch\nL1:\nL2:\nL3:\nL4:\nL5:\nL6:\nL7:\nL8:\nconcatenate\nFigure 2. The architecture of our convolutional neural network.\nthrough L7, with 300 neurons each. The \ufb01nal layer, L8,\nprojects the output to two real numbers that are fed through\na softmax function, producing a distribution over the two\nclasses (good match and bad match). The weights in L1,\nL2, and L3 of the networks for the left and right image\npatch are tied. Recti\ufb01ed linear units follow each layer, ex-\ncept L8. We did not use pooling in our architecture. The\nnetwork contains almost 600 thousand parameters. The ar-\nchitecture is appropriate for gray images, but can easily be\nextended to handle RGB images by learning 5 \u00d7 5 \u00d7 3, in-\nstead of 5 \u00d7 5 \u00d7 1 \ufb01lters in L1. The best hyperparameters\nof the network (such as the number of layers, the number of\nneurons in each layer, and the size of input patches) will dif-\nfer from one dataset to another. We chose this architecture\nbecause it performed well on the KITTI stereo dataset.\n3.3. Matching cost\nThe matching cost CCNN(p, d) is computed directly from\nthe output of the network:\nCCNN(p, d) = fneg(< PL\n9\u00d79(p), PR\n9\u00d79(pd) >),\n(6)\nwhere fneg(< PL, PR >) is the output of the network for\nthe negative class when run on input patches PL and PR.\nNaively, we would have to perform the forward pass for\neach image location p and each disparity d under consider-\nation. The following three implementation details kept the\nruntime manageable:\n1. The output of layers L1, L2, and L3 need to be com-\nputed only once per location p and need not be recom-\nputed for every disparity d.\n2. The output of L3 can be computed for all loca-\ntions in a single forward pass by feeding the net-\nwork full-resolution images, instead of 9 \u00d7 9 image\npatches. To achieve this, we apply layers L2 and L3\nconvolutionally\u2014layer L2 with \ufb01lters of size 5\u00d75\u00d732\nand layer L3 with \ufb01lters of size 1 \u00d7 1 \u00d7 200, both out-\nputting 200 feature maps.\n3. Similarly, L4 through L8 can be replaced with convo-\nlutional \ufb01lters of size 1 \u00d7 1 in order to compute the\noutput of all locations in a single forward pass. Unfor-\ntunately, we still have to perform the forward pass for\neach disparity under consideration.\n4. Stereo method\nIn order to meaningfully evaluate the matching cost, we\nneed to pair it with a stereo method. The stereo method we\nused was in\ufb02uenced by Mei et al. [11].\n4.1. Cross-based cost aggregation\nInformation from neighboring pixels can be combined\nby averaging the matching cost over a \ufb01xed window. This\napproach fails near depth discontinuities where the assump-\ntion of constant depth within a window is violated.\nWe\nmight prefer a method that adaptively selects the neighbor-\nhood for each pixel so that support is collected only from\npixels with similar disparities. In cross-based cost aggrega-\ntion [21] we build a local neighborhood around each loca-\ntion comprising pixels with similar image intensity values.\nCross-based cost aggregation begins by constructing an\nupright cross at each position. The left arm pl at position p\nextends left as long as the following two conditions hold:\n\u2022 |I(p) \u2212I(pl)| < \u03c4. The absolute difference in image\nintensities at positions p and pl is smaller than \u03c4.\n\u2022 \u2225p \u2212pl\u2225< \u03b7. The horizontal distance (or vertical\ndistance, in case of top and bottom arms) between p\nand pl is less than \u03b7.\nThe right, bottom, and top arms are constructed analo-\ngously. Once the four arms are known, we can de\ufb01ne the\nsupport region U(p) as the union of horizontal arms of all\npositions q laying on p\u2019s vertical arm (see Figure 3). Zhang\np\nq\nright arm\nleft arm\nbottom arm\ntop arm\nhorizontal arms of q\npl\nFigure 3. The support region for position p, is the union of hori-\nzontal arms of all positions q on p\u2019s vertical arm.\net al. [21] suggest that aggregation should consider the sup-\nport regions of both images in a stereo pair. Let U L and U R\ndenote the support regions in the left and right image. We\nde\ufb01ne the combined support region Ud as\nUd(p) = {q|q \u2208U L(p), qd \u2208U R(pd)}.\n(7)\nThe matching cost is averaged over the combined support\nregion:\nC0\nCBCA(p, d) = CCNN(p, d),\n(8)\nCi\nCBCA(p, d) =\n1\n|Ud(p)|\nX\nq\u2208Ud(p)\nCi\u22121\nCBCA(q, d),\n(9)\nwhere i is the iteration number. We repeat the averag-\ning four times; the output of cross-based cost aggregation is\nC4\nCBCA.\n4.2. Semiglobal matching\nWe re\ufb01ne the matching cost by enforcing smoothness\nconstraints on the disparity image. Following Hirschmuller\n[4], we de\ufb01ne an energy function E(D) that depends on the\ndisparity image D:\nE(D) =\nX\np\n\u0012\nC4\nCBCA(p, D(p))\n+\nX\nq\u2208Np\nP1 \u00d7 1{|D(p) \u2212D(q)| = 1}\n+\nX\nq\u2208Np\nP2 \u00d7 1{|D(p) \u2212D(q)| > 1}\n\u0013\n,\n(10)\nwhere 1{\u00b7} denotes the indicator function. The \ufb01rst term\npenalizes disparities D(p) with high matching costs. The\nsecond term adds a penalty P1 when the disparity of neigh-\nboring pixels differ by one. The third term adds a larger\npenalty P2 when the neighboring disparities differ by more\nthan one. Rather than minimizing E(D) in 2D, we per-\nform the minimization in a single direction with dynamic\nprogramming. This solution introduces unwanted streak-\ning effects, since there is no incentive to make the disparity\nimage smooth in the directions we are not optimizing over.\nIn semiglobal matching we minimize the energy E(D) in\nmany directions and average to obtain the \ufb01nal result. Al-\nthough Hirschmuller [4] suggests choosing sixteen direc-\ntion, we only optimized along the two horizontal and the\ntwo vertical directions; adding the diagonal directions did\nnot improve the accuracy of our system.\nTo minimize E(D) in direction r, we de\ufb01ne a matching\ncost Cr(p, d) with the following recurrence relation:\nCr(p, d) = C4\nCBCA(p, d) \u2212min\nk Cr(p \u2212r, k)\n+ min\n\u001a\nCr(p \u2212r, d), Cr(p \u2212r, d \u22121) + P1,\nCr(p \u2212r, d + 1) + P1, min\nk Cr(p \u2212r, k) + P2\n\u001b\n.\n(11)\nThe second term is included to prevent values of Cr(p, d)\nfrom growing too large and does not affect the optimal dis-\nparity map. The parameters P1 and P2 are set according to\nthe image gradient so that jumps in disparity coincide with\nedges in the image. Let D1 = |IL(p) \u2212IL(p \u2212r)| and\nD2 = |IR(pd) \u2212IR(pd \u2212r)|. We set P1 and P2 according\nto the following rules:\nP1 = \u03a01,\nP2 = \u03a02\nif D1 < \u03c4SO, D2 < \u03c4SO,\nP1 = \u03a01/4,\nP2 = \u03a02/4\nif D1 \u2265\u03c4SO, D2 < \u03c4SO,\nP1 = \u03a01/4,\nP2 = \u03a02/4\nif D1 < \u03c4SO, D2 \u2265\u03c4SO,\nP1 = \u03a01/10,\nP2 = \u03a02/10\nif D1 \u2265\u03c4SO, D2 \u2265\u03c4SO;\nwhere \u03a01, \u03a02, and \u03c4SO are hyperparameters. The value\nof P1 is halved when minimizing in the vertical directions.\nThe \ufb01nal cost CSGM(p, d) is computed by taking the average\nacross all four directions:\nCSGM(p, d) = 1\n4\nX\nr\nCr(p, d).\n(12)\nAfter semiglobal matching we repeat cross-based cost ag-\ngregation, as described in the previous section.\n4.3. Computing the disparity image\nThe disparity image D is computed by the winner-take-\nall strategy, i.e. by \ufb01nding the disparity d that minimizes\nC(p, d),\nD(p) = argmin\nd C(p, d).\n(13)\n4.3.1\nInterpolation\nLet DL denote the disparity map obtained by treating the\nleft image as the reference image\u2014this was the case so far,\ni.e. DL(p) = D(p)\u2014and let DR denote the disparity map\nobtained by treating the right image as the reference im-\nage. Both DL and DR contain errors in occluded regions.\nWe attempt to detect these errors by performing a left-right\nconsistency check. We label each position p as either\ncorrect\nif |d \u2212DR(pd)| \u22641 for d = DL(p),\nmismatch\nif |d \u2212DR(pd)| \u22641 for any other d,\nocclusion\notherwise.\nFor positions marked as occlusion, we want the new dispar-\nity value to come from the background. We interpolate by\nmoving left until we \ufb01nd a position labeled correct and use\nits value. For positions marked as mismatch, we \ufb01nd the\nnearest correct pixels in 16 different directions and use the\nmedian of their disparities for interpolation. We refer to the\ninterpolated disparity map as DINT.\n4.3.2\nSubpixel enhancement\nSubpixel enhancement provides an easy way to increase the\nresolution of a stereo algorithm. We \ufb01t a quadratic curve\nthrough the neighboring costs to obtain a new disparity im-\nage:\nDSE(p) = d \u2212\nC+ \u2212C\u2212\n2(C+ \u22122C + C\u2212),\n(14)\nwhere d = DINT(p), C\u2212\n= CSGM(p, d \u22121), C\n=\nCSGM(p, d), and C+ = CSGM(p, d + 1).\n4.3.3\nRe\ufb01nement\nThe size of the disparity image DSE is smaller than the size\nof the original image, due to the bordering effects of convo-\nlution. The disparity image is enlarged to match the size of\nthe input by copying the disparities of the border pixels. We\nproceed by applying a 5 \u00d7 5 median \ufb01lter and the following\nbilateral \ufb01lter:\nDBF(p) =\n1\nW(p)\nX\nq\u2208Np\nDSE(q) \u00b7 g(\u2225p \u2212q\u2225)\n\u00b7 1{|IL(p) \u2212IL(q)| < \u03c4BF},\n(15)\nwhere g(x) is the probability density function of a zero\nmean normal distribution with standard deviation \u03c3 and\nW(p) is the normalizing constant:\nW(p) =\nX\nq\u2208Np\ng(\u2225p\u2212q\u2225)\u00b71{|IL(p)\u2212IL(q)| < \u03c4BF}. (16)\n\u03c4BF and \u03c3 are hyperparameters. DBF is the \ufb01nal output of\nour stereo method.\n5. Experimental results\nWe evaluate our method on the KITTI stereo dataset,\nbecause of its large training set size required to learn the\nweights of the convolutional neural network.\n5.1. KITTI stereo dataset\nThe KITTI stereo dataset [2] is a collection of gray im-\nage pairs taken from two video cameras mounted on the\nroof of a car, roughly 54 centimeters apart. The images\nare recorded while driving in and around the city of Karl-\nsruhe, in sunny and cloudy weather, at daytime. The dataset\ncomprises 194 training and 195 test image pairs at resolu-\ntion 1240 \u00d7 376. Each image pair is recti\ufb01ed, i.e. trans-\nformed in such a way that an object appears on the same\nvertical position in both images.\nA rotating laser scan-\nner, mounted behind the left camera, provides ground truth\ndepth. The true disparities for the test set are withheld and\nan online leaderboard1 is provided where researchers can\nevaluate their method on the test set. Submissions are al-\nlowed only once every three days. The goal of the KITTI\nstereo dataset is to predict the disparity for each pixel on\nthe left image. Error is measured by the percentage of pix-\nels where the true disparity and the predicted disparity differ\nby more than three pixels. Translated into depth, this means\nthat, for example, the error tolerance is \u00b13 centimeters for\nobjects 2 meters from the camera and \u00b180 centimeters for\nobjects 10 meters from the camera.\n5.2. Details of learning\nWe train the network using stochastic gradient descent\nto minimize the cross-entropy loss. The batch size was set\nto 128. We trained for 16 epochs with the learning rate ini-\ntially set to 0.01 and decreased by a factor of 10 on the 12th\nand 15th iteration. We shuf\ufb02e the training examples prior to\nlearning. From the 194 training image pairs we extracted\n45 million examples. Half belonging to the positive class;\nhalf to the negative class. We preprocessed each image by\nsubtracting the mean and dividing by the standard deviation\nof its pixel intensity values. The stereo method is imple-\nmented in CUDA, while the network training is done with\n1http://www.cvlibs.net/datasets/kitti/eval_\nstereo_flow.php?benchmark=stereo\nthe Torch7 environment [1]. The hyperparameters of the\nstereo method were:\nNlo = 4,\n\u03b7 = 4,\n\u03a01 = 1,\n\u03c3 = 5.656,\nNhi = 8,\n\u03c4 = 0.0442,\n\u03a02 = 32,\n\u03c4BF = 5,\nPhi = 1,\n\u03c4SO = 0.0625.\n5.3. Results\nOur method achieves an error rate of 2.61 % on the\nKITTI stereo test set and is currently ranked \ufb01rst on the on-\nline leaderboard. Table 1 compares the error rates of the\nbest performing stereo algorithms on this dataset.\nRank\nMethod\nError\n1\nMC-CNN\nThis paper\n2.61 %\n2\nSPS-StFl\nYamaguchi et al. [20]\n2.83 %\n3\nVC-SF\nVogel et al. [16]\n3.05 %\n4\nCoP\nAnonymous submission\n3.30 %\n5\nSPS-St\nYamaguchi et al. [20]\n3.39 %\n6\nPCBP-SS\nYamaguchi et al. [19]\n3.40 %\n7\nDDS-SS\nAnonymous submission\n3.83 %\n8\nStereoSLIC\nYamaguchi et al. [19]\n3.92 %\n9\nPR-Sf+E\nVogel et al. [17]\n4.02 %\n10\nPCBP\nYamaguchi et al. [18]\n4.04 %\nTable 1. The KITTI stereo leaderboard as it stands in November\n2014.\nA selected set of examples, together with predictions\nfrom our method, are shown in Figure 5.\n5.4. Runtime\nWe measure the runtime of our implementation on a\ncomputer with a Nvidia GeForce GTX Titan GPU. Train-\ning takes 5 hours. Predicting a single image pair takes 100\nseconds. It is evident from Table 2 that the majority of time\nduring prediction is spent in the forward pass of the convo-\nlutional neural network.\nComponent\nRuntime\nConvolutional neural network\n95 s\nSemiglobal matching\n3 s\nCross-based cost aggregation\n2 s\nEverything else\n0.03 s\nTable 2. Time required for prediction of each component.\n5.5. Training set size\nWe would like to know if more training data would lead\nto a better stereo method. To answer this question, we train\nour convolutional neural network on many instances of the\nKITTI stereo dataset while varying the training set size. The\nresults of the experiment are depicted in Figure 4. We ob-\n20\n40\n60\n80\n100\n120\n140\n160\nNumber of training stereo pairs\n3.25 %\n3.3 %\n3.35 %\n3.4 %\n3.45 %\n3.5 %\n3.55 %\n3.6 %\n3.65 %\nError\nFigure 4. The error on the test set as a function of the number of\nstereo pairs in the training set.\nserve an almost linear relationship between the training set\nsize and error on the test set. These results imply that our\nmethod will improve as larger datasets become available in\nthe future.\n6. Conclusion\nOur result on the KITTI stereo dataset seems to suggest\nthat convolutional neural networks are a good \ufb01t for com-\nputing the stereo matching cost. Training on bigger datasets\nwill reduce the error rate even further. Using supervised\nlearning in the stereo method itself could also be bene\ufb01-\ncial. Our method is not yet suitable for real-time applica-\ntions such as robot navigation. Future work will focus on\nimproving the network\u2019s runtime performance.\nReferences\n[1] Collobert, R., Kavukcuoglu, K., and Farabet, C. (2011).\nTorch7: A matlab-like environment for machine learn-\ning.\nIn BigLearn, NIPS Workshop, number EPFL-\nCONF-192376.\n[2] Geiger, A., Lenz, P., Stiller, C., and Urtasun, R. (2013).\nVision meets robotics: The KITTI dataset. International\nJournal of Robotics Research (IJRR).\n[3] Haeusler, R., Nair, R., and Kondermann, D. (2013). En-\nsemble learning for con\ufb01dence measures in stereo vision.\nIn Computer Vision and Pattern Recognition (CVPR),\n2013 IEEE Conference on, pages 305\u2013312. IEEE.\n[4] Hirschmuller,\nH. (2008).\nStereo processing by\nsemiglobal matching and mutual information. Pattern\nAnalysis and Machine Intelligence, IEEE Transactions\non, 30(2):328\u2013341.\n[5] Hirschmuller, H. and Scharstein, D. (2009). Evalua-\ntion of stereo matching costs on images with radiometric\nFigure 5. The left column displays the left input image, while the right column displays the output of our stereo method. Examples are\nsorted by dif\ufb01culty, with easy examples appearing at the top. Some of the dif\ufb01culties include re\ufb02ective surfaces, occlusions, as well as\nregions with many jumps in disparity, e.g. fences and shrubbery. The examples towards the bottom were selected to highlight the \ufb02aws in\nour method and to demonstrate the inherent dif\ufb01culties of stereo matching on real-world images.\ndifferences. Pattern Analysis and Machine Intelligence,\nIEEE Transactions on, 31(9):1582\u20131599.\n[6] Kong, D. and Tao, H. (2004). A method for learning\nmatching errors for stereo computation. In BMVC, pages\n1\u201310.\n[7] Kong, D. and Tao, H. (2006).\nStereo matching via\nlearning multiple experts behaviors.\nIn BMVC, pages\n97\u2013106.\n[8] Krizhevsky, A., Sutskever, I., and Hinton, G. (2012).\nImagenet classi\ufb01cation with deep convolutional neural\nnetworks. In Advances in Neural Information Processing\nSystems 25, pages 1106\u20131114.\n[9] LeCun, Y., Bottou, L., Bengio, Y., and Haffner, P.\n(1998).\nGradient-based learning applied to document\nrecognition.\nProceedings of the IEEE, 86(11):2278\u2013\n2324.\n[10] Li, Y. and Huttenlocher, D. P. (2008). Learning for\nstereo vision using the structured support vector ma-\nchine.\nIn Computer Vision and Pattern Recognition,\n2008. CVPR 2008. IEEE Conference on, pages 1\u20138.\nIEEE.\n[11] Mei, X., Sun, X., Zhou, M., Wang, H., Zhang, X.,\net al. (2011). On building an accurate stereo matching\nsystem on graphics hardware. In Computer Vision Work-\nshops (ICCV Workshops), 2011 IEEE International Con-\nference on, pages 467\u2013474. IEEE.\n[12] Peris, M., Maki, A., Martull, S., Ohkawa, Y., and\nFukui, K. (2012). Towards a simulation driven stereo\nvision system. In Pattern Recognition (ICPR), 2012 21st\nInternational Conference on, pages 1038\u20131042. IEEE.\n[13] Scharstein, D. and Pal, C. (2007).\nLearning condi-\ntional random \ufb01elds for stereo. In Computer Vision and\nPattern Recognition, 2007. CVPR\u201907. IEEE Conference\non, pages 1\u20138. IEEE.\n[14] Scharstein, D. and Szeliski, R. (2002).\nA taxon-\nomy and evaluation of dense two-frame stereo corre-\nspondence algorithms. International journal of computer\nvision, 47(1-3):7\u201342.\n[15] Spyropoulos, A., Komodakis, N., and Mordohai, P.\n(2014). Learning to detect ground control points for im-\nproving the accuracy of stereo matching. In Computer\nVision and Pattern Recognition (CVPR), 2014 IEEE\nConference on, pages 1621\u20131628. IEEE.\n[16] Vogel, C., Roth, S., and Schindler, K. (2014).\nView-consistent 3d scene \ufb02ow estimation over multiple\nframes. In Computer Vision\u2013ECCV 2014, pages 263\u2013\n278. Springer.\n[17] Vogel, C., Schindler, K., and Roth, S. (2013). Piece-\nwise rigid scene \ufb02ow. In Computer Vision (ICCV), 2013\nIEEE International Conference on, pages 1377\u20131384.\nIEEE.\n[18] Yamaguchi, K., Hazan, T., McAllester, D., and Urta-\nsun, R. (2012). Continuous markov random \ufb01elds for ro-\nbust stereo estimation. In Computer Vision\u2013ECCV 2012,\npages 45\u201358. Springer.\n[19] Yamaguchi, K., McAllester, D., and Urtasun, R.\n(2013). Robust monocular epipolar \ufb02ow estimation. In\nComputer Vision and Pattern Recognition (CVPR), 2013\nIEEE Conference on, pages 1862\u20131869. IEEE.\n[20] Yamaguchi, K., McAllester, D., and Urtasun, R.\n(2014). Ef\ufb01cient joint segmentation, occlusion labeling,\nstereo and \ufb02ow estimation. In Computer Vision\u2013ECCV\n2014, pages 756\u2013771. Springer.\n[21] Zhang, K., Lu, J., and Lafruit, G. (2009). Cross-based\nlocal stereo matching using orthogonal integral images.\nCircuits and Systems for Video Technology, IEEE Trans-\nactions on, 19(7):1073\u20131079.\n[22] Zhang, L. and Seitz, S. M. (2007). Estimating opti-\nmal parameters for mrf stereo from a single image pair.\nPattern Analysis and Machine Intelligence, IEEE Trans-\nactions on, 29(2):331\u2013342.\n",
        "sentence": " An alternative view of our work is as a multi-view extension of learning-based stereo correspondence [23, 26, 27] to more than two views. The advent of deep learning suggested that the bottleneck might be the descriptors themselves rather than the distance metric, so it was proposed to learn similarity directly from raw images [27].",
        "context": "the stereo method.\nWe propose training a convolutional neural network [9]\non pairs of small image patches where the true disparity is\nknown (e.g. obtained by LIDAR). The output of the net-\nwork is used to initialize the matching cost between a pair\nof 2.83 %.\n2. Related work\nBefore the introduction of large stereo datasets [2, 13],\nrelatively few stereo algorithms used ground-truth informa-\ntion to learn parameters of their models; in this section, we\nthat convolutional neural networks are a good \ufb01t for com-\nputing the stereo matching cost. Training on bigger datasets\nwill reduce the error rate even further. Using supervised\nlearning in the stereo method itself could also be bene\ufb01-"
    }
]