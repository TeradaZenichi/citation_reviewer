[
    {
        "title": "Uncovering shared structures in multiclass classification",
        "author": [
            "Yonatan Amit",
            "Michael Fink",
            "Nathan Srebro",
            "Shimon Ullman"
        ],
        "venue": "In Proceedings of the 24th international conference on Machine learning,",
        "citeRegEx": "Amit et al\\.,? \\Q2007\\E",
        "shortCiteRegEx": "Amit et al\\.",
        "year": 2007,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "On the low-rank approach for semidefinite programs arising in synchronization and community detection",
        "author": [
            "Afonso S Bandeira",
            "Nicolas Boumal",
            "Vladislav Voroninski"
        ],
        "venue": "arXiv preprint arXiv:1602.04426,",
        "citeRegEx": "Bandeira et al\\.,? \\Q2016\\E",
        "shortCiteRegEx": "Bandeira et al\\.",
        "year": 2016,
        "abstract": "",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "A nonlinear programming algorithm for solving semidefinite programs via low-rank factorization",
        "author": [
            "Samuel Burer",
            "Renato DC Monteiro"
        ],
        "venue": "Mathematical Programming,",
        "citeRegEx": "Burer and Monteiro.,? \\Q2003\\E",
        "shortCiteRegEx": "Burer and Monteiro.",
        "year": 2003,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "Do we need good initialization for low rank matrix recovery",
        "author": [
            "Srinadh Bhojanapalli",
            "Behnam Neyshabur",
            "Nathan Srebro"
        ],
        "venue": "Personal Communication,",
        "citeRegEx": "Bhojanapalli et al\\.,? \\Q2016\\E",
        "shortCiteRegEx": "Bhojanapalli et al\\.",
        "year": 2016,
        "abstract": "",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "Robust principal component analysis",
        "author": [
            "Emmanuel J Cand\u00e8s",
            "Xiaodong Li",
            "Yi Ma",
            "John Wright"
        ],
        "venue": "Journal of the ACM (JACM),",
        "citeRegEx": "Cand\u00e8s et al\\.,? \\Q2011\\E",
        "shortCiteRegEx": "Cand\u00e8s et al\\.",
        "year": 2011,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "Exact matrix completion via convex optimization",
        "author": [
            "Emmanuel J Cand\u00e8s",
            "Benjamin Recht"
        ],
        "venue": "Foundations of Computational mathematics,",
        "citeRegEx": "Cand\u00e8s and Recht.,? \\Q2009\\E",
        "shortCiteRegEx": "Cand\u00e8s and Recht.",
        "year": 2009,
        "abstract": "We consider a problem of considerable practical interest: the recovery of a\ndata matrix from a sampling of its entries. Suppose that we observe m entries\nselected uniformly at random from a matrix M. Can we complete the matrix and\nrecover the entries that we have not seen?\n  We show that one can perfectly recover most low-rank matrices from what\nappears to be an incomplete set of entries. We prove that if the number m of\nsampled entries obeys m >= C n^{1.2} r log n for some positive numerical\nconstant C, then with very high probability, most n by n matrices of rank r can\nbe perfectly recovered by solving a simple convex optimization program. This\nprogram finds the matrix with minimum nuclear norm that fits the data. The\ncondition above assumes that the rank is not too large. However, if one\nreplaces the 1.2 exponent with 1.25, then the result holds for all values of\nthe rank. Similar results hold for arbitrary rectangular matrices as well. Our\nresults are connected with the recent literature on compressed sensing, and\nshow that objects other than signals and images can be perfectly reconstructed\nfrom very limited information.",
        "full_text": "Exact Matrix Completion via Convex Optimization\nEmmanuel J. Cand`es\u2020 and Benjamin Recht\u266f\n\u2020 Applied and Computational Mathematics, Caltech, Pasadena, CA 91125\n\u266fCenter for the Mathematics of Information, Caltech, Pasadena, CA 91125\nMay 2008\nAbstract\nWe consider a problem of considerable practical interest: the recovery of a data matrix from\na sampling of its entries. Suppose that we observe m entries selected uniformly at random from\na matrix M. Can we complete the matrix and recover the entries that we have not seen?\nWe show that one can perfectly recover most low-rank matrices from what appears to be an\nincomplete set of entries. We prove that if the number m of sampled entries obeys\nm \u2265C n1.2r log n\nfor some positive numerical constant C, then with very high probability, most n \u00d7 n matrices\nof rank r can be perfectly recovered by solving a simple convex optimization program. This\nprogram \ufb01nds the matrix with minimum nuclear norm that \ufb01ts the data. The condition above\nassumes that the rank is not too large. However, if one replaces the 1.2 exponent with 1.25,\nthen the result holds for all values of the rank. Similar results hold for arbitrary rectangular\nmatrices as well. Our results are connected with the recent literature on compressed sensing,\nand show that objects other than signals and images can be perfectly reconstructed from very\nlimited information.\nKeywords. Matrix completion, low-rank matrices, convex optimization, duality in optimiza-\ntion, nuclear norm minimization, random matrices, noncommutative Khintchine inequality, decou-\npling, compressed sensing.\n1\nIntroduction\nIn many practical problems of interest, one would like to recover a matrix from a sampling of its\nentries. As a motivating example, consider the task of inferring answers in a partially \ufb01lled out\nsurvey. That is, suppose that questions are being asked to a collection of individuals. Then we\ncan form a matrix where the rows index each individual and the columns index the questions.\nWe collect data to \ufb01ll out this table but unfortunately, many questions are left unanswered. Is it\npossible to make an educated guess about what the missing answers should be? How can one make\nsuch a guess? Formally, we may view this problem as follows. We are interested in recovering a\ndata matrix M with n1 rows and n2 columns but only get to observe a number m of its entries\nwhich is comparably much smaller than n1n2, the total number of entries. Can one recover the\nmatrix M from m of its entries? In general, everyone would agree that this is impossible without\nsome additional information.\n1\narXiv:0805.4471v1  [cs.IT]  29 May 2008\nIn many instances, however, the matrix we wish to recover is known to be structured in the\nsense that it is low-rank or approximately low-rank. (We recall for completeness that a matrix with\nn1 rows and n2 columns has rank r if its rows or columns span an r-dimensional space.) Below are\ntwo examples of practical scenarios where one would like to be able to recover a low-rank matrix\nfrom a sampling of its entries.\n\u2022 The Net\ufb02ix problem. In the area of recommender systems, users submit ratings on a subset\nof entries in a database, and the vendor provides recommendations based on the user\u2019s pref-\nerences [28,32]. Because users only rate a few items, one would like to infer their preference\nfor unrated items.\nA special instance of this problem is the now famous Net\ufb02ix problem [2]. Users (rows of the\ndata matrix) are given the opportunity to rate movies (columns of the data matrix) but users\ntypically rate only very few movies so that there are very few scattered observed entries of\nthis data matrix. Yet one would like to complete this matrix so that the vendor (here Net\ufb02ix)\nmight recommend titles that any particular user is likely to be willing to order. In this case,\nthe data matrix of all user-ratings may be approximately low-rank because it is commonly\nbelieved that only a few factors contribute to an individual\u2019s tastes or preferences.\n\u2022 Triangulation from incomplete data. Suppose we are given partial information about the dis-\ntances between objects and would like to reconstruct the low-dimensional geometry describing\ntheir locations. For example, we may have a network of low-power wirelessly networked sen-\nsors scattered randomly across a region. Suppose each sensor only has the ability to construct\ndistance estimates based on signal strength readings from its nearest fellow sensors. From\nthese noisy distance estimates, we can form a partially observed distance matrix. We can\nthen estimate the true distance matrix whose rank will be equal to two if the sensors are\nlocated in a plane or three if they are located in three dimensional space [24,31]. In this case,\nwe only need to observe a few distances per node to have enough information to reconstruct\nthe positions of the objects.\nThese examples are of course far from exhaustive and there are many other problems which fall in\nthis general category. For instance, we may have some very limited information about a covariance\nmatrix of interest. Yet, this covariance matrix may be low-rank or approximately low-rank because\nthe variables only depend upon a comparably smaller number of factors.\n1.1\nImpediments and solutions\nSuppose for simplicity that we wish to recover a square n \u00d7 n matrix M of rank r.1\nSuch a\nmatrix M can be represented by n2 numbers, but it only has (2n \u2212r)r degrees of freedom. This\nfact can be revealed by counting parameters in the singular value decomposition (the number of\ndegrees of freedom associated with the description of the singular values and of the left and right\nsingular vectors). When the rank is small, this is considerably smaller than n2. For instance, when\nM encodes a 10-dimensional phenomenon, then the number of degrees of freedom is about 20 n\no\ufb00ering a reduction in dimensionality by a factor about equal to n/20. When n is large (e.g. in the\nthousands or millions), the data matrix carries much less information than its ambient dimension\n1We emphasize that there is nothing special about M being square and all of our discussion would apply to\narbitrary rectangular matrices as well. The advantage of focusing on square matrices is a simpli\ufb01ed exposition and\nreduction in the number of parameters of which we need to keep track.\n2\nsuggests. The problem is now whether it is possible to recover this matrix from a sampling of its\nentries without having to probe all the n2 entries, or more generally collect n2 or more measurements\nabout M.\n1.1.1\nWhich matrices?\nIn general, one cannot hope to be able to recover a low-rank matrix from a sample of its entries.\nConsider the rank-1 matrix M equal to\nM = e1e\u2217\nn =\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8f0\n0\n0\n\u00b7 \u00b7 \u00b7\n0\n1\n0\n0\n\u00b7 \u00b7 \u00b7\n0\n0\n...\n...\n...\n...\n...\n0\n0\n\u00b7 \u00b7 \u00b7\n0\n0\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fb,\n(1.1)\nwhere here and throughout, ei is the ith canonical basis vector in Euclidean space (the vector with\nall entries equal to 0 but the ith equal to 1). This matrix has a 1 in the top-right corner and all the\nother entries are 0. Clearly this matrix cannot be recovered from a sampling of its entries unless\nwe pretty much see all the entries. The reason is that for most sampling sets, we would only get to\nsee zeros so that we would have no way of guessing that the matrix is not zero. For instance, if we\nwere to see 90% of the entries selected at random, then 10% of the time we would only get to see\nzeroes.\nIt is therefore impossible to recover all low-rank matrices from a set of sampled entries but\ncan one recover most of them? To investigate this issue, we introduce a simple model of low-rank\nmatrices. Consider the singular value decomposition (SVD) of a matrix M\nM =\nr\nX\nk=1\n\u03c3kukv\u2217\nk,\n(1.2)\nwhere the uk\u2019s and vk\u2019s are the left and right singular vectors, and the \u03c3k\u2019s are the singular values\n(the roots of the eigenvalues of M \u2217M). Then we could think of a generic low-rank matrix as follows:\nthe family {uk}1\u2264k\u2264r is selected uniformly at random among all families of r orthonormal vectors,\nand similarly for the the family {vk}1\u2264k\u2264r. The two families may or may not be independent of\neach other. We make no assumptions about the singular values \u03c3k. In the sequel, we will refer to\nthis model as the random orthogonal model. This model is convenient in the sense that it is both\nvery concrete and simple, and useful in the sense that it will help us \ufb01x the main ideas. In the\nsequel, however, we will consider far more general models. The question for now is whether or not\none can recover such a generic matrix from a sampling of its entries.\n1.1.2\nWhich sampling sets?\nClearly, one cannot hope to reconstruct any low-rank matrix M\u2014even of rank 1\u2014if the sampling\nset avoids any column or row of M. Suppose that M is of rank 1 and of the form xy\u2217, x, y \u2208Rn\nso that the (i, j)th entry is given by\nMij = xiyj.\nThen if we do not have samples from the \ufb01rst row for example, one could never guess the value of\nthe \ufb01rst component x1, by any method whatsoever; no information about x1 is observed. There is\n3\nof course nothing special about the \ufb01rst row and this argument extends to any row or column. To\nhave any hope of recovering an unknown matrix, one needs at least one observation per row and\none observation per column.\nWe have just seen that if the sampling is adversarial, e.g. one observes all of the entries of M\nbut those in the \ufb01rst row, then one would not even be able to recover matrices of rank 1. But what\nhappens for most sampling sets? Can one recover a low-rank matrix from almost all sampling sets of\ncardinality m? Formally, suppose that the set \u2126of locations corresponding to the observed entries\n((i, j) \u2208\u2126if Mij is observed) is a set of cardinality m sampled uniformly at random. Then can\none recover a generic low-rank matrix M, perhaps with very large probability, from the knowledge\nof the value of its entries in the set \u2126?\n1.1.3\nWhich algorithm?\nIf the number of measurements is su\ufb03ciently large, and if the entries are su\ufb03ciently uniformly\ndistributed as above, one might hope that there is only one low-rank matrix with these entries. If\nthis were true, one would want to recover the data matrix by solving the optimization problem\nminimize\nrank(X)\nsubject to\nXij = Mij\n(i, j) \u2208\u2126,\n(1.3)\nwhere X is the decision variable and rank(X) is equal to the rank of the matrix X. The program\n(1.3) is a common sense approach which simply seeks the simplest explanation \ufb01tting the observed\ndata.\nIf there were only one low-rank object \ufb01tting the data, this would recover M.\nThis is\nunfortunately of little practical use because this optimization problem is not only NP-hard, but all\nknown algorithms which provide exact solutions require time doubly exponential in the dimension\nn of the matrix in both theory and practice [14].\nIf a matrix has rank r, then it has exactly r nonzero singular values so that the rank function\nin (1.3) is simply the number of nonvanishing singular values.\nIn this paper, we consider an\nalternative which minimizes the sum of the singular values over the constraint set. This sum is\ncalled the nuclear norm,\n\u2225X\u2225\u2217=\nn\nX\nk=1\n\u03c3k(X)\n(1.4)\nwhere, here and below, \u03c3k(X) denotes the kth largest singular value of X. The heuristic optimiza-\ntion is then given by\nminimize\n\u2225X\u2225\u2217\nsubject to\nXij = Mij\n(i, j) \u2208\u2126.\n(1.5)\nWhereas the rank function counts the number of nonvanishing singular values, the nuclear norm\nsums their amplitude and in some sense, is to the rank functional what the convex \u21131 norm is to\nthe counting \u21130 norm in the area of sparse signal recovery. The main point here is that the nuclear\nnorm is a convex function and, as we will discuss in Section 1.4 can be optimized e\ufb03ciently via\nsemide\ufb01nite programming.\n1.1.4\nA \ufb01rst typical result\nOur \ufb01rst result shows that, perhaps unexpectedly, this heuristic optimization recovers a generic M\nwhen the number of randomly sampled entries is large enough. We will prove the following:\n4\nTheorem 1.1 Let M be an n1 \u00d7 n2 matrix of rank r sampled from the random orthogonal model,\nand put n = max(n1, n2). Suppose we observe m entries of M with locations sampled uniformly at\nrandom. Then there are numerical constants C and c such that if\nm \u2265C n5/4r log n ,\n(1.6)\nthe minimizer to the problem (1.5) is unique and equal to M with probability at least 1\u2212cn\u22123; that\nis to say, the semide\ufb01nite program (1.5) recovers all the entries of M with no error. In addition,\nif r \u2264n1/5, then the recovery is exact with probability at least 1 \u2212cn\u22123 provided that\nm \u2265C n6/5r log n .\n(1.7)\nThe theorem states that a surprisingly small number of entries are su\ufb03cient to complete a generic\nlow-rank matrix. For small values of the rank, e.g. when r = O(1) or r = O(log n), one only needs\nto see on the order of n6/5 entries (ignoring logarithmic factors) which is considerably smaller than\nn2\u2014the total number of entries of a squared matrix. The real feat, however, is that the recovery\nalgorithm is tractable and very concrete. Hence the contribution is twofold:\n\u2022 Under the hypotheses of Theorem 1.1, there is a unique low-rank matrix which is consistent\nwith the observed entries.\n\u2022 Further, this matrix can be recovered by the convex optimization (1.5). In other words, for\nmost problems, the nuclear norm relaxation is formally equivalent to the combinatorially hard\nrank minimization problem (1.3).\nTheorem 1.1 is in fact a special instance of a far more general theorem that covers a much larger\nset of matrices M. We describe this general class of matrices and precise recovery conditions in\nthe next section.\n1.2\nMain results\nAs seen in our \ufb01rst example (1.1), it is impossible to recover a matrix which is equal to zero in\nnearly all of its entries unless we see all the entries of the matrix. To recover a low-rank matrix,\nthis matrix cannot be in the null space of the sampling operator giving the values of a subset of the\nentries. Now it is easy to see that if the singular vectors of a matrix M are highly concentrated,\nthen M could very well be in the null-space of the sampling operator. For instance consider the\nrank-2 symmetric matrix M given by\nM =\n2\nX\nk=1\n\u03c3kuku\u2217\nk,\nu1\n= (e1 + e2)/\n\u221a\n2,\nu2\n= (e1 \u2212e2)/\n\u221a\n2,\nwhere the singular values are arbitrary. Then this matrix vanishes everywhere except in the top-left\n2 \u00d7 2 corner and one would basically need to see all the entries of M to be able to recover this\nmatrix exactly by any method whatsoever. There is an endless list of examples of this sort. Hence,\nwe arrive at the notion that, somehow, the singular vectors need to be su\ufb03ciently spread\u2014that is,\nuncorrelated with the standard basis\u2014in order to minimize the number of observations needed to\nrecover a low-rank matrix.2 This motivates the following de\ufb01nition.\n2Both the left and right singular vectors need to be uncorrelated with the standard basis. Indeed, the matrix e1v\u2217\nhas its \ufb01rst row equal to v and all the others equal to zero. Clearly, this rank-1 matrix cannot be recovered unless\nwe basically see all of its entries.\n5\nDe\ufb01nition 1.2 Let U be a subspace of Rn of dimension r and PU be the orthogonal projection\nonto U. Then the coherence of U (vis-`a-vis the standard basis (ei)) is de\ufb01ned to be\n\u00b5(U) \u2261n\nr max\n1\u2264i\u2264n \u2225PUei\u22252.\n(1.8)\nNote that for any subspace, the smallest \u00b5(U) can be is 1, achieved, for example, if U is spanned\nby vectors whose entries all have magnitude 1/\u221an. The largest possible value for \u00b5(U) is n/r\nwhich would correspond to any subspace that contains a standard basis element.\nWe shall be\nprimarily interested in subspace with low coherence as matrices whose column and row spaces have\nlow coherence cannot really be in the null space of the sampling operator. For instance, we will see\nthat the random subspaces discussed above have nearly minimal coherence.\nTo state our main result, we introduce two assumptions about an n1 \u00d7 n2 matrix M whose\nSVD is given by M = P\n1\u2264k\u2264r \u03c3kukv\u2217\nk and with column and row spaces denoted by U and V\nrespectively.\nA0 The coherences obey max(\u00b5(U), \u00b5(V )) \u2264\u00b50 for some positive \u00b50.\nA1 The n1 \u00d7 n2 matrix P\n1\u2264k\u2264r ukv\u2217\nk has a maximum entry bounded by \u00b51\np\nr/(n1n2) in absolute\nvalue for some positive \u00b51.\nThe \u00b5\u2019s above may depend on r and n1, n2. Moreover, note that A1 always holds with \u00b51 = \u00b50\n\u221ar\nsince the (i, j)th entry of the matrix P\n1\u2264k\u2264r ukv\u2217\nk is given by P\n1\u2264k\u2264r uikvjk and by the Cauchy-\nSchwarz inequality,\n\f\f\f\f\f\f\nX\n1\u2264k\u2264r\nuikvjk\n\f\f\f\f\f\f\n\u2264\ns X\n1\u2264k\u2264r\n|uik|2\ns X\n1\u2264k\u2264r\n|vjk|2 \u2264\n\u00b50r\n\u221an1n2\n.\nHence, for su\ufb03ciently small ranks, \u00b51 is comparable to \u00b50. As we will see in Section 2, for larger\nranks, both subspaces selected from the uniform distribution and spaces constructed as the span\nof singular vectors with bounded entries are not only incoherent with the standard basis, but also\nobey A1 with high probability for values of \u00b51 at most logarithmic in n1 and/or n2. Below we will\nassume that \u00b51 is greater than or equal to 1.\nWe are in the position to state our main result: if a matrix has row and column spaces that are\nincoherent with the standard basis, then nuclear norm minimization can recover this matrix from\na random sampling of a small number of entries.\nTheorem 1.3 Let M be an n1\u00d7n2 matrix of rank r obeying A0 and A1 and put n = max(n1, n2).\nSuppose we observe m entries of M with locations sampled uniformly at random. Then there exist\nconstants C, c such that if\nm \u2265C max(\u00b52\n1, \u00b51/2\n0\n\u00b51, \u00b50n1/4) nr(\u03b2 log n)\n(1.9)\nfor some \u03b2 > 2, then the minimizer to the problem (1.5) is unique and equal to M with probability\nat least 1 \u2212cn\u2212\u03b2. For r \u2264\u00b5\u22121\n0 n1/5 this estimate can be improved to\nm \u2265C \u00b50 n6/5r(\u03b2 log n)\n(1.10)\nwith the same probability of success.\n6\nTheorem 1.3 asserts that if the coherence is low, few samples are required to recover M. For\nexample, if \u00b50 = O(1) and the rank is not too large, then the recovery is exact with large probability\nprovided that\nm \u2265C n6/5r log n .\n(1.11)\nWe give two illustrative examples of matrices with incoherent column and row spaces. This list is\nby no means exhaustive.\n1. The \ufb01rst example is the random orthogonal model. For values of the rank r greater than\nlog n, \u00b5(U) and \u00b5(V ) are O(1), \u00b51 = O(log n) both with very large probability. Hence, the\nrecovery is exact provided that m obeys (1.6) or (1.7). Specializing Theorem 1.3 to these\nvalues of the parameters gives Theorem 1.1. Hence, Theorem 1.1 is a special case of our\ngeneral recovery result.\n2. The second example is more general and, in a nutshell, simply requires that the components\nof the singular vectors of M are small. Assume that the uj and vj\u2019s obey\nmax\nij\n|\u27e8ei, uj\u27e9|2 \u2264\u00b5B/n,\nmax\nij\n|\u27e8ei, vj\u27e9|2 \u2264\u00b5B/n,\n(1.12)\nfor some value of \u00b5B = O(1). Then the maximum coherence is at most \u00b5B since \u00b5(U) \u2264\u00b5B\nand \u00b5(V ) \u2264\u00b5B. Further, we will see in Section 2 that A1 holds most of the time with\n\u00b51 = O(\u221alog n). Thus, for matrices with singular vectors obeying (1.12), the recovery is\nexact provided that m obeys (1.11) for values of the rank not exceeding \u00b5\u22121\nB n1/5.\n1.3\nExtensions\nOur main result (Theorem 1.3) extends to a variety of other low-rank matrix completion problems\nbeyond the sampling of entries. Indeed, suppose we have two orthonormal bases f1, . . . , fn and\ng1, . . . , gn of Rn, and that we are interested in solving the rank minimization problem\nminimize\nrank(X)\nsubject to\nf \u2217\ni Xgj = f \u2217\ni Mgj,\n(i, j) \u2208\u2126, .\n(1.13)\nThis comes up in a number of applications. As a motivating example, there has been a great deal of\ninterest in the machine learning community in developing specialized algorithms for the multiclass\nand multitask learning problems (see, e.g., [1, 3, 5]). In multiclass learning, the goal is to build\nmultiple classi\ufb01ers with the same training data to distinguish between more than two categories.\nFor example, in face recognition, one might want to classify whether an image patch corresponds\nto an eye, nose, or mouth. In multitask learning, we have a large set of data, but have a variety of\ndi\ufb00erent classi\ufb01cation tasks, and, for each task, only partial subsets of the data are relevant. For\ninstance, in activity recognition, we may have acquired sets of observations of multiple subjects\nand want to determine if each observed person is walking or running. However, a di\ufb00erent classi\ufb01er\nis to be learned for each individual, and it is not clear how having access to the full collection\nof observations can improve classi\ufb01cation performance. Multitask learning aims precisely to take\nadvantage of the access to the full database to improve performance on the individual tasks.\nIn the abstract formulation of this problem for linear classi\ufb01ers, we have K classes to distin-\nguish and are given training examples f1, . . . , fn. For each example, we are given partial labeling\ninformation about which classes it belongs or does not belong to. That is, for each example fj\n7\nand class k, we may either be told that fj belongs to class k, be told fj does not belong to class\nk, or provided no information about the membership of fj to class k. For each class 1 \u2264k \u2264K,\nwe would like to produce a linear function wk such that w\u2217\nkfi > 0 if fi belongs to class k and\nw\u2217\nkfi < 0 otherwise. Formally, we can search for the vector wk that satis\ufb01es the equality con-\nstraints w\u2217\nkfi = yik where yik = 1 if we are told that fi belongs to class k, yik = \u22121 if we are\ntold that fi does not belong to class k, and yik unconstrained if we are not provided information.\nA common hypothesis in the multitask setting is that the wk corresponding to each of the classes\ntogether span a very low dimensional subspace with dimension signi\ufb01cantly smaller than K [1,3,5].\nThat is, the basic assumption is that\nW = [w1, . . . , wK]\nis low-rank. Hence, the multiclass learning problem can be cast as (1.13) with observations of the\nform f \u2217\ni W ej.\nTo see that our theorem provides conditions under which (1.13) can be solved via nuclear norm\nminimization, note that there exist unitary transformations F and G such that ej = F fj and\nej = Ggj for each j = 1, . . . , n. Hence,\nf \u2217\ni Xgj = e\u2217\ni (F XG\u2217)ej.\nThen if the conditions of Theorem 1.3 hold for the matrix F XG\u2217, it is immediate that nuclear\nnorm minimization \ufb01nds the unique optimal solution of (1.13) when we are provided a large enough\nrandom collection of the inner products f \u2217\ni Mgj. In other words, all that is needed is that the\ncolumn and row spaces of M be respectively incoherent with the basis (fi) and (gi).\nFrom this perspective, we additionally remark that our results likely extend to the case where\none observes a small number of arbitrary linear functionals of a hidden matrix M. Set N = n2\nand A1, . . . , AN be an orthonormal basis for the linear space of n \u00d7 n matrices with the usual\ninner product \u27e8X, Y \u27e9= trace(X\u2217Y ). Then we expect our results should also apply to the rank\nminimization problem\nminimize\nrank(X)\nsubject to\n\u27e8Ak, X\u27e9= \u27e8Ak, M\u27e9\nk \u2208\u2126,\n(1.14)\nwhere \u2126\u2282{1, . . . , N} is selected uniformly at random. In fact, (1.14) is (1.3) when the orthobasis\nis the canonical basis (eie\u2217\nj)1\u2264i,j\u2264n. Here, those low-rank matrices which have small inner product\nwith all the basis elements Ak may be recoverable by nuclear norm minimization. To avoid unnec-\nessary confusion and notational clutter, we leave this general low-rank recovery problem for future\nwork.\n1.4\nConnections, alternatives and prior art\nNuclear norm minimization is a recent heuristic introduced by Fazel in [18], and is an extension of\nthe trace heuristic often used by the control community, see e.g. [6,26]. Indeed, when the matrix\nvariable is symmetric and positive semide\ufb01nite, the nuclear norm of X is the sum of the (nonneg-\native) eigenvalues and thus equal to the trace of X. Hence, for positive semide\ufb01nite unknowns,\n(1.5) would simply minimize the trace over the constraint set:\nminimize\ntrace(X)\nsubject to\nXij = Mij\n(i, j) \u2208\u2126\nX \u2ab00\n.\n8\nThis is a semide\ufb01nite program. Even for the general matrix M which may not be positive de\ufb01nite or\neven symmetric, the nuclear norm heuristic can be formulated in terms of semide\ufb01nite programming\nas, for instance, the program (1.5) is equivalent to\nminimize\ntrace(W1) + trace(W2)\nsubject to\nXij = Mij\n(i, j) \u2208\u2126\n\u0014W1\nX\nX\u2217\nW2\n\u0015\n\u2ab00\nwith optimization variables X, W1 and W2, (see, e.g., [18,35]). There are many e\ufb03cient algorithms\nand high-quality software available for solving these types of problems.\nOur work is inspired by results in the emerging \ufb01eld of compressive sampling or compressed\nsensing, a new paradigm for acquiring information about objects of interest from what appears to\nbe a highly incomplete set of measurements [11, 13, 17]. In practice, this means for example that\nhigh-resolution imaging is possible with fewer sensors, or that one can speed up signal acquisition\ntime in biomedical applications by orders of magnitude, simply by taking far fewer specially coded\nsamples. Mathematically speaking, we wish to reconstruct a signal x \u2208Rn from a small number\nmeasurements y = \u03a6x, y \u2208Rm, and m is much smaller than n; i.e. we have far fewer equations\nthan unknowns. In general, one cannot hope to reconstruct x but assume now that the object we\nwish to recover is known to be structured in the sense that it is sparse (or approximately sparse).\nThis means that the unknown object depends upon a smaller number of unknown parameters.\nThen it has been shown that \u21131 minimization allows recovery of sparse signals from remarkably few\nmeasurements: supposing \u03a6 is chosen randomly from a suitable distribution, then with very high\nprobability, all sparse signals with about k nonzero entries can be recovered from on the order of\nk log n measurements. For instance, if x is k-sparse in the Fourier domain, i.e. x is a superposition\nof k sinusoids, then it can be perfectly recovered with high probability\u2014by \u21131 minimization\u2014from\nthe knowledge of about k log n of its entries sampled uniformly at random [11].\nFrom this viewpoint, the results in this paper greatly extend the theory of compressed sensing\nby showing that other types of interesting objects or structures, beyond sparse signals and images,\ncan be recovered from a limited set of measurements. Moreover, the techniques for proving our\nmain results build upon ideas from the compressed sensing literature together with probabilistic\ntools such as the powerful techniques of Bourgain and of Rudelson for bounding norms of operators\nbetween Banach spaces.\nOur notion of incoherence generalizes the concept of the same name in compressive sampling.\nNotably, in [10], the authors introduce the notion of the incoherence of a unitary transformation.\nLetting U be an n \u00d7 n unitary matrix, the coherence of U is given by\n\u00b5(U) = n max\nj,k |Ujk|2.\nThis quantity ranges in values from 1 for a unitary transformation whose entries all have the same\nmagnitude to n for the identity matrix. Using this notion, [10] showed that with high probability,\na k-sparse signal could be recovered via linear programming from the observation of the inner\nproduct of the signal with m = \u2126(\u00b5(U)k log n) randomly selected columns of the matrix U.\nThis result provided a generalization of the celebrated results about partial Fourier observations\ndescribed in [11], a special case where \u00b5(U) = 1. This paper generalizes the notion of incoherence\nto problems beyond the setting of sparse signal recovery.\n9\nIn [27], the authors studied the nuclear norm heuristic applied to a related problem where\npartial information about a matrix M is available from m equations of the form\n\u27e8A(k), M\u27e9=\nX\nij\nA(k)\nij Mij = bk,\nk = 1, . . . , m,\n(1.15)\nwhere for each k, {A(k)\nij }ij is an i.i.d. sequence of Gaussian or Bernoulli random variables and\nthe sequences {A(k)} are also independent from each other (the sequences {A(k)} and {bk} are\navailable to the analyst).\nBuilding on the concept of restricted isometry introduced in [12] in\nthe context of sparse signal recovery, [27] establishes the \ufb01rst su\ufb03cient conditions for which the\nnuclear norm heuristic returns the minimum rank element in the constraint set. They prove that\nthe heuristic succeeds with large probability whenever the number m of available measurements is\ngreater than a constant times 2nr log n for n \u00d7 n matrices. Although this is an interesting result, a\nserious impediment to this approach is that one needs to essentially measure random projections of\nthe unknown data matrix\u2014a situation which unfortunately does not commonly arise in practice.\nFurther, the measurements in (1.15) give some information about all the entries of M whereas\nin our problem, information about most of the entries is simply not available. In particular, the\nresults and techniques introduced in [27] do not begin to address the matrix completion problem of\ninterest to us in this paper. As a consequence, our methods are completely di\ufb00erent; for example,\nthey do not rely on any notions of restricted isometry. Instead, as we discuss below, we prove\nthe existence of a Lagrange multiplier for the optimization (1.5) that certi\ufb01es the unique optimal\nsolution is precisely the matrix that we wish to recover.\nFinally, we would like to brie\ufb02y discuss the possibility of other recovery algorithms when the\nsampling happens to be chosen in a very special fashion. For example, suppose that M is generic\nand that we precisely observe every entry in the \ufb01rst r rows and columns of the matrix. Write M\nin block form as\nM =\n\u0014 M11\nM12\nM21\nM22\n\u0015\nwith M11 an r \u00d7 r matrix. In the special case that M11 is invertible and M has rank r, then it is\neasy to verify that M22 = M21M \u22121\n11 M12. One can prove this identity by forming the SVD of M,\nfor example. That is, if M is generic, and the upper r \u00d7 r block is invertible, and we observe every\nentry in the \ufb01rst r rows and columns, we can recover M. This result immediately generalizes to the\ncase where one observes precisely r rows and r columns and the r \u00d7 r matrix at the intersection of\nthe observed rows and columns is invertible. However, this scheme has many practical drawbacks\nthat stand in the way of a generalization to a completion algorithm from a general set of entries.\nFirst, if we miss any entry in these rows or columns, we cannot recover M, nor can we leverage\nany information provided by entries of M22. Second, if the matrix has rank less than r, and we\nobserve r rows and columns, a combinatorial search to \ufb01nd the collection that has an invertible\nsquare sub-block is required. Moreover, because of the matrix inversion, the algorithm is rather\nfragile to noise in the entries.\n1.5\nNotations and organization of the paper\nThe paper is organized as follows. We \ufb01rst argue in Section 2 that the random orthogonal model\nand, more generally, matrices with incoherent column and row spaces obey the assumptions of the\ngeneral Theorem 1.3. To prove Theorem 1.3, we \ufb01rst establish su\ufb03cient conditions which guarantee\n10\nthat the true low-rank matrix M is the unique solution to (1.5) in Section 3. One of these conditions\nis the existence of a dual vector obeying two crucial properties. Section 4 constructs such a dual\nvector and provides the overall architecture of the proof which shows that, indeed, this vector obeys\nthe desired properties provided that the number of measurements is su\ufb03ciently large. Surprisingly,\nas explored in Section 5, the existence of a dual vector certifying that M is unique is related to\nsome problems in random graph theory including \u201cthe coupon collector\u2019s problem.\u201d Following this\ndiscussion, we prove our main result via several intermediate results which are all proven in Section\n6. Section 7 introduces numerical experiments showing that matrix completion based on nuclear\nnorm minimization works well in practice. Section 8 closes the paper with a short summary of\nour \ufb01ndings, a discussion of important extensions and improvements. In particular, we will discuss\npossible ways of improving the 1.2 exponent in (1.10) so that it gets closer to 1.\nFinally, the\nAppendix provides proofs of auxiliary lemmas supporting our main argument.\nBefore continuing, we provide here a brief summary of the notations used throughout the\npaper. Matrices are bold capital, vectors are bold lowercase and scalars or entries are not bold. For\ninstance, X is a matrix and Xij its (i, j)th entry. Likewise x is a vector and xi its ith component.\nWhen we have a collection of vectors uk \u2208Rn for 1 \u2264k \u2264d, we will denote by uik the ith\ncomponent of the vector uk and [u1, . . . , ud] will denote the n \u00d7 d matrix whose kth column is uk.\nA variety of norms on matrices will be discussed. The spectral norm of a matrix is denoted\nby \u2225X\u2225. The Euclidean inner product between two matrices is \u27e8X, Y \u27e9= trace(X\u2217Y ), and the\ncorresponding Euclidean norm, called the Frobenius or Hilbert-Schmidt norm, is denoted \u2225X\u2225F .\nThat is, \u2225X\u2225F = \u27e8X, X\u27e91/2. The nuclear norm of a matrix X is \u2225X\u2225\u2217. The maximum entry of\nX (in absolute value) is denoted by \u2225X\u2225\u221e\u2261maxij |Xij|. For vectors, we will only consider the\nusual Euclidean \u21132 norm which we simply write as \u2225x\u2225.\nFurther, we will also manipulate linear transformation which acts on matrices and will use\ncaligraphic letters for these operators as in A(X).\nIn particular, the identity operator will be\ndenoted by I. The only norm we will consider for these operators is their spectral norm (the top\nsingular value) denoted by \u2225A\u2225= supX:\u2225X\u2225F \u22641 \u2225A(X)\u2225F .\nFinally, we adopt the convention that C denotes a numerical constant independent of the matrix\ndimensions, rank, and number of measurements, whose value may change from line to line. Certain\nspecial constants with precise numerical values will be ornamented with subscripts (e.g., CR). Any\nexceptions to this notational scheme will be noted in the text.\n2\nWhich matrices are incoherent?\nIn this section we restrict our attention to square n \u00d7 n matrices, but the extension to rectangular\nn1 \u00d7 n2 matrices immediately follows by setting n = max(n1, n2).\n2.1\nIncoherent bases span incoherent subspaces\nAlmost all n \u00d7 n matrices M with singular vectors {uk}1\u2264k\u2264r and {vk}1\u2264k\u2264r obeying the size\nproperty (1.12) also satisfy the assumptions A0 and A1 with \u00b50 = \u00b5B, \u00b51 = C\u00b5B\n\u221alog n for some\npositive constant C. As mentioned above, A0 holds automatically, but, observe that A1 would not\nhold with a small value of \u00b51 if two rows of the matrices [u1, . . . , ur] and [v1, . . . , vr] are identical\n11\nwith all entries of magnitude\np\n\u00b5B/n since it is not hard to see that in this case\n\u2225\nX\nk\nukv\u2217\nk\u2225\u221e= \u00b5B r/n.\nCertainly, this example is constructed in a very special way, and should occur infrequently. We\nnow show that it is generically unlikely.\nConsider the matrix\nr\nX\nk=1\n\u03f5kukv\u2217\nk,\n(2.1)\nwhere {\u03f5k}1\u2264k\u2264r is an arbitrary sign sequence. For almost all choices of sign sequences, A1 is\nsatis\ufb01ed with \u00b51 = O(\u00b5B\n\u221alog n). Indeed, if one selects the signs uniformly at random, then for\neach \u03b2 > 0,\nP(\u2225\nr\nX\nk=1\n\u03f5kukvk\u2225\u221e\u2265\u00b5B\np\n8\u03b2r log n/n) \u2264(2n2) n\u2212\u03b2.\n(2.2)\nThis is of interest because suppose the low-rank matrix we wish to recover is of the form\nM =\nr\nX\nk=1\n\u03bbkukv\u2217\nk\n(2.3)\nwith scalars \u03bbk. Since the vectors {uk} and {vk} are orthogonal, the singular values of M are\ngiven by |\u03bbk| and the singular vectors are given by sgn(\u03bbk)uk and vk for k = 1, . . . , r. Hence, in\nthis model A1 concerns the maximum entry of the matrix given by (2.1) with \u03f5k = sgn(\u03bbk). That\nis to say, for most sign patterns, the matrix of interest obeys an appropriate size condition. We\nemphasize here that the only thing that we assumed about the uk\u2019s and vk\u2019s was that they had\nsmall entries. In particular, they could be equal to each other as would be the case for a symmetric\nmatrix.\nThe claim (2.2) is a simple application of Hoe\ufb00ding\u2019s inequality. The (i, j)th entry of (2.1) is\ngiven by\nZij =\nX\n1\u2264k\u2264r\n\u03f5kuikvjk,\nand is a sum of r zero-mean independent random variables, each bounded by \u00b5B/n. Therefore,\nP(|Zij| \u2265\u03bb\u00b5B\n\u221ar/n) \u22642e\u2212\u03bb2/8.\nSetting \u03bb proportional to \u221alog n and applying the union bound gives the claim.\nTo summarize, we say that M is sampled from the incoherent basis model if it is of the form\nM =\nr\nX\nk=1\n\u03f5k\u03c3kukv\u2217\nk;\n(2.4)\n{\u03f5k}1\u2264k\u2264r is a random sign sequence, and {uk}1\u2264k\u2264r and {vk}1\u2264k\u2264r have maximum entries of size\nat most\np\n\u00b5B/n.\nLemma 2.1 There exist numerical constants c and C such that for any \u03b2 > 0, matrices from the\nincoherent basis model obey the assumption A1 with \u00b51 \u2264C\u00b5B\np\n(\u03b2 + 2) log n with probability at\nleast 1 \u2212cn\u2212\u03b2.\n12\n2.2\nRandom subspaces span incoherent subspaces\nIn this section, we prove that the random orthogonal model obeys the two assumptions A0 and\nA1 (with appropriate values for the \u00b5\u2019s) with large probability.\nLemma 2.2 Set \u00afr = max(r, log n).\nThen there exist constants C and c such that the random\northogonal model obeys:3\n1. maxi \u2225PUei\u22252 \u2264C \u00afr/n,\n2. \u2225P\n1\u2264k\u2264r ukv\u2217\nk\u2225\u221e\u2264C log n \u221a\u00afr/n.\nwith probability 1 \u2212cn\u22123 log n.\nWe note that an argument similar to the following proof would give that if C of the form K\u03b2 where\nK is a \ufb01xed numerical constant, we can achieve a probability at least 1 \u2212cn\u2212\u03b2 provided that n is\nsu\ufb03ciently large. To establish these facts, we make use of the standard result below [21].\nLemma 2.3 Let Yd be distributed as a chi-squared random variable with d degrees of freedom. Then\nfor each t > 0\nP(Yd \u2212d \u2265t\n\u221a\n2d + t2) \u2264e\u2212t2/2\nand\nP(Yd \u2212d \u2264\u2212t\n\u221a\n2d) \u2264e\u2212t2/2.\n(2.5)\nWe will use (2.5) as follows: for each \u03f5 \u2208(0, 1) we have\nP(Yd \u2265d (1 \u2212\u03f5)\u22121) \u2264e\u2212\u03f52d/4\nand\nP(Yd \u2264d (1 \u2212\u03f5)) \u2264e\u2212\u03f52d/4.\n(2.6)\nWe begin with the second assertion of Lemma 2.2 since it will imply the \ufb01rst as well. Observe\nthat it follows from\n\u2225PUei\u22252 =\nX\n1\u2264k\u2264r\nu2\nik,\n(2.7)\nthat Zr \u2261\u2225PUei\u22252 (a is \ufb01xed) is the squared Euclidean length of the \ufb01rst r components of a unit\nvector uniformly distributed on the unit sphere in n dimensions. Now suppose that x1, x2, . . . , xn\nare i.i.d. N(0, 1). Then the distribution of a unit vector uniformly distributed on the sphere is\nthat of x/\u2225x\u2225and, therefore, the law of Zr is that of Yr/Yn, where Yr = P\nk\u2264r x2\nk. Fix \u03f5 > 0 and\nconsider the event An,\u03f5 = {Yn/n \u22651 \u2212\u03f5}. For each \u03bb > 0, it follows from (2.6) that\nP(Zr \u2212r/n \u2265\u03bb\n\u221a\n2r/n) = P(Yr \u2265[r + \u03bb\n\u221a\n2r]Yn/n)\n\u2264P(Yr \u2265[r + \u03bb\n\u221a\n2r]Yn/n and An,\u03f5) + P(Ac\nn,\u03f5)\n\u2264P(Yr \u2265[r + \u03bb\n\u221a\n2r][1 \u2212\u03f5]) + e\u2212\u03f52n/4\n= P(Yr \u2212r \u2265\u03bb\n\u221a\n2r[1 \u2212\u03f5 \u2212\u03f5\np\nr/2\u03bb2]) + e\u2212\u03f52n/4.\nNow pick \u03f5 = 4(n\u22121 log n)1/2, \u03bb = 8\u221a2 log n and assume that n is su\ufb03ciently large so that\n\u03f5(1 +\np\nr/2\u03bb2) \u22641/2.\n3When r \u2265C\u2032(log n)3 for some positive constant C\u2032, a better estimate is possible, namely, \u2225P\nP\nP\n1\u2264k\u2264r ukv\u2217\nk\u2225\u221e\u2264\nC \u221ar log n/n.\n13\nThen\nP(Zr \u2212r/n \u2265\u03bb\n\u221a\n2r/n) \u2264P(Yr \u2212r \u2265(\u03bb/2)\n\u221a\n2r) + n\u22124.\nAssume now that r \u22654 log n (which means that \u03bb \u22644\n\u221a\n2r). Then it follows from (2.5) that\nP(Yr \u2212r \u2265(\u03bb/2)\n\u221a\n2r) \u2264P(Yr \u2212r \u2265(\u03bb/4)\n\u221a\n2r + (\u03bb/4)2) \u2264e\u2212\u03bb2/32 = n\u22124.\nHence\nP(Zr \u2212r/n \u226516\np\nr log n/n) \u22642n\u22124\nand, therefore,\nP(max\ni\n\u2225PUei\u22252 \u2212r/n \u226516\np\nr log n/n) \u22642n\u22123\n(2.8)\nby the union bound. Note that (2.8) establishes the \ufb01rst claim of the lemma (even for r < 4 log n\nsince in this case Zr \u2264Z\u23084 log n\u2309).\nIt remains to establish the second claim. Notice that by symmetry, E = P\n1\u2264k\u2264r ukv\u2217\nk has the\nsame distribution as\nF =\nr\nX\nk=1\n\u03f5kukv\u2217\nk,\nwhere {\u03f5k} is an independent Rademacher sequence. It then follows from Hoe\ufb00ding\u2019s inequality\nthat conditional on {uk} and {vk} we have\nP(|Fij| > t) \u22642e\u2212t2/2\u03c32\nij,\n\u03c32\nij =\nX\n1\u2264k\u2264r\nu2\nikv2\nik.\nOur previous results indicate that maxij |vij|2 \u2264(10 log n)/n with large probability and thus\n\u03c32\nij \u226410 log n\nn\n\u2225PUei\u22252.\nSet \u00afr = max(r, log n). Since \u2225PUei\u22252 \u2264C\u00afr/n with large probability, we have\n\u03c32\nij \u2264C(log n) \u00afr/n2\nwith large probability. Hence the marginal distribution of Fij obeys\nP(|Fij| > \u03bb\n\u221a\n\u00afr/n) \u22642e\u2212\u03b3\u03bb2/ log n + P(\u03c32\nij \u2265C(log n)\u00afr/n2).\nfor some numerical constant \u03b3.\nPicking \u03bb = \u03b3\u2032 log n where \u03b3\u2032 is a su\ufb03ciently large numerical\nconstant gives\n\u2225F \u2225\u221e\u2264C (log n)\n\u221a\n\u00afr/n\nwith large probability. Since E and F have the same distribution, the second claim follows.\nThe claim about the size of maxij |vij|2 is straightforward since our techniques show that for\neach \u03bb > 0\nP(Z1 \u2265\u03bb(log n)/n) \u2264P(Y1 \u2265\u03bb(1 \u2212\u03f5) log n) + e\u2212\u03f52n/4.\nMoreover,\nP(Y1 \u2265\u03bb(1 \u2212\u03f5) log n) = P(|x1| \u2265\np\n\u03bb(1 \u2212\u03f5) log n) \u22642e\u22121\n2 \u03bb(1\u2212\u03f5) log n.\nIf n is su\ufb03ciently large so that \u03f5 \u22641/5, this gives P(Z1 \u226510(log n)/n) \u22643n\u22124 and, therefore,\nP(max\nij\n|vij|2 \u226510(log n)/n) \u226412n\u22123 log n\nsince the maximum is taken over at most 4n log n pairs.\n14\n3\nDuality\nLet R\u2126: Rn1\u00d7n2 \u2192R|\u2126| be the sampling operator which extracts the observed entries, R\u2126(X) =\n(Xij)ij\u2208\u2126, so that the constraint in (1.5) becomes R\u2126(X) = R\u2126(M). Standard convex optimization\ntheory asserts that X is solution to (1.5) if there exists a dual vector (or Lagrange multiplier)\n\u03bb \u2208R|\u2126| such that R\u2217\n\u2126\u03bb is a subgradient of the nuclear norm at the point X, which we denote by\nR\u2217\n\u2126\u03bb \u2208\u2202\u2225X\u2225\u2217\n(3.1)\n(see, e.g. [7]). Recall the de\ufb01nition of a subgradient of a convex function f : Rn1\u00d7n2 \u2192R. We say\nthat Y is a subgradient of f at X0, denoted Y \u2208\u2202f(X0), if\nf(X) \u2265f(X0) + \u27e8Y , X \u2212X0\u27e9\n(3.2)\nfor all X.\nSuppose X0 \u2208Rn1\u00d7n2 has rank r with a singular value decomposition given by\nX0 =\nX\n1\u2264k\u2264r\n\u03c3k ukv\u2217\nk,\n(3.3)\nWith these notations, Y is a subgradient of the nuclear norm at X0 if and only if it is of the form\nY =\nX\n1\u2264k\u2264r\nukv\u2217\nk + W ,\n(3.4)\nwhere W obeys the following two properties:\n(i) the column space of W is orthogonal to U \u2261span (u1, . . . , ur), and the row space of W is\northogonal to V \u2261span (v1, . . . , vr);\n(ii) the spectral norm of W is less than or equal to 1.\n(see, e.g., [23,36]). To express these properties concisely, it is convenient to introduce the orthogonal\ndecomposition Rn1\u00d7n2 = T \u2295T \u22a5where T is the linear space spanned by elements of the form ukx\u2217\nand yv\u2217\nk, 1 \u2264k \u2264r, where x and y are arbitrary, and T \u22a5is its orthogonal complement. Note that\ndim(T) = r(n1 + n2 \u2212r), precisely the number of degrees of freedom in the set of n1 \u00d7 n2 matrices\nof rank r. T \u22a5is the subspace of matrices spanned by the family (xy\u2217), where x (respectively y) is\nany vector orthogonal to U (respectively V ).\nThe orthogonal projection PT onto T is given by\nPT (X) = PUX + XPV \u2212PUXPV ,\n(3.5)\nwhere PU and PV are the orthogonal projections onto U and V . Note here that while PU and PV\nare matrices, PT is a linear operator mapping matrices to matrices. We also have\nPT \u22a5(X) = (I \u2212PT )(X) = (In1 \u2212PU)X(In2 \u2212PV )\nwhere Id denotes the d \u00d7 d identity matrix. With these notations, Y \u2208\u2202\u2225X0\u2225\u2217if\n(i\u2019) PT (Y ) = P\n1\u2264k\u2264r ukv\u2217\nk,\n15\n(ii\u2019) and \u2225PT \u22a5Y \u2225\u22641.\nNow that we have characterized the subgradient of the nuclear norm, the lemma below gives\nsu\ufb03cient conditions for the uniqueness of the minimizer to (1.5).\nLemma 3.1 Consider a matrix X0 = Pr\nk=1 \u03c3k ukv\u2217\nk of rank r which is feasible for the problem\n(1.5), and suppose that the following two conditions hold:\n1. there exists a dual point \u03bb such that Y = R\u2217\n\u2126\u03bb obeys\nPT (Y ) =\nr\nX\nk=1\nukv\u2217\nk,\n\u2225PT \u22a5(Y )\u2225< 1;\n(3.6)\n2. the sampling operator R\u2126restricted to elements in T is injective.\nThen X0 is the unique minimizer.\nBefore proving this result, we would like to emphasize that this lemma provides a clear strategy\nfor proving our main result, namely, Theorem 1.3. Letting M = Pr\nk=1 \u03c3k ukv\u2217\nk, M is the unique\nsolution to (1.5) if the injectivity condition holds and if one can \ufb01nd a dual point \u03bb such that\nY = R\u2217\n\u2126\u03bb obeys (3.6).\nThe proof of Lemma 3.1 uses a standard fact which states that the nuclear norm and the spectral\nnorm are dual to one another.\nLemma 3.2 For each pair W and H, we have\n\u27e8W , H\u27e9\u2264\u2225W \u2225\u2225H\u2225\u2217.\nIn addition, for each H, there is a W obeying \u2225W \u2225= 1 which achieves the equality.\nA variety of proofs are available for this Lemma, and an elementary argument is sketched in [27].\nWe now turn to the proof of Lemma 3.1.\nProof [of Lemma 3.1] Consider any perturbation X0 + H where R\u2126(H) = 0. Then for any W 0\nobeying (i)\u2013(ii), Pr\nk=1 ukv\u2217\nk + W 0 is a subgradient of the nuclear norm at X0 and, therefore,\n\u2225X0 + H\u2225\u2217\u2265\u2225X0\u2225\u2217+ \u27e8\nr\nX\nk=1\nukv\u2217\nk + W 0, H\u27e9.\nLetting W = PT \u22a5(Y ), we may write Pr\nk=1 ukv\u2217\nk = R\u2217\n\u2126\u03bb \u2212W . Since \u2225W \u2225< 1 and R\u2126(H) = 0,\nit then follows that\n\u2225X0 + H\u2225\u2217\u2265\u2225X0\u2225\u2217+ \u27e8W 0 \u2212W , H\u27e9.\nNow by construction\n\u27e8W 0 \u2212W , H\u27e9= \u27e8PT \u22a5(W 0 \u2212W ), H\u27e9= \u27e8W 0 \u2212W , PT \u22a5(H)\u27e9.\nWe use Lemma 3.2 and set W 0 = PT \u22a5(Z) where Z is any matrix obeying \u2225Z\u2225\u22641 and\n\u27e8Z, PT \u22a5(H)\u27e9= \u2225PT \u22a5(H)\u2225\u2217. Then W 0 \u2208T \u22a5, \u2225W 0\u2225\u22641, and\n\u27e8W 0 \u2212W , H\u27e9\u2265(1 \u2212\u2225W \u2225) \u2225PT \u22a5(H)\u2225\u2217,\nwhich by assumption is strictly positive unless PT \u22a5(H) = 0. In other words, \u2225X0 + H\u2225\u2217> \u2225X0\u2225\u2217\nunless PT \u22a5(H) = 0. Assume then that PT \u22a5(H) = 0 or equivalently that H \u2208T. Then R\u2126(H) = 0\nimplies that H = 0 by the injectivity assumption. In conclusion, \u2225X0+H\u2225\u2217> \u2225X\u2225\u2217unless H = 0.\n16\n4\nArchitecture of the proof\nOur strategy to prove that M = P\n1\u2264k\u2264r \u03c3kukv\u2217\nk is the unique minimizer to (1.5) is to construct a\nmatrix Y which vanishes on \u2126c and obeys the conditions of Lemma 3.1 (and show the injectivity\nof the sampling operator restricted to matrices in T along the way). Set P\u2126to be the orthogonal\nprojector onto the indices in \u2126so that the (i, j)th component of P\u2126(X) is equal to Xij if (i, j) \u2208\u2126\nand zero otherwise. Our candidate Y will be the solution to\nminimize\n\u2225X\u2225F\nsubject to\n(PT P\u2126)(X) = Pr\nk=1 ukv\u2217\nk.\n(4.1)\nThe matrix Y vanishes on \u2126c as otherwise it would not be an optimal solution since P\u2126(Y )\nwould obey the constraint and have a smaller Frobenius norm. Hence Y = P\u2126(Y ) and PT (Y ) =\nPr\nk=1 ukv\u2217\nk. Since the Pythagoras formula gives\n\u2225Y \u22252\nF = \u2225PT (Y )\u22252\nF + \u2225PT \u22a5(Y )\u22252\nF = \u2225\nr\nX\nk=1\nukv\u2217\nk\u22252\nF + \u2225PT \u22a5(Y )\u22252\nF\n= r + \u2225PT \u22a5(Y )\u22252\nF ,\nminimizing the Frobenius norm of X amounts to minimizing the Frobenius norm of PT \u22a5(X) under\nthe constraint PT (X) = Pr\nk=1 ukv\u2217\nk. Our motivation is twofold. First, the solution to the least-\nsquares problem (4.1) has a closed form that is amenable to analysis. Second, by forcing PT \u22a5(Y )\nto be small in the Frobenius norm, we hope that it will be small in the spectral norm as well, and\nestablishing that \u2225PT \u22a5(Y )\u2225< 1 would prove that M is the unique solution to (1.5).\nTo compute the solution to (4.1), we introduce the operator A\u2126T de\ufb01ned by\nA\u2126T (M) = P\u2126PT (M).\nThen, if A\u2217\n\u2126T A\u2126T = PT P\u2126PT has full rank when restricted to T, the minimizer to (4.1) is given by\nY = A\u2126T (A\u2217\n\u2126T A\u2126T )\u22121(E),\nE \u2261\nr\nX\nk=1\nukv\u2217\nk.\n(4.2)\nWe clarify the meaning of (4.2) to avoid any confusion.\n(A\u2217\n\u2126T A\u2126T )\u22121(E) is meant to be that\nelement F in T obeying (A\u2217\n\u2126T A\u2126T )(F ) = E.\nTo summarize the aims of our proof strategy,\n\u2022 We must \ufb01rst show that A\u2217\n\u2126T A\u2126T = PT P\u2126PT is a one-to-one linear mapping from T onto\nitself. In this case, A\u2126T = P\u2126PT \u2014as a mapping from T to Rn1\u00d7n2\u2014is injective. This is\nthe second su\ufb03cient condition of Lemma 3.1. Moreover, our ansatz for Y given by (4.2) is\nwell-de\ufb01ned.\n\u2022 Having established that Y is well-de\ufb01ned, we will show that\n\u2225PT \u22a5(Y )\u2225< 1,\nthus proving the \ufb01rst su\ufb03cient condition.\n17\n4.1\nThe Bernoulli model\nInstead of showing that the theorem holds when \u2126is a set of size m sampled uniformly at random,\nwe prove the theorem for a subset \u2126\u2032 sampled according to the Bernoulli model. Here and be-\nlow, {\u03b4ij}1\u2264i\u2264n1,1\u2264j\u2264n2 is a sequence of independent identically distributed 0/1 Bernoulli random\nvariables with\nP(\u03b4ij = 1) = p \u2261\nm\nn1n2\n,\n(4.3)\nand de\ufb01ne\n\u2126\u2032 = {(i, j) : \u03b4ij = 1}.\n(4.4)\nNote that E |\u2126\u2032| = m, so that the average cardinality of \u2126\u2032 is that of \u2126. Then following the same\nreasoning as the argument developed in Section II.C of [11] shows that the probability of \u2018failure\u2019\nunder the uniform model is bounded by 2 times the probability of failure under the Bernoulli model;\nthe failure event is the event on which the solution to (1.5) is not exact. Hence, we can restrict our\nattention to the Bernoulli model and from now on, we will assume that \u2126is given by (4.4). This is\nadvantageous because the Bernoulli model admits a simpler analysis than uniform sampling thanks\nto the independence between the \u03b4ij\u2019s.\n4.2\nThe injectivity property\nWe study the injectivity of A\u2126T , which also shows that Y is well-de\ufb01ned. To prove this, we will\nshow that the linear operator p\u22121PT (P\u2126\u2212pI)PT has small operator norm, which we recall is\nsup\u2225X\u2225F \u22641 p\u22121\u2225PT (P\u2126\u2212pI)PT (X)\u2225F .\nTheorem 4.1 Suppose \u2126is sampled according to the Bernoulli model (4.3)\u2013(4.4) and put n =\nmax(n1, n2). Suppose that the coherences obey max(\u00b5(U), \u00b5(V )) \u2264\u00b50. Then, there is a numerical\nconstants CR such that for all \u03b2 > 1,\np\u22121 \u2225PT P\u2126PT \u2212pPT \u2225\u2264CR\nr\n\u00b50 nr(\u03b2 log n)\nm\n(4.5)\nwith probability at least 1 \u22123n\u2212\u03b2 provided that CR\nq\n\u00b50 nr(\u03b2 log n)\nm\n< 1.\nProof Decompose any matrix X as X = P\nab\u27e8X, eae\u2217\nb\u27e9eae\u2217\nb so that\nPT (X) =\nX\nab\n\u27e8PT (X), eae\u2217\nb\u27e9eae\u2217\nb =\nX\nab\n\u27e8X, PT (eae\u2217\nb)\u27e9eae\u2217\nb.\nHence, P\u2126PT (X) = P\nab \u03b4ab \u27e8X, PT (eae\u2217\nb)\u27e9eae\u2217\nb which gives\n(PT P\u2126PT )(X) =\nX\nab\n\u03b4ab \u27e8X, PT (eae\u2217\nb)\u27e9PT (eae\u2217\nb).\nIn other words,\nPT P\u2126PT =\nX\nab\n\u03b4ab PT (eae\u2217\nb) \u2297PT (eae\u2217\nb).\nIt follows from the de\ufb01nition (3.5) of PT that\nPT (eae\u2217\nb) = (PUea)e\u2217\nb + ea(PV eb)\u2217\u2212(PUea)(PV eb)\u2217.\n(4.6)\n18\nThis gives\n\u2225PT (eae\u2217\nb)\u22252\nF = \u27e8PT (eae\u2217\nb), eae\u2217\nb\u27e9= \u2225PUea\u22252 + \u2225PV eb\u22252 \u2212\u2225PUea\u22252 \u2225PV eb\u22252\n(4.7)\nand since \u2225PUea\u22252 \u2264\u00b5(U)r/n1 and \u2225PV eb\u22252 \u2264\u00b5(U)r/n2,\n\u2225PT (eae\u2217\nb)\u22252\nF \u22642\u00b50r/ min(n1, n2).\n(4.8)\nNow the fact that the operator PT P\u2126PT does not deviate from its expected value\nE(PT P\u2126PT ) = PT (E P\u2126)PT = PT (pI)PT = pPT\nin the spectral norm is related to Rudelson\u2019s selection theorem [29]. The \ufb01rst part of the theorem\nbelow may be found in [10] for example, see also [30] for a very similar statement.\nTheorem 4.2\n[10] Let {\u03b4ab} be independent 0/1 Bernoulli variables with P(\u03b4ab = 1) = p =\nm\nn1n2\nand put n = max(n1, n2). Suppose that \u2225PT (eae\u2217\nb)\u22252\nF \u22642\u00b50r/n. Set\nZ \u2261p\u22121\u2225\nX\nab\n(\u03b4ab \u2212p) PT (eae\u2217\nb) \u2297PT (eae\u2217\nb)\u2225= p\u22121\u2225PT P\u2126PT \u2212pPT \u2225.\n1. There exists a constant C\u2032\nR such that\nE Z \u2264C\u2032\nR\nr\n\u00b50 nr log n\nm\n(4.9)\nprovided that the right-hand side is smaller than 1.\n2. Suppose E Z \u22641. Then for each \u03bb > 0, we have\nP\n \n|Z \u2212E Z| > \u03bb\nr\n\u00b50 nr log n\nm\n!\n\u22643 exp\n \n\u2212\u03b3\u2032\n0 min\n(\n\u03bb2 log n, \u03bb\ns\nm log n\n\u00b50 nr\n)!\n(4.10)\nfor some positive constant \u03b3\u2032\n0.\nAs mentioned above, the \ufb01rst part, namely, (4.9) is an application of an established result which\nstates that if {yi} is a family of vectors in Rd and {\u03b4i} is a 0/1 Bernoulli sequence with P(\u03b4i = 1) = p,\nthen\np\u22121\u2225\nX\ni\n(\u03b4i \u2212p)yi \u2297yi\u2225\u2264C\ns\nlog d\np\nmax\ni\n\u2225yi\u2225\nfor some C > 0 provided that the right-hand side is less than 1. The proof may be found in the\ncited literature, e.g. in [10]. Hence, the \ufb01rst part follows from applying this result to vectors of\nthe form PT (eae\u2217\nb) and using the available bound on \u2225PT (eae\u2217\nb)\u2225F . The second part follows from\nTalagrand\u2019s concentration inequality and may be found in the Appendix.\nSet \u03bb =\np\n\u03b2/\u03b3\u2032\n0 and assume that m > (\u03b2/\u03b3\u2032\n0)\u00b50 nr log n. Then the left-hand side of (4.10) is\nbounded by 3n\u2212\u03b2 and thus, we established that\nZ \u2264C\u2032\nR\nr\n\u00b50 nr log n\nm\n+\n1\np\n\u03b3\u2032\n0\nr\n\u00b50 nr \u03b2 log n\nm\n19\nwith probability at least 1 \u22123n\u2212\u03b2. Setting CR = C\u2032\nR + 1/\np\n\u03b3\u2032\n0 \ufb01nishes the proof.\nTake m large enough so that CR\np\n\u00b50 (nr/m) log n \u22641/2. Then it follows from (4.5) that\np\n2\u2225PT (X)\u2225F \u2264\u2225(PT P\u2126PT )(X)\u2225F \u22643p\n2 \u2225PT (X)\u2225F\n(4.11)\nfor all X with large probability. In particular, the operator A\u2217\n\u2126T A\u2126T = PT P\u2126PT mapping T onto\nitself is well-conditioned and hence invertible. An immediate consequence is the following:\nCorollary 4.3 Assume that CR\np\n\u00b50nr(log n)/m \u22641/2. With the same probability as in Theorem\n4.1, we have\n\u2225P\u2126PT (X)\u2225F \u2264\np\n3p/2\u2225PT (X)\u2225F .\n(4.12)\nProof We have \u2225P\u2126PT (X)\u22252\nF = \u27e8X, (P\u2126PT )\u2217(P\u2126PT )X\u27e9= \u27e8X, (PT P\u2126PT )X\u27e9and thus\n\u2225P\u2126PT (X)\u22252\nF = \u27e8PT X, (PT P\u2126PT )X\u27e9\u2264\u2225PT (X)\u2225F \u2225(PT P\u2126PT )(X)\u2225F ,\nwhere the inequality is due to Cauchy-Schwarz. The conclusion (4.12) follows from (4.11).\n4.3\nThe size property\nIn this section, we explain how we will show that \u2225PT \u22a5(Y )\u2225< 1. This result will follow from \ufb01ve\nlemmas that we will prove in Section 6. Introduce\nH \u2261PT \u2212p\u22121PT P\u2126PT ,\nwhich obeys \u2225H(X)\u2225F \u2264CR\np\n\u00b50(nr/m) \u03b2 log n\u2225PT (X)\u2225F with large probability because of The-\norem 4.1. For any matrix X \u2208T, (PT P\u2126PT )\u22121(X) can be expressed in terms of the power series\n(PT P\u2126PT )\u22121(X) = p\u22121(X + H(X) + H2(X) + . . .)\nfor H is a contraction when m is su\ufb03ciently large. Since Y = P\u2126PT (PT P\u2126PT )\u22121(P\n1\u2264k\u2264r ukv\u2217\nk),\nPT \u22a5(Y ) may be decomposed as\nPT \u22a5(Y ) = p\u22121(PT \u22a5P\u2126PT )(E + H(E) + H2(E) + . . .),\nE =\nX\n1\u2264k\u2264r\nukv\u2217\nk.\n(4.13)\nTo bound the norm of the left-hand side, it is of course su\ufb03cient to bound the norm of the summands\nin the right-hand side. Taking the following \ufb01ve lemmas together establishes Theorem 1.3.\nLemma 4.4 Fix \u03b2 \u22652 and \u03bb \u22651. There is a numerical constant C0 such that if m \u2265\u03bb \u00b52\n1 nr\u03b2 log n,\nthen\np\u22121 \u2225(PT \u22a5P\u2126PT )E\u2225\u2264C0 \u03bb\u22121/2.\n(4.14)\nwith probability at least 1 \u2212n\u2212\u03b2.\nLemma 4.5 Fix \u03b2 \u22652 and \u03bb \u22651. There are numerical constants C1 and c1 such that if m \u2265\n\u03bb \u00b51 max(\u221a\u00b50, \u00b51) nr\u03b2 log n, then\np\u22121 \u2225(PT \u22a5P\u2126PT )H(E)\u2225\u2264C1 \u03bb\u22121\n(4.15)\nwith probability at least 1 \u2212c1n\u2212\u03b2.\n20\nLemma 4.6 Fix \u03b2 \u22652 and \u03bb \u22651. There are numerical constants C2 and c2 such that if m \u2265\n\u03bb \u00b54/3\n0\nnr4/3\u03b2 log n, then\np\u22121 \u2225(PT \u22a5P\u2126PT )H2(E)\u2225\u2264C2 \u03bb\u22123/2\n(4.16)\nwith probability at least 1 \u2212c2n\u2212\u03b2.\nLemma 4.7 Fix \u03b2 \u22652 and \u03bb \u22651. There are numerical constants C3 and c3 such that if m \u2265\n\u03bb\u00b52\n0 nr2\u03b2 log n, then\np\u22121 \u2225(PT \u22a5P\u2126PT )H3(E)\u2225\u2264C3 \u03bb\u22121/2\n(4.17)\nwith probability at least 1 \u2212c3n\u2212\u03b2.\nLemma 4.8 Under the assumptions of Theorem 4.1, there is a numerical constant Ck0 such that\nif m \u2265(2CR)2\u00b50nr\u03b2 log n, then\np\u22121 \u2225(PT \u22a5P\u2126PT )\nX\nk\u2265k0\nHk(E)\u2225\u2264Ck0\n\u0012n2r\nm\n\u00131/2 \u0012\u00b50nr\u03b2 log n\nm\n\u0013k0/2\n(4.18)\nwith probability at least 1 \u2212n\u2212\u03b2.\nLet us now show how we may combine these lemmas to prove our main results. Under all of the\nassumptions of Theorem 1.3, consider the four Lemmas 4.4, 4.5, 4.6 and 4.8, the latter applied with\nk0 = 3. Together they imply that there are numerical constants c and C such that \u2225PT \u22a5(Y )\u2225< 1\nwith probability at least 1 \u2212cn\u2212\u03b2 provided that the number of samples obeys\nm \u2265C max(\u00b52\n1, \u00b51/2\n0\n\u00b51, \u00b54/3\n0\nr1/3, \u00b50n1/4) nr\u03b2 log n\n(4.19)\nfor some constant C. The four expressions in the maximum come from Lemmas 4.4, 4.5, 4.6 and\n4.8 in this order. Now the bound (4.19) is only interesting in the range when \u00b50n1/4r is smaller\nthan a constant times n as otherwise the right-hand side is greater than n2 (this would say that one\nwould see all the entries in which case our claim is trivial). When \u00b50r \u2264n3/4, (\u00b50r)4/3 \u2264\u00b50n5/4r\nand thus the recovery is exact provided that m obeys (1.9).\nFor the case concerning small values of the rank, we consider all \ufb01ve lemmas and apply Lemma\n4.8, the latter applied with k0 = 4. Together they imply that \u2225PT \u22a5(Y )\u2225< 1 with probability at\nleast 1 \u2212cn\u2212\u03b2 provided that the number of samples obeys\nm \u2265C max(\u00b52\n0r, \u00b50n1/5) nr\u03b2 log n\n(4.20)\nfor some constant C. The two expressions in the maximum come from Lemmas 4.7 and 4.8 in this\norder. The reason for this simpli\ufb01ed formulation is that the terms \u00b52\n1, \u00b51/2\n0\n\u00b51 and \u00b54/3\n0\nr1/3 which\ncome from Lemmas 4.4, 4.5 and 4.6 are bounded above by \u00b52\n0r since \u00b51 \u2264\u00b50\n\u221ar. When \u00b50r \u2264n1/5,\nthe recovery is exact provided that m obeys (1.10).\n5\nConnections with Random Graph Theory\n5.1\nThe injectivity property and the coupon collector\u2019s problem\nWe argued in the Introduction that to have any hope of recovering an unknown matrix of rank 1\nby any method whatsoever, one needs at least one observation per row and one observation per\n21\ncolumn. Sample m entries uniformly at random. Viewing the row indices as bins, assign the kth\nsampled entry to the bin corresponding to its row index. Then to have any hope of recovering our\nmatrix, all the bins need to be occupied. Quantifying how many samples are required to \ufb01ll all of\nthe bins is the famous coupon collector\u2019s problem.\nCoupon collection is also connected to the injectivity of the sampling operator P\u2126restricted to\nelements in T. Suppose we sample the entries of a rank 1 matrix equal to xy\u2217with left and right\nsingular vectors u = x/\u2225x\u2225and v = y/\u2225y\u2225respectively and have not seen anything in the ith\nrow. Then we claim that P\u2126(restricted to T) has a nontrivial null space and thus PT P\u2126PT is not\ninvertible. Indeed, consider the matrix eiv\u2217. This matrix is in T and\nP\u2126(eiv\u2217) = 0\nsince eiv\u2217vanishes outside of the ith row. The same applies to the columns as well. If we have not\nseen anything in column j, then the rank-1 matrix ue\u2217\nj \u2208T and P\u2126(ue\u2217\nj) = 0. In conclusion, the\ninvertibility of PT P\u2126PT implies a complete collection.\nWhen the entries are sampled uniformly at random, it is well known that one needs on the\norder of n log n samples to sample all the rows. What is interesting is that Theorem 4.1 implies\nthat PT P\u2126PT is invertible\u2014a stronger property\u2014when the number of samples is also on the order\nof n log n. A particular implication of this discussion is that the logarithmic factors in Theorem 4.1\nare unavoidable.\n5.2\nThe injectivity property and the connectivity problem\nTo recover a matrix of rank 1, one needs much more than at least one observation per row and\ncolumn. Let R be the set of row indices, 1 \u2264i \u2264n, and C be the set of column indices, 1 \u2264j \u2264n,\nand consider the bipartite graph connecting vertices i \u2208R to vertices j \u2208C if and only if (i, j) \u2208\u2126,\ni.e. the (i, j)th entry is observed. We claim that if this graph is not fully connected, then one cannot\nhope to recover a matrix of rank 1.\nTo see this, we let I be the set of row indices and J be the set of column indices in any\nconnected component. We will assume that I and J are nonempty as otherwise, one is in the\npreviously discussed situation where some rows or columns are not sampled. Consider a rank 1\nmatrix equal to xy\u2217as before with singular vectors u = x/\u2225x\u2225and v = y/\u2225y\u2225. Then all the\ninformation about the values of the xi\u2019s with i \u2208I and of the yj\u2019s with j \u2208J are given by the\nsampled entries connecting I to J since all the other observed entries connect vertices in Ic to those\nin Jc. Now even if one observes all the entries xiyj with i \u2208I and j \u2208J, then at least the signs of\nxi, i \u2208I, and of yj, j \u2208J, would remain undetermined. Indeed, if the values (xi)i\u2208I, (yj)j\u2208J are\nconsistent with the observed entries, so are the values (\u2212xi)i\u2208I, (\u2212yj)j\u2208J. However, since the same\nanalysis holds for the sets Ic and Jc, there are at least two matrices consistent with the observed\nentries and exact matrix completion is impossible.\nThe connectivity of the graph is also related to the injectivity of the sampling operator P\u2126\nrestricted to elements in T. If the graph is not fully connected, then we claim that P\u2126(restricted\nto T) has a nontrivial null space and thus PT P\u2126PT is not invertible. Indeed, consider the matrix\nM = av\u2217+ ub\u2217,\nwhere ai = \u2212ui if i \u2208I and ai = ui otherwise, and bj = vj if j \u2208J and bj = \u2212vj otherwise. Then\nthis matrix is in T and obeys\nMij = 0\n22\nif (i, j) \u2208I \u00d7J or (i, j) \u2208Ic \u00d7Jc. Note that on the complement, i.e. (i, j) \u2208I \u00d7Jc or (i, j) \u2208Ic \u00d7J,\none has Mij = 2uivj and one can show that M \u0338= 0 unless uv\u2217= 0. Since \u2126is included in the\nunion of I \u00d7 J and Ic \u00d7 Jc, we have that P\u2126(M) = 0. In conclusion, the invertibility of PT P\u2126PT\nimplies a fully connected graph.\nWhen the entries are sampled uniformly at random, it is well known that one needs on the order\nof n log n samples to obtain a fully connected graph with large probability (see, e.g., [8]). Remark-\nably, Theorem 4.1 implies that PT P\u2126PT is invertible\u2014a stronger property\u2014when the number of\nsamples is also on the order of n log n.\n6\nProofs of the Critical Lemmas\nIn this section, we prove the \ufb01ve lemmas of Section 4.3. Before we begin, however, we develop a\nsimple estimate which we will use throughout. For each pair (a, b) and (a\u2032, b\u2032), it follows from the\nexpression of PT (eae\u2217\nb) (4.6) that\n\u27e8PT (ea\u2032e\u2217\nb\u2032), eae\u2217\nb\u27e9= \u27e8ea, PUea\u2032\u27e91{b=b\u2032} + \u27e8eb, PV eb\u2032\u27e91{a=a\u2032} \u2212\u27e8ea, PUea\u2032\u27e9\u27e8eb, PV eb\u2032\u27e9.\n(6.1)\nFix \u00b50 obeying \u00b5(U) \u2264\u00b50 and \u00b5(V ) \u2264\u00b50 and note that\n|\u27e8ea, PUea\u2032\u27e9| = |\u27e8PUea, PUea\u2032\u27e9| \u2264\u2225PUea\u2225\u2225PUea\u2032\u2225\u2264\u00b50r/n1\nand similarly for \u27e8eb, PV eb\u2032\u27e9. Suppose that b = b\u2032 and a \u0338= a\u2032, then\n|\u27e8PT (ea\u2032e\u2217\nb\u2032), eae\u2217\nb\u27e9| = |\u27e8ea, PUea\u2032\u27e9|(1 \u2212\u2225PV eb\u22252) \u2264\u00b50r/n1.\nWe have a similar bound when a = a\u2032 and b \u0338= b\u2032 whereas when a \u0338= a\u2032 and b \u0338= b\u2032,\n|\u27e8PT (ea\u2032e\u2217\nb\u2032), eae\u2217\nb\u27e9| \u2264(\u00b50r)2/(n1n2).\nIn short, it follows from this analysis (and from (4.8) for the case where (a, b) = (a\u2032, b\u2032)) that\nmax\nab,a\u2032b\u2032 |\u27e8PT (ea\u2032e\u2217\nb\u2032), eae\u2217\nb\u27e9| \u22642\u00b50r/ min(n1, n2).\n(6.2)\nA consequence of (4.8) is the estimate:\nX\na\u2032b\u2032\n|\u27e8PT (ea\u2032e\u2217\nb\u2032), eae\u2217\nb\u27e9|2 =\nX\na\u2032b\u2032\n|\u27e8PT (eae\u2217\nb), ea\u2032e\u2217\nb\u2032\u27e9|2\n= \u2225PT (eae\u2217\nb)\u22252\nF \u22642\u00b50r/ min(n1, n2),\n(6.3)\nwhich we will apply several times. A related estimate is this:\nmax\na\nX\nb\n|Eab|2 \u2264\u00b50r/ min(n1, n2),\n(6.4)\nand the same is true by exchanging the role of a and b. To see this, write\nX\nb\n|Eab|2 = \u2225e\u2217\naE\u22252 = \u2225\nX\nj\u2264r\nvj\u27e8uj, ea\u27e9\u22252 =\nX\nj\u2264r\n|\u27e8uj, ea\u27e9|2 = \u2225PUea\u22252,\n23\nand the conclusion follows from the coherence property.\nWe will prove the lemmas in the case where n1 = n2 = n for simplicity, i.e. in the case of\nsquare matrices of dimension n. The general case is treated in exactly the same way. In fact, the\nargument only makes use of the bounds (6.2), (6.3) (and sometimes (6.4)), and the general case is\nobtained by replacing n with min(n1, n2).\nEach of the following subsections computes the operator norm of some random variable. In\neach section, we denote S as the quantity whose norm we wish to analyze. We will also frequently\nuse the notation H for some auxiliary matrix variable whose norm we will need to bound. Hence,\nwe will reuse the same notation many times rather than introducing a dozens new names\u2014just like\nin computer programming where one uses the same variable name in distinct routines.\n6.1\nProof of Lemma 4.4\nIn this section, we develop a bound on\np\u22121\u2225PT \u22a5P\u2126PT (E)\u2225= p\u22121\u2225PT \u22a5(P\u2126\u2212pI)PT (E)\u2225\n\u2264p\u22121\u2225(P\u2126\u2212pI)(E)\u2225,\nwhere the equality follows from PT \u22a5PT = 0, and the inequality from PT (E) = E together with\n\u2225PT \u22a5(X)\u2225\u2264\u2225X\u2225which is valid for any matrix X. Set\nS \u2261p\u22121(P\u2126\u2212pI)(E) = p\u22121 X\nab\n(\u03b4ab \u2212p)Eabeae\u2217\nb.\n(6.5)\nWe think of S as a random variable since it depends on the random \u03b4ab\u2019s, and note that E S = 0.\nThe proof of Lemma 4.4 operates by developing an estimate on the size of (E \u2225S\u2225q)1/q for some\nq \u22651 and by applying Markov inequality to bound the tail of the random variable \u2225S\u2225. To do this,\nwe shall use a symmetrization argument and the noncommutative Khintchine inequality. Since the\nfunction f(S) = \u2225S\u2225q is convex, Jensen\u2019s inequality gives that\nE \u2225S\u2225q \u2264E \u2225S \u2212S\u2032\u2225q,\nwhere S\u2032 = p\u22121 P\nab(\u03b4\u2032\nab \u2212p)Eabeae\u2217\nb is an independent copy of S. Since (\u03b4ab \u2212\u03b4\u2032\nab) is symmetric,\nS \u2212S\u2032 has the same distribution as\np\u22121 X\nab\n\u03f5ab(\u03b4ab \u2212\u03b4\u2032\nab)Eabeae\u2217\nb \u2261S\u03f5 \u2212S\u2032\n\u03f5,\nwhere {\u03f5ab} is an independent Rademacher sequence and S\u03f5 = p\u22121 P\nab \u03f5ab\u03b4abEabeae\u2217\nb. Further, the\ntriangle inequality gives\n(E \u2225S\u03f5 \u2212S\u2032\n\u03f5\u2225q)1/q \u2264(E \u2225S\u03f5\u2225q)1/q + (E \u2225S\u2032\n\u03f5\u2225q)1/q = 2(E \u2225S\u03f5\u2225q)1/q\nsince S\u03f5 and S\u2032\n\u03f5 have the same distribution and, therefore,\n(E \u2225S\u2225q)1/q \u22642p\u22121\n \nE\u03b4 E\u03f5 \u2225\nX\nab\n\u03f5ab\u03b4ab Eabeae\u2217\nb\u2225q\n!1/q\n.\n24\nWe are now in position to apply the noncommutative Khintchine inequality which bounds the\nSchatten norm of a Rademacher series. For q \u22651, the Schatten q-norm of a matrix is denoted by\n\u2225X\u2225Sq =\n n\nX\ni=1\n\u03c3i(X)q\n!1/q\n.\nNote that the nuclear norm is equal to the Schatten 1-norm and the Frobenius norm is equal to\nthe Schatten 2-norm. The following theorem was originally proven by Lust-Picquard [25], and was\nlater sharpened by Buchholz [9].\nLemma 6.1 (Noncommutative Khintchine inequality) Let (Xi)1\u2264i\u2264r be a \ufb01nite sequence of\nmatrices of the same dimension and let {\u03f5i} be a Rademacher sequence. For each q \u22652\n\uf8ee\n\uf8f0E\u03f5\n\r\r\r\r\r\nX\ni\n\u03f5iXi\n\r\r\r\r\r\nq\nSq\n\uf8f9\n\uf8fb\n1/q\n\u2264CK\n\u221aq max\n\uf8ee\n\uf8ef\uf8f0\n\r\r\r\r\r\r\n X\ni\nX\u2217\ni Xi\n!1/2\r\r\r\r\r\r\nSq\n,\n\r\r\r\r\r\r\n X\ni\nXiX\u2217\ni\n!1/2\r\r\r\r\r\r\nSq\n\uf8f9\n\uf8fa\uf8fb,\nwhere CK = 2\u22121/4p\n\u03c0/e.\nFor reference, if X is an n \u00d7 n matrix and q \u2265log n, we have\n\u2225X\u2225\u2264\u2225X\u2225Sq \u2264e\u2225X\u2225,\nso that the Schatten q-norm is within a multiplicative constant from the operator norm. Observe\nnow that with q\u2032 \u2265q\n(E\u03b4 E\u03f5 \u2225S\u03f5\u2225q)1/q \u2264\n\u0010\nE\u03b4 E\u03f5 \u2225S\u03f5\u2225q\nSq\u2032\n\u00111/q\n\u2264\n\u0010\nE\u03b4 E\u03f5 \u2225S\u03f5\u2225q\u2032\nSq\u2032\n\u00111/q\u2032\n.\nWe apply the noncommutative Khintchine inequality with q\u2032 \u2265log n, and after a little algebra,\nobtain\n\u0010\nE\u03b4 E\u03f5 \u2225S\u03f5\u2225q\u2032\nSq\u2032\n\u00111/q\u2032\n\u2264CK\ne \u221aq\u2032\np\n \nE\u03b4 max\n\"\n\u2225\nX\nab\n\u03b4abE2\nabeae\u2217\na\u2225q\u2032/2, \u2225\nX\nab\n\u03b4abE2\nabebe\u2217\nb\u2225q\u2032/2\n#!1/q\u2032\n.\nThe two terms in the right-hand side are essentially the same and if we can bound any one of them,\nthe same technique will apply to the other. We consider the \ufb01rst and since P\nab \u03b4abE2\nabeae\u2217\na is a\ndiagonal matrix,\n\u2225\nX\nab\n\u03b4abE2\nabeae\u2217\na\u2225= max\na\nX\nb\n\u03b4abE2\nab.\nThe following lemma bounds the qth moment of this quantity.\nLemma 6.2 Suppose that q is an integer obeying 1 \u2264q \u2264np and assume np \u22652 log n. Then\nE\u03b4\n \nmax\na\nX\nb\n\u03b4abE2\nab\n!q\n\u22642\n\u00002np \u2225E\u22252\n\u221e\n\u0001q .\n(6.6)\n25\nThe proof of this lemma is in the Appendix. The same estimate applies to E\n\u0000maxb\nP\na \u03b4abE2\nab\n\u0001q\nand thus for each q \u22651\nE\u03b4 max\n\"\n\u2225\nX\nab\n\u03b4abE2\nabeae\u2217\na\u2225q, \u2225\nX\nab\n\u03b4abE2\nabebe\u2217\nb\u2225q\n#\n\u22644\n\u00002np \u2225E\u22252\n\u221e\n\u0001q .\n(In the rectangular case, the same estimate holds with n = max(n1, n2).)\nTake q = \u03b2 log n for some \u03b2 \u22651, and set q\u2032 = q. Then since \u2225E\u2225\u221e\u2264\u00b51\n\u221ar/n, we established\nthat\n(E \u2225S\u2225q)1/q \u2264C 1\np\np\n\u03b2 log n \u221anp \u2225E\u2225\u221e= C \u00b51\nr\nnr \u03b2 log n\nm\n\u2261K0.\nThen by Markov\u2019s inequality, for each t > 0,\nP(\u2225S\u2225> tK0) \u2264t\u2212q,\nand for t = e, we conclude that\nP\n \n\u2225S\u2225> Ce \u00b51\nr\nnr \u03b2 log n\nm\n!\n\u2264n\u2212\u03b2\nwith the proviso that m \u2265max(\u03b2, 2) n log n so that Lemma 6.2 holds.\nWe have not made any assumption in this section about the matrix E (except that we have a\nbound on the maximum entry) and, therefore, have proved the theorem below, which shall be used\nmany times in the sequel.\nTheorem 6.3 Let X be a \ufb01xed n \u00d7 n matrix. There is a constant C0 such that for each \u03b2 > 2\np\u22121\u2225(P\u2126\u2212pI)(X)\u2225\u2264C0\n\u0012\u03b2n log n\np\n\u00131/2\n\u2225X\u2225\u221e\n(6.7)\nwith probability at least 1 \u2212n\u2212\u03b2 provided that np \u2265\u03b2 log n.\nNote that this is the same C0 described in Lemma 4.4.\n6.2\nProof of Lemma 4.5\nWe now need to bound the spectral norm of PT \u22a5P\u2126PT H(E) and will use some of the ideas\ndeveloped in the previous section. Just as before,\np\u22121\u2225PT \u22a5P\u2126PT H(E)\u2225\u2264p\u22121\u2225(P\u2126\u2212pI) H(E)\u2225,\nand put\nS \u2261p\u22121(P\u2126\u2212pI) H(E) = p\u22122 X\nab,a\u2032b\u2032\n\u03beab\u03bea\u2032b\u2032 Ea\u2032b\u2032\u27e8PT ea\u2032e\u2217\nb\u2032, eae\u2217\nb\u27e9eae\u2217\nb,\nwhere here and below, \u03beab \u2261\u03b4ab \u2212p. Decompose S as\nS\n= p\u22122\nX\n(a,b)=(a\u2032,b\u2032)\n+ p\u22122\nX\n(a,b) \u0338= (a\u2032,b\u2032)\n\u2261S0 + S1.\n(6.8)\n26\nWe bound the spectral norm of the diagonal and o\ufb00-diagonal contributions separately.\nWe begin with S0 and decompose (\u03beab)2 as\n\u03be2\nab = (\u03b4ab \u2212p)2 = (1 \u22122p)(\u03b4ab \u2212p) + p(1 \u2212p) = (1 \u22122p)\u03beab + p(1 \u2212p),\nwhich allows us to express S0 as\nS0 = 1 \u22122p\np\nX\nab\n\u03beab Habeae\u2217\nb + (1 \u2212p)\nX\nab\nHabeae\u2217\nb,\nHab \u2261p\u22121 Eab\u27e8PT eae\u2217\nb, eae\u2217\nb\u27e9.\n(6.9)\nTheorem 6.3 bounds the spectral norm of the \ufb01rst term of the right-hand side and we have\np\u22121\u2225\nX\nab\n\u03beab Habeae\u2217\nb\u2225\u2264C0\nr\nn3\u03b2 log n\nm\n\u2225H\u2225\u221e\nwith probability at least 1 \u2212n\u2212\u03b2. Now since \u2225E\u2225\u221e\u2264\u00b51\n\u221ar/n and |\u27e8PT eae\u2217\nb, eae\u2217\nb\u27e9| \u22642\u00b50r/n by\n(6.2), \u2225H\u2225\u221e\u2264\u00b50\u00b51(2r/np) \u221ar/n, and\np\u22121\u2225\nX\nab\n\u03beab Habeae\u2217\nb\u2225\u2264C\u00b50\u00b51\nnr\nm\nr\nnr\u03b2 log n\nm\nwith the same probability. The second term of the right-hand side in (6.9) is deterministic and we\ndevelop an argument that we will reuse several times. We record a useful lemma.\nLemma 6.4 Let X be a \ufb01xed matrix and set Z \u2261P\nab Xab\u27e8PT (eae\u2217\nb), eae\u2217\nb\u27e9eae\u2217\nb. Then\n\u2225Z\u2225\u22642\u00b50r\nn\n\u2225X\u2225.\nProof Let \u039bU and \u039bV be the diagonal matrices with entries \u2225PUea\u22252 and \u2225PV eb\u22252 respectively,\n\u039bU = diag(\u2225PUea\u22252),\n\u039bV = diag(\u2225PV eb\u22252).\n(6.10)\nTo bound the spectral norm of Z, observe that it follows from (4.7) that\nZ = \u039bUX + X\u039bV \u2212\u039bUX\u039bV = \u039bUX(I \u2212\u039bV ) + X\u039bV .\n(6.11)\nHence, since \u2225\u039bU\u2225and \u2225\u039bV \u2225are bounded by min(\u00b50r/n, 1) and \u2225I \u2212\u039bV \u2225\u22641, we have\n\u2225Z\u2225\u2264\u2225\u039bU\u2225\u2225X\u2225\u2225I \u2212\u039bV \u2225+ \u2225X\u2225\u2225\u039bV \u2225\u2264(2\u00b50r/n)\u2225X\u2225.\nClearly, this lemma and \u2225E\u2225= 1 give that H de\ufb01ned in (6.9) obeys \u2225H\u2225\u22642\u00b50r/np. In summary,\n\u2225S0\u2225\u2264C nr\nm\n \n\u00b50\u00b51\nr\n\u03b2nr log n\nm\n+ \u00b50\n!\nfor some C > 0 with the same probability as in Lemma 4.4.\nIt remains to bound the o\ufb00-diagonal term. To this end, we use a useful decoupling lemma:\n27\nLemma 6.5\n[16] Let {\u03b7i}1\u2264i\u2264n be a sequence of independent random variables, and {xij}i\u0338=j be\nelements taken from a Banach space. Then\nP(\u2225\nX\ni\u0338=j\n\u03b7i\u03b7jxij\u2225\u2265t) \u2264CD P(\u2225\nX\ni\u0338=j\n\u03b7i\u03b7\u2032\njxij\u2225> t/CD),\n(6.12)\nwhere {\u03b7\u2032\ni} is an independent copy of {\u03b7i}.\nThis lemma asserts that it is su\ufb03cient to estimate P(\u2225S\u2032\n1\u2225\u2265t) where S\u2032\n1 is given by\nS\u2032\n1 \u2261p\u22122 X\nab\u0338=a\u2032b\u2032\n\u03beab\u03be\u2032\na\u2032b\u2032 Ea\u2032b\u2032\u27e8PT ea\u2032e\u2217\nb\u2032, eae\u2217\nb\u27e9eae\u2217\nb\n(6.13)\nin which {\u03be\u2032\nab} is an independent copy of {\u03beab}. We write S\u2032\n1 as\nS\u2032\n1 = p\u22121 X\nab\n\u03beab Habeae\u2217\nb,\nHab \u2261p\u22121\nX\na\u2032b\u2032:(a\u2032,b\u2032)\u0338=(a,b)\n\u03be\u2032\na\u2032b\u2032 Ea\u2032b\u2032\u27e8PT ea\u2032e\u2217\nb\u2032, eae\u2217\nb\u27e9.\n(6.14)\nTo bound the tail of \u2225S\u2032\n1\u2225, observe that\nP(\u2225S\u2032\n1\u2225\u2265t) \u2264P(\u2225S\u2032\n1\u2225\u2265t | \u2225H\u2225\u221e\u2264K) + P(\u2225H\u2225\u221e> K).\nBy independence, the \ufb01rst term of the right-hand side is bounded by Theorem 6.3. On the event\n{\u2225H\u2225\u221e\u2264K}, we have\np\u22121\u2225\nX\nab\n\u03beab Habeae\u2217\nb\u2225\u2264C\nr\nn3\u03b2 log n\nm\nK.\nwith probability at least 1 \u2212n\u2212\u03b2. To bound \u2225H\u2225\u221e, we use Bernstein\u2019s inequality.\nLemma 6.6 Let X be a \ufb01xed matrix and de\ufb01ne Q(X) as the matrix whose (a, b)th entry is\n[Q(X)]ab = p\u22121\nX\na\u2032b\u2032:(a\u2032,b\u2032)\u0338=(a,b)\n(\u03b4a\u2032b\u2032 \u2212p) Xa\u2032b\u2032\u27e8PT ea\u2032e\u2217\nb\u2032, eae\u2217\nb\u27e9,\nwhere {\u03b4ab} is an independent Bernoulli sequence obeying P(\u03b4ab = 1) = p. Then\nP\n\u0012\n\u2225Q(X)\u2225\u221e> \u03bb\nr\u00b50r\nnp \u2225X\u2225\u221e\n\u0013\n\u22642n2 exp\n\uf8eb\n\uf8ed\u2212\n\u03bb2\n2 + 2\n3\nq\n\u00b50r\nnp \u03bb\n\uf8f6\n\uf8f8.\n(6.15)\nWith \u03bb = \u221a3\u03b2 log n, the right-hand side is bounded by 2n2\u2212\u03b2 provided that np \u22654\u03b2\n3 \u00b50r log n. In\nparticular, for \u03bb = \u221a6\u03b2 log n with \u03b2 > 2, the bound is less than 2n\u2212\u03b2 provided that np \u22658\u03b2\n3 \u00b50r log n.\nProof\nThe inequality (6.15) is an application of Bernstein\u2019s inequality, which states that for a\nsum of uniformly bounded independent zero-mean random variables obeying |Yk| \u2264c,\nP\n \n|\nn\nX\nk=1\nYk| > t\n!\n\u22642e\u2212t2/(2\u03c32+2ct/3),\n(6.16)\n28\nwhere \u03c32 is the sum of the variances, \u03c32 \u2261Pn\nk=1 Var(Yk). We have\nVar([Q(X)]ab) = 1 \u2212p\np\nX\na\u2032b\u2032:(a\u2032,b\u2032)\u0338=(a,b)\n|Xa\u2032b\u2032|2|\u27e8PT ea\u2032e\u2217\nb\u2032, eae\u2217\nb\u27e9|2\n\u22641 \u2212p\np\n\u2225X\u22252\n\u221e\nX\na\u2032b\u2032:(a\u2032,b\u2032)\u0338=(a,b)\n|\u27e8PT eae\u2217\nb, ea\u2032e\u2217\nb\u2032\u27e9|2 \u22641 \u2212p\np\n\u2225X\u22252\n\u221e2\u00b50r/n\nby (6.3). Also,\np\u22121 |(\u03b4a\u2032b\u2032 \u2212p)Xa\u2032b\u2032\u27e8PT ea\u2032e\u2217\nb\u2032, eae\u2217\nb\u27e9| \u2264p\u22121 \u2225X\u2225\u221e2\u00b50r/n\nand hence, for each t > 0, (6.16) gives\nP(|[Q(X)]ab| > t) \u22642 exp\n \n\u2212\nt2\n2\u00b50r\nnp \u2225X\u22252\u221e+ 2\n3\n\u00b50r\nnp \u2225X\u2225\u221et\n!\n.\n(6.17)\nPutting t = \u03bb\np\n\u00b50r/np\u2225X\u2225\u221efor some \u03bb > 0 and applying the union bound gives (6.15).\nSince \u2225E\u2225\u221e\u2264\u00b51\n\u221ar/n it follows that H = Q(E) introduced in (6.14) obeys\n\u2225H\u2225\u221e\u2264C \u00b51\n\u221ar\nn\nr\n\u00b50nr\u03b2 log n\nm\nwith probability at least 1 \u22122n\u2212\u03b2 for each \u03b2 > 2 and, therefore,\n\u2225S\u2032\n1\u2225\u2264C \u221a\u00b50\u00b51\nnr\u03b2 log n\nm\nwith probability at least 1 \u22123n\u2212\u03b2. In conclusion, we have\np\u22121\u2225(P\u2126\u2212pI) H(E)\u2225\u2264C nr\nm\n \n\u221a\u00b50\u00b51\n r\n\u00b50nr\u03b2 log n\nm\n+ \u03b2 log n\n!\n+ \u00b50\n!\n(6.18)\nwith probability at least 1 \u2212(1 + 3CD)n\u2212\u03b2. A simple algebraic manipulation concludes the proof\nof Lemma 4.5. Note that we have not made any assumption about the matrix E and, therefore,\nestablished the following:\nLemma 6.7 Let X be a \ufb01xed n \u00d7 n matrix. There is a constant C\u2032\n0 such that\np\u22122\u2225\nX\n(a,b)\u0338=(a\u2032,b\u2032)\n\u03beab\u03bea\u2032b\u2032Xab\u27e8PT (ea\u2032e\u2217\nb\u2032), eae\u2217\nb\u27e9eae\u2217\nb\u2225\u2264C\u2032\n0\n\u221a\u00b50r \u03b2 log n\np\n\u2225X\u2225\u221e\n(6.19)\nwith probability at least 1 \u2212O(n\u2212\u03b2) for all \u03b2 > 2 provided that np \u22653\u00b50r\u03b2 log n.\n29\n6.3\nProof of Lemma 4.6\nTo prove Lemma 4.6, we need to bound the spectral norm of p\u22121 (P\u2126\u2212pI) H2(E), a matrix given\nby\np\u22123\nX\na1b1,a2b2,a3b3\n\u03bea1b1\u03bea2b2\u03bea3b3Ea3b3\u27e8PT ea3e\u2217\nb3, ea2e\u2217\nb2\u27e9\u27e8PT ea2e\u2217\nb2, ea1e\u2217\nb1\u27e9ea1e\u2217\nb1,\nwhere \u03beab = \u03b4ab \u2212p as before. It is convenient to introduce notations to compress this expression.\nSet \u03c9 = (a, b) (and \u03c9i = (ai, bi) for i = 1, 2, 3), F\u03c9 = eae\u2217\nb, and P\u03c9\u2032\u03c9 = \u27e8PT ea\u2032e\u2217\nb\u2032, eae\u2217\nb\u27e9so that\np\u22121 (P\u2126\u2212pI) H2(E) = p\u22123\nX\n\u03c91,\u03c92,\u03c93\n\u03be\u03c91\u03be\u03c92\u03be\u03c93 E\u03c93P\u03c93\u03c92P\u03c92\u03c91F\u03c91.\nPartition the sum depending on whether some of the \u03c9i\u2019s are the same or not\n1\np(P\u2126\u2212pI)H2(E) = 1\np3\n\uf8ee\n\uf8f0\nX\n\u03c91=\u03c92=\u03c93\n+\nX\n\u03c91\u0338=\u03c92=\u03c93\n+\nX\n\u03c91=\u03c93\u0338=\u03c92\n+\nX\n\u03c91=\u03c92\u0338=\u03c93\n+\nX\n\u03c91\u0338=\u03c92\u0338=\u03c93\n\uf8f9\n\uf8fb.\n(6.20)\nThe meaning should be clear; for instance, the sum P\n\u03c91\u0338=\u03c92=\u03c93 is the sum over the \u03c9\u2019s such that\n\u03c92 = \u03c93 and \u03c91 \u0338= \u03c92. Similarly, P\n\u03c91\u0338=\u03c92\u0338=\u03c93 is the sum over the \u03c9\u2019s such that they are all distinct.\nThe idea is now to use a decoupling argument to bound each sum in the right-hand side of (6.20)\n(except for the \ufb01rst which does not need to be decoupled) and show that all terms are appropriately\nsmall in the spectral norm.\nWe begin with the \ufb01rst term which is equal to\n1\np3\nX\n\u03c9\n(\u03be\u03c9)3 E\u03c9P 2\n\u03c9\u03c9F\u03c9 = 1 \u22123p + 3p2\np3\nX\n\u03c9\n\u03be\u03c9 E\u03c9P 2\n\u03c9\u03c9F\u03c9 + 1 \u22123p + 2p2\np2\nX\n\u03c9\nE\u03c9P 2\n\u03c9\u03c9F\u03c9,\n(6.21)\nwhere we have used the identity\n(\u03be\u03c9)3 = (1 \u22123p + 3p2)\u03be\u03c9 + p(1 \u22123p + 2p2).\nSet H\u03c9 = E\u03c9(p\u22121P\u03c9\u03c9)2. For the \ufb01rst term in the right-hand side of (6.21), we need to control\n\u2225P\n\u03c9 \u03be\u03c9 H\u03c9F\u03c9\u2225. This is easily bounded by Theorem 6.3. Indeed, it follows from\n|H\u03c9| \u2264\n\u00122\u00b50r\nnp\n\u00132\n\u2225E\u2225\u221e\nthat for each \u03b2 > 0,\np\u22121\u2225\nX\n\u03c9\n\u03be\u03c9 H\u03c9 F\u03c9\u2225\u2264C\n\u0010\u00b50nr\nm\n\u00112\n\u00b51\nr\nnr\u03b2 log n\nm\n= C \u00b52\n0\u00b51\np\n\u03b2 log n\n\u0010nr\nm\n\u00115/2\nwith probably at least 1 \u2212n\u2212\u03b2. For the second term in the right-hand side of (6.21), we apply\nLemma 6.4 which gives\n\u2225\nX\n\u03c9\nE\u03c9P 2\n\u03c9\u03c9F\u03c9\u2225\u2264(2\u00b50r/n)2\n30\nso that \u2225H\u2225\u2264(2\u00b50r/np)2. In conclusion, the \ufb01rst term in (6.20) has a spectral norm which is\nbounded by\nC\n\u0010nr\nm\n\u00112\n \n\u00b52\n0\u00b51\n\u0012nr\u03b2 log n\nm\n\u00131/2\n+ \u00b52\n0\n!\nwith probability at least 1 \u2212n\u2212\u03b2.\nWe now turn our attention to the second term which can be written as\np\u22123 X\n\u03c91\u0338=\u03c92\n\u03be\u03c91(\u03be\u03c92)2 E\u03c92P\u03c92\u03c92P\u03c92\u03c91F\u03c91 = 1 \u22122p\np3\nX\n\u03c91\u0338=\u03c92\n\u03be\u03c91\u03be\u03c92 E\u03c92P\u03c92\u03c92P\u03c92\u03c91F\u03c91\n+ 1 \u2212p\np2\nX\n\u03c91\u0338=\u03c92\n\u03be\u03c91 E\u03c92P\u03c92\u03c92P\u03c92\u03c91F\u03c91.\nPut S1 for the \ufb01rst term; bounding \u2225S1\u2225is a simple application of Lemma 6.7 with X\u03c9 = p\u22121E\u03c9P\u03c9\u03c9,\nwhich gives\n\u2225S1\u2225\u2264C \u00b53/2\n0\n\u00b51 (\u03b2 log n)\n\u0010nr\nm\n\u00112\nsince \u2225E\u2225\u221e\u2264\u00b51\n\u221ar/n. For the second term, we need to bound the spectral norm of S2 where\nS2 \u2261p\u22121 X\n\u03c91\n\u03be\u03c91H\u03c91F\u03c91,\nH\u03c91 = p\u22121\nX\n\u03c92:\u03c92\u0338=\u03c91\nE\u03c92P\u03c92\u03c92P\u03c92\u03c91.\nNote that H is deterministic. The lemma below provides an estimate about \u2225H\u2225\u221e.\nLemma 6.8 The matrix H obeys\n\u2225H\u2225\u221e\u2264\u00b50r\nnp\n\u0010\n3\u2225E\u2225\u221e+ 2\u00b50r\nn\n\u0011\n.\n(6.22)\nProof We begin by rewriting H as\npH\u03c9 =\nX\n\u03c9\u2032\nE\u03c9\u2032P\u03c9\u2032\u03c9\u2032P\u03c9\u2032\u03c9 \u2212E\u03c9P 2\n\u03c9\u03c9.\nClearly, |E\u03c9P 2\n\u03c9\u03c9| \u2264(\u00b50r/n)2\u2225E\u2225\u221eso that it su\ufb03ces to bound the \ufb01rst term, which is the \u03c9th entry\nof the matrix\nX\n\u03c9,\u03c9\u2032\nE\u03c9\u2032P\u03c9\u2032\u03c9\u2032P\u03c9\u2032\u03c9F\u03c9 = PT (\u039bUE + E\u039bV \u2212\u039bUE\u039bV ).\nNow it is immediate to see that \u039bUE \u2208T and likewise for E\u039bV . Hence,\n\u2225PT (\u039bUE + E\u039bV \u2212\u039bUE\u039bV )\u2225\u221e\u2264\u2225\u039bUE\u2225\u221e+ \u2225E\u039bV \u2225\u221e+ \u2225PT (\u039bUE\u039bV )\u2225\u221e\n\u22642\u2225E\u2225\u221e\u00b50r/n + \u2225PT (\u039bUE\u039bV )\u2225\u221e.\nWe \ufb01nally use the crude estimate\n\u2225PT (\u039bUE\u039bV )\u2225\u221e\u2264\u2225PT (\u039bUE\u039bV )\u2225\u22642\u2225\u039bUE\u039bV \u2225\u22642(\u00b50r/n)2\nto complete the proof of the lemma.\n31\nAs a consequence of this lemma, Theorem 6.3 gives\n\u2225S2\u2225\u2264C\np\n\u03b2 log n\n\u0010nr\nm\n\u00113/2\n(\u00b50\u00b51 + \u00b52\n0\n\u221ar)\nwith probability at least 1 \u2212n\u2212\u03b2.\nIn conclusion, the second term in (6.20) has spectral norm\nbounded by\nC\np\n\u03b2 log n\n\u0010nr\nm\n\u00113/2\n \n\u00b50\u00b51\nr\n\u00b50nr\u03b2 log n\nm\n+ \u00b50\u00b51 + \u00b52\n0\n\u221ar\n!\nwith probability at least 1 \u2212O(n\u2212\u03b2).\nWe now examine the third term which can be written as\np\u22123 X\n\u03c91\u0338=\u03c92\n(\u03be\u03c91)2\u03be\u03c92 E\u03c91P\u03c91\u03c92P\u03c92\u03c91F\u03c91 = 1 \u22122p\np3\nX\n\u03c91\u0338=\u03c92\n\u03be\u03c91\u03be\u03c92 E\u03c91P 2\n\u03c92\u03c91F\u03c91\n+ 1 \u2212p\np2\nX\n\u03c91\u0338=\u03c92\n\u03be\u03c92 E\u03c91P 2\n\u03c92\u03c91F\u03c91.\nWe use the decoupling argument once more so that for the \ufb01rst term of the right-hand side, it\nsu\ufb03ces to estimate the tail of the norm of\nS1 \u2261p\u22121 X\n\u03c91\n\u03be(1)\n\u03c91 E\u03c91H\u03c91F\u03c91,\nH\u03c91 \u2261p\u22122\nX\n\u03c92:\u03c92\u0338=\u03c91\n\u03be(2)\n\u03c92 P 2\n\u03c92\u03c91,\nwhere {\u03be(1)\n\u03c9 } and {\u03be(2)\n\u03c9 } are independent copies of {\u03be\u03c9}. It follows from Bernstein\u2019s inequality and\nthe estimates\n|P\u03c92\u03c91| \u22642\u00b50r/n\nand\nX\n\u03c92:\u03c92\u0338=\u03c91\n|P\u03c92\u03c91|4 \u2264\nmax\n\u03c92:\u03c92\u0338=\u03c91 |P\u03c92\u03c91|2\nX\n\u03c92:\u03c92\u0338=\u03c91\n|P\u03c92\u03c91|2 \u2264\n\u00122\u00b50r\nn\n\u00132 2\u00b50r\nn\nthat for each \u03bb > 0,4\nP\n \n|H\u03c91| > \u03bb\n\u00122\u00b50r\nnp\n\u00133/2!\n\u22642 exp\n\uf8eb\n\uf8ec\n\uf8ed\u2212\n\u03bb2\n2 + 2\n3\u03bb\n\u0010\n2\u00b50r\nnp\n\u00111/2\n\uf8f6\n\uf8f7\n\uf8f8.\nIt is now not hard to see that this inequality implies that\nP\n \n\u2225H\u2225\u221e>\np\n8\u03b2 log n\n\u00122\u00b50nr\nm\n\u00133/2!\n\u22642 n\u22122\u03b2+2\nprovided that m \u226516\n9 \u00b50nr \u03b2 log n. As a consequence, for each \u03b2 > 2, Theorem 6.3 gives\n\u2225S1\u2225\u2264C \u00b53/2\n0\n\u00b51 \u03b2 log n\n\u0010nr\nm\n\u00112\n4We would like to remark that one can often get better estimates; when \u03c91 \u0338= \u03c92, the bound |P\u03c92\u03c91| \u22642\u00b50r/n\nmay be rather crude. Indeed, one can derive better estimates for the random orthogonal model, for example.\n32\nwith probability at least 1 \u22123n\u2212\u03b2. The other term is equal to (1 \u2212p) times P\n\u03c91 E\u03c91H\u03c91F\u03c91, and\n\u2225\nX\n\u03c91\nE\u03c91H\u03c91F\u03c91\u2225\u2264\u2225\nX\n\u03c91\nE\u03c91H\u03c91F\u03c91\u2225F\n\u2264\u2225H\u2225\u221e\u2225E\u2225F \u2264C\np\n\u03b2 log n\n\u0010\u00b50nr\nm\n\u00113/2 \u221ar.\nIn conclusion, the third term in (6.20) has spectral norm bounded by\nC \u00b50\np\n\u03b2 log n\n\u0010nr\nm\n\u00113/2\n \n\u00b51\nr\n\u00b50nr\u03b2 log n\nm\n+ \u221a\u00b50r\n!\nwith probability at least 1 \u2212O(n\u2212\u03b2).\nWe proceed to the fourth term which can be written as\np\u22123 X\n\u03c91\u0338=\u03c93\n(\u03be\u03c91)2\u03be\u03c93 E\u03c93P\u03c93\u03c91P\u03c91\u03c91F\u03c91 = 1 \u22122p\np3\nX\n\u03c91\u0338=\u03c93\n\u03be\u03c91\u03be\u03c93 E\u03c93P\u03c93\u03c91P\u03c91\u03c91F\u03c91\n+ 1 \u2212p\np2\nX\n\u03c91\u0338=\u03c93\n\u03be\u03c93 E\u03c93P\u03c93\u03c91P\u03c91\u03c91F\u03c91.\nLet S1 be the \ufb01rst term and set H\u03c91 = p\u22122 P\n\u03c91\u0338=\u03c93 \u03be\u03c91\u03be\u03c93 E\u03c93P\u03c93\u03c91F\u03c91. Then Lemma 6.4 gives\n\u2225S1\u2225\u22642\u00b50r\nnp \u2225H\u2225\u2264C \u00b53/2\n0\n\u00b51 (\u03b2 log n)\n\u0010nr\nm\n\u00112\nwhere the last inequality is given by Lemma 6.7.\nFor the other term\u2014call it S2\u2014set H\u03c91 =\np\u22121 P\n\u03c93:\u03c93\u0338=\u03c91 \u03be\u03c93 E\u03c93P\u03c93\u03c91. Then Lemma 6.4 gives\n\u2225S2\u2225\u22642\u00b50r\nnp \u2225H\u2225.\nNotice that H\u03c91 = p\u22121 P\n\u03c93 \u03be\u03c93 E\u03c93P\u03c93\u03c91 \u2212p\u22121\u03be\u03c91E\u03c91P\u03c91\u03c91 so that with G\u03c91 = E\u03c91P\u03c91\u03c91\nH = p\u22121[PT (P\u2126\u2212pI)(E) \u2212(P\u2126\u2212pI)(G)].\nNow for any matrix X, \u2225PT (X)\u2225= \u2225X \u2212PT \u22a5(X)\u2225\u22642\u2225X\u2225and, therefore,\n\u2225H\u2225\u22642p\u22121\u2225(P\u2126\u2212pI)(E)\u2225+ p\u22121\u2225(P\u2126\u2212pI)(G)\u2225.\nAs a consequence and since \u2225G\u2225\u221e\u2264\u2225E\u2225\u221e, Theorem 6.3 gives for each \u03b2 > 2,\n\u2225H\u2225\u2264C\u00b51\nr\nnr\u03b2 log n\nm\nwith probability at least 1 \u2212n\u2212\u03b2.\nIn conclusion, the fourth term in (6.20) has spectral norm\nbounded by\nC \u00b50\u00b51\np\n\u03b2 log n\n\u0010nr\nm\n\u00113/2\n r\n\u00b50nr\u03b2 log n\nm\n+ 1\n!\n33\nwith probability at least 1 \u2212O(n\u2212\u03b2).\nWe \ufb01nally examine the last term\np\u22123\nX\n\u03c91\u0338=\u03c92\u0338=\u03c93\n\u03be\u03c91\u03be\u03c92\u03be\u03c93 E\u03c93P\u03c93\u03c92P\u03c92\u03c91F\u03c91.\nNow just as one has a decoupling inequality for pairs of variables, we have a decoupling inequality\nfor triples as well and we thus simply need to bound the tail of\nS1 \u2261p\u22123\nX\n\u03c91\u0338=\u03c92\u0338=\u03c93\n\u03be(1)\n\u03c91 \u03be(2)\n\u03c92 \u03be(3)\n\u03c93 E\u03c93P\u03c93\u03c92P\u03c92\u03c91F\u03c91\nin which the sequences {\u03be(1)\n\u03c9 }, {\u03be(2)\n\u03c9 } and {\u03be(3)\n\u03c9 } are independent copies of {\u03be\u03c9}. We refer to [16]\nfor details. We now argue as in Section 6.2 and write S1 as\nS1 = p\u22121 X\n\u03c91\n\u03be(1)\n\u03c91 H\u03c91F\u03c91,\nwhere\nH\u03c91 \u2261p\u22121\nX\n\u03c92:\u03c92\u0338=\u03c91\n\u03be(2)\n\u03c92 G\u03c92 P\u03c92\u03c91,\nG\u03c92 \u2261p\u22121\nX\n\u03c93:\u03c93\u0338=\u03c91,\u03c93\u0338=\u03c92\n\u03be(3)\n\u03c93 E\u03c93 P\u03c93\u03c92.\n(6.23)\nBy Lemma 6.6, we have for each \u03b2 > 2\n\u2225G\u2225\u221e\u2264C\nr\n\u00b50nr\u03b2 log n\nm\n\u2225E\u2225\u221e\nwith large probability and the same argument then gives\n\u2225H\u2225\u221e\u2264C\nr\n\u00b50nr\u03b2 log n\nm\n\u2225G\u2225\u221e\u2264C \u00b50nr\u03b2 log n\nm\n\u2225E\u2225\u221e\nwith probability at least 1 \u22124n\u2212\u03b2. As a consequence, Theorem 6.3 gives\n\u2225S\u2225\u2264C \u00b50\u00b51\n\u0012nr\u03b2 log n\nm\n\u00133/2\nwith probability at least 1 \u2212O(n\u2212\u03b2).\nTo summarize the calculations of this section and using the fact that \u00b50 \u22651 and \u00b51 \u2264\u00b50\n\u221ar,\nwe have established that if m \u2265\u00b50 nr(\u03b2 log n),\np\u22121\u2225(P\u2126\u2212pI) H2(E)\u2225\u2264C\n\u0010nr\nm\n\u00112\n \n\u00b52\n0\u00b51\nr\nnr\u03b2 log n\nm\n+ \u00b52\n0\n!\n+ C\np\n\u03b2 log n\n\u0010nr\nm\n\u00113/2\n\u00b52\n0\n\u221ar + C\n\u0012nr\u03b2 log n\nm\n\u00133/2\n\u00b50\u00b51\nwith probability at least 1\u2212O(n\u2212\u03b2). One can check that if m = \u03bb \u00b54/3\n0\nnr4/3\u03b2 log n for a \ufb01xed \u03b2 \u22652\nand \u03bb \u22651, then there is a constant C such that\n\u2225p\u22121 (P\u2126\u2212pI) H2(E)\u2225\u2264C\u03bb\u22123/2\nwith probability at least 1 \u2212O(n\u2212\u03b2). This is the content of Lemma 4.6.\n34\n6.4\nProof of Lemma 4.7\nClearly, one could continue on the same path and estimate the spectral norm of p\u22121(P\u2126\u2212pI) H3(E)\nby the same technique as in the previous sections. That is to say, we would write\np\u22121(P\u2126\u2212pI) H3(E) = p\u22124\nX\n\u03c91,\u03c92,\u03c93,\u03c94\n\" 4\nY\ni=1\n\u03be\u03c9i\n#\nE\u03c94\n\" 3\nY\ni=1\nP\u03c9i+1\u03c9i\n#\nF\u03c91\nwith the same notations as before, and partition the sum depending on whether some of the \u03c9i\u2019s\nare the same or not. Then we would use the decoupling argument to bound each term in the sum.\nAlthough this is a clear possibility, one would need to consider 18 cases and the calculations would\nbecome a little laborious. In this section, we propose to bound the term p\u22121(P\u2126\u2212pI) H3(E) with\na di\ufb00erent argument which has two main advantages: \ufb01rst, it is much shorter and second, it uses\nmuch of what we have already established. The downside is that it is not as sharp.\nThe starting point is to note that\np\u22121(P\u2126\u2212pI) H3(E) = p\u22121(\u039e \u25e6H3(E)),\nwhere \u039e is the matrix with i.i.d. entries equal to \u03beab = \u03b4ab \u2212p and \u25e6denotes the Hadamard product\n(componentwise multiplication). To bound the spectral norm of this Hadamard product, we apply\nan inequality due to Ando, Horn, and Johnson [4]. An elementary proof can be found in \u00a75.6 of\n[19].\nLemma 6.9\n[19] Let A and B be two n1 \u00d7 n2 matrices. Then\n\u2225A \u25e6B\u2225\u2264\u2225A\u2225\u03bd(B),\n(6.24)\nwhere \u03bd is the function\n\u03bd(B) = inf{c(X)c(Y ) : XY \u2217= B},\nand c(X) is the maximum Euclidean norm of the rows\nc(X)2 = max\n1\u2264i\u2264n\nX\nj\nX2\nij.\nTo apply (6.24), we \ufb01rst notice that one can estimate the norm of \u039e via Theorem 6.3. Indeed,\nlet Z = 11\u2217be the matrix with all entries equal to one. Then p\u22121\u039e = p\u22121(P\u2126\u2212pI)(Z) and thus\np\u22121\u2225\u039e\u2225\u2264C\n\u0012n3\u03b2 log n\nm\n\u00131/2\n(6.25)\nwith probability at least 1 \u2212n\u2212\u03b2. One could obtain a similar result by appealing to the recent\nliterature on random matrix theory and on concentration of measure. Potentially this could allow\nto derive an upper bound without the logarithmic term but we will not consider these re\ufb01nements\nhere. (It is interesting to note in passing, however, that the two page proof of Theorem 6.3 gives a\nlarge deviation result about the largest singular value of a matrix with i.i.d. entries which is sharp\nup to a multiplicative factor proportional to at most \u221alog n.)\nSecond, we bound the second factor in (6.24) via the following estimate:\n35\nLemma 6.10 There are numerical constants C and c so that for each \u03b2 > 2, H3(E) obeys\n\u03bd(H3(E)) \u2264C\u00b50r/n\n(6.26)\nwith probability at least 1 \u2212O(n\u2212\u03b2) provided that m \u2265c \u00b54/3\n0\nnr5/3(\u03b2 log n).\nThe two inequalities (6.25) and (6.26) give\np\u22121\u2225\u039e \u25e6H3(E)\u2225\u2264C\nr\n\u00b52\n0 nr2 \u03b2 log n\nm\n,\nwith large probability. Hence, when m is substantially larger than a constant times \u00b52\n0nr2(\u03b2 log n),\nwe have that the spectral norm of p\u22121(P\u2126\u2212pI) H3(E) is much less than 1. This is the content of\nLemma 4.7.\nThe remainder of this section proves Lemma 6.10. Set S \u2261H3(E) for short. Because S is in\nT, S = PT (S) = PUS + SPV \u2212PUSPV . Writing PU = Pr\nj=1 uju\u2217\nj and similarly for PV gives\nS =\nr\nX\nj=1\nuj(u\u2217\njS) +\nr\nX\nj=1\n((I \u2212PU)Svj)v\u2217\nj .\nFor each 1 \u2264j \u2264r, let \u03b1j \u2261Svj and \u03b2\u2217\nj \u2261u\u2217\njS. Then the decomposition\nS =\nr\nX\nj=1\nuj\u03b2\u2217\nj +\nr\nX\nj=1\n(PU\u22a5\u03b1j)v\u2217\nj ,\nwhere PU\u22a5= I \u2212PU, provides a factorization of the form\nS = XY \u2217,\n(\nX = [u1, . . . , ur, PU\u22a5\u03b11, . . . , PU\u22a5\u03b1r],\nY = [v1, . . . , vr, \u03b21, . . . , \u03b2r].\nIt follows from our assumption that\nc2([u1, . . . , ur]) = max\n1\u2264i\u2264n\nX\n1\u2264j\u2264r\nu2\nij = max\n1\u2264i\u2264n \u2225PUei\u22252 \u2264\u00b50r/n,\nand similarly for [v1, . . . , vr]. Hence, to prove Lemma 6.10, it su\ufb03ces to prove that the maximum\nrow norm obeys c([\u03b21, . . . , \u03b2r]) \u2264C\np\n\u00b50r/n for some constant C > 0, and similarly for the matrix\n[PU\u22a5\u03b11, . . . , PU\u22a5\u03b1r].\nLemma 6.11 There is a numerical constant C such that for each \u03b2 > 2,\nc([\u03b11, . . . , \u03b1r]) \u2264C\np\n\u00b50r/n\n(6.27)\nwith probability at least 1 \u2212O(n\u2212\u03b2) provided that m obeys the condition of Lemma 6.10.\nA similar estimate for [\u03b21, . . . , \u03b2r] is obtained in the same way by exchanging the roles of u and v.\nMoreover, a minor modi\ufb01cation of the argument gives\nc([PU\u22a5\u03b11, . . . , PU\u22a5\u03b1r]) \u2264C\np\n\u00b50r/n\n(6.28)\n36\nas well, and we will omit the details. In short, the estimate (6.27) implies Lemma 6.10.\nProof [of Lemma 6.11] To prove (6.27), we use the notations of the previous section and write\n\u03b1j = p\u22123\nX\na1b1,a2b2,a3b3\n\u03bea1b1\u03bea2b2\u03bea3b3Ea3b3\u27e8PT ea3e\u2217\nb3, ea2e\u2217\nb2\u27e9\u27e8PT ea2e\u2217\nb2, ea1e\u2217\nb1\u27e9PT (ea1e\u2217\nb1)vj\n= p\u22123\nX\n\u03c91,\u03c92,\u03c93\n\u03be\u03c91\u03be\u03c92\u03be\u03c93 E\u03c93P\u03c93\u03c92P\u03c92\u03c91PT (F\u03c91)vj\n= p\u22123\nX\n\u03c91,\u03c92,\u03c93\n\u03be\u03c91\u03be\u03c92\u03be\u03c93 E\u03c93P\u03c93\u03c92P\u03c92\u03c91(F\u03c91vj)\nsince for any matrix X, PT (X)vj = Xvj for each 1 \u2264j \u2264r. We then follow the same steps as in\nSection 6.3 and partition the sum depending on whether some of the \u03c9i\u2019s are the same or not\n\u03b1j = p\u22123\n\uf8ee\n\uf8f0\nX\n\u03c91=\u03c92=\u03c93\n+\nX\n\u03c91\u0338=\u03c92=\u03c93\n+\nX\n\u03c91=\u03c93\u0338=\u03c92\n+\nX\n\u03c91=\u03c92\u0338=\u03c93\n+\nX\n\u03c91\u0338=\u03c92\u0338=\u03c93\n\uf8f9\n\uf8fb.\n(6.29)\nThe idea is this: to establish (6.27), it is su\ufb03cient to show that if \u03b3j is any of the \ufb01ve terms above,\nit obeys\ns X\n1\u2264j\u2264r\n|\u03b3ij|2 \u2264C\np\n\u00b50r/n\n(6.30)\n(\u03b3ij is the ith component of \u03b3j as usual) with large probability. The strategy for getting such\nestimates is to use decoupling whenever applicable.\nJust as Theorem 6.3 proved useful to bound the norm of p\u22121(P\u2126\u2212pI)H2(E) in Section 6.3,\nthe lemma below will help bounding the magnitudes of the components of \u03b1j.\nLemma 6.12 De\ufb01ne S \u2261p\u22121 P\nij\nP\n\u03c9 \u03be\u03c9H\u03c9\u27e8ei, F\u03c9vj\u27e9eie\u2217\nj. Then for each \u03bb > 0\nP(\u2225S\u2225\u221e\u2265\np\n\u00b50/n) \u22642n2 exp\n \n\u2212\n1\n2n\n\u00b50p\u2225H\u22252\u221e+ 2\n3p\n\u221ar\u2225H\u2225\u221e\n!\n.\n(6.31)\nProof The proof is an application of Bernstein\u2019s inequality (6.16). Note that \u27e8ei, F\u03c9vj\u27e9= 1{a=i}vbj\nand hence\nVar(Sij) \u2264p\u22121\u2225H\u22252\n\u221e\nX\n\u03c9\n|\u27e8ei, F\u03c9vj\u27e9|2 = p\u22121\u2225H\u22252\n\u221e\nsince P\n\u03c9 |\u27e8ei, F\u03c9vj\u27e9|2 = 1, and |p\u22121H\u03c9\u27e8ei, F\u03c9vj\u27e9| \u2264p\u22121 \u2225H\u2225\u221e\np\n\u00b50r/n since |\u27e8ei, F\u03c9vj\u27e9| \u2264|vbj|\nand\n|vbj| \u2264\u2225PV eb\u2225\u2264\np\n\u00b50r/n.\nEach term in (6.29) is given by the corresponding term in (6.20) after formally substituting F\u03c9\nwith F\u03c9vj. We begin with the \ufb01rst term whose ith component is equal to\n\u03b3ij \u2261p\u22123(1 \u22123p + 3p2)\nX\n\u03c9\n\u03be\u03c9 E\u03c9P 2\n\u03c9\u03c9\u27e8ei, F\u03c9vj\u27e9+ p\u22122(1 \u22123p + 2p2)\nX\n\u03c9\nE\u03c9P 2\n\u03c9\u03c9\u27e8ei, F\u03c9vj\u27e9.\n(6.32)\n37\nIgnoring the constant factor (1 \u22123p + 3p2) which is bounded by 1, we write the \ufb01rst of these two\nterms as\n(S0)ij \u2261p\u22121 X\n\u03c9\n\u03be\u03c9 H\u03c9\u27e8ei, F\u03c9vj\u27e9,\nH\u03c9 = E\u03c9 (p\u22121P\u03c9\u03c9)2.\nSince \u2225H\u2225\u221e\u2264(\u00b50nr/m)2 \u00b51\n\u221ar/n, it follows from Lemma (6.12) that\nP\n\u0010\n\u2225S0\u2225\u221e\u2265\np\n\u00b50/n\n\u0011\n\u22642n2 e\u22121/D,\nD \u2264C\n\u0012\n\u00b53\n0\u00b52\n1\n\u0010nr\nm\n\u00115\n+ \u00b52\n0\u00b51\n\u0010nr\nm\n\u00113\u0013\nfor some numerical C > 0. Since \u00b51 \u2264\u00b50\n\u221ar, we have that when m \u2265\u03bb\u00b50 nr6/5(\u03b2 log n) for some\nnumerical constant \u03bb > 0, \u2225S0\u2225\u221e\u2265\np\n\u00b50/n with probability at most 2n2e\u2212(\u03b2 log n)3; this probability\nis inversely proportional to a superpolynomial in n. For the second term, the matrix with entries\nE\u03c9P 2\n\u03c9\u03c9 is given by\n\u039b2\nUE + E\u039b2\nV + 2\u039bUE\u039bV + \u039b2\nUE\u039b2\nV \u22122\u039b2\nUE\u039bV \u22122\u039bUE\u039b2\nV\nand thus\nX\n\u03c9\nE\u03c9P 2\n\u03c9\u03c9\u27e8ei, F\u03c9vj\u27e9= \u27e8ei, (\u039b2\nUE + E\u039b2\nV + 2\u039bUE\u039bV + \u039b2\nUE\u039b2\nV \u22122\u039b2\nUE\u039bV \u22122\u039bUE\u039b2\nV )vj\u27e9.\nThis is a sum of six terms and we will show how to bound the \ufb01rst three; the last three are dealt\nin exactly the same way and obey better estimates. For the \ufb01rst, we have\n\u27e8ei, \u039b2\nUEvj\u27e9= \u27e8\u039b2\nUei, Evj\u27e9= \u2225PUei\u22254\u27e8ei, uj\u27e9\nHence\np\u22122\ns X\n1\u2264j\u2264r\n|\u27e8ei, \u039b2\nUEvj\u27e9|2 = p\u22122\u2225PUei\u22254\ns X\n1\u2264j\u2264r\n|\u27e8ei, uj\u27e9|2 = p\u22122\u2225PUei\u22255 \u2264\n\u0012\u00b50r\nnp\n\u00132 r\u00b50r\nn .\nIn other words, when m \u2265\u00b50nr, the right hand-side is bounded by\np\n\u00b50r/n as desired. For the\nsecond term, we have\n\u27e8ei, E\u039b2\nV vj\u27e9=\nX\nb\n\u2225PV eb\u22254vbj\u27e8ei, Eeb\u27e9=\nX\nb\n\u2225PV eb\u22254vbjEib.\nHence it follows from the Cauchy-Schwarz inequality and (6.4) that\np\u22122|\u27e8ei, E\u039b2\nV vj\u27e9| \u2264\n\u0012\u00b50r\nnp\n\u00132 r\u00b50r\nn .\nIn other words, when m \u2265\u00b50nr5/4,\np\u22122\ns X\n1\u2264j\u2264r\n|\u27e8ei, E\u039b2\nV vj\u27e9|2 \u2264\nr\u00b50r\nn\n(6.33)\nas desired. For the third term, we have\n\u27e8ei, \u039bUE\u039bV vj\u27e9= \u2225PUei\u22252 X\nb\n\u2225PV eb\u22252vbjEib.\n38\nThe Cauchy-Schwarz inequality gives\n2p\u22122|\u27e8ei, \u039bUE\u039bV vj\u27e9| \u22642\n\u0012\u00b50r\nnp\n\u00132 r\u00b50r\nn\njust as before. In other words, when m \u2265\u00b50nr5/4, 2p\u22122qP\n1\u2264j\u2264r |\u27e8ei, \u039bUE\u039bV vj\u27e9|2 is bounded by\n2\np\n\u00b50r/n. The other terms obey (6.33) as well when m \u2265\u00b50nr5/4. In conclusion, the \ufb01rst term\n(6.32) in (6.29) obeys (6.30) with probability at least 1\u2212O(n\u2212\u03b2) provided that m \u2265\u00b50nr5/4(\u03b2 log n).\nWe now turn our attention to the second term which can be written as\n\u03b3ij \u2261p\u22123(1 \u22122p)\nX\n\u03c91\u0338=\u03c92\n\u03be\u03c91\u03be\u03c92 E\u03c92P\u03c92\u03c92P\u03c92\u03c91\u27e8ei, F\u03c91vj\u27e9\n+ p\u22122(1 \u2212p)\nX\n\u03c91\u0338=\u03c92\n\u03be\u03c91 E\u03c92P\u03c92\u03c92P\u03c92\u03c91\u27e8ei, F\u03c91vj\u27e9.\nWe decouple the \ufb01rst term so that it su\ufb03ces to bound\n(S0)ij \u2261p\u22121 X\n\u03c91\n\u03be(1)\n\u03c91 H\u03c91\u27e8ei, F\u03c91vj\u27e9,\nH\u03c91 \u2261p\u22122\nX\n\u03c92:\u03c92\u0338=\u03c91\n\u03be(2)\n\u03c92 E\u03c92P\u03c92\u03c92P\u03c92\u03c91,\nwhere the sequences {\u03be(1)\n\u03c9 } and {\u03be(2)\n\u03c9 } are independent. The method from Section 6.2 shows that\n\u2225H\u2225\u221e\u2264C\nr\n\u00b50nr\u03b2 log n\nm\nsup\n\u03c9 |E\u03c9(p\u22121P\u03c9\u03c9)| \u2264C\np\n\u03b2 log n\n\u0010\u00b50nr\nm\n\u00113/2\n\u2225E\u2225\u221e\nwith probability at least 1 \u22122n\u2212\u03b2 for each \u03b2 > 2. Therefore, Lemma 6.12 gives\nP\n\u0010\n\u2225S0\u2225\u221e\u2265\np\n\u00b50/n\n\u0011\n\u22642n2e\u22121/D,\n(6.34)\nwhere D obeys\nD \u2264C\n\u0012\n\u00b52\n0\u00b52\n1(\u03b2 log n)\n\u0010nr\nm\n\u00114\n+ \u00b53/2\n0\n\u00b51\np\n\u03b2 log n\n\u0010nr\nm\n\u00115/2\u0013\n.\n(6.35)\nfor some positive constant C.\nHence, when m \u2265\u03bb\u00b50 nr5/4(\u03b2 log n) for some su\ufb03ciently large\nnumerical constant \u03bb > 0, we have that \u2225S0\u2225\u221e\u2265\np\n\u00b50/n with probability at most 2n2e\u2212(\u03b2 log n)2.\nThis is inversely proportional to a superpolynomial in n. We write the second term as\n(S1)ij \u2261p\u22121 X\n\u03c91\u0338=\u03c92\n\u03be\u03c91H\u03c91\u27e8ei, F\u03c91vj\u27e9,\nH\u03c91 = p\u22121\nX\n\u03c92:\u03c92\u0338=\u03c91\nE\u03c92P\u03c92\u03c92P\u03c92\u03c91.\nWe know from Section 6.3 that H obeys \u2225H\u2225\u221e\u2264C \u00b52\n0 r2/m since \u00b51 \u2264\u00b50\n\u221ar so that Lemma 6.12\ngives\nP\n\u0010\n\u2225S1\u2225\u221e\u2265\np\n\u00b50/n\n\u0011\n\u22642n2e\u22121/D,\nD \u2264C\n \n\u00b53\n0\nn3r4\nm3 + \u00b52\n0\nn2r5/2\nm2\n!\nfor some C > 0. Hence, when m \u2265\u03bb\u00b50 nr4/3(\u03b2 log n) for some numerical constant \u03bb > 0, we have\nthat \u2225S1\u2225\u221e\u2265\np\n\u00b50/n with probability at most 2n2e\u2212(\u03b2 log n)2. This is inversely proportional to a\nsuperpolynomial in n. In conclusion and taking into account the decoupling constants in (6.12),\n39\nthe second term in (6.29) obeys (6.30) with probability at least 1 \u2212O(n\u2212\u03b2) provided that m is\nsu\ufb03ciently large as above.\nWe now examine the third term which can be written as\np\u22123(1 \u22122p)\nX\n\u03c91\u0338=\u03c92\n\u03be\u03c91\u03be\u03c92 E\u03c91P 2\n\u03c92\u03c91\u27e8ei, F\u03c91vj\u27e9+ p\u22122(1 \u2212p)\nX\n\u03c91\u0338=\u03c92\n\u03be\u03c92 E\u03c91P 2\n\u03c92\u03c91\u27e8ei, F\u03c91vj\u27e9.\nFor the \ufb01rst term of the right-hand side, it su\ufb03ces to estimate the tail of\n(S0)ij \u2261p\u22121 X\n\u03c91\n\u03be(1)\n\u03c91 E\u03c91H\u03c91\u27e8ei, F\u03c91vj\u27e9,\nH\u03c91 \u2261p\u22122\nX\n\u03c92:\u03c92\u0338=\u03c91\n\u03be(2)\n\u03c92 P 2\n\u03c92\u03c91,\nwhere {\u03be(1)\n\u03c9 } and {\u03be(2)\n\u03c9 } are independent. We know from Section 6.3 that \u2225H\u2225\u221eobeys \u2225H\u2225\u221e\u2264\nC \u221a\u03b2 log n (\u00b50nr/m)3/2 with probability at least 1 \u22122n\u2212\u03b2 for each \u03b2 > 2. Thus, Lemma (6.12)\nshows that S0 obeys (6.34)\u2013(6.35) just as before.\nThe other term is equal to (1 \u2212p) times\nP\n\u03c91 E\u03c91H\u03c91\u27e8ei, F\u03c91vj\u27e9, and by the Cauchy-Schwarz inequality and (6.4)\n\f\f\f\f\f\nX\n\u03c91\nE\u03c91H\u03c91\u27e8ei, F\u03c91vj\u27e9\n\f\f\f\f\f \u2264\u2225H\u2225\u221e\u2225e\u2217\ni E\u2225\n X\nb\nv2\nbj\n!1/2\n\u2264C\nr\u00b50\nn\np\n\u03b2 log n\n \n\u00b50nr4/3\nm\n!3/2\non the event where \u2225H\u2225\u221e\u2264C \u221a\u03b2 log n (\u00b50nr/m)3/2. Hence, when m \u2265\u03bb\u00b50 nr4/3 (\u03b2 log n) for\nsome numerical constant \u03bb > 0, we have that | P\n\u03c91 E\u03c91H\u03c91\u27e8ei, F\u03c91vj\u27e9| \u2264\np\n\u00b50/n on this event. In\nconclusion, the third term in (6.29) obeys (6.30) with probability at least 1\u2212O(n\u2212\u03b2) provided that\nm is su\ufb03ciently large as above.\nWe proceed to the fourth term which can be written as\np\u22123(1 \u22122p)\nX\n\u03c91\u0338=\u03c93\n\u03be\u03c91\u03be\u03c93 E\u03c93P\u03c93\u03c91P\u03c91\u03c91\u27e8ei, F\u03c91vj\u27e9+ p\u22122(1 \u2212p)\nX\n\u03c91\u0338=\u03c93\n\u03be\u03c93 E\u03c93P\u03c93\u03c91P\u03c91\u03c91\u27e8ei, F\u03c91vj\u27e9.\nWe use the decoupling trick for the \ufb01rst term and bound the tail of\n(S0)ij \u2261p\u22121 X\n\u03c91\n\u03be(1)\n\u03c91 H\u03c91(p\u22121P\u03c91\u03c91) \u27e8ei, F\u03c91vj\u27e9,\nH\u03c91 \u2261p\u22121\nX\n\u03c93:\u03c93\u0338=\u03c91\n\u03be(3)\n\u03c93 E\u03c93P\u03c93\u03c91,\nwhere {\u03be(1)\n\u03c9 } and {\u03be(3)\n\u03c9 } are independent. We know from Section 6.2 that\n\u2225H\u2225\u221e\u2264C\nr\n\u00b50nr\u03b2 log n\nm\n\u2225E\u2225\u221e\nwith probability at least 1 \u22122n\u2212\u03b2 for each \u03b2 > 2. Therefore, Lemma 6.12 shows that S0 obeys\n(6.34)\u2013(6.35) just as before. The other term is equal to (1\u2212p) times P\n\u03c91 H\u03c91(p\u22121P\u03c91\u03c91) \u27e8ei, F\u03c91vj\u27e9,\nand the Cauchy-Schwarz inequality gives\n\f\f\f\f\f\nX\n\u03c91\nH\u03c91(p\u22121P\u03c91\u03c91) \u27e8ei, F\u03c91vj\u27e9\n\f\f\f\f\f \u2264\u221an \u2225H\u2225\u221e\n\u00b50nr\nm\n\u2264C \u00b51\n\u221ar\u03b2 log n\n\u221an\n\u0010\u00b50nr\nm\n\u00113/2\non the event \u2225H\u2225\u221e\u2264C\np\n\u00b50nr(\u03b2 log n)/m \u2225E\u2225\u221e. Because \u00b51 \u2264\u00b50\n\u221ar, we have that whenever\nm \u2265\u03bb \u00b54/3\n0\nnr5/3 (\u03b2 log n) for some numerical constant \u03bb > 0, p\u22121| P\n\u03c91 H\u03c91P\u03c91\u03c91\u27e8ei, F\u03c91vj\u27e9| \u2264\n40\np\n\u00b50/n just as before. In conclusion, the fourth term in (6.29) obeys (6.30) with probability at\nleast 1 \u2212O(n\u2212\u03b2) provided that m is su\ufb03ciently large as above.\nWe \ufb01nally examine the last term\np\u22123\nX\n\u03c91\u0338=\u03c92\u0338=\u03c93\n\u03be\u03c91\u03be\u03c92\u03be\u03c93 E\u03c93P\u03c93\u03c92P\u03c92\u03c91\u27e8ei, F\u03c91vj\u27e9.\nJust as before, we need to bound the tail of\n(S0)ij \u2261p\u22121\nX\n\u03c91,\u03c92,\u03c93\n\u03be(1)\n\u03c91 H\u03c91\u27e8ei, F\u03c91vj\u27e9,\nwhere H is given by (6.23). We know from Section 6.3 that H obeys\n\u2225H\u2225\u221e\u2264C (\u03b2 log n) \u00b50nr\nm\n\u00b51\n\u221ar\nn\nwith probability at least 1 \u22124n\u2212\u03b2 for each \u03b2 > 2. Therefore, Lemma 6.12 gives\nP\n\u0012\n\u2225S0\u2225\u221e\u22651\n5\np\n\u00b50/n\n\u0013\n\u22642n2e\u22121/D,\nD \u2264C\n\u0012\n\u00b50\u00b52\n1(\u03b2 log n)2 \u0010nr\nm\n\u00113\n+ \u00b50\u00b51(\u03b2 log n)\n\u0010nr\nm\n\u00112\u0013\nfor some C > 0. Hence, when m \u2265\u03bb\u00b50 nr4/3(\u03b2 log n) for some numerical constant \u03bb > 0, we have\nthat \u2225S0\u2225\u221e\u22651\n5\np\n\u00b50/n with probability at most 2n2e\u2212(\u03b2 log n). In conclusion, the \ufb01fth term in\n(6.29) obeys (6.30) with probability at least 1 \u2212O(n\u2212\u03b2) provided that m is su\ufb03ciently large as\nabove.\nTo summarize the calculations of this section, if m = \u03bb \u00b54/3\n0\nnr5/3 (\u03b2 log n) where \u03b2 \u22652 is \ufb01xed\nand \u03bb is some su\ufb03ciently large numerical constant, then\nX\n1\u2264j\u2264r\n|\u03b1ij|2 \u2264\u00b50r/n\nwith probability at least 1 \u2212O(n\u2212\u03b2). This concludes the proof.\n6.5\nProof of Lemma 4.8\nIt remains to study the spectral norm of p\u22121(PT \u22a5P\u2126PT ) P\nk\u2265k0 Hk(E) for some positive integer\nk0, which we bound by the Frobenius norm\np\u22121\u2225(PT \u22a5P\u2126PT )\nX\nk\u2265k0\nHk(E)\u2225\u2264p\u22121\u2225(P\u2126PT )\nX\nk\u2265k0\nHk(E)\u2225F\n\u2264\np\n3/2p \u2225\nX\nk\u2265k0\nHk(E)\u2225F ,\nwhere the inequality follows from Corollary 4.3. To bound the Frobenius of the series, write\n\u2225\nX\nk\u2265k0\nHk(E)\u2225F \u2264\u2225H\u2225k0\u2225E\u2225F + \u2225H\u2225k0+1\u2225E\u2225F + . . .\n\u2264\n\u2225H\u2225k0\n1 \u2212\u2225H\u2225\u2225E\u2225F .\n41\nTheorem 4.1 gives an upper bound on \u2225H\u2225since \u2225H\u2225\u2264CR\np\n\u00b50nr\u03b2 log n/m < 1/2 on an event\nwith probability at least 1 \u22123n\u2212\u03b2. Since \u2225E\u2225F = \u221ar, we conclude that\np\u22121\u2225(P\u2126PT )\nX\nk\u2265k0\nHk(E)\u2225F \u2264C 1\n\u221ap\n\u0012\u00b50nr\u03b2 log n\nm\n\u0013k0/2 \u221ar = C\n\u0012n2r\nm\n\u00131/2 \u0012\u00b50nr\u03b2 log n\nm\n\u0013k0/2\nwith large probability. This is the content of Lemma 4.8.\n7\nNumerical Experiments\nTo demonstrate the practical applicability of the nuclear norm heuristic for recovering low-rank\nmatrices from their entries, we conducted a series of numerical experiments for a variety of the\nmatrix sizes n, ranks r, and numbers of entries m.\nFor each (n, m, r) triple, we repeated the\nfollowing procedure 50 times. We generated M, an n \u00d7 n matrix of rank r, by sampling two n \u00d7 r\nfactors ML and MR with i.i.d. Gaussian entries and setting M = MLM \u2217\nR. We sampled a subset\n\u2126of m entries uniformly at random. Then the nuclear norm minimization\nminimize\n\u2225X\u2225\u2217\nsubject to\nXij = Mij,\n(i, j) \u2208\u2126\nwas solved using the SDP solver SDPT3 [34].\nWe declared M to be recovered if the solution\nreturned by the SDP, Xopt, satis\ufb01ed \u2225Xopt \u2212M\u2225F /\u2225M\u2225F < 10\u22123. Figure 1 shows the results\nof these experiments for n = 40 and 50. The x-axis corresponds to the fraction of the entries of\nthe matrix that are revealed to the SDP solver. The y-axis corresponds to the ratio between the\ndimension of the set of rank r matrices, dr = r(2n \u2212r), and the number of measurements m. Note\nthat both of these axes range from zero to one as a value greater than one on the x-axis corresponds\nto an overdetermined linear system where the semide\ufb01nite program always succeeds, and a value of\ngreater than one on the y-axis corresponds to a situation where there is always an in\ufb01nite number\nof matrices with rank r with the given entries. The color of each cell in the \ufb01gures re\ufb02ects the\nempirical recovery rate of the 50 runs (scaled between 0 and 1). White denotes perfect recovery in\nall experiments, and black denotes failure for all experiments. Interestingly, the experiments reveal\nvery similar plots for di\ufb00erent n, suggesting that our asymptotic conditions for recovery may be\nrather conservative.\nFor a second experiment, we generated random positive semide\ufb01nite matrices and tried to\nrecover them from their entries using the nuclear norm heuristic. As above, we repeated the same\nprocedure 50 times for each (n, m, r) triple. We generated M, an n\u00d7n positive semide\ufb01nite matrix\nof rank r, by sampling an n \u00d7 r factor MF with i.i.d. Gaussian entries and setting M = MF M \u2217\nF .\nWe sampled a subset \u2126of m entries uniformly at random.\nThen we solved the nuclear norm\nminimization problem\nminimize\ntrace(X)\nsubject to\nXij = Mij,\n(i, j) \u2208\u2126\nX \u2ab00\n.\nAs above, we declared M to be recovered if \u2225Xopt \u2212M\u2225F /\u2225M\u2225F < 10\u22123. Figure 2 shows the\nresults of these experiments for n = 40 and 50.\nThe x-axis again corresponds to the fraction\nof the entries of the matrix that are revealed to the SDP solver, but, in this case, the number of\n42\nm/n2\ndr/m\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\n1\n0.9\n0.8\n0.7\n0.6\n0.5\n0.4\n0.3\n0.2\n0.1\nm/n2\ndr/m\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\n1\n0.9\n0.8\n0.7\n0.6\n0.5\n0.4\n0.3\n0.2\n0.1\n(a)\n(b)\nFigure 1: Recovery of full matrices from their entries. For each (n, m, r) triple, we\nrepeated the following procedure 50 times. A matrix M of rank r and a subset of m entries were\nselected at random. Then we solved the nuclear norm minimization for X subject to Xij = Mij\non the selected entries. We declared M to be recovered if \u2225Xopt \u2212M\u2225F /\u2225M\u2225F < 10\u22123. The\nresults are shown for (a) n = 40 and (b) n = 50. The color of each cell re\ufb02ects the empirical\nrecovery rate (scaled between 0 and 1). White denotes perfect recovery in all experiments, and\nblack denotes failure for all experiments.\nmeasurements is divided by Dn = n(n+1)/2, the number of unique entries in a positive-semide\ufb01nite\nmatrix and the dimension of the rank r matrices is dr = nr \u2212r(r \u22121)/2. The color of each cell\nis chosen in the same fashion as in the experiment with full matrices. Interestingly, the recovery\nregion is much larger for positive semide\ufb01nite matrices, and future work is needed to investigate if\nthe theoretical scaling is also more favorable in this scenario of low-rank matrix completion.\nFinally, in Figure 3, we plot the performance of the nuclear norm heuristic when recovering\nlow-rank matrices from Gaussian projections of these matrices. In these cases, M was generated\nin the same fashion as above, but, in place of sampling entries, we generated m random Gaussian\nprojections of the data (see the discussion in Section 1.4). Then we solved the optimization\nminimize\n\u2225X\u2225\u2217\nsubject to\nA(X) = A(M) .\nwith the additional constraint that X \u2ab00 in the positive semide\ufb01nite case. Here A(X) denotes a\nlinear map of the form (1.15) where the entries are sampled i.i.d. from a zero-mean unit variance\nGaussian distribution. In these experiments, the recovery regime is far larger than in the case\nof that of sampling entries, but this is not particularly surprising as each Gaussian observation\nmeasures a contribution from every entry in the matrix M. These Gaussian models were studied\nextensively in [27].\n43\nm/Dn\ndr/m\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\n1\n0.9\n0.8\n0.7\n0.6\n0.5\n0.4\n0.3\n0.2\n0.1\nm/Dn\ndr/m\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\n1\n0.9\n0.8\n0.7\n0.6\n0.5\n0.4\n0.3\n0.2\n0.1\n(a)\n(b)\nFigure 2: Recovery of positive semide\ufb01nite matrices from their entries. For each\n(n, m, r) triple, we repeated the following procedure 50 times. A positive semide\ufb01nite matrix\nM of rank r and a set of m entries were selected at random. Then we solved the nuclear norm\nminimization subject to Xij = Mij on the selected entries with the constraint that X \u2ab00.\nThe color scheme for each cell denotes empirical recovery probability and is the same as in\nFigure 1. The results are shown for (a) n = 40 and (b) n = 50.\nm/n2\ndr/m\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\n1\n0.9\n0.8\n0.7\n0.6\n0.5\n0.4\n0.3\n0.2\n0.1\nm/Dn\ndr/m\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\n1\n0.9\n0.8\n0.7\n0.6\n0.5\n0.4\n0.3\n0.2\n0.1\n(a)\n(b)\nFigure 3: Recovery of matrices from Gaussian observations. For each (n, m, r) triple,\nwe repeated the following procedure 10 times. In (a), a matrix of rank r was generated as in\nFigures 1. In (b) a positive semide\ufb01nite matrix of rank r was generated as in Figures 2. In\nboth plots, we select a matrix A from the Gaussian ensemble with m rows and n2 (in (a)) or\nDn = n(n + 1)/2 (in (b)) columns. Then we solve the nuclear norm minimization subject to\nA(X) = A(M). The color scheme for each cell denotes empirical recovery probability and is\nthe same as in Figures 1 and 2.\n44\n8\nDiscussion\n8.1\nImprovements\nIn this paper, we have shown that under suitable conditions, one can reconstruct an n \u00d7 n matrix\nof rank r from a small number of its sampled entries provided that this number is on the order\nof n1.2r log n, at least for moderate values of the rank. One would like to know whether better\nresults hold in the sense that exact matrix recovery would be guaranteed with a reduced number\nof measurements. In particular, recall that an n \u00d7 n matrix of rank r depends on (2n \u2212r)r degrees\nof freedom; is it true then that it is possible to recover most low-rank matrices from on the order\nof nr\u2014up to logarithmic multiplicative factors\u2014randomly selected entries? Can the sample size\nbe merely proportional to the true complexity of the low-rank object we wish to recover?\nIn this direction, we would like to emphasize that there is nothing in our approach that appar-\nently prevents us from getting stronger results. Indeed, we developed a bound on the spectral norm\nof each of the \ufb01rst four terms (PT \u22a5P\u2126PT )Hk(E) in the series (4.13) (corresponding to values of k\nequal to 0, 1, 2, 3) and used a general argument to bound the remainder of the series. Presumably,\none could bound higher order terms by the same techniques. Getting an appropriate bound on\n\u2225(PT \u22a5P\u2126PT )H4(E)\u2225would lower the exponent of n from 6/5 to 7/6. The appropriate bound on\n\u2225(PT \u22a5P\u2126PT )H5(E)\u2225would further lower the exponent to 8/7, and so on. To obtain an optimal\nresult, one would need to reach k of size about log n. In doing so, however, one would have to\npay special attention to the size of the decoupling constants (the constant CD for two variables in\nLemma 6.5) which depend on k\u2014the number of decoupled variables. These constants grow with k\nand upper bounds are known [15,16].\n8.2\nFurther directions\nIt would be of interest to extend our results to the case where the unknown matrix is approximately\nlow-rank. Suppose we write the SVD of a matrix M as\nM =\nX\n1\u2264k\u2264n\n\u03c3kukv\u2217\nk,\nwhere \u03c31 \u2265\u03c32 \u2265. . . \u2265\u03c3n \u22650 and assume for simplicity that none of the \u03c3k\u2019s vanish. In general, it\nis impossible to complete such a matrix exactly from a partial subset of its entries. However, one\nmight hope to be able to recover a good approximation if, for example, most of the singular values\nare small or negligible. For instance, consider the truncated SVD of the matrix M,\nMr =\nX\n1\u2264k\u2264r\n\u03c3kukv\u2217\nk,\nwhere the sum extends over the r largest singular values and let M\u22c6be the solution to (1.5). Then\none would not expect to have M\u22c6= M but it would be of great interest to determine whether the\nsize of M\u22c6\u2212M is comparable to that of M \u2212Mr provided that the number of sampled entries\nis su\ufb03ciently large. For example, one would like to know whether it is reasonable to expect that\n\u2225M\u22c6\u2212M\u2225\u2217is on the same order as \u2225M \u2212Mr\u2225\u2217(one could ask for a similar comparison with a\ndi\ufb00erent norm). If the answer is positive, then this would say that approximately low-rank matrices\ncan be accurately recovered from a small set of sampled entries.\n45\nAnother important direction is to determine whether the reconstruction is robust to noise as in\nsome applications, one would presumably observe\nYij = Mij + zij,\n(i, j) \u2208\u2126,\nwhere z is a deterministic or stochastic perturbation. In this setup, one would perhaps want to\nminimize the nuclear norm subject to \u2225P\u2126(X \u2212Y )\u2225F \u2264\u03f5 where \u03f5 is an upper bound on the\nnoise level instead of enforcing the equality constraint P\u2126(X) = P\u2126(Y ). Can one expect that this\nalgorithm or a variation thereof provides accurate answers? That is, can one expect that the error\nbetween the recovered and the true data matrix be proportional to the noise level?\n9\nAppendix\n9.1\nProof of Theorem 4.2\nThe proof of (4.10) follows that in [10] but we shall use slightly more precise estimates.\nLet Y1, . . . , Yn be a sequence of independent random variables taking values in a Banach space\nand let Y\u22c6be the supremum de\ufb01ned as\nY\u22c6= sup\nf\u2208F\nn\nX\ni=1\nf(Yi),\n(9.1)\nwhere F is a countable family of real-valued functions such that if f \u2208F, then \u2212f \u2208F. Talagrand\n[33] proved a concentration inequality about Y\u22c6, see also [22, Corollary 7.8].\nTheorem 9.1 Assume that |f| \u2264B and E f(Yi) = 0 for every f in F and i = 1, . . . , n. Then for\nall t \u22650,\nP(|Y\u22c6\u2212E Y\u22c6| > t) \u22643 exp\n\u0012\n\u2212t\nKB log\n\u0012\n1 +\nBt\n\u03c32 + B E Y\u22c6\n\u0013\u0013\n,\n(9.2)\nwhere \u03c32 = supf\u2208F\nPn\ni=1 E f2(Yi), and K is a numerical constant.\nWe note that very precise values of the numerical constant K are known and are small, see [20].\nWe will apply this theorem to the random variable Z de\ufb01ned in the statement of Theorem 4.2.\nPut Yab = p\u22121(\u03b4ab \u2212p) PT (eae\u2217\nb) \u2297PT (eae\u2217\nb) and Y = P\nab Yab. By de\ufb01nition,\nZ = sup \u27e8X1, Y(X2)\u27e9= sup\nX\nab\n\u27e8X1, Yab(X2)\u27e9\n= sup p\u22121 X\nab\n(\u03b4ab \u2212p)\u27e8X1, PT (eae\u2217\nb)\u27e9\u27e8PT (eae\u2217\nb), X2\u27e9,\nwhere the supremum is over a countable collection of matrices X1 and X2 obeying \u2225X1\u2225F \u22641 and\n\u2225X2\u2225F \u22641. Note that it follows from (4.8)\n|\u27e8X1, Yab(X2)\u27e9| = p\u22121 |\u03b4ab \u2212p| |\u27e8X1, PT (eae\u2217\nb)\u27e9| |\u27e8PT (eae\u2217\nb), X2\u27e9|\n\u2264p\u22121 \u2225PT (eae\u2217\nb)\u22252\nF \u22642\u00b50r/(min(n1, n2)p) = 2\u00b50 nr/m\n46\n(recall that n = max(n1, n2)). Hence, we can apply Theorem 9.1 with B = 2\u00b50(nr/m). Also\nE |\u27e8X1, Yab(X2)\u27e9|2 = p\u22121(1 \u2212p) |\u27e8X1, PT (eae\u2217\nb)\u27e9|2 |\u27e8X2, PT (eae\u2217\nb)\u27e9|2\n\u2264p\u22121 \u2225PT (eae\u2217\nb)\u22252\nF |\u27e8PT (X2), eae\u2217\nb\u27e9|2\nso that\nX\nab\nE |\u27e8X1, Yab(X2)\u27e9|2 \u2264(2\u00b50 nr/m)\nX\nab\n|\u27e8PT (X2), eae\u2217\nb\u27e9|2\n= (2\u00b50 nr/m) \u2225PT (X2)\u22252\nF \u22642\u00b50nr/m.\nSince E Z \u22641, Theorem 9.1 gives\nP(|Z \u2212E Z| > t) \u22643 exp\n\u0012\n\u2212t\nKB log(1 + t/2)\n\u0013\n\u22643 exp\n\u0012\n\u2212t log 2\nKB min(1, t/2)\n\u0013\n,\nwhere we have used the fact that log(1+u) \u2265(log 2) min(1, u) for u \u22650. Plugging t = \u03bb\nq\n\u00b50 nr log n\nm\nand B = 2\u00b50 nr/m establishes the claim.\n9.2\nProof of Lemma 6.2\nWe shall make use of the following lemma which is an application of well-known deviation bounds\nabout binomial variables.\nLemma 9.2 Let {\u03b4i}1\u2264i\u2264n be a sequence of i.i.d. Bernoulli variables with P(\u03b4i = 1) = p and\nY = Pn\ni=1 \u03b4i. Then for each \u03bb > 0,\nP(Y > \u03bb E Y ) \u2264exp\n\u0012\n\u2212\n\u03bb2\n2 + 2\u03bb/3 E Y\n\u0013\n.\n(9.3)\nThe random variable P\nb \u03b4abE2\nab is bounded by \u2225E\u22252\n\u221e\nP\nb \u03b4ab and it thus su\ufb03ces to estimate the\nqth moment of Y\u2217= max Ya where Ya = P\nb \u03b4ab. The inequality (9.3) implies that\nP(Y\u2217> \u03bbnp) \u2264n exp\n\u0012\n\u2212\n\u03bb2\n2 + 2\u03bb/3 np\n\u0013\n,\nand for \u03bb \u22652, this gives P(Y\u2217> \u03bbnp) \u2264n e\u2212\u03bbnp/2. Hence\nE Y q\n\u2217=\nZ \u221e\n0\nP(Y\u2217> t) qtq\u22121 dt \u2264(2np)q +\nZ \u221e\n2np\nn e\u2212t/2 qtq\u22121 dt.\nBy integrating by parts, one can check that when q \u2264np, we have\nZ \u221e\n2np\nn e\u2212t/2 qtq\u22121 dt \u2264nq (2np)q e\u2212np.\nUnder the assumptions of the lemma, we have nq e\u2212np \u22641 and, therefore,\nE Y q\n\u2217\u22642 (2np)q.\nThe conclusion follows.\n47\nAcknowledgments\nE. C. was partially supported by a National Science Foundation grant CCF-515362, by the 2006\nWaterman Award (NSF) and by an ONR grant. The authors would like to thank Ali Jadbabaie,\nPablo Parrilo, Ali Rahimi, Terence Tao, and Joel Tropp for fruitful discussions about parts of this\npaper. E. C. would like to thank Arnaud Durand for his careful proof-reading and comments.\nReferences\n[1] J. Abernethy, F. Bach, T. Evgeniou, and J.-P. Vert. Low-rank matrix factorization with attributes.\nTechnical Report N24/06/MM, Ecole des Mines de Paris, 2006.\n[2] ACM SIGKDD and Net\ufb02ix. Proceedings of KDD Cup and Workshop, 2007. Proceedings available online\nat http://www.cs.uic.edu/\u223cliub/KDD-cup-2007/proceedings.html.\n[3] Y. Amit, M. Fink, N. Srebro, and S. Ullman. Uncovering shared structures in multiclass classi\ufb01cation.\nIn Proceedings of the Twenty-fourth International Conference on Machine Learning, 2007.\n[4] T. Ando, R. A. Horn, and C. R. Johnson.\nThe singular values of a Hadamard product: A basic\ninequality. Linear and Multilinear Algebra, 21:345\u2013365, 1987.\n[5] A. Argyriou, T. Evgeniou, and M. Pontil. Multi-task feature learning. In Neural Information Processing\nSystems, 2007.\n[6] C. Beck and R. D\u2019Andrea. Computational study and comparisons of LFT reducibility methods. In\nProceedings of the American Control Conference, 1998.\n[7] D. P. Bertsekas, A. Nedic, and A. E. Ozdaglar. Convex Analysis and Optimization. Athena Scienti\ufb01c,\nBelmont, MA, 2003.\n[8] B. Bollob\u00b4as. Random Graphs. Cambridge University Press, Cambridge, 2nd edition, 2001.\n[9] A. Buchholz. Operator Khintchine inequality in non-commutative probability. Math. Annalen, 319:1\u201316,\n2001.\n[10] E. J. Cand`es and J. Romberg. Sparsity and incoherence in compressive sampling. Inverse Problems,\n23(3):969\u2013985, 2007.\n[11] E. J. Cand`es, J. Romberg, and T. Tao. Robust uncertainty principles: exact signal reconstruction from\nhighly incomplete frequency information. IEEE Trans. Inform. Theory, 52(2):489\u2013509, 2006.\n[12] E. J. Cand`es and T. Tao. Decoding by linear programming. IEEE Transactions on Information Theory,\n51(12):4203\u20134215, 2005.\n[13] E. J. Cand`es and T. Tao. Near optimal signal recovery from random projections: Universal encoding\nstrategies? IEEE Trans. Inform. Theory, 52(12):5406\u20135425, December 2006.\n[14] A. L. Chistov and D. Yu. Grigoriev. Complexity of quanti\ufb01er elimination in the theory of algebraically\nclosed \ufb01elds. In Proceedings of the 11th Symposium on Mathematical Foundations of Computer Science,\nvolume 176 of Lecture Notes in Computer Science, pages 17\u201331. Springer Verlag, 1984.\n[15] V. H. de la Pe\u02dcna. Decoupling and Khintchine\u2019s inequalities for U-statistics. Ann. Probab., 20(4):1877\u2013\n1892, 1992.\n[16] V. H. de la Pe\u02dcna and S. J. Montgomery-Smith. Decoupling inequalities for the tail probabilities of\nmultivariate U-statistics. Ann. Probab., 23(2):806\u2013816, 1995.\n[17] D. L. Donoho. Compressed sensing. IEEE Trans. Inform. Theory, 52(4):1289\u20131306, 2006.\n[18] M. Fazel. Matrix Rank Minimization with Applications. PhD thesis, Stanford University, 2002.\n48\n[19] R. A. Horn and C. R. Johnson. Topics in matrix analysis. Cambridge University Press, Cambridge,\n1994. Corrected reprint of the 1991 original.\n[20] T. Klein and E. Rio. Concentration around the mean for maxima of empirical processes. Ann. Probab.,\n33(3):1060\u20131077, 2005.\n[21] B. Laurent and P. Massart. Adaptive estimation of a quadratic functional by model selection. Ann.\nStatist., 28(5):1302\u20131338, 2000.\n[22] M. Ledoux. The Concentration of Measure Phenomenon. American Mathematical Society, 2001.\n[23] A. S. Lewis. The mathematics of eigenvalue optimization. Mathematical Programming, 97(1\u20132):155\u2013176,\n2003.\n[24] N. Linial, E. London, and Y. Rabinovich. The geometry of graphs and some of its algorithmic applica-\ntions. Combinatorica, 15:215\u2013245, 1995.\n[25] F. Lust-Picquard. In\u00b4egalit\u00b4es de Khintchine dans Cp (1 < p < \u221e). Comptes Rendus Acad. Sci. Paris,\nS\u00b4erie I, 303(7):289\u2013292, 1986.\n[26] M. Mesbahi and G. P. Papavassilopoulos. On the rank minimization problem over a positive semide\ufb01nite\nlinear matrix inequality. IEEE Transactions on Automatic Control, 42(2):239\u2013243, 1997.\n[27] B. Recht, M. Fazel, and P. Parrilo. Guaranteed minimum rank solutions of matrix equations via nuclear\nnorm minimization. 2007. Submitted to SIAM Review.\n[28] J. D. M. Rennie and N. Srebro. Fast maximum margin matrix factorization for collaborative prediction.\nIn Proceedings of the International Conference of Machine Learning, 2005.\n[29] M. Rudelson. Random vectors in the isotropic position. J. Funct. Anal., 164(1):60\u201372, 1999.\n[30] M. Rudelson and R. Vershynin. Sampling from large matrices: an approach through geometric functional\nanalysis. J. ACM, 54(4):Art. 21, 19 pp. (electronic), 2007.\n[31] A. M.-C. So and Y. Ye. Theory of semide\ufb01nite programming for sensor network localization. Mathe-\nmatical Programming, Series B, 109, 2007.\n[32] N. Srebro. Learning with Matrix Factorizations. PhD thesis, Massachusetts Institute of Technology,\n2004.\n[33] M. Talagrand. New concentration inequalities in product spaces. Invent. Math., 126(3):505\u2013563, 1996.\n[34] K. C. Toh, M.J. Todd, and R. H. T\u00a8ut\u00a8unc\u00a8u. SDPT3 - a MATLAB software package for semide\ufb01nite-\nquadratic-linear programming. Available from http://www.math.nus.edu.sg/~mattohkc/sdpt3.html.\n[35] L. Vandenberghe and S. P. Boyd. Semide\ufb01nite programming. SIAM Review, 38(1):49\u201395, 1996.\n[36] G. A. Watson.\nCharacterization of the subdi\ufb00erential of some matrix norms.\nLinear Algebra and\nApplications, 170:1039\u20131053, 1992.\n49\n",
        "sentence": "",
        "context": "Pablo Parrilo, Ali Rahimi, Terence Tao, and Joel Tropp for fruitful discussions about parts of this\npaper. E. C. would like to thank Arnaud Durand for his careful proof-reading and comments.\nReferences\nThe conclusion follows.\n47\nAcknowledgments\nE. C. was partially supported by a National Science Foundation grant CCF-515362, by the 2006\nWaterman Award (NSF) and by an ONR grant. The authors would like to thank Ali Jadbabaie,\nclosed \ufb01elds. In Proceedings of the 11th Symposium on Mathematical Foundations of Computer Science,\nvolume 176 of Lecture Notes in Computer Science, pages 17\u201331. Springer Verlag, 1984."
    },
    {
        "title": "The power of convex relaxation: Near-optimal matrix completion",
        "author": [
            "Emmanuel J Cand\u00e8s",
            "Terence Tao"
        ],
        "venue": "Information Theory, IEEE Transactions on,",
        "citeRegEx": "Cand\u00e8s and Tao.,? \\Q2010\\E",
        "shortCiteRegEx": "Cand\u00e8s and Tao.",
        "year": 2010,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "Fast low-rank estimation by projected gradient descent: General statistical and algorithmic guarantees",
        "author": [
            "Yudong Chen",
            "Martin J Wainwright"
        ],
        "venue": "arXiv preprint arXiv:1509.03025,",
        "citeRegEx": "Chen and Wainwright.,? \\Q2015\\E",
        "shortCiteRegEx": "Chen and Wainwright.",
        "year": 2015,
        "abstract": "",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "1-bit matrix completion",
        "author": [
            "Mark A Davenport",
            "Yaniv Plan",
            "Ewout van den Berg",
            "Mary Wootters"
        ],
        "venue": "Information and Inference,",
        "citeRegEx": "Davenport et al\\.,? \\Q2014\\E",
        "shortCiteRegEx": "Davenport et al\\.",
        "year": 2014,
        "abstract": "In this paper we develop a theory of matrix completion for the extreme case\nof noisy 1-bit observations. Instead of observing a subset of the real-valued\nentries of a matrix M, we obtain a small number of binary (1-bit) measurements\ngenerated according to a probability distribution determined by the real-valued\nentries of M. The central question we ask is whether or not it is possible to\nobtain an accurate estimate of M from this data. In general this would seem\nimpossible, but we show that the maximum likelihood estimate under a suitable\nconstraint returns an accurate estimate of M when ||M||_{\\infty} <= \\alpha, and\nrank(M) <= r. If the log-likelihood is a concave function (e.g., the logistic\nor probit observation models), then we can obtain this maximum likelihood\nestimate by optimizing a convex program. In addition, we also show that if\ninstead of recovering M we simply wish to obtain an estimate of the\ndistribution generating the 1-bit measurements, then we can eliminate the\nrequirement that ||M||_{\\infty} <= \\alpha. For both cases, we provide lower\nbounds showing that these estimates are near-optimal. We conclude with a suite\nof experiments that both verify the implications of our theorems as well as\nillustrate some of the practical applications of 1-bit matrix completion. In\nparticular, we compare our program to standard matrix completion methods on\nmovie rating data in which users submit ratings from 1 to 5. In order to use\nour program, we quantize this data to a single bit, but we allow the standard\nmatrix completion program to have access to the original ratings (from 1 to 5).\nSurprisingly, the approach based on binary data performs significantly better.",
        "full_text": "1-Bit Matrix Completion\nMark A. Davenport, Yaniv Plan, Ewout van den Berg, Mary Wootters\u2217\nSeptember 2012 (Revised May 2014)\nAbstract\nIn this paper we develop a theory of matrix completion for the extreme case of noisy 1-bit observa-\ntions. Instead of observing a subset of the real-valued entries of a matrix M, we obtain a small number\nof binary (1-bit) measurements generated according to a probability distribution determined by the real-\nvalued entries of M. The central question we ask is whether or not it is possible to obtain an accurate\nestimate of M from this data. In general this would seem impossible, but we show that the maximum\nlikelihood estimate under a suitable constraint returns an accurate estimate of M when \u2225M\u2225\u221e\u2264\u03b1 and\nrank(M) \u2264r. If the log-likelihood is a concave function (e.g., the logistic or probit observation models),\nthen we can obtain this maximum likelihood estimate by optimizing a convex program. In addition,\nwe also show that if instead of recovering M we simply wish to obtain an estimate of the distribution\ngenerating the 1-bit measurements, then we can eliminate the requirement that \u2225M\u2225\u221e\u2264\u03b1. For both\ncases, we provide lower bounds showing that these estimates are near-optimal. We conclude with a\nsuite of experiments that both verify the implications of our theorems as well as illustrate some of the\npractical applications of 1-bit matrix completion. In particular, we compare our program to standard\nmatrix completion methods on movie rating data in which users submit ratings from 1 to 5. In order\nto use our program, we quantize this data to a single bit, but we allow the standard matrix completion\nprogram to have access to the original ratings (from 1 to 5). Surprisingly, the approach based on binary\ndata performs signi\ufb01cantly better.\n1\nIntroduction\nThe problem of recovering a matrix from an incomplete sampling of its entries\u2014also known as matrix\ncompletion\u2014arises in a wide variety of practical situations. In many of these settings, however, the ob-\nservations are not only incomplete, but also highly quantized, often even to a single bit. In this paper we\nconsider a statistical model for such data where instead of observing a real-valued entry as in the original\nmatrix completion problem, we are now only able to see a positive or negative rating. This binary output\nis generated according to a probability distribution which is parameterized by the corresponding entry of\nthe unknown low-rank matrix M. The central question we ask in this paper is: \u201cGiven observations of\nthis form, can we recover the underlying matrix?\u201d\nQuestions of this form are often asked in the context of binary PCA or logistic PCA. There are a number\nof compelling algorithmic papers on these subjects, including [19, 22, 38, 57, 64], which suggest positive\nanswers on simulated and real-world data. In this paper, we give the \ufb01rst theoretical accuracy guarantees\nunder a generalized linear model. We show that O(rd) binary observations are su\ufb03cient to accurately\nrecover a d \u00d7 d, rank-r matrix by convex programming. Our theory is inspired by the unquantized matrix\ncompletion problem and the closely related problem of 1-bit compressed sensing, described below.\n\u2217M.A. Davenport is with the School of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta,\nGA. Email: {mdav@gatech.edu}.\nY. Plan is with the Department of Mathematics, University of Michigan, Ann Arbor, MI. Email: {yplan@umich.edu}.\nE. van den Berg is with the IBM T.J. Watson Research Center, Yorktown Heights, NY. Email: {evandenberg@us.ibm.com}.\nM.\nWootters\nis\nwith\nthe\nDepartment\nof\nMathematics,\nUniversity\nof\nMichigan,\nAnn\nArbor,\nMI.\nEmail:\n{wootters@umich.edu}.\nThis work was partially supported by NSF grants DMS-0906812, DMS-1004718, DMS-1103909, CCF-0743372, and CCF-\n1350616 and NRL grant N00173-14-2-C001.\n1\narXiv:1209.3672v3  [math.ST]  1 Jul 2014\n1.1\nMatrix completion\nMatrix completion arises in a wide variety of practical contexts, including collaborative \ufb01ltering [28],\nsystem identi\ufb01cation [45], sensor localization [7, 59, 60], rank aggregation [26], and many more. While\nmany of these applications have a relatively long history, recent advances in the closely related \ufb01eld of\ncompressed sensing [13, 21, 23] have enabled a burst of progress in the last few years, and we now have\na strong base of theoretical results concerning matrix completion [15\u201317, 25, 30, 36, 37, 39\u201342, 49, 55, 56].\nA typical result from this literature is that a generic d \u00d7 d matrix of rank r can be exactly recovered\nfrom O(r d polylog(d)) randomly chosen entries. Similar results can be established in the case of noisy\nobservations and approximately low-rank matrices [15,25,37,39\u201342,49,56].\nAlthough these results are quite impressive, there is an important gap between the statement of the\nproblem as considered in the matrix completion literature and many of the most common applications\ndiscussed therein. As an example, consider collaborative \ufb01ltering and the now-famous \u201cNet\ufb02ix problem.\u201d\nIn this setting, we assume that there is some unknown matrix whose entries each represent a rating for a\nparticular user on a particular movie. Since any user will rate only a small subset of possible movies, we are\nonly able to observe a small fraction of the total entries in the matrix, and our goal is to infer the unseen\nratings from the observed ones. If the rating matrix has low rank, then this would seem to be the exact\nproblem studied in the matrix completion literature. However, there is a subtle di\ufb00erence: the theory\ndeveloped in this literature generally assumes that observations consist of (possibly noisy) continuous-\nvalued entries of the matrix, whereas in the Net\ufb02ix problem the observations are \u201cquantized\u201d to the set of\nintegers between 1 and 5. If we believe that it is possible for a user\u2019s true rating for a particular movie to\nbe, for example, 4.5, then we must account for the impact of this \u201cquantization noise\u201d on our recovery. Of\ncourse, one could potentially treat quantization simply as a form of bounded noise, but this is somewhat\nunsatisfying because the ratings aren\u2019t just quantized \u2014 there are also hard limits placed on the minimum\nand maximum allowable ratings. (Why should we suppose that a movie given a rating of 5 could not\nhave a true underlying rating of 6 or 7 or 10?) The inadequacy of standard matrix completion techniques\nin dealing with this e\ufb00ect is particularly pronounced when we consider recommender systems where each\nrating consists of a single bit representing a positive or negative rating (consider for example rating music\non Pandora, the relevance of advertisements on Hulu, or posts on sites such as MathOver\ufb02ow). In such a\ncase, the assumptions made in the existing theory of matrix completion do not apply, standard algorithms\nare ill-posed, and alternative theory is required.\n1.2\nStatistical learning theory and matrix completion\nWhile the theory of matrix completion gained quite a bit of momentum following advances in compressed\nsensing, earlier results from Srebro et al. [62, 63] also addressed this problem from a slightly di\ufb00erent\nperspective rooted in the framework of statistical learning theory. These results also deal with the binary\nsetting that we consider here. They take a model-free approach and prove generalization error bounds;\nthat is, they give conditions under which good agreement on the observed data implies good agreement on\nthe entire matrix. For example, in [63], agreement is roughly measured via the fraction of predicted signs\nthat are correct, but this can also be extended to other notions of agreement [62]. From the perspective of\nstatistical learning theory, this corresponds to bounding the generalization error under various classes of\nloss functions.\nOne important di\ufb00erence between the statistical learning approach and the path taken in this paper is\nthat here we focus on parameter estimation \u2014 that is, we seek to recover the matrix M itself (or the distri-\nbution parameterized by M that governs our observations) \u2014 although we also prove generalization error\nbounds en route to our main results on parameter and distribution recovery. We discuss the relationship\nbetween our approach and the statistical learning approach in more detail in Section A.1 (see Remark 3).\nBrie\ufb02y, our generalization error bounds correspond to the case where the loss function is the log likelihood,\nand do not seem to \ufb01t directly within the framework of the existing literature.\n2\n1.3\n1-bit compressed sensing and sparse logistic regression\nAs noted above, matrix completion is closely related to the \ufb01eld of compressed sensing, where a theory to\ndeal with single-bit quantization has recently been developed [10,32,33,43,51,52]. In compressed sensing,\none can recover an s-sparse vector in Rd from O(s log(d/s)) random linear measurements\u2014several di\ufb00erent\nrandom measurement structures are compatible with this theory. In 1-bit compressed sensing, only the\nsigns of these measurements are observed, but an s-sparse signal can still be approximately recovered from\nthe same number of measurements [1, 33, 51, 52]. However, the only measurement ensembles which are\ncurrently known to give such guarantees are Gaussian or sub-Gaussian [1], and are thus of a quite di\ufb00erent\n\ufb02avor than the kinds of samples obtained in the matrix completion setting. A similar theory is available\nfor the closely related problem of sparse binomial regression, which considers more classical statistical\nmodels [2, 11, 34, 46, 48, 52, 54, 65] and allows non-Gaussian measurements. Our aim here is to develop\nresults for matrix completion of the same \ufb02avor as 1-bit compressed sensing and sparse logistic regression.\n1.4\nChallenges\nIn this paper, we extend the theory of matrix completion to the case of 1-bit observations. We consider a\ngeneral observation model but focus mainly on two particular possibilities: the models of logistic and probit\nregression. We discuss these models in greater detail in Section 2.1, but \ufb01rst we note that several new\nchallenges arise when trying to leverage results in 1-bit compressed sensing and sparse logistic regression to\ndevelop a theory for 1-bit matrix completion. First, matrix completion is in some sense a more challenging\nproblem than compressed sensing. Speci\ufb01cally, some additional di\ufb03culty arises because the set of low-rank\nmatrices is \u201ccoherent\u201d with single entry measurements (see [30]). In particular, the sampling operator\ndoes not act as a near-isometry on all matrices of interest, and thus the natural analogue to the restricted\nisometry property from compressed sensing cannot hold in general\u2014there will always be certain low-rank\nmatrices that we cannot hope to recover without essentially sampling every entry of the matrix.\nFor\nexample, consider a matrix that consists of a single nonzero entry (which we might never observe). The\ntypical way to deal with this possibility is to consider a reduced set of low-rank matrices by placing\nrestrictions on the entry-wise maximum of the matrix or its singular vectors\u2014informally, we require that\nthe matrix is not too \u201cspiky\u201d.\nWe introduce an entirely new dimension of ill-posedness by restricting ourselves to 1-bit observations.\nTo illustrate this, we describe one version of 1-bit matrix completion in more detail (the general problem\nde\ufb01nition is given in Section 2.1 below). Consider a d \u00d7 d matrix M with rank r. Suppose we observe a\nsubset \u2126of entries of a matrix Y . The entries of Y depend on M in the following way:\nYi,j =\n(\n+1\nif Mi,j + Zi,j \u22650\n\u22121\nif Mi,j + Zi,j < 0\n(1)\nwhere Z is a matrix containing noise. This latent variable model is the direct analogue to the usual 1-\nbit compressed sensing observation model. In this setting, we view the matrix M as more than just a\nparameter of the distribution of Y ; M represents the real underlying quantity of interest that we would\nlike to estimate. Unfortunately, in what would seem to be the most benign setting\u2014when \u2126is the set of\nall entries, Z = 0, and M has rank 1 and a bounded entry-wise maximum\u2014the problem of recovering\nM is ill-posed. To see this, let M = uv\u2217for any vectors u, v \u2208Rd, and for simplicity assume that there\nare no zero entries in u or v. Now let eu and ev be any vectors with the same sign pattern as u and v\nrespectively. It is apparent that both M and f\nM = euev\u2217will yield the same observations Y , and thus M\nand f\nM are indistinguishable. Note that while it is obvious that this 1-bit measurement process will destroy\nany information we have regarding the scaling of M, this ill-posedness remains even if we knew something\nabout the scaling a priori (such as the Frobenius norm of M). For any given set of observations, there will\nalways be radically di\ufb00erent possible matrices that are all consistent with observed measurements.\nAfter considering this example, the problem might seem hopeless. However, an interesting surprise is\nthat when we add noise to the problem (that is, when Z \u0338= 0 is an appropriate stochastic matrix) the\n3\npicture completely changes\u2014this noise has a \u201cdithering\u201d e\ufb00ect and the problem becomes well-posed. In\nfact, we will show that in this setting we can sometimes recover M to the same degree of accuracy that\nis possible when given access to completely unquantized measurements! In particular, under appropriate\nconditions, O(rd) measurements are su\ufb03cient to accurately recover M.\n1.5\nApplications\nThe problem of 1-bit matrix completion arises in nearly every application that has been proposed for\n\u201cunquantized\u201d matrix completion. To name a few:\n\u2022 Recommender systems: As mentioned above, collaborative \ufb01ltering systems often involve dis-\ncretized recommendations [28]. In many cases, each observation will consist simply of a \u201cthumbs up\u201d\nor \u201cthumbs down\u201d thus delivering only 1 bit of information (consider for example rating music on\nPandora, the relevance of advertisements on Hulu, or posts on sites such as MathOver\ufb02ow). Such\ncases are a natural application for 1-bit matrix completion.\n\u2022 Analysis of survey data: Another potential application for matrix completion is to analyze incom-\nplete survey data. Such data is almost always heavily quantized since people are generally not able\nto distinguish between more than 7 \u00b1 2 categories [47]. 1-bit matrix completion provides a method\nfor analyzing incomplete (or potentially even complete) survey designs containing simple yes/no or\nagree/disagree questions.\n\u2022 Distance matrix recovery and multidimensional scaling: Yet another common motivation\nfor matrix completion is to localize nodes in a sensor network from the observation of just a few\ninter-node distances [7, 59, 60]. This is essentially a special case of multidimensional scaling (MDS)\nfrom incomplete data [8]. In general, work in the area assumes real-valued measurements. However,\nin the sensor network example (as well as many other MDS scenarios), the measurements may be\nvery coarse and might only indicate whether the nodes are within or outside of some communication\nrange. While there is some existing work on MDS using binary data [29] and MDS using incomplete\nobservations with other kinds of non-metric data [61], 1-bit matrix completion promises to provide a\nprincipled and unifying approach to such problems.\n\u2022 Quantum state tomography: Low-rank matrix recovery from incomplete observations also has\napplications to quantum state tomography [31]. In this scenario, mixed quantum states are rep-\nresented as Hermitian matrices with nuclear norm equal to 1. When the state is nearly pure, the\nmatrix can be well approximated by a low-rank matrix and, in particular, \ufb01ts the model given in\nSection 2.2 up to a rescaling. Furthermore, Pauli-operator-based measurements give probabilistic\nbinary outputs. However, these are based on the inner products with the Pauli matrices, and thus\nof a slightly di\ufb00erent \ufb02avor than the measurements considered in this paper. Nevertheless, while we\ndo not address this scenario directly, our theory of 1-bit matrix completion could easily be adapted\nto quantum state tomography.\n1.6\nNotation\nWe now provide a brief summary of some of the key notation used in this paper. We use [d] to denote the\nset of integers {1, . . . , d}. We use capital boldface to denote a matrix (e.g., M) and standard text to denote\nits entries (e.g., Mi,j). Similarly, we let 0 denote the matrix of all-zeros and 1 the matrix of all-ones. We\nlet \u2225M\u2225denote the operator norm of M, \u2225M\u2225F =\nqP\ni,j M2\ni,j denote the Frobenius norm of M, \u2225M\u2225\u2217\ndenote the nuclear or Schatten-1 norm of M (the sum of the singular values), and \u2225M\u2225\u221e= maxi,j |Mi,j|\ndenote the entry-wise in\ufb01nity-norm of M.\nWe will use the Hellinger distance, which, for two scalars\np, q \u2208[0, 1], is given by\nd2\nH(p, q) := (\u221ap \u2212\u221aq)2 + (\np\n1 \u2212p \u2212\np\n1 \u2212q)2.\n4\nThis gives a standard notion of distance between two binary probability distributions. We also allow the\nHellinger distance to act on matrices via the average Hellinger distance over their entries: for matrices\nP , Q \u2208[0, 1]d1\u00d7d2, we de\ufb01ne\nd2\nH(P , Q) =\n1\nd1d2\nX\ni,j\nd2\nH(Pi,j, Qi,j).\nFinally, for an event E ,1[E] is the indicator function for that event, i.e., 1[E] is 1 if E occurs and 0 otherwise.\n1.7\nOrganization of the paper\nWe proceed in Section 2 by describing the 1-bit matrix completion problem in greater detail. In Section 3\nwe state our main results. Speci\ufb01cally, we propose a pair of convex programs for the 1-bit matrix completion\nproblem and establish upper bounds on the accuracy with which these can recover the matrix M and the\ndistribution of the observations Y . We also establish lower bounds, showing that our upper bounds are\nnearly optimal. In Section 4 we describe numerical implementations of our proposed convex programs and\ndemonstrate their performance on a number of synthetic and real-world examples. Section 5 concludes\nwith a brief discussion of future directions. The proofs of our main results are provided in the appendix.\n2\nThe 1-bit matrix completion problem\n2.1\nObservation model\nWe now introduce the more general observation model that we study in this paper.\nGiven a matrix\nM \u2208Rd1\u00d7d2, a subset of indices \u2126\u2282[d1] \u00d7 [d2], and a di\ufb00erentiable function f : R \u2192[0, 1], we observe\nYi,j =\n(\n+1\nwith probability f(Mi,j),\n\u22121\nwith probability 1 \u2212f(Mi,j)\nfor (i, j) \u2208\u2126.\n(2)\nWe will leave f general for now and discuss a few common choices just below. As has been important in\nprevious work on matrix completion, we assume that \u2126is chosen at random with E |\u2126| = n. Speci\ufb01cally,\nwe assume that \u2126follows a binomial model in which each entry (i, j) \u2208[d1] \u00d7 [d2] is included in \u2126with\nprobability\nn\nd1d2 , independently.\nBefore discussing some particular choices for f, we \ufb01rst note that while the observation model described\nin (2) may appear on the surface to be somewhat di\ufb00erent from the setup in (1), they are actually equivalent\nif f behaves likes a cumulative distribution function. Speci\ufb01cally, for the model in (1), if Z has i.i.d. entries,\nthen by setting f(x) := P (Z1,1 \u2265\u2212x), the model in (1) reduces to that in (2). Similarly, for any choice of\nf(x) in (2), if we de\ufb01ne Z as having i.i.d. entries drawn from a distribution whose cumulative distribution\nfunction is given by FZ(x) = P (z \u2264x) = 1 \u2212f(\u2212x), then (2) reduces to (1). Of course, in any given\nsituation one of these observation models may seem more or less natural than the other\u2014for example, (1)\nmay seem more appropriate when M is viewed as a latent variable which we might be interested in\nestimating, while (2) may make more sense when M is viewed as just a parameter of a distribution.\nUltimately, however, the two models are equivalent.\nWe now consider two natural choices for f (or equivalently, for Z):\nExample 1 (Logistic regression/Logistic noise). The logistic regression model, which is common in statis-\ntics, is captured by (2) with f(x) =\nex\n1+ex and by (1) with Zi,j i.i.d. according to the standard logistic\ndistribution.\nExample 2 (Probit regression/Gaussian noise). The probit regression model is captured by (2) by setting\nf(x) = 1 \u2212\u03a6(\u2212x/\u03c3) = \u03a6(x/\u03c3) where \u03a6 is the cumulative distribution function of a standard Gaussian\nand by (1) with Zi,j i.i.d. according to a mean-zero Gaussian distribution with variance \u03c32.\n5\n2.2\nApproximately low-rank matrices\nThe majority of the literature on matrix completion assumes that the \ufb01rst r singular values of M are\nnonzero and the remainder are exactly zero. However, in many applications the singular values instead\nexhibit only a gradual decay towards zero. Thus, in this paper we allow a relaxation of the assumption\nthat M has rank exactly r. Instead, we assume that \u2225M\u2225\u2217\u2264\u03b1\u221ard1d2, where \u03b1 is a parameter left to be\ndetermined, but which will often be of constant order. In other words, the singular values of M belong to\na scaled \u21131 ball. In compressed sensing, belonging to an \u2113p ball with p \u2208(0, 1] is a common relaxation of\nexact sparsity; in matrix completion, the nuclear-norm ball (or Schatten-1 ball) plays an analogous role.\nSee [18] for further compelling reasons to use the nuclear-norm ball relaxation.\nThe particular choice of scaling, \u03b1\u221ard1d2, arises from the following considerations. Suppose that each\nentry of M is bounded in magnitude by \u03b1 and that rank(M) \u2264r. Then\n\u2225M\u2225\u2217\u2264\u221ar \u2225M\u2225F \u2264\np\nrd1d2 \u2225M\u2225\u221e\u2264\u03b1\np\nrd1d2.\nThus, the assumption that \u2225M\u2225\u2217\u2264\u03b1\u221ard1d2 is a relaxation of the conditions that rank(M) \u2264r and\n\u2225M\u2225\u221e\u2264\u03b1. The condition that \u2225M\u2225\u221e\u2264\u03b1 essentially means that the probability of seeing a +1 or\n\u22121 does not depend on the dimension. It is also a way of enforcing that M should not be too \u201cspiky\u201d;\nas discussed above this is an important assumption in order to make the recovery of M well-posed (e.g.,\nsee [49]).\n3\nMain results\nWe now state our main results. We will have two goals\u2014the \ufb01rst is to accurately recover M itself, and\nthe second is to accurately recover the distribution of Y given by f(M).1 All proofs are contained in the\nappendix.\n3.1\nConvex programming\nIn order to approximate either M or f(M), we will maximize the log-likelihood function of the optimization\nvariable X given our observations subject to a set of convex constraints. In our case, the log-likelihood\nfunction is given by\nL\u2126,Y (X) :=\nX\n(i,j)\u2208\u2126\n\u0010\n1[Yi,j=1] log(f(Xi,j)) + 1[Yi,j=\u22121] log(1 \u2212f(Xi,j))\n\u0011\n.\nTo recover M, we will use the solution to the following program:\nc\nM = arg max\nX\nL\u2126,Y (X)\nsubject to\n\u2225X\u2225\u2217\u2264\u03b1\np\nrd1d2\nand\n\u2225X\u2225\u221e\u2264\u03b1.\n(3)\nTo recover the distribution f(M), we need not enforce the in\ufb01nity-norm constraint, and will use the\nfollowing simpler program:\nc\nM = arg max\nX\nL\u2126,Y (X)\nsubject to\n\u2225X\u2225\u2217\u2264\u03b1\np\nrd1d2\n(4)\nIn many cases, L\u2126,Y (X) is a concave function and thus the above programs are convex. This can be\neasily checked in the case of the logistic model and can also be veri\ufb01ed in the case of the probit model\n(e.g., see [68]).\n1Strictly speaking, f(M) \u2208[0, 1]d1\u00d7d2 is simply a matrix of scalars, but these scalars implicitly de\ufb01ne the distribution of\nY , so we will sometimes abuse notation slightly and refer to f(M) as the distribution of Y .\n6\n3.2\nRecovery of the matrix\nWe now state our main result concerning the recovery of the matrix M. As discussed in Section 1.4 we\nplace a \u201cnon-spikiness\u201d condition on M to make recovery possible; we enforce this with an in\ufb01nity-norm\nconstraint. Further, some assumptions must be made on f for recovery of M to be feasible. We de\ufb01ne\ntwo quantities L\u03b1 and \u03b2\u03b1 which control the \u201csteepness\u201d and \u201c\ufb02atness\u201d of f, respectively:\nL\u03b1 := sup\n|x|\u2264\u03b1\n|f\u2032(x)|\nf(x)(1 \u2212f(x))\nand\n\u03b2\u03b1 := sup\n|x|\u2264\u03b1\nf(x)(1 \u2212f(x))\n(f\u2032(x))2\n.\n(5)\nIn this paper we will restrict our attention to f such that L\u03b1 and \u03b2\u03b1 are well-de\ufb01ned. In particular, we\nassume that f and f\u2032 are non-zero in [\u2212\u03b1, \u03b1]. This assumption is fairly mild\u2014for example, it includes the\nlogistic and probit models (as we will see below in Remark 1). The quantity L\u03b1 appears only in our upper\nbounds, but it is generally well behaved. The quantity \u03b2\u03b1 appears both in our upper and lower bounds.\nIntuitively, it controls the \u201c\ufb02atness\u201d of f in the interval [\u2212\u03b1, \u03b1]\u2014the \ufb02atter f is, the larger \u03b2\u03b1 is. It is\nclear that some dependence on \u03b2\u03b1 is necessary. Indeed, if f is perfectly \ufb02at, then the magnitudes of the\nentries of M cannot be recovered, as seen in the noiseless case discussed in Section 1.4. Of course, when \u03b1\nis a \ufb01xed constant and f is a \ufb01xed function, both L\u03b1 and \u03b2\u03b1 are bounded by \ufb01xed constants independent\nof the dimension.\nTheorem 1. Assume that \u2225M\u2225\u2217\u2264\u03b1\u221ad1d2r and \u2225M\u2225\u221e\u2264\u03b1. Suppose that \u2126is chosen at random\nfollowing the binomial model of Section 2.1 with E |\u2126| = n. Suppose that Y is generated as in (2). Let L\u03b1\nand \u03b2\u03b1 be as in (5). Consider the solution c\nM to (3). Then with probability at least 1 \u2212C1/(d1 + d2),\n1\nd1d2\n\u2225c\nM \u2212M\u22252\nF \u2264C\u03b1\nr\nr(d1 + d2)\nn\nr\n1 + (d1 + d2) log(d1d2)\nn\nwith C\u03b1 := C2\u03b1L\u03b1\u03b2\u03b1. If n \u2265(d1 + d2) log(d1d2) then this simpli\ufb01es to\n1\nd1d2\n\u2225c\nM \u2212M\u22252\nF \u2264\n\u221a\n2C\u03b1\nr\nr(d1 + d2)\nn\n.\n(6)\nAbove, C1 and C2 are absolute constants.\nNote that the theorem also holds if \u2126= [d1]\u00d7[d2], i.e., if we sample each entry exactly once or observe a\ncomplete realization of Y . Even in this context, the ability to accurately recover M is somewhat surprising.\nRemark 1 (Recovery in the logistic and probit models). The logistic model satis\ufb01es the hypotheses of\nTheorem 1 with \u03b2\u03b1 = (1+e\u03b1)2\ne\u03b1\n\u2248e\u03b1 and L\u03b1 = 1. The probit model has\n\u03b2\u03b1 \u2264c1\u03c32e\n\u03b12\n2\u03c32\nand\nL\u03b1 \u2264c2\n\u03b1\n\u03c3 + 1\n\u03c3\nwhere we can take c1 = \u03c0 and c2 = 8. In particular, in the probit model the bound in (6) reduces to\n1\nd1d2\n\u2225c\nM \u2212M\u22252\nF \u2264C\n\u0010\u03b1\n\u03c3 + 1\n\u0011\nexp\n\u0012 \u03b12\n2\u03c32\n\u0013\n\u03c3\u03b1\nr\nr(d1 + d2)\nn\n.\n(7)\nHence, when \u03c3 < \u03b1, increasing the size of the noise leads to signi\ufb01cantly improved error bounds\u2014this is\nnot an artifact of the proof. We will see in Section 3.4 that the exponential dependence on \u03b1 in the logistic\nmodel (and on \u03b1/\u03c3 in the probit model) is intrinsic to the problem. Intuitively, we should expect this since\nfor such models, as \u2225M\u2225\u221egrows large, we can essentially revert to the noiseless setting where estimation\nof M is impossible. Furthermore, in Section 3.4 we will also see that when \u03b1 (or \u03b1/\u03c3) is bounded by a\nconstant, the error bound (6) is optimal up to a constant factor. Fortunately, in many applications, one\nwould expect \u03b1 to be small, and in particular to have little, if any, dependence on the dimension. This\nensures that each measurement will always have a non-vanishing probability of returning 1 as well as a\nnon-vanishing probability of returning \u22121.\n7\nRemark 2 (Nuclear norm constraint). The assumption made in Theorem 1 (as well as Theorem 2 below)\nthat \u2225M\u2225\u2217\u2264\u03b1\u221ad1d2r does not mean that we are requiring the matrix M to be low rank. We express this\nconstraint in terms of r to aid the intuition of researchers well-versed in the existing literature on low-rank\nmatrix recovery. (If M is exactly rank r and satis\ufb01es \u2225M\u2225\u221e\u2264\u03b1, then as discussed in Section 2.2, M\nwill automatically satisfy this constraint.) If one desires, one may simplify the presentation by replacing\n\u03b1\u221ad1d2r with a parameter \u03bb and simply requiring \u2225M\u2225\u2217\u2264\u03bb, in which case (6) reduces to\nd2\nH(f(c\nM), f(M)) \u2264C3L\u03b1\u03b2\u03b1\n\u03bb\np\nmin(d1, d2) \u00b7 n\nfor a numerical constant C3.\n3.3\nRecovery of the distribution\nIn many situations, we might not be interested in the underlying matrix M, but rather in determining\nthe distribution of the entries of Y . For example, in recommender systems, a natural question would be\nto determine the likelihood that a user would enjoy a particular unrated item.\nSurprisingly, this distribution may be accurately recovered without any restriction on the in\ufb01nity-norm\nof M. This may be unexpected to those familiar with the matrix completion literature in which \u201cnon-\nspikiness\u201d constraints seem to be unavoidable. In fact, we will show in Section 3.4 that the bound in\nTheorem 2 is near-optimal; further, we will show that even under the added constraint that \u2225M\u2225\u221e\u2264\u03b1,\nit would be impossible to estimate f(M) signi\ufb01cantly more accurately.\nTheorem 2. Assume that \u2225M\u2225\u2217\u2264\u03b1\u221ad1d2r. Suppose that \u2126is chosen at random following the binomial\nmodel of Section 2.1 with E |\u2126| = n. Suppose that Y is generated as in (2), and let L = lim\u03b1\u2192\u221eL\u03b1. Let\nc\nM be the solution to (4). Then with probability at least 1 \u2212C1/(d1 + d2),\nd2\nH(f(c\nM), f(M)) \u2264C2\u03b1L\nr\nr(d1 + d2)\nn\nr\n1 + (d1 + d2) log(d1d2)\nn\n.\n(8)\nFurthermore, as long as n \u2265(d1 + d2) log(d1d2), we have\nd2\nH(f(c\nM), f(M)) \u2264\n\u221a\n2C2\u03b1L\nr\nr(d1 + d2)\nn\n.\n(9)\nAbove, C1 and C2 are absolute constants.\nWhile L = 1 for the logistic model, the astute reader will have noticed that for the probit model L\nis unbounded\u2014that is, L\u03b1 tends to \u221eas \u03b1 \u2192\u221e. L would also be unbounded for the case where f(x)\ntakes values of 1 or 0 outside of some range (as would be the case in (1) if the distribution of the noise had\ncompact support). Fortunately, however, we can recover a result for these cases by enforcing an in\ufb01nity-\nnorm constraint, as described in Theorem 6 below. Moreover, for a large class of functions, f, L is indeed\nbounded. For example, in the latent variable version of (1) if the entries Zi,j are at least as fat-tailed as\nan exponential random variable, then L is bounded. To be more precise, suppose that f is continuously\ndi\ufb00erentiable and for simplicity assume that the distribution of Zi,j is symmetric and |f\u2032(x)| /(1 \u2212f(x))\nis monotonic for x su\ufb03ciently large. If P (|Zi,j| \u2265t) \u2265C exp(\u2212ct) for all t \u22650, then one can show that L\nis \ufb01nite. This property is also essentially equivalent to the requirement that a distribution have bounded\nhazard rate. As noted above, this property holds for the logistic distribution, but also for many other\ncommon distributions, including the Laplacian, Student\u2019s t, Cauchy, and others.\n3.4\nRoom for improvement?\nWe now discuss the extent to which Theorems 1 and 2 are optimal. We give three theorems, all proved\nusing information theoretic methods, which show that these results are nearly tight, even when some of\n8\nour assumptions are relaxed. Theorem 3 gives a lower bound to nearly match the upper bound on the\nerror in recovering M derived in Theorem 1. Theorem 4 compares our upper bounds to those available\nwithout discretization and shows that very little is lost when discretizing to a single bit. Finally, Theorem\n5 gives a lower bound matching, up to a constant factor, the upper bound on the error in recovering the\ndistribution f(M) given in Theorem 2. Theorem 5 also shows that Theorem 2 does not su\ufb00er by dropping\nthe canonical \u201cspikiness\u201d constraint.\nOur lower bounds require a few assumptions, so before we delve into the bounds themselves, we brie\ufb02y\nargue that these assumptions are rather innocuous. First, without loss of generality (since we can always\nadjust f to account for rescaling M), we assume that \u03b1 \u22651. Next, we require that the parameters be\nsu\ufb03ciently large so that\n\u03b12r max{d1, d2} \u2265C0\n(10)\nfor an absolute constant C0. Note that we could replace this with a simpler, but still mild, condition that\nd1 > C0. Finally, we also require that r \u2265c where c is either 1 or 4 and that r \u2264O(min{d1, d2}/\u03b12),\nwhere O(\u00b7) hides parameters (which may di\ufb00er in each Theorem) that we make explicit below. This last\nassumption simply means that we are in the situation where r is signi\ufb01cantly smaller than d1 and d2, i.e.,\nthe matrix is of approximately low rank.\nIn the following, let\nK =\nn\nM : \u2225M\u2225\u2217\u2264\u03b1\np\nrd1d2, \u2225M\u2225\u221e\u2264\u03b1\no\n(11)\ndenote the set of matrices whose recovery is guaranteed by Theorem 1.\n3.4.1\nRecovery from 1-bit measurements\nTheorem 3. Fix \u03b1, r, d1, and d2 to be such that r \u22654 and (10) holds. Let \u03b2\u03b1 be de\ufb01ned as in (5), and\nsuppose that f\u2032(x) is decreasing for x > 0. Let \u2126be any subset of [d1] \u00d7 [d2] with cardinality n, and let Y\nbe as in (2). Consider any algorithm which, for any M \u2208K, takes as input Yi,j for (i, j) \u2208\u2126and returns\nc\nM. Then there exists M \u2208K such that with probability at least 3/4,\n1\nd1d2\n\u2225M \u2212c\nM\u22252\nF \u2265min\n(\nC1, C2\u03b1\nq\n\u03b2 3\n4 \u03b1\nr\nr max{d1, d2}\nn\n)\n(12)\nas long as the right-hand side of (12) exceeds r\u03b12/ min(d1, d2). Above, C1 and C2 are absolute constants.2\nThe requirement that the right-hand side of (12) be larger than r\u03b12/ min(d1, d2) is satis\ufb01ed as long as\nr \u2264O(min{d1, d2}/\u03b12). In particular, it is satis\ufb01ed whenever\nr \u2264C3\nmin(1, \u03b20) \u00b7 min(d1, d2)\n\u03b12\nfor a \ufb01xed constant C3. Note also that in the latent variable model in (1), f\u2032(x) is simply the probability\ndensity of Zi,j. Thus, the requirement that f\u2032(x) be decreasing is simply asking the probability density to\nhave decreasing tails. One can easily check that this is satis\ufb01ed for the logistic and probit models.\nNote that if \u03b1 is bounded by a constant and f is \ufb01xed (in which case \u03b2\u03b1 and \u03b2\u03b1\u2032 are bounded by a\nconstant), then the lower bound of Theorem 3 matches the upper bound given in (6) up to a constant.\nWhen \u03b1 is not treated as a constant, the bounds di\ufb00er by a factor of \u221a\u03b2\u03b1. In the logistic model \u03b2\u03b1 \u2248e\u03b1\nand so this amounts to the di\ufb00erence between e\u03b1/2 and e\u03b1. The probit model has a similar change in the\nconstant of the exponent.\n2Here and in the theorems below, the choice of 3/4 in the probability bound is arbitrary, and can be adjusted at the cost\nof changing C0 in (10) and C1 and C2. Similarly, \u03b2 3\n4 \u03b1 can be replaced by \u03b2(1\u2212\u03f5)\u03b1 for any \u03f5 > 0.\n9\n3.4.2\nRecovery from unquantized measurements\nNext we show that, surprisingly, very little is lost by discretizing to a single bit. In Theorem 4, we consider\nan \u201cunquantized\u201d version of the latent variable model in (1) with Gaussian noise. That is, let Z be a\nmatrix of i.i.d. Gaussian random variables, and suppose the noisy entries Mi,j + Zi,j are observed directly,\nwithout discretization. In this setting, we give a lower bound that still nearly matches the upper bound\ngiven in Theorem 1, up to the \u03b2\u03b1 term.\nTheorem 4. Fix \u03b1, r, d1, and d2 to be such that r \u22651 and (10) holds. Let \u2126be any subset of [d1] \u00d7 [d2]\nwith cardinality n, and let Z be a d1 \u00d7 d2 matrix with i.i.d. Gaussian entries with variance \u03c32. Consider\nany algorithm which, for any M \u2208K, takes as input Yi,j = Mi,j +Zi,j for (i, j) \u2208\u2126and returns c\nM. Then\nthere exists M \u2208K such that with probability at least 3/4,\n1\nd1d2\n\u2225M \u2212c\nM\u22252\nF \u2265min\n(\nC1, C2\u03b1\u03c3\nr\nr max{d1, d2}\nn\n)\n(13)\nas long as the right-hand side of (13) exceeds r\u03b12/ min(d1, d2). Above, C1 and C2 are absolute constants.\nThe requirement that the right-hand side of (13) be larger than r\u03b12/ min(d1, d2) is satis\ufb01ed whenever\nr \u2264C3\nmin(1, \u03c32) \u00b7 min(d1, d2)\n\u03b12\nfor a \ufb01xed constant C3.\nFollowing Remark 1, the lower bound given in (13) matches the upper bound proven in Theorem 1 for\nthe solution to (4) up to a constant, as long as \u03b1/\u03c3 is bounded by a constant. In other words:\nWhen the signal-to-noise ratio is constant, almost nothing is lost by quantizing to a single bit.\nPerhaps it is not particularly surprising that 1-bit quantization induces little loss of information in the\nregime where the noise is comparable to the underlying quantity we wish to estimate\u2014however, what\nis somewhat of a surprise is that the simple convex program in (4) can successfully recover all of the\ninformation contained in these 1-bit measurements.\nBefore proceeding, we also brie\ufb02y note that our Theorem 4 is somewhat similar to Theorem 3 in [49].\nThe authors in [49] consider slightly di\ufb00erent sets K: these sets are more restrictive in the sense that it\nis required that \u03b1 \u2265\u221a32 log n and less restrictive because the nuclear-norm constraint may be replaced\nby a general Schatten-p norm constraint. It was important for us to allow \u03b1 = O(1) in order to compare\nwith our upper bounds due to the exponential dependence of \u03b2\u03b1 on \u03b1 in Theorem 1 for the probit model.\nThis led to some new challenges in the proof. Finally, it is also noteworthy that our statements hold for\narbitrary sets \u2126, while the argument in [49] is only valid for a random choice of \u2126.\n3.4.3\nRecovery of the distribution from 1-bit measurements\nTo conclude we address the optimality of Theorem 2. We show that under mild conditions on f, any\nalgorithm that recovers the distribution f(M) must yield an estimate whose Hellinger distance deviates\nfrom the true distribution by an amount proportional to \u03b1\np\nrd1d2/n, matching the upper bound of (9)\nup to a constant. Notice that the lower bound holds even if the algorithm is promised that \u2225M\u2225\u221e\u2264\u03b1,\nwhich the upper bound did not require.\nTheorem 5. Fix \u03b1, r, d1, and d2 to be such that r \u22654 and (10) holds. Let L1 be de\ufb01ned as in (5), and\nsuppose that f\u2032(x) \u2265c and c\u2032 \u2264f(x) \u22641 \u2212c\u2032 for x \u2208[\u22121, 1], for some constants c, c\u2032 > 0. Let \u2126be\nany subset of [d1] \u00d7 [d2] with cardinality n, and let Y be as in (2). Consider any algorithm which, for\n10\nany M \u2208K, takes as input Yi,j for (i, j) \u2208\u2126and returns c\nM. Then there exists M \u2208K such that with\nprobability at least 3/4,\nd2\nH(f(M), f(c\nM)) \u2265min\n(\nC1, C2\n\u03b1\nL1\nr\nr max{d1, d2}\nn\n)\n(14)\nas long as the right-hand side of (14) exceeds r\u03b12/ min(d1, d2). Above, C1 and C2 are constants that depend\non c, c\u2032.\nThe requirement that the right-hand side of (14) be larger than r\u03b12/ min(d1, d2) is satis\ufb01ed whenever\nr \u2264C3\nmin(1, L\u22122\n1 ) \u00b7 min(d1, d2)\n\u03b12\nfor a constant C3 that depends only on c, c\u2032. Note also that the condition that f and f\u2032 be well-behaved\nin the interval [\u22121, 1] is satis\ufb01ed for the logistic model with c = 1/4 and c\u2032 =\n1\n1+e \u22640.269. Similarly, we\nmay take c = 0.242 and c\u2032 = 0.159 in the probit model.\n4\nSimulations\n4.1\nImplementation\nBefore presenting the proofs of our main results, we provide algorithms and a suite of numerical experiments\nto demonstrate their usefulness in practice.3 We present algorithms to solve the convex programs (3) and\n(4), and using these we can recover M (or f(M)) via 1-bit matrix completion.\n4.1.1\nOptimization algorithm\nWe begin with the observation that both (3) and (4) are instances of the more general formulation\nminimize\nx\nf(x)\nsubject to\nx \u2208C,\n(15)\nwhere f(x) is a smooth convex function from Rd \u2192R, and C is a closed convex set in Rd. In particular,\nde\ufb01ning V to be the bijective linear mapping that vectorizes Rd1\u00d7d2 to Rd1d2, we have f(x) := \u2212F\u2126,Y (V\u22121x)\nand, depending on whether we want to solve (3) or (4), C equal to either V(C1) or V(C2), where\nC1 := {X : \u2225X\u2225\u2217\u2264\u03c4}\nand\nC2 := {X : \u2225X\u2225\u2217\u2264\u03c4, \u2225X\u2225\u221e\u2264\u03ba}.\n(16)\nOne possible solver for problems of the form (15) is the nonmonotone spectral projected-gradient (SPG)\nalgorithm proposed by Birgin et al. [6]. Another possibility is to use an accelerated proximal-gradient\nmethods for the minimization of composite functions [4, 50], which are useful for solving optimization\nproblems of the form\nminimize\nx\nf(x) + g(x),\nwhere f(x) and g(x) are convex functions with g(x) possibly non-smooth. This formulation reduces to (15)\nwhen choosing g(x) to be the extended-real indicator function corresponding to C:\ng(x) =\n(\n0\nx \u2208C\n+\u221e\notherwise.\n3The\ncode\nfor\nthese\nalgorithms,\nas\nwell\nas\nfor\nthe\nsubsequent\nexperiments,\nis\navailable\nonline\nat\nhttp://users.ece.gatech.edu/\u223cmdavenport/.\n11\nBoth algorithms are iterative and require at each iteration the evaluation of f(x), its gradient \u2207f(x), and\nan orthogonal projection onto C (i.e., the prox-function of g(x))\nPC(v) := arg min\nx\n\u2225x \u2212v\u22252\nsubject to\nx \u2208C.\n(17)\nFor our experiments we use the SPG algorithm, which we describe in more detail below. The implemen-\ntation of the algorithm is based on the SPGL1 code [66,67].\n4.1.2\nSpectral projected-gradient method\nIn basic gradient-descent algorithms for unconstrained minimization of a convex function f(x), iterates are\nof the form xk+1 = xk \u2212\u03b1k\u2207f(xk), where the step length \u03b1k \u2208(0, 1] is chosen such that su\ufb03cient descent\nin the objective function f(x) is achieved. When the constraint x \u2208C is added, the basic scheme can no\nlonger guarantee feasibility of the iterates. Projected gradient methods resolve this problem by including\non orthogonal projections back onto the feasible set (17) at each iteration.\nThe nonmonotone SPG algorithm described in [6] modi\ufb01es the basic projected gradient method in two\nmajor ways. First, it scales the initial search direction using the spectral step-length \u03b3k as proposed by\nBarzilai and Borwein [3]. Second, it relaxes monotonicity of the objective values by requiring su\ufb03cient\ndescent relative to the maximum objective over the last t iterates (or k when k < t). Two types of line\nsearch are considered. The \ufb01rst type is curvilinear and traces the following path:\nx(\u03b1) := PC(xk \u2212\u03b1\u03b3k\u2207f(xk)).\n(18)\nThe second type \ufb01rst determines a projected gradient step, and uses this to obtain the search direction dk:\ndk = PC(xk \u2212\u03b3k\u2207f(xk)) \u2212xk.\nNext, a line search is done along the linear trajectory\nx(\u03b1) := xk + \u03b1dk.\n(19)\nIn either case, once the step length \u03b1 is chosen, we set xk+1 = x(\u03b1), and proceed with the next iteration.\nIn the 1-bit matrix completion formulation proposed in this paper, the projection onto C forms the main\ncomputational bottleneck. As a result, it is crucial to keep the number of projections to a minimum, and\nour implementation therefore relies primarily on the line search along the linear trajectory given by (19).\nThe more expensive curvilinear line search is used only when the linear one fails. We have observed that\nthis situation tends to arise only when xk is near optimal.\nThe optimality condition for (15) is that\nPC(x \u2212\u2207f(x)) = x.\n(20)\nOur implementation checks if (20) is approximately satis\ufb01ed. In addition it imposes bounds on the total\nnumber of iterations and the run time.\n4.1.3\nOrthogonal projections onto the feasible sets\nIt is well known that the orthogonal projection onto the nuclear-norm ball C1 amounts to singular-value\nsoft thresholding [12]. In particular, let X = U\u03a3V \u2217with \u03a3 = diag(\u03c31, . . . , \u03c3d), then\nPC(X) = S\u03bb(X) := U max{\u03a3 \u2212\u03bbI, 0}V \u2217,\nwhere the maximum is taken entrywise, and \u03bb \u22650 is the smallest value for which Pd\ni=1 max{\u03c3i \u2212\u03bb, 0} \u2264\u03c4.\n12\nUnfortunately, no closed form solution is known for the orthogonal projection onto C2. However, the\nunderlying problem\nPC2(X) := arg min\nZ\n1\n2 \u2225X \u2212Z\u22252\nF\nsubject to\n\u2225Z\u2225\u2217\u2264\u03c4, \u2225Z\u2225\u221e\u2264\u03ba,\n(21)\ncan be solved using iterative methods. In particular, we can rewrite (21) as\nminimize\nZ,W\n1\n2 \u2225X \u2212W \u22252\nF\nsubject to\n\u2225W \u2225\u221e\u2264\u03ba,\n\u2225Z\u2225\u2217\u2264\u03c4,\nW = Z.\n(22)\nand apply the alternating-direction method of multipliers (ADMM) [24, 27]. The augmented Lagrangian\nfor (22) with respect to the constraint W = Z is given by\nL\u00b5(Y , W , Z) = 1\n2 \u2225X \u2212W \u22252\nF \u2212\u27e8Y , W \u2212Z\u27e9+ \u00b5\n2 \u2225W \u2212Z\u22252\nF + I[\u2225W \u2225\u221e\u2264\u03ba] + I[\u2225Z\u2225\u2217\u2264\u03c4]\nThe ADMM iterates the following steps to solve (22):\nStep 0. Initialize k = 0, and select \u00b5k, Y k, W k, Zk such that \u2225W k\u2225\u221e\u2264\u03ba and \u2225Zk\u2225\u2217\u2264\u03c4.\nStep 1. Minimize L\u00b5(Y k, W, Zk) with respect to W , which can be rewritten as\nW k+1 := arg min\nW\n\u2225W \u2212(X + Y k + \u00b5Zk)/(1 + \u00b5))\u22252\nF\nsubject to\n\u2225W \u2225\u221e\u2264\u03ba.\nThis is exactly the orthogonal projection of B = (X + Y k + \u00b5Zk)/(1 + \u00b5) onto {W | \u2225W \u2225\u221e\u2264\u03ba},\nand gives W k+1(i, j) = min{\u03ba, max{\u2212\u03ba, B(i, j)}}.\nStep 2. Minimize L\u00b5(Y k, W k+1, Z) with respect to Z. This gives\nZk+1 = arg min\nZ\n\u2225Z \u2212(W k+1 \u22121/\u00b5kY k)\u22252\nF\nsubject to\n\u2225Z\u2225\u2217\u2264\u03c4,\nand simpli\ufb01es to Zk+1 = PC1(W k+1 \u22121/\u00b5kY k).\nStep 3. Update Y k+1 = Y k \u2212\u00b5(W k+1 \u2212Zk+1), set \u00b5k+1 = 1.05 \u00b5k, and increment k.\nStep 4. Return Z = Zk when \u2225W k \u2212Zk\u2225F \u2264\u03b5 and \u2225Zk\u2225\u221e\u2212\u03ba \u2264\u03b5 for some su\ufb03ciently small \u03b5 > 0.\nOtherwise, repeat steps 1\u20134.\n4.2\nSynthetic experiments\nTo evaluate the performance of this algorithm in practice and to con\ufb01rm the theoretical results described\nabove, we \ufb01rst performed a number of synthetic experiments. In particular, we constructed a random d\u00d7d\nmatrix M with rank r by forming M = M 1M \u2217\n2 where M 1 and M 2 are d \u00d7 r matrices with entries drawn\ni.i.d. from a uniform distribution on [\u22121\n2, 1\n2]. The matrix is then scaled so that \u2225M\u2225\u221e= 1. We then\nobtained 1-bit observations by adding Gaussian noise of variance \u03c32 and recording the sign of the resulting\nvalue.\nWe begin by comparing the performance of the algorithms in (3) and (4) over a range of di\ufb00erent values\nof \u03c3. In this experiment we set d = 500, r = 1, and n = 0.15d2, and we measured performance of each\napproach using the squared Frobenius norm of the error (normalized by the norm of the original matrix\nM) and averaged the results over 15 draws of M.4 The results are shown in Figure 1. We observe that for\nboth approaches, the performance is poor when there is too little noise (when \u03c3 is small) and when there\n4In evaluating these algorithms we found that it was bene\ufb01cial in practice to follow the recovery by a \u201cdebiasing\u201d step\nwhere the recovered matrix c\nM is forced to be rank r by computing the SVD of c\nM and hard thresholding the singular values.\nIn cases where we report the Frobenius norm of the error, we performed this debiasing step, although it does not dramatically\nimpact the performance.\n13\n\u22123\n\u22122\n\u22121\n0\n1\n0.2\n0.4\n0.6\n0.8\n1\n1.2\n1.4\n\u2225\u001f\nM \u2212M\u22252\nF\n\u2225M\u22252\nF\nProgram in (3)\nProgram in (4)\nlog10 \u03c3\nFigure 1: Average relative error in the results obtained using algorithms (3) and (4) over a range of di\ufb00erent values\nof \u03c3. We observe that for both approaches, the performance is poor when there is too little noise (when \u03c3 is small)\nand when there is too much noise (when \u03c3 is large). In the case of moderate noise, we observe that (4) appears to\nperform almost as well as (3), even though we do not have any theoretical guarantees for (4).\nis too much noise (when \u03c3 is large). These two regimes correspond to the cases where the noise is either so\nsmall that the observations are essentially noise-free or when the noise is so large that each observation is\nessentially a coin toss. In the regime where the noise is of moderate power, we observe better performance\nfor both approaches. Perhaps somewhat surprisingly, we \ufb01nd that for much of this range, the approach\nin (4) appears to perform almost as well as (3), even though we do not have any theoretical guarantees\nfor (4). This suggests that adding the in\ufb01nity-norm constraint as in (3) may have only limited practical\nbene\ufb01t, despite the key role this constraint played in our analysis. By using the simpler program in (4) one\ncan greatly simplify the projection step in the algorithm, so in practice this approach may be preferable.\nWe also conducted experiments evaluating the performance of both (3) and (4) as a function of n for a\nparticular choice of \u03c3. The results showing the impact of n on (4) are shown in Figure 2 (the results for (3)\nat this noise level are almost indistinguishable). In this experiment we set d = 200, and chose \u03c3 \u22480.18\nsuch that log10(\u03c3) = 0.75, which lies in the regime where the noise is neither negligible nor overwhelming.\nWe considered matrices with rank r = 3, 5, 10 and evaluated the performance over a range of n. Figure 2(a)\nshows the performance in terms of the relative Frobenius norm of the error, and Figure 2(b) shows the\nperformance in terms of the Hellinger distance between the recovered distributions. Consistent with our\ntheoretical results, we observe a decay in the error (under both performance metrics) that appears to\nbehave roughly on the order of n\u22121/2.\n4.3\nCollaborative \ufb01ltering\nTo evaluate the performance of our algorithm in a practical setting, we consider the MovieLens (100k)\ndata set, which is available for download at http://www.grouplens.org/node/73. This data set consists\nof 100,000 movie ratings from 1000 users on 1700 movies, with each rating occurring on a scale from 1\nto 5. For testing purposes, we converted these ratings to binary observations by comparing each rating\nto the average rating for the entire dataset (which is approximately 3.5). We then apply the algorithm\nin (4) (using the logistic model of f(x) =\nex\n1+ex ) on a subset of 95,000 ratings to recover an estimate of M.\nHowever, since our only source of data is the quantized ratings, there is no \u201cground truth\u201d against which\nto measure the accuracy of our recovery. Thus, we instead evaluate our performance by checking to see if\nthe estimate of M accurately predicts the sign of the remaining 5000 unobserved ratings in our dataset.\nThe result of this simulation is shown in the \ufb01rst line of Table 1, which gives the accuracy in predicting\nwhether the unobserved ratings are above or below the average rating of 3.5.\n14\n0\n0.5\n1\n1.5\n2\n\u2225\u001f\nM \u2212M\u22252\nF\n\u2225M\u22252\nF\n0\n0.2\n0.4\n0.6\n0.8\n1\nm/d2\nr = 3\nr = 5\nr = 10\n0\n0.1\n0.2\n0.3\n0.4\n0.5\nr = 3\nr = 5\nr = 10\n0\n0.2\n0.4\n0.6\n0.8\n1\nm/d2\nd2\nH(f(\u001f\nM), f(M))\n(a)\n(b)\nFigure 2: The impact of n on (4). (a) shows the performance in terms of the relative Frobenius norm of the error.\n(b) shows the performance in terms of the Hellinger distance between the recovered distributions. In both cases, we\nobserve a decay in the error (under both performance metrics) that appears to behave roughly on the order of n\u22121/2.\nOriginal rating\n1\n2\n3\n4\n5\nOverall\n1-bit matrix completion\n79%\n73%\n58%\n75%\n89%\n73%\n\u201cStandard\u201d matrix completion\n64%\n56%\n44%\n65%\n75%\n60%\nTable 1: Results of a comparison between a 1-bit matrix completion approach and \u201cstandard\u201d matrix completion on\nthe MovieLens 100k dataset, as evaluated by predicting whether the unobserved ratings were above or below average.\nOverall, the 1-bit approach achieves an accuracy of 73%, which is signi\ufb01cantly better than the 60% accuracy achieved\nby standard methods. The 1-bit approach also outperforms the standard approach for each possible individual rating,\nalthough we see that both methods perform poorly when the true rating is very close to the average rating.\nBy comparison, the second line in the table shows the results obtained using a \u201cstandard\u201d method\nthat uses the raw ratings (on a scale from 1 to 5) and tries to minimize the nuclear norm of the recovered\nmatrix subject to a constraint that requires the Frobenius norm of the di\ufb00erence between the recovered\nand observed entries to be su\ufb03ciently small. We implement this traditional approach using the TFOCS\nsoftware package [5], and evaluate the results using the same error criterion as the 1-bit matrix completion\napproach\u2014namely, we compare the recovered ratings to the average (recovered) rating. This approach\ndepends on a number of input parameters: \u03b1 in (3), the constraint on the Frobenius norm in the tradi-\ntional case, as well as the internal parameter \u00b5 in TFOCS. We determine the parameter values by simply\nperforming a grid search and selecting those values that lead to the best performance.\nPerhaps somewhat surprisingly, the 1-bit approach performs signi\ufb01cantly better than the traditional\none, even though the traditional approach is given more information in the form of the raw ratings, instead\nof the binary observations.5 The intuition as to how this might be possible is that the standard approach is\nlikely paying a signi\ufb01cant penalty for actually requiring that the recovered matrix yields numerical ratings\nclose to \u201c1\u201d or \u201c5\u201d when a user\u2019s true preference could extend beyond this scale.\n5While not reported in the table, we also observed that the 1-bit approach is relatively insensitive to the choice of \u03b1, so\nthat this improvement in performance does not rely on a careful parameter setting.\n15\n5\nDiscussion\nMany of the applications of matrix completion consider discrete observations, often in the form of binary\nmeasurements. However, matrix completion from noiseless binary measurements is extremely ill-posed,\neven if one collects a binary measurement for each of the matrix entries. Fortunately, when there are some\nstochastic variations (noise) in the observations, recovery becomes well-posed. In this paper we have shown\nthat the unknown matrix can be accurately and e\ufb03ciently recovered from binary measurements in this\nsetting. When the in\ufb01nity norm of the unknown matrix is bounded by a constant, our error bounds are\ntight to within a constant and even match what is possible for undiscretized data. We have also shown that\nthe binary probability distribution can be reconstructed over the entire matrix without any assumption on\nthe in\ufb01nity-norm, and we have provided a matching lower bound (up to a constant).\nOur theory considers approximately low-rank matrices\u2014in particular, we assume that the singular val-\nues belong to a scaled Schatten-1 ball. It would be interesting to see whether more accurate reconstruction\ncould be achieved under the assumption that the unknown matrix has precisely r nonzero singular val-\nues. We conjecture that the Lagrangian formulation of the problem could be fruitful for this endeavor. It\nwould also be interesting to study whether our ideas can be extended to deal with measurements that are\nquantized to more than 2 (but still a small number) of di\ufb00erent values, but we leave such investigations\nfor future work.\nAcknowledgements\nWe would like to thank Roman Vershynin for helpful discussions and Wenxin Zhou for pointing out an\nerror in an earlier version of the proof of Theorem 6.\nA\nProofs of the main results\nWe now provide the proofs of the main theorems presented in Section 3. To begin, we \ufb01rst de\ufb01ne some\nadditional notation that we will need for the proofs. For two probability distributions P and Q on a \ufb01nite\nset A, D(P\u2225Q) will denote the Kullback-Leibler (KL) divergence,\nD(P\u2225Q) =\nX\nx\u2208A\nP(x) log\n\u0012P(x)\nQ(x)\n\u0013\n,\nwhere P(x) denotes the probability of the outcome x under the distribution P. We will abuse this notation\nslightly by overloading it in two ways. First, for scalar inputs p, q \u2208[0, 1], we will set\nD(p\u2225q) = p log\n\u0012p\nq\n\u0013\n+ (1 \u2212p) log\n\u00121 \u2212p\n1 \u2212q\n\u0013\n.\nSecond, for two matrices P , Q \u2208[0, 1]d1\u00d7d2, we de\ufb01ne\nD(P \u2225Q) =\n1\nd1d2\nX\ni,j\nD(Pi,j\u2225Qi,j).\nWe \ufb01rst prove Theorem 2. Theorem 1 will then follow from an approximation argument. Finally, our lower\nbounds will be proved in Section A.3 using information theoretic arguments.\nA.1\nProof of Theorem 2\nWe will actually prove a slightly more general statement, which will be helpful in the proof of Theorem 1.\nWe will assume that \u2225M\u2225\u221e\u2264\u03b3, and we will modify the program (4) to enforce \u2225X\u2225\u221e\u2264\u03b3. That is, we\n16\nwill consider the program\nc\nM = arg max\nX\nL\u2126,Y (X)\nsubject to\n\u2225X\u2225\u2217\u2264\u03b1\np\nrd1d2\nand\n\u2225X\u2225\u221e\u2264\u03b3.\n(23)\nWe will then send \u03b3 \u2192\u221eto recover the statement of Theorem 2. Formally, we prove the following theorem.\nTheorem 6. Assume that \u2225M\u2225\u2217\u2264\u03b1\u221ard1d2 and \u2225M\u2225\u221e\u2264\u03b3. Suppose that \u2126is chosen at random\nfollowing the binomial model of Section 2.1 and satisfying E |\u2126| = n. Suppose that Y is generated as in (2),\nand let L\u03b3 be as in (5). Let c\nM be the solution to (23). Then with probability at least 1 \u2212C1/(d1 + d2),\nd2\nH(f(c\nM), f(M)) \u2264C2L\u03b3\u03b1\nr\nr(d1 + d2)\nn\nr\n1 + (d1 + d2) log(d1d2)\nn\n.\n(24)\nAbove, C1 and C2 are absolute constants.\nFor the proof of Theorem 6, it will be convenient to work with the function\n\u00afL\u2126,Y (X) = L\u2126,Y (X) \u2212L\u2126,Y (0)\nrather than with L\u2126,Y itself. The key will be to establish the following concentration inequality.\nLemma 1. Let G \u2282Rd1\u00d7d2 be\nG =\nn\nX \u2208Rd1\u00d7d2 : \u2225X\u2225\u2217\u2264\u03b1\np\nrd1d2\no\nfor some r \u2264min{d1, d2} and \u03b1 \u22650. Then\nP\n\u0012\nsup\nX\u2208G\n| \u00afL\u2126,Y (X) \u2212E \u00afL\u2126,Y (X)| \u2265C0\u03b1L\u03b3\n\u221ar\np\nn(d1 + d2) + d1d2 log(d1d2)\n\u0013\n\u2264\nC1\nd1 + d2\n,\n(25)\nwhere C0 and C1 are absolute constants and the probability and the expectation are both over the choice of\n\u2126and the draw of Y .\nWe will prove this lemma below, but \ufb01rst we show how it implies Theorem 6. To begin, notice that for\nany choice of X,\nE\n\u0002 \u00afL\u2126,Y (X) \u2212\u00afL\u2126,Y (M)\n\u0003\n= E [L\u2126,Y (X) \u2212L\u2126,Y (M)]\n=\nn\nd1d2\nX\ni,j\n\u0012\nf(Mi,j) log\n\u0012 f(Xi,j)\nf(Mi,j)\n\u0013\n+ (1 \u2212f(Mi,j)) log\n\u0012 1 \u2212f(Xi,j)\n1 \u2212f(Mi,j)\n\u0013\u0013\n= \u2212nD(f(M)\u2225f(X)),\nwhere the expectation is over both \u2126and Y . Next, note that by assumption M \u2208G. Then, we have for\nany X \u2208G\n\u00afL\u2126,Y (X) \u2212\u00afL\u2126,Y (M) = E\n\u0002 \u00afL\u2126,Y (X) \u2212\u00afL\u2126,Y (M)\n\u0003\n+\n\u0000 \u00afL\u2126,Y (X) \u2212E\n\u0002 \u00afL\u2126,Y (X)\n\u0003\u0001\n\u2212\n\u0000 \u00afL\u2126,Y (M) \u2212E\n\u0002 \u00afL\u2126,Y (M)\n\u0003\u0001\n\u2264E\n\u0002 \u00afL\u2126,Y (X) \u2212\u00afL\u2126,Y (M)\n\u0003\n+ 2 sup\nX\u2208G\n\f\f \u00afL\u2126,Y (X) \u2212E\n\u0002 \u00afL\u2126,Y (X)\n\u0003\f\f\n= \u2212nD(f(M)\u2225f(X)) + 2 sup\nX\u2208G\n\f\f \u00afL\u2126,Y (X) \u2212E\n\u0002 \u00afL\u2126,Y (X)\n\u0003\f\f .\nMoreover, from the de\ufb01nition of c\nM we also have that c\nM \u2208G and L\u2126,Y (c\nM) \u2265L\u2126,Y (M). Thus\n0 \u2264\u2212nD(f(M)\u2225f(c\nM)) + 2 sup\nX\u2208G\n\f\f \u00afL\u2126,Y (X) \u2212E\n\u0002 \u00afL\u2126,Y (X)\n\u0003\f\f .\n17\nApplying Lemma 1, we obtain that with probability at least 1 \u2212C1/(d1 + d2), we have\n0 \u2264\u2212nD(f(M)\u2225f(c\nM)) + 2C0\u03b1L\u03b3\n\u221ar\np\nn(d1 + d2) + d1d2 log(d1d2)\nIn this case, by rearranging and applying the fact that \u221ad1d2 \u2264d1 + d2, we obtain\nD(f(M)\u2225f(c\nM)) \u22642C0\u03b1L\u03b3\nr\nr(d1 + d2)\nn\nr\n1 + (d1 + d2) log(d1d2)\nn\n(26)\nFinally, we note that the KL divergence can easily be bounded below by the Hellinger distance:\nd2\nH(p, q) \u2264D(p\u2225q).\nThis is a simple consequence of Jensen\u2019s inequality combined with the fact that 1 \u2212x \u2264\u2212log x. Thus,\nfrom (26) we obtain\nd2\nH(f(M), f(c\nM)) \u22642C0\u03b1L\u03b3\nr\nr(d1 + d2)\nn\nr\n1 + (d1 + d2) log(d1d2)\nn\n,\nwhich establishes Theorem 6. Theorem 2 then follows by taking the limit as \u03b3 \u2192\u221e.\nRemark 3 (Relationship to existing theory). The crux of the proof of the theorem involves bounding the\nquantity\n\u00afL\u2126,Y (c\nM) \u2212\u00afL\u2126,Y (M)\n(27)\nvia Lemma 1. In the language of statistical learning theory [9, 35], (27) is a generalization error, with\nrespect to a loss function corresponding to \u00afL. In that literature, there are standard approaches to proving\nbounds on quantities of this type. We use a few of these tools (namely, symmetrization and contraction\nprinciples) in the proof of Lemma 1. However, our proof also deviates from these approaches in a few ways.\nMost notably, we use a moment argument to establish concentration, while in the classi\ufb01cation literature\nit is common to use approaches based on the method of bounded di\ufb00erences. In our problem setup, using\nbounded di\ufb00erences fails to yield a satisfactory answer because in the most general case we do not impose\nan \u2113\u221e-norm constraint on M. This in turn implies that our di\ufb00erences are not well-bounded. Speci\ufb01cally,\nconsider the empirical process whose moment we bound in (30), i.e.,\nsup\nX\u2208G\n\u27e8E \u25e6\u2206\u2126, X\u27e9.\nThe supremum is a function of the independent random variables Ei,j \u00b7 \u2206i,j. To check the e\ufb00ectiveness of\nthe method of bounded di\ufb00erences, suppose Ei,j\u2206i,j = 1 (for some i, j), and is replaced by \u22121. Then the\nempirical process can change by as much as 2 maxX\u2208G \u2225X\u2225\u221e= \u03b1\u221ard1d2, which is too large to yield an\ne\ufb00ective answer.\nThe fact that \u00afL is not well-bounded in this way is also why the generalization error bounds of [62,63]\ndo not immediately generalize to provide results analogous to Theorem 2. To obtain this result, we must\nchoose a loss function such that (27) reduces to the KL divergence, and unfortunately, this loss function is\nnot well-behaved.\nFinally, we also note that symmetrization and contraction principles, applied to bound empirical pro-\ncesses, are key tools in the theory of unquantized matrix completion. The empirical process that we must\nbound to prove Lemma 1 is a discrete analog of a similar process considered in Section 5 of [49].\nProof of Lemma 1. We begin by noting that for any h > 0, by using Markov\u2019s inequality we have that\nP\n\u0012\nsup\nX\u2208G\n| \u00afL\u2126,Y (X) \u2212E \u00afL\u2126,Y (X)| \u2265C0\u03b1L\u03b3\n\u221ar\np\nn(d1 + d2) + d1d2 log(d1d2)\n\u0013\n= P\n\u0012\nsup\nX\u2208G\n| \u00afL\u2126,Y (X) \u2212E \u00afL\u2126,Y (X)|h \u2265\n\u0010\nC0\u03b1L\u03b3\n\u221ar\np\nn(d1 + d2) + d1d2 log(d1d2)\n\u0011h \u0013\n\u2264\nE\n\u0002\nsupX\u2208G | \u00afL\u2126,Y (X) \u2212E \u00afL\u2126,Y (X)|h\u0003\n\u0010\nC0\u03b1L\u03b3\n\u221ar\np\nn(d1 + d2) + d1d2 log(d1d2)\n\u0011h .\n(28)\n18\nThe bound in (25) will follow by combining this with an upper bound on E\n\u0002\nsupX\u2208G | \u00afL\u2126,Y (X) \u2212E \u00afL\u2126,Y (X)|h\u0003\nand setting h = log(d1 + d2). Towards this end, note that we can write the de\ufb01nition of \u00afL\u2126,Y as\n\u00afL\u2126,Y (X) =\nX\ni,j\n\u0012\n1[(i,j)\u2208\u2126]\n\u0012\n1[Yi,j=1] log\n\u0012f(Xi,j)\nf(0)\n\u0013\n+ 1[Yi,j=\u22121] log\n\u00121 \u2212f(Xi,j)\n1 \u2212f(0)\n\u0013\u0013\u0013\n.\nBy a symmetrization argument (Lemma 6.3 in [44]),\nE\n\u0014\nsup\nX\u2208G\n| \u00afL\u2126,Y (X) \u2212E \u00afL\u2126,Y (X)|h\n\u0015\n\u22642h E\n\uf8ee\n\uf8ef\uf8f0sup\nX\u2208G\n\f\f\f\f\f\f\nX\ni,j\n\u03b5i,j1[(i,j)\u2208\u2126]\n\u0012\n1[Yi,j=1] log\n\u0012f(Xi,j)\nf(0)\n\u0013\n+ 1[Yi,j=\u22121] log\n\u00121 \u2212f(Xi,j)\n1 \u2212f(0)\n\u0013\u0013\f\f\f\f\f\f\nh\uf8f9\n\uf8fa\uf8fb,\nwhere the \u03b5i,j are i.i.d. Rademacher random variables and the expectation in the upper bound is with\nrespect to both \u2126and Y as well as with respect to the \u03b5i,j.\nTo bound the latter term, we apply a\ncontraction principle (Theorem 4.12 in [44]). By the de\ufb01nition of L\u03b3 and the assumption that \u2225c\nM\u2225\u221e\u2264\u03b3,\nboth\n1\nL\u03b3\nlog\n\u0012f(x)\nf(0)\n\u0013\nand\n1\nL\u03b3\nlog\n\u00121 \u2212f(x)\n1 \u2212f(0)\n\u0013\nare contractions that vanish at 0. Thus, up to a factor of 2, the expected value of the supremum can only\ndecrease when these are replaced by Xi,j and \u2212Xi,j respectively. We obtain\nE\n\u0014\nsup\nX\u2208G\n\f\f \u00afL\u2126,Y (X) \u2212E \u00afL\u2126,Y (X)\n\f\fh\n\u0015\n\u22642h(2L\u03b3)h E\n\uf8ee\n\uf8ef\uf8f0sup\nX\u2208G\n\f\f\f\f\f\f\nX\ni,j\n\u03b5i,j1[(i,j)\u2208\u2126]\n\u0010\n1[Yi,j=1]Xi,j \u22121[Yi,j=\u22121]Xi,j\n\u0011\n\f\f\f\f\f\f\nh\uf8f9\n\uf8fa\uf8fb\n= (4L\u03b3)h E\n\u0014\nsup\nX\u2208G\n|\u27e8\u2206\u2126\u25e6E \u25e6Y , X\u27e9|h\n\u0015\n,\n(29)\nwhere E denotes the matrix with entries given by \u03b5i,j, \u2206\u2126denotes the indicator matrix for \u2126(so that\n[\u2206\u2126]i,j = 1 if (i, j) \u2208\u2126and 0 otherwise), and \u25e6denotes the Hadamard product. Using the facts that the\ndistribution of E \u25e6Y is the same as the distribution of E and that |\u27e8A, B\u27e9| \u2264\u2225A\u2225\u2225B\u2225\u2217, we have that\nE\n\u0014\nsup\nX\u2208G\n|\u27e8\u2206\u2126\u25e6E \u25e6Y , X\u27e9|h\n\u0015\n= E\n\u0014\nsup\nX\u2208G\n|\u27e8E \u25e6\u2206\u2126, X\u27e9|h\n\u0015\n(30)\n\u2264E\n\u0014\nsup\nX\u2208G\n\u2225E \u25e6\u2206\u2126\u2225h \u2225X\u2225h\n\u2217\n\u0015\n=\n\u0010\n\u03b1\np\nd1d2r\n\u0011h\nE\nh\n\u2225E \u25e6\u2206\u2126\u2225hi\n,\n(31)\nTo bound E\n\u0002\n\u2225E \u25e6\u2206\u2126\u2225h\u0003\n, observe that E \u25e6\u2206\u2126is a matrix with i.i.d. zero mean entries and thus by\nTheorem 1.1 of [58],\nE\nh\n\u2225E \u25e6\u2206\u2126\u2225hi\n\u2264C\n\uf8eb\n\uf8ec\n\uf8edE\n\uf8ee\n\uf8ef\uf8f0max\n1\u2264i\u2264d1\n\uf8eb\n\uf8ed\nd2\nX\nj=1\n\u2206i,j\n\uf8f6\n\uf8f8\nh\n2\n\uf8f9\n\uf8fa\uf8fb+ E\n\uf8ee\n\uf8ef\uf8f0max\n1\u2264j\u2264d2\n d1\nX\ni=1\n\u2206i,j\n! h\n2\n\uf8f9\n\uf8fa\uf8fb\n\uf8f6\n\uf8f7\n\uf8f8\nfor some constant C. This in turn implies that\n\u0010\nE\nh\n\u2225E \u25e6\u2206\u2126\u2225hi\u0011 1\nh \u2264C\n1\nh\n\uf8eb\n\uf8ec\n\uf8ec\n\uf8ed\n\uf8eb\n\uf8ec\n\uf8edE\n\uf8ee\n\uf8ef\uf8f0max\n1\u2264i\u2264d1\n\uf8eb\n\uf8ed\nd2\nX\nj=1\n\u2206i,j\n\uf8f6\n\uf8f8\nh\n2\n\uf8f9\n\uf8fa\uf8fb\n\uf8f6\n\uf8f7\n\uf8f8\n1\nh\n+\n\uf8eb\n\uf8ec\n\uf8edE\n\uf8ee\n\uf8ef\uf8f0max\n1\u2264j\u2264d2\n d1\nX\ni=1\n\u2206i,j\n! h\n2\n\uf8f9\n\uf8fa\uf8fb\n\uf8f6\n\uf8f7\n\uf8f8\n1\nh\n\uf8f6\n\uf8f7\n\uf8f7\n\uf8f8.\n(32)\n19\nWe \ufb01rst focus on the row sum Pd2\nj=1 \u2206i,j for a particular choice of i. Using Bernstein\u2019s inequality, for all\nt > 0 we have\nP\n\uf8eb\n\uf8ed\n\f\f\f\f\f\f\nd2\nX\nj=1\n\u0012\n\u2206i,j \u2212\nn\nd1d2\n\u0013\f\f\f\f\f\f\n> t\n\uf8f6\n\uf8f8\u22642 exp\n\u0012\n\u2212t2/2\nn/d1 + t/3\n\u0013\n.\nIn particular, if we set t \u22656n/d1, then for each i we have\nP\n\uf8eb\n\uf8ed\n\f\f\f\f\f\f\nd2\nX\nj=1\n\u0012\n\u2206i,j \u2212\nn\nd1d2\n\u0013\f\f\f\f\f\f\n> t\n\uf8f6\n\uf8f8\u22642 exp(\u2212t) = 2P (Wi > t) ,\n(33)\nwhere W1, . . . , Wd1 are i.i.d. exponential random variables.\nBelow we use the fact that for any positive random variable q we can write E q =\nR \u221e\n0 P (q \u2265t), allowing\nus to bound\n\uf8eb\n\uf8ec\n\uf8edE\n\uf8ee\n\uf8ef\uf8f0max\n1\u2264i\u2264d1\n\uf8eb\n\uf8ed\nd2\nX\nj=1\n\u2206i,j\n\uf8f6\n\uf8f8\nh\n2\n\uf8f9\n\uf8fa\uf8fb\n\uf8f6\n\uf8f7\n\uf8f8\n1\nh\n\u2264\nr n\nd1\n+\n\uf8eb\n\uf8ec\n\uf8edE\n\uf8ee\n\uf8ef\uf8f0max\n1\u2264i\u2264d1\n\f\f\f\f\f\f\nd2\nX\nj=1\n\u0012\n\u2206i,j \u2212\nn\nd1d2\n\u0013\f\f\f\f\f\f\nh\n2\n\uf8f9\n\uf8fa\uf8fb\n\uf8f6\n\uf8f7\n\uf8f8\n1\nh\n\u2264\nr n\nd1\n+\n\uf8eb\n\uf8ec\n\uf8edE\n\uf8ee\n\uf8ef\uf8f0max\n1\u2264i\u2264d1\n\f\f\f\f\f\f\nd2\nX\nj=1\n\u0012\n\u2206i,j \u2212\nn\nd1d2\n\u0013\f\f\f\f\f\f\nh\uf8f9\n\uf8fa\uf8fb\n\uf8f6\n\uf8f7\n\uf8f8\n1\n2h\n=\nr n\nd1\n+\n\uf8eb\n\uf8ec\n\uf8ed\nZ \u221e\n0\nP\n\uf8eb\n\uf8ec\n\uf8edmax\n1\u2264i\u2264d1\n\f\f\f\f\f\f\nd2\nX\nj=1\n\u0012\n\u2206i,j \u2212\nn\nd1d2\n\u0013\f\f\f\f\f\f\nh\n\u2265t\n\uf8f6\n\uf8f7\n\uf8f8dt\n\uf8f6\n\uf8f7\n\uf8f8\n1\n2h\n\u2264\nr n\nd1\n+\n\uf8eb\n\uf8ec\n\uf8ed\n\u00126n\nd1\n\u0013h\n+\nZ \u221e\n(6n/d1)h P\n\uf8eb\n\uf8ec\n\uf8edmax\n1\u2264i\u2264d1\n\f\f\f\f\f\f\nd2\nX\nj=1\n\u0012\n\u2206i,j \u2212\nn\nd1d2\n\u0013\f\f\f\f\f\f\nh\n\u2265t\n\uf8f6\n\uf8f7\n\uf8f8dt\n\uf8f6\n\uf8f7\n\uf8f8\n1\n2h\n\u2264\nr n\nd1\n+\n \u00126n\nd1\n\u0013h\n+ 2\nZ \u221e\n(6n/d1)h P\n\u0012\nmax\n1\u2264i\u2264d1 W h\ni \u2265t\n\u0013\ndt\n! 1\n2h\n\u2264\nr n\nd1\n+\n \u00126n\nd1\n\u0013h\n+ 2E\n\u0014\n( max\n1\u2264i\u2264d1 Wi)h\n\u0015! 1\n2h\n.\nAbove, we have used the triangle inequality in the \ufb01rst line, followed by Jensen\u2019s inequality in the second\nline.\nIn the \ufb01fth line, (33), along with independence, allows us to introduce maxi Wi.\nBy standard\ncomputations for exponential random variables,\nE\n\u0014\nmax\n1\u2264i\u2264d1 W h\ni\n\u0015\n\u2264E\n\"\f\f\f\f max\n1\u2264i\u2264d1 Wi \u2212log d1\n\f\f\f\f\nh#\n+ logh(d1)\n\u22642h! + logh(d1).\nThus, we have\n\uf8eb\n\uf8ec\n\uf8edE\n\uf8ee\n\uf8ef\uf8f0max\n1\u2264i\u2264d1\n\uf8eb\n\uf8ed\nd2\nX\nj=1\n\u2206i,j\n\uf8f6\n\uf8f8\nh\n2\n\uf8f9\n\uf8fa\uf8fb\n\uf8f6\n\uf8f7\n\uf8f8\n1\nh\n\u2264\nr n\nd1\n+\n \u00126n\nd1\n\u0013h\n+ 2\n\u0010\nlogh(d1) + 2(h!)\n\u0011! 1\n2h\n20\n\u2264(1 +\n\u221a\n6)\nr n\nd1\n+ 2\n1\n2h\n\u0010p\nlog d1 + 2\n1\n2h \u221a\nh\n\u0011\n\u2264(1 +\n\u221a\n6)\nr n\nd1\n+ (2 +\n\u221a\n2)\np\nlog(d1 + d2),\nusing the choice h = log(d1 + d2) \u22651 in the \ufb01nal line.\nA similar argument bounds the column sums, and thus from (32) we conclude that\n\u0010\nE\nh\n\u2225E \u25e6\u2206\u2126\u2225hi\u0011 1\nh \u2264C\n1\nh\n\u0012\n(1 +\n\u221a\n6)\n\u0012r n\nd1\n+\nr n\nd2\n\u0013\n+ (2 +\n\u221a\n2)\np\nlog(d1 + d2)\n\u0013\n\u2264C\n1\nh\n\uf8eb\n\uf8ed(1 +\n\u221a\n6)\n\uf8eb\n\uf8ed\ns\n2n(d1 + d2)\nd1d2\n\uf8f6\n\uf8f8+ (2 +\n\u221a\n2)\np\nlog(d1 + d2)\n\uf8f6\n\uf8f8\n\u2264C\n1\nh 2(1 +\n\u221a\n6)\ns\nn(d1 + d2) + d1d2 log(d1 + d2)\nd1d2\n,\nwhere the second and third inequalities both follow from Jensen\u2019s inequality. Combining this with (29)\nand (31), we obtain\n\u0012\nE\n\u0014\nsup\nX\u2208G\n\f\f \u00afL\u2126,Y (X) \u2212E \u00afL\u2126,Y (X)\n\f\fh\n\u0015\u0013 1\nh\n\u2264C\n1\nh 8(1 +\n\u221a\n6)\u03b1L\u03b3\n\u221ar\np\nn(d1 + d2) + d1d2 log(d1 + d2).\nPlugging this into (28) we obtain that the probability in (28) is upper bounded by\nC\n \n8(1 +\n\u221a\n6)\nC0\n!log(d1+d2)\n\u2264\nC\nd1 + d2\n,\nprovided that C0 \u22658(1 +\n\u221a\n6)/e, which establishes the lemma.\nA.2\nProof of Theorem 1\nThe proof of Theorem 1 follows immediately from Theorem 6 (with \u03b3 = \u03b1) combined with the following\nlemma.\nLemma 2. Let f be a di\ufb00erentiable function and let \u2225M\u2225\u221e, \u2225c\nM\u2225\u221e\u2264\u03b1. Then\nd2\nH(f(M), f(c\nM)) \u2265inf\n|\u03be|\u2264\u03b1\n(f\u2032(\u03be))2\n8f(\u03be)(1 \u2212f(\u03be))\n\u2225M \u2212c\nM\u22252\nF\nd1d2\n.\nProof. For any pair of entries x = Mi,j and y = c\nMi,j, write\n\u0010p\nf(x) \u2212\np\nf(y)\n\u00112\n+\n\u0010p\n1 \u2212f(x) \u2212\np\n1 \u2212f(y)\n\u00112\n\u22651\n2\n\u0010\u0010p\nf(x) \u2212\np\nf(y)\n\u0011\n\u2212\n\u0010p\nf(x) \u2212\np\nf(y)\n\u0011\u00112\n.\nUsing Taylor\u2019s theorem to expand the quantity inside the square, for some \u03be between x and y,\n\u0010p\nf(x) \u2212\np\nf(y)\n\u00112\n+\n\u0010p\n1 \u2212f(x) \u2212\np\n1 \u2212f(y)\n\u00112\n\u22651\n2\n \nf\u2032(\u03be)(y \u2212x)\n2\np\nf(\u03be)\n+ f\u2032(\u03be)(y \u2212x)\n2\np\n1 \u2212f(\u03be)\n!2\n\u22651\n8(f\u2032(\u03be))2(y \u2212x)2\n\u0012 1\nf(\u03be) +\n1\n1 \u2212f(\u03be)\n\u0013\n=\n(f\u2032(\u03be))2\n8f(\u03be)(1 \u2212f(\u03be))(y \u2212x)2.\nThe lemma follows by summing across all entries and dividing by d1d2.\n21\nA.3\nLower bounds\nThe proofs of our lower bounds each follow a similar outline, using classical information theoretic techniques\nthat have also proven useful in the context of compressed sensing [14,53]. At a high level, our argument\ninvolves \ufb01rst showing the existence of a set of matrices X, so that for each X(i) \u0338= X(j) \u2208X, \u2225X(i)\u2212X(j)\u2225F\nis large. We will imagine obtaining measurements of a randomly chosen matrix in X and then running\nan arbitrary recovery procedure. If the recovered matrix is su\ufb03ciently close to the original matrix, then\nwe could determine which element of X was chosen.\nHowever, Fano\u2019s inequality will imply that the\nprobability of correctly identifying the chosen matrix is small, which will induce a lower bound on how\nclose the recovered matrix can be to the original matrix.\nIn the proofs of Theorems 3, 4, and 5, we will assume without loss of generality that d2 \u2265d1. Before\nproviding these proofs, however, we \ufb01rst consider the construction of the set X.\nA.3.1\nPacking set construction\nLemma 3. Let K be de\ufb01ned as in (11), let \u03b3 \u22641 be such that\nr\n\u03b32 is an integer, and suppose that\nr\n\u03b32 \u2264d1.\nThere is a set X \u2282K with\n|X| \u2265exp\n\u0012 rd2\n16\u03b32\n\u0013\nwith the following properties:\n1. For all X \u2208X, each entry has |Xi,j| = \u03b1\u03b3.\n2. For all X(i), X(j) \u2208X, i \u0338= j,\n\u2225X(i) \u2212X(j)\u22252\nF > \u03b12\u03b32d1d2\n2\n.\nProof. We use a probabilistic argument. The set X will by constructed by drawing\n|X| =\n\u0018\nexp\n\u0012 rd2\n16\u03b32\n\u0013\u0019\n(34)\nmatrices X independently from the following distribution. Set B =\nr\n\u03b32 . The matrix will consist of blocks\nof dimensions B \u00d7 d2, stacked on top of each other.\nThe entries of the \ufb01rst block (that is, Xi,j for\n(i, j) \u2208[B] \u00d7 [d2]) will be i.i.d. symmetric random variables with values \u00b1\u03b1\u03b3. Then X will be \ufb01lled out\nby copying this block as many times as will \ufb01t. That is,\nXi,j := Xi\u2032,j where\ni\u2032 = i (mod B) + 1.\nNow we argue that with nonzero probability, this set will have all the desired properties. For X \u2208X,\n\u2225X\u2225\u221e= \u03b1\u03b3 \u2264\u03b1.\nFurther, because rank X \u2264B,\n\u2225X\u2225\u2217\u2264\n\u221a\nB \u2225X\u2225F =\nr r\n\u03b32\np\nd1d2\u03b1\u03b3 = \u03b1\np\nrd1d2.\nThus X \u2282K, and all that remains is to show that X satis\ufb01es requirement 2.\n22\nFor X, W drawn from the above distribution,\n\u2225X \u2212W \u22252\nF =\nX\ni,j\n(Xi,j \u2212Wi,j)2\n\u2265\n\u0016d1\nB\n\u0017 X\ni\u2208[B]\nX\nj\u2208[d2]\n(Xi,j \u2212Wi,j)2\n= 4\u03b12\u03b32\n\u0016d1\nB\n\u0017 X\ni\u2208[B]\nX\nj\u2208[d2]\n\u03b4i,j\n=: 4\u03b12\u03b32\n\u0016d1\nB\n\u0017\nZ(X, W ).\nwhere the \u03b4i,j are independent 0/1 Bernoulli random variables with mean 1/2. By Hoe\ufb00ding\u2019s inequality\nand a union bound,\nP\n\u0012\nmin\nX\u0338=W \u2208X Z(X, W ) \u2264d2B\n4\n\u0013\n\u2264\n\u0012|X|\n2\n\u0013\nexp(\u2212Bd2/8).\nOne can check that for X of the size given in (34), the right-hand side of the above tail bound is less than\n1, and thus the event that Z(X, W ) > d2B/4 for all X \u0338= W \u2208X has non-zero probability. In this event,\n\u2225X \u2212W \u22252\nF > \u03b12\u03b32\n\u0016d1\nB\n\u0017\nd2B \u2265\u03b12\u03b32d1d2\n2\n,\nwhere the second inequality uses the assumption that d1 \u2265B and the fact that \u230ax\u230b\u2265x/2 for all x \u22651.\nHence, requirement (2) holds with nonzero probability and thus the desired set exists.\nA.3.2\nProof of Theorem 3\nBefore we prove Theorem 3, we will need the following lemma about the KL divergence.\nLemma 4. Suppose that x, y \u2208(0, 1). Then\nD(x\u2225y) \u2264(x \u2212y)2\ny(1 \u2212y).\nProof. Without loss of generality, we may assume that x \u2264y. Indeed, D(1 \u2212x\u22251 \u2212y) = D(x\u2225y), and\neither x \u2264y or 1 \u2212x \u22641 \u2212y. Let z = y \u2212x. A simple computation shows that\n\u2202\n\u2202z D(x\u2225x + z) =\nz\n(x + z)(1 \u2212x \u2212z).\nThus, by Taylor\u2019s theorem, there is some \u03be \u2208[0, z] so that\nD(x\u2225y) = D(x\u2225x) + z\n\u0012\n\u03be\n(x + \u03be)(1 \u2212x \u2212\u03be)\n\u0013\n.\nSince the right hand side is increasing in \u03be, we may replace \u03be with z and conclude\nD(x\u2225y) \u2264(y \u2212x)2\ny(1 \u2212y),\nas desired.\n23\nNow, for the proof of Theorem 3, we choose \u03f5 so that\n\u03f52 = min\n(\n1\n1024, C2\u03b1\nq\n\u03b23\u03b1/4\nr\nrd2\nn\n)\n,\n(35)\nwhere C2 is an absolute constant to be speci\ufb01ed later. We will next use Lemma 3 to construct a set X,\nchoosing \u03b3 so that\nr\n\u03b32 is an integer and\n4\n\u221a\n2\u03f5\n\u03b1\n\u2264\u03b3 \u22648\u03f5\n\u03b1 .\nWe can make such a choice because \u03f5 \u22641/32 and r \u22654. We verify that such a choice for \u03b3 satis\ufb01es the\nrequirements of Lemma 3. Indeed, since \u03f5 \u22641/32 and \u03b1 \u22651 we have \u03b3 \u22641/4 < 1. Further, we assume in\nthe theorem that the right-hand side of (35) is larger than Cr\u03b12/d1 which implies that r/\u03b32 \u2264d1 for an\nappropriate choice of C.\nLet X \u2032\n\u03b1/2,\u03b3 be the set whose existence is guaranteed by Lemma 3 with this choice of \u03b3, and with \u03b1/2\ninstead of \u03b1. We will construct X by setting\nX :=\nn\nX\u2032 + \u03b1\n\u0010\n1 \u2212\u03b3\n2\n\u0011\n1 : X\u2032 \u2208X \u2032\n\u03b1/2,\u03b3\no\nNote that X has the same size as X \u2032\n\u03b1/2,\u03b3, i.e., |X| satis\ufb01es (34). X also has the same bound on pairwise\ndistances\n\u2225X(i) \u2212X(j)\u22252\nF \u2265\u03b12\u03b32d1d2\n8\n\u22654d1d2\u03f52,\n(36)\nand every entry of X \u2208X has\n|Xi,j| \u2208{\u03b1, \u03b1\u2032},\nwhere \u03b1\u2032 = (1 \u2212\u03b3)\u03b1. Further, since for X\u2032 \u2208X \u2032\n\u03b1/2,\u03b3,\n\r\rX\u2032 + \u03b1(1 \u2212\u03b3/2)1\n\r\r\n\u2217\u2264\n\r\rX\u2032\r\r\n\u2217+ \u03b1(1 \u2212\u03b3/2)\np\nd1d2 \u2264\u03b1\np\nrd1d2\nfor r \u22654 as in the theorem statement.\nNow suppose for the sake of a contradiction that there exists an algorithm such that for any X \u2208K,\nwhen given access to the measurements Y \u2126, returns an c\nX such that\n1\nd1d2\n\u2225X \u2212c\nX\u22252\nF < \u03f52\n(37)\nwith probability at least 1/4. We will imagine running this algorithm on a matrix X chosen uniformly at\nrandom from X. Let\nX\u2217= arg min\nX(i)\u2208X\n\u2225X(i) \u2212c\nX\u22252\nF .\nIt is easy to check that if (37) holds, then X\u2217= X. Indeed, for any X\u2032 \u2208X with X\u2032 \u0338= X, from (37)\nand (36) we have that\n\u2225X\u2032 \u2212c\nX\u2225F = \u2225X\u2032 \u2212X + X \u2212c\nX\u2225F \u2265\u2225X\u2032 \u2212X\u2225F \u2212\u2225X \u2212c\nX\u2225F > 2\np\nd1d2\u03f5 \u2212\np\nd1d2\u03f5 =\np\nd1d2\u03f5.\nAt the same time, since X \u2208X is a candidate for X\u2217, we have that\n\u2225X\u2217\u2212c\nX\u2225F \u2264\u2225X \u2212c\nX\u2225F \u2264\np\nd1d2\u03f5.\nThus, if (37) holds, then \u2225X\u2217\u2212c\nX\u2225F < \u2225X\u2032 \u2212c\nX\u2225F for any X\u2032 \u2208X with X\u2032 \u0338= X, and hence we must\nhave X\u2217= X. By assumption, (37) holds with probability at least 1/4, and thus\nP (X \u0338= X\u2217) \u22643\n4.\n(38)\n24\nWe will show that this probability must in fact be large, generating our contradiction.\nBy a variant of Fano\u2019s inequality\nP (X \u0338= X\u2217) \u22651 \u2212\nmaxX(k)\u0338=X(\u2113) D(Y \u2126|X(k) \u2225Y \u2126|X(\u2113)) + 1\nlog |X|\n.\n(39)\nBecause each entry of Y is independent,6\nD := D(Y \u2126|X(k) \u2225Y \u2126|X(\u2113)) =\nX\n(i,j)\u2208\u2126\nD(Yi,j|X(k)\ni,j \u2225Yi,j|X(\u2113)\ni,j ).\nEach term in the sum is either 0, D(\u03b1\u2225\u03b1\u2032), or D(\u03b1\u2032\u2225\u03b1). By Lemma 4, all of these are bounded above by\nD(Yi,j|X(k)\ni,j \u2225Yi,j|X(\u2113)\ni,j ) \u2264(f(\u03b1) \u2212f(\u03b1\u2032))2\nf(\u03b1\u2032)(1 \u2212f(\u03b1\u2032)),\nand so, from the intermediate value theorem, for some \u03be \u2208[\u03b1\u2032, \u03b1],\nD \u2264n (f(\u03b1) \u2212f(\u03b1\u2032))2\nf(\u03b1\u2032)(1 \u2212f(\u03b1\u2032)) \u2264n(f\u2032(\u03be))2(\u03b1 \u2212\u03b1\u2032)2\nf(\u03b1\u2032)(1 \u2212f(\u03b1\u2032)) .\nUsing the assumption that f\u2032(x) is decreasing for x > 0 and the de\ufb01nition of \u03b1\u2032 = (1 \u2212\u03b3)\u03b1, we have\nD \u2264n(\u03b3\u03b1)2\n\u03b2\u03b1\u2032\n\u226464n\u03f52\n\u03b2\u03b1\u2032 .\nThen from (39) and (38),\n1\n4 \u22641 \u2212P (X \u0338= X\u2217) \u2264D + 1\nlog |X| \u226416\u03b32\n\uf8eb\n\uf8ed\n64n\u03f52\n\u03b2\u03b1\u2032\n+ 1\nrd2\n\uf8f6\n\uf8f8\u22641024\u03f52\n\uf8eb\n\uf8ed\n64n\u03f52\n\u03b2\u03b1\u2032\n+ 1\n\u03b12rd2\n\uf8f6\n\uf8f8.\n(40)\nWe now show that for appropriate values of C0 and C2, this leads to a contradiction. First suppose that\n64n\u03f52 \u2264\u03b2\u03b1\u2032. In this case we have\n1\n4 \u22641024\u03f52\n2\n\u03b12rd2\n,\nwhich together with (35) implies that \u03b12rd2 \u22648. If we set C0 > 8 in (10), then this would lead to a\ncontradiction. Thus, suppose now that 64n\u03f52 > \u03b2\u03b1\u2032. Then (40) simpli\ufb01es to\n1\n4 < 1024 \u00b7 128 \u00b7 n\u03f54\n\u03b2\u03b1\u2032\u03b12rd2\n.\nThus\n\u03f52 > \u03b1\u221a\u03b2\u03b1\u2032\n512\n\u221a\n2\nr\nrd2\nn .\nNote \u03b2 is increasing as a function of \u03b1 and \u03b1\u2032 \u22653\u03b1/4 (since \u03b3 \u22641/4). Thus, \u03b2\u03b1\u2032 \u2265\u03b23\u03b1/4. Setting\nC2 \u22641/512\n\u221a\n2 in (35) now leads to a contradiction, and hence (37) must fail to hold with probability at\nleast 3/4, which proves the theorem.\n6Note that here, to be consistent with the literature we are referencing regarding Fano\u2019s inequality, D is de\ufb01ned slightly\ndi\ufb00erently than elsewhere in the paper where we would weight D by 1/d1d2.\n25\nA.3.3\nProof of Theorem 4\nChoose \u03f5 so that\n\u03f52 = min\n(\n1\n16, C2\u03b1\u03c3\nr\nrd2\nn\n)\n(41)\nfor an absolute constant C2 to be determined later. As in the proof of Theorem 3, we will consider running\nsuch an algorithm on a random element in a set X \u2282K. For our set X, we will use the set whose existence\nis guaranteed by Lemma 3. We will set \u03b3 so that\nr\n\u03b32 is an integer and\n2\n\u221a\n2\u03f5\n\u03b1\n\u2264\u03b3 \u22644\u03f5\n\u03b1 .\nThis is possible since \u03f5 \u22641/4 and r, \u03b1 \u22651. One can check that \u03b3 satis\ufb01es the assumptions of Lemma 3.\nNow suppose that X \u2208X is chosen uniformly at random, and let Y = (X + Z)|\u2126as in the statement\nof the theorem. Let c\nX be any estimate of X obtained from Y \u2126. We begin by bounding the mutual\ninformation I(X; c\nX) in the following lemma (which is analogous to [20, Equation 9.16]).\nLemma 5.\nI(X; c\nX) \u2264n\n2 log\n\u0000\u03c32 +\n\u0000\u03b12\u03b32\u0001\u0001\n.\nProof. We begin by noting that\nI(X\u2126; Y ) = h(X\u2126+ Z\u2126) \u2212h(X\u2126+ Z\u2126|X\u2126) = h(X\u2126+ Z\u2126) \u2212h(Z\u2126),\nwhere h denotes the di\ufb00erential entropy. Let \u03be denote a matrix of i.i.d. \u00b11 entries. Then\nh(X\u2126\u25e6\u03be + Z\u2126) = h((X\u2126+ Z\u2126) \u25e6\u03be) \u2265h((X\u2126+ Z\u2126) \u25e6\u03be | \u03be) = h(X\u2126+ Z\u2126),\nand so, letting f\nX = X \u25e6\u03be,\nI(X\u2126; Y ) \u2264h(f\nX\u2126+ Z\u2126) \u2212h(Z\u2126).\nTreating f\nX\u2126+ Z\u2126as a random vector of length n, we compute the covariance matrix as\n\u03a3 := E\nh\nvec(f\nX\u2126+ Z\u2126) vec(f\nX\u2126+ Z\u2126)T i\n=\n\u0000\u03c32 + (\u03b1\u03b3)2\u0001\nIn.\nBy Theorem 8.6.5 in [20],\nh(f\nX\u2126+ Z\u2126) \u22641\n2 log ((2\u03c0e)n det(\u03a3)) = 1\n2 log\n\u0000(2\u03c0e)n(\u03c32 + (\u03b1\u03b3)2)n\u0001\n.\nWe have that h(Z\u2126) = 1\n2 log\n\u0000(2\u03c0e)n\u03c32n\u0001\n, and so\nI(X\u2126; Y ) \u2264n\n2 log\n\u0012\n1 +\n\u0010\u03b1\u03b3\n\u03c3\n\u00112\u0013\n.\nThen the data processing inequality implies\nI(X; c\nX) \u2264n\n2 log\n\u0012\n1 +\n\u0010\u03b1\u03b3\n\u03c3\n\u00112\u0013\n,\nwhich establishes the lemma.\n26\nWe now proceed by using essentially the same argument as in the proof of Theorem 3. Speci\ufb01cally,\nwe suppose for the sake of a contradiction that there exists an algorithm such that for any X \u2208K, when\ngiven access to the measurements Y \u2126, returns an c\nX such that\n1\nd1d2\n\u2225X \u2212c\nX\u22252\nF < \u03f52\n(42)\nwith probability at least 1/4. As before, if we set\nX\u2217= arg min\nX(i)\u2208X\n\u2225X(i) \u2212c\nX\u22252\nF\nthen we can show that if (42) holds, then X\u2217= X. Thus, if (42) holds with probability at least 1/4 then\nP (X \u0338= X\u2217) \u22643\n4.\n(43)\nHowever, by Fano\u2019s inequality, the probability that X \u0338= c\nX is at least\nP\n\u0010\nX \u0338= c\nX\n\u0011\n\u2265H(X|c\nX) \u22121\nlog(|X|)\n= H(X) \u2212I(X; c\nX) \u22121\nlog(|X|)\n\u22651 \u2212I(X; c\nX) + 1\nlog |X|\nPlugging in |X| from Lemma 3 and I(X; c\nX) from Lemma 5, and using the inequality log(1 + z) \u2264z, we\nobtain\nP\n\u0010\nX \u0338= c\nX\n\u0011\n\u22651 \u221216\u03b32\nrd2\n\u0012n\n2\n\u0010\u03b1\u03b3\n\u03c3\n\u00112\n+ 1\n\u0013\n.\nCombining this with (43) and using the fact that \u03b3 \u22644\u03f5/\u03b1, we obtain\n1\n4 \u2264256\u03f52\n\u03b12rd2\n\u0012\n8n\n\u0012 \u03f52\n\u03c32\n\u0013\n+ 1\n\u0013\n.\nWe now argue, as before, that this leads to a contradiction. Speci\ufb01cally, if 8n\u03f52/\u03c32 \u22641, then together\nwith (41) this implies that \u03b12rd2 \u2264128. If we set C0 > 128 in (10), then this would lead to a contradiction.\nThus, suppose now that 8n\u03f52/\u03c32 > 1, in which case we have\n\u03f52 > \u03b1\u03c3\n128\nr\nrd2\nn .\nThus, setting C2 \u22641/128 in (41) leads to a contradiction, and hence (42) must fail to hold with probability\nat least 3/4, which proves the theorem.\nA.3.4\nProof of Theorem 5\nThe proof of Theorem 5 also mirrors the proof of Theorem 3. The main di\ufb00erence is the observation that\nthe set constructed in Lemma 3 also works with the Hellinger distance. We begin as before by choosing \u03f5\nso that\n\u03f52 = min\n(\nc\n16, C2\n\u03b1\nL1\nr\nrd2\nn\n)\n,\n(44)\nwhere C2 is an absolute constant to be determined. Set \u03b3 to be an integer so that\n2\n\u221a\n2 \u03f5\n\u03b1c \u2264\u03b3 \u22644\u03f5\n\u03b1c.\nThis is possible since by assumption \u03b1 \u22651 and \u03f5 \u2264c\n4. One can check that \u03b3 satis\ufb01es the assumptions of\nLemma 3.\n27\nAs in the proof of Theorem 3, we will consider running such an algorithm on a random element in a\nset X \u2282K. For our set X, we will use the set whose existence is guaranteed by Lemma 3. Note that since\nthe Hellinger distance is bounded below by the Frobenius norm, we have that for all X(i) \u0338= X(j) \u2208X,\nd2\nH(f(X(i)) \u2212f(X(j))) \u2265\u2225f(X(i)) \u2212f(X(j))\u22252\nF \u2265c2\u2225X(i) \u2212X(j)\u22252\nF > c2\n2 \u03b12\u03b32d1d2 \u22654d1d2\u03f52.\nNow suppose for the sake of a contradiction that there exists an algorithm such that for any X \u2208K, when\ngiven access to the measurements Y \u2126, returns an c\nX such that\nd2\nH(f(X), f(c\nX)) < \u03f52\n(45)\nwith probability at least 1/4. If we set\nX\u2217= arg min\nX(i)\u2208X\nd2\nH(f(X(i)) \u2212f(c\nX))\nthen we can show that if (45) holds, then X\u2217= X. Thus, if (45) holds with probability at least 1/4 then\nP (X \u0338= X\u2217) \u22643\n4.\n(46)\nHowever, we may again apply Fano\u2019s inequality as in (39). Using Lemma 4 we have\nD(Yi,j|X(k)\ni,j \u2225Yi,j|X(\u2113)\ni,j ) \u2264(f(\u03b1\u03b3) \u2212f(\u2212\u03b1\u03b3))2\nf(\u03b1\u03b3)(1 \u2212f(\u03b1\u03b3)) \u2264\n4(f\u2032(\u03be))2\u03b12\u03b32\nf(\u03b1\u03b3)(1 \u2212f(\u03b1\u03b3)) \u2264\n4f2(\u03be)L2\n\u03b1\u03b3\u03b12\u03b32\nf(\u03b1\u03b3)(1 \u2212f(\u03b1\u03b3)),\nfor some |\u03be| \u2264\u03b1\u03b3, where L\u03b1\u03b3 is as in (5). By the assumption that c\u2032 < |f(x)| < 1 \u2212c\u2032 for |x| < 1, and that\n\u03b1\u03b3 \u2264\u03b1\n\u0012 4\u03f5\n\u03b1c\n\u0013\n\u22644\u03f5\nc \u22641,\nwe obtain\nD(Yi,j|X(k)\ni,j \u2225Yi,j|X(\u2113)\ni,j ) \u22644c\u2032L2\n1\u03b12\u03b32\n1 \u2212c\u2032\n\u2264C\u2032L2\n1\u03f52,\nwhere C\u2032 = 64c\u2032/(c2(1 \u2212c\u2032)). Thus, from (39), we have\n1\n4 \u2264C\u2032nL2\n1\u03f52 + 1\nlog |X|\n\u2264256\nc2 \u03f52\n\u0012C\u2032nL2\n1\u03f52 + 1\n\u03b12rd2\n\u0013\n.\nWe now argue once again that this leads to a contradiction. Speci\ufb01cally, if C\u2032nL2\n1\u03f52 \u22641, then together\nwith (44) this implies that \u03b12rd2 \u2264128/c.\nIf we set C0 > 128/c in (10), then this would lead to a\ncontradiction. Thus, suppose now that C\u2032nL2\n1\u03f52 > 1, in which case we have\n\u03f52 >\nc\n32\n\u221a\n2C\u2032\n\u03b1\nL1\nr\nrd2\nn .\nThus setting C2 \u2264c/32\n\u221a\n2C\u2032 in (44) leads to a contradiction, and hence (45) must fail to hold with\nprobability at least 3/4, which proves the theorem.\nReferences\n[1] A. Ai, A. Lapanowski, Y. Plan, and R. Vershynin. One-bit compressed sensing with non-gaussian\nmeasurements. Linear Alg. Appl., 441:222\u2013239, 2014.\n[2] F. Bach. Self-concordant analysis for logistic regression. Elec. J. Stat., 4:384\u2013414, 2010.\n28\n[3] J. Barzilai and J. Borwein. Two-point step size gradient methods. IMA J. Num. Anal., 8(1):141\u2013148,\n1988.\n[4] A. Beck and M. Teboulle. A fast iterative shrinkage-thresholding algorithm for linear inverse problems.\nSIAM J. Imag. Sci., 2(1):183\u2013202, 2009.\n[5] S. Becker, E. Cand`es, and M. Grant. Templates for convex cone problems with applications to sparse\nsignal recovery. Math. Program. Comput., 3(3):165\u2013218, 2011.\n[6] E. Birgin, J. Mart\u00b4\u0131nez, and M. Raydan. Nonmonotone spectral projected gradient methods on convex\nsets. SIAM J. Optimization, 10(4):1196\u20131211, 2000.\n[7] P. Biswas, T. C. Lian, T. C. Wang, and Y. Ye. Semide\ufb01nite programming based algorithms for sensor\nnetwork localization. ACM Trans. Sen. Netw., 2(2):188\u2013220, 2006.\n[8] I. Borg and P. Groenen. Modern multidimensional scaling. Springer Science, New York, NY, 2010.\n[9] S. Boucheron, O. Bousquet, and G. Lugosi. Theory of classi\ufb01cation: A survey of some recent advances.\nESAIM: Probability and Statistics, 9:323\u2013375, 2005.\n[10] P. Boufounos and R. Baraniuk. 1-Bit compressive sensing. In Proc. IEEE Conf. Inform. Science and\nSystems (CISS), Princeton, NJ, Mar. 2008.\n[11] F. Bunea. Honest variable selection in linear and logistic regression models via \u21131 and \u21131 + \u21132 penal-\nization. Elec. J. Stat., 2:1153\u20131194, 2008.\n[12] J.-F. Cai, E. Cand`es, and Z. Shen. A singular value thresholding algorithm for matrix completion.\nSIAM J. Optimization, 20(4):1956\u20131982, 2010.\n[13] E. Cand`es. Compressive sampling. In Proc. Int. Congress of Math., Madrid, Spain, Aug. 2006.\n[14] E. Cand`es and M. Davenport. How well can we estimate a sparse vector? Appl. Comput. Harmon.\nAnal., 34(2):317\u2013323, 2013.\n[15] E. Cand`es and Y. Plan. Matrix completion with noise. Proc. IEEE, 98(6):925\u2013936, 2010.\n[16] E. Cand`es and B. Recht. Exact matrix completion via convex optimization. Found. Comput. Math.,\n9(6):717\u2013772, 2009.\n[17] E. Cand`es and T. Tao. The power of convex relaxation: Near-optimal matrix completion. IEEE\nTrans. Inform. Theory, 56(5):2053\u20132080, 2010.\n[18] S. Chatterjee.\nMatrix estimation by universal singular value thresholding.\n2012.\nArxiv preprint\n1212.1247.\n[19] M. Collins, S. Dasgupta, and R. Schapire. A generalization of principal component analysis to the\nexponential family. In Proc. Adv. in Neural Processing Systems (NIPS), Vancouver, BC, Dec. 2001.\n[20] T. Cover and J. Thomas. Elements of information theory. Wiley-Interscience, New York, NY, 1991.\n[21] M. Davenport, M. Duarte, Y. Eldar, and G. Kutyniok. Introduction to compressed sensing. In Y. Eldar\nand G. Kutyniok, editors, Compressed Sensing: Theory and Applications. Cambridge University Press,\nCambridge, UK, 2012.\n[22] J. De Leeuw. Principal component analysis of binary data by iterated singular value decomposition.\nComput. Stat. Data Anal., 50:21\u201339, 2006.\n[23] D. Donoho. Compressed sensing. IEEE Trans. Inform. Theory, 52(4):1289\u20131306, 2006.\n[24] D. Gabay and B. Mercier. A dual algorithm for the solution of nonlinear variational problems via\n\ufb01nite element approximations. Comp. Math. Appl., 2:17\u201340, 1976.\n[25] S. Ga\u00a8\u0131\ufb00as and G. Lecu\u00b4e. Sharp oracle inequalities for the prediction of a high-dimensional matrix.\n2010. Arxiv preprint 1008.4886.\n29\n[26] D. Gleich and L.-H. Lim. Rank aggregation via nuclear norm minimization. In Proc. ACM SIGKDD\nInt. Conf. on Knowledge, Discovery, and Data Mining (KDD), San Diego, CA, Aug. 2011.\n[27] R. Glowinski and A. Marrocco.\nSur l\u2019approximation, par elements \ufb01nis d\u2019ordre un, et la resolu-\ntion, par penalisation-dualit\u00b4e, d\u2019une classe de problems de Dirichlet non lineares. Revue Fran\u00b8caise\nd\u2019Automatique, Informatique, et Recherche Op\u00b4erationelle, 9:41\u201376, 1975.\n[28] D. Goldberg, D. Nichols, B. Oki, and D. Terry. Using collaborative \ufb01ltering to weave an information\ntapestry. Comm. ACM, 35(12):61\u201370, 1992.\n[29] P. Green and Y. Wind.\nMultivariate decisions in marketing: A measurement approach.\nDryden,\nHinsdale, IL, 1973.\n[30] D. Gross. Recovering low-rank matrices from few coe\ufb03cients in any basis. IEEE Trans. Inform.\nTheory, 57(3):1548\u20131566, 2011.\n[31] D. Gross, Y. Liu, S. Flammia, S. Becker, and J. Eisert. Quantum state tomography via compressed\nsensing. Physical Review Letters, 105(15):150401, 2010.\n[32] A. Gupta, R. Nowak, and B. Recht.\nSample complexity for 1-bit compressed sensing and sparse\nclassi\ufb01cation. In Proc. IEEE Int. Symp. Inform. Theory (ISIT), Austin, TX, June 2010.\n[33] L. Jacques, J. Laska, P. Boufounos, and R. Baraniuk. Robust 1-bit compressive sensing via binary\nstable embeddings of sparse vectors. IEEE Trans. Inform. Theory, 59(4):2082\u20132102, 2013.\n[34] S. Kakade, O. Shamir, K. Sridharan, and A. Tewari. Learning exponential families in high-dimensions:\nStrong convexity and sparsity. In Proc. Int. Conf. Art. Intell. Stat. (AISTATS), Clearwater Beach,\nFL, Apr. 2009.\n[35] P. Kar, B. Sriperumbudur, P. Jain, and H. Karnick. On the generalization ability of online learning\nalgorithms for pairwise loss functions. In Proc. Int. Conf. Machine Learning (ICML), Atlanta, GA,\nJune 2013.\n[36] R. Keshavan, A. Montanari, and S. Oh. Matrix completion from a few entries. IEEE Trans. Inform.\nTheory, 56(6):2980\u20132998, 2010.\n[37] R. Keshavan, A. Montanari, and S. Oh. Matrix completion from noisy entries. J. Machine Learning\nResearch, 11:2057\u20132078, 2010.\n[38] M. Khan, B. Marlin, G. Bouchard, and K. Murphy. Variational bounds for mixed-data factor analysis.\nIn Proc. Adv. in Neural Processing Systems (NIPS), Vancouver, BC, Dec. 2010.\n[39] O. Klopp. High dimensional matrix estimation with unknown variance of the noise. 2011. Arxiv\npreprint 1112.3055.\n[40] O. Klopp. Rank penalized estimators for high-dimensional matrices. Elec. J. Stat., 5:1161\u20131183, 2011.\n[41] V. Koltchinskii. Von Neumann entropy penalization and low-rank matrix estimation. Ann. Stat.,\n39(6):2936\u20132973, 2012.\n[42] V. Koltchinskii, K. Lounici, and A. Tsybakov. Nuclear-norm penalization and optimal rates for noisy\nlow-rank matrix completion. Ann. Stat., 39(5):2302\u20132329, 2011.\n[43] J. Laska and R. Baraniuk. Regime change: Bit-depth versus measurement-rate in compressive sensing.\nIEEE Trans. Signal Processing, 60(7):3496\u20133505, 2012.\n[44] M. Ledoux and M. Talagrand. Probability in Banach Spaces: Isoperimetry and Processes. Springer-\nVerlag, Berlin, Germany, 1991.\n[45] Z. Liu and L. Vandenberghe. Interior-point method for nuclear norm approximation with application\nto system identi\ufb01cation. SIAM J. Matrix Analysis and Applications, 31(3):1235\u20131256, 2009.\n[46] L. Meier, S. Van De Geer, and P. B\u00a8uhlmann. The group lasso for logistic regression. J. Royal Stat.\nSoc. B, 70(1):53\u201371, 2008.\n30\n[47] G. Miller. The magical number seven, plus or minus two: Some limits on our capacity for processing\ninformation. Psychological Rev., 63(2):81\u201397, 1956.\n[48] S. Negahban, P. Ravikumar, M. Wainwright, and B. Yu. A uni\ufb01ed framework for high-dimensional\nanalysis of m-estimators with decomposable regularizers. Stat. Sci., 27(4):538\u2013557, 2012.\n[49] S. Negahban and M. Wainwright. Restricted strong convexity and weighted matrix completion: Op-\ntimal bounds with noise. J. Machine Learning Research, 13:1665\u20131697, 2012.\n[50] Y. Nesterov. Introductory Lectures on Convex Optimization. Kluwer Academic Publisher, Dordrecht,\nThe Netherlands, 2004.\n[51] Y. Plan and R. Vershynin. One-bit compressed sensing by linear programming. Comm. Pure Appl.\nMath., 66(8):1275\u20131297, 2013.\n[52] Y. Plan and R. Vershynin. Robust 1-bit compressed sensing and sparse logistic regression: A convex\nprogramming approach. IEEE Trans. Inform. Theory, 59(1):482\u2013494, 2013.\n[53] G. Raskutti, M. Wainwright, and B. Yu. Minimax rates of estimation for high-dimensional linear\nregression over \u2113q-balls. IEEE Trans. Inform. Theory, 57(10):6976\u20136994, 2011.\n[54] P. Ravikumar, M. Wainwright, and J. La\ufb00erty.\nHigh-dimensional Ising model selection using \u21131-\nregularized logistic regression. Ann. Stat., 38(3):1287\u20131319, 2010.\n[55] B. Recht. A simpler approach to matrix completion. J. Machine Learning Research, 12:3413\u20133430,\n2011.\n[56] A. Rohde and A. Tsybakov. Estimation of high-dimensional low-rank matrices. Ann. Stat., 39(2):887\u2013\n930, 2011.\n[57] A. Schein, L. Saul, and L. Ungar. A generalized linear model for principal component analysis of\nbinary data. In Proc. Int. Conf. Art. Intell. Stat. (AISTATS), Key West, FL, Jan. 2003.\n[58] Y. Seginer. The expected norm of random matrices. Combinatorics, Probability, and Computing,\n9(2):149\u2013166, 2000.\n[59] A. Singer. A remark on global positioning from local distances. Proc. Natl. Acad. Sci., 105(28):9507\u2013\n9511, 2008.\n[60] A. Singer and M. Cucuringu. Uniqueness of low-rank matrix completion by rigidity theory. SIAM J.\nMatrix Analysis and Applications, 31(4):1621\u20131641, 2010.\n[61] I. Spence and D. Domoney. Single subject incomplete designs for nonmetric multidimensional scaling.\nPsychometrika, 39(4):469\u2013490, 1974.\n[62] N. Srebro, N. Alon, and T. Jaakkola. Generalization error bounds for collaborative prediction with\nlow-rank matrices. In Proc. Adv. in Neural Processing Systems (NIPS), Vancouver, BC, Dec. 2004.\n[63] N. Srebro, J. D. Rennie, and T. Jaakkola. Maximum-margin matrix factorization. In Proc. Adv. in\nNeural Processing Systems (NIPS), Vancouver, BC, Dec. 2004.\n[64] M. Tipping. Probabilistic visualization of high-dimensional binary data. In Proc. Adv. in Neural\nProcessing Systems (NIPS), Vancouver, BC, Dec. 1998.\n[65] S. Van De Geer. High-dimensional generalized linear models and the lasso. Ann. Stat., 36(2):614\u2013645,\n2008.\n[66] E. van den Berg and M. P. Friedlander. SPGL1, 2007. http://www.cs.ubc.ca/ mpf/spgl1/.\n[67] E. van den Berg and M. P. Friedlander. Probing the Pareto frontier for basis-pursuit solutions. SIAM\nJ. on Scienti\ufb01c Computing, 31(2):890\u2013912, 2008.\n[68] A. Zymnis, S. Boyd, and E. Cand`es. Compressed sensing with quantized measurements. IEEE Signal\nProcessing Lett., 17(2):149\u2013152, 2010.\n31\n",
        "sentence": "",
        "context": "Processing Lett., 17(2):149\u2013152, 2010.\n31\n\u2217M.A. Davenport is with the School of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta,\nGA. Email: {mdav@gatech.edu}.\ncompletion\u2014arises in a wide variety of practical situations. In many of these settings, however, the ob-\nservations are not only incomplete, but also highly quantized, often even to a single bit. In this paper we"
    },
    {
        "title": "Escaping from saddle points\u2014online stochastic gradient for tensor decomposition",
        "author": [
            "Rong Ge",
            "Furong Huang",
            "Chi Jin",
            "Yang Yuan"
        ],
        "venue": null,
        "citeRegEx": "Ge et al\\.,? \\Q2015\\E",
        "shortCiteRegEx": "Ge et al\\.",
        "year": 2015,
        "abstract": "",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "Understanding alternating minimization for matrix completion",
        "author": [
            "Moritz Hardt"
        ],
        "venue": "In FOCS 2014. IEEE,",
        "citeRegEx": "Hardt.,? \\Q2014\\E",
        "shortCiteRegEx": "Hardt.",
        "year": 2014,
        "abstract": "Alternating Minimization is a widely used and empirically successful\nheuristic for matrix completion and related low-rank optimization problems.\nTheoretical guarantees for Alternating Minimization have been hard to come by\nand are still poorly understood. This is in part because the heuristic is\niterative and non-convex in nature. We give a new algorithm based on\nAlternating Minimization that provably recovers an unknown low-rank matrix from\na random subsample of its entries under a standard incoherence assumption. Our\nresults reduce the sample size requirements of the Alternating Minimization\napproach by at least a quartic factor in the rank and the condition number of\nthe unknown matrix. These improvements apply even if the matrix is only close\nto low-rank in the Frobenius norm. Our algorithm runs in nearly linear time in\nthe dimension of the matrix and, in a broad range of parameters, gives the\nstrongest sample bounds among all subquadratic time algorithms that we are\naware of.\n  Underlying our work is a new robust convergence analysis of the well-known\nPower Method for computing the dominant singular vectors of a matrix. This\nviewpoint leads to a conceptually simple understanding of Alternating\nMinimization. In addition, we contribute a new technique for controlling the\ncoherence of intermediate solutions arising in iterative algorithms based on a\nsmoothed analysis of the QR factorization. These techniques may be of interest\nbeyond their application here.",
        "full_text": "Understanding Alternating Minimization\nfor Matrix Completion\nMoritz Hardt\u2217\nMay 15, 2014\nAbstract\nAlternating minimization is a widely used and empirically successful heuristic for matrix\ncompletion and related low-rank optimization problems. Theoretical guarantees for alternating\nminimization have been hard to come by and are still poorly understood. This is in part because\nthe heuristic is iterative and non-convex in nature. We give a new algorithm based on alternating\nminimization that provably recovers an unknown low-rank matrix from a random subsample\nof its entries under a standard incoherence assumption. Our results reduce the sample size\nrequirements of the alternating minimization approach by at least a quartic factor in the rank\nand the condition number of the unknown matrix. These improvements apply even if the matrix\nis only close to low-rank in the Frobenius norm. Our algorithm runs in nearly linear time in the\ndimension of the matrix and, in a broad range of parameters, gives the strongest sample bounds\namong all subquadratic time algorithms that we are aware of.\nUnderlying our work is a new robust convergence analysis of the well-known Power Method\nfor computing the dominant singular vectors of a matrix. This viewpoint leads to a conceptually\nsimple understanding of alternating minimization. In addition, we contribute a new technique\nfor controlling the coherence of intermediate solutions arising in iterative algorithms based on\na smoothed analysis of the QR factorization. These techniques may be of interest beyond their\napplication here.\n\u2217IBM Research Almaden. Email: mhardt@us.ibm.com\narXiv:1312.0925v3  [cs.LG]  14 May 2014\n1\nIntroduction\nAlternating minimization is an empirically successful heuristic for the matrix completion problem\nin which the goal is to recover an unknown low-rank matrix from a subsample of its entries. Matrix\ncompletion has received a tremendous amount of attention over the past few years due to its\nfundamental role as an optimization problem and its applicability in number of areas including\ncollaborative \ufb01ltering and quantum tomography. Alternating minimization has been used early on\nin the context of matrix completion [BK, HH] and continues to play an important role in practical\napproaches to the problem. The approach also formed an important component in the winning\nsubmission for the Net\ufb02ix Prize [KBV].\nGiven a subset \u2126of entries drawn from an unknown matrix A, Alternating minimization starts\nfrom a poor approximation X0Y \u22a4\n0 to the target matrix and gradually improves the approximation\nquality by \ufb01xing one of the factors and minimizing a certain objective over the other factor. Here,\nX0,Y0 each have k columns where k is the target rank of the factorization. The least squares\nobjective is the typical choice. In this case, at step \u2113we solve the optimization problem\nX\u2113= argmin\nX\nX\n(i,j)\u2208\u2126\nh\nAij \u2212(XY \u22a4\n\u2113\u22121)ij\ni2 .\nThis optimization step is then repeated with X\u2113\ufb01xed in order to determine Y\u2113as\nY\u2113= argmin\nX\nX\n(i,j)\u2208\u2126\nh\nAij \u2212(X\u2113Y \u22a4)ij\ni2 .\nSeparating the factors X\u2113and Y\u2113is what makes the optimization step tractable. This basic update\nstep is usually combined with an initialization procedure for \ufb01nding X0,Y0, as well as methods for\nmodifying intermediate solutions, e.g., truncating large entries. More than a speci\ufb01c algorithm we\nthink of alternating minimization as a framework for solving a non-convex low-rank optimization\nproblem.\nA major advantage of alternating minimization over alternatives is that each update is com-\nputationally cheap and has a small memory footprint as we only need to keep track of 2k vectors.\nIn contrast, the nuclear norm approach to matrix completion [CR, Rec, CT] requires solving a\nsemide\ufb01nite program. The advantage of the nuclear norm approach is that it comes with strong\ntheoretical guarantees under certain assumptions on the unknown matrix and the subsample of its\nentries. There are two (by now standard) assumptions which together imply that nuclear norm\nminimization succeeds. The \ufb01rst is that the subsample \u2126includes each entry of A uniformly at\nrandom with probability p. The second assumption is that the \ufb01rst k singular vectors of A span\nan incoherent subspace. Informally coherence measures the correlation of the subspace with any\nstandard basis vector. More formally, the coherence of a k-dimensional subspace of Rn is at most \u00b5\nif the projection of each standard basis vector has norm at most\np\n\u00b5k/n. The space spanned by the\ntop k singular space of various random matrix models typically satis\ufb01es this property with small \u00b5.\nBut also real-world matrices tend to exhibit incoherence when k is reasonably small.\nTheoretical results on matrix completion primarily apply to the nuclear norm semide\ufb01nite\nprogram which is prohibitive to execute on realistic instance sizes. There certainly has been\nprogress on practical algorithms for solving related convex programs [JY, MHT, JS, AKKS, HO].\nUnfortunately, these algorithms are not known to achieve the same type of recovery guarantees\nattained by exact nuclear norm minimization. This raises the important question if there are\nfast algorithms for matrix completion that come with guarantees on the required sample size\ncomparable to those achieved by nuclear norm minimization. In this work we make progress on\n2\nthis problem by proving strong sample complexity bounds for alternating minimization. Along\nthe way our work helps to give a theoretical justi\ufb01cation and understanding for why alternating\nminimization works.\n1.1\nOur results\nWe begin with our result on the exact matrix completion problem where the goal is to recover\nan unknown rank k matrix M from a subsample \u2126of its entries where each entry is included\nindependently with probability p. Here and in the following we will always assume that M =\nU\u039bU\u22a4is a symmetric n \u00d7 n matrix with singular values \u03c31 \u2a7e... \u2a7e\u03c3k. Our result generalizes\nstraightforwardly to rectangular matrices as we will see.\nOur algorithm will output a pair of matrices (X,Y) where X is an orthonormal n \u00d7 k matrix\nthat approximates U in the strong sense that \u2225(I \u2212UU\u22a4)X\u2225\u2a7d\u03b5. Moreover, the matrix XY \u22a4is\nclose to M in Frobenius norm. To state the theorem we formally de\ufb01ne the coherence of U as\n\u00b5(U) def\n= maxi\u2208[n](n/k)\u2225e\u22a4\ni U\u22252\n2 where ei is the i-th standard basis vector.\nTheorem 1.1. Given a sample of size e\nO(pn2) drawn from an unknown n \u00d7 n matrix M = U\u039bU\u22a4of\nrank k by including each entry with probability p, our algorithm outputs with high probability a pair of\nmatrices (X,Y) such that \u2225(I \u2212UU\u22a4)X\u2225\u2a7d\u03b5 and \u2225M \u2212XY \u22a4\u2225F \u2a7d\u03b5\u2225M\u2225F provided that\npn \u2a7ek(k + log(n/\u03b5))\u00b5(U)(\u2225M\u2225F/\u03c3k)2 .\n(1)\nOur result should be compared with two remarkable recent works by Jain, Netrapalli and\nSanghavi [JNS] and Keshavan [Kes] who gave rigorous sample complexity bounds for alternating\nminimization. [JNS] obtained the bound pn \u2a7ek7(\u03c31/\u03c3k)6\u00b5(U)2 and Keshavan obtained the incompa-\nrable bound pn \u2a7ek(\u03c31/\u03c3k)8\u00b5(U) that is superior when the matrix has small condition number \u03c31/\u03c3k.\nSince \u2225M\u2225F \u2a7d\n\u221a\nk\u03c31 our result improves upon [JNS] by at least a factor of k4(\u03c31/\u03c3k)4\u00b5(U) and\nimproves on [Kes] as soon as \u03c31/\u03c3k \u226bk1/3. The improvement is larger when \u2225M\u2225F = O(\u03c31) which\nwe expect if the singular values decay rapidly.\nTheorem 1.1 is a special case of Theorem 6.1. We remark that the number of least squares\nupdate steps is bounded by O(log(n/\u03b5)logn). The cost of performing these update steps is up to a\nlogarithmic factor what dominates the worst-case running time of our algorithm. It can be seen\nthat the least squares problem can be solved in time O\n\u0010\nnk3 + |\u2126| \u00b7 k\n\u0011\nwhich is is linear in n +|\u2126| and\npolynomial in k. The number of update steps enters the sample complexity since we assume (as in\nprevious work) that fresh samples are used in each step. However, the logarithmic dependence\non 1/\u03b5 guarantees exponentially fast convergence and allows us to obtain any inverse polynomial\nerror with only a constant factor overhead in sample complexity.\nNoisy matrix completion.\nIn noisy matrix completion the unknown matrix is only close to low-\nrank, typically in Frobenius norm. Our results apply to any matrix of the form A = M + N, where\nM = U\u039bU\u22a4is a matrix of rank k as before and N = (I \u2212UU\u22a4)A is the part of A not captured by\nthe dominant singular vectors. Here, N can be an arbitrary deterministic matrix that satis\ufb01es the\nfollowing constraints:\nmax\ni\u2208[n] \u2225e\u22a4\ni N\u22252 \u2a7d\u00b5N\nn \u00b7 \u03c32\nk\nand\nmax\nij\u2208[n]|Nij| \u2a7d\u00b5N\nn \u00b7 \u2225A\u2225F .\n(2)\nHere, ei denotes the i-th standard basis vector so that \u2225e\u22a4\ni N\u2225is the Euclidean norm of the i-th row\nof N. The conditions state no entry and no row of N should be too large compared to the Frobenius\n3\nnorm of N. We can think of the parameter \u00b5N as an analog to the coherence parameter \u00b5(U) that we\nsaw earlier. Since N could be close to full rank, \u00b5(V ) is not a meaningful parameter in general. If\nthe rank of V is k, then our assumptions roughly reduce to what is implied by requiring \u00b5(V ) \u2a7d\u00b5N.\nFrom here on we let \u00b5\u2217= max\b\u00b5(U),\u00b5N,logn\t. We have the following theorem.\nTheorem 1.2. Given a sample of size e\nO(pn2) drawn from an unknown n \u00d7 n matrix A = M + N where\nM = U\u039bU\u22a4has rank k and N = (I \u2212UU\u22a4)M satis\ufb01es (2), our algorithm outputs with high probability\n(X,Y) such that \u2225(I \u2212UU\u22a4)X\u2225\u2a7d\u03b5 and \u2225M \u2212XY \u22a4\u2225F \u2a7d\u03b5\u2225A\u2225F provided that\npn \u2a7ek(k + log(n/\u03b5))\u00b5\u2217\n \u2225M\u2225F + \u2225N\u2225F/\u03b5\n\u03c3k\n!2 \u0012\n1 \u2212\u03c3k+1\n\u03c3k\n\u00135\n.\n(3)\nThe theorem is a strict generalization of the noise-free case which we recover by setting N = 0\nin which case the separation parameter \u03b3k := 1 \u2212\u03c3k+1/\u03c3k is equal to 1. The result follows from\nTheorem 6.1 that gives a somewhat stronger sample complexity bound. Compared to our noise-\nfree bound, there are two new parameters that enter the sample complexity. The \ufb01rst one is the\nseparation parameter \u03b3k. The second is the quantity \u2225N\u2225F/\u03b5. To interpret this quantity, suppose\nthat that A has a good low-rank approximation in Frobenius norm, formally, \u2225N\u2225F \u2a7d\u03b5\u2225A\u2225F for\n\u03b5 \u2a7d1/2, then it must also be the case that \u2225N\u2225F/\u03b5 \u2a7d2\u2225M\u2225F. Our algorithm then \ufb01nds a good rank k\napproximation with at most e\nO(k3(\u03c31/\u03c3k)2\u00b5\u2217n) samples assuming \u03b3k = \u2126(1). Hence, assuming that\nA has a good rank k approximation in Frobenius norm and that \u03c3k and \u03c3k+1 are well-separated, our\nbound recovers the noise-free bound up to a constant factor.\nNote that if we\u2019re only interested in the second error bound \u2225M \u2212XY \u22a4\u2225F \u2a7d\u03b5\u2225M\u2225F + \u2225N\u2225F, we\nwe can eliminate the dependence on the condition number in the sample complexity entirely. The\nreason is that any singular value smaller than \u03b5\u03c31/k can be treated as part of the noise matrix.\nAssuming the condition number is at least k to begin with we can always \ufb01nd two singular values\nthat have separation at least \u2126(k). This ensures that the sample requirement is polynomial in k\nwithout any dependence on the condition number and gives us the following corollary.\nCorollary 1.3. Under the assumptions of Theorem 1.2, if \u03c31 \u2a7ek\u03c3k/\u03b5, then we can \ufb01nd X,Y such that\n\u2225M \u2212XY \u22a4\u2225F \u2a7d\u03b5\u2225A\u2225F provided that pn \u2a7epoly(k)\u00b5\u2217.\nThe previous corollary is remarkable, because small error in Frobenius norm is the most\ncommon error measure in the literature on matrix completion. The result shows that in this error\nmeasure, there is no dependence on the condition number. The result is tight for k = O(1) up to\nconstant factors even information-theoretically as we will discuss below.\nThe approach of Jain et al. was adapted to the noisy setting by Gunasekar et al. [GAGG] showing\nroughly same sample complexity in the noisy setting under some assumptions on the noise matrix.\nWe achieve the same improvements over [GAGG] as we did compared to [JNS] in the noise-free\ncase. Moreover, our assumptions in (2) are substantially weaker than the assumption of [GAGG].\nThe latter work required the largest entry of N in absolute value to be bounded by O(\u03c3k/n\n\u221a\nk).\nThis directly implies that each row of N has norm at most O(\u03c3k/\n\u221a\nkn) and that \u2225N\u2225F \u2a7dO(\u03c3k/\n\u221a\nk).\nMoreover under this assumption we would have \u03b3k \u2a7e1 \u2212ok(1). Keshavan\u2019s result [Kes] also applies\nto the noisy setting, but it requires \u2225N\u2225\u2a7d(\u03c3k/\u03c31)3 and maxi \u2225e\u22a4\ni N\u2225\u2a7d\np\n\u00b5(U)k/n\u2225N\u2225. In particular\nthis bound does not allow \u2225N\u2225F to grow with \u2225M\u2225F. Since neither result allows arbitrarily small\nsingular value separation, we cannot use these results to eliminate the dependence on the condition\nnumber as is possible using our technique.\n4\nRemark on required sample complexity and assumptions.\nIt is known that information-theoretically\n\u2126(k\u00b5(U)n) measurements are necessary to recover the unknown matrix [CT] and this bound is\nachieved (up to log-factors) by the nuclear norm semide\ufb01nite program. Compared with the\ninformation-theoretic optimum our bound su\ufb00ers a factor O(k(\u2225M\u2225F/\u03c3k)2) loss. While we do not\nknow if this loss is necessary, there is a natural barrier. If we denote by P\u2126(A) the matrix in which\nall unobserved entries are 0 and the others are scaled by 1/p, then \u2126(k\u00b5(U)(\u2225M\u2225F/\u03c3k)2n) samples\nare necessary to ensure that P\u2126(A) preserves the k-th singular value to within constant relative\nerror. Formally, \u2225P\u2126(A) \u2212A\u22252 \u2a7d0.1\u03c3k. While this is not a necessary requirement for alternating\nleast squares, it represents the current bottleneck for \ufb01nding a good initial matrix.\nIt is also known that without an incoherence assumption the matrix completion problem\ncan be ill-posed and recovery becomes infeasible even information-theoretically [CT]. Moreover,\neven on incoherent matrices it was recently shown that already the exact matrix completion\nproblem remains computationally hard to approximate in a strong sense [HMRW]. This shows that\nadditional assumptions are needed beyond incoherence to make the problem tractable.\n2\nProof overview and techniques\nRobust convergence of subspace iteration.\nAn important observation of [JNS] is that the update\nrule in alternating minimization can be analyzed as a noisy update step of the well known power\nmethod for computing eigenvectors, also called subspace iteration when applied to multiple vectors\nsimultaneously. The noise term that arises depends on the sampling error induced by the subsample\nof the entries. We further develop this point of view by giving a new robust convergence analysis\nof the power method.\nTo illustrate the technique, consider a model of numerical linear algebra in which an input\nmatrix A can only be accessed through noisy matrix vector products of the form Ax + g, where\nx is a chosen vector and g is a possibly adversarial noise term. Our goal is to compute the\ndominant singular vectors u1,...,uk of the matrix A. Subspace iteration starts with an initial guess,\nan orthonormal matrix X0 \u2208Rn\u00d7k typically chosen at random. The algorithm then repeatedly\ncomputes Y\u2113= AX\u2113\u22121 + G\u2113, followed by an orthonormalization step in order to obtain X\u2113from Y\u2113.\nHere, G\u2113is the noise variable added to the computation.\nTheorem 3.8 characterizes the convergence behavior of this general algorithm. An important\ncomponent of our analysis is the choice of a suitable potential function that decreases at each step.\nHere we make use of the tangent of the largest principal angle between the subspace U spanned\nby the \ufb01rst k singular vectors of the input matrix and the k-dimensional space spanned by the\ncolumns of the iterate X\u2113. Principal angles are a very useful tool in numerical analysis that we\nbrie\ufb02y recap in Section 3. Our analysis shows that the algorithm essentially converges at the rate of\n(\u03c3k+1 + \u2206)/(\u03c3k \u2212\u2206) for some \u2206\u226a\u03c3k under suitable conditions on the noise matrix G\u2113.\nAlternating least squares.\nWe recall the well-known least squares update:\nY\u2113= argmin\nY\n\u2225P\u2126(A \u2212X\u2113\u22121Y \u22a4)\u22252\nF .\n(4)\nSince we can focus on symmetric matrices without loss of generality, there is no need for an\nalternating update in which the left and right factor are \ufb02ipped. We therefore drop the term\n\u201calternating\u201d. We can express the optimal Y\u2113as Y\u2113= AX\u2113\u22121 + G\u2113using gradient information about\nthe least squares objective. The error term G\u2113has an intriguing property. Its norm \u2225G\u2113\u2225depends\non the quantity \u2225V \u22a4X\u2113\u22121\u2225which coincides with the sine of the largest principal angle between U\n5\nand X\u2113\u22121. This property ensures that as the algorithm begins to converge the norm of the error\nterm starts to diminish. Near exact recovery is now possible (assuming the matrix has rank at\nmost k). A novelty in our approach is that we obtain strong bounds on \u2225G\u2113\u2225by computing O(logn)\nindependent copies of Y\u2113(using fresh samples) and taking the componentwise median of the\nresulting matrices. The resulting procedure called MedianLS is analyzed in Section 4.\nA di\ufb03culty with iterating the least squares update in general is that it is unclear how well\nit converges from a random initial matrix X0. In our analysis we therefore use an initialization\nprocedure that \ufb01nds a matrix X0 that satis\ufb01es \u2225V \u22a4X0\u2225\u2a7d1/4. Our initialization procedure is based\non (approximately) computing the \ufb01rst k singular vectors of P\u2126(A). To rule out large entries in the\nvectors we truncate the resulting vectors. While this general approach is standard, our truncation\nprocedure \ufb01rst applies a random rotation to the vectors that leads to a tighter analysis than the\nnaive approach.\nSmooth orthonormalization.\nA key novelty in our approach is the way we argue about the\ncoherence of each iterate X\u2113. Ideally, we would like to argue that \u00b5(X\u2113) = O(\u00b5\u2217). A direct approach\nwould be to argue that X\u2113was obtained from Y\u2113using the QR-factorization and so X\u2113= Y\u2113R\u22121 for\nsome invertible R. This gives the bound \u2225e\u22a4\ni X\u2113\u2225\u2a7d\u2225e\u22a4\ni Y\u2113\u2225\u00b7 \u2225R\u22121\u2225that unfortunately is quite lossy\nand leads to a dependence on the condition number.\nWe avoid this problem using an idea that\u2019s closely related to the smoothed analysis of the QR-\nfactorization. Sankar, Spielman and Teng [SST] showed that while the perturbation stability of\nQR can be quadratic, it is constant after adding a su\ufb03ciently large amount of Gaussian noise.\nIn the context of smoothed analysis this is usually interpreted as saying that there are \u201cfew bad\ninputs\u201d for the QR factorization. In our context, the matrix Y\u2113is already the outcome of a noisy\noperation Y\u2113= AX\u2113\u22121 + G\u2113and so there is no harm in actually adding a Gaussian noise matrix H\u2113\nto Y\u2113provided that the norm of that matrix is no larger than that of G\u2113. Roughly speaking, this\nwill allow us to argue that there is no dependence on the condition number when applying the\nQR-factorization to Y\u2113. There are some important complications. The magnitude of Y\u2113may be too\nlarge to apply the smoothed analysis argument directly to Y\u2113. Instead we observe that the columns\nof X\u2113are contained in the range S of the matrix [U | (NX\u2113\u22121 + G\u2113+ H\u2113)]. Since S has dimension at\nmost 2k it su\ufb03ces to argue that this space has small coherence. Moreover we can choose H\u2113to\nbe roughly on the same order as NX\u2113\u22121 and G\u2113so that the smoothed analysis argument leads to\nan excellent bound bound on the smallest singular value of NX\u2113\u22121 + G\u2113+ H\u2113. To prove that the\ncoherence is small we need to exhibit a basis for S. This requires us to argue about the related\nmatrix (I \u2212UU\u22a4)(NX\u2113\u22121 + G + H\u2113) since we need to orthonormalize the last k vectors against the\n\ufb01rst when constructing a basis. Another minor complication is that we don\u2019t know the magnitude\nof G\u2113so we need to \ufb01nd the right scaling of H\u2113on the \ufb02y. We call the resulting procedure that\nSmoothQR and analyze its guarantees in Section 5.\nPutting things together.\nThe \ufb01nal algorithm that we analyze is quite simple to describe as\nshown in Figure 1. The algorithm makes use of an initialization procedure Initialize that we\ndefer to Section 7. In Section 6 we prove our main theorem. The generalization of our result to\nrectangular matrices follows from a standard \u201cdilation\u201d argument that we describe in Section D.\nThe description of the algorithm also uses a helper function called Split that\u2019s used to split\nthe subsample into independent pieces of roughly equal size while preserving the distributional\nassumption that our theorems use. We discuss Split in Section C.\n6\nInput: Observed set of indices \u2126\u2286[n] \u00d7 [n] of an unknown symmetric matrix A \u2208Rn\u00d7n with\nentries P\u2126(A), number of iterations L \u2208N, error parameter \u03b5 > 0, target dimension k, coherence\nparameter \u00b5.\nAlgorithm SAltLS(P\u2126(A),\u2126,L,k,\u03b5,\u00b5) :\n1. (\u21260,\u2126\u2032) \u2190Split(\u2126,2), (\u21261,...,\u2126L) \u2190Split(\u2126\u2032,L)\n2. X0 \u2190Initialize(P\u21260(A),\u21260,k,\u00b5)\n3. For \u2113= 1 to L:\n(a) Y\u2113\u2190MedianLS(P\u2126\u2113(A),\u2126\u2113,X\u2113\u22121,L,k)\n(b) X\u2113\u2190SmoothQR(Y\u2113,\u03b5,\u00b5)\nOutput: Pair of matrices (XL\u22121,YL)\nFigure 1: Smoothed alternating least squares (SAltLS)\n2.1\nFurther discussion of related work\nThere is a vast literature on the topic that we cannot completely survey here. Most closely related is\nthe work of Jain et al. [JNS] that suggested the idea of thinking of alternating least squares as a noisy\nupdate step in the Power Method. Our approach takes inspiration from this work by analyzing\nleast squares using the noisy power method. However, our analysis is substantially di\ufb00erent in\nboth how convergence and low coherence is argued. The approach of Keshavan [Kes] uses a rather\ndi\ufb00erent argument.\nAs an alternative to the nuclear norm approach, Keshavan, Montanari and Oh [KMO1, KMO2]\npresent two approaches, a spectral approach and an algorithm called OptSpace. The spectral ap-\nproach roughly corresponds to our initialization procedure and gives similar guarantees. OptSpace\nrequires a stronger incoherence assumption, has larger sample complexity in terms of the condition\nnumber, namely (\u03c31/\u03c3k)6, and requires optimizing over the Grassmanian manifold. However, the\nrequirement on N achieved by OptSpace can be weaker than ours in the noisy setting. In the exact\ncase, our algorithm has a much faster convergence rate (logarithmic dependence on 1/\u03b5 rather than\npolynomial).\nThere are a number of fast algorithms for matrix completion based on either (stochastic)\ngradient descent [RR] or (online) Frank-Wolfe [JS, HK]. These algorithms generally minimize\nsquared loss on the observed entries subject to a nuclear norm constraint and in general do not\nproduce a matrix that is close to the true unknown matrix on all entries. In contrast, our algorithm\nguarantees convergence in domain, that is, to the unknown matrix itself. Moreover, our dependence\non the error is logarithmic whereas in these algorithms it is polynomial.\nPrivacy-preserving spectral analysis.\nOur work is also closely related to a line of work on\ndi\ufb00erentially private singular vector computation [HR1, HR2, Har]. These papers each consider\nalgorithms based on the power method where noise is injected to achieve the privacy guarantee\nknown as Di\ufb00erential Privacy [DMNS]. Hardt and Roth [HR1, HR2, Har] observed that incoherence\ncould be used to obtain improved guarantees. This requires controlling the coherence of the iterates\nproduced by the noisy power method which leads to similar problems as the ones faced here.\nWhat\u2019s simpler in the privacy setting is that the noise term is typically Gaussian leading to a cleaner\nanalysis. Our work uses a similar convergence analysis for noisy subspace iteration that was used\n7\nin a concurrent work by the author [HR2].\n2.2\nPreliminaries and Notation\nWe denote by A\u22a4the transpose of a matrix (or vector) A. We use the notation x \u2273y do denote that\nthe relation x \u2a7eCy holds for a su\ufb03ciently large absolute constant C > 0 independent of x and y.\nWe let R(A) denote the range of the matrix A. The coherence of a subspace plays an important role\nin our analysis.\nDe\ufb01nition 2.1 (Coherence). The \u00b5-coherence of a k-dimensional subspace U of Rn is de\ufb01ned as\n\u00b5(U) def\n= maxi\u2208[n] n\nk \u2225PUei\u22252\n2 , where ei denotes the i-th standard basis vector.\n3\nRobust local convergence of subspace iteration\nFigure 2 presents our basic template algorithm. The algorithm is identical to the standard subspace\niteration algorithm except that in each iteration \u2113, the computation is perturbed by a matrix G\u2113.\nThe matrix G\u2113can be adversarially and adaptively chosen in each round. We will analyze under\nwhich conditions on the perturbation we can expect the algorithm to converge rapidly.\nInput: Matrix A \u2208Rn\u00d7n, number of iterations L \u2208N, target dimension k\n1. Let X0 \u2208Rn\u00d7k be an orthonormal matrix.\n2. For \u2113= 1 to L:\n(a) Let G\u2113\u2208Rn\u00d7k be an arbitrary perturbation.\n(b) Y\u2113\u2190AX\u2113\u22121 + G\u2113\n(c) X\u2113\u2190GS(Y\u2113)\nOutput: Matrix XL with k orthonormal columns\nFigure 2: Noisy Subspace Iteration (NSI)\nPrincipal angles are a useful tool in analyzing the convergence behavior of numerical eigenvalue\nmethods. We will use the largest principal angle between two subspaces as a potential function in\nour convergence analysis.\nDe\ufb01nition 3.1. Let X,Y \u2208Rn\u00d7k be orthonormal bases for subspaces X ,Y, respectively. Then, the\nsine of the largest principal angle between X and Y is de\ufb01ned as sin\u03b8(X ,Y) def\n= \u2225(I \u2212XX\u22a4)Y\u2225.\nWe use some standard properties of the largest principal angle.\nProposition 3.2 ([ZK]). Let X ,Y,X,Y be as in De\ufb01nition 3.1 and let X\u22a5be an orthonormal basis for the\northogonal complement of X . Then, we have cos\u03b8(X ,Y) = \u03c3k(X\u22a4Y). and assuming X\u22a4Y is invertible,\ntan\u03b8(X ,Y) = \u2225X\u22a4\n\u22a5Y(X\u22a4Y)\u22121\u2225\nFrom here on we will always assume that A has the spectral decomposition\nA = U\u039bUU\u22a4+ V \u039bV V \u22a4,\n(5)\n8\nwhere U \u2208Rn\u00d7k,V \u2208Rn\u00d7(n\u2212k) corresponding to the \ufb01rst k and last n\u2212k eigenvectors respectively. We\nwill let \u03c31 \u2a7e... \u2a7e\u03c3n denote the singular values of A which coincide with the absolute eigenvalues\nof A sorted in non-increasing order.\nOur convergence analysis tracks the tangent of the largest principal angles between the sub-\nspaces R(U) and R(X\u2113). The next lemma shows a natural condition under which the potential\ndecreases multiplicatively in step \u2113. We think of this lemma as a local convergence guarantee, since\nit assumes that the cosine of the largest principal angle between R(U) and R(X\u2113\u22121) is already lower\nbounded by a constant.\nLemma 3.3 (One Step Local Convergence). Let \u2113\u2208{1,...,L}. Assume that\ncos\u03b8k(U,X\u2113\u22121) \u2a7e1\n2 > \u2225U\u22a4G\u2113\u2225\n\u03c3k\n.\nThen,\ntan\u03b8(U,X\u2113) \u2a7dtan\u03b8(U,X\u2113\u22121) \u00b7\n\u03c3k+1 +\n2\u2225V \u22a4G\u2113\u2225\ntan\u03b8(U,X\u2113\u22121)\n\u03c3k \u22122\u2225U\u22a4G\u2113\u2225\n.\n(6)\nProof. We \ufb01rst need to verify that X\u2113has rank k. This follows if we can show that \u03c3k(Y\u2113) > 0. Indeed,\n\u03c3k(Y\u2113) \u2a7e\u03c3k(U\u22a4Y\u2113) = \u03c3k(\u039bUU\u22a4X\u2113\u22121 + U\u22a4G\u2113) \u2a7e\u03c3k \u00b7 \u03c3k(U\u22a4X\u2113\u22121) \u2212\u2225U\u22a4G\u2113\u2225.\nThe right hand side is strictly greater than zero by our assumption, because \u03c3k(U\u22a4X\u2113\u22121) =\ncos\u03b8k(U,X\u2113\u22121). Further, we have X\u2113= Y\u2113R for some invertible transformation R. Therefore, U\u22a4X\u2113\nis invertible and we can invoke Proposition 3.2 to express tan\u03b8(U,X\u2113) as:\n\r\r\rV \u22a4X\u2113(U\u22a4X\u2113)\u22121\r\r\r =\n\r\r\rV \u22a4Y\u2113RR\u22121(U\u22a4Y\u2113)\u22121\r\r\r =\n\r\r\rV \u22a4Y\u2113(U\u22a4Y\u2113)\u22121\r\r\r .\nUsing the fact that Y\u2113= AX\u2113\u22121 + G\u2113,\n\r\r\rV \u22a4Y\u2113(U\u22a4Y\u2113)\u22121\r\r\r =\n\r\r\rV \u22a4Y\u2113(\u039bUU\u22a4X\u2113\u22121 + U\u22a4G\u2113)\u22121\r\r\r\n=\n\r\r\r\rV \u22a4Y\u2113\n\u0010\u0010\n\u039bU + U\u22a4G\u2113(U\u22a4X\u2113\u22121)\u22121\u0011\nU\u22a4X\u2113\u22121\n\u0011\u22121\r\r\r\r\n=\n\r\r\r\rV \u22a4Y\u2113(U\u22a4X\u2113\u22121)\u22121 \u0010\n\u039bU + U\u22a4G\u2113(U\u22a4X\u2113\u22121)\u22121\u0011\u22121\r\r\r\r\nPutting S = \u039bU + U\u22a4G\u2113(U\u22a4X\u2113\u22121)\u22121, we therefore get\n\r\r\rV \u22a4Y\u2113(U\u22a4Y\u2113)\u22121\r\r\r \u2a7d\n\r\r\rV \u22a4Y\u2113(U\u22a4X\u2113\u22121)\u22121S\u22121\r\r\r\n\u2a7d\n\r\r\rV \u22a4Y\u2113(U\u22a4X\u2113\u22121)\u22121\r\r\r \u00b7\n\r\r\rS\u22121\r\r\r =\n\r\r\rV \u22a4Y\u2113(U\u22a4X\u2113\u22121)\u22121\r\r\r\n\u03c3k(S)\n.\nIn the second inequality we used the fact that for any two matrices P,Q we have \u2225PQ\u2225\u2a7d\u2225P\u2225\u00b7 \u2225Q\u2225.\nLet us bound the numerator of the RHS as follows:\n\r\r\rV \u22a4Y\u2113(U\u22a4X\u2113\u22121)\u22121\r\r\r =\n\r\r\r\u039bV V \u22a4X\u2113\u22121(U\u22a4X\u2113\u22121)\u22121 + V \u22a4G\u2113(U\u22a4X\u2113\u22121)\u22121\r\r\r\n=\n\r\r\r\u039bV V \u22a4X\u2113\u22121(U\u22a4X\u2113\u22121)\u22121\r\r\r +\n\r\r\rV \u22a4G\u2113(U\u22a4X\u2113\u22121)\u22121\r\r\r\n= \u2225\u039bV \u2225\u00b7\n\r\r\rV \u22a4X\u2113\u22121(U\u22a4X\u2113\u22121)\u22121\r\r\r +\n\r\r\rV \u22a4G\u2113(U\u22a4X\u2113\u22121)\u22121\r\r\r\n= \u03c3k+1 \u00b7 tan\u03b8(U,X\u2113\u22121) +\n\r\r\rV \u22a4G\u2113(U\u22a4X\u2113\u22121)\u22121\r\r\r\n\u2a7d\u03c3k+1 \u00b7 tan\u03b8(U,X\u2113\u22121) +\n\r\r\rV \u22a4G\u2113\n\r\r\r \u00b7\n\r\r\r(U\u22a4X\u2113\u22121)\u22121\r\r\r\n= \u03c3k+1 \u00b7 tan\u03b8(U,X\u2113\u22121) +\n\u2225V \u22a4G\u2113\u2225\ncos\u03b8k(U,X\u2113\u22121)\n= \u03c3k+1 \u00b7 tan\u03b8(U,X\u2113\u22121) + 2\n\r\r\rV \u22a4G\u2113\n\r\r\r .\n9\nHere we used the fact that\n\u2225(U\u22a4X\u2113\u22121)\u22121\u2225=\n1\n\u03c3k(U\u22a4X\u2113\u22121) =\n1\ncos\u03b8k(U,X\u2113\u22121) .\nWe also need a lower bound on \u03c3k(S). Indeed,\n\u03c3k(S) \u2a7e\u03c3k(\u039bU) \u2212\n\r\r\rU\u22a4G\u2113(U\u22a4X\u2113\u22121)\u22121\r\r\r\n\u2a7e\u03c3k \u2212\n\r\r\rU\u22a4G\u2113\n\r\r\r \u00b7\n\r\r\r(U\u22a4X\u2113\u22121)\u22121\r\r\r = \u03c3k \u22122\n\r\r\rU\u22a4G\u2113\n\r\r\r .\nNote that the RHS is strictly positive due to the assumption of the lemma. Summarizing what we\nhave,\ntan\u03b8(U,X\u2113) \u2a7d\u03c3k+1 \u00b7 tan\u03b8(U,X\u2113\u22121) + 2\u2225V \u22a4G\u2113\u2225\n\u03c3k \u22122\u2225U\u22a4G\u2113\u2225\n.\nThis is equivalent to the statement of the lemma as we can see from a simple rearrangement.\n\u25a0\nThe next lemma essentially follows by iterating the previous lemma.\nLemma 3.4 (Local Convergence). Let 0 \u2a7d\u03b5 \u2a7d1/4. Let \u2206= max1\u2a7d\u2113\u2a7dL \u2225G\u2113\u2225and \u03b3k = 1 \u2212\u03c3k+1/\u03c3k.\nAssume that \u2225V \u22a4X0\u2225\u2a7d1/4 and \u03c3k \u2a7e8\u2206/\u03b3k\u03b5. Then,\n\r\r\rV \u22a4XL\n\r\r\r \u2a7dmax\nn\n\u03b5,2 \u00b7\n\r\r\rV \u22a4X0\n\r\r\r \u00b7 exp(\u2212\u03b3kL/2)\no\n.\nProof. Our \ufb01rst claim shows that once the potential function is below \u03b5 at step \u2113\u22121, it cannot\nincrease beyond \u03b5.\nClaim 3.5. Let \u2113\u2a7e1. Suppose that tan\u03b8(U,X\u2113\u22121) \u2a7d\u03b5. Then, tan\u03b8(U,X\u2113) \u2a7d\u03b5.\nProof. By our assumption, cos\u03b8k(U,X\u2113\u22121) \u2a7e\n\u221a\n1 \u2212\u03b52 \u2a7e15/16. Together with the lower bound on \u03c3k,\nthe assumptions for Lemma 3.3 are met. Hence, using our assumptions,\ntan\u03b8(U,X\u2113) \u2a7d(1 \u2212\u03b3k)\u03c3k\u03b5 + 2\u2206\n\u03c3k \u22122\u2206\n\u2a7d\u03b5.\n\u25a0\nOur second claim shows that if the potential is at least \u03b5 at step \u2113\u22121, it will decrease by a factor\n1 \u2212\u03b3k/2.\nClaim 3.6. Let \u2113\u2a7e1 Suppose that tan\u03b8(U,X\u2113\u22121) \u2208[\u03b5,1/2]. Then,\ntan\u03b8(U,X\u2113) \u2a7d(1 \u2212\u03b3k/2)tan\u03b8(U,X\u2113\u22121).\nProof. Using the assumption of the claim we have cos\u03b8(U,X\u2113\u22121) \u2a7e\n1\ntan\u03b8(U,X\u2113\u22121) \u2a7e1/2 > \u2206/\u03c3k. We can\ntherefore apply Lemma 3.3 to conclude\ntan\u03b8(U,X\u2113) \u2a7dtan\u03b8(U,X\u2113\u22121) \u00b7 (1 \u2212\u03b3k)\u03c3k + 2\u2206\n\u03c3k \u22122\u2206\n\u2a7dtan\u03b8(U,X\u2113\u22121) \u00b7 (1 \u2212\u03b3k)(1 + \u03b3k/4)\n1 \u2212\u03b3k/4\n\u2a7dtan\u03b8(U,X\u2113\u22121)(1 \u2212\u03b3k/2)\n\u25a0\nThe two previous claims together imply that\ntan\u03b8(U,XL) \u2a7dmax\nn\ntan\u03b8(U,X0)(1 \u2212\u03b3k/2)L,\u03b5\no\n,\nprovided that tan\u03b8(U,X0) \u2a7d1/2. This is the case since we assumed that sin\u03b8(U,X0) \u2a7d1/4. Note\nthat (1 \u2212\u03b3k/2)L \u2a7dexp(\u2212\u03b3kL/2). It remains to observe that \u2225V \u22a4XL\u2225\u2a7dtan\u03b8(U,XL) and further\ntan\u03b8(U,X0) \u2a7d2\u2225V \u22a4X0\u2225by our assumption on X0.\n\u25a0\n10\nIn our application later on the error terms \u2225G\u2113\u2225decrease as \u2113increases and the algorithm starts\nto converge. We need a convergence bound for this type of shrinking error. The next de\ufb01nition\nexpresses a condition on G\u2113that allows for a useful convergence bound.\nDe\ufb01nition 3.7 (Admissible). Let \u03b3k = 1 \u2212\u03c3k+1/\u03c3k. We say that the pair of matrices (X\u2113\u22121,G\u2113) is\n\u03b5-admissible for NSI if\n\u2225G\u2113\u2225\u2a7d1\n32\u03b3k\u03c3k\u2225V \u22a4X\u2113\u22121\u2225+ \u03b5\n32\u03b3k\u03c3k.\n(7)\nWe say that a family of matrices {(X\u2113\u22121,G\u2113)}L\n\u2113=1 is \u03b5-admissible for NSI if each member of the set is\n\u03b5-admissible. We will use the notation {G\u2113} as a shorthand for {(X\u2113\u22121,G\u2113)}L\n\u2113=1 .\nWe have the following convergence guarantee for admissible noise matrices.\nTheorem 3.8. Let \u03b3k = 1 \u2212\u03c3k+1/\u03c3k. Let \u03b5 \u2a7d1/2. Assume that the family of noise matrices {G\u2113} is\n(\u03b5/2)-admissible for NSI and that \u2225V \u22a4X0\u2225\u2a7d1/4. Then, we have \u2225V \u22a4XL\u2225\u2a7d\u03b5 for any L \u2a7e4\u03b3\u22121\nk log(1/\u03b5).\nProof. We prove by induction that for every t \u2a7e0 after Lt = 4t\u03b3\u22121\nk\nsteps, we have\n\r\r\rV \u22a4XLt\n\r\r\r \u2a7dmax\nn\n2\u2212(t+1),\u03b5\no\n.\nThe base case (t = 0) follows directly from the assumption that \u2225V \u22a4X0\u2225\u2a7d1/4. We turn to the\ninductive step. By induction hypothesis, we have\n\r\r\rV \u22a4XLt\n\r\r\r \u2a7dmax\nn\n2\u2212(t+1),\u03b5\no\n. We apply Lemma 3.4\nwith \u201cX0 = XLt\u201d and error parameter max\nn\n2\u2212t+2,\u03b5\no\nand L = Lt+1 \u2212Lt. The conditions of the lemma\nare satis\ufb01ed as can be easily checked using the assumption that {G\u2113} is \u03b5/2-admissible. Using the\nfact that Lt+1 \u2212Lt = 4/\u03b3k, the conclusion of the lemma gives\n\r\r\rV \u22a4XLt+1\n\r\r\r \u2a7dmax\n(\n\u03b5,2 \u00b7 max\nn\n\u03b5,2\u2212(t+1)o\nexp\n \n\u2212\u03b3k(Lt+1 \u2212Lt)\n2\n!)\n\u2a7dmax\nn\n\u03b5,2\u2212(t+2)o\n.\n\u25a0\n4\nLeast squares update rule\nInput: Target dimension k, observed set of indices \u2126\u2286[n] \u00d7 [n] of an unknown symmetric matrix\nA \u2208Rn\u00d7n with entries P\u2126(A), orthonormal matrix X \u2208Rn\u00d7k.\nAlgorithm LS(P\u2126(A),\u2126,X,L,k) :\nY \u2190argminY\u2208Rn\u00d7k \u2225P\u2126(A \u2212XY \u22a4)\u22252\nF\nOutput: Pair of matrices (X,Y)\nFigure 3: Least squares update\nFigure 4 describes the least squares update step specialized to the case of a symmetric matrix.\nOur goal is to express this update step as an update step of the form Y = AX + G so that we\nmay apply our analysis of noisy subspace iteration. This syntactic transformation is explained in\nSection 4.1 followed by a bound on the norm of the error term G in Section 4.2.\n11\n4.1\nFrom alternating least squares to noisy subspace iteration\nThe optimizer Y satis\ufb01es a set of linear equations that we derive from the gradient of the objective\nfunction.\nLemma 4.1 (Optimality Condition). Let Pi : Rn \u2192Rn be the linear projection onto the coordinates\nin \u2126i = {j : (i,j) \u2208\u2126} scaled by p\u22121 = n2/(E|\u2126|), i.e., Pi = p\u22121 P\nj\u2208\u2126i eje\u22a4\nj . Further, de\ufb01ne the matrix\nBi \u2208Rk\u00d7k as Bi = X\u22a4PiX and assume that Bi is invertible. Then, for every i \u2208[n], the i-th row of Y\nsatis\ufb01es e\u22a4\ni Y = e\u22a4\ni APiXB\u22121\ni\n.\nProof. Call the objective function f (Y) = \u2225P\u2126(A \u2212XY \u22a4)\u22252\nF . We note that for every i \u2208[n],j \u2208[k],\nwe have \u2202f\n\u2202Yij = \u22122P\ns\u2208\u2126i AisXsj + 2Pk\nr=1 Yir\nP\ns\u2208\u2126i XsjXsr . From this we conclude that the optimal Y\nmust satisfy e\u22a4\ni APiX = e\u22a4\ni YX\u22a4PiX = e\u22a4\ni YBi. Hence, e\u22a4\ni Y = e\u22a4\ni APiXB\u22121\ni\n.\n\u25a0\nThe assumption that Bi is invertible is essentially without loss of generality. Indeed, we will\nlater see that Bi is invertible (and in fact close to the identity matrix) with very high probability. We\ncan now express the least squares update as Y = AX + G where we derive some useful expression\nfor G.\nLemma 4.2. Let E = (I \u2212XX\u22a4)U. We have Y = AX + G where G = GM + GN and the matrices GM and\nGN are fo each row i \u2208[n] if Bi is invertible we have the following expressions:\ne\u22a4\ni GM = e\u22a4\ni U\u039bUE\u22a4PiXB\u22121\ni\ne\u22a4\ni GN = e\u22a4\ni (NPiXB\u22121\ni\n\u2212NX).\nProof. By Lemma 4.1, e\u22a4\ni Y = e\u22a4\ni APiXB\u22121\ni\n= e\u22a4\ni Y = e\u22a4\ni MPiXB\u22121\ni\n+ e\u22a4\ni NPiXB\u22121\ni\n. Let Ci = U\u22a4PiX and\nput D = U\u22a4X. On the one hand,\ne\u22a4\ni MPiXB\u22121\ni\n= e\u22a4\ni U\u039bUCiB\u22121\ni\n= e\u22a4\ni (U\u039bUD \u2212U\u039bU(DBi \u2212Ci)B\u22121\ni )\n= e\u22a4\ni MX \u2212e\u22a4\ni U\u039bU(DBi \u2212Ci)B\u22121\ni\nOn the other hand,\nCi = U\u22a4PiX = (XX\u22a4U + E)\u22a4PiX = (U\u22a4X)X\u22a4PiX + E\u22a4(PiX) = DBi + E\u22a4PiX .\nHence, as desired, e\u22a4\ni MPiXB\u22121\ni\n= e\u22a4\ni MX\u2212e\u22a4\ni U\u039bUE\u22a4PiXB\u22121\ni\n. Finally, it follows directly by de\ufb01nition\nthat e\u22a4\ni NPiXB\u22121\ni\n= e\u22a4\ni NX + e\u22a4\ni GN. Putting the previous two equations together, we conclude that\nY = MX + GM + NX + GN = AX + GM + GN .\n\u25a0\n4.2\nDeviation bounds for the least squares update\nIn this section we analyze the norm of the error term G from the previous section. More speci\ufb01cally,\nwe prove a bound on the norm of each row of G. Our bound uses the fact that the matrix E\nappearing in the expression for the error term satis\ufb01es \u2225E\u2225= \u2225V \u22a4X\u2225. This gives us a bound in\nterms of the quantity \u2225V \u22a4X\u2225.\nLemma 4.3. Let \u03b4 \u2208(0,1). Assume that each entry is included in \u2126independently with probability\np \u2273k\u00b5(X)logn\n\u03b42n\n.\n(8)\nThen, for every i \u2208[n], P\nn\r\r\re\u22a4\ni G\n\r\r\r > \u03b4 \u00b7\n\u0010\n\u2225e\u22a4\ni M\u2225\u00b7 \u2225V \u22a4X\u2225+ \u2225e\u22a4\ni N\u2225\n\u0011o\n\u2a7d1\n5 .\n12\nProof. Fix i \u2208[n]. Lemma A.5 shows that with probability 9/10 we have\n\r\r\re\u22a4\ni GM\r\r\r \u2a7d\u03b4\u00b7\u2225e\u22a4\ni M\u2225\u00b7\u2225V \u22a4X\u2225.\nSimilarly, Lemma A.6 shows that with probability 9/10 we have\n\r\r\re\u22a4\ni GN\r\r\r \u2a7d\u03b4 \u00b7 \u2225e\u22a4\ni N\u2225. Both events\noccur with probability 4/5 and in this case we have\n\r\r\re\u22a4\ni G\n\r\r\r \u2a7d\n\r\r\re\u22a4\ni GM\r\r\r +\n\r\r\re\u22a4\ni GN\r\r\r \u2a7d\u03b4 \u00b7\n\u0010\n\u2225e\u22a4\ni M\u2225\u00b7\n\r\r\rV \u22a4X\n\r\r\r + \u2225e\u22a4\ni N\u2225\n\u0011\n.\n\u25a0\n4.3\nMedian least squares update\nGiven the previous error bound we can achieve a strong concentration bound by taking the\ncomponent-wise median of multiple independent samples of the error term.\nLemma 4.4. Let G1,...,Gt be i.i.d. copies of G. Let G = median(G1,...,Gt) be the component-wise\nmedian of G1,...,Gt and assume p satis\ufb01es (8). Then, for every i \u2208[n],\nP\nn\r\r\re\u22a4\ni G\n\r\r\r > \u03b4\n\u0010\n\u2225e\u22a4\ni M\u2225\u00b7\n\r\r\rV \u22a4X\n\r\r\r + \u2225e\u22a4\ni N\u2225\n\u0011o\n\u2a7dexp(\u2212\u2126(t)).\nProof. Fix i \u2208[n] and let g1,...,gt \u2208Rk denote the i-th rows of G1,...,Gt. Let S = {j \u2208[t]: \u2225gj\u2225\u2a7dB}\nwhere B = (\u03b4/4)\n\u0010\n\u2225e\u22a4\ni M\u2225\u00b7 \u2225V \u22a4X\u2225+ \u2225e\u22a4\ni N\u2225\n\u0011\n. Applying Lemma 4.3 with error parameter \u03b4/4 it follows\nthat E|S| \u2a7e4t/5. Moreover, the draws of gj are independent. So we can apply a Cherno\ufb00bound to\nargue that |S| > 2t/3 with probability 1 \u2212exp(\u2212\u2126(t)). Assuming that this event occurs, we claim\nthat g = median(g1,...,gt) satis\ufb01es \u2225g\u2225\u2a7d4B and this claim establishes the lemma.\nTo prove this claim, \ufb01x any coordinate of r \u2208[k]. By the median property |{j : (gj)2\nr \u2a7eg2\nr }| \u2a7et/2.\nSince |S| > 2t/3 this means that at least t/3 vectors with j \u2208S have (gj)2\nr > g2\nr . In particular, the\naverage value of (gj)2\nr over all j \u2208S must be at least tg2\nr /3|S| \u2a7eg2\nr /3. This shows that the average\nof \u2225gj\u22252 over all j \u2208S must be at least \u2225g\u22252/3. On the other hand, we also know that the average\nsquared norm in S is at most B2 by de\ufb01nition of the set S. It follows that \u2225g\u22252 \u2a7d3B2. This implies\nwhat we needed to show.\n\u25a0\nWe can now conclude a strong concentration bound for the median of multiple independent\nsolutions to the least squares minimization step. This way we can obtain the desired error bound\nfor all rows simultaneously. This leads to the following extension of the least squares update rule.\nInput: Target dimension k, observed set of indices \u2126\u2286[n] \u00d7 [n] of an unknown symmetric matrix\nA \u2208Rn\u00d7n with entries P\u2126(A), orthonormal matrix X \u2208Rn\u00d7k.\nAlgorithm MedianLS(P\u2126(A),\u2126,X,L,k) :\n1. (\u21261,...,\u2126t) \u2190Split(\u2126,t) for t = O(logn).\n2. Yi \u2190LS(P\u2126i(A),\u2126i,X,L,k)\nOutput: Pair of matrices (X,median(Y1,...,Yt))\nFigure 4: Median least squares update\nLemma 4.5. Let \u2126be a sample in which each entry is included independently with probability p \u2273\nk\u00b5(X)log2 n\n\u03b42n\n. Let Y \u2190MedianLS(P\u2126(A),\u2126,X,L,k). Then, we have with probability 1 \u22121/n3 that Y =\nAX + G and G satis\ufb01es for every i \u2208[n] the bound\n\r\r\re\u22a4\ni G\n\r\r\r \u2a7d\u03b4\n\r\r\re\u22a4\ni M\n\r\r\r \u00b7 \u2225V \u22a4X\u2225+ \u03b4\n\r\r\re\u22a4\ni N\n\r\r\r .\n13\nProof. By Lemma C.1, the samples \u21261,...,\u2126t are independent and each set \u2126j includes each entry\nwith probability at least p/t. The output satis\ufb01es Y = median(Y1,...,Yt), where each Yj is of the\nform Yj = AX + Gj. It follows that median(Y1,...,Yt) = AX + G where G = median(G1,...,Gt). We\ncan therefore apply Lemma 4.4 to conclude the lemma using the fact that t = O(logn) allows us to\ntake a union bound over all n rows.\n\u25a0\n5\nIncoherence via smooth QR factorization\nAs part of our analysis of alternating minimization we need to show that the intermediate solutions\nX\u2113have small coherence. For this purpose we propose an idea inspired by Smoothed Analysis\nof the QR factorization [SST]. The problem with applying the QR factorization directly to Y\u2113is\nthat Y\u2113might be ill-conditioned. This can lead to a matrix X\u2113(via QR-factorization) that has large\ncoordinates and whose coherence is therefore no longer as small as we desire. A naive bound\non the condition number of Y\u2113would lead to a large loss in sample complexity. What we show\ninstead is that a small Gaussian perturbation to Y\u2113leads to a su\ufb03ciently well-conditioned matrix\neY\u2113= Y\u2113+ H\u2113. Orthonormalizing eY\u2113now leads to a matrix of small coherence. Intuitively, since the\ncomputation of Y\u2113is already noisy the additional noise term has little e\ufb00ect so long as its norm is\nbounded by that of G\u2113. Since we don\u2019t know the norm of G\u2113, we have to search for the right noise\nparameter using a simple binary search. We call the resulting procedure SmoothQR and describe\nin in Figure 5.\nInput: Matrix Y \u2208Rn\u00d7k, parameters \u00b5,\u03b5 > 0.\nAlgorithm SmoothQR(Y,\u03b5,\u00b5) :\n1. X \u2190QR(Y),H \u21900,\u03c3 \u2190\u03b5\u2225Y\u2225/n.\n2. While \u00b5(X) > \u00b5 and \u03c3 \u2a7d\u2225Y\u2225:\n(a) X \u2190GS(Y + H) where H \u223cN(0,\u03c32/n)\n(b) \u03c3 \u21902\u03c3\nOutput: Pair of matrices (X,H)\nFigure 5: Smooth Orthonormalization (SmoothQR)\nTo analyze the algorithm we begin with a lemma that analyzes the smallest singular value\nunder a Gaussian perturbation. What makes the analysis easier is the fact that the matrices we\u2019re\ninterested in are rectangular. The square case was considered in [SST] and requires more involved\narguments.\nLemma 5.1. Let G \u2208Rn\u00d7k be any matrix with \u2225G\u2225\u2a7d1 and let V be a n \u2212k dimensional subspace with\northogonal projection PV . Let H \u223cN(0,\u03c42/n)n\u00d7k be a random Gaussian matrix. Assume k = o(n/ logn).\nThen, with probability 1 \u2212exp(\u2212\u2126(n)), we have \u03c3k (PV (G + H)) \u2a7e\u2126(\u03c4).\nThe proof follows from standard concentration arguments and is contained in Section B. To use\nthis lemma in our context we\u2019ll introduce a variant of \u00b5-coherent that applies to matrices rather\nthan subspaces.\nDe\ufb01nition 5.2 (\u03c1-coherence). Given a matrix G \u2208Rn\u00d7k we let \u03c1(G) def\n=\nn\nk maxi\u2208[n] \u2225e\u22a4\ni G\u22252 .\n14\nThe next lemma is our main technical tool in this section. It shows that adding a Gaussian noise\nterm leads to a bound on the coherence after applying the QR-factorization.\nLemma 5.3. Let k = o(n/ logn) and \u03c4 \u2208(0,1). Let U \u2208Rn\u00d7k be an orthonormal matrix. Let G \u2208Rn\u00d7k be\na matrix such that \u2225G\u2225\u2a7d1. Let H \u223cN(0,\u03c42/n)k\u00d7n be a random Gaussian matrix. Then, with probability\n1 \u2212exp(\u2212\u2126(n)) \u2212n5, there is an orthonormal matrix Q \u2208Rn\u00d72k such that:\n1. R(Q) = R([U | G + H]).\n2. \u00b5(Q) \u2a7dO\n\u0010 1\n\u03c42 \u00b7 (\u03c1(G) + \u00b5(U) + logn)\n\u0011\n.\nProof. First note that R([U | G+H]) = R([U | (I \u2212UU\u22a4)(G+H)]). Let B = (I \u2212UU\u22a4)(G+H). Applying\nthe QR-factorization to [U | B], we can \ufb01nd two orthonormal matrices Q1,Q2 \u2208Rn\u00d7k such that\nhave that [Q1 | Q2] = [U | BR\u22121] where R \u2208Rk\u00d7k. That is Q1 = U since U is already orthonormal.\nMoreover, the columns of B are orthogonal to U and therefore we can apply the QR-factorization to\nU and B independently. We can now apply Lemma 5.1 to the (n \u2212k)-dimensional subspace U\u22a5and\nthe matrix G + H. It follows that with probability 1 \u2212exp(\u2212\u2126(n)), we have \u03c3k(B) \u2a7e\u2126(\u03c4). Assume\nthat this event occurs.\nAlso, observe that \u03c3k(B) = \u03c3k(R). The second condition is now easy to verify\nn\nk\n\r\r\re\u22a4\ni Q\n\r\r\r2 = n\nk\n\r\r\re\u22a4\ni U\n\r\r\r2 + n\nk\n\r\r\re\u22a4\ni BR\u22121\r\r\r2 = \u00b5(U) + n\nk\n\r\r\re\u22a4\ni BR\u22121\r\r\r2\nOn the other hand,\nn\nk\n\r\r\re\u22a4\ni BR\u22121\r\r\r2 \u2a7dn\nk\n\r\r\re\u22a4\ni B\n\r\r\r2 \r\r\rR\u22121\r\r\r2 \u2a7dO\n\u0012 n\nk\u03c42\n\r\r\re\u22a4\ni B\n\r\r\r2\u0013\n,\nwhere we used the fact that\n\r\r\rR\u22121\r\r\r = 1/\u03c3k(R) = O(1/\u03c4). Moreover,\nn\nk\n\r\r\re\u22a4\ni B\n\r\r\r2 \u2a7d2n\nk\n\r\r\re\u22a4\ni (I \u2212UU\u22a4)G\n\r\r\r2 + 2\u03c1((I \u2212UU\u22a4)H) \u2a7d2\u03c1(G) + 2\u03c1(UU\u22a4G) + 2\u03c1((I \u2212UU\u22a4)H).\nFinally, \u03c1(UU\u22a4G) \u2a7d\u00b5(U)\u2225U\u22a4G\u22252 \u2a7d\u00b5(U) and, by Lemma B.1, we have \u03c1((I \u2212UU\u22a4)H) \u2a7dO(logn)\nwith probability 1 \u22121/n5. The lemma follows with a union bound over the failure probabilities.\n\u25a0\nThe next lemma states that when SmoothQR is invoked on an input of the form AX + G with\nsuitable parameters, the algorithm outputs a matrix of the form X\u2032 = QR(AX + G + H) whose\ncoherence is bounded in terms of \u00b5(U) and \u03c1(G) and moreover H satis\ufb01es a bound on its norm.\nThe lemma also permits to trade-o\ufb00the amount of additional noise introduced with the resulting\ncoherence parameter.\nLemma 5.4. Let \u03c4 > 0 and assume k = o(n/ logn) and \u00b5(U)k \u2a7dn. There is an absolute constant C5.4 > 0\nsuch that the following claim holds. Let G \u2208Rn\u00d7k. Let X \u2208Rn\u00d7k be an orthonormal matrix such that\n\u03bd \u2a7emax{\u2225G\u2225,\u2225NX\u2225}. Assume that\n\u00b5 \u2a7eC5.4\n\u03c42\n\u0012\n\u00b5(U) + \u03c1(G) + \u03c1(NX)\n\u03bd2\n+ logn\n\u0013\n.\nThen, for every \u03b5 \u2a7d\u03c4\u03bd satisfying log(n/\u03b5) \u2a7dn and every \u00b5 \u2a7dn, we have with probability 1 \u2212O(n\u22124),\nthe algorithm SmoothQR(AX + G,\u03b5,\u00b5) terminates in O(log(n/\u03b5)) steps and outputs (X\u2032,H) such that\n\u00b5(X\u2032) \u2a7d\u00b5 and where H satis\ufb01es \u2225H\u2225\u2a7d\u03c4\u03bd.\n15\nProof. Suppose that SmoothQR terminates in an iteration where \u03c32 \u2a7d\u03c42\u03bd2/4. We claim that in\nthis case with probability 1 \u2212exp(\u2212\u2126(n)) we must have that \u2225H\u2225\u2a7d\u03c4\u03bd. Indeed, assuming the\nalgorithm terminates when \u03c32 \u2a7dc\u03c42\u03bd2/k, the algorithm took at most t = O(log(n/\u03b5)) \u2a7dO(n) steps.\nLet H1,...,Ht denote the random Gaussian matrices generated in each step. We claim that each of\nthem satis\ufb01es \u2225Ht\u2225\u2a7d\u03c4\u03bd. Note that for all t we have E\u2225Ht\u22252 \u2a7d\u03c42\u03bd2/4. The claim therefore follows\ndirectly from tail bounds for the Frobenius norm of Gaussian random matrices and holds with\nprobability 1 \u2212exp(\u2212\u2126(n)). The next claim now \ufb01nishes the proof.\nClaim 5.5. With probability 1 \u2212O(1/n4), the algorithm terminates in an iteration where \u03c32 \u2a7d\u03c42\u03bd2/4.\nTo prove the claim, consider the \ufb01rst iteration in which \u03c32 \u2a7e\u03c42\u03bd2/8. Let us de\ufb01ne G\u2032 =\n(NX + G)/2\u03bd. We can now apply Lemma 5.3 to the matrix G\u2032 which satis\ufb01es the assumption of the\nlemma that \u2225G\u2032\u2225\u2a7d1. The lemma then entails that with the stated probability bound there is an\northonormal n \u00d7 2k matrix Q such that\nR(Q) = R([U | G\u2032 + H]) = R([U | G + NX + H]),\nand moreover \u00b5(Q) \u2a7dO\n\u0010 1\n\u03c42 \u00b7 (\u03c1(G) + \u00b5(U) + logn)\n\u0011\n. On the one hand,\nR(X\u2032) = R(AX + G + H) = R(MX + NX + G + H) \u2286R([U | NX + G + H]) = R(V ).\nThe inclusion follows from the fact that U is an orthonormal basis for the range of MX = U\u03a3UU\u22a4X.\nOn the other hand, \u03c1(G\u2032) = O(\u03c1(G/\u03bd) + \u03c1(NX/\u03bd\u2032)) . Hence, by Lemma B.2 and the fact that dim(Q) \u2a7d\n2dim(X\u2032), we have \u00b5(X\u2032) \u2a7d2\u00b5(Q) \u2a7d\u00b5. This shows that the termination criterion of the algorithm is\nsatis\ufb01ed provided we pick C5.4 large enough.\n\u25a0\n6\nConvergence bounds for alternating minimization\nThe total sample complexity we achieve is the sum of two terms. The \ufb01rst one is used by the\ninitialization step that we discuss in Section 7. The second term speci\ufb01es the sample requirements\nfor iterating the least squares algorithm. It therefore makes sense to de\ufb01ne the following two\nquantities:\npinit = k2\u00b5\u2217\u2225A\u22252\nF logn\n\u03b32\nk \u03c32\nk n\nand\npLS = k\u00b5\u2217(\u2225M\u22252\nF + \u2225N\u22252\nF/\u03b52)log(n/\u03b5)log2 n\n\u03b35\nk \u03c32\nk n\nWhile the \ufb01rst term has a quadratic dependence on k it does not depend on \u03b5 at all and it has single\nlogarithmic factor. The second term features a linear dependence on k. Our main theorem shows\nthat if the sampling probability is larger than the sum of these two terms, the algorithm converges\nrapidly to the true unknown matrix.\nTheorem 6.1 (Main). Let k,\u03b5 > 0. Let A = M + N be a symmetric n \u00d7 n matrix where M is a matrix of\nrank k with the spectral decomposition M = U\u039bUU\u22a4and N = (I \u2212UU\u22a4)A = V \u039bV V \u22a4satis\ufb01es (2). Let\n\u03b3k = 1 \u2212\u03c3k+1/\u03c3k where \u03c3k is the smallest singular value of M and \u03c3k+1 is the largest singular value of N.\nThen, there are parameters \u00b5 = \u0398(\u03b3\u22122\nk k(\u00b5\u2217+ logn)) and L = \u0398(\u03b3\u22121\nk log(n/\u03b5)) such that the out-\nput (X,Y) of SAltLS(P\u2126(A),\u2126,k,L,\u03b5,\u00b5) satis\ufb01es \u2225(I \u2212UU\u22a4)XL\u2225\u2a7d\u03b5 with probability 9/10.\nBefore we prove the theorem in Section 6.1, we will state an immediate corollary that gives\nbounds on the reconstruction error in the Frobenius norm.\n16\nCorollary 6.2 (Reconstruction error). Under the assumptions of Theorem 6.1, we have that the out-\nput (X,Y) of SAltLS satis\ufb01es \u2225M \u2212XY \u22a4\u2225F \u2a7d\u03b5\u2225A\u2225F with probability 9/10.\nProof. Let (X,Y) be the matrices given by our algorithm when invoked with error parameter \u03b5/2.\nBy Theorem 6.1 we have \u2225UU\u22a4\u2212XX\u22a4\u2225= \u2225(I \u2212UU\u22a4)X\u2225\u2a7d\u03b5\n2 . Using the proof of Theorem 6.1 we\nalso know that Y = AX + G where G is (\u03b5/4)-admissible so that \u2225G\u2225F \u2a7d\u03b5\u03c3k/2. Consequently,\n\r\r\rM \u2212XY \u22a4\r\r\rF =\n\r\r\rM \u2212XX\u22a4A + XG\n\r\r\rF \u2a7d\n\r\r\rUU\u22a4A \u2212XX\u22a4A\n\r\r\rF + \u2225XG\u2225F\n\u2a7d\n\r\r\rUU\u22a4\u2212XX\u22a4\r\r\r\u2225A\u2225F + \u2225G\u2225F\n\u2a7d(\u03b5/2)\u2225A\u2225F + (\u03b5/2)\u03c3k \u2a7d\u03b5\u2225A\u2225F .\nIn the second inequality we used that for all matrices P,Q we have \u2225PQ\u2225F \u2a7d\u2225P\u2225\u00b7 \u2225Q\u2225F.\n\u25a0\n6.1\nProof of Theorem 6.1\nProof. We \ufb01rst apply Theorem 7.1 (shown below) to conclude that with probability 19/20, the\ninitial matrix X0 satis\ufb01es \u2225V \u22a4X0\u2225\u2a7d1/4 and \u00b5(X0) \u2a7d32\u00b5(U)logn. Assume that this event occurs.\nOur goal is now to apply Theorem 3.8. Consider the sequence of matrices\nn\n(X\u2113\u22121, e\nG\u2113)\noL\n\u2113=1 obtained\nby the execution of SAltLS starting from X0 and letting e\nG\u2113= G\u2113+ H\u2113where G\u2113is the error term\ncorresponding to the \u2113-step of MedianLS, and H\u2113is the error term introduced by the application\nof SmoothQR at step \u2113. To apply Theorem 3.8, we need to show that this sequence of matrices is\n(\u03b5/2)-admissible for NSI with probability 19/20. The theorem then directly gives that \u2225V \u22a4XL\u2225\u2a7d\u03b5\nand this would conclude our proof by summing up the error probabilities.\nLet\n\u03c4 = \u03b3k\n128\nand\nb\u00b5 = C5.4\n\u03c42 (20\u00b5\u2217+ logn) .\nLet \u00b5 be any number satisfying \u00b5 \u2a7eb\u00b5. Since b\u00b5 = \u0398(\u03b3\u22122\nk k(\u00b5\u2217+logn)), this satis\ufb01es the requirement in\nthe theorem. We prove that with probability 9/20, the following three claims hold:\n1. {(X\u2113\u22121,G\u2113)}L\n\u2113=1 is (\u03b5/4)-admissible,\n2. {(X\u2113\u22121,H\u2113)}L\n\u2113=1 is (\u03b5/4)-admissible,\n3. for all \u2113\u2208{0,...,L \u22121}, we have \u00b5(X\u2113) \u2a7d\u00b5.\nThis implies the claim that we want using a triangle inequality since e\nG\u2113= G\u2113+ H\u2113.\nThe proof of these three claims is by mutual induction. For \u2113= 0, we only need to check the\nthird claim which follows form the fact that X0 satis\ufb01es the coherence bound. Now assume that all\nthree claims hold at step \u2113\u22121, we will argue that the with probability 1 \u2212n/100, all three claims\nhold at step \u2113. Since L \u2a7dn, this is su\ufb03cient.\nThe \ufb01rst claim follows from Lemma 4.4 using the induction hypothesis that \u00b5(X\u2113\u22121) \u2a7db\u00b5.\nSpeci\ufb01cally, we apply the lemma with \u03b4 = cmin{\u03b3k\u03c3k/\u2225M\u2225F,\u03b5\u03b3k\u03c3k/\u2225N\u2225F} for su\ufb03ciently small\nconstant c > 0. The lemma requires the lower bound p \u2273k\u00b5\u2217log2 n\n\u03b42n\n. We can easily verify that the right\nhand side is a factor L = \u0398(\u03b3\u22121\nk log(n/\u03b5)) smaller than what is provided by the assumption of the\ntheorem. This is because new samples are used in each of the L steps so that we need to divide the\ngiven bound by L. Lemma 4.4 now gives with probability 1 \u22121/n3 the upper bound\n\u2225G\u2113\u2225F \u2a7d1\n4\n\u0012 1\n32\u03b3k\u03c3k\n\r\r\rV \u22a4X\u2113\u22121\n\r\r\r + \u03b5\n32\u03b3k\u03c3k\n\u0013\n.\n17\nIn particular, this satis\ufb01es the de\ufb01nition of \u03b5/4-admissibility. We proceed assuming that this event\noccurs as the error probability is small enough to ignore.\nThe remaining two claims follow from Lemma 5.4. We will apply the lemma to AX\u2113+ G\u2113with\n\u03bd = \u03c3k(\u2225V \u22a4X\u2113\u22121\u2225+ \u03b5) and \u03c4 as above. Note that\n\u2225NX\u2113\u22121\u2225\u2a7d\u03c3k\n\r\r\rV \u22a4X\u2113\u22121\n\r\r\r .\nHence we have \u03bd \u2a7emax{\u2225G\u2113\u2225,\u2225NX\u2113\u22121\u2225} as required by the lemma. The lemma also requires a lower\nbound \u00b5. To satisfy the lower bound we invoke Lemma B.3 showing that with probability 1 \u22121/n2,\nwe have\n1\n\u03bd2 (\u03c1(G) + \u03c1(NX)) \u2a7d10\u00b5\u2217.\nWe remark that this is the lemma that uses the assumption on N provided by (2). Again we assume\nthis event occurs. In this case we have\n\u00b5 \u2a7eb\u00b5 = C5.4\n\u03c42 (20\u00b5\u2217+ logn)\nand so we see that \u00b5 satis\ufb01es the requirement of Lemma 5.4. It follows that SmoothQR produces\nwith probability 1 \u22121/n4 a matrix H\u2113such that\n\u2225H\u2113\u2225\u2a7d\u03c4\u03bd \u2a7d\u03b3k\u03bd\n128 \u2a7d1\n4\n\u0012 1\n32\u03b3k\u03c3k\n\r\r\rV \u22a4X\u2113\u22121\n\r\r\rF + \u03b5\n32\u03b3k\u03c3k\n\u0013\n.\nIn particular, H\u2113satis\ufb01es the requirement of (\u03b5/4)-admissibility. Moreover, the lemma gives that\n\u00b5(X\u2113) \u2a7d\u00b5. This shows that also the second and third claim of our inductive claim continue to hold.\nAll error probabilities we incurred were o(1/n) and we can sum up the error probabilities over all\nL \u2a7dn steps to concludes the proof of the theorem.\n\u25a0\n7\nFinding a good starting point\nFigure 6 describes an algorithm that computes the top k singular vectors of P\u2126(A) and truncates\nthem in order to ensure incoherence. The algorithm serves as a fast initialization procedure for\nour main algorithm. This general approach is relatively standard in the literature. However, our\ntruncation argument di\ufb00ers from previous approaches. Speci\ufb01cally, we use a random orthonormal\ntransformation to spread out the entries of the singular vectors before truncation. This leads to a\ntighter bound on the coherence.\nTheorem 7.1 (Initialization). Let A = M +N be a symmetric n\u00d7n matrix where M is a matrix of rank k\nwith the spectral decomposition M = U\u039bUU\u22a4and N = (I \u2212UU\u22a4)A satis\ufb01es (2). Assume that each entry\nis included in \u2126independently probability\np \u2a7eCk(k\u00b5(U) + \u00b5N)(\u2225A\u2225F/\u03b3k\u03c3k)2 logn\nn\n(9)\nfor a su\ufb03ciently large constant C > 0. Then, the algorithm Initialize returns an orthonormal matrix\nX \u2208Rn\u00d7k such that with probability 9/10, \u2225V \u22a4X\u2225F \u2a7d1/4 and \u00b5(X) \u2a7d32\u00b5(U)logn.\nProof. The proof follows directly from Lemma 7.3 and Lemma 7.4 below.\n\u25a0\n18\nInput: Target dimension k, observed set of indices \u2126\u2286[n] \u00d7 [n] of an unknown symmetric matrix\nA \u2208Rn\u00d7n with entries P\u2126(A), coherence parameter \u00b5 \u2208R.\nAlgorithm Initialize(P\u2126(A),\u2126,k,\u00b5) :\n1. Compute the \ufb01rst k singular vectors W \u2208Rn\u00d7k of P\u2126(A).\n2. f\nW \u2190WO where O \u2208Rk\u00d7k is a random orthonormal matrix.\n3. T \u2190T\u00b5\u2032(f\nW) with \u00b5\u2032 =\np\n8\u00b5log(n)/n where Tc replaces each entry of its input with the nearest\nnumber in the interval [\u2212c,c].\n4. X \u2190QR(T )\nOutput: Orthonormal matrix X \u2208Rn\u00d7k.\nFigure 6: Initialization Procedure (Initialize)\nRemark 7.2. To implement Initialize it is su\ufb03cient to compute an approximate singular value decom-\nposition of P\u2126(A). From our analysis it is easy to see that it is su\ufb03cient to compute the k-th singular\nvalue to accuracy, say, \u03b3k\u03c3k/100k. This can be done e\ufb03ciently using, for example, the Power Method\n(Subspace Iteration) with O(k\u03b3\u22121)logn) iterations. See [Hig, Ste] for details on the Power Method. In\nparticular, the running time of this step is dominated by the running time of LS.\nLemma 7.3. Assume that \u2126satis\ufb01es Equation 9. Then, P\nn\n\u2225V \u22a4W\u22252 \u2a7d1/16\n\u221a\nk\no\n\u2a7e1 \u22121/n2.\nProof. By our assumption on A, we have maxi\u2208[n]\n\r\r\re\u22a4\ni N\n\r\r\r2 \u2a7d(\u00b5N/n)\u2225A\u22252\nF and maxi,j\u2208[n] |Nij| \u2a7d(\u00b5N/n)\u2225A\u2225F.\nMoreover, maxi\n\r\r\re\u22a4\ni M\n\r\r\r2 \u2a7d(\u00b5(U)k/n)\u2225M\u22252\nF and maxi,j |Mij| \u2a7d(\u00b5(U)k/n)\u2225M\u2225F. This shows that\nmax\ni\n\r\r\re\u22a4\ni A\n\r\r\r2 \u2a7d\u00b5(U)k + \u00b5N\nn\n\u2225A\u22252\nF\nand\nmax\ni,j |Aij| \u2a7d\u00b5(U)k + \u00b5N\nn\n\u2225A\u2225F .\nPlugging these upper bounds into Lemma A.3 together with our sample bound in Equation 9, we\nget that\nP\n(\n\u2225A \u2212P\u2126(A)\u2225> \u03b3k\u03c3k\n32\n\u221a\nk\n)\n\u2a7d1/n2 .\nPut \u03b5 = \u03b3k\u03c3k/32\n\u221a\nk. Now assume that \u2225A \u2212P\u2126(A)\u2225\u2a7d\u03b5 and let W be the top k singular vectors of\nP\u2126(A). On the one hand, \u03c3k(P\u2126(A)) \u2a7e\u03c3k(A) \u2212\u03b5 \u2a7e\u03c3k \u2212\u03b3k\u03c3k/2. One the other hand, by de\ufb01nition,\n\u03c3k+1(A) = \u03c3k \u2212\u03b3k\u03c3k . Hence, by the Davis-Kahan sin\u03b8-theorem [DK, Ste] we have that\n\u2225V \u22a4W\u2225= sin\u03b8k(U,W) \u2a7d\n\u03b5\n\u03c3k(P\u2126(A)) \u2212\u03c3k+1(A) \u2a7d\n2\u03b5\n\u03b3k\u03c3k\n=\n1\n16\n\u221a\nk\n.\n\u25a0\nLemma 7.4. Assume that \u2225V \u22a4W\u22252 \u2a7d1/16\n\u221a\nk. Then, with probability 99/100 we have \u2225V \u22a4X\u2225F \u2a7d1/4\nand \u00b5(X) \u2a7d32\u00b5(U)logn.\nProof. By our assumption on W, there exists an orthonormal transformation Q \u2208Rk\u00d7k such that\n\u2225UQ \u2212W\u2225F \u2a7d1/16. Moreover, \u00b5(UQ) = \u00b5(U) \u2a7d\u00b5. In other words, W is close in Frobenius norm\nto an orthonormal basis of small coherence. A priori it could be that some entries of UQ are as\nlarge as\np\n\u00b5k/n. However, after rotating UQ be a random rotation, all entries will be as small as\np\n\u00b5log(n)/n. This is formalized in the next claim.\n19\nClaim 7.5. Let Y \u2208Rn\u00d7k be any orthonormal basis with \u00b5(Y) \u2a7d\u00b5. Then, for a random orthonormal\nmatrix O \u2208Rk\u00d7k, we have P\nn\nmaxij |(YO)ij| >\np\n8\u00b5log(n)/n\no\n\u2a7d1\nn2 .\nProof. Consider a single entry Z = (YO)ij. Observe that Z is distributed like a coordinate of a\nrandom vector in Rk of norm at most\np\n\u00b5k/n. By measure concentration, we have\nP\nn\n|Z| > \u03b5\np\n\u00b5k/n\no\n\u2a7d4exp(\u2212\u03b52k/2).\nThis follows from Levy\u2019s Lemma (see [Mat]) using the fact that projection onto a single coordinate\nin Rk is a Lipschitz function on the (k \u22121)-dimensional sphere. The median of this function is 0\ndue to spherical symmetry. Hence, the above bound follows. Putting \u03b5 =\np\n8log(n)/k, we have that\nP\n\u001a\n|Z| >\nq\n8\u00b5log(n)/n\n\u001b\n\u2a7d4exp(3log(n)) = 4n\u22124 .\nTaking a union bound over all kn \u2a7dn2/4 entries, we have that with probability 1 \u22121/n2,\nmax\ni,j |(YO)ij| \u2a7d\nq\n8\u00b5log(n)/n.\n\u25a0\nApplying the previous claim to UQ, we have that with probability 1\u22121/n2, for all i,j, (UQO)ij \u2a7d\n\u00b5\u2032. Furthermore, because a rotation does not increase Frobenius norm, we also have \u2225UQO \u2212WO\u2225F \u2a7d\n1/16. Truncating the entries of WO to \u00b5\u2032 can therefore only decrease the distance in Frobenius\nnorm to UQO. Hence, \u2225UQO \u2212T \u2225F \u2a7d1/16. Also, since truncation is a projection onto the set\n{B: |Bij| \u2a7d\u00b5\u2032} with respect to Frobenius norm, we have\n\u2225WO \u2212T \u2225F \u2a7d\u2225UQO \u2212T \u2225F \u2a7d1\n16 .\nWe can write X = TR\u22121 where R is an invertible linear transformation with the same singular values\nas T and thus satis\ufb01es\n\u2225R\u22121\u2225=\n1\n\u03c3k(T ) \u2a7d\n1\n\u03c3k(WO) \u2212\u03c31(WO \u2212T ) \u2a7d\n1\n1 \u22121/16 \u2a7d2.\nTherefore,\n\r\r\re\u22a4\ni X\n\r\r\r =\n\r\r\re\u22a4\ni TR\u22121\r\r\r \u2a7d\n\r\r\re\u22a4\ni T\n\r\r\r\n\r\r\rR\u22121\r\r\r \u2a7d2\n\r\r\re\u22a4\ni T\n\r\r\r \u2a7d2\nq\n8k\u00b5(U)log(n)/n.\nHence,\n\u00b5(X) \u2a7dn\nk \u00b7 32k\u00b5(U)log(n)\nn\n\u2a7d32\u00b5(U)log(n).\nFinally,\n\r\r\rV \u22a4X\n\r\r\rF =\n\r\r\rV \u22a4TR\u22121\r\r\rF \u2a7d\n\r\r\rV \u22a4T\n\r\r\rF\n\r\r\rR\u22121\r\r\r \u2a7d2\n\r\r\rV \u22a4T\n\r\r\rF\n\u2a7d2\n\r\r\rV \u22a4WO\n\r\r\rF + 2\u2225WO \u2212T \u2225F \u2a7d2\n\r\r\rV \u22a4W\n\r\r\rF + 1\n8 \u2a7d1\n4 .\n\u25a0\nAcknowledgments\nThanks to David Gleich, Prateek Jain, Jonathan Kelner, Raghu Meka, Ankur Moitra, Nikhil Sri-\nvastava, and Mary Wootters for many helpful discussions. We thank the Simons Institute for\nTheoretical Computer Science at Berkeley, where some of this research was done.\n20\nReferences\n[AKKS]\nHaim Avron, Satyen Kale, Shiva Prasad Kasiviswanathan, and Vikas Sindhwani. E\ufb03-\ncient and practical stochastic subgradient descent for nuclear norm regularization. In\nProc. 29th ICML. ACM, 2012.\n[BK]\nRobert M. Bell and Yehuda Koren. Scalable collaborative \ufb01ltering with jointly derived\nneighborhood interpolation weights. In ICDM, pages 43\u201352. IEEE Computer Society,\n2007.\n[CR]\nEmmanuel J. Cand\u00e8s and Benjamin Recht. Exact matrix completion via convex optimiza-\ntion. Foundations of Computional Mathematics, 9:717\u2013772, December 2009.\n[CT]\nEmmanuel J. Cand\u00e8s and Terence Tao. The power of convex relaxation: near-optimal\nmatrix completion. IEEE Transactions on Information Theory, 56(5):2053\u20132080, 2010.\n[DK]\nChandler Davis and W. M. Kahan. The rotation of eigenvectors by a perturbation. iii.\nSIAM J. Numer. Anal., 7:1\u201346, 1970.\n[DMNS]\nCynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith. Calibrating noise to\nsensitivity in private data analysis. In Proc. 3rd TCC, pages 265\u2013284. Springer, 2006.\n[GAGG]\nSuriya Gunasekar, Ayan Acharya, Neeraj Gaur, and Joydeep Ghosh.\nNoisy matrix\ncompletion using alternating minimization. In Proc. ECML PKDD, pages 194\u2013209.\nSpringer, 2013.\n[Har]\nMoritz Hardt. Robust subspace iteration and privacy-preserving spectral analysis. arXiv,\n1311:2495, 2013.\n[HH]\nJustin P. Haldar and Diego Hernando. Rank-constrained solutions to linear matrix\nequations using powerfactorization. IEEE Signal Process. Lett., 16(7):584\u2013587, 2009.\n[Hig]\nNicholas J. Higham. Accuracy and Stability of Numerical Algorithms. Society for Industrial\nand Applied Mathematics, 2002.\n[HK]\nElad Hazan and Satyen Kale. Projection-free online learning. In ICML. ACM, 2012.\n[HMRW] Moritz Hardt, Raghu Meka, Prasad Raghavendra, and Benjamin Weitz. Computational\nlimits for matrix completion. CoRR, abs/1402.2331, 2014.\n[HO]\nCho-Jui Hsieh and Peder A. Olsen. Nuclear norm minimization via active subspace\nselection. In Proc. 31st ICML. ACM, 2014.\n[HR1]\nMoritz Hardt and Aaron Roth. Beating randomized response on incoherent matrices. In\nProc. 44th Symposium on Theory of Computing (STOC), pages 1255\u20131268. ACM, 2012.\n[HR2]\nMoritz Hardt and Aaron Roth. Beyond worst-case analysis in private singular vector\ncomputation. In Proc. 45th Symposium on Theory of Computing (STOC). ACM, 2013.\n[JNS]\nPrateek Jain, Praneeth Netrapalli, and Sujay Sanghavi. Low-rank matrix completion\nusing alternating minimization. In Proc. 45th Symposium on Theory of Computing (STOC),\npages 665\u2013674. ACM, 2013.\n21\n[JS]\nMartin Jaggi and Marek Sulovsk\u00fd. A simple algorithm for nuclear norm regularized\nproblems. In Proc. 27th ICML, pages 471\u2013478. ACM, 2010.\n[JY]\nShuiwang Ji and Jieping Ye. An accelerated gradient method for trace norm minimization.\nIn Proc. 26th ICML, page 58. ACM, 2009.\n[KBV]\nYehuda Koren, Robert M. Bell, and Chris Volinsky. Matrix factorization techniques for\nrecommender systems. IEEE Computer, 42(8):30\u201337, 2009.\n[Kes]\nRaghunandan H. Keshavan. E\ufb03cient algorithms for collaborative \ufb01ltering. PhD thesis,\nStanford University, 2012.\n[KMO1]\nRaghunandan H. Keshavan, Andrea Montanari, and Sewoong Oh. Matrix completion\nfrom a few entries. IEEE Transactions on Information Theory, 56(6):2980\u20132998, 2010.\n[KMO2]\nRaghunandan H. Keshavan, Andrea Montanari, and Sewoong Oh. Matrix completion\nfrom noisy entries. Journal of Machine Learning Research, 11:2057\u20132078, 2010.\n[Mat]\nJiri Matousek. Lectures on Discrete Geometry. Springer-Verlag New York, Inc., 2002.\n[MHT]\nRahul Mazumder, Trevor Hastie, and Robert Tibshirani. Spectral regularization algo-\nrithms for learning large incomplete matrices. Journal of Machine Learning Research,\n11:2287\u20132322, 2010.\n[Rec]\nBenjamin Recht. A simpler approach to matrix completion. Journal of Machine Learning\nResearch, 12:3413\u20133430, 2011.\n[RR]\nBenjamin Recht and Christopher R\u00e9. Parallel stochastic gradient algorithms for large-\nscale matrix completion. Math. Program. Comput., 5(2):201\u2013226, 2013.\n[SST]\nArvind Sankar, Daniel A. Spielman, and Shang-Hua Teng. Smoothed analysis of the\ncondition numbers and growth factors of matrices. SIAM J. Matrix Analysis Applications,\n28(2):446\u2013476, 2006.\n[Ste]\nG.W. Stewart. Matrix Algorithms. Volume II: Eigensystems. Society for Industrial and\nApplied Mathematics, 2001.\n[Tro]\nJoel A. Tropp. User-friendly tail bounds for sums of random matrices. Foundations of\nComputational Mathematics, 12(4):389\u2013434, 2012.\n[ZK]\nPheizhen Zhu and Andrew V. Knyazev. Angles between subspaces and their tangents.\nArxiv preprint arXiv:1209.0523, 2012.\nA\nLarge deviation bounds\nWe need some matrix concentration inequalities. Turn to [Tro] for background.\nTheorem A.1 (Matrix Bernstein). Consider a \ufb01nite sequence {Zk} of independent random matrices with\ndimensions d1 \u00d7 d2. Assume that each random matrix satis\ufb01es EZk = 0 and \u2225Zk\u2225\u2a7dR almost surely.\nDe\ufb01ne \u03c32 def\n= max\nn\r\r\rP\nk EZkZ\u22a4\nk\n\r\r\r,\n\r\r\rP\nk EZ\u22a4\nk Zk\n\r\r\r\no\n. Then, for all t \u2a7e0,\nP\nn\r\r\rP\nk Zk\n\r\r\r \u2a7et\no\n\u2a7d(d1 + d2) \u00b7 exp\n \n\u2212t2/2\n\u03c32 + Rt/3\n!\n.\n22\nTheorem A.2 (Matrix Cherno\ufb00). Consider a \ufb01nite sequence {Xk} of independent self-adjoint matrices of\ndimension d. Assume that each random matrix satis\ufb01es Xk \u2ab00 and \u03bbmax(Xk) \u2a7dR almost surely. De\ufb01ne\n\u00b5min\ndef\n= \u03bbmin (P\nk EXk) . Then,\nP\n\u001a\n\u03bbmin\n\u0012X\nk Xk\n\u0013\n\u2a7d(1 \u2212\u03b4)\u00b5min\n\u001b\n\u2a7dd \u00b7 exp\n \u2212\u03b42\u00b5min\n2R\n!\nA.1\nError bounds for initialization\nLemma A.3. Suppose that A \u2208Rm\u00d7n and let \u2126\u2282[m] \u00d7 [n] be a random subset where each entry is\nincluded independently with probability p. Then\nP{\u2225P\u2126(A) \u2212A\u2225> u} \u2a7dnexp\n \n\u2212u2/2\n\u03c32 + u\n3(1/p \u22121)maxij |Aij|\n!\n.\nwhere \u03c32 = (1/p \u22121)max\n\u001a\nmaxi\n\r\r\re\u22a4\ni A\n\r\r\r2 ,maxj\n\r\r\rAej\n\r\r\r2\u001b\n.\nProof. Let \u03beij be independent Bernoulli-p random variables, which are 1 if (i,j) \u2208\u2126and 0 otherwise.\nConsider the sum of independent random matrices P\u2126(A) \u2212A = P\ni,j\n\u0012\u03beij\np \u22121\n\u0013\nAijeie\u22a4\nj . Applying\nTheorem A.1, we conclude that\nP{\u2225P\u2126(A) \u2212A\u2225> u} \u2a7dnexp\n \n\u2212u2/2\n\u03c32 + Ru/3\n!\n.\nHere we use that\n\r\r\r\r\rEP\ni,j\n\u0012\u03beij\np \u22121\n\u00132\nA2\nijeie\u22a4\nj ejei\u22a4\n\r\r\r\r\r =\n\u0010 1\np \u22121\n\u0011\nmaxi\n\r\r\re\u22a4\ni A\n\r\r\r2\nand similarly\n\r\r\r\r\r\rE\nX\ni,j\n \u03beij\np \u22121\n!2\nA2\nijeje\u22a4\ni eie\u22a4\nj\n\r\r\r\r\r\r =\n\u00121\np \u22121\n\u0013\nmax\ni\n\r\r\rAej\n\r\r\r2 .\nFurther,\n\r\r\r\r\r\n\u0012\u03beij\np \u22121\n\u0013\nAijeie\u22a4\nj\n\r\r\r\r\r \u2a7dR =\n\u0010 1\np \u22121\n\u0011\nmaxij |Aij|. This concludes the proof.\n\u25a0\nA.2\nError bounds for least squares\nLemma A.4. Let 0 < \u03b4 < 1 and let i \u2208[n]. Assume that p \u2273k\u00b5(X)logn\n\u03b42n\n. Then, P\nn\r\r\rB\u22121\ni\n\r\r\r \u2a7e\n1\n1\u2212\u03b4\no\n\u2a7d1\nn5 .\nProof. Let B = Bi and p = p\u2113. Clearly,\n\r\r\rB\u22121\r\r\r = 1/\u03bbmin(B). We will use Theorem A.2 to lower bound\nthe smallest eigenvalue of B by 1 \u2212\u03b4. Denoting the rows of X by x1,...,xn \u2208Rk we have B =\nPn\ni=1\n1\npZixix\u22a4\ni , where {Zi} are independent Bernoulli(p) random variables. Moreover, EB = X\u22a4X =\nIdk\u00d7k . Therefore, in the notation of Theorem A.2, this is \u00b5min(B) = 1. Moreover, using our lower\nbound on p,\n\r\r\r\r 1\npZixix\u22a4\ni\n\r\r\r\r \u2a7d1\np\u2225xi\u22252 \u2a7d\u00b5(X\u2113\u22121)k\npn\n\u2a7d\n\u03b42\n20logn . Hence, by Theorem A.2, P{\u03bbmin(B) \u2a7d1 \u2212\u03b4} \u2a7d\nk exp(\u221210logn) . The claim follows.\n\u25a0\nLemma A.5. Let 0 < \u03b4 < 1 Assume that p \u2273k\u00b5(X)logn\n\u03b42n\n. Then, for every i \u2208[n], we have\nP\nn\r\r\re\u22a4\ni U\u039bUE\u22a4PiX\n\r\r\r \u2a7e\u03b4\u2225e\u22a4\ni M\u2225\u00b7\n\r\r\rV \u22a4X\n\r\r\r\no\n\u2a7d1\n10 .\n23\nProof. Let \u2126\u2208Rk\u00d7k be an orthonormal transformation such that all columns of X\u2032 = X\u2126have\n\u2113\u221e-norm at most\np\n8\u00b5(X)log(n)/n. Such a transformation exists as shown in Claim 7.5. Then,\nE\n\r\r\re\u22a4\ni U\u039bUE\u22a4PiX\n\r\r\r2 = E\n\r\r\re\u22a4\ni U\u039bUE\u22a4PiX\u2032\r\r\r2 =\nk\nX\nr=1\nE(e\u22a4\ni U\u039bUE\u22a4Pix\u2032\nr)2\nwhere x\u2032\nr is the r-th column of X\u2032. Let w\u22a4= e\u22a4\ni U\u039bUE\u22a4. We have\n\u2225w\u22252 \u2a7d\u2225e\u22a4\ni U\u039bU\u2225\u00b7 \u2225E\u2225= \u2225e\u22a4\ni M\u2225\u00b7 \u2225V \u22a4X\u2225.\nFinally,\nE(w\u22a4Pix\u2032\nr)2 =\nn\nX\nj=1\nE(wj(Pi)jj(x\u2032\nr)j)2 \u2a7d1\np \u00b7 \u2225w\u22252\n2 \u00b7 \u2225x\u2032\nr\u22252\n\u221e\u2a7d8\u00b5(X)logn\npn\n.\nHence,\nE\n\r\r\re\u22a4\ni U\u039bUE\u22a4PiX\n\r\r\r2 \u2a7d8k\u00b5(X)logn\npn\n\r\r\re\u22a4\ni M\n\r\r\r2 \u00b7\n\r\r\rV \u22a4X\n\r\r\r2 = \u03b42\n10\n\r\r\re\u22a4\ni M\n\r\r\r2 \u00b7\n\r\r\rV \u22a4X\n\r\r\r2 .\nThe claim now follows from Markov\u2019s inequality.\n\u25a0\nLemma A.6. Let 0 < \u03b4 < 1. Assume that p \u2273k\u00b5(X)logn\n\u03b42n\n. Then, for every i \u2208[n],\nP\nn\n\u2225e\u22a4\ni GN\u2225> \u03b4\u2225e\u22a4\ni N\u2225\no\n\u2a7d1\n10 .\nProof. Recall, by Lemma 4.2, we have\ne\u22a4\ni GN = e\u22a4\ni NPiXB\u22121\ni\n\u2212e\u22a4\ni NX = (e\u22a4\ni NPiX \u2212e\u22a4\ni NXBi)B\u22121\ni\nPlugging in the lower bound on p into Lemma A.4, we get that \u2225Bi \u2212I\u2225\u2a7d\u03b4/4 for all i \u2208[n] with\nprobability 19/20.\nWe will show that for every \ufb01xed i \u2208[n], we have with probability 19/20 that\n\u2225e\u22a4\ni NPiX \u2212e\u22a4\ni NX\u2225\u2a7d(\u03b4/4)\u2225e\u22a4\ni N\u2225\n(10)\nBoth events simultaneously occur with probablity 9/10 and in this case we have:\n\r\r\re\u22a4\ni GN\r\r\r \u2a7d\n\r\r\re\u22a4\ni NPiX \u2212e\u22a4\ni NX\n\r\r\r\n\r\r\rB\u22121\ni\n\r\r\r +\n\r\r\re\u22a4\ni NX\n\r\r\r\u2225Bi \u2212I\u2225\n\r\r\rB\u22121\ni\n\r\r\r\n\u2a7d(\u03b4/2)\n\r\r\re\u22a4\ni N\n\r\r\r + (\u03b4/2)\n\r\r\re\u22a4\ni N\n\r\r\r = \u03b4\n\r\r\re\u22a4\ni N\n\r\r\r .\nSo, it remains to show Equation 10. Fix i \u2208[n]. Let \u2126\u2208Rk\u00d7k be an orthonormal transformation\nsuch that all columns of X\u2032 = X\u2126have \u2113\u221e-norm at most\np\n8\u00b5(X)log(n)/n. Let w\u22a4= e\u22a4\ni N be the i-th\nrow of N. Let us denote by x\u2032\n1,...,x\u2032\nk the k columns of X\u2032. We have that\nE\n\r\r\rw\u22a4PiX \u2212w\u22a4X\n\r\r\r2 = E\n\r\r\rw\u22a4(Pi \u2212I)X\u2032\r\r\r2 =\nk\nX\nr=1\nE\u27e8w,(Pi \u2212I)x\u2032\nr\u27e92\n=\nk\nX\nr=1\nn\nX\nj=1\nE((Pi)jj \u22121)2w2\nj (x\u2032\nr)2\nj\n\u2a7d\u2225w\u22252 \u00b7 8k\u00b5(X)logn\npn\n\u2a7d\u03b4\n80\u2225e\u22a4\ni N\u2225.\nThe bound in Equation 10 now follows from Markov\u2019s inequality.\n\u25a0\n24\nB\nAdditional lemmas and proofs for smooth QR factorization\nProof of Lemma 5.1. Fix a unit vector x \u2208Rk. We have\n\u2225PV (G + H)x\u22252 \u2a7e\u2225PV Hx\u22252 \u2212|\u27e8PV Gx,PV Hx\u27e9|\nNote that g = Hx is distributed like N(0,\u03c42/n)n and y = PV Cx has norm at most 1. Due to the\nrotational invariance of the Gaussian measure, we may assume without loss of generality that\nV is the subspace spanned by the \ufb01rst n \u2212k standard basis vectors in Rn. Hence, denoting h \u223c\nN(0,\u03c42/n)n\u2212k, our goal is to lower bound \u2225h\u22252 \u2212|\u27e8y,h\u27e9|. Note that E\u2225h\u22252 \u2a7e\u03c42/2 and by standard\nconcentration bounds for the norm of a Gaussian variable we have\nP\nn\n\u2225h\u22252 \u2a7d\u03c42/4\no\n\u2a7dexp(\u2212\u2126(n)).\nOn the other hand \u27e8y,h\u27e9is distributed like a one-dimensional Gaussian variable of variance at most\n\u03c42/n. Hence, by Gaussian tail bounds, P\nn\n\u27e8y,h\u27e92 > \u03c42/8\no\n\u2a7dexp(\u2212\u2126(n)). Hence, with probability\n1\u2212exp(\u2212\u2126(n)), we have \u2225PV (G+H)x\u2225\u2a7e\u2126(\u03c4). We can now take a union bound over a net of the unit\nsphere in Rk of size exp(O(k logk)) to conclude that with probability 1\u2212exp(O(k logk))exp(\u2212\u2126(n)),\nwe have for all unit vectors x \u2208Rk that \u2225PV (G + H)x\u2225\u2a7e\u2126(\u03c4). Therefore \u03c3k(PV (G + H)) \u2a7e\u2126(\u03c4).\nBy our assumption exp(O(k logk)) = exp(o(n)) and hence this event occurs with probability 1 \u2212\nexp(\u2212\u2126(n)).\n\u25a0\nLemma B.1. Let P be the projection onto an (n \u2212k)-dimensional subspace. Let H \u223cN(0,1/n)n\u00d7k. Then,\n\u03c1(PH) \u2a7dO(logn) with probability 1 \u22121/n5.\nProof. We have that P = (I \u2212UU\u22a4) for some k-dimensional basis U. Hence,\n\u03c1(PU) \u2a7dO(\u03c1(H)) + O(\u03c1(UU\u22a4H)).\nUsing concentration bounds for the norm of each row of H and a union bound over all rows it\nfollows straightforwardly that \u03c1(H) \u2a7dO(logn) with probability 1 \u22121/2n5. The second term satis\ufb01es\n\u03c1(UU\u22a4) \u2a7d\u03c1(U)\u2225U\u22a4H\u22252 = \u00b5(U)\u2225U\u22a4H\u22252 .\nBut U\u22a4H is a Gaussian matrix N(0,1/n)k\u00d7k and hence its largest singular value satis\ufb01es \u2225U\u22a4H\u22252 \u2a7d\nO(k log(n)/n) with probability 1 \u22121/2n5.\n\u25a0\nLemma B.2. Let X,Y be k and k\u2032 dimensional subspaces, respectively, such that R(X) \u2286R(Y). Then,\n\u00b5(X) \u2a7dk\u2032\nk \u00b5(Y).\nProof. We know that \u00b5(Y) is rotationally invariant. Therefore, without loss of generality we\nmay assume that Y = [X | X\u2032] for some orthonormal matrix X\u2032. Here, we identify X and Y with\northonormal bases. Hence,\n\u00b5(X) = n\nk max\ni\u2208[n] \u2225e\u22a4\ni X\u22252 \u2a7dn\nk max\ni\u2208[n]\n\u0010\n\u2225e\u22a4\ni X\u22252 + \u2225e\u22a4\ni X\u2032\u22252\u0011\n= n\nk max\ni\u2208[n] \u2225e\u22a4\ni Y\u22252 = k\u2032\nk \u00b5(Y).\n\u25a0\nThe following technical lemma was needed in the proof of Theorem 6.1.\nLemma B.3. Under the assumptions of Theorem 6.1, we have for every \u2113\u2208[L] and \u03bd = \u03c3k\n32(\u2225V \u22a4X\u2113\u22121\u2225+\u03b5)\nwith probability 1 \u22121/n2,\n1\n\u03bd2 (\u03c1(G) + \u03c1(NX\u2113\u22121)) \u2a7d3\u00b5\u2217.\n25\nProof. Given the lower bound on p in Theorem 6.1 we can apply Lemma 4.4 to conclude that\n\u2225e\u22a4\ni GM\n\u2113\u2225\u2a7d\np\nk\u00b5(U)/n \u00b7 \u03bd and \u2225e\u22a4\ni GN\n\u2113\u2225\u2a7d\np\n\u00b5\u2217/n \u00b7 \u03bd. Hence, \u03c1(G\u2113)/\u03bd2 \u2a7d\u00b5\u2217.\nFurther, we claim that \u2225e\u22a4\ni NX\u22252 \u2a7d(\u00b5\u2217/n)\u03c3k\u2225V \u22a4U\u2225for all i \u2208[n], because\n\r\r\re\u22a4\ni NX\n\r\r\r \u2a7d\n\r\r\re\u22a4\ni V \u03a3V\n\r\r\r \u00b7\n\r\r\rV \u22a4X\u2113\u22121\n\r\r\r =\n\r\r\re\u22a4\ni N\n\r\r\r \u00b7\n\r\r\rV \u22a4X\u2113\u22121\n\r\r\r .\nHere we used the fact that\n\r\r\re\u22a4\ni N\n\r\r\r2 =\n\r\r\re\u22a4\ni NV\n\r\r\r2 +\n\r\r\re\u22a4\ni NU\n\r\r\r2 =\n\r\r\re\u22a4\ni NV\n\r\r\r2 =\n\r\r\re\u22a4\ni V \u03a3V\n\r\r\r2 .\nUsing Equation 2, this shows that \u03c1(NX\u2113\u22121)/\u03bd2 \u2a7d\u00b5\u2217and \ufb01nishes the proof.\n\u25a0\nC\nSplitting up the subsample\nWe needed a procedure Split(\u2126,t) that takes a sample \u2126and splits it into t independent samples\nthat preserve the distributional assumption that we need. The next lemma is standard.\nLemma C.1. There is a procedure Split(\u2126,t) such that if \u2126is sampled by including each element\nindependently with probability p, then Split(\u2126,t) outputs independent random variables \u21261,...,\u2126t\nsuch that each set \u2126i includes each element independently with probability pi \u2a7ep/t.\nProof sketch. Consider independent random samples \u2126\u2032\n1,...,\u2126\u2032\nt where each set contains every\nelement independently with probability p/2t. Consider the multi-set \u2126\u2032 obtained from taking\nthe union of these sets (counting multiplicities). Each element occurs in \u2126\u2032 at least once with\nprobability p\u2032 = 1 \u2212(1 \u2212p/t)t \u2a7dp. The multiplicity is distributed according to a binomial random\nvariable. Hence, we can simulate the distribution of \u2126\u2032 given the random sample \u2126by subsampling\nso that each entry is included with probability p\u2032 and then introducing multiplicities randomly\naccording to the correct Binomial distribution. On the other hand, given the random variable \u2126\u2032\nwe can easily simulate \u2126\u2032\n1,...,\u2126\u2032\nt by assigning each element present in \u2126\u2032 with multiplicity k to a\nrandom subset of k out t sets.\n\u25a0\nD\nGeneralization to rectangular matrices\nFor our purposes it will su\ufb03ce to consider symmetric square matrices. This follows from a\nsimple transformation that preserves the matrix coherence and singular vectors of the matrix.\nIndeed, given a matrix B \u2208Rm\u00d7n and m \u2a7dn with singular value decomposition B = Pr\ni=1 \u03c3iuiv\u22a4\ni ,\nwe may consider the symmetric (m + n) \u00d7 (m + n) matrix A =\n\"\n0\nB\nB\u22a4\n0\n#\n. The matrix A has the\nfollowing properties: A has a rank 2 \u00b7 rank(B) and singular values \u03c31(B),...,\u03c3r(B) each occuring\nwith multiplicity two. The singular vectors corresponding to a singular value \u03c3i are spanned by the\nvectors {(ui,0),(0,vi)}. In particular, an algorithm to \ufb01nd a rank 2k approximation to A also \ufb01nds a\nrank 2k approximation to B up to the same error.\nMoreover, let e\nU denote the space spanned by the top 2k singular vectors of A, and let U,\nrespectively V , denote the space spanned by the top k left, repectively right, singular vectors of B.\nThen \u00b5(e\nU) \u2a7dn+m\n2k\n\u0012\n\u00b5(U)k\nm\n+ \u00b5(V )k\nn\n\u0013\n\u2a7dn+m\nm max\b\u00b5(U),\u00b5(V )\t . Note that we can assume that (n + m)/m is\nconstant by splitting B into a sequence of m \u00d7 O(m) matrices and recovering each matrix separately.\nIt will also be important for us that we can turn a uniformly random subsample of B into a\nuniformly random subsample of A. This is easily accomplished by splitting the sample into two\nequally sized halves, using one for B and one for B\u22a4. The remaining quadrants of A are 0 and can\nbe subsampled trivially of any given density.\n26\n",
        "sentence": "",
        "context": "vastava, and Mary Wootters for many helpful discussions. We thank the Simons Institute for\nTheoretical Computer Science at Berkeley, where some of this research was done.\n20\nReferences\n[AKKS]\nV \u22a4WO\n\r\r\rF + 2\u2225WO \u2212T \u2225F \u2a7d2\n\r\r\rV \u22a4W\n\r\r\rF + 1\n8 \u2a7d1\n4 .\n\u25a0\nAcknowledgments\nThanks to David Gleich, Prateek Jain, Jonathan Kelner, Raghu Meka, Ankur Moitra, Nikhil Sri-\nfundamental role as an optimization problem and its applicability in number of areas including\ncollaborative \ufb01ltering and quantum tomography. Alternating minimization has been used early on"
    },
    {
        "title": "A tail inequality for quadratic forms of subgaussian random vectors",
        "author": [
            "Daniel Hsu",
            "Sham M Kakade",
            "Tong Zhang"
        ],
        "venue": "Electron. Commun. Probab,",
        "citeRegEx": "Hsu et al\\.,? \\Q2012\\E",
        "shortCiteRegEx": "Hsu et al\\.",
        "year": 2012,
        "abstract": "We prove an exponential probability tail inequality for positive semidefinite\nquadratic forms in a subgaussian random vector. The bound is analogous to one\nthat holds when the vector has independent Gaussian entries.",
        "full_text": "arXiv:1110.2842v1  [math.PR]  13 Oct 2011\nA tail inequality for quadratic forms of subgaussian random vectors\nDaniel Hsu1, Sham M. Kakade1,2, and Tong Zhang3\n1Microsoft Research New England\n2Department of Statistics, Wharton School, University of Pennsylvania\n3Department of Statistics, Rutgers University\nOctober 14, 2011\nAbstract\nWe prove an exponential probability tail inequality for positive semide\ufb01nite quadratic forms\nin a subgaussian random vector. The bound is analogous to one that holds when the vector has\nindependent Gaussian entries.\n1\nIntroduction\nSuppose that x = (x1, . . . , xn) is a random vector. Let A \u2208Rm\u00d7n be a \ufb01xed matrix. A natural\nquantity that arises in many settings is the quadratic form \u2225Ax\u22252 = x\u22a4(A\u22a4A)x. Throughout \u2225v\u2225\ndenotes the Euclidean norm of a vector v, and \u2225M\u2225denotes the spectral (operator) norm of a\nmatrix M. We are interested in how close \u2225Ax\u22252 is to its expectation.\nConsider the special case where x1, . . . , xn are independent standard Gaussian random variables.\nThe following proposition provides an (upper) tail bound for \u2225Ax\u22252.\nProposition 1. Let A \u2208Rm\u00d7n be a matrix, and let \u03a3 := A\u22a4A.\nLet x = (x1, . . . , xn) be an\nisotropic multivariate Gaussian random vector with mean zero. For all t > 0,\nPr\nh\n\u2225Ax\u22252 > tr(\u03a3) + 2\np\ntr(\u03a32)t + 2\u2225\u03a3\u2225t\ni\n\u2264e\u2212t.\nThe proof, given in Appendix A.2, is straightforward given the rotational invariance of the\nmultivariate Gaussian distribution, together with a tail bound for linear combinations of \u03c72 random\nvariables due to Laurent and Massart (2000). We note that a slightly weaker form of Proposition 1\ncan be proved directly using Gaussian concentration (Pisier, 1989).\nIn this note, we consider the case where x = (x1, . . . , xn) is a subgaussian random vector. By\nthis, we mean that there exists a \u03c3 \u22650, such that for all \u03b1 \u2208Rn,\nE\nh\nexp\n\u0010\n\u03b1\u22a4x\n\u0011i\n\u2264exp\n\u0000\u2225\u03b1\u22252\u03c32/2\n\u0001\n.\nWe provide a sharp upper tail bound for this case analogous to one that holds in the Gaussian case\n(indeed, the same as Proposition 1 when \u03c3 = 1).\nE-mail: dahsu@microsoft.com, skakade@wharton.upenn.edu, tzhang@stat.rutgers.edu\n1\nTail inequalities for sums of random vectors\nOne motivation for our main result comes from the following observations about sums of random\nvectors. Let a1, . . . , an be vectors in a Euclidean space, and let A = [a1| \u00b7 \u00b7 \u00b7 |an] be the matrix with\nai as its ith column. Consider the squared norm of the random sum\n\u2225Ax\u22252 =\n\r\r\r\r\nn\nX\ni=1\naixi\n\r\r\r\r\n2\n(1)\nwhere x := (x1, . . . , xn) is a martingale di\ufb00erence sequence with E[xi | x1, . . . , xi\u22121] = 0 and\nE[x2\ni | x1, . . . , xi\u22121] = \u03c32. Under mild boundedness assumptions on the xi, the probability that the\nsquared norm in (1) is much larger than its expectation\nE[\u2225Ax\u22252] = \u03c32\nn\nX\ni=1\n\u2225ai\u22252 = \u03c32 tr(A\u22a4A)\nfalls o\ufb00exponentially fast. This can be shown, for instance, using the following lemma by taking\nui = aixi (the proof is standard, but we give it for completeness in Appendix A.1).\nProposition 2. Let u1, . . . , un be a martingale di\ufb00erence vector sequence ( i.e., E[ui|u1, . . . , ui\u22121] =\n0 for all i = 1, . . . , n) such that\nn\nX\ni=1\nE\n\u0002\n\u2225ui\u22252 | u1, . . . , ui\u22121\n\u0003\n\u2264v\nand\n\u2225ui\u2225\u2264b\nfor all i = 1, . . . , n, almost surely. For all t > 0,\nPr\n\"\r\r\r\r\nn\nX\ni=1\nui\n\r\r\r\r > \u221av +\n\u221a\n8vt + (4/3)bt\n#\n\u2264e\u2212t.\nAfter squaring the quantities in the stated probabilistic event, Proposition 2 gives the bound\n\u2225Ax\u22252 \u2264\u03c32 \u00b7 tr(A\u22a4A) + \u03c32 \u00b7 O\n\u0012\ntr(A\u22a4A)(\n\u221a\nt + t) +\nq\ntr(A\u22a4A) max\ni\n\u2225ai\u2225(t + t3/2) + max\ni\n\u2225ai\u22252t2\n\u0013\nwith probability at least 1 \u2212e\u2212t when the xi are almost surely bounded by 1 (or any constant).\nUnfortunately, this bound obtained from Proposition 2 can be suboptimal when the xi are\nsubgaussian. For instance, if the xi are Rademacher random variables, so Pr[xi = +1] = Pr[xi =\n\u22121] = 1/2, then it is known that\n\u2225Ax\u22252 \u2264tr(A\u22a4A) + O\n\u0012q\ntr((A\u22a4A)2)t + \u2225A\u22252t\n\u0013\n(2)\nwith probability at least 1 \u2212e\u2212t. A similar result holds for any subgaussian distribution on the\nxi (Hanson and Wright, 1971).\nThis is an improvement over the previous bound because the\ndeviation terms (i.e., those involving t) can be signi\ufb01cantly smaller, especially for large t.\nIn this work, we give a simple proof of (2) with explicit constants that match the analogous\nbound when the xi are independent standard Gaussian random variables.\n2\n2\nPositive semide\ufb01nite quadratic forms\nOur main theorem, given below, is a generalization of (2).\nTheorem 1. Let A \u2208Rm\u00d7n be a matrix, and let \u03a3 := A\u22a4A. Suppose that x = (x1, . . . , xn) is a\nrandom vector such that, for some \u00b5 \u2208Rn and \u03c3 \u22650,\nE\nh\nexp\n\u0010\n\u03b1\u22a4(x \u2212\u00b5)\n\u0011i\n\u2264exp\n\u0000\u2225\u03b1\u22252\u03c32/2\n\u0001\n(3)\nfor all \u03b1 \u2208Rn. For all t > 0,\nPr\n\"\n\u2225Ax\u22252 > \u03c32\u00b7\n\u0010\ntr(\u03a3)+2\np\ntr(\u03a32)t+2\u2225\u03a3\u2225t\n\u0011\n+\u2225A\u00b5\u22252\u00b7\n\u0012\n1+4\n\u0012 \u2225\u03a3\u22252\ntr(\u03a32)t\n\u00131/2\n+4\u2225\u03a3\u22252\ntr(\u03a32)t\n\u00131/2 #\n\u2264e\u2212t.\nRemark 1. Note that when \u00b5 = 0 and \u03c3 = 1 we have:\nPr\nh\n\u2225Ax\u22252 > tr(\u03a3) + 2\np\ntr(\u03a32)t + 2\u2225\u03a3\u2225t\ni\n\u2264e\u2212t\nwhich is the same as Proposition 1.\nRemark 2. Our proof actually establishes the following upper bounds on the moment generating\nfunction of \u2225Ax\u22252 for 0 \u2264\u03b7 < 1/(2\u03c32\u2225\u03a3\u2225):\nE\n\u0002\nexp\n\u0000\u03b7\u2225Ax\u22252\u0001\u0003\n\u2264E\nh\nexp\n\u0010\n\u03c32\u2225A\u22a4z\u22252\u03b7 + \u00b5\u22a4A\u22a4z\np\n2\u03b7\n\u0011i\n\u2264exp\n\u0012\n\u03c32 tr(\u03a3)\u03b7 + \u03c34 tr(\u03a32)\u03b72 + \u2225A\u00b5\u22252\u03b7\n1 \u22122\u03c32\u2225\u03a3\u2225\u03b7\n\u0013\nwhere z is a vector of m independent standard Gaussian random variables.\nProof of Theorem 1. Let z be a vector of m independent standard Gaussian random variables\n(sampled independently of x). For any \u03b1 \u2208Rm,\nE\nh\nexp\n\u0010\nz\u22a4\u03b1\n\u0011i\n= exp\n\u0000\u2225\u03b1\u22252/2\n\u0001\n.\nThus, for any \u03bb \u2208R and \u03b5 \u22650,\nE\nh\nexp\n\u0010\n\u03bbz\u22a4Ax\n\u0011i\n\u2265E\n\u0014\nexp\n\u0010\n\u03bbz\u22a4Ax\n\u0011 \f\f\f\f \u2225Ax\u22252 > \u03b5\n\u0015\n\u00b7 Pr\n\u0002\n\u2225Ax\u22252 > \u03b5\n\u0003\n\u2265exp\n\u0012\u03bb2\u03b5\n2\n\u0013\n\u00b7 Pr\n\u0002\n\u2225Ax\u22252 > \u03b5\n\u0003\n.\n(4)\nMoreover,\nE\nh\nexp\n\u0010\n\u03bbz\u22a4Ax\n\u0011i\n= E\n\u0014\nE\n\u0014\nexp\n\u0010\n\u03bbz\u22a4A(x \u2212\u00b5)\n\u0011 \f\f\f\f z\n\u0015\nexp\n\u0010\n\u03bbz\u22a4A\u00b5\n\u0011\u0015\n\u2264E\n\u0014\nexp\n\u0012\u03bb2\u03c32\n2\n\u2225A\u22a4z\u22252 + \u03bb\u00b5\u22a4A\u22a4z\n\u0013\u0015\n(5)\n3\nLet USV \u22a4be a singular value decomposition of A; where U and V are, respectively, matrices of\northonormal left and right singular vectors; and S = diag(\u221a\u03c11, . . . , \u221a\u03c1m) is the diagonal matrix of\ncorresponding singular values. Note that\n\u2225\u03c1\u22251 =\nm\nX\ni=1\n\u03c1i = tr(\u03a3),\n\u2225\u03c1\u22252\n2 =\nm\nX\ni=1\n\u03c12\ni = tr(\u03a32),\nand\n\u2225\u03c1\u2225\u221e= max\ni\n\u03c1i = \u2225\u03a3\u2225.\nBy rotational invariance, y := U \u22a4z is an isotropic multivariate Gaussian random vector with mean\nzero. Therefore \u2225A\u22a4z\u22252 = z\u22a4US2U \u22a4z = \u03c11y2\n1 +\u00b7 \u00b7 \u00b7+\u03c1my2\nm and \u00b5\u22a4A\u22a4z = \u03bd\u22a4y = \u03bd1y1+\u00b7 \u00b7 \u00b7+\u03bdmym,\nwhere \u03bd := SV \u22a4\u00b5 (note that \u2225\u03bd\u22252 = \u2225SV \u22a4\u00b5\u22252 = \u2225A\u00b5\u22252). Let \u03b3 := \u03bb2\u03c32/2. By Lemma 1,\nE\n\"\nexp\n \n\u03b3\nm\nX\ni=1\n\u03c1iy2\ni +\n\u221a2\u03b3\n\u03c3\nm\nX\ni=1\n\u03bdiyi\n!#\n\u2264exp\n\u0012\n\u2225\u03c1\u22251\u03b3 + \u2225\u03c1\u22252\n2\u03b32 + \u2225\u03bd\u22252\u03b3/\u03c32\n1 \u22122\u2225\u03c1\u2225\u221e\u03b3\n\u0013\n(6)\nfor 0 \u2264\u03b3 < 1/(2\u2225\u03c1\u2225\u221e). Combining (4), (5), and (6) gives\nPr\n\u0002\n\u2225Ax\u22252 > \u03b5\n\u0003\n\u2264exp\n\u0012\n\u2212\u03b5\u03b3/\u03c32 + \u2225\u03c1\u22251\u03b3 + \u2225\u03c1\u22252\n2\u03b32 + \u2225\u03bd\u22252\u03b3/\u03c32\n1 \u22122\u2225\u03c1\u2225\u221e\u03b3\n\u0013\nfor 0 \u2264\u03b3 < 1/(2\u2225\u03c1\u2225\u221e) and \u03b5 \u22650. Choosing\n\u03b5 := \u03c32(\u2225\u03c1\u22251 + \u03c4) + \u2225\u03bd\u22252\ns\n1 + 2\u2225\u03c1\u2225\u221e\u03c4\n\u2225\u03c1\u22252\n2\nand\n\u03b3 :=\n1\n2\u2225\u03c1\u2225\u221e\n \n1 \u2212\ns\n\u2225\u03c1\u22252\n2\n\u2225\u03c1\u22252\n2 + 2\u2225\u03c1\u2225\u221e\u03c4\n!\n,\nwe have\nPr\n\"\n\u2225Ax\u22252 > \u03c32(\u2225\u03c1\u22251 + \u03c4) + \u2225\u03bd\u22252\ns\n1 + 2\u2225\u03c1\u2225\u221e\u03c4\n\u2225\u03c1\u22252\n2\n#\n\u2264exp\n \n\u2212\u2225\u03c1\u22252\n2\n2\u2225\u03c1\u22252\u221e\n \n1 + \u2225\u03c1\u2225\u221e\u03c4\n\u2225\u03c1\u22252\n2\n\u2212\ns\n1 + 2\u2225\u03c1\u2225\u221e\u03c4\n\u2225\u03c1\u22252\n2\n!!\n= exp\n \n\u2212\u2225\u03c1\u22252\n2\n2\u2225\u03c1\u22252\u221e\nh1\n \n\u2225\u03c1\u2225\u221e\u03c4\n\u2225\u03c1\u22252\n2\n!!\nwhere h1(a) := 1+a\u2212\u221a1 + 2a, which has the inverse function h\u22121\n1 (b) =\n\u221a\n2b+b. The result follows\nby setting \u03c4 := 2\np\n\u2225\u03c1\u22252\n2t + 2\u2225\u03c1\u2225\u221et = 2\np\ntr(\u03a32)t + 2\u2225\u03a3\u2225t.\nThe following lemma is a standard estimate of the logarithmic moment generating function of a\nquadratic form in standard Gaussian random variables, proved much along the lines of the estimate\ndue to Laurent and Massart (2000).\nLemma 1. Let z be a vector of m independent standard Gaussian random variables.\nFix any\nnon-negative vector \u03b1 \u2208Rm\n+ and any vector \u03b2 \u2208Rm. If 0 \u2264\u03bb < 1/(2\u2225\u03b1\u2225\u221e), then\nlog E\n\"\nexp\n \n\u03bb\nm\nX\ni=1\n\u03b1iz2\ni +\nm\nX\ni=1\n\u03b2izi\n!#\n\u2264\u2225\u03b1\u22251\u03bb + \u2225\u03b1\u22252\n2\u03bb2 + \u2225\u03b2\u22252\n2/2\n1 \u22122\u2225\u03b1\u2225\u221e\u03bb\n.\n4\nProof. Fix \u03bb \u2208R such that 0 \u2264\u03bb < 1/(2\u2225\u03b1\u2225\u221e), and let \u03b7i := 1/\u221a1 \u22122\u03b1i\u03bb > 0 for i = 1, . . . , m.\nWe have\nE\n\u0002\nexp\n\u0000\u03bb\u03b1iz2\ni + \u03b2izi\n\u0001\u0003\n=\nZ \u221e\n\u2212\u221e\n1\n\u221a\n2\u03c0 exp\n\u0000\u2212z2\ni /2\n\u0001\nexp\n\u0000\u03bb\u03b1iz2\ni + \u03b2izi\n\u0001\ndzi\n= \u03b7i exp\n\u0012\u03b22\ni \u03b72\ni\n2\n\u0013 Z \u221e\n\u2212\u221e\n1\nq\n2\u03c0\u03b72\ni\nexp\n\u0012\n\u22121\n2\u03b72\ni\n\u0000zi \u2212\u03b2i\u03b72\ni\n\u00012\n\u0013\ndzi\nso\nlog E\n\"\nexp\n \n\u03bb\nm\nX\ni=1\n\u03b1iz2\ni +\nm\nX\ni=1\n\u03b2izi\n!#\n= 1\n2\nm\nX\ni=1\n\u03b22\ni \u03b72\ni + 1\n2\nm\nX\ni=1\nlog \u03b72\ni .\nThe right-hand side can be bounded using the inequalities\n1\n2\nm\nX\ni=1\nlog \u03b72\ni = \u22121\n2\nm\nX\ni=1\nlog(1 \u22122\u03b1i\u03bb) = 1\n2\nm\nX\ni=1\n\u221e\nX\nj=1\n(2\u03b1i\u03bb)j\nj\n\u2264\u2225\u03b1\u22251\u03bb +\n\u2225\u03b1\u22252\n2\u03bb2\n1 \u22122\u2225\u03b1\u2225\u221e\u03bb\nand\n1\n2\nm\nX\ni=1\n\u03b22\ni \u03b72\ni \u2264\n\u2225\u03b2\u22252\n2/2\n1 \u22122\u2225\u03b1\u2225\u221e\u03bb.\nExample: \ufb01xed-design regression with subgaussian noise\nWe give a simple application of Theorem 1 to \ufb01xed-design linear regression with the ordinary least\nsquares estimator.\nLet x1, . . . , xn be \ufb01xed design vectors in Rd. Let the responses y1, . . . , yn be random variables\nfor which there exists \u03c3 > 0 such that\nE\n\"\nexp\n n\nX\ni=1\n\u03b1i(yi \u2212E[yi])\n!#\n\u2264exp\n \n\u03c32\nn\nX\ni=1\n\u03b12\ni\n!\nfor any \u03b11, . . . , \u03b1n \u2208R. This condition is satis\ufb01ed, for instance, if\nyi = E[yi] + \u03b5i\nfor independent subgaussian zero-mean noise variables \u03b51, . . . , \u03b5n. Let \u03a3 := Pn\ni=1 xix\u22a4\ni /n, which\nwe assume is invertible without loss of generality. Let\n\u03b2 := \u03a3\u22121\n \n1\nn\nn\nX\ni=1\nxiE[yi]\n!\nbe the coe\ufb03cient vector of minimum expected squared error. The ordinary least squares estimator\nis given by\n\u02c6\u03b2 := \u03a3\u22121\n \n1\nn\nn\nX\ni=1\nxiyi\n!\n.\n5\nThe excess loss R(\u02c6\u03b2) of \u02c6\u03b2 is the di\ufb00erence between the expected squared error of \u02c6\u03b2 and that of \u03b2:\nR(\u02c6\u03b2) := E\n\"\n1\nn\nn\nX\ni=1\n(x\u22a4\ni \u02c6\u03b2 \u2212yi)2\n#\n\u2212E\n\"\n1\nn\nn\nX\ni=1\n(x\u22a4\ni \u03b2 \u2212yi)2\n#\n.\nIt is easy to see that\nR(\u02c6\u03b2) =\n\r\r\u03a31/2(\u02c6\u03b2 \u2212\u03b2)\n\r\r2 =\n\r\r\r\nn\nX\ni=1\n\u0000\u03a3\u22121/2xi\n\u0001\n(yi \u2212E[yi])\n\r\r\r\n2\n.\nBy Theorem 1,\nPr\n\"\nR(\u02c6\u03b2) > \u03c32\u0000d + 2\n\u221a\ndt + 2t\n\u0001\nn\n#\n\u2264e\u2212t.\nNote that in the case that E[(yi \u2212E[yi])2] = \u03c32 for each i, then\nE[R(\u02c6\u03b2)] = \u03c32d\nn ;\nso the tail inequality above is essentially tight when the yi are independent Gaussian random\nvariables.\nReferences\nD. L. Hanson and F. T. Wright. A bound on tail probabilities for quadratic forms in independent\nrandom variables. The Annals of Mathematical Statistics, 42(3):1079\u20131083, 1971.\nB. Laurent and P. Massart. Adaptive estimation of a quadratic functional by model selection. The\nAnnals of Statistics, 28(5):1302\u20131338, 2000.\nG. Pisier. The volume of convex bodies and Banach space geometry. Cambridge University Press,\n1989.\nA\nStandard tail inequalities\nA.1\nMartingale tail inequalities\nThe following is a standard form of Bernstein\u2019s inequality stated for martingale di\ufb00erence sequences.\nLemma 2 (Bernstein\u2019s inequality for martingales). Let d1, . . . , dn be a martingale di\ufb00erence se-\nquence with respect to random variables x1, . . . , xn ( i.e., E[di|x1, . . . , xi\u22121] = 0 for all i = 1, . . . , n)\nsuch that |di| \u2264b and Pn\ni=1 E[d2\ni |x1, . . . , xi\u22121] \u2264v. For all t > 0,\nPr\n\" n\nX\ni=1\ndi >\n\u221a\n2vt + (2/3)bt\n#\n\u2264e\u2212t.\nThe proof of Proposition 2, which is entirely standard, is an immediate consequence of the\nfollowing two lemmas together with Jensen\u2019s inequality.\n6\nLemma 3. Let u1, . . . , un be random vectors such that\nn\nX\ni=1\nE\n\u0002\n\u2225ui\u22252 | u1, . . . , ui\u22121\n\u0003\n\u2264v\nand\n\u2225ui\u2225\u2264b.\nfor all i = 1, . . . , n, almost surely. For all t > 0,\nPr\n\" \r\r\r\r\nn\nX\ni=1\nui\n\r\r\r\r \u2212E\n\u0014\r\r\r\r\nn\nX\ni=1\nui\n\r\r\r\r\n\u0015\n>\n\u221a\n8vt + (4/3)bt\n#\n\u2264e\u2212t.\nProof. Let sn := u1 + \u00b7 \u00b7 \u00b7 + un. De\ufb01ne the Doob martingale\ndi := E[\u2225sn\u2225| u1, . . . , ui] \u2212E[\u2225sn\u2225| u1, . . . , ui\u22121]\nfor i = 1, . . . , n, so d1 + \u00b7 \u00b7 \u00b7 + dn = \u2225sn\u2225\u2212E[\u2225sn\u2225]. First, clearly, E[di|u1, . . . , ui\u22121] = 0. Next, the\ntriangle inequality implies\ndi = E [\u2225(sn \u2212ui) + ui\u2225| u1, . . . , ui] \u2212E [\u2225(sn \u2212ui) + ui\u2225| u1, . . . , ui\u22121]\n\u2264E [\u2225sn \u2212ui\u2225+ \u2225ui\u2225| u1, . . . , ui] \u2212E [\u2225\u2225sn \u2212ui\u2225\u2212\u2225ui\u2225| u1, . . . , ui\u22121]\n= \u2225ui\u2225+ E [\u2225ui\u2225| u1, . . . , ui\u22121] ,\nand similarly, di \u2265\u2212\u2225ui\u2225\u2212E [\u2225ui\u2225| u1, . . . , ui\u22121] .\nTherefore,\n|di| \u2264\u2225ui\u2225+ E [\u2225ui\u2225| u1, . . . , ui\u22121] \u22642b\nalmost surely.\nMoreover,\nE\n\u0002\nd2\ni | u1, . . . , ui\u22121\n\u0003\n\u2264E\nh\n\u2225ui\u22252 + 2 \u00b7 \u2225ui\u2225\u00b7 E [\u2225ui\u2225| u1, . . . , ui\u22121]\n+ E [\u2225ui\u2225| u1, . . . , ui\u22121]2 | u1, . . . , ui\u22121\ni\n= E\n\u0002\n\u2225ui\u22252 | u1, . . . , ui\u22121\n\u0003\n+ 3 \u00b7 E [\u2225ui\u2225| u1, . . . , ui\u22121]2\n\u22644 \u00b7 E\n\u0002\n\u2225ui\u22252 | u1, . . . , ui\u22121\n\u0003\n,\nso\nn\nX\ni=1\nE\n\u0002\nd2\ni | u1, . . . , ui\u22121\n\u0003\n\u22644v\nalmost surely.\nThe claim now follows from Bernstein\u2019s inequality (Lemma 2).\nLemma 4. If u1, . . . , un is a martingale di\ufb00erence vector sequence ( i.e., E[ui|u1, . . . , ui\u22121] = 0 for\nall i = 1, . . . , n), then\nE\n\u0014\r\r\r\r\nn\nX\ni=1\nui\n\r\r\r\r\n2\u0015\n=\nn\nX\ni=1\nE\n\u0002\n\u2225ui\u22252\u0003\n.\nProof. Let si := u1 + \u00b7 \u00b7 \u00b7 + ui for i = 1, . . . , n; we have\nE\n\u0002\n\u2225sn\u22252\u0003\n= E\n\u0002\nE\n\u0002\n\u2225un + sn\u22121\u22252 | u1, . . . , un\u22121\n\u0003\u0003\n= E\nh\nE\nh\n\u2225un\u22252 + 2u\u22a4\nn sn\u22121 + \u2225sn\u22121\u22252 | u1, . . . , un\u22121\nii\n= E\n\u0002\n\u2225un\u22252\u0003\n+ E\n\u0002\n\u2225sn\u22121\u22252\u0003\nso the claim follows by induction.\n7\nA.2\nGaussian quadratic forms and \u03c72 tail inequalities\nIt is well-known that if z \u223cN(0, 1) is a standard Gaussian random variable, then z2 follows a\n\u03c72 distribution with one degree of freedom. The following inequality due to Laurent and Massart\n(2000) gives a bound on linear combinations of \u03c72 random variables.\nLemma 5 (\u03c72 tail inequality; Laurent and Massart, 2000). Let q1, . . . , qn be independent \u03c72 random\nvariables, each with one degree of freedom. For any vector \u03b3 = (\u03b31, . . . , \u03b3n) \u2208Rn\n+ with non-negative\nentries, and any t > 0,\nPr\n\" n\nX\ni=1\n\u03b3iqi > \u2225\u03b3\u22251 + 2\nq\n\u2225\u03b3\u22252\n2t + 2\u2225\u03b3\u2225\u221et\n#\n\u2264e\u2212t.\nProof of Proposition 1. Let V \u039bV \u22a4be an eigen-decomposition of A\u22a4A, where V is a matrix of\northonormal eigenvectors, and \u039b := diag(\u03c11, . . . , \u03c1n) is the diagonal matrix of corresponding eigen-\nvalues \u03c11, . . . , \u03c1n. By the rotational invariance of the distribution, z := V \u22a4x is an isotropic multi-\nvariate Gaussian random vector with mean zero. Thus, \u2225Ax\u22252 = z\u22a4\u039bz = \u03c11z2\n1 +\u00b7 \u00b7 \u00b7+\u03c1nz2\nn, and the\nz2\ni are independent \u03c72 random variables, each with one degree of freedom. The claim now follows\nfrom a tail bound for \u03c72 random variables (Lemma 5, due to Laurent and Massart, 2000).\n8\n",
        "sentence": "",
        "context": "1Microsoft Research New England\n2Department of Statistics, Wharton School, University of Pennsylvania\n3Department of Statistics, Rutgers University\nOctober 14, 2011\nAbstract\nn\nX\ni=1\nE\n\u0002\n\u2225ui\u22252 | u1, . . . , ui\u22121\n\u0003\n\u2264v\nand\n\u2225ui\u2225\u2264b\nfor all i = 1, . . . , n, almost surely. For all t > 0,\nPr\n\"\r\r\r\r\nn\nX\ni=1\nui\n\r\r\r\r > \u221av +\n\u221a\n8vt + (4/3)bt\n#\n\u2264e\u2212t.\nG. Pisier. The volume of convex bodies and Banach space geometry. Cambridge University Press,\n1989.\nA\nStandard tail inequalities\nA.1\nMartingale tail inequalities"
    },
    {
        "title": "Fast matrix completion without the condition number",
        "author": [
            "Moritz Hardt",
            "Mary Wootters"
        ],
        "venue": "COLT",
        "citeRegEx": "Hardt and Wootters.,? \\Q2014\\E",
        "shortCiteRegEx": "Hardt and Wootters.",
        "year": 2014,
        "abstract": "We give the first algorithm for Matrix Completion whose running time and\nsample complexity is polynomial in the rank of the unknown target matrix,\nlinear in the dimension of the matrix, and logarithmic in the condition number\nof the matrix. To the best of our knowledge, all previous algorithms either\nincurred a quadratic dependence on the condition number of the unknown matrix\nor a quadratic dependence on the dimension of the matrix in the running time.\nOur algorithm is based on a novel extension of Alternating Minimization which\nwe show has theoretical guarantees under standard assumptions even in the\npresence of noise.",
        "full_text": "Fast Matrix Completion Without the Condition Number\nMoritz Hardt\nMary Wootters\u2217\nJune 22, 2021\nAbstract\nWe give the \ufb01rst algorithm for Matrix Completion whose running time and sample complexity is\npolynomial in the rank of the unknown target matrix, linear in the dimension of the matrix, and logarithmic\nin the condition number of the matrix. To the best of our knowledge, all previous algorithms either incurred\na quadratic dependence on the condition number of the unknown matrix or a quadratic dependence on the\ndimension of the matrix in the running time.\nOur algorithm is based on a novel extension of Alternating Minimization which we show has theoretical\nguarantees under standard assumptions even in the presence of noise.\n1\nIntroduction\nMatrix Completion is the problem of recovering an unknown real-valued low-rank matrix from a possibly\nnoisy subsample of its entries. The problem has received a tremendous amount of attention in signal\nprocessing and machine learning partly due to its wide applicability to recommender systems. A beautiful\nline of work showed that a particular convex program\u2014known as nuclear norm minimization\u2014achieves\nstrong recovery guarantees under certain reasonable feasibility assumptions [CR09, CT10, RFP10, Rec11].\nNuclear norm minimization boils down to solving a semide\ufb01nite program and therefore can be solved in\npolynomial time in the dimension of the matrix. Unfortunately, the approach is not immediately practical\ndue to the large polynomial dependence on the dimension of the matrix. An ongoing research e\ufb00ort aims to\ndesign large-scale algorithms for nuclear norm minimization [JY09, MHT10, JS10, AKKS12, HO14]. Such\nfast solvers, generally speaking, involve heuristics that improve empirical performance but may no longer\npreserve the strong theoretical guarantees of the nuclear norm approach.\nA successful scalable algorithmic alternative to Nuclear Norm Minimization is based on Alternating\nMinimization [BK07, HH09, KBV09]. Alternating Minimization aims to recover the unknown low-rank\nmatrix by alternatingly optimizing over one of two factors in a purported low-rank decomposition. Each\nupdate is a simple least squares regression problem that can be solved very e\ufb03ciently. As pointed out\nin [HO14], even state of the art nuclear norm solvers often cannot compete with Alternating Minimiza-\ntion with regards to scalability. A shortcoming of Alternating Minimization is that formal guarantees\nare less developed than for Nuclear Norm Minimization. Only recently has there been progress in this\ndirection [Kes12, JNS13, GAGG13, Har13a].\nUnfortunately, despite this recent progress all known convergence bounds for Alternating Minimization\nhave at least a quadratic dependence on the condition number of the matrix. Here, the condition number\nrefers to the ratio of the \ufb01rst to the k-th singular value of the matrix, where k is the target rank of the\ndecomposition. This dependence on the condition number can be a serious shortcoming. After all, Matrix\nCompletion rests on the assumption that the unknown matrix is approximately low-rank and hence we\nshould expect its singular values to decay rapidly. Indeed, strongly decaying singular values are a typical\nfeature of large real-world matrices.\n\u2217MW\u2019s work was supported in part by the Simons Institute and by a Rackham predoctoral fellowship.\n1\narXiv:1407.4070v1  [cs.LG]  15 Jul 2014\nThe dependence on the condition number in Alternating Minimization is not a mere artifact of the analy-\nsis. It arises naturally with the use of the Singular Value Decomposition (SVD). Alternating Minimization\nis typically intialized with a decomposition based on a truncated SVD of the partial input matrix. Such\nan approach must incur a polynomial dependence on the condition number. Many other approaches also\ncrucially rely on the SVD as a sub-routine, e.g., [JMD10, KMO10a, KMO10b], as well as most fast solvers\nfor the nuclear norm. In fact, there appears to be a kind of dichotomy in the current literature on Matrix\nCompletion: either the algorithm is not fast and has at least a quadratic dependence on the dimension of\nthe matrix in its running time, or it is not well-conditioned and has at least a quadratic dependence on the\ncondition number in the sample complexity. We emphasize that here we focus on formal guarantees rather\nthan observed empirical performance which may be better on certain instances. This situation leads us to\nthe following problem.\nMain Problem: Is there a sub-quadratic time algorithm for Matrix Completion\nwith a sub-linear dependence on the condition number?\nIn fact, eliminating the polynomial dependence on the condition number was posed explicitly as an open\nproblem in the context of Alternating Minimization by Jain, Netrapalli and Sanghavi [JNS13].\nIn this work, we resolve the question in the a\ufb03rmative. Speci\ufb01cally, we design a new variant of\nAlternating Minimization that achieves a logarithmic dependence on the condition number while retaining the\nfast running time of the standard Alternating Minimization framework. This is an exponential improvement\nin the condition number compared with all subquadratic time algorithms for Matrix Completion that\nwe are aware of. Our algorithm works even in the noisy Matrix Completion setting and under standard\nassumptions\u2014speci\ufb01cally, the same assumptions that support theoretical results for the nuclear norm. That\nis, we assume that the \ufb01rst k singular vector of the matrix span an incoherent subspace and that each entry\nof the matrix is revealed independently with a certain probability. While strong, these assumptions led to an\ninteresting theory of Matrix Completion and have become a de facto standard when comparing theoretical\nguarantees.\n1.1\nOur Results\nFor the sake of exposition we begin by explaining our results in the exact Matrix Completion setting, even\nthough our results here are a direct consequence of our theorem for the noisy case. In the exact problem\nthe goal is to recover an unknown rank k matrix M from a subsample \u2126\u2282[n] \u00d7 [n] of its entries where\neach entry is included independently with probability p. We assume that the unknown matrix M = U\u039bUT\nis a symmetric n \u00d7 n matrix with nonzero singular values \u03c31 \u2265\u00b7\u00b7\u00b7 \u2265\u03c3k > 0. Following [Har13a], our result\ngeneralizes straightforwardly to rectangular matrices. To state our result we need to de\ufb01ne the coherence of\nthe subspace spanned by U. Intuitively, the coherence controls how large the projection is of any standard\nbasis vector onto the space spanned by U. Formally, for a n \u00d7 k matrix U with orthonormal columns, we\nde\ufb01ne the coherence of U to be\n\u00b5(U) = max\ni\u2208[n]\nn\nk \u2225eT\ni U\u22252\n2 ,\nwhere e1,...,en is the standard basis of Rn. Note that this parameter varies between 1 and n/k. With this\nde\ufb01nition, we can state the formal sample complexity of our algorithm.\nWe show that our algorithm outputs a low-rank factorization XY T such that with high probability\n\u2225M \u2212XY T \u22252 \u2264\u03b5\u2225M\u2225provided that the expected size of \u2126satis\ufb01es\npn2 = O\n \nnkc\u00b5(U)2 log\n \u03c31\n\u03c3k\n!\nlog2 \u0012n\n\u03b5\n\u0013!\n.\n(1)\nHere, the exponent c > 0 is bounded by an absolute constant. While we did not focus on minimizing\nthe exponent, our results imply that the value of c can be chosen smaller if the singular values of M are\nwell-separated. The formal statement follows from Theorem 1. A notable advantage of our algorithm\ncompared to several fast algorithms for Matrix Completion is that the dependence on the error \u03b5 is only\n2\npoly-logarithmic. This linear convergence rate makes near exact recovery feasible with a small number of\nsteps.\nWe also show that the running time of our algorithm is bounded by e\nO(poly(k)pn2). That is, the running\ntime is nearly linear in the number of revealed entries except for a polynomial overhead in k. For small\nvalues of k and \u00b5(U), the total running time is nearly linear in n.\nNoisy Matrix Completion.\nWe now discuss our more general result that applies to the noisy or robust\nMatrix Completion problem. Here, the unknown matrix is only close to low-rank, typically in Frobenius\nnorm. Our results apply to any matrix of the form\nA = M + N = U\u039bUT + N,\n(2)\nwhere M = U\u039bUT is a matrix of rank k as before and N = (I \u2212UUT )A is the part of A not captured by the\ndominant singular vectors. We note that N can be an arbitrary deterministic matrix. The assumption that\nwe will make is that N satis\ufb01es the following incoherence conditions:\nmax\ni\u2208[n]\n\r\r\reT\ni N\n\r\r\r2\n2 \u2264\u00b5N\nn \u00b7 min\nn\n\u2225N\u22252\nF ,\u03c32\nk\no\nand\nmax\ni,j\u2208N |Nij| \u2264\u00b5N \u2225N\u2225F\nn\n.\n(3)\nRecall that ei denotes the i-th standard basis vector so that\n\r\r\reT\ni N\n\r\r\r2 is the Euclidean norm of the i-th row\nof N. The conditions state no entry of N should be too large compared to the norm of the corresponding\nrow in N, and no row of N should be too large compared to \u03c3k. Our bounds will be in terms of a combined\ncoherence parameter \u00b5\u2217satisfying\n\u00b5\u2217\u2265max\b\u00b5(U),\u00b5N\n\t.\n(4)\nWe show that our algorithm outputs a rank k factorization XY T such that with high probability\n\u2225A \u2212XY T \u2225\u2264\u03b5\u2225M\u2225+ (1 + o(1))\u2225N\u2225,\nwhere \u2225\u00b7\u2225denotes the spectral norm. It follows from our argument that we can have the same guaranteee in\nFrobenius norm as well. To achieve the above bound we show that it is su\ufb03cient to have an expected sample\nsize\npn2 = O\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8edn \u00b7 poly(k/\u03b3k)(\u00b5\u2217)2 log\n \u03c31\n\u03c3k\n!\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8edlog2 \u0012n\n\u03b5\n\u0013\n+\n \u2225N\u2225F\n\u03b5\u2225M\u2225F\n!2\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8.\n(5)\nHere, \u03b3k = 1 \u2212\u03c3k+1/\u03c3k indicates the separation between the singular values \u03c3k and \u03c3k+1. The theorem is\na strict generalization of the noise-free case, which we recover by setting N = 0 and hence \u03b3k = 1. The\nformal statement is Theorem 1. Compared to our noise-free bound above, there are two new parameters\nthat enter the sample complexity. The \ufb01rst one is \u03b3k. The second is the term \u2225N\u2225F/\u03b5\u2225M\u2225F. To interpret this\nquantity, suppose that that A has a good low-rank approximation in Frobenius norm: formally, \u2225N\u2225F \u2264\u03b5\u2225A\u2225F\nfor \u03b5 \u22641/2. Then it must also be the case that \u2225N\u2225F/\u03b5 \u22642\u2225M\u2225F. Our algorithm then \ufb01nds a good rank k\napproximation with at most O(poly(k)log(\u03c31/\u03c3k)(\u00b5\u2217)2n) samples assuming \u03b3k = \u2126(1). Thus, in the case that\nA has a good rank k approximation in Frobenius norm and that \u03c3k and \u03c3k+1 are well-separated, our bound\nrecovers the noise-free bound up to a constant factor.\nFor an extended discussion of related work see Section 2.2. We proceed in the next section with a detailed\nproof overview and a description of our notation.\n2\nPreliminaries\nIn this section, we will give an overview of our proof, give a more in-depth survey of previous work, and set\nnotation.\n3\n2.1\nTechnical Overview\nAs the proof of our main theorem is somewhat complex we will begin with an extensive informal overview of\nthe argument. In order to understand our main algorithm, it is necessary to understand the basic Alternating\nMinimization algorithm \ufb01rst.\nAlternating Least Squares.\nGiven a subsample \u2126of entries drawn from an unknown matrix A, Alternating\nMinimization starts from a poor approximation X0Y T\n0 to the target matrix and iteratively re\ufb01nes the\napproximation by \ufb01xing one of the factors and minimizing a certain objective over the other factor. Here,\nX0,Y0 each have k columns where k is the target rank of the factorization. The least squares objective is the\ntypical choice. In this case, at step \u2113we solve the optimization problem\nX\u2113= argmin\nX\nX\n(i,j)\u2208\u2126\nh\nAij \u2212(XY T\n\u2113\u22121)ij\ni2 .\nThis optimization step is then repeated with X\u2113\ufb01xed in order to determine Y\u2113. Since we assume without\nloss of generality that A is symmetric these steps can be combined into one least squares step at each point.\nWhat previous work exploited is that this Alternating Least Squares update can be interpreted as a noisy\npower method update step. That is, Y\u2113= AX\u2113\u22121 + G\u2113for a noise matrix G\u2113. In this view, the convergence of\nthe algorithm can be controlled by \u2225G\u2113\u2225, the spectral norm of the noise matrix. To a rough approximation,\nthis spectral norm initially behaves like O(\u03c31/\u221apn), ignoring factors of k and \u00b5(U). Since we would like to\ndiscover singular vectors corresponding to singular values of magnitude \u03c3k, we need that the error term\nsatis\ufb01es \u2225G\u2113\u2225\u226a\u03c3k: otherwise we cannot rule out that the noise term wipes out any correlation between X\nand the k-th singular vector. In order to achieve this, we would need to set pn = O((\u03c31/\u03c3k)2) and this is where\na quadratic dependence on the condition number arises. This is not the only reason for this dependence:\nAlternating Minimization seems to exhibit a linear convergence rate only once X\u2113is already \u201csomewhat\nclose\u201d to the desired subspace U. This is why typically the algorithm is initialized with a truncated SVD of\nthe matrix P\u2126(A) where P\u2126is the projection onto the subsample \u2126. We again face the issue that \u2225A \u2212P\u2126(A)\u2225\nbehaves roughly like O(\u03c31/\u221apn) and so we run into the same problem here as well.\nA natural idea ot \ufb01x these problems is the so-called de\ufb02ation approach. If it so happens that \u03c31 \u226b\u03c3k, then\nthere must be an r < k such that \u03c31 \u2248\u03c3r \u226b\u03c3k. In this case, we can try to \ufb01rst run Alternating Minimization\nwith r vectors instead of k vectors. This results in a rank r factorization XY T . We then subtract this matrix\no\ufb00of the original matrix and continue with A\u2032 = A \u2212XY T . This approach was in particular suggested by\nJain et al. [JNS13] to eliminate the condition number dependence. Unfortunately, as we will see next, this\napproach runs into serious issues.\nWhy standard de\ufb02ation does not work.\nGiven any algorithm NoisyMC for noisy matrix completion,\nwhose performance depends on the condition number of A, we may hope to use NoisyMC in a black-box way\nto obtain a de\ufb02ation-based algorithm which does not depend on the condition number, as follows. Suppose\nthat we know that the spectrum of A comes in blocks,\n\u03c31 = \u03c32 = ... = \u03c3r1 \u226b\u03c3r1+1 = \u03c3r1+2 = \u00b7\u00b7\u00b7 = \u03c3r2 \u226b\u03c3r2+1 = \u00b7\u00b7\u00b7\nand so on. We could imagine running NoisyMC on P\u2126(A) with target rank r1, to obtain an estimate M(1).\nThen we may run NoisyMC again on P\u2126(A \u2212M(1)) = P\u2126(A) \u2212P\u2126(M(1)) with target rank r2 \u2212r1, to obtain\nM(2), and so on. At the end of the day, we would hope to approximate A \u2248M(1) + M(2) + \u00b7\u00b7\u00b7. Because we are\nfocusing only on a given \u201c\ufb02at\" part of the spectrum at a time, the dependence of NoisyMC on the condition\nnumber should not matter. A major problem with this approach is that the error builds up rather quickly.\nMore precisely, any matrix completion algorithm run on A with target rank r1 must have error on the order\nof \u03c3r1+1 since this is the spectral norm of the \u201cnoise part\u201d that prevents the algorithm from converging\nfurther. Therefore, the matrix A \u2212M(1) might now have 2r1 problematic singular vectors corresponding\nto relatively large singular values, namely those vectors arising from the residuals of the \ufb01rst r1 singular\n4\nvectors, as well as those arising from the approximation error. This multiplicative blow-up makes it di\ufb03cult\nto ensure convergence.\nSoft de\ufb02ation.\nThe above intuition may make a \u201cde\ufb02ation\u201d-based argument seem hopeless. We instead\nuse an approach that looks similar to de\ufb02ation but makes an important departure from it. Intuitively, our\nalgorithm is a single execution of Alternating Minimization. However, we dynamically grow the number\nof vectors that Alternating Minimization maintains until we\u2019ve reached k vectors. At that point we let the\nalgorithm run to convergence. More precisely, the algorithm proceeds in at most k epochs. Each epoch\nroughly proceeds as follows:\nInductive Hypothesis: At the beginning of epoch t, the algorithm has a rank rt\u22121 factorization Xt\u22121Y T\nt\u22121\nthat has converged to within error \u03c3rt\u22121+1/100. At this point, the (rt\u22121 + 1)-th singular vector prevents\nfurther convergence.\nGap \ufb01nding: What can we say about the matrix At = A \u2212Xt\u22121Y T\nt\u22121 at this point? We know that the \ufb01rst\nrt\u22121 singular vectors of A are removed from the top of the spectrum of At. Moreover, each of the\nremaining singular vectors in A is preserverd so long as the corresponding singular value is greater\nthan \u03c3rt\u22121+1/10. This follows from perturbation bounds and we ignore a polynomial loss in k at this\npoint. Importantly, the top of the spectrum of At corresponds is correlated with the next block of\nsingular vectors in A. This motivates the next step in epoch t, which is to compute the top k \u2212rt\u22121\nsingular vectors of At up to an approximation error of \u03c3rt\u22121+1/10. Among these singular vectors we\nnow identify a gap in singular values, that is we look for a number dt such that \u03c3rt\u22121+dt \u2264\u03c3rt\u22121+1/2.\nAlternating Least Squares: At this point we have identi\ufb01ed a new block of dt singular vectors and we\narrange them into an orthognormal matrix Pt \u2208Rn\u00d7dt. We can now argue that the matrix W = [Xt\u22121|Pt]\nis close (in principal angle) to the \ufb01rst rt = rt\u22121 +dt singular vectors of A. What this means is that W is a\ngood initializer for the Alternating Minimization algorithm which we now run on W until it converges\nto a rank rt factorization XtY T\nt that satis\ufb01es the induction hypothesis of the next epoch.\nWe call this algorithm SoftDeflate. The crucial di\ufb00erence to the de\ufb02ation approach is that we always run\nAlternating Minimization on a subsampling P\u2126(A) of the original matrix A. We only ever compute a de\ufb02ated\nmatrix P\u2126(A \u2212XY T ) for the purpose of initializing the next epoch of the algorithm. This prevents the error\naccumulation present in the basic de\ufb02ation approach.\nThis simple description glosses over many details and there are a few challenges to be overcome in\norder to make the idea work. For example, we have not said how to determine the appropriate \u201cgaps\" dt.\nThis requires a little bit of care. Indeed, these gaps might be quite small: if the (additive) gap between\n\u03c3r and \u03c3r+1 is on the order of, say, log2(k)\nk\n\u03c3r, for all r \u2264k, then the condition number of the matrix may be\nsuper-polynomial in k, a price we are not willing to pay. Thus, we need to be able to identify gaps between\n\u03c3r and \u03c3r+1 which are on the order of \u03c3r/k. To do this, we must make sure that our estimates of the singular\nvalues of A \u2212Xt\u22121Y T\nt\u22121 are su\ufb03ciently precise.\nEnsuring Coherence.\nAnother major issue that such an algorithm faces is that of coherence. As mentioned\nabove, incoherence is a standard (and necessary) requirement of matrix completion algorithms, and so in\norder to pursue the strategy outlined above, we need to be sure that the estimates Xt\u22121 stay incoherent. For\nour \ufb01rst \u201crough estimation\" step, our algorithm carefully truncates (entrywise) its estimates, in order to\npreserve the incoherence conditions, without introducing too much error. In particular, we cannot reuse the\ntruncation analysis of Jain et al. [JNS13] which incurred a dependence on the condition number. Coherence\nin the Alternating Minimization step is handled by the algorithm and analysis of [Har13a], upon which we\nbuild. Speci\ufb01cally, Hardt used a form of regularization by noise addition called SmoothQR, as well as an\nextra step which involves taking medians, which ensures that various iterates of Alternating Minimization\nremain incoherent.\n5\n2.2\nFurther Discussion of Related Work\nOur work is most closely related to recent works on convergence bound for Alternating Minimization [Kes12,\nJNS13, GAGG13, Har13b]. Our bounds are in general incomparable. We achieve an exponential improve-\nment in the condition number compared to all previous works, while losing polynomial factors in k. Our\nalgorithm and analysis crucially builds on [Har13a]. In particular we use the version and analysis of Alternat-\ning Minimization derived in that work more or less as a black box. We note that the analyses of Alternating\nMinimization in other previous works would not be su\ufb03ciently strong to be used in our algorithm. In\nparticular, the use of noise addition to ensure coherence already gets rid of one source of the condition\nnumber that all previous papers incur.\nWe are not aware of a fast nuclear norm solver that has theoretical guarantees that do not depend\npolynomially on the condition number. The work of Keshavan et al. [KMO10a, KMO10b] gives another\nalternative to nuclear norm minimization that has theoretical guarantees. However, these bounds have\na quartic dependence on the condition number. We are not aware of any fast nuclear norm solver with\ntheoretical guarantees that do not depend polynomially on the condition number. The work of Keshavan\net al. [KMO10a, KMO10b] gives another alternative to nuclear norm minimization that has theoretical\nguarantees. However, these bounds have a quartic dependence on the condition number. There are a number\nof fast algorithms for matrix completion: for example, based on (Stochastic) Gradient Descent [RR13];\n(Online) Frank-Wolfe [JS10, HK12]; or CoSAMP [LB10]. However, the theoretical guarantees for these\nalgorithms are typically in terms of the error on the observed entries, rather than on the error between\nthe recovered matrix and the unknown matrix itself. For the matrix completion problem, convergence on\nobservations does not imply convergence on the entire matrix.1 Further, these algorithms typically have\npolynomial, rather than logarithmic, dependence on the accuracy parameter \u03b5. Since setting \u03b5 \u2248\u03c3k/\u03c31 is\nrequired in order to accurately recover the \ufb01rst k singular vectors of A, a polynomial dependence in \u03b5 implies\na polynomial dependence on the condition number.\n2.3\nNotation\nFor a matrix A, \u2225A\u2225denotes the spectral norm, and \u2225A\u2225F the Frobenius norm. We will also use \u2225A\u2225\u221e=\nmaxi,j |Ai,j| to mean the entry-wise \u2113\u221enorm. For a vector v, \u2225v\u22252 denotes the \u21132 norm. Throughout,\nC,C0,C1,C2,... will denote absolute constants, and C may change from instance to instance. We also use\nstandard asymptotic notation O(\u00b7) and \u2126(\u00b7), and we occasionally use f \u2272g (resp. \u2273) to mean f = O(g) (resp.\nf = \u2126(g)) to remove notational clutter. Here, the asymptotics are taken as k,n \u2192\u221e. For a matrix X \u2208Rn\u00d7k,\nR(X) denotes the span of the columns of X, and \u03a0X denotes the orthogonal projection onto R(X). Similarly,\n\u03a0X\u22a5denotes the projection onto R(X)\u22a5. For a set random \u2126\u2282[n] \u00d7 [n] and a matrix A \u2208Rn\u00d7n, we de\ufb01ne the\n(normalized) projection operator P\u2126as\nP\u2126(A) :=\nn2\nE|\u2126|\nX\n(i,j)\u2208\u2126\nAi,jeieT\nj\nto the be matrix A, restricted to the entries indexed by \u2126and renormalized.\n2.3.1\nDecomposition of A\nOur algorithm, and its proof, will involve choosing a sequence of integers r1 < \u00b7\u00b7\u00b7 < rt \u2264k, which will mark\nthe signi\ufb01cant \u201cgaps\u201d in the spectrum of A. Given such a sequence, we will decompose A as\nA = M(\u2264t) + Nt = M(1) + M(2) + \u00b7\u00b7\u00b7 + M(t) + Nt,\n(6)\n1 For some matrix recovery problems\u2014in particular, those where the observations obey a rank-restricted isometry property\u2014\nconvergence on the observations is enough to imply convergence on the entire matrix. However, for matrix completion, the relevant\noperator does not satisfy this condition [CR09].\n6\nwhere M(\u2264t) has the spectral decomposition M(\u2264t) = U(\u2264t)\u039b(\u2264t)(U(\u2264t))T and \u039b(\u2264t) contains the eigenvalues\ncorresponding to singular values \u03c31 \u2265\u00b7\u00b7\u00b7 \u2265\u03c3rt. We may decompose M(\u2264t) as the sum of M(j) for j = 1...t,\nwhere each M(j) has the spectral decomposition M(j) = U(j)\u039bj\n\u0010\nU(j)\u0011T corresponding to the singular values\n\u03c3rj\u22121+1,...,\u03c3rj. Similarly, the matrix Nt may be written as Nt = (Vt)\u039b(>t)(Vt)T , and contains the singular\nvalues \u03c3rt+1,...,\u03c3n. Eventually, our algorithm will stop at some maximum t = T , for which rt = k, and we will\nhave A = M + N = M(\u2264T) + NT as in (2). We will use the notation U(\u2264j) to denote the concatenation\nU(\u2264j) = [U(1)|U(2)|\u00b7\u00b7\u00b7|U(j)].\n(7)\nObserve that this is consistent with the de\ufb01nition of U(\u2264t) above. Additionally, for a matrix X \u2208Rn\u00d7rt, we\nwill write X = [X(1)|X(2)|\u00b7\u00b7\u00b7|X(t)], where X(j) contains the rj\u22121 + 1,...,rj columns of X, and we will write\nX(\u2264j) = [X(1)|X(2)|\u00b7\u00b7\u00b7|X(j)]. Occasionally, we will wish to use notation like U(\u2264r) to denote the \ufb01rst r columns\n(rather than the \ufb01rst rr columns). This will be pointed out when it occurs.\nFor an index r \u2264n, we quantify the gap between \u03c3r and \u03c3r+1 by\n\u03b3r := 1 \u2212\u03c3r+1\n\u03c3r\n.\n(8)\nand we will de\ufb01ne\n\u03b3 := min\n\u001a\n\u03b3r : r \u2208[n],\u03b3r \u22651\n4k\n\u001b\n.\n(9)\nBy de\ufb01nition, we always have \u03b3 \u22651/4k; for some matrices A, it may be much larger, and this will lead to\nimproved bounds. Our analysis will also depend on the \u201c\ufb01nal\" gap quanti\ufb01ed by \u03b3k, whether or not it is\nlarger than 1/4k. To this end, we de\ufb01ne\n\u03b3\u2217:= min{\u03b3,\u03b3k}.\n(10)\n3\nAlgorithms and Results\nIn Algorithm 1 we present our main algorithm SoftDeflate. It uses several subroutines that are presented\nin Section 3.1.\nRemark 1. In the Matrix Completion literature, the most common assumption on the distribution of the set \u2126of\nobserved entries is that each index (i,j) is included independently with some probability p. Call this distribution\nD(p). In order for our results to be comparable with existing results, this is the model we adopt as well. However,\nfor our analysis, it is much more convenient to imagine that \u2126is the union of several subsets \u2126t, so that the \u2126t\nthemselves follow the distribution D(pt) (for some probability pt, where P\nt pt = p), and so that all of the \u2126t are\nindependent. Algorithmically, the easiest thing to do to obtain subsets \u2126t from \u2126is to partition \u2126into random\nsubsets of equal size. However, if we do this, the subsets \u2126t will not follow the right distribution; in particular they\nwill not be independent. For theoretical completeness, we show in Appendix A (Algorithm 6) how to split up the\nset \u2126in the correct way. More precisely, given pt and p so that P\nt pt = p, we show how to break \u2126\u223cD(p) into\n(possibly overlapping) subsets \u2126t, so that the \u2126t are independent and each \u2126t \u223cD(pt).\n3.1\nOverview of Subroutines\nSoftDeflate uses a number of subroutines that we outline here before explicitly presenting them:\n\u2022 S-M-AltLS (Algorithm 2) is the main Alternating Least Squares procedure that was given and analyzed\nin [Har13a]. We use this algorithm and its analysis. S-M-AltLS by itself has a quadratic dependence\non the condition number which is why we can only use it as a subroutine.\n7\nAlgorithm 1: SoftDeflate: Approximates an approximately low-rank matrix from a few entries.\nInput: Target dimension k; Observed set of indices \u2126\u2286[n] \u00d7 [n] of an unknown symmetric matrix\nA \u2208Rn\u00d7n with entries P\u2126(A); Accuracy parameter \u03b5; Noise parameter \u2206with \u2225A \u2212Ak\u2225F \u2264\u2206;\nCoherence parameter \u00b5\u2217, satisfying (4), and a parameter \u00b50; Probabilities p0 and pt,p\u2032\nt for\nt = 1,...,k; Number of iterations Lt \u2208N, for t = 1,...,k runs of S-M-AltLS, and a parameter\nsmax \u2208N for S-M-AltLS, and a number of iterations L for runs of SubsIt.\n1 Let p = P\nt(pt + p\u2032\nt).\n2 Break \u2126randomly into 2k + 1 sets, \u21260 and \u21261,\u2126\u2032\n1,...,\u2126k,\u2126\u2032\nk, so that E|\u2126t| = pt\np |\u2126| and E|\u2126\u2032\nt| = p\u2032\nt\np |\u2126|\n(See Remark 1).\n3 s0 \u2190\n\r\r\rP\u21260(A)\n\r\r\r\n// Estimate \u03c31(A)\n4 Initialize X0 = Y0 = 0, r0 = 0\n5 for t = 1...k do\n6\n\u03c4t \u2190\u00b5\u2217\nnpt (2kst\u22121 + \u2206)\n7\nTt \u2190Truncate\n\u0010\nP\u2126t(A) \u2212P\u2126t(Xt\u22121Y T\nt\u22121),\u03c4t\n\u0011\n// Truncate(M,c) truncates M so that |Mij| \u2264c\n8\ne\nUt,\u20d7e\u03c3 \u2190SubsIt(Tt,k \u2212rt\u22121,L)\n// Estimate the top k \u2212rt\u22121 spectrum of Tt.\n9\nIf e\u03c31 < 10\u03b5s0 then return Xt\u22121,Yt\u22121\n10\ndt \u2190min\nn\ni \u2264k \u2212rt\u22121 : \u03c3i+1(eTt) \u2264\n\u0010\n1 \u22121\n4k\n\u0011\n\u03c3i(eTt)\no\n\u222a{k \u2212rr\u22121}\n11\nrt \u2190rt\u22121 + dt\n// rt is an estimate of the next \u201cgap\" in the spectrum of A\n12\nst \u2190e\u03c3dt\n// st is an estimate of \u03c3rt(A)\n13\ne\nQt \u2190\n\u0010e\nUt\n\u0011(\u2264dt)\n// Keep the \ufb01rst dt columns of e\nUt\n14\nQt \u2190Truncate\n \ne\nQtB,8\nq\n\u00b5\u2217log(n)\nn\n!\n// where B \u2208Rn\u00d7n is a random orthonormal matrix.\n15\nWt \u2190QR([Xt\u22121 | Qt])\n// Wt is a rough estimate of U(\u2264t)\n16\n\u00b5t \u2190\n\u0010\u221a\u00b50 + (t \u22121)\np\n\u00b5\u2217k\n\u00112\n17\n(Xt,Yt) \u2190S-M-AltLS(A,\u2126\u2032\nt,R0 = Wt,L = Lt,smax = smax,k = rt,\u03b6 = \u03b5s0k\u22125,\u00b5 = \u00b5t)\n// Xt is a good\nestimate of U(\u2264t)\n18\nIf rt \u2265k then return(Xt,Yt)\n19 end\nOutput: Pair of matrices (X,Y).\n8\n\u2022 SmoothQR (Algorithm 3) is a subroutine of S-M-AltLS which is used to control the coherence of\nintermediate solutions arising in S-M-AltLS. Again, we reuse the analysis of SmoothQR from [Har13a].\nSmoothQR orthonormalizes its input matrix after adding a Gaussian noise matrix. This step allows\ntight control of the coherence of the resulting matrix. We defer the description of SmoothQR to Section\n6 where we need it for the \ufb01rst time.\n\u2022 SubsIt is a standard textbook version of the Subspace Iteration algorithm (Power Method). We use this\nalgorithm as a fast way to approximate the top singular vectors of a matrix arising in SoftDeflate. We\nuse only standard properties of SubsIt in our analysis. For this reason we defer the description and\nanalysis of SubsIt to Section B.3.\nAlgorithm 2: S-M-AltLS(P\u2126(A),\u2126,R0,L,smax,k,\u03b6,\u00b5) (Smoothed-Median-Alternating Least Squares)\nInput: Number of iterations L \u2208N, parameter smax \u2208N, target dimension k, observed set of indices\n\u2126\u2286[n] \u00d7 [n] of an unknown symmetric matrix A \u2208Rn\u00d7n with entries P\u2126(A), initial orthonormal\nmatrix R0 \u2208Rn\u00d7k, and parameters \u03b6,\u00b5\n1 Break \u2126randomly into sets \u21261,...,\u2126L with equal expected sizes. (See Remark 1).\n2 for \u2113= 1 to L do\n3\nBreak \u2126\u2113randomly into subsets \u2126(1)\n\u2113,...,\u2126(T)\n\u2113\nwith equal expected sizes.\n4\nfor s = 1 to smax do\n5\nS(s)\n\u2113\n\u2190argminS\u2208Rn\u00d7k\n\r\r\rP\u2126\u2113(A \u2212R\u2113\u22121ST )\n\r\r\r2\nF\n6\nend\n7\nS\u2113\u2190medians(S(s)\n\u2113)\n// The median is applied entry-wise.\n8\nR\u2113\u2190SmoothQR(S\u2113,\u03b6,\u00b5)\n9 end\nOutput: Pair of matrices (RL\u22121,SL)\n3.2\nStatement of the main theorem\nOur main theorem is that, when the number of samples is poly(k)n, SoftDeflate returns a good estimate of\nA, with at most logarithmic dependence on the condition number.\nTheorem 1. There is a constant C so that the following holds. Let A \u2208Rn\u00d7n, k \u2264n, and write A = M + N, where\nM is the best rank-k approximation to A. Let \u03b3,\u03b3\u2217be as in (9), (10). Choose parameters for Algorithm 1 so that\n\u03b5 > 0 and\n\u2022 \u00b5\u2217satis\ufb01es (4) and \u00b50 \u2265\nC\n(\u03b3\u2217)2\n\u0012\n\u00b5\u2217\n\u0012\nk +\n\u0010 k4\u2206\n\u03b5\u03c31\n\u00112\u0013\n+ log(n)\n\u0013\n\u2022 \u2206\u2265\u2225N\u2225F\n\u2022 Lt \u2265C\n\u03b3\u2217log\n\u0012\nk\u03c3rt\n\u03c3rt+1+\u03b5\u03c31\n\u0013\n, and L \u2265Ck7/2 log(n)\n\u2022 smax \u2265C log(n).\nThere is a choice of pt,p\u2032\nt (given in the proof below) so that\np =\nX\npt +\nX\np\u2032\nt \u2264C\nk9\n(\u03b3\u2217)3n log\n \nk \u00b7\n\u03c31\n\u03c3k + \u03b5\u03c31\n!\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed1 +\n \n\u2206\n\u03b5\u2225M\u2225\n!2\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8(\u00b50 + \u00b5\u2217k log(n))log2(n)\nso that the following holds.\n9\nSuppose that each element of [n] \u00d7 [n] is included in \u2126independently with probability p. Then the matrices\nX,Y returned by SoftDeflate satisfy with probability at least 1 \u22121/n,\n\r\r\rA \u2212XY T \r\r\r \u2264(1 + o(1))\u2225N\u2225+ \u03b5\u2225M\u2225.\nRemark 2 (Error guarantee). The guarantee of\n\r\r\rA \u2212XY T \r\r\r \u2264(1 + o(1))\u2225N\u2225+ \u03b5\u2225M\u2225is what naturally falls out of\nour analysis: the natural value for the o(1) term is polynomially small in k. It is not hard to see in the proof that we\nmay make this term as small as we like, say, (1 + \u03b1)\u2225N\u2225, by paying a logarithmic penalty log(1/\u03b1) in the choice of\np. It is also not hard to see that we may have a similar conclusion for the Frobenius norm.\nRemark 3 (Obtaining the parameters). As written, then algorithm requires the user to know several parameters\nwhich depend on the unknown matrix A. For some parameters, these requirements are innocuous. For example,\nto obtain p\u2032\nt or Lt (whose values are given in Section 4.1), the user is required to have a bound on log(\u03c3rt/\u03c3rt+1).\nClearly, a bound on the condition number of A will su\ufb03ce, but more importantly, the estimates st which appear in\nAlgorithm 1 may be used as proxies for \u03c3rt, and so the parameters p\u2032\nt can actually be determined relatively precisely\non the \ufb02y. For other parameters, like \u00b5\u2217or k, we assume that the user has a good estimate from other sources. While\nthis is standard in the Matrix Completion literature, we acknowledge that these values may be di\ufb03cult to come by.\n3.3\nRunning Time\nThe running time of SoftDeflate is linear in n, polynomial in k, and logarithmic in the condition number\n\u03c31/\u03c3k of A. Indeed, the outer loop performs at most k epochs, and the nontrivial operations in each epoch\nare S-M-AltLS, QR, and SubsIt. All of the other operations (truncation, concatenation) are done on matrices\nwhich are either n \u00d7 k (requiring at most nk operations) or on the subsampled matrices P\u2126t(A), requiring on\nthe order of pn2 operations.\nRunning SubsIt requires L = O(k7/2 log(n)) iterations; each iteration includes multiplication by a sparse\nmatrix, followed by QR. The matrix multiplication takes time on the order of\nptn2 = npoly(k) log(n)\n \n1 + \u2206\n\u03b5\u03c31\n!\n,\nthe number of nonzero entries of A, and QR takes time O(k2n). Each time S-M-AltLS is run, it takes Lt\niterations, and we will show (following the analysis of [Har13a]) that it requires poly(k)nlog(n)log(n/\u03b5)\noperations per iteration. Thus, given the choice of Lt in Theorem 1, the total running time of SoftDeflate\non the order of\ne\nO\n \nn \u00b7 poly(k) \u00b7\n \n1 + \u2206\n\u03b5\u03c31\n!\n\u00b7 log\n \n\u03c31\n\u03c3k + \u03b5\u03c31\n!!\n,\nwhere the e\nO hides logarithmic factors in n.\n4\nProof of Main Theorem\nIn this section, we prove Theorem 1. The proof proceeds by maintaining a few inductive hypotheses, given\nbelow, at each epoch. When the algorithm terminates, we will show that the fact that these hypotheses still\nhold imply the desired results. Suppose that at the beginning of step t of Algorithm 1, we have identi\ufb01ed\nsome indices r1,...,rt\u22121, and recovered estimates Xt\u22121,Yt\u22121 which capture the singular values \u03c31,...,\u03c3rt\u22121 and\nthe corresponding singular vectors. The goals of the current step of Algorithm 1 are then to (a) identify the\nnext index rt which exhibits a large \u201cgap\" in the spectrum, and (b) estimate the singular values \u03c3rt\u22121+1,...,\u03c3rt\nand the corresponding singular vectors.\nLetting rt be the index obtained by Algorithm 1, we will decompose A = M(<t) + Nt\u22121 = M(\u2264t) + Nt as in\n(6). To help keep the notation straight, we include a diagram below, which indicates which singular values\nof A are included in which matrix.\n10\n0\nrt\u22121\nrt\nn\nM(t)\nNt\nM(\u2264t)\nM(<t)\nNt\u22121\nFollowing Remark 1, we treat the \u2126t and \u2126\u2032\nt as independent random sets, with each entry included\nwith probability pt or p\u2032\nt, respectively. We will keep track of the principal angles between the subspaces\nR(((\u2264j)Xt\u22121)) and R(((\u2264j)U)). More precisely, for matrices A,B \u2208Rn\u00d7rj with orthogonal columns, we de\ufb01ne\nsin\u03b8(A,B) :=\n\r\r\r(A\u22a5)T B\n\r\r\r.\nWe will maintain the following inductive hypotheses. At the beginning of epoch t of SoftDeflate, we\nassert\n\u03c3rjsin\u03b8(Xt\u22121\n(\u2264j),U(\u2264j)) \u22641\nk4\n\u0010\n\u03c3rt\u22121+1 + \u03b5\u2225M\u2225\n\u0011\n\u2200j \u2264t \u22121\n(H1)\nand\n\r\r\rM(<t) \u2212Xt\u22121Y T\nt\u22121\n\r\r\r \u2264\u03c3rt\u22121+1 + \u03b5\u2225M\u2225\nC0k3\n(H2)\nfor some su\ufb03ciently large constant C0 determined by the proof. We also maintain that the current estimate\nXt\u22121 is incoherent:\nmax\ni\u2208[n]\n\r\r\reT\ni Xt\u22121\n\r\r\r2 \u2264\nr\nk\nn\n\u0012\u221a\u00b50(1 + C5/k)t\u22121 + (t \u22121)16\nq\n\u00b5\u2217log(n)\n\u0013\n=:\nr\nk\u00b5t\u22121\nn\n(H3)\nfor a constant C5. Above, equation (H3) de\ufb01nes \u00b5t\u22121. Observe that when t = 1, everything in sight is zero\nand the hypotheses (H1), (H2),(H3) are satis\ufb01ed. Finally, we assume that the estimate st\u22121 of \u03c3rt\u22121+1 is good.\n1\n2\u03c3rt\u22121+1 \u2264st\u22121 \u22642\u03c3rt\u22121+1\n(H4)\nThe base case for (H4) is handled by the choice of s0 in Algorithm 1. Indeed, Lemma 18 in the appendix\nimplies that, with probability 1 \u22121/poly(n),\n\r\r\rA \u2212P\u21260(A)\n\r\r\r \u2272\nv\nt\nmaxi\n\r\r\reT\ni A\n\r\r\r2\n2 log(n)\np0\n+ \u2225A\u2225\u221elog(n)\np0\n\u2264\ns\n\u00b5\u2217(\n\u221a\nk\u03c31 + \u2206)2 log(n)\nnp0\n+ \u00b5\u2217(k\u03c31 + \u2206)log(n)\nnp0\n\u2264\n\u0012\u03c31\n2\n\u0013\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\nv\nt\n4\u00b5\u2217\u0010\u221a\nk + (\u2206/\u03c31)\n\u00112 log(n)\nnp0\n+ 2\u00b5\u2217(k + \u2206/\u03c31)log(n)\nnp0\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\n,\nwhere we used the incoherence bounds (33) and (34) in the appendix to bound \u2225A\u2225\u221eand\n\r\r\reT\ni A\n\r\r\r2. Thus, as\nlong as\np0 \u2273\n\u00b5\u2217log(n)\n\u0010\u221a\nk + \u2206\n\u03c31\n\u00112\nn\n,\n(11)\n11\nthen\n1\n2\u03c31 \u2264\n\r\r\rP\u21260(A)\n\r\r\r \u22642\u03c31.\nand so (H4) is satis\ufb01ed.\nNow, suppose that the inductive hypotheses (H1), (H2), (H3), and (H4) hold. We break up the inner loop\nof SoftDeflate into two main steps. In the \ufb01rst step, lines 6 to 15 in Algorithm 1, the goal is to obtain an\nestimate rt of the next \u201cgap,\" as well as an estimate Wt of the subspace U(\u2264t). We analyze this step in Lemma\n2 below.\nLemma 2. There exists a constants C,C1 so that the following holds. Suppose that\npt \u2265\nC(\u00b5\u2217)2 log(n)\n\u0012\nk2 +\n\u0010\n\u2206\n\u03b5\u2225M\u2225\n\u00112\u0013\nn\u03b52\n0\n,\nwhere \u03b50 \u2264\n1\n4C1k5/2 . Further assume that the inductive hypotheses (H1), (H2), (H3), and (H4) hold. Then with\nprobability at least 1 \u22121/n2 over the choice of \u2126t and the randomness in SubsIt, one of the following statements\nmust hold:\n(a) Algorithm 1 terminates at line 9, and returns Xt\u22121,Yt\u22121 so that\n\r\r\rA \u2212Xt\u22121Y T\nt\u22121\n\r\r\r \u2264C\u03b5\u2225M\u2225; or\n(b) Algorithm 1 does not terminate at line 9, and the following conditions hold:\n\u2022 The error level \u03b5 has not yet been reached:\n\u03b5\u2225M\u2225\u2264\u03c3rt\u22121+1 .\n(12)\n\u2022 The index rt recovered obeys\n\u03c3rt+1\n\u03c3rt\n\u22641 \u2212\u03b3\nand\n\u03c3rt\u22121+1\n\u03c3rt\n\u2264e.\n(13)\n\u2022 The matrix Wt has orthonormal columns, and satis\ufb01es\nsin\u03b8(Wt,U(\u2264t)) \u22641\nk\nand\nmax\ni\n\r\r\reT\ni Wt\n\r\r\r2 \u2264\nr\nk\u00b5t\nn ,\n(14)\nwhere \u00b5t is de\ufb01ned as in (H3).\n\u2022 The estimate st satis\ufb01es (H4).\nThe proof of Lemma 2 is given in Section 5. In the second part of SoftDeflate, lines 16 to 17 in Algorithm\n1, we run S-M-AltLS, initialized with the subspace Wt returned by the \ufb01rst part of the algorithm. Lemma 3\nbelow shows that S-M-AltLS improves the estimate Wt to the desired accuracy, so that we may move on to\nthe next iteration of SoftDeflate.\nLemma 3. Assume that the conclusion (b) of Lemma 2 holds, as well as the inductive hypotheses (H1), (H2), (H3),\nand (H4) . There is a constant C so that the following holds. Let \u03b3\u2217be as in (10). Suppose that\n\u00b5t \u2265\nC\n(\u03b3\u2217)2\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\u00b5\u2217\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8edk +\n k4\u2206\n\u03b5\u03c31\n!2\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8+ log(n)\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\nand\np\u2032\nt \u2265\nCLtsmax \u00b7 k9\u00b5t log(n)\n\u0012\nk +\n\u0010\n\u2206\n\u03b5\u2225M\u2225\n\u00112\u0013\n(\u03b3\u2217)2n\nwith\nLt \u2265C\n\u03b3\u2217log\n \nk\u03c3rt\n\u03c3rt+1 + \u03b5\u2225M\u2225\n!\nand\nsmax \u2265C log(n).\nThen after Lt steps of S-M-AltLS with the initial matrix Wt, and parameters \u00b5t,\u03b5, the following hold with\nprobability at least 1 \u22121/n2, over the choice of \u2126\u2032\nt.\n12\n\u2022 The inductive hypothesis (H1) holds for the next round:\n\u2200j \u2264t,\u03c3rjsin\u03b8(X(\u2264j)\nt\n,U(\u2264j)) \u2264\u03c3rt+1 + \u03b5\u2225M\u2225\nk4\n.\n\u2022 The inductive hypothesis (H2) holds for the next round:\n\r\r\rM(\u2264t) \u2212XtY T\nt\n\r\r\r \u2264\u03c3rt+1 + \u03b5\u2225M\u2225\nC0k3\n.\n\u2022 The inductive hypothesis (H3) holds for the next round: \u00b5(Xt) \u2264\u00b5t.\nThe proof of Lemma 3 is addressed in Section 6.\n4.1\nPutting it together\nTheorem 1 now follows using 2 and 3. First, we choose \u00b50 as in the statement of Theorem 1. Because \u00b5t \u2265\u00b50\nfor all t = 1,...,T , this implies that \u00b5t satis\ufb01es the requirements of Lemma 3. Then, the hypotheses of Lemma\n3 are implied by the conclusions of the favorable case of Lemma 2. Now, a union bound over at most k\nepochs of SoftDeflate ensures that with probability at least 1 \u22122k/n2 \u22651 \u22121/n, the conclusions of both\nlemmas hold every round that their hypotheses hold.\nIf SoftDeflate terminates with the guarantees (a) of Lemma 2, then\n\r\r\rA \u2212XT Y T\nT\n\r\r\r \u2264C\u03b5\u2225M\u2225. On the other\nhand, if (b) holds, then Lemma 2 implies (H4) and the hypotheses of Lemma 3, and then Lemma 3 implies\nthat with probability 1 \u22121/n2, the remaining inductive hypotheses (H1), (H2), and (H3) for the next round.\nThus, if the situation (a) above never occurs, then the hypotheses of Lemma 3 hold until SoftDeflate\nterminates because rt = k. In this case, Lemma 3 implies that\n\r\r\rA \u2212XT Y T\nT\n\r\r\r \u2264\n\r\r\rMT \u2212XT Y T\nT\n\r\r\r + \u2225NT \u2225\u2264\u03c3k+1 + \u03b5\u2225M\u2225\nC0k3\n+ \u03c3k+1 .\nIn either case,\n\r\r\rA \u2212XtY T\nT\n\r\r\r \u2264\u2225N\u2225\n \n1 +\n1\nC0k3\n!\n+ C\u03b5\u2225M\u2225.\nFinally, we tally up the number of samples. The base case (11) required\np0 \u2273\n\u00b5\u2217log(n)\n\u0010\u221a\nk +\n\u2206\n\u2225M\u2225\n\u00112\nn\n.\nLemma 2 required\npt \u2273\n(\u00b5\u2217)2k5 log(n)\n\u0012\nk2 +\n\u0010\n\u2206\n\u03b5\u2225M\u2225\n\u00112\u0013\nn\n.\nFor Lemma 3, we required, for a su\ufb03ciently large constant C,\np\u2032\nt \u2273\nCLtsmax \u00b7 k9\u00b5t log(n)\n\u0012\nk +\n\u0010\n\u2206\n\u03b5\u2225M\u2225\n\u00112\u0013\n(\u03b3\u2217)2n\nwith\nLt \u2265C\n\u03b3\u2217log\n \nk\u03c3rt\n\u03c3rt+1 + \u03b5\u2225M\u2225\n!\nand\nsmax \u2265C log(n).\nFrom the de\ufb01nition of \u00b5t (in (H3)), we may bound \u00b5t for all t \u2264k by\n\u00b5t \u2264C\u00b50 + k log(n)\u00b5\u2217\nfor some constant C. Summing over t gives the result.\n13\n5\nProof of Lemma 2\nIn this section, we prove Lemma 2, which shows that either Algorithm 1 hits the precision parameter \u03b5\nand returns, or else produces an estimate Wt for U(\u2264t) that is close enough to run S-M-AltLS on. There are\nseveral rounds of approximations between the beginning of iteration t and the output Wt. For the reader\u2019s\nconvenience, we include an informal synopsis of the notation in Figure 1. We will \ufb01rst argue that the matrix\nNt\u22121 \u2248A \u2212Xt\u22121YT\nt\u22121\nTt\ne\nUt,(e\u03c31,..., e\u03c3k\u2212rt )\n\u03c31(Tt),...,\u03c3k\u2212rt (Tt)\ne\nQt\nQt\nQt\nWt\nReturn!\nIf e\u03c31 is small\ntruncate,\nsubsample\nSubsIt\nUse e\n\u03c3i to\nestimate the\nnext gap rt, and\ntake the top dt\nvectors of e\nUt\nrotate,\ntruncate\nQR ([Xt\u22121|Qt])\nactual spectrum\ntake the top dt\nsingular vectors\nFigure 1: Schematic of the \ufb01rst part of SoftDeflate. The top line indicates how Wt is formed from the matrix Tt. We\nwill show that Qt approximates U(t), the next chunk of singular vectors in Nt\u22121, and this will imply by induction that\nWt approximates U(\u2264t). The second line in the \ufb01gure indicates some notation which will be useful for our analysis, but\nwhich is not used by the algorithm.\nNt\u22121 is close to the truncated, subsampled, noisy estimate Tt.\nLemma 4. Let Tt be as in Algorithm 1, and choose any constant C1 > 0. Suppose that the inductive hypotheses\n(H2) and (H4) hold. Suppose that pt is as in the statement of Lemma 2. Then, for a su\ufb03ciently large choice of C0\nin the hypothesis (H2) (depending only on C1), with probability at least 1 \u22121/n2,\n\u2225Tt \u2212Nt\u22121\u2225\u2264\u03c3rt\u22121+1 + \u03b5\u2225M\u2225\n2C1k5/2\n.\nProof. Write\nA \u2212Xt\u22121Y T\nt\u22121 = Nt\u22121 +\n\u0010\nM(<t) \u2212Xt\u22121Y T\nt\u22121\n\u0011\n=: Nt\u22121 + Et\u22121 =: \u0083\nNt\u22121.\nLet T denote the Truncate operator. As in Algorithm 1, consider\nTt = T (P\u2126t(\u0083\nNt\u22121),\u03c4t) = P\u2126t\n\u0010\nT (\u0083\nNt\u22121,pt\u03c4t)\n\u0011\n,\nwhere as in Line 6, \u03c4t = \u00b5\u2217\nnpt (2kst\u22121 + \u2206). Above, use used that the sampling operation P\u2126t and the truncate\noperator T commute after adjusting for the normalization factor p\u22121\nt\nin the de\ufb01nition of P\u2126t. Because Nt\u22121 is\nincoherent, each of its entries is small. More precisely, by the incoherence implication (35) along with the\nguarantee (H4) on st\u22121, we have\n\u2225Nt\u22121\u2225\u221e\u2264\u00b5\u2217\nn\n\u0010\nk\u03c3rt\u22121+1 + \u2206\n\u0011\n\u2264\u00b5\u2217\nn (2kst\u22121 + \u2206) = pt\u03c4t.\nThus, each entry of \u0083\nNt\u22121 = Nt\u22121 + Et\u22121 is the sum of something smaller than pt\u03c4t from Nt\u22121, and an error\nterm from Et\u22121, and so truncating entrywise to pt\u03c4t can only remove mass from the contribution of Et\u22121.\nThis implies that for all i,j,\n\f\f\f\u0083\nNt\u22121 \u2212T (\u0083\nNt\u22121,pt\u03c4t)\n\f\f\fi,j \u2264|Et\u22121|i,j ,\nand so using (H2),\n\r\r\r\u0083\nNt\u22121 \u2212T (\u0083\nNt\u22121,pt\u03c4t)\n\r\r\rF \u2264\u2225Et\u22121\u2225F \u2264\n\u221a\n2k (\u03c3rt\u22121+1 + \u03b5\u2225M\u2225)\nC0k3\n=\n\u221a\n2(\u03c3rt\u22121+1 + \u03b5\u2225M\u2225)\nC0k5/2\n.\n(15)\n14\nAbove, we used the fact that Et\u22121 = M(<t) \u2212Xt\u22121Y T\nt\u22121 has rank at most 2k, and hence \u2225Et\u22121\u2225F \u2264\n\u221a\n2k \u2225Et\u22121\u2225.\nNext, we bound the di\ufb00erence between Tt and T (\u0083\nNt\u22121,pt\u03c4t). Lemma 18 in the appendix bounds the e\ufb00ect of\nsubsampling in operator norm. It implies that with probability 1 \u22121/poly(n) over the choice of \u2126t, we have\n\r\r\rT (\u0083\nNt\u22121,pt\u03c4t) \u2212Tt\n\r\r\r =\n\r\r\rT (\u0083\nNt\u22121,pt\u03c4t) \u2212P\u2126(T (\u0083\nNt\u22121,pt\u03c4t))\n\r\r\r\n\u2272\nv\nt\nmaxi\n\r\r\reT\ni T (\u0083\nNt\u22121,pt\u03c4t)\n\r\r\r\n2\n2 log(n)\npt\n+\n\r\r\rT (\u0083\nNt\u22121,pt\u03c4t)\n\r\r\r\u221elog(n)\npt\n\u2264\ns\nn(pt\u03c4t)2 log(n)\npt\n+ (pt\u03c4t)log(n)\npt\n\u2264\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\ns\nlog(n)\nptn\n+ log(n)\nptn\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\n\u0010\n\u00b5\u2217(4k\u03c3rt\u22121+1 + \u2206)\n\u0011\n,\nusing the fact that\npt\u03c4t = \u00b5\u2217\nn (2kst\u22121 + \u2206) \u2264\u00b5\u2217\nn\n\u0010\n4k\u03c3rt\u22121+1 + \u2206\n\u0011\nby (H4). Thus, our choice of pt implies that\n\r\r\rT (\u0083\nNt\u22121,p\u03c4t) \u2212Tt\n\r\r\r \u2264\u03b50\n\u0010\n\u03c3rt\u22121+1 + \u03b5\u2225M\u2225\n\u0011\n.\n(16)\nTogether with (15) we conclude that\n\u2225Nt\u22121 \u2212Tt\u2225\u2264\n\r\r\rNt\u22121 \u2212\u0083\nNt\u22121\n\r\r\r+\n\r\r\r\u0083\nNt\u22121 \u2212T (\u0083\nNt\u22121,p\u03c4t)\n\r\r\r+\n\r\r\rT (\u0083\nNt\u22121,p\u03c4t) \u2212Tt\n\r\r\r \u2264\u03b50\n\u0010\n\u03c3rt\u22121+1 + \u03b5\u2225M\u2225\n\u0011\n+2\n\u221a\n2(\u03c3rt\u22121+1 + \u03b5\u2225M\u2225)\nC0k5/2\n.\nThe choice of \u03b50 and a su\ufb03cient choice of C0 (depending only on C1) completes the proof.\nSuppose for the rest of the proof that the conclusion of Lemma 4 holds. The \ufb01rst thing SoftDeflate does\nafter computing Tt is to obtain estimates e\nUt and e\u03c31,..., e\u03c3k\u2212rt for the top singular values and vectors of Tt.\nThese estimates are recovered by SubsIt in Line 8 of Algorithm 1. We \ufb01rst wish to show that the estimated\nsingular values are close to the actual singular values of Tt. For this, we will invoke Theorem 16 in the\nappendix, which implies that as long as the number of iterations L of SubsIt satis\ufb01es\nL \u2265Ck7/2 log(n),\nfor a su\ufb03ciently large constant C, then with probability 1 \u22121/poly(n), we have\n|\u03c3j(Tt) \u2212e\u03c3j| \u2264\n\u2225Tt\u2225\n2C1k5/2\nfor all j.\n(17)\nAbove, we took a union bound over all j. Again, we condition on this event occuring. Thus, with our\nchoice of L, the estimates e\u03c3j are indeed close to the singular values \u03c3j(Tt), which by Lemma 4 are with high\nprobability close to the singular values \u03c3rt\u22121+j of Nt\u22121 itself.\nBefore we consider the next step (to e\nQt) in Figure 1, consider the case when Algorithm 1 returns at line 9.\nThen e\u03c31 \u226410\u03b5s0 \u226420\u03b5\u03c31, and so using (17) above we \ufb01nd that \u2225Tt\u2225\u226421\u03b5\u03c31. Then by Lemma 4,\n\u03c3rt\u22121+1 = \u2225Nt\u22121\u2225\u2264\u2225Tt\u2225+ \u2225Nt\u22121 \u2212Tt\u2225\u226421\u03b5\u03c31 + \u03c3rt\u22121+1 + \u03b5\u03c31\n2C1k5/2\n.\n15\nThus, for su\ufb03ciently large C1, we conclude \u03c3rt\u22121+1 \u226422\u03b5\u03c31. In this case, we are done:\n\r\r\rA \u2212Xt\u22121Y T\nt\u22121\n\r\r\r \u2264\n\r\r\rM(<t) \u2212Xt\u22121Y T\nt\u22121\n\r\r\r + \u2225Nt\u22121\u2225\n\u2264\u03c3rt\u22121+1 + \u03b5\u2225M\u2225\nC0k3\n+ \u03c3rt\u22121+1\n\u226423\u03b5\u03c31.\nand case (a) of the conclusion holds, as long as Lemma 4 does.\nOn the other hand, suppose that Algorithm 1 does not return at line 9 (and continue to assume that\nLemma 4 holds). As above, (17) implies that since e\u03c31 \u226510\u03b5, we must have\n\u2225Tt\u2225\u2265\n5\u03b5\u03c31\n1 \u2212\n1\n2C1k5/2\n.\nThen by Lemma 4,\n\u03c3rt\u22121+1 \u2265\n5\u03b5\u03c31\n1 \u2212\n1\n2C1k5/2\n\u2212\u03c3rt\u22121+1 + \u03b5\u03c31\n2C1k5/2\n,\nwhich implies that\n\u03b5\u03c31 < \u03c3rt\u22121+1.\n(18)\nThis establishes the conclusion (12). With (18), Lemma 4 and (17) together imply that\n\u2200r \u2264n,\n|\u03c3r \u2212e\u03c3r\u2212rt\u22121| \u2264\u2225Nt\u22121 \u2212Tt\u2225+ |\u03c3r\u2212rt\u22121(Tt) \u2212e\u03c3r\u2212rt\u22121| \u2264\u03c3rt\u22121+1\nC1k5/2 .\n(19)\nAbove, we use Lemma 13 in the appendix in the \ufb01rst inequality.\nWe now show that the choice of dt in Line 10 of Algorithm 1 accurately identi\ufb01es a \u201cgap\" in the spectrum.\nLemma 5. Suppose that the hypotheses and conclusions of Lemma 4 hold, and in particular that (19) holds. Then\nthe value rt = rt\u22121 + dt obtained in Line 10 of Algorithm 1 satis\ufb01es:\n\u03c3rt+1\n\u03c3rt\n\u22641 \u2212\u03b3\nand\n\u03c3rt\u22121+1\n\u03c3rt\n\u2264e.\nProof. Let d\u2217\nt be the \u201ccorrect\" choice of dt; that is, d\u2217\nt be the smallest positive integer d \u2264k \u2212rt\u22121 so that\n1 \u2212\u03c3rt\u22121+d+1\n\u03c3rt\u22121+d\n\u22651 \u22121\nk ,\nor let d\u2217\nt = d \u2212rt\u22121 if such an index does not exist. Write r\u2217\nt = rt\u22121 +d\u2217\nt . By de\ufb01nition, because d\u2217\nt is the smallest\nsuch d (or smaller than any such d in the case that r\u2217\nt = k), we have\n\u03c3rt\u22121+1\n\u03c3r\u2217\nt\n\u2264\n\u0012\n1 + 1\nk\n\u0013d\u2217\nt\n\u2264e.\n(20)\nThus, (19) reads\n|e\n\u03c3j \u2212\u03c3rt\u22121+j| \u2264\u03c3rt\u22121+1\nC1k5/2 \u2264\ne\u03c3r\u2217\nt\nC1k5/2 .\n(21)\nSuppose that, for some j \u2264d\u2217\nt , we have\n\u03c3rt\u22121+j+1\n\u03c3rt\u22121+j\n\u22651 \u22121\n4k .\n16\nThen, using (21),\ne\u03c3j+1\ne\u03c3j\n\u2265\n\u03c3rt\u22121+j+1 \u2212\ne\u03c3r\u2217t\nC1k5/2\n\u03c3rt\u22121+j +\ne\u03c3r\u2217t\nC1k5/2\n\u2265\n\u03c3rt\u22121+j+1\n\u0012\n1 \u2212\ne\nC1k5/2\n\u0013\n\u03c3rt\u22121+j\n\u0012\n1 +\ne\nC1k5/2\n\u0013 \u22651 \u22121\n2k ,\nassuming C1 is su\ufb03ciently large. In Algorithm 1, we choose dt in Line 10 so that there is no j < dt with\ne\u03c3j+1\ne\u03c3j\n\u22641 \u22121\n2k .\nThus, if there were a big gap, the algorithm would have found it: more precisely, using the de\ufb01nition of \u03b3,\nwe have\n\u03c3rt+1\n\u03c3rt\n< 1 \u22121\n4k \u22641 \u2212\u03b3.\nThis establishes the \ufb01rst conclusion of the lemma. Now, a similar analysis as above shows that if for any\nj \u2264d\u2217\nt we have\n\u03c3rt\u22121+j+1\n\u03c3rt\u22121+j\n\u22641 \u22121\nk ,\nthen\ne\u03c3j+1\ne\u03c3j\n\u22641 \u22121\n2k ,\nassuming C1 is su\ufb03ciently large. That is, our algorithm will always \ufb01nd a small gap, if it exists. In particular,\nif r\u2217\nt < k, we have\n\u03c3r\u2217\nt +1\n\u03c3r\u2217\nt\n\u22641 \u22121\nk\nand hence dt \u2264d\u2217\nt . On the other hand, if r\u2217\nt = k, then we must have dt = d\u2217\nt . In either case, dt \u2264d\u2217\nt , and so\n\u03c3rt\u22121+1\n\u03c3rt\n\u2264\u03c3rt\u22121+1\n\u03c3r\u2217\nt\n\u2264\n\u0012\n1 + 1\nk\n\u0013d\u2217\nt\n\u2264e.\nThis completes the proof of Lemma 5.\nNow, we are in a position to verify the inductive hypothesis (H4) for the next round, in the favorable case\nthat Lemma 4 holds. By de\ufb01nition, we have st = e\u03c3dt, and (19), followed by Lemma 5 implies that\n|\u03c3rt \u2212st| \u2264\u03c3rt\u22121+1\nC1k2 \u2264\ne\u03c3rt\nC1k5/2 .\nIn particular,\n \n1 \u2212\n2e\nC1k5/2\n!\n\u03c3rt \u2264st \u2264\n \n1 +\n2e\nC1k5/2\n!\n\u03c3rt,\nestablishing (H4) for st.\nNow that we know that the \u201cgap\" structure of the singular values of Nt\u22121 is re\ufb02ected by the estimates\ne\u03c3j, we will show that the top singular vectors are also well-approximated by the estimates e\nQt. Recall from\nAlgorithm 1 that e\nQt \u2208Rn\u00d7dt denotes the \ufb01rst dt columns of e\nUt, which are estimates of the top singular\nvectors of Tt. Let Qt denote the (actual) top dt singular vectors of Tt. We will \ufb01rst show that Qt is close to\nU(t), and then that Qt is also close to e\nQt.\n17\nLemma 6. Suppose that the conclusions of Lemma 4 and Lemma 5 hold, and that (18) holds. Then\nsin\u03b8(U(t),Qt) \u2264\n4e\nC1k3/2 .\nProof. We will use a sin\u03b8 theorem (Theorem 14, due to Wedin, in the appendix) to control the perturbation\nof the subspaces. Theorem 14 implies\nsin\u03b8(U(t),Qt) \u2264\n\u2225Tt \u2212Nt\u22121\u2225\n\f\f\f\u03c3dt(Tt) \u2212\u03c3rt+1\n\f\f\f\nTheorem 14\n\u2264\n2e\u03c3rt\nC1k5/2 \f\f\f\u03c3dt(Tt) \u2212\u03c3rt+1\n\f\f\f\nBy Lemmas 4 and 5, and (18)\n\u2264\n2e\u03c3rt\nC1k5/2\n\u0012\n\u03c3rt\n\u0012\n1 \u2212\n2e\nC1k5/2\n\u0013\n\u2212\u03c3rt+1\n\u0013\nBy (19) and Lemma 5\n\u2264\ne\u03c3rt\nC1k5/2\n\u0012\n\u03c3rt\n\u0012\n1 \u2212\n2e\nC1k5/2\n\u0013\n\u2212\u03c3rt(1 \u2212\u03b3)\n\u0013\nBy Lemma 5\n\u2264\n4e\nC1k3/2 .\nNow, we show that Qt is close to e\nQt.\nLemma 7. Suppose that the conclusions of Lemma 4 and Lemma 5 hold, and that (18) holds. Then with probability\n1 \u22121/n2,\nsin\u03b8(Qt, e\nQt) \u2264\n1\npoly(n).\nProof. By (17), Lemma 4, and Lemma 5, a similar computation as in the proof of Lemma 6 shows that\n\u03c3dt+1(Tt)\n\u03c3dt(Tt) \u2264\n e\u03c3dt+1\nf\n\u03c3dt\n! \n1 +\n8e\nC1k5/2\n!\n\u2264\n\u0012\n1 \u22121\n4k\n\u0013 \n1 +\n8e\nC1k5/2\n!\n\u22641 \u22121\nk\nusing the choice of dt in the second-to-last line. Thus, by Theorem 16 in the appendix, and the choice of\nL \u2273k log(n) in SubsIt, we have with probability 1 \u22121/poly(n) that\nsin\u03b8(Qt, e\nQt) \u2264poly(n)\n\u0012\n1 \u22121\n2k\n\u0013L\n\u2264\n1\npoly(n).\nTogether, Lemmas 6 and 7 imply that, when Lemma 4 and the favorable case for SubsIt hold,\nsin\u03b8(U(t), e\nQt) \u2264\n8e\nC1k3/2 .\nFinally, this implies, via Lemma 15 in the appendix, that there is some unitary matrix O \u2208Rk\u00d7k so that\n\r\r\rU(t)O \u2212e\nQt\n\r\r\r \u2264\n16e\nC1k3/2 ,\n18\nand using the fact that U(t) and e\nQt have rank at most k, we have that\n\r\r\rU(t)O \u2212e\nQt\n\r\r\rF \u226416\n\u221a\n2e\nC1k .\n(22)\nAs in Algorithm 1, let B be a random orthogonal matrix, and let Qt be the truncation\nQt = Truncate\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ede\nQtB,8\nr\n\u00b5\u2217log(n)\nn\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8.\nThe reason for the random rotation is that while U(t)O is reasonably incoherent (because U(t) is), U(t)OB is,\nwith high probability, even more incoherent. More precisely, as in [Har13a], we have\nP\n\uf8f1\uf8f4\uf8f4\uf8f2\uf8f4\uf8f4\uf8f3\n\r\r\rU(t)OB\n\r\r\r\u221e> 8\nr\n\u00b5\u2217log(n)\nn\n\uf8fc\uf8f4\uf8f4\uf8fd\uf8f4\uf8f4\uf8fe\u22641\nn2 ,\n(23)\nwhere the probability is over the choice of B. Suppose that the favorable case in (23) occurs, so that\n\r\r\rU(t)OB\n\r\r\r\u221e\u22648\np\n\u00b5\u2217log(n)/n. In the Frobenius norm, Qt is the projection of e\nQt onto the (entrywise) \u2113\u221e-ball of\nradius 8\np\n\u00b5\u2217log(n)/n in Rn\u00d7dt. Thus,\n\r\r\rQt \u2212e\nQtB\n\r\r\rF \u2264\n\r\r\rX \u2212e\nQtB\n\r\r\rF\nfor any X in this scaled \u2113\u221e-ball, and in particular\n\r\r\rQt \u2212e\nQtB\n\r\r\rF \u2264\n\r\r\rU(t)OB \u2212e\nQtB\n\r\r\rF .\nThus, (22) implies that\n\r\r\rU(t)OB \u2212Qt\n\r\r\rF \u2264\n\r\r\rU(t)OB \u2212e\nQtB\n\r\r\rF +\n\r\r\r e\nQtB \u2212Qt\n\r\r\rF \u22642\n\r\r\rU(t)OB \u2212e\nQtB\n\r\r\rF = 2\n\r\r\rU(t)O \u2212e\nQt\n\r\r\rF \u226432\n\u221a\n2e\nC1k .\n(24)\nNext, we consider the matrix Wt = QR([Xt\u22121|Qt]). Because Xt\u22121 has orthonormal columns, this matrix has\nthe form Wt = [Xt\u22121|Pt], where Pt \u2208Rn\u00d7dt has orthonormal columns, Pt \u22a5Xt\u22121, and\nR(Pt) = R((I \u2212Xt\u22121XT\nt\u22121)Qt) = R(Zt),\nwhere we de\ufb01ne Zt := (I \u2212Xt\u22121XT\nt\u22121)Qt to be the projection of Qt onto R(Xt\u22121)\u22a5. Because Qt is close to\nU(t)OB, and Xt\u22121 is close to U(<t), Zt is close to U(t)OB. More precisely,\n\r\r\rZt \u2212U(t)OB\n\r\r\r \u2264\n\r\r\r(I \u2212Xt\u22121XT\nt\u22121)(Qt \u2212U(t)OB)\n\r\r\r +\n\r\r\rXt\u22121XT\nt\u22121U(t)OB\n\r\r\r\nby the triangle inequality\n\u2264\n\r\r\rQ \u2212U(t)OB\n\r\r\rF + sin\u03b8(Xt\u22121,U(<t))\n\u226432\n\u221a\n2e\nC1k\n+ 1\nk4\n \u03c3rt\u22121+1 + \u03b5\u2225M\u2225\n\u03c3rt\u22121\n!\nby (24) and (H2)\n\u226432\n\u221a\n2e\nC1k\n+ 1\nk4\n 2\u03c3rt\u22121+1\n\u03c3rt\u22121\n!\nby (18)\n\u226464\n\u221a\n2e\nC1k\nfor su\ufb03ciently large k.\nFurther, the Gram-Schmidt process gives a decomposition\nPtR = Zt,\n19\nwhere the triangular matrix R has the same spectrum as Zt. In particular,\n\r\r\rR\u22121\r\r\r =\n1\n\u03c3min(Zt) \u2264\n1\n\r\r\rU(t)\r\r\r \u221264\n\u221a\n2e\nC1k\n\u22642\nfor su\ufb03ciently large C1. Thus,\nsin\u03b8(U(\u2264t),Pt) =\n\r\r\r\r(U(\u2264t)\n\u22a5\n)T Pt\n\r\r\r\r\n=\n\r\r\r\r(U(\u2264t)\n\u22a5\n)T ZtR\u22121\r\r\r\r\n\u22642\n\r\r\r\r(U(\u2264t)\n\u22a5\n)T Zt\n\r\r\r\r\n\u22642\n\r\r\r\r(U(\u2264t)\n\u22a5\n)T U(t)OB\n\r\r\r\r + 2\n\r\r\r\r(U(\u2264t)\n\u22a5\n)T (Zt \u2212U(t)OB)\n\r\r\r\r\n= 2\n\r\r\r\r(U(\u2264t)\n\u22a5\n)T (Zt \u2212U(t)OB)\n\r\r\r\r\n\u2264128\n\u221a\n2e\nC1k\n,\n(25)\nwhere above we used that (U(\u2264t)\n\u22a5\n)T U(t) = 0. Next,\nmax\ni\n\r\r\reT\ni Pt\n\r\r\r2 \u2264max\ni\n\r\r\reT\ni Zt\n\r\r\r2\n\r\r\rR\u22121\r\r\r\n\u22642\n\u0012\nmax\ni\n\r\r\reT\ni Qt\n\r\r\r2 + max\ni\n\r\r\reT\ni Xt\u22121XT\nt\u22121Qt\n\r\r\r2\n\u0013\n\u22642\n\u0012\nmax\ni\n\r\r\reT\ni Qt\n\r\r\r2 + max\ni\n\r\r\reT\ni Xt\u22121\n\r\r\r2\n\u0010\r\r\rXT\nt\u22121U(t)OB\n\r\r\r +\n\r\r\rXT\nt\u22121(U(t)OB \u2212Qt)\n\r\r\r\n\u0011\u0013\n\u22642\n\u0012\nmax\ni\n\r\r\reT\ni Qt\n\r\r\r2 + max\ni\n\r\r\reT\ni Xt\u22121\n\r\r\r2\n\u0010\r\r\rXT\nt\u22121U(t)\r\r\r +\n\r\r\rU(t)OB \u2212Qt\n\r\r\r\n\u0011\u0013\n\u226416\nr\nk\u00b5\u2217log(n)\nn\n+ 2\nr\nk\u00b5t\u22121\nn\n 2\nk4 + 32\n\u221a\n2e\nC1k\n!\n,\nwhere we have used the de\ufb01nition of Qt, the incoherence of Xt\u22121, and the computations above in the \ufb01nal\nline. Thus,\nmax\ni\n\r\r\reT\ni Pt\n\r\r\r2 \u2264\nr\nk\nn\n \n16\nq\n\u00b5\u2217log(n) +\nC5\n\u221a\u00b5t\u22121\nk\n!\n(26)\nfor some constant C5. Thus, when the conclusions of Lemma 4 hold, Pt is both close to U(t) and incoherent.\nBy induction, the same is true for Wt. Indeed, if t = 1, then Pt = Wt, and we are done. If t \u22652, then we have\nsin\u03b8(Wt,U(\u2264t)) \u2264sin\u03b8(Xt\u22121,U(\u2264t\u22121)) + sin\u03b8(Pt,U(t)).\nThen, the inductive hypothesis (H1) and our conclusion (25) imply that\nsin\u03b8(Wt,U(\u2264t)) \u22641\nk\nfor suitably large C0,C1. Finally, (26), along with the inductive hypothesis (H3) implies that\nmax\ni\n\r\r\reT\ni Wt\n\r\r\r2 \u2264max\ni\n\r\r\reT\ni Xt\u22121\n\r\r\r2 + max\ni\n\r\r\reT\ni Pt\n\r\r\r2 \u2264\nr\nk\nn\n\u0012\u221a\u00b5t\u22121\n\u0012\n1 + C5\nk\n\u0013\n+ 16\nq\n\u00b5\u2217log(n)\n\u0013\n\u2264\nr\nk\u00b5t\nn .\n20\nWe remark that this last computation is the only reason we need sin\u03b8(Pt,U(t)) \u22721/k, rather than bounded by\n1/4; eventually, we will iterate and have\n\u221a\u00b5T \u2264\u221a\u00b50\n\u0012\n1 + C5\nk\n\u0013T\n+ 16T\nq\n\u00b5\u2217log(n) \u2264eC5\u221a\u00b50 + 16T\nq\n\u00b5\u2217log(n),\nand we need that\n\u0010\n1 + C5\nk\n\u0011T \u2264eC5 is bounded by a constant (rather than exponential in T).\nFinally, we have shown that with probability 1 \u22121/n2 (that is, in the case that Lemma 4 holds and SubsIt\nworks), all of the conclusions of Lemma 2 hold as well. This completes the proof of Lemma 2.\n6\nProof of Lemma 3\nIn the proof of Lemma 3 we will need an explicit description of the subroutine SmoothQR that we include\nin Algorithm 3.\nAlgorithm 3: SmoothQR (S,\u03b6,\u00b5) (Smooth Orthonormalization)\nInput: Matrix S \u2208Rn\u00d7k, parameters \u00b5,\u03b6 > 0.\n1 X \u2190QR(S),H \u21900;\n2 \u03c3 \u2190\u03b6\u2225S\u2225/n.;\n3 while \u00b5(R) > \u00b5 and \u03c3 \u2264\u2225S\u2225do\n4\nR \u2190QR(S + H) where H \u223cN(0,\u03c32/n);\n5\n\u03c3 \u21902\u03c3;\n6 end\nOutput: Matrix R\nTo prove Lemma 3, we will induct on the iteration \u2113in S-M-AltLS (Algorithm 2). Let R\u2113denote the\napproximation in iteration \u2113. Thus, R0 = Xt\u22121. Above, we are suppressing the dependence of R\u2113on the epoch\nnumber t, and in general, for this section we will drop the subscripts t when there is no ambiguity. We\u2019ll use\nthe shorthand\n\u0398j\n\u2113= \u03b8(R\u2113\n(\u2264j),U(\u2264j))\nand\nEj\n\u2113= (I \u2212R\u2113\n(\u2264j)(R\u2113\n(\u2264j))T )U(\u2264j),\nso that\n\r\r\r\rEj\n\u2113\n\r\r\r\r = sin(\u0398j\n\u2113).Recall the de\ufb01nition (10) that \u03b3\u2217= min{\u03b3,\u03b3k}. Notice that this choice ensures that\n\u03b3\u2217\u2264\u03b3rj for all choices of j, including the case of j = t, in the \ufb01nal epoch of SoftDeflate, when rt = k.\nWe will maintain the following inductive hypothesis:\n\u03c3rj tan\u0398j\n\u2113\u2264max\n( 2e\u03c3rt\nk\n!\nexp(\u2212\u03b3\u2217\u2113/2), \u03c3rt+1 + \u03b5\u2225M\u2225\n2eC0k4\n)\n=: \u03bd\u2113\n\u2200j \u2264t.\n(J1)\nAbove, the tangent of the principal angle obeys\n\r\r\r\rEj\n\u2113\n\r\r\r\r \u2264tan\u0398j\n\u2113=\n\r\r\r\rEj\n\u2113\u22121\n\r\r\r\r\nr\n1 \u2212\n\r\r\r\rEj\n\u2113\u22121\n\r\r\r\r\n2\n\u22642\n\r\r\r\rEj\n\u2113\u22121\n\r\r\r\r,\n(27)\nwhenever\n\r\r\r\rEj\n\u2113\u22121\n\r\r\r\r \u22641/4. We will also maintain the inductive hypothesis\nmax\ni\n\r\r\reT\ni R\u2113\n\r\r\r2 \u2264\nr\nk\u00b5t\nn .\n(J2)\n21\nTo establish the base case of (J1) for j = t, we have\n\u03c3rt sin\u03b8(Wt,U(\u2264t)) \u2264\u03c3rt\nk ,\nby conclusion (14) of Lemma 2, and hence by (27),\n\u03c3rt tan\u03b8(Wt,U(\u2264t)) \u22642\u03c3rt\nk .\nIf t = 1, then Wt = R0, and we are done with the base case for (J1); if t \u22652, then for j \u2264t \u22121, we have\nR0\n(\u2264j) = Xt\u22121\n(\u2264j).\nThus, for j \u2264t \u22121, (J1) is implied by (27) again, along with the fact that\n\u03c3rjsin\u03b8(Xt\u22121\n(\u2264j),U(\u2264j)) \u22641\nk4\n\u0010\n\u03c3rt\u22121+1 + \u03b5\u2225M\u2225\n\u0011\n\u2264e\u03c3rt + \u03b5\u2225M\u2225\nk4\n\u22642e\u03c3rt\nk4 ,\nwhich is the (outer) inductive hypothesis (H1), followed by the conclusions (12) and (13) from Lemma 2.\nThis establishes the base case for (J1). The base case for (J2) follows from the conclusion (14) of Lemma 2\ndirectly.\nHaving established (J1), (J2) for \u2113= 0, we now suppose that they hold for \u2113\u22121 and consider step \u2113.\nNotice that, by running SmoothQR with parameter \u00b5 = \u00b5t, we automatically ensure (J2) for the next round\nof induction, and so our next goal is to establish (J1). For this, we need to go deeper into the workings of\nS-M-AltLS. The analysis of S-M-AltLS in [Har13a] is based on an analysis of NSI, given in Algorithm 4.\nWe may view S-M-AltLS as a special case of NSI. More precisely, let H\u2113be the noise matrix added from\nAlgorithm 4: NSI(A,R0,L) (Noisy Subspace Iteration)\nInput: Number of iterations L \u2208N, symmetric matrix A \u2208Rn\u00d7n, initial matrix R0 \u2208Rn\u00d7r.\n1 for \u2113= 1,...,L do\n2\nS\u2113\u2190AR\u2113\u22121 + e\nG\u2113\n3\nR\u2113\u2190QR(S\u2113)\n4 end\nOutput: Pair of matrices (RL,SL)\nSmoothQR in the \u2113\u2019th iteration of S-M-AltLS, and de\ufb01ne G(s)\n\u2113\nto be\nG(s)\n\u2113= argminS\u2208Rn\u00d7r\n\r\r\r\r\rP\u2126(s)\n\u2113(A \u2212R\u2113\u22121ST )\n\r\r\r\r\r\n2\nF\n\u2212AR\u2113\u22121,\n(28)\nand let\nG\u2113= medians(G(s)\n\u2113).\nThen we may write R\u2113, the \u2113\u2019th iterate in S-M-AltLS, as\nR\u2113= SmoothQR(AR\u2113\u22121 + G\u2113) = QR(AR\u2113\u22121 + G\u2113+ H\u2113) =: QR(AR\u2113\u22121 + e\nG\u2113).\nThat is, R\u2113is also the \u2113\u2019th iterate in NSI, when the noise matrices are e\nG\u2113= G\u2113+ H\u2113. We will take this view\ngoing forward, and analyze S-M-AltLS as a special case of NSI. We have the following theorem, which is\ngiven in [Har13a, Lemma 3.4].\n22\nTheorem 8. Let e\nG\u2113= G\u2113+ H\u2113be as above. Let j \u2264t and suppose that\n\r\r\r\rEj\n\u2113\u22121\n\r\r\r\r \u22641\n4 and that\n\r\r\re\nG\u2113\n\r\r\r \u2264\n\u03c3rj\u03b3rj\n32 .\nThen the next iterate R\u2113of NSI satis\ufb01es\ntan\u03b8(U(\u2264j),R\u2113\u22121) \u2264max\n\uf8f1\uf8f4\uf8f4\uf8f2\uf8f4\uf8f4\uf8f3\n8\n\r\r\re\nG\u2113\n\r\r\r\n\u03c3rj\u03b3rj\n,tan\u03b8(U(\u2264j),R\u2113\u22121)exp(\u2212\u03b3rj/2)\n\uf8fc\uf8f4\uf8f4\uf8fd\uf8f4\uf8f4\uf8fe.\nTo use Theorem 8, we must understand the noise matrices e\nG\u2113= G\u2113+ H\u2113. We begin with G\u2113.\nLemma 9 (Noise term G\u2113in NSI). There is a constant C so that the following holds. Fix \u2113and suppose that (J2)\nholds for \u2113\u22121: that is, \u00b5(R\u2113\u22121) \u2264\u00b5t. Let 0 < \u03b4 < 1/2, and suppose that the samples \u2126\u2032\nt for S-M-AltLS are sampled\nindependently with probability\np\u2032\nt \u2265CLtsmax\nk\u00b5t log(n)\n\u03b42n\n,\nwhere Lt is the number of iterations of S-M-AltLS, and smax \u2265C log(n) is the number of trials each iteration of\nS-M-AltLS performs before taking a median. Then with probability at least 1 \u22121/n5 over the choice of \u2126\u2032\nt, the\nnoise matrix G\u2113satis\ufb01es\n\u2225G\u2113\u2225F \u2264\u03b4\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\u2225Nt\u2225F +\nn\nX\nj=1\n\r\r\r\rEj\n\u2113\u22121\n\r\r\r\r\n\r\r\rM(j)\r\r\rF\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8=: \u03c9\u2113\u22121\nand for all i \u2208[n],\n\r\r\reT\ni G\u2113\n\r\r\r2 \u2264\u03b4\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\n\r\r\reT\ni Nt\n\r\r\r2 +\nn\nX\nj=1\n\r\r\r\rEj\n\u2113\u22121\n\r\r\r\r\n\r\r\reT\ni M(j)\r\r\r2\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8=: \u03c9(i)\n\u2113\u22121 .\nThe proof of Lemma 9 is similar to the analysis in [Har13a]. For completeness, we include the proof in\nAppendix C. Using the inductive hypothesis (J1), and the fact that\n\r\r\rM(j)\r\r\rF \u2264\n\u221a\nk\u03c3rj ,\n\u03c9\u2113\u22121 \u2264\u03b4\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\nX\nj\n\r\r\r\rEj\n\u2113\u22121\n\r\r\r\r\n\u0010\u221a\nk\u03c3rj\n\u0011\n+\n\u221a\nk\u03c3rt+1 + \u2206\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\u2264\u03b4\n\u0010\nt\n\u221a\nk\u03bd\u2113\u22121 +\n\u221a\nk\u03c3rt+1 + \u2206\n\u0011\n.\nWe will choose\n\u03b4 =\n\u03b3\u2217\n4eC0C3k4 min\n( 1\n\u221a\nk\n, \u03b5\u2225M\u2225\n\u2206\n)\n,\n(29)\nfor a constant C3 to be chosen su\ufb03ciently large. Observe that with this choice of \u03b4, the requirement on p\u2032\nt in\nLemma 9 is implied by the requirement on p\u2032\nt in the statement in Lemma 3. Then the choice of \u03b4 implies\n\u2225G\u2113\u2225F \u2264\u03c9\u2113\u22121 \u2264\n\u03b3\u2217\n4eC0C3k4\n\u0010\nt\u03bd\u2113\u22121 + \u03c3rt+1 + \u03b5\u2225M\u2225\n\u0011\n\u2264\u03b3\u22174eC0\n43C0C3\n\u03bd\u2113\u22121 \u2264\u03b3\u2217\nC3\n\u03bd\u2113\u22121.\n(30)\nNow, we turn to the noise term H\u2113added by SmoothQR. For a matrix G \u2208Rn\u00d7k (not necessarily orthonor-\nmal), we will de\ufb01ne\n\u03c1(A) := n\nk max\ni\u2208[n]\n\r\r\reT\ni G\n\r\r\r2\n2 .\nOur analysis of H\u2113relies on the following lemma from [Har13a].\n23\nLemma 10 (Lemma 5.4 in [Har13a]). Let \u03c4 > 0 and suppose that rt = o(n/ log(n)). There is an absolute constant\nC so that the following claim holds. Let G \u2208Rn\u00d7rt, and let R \u2208Rn\u00d7rt be an orthonormal matrix, and let \u03bd \u2208R so\nthat \u03bd \u2265max\u2225G\u2225,\u2225NtR\u2225. Assume that\n\u00b5t \u22652\u00b5(U) + C\n\u03c42\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\n\u03c1(G) + \u00b5(U)\n\r\r\r(U(\u2264t))T G\n\r\r\r2 + \u03c1(NtR)\n\u03bd2\n+ log(n)\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8.\nThen, for every \u03b6 \u2264\u03c4\u03bd satisfying log(n/\u03b6) \u2264n, we have with probability at least 1 \u22121/n4 that the algorithm\nSmoothQR (AR + G,\u03b6,\u00b5t) terminates in log(n/\u03b6) iterations, and the output R\u2032 satis\ufb01es \u00b5(R\u2032) \u2264\u00b5t. Further, the\n\ufb01nal noise matrix H added by SmoothQR satis\ufb01es \u2225H\u2225\u2264\u03c4\u03bd.\nWe will apply Lemma 10 to our situation.\nLemma 11 (Noise term H\u2113in NSI added by SmoothQR). Suppose that k = o(n/ log(n)). There is a constant C2\nso that the following holds. Suppose that\n\u00b5t \u2265\nC2\n(\u03b3\u2217)2\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\u00b5\u2217\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8edk +\n k4 \u2225N\u2225F\n\u03b5\u2225M\u2225\n!2\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8+ log(n)\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8.\nSuppose that the favorable conclusion of Lemma 9 occurs. Choose \u03b6 = \u03b5s0k\u22125, as in Algorithm 1. Then, with\nprobability at least 1 \u22121/n4 over the randomness of SmoothQR, the output R\u2113of SmoothQR(AR\u2113\u22121 + G\u2113,\u03b6,\u00b5t)\nsatis\ufb01es\n\u00b5(R\u2113) \u2264\u00b5t,\nand the number of iterations is O(log(n/(\u03b5\u2225M\u2225))). Further, the noise matrix H\u2113satis\ufb01es\n\u2225H\u2113\u2225\u2264\u03b3\u2217\u03bd\u2113\u22121\nC3\n.\nProof. We apply Lemma 10 with G = G\u2113,R = R\u2113\u22121, and \u03bd = \u03bd\u2113, and\n\u03c4 = \u03b3\u2217\nC3\n.\n(31)\nFirst, we observe that the choice of \u03b6 = \u03b5s0k\u22125 \u2264\u03b5\u2225M\u2225\u03b3\u2217k\u22124 \u2264\u03c4\u03bd\u2113\u22121 indeed satis\ufb01es the requirements of\nLemma 10. Next, we verify that max{\u2225G\u2113\u2225,\u2225NtR\u2113\u22121\u2225} \u2264\u03bd\u2113\u22121. Indeed, from (30),\n\u2225G\u2113\u2225\u2264\u03c9\u2113\u22121 \u2264\u03b3\u2217\nC3\n\u03bd\u2113\u22121 \u2264\u03bd\u2113\u22121.\nFurther, we have\n\u2225NtR\u2113\u22121\u2225\u2264\u03c3rtsin\u03b8(U(\u2264t),R\u2113\u22121) \u2264\u03bd\u2113\u22121\nby the inductive hypothesis (J1) for j = t.\nNext, we compute the parameters that show up in Lemma 10. From Lemma 9, we have\n\u03c1(G\u2113) \u2264n\nrt\nmax\ni\n\u0012\n\u03c9(i)\n\u2113\u22121\n\u00132\nand\n\u00b5(U)\n\r\r\rU(\u2264t)G\u2113\n\r\r\r\n2 \u2264\u2225G\u2113\u22252 \u2264\u00b5\u2217\u03c92\n\u2113\u22121.\n24\nWe also have\n\u03c1(NtR\u2113\u22121) = n\nrt\nmax\ni\n\r\r\reT\ni NtR\u2113\u22121\n\r\r\r2\n2\n\u2264n\nrt\n\u0012\nmax\ni\n\r\r\reT\ni U(t:k)\r\r\r2 \u03c3rt\n\r\r\r(U(t:k))T R\u2113\u22121\n\r\r\r2 + max\ni\n\r\r\reT\ni N\n\r\r\r2 \u2225R\u2113\u22121\u22252\n\u00132\n\u2264n\nrt\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\nr\nk\u00b5(U)\nn\n\u03c3rt\n\r\r\rEt\n\u2113\u22121\n\r\r\r +\nr\n\u00b5N \u2225N\u2225F\nn\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\n2\n\u22642\u00b5\u2217\n k\nrt\n\u03c32\nrt\n\r\r\rEt\n\u2113\u22121\n\r\r\r2 + \u2225N\u22252\nF\nrt\n!\n\u22642\u00b5\u2217\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\nk\u03bd2\n\u2113\u22121\nrt\n+ \u2225N\u22252\nF\nrt\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8,\nwhere we have used the inductive hypothesis (J1) in the \ufb01nal line. Then, the requirement of Lemma 10 on \u00b5t\nreads\n\u00b5t \u22652\u00b5\u2217+ C\n\u03c42\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\nn\nrt maxi\n\u0012\n\u03c9(i)\n\u2113\u22121\n\u00132\n+ \u00b5\u2217\u03c92\n\u2113\u22121 + 2\u00b5\u2217\n\u0012\nk\nrt \u03bd2\n\u2113\u22121 + \u2225N\u22252\nF\nrt\n\u0013\n\u03bd2\n\u2113\u22121\n+ log(n)\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\n.\nWe have, for all i,\n\u03c9(i)\n\u2113\u22121\n\u03c9\u2113\u22121\n=\n\r\r\reT\ni Nt\n\r\r\r2 + Pt\nj=1\n\r\r\r\rEj\n\u2113\u22121\n\r\r\r\r\n\r\r\reT\ni M(j)\r\r\r2\n\u2225Nt\u2225F + Pt\nj=1\n\r\r\r\rEj\n\u2113\u22121\n\r\r\r\r\n\r\r\rM(j)\r\r\rF\n\u2264\n\u03c3rt\n\u221a\n\u22062\u00b5\u2217/n + Pt\nj=1\n\r\r\r\rEj\n\u2113\u22121\n\r\r\r\r\u03c3rj\n\u221a\nk\u00b5\u2217/n\n\u2225Nt\u2225F + Pt\nj=1\n\r\r\r\rEj\n\u2113\u22121\n\r\r\r\r\n\r\r\rM(j)\r\r\rF\n\u2264\n\u2225Nt\u2225F\n\u221a\n\u22062\u00b5\u2217/n + Pt\nj=1\n\r\r\r\rEj\n\u2113\u22121\n\r\r\r\r\n\r\r\rM(j)\r\r\rF\n\u221a\nk\u00b5\u2217/n\n\u2225Nt\u2225F + Pt\nj=1\n\r\r\r\rEj\n\u2113\u22121\n\r\r\r\r\n\r\r\rM(j)\r\r\rF\n=\nr\n\u00b5\u2217\nn\n\u0010\u221a\nk + \u2206\n\u0011\n.\n25\nWe may simplify and bound the requirement on \u00b5t as\n2\u00b5\u2217+ C\n\u03c42\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\nn\nrt maxi\n\u0012\n\u03c9(i)\n\u2113\u22121\n\u00132\n+ \u00b5\u2217\u03c92\n\u2113\u22121 + 2\u00b5\u2217\n\u0012\nk\nrt \u03bd2\n\u2113\u22121 + \u2225N\u22252\nF\nrt\n\u0013\n\u03bd2\n\u2113\u22121\n+ log(n)\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\n\u22642\u00b5\u2217+ C\n\u03c42\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\nn\nrt maxi\n\u0012\n\u03c9(i)\n\u2113\u22121\n\u00132\n(\u03b3\u2217)2\nC2\n3\u03c92\n\u2113\u22121\n+ \u00b5\u2217(\u03b3\u2217)2 \u03c92\n\u2113\u22121\nC2\n3\u03c92\n\u2113\u22121\n+\n2\u00b5\u2217\n\u0012\nk\nrt \u03bd2\n\u2113\u22121 + \u2225N\u22252\nF\nrt\n\u0013\n\u03bd2\n\u2113\u22121\n+ log(n)\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\nusing \u03bd\u2113\u22121 \u2265C3\u03c9\u2113\u22121/\u03b3\u2217, by (30)\n\u22642\u00b5\u2217+ C\n\u03c42\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\nk+\u22062\nrt\n\u00b5\u2217(\u03b3\u2217)2\nC2\n3\n+ \u00b5\u2217(\u03b3\u2217)2\nC2\n3\n+ 2\u00b5\u2217\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\nk\nrt\n+ \u2225N\u22252\nF\nrt\u03bd2\n\u2113\u22121\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8+ log(n)\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\nby the bound on \u03c9(i)\n\u2113\u22121/\u03c9\u2113, above\n\u2264C\u2032\u00b5\u2217\n(\u03b3\u2217)2\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\nk\nrt\n+ \u2225N\u22252\nF\n\u03bd2\n\u2113\u22121\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8+ C2\n3 log(n)\n(\u03b3\u2217)2\nby the de\ufb01nition of \u03c4 and gathering terms\n\u2264C2\u00b5\u2217\n(\u03b3\u2217)2\n k\nrt\n+ k8 \u2225N\u22252\nF\n\u03b52 \u2225M\u22252\n!\n+ C2\n3 log(n)\n(\u03b3\u2217)2\nby the fact that \u03bd\u2113\u22121 \u2265\u03b5\u2225M\u2225\n2eC0k4 .\nfor some constant C2, which was the requirement in the statement of the lemma. Thus, as long as the\nhypotheses of the current lemma hold, Lemma 10 implies that with probability at least 1 \u22121/n4,\n\u2225H\u2113\u2225\u2264\u03c4\u03bd\u2113\u22121 = \u03b3\u2217\u03bd\u2113\u22121\nC3\n.\nThis completes the proof of Lemma 11.\nThus, using the inductive hypothesis (J2), Lemmas 9 and 11 imply that as long as the requirements on p\u2032\nt\nand \u00b5t in the statements of those lemmas are satis\ufb01ed (which they are, by the choices in Lemma 3), with\nprobability at least 1 \u22122/n4 the noise matrices e\nG\u2113satisfy\n\r\r\re\nG\u2113\n\r\r\r \u2264\u2225G\u2113\u2225+ \u2225H\u2113\u2225\u2264\u03c9\u2113\u22121 + \u03b3\u2217\u03bd\u2113\u22121\nC3\n\u22642\u03b3\u2217\u03bd\u2113\u22121\nC3\n,\nusing (30) in the \ufb01nal inequality. Now, we wish to apply Theorem 8. The hypothesis (J1), along with the\nconclusion (12) from Lemma 2, immediately implies that\n\r\r\rEt\n\u2113\u22121\n\r\r\r \u22641\nk\nfor all j \u2264t, and so in particular the \ufb01rst requirement of Theorem 8 is satis\ufb01ed. To satisfy the second\nrequirement of Theorem 8, we must show that\n\r\r\re\nG\u2113\n\r\r\r \u2264\u03c3rj\u03b3rj/32,\nfor which it su\ufb03ces to show that\n2\u03b3\u2217\u03bd\u2113\u22121\nC3\n\u2264\u03c3rj\u03b3rj/32.\n(32)\nFrom the de\ufb01nition of \u03bd\u2113\u22121, and the fact that \u03b3\u2217\u2264\u03b3rj, we see that (32) is satis\ufb01ed for a su\ufb03ciently large\n26\nchoice of C3. Then Theorem 8 implies that with probability at least 1 \u22122/n4, for any \ufb01xed j, we have\n\u03c3rj tan\u0398j\n\u2113\u2264\u03c3rj max\n\uf8f1\uf8f4\uf8f4\uf8f2\uf8f4\uf8f4\uf8f3\n8\n\r\r\re\nG\u2113\n\r\r\r\n\u03c3rj\u03b3rj\n,tan\u0398j\n\u2113\u22121 exp(\u2212\u03b3rj/2)\n\uf8fc\uf8f4\uf8f4\uf8fd\uf8f4\uf8f4\uf8fe\n\u2264max\n\uf8f1\uf8f4\uf8f4\uf8f2\uf8f4\uf8f4\uf8f3\n16\u03bd\u2113\u22121\u03b3\u2217\nC3\u03b3rj\n,\u03bd\u2113\u22121 exp(\u2212\u03b3rj/2)\n\uf8fc\uf8f4\uf8f4\uf8fd\uf8f4\uf8f4\uf8feby (J1) and (30)\n\u2264\u03bd\u2113\u22121 exp(\u2212\u03b3\u2217/2)\n\u2264\u03bd\u2113\nprovided C3 is suitably large. A union bound over all j establishes (J1) for the next iteration of S-M-AltLS.\nAfter another union bound over\nLt = C\n\u03b3\u2217log\n \nk \u00b7\n\u03c3rt\n\u03c3rt+1 + \u03b5\u2225M\u2225\n!\nsteps of S-M-AltLS, for some constant C depending on C0, we conclude that with probability at least 1\u22121/n2,\nfor all j,\n\u03c3rj sin\u03b8(R\u2113\u22121\n(\u2264j),U(\u2264j)) \u2264\u03c3rj tan\u03b8(R\u2113\u22121\n(\u2264j),U(\u2264j)) \u2264\u03c3rt+1 + \u03b5\u2225M\u2225\n2eC0k4\n.\nTo establish the second conclusion, we note that we have already conditioned on the event that (30) holds,\nand so we have\n\r\r\rM(\u2264t) \u2212XtY T\nt\n\r\r\r =\n\r\r\r\u03a0XNt + \u03a0X\u22a5M(\u2264t) + Xt(AXt \u2212Yt)T \r\r\r\n\u2264\u2225\u03a0XNt\u2225+\n\r\r\r\u03a0X\u22a5M(\u2264t)\r\r\r + \u2225Xt(AXt \u2212Yt)\u2225\n\u2264\u03c3rt+1sin\u03b8(Xt,U(\u2264t)) + e\nt\nX\nj=1\n\u03c3rjsin\u03b8(Xt\n(\u2264j),U(\u2264j)) + \u2225GL\u2225\n\u2264ke \u03c3rt+1 + \u03b5\u2225M\u2225\n2eC0k4\n+\n\u03b3\u2217\n2eC0C3k4 (\u03c3rt+1 + \u03b5\u2225M\u2225)\nby (30) and the de\ufb01nition of \u03bdL\n\u2264\u03c3rt+1 + \u03b5\u2225M\u2225\nC0k3\n.\nAbove, we used the inequality\n\r\r\r\u03a0X\u22a5M(\u2264t)\r\r\r =\n\r\r\r\r\r\r\r\r\nt\nX\nj=1\n\u03a0X\u22a5M(j)\n\r\r\r\r\r\r\r\r\n\u2264\nt\nX\nj=1\n\r\r\r\u03a0X\u22a5M(j)\r\r\r \u2264\nt\nX\nj=1\n\u03c3rj\u22121+1\n\r\r\r\u03a0X\u22a5U(j)\r\r\r\n\u2264\nt\nX\nj=1\n\u03c3rj\u22121+1\n\r\r\r\r\u03a0X(\u2264j)\n\u22a5U(\u2264j)\r\r\r\r \u2264\nt\nX\nj=1\ne\u03c3rjsin\u03b8(X(\u2264j),U(\u2264j)),\nusing (13) in the \ufb01nal inequality. Finally, the third conclusion, that (H3) holds, follows from the de\ufb01nition\nof SmoothQR.\n7\nSimulations\nIn this section, we compare the performance of SoftDeflate to that of other fast algorithms for matrix\ncompletion. In particular, we investigate the performance of SoftDeflate compared to the Frank-Wolfe\n(FW) algorithm analyzed in [JS10], and also compared to the naive algorithm which simply takes the SVD of\nthe subsampled matrix A\u2126. All of the code that generated the results in this section can be found online at\nhttp://sites.google.com/site/marywootters.\n27\nFigure 2: Performance of SoftDeflate compared to FW and SVD.\n7.1\nPerformance of SoftDeflate compared to FW and SVD\nTo compare SoftDeflate against FW and SVD, we generated random rank 3, 10,000 \u00d7 10,000 matrices,\nas follows. First, we speci\ufb01ed a spectrum, either (1,1,1),(1,1,.1), or (1,1,.01), with the aim of observing\nthe dependence on the condition number. Next, we chose a random 10,000 \u00d7 3 matrix U with orthogonal\ncolumns, and let A = U\u03a3UT , where \u03a3 \u2208R3\u00d73 is the diagonal matrix with the speci\ufb01ed spectrum. We\nsubsampled the matrix to various levels m, and ran all three algorithms on the samples, to obtain a low-rank\nfactorization A = XY T .\nWe implemented SoftDeflate, as described in Algorithm 1, \ufb01xing 30,000 observations per iteration; to\nincrease the number of measurements, we increased the parameters Lt (which were the same for all t). For\nsimplicity, we used a version of S-M-AltLS which did not implement the smoothing in SmoothQR or the\nmedian. We implemented the Frank-Wolfe algorithm as per the pseudocode in Algorithm 5, with accuracy\nparameter \u03b5 = 0.05. We remark that decreasing the accuracy parameter did improve the performance of the\nalgorithm (at the cost of increasing the running time), but did not change its qualitative dependence on m,\nthe number of observations. We implemented SVD via subspace iteration, as in Algorithm 8, with L = 100.\nAlgorithm 5: FW(P\u2126(A),\u2126,\u03b5): Frank-Wolfe algorithm for Matrix Completion of symmetric matrices.\nInput: Observed set of indices \u2126\u2286[n] \u00d7 [n] of an unknown, trace 1, symmetric matrix A \u2208Rn\u00d7n with\nentries P\u2126(A), and an accuracy parameter \u03b5.\n1 Initialize Z = vvT for a random unit vector v \u2208Rn.\n2 for \u2113= 1 to 1/\u03b5 do\n3\nLet w be the eigenvector corresponding to the largest eigenvalue of \u2212\u2207f (Z).\n4\n// f (Z) := 1\n2 \u2225A\u2126\u2212Z\u2126\u22252\nF\n5\n\u03b1\u2113\u21901\n\u2113\n6\nZ \u2190\u03b1\u2113wwT + (1 \u2212\u03b1\u2113)Z\n7 end\nOutput: Trace 1 matrix Z with rank at most 1/\u03b5.\nThe error was measured in two ways: the Frobenius error\n\r\r\rA \u2212XY T \r\r\rF, and the error between the\nrecovered subspaces, sin\u0398(U,X). The results are shown in Figure 2. The experiments show that SoftDeflate\nsigni\ufb01cantly outperforms the other \u201cfast\" algorithms in both metrics. In particular, of the three algorithms,\nSoftDeflate is the only one which converges enough to reliably capture the singular vector associated with\nthe 0.1 eigenvalue; none of the algorithms converge enough to \ufb01nd the 0.01 eigenvalue with the number of\n28\nFigure 3: Performance of SoftDeflate compared to FW and SVD on a rank-2, 1000\u00d71000 random matrix with spectrum\n(1,1); average of 10 trials.\nmeasurements allowed. The other two algorithms show basically no progress for these small values of m. To\nillustrate what happens when FW and SVD do converge, we repeated the same experiment for n = 1000 and\nk = 2; for this smaller value of n, we can let the number of measurements to get quite large compared to\nn2. We \ufb01nd that even though FW and SVD do begin to converge eventually, they are still outperformed by\nSoftDeflate. The results of these smaller tests are shown in Figure 3.\n7.2\nFurther comments on the Frank-Wolfe algorithm\nAs algorithms like Frank-Wolfe are often cited as viable fast algorithms for the Matrix Completion problem,\nthe reader may be surprised by the performance of FW depicted in Figures 2 and 3. There are two reasons for\nthis. The \ufb01rst reason, noted in Section 2.2, is that while FW is guaranteed to converge on the sampled entries,\nit may not converge so well on the actual matrix; the errors plotted above are with respect to the entire\nmatrix. To illustrate this point, we include in Figure 4 the results of an experiment showing the convergence\nof Frank-Wolfe (Algorithm 5), both on the samples and o\ufb00the samples. As above, we considered random\n10,000\u00d710,000 matrices with a pre-speci\ufb01ed spectrum. We \ufb01xed the number of observations at 5\u00d7106, and\nran the Frank-Wolfe algorithm for 40 iterations, plotting its progress both on the observed entries and on the\nentire matrix. While the error on the observed entries does converge as predicted, the matrix itself does not\nconverge so quickly.\nThe second reason that FW (and SVD) perform comparatively poorly above is that the convergence of\nFW, in the number of samples, is much worse than that of SoftDeflate. More precisely, in order to achieve\nerror on the order of \u03b5, the number of samples required by FW has a dependence of 1/poly(\u03b5); in contrast, as\nwe have shown, the dependence on \u03b5 of SoftDeflate is on the order of log(1/\u03b5). In particular, because in the\ntests above there were never enough samples for FW to converge past the error level of 0.1 in Figure 2, FW\n29\nFigure 4: Performance of the Frank-Wolfe algorithm on random 10,000 \u00d7 10,000, rank 3 matrices with 5,000,000\nobservations. Average of 10 trials.\n30\nnever found the singular vector associated with the singular value 0.1. Thus, the error when measured by\nsin\u0398(U,X) remained very near to 1 for the entire experiment.\nAcknowledgements\nWe thank the Simons Institute for Theoretical Computer Science at Berkeley, where part of this work was\ndone.\nReferences\n[AKKS12] Haim Avron, Satyen Kale, Shiva Prasad Kasiviswanathan, and Vikas Sindhwani. E\ufb03cient and\npractical stochastic subgradient descent for nuclear norm regularization. In Proc. 29th ICML.\nACM, 2012.\n[BK07]\nRobert M. Bell and Yehuda Koren. Scalable collaborative \ufb01ltering with jointly derived neighbor-\nhood interpolation weights. In ICDM, pages 43\u201352. IEEE Computer Society, 2007.\n[CR09]\nEmmanuel J. Cand\u00e8s and Benjamin Recht. Exact matrix completion via convex optimization.\nFoundations of Computional Mathematics, 9:717\u2013772, December 2009.\n[CT10]\nEmmanuel J. Cand\u00e8s and Terence Tao. The power of convex relaxation: near-optimal matrix\ncompletion. IEEE Transactions on Information Theory, 56(5):2053\u20132080, 2010.\n[GAGG13] Suriya Gunasekar, Ayan Acharya, Neeraj Gaur, and Joydeep Ghosh. Noisy matrix completion\nusing alternating minimization. In Proc. ECML PKDD, pages 194\u2013209. Springer, 2013.\n[Har13a]\nMoritz Hardt. On the provable convergence of alternating minimization for matrix completion.\narXiv, 1312.0925, 2013.\n[Har13b]\nMoritz Hardt.\nRobust subspace iteration and privacy-preserving spectral analysis.\narXiv,\n1311:2495, 2013.\n[HH09]\nJustin P. Haldar and Diego Hernando. Rank-constrained solutions to linear matrix equations\nusing powerfactorization. IEEE Signal Process. Lett., 16(7):584\u2013587, 2009.\n[HK12]\nElad Hazan and Satyen Kale. Projection-free online learning. In Proc. 29th ICML. ACM, 2012.\n[HO14]\nCho-Jui Hsieh and Peder A. Olsen. Nuclear norm minimization via active subspace selection. In\nProc. 31st ICML. ACM, 2014.\n[JMD10]\nPrateek Jain, Raghu Meka, and Inderjit S. Dhillon. Guaranteed rank minimization via singular\nvalue projection. In Proc. 24th Neural Information Processing Systems (NIPS), pages 937\u2013945,\n2010.\n[JNS13]\nPrateek Jain, Praneeth Netrapalli, and Sujay Sanghavi. Low-rank matrix completion using\nalternating minimization. In Proc. 45th Symposium on Theory of Computing (STOC), pages\n665\u2013674. ACM, 2013.\n[JS10]\nMartin Jaggi and Marek Sulovsk\u00fd. A simple algorithm for nuclear norm regularized problems.\nIn Proc. 27th ICML, pages 471\u2013478. ACM, 2010.\n[JY09]\nShuiwang Ji and Jieping Ye. An accelerated gradient method for trace norm minimization. In\nProc. 26th ICML, page 58. ACM, 2009.\n[KBV09]\nYehuda Koren, Robert M. Bell, and Chris Volinsky. Matrix factorization techniques for recom-\nmender systems. IEEE Computer, 42(8):30\u201337, 2009.\n31\n[Kes12]\nRaghunandan H. Keshavan. E\ufb03cient algorithms for collaborative \ufb01ltering. PhD thesis, Stanford\nUniversity, 2012.\n[KMO10a] Raghunandan H. Keshavan, Andrea Montanari, and Sewoong Oh. Matrix completion from a few\nentries. IEEE Transactions on Information Theory, 56(6):2980\u20132998, 2010.\n[KMO10b] Raghunandan H. Keshavan, Andrea Montanari, and Sewoong Oh. Matrix completion from noisy\nentries. Journal of Machine Learning Research, 11:2057\u20132078, 2010.\n[LB10]\nK. Lee and Y Bresler.\nAdmira: Atomic decomposition for minimum rank approximation.\nInformation Theory, IEEE Transactions on, 56(9):4402\u20134416, 2010.\n[MHT10]\nRahul Mazumder, Trevor Hastie, and Robert Tibshirani. Spectral regularization algorithms for\nlearning large incomplete matrices. Journal of Machine Learning Research, 11:2287\u20132322, 2010.\n[Rec11]\nBenjamin Recht. A simpler approach to matrix completion. Journal of Machine Learning Research,\n12:3413\u20133430, 2011.\n[RFP10]\nBenjamin Recht, Maryam Fazel, and Pablo A. Parrilo. Guaranteed minimum-rank solutions of\nlinear matrix equations via nuclear norm minimization. SIAM Review, 52(3):471\u2013501, 2010.\n[RR13]\nBenjamin Recht and Christopher R\u00e9. Parallel stochastic gradient algorithms for large-scale\nmatrix completion. Mathematical Programming Computation, 5(2):201\u2013226, 2013.\n[SS90]\nGilbert W. Stewart and Ji-Guang Sun. Matrix Perturbation Theory. Academic Press London, 1990.\n[Ste01]\nG.W. Stewart. Matrix Algorithms. Volume II: Eigensystems. Society for Industrial and Applied\nMathematics, 2001.\n[Tro12]\nJoel A. Tropp. User-friendly tail bounds for sums of random matrices. Foundations of Computa-\ntional Mathematics, 12(4):389\u2013434, 2012.\nA\nDividing up \u2126\nIn this section, we show how to take a set \u2126\u2282[n] \u00d7 [n], so that each index (i,j) is included in \u2126with\nprobability p, and return subsets \u21261,...,\u2126L which follow a distribution more convenient for our analysis.\nAlgorithm 6 has the details. Observe that the \ufb01rst thing that Algorithm 6 does is throw away samples from\n\u2126. Thus, while this step is convenient for the analysis, and we include it for theoretical completeness, in\npractice it may be uneccessary\u2014especially if the assumption on the distribution of \u2126is an approximation to\nbegin with.\nThe correctness of Algorithm 6 follows from the following lemma, about the properties of Algorithm 7.\nLemma 12. Pick p1,...,p\u2113\u2208[0,1], and suppose that \u2126\u2282U includes each u \u2208U independently with probability\np1 \u2212QL\n\u2113=1(1 \u2212p\u2113). Then the sets \u21261,...,\u2126L returned by Algorithm 7 are distributed as follows. Each \u2126\u2113is\nindependent, and includes each u \u2208U independently with probability p\u2113.\nProof. Let D denote the distribution we would like to show that that \u2126\u2113follow; so we want to show that the\nsets returned by Algorithm 7 are distributed according to D.\nLet PA {\u00b7} denote the probability of an event\noccuring in Algorithm 7, and let and PD {\u00b7} denote the probability of an event occuring under the target\ndistribution D. Let Nu be the random variable that counts the number of times u occurs between \u21261,...,\u2126\u2113.\nThen observe that by de\ufb01nition,\nqr = PD {Nu = r|Nu \u22651},\nand\np = PD {Nu \u22651}.\n32\nAlgorithm 6: SplitUp: Split a set of indices \u2126(as in the input to Algorithm 1) into subsets \u21261,...,\u2126t\nwhose distributions are convenient for our analysis.\nInput: Parameters p1,...,pL, and a set \u2126\u2282[n] \u00d7 [n] so that each index (i,j) is included in \u2126\nindependently with probability p = P\n\u2113p\u2113.\nOutput: Subset \u21261,...,\u2126L \u2282\u2126so that each index (i,j) is included in \u2126\u2113independently with\nprobability p\u2113, and so that all of the \u2113are independent.\n1 Choose\np\u2032 = 1 \u2212\nL\nY\n\u2113=1\n(1 \u2212p\u2113).\nObserve that p\u2032 \u2264p.\n2 Let \u2126\u2032 be a set that includes each element of \u2126independently with probability p\u2032/p.\n3 return SubSample( p1,...,pL, [n] \u00d7 [n], \u2126\u2032 )\nAlgorithm 7: SubSample: Divide a random set \u2126into L subsets \u21261,...,\u2126L\nInput: Parameters p1,...,pL, a universe U, and a set \u2126\u2282U, so that each element u \u2208U is included\nindependently with probability p = 1 \u2212QL\n\u2113=1(1 \u2212p\u2113).\nOutput: Set \u21261,...,\u2126L \u2282U, so that each entry is included in \u2126\u2113idependendently with probability p\u2113,\nand so that \u21261,...,\u2126L are independent.\n1 For r \u2208{1,...,L}, let\nqr = 1\np\nX\nS\u2282U,|S|=r\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\nY\n\u2113\u2208S\np\u2113\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\nY\n\u2113<S\n(1 \u2212p\u2113)\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8.\nThen PL\nr=1 qr = 1.\n2 Initialize L empty sets \u21261,...,\u2126L.\n3 for u \u2208\u2126do\n4\nDraw r \u2208{1,...,L} with probability qr.\n5\nDraw a random set T \u2282[L] of size r.\n6\nAdd u to \u2126\u2113for each \u2113\u2208T .\n7 end\n8 return \u21261,...,\u2126L\n33\nWe aim to show PA {\u00b7} = PD {\u00b7}. First, \ufb01x u \u2208U, and \ufb01x any set S \u2282[L], and consider the event\nE(u,S) = (\u2200\u2113\u2208S,u \u2208\u2126\u2113) \u2227(\u2200\u2113< S,u < \u2126\u2113).\nWe compute PA {E(u,S)}.\nPA {E(u,S)} = PA {u \u2208\u2126}\nL\nX\nr=1\nqrPA {The random set T of size r is precisely S}\n= PD {Nu \u22651}\nL\nX\nr=1\nPD {Nu = r|Nu \u22651}P{ A random subset of [L] size r is precisely S}\n=\nL\nX\nr=1\nPD {Nu = r}P{A random subset of [L] of size r is precisely S}\n=\nL\nX\nr=1\nPD {Nu = r}PD {E(u,S)|Nu = r}\n= PD {E(u,S)}.\nNext, we observe that for any \ufb01xed S, the events {E(u,S)}u\u2208U are independent under the distribution induced\nby Algorithm 7. This follows from the fact that in all of the random steps (including the generation of \u2126\nand within Algorithm 7), the u \u2208U are treated independently. Notice that these events are also independent\nunder D by de\ufb01nition.\nNow, for any instantiation \u20d7\u2126\u2032 = (\u2126\u2032\n1,...,\u2126\u2032\nL) of the random variables (\u21261,...,\u2126L), consider the event\nE(\u20d7\u2126\u2032) = \u2200\u2113,\u2126\u2113= \u2126\u2032\n\u2113.\nWe have\nPA\nn\nE(\u20d7\u2126\u2032)\no\n= PA\nn\n\u2200u,E(u,\nn\n\u2113: u \u2208\u2126\u2032\n\u2113\no\n)\no\n=\nY\nu\u2208U\nPA\nn\nE(u,\nn\n\u2113: u \u2208\u2126\u2032\n\u2113\no\n)\no\nby independence in Alg. 7\n=\nY\nu\u2208U\nPD\nn\nE(u,\nn\n\u2113: u \u2208\u2126\u2032\n\u2113\no\n)\no\nby the above derivation\n= PD\nn\nE(\u20d7\u2126\u2032)\no\nby independence under D\nThus the probability of any outcome \u20d7\u2126\u2032 is the same under D and under Algorithm 7, and this completes the\nproof of the lemma.\nB\nUseful statements\nIn this appendix, we collect a few useful statements upon which we rely.\nB.1\nCoherence bounds\nFirst, we record some consequences of the bound (4) on the coherence of A. We always have\n\u2225A\u2225\u221e\u2264\u2225M\u2225\u221e+ \u2225N\u2225\u221e\u2264max\ni,j |eT\ni U\u039bUUT ej| + \u2225N\u2225\u221e\u2264\u03c31\n\u00b5\u2217k\nn + \u00b5\u2217\u2225N\u2225F\nn\n\u2264\u00b5\u2217\nn (k\u03c31 + \u2206),\n(33)\n34\nand similarly\nmax\ni\n\r\r\reT\ni A\n\r\r\r2 \u2264\nr\n\u00b5\u2217\nn\n\u0010\u221a\nk\u03c31 + \u2206\n\u0011\n.\n(34)\nIt will also be useful to notice that since\n\r\r\reT\ni U(>t)\r\r\r2 \u2264\n\r\r\reT\ni U\n\r\r\r2 , (4) implies that for all t,\n\u2225Nt\u2225\u221e\u2264\n\r\r\rM(>t)\r\r\r\u221e+ \u2225N\u2225\u221e\u2264\u00b5\u2217\nn\n\u0010\nk\u03c3rt+1 + \u2206\n\u0011\n.\n(35)\nB.2\nPerturbation statements\nNext, we will use the following lemma about perturbations of singular values, due to Weyl.\nLemma 13. Let N,E \u2208Rn\u00d7n, and let e\nN = N + E. Let \u03c31 \u2265\u03c32 \u2265\u00b7\u00b7\u00b7 \u2265\u03c3n denote the singular values of N, and\nsimilarly let e\u03c3i denote the singular values of e\nN. Then for all i, |\u03c3i \u2212e\u03c3i| \u2264\u2225E\u2225.\nIn order to compare the singular vectors of a matrix A with those of a perturbed version e\nA, we will\n\ufb01nd the following theorem helpful. We recall that for subspaces U,V , sin\u03b8(U,V ) refers to the sine of the\nprincipal angle between U and V . (See [SS90] for more on principal angles).\nTheorem 14 (Thm. 4.4 in [SS90]). Suppose that A has the singular value decomposition\nA =\nh\nU1\nU2\ni \"\n\u03a31\n\u03a32\n# \"\nV T\n1\nV T\n2\n#\n,\nand let e\nA = A + E be a perturbed matrix with SVD\nA =\nh e\nU1\ne\nU2\ni \"e\u03a31\ne\u03a32\n# \"e\nV T\n1\ne\nV T\n2\n#\n.\nLet\nR = Ae\nV1 \u2212e\nU1e\u03a31\nand\nS = AT e\nU1 \u2212e\nV1e\u03a31.\nSuppose there are numbers \u03b1,\u03b4 > 0 so that \u03c3min(f\n\u03a31) \u2265\u03b1 + \u03b4 and \u03c3max(\u03a32) \u2264\u03b1. Then,\nmax{sin\u0398(U1,V1),sin\u0398(U2,V2)} \u2264max{\u2225R\u2225,\u2225S\u2225}\n\u03b4\n.\nWe will also use the fact that if the angle between (the subspaces spanned by) two matrices is small, then\nthere is some unitary transformation so that the two matrices are close.\nLemma 15. Let U,V \u2208Rn\u00d7k have orthonormal columns, and suppose that sin\u03b8(U,V ) \u2264\u03b5 for some \u03b5 < 1/2. Then\nthere is some unitary matrix Q \u2208Rk\u00d7k so that \u2225UQ \u2212V \u2225\u22642\u03b5.\nProof. We have V = \u03a0UV + \u03a0U\u22a5V = U(UT V ) + \u03a0U\u22a5V . Since sin\u03b8(U,V ) \u2264\u03b5, we have\n\r\r\r\u03a0U\u22a5V\n\r\r\r \u2264\u03b5, and\n\u03c3k(UT V ) = cos\u03b8(U,V ) \u2265\n\u221a\n1 \u2212\u03b52. Thus, we can write UT V = Q + E, where \u2225E\u2225\u22641 \u2212\n\u221a\n1 \u2212\u03b52. The claim\nfollows from the triangle inequality.\nB.3\nSubspace Iteration\nOur algorithm uses the following standard version of the well-known Subspace Iteration algorithm\u2014also\nknown as Power Method.\nWe have the following theorem about the convergence of SubsIt.\n35\nAlgorithm 8: SubsIt (A,k,L) (Subspace Iteration)\nInput: Matrix A, target rank k, number of iterations L\n1 S0 \u2208Rn\u00d7k \u2190random matrix with orthogonal rows;\n2 for \u2113= 1,...,L do\n3\nR\u2113\u2190AS\u2113\u22121;\n4\nS\u2113\u2190QR(R\u2113);\n5 end\n6 for i = 1,...,k do\n7\ne\u03c32\ni \u2190(RL)T\ni AT A(RL)i\n// (RL)i is the i-th column of RL\n8 end\n9 return (RL,\u20d7e\u03c3);\nOutput: A matrix R \u2208Rn\u00d7k approximating the top k singular vectors of A, and estimates e\u03c31,..., e\u03c3k of\nthe singular values.\nTheorem 16. Let A \u2208Rn\u00d7n be any matrix, with singular values \u03c31 \u2265\u03c32 \u2265\u00b7\u00b7\u00b7 \u2265\u03c3n. Let RL \u2208Rn\u00d7k be the matrix\nwith orthonormal columns returned after L iterations of SubsIt (Algorithm 8) with target rank k. for some suitably\nsmall parameter \u03b3 < 1. Then the values e\u03c3i = (Ri)T ARi satisfy\n|e\u03c3i \u2212\u03c3i| \u2264\u03c3i\n\u0010\n1 \u2212(1 \u2212\u03b3)k\u0011\n+ 2n\u03c31 (1 \u2212\u03b3)L .\nIn particular, if \u03b3 = o(1/k) and if L = C log(n)/\u03b3 then with probability 1 \u22121/poly(n),\n|e\u03c3i \u2212\u03c3i| \u2272\u03c31\nn + \u03c3ik\u03b3 \u2272\u03c31k\u03b3.\nProof. Let r1 \u2264r2 \u2264\u00b7\u00b7\u00b7 \u2264rt be the indices r \u2264k so that \u03c3r+1/\u03c3r \u22641 \u2212\u03b3. Notice that we may assume without\nloss of generality that rt = k. Indeed, the result of running SubsIt with target rank k is the same as the result\nof running SubsIt with a larger rank and restricting to the \ufb01rst k columns of R\u2113. Write A = P\nj U(j)\u03a3jV (j),\nwhere \u03a3j contains the singular values \u03c3rj+1,...,\u03c3rj+1. Then using [Ste01, Chapter 6, Thm 1.1] and deviation\nbounds for the principal angle between a random subspace and \ufb01xed subspace, we have\nPr\n\u001a\nsin\u03b8\n\u0012\nU(j),R(j)\nL\n\u0013\n\u2264Cnc (1 \u2212\u03b3)L\u001b\n\u22651 \u22121/nc\u2032 .\nHere, c\u2032 can be made any constant by increasing c and C is an absolute constant. Fix i and let xi = (RL)i\ndenote the i \u2212th column of RL. Suppose that i \u2208\nn\nrj + 1,...,rj+1\no\n. Then, the estimates e\u03c3i satisfy\ne\u03c3i = xT\ni AT Axi =\n\r\r\rA(j)xi\n\r\r\r2 +\nX\ns,j\n\r\r\rA(s)xi\n\r\r\r\n2\n2 .\nThe second term satis\ufb01es\nX\ns,j\n\r\r\rA(s)xi\n\r\r\r\n2\n2 \u2264\u03c32\n1 sin2 \u03b8(U(s),R(s)\nL ) \u2264\u03c32\n1 n2(1 \u2212\u03b3)2L.\nThe \ufb01rst term has\n\r\r\rA(j)xi\n\r\r\r\n2\n2 \u2264\n\r\r\rA(j)\r\r\r\n2 = \u03c32\nrj+1\nand\n\r\r\rA(j)xi\n\r\r\r\n2\n2 \u2265cos2 \u03b8\n\u0012\nU(s),R(s)\nL\n\u0013\n\u00b7 \u03c3min(A(j)) \u2265\n\u0010\n1 \u2212n2(1 \u2212\u03b3)2L\u0011\n\u00b7 \u03c32\nrj.\n36\nBy de\ufb01nition, as there are no signi\ufb01cant gaps between \u03c3rj+1 and \u03c3rj, we have\n\u03c3rj+1\n\u03c3rj+1\n\u2265(1 \u2212\u03b3)k,\nand so this completes the proof after collecting terms.\nB.4\nMatrix concentration inequalities\nWe will repeatedly use the Matrix Bernstein and Matrix Cherno\ufb00inequalities; we use the versions from [Tro12]:\nLemma 17. [Matrix Bernstein [Tro12]] Consider a \ufb01nite sequence {Zk} of independent, random, d \u00d7 d matrices.\nAssume that each matrix satis\ufb01es\nEXk = 0,\n\u2225Xk\u2225\u2264R almost surely.\nDe\ufb01ne\n\u03c32 := max\n\uf8f1\uf8f4\uf8f4\uf8f2\uf8f4\uf8f4\uf8f3\n\r\r\r\r\r\r\r\nX\nk\nEXkXT\nk\n\r\r\r\r\r\r\r\n,\n\r\r\r\r\r\r\r\nX\nk\nEXT\nk Xk\n\r\r\r\r\r\r\r\n\uf8fc\uf8f4\uf8f4\uf8fd\uf8f4\uf8f4\uf8fe.\nThen, for all t \u22650,\nP\n\uf8f1\uf8f4\uf8f4\uf8f2\uf8f4\uf8f4\uf8f3\n\r\r\r\r\r\r\r\nX\nk\nXk\n\r\r\r\r\r\r\r\n\u2265t\n\uf8fc\uf8f4\uf8f4\uf8fd\uf8f4\uf8f4\uf8fe\u22642d exp\n \u2212t2/2\n\u03c32 + R/3\n!\n.\nOne corollary of Lemma 17 is the following lemma about the concentration of the matrix P\u2126(A).\nLemma 18. Suppose that A \u2208Rn\u00d7n and let \u2126\u2282[n] \u00d7 [n] be a random subset where each entry is included\nindependently with probability p. Then\nP{\u2225P\u2126(A) \u2212A\u2225> u} \u22642nexp\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\n\u2212u2/2\n\u0010 1\np \u22121\n\u0011\u0012\nmaxi\n\r\r\reT\ni A\n\r\r\r2\n2 + u\n3 \u2225A\u2225\u221e\n\u0013\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\n.\nProof. Let \u03beij be independent Bernoulli-p random variables, which are 1 if (i,j) \u2208\u2126and 0 otherwise.\nP\u2126(A) \u2212A =\nX\ni,j\n \u03beij\np \u22121\n!\nAi,jeieT\nj ,\nwhich is a sum of independent random matrices. Using the Matrix Bernstein inequality, Lemma 17, we\nconclude that\nP{\u2225P\u2126(A) \u2212A\u2225> u} \u22642nexp\n \n\u2212u2/2\n\u03c32 + Ru/3\n!\n,\nwhere\n\u03c32 =\n\r\r\r\r\r\r\r\r\nE\nX\ni,j\n \u03beij\np \u22121\n!2\nA2\ni,jeieT\nj ejeT\ni\n\r\r\r\r\r\r\r\r\n=\n 1\np \u22121\n!\nmax\ni\n\u2225Ai\u22252\n2\nand\n\r\r\r\r\r\r\n \u03beij\np \u22121\n!\nAi,jeieT\nj\n\r\r\r\r\r\r \u2264R =\n 1\np \u22121\n!\n\u2225A\u2225\u221e\nalmost surely. This concludes the proof.\n37\nFinally, we will use the Matrix Cherno\ufb00inequality.\nLemma 19. [Matrix Cherno\ufb00[Tro12]] Consider a \ufb01nite sequence {Xk} of independent, self-adjoint, d \u00d7d matrices.\nAssume that each Xk satis\ufb01es\nXk \u227d0,\n\u03bbmax(Xk) \u2264R almost surely.\nDe\ufb01ne\n\u00b5min := \u03bbmin\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\n\r\r\r\r\r\r\r\nX\nk\nEXk\n\r\r\r\r\r\r\r\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8,\n\u00b5max := \u03bbmax\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\n\r\r\r\r\r\r\r\nX\nk\nEXk\n\r\r\r\r\r\r\r\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8.\nThen for \u03b4 \u2208(0,1),\nP\n\uf8f1\uf8f4\uf8f4\uf8f2\uf8f4\uf8f4\uf8f3\u03bbmin\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\nX\nk\nXk\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\u2264(1 \u2212\u03b4)\u00b5min\n\uf8fc\uf8f4\uf8f4\uf8fd\uf8f4\uf8f4\uf8fe\u2264d exp(\u2212\u03b42\u00b5min/2R)\nand\nP\n\uf8f1\uf8f4\uf8f4\uf8f2\uf8f4\uf8f4\uf8f3\u03bbmax\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\nX\nk\nXk\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\u2265(1 + \u03b4)\u00b5max\n\uf8fc\uf8f4\uf8f4\uf8fd\uf8f4\uf8f4\uf8fe\u2264d exp(\u2212\u03b42\u00b5max/3R).\nB.5\nMedians of vectors\nFor v \u2208Rk, let median(v) be the entry-wise median.\nLemma 20. Suppose that v(s), for s = 1,...,T are i.i.d. random vectors, so that for all s,\nP\n\u001a\r\r\rv(s)\r\r\r\n2\n2 > \u03b1\n\u001b\n\u22641/5.\nThen\nP\n\u001a\r\r\rmedian(v(s))\n\r\r\r\n2\n2 > 4\u03b1\n\u001b\n\u2264exp(\u2212\u2126(T )).\nProof. Let S \u2282[T ] be the set of s so that\n\r\r\rv(s)\r\r\r2\n2 \u2264\u03b1. By a Cherno\ufb00bound,\nP\n\u001a\n|S| \u22643T\n4\n\u001b\n= P\n\uf8f1\uf8f4\uf8f4\uf8f2\uf8f4\uf8f4\uf8f3\nT\nX\ns=1\n1\u2225v(s)\u2225\n2\n2>\u03b1 > T\n4\n\uf8fc\uf8f4\uf8f4\uf8fd\uf8f4\uf8f4\uf8fe\u2264exp(\u2212\u2126(T )).\nSuppose that the likely event occurs, so |S| > 3T/4. For j \u2208[k], let\nSj =\n\u001a\ns \u2208S : (v(s)\nj )2 \u2265medians((v(s)\nj )2)\n\u001b\n.\nBecause |S| > 3T /4, we have |Sj| \u2265T /4. Then\n\r\r\r\rmedians(v(s)\nj )\n\r\r\r\r\n2\n2 =\nn\nX\nj=1\nmedians\n\u0012\n(v(s)\nj )2\u0013\n\u2264\nn\nX\nj=1\n1\n|Sj|\nX\ns\u2208Sj\n(v(s)\nj )2\n\u2264\nn\nX\nj=1\n4\nT\nX\ns\u2208Sj\n(v(s)\nj )2 \u2264\nn\nX\nj=1\n4\nT\nX\ns\u2208S\n(v(s)\nj )2 \u22644\nT\nX\ns\u2208S\n\r\r\rv(s)\r\r\r\n2\n2 \u22644|S|\u03b1\nT\n\u22644\u03b1.\nThis completes the proof.\n38\nC\nProof of Lemma 9\nIn this section, we prove Lemma 9, which bounds the noise matrices G(s)\n\u2113. The proof of Lemma 9 is similar\nto the analysis in [Har13a], Lemmas 4.2 and 4.3. For completeness, we include the details here. Following\nRemark 1, we assume that sets \u2126(s)\n\u2113are independent random sets, which include each index independently\nwith probability\np\u2032 :=\np\u2032\nt\nsmaxLt\n.\nConsider each noise matrix G(s)\n\u2113, as in (28). In Lemma 4.2 in [Har13a], an explicit expression for G(s)\n\u2113\nis\nderived:\nProposition 21. Let G(s)\n\u2113\nbe as in (28). Then we have\nG(s)\n\u2113= (G(s)\n\u2113)M + (G(s)\n\u2113)N,\nwhere\neT\ni (G(s)\n\u2113)M = eT\ni Mt(I \u2212R\u2113\u22121RT\n\u2113\u22121)P(s)\ni\nR\u2113\u22121(B(s)\ni )\u22121.\nand\neT\ni (G(s)\n\u2113)N = eT\ni\n\u0012\nNtP(s)\ni\nR\u2113\u22121(B(s)\ni )\u22121 \u2212NtR\u2113\u22121\n\u0013\n.\nAbove, P(s)\ni\nis the projection onto the coordinates j so that (i,j) \u2208\u2126(s)\n\u2113, and\nB(s)\ni\n= RT\n\u2113\u22121P(s)\ni\nR\u2113\u22121.\nWe \ufb01rst bound the expression for (G(s)\n\u2113)M in terms of the decomposition in Proposition 21. Let\nDj\n\u2113\u22121 = (I \u2212R\u2113\u22121RT\n\u2113\u22121)U(j).\nThus, Dj\n\u2113\u22121 is similar to Ej\n\u2113\u22121, and more precisely we have\n\r\r\r\rDj\n\u2113\u22121\n\r\r\r\r \u2264\n\r\r\r\rEj\n\u2113\u22121\n\r\r\r\r.\n(36)\nTo see (36), observe that (dropping the \u2113subscripts for readability)\n\r\r\rEj\r\r\r =\nmax\n\u2225x\u22252=1,\u2225y\u22252=1\nxT Ejy\n= max\nx,y xT\n\"\n(R(j+1:t))T U(<j)\n(R(j+1:t))T U(j)\n(R\u22a5)T U(<j)\n(R\u22a5)T U(j)\n#\ny\n\u2265\nmax\nx=(0,x\u2032),y=(0,y\u2032)(x\u2032)T (R\u22a5)T U(j)y\u2032\n=\n\r\r\rDj\r\r\r\nFirst, we observe that with very high probability, B(s)\ni\nis close to the identity.\nClaim 22. There is a constant C so that the following holds. Suppose that p\u2032 \u2265Ck\u00b5t log(n)/(n\u03b42). Then\nP\n\u001a\n\u03bbmin(B(s)\ni ) \u22641 \u2212\u03b4/2 or \u03bbmax(B(s)\ni ) \u22651 + \u03b4/2\n\u001b\n\u22641/n5.\n39\nProof. We write\nB(s)\ni\n= RT\n\u2113\u22121P(s)\ni\nR\u2113\u22121 =\nn\nX\nr=1\n1\np\u2032 \u03ber(RT\n\u2113\u22121er)(eT\nr R\u2113\u22121),\nwhere \u03ber is 1 with probability p\u2032 and 0 otherwise. We apply the Matrix Cherno\ufb00bound (Lemma 19); we have\n\r\r\r\r\r\n1\np\u2032 \u03ber(RT\n\u2113\u22121er)(eT\nr R\u2113\u22121)\n\r\r\r\r\r \u2264\n\r\r\reTr R\u2113\u22121\n\r\r\r2\n2\np\u2032\n\u2264\u00b5tk\nnp\u2032\nalmost surely,\nand \u03bbmin(EB(s)\ni ) = \u03bbmax(EB(s)\ni ) = 1. Then Lemma 19 implies that\nP\n\u001a\n\u03bbmin(B(s)\ni ) \u22641 \u2212\u03b4/2 or \u03bbmax(B(s)\ni ) \u22651 + \u03b4/2\n\u001b\n\u2264nexp(\u2212\u03b42p\u2032n/(8\u00b5tk)) + nexp(\u2212\u03b42p\u2032n/(12\u00b5tk)).\nThe claim follows from the choice of p\u2032.\nNext, we will bound the other part of the expression for (G(s)\n\u2113)M in Proposition 21.\nClaim 23. There is a constant C so that the following holds. Suppose that p\u2032 \u2265C\u00b5tk\nn\u03b42 . Then for each s,\nP\n\uf8f1\uf8f4\uf8f4\uf8f4\uf8f2\uf8f4\uf8f4\uf8f4\uf8f3\n\r\r\r\reT\ni Mt(I \u2212R\u2113\u22121R\u2113\u22121)T P(s)\ni\nR\u2113\u22121\n\r\r\r\r2 \u2265\u03b4\n4\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\nt\nX\nj=1\n\r\r\reT\ni M(j)\r\r\r2\n\r\r\r\rEj\n\u2113\u22121\n\r\r\r\r\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\n\uf8fc\uf8f4\uf8f4\uf8f4\uf8fd\uf8f4\uf8f4\uf8f4\uf8fe\n\u22641\n20.\nProof. We compute the expectation of\n\r\r\r\reT\ni Mt(I \u2212R\u2113\u22121R\u2113\u22121)T P(s)\ni\nR\u2113\u22121\n\r\r\r\r2 and use Markov\u2019s inequality. For the\nproof of this claim, let Y = Mi(I \u2212R\u2113\u22121RT\n\u2113\u22121).\nE\n\r\r\r\reT\ni YP(s)\ni\nR\u2113\u22121\n\r\r\r\r\n2\n2 = EeT\ni YP(s)\ni\nR\u2113\u22121RT\n\u2113\u22121P(s)\ni\nY T ei\n= eT\ni YE\n\u0012\nP(s)\ni\nR\u2113\u22121RT\n\u2113\u22121P(s)\ni\n\u0013\nY T ei\n= eT\ni Y\n \nR\u2113\u22121RT\n\u2113\u22121 +\n 1\np\u2032 \u22121\n!\ndiagr\n\u0012\r\r\reT\nr R\u2113\u22121\n\r\r\r2\n2\n\u0013!\nY T ei\n=\n\r\r\reT\ni YR\u2113\u22121\n\r\r\r2\n2 +\n 1\np\u2032 \u22121\n!\nn\nX\nr=1\n\r\r\reT\nr R\u2113\u22121\n\r\r\r2\n2 (Yi,r)2\n=\n 1\np\u2032 \u22121\n!\nn\nX\nr=1\n\r\r\reT\nr R\u2113\u22121\n\r\r\r2\n2 (Yi,r)2\n\u2264\n\r\r\reT\ni Y\n\r\r\r2\n2\n 1\np\u2032 \u22121\n! \u00b5tk\nn\n!\n\u2264\n\u03b42 \r\r\reT\ni Y\n\r\r\r2\n2\n400\n,\nusing the fact that YR\u2113\u22121 = 0, and \ufb01nally our choice of p\u2032 (with an appropriately large constant C). Now,\nusing (36),\n\r\r\reT\ni Y\n\r\r\r2 =\n\r\r\reT\ni U(\u2264t)\u039b(t)U(\u2264t)(I \u2212R\u2113\u22121RT\n\u2113\u22121)\n\r\r\r2 \u2264\nt\nX\nj=1\n\r\r\reT\ni M(j)\r\r\r2\n\r\r\r\rDj\n\u2113\u22121\n\r\r\r\r \u2264\nt\nX\nj=1\n\r\r\reT\ni M(j)\r\r\r2\n\r\r\r\rEj\n\u2113\u22121\n\r\r\r\r.\nAlong with Markov\u2019s inequality, this completes the proof.\n40\nFinally, we control the term (G(s)\n\u2113)N.\nClaim 24. There is a constant C so that the following holds. Suppose that p\u2032 \u2265Ck log(n)\u00b5t/(\u03b42n) for a constant C.\nThen for each s \u2264T ,\nP\n\u001a\r\r\r\reT\ni (G(s)\n\u2113)N\r\r\r\r2 \u2265\u03b4\n4\n\r\r\reT\ni Nt\n\r\r\r2\n\u001b\n\u22641\n15.\nProof. Using Proposition 21,\neT\ni (G(s)\n\u2113)N = eT\ni\n \nNtP(s)\ni\nR\u2113\u22121\n\u0012\nB(s)\ni\n\u0013\u22121\n\u2212NtR\u2113\u22121\n!\n= eT\ni\n\u0012\nNtP(s)\ni\nR\u2113\u22121 \u2212NtR\u2113\u22121B(s)\ni\n\u0013\n(B(s)\ni )\u22121\n=\n\u0012\neT\ni Nt(P(s)\ni\n\u2212I)R\u2113\u22121 + eT\ni NtR\u2113\u22121(I \u2212B(s)\ni )\n\u0013\u0012\nB(s)\ni\n\u0013\u22121\n=: (y1 + y2)\n\u0012\nB(s)\ni\n\u0013\u22121\n.\nWe have already bounded\n\r\r\r\r(B(s)\ni )\u22121\n\r\r\r\r with high probability in Claim 22, when the bound on p\u2032 holds, and so\nwe now bound\n\r\r\ry1\n\r\r\r2 and\n\r\r\ry2\n\r\r\r2 with decent probability. As we did in Claim 23, we compute the expectation\nof\n\r\r\ry1\n\r\r\r2\n2 and use Markov\u2019s inequality.\nE\n\r\r\ry1\n\r\r\r2\n2 = E\n\r\r\r\r\reT\ni Nt\n\u0012\nP(s)\ni\n\u2212I\n\u0013\nR\u2113\u22121\n\r\r\r\r\r\n2\n2\n= eT\ni NtE\n\u0014\n(P(s)\ni\n\u2212I)R\u2113\u22121RT\n\u2113\u22121(P(s)\ni\n\u2212I)\n\u0015\nNT\nt ei\n= eT\ni Nt\n 1\np\u2032 \u22121\n!\ndiagr(\n\r\r\reT\nr R\u2113\u22121\n\r\r\r2\n2)NT\nt ei\n=\n 1\np\u2032 \u22121\n!\nn\nX\nr=1\n(Nt)2\nir\n\r\r\reT\nr R\u2113\u22121\n\r\r\r2\n2\n\u2264\n \u00b5tk\nnp\u2032\n!\r\r\reT\ni Nt\n\r\r\r2\n2 .\nThus, by Markov\u2019s inequality, we have\nP\n\uf8f1\uf8f4\uf8f4\uf8f2\uf8f4\uf8f4\uf8f3\n\r\r\ry1\n\r\r\r2 \u226520\ns\n\u00b5tk\nnp\u2032\n\r\r\reT\ni Nt\n\r\r\r2\n\uf8fc\uf8f4\uf8f4\uf8fd\uf8f4\uf8f4\uf8fe\u22641\n20.\nNext, we turn our attention to the second term\n\r\r\ry2\n\r\r\r2. We have\n\r\r\ry2\n\r\r\r2 =\n\r\r\r\r\reT\ni (NtR\u2113\u22121)\n\u0012\nI \u2212B(s)\ni\n\u0013\r\r\r\r\r2\n\u2264\n\r\r\reT\ni NtR\u2113\u22121\n\r\r\r2\n\r\r\r\rI \u2212B(s)\ni\n\r\r\r\r.\nBy Claim 22, we established that with probability 1 \u22121/n5,\n\r\r\r\rI \u2212(B(s)\ni )\n\r\r\r\r \u2264\u03b4\n2, with our choice of p\u2032. Thus, with\nprobability at least 1 \u22121/n5,\n\r\r\ry2\n\r\r\r2 \u2264\u03b4\n\r\r\reT\ni NtR\u2113\u22121\n\r\r\r2 \u2264\u03b4\n2\n\r\r\reT\ni Nt\n\r\r\r2 .\nAltogether, we conclude that with probability at least 1 \u22121/20 \u22122/n5, we have\n\r\r\r\reT\ni (G(s)\n\u2113)N\r\r\r\r2 \u2264\n\u0010\r\r\ry1\n\r\r\r2 +\n\r\r\ry2\n\r\r\r2\n\u0011\r\r\r\r(B(s)\ni )\u22121\r\r\r\r \u2264\n3\u03b4\n4(1 \u2212\u03b4/2)\n\r\r\reT\ni Nt\n\r\r\r2 \u2264\u03b4\n\r\r\reT\ni Nt\n\r\r\r2\nas long as \u03b4 \u22641/2. This proves the claim.\n41\nPutting Claims 22, 23 and 24 together, along with the choice of p\u2032\nt = Ltsmaxp\u2032, we conclude that, for each\ns \u2208[T ] and for any \u03b4 < 1/2,\nP\n\uf8f1\uf8f4\uf8f4\uf8f4\uf8f2\uf8f4\uf8f4\uf8f4\uf8f3\n\r\r\r\reT\ni G(s)\n\u2113\n\r\r\r\r2 \u2265\n\u03b4\n4(1 \u2212\u03b4/2)\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\n\r\r\reT\ni Nt\n\r\r\r2 +\nt\nX\nj=1\n\r\r\reT\ni M(j)\r\r\r2\n\r\r\r\rEj\n\u2113\u22121\n\r\r\r\r\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\n\uf8fc\uf8f4\uf8f4\uf8f4\uf8fd\uf8f4\uf8f4\uf8f4\uf8fe\n\u22641\n5.\n(37)\nThis implies that\n\r\r\reT\ni G\u2113\n\r\r\r2 =\n\r\r\r\reT\ni medians G(s)\n\u2113\n\r\r\r\r2 =\n\r\r\r\rmedians(eT\ni G(s)\n\u2113)\n\r\r\r\r2\nis small with exponentially large probability. Indeed, by Lemma 20,\nP\n\uf8f1\uf8f4\uf8f4\uf8f4\uf8f2\uf8f4\uf8f4\uf8f4\uf8f3\n\r\r\reT\ni G\u2113\n\r\r\r2 \u2265\n\u03b4\n2(1 \u2212\u03b4/2)\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\n\r\r\reT\ni Nt\n\r\r\r2 +\nt\nX\nj=1\n\r\r\reT\ni M(j)\r\r\r2\n\r\r\r\rEj\n\u2113\u22121\n\r\r\r\r\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\n\uf8fc\uf8f4\uf8f4\uf8f4\uf8fd\uf8f4\uf8f4\uf8f4\uf8fe\n\u2264exp(\u2212csmax),\nfor some constant c. By the choice of smax, the failure probability is at most 1/n6, and a union bound over all\ni shows that, with probability at least 1 \u22121/n5,\n\r\r\reT\ni G\u2113\n\r\r\r2 \u2264\u03b4\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\n\r\r\reT\ni Nt\n\r\r\r2 +\nt\nX\nj=1\n\r\r\reT\ni M(j)\r\r\r2\n\r\r\r\rEj\n\u2113\u22121\n\r\r\r\r\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8= \u03c9(i)\n\u2113\u22121.\n(38)\nThis was the second claim in Lemma 9. Now, we show that in the favorable case that (38) holds, so does the\n\ufb01rst claim of Lemma 9, and this will complete the proof of the lemma. Suppose that (38) holds. Then\n\u2225G\u2113\u2225F =\nv\nt n\nX\ni=1\n\r\r\reT\ni G\u2113\n\r\r\r2\n2\n\u2264\nv\nu\nu\nu\nt n\nX\ni=1\n\u03b42\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\n\r\r\reT\ni Nt\n\r\r\r2 +\nt\nX\nj=1\n\r\r\reT\ni M(j)\r\r\r2\n\r\r\r\rEj\n\u2113\u22121\n\r\r\r\r\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\n2\n\u2264\u03b4\nv\nt n\nX\ni=1\n\r\r\reT\ni Nt\n\r\r\r2\n2 + \u03b4\nv\nu\nu\nu\nt n\nX\ni=1\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\nt\nX\nj=1\n\r\r\reT\ni M(j)\r\r\r2\n\r\r\r\rEj\n\u2113\u22121\n\r\r\r\r\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\n2\n\u2264\u03b4\u2225Nt\u2225F + \u03b4\nv\nu\nu\nu\nt n\nX\ni=1\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\nt\nX\nj=1\n\r\r\reT\ni M(j)\r\r\r2\n\r\r\r\rEj\n\u2113\u22121\n\r\r\r\r\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\n2\n.\nNotice that, for any real numbers (ai,j), i \u2208[n],j \u2208[t], and for any real number bj, j \u2208[t], we have\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\nn\nX\ni=1\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\nt\nX\nj=1\nai,jbj\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\n2\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\n1/2\n= \u2225Ab\u22252 = max\n\u2225z\u22252=1zT Ab = max\n\u2225z\u22252=1\nt\nX\nj=1\n(zT Aej)bj\n\u2264\nt\nX\nj=1\nmax\nz(j) ((z(j))T Aej)bj =\nt\nX\nj=1\n\r\r\rAej\n\r\r\r2 bj =\nt\nX\nj=1\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\nn\nX\ni=1\na2\ni,j\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\n1/2\nbj.\n42\nThus, we may bound the second term above by\n\u03b4\nv\nu\nu\nu\nt n\nX\ni=1\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\nt\nX\nj=1\n\r\r\reT\ni M(j)\r\r\r2\n\r\r\r\rEj\n\u2113\u22121\n\r\r\r\r\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\n2\n\u2264\u03b4\nt\nX\nj=1\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\nn\nX\ni=1\n\r\r\reT\ni M(j)\r\r\r\n2\n2\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\n1/2 \r\r\r\rEj\n\u2113\u22121\n\r\r\r\r\n= \u03b4\nt\nX\nj=1\n\r\r\rM(j)\r\r\rF\n\r\r\r\rEj\n\u2113\u22121\n\r\r\r\r.\nAltogether, we conclude that, in the favorable case the (38) holds,\n\u2225G\u2113\u2225F \u2264\u03b4\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\u2225Nt\u2225F +\nt\nX\nj=1\n\r\r\rM(j)\r\r\rF\n\r\r\r\rEj\n\u2113\u22121\n\r\r\r\r\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8= \u03c9\u2113\u22121,\nas desired. This completes the proof of Lemma 9.\n43\n",
        "sentence": "",
        "context": "This simple description glosses over many details and there are a few challenges to be overcome in\norder to make the idea work. For example, we have not said how to determine the appropriate \u201cgaps\" dt.\n1\np\u2032 \u22121\n!\nn\nX\nr=1\n\r\r\reT\nr R\u2113\u22121\n\r\r\r2\n2 (Yi,r)2\n=\n 1\np\u2032 \u22121\n!\nn\nX\nr=1\n\r\r\reT\nr R\u2113\u22121\n\r\r\r2\n2 (Yi,r)2\n\u2264\n\r\r\reT\ni Y\n\r\r\r2\n2\n 1\np\u2032 \u22121\n! \u00b5tk\nn\n!\n\u2264\n\u03b42 \r\r\reT\ni Y\n\r\r\r2\n2\n400\n,\n\u2113\u22121\n\r\r\r\r\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\n2\n\u2264\u03b4\nv\nt n\nX\ni=1\n\r\r\reT\ni Nt\n\r\r\r2\n2 + \u03b4\nv\nu\nu\nu\nt n\nX\ni=1\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\nt\nX\nj=1\n\r\r\reT\ni M(j)\r\r\r2\n\r\r\r\rEj\n\u2113\u22121\n\r\r\r\r\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\n2\n\u2264\u03b4\u2225Nt\u2225F + \u03b4\nv\nu\nu\nu\nt n\nX\ni=1\n\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ed\nt\nX\nj=1\n\r\r\reT\ni M(j)\r\r\r2\n\r\r\r\rEj\n\u2113\u22121\n\r\r\r\r\n\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8\n2\n."
    },
    {
        "title": "Sums of random Hermitian matrices and an inequality by Rudelson",
        "author": [
            "R. Imbuzeiro Oliveira"
        ],
        "venue": "ArXiv e-prints,",
        "citeRegEx": "Oliveira.,? \\Q2010\\E",
        "shortCiteRegEx": "Oliveira.",
        "year": 2010,
        "abstract": "We give a new, elementary proof of a key inequality used by Rudelson in the\nderivation of his well-known bound for random sums of rank-one operators. Our\napproach is based on Ahlswede and Winter's technique for proving operator\nChernoff bounds. We also prove a concentration inequality for sums of random\nmatrices of rank one with explicit constants.",
        "full_text": "arXiv:1004.3821v1  [math.PR]  22 Apr 2010\nSums of random Hermitian matrices and an inequality\nby Rudelson\nRoberto Imbuzeiro Oliveira\u2217\nApril 23, 2010\nAbstract\nWe give a new, elementary proof of a key inequality used by Rudelson in the\nderivation of his well-known bound for random sums of rank-one operators.\nOur\napproach is based on Ahlswede and Winter\u2019s technique for proving operator Cherno\ufb00\nbounds. We also prove a concentration inequality for sums of random matrices of\nrank one with explicit constants.\n1\nIntroduction\nThis note mainly deals with estimates for the operator norm \u2225Zn\u2225of random sums\nZn \u2261\nn\nX\ni=1\n\u01ebiAi\n(1)\nof deterministic Hermitian matrices A1, . . . , An multiplied by random coe\ufb03cients. Recall\nthat a Rademacher sequence is a sequence {\u01ebi}n\ni=1 of i.i.d. random variables with \u01eb1 uniform\nover {\u22121, +1}.\nA standard Gaussian sequence is a sequence i.i.d.\nstandard Gaussian\nrandom variables. Our main goal is to prove the following result.\nTheorem 1 (proven in Section 3) Given positive integers d, n \u2208N, let A1, . . . , An be\ndeterministic d \u00d7 d Hermitian matrices and {\u01ebi}n\ni=1 be either a Rademacher sequence or a\nstandard Gaussian sequence. De\ufb01ne Zn as in (1). Then for all p \u2208[1, +\u221e),\nE [\u2225Zn\u2225p]1/p \u2264(\np\n2 ln(2d) + Cp)\n\r\r\r\r\r\nn\nX\ni=1\nA2\ni\n\r\r\r\r\r\n1/2\n\u2217IMPA, Rio de Janeiro, RJ, Brazil, 22430-040. rimfo@impa.br\n1\nwhere\nCp \u2261\n\u0012\np\nZ +\u221e\n0\ntp\u22121e\u2212t2\n2 dt\n\u00131/p\n(\u2264c\u221ap for some universal c > 0).\nFor d = 1, this result corresponds to the classical Khintchine inequalities, which give\nsub-Guassian bounds for the moments of Pn\ni=1 \u01ebiai (a1, . . . , an \u2208R). Theorem 1 is implicit\nin Section 3 of Rudelson\u2019s paper [11], albeit with non-explicit constants. The main Theorem\nin that paper is the following inequality, which is a simple corollary of Theorem 1: if\nY1, . . . , Yn are i.i.d. random (column) vectors in Cd which are isotropic (i.e E [Y1Y \u2217\n1 ] = I,\nthe d \u00d7 d identity matrix), then:\nE\n\"\r\r\r\r\r\n1\nn\nn\nX\ni=1\nYiY \u2217\ni \u2212I\n\r\r\r\r\r\n#\n\u2264C E\n\u0002\n|Y1|log n\u00031/ log n\nr\nlog d\nn\n(2)\nfor some universal C > 0, whenever the RHS of the above inequality is at most 1. This\nimportant result has been applied to several di\ufb00erent problems, such as bringing a convex\nbody to near-isotropic position [11]; the analysis of for low-rank approximations of matrices\n[12, 6] and graph sparsi\ufb01cation [13]; estimating of singular values of matrices with inde-\npendent rows [10]; analysing compressive sensing [3]; and related problems in Harmonic\nAnalysis [16, 15].\nThe key ingredient of the original proof of Theorem 1 is a non-commutative Khintchine\ninequality by Lust-Picard and Pisier [9]. This states that there exists a universal c > 0\nsuch that for all Zn as in the Theorem, all p \u22651 and all d \u00d7 d matrices {Bi, Di}n\ni=1 with\nBi + Di = Ai, 1 \u2264i \u2264n,\nE [\u2225Zn\u2225p\nSp]1/p \u2264c\u221ap\n\uf8eb\n\uf8ed\n\r\r\r\r\r\nn\nX\ni=1\nBiB\u2217\ni\n\r\r\r\r\r\n1/2\nSp\n+\n\r\r\r\r\r\nn\nX\ni=1\nD\u2217\ni Di\n\r\r\r\r\r\n1/2\nSp\n\uf8f6\n\uf8f8,\nwhere \u2225\u00b7 \u2225Sp denotes the p-th Schatten norm: \u2225A\u2225p\nSp \u2261Tr[(A\u2217A)p/2]. Unfortunately, the\nproof of the Lust-Picard/Pisier inequality employs language and tools from non-commutative\nprobability that are rather foreign to most potential users of (2).\nThis note presents an elementary proof of Theorem 1 that bypasses the above inequal-\nity. Our argument is based on an improvement of the methodology created by Ahlswede\nand Winter [2] in order to prove their operator Cherno\ufb00bound, which also has many appli-\ncations e.g. [7] (the improvement is discussed in Section 3.1). This approach only requires\nelementary facts from Linear Algebra and Matrix Analysis. The most complicated result\nthat we use is the Golden-Thompspon inequality [5, 14]:\n\u2200d \u2208N, \u2200d \u00d7 d Hermitian matrices A, B, Tr(eA+B) \u2264Tr(eAeB).\n(3)\n2\nThe elementary proof of this classical inequality is sketched in Section 5 below.\nWe have already noted that Rudelson\u2019s bound (2) follows simply from Theorem 1; see\n[11, Section 3] for detais. Here we prove a concentration lemma corresponding to that\nresult under the stronger assumption that |Y1| is a.s. bounded. While similar results have\nappeared in other papers [10, 12, 16], our proof is simpler and gives explicit (albeit quite\nlarge) constants.\nLemma 1 (Proven in Section 4) Let Y1, . . . , Yn be i.i.d. random column vectors in Cd\nwith |Y1| \u2264M almost surely and \u2225E [Y1Y \u2217\n1 ] \u2225\u22641. Then:\n\u2200t \u22650, P\n \r\r\r\r\r\n1\nn\nn\nX\ni=1\nYiY \u2217\ni \u2212E [Y1Y \u2217\n1 ]\n\r\r\r\r\r \u2265t\n!\n\u2264(2n)2e\u2212\nnt2\n16M2+8M2t.\nIn particular, a calculation shows that:\n\r\r\r\r\r\n1\nn\nn\nX\ni=1\nYiY \u2217\ni \u2212E [Y1Y \u2217\n1 ]\n\r\r\r\r\r < \u01eb(n, M) \u2261M\nr\n72 ln n + 48 ln 2\nn\nwith probability \u22651 \u22121\nn\nwhenever \u01eb(n, M) \u22641. A key feature both of this Lemma is that the ambient dimension d\nplays no direct role in the bound. In fact, the same result holds for Yi taking values in a\nseparable Hilbert space (as in the last section of [10]).\nTo conclude the introduction, we present an open problem: is it possible to improve\nupon Rudelson\u2019s bound under further assumptions? There is some evidence that the depen-\ndence on ln(d) in the Theorem, while necessary in general [12, Remark 3.4], can sometimes\nbe removed. For instance, Adamczak et al. [1] have improved upon Rudelson\u2019s original\napplication of Theorem 1 to convex bodies, obtaining exactly what one would expect in\nthe absence of the\np\nlog(2d) term. Another setting where our bound is a \u0398\n\u0010\u221a\nln d\n\u0011\nfactor\naway from optimality is that of more classical random matrices (cf. the end of Section 3.1\nbelow). It would be interesting if one could sharpen the proof of Theorem 1 in order to\nreobtain these results. [Related issues are raised by Vershynin [17].]\n2\nPreliminaries\nWe let Cd\u00d7d\nHerm denote the set of d \u00d7 d Hermitian matrices, which is a subset of the set Cd\u00d7d\nof all d \u00d7 d matrices with complex entries. The spectral theorem states that all A \u2208Cd\u00d7d\nHerm\nhave d real eigenvalues (possibly with repetitions) that correspond to an orthonormal set\nof eigenvectors. \u03bbmax(A) is the largest eigenvalue of A. The spectrum of A, denoted by\n3\nspec(A), is the multiset of all eigenvalues, where each eigenvalue appears a number of times\nequal to its multiplicity. We let\n\u2225C\u2225\u2261\nmax\nv\u2208Cd |v|=1 |Cv|\ndenote the operator norm of C \u2208Cd\u00d7d (|\u00b7| is the Euclidean norm). By the spectral theorem,\n\u2200A \u2208Cd\u00d7d\nHerm, \u2225A\u2225= max{\u03bbmax(A), \u03bbmax(\u2212A)}.\nMoreover, Tr(A) (the trace of A) is the sum of the eigenvalues of A.\n2.1\nSpectral mapping\nLet f : C \u2192C be an entire analytic function with a power-series representation f(z) \u2261\nP\nn\u22650 cn zn (z \u2208C). If all cn are real, the expression:\nf(A) \u2261\nX\nn\u22650\ncnAn (A \u2208Cd\u00d7d\nHerm)\ncorresponds to a map from Cd\u00d7d\nHerm to itself. We will sometimes use the so-called spectral\nmapping property:\nspecf(A) = f(spec(A)).\n(4)\nBy this we mean that the eigenvalues of f(A) are the numbers f(\u03bb) with \u03bb \u2208spec(A).\nMoreover, the multiplicity of \u03be \u2208specf(A) is the sum of the multiplicities of all preimages\nof \u03be under f that lie in spec(A).\n2.2\nThe positive-semide\ufb01nite order\nWe will use the notation A \u2ab00 to say that A is positive-semide\ufb01nite, i.e. A \u2208Cd\u00d7d\nHerm and\nits eigenvalues are A are non-negative. This is equivalent to saying that (v, Av) \u22650 for all\nv \u2208Cd, where (\u00b7, \u00b7\u00b7) is the standard Euclidean inner product.\nIf A, B \u2208Cd\u00d7d\nHerm, we write A \u2ab0B or B \u2aafA to say that A \u2212B \u2ab00. Notice that \u201c\u2ab0\u201d is\na partial order and that:\n\u2200A, B, A\u2032, B\u2032 \u2208Cd\u00d7d\nHerm, (A \u2aafA\u2032) \u2227(B \u2aafB\u2032) \u21d2A + A\u2032 \u2aafB + B\u2032.\n(5)\nMoreover, spectral mapping (4) implies that:\n\u2200A \u2208Cd\u00d7d\nHerm, A2 \u2ab00.\n(6)\nWe will also need the following simple fact.\n4\nProposition 1 For all A, B, C \u2208Cd\u00d7d\nHerm :\n(C \u2ab00) \u2227(A \u2aafB) \u21d2Tr(AC) \u2264Tr(BC).\n(7)\nProof: To prove this, assume the LHS and observe that the RHS is equivalent to Tr(C\u2206) \u2265\n0 where \u2206\u2261B \u2212A. By assumption, \u2206\u2ab00, hence it has a Hermitian square root \u22061/2.\nThe cyclic property of the trace implies:\nTr(C\u2206) = Tr(\u22061/2C\u22061/2).\nSince the trace is the sum of the eigenvalues, we will be done once we show that \u22061/2C\u22061/2 \u2ab0\n0. But, since \u22061/2 is Hermitian and C \u2ab00,\n\u2200v \u2208Cd, (v, \u22061/2C\u22061/2v) = ((\u22061/2v), C(\u22061/2v)) = (w, Cw) \u22650 (with w = \u22061/2v),\nwhich shows that \u22061/2C\u22061/2 \u2ab00, as desired.\n2\n2.3\nProbability with matrices\nAssume (\u2126, F, P) is a probability space and Z : \u2126\u2192Cd\u00d7d\nHerm is measurable with respect\nto F and the Borel \u03c3-\ufb01eld on Cd\u00d7d\nHerm (this is equivalent to requiring that all entries of Z\nbe complex-valued random variables). Cd\u00d7d\nHerm is a metrically complete vector space and\none can naturally de\ufb01ne an expected value E [Z] \u2208Cd\u00d7d\nHerm. This turns out to be the matrix\nE [Z] \u2208Cd\u00d7d\nHerm whose (i, j)-entry is the expected value of the (i, j)-th entry of Z. [Of course,\nE [Z] is only de\ufb01ned if all entries of Z are integrable, but this will always be the case in\nthis paper.]\nThe de\ufb01nition of expectations implies that traces and expectations commute:\nTr(E [Z]) = E [Tr(Z)] .\n(8)\nMoreover, one can check that the usual product rule is satis\ufb01ed:\nIf Z, W : \u2126\u2192Cd\u00d7d\nHerm are measurable and independent, E [ZW] = E [Z] E [W] .\n(9)\n3\nProof of Theorem 1\nProof: [of Theorem 1] We wish to control the tail behavior of:\n\u2225Zn\u2225= max{\u03bbmax(Zn), \u03bbmax(\u2212Zn)}.\n5\nHowever, Zn and \u2212Zn have the same distribution. It follows that:\n\u2200t \u22650, P (\u2225Zn\u2225\u2265t) \u22642 P (\u03bbmax(Zn) \u2265t) .\nThe usual Bernstein trick implies that for all t \u22650,\n\u2200t \u22650, P (\u03bbmax(Zn) \u2265t) \u2264inf\ns>0 e\u2212stE\n\u0002\nes\u03bbmax(Zn)\u0003\n.\nThe function \u201cx 7\u2192esx\u201d is monotone non-decreasing and positive for all s \u22650. It follows\nfrom the spectral mapping property (4) that for all s \u22650, the largest eigenvalue of esZn is\nes\u03bbmax(Zn) and all eigenvalues of esZn are non-negative. Using the equality \u201ctrace = sum of\neigenvalues\u201d implies that for all s \u22650,\nE\n\u0002\nes\u03bbmax(Zn)\u0003\n= E\n\u0002\n\u03bbmax\n\u0000esZn\u0001\u0003\n\u2264E\n\u0002\nTr\n\u0000esZn\u0001\u0003\n.\nAs a result, we have the inequality:\n\u2200t \u22650, P (\u2225Zn\u2225\u2265t) \u22642 inf\ns\u22650 e\u2212stE\n\u0002\nTr\n\u0000esZn\u0001\u0003\n.\n(10)\nUp to now, our proof has followed Ahlswede and Winter\u2019s argument. The next lemma,\nhowever, will require new ideas.\nLemma 2 For all s \u2208R,\nE\n\u0002\nTr(esZn)\n\u0003\n\u2264Tr\n\u0012\ne\ns2 Pn\ni=1 A2\ni\n2\n\u0013\n.\nThis lemma is proven below. We will now show how it implies Rudelson\u2019s bound. Let\n\u03c32 \u2261\n\r\r\r\r\r\nn\nX\ni=1\nA2\ni\n\r\r\r\r\r = \u03bbmax\n n\nX\ni=1\nA2\ni\n!\n.\n[The second inequality follows from Pn\ni=1 A2\ni \u2ab00, which holds because of (5) and (6).] We\nnote that:\nTr\n\u0012\ne\ns2 Pn\ni=1 A2\ni\n2\n\u0013\n\u2264d \u03bbmax\n\u0012\ne\ns2 Pn\ni=1 A2\ni\n2\n\u0013\n= d e\ns2\u03c32\n2\nwhere the equality is yet another application of spectral mapping (4) and the fact that\n\u201cx 7\u2192es2x/2\u201d is monotone increasing. We deduce from the Lemma and (10) that:\n\u2200t \u22650, P (\u2225Zn\u2225\u2265t) \u22642d inf\ns\u22650 e\u2212st+ s2t2\n2\n= 2d e\u2212t2\n2\u03c32 .\n(11)\n6\nThis implies that for any p \u22651,\n1\n\u03c3pE\nh\n(\u2225Zn\u2225\u2212\np\n2 ln(2d)\u03c3)p\n+\ni\n=\np\nZ +\u221e\n0\ntp\u22121P\n\u0010\n\u2225Zn\u2225\u2265(\np\n2 ln(2d) + t)\u03c3\n\u0011\ndt\n(use(11))\n\u2264\n2pd\nZ +\u221e\n0\ntp\u22121e\u2212(t+\u221a\n2 ln(2d))2\n2\ndt\n\u2264\n2pd\nZ +\u221e\n0\ntp\u22121e\u2212t2+2 ln(2d)\n2\ndt = Cp\np\nSince 0 \u2264\u2225Zn\u2225\u2264\np\n2 ln(2d)\u03c3 + (\u2225Zn\u2225\u2212\np\n2 ln(2d)\u03c3)+, this implies the Lp estimate in the\nTheorem. The bound \u201cCp \u2264c\u221ap\u201d is standard and we omit its proof.\n2\nTo \ufb01nish, we now prove Lemma 2.\nProof: [of Lemma 2] De\ufb01ne D0 \u2261Pn\ni=1 s2A2\ni /2 and\nDj \u2261D0 +\nj\nX\ni=1\n\u0012\ns\u01ebiAi \u2212s2A2\ni\n2\n\u0013\n(1 \u2264j \u2264n).\nWe will prove that for all 1 \u2264j \u2264n:\nE [Tr (exp (Dj))] \u2264E [Tr (exp (Dj\u22121))] .\n(12)\nNotice that this implies E\n\u0002\nTr(eDn)\n\u0003\n\u2264E\n\u0002\nTr(eD0)\n\u0003\n, which is the precisely the Lemma. To\nprove (12), \ufb01x 1 \u2264j \u2264n. Notice that Dj\u22121 is independent from s\u01ebjAj \u2212s2A2\nj/2 since the\n{\u01ebi}n\ni=1 are independent. This implies that:\nE [Tr (exp (Dj))]\n=\nE\n\u0014\nTr\n\u0012\nexp\n\u0012\nDj\u22121 + s\u01ebjAj \u2212s2A2\nj\n2\n\u0013\u0013\u0015\n(use Golden-Thompson (3))\n\u2264\nE\n\u0014\nTr\n\u0012\nexp (Dj\u22121) exp\n\u0012\ns\u01ebjAj \u2212s2A2\nj\n2\n\u0013\u0013\u0015\n(Tr(\u00b7) and E [\u00b7] commute, (8))\n=\nTr\n\u0012\nE\n\u0014\nexp (Dj\u22121) exp\n\u0012\ns\u01ebjAj \u2212s2A2\nj\n2\n\u0013\u0015\u0013\n.\n(use product rule, (9))\n=\nTr\n\u0012\nE [exp (Dj\u22121)] E\n\u0014\nexp\n\u0012\ns\u01ebjAj \u2212s2A2\nj\n2\n\u0013\u0015\u0013\n.\nBy the monotonicity of the trace (7) and the fact that exp (Dj\u22121) \u2ab00 (which follows\nfrom (4)), we will be done once we show that:\nE\n\u0014\nexp\n\u0012\ns\u01ebjAj \u2212s2A2\nj\n2\n\u0013\u0015\n\u2aafI.\n(13)\n7\nThe key fact is that s\u01ebjAj and \u2212s2A2\nj/2 always commute, hence the exponential of the sum\nis the product of the exponentials. Applying (9) and noting that e\u2212s2A2\nj/2 is constant, we\nsee that:\nE\n\u0014\nexp\n\u0012\ns\u01ebjAj \u2212s2A2\nj\n2\n\u0013\u0015\n= E [exp (s\u01ebjAj)] e\u2212\ns2A2\nj\n2 .\nIn the Gaussian case, an explicit calculation shows that E [exp (s\u01ebjAj)] = es2A2\nj/2, hence\n(13) holds. In the Rademacher case, we have:\nE [exp (s\u01ebjAj)] e\u2212\ns2A2\nj\n2\n= f(Aj)\nwhere f(z) = cosh(sz)e\u2212s2z2/2. It is a classical fact that 0 \u2264cosh(x) \u2264ex2/2 for all x \u2208R\n(just compare the Taylor expansions); this implies that 0 \u2264f(\u03bb) \u22641 for all eigenvalues of\nAj. Using spectral mapping (4), we see that:\nspecf(Aj) = f(spec(Aj)) \u2282[0, 1],\nwhich implies that f(Aj) \u2aafI. This proves (13) in this case and \ufb01nishes the proof of (12)\nand of the Lemma.\n2\n3.1\nRemarks on the original AW approach\nA direct adaptation of the original argument of Ahlswede and Winter [2] would lead to an\ninequality of the form:\nE\n\u0002\nTr(esZn)\n\u0003\n\u2264Tr\n\u0000E\n\u0002\nes\u01ebnAn\u0003\nE\n\u0002\nesZn\u22121\u0003\u0001\n.\nOne sees that:\nE\n\u0002\nes\u01ebnAn\u0003\n\u2aafe\ns2A2n\n2\n\u2aafe\ns2\u2225A2n\u2225\n2\nI.\nHowever, only the second inequality seems to be useful, as there is no obvious relationship\nbetween\nTr\n\u0012\ne\ns2A2n\n2 E\n\u0002\nesZn\u22121\u0003\u0013\nand\nTr\n\u0012\nE\n\u0002\nes\u01ebn\u22121An\u22121\u0003\nE\n\u0014\nesZn\u22122+ s2A2n\n2\n\u0015\u0013\n,\n8\nwhich is what we would need to proceed with induction. [Note that Golden-Thompson (3)\ncannot be undone and fails for three summands, [14].] The best one can do with the second\ninequality is:\nE\n\u0002\nTr(esZn)\n\u0003\n\u2264d e\ns2 Pn\ni=1 \u2225Ai\u22252\n2\n.\nThis would give a version of Theorem 1 with Pn\ni=1 \u2225Ai\u22252 replacing \u2225Pn\ni=1 A2\ni \u2225. This mod-\ni\ufb01ed result is always worse than the actual Theorem, and can be dramatically so. For\ninstance, consider the case of a Wigner matrix where:\nZn \u2261\nX\n1\u2264i\u2264j\u2264m\n\u01ebijAij\nwith the \u01ebij i.i.d. standard Gaussian and each Aij has ones at positions (i, j) and (j, i) and\nzeros elsewhere (we take d = m and n =\n\u0000m\n2\n\u0001\nin this case). Direct calculation reveals:\n\r\r\r\r\r\nX\nij\nA2\nij\n\r\r\r\r\r = \u2225(m \u22121)I\u2225= m \u22121 \u226a\n\u0012m\n2\n\u0013\n=\nX\nij\n\u2225Aij\u22252.\nWe note in passing that neither approach is sharp in this case, as \u2225P\nij \u01ebijAij\u2225concen-\ntrates around 2\u221am [4].\n4\nConcentration for rank-one operators\nIn this section we prove Lemma 1.\nProof: [of Lemma 1] Let\n\u03c6(s) \u2261E\n\"\nexp\n \ns\n\r\r\r\r\r\n1\nn\nn\nX\ni=1\nYiY \u2217\ni \u2212E [Y1Y \u2217\n1 ]\n\r\r\r\r\r\n!#\n.\nWe will show below that:\n\u2200s \u22650, \u03c6(s) \u22642n e2M2s2/n\u03c6(2M2s2/n).\n(14)\nBy Jensen\u2019s inequality, \u03c6(2Ms2/n) \u2264\u03c6(s)2M2s/n whenever 2M2s/n \u22641, hence (14) implies:\n\u22000 \u2264s \u2264n/2M2, \u03c6(s) \u2264(2n)\n1\n1\u22122M2s/ne\n2M2s2\nn\u22122M2s.\nSince\n\u2200s \u22650, P\n \r\r\r\r\r\n1\nn\nn\nX\ni=1\nYiY \u2217\ni \u2212E [Y1Y \u2217\n1 ]\n\r\r\r\r\r \u2265t\n!\n\u2264e\u2212st\u03c6(s),\n9\nthe Lemma then follows from the choice\ns \u2261\ntn\n8M2 + 4M2t\nand a few simple calculations. [Notice that 2M2s/n \u22641/2 with this choice, hence 1/(1 \u2212\n2M2s/n) \u22642.]\nTo prove (14), we begin with symmetrization (see e.g. [8]):\n\u03c6(s) \u2264E\n\"\nexp\n \n2s\n\r\r\r\r\r\n1\nn\nn\nX\ni=1\n\u01ebiYiY \u2217\ni\n\r\r\r\r\r\n!#\n,\nwhere {\u01ebi}n\ni=1 is a Rademacher sequence independent of Y1, . . . , Yn. Let S be the (random)\nspan of Y1, . . . , Yn and TrS denote the trace operation on linear operators mapping S to\nitself. Following the argument in Theorem 1, we notice that:\nE\n\"\nexp\n \n2s\n\r\r\r\r\r\n1\nn\nn\nX\ni=1\n\u01ebiYiY \u2217\ni\n\r\r\r\r\r\n!\n| Y1, . . . , Yn\n#\n\u22642E\n\"\nTrS\n(\nexp\n \n2s\nn\nn\nX\ni=1\n\u01ebiYiY \u2217\ni\n!)\n| Y1, . . . , Yn\n#\n.\nLemma 2 implies:\nE\n\"\nexp\n \n2s\n\r\r\r\r\r\n1\nn\nn\nX\ni=1\n\u01ebiYiY \u2217\ni\n\r\r\r\r\r\n!\n| Y1, . . . , Yn\n#\n\u2264\n2TrS\n(\nexp\n \n2s2\nn2\nn\nX\ni=1\n(YiY \u2217\ni )2\n!)\n\u2264\n2n exp\n \r\r\r\r\r\n2s2\nn2\nn\nX\ni=1\n(YiY \u2217\ni )2\n\r\r\r\r\r\n!\n,\nusing spectral mapping (4), the equality \u201ctrace = sum of eigenvalues\u201d and the fact that S\nhas dimension \u2264n. A quick calculation shows that 0 \u2aaf(YiY \u2217\ni )2 = |Yi|2 YiY \u2217\ni \u2aafM2YiY \u2217\ni ,\nhence (5) implies:\n0 \u2aaf2s2\nn2\nn\nX\ni=1\n(YiY \u2217\ni )2 \u2aaf2M2s2\nn\n \n1\nn\nn\nX\ni=1\nYiY \u2217\ni\n!\n.\nTherefore:\n\r\r\r\r\r\n2s2\nn2\nn\nX\ni=1\n(YiY \u2217\ni )2\n\r\r\r\r\r \u22642M2s2\nn\n\r\r\r\r\r\n1\nn\nn\nX\ni=1\nYiY \u2217\ni\n\r\r\r\r\r \u22642M2s2\nn\n\r\r\r\r\r\n1\nn\nn\nX\ni=1\nYiY \u2217\ni \u2212E [Y1Y \u2217\n1 ]\n\r\r\r\r\r + 2M2s2\nn\n.\n[We used \u2225E [Y1Y \u2217\n1 ] \u2225\u22641 in the last inequality.] Plugging this into the conditional expec-\ntation above and integrating, we obtain (14):\n\u03c6(s) \u22642n E\n\"\nexp\n \n2M2s2\nn\n\r\r\r\r\r\n1\nn\nn\nX\ni=1\nYiY \u2217\ni \u2212E [Y1Y \u2217\n1 ]\n\r\r\r\r\r + 2M2s2\nn\n!#\n= 2ne2M2s2/n \u03c6(2M2s2/n).\n2\n10\n5\nProof sketch for Golden-Thompson inequality\nAs promised in the Introduction, we sketch an elementary proof of inequality (3). We will\nneed the Trotter-Lie formula, a simple consequence of the Taylor formula for eX:\n\u2200A, B \u2208Cd\u00d7d\nHerm,\nlim\nn\u2192+\u221e(eA/neB/n)n = eA+B.\n(15)\nThe second ingredient is the inequality:\n\u2200k \u2208N, \u2200X, Y \u2208Cd\u00d7d\nHerm : X, Y \u2ab00 \u21d2Tr((XY )2k+1) \u2264Tr((X2Y 2)2k).\n(16)\nThis is proven in of [5] via an argument using the existence of positive-semide\ufb01nite square-\nroots for positive-semide\ufb01nite matrices, and the Cauchy-Schwartz inequality for the stan-\ndard inner product over Cd\u00d7d. Iterating (16) implies:\n\u2200X, Y \u2208Cd\u00d7d\nHerm : X, Y \u2ab00 \u21d2Tr((XY )2k) \u2264Tr(X2kY 2k).\nApply this to X = eA/2k and Y = eB/2k with A, B \u2208Cd\u00d7d\nHerm. Spectral mapping (4) implies\nX, Y \u2ab00 and we deduce:\nTr((eA/2keB/2k)2k) \u2264Tr(eAeB).\nInequality (3) follows from letting k \u2192+\u221e, using (15) and noticing that Tr(\u00b7) is continuous.\nReferences\n[1] Radoslaw Adamczak, Alexander E. Litvak, Alain Pajor and Nicole Tomczak-\nJaegermann.\u201cQuantitative estimates of the convergence of the empirical covariance\nmatrix in log-concave ensembles\u201d. Journal of the American Mathematical Society, to\nappear (2009).\n[2] Rudolf Ahlswede and Andreas Winter. \u201cStrong converse for identi\ufb01cation via quan-\ntum channels.\u201d IEEE Transactions on Information Theory 48(3): 569-579 (2002).\n[3] Emmanuel Cand`es and Justin Romberg. \u201cSparsity and incoherence in compressive\nsampling.\u201d Inverse Problems 23:969985 (2007).\n[4] Zoltan F\u00a8uredi and J\u00b4anos Koml\u00b4os. \u201cThe eigenvalues of random symmetric matrices.\u201d\nCombinatorica 1(3), 233-241 (1981).\n[5] Sidney Golden. \u201cLower Bounds for the Helmholtz Function.\u201d Physical Review 137,\nB1127B1128 (1965).\n11\n[6] Nathan Halko, Per-Gunnar Martinsson and Joel A. Tropp. \u201cFinding structure with\nrandomness: Stochastic algorithms for constructing approximate matrix decomposi-\ntions.\u201d arXiv:0909.4061 (2009).\n[7] Zeph Landau and Alexander Russell. \u201cRandom Cayley graphs are expanders: a sim-\npli\ufb01ed proof of the Alon-Roichman theorem.\u201d The Electronic Journal of Combina-\ntorics, 11(1) (2004).\n[8] Michel Ledoux and Michel Talagrand. Probability in Banach Spaces. Springer (1991).\n[9] Fran\u00b8coise Lust-Piquard and Gilles Pisier. \u201cNon commutative Khintchine and Paley\ninequalities.\u201d Arkiv f\u00a8or Matematik, 29(2): 241-260 (1991).\n[10] Shahar Mendelson and Alain Pajor. \u201cOn singular values of matrices with independent\nrows\u201d. Bernoulli 12(5): 761-773 (2006).\n[11] Mark Rudelson. \u201cRandom vectors in the isotropic position.\u201d Journal of Functional\nAnalysis, 164 (1): 60-72 (1999).\n[12] Mark Rudelson and Roman Vershynin. \u201cSampling from large matrices: an approach\nthrough geometric functional analysis.\u201d Journal of the ACM 54(4):Article 21 (2007).\n[13] Daniel Spielman and Nikhil Srivastava. \u201cGraph sparsi\ufb01cation by e\ufb00ective resis-\ntances.\u201d In Proceedings of the 40th annual ACM symposium on Theory of Computing\n(STOC 2008).\n[14] C.J. Thompson. \u201cInequality with applications in statistical mechanics.\u201d Journal of\nMathematical Physics, 6: 1812-1813 (1965).\n[15] Joel A. Tropp. \u201cOn the conditioning of random subdictionaries.\u201d Applied and Com-\nputational Harmonic Analysis 25(1): 1-24 (2008).\n[16] Roman Vershynin. \u201cFrame expansions with erasures: an approach through the non-\ncommutative operator theory.\u201d Applied and Computational Harmonic Analysis 18:\n167-176 (2005).\n[17] Roman Vershynin. \u201cSpectral norm of products of random and deterministic matri-\nces.\u201d Manuscript available from:\nhttp://www-personal.umich.edu/\u223cromanv/papers/papers.html.\n12\n",
        "sentence": "",
        "context": "appear (2009).\n[2] Rudolf Ahlswede and Andreas Winter. \u201cStrong converse for identi\ufb01cation via quan-\ntum channels.\u201d IEEE Transactions on Information Theory 48(3): 569-579 (2002).\nA direct adaptation of the original argument of Ahlswede and Winter [2] would lead to an\ninequality of the form:\nE\n\u0002\nTr(esZn)\n\u0003\n\u2264Tr\n\u0000E\n\u0002\nes\u01ebnAn\u0003\nE\n\u0002\nesZn\u22121\u0003\u0001\n.\nOne sees that:\nE\n\u0002\nes\u01ebnAn\u0003\n\u2aafe\ns2A2n\n2\n\u2aafe\ns2\u2225A2n\u2225\n2\nI.\npli\ufb01ed proof of the Alon-Roichman theorem.\u201d The Electronic Journal of Combina-\ntorics, 11(1) (2004).\n[8] Michel Ledoux and Michel Talagrand. Probability in Banach Spaces. Springer (1991)."
    },
    {
        "title": "Low-rank matrix completion using alternating minimization",
        "author": [
            "Prateek Jain",
            "Praneeth Netrapalli",
            "Sujay Sanghavi"
        ],
        "venue": "In Proceedings of the forty-fifth annual ACM symposium on Theory of computing,",
        "citeRegEx": "Jain et al\\.,? \\Q2013\\E",
        "shortCiteRegEx": "Jain et al\\.",
        "year": 2013,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "Matrix completion from a few entries",
        "author": [
            "Raghunandan H Keshavan",
            "Andrea Montanari",
            "Sewoong Oh"
        ],
        "venue": "Information Theory, IEEE Transactions on,",
        "citeRegEx": "Keshavan et al\\.,? \\Q2010\\E",
        "shortCiteRegEx": "Keshavan et al\\.",
        "year": 2010,
        "abstract": "Let M be a random (alpha n) x n matrix of rank r<<n, and assume that a\nuniformly random subset E of its entries is observed. We describe an efficient\nalgorithm that reconstructs M from |E| = O(rn) observed entries with relative\nroot mean square error RMSE <= C(rn/|E|)^0.5 . Further, if r=O(1), M can be\nreconstructed exactly from |E| = O(n log(n)) entries. These results apply\nbeyond random matrices to general low-rank incoherent matrices.\n  This settles (in the case of bounded rank) a question left open by Candes and\nRecht and improves over the guarantees for their reconstruction algorithm. The\ncomplexity of our algorithm is O(|E|r log(n)), which opens the way to its use\nfor massive data sets. In the process of proving these statements, we obtain a\ngeneralization of a celebrated result by Friedman-Kahn-Szemeredi and Feige-Ofek\non the spectrum of sparse random matrices.",
        "full_text": "arXiv:0901.3150v4  [cs.LG]  17 Sep 2009\nMatrix Completion from a Few Entries\nRaghunandan H. Keshavan\u2217, Andrea Montanari\u2217\u2020, and Sewoong Oh\u2217\nSeptember 17, 2009\nAbstract\nLet M be an n\u03b1 \u00d7 n matrix of rank r \u226an, and assume that a uniformly random subset E of\nits entries is observed. We describe an e\ufb03cient algorithm that reconstructs M from |E| = O(r n)\nobserved entries with relative root mean square error\nRMSE \u2264C(\u03b1)\n\u0012 nr\n|E|\n\u00131/2\n.\nFurther, if r = O(1) and M is su\ufb03ciently unstructured, then it can be reconstructed exactly from\n|E| = O(n log n) entries.\nThis settles (in the case of bounded rank) a question left open by Cand`es and Recht and\nimproves over the guarantees for their reconstruction algorithm. The complexity of our algorithm\nis O(|E|r log n), which opens the way to its use for massive data sets. In the process of proving\nthese statements, we obtain a generalization of a celebrated result by Friedman-Kahn-Szemer\u00b4edi\nand Feige-Ofek on the spectrum of sparse random matrices.\n1\nIntroduction\nImagine that each of m customers watches and rates a subset of the n movies available through a\nmovie rental service. This yields a dataset of customer-movie pairs (i, j) \u2208E \u2286[m] \u00d7 [n] and, for\neach such pair, a rating Mij \u2208R. The objective of collaborative \ufb01ltering is to predict the rating for\nthe missing pairs in such a way as to provide targeted suggestions.1 The general question we address\nhere is: Under which conditions do the known ratings provide su\ufb03cient information to infer the\nunknown ones? Can this inference problem be solved e\ufb03ciently? The second question is particularly\nimportant in view of the massive size of actual data sets.\n1.1\nModel de\ufb01nition\nA simple mathematical model for such data assumes that the (unknown) matrix of ratings has rank\nr \u226am, n. More precisely, we denote by M the matrix whose entry (i, j) \u2208[m] \u00d7 [n] corresponds\nto the rating user i would assign to movie j. We assume that there exist matrices U, of dimensions\nm \u00d7 r, and V , of dimensions n \u00d7 r, and a diagonal matrix \u03a3, of dimensions r \u00d7 r such that\nM = U\u03a3V T .\n(1)\n\u2217Department of Electrical Engineering, Stanford University\n\u2020Departments of Statistics, Stanford University\n1Indeed, in 2006, Netflix made public such a dataset with m \u22485 \u00b7 105, n \u22482 \u00b7 104 and |E| \u2248108 and challenged\nthe research community to predict the missing ratings with root mean square error below 0.8563 [Net].\n1\nFor justi\ufb01cation of these assumptions and background on the use of low rank matrices in information\nretrieval, we refer to [BDJ99]. Since we are interested in very large data sets, we shall focus on the\nlimit m, n \u2192\u221ewith m/n = \u03b1 bounded away from 0 and \u221e.\nWe further assume that the factors U, V are unstructured. This notion is formalized by the\nincoherence condition introduced by Cand\u00b4es and Recht [CR08], and de\ufb01ned in Section 2. In particular\nthe incoherence condition is satis\ufb01ed with high probability if M = U\u03a3V T with U and V uniformly\nrandom matrices with UT U = m1 and V T V = n1. Alternatively, incoherence holds if the entries of\nU and V are i.i.d. bounded random variables.\nOut of the m \u00d7 n entries of M, a subset E \u2286[m] \u00d7 [n] (the user/movie pairs for which a rating\nis available) is revealed. We let ME be the m \u00d7 n matrix that contains the revealed entries of M,\nand is \ufb01lled with 0\u2019s in the other positions\nME\ni,j =\n\u001a Mi,j\nif (i, j) \u2208E ,\n0\notherwise.\n(2)\nThe set E will be uniformly random given its size |E|.\n1.2\nAlgorithm\nA naive algorithm consists of the following projection operation.\nProjection. Compute the singular value decomposition (SVD) of ME (with \u03c31 \u2265\u03c32 \u2265\u00b7 \u00b7 \u00b7 \u22650)\nME =\nmin(m,n)\nX\ni=1\n\u03c3ixiyT\ni ,\n(3)\nand return the matrix Tr(ME) = (mn/|E|) Pr\ni=1 \u03c3ixiyT\ni obtained by setting to 0 all but the r largest\nsingular values. Notice that, apart from the rescaling factor (mn/|E|), Tr(ME) is the orthogonal\nprojection of ME onto the set of rank-r matrices. The rescaling factor compensates the smaller\naverage size of the entries of ME with respect to M.\nIt turns out that, if |E| = \u0398(n), this algorithm performs very poorly. The reason is that the\nmatrix ME contains columns and rows with \u0398(log n/ log log n) non-zero (revealed) entries.\nThe\nlargest singular values of ME are of order \u0398(\np\nlog n/ log log n). The corresponding singular vectors\nare highly concentrated on high-weight column or row indices (respectively, for left and right singular\nvectors). Such singular vectors are an artifact of the high-weight columns/rows and do not provide\nuseful information about the hidden entries of M. This motivates the de\ufb01nition of the following\noperation (hereafter the degree of a column or of a row is the number of its revealed entries).\nTrimming. Set to zero all columns in ME with degree larger that 2|E|/n. Set to 0 all rows with\ndegree larger than 2|E|/m.\nFigure 1 shows the singular value distributions of ME and f\nME for a random rank-3 matrix M.\nThe surprise is that trimming (which amounts to \u2018throwing out information\u2019) makes the underlying\nrank-3 structure much more apparent. This e\ufb00ect becomes even more important when the number\nof revealed entries per row/column follows a heavy tail distribution, as for real data.\nIn terms of the above routines, our algorithm has the following structure.\nSpectral Matrix Completion( matrix ME )\n1:\nTrim ME, and let f\nME be the output;\n2:\nProject f\nME to Tr(f\nME);\n3:\nClean residual errors by minimizing the discrepancy F(X, Y ).\n2\n0\n10\n20\n30\n40\n50\n60\n0\n5\n10\n15\n20\n25\n30\n35\n40\n45\n0\n10\n20\n30\n40\n50\n60\n0\n5\n10\n15\n20\n25\n30\n35\n40\n45\n\u03c34 \u03c33 \u03c32\n\u03c31\n\u03c33 \u03c32 \u03c31\nFigure 1: Histogram of the singular values of a partially revealed matrix M E before trimming (left) and after\ntrimming (right) for 104 \u00d7104 random rank-3 matrix M with \u01eb = 30 and \u03a3 = diag(1, 1.1, 1.2). After trimming\nthe underlying rank-3 structure becomes clear. Here the number of revealed entries per row follows a heavy\ntail distribution with P{N = k} = const./k3.\nThe last step of the above algorithm allows to reduce (or eliminate) small discrepancies between\nTr(f\nME) and M, and is described below.\nCleaning. Various implementations are possible, but we found the following one particularly ap-\npealing. Given X \u2208Rm\u00d7r, Y \u2208Rn\u00d7r with XT X = m1 and Y T Y = n1, we de\ufb01ne\nF(X, Y )\n\u2261\nmin\nS\u2208Rr\u00d7r F(X, Y, S) ,\n(4)\nF(X, Y, S)\n\u2261\n1\n2\nX\n(i,j)\u2208E\n(Mij \u2212(XSY T )ij)2 .\n(5)\nThe cleaning step consists in writing Tr(f\nME) = X0S0Y T\n0 and minimizing F(X, Y ) locally with initial\ncondition X = X0, Y = Y0.\nNotice that F(X, Y ) is easy to evaluate since it is de\ufb01ned by minimizing the quadratic function\nS 7\u2192F(X, Y, S) over the low-dimensional matrix S. Further it depends on X and Y only through\ntheir column spaces.\nIn geometric terms, F is a function de\ufb01ned over the cartesian product of\ntwo Grassmann manifolds (we refer to Section 6 for background and references).\nOptimization\nover Grassmann manifolds is a well understood topic [EAS99] and e\ufb03cient algorithms (in particular\nNewton and conjugate gradient) can be applied. To be de\ufb01nite, we assume that gradient descent\nwith line search is used to minimize F(X, Y ).\nFinally, the implementation proposed here implicitly assumes that the rank r is known.\nIn\npractice this is a non-issue. Since r \u226an, a loop over the value of r can be added at little extra cost.\nFor instance, in collaborative \ufb01ltering applications, r ranges between 10 and 30.\n1.3\nMain results\nNotice that computing Tr(f\nME) only requires to \ufb01nd the \ufb01rst r singular vectors of a sparse matrix.\nOur main result establishes that this simple procedure achieves arbitrarily small relative root mean\n3\nsquare error from O(nr) revealed entries. We de\ufb01ne the relative root mean square error as\nRMSE \u2261\n\u0014\n1\nmnM2max\n||M \u2212Tr(f\nME)||2\nF\n\u00151/2\n.\n(6)\nwhere we denote by ||A||F the Frobenius norm of matrix A. Notice that the factor (1/mn) corresponds\nto the usual normalization by the number of entries and the factor (1/M2\nmax) corresponds to the\nmaximum size of the matrix entries where M satis\ufb01es |Mi,j| \u2264Mmax for all i and j.\nTheorem 1.1. Assume M to be a rank r matrix of dimension n\u03b1 \u00d7 n that satis\ufb01es |Mi,j| \u2264Mmax\nfor all i, j. Then with probability larger than 1 \u22121/n3\n1\nmnM2max\n||M \u2212Tr(f\nME)||2\nF \u2264C \u03b13/2rn\n|E|\n,\n(7)\nfor some numerical constant C.\nThis theorem is proved in Section 3.\nNotice that the top r singular values and singular vectors of the sparse matrix f\nME can be\ncomputed e\ufb03ciently by subspace iteration [Ber92]. Each iteration requires O(|E|r) operations. As\nproved in Section 3, the (r + 1)-th singular value is smaller than one half of the r-th one. As a\nconsequence, subspace iteration converges exponentially. A simple calculation shows that O(log n)\niterations are su\ufb03cient to ensure the error bound mentioned.\nThe \u2018cleaning\u2019 step in the above pseudocode improves systematically over Tr(f\nME) and, for large\nenough |E|, reconstructs M exactly.\nTheorem 1.2. Assume M to be a rank r matrix that satis\ufb01es the incoherence conditions A1 and\nA2 with (\u00b50, \u00b51). Let \u00b5 = max{\u00b50, \u00b51}. Further, assume \u03a3min \u2264\u03a31, . . . , \u03a3r \u2264\u03a3max with \u03a3min, \u03a3max\nbounded away from 0 and \u221e. Then there exists a numerical constant C\u2032 such that, if\n|E| \u2265C\u2032nr\u221a\u03b1\n\u0010\u03a3max\n\u03a3min\n\u00112\nmax\nn\n\u00b50 log n , \u00b52r\u221a\u03b1\n\u0010\u03a3max\n\u03a3min\n\u00114o\n,\n(8)\nthen the cleaning procedure in Spectral Matrix Completion converges, with high probability, to\nthe matrix M.\nThis theorem is proved in Section 6. The basic intuition is that, for |E| \u2265C\u2032(\u03b1)nr max{log n, r},\nTr(f\nME) is so close to M that the cost function is well approximated by a quadratic function.\nTheorem 1.1 is optimal: the number of degrees of freedom in M is of order nr, without the same\nnumber of observations is impossible to \ufb01x them. The extra log n factor in Theorem 1.2 is due to a\ncoupon-collector e\ufb00ect [CR08, KMO08, KOM09]: it is necessary that E contains at least one entry\nper row and one per column and this happens only for |E| \u2265Cn log n. As a consequence, for rank r\nbounded, Theorem 1.2 is optimal. It is suboptimal by a polylogarithmic factor for r = O(log n).\n1.4\nRelated work\nBeyond collaborative \ufb01ltering, low rank models are used for clustering, information retrieval, machine\nlearning, and image processing. In [Faz02], the NP-hard problem of \ufb01nding a matrix of minimum\nrank satisfying a set of a\ufb03ne constraints was addresses through convex relaxation. This problem is\nanalogous to the problem of \ufb01nding the sparsest vector satisfying a set of a\ufb03ne constraints, which\nis at the heart of compressed sensing [Don06, CRT06]. The connection with compressed sensing was\n4\nemphasized in [RFP07], that provided performance guarantees under appropriate conditions on the\nconstraints.\nIn the case of collaborative \ufb01ltering, we are interested in \ufb01nding a matrix M of minimum rank\nthat matches the known entries {Mij : (i, j) \u2208E}.\nEach known entry thus provides an a\ufb03ne\nconstraint. Cand`es and Recht [CR08] introduced the incoherent model for M. Within this model,\nthey proved that, if E is random, the convex relaxation correctly reconstructs M as long as |E| \u2265\nC r n6/5 log n. On the other hand, from a purely information theoretic point of view (i.e. disregarding\nalgorithmic considerations), it is clear that |E| = O(n r) observations should allow to reconstruct M\nwith arbitrary precision. Indeed this point was raised in [CR08] and proved in [KMO08], through a\ncounting argument.\nThe present paper describes an e\ufb03cient algorithm that reconstructs a rank-r matrix from O(n r)\nrandom observations. The most complex component of our algorithm is the SVD in step 2. We were\nable to treat realistic data sets with n \u2248105. This must be compared with the O(n4) complexity of\nsemide\ufb01nite programming [CR08].\nCai, Cand`es and Shen [CCS08] recently proposed a low-complexity procedure to solve the convex\nprogram posed in [CR08]. Our spectral method is akin to a single step of this procedure, with the\nimportant novelty of the trimming step that improves signi\ufb01cantly its performances. Our analysis\ntechniques might provide a new tool for characterizing the convex relaxation as well.\nTheorem 1.1 can also be compared with a copious line of work in the theoretical computer science\nliterature [FKV04, AFK+01, AM07]. An important motivation in this context is the development of\nfast algorithms for low-rank approximation. In particular, Achlioptas and McSherry [AM07] prove\na theorem analogous to 1.1, but holding only for |E| \u2265(8 log n)4n (in the case of square matrices).\nA short account of our results was submitted to the 2009 International Symposium on Information\nTheory [KOM09]. While the present paper was under completion, C\u00b4andes and Tao posted online\na preprint proving a theorem analogous to 1.2 [CT09]. Once more, their approach is substantially\ndi\ufb00erent from ours.\n1.5\nOpen problems and future directions\nIt is worth pointing out some limitations of our results, and interesting research directions:\n1. Optimal RMSE with O(n) entries. Numerical simulations with the Spectral Matrix Com-\npletion algorithm suggest that the RMSE decays much faster with the number of observations per\ndegree of freedom (|E|/nr), than indicated by Eq. (7). This improved behavior is a consequence of\nthe cleaning step in the algorithm. It would be important to characterize the decay of RMSE with\n(|E|/nr).\n2. Threshold for exact completion. As pointed out, Theorem 1.2 is order optimal for r bounded.\nIt would nevertheless be useful to derive quantitatively sharp estimates in this regime. A systematic\nnumerical study was initiated in [KMO08]. It appears that available theoretical estimates (including\nthe recent ones in [CT09]) are for larger values of the rank, we expect that our arguments can be\nstrenghtened to prove exact reconstruction for |E| \u2265C\u2032(\u03b1)nr log n for all values of r.\n3. More general models. The model studied here and introduced in [CR08] presents obvious\nlimitations. In applications to collaborative \ufb01ltering, the subset of observed entries E is far from\nuniformly random. A recent paper [SC09] investigates the uniqueness of the solution of the matrix\ncompletion problem for general sets E. In applications to fast low-rank approximation, it would be\ndesirable to consider non-incoherent matrices as well (as in [AM07]).\n5\n2\nIncoherence property and some notations\nIn order to formalize the notion of incoherence, we write U = [u1, u2, . . . , ur] and V = [v1, v2, . . . , vr]\nfor the columns of the two factors, with ||ui|| = \u221am, ||vi|| = \u221an and uT\ni uj = 0, vT\ni vj = 0 for i \u0338= j\n(there is no loss of generality in this, since normalizations can be adsorbed by rede\ufb01ning \u03a3). We\nshall further write \u03a3 = diag(\u03a31, . . . , \u03a3r) with \u03a31 \u2265\u03a32 \u2265\u00b7 \u00b7 \u00b7 \u2265\u03a3r > 0.\nThe matrices U, V and \u03a3 will be said to be (\u00b50, \u00b51)-incoherent if they satisfy the following\nproperties:\nA1. For all i \u2208[m], j \u2208[n], we have Pr\nk=1 U2\ni,k \u2264\u00b50r, Pr\nk=1 V 2\ni,k \u2264\u00b50r.\nA2. For all i \u2208[m], j \u2208[n], we have | Pr\nk=1 Ui,k(\u03a3k/\u03a31)Vj,k| \u2264\u00b51r1/2.\nApart from di\ufb00erence in normalization, these assumptions coincide with the ones in [CR08].\nNotice that the second incoherence assumption A2 implies the bounded entry condition in Theo-\nrem 1.1 with Mmax = \u00b51r1/2. In the following, whenever we write that a property A holds with high\nprobability (w.h.p.), we mean that there exists a function f(n) = f(n; \u03b1) such that P(A) \u22651 \u2212f(n)\nand f(n) \u21920. In the case of exact completion (i.e. in the proof of Theorem 1.2) f( \u00b7 ) can also\ndepend on \u00b50, \u00b51, \u03a3min, \u03a3max, and f(n) \u21920 for \u00b50, \u00b51, \u03a3min, \u03a3max bounded away from 0 and \u221e.\nProbability is taken with respect to the uniformly random subset E \u2286[m] \u00d7 [n]. De\ufb01ne \u01eb \u2261\n|E|/\u221amn. In the case when m = n, \u01eb corresponds to the average number of revealed entries per row\nor column. Then, it is convenient to work with a model in which each entry is revealed independently\nwith probability \u01eb/\u221amn. Since, with high probability |E| \u2208[\u01eb\u221a\u03b1 n\u2212A\u221an log n, \u01eb\u221a\u03b1 n+A\u221an log n],\nany guarantee on the algorithm performances that holds within one model, holds within the other\nmodel as well if we allow for a vanishing shift in \u01eb.\nNotice that we can assume m \u2265n, since we can always apply our theorem to the transpose of\nthe matrix M. Throughout this paper, therefore, we will assume \u03b1 \u22651. Finally, we will use C, C\u2032\netc. to denote numerical constants.\nGiven a vector x \u2208Rn, ||x|| will denote its Euclidean norm. For a matrix X \u2208Rn\u00d7n\u2032, ||X||F is its\nFrobenius norm, and ||X||2 its operator norm (i.e. ||X||2 = supu\u0338=0 ||Xu||/||u||). The standard scalar\nproduct between vectors or matrices will sometimes be indicated by \u27e8x, y\u27e9or \u27e8X, Y \u27e9, respectively.\nFinally, we use the standard combinatorics notation [N] = {1, 2, . . . , N} to denote the set of \ufb01rst N\nintegers.\n3\nProof of Theorem 1.1 and technical results\nAs explained in the previous section, the crucial idea is to consider the singular value decomposition\nof the trimmed matrix f\nME instead of the original matrix ME, as in Eq. (3). We shall then rede\ufb01ne\n{\u03c3i}, {xi}, {yi}, by letting\nf\nME =\nmin(m,n)\nX\ni=1\n\u03c3ixiyT\ni .\n(9)\nHere ||xi|| = ||yi|| = 1, xT\ni xj = yT\ni yj = 0 for i \u0338= j and \u03c31 \u2265\u03c32 \u2265\u00b7 \u00b7 \u00b7 \u22650. Our key technical result is\nthat, apart from a trivial rescaling, these singular values are close to the ones of the full matrix M.\nLemma 3.1. There exists a numerical constant C > 0 such that, with probability larger than 1\u22121/n3\n\f\f\f\u03c3q\n\u01eb \u2212\u03a3q\n\f\f\f \u2264CMmax\nr\u03b1\n\u01eb ,\n(10)\n6\nwhere it is understood that \u03a3q = 0 for q > r.\nThis result generalizes a celebrated bound on the second eigenvalue of random graphs [FKS89,\nFO05] and is illustrated in Fig. 1: the spectrum of f\nME clearly reveals the rank-3 structure of M.\nAs shown in Section 5, Lemma 3.1 is a direct consequence of the following estimate.\nLemma 3.2. There exists a numerical constant C > 0 such that, with probability larger than 1\u22121/n3\n\f\f\f\f\n\f\f\f\f\n\u01eb\n\u221amnM \u2212f\nME\n\f\f\f\f\n\f\f\f\f\n2\n\u2264CMmax\n\u221a\u03b1\u01eb .\n(11)\nThe proof of this lemma is given in Section 4.\nWe will now prove Theorem 1.1.\nProof. (Theorem 1.1) By triangle inequality\n\f\f\f\n\f\f\fM \u2212Tr(f\nME)\n\f\f\f\n\f\f\f\n2 \u2264\n\f\f\f\f\n\f\f\f\f\n\u221amn\n\u01eb\nf\nME \u2212Tr(f\nME)\n\f\f\f\f\n\f\f\f\f\n2\n+\n\f\f\f\f\n\f\f\f\fM \u2212\n\u221amn\n\u01eb\nf\nME\n\f\f\f\f\n\f\f\f\f\n2\n\u2264\u221amn\u03c3r+1/\u01eb + CMmax\n\u221a\u03b1mn/\u221a\u01eb\n\u22642CMmax\nr\u03b1mn\n\u01eb\n,\nwhere we used Lemma 3.2 for the second inequality and Lemma 3.1 for the last inequality. Now, for\nany matrix A of rank at most 2r, ||A||F \u2264\n\u221a\n2r||A||2, whence\n1\n\u221amn\n\f\f\f\fM \u2212Tr(f\nME)\n\f\f\f\f\nF \u2264\n\u221a\n2r\n\u221amn\n\f\f\f\fM \u2212Tr(f\nME)\n\f\f\f\f\n2\n\u2264C\u2032Mmax\nr\u03b1r\n\u01eb .\nThe result follows by using |E| = \u01eb\u221amn.\n4\nProof of Lemma 3.2\nWe want to show that |xT (f\nME \u2212\n\u01eb\n\u221amnM)y| \u2264CMmax\n\u221a\u03b1\u01eb for each x \u2208Rm, y \u2208Rn such that\n||x|| = ||y|| = 1. Our basic strategy (inspired by [FKS89]) will be the following:\n(1) Reduce to x, y belonging to discrete sets Tm, Tn;\n(2) Bound the contribution of light couples by applying union bound to these discretized sets, with\na large deviation estimate on the random variable Z, de\ufb01ned as Z \u2261P\nL xi f\nME\ni,jyj \u2212\n\u01eb\n\u221amnxT My;\n(3) Bound the contribution of heavy couples using bound on the discrepancy of corresponding graph.\nThe technical challenge is that a worst-case bound on the tail probability of Z is not good enough,\nand we must keep track of its dependence on x and y. The de\ufb01nition of light and heavy couples is\nprovided in the following section.\n4.1\nDiscretization\nWe de\ufb01ne\nTn\n=\n\u001a\nx \u2208\nn \u2206\n\u221anZ\non\n: ||x|| \u22641\n\u001b\n,\n7\nNotice that Tn \u2286Sn \u2261{x \u2208Rn : ||x|| \u22641}. Next remark is proved in [FKS89, FO05], and relates\nthe original problem to the discretized one.\nRemark 4.1. Let R \u2208Rm\u00d7n be a matrix.\nIf |xT Ry| \u2264B for all x \u2208Tm and y \u2208Tn, then\n|x\u2032T Ry\u2032| \u2264(1 \u2212\u2206)\u22122B for all x\u2032 \u2208Sm and y\u2032 \u2208Sn.\nHence it is enough to show that, with high probability, |xT (f\nME \u2212\n\u01eb\n\u221amnM)y| \u2264CMmax\n\u221a\u03b1\u01eb for\nall x \u2208Tm and y \u2208Tn.\nA naive approach would be to apply concentration inequalities directly to the random variable\nxT (f\nME \u2212\n\u01eb\n\u221amnM)y. This fails because the vectors x, y can contain entries that are much larger than\nthe typical size O(n\u22121/2). We thus separate two contributions. The \ufb01rst contribution is due to light\ncouples L \u2286[m] \u00d7 [n], de\ufb01ned as\nL =\n\u001a\n(i, j) : |xiMijyj| \u2264Mmax\n\u0010 \u01eb\nmn\n\u00111/2\u001b\n.\nThe second contribution is due to its complement L, which we call heavy couples. We have\n\f\f\f\fxT\n\u0012\nf\nME \u2212\n\u01eb\n\u221amnM\n\u0013\ny\n\f\f\f\f \u2264\n\f\f\f\f\f\f\nX\n(i,j)\u2208L\nxi f\nME\nij yj \u2212\n\u01eb\n\u221amnxT My\n\f\f\f\f\f\f\n+\n\f\f\f\f\f\f\nX\n(i,j)\u2208L\nxi f\nME\nij yj\n\f\f\f\f\f\f\n(12)\nIn the next two subsections, we will prove that both contributions are upper bounded by CMmax\n\u221a\u03b1\u01eb\nfor all x \u2208Tm, y \u2208Tn. Applying Remark 4.1 to |xT (f\nME \u2212\n\u01eb\n\u221amnM)y|, this proves the thesis.\n4.2\nBounding the contribution of light couples\nLet us de\ufb01ne the subset of row and column indices which have not been trimmed as Al and Ar:\nAl\n=\n{i \u2208[m] : deg(i) \u22642\u01eb\n\u221a\u03b1} ,\nAr\n=\n{j \u2208[n] : deg(j) \u22642\u01eb\u221a\u03b1} ,\nwhere deg(\u00b7) denotes the degree (number of revealed entries) of a row or a column. Notice that\nA = (Al, Ar) is a function of the random set E. It is easy to get a rough estimate of the sizes of Al,\nAr.\nRemark 4.2. There exists C1 and C2 depending only on \u03b1 such that, with probability larger than\n1 \u22121/n4, |Al| \u2265m \u2212max{e\u2212C1\u01ebm, C2\u03b1}, and |Ar| \u2265n \u2212max{e\u2212C1\u01ebn, C2}.\nFor the proof of this claim, we refer to Appendix A. For any E \u2286[m] \u00d7 [n] and A = (Al, Ar)\nwith Al \u2286[m], Ar \u2286[n], we de\ufb01ne ME,A by setting to zero the entries of M that are not in E, those\nwhose row index is not in Al, and those whose column index not in Ar. Consider the event\nH(E, A) =\n\uf8f1\n\uf8f2\n\uf8f3\u2203x, y :\n\f\f\f\f\f\f\nX\n(i,j)\u2208L\nxiME,A\nij\nyj \u2212\n\u01eb\n\u221amnxT My\n\f\f\f\f\f\f\n> CMmax\n\u221a\u03b1\u01eb\n\uf8fc\n\uf8fd\n\uf8fe,\n(13)\n8\nwhere it is understood that x and y belong, respectively, to Tm and Tn. Note that f\nME = ME,A,\nand hence we want to bound P{H(E, A)}. We proceed as follows\nP {H(E, A)}\n=\nX\nA\nP {H(E, A), A = A}\n\u2264\nX\n|Al|\u2265m(1\u2212\u03b4),\n|Ar|\u2265n(1\u2212\u03b4)\nP {H(E, A), A = A} + 1\nn4\n\u2264\n2(n+m)H(\u03b4)\nmax\n|Al|\u2265m(1\u2212\u03b4),\n|Ar|\u2265n(1\u2212\u03b4)\nP {H(E; A)} + 1\nn4 ,\n(14)\nwith \u03b4 \u2261max{e\u2212C1\u01eb, C2\u03b1} and H(x) the binary entropy function.\nWe are now left with the task of bounding P {H(E; A)} uniformly over A where H is de\ufb01ned as\nin Eq. (13). The key step consists in proving the following tail estimate\nLemma 4.3. Let x \u2208Sm, y \u2208Sn, Z = P\n(i,j)\u2208L xiME,A\nij\nyj\u2212\n\u01eb\n\u221amnxT My, and assume |Al| \u2265m(1\u2212\u03b4),\n|Ar| \u2265n(1 \u2212\u03b4) with \u03b4 small enough. Then\nP\n\u0000Z > LMmax\n\u221a\u01eb\n\u0001\n\u2264exp\nn\n\u2212\n\u221a\u03b1(L \u22123)n\n2\no\n.\nProof. We begin by bounding the mean of Z as follows (for the proof of this statement we refer to\nAppendix B).\nRemark 4.4. |E [Z]| \u22642Mmax\n\u221a\u01eb.\nFor A = (Al, Ar), let MA be the matrix obtained from M by setting to zero those entries whose\nrow index is not in Al, and those whose column index not in Ar. De\ufb01ne the potential contribution\nof the light couples aij and independent random variables Zij as\naij\n=\n(\nxiMA\nij yj\nif |xiMA\nij yj| \u2264Mmax (\u01eb/mn)1/2 ,\n0\notherwise,\nZij\n=\n\u001a ai,j\nw.p. \u01eb/\u221amn,\n0\nw.p. 1 \u2212\u01eb/\u221amn,\nLet Z1 = P\ni,j Zij so that Z = Z1 \u2212\n\u01eb\n\u221amnxT My. Note that P\ni,j a2\nij \u2264P\ni,j\n\u0010\nxiMA\nij yj\n\u00112\n\u2264M2\nmax. Fix\n\u03bb = \u221amn/2Mmax\n\u221a\u01eb so that |\u03bbai,j| \u22641/2, whence e\u03bbaij \u22121 \u2264\u03bbaij + 2(\u03bbaij)2. It then follows that\nE[e\u03bbZ]\n=\nexp\nn\n\u01eb\n\u221amn\n\u0010 X\ni,j\n\u03bbai,j + 2\nX\ni,j\n(\u03bbai,j)2\u0011\n\u2212\n\u03bb \u01eb\n\u221amnxT My\no\n\u2264\nexp\nn\n\u03bbE[Z] +\n\u221amn\n2\no\n.\nThe thesis follows by Cherno\ufb00bound P(Z > a) \u2264e\u2212\u03bbaE[e\u03bbZ] after simple calculus.\nNote that P (\u2212Z > LMmax\n\u221a\u01eb) can also be bounded analogously. We can now \ufb01nish the upper\nbound on the light couples contribution.\nConsider the error event Eq. (13).\nA simple volume\n9\ncalculation shows that |Tm| \u2264(10/\u2206)m. We can apply union bound over Tm and Tn to Eq. (14) to\nobtain\nP{H(E, A)}\n\u2264\n2 \u00b7 2(n+m)H(\u03b4) \u00b7\n\u001220\n\u2206\n\u0013n+m\ne\u2212(C\u22123)\u221a\u03b1n\n2\n+ 1\nn4\n\u2264\nexp\n\u001a\nlog 2 + (1 + \u03b1) (H(\u03b4) log 2 + log(20/\u2206)) n \u2212(C \u22123)\u221a\u03b1n\n2\n\u001b\n+ 1\nn4 .\nHence, assuming \u03b1 \u22651, there exists a numerical constant C\u2032 such that, for C > C\u2032\u221a\u03b1, the \ufb01rst term\nis of order e\u2212\u0398(n), and this \ufb01nishes the proof.\n4.3\nBounding the contribution of heavy couples\nLet Q be an m \u00d7 n matrix with Qij = 1 if (i, j) \u2208E and i \u0338\u2208Ar, j \u0338\u2208Al (i.e. entry (i, j) is not\ntrimmed by our algorithm), and Qij = 0 otherwise. Since |Mij| \u2264Mmax, the heavy couples satisfy\n|xiyj| \u2265\np\n\u01eb/mn. We then have\n\f\f\f\f\f\f\nX\n(i,j)\u2208L\nxi f\nME\nij yj\n\f\f\f\f\f\f\n\u2264\nMmax\nX\n(i,j)\u2208L\nQij|xiyj|\n\u2264\nMmax\nX\n(i,j)\u2208E:\n|xiyj|\u2265\u221a\n\u01eb/mn\nQij|xiyj| .\nNotice that Q is the adjacency matrix of a random bipartite graph with vertex sets [m] and [n]\nand maximum degree bounded by 2\u01eb max(\u03b11/2, \u03b1\u22121/2). The following remark strengthens a result of\n[FO05].\nRemark 4.5. Given vectors x, y, let L\n\u2032 = {(i, j) : |xiyj| \u2265C\np\n\u01eb/mn}. Then there exist a constant\nC\u2032 such that, P\n(i,j)\u2208L\n\u2032 Qij|xiyj| \u2264C\u2032(\u221a\u03b1 +\n1\n\u221a\u03b1)\u221a\u01eb, for all x \u2208Tm, y \u2208Tn with probability larger\nthan 1 \u22121/2n3.\nFor the reader\u2019s convenience, a proof of this fact is proposed in Appendix C. The analogous result\nin [FO05] (for the adjacency matrix of a non-bipartite graph) is proved to hold only with probability\nlarger than 1 \u2212e\u2212C\u01eb. The stronger statement quoted here can be proved using concentration of\nmeasure inequalities. The last remark implies that for all x \u2208Tm, y \u2208Tn, and \u03b1 \u22651, the contribution\nof heavy couples is bounded by CMmax\n\u221a\u03b1\u01eb for some numerical constant C with probability larger\nthan 1 \u22121/2n3.\n5\nProof of Lemma 3.1\nRecall the variational principle for the singular values.\n\u03c3q\n=\nmin\nH,dim(H)=n\u2212q+1\nmax\ny\u2208H,||y||=1 ||f\nMEy||\n(15)\n=\nmax\nH,dim(H)=q\nmin\ny\u2208H,||y||=1 ||f\nMEy|| .\n(16)\nHere H is understood to be a linear subspace of Rn.\n10\nUsing Eq. (15) with H the orthogonal complement of span(v1, . . . , vq\u22121), we have, by Lemma 3.2,\n\u03c3q\n\u2264\nmax\ny\u2208H,||y||=1\n\f\f\f\ff\nMEy\n\f\f\f\f\n\u2264\n\u01eb\n\u221amn\n\u0012\nmax\ny\u2208H,||y||=1\n\f\f\f\fMy\n\f\f\f\f\n\u0013\n+\nmax\ny\u2208H,||y||=||x||=1\n\f\f\f\fxT\n\u0012\nf\nME \u2212\n\u01eb\n\u221amnM\n\u0013\ny\n\f\f\f\f\n\u2264\n\u01eb\u03a3q + CMmax\n\u221a\u03b1\u01eb\nThe lower bound is proved analogously, by using Eq. (16) with H = span(v1, . . . , vq).\n6\nMinimization on Grassmann manifolds and proof of Theorem 1.2\nThe function F(X, Y ) de\ufb01ned in Eq. (4) and to be minimized in the last part of the algorithm\ncan naturally be viewed as de\ufb01ned on Grassmann manifolds. Here we recall from [EAS99] a few\nimportant facts on the geometry of Grassmann manifold and related optimization algorithms. We\nthen prove Theorem 1.2. Technical calculations are deferred to Sections 7, 8, and to the appendices.\nWe recall that, for the proof of Theorem 1.2, it is assumed that \u03a3min, \u03a3max are bounded away\nfrom 0 and \u221e. Numerical constants are denoted by C, C\u2032 etc. Finally, throughout this section, we\nuse the notation X(i) \u2208Rr to refer to the i-th row of the matrix X \u2208Rm\u00d7r or X \u2208Rn\u00d7r.\n6.1\nGeometry of the Grassmann manifold\nDenote by O(d) the orthogonal group of d \u00d7 d matrices. The Grassmann manifold is de\ufb01ned as the\nquotient G(n, r) \u2243O(n)/O(r) \u00d7 O(n \u2212r). In other words, a point in the manifold is the equivalence\nclass of an n \u00d7 r orthogonal matrix A\n[A] = {AQ : Q \u2208O(r)} .\n(17)\nFor consistency with the rest of the paper, we will assume the normalization AT A = n 1. To represent\na point in G(n, r), we will use an explicit representative of this form. More abstractly, G(n, r) is the\nmanifold of r-dimensional subspaces of Rn.\nIt is easy to see that F(X, Y ) depends on the matrices X, Y only through their equivalence\nclasses [X], [Y ]. We will therefore interpret it as a function de\ufb01ned on the manifold M(m, n) \u2261\nG(m, r) \u00d7 G(n, r):\nF : M(m, n)\n\u2192\nR ,\n(18)\n([X], [Y ])\n7\u2192\nF(X, Y ) .\n(19)\nIn the following, a point in this manifold will be represented as a pair x = (X, Y ), with X an n \u00d7 r\northogonal matrix and Y an m\u00d7r orthogonal matrix. Boldface symbols will be reserved for elements\nof M(m, n) or of its tangent space, and we shall use u = (U, V ) for the point corresponding to the\nmatrix M = U\u03a3V T to be reconstructed.\nGiven x = (X, Y ) \u2208M(m, n), the tangent space at x is denoted by Tx and can be identi\ufb01ed with\nthe vector space of matrix pairs w = (W, Z), W \u2208Rm\u00d7r, Z \u2208Rn\u00d7r such that W TX = ZTY = 0.\nThe \u2018canonical\u2019 Riemann metric on the Grassmann manifold corresponds to the usual scalar product\n\u27e8W, W \u2032\u27e9\u2261Tr(W T W \u2032). The induced scalar product on Tx between w = (W, Z) and w\u2032 = (W \u2032, Z\u2032)\nis \u27e8w, w\u2032\u27e9= \u27e8W, W \u2032\u27e9+ \u27e8Z, Z\u2032\u27e9.\nThis metric induces a canonical notion of distance on M(m, n) which we denote by d(x1, x2)\n(geodesic or arc-length distance). If x1 = (X1, Y1) and x2 = (X2, Y2) then\nd(x1, x2) \u2261\np\nd(X1, X2)2 + d(Y1, Y2)2\n(20)\n11\nwhere the arc-length distances d(X1, X2), d(Y1, Y2) on the Grassmann manifold can be de\ufb01ned explic-\nitly as follows. Let cos \u03b8 = (cos \u03b81, . . . , cos \u03b8r), \u03b8i \u2208[\u2212\u03c0/2, \u03c0/2] be the singular values of XT\n1 X2/m.\nThen\nd(X1, X2) = ||\u03b8||2 .\n(21)\nThe \u03b8i\u2019s are called the \u2018principal angles\u2019 between the subspaces spanned by the columns of X1 and\nX2. It is useful to introduce two equivalent notions of distance:\ndc(X1, X2) =\n1\n\u221an\nmin\nQ1,Q2\u2208O(r) ||X1Q1 \u2212X2Q2||F\n(chordal distance),\n(22)\ndp(X1, X2) =\n1\n\u221a\n2n||X1XT\n1 \u2212X2XT\n2 ||F\n(projection distance).\n(23)\nNotice that dc and dp do not depend on the speci\ufb01c representatives X1, X2, but only on the equiv-\nalence classes [X1] and [X2]. Distances on M(m, n) are de\ufb01ned through Pythagorean theorem, e.g.\ndc(x1, x2) =\np\ndc(X1, X2)2 + dc(Y1, Y2)2.\nRemark 6.1. The geodesic, chordal and projection distance are equivalent, namely\n1\n\u03c0d(X1, X2) \u2264\n1\n\u221a\n2 dc(X1, X2) \u2264dp(X1, X2) \u2264dc(X1, X2) \u2264d(X1, X2) .\n(24)\nFor the reader\u2019s convenience, a proof of this fact is proposed in Appendix D.\nAn important remark is that geodesics with respect to the canonical Riemann metric admit an\nexplicit and e\ufb03ciently computable form. Given u \u2208M(m, n), w \u2208Tu the corresponding geodesic\nis a curve t 7\u2192x(t), with x(t) = u + wt + O(t2) which minimizes arc-length. If u = (U, V ) and\nw = (W, Z) then x(t) = (X(t), Y (t)) where X(t) can be expressed in terms of the singular value\ndecomposition W = L\u0398RT [EAS99]:\nX(t) = UR cos(\u0398t)RT + L sin(\u0398t)RT ,\n(25)\nwhich can be evaluated in time of order O(nr). An analogous expression holds for Y (t).\n6.2\nGradient and incoherence\nThe gradient of F at x is the vector grad F(x) \u2208Tx such that, for any smooth curve t 7\u2192x(t) \u2208\nM(m, n) with x(t) = x + w t + O(t2), one has\nF(x(t)) = F(x) + \u27e8grad F(x), w\u27e9t + O(t2) .\n(26)\nIn order to write an explicit representation of the gradient of our cost function F, it is convenient to\nintroduce the projector operator\nPE(M)ij =\n\u001a Mij\nif (i, j) \u2208E,\n0\notherwise.\n(27)\nThe two components of the gradient are then\ngrad F(x)X\n=\nPE(XSY T \u2212M)Y ST \u2212XQX ,\n(28)\ngrad F(x)Y\n=\nPE(XSY T \u2212M)T XS \u2212Y QY ,\n(29)\nwhere QX, QY \u2208Rr\u00d7r are determined by the condition grad F(x) \u2208Tx. This yields\nQX\n=\n1\nmXT PE(M \u2212XSY T )Y ST ,\n(30)\nQY\n=\n1\nnY T PE(M \u2212XSY T )T XS .\n(31)\n12\n6.3\nAlgorithm\nAt this point the gradient descent algorithm is fully speci\ufb01ed. It takes as input the factors of Tr(f\nME),\nto be denoted as x0 = (X0, Y0), and minimizes a regularized cost function\neF(X, Y )\n=\nF(X, Y ) + \u03c1 G(X, Y )\n(32)\n\u2261\nF(X, Y ) + \u03c1\nm\nX\ni=1\nG1\n \n||X(i)||2\n3\u00b50r\n!\n+ \u03c1\nn\nX\nj=1\nG1\n \n||Y (j)||2\n3\u00b50r\n!\n,\n(33)\nwhere X(i) denotes the i-th row of X, and Y (j) the j-th row of Y . The role of the regularization is\nto force x to remain incoherent during the execution of the algorithm.\nG1(z) =\n\u001a 0\nif z \u22641,\ne(z\u22121)2 \u22121\nif z \u22651.\n(34)\nWe will take \u03c1 = n\u01eb. Notice that G(X, Y ) is again naturally de\ufb01ned on the Grassmann manifold,\ni.e. G(X, Y ) = G(XQ, Y Q\u2032) for any Q, Q\u2032 \u2208O(r).\nLet\nK(\u00b5\u2032) \u2261\nn\n(X, Y ) such that ||X(i)||2 \u2264\u00b5\u2032r, ||Y (j)||2 \u2264\u00b5\u2032r for all i \u2208[m], j \u2208[n]\no\n.\n(35)\nWe have G(X, Y ) = 0 on K(3\u00b50). Notice that u \u2208K(\u00b50) by the incoherence property. Also, by the\nfollowing remark proved in Appendix D, we can assume that x0 \u2208K(3\u00b50).\nRemark 6.2. Let U, X \u2208Rn\u00d7r with UT U = XT X = n1 and U \u2208K(\u00b50) and d(X, U) \u2264\u03b4 \u2264\n1\n16.\nThen there exists X\u2032\u2032 \u2208Rn\u00d7r such that X\u2032\u2032T X\u2032\u2032 = n1, X\u2032\u2032 \u2208K(3\u00b50) and d(X\u2032\u2032, U) \u22644\u03b4. Further,\nsuch an X\u2032\u2032 can be computed in a time of O(nr2).\nGradient descent( matrix ME, factors x0 )\n1:\nFor k = 0, 1, . . . do:\n2:\nCompute wk = grad eF(xk);\n4:\nLet t 7\u2192xk(t) be the geodesic with xk(t) = xk + wkt + O(t2);\n5:\nMinimize t 7\u2192eF(xk(t)) for t \u22650, subject to d(xk(t), x0) \u2264\u03b3;\n6:\nSet xk+1 = xk(tk) where tk is the minimum location;\n7:\nEnd For.\nIn the above, \u03b3 must be set in such a way that d(u, x0) \u2264\u03b3. The next remark determines the\ncorrect scale.\nRemark 6.3. Let U, X \u2208Rm\u00d7r with UT U = XT X = m1, V, Y \u2208Rn\u00d7r with V T V = Y T Y = n1,\nand M = U\u03a3V T , c\nM = XSY T for \u03a3 = diag(\u03a31, . . . , \u03a3r) and S \u2208Rr\u00d7r. If \u03a31, . . . , \u03a3r \u2265\u03a3min, then\ndp(U, X) \u2264\n1\n\u221a\n2\u03b1n\u03a3min\n||M \u2212c\nM||F\n,\ndp(V, Y ) \u2264\n1\n\u221a\n2\u03b1n\u03a3min\n||M \u2212c\nM||F\n(36)\nAs a consequence of this remark and Theorem 1.1, we can assume that d(u, x0) \u2264C(\u03a3max\n\u03a3min ) \u00b51r\u221a\u03b1\n\u221a\u01eb\n.\nWe shall then set \u03b3 = C\u2032(\u03a3max\n\u03a3min ) \u00b51r\u221a\u03b1\n\u221a\u01eb\n(the value of C\u2032 is set in the course of the proof).\nBefore passing to the proof of Theorem 1.2, it is worth discussing a few important points con-\ncerning the gradient descent algorithm.\n13\n(i) The appropriate choice of \u03b3 might seem to pose a di\ufb03culty.\nIn reality, this parameter is\nintroduced only to simplify the proof. We will see that the constraint d(xk(t), x0) \u2264\u03b3 is, with\nhigh probability, never saturated.\n(ii) Indeed, the line minimization instruction 5 (which might appear complex to implement) can\nbe replaced by a standard step selection procedure, such as the one in [Arm66].\n(iii) Similarly, there is no need to know the actual value of \u00b50 in the regularization term. One can\nstart with \u00b50 = 1 and then repeat the optimization doubling it at each step.\n(iv) The Hessian of F can be computed explicitly as well. This opens the way to quadratically\nconvergent minimization algorithms (e.g. the Newton method).\n6.4\nProof of Theorem 1.2\nThe proof of Theorem 1.2 breaks down in two lemmas. The \ufb01rst one implies that, in a su\ufb03ciently\nsmall neighborhood of u, the function x 7\u2192F(x) is well approximated by a parabola.\nLemma 6.4. There exists numerical constants C0, C1, C2 such that the following happens. Assume\n\u01eb \u2265C0\u00b50\n\u221a\u03b1 r max{log n; \u00b50r\u221a\u03b1(\u03a3max/\u03a3min)4} and \u03b4 \u2264\u03a3min/C0\u03a3max. Then\nC1\n\u221a\u03b1\u03a32\nmin d(x, u)2 + C1\n\u221a\u03b1 ||S \u2212\u03a3||2\nF \u22641\nn\u01eb F(x) \u2264C2\n\u221a\u03b1\u03a32\nmaxd(x, u)2\n(37)\nfor all x \u2208M(m, n)\u2229K(4\u00b50) such that d(x, u) \u2264\u03b4, with probability at least 1\u22121/n4. Here S \u2208Rr\u00d7r\nis the matrix realizing the minimum in Eq. (4).\nThe second Lemma implies that x 7\u2192F(x) does not have any other stationary point (apart from\nu) within such a neighborhood.\nLemma 6.5. There exists numerical constants C0, C such that the following happens.\nAssume\n\u01eb \u2265C0\u00b50r\u221a\u03b1(\u03a3max/\u03a3min)2 max{log n; \u00b50r\u221a\u03b1(\u03a3max/\u03a3min)4} and \u03b4 \u2264\u03a3min/C0\u03a3max. Then\n||grad eF(x)||2 \u2265C n\u01eb2\u03a34\nmin d(x, u)2\nfor all x \u2208M(m, n) \u2229K(4\u00b50) such that d(x, u) \u2264\u03b4, with probability at least 1 \u22121/n4.\nWe can now prove Theorem 1.2.\nProof. (Theorem 1.2) Let \u03b4 > 0 be such that Lemma 6.4 and Lemma 6.5 are veri\ufb01ed, and C1, C2 be\nde\ufb01ned as in Lemma 6.4. We further assume \u03b4 \u2264\np\n(e1/9 \u22121)/C2. Take \u01eb large enough such that,\nd(u, x0) \u2264min(1, (C1/C2)1/2(\u03a3min/\u03a3max))\u03b4/10. Further, set the algorithm parameter to \u03b3 = \u03b4/4.\nWe make the following claims:\n1. xk \u2208K(4\u00b50) for all k.\nIndeed x0 \u2208K(3\u00b50) whence eF(x0) = F(x0) \u2264C2\n\u221a\u03b1n\u01eb\u03a32\nmax \u03b42. The claim follows because\neF(xk) is non-increasing and eF(x) \u2265\u03c1 G(X, Y ) \u2265n\u01eb\u221a\u03b1\u03a32\nmax(e1/9 \u22121) for x \u0338\u2208K(4\u00b50), where\nwe choose \u03c1 to be n\u01eb\u221a\u03b1\u03a32\nmax.\n2. d(xk, u) \u2264\u03b4/10 for all k.\nSince we set \u03b3 = \u03b4/4, by triangular inequality, we can assume to have d(xk, u) \u2264\u03b4/2. Since\nd(x0, u)2 \u2264(C1\u03a32\nmin/C2\u03a32\nmax)(\u03b4/10)2, we have eF(x) \u2265F(x) \u2265F(x0) for all x such that\nd(x, u) \u2208[\u03b4/10, \u03b4]. Since eF(xk) is non-increasing and eF(x0) = F(x0), the claim follows.\n14\nNotice that, by the last observation, the constraint d(xk(t), x0) \u2264\u03b3 is never saturated, and\ntherefore our procedure is just gradient descent with exact line search. Therefore by [Arm66] this\nmust converge to the unique stationary point of eF in K(4\u00b50) \u2229{x : d(x, u) \u2264\u03b4/10}, which, by\nLemma 6.5, is u.\n7\nProof of Lemma 6.4\n7.1\nA random graph Lemma\nThe following Lemma will be used several times in the following.\nLemma 7.1. There exist two numerical constants C1, C2 suct that the following happens. If \u01eb \u2265\nC1 log n then, with probability larger than 1 \u22121/n5,\nX\n(i,j)\u2208E\nxiyj \u2264C2\u01eb\nn\u221a\u03b1 ||x||1||y||1 + C2\n\u221a\u03b1\u01eb||x||2 ||y||2 .\n(38)\nfor all x \u2208Rm, y \u2208Rn.\nProof. Write xi = x0 + x\u2032\ni where P\ni x\u2032\ni = 0. Then\nX\n(i,j)\u2208E\nxiyj = x0\nX\nj\u2208[n]\ndeg(j)yj +\nX\n(i,j)\u2208E\nx\u2032\niyj ,\n(39)\nwhere we recall that deg(j) = {i \u2208[m] : such that (i, j) \u2208E}. Further |x0| = | P\ni xi/m| \u2264||x||1/m.\nThe \ufb01rst term is upper bounded by\nx0 max\nj\u2208n deg(j)||y||1 \u2264max\nj\u2208n deg(j)||x||1||y||1/m .\n(40)\nFor \u01eb \u2265C1 log n, with probability larger than 1 \u22121/2n5, the maximum degree is bounded by\n(9/C1)\u221a\u03b1\u01eb which is of same order as the average degree. Therefore this term is at most C2\n\u221a\u03b1\u01eb||x||1||y||1/m.\nThe second term is upper bounded by C2\n\u221a\u03b1\u01eb||x\u2032||2||y||2 using Theorem 1.1 in [FO05] or, equiv-\nalently, Theorem 3.1 in the case r = 1 and Mmax = 1. It can be shown to hold with probabil-\nity larger than 1 \u22121/2n5 with a large enough numerical constant C2. The thesis follows because\n||x\u2032||2 \u2264||x||2.\n7.2\nPreliminary facts and estimates\nThis subsection contains some remarks that will be useful in the proof of Lemma 6.5 as well.\nLet w = (W, Z) \u2208Tu, and t 7\u2192(X(t), Y (t)) be the geodesic such that (X(t), Y (t)) = (U, V ) +\n(W, Z)t+O(t2). By setting (X, Y ) = (X(1), Y (1)), we establish a one-to-one correspondence between\nthe points x as in the statement and a neighborhood of the origin in Tu. If we let W = L\u0398RT be\nthe singular value decomposition of W (with LT L = m1 and RT R = 1), the explicit expression for\ngeodesics in Eq. (25) yields\nX = U + W ,\nW = UR(cos \u0398 \u22121)RT + L sin \u0398RT .\n(41)\nAn analogous expression can obviously be written for Y = V + Z. Notice that, by the equivalence\nbetween chordal and canonical distance, Remark 6.1, we have\n1\nm||W||2\nF + 1\nn||Z||2\nF \u22642 d(u, x)2 .\n(42)\n15\nRemark 7.2. If u \u2208K(\u00b50) and x \u2208K(4\u00b50), then (W, Z) \u2208K(10\u00b50) and w = (W, Z) \u2208K(5\u03c02\u00b50/2).\nProof. The \ufb01rst fact follows from ||W\n(i)||2 \u22642||X(i)||2+2||U(i)||2. In order to prove w \u2208K(5\u03c02\u00b50/2),\nwe notice that\n||W (i)||2\n=\n||\u0398L(i)||2 \u2264\u03c02\n4 || sin \u0398L(i)||2\n\u2264\n\u03c02\n4 ||X(i) \u2212R cos \u0398RT U(i)||2 \u2264\u03c02\n2\n\u0010\n||X(i)||2 + ||U(i)||2\u0011\n.\nThe claim follows by showing a similar bound for ||Z(i)||2.\nWe next prove a simple a priori estimate.\nRemark 7.3. There exist numerical constants C1, C2 such that the following holds with probability\nlarger than 1 \u22121/n5. If \u01eb \u2265C1 log n, then for any (X, Y ) \u2208K(\u00b5) and S \u2208Rr\u00d7r,\nX\n(i,j)\u2208E\n(XSY T )2\nij \u2264C2||S||2\n2\n\u221a\u03b1 n\u01eb\n\u0012 1\nm||X||2\nF + 1\nn||Y ||2\nF\n\u0013 \u0012 1\nm||X||2\nF + 1\nn||Y ||2\nF + \u00b5r\u221a\u03b1\n\u221a\u01eb\n\u0013\n.\n(43)\nProof. Using Lemma 7.1, P\n(i,j)\u2208E(XSY T )2\nij is upper bounded by\n\u03c3max(S)2 X\na,b\nX\n(i,j)\u2208E\nX2\niaY 2\njb\n\u2264C2\u01eb\nn\u221a\u03b1\u03c3max(S)2 X\ni,j\n||X(i)||2||Y (j)||2 + C2\u03c3max(S)2\u221a\u03b1\u01eb\n\u0010 X\ni\n||X(i)||4\u00111/2\u0010 X\nj\n||Y (j)||4\u00111/2\n\u2264C2\u01eb\nn\u221a\u03b1\u03c3max(S)2 X\ni,j\n||X(i)||2||Y (j)||2 + C2\u03c3max(S)2\u221a\u03b1\u01eb\u00b5r\n\u0010 X\ni\n||X(i)||2\u00111/2\u0010 X\nj\n||Y (j)||2\u00111/2\n\u2264C2||S||2\n2\n\u221a\u03b1 n\u01eb\n\u0010 1\nm||X||2\nF + 1\nn||Y ||2\nF\n\u00112\n+ C2||S||2\n2 \u03b1\u00b5r n\u221a\u01eb\n\u0010 1\nm||X||2\nF + 1\nn||Y ||2\nF\n\u0011\n,\nwhere in the second step we used the incoherence condition. The last step follows from the inequalities\n2ab \u2264\u03b1(a/\u03b1 + b)2 and 2ab \u2264\u221a\u03b1(a2/\u03b1 + b2).\n7.3\nThe proof\nProof. (Lemma 6.4) Denote by S \u2208Rr\u00d7r the matrix realizing the minimum in Eq. (4). We will start\nby proving a lower bound on F(x) of the form\n1\nn\u01ebF(x) \u2265C1\n\u221a\u03b1 \u03a32\nmin d(x, u)2 + C1\n\u221a\u03b1 ||S \u2212\u03a3||2\nF \u2212C\u2032\n1\n\u221a\u03b1\u03a3maxd(x, u)2||S \u2212\u03a3||F ,\n(44)\nand an upper bound as in Eq. (37).\nTogether, for d(x, u) \u2264\u03b4 \u22641, these imply ||S \u2212\u03a3||2\nF \u2264\nC\u03a32\nmaxd(x, u)2, whence the lower bound in Eq. (37) follows for \u03b4 \u2264\u03a3min/C0\u03a3max.\nIn order to prove the bound (44) we write X = U + W, Y = V + Z, and\nF(X, Y )\n=\n1\n2\nX\n(i,j)\u2208E\n(U(S \u2212\u03a3)V T + USZ\nT + WSV T + WSZ\nT )2\nij\n\u2265\n1\n4A2 \u22121\n2B2\n16\nwhere we used the inequality (1/2)(a + b)2 \u2265(a2/4) \u2212(b2/2), and de\ufb01ned\nA2\n\u2261\nX\n(i,j)\u2208E\n(U(S \u2212\u03a3)V T + USZ\nT + WSV T )2\nij ,\nB2\n\u2261\nX\n(i,j)\u2208E\n(WSZ\nT )2\nij .\nUsing Remark 7.3, and Eq. (42) we get\nB2 \u2264C\u221a\u03b1n\u01eb ||S||2\n2\n\u0012\nd(x, u)2 + \u00b50r\u221a\u03b1\n\u221a\u01eb\n\u0013\nd(x, u)2\n\u22642C\u221a\u03b1n\u01eb\n\u0000\u03a32\nmax + ||S \u2212\u03a3||2\nF\n\u0001 \u0012\n\u03b42 + \u00b50r\u221a\u03b1\n\u221a\u01eb\n\u0013\nd(x, u)2 ,\nwhere the second inequality follows from the inequality \u03c3max(S)2 \u22642\u03a32\nmax + 2 ||S \u2212\u03a3||2\nF\nBy Theorem 4.1 in [CR08], we have A2 \u2265(1/2)E{A2} with probability larger than 1 \u22121/n5 for\n\u01eb \u2265C\u00b50\n\u221a\u03b1 r log n. Further\nE{A2}\n=\n\u01eb\n\u221amn||U(S \u2212\u03a3)V T + USZ\nT + WSV T ||2\nF\n=\n\u01eb\n\u221amn||U(S \u2212\u03a3)V T ||2\nF +\n\u01eb\n\u221amn||USZ\nT ||2\nF +\n\u01eb\n\u221amn||WSV T ||2\nF\n+\n2\u01eb\n\u221amn\u27e8USZ\nT , WSV T \u27e9+\n2\u01eb\n\u221amn\u27e8U(S \u2212\u03a3)V T , WSV T \u27e9+\n2\u01eb\n\u221amn\u27e8USZ\nT , U(S \u2212\u03a3)V T \u27e9.\nLet us call the absolute value of the six terms on the right hand side E1, . . . E6. A simple calculation\nyields\nE1\n=\nn\u01eb\u221a\u03b1||S \u2212\u03a3||2\nF ,\n(45)\nE2 + E3\n\u2265\nn\u01eb\u221a\u03b1\u03c3min(S)2\u0010 1\nm||W||2\nF + 1\nn||Z||2\nF\n\u0011\n\u2265C\u2032\u03c3min(S)2n\u01eb\u221a\u03b1d(x, u)2 .\n(46)\nThe absolute value of the fourth term can be written as\nE4\n=\n2\u01eb\nn\u221a\u03b1|\u27e8USZ\nT , WSV T \u27e9| \u2264\n2\u01eb\nn\u221a\u03b1 \u03c3max(S)2||W\nT U||F ||V T Z||F\n\u2264\n2\u01eb\u03b1\nn\u221a\u03b1\u03c3max(S)2( 1\n\u03b12 ||W\nT U||2\nF + ||V T Z||2\nF ) .\nIn order proceed, consider Eq. (41).\nSince by tangency condition UT L = 0, we have UT W =\nmR(cos \u0398 \u22121)RT whence\n||UT W||F = m|| cos \u03b8 \u22121|| = m\n2 ||4 sin2(\u03b8/2)|| \u2264m\n2 ||2 sin(\u03b8/2)||2\n(47)\n(here \u03b8 = (\u03b81, . . . , \u03b8r) is the vector containing the diagonal elements of \u0398). A similar calculation\nreveals that ||W||2\nF = m||2 sin(\u03b8/2)||2 thus proving ||UT W||2\nF \u2264||W||4\nF /4 \u2264Cm\u03b42||W||2\nF . The bound\n||V T Z||2\nF \u2264Cn\u03b42||Z||2\nF is proved in the same way, thus yielding\nE4\n\u2264\nCn\u01eb\u221a\u03b1\u03c3max(S)2\u03b42 d(x, u)2 .\n(48)\n17\nBy a similar calculation\nE5\n=\n2\u01eb\n\u221a\u03b1Tr{(S \u2212\u03a3)ST W\nT U} \u22642\u01eb\n\u221a\u03b1\u03c3max((S \u2212\u03a3)ST )||W\nT U||F\n\u2264\nn\u01eb\u221a\u03b1\u03c3max(S)||S \u2212\u03a3||F d(u, x)2 .\nand analogously\nE6 \u2264n\u01eb\u221a\u03b1\u03c3max(S)||S \u2212\u03a3||F d(u, x)2 .\nCombining these estimates, and using A2 \u2265E{A2}/2, we get\n1\nn\u01ebA2\n\u2265\nC1\n\u221a\u03b1||S \u2212\u03a3||2\nF + C1\n\u221a\u03b1\u03c3min(S)2d(u, x)2\n\u2212C2\n\u221a\u03b1\u03c3max(S)2\u03b42d(u, x)2 \u2212C2\n\u221a\u03b1\u03c3max(S) ||S \u2212\u03a3||F d(u, x)2\nfor some numerical constants C1, C2 > 0.\nUsing the bounds \u03c3min(S)2 \u2265\u03a32\nmin/2 \u2212||S \u2212\u03a3||2\nF ,\n\u03c3max(S)2 \u22642\u03a32\nmax + 2 ||S \u2212\u03a3||2\nF, and the assumption d(x, u) \u2264\u03b4 for \u03b4 \u2264\u03a3min/C0\u03a3max, we get the\nclaim (44).\nWe are now left with the task of proving the upper bound in Eq. (37). We can set \u03a3 = S, thus\nobtaining\nF(X, Y )\n\u2264\n1\n2\nX\n(i,j)\u2208E\n(U\u03a3Z\nT + W\u03a3V T + W\u03a3Z\nT )2\nij\n\u2264\nbA2 + bB2 ,\nwhere we de\ufb01ned\nbA2\n\u2261\nX\n(i,j)\u2208E\n(U\u03a3Z\nT + W\u03a3V T )2\nij ,\nbB2\n\u2261\nX\n(i,j)\u2208E\n(W\u03a3Z\nT )2\nij .\nBounds for these two quantities are derived as for A2 and B2. More precisely, by Theorem 4.1 in\n[CR08], we have bA2 \u22642E{ bA2} with probability at least 1 \u22121/n5 and\nE{ bA2}\n=\n\u01eb\nn\u221a\u03b1||W\u03a3V T + U\u03a3Z\nT ||2\nF\n=\n2\u01eb\nn\u221a\u03b1||W\u03a3V T ||2\nF +\n2\u01eb\nn\u221a\u03b1||U\u03a3Z\nT ||2\nF\n\u2264\n2\u221a\u03b1n\u01eb\u03a32\nmax\n\u0010 1\nm||W||2\nF + 1\nn||Z||2\nF\n\u0011\n\u22644\u221a\u03b1n\u01eb\u03a32\nmaxd(x, u)2 .\nbB2 is bounded similar to B2 and we get,\nbB2 \u2264C\u2032\u221a\u03b1n\u01eb\u03a32\nmaxd(u, x)2 .\n18\n8\nProof of Lemma 6.5\nAs in the proof of Lemma 6.4, see Section 7.2, we let t 7\u2192x(t) = (X(t), Y (t)) be the geodesic\nstarting at x(0) = u with velocity \u02d9x(0) = w = (W, Z) \u2208Tu. We also de\ufb01ne x = x(1) = (X, Y ) with\nX = U + W and Y = V + Z. Let bw = \u02d9x(1) = (c\nW, bZ) be its velocity when passing through x. An\nexplicit expression is obtained in terms of the singular value decomposition of W and Z. If we let\nW = L\u0398RT, and di\ufb00erentiate Eq. (25) with respect to t at t = 1, we obtain\nc\nW = \u2212UR\u0398 sin \u0398 RT + L\u0398 cos \u0398 RT .\n(49)\nAn analogous expression holds for bZ. Since LT U = 0, we have ||c\nW||2\nF = m||\u0398 sin \u0398||2\nF +m||\u0398 cos \u0398||2\nF =\nm||\u03b8||2. Hence2\n1\nm||c\nW||2\nF + 1\nn|| bZ||2\nF = d(x, u)2.\n(50)\nIn order to prove the thesis, it is therefore su\ufb03cient to lower bound \u27e8grad eF(x), bw\u27e9. In the following\nwe will indeed show that\n\u27e8grad F(x), bw\u27e9\u2265C\u221a\u03b1 n\u01eb\u03a32\nmin d(x, u)2 ,\nand \u27e8grad G(x), bw\u27e9\u22650, which together imply the thesis by Cauchy-Schwarz inequality.\nLet us prove a few preliminary estimates.\nRemark 8.1. With the above de\ufb01nitions, bw \u2208K((11/2)\u03c02\u00b50).\nProof. Since \u0398 = diag(\u03b81, . . . , \u03b8r) with |\u03b8i| \u2264\u03c0/2, we get\n||c\nW (i)||2 \u22642||\u0398 sin \u0398RTU(i)||2 + 2||\u0398 cos \u0398L(i)||2 \u2264\u03c02\n2 ||U(i)||2 + 2||W (i)||2 .\n(51)\nBy assumption we have ||U(i)||2 \u2264\u00b50r and by Remark 7.2 we have ||W (i)||2 \u22645\u03c02\u00b50r/2.\nOne important fact that we will use is that c\nW is well approximated by W or by W, and bZ is\nwell approximated by Z or by Z. Using Eqs. (41) and (49) we get\n||c\nW||2\nF\n=\n||W||2\nF = m||\u03b8||2 ,\n(52)\n||W||2\nF\n=\nm||2 sin \u03b8/2||2 ,\n(53)\n\u27e8c\nW, W\u27e9\n=\nm\nr\nX\na=1\n\u03b8a sin \u03b8a ,\n(54)\n\u27e8c\nW, W\u27e9\n=\nm\nr\nX\na=1\n\u03b82\na cos \u03b8a ,\n(55)\nand therefore\n||c\nW \u2212W||2\nF\n=\nm\nr\nX\na=1\n[(2 sin(\u03b8a/2))2 + \u03b82\na \u22122\u03b8a sin \u03b8a]\n(56)\n\u2264\nm\nr\nX\na=1\n(\u03b8a \u22122 sin(\u03b8a/2))2 \u2264m\n242 ||\u03b8||4 \u2264m\n242 d(u, x)4 .\n(57)\n2Indeed this conclusion could have been reached immediately, since t 7\u2192x(t) is a geodesic parametrized proportion-\nally to the arclength in th interval t \u2208[0, 1].\n19\nAnalogously\n||c\nW \u2212W||2\nF\n=\nn\nr\nX\na=1\n[2\u03b82\na \u22122\u03b82\na cos \u03b8a] \u2264m ||\u03b8||4 \u2264m d(u, x)4 ,\n(58)\nwhere we used the inequlity 2(1 \u2212cos x) \u2264x2. The last inequality implies in particular\n||UT c\nW||F = ||UT (W \u2212c\nW)||F \u2264md(u, x)2 .\n(59)\nSimilar bounds hold of course for Z, bZ, Z (for instance we have ||V T bZ||F \u2264nd(u, x)2). Finally, we\nshall use repeatedly the fact that ||S \u2212\u03a3||2\nF \u2264C\u03a32\nmaxd(x, u)2, which follows from Lemma 6.4. This\nin turns implies\n\u03c3max(S)\n\u2264\n\u03a3max + C \u03a3maxd(x, u) \u22642 \u03a3max ,\n(60)\n\u03c3min(S)\n\u2265\n\u03a3min \u2212C \u03a3maxd(x, u) \u22651\n2 \u03a3min ,\n(61)\nwhere we used the hypothesis d(x, u) \u2264\u03b4 = \u03a3min/C0\u03a3max.\n8.1\nLower bound on grad F(x)\nRecalling that PE is the projector de\ufb01ned in Eq. (27), and using the expression (28), (29), for the\ngradient, we have\n\u27e8grad F(x), bw\u27e9= \u27e8PE(XSY T \u2212M), (XS bZT + c\nWSY T )\u27e9\n= \u27e8PE(U(S \u2212\u03a3)V T + USZ\nT + WSV T + WSZ\nT ), (US bZT + c\nWSV T + WS bZT + c\nWSZ\nT )\u27e9\n\u2265A \u2212B1 \u2212B2 \u2212B3\n(62)\nwhere we de\ufb01ned\nA\n=\n\u27e8PE(USZ\nT + WSV T ), (US bZT + c\nWSV T )\u27e9,\nB1\n=\n|\u27e8PE(USZ\nT + WSV T ), (WS bZT + c\nWSZ\nT )\u27e9| ,\nB2\n=\n|\u27e8PE(U(S \u2212\u03a3)V T + WSZ\nT ), (US bZT + c\nWSV T )\u27e9| ,\nB3\n=\n|\u27e8PE(U(S \u2212\u03a3)V T + WSZ\nT ), (W S bZT + c\nWSZ\nT )\u27e9| .\nAt this point the proof becomes very similar to the one in the previous section and consists in lower\nbounding A and upper bounding B1, B2, B3.\n8.1.1\nLower bound on A\nUsing Theorem 4.1 in [CR08] we obtain, with probability larger than 1 \u22121/n5.\nA\n\u2265\n\u01eb\n2\u221amn\u27e8(USZ\nT + WSV T ), (US bZT + c\nWSV T )\u27e9\n\u22651\n2 A0 \u22121\n2 B0\nwhere\nA0\n=\n\u01eb\n2\u221amn||USZ\nT + WSV T ||2\nF ,\nB0\n=\n\u01eb\n2\u221amn||USZ\nT + WSV T ||F ||US(Z \u2212bZ)T + (W \u2212c\nW)SV T ||F .\n20\nThe term A0 is lower bounded analogously to E{A2} in the proof of Lemma 6.4, see Eqs. (46) and\n(48):\nA0\n=\n\u01eb\n2\u221amn||USZ\nT + WSV T ||2\nF\n=\n\u01eb\n2\u221amn||USZ\nT ||2\nF +\n\u01eb\n2\u221amn||WSV T ||2\nF +\n\u01eb\n2\u221amn\u27e8USZ\nT , WSV T \u27e9\n\u2265\nCn\u01eb(\u221a\u03b1 \u03c3min(S)2 \u2212\u221a\u03b1\u03b42\u03c3max(S)2)d(x, u)2 \u2265C\u221a\u03b1 n\u01eb \u03a32\nmin d(x, u)2 ,\nwhere we used the bounds (60), (61) and the hypothesis d(x, u) \u2264\u03b4 = \u03a3min/C0\u03a3max.\nAs for the second term we notice that\nB2\n0\nA0\n\u2264\nn\u01eb\u221a\u03b1\n\u0010 1\nm||S(W \u2212c\nW)||2\nF + 1\nn||ST (Z \u2212bZ)||2\nF\n\u0011\n(63)\n\u2264\nn\u01eb\u221a\u03b1 \u03c3max(S)2\u0010 1\nm||W \u2212c\nW||2\nF + 1\nn||Z \u2212bZ||2\nF\n\u0011\n\u2264Cn\u01eb\u221a\u03b1 \u03a32\nmaxd(x, u)4 ,\n(64)\nwhere, in the last step, we used the estimate (57) and the analogous one for ||Z \u2212bZ||2\nF . Therefore\nfor d(x, u) \u2264\u03b4 \u2264\u03a3min/C0\u03a3max and C0 large enough A0 > 2B0, whence\nA \u2265C\u221a\u03b1 n\u01eb \u03a32\nmin d(x, u)2 .\n(65)\n8.1.2\nUpper bound on B1\nWe begin by noting that B1 can be bounded above by the sum of four terms of the form B\u2032\n1 =\n|\u27e8PE(USZ\nT ), W S bZT\u27e9|. We show that B\u2032\n1 < A/100. The other terms are bounded similarly.\nUsing Remark 7.3, we have\n||PE(WS bZT)||2\nF\n\u2264\nC\n\u01eb\n\u221amn||W||2\nF ||S bZT ||2\nF + C\u2032\u221a\u01eb\u00b50r\u221a\u03b1\u03a3max||W||F ||S bZ||F\n\u2264\n2C\n\u01eb\n\u221amn||W||2\nF ||SZ\nT ||2\nF + 2C\n\u01eb\n\u221amn||W||2\nF ||S( bZ \u2212Z)T ||2\nF\n+C\u2032\u221a\u01eb\u00b50r\u221a\u03b1\u03a3max||W||F ||SZ||F + C\u2032\u221a\u01eb\u00b50r\u221a\u03b1\u03a3max||W||F ||S( bZ \u2212Z)||F\n\u2264\nC\u2032\u2032A\n\u0012\n\u03b42 + \u00b50r\u221a\u03b1\u03a3max\n\u221a\u01eb\u03a3min\n\u0013\nwhere we have used\n\u01ebm\n\u221amn||SZ\nT ||2\nF \u22643A0 \u226412A from Section 8.1.1. Therefore we have,\nB\u20322\n1\n\u2264\n||PE(USZ\nT )||2\nF ||PE(WS bZT)||2\nF\n\u2264\nC\n\u01eb\n\u221amn||USZ\nT ||2\nF A\n\u0012\n\u03b42 + \u00b50r\u221a\u03b1\u03a3max\n\u221a\u01eb\u03a3min\n\u0013\n\u2264\nC\u2032A2\n\u0012\n\u03b42 + \u00b50r\u221a\u03b1\u03a3max\n\u221a\u01eb\u03a3min\n\u0013\nThe thesis follows for \u03b4 and \u01eb as in the hypothesis.\n21\n8.1.3\nUpper bound on B2\nWe have\nB2\n\u2264\n|\u27e8PE(US bZT + c\nWSV T ), WSZ\nT \u27e9| + |\u27e8PE(US bZT ), U(S \u2212\u03a3)V T \u27e9|\n+|\u27e8PE(c\nWSV T ), U(S \u2212\u03a3)V T \u27e9|\n\u2261\nB\u2032\n2 + B\u2032\u2032\n2 + B\u2032\u2032\u2032\n2 .\nWe claim that each of these three terms is smaller than A/30, whence B2 \u2264A/10.\nThe upper bound on B\u2032\n2 is obtained similarly to the one on B1 to get B\u2032\n2 \u2264A/30.\nConsider now B\u2032\u2032\n2. By Theorem 4.1 in [CR08],\nB\u2032\u2032\n2\n\u2264\n\u01eb\n\u221amn|\u27e8U(S \u2212\u03a3)V T , US bZT \u27e9| + C\n\u01eb\n\u221amn\ns\n\u00b50nr\u03b1 log n\nn\u221a\u03b1\u01eb\n||U(S \u2212\u03a3)V T ||F ||US bZ||F\nTo bound the second term, observe\n||US bZT ||F\n\u2264\n||USZ\nT ||F + ||US( bZ \u2212Z)T ||F\n\u2264\n||USZ\nT ||F + \u03a3max\n\u221amn d(x, u)2\nAlso,\n\u01eb\n\u221amn||USZ\nT ||2\nF \u22643A0 \u226412A from Section 8.1.1. Combining these, we have that the second\nterm in B\u2032\u2032\n2 is smaller than A/60 for \u01eb as in the hypothesis.\nTo bound the \ufb01rst term in B\u2032\u2032\n2,\n|\u27e8U(S \u2212\u03a3)V T , US bZT \u27e9|\n=\n|\u27e8U(S \u2212\u03a3)(Y \u2212V )T , US bZT \u27e9|\n\u2264\n||U(S \u2212\u03a3)Z\nT ||F ||USZ\nT ||F + ||U(S \u2212\u03a3)Z\nT ||F ||US(Z \u2212bZ||F\nTherefore\nB\u2032\u2032\n2\n\u2264\n\u01eb\n\u221amn||U(S \u2212\u03a3)Z||F ||USZ||F + \u01ebn\u221a\u03b1\u03a32\nmaxd(x, u)4 + A/60\n\u2264\n\u01eb\n\u221amn||U(S \u2212\u03a3)Z||F ||USZ||F + A/40\nfor d(x, u) \u2264\u03b4 as in the hypothesis.\nWe are now left with upper bounding eB\u2032\u2032\n2 \u2261\n\u01eb\n\u221amn||U(S \u2212\u03a3)Z||F ||USZ||F .\neB\u2032\u20322\n2\n\u2264\n\u0012\n\u01eb\n\u221amn||U(S \u2212\u03a3)Z\nT ||2\nF\n\u0013 \u0012\n\u01eb\n\u221amn||USZ\nT ||2\nF\n\u0013\n\u2264\n\u0000\u01ebn\u221a\u03b1\u03a32\nmaxd(x, u)4\u0001 \u0012\n\u01eb\n\u221amn||USZ\nT ||2\nF\n\u0013\nAlso from the lower bound on A, we have,\n\u01eb\n\u221amn||USZ\nT ||2\nF \u22643A0 \u226412A. Using d(x, u) \u2264\u03b4, we\nhave eB\u2032\u2032\n2 \u2264A/120 for \u03b4 as in the hypothesis. This proves the desired result. The bound on B\u2032\u2032\u2032\n2 is\ncalculated analogously.\n22\n8.1.4\nUpper bound on B3\nFinally for the last term it is su\ufb03cient to use a crude bound\nB3 \u22644\n\u0010\n||PE(WS bZT)||F + ||PE(c\nWSZ\nT )||F\n\u0011\u0010\n||PE(U(S \u2212\u03a3)V T )||F + ||PE(WSZ\nT )||F\n\u0011\n,\nThe terms of the form ||PE(WS bZT )||F are all estimated as in Section 8.1.2. Also, by Theorem 4.1\nof [CR08]\n||PE(U(S \u2212\u03a3)V T )||F\n\u2264\nC\n\u01eb\n\u221amn||U(S \u2212\u03a3)V T ||2\nF\n\u2264\nCn\u01eb\u221a\u03b1\u03a32\nmaxd(x, u)2\nCombining these estimates with the \u03b4 and the \u01eb in the hypothesis, we get B3 \u2264A/10\n8.2\nLower bound on grad G(x)\nBy the de\ufb01nition of G in Eq. (33), we have\n\u27e8grad G(x), bw\u27e9=\n1\n\u00b50r\nm\nX\ni=1\nG\u2032\n1\n \n||X(i)||2\n3\u00b50r\n!\n\u27e8X(i), c\nW (i)\u27e9+\n1\n\u00b50r\nn\nX\nj=1\nG\u2032\n1\n \n||Y (i)||2\n3\u00b50r\n!\n\u27e8Y (i), bZ(i)\u27e9.\n(66)\nIt is therefore su\ufb03cient to show that if ||X(i)||2 > 3\u00b50r, then \u27e8X(i), c\nW (i)\u27e9> 0, and if ||Y (j)||2 >\n3\u00b50r, then \u27e8Y (j), bZ(j)\u27e9> 0. We will just consider the \ufb01rst statement, the second being completely\nsymmetrical.\nFrom the explicit expressions (41) and (49) we get\nX(i)\n=\nR\nn\ncos \u0398 RT U(i) + sin \u0398 L(i)o\n,\n(67)\nc\nW (i)\n=\nR\nn\n\u0398 cos \u0398L(i) \u2212\u0398 sin \u0398RTU(i)o\n.\n(68)\nFrom the \ufb01rst expression it follows that\n|| sin \u0398 L(i)||2 \u2264||X(i)||2 + || cos \u0398 RT U(i)||2 \u22645 \u00b50r .\nOn the other hand, by taking the di\ufb00erence of Eqs. (67) and (68) we have\n||X(i) \u2212c\nW (i)||\n\u2264\n||(sin \u0398 \u2212\u0398 cos \u0398)L(i)|| + ||(cos \u0398 + \u0398 sin \u0398)RT U(i)||\n\u2264\nmax\ni\n(\u03b82\ni )|| sin \u0398L(i)|| + \u03c0\n2 ||U(i)|| \u2264\u03b4\np\n4\u00b50r + \u03c0\n2\n\u221a\u00b50r .\nwhere we used the inequality (sin \u03c9 \u2212\u03c9 cos \u03c9) \u2264\u03c92 sin \u03c9 valid for \u03c9 \u2208[0, \u03c0/2]. For \u03b4 small enough\nwe have therefore ||X(i) \u2212c\nW (i)|| \u2264(99/100)\u221a3\u00b50r. To conclude, for ||X(i)|| \u22653\u00b50r\n\u27e8X(i), c\nW (i)\u27e9\u2265||X(i)||2 \u2212||X(i)|| ||X(i) \u2212c\nW (i)|| \u2265||X(i)||(\np\n3\u00b50r \u2212(99/100)\np\n3\u00b50r) \u22650 .\nAcknowledgements\nWe thank Emmanuel Cand\u00b4es and Benjamin Recht for stimulating discussions on the subject of this\npaper. This work was partially supported by a Terman fellowship and an NSF CAREER award\n(CCF-0743978).\n23\nA\nProof of Remark 4.2\nThe proof is a generalization of analogous result in [FO05], which is proved to hold only with prob-\nability larger than 1 \u2212e\u2212C\u01eb. The stronger statement quoted here can be proved using concentration\nof measure inequalities.\nFirst, we apply Cherno\ufb00bound to the event\nn\n|Al| > max{e\u2212C1\u01ebm, C2\u03b1}\no\n. In the case of large\n\u01eb, when \u01eb > 3\u221a\u03b1 log n, we have P\n\b\n|Al| > C2\u03b1\n\t\n\u22641/2n4, for C2 \u2265max{e, 26/\u03b1}. In the case of\nsmall \u01eb, when \u01eb \u22643\u221a\u03b1 log n, we have P\n\b\n|Al| > max{e\u2212C1\u01ebm, C2\u03b1}\n\t\n\u22641/2n4, for C1 \u22641/600\u221a\u03b1\nand C2 \u2265130. Here we made a moderate assumption of \u01eb \u22653\u221a\u03b1, which is typically in the region of\ninterest.\nAnalogously, we can prove that P\n\b\n|Ar| > max{e\u2212C1\u01ebn, C2}\n\t\n\u22641/2n4 , which \ufb01nishes the proof\nof Remark 4.2.\nB\nProof of Remark 4.4\nThe expectation of the contribution of light couples, when each edge is independently revealed with\nprobability \u01eb/\u221amn, is\nE[Z]\n=\n\u01eb\n\u221amn\n\uf8eb\n\uf8edX\n(i,j)\u2208L\nxiMA\nij yj \u2212xT My\n\uf8f6\n\uf8f8,\nwhere we de\ufb01ne MA by setting to zero the rows of M whose index is not in Al and the columns of\nM whose index is not in Ar.\nIn order to bound P\nL xiMA\nij yj \u2212xT My, we write,\n\f\f\f\f\f\f\nX\n(i,j)\u2208L\nxiMA\nij yj \u2212xT My\n\f\f\f\f\f\f\n=\n\f\f\f\f\f\f\nxT \u0010\nMA \u2212M\n\u0011\ny \u2212\nX\n(i,j)\u2208L\nxiMA\nij yj\n\f\f\f\f\f\f\n\u2264\n\f\f\fxT \u0010\nMA \u2212M\n\u0011\ny\n\f\f\f +\n\f\f\f\f\f\f\nX\n(i,j)\u2208L\nxiMA\nij yj\n\f\f\f\f\f\f\n.\nNote that |(MA \u2212M)ij| is non-zero only if i /\u2208Al or j /\u2208Ar, in which case |(MA \u2212M)ij| \u2264\nMmax. Also, by Remark 4.2, there exists \u03b4 = max{e\u2212C1\u01eb, C2/n} such that |i : i /\u2208Al| \u2264\u03b4m and\n|j : j /\u2208Ar| \u2264\u03b4n. Denoting by I( \u00b7 ) the indicator function, we have\n\f\f\fxT \u0010\nMA \u2212M\n\u0011\ny\n\f\f\f\n\u2264\nX\nij\n\f\fxi\n\f\f\f\fyj\n\f\f\n\u0010\nI(i /\u2208Al) + I(j /\u2208Ar)\n\u0011\nMmax\n=\n\uf8eb\n\uf8edX\ni\n\f\fxi\n\f\fI(i /\u2208Al)\nX\nj\n\f\fyj\n\f\f +\nX\nj\n\f\fyj\n\f\fI(j /\u2208Ar)\nX\ni\n\f\fxi\n\f\f\n\uf8f6\n\uf8f8Mmax\n\u2264\n\u0010\u221a\n\u03b4m\u221an +\n\u221a\n\u03b4n\u221am\n\u0011\nMmax\n\u2264\nMmax\nrmn\n\u01eb\n.\n24\nfor \u03b4 \u22641\n4\u01eb. We can bound the second term as follows\n\f\f\f\f\f\f\nX\n(i,j)\u2208L\nxiMA\nij yj\n\f\f\f\f\f\f\n\u2264\nX\n(i,j)\u2208L\n\f\f\fxiMA\nij yj\n\f\f\f\n2\n\f\f\fxiMA\nij yj\n\f\f\f\n\u2264\n1\nMmax\nrmn\n\u01eb\nX\n(i,j)\u2208L\n\f\fxiMA\nij yj\n\f\f2\n\u2264\n1\nMmax\nrmn\n\u01eb\nX\ni\u2208[m],j\u2208[n]\n\f\fxiMA\nij yj\n\f\f2\n\u2264\nMmax\nrmn\n\u01eb\n,\nwhere the second inequality follows from the de\ufb01nition of heavy couples.\nHence, summing up the two contributions, we get\n|E [Z]|\n\u2264\n2Mmax\n\u221a\u01eb .\nC\nProof of Remark 4.5\nWe can associate to the matrix Q a bipartite graph G = ([m], [n], E). The proof is similar to the one\nin [FKS89, FO05] and is based on two properties of the graph G:\n1. Bounded degree. The graph G has maximum degree bounded by a constant times the average\ndegree:\ndeg(i)\n\u2264\n2\u01eb\n\u221a\u03b1 ,\n(69)\ndeg(j)\n\u2264\n2\u01eb\u221a\u03b1 ,\n(70)\nfor all i \u2208[m] and j \u2208[n].\n2. Discrepancy. We say that G (equivalently, the adjacency matrix Q) has the discrepancy prop-\nerty if, for any A \u2286[m] and B \u2286[n], one of the following is true:\n1.\ne(A, B)\n\u00b5(A, B) \u2264\u03be1 ,\n(71)\n2.\ne(A, B) ln\n\u0010 e(A, B)\n\u00b5(A, B)\n\u0011\n\u2264\u03be2 max{|A|/\u221a\u03b1, |B|\u221a\u03b1} ln\n\u0010\n\u221amn\nmax{|A|/\u221a\u03b1, |B|\u221a\u03b1}\n\u0011\n. (72)\nfor two numerical constants \u03be1, \u03be2 (independent of n and \u01eb). Here e(A, B) denotes the number\nof edges between A and B and \u00b5(A, B) = |A||B||E|/mn denotes the average number of edges\nbetween A and B before trimming.\nWe will prove, later in this section, that the discrepancy property holds with high probability.\nLet us partition row and column indices with respect to the value of xu and yv:\nAi\n=\n{u \u2208[m] :\n\u2206\n\u221am2i\u22121 \u2264|xu| <\n\u2206\n\u221am2i} ,\nBj\n=\n{v \u2208[n] :\n\u2206\n\u221an2j\u22121 \u2264|yv| < \u2206\n\u221an2j} ,\n25\nfor i \u2208{1, 2, . . . , \u2308ln (\u221am/\u2206)/ ln 2\u2309}, and j \u2208{1, 2, . . . , \u2308ln (\u221an/\u2206)/ ln 2\u2309}, and we denote the size of\nsubsets Ai and Bj by ai and bj respectively. Furthermore, we de\ufb01ne ei,j to be the number of edges\nbetween two subsets Ai and Bj, and we let \u00b5i,j = aibj(\u01eb/\u221amn). Notice that all indices u of non\nzero xu fall into one of the subsets Ai\u2019s de\ufb01ned above, since, by discretization, the smallest non-zero\nelement of x \u2208Tm in absolute value is at least \u2206/\u221am. The same applies for the entries of y \u2208Tn.\nBy grouping the summation into Ai\u2019s and Bj\u2019s, we get\nX\n(u,v):\n|xuyv|\u2265C\u221a\u01eb\n\u221amn\nQuv|xuyv|\n\u2264\nX\n(i,j):2i+j\u22654C\u221a\u01eb\n\u22062\nei,j\n\u22062i\n\u221am\n\u22062j\n\u221an\n=\n\u22062 X\naibj\n\u01eb\n\u221amn\nei,j\n\u00b5i,j\n2i\n\u221am\n2j\n\u221an\n=\n\u22062\u221a\u01eb\nX\nai\n22i\nm\n| {z }\n\u03b1i\nbj\n22j\nn\n| {z }\n\u03b2j\nei,j\n\u221a\u01eb\n\u00b5i,j2i+j\n|\n{z\n}\n\u03c3i,j\n.\nNote that, by de\ufb01nition, we have\nX\ni\n\u03b1i \u22644||x||2/\u22062 ,\n(73)\nX\ni\n\u03b2i \u22644||y||2/\u22062 .\n(74)\nWe are now left with task of bounding P \u03b1i\u03b2j\u03c3i,j, for Q that satis\ufb01es bounded degree property and\ndiscrepancy property.\nDe\ufb01ne,\nC1\n\u2261\n\u001a\n(i, j) : 2i+j \u22654C\u221a\u01eb\n\u22062\nand (Ai, Bj) satis\ufb01es (71)\n\u001b\n,\n(75)\nC2\n\u2261\n\u001a\n(i, j) : 2i+j \u22654C\u221a\u01eb\n\u22062\nand (Ai, Bj) satis\ufb01es (72)\n\u001b\n\\ C1 .\n(76)\nWe need to show that P\n(i,j)\u2208C1\u222aC2 \u03b1i\u03b2j\u03c3i,j is bounded.\nFor the terms in C1 this bound is easy. Since summation is over pairs of indices (i, j) such that\n2i+j \u22654C\u221a\u01eb\n\u22062 , it follows from bounded degree property that \u03c3i,j \u2264\u03be1\u22062/4C. By Eqs. (73) and (74),\nwe have P\nC1 \u03b1i\u03b2j\u03c3i,j \u2264(\u03be1\u22062/4C)(2/\u2206)4 = O(1).\nFor the terms in C2 the bound is more complicated. We assume ai \u2264\u03b1bj for simplicity and\nthe other case can be treated in the same manner. By change of notation the second discrepancy\ncondition becomes\nei,j log\n\u0012 ei,j\n\u00b5i,j\n\u0013\n\u2264\u03be2 max{ai/\u221a\u03b1, bj\n\u221a\u03b1} log\n\u0012\n\u221amn\nmax{ai/\u221a\u03b1, bj\n\u221a\u03b1}\n\u0013\n.\n(77)\nWe start by changing variables on both sides of Eq. (77).\nei,jaibj\u01eb\n\u00b5i,j\n\u221amn log\n\u0012 ei,j\n\u00b5i,j\n\u0013\n\u2264\n\u03be2bj\n\u221a\u03b1 log\n\u001222j\n\u03b2j\n\u0013\n.\n26\nNow, multiply each side by 2i/bj\n\u221a\u01eb2j to get\n\u03c3i,j\u03b1i log\n\u0012 ei,j\n\u00b5i,j\n\u0013\n\u2264\u03be22i\n\u221a\u01eb2j\n\u0002\nlog(22j) \u2212log \u03b2j\n\u0003\n.\n(78)\nTo achieve the desired bound, we partition the analysis into 5 cases:\n1. \u03c3i,j \u22641 : By Eqs. (73) and (74), we have P \u03b1i\u03b2j\u03c3i,j \u2264(2/\u2206)4 = O(1).\n2. 2i > \u221a\u01eb2j : By the bounded degree property in Eq. (70), we have ei,j \u2264ai2\u01eb/\u221a\u03b1, which implies\nthat ei,j/\u00b5i,j \u22642n/bj. For a \ufb01xed i we have, P\nj \u03b2j\u03c3i,jI(2i > \u221a\u01eb2j) \u22642\u221a\u01eb P\nj 2j\u2212iI(2i > \u221a\u01eb2j) \u2264\n4. Then, P \u03b1i\u03b2j\u03c3i,j \u226416/\u22062 = O(1).\n3. log (ei,j/\u00b5i,j) > 1\n4\n\u0002\nlog(22j) \u2212log \u03b2j\n\u0003\n: From Eq.(78), it immediately follows that \u03c3i,j\u03b1i \u22644\u03be22i\n\u221a\u01eb2j .\nDue to case 2, we can assume 2i \u2264\u221a\u01eb2j, which implies that for a \ufb01xed j we have the following\ninequality : P\ni \u03c3i,j\u03b1i \u22644\u03be2\nP\ni\n2i\n\u221a\u01eb2j I(2i \u2264\u221a\u01eb2j) \u22648\u03be2. Then it follows by Eq. (74) that\nP \u03b1i\u03b2j\u03c3i,j \u226432\u03be2/\u22062 = O(1).\n4. log(22j) \u2265\u2212log \u03b2j : Due to case 3, we can assume log (ei,j/\u00b5i,j) \u22641\n4\n\u0002\nlog(22j) \u2212log \u03b2j\n\u0003\n, which\nimplies that log (ei,j/\u00b5i,j) \u2264log(2j).\nFurther, since we are not in case 1, we can assume\n1 < \u03c3i,j = ei,j\n\u221a\u01eb/\u00b5i,j2i+j. Combining those two inequalities, we get 2i \u2264\u221a\u01eb.\nSince in de\ufb01ning C2 we excluded C1, if (i, j) \u2208C2 then log (ei,j/\u00b5i,j) \u22651. Applying Eq. (78) we\nget \u03c3i,j\u03b1i \u2264\u03c3i,j\u03b1i log (ei,j/\u00b5i,j) \u2264(\u03be22i\u2212j/\u221a\u01eb)\n\u0002\nlog(22j) \u2212log \u03b2j\n\u0003\n\u22644\u03be22i/\u221a\u01eb.\nCombining above two results, it follows that P\ni \u03c3i,j\u03b1i \u22644\u03be2\nP\ni\n2i\n\u221a\u01ebI(2i \u2264\u221a\u01eb) \u22648\u03be2 . Then,\nwe have the desired bound : P \u03b1i\u03b2j\u03c3i,j \u226432\u03be2\n\u22062 = O(1).\n5. log(22j) < \u2212log \u03b2j : It follows, since we are not in case 3, that log (ei,j/\u00b5i,j) \u22641\n4\n\u0002\nlog(22j) \u2212log \u03b2j\n\u0003\n\u2264\n\u2212log \u03b2j. Hence, ei,j/\u00b5i,j \u22641/\u03b2j. This implies that \u03c3i,j = ei,j\n\u221a\u01eb/\u00b5i,j2i+j \u2264\u221a\u01eb/\u03b2j2i+j. Since\nthe summation is over pairs of indices (i, j) such that 2i+j \u22654C\u221a\u01eb/\u22062, we have P\nj \u03c3i,j\u03b2j \u2264\u22062\n2C .\nThen it follows that P \u03b1i\u03b2j\u03c3i,j \u22642\nC = O(1).\nAnalogous analysis for the set of indices (i, j) such that ai > \u03b1bj will give us similar bounds.\nSumming up the results, we get that there exists a constant C\u2032 \u226432\n\u22064 +\n4\u03be1\nC\u22062 + 32\n\u22062 + 128\u03be2\n\u22062\n+ 4\nC , such\nthat\nX\n(i,j):2i+j\u22654C\u221a\u01eb\n\u22062\n\u03b1i\u03b2j\u03c3i,j \u2264C\u2032 .\nThis \ufb01nishes the proof of Remark 4.5.\nLemma C.1. The adjacency matrix Q has discrepancy property with probability at least 1 \u22121/2n3.\nProof. The proof is a generalization of analogous result in [FKS89, FO05] which is proved to hold only\nwith probability larger than 1\u2212e\u2212C\u01eb. The stronger statement quoted here is a result of the observation\nthat, when we trim the graph the number of edges between any two subsets does not increase. De\ufb01ne\nQ0 to be the adjacency matrix corresponding to original random matrix ME before trimming. If\nthe discrepancy assumption holds for Q0, then it also holds for Q, since eQ(A, B) \u2264eQ0(A, B), for\nA \u2286[m] and B \u2286[n].\nNow we need to show that the desired property is satis\ufb01ed for Q0. This is proved for the case\nof non-bipartite graph in Section 2.2.5 of [FO05], and analogous analysis for bipartite graph shows\n27\nthat for all subsets A \u2286[m] and B \u2286[n], with probability at least 1 \u22121/2(mn)p, the discrepancy\ncondition holds with \u03be1 = 2e and \u03be2 = (3p + 12)(\u03b11/2 + \u03b1\u22121/2). Since we assume \u03b1 \u22651, taking p to\nbe 3/2 proves the desired thesis.\nD\nProof of remarks 6.1, 6.2 and 6.3\nProof. (Remark 6.1.) Let \u03b8 = (\u03b81, . . . , \u03b8p), \u03b8i \u2208[\u2212\u03c0/2, \u03c0/2] be the principal angles between the\nplanes spanned by the columns of X1 and X2. It is known that dc(X1, X2) = ||2 sin(\u03b8/2)||2 and\ndp(X1, X2) = || sin \u03b8||2. The thesis follows from the elementary inequalities\n1\n\u03c0\u03b1 \u2264\n\u221a\n2 sin(\u03b1/2) \u2264sin \u03b1 \u22642 sin(\u03b1/2)\n(79)\nvalid for \u03b1 \u2208[0, \u03c0/2].\nProof. (Remark 6.2) Given X \u2208Rn\u00d7r, de\ufb01ne X\u2032 by\nX\u2032(i) =\nX(i)\n||X(i)|| min\n\u0010\n||X(i)||, \u221a\u00b50r\n\u0011\nfor all i \u2208[n].\nLet A be a matrix for extracting the ortho-normal basis of the columns of X\u2032. That is A \u2208Rr\u00d7r\nsuch that X\u2032\u2032 = X\u2032A and X\u2032\u2032T X\u2032\u2032 = n1.\nWithout loss of generality, A can be taken to be a\nsymmetric matrix. In the following, let \u03c3i = \u03c3i(A\u22121) for all i \u2208[n]. Note that by construction\nd(U, X\u2032) \u2264d(U, X) \u2264\u03b4. Hence there is a Q1 \u2208O(r) such that,\n||U \u2212X\u2032Q1||2\nF\n\u2264\nn\u03b42\n(80)\nWe start by writing\nnA\u2212T A\u22121 = X\u2032T X\u2032 = Q1(n1 \u2212(U \u2212X\u2032Q1)T U + UT (U \u2212X\u2032Q1) + (U \u2212X\u2032Q1)T (U \u2212X\u2032Q1))QT\n1 .(81)\nUsing (80), we have\n||(U \u2212X\u2032Q1)T U||F\n\u2264\n||U||2||(U \u2212X\u2032Q1)||F\n\u2264\nn\u03b4,\nand\n||(U \u2212X\u2032Q1)T (U \u2212X\u2032Q1)||F\n\u2264\nn\u03b42.\nTherefore, using (81)\n\u03c32\n1\n\u2264\n1 + 2\u03b4 + \u03b42,\n(82)\n\u03c32\nr\n\u2265\n1 \u22122\u03b4 \u2212\u03b42.\n(83)\nFrom (82), (83) and \u03b4 \u22641/16, we get \u03c31 \u2264\n\u221a\n3 and \u03c3r \u22651/\n\u221a\n3. Since ||X\u2032\u2032(i)||2 = ||X\u2032(i)A||2 \u22643\u00b50r\nfor all i \u2208[n], we have that X\u2032\u2032 \u2208K(3\u00b50).\n28\nWe next prove that d(X\u2032, X\u2032\u2032) \u22643\u03b4 which implies the thesis by triangular inequality.\nd(X\u2032, X\u2032\u2032)2\n=\n1\nn min\nQ\u2208O(r) ||X\u2032 \u2212X\u2032\u2032Q||2\nF\n\u2264\n1\nn||X\u2032\u2032A\u22121 \u2212X\u2032\u2032||2\nF\n=\n||A\u22121 \u22121||2\nF\n\u2264\n||(A\u22121 \u22121)(A\u22121 + 1)||2\nF\n\u2264\n||A\u2212T A\u22121 \u22121||2\nF\n\u2264\n9\u03b42\nwhere the last inequality is from (81).\nProof. (Remark 6.3.) We start by observing that\ndp(V, Y ) =\n1\n\u221an\nmin\nA\u2208Rr\u00d7r ||V \u2212Y A||F .\n(84)\nIndeed the minimization on the right hand side can be performed explicitly (as ||V \u2212Y A||2\nF is a\nquadratic function of A) and the minimum is achieved at A = Y T V/n. The inequality follows by\nsimple algebraic manipulations.\nTake A = ST XT U\u03a3\u22121/m. Then\n||V \u2212Y A||F\n=\nsup\nB,||B||F \u22641\n\u27e8B, (V \u2212Y A)\u27e9\n(85)\n=\nsup\nB,||B||F \u22641\n\u27e8BT , 1\nm\u03a3\u22121UT (U\u03a3V T \u2212XSY T )\u27e9\n(86)\n=\n1\nm\nsup\nB,||B||F \u22641\n\u27e8U\u03a3\u22121BT , (M \u2212c\nM)\u27e9\n(87)\n\u2264\n1\nm\nsup\nB,||B||F \u22641\n||U\u03a3\u22121BT||F ||M \u2212c\nM||F .\n(88)\nOn the other hand\n||U\u03a3\u22121BT ||2\nF = Tr(B\u03a3\u22121UT U\u03a3\u22121BT) = mTr(BT B\u03a3\u22122) \u2264m\u03a3\u22122\nmin||B||2\nF ,\nwhereby the last inequality follows from the fact that \u03a3 is diagonal. Together (84) and (88), this\nimplies the thesis.\nReferences\n[AFK+01] Y. Azar, A. Fiat, A. Karlin, F. McSherry, and J. Saia, Spectral analysis of data, Pro-\nceedings of the thirty-third annual ACM symposium on Theory of computing (New York,\nNY, USA), ACM, 2001, pp. 619\u2013626.\n[AM07]\nD. Achlioptas and F. McSherry, Fast computation of low-rank matrix approximations, J.\nACM 54 (2007), no. 2, 9.\n[Arm66]\nL. Armijo, Minimization of functions having lipschitz continuous \ufb01rst partial derivatives,\nPaci\ufb01c J. Math. 16 (1966), no. 1, 1\u20133.\n29\n[BDJ99]\nM. W. Berry, Z. Drma\u00b4c, and E. R. Jessup, Matrices, vector spaces, and information\nretrieval, SIAM Review 41 (1999), no. 2, 335\u2013362.\n[Ber92]\nM. W. Berry, Large scale sparse singular value computations, International Journal of\nSupercomputer Applications 6 (1992), 13\u201349.\n[CCS08]\nJ-F Cai, E. J. Cand`es, and Z. Shen, A singular value thresholding algorithm for matrix\ncompletion, arXiv:0810.3286, 2008.\n[CR08]\nE. J. Cand`es and B. Recht,\nExact matrix completion via convex optimization,\narxiv:0805.4471, 2008.\n[CRT06]\nE. J. Candes, J. K. Romberg, and T. Tao, Robust uncertainty principles: exact signal\nreconstruction from highly incomplete frequency information, IEEE Trans. on Inform.\nTheory 52 (2006), 489\u2013 509.\n[CT09]\nE. J. Cand`es and T. Tao, The power of convex relaxation: Near-optimal matrix comple-\ntion, arXiv:0903.1476, 2009.\n[Don06]\nD. L. Donoho, Compressed Sensing, IEEE Trans. on Inform. Theory 52 (2006), 1289\u2013\n1306.\n[EAS99]\nA. Edelman, T. A. Arias, and S. T. Smith, The geometry of algorithms with orthogonality\nconstraints, SIAM J. Matr. Anal. Appl. 20 (1999), 303\u2013353.\n[Faz02]\nM. Fazel, Matrix rank minimization with applications, Ph.D. thesis, Stanford University,\n2002.\n[FKS89]\nJ. Friedman, J. Kahn, and E. Szemer\u00b4edi, On the second eigenvalue in random regular\ngraphs, Proceedings of the Twenty-First Annual ACM Symposium on Theory of Com-\nputing (Seattle, Washington, USA), ACM, may 1989, pp. 587\u2013598.\n[FKV04]\nA. Frieze, R. Kannan, and S. Vempala, Fast monte-carlo algorithms for \ufb01nding low-rank\napproximations, J. ACM 51 (2004), no. 6, 1025\u20131041.\n[FO05]\nU. Feige and E. Ofek, Spectral techniques applied to sparse random graphs, Random Struct.\nAlgorithms 27 (2005), no. 2, 251\u2013275.\n[KMO08]\nR. H. Keshavan, A. Montanari, and S. Oh, Learning low rank matrices from O(n) entries,\nProc. of the Allerton Conf. on Commun., Control and Computing, September 2008.\n[KOM09]\nR. H. Keshavan, S. Oh, and A. Montanari, Matrix completion from a few entries,\narXiv:0901.3150, January 2009.\n[Net]\nNet\ufb02ix prize, http://www.net\ufb02ixprize.com/.\n[RFP07]\nB. Recht, M. Fazel, and P. Parrilo, Guaranteed minimum rank solutions of matrix equa-\ntions via nuclear norm minimization, arxiv:0706.4138, 2007.\n[SC09]\nA. Singer and M. Cucuringu, Uniqueness of low-rank matrix completion by rigidity theory,\narXiv:0902.3846, January 2009.\n30\n",
        "sentence": "",
        "context": "arXiv:0902.3846, January 2009.\n30\nA short account of our results was submitted to the 2009 International Symposium on Information\nTheory [KOM09]. While the present paper was under completion, C\u00b4andes and Tao posted online\na preprint proving a theorem analogous to 1.2 [CT09]. Once more, their approach is substantially\ndi\ufb00erent from ours.\n1.5\nOpen problems and future directions\nIt is worth pointing out some limitations of our results, and interesting research directions:"
    },
    {
        "title": "The bellkor solution to the netflix grand prize",
        "author": [
            "Yehuda Koren"
        ],
        "venue": "Netflix prize documentation,",
        "citeRegEx": "Koren.,? \\Q2009\\E",
        "shortCiteRegEx": "Koren.",
        "year": 2009,
        "abstract": "",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "Recovery guarantee of weighted low-rank approximation via alternating minimization",
        "author": [
            "Yuanzhi Li",
            "Yingyu Liang",
            "Andrej Risteski"
        ],
        "venue": "arXiv preprint arXiv:1602.02262,",
        "citeRegEx": "Li et al\\.,? \\Q2016\\E",
        "shortCiteRegEx": "Li et al\\.",
        "year": 2016,
        "abstract": "",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "Gradient descent converges to minimizers",
        "author": [
            "Jason D Lee",
            "Max Simchowitz",
            "Michael I Jordan",
            "Benjamin Recht"
        ],
        "venue": "University of California, Berkeley,",
        "citeRegEx": "Lee et al\\.,? \\Q2016\\E",
        "shortCiteRegEx": "Lee et al\\.",
        "year": 2016,
        "abstract": "We show that gradient descent converges to a local minimizer, almost surely\nwith random initialization. This is proved by applying the Stable Manifold\nTheorem from dynamical systems theory.",
        "full_text": "arXiv:1602.04915v2  [stat.ML]  4 Mar 2016\nGradient Descent Converges to Minimizers\nJason D. Lee\u266f, Max Simchowitz\u266f, Michael I. Jordan\u266f\u2020, and Benjamin Recht\u266f\u2020\n\u266fDepartment of Electrical Engineering and Computer Sciences\n\u2020Department of Statistcs\nUniversity of California, Berkeley\nMarch 7, 2016\nAbstract\nWe show that gradient descent converges to a local minimizer, almost surely with random initializa-\ntion. This is proved by applying the Stable Manifold Theorem from dynamical systems theory.\nKeywords: Gradient descent, smooth optimization, saddle points, local minimum, dynamical systems.\n1\nIntroduction\nSaddle points have long been regarded as a tremendous obstacle for continuous optimization. There are\nmany well known examples when worst case initialization of gradient descent provably converge to saddle\npoints [20, Section 1.2.3], and hardness results which show that \ufb01nding even a local minimizer of non-\nconvex functions is NP-Hard in the worst case [19]. However, such worst-case analyses have not daunted\npractitioners, and high quality solutions of continuous optimization problems are readily found by a variety\nof simple algorithms. Building on tools from the theory of dynamical systems, this paper demonstrates that,\nunder very mild regularity conditions, saddle points are indeed of little concern for the gradient method.\nMore precisely, let f : Rd \u2192R be twice continuously differentiable, and consider the classic gradient\nmethod with constant step size \u03b1:\nxk+1 = xk \u2212\u03b1\u2207f(xk).\n(1)\nWe call x a critical point of f if \u2207f(x) = 0, and say that f satis\ufb01es the strict saddle property if each critical\npoint x of f is either a local minimizer, or a \u201cstrict saddle\u201d, i.e, \u22072f(x) has at least one strictly negative\neigenvalue. We prove:\nIf f : Rd \u2192R is twice continuously differentiable and satis\ufb01es the strict saddle property, then\ngradient descent (Equation 1) with a random initialization and suf\ufb01ciently small constant step\nsize converges to a local minimizer or negative in\ufb01nity almost surely.\nHere, by suf\ufb01ciently small, we simply mean less than the inverse of the Lipschitz constant of the gradient.\nAs we discuss below, such step sizes are standard for the gradient method. We remark that the strict saddle\nassumption is necessary in the worst case, due to hardness results regarding testing the local optimality of\nfunctions whose Hessians are highly degenerate at critical points (e.g, quartic polynomials) [19].\n1\n1.1\nRelated work\nPrior work has show that \ufb01rst-order descent methods can circumvent strict saddle points, provided that\nthey are augmented with unbiased noise whose variance is suf\ufb01ciently large along each direction. For\nexample, [23] establishes convergence of the Robbins-Monro stochastic approximation to local minimizers\nfor strict saddle functions. More recently, [13] give quantitative rates on the convergence of noise-added\nstochastic gradient descent to local minimizers, for strict saddle functions. The condition that the noise\nhave large variance along all directions is often not satis\ufb01ed by the randomness which arises in sample-wise\nor coordinate-wise stochastic updates. In fact, it generally requires that additional, near-isotropic noise be\nadded at each iteration, which yields convergence rates that depend heavily on problem parameters like\ndimension. In contrast, our results hold for the simplest implementation of gradient descent and thus do not\nsuffer from the slow convergence associated with adding high-variance noise to each iterate.\nBut is this strict saddle property reasonable? Many works have answered in the af\ufb01rmative by demon-\nstrating that many objectives of interest do in fact satisfy the \u201cstrict saddle\u201d property: PCA, a fourth-order\ntensor factorization [13], formulations of dictionary learning [27, 26] and phase retrieval [28].\nTo obtain provable guarantees, the authors of [27, 26] and [28] adopt trust-region methods which lever-\nage Hessian information in order to circumvent saddle points. This approach joins a long line of related\nstrategies, including: a modi\ufb01ed Newton\u2019s method with curvilinear line search [18], the modi\ufb01ed Cholesky\nmethod [14], trust-region methods [11], and the related cubic regularized Newton\u2019s method [21], to name a\nfew. Specialized to deep learning applications, [12, 22] have introduced a saddle-free Newton method.\nUnfortunately, such curvature-based optimization algorithms have a per-iteration computational com-\nplexity which scales quadratically or even cubically in the dimension d, rendering them unsuitable for op-\ntimization of high dimensional functions. In contrast, the complexity of an iteration of gradient descent is\nlinear in dimension. We also remark that the authors of [28] empirically observe gradient descent with 100\nrandom initializations on the phase retrieval problem reliably converges to a local minimizer, and one whose\nquality matches that of the solution found using more costly trust-region techniques.\nMore broadly, many recent works have shown that gradient descent plus smart initialization provably\nconverges to the global minimum for a variety of non-convex problems: such settings include matrix fac-\ntorization [16, 30] , phase retrieval [9, 8], dictionary learning [3], and latent-variable models [29]. While\nour results only guarantee convergence to local minimizers, they eschew the need for complex and often\ncomputationally prohibitive initialization procedures.\nFinally, some preliminary results have shown that there are settings in which if an algorithm converges\nto a saddle point it necessarily has a small objective value. For example, [10] studies the loss surface of a\nparticular Gaussian random \ufb01eld as a proxy for understanding the objective landscape of deep neural nets.\nThe results leverage the Kac-Rice Theorem [2, 6], and establish that that critical points with more positive\neigenvalues have lower expected function value, often close to that of the global minimizer. We remark that\nfunctions drawn from this Gaussian random \ufb01eld model share the strict saddle property de\ufb01ned above, and\nso our results apply in this setting. On the other hand, our results are considerably more general, as they do\nnot place stringent generative assumptions on the objective function f.\n1.2\nOrganization\nThe rest of the paper is organized as follows. Section 2 introduces the notation and de\ufb01nitions used through-\nout the paper. Section 3 provides an intuitive explanation for why it is unlikely that gradient descent con-\nverges to a saddle point, by studying a non-convex quadratic and emphasizing the analogy with power\niteration. Section 4 states our main results which guarantee gradient descent converges to only local min-\n2\nimizers, and also establish rates of convergence depending on the local geometry of the minimizer. The\nprimary tool we use is the local stable manifold theorem, accompanied by inversion of gradient descent via\nthe proximal point algorithm. Finally, we conclude in Section 5 by suggesting several directions of future\nwork.\n2\nPreliminaries\nThroughout the paper, we will use f to denote a real-valued function in C2, the space of twice-continuously\ndifferentiable functions, and g to denote the corresponding gradient map with step size \u03b1,\ng(x) = x \u2212\u03b1\u2207f(x).\n(2)\nThe Jacobian of g is given by Dg(x)ij = \u2202gi\n\u2202xj (x), or Dg(x) = I \u2212\u03b1\u22072f(x). In addition to being C2, our\nmain regularity assumption on f is that it has a Lipschitz gradient:\n\u2225\u2207f(x) \u2212\u2207f(y)\u22252 \u2264L \u2225x \u2212y\u22252 .\nThe k-fold composition of the gradient map gk(x) corresponds to performing k steps of gradient descent\ninitialized at x. The iterates of gradient descent will be denoted xk := gk(x0). All the probability statements\nare with respect to \u03bd, the distribution of x0, which we assume is absolutely continuous with respect to\nLebesgue measure.\nA \ufb01xed point of the gradient map g is a critical point of the function f. Critical points can be saddle\npoints, local minima, or local maxima. In this paper, we will study the critical points of f via the \ufb01xed\npoints of g, and then apply dynamical systems theory to g.\nDe\ufb01nition 2.1.\n1. A point x\u2217is a critical point of f if it is a \ufb01xed point of the gradient map g(x\u2217) = x\u2217,\nor equivalently \u2207f(x\u2217) = 0.\n2. A critical point x\u2217is isolated if there is a neighborhood U around x\u2217, and x\u2217is the only critical point\nin U.\n3. A critical point is a local minimum if there is a neighborhood U around x\u2217such that f(x\u2217) \u2264f(x)\nfor all x \u2208U, and a local maximum if f(x\u2217) \u2265f(x).\n4. A critical point is a saddle point if for all neighborhoods U around x\u2217, there are x, y \u2208U such that\nf(x) \u2264f(x\u2217) \u2264f(y).\nAs mentioned in the introduction, we will be focused on saddle points that have directions of strictly\nnegative curvature. This notion is made precise by the following de\ufb01nition.\nDe\ufb01nition 2.2 (Strict Saddle). A critical point x\u2217of f is a strict saddle if \u03bbmin(\u22072f(x\u2217)) < 0.\nSince we are interested in the attraction region of a critical point, we de\ufb01ne the stable set.\nDe\ufb01nition 2.3 (Global Stable Set). The global stable set W s(x\u2217) of a critical point x\u2217is the set of initial\nconditions of gradient descent that converge to x\u2217:\nW s(x\u2217) = {x : lim\nk gk(x) = x\u2217}.\n3\n3\nIntuition\nTo illustrate why gradient descent does not converge to saddle points, consider the case of a non-convex\nquadratic, f(x) = 1\n2xT Hx. Without loss of generality, assume H = diag(\u03bb1, ..., \u03bbn) with \u03bb1, ..., \u03bbk > 0\nand \u03bbk+1, . . . , \u03bbn < 0. x\u2217= 0 is the unique critical point of this function and the Hessian at x\u2217is H. Note\nthat gradient descent initialized from x0 has iterates\nxk+1 =\nn\nX\ni=1\n(1 \u2212\u03b1\u03bbi)k+1\u27e8ei, x0\u27e9ei .\nwhere ei denote the standard basis vectors. This iteration resembles power iteration with the matrix I \u2212\u03b1H.\nThe gradient method is guaranteed to converge with a constant step size provided 0 < \u03b1 < 2\nL [20]. For\nthis quadratic f, L is equal to max |\u03bbi|. Suppose \u03b1 < 1/L, a slightly stronger condition. Then we will have\n(1 \u2212\u03b1\u03bbi) < 1 for i \u2264k and (1 \u2212\u03b1\u03bbi) > 1 for i > k. If x0 \u2208Es := span(e1, . . . , ek), then xk converges\nto the saddle point at 0 since (1 \u2212\u03b1\u03bbi)k+1 \u21920. However, if x0 has a component outside Es then gradient\ndescent diverges to \u221e. For this simple quadratic function, we see that the global stable set (attractive set) of\n0 is the subspace Es. Now, if we choose our initial point at random, the probability of that point landing in\nEs is zero.\nAs an example of this phenomena for a non-quadratic function, consider the following example from\n[20, Section 1.2.3]. Letting f(x, y) = 1\n2x2 + 1\n4y4 \u22121\n2y2, the corresponding gradient mapping is\ng(x) =\n\u0014\n(1 \u2212\u03b1)x\n(1 + \u03b1)y \u2212\u03b1y3\n\u0015\n.\nThe critical points are\nz1 =\n\u00140\n0\n\u0015\n,\nz2 =\n\u0014 0\n\u22121\n\u0015\n,\nz3 =\n\u00140\n1\n\u0015\n.\nThe points z2 and z3 are isolated local minima, and z1 is a saddle point.\nGradient descent initialized from any point of the form\n\u0014x\n0\n\u0015\nconverges to the saddle point z1. Any other\ninitial point either diverges, or converges to a local minimum, so the stable set of z1 is the x-axis, which is\na zero measure set in R2. By computing the Hessian,\n\u22072f(x) =\n\u00141\n0\n0\n3y2 \u22121\n\u0015\nwe \ufb01nd that \u22072f(z1) has one positive eigenvalue with eigenvector that spans the x-axis, thus agreeing with\nour above characterization of the stable set. If the initial point is chosen randomly, there is zero probability\nof initializing on the x-axis and thus zero probability of converging to the saddle point z1.\nIn the general case, the local stable set W s\nloc(x\u2217) of a critical point x\u2217is well-approximated by the span\nof the eigenvectors corresponding to positive eigenvalues. By an application of Taylor\u2019s theorem, one can\nsee that if the initial point x0 is uniformly random in a small neighborhood around x\u2217, then the probability\nof initializing in the span of these eigenvectors is zero whenever there is a negative eigenvalue. Thus,\ngradient descent initialized at x0 will leave the neighborhood. The primary dif\ufb01culty is that x0 is randomly\ndistributed over the entire domain, not a small neighborhood around x\u2217, and Taylor\u2019s theorem does not\nprovide any global guarantees.\n4\nHowever, the global stable set can be found by inverting the gradient map via g\u22121. Indeed, the global\nstable set is precisely \u222a\u221e\nk=0g\u2212k(W s\nloc(x\u2217)). This follows because if a point x converges to x\u2217, then for some\nsuf\ufb01ciently large k it must enter the local stable set. That is, x converges to x\u2217if and only if gk(x) \u2208W s\nloc\nfor suf\ufb01ciently large k. If W s\nloc(x\u2217) is of measure zero, then g\u2212k(W s\nloc(x\u2217)) is also of measure zero, and\nhence the global stable set is of measure zero. Thus, gradient descent will never converge to x\u2217from a\nrandom initialization.\nIn Section 4, we formalize the above arguments by showing the existence of an inverse gradient map.\nThe case of degenerate critical points, critical points with zero eigenvalues, is more delicate; the geometry\nof the global stable set is no longer characterized by only the number of positive eigenvectors. However in\nSection 4, we show that if a critical point has at least one negative eigenvalue, then the global stable set is of\nmeasure zero.\n4\nMain Results\nWe now state and prove our main theorem, making our intuition rigorous.\nTheorem 4.1. Let f be a C2 function and x\u2217be a strict saddle. Assume that 0 < \u03b1 < 1\nL, then\nPr(lim\nk xk = x\u2217) = 0.\nThat is, the gradient method never converges to saddle points, provided the step size is not chosen\naggressively. Greedy methods that use precise line search may still get stuck at stationary points. However,\na short-step gradient method will only converge to minimizers.\nRemark 4.2. Note that even for the convex functions method, a constant step size slightly less than 1/L is a\nnearly optimal choice. Indeed, for \u03b8 < 1, if one runs the gradient method with step size of \u03b8/L on a convex\nfunction a convergence rate of O( 1\n\u03b8T ) is attained.\nRemark 4.3. When limk xk does not exist, the above theorem is trivially true.\nTo prove Theorem 4.1, our primary tool will be the theory of Invariant Manifolds. Speci\ufb01cally, we will\nuse Stable-Center Manifold theorem developed in [25, 24, 15], which allows for a local characterization of\nthe stable set. Recall that a map g : X \u2192Y is a diffeomorphism if g is a bijection, and g and g\u22121 are\ncontinuously differentiable.\nTheorem 4.4 (Theorem III.7, [24]). Let 0 be a \ufb01xed point for the Cr local diffeomorphism \u03c6 : U \u2192E,\nwhere U is a neighborhood of 0 in the Banach space E. Suppose that E = Es \u2295Eu, where Es is the span\nof the eigenvectors corresponding to eigenvalues less than or equal to 1 of D\u03c6(0), and Eu is the span of the\neigenvectors corresponding to eigenvalues greater than 1 of D\u03c6(0). Then there exists a Cr embedded disk\nW cs\nloc that is tangent to Es at 0 called the local stable center manifold. Moreover, there exists a neighborhood\nB of 0, such that \u03c6(W cs\nloc) \u2229B \u2282W cs\nloc, and \u2229\u221e\nk=0\u03c6\u2212k(B) \u2282W cs\nloc.\nTo unpack all of this terminology, what the stable manifold theorem says is that if there is a map that\ndiffeomorphically deforms a neighborhood of a critical point, then this implies the existence of a local\nstable center manifold W cs\nloc containing the critical point. This manifold has dimension equal to the number\nof eigenvalues of the Jacobian of the critical point that are less than 1. W sc\nloc contains all points that are\nlocally forward non-escaping meaning, in a smaller neighborhood B, a point converges to x\u2217after iterating\n\u03c6 only if it is in W cs\nloc \u2229B.\n5\nRelating this back to the gradient method, replace \u03c6 with our gradient map g and let x\u2217be a strict saddle\npoint. We \ufb01rst record a very useful fact:\nProposition 4.5. The gradient mapping g with step size \u03b1 < 1\nL is a diffeomorphism.\nWe will prove this proposition below. But let us \ufb01rst continue to apply the stable manifold theorem.\nNote that Dg(x) = I \u2212\u03b1\u22072f(x). Thus, the set W cs\nloc is a manifold of dimension equal to the number of\nnon-negative eigenvalues of the \u22072f(x). Note that by the strict saddle assumption, this manifold has strictly\npositive codimension and hence has measure zero.\nLet B be the neighborhood of x\u2217promised by the Stable Manifold Theorem. If x converges to x\u2217\nunder the gradient map, then there exists a T such that gt(x) \u2208B for all t \u2265T. This means that gt(x) \u2208\n\u2229\u221e\nk=0g\u2212k(B), and hence, gt(x) \u2208W cs\nloc. That is, we have shown that\nW s(x\u2217) \u2286\n\u221e\n[\nl\u22650\ng\u2212l\n \u221e\n\\\nk=0\ng\u2212k(B)\n!\n.\nSince diffeomorphisms map sets of measure zero to sets of measure zero, and countable unions of measure\nzero sets have measure zero, we conclude that W s has measure zero. That is, we have proven Theorem 4.1.\n4.1\nProof of Proposition 4.5\nWe \ufb01rst check that g is injective from Rn \u2192Rn for \u03b1 < 1\nL. Suppose that there exist x and y such that\ng(x) = g(y). Then we would have x \u2212y = \u03b1(\u2207f(x) \u2212\u2207f(y)) and hence\n\u2225x \u2212y\u2225= \u03b1\u2225\u2207f(x) \u2212\u2207f(y)\u2225\u2264\u03b1L\u2225x \u2212y\u2225.\nSince \u03b1L < 1, this means x = y.\nTo show the gradient map is surjective, we will construct an explicit inverse function. The inverse of the\ngradient mapping is given by performing the proximal point algorithm on the function \u2212f. The proximal\npoint mapping of \u2212f centered at y is given by\nxy = arg min\nx\n1\n2 \u2225x \u2212y\u22252 \u2212\u03b1f(x).\nFor \u03b1 < 1\nL, the function above is strongly convex with respect to x, so there is a unique minimizer. Let xy\nbe the unique minimizer, then by KKT conditions,\ny = xy \u2212\u2207f(xy) = g(xy) .\nHence, xy is mapped to y by the gradient map.\nWe have already shown that g is a bijection, and continuously differentiable. Since Dg(x) = I \u2212\n\u03b1\u22072f(x) is invertible for \u03b1 < 1\nL, the inverse function theorem guarantees g\u22121 is continuously differentiable,\ncompleting the proof that g is a diffeomorphism.\n4.2\nFurther consequences of Theorem 4.1\nCorollary 4.6. Let C be the set of saddle points and assume they are all strict. If C has at most countably\nin\ufb01nite cardinality, then\nPr(lim\nk xk \u2208C) = 0.\n6\nProof. By applying Corollary 4.1 to each point x\u2217\u2208C, we have that Pr(limk xk = x\u2217) = 0. Since the\ncritical points are countable, the conclusion follows since countable union of null sets is a null set.\nRemark 4.7. If the saddle points are isolated points, then the set of saddle points is at most countably in\ufb01nite.\nTheorem 4.8. Assume the same conditions as Theorem 4.6 and limk xk exists, thien Pr(limk xk = x\u22c6) = 1,\nwhere x\u22c6is a local minimizer.\nProof. Using the previous theorem, Pr(limk xk \u2208C) = 0. Since limk xk exists and there is zero probability\nof converging to a saddle, then Pr(limk xk = x\u2217) = 1, where x\u2217is a local minimizer.\nWe now discuss two suf\ufb01cient conditions for limk xk to exist. The following proposition prevents xk\nfrom escaping to \u221e, by enforcing that f has compact sublevel sets, {x : f(x) \u2264c}. This is true for any\ncoercive function, lim\u2225x\u2225\u2192\u221ef(x) = \u221e, which holds in most machine learning applications since f is\nusually a loss function.\nProposition 4.9 (Proposition 12.4.4 of [17]). Assume that f is continuously differentiable, has isolated\ncritical points, and compact sublevel sets, then limk xk exists and that limit is a critical point of f.\nThe second suf\ufb01cient condition for limk xk to exist is based on the Lojasiewicz gradient inequality,\nwhich characterizes the steepness of the gradient near a critical point. The Lojasiewicz inequality ensures\nthat the length traveled by the iterates of gradient descent is \ufb01nite. This will also allow us to derive rates of\nconvergence to a local minimum.\nDe\ufb01nition 4.10 (Lojasiewicz Gradient Inequality). A critical point x\u2217is satis\ufb01es the Lojasiewicz gradient\ninequality if there exists a neighborhood U, m, \u01eb > 0, and 0 \u2264a < 1 such that\n\u2225\u2207f(x)\u2225\u2265m|f(x) \u2212f(x\u2217)|a\n(3)\nfor all x in {x \u2208U : f(x\u2217) < f(x) < f(x\u2217) + \u01eb}.\nThe Lojasiewicz inequality is very general as discussed in [7, 4, 5]. In fact every analytic function\nsatis\ufb01es the Lojasiewicz inequality. Also if the solution is \u00b5-strongly convex in a neighborhood, then the\nLojasiewicz inequality is satis\ufb01ed with parameters a = 1\n2, and m = \u221a2\u00b5.\nProposition 4.11. Assume the same conditions as Theorem 4.6, and the iterates do not escape to \u221e, mean-\ning {xk} is a bounded sequence. Then limk xk exists and limk xk = x\u2217for a local minimum x\u2217.\nFurthermore if x\u2217satis\ufb01es the Lojasiewicz gradient inequality for 0 < a \u22641\n2, then for some C and\nb < 1 independent of k,\n\u2225xk \u2212x\u2217\u2225\u2264Cbk.\nFor 1\n2 < a < 1,\n\u2225xk \u2212x\u2217\u2225\u2264\nC\nk(1\u2212a)/(2a\u22121) .\n7\nProof. The \ufb01rst part of the theorem follows from [1], which shows that limk xk exists. By Theorem 4.8,\nlimk xk is a local minimizer x\u2217. Without loss of generality, we may assume that f(x\u2217) = 0 by shifting the\nfunction.\n[1] also establish\n\u221e\nX\nj=k\n\u2225xj+1 \u2212xj\u2225\u2264\n2\n\u03b1m(1 \u2212a)f(xk)1\u2212a.\nDe\ufb01ne ek = P\u221e\nj=k \u2225xj+1 \u2212xj\u2225, and since ek \u2265\u2225xk \u2212x\u2217\u2225it suf\ufb01ces to upper bound ek.\nSince we have established that xk converges, for k large enough we can use the gradient inequality and\n\u2207f(xk) = xk\u2212xk+1\n\u03b1\n:\nek \u2264\n2\n\u03b1m(1 \u2212a)f(xk)1\u2212a\n\u2264\n2\n\u03b1m1/a(1 \u2212a) \u2225\u2207f(xk)\u2225(1\u2212a)/a\n\u2264\n2\n(m\u03b1)1/a(1 \u2212a)(ek \u2212ek+1)(1\u2212a)/a.\nDe\ufb01ne \u03b2 =\n2\n(m\u03b1)1/a(1\u2212a) and d =\na\n1\u2212a. First consider the case 0 \u2264a \u22641\n2, then d \u22641. Thus,\nek \u2264\u03b2(ek \u2212ek+1)1/d\nek+1 \u2264ek \u2212\n\u0012ek\n\u03b2\n\u0013d\n\u2264\n\u0012\n1 \u22121\n\u03b2d\n\u0013\nek,\nwhere the last inequality uses ek < 1 and d \u22641.\nFor 1\n2 < a < 1, we have established ek+1 \u2264ek \u2212\n1\n\u03b2d ed\nk.\nWe show by induction that ek+1 \u2264\nC\n(k+1)(1\u2212a)/(2a\u22121) . The inductive hypothesis guarantees us ek \u2264\nC\nk(1\u2212a)/(2a\u22121) , so\nek+1 \u2264\nC\nk(1\u2212a)/(2a\u22121) \u2212\nCd/\u03b2d\nka/(2a\u22121)\n=\nCk \u2212Cd/\u03b2d\nk \u00b7 k(1\u2212a)/(2a\u22121)\n\u2264\nC(k \u2212Cd\u22121/\u03b2d)\n(k \u22121)(k + 1)(1\u2212a)/(2a\u22121) .\nFor Cd\u22121 > \u03b2d,we have shown ek+1 \u2264\nC\n(k+1)(1\u2212a)/(2a\u22121) .\n5\nConclusion\nWe have shown that gradient descent with random initialization and appropriate constant step size does not\nconverge to a saddle point. Our analysis relies on a characterization of the local stable set from the theory\nof invariant manifolds. The geometric characterization is not speci\ufb01c to the gradient descent algorithm. To\n8\nuse Theorem 4.1, we simply need the update step of the algorithm to be a diffeomorphism. For example if\ng is the mapping induced by the proximal point algorithm, then g is a diffeomorphism with inverse given\nby gradient ascent on \u2212f. Thus the results in Section 4 also apply to the proximal point algorithm. That is,\nthe proximal point algorithm does not converge to saddles. We expect that similar arguments can be used\nto show ADMM, mirror descent and coordinate descent do not converge to saddle points under appropriate\nchoices of step size. Indeed, convergence to minimizers has been empirically observed for the ADMM\nalgorithm [26].\nIt is not clear if the step size restriction (\u03b1 < 1/L) is necessary to avoid saddle points. Most of the\nconstructions where the gradient method converges to saddle points require fragile initial conditions as\ndiscussed in Section 3. It remains a possibility that methods that choose step sizes greedily, by Wolfe Line\nSearch or backtracking, may still avoid saddle points provided the initial point is chosen at random. We\nleave such investigations for future work.\nAnother important piece of future work would be relaxing the conditions on isolated saddle points. It\nis possible that for the structured problems that arise in machine learning, whether in matrix factorization\nor convolutional neural networks, that saddle points are isolated after taking a quotient with respect to\nthe associated symmetry group of the problem. Techniques from dynamical systems on manifolds may\nbe applicable to understand the behavior of optimization algorithms on problems with a high degree of\nsymmetry.\nIt is also important to understand how stringent the strict saddle assumption is. Will a perturbation of a\nfunction always satisfy the strict saddle property? [2] provide very general suf\ufb01cient conditions for a random\nfunction to be Morse, meaning the eigenvalues at critical points are non-zero, which implies the strict saddle\ncondition. These conditions rely on checking the density of \u22072f(x) has full support conditioned on the\nevent that \u2207f(x) = 0. This can be explicitly veri\ufb01ed for functions f that arise from learning problems.\nHowever, we note that there are very dif\ufb01cult unconstrained optimization problems where the strict\nsaddle condition fails. Perhaps the simplest is optimization of quartic polynomials. Indeed, checking if 0 is\na local minimizer of the quartic\nf(x) =\nn\nX\ni,j=1\nqijx2\ni x2\nj\nis equivalent to checking whether the matrix Q = [qij] is co-positive, a co-NP complete problem. For this\nf, the Hessian at x = 0 is zero. Interestingly, the strict saddle property failing is analogous in dynamical\nsystems to the existence of a slow manifold where complex dynamics may emerge. Slow manifolds give rise\nto metastability, bifurcation, and other chaotic dynamics, and it would be intriguing to see how the analysis\nof chaotic systems could be applied to understand the behavior of optimization algorithms around these\ndif\ufb01cult critical points.\nAcknowledgements\nThe authors would like to thank Chi Jin, Tengyu Ma, Robert Nishihara, Mahdi Soltanolkotabi, Yuekai Sun,\nJonathan Taylor, and Yuchen Zhang for their insightful feedback. MS is generously supported by an NSF\nGraduate Research Fellowship. BR is generously supported by ONR awards N00014-14-1-0024, N00014-\n15-1-2620, and N00014-13-1-0129, and NSF awards CCF-1148243 and CCF-1217058. MIJ is generously\nsupported by ONR award N00014-11-1-0688 and by the ARL and the ARO under grant number W911NF-\n11-1-0391. This research is supported in part by NSF CISE Expeditions Award CCF-1139158, DOE Award\nSN10040 de-sc0012463, and DARPA XData Award FA8750-12-2-0331, and gifts from Amazon Web Ser-\n9\nvices, Google, IBM, SAP, The Thomas and Stacey Siebel Foundation, Adatao, Adobe, Apple Inc., Blue\nGoji, Bosch, Cisco, Cray, Cloudera, Ericsson, Facebook, Fujitsu, Guavus, HP, Huawei, Intel, Microsoft,\nPivotal, Samsung, Schlumberger, Splunk, State Farm, Virdata and VMware.\nReferences\n[1] Pierre-Antoine Absil, Robert Mahony, and Benjamin Andrews. Convergence of the iterates of descent methods\nfor analytic cost functions. SIAM Journal on Optimization, 16(2):531\u2013547, 2005.\n[2] Robert J Adler and Jonathan E Taylor. Random \ufb01elds and geometry. Springer Science & Business Media, 2009.\n[3] Sanjeev Arora, Rong Ge, Tengyu Ma, and Ankur Moitra. Simple, ef\ufb01cient, and neural algorithms for sparse\ncoding. In Proceedings of The 28th Conference on Learning Theory, pages 113\u2013149, 2015.\n[4] H\u00b4edy Attouch, J\u00b4er\u02c6ome Bolte, Patrick Redont, and Antoine Soubeyran. Proximal alternating minimization and\nprojection methods for nonconvex problems: an approach based on the Kurdyka-Lojasiewicz inequality. Math-\nematics of Operations Research, 35(2):438\u2013457, 2010.\n[5] Hedy Attouch, J\u00b4er\u02c6ome Bolte, and Benar Fux Svaiter. Convergence of descent methods for semi-algebraic and\ntame problems: proximal algorithms, forward\u2013backward splitting, and regularized Gauss\u2013Seidel methods. Math-\nematical Programming, 137(1-2):91\u2013129, 2013.\n[6] Antonio Auf\ufb01nger, G\u00b4erard Ben Arous, and Ji\u02c7r\u00b4\u0131 \u02c7Cern`y. Random matrices and complexity of spin glasses. Com-\nmunications on Pure and Applied Mathematics, 66(2):165\u2013201, 2013.\n[7] J\u00b4er\u02c6ome Bolte, Aris Daniilidis, Olivier Ley, Laurent Mazet, et al. Characterizations of Lojasiewicz inequalities:\nsubgradient \ufb02ows, talweg, convexity. Trans. Amer. Math. Soc, 362(6):3319\u20133363, 2010.\n[8] T Tony Cai, Xiaodong Li, and Zongming Ma. Optimal rates of convergence for noisy sparse phase retrieval via\nthresholded Wirtinger \ufb02ow. arXiv preprint arXiv:1506.03382, 2015.\n[9] Emmanuel J Candes, Xiaodong Li, and Mahdi Soltanolkotabi. Phase retrieval via Wirtinger \ufb02ow: Theory and\nalgorithms. IEEE Transactions on Information Theory, 61(4):1985\u20132007, 2015.\n[10] Anna Choromanska, Mikael Henaff, Michael Mathieu, G\u00b4erard Ben Arous, and Yann LeCun. The loss surface of\nmultilayer networks. arXiv:1412.0233, 2014.\n[11] Andrew R Conn, Nicholas IM Gould, and Ph L Toint. Trust region methods, volume 1. SIAM, 2000.\n[12] Yann N Dauphin, Razvan Pascanu, Caglar Gulcehre, Kyunghyun Cho, Surya Ganguli, and Yoshua Bengio.\nIdentifying and attacking the saddle point problem in high-dimensional non-convex optimization. In Advances\nin Neural Information Processing Systems, pages 2933\u20132941, 2014.\n[13] Rong Ge, Furong Huang, Chi Jin, and Yang Yuan. Escaping from saddle points\u2014online stochastic gradient for\ntensor decomposition. arXiv:1503.02101, 2015.\n[14] Philip E Gill and Walter Murray. Newton-type methods for unconstrained and linearly constrained optimization.\nMathematical Programming, 7(1):311\u2013350, 1974.\n[15] M.W. Hirsch, C.C. Pugh, and M. Shub. Invariant Manifolds. Number no. 583 in Lecture Notes in Mathematics.\nSpringer-Verlag, 1977.\n[16] Raghunandan H. Keshavan, Andrea Montanari, and Sewoong Oh. Matrix completion from a few entries. IEEE\nTransactions on Information Theory, 56(6):2980\u20132998, 2009.\n10\n[17] K Lange. Optimization. springer texts in statistics. 2013.\n[18] Jorge J Mor\u00b4e and Danny C Sorensen. On the use of directions of negative curvature in a modi\ufb01ed Newton\nmethod. Mathematical Programming, 16(1):1\u201320, 1979.\n[19] Katta G Murty and Santosh N Kabadi. Some NP-complete problems in quadratic and nonlinear programming.\nMathematical programming, 39(2):117\u2013129, 1987.\n[20] Yurii Nesterov. Introductory lectures on convex optimization, volume 87. Springer Science & Business Media,\n2004.\n[21] Yurii Nesterov and Boris T Polyak. Cubic regularization of Newton method and its global performance. Mathe-\nmatical Programming, 108(1):177\u2013205, 2006.\n[22] Razvan Pascanu, Yann N Dauphin, Surya Ganguli, and Yoshua Bengio. On the saddle point problem for non-\nconvex optimization. arXiv:1405.4604, 2014.\n[23] Robin Pemantle. Nonconvergence to unstable points in urn models and stochastic approximations. The Annals\nof Probability, pages 698\u2013712, 1990.\n[24] Michael Shub. Global stability of dynamical systems. Springer Science & Business Media, 1987.\n[25] Stephen Smale. Differentiable dynamical systems. Bulletin of the American mathematical Society, 73(6):747\u2013\n817, 1967.\n[26] Ju Sun, Qing Qu, and John Wright. Complete dictionary recovery over the sphere I: Overview and the geometric\npicture. arXiv:1511.03607, 2015.\n[27] Ju Sun, Qing Qu, and John Wright. Complete dictionary recovery over the sphere II: Recovery by Riemannian\ntrust-region method. arXiv:1511.04777, 2015.\n[28] Ju Sun, Qing Qu, and John Wright. A geometric analysis of phase retrieval. Forthcoming, 2016.\n[29] Yuchen Zhang, Xi Chen, Denny Zhou, and Michael I Jordan. Spectral methods meet EM: A provably optimal\nalgorithm for crowdsourcing. In Advances in neural information processing systems, pages 1260\u20131268, 2014.\n[30] Tuo Zhao, Zhaoran Wang, and Han Liu. Nonconvex low rank matrix factorization via inexact \ufb01rst order oracle.\n11\n",
        "sentence": "",
        "context": "11-1-0391. This research is supported in part by NSF CISE Expeditions Award CCF-1139158, DOE Award\nSN10040 de-sc0012463, and DARPA XData Award FA8750-12-2-0331, and gifts from Amazon Web Ser-\n9\nJonathan Taylor, and Yuchen Zhang for their insightful feedback. MS is generously supported by an NSF\nGraduate Research Fellowship. BR is generously supported by ONR awards N00014-14-1-0024, N00014-\n9\nvices, Google, IBM, SAP, The Thomas and Stacey Siebel Foundation, Adatao, Adobe, Apple Inc., Blue\nGoji, Bosch, Cisco, Cray, Cloudera, Ericsson, Facebook, Fujitsu, Guavus, HP, Huawei, Intel, Microsoft,"
    },
    {
        "title": "Cubic regularization of Newton method and its global performance",
        "author": [
            "Yurii Nesterov",
            "Boris T Polyak"
        ],
        "venue": "Mathematical Programming,",
        "citeRegEx": "Nesterov and Polyak.,? \\Q2006\\E",
        "shortCiteRegEx": "Nesterov and Polyak.",
        "year": 2006,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "Nonconvergence to unstable points in urn models and stochastic approximations",
        "author": [
            "Robin Pemantle"
        ],
        "venue": "The Annals of Probability,",
        "citeRegEx": "Pemantle.,? \\Q1990\\E",
        "shortCiteRegEx": "Pemantle.",
        "year": 1990,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "A simpler approach to matrix completion",
        "author": [
            "Benjamin Recht"
        ],
        "venue": "The Journal of Machine Learning Research,",
        "citeRegEx": "Recht.,? \\Q2011\\E",
        "shortCiteRegEx": "Recht.",
        "year": 2011,
        "abstract": "This paper provides the best bounds to date on the number of randomly sampled\nentries required to reconstruct an unknown low rank matrix. These results\nimprove on prior work by Candes and Recht, Candes and Tao, and Keshavan,\nMontanari, and Oh. The reconstruction is accomplished by minimizing the nuclear\nnorm, or sum of the singular values, of the hidden matrix subject to agreement\nwith the provided entries. If the underlying matrix satisfies a certain\nincoherence condition, then the number of entries required is equal to a\nquadratic logarithmic factor times the number of parameters in the singular\nvalue decomposition. The proof of this assertion is short, self contained, and\nuses very elementary analysis. The novel techniques herein are based on recent\nwork in quantum information theory.",
        "full_text": "arXiv:0910.0651v2  [cs.IT]  21 Oct 2009\nA Simpler Approach to Matrix Completion\nBenjamin Recht\nDepartment of Computer Sciences, University of Wisconsin-Madison\n1210 W Dayton St, Madison, WI 53706\nemail: brecht@cs.wisc.edu\nOctober 2009\nAbstract\nThis paper provides the best bounds to date on the number of randomly sampled entries required to reconstruct\nan unknown low rank matrix. These results improve on prior work by Cand`es and Recht [4], Cand`es and Tao [7],\nand Keshavan, Montanari, and Oh [18]. The reconstruction is accomplished by minimizing the nuclear norm, or\nsum of the singular values, of the hidden matrix subject to agreement with the provided entries. If the underlying\nmatrix satis\ufb01es a certain incoherence condition, then the number of entries required is equal to a quadratic logarithmic\nfactor times the number of parameters in the singular value decomposition. The proof of this assertion is short, self\ncontained, and uses very elementary analysis. The novel techniques herein are based on recent work in quantum\ninformation theory.\nKeywords. Matrix completion, low-rank matrices, convex optimization, nuclear norm minimization, random\nmatrices, operator Chernoff bound, compressed sensing.\n1\nIntroduction\nRecovering a low rank matrix from a given subset of its entries is a recurring problem in collaborative \ufb01ltering [25],\ndimensionality reduction [20, 28], and multi-class learning [2, 22]. While a variety of heuristics have been devel-\noped across many disciplines, the general problem of \ufb01nding the lowest rank matrix satisfying equality constraints\nis NP-hard. All known algorithms which can compute the lowest rank solution for all instances require time at least\nexponential in the dimensions of the matrix in both theory and practice [9].\nIn sharp contrast to such worst case pessimism, Cand`es and Recht showed that most low rank matrices could be\nrecovered from most suf\ufb01ciently large sets of entries by computing the matrix of minimum nuclear norm that agreed\nwith the provided entries [4], and furthermore the revealed set of entries could comprise a vanishing fraction of the\nentire matrix. The nuclear norm is equal to the sum of the singular values of a matrix and is the best convex lower\nbound of the rank function on the set of matrices whose singular values are all bounded by 1. The intuition behind\nthis heuristic is that whereas the rank function counts the number of nonvanishing singular values, the nuclear norm\nsums their amplitude, much like how the \u21131 norm is a useful surrogate for counting the number of nonzeros in a vector.\nMoreover, the nuclear norm can be minimized subject to equality constraints via semide\ufb01nite programming.\nNuclear norm minimization had long been observed to produce very low-rank solutions in practice (see, for exam-\nple [3,11,12,21,26]), but only very recently was there any theoretical basis for when it produced the minimum rank\nsolution. The \ufb01rst paper to provide such foundations was [24], where Recht, Fazel, and Parrilo developed probabilistic\ntechniques to study average case behavior and showed that the nuclear norm heuristic could solve most instances of\nthe rank minimization problem assuming the number of linear constraints was suf\ufb01ciently large. The results in [24] in-\nspired a groundswell of interest in theoretical guarantees for rank minimization, and these results lay the foundation\nfor [4]. Cand`es and Recht\u2019s bounds were subsequently improved by Cand`es and Tao [7] and Keshavan, Montanari,\nand Oh [18] to show that one could, in special cases, reconstruct a low-rank matrix by observing a set of entries of size\nat most a polylogarithmic factor larger than the intrinsic dimension of the variety of rank r matrices.\n1\nThis paper sharpens the results in [7,18] to provide a bound on the number of entries required to reconstruct a low\nrank matrix which is optimal up to a small numerical constant and one logarithmic factor. The main theorem makes\nminimal assumptions about the low rank matrix of interest. Moreover, the proof is very short and relies on mostly\nelementary analysis.\nIn order to precisely state the main result, we need one de\ufb01nition. Cand`es and Recht observed that it is impossible\nto recover a matrix which is equal to zero in nearly all of its entries unless all of the entries of the matrix are observed\n(consider, for example, the rank one matrix which is equal to 1 in one entry and zeros everywhere else). In other\nwords, the matrix cannot be mostly equal to zero on the observed entries. This motivated the following de\ufb01nition\nDe\ufb01nition 1.1 Let U be a subspace of Rn of dimension r and PU be the orthogonal projection onto U. Then the\ncoherence of U (vis-`a-vis the standard basis (ei)) is de\ufb01ned to be\n\u00b5(U) \u2261n\nr max\n1\u2264i\u2264n \u2225PUei\u22252.\n(1.1)\nNote that for any subspace, the smallest \u00b5(U) can be is 1, achieved, for example, if U is spanned by vectors whose\nentries all have magnitude 1/\u221an. The largest possible value for \u00b5(U) is n/r which would correspond to any subspace\nthat contains a standard basis element. If a matrix has row and column spaces with low coherence, then each entry can\nbe expected to provide about the same amount of information.\nRecall that the nuclear norm of an n1\u00d7n2 matrix X is the sum of the singular values of X, \u2225X\u2225\u2217= Pmin{n1,n2}\nk=1\n\u03c3k(X),\nwhere, here and below, \u03c3k(X) denotes the kth largest singular value of X. The main result of this paper is the fol-\nlowing\nTheorem 1.1 Let M be an n1 \u00d7 n2 matrix of rank r with singular value decomposition U\u03a3V \u2217. Without loss of\ngenerality, impose the conventions n1 \u2264n2, \u03a3 is r \u00d7 r, U is n1 \u00d7 r and V is n2 \u00d7 r. Assume that\nA0 The row and column spaces have coherences bounded above by some positive \u00b50.\nA1 The matrix UV \u2217has a maximum entry bounded by \u00b51\np\nr/(n1n2) in absolute value for some positive \u00b51.\nSuppose m entries of M are observed with locations sampled uniformly at random. Then if\nm \u226532 max{\u00b52\n1, \u00b50} r(n1 + n2) \u03b2 log2(2n2)\n(1.2)\nfor some \u03b2 > 1, the minimizer to the problem\nminimize\n\u2225X\u2225\u2217\nsubject to\nXij = Mij\n(i, j) \u2208\u2126.\n(1.3)\nis unique and equal to M with probability at least 1 \u22126 log(n2)(n1 + n2)2\u22122\u03b2 \u2212n2\u22122\u03b21/2\n2\n.\nThe assumptions A0 and A1 were introduced in [4]. Both \u00b50 and \u00b51 may depend on r, n1, or n2. Moreover,\nnote that \u00b51 \u2264\u00b50\n\u221ar by the Cauchy-Schwarz inequality. As shown in [4], both subspaces selected from the uniform\ndistribution and spaces constructed as the span of singular vectors with bounded entries are not only incoherent with\nthe standard basis, but also obey A1 with high probability for values of \u00b51 at most logarithmic in n1 and/or n2.\nApplying this theorem to the models studied in Section 2 of [4], we \ufb01nd that there is a numerical constant cu such\nthat cur(n1 + n2) log5(n2) entries are suf\ufb01cient to reconstruct a rank r matrix whose row and column spaces are\nsampled from the Haar measure on the Grassmann manifold. If r > log(n2), the number of entries can be reduced\nto cur(n1 + n2) log4(n2). Similarly, there is a numerical constant ci such that ci\u00b52\n0r(n1 + n2) log3(n2) entries are\nsuf\ufb01cient to recover a matrix of arbitrary rank r whose singular vectors have entries with magnitudes bounded by\np\n\u00b50/n1.\nTheorem 1.1 greatly improves upon prior results. First of all, it has the weakest assumptions on the matrix to be\nrecovered. In addition to assumption A1, Cand`es and Tao require a \u201cstrong incoherence condition\u201d (see [7]) which is\nconsiderably more restrictive than the assumption A0 in Theorem 1.1. Many of their results also require restrictions\non the rank of M, and their bounds depend superlinearly on \u00b50. Keshavan et al require the matrix rank to be no more\n2\nthan log(n2), and require bounds on the maximum magnitude of the entries in M and the ratios \u03c31(M)/\u03c3r(M) and\nn2/n1. Theorem 1.1 makes no such assumptions about the rank, aspect ratio, nor condition number of M. Moreover,\n(1.2) has a smaller log factor than [7], and features numerical constants that are both explicit and small.\nAlso note that there is not much room for improvement in the bound for m. It is a consequence of the coupon\ncollector\u2019s problem that at least n2 log n2 uniformly sampled entries are necessary just to guarantee that at least one\nentry in every row and column is observed with high probability. In addition, rank r matrices have r(n1 + n2 \u2212r)\nparameters, a fact that can be veri\ufb01ed by counting the number of degrees of freedom in the singular value decompo-\nsition. Interestingly, Cand`es and Tao showed that C\u00b50n2r log(n2) entries were necessary for completion when the\nentries are sampled uniformly at random [7]. Hence, (1.2) is optimal up to a small numerical constant times log(n2).\nMost importantly, the proof of Theorem 1.1 is short and straightforward. Cand`es and Recht employed sophisti-\ncated tools from the study of random variables on Banach spaces including decoupling tools and powerful moment\ninequalities for the norms of random matrices. Cand`es and Tao rely on intricate moment calculations spanning over 30\npages. The present work only uses basic matrix analysis, elementary large deviation bounds, and a noncommutative\nversion of Bernstein\u2019s Inequality proven here in the Appendix.\nThe proof of Theorem 1.1 is inspired by a recent paper in quanutm information which considered the problem\nof reconstructing the density matrix of a quantum ensemble using as few measurements as possible [16]. Their work\nadapted results from [4] and [5] to the quantum regime by using special algebraic properties of quantum measurements.\nTheir proof followed a methodology analogous to the approach of Cand`es and Recht but had two main differences:\nthey used a sampling with replacement model as a proxy for uniform sampling, and they deployed a powerful non-\ncommutative Chernoff bound developed by Ahlswede and Winter for use in quantum information theory [1]. In this\npaper, I adapt these two strategies from [16] to the matrix completion problem. In section 3 I show how the sampling\nwith replacement model bounds probabilities in the uniform sampling model, and present very short proofs of some\nof the main results in [4]. Surprisingly, this yields a simple proof of Theorem 1.1, provided in Section 4, which has\nthe least restrictive assumptions of any assertion proven thus far.\n2\nPreliminaries and notation\nBefore continuing, let us survey the notations used throughout the paper. I closely follow the conventions established\nin [4], and invite the reader to consult this reference for a more thorough discussion of the matrix completion problem\nand the associated convex geometry. A thorough introduction to the necessary matrix analysis used in this paper can\nbe found in [24].\nMatrices are bold capital, vectors are bold lowercase and scalars or entries are not bold. For example, X is a\nmatrix, and Xij its (i, j)th entry. Likewise x is a vector, and xi its ith component. If uk \u2208Rn for 1 \u2264k \u2264d is\na collection of vectors, [u1, . . . , ud] will denote the n \u00d7 d matrix whose kth column is uk. ek will denote the kth\nstandard basis vector in Rd, equal to 1 in component k and 0 everywhere else. The dimension of ek will always be\nclear from context. X\u2217and x\u2217denote the transpose of matrices X and vectors x respectively.\nA variety of norms on matrices will be discussed. The spectral norm of a matrix is denoted by \u2225X\u2225. The Euclidean\ninner product between two matrices is \u27e8X, Y \u27e9= Tr(X\u2217Y ), and the corresponding Euclidean norm, called the\nFrobenius or Hilbert-Schmidt norm, is denoted \u2225X\u2225F. That is, \u2225X\u2225F = \u27e8X, X\u27e91/2. The nuclear norm of a matrix\nX is \u2225X\u2225\u2217. The maximum entry of X (in absolute value) is denoted by \u2225X\u2225\u221e\u2261maxij |Xij|. For vectors, the only\nnorm applied is the usual Euclidean \u21132 norm, simply denoted as \u2225x\u2225.\nLinear transformations that act on matrices will be denoted by calligraphic letters. In particular, the identity\noperator will be denoted by I. The spectral norm (the top singular value) of such an operator will be denoted by\n\u2225A\u2225= supX:\u2225X\u2225F \u22641 \u2225A(X)\u2225F .\nFix once and for all a matrix M obeying the assumptions of Theorem 1.1. Let uk (respectively vk) denote the kth\ncolumn of U (respectively V ). Set U \u2261span (u1, . . . , ur), and V \u2261span (v1, . . . , vr). Also assume, without loss\nof generality, that n1 \u2264n2. It is convenient to introduce the orthogonal decomposition Rn1\u00d7n2 = T \u2295T \u22a5where T\nis the linear space spanned by elements of the form uky\u2217and xv\u2217\nk, 1 \u2264k \u2264r, where x and y are arbitrary, and T \u22a5\nis its orthogonal complement. T \u22a5is the subspace of matrices spanned by the family (xy\u2217), where x (respectively y)\nis any vector orthogonal to U (respectively V ).\n3\nThe orthogonal projection PT onto T is given by\nPT (Z) = PUZ + ZPV \u2212PUZPV ,\n(2.1)\nwhere PU and PV are the orthogonal projections onto U and V respectively. Note here that while PU and PV are\nmatrices, PT is a linear operator mapping matrices to matrices. The orthogonal projection onto T \u22a5is given by\nPT \u22a5(Z) = (I \u2212PT )(Z) = (In1 \u2212PU)Z(In2 \u2212PV )\nwhere Id denotes the d \u00d7 d identity matrix. It follows from the de\ufb01nition (2.1) of PT that\nPT (eae\u2217\nb) = (PUea)e\u2217\nb + ea(PV eb)\u2217\u2212(PUea)(PV eb)\u2217.\nThis gives\n\u2225PT (eae\u2217\nb)\u22252\nF = \u27e8PT (eae\u2217\nb), eae\u2217\nb\u27e9= \u2225PUea\u22252 + \u2225PV eb\u22252 \u2212\u2225PUea\u22252 \u2225PV eb\u22252 .\nSince \u2225PUea\u22252 \u2264\u00b5(U)r/n1 and \u2225PV eb\u22252 \u2264\u00b5(V )r/n2,\n\u2225PT(eae\u2217\nb)\u22252\nF \u2264max{\u00b5(U), \u00b5(V )}rn1 + n2\nn1n2\n\u2264\u00b50rn1 + n2\nn1n2\n(2.2)\nI will make frequent use of this calculation throughout the sequel.\n3\nSampling with Replacement\nAs discussed above, the main contribution of this work is an analysis of uniformly sampled sets of entries via the study\nof a sampling with replacement model. All of the previous work [4, 7, 18] studied a Bernoulli sampling model as a\nproxy for uniform sampling. There, each entry was revealed independently with probability equal to p. In all of these\nresults, the theorem statements concerned sampling sets of m entries uniformly, but it was shown that probability\nof failure under Bernoulli sampling with p =\nm\nn1n2 closely approximated the probability of failure under uniform\nsampling. The present work will analyze the situation where each entry index is sampled independently from the\nuniform distribution on {1, . . ., n1} \u00d7 {1, . . . , n2}. This modi\ufb01cation of the sampling model gives rise to all of the\nsimpli\ufb01cations below.\nIt would appear that sampling with replacement is not suitable for analyzing matrix completion as one might\nencounter duplicate entries. However, just as is the case with Bernoulli sampling, bounding the likelihood of error\nwhen sampling with replacement allows us to bound the probability of the nuclear norm heuristic failing under uniform\nsampling.\nProposition 3.1 The probability that the nuclear norm heuristic fails when the set of observed entries is sampled\nuniformly from the collection of sets of size m is less than or equal to the probability that the heuristic fails when m\nentries are sampled independently with replacement.\nProof The proof follows the argument in Section II.C of [6]. Let \u2126\u2032 be a collection of m entries, each sampled\nindependently from the uniform distribution on {1, . . . , n1} \u00d7 {1, . . ., n2}. Let \u2126k denote a set of entries of size k\nsampled uniformly from all collections of entries of size k. It follows that\nP(Failure(\u2126\u2032)) =\nm\nX\nk=0\nP(Failure(\u2126\u2032) | |\u2126\u2032| = k)P(|\u2126\u2032| = k)\n=\nm\nX\nk=0\nP(Failure(\u2126k))P(|\u2126\u2032| = k)\n\u2265P(Failure(\u2126m))\nm\nX\nk=0\nP(|\u2126\u2032| = k) = P(Failure(\u2126m)) .\n4\nWhere the inequality follows because P(Failure(\u2126m)) \u2265P(Failure(\u2126m\u2032)) if m \u2264m\u2032. That is, the probability\ndecreases as the number of entries revealed is increased.\nSurprisingly, changing the sampling model makes most of the theorems from [4] simple consequences of a non-\ncommutative variant of Bernstein\u2019s Inequality.\nTheorem 3.2 (Noncommutative Bernstein Inequality) Let X1, . . . , XL be independent zero-mean random matri-\nces of dimension d1 \u00d7 d2. Suppose \u03c12\nk = max{\u2225E[XkX\u2217\nk]\u2225, \u2225E[X\u2217\nkXk]\u2225} and \u2225Xk\u2225\u2264M almost surely for all k.\nThen for any \u03c4 > 0,\nP\n\"\r\r\r\r\r\nL\nX\nk=1\nXk\n\r\r\r\r\r > \u03c4\n#\n\u2264(d1 + d2) exp\n \n\u2212\u03c4 2/2\nPL\nk=1 \u03c12\nk + M\u03c4/3\n!\n.\nNote that in the case that d1 = d2 = 1, this is precisely the two sided version of the standard Bernstein In-\nequality. When the Xk are diagonal, this bound is the same as applying the standard Bernstein Inequality and a\nunion bound to the diagonal of the matrix summation. Furthermore, observe that the right hand side is less than\n(d1 + d2) exp(\u22123\n8\u03c4 2/(PL\nk=1 \u03c12\nk)) as long as \u03c4 \u2264\n1\nM\nPL\nk=1 \u03c12\nk. This condensed form of the inequality will be used\nexclusively throughout. Theorem 3.2 is a corollary of an Chernoff bound for \ufb01nite dimensional operators developed\nby Ahlswede and Winter [1]. A similar inequality for symmetric i.i.d. matrices is proposed in [16]. The proof is\nprovided in the Appendix.\nLet us now record two theorems, proven for the Bernoulli model in [4], that admit very simple proofs in the\nsampling with replacement model. The theorem statements requires some additional notation. Let \u2126= {(ak, bk)}l\nk=1\nbe a collection of indices sampled uniformly with replacement. Set R\u2126to be the operator\nR\u2126(Z) =\n|\u2126|\nX\nk=1\n\u27e8eake\u2217\nbk, Z\u27e9eake\u2217\nbk .\nNote that the (i, j)th component of R\u2126(X) is zero unless (i, j) \u2208\u2126. For (i, j) \u2208\u2126, R\u2126(X) is equal to Xij times\nthe multiplicity of (i, j) \u2208\u2126. Unlike in previous work on matrix completion, R\u2126is not a projection operator if there\nare duplicates in \u2126. Nonetheless, this does not adversely affect the argument, and R\u2126(X) = 0 if and only if Xab = 0\nfor all (a, b) \u2208\u2126. Moreover, we can show that the maximum duplication of any entry is always less than 8\n3 log(n2)\nwith very high probability.\nProposition 3.3 With probability at least 1 \u2212n2\u22122\u03b2\n2\n, the maximum number of repetitions of any entry in \u2126is less than\n8\n3\u03b2 log(n2) for n2 \u22659 and \u03b2 > 1.\nProof This assertion can be proven by applying a standard Chernoff bound for the Bernoulli distribution. Note that for\na \ufb01xed entry, the probability it is sampled more than t times is equal to the probability of more than t heads occurring\nin a sequence of m tosses where the probability of a head is\n1\nn1n2 . This probability can be upper bounded by\nP[more than t heads in m trials] \u2264\n\u0012\nm\nn1n2t\n\u0013t\nexp\n\u0012\nt \u2212\nm\nn1n2\n\u0013\n(see [17], for example). Applying the union bound over all of the n1n2 entries and the fact that\nm\nn1n2 < 1, we have\nP[any entry is selected more than 8\n3\u03b2 log(n2) times] \u2264n1n2\n\u0000 8\n3\u03b2 log(n2)\n\u0001\u22128\n3 \u03b2 log(n2) exp\n\u0000 8\n3\u03b2 log(n2)\n\u0001\n\u2264n2\u22122\u03b2\n2\nwhen n2 \u22659.\nThis application of the Chernoff bound is very crude, and much tighter bounds can be derived using more careful\nanalysis. For example in [15], the maximum oversampling is shown to be bounded by O(\nlog(n2)\nlog log(n2)). For our purposes\nhere, the loose upper bound provided by Proposition 3.3 will be more than suf\ufb01cient.\nIn addition to this bound on the norm of R\u2126, the following theorem asserts that the operator PT R\u2126PT is also\nvery close to an isometry on T if the number of sampled entries is suf\ufb01ciently large. This result is analgous to the\nTheorem 4.1 in [4] for the Bernoulli model, whose proof uses several powerful theorems from the study of probability\nin Banach spaces. Here, one only needs to compute a few low order moments and then apply Theorem 3.2.\n5\nTheorem 3.4 Suppose \u2126is a set of entries of size m sampled independently and uniformly with replacement. Then\nfor all \u03b2 > 1,\nn1n2\nm\n\r\r\r\rPT R\u2126PT \u2212\nm\nn1n2\nPT\n\r\r\r\r \u2264\nr\n16\u00b50r(n1 + n2) \u03b2 log(n2)\n3m\nwith probability at least 1 \u22122n2\u22122\u03b2\n2\nprovided that m > 16\n3 \u00b50r(n1 + n2) \u03b2 log(n2).\nProof Decompose any matrix Z as Z = P\nab\u27e8Z, eae\u2217\nb\u27e9eae\u2217\nb so that\nPT (Z) =\nX\nab\n\u27e8PT (Z), eae\u2217\nb\u27e9eae\u2217\nb =\nX\nab\n\u27e8Z, PT (eae\u2217\nb)\u27e9eae\u2217\nb.\n(3.1)\nFor k = 1, . . . , m sample (ak, bk) from {1, . . . , n1} \u00d7 {1, . . . , n2} uniformly with replacement. Then R\u2126PT (Z) =\nPm\nk=1 \u27e8Z, PT (eake\u2217\nbk)\u27e9eake\u2217\nbk which gives\n(PT R\u2126PT )(Z) =\nm\nX\nk=1\n\u27e8Z, PT (eake\u2217\nbk)\u27e9PT (eake\u2217\nbk).\nNow the fact that the operator PT R\u2126PT does not deviate from its expected value\nE(PT R\u2126PT ) = PT (E R\u2126)PT = PT ( m\nn1n2\nI)PT =\nm\nn1n2\nPT\nin the spectral norm can be proven using the Noncommutative Bernstein Inequality.\nTo proceed, de\ufb01ne the operator Tab which maps Z to \u27e8PT (eae\u2217\nb), Z\u27e9PT (eae\u2217\nb). This operator is rank one, has\noperator norm \u2225Tab\u2225= \u2225PT (eae\u2217\nb)\u22252\nF , and we have PT = P\na,b Tab by (3.1). Hence, for k = 1, . . . , m, E[Takbk] =\n1\nn1n2 PT .\nObserve that if A and B are positive semide\ufb01nite, we have \u2225A \u2212B\u2225\u2264max{\u2225A\u2225, \u2225B\u2225}. Using this fact, we\ncan compute the bound\n\u2225Takbk \u2212\n1\nn1n2 PT \u2225\u2264max{\u2225PT(eake\u2217\nbk)\u22252\nF ,\n1\nn1n2 } \u2264\u00b50rn1 + n2\nn1n2\n,\nwhere the \ufb01nal inequality follows from (2.2). We also have\n\u2225E[(Takbk \u2212\n1\nn1n2 PT )2]\u2225= \u2225E[\u2225PT (eake\u2217\nbk)\u22252\nF Takbk] \u2212\n1\nn2\n1n2\n2\nPT ]\u2225\n\u2264max{\u2225E[\u2225PT (eake\u2217\nbk)\u22252\nF Takbk]\u2225,\n1\nn2\n1n2\n2\n}\n\u2264max{\u2225E[Takbk]\u2225\u00b50rn1 + n2\nn1n2\n,\n1\nn2\n1n2\n2\n} \u2264\u00b50rn1 + n2\nn2\n1n2\n2\nThe theorem now follows by applying the Noncommutative Bernstein Inequality.\nThe next theorem is an analog of Theorem 6.3 in [4] or Lemma 3.2 in [18]. This theorem asserts that for a \ufb01xed\nmatrix, if one sets all of the entries not in \u2126to zero it remains close to a multiple of the original matrix in the operator\nnorm.\nTheorem 3.5 Suppose \u2126is a set of entries of size m sampled independently and uniformly with replacement and let\nZ be a \ufb01xed n1 \u00d7 n2 matrix. Assume without loss of generality that n1 \u2264n2, Then for all \u03b2 > 1,\n\r\r\r\n\u0010n1n2\nm R\u2126\u2212I\n\u0011\n(Z)\n\r\r\r \u2264\nr\n8\u03b2n1n2\n2 log(n1 + n2)\n3m\n\u2225Z\u2225\u221e\nwith probability at least 1 \u2212(n1 + n2)1\u2212\u03b2 provided that m > 6\u03b2n1 log(n1 + n2).\n6\nProof First observe that the operator norm can be upper bounded by a multiple of the matrix in\ufb01nity norm\n\u2225Z\u2225= sup\n\u2225x\u2225=1\n\u2225y\u2225=1\nX\na,b\nZabyaxb \u2264\n\uf8eb\n\uf8edX\na,b\nZ2\naby2\na\n\uf8f6\n\uf8f8\n1/2 \uf8eb\n\uf8edX\na,b\nx2\nb\n\uf8f6\n\uf8f8\n1/2\n\u2264\u221an2 max\na\n X\nb\nZ2\nab\n!1/2\n\u2264\u221an1n2\u2225Z\u2225\u221e\nNote that n1n2\nm R\u2126(Z)\u2212Z = 1\nm\nPm\nk=1 n1n2Zakbkeake\u2217\nbk \u2212Z. This is a sum of zero-mean random matrices, and\n\u2225n1n2Zakbkeake\u2217\nbk \u2212Z\u2225\u2264\u2225n1n2Zakbkeake\u2217\nbk\u2225+ \u2225Z\u2225< 3\n2n1n2\u2225Z\u2225\u221efor n1 \u22652. We also have\n\r\rE\n\u0002\n(n1n2Zakbkeake\u2217\nbk \u2212Z)\u2217(n1n2Zakbkeake\u2217\nbk \u2212Z)\n\u0003\r\r =\n\r\r\r\r\r\r\nn1n2\nX\nc,d\nZ2\ncdede\u2217\nd \u2212Z\u2217Z\n\r\r\r\r\r\r\n\u2264max\n\uf8f1\n\uf8f2\n\uf8f3\n\r\r\r\r\r\r\nn1n2\nX\nc,d\nZ2\ncdede\u2217\nd\n\r\r\r\r\r\r\n, \u2225Z\u2217Z\u2225\n\uf8fc\n\uf8fd\n\uf8fe\n\u2264n1n2\n2\u2225Z\u22252\n\u221e\nwhere we again use the fact that \u2225A \u2212B\u2225\u2264max{\u2225A\u2225, \u2225B\u2225} for positive semide\ufb01nite A and B. A similar calcula-\ntion holds for (n1n2Zakbkeake\u2217\nbk \u2212Z)(n1n2Zakbkeake\u2217\nbk \u2212Z)\u2217. The theorem now follows by the Noncommutative\nBernstein Inequality.\nFinally, the following Lemma is required to prove Theorem 1.1. Succinctly, it says that for a \ufb01xed matrix in T , the\noperator PT R\u2126does not increase the matrix in\ufb01nity norm.\nLemma 3.6 Suppose \u2126is a set of entries of size m sampled independently and uniformly with replacement and let\nZ \u2208T be a \ufb01xed n1 \u00d7 n2 matrix. Assume without loss of generality that n1 \u2264n2. Then for all \u03b2 > 2,\n\r\r\rn1n2\nm PT R\u2126(Z) \u2212Z\n\r\r\r\n\u221e\u2264\nr\n8\u03b2\u00b50r(n1 + n2) log n2\n3m\n\u2225Z\u2225\u221e\nwith probability at least 1 \u22122n2\u2212\u03b2\n2\nprovided that m > 8\n3\u03b2\u00b50r(n1 + n2) log n2.\nProof This lemma can be proven using the standard Bernstein Inequality. For each matrix index (c, d), sample (a, b)\nuniformly at random to de\ufb01ne the random variable \u03becd = \u27e8ece\u2217\nd, n1n2\u27e8eae\u2217\nb, Z\u27e9PT (eae\u2217\nb)\u2212Z\u27e9. We have E[\u03becd] = 0,\n|\u03becd| \u2264\u00b50r(n1 + n2)\u2225Z\u2225\u221e, and\nE[\u03be2\ncd] =\n1\nn1n2\nX\na,b\n\u27e8ece\u2217\nd, n1n2\u27e8eae\u2217\nb, Z\u27e9PT (eae\u2217\nb) \u2212Z\u27e92\n= n1n2\nX\na,b\n\u27e8PT (ece\u2217\nd), eae\u2217\nb\u27e92\u27e8eae\u2217\nb, Z\u27e92 \u2212Z2\ncd\n\u2264n1n2\u2225PT (ece\u2217\nd)\u22252\nF \u2225Z\u22252\n\u221e\u2264\u00b50r(n1 + n2)\u2225Z\u22252\n\u221e.\nSince the (c, d) entry of n1n2\nm PT R\u2126(Z) \u2212Z is identically distributed to 1\nm\nPm\nk=1 \u03be(k)\ncd , where \u03be(k)\ncd are i.i.d. copies of\n\u03becd, we have by Bernstein\u2019s Inequality and the union bound:\nPr\n\"\r\r\rn1n2\nm PT R\u2126(Z) \u2212Z\n\r\r\r\n\u221e>\nr\n8\u03b2\u00b50r(n1 + n2) log(n2)\n3m\n\u2225Z\u2225\u221e\n#\n\u22642n1n2 exp(\u2212\u03b2 log(n2)) \u22642n2\u2212\u03b2\n2\n.\n7\n4\nProof of Theorem 1.1\nThe proof follows the program developed in [16] which itself adapted the strategy proposed in [4]. The main idea is\nto approximate a dual feasible solution of (1.3) which certi\ufb01es that M is the unique minimum nuclear norm solution.\nIn [4] such a certi\ufb01cate was constructed via an in\ufb01nite series using a construction developed in the compressed sensing\nliterature [6, 13]. The terms in this series were then analyzed individually using the decoupling inequalities of de la\nPe\u02dcna and Montgomery-Smith [10]. Truncating the in\ufb01nite series after 4 terms gave their result. In [7], the authors\nbounded the contribution of O(log(n2)) terms in this series using intensive combinatorial analysis of each term. The\ninsight in [16] was that, when sampling observations with replacement, a dual feasible solution could be closely\napproximated by a modi\ufb01ed series where each term involved the product of independent random variables. This\nchange in the sampling model allows one to avoid decoupling inequalities and gives rise to the dramatic simpli\ufb01cation\nhere.\nTo proceed, recall again that by Proposition 3.1 it suf\ufb01ces to consider the scenario when the entries are sampled\nindependently and uniformly with replacement. I will \ufb01rst develop the main argument of the proof assuming many\nconditions hold with high probability. The proof is completed by subsequently bounding probability that all of these\nevents hold. Suppose that\nn1n2\nm\n\r\r\r\rPT R\u2126PT \u2212\nm\nn1n2\nPT\n\r\r\r\r \u22641\n2 ,\n\u2225R\u2126\u2225\u22648\n3\u03b21/2 log(n2) .\n(4.1)\nAlso suppose there exists a Y in the range of R\u2126such that\n\u2225PT (Y ) \u2212UV \u2217\u2225F \u2264\nr r\n2n2\n,\n\u2225PT \u22a5(Y )\u2225< 1\n2\n(4.2)\nIf (4.1) holds, then for any Z \u2208ker R\u2126, PT (Z) cannot be too large. Indeed, we have\n0 = \u2225R\u2126(Z)\u2225F \u2265\u2225R\u2126PT (Z)\u2225F \u2212\u2225R\u2126PT \u22a5(Z)\u2225F .\nNow observe that\n\u2225R\u2126PT (Z)\u22252\nF = \u27e8Z, PT R2\n\u2126PT (Z)\u27e9\u2265\u27e8Z, PT R\u2126PT (Z)\u27e9\u2265\nm\n2n1n2\n\u2225PT(Z)\u22252\nF\nand \u2225R\u2126PT \u22a5(Z)\u2225F \u22648\n3\u03b21/2 log(n2)\u2225PT \u22a5(Z)\u2225F . Collecting these facts gives that for any Z \u2208ker R\u2126,\n\u2225PT \u22a5(Z)\u2225F \u2265\ns\n9m\n128\u03b2n1n2 log2(n2)\u2225PT (Z)\u2225F >\nr\n2r\nn2\n\u2225PT(Z)\u2225F .\nNow recall that \u2225A\u2225\u2217= sup\u2225B\u2225\u22641\u27e8A, B\u27e9. For Z \u2208ker R\u2126, pick U\u22a5and V\u22a5such that [U, U\u22a5] and [V , V\u22a5] are\nunitary matrices and that \u27e8U\u22a5V \u2217\n\u22a5, PT \u22a5(Z)\u27e9= \u2225PT \u22a5(Z)\u2225\u2217. Then it follows that\n\u2225M + Z\u2225\u2217\u2265\u27e8UV \u2217+ U\u22a5V \u2217\n\u22a5, M + Z\u27e9\n= \u2225M\u2225\u2217+ \u27e8UV \u2217+ U\u22a5V \u2217\n\u22a5, Z\u27e9\n= \u2225M\u2225\u2217+ \u27e8UV \u2217\u2212PT (Y ), PT (Z)\u27e9+ \u27e8U\u22a5V \u2217\n\u22a5\u2212PT \u22a5(Y ), PT \u22a5(Z)\u27e9\n> \u2225M\u2225\u2217\u2212\nr r\n2n2\n\u2225PT(Z)\u2225F + 1\n2\u2225PT \u22a5(Z)\u2225\u2217\u2265\u2225M\u2225\u2217.\nThe \ufb01rst inequality holds from the variational characterization of the nuclear norm. We also used the fact that \u27e8Y , Z\u27e9=\n0 for all Z \u2208ker R\u2126. Thus, if a Y exists obeying (4.2), we have that for any X obeying R\u2126(X \u2212M) = 0,\n\u2225X\u2225\u2217> \u2225M\u2225\u2217. That is, any if X has Mab = Xab for all (a, b) \u2208\u2126, X has strictly larger nuclear norm than M,\nand hence M is the unique minimizer of (1.3). The remainder of the proof shows that such a Y exists with high\nprobability.\n8\nTo this end, partition 1, . . . , m into p partitions of size q. By assumption, we may choose\nq \u2265128\n3 max{\u00b50, \u00b52\n1}r(n1 + n2)\u03b2 log(n1 + n2)\nand\np \u22653\n4 log(2n2) .\nLet \u2126j denote the set of indices corresponding to the jth partition. Note that each of these partitions are independent\nof one another when the indices are sampled with replacement. Assume that\nn1n2\nq\n\r\r\r\rPT R\u2126kPT \u2212\nq\nn1n2\nPT\n\r\r\r\r \u22641\n2\n(4.3)\nfor all k. De\ufb01ne W0 = UV \u2217and set Yk = n1n2\nq\nPk\nj=1 R\u2126j(Wj\u22121), Wk = UV \u2217\u2212PT(Yk) for k = 1, . . . , p. Then\n\u2225Wk\u2225F =\n\r\r\r\rWk\u22121 \u2212n1n2\nq\nPT R\u2126k(Wk\u22121)\n\r\r\r\r\nF\n=\n\r\r\r\r(PT \u2212n1n2\nq\nPT R\u2126kPT )(Wk\u22121)\n\r\r\r\r\nF\n\u22641\n2\u2225Wk\u22121\u2225F ,\nand it follows that \u2225Wk\u2225F \u22642\u2212k\u2225W0\u2225F = 2\u2212k\u221ar. Since p \u2265\n3\n4 log(2n2) \u22651\n2 log2(2n2) = log2\n\u221a2n2, then\nY = Yp will satisfy the \ufb01rst inequality of (4.2). Also suppose that\n\r\r\r\rWk\u22121 \u2212n1n2\nq\nPT R\u2126k(Wk\u22121)\n\r\r\r\r\n\u221e\n\u22641\n2\u2225Wk\u22121\u2225\u221e\n(4.4)\n\r\r\r\r\n\u0012n1n2\nq\nR\u2126j \u2212I\n\u0013\n(Wj\u22121)\n\r\r\r\r \u2264\ns\n8n1n2\n2\u03b2 log n2\n3q\n\u2225Wj\u22121\u2225\u221e\n(4.5)\nfor k = 1, . . . , p.\nTo see that \u2225PT \u22a5(Yp)\u2225\u22641\n2 when (4.4) and (4.5) hold, observe \u2225Wk\u2225\u221e\u22642\u2212k\u2225UV \u2217\u2225\u221e, and it follows that\n\u2225PT \u22a5Yp\u2225\u2264\np\nX\nj=1\n\u2225n1n2\nq\nPT \u22a5R\u2126jWj\u22121\u2225\n=\np\nX\nj=1\n\u2225PT \u22a5( n1n2\nq\nR\u2126jWj\u22121 \u2212Wj\u22121)\u2225\n\u2264\np\nX\nj=1\n\u2225( n1n2\nq\nR\u2126j \u2212I)(Wj\u22121)\u2225\n\u2264\np\nX\nj=1\ns\n8n1n2\n2 \u03b2 log n2\n3q\n\u2225Wj\u22121\u2225\u221e\n= 2\np\nX\nj=1\n2\u2212j\ns\n8n1n2\n2 \u03b2 log n2\n3q\n\u2225UV \u2217\u2225\u221e<\ns\n32\u00b52\n1rn2 \u03b2 log n2\n3q\n< 1/2\nsince q >\n128\n3 \u00b52\n1rn2\u03b2 log(n2). The \ufb01rst inequality follows from the triangle inequality. The second line follows\nbecause Wj\u22121 \u2208T for all j. The third line follows because, for any Z,\n\u2225PT \u22a5(Z)\u2225= \u2225(In1 \u2212PU)Z(In2 \u2212PV )\u2225\u2264\u2225Z\u2225.\nThe fourth line applies (4.5). The next line follows from (4.4). The \ufb01nal line follows from the assumption A1.\nAll that remains is to bound the probability that all of the invoked events hold. With m satisfying the bound in\nthe main theorem statement, the \ufb01rst inequality in (4.1) fails to hold with probability at most 2n2\u22122\u03b2\n2\nby Theorem 3.4,\nand the second inequality fails to hold with probability at most n2\u22122\u03b21/2\n2\nby Proposition 3.3. For all k, (4.3) fails to\nhold with probability at most 2n2\u22122\u03b2\n2\n, (4.4) fails to hold with probability at most 2n2\u22122\u03b2\n2\n, and (4.5) fails to hold with\nprobability at most (n1 + n2)1\u22122\u03b2. Summing these all together, all of the events hold with probability at least\n1 \u22126 log(n2)(n1 + n2)2\u22122\u03b2 \u2212n2\u22122\u03b21/2\n2\nby the union bound. This completes the proof.\n9\n5\nDiscussion and Conclusions\nThe results proven here are nearly optimal, but small improvements can possibly be made. The numerical constant\n32 in the statement of the theorem may be reducible by more clever bookkeeping, and it may be possible to derive\na linear dependence on the logarithm of the matrix dimensions. But further reduction is not possible because of the\nnecessary conditions provided by Cand`es and Tao. One minor improvement that could be made would be to remove\nthe assumption A1. For instance, while \u00b51 is known to be small in most of the models of low rank matrices that have\nbeen analyzed, no one has shown that an assumption of the form A1 is necessary for completion. Nonetheless, all\nprior results on matrix completion have imposed an assumption like A1 [4,7,18], and it would be interesting to see if\nit can be removed as a requirement, or if it is somehow necessary.\nSurprisingly, the simplicity of the argument presented here mostly arises from the abandonment of Bernoulli\nsampling in favor of sampling with replacement. It would be of interest to review results investigating noise robustness\nof matrix completion [5, 19] or deconvolution of sparse and low rank matrices [8] to see if results can be improved\nby appealing to sampling with replacement. Furthermore, since much of the work on rank minimization and matrix\ncompletion borrows tools from the compressed sensing community, it is of interest to revisit this related body of work\nand to see if proofs can be simpli\ufb01ed or bounds can be improved there as well. The noncommutative versions of\nChernoff and Bernstein\u2019 s Inequalities may be useful throughout machine learning and statistical signal processing,\nand a fruitful line of inquiry would examine how to apply these tools from quantum information to the study of\nclassical signals and systems.\nAcknowledgments\nB.R. would like to thank Aram Harrow for introducing him to the operator Chernoff bound and many helpful clarifying\nconversations, Silvia Gandy for pointing out several typos in the original version of this manuscript, and Rob Nowak,\nAli Rahimi, and Stephen Wright for many fruitful discussions about this paper.\nReferences\n[1] R. Ahlswede and A. Winter. Strong converse for identi\ufb01cation via quantum channels. IEEE Transactions on Information\nTheory, 48(3):569\u2013579, 2002.\n[2] A. Argyriou, C. A. Micchelli, and M. Pontil. Convex multi-task feature learning. Machine Learning, 2008. Published online\n\ufb01rst at http://www.springerlink.com/.\n[3] C. Beck and R. D\u2019Andrea.\nComputational study and comparisons of LFT reducibility methods.\nIn Proceedings of the\nAmerican Control Conference, 1998.\n[4] E. Cand`es and B. Recht. Exact matrix completion via convex optimization. Foundations of Computational Mathematics,\n2008. To appear. Preprint available at http://lanl.arxiv.org/abs/0805.4471.\n[5] E. J. Cand`es and Y. Plan.\nMatrix completion with noise.\nSubmitted to Proceedings of the IEEE. Preprint available at\nhttp://www-stat.stanford.edu/\u02dccandes/publications.html, 2009.\n[6] E. J. Cand`es, J. Romberg, and T. Tao. Robust uncertainty principles: exact signal reconstruction from highly incomplete\nfrequency information. IEEE Trans. Inform. Theory, 52(2):489\u2013509, 2006.\n[7] E. J. Cand`es and T. Tao. The power of convex relaxation: Near-optimal matrix completion. Submitted for publication. Preprint\navailable at http://www-stat.stanford.edu/\u02dccandes/publications.html, 2009.\n[8] V. Chandrasekaran, S. Sanghavi, P. A. Parrilo, and A. S. Willsky. Rank-sparsity incoherence for matrix decomposition.\nSubmitted for publication. Preprint available at http://ssg.mit.edu/group/venkatc/venkatc.shtml, 2009.\n[9] A. L. Chistov and D. Y. Grigoriev. Complexity of quanti\ufb01er elimination in the theory of algebraically closed \ufb01elds. In\nProceedings of the 11th Symposium on Mathematical Foundations of Computer Science, volume 176 of Lecture Notes in\nComputer Science, pages 17\u201331. Springer Verlag, 1984.\n[10] V. H. de la Pe\u02dcna and S. J. Montgomery-Smith. Decoupling inequalities for the tail probabilities of multivariate U-statistics.\nAnn. Probab., 23(2):806\u2013816, 1995.\n[11] M. Fazel. Matrix Rank Minimization with Applications. PhD thesis, Stanford University, 2002.\n10\n[12] M. Fazel, H. Hindi, and S. Boyd. A rank minimization heuristic with application to minimum order system approximation.\nIn Proceedings of the American Control Conference, 2001.\n[13] J. J. Fuchs. On sparse representations in arbitrary redundant bases. IEEE Transactions on Information Theory, 50:1341\u20131344,\n2004.\n[14] S. Golden. Lower bounds for the Helmholtz function. Physical Review, 137B(4):B1127\u20131128, 1965.\n[15] G. H. Gonnet. Expected length of the longest probe sequence in hash code searching. Journal of the Association for Computing\nMachinery, 28(2):289\u2013304, 1981.\n[16] D. Gross, Y.-K. Liu, S. T. Flammia, S. Becker, and J. Eisert. Quantum state tomography via compressed sensing. Preprint\navailable at http//arxiv.org/abs/0909.3304, 2009.\n[17] T. Hagerup and C. R\u00a8ub. A guided tour of chernoff bounds. Information Processing Letters, 33:305\u2013308, 1990.\n[18] R. H. Keshavan, A. Montanari, and S. Oh.\nMatrix completion from a few entries.\n2009.\nPreprint available at\nhttp://arxiv.org/abs/0901.3150.\n[19] R. H. Keshavan,\nA. Montanari,\nand S. Oh.\nMatrix completion from noisy entries.\nPreprint available at\nhttp://arxiv.org/abs/0906.2027, 2009.\n[20] N. Linial, E. London, and Y. Rabinovich. The geometry of graphs and some of its algorithmic applications. Combinatorica,\n15:215\u2013245, 1995.\n[21] M. Mesbahi and G. P. Papavassilopoulos.\nOn the rank minimization problem over a positive semide\ufb01nite linear matrix\ninequality. IEEE Transactions on Automatic Control, 42(2):239\u2013243, 1997.\n[22] G. Obozinski,\nB. Taskar,\nand M. Jordan.\nJoint covariate selection and joint subspace selection for mul-\ntiple\nclassi\ufb01cation\nproblems.\nTo\nAppear\nin\nJournal\nof\nStatistics\nand\nComputing.\nPreprint\navailble\nat\nhttp://www.seas.upenn.edu/\u02dctaskar/, 2009.\n[23] D.\nPanchenko.\nMIT\n18.465:\nStatistical\nLearning\nTheory.\nMIT\nOpen\nCourseware\nhttp://ocw.mit.edu/OcwWeb/Mathematics/18-465Spring-2007/CourseHome/, 2007.\n[24] B. Recht, M. Fazel, and P. Parrilo. Guaranteed minimum rank solutions of matrix equations via nuclear norm minimization.\nSIAM Review. To appear. Preprint Available at http://pages.cs.wisc.edu/\u02dcbrecht/publications.html.\n[25] J. D. M. Rennie and N. Srebro. Fast maximum margin matrix factorization for collaborative prediction. In Proceedings of the\nInternational Conference of Machine Learning, 2005.\n[26] N. Srebro. Learning with Matrix Factorizations. PhD thesis, Massachusetts Institute of Technology, 2004.\n[27] C. J. Thompson. Inequality with applications in statistical mechanics. Journal of Mathematical Physics, 6(11):1812\u20131823,\n1965.\n[28] K. Q. Weinberger and L. K. Saul. Unsupervised learning of image manifolds by semide\ufb01nite programming. International\nJournal of Computer Vision, 70(1):77\u201390, 2006.\nA\nOperator Chernoff Bounds\nIn this section, I present a proof of 3.2, and also provide new proofs of some probability bounds from quantum\ninformation theory. To review, a symmetric matrix A is positive semide\ufb01nite if all of its eigenvalues are nonnegative.\nIf A and B are positive semide\ufb01nite matrices, A \u2aafB means B \u2212A is positive semide\ufb01nite. For square matrices A,\nthe matrix exponential will be denoted exp(A) and is given by the power series\nexp(A) =\n\u221e\nX\nk=0\nAk\nk!\nThe following theorem is a generalization of Markov\u2019s inequality originally proven in [1]. My proof closely\nfollows the standard proof of the traditional Markov inequality, and does not rely on discrete summations.\nTheorem A.1 (Operator Markov Inequality [1]) Let X be a random positive semide\ufb01nite matrix and A a \ufb01xed\npositive de\ufb01nite matrix. Then\nP [X \u0338\u2aafA] \u2264Tr(E[X]A\u22121)\n11\nProof Note that if X \u0338\u2aafA, then A\u22121/2XA\u22121/2 \u0338\u2aafI, and hence \u2225A\u22121/2XA\u22121/2\u2225> 1. Let IX\u0338\u2aafA denote the\nindicator of the event X \u0338\u2aafA. Then IX\u0338\u2aafA \u2264Tr(A\u22121/2XA\u22121/2) as the right hand side is always nonnegative, and,\nif the left hand side equals 1, the trace of the right hand side must exceed the norm of the right hand side which is\ngreater than 1. Thus we have\nP[X \u0338\u2aafA] = E[IX\u0338\u2aafA] \u2264E[Tr(A\u22121/2XA\u22121/2)] = Tr(E[X]A\u22121) .\nwhere the last equality follows from the linearity and cyclic properties of the trace.\nNext I will derive a noncommutative version of the Chernoff bound. This was also proven in [1] for i.i.d. matrices.\nThe version stated here is more general in that the random matrices need not be identically distributed, but the proof\nis essentially the same.\nTheorem A.2 (Noncommutative Chernoff Bound) Let X1, . . . , Xn be independent symmetric random matrices in\nRd\u00d7d. Let A be an arbitrary symmetric matrix. Then for any invertible d \u00d7 d matrix T\nP\n\" n\nX\nk=1\nXk \u0338\u2aafnA\n#\n\u2264d\nn\nY\nk=1\n\u2225E[exp(T XkT \u2217\u2212T AT \u2217)]\u2225\nProof The proof relies on an estimate from statistical physics which is stated here without proof.\nLemma A.3 (Golden-Thompson inequality [14,27]) For any symmetric matrices A and B,\nTr(exp(A + B)) \u2264Tr((exp A)(exp B)) .\nMuch like the proof of the standard Chernoff bound, the theorem now follows from a long chain of inequalities.\nP\n\" n\nX\nk=1\nXk \u0338\u2aafnA\n#\n= P\n\" n\nX\nk=1\n(Xk \u2212A) \u0338\u2aaf0\n#\n= P\n\" n\nX\nk=1\nT (Xk \u2212A)T \u2217\u0338\u2aaf0\n#\n= P\n\"\nexp\n n\nX\nk=1\nT (Xk \u2212A)T \u2217\n!\n\u0338\u2aafId\n#\n\u2264Tr\n \nE\n\"\nexp\n n\nX\nk=1\nT (Xk \u2212A)T \u2217\n!#!\n= E\n\"\nTr\n \nexp\n n\nX\nk=1\nT (Xk \u2212A)T \u2217\n!!#\n\u2264E\n\"\nTr\n \nexp\n n\u22121\nX\nk=1\nT (Xk \u2212A)T \u2217\n!\nexp (T (Xn \u2212A)T \u2217)\n!#\n\u2264E1,...,n\u22121\n\"\nTr\n \nexp\n n\u22121\nX\nk=1\nT (Xk \u2212A)T \u2217\n!\nE[exp (T (Xn \u2212A)T \u2217)]\n!#\n\u2264\u2225E[exp (T (Xn \u2212A)T \u2217)]\u2225E1,...,n\u22121\n\"\nTr\n \nexp\n n\u22121\nX\nk=1\nT (Xk \u2212A)T \u2217\n!!#\n\u2264\nn\nY\nk=2\n\u2225E[exp (T (Xk \u2212A)T \u2217)]\u2225E [Tr (exp (T (X1 \u2212A)T \u2217))]\n\u2264d\nn\nY\nk=1\n\u2225E[exp (T (Xk \u2212A)T \u2217)]\u2225\n12\nHere, the \ufb01rst three lines follow from standard properties of the semide\ufb01nite ordering. The fourth line invokes the\nOperator Markov Inequality. The sixth line follows from the Golden-Thompson inequality. The seventh line follows\nfrom independence of the Xk. The eighth line follows because for positive de\ufb01nite matrices Tr(AB) \u2264Tr(A)\u2225B\u2225.\nThis is just another statement of the duality between the nuclear and operator norms. The ninth line iteratively repeats\nthe previous two steps. The \ufb01nal line follows because for a positive de\ufb01nite matrix A, Tr(A) is the sum of the\neigenvalues of A, and all of the eigenvalues are at most \u2225A\u2225.\nLet us now turn to proving the Noncommutative Bernstein Inequality presented in Section 3. The authors in [16]\nproposed a similar inequality for symmetric i.i.d. random matrices with a slightly worse constant. The proof here is\nmore general and follows the standard derivation of Bernstein\u2019s inequality.\nProof [of Theorem 3.2] Set\nYk =\n\u0014\n0\nXk\nX\u2217\nk\n0\n\u0015\nThen Yk are symmetric random variables, and for all k\n\u2225E[Y 2\nk ]\u2225=\n\r\r\r\rE\n\u0014\u0014\nXkX\u2217\nk\n0\n0\nX\u2217\nkXk\n\u0015\u0015\r\r\r\r = max{\u2225E[XkX\u2217\nk]\u2225, \u2225E[X\u2217\nkXk]\u2225} = \u03c12\nk .\nMoreover, the maximum singular value of PL\nk=1 Xk is equal to the maximum eigenvalue of PL\nk=1 Yk. By Theo-\nrem A.2, we have for all \u03bb > 0\nP\n\"\r\r\r\r\r\nL\nX\nk=1\nXk\n\r\r\r\r\r > Lt\n#\n= P\n\" L\nX\nk=1\nYk \u0338\u2aafLtI\n#\n\u2264(d1 + d2) exp(\u2212L\u03bbt)\nL\nY\nk=1\n\u2225E[exp(\u03bbYk)]\u2225.\nFor each k, let Yk = Uk\u039bkU\u2217\nk be an eigenvalue decomposition, where \u039bk is the diagonal matrix of the eigenvalues\nof Yk. In turn, it follows that for s > 0\n\u2212M sY 2\nk \u2aaf\u2212UkM s\u039b2\nkU\u2217\nk \u2aafUk\u039b2+s\nk\nU\u2217\nk = Y 2+s\nk\n\u2aafUkM s\u039b2\nkU\u2217\nk \u2aafM sY 2\nk ,\nwhich then implies\n\u2225E[Y s+2\nk\n]\u2225\u2264M s\u2225E[Y 2\nk ]\u2225.\n(A.1)\nFor \ufb01xed k, we have\n\u2225E[exp(\u03bbYk)]\u2225\u2264\u2225I\u2225+\n\u221e\nX\nj=2\n\u03bbj\nj! \u2225E[Y j\nk ]\u2225\n\u22641 +\n\u221e\nX\nj=2\n\u03bbj\nj! \u2225E[Y 2\nk ]\u2225M j\u22122\n= 1 + \u03c12\nk\nM 2\n\u221e\nX\nj=2\n\u03bbj\nj! M j = 1 + \u03c12\nk\nM 2 (exp(\u03bbM) \u22121 \u2212\u03bbM)\n\u2264exp\n\u0012 \u03c12\nk\nM 2 (exp(\u03bbM) \u22121 \u2212\u03bbM)\n\u0013\n.\nThe \ufb01rst inequality follows from the triangle inequality and the fact that E[Yk] = 0, the second inequality follows\nfrom (A.1), and the \ufb01nal inequality follows from the fact that 1 + x \u2264exp(x) for all x. Putting this together gives\nP\n\"\r\r\r\r\r\nL\nX\nk=1\nXk\n\r\r\r\r\r > Lt\n#\n\u2264(d1 + d2) exp\n \n\u2212\u03bbLt +\nPL\nk=1 \u03c12\nk\nM 2\n(exp(\u03bbM) \u22121 \u2212\u03bbM)\n!\n.\nThis \ufb01nal expression is now just a real number, and only has to be minimized as a function of \u03bb. The theorem now\nfollows by algebraic manipulation: the right hand side is minimized by setting \u03bb =\n1\nM log(1 +\ntLM\nPL\nk=1 \u03c12\nk ), then basic\napproximations can be employed to complete the argument (see, for example [23], lectures 4 and 5).\n13\n",
        "sentence": "",
        "context": "Acknowledgments\nB.R. would like to thank Aram Harrow for introducing him to the operator Chernoff bound and many helpful clarifying\nconversations, Silvia Gandy for pointing out several typos in the original version of this manuscript, and Rob Nowak,\nAli Rahimi, and Stephen Wright for many fruitful discussions about this paper.\nReferences\n[1] R. Ahlswede and A. Winter. Strong converse for identi\ufb01cation via quantum channels. IEEE Transactions on Information\nTheory, 48(3):569\u2013579, 2002.\nMachinery, 28(2):289\u2013304, 1981.\n[16] D. Gross, Y.-K. Liu, S. T. Flammia, S. Becker, and J. Eisert. Quantum state tomography via compressed sensing. Preprint\navailable at http//arxiv.org/abs/0909.3304, 2009."
    },
    {
        "title": "Fast maximum margin matrix factorization for collaborative prediction",
        "author": [
            "Jasson DM Rennie",
            "Nathan Srebro"
        ],
        "venue": "In Proceedings of the 22nd international conference on Machine learning,",
        "citeRegEx": "Rennie and Srebro.,? \\Q2005\\E",
        "shortCiteRegEx": "Rennie and Srebro.",
        "year": 2005,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "Guaranteed matrix completion via nonconvex factorization",
        "author": [
            "Ruoyu Sun",
            "Zhi-Quan Luo"
        ],
        "venue": "In Foundations of Computer Science (FOCS),",
        "citeRegEx": "Sun and Luo.,? \\Q2015\\E",
        "shortCiteRegEx": "Sun and Luo.",
        "year": 2015,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "When are nonconvex problems not scary",
        "author": [
            "Ju Sun",
            "Qing Qu",
            "John Wright"
        ],
        "venue": "arXiv preprint arXiv:1510.06096,",
        "citeRegEx": "Sun et al\\.,? \\Q2015\\E",
        "shortCiteRegEx": "Sun et al\\.",
        "year": 2015,
        "abstract": "",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "A nonconvex optimization framework for low rank matrix estimation",
        "author": [
            "Tuo Zhao",
            "Zhaoran Wang",
            "Han Liu"
        ],
        "venue": "In Advances in Neural Information Processing Systems,",
        "citeRegEx": "Zhao et al\\.,? \\Q2015\\E",
        "shortCiteRegEx": "Zhao et al\\.",
        "year": 2015,
        "abstract": "",
        "full_text": "",
        "sentence": "",
        "context": null
    }
]