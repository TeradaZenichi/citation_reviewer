[
    {
        "title": "TensorFlow: Large-scale machine learning",
        "author": [
            "Fernanda",
            "Vinyals",
            "Oriol",
            "Warden",
            "Pete",
            "Wattenberg",
            "Martin",
            "Wicke",
            "Yu",
            "Yuan",
            "Zheng",
            "Xiaoqiang"
        ],
        "venue": "on heterogeneous systems,",
        "citeRegEx": "Fernanda et al\\.,? \\Q2015\\E",
        "shortCiteRegEx": "Fernanda et al\\.",
        "year": 2015,
        "abstract": "",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "Deep speech 2: End-to-end speech recognition in english and mandarin",
        "author": [
            "Amodei",
            "Dario",
            "Anubhai",
            "Rishita",
            "Battenberg",
            "Eric",
            "Case",
            "Carl",
            "Casper",
            "Jared",
            "Catanzaro",
            "Bryan",
            "Chen",
            "Jingdong",
            "Chrzanowski",
            "Mike",
            "Coates",
            "Adam",
            "Diamos",
            "Greg"
        ],
        "venue": "arXiv preprint arXiv:1512.02595,",
        "citeRegEx": "Amodei et al\\.,? \\Q2015\\E",
        "shortCiteRegEx": "Amodei et al\\.",
        "year": 2015,
        "abstract": "",
        "full_text": "",
        "sentence": " We adapt the convolutional recurrent neural network architecture from a state-of-the-art speech recognition system (Amodei et al., 2015) for phoneme boundary detection.",
        "context": null
    },
    {
        "title": "Praat, a system for doing phonetics by computer",
        "author": [
            "Boersma",
            "Paulus Petrus Gerardus"
        ],
        "venue": "Glot international,",
        "citeRegEx": "Boersma and Gerardus,? \\Q2002\\E",
        "shortCiteRegEx": "Boersma and Gerardus",
        "year": 2002,
        "abstract": "",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "Empirical evaluation of gated recurrent neural networks on sequence modeling",
        "author": [
            "Chung",
            "Junyoung",
            "Gulcehre",
            "Caglar",
            "Cho",
            "KyungHyun",
            "Bengio",
            "Yoshua"
        ],
        "venue": "arXiv preprint arXiv:1412.3555,",
        "citeRegEx": "Chung et al\\.,? \\Q2014\\E",
        "shortCiteRegEx": "Chung et al\\.",
        "year": 2014,
        "abstract": "In this paper we compare different types of recurrent units in recurrent\nneural networks (RNNs). Especially, we focus on more sophisticated units that\nimplement a gating mechanism, such as a long short-term memory (LSTM) unit and\na recently proposed gated recurrent unit (GRU). We evaluate these recurrent\nunits on the tasks of polyphonic music modeling and speech signal modeling. Our\nexperiments revealed that these advanced recurrent units are indeed better than\nmore traditional recurrent units such as tanh units. Also, we found GRU to be\ncomparable to LSTM.",
        "full_text": "Empirical Evaluation of\nGated Recurrent Neural Networks\non Sequence Modeling\nJunyoung Chung\nCaglar Gulcehre\nKyungHyun Cho\nUniversit\u00b4e de Montr\u00b4eal\nYoshua Bengio\nUniversit\u00b4e de Montr\u00b4eal\nCIFAR Senior Fellow\nAbstract\nIn this paper we compare different types of recurrent units in recurrent neural net-\nworks (RNNs). Especially, we focus on more sophisticated units that implement\na gating mechanism, such as a long short-term memory (LSTM) unit and a re-\ncently proposed gated recurrent unit (GRU). We evaluate these recurrent units on\nthe tasks of polyphonic music modeling and speech signal modeling. Our exper-\niments revealed that these advanced recurrent units are indeed better than more\ntraditional recurrent units such as tanh units. Also, we found GRU to be compa-\nrable to LSTM.\n1\nIntroduction\nRecurrent neural networks have recently shown promising results in many machine learning tasks,\nespecially when input and/or output are of variable length [see, e.g., Graves, 2012]. More recently,\nSutskever et al. [2014] and Bahdanau et al. [2014] reported that recurrent neural networks are able to\nperform as well as the existing, well-developed systems on a challenging task of machine translation.\nOne interesting observation, we make from these recent successes is that almost none of these suc-\ncesses were achieved with a vanilla recurrent neural network. Rather, it was a recurrent neural net-\nwork with sophisticated recurrent hidden units, such as long short-term memory units [Hochreiter\nand Schmidhuber, 1997], that was used in those successful applications.\nAmong those sophisticated recurrent units, in this paper, we are interested in evaluating two closely\nrelated variants. One is a long short-term memory (LSTM) unit, and the other is a gated recurrent\nunit (GRU) proposed more recently by Cho et al. [2014]. It is well established in the \ufb01eld that the\nLSTM unit works well on sequence-based tasks with long-term dependencies, but the latter has only\nrecently been introduced and used in the context of machine translation.\nIn this paper, we evaluate these two units and a more traditional tanh unit on the task of sequence\nmodeling. We consider three polyphonic music datasets [see, e.g., Boulanger-Lewandowski et al.,\n2012] as well as two internal datasets provided by Ubisoft in which each sample is a raw speech\nrepresentation.\nBased on our experiments, we concluded that by using \ufb01xed number of parameters for all models\non some datasets GRU, can outperform LSTM units both in terms of convergence in CPU time and\nin terms of parameter updates and generalization.\n2\nBackground: Recurrent Neural Network\nA recurrent neural network (RNN) is an extension of a conventional feedforward neural network,\nwhich is able to handle a variable-length sequence input. The RNN handles the variable-length\n1\narXiv:1412.3555v1  [cs.NE]  11 Dec 2014\nsequence by having a recurrent hidden state whose activation at each time is dependent on that of\nthe previous time.\nMore formally, given a sequence x = (x1, x2, \u00b7 \u00b7 \u00b7 , xT), the RNN updates its recurrent hidden state\nht by\nht =\n\u001a0,\nt = 0\n\u03c6 (ht\u22121, xt) ,\notherwise\n(1)\nwhere \u03c6 is a nonlinear function such as composition of a logistic sigmoid with an af\ufb01ne transforma-\ntion. Optionally, the RNN may have an output y = (y1, y2, . . . , yT) which may again be of variable\nlength.\nTraditionally, the update of the recurrent hidden state in Eq. (1) is implemented as\nht = g (Wxt + Uht\u22121) ,\n(2)\nwhere g is a smooth, bounded function such as a logistic sigmoid function or a hyperbolic tangent\nfunction.\nA generative RNN outputs a probability distribution over the next element of the sequence, given\nits current state ht, and this generative model can capture a distribution over sequences of vari-\nable length by using a special output symbol to represent the end of the sequence. The sequence\nprobability can be decomposed into\np(x1, . . . , xT) = p(x1)p(x2 | x1)p(x3 | x1, x2) \u00b7 \u00b7 \u00b7 p(xT | x1, . . . , xT \u22121),\n(3)\nwhere the last element is a special end-of-sequence value. We model each conditional probability\ndistribution with\np(xt | x1, . . . , xt\u22121) =g(ht),\nwhere ht is from Eq. (1). Such generative RNNs are the subject of this paper.\nUnfortunately, it has been observed by, e.g., Bengio et al. [1994] that it is dif\ufb01cult to train RNNs\nto capture long-term dependencies because the gradients tend to either vanish (most of the time) or\nexplode (rarely, but with severe effects). This makes gradient-based optimization method struggle,\nnot just because of the variations in gradient magnitudes but because of the effect of long-term\ndependencies is hidden (being exponentially smaller with respect to sequence length) by the effect\nof short-term dependencies. There have been two dominant approaches by which many researchers\nhave tried to reduce the negative impacts of this issue. One such approach is to devise a better\nlearning algorithm than a simple stochastic gradient descent [see, e.g., Bengio et al., 2013, Pascanu\net al., 2013, Martens and Sutskever, 2011], for example using the very simple clipped gradient, by\nwhich the norm of the gradient vector is clipped, or using second-order methods which may be less\nsensitive to the issue if the second derivatives follow the same growth pattern as the \ufb01rst derivatives\n(which is not guaranteed to be the case).\nThe other approach, in which we are more interested in this paper, is to design a more sophisticated\nactivation function than a usual activation function, consisting of af\ufb01ne transformation followed\nby a simple element-wise nonlinearity by using gating units. The earliest attempt in this direction\nresulted in an activation function, or a recurrent unit, called a long short-term memory (LSTM)\nunit [Hochreiter and Schmidhuber, 1997]. More recently, another type of recurrent unit, to which\nwe refer as a gated recurrent unit (GRU), was proposed by Cho et al. [2014]. RNNs employing either\nof these recurrent units have been shown to perform well in tasks that require capturing long-term\ndependencies. Those tasks include, but are not limited to, speech recognition [see, e.g., Graves et al.,\n2013] and machine translation [see, e.g., Sutskever et al., 2014, Bahdanau et al., 2014].\n2\nf\nc\nc~\n+\n+\no\ni\nIN\nOUT\nz\nr\nh\nh~\nIN\nOUT\n(a) Long Short-Term Memory\n(b) Gated Recurrent Unit\nFigure 1: Illustration of (a) LSTM and (b) gated recurrent units. (a) i, f and o are the input, forget\nand output gates, respectively. c and \u02dcc denote the memory cell and the new memory cell content. (b)\nr and z are the reset and update gates, and h and \u02dch are the activation and the candidate activation.\n3\nGated Recurrent Neural Networks\nIn this paper, we are interested in evaluating the performance of those recently proposed recurrent\nunits (LSTM unit and GRU) on sequence modeling. Before the empirical evaluation, we \ufb01rst de-\nscribe each of those recurrent units in this section.\n3.1\nLong Short-Term Memory Unit\nThe Long Short-Term Memory (LSTM) unit was initially proposed by Hochreiter and Schmidhuber\n[1997]. Since then, a number of minor modi\ufb01cations to the original LSTM unit have been made.\nWe follow the implementation of LSTM as used in Graves [2013].\nUnlike to the recurrent unit which simply computes a weighted sum of the input signal and applies\na nonlinear function, each j-th LSTM unit maintains a memory cj\nt at time t. The output hj\nt, or the\nactivation, of the LSTM unit is then\nhj\nt = oj\nt tanh\n\u0010\ncj\nt\n\u0011\n,\nwhere oj\nt is an output gate that modulates the amount of memory content exposure. The output gate\nis computed by\noj\nt = \u03c3 (Woxt + Uoht\u22121 + Voct)j ,\nwhere \u03c3 is a logistic sigmoid function. Vo is a diagonal matrix.\nThe memory cell cj\nt is updated by partially forgetting the existing memory and adding a new memory\ncontent \u02dccj\nt :\ncj\nt = f j\nt cj\nt\u22121 + ij\nt\u02dccj\nt,\n(4)\nwhere the new memory content is\n\u02dccj\nt = tanh (Wcxt + Ucht\u22121)j .\nThe extent to which the existing memory is forgotten is modulated by a forget gate f j\nt , and the\ndegree to which the new memory content is added to the memory cell is modulated by an input gate\nij\nt. Gates are computed by\nf j\nt =\u03c3 (Wfxt + Ufht\u22121 + Vfct\u22121)j ,\nij\nt =\u03c3 (Wixt + Uiht\u22121 + Vict\u22121)j .\nNote that Vf and Vi are diagonal matrices.\n3\nUnlike to the traditional recurrent unit which overwrites its content at each time-step (see Eq. (2)),\nan LSTM unit is able to decide whether to keep the existing memory via the introduced gates.\nIntuitively, if the LSTM unit detects an important feature from an input sequence at early stage, it\neasily carries this information (the existence of the feature) over a long distance, hence, capturing\npotential long-distance dependencies.\nSee Fig. 1 (a) for the graphical illustration.\n3.2\nGated Recurrent Unit\nA gated recurrent unit (GRU) was proposed by Cho et al. [2014] to make each recurrent unit to\nadaptively capture dependencies of different time scales. Similarly to the LSTM unit, the GRU has\ngating units that modulate the \ufb02ow of information inside the unit, however, without having a separate\nmemory cells.\nThe activation hj\nt of the GRU at time t is a linear interpolation between the previous activation hj\nt\u22121\nand the candidate activation \u02dchj\nt:\nhj\nt = (1 \u2212zj\nt )hj\nt\u22121 + zj\nt \u02dchj\nt,\n(5)\nwhere an update gate zj\nt decides how much the unit updates its activation, or content. The update\ngate is computed by\nzj\nt = \u03c3 (Wzxt + Uzht\u22121)j .\nThis procedure of taking a linear sum between the existing state and the newly computed state is\nsimilar to the LSTM unit. The GRU, however, does not have any mechanism to control the degree\nto which its state is exposed, but exposes the whole state each time.\nThe candidate activation \u02dchj\nt is computed similarly to that of the traditional recurrent unit (see Eq. (2))\nand as in [Bahdanau et al., 2014],\n\u02dchj\nt = tanh (Wxt + U (rt \u2299ht\u22121))j ,\nwhere rt is a set of reset gates and \u2299is an element-wise multiplication. 1 When off (rj\nt close to 0),\nthe reset gate effectively makes the unit act as if it is reading the \ufb01rst symbol of an input sequence,\nallowing it to forget the previously computed state.\nThe reset gate rj\nt is computed similarly to the update gate:\nrj\nt = \u03c3 (Wrxt + Urht\u22121)j .\nSee Fig. 1 (b) for the graphical illustration of the GRU.\n3.3\nDiscussion\nIt is easy to notice similarities between the LSTM unit and the GRU from Fig. 1.\nThe most prominent feature shared between these units is the additive component of their update\nfrom t to t + 1, which is lacking in the traditional recurrent unit. The traditional recurrent unit\nalways replaces the activation, or the content of a unit with a new value computed from the current\ninput and the previous hidden state. On the other hand, both LSTM unit and GRU keep the existing\ncontent and add the new content on top of it (see Eqs. (4) and (5)).\n1 Note that we use the reset gate in a slightly different way from the original GRU proposed in Cho et al.\n[2014]. Originally, the candidate activation was computed by\n\u02dchj\nt = tanh (Wxt + rt \u2299(Uht\u22121))j ,\nwhere rj\nt is a reset gate. We found in our preliminary experiments that both of these formulations performed\nas well as each other.\n4\nThis additive nature has two advantages. First, it is easy for each unit to remember the existence of\na speci\ufb01c feature in the input stream for a long series of steps. Any important feature, decided by\neither the forget gate of the LSTM unit or the update gate of the GRU, will not be overwritten but\nbe maintained as it is.\nSecond, and perhaps more importantly, this addition effectively creates shortcut paths that bypass\nmultiple temporal steps. These shortcuts allow the error to be back-propagated easily without too\nquickly vanishing (if the gating unit is nearly saturated at 1) as a result of passing through multiple,\nbounded nonlinearities, thus reducing the dif\ufb01culty due to vanishing gradients [Hochreiter, 1991,\nBengio et al., 1994].\nThese two units however have a number of differences as well. One feature of the LSTM unit that\nis missing from the GRU is the controlled exposure of the memory content. In the LSTM unit, the\namount of the memory content that is seen, or used by other units in the network is controlled by the\noutput gate. On the other hand the GRU exposes its full content without any control.\nAnother difference is in the location of the input gate, or the corresponding reset gate. The LSTM\nunit computes the new memory content without any separate control of the amount of information\n\ufb02owing from the previous time step. Rather, the LSTM unit controls the amount of the new memory\ncontent being added to the memory cell independently from the forget gate. On the other hand, the\nGRU controls the information \ufb02ow from the previous activation when computing the new, candidate\nactivation, but does not independently control the amount of the candidate activation being added\n(the control is tied via the update gate).\nFrom these similarities and differences alone, it is dif\ufb01cult to conclude which types of gating units\nwould perform better in general. Although Bahdanau et al. [2014] reported that these two units per-\nformed comparably to each other according to their preliminary experiments on machine translation,\nit is unclear whether this applies as well to tasks other than machine translation. This motivates us\nto conduct more thorough empirical comparison between the LSTM unit and the GRU in this paper.\n4\nExperiments Setting\n4.1\nTasks and Datasets\nWe compare the LSTM unit, GRU and tanh unit in the task of sequence modeling. Sequence\nmodeling aims at learning a probability distribution over sequences, as in Eq. (3), by maximizing\nthe log-likelihood of a model given a set of training sequences:\nmax\n\u03b8\n1\nN\nN\nX\nn=1\nTn\nX\nt=1\nlog p\n\u0000xn\nt | xn\n1, . . . , xn\nt\u22121; \u03b8\n\u0001\n,\nwhere \u03b8 is a set of model parameters. More speci\ufb01cally, we evaluate these units in the tasks of\npolyphonic music modeling and speech signal modeling.\nFor the polyphonic music modeling, we use three polyphonic music datasets from [Boulanger-\nLewandowski et al., 2012]: Nottingham, JSB Chorales, MuseData and Piano-midi. These datasets\ncontain sequences of which each symbol is respectively a 93-, 96-, 105-, and 108-dimensional binary\nvector. We use logistic sigmoid function as output units.\nWe use two internal datasets provided by Ubisoft2 for speech signal modeling. Each sequence is an\none-dimensional raw audio signal, and at each time step, we design a recurrent neural network to\nlook at 20 consecutive samples to predict the following 10 consecutive samples. We have used two\ndifferent versions of the dataset: One with sequences of length 500 (Ubisoft A) and the other with\nsequences of length 8, 000 (Ubisoft B). Ubisoft A and Ubisoft B have 7, 230 and 800 sequences\neach. We use mixture of Gaussians with 20 components as output layer. 3\n2 http://www.ubi.com/\n3Our implementation is available at https://github.com/jych/librnn.git\n5\n4.2\nModels\nFor each task, we train three different recurrent neural networks, each having either LSTM units\n(LSTM-RNN, see Sec. 3.1), GRUs (GRU-RNN, see Sec. 3.2) or tanh units (tanh-RNN, see Eq. (2)).\nAs the primary objective of these experiments is to compare all three units fairly, we choose the size\nof each model so that each model has approximately the same number of parameters. We intention-\nally made the models to be small enough in order to avoid over\ufb01tting which can easily distract the\ncomparison. This approach of comparing different types of hidden units in neural networks has been\ndone before, for instance, by Gulcehre et al. [2014]. See Table 1 for the details of the model sizes.\nUnit\n# of Units\n# of Parameters\nPolyphonic music modeling\nLSTM\n36\n\u224819.8 \u00d7 103\nGRU\n46\n\u224820.2 \u00d7 103\ntanh\n100\n\u224820.1 \u00d7 103\nSpeech signal modeling\nLSTM\n195\n\u2248169.1 \u00d7 103\nGRU\n227\n\u2248168.9 \u00d7 103\ntanh\n400\n\u2248168.4 \u00d7 103\nTable 1: The sizes of the models tested in the experiments.\ntanh\nGRU\nLSTM\nMusic Datasets\nNottingham\ntrain\ntest\n3.22\n3.13\n2.79\n3.23\n3.08\n3.20\nJSB Chorales\ntrain\ntest\n8.82\n9.10\n6.94\n8.54\n8.15\n8.67\nMuseData\ntrain\ntest\n5.64\n6.23\n5.06\n5.99\n5.18\n6.23\nPiano-midi\ntrain\ntest\n5.64\n9.03\n4.93\n8.82\n6.49\n9.03\nUbisoft Datasets\nUbisoft dataset A\ntrain\ntest\n6.29\n6.44\n2.31\n3.59\n1.44\n2.70\nUbisoft dataset B\ntrain\ntest\n7.61\n7.62\n0.38\n0.88\n0.80\n1.26\nTable 2: The average negative log-probabilities of the training and test sets.\nWe train each model with RMSProp [see, e.g., Hinton, 2012] and use weight noise with standard\ndeviation \ufb01xed to 0.075 [Graves, 2011]. At every update, we rescale the norm of the gradient to 1,\nif it is larger than 1 [Pascanu et al., 2013] to prevent exploding gradients. We select a learning rate\n(scalar multiplier in RMSProp) to maximize the validation performance, out of 10 randomly chosen\nlog-uniform candidates sampled from U(\u221212, \u22126) [Bergstra and Bengio, 2012]. The validation set\nis used for early-stop training as well.\n5\nResults and Analysis\nTable 2 lists all the results from our experiments. In the case of the polyphonic music datasets, the\nGRU-RNN outperformed all the others (LSTM-RNN and tanh-RNN) on all the datasets except for\nthe Nottingham. However, we can see that on these music datasets, all the three models performed\nclosely to each other.\nOn the other hand, the RNNs with the gating units (GRU-RNN and LSTM-RNN) clearly outper-\nformed the more traditional tanh-RNN on both of the Ubisoft datasets. The LSTM-RNN was best\nwith the Ubisoft A, and with the Ubisoft B, the GRU-RNN performed best.\nIn Figs. 2\u20133, we show the learning curves of the best validation runs. In the case of the music\ndatasets (Fig. 2), we see that the GRU-RNN makes faster progress in terms of both the number of\n6\nupdates and actual CPU time. If we consider the Ubisoft datasets (Fig. 3), it is clear that although the\ncomputational requirement for each update in the tanh-RNN is much smaller than the other models,\nit did not make much progress each update and eventually stopped making any progress at much\nworse level.\nThese results clearly indicate the advantages of the gating units over the more traditional recurrent\nunits. Convergence is often faster, and the \ufb01nal solutions tend to be better. However, our results are\nnot conclusive in comparing the LSTM and the GRU, which suggests that the choice of the type of\ngated recurrent unit may depend heavily on the dataset and corresponding task.\nPer epoch\nWall Clock Time (seconds)\n(a) Nottingham Dataset\n(b) MuseData Dataset\nFigure 2: Learning curves for training and validation sets of different types of units with respect to\n(top) the number of iterations and (bottom) the wall clock time. y-axis corresponds to the negative-\nlog likelihood of the model shown in log-scale.\n6\nConclusion\nIn this paper we empirically evaluated recurrent neural networks (RNN) with three widely used\nrecurrent units; (1) a traditional tanh unit, (2) a long short-term memory (LSTM) unit and (3)\na recently proposed gated recurrent unit (GRU). Our evaluation focused on the task of sequence\nmodeling on a number of datasets including polyphonic music data and raw speech signal data.\nThe evaluation clearly demonstrated the superiority of the gated units; both the LSTM unit and GRU,\nover the traditional tanh unit. This was more evident with the more challenging task of raw speech\nsignal modeling. However, we could not make concrete conclusion on which of the two gating units\nwas better.\n7\nPer epoch\nWall Clock Time (seconds)\n(a) Ubisoft Dataset A\n(b) Ubisoft Dataset B\nFigure 3: Learning curves for training and validation sets of different types of units with respect to\n(top) the number of iterations and (bottom) the wall clock time. x-axis is the number of epochs and\ny-axis corresponds to the negative-log likelihood of the model shown in log-scale.\nWe consider the experiments in this paper as preliminary. In order to understand better how a gated\nunit helps learning and to separate out the contribution of each component, for instance gating units\nin the LSTM unit or the GRU, of the gating units, more thorough experiments will be required in\nthe future.\nAcknowledgments\nThe authors would like to thank Ubisoft for providing the datasets and for the support. The au-\nthors would like to thank the developers of Theano [Bergstra et al., 2010, Bastien et al., 2012] and\nPylearn2 [Goodfellow et al., 2013]. We acknowledge the support of the following agencies for\nresearch funding and computing support: NSERC, Calcul Qu\u00b4ebec, Compute Canada, the Canada\nResearch Chairs and CIFAR.\n8\nReferences\nD. Bahdanau, K. Cho, and Y. Bengio. Neural machine translation by jointly learning to align and\ntranslate. Technical report, arXiv preprint arXiv:1409.0473, 2014.\nF. Bastien, P. Lamblin, R. Pascanu, J. Bergstra, I. J. Goodfellow, A. Bergeron, N. Bouchard, and\nY. Bengio. Theano: new features and speed improvements. Deep Learning and Unsupervised\nFeature Learning NIPS 2012 Workshop, 2012.\nY. Bengio, P. Simard, and P. Frasconi. Learning long-term dependencies with gradient descent is\ndif\ufb01cult. IEEE Transactions on Neural Networks, 5(2):157\u2013166, 1994.\nY. Bengio, N. Boulanger-Lewandowski, and R. Pascanu. Advances in optimizing recurrent net-\nworks. In Proc. ICASSP 38, 2013.\nJ. Bergstra and Y. Bengio. Random search for hyper-parameter optimization. The Journal of Ma-\nchine Learning Research, 13(1):281\u2013305, 2012.\nJ. Bergstra, O. Breuleux, F. Bastien, P. Lamblin, R. Pascanu, G. Desjardins, J. Turian, D. Warde-\nFarley, and Y. Bengio. Theano: a CPU and GPU math expression compiler. In Proceedings of the\nPython for Scienti\ufb01c Computing Conference (SciPy), June 2010. Oral Presentation.\nN. Boulanger-Lewandowski, Y. Bengio, and P. Vincent. Modeling temporal dependencies in high-\ndimensional sequences: Application to polyphonic music generation and transcription. In Pro-\nceedings of the Twenty-nine International Conference on Machine Learning (ICML\u201912). ACM,\n2012. URL http://icml.cc/discuss/2012/590.html.\nK. Cho, B. van Merrienboer, D. Bahdanau, and Y. Bengio. On the properties of neural machine\ntranslation: Encoder-decoder approaches. arXiv preprint arXiv:1409.1259, 2014.\nI. J. Goodfellow, D. Warde-Farley, P. Lamblin, V. Dumoulin, M. Mirza, R. Pascanu, J. Bergstra,\nF. Bastien, and Y. Bengio.\nPylearn2: a machine learning research library.\narXiv preprint\narXiv:1308.4214, 2013.\nA. Graves. Supervised Sequence Labelling with Recurrent Neural Networks. Studies in Computa-\ntional Intelligence. Springer, 2012.\nA. Graves. Practical variational inference for neural networks. In Advances in Neural Information\nProcessing Systems, pages 2348\u20132356, 2011.\nA. Graves. Generating sequences with recurrent neural networks. arXiv preprint arXiv:1308.0850,\n2013.\nA. Graves, A.-r. Mohamed, and G. Hinton. Speech recognition with deep recurrent neural networks.\nIn ICASSP\u20192013, pages 6645\u20136649. IEEE, 2013.\nC. Gulcehre, K. Cho, R. Pascanu, and Y. Bengio. Learned-norm pooling for deep feedforward and\nrecurrent neural networks. In Machine Learning and Knowledge Discovery in Databases, pages\n530\u2013546. Springer, 2014.\nG. Hinton. Neural networks for machine learning. Coursera, video lectures, 2012.\nS. Hochreiter.\nUntersuchungen zu dynamischen neuronalen Netzen. Diploma thesis, Institut f\u00a8ur\nInformatik, Lehrstuhl Prof. Brauer, Technische Universit\u00a8at M\u00a8unchen, 1991.\nURL http://\nwww7.informatik.tu-muenchen.de/\u02dcEhochreit.\nS. Hochreiter and J. Schmidhuber. Long short-term memory. Neural Computation, 9(8):1735\u20131780,\n1997.\nJ. Martens and I. Sutskever. Learning recurrent neural networks with Hessian-free optimization. In\nProc. ICML\u20192011. ACM, 2011.\nR. Pascanu, T. Mikolov, and Y. Bengio. On the dif\ufb01culty of training recurrent neural networks. In\nProceedings of the 30th International Conference on Machine Learning (ICML\u201913). ACM, 2013.\nURL http://icml.cc/2013/.\nI. Sutskever, O. Vinyals, and Q. V. Le. Sequence to sequence learning with neural networks. Tech-\nnical report, arXiv preprint arXiv:1409.3215, 2014.\n9\n",
        "sentence": " However, we use a multi-layer bidirectional encoder with a gated recurrent unit (GRU) nonlinearity and an equally deep unidirectional GRU decoder (Chung et al., 2014).",
        "context": "we refer as a gated recurrent unit (GRU), was proposed by Cho et al. [2014]. RNNs employing either\nof these recurrent units have been shown to perform well in tasks that require capturing long-term\na recently proposed gated recurrent unit (GRU). Our evaluation focused on the task of sequence\nmodeling on a number of datasets including polyphonic music data and raw speech signal data.\na gating mechanism, such as a long short-term memory (LSTM) unit and a re-\ncently proposed gated recurrent unit (GRU). We evaluate these recurrent units on\nthe tasks of polyphonic music modeling and speech signal modeling. Our exper-"
    },
    {
        "title": "Peachpy meets opcodes: direct machine code generation from python",
        "author": [
            "Dukhan",
            "Marat"
        ],
        "venue": "In Proceedings of the 5th Workshop on Python for High-Performance and Scientific Computing,",
        "citeRegEx": "Dukhan and Marat.,? \\Q2015\\E",
        "shortCiteRegEx": "Dukhan and Marat.",
        "year": 2015,
        "abstract": "",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "Adam: A method for stochastic optimization",
        "author": [
            "D. Kingma",
            "J. Ba"
        ],
        "venue": "arXiv preprint arXiv:1412.6980,",
        "citeRegEx": "Kingma and Ba,? \\Q2014\\E",
        "shortCiteRegEx": "Kingma and Ba",
        "year": 2014,
        "abstract": "We introduce Adam, an algorithm for first-order gradient-based optimization\nof stochastic objective functions, based on adaptive estimates of lower-order\nmoments. The method is straightforward to implement, is computationally\nefficient, has little memory requirements, is invariant to diagonal rescaling\nof the gradients, and is well suited for problems that are large in terms of\ndata and/or parameters. The method is also appropriate for non-stationary\nobjectives and problems with very noisy and/or sparse gradients. The\nhyper-parameters have intuitive interpretations and typically require little\ntuning. Some connections to related algorithms, on which Adam was inspired, are\ndiscussed. We also analyze the theoretical convergence properties of the\nalgorithm and provide a regret bound on the convergence rate that is comparable\nto the best known results under the online convex optimization framework.\nEmpirical results demonstrate that Adam works well in practice and compares\nfavorably to other stochastic optimization methods. Finally, we discuss AdaMax,\na variant of Adam based on the infinity norm.",
        "full_text": "Published as a conference paper at ICLR 2015\nADAM: A METHOD FOR STOCHASTIC OPTIMIZATION\nDiederik P. Kingma*\nUniversity of Amsterdam, OpenAI\ndpkingma@openai.com\nJimmy Lei Ba\u2217\nUniversity of Toronto\njimmy@psi.utoronto.ca\nABSTRACT\nWe introduce Adam, an algorithm for \ufb01rst-order gradient-based optimization of\nstochastic objective functions, based on adaptive estimates of lower-order mo-\nments. The method is straightforward to implement, is computationally ef\ufb01cient,\nhas little memory requirements, is invariant to diagonal rescaling of the gradients,\nand is well suited for problems that are large in terms of data and/or parameters.\nThe method is also appropriate for non-stationary objectives and problems with\nvery noisy and/or sparse gradients. The hyper-parameters have intuitive interpre-\ntations and typically require little tuning. Some connections to related algorithms,\non which Adam was inspired, are discussed. We also analyze the theoretical con-\nvergence properties of the algorithm and provide a regret bound on the conver-\ngence rate that is comparable to the best known results under the online convex\noptimization framework. Empirical results demonstrate that Adam works well in\npractice and compares favorably to other stochastic optimization methods. Finally,\nwe discuss AdaMax, a variant of Adam based on the in\ufb01nity norm.\n1\nINTRODUCTION\nStochastic gradient-based optimization is of core practical importance in many \ufb01elds of science and\nengineering. Many problems in these \ufb01elds can be cast as the optimization of some scalar parameter-\nized objective function requiring maximization or minimization with respect to its parameters. If the\nfunction is differentiable w.r.t. its parameters, gradient descent is a relatively ef\ufb01cient optimization\nmethod, since the computation of \ufb01rst-order partial derivatives w.r.t. all the parameters is of the same\ncomputational complexity as just evaluating the function. Often, objective functions are stochastic.\nFor example, many objective functions are composed of a sum of subfunctions evaluated at different\nsubsamples of data; in this case optimization can be made more ef\ufb01cient by taking gradient steps\nw.r.t. individual subfunctions, i.e. stochastic gradient descent (SGD) or ascent. SGD proved itself\nas an ef\ufb01cient and effective optimization method that was central in many machine learning success\nstories, such as recent advances in deep learning (Deng et al., 2013; Krizhevsky et al., 2012; Hinton\n& Salakhutdinov, 2006; Hinton et al., 2012a; Graves et al., 2013). Objectives may also have other\nsources of noise than data subsampling, such as dropout (Hinton et al., 2012b) regularization. For\nall such noisy objectives, ef\ufb01cient stochastic optimization techniques are required. The focus of this\npaper is on the optimization of stochastic objectives with high-dimensional parameters spaces. In\nthese cases, higher-order optimization methods are ill-suited, and discussion in this paper will be\nrestricted to \ufb01rst-order methods.\nWe propose Adam, a method for ef\ufb01cient stochastic optimization that only requires \ufb01rst-order gra-\ndients with little memory requirement. The method computes individual adaptive learning rates for\ndifferent parameters from estimates of \ufb01rst and second moments of the gradients; the name Adam\nis derived from adaptive moment estimation. Our method is designed to combine the advantages\nof two recently popular methods: AdaGrad (Duchi et al., 2011), which works well with sparse gra-\ndients, and RMSProp (Tieleman & Hinton, 2012), which works well in on-line and non-stationary\nsettings; important connections to these and other stochastic optimization methods are clari\ufb01ed in\nsection 5. Some of Adam\u2019s advantages are that the magnitudes of parameter updates are invariant to\nrescaling of the gradient, its stepsizes are approximately bounded by the stepsize hyperparameter,\nit does not require a stationary objective, it works with sparse gradients, and it naturally performs a\nform of step size annealing.\n\u2217Equal contribution. Author ordering determined by coin \ufb02ip over a Google Hangout.\n1\narXiv:1412.6980v9  [cs.LG]  30 Jan 2017\nPublished as a conference paper at ICLR 2015\nAlgorithm 1: Adam, our proposed algorithm for stochastic optimization. See section 2 for details,\nand for a slightly more ef\ufb01cient (but less clear) order of computation. g2\nt indicates the elementwise\nsquare gt \u2299gt. Good default settings for the tested machine learning problems are \u03b1 = 0.001,\n\u03b21 = 0.9, \u03b22 = 0.999 and \u03f5 = 10\u22128. All operations on vectors are element-wise. With \u03b2t\n1 and \u03b2t\n2\nwe denote \u03b21 and \u03b22 to the power t.\nRequire: \u03b1: Stepsize\nRequire: \u03b21, \u03b22 \u2208[0, 1): Exponential decay rates for the moment estimates\nRequire: f(\u03b8): Stochastic objective function with parameters \u03b8\nRequire: \u03b80: Initial parameter vector\nm0 \u21900 (Initialize 1st moment vector)\nv0 \u21900 (Initialize 2nd moment vector)\nt \u21900 (Initialize timestep)\nwhile \u03b8t not converged do\nt \u2190t + 1\ngt \u2190\u2207\u03b8ft(\u03b8t\u22121) (Get gradients w.r.t. stochastic objective at timestep t)\nmt \u2190\u03b21 \u00b7 mt\u22121 + (1 \u2212\u03b21) \u00b7 gt (Update biased \ufb01rst moment estimate)\nvt \u2190\u03b22 \u00b7 vt\u22121 + (1 \u2212\u03b22) \u00b7 g2\nt (Update biased second raw moment estimate)\nbmt \u2190mt/(1 \u2212\u03b2t\n1) (Compute bias-corrected \ufb01rst moment estimate)\nbvt \u2190vt/(1 \u2212\u03b2t\n2) (Compute bias-corrected second raw moment estimate)\n\u03b8t \u2190\u03b8t\u22121 \u2212\u03b1 \u00b7 bmt/(\u221abvt + \u03f5) (Update parameters)\nend while\nreturn \u03b8t (Resulting parameters)\nIn section 2 we describe the algorithm and the properties of its update rule. Section 3 explains\nour initialization bias correction technique, and section 4 provides a theoretical analysis of Adam\u2019s\nconvergence in online convex programming. Empirically, our method consistently outperforms other\nmethods for a variety of models and datasets, as shown in section 6. Overall, we show that Adam is\na versatile algorithm that scales to large-scale high-dimensional machine learning problems.\n2\nALGORITHM\nSee algorithm 1 for pseudo-code of our proposed algorithm Adam. Let f(\u03b8) be a noisy objec-\ntive function: a stochastic scalar function that is differentiable w.r.t. parameters \u03b8. We are in-\nterested in minimizing the expected value of this function, E[f(\u03b8)] w.r.t. its parameters \u03b8. With\nf1(\u03b8), ..., , fT (\u03b8) we denote the realisations of the stochastic function at subsequent timesteps\n1, ..., T. The stochasticity might come from the evaluation at random subsamples (minibatches)\nof datapoints, or arise from inherent function noise. With gt = \u2207\u03b8ft(\u03b8) we denote the gradient, i.e.\nthe vector of partial derivatives of ft, w.r.t \u03b8 evaluated at timestep t.\nThe algorithm updates exponential moving averages of the gradient (mt) and the squared gradient\n(vt) where the hyper-parameters \u03b21, \u03b22 \u2208[0, 1) control the exponential decay rates of these moving\naverages. The moving averages themselves are estimates of the 1st moment (the mean) and the\n2nd raw moment (the uncentered variance) of the gradient. However, these moving averages are\ninitialized as (vectors of) 0\u2019s, leading to moment estimates that are biased towards zero, especially\nduring the initial timesteps, and especially when the decay rates are small (i.e. the \u03b2s are close to 1).\nThe good news is that this initialization bias can be easily counteracted, resulting in bias-corrected\nestimates bmt and bvt. See section 3 for more details.\nNote that the ef\ufb01ciency of algorithm 1 can, at the expense of clarity, be improved upon by changing\nthe order of computation, e.g. by replacing the last three lines in the loop with the following lines:\n\u03b1t = \u03b1 \u00b7\np\n1 \u2212\u03b2t\n2/(1 \u2212\u03b2t\n1) and \u03b8t \u2190\u03b8t\u22121 \u2212\u03b1t \u00b7 mt/(\u221avt + \u02c6\u03f5).\n2.1\nADAM\u2019S UPDATE RULE\nAn important property of Adam\u2019s update rule is its careful choice of stepsizes. Assuming \u03f5 = 0, the\neffective step taken in parameter space at timestep t is \u2206t = \u03b1 \u00b7 bmt/\u221abvt. The effective stepsize has\ntwo upper bounds: |\u2206t| \u2264\u03b1 \u00b7 (1 \u2212\u03b21)/\u221a1 \u2212\u03b22 in the case (1 \u2212\u03b21) > \u221a1 \u2212\u03b22, and |\u2206t| \u2264\u03b1\n2\nPublished as a conference paper at ICLR 2015\notherwise. The \ufb01rst case only happens in the most severe case of sparsity: when a gradient has\nbeen zero at all timesteps except at the current timestep. For less sparse cases, the effective stepsize\nwill be smaller. When (1 \u2212\u03b21) = \u221a1 \u2212\u03b22 we have that | bmt/\u221abvt| < 1 therefore |\u2206t| < \u03b1. In\nmore common scenarios, we will have that bmt/\u221abvt \u2248\u00b11 since |E[g]/\np\nE[g2]| \u22641. The effective\nmagnitude of the steps taken in parameter space at each timestep are approximately bounded by\nthe stepsize setting \u03b1, i.e., |\u2206t| \u2a85\u03b1. This can be understood as establishing a trust region around\nthe current parameter value, beyond which the current gradient estimate does not provide suf\ufb01cient\ninformation. This typically makes it relatively easy to know the right scale of \u03b1 in advance. For\nmany machine learning models, for instance, we often know in advance that good optima are with\nhigh probability within some set region in parameter space; it is not uncommon, for example, to\nhave a prior distribution over the parameters. Since \u03b1 sets (an upper bound of) the magnitude of\nsteps in parameter space, we can often deduce the right order of magnitude of \u03b1 such that optima\ncan be reached from \u03b80 within some number of iterations. With a slight abuse of terminology,\nwe will call the ratio bmt/\u221abvt the signal-to-noise ratio (SNR). With a smaller SNR the effective\nstepsize \u2206t will be closer to zero. This is a desirable property, since a smaller SNR means that\nthere is greater uncertainty about whether the direction of bmt corresponds to the direction of the true\ngradient. For example, the SNR value typically becomes closer to 0 towards an optimum, leading\nto smaller effective steps in parameter space: a form of automatic annealing. The effective stepsize\n\u2206t is also invariant to the scale of the gradients; rescaling the gradients g with factor c will scale bmt\nwith a factor c and bvt with a factor c2, which cancel out: (c \u00b7 bmt)/(\u221a\nc2 \u00b7 bvt) = bmt/\u221abvt.\n3\nINITIALIZATION BIAS CORRECTION\nAs explained in section 2, Adam utilizes initialization bias correction terms. We will here derive\nthe term for the second moment estimate; the derivation for the \ufb01rst moment estimate is completely\nanalogous. Let g be the gradient of the stochastic objective f, and we wish to estimate its second\nraw moment (uncentered variance) using an exponential moving average of the squared gradient,\nwith decay rate \u03b22. Let g1, ..., gT be the gradients at subsequent timesteps, each a draw from an\nunderlying gradient distribution gt \u223cp(gt). Let us initialize the exponential moving average as\nv0 = 0 (a vector of zeros). First note that the update at timestep t of the exponential moving average\nvt = \u03b22 \u00b7 vt\u22121 + (1 \u2212\u03b22) \u00b7 g2\nt (where g2\nt indicates the elementwise square gt \u2299gt) can be written as\na function of the gradients at all previous timesteps:\nvt = (1 \u2212\u03b22)\nt\nX\ni=1\n\u03b2t\u2212i\n2\n\u00b7 g2\ni\n(1)\nWe wish to know how E[vt], the expected value of the exponential moving average at timestep t,\nrelates to the true second moment E[g2\nt ], so we can correct for the discrepancy between the two.\nTaking expectations of the left-hand and right-hand sides of eq. (1):\nE[vt] = E\n\"\n(1 \u2212\u03b22)\nt\nX\ni=1\n\u03b2t\u2212i\n2\n\u00b7 g2\ni\n#\n(2)\n= E[g2\nt ] \u00b7 (1 \u2212\u03b22)\nt\nX\ni=1\n\u03b2t\u2212i\n2\n+ \u03b6\n(3)\n= E[g2\nt ] \u00b7 (1 \u2212\u03b2t\n2) + \u03b6\n(4)\nwhere \u03b6 = 0 if the true second moment E[g2\ni ] is stationary; otherwise \u03b6 can be kept small since\nthe exponential decay rate \u03b21 can (and should) be chosen such that the exponential moving average\nassigns small weights to gradients too far in the past. What is left is the term (1 \u2212\u03b2t\n2) which is\ncaused by initializing the running average with zeros. In algorithm 1 we therefore divide by this\nterm to correct the initialization bias.\nIn case of sparse gradients, for a reliable estimate of the second moment one needs to average over\nmany gradients by chosing a small value of \u03b22; however it is exactly this case of small \u03b22 where a\nlack of initialisation bias correction would lead to initial steps that are much larger.\n3\nPublished as a conference paper at ICLR 2015\n4\nCONVERGENCE ANALYSIS\nWe analyze the convergence of Adam using the online learning framework proposed in (Zinkevich,\n2003). Given an arbitrary, unknown sequence of convex cost functions f1(\u03b8), f2(\u03b8),..., fT (\u03b8). At\neach time t, our goal is to predict the parameter \u03b8t and evaluate it on a previously unknown cost\nfunction ft. Since the nature of the sequence is unknown in advance, we evaluate our algorithm\nusing the regret, that is the sum of all the previous difference between the online prediction ft(\u03b8t)\nand the best \ufb01xed point parameter ft(\u03b8\u2217) from a feasible set X for all the previous steps. Concretely,\nthe regret is de\ufb01ned as:\nR(T) =\nT\nX\nt=1\n[ft(\u03b8t) \u2212ft(\u03b8\u2217)]\n(5)\nwhere \u03b8\u2217= arg min\u03b8\u2208X\nPT\nt=1 ft(\u03b8). We show Adam has O(\n\u221a\nT) regret bound and a proof is given\nin the appendix. Our result is comparable to the best known bound for this general convex online\nlearning problem. We also use some de\ufb01nitions simplify our notation, where gt \u225c\u2207ft(\u03b8t) and gt,i\nas the ith element. We de\ufb01ne g1:t,i \u2208Rt as a vector that contains the ith dimension of the gradients\nover all iterations till t, g1:t,i = [g1,i, g2,i, \u00b7 \u00b7 \u00b7 , gt,i]. Also, we de\ufb01ne \u03b3 \u225c\n\u03b22\n1\n\u221a\u03b22 . Our following\ntheorem holds when the learning rate \u03b1t is decaying at a rate of t\u22121\n2 and \ufb01rst moment running\naverage coef\ufb01cient \u03b21,t decay exponentially with \u03bb, that is typically close to 1, e.g. 1 \u221210\u22128.\nTheorem 4.1. Assume that the function ft has bounded gradients, \u2225\u2207ft(\u03b8)\u22252 \u2264G, \u2225\u2207ft(\u03b8)\u2225\u221e\u2264\nG\u221efor all \u03b8 \u2208Rd and distance between any \u03b8t generated by Adam is bounded, \u2225\u03b8n \u2212\u03b8m\u22252 \u2264D,\n\u2225\u03b8m \u2212\u03b8n\u2225\u221e\u2264D\u221efor any m, n \u2208{1, ..., T}, and \u03b21, \u03b22 \u2208[0, 1) satisfy\n\u03b22\n1\n\u221a\u03b22 < 1. Let \u03b1t =\n\u03b1\n\u221a\nt\nand \u03b21,t = \u03b21\u03bbt\u22121, \u03bb \u2208(0, 1). Adam achieves the following guarantee, for all T \u22651.\nR(T) \u2264\nD2\n2\u03b1(1 \u2212\u03b21)\nd\nX\ni=1\np\nTbvT,i+\n\u03b1(1 + \u03b21)G\u221e\n(1 \u2212\u03b21)\u221a1 \u2212\u03b22(1 \u2212\u03b3)2\nd\nX\ni=1\n\u2225g1:T,i\u22252+\nd\nX\ni=1\nD2\n\u221eG\u221e\n\u221a1 \u2212\u03b22\n2\u03b1(1 \u2212\u03b21)(1 \u2212\u03bb)2\nOur Theorem 4.1 implies when the data features are sparse and bounded gradients, the sum-\nmation term can be much smaller than its upper bound Pd\ni=1 \u2225g1:T,i\u22252\n<< dG\u221e\n\u221a\nT and\nPd\ni=1\np\nTbvT,i << dG\u221e\n\u221a\nT, in particular if the class of function and data features are in the form of\nsection 1.2 in (Duchi et al., 2011). Their results for the expected value E[Pd\ni=1 \u2225g1:T,i\u22252] also apply\nto Adam. In particular, the adaptive method, such as Adam and Adagrad, can achieve O(log d\n\u221a\nT),\nan improvement over O(\n\u221a\ndT) for the non-adaptive method. Decaying \u03b21,t towards zero is impor-\ntant in our theoretical analysis and also matches previous empirical \ufb01ndings, e.g. (Sutskever et al.,\n2013) suggests reducing the momentum coef\ufb01cient in the end of training can improve convergence.\nFinally, we can show the average regret of Adam converges,\nCorollary 4.2. Assume that the function ft has bounded gradients, \u2225\u2207ft(\u03b8)\u22252 \u2264G, \u2225\u2207ft(\u03b8)\u2225\u221e\u2264\nG\u221efor all \u03b8 \u2208Rd and distance between any \u03b8t generated by Adam is bounded, \u2225\u03b8n \u2212\u03b8m\u22252 \u2264D,\n\u2225\u03b8m \u2212\u03b8n\u2225\u221e\u2264D\u221efor any m, n \u2208{1, ..., T}. Adam achieves the following guarantee, for all\nT \u22651.\nR(T)\nT\n= O( 1\n\u221a\nT\n)\nThis result can be obtained by using Theorem 4.1 and Pd\ni=1 \u2225g1:T,i\u22252 \u2264dG\u221e\n\u221a\nT.\nThus,\nlimT \u2192\u221e\nR(T )\nT\n= 0.\n5\nRELATED WORK\nOptimization methods bearing a direct relation to Adam are RMSProp (Tieleman & Hinton, 2012;\nGraves, 2013) and AdaGrad (Duchi et al., 2011); these relationships are discussed below. Other\nstochastic optimization methods include vSGD (Schaul et al., 2012), AdaDelta (Zeiler, 2012) and the\nnatural Newton method from Roux & Fitzgibbon (2010), all setting stepsizes by estimating curvature\n4\nPublished as a conference paper at ICLR 2015\nfrom \ufb01rst-order information. The Sum-of-Functions Optimizer (SFO) (Sohl-Dickstein et al., 2014)\nis a quasi-Newton method based on minibatches, but (unlike Adam) has memory requirements linear\nin the number of minibatch partitions of a dataset, which is often infeasible on memory-constrained\nsystems such as a GPU. Like natural gradient descent (NGD) (Amari, 1998), Adam employs a\npreconditioner that adapts to the geometry of the data, since bvt is an approximation to the diagonal\nof the Fisher information matrix (Pascanu & Bengio, 2013); however, Adam\u2019s preconditioner (like\nAdaGrad\u2019s) is more conservative in its adaption than vanilla NGD by preconditioning with the square\nroot of the inverse of the diagonal Fisher information matrix approximation.\nRMSProp:\nAn optimization method closely related to Adam is RMSProp (Tieleman & Hinton,\n2012). A version with momentum has sometimes been used (Graves, 2013). There are a few impor-\ntant differences between RMSProp with momentum and Adam: RMSProp with momentum gener-\nates its parameter updates using a momentum on the rescaled gradient, whereas Adam updates are\ndirectly estimated using a running average of \ufb01rst and second moment of the gradient. RMSProp\nalso lacks a bias-correction term; this matters most in case of a value of \u03b22 close to 1 (required in\ncase of sparse gradients), since in that case not correcting the bias leads to very large stepsizes and\noften divergence, as we also empirically demonstrate in section 6.4.\nAdaGrad:\nAn algorithm that works well for sparse gradients is AdaGrad (Duchi et al., 2011). Its\nbasic version updates parameters as \u03b8t+1 = \u03b8t \u2212\u03b1 \u00b7 gt/\nqPt\ni=1 g2\nt . Note that if we choose \u03b22 to be\nin\ufb01nitesimally close to 1 from below, then lim\u03b22\u21921 bvt = t\u22121 \u00b7 Pt\ni=1 g2\nt . AdaGrad corresponds to a\nversion of Adam with \u03b21 = 0, in\ufb01nitesimal (1 \u2212\u03b22) and a replacement of \u03b1 by an annealed version\n\u03b1t = \u03b1 \u00b7 t\u22121/2, namely \u03b8t \u2212\u03b1 \u00b7 t\u22121/2 \u00b7 bmt/\np\nlim\u03b22\u21921 bvt = \u03b8t \u2212\u03b1 \u00b7 t\u22121/2 \u00b7 gt/\nq\nt\u22121 \u00b7 Pt\ni=1 g2\nt =\n\u03b8t \u2212\u03b1 \u00b7 gt/\nqPt\ni=1 g2\nt . Note that this direct correspondence between Adam and Adagrad does\nnot hold when removing the bias-correction terms; without bias correction, like in RMSProp, a \u03b22\nin\ufb01nitesimally close to 1 would lead to in\ufb01nitely large bias, and in\ufb01nitely large parameter updates.\n6\nEXPERIMENTS\nTo empirically evaluate the proposed method, we investigated different popular machine learning\nmodels, including logistic regression, multilayer fully connected neural networks and deep convolu-\ntional neural networks. Using large models and datasets, we demonstrate Adam can ef\ufb01ciently solve\npractical deep learning problems.\nWe use the same parameter initialization when comparing different optimization algorithms. The\nhyper-parameters, such as learning rate and momentum, are searched over a dense grid and the\nresults are reported using the best hyper-parameter setting.\n6.1\nEXPERIMENT: LOGISTIC REGRESSION\nWe evaluate our proposed method on L2-regularized multi-class logistic regression using the MNIST\ndataset. Logistic regression has a well-studied convex objective, making it suitable for comparison\nof different optimizers without worrying about local minimum issues. The stepsize \u03b1 in our logistic\nregression experiments is adjusted by 1/\n\u221a\nt decay, namely \u03b1t =\n\u03b1\n\u221a\nt that matches with our theorat-\nical prediction from section 4. The logistic regression classi\ufb01es the class label directly on the 784\ndimension image vectors. We compare Adam to accelerated SGD with Nesterov momentum and\nAdagrad using minibatch size of 128. According to Figure 1, we found that the Adam yields similar\nconvergence as SGD with momentum and both converge faster than Adagrad.\nAs discussed in (Duchi et al., 2011), Adagrad can ef\ufb01ciently deal with sparse features and gradi-\nents as one of its main theoretical results whereas SGD is low at learning rare features. Adam with\n1/\n\u221a\nt decay on its stepsize should theoratically match the performance of Adagrad. We examine the\nsparse feature problem using IMDB movie review dataset from (Maas et al., 2011). We pre-process\nthe IMDB movie reviews into bag-of-words (BoW) feature vectors including the \ufb01rst 10,000 most\nfrequent words. The 10,000 dimension BoW feature vector for each review is highly sparse. As sug-\ngested in (Wang & Manning, 2013), 50% dropout noise can be applied to the BoW features during\n5\nPublished as a conference paper at ICLR 2015\n0\n5\n10\n15\n20\n25\n30\n35\n40\n45\niterations over entire dataset\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\ntraining cost\nMNIST Logistic Regression\nAdaGrad\nSGDNesterov\nAdam\n0\n20\n40\n60\n80\n100\n120\n140\n160\niterations over entire dataset\n0.20\n0.25\n0.30\n0.35\n0.40\n0.45\n0.50\ntraining cost\nIMDB BoW feature Logistic Regression\nAdagrad+dropout\nRMSProp+dropout\nSGDNesterov+dropout\nAdam+dropout\nFigure 1: Logistic regression training negative log likelihood on MNIST images and IMDB movie\nreviews with 10,000 bag-of-words (BoW) feature vectors.\ntraining to prevent over-\ufb01tting. In \ufb01gure 1, Adagrad outperforms SGD with Nesterov momentum\nby a large margin both with and without dropout noise. Adam converges as fast as Adagrad. The\nempirical performance of Adam is consistent with our theoretical \ufb01ndings in sections 2 and 4. Sim-\nilar to Adagrad, Adam can take advantage of sparse features and obtain faster convergence rate than\nnormal SGD with momentum.\n6.2\nEXPERIMENT: MULTI-LAYER NEURAL NETWORKS\nMulti-layer neural network are powerful models with non-convex objective functions. Although\nour convergence analysis does not apply to non-convex problems, we empirically found that Adam\noften outperforms other methods in such cases. In our experiments, we made model choices that are\nconsistent with previous publications in the area; a neural network model with two fully connected\nhidden layers with 1000 hidden units each and ReLU activation are used for this experiment with\nminibatch size of 128.\nFirst, we study different optimizers using the standard deterministic cross-entropy objective func-\ntion with L2 weight decay on the parameters to prevent over-\ufb01tting. The sum-of-functions (SFO)\nmethod (Sohl-Dickstein et al., 2014) is a recently proposed quasi-Newton method that works with\nminibatches of data and has shown good performance on optimization of multi-layer neural net-\nworks. We used their implementation and compared with Adam to train such models. Figure 2\nshows that Adam makes faster progress in terms of both the number of iterations and wall-clock\ntime. Due to the cost of updating curvature information, SFO is 5-10x slower per iteration com-\npared to Adam, and has a memory requirement that is linear in the number minibatches.\nStochastic regularization methods, such as dropout, are an effective way to prevent over-\ufb01tting and\noften used in practice due to their simplicity. SFO assumes deterministic subfunctions, and indeed\nfailed to converge on cost functions with stochastic regularization. We compare the effectiveness of\nAdam to other stochastic \ufb01rst order methods on multi-layer neural networks trained with dropout\nnoise. Figure 2 shows our results; Adam shows better convergence than other methods.\n6.3\nEXPERIMENT: CONVOLUTIONAL NEURAL NETWORKS\nConvolutional neural networks (CNNs) with several layers of convolution, pooling and non-linear\nunits have shown considerable success in computer vision tasks. Unlike most fully connected neural\nnets, weight sharing in CNNs results in vastly different gradients in different layers. A smaller\nlearning rate for the convolution layers is often used in practice when applying SGD. We show the\neffectiveness of Adam in deep CNNs. Our CNN architecture has three alternating stages of 5x5\nconvolution \ufb01lters and 3x3 max pooling with stride of 2 that are followed by a fully connected layer\nof 1000 recti\ufb01ed linear hidden units (ReLU\u2019s). The input image are pre-processed by whitening, and\n6\nPublished as a conference paper at ICLR 2015\n0\n50\n100\n150\n200\niterations over entire dataset\n10\n-2\n10\n-1\ntraining cost\nMNIST Multilayer Neural Network + dropout\nAdaGrad\nRMSProp\nSGDNesterov\nAdaDelta\nAdam\n(a)\n(b)\nFigure 2: Training of multilayer neural networks on MNIST images. (a) Neural networks using\ndropout stochastic regularization. (b) Neural networks with deterministic cost function. We compare\nwith the sum-of-functions (SFO) optimizer (Sohl-Dickstein et al., 2014)\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\niterations over entire dataset\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\ntraining cost\nCIFAR10 ConvNet First 3 Epoches\nAdaGrad\nAdaGrad+dropout\nSGDNesterov\nSGDNesterov+dropout\nAdam\nAdam+dropout\n0\n5\n10\n15\n20\n25\n30\n35\n40\n45\niterations over entire dataset\n10-4\n10-3\n10-2\n10-1\n100\n101\n102\ntraining cost\nCIFAR10 ConvNet\nAdaGrad\nAdaGrad+dropout\nSGDNesterov\nSGDNesterov+dropout\nAdam\nAdam+dropout\nFigure 3: Convolutional neural networks training cost. (left) Training cost for the \ufb01rst three epochs.\n(right) Training cost over 45 epochs. CIFAR-10 with c64-c64-c128-1000 architecture.\ndropout noise is applied to the input layer and fully connected layer. The minibatch size is also set\nto 128 similar to previous experiments.\nInterestingly, although both Adam and Adagrad make rapid progress lowering the cost in the initial\nstage of the training, shown in Figure 3 (left), Adam and SGD eventually converge considerably\nfaster than Adagrad for CNNs shown in Figure 3 (right). We notice the second moment estimate bvt\nvanishes to zeros after a few epochs and is dominated by the \u03f5 in algorithm 1. The second moment\nestimate is therefore a poor approximation to the geometry of the cost function in CNNs comparing\nto fully connected network from Section 6.2. Whereas, reducing the minibatch variance through\nthe \ufb01rst moment is more important in CNNs and contributes to the speed-up. As a result, Adagrad\nconverges much slower than others in this particular experiment. Though Adam shows marginal\nimprovement over SGD with momentum, it adapts learning rate scale for different layers instead of\nhand picking manually as in SGD.\n7\nPublished as a conference paper at ICLR 2015\n\u03b21=0\n\u03b21=0.9\n\u03b22=0.99\n\u03b22=0.999\n\u03b22=0.9999\n\u03b22=0.99\n\u03b22=0.999\n\u03b22=0.9999\n(a) after 10 epochs\n(b) after 100 epochs\nlog10(\u03b1)\nLoss\nFigure 4: Effect of bias-correction terms (red line) versus no bias correction terms (green line)\nafter 10 epochs (left) and 100 epochs (right) on the loss (y-axes) when learning a Variational Auto-\nEncoder (VAE) (Kingma & Welling, 2013), for different settings of stepsize \u03b1 (x-axes) and hyper-\nparameters \u03b21 and \u03b22.\n6.4\nEXPERIMENT: BIAS-CORRECTION TERM\nWe also empirically evaluate the effect of the bias correction terms explained in sections 2 and 3.\nDiscussed in section 5, removal of the bias correction terms results in a version of RMSProp (Tiele-\nman & Hinton, 2012) with momentum. We vary the \u03b21 and \u03b22 when training a variational auto-\nencoder (VAE) with the same architecture as in (Kingma & Welling, 2013) with a single hidden\nlayer with 500 hidden units with softplus nonlinearities and a 50-dimensional spherical Gaussian\nlatent variable. We iterated over a broad range of hyper-parameter choices, i.e. \u03b21 \u2208[0, 0.9] and\n\u03b22 \u2208[0.99, 0.999, 0.9999], and log10(\u03b1) \u2208[\u22125, ..., \u22121]. Values of \u03b22 close to 1, required for robust-\nness to sparse gradients, results in larger initialization bias; therefore we expect the bias correction\nterm is important in such cases of slow decay, preventing an adverse effect on optimization.\nIn Figure 4, values \u03b22 close to 1 indeed lead to instabilities in training when no bias correction term\nwas present, especially at \ufb01rst few epochs of the training. The best results were achieved with small\nvalues of (1\u2212\u03b22) and bias correction; this was more apparent towards the end of optimization when\ngradients tends to become sparser as hidden units specialize to speci\ufb01c patterns. In summary, Adam\nperformed equal or better than RMSProp, regardless of hyper-parameter setting.\n7\nEXTENSIONS\n7.1\nADAMAX\nIn Adam, the update rule for individual weights is to scale their gradients inversely proportional to a\n(scaled) L2 norm of their individual current and past gradients. We can generalize the L2 norm based\nupdate rule to a Lp norm based update rule. Such variants become numerically unstable for large\np. However, in the special case where we let p \u2192\u221e, a surprisingly simple and stable algorithm\nemerges; see algorithm 2. We\u2019ll now derive the algorithm. Let, in case of the Lp norm, the stepsize\nat time t be inversely proportional to v1/p\nt\n, where:\nvt = \u03b2p\n2vt\u22121 + (1 \u2212\u03b2p\n2)|gt|p\n(6)\n= (1 \u2212\u03b2p\n2)\nt\nX\ni=1\n\u03b2p(t\u2212i)\n2\n\u00b7 |gi|p\n(7)\n8\nPublished as a conference paper at ICLR 2015\nAlgorithm 2: AdaMax, a variant of Adam based on the in\ufb01nity norm. See section 7.1 for details.\nGood default settings for the tested machine learning problems are \u03b1 = 0.002, \u03b21 = 0.9 and\n\u03b22 = 0.999. With \u03b2t\n1 we denote \u03b21 to the power t. Here, (\u03b1/(1 \u2212\u03b2t\n1)) is the learning rate with the\nbias-correction term for the \ufb01rst moment. All operations on vectors are element-wise.\nRequire: \u03b1: Stepsize\nRequire: \u03b21, \u03b22 \u2208[0, 1): Exponential decay rates\nRequire: f(\u03b8): Stochastic objective function with parameters \u03b8\nRequire: \u03b80: Initial parameter vector\nm0 \u21900 (Initialize 1st moment vector)\nu0 \u21900 (Initialize the exponentially weighted in\ufb01nity norm)\nt \u21900 (Initialize timestep)\nwhile \u03b8t not converged do\nt \u2190t + 1\ngt \u2190\u2207\u03b8ft(\u03b8t\u22121) (Get gradients w.r.t. stochastic objective at timestep t)\nmt \u2190\u03b21 \u00b7 mt\u22121 + (1 \u2212\u03b21) \u00b7 gt (Update biased \ufb01rst moment estimate)\nut \u2190max(\u03b22 \u00b7 ut\u22121, |gt|) (Update the exponentially weighted in\ufb01nity norm)\n\u03b8t \u2190\u03b8t\u22121 \u2212(\u03b1/(1 \u2212\u03b2t\n1)) \u00b7 mt/ut (Update parameters)\nend while\nreturn \u03b8t (Resulting parameters)\nNote that the decay term is here equivalently parameterised as \u03b2p\n2 instead of \u03b22. Now let p \u2192\u221e,\nand de\ufb01ne ut = limp\u2192\u221e(vt)1/p, then:\nut = lim\np\u2192\u221e(vt)1/p = lim\np\u2192\u221e\n \n(1 \u2212\u03b2p\n2)\nt\nX\ni=1\n\u03b2p(t\u2212i)\n2\n\u00b7 |gi|p\n!1/p\n(8)\n= lim\np\u2192\u221e(1 \u2212\u03b2p\n2)1/p\n \nt\nX\ni=1\n\u03b2p(t\u2212i)\n2\n\u00b7 |gi|p\n!1/p\n(9)\n= lim\np\u2192\u221e\n \nt\nX\ni=1\n\u0010\n\u03b2(t\u2212i)\n2\n\u00b7 |gi|\n\u0011p\n!1/p\n(10)\n= max\n\u0000\u03b2t\u22121\n2\n|g1|, \u03b2t\u22122\n2\n|g2|, . . . , \u03b22|gt\u22121|, |gt|\n\u0001\n(11)\nWhich corresponds to the remarkably simple recursive formula:\nut = max(\u03b22 \u00b7 ut\u22121, |gt|)\n(12)\nwith initial value u0 = 0. Note that, conveniently enough, we don\u2019t need to correct for initialization\nbias in this case. Also note that the magnitude of parameter updates has a simpler bound with\nAdaMax than Adam, namely: |\u2206t| \u2264\u03b1.\n7.2\nTEMPORAL AVERAGING\nSince the last iterate is noisy due to stochastic approximation, better generalization performance is\noften achieved by averaging. Previously in Moulines & Bach (2011), Polyak-Ruppert averaging\n(Polyak & Juditsky, 1992; Ruppert, 1988) has been shown to improve the convergence of standard\nSGD, where \u00af\u03b8t = 1\nt\nPn\nk=1 \u03b8k. Alternatively, an exponential moving average over the parameters can\nbe used, giving higher weight to more recent parameter values. This can be trivially implemented\nby adding one line to the inner loop of algorithms 1 and 2: \u00af\u03b8t \u2190\u03b22 \u00b7 \u00af\u03b8t\u22121 +(1\u2212\u03b22)\u03b8t, with \u00af\u03b80 = 0.\nInitalization bias can again be corrected by the estimator b\u03b8t = \u00af\u03b8t/(1 \u2212\u03b2t\n2).\n8\nCONCLUSION\nWe have introduced a simple and computationally ef\ufb01cient algorithm for gradient-based optimiza-\ntion of stochastic objective functions. Our method is aimed towards machine learning problems with\n9\nPublished as a conference paper at ICLR 2015\nlarge datasets and/or high-dimensional parameter spaces. The method combines the advantages of\ntwo recently popular optimization methods: the ability of AdaGrad to deal with sparse gradients,\nand the ability of RMSProp to deal with non-stationary objectives. The method is straightforward\nto implement and requires little memory. The experiments con\ufb01rm the analysis on the rate of con-\nvergence in convex problems. Overall, we found Adam to be robust and well-suited to a wide range\nof non-convex optimization problems in the \ufb01eld machine learning.\n9\nACKNOWLEDGMENTS\nThis paper would probably not have existed without the support of Google Deepmind. We would\nlike to give special thanks to Ivo Danihelka, and Tom Schaul for coining the name Adam. Thanks to\nKai Fan from Duke University for spotting an error in the original AdaMax derivation. Experiments\nin this work were partly carried out on the Dutch national e-infrastructure with the support of SURF\nFoundation. Diederik Kingma is supported by the Google European Doctorate Fellowship in Deep\nLearning.\nREFERENCES\nAmari, Shun-Ichi. Natural gradient works ef\ufb01ciently in learning. Neural computation, 10(2):251\u2013276, 1998.\nDeng, Li, Li, Jinyu, Huang, Jui-Ting, Yao, Kaisheng, Yu, Dong, Seide, Frank, Seltzer, Michael, Zweig, Geoff,\nHe, Xiaodong, Williams, Jason, et al. Recent advances in deep learning for speech research at microsoft.\nICASSP 2013, 2013.\nDuchi, John, Hazan, Elad, and Singer, Yoram. Adaptive subgradient methods for online learning and stochastic\noptimization. The Journal of Machine Learning Research, 12:2121\u20132159, 2011.\nGraves, Alex. Generating sequences with recurrent neural networks. arXiv preprint arXiv:1308.0850, 2013.\nGraves, Alex, Mohamed, Abdel-rahman, and Hinton, Geoffrey. Speech recognition with deep recurrent neural\nnetworks. In Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on,\npp. 6645\u20136649. IEEE, 2013.\nHinton, G.E. and Salakhutdinov, R.R. Reducing the dimensionality of data with neural networks. Science, 313\n(5786):504\u2013507, 2006.\nHinton, Geoffrey, Deng, Li, Yu, Dong, Dahl, George E, Mohamed, Abdel-rahman, Jaitly, Navdeep, Senior,\nAndrew, Vanhoucke, Vincent, Nguyen, Patrick, Sainath, Tara N, et al. Deep neural networks for acoustic\nmodeling in speech recognition: The shared views of four research groups. Signal Processing Magazine,\nIEEE, 29(6):82\u201397, 2012a.\nHinton, Geoffrey E, Srivastava, Nitish, Krizhevsky, Alex, Sutskever, Ilya, and Salakhutdinov, Ruslan R. Im-\nproving neural networks by preventing co-adaptation of feature detectors. arXiv preprint arXiv:1207.0580,\n2012b.\nKingma, Diederik P and Welling, Max. Auto-Encoding Variational Bayes. In The 2nd International Conference\non Learning Representations (ICLR), 2013.\nKrizhevsky, Alex, Sutskever, Ilya, and Hinton, Geoffrey E. Imagenet classi\ufb01cation with deep convolutional\nneural networks. In Advances in neural information processing systems, pp. 1097\u20131105, 2012.\nMaas, Andrew L, Daly, Raymond E, Pham, Peter T, Huang, Dan, Ng, Andrew Y, and Potts, Christopher.\nLearning word vectors for sentiment analysis. In Proceedings of the 49th Annual Meeting of the Association\nfor Computational Linguistics: Human Language Technologies-Volume 1, pp. 142\u2013150. Association for\nComputational Linguistics, 2011.\nMoulines, Eric and Bach, Francis R.\nNon-asymptotic analysis of stochastic approximation algorithms for\nmachine learning. In Advances in Neural Information Processing Systems, pp. 451\u2013459, 2011.\nPascanu, Razvan and Bengio, Yoshua.\nRevisiting natural gradient for deep networks.\narXiv preprint\narXiv:1301.3584, 2013.\nPolyak, Boris T and Juditsky, Anatoli B. Acceleration of stochastic approximation by averaging. SIAM Journal\non Control and Optimization, 30(4):838\u2013855, 1992.\n10\nPublished as a conference paper at ICLR 2015\nRoux, Nicolas L and Fitzgibbon, Andrew W. A fast natural newton method. In Proceedings of the 27th\nInternational Conference on Machine Learning (ICML-10), pp. 623\u2013630, 2010.\nRuppert, David. Ef\ufb01cient estimations from a slowly convergent robbins-monro process. Technical report,\nCornell University Operations Research and Industrial Engineering, 1988.\nSchaul, Tom, Zhang, Sixin, and LeCun, Yann. No more pesky learning rates. arXiv preprint arXiv:1206.1106,\n2012.\nSohl-Dickstein, Jascha, Poole, Ben, and Ganguli, Surya. Fast large-scale optimization by unifying stochas-\ntic gradient and quasi-newton methods. In Proceedings of the 31st International Conference on Machine\nLearning (ICML-14), pp. 604\u2013612, 2014.\nSutskever, Ilya, Martens, James, Dahl, George, and Hinton, Geoffrey. On the importance of initialization and\nmomentum in deep learning. In Proceedings of the 30th International Conference on Machine Learning\n(ICML-13), pp. 1139\u20131147, 2013.\nTieleman, T. and Hinton, G. Lecture 6.5 - RMSProp, COURSERA: Neural Networks for Machine Learning.\nTechnical report, 2012.\nWang, Sida and Manning, Christopher. Fast dropout training. In Proceedings of the 30th International Confer-\nence on Machine Learning (ICML-13), pp. 118\u2013126, 2013.\nZeiler, Matthew D. Adadelta: An adaptive learning rate method. arXiv preprint arXiv:1212.5701, 2012.\nZinkevich, Martin. Online convex programming and generalized in\ufb01nitesimal gradient ascent. 2003.\n11\nPublished as a conference paper at ICLR 2015\n10\nAPPENDIX\n10.1\nCONVERGENCE PROOF\nDe\ufb01nition 10.1. A function f : Rd \u2192R is convex if for all x, y \u2208Rd, for all \u03bb \u2208[0, 1],\n\u03bbf(x) + (1 \u2212\u03bb)f(y) \u2265f(\u03bbx + (1 \u2212\u03bb)y)\nAlso, notice that a convex function can be lower bounded by a hyperplane at its tangent.\nLemma 10.2. If a function f : Rd \u2192R is convex, then for all x, y \u2208Rd,\nf(y) \u2265f(x) + \u2207f(x)T (y \u2212x)\nThe above lemma can be used to upper bound the regret and our proof for the main theorem is\nconstructed by substituting the hyperplane with the Adam update rules.\nThe following two lemmas are used to support our main theorem. We also use some de\ufb01nitions sim-\nplify our notation, where gt \u225c\u2207ft(\u03b8t) and gt,i as the ith element. We de\ufb01ne g1:t,i \u2208Rt as a vector\nthat contains the ith dimension of the gradients over all iterations till t, g1:t,i = [g1,i, g2,i, \u00b7 \u00b7 \u00b7 , gt,i]\nLemma 10.3. Let gt = \u2207ft(\u03b8t) and g1:t be de\ufb01ned as above and bounded, \u2225gt\u22252 \u2264G, \u2225gt\u2225\u221e\u2264\nG\u221e. Then,\nT\nX\nt=1\ns\ng2\nt,i\nt\n\u22642G\u221e\u2225g1:T,i\u22252\nProof. We will prove the inequality using induction over T.\nThe base case for T = 1, we have\nq\ng2\n1,i \u22642G\u221e\u2225g1,i\u22252.\nFor the inductive step,\nT\nX\nt=1\ns\ng2\nt,i\nt\n=\nT \u22121\nX\nt=1\ns\ng2\nt,i\nt\n+\ns\ng2\nT,i\nT\n\u22642G\u221e\u2225g1:T \u22121,i\u22252 +\ns\ng2\nT,i\nT\n= 2G\u221e\nq\n\u2225g1:T,i\u22252\n2 \u2212g2\nT +\ns\ng2\nT,i\nT\nFrom, \u2225g1:T,i\u22252\n2 \u2212g2\nT,i +\ng4\nT,i\n4\u2225g1:T,i\u22252\n2 \u2265\u2225g1:T,i\u22252\n2 \u2212g2\nT,i, we can take square root of both side and\nhave,\nq\n\u2225g1:T,i\u22252\n2 \u2212g2\nT,i \u2264\u2225g1:T,i\u22252 \u2212\ng2\nT,i\n2\u2225g1:T,i\u22252\n\u2264\u2225g1:T,i\u22252 \u2212\ng2\nT,i\n2\np\nTG2\u221e\nRearrange the inequality and substitute the\nq\n\u2225g1:T,i\u22252\n2 \u2212g2\nT,i term,\nG\u221e\nq\n\u2225g1:T,i\u22252\n2 \u2212g2\nT +\ns\ng2\nT,i\nT\n\u22642G\u221e\u2225g1:T,i\u22252\n12\nPublished as a conference paper at ICLR 2015\nLemma 10.4. Let \u03b3 \u225c\n\u03b22\n1\n\u221a\u03b22 . For \u03b21, \u03b22 \u2208[0, 1) that satisfy\n\u03b22\n1\n\u221a\u03b22 < 1 and bounded gt, \u2225gt\u22252 \u2264G,\n\u2225gt\u2225\u221e\u2264G\u221e, the following inequality holds\nT\nX\nt=1\nbm2\nt,i\np\ntbvt,i\n\u2264\n2\n1 \u2212\u03b3\n1\n\u221a1 \u2212\u03b22\n\u2225g1:T,i\u22252\nProof. Under the assumption,\n\u221a\n1\u2212\u03b2t\n2\n(1\u2212\u03b2t\n1)2 \u2264\n1\n(1\u2212\u03b21)2 . We can expand the last term in the summation\nusing the update rules in Algorithm 1,\nT\nX\nt=1\nbm2\nt,i\np\ntbvt,i\n=\nT \u22121\nX\nt=1\nbm2\nt,i\np\ntbvt,i\n+\np\n1 \u2212\u03b2T\n2\n(1 \u2212\u03b2T\n1 )2\n(PT\nk=1(1 \u2212\u03b21)\u03b2T \u2212k\n1\ngk,i)2\nq\nT PT\nj=1(1 \u2212\u03b22)\u03b2T \u2212j\n2\ng2\nj,i\n\u2264\nT \u22121\nX\nt=1\nbm2\nt,i\np\ntbvt,i\n+\np\n1 \u2212\u03b2T\n2\n(1 \u2212\u03b2T\n1 )2\nT\nX\nk=1\nT((1 \u2212\u03b21)\u03b2T \u2212k\n1\ngk,i)2\nq\nT PT\nj=1(1 \u2212\u03b22)\u03b2T \u2212j\n2\ng2\nj,i\n\u2264\nT \u22121\nX\nt=1\nbm2\nt,i\np\ntbvt,i\n+\np\n1 \u2212\u03b2T\n2\n(1 \u2212\u03b2T\n1 )2\nT\nX\nk=1\nT((1 \u2212\u03b21)\u03b2T \u2212k\n1\ngk,i)2\nq\nT(1 \u2212\u03b22)\u03b2T \u2212k\n2\ng2\nk,i\n\u2264\nT \u22121\nX\nt=1\nbm2\nt,i\np\ntbvt,i\n+\np\n1 \u2212\u03b2T\n2\n(1 \u2212\u03b2T\n1 )2\n(1 \u2212\u03b21)2\np\nT(1 \u2212\u03b22)\nT\nX\nk=1\nT\n\u0012 \u03b22\n1\n\u221a\u03b22\n\u0013T \u2212k\n\u2225gk,i\u22252\n\u2264\nT \u22121\nX\nt=1\nbm2\nt,i\np\ntbvt,i\n+\nT\np\nT(1 \u2212\u03b22)\nT\nX\nk=1\n\u03b3T \u2212k\u2225gk,i\u22252\nSimilarly, we can upper bound the rest of the terms in the summation.\nT\nX\nt=1\nbm2\nt,i\np\ntbvt,i\n\u2264\nT\nX\nt=1\n\u2225gt,i\u22252\np\nt(1 \u2212\u03b22)\nT \u2212t\nX\nj=0\nt\u03b3j\n\u2264\nT\nX\nt=1\n\u2225gt,i\u22252\np\nt(1 \u2212\u03b22)\nT\nX\nj=0\nt\u03b3j\nFor \u03b3 < 1, using the upper bound on the arithmetic-geometric series, P\nt t\u03b3t <\n1\n(1\u2212\u03b3)2 :\nT\nX\nt=1\n\u2225gt,i\u22252\np\nt(1 \u2212\u03b22)\nT\nX\nj=0\nt\u03b3j \u2264\n1\n(1 \u2212\u03b3)2\u221a1 \u2212\u03b22\nT\nX\nt=1\n\u2225gt,i\u22252\n\u221a\nt\nApply Lemma 10.3,\nT\nX\nt=1\nbm2\nt,i\np\ntbvt,i\n\u2264\n2G\u221e\n(1 \u2212\u03b3)2\u221a1 \u2212\u03b22\n\u2225g1:T,i\u22252\nTo simplify the notation, we de\ufb01ne \u03b3 \u225c\n\u03b22\n1\n\u221a\u03b22 . Intuitively, our following theorem holds when the\nlearning rate \u03b1t is decaying at a rate of t\u22121\n2 and \ufb01rst moment running average coef\ufb01cient \u03b21,t decay\nexponentially with \u03bb, that is typically close to 1, e.g. 1 \u221210\u22128.\nTheorem 10.5. Assume that the function ft has bounded gradients, \u2225\u2207ft(\u03b8)\u22252 \u2264G, \u2225\u2207ft(\u03b8)\u2225\u221e\u2264\nG\u221efor all \u03b8 \u2208Rd and distance between any \u03b8t generated by Adam is bounded, \u2225\u03b8n \u2212\u03b8m\u22252 \u2264D,\n13\nPublished as a conference paper at ICLR 2015\n\u2225\u03b8m \u2212\u03b8n\u2225\u221e\u2264D\u221efor any m, n \u2208{1, ..., T}, and \u03b21, \u03b22 \u2208[0, 1) satisfy\n\u03b22\n1\n\u221a\u03b22 < 1. Let \u03b1t =\n\u03b1\n\u221a\nt\nand \u03b21,t = \u03b21\u03bbt\u22121, \u03bb \u2208(0, 1). Adam achieves the following guarantee, for all T \u22651.\nR(T) \u2264\nD2\n2\u03b1(1 \u2212\u03b21)\nd\nX\ni=1\np\nTbvT,i+\n\u03b1(\u03b21 + 1)G\u221e\n(1 \u2212\u03b21)\u221a1 \u2212\u03b22(1 \u2212\u03b3)2\nd\nX\ni=1\n\u2225g1:T,i\u22252+\nd\nX\ni=1\nD2\n\u221eG\u221e\n\u221a1 \u2212\u03b22\n2\u03b1(1 \u2212\u03b21)(1 \u2212\u03bb)2\nProof. Using Lemma 10.2, we have,\nft(\u03b8t) \u2212ft(\u03b8\u2217) \u2264gT\nt (\u03b8t \u2212\u03b8\u2217) =\nd\nX\ni=1\ngt,i(\u03b8t,i \u2212\u03b8\u2217\n,i)\nFrom the update rules presented in algorithm 1,\n\u03b8t+1 = \u03b8t \u2212\u03b1t bmt/\np\nbvt\n= \u03b8t \u2212\n\u03b1t\n1 \u2212\u03b2t\n1\n\u0012 \u03b21,t\n\u221abvt\nmt\u22121 + (1 \u2212\u03b21,t)\n\u221abvt\ngt\n\u0013\nWe focus on the ith dimension of the parameter vector \u03b8t \u2208Rd. Subtract the scalar \u03b8\u2217\n,i and square\nboth sides of the above update rule, we have,\n(\u03b8t+1,i \u2212\u03b8\u2217\n,i)2 =(\u03b8t,i \u2212\u03b8\u2217\n,i)2 \u2212\n2\u03b1t\n1 \u2212\u03b2t\n1\n( \u03b21,t\np\nbvt,i\nmt\u22121,i + (1 \u2212\u03b21,t)\np\nbvt,i\ngt,i)(\u03b8t,i \u2212\u03b8\u2217\n,i) + \u03b12\nt ( bmt,i\np\nbvt,i\n)2\nWe can rearrange the above equation and use Young\u2019s inequality, ab \u2264a2/2 + b2/2. Also, it can be\nshown that\np\nbvt,i =\nqPt\nj=1(1 \u2212\u03b22)\u03b2t\u2212j\n2\ng2\nj,i/\np\n1 \u2212\u03b2t\n2 \u2264\u2225g1:t,i\u22252 and \u03b21,t \u2264\u03b21. Then\ngt,i(\u03b8t,i \u2212\u03b8\u2217\n,i) =(1 \u2212\u03b2t\n1)\np\nbvt,i\n2\u03b1t(1 \u2212\u03b21,t)\n\u0012\n(\u03b8t,i \u2212\u03b8\u2217\n,t)2 \u2212(\u03b8t+1,i \u2212\u03b8\u2217\n,i)2\n\u0013\n+\n\u03b21,t\n(1 \u2212\u03b21,t)\nbv\n1\n4\nt\u22121,i\n\u221a\u03b1t\u22121\n(\u03b8\u2217\n,i \u2212\u03b8t,i)\u221a\u03b1t\u22121\nmt\u22121,i\nbv\n1\n4\nt\u22121,i\n+ \u03b1t(1 \u2212\u03b2t\n1)\np\nbvt,i\n2(1 \u2212\u03b21,t)\n( bmt,i\np\nbvt,i\n)2\n\u2264\n1\n2\u03b1t(1 \u2212\u03b21)\n\u0012\n(\u03b8t,i \u2212\u03b8\u2217\n,t)2 \u2212(\u03b8t+1,i \u2212\u03b8\u2217\n,i)2\n\u0013p\nbvt,i +\n\u03b21,t\n2\u03b1t\u22121(1 \u2212\u03b21,t)(\u03b8\u2217\n,i \u2212\u03b8t,i)2p\nbvt\u22121,i\n+\n\u03b21\u03b1t\u22121\n2(1 \u2212\u03b21)\nm2\nt\u22121,i\np\nbvt\u22121,i\n+\n\u03b1t\n2(1 \u2212\u03b21)\nbm2\nt,i\np\nbvt,i\nWe apply Lemma 10.4 to the above inequality and derive the regret bound by summing across all\nthe dimensions for i \u22081, ..., d in the upper bound of ft(\u03b8t) \u2212ft(\u03b8\u2217) and the sequence of convex\nfunctions for t \u22081, ..., T:\nR(T) \u2264\nd\nX\ni=1\n1\n2\u03b11(1 \u2212\u03b21)(\u03b81,i \u2212\u03b8\u2217\n,i)2p\nbv1,i +\nd\nX\ni=1\nT\nX\nt=2\n1\n2(1 \u2212\u03b21)(\u03b8t,i \u2212\u03b8\u2217\n,i)2(\np\nbvt,i\n\u03b1t\n\u2212\np\nbvt\u22121,i\n\u03b1t\u22121\n)\n+\n\u03b21\u03b1G\u221e\n(1 \u2212\u03b21)\u221a1 \u2212\u03b22(1 \u2212\u03b3)2\nd\nX\ni=1\n\u2225g1:T,i\u22252 +\n\u03b1G\u221e\n(1 \u2212\u03b21)\u221a1 \u2212\u03b22(1 \u2212\u03b3)2\nd\nX\ni=1\n\u2225g1:T,i\u22252\n+\nd\nX\ni=1\nT\nX\nt=1\n\u03b21,t\n2\u03b1t(1 \u2212\u03b21,t)(\u03b8\u2217\n,i \u2212\u03b8t,i)2p\nbvt,i\n14\nPublished as a conference paper at ICLR 2015\nFrom the assumption, \u2225\u03b8t \u2212\u03b8\u2217\u22252 \u2264D, \u2225\u03b8m \u2212\u03b8n\u2225\u221e\u2264D\u221e, we have:\nR(T) \u2264\nD2\n2\u03b1(1 \u2212\u03b21)\nd\nX\ni=1\np\nTbvT,i +\n\u03b1(1 + \u03b21)G\u221e\n(1 \u2212\u03b21)\u221a1 \u2212\u03b22(1 \u2212\u03b3)2\nd\nX\ni=1\n\u2225g1:T,i\u22252 + D2\n\u221e\n2\u03b1\nd\nX\ni=1\nt\nX\nt=1\n\u03b21,t\n(1 \u2212\u03b21,t)\np\ntbvt,i\n\u2264\nD2\n2\u03b1(1 \u2212\u03b21)\nd\nX\ni=1\np\nTbvT,i +\n\u03b1(1 + \u03b21)G\u221e\n(1 \u2212\u03b21)\u221a1 \u2212\u03b22(1 \u2212\u03b3)2\nd\nX\ni=1\n\u2225g1:T,i\u22252\n+ D2\n\u221eG\u221e\n\u221a1 \u2212\u03b22\n2\u03b1\nd\nX\ni=1\nt\nX\nt=1\n\u03b21,t\n(1 \u2212\u03b21,t)\n\u221a\nt\nWe can use arithmetic geometric series upper bound for the last term:\nt\nX\nt=1\n\u03b21,t\n(1 \u2212\u03b21,t)\n\u221a\nt \u2264\nt\nX\nt=1\n1\n(1 \u2212\u03b21)\u03bbt\u22121\u221a\nt\n\u2264\nt\nX\nt=1\n1\n(1 \u2212\u03b21)\u03bbt\u22121t\n\u2264\n1\n(1 \u2212\u03b21)(1 \u2212\u03bb)2\nTherefore, we have the following regret bound:\nR(T) \u2264\nD2\n2\u03b1(1 \u2212\u03b21)\nd\nX\ni=1\np\nTbvT,i +\n\u03b1(1 + \u03b21)G\u221e\n(1 \u2212\u03b21)\u221a1 \u2212\u03b22(1 \u2212\u03b3)2\nd\nX\ni=1\n\u2225g1:T,i\u22252 +\nd\nX\ni=1\nD2\n\u221eG\u221e\n\u221a1 \u2212\u03b22\n2\u03b1\u03b21(1 \u2212\u03bb)2\n15\n",
        "sentence": "",
        "context": "modeling in speech recognition: The shared views of four research groups. Signal Processing Magazine,\nIEEE, 29(6):82\u201397, 2012a.\nHinton, Geoffrey E, Srivastava, Nitish, Krizhevsky, Alex, Sutskever, Ilya, and Salakhutdinov, Ruslan R. Im-\nDeng, Li, Li, Jinyu, Huang, Jui-Ting, Yao, Kaisheng, Yu, Dong, Seide, Frank, Seltzer, Michael, Zweig, Geoff,\nHe, Xiaodong, Williams, Jason, et al. Recent advances in deep learning for speech research at microsoft.\nICASSP 2013, 2013.\n(5786):504\u2013507, 2006.\nHinton, Geoffrey, Deng, Li, Yu, Dong, Dahl, George E, Mohamed, Abdel-rahman, Jaitly, Navdeep, Senior,\nAndrew, Vanhoucke, Vincent, Nguyen, Patrick, Sainath, Tara N, et al. Deep neural networks for acoustic"
    },
    {
        "title": "Samplernn: An unconditional end-to-end neural audio generation model",
        "author": [
            "Mehri",
            "Soroush",
            "Kumar",
            "Kundan",
            "Gulrajani",
            "Ishaan",
            "Rithesh",
            "Jain",
            "Shubham",
            "Sotelo",
            "Jose",
            "Courville",
            "Aaron",
            "Bengio",
            "Yoshua"
        ],
        "venue": "arXiv preprint arXiv:1612.07837,",
        "citeRegEx": "Mehri et al\\.,? \\Q2016\\E",
        "shortCiteRegEx": "Mehri et al\\.",
        "year": 2016,
        "abstract": "",
        "full_text": "",
        "sentence": " , 2016), and audio synthesis models (van den Oord et al., 2016; Mehri et al., 2016). Most recently, there has been a lot of work in parametric audio synthesis, notably WaveNet, SampleRNN, and Char2Wav (van den Oord et al., 2016; Mehri et al., 2016; Sotelo et al., 2017).",
        "context": null
    },
    {
        "title": "World: a vocoder-based high-quality speech synthesis system for real-time applications",
        "author": [
            "Morise",
            "Masanori",
            "Yokomori",
            "Fumiya",
            "Ozawa",
            "Kenji"
        ],
        "venue": "IEICE TRANSACTIONS on Information and Systems,",
        "citeRegEx": "Morise et al\\.,? \\Q2016\\E",
        "shortCiteRegEx": "Morise et al\\.",
        "year": 2016,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " Char2Wav relies on vocoder features from the WORLD TTS system (Morise et al., 2016) for pre-training their alignment module which include F0, spectral envelope, and aperiodic parameters.",
        "context": null
    },
    {
        "title": "Pixel recurrent neural networks",
        "author": [
            "Oord",
            "Aaron van den",
            "Kalchbrenner",
            "Nal",
            "Kavukcuoglu",
            "Koray"
        ],
        "venue": "arXiv preprint arXiv:1601.06759,",
        "citeRegEx": "Oord et al\\.,? \\Q2016\\E",
        "shortCiteRegEx": "Oord et al\\.",
        "year": 2016,
        "abstract": "Modeling the distribution of natural images is a landmark problem in\nunsupervised learning. This task requires an image model that is at once\nexpressive, tractable and scalable. We present a deep neural network that\nsequentially predicts the pixels in an image along the two spatial dimensions.\nOur method models the discrete probability of the raw pixel values and encodes\nthe complete set of dependencies in the image. Architectural novelties include\nfast two-dimensional recurrent layers and an effective use of residual\nconnections in deep recurrent networks. We achieve log-likelihood scores on\nnatural images that are considerably better than the previous state of the art.\nOur main results also provide benchmarks on the diverse ImageNet dataset.\nSamples generated from the model appear crisp, varied and globally coherent.",
        "full_text": "Pixel Recurrent Neural Networks\nA\u00a8aron van den Oord\nAVDNOORD@GOOGLE.COM\nNal Kalchbrenner\nNALK@GOOGLE.COM\nKoray Kavukcuoglu\nKORAYK@GOOGLE.COM\nGoogle DeepMind\nAbstract\nModeling the distribution of natural images is\na landmark problem in unsupervised learning.\nThis task requires an image model that is at\nonce expressive, tractable and scalable.\nWe\npresent a deep neural network that sequentially\npredicts the pixels in an image along the two\nspatial dimensions. Our method models the dis-\ncrete probability of the raw pixel values and en-\ncodes the complete set of dependencies in the\nimage. Architectural novelties include fast two-\ndimensional recurrent layers and an effective use\nof residual connections in deep recurrent net-\nworks. We achieve log-likelihood scores on nat-\nural images that are considerably better than the\nprevious state of the art. Our main results also\nprovide benchmarks on the diverse ImageNet\ndataset. Samples generated from the model ap-\npear crisp, varied and globally coherent.\n1. Introduction\nGenerative image modeling is a central problem in unsu-\npervised learning. Probabilistic density models can be used\nfor a wide variety of tasks that range from image compres-\nsion and forms of reconstruction such as image inpainting\n(e.g., see Figure 1) and deblurring, to generation of new\nimages. When the model is conditioned on external infor-\nmation, possible applications also include creating images\nbased on text descriptions or simulating future frames in a\nplanning task. One of the great advantages in generative\nmodeling is that there are practically endless amounts of\nimage data available to learn from. However, because im-\nages are high dimensional and highly structured, estimating\nthe distribution of natural images is extremely challenging.\nOne of the most important obstacles in generative mod-\nProceedings of the 33 rd International Conference on Machine\nLearning, New York, NY, USA, 2016. JMLR: W&CP volume\n48. Copyright 2016 by the author(s).\noccluded\ncompletions\noriginal\nFigure 1. Image completions sampled from a PixelRNN.\neling is building complex and expressive models that are\nalso tractable and scalable. This trade-off has resulted in\na large variety of generative models, each having their ad-\nvantages. Most work focuses on stochastic latent variable\nmodels such as VAE\u2019s (Rezende et al., 2014; Kingma &\nWelling, 2013) that aim to extract meaningful representa-\ntions, but often come with an intractable inference step that\ncan hinder their performance.\nOne effective approach to tractably model a joint distribu-\ntion of the pixels in the image is to cast it as a product of\nconditional distributions; this approach has been adopted in\nautoregressive models such as NADE (Larochelle & Mur-\nray, 2011) and fully visible neural networks (Neal, 1992;\nBengio & Bengio, 2000). The factorization turns the joint\nmodeling problem into a sequence problem, where one\nlearns to predict the next pixel given all the previously gen-\nerated pixels. But to model the highly nonlinear and long-\nrange correlations between pixels and the complex condi-\ntional distributions that result, a highly expressive sequence\nmodel is necessary.\nRecurrent Neural Networks (RNN) are powerful models\nthat offer a compact, shared parametrization of a series of\nconditional distributions. RNNs have been shown to excel\nat hard sequence problems ranging from handwriting gen-\neration (Graves, 2013), to character prediction (Sutskever\net al., 2011) and to machine translation (Kalchbrenner &\nBlunsom, 2013). A two-dimensional RNN has produced\nvery promising results in modeling grayscale images and\ntextures (Theis & Bethge, 2015).\nIn this paper we advance two-dimensional RNNs and ap-\narXiv:1601.06759v3  [cs.CV]  19 Aug 2016\nPixel Recurrent Neural Networks\nx1\nxi\nxn\nxn2\nContext \nxn2\nMulti-scale context\nx1\nxi\nxn\nxn2\nR\nG\nB\nR\nG\nB\nR\nG\nB\nMask A\nMask B\nContext\nFigure 2. Left: To generate pixel xi one conditions on all the pre-\nviously generated pixels left and above of xi. Center: To gen-\nerate a pixel in the multi-scale case we can also condition on the\nsubsampled image pixels (in light blue). Right: Diagram of the\nconnectivity inside a masked convolution. In the \ufb01rst layer, each\nof the RGB channels is connected to previous channels and to the\ncontext, but is not connected to itself. In subsequent layers, the\nchannels are also connected to themselves.\nply them to large-scale modeling of natural images. The\nresulting PixelRNNs are composed of up to twelve, fast\ntwo-dimensional Long Short-Term Memory (LSTM) lay-\ners. These layers use LSTM units in their state (Hochreiter\n& Schmidhuber, 1997; Graves & Schmidhuber, 2009) and\nadopt a convolution to compute at once all the states along\none of the spatial dimensions of the data. We design two\ntypes of these layers. The \ufb01rst type is the Row LSTM layer\nwhere the convolution is applied along each row; a similar\ntechnique is described in (Stollenga et al., 2015). The sec-\nond type is the Diagonal BiLSTM layer where the convolu-\ntion is applied in a novel fashion along the diagonals of the\nimage. The networks also incorporate residual connections\n(He et al., 2015) around LSTM layers; we observe that this\nhelps with training of the PixelRNN for up to twelve layers\nof depth.\nWe also consider a second, simpli\ufb01ed architecture which\nshares the same core components as the PixelRNN. We ob-\nserve that Convolutional Neural Networks (CNN) can also\nbe used as sequence model with a \ufb01xed dependency range,\nby using Masked convolutions. The PixelCNN architec-\nture is a fully convolutional network of \ufb01fteen layers that\npreserves the spatial resolution of its input throughout the\nlayers and outputs a conditional distribution at each loca-\ntion.\nBoth PixelRNN and PixelCNN capture the full generality\nof pixel inter-dependencies without introducing indepen-\ndence assumptions as in e.g., latent variable models. The\ndependencies are also maintained between the RGB color\nvalues within each individual pixel. Furthermore, in con-\ntrast to previous approaches that model the pixels as con-\ntinuous values (e.g., Theis & Bethge (2015); Gregor et al.\n(2014)), we model the pixels as discrete values using a\nmultinomial distribution implemented with a simple soft-\nmax layer. We observe that this approach gives both repre-\nsentational and training advantages for our models.\nThe contributions of the paper are as follows. In Section\n3 we design two types of PixelRNNs corresponding to the\ntwo types of LSTM layers; we describe the purely convo-\nlutional PixelCNN that is our fastest architecture; and we\ndesign a Multi-Scale version of the PixelRNN. In Section 5\nwe show the relative bene\ufb01ts of using the discrete softmax\ndistribution in our models and of adopting residual connec-\ntions for the LSTM layers. Next we test the models on\nMNIST and on CIFAR-10 and show that they obtain log-\nlikelihood scores that are considerably better than previous\nresults. We also provide results for the large-scale Ima-\ngeNet dataset resized to both 32 \u00d7 32 and 64 \u00d7 64 pixels;\nto our knowledge likelihood values from generative models\nhave not previously been reported on this dataset. Finally,\nwe give a qualitative evaluation of the samples generated\nfrom the PixelRNNs.\n2. Model\nOur aim is to estimate a distribution over natural images\nthat can be used to tractably compute the likelihood of im-\nages and to generate new ones. The network scans the im-\nage one row at a time and one pixel at a time within each\nrow. For each pixel it predicts the conditional distribution\nover the possible pixel values given the scanned context.\nFigure 2 illustrates this process. The joint distribution over\nthe image pixels is factorized into a product of conditional\ndistributions. The parameters used in the predictions are\nshared across all pixel positions in the image.\nTo capture the generation process, Theis & Bethge (2015)\npropose to use a two-dimensional LSTM network (Graves\n& Schmidhuber, 2009) that starts at the top left pixel and\nproceeds towards the bottom right pixel. The advantage of\nthe LSTM network is that it effectively handles long-range\ndependencies that are central to object and scene under-\nstanding. The two-dimensional structure ensures that the\nsignals are well propagated both in the left-to-right and top-\nto-bottom directions.\nIn this section we \ufb01rst focus on the form of the distribution,\nwhereas the next section will be devoted to describing the\narchitectural innovations inside PixelRNN.\n2.1. Generating an Image Pixel by Pixel\nThe goal is to assign a probability p(x) to each image x\nformed of n\u00d7n pixels. We can write the image x as a one-\ndimensional sequence x1, ..., xn2 where pixels are taken\nfrom the image row by row. To estimate the joint distri-\nbution p(x) we write it as the product of the conditional\ndistributions over the pixels:\np(x) =\nn2\nY\ni=1\np(xi|x1, ..., xi\u22121)\n(1)\nPixel Recurrent Neural Networks\nThe value p(xi|x1, ..., xi\u22121) is the probability of the i-th\npixel xi given all the previous pixels x1, ..., xi\u22121. The gen-\neration proceeds row by row and pixel by pixel. Figure 2\n(Left) illustrates the conditioning scheme.\nEach pixel xi is in turn jointly determined by three values,\none for each of the color channels Red, Green and Blue\n(RGB). We rewrite the distribution p(xi|x<i) as the fol-\nlowing product:\np(xi,R|x<i)p(xi,G|x<i, xi,R)p(xi,B|x<i, xi,R, xi,G) (2)\nEach of the colors is thus conditioned on the other channels\nas well as on all the previously generated pixels.\nNote that during training and evaluation the distributions\nover the pixel values are computed in parallel, while the\ngeneration of an image is sequential.\n2.2. Pixels as Discrete Variables\nPrevious approaches use a continuous distribution for the\nvalues of the pixels in the image (e.g. Theis & Bethge\n(2015); Uria et al. (2014)). By contrast we model p(x) as\na discrete distribution, with every conditional distribution\nin Equation 2 being a multinomial that is modeled with a\nsoftmax layer. Each channel variable xi,\u2217simply takes one\nof 256 distinct values. The discrete distribution is represen-\ntationally simple and has the advantage of being arbitrarily\nmultimodal without prior on the shape (see Fig. 6). Exper-\nimentally we also \ufb01nd the discrete distribution to be easy\nto learn and to produce better performance compared to a\ncontinuous distribution (Section 5).\n3. Pixel Recurrent Neural Networks\nIn this section we describe the architectural components\nthat compose the PixelRNN. In Sections 3.1 and 3.2, we\ndescribe the two types of LSTM layers that use convolu-\ntions to compute at once the states along one of the spatial\ndimensions. In Section 3.3 we describe how to incorporate\nresidual connections to improve the training of a PixelRNN\nwith many LSTM layers. In Section 3.4 we describe the\nsoftmax layer that computes the discrete joint distribution\nof the colors and the masking technique that ensures the\nproper conditioning scheme. In Section 3.5 we describe the\nPixelCNN architecture. Finally in Section 3.6 we describe\nthe multi-scale architecture.\n3.1. Row LSTM\nThe Row LSTM is a unidirectional layer that processes\nthe image row by row from top to bottom computing fea-\ntures for a whole row at once; the computation is per-\nformed with a one-dimensional convolution. For a pixel\nxi the layer captures a roughly triangular context above the\npixel as shown in Figure 4 (center). The kernel of the one-\nFigure 3. In the Diagonal BiLSTM, to allow for parallelization\nalong the diagonals, the input map is skewed by offseting each\nrow by one position with respect to the previous row. When the\nspatial layer is computed left to right and column by column, the\noutput map is shifted back into the original size. The convolution\nuses a kernel of size 2 \u00d7 1.\ndimensional convolution has size k \u00d7 1 where k \u22653; the\nlarger the value of k the broader the context that is captured.\nThe weight sharing in the convolution ensures translation\ninvariance of the computed features along each row.\nThe computation proceeds as follows. An LSTM layer has\nan input-to-state component and a recurrent state-to-state\ncomponent that together determine the four gates inside the\nLSTM core. To enhance parallelization in the Row LSTM\nthe input-to-state component is \ufb01rst computed for the entire\ntwo-dimensional input map; for this a k \u00d7 1 convolution is\nused to follow the row-wise orientation of the LSTM itself.\nThe convolution is masked to include only the valid context\n(see Section 3.4) and produces a tensor of size 4h \u00d7 n \u00d7 n,\nrepresenting the four gate vectors for each position in the\ninput map, where h is the number of output feature maps.\nTo compute one step of the state-to-state component of\nthe LSTM layer, one is given the previous hidden and cell\nstates hi\u22121 and ci\u22121, each of size h \u00d7 n \u00d7 1. The new\nhidden and cell states hi, ci are obtained as follows:\n[oi, fi, ii, gi] = \u03c3(Kss \u229bhi\u22121 + Kis \u229bxi)\nci = fi \u2299ci\u22121 + ii \u2299gi\nhi = oi \u2299tanh(ci)\n(3)\nwhere xi of size h \u00d7 n \u00d7 1 is row i of the input map, and\n\u229brepresents the convolution operation and \u2299the element-\nwise multiplication.\nThe weights Kss and Kis are the\nkernel weights for the state-to-state and the input-to-state\ncomponents, where the latter is precomputed as described\nabove. In the case of the output, forget and input gates oi,\nfi and ii, the activation \u03c3 is the logistic sigmoid function,\nwhereas for the content gate gi, \u03c3 is the tanh function.\nEach step computes at once the new state for an entire row\nof the input map. Because the Row LSTM has a triangular\nreceptive \ufb01eld (Figure 4), it is unable to capture the entire\navailable context.\nPixel Recurrent Neural Networks\nPixelCNN\nRow LSTM\nDiagonal BiLSTM\nFigure 4. Visualization of the input-to-state and state-to-state\nmappings for the three proposed architectures.\n3.2. Diagonal BiLSTM\nThe Diagonal BiLSTM is designed to both parallelize the\ncomputation and to capture the entire available context for\nany image size. Each of the two directions of the layer\nscans the image in a diagonal fashion starting from a cor-\nner at the top and reaching the opposite corner at the bot-\ntom. Each step in the computation computes at once the\nLSTM state along a diagonal in the image. Figure 4 (right)\nillustrates the computation and the resulting receptive \ufb01eld.\nThe diagonal computation proceeds as follows. We \ufb01rst\nskew the input map into a space that makes it easy to ap-\nply convolutions along diagonals. The skewing operation\noffsets each row of the input map by one position with re-\nspect to the previous row, as illustrated in Figure 3; this\nresults in a map of size n \u00d7 (2n \u22121). At this point we can\ncompute the input-to-state and state-to-state components of\nthe Diagonal BiLSTM. For each of the two directions, the\ninput-to-state component is simply a 1\u00d71 convolution Kis\nthat contributes to the four gates in the LSTM core; the op-\neration generates a 4h \u00d7 n \u00d7 n tensor. The state-to-state\nrecurrent component is then computed with a column-wise\nconvolution Kss that has a kernel of size 2 \u00d7 1. The step\ntakes the previous hidden and cell states, combines the con-\ntribution of the input-to-state component and produces the\nnext hidden and cell states, as de\ufb01ned in Equation 3. The\noutput feature map is then skewed back into an n \u00d7 n map\nby removing the offset positions. This computation is re-\npeated for each of the two directions. Given the two out-\nput maps, to prevent the layer from seeing future pixels,\nthe right output map is then shifted down by one row and\nadded to the left output map.\nBesides reaching the full dependency \ufb01eld, the Diagonal\nBiLSTM has the additional advantage that it uses a con-\nvolutional kernel of size 2 \u00d7 1 that processes a minimal\namount of information at each step yielding a highly non-\nlinear computation. Kernel sizes larger than 2 \u00d7 1 are not\nparticularly useful as they do not broaden the already global\nreceptive \ufb01eld of the Diagonal BiLSTM.\n3.3. Residual Connections\nWe train PixelRNNs of up to twelve layers of depth. As\na means to both increase convergence speed and propagate\nsignals more directly through the network, we deploy resid-\nual connections (He et al., 2015) from one LSTM layer to\nthe next. Figure 5 shows a diagram of the residual blocks.\nThe input map to the PixelRNN LSTM layer has 2h fea-\ntures. The input-to-state component reduces the number of\nfeatures by producing h features per gate. After applying\nthe recurrent layer, the output map is upsampled back to 2h\nfeatures per position via a 1 \u00d7 1 convolution and the input\nmap is added to the output map. This method is related to\nprevious approaches that use gating along the depth of the\nrecurrent network (Kalchbrenner et al., 2015; Zhang et al.,\n2016), but has the advantage of not requiring additional\ngates. Apart from residual connections, one can also use\nlearnable skip connections from each layer to the output.\nIn the experiments we evaluate the relative effectiveness of\nresidual and layer-to-output skip connections.\nReLU - 1x1 Conv\n+\nReLU - 3x3 Conv\nh\n2h\nReLU - 1x1 Conv\nh\n2h\nLSTM\n+\n2h\n1x1 Conv\nh\n2h\nFigure 5. Residual blocks for a PixelCNN (left) and PixelRNNs.\n3.4. Masked Convolution\nThe h features for each input position at every layer in the\nnetwork are split into three parts, each corresponding to\none of the RGB channels. When predicting the R chan-\nnel for the current pixel xi, only the generated pixels left\nand above of xi can be used as context. When predicting\nthe G channel, the value of the R channel can also be used\nas context in addition to the previously generated pixels.\nLikewise, for the B channel, the values of both the R and\nG channels can be used. To restrict connections in the net-\nwork to these dependencies, we apply a mask to the input-\nto-state convolutions and to other purely convolutional lay-\ners in a PixelRNN.\nWe use two types of masks that we indicate with mask A\nand mask B, as shown in Figure 2 (Right). Mask A is ap-\nplied only to the \ufb01rst convolutional layer in a PixelRNN\nand restricts the connections to those neighboring pixels\nand to those colors in the current pixels that have already\nbeen predicted. On the other hand, mask B is applied to\nall the subsequent input-to-state convolutional transitions\nand relaxes the restrictions of mask A by also allowing the\nconnection from a color to itself. The masks can be eas-\nily implemented by zeroing out the corresponding weights\nin the input-to-state convolutions after each update. Simi-\nPixel Recurrent Neural Networks\nPixelCNN\nRow LSTM\nDiagonal BiLSTM\n7 \u00d7 7 conv mask A\nMultiple residual blocks: (see \ufb01g 5)\nConv\nRow LSTM\nDiagonal BiLSTM\n3 \u00d7 3 mask B\ni-s: 3 \u00d7 1 mask B\ni-s: 1 \u00d7 1 mask B\ns-s: 3 \u00d7 1 no mask\ns-s: 1 \u00d7 2 no mask\nReLU followed by 1 \u00d7 1 conv, mask B (2 layers)\n256-way Softmax for each RGB color (Natural images)\nor Sigmoid (MNIST)\nTable 1. Details of the architectures. In the LSTM architectures\ni-s and s-s stand for input-state and state-state convolutions.\nlar masks have also been used in variational autoencoders\n(Gregor et al., 2014; Germain et al., 2015).\n3.5. PixelCNN\nThe Row and Diagonal LSTM layers have a potentially\nunbounded dependency range within their receptive \ufb01eld.\nThis comes with a computational cost as each state needs\nto be computed sequentially. One simple workaround is\nto make the receptive \ufb01eld large, but not unbounded. We\ncan use standard convolutional layers to capture a bounded\nreceptive \ufb01eld and compute features for all pixel positions\nat once. The PixelCNN uses multiple convolutional lay-\ners that preserve the spatial resolution; pooling layers are\nnot used. Masks are adopted in the convolutions to avoid\nseeing the future context; masks have previously also been\nused in non-convolutional models such as MADE (Ger-\nmain et al., 2015). Note that the advantage of paralleliza-\ntion of the PixelCNN over the PixelRNN is only available\nduring training or during evaluating of test images. The\nimage generation process is sequential for both kinds of\nnetworks, as each sampled pixel needs to be given as input\nback into the network.\n3.6. Multi-Scale PixelRNN\nThe Multi-Scale PixelRNN is composed of an uncondi-\ntional PixelRNN and one or more conditional PixelRNNs.\nThe unconditional network \ufb01rst generates in the standard\nway a smaller s\u00d7s image that is subsampled from the orig-\ninal image. The conditional network then takes the s \u00d7 s\nimage as an additional input and generates a larger n \u00d7 n\nimage, as shown in Figure 2 (Middle).\nThe conditional network is similar to a standard PixelRNN,\nbut each of its layers is biased with an upsampled version\nof the small s \u00d7 s image. The upsampling and biasing pro-\ncesses are de\ufb01ned as follows. In the upsampling process,\none uses a convolutional network with deconvolutional lay-\ners to construct an enlarged feature map of size c \u00d7 n \u00d7 n,\nwhere c is the number of features in the output map of the\nupsampling network. Then, in the biasing process, for each\nlayer in the conditional PixelRNN, one simply maps the\nc \u00d7 n \u00d7 n conditioning map into a 4h \u00d7 n \u00d7 n map that is\nadded to the input-to-state map of the corresponding layer;\nthis is performed using a 1 \u00d7 1 unmasked convolution. The\nlarger n \u00d7 n image is then generated as usual.\n4. Speci\ufb01cations of Models\nIn this section we give the speci\ufb01cations of the PixelRNNs\nused in the experiments. We have four types of networks:\nthe PixelRNN based on Row LSTM, the one based on Di-\nagonal BiLSTM, the fully convolutional one and the Multi-\nScale one.\nTable 1 speci\ufb01es each layer in the single-scale networks.\nThe \ufb01rst layer is a 7 \u00d7 7 convolution that uses the mask of\ntype A. The two types of LSTM networks then use a vari-\nable number of recurrent layers. The input-to-state con-\nvolution in this layer uses a mask of type B, whereas the\nstate-to-state convolution is not masked. The PixelCNN\nuses convolutions of size 3 \u00d7 3 with a mask of type B.\nThe top feature map is then passed through a couple of\nlayers consisting of a Recti\ufb01ed Linear Unit (ReLU) and a\n1\u00d71 convolution. For the CIFAR-10 and ImageNet experi-\nments, these layers have 1024 feature maps; for the MNIST\nexperiment, the layers have 32 feature maps. Residual and\nlayer-to-output connections are used across the layers of all\nthree networks.\nThe networks used in the experiments have the following\nhyperparameters. For MNIST we use a Diagonal BiLSTM\nwith 7 layers and a value of h = 16 (Section 3.3 and Figure\n5 right). For CIFAR-10 the Row and Diagonal BiLSTMs\nhave 12 layers and a number of h = 128 units. The Pixel-\nCNN has 15 layers and h = 128. For 32 \u00d7 32 ImageNet\nwe adopt a 12 layer Row LSTM with h = 384 units and\nfor 64 \u00d7 64 ImageNet we use a 4 layer Row LSTM with\nh = 512 units; the latter model does not use residual con-\nnections.\n5. Experiments\nIn this section we describe our experiments and results. We\nbegin by describing the way we evaluate and compare our\nresults. In Section 5.2 we give details about the training.\nThen we give results on the relative effectiveness of archi-\ntectural components and our best results on the MNIST,\nCIFAR-10 and ImageNet datasets.\n5.1. Evaluation\nAll our models are trained and evaluated on the log-\nlikelihood loss function coming from a discrete distribu-\ntion. Although natural image data is usually modeled with\ncontinuous distributions using density functions, we can\ncompare our results with previous art in the following way.\nPixel Recurrent Neural Networks\nIn the literature it is currently best practice to add real-\nvalued noise to the pixel values to dequantize the data when\nusing density functions (Uria et al., 2013). When uniform\nnoise is added (with values in the interval [0, 1]), then the\nlog-likelihoods of continuous and discrete models are di-\nrectly comparable (Theis et al., 2015). In our case, we can\nuse the values from the discrete distribution as a piecewise-\nuniform continuous function that has a constant value for\nevery interval [i, i + 1], i = 1, 2, . . . 256. This correspond-\ning distribution will have the same log-likelihood (on data\nwith added noise) as the original discrete distribution (on\ndiscrete data).\nFor MNIST we report the negative log-likelihood in nats\nas it is common practice in literature. For CIFAR-10 and\nImageNet we report negative log-likelihoods in bits per di-\nmension. The total discrete log-likelihood is normalized by\nthe dimensionality of the images (e.g., 32 \u00d7 32 \u00d7 3 = 3072\nfor CIFAR-10).\nThese numbers are interpretable as the\nnumber of bits that a compression scheme based on this\nmodel would need to compress every RGB color value\n(van den Oord & Schrauwen, 2014b; Theis et al., 2015);\nin practice there is also a small overhead due to arithmetic\ncoding.\n5.2. Training Details\nOur models are trained on GPUs using the Torch toolbox.\nFrom the different parameter update rules tried, RMSProp\ngives best convergence performance and is used for all ex-\nperiments. The learning rate schedules were manually set\nfor every dataset to the highest values that allowed fast con-\nvergence. The batch sizes also vary for different datasets.\nFor smaller datasets such as MNIST and CIFAR-10 we use\nsmaller batch sizes of 16 images as this seems to regularize\nthe models. For ImageNet we use as large a batch size as\nallowed by the GPU memory; this corresponds to 64 im-\nages/batch for 32 \u00d7 32 ImageNet, and 32 images/batch for\n64 \u00d7 64 ImageNet. Apart from scaling and centering the\nimages at the input of the network, we don\u2019t use any other\npreprocessing or augmentation. For the multinomial loss\nfunction we use the raw pixel color values as categories.\nFor all the PixelRNN models, we learn the initial recurrent\nstate of the network.\n5.3. Discrete Softmax Distribution\nApart from being intuitive and easy to implement, we \ufb01nd\nthat using a softmax on discrete pixel values instead of a\nmixture density approach on continuous pixel values gives\nbetter results. For the Row LSTM model with a softmax\noutput distribution we obtain 3.06 bits/dim on the CIFAR-\n10 validation set. For the same model with a Mixture of\nConditional Gaussian Scale Mixtures (MCGSM) (Theis &\nBethge, 2015) we obtain 3.22 bits/dim.\nIn Figure 6 we show a few softmax activations from the\nmodel. Although we don\u2019t embed prior information about\nthe meaning or relations of the 256 color categories, e.g.\nthat pixel values 51 and 52 are neighbors, the distributions\npredicted by the model are meaningful and can be multi-\nmodal, skewed, peaked or long tailed. Also note that values\n0 and 255 often get a much higher probability as they are\nmore frequent. Another advantage of the discrete distribu-\ntion is that we do not worry about parts of the distribution\nmass lying outside the interval [0, 255], which is something\nthat typically happens with continuous distributions.\n 0\n 50\n 100\n 150\n 200\n 250\n 0\n 50\n 100\n 150\n 200\n 250\n 0\n 50\n 100\n 150\n 200\n 250\n 0\n 50\n 100\n 150\n 200\n 250\n 0                                                                               255\n0                                                                            255\n 0                                                                               255\n 0                                                                               255\n 0                                                                               255\nFigure 6. Example softmax activations from the model. The top\nleft shows the distribution of the \ufb01rst pixel red value (\ufb01rst value\nto sample).\n5.4. Residual Connections\nAnother core component of the networks is residual con-\nnections. In Table 2 we show the results of having residual\nconnections, having standard skip connections or having\nboth, in the 12-layer CIFAR-10 Row LSTM model. We\nsee that using residual connections is as effective as using\nskip connections; using both is also effective and preserves\nthe advantage.\nNo skip\nSkip\nNo residual:\n3.22\n3.09\nResidual:\n3.07\n3.06\nTable 2. Effect of residual and skip connections in the Row LSTM\nnetwork evaluated on the Cifar-10 validation set in bits/dim.\nWhen using both the residual and skip connections, we see\nin Table 3 that performance of the Row LSTM improves\nwith increased depth. This holds for up to the 12 LSTM\nlayers that we tried.\nPixel Recurrent Neural Networks\nFigure 7. Samples from models trained on CIFAR-10 (left) and ImageNet 32x32 (right) images. In general we can see that the models\ncapture local spatial dependencies relatively well. The ImageNet model seems to be better at capturing more global structures than the\nCIFAR-10 model. The ImageNet model was larger and trained on much more data, which explains the qualitative difference in samples.\n# layers:\n1\n2\n3\n6\n9\n12\nNLL:\n3.30\n3.20\n3.17\n3.09\n3.08\n3.06\nTable 3. Effect of the number of layers on the negative log likeli-\nhood evaluated on the CIFAR-10 validation set (bits/dim).\n5.5. MNIST\nAlthough the goal of our work was to model natural images\non a large scale, we also tried our model on the binary ver-\nsion (Salakhutdinov & Murray, 2008) of MNIST (LeCun\net al., 1998) as it is a good sanity check and there is a lot\nof previous art on this dataset to compare with. In Table 4\nwe report the performance of the Diagonal BiLSTM model\nand that of previous published results. To our knowledge\nthis is the best reported result on MNIST so far.\n5.6. CIFAR-10\nNext we test our models on the CIFAR-10 dataset\n(Krizhevsky, 2009). Table 5 lists the results of our mod-\nels and that of previously published approaches. All our\nresults were obtained without data augmentation. For the\nproposed networks, the Diagonal BiLSTM has the best\nperformance, followed by the Row LSTM and the Pixel-\nCNN. This coincides with the size of the respective recep-\ntive \ufb01elds: the Diagonal BiLSTM has a global view, the\nRow LSTM has a partially occluded view and the Pixel-\nCNN sees the fewest pixels in the context. This suggests\nthat effectively capturing a large receptive \ufb01eld is impor-\ntant. Figure 7 (left) shows CIFAR-10 samples generated\nModel\nNLL Test\nDBM 2hl [1]:\n\u224884.62\nDBN 2hl [2]:\n\u224884.55\nNADE [3]:\n88.33\nEoNADE 2hl (128 orderings) [3]:\n85.10\nEoNADE-5 2hl (128 orderings) [4]:\n84.68\nDLGM [5]:\n\u224886.60\nDLGM 8 leapfrog steps [6]:\n\u224885.51\nDARN 1hl [7]:\n\u224884.13\nMADE 2hl (32 masks) [8]:\n86.64\nDRAW [9]:\n\u226480.97\nPixelCNN:\n81.30\nRow LSTM:\n80.54\nDiagonal BiLSTM (1 layer, h = 32):\n80.75\nDiagonal BiLSTM (7 layers, h = 16):\n79.20\nTable 4. Test set performance of different models on MNIST\nin nats (negative log-likelihood).\nPrior results taken from [1]\n(Salakhutdinov & Hinton, 2009), [2] (Murray & Salakhutdinov,\n2009), [3] (Uria et al., 2014), [4] (Raiko et al., 2014), [5] (Rezende\net al., 2014), [6] (Salimans et al., 2015), [7] (Gregor et al., 2014),\n[8] (Germain et al., 2015), [9] (Gregor et al., 2015).\nfrom the Diagonal BiLSTM.\n5.7. ImageNet\nAlthough to our knowledge the are no published results on\nthe ILSVRC ImageNet dataset (Russakovsky et al., 2015)\nthat we can compare our models with, we give our Ima-\nPixel Recurrent Neural Networks\nFigure 8. Samples from models trained on ImageNet 64x64 images. Left: normal model, right: multi-scale model. The single-scale\nmodel trained on 64x64 images is less able to capture global structure than the 32x32 model. The multi-scale model seems to resolve\nthis problem. Although these models get similar performance in log-likelihood, the samples on the right do seem globally more coherent.\nModel\nNLL Test (Train)\nUniform Distribution:\n8.00\nMultivariate Gaussian:\n4.70\nNICE [1]:\n4.48\nDeep Diffusion [2]:\n4.20\nDeep GMMs [3]:\n4.00\nRIDE [4]:\n3.47\nPixelCNN:\n3.14 (3.08)\nRow LSTM:\n3.07 (3.00)\nDiagonal BiLSTM:\n3.00 (2.93)\nTable 5. Test set performance of different models on CIFAR-10 in\nbits/dim. For our models we give training performance in brack-\nets. [1] (Dinh et al., 2014), [2] (Sohl-Dickstein et al., 2015), [3]\n(van den Oord & Schrauwen, 2014a), [4] personal communication\n(Theis & Bethge, 2015).\nImage size\nNLL Validation (Train)\n32x32:\n3.86 (3.83)\n64x64:\n3.63 (3.57)\nTable 6. Negative log-likelihood performance on 32\u00d732 and 64\u00d7\n64 ImageNet in bits/dim.\ngeNet log-likelihood performance in Table 6 (without data\naugmentation). On ImageNet the current PixelRNNs do\nnot appear to over\ufb01t, as we saw that their validation per-\nformance improved with size and depth. The main con-\nstraint on model size are currently computation time and\nGPU memory.\nNote that the ImageNet models are in general less com-\npressible than the CIFAR-10 images. ImageNet has greater\nvariety of images, and the CIFAR-10 images were most\noccluded\ncompletions\noriginal\nFigure 9. Image completions sampled from a model that was\ntrained on 32x32 ImageNet images. Note that diversity of the\ncompletions is high, which can be attributed to the log-likelihood\nloss function used in this generative model, as it encourages mod-\nels with high entropy. As these are sampled from the model, we\ncan easily generate millions of different completions. It is also\ninteresting to see that textures such as water, wood and shrubbery\nare also inputed relative well (see Figure 1).\nlikely resized with a different algorithm than the one we\nused for ImageNet images. The ImageNet images are less\nblurry, which means neighboring pixels are less correlated\nto each other and thus less predictable. Because the down-\nsampling method can in\ufb02uence the compression perfor-\nmance, we have made the used downsampled images avail-\nable1.\nFigure 7 (right) shows 32 \u00d7 32 samples drawn from our\nmodel trained on ImageNet. Figure 8 shows 64 \u00d7 64 sam-\nples from the same model with and without multi-scale\n1http://image-net.org/small/download.php\nPixel Recurrent Neural Networks\nconditioning.\nFinally, we also show image completions\nsampled from the model in Figure 9.\n6. Conclusion\nIn this paper we signi\ufb01cantly improve and build upon deep\nrecurrent neural networks as generative models for natural\nimages. We have described novel two-dimensional LSTM\nlayers: the Row LSTM and the Diagonal BiLSTM, that\nscale more easily to larger datasets.\nThe models were\ntrained to model the raw RGB pixel values. We treated the\npixel values as discrete random variables by using a soft-\nmax layer in the conditional distributions. We employed\nmasked convolutions to allow PixelRNNs to model full de-\npendencies between the color channels. We proposed and\nevaluated architectural improvements in these models re-\nsulting in PixelRNNs with up to 12 LSTM layers.\nWe have shown that the PixelRNNs signi\ufb01cantly improve\nthe state of the art on the MNIST and CIFAR-10 datasets.\nWe also provide new benchmarks for generative image\nmodeling on the ImageNet dataset. Based on the samples\nand completions drawn from the models we can conclude\nthat the PixelRNNs are able to model both spatially local\nand long-range correlations and are able to produce images\nthat are sharp and coherent. Given that these models im-\nprove as we make them larger and that there is practically\nunlimited data available to train on, more computation and\nlarger models are likely to further improve the results.\nAcknowledgements\nThe authors would like to thank Shakir Mohamed and Guil-\nlaume Desjardins for helpful input on this paper and Lu-\ncas Theis, Alex Graves, Karen Simonyan, Lasse Espeholt,\nDanilo Rezende, Karol Gregor and Ivo Danihelka for in-\nsightful discussions.\nReferences\nBengio, Yoshua and Bengio, Samy.\nModeling high-\ndimensional discrete data with multi-layer neural net-\nworks. pp. 400\u2013406. MIT Press, 2000.\nDinh, Laurent, Krueger, David, and Bengio, Yoshua.\nNICE: Non-linear independent components estimation.\narXiv preprint arXiv:1410.8516, 2014.\nGermain, Mathieu, Gregor, Karol, Murray, Iain, and\nLarochelle, Hugo. MADE: Masked autoencoder for dis-\ntribution estimation. arXiv preprint arXiv:1502.03509,\n2015.\nGraves, Alex. Generating sequences with recurrent neural\nnetworks. arXiv preprint arXiv:1308.0850, 2013.\nGraves, Alex and Schmidhuber, J\u00a8urgen. Of\ufb02ine handwrit-\ning recognition with multidimensional recurrent neural\nnetworks. In Advances in Neural Information Process-\ning Systems, 2009.\nGregor, Karol, Danihelka, Ivo, Mnih, Andriy, Blundell,\nCharles, and Wierstra, Daan. Deep autoregressive net-\nworks. In Proceedings of the 31st International Confer-\nence on Machine Learning, 2014.\nGregor, Karol, Danihelka, Ivo, Graves, Alex, and Wierstra,\nDaan.\nDRAW: A recurrent neural network for image\ngeneration. Proceedings of the 32nd International Con-\nference on Machine Learning, 2015.\nHe, Kaiming, Zhang, Xiangyu, Ren, Shaoqing, and Sun,\nJian. Deep residual learning for image recognition. arXiv\npreprint arXiv:1512.03385, 2015.\nHochreiter, Sepp and Schmidhuber, J\u00a8urgen. Long short-\nterm memory. Neural computation, 1997.\nKalchbrenner, Nal and Blunsom, Phil. Recurrent continu-\nous translation models. In Proceedings of the 2013 Con-\nference on Empirical Methods in Natural Language Pro-\ncessing, 2013.\nKalchbrenner, Nal, Danihelka, Ivo, and Graves, Alex.\nGrid\nlong\nshort-term\nmemory.\narXiv\npreprint\narXiv:1507.01526, 2015.\nKingma, Diederik P and Welling, Max.\nAuto-encoding\nvariational bayes.\narXiv preprint arXiv:1312.6114,\n2013.\nKrizhevsky, Alex.\nLearning multiple layers of features\nfrom tiny images. 2009.\nLarochelle, Hugo and Murray, Iain.\nThe neural autore-\ngressive distribution estimator. The Journal of Machine\nLearning Research, 2011.\nLeCun, Yann, Bottou, L\u00b4eon, Bengio, Yoshua, and Haffner,\nPatrick.\nGradient-based learning applied to document\nrecognition. Proceedings of the IEEE, 1998.\nMurray, Iain and Salakhutdinov, Ruslan R.\nEvaluat-\ning probabilities under high-dimensional latent variable\nmodels. In Advances in Neural Information Processing\nSystems, 2009.\nNeal, Radford M.\nConnectionist learning of belief net-\nworks. Arti\ufb01cial intelligence, 1992.\nRaiko, Tapani, Li, Yao, Cho, Kyunghyun, and Bengio,\nYoshua. Iterative neural autoregressive distribution es-\ntimator NADE-k.\nIn Advances in Neural Information\nProcessing Systems, 2014.\nPixel Recurrent Neural Networks\nRezende, Danilo J, Mohamed, Shakir, and Wierstra, Daan.\nStochastic backpropagation and approximate inference\nin deep generative models. In Proceedings of the 31st\nInternational Conference on Machine Learning, 2014.\nRussakovsky, Olga, Deng, Jia, Su, Hao, Krause, Jonathan,\nSatheesh, Sanjeev, Ma, Sean, Huang, Zhiheng, Karpa-\nthy, Andrej, Khosla, Aditya, Bernstein, Michael, Berg,\nAlexander C., and Fei-Fei, Li. ImageNet Large Scale\nVisual Recognition Challenge. International Journal of\nComputer Vision (IJCV), 2015.\nSalakhutdinov, Ruslan and Hinton, Geoffrey E. Deep boltz-\nmann machines. In International Conference on Arti\ufb01-\ncial Intelligence and Statistics, 2009.\nSalakhutdinov, Ruslan and Murray, Iain. On the quantita-\ntive analysis of deep belief networks. In Proceedings of\nthe 25th international conference on Machine learning,\n2008.\nSalimans, Tim, Kingma, Diederik P, and Welling, Max.\nMarkov chain monte carlo and variational inference:\nBridging the gap. Proceedings of the 32nd International\nConference on Machine Learning, 2015.\nSohl-Dickstein, Jascha, Weiss, Eric A., Maheswaranathan,\nNiru, and Ganguli, Surya. Deep unsupervised learning\nusing nonequilibrium thermodynamics. Proceedings of\nthe 32nd International Conference on Machine Learn-\ning, 2015.\nStollenga, Marijn F, Byeon, Wonmin, Liwicki, Marcus,\nand Schmidhuber, Juergen. Parallel multi-dimensional\nlstm, with application to fast biomedical volumetric im-\nage segmentation. In Advances in Neural Information\nProcessing Systems 28. 2015.\nSutskever, Ilya, Martens, James, and Hinton, Geoffrey E.\nGenerating text with recurrent neural networks. In Pro-\nceedings of the 28th International Conference on Ma-\nchine Learning, 2011.\nTheis, Lucas and Bethge, Matthias. Generative image mod-\neling using spatial LSTMs. In Advances in Neural Infor-\nmation Processing Systems, 2015.\nTheis, Lucas, van den Oord, A\u00a8aron, and Bethge, Matthias.\nA note on the evaluation of generative models. arXiv\npreprint arXiv:1511.01844, 2015.\nUria, Benigno, Murray, Iain, and Larochelle, Hugo.\nRNADE: The real-valued neural autoregressive density-\nestimator. In Advances in Neural Information Processing\nSystems, 2013.\nUria, Benigno, Murray, Iain, and Larochelle, Hugo.\nA\ndeep and tractable density estimator. In Proceedings of\nthe 31st International Conference on Machine Learning,\n2014.\nvan den Oord, A\u00a8aron and Schrauwen, Benjamin. Factoring\nvariations in natural images with deep gaussian mixture\nmodels. In Advances in Neural Information Processing\nSystems, 2014a.\nvan den Oord, A\u00a8aron and Schrauwen, Benjamin.\nThe\nstudent-t mixture as a natural image patch prior with ap-\nplication to image compression. The Journal of Machine\nLearning Research, 2014b.\nZhang, Yu, Chen, Guoguo, Yu, Dong, Yao, Kaisheng, Khu-\ndanpur, Sanjeev, and Glass, James. Highway long short-\nterm memory RNNs for distant speech recognition. In\nProceedings of the International Conference on Acous-\ntics, Speech and Signal Processing, 2016.\nPixel Recurrent Neural Networks\nFigure 10. Additional samples from a model trained on ImageNet 32x32 (right) images.\n",
        "sentence": " These same techniques could be used to accelerate image synthesis with PixelCNN (Oord et al., 2016) to fractions of a second per image.",
        "context": "main et al., 2015). Note that the advantage of paralleliza-\ntion of the PixelCNN over the PixelRNN is only available\nduring training or during evaluating of test images. The\nimage generation process is sequential for both kinds of\nPixel Recurrent Neural Networks\nFigure 10. Additional samples from a model trained on ImageNet 32x32 (right) images.\naugmentation). On ImageNet the current PixelRNNs do\nnot appear to over\ufb01t, as we saw that their validation per-\nformance improved with size and depth. The main con-\nstraint on model size are currently computation time and\nGPU memory."
    },
    {
        "title": "Multi-output rnn-lstm for multiple speaker speech synthesis with \u03b1interpolation model",
        "author": [
            "Pascual",
            "Santiago",
            "Bonafonte",
            "Antonio"
        ],
        "venue": null,
        "citeRegEx": "Pascual et al\\.,? \\Q2016\\E",
        "shortCiteRegEx": "Pascual et al\\.",
        "year": 2016,
        "abstract": "",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "The blizzard challenge 2013indian language task",
        "author": [
            "Prahallad",
            "Kishore",
            "Vadapalli",
            "Anandaswarup",
            "Elluru",
            "Naresh"
        ],
        "venue": "In In Blizzard Challenge Workshop",
        "citeRegEx": "Prahallad et al\\.,? \\Q2013\\E",
        "shortCiteRegEx": "Prahallad et al\\.",
        "year": 2013,
        "abstract": "",
        "full_text": "",
        "sentence": " audio synthesis results for our models trained on a subset of the Blizzard 2013 data (Prahallad et al., 2013). To demonstrate the flexibility of our system, we retrained all of our models with identical hyperparameters on the Blizzard 2013 dataset (Prahallad et al., 2013).",
        "context": null
    },
    {
        "title": "Grapheme-to-phoneme conversion using long short-term memory recurrent neural networks",
        "author": [
            "Rao",
            "Kanishka",
            "Peng",
            "Fuchun",
            "Sak",
            "Ha\u015fim",
            "Beaufays",
            "Fran\u00e7oise"
        ],
        "venue": "In Acoustics, Speech and Signal Processing (ICASSP),",
        "citeRegEx": "Rao et al\\.,? \\Q2015\\E",
        "shortCiteRegEx": "Rao et al\\.",
        "year": 2015,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " Previous work uses neural networks as substitutes for several TTS system components, including grapheme-tophoneme conversion models (Rao et al., 2015; Yao & Zweig, 2015), phoneme duration prediction models (Zen & Sak, 2015), fundamental frequency prediction models (Pascual & Bonafonte, 2016; Ronanki et al.",
        "context": null
    },
    {
        "title": "Crowdmos: An approach for crowdsourcing mean opinion score studies",
        "author": [
            "Ribeiro",
            "Fl\u00e1vio",
            "Flor\u00eancio",
            "Dinei",
            "Zhang",
            "Cha",
            "Seltzer",
            "Michael"
        ],
        "venue": "In Acoustics, Speech and Signal Processing (ICASSP),",
        "citeRegEx": "Ribeiro et al\\.,? \\Q2011\\E",
        "shortCiteRegEx": "Ribeiro et al\\.",
        "year": 2011,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " To estimate perceptual quality of the individual stages of our TTS pipeline, we crowdsourced mean opinion score (MOS) ratings (ratings between one and five, higher values being better) from Mechanical Turk using the CrowdMOS toolkit and methodology (Ribeiro et al., 2011).",
        "context": null
    },
    {
        "title": "A template-based approach for speech synthesis intonation generation using lstms",
        "author": [
            "Ronanki",
            "Srikanth",
            "Henter",
            "Gustav Eje",
            "Wu",
            "Zhizheng",
            "King",
            "Simon"
        ],
        "venue": "Interspeech 2016,",
        "citeRegEx": "Ronanki et al\\.,? \\Q2016\\E",
        "shortCiteRegEx": "Ronanki et al\\.",
        "year": 2016,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " , 2015; Yao & Zweig, 2015), phoneme duration prediction models (Zen & Sak, 2015), fundamental frequency prediction models (Pascual & Bonafonte, 2016; Ronanki et al., 2016), and audio synthesis models (van den Oord et al.",
        "context": null
    },
    {
        "title": "Char2wav: End-to-end speech synthesis",
        "author": [
            "Sotelo",
            "Jose",
            "Mehri",
            "Soroush",
            "Kumar",
            "Kundan",
            "Santos",
            "Joao Felipe",
            "Kastner",
            "Kyle",
            "Courville",
            "Aaron",
            "Bengio",
            "Yoshua"
        ],
        "venue": "In ICLR 2017 workshop submission,",
        "citeRegEx": "Sotelo et al\\.,? \\Q2017\\E",
        "shortCiteRegEx": "Sotelo et al\\.",
        "year": 2017,
        "abstract": "",
        "full_text": "",
        "sentence": " Most recently, there has been a lot of work in parametric audio synthesis, notably WaveNet, SampleRNN, and Char2Wav (van den Oord et al., 2016; Mehri et al., 2016; Sotelo et al., 2017).",
        "context": null
    },
    {
        "title": "Production Rendering, Design and Implementation",
        "author": [
            "Stephenson",
            "Ian"
        ],
        "venue": null,
        "citeRegEx": "Stephenson and Ian.,? \\Q2005\\E",
        "shortCiteRegEx": "Stephenson and Ian.",
        "year": 2005,
        "abstract": "",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "Text-to-Speech Synthesis",
        "author": [
            "Taylor",
            "Paul"
        ],
        "venue": "USA, 1st edition,",
        "citeRegEx": "Taylor and Paul.,? \\Q2009\\E",
        "shortCiteRegEx": "Taylor and Paul.",
        "year": 2009,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "A note on the evaluation of generative models",
        "author": [
            "Theis",
            "Lucas",
            "Oord",
            "A\u00e4ron van den",
            "Bethge",
            "Matthias"
        ],
        "venue": "arXiv preprint arXiv:1511.01844,",
        "citeRegEx": "Theis et al\\.,? \\Q2015\\E",
        "shortCiteRegEx": "Theis et al\\.",
        "year": 2015,
        "abstract": "Probabilistic generative models can be used for compression, denoising,\ninpainting, texture synthesis, semi-supervised learning, unsupervised feature\nlearning, and other tasks. Given this wide range of applications, it is not\nsurprising that a lot of heterogeneity exists in the way these models are\nformulated, trained, and evaluated. As a consequence, direct comparison between\nmodels is often difficult. This article reviews mostly known but often\nunderappreciated properties relating to the evaluation and interpretation of\ngenerative models with a focus on image models. In particular, we show that\nthree of the currently most commonly used criteria---average log-likelihood,\nParzen window estimates, and visual fidelity of samples---are largely\nindependent of each other when the data is high-dimensional. Good performance\nwith respect to one criterion therefore need not imply good performance with\nrespect to the other criteria. Our results show that extrapolation from one\ncriterion to another is not warranted and generative models need to be\nevaluated directly with respect to the application(s) they were intended for.\nIn addition, we provide examples demonstrating that Parzen window estimates\nshould generally be avoided.",
        "full_text": "Published as a conference paper at ICLR 2016\nA NOTE ON THE EVALUATION OF GENERATIVE MODELS\nLucas Theis\u2217\nUniversity of T\u00a8ubingen\n72072 T\u00a8ubingen, Germany\nlucas@bethgelab.org\nA\u00a8aron van den Oord\u2217\u2020\nGhent University\n9000 Ghent, Belgium\naaron.vandenoord@ugent.be\nMatthias Bethge\nUniversity of T\u00a8ubingen\n72072 T\u00a8ubingen, Germany\nmatthias@bethgelab.org\nABSTRACT\nProbabilistic generative models can be used for compression, denoising, inpaint-\ning, texture synthesis, semi-supervised learning, unsupervised feature learning,\nand other tasks. Given this wide range of applications, it is not surprising that a\nlot of heterogeneity exists in the way these models are formulated, trained, and\nevaluated. As a consequence, direct comparison between models is often dif-\n\ufb01cult. This article reviews mostly known but often underappreciated properties\nrelating to the evaluation and interpretation of generative models with a focus\non image models. In particular, we show that three of the currently most com-\nmonly used criteria\u2014average log-likelihood, Parzen window estimates, and vi-\nsual \ufb01delity of samples\u2014are largely independent of each other when the data is\nhigh-dimensional. Good performance with respect to one criterion therefore need\nnot imply good performance with respect to the other criteria. Our results show\nthat extrapolation from one criterion to another is not warranted and generative\nmodels need to be evaluated directly with respect to the application(s) they were\nintended for. In addition, we provide examples demonstrating that Parzen window\nestimates should generally be avoided.\n1\nINTRODUCTION\nGenerative models have many applications and can be evaluated in many ways. For density esti-\nmation and related tasks, log-likelihood (or equivalently Kullback-Leibler divergence) has been the\nde-facto standard for training and evaluating generative models. However, the likelihood of many\ninteresting models is computationally intractable. For example, the normalization constant of un-\nnormalized energy-based models is generally dif\ufb01cult to compute, and latent-variable models often\nrequire us to solve complex integrals to compute the likelihood. These models may still be trained\nwith respect to a different objective that is more or less related to log-likelihood, such as contrastive\ndivergence (Hinton, 2002), score matching (Hyv\u00a8arinen, 2005), lower bounds on the log-likelihood\n(Bishop, 2006), noise-contrastive estimation (Gutmann & Hyv\u00a8arinen, 2010), probability \ufb02ow (Sohl-\nDickstein et al., 2011), maximum mean discrepancy (MMD) (Gretton et al., 2007; Li et al., 2015),\nor approximations to the Jensen-Shannon divergence (JSD) (Goodfellow et al., 2014).\nFor computational reasons, generative models are also often compared in terms of properties more\nreadily accessible than likelihood, even when the task is density estimation. Examples include vi-\nsualizations of model samples, interpretations of model parameters (Hyv\u00a8arinen et al., 2009), Parzen\nwindow estimates of the model\u2019s log-likelihood (Breuleux et al., 2009), and evaluations of model\nperformance in surrogate tasks such as denoising or missing value imputation.\nIn this paper, we look at some of the implications of choosing certain training and evaluation criteria.\nWe \ufb01rst show that training objectives such as JSD and MMD can result in very different optima than\n\u2217These authors contributed equally to this work.\n\u2020Now at Google DeepMind.\n1\narXiv:1511.01844v3  [stat.ML]  24 Apr 2016\nPublished as a conference paper at ICLR 2016\nData\nKLD\nMMD\nJSD\nFigure 1: An isotropic Gaussian distribution was \ufb01t to data drawn from a mixture of Gaussians\nby either minimizing Kullback-Leibler divergence (KLD), maximum mean discrepancy (MMD), or\nJensen-Shannon divergence (JSD). The different \ufb01ts demonstrate different tradeoffs made by the\nthree measures of distance between distributions.\nlog-likelihood. We then discuss the relationship between log-likelihood, classi\ufb01cation performance,\nvisual \ufb01delity of samples and Parzen window estimates. We show that good or bad performance with\nrespect to one metric is no guarantee of good or bad performance with respect to the other metrics.\nIn particular, we show that the quality of samples is generally uninformative about the likelihood and\nvice versa, and that Parzen window estimates seem to favor models with neither good likelihood nor\nsamples of highest possible quality. Using Parzen window estimates as a criterion, a simple model\nbased on k-means outperforms the true distribution of the data.\n2\nTRAINING OF GENERATIVE MODELS\nMany objective functions and training procedures have been proposed for optimizing generative\nmodels. The motivation for introducing new training methods is typically the wish to \ufb01t probabilistic\nmodels with computationally intractable likelihoods, rendering direct maximum likelihood learning\nimpractical. Most of the available training procedures are consistent in the sense that if the data\nis drawn from a model distribution, then this model distribution will be optimal under the training\nobjective in the limit of an in\ufb01nite number of training examples. That is, if the model is correct, and\nfor extremely large amounts of data, all of these methods will produce the same result. However,\nwhen there is a mismatch between the data distribution and the model, different objective functions\ncan lead to very different results.\nFigure 1 illustrates this on a simple toy example where an isotropic Gaussian distribution has been \ufb01t\nto a mixture of Gaussians by minimizing various measures of distance. Maximum mean discrepancy\n(MMD) has been used with generative moment matching networks (Li et al., 2015; Dziugaite et al.,\n2015) and Jensen-Shannon divergence (JSD) has connections to the objective function optimized\nby generative adversarial networks (Goodfellow et al., 2014) (see box for a de\ufb01nition). Minimizing\nMMD or JSD yields a Gaussian which \ufb01ts one mode well, but which ignores other parts of the data.\nOn the other hand, maximizing average log-likelihood or equivalently minimizing Kullback-Leibler\ndivergence (KLD) avoids assigning extremely small probability to any data point but assigns a lot\nof probability mass to non-data regions.\nUnderstanding the trade-offs between different measures is important for several reasons. First,\ndifferent applications require different trade-offs, and we want to choose the right metric for a given\napplication. Assigning suf\ufb01cient probability to all plausible images is important for compression, but\nit may be enough to generate a single plausible example in certain image reconstruction applications\n(e.g., Hays & Efros, 2007). Second, a better understanding of the trade-offs allows us to better\ninterpret and relate empirical \ufb01ndings. Generative image models are often assessed based on the\nvisual \ufb01delity of generated samples (e.g., Goodfellow et al., 2014; Gregor et al., 2015; Denton et al.,\n2015; Li et al., 2015). Figure 1 suggests that a model optimized with respect to KLD is more\nlikely to produce atypical samples than the same model optimized with respect to one of the other\ntwo measures. That is, plausible samples\u2014in the sense of having large density under the target\n2\nPublished as a conference paper at ICLR 2016\nMMD (Gretton et al., 2007) is de\ufb01ned as,\nMMD[p, q] = (Ep,q[k(x, x\u2032) \u22122k(x, y) + k(y, y\u2032)])\n1\n2 ,\n(1)\nwhere x, x\u2032 are indepent and distributed according to the data distribution p, and y, y\u2032 are\nindependently distributed according to the model distribution q. We followed the approach\nof Li et al. (2015), optimizing an empirical estimate of MMD and using a mixture of\nGaussian kernels with various bandwidths for k.\nJSD is de\ufb01ned as\nJSD[p, q] = 1\n2KLD[p || m] + 1\n2KLD[q || m],\n(2)\nwhere m = (p+q)/2 is an equal mixture of distributions p and q. We optimized JSD directly\nusing the data density, which is generally not possible in practice where we only have access\nto samples from the data distribution. In this case, generative adversarial networks (GANs)\nmay be used to approximately optimize JSD, although in practical applications the objective\nfunction optimized by GANs can be very different from JSD. Parameters were initialized at\nthe maximum likelihood solution in all cases, but the same optimum was consistently found\nusing random initializations.\ndistribution\u2014are not necessarily an indication of a good density model as measured by KLD, but\nmay be expected when optimizing JSD.\n3\nEVALUATION OF GENERATIVE MODELS\nJust as choosing the right training method is important for achieving good performance in a given\napplication, so is choosing the right evaluation metric for drawing the right conclusions. In the\nfollowing, we \ufb01rst continue to discuss the relationship between average log-likelihood and the visual\nappearance of model samples.\nModel samples can be a useful diagnostic tool, often allowing us to build an intuition for why a\nmodel might fail and how it could be improved. However, qualitative as well as quantitative analyses\nbased on model samples can be misleading about a model\u2019s density estimation performance, as well\nas the probabilistic model\u2019s performance in applications other than image synthesis. Below we\nsummarize a few examples demonstrating this.\n3.1\nLOG-LIKELIHOOD\nAverage log-likelihood is widely considered as the default measure for quantifying generative image\nmodeling performance. However, care needs to be taken to ensure that the numbers measured are\nmeaningful. While natural images are typically stored using 8-bit integers, they are often modeled\nusing densities, i.e., an image is treated as an instance of a continuous random variable. Since the\ndiscrete data distribution has differential entropy of negative in\ufb01nity, this can lead to arbitrary high\nlikelihoods even on test data. To avoid this case, it is becoming best practice to add real-valued noise\nto the integer pixel values to dequantize the data (e.g., Uria et al., 2013; van den Oord & Schrauwen,\n2014; Theis & Bethge, 2015).\nIf we add the right amount of uniform noise, the log-likelihood of the continuous model on the\ndequantized data is closely related to the log-likelihood of a discrete model on the discrete data.\nMaximizing the log-likelihood on the continuous data also optimizes the log-likelihood of the dis-\ncrete model on the original data. This can be seen as follows.\nConsider images x \u2208{0, ..., 255}D with a discrete probability distribution P(x), uniform noise\nu \u2208[0, 1[D, and noisy data y = x + u. If p refers to the noisy data density and q refers to the model\ndensity, then we have for the average log-likelihood:\n3\nPublished as a conference paper at ICLR 2016\nZ\np(y) log q(y) dy =\nX\nx\nP(x)\nZ\n[0,1[D log q(x + u) du\n(3)\n\u2264\nX\nx\nP(x) log\nZ\n[0,1[D q(x + u) du\n(4)\n=\nX\nx\nP(x) log Q(x),\n(5)\nwhere the second step follows from Jensen\u2019s inequality and we have de\ufb01ned\nQ(x) =\nZ\n[0,1[D q(x + u) du\n(6)\nfor x \u2208ZD. The left-hand side in Equation 3 is the expected log-likelihood which would be es-\ntimated in a typical benchmark. The right-hand side is the log-likelihood of the probability mass\nfunction Q on the original discrete-valued image data. The negative of this log-likelihood is equiv-\nalent to the average number of bits (assuming base-2 logarithm) required to losslessly compress the\ndiscrete data with an entropy coding scheme optimized for Q (Shannon, 2001).\nSEMI-SUPERVISED LEARNING\nA second motivation for using log-likelihood comes from semi-supervised learning. Consider a\ndataset consisting of images X and corresponding labels Y for some but not necessarily all of the\nimages. In classi\ufb01cation, we are interested in the prediction of a class label y for a previously\nunseen query image x. For a given model relating x, y, and parameters \u03b8, the only correct way to\ninfer the distribution over y\u2014from a Bayesian point of view \u2014is to integrate out the parameters\n(e.g., Lasserre et al., 2006),\np(y | x, X, Y) =\nZ\np(\u03b8 | X, Y)p(y | x, \u03b8) d\u03b8.\n(7)\nWith suf\ufb01cient data and under certain assumptions, the above integral will be close to p(y | x, \u02c6\u03b8MAP),\nwhere\n\u02c6\u03b8MAP = argmax\u03b8 p(\u03b8 | X, Y)\n(8)\n= argmax\u03b8 [log p(\u03b8) + log p(X | \u03b8) + log p(Y | X, \u03b8)] .\n(9)\nWhen no training labels are given, i.e., in the unsupervised setting, and for a uniform prior over\nparameters, it is therefore natural to try to optimize the log-likelihood, log p(X | \u03b8).\nIn practice, this approach might fail because of a mismatch between the model and the data, because\nof an inability to solve Equation 9, or because of over\ufb01tting induced by the MAP approximation.\nThese issues can be addressed by better image models (e.g., Kingma et al., 2014), better optimization\nand inference procedures, or a more Bayesian treatment of the parameters (e.g., Lacoste-Julien et al.,\n2011; Welling & Teh, 2011).\n3.2\nSAMPLES AND LOG-LIKELIHOOD\nFor many interesting models, average log-likelihood is dif\ufb01cult to evaluate or even approximate. For\nsome of these models at least, generating samples is a lot easier. It would therefore be useful if we\ncould use generated samples to infer something about a model\u2019s log-likelihood. This approach is\nalso intuitive given that a model with zero KL divergence will produce perfect samples, and visual\ninspection can work well in low dimensions for assessing a model\u2019s \ufb01t to data. Unfortunately these\nintuitions can be misleading when the image dimensionality is high. A model can have poor log-\nlikelihood and produce great samples, or have great log-likelihood and produce poor samples.\nPOOR LOG-LIKELIHOOD AND GREAT SAMPLES\nA simple lookup table storing enough training images will generate convincing looking images but\nwill have poor average log-likelihood on unseen test data. Somewhat more generally we might\n4\nPublished as a conference paper at ICLR 2016\nconsider a mixture of Gaussian distributions,\nq(x) = 1\nN\nX\nn\nN(x; xn, \u03b52I),\n(10)\nwhere the means xn are either training images or a number of plausible images derived from the\ntraining set (e.g., using a set of image transformations). If \u03b5 is small enough such that the Gaussian\nnoise becomes imperceptible, this model will generate great samples but will still have very poor\nlog-likelihood. This shows that plausible samples are clearly not suf\ufb01cient for a good log-likelihood.\nGerhard et al. (2013) empirically found a correlation between some models\u2019 log-likelihoods and\ntheir samples\u2019 ability to fool human observers into thinking they were extracted from real images.\nHowever, the image patches were small and all models used in the study were optimized to mini-\nmize KLD. The correlation between log-likelihood and sample quality may disappear, for example,\nwhen considering models optimized for different objective functions or already when considering a\ndifferent set of models.\nGREAT LOG-LIKELIHOOD AND POOR SAMPLES\nPerhaps surprisingly, the ability to produce plausible samples is not only not suf\ufb01cient, but also\nnot necessary for high likelihood as a simple argument by van den Oord & Dambre (2015) shows:\nAssume p is the density of a model for d dimensional data x which performs arbitrarily well with\nrespect to average log-likelihood and q corresponds to some bad model (e.g., white noise). Then\nsamples generated by the mixture model\n0.01p(x) + 0.99q(x)\n(11)\nwill come from the poor model 99% of the time. Yet the log-likelihood per pixel will hardly change\nif d is large:\nlog [0.01p(x) + 0.99q(x)] \u2265log [0.01p(x)] = log p(x) \u2212log 100\n(12)\nFor high-dimensional data, log p(x) will be proportional to d while log 100 stays constant. For\ninstance, already for the 32 by 32 images found in the CIFAR-10 dataset the difference between\nlog-likelihoods of different models can be in the thousands, while log(100) is only about 4.61 nats\n(van den Oord & Dambre, 2015). This shows that a model can have large average log-likelihood but\ngenerate very poor samples.\nGOOD LOG-LIKELIHOOD AND GREAT SAMPLES\nNote that we could have also chosen q (Equation 11) such that it reproduces training examples, e.g.,\nby choosing q as in Equation 10. In this case, the mixture model would generate samples indistin-\nguishable from real images 99% of the time while the log-likelihood would again only change by\nat most 4.61 nats. This shows that any model can be turned into a model which produces realistic\nsamples at little expense to its log-likelihood. Log-likelihood and visual appearance of samples are\ntherefore largely independent.\n3.3\nSAMPLES AND APPLICATIONS\nOne might conclude that something must be wrong with log-likelihood if it does not care about a\nmodel\u2019s ability to generate plausible samples. However, note that the mixture model in Equation 11\nmight also still work very well in applications. While q is much more likely a priori, p is going\nto be much more likely a posteriori in tasks like inpainting, denoising, or classi\ufb01cation. Consider\nprediction of a quantity y representing, for example, a class label or missing pixels. A model with\njoint distribution\n0.01p(x)p(y | x) + 0.99q(x)q(y | x)\n(13)\nmay again generate poor samples 99% of the time. For a given \ufb01xed x, the posterior over y will be\na mixture\n\u03b1p(y | x) + (1 \u2212\u03b1)q(y | x),\n(14)\n5\nPublished as a conference paper at ICLR 2016\nA\nB\n0\n1\n2\n3\n4\n0\n20\n40\n60\n80\n100\nShift [pixels]\nPrecision [%]\n0\n1\n2\n3\n4\n0\n2,000\n4,000\n6,000\nShift [pixels]\nEuclidean distance\n0\n1\n2\n3\n4\n0\n2,000\n4,000\n6,000\nShift [pixels]\nFigure 2: A: Two examples demonstrating that small changes of an image can lead to large changes\nin Euclidean distance affecting the choice of nearest neighbor. The images shown represent the\nquery image shifted by between 1 and 4 pixels (left column, top to bottom), and the corresponding\nnearest neighbor from the training set (right column). The gray lines indicate Euclidean distance of\nthe query image to 100 randomly picked images from the training set. B: Fraction of query images\nassigned to the correct training image. The average was estimated from 1,000 images. Dashed lines\nindicate a 90% con\ufb01dence interval.\nwhere a few simple calculations show that\n\u03b1 = \u03c3 (ln p(x) \u2212ln q(x) \u2212ln 99)\n(15)\nand \u03c3 is the sigmoidal logistic function. Since we assume that p is a good model, q is a poor model,\nand x is high-dimensional, we have\nln p(x) \u226bln q(x) + ln 99\n(16)\nand therefore \u03b1 \u22481. That is, mixing with q has hardly changed the posterior over y. While the\nsamples are dominated by q, the classi\ufb01cation performance is dominated by p. This shows that high\nvisual \ufb01delity of samples is generally not necessary for achieving good performance in applications.\n3.4\nEVALUATION BASED ON SAMPLES AND NEAREST NEIGHBORS\nA qualitative assessment based on samples can be biased towards models which over\ufb01t (Breuleux\net al., 2009). To detect over\ufb01tting to the training data, it is common to show samples next to nearest\nneighbors from the training set. In the following, we highlight two limitations of this approach and\nargue that it is un\ufb01t to detect any but the starkest forms of over\ufb01tting.\nNearest neighbors are typically determined based on Euclidean distance. But already perceptually\nsmall changes can lead to large changes in Euclidean distance, as is well known in the psychophysics\nliterature (e.g., Wang & Bovik, 2009). To illustrate this property, we used the top-left 28 by 28 pixels\nof each image from the 50,000 training images of the CIFAR-10 dataset. We then shifted this 28\nby 28 window one pixel down and one pixel to the right and extracted another set of images. We\nrepeated this 4 times, giving us 4 sets of images which are increasingly different from the training\nset. Figure 2A shows nearest neighbors of corresponding images from the query set. Although the\nimages have hardly changed visually, a shift by only two pixels already caused a different nearest\nneighbor. The plot also shows Euclidean distances to 100 randomly picked images from the training\nset. Note that with a bigger dataset, a switch to a different nearest neighbor becomes more likely.\nFigure 2B shows the fraction of query images assigned to the correct training image in our example.\nA model which stores transformed training images can trivially pass the nearest-neighbor over\ufb01tting\ntest. This problem can be alleviated by choosing nearest neighbors based on perceptual metrics, and\nby showing more than one nearest neighbor.\nA second problem concerns the entropy of the model distribution and is harder to address. There\nare different ways a model can over\ufb01t. Even when over\ufb01tting, most models will not reproduce\nperfect or trivially transformed copies of the training data. In this case, no distance metric will\n\ufb01nd a close match in the training set. A model which over\ufb01ts might still never generate a plausible\nimage or might only be able to generate a small fraction of all plausible images (e.g., a model\nas in Equation 10 where instead of training images we store several transformed versions of the\n6\nPublished as a conference paper at ICLR 2016\n101 102 103 104 105 106 107\n0\n40\n80\n120\n160\n200\n240\nNumber of samples\nLog-likelihood [nat]\nLog-likelihood\nEstimate\nFigure 3: Parzen window estimates for a Gaus-\nsian evaluated on 6 by 6 pixel image patches\nfrom the CIFAR-10 dataset.\nEven for small\npatches and a very large number of samples, the\nParzen window estimate is far from the true log-\nlikelihood.\nModel\nParzen est. [nat]\nStacked CAE\n121\nDBN\n138\nGMMN\n147\nDeep GSN\n214\nDiffusion\n220\nGAN\n225\nTrue distribution\n243\nGMMN + AE\n282\nk-means\n313\nTable 1:\nUsing Parzen window estimates to\nevaluate various models trained on MNIST,\nsamples from the true distribution perform\nworse than samples from a simple model trained\nwith k-means.\ntraining images, or a model which only describes data in a lower-dimensional subspace). Because\nthe number of images we can process is vanishingly small compared to the vast number of possible\nimages, we would not be able to detect this by looking at samples from the model.\n3.5\nEVALUATION BASED ON PARZEN WINDOW ESTIMATES\nWhen log-likelihoods are unavailable, a common alternative is to use Parzen window estimates.\nHere, samples are generated from the model and used to construct a tractable model, typically a\nkernel density estimator with Gaussian kernel. A test log-likelihood is then evaluated under this\nmodel and used as a proxy for the true model\u2019s log-likelihood (Breuleux et al., 2009). Breuleux\net al. (2009) suggested to \ufb01t the Parzen windows on both samples and training data, and to use\nat least as many samples as there are images in the training set. Following Bengio et al. (2013a),\nParzen windows are in practice commonly \ufb01t to only 10,000 samples (e.g., Bengio et al., 2013b;\nGoodfellow et al., 2014; Li et al., 2015; Sohl-Dickstein et al., 2015). But even for a large number\nof samples Parzen window estimates generally do not come close to a model\u2019s true log-likelihood\nwhen the data dimensionality is high. In Figure 3 we plot Parzen window estimates for a multivariate\nGaussian distribution \ufb01t to small CIFAR-10 image patches (of size 6 by 6). We added uniform noise\nto the data (as explained in Section 3.1) and rescaled between 0 and 1. As we can see, a completely\ninfeasible number of samples would be needed to get close to the actual log-likelihood even for this\nsmall scale example. For higher dimensional data this effect would only be more pronounced.\nWhile the Parzen window estimate may be far removed from a model\u2019s true log-likelihood, one could\nstill hope that it produces a similar or otherwise useful ranking when applied to different models.\nCounter to this idea, Parzen window estimates of the likelihood have been observed to produce rank-\nings different from other estimates (Bachman & Precup, 2015). More worryingly, a GMMN+AE\n(Li et al., 2015) is assigned a higher score than images from the training set (which are samples\nfrom the true distribution) when evaluated on MNIST (Table 1). Furthermore it is relatively easy to\nexploit the Parzen window loss function to achieve even better results. To illustrate this, we \ufb01tted\n10,000 centroids to the training data using k-means. We then generated 10,000 independent samples\nby sampling centroids with replacement. Note that this corresponds to the model in Equation 10,\nwhere the standard deviation of the Gaussian noise is zero and instead of training examples we use\nthe centroids. We \ufb01nd that samples from this k-means based model are assigned a higher score than\nany other model, while its actual log-likelihood would be \u2212\u221e.\n7\nPublished as a conference paper at ICLR 2016\n4\nCONCLUSION\nWe have discussed the optimization and evaluation of generative image models. Different metrics\ncan lead to different trade-offs, and different evaluations favor different models. It is therefore\nimportant that training and evaluation match the target application. Furthermore, we should be\ncautious not to take good performance in one application as evidence of good performance in another\napplication.\nAn evaluation based on samples is biased towards models which over\ufb01t and therefore a poor indi-\ncator of a good density model in a log-likelihood sense, which favors models with large entropy.\nConversely, a high likelihood does not guarantee visually pleasing samples. Samples can take on\narbitrary form only a few bits from the optimum. It is therefore unsurprising that other approaches\nthan density estimation are much more effective for image synthesis (Portilla & Simoncelli, 2000;\nDosovitskiy et al., 2015; Gatys et al., 2015). Samples are in general also an unreliable proxy for a\nmodel\u2019s performance in applications such as classi\ufb01cation or inpainting, as discussed in Section 3.3.\nA subjective evaluation based on visual \ufb01delity of samples is still clearly appropriate when the\ngoal is image synthesis. Such an analysis at least has the property that the data distribution will\nperform very well in this task. This cannot be said about Parzen window estimates, where the data\ndistribution performs worse than much less desirable models1. We therefore argue Parzen window\nestimates should be avoided for evaluating generative models, unless the application speci\ufb01cally\nrequires such a loss function. In this case, we have shown that a k-means based model can perform\nbetter than the true density. To summarize, our results demonstrate that for generative models there\nis no one-\ufb01ts-all loss function but a proper assessment of model performance is only possible in the\nthe context of an application.\nACKNOWLEDGMENTS\nThe authors would like to thank Jascha Sohl-Dickstein, Ivo Danihelka, Andriy Mnih, and Leon\nGatys for their valuable input on this manuscript.\nREFERENCES\nBachman, P. and Precup, D. Variational Generative Stochastic Networks with Collaborative Shap-\ning. Proceedings of the 32nd International Conference on Machine Learning, pp. 1964\u20131972,\n2015.\nBengio, Y., Mesnil, G., Dauphin, Y., and Rifai, S. Better mixing via deep representations. In\nProceedings of the 30th International Conference on Machine Learning, 2013a.\nBengio, Y., Thibodeau-Laufer, E., Alain, G., and Yosinski, J. Deep generative stochastic networks\ntrainable by backprop, 2013b. arXiv:1306.1091.\nBishop, C. M. Pattern Recognition and Machine Learning. Springer, 2006.\nBreuleux, O., Bengio, Y., and Vincent, P. Unlearning for better mixing. Technical report, Universite\nde Montreal, 2009.\nDenton, E., Chintala, S., Szlam, A., and Fergus, R. Deep Generative Image Models using a Lapla-\ncian Pyramid of Adversarial Networks. arXiv.org, 2015.\nDosovitskiy, A., Springenberg, J. T., and Brox, T. Learning to Generate Chairs with Convolutional\nNeural Networks. In IEEE International Conference on Computer Vision and Pattern Recogni-\ntion, 2015.\nDziugaite, G. K., Roy, D. M., and Ghahramani, Z. Training generative neural networks via maxi-\nmum mean discrepancy optimization, 2015. arXiv:1505.0390.\nGatys, L. A., Ecker, A. S., and Bethge, M. Texture synthesis and the controlled generation of natural\nstimuli using convolutional neural networks, 2015. arXiv:1505.07376.\n1In decision theory, such a metric is called an improper scoring function.\n8\nPublished as a conference paper at ICLR 2016\nGerhard, H. E., Wichmann, F. A., and Bethge, M. How sensitive is the human visual system to the\nlocal statistics of natural images? PLoS Computational Biology, 9(1), 2013.\nGoodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., and\nBengio, Y. Generative adversarial nets. In Advances in Neural Information Processing Systems\n27, 2014.\nGregor, K., Danihelka, I., Graves, A., and Wierstra, D. DRAW: A recurrent neural network for\nimage generation. In Proceedings of the 32nd International Conference on Machine Learning,\n2015.\nGretton, A., Borgwardt, K. M., Rasch, M., Sch\u00a8olkopf, B., and Smola, A. J. A kernel method for the\ntwo-sample-problem. In Advances in Neural Information Processing Systems 20, 2007.\nGutmann, M. and Hyv\u00a8arinen, A.\nNoise-contrastive estimation: A new estimation principle for\nunnormalized statistical models. In Proceedings of the 13th International Conference on Arti\ufb01cial\nIntelligence and Statistics, 2010.\nHays, J. and Efros, A. A. Scene completion using millions of photographs. ACM Transactions on\nGraphics (SIGGRAPH), 26, 2007.\nHinton, G. E. Training Products of Experts by Minimizing Contrastive Divergence. Neural Compu-\ntation, 14(8):1771\u20131800, 2002.\nHyv\u00a8arinen, A., Hurri, J., and Hoyer, P. O. Natural Image Statistics: A Probabilistic Approach to\nEarly Computational Vision. Springer, 2009.\nHyv\u00a8arinen, A. Estimation of non-normalized statistical models using score matching. Journal of\nMachine Learning Research, pp. 695\u2013709, 2005.\nKingma, D. P., Rezende, D. J., Mohamed, S., and Welling, M. Semi-supervised learning with deep\ngenerative models. In Advances in Neural Information Processing Systems 27, 2014.\nLacoste-Julien, S., Huszar, F., and Ghahramani, Z. Approximate inference for the loss-calibrated\nBayesian. In Proceedings of the 14th International Conference on Arti\ufb01cial Intelligence and\nStatistics, 2011.\nLasserre, J. A., Bishop, C. M., and Minka, T. P. Principled hybrids of generative and discriminative\nmodels. In Proceedings of the Computer Vision and Pattern Recognition Conference, 2006.\nLi, Y., Swersky, K., and Zemel, R. Generative moment matching networks. In Proceedings of the\n32nd International Conference on Machine Learning, 2015.\nPortilla, J. and Simoncelli, E. P. A parametric texture model based on joint statistics of complex\nwavelet coef\ufb01cients. International Journal of Computer Vision, 40:49\u201370, 2000.\nShannon, C. E. A mathematical theory of communication. ACM SIGMOBILE Mobile Computing\nand Communications Review, 5(1):3\u201355, 2001.\nSohl-Dickstein, J., Battaglino, P., and DeWeese, M. R. Minimum Probability Flow Learning. In\nProceedings of the 28th International Conference on Machine Learning, 2011.\nSohl-Dickstein, J., Weiss, E. A., Maheswaranathan, N., and Ganguli, S. Deep unsupervised learning\nusing nonequilibrium thermodynamics. In Proceedings of the 32nd International Conference on\nMachine Learning, 2015.\nTheis, L. and Bethge, M. Generative Image Modeling Using Spatial LSTMs. In Advances in Neural\nInformation Processing Systems 28, 2015.\nUria, B., Murray, I., and Larochelle, H. RNADE: The real-valued neural autoregressive density-\nestimator. In Advances in Neural Information Processing Systems 26, 2013.\nvan den Oord, A. and Dambre, J. Locally-connected transformations for deep GMMs, 2015. Deep\nLearning Workshop, ICML.\n9\nPublished as a conference paper at ICLR 2016\nvan den Oord, A. and Schrauwen, B. Factoring Variations in Natural Images with Deep Gaussian\nMixture Models. In Advances in Neural Information Processing Systems 27, 2014.\nWang, Z. and Bovik, A. C. Mean squared error: Love it or leave it?\nIEEE Signal Processing\nMagazine, 2009.\nWelling, M. and Teh, Y. W. Bayesian Learning via Stochastic Gradient Langevin Dynamics. In\nProceedings of the 28th International Conference on Machine Learning, 2011.\n10\n",
        "sentence": " As is common with high-dimensional generative models (Theis et al., 2015), model loss is somewhat uncorrelated with perceptual quality of individual samples.",
        "context": "Dosovitskiy et al., 2015; Gatys et al., 2015). Samples are in general also an unreliable proxy for a\nmodel\u2019s performance in applications such as classi\ufb01cation or inpainting, as discussed in Section 3.3.\nbetter than the true density. To summarize, our results demonstrate that for generative models there\nis no one-\ufb01ts-all loss function but a proper assessment of model performance is only possible in the\nthe context of an application.\nACKNOWLEDGMENTS\ndifferent set of models.\nGREAT LOG-LIKELIHOOD AND POOR SAMPLES\nPerhaps surprisingly, the ability to produce plausible samples is not only not suf\ufb01cient, but also\nnot necessary for high likelihood as a simple argument by van den Oord & Dambre (2015) shows:"
    },
    {
        "title": "Wavenet: A generative model for raw audio",
        "author": [
            "van den Oord",
            "A\u00e4ron",
            "Dieleman",
            "Sander",
            "Zen",
            "Heiga",
            "Simonyan",
            "Karen",
            "Vinyals",
            "Oriol",
            "Graves",
            "Alex",
            "Kalchbrenner",
            "Nal",
            "Senior",
            "Andrew",
            "Kavukcuoglu",
            "Koray"
        ],
        "venue": "CoRR abs/1609.03499,",
        "citeRegEx": "Oord et al\\.,? \\Q2016\\E",
        "shortCiteRegEx": "Oord et al\\.",
        "year": 2016,
        "abstract": "This paper introduces WaveNet, a deep neural network for generating raw audio\nwaveforms. The model is fully probabilistic and autoregressive, with the\npredictive distribution for each audio sample conditioned on all previous ones;\nnonetheless we show that it can be efficiently trained on data with tens of\nthousands of samples per second of audio. When applied to text-to-speech, it\nyields state-of-the-art performance, with human listeners rating it as\nsignificantly more natural sounding than the best parametric and concatenative\nsystems for both English and Mandarin. A single WaveNet can capture the\ncharacteristics of many different speakers with equal fidelity, and can switch\nbetween them by conditioning on the speaker identity. When trained to model\nmusic, we find that it generates novel and often highly realistic musical\nfragments. We also show that it can be employed as a discriminative model,\nreturning promising results for phoneme recognition.",
        "full_text": "WAVENET: A GENERATIVE MODEL FOR RAW AUDIO\nA\u00a8aron van den Oord\nSander Dieleman\nHeiga Zen\u2020\nKaren Simonyan\nOriol Vinyals\nAlex Graves\nNal Kalchbrenner\nAndrew Senior\nKoray Kavukcuoglu\n{avdnoord, sedielem, heigazen, simonyan, vinyals, gravesa, nalk, andrewsenior, korayk}@google.com\nGoogle DeepMind, London, UK\n\u2020 Google, London, UK\nABSTRACT\nThis paper introduces WaveNet, a deep neural network for generating raw audio\nwaveforms. The model is fully probabilistic and autoregressive, with the predic-\ntive distribution for each audio sample conditioned on all previous ones; nonethe-\nless we show that it can be ef\ufb01ciently trained on data with tens of thousands of\nsamples per second of audio. When applied to text-to-speech, it yields state-of-\nthe-art performance, with human listeners rating it as signi\ufb01cantly more natural\nsounding than the best parametric and concatenative systems for both English and\nMandarin. A single WaveNet can capture the characteristics of many different\nspeakers with equal \ufb01delity, and can switch between them by conditioning on the\nspeaker identity. When trained to model music, we \ufb01nd that it generates novel and\noften highly realistic musical fragments. We also show that it can be employed as\na discriminative model, returning promising results for phoneme recognition.\n1\nINTRODUCTION\nThis work explores raw audio generation techniques, inspired by recent advances in neural autore-\ngressive generative models that model complex distributions such as images (van den Oord et al.,\n2016a;b) and text (J\u00b4ozefowicz et al., 2016). Modeling joint probabilities over pixels or words using\nneural architectures as products of conditional distributions yields state-of-the-art generation.\nRemarkably, these architectures are able to model distributions over thousands of random variables\n(e.g. 64\u00d764 pixels as in PixelRNN (van den Oord et al., 2016a)). The question this paper addresses\nis whether similar approaches can succeed in generating wideband raw audio waveforms, which are\nsignals with very high temporal resolution, at least 16,000 samples per second (see Fig. 1).\nFigure 1: A second of generated speech.\nThis paper introduces WaveNet, an audio generative model based on the PixelCNN (van den Oord\net al., 2016a;b) architecture. The main contributions of this work are as follows:\n\u2022 We show that WaveNets can generate raw speech signals with subjective naturalness never\nbefore reported in the \ufb01eld of text-to-speech (TTS), as assessed by human raters.\n1\narXiv:1609.03499v2  [cs.SD]  19 Sep 2016\n\u2022 In order to deal with long-range temporal dependencies needed for raw audio generation,\nwe develop new architectures based on dilated causal convolutions, which exhibit very\nlarge receptive \ufb01elds.\n\u2022 We show that when conditioned on a speaker identity, a single model can be used to gener-\nate different voices.\n\u2022 The same architecture shows strong results when tested on a small speech recognition\ndataset, and is promising when used to generate other audio modalities such as music.\nWe believe that WaveNets provide a generic and \ufb02exible framework for tackling many applications\nthat rely on audio generation (e.g. TTS, music, speech enhancement, voice conversion, source sep-\naration).\n2\nWAVENET\nIn this paper we introduce a new generative model operating directly on the raw audio waveform.\nThe joint probability of a waveform x = {x1, . . . , xT } is factorised as a product of conditional\nprobabilities as follows:\np (x) =\nT\nY\nt=1\np (xt | x1, . . . , xt\u22121)\n(1)\nEach audio sample xt is therefore conditioned on the samples at all previous timesteps.\nSimilarly to PixelCNNs (van den Oord et al., 2016a;b), the conditional probability distribution is\nmodelled by a stack of convolutional layers. There are no pooling layers in the network, and the\noutput of the model has the same time dimensionality as the input. The model outputs a categorical\ndistribution over the next value xt with a softmax layer and it is optimized to maximize the log-\nlikelihood of the data w.r.t. the parameters. Because log-likelihoods are tractable, we tune hyper-\nparameters on a validation set and can easily measure if the model is over\ufb01tting or under\ufb01tting.\n2.1\nDILATED CAUSAL CONVOLUTIONS\nInput\nHidden Layer\nHidden Layer\nHidden Layer\nOutput\nFigure 2: Visualization of a stack of causal convolutional layers.\nThe main ingredient of WaveNet are causal convolutions.\nBy using causal convolutions, we\nmake sure the model cannot violate the ordering in which we model the data: the prediction\np (xt+1 | x1, ..., xt) emitted by the model at timestep t cannot depend on any of the future timesteps\nxt+1, xt+2, . . . , xT as shown in Fig. 2. For images, the equivalent of a causal convolution is a\nmasked convolution (van den Oord et al., 2016a) which can be implemented by constructing a mask\ntensor and doing an elementwise multiplication of this mask with the convolution kernel before ap-\nplying it. For 1-D data such as audio one can more easily implement this by shifting the output of a\nnormal convolution by a few timesteps.\nAt training time, the conditional predictions for all timesteps can be made in parallel because all\ntimesteps of ground truth x are known. When generating with the model, the predictions are se-\nquential: after each sample is predicted, it is fed back into the network to predict the next sample.\n2\nBecause models with causal convolutions do not have recurrent connections, they are typically faster\nto train than RNNs, especially when applied to very long sequences. One of the problems of causal\nconvolutions is that they require many layers, or large \ufb01lters to increase the receptive \ufb01eld. For\nexample, in Fig. 2 the receptive \ufb01eld is only 5 (= #layers + \ufb01lter length - 1). In this paper we use\ndilated convolutions to increase the receptive \ufb01eld by orders of magnitude, without greatly increasing\ncomputational cost.\nA dilated convolution (also called `a trous, or convolution with holes) is a convolution where the\n\ufb01lter is applied over an area larger than its length by skipping input values with a certain step. It is\nequivalent to a convolution with a larger \ufb01lter derived from the original \ufb01lter by dilating it with zeros,\nbut is signi\ufb01cantly more ef\ufb01cient. A dilated convolution effectively allows the network to operate on\na coarser scale than with a normal convolution. This is similar to pooling or strided convolutions, but\nhere the output has the same size as the input. As a special case, dilated convolution with dilation\n1 yields the standard convolution. Fig. 3 depicts dilated causal convolutions for dilations 1, 2, 4,\nand 8. Dilated convolutions have previously been used in various contexts, e.g. signal processing\n(Holschneider et al., 1989; Dutilleux, 1989), and image segmentation (Chen et al., 2015; Yu &\nKoltun, 2016).\nInput\nHidden Layer\nDilation = 1\nHidden Layer\nDilation = 2\nHidden Layer\nDilation = 4\nOutput\nDilation = 8\nFigure 3: Visualization of a stack of dilated causal convolutional layers.\nStacked dilated convolutions enable networks to have very large receptive \ufb01elds with just a few lay-\ners, while preserving the input resolution throughout the network as well as computational ef\ufb01ciency.\nIn this paper, the dilation is doubled for every layer up to a limit and then repeated: e.g.\n1, 2, 4, . . . , 512, 1, 2, 4, . . . , 512, 1, 2, 4, . . . , 512.\nThe intuition behind this con\ufb01guration is two-fold. First, exponentially increasing the dilation factor\nresults in exponential receptive \ufb01eld growth with depth (Yu & Koltun, 2016). For example each\n1, 2, 4, . . . , 512 block has receptive \ufb01eld of size 1024, and can be seen as a more ef\ufb01cient and dis-\ncriminative (non-linear) counterpart of a 1\u00d71024 convolution. Second, stacking these blocks further\nincreases the model capacity and the receptive \ufb01eld size.\n2.2\nSOFTMAX DISTRIBUTIONS\nOne approach to modeling the conditional distributions p (xt | x1, . . . , xt\u22121) over the individual\naudio samples would be to use a mixture model such as a mixture density network (Bishop, 1994)\nor mixture of conditional Gaussian scale mixtures (MCGSM) (Theis & Bethge, 2015). However,\nvan den Oord et al. (2016a) showed that a softmax distribution tends to work better, even when the\ndata is implicitly continuous (as is the case for image pixel intensities or audio sample values). One\nof the reasons is that a categorical distribution is more \ufb02exible and can more easily model arbitrary\ndistributions because it makes no assumptions about their shape.\nBecause raw audio is typically stored as a sequence of 16-bit integer values (one per timestep), a\nsoftmax layer would need to output 65,536 probabilities per timestep to model all possible values.\nTo make this more tractable, we \ufb01rst apply a \u00b5-law companding transformation (ITU-T, 1988) to\nthe data, and then quantize it to 256 possible values:\nf (xt) = sign(xt)ln (1 + \u00b5 |xt|)\nln (1 + \u00b5)\n,\n3\nwhere \u22121 < xt < 1 and \u00b5 = 255. This non-linear quantization produces a signi\ufb01cantly better\nreconstruction than a simple linear quantization scheme. Especially for speech, we found that the\nreconstructed signal after quantization sounded very similar to the original.\n2.3\nGATED ACTIVATION UNITS\nWe use the same gated activation unit as used in the gated PixelCNN (van den Oord et al., 2016b):\nz = tanh (Wf,k \u2217x) \u2299\u03c3 (Wg,k \u2217x) ,\n(2)\nwhere \u2217denotes a convolution operator, \u2299denotes an element-wise multiplication operator, \u03c3(\u00b7) is\na sigmoid function, k is the layer index, f and g denote \ufb01lter and gate, respectively, and W is a\nlearnable convolution \ufb01lter. In our initial experiments, we observed that this non-linearity worked\nsigni\ufb01cantly better than the recti\ufb01ed linear activation function (Nair & Hinton, 2010) for modeling\naudio signals.\n2.4\nRESIDUAL AND SKIP CONNECTIONS\n1 \u21e51\nReLU\nReLU\n1 \u21e51\nDilated\nConv\ntanh\n\u21e5\n+\n\u03c3\n1 \u21e51\n+\nSoftmax\nResidual\nSkip-connections\nk Layers\nOutput\nCausal\nConv\nInput\nFigure 4: Overview of the residual block and the entire architecture.\nBoth residual (He et al., 2015) and parameterised skip connections are used throughout the network,\nto speed up convergence and enable training of much deeper models. In Fig. 4 we show a residual\nblock of our model, which is stacked many times in the network.\n2.5\nCONDITIONAL WAVENETS\nGiven an additional input h, WaveNets can model the conditional distribution p (x | h) of the audio\ngiven this input. Eq. (1) now becomes\np (x | h) =\nT\nY\nt=1\np (xt | x1, . . . , xt\u22121, h) .\n(3)\nBy conditioning the model on other input variables, we can guide WaveNet\u2019s generation to produce\naudio with the required characteristics. For example, in a multi-speaker setting we can choose the\nspeaker by feeding the speaker identity to the model as an extra input. Similarly, for TTS we need\nto feed information about the text as an extra input.\nWe condition the model on other inputs in two different ways: global conditioning and local condi-\ntioning. Global conditioning is characterised by a single latent representation h that in\ufb02uences the\noutput distribution across all timesteps, e.g. a speaker embedding in a TTS model. The activation\nfunction from Eq. (2) now becomes:\nz = tanh\n\u0000Wf,k \u2217x + V T\nf,kh\n\u0001\n\u2299\u03c3\n\u0000Wg,k \u2217x + V T\ng,kh\n\u0001\n.\n4\nwhere V\u2217,k is a learnable linear projection, and the vector V T\n\u2217,kh is broadcast over the time dimen-\nsion.\nFor local conditioning we have a second timeseries ht, possibly with a lower sampling frequency\nthan the audio signal, e.g. linguistic features in a TTS model. We \ufb01rst transform this time series\nusing a transposed convolutional network (learned upsampling) that maps it to a new time series\ny = f(h) with the same resolution as the audio signal, which is then used in the activation unit as\nfollows:\nz = tanh (Wf,k \u2217x + Vf,k \u2217y) \u2299\u03c3 (Wg,k \u2217x + Vg,k \u2217y) ,\nwhere Vf,k \u2217y is now a 1\u00d71 convolution. As an alternative to the transposed convolutional network,\nit is also possible to use Vf,k\u2217h and repeat these values across time. We saw that this worked slightly\nworse in our experiments.\n2.6\nCONTEXT STACKS\nWe have already mentioned several different ways to increase the receptive \ufb01eld size of a WaveNet:\nincreasing the number of dilation stages, using more layers, larger \ufb01lters, greater dilation factors,\nor a combination thereof. A complementary approach is to use a separate, smaller context stack\nthat processes a long part of the audio signal and locally conditions a larger WaveNet that processes\nonly a smaller part of the audio signal (cropped at the end). One can use multiple context stacks\nwith varying lengths and numbers of hidden units. Stacks with larger receptive \ufb01elds have fewer\nunits per layer. Context stacks can also have pooling layers to run at a lower frequency. This keeps\nthe computational requirements at a reasonable level and is consistent with the intuition that less\ncapacity is required to model temporal correlations at longer timescales.\n3\nEXPERIMENTS\nTo measure WaveNet\u2019s audio modelling performance, we evaluate it on three different tasks: multi-\nspeaker speech generation (not conditioned on text), TTS, and music audio modelling. We provide\nsamples drawn from WaveNet for these experiments on the accompanying webpage:\nhttps://www.deepmind.com/blog/wavenet-generative-model-raw-audio/.\n3.1\nMULTI-SPEAKER SPEECH GENERATION\nFor the \ufb01rst experiment we looked at free-form speech generation (not conditioned on text). We\nused the English multi-speaker corpus from CSTR voice cloning toolkit (VCTK) (Yamagishi, 2012)\nand conditioned WaveNet only on the speaker. The conditioning was applied by feeding the speaker\nID to the model in the form of a one-hot vector. The dataset consisted of 44 hours of data from 109\ndifferent speakers.\nBecause the model is not conditioned on text, it generates non-existent but human language-like\nwords in a smooth way with realistic sounding intonations. This is similar to generative models\nof language or images, where samples look realistic at \ufb01rst glance, but are clearly unnatural upon\ncloser inspection. The lack of long range coherence is partly due to the limited size of the model\u2019s\nreceptive \ufb01eld (about 300 milliseconds), which means it can only remember the last 2\u20133 phonemes\nit produced.\nA single WaveNet was able to model speech from any of the speakers by conditioning it on a one-\nhot encoding of a speaker. This con\ufb01rms that it is powerful enough to capture the characteristics of\nall 109 speakers from the dataset in a single model. We observed that adding speakers resulted in\nbetter validation set performance compared to training solely on a single speaker. This suggests that\nWaveNet\u2019s internal representation was shared among multiple speakers.\nFinally, we observed that the model also picked up on other characteristics in the audio apart from\nthe voice itself. For instance, it also mimicked the acoustics and recording quality, as well as the\nbreathing and mouth movements of the speakers.\n5\n3.2\nTEXT-TO-SPEECH\nFor the second experiment we looked at TTS. We used the same single-speaker speech databases\nfrom which Google\u2019s North American English and Mandarin Chinese TTS systems are built. The\nNorth American English dataset contains 24.6 hours of speech data, and the Mandarin Chinese\ndataset contains 34.8 hours; both were spoken by professional female speakers.\nWaveNets for the TTS task were locally conditioned on linguistic features which were derived\nfrom input texts. We also trained WaveNets conditioned on the logarithmic fundamental frequency\n(log F0) values in addition to the linguistic features. External models predicting log F0 values and\nphone durations from linguistic features were also trained for each language. The receptive \ufb01eld size\nof the WaveNets was 240 milliseconds. As example-based and model-based speech synthesis base-\nlines, hidden Markov model (HMM)-driven unit selection concatenative (Gonzalvo et al., 2016) and\nlong short-term memory recurrent neural network (LSTM-RNN)-based statistical parametric (Zen\net al., 2016) speech synthesizers were built. Since the same datasets and linguistic features were\nused to train both the baselines and WaveNets, these speech synthesizers could be fairly compared.\nTo evaluate the performance of WaveNets for the TTS task, subjective paired comparison tests and\nmean opinion score (MOS) tests were conducted. In the paired comparison tests, after listening to\neach pair of samples, the subjects were asked to choose which they preferred, though they could\nchoose \u201cneutral\u201d if they did not have any preference. In the MOS tests, after listening to each\nstimulus, the subjects were asked to rate the naturalness of the stimulus in a \ufb01ve-point Likert scale\nscore (1: Bad, 2: Poor, 3: Fair, 4: Good, 5: Excellent). Please refer to Appendix B for details.\nFig. 5 shows a selection of the subjective paired comparison test results (see Appendix B for the\ncomplete table). It can be seen from the results that WaveNet outperformed the baseline statisti-\ncal parametric and concatenative speech synthesizers in both languages. We found that WaveNet\nconditioned on linguistic features could synthesize speech samples with natural segmental quality\nbut sometimes it had unnatural prosody by stressing wrong words in a sentence. This could be due\nto the long-term dependency of F0 contours: the size of the receptive \ufb01eld of the WaveNet, 240\nmilliseconds, was not long enough to capture such long-term dependency. WaveNet conditioned on\nboth linguistic features and F0 values did not have this problem: the external F0 prediction model\nruns at a lower frequency (200 Hz) so it can learn long-range dependencies that exist in F0 contours.\nTable 1 show the MOS test results. It can be seen from the table that WaveNets achieved 5-scale\nMOSs in naturalness above 4.0, which were signi\ufb01cantly better than those from the baseline systems.\nThey were the highest ever reported MOS values with these training datasets and test sentences.\nThe gap in the MOSs from the best synthetic speech to the natural ones decreased from 0.69 to 0.34\n(51%) in US English and 0.42 to 0.13 (69%) in Mandarin Chinese.\nSubjective 5-scale MOS in naturalness\nSpeech samples\nNorth American English\nMandarin Chinese\nLSTM-RNN parametric\n3.67 \u00b1 0.098\n3.79 \u00b1 0.084\nHMM-driven concatenative\n3.86 \u00b1 0.137\n3.47 \u00b1 0.108\nWaveNet (L+F)\n4.21 \u00b1 0.081\n4.08 \u00b1 0.085\nNatural (8-bit \u00b5-law)\n4.46 \u00b1 0.067\n4.25 \u00b1 0.082\nNatural (16-bit linear PCM)\n4.55 \u00b1 0.075\n4.21 \u00b1 0.071\nTable 1: Subjective 5-scale mean opinion scores of speech samples from LSTM-RNN-based sta-\ntistical parametric, HMM-driven unit selection concatenative, and proposed WaveNet-based speech\nsynthesizers, 8-bit \u00b5-law encoded natural speech, and 16-bit linear pulse-code modulation (PCM)\nnatural speech. WaveNet improved the previous state of the art signi\ufb01cantly, reducing the gap be-\ntween natural speech and best previous model by more than 50%.\n3.3\nMUSIC\nFor out third set of experiments we trained WaveNets to model two music datasets:\n6\n0\n20\n40\n60\n80\n100\nNo pref.\nConcat\nLSTM\nMandarin Chinese\nNorth American English\nPreference scores (%)\n23.3\n13.1\n63.6\n50.6\n33.8\n15.6\n0\n20\n40\n60\n80\n100\nNo pref.\nWaveNet (L+F)\nWaveNet (L)\nMandarin Chinese\nNorth American English\nPreference scores (%)\n17.8\n44.3\n37.9\n10.0\n64.5\n25.5\n0\n20\n40\n60\n80\n100\nNo pref.\nWaveNet (L+F)\nBest baseline\nMandarin Chinese\nNorth American English\nPreference scores (%)\n20.1\n49.3\n30.6\n12.5\n29.3\n58.2\nFigure 5: Subjective preference scores (%) of speech samples between (top) two baselines, (middle)\ntwo WaveNets, and (bottom) the best baseline and WaveNet. Note that LSTM and Concat cor-\nrespond to LSTM-RNN-based statistical parametric and HMM-driven unit selection concatenative\nbaseline synthesizers, and WaveNet (L) and WaveNet (L+F) correspond to the WaveNet condi-\ntioned on linguistic features only and that conditioned on both linguistic features and log F0 values.\n7\n\u2022 the MagnaTagATune dataset (Law & Von Ahn, 2009), which consists of about 200 hours of\nmusic audio. Each 29-second clip is annotated with tags from a set of 188, which describe\nthe genre, instrumentation, tempo, volume and mood of the music.\n\u2022 the YouTube piano dataset, which consists of about 60 hours of solo piano music obtained\nfrom YouTube videos. Because it is constrained to a single instrument, it is considerably\neasier to model.\nAlthough it is dif\ufb01cult to quantitatively evaluate these models, a subjective evaluation is possible by\nlistening to the samples they produce. We found that enlarging the receptive \ufb01eld was crucial to ob-\ntain samples that sounded musical. Even with a receptive \ufb01eld of several seconds, the models did not\nenforce long-range consistency which resulted in second-to-second variations in genre, instrumen-\ntation, volume and sound quality. Nevertheless, the samples were often harmonic and aesthetically\npleasing, even when produced by unconditional models.\nOf particular interest are conditional music models, which can generate music given a set of tags\nspecifying e.g. genre or instruments. Similarly to conditional speech models, we insert biases that\ndepend on a binary vector representation of the tags associated with each training clip. This makes\nit possible to control various aspects of the output of the model when sampling, by feeding in a\nbinary vector that encodes the desired properties of the samples. We have trained such models on\nthe MagnaTagATune dataset; although the tag data bundled with the dataset was relatively noisy and\nhad many omissions, after cleaning it up by merging similar tags and removing those with too few\nassociated clips, we found this approach to work reasonably well.\n3.4\nSPEECH RECOGNITION\nAlthough WaveNet was designed as a generative model, it can straightforwardly be adapted to dis-\ncriminative audio tasks such as speech recognition.\nTraditionally, speech recognition research has largely focused on using log mel-\ufb01lterbank energies\nor mel-frequency cepstral coef\ufb01cients (MFCCs), but has been moving to raw audio recently (Palaz\net al., 2013; T\u00a8uske et al., 2014; Hoshen et al., 2015; Sainath et al., 2015). Recurrent neural networks\nsuch as LSTM-RNNs (Hochreiter & Schmidhuber, 1997) have been a key component in these new\nspeech classi\ufb01cation pipelines, because they allow for building models with long range contexts.\nWith WaveNets we have shown that layers of dilated convolutions allow the receptive \ufb01eld to grow\nlonger in a much cheaper way than using LSTM units.\nAs a last experiment we looked at speech recognition with WaveNets on the TIMIT (Garofolo et al.,\n1993) dataset. For this task we added a mean-pooling layer after the dilated convolutions that ag-\ngregated the activations to coarser frames spanning 10 milliseconds (160\u00d7 downsampling). The\npooling layer was followed by a few non-causal convolutions. We trained WaveNet with two loss\nterms, one to predict the next sample and one to classify the frame, the model generalized better\nthan with a single loss and achieved 18.8 PER on the test set, which is to our knowledge the best\nscore obtained from a model trained directly on raw audio on TIMIT.\n4\nCONCLUSION\nThis paper has presented WaveNet, a deep generative model of audio data that operates directly at\nthe waveform level. WaveNets are autoregressive and combine causal \ufb01lters with dilated convolu-\ntions to allow their receptive \ufb01elds to grow exponentially with depth, which is important to model\nthe long-range temporal dependencies in audio signals. We have shown how WaveNets can be con-\nditioned on other inputs in a global (e.g. speaker identity) or local way (e.g. linguistic features).\nWhen applied to TTS, WaveNets produced samples that outperform the current best TTS systems\nin subjective naturalness. Finally, WaveNets showed very promising results when applied to music\naudio modeling and speech recognition.\nACKNOWLEDGEMENTS\nThe authors would like to thank Lasse Espeholt, Jeffrey De Fauw and Grzegorz Swirszcz for their\ninputs, Adam Cain, Max Cant and Adrian Bolton for their help with artwork, Helen King, Steven\n8\nGaffney and Steve Crossan for helping to manage the project, Faith Mackinder for help with prepar-\ning the blogpost, James Besley for legal support and Demis Hassabis for managing the project and\nhis inputs.\nREFERENCES\nAgiomyrgiannakis, Yannis. Vocaine the vocoder and applications is speech synthesis. In ICASSP,\npp. 4230\u20134234, 2015.\nBishop, Christopher M. Mixture density networks. Technical Report NCRG/94/004, Neural Com-\nputing Research Group, Aston University, 1994.\nChen, Liang-Chieh, Papandreou, George, Kokkinos, Iasonas, Murphy, Kevin, and Yuille, Alan L.\nSemantic image segmentation with deep convolutional nets and fully connected CRFs. In ICLR,\n2015. URL http://arxiv.org/abs/1412.7062.\nChiba, Tsutomu and Kajiyama, Masato. The Vowel: Its Nature and Structure. Tokyo-Kaiseikan,\n1942.\nDudley, Homer. Remaking speech. The Journal of the Acoustical Society of America, 11(2):169\u2013\n177, 1939.\nDutilleux, Pierre. An implementation of the \u201calgorithme `a trous\u201d to compute the wavelet transform.\nIn Combes, Jean-Michel, Grossmann, Alexander, and Tchamitchian, Philippe (eds.), Wavelets:\nTime-Frequency Methods and Phase Space, pp. 298\u2013304. Springer Berlin Heidelberg, 1989.\nFan, Yuchen, Qian, Yao, and Xie, Feng-Long, Soong Frank K. TTS synthesis with bidirectional\nLSTM based recurrent neural networks. In Interspeech, pp. 1964\u20131968, 2014.\nFant, Gunnar. Acoustic Theory of Speech Production. Mouton De Gruyter, 1970.\nGarofolo, John S., Lamel, Lori F., Fisher, William M., Fiscus, Jonathon G., and Pallett, David S.\nDARPA TIMIT acoustic-phonetic continuous speech corpus CD-ROM. NIST speech disc 1-1.1.\nNASA STI/Recon technical report, 93, 1993.\nGonzalvo, Xavi, Tazari, Siamak, Chan, Chun-an, Becker, Markus, Gutkin, Alexander, and Silen,\nHanna. Recent advances in Google real-time HMM-driven unit selection synthesizer. In Inter-\nspeech, 2016. URL http://research.google.com/pubs/pub45564.html.\nHe, Kaiming, Zhang, Xiangyu, Ren, Shaoqing, and Sun, Jian. Deep residual learning for image\nrecognition. CoRR, abs/1512.03385, 2015.\nHochreiter, S. and Schmidhuber, J. Long short-term memory. Neural Comput., 9(8):1735\u20131780,\n1997.\nHolschneider, Matthias, Kronland-Martinet, Richard, Morlet, Jean, and Tchamitchian, Philippe. A\nreal-time algorithm for signal analysis with the help of the wavelet transform. In Combes, Jean-\nMichel, Grossmann, Alexander, and Tchamitchian, Philippe (eds.), Wavelets: Time-Frequency\nMethods and Phase Space, pp. 286\u2013297. Springer Berlin Heidelberg, 1989.\nHoshen, Yedid, Weiss, Ron J., and Wilson, Kevin W. Speech acoustic modeling from raw multi-\nchannel waveforms. In ICASSP, pp. 4624\u20134628. IEEE, 2015.\nHunt, Andrew J. and Black, Alan W. Unit selection in a concatenative speech synthesis system using\na large speech database. In ICASSP, pp. 373\u2013376, 1996.\nImai, Satoshi and Furuichi, Chieko. Unbiased estimation of log spectrum. In EURASIP, pp. 203\u2013\n206, 1988.\nItakura, Fumitada. Line spectrum representation of linear predictor coef\ufb01cients of speech signals.\nThe Journal of the Acoust. Society of America, 57(S1):S35\u2013S35, 1975.\nItakura, Fumitada and Saito, Shuzo. A statistical method for estimation of speech spectral density\nand formant frequencies. Trans. IEICE, J53A:35\u201342, 1970.\n9\nITU-T. Recommendation G. 711. Pulse Code Modulation (PCM) of voice frequencies, 1988.\nJ\u00b4ozefowicz, Rafal, Vinyals, Oriol, Schuster, Mike, Shazeer, Noam, and Wu, Yonghui. Exploring the\nlimits of language modeling. CoRR, abs/1602.02410, 2016. URL http://arxiv.org/abs/\n1602.02410.\nJuang, Biing-Hwang and Rabiner, Lawrence. Mixture autoregressive hidden Markov models for\nspeech signals. IEEE Trans. Acoust. Speech Signal Process., pp. 1404\u20131413, 1985.\nKameoka, Hirokazu, Ohishi, Yasunori, Mochihashi, Daichi, and Le Roux, Jonathan. Speech anal-\nysis with multi-kernel linear prediction. In Spring Conference of ASJ, pp. 499\u2013502, 2010. (in\nJapanese).\nKaraali, Orhan, Corrigan, Gerald, Gerson, Ira, and Massey, Noel. Text-to-speech conversion with\nneural networks: A recurrent TDNN approach. In Eurospeech, pp. 561\u2013564, 1997.\nKawahara, Hideki, Masuda-Katsuse, Ikuyo, and de Cheveign\u00b4e, Alain. Restructuring speech rep-\nresentations using a pitch-adaptive time-frequency smoothing and an instantaneous-frequency-\nbased f0 extraction: possible role of a repetitive structure in sounds. Speech Commn., 27:187\u2013\n207, 1999.\nKawahara, Hideki, Estill, Jo, and Fujimura, Osamu. Aperiodicity extraction and control using mixed\nmode excitation and group delay manipulation for a high quality speech analysis, modi\ufb01cation and\nsynthesis system STRAIGHT. In MAVEBA, pp. 13\u201315, 2001.\nLaw, Edith and Von Ahn, Luis. Input-agreement: a new mechanism for collecting data using human\ncomputation games. In Proceedings of the SIGCHI Conference on Human Factors in Computing\nSystems, pp. 1197\u20131206. ACM, 2009.\nMaia, Ranniery, Zen, Heiga, and Gales, Mark J. F. Statistical parametric speech synthesis with joint\nestimation of acoustic and excitation model parameters. In ISCA SSW7, pp. 88\u201393, 2010.\nMorise, Masanori, Yokomori, Fumiya, and Ozawa, Kenji. WORLD: A vocoder-based high-quality\nspeech synthesis system for real-time applications. IEICE Trans. Inf. Syst., E99-D(7):1877\u20131884,\n2016.\nMoulines, Eric and Charpentier, Francis. Pitch synchronous waveform processing techniques for\ntext-to-speech synthesis using diphones. Speech Commn., 9:453\u2013467, 1990.\nMuthukumar, P. and Black, Alan W. A deep learning approach to data-driven parameterizations for\nstatistical parametric speech synthesis. arXiv:1409.8558, 2014.\nNair, Vinod and Hinton, Geoffrey E. Recti\ufb01ed linear units improve restricted Boltzmann machines.\nIn ICML, pp. 807\u2013814, 2010.\nNakamura, Kazuhiro, Hashimoto, Kei, Nankaku, Yoshihiko, and Tokuda, Keiichi. Integration of\nspectral feature extraction and modeling for HMM-based speech synthesis. IEICE Trans. Inf.\nSyst., E97-D(6):1438\u20131448, 2014.\nPalaz, Dimitri, Collobert, Ronan, and Magimai-Doss, Mathew. Estimating phoneme class condi-\ntional probabilities from raw speech signal using convolutional neural networks. In Interspeech,\npp. 1766\u20131770, 2013.\nPeltonen, Sari, Gabbouj, Moncef, and Astola, Jaakko. Nonlinear \ufb01lter design: methodologies and\nchallenges. In IEEE ISPA, pp. 102\u2013107, 2001.\nPoritz, Alan B. Linear predictive hidden Markov models and the speech signal. In ICASSP, pp.\n1291\u20131294, 1982.\nRabiner, Lawrence and Juang, Biing-Hwang. Fundamentals of Speech Recognition. PrenticeHall,\n1993.\nSagisaka, Yoshinori, Kaiki, Nobuyoshi, Iwahashi, Naoto, and Mimura, Katsuhiko.\nATR \u03bd-talk\nspeech synthesis system. In ICSLP, pp. 483\u2013486, 1992.\n10\nSainath, Tara N., Weiss, Ron J., Senior, Andrew, Wilson, Kevin W., and Vinyals, Oriol. Learning\nthe speech front-end with raw waveform CLDNNs. In Interspeech, pp. 1\u20135, 2015.\nTakaki, Shinji and Yamagishi, Junichi. A deep auto-encoder based low-dimensional feature ex-\ntraction from FFT spectral envelopes for statistical parametric speech synthesis. In ICASSP, pp.\n5535\u20135539, 2016.\nTakamichi, Shinnosuke, Toda, Tomoki, Black, Alan W., Neubig, Graham, Sakriani, Sakti, and Naka-\nmura, Satoshi. Post\ufb01lters to modify the modulation spectrum for statistical parametric speech\nsynthesis. IEEE/ACM Trans. Audio Speech Lang. Process., 24(4):755\u2013767, 2016.\nTheis, Lucas and Bethge, Matthias. Generative image modeling using spatial LSTMs. In NIPS, pp.\n1927\u20131935, 2015.\nToda, Tomoki and Tokuda, Keiichi. A speech parameter generation algorithm considering global\nvariance for HMM-based speech synthesis. IEICE Trans. Inf. Syst., E90-D(5):816\u2013824, 2007.\nToda, Tomoki and Tokuda, Keiichi. Statistical approach to vocal tract transfer function estimation\nbased on factor analyzed trajectory hmm. In ICASSP, pp. 3925\u20133928, 2008.\nTokuda, Keiichi. Speech synthesis as a statistical machine learning problem. http://www.sp.\nnitech.ac.jp/\u02dctokuda/tokuda_asru2011_for_pdf.pdf, 2011. Invited talk given\nat ASRU.\nTokuda, Keiichi and Zen, Heiga.\nDirectly modeling speech waveforms by neural networks for\nstatistical parametric speech synthesis. In ICASSP, pp. 4215\u20134219, 2015.\nTokuda, Keiichi and Zen, Heiga. Directly modeling voiced and unvoiced components in speech\nwaveforms by neural networks. In ICASSP, pp. 5640\u20135644, 2016.\nTuerk, Christine and Robinson, Tony. Speech synthesis using arti\ufb01cial neural networks trained on\ncepstral coef\ufb01cients. In Proc. Eurospeech, pp. 1713\u20131716, 1993.\nT\u00a8uske, Zolt\u00b4an, Golik, Pavel, Schl\u00a8uter, Ralf, and Ney, Hermann. Acoustic modeling with deep neural\nnetworks using raw time signal for LVCSR. In Interspeech, pp. 890\u2013894, 2014.\nUria, Benigno, Murray, Iain, Renals, Steve, Valentini-Botinhao, Cassia, and Bridle, John. Modelling\nacoustic feature dependencies with arti\ufb01cial neural networks: Trajectory-RNADE. In ICASSP,\npp. 4465\u20134469, 2015.\nvan den Oord, A\u00a8aron, Kalchbrenner, Nal, and Kavukcuoglu, Koray. Pixel recurrent neural networks.\narXiv preprint arXiv:1601.06759, 2016a.\nvan den Oord, A\u00a8aron, Kalchbrenner, Nal, Vinyals, Oriol, Espeholt, Lasse, Graves, Alex,\nand Kavukcuoglu, Koray.\nConditional image generation with PixelCNN decoders.\nCoRR,\nabs/1606.05328, 2016b. URL http://arxiv.org/abs/1606.05328.\nWu, Yi-Jian and Tokuda, Keiichi. Minimum generation error training with direct log spectral distor-\ntion on LSPs for HMM-based speech synthesis. In Interspeech, pp. 577\u2013580, 2008.\nYamagishi, Junichi. English multi-speaker corpus for CSTR voice cloning toolkit, 2012. URL\nhttp://homepages.inf.ed.ac.uk/jyamagis/page3/page58/page58.html.\nYoshimura, Takayoshi. Simultaneous modeling of phonetic and prosodic parameters, and char-\nacteristic conversion for HMM-based text-to-speech systems. PhD thesis, Nagoya Institute of\nTechnology, 2002.\nYu, Fisher and Koltun, Vladlen. Multi-scale context aggregation by dilated convolutions. In ICLR,\n2016. URL http://arxiv.org/abs/1511.07122.\nZen, Heiga. An example of context-dependent label format for HMM-based speech synthesis in\nEnglish, 2006. URL http://hts.sp.nitech.ac.jp/?Download.\n11\nSpeech\nSpeech\nText\nText\nFeature\nprediction\nVocoder\nsynthesis\nText\nanalysis\nVocoder\nanalysis\nText\nanalysis\nModel\ntraining\nl\no\nl\n\u039b\u02c6\nAcoustic\nmodel\n\u02c6o\nTraining\nSynthesis\nFigure 6: Outline of statistical parametric speech synthesis.\nZen, Heiga, Tokuda, Keiichi, and Kitamura, Tadashi. Reformulating the HMM as a trajectory model\nby imposing explicit relationships between static and dynamic features. Comput. Speech Lang.,\n21(1):153\u2013173, 2007.\nZen, Heiga, Tokuda, Keiichi, and Black, Alan W. Statistical parametric speech synthesis. Speech\nCommn., 51(11):1039\u20131064, 2009.\nZen, Heiga, Senior, Andrew, and Schuster, Mike. Statistical parametric speech synthesis using deep\nneural networks. In Proc. ICASSP, pp. 7962\u20137966, 2013.\nZen, Heiga, Agiomyrgiannakis, Yannis, Egberts, Niels, Henderson, Fergus, and Szczepaniak, Prze-\nmys\u0142aw. Fast, compact, and high quality LSTM-RNN based statistical parametric speech synthe-\nsizers for mobile devices. In Interspeech, 2016. URL https://arxiv.org/abs/1606.\n06061.\nA\nTEXT-TO-SPEECH BACKGROUND\nThe goal of TTS synthesis is to render naturally sounding speech signals given a text to be syn-\nthesized. Human speech production process \ufb01rst translates a text (or concept) into movements of\nmuscles associated with articulators and speech production-related organs. Then using air-\ufb02ow from\nlung, vocal source excitation signals, which contain both periodic (by vocal cord vibration) and\naperiodic (by turbulent noise) components, are generated. By \ufb01ltering the vocal source excitation\nsignals by time-varying vocal tract transfer functions controlled by the articulators, their frequency\ncharacteristics are modulated. Finally, the generated speech signals are emitted. The aim of TTS is\nto mimic this process by computers in some way.\nTTS can be viewed as a sequence-to-sequence mapping problem; from a sequence of discrete sym-\nbols (text) to a real-valued time series (speech signals). A typical TTS pipeline has two parts; 1)\ntext analysis and 2) speech synthesis. The text analysis part typically includes a number of natural\nlanguage processing (NLP) steps, such as sentence segmentation, word segmentation, text normal-\nization, part-of-speech (POS) tagging, and grapheme-to-phoneme (G2P) conversion. It takes a word\nsequence as input and outputs a phoneme sequence with a variety of linguistic contexts. The speech\nsynthesis part takes the context-dependent phoneme sequence as its input and outputs a synthesized\nspeech waveform. This part typically includes prosody prediction and speech waveform generation.\nThere are two main approaches to realize the speech synthesis part; non-parametric, example-based\napproach known as concatenative speech synthesis (Moulines & Charpentier, 1990; Sagisaka et al.,\n1992; Hunt & Black, 1996), and parametric, model-based approach known as statistical parametric\nspeech synthesis (Yoshimura, 2002; Zen et al., 2009). The concatenative approach builds up the\nutterance from units of recorded speech, whereas the statistical parametric approach uses a gener-\native model to synthesize the speech. The statistical parametric approach \ufb01rst extracts a sequence\nof vocoder parameters (Dudley, 1939) o = {o1, . . . , oN} from speech signals x = {x1, . . . , xT }\nand linguistic features l from the text W, where N and T correspond to the numbers of vocoder\nparameter vectors and speech signals. Typically a vocoder parameter vector on is extracted at ev-\nery 5 milliseconds. It often includes cepstra (Imai & Furuichi, 1988) or line spectral pairs (Itakura,\n1975), which represent vocal tract transfer function, and fundamental frequency (F0) and aperiodic-\nity (Kawahara et al., 2001), which represent characteristics of vocal source excitation signals. Then a\nset of generative models, such as hidden Markov models (HMMs) (Yoshimura, 2002), feed-forward\nneural networks (Zen et al., 2013), and recurrent neural networks (Tuerk & Robinson, 1993; Karaali\net al., 1997; Fan et al., 2014), is trained from the extracted vocoder parameters and linguistic features\n12\nas\n\u02c6\u039b = arg max\n\u039b\np (o | l, \u039b) ,\n(4)\nwhere \u039b denotes the set of parameters of the generative model. At the synthesis stage, the most\nprobable vocoder parameters are generated given linguistic features extracted from a text to be syn-\nthesized as\n\u02c6o = arg max\no\np(o | l, \u02c6\u039b).\n(5)\nThen a speech waveform is reconstructed from \u02c6o using a vocoder. The statistical parametric ap-\nproach offers various advantages over the concatenative one such as small footprint and \ufb02exibility\nto change its voice characteristics. However, its subjective naturalness is often signi\ufb01cantly worse\nthan that of the concatenative approach; synthesized speech often sounds muf\ufb02ed and has artifacts.\nZen et al. (2009) reported three major factors that can degrade the subjective naturalness; quality of\nvocoders, accuracy of generative models, and effect of oversmoothing. The \ufb01rst factor causes the\nartifacts and the second and third factors lead to the muf\ufb02eness in the synthesized speech. There\nhave been a number of attempts to address these issues individually, such as developing high-quality\nvocoders (Kawahara et al., 1999; Agiomyrgiannakis, 2015; Morise et al., 2016), improving the ac-\ncuracy of generative models (Zen et al., 2007; 2013; Fan et al., 2014; Uria et al., 2015), and compen-\nsating the oversmoothing effect (Toda & Tokuda, 2007; Takamichi et al., 2016). Zen et al. (2016)\nshowed that state-of-the-art statistical parametric speech syntheziers matched state-of-the-art con-\ncatenative ones in some languages. However, its vocoded sound quality is still a major issue.\nExtracting vocoder parameters can be viewed as estimation of a generative model parameters given\nspeech signals (Itakura & Saito, 1970; Imai & Furuichi, 1988). For example, linear predictive anal-\nysis (Itakura & Saito, 1970), which has been used in speech coding, assumes that the generative\nmodel of speech signals is a linear auto-regressive (AR) zero-mean Gaussian process;\nxt =\nP\nX\np=1\napxt\u2212p + \u03f5t\n(6)\n\u03f5t \u223cN(0, G2)\n(7)\nwhere ap is a p-th order linear predictive coef\ufb01cient (LPC) and G2 is a variance of modeling error.\nThese parameters are estimated based on the maximum likelihood (ML) criterion. In this sense,\nthe training part of the statistical parametric approach can be viewed as a two-step optimization\nand sub-optimal: extract vocoder parameters by \ufb01tting a generative model of speech signals then\nmodel trajectories of the extracted vocoder parameters by a separate generative model for time series\n(Tokuda, 2011). There have been attempts to integrate these two steps into a single one (Toda\n& Tokuda, 2008; Wu & Tokuda, 2008; Maia et al., 2010; Nakamura et al., 2014; Muthukumar &\nBlack, 2014; Tokuda & Zen, 2015; 2016; Takaki & Yamagishi, 2016). For example, Tokuda &\nZen (2016) integrated non-stationary, nonzero-mean Gaussian process generative model of speech\nsignals and LSTM-RNN-based sequence generative model to a single one and jointly optimized\nthem by back-propagation. Although they showed that this model could approximate natural speech\nsignals, its segmental naturalness was signi\ufb01cantly worse than the non-integrated model due to over-\ngeneralization and over-estimation of noise components in speech signals.\nThe conventional generative models of raw audio signals have a number of assumptions which are\ninspired from the speech production, such as\n\u2022 Use of \ufb01xed-length analysis window; They are typically based on a stationary stochas-\ntic process (Itakura & Saito, 1970; Imai & Furuichi, 1988; Poritz, 1982; Juang & Rabiner,\n1985; Kameoka et al., 2010). To model time-varying speech signals by a stationary stochas-\ntic process, parameters of these generative models are estimated within a \ufb01xed-length, over-\nlapping and shifting analysis window (typically its length is 20 to 30 milliseconds, and shift\nis 5 to 10 milliseconds). However, some phones such as stops are time-limited by less than\n20 milliseconds (Rabiner & Juang, 1993). Therefore, using such \ufb01xed-size analysis win-\ndow has limitations.\n\u2022 Linear \ufb01lter; These generative models are typically realized as a linear time-invariant \ufb01l-\nter (Itakura & Saito, 1970; Imai & Furuichi, 1988; Poritz, 1982; Juang & Rabiner, 1985;\nKameoka et al., 2010) within a windowed frame. However, the relationship between suc-\ncessive audio samples can be highly non-linear.\n13\n\u2022 Gaussian process assumption; The conventional generative models are based on Gaussian\nprocess (Itakura & Saito, 1970; Imai & Furuichi, 1988; Poritz, 1982; Juang & Rabiner,\n1985; Kameoka et al., 2010; Tokuda & Zen, 2015; 2016). From the source-\ufb01lter model of\nspeech production (Chiba & Kajiyama, 1942; Fant, 1970) point of view, this is equivalent\nto assuming that a vocal source excitation signal is a sample from a Gaussian distribu-\ntion (Itakura & Saito, 1970; Imai & Furuichi, 1988; Poritz, 1982; Juang & Rabiner, 1985;\nTokuda & Zen, 2015; Kameoka et al., 2010; Tokuda & Zen, 2016). Together with the lin-\near assumption above, it results in assuming that speech signals are normally distributed.\nHowever, distributions of real speech signals can be signi\ufb01cantly different from Gaussian.\nAlthough these assumptions are convenient, samples from these generative models tend to be noisy\nand lose important details to make these audio signals sounding natural.\nWaveNet, which was described in Section 2, has none of the above-mentioned assumptions. It\nincorporates almost no prior knowledge about audio signals, except the choice of the receptive \ufb01eld\nand \u00b5-law encoding of the signal. It can also be viewed as a non-linear causal \ufb01lter for quantized\nsignals. Although such non-linear \ufb01lter can represent complicated signals while preserving the\ndetails, designing such \ufb01lters is usually dif\ufb01cult (Peltonen et al., 2001). WaveNets give a way to\ntrain them from data.\nB\nDETAILS OF TTS EXPERIMENT\nThe HMM-driven unit selection and WaveNet TTS systems were built from speech at 16 kHz sam-\npling. Although LSTM-RNNs were trained from speech at 22.05 kHz sampling, speech at 16\nkHz sampling was synthesized at runtime using a resampling functionality in the Vocaine vocoder\n(Agiomyrgiannakis, 2015). Both the LSTM-RNN-based statistical parametric and HMM-driven unit\nselection speech synthesizers were built from the speech datasets in the 16-bit linear PCM, whereas\nthe WaveNet-based ones were trained from the same speech datasets in the 8-bit \u00b5-law encoding.\nThe linguistic features include phone, syllable, word, phrase, and utterance-level features (Zen,\n2006) (e.g. phone identities, syllable stress, the number of syllables in a word, and position of the\ncurrent syllable in a phrase) with additional frame position and phone duration features (Zen et al.,\n2013). These features were derived and associated with speech every 5 milliseconds by phone-level\nforced alignment at the training stage. We used LSTM-RNN-based phone duration and autoregres-\nsive CNN-based log F0 prediction models. They were trained so as to minimize the mean squared\nerrors (MSE). It is important to note that no post-processing was applied to the audio signals gener-\nated from the WaveNets.\nThe subjective listening tests were blind and crowdsourced. 100 sentences not included in the train-\ning data were used for evaluation. Each subject could evaluate up to 8 and 63 stimuli for North\nAmerican English and Mandarin Chinese, respectively. Test stimuli were randomly chosen and pre-\nsented for each subject. In the paired comparison test, each pair of speech samples was the same\ntext synthesized by the different models. In the MOS test, each stimulus was presented to subjects in\nisolation. Each pair was evaluated by eight subjects in the paired comparison test, and each stimulus\nwas evaluated by eight subjects in the MOS test. The subjects were paid and native speakers per-\nforming the task. Those ratings (about 40%) where headphones were not used were excluded when\ncomputing the preference and mean opinion scores. Table 2 shows the full details of the paired\ncomparison test shown in Fig. 5.\n14\nSubjective preference (%) in naturalness\nWaveNet\nWaveNet\nNo\nLanguage\nLSTM\nConcat\n(L)\n(L+F)\npreference\np value\nNorth\n23.3\n63.6\n13.1\n\u226a10\u22129\nAmerican\n18.7\n69.3\n12.0\n\u226a10\u22129\nEnglish\n7.6\n82.0\n10.4\n\u226a10\u22129\n32.4\n41.2\n26.4\n0.003\n20.1\n49.3\n30.6\n\u226a10\u22129\n17.8\n37.9\n44.3\n\u226a10\u22129\nMandarin\n50.6\n15.6\n33.8\n\u226a10\u22129\nChinese\n25.0\n23.3\n51.8\n0.476\n12.5\n29.3\n58.2\n\u226a10\u22129\n17.6\n43.1\n39.3\n\u226a10\u22129\n7.6\n55.9\n36.5\n\u226a10\u22129\n10.0\n25.5\n64.5\n\u226a10\u22129\nTable 2: Subjective preference scores of speech samples between LSTM-RNN-based statistical para-\nmetric (LSTM), HMM-driven unit selection concatenative (Concat), and proposed WaveNet-based\nspeech synthesizers. Each row of the table denotes scores of a paired comparison test between two\nsynthesizers. Scores of the synthesizers which were signi\ufb01cantly better than their competing ones at\np < 0.01 level were shown in the bold type. Note that WaveNet (L) and WaveNet (L+F) correspond\nto WaveNet conditioned on linguistic features only and that conditioned on both linguistic features\nand F0 values.\n15\n",
        "sentence": "",
        "context": "ACKNOWLEDGEMENTS\nThe authors would like to thank Lasse Espeholt, Jeffrey De Fauw and Grzegorz Swirszcz for their\ninputs, Adam Cain, Max Cant and Adrian Bolton for their help with artwork, Helen King, Steven\n8\n8\nGaffney and Steve Crossan for helping to manage the project, Faith Mackinder for help with prepar-\ning the blogpost, James Besley for legal support and Demis Hassabis for managing the project and\nhis inputs.\nREFERENCES\nAndrew Senior\nKoray Kavukcuoglu\n{avdnoord, sedielem, heigazen, simonyan, vinyals, gravesa, nalk, andrewsenior, korayk}@google.com\nGoogle DeepMind, London, UK\n\u2020 Google, London, UK\nABSTRACT"
    },
    {
        "title": "The CMU pronunciation dictionary 0.7",
        "author": [
            "R. Weide"
        ],
        "venue": "Carnegie Mellon University,",
        "citeRegEx": "Weide,? \\Q2008\\E",
        "shortCiteRegEx": "Weide",
        "year": 2008,
        "abstract": "",
        "full_text": "",
        "sentence": " We train a grapheme-to-phoneme model on data obtained from CMUDict (Weide, 2008).",
        "context": null
    },
    {
        "title": "Sequence-tosequence neural net models for grapheme-to-phoneme conversion",
        "author": [
            "Yao",
            "Kaisheng",
            "Zweig",
            "Geoffrey"
        ],
        "venue": "arXiv preprint arXiv:1506.00196,",
        "citeRegEx": "Yao et al\\.,? \\Q2015\\E",
        "shortCiteRegEx": "Yao et al\\.",
        "year": 2015,
        "abstract": "",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "Unidirectional long short-term memory recurrent neural network with recurrent output layer for low-latency speech synthesis",
        "author": [
            "Zen",
            "Heiga",
            "Sak",
            "Ha\u015fim"
        ],
        "venue": "In Acoustics, Speech and Signal Processing (ICASSP),",
        "citeRegEx": "Zen et al\\.,? \\Q2015\\E",
        "shortCiteRegEx": "Zen et al\\.",
        "year": 2015,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "Statistical parametric speech synthesis using deep neural networks",
        "author": [
            "Zen",
            "Heiga",
            "Senior",
            "Andrew",
            "Schuster",
            "Mike"
        ],
        "venue": "In Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP),",
        "citeRegEx": "Zen et al\\.,? \\Q2013\\E",
        "shortCiteRegEx": "Zen et al\\.",
        "year": 2013,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " WaveNet uses several features from a TTS system (Zen et al., 2013), that include values such as the number of syllables in a word, position of syllables in the phrase, position of the current frame in the phoneme, and dynamic features of the speech spectrum like spectral and excitation parameters, as well as their time derivatives.",
        "context": null
    }
]