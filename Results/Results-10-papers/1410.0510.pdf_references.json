[
    {
        "title": "The dropout learning algorithm",
        "author": [
            "Pierre Baldi",
            "Peter J. Sadowski"
        ],
        "venue": "Artif. Intell.,",
        "citeRegEx": "1",
        "shortCiteRegEx": "1",
        "year": 2014,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " Many variants of learning algorithms have been proposed, from complex gradient computations [11], to dropout methods [1], but the baseline learning algorithm still consists in recursively computing the gradient by using the back-propagation algorithm and performing (stochastic) gradient descent.",
        "context": null
    },
    {
        "title": "Fast classification using sparse decision dags",
        "author": [
            "R\u00f3bert Busa-Fekete",
            "Djalel Benbouzid",
            "Bal\u00e1zs K\u00e9gl"
        ],
        "venue": "In Proceedings of the 29th International Conference on Machine Learning,",
        "citeRegEx": "2",
        "shortCiteRegEx": "2",
        "year": 2012,
        "abstract": "In this paper we propose an algorithm that builds sparse decision DAGs\n(directed acyclic graphs) from a list of base classifiers provided by an\nexternal learning method such as AdaBoost. The basic idea is to cast the DAG\ndesign task as a Markov decision process. Each instance can decide to use or to\nskip each base classifier, based on the current state of the classifier being\nbuilt. The result is a sparse decision DAG where the base classifiers are\nselected in a data-dependent way. The method has a single hyperparameter with a\nclear semantics of controlling the accuracy/speed trade-off. The algorithm is\ncompetitive with state-of-the-art cascade detectors on three object-detection\nbenchmarks, and it clearly outperforms them when there is a small number of\nbase classifiers. Unlike cascades, it is also readily applicable for\nmulti-class classification. Using the multi-class setup, we show on a benchmark\nweb page ranking data set that we can significantly improve the decision speed\nwithout harming the performance of the ranker.",
        "full_text": "Fast classi\ufb01cation using sparse decision DAGs\nDjalel Benbouzid1\ndjalel.benbouzid@gmail.com\nR\u00b4obert Busa-Fekete1,2\nbusarobi@gmail.com\nBal\u00b4azs K\u00b4egl1\nbalazs.kegl@gmail.com\n1LAL/LRI, University of Paris-Sud, CNRS, 91898 Orsay, France\n2Research Group on Arti\ufb01cial Intelligence of the Hungarian Academy of Sciences and University of Szeged, Aradi\nv\u00b4ertan\u00b4uk tere 1., H-6720 Szeged, Hungary\nAbstract\nIn this paper we propose an algorithm that\nbuilds sparse decision DAGs (directed acyclic\ngraphs) from a list of base classi\ufb01ers pro-\nvided by an external learning method such\nas AdaBoost. The basic idea is to cast the\nDAG design task as a Markov decision pro-\ncess. Each instance can decide to use or to\nskip each base classi\ufb01er, based on the current\nstate of the classi\ufb01er being built.\nThe re-\nsult is a sparse decision DAG where the base\nclassi\ufb01ers are selected in a data-dependent\nway.\nThe method has a single hyperpa-\nrameter with a clear semantics of control-\nling the accuracy/speed trade-o\ufb00.\nThe al-\ngorithm is competitive with state-of-the-art\ncascade detectors on three object-detection\nbenchmarks, and it clearly outperforms them\nwhen there is a small number of base classi-\n\ufb01ers. Unlike cascades, it is also readily appli-\ncable for multi-class classi\ufb01cation. Using the\nmulti-class setup, we show on a benchmark\nWeb page ranking data set that we can sig-\nni\ufb01cantly improve the decision speed without\nharming the performance of the ranker.\n1. Introduction\nThere are numerous applications where the computa-\ntional requirements of classifying a test instance are\nas important as the performance of the classi\ufb01er it-\nself. Object detection in images (Viola & Jones, 2004)\nand web page ranking (Chapelle & Chang, 2011) are\nwell-known examples. A more recent application do-\nmain with similar requirements is trigger design in\nAppearing in Proceedings of the 29 th International Confer-\nence on Machine Learning, Edinburgh, Scotland, UK, 2012.\nCopyright 2012 by the author(s)/owner(s).\nhigh energy physics (Gligorov, 2011). Most of these\napplications come with another common feature: the\nnegative class (usually called noise or background)\nsometimes has orders of magnitudes higher probabil-\nity than the positive class. Besides the testing time\nconstraints, this also makes training di\ufb03cult: tradi-\ntional classi\ufb01cation-error-based measures are not ade-\nquate, and using prior class probabilities in construct-\ning training samples leads to either enormous data\nsizes or little representativity of the positive class.\nA common solution to these problems is to design cas-\ncade classi\ufb01ers (Viola & Jones, 2004). A cascade clas-\nsi\ufb01er consists of stages. In each stage a binary clas-\nsi\ufb01er attempts to eliminate background instances by\nclassifying them negatively. Positive classi\ufb01cation in\ninner stages sends the instance to the next stage, so\ndetection can only be made in the last stage. By using\nsimple and fast classi\ufb01ers in the \ufb01rst stages, \u201ceasy\u201d\nbackground instances can be rejected fast, shorten-\ning the expected testing time. The cascade structure\nalso allows us to use di\ufb00erent training sets in di\ufb00er-\nent stages, having more di\ufb03cult background samples\nin later stages.\nCascade classi\ufb01ers, however, have many disadvantages\nin both the training and test phases. The training pro-\ncess requires a lot of hand-tuning of control parame-\nters, and it is non-trivial how to handle the trade-\no\ufb00between the performance and the complexity of\nthe cascade. Also, each individual stage needs to be\ntrained with examples that have been classi\ufb01ed posi-\ntively by all the previous stages, which becomes dif-\n\ufb01cult to satisfy in the later stages. Moreover, during\ntest time, the cascade structure itself has several draw-\nbacks. First, for a given stage, the margin information\nof a test example is lost and is not exploited in the\nsubsequent stages. Second, all the positive instances\nhave to pass through all the stages for a correct clas-\nFast classi\ufb01cation using sparse decision DAGs\nsi\ufb01cation. Finally, extending the cascade architecture\nto the multi-class case is non-trivial. For example in\nweb page ranking, it is just as crucial to make a fast\nprediction on the relevance of a web page to a query as\nin object detection (Chapelle et al., 2011a), but unlike\nin object detection, human annotation often provides\nmore than two relevance levels.\nIn this paper we propose a method intended to over-\ncome these problems. In our setup we assume that we\nare given a sequence of low-complexity, possibly multi-\nclass base classi\ufb01ers (or features) sorted by importance\nor quality. Our strategy is to design a controller or\na decision maker which decides which base classi\ufb01ers\nshould be evaluated for a given instance.\nThe con-\ntroller makes its decision sequentially based on the out-\nput of the base classi\ufb01ers evaluated so far. It has three\npossibilities in each step: 1) it can decide to continue\nthe classi\ufb01cation by evaluating the next classi\ufb01er, 2)\nskip a classi\ufb01er by jumping over it, or 3) quit and use\nthe current combined classi\ufb01er. The goal of the con-\ntroller is to achieve a good performance with as few\nbase classi\ufb01er evaluations as possible.\nThis \ufb02exible\nsetup can accommodate any performance evaluation\nmetric and an arbitrary computational cost function.\nDesigning the controller can be naturally cast into a\nMarkov decision process (MDP) framework where the\nroles are the following: the policy is the controller, the\nindex of the base classi\ufb01er and the output of classi-\n\ufb01er constitute the states, the alternatives correspond\nto the actions, and the rewards are de\ufb01ned based on\nthe target metric and the cost of evaluating the base\nclassi\ufb01ers.\nOur approach has several advantages over cascades.\nFirst, we can eliminate stages. Similar to SoftCas-\ncade (Bourdev & Brandt, 2005), the base classi\ufb01ers\ndo not have to be organized into a small number\nof stages before or while learning the cascade.\nSec-\nond, we can easily control the trade-o\ufb00between the\naverage number of evaluated base classi\ufb01ers and the\nquality of the classi\ufb01cation by combining these two\ncompeting goals into an appropriate reward.\nThe\nform of the reward can also easily accommodate cost-\nsensitivity (Saberian & Vasconcelos, 2010) of the base\nclassi\ufb01ers although we will not investigate this is-\nsue here. The fact that some base classi\ufb01ers can be\nskipped has an important consequence: the resulting\nclassi\ufb01er is sparse, moreover, the number and identities\nof base classi\ufb01ers depend on the particular instances.\nThird, eliminating stages allows each instance to \u201cde-\ncide\u201d its own path within the list of base-classi\ufb01ers.\nTheoretically, we could have as many di\ufb00erent paths as\ntraining instances, but, a-posteriori, we observe clus-\ntering in the \u201cpath-space\u201d. Fourth, eliminating stages\nalso greatly simpli\ufb01es the design.\nOur algorithm is\nbasically turn-key: it comes with an important design\nparameter (the trade-o\ufb00coe\ufb03cient between accuracy\nand speed) and a couple of technical hyperparame-\nters of the MDP algorithm that can be kept constant\nacross the benchmark problems we use. Finally, the\nmulti-class extension of the technique is quite straight-\nforward.\nAllowing skipping is an important feature of the al-\ngorithm. The result of this design choice is that the\nstructure of the learned classi\ufb01er is not a cascade, but a\nmore general directed acyclic graph or a decision DAG.\nIn fact, the main reason for sticking to the cascade\ndesign is that it is easy to control with semi-manual\nheuristics. Once the construction is automatic, keep-\ning the cascade architecture is no longer a necessary\nconstraint. Allowing skipping is also a crucial di\ufb00er-\nence compared to the approach of (P\u00b4oczos et al., 2009)\nwho also proposed to learn a cascade in an MDP setup.\nWhile their policy simply designs optimal thresholds\nin stages of a classical cascade, MDDAG outputs a\nclassi\ufb01er with a di\ufb00erent structure. Our method can\nalso be related to the sequential classi\ufb01er design of\n(Dulac-Arnold et al., 2011). In their approach the ac-\ntion space is much larger: at any state the controler\ncan decide to jump to any of the base classi\ufb01ers, and\nso the action space grows with the number of base\nlearners. Whereas this design choice makes feature se-\nlection more \ufb02exible, it also generates a harder learning\nproblem for the MDP.\nThe paper is organized as follows.\nIn Section 2 we\ndescribe the algorithm, then in Section 3 we present\nour experimental results. In Section 4 we discuss the\nalgorithm and its connection with existing methods,\nand in Section 5 we draw some pertinent conclusions.\n2. The MDDAG algorithm\nWe will assume that we are given a sequence of N\nbase classi\ufb01ers H = (h1, . . . , hN). Although in most\ncases cascades are built for binary classi\ufb01cation, we\nwill describe the method for the more general multi-\nclass case, which means that hj : X \u2192RK, where X\nis the input space and K is the number of classes. The\nsemantics of h is that, given an observation x \u2208X, it\nvotes for class \u2113if its \u2113th element h\u2113(x) is positive, and\nvotes against class \u2113if h\u2113(x) is negative. The absolute\nvalue |h\u2113(x)| can be interpreted as the con\ufb01dence of\nthe vote. This assumption is naturally satis\ufb01ed by the\noutput of AdaBoost.MH (Schapire & Singer, 1999),\nbut in principle any algorithm that builds its \ufb01nal clas-\nsi\ufb01er as a linear combination of simpler functions can\nbe used to provide H. In the case of AdaBoost.MH\nor multi-class neural networks, the \ufb01nal (or strong or\nFast classi\ufb01cation using sparse decision DAGs\naveraged) classi\ufb01er de\ufb01ned by the full sequence H is\nf(x) = PN\nj=1 hj(x), and its prediction for the class in-\ndex of x is b\u2113= arg max\u2113f\u2113(x). In binary detection, f\nis usually used as a scoring function. The observation\nx is classi\ufb01ed as positive if f1(x) = \u2212f2(x) > \u03b8 and\nbackground otherwise. The threshold \u03b8 is a free pa-\nrameter that can be tuned to achieve, for instance, a\ngiven false positive rate.\nThe goal of the MDDAG (Markov decision direct\nacyclic graph) algorithm is to build a sparse \ufb01nal clas-\nsi\ufb01er from H that does not use all the base classi\ufb01ers,\nand which selects them in a way depending on the in-\nstance x to be classi\ufb01ed. For a given observation x, we\nprocess the base classi\ufb01ers in their original order. For\neach base classi\ufb01er hj, we choose from among three\npossible actions: 1) we Evaluate hj and continue, 2)\nwe Skip hj and continue, or 3) we Quit and return\nthe classi\ufb01er built so far. Let\nbj(x) = 1 \u2212I {aj = Skip \u2228\u2203j\u2032 < j : aj\u2032 = Quit} (1)\nbe the indicator that hj is evaluated on x, where aj \u2208\n{Eval, Skip, Quit} is the action taken at step j and\nthe indicator function I {A} is 1 if its argument A is\ntrue and 0 otherwise. Then the \ufb01nal classi\ufb01er built by\nthe procedure is\nf (N)(x) =\nN\nX\nj=1\nbj(x)hj(x).\n(2)\nThe decision on action aj will be made based on the\nindex of the base classi\ufb01er j and the output vector of\nthe classi\ufb01er\nf (j)(x) =\nj\nX\nj\u2032=1\nbj\u2032(x)hj\u2032(x).\n(3)\nbuilt up to step j.1 Formally, aj = \u03c0\n\u0000(sj(x)\n\u0001\n, where\nsj(x) =\n\u0000f (j\u22121)\n1\n(x), . . . , f (j\u22121)\nK\n(x), j \u22121\n\u0001\n\u2208RK \u00d7 N+\n(4)\nis the state we are in before visiting hj, and \u03c0 is a policy\nthat determines the action in state sj. The initial state\ns1 is the zero vector with K + 1 elements.\nThis setup formally de\ufb01nes a Markov decision process\n(MDP). An MDP is a 4-tuple M = (S, A, P, R), where\nS is the (possibly in\ufb01nite) state space and A is the\n1When using AdaBoost.MH, the base classi\ufb01ers are\nbinary hj(x) = {\u00b1\u03b1j}K, and we normalize the output (3)\nby PN\nj=1 \u03b1j, but since this factor is constant, the only rea-\nson to do so is to make the range of the state space uniform\nacross experiments.\ncountable set of actions. P : S \u00d7 S \u00d7 A \u2192[0, 1] is the\ntransition probability kernel which de\ufb01nes the random\ntransitions s(t+1) \u223cP(\u00b7|s(t), a(t)) from a state s(t) ap-\nplying the action a(t), and R : R \u00d7 S \u00d7 A \u2192[0, 1]\nde\ufb01nes the distribution R(\u00b7|s(t), a(t)) of the immediate\nreward r(t) for each state-action pair. A deterministic\npolicy \u03c0 assigns an action to each state \u03c0 : S \u2192A. We\nwill only use undiscounted and episodic MDPs where\nthe policy \u03c0 is evaluated using the expected sum of\nrewards\n\u03f1 = E\n( T\nX\nt=1\nr(t)\n)\n(5)\nwith a \ufb01nite horizon T. In the episodic setup we also\nhave an initial state (s1 in our case) and a terminal\nstate s\u221ewhich is impossible to leave. In our setup,\nthe state s(t) is equivalent to sj(x) (4) with j = t. The\naction Quit brings the process to the terminal state\ns\u221e. Note that in s(T ) only the Quit action is allowed.\n2.1. The rewards\nAs our primary goal is to achieve a good performance\nin terms of the evaluation metric of interest, we will\npenalize the error of f (t) when the action a(t) = Quit\nis applied. The setup can handle any loss function.\nHere, we will use the multi-class 0-1 loss function\nLI(f, (x, \u2113)) = I\n\u001a\nf\u2113(x) \u2212max\n\u2113\u2032\u0338=\u2113f\u2113\u2032(x) < 0\n\u001b\nand the multi-class exponential loss function\nLexp(f, (x, \u2113)) = exp\n\uf8eb\n\uf8ed\nK\nX\n\u2113\u2032\u0338=\u2113\nf\u2113\u2032(x) \u2212f\u2113(x)\n\uf8f6\n\uf8f8,\nwhere\nthe\ntraining\nobservations\n(x, \u2113)\n\u2208\nRd \u00d7\n{1, . . . , K} are drawn from a distribution D. Note that\nin the binary case, LI and Lexp recover the classical bi-\nnary notions.\nWith these notations, the reward for the Quit action\ncomes from the distribution\nR(r|s(t), Quit) =\nP(x,\u2113)\u223cD\n\u0000\u2212L(f, (x, \u2113))|s(t) = (f (t\u22121)(x), t \u22121)\n\u0001\n.\n(6)\nFrom now on we will refer to our algorithm as\nMDDAG.I or MDDAG.EXP when we use 0-1 loss\nor exponential loss, respectively. In principle, any of\nthe usual convex upper bounds (e.g., logistic, hinge,\nquadratic) could be used in the MDP framework. The\nexponential loss function was inspired by the setup\nof AdaBoost (Freund & Schapire, 1997; Schapire &\nSinger, 1999).\nFast classi\ufb01cation using sparse decision DAGs\nTo encourage sparsity, we will also penalize each eval-\nuated base classi\ufb01er h by a uniform \ufb01xed negative re-\nward\nR(r|s, Eval) = \u03b4(\u2212\u03b2 \u2212r),\n(7)\nwhere \u03b4 is the Dirac delta and \u03b2 is a hyperparame-\nter that represents the accuracy-speed trade-o\ufb00. Note\nthat, again, this \ufb02exible setup can accommodate any\ncost function penalizing the evaluation of base classi-\n\ufb01ers. Finally, choosing the Skip action does not incur\nany reward, so R(r|s, Skip) = \u03b4(0).\nThe goal of reinforcement learning (RL) in our case is\nto learn a policy which maximizes the expected sum\nof rewards (5). Since in our setup, the transition P is\ndeterministic given the observation x, the expectation\nin (5) is taken with respect to the random input point\n(x, \u2113).\nThis means that the global objective of the\nMDP is to minimize\nE(x,\u2113)\u223cD\n\uf8f1\n\uf8f2\n\uf8f3L\n\u0000f, (x, \u2113)\n\u0001\n+ \u03b2\nN\nX\nj=1\nbj(x)\n\uf8fc\n\uf8fd\n\uf8fe.\n(8)\n2.2. Learning the policy\nThere are several e\ufb03cient algorithms available for\nlearning the policy \u03c0 using an iid sample D\n=\n\u0000(x1, \u21131), . . . , (xn, \u2113n)\n\u0001\ndrawn from D (Sutton & Barto,\n1998).\nWhen P and R are unknown, model-free\nmethods are commonly used for learning the policy\n\u03c0.\nThese methods directly learn a value function\n(the expected reward in a state or for a state-action\npair) and derive a policy from it. Among model-free\nRL algorithms, temporal-di\ufb00erence (TD) learning al-\ngorithms are the most widely used. They can be di-\nvided into two groups: o\ufb00-policy and on-policy meth-\nods. In the case of o\ufb00-policy methods the policy search\nmethod learns about one policy while following an-\nother, whereas in the on-policy case the policy search\nalgorithm seeks to improve the current policy by main-\ntaining su\ufb03cient exploration. On-policy methods have\nan appealing practical advantage: they usually con-\nverge faster to the optimal policy than o\ufb00-policy meth-\nods.\nWe shall use the SARSA(\u03bb) algorithm (Rummery\n& Niranjan, 1994) with replacing traces to learn the\npolicy \u03c0.\nFor more details, we refer the reader\nto (Szepesv\u00b4ari, 2010).\nSARSA(\u03bb) is an on-policy\nmethod, so to make sure that all policies can be vis-\nited with nonzero probability, we use an \u03f5-greedy ex-\nploration strategy. To be precise, we apply SARSA in\nan episodic setup: we use a random training instance x\nfrom D per episode. The instance follows the current\npolicy with probability 1 \u2212\u03f5 and chooses a random\naction with probability \u03f5. The instance observes the\nimmediate rewards de\ufb01ned based on some loss func-\ntion, or (7) after each action. The policy is updated\nduring the episode according to SARSA(\u03bb).\nIn our experiments we used AdaBoost.MH2 to ob-\ntain a pool of weak classi\ufb01ers H, and the RL Tool-\nbox 2.03 for training the MDDAG. We ran Ad-\naBoost.MH for N\n= 1000 iterations, and then\ntrained SARSA(\u03bb) on the same training set.\nThe\nhyperparameters of SARSA(\u03bb) were kept constant\nthroughout the experiments.\nWe set \u03bb to 0.95.\nIn\nprinciple, the learning rate should decrease to 0, but\nwe found that this setting forced the algorithm to con-\nverge too fast to suboptimal solutions. Instead we set\nthe learning rate to a constant 0.2, we evaluated the\ncurrent policy after every 10000 episodes, and we se-\nlected the best policy based on their performance also\non the training set (over\ufb01tting the MDP was a non-\nissue). The exploration term \u03f5 was decreased gradually\nas 0.3 \u00d7 1/\u230810000\n\u03c4\n\u2309, where \u03c4 is the number of training\nepisodes. We trained SARSA(\u03bb) for 106 episodes.\nAs a \ufb01nal remark, note that maximizing (8) over the\ndata set D is equivalent to minimizing a margin-based\nloss with an L0 constraint. If rI (6) is used as a reward,\nthe loss is also non-convex, but minimizing a loss with\nan L0 constraint is NP-hard even if the loss is con-\nvex (Davis et al., 1997). So, what we are aiming at is\nan MDP-based heuristic to solve an NP-hard problem,\nsomething that is not without precedent (Ejov et al.,\n2004). This equivalence implies that even though the\nalgorithm would converge in the ideal case (with a de-\ncreasing learning rate), in principle, convergence can\nbe exponentially slow in n. In practice, however, we\nhad no problem \ufb01nding good policies in reasonable\ntraining time.\n3. Experiments\nIn Section 3.1 we \ufb01rst verify the sparsity and hetero-\ngeneity hypotheses on a synthetic toy example.\nIn\nSection 3.2, we compare MDDAG with state-of-the-\nart cascade detectors on three object detection bench-\nmarks. After, in Section 3.3 we show how the multi-\nclass version of MDDAG performs on a benchmark\nweb page ranking problem.\n3.1. Synthetic data\nThe aim of this experiment was to verify whether\nMDDAG can learn the subset of \u201cuseful\u201d base clas-\nsi\ufb01ers in a data-dependent way.\nWe created a two-\ndimensional binary dataset with real-valued features\n2http://www.multiboost.org (Benbouzid et al., 2012).\n3http://www.igi.tugraz.at/ril-toolbox/general/\noverview.html\nFast classi\ufb01cation using sparse decision DAGs\nwhere the positive class was composed of two easily\nseparable clusters (see Figure 1(a)). This is a typical\ncase where AdaBoost or a traditional cascade is sub-\noptimal since they both have to use all the base classi-\n\ufb01ers for all the positive instances (Bourdev & Brandt,\n2005).\nWe ran MDDAG.I with \u03b2 = 0.01 on the 1000 decision\nstumps learned by AdaBoost.MH. In Figure 1(b),\nwe plot the number of base classi\ufb01ers used for each\nindividual positive instance as a function of the two-\ndimensional instance itself. As expected, the \u201ceasier\u201d\nthe instance, the smaller the number of base classi\ufb01ers\nare needed for classi\ufb01cation. Figure 1(c) con\ufb01rms our\nsecond hypothesis: base classi\ufb01ers are used selectively,\ndepending on whether the positive instance is in the\nblue or red cluster.\nThe lower panel of Figure 1 shows a graphical represen-\ntation of the MDDAG classi\ufb01er f acting on a data set\nD. The nodes of the directed acyclic graph (DAG) are\nthe base classi\ufb01ers in H. Each observation (x, \u2113) \u2208D\ndetermines a set of edges\nUx = {(j, j\u2032) : bj(x) = bj\u2032(x) = 1\u2227\nbj\u2032\u2032(x) = 0 for all j < j\u2032\u2032 < j\u2032}.\nIn other words, we take all the base classi\ufb01ers that\nare evaluated on the instance (x, \u2113) and connect the\nnodes representing these base classi\ufb01ers with a direct\nedge. The edge set Ux is called the classi\ufb01cation path\nof x which constitutes a directed path by de\ufb01nition.\nThe DAG we plot in Figure 1(d) includes all of the\nedges U = S\n(x,1)\u2208D Ux generated by the positive in-\nstances taken from the training data D. The width\nof an edge (j, j\u2032) is proportional to its multiplicity\n#{x : (j, j\u2032) \u2208Ux, (x, 1) \u2208D}. The color of an edge\n(j, j\u2032) represents the proportion of observations taken\nfrom the blue and red sub-classes, whose classi\ufb01cation\npath includes (j, j\u2032). Similarly, the size of the node\nis proportional to #{x : bj(x) = 1, (x, 1) \u2208D}, and\nthe color of the nodes represent sub-class proportions.\nThe structure of the DAG also agrees with our orig-\ninal intuition, namely that the bulk of the two sub-\nclasses are separated early and follow di\ufb00erent classi\ufb01-\ncation paths. It is also worth noting that even though\nthe number of possible classi\ufb01cation paths is exponen-\ntially large, the number of realized paths is quite small.\nSome \u201cnoisy\u201d points along the main diagonal (border\nbetween the subclasses) generate rare subpaths, but\nthe bulk of the data mostly follows two paths.\n3.2. Binary detection benchmarks\nIn these experiments we applied MDDAG on three\nimage data sets often used for benchmarking object de-\ntection cascades. VJ (Viola & Jones, 2004) and CBCL\nare face recognition benchmarks, and DPED (Munder\n& Gavrila, 2006) is a pedestrian recognition data set.\nWe divided the data sets into training and test sets.\nWe compared MDDAG to three state-of-the-art ob-\nject detection algorithms (the original Viola-Jones\ncascade VJCascade (Viola & Jones, 2004), FC-\nBoost (Saberian & Vasconcelos, 2010), and Soft-\nCascade (Bourdev & Brandt, 2005)). VJCascade\nbuilds the cascade stage-by-stage by running Ad-\naBoost in each stage.\nIt stops adding base clas-\nsi\ufb01ers to the mth stage when the false positive rate\n(FPR) falls below pm\nfpr and true positive rate (TPR)\nexceeds pm\ntpr, where ptpr and pftr are hyperparam-\neters of the algorithm.\nThe total number of stages\nis also a hyperparameter. FCBoost also adds base\nclassi\ufb01ers iteratively to the cascade, but the base clas-\nsi\ufb01er can be inserted into any of the stages. The goal\nis to minimize a global criterion which, similarly to\n(8), is composed of a performance-based term and a\ncomplexity-based term. The number of iterations and\nthe parameter \u03b7 that determines the trade-o\ufb00between\nthe two competing objectives are hyperparameters of\nthe algorithm. SoftCascade, like MDDAG, builds\na cascade on the output of AdaBoost, where each\nstage consists of exactly one base classi\ufb01er. The \ufb01nal\nnumber of base classi\ufb01ers is decided beforehand by the\nhyperparameter N. In each iteration j, the base clas-\nsi\ufb01er with the highest balanced edge is selected, and\nthe detection threshold \u03b8j is set to achieve a TPR of\n1 \u2212exp (\u03b1j/N \u2212\u03b1I {\u03b1 < 0}), where \u03b1 is a second hy-\nperparameter of the algorithm. Both the TPR and the\nnumber of base classi\ufb01ers increase with \u03b1, so the choice\nof \u03b1 in\ufb02uences the speed/accuracy trade-o\ufb00(although\nnot as explicitly as our \u03b2 or FCBoost\u2019s \u03b7).\nComparing test-time-constrained detection algorithms\nis quite di\ufb03cult.\nThe usual trade-o\ufb00between the\nfalse positive rate (FPR) and the true positive rate\n(TPR) can be captured by ROC curves, but here we\nalso have to take into account the computational e\ufb03-\nciency of the detector. In (Bourdev & Brandt, 2005)\nthis problem is solved by displaying three-dimensional\nFPR/TPR/number-of-features surfaces. Here, we de-\ncided to show two-dimensional slices of these surfaces:\nwe \ufb01x the FPR to reasonable values and plot the TPR\nagainst the detection time. In each of the algorithms\nwe used Haar features as base classi\ufb01ers, so the de-\ntection time can be uniformly measured in terms of\nthe average number of base classi\ufb01ers needed for de-\ntection. In typical detection problems the number of\nbackground (negative) instances is orders of magni-\ntudes higher than the number of signal (positive) in-\nstances, so we computed this average only over the\nnegative test set. It turns out that using this measure,\nFast classi\ufb01cation using sparse decision DAGs\n\u22122\n0\n2\n4\n6\n\u22122\n0\n2\n4\n6\n(a)\n \n \n(b)\n \n \n\u22122\n0\n2\n4\n6\n\u22122\n0\n2\n4\n6\n4\n6\n8\n10\n0\n10\n20\n30\n0\n200\n400\n600\n800\n1000\nNumber of evaluations\nIndex of weak classifiers\n(c)\n \n \n\u0001\n\u0002\n\u0003\n\u0004\n\u0005\n\u0006\u0001\n\u0007\n\u0006\b\n\u0006\u0002\n\u0006\u0003\n\u0006\u0004\n\u0006\u0005\n\u0006\t\n\u0001\u0006\nFigure 1. Experiments with synthetic data.\n(a) The positive class is composed of the blue and red clusters, and the\nnegative class is the green cluster. (b) The number of base classi\ufb01ers used for each individual positive instance versus the\ntwo-dimensional feature coordinates. (c) The number of positive instances from the blue/red clusters on which a given\nbase classi\ufb01er was applied according to the policy learned by MDDAG.I. Lower panel: the decision DAG for the positive\nclass.\nvanilla AdaBoost is fairly competitive with tailor-\nmade cascade detectors, so we also included it in the\ncomparison.\nComputing the TPR versus number-of-features curve\nat a \ufb01xed FPR cannot be done in a generic algorithm-\nindependent way. For AdaBoost, in each iteration\nj (that is, for each number j of base classi\ufb01ers) we\ntune the detection threshold \u03b8 to achieve the given\ntest FPR, and plot the achieved test TPR versus j. In\nthe other three algorithms we have 2-3 hyperparam-\neters that explicitly or implicitly in\ufb02uence the aver-\nage number of base classi\ufb01ers and the overall perfor-\nmance. We ran the algorithms using di\ufb00erent hyper-\nparameter combinations. In each run we set the detec-\ntion threshold \u03b8 to achieve the given test FPR. With\nthis threshold, each run k determines a TPR/average-\nnumber-of-features pair (pk, Nk) on the training set\nand (p\u2032\nk, N \u2032\nk) on the test set. For each N, we \ufb01nd the\nrun k\u2217(N) = arg maxk:Nk\u2264N pk that achieves the best\ntraining pk using at most N base classi\ufb01ers, and plot\nthe test TPR p\u2032\nk\u2217(N) versus N. Although over\ufb01tting is\nnot an issue here (the complexity of the classi\ufb01ers is\nrelatively low), this setup is important for a fair com-\nparison. If over\ufb01tting were an issue, the optimization\ncould also be carried out on a validation set, indepen-\ndent of both the test and the training sets. Optimizing\nthe TPR on the training set and plotting it on the test\nset also explains why the curves are non-monotonic.\nFigure 2 shows the results we obtained.\nAlthough\nthe di\ufb00erences are quite small, MDDAG outper-\nforms the three benchmarks algorithms consistently\nin the regime of low number base classi\ufb01ers, and\nit is competitive with them over the full range.\nMDDAG.I is slightly better at low complexities,\nwhereas MDDAG.EXP is more e\ufb00ective at a slightly\nhigher number of base classi\ufb01ers.\nThis is not sur-\nprising as in the low complexity regime the 0-1 error\nis more aggressive and closer to the measured TPR,\nwhereas when most of the instances are classi\ufb01ed cor-\nrectly, without the margin information it is impossible\nto improve the policy any further.\n3.3. Ranking with multi-class DAGs\nAlthough object detection is arguably the best-known\ntest-time-constrained problem, it is far from being\nunique.\nIn web page ranking, the problem is simi-\nlar: training time can be almost unlimited, but the\nlearned ranker must be fast to execute. State-of-the-\nart techniques often use thousands of trained models\nin an ensemble setup (Chapelle et al., 2011b), so ex-\ntracting lean rankers from the full models is an impor-\ntant practical issue. One of the di\ufb03culties in this case\nis that relevance labels may be non-binary, so classi-\ncal object-detection cascades cannot be applied. At\nthe same time, the principles used to design cascades\nre-surface also in this domain, although the setup is\nrather new and the algorithms require a fair amount\nFast classi\ufb01cation using sparse decision DAGs\n10\n1\n0.5\n0.6\n0.7\n0.8\n0.9\n1\n#base classifiers for negative class\nTPR\nVJ/FPR=0.02\n \n \nAdaBoost\nVJCascade\nFCBoost\nSoftCascade\nMDDAG.I\nMDDAG.EXP\n10\n1\n0.4\n0.45\n0.5\n0.55\n0.6\n0.65\nCBCL/FPR=0.1\n#base classifiers for negative class\nTPR\n10\n1\n0.5\n0.55\n0.6\n0.65\n0.7\n0.75\n0.8\nDPED/FPR=0.1\n#base classifiers for negative class\nTPR\n\uf001\uf002\uf003\uf002\uf004\uf005\n\uf006\uf007\uf008\uf009\uf009\uf00a\uf00b\uf008\uf006\uf002\n\uf006\uf007\uf008\uf009\uf009\uf00a\uf004\uf00c\uf004\uf00b\uf008\uf006\uf002\n\uf00d\n\uf00e\n\uf00f\n\uf010\n\uf011\n\uf012\n\uf013\n\uf014\n\uf015\n\uf00d\uf016\n\uf00d\uf00d\n\uf00d\uf00e\n\uf00d\uf00f\n\uf00d\uf010\n\uf00d\uf012\n\uf00d\uf011\n\uf00d\uf014\n\uf00e\uf00d\n\uf00d\uf015\n\uf00e\uf00e\n\uf00e\uf010\n\uf00e\uf012\n\uf00e\uf014\n\uf00e\uf015\n\uf00f\uf00e\nFigure 2. The true positive rate (TPR) vs. the average number of base classi\ufb01ers evaluated on negative test instances.\nThe results are computed for di\ufb00erent \ufb01xed false positive rates (FPR) shown in the title of panels. Lower panel: one of\nthe DAGs learned for the VJ database.\nof manual tuning (Cambazoglu et al., 2010). Despite\nthis, MDDAG can be used for this task as is.\nTo\nevaluate\nMDDAG\non\na\nmulti-class\nclassi-\n\ufb01cation/ranking\nproblem,\nwe\npresent\nresults\non\nthe MQ2007 and MQ2008 data sets taken from\nLETOR 4.0. In web page ranking, observations come\nin the form of query-document pairs, and the perfor-\nmance of the ranker is evaluated using tailor-made\nloss or gain functions that take as input the order-\ning of all the documents given a query.\nTo train a\nranker, query-document pairs come with manually an-\nnotated relevance labels that are usually multi-valued\n({0, 1, 2} in our case). One common performance mea-\nsure is the Normalized Discounted Cumulative Gain\n(NDCGm) (J\u00a8arvelin & Kek\u00a8al\u00a8ainen, 2002) which is\nbased on the \ufb01rst m documents in the order output\nby the ranker.\nWe used the averaged NDCG score\nndcg, provided by LETOR 4.0, that takes an average\nof the query-wise NDCGm values to evaluate the al-\ngorithms. In these experiments the base learners were\ndecision trees with eight leaves.\nThe goal of MDDAG is similar to the binary case,\nnamely to achieve a comparable performance to Ad-\naBoost.MH using fewer base learners. To make the\ncomparison fair, we employed the same calibration\nmethod to convert the output of the multi-class clas-\nsi\ufb01ers to a scoring function and then to a ranking (Li\net al., 2007).\nSince the goal this time was not de-\ntection, we simply evaluated the average NDCG for\neach run k to obtain (ndcgk, Nk) on the training set\nand (ndcg\u2032\nk, N \u2032\nk) on the test set.\nWe then selected\nk\u2217(N) = arg maxk:Nk\u2264N ndcgk and plotted the test\nndcg\u2032\nk\u2217(N) against N. Figure 3 tells us that MDDAG\nperforms as well as AdaBoost.MH with roughly two-\nfold savings in the number of base classi\ufb01ers.\n5\n10\n15\n20\n0.45\n0.46\n0.47\n0.48\n#evaluated base classifiers\nAverage NDCG\nMQ2008\n \n \nAdaBoost.MH\nMDDAG.I\nMDDAG.EXP\n5\n10\n15\n20\n0.43\n0.44\n0.45\n0.46\n0.47\n0.48\n0.49\n#evaluated base classifiers\nAverage NDCG\nMQ2007\n \n \nAdaBoost.MH\nMDDAG.I\nMDDAG.EXP\nFigure 3. The average NDCG vs. the average number of\nbase classi\ufb01ers evaluated on test queries.\n4. Related works\nBesides (P\u00b4oczos et al., 2009) and (Dulac-Arnold et al.,\n2011), MDDAG has several close relatives in the fam-\nily of supervised methods. It is obviously related to\nalgorithms taken from the vast array of sparse meth-\nFast classi\ufb01cation using sparse decision DAGs\nods. The main advantage here is that the MDP setup\nallows one to achieve sparsity in a dynamical data-\ndependent way. This feature relates the technique to\nunsupervised sparse coding (Lee et al., 2007; Ranzato\net al., 2007) rather than to sparse classi\ufb01cation or re-\ngression. On a more abstract level, MDDAG is also\nsimilar to (Larochelle & Hinton, 2010)\u2019s approach to\n\u201clearn where to look\u201d. Their goal is to \ufb01nd a sequence\nof two-dimensional features for classifying images in a\ndata-dependent way, whereas we do a similar search in\na one-dimensional ordered sequence of features.\n5. Conclusions\nIn this paper, we introduced an MDP-based design\nof decision DAGs. The output of the algorithm is a\ndata-dependent sparse classi\ufb01er, which means that ev-\nery instance \u201cchooses\u201d the base classi\ufb01ers or features\nthat it needs to predict its class index.\nThe algo-\nrithm is competitive with state-of-the-art cascade de-\ntectors on object detection benchmarks, and it is also\ndirectly applicable to test-time-constrained problems\ninvolving multi-class classi\ufb01cation, such as web page\nranking.\nHowever, in our view, the main bene\ufb01t of\nthe algorithm is not necessarily its performance, but\nits simplicity and versatility. First, MDDAG is ba-\nsically a turn-key procedure: it comes with one user-\nprovided hyperparameter with a clear semantics of di-\nrectly determining the accuracy-speed trade-o\ufb00. Sec-\nond, MDDAG can be readily applied to problems dif-\nferent from classi\ufb01cation by rede\ufb01ning the rewards on\nthe Quit and Eval actions.\nFor example, one can\neasily design regression or cost-sensitive classi\ufb01cation\nDAGs by using an appropriate reward in (6), or add\na weighting to (7) if the features have di\ufb00erent evalu-\nation costs.\nAcknowledgments\nThis work was supported by the ANR-2010-COSI-002\ngrant of the French National Research Agency.\nReferences\nBenbouzid, D., Busa-Fekete, R., Casagrande, N., Collin,\nF.-D., and K\u00b4egl, B. MultiBoost: a multi-purpose boost-\ning package. JMLR, 13:549\u2013553, 2012.\nBourdev, L. and Brandt, J. Robust object detection via\nsoft cascade. In CVPR, volume 2, pp. 236\u2013243, 2005.\nCambazoglu et al.\nEarly exit optimizations for additive\nmachine learned ranking systems. In WSDM, pp. 411\u2013\n420, 2010.\nChapelle, O., Chang, Y., and Liu, T.Y. Future directions\nin learning to rank. In JMLR W&CP, volume 14, pp.\n91\u2013100, 2011a.\nChapelle, O., Chang, Y., and Liu, T.Y. (eds.).\nYa-\nhoo! Learning-to-Rank Challenge, volume 14 of JMLR\nW&CP, 2011b.\nChapelle, Olivier and Chang, Yi.\nYahoo!\nLearning-to-\nRank Challenge overview. In JMLR W&CP, volume 14,\npp. 1\u201324, 2011.\nDavis, G., Mallat, S., and Avellaneda, M. Adaptive greedy\napproximations. Constructive Approximation, 13(1):57\u2013\n98, 1997.\nDulac-Arnold, G., Denoyer, L., Preux, P., and Gallinari,\nP. Datum-wise classi\ufb01cation: A sequential approach to\nsparsity. In ECML, 2011.\nEjov, V., Filar, J., and Gondzio, J.\nAn interior point\nheuristic for the Hamiltonian cycle problem via Markov\nDecision Processes. JGO, 29(3):315\u2013334, 2004.\nFreund, Y. and Schapire, R. E. A decision-theoretic gener-\nalization of on-line learning and an application to boost-\ning. JCSS, 55:119\u2013139, 1997.\nGligorov, V. A single track HLT1 trigger. Technical Report\nLHCb-PUB-2011-003, CERN, 2011.\nJ\u00a8arvelin, K. and Kek\u00a8al\u00a8ainen, J.\nCumulated gain-based\nevaluation of IR techniques.\nACM TIS, 20:422\u2013446,\n2002.\nLarochelle, H. and Hinton, G. Learning to combine foveal\nglimpses with a third-order Boltzmann machine.\nIn\nNIPS, pp. 1243\u20131251, 2010.\nLee, H., Battle, A., Raina, R., and Ng, A. Y.\nE\ufb03cient\nsparse coding algorithms. In NIPS, pp. 801\u2013808, 2007.\nLi, P., Burges, C., and Wu, Q. McRank: Learning to rank\nusing multiple classi\ufb01cation and gradient boosting. In\nNIPS, pp. 897\u2013904, 2007.\nMunder, S. and Gavrila, D. M.\nAn experimental study\non pedestrian classi\ufb01cation. IEEE PAMI, 28:1863\u20131868,\n2006.\nP\u00b4oczos, B., Abbasi-Yadkori, Y., Szepesv\u00b4ari, Cs., Greiner,\nR., and Sturtevant, N. Learning when to stop thinking\nand do something! In ICML, pp. 825\u2013832, 2009.\nRanzato, M., Poultney, C., Chopra, S., and LeCun, Y. Ef-\n\ufb01cient learning of sparse representations with an energy-\nbased model. In NIPS, pp. 1137\u20131144, 2007.\nRummery, G. A. and Niranjan, M. On-line Q-learning us-\ning connectionist systems. Technical Report CUED/F-\nINFENG/TR 166, Cambridge University, 1994.\nSaberian, M. and Vasconcelos, N. Boosting classi\ufb01er cas-\ncades. In NIPS, pp. 2047\u20132055, 2010.\nSchapire, R.E. and Singer, Y.\nImproved boosting al-\ngorithms using con\ufb01dence-rated predictions.\nMachine\nLearning, 37(3):297\u2013336, 1999.\nSutton, R.S. and Barto, A.G. Reinforcement learning: an\nintroduction. Adaptive computation and machine learn-\ning. MIT Press, 1998.\nSzepesv\u00b4ari, Cs.\nAlgorithms for Reinforcement Learning.\nMorgan and Claypool, 2010.\nViola, P. and Jones, M. Robust real-time face detection.\nIJCV, 57:137\u2013154, 2004.\n",
        "sentence": " Reinforcement Learning (RL) techniques which are usually devoted to problems in dynamic environments have been recently used for classical machine learning tasks like classification [5, 2]. At last, some links have already be done between classification and reinforcement learning algorithms [4, 2].",
        "context": "RL algorithms, temporal-di\ufb00erence (TD) learning al-\ngorithms are the most widely used. They can be di-\nvided into two groups: o\ufb00-policy and on-policy meth-\nods. In the case of o\ufb00-policy methods the policy search\ning. MIT Press, 1998.\nSzepesv\u00b4ari, Cs.\nAlgorithms for Reinforcement Learning.\nMorgan and Claypool, 2010.\nViola, P. and Jones, M. Robust real-time face detection.\nIJCV, 57:137\u2013154, 2004.\nRummery, G. A. and Niranjan, M. On-line Q-learning us-\ning connectionist systems. Technical Report CUED/F-\nINFENG/TR 166, Cambridge University, 1994.\nSaberian, M. and Vasconcelos, N. Boosting classi\ufb01er cas-\ncades. In NIPS, pp. 2047\u20132055, 2010."
    },
    {
        "title": "Text classification: A sequential reading approach",
        "author": [
            "Gabriel Dulac-Arnold",
            "Ludovic Denoyer",
            "Patrick Gallinari"
        ],
        "venue": "In Advances in Information Retrieval - 33rd European Conference on IR Research,",
        "citeRegEx": "3",
        "shortCiteRegEx": "3",
        "year": 2011,
        "abstract": "We propose to model the text classification process as a sequential decision\nprocess. In this process, an agent learns to classify documents into topics\nwhile reading the document sentences sequentially and learns to stop as soon as\nenough information was read for deciding. The proposed algorithm is based on a\nmodelisation of Text Classification as a Markov Decision Process and learns by\nusing Reinforcement Learning. Experiments on four different classical\nmono-label corpora show that the proposed approach performs comparably to\nclassical SVM approaches for large training sets, and better for small training\nsets. In addition, the model automatically adapts its reading process to the\nquantity of training information provided.",
        "full_text": "Text Classi\ufb01cation: A Sequential Reading\nApproach\nGabriel Dulac-Arnold, Ludovic Denoyer, and Patrick Gallinari\nUniversity Pierre et Marie Curie - UPMC, LIP6\nCase 169 - 4 Place Jussieu - 75005 PARIS - FRANCE\n\ufb01rstname.lastname@lip6.fr\nAbstract. We propose to model the text classi\ufb01cation process as a se-\nquential decision process. In this process, an agent learns to classify docu-\nments into topics while reading the document sentences sequentially and\nlearns to stop as soon as enough information was read for deciding. The\nproposed algorithm is based on a modelisation of Text Classi\ufb01cation as\na Markov Decision Process and learns by using Reinforcement Learning.\nExperiments on four di\ufb00erent classical mono-label corpora show that the\nproposed approach performs comparably to classical SVM approaches\nfor large training sets, and better for small training sets. In addition,\nthe model automatically adapts its reading process to the quantity of\ntraining information provided.\n1\nIntroduction\nText Classi\ufb01cation (TC) is the act of taking a set of labeled text documents,\nlearning a correlation between a document\u2019s contents and its corresponding la-\nbels, and then predicting the labels of a set of unlabeled test documents as best\nas possible. TC has been studied extensively, and is one of the older specialties\nof Information Retrieval. Classical statistical TC approaches are based on well-\nknown machine learning models such as generative models \u2014 Naive Bayes for\nexample [1][2] \u2014 or discriminant models such as Support Vector Machines [3].\nThey mainly consider the bag of words representation of a document (where the\norder of the words or sentences is lost) and try to compute a category score by\nlooking at the entire document content. Linear SVMs in particular \u2014 especially\nfor multi-label classi\ufb01cation with many binary SVMs \u2014 have been shown to\nwork particularly well [4]. Some major drawbacks to these global methods have\nbeen identi\ufb01ed in the literature:\n\u2013 These methods take into consideration a document\u2019s entire word set in order\nto decide to which categories it belongs. The underlying assumption is that\nthe category information is homogeneously dispatched inside the document.\nThis is well suited for corpora where documents are short, with little noise,\nso that global word frequencies can easily be correlated to topics. However,\nthese methods will not be well suited in predicting the categories of large\ndocuments where the topic information is concentrated in only a few sen-\ntences.\narXiv:1107.1322v3  [cs.AI]  29 Aug 2011\n2\nGabriel Dulac-Arnold, Ludovic Denoyer, and Patrick Gallinari\n\u2013 Additionally, for these methods to be applicable, the entire document must\nbe known at the time of classi\ufb01cation. In cases where there is a cost associated\nwith acquiring the textual information, methods that consider the entire\ndocument cannot be e\ufb03ciently or reliably applied as we do not know at\nwhat point their classi\ufb01cation decision is well-informed while considering\nonly a subset of the document.\nConsidering these drawbacks, some attempts have been made to use the se-\nquential nature of these documents for TC and similar problems such as passage\nclassi\ufb01cation. The earliest models developed especially for sequence processing\nextend Naive Bayes with Hidden Markov Models. Denoyer et al. [5] propose an\noriginal model which aims at modeling a document as a sequence of irrelevant\nand relevant sections relative to a particular topic. In [6], the authors propose\na model based on recurrent Neural Networks for document routing. Other ap-\nproaches have proposed to extend the use of linear SVMs to sequential data,\nmainly through the use of string kernels [7]. Finally, sequential models have\nbeen used for Information Extraction [8,9], passage classi\ufb01cation [10,11], or the\ndevelopment of search engines [12,13].\nWe propose a new model for Text Classi\ufb01cation that is less a\ufb00ected by the\naforementioned issues. Our approach models an agent that sequentially reads a\ntext document while concurrently deciding to assign topic labels. This is modeled\nas a sequential process whose goal is to classify a document by focusing on its\nrelevant sentences. The proposed model learns not only to classify a document\ninto one or many classes, but also when to label, and when to stop reading the\ndocument. This last point is very important because it means that the systems is\nable to learn to label a document with the correct categories as soon as possible,\nwithout reading the entire text.\nThe contributions of this paper are three-fold:\n1. We propose a new type of sequential model for text classi\ufb01cation based on\nthe idea of sequentially reading sentences and assigning topics to a document.\n2. Additionally, we propose an algorithm using Reinforcement Learning that\nlearns to focus on relevant sentences in the document. This algorithm also\nlearns when to stop reading a document so that the document is classi\ufb01ed\nas soon as possible. This characteristic can be useful for documents where\nsentence acquisition is expensive, such as large Web documents or conversa-\ntional documents.\n3. We show that on popular text classi\ufb01cation corpora our model outperforms\nclassical TC methods for small training sets and is equivalent to a baseline\nSVM for larger training sets while only reading a small portion of the doc-\numents. The model also shows its ability to classify by reading only a few\nsentences when the classi\ufb01cation problem is easy (large training sets) and to\nlearn to read more sentences when the task is harder (small training sets).\nThis document is organized as follows: In Section 2, we present an overview of our\nmethod. We formalize the algorithm as a Markov Decision Process in Section\n3 and detail the approach for both multi-label and mono-label TC. We then\npresent the set of experiments made on four di\ufb00erent text corpora in Section 4.\nText Classi\ufb01cation: A Sequential Reading Approach\n3\n2\nTask De\ufb01nition and General Principles of the Approach\nLet D denote the set of all possible textual documents, and Y the set of C\ncategories numbered from 1 to C. Each document d in D is associated with one\nor many1 categories of C. This label information is only known for a subset of\ndocuments Dtrain \u2282D called training documents, composed of Ntrain documents\ndenoted Dtrain = (d1, ..., dNtrain). The labels of document di are given by a vector\nof scores yi = (yi\n1, ..., yi\nC). We assume that:\nyi\nk =\n(\n1 if di belongs to category k\n0 otherwise\n.\n(1)\nThe goal of TC is to compute, for each document d in D, the corresponding\nscore for each category. The classi\ufb01cation function f\u03b8 with parameters \u03b8 is thus\nde\ufb01ned as :\nf\u03b8 :\n(\nD : [0; 1]C\nd \u2192yd\n.\n(2)\nLearning the classi\ufb01er consists in \ufb01nding an optimal parameterization \u03b8\u2217that\nreduces the mean loss such that:\n\u03b8\u2217= argmin\n\u03b8\n1\nNtrain\nNtrain\nX\ni=1\nL(f\u03b8(di), ydi),\n(3)\nwhere L is a loss function proportional to the classi\ufb01cation error of f\u03b8(di).\n2.1\nOverview of the approach\nThis section aims to provide an intuitive overview of our approach. The ideas\npresented here are formally presented in Section 3, and will only be described in\na cursory manner below.\nInference We propose to model the process of text classi\ufb01cation as a sequential\ndecision process. In this process, our classi\ufb01er reads a document sentence-by-\nsentence and can decide \u2014 at each step of the reading process \u2014 if the document\nbelongs to one of the possible categories. This classi\ufb01er can also chose to stop\nreading the document once it considers that the document has been correctly\ncategorized.\nIn the example described in Fig. 1, the task is to classify a document com-\nposed of 4 sentences. The documents starts o\ufb00unclassi\ufb01ed, and the classi\ufb01er\nbegins by reading the \ufb01rst sentence of the document. Because it considers that\nthe \ufb01rst sentence does not contain enough information to reliably classify the\ndocument, the classi\ufb01er decides to read the following sentence. Having now read\nthe \ufb01rst two sentences, the classi\ufb01er decides that it has enough information at\nhand to classify the document as cocoa.\n1 In this article, we consider both the mono-label classi\ufb01cation task, where each doc-\nument is associated with exactly one category, and the multi-label task where a\ndocument can be associated with several categories.\n4\nGabriel Dulac-Arnold, Ludovic Denoyer, and Patrick Gallinari\nThe dry period means the \ntemporao will be late this \nyear. Again it seems that \ncocoa delivered earlier on \nconsignment was \nincluded in the arrivals \nfigures. In view of the \nlower quality over recent \nweeks farmers have sold \na good part of their cocoa \nheld on consignment. \nComissaria Smith said \nspot bean prices rose to \n340 to 350 cruzados per \narroba of 15 kilos. \nRead Next\nRead  Next\nStop\nClassify as \ncocoa\nThe dry period means the \ntemporao will be late this \nyear. Again it seems that \ncocoa delivered earlier on \nconsignment was \nincluded in the arrivals \nfigures. In view of the \nlower quality over recent \nweeks farmers have sold \na good part of their cocoa \nheld on consignment. \nComissaria Smith said \nspot bean prices rose to \n340 to 350 cruzados per \narroba of 15 kilos. \nThe dry period means the \ntemporao will be late this \nyear. Again it seems that \ncocoa delivered earlier on \nconsignment was \nincluded in the arrivals \nfigures. In view of the \nlower quality over recent \nweeks farmers have sold \na good part of their cocoa \nheld on consignment.\nComissaria Smith said \nspot bean prices rose to \n340 to 350 cruzados per \narroba of 15 kilos. \n \nFig. 1. Inference on a document\nThe classi\ufb01er now reads the third sentence and \u2014 considering the informa-\ntion present in this sentence \u2014 decides that the reading process is \ufb01nished; the\ndocument is therefore classi\ufb01ed in the cocoa category.\nHad the document belonged to multiple classes, the classi\ufb01er could have\ncontinued to assign other categories to the document as additional information\nwas discovered.\nIn this example, the model took four actions: next, classify as cocoa, next and\nthen stop. The choice of each action was entirely dependent on the corresponding\nstate of the reading process. The choice of actions given the state, such as those\npicked while classifying the example document above, is called the policy of\nthe classi\ufb01er. This policy \u2014 denoted \u03c0 \u2014 consists of a mapping of states to\nactions relative to a score. This score is called a Q-value \u2014 denoted Q(s, a) \u2014\nand re\ufb02ects the worth of choosing action a during state s of the process. Using\nthe Q-value, the inference process can be seen as a greedy process which, for\neach timestep, chooses the best action a\u2217de\ufb01ned as the action with the highest\nscore w.r.t. Q(s, a):\na\u2217= argmax\na\nQ(s, a).\n(4)\nTraining The learning process consists in computing a Q-function2 which\nminimizes the classi\ufb01cation loss (as in equation (3)) of the documents in the\ntraining set. The learning procedure uses a monte-carlo approach to \ufb01nd a set of\ngood and bad actions relative to each state. Good actions are actions that result\nin a small classi\ufb01cation loss for a document. The good and bad actions are then\nlearned by a statistical classi\ufb01er, such as an SVM.\nAn example of the training procedure on the same example document as\nabove is illustrated in Fig 2. To begin with, a random state of the classi\ufb01cation\nprocess is picked. Then, for each action possible in that state, the current policy\nis run until it stops and the \ufb01nal classi\ufb01cation loss is computed. The training\nalgorithm then builds a set of good actions \u2014 the actions for which the simu-\nlation obtains the minimum loss value \u2014 and a set of remaining bad actions.\n2 The Q-function is an approximation of Q(s, a).\nText Classi\ufb01cation: A Sequential Reading Approach\n5\nThis is repeated on many di\ufb00erent states and training documents until, at last,\nthe model learns a classi\ufb01er able to discriminate between good and bad actions\nrelative to the current state.\nThe dry period means \nthe temporao will be \nlate this year. Again it \nseems that cocoa \ndelivered earlier on \nconsignment was \nincluded in the arrivals \nfigures. In view of the \nlower quality over \nrecent weeks farmers \nhave sold a good part \nof their cocoa held on \nconsignment. \nComissaria Smith said \nspot bean prices rose \nto 340 to 350 cruzados \nper arroba of 15 kilos. \nThe dry period means \nthe temporao will be \nlate this year. Again it \nseems that cocoa \ndelivered earlier on \nconsignment was \nincluded in the arrivals \nfigures. In view of the \nlower quality over \nrecent weeks farmers \nhave sold a good part \nof their cocoa held on \nconsignment. \nComissaria Smith said \nspot bean prices rose \nto 340 to 350 cruzados \nper arroba of 15 kilos. \nRead Next\nStop\nClassify as \ncocoa\nClassify \nas acq\nNext\nStop\nLoss = 0.5 \nLoss = 0 \nLoss = 1 \nLoss = 1 \nSampling a state \nover the training \ndocument.\nEnumerating \nall the \npossible \nactions.\nSimulating the current policy.\nComputing the \nclassification \nloss at the end \nof the process.\nLabeling the actions \nas good action (+1) \nand bad action (-1)\nGood action \n (+1)\nBad action  \n(-1)\nBad action  \n(-1)\nBad action \n(-1)\nThe actions are used \nas training examples \nfor a classical \nclassifier.\nFig. 2. Learning the sequential model. The di\ufb00erent steps of one learning iteration are\nillustrated from left to right on a single training document.\n2.2\nPreliminaries\nWe have presented the principles of our approach and given an intuitive de-\nscription of the inference and learning procedures. We will now formalize this\nalgorithm as a Markov Decision Process (MDP) for which an optimal policy is\nfound via Reinforcement Learning. Note that we will only go over notations per-\ntinent to our approach, and that this section lacks many MDP or Reinforcement\nLearning de\ufb01nitions that are not necessary for our explanation.\nMarkov Decision Process A Markov Decision Process (MDP) is a math-\nematical formalism to model sequential decision processes. We only consider\ndeterministic MDPs, de\ufb01ned by a 4-tuple: (S, A, T, r). Here, S is the set of pos-\nsible states, A is the is the set of possible actions, and T : S \u00d7A \u2192S is the state\ntransition function such that T(s, a) \u2192s\u2032 (this symbolizes the system moving\nfrom state s to state s\u2032 by applying action a). The reward, r : S \u00d7 A \u2192R,\nis a value that re\ufb02ects the quality of taking action a in state s relative to the\nagent\u2019s ultimate goal. We will use A(s) \u2286A to refer to the set of possible actions\navailable to an agent in a particular state s.\nAn agent interacts with the MDP by starting o\ufb00in a state s \u2208S. The agent\nthen chooses an action a \u2208A(s) which moves it to a new state s\u2032 by applying\n6\nGabriel Dulac-Arnold, Ludovic Denoyer, and Patrick Gallinari\nthe transition T(s, a). The agent obtains a reward r(s, a) and then continues the\nprocess until it reaches a terminal state sfinal where the set of possible actions\nis empty i.e A(sfinal) = \u2205.\nReinforcement Learning Let us de\ufb01ne \u03c0 : S \u2192A, a stochastic policy such\nthat \u2200a \u2208A(s), \u03c0(s) = a with probability P(a|s). The goal of RL is to \ufb01nd an\noptimal policy \u03c0\u2217that maximizes the cumulative reward obtain by an agent.\nWe consider here the \ufb01nite-horizon context for which the cumulative reward\ncorresponds to the sum of the reward obtained at each step by the system,\nfollowing the policy \u03c0. The goal of Reinforcement Learning is to \ufb01nd an optimal\npolicy denoted \u03c0\u2217which maximizes the cumulative reward obtained for all the\nstates of the process i.e.:\n\u03c0\u2217= argmax\n\u03c0\nX\ns0\u2208S\nE\u03c0[\nT\nX\nt=0\nr(st, at)].\n(5)\nMany algorithms have been developed for \ufb01nding such a policy, depending\non the assumptions made on the structure of the MDP, the nature of the states\n(discrete or continuous), etc. In many approaches, a policy \u03c0 is de\ufb01ned through\nthe use of a Q-function which re\ufb02ects how much reward one can expect by\ntaking action a on state s. With such a function, the policy \u03c0 is de\ufb01ned as:\n\u03c0 = argmax\na\u2208A(s)\nQ(s, a).\n(6)\nIn such a case, the learning problem consists in \ufb01nding the optimal value Q\u2217\nwhich results in the optimal policy \u03c0\u2217.\nDue to the very large number of states we are facing in our approach, we\nconsider the Approximated Reinforcement Learning context where the Q func-\ntion is approximated by a parameterized function Q\u03b8(s, a), where \u03b8 is a set of\nparameters such that:\nQ\u03b8(s, a) =< \u03b8, \u03a6(s, a) >,\n(7)\nwhere < \u00b7, \u00b7 > denotes the dot product and \u03a6(s, a) is a feature vector representing\nthe state-action pair (s, a). The learning problem consists in \ufb01nding the optimal\nparameters \u03b8\u2217that results in an optimal policy:\n\u03c0\u2217= argmax\na\u2208A(s)\n< \u03b8\u2217, \u03a6(s, a) > .\n(8)\n3\nText Classi\ufb01cation as a Sequential Decision Problem\nFormally, we consider that a document d is composed of a sequence of sentences\nsuch that d = (\u03b4d\n1 , . . . , \u03b4d\nnd ), where \u03b4d\ni is the i-th sentence of the document and\nnd is the total number of sentences making up the document. Each sentence \u03b4d\ni\nhas a corresponding feature vector \u2014 a normalized tf-idf vector in our case \u2014\nthat describes its content.\nText Classi\ufb01cation: A Sequential Reading Approach\n7\n3.1\nMDP for Multi-Label Text Classi\ufb01cation\nWe can describe our sequential decision problem using an MDP. Below, we de-\nscribe the MDP for the multilabel classi\ufb01cation problem, of which monolabel\nclassi\ufb01cation is just a speci\ufb01c instance:\n\u2013 Each state s is a triplet (d, p, \u02c6y) such that:\n\u2022 d is the document the agent is currently reading.\n\u2022 p \u2208[1, nd] corresponds to the current sentence being read; this implies\nthat \u03b4d\n1 to \u03b4d\np\u22121 have already been read.\n\u2022 \u02c6y is the set of currently assigned categories \u2014 categories previously as-\nsigned by the agent during the current reading process \u2014 where \u02c6yk = 1\ni\ufb00the document has been assigned to category k during the reading\nprocess, 0 otherwise.\n\u2013 The set of actions A(s) is composed of:\n\u2022 One or many classi\ufb01cation actions denoted classify as k for each cate-\ngory k where \u02c6yk = 0. These actions correspond to assigning document d\nto category k.\n\u2022 A next sentence action denoted next which corresponds to reading the\nnext sentence of the document.\n\u2022 A stop action denoted stop which corresponds to \ufb01nishing the reading\nprocess.\n\u2013 The set of transitions T(s, a) act such that:\n\u2022 T(s, classify as k) sets \u02c6yk \u21901.\n\u2022 T(s, next) sets p \u2190p + 1.\n\u2022 T(s, stop) halts the decision process.\n\u2013 The reward r(s, a) is de\ufb01ned as:\nr(s, a) =\n\u001a\nF1(y, \u02c6y) if a is a stop action\n0\notherwise\n,\n(9)\nwhere y is the real vector of categories for d and \u02c6y is the predicted vector of\ncategories at the end of the classi\ufb01cation process. The F1 score of a single\ndocument is de\ufb01ned as:\nF1(y, \u02c6y) = 2 \u00b7 p(y, \u02c6y) \u00b7 r(y, \u02c6y)\np(y, \u02c6y) + r(y, \u02c6y)\n(10)\nwith\n(11)\np(y, \u02c6y) =\nC\nX\nk=0\n1( \u02c6yk = yk)/\nC\nX\nk=0\n\u02c6yk and r(y, \u02c6y) =\nC\nX\nk=0\n1( \u02c6yk = yk)/\nC\nX\nk=0\nyk. (12)\nMDP for Mono-Label Text Classi\ufb01cation In mono-label classi\ufb01cation, we\nrestrict the set of possible actions. The classify as k action leads to a stopping\nstate such that A(s) = {stop}. This brings the episode to an end after the attri-\nbution of a single label. Note that in the case of a mono-label system \u2014 where\nonly one category can be assigned to a document \u2014 the reward corresponds to a\nclassical accuracy measure: 1 if the chosen category is correct, and 0 otherwise.\n8\nGabriel Dulac-Arnold, Ludovic Denoyer, and Patrick Gallinari\n3.2\nFeatures over states\nWe must now de\ufb01ne a feature function which provides a vector representation of\na state-action pair (s, a). The purpose of this vector is to be able to present (s, a)\nto a statistical classi\ufb01er to know whether it is good or bad. Comparing the scores\nof various (s, a) pairs for a given state s allows us to choose the best action for\nthat state.\nClassical text classi\ufb01cation methods only represent documents by a global \u2014\nand usually tf-idf weighted \u2014 vector. We choose, however, to include not only\na global representation of the sentences read so far, but also a local component\ncorresponding to the most recently read sentence. Moreover, while in state s, a\ndocument may have been already assigned to a set of categories; the global fea-\nture vector \u03a6(s, a) must describe all this information. The vector representation\nof a state s is thus de\ufb01ned as \u03a6(s):\n\u03a6(s) =\n\uf8eb\n\uf8ec\n\uf8ec\n\uf8ed\npP\ni=1\n\u03b4d\ni\np\n\u03b4d\np \u02c6y0 . . . \u02c6\nyC\n\uf8f6\n\uf8f7\n\uf8f7\n\uf8f8.\n(13)\n\u03a6(s) is the concatenation of a set of sub-vectors describing: the mean of the\nfeature vectors of the read sentences, the feature vector of the last sentence, and\nthe set of already assigned categories.\nIn order to include the action information, we use the block-vector trick\nintroduced by [14] which consists in projecting \u03a6(s) into a higher dimensional\nspace such that:\n\u03a6(s, a) = (0 . . . \u03c6(s) . . . 0) .\n(14)\nThe position of \u03a6(s) inside the global vector \u03a6(s, a) is dependent on action a.\nThis results in a very high dimensional space which is easier to classify in with\na linear model.\n3.3\nLearning and Finding the optimal classi\ufb01cation policy\nIn order to \ufb01nd the best classi\ufb01cation policy, we used a recent Reinforcement\nLearning algorithm called Approximate Policy Iteration with Rollouts. In brief,\nthis method uses a monte-carlo approach to evaluate the quality of all the pos-\nsible actions amongst some random sampled states, and then learns a classi\ufb01er\nwhose goal is to discriminate between the good and bad actions relative to each\nstate. Due to a lack of space, we do not detail the learning procedure here and\nrefer to the paper by Lagoudakis et al [15]. An intuitive description of the pro-\ncedure is given in Section 2.1.\n4\nExperimental Results\nWe have applied our model on four di\ufb00erent popular datasets: three mono-label\nand one multi-label. All datasets were pre-processed in the same manner: all\npunctuation except for periods were removed, SMART stop-words[16] and words\nless than three characters long were removed, and all words were stemmed with\nText Classi\ufb01cation: A Sequential Reading Approach\n9\nPorter stemming. Baseline evaluations were performed with libSVM[17] on nor-\nmalized tf-idf weighted vectorial representations of each document as has been\ndone in [3]. Published performance benchmarks can be found in [18] and [19].\nThe datasets are:\n\u2013 The Reuters-215783 dataset which provides two corpora:\n\u2022 The Reuters8 corpus, a mono-label corpus composed of the 8 largest\ncategories.\n\u2022 The Reuters10 corpus, a multi-label corpus composed of the 10 largest\ncategories.\n\u2013 The WebKB4[20] dataset is a mono-label corpus composed of Web pages\ndispatched into 4 di\ufb00erent categories.\n\u2013 The 20 Newsgroups5 (20NG) dataset is a mono-label corpus of news com-\nposed of 20 classes.\nCorpus\nNb of documents Nb of categories Nb of sentences by doc. Task\nR8\n7678\n8\n8.19\nMono-label\nR10\n12 902\n10\n9.13\nMulti-label\nNewsgroup 18 846\n20\n22.34\nMono-label\nWebKB\n4 177\n4\n42.36\nMono-label\nTable 1. Corpora statistics.\n4.1\nEvaluation Protocol\nMany classi\ufb01cation systems are soft classi\ufb01cation systems that compute a score\nfor each possible category-document pair. Our system is a hard classi\ufb01cation\nsystem that assigns a document to one or many categories, with a score of either 1\nor 0. The evaluation measures used in the litterature, such as the breakeven point,\nare not suitable for hard classi\ufb01cation models and cannot be used to evaluate\nand compare our approach with other methods. We have therefore chosen to\nuse the micro-F1 and macro-F1 measures. These measures correspond to a\nclassical F1 score computed for each category and averaged over the categories.\nThe macro-F1 measure does not take into account the size of the categories,\nwhereas the micro-F1 average is weighted by the size of each category. We\naveraged the di\ufb00erent models\u2019 performances on various train/test splits that\nwere randomly generated from the original dataset. We used the same approach\nboth for evaluating our approach and the baseline approaches to be able to\ncompare our results properly. For each training size, the performance of the\nmodels were averaged over 5 runs. The hyper-parameters of the SVM and the\nhyper-parameters of the RL-based approach were manually tuned. What we\npresent here are the best results obtained over the various parameter choices\nwe tested. For the RL approach, each policy was learned on 10,000 randomly\ngenerated states, with 1 rollout per state, using a random initial policy. It is\nimportant to note that, in a practical sense, the RL method is not much more\ncomplicated to tune than a classical SVM since it is rather robust regarding the\nvalues of the hyper-parameters.\n3 http://web.ist.utl.pt/%7Eacardoso/datasets/\n4 http://www.cs.cmu.edu/afs/cs.cmu.edu/project/theo-20/www/data/\n5 http://people.csail.mit.edu/jrennie/20Newsgroups/\n10\nGabriel Dulac-Arnold, Ludovic Denoyer, and Patrick Gallinari\n4.2\nExperimental Results\nOur performance \ufb01gures use SVM to denote baseline Support Vector Machine\nperformance, and STC (Sequential Text Classi\ufb01cation) to denote our approach.\nIn the case of the mono-label experiments (Figure 3 and 4-left), performance of\nboth the SVM method and our method are comparable. It is important to note,\nhowever, that in the case of small training sizes (1%, 5%), the STC approach\noutperforms SVM by 1-10% depending on the corpus. For example, on the R8\ndataset we can see that for both F1 scores, STC is better by \u223c5% with a\ntraining size of 1%. This is also visible with the NewsGroup dataset, where STC\nis better by 10% for both metrics using a 1% training set. This shows that STC\nis particularly advantageous with small training sets.\nThe reading process\u2019 behaviour is explored in Figure 5. Here, Reading Size\ncorresponds to the mean percentage of sentences read for each document6. We\ncan see that Reading Size decreases as the training size gets bigger for mono-label\ncorpora. This is due to the fact that the smaller training sizes are harder to learn,\nand therefore the agent needs more information to properly label documents. In\nthe right-hand side of Figure 5, we can see a histogram of number of documents\ngrouped by Reading Size. We notice that although there is a mean Reading Size\nof 41%, most of the documents are barely read, with a few outliers that are read\nuntil the end. The agent is clearly capable of choosing to read more or less of\nthe document depending on its content.\nIn the case of multi-label classi\ufb01cation, results are quite di\ufb00erent. First, we see\nthat for the R10 corpus, our model\u2019s performance is lower than the baseline on\nlarge training sets. Moreover, the multi-label model reads all the sentences of the\ndocument during the classi\ufb01cation process. This behaviour seems normal because\nwhen dealing with multi-label documents, one cannot be sure that the remaining\nsentences will not contain relevant information pertaining to a particular topic.\nWe hypothesize that the lower performances are due to the much larger action\nspace in the multi-label problem, and the fact that we are learning a single model\nfor all classes instead of one independent models per class.\n5\nConclusions\nWe have presented a new model that learns to classify by sequentially reading\nthe sentences of a document, and which labels this document as soon as it has\ncollected enough information. This method shows some interesting properties\non di\ufb00erent datasets. Particularly in mono-label TC, the model automatically\nlearns to read only a small part of the documents when the training set is large,\nand the whole documents when the training set is small. It is thus able to adapt\nits behaviour to the di\ufb03culty of the classi\ufb01cation task, which results in obtaining\nfaster systems for easier problems. The performances obtained are close to the\nperformance of a baseline SVM model for large training sets, and better for small\ntraining sets.\n6 If li is the number of sentences in document i read during the classi\ufb01cation process,\nand ni is the total number of sentences in this document. Let N be the number of\ntest documents, then the reading size value is\n1\nN\nP\ni\nli\nni .\nText Classi\ufb01cation: A Sequential Reading Approach\n11\n40%\n50%\n60%\n70%\n80%\n90%\n100%\n110%\n1%\n5%\n10%\n30%\n50%\n90%\nPerformance\nTraining Size\nSVM - Micro F1\nSTC - Micro F1\nSVM - Macro F1\nSTC - Macro F1\n30%\n40%\n50%\n60%\n70%\n80%\n90%\n100%\n1%\n5%\n10%\n30%\n50%\n90%\nPerformance\nTraining Size\nSVM - Micro F1\nSTC - Micro F1\nSVM - Macro F1\nSTC - Macro F1\nFig. 3. Performances over the R8 Corpus (left) and NewsGroup Corpus (right)\n0%\n10%\n20%\n30%\n40%\n50%\n60%\n70%\n80%\n90%\n100%\n1%\n5%\n10%\n30%\n50%\n90%\nPerformance\nTraining Size\nSVM - Micro F1\nSTC - Micro F1\nSVM - Macro F1\nSTC - Macro F1\n30%\n40%\n50%\n60%\n70%\n80%\n90%\n100%\n1%\n5%\n10%\n30%\n50%\n90%\nPerformance\nTraining Size\nSVM - Micro F1\nSTC - Micro F1\nSVM - Macro F1\nSTC - Macro F1\nFig. 4. Performances over the WebKB Corpus (left) and R10 Corpus (right)\n0%\n20%\n40%\n60%\n80%\n100%\n120%\nMicro \nF1\nReading \nSize\nMicro \nF1\nReading \nSize\nMicro \nF1\nReading \nSize\nMicro \nF1\nReading \nSize\nR8\nNewsGroup\nWebKB\nR10\nPerformance / Reading Size\n0.01\n0.05\n0.1\n0.3\n0.5\n0.9\n0\n500\n1000\n1500\n2000\n2500\n[0-0.1] [0.1-0.2][0.2-0.3][0.3-0.4][0.4-0.5][0.5-0.6][0.6-0.7][0.7-0.8][0.8-0.9][0.9-1.0]\nNumber of Documents (test)\nReading Size\nFig. 5. Overview of the Reading Sizes for all the corpora (left). Number of documents\nand Reading Sizes on R8 with 30% of documents as a training set (right).\n12\nGabriel Dulac-Arnold, Ludovic Denoyer, and Patrick Gallinari\nThis work opens many new perspectives in the Text Classi\ufb01cation domain.\nParticularly, it is possible to imagine some additional MDP actions for the clas-\nsi\ufb01cation agent allowing the agent to parse the document in a more complex\nmanner. For example, this idea can be extended to learn to classify XML docu-\nments reading only the relevant parts.\nAcknowledgments\nThis work was partially supported by the French National Agency of Research\n(Lampada ANR-09-EMER-007).\nReferences\n1. D. Lewis and M. Ringuette, \u201cA comparison of two learning algorithms for text\ncategorization,\u201d Third annual symposium on document analysis, pp. 1\u201314, 1994.\n2. D. Lewis, R. Schapire, J. Callan, and R, \u201cTraining algorithms for linear text clas-\nsi\ufb01ers,\u201d ACM SIGIR, pp. 120\u2013123, 1996.\n3. T. Joachims, \u201cText categorization with support vector machines: Learning with\nmany relevant features,\u201d Machine Learning: ECML-98, 1998.\n4. S. Dumais, J. Platt, D. Heckerman, and M, \u201cInductive learning algorithms and\nrepresentations for text categorization,\u201d Proceedings of CIKM, 1998.\n5. L. Denoyer, H. Zaragoza, and P. Gallinari, \u201cHMM-based passage models for doc-\nument classi\ufb01cation and ranking,\u201d in Proceedings of ECIR-01, 2001, pp. 126\u2013135.\n6. S. Wermter, G. Arevian, and C. Panchev, \u201cRecurrent neural network learning for\ntext routing,\u201d vol. 2, 1999, pp. 898 \u2013903 vol.2.\n7. H. Lodhi, C. Saunders, J. Shawe-Taylor, N. Cristianini, and C. Watkins, \u201cText\nclassi\ufb01cation using string kernels,\u201d J. Mach. Learn. Res., vol. 2, pp. 419\u2013444, 2002.\n8. T. R. Leek, \u201cInformation extraction using hidden markov models,\u201d 1997.\n9. M.-R. Amini, H. Zaragoza, and P. Gallinari, \u201cLearning for sequence extraction\ntasks,\u201d in RIAO, 2000, pp. 476\u2013490.\n10. M. Kaszkiel, J. Zobel, and R. Sacks-Davis, \u201cE\ufb03cient passage ranking for document\ndatabases,\u201d ACM Trans. Inf. Syst., vol. 17, no. 4, pp. 406\u2013439, 1999.\n11. J. Jiang and C. Zhai, \u201cExtraction of coherent relevant passages using hidden\nmarkov models,\u201d ACM Trans. Inf. Syst., vol. 24, no. 3, pp. 295\u2013319, 2006.\n12. D. R. H. Miller, T. Leek, and R. M. Schwartz, \u201cBbn at trec7: Using hidden markov\nmodels for information retrieval,\u201d in In Proceedings of TREC-7, 1999, pp. 133\u2013142.\n13. M. Bendersky and O. Kurland, \u201cUtilizing passage-based language models for doc-\nument retrieval,\u201d in ECIR\u201908, 2008, pp. 162\u2013174.\n14. S. Har-Peled, D. Roth, and D. Zimak, \u201cConstraint classi\ufb01cation: A new approach\nto multiclass classi\ufb01cation,\u201d Algorithmic Learning Theory, pp. 1 \u2013 11, 2002.\n15. M. G. Lagoudakis and R. Parr, \u201cReinforcement learning as classi\ufb01cation: Leverag-\ning modern classi\ufb01ers,\u201d ICML, 2003.\n16. G. Salton, Ed., The SMART Retrieval System - Experiments in Automatic Docu-\nment Processing.\nEnglewood, Cli\ufb00s, New Jersey: Prentice Hall, 1971.\n17. C.-C. Chang and C.-J. Lin, \u201cLIBSVM: a library for SVMs,\u201d 2001.\n18. F. Sebastiani, \u201cMachine learning in automated text categorization,\u201d ACM Com-\nputing Surveys, vol. 34, no. 1, pp. 1\u201347, Mar. 2002.\n19. M. A. Kumar and M. Gopal, \u201cA comparison study on multiple binary-class SVM\nmethods for unilabel text categorization,\u201d Pattern Recognition Letters, vol. 31,\nno. 11, pp. 1437\u20131444, Aug. 2010.\n20. M. Craven, D. DiPasquo, D. Freitag, A. McCallum, T. Mitchell, K. Nigam, and\nS. Slattery, \u201cLearning to extract symbolic knowledge from the World Wide Web,\u201d\nWorld, 1998.\n",
        "sentence": " For example [3] and [12] consider that the sequential process is an acquisition process able to focus on relevant parts of the input data; [9] for example focuses on the sequential prediction process with a cascade approach. They also share the idea of processing different inputs with different computations which is the a major idea underlying decision trees [13] and also more recent classification techniques like [3].",
        "context": "quential nature of these documents for TC and similar problems such as passage\nclassi\ufb01cation. The earliest models developed especially for sequence processing\nextend Naive Bayes with Hidden Markov Models. Denoyer et al. [5] propose an\npresented here are formally presented in Section 3, and will only be described in\na cursory manner below.\nInference We propose to model the process of text classi\ufb01cation as a sequential\ndecision process. In this process, our classi\ufb01er reads a document sentence-by-\nsentence and can decide \u2014 at each step of the reading process \u2014 if the document\nbelongs to one of the possible categories. This classi\ufb01er can also chose to stop"
    },
    {
        "title": "Sequential approaches for learning datum-wise sparse representations",
        "author": [
            "Gabriel Dulac-Arnold",
            "Ludovic Denoyer",
            "Philippe Preux",
            "Patrick Gallinari"
        ],
        "venue": "Machine Learning,",
        "citeRegEx": "4",
        "shortCiteRegEx": "4",
        "year": 2012,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " RL opens now some interesting research directions for classical ML tasks and allows one to imagine solutions to complex problems like budgeted classification [4] or anytime prediction [6]. At last, some links have already be done between classification and reinforcement learning algorithms [4, 2].",
        "context": null
    },
    {
        "title": "Sequentially generated instance-dependent image representations for classification",
        "author": [
            "Gabriel Dulac-Arnold",
            "Ludovic Denoyer",
            "Nicolas Thome",
            "Matthieu Cord",
            "Patrick Gallinari"
        ],
        "venue": "Internation Conference on Learning Representations - ICLR 2014,",
        "citeRegEx": "5",
        "shortCiteRegEx": "5",
        "year": 2014,
        "abstract": "",
        "full_text": "",
        "sentence": " Reinforcement Learning (RL) techniques which are usually devoted to problems in dynamic environments have been recently used for classical machine learning tasks like classification [5, 2].",
        "context": null
    },
    {
        "title": "Learning to segment from a few well-selected training images",
        "author": [
            "Alireza Farhangfar",
            "Russell Greiner",
            "Csaba Szepesv\u00e1ri"
        ],
        "venue": "In Proceedings of the 26th Annual International Conference on Machine Learning,",
        "citeRegEx": "6",
        "shortCiteRegEx": "6",
        "year": 2009,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " RL opens now some interesting research directions for classical ML tasks and allows one to imagine solutions to complex problems like budgeted classification [4] or anytime prediction [6].",
        "context": null
    },
    {
        "title": "Speech recognition with deep recurrent neural networks",
        "author": [
            "Alex Graves",
            "Abdel-rahman Mohamed",
            "Geoffrey E. Hinton"
        ],
        "venue": "In IEEE International Conference on Acoustics, Speech and Signal Processing,",
        "citeRegEx": "7",
        "shortCiteRegEx": "7",
        "year": 2013,
        "abstract": "Recurrent neural networks (RNNs) are a powerful model for sequential data.\nEnd-to-end training methods such as Connectionist Temporal Classification make\nit possible to train RNNs for sequence labelling problems where the\ninput-output alignment is unknown. The combination of these methods with the\nLong Short-term Memory RNN architecture has proved particularly fruitful,\ndelivering state-of-the-art results in cursive handwriting recognition. However\nRNN performance in speech recognition has so far been disappointing, with\nbetter results returned by deep feedforward networks. This paper investigates\n\\emph{deep recurrent neural networks}, which combine the multiple levels of\nrepresentation that have proved so effective in deep networks with the flexible\nuse of long range context that empowers RNNs. When trained end-to-end with\nsuitable regularisation, we find that deep Long Short-term Memory RNNs achieve\na test set error of 17.7% on the TIMIT phoneme recognition benchmark, which to\nour knowledge is the best recorded score.",
        "full_text": "SPEECH RECOGNITION WITH DEEP RECURRENT NEURAL NETWORKS\nAlex Graves, Abdel-rahman Mohamed and Geoffrey Hinton\nDepartment of Computer Science, University of Toronto\nABSTRACT\nRecurrent neural networks (RNNs) are a powerful model for\nsequential data. End-to-end training methods such as Connec-\ntionist Temporal Classi\ufb01cation make it possible to train RNNs\nfor sequence labelling problems where the input-output align-\nment is unknown. The combination of these methods with\nthe Long Short-term Memory RNN architecture has proved\nparticularly fruitful, delivering state-of-the-art results in cur-\nsive handwriting recognition. However RNN performance in\nspeech recognition has so far been disappointing, with better\nresults returned by deep feedforward networks. This paper in-\nvestigates deep recurrent neural networks, which combine the\nmultiple levels of representation that have proved so effective\nin deep networks with the \ufb02exible use of long range context\nthat empowers RNNs. When trained end-to-end with suit-\nable regularisation, we \ufb01nd that deep Long Short-term Mem-\nory RNNs achieve a test set error of 17.7% on the TIMIT\nphoneme recognition benchmark, which to our knowledge is\nthe best recorded score.\nIndex Terms\u2014 recurrent neural networks, deep neural\nnetworks, speech recognition\n1. INTRODUCTION\nNeural networks have a long history in speech recognition,\nusually in combination with hidden Markov models [1, 2].\nThey have gained attention in recent years with the dramatic\nimprovements in acoustic modelling yielded by deep feed-\nforward networks [3, 4]. Given that speech is an inherently\ndynamic process, it seems natural to consider recurrent neu-\nral networks (RNNs) as an alternative model. HMM-RNN\nsystems [5] have also seen a recent revival [6, 7], but do not\ncurrently perform as well as deep networks.\nInstead of combining RNNs with HMMs, it is possible\nto train RNNs \u2018end-to-end\u2019 for speech recognition [8, 9, 10].\nThis approach exploits the larger state-space and richer dy-\nnamics of RNNs compared to HMMs, and avoids the prob-\nlem of using potentially incorrect alignments as training tar-\ngets. The combination of Long Short-term Memory [11], an\nRNN architecture with an improved memory, with end-to-end\ntraining has proved especially effective for cursive handwrit-\ning recognition [12, 13]. However it has so far made little\nimpact on speech recognition.\nRNNs are inherently deep in time, since their hidden state\nis a function of all previous hidden states. The question that\ninspired this paper was whether RNNs could also bene\ufb01t from\ndepth in space; that is from stacking multiple recurrent hid-\nden layers on top of each other, just as feedforward layers are\nstacked in conventional deep networks. To answer this ques-\ntion we introduce deep Long Short-term Memory RNNs and\nassess their potential for speech recognition. We also present\nan enhancement to a recently introduced end-to-end learning\nmethod that jointly trains two separate RNNs as acoustic and\nlinguistic models [10]. Sections 2 and 3 describe the network\narchitectures and training methods, Section 4 provides exper-\nimental results and concluding remarks are given in Section 5.\n2. RECURRENT NEURAL NETWORKS\nGiven an input sequence x = (x1, . . . , xT ), a standard recur-\nrent neural network (RNN) computes the hidden vector se-\nquence h = (h1, . . . , hT ) and output vector sequence y =\n(y1, . . . , yT ) by iterating the following equations from t = 1\nto T:\nht = H (Wxhxt + Whhht\u22121 + bh)\n(1)\nyt = Whyht + by\n(2)\nwhere the W terms denote weight matrices (e.g. Wxh is the\ninput-hidden weight matrix), the b terms denote bias vectors\n(e.g. bh is hidden bias vector) and H is the hidden layer func-\ntion.\nH is usually an elementwise application of a sigmoid\nfunction. However we have found that the Long Short-Term\nMemory (LSTM) architecture [11], which uses purpose-built\nmemory cells to store information, is better at \ufb01nding and ex-\nploiting long range context. Fig. 1 illustrates a single LSTM\nmemory cell. For the version of LSTM used in this paper [14]\nH is implemented by the following composite function:\nit = \u03c3 (Wxixt + Whiht\u22121 + Wcict\u22121 + bi)\n(3)\nft = \u03c3 (Wxfxt + Whfht\u22121 + Wcfct\u22121 + bf)\n(4)\nct = ftct\u22121 + it tanh (Wxcxt + Whcht\u22121 + bc)\n(5)\not = \u03c3 (Wxoxt + Whoht\u22121 + Wcoct + bo)\n(6)\nht = ot tanh(ct)\n(7)\nwhere \u03c3 is the logistic sigmoid function, and i, f, o and c\nare respectively the input gate, forget gate, output gate and\narXiv:1303.5778v1  [cs.NE]  22 Mar 2013\nFig. 1. Long Short-term Memory Cell\nFig. 2. Bidirectional RNN\ncell activation vectors, all of which are the same size as the\nhidden vector h. The weight matrices from the cell to gate\nvectors (e.g. Wsi) are diagonal, so element m in each gate\nvector only receives input from element m of the cell vector.\nOne shortcoming of conventional RNNs is that they are\nonly able to make use of previous context. In speech recog-\nnition, where whole utterances are transcribed at once, there\nis no reason not to exploit future context as well. Bidirec-\ntional RNNs (BRNNs) [15] do this by processing the data in\nboth directions with two separate hidden layers, which are\nthen fed forwards to the same output layer. As illustrated in\nFig. 2, a BRNN computes the forward hidden sequence \u2212\n\u2192\nh ,\nthe backward hidden sequence \u2190\n\u2212\nh and the output sequence y\nby iterating the backward layer from t = T to 1, the forward\nlayer from t = 1 to T and then updating the output layer:\n\u2212\u2192h t = H\n\u0010\nWx\u2212\n\u2192\nh xt + W\u2212\n\u2192\nh \u2212\n\u2192\nh\n\u2212\u2192h t\u22121 + b\u2212\n\u2192\nh\n\u0011\n(8)\n\u2190\u2212h t = H\n\u0010\nWx\u2190\n\u2212\nh xt + W\u2190\n\u2212\nh \u2190\n\u2212\nh\n\u2190\u2212h t+1 + b\u2190\n\u2212\nh\n\u0011\n(9)\nyt = W\u2212\n\u2192\nh y\n\u2212\u2192h t + W\u2190\n\u2212\nh y\n\u2190\u2212h t + by\n(10)\nCombing BRNNs with LSTM gives bidirectional LSTM [16],\nwhich can access long-range context in both input directions.\nA crucial element of the recent success of hybrid HMM-\nneural network systems is the use of deep architectures, which\nare able to build up progressively higher level representations\nof acoustic data. Deep RNNs can be created by stacking mul-\ntiple RNN hidden layers on top of each other, with the out-\nput sequence of one layer forming the input sequence for the\nnext. Assuming the same hidden layer function is used for\nall N layers in the stack, the hidden vector sequences hn are\niteratively computed from n = 1 to N and t = 1 to T:\nhn\nt = H\n\u0000Whn\u22121hnhn\u22121\nt\n+ Whnhnhn\nt\u22121 + bn\nh\n\u0001\n(11)\nwhere we de\ufb01ne h0 = x. The network outputs yt are\nyt = WhNyhN\nt + by\n(12)\nDeep bidirectional RNNs can be implemented by replacing\neach hidden sequence hn with the forward and backward se-\nquences \u2212\n\u2192\nh n and \u2190\n\u2212\nh n, and ensuring that every hidden layer\nreceives input from both the forward and backward layers at\nthe level below. If LSTM is used for the hidden layers we get\ndeep bidirectional LSTM, the main architecture used in this\npaper. As far as we are aware this is the \ufb01rst time deep LSTM\nhas been applied to speech recognition, and we \ufb01nd that it\nyields a dramatic improvement over single-layer LSTM.\n3. NETWORK TRAINING\nWe focus on end-to-end training, where RNNs learn to map\ndirectly from acoustic to phonetic sequences. One advantage\nof this approach is that it removes the need for a prede\ufb01ned\n(and error-prone) alignment to create the training targets. The\n\ufb01rst step is to to use the network outputs to parameterise a\ndifferentiable distribution Pr(y|x) over all possible phonetic\noutput sequences y given an acoustic input sequence x. The\nlog-probability log Pr(z|x) of the target output sequence z\ncan then be differentiated with respect to the network weights\nusing backpropagation through time [17], and the whole sys-\ntem can be optimised with gradient descent. We now describe\ntwo ways to de\ufb01ne the output distribution and hence train the\nnetwork. We refer throughout to the length of x as T, the\nlength of z as U, and the number of possible phonemes as K.\n3.1. Connectionist Temporal Classi\ufb01cation\nThe \ufb01rst method, known as Connectionist Temporal Classi-\n\ufb01cation (CTC) [8, 9], uses a softmax layer to de\ufb01ne a sepa-\nrate output distribution Pr(k|t) at every step t along the in-\nput sequence. This distribution covers the K phonemes plus\nan extra blank symbol \u2205which represents a non-output (the\nsoftmax layer is therefore size K + 1). Intuitively the net-\nwork decides whether to emit any label, or no label, at every\ntimestep. Taken together these decisions de\ufb01ne a distribu-\ntion over alignments between the input and target sequences.\nCTC then uses a forward-backward algorithm to sum over all\npossible alignments and determine the normalised probability\nPr(z|x) of the target sequence given the input sequence [8].\nSimilar procedures have been used elsewhere in speech and\nhandwriting recognition to integrate out over possible seg-\nmentations [18, 19]; however CTC differs in that it ignores\nsegmentation altogether and sums over single-timestep label\ndecisions instead.\nRNNs trained with CTC are generally bidirectional, to en-\nsure that every Pr(k|t) depends on the entire input sequence,\nand not just the inputs up to t. In this work we focus on deep\nbidirectional networks, with Pr(k|t) de\ufb01ned as follows:\nyt = W\u2212\n\u2192\nh Ny\n\u2212\u2192h N\nt + W\u2190\n\u2212\nh Ny\n\u2190\u2212h N\nt + by\n(13)\nPr(k|t) =\nexp(yt[k])\nPK\nk\u2032=1 exp(yt[k\u2032])\n,\n(14)\nwhere yt[k] is the kth element of the length K + 1 unnor-\nmalised output vector yt, and N is the number of bidirectional\nlevels.\n3.2. RNN Transducer\nCTC de\ufb01nes a distribution over phoneme sequences that de-\npends only on the acoustic input sequence x. It is therefore\nan acoustic-only model. A recent augmentation, known as an\nRNN transducer [10] combines a CTC-like network with a\nseparate RNN that predicts each phoneme given the previous\nones, thereby yielding a jointly trained acoustic and language\nmodel. Joint LM-acoustic training has proved bene\ufb01cial in\nthe past for speech recognition [20, 21].\nWhereas CTC determines an output distribution at every\ninput timestep, an RNN transducer determines a separate dis-\ntribution Pr(k|t, u) for every combination of input timestep t\nand output timestep u. As with CTC, each distribution cov-\ners the K phonemes plus \u2205.\nIntuitively the network \u2018de-\ncides\u2019 what to output depending both on where it is in the\ninput sequence and the outputs it has already emitted. For a\nlength U target sequence z, the complete set of TU decisions\njointly determines a distribution over all possible alignments\nbetween x and z, which can then be integrated out with a\nforward-backward algorithm to determine log Pr(z|x) [10].\nIn the original formulation Pr(k|t, u) was de\ufb01ned by tak-\ning an \u2018acoustic\u2019 distribution Pr(k|t) from the CTC network,\na \u2018linguistic\u2019 distribution Pr(k|u) from the prediction net-\nwork, then multiplying the two together and renormalising.\nAn improvement introduced in this paper is to instead feed\nthe hidden activations of both networks into a separate feed-\nforward output network, whose outputs are then normalised\nwith a softmax function to yield Pr(k|t, u). This allows a\nricher set of possibilities for combining linguistic and acous-\ntic information, and appears to lead to better generalisation.\nIn particular we have found that the number of deletion errors\nencountered during decoding is reduced.\nDenote by \u2212\n\u2192\nh N and \u2190\n\u2212\nh N the uppermost forward and\nbackward hidden sequences of the CTC network, and by p\nthe hidden sequence of the prediction network. At each t, u\nthe output network is implemented by feeding \u2212\n\u2192\nh N and \u2190\n\u2212\nh N\nto a linear layer to generate the vector lt, then feeding lt and\npu to a tanh hidden layer to yield ht,u, and \ufb01nally feeding\nht,u to a size K + 1 softmax layer to determine Pr(k|t, u):\nlt = W\u2212\n\u2192\nh Nl\n\u2212\u2192h N\nt + W\u2190\n\u2212\nh Nl\n\u2190\u2212h N\nt + bl\n(15)\nht,u = tanh (Wlhlt,u + Wpbpu + bh)\n(16)\nyt,u = Whyht,u + by\n(17)\nPr(k|t, u) =\nexp(yt,u[k])\nPK\nk\u2032=1 exp(yt,u[k\u2032])\n,\n(18)\nwhere yt,u[k] is the kth element of the length K + 1 unnor-\nmalised output vector. For simplicity we constrained all non-\noutput layers to be the same size (|\u2212\u2192h n\nt | = |\u2190\u2212h n\nt | = |pu| =\n|lt| = |ht,u|); however they could be varied independently.\nRNN transducers can be trained from random initial\nweights. However they appear to work better when initialised\nwith the weights of a pretrained CTC network and a pre-\ntrained next-step prediction network (so that only the output\nnetwork starts from random weights). The output layers (and\nall associated weights) used by the networks during pretrain-\ning are removed during retraining. In this work we pretrain\nthe prediction network on the phonetic transcriptions of the\naudio training data; however for large-scale applications it\nwould make more sense to pretrain on a separate text corpus.\n3.3. Decoding\nRNN transducers can be decoded with beam search [10] to\nyield an n-best list of candidate transcriptions. In the past\nCTC networks have been decoded using either a form of best-\n\ufb01rst decoding known as pre\ufb01x search, or by simply taking the\nmost active output at every timestep [8]. In this work however\nwe exploit the same beam search as the transducer, with the\nmodi\ufb01cation that the output label probabilities Pr(k|t, u) do\nnot depend on the previous outputs (so Pr(k|t, u) = Pr(k|t)).\nWe \ufb01nd beam search both faster and more effective than pre-\n\ufb01x search for CTC. Note the n-best list from the transducer\nwas originally sorted by the length normalised log-probabilty\nlog Pr(y)/|y|; in the current work we dispense with the nor-\nmalisation (which only helps when there are many more dele-\ntions than insertions) and sort by Pr(y).\n3.4. Regularisation\nRegularisation is vital for good performance with RNNs, as\ntheir \ufb02exibility makes them prone to over\ufb01tting. Two regu-\nlarisers were used in this paper: early stopping and weight\nnoise (the addition of Gaussian noise to the network weights\nduring training [22]). Weight noise was added once per train-\ning sequence, rather than at every timestep. Weight noise\ntends to \u2018simplify\u2019 neural networks, in the sense of reducing\nthe amount of information required to transmit the parame-\nters [23, 24], which improves generalisation.\n4. EXPERIMENTS\nPhoneme recognition experiments were performed on the\nTIMIT corpus [25]. The standard 462 speaker set with all\nSA records removed was used for training, and a separate\ndevelopment set of 50 speakers was used for early stop-\nping. Results are reported for the 24-speaker core test set.\nThe audio data was encoded using a Fourier-transform-based\n\ufb01lter-bank with 40 coef\ufb01cients (plus energy) distributed on\na mel-scale, together with their \ufb01rst and second temporal\nderivatives. Each input vector was therefore size 123. The\ndata were normalised so that every element of the input vec-\ntors had zero mean and unit variance over the training set. All\n61 phoneme labels were used during training and decoding\n(so K = 61), then mapped to 39 classes for scoring [26].\nNote that all experiments were run only once, so the vari-\nance due to random weight initialisation and weight noise is\nunknown.\nAs shown in Table 1, nine RNNs were evaluated, vary-\ning along three main dimensions: the training method used\n(CTC, Transducer or pretrained Transducer), the number of\nhidden levels (1\u20135), and the number of LSTM cells in each\nhidden layer. Bidirectional LSTM was used for all networks\nexcept CTC-3l-500h-tanh, which had tanh units instead of\nLSTM cells, and CTC-3l-421h-uni where the LSTM layers\nwere unidirectional. All networks were trained using stochas-\ntic gradient descent, with learning rate 10\u22124, momentum 0.9\nand random initial weights drawn uniformly from [\u22120.1, 0.1].\nAll networks except CTC-3l-500h-tanh and PreTrans-3l-250h\nwere \ufb01rst trained with no noise and then, starting from the\npoint of highest log-probability on the development set, re-\ntrained with Gaussian weight noise (\u03c3 = 0.075) until the\npoint of lowest phoneme error rate on the development set.\nPreTrans-3l-250h was initialised with the weights of CTC-\n3l-250h, along with the weights of a phoneme prediction net-\nwork (which also had a hidden layer of 250 LSTM cells), both\nof which were trained without noise, retrained with noise, and\nstopped at the point of highest log-probability. PreTrans-3l-\n250h was trained from this point with noise added. CTC-3l-\n500h-tanh was entirely trained without weight noise because\nit failed to learn with noise added. Beam search decoding was\nused for all networks, with a beam width of 100.\nThe advantage of deep networks is immediately obvious,\nwith the error rate for CTC dropping from 23.9% to 18.4%\nas the number of hidden levels increases from one to \ufb01ve.\nThe four networks CTC-3l-500h-tanh, CTC-1l-622h, CTC-\n3l-421h-uni and CTC-3l-250h all had approximately the same\nnumber of weights, but give radically different results. The\nthree main conclusions we can draw from this are (a) LSTM\nworks much better than tanh for this task, (b) bidirectional\nTable 1. TIMIT Phoneme Recognition Results. \u2018Epochs\u2019 is\nthe number of passes through the training set before conver-\ngence. \u2018PER\u2019 is the phoneme error rate on the core test set.\nNETWORK\nWEIGHTS\nEPOCHS\nPER\nCTC-3L-500H-TANH\n3.7M\n107\n37.6%\nCTC-1L-250H\n0.8M\n82\n23.9%\nCTC-1L-622H\n3.8M\n87\n23.0%\nCTC-2L-250H\n2.3M\n55\n21.0%\nCTC-3L-421H-UNI\n3.8M\n115\n19.6%\nCTC-3L-250H\n3.8M\n124\n18.6%\nCTC-5L-250H\n6.8M\n150\n18.4%\nTRANS-3L-250H\n4.3M\n112\n18.3%\nPRETRANS-3L-250H\n4.3M\n144\n17.7%\nFig. 3. Input Sensitivity of a deep CTC RNN. The heatmap\n(top) shows the derivatives of the \u2018ah\u2019 and \u2018p\u2019 outputs printed\nin red with respect to the \ufb01lterbank inputs (bottom).\nThe\nTIMIT ground truth segmentation is shown below. Note that\nthe sensitivity extends to surrounding segments; this may be\nbecause CTC (which lacks an explicit language model) at-\ntempts to learn linguistic dependencies from the acoustic data.\nLSTM has a slight advantage over unidirectional LSTMand\n(c) depth is more important than layer size (which supports\nprevious \ufb01ndings for deep networks [3]). Although the advan-\ntage of the transducer is slight when the weights are randomly\ninitialised, it becomes more substantial when pretraining is\nused.\n5. CONCLUSIONS AND FUTURE WORK\nWe have shown that the combination of deep, bidirectional\nLong Short-term Memory RNNs with end-to-end training and\nweight noise gives state-of-the-art results in phoneme recog-\nnition on the TIMIT database. An obvious next step is to ex-\ntend the system to large vocabulary speech recognition. An-\nother interesting direction would be to combine frequency-\ndomain convolutional neural networks [27] with deep LSTM.\n6. REFERENCES\n[1] H.A. Bourlard and N. Morgan, Connnectionist Speech\nRecognition: A Hybrid Approach,\nKluwer Academic\nPublishers, 1994.\n[2] Qifeng Zhu, Barry Chen, Nelson Morgan, and Andreas\nStolcke, \u201cTandem connectionist feature extraction for\nconversational speech recognition,\u201d\nin International\nConference on Machine Learning for Multimodal Inter-\naction, Berlin, Heidelberg, 2005, MLMI\u201904, pp. 223\u2013\n231, Springer-Verlag.\n[3] A. Mohamed, G.E. Dahl, and G. Hinton,\n\u201cAcoustic\nmodeling using deep belief networks,\u201d Audio, Speech,\nand Language Processing, IEEE Transactions on, vol.\n20, no. 1, pp. 14 \u201322, jan. 2012.\n[4] G. Hinton, Li Deng, Dong Yu, G.E. Dahl, A. Mohamed,\nN. Jaitly, A. Senior, V. Vanhoucke, P. Nguyen, T.N.\nSainath, and B. Kingsbury, \u201cDeep neural networks for\nacoustic modeling in speech recognition,\u201d Signal Pro-\ncessing Magazine, IEEE, vol. 29, no. 6, pp. 82 \u201397, nov.\n2012.\n[5] A. J. Robinson, \u201cAn Application of Recurrent Nets to\nPhone Probability Estimation,\u201d IEEE Transactions on\nNeural Networks, vol. 5, no. 2, pp. 298\u2013305, 1994.\n[6] Oriol Vinyals, Suman Ravuri, and Daniel Povey, \u201cRe-\nvisiting Recurrent Neural Networks for Robust ASR,\u201d\nin ICASSP, 2012.\n[7] A. Maas, Q. Le, T. O\u2019Neil, O. Vinyals, P. Nguyen, and\nA. Ng, \u201cRecurrent neural networks for noise reduction\nin robust asr,\u201d in INTERSPEECH, 2012.\n[8] A. Graves, S. Fern\u00b4andez, F. Gomez, and J. Schmidhuber,\n\u201cConnectionist Temporal Classi\ufb01cation: Labelling Un-\nsegmented Sequence Data with Recurrent Neural Net-\nworks,\u201d in ICML, Pittsburgh, USA, 2006.\n[9] A. Graves, Supervised sequence labelling with recurrent\nneural networks, vol. 385, Springer, 2012.\n[10] A. Graves, \u201cSequence transduction with recurrent neu-\nral networks,\u201d in ICML Representation Learning Work-\nsop, 2012.\n[11] S. Hochreiter and J. Schmidhuber, \u201cLong Short-Term\nMemory,\u201d Neural Computation, vol. 9, no. 8, pp. 1735\u2013\n1780, 1997.\n[12] A. Graves, S. Fern\u00b4andez, M. Liwicki, H. Bunke, and\nJ. Schmidhuber,\n\u201cUnconstrained Online Handwriting\nRecognition with Recurrent Neural Networks,\u201d in NIPS.\n2008.\n[13] Alex Graves and Juergen Schmidhuber, \u201cOf\ufb02ine Hand-\nwriting Recognition with Multidimensional Recurrent\nNeural Networks,\u201d in NIPS. 2009.\n[14] F. Gers, N. Schraudolph, and J. Schmidhuber, \u201cLearning\nPrecise Timing with LSTM Recurrent Networks,\u201d Jour-\nnal of Machine Learning Research, vol. 3, pp. 115\u2013143,\n2002.\n[15] M. Schuster and K. K. Paliwal, \u201cBidirectional Recur-\nrent Neural Networks,\u201d IEEE Transactions on Signal\nProcessing, vol. 45, pp. 2673\u20132681, 1997.\n[16] A. Graves and J. Schmidhuber, \u201cFramewise Phoneme\nClassi\ufb01cation with Bidirectional LSTM and Other Neu-\nral Network Architectures,\u201d Neural Networks, vol. 18,\nno. 5-6, pp. 602\u2013610, June/July 2005.\n[17] David\nE.\nRumelhart,\nGeoffrey\nE.\nHinton,\nand\nRonald J. Williams, Learning representations by back-\npropagating errors, pp. 696\u2013699, MIT Press, 1988.\n[18] Georey Zweig and Patrick Nguyen, \u201cSCARF: A seg-\nmental CRF speech recognition system,\u201d\nTech. Rep.,\nMicrosoft Research, 2009.\n[19] Andrew W. Senior and Anthony J. Robinson, \u201cForward-\nbackward retraining of recurrent neural networks,\u201d in\nNIPS, 1995, pp. 743\u2013749.\n[20] Abdel rahman Mohamed, Dong Yu, and Li Deng, \u201cIn-\nvestigation of full-sequence training of deep belief net-\nworks for speech recognition,\u201d in in Interspeech, 2010.\n[21] M. Lehr and I. Shafran,\n\u201cDiscriminatively estimated\njoint acoustic, duration, and language model for speech\nrecognition,\u201d in ICASSP, 2010, pp. 5542 \u20135545.\n[22] Kam-Chuen Jim, C.L. Giles, and B.G. Horne, \u201cAn anal-\nysis of noise in recurrent neural networks: convergence\nand generalization,\u201d Neural Networks, IEEE Transac-\ntions on, vol. 7, no. 6, pp. 1424 \u20131438, nov 1996.\n[23] Geoffrey E. Hinton and Drew van Camp, \u201cKeeping the\nneural networks simple by minimizing the description\nlength of the weights,\u201d in COLT, 1993, pp. 5\u201313.\n[24] Alex Graves, \u201cPractical variational inference for neural\nnetworks,\u201d in NIPS, pp. 2348\u20132356. 2011.\n[25] DARPA-ISTO, The DARPA TIMIT Acoustic-Phonetic\nContinuous Speech Corpus (TIMIT), speech disc cd1-\n1.1 edition, 1990.\n[26] Kai fu Lee and Hsiao wuen Hon, \u201cSpeaker-independent\nphone recognition using hidden markov models,\u201d IEEE\nTransactions on Acoustics, Speech, and Signal Process-\ning, 1989.\n[27] O. Abdel-Hamid, A. Mohamed, Hui Jiang, and G. Penn,\n\u201cApplying convolutional neural networks concepts to\nhybrid nn-hmm model for speech recognition,\u201d\nin\nICASSP, march 2012, pp. 4277 \u20134280.\n",
        "sentence": " The use of deep architectures have shown impressive results for many different tasks, from image classification [10, 14], speech recognition [7] to machine translation [19] or even for natural language processing [16].",
        "context": "A crucial element of the recent success of hybrid HMM-\nneural network systems is the use of deep architectures, which\nare able to build up progressively higher level representations\nof acoustic data. Deep RNNs can be created by stacking mul-\ndeep bidirectional LSTM, the main architecture used in this\npaper. As far as we are aware this is the \ufb01rst time deep LSTM\nhas been applied to speech recognition, and we \ufb01nd that it\nyields a dramatic improvement over single-layer LSTM.\n3. NETWORK TRAINING\nThey have gained attention in recent years with the dramatic\nimprovements in acoustic modelling yielded by deep feed-\nforward networks [3, 4]. Given that speech is an inherently\ndynamic process, it seems natural to consider recurrent neu-"
    },
    {
        "title": "Hierarchical mixtures of experts and the em algorithm",
        "author": [
            "Michael I. Jordan",
            "Robert A. Jacobs"
        ],
        "venue": "Neural Comput.,",
        "citeRegEx": "8",
        "shortCiteRegEx": "8",
        "year": 1994,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " The idea of processing input data by different functions is not new and have been proposed for example in Neural Tree Networks [17, 15], with Hierarchical Mixture of Experts [8] where the idea is to compute different transformations of data and to aggregate these transformations.",
        "context": null
    },
    {
        "title": "Timely object recognition",
        "author": [
            "Sergey Karayev",
            "Tobias Baumgartner",
            "Mario Fritz",
            "Trevor Darrell"
        ],
        "venue": "In Advances in Neural Information Processing Systems",
        "citeRegEx": "9",
        "shortCiteRegEx": "9",
        "year": 2012,
        "abstract": "",
        "full_text": "",
        "sentence": " For example [3] and [12] consider that the sequential process is an acquisition process able to focus on relevant parts of the input data; [9] for example focuses on the sequential prediction process with a cascade approach.",
        "context": null
    },
    {
        "title": "Imagenet classification with deep convolutional neural networks",
        "author": [
            "Alex Krizhevsky",
            "Ilya Sutskever",
            "Geoffrey E. Hinton"
        ],
        "venue": "In Advances in Neural Information Processing Systems",
        "citeRegEx": "10",
        "shortCiteRegEx": "10",
        "year": 2012,
        "abstract": "We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5% and 17.0%, respectively, which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully connected layers we employed a recently developed regularization method called \"dropout\" that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3%, compared to 26.2% achieved by the second-best entry.",
        "full_text": "",
        "sentence": " The use of deep architectures have shown impressive results for many different tasks, from image classification [10, 14], speech recognition [7] to machine translation [19] or even for natural language processing [16].",
        "context": null
    },
    {
        "title": "Training deep and recurrent networks with hessian-free optimization",
        "author": [
            "James Martens",
            "Ilya Sutskever"
        ],
        "venue": "In Neural Networks: Tricks of the Trade - Second Edition,",
        "citeRegEx": "11",
        "shortCiteRegEx": "11",
        "year": 2012,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " Many variants of learning algorithms have been proposed, from complex gradient computations [11], to dropout methods [1], but the baseline learning algorithm still consists in recursively computing the gradient by using the back-propagation algorithm and performing (stochastic) gradient descent.",
        "context": null
    },
    {
        "title": "Recurrent models of visual attention",
        "author": [
            "Volodymyr Mnih",
            "Nicolas Heess",
            "Alex Graves",
            "Koray Kavukcuoglu"
        ],
        "venue": "CoRR, abs/1406.6247,",
        "citeRegEx": "12",
        "shortCiteRegEx": "12",
        "year": 2014,
        "abstract": "Applying convolutional neural networks to large images is computationally\nexpensive because the amount of computation scales linearly with the number of\nimage pixels. We present a novel recurrent neural network model that is capable\nof extracting information from an image or video by adaptively selecting a\nsequence of regions or locations and only processing the selected regions at\nhigh resolution. Like convolutional neural networks, the proposed model has a\ndegree of translation invariance built-in, but the amount of computation it\nperforms can be controlled independently of the input image size. While the\nmodel is non-differentiable, it can be trained using reinforcement learning\nmethods to learn task-specific policies. We evaluate our model on several image\nclassification tasks, where it significantly outperforms a convolutional neural\nnetwork baseline on cluttered images, and on a dynamic visual control problem,\nwhere it learns to track a simple object without an explicit training signal\nfor doing so.",
        "full_text": "Recurrent Models of Visual Attention\nVolodymyr Mnih\nNicolas Heess\nAlex Graves\nKoray Kavukcuoglu\nGoogle DeepMind\n{vmnih,heess,gravesa,korayk} @ google.com\nAbstract\nApplying convolutional neural networks to large images is computationally ex-\npensive because the amount of computation scales linearly with the number of\nimage pixels.\nWe present a novel recurrent neural network model that is ca-\npable of extracting information from an image or video by adaptively selecting\na sequence of regions or locations and only processing the selected regions at\nhigh resolution. Like convolutional neural networks, the proposed model has a\ndegree of translation invariance built-in, but the amount of computation it per-\nforms can be controlled independently of the input image size. While the model\nis non-differentiable, it can be trained using reinforcement learning methods to\nlearn task-speci\ufb01c policies. We evaluate our model on several image classi\ufb01cation\ntasks, where it signi\ufb01cantly outperforms a convolutional neural network baseline\non cluttered images, and on a dynamic visual control problem, where it learns to\ntrack a simple object without an explicit training signal for doing so.\n1\nIntroduction\nNeural network-based architectures have recently had great success in signi\ufb01cantly advancing the\nstate of the art on challenging image classi\ufb01cation and object detection datasets [8, 12, 19]. Their\nexcellent recognition accuracy, however, comes at a high computational cost both at training and\ntesting time. The large convolutional neural networks typically used currently take days to train on\nmultiple GPUs even though the input images are downsampled to reduce computation [12]. In the\ncase of object detection processing a single image at test time currently takes seconds when running\non a single GPU [8, 19] as these approaches effectively follow the classical sliding window paradigm\nfrom the computer vision literature where a classi\ufb01er, trained to detect an object in a tightly cropped\nbounding box, is applied independently to thousands of candidate windows from the test image at\ndifferent positions and scales. Although some computations can be shared, the main computational\nexpense for these models comes from convolving \ufb01lter maps with the entire input image, therefore\ntheir computational complexity is at least linear in the number of pixels.\nOne important property of human perception is that one does not tend to process a whole scene\nin its entirety at once. Instead humans focus attention selectively on parts of the visual space to\nacquire information when and where it is needed, and combine information from different \ufb01xations\nover time to build up an internal representation of the scene [18], guiding future eye movements\nand decision making. Focusing the computational resources on parts of a scene saves \u201cbandwidth\u201d\nas fewer \u201cpixels\u201d need to be processed. But it also substantially reduces the task complexity as\nthe object of interest can be placed in the center of the \ufb01xation and irrelevant features of the visual\nenvironment (\u201cclutter\u201d) outside the \ufb01xated region are naturally ignored.\nIn line with its fundamental role, the guidance of human eye movements has been extensively studied\nin neuroscience and cognitive science literature. While low-level scene properties and bottom up\nprocesses (e.g. in the form of saliency; [11]) play an important role, the locations on which humans\n\ufb01xate have also been shown to be strongly task speci\ufb01c (see [9] for a review and also e.g. [15, 22]). In\nthis paper we take inspiration from these results and develop a novel framework for attention-based\ntask-driven visual processing with neural networks. Our model considers attention-based processing\n1\narXiv:1406.6247v1  [cs.LG]  24 Jun 2014\nof a visual scene as a control problem and is general enough to be applied to static images, videos,\nor as a perceptual module of an agent that interacts with a dynamic visual environment (e.g. robots,\ncomputer game playing agents).\nThe model is a recurrent neural network (RNN) which processes inputs sequentially, attending to\ndifferent locations within the images (or video frames) one at a time, and incrementally combines\ninformation from these \ufb01xations to build up a dynamic internal representation of the scene or envi-\nronment. Instead of processing an entire image or even bounding box at once, at each step, the model\nselects the next location to attend to based on past information and the demands of the task. Both\nthe number of parameters in our model and the amount of computation it performs can be controlled\nindependently of the size of the input image, which is in contrast to convolutional networks whose\ncomputational demands scale linearly with the number of image pixels. We describe an end-to-end\noptimization procedure that allows the model to be trained directly with respect to a given task and\nto maximize a performance measure which may depend on the entire sequence of decisions made by\nthe model. This procedure uses backpropagation to train the neural-network components and policy\ngradient to address the non-differentiabilities due to the control problem.\nWe show that our model can learn effective task-speci\ufb01c strategies for where to look on several\nimage classi\ufb01cation tasks as well as a dynamic visual control problem. Our results also suggest that\nan attention-based model may be better than a convolutional neural network at both dealing with\nclutter and scaling up to large input images.\n2\nPrevious Work\nComputational limitations have received much attention in the computer vision literature. For in-\nstance, for object detection, much work has been dedicated to reducing the cost of the widespread\nsliding window paradigm, focusing primarily on reducing the number of windows for which the\nfull classi\ufb01er is evaluated, e.g. via classi\ufb01er cascades (e.g. [7, 24]), removing image regions from\nconsideration via a branch and bound approach on the classi\ufb01er output (e.g. [13]), or by proposing\ncandidate windows that are likely to contain objects (e.g. [1, 23]). Even though substantial speedups\nmay be obtained with such approaches, and some of these can be combined with or used as an add-on\nto CNN classi\ufb01ers [8], they remain \ufb01rmly rooted in the window classi\ufb01er design for object detection\nand only exploit past information to inform future processing of the image in a very limited way.\nA second class of approaches that has a long history in computer vision and is strongly motivated\nby human perception are saliency detectors (e.g. [11]). These approaches prioritize the processing\nof potentially interesting (\u201csalient\u201d) image regions which are typically identi\ufb01ed based on some\nmeasure of local low-level feature contrast. Saliency detectors indeed capture some of the properties\nof human eye movements, but they typically do not to integrate information across \ufb01xations, their\nsaliency computations are mostly hardwired, and they are based on low-level image properties only,\nusually ignoring other factors such as semantic content of a scene and task demands (but see [22]).\nSome works in the computer vision literature and elsewhere e.g. [2, 4, 6, 14, 16, 17, 20] have em-\nbraced vision as a sequential decision task as we do here. There, as in our work, information about\nthe image is gathered sequentially and the decision where to attend next is based on previous \ufb01xa-\ntions of the image. [4] employs the learned Bayesian observer model from [5] to the task of object\ndetection. The learning framework of [5] is related to ours as they also employ a policy gradient\nformulation (cf. section 3) but their overall setup is considerably more restrictive than ours and only\nsome parts of the system are learned.\nOur work is perhaps the most similar to the other attempts to implement attentional processing in a\ndeep learning framework [6, 14, 17]. Our formulation which employs an RNN to integrate visual\ninformation over time and to decide how to act is, however, more general, and our learning procedure\nallows for end-to-end optimization of the sequential decision process instead of relying on greedy\naction selection. We further demonstrate how the same general architecture can be used for ef\ufb01cient\nobject recognition in still images as well as to interact with a dynamic visual environment in a\ntask-driven way.\n3\nThe Recurrent Attention Model (RAM)\nIn this paper we consider the attention problem as the sequential decision process of a goal-directed\nagent interacting with a visual environment. At each point in time, the agent observes the environ-\nment only via a bandwidth-limited sensor, i.e. it never senses the environment in full. It may extract\n2\nlt-1\ngt\nGlimpse\nSensor\nxt\n\u03c1(xt , lt-1)\n\u03b8g\n0\n\u03b8g\n1\n\u03b8g\n2\nGlimpse Network : fg( \u03b8g )\nlt-1\ngt\nlt\nat\nlt\ngt+1\nlt+1\nat+1\nht\nht+1\nfg(\u03b8g)\nht-1\nfl(\u03b8l)\nfa(\u03b8a)\nfh(\u03b8h)\nfg(\u03b8g)\nfl(\u03b8l)\nfa(\u03b8a)\nfh(\u03b8h)\nxt\n\u03c1(xt , lt-1)\nlt-1\nGlimpse Sensor\nA)\nB)\nC)\nFigure 1: A) Glimpse Sensor: Given the coordinates of the glimpse and an input image, the sen-\nsor extracts a retina-like representation \u03c1(xt, lt\u22121) centered at lt\u22121 that contains multiple resolution\npatches. B) Glimpse Network: Given the location (lt\u22121) and input image (xt), uses the glimpse\nsensor to extract retina representation \u03c1(xt, lt\u22121). The retina representation and glimpse location is\nthen mapped into a hidden space using independent linear layers parameterized by \u03b80\ng and \u03b81\ng respec-\ntively using recti\ufb01ed units followed by another linear layer \u03b82\ng to combine the information from both\ncomponents. The glimpse network fg(.; {\u03b80\ng, \u03b81\ng, \u03b82\ng}) de\ufb01nes a trainable bandwidth limited sensor\nfor the attention network producing the glimpse representation gt. C) Model Architecture: Overall,\nthe model is an RNN. The core network of the model fh(.; \u03b8h) takes the glimpse representation gt as\ninput and combining with the internal representation at previous time step ht\u22121, produces the new\ninternal state of the model ht. The location network fl(.; \u03b8l) and the action network fa(.; \u03b8a) use the\ninternal state ht of the model to produce the next location to attend to lt and the action/classi\ufb01cation\nat respectively. This basic RNN iteration is repeated for a variable number of steps.\ninformation only in a local region or in a narrow frequency band. The agent can, however, actively\ncontrol how to deploy its sensor resources (e.g. choose the sensor location). The agent can also\naffect the true state of the environment by executing actions. Since the environment is only partially\nobserved the agent needs to integrate information over time in order to determine how to act and\nhow to deploy its sensor most effectively. At each step, the agent receives a scalar reward (which\ndepends on the actions the agent has executed and can be delayed), and the goal of the agent is to\nmaximize the total sum of such rewards.\nThis formulation encompasses tasks as diverse as object detection in static images and control prob-\nlems like playing a computer game from the image stream visible on the screen. For a game, the\nenvironment state would be the true state of the game engine and the agent\u2019s sensor would operate\non the video frame shown on the screen. (Note that for most games, a single frame would not fully\nspecify the game state). The environment actions here would correspond to joystick controls, and\nthe reward would re\ufb02ect points scored. For object detection in static images the state of the envi-\nronment would be \ufb01xed and correspond to the true contents of the image. The environmental action\nwould correspond to the classi\ufb01cation decision (which may be executed only after a \ufb01xed number\nof \ufb01xations), and the reward would re\ufb02ect if the decision is correct.\n3.1\nModel\nThe agent is built around a recurrent neural network as shown in Fig. 1. At each time step, it\nprocesses the sensor data, integrates information over time, and chooses how to act and how to\ndeploy its sensor at next time step:\nSensor: At each step t the agent receives a (partial) observation of the environment in the form of\nan image xt. The agent does not have full access to this image but rather can extract information\nfrom xt via its bandwidth limited sensor \u03c1, e.g. by focusing the sensor on some region or frequency\nband of interest.\nIn this paper we assume that the bandwidth-limited sensor extracts a retina-like representation\n\u03c1(xt, lt\u22121) around location lt\u22121 from image xt. It encodes the region around l at a high-resolution\nbut uses a progressively lower resolution for pixels further from l, resulting in a vector of much\n3\nlower dimensionality than the original image x. We will refer to this low-resolution representation\nas a glimpse [14]. The glimpse sensor is used inside what we call the glimpse network fg to produce\nthe glimpse feature vector gt = fg(xt, lt\u22121; \u03b8g) where \u03b8g = {\u03b80\ng, \u03b81\ng, \u03b82\ng} (Fig. 1B).\nInternal state: The agent maintains an interal state which summarizes information extracted from\nthe history of past observations; it encodes the agent\u2019s knowledge of the environment and is in-\nstrumental to deciding how to act and where to deploy the sensor. This internal state is formed\nby the hidden units ht of the recurrent neural network and updated over time by the core network:\nht = fh(ht\u22121, gt; \u03b8h). The external input to the network is the glimpse feature vector gt.\nActions: At each step, the agent performs two actions: it decides how to deploy its sensor via the\nsensor control lt, and an environment action at which might affect the state of the environment.\nThe nature of the environment action depends on the task. In this work, the location actions are\nchosen stochastically from a distribution parameterized by the location network fl(ht; \u03b8l) at time t:\nlt \u223cp(\u00b7|fl(ht; \u03b8l)). The environment action at is similarly drawn from a distribution conditioned\non a second network output at \u223cp(\u00b7|fa(ht; \u03b8a)). For classi\ufb01cation it is formulated using a softmax\noutput and for dynamic environments, its exact formulation depends on the action set de\ufb01ned for\nthat particular environment (e.g. joystick movements, motor control, ...).\nReward: After executing an action the agent receives a new visual observation of the environment\nxt+1 and a reward signal rt+1. The goal of the agent is to maximize the sum of the reward signal1\nwhich is usually very sparse and delayed: R = PT\nt=1 rt. In the case of object recognition, for\nexample, rT = 1 if the object is classi\ufb01ed correctly after T steps and 0 otherwise.\nThe above setup is a special instance of what is known in the RL community as a Partially Observ-\nable Markov Decision Process (POMDP). The true state of the environment (which can be static or\ndynamic) is unobserved. In this view, the agent needs to learn a (stochastic) policy \u03c0((lt, at)|s1:t; \u03b8)\nwith parameters \u03b8 that, at each step t, maps the history of past interactions with the environment\ns1:t = x1, l1, a1, . . . xt\u22121, lt\u22121, at\u22121, xt to a distribution over actions for the current time step, sub-\nject to the constraint of the sensor. In our case, the policy \u03c0 is de\ufb01ned by the RNN outlined above,\nand the history st is summarized in the state of the hidden units ht. We will describe the speci\ufb01c\nchoices for the above components in Section 4.\n3.2\nTraining\nThe parameters of our agent are given by the parameters of the glimpse network, the core network\n(Fig. 1C), and the action network \u03b8 = {\u03b8g, \u03b8h, \u03b8a} and we learn these to maximize the total reward\nthe agent can expect when interacting with the environment.\nMore formally, the policy of the agent, possibly in combination with the dynamics of the environ-\nment (e.g. for game-playing), induces a distribution over possible interaction sequences s1:N and we\naim to maximize the reward under this distribution: J(\u03b8) = Ep(s1:T ;\u03b8)\nhPT\nt=1 rt\ni\n= Ep(s1:T ;\u03b8) [R],\nwhere p(s1:T ; \u03b8) depends on the policy\nMaximizing J exactly is non-trivial since it involves an expectation over the high-dimensional inter-\naction sequences which may in turn involve unknown environment dynamics. Viewing the problem\nas a POMDP, however, allows us to bring techniques from the RL literature to bear: As shown by\nWilliams [26] a sample approximation to the gradient is given by\n\u2207\u03b8J =\nT\nX\nt=1\nEp(s1:T ;\u03b8) [\u2207\u03b8 log \u03c0(ut|s1:t; \u03b8)R] \u22481\nM\nM\nX\ni=1\nT\nX\nt=1\n\u2207\u03b8 log \u03c0(ui\nt|si\n1:t; \u03b8)Ri,\n(1)\nwhere si\u2019s are interaction sequences obtained by running the current agent \u03c0\u03b8 for i = 1 . . . M\nepisodes.\nThe learning rule (1) is also known as the REINFORCE rule, and it involves running the agent with\nits current policy to obtain samples of interaction sequences s1:T and then adjusting the parameters\n\u03b8 of our agent such that the log-probability of chosen actions that have led to high cumulative reward\nis increased, while that of actions having produced low reward is decreased.\n1 Depending on the scenario it may be more appropriate to consider a sum of discounted rewards, where\nrewards obtained in the distant future contribute less: R = PT\nt=1 \u03b3t\u22121rt. In this case we can have T \u2192\u221e.\n4\nEq. (1) requires us to compute \u2207\u03b8 log \u03c0(ui\nt|si\n1:t; \u03b8). But this is just the gradient of the RNN that\nde\ufb01nes our agent evaluated at time step t and can be computed by standard backpropagation [25].\nVariance Reduction : Equation (1) provides us with an unbiased estimate of the gradient but it may\nhave high variance. It is therefore common to consider a gradient estimate of the form\n1\nM\nM\nX\ni=1\nT\nX\nt=1\n\u2207\u03b8 log \u03c0(ui\nt|si\n1:t; \u03b8)\n\u0000Ri\nt \u2212bt\n\u0001\n,\n(2)\nwhere Ri\nt = PT\nt\u2032=1 ri\nt\u2032 is the cumulative reward obtained following the execution of action ui\nt, and\nbt is a baseline that may depend on si\n1:t (e.g. via hi\nt) but not on the action ui\nt itself. This estimate\nis equal to (1) in expectation but may have lower variance. It is natural to select bt = E\u03c0 [Rt] [21],\nand this form of baseline known as the value function in the reinforcement learning literature. The\nresulting algorithm increases the log-probability of an action that was followed by a larger than\nexpected cumulative reward, and decreases the probability if the obtained cumulative reward was\nsmaller. We use this type of baseline and learn it by reducing the squared error between Ri\nt\u2019s and bt.\nUsing a Hybrid Supervised Loss: The algorithm described above allows us to train the agent when\nthe \u201cbest\u201d actions are unknown, and the learning signal is only provided via the reward. For instance,\nwe may not know a priori which sequence of \ufb01xations provides most information about an unknown\nimage, but the total reward at the end of an episode will give us an indication whether the tried\nsequence was good or bad.\nHowever, in some situations we do know the correct action to take: For instance, in an object\ndetection task the agent has to output the label of the object as the \ufb01nal action. For the training\nimages this label will be known and we can directly optimize the policy to output the correct label\nassociated with a training image at the end of an observation sequence. This can be achieved, as is\ncommon in supervised learning, by maximizing the conditional probability of the true label given\nthe observations from the image, i.e. by maximizing log \u03c0(a\u2217\nT |s1:T ; \u03b8), where a\u2217\nT corresponds to the\nground-truth label(-action) associated with the image from which observations s1:T were obtained.\nWe follow this approach for classi\ufb01cation problems where we optimize the cross entropy loss to\ntrain the action network fa and backpropagate the gradients through the core and glimpse networks.\nThe location network fl is always trained with REINFORCE.\n4\nExperiments\nWe evaluated our approach on several image classi\ufb01cation tasks as well as a simple game. We \ufb01rst\ndescribe the design choices that were common to all our experiments:\nRetina and location encodings: The retina encoding \u03c1(x, l) extracts k square patches centered at\nlocation l, with the \ufb01rst patch being gw \u00d7 gw pixels in size, and each successive patch having twice\nthe width of the previous. The k patches are then all resized to gw \u00d7 gw and concatenated. Glimpse\nlocations l were encoded as real-valued (x, y) coordinates2 with (0, 0) being the center of the image\nx and (\u22121, \u22121) being the top left corner of x.\nGlimpse network: The glimpse network fg(x, l) had two fully connected layers. Let Linear(x) de-\nnote a linear transformation of the vector x, i.e. Linear(x) = Wx+b for some weight matrix W and\nbias vector b, and let Rect(x) = max(x, 0) be the recti\ufb01er nonlinearity. The output g of the glimpse\nnetwork was de\ufb01ned as g = Rect(Linear(hg) + Linear(hl)) where hg = Rect(Linear(\u03c1(x, l)))\nand hl = Rect(Linear(l)). The dimensionality of hg and hl was 128 while the dimensionality of\ng was 256 for all attention models trained in this paper.\nLocation network: The policy for the locations l was de\ufb01ned by a two-component Gaussian with a\n\ufb01xed variance. The location network outputs the mean of the location policy at time t and is de\ufb01ned\nas fl(h) = Linear(h) where h is the state of the core network/RNN.\nCore network: For the classi\ufb01cation experiments that follow the core fh was a network of recti\ufb01er\nunits de\ufb01ned as ht = fh(ht\u22121) = Rect(Linear(ht\u22121) + Linear(gt)). The experiment done on a\ndynamic environment used a core of LSTM units [10].\n2We also experimented with using a discrete representation for the locations l but found that it was dif\ufb01cult\nto learn policies over more than 25 possible discrete locations.\n5\n(a) 28x28 MNIST\nModel\nError\nFC, 2 layers (256 hiddens each)\n1.35%\n1 Random Glimpse, 8 \u00d7 8, 1 scale\n42.85%\nRAM, 2 glimpses, 8 \u00d7 8, 1 scale\n6.27%\nRAM, 3 glimpses, 8 \u00d7 8, 1 scale\n2.7%\nRAM, 4 glimpses, 8 \u00d7 8, 1 scale\n1.73%\nRAM, 5 glimpses, 8 \u00d7 8, 1 scale\n1.55%\nRAM, 6 glimpses, 8 \u00d7 8, 1 scale\n1.29%\nRAM, 7 glimpses, 8 \u00d7 8, 1 scale\n1.47%\n(b) 60x60 Translated MNIST\nModel\nError\nFC, 2 layers (64 hiddens each)\n7.56%\nFC, 2 layers (256 hiddens each)\n3.7%\nConvolutional, 2 layers\n2.31%\nRAM, 4 glimpses, 12 \u00d7 12, 3 scales\n2.29%\nRAM, 6 glimpses, 12 \u00d7 12, 3 scales\n1.86%\nRAM, 8 glimpses, 12 \u00d7 12, 3 scales\n1.84%\nTable 1: Classi\ufb01cation results on the MNIST and Translated MNIST datasets. FC denotes a fully-\nconnected network with two layers of recti\ufb01er units. The convolutional network had one layer of 8\n10 \u00d7 10 \ufb01lters with stride 5, followed by a fully connected layer with 256 units with recti\ufb01ers after\neach layer. Instances of the attention model are labeled with the number of glimpses, the number of\nscales in the retina, and the size of the retina.\n(a) Random test cases for the Translated MNIST\ntask.\n(b) Random test cases for the Cluttered Translated\nMNIST task.\nFigure 2: Examples of test cases for the Translated and Cluttered Translated MNIST tasks.\n4.1\nImage Classi\ufb01cation\nThe attention network used in the following classi\ufb01cation experiments made a classi\ufb01cation decision\nonly at the last timestep t = N. The action network fa was simply a linear softmax classi\ufb01er de\ufb01ned\nas fa(h) = exp (Linear(h)) /Z, where Z is a normalizing constant. The RNN state vector h had\ndimensionality 256. All methods were trained using stochastic gradient descent with momentum of\n0.9. Hyperparameters such as the learning rate and the variance of the location policy were selected\nusing random search [3]. The reward at the last time step was 1 if the agent classi\ufb01ed correctly and\n0 otherwise. The rewards for all other timesteps were 0.\nCentered Digits: We \ufb01rst tested the ability of our training method to learn successful glimpse\npolicies by using it to train RAM models with up to 7 glimpses on the MNIST digits dataset. The\n\u201cretina\u201d for this experiment was simply an 8\u00d78 patch, which is only big enough to capture a part of\na digit, hence the experiment also tested the ability of RAM to combine information from multiple\nglimpses. Note that since the \ufb01rst glimpse is always random, the single glimpse model is effectively\na classi\ufb01er that gets a single random 8 \u00d7 8 patch as input. We also trained a standard feedforward\nneural network with two hidden layers of 256 recti\ufb01ed linear units as a baseline. The error rates\nachieved by the different models on the test set are shown in Table 1a. We see that each additional\nglimpse improves the performance of RAM until it reaches its minimum with 6 glimpses, where it\nmatches the performance of the fully connected model training on the full 28 \u00d7 28 centered digits.\nThis demonstrates the model can successfully learn to combine information from multiple glimpses.\nNon-Centered Digits: The second problem we considered was classifying non-centered digits. We\ncreated a new task called Translated MNIST, for which data was generated by placing an MNIST\ndigit in a random location of a larger blank patch. Training cases were generated on the \ufb02y so the\neffective training set size was 50000 (the size of the MNIST training set) multiplied by the possible\nnumber of locations. Figure 2a contains a random sample of test cases for the 60 by 60 Translated\nMNIST task. Table 1b shows the results for several different models trained on the Translated\nMNIST task with 60 by 60 patches. In addition to RAM and two fully-connected networks we\nalso trained a network with one convolutional layer of 16 10 \u00d7 10 \ufb01lters with stride 5 followed\nby a recti\ufb01er nonlinearity and then a fully-connected layer of 256 recti\ufb01er units. The convolutional\nnetwork, the RAM networks, and the smaller fully connected model all had roughly the same number\nof parameters. Since the convolutional network has some degree of translation invariance built in, it\n6\n(a) 60x60 Cluttered Translated MNIST\nModel\nError\nFC, 2 layers (64 hiddens each)\n28.96%\nFC, 2 layers (256 hiddens each)\n13.2%\nConvolutional, 2 layers\n7.83%\nRAM, 4 glimpses, 12 \u00d7 12, 3 scales\n7.1%\nRAM, 6 glimpses, 12 \u00d7 12, 3 scales\n5.88%\nRAM, 8 glimpses, 12 \u00d7 12, 3 scales\n5.23%\n(b) 100x100 Cluttered Translated MNIST\nModel\nError\nConvolutional, 2 layers\n16.51%\nRAM, 4 glimpses, 12 \u00d7 12, 4 scales\n14.95%\nRAM, 6 glimpses, 12 \u00d7 12, 4 scales\n11.58%\nRAM, 8 glimpses, 12 \u00d7 12, 4 scales\n10.83%\nTable 2: Classi\ufb01cation on the Cluttered Translated MNIST dataset. FC denotes a fully-connected\nnetwork with two layers of recti\ufb01er units. The convolutional network had one layer of 8 10 \u00d7 10\n\ufb01lters with stride 5, followed by a fully connected layer with 256 units in the 60 \u00d7 60 case and\n86 units in the 100 \u00d7 100 case with recti\ufb01ers after each layer. Instances of the attention model are\nlabeled with the number of glimpses, the size of the retina, and the number of scales in the retina.\nAll models except for the big fully connected network had roughly the same number of parameters.\nFigure 3: Examples of the learned policy on 60 \u00d7 60 cluttered-translated MNIST task. Column 1:\nThe input image with glimpse path overlaid in green. Columns 2-7: The six glimpses the network\nchooses. The center of each image shows the full resolution glimpse, the outer low resolution areas\nare obtained by upscaling the low resolution glimpses back to full image size. The glimpse paths\nclearly show that the learned policy avoids computation in empty or noisy parts of the input space\nand directly explores the area around the object of interest.\nattains a signi\ufb01cantly lower error rate of 2.3% than the fully connected networks. However, RAM\nwith 4 glimpses gets roughly the same performance as the convolutional network and outperforms\nit for 6 and 8 glimpses, reaching roughly 1.9% error. This is possible because the attention model\ncan focus its retina on the digit and hence learn a translation invariant policy. This experiment also\nshows that the attention model is able to successfully search for an object in a big image when the\nobject is not centered.\nCluttered Non-Centered Digits: One of the most challenging aspects of classifying real-world\nimages is the presence of a wide range clutter. Systems that operate on the entire image at full\nresolution are particularly susceptible to clutter and must learn to be invariant to it. One possible\nadvantage of an attention mechanism is that it may make it easier to learn in the presence of clutter\nby focusing on the relevant part of the image and ignoring the irrelevant part. We test this hypothesis\nwith several experiments on a new task we call Cluttered Translated MNIST. Data for this task was\ngenerated by \ufb01rst placing an MNIST digit in a random location of a larger blank image and then\nadding random 8 by 8 subpatches from other random MNIST digits to random locations of the\nimage. The goal is to classify the complete digit present in the image. Figure 2b shows a random\nsample of test cases for the 60 by 60 Cluttered Translated MNIST task.\nTable 2a shows the classi\ufb01cation results for the models we trained on 60 by 60 Cluttered Translated\nMNIST with 4 pieces of clutter. The presence of clutter makes the task much more dif\ufb01cult but the\nperformance of the attention model is affected less than the performance of the other models. RAM\nwith 4 glimpses reaches 7.1% error, which outperforms fully-connected models by a wide margin\nand the convolutional neural network by 0.7%, and RAM trained with 6 and 8 glimpses achieves\neven lower error. Since RAM achieves larger relative error improvements over a convolutional\nnetwork in the presence of clutter these results suggest the attention-based models may be better at\ndealing with clutter than convolutional networks because they can simply ignore it by not looking at\n7\nit. Two samples of learned policy is shown in Figure 6 and more are included in the supplementary\nmaterials. The \ufb01rst column shows the original data point with the glimpse path overlaid. The\nlocation of the \ufb01rst glimpse is marked with a \ufb01lled circle and the location of the \ufb01nal glimpse is\nmarked with an empty circle. The intermediate points on the path are traced with solid straight\nlines. Each consecutive image to the right shows a representation of the glimpse that the network\nsees. It can be seen that the learned policy can reliably \ufb01nd and explore around the object of interest\nwhile avoiding clutter at the same time.\nTo further test this hypothesis we also performed experiments on 100 by 100 Cluttered Translated\nMNIST with 8 pieces of clutter. The test errors achieved by the models we compared are shown\nin Table 2b. The results show similar improvements of RAM over a convolutional network. It has\nto be noted that the overall capacity and the amount of computation of our model does not change\nfrom 60 \u00d7 60 images to 100 \u00d7 100, whereas the hidden layer of the convolutional network that is\nconnected to the linear layer grows linearly with the number of pixels in the input.\n4.2\nDynamic Environments\nOne appealing property of the recurrent attention model is that it can be applied to videos or inter-\nactive problems with a visual input just as easily as to static image tasks. We test the ability of our\napproach to learn a control policy in a dynamic visual environment while perceiving the environment\nthrough a bandwidth-limited retina by training it to play a simple game. The game is played on a 24\nby 24 screen of binary pixels and involves two objects: a single pixel that represents a ball falling\nfrom the top of the screen while bouncing off the sides of the screen and a two-pixel paddle posi-\ntioned at the bottom of the screen which the agent controls with the aim of catching the ball. When\nthe falling pixel reaches the bottom of the screen the agent either gets a reward of 1 if the paddle\noverlaps with the ball and a reward of 0 otherwise. The game then restarts from the beginning.\nWe trained the recurrent attention model to play the game of \u201cCatch\u201d using only the \ufb01nal reward\nas input. The network had a 6 by 6 retina at three scales as its input, which means that the agent\nhad to capture the ball in the 6 by 6 highest resolution region in order to know its precise position.\nIn addition to the two location actions, the attention model had three game actions (left, right, and\ndo nothing) and the action network fa used a linear softmax to model a distribution over the game\nactions. We used a core network of 256 LSTM units.\nWe performed random search to \ufb01nd suitable hyper-parameters and trained each agent for 20 mil-\nlion frames.\nA video of the best agent, which catches the ball roughly 85% of the time, can\nbe downloaded from http://www.cs.toronto.edu/\u02dcvmnih/docs/attention.mov.\nThe video shows that the recurrent attention model learned to play the game by tracking the ball\nnear the bottom of the screen. Since the agent was not in any way told to track the ball and was\nonly rewarded for catching it, this result demonstrates the ability of the model to learn effective\ntask-speci\ufb01c attention policies.\n5\nDiscussion\nThis paper introduced a novel visual attention model that is formulated as a single recurrent neural\nnetwork which takes a glimpse window as its input and uses the internal state of the network to\nselect the next location to focus on as well as to generate control signals in a dynamic environment.\nAlthough the model is not differentiable, the proposed uni\ufb01ed architecture is trained end-to-end\nfrom pixel inputs to actions using a policy gradient method. The model has several appealing prop-\nerties. First, both the number of parameters and the amount of computation RAM performs can\nbe controlled independently of the size of the input images. Second, the model is able to ignore\nclutter present in an image by centering its retina on the relevant regions. Our experiments show that\nRAM signi\ufb01cantly outperforms a convolutional architecture with a comparable number of parame-\nters on a cluttered object classi\ufb01cation task. Additionally, the \ufb02exibility of our approach allows for\na number of interesting extensions. For example, the network can be augmented with another action\nthat allows it terminate at any time point and make a \ufb01nal classi\ufb01cation decision. Our preliminary\nexperiments show that this allows the network to learn to stop taking glimpses once it has enough in-\nformation to make a con\ufb01dent classi\ufb01cation. The network can also be allowed to control the scale at\nwhich the retina samples the image allowing it to \ufb01t objects of different size in the \ufb01xed size retina.\nIn both cases, the extra actions can be simply added to the action network fa and trained using the\npolicy gradient procedure we have described. Given the encouraging results achieved by RAM, ap-\nplying the model to large scale object recognition and video classi\ufb01cation is a natural direction for\nfuture work.\n8\nSupplementary Material\nFigure 4: Examples of the learned policy on 60 \u00d7 60 cluttered-translated MNIST task. Column 1:\nThe input image from MNIST test set with glimpse path overlaid in green (correctly classi\ufb01ed) or\nred (false classi\ufb01ed). Columns 2-7: The six glimpses the network chooses. The center of each image\nshows the full resolution glimpse, the outer low resolution areas are obtained by upscaling the low\nresolution glimpses back to full image size. The glimpse paths clearly show that the learned policy\navoids computation in empty or noisy parts of the input space and directly explores the area around\nthe object of interest.\n9\nFigure 5: Examples of the learned policy on 60 \u00d7 60 cluttered-translated MNIST task. Column 1:\nThe input image from MNIST test set with glimpse path overlaid in green (correctly classi\ufb01ed) or\nred (false classi\ufb01ed). Columns 2-7: The six glimpses the network chooses. The center of each image\nshows the full resolution glimpse, the outer low resolution areas are obtained by upscaling the low\nresolution glimpses back to full image size. The glimpse paths clearly show that the learned policy\navoids computation in empty or noisy parts of the input space and directly explores the area around\nthe object of interest.\n10\nFigure 6: Examples of the learned policy on 60 \u00d7 60 cluttered-translated MNIST task. Column 1:\nThe input image from MNIST test set with glimpse path overlaid in green (correctly classi\ufb01ed) or\nred (false classi\ufb01ed). Columns 2-7: The six glimpses the network chooses. The center of each image\nshows the full resolution glimpse, the outer low resolution areas are obtained by upscaling the low\nresolution glimpses back to full image size. The glimpse paths clearly show that the learned policy\navoids computation in empty or noisy parts of the input space and directly explores the area around\nthe object of interest.\n11\nReferences\n[1] Bogdan Alexe, Thomas Deselaers, and Vittorio Ferrari. What is an object? In CVPR, 2010.\n[2] Bogdan Alexe, Nicolas Heess, Yee Whye Teh, and Vittorio Ferrari. Searching for objects driven by\ncontext. In NIPS, 2012.\n[3] James Bergstra and Yoshua Bengio. Random search for hyper-parameter optimization. The Journal of\nMachine Learning Research, 13:281\u2013305, 2012.\n[4] Nicholas J. Butko and Javier R. Movellan. Optimal scanning for faster object detection. In CVPR, 2009.\n[5] N.J. Butko and J.R. Movellan. I-pomdp: An infomax model of eye movement. In Proceedings of the 7th\nIEEE International Conference on Development and Learning, ICDL \u201908, pages 139 \u2013144, 2008.\n[6] Misha Denil, Loris Bazzani, Hugo Larochelle, and Nando de Freitas. Learning where to attend with deep\narchitectures for image tracking. Neural Computation, 24(8):2151\u20132184, 2012.\n[7] Pedro F. Felzenszwalb, Ross B. Girshick, and David A. McAllester. Cascade object detection with de-\nformable part models. In CVPR, 2010.\n[8] Ross B. Girshick, Jeff Donahue, Trevor Darrell, and Jitendra Malik. Rich feature hierarchies for accurate\nobject detection and semantic segmentation. CoRR, abs/1311.2524, 2013.\n[9] Mary Hayhoe and Dana Ballard. Eye movements in natural behavior. Trends in Cognitive Sciences,\n9(4):188 \u2013 194, 2005.\n[10] Sepp Hochreiter and J\u00a8urgen Schmidhuber. Long short-term memory. Neural computation, 9(8):1735\u2013\n1780, 1997.\n[11] L. Itti, C. Koch, and E. Niebur. A model of saliency-based visual attention for rapid scene analysis. IEEE\nTransactions on Pattern Analysis and Machine Intelligence, 20(11):1254\u20131259, 1998.\n[12] Alex Krizhevsky, Ilya Sutskever, and Geoff Hinton.\nImagenet classi\ufb01cation with deep convolutional\nneural networks. In Advances in Neural Information Processing Systems 25, pages 1106\u20131114, 2012.\n[13] Christoph H. Lampert, Matthew B. Blaschko, and Thomas Hofmann. Beyond sliding windows: Object\nlocalization by ef\ufb01cient subwindow search. In CVPR, 2008.\n[14] Hugo Larochelle and Geoffrey E. Hinton. Learning to combine foveal glimpses with a third-order boltz-\nmann machine. In NIPS, 2010.\n[15] Stefan Mathe and Cristian Sminchisescu. Action from still image dataset and inverse optimal control to\nlearn task speci\ufb01c visual scanpaths. In NIPS, 2013.\n[16] Lucas Paletta, Gerald Fritz, and Christin Seifert. Q-learning of sequential attention for visual object\nrecognition from informative local descriptors. In CVPR, 2005.\n[17] M. Ranzato. On Learning Where To Look. ArXiv e-prints, 2014.\n[18] Ronald A. Rensink. The dynamic representation of scenes. Visual Cognition, 7(1-3):17\u201342, 2000.\n[19] Pierre Sermanet, David Eigen, Xiang Zhang, Micha\u00a8el Mathieu, Rob Fergus, and Yann LeCun. Overfeat:\nIntegrated recognition, localization and detection using convolutional networks. CoRR, abs/1312.6229,\n2013.\n[20] Kenneth O. Stanley and Risto Miikkulainen. Evolving a roving eye for go. In GECCO, 2004.\n[21] Richard S. Sutton, David Mcallester, Satinder Singh, and Yishay Mansour. Policy gradient methods for\nreinforcement learning with function approximation. In NIPS, pages 1057\u20131063. MIT Press, 2000.\n[22] Antonio Torralba, Aude Oliva, Monica S Castelhano, and John M Henderson. Contextual guidance of eye\nmovements and attention in real-world scenes: the role of global features in object search. Psychol Rev,\npages 766\u2013786, 2006.\n[23] K E A van de Sande, J.R.R. Uijlings, T Gevers, and A.W.M. Smeulders. Segmentation as Selective Search\nfor Object Recognition. In ICCV, 2011.\n[24] Paul A. Viola and Michael J. Jones. Rapid object detection using a boosted cascade of simple features. In\nCVPR, 2001.\n[25] Daan Wierstra, Alexander Foerster, Jan Peters, and Juergen Schmidhuber. Solving deep memory pomdps\nwith recurrent policy gradients. In ICANN. 2007.\n[26] R.J. Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning.\nMachine Learning, 8(3):229\u2013256, 1992.\n12\n",
        "sentence": " For example [3] and [12] consider that the sequential process is an acquisition process able to focus on relevant parts of the input data; [9] for example focuses on the sequential prediction process with a cascade approach. More precisely, our learning method is close to the methods proposed in [18] and [12] with the difference that, instead of considering a reward signal which is usual in reinforcement learning, we consider a loss function \u2206 computing the quality of the system. Particularly, the use of recurrent neural networks from modelling Markov Decision Processes learned by Policy gradient techniques has been deeply explored in [18] and in a recent work that proposes the use of such models for image classification [12].",
        "context": "deep learning framework [6, 14, 17]. Our formulation which employs an RNN to integrate visual\ninformation over time and to decide how to act is, however, more general, and our learning procedure\n[26] R.J. Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning.\nMachine Learning, 8(3):229\u2013256, 1992.\n12\n3.1\nModel\nThe agent is built around a recurrent neural network as shown in Fig. 1. At each time step, it\nprocesses the sensor data, integrates information over time, and chooses how to act and how to\ndeploy its sensor at next time step:"
    },
    {
        "title": "Induction of decision trees",
        "author": [
            "J. Ross Quinlan"
        ],
        "venue": "Machine Learning,",
        "citeRegEx": "13",
        "shortCiteRegEx": "13",
        "year": 1986,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " They also share the idea of processing different inputs with different computations which is the a major idea underlying decision trees [13] and also more recent classification techniques like [3].",
        "context": null
    },
    {
        "title": "Multi-column deep neural networks for image classification",
        "author": [
            "Jurgen Schmidhuber"
        ],
        "venue": "In Proceedings of the 2012 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
        "citeRegEx": "14",
        "shortCiteRegEx": "14",
        "year": 2012,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " The use of deep architectures have shown impressive results for many different tasks, from image classification [10, 14], speech recognition [7] to machine translation [19] or even for natural language processing [16].",
        "context": null
    },
    {
        "title": "Neural trees: a new tool for classification",
        "author": [
            "J A Sirat",
            "J-P Nadal"
        ],
        "venue": "Network: Computation in Neural Systems,",
        "citeRegEx": "15",
        "shortCiteRegEx": "15",
        "year": 1990,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " The idea of processing input data by different functions is not new and have been proposed for example in Neural Tree Networks [17, 15], with Hierarchical Mixture of Experts [8] where the idea is to compute different transformations of data and to aggregate these transformations.",
        "context": null
    },
    {
        "title": "Deep learning for NLP (without magic)",
        "author": [
            "Richard Socher",
            "Christopher D. Manning"
        ],
        "venue": "In Human Language Technologies,",
        "citeRegEx": "16",
        "shortCiteRegEx": "16",
        "year": 2013,
        "abstract": "",
        "full_text": "",
        "sentence": " The use of deep architectures have shown impressive results for many different tasks, from image classification [10, 14], speech recognition [7] to machine translation [19] or even for natural language processing [16].",
        "context": null
    },
    {
        "title": "Perceptron trees: A case study in hybrid concept representations",
        "author": [
            "Paul E. Utgoff"
        ],
        "venue": "In Proceedings of the 7th National Conference on Artificial Intelligence. St. Paul,",
        "citeRegEx": "17",
        "shortCiteRegEx": "17",
        "year": 1988,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " The idea of processing input data by different functions is not new and have been proposed for example in Neural Tree Networks [17, 15], with Hierarchical Mixture of Experts [8] where the idea is to compute different transformations of data and to aggregate these transformations.",
        "context": null
    },
    {
        "title": "Solving deep memory pomdps with recurrent policy gradients",
        "author": [
            "Daan Wierstra",
            "Alexander F\u00f6rster",
            "Jan Peters",
            "J\u00fcrgen Schmidhuber"
        ],
        "venue": "In Artificial Neural Networks - ICANN",
        "citeRegEx": "18",
        "shortCiteRegEx": "18",
        "year": 2007,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " More precisely, our learning method is close to the methods proposed in [18] and [12] with the difference that, instead of considering a reward signal which is usual in reinforcement learning, we consider a loss function \u2206 computing the quality of the system. Instead of using this estimate, we replace \u2206(F (xi, H), y) by \u2206(F (xi, H), y) \u2212 b where b = Ep(x,H,y)[\u2206(F (xi, H), y)] which can be easily estimated on the training set [18]. Particularly, the use of recurrent neural networks from modelling Markov Decision Processes learned by Policy gradient techniques has been deeply explored in [18] and in a recent work that proposes the use of such models for image classification [12].",
        "context": null
    },
    {
        "title": "Bilingual word embeddings for phrase-based machine translation",
        "author": [
            "Will Y. Zou",
            "Richard Socher",
            "Daniel M. Cer",
            "Christopher D. Manning"
        ],
        "venue": "In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,",
        "citeRegEx": "19",
        "shortCiteRegEx": "19",
        "year": 2013,
        "abstract": "",
        "full_text": "",
        "sentence": " The use of deep architectures have shown impressive results for many different tasks, from image classification [10, 14], speech recognition [7] to machine translation [19] or even for natural language processing [16].",
        "context": null
    }
]