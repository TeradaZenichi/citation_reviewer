[
    {
        "title": "Bayesian posterior sampling via stochastic gradient Fisher scoring",
        "author": [
            "S. Ahn",
            "A. Korattikara",
            "M. Welling"
        ],
        "venue": "In Proc. 29th ICML,",
        "citeRegEx": "Ahn et al\\.,? \\Q2012\\E",
        "shortCiteRegEx": "Ahn et al\\.",
        "year": 2012,
        "abstract": "In this paper we address the following question: Can we approximately sample\nfrom a Bayesian posterior distribution if we are only allowed to touch a small\nmini-batch of data-items for every sample we generate?. An algorithm based on\nthe Langevin equation with stochastic gradients (SGLD) was previously proposed\nto solve this, but its mixing rate was slow. By leveraging the Bayesian Central\nLimit Theorem, we extend the SGLD algorithm so that at high mixing rates it\nwill sample from a normal approximation of the posterior, while for slow mixing\nrates it will mimic the behavior of SGLD with a pre-conditioner matrix. As a\nbonus, the proposed algorithm is reminiscent of Fisher scoring (with stochastic\ngradients) and as such an efficient optimizer during burn-in.",
        "full_text": "Bayesian Posterior Sampling via Stochastic Gradient Fisher Scoring\nSungjin Ahn\nSUNGJIA@ICS.UCI.EDU\nDept. of Computer Science, UC Irvine, Irvine, CA 92697-3425, USA\nAnoop Korattikara\nAKORATTI@ICS.UCI.EDU\nDept. of Computer Science, UC Irvine, Irvine, CA 92697-3425, USA\nMax Welling\nWELLING@ICS.UCI.EDU\nDept. of Computer Science, UC Irvine, Irvine, CA 92697-3425, USA\nAbstract\nIn this paper we address the following question:\n\u201cCan we approximately sample from a Bayesian\nposterior distribution if we are only allowed to\ntouch a small mini-batch of data-items for ev-\nery sample we generate?\u201d. An algorithm based\non the Langevin equation with stochastic gradi-\nents (SGLD) was previously proposed to solve\nthis, but its mixing rate was slow. By leverag-\ning the Bayesian Central Limit Theorem, we ex-\ntend the SGLD algorithm so that at high mix-\ning rates it will sample from a normal approx-\nimation of the posterior, while for slow mixing\nrates it will mimic the behavior of SGLD with a\npre-conditioner matrix. As a bonus, the proposed\nalgorithm is reminiscent of Fisher scoring (with\nstochastic gradients) and as such an ef\ufb01cient op-\ntimizer during burn-in.\n1. Motivation\nWhen a dataset has a billion data-cases (as is not uncom-\nmon these days) MCMC algorithms will not even have gen-\nerated a single (burn-in) sample when a clever learning al-\ngorithm based on stochastic gradients may already be mak-\ning fairly good predictions. In fact, the intriguing results of\nBottou and Bousquet (2008) seem to indicate that in terms\nof \u201cnumber of bits learned per unit of computation\u201d, an al-\ngorithm as simple as stochastic gradient descent is almost\noptimally ef\ufb01cient. We therefore argue that for Bayesian\nmethods to remain useful in an age when the datasets grow\nat an exponential rate, they need to embrace the ideas of the\nstochastic optimization literature.\nAppearing in Proceedings of the 29 th International Conference\non Machine Learning, Edinburgh, Scotland, UK, 2012. Copyright\n2012 by the author(s)/owner(s).\nA \ufb01rst attempt in this direction was proposed by Welling\nand Teh (2011) where the authors show that (uncorrected)\nLangevin dynamics with stochastic gradients (SGLD) will\nsample from the correct posterior distribution when the\nstepsizes are annealed to zero at a certain rate.\nWhile\nSGLD succeeds in (asymptotically) generating samples\nfrom the posterior at O(n) computational cost with (n \u226a\nN) it\u2019s mixing rate is unnecessarily slow.\nThis can be\ntraced back to its lack of a proper pre-conditioner: SGLD\ntakes large steps in directions of small variance and re-\nversely, small steps in directions of large variance which\nhinders convergence of the Markov chain.\nOur work\nbuilds on top of Welling and Teh (2011). We leverage the\n\u201cBayesian Central Limit Theorem\u201d which states that when\nN is large (and under certain conditions) the posterior will\nbe well approximated by a normal distribution. Our al-\ngorithm is designed so that for large stepsizes (and thus\nat high mixing rates) it will sample from this approximate\nnormal distribution, while at smaller stepsizes (and thus at\nslower mixing rates) it will generate samples from an in-\ncreasingly accurate (non-Gaussian) approximation of the\nposterior. Our main claim is therefore that we can trade-in\na usually small bias in our estimate of the posterior distri-\nbution against a potentially very large computational gain,\nwhich could in turn be used to draw more samples and re-\nduce sampling variance.\nFrom an optimization perspective one may view this algo-\nrithm as a Fisher scoring method based on stochastic gradi-\nents (see e.g. (Schraudolph et al., 2007)) but in such a way\nthat the randomness introduced in the subsampling process\nis used to sample from the posterior distribution when we\narrive at its mode. Hence, it is an ef\ufb01cient optimization al-\ngorithm that smoothly turns into a sampler when the correct\n(statistical) scale of precision is reached.\nBayesian Posterior Sampling via Stochastic Gradient Fisher Scoring\n2. Preliminaries\nWe will start with some notation, de\ufb01nitions and prelimi-\nnaries. We have a large dataset XN consisting of N i.i.d.\ndata-points {x1...xN} and we use a family of distributions\nparametrized by \u03b8 \u2208RD to model the distribution of the\nxi\u2019s. We choose a prior distribution p(\u03b8) and are inter-\nested in obtaining samples from the posterior distribution,\np(\u03b8|XN) \u221dp(XN|\u03b8)p(\u03b8).\nAs is common in Bayesian asymptotic theory, we will also\nmake use of some frequentist concepts in the develop-\nment of our method. We assume that the true data gen-\nerating distribution is in our family of models and denote\nthe true parameter which generated the dataset XN by \u03b80.\nWe denote the score or the gradient of the log likelihood\nw.r.t. data-point xi by gi(\u03b8) = g(\u03b8; xi) = \u2207\u03b8 log p(\u03b8; xi).\nWe denote the sum of scores of a batch of n data-points\nXr = {xr1...xrn} by Gn(\u03b8; Xr) = Pn\ni=1 g(\u03b8; xri) and\nthe average by gn(\u03b8; Xr) = 1\nn Gn(\u03b8;Xr). Sometimes we will\ndrop the argument Xr and instead simply write Gn(\u03b8) and\ngn(\u03b8) for convenience.\nThe covariance of the gradients is called the Fisher infor-\nmation de\ufb01ned as I(\u03b8) = Ex[g(\u03b8; x)g(\u03b8; x)T ], where Ex\ndenotes expectation w.r.t the distribution p(x; \u03b8) and we\nhave used the fact that Ex[g(\u03b8; x)] = 0. It can also be\nshown that I(\u03b8) = \u2212Ex[H(\u03b8; x)], where H is the Hessian\nof the log likelihood.\nSince we are dealing with a dataset with samples only\nfrom p(x; \u03b80) we will henceforth be interested only in\nI(\u03b80) which we will denote by I1. It is easy to see that\nthe Fisher information of n data-points, In = nI1. The\nempirical covariance of the scores computed from a batch\nof n data-points is called the empirical Fisher information,\nV (\u03b8; Xr) =\n1\nn\u22121\nPn\ni=1 (gri(\u03b8) \u2212gn(\u03b8)) (gri(\u03b8) \u2212gn(\u03b8))T\n(Scott, 2002). Also, it can be shown that V (\u03b80) is a consis-\ntent estimator of I1 = I(\u03b80).\nWe now introduce an important result in Bayesian asymp-\ntotic theory. As N becomes large, the posterior distribution\nbecomes concentrated in a small neighbourhood around \u03b80\nand becomes asymptotically Gaussian. This is formalized\nby the Bernstein-von Mises theorem, a.k.a the Bayesian\nCentral Limit Theorem, (Le Cam, 1986), which states that\nunder suitable regularity conditions, p(\u03b8| {x1...xN}) ap-\nproximately equals N(\u03b80, I\u22121\nN ) as N becomes very large.\n3. Stochastic Gradient Fisher Scoring\nWe are now ready to derive our Stochastic Gradient Fisher\nScoring (SGFS) algorithm. The starting point in the deriva-\ntion of our method is the Stochastic Gradient Langevin Dy-\nnamics (SGLD) algorithm (Welling & Teh, 2011) which\nwe describe in section 3.1. SGLD can sample accurately\nfrom the posterior but suffers from a low mixing rate. In\nsection 3.2, we show that it is easy to construct a Markov\nchain that can sample from a normal approximation of the\nposterior at any mixing rate. We will then combine these\nmethods to develop our Stochastic Gradient Fisher Scoring\n(SGFS) algorithm in section 3.3.\n3.1. Stochastic Gradient Langevin Dynamics\nThe SGLD algorithm has the following update equation:\n\u03b8t+1 \u2190\u03b8t + \u03f5C\n2\n\b\n\u2207log p(\u03b8t) + Ngn(\u03b8t; Xt\nn)\n\t\n+ \u03bd\nwhere\n\u03bd \u223cN(0, \u03f5C) (1)\nHere \u03f5 is the step size, C is called the preconditioning ma-\ntrix (Girolami & Calderhead, 2010) and \u03bd is a random vari-\nable representing injected Gaussian noise. The gradient of\nthe log likelihood GN(\u03b8; XN) over the whole dataset is ap-\nproximated by scaling the mean gradient gn(\u03b8t; Xt\nn) com-\nputed from a mini-batch Xt\nn = {xt1...xtn} of size n \u226aN.\nWelling & Teh (2011) showed that Eqn. (1) generates sam-\nples from the posterior distribution if the step size is an-\nnealed to zero at a certain rate. As the step size goes to zero,\nthe discretization error in the Langevin equation disap-\npears and we do not need to conduct expensive Metropolis-\nHasting(MH) accept/reject tests that use the whole dataset.\nThus, this algorithm requires only O(n) computations to\ngenerate each sample, unlike traditional MCMC algorithms\nwhich require O(N) computations per sample.\nHowever, since the step sizes are reduced to zero, the mix-\ning rate is reduced as well, and a large number of iterations\nare required to obtain a good coverage of the parameter\nspace. One way to make SGLD work at higher step sizes is\nto introduce MH accept/reject steps to correct for the higher\ndiscretization error, but our initial attempts using only a\nmini-batch instead of the whole dataset were unsuccessful.\n3.2. Sampling from the Approximate Posterior\nSince it is not clear how to use Eqn. (1) at high step sizes,\nwe will move away from Langevin dynamics and explore\na different approach. As mentioned in section 2, the poste-\nrior distribution can be shown to approach a normal distri-\nbution, N(\u03b80, I\u22121\nN ), as the size of the dataset becomes very\nlarge. It is easy to construct a Markov chain which will\nsample from this approximation of the posterior at any step\nsize. We will now show that the following update equation\nachieves this:\n\u03b8t+1 \u2190\u03b8t + \u03f5C\n2 {\u2212IN(\u03b8t \u2212\u03b80)} + \u03c9\nwhere\n\u03c9 \u223cN(0, \u03f5C \u2212\u03f52\n4 CINC)\n(2)\nBayesian Posterior Sampling via Stochastic Gradient Fisher Scoring\nThe update is an af\ufb01ne transformation of \u03b8t plus injected\nindependent Gaussian noise, \u03c9. Thus if \u03b8t has a Gaussian\ndistribution N(\u00b5t, \u03a3t), \u03b8t+1 will also have a Gaussian dis-\ntribution, which we will denote as N(\u00b5t+1, \u03a3t+1). These\ndistributions are related by:\n\u00b5t+1 = (I \u2212\u03f5C\n2 IN)\u00b5t + \u03f5C\n2 IN\u03b80\n\u03a3t+1 = (I \u2212\u03f5C\n2 IN)\u03a3t(I \u2212\u03f5C\n2 IN)T + \u03f5C \u2212\u03f52\n4 CINC\n(3)\nIf we choose C to be symmetric, it is easy to see that the\napproximate posterior distribution, N(\u03b80, I\u22121\nN ), is an in-\nvariant distribution of this Markov chain. Since Eqn. (2) is\nnot a Langevin equation, it samples from the approximate\nposterior at large step-size and does not require any MH\naccept/reject steps. The only requirement is that C should\nbe symmetric and should be chosen so that the covariance\nmatrix of the injected noise in Eqn. (2) is positive-de\ufb01nite.\n3.3. Stochastic Gradient Fisher Scoring\nIn practical problems both sampling accuracy and mixing\nrate are important, and the extreme regimes dictated by\nboth the above methods are very limiting. If the posterior\nis close to Gaussian (as is usually the case), we would like\nto take advantage of the high mixing rate. However, if we\nneed to capture a highly non-Gaussian posterior, we should\nbe able to trade-off mixing rate for sampling accuracy. One\ncould also think about doing this in an \u201canytime\u201d fashion\nwhere if the posterior is somewhat close to Gaussian, we\ncan start by sampling from a Gaussian approximation at\nhigh mixing rates, but slow down the mixing rate to capture\nthe non-Gaussian structure if more computation becomes\navailable. In other words, one should have the freedom to\nmanage the right trade off between sampling accuracy and\nmixing rate depending on the problem at hand.\nWith this goal in mind, we combine the above methods to\ndevelop our Stochastic Gradient Fisher Scoring (SGFS) al-\ngorithm. We accomplish this using a Markov chain with\nthe following update equation:\n\u03b8t+1 \u2190\u03b8t + \u03f5C\n2\n\b\n\u2207log p(\u03b8t) + Ngn(\u03b8t; Xt\nn)\n\t\n+ \u03c4\nwhere\n\u03c4 \u223cN(0, Q) (4)\nWhen the step size is small, we want to choose Q = \u03f5C so\nthat it behaves like the Markov chain in Eqn (1). Now we\nwill see how to choose Q so that when the step size is large\nand the posterior is approximately Gaussian, our algorithm\nbehaves like the Markov chain in Eqn. (2). First, note that\nif n is large enough for the central limit theorem to hold,\nwe have:\ngn(\u03b8t; Xt\nn) \u223cN\n\u0012\nEx[g(\u03b8t; x)], 1\nnCov [g(\u03b8t; x)]\n\u0013\n(5)\nHere Cov [g(\u03b8t; x)] is the covariance of the scores at\n\u03b8t. Using NCov [g(\u03b8t; x)] \u2248IN and NEx[g(\u03b8t; x)] \u2248\nGN(\u03b8t; XN), we have:\n\u2207log p(\u03b8t) + Ngn(\u03b8t; Xt\nn)\n\u2248\u2207log p(\u03b8t) + GN(\u03b8t; XN) + \u03c6\nwhere\n\u03c6 \u223cN\n\u0012\n0, NIN\nn\n\u0013\n(6)\nNow, \u2207log p(\u03b8t) + GN(\u03b8t; XN) = \u2207log p(\u03b8t|XN), the\ngradient of the log posterior. If we assume that the posterior\nis close to its Bernstein-von Mises approximation, we have\n\u2207log p(\u03b8t|XN) = \u2212IN(\u03b8t \u2212\u03b80). Using this in Eqn. (6)\nand then substituting in Eqn. (4), we have:\n\u03b8t+1 \u2190\u03b8t + \u03f5C\n2 {\u2212IN(\u03b8t \u2212\u03b80)} + \u03c8 + \u03c4\n(7)\nwhere,\n\u03c8 \u223cN\n\u0012\n0, \u03f52\n4\nN\nn CINC\n\u0013\nand \u03c4 \u223cN(0, Q)\nComparing Eqn. (7) and Eqn. (2), we see that at high step\nsizes, we need:\nQ + \u03f52\n4\nN\nn CINC = \u03f5C \u2212\u03f52\n4 CINC \u21d2\nQ = \u03f5C \u2212\u03f52\n4\nN + n\nn\nCINC\n(8)\nThus, we should choose Q such that:\nQ =\n\u001a \u03f5C\nfor small \u03f5\n\u03f5C \u2212\u03f52\n4 \u03b3CINC\nfor large \u03f5\nwhere we have de\ufb01ned \u03b3 =\nN+n\nn . Since \u03f5 dominates \u03f52\nwhen \u03f5 is small, we can choose Q = \u03f5C \u2212\u03f52\n4 \u03b3CINC for\nboth the cases above. With this, our update equation be-\ncomes:\n\u03b8t+1 \u2190\u03b8t + \u03f5C\n2\n\b\n\u2207log p(\u03b8t) + Ngn(\u03b8t; Xt\nn)\n\t\n+ \u03c4\nwhere \u03c4 \u223cN\n\u0012\n0, \u03f5C \u2212\u03f52\n4 \u03b3CINC\n\u0013\n(9)\nNow, we have to choose C so that the covariance matrix of\nthe injected noise in Eqn. (9) is positive-de\ufb01nite. One way\nto enforce this, is by setting:\n\u03f5C \u2212\u03f52\n4 \u03b3CINC = \u03f5CBC \u21d2C = 4 [\u03f5\u03b3IN + 4B]\u22121\n(10)\nwhere B is any symmetric positive-de\ufb01nite matrix. Plug-\nging in this choice of C in Eqn. 9, we get:\n\u03b8t+1 \u2190\u03b8t + 2\n\u0014\n\u03b3IN + 4B\n\u03f5\n\u0015\u22121\n\u00d7\n{\u2207log p(\u03b8t) + Ngn(\u03b8t; Xt) + \u03b7}\nwhere \u03b7 \u223cN\n\u0012\n0, 4B\n\u03f5\n\u0013\n(11)\nBayesian Posterior Sampling via Stochastic Gradient Fisher Scoring\nHowever, the above method considers IN to be a known\nconstant. In practice, we use N \u02c6I1,t as an estimate of IN,\nwhere \u02c6I1,t is an online average of the empirical covari-\nance of gradients (empirical Fisher information) computed\nat each \u03b8t.\n\u02c6I1,t = (1 \u2212\u03bat)\u02c6I1,t\u22121 + \u03batV (\u03b8t; Xt\nn)\n(12)\nwhere \u03bat = 1/t. In the supplementary material we prove\nthat this online average converges to I1 plus O(1/N) cor-\nrections if we assume that the samples are actually drawn\nfrom the posterior:\nTheorem 1.\nConsider a sampling algorithm which\ngenerates a sample \u03b8t from the posterior distribution\nof\nthe\nmodel\nparameters\np(\u03b8|XN)\nin\neach\nitera-\ntion t.\nIn each iteration, we draw a random mini-\nbatch of size n, Xt\nn\n=\n{xt1...xtn}, and compute\nthe empirical covariance of the scores V (\u03b8t; Xt\nn)\n=\n1\nn\u22121\nPn\ni=1 {g(\u03b8t; xti) \u2212gn(\u03b8t)} {g(\u03b8t; xti) \u2212gn(\u03b8t)}T .\nLet VT be the average of V (\u03b8t) across T iterations.\nFor large N, as T \u2192\u221e, VT converges to the Fisher\ninformation I(\u03b80) plus O( 1\nN ) corrections, i.e.\nlim\nT \u2192\u221e\n\"\nVT \u225c1\nT\nT\nX\nt=1\nV (\u03b8t; Xt\nn)\n#\n= I(\u03b80) + O( 1\nN ) (13)\nNote that this is not a proof of convergence of the Markov\nchain to the correct distribution. Rather, assuming that the\nsamples are from the posterior, it shows that the online av-\nerage of the covariance of the gradients converges to the\nFisher information (as desired). Thus, it strengthens our\ncon\ufb01dence that if the samples are almost from the posterior,\nthe learned pre-conditioner converges to something sensi-\nble. What we do know is that if we anneal the stepsizes\naccording to a certain polynomial schedule, and we keep\nthe pre-conditioner \ufb01xed, then SGFS is a version of SGLD\nwhich was shown to converge to the correct equilibrium\ndistribution (Welling & Teh, 2011). We believe the adap-\ntation of the Fisher information through an online average\nis slow enough for the resulting Markov chain to still be\nvalid, but a proof is currently lacking. The theory of adap-\ntive MCMC (Andrieu & Thoms, 2009) or two time scale\nstochastic approximations (Borkar, 1997) might hold the\nkey to such a proof which we leave for future work. Putting\nit all together, we arrive at algorithm 1 below.\nThe general method still has a free symmetric positive-\nde\ufb01nite matrix, B, which may be chosen according to our\nconvenience. Examine the limit \u03f5 \u21920. In this case our\nmethod becomes SGLD with preconditioning matrix B\u22121\nand step size \u03f5.\nIf the posterior is Gaussian, as is usually the case when N is\nlarge, the proposed SGFS algorithm will sample correctly\nfor arbitrary choice of B even when the step size \u03f5 is large.\nAlgorithm 1: Stochastic Gradient Fisher Scoring (SGFS)\nInput: n, B, {\u03bat}t=1:T\nOutput: {\u03b8t}t=1:T\n1: Initialize \u03b81, \u02c6I1,0\n2: \u03b3 \u2190n+N\nn\n3: for t = 1 : T do\n4:\nChoose random minibatch Xt\nn = {xt1...xtn}\n5:\ngn(\u03b8t) \u21901\nn\nPn\ni=1 gti(\u03b8t)\n6:\nV (\u03b8t) \u2190\n1\nn\u22121\nPn\ni=1 {gti(\u03b8t) \u2212gn(\u03b8t)} {gti(\u03b8t) \u2212gn(\u03b8t)}T\n7:\n\u02c6I1,t \u2190(1 \u2212\u03bat)\u02c6I1,t\u22121 + \u03batV (\u03b8t)\n8:\nDraw \u03b7 \u223cN[0, 4B\n\u03f5 ]\n9:\n\u03b8t+1 \u2190\u03b8t+\n2\n\u0010\n\u03b3N \u02c6I1,t + 4B\n\u03f5\n\u0011\u22121\n{\u2207log p(\u03b8t) + Ngn(\u03b8t) + \u03b7}\n10: end for\nHowever, for some models the conditions of the Bernstein-\nvon Mises theorem are violated and the posterior may not\nbe well approximated by a Gaussian. This is the case for\ne.g. neural networks and discriminative RBMs, where the\nidenti\ufb01ability condition of the parameters do not hold. In\nthis case, we have to choose a small \u03f5 to achieve accurate\nsampling (see section 5). These two extremes can be com-\nbined in a single \u201canytime\u201d algorithm by slowly annealing\nthe stepsize. For a non-adaptive version of our algorithm\n(i.e. where we would stop changing \u02c6I1) after a \ufb01xed num-\nber of iterations) this would according to the results from\nWelling and Teh (2011) lead to a valid Markov chain for\nposterior sampling.\nWe recommend choosing B \u221dIN. With this choice, our\nmethod is highly reminiscent of \u201cFisher scoring\u201d which\nis why we named it \u201cStochastic Gradient Fisher Scoring\u201d\n(SGFS). In fact we can think of the proposed updates as a\nstochastic version of Fisher scoring based on small mini-\nbatches of gradients. But remarkably, the proposed algo-\nrithm is not only much faster than Fisher scoring (because\nit only requires small minibatches to compute an update), it\nalso samples approximately from the posterior distribution.\nSo the knife cuts on both sides: SGFS is a faster optimiza-\ntion algorithm but also doesn\u2019t over\ufb01t due to the fact that\nit switches to sampling when the right statistical scale of\nprecision is reached.\n4. Computational Ef\ufb01ciency\nClearly, the main computational bene\ufb01t relative to stan-\ndard MCMC algorithms comes from the fact that we use\nstochastic minibatches instead of the entire dataset at every\niteration. However, for a model with a large number of pa-\nrameters another source of signi\ufb01cant computational effort\nis the computation of the D \u00d7 D matrix \u03b3N \u02c6I1,t + 4B\n\u03f5 and\nBayesian Posterior Sampling via Stochastic Gradient Fisher Scoring\nmultiplying its inverse with the mean gradient resulting in a\ntotal computational complexity of O(D3) per iteration. In\nthe case n < D the computational complexity per iteration\ncan be brought down to O(nD2) by using the Sherman-\nMorrison-Woodbury equation. A more numerically stable\nalternative is to update Cholesky factors (Seeger, 2004).\nIn case even this is infeasible one can factor the Fisher in-\nformation into k independent blocks of variables of, say\nsize d, in which case we have brought down the complexity\nto O(kd3). The extreme case of this is when we treat every\nparameter as independent which boils down to replacing\nthe Fisher information by a diagonal matrix with the vari-\nances of the individual parameters populating the diagonal.\nWhile for a large stepsize this algorithm will not sample\nfrom the correct Gaussian approximation, it will still sam-\nple correctly from the posterior for very small stepsizes. In\nfact, it is expected to do this more ef\ufb01ciently than SGLD\nwhich does not rescale its stepsizes at all. We have used\nthe full covariance algorithm (SGFS-f) and the diagonal co-\nvariance algorithm (SGFS-d) in the experiments section.\n5. Experiments\nBelow we report experimental results where we test SGFS-\nf, SGFS-d, SGLD, SGD and HMC on three different mod-\nels: logistic regression, neural networks and discriminative\nRBMs. The experiments share the following practice in\ncommon. Stepsizes for SGD and SGLD are always se-\nlected through cross-validation for at least \ufb01ve settings.\nThe minibatch size n is set to either 300 or 500, but the\nresults are not sensitive to the precise value as long as it\nis large enough for the central limit theorem to hold (typi-\ncally, n > 100 is recommended). Also, we used \u03bat = 1\nt .\n5.1. Logistic Regression\nA logistic regression model (LR) was trained on the\nMNIST dataset for binary classi\ufb01cation of two digits 7 and\n9 using a total of 10,000 data-items. We used a 50 dimen-\nsional random projection of the original features and ran\nSGFS with \u03bb = 1. We used B = \u03b3IN and tested the algo-\nrithm for a number of \u03b1 values (where \u03b1 =\n2\n\u221a\u03f5). We ran\nthe algorithm for 3,000 burn-in iterations and then collected\n100,000 samples.\nWe compare the algorithm to Hamil-\ntonian Monte Carlo sampling (Neal, 1993) and to SGLD\n(Welling & Teh, 2011). For HMC, the \u201cleapfrogstep\u201d size\nwas adapted during burn-in so that the acceptance ratio was\naround 0.8. For SGLD we also used a range of \ufb01xed step-\nsizes.\nIn \ufb01gure 1 we show 2-d marginal distributions of SGFS\ncompared to the ground truth from a long HMC run where\nwe used \u03b1 = 0 for SGFS. From this we conclude that even\nfor the largest possible stepsize the \ufb01t for SGFS-f is al-\n0.5\n0.6\n0.7\n0.8\n0.1\n0.15\n0.2\n0.25\n0.3\n0.35\n0.4\n0.45\nSGFS\u2212f (BEST)\n\u22120.4\n\u22120.3\n\u22120.2\n\u22120.1\n\u22120.75\n\u22120.7\n\u22120.65\n\u22120.6\n\u22120.55\n\u22120.5\n\u22120.45\n\u22120.4\nSGFS\u2212f (WORST)\n0.5\n0.6\n0.7\n0.8\n0.1\n0.15\n0.2\n0.25\n0.3\n0.35\n0.4\n0.45\nSGFS\u2212d (BEST)\n\u22120.4\n\u22120.3\n\u22120.2\n\u22120.1\n\u22120.75\n\u22120.7\n\u22120.65\n\u22120.6\n\u22120.55\n\u22120.5\n\u22120.45\n\u22120.4\nSGFS\u2212d (WORST)\nFigure 1. 2-d marginal posterior distributions for logistic regres-\nsion. Grey colors correspond to samples from SGFS. Red solid\nand blue dotted ellipses represent iso-probability contours at two\nstandard deviations away from the mean computed from HMC\nand SGFS, respectively. Top plots are the results for SGFS-f and\nbottom plots represent SGFS-d. Plots on the left represent the 2-d\nmarginals with the smallest difference between HMC and SGFS\nwhile the plots on the right represent the 2-d marginals with the\nlargest difference. Value for \u03b1 is 0 meaning that no additional\nnoise was added.\nmost perfect while SGFS-d underestimates the variance in\nthis case (note however that for smaller stepsizes (larger \u03b1)\nSGFS-d becomes very similar to SGLD and is thus guaran-\nteed to sample correctly albeit with a low mixing rate).\nNext, we studied the inverse autocorrelation time per unit\ncomputation (ATUC)1 averaged over the 51 parameters and\ncompared this with the relative error after a \ufb01xed amount of\ncomputation time. The relative error is computed as fol-\nlows: \ufb01rst we compute the mean and covariance of the\nparameter samples up to time t : \u03b8t =\n1\nt\nPt\nt\u2032=1 \u03b8t\u2032 and\nCt = 1\nt\nPt\nt\u2032=1(\u03b8t\u2032 \u2212\u03b8t)(\u03b8t\u2032 \u2212\u03b8t)T . We do the same for\nthe long HMC run which we indicate with \u03b8\u221eand C\u221e.\nFinally we compute\nE1t =\nP\ni |\u03b8t\ni \u2212\u03b8\u221e\ni |\nP\ni |\u03b8\u221e\ni |\n,\nE2t =\nP\nij |Ct\nij \u2212C\u221e\nij |\nP\nij |C\u221e\nij |\n(14)\nIn Figure 2 we plot the \u201cError at time T\u201d for two val-\nues of T (T=100, T=3000) as a function of the inverse\nATUC, which is a measure of the mixing rate. Top plots\nshow the results for the mean and bottom plots for the\ncovariance.\nEach point denoted by a cross is obtained\n1ATUC = Autocorrelation Time \u00d7 Time per Sample. Auto-\ncorrelation time is de\ufb01ned as 1 + 2 P\u221e\ns=1 \u03c1(s) with \u03c1(s) the au-\ntocorrelation at lag s Neal (1993).\nBayesian Posterior Sampling via Stochastic Gradient Fisher Scoring\n10\n\u22122\n10\n\u22121\n10\n0\n10\n1\n10\n\u22123\n10\n\u22122\n10\n\u22121\nRelative Error in Mean at 100 sec.\nMixing Rate (1/ATUC)\nSGLD\nHMC\nSGFS\u2212f\nSGFS\u2212d\n10\n\u22122\n10\n\u22121\n10\n0\n10\n1\n10\n\u22123\n10\n\u22122\n10\n\u22121\nRelative Error in Mean at 3000 sec.\nMixing Rate (1/ATUC)\nHMC\nSGLD\nSGFS\u2212f\nSGFS\u2212d\n10\n\u22122\n10\n0\n10\n\u22121\n10\n0\nRelative Error in Cov at 100 sec.\nMixing Rate (1/ATUC)\nHMC\nSGLD\nSGFS\u2212d\nSGFS\u2212f\n10\n\u22122\n10\n0\n10\n\u22121\n10\n0\nRelative Error in Cov at 3000 sec.\nMixing Rate (1/ATUC)\nHMC\nSGLD\nSGFS\u2212d\nSGFS\u2212f\nFigure 2. Final error of logistic regression at time T versus mixing\nrate for the mean (top) and covariance (bottom) estimates after\n100 (left) and 3000 (right) seconds of computation. See main text\nfor detailed explanation.\nfrom a different setting of parameters that control the mix-\ning rate: \u03b1 = [0, 1, 2, 3, 4, 5, 6] for SGFS, stepsizes \u03f5 =\n[1e\u22123, 5e\u22124, 1e\u22124, 5e\u22125, 1e\u22125, 5e\u22126, 1e\u22126] for SGLD,\nand number of leapfrog steps s = [50, 40, 30, 20, 10, 1] for\nHMC. The circle is the result for the fastest mixing chain.\nFor SGFS and SGLD, if the slope of the curve is nega-\ntive (downward trend) then the corresponding algorithm\nwas still in the phase of reducing error by reducing sam-\npling variance at time T. However, when the curve bends\nupwards and develops a positive slope the algorithm has\nreached its error \ufb02oor corresponding to the approximation\nbias. The situation is different for HMC, (which has no\nbias) but where the bending occurs because the number of\nleapfrog steps has become so large that it is turning back\non itself. HMC is not faring well because it is computa-\ntionally expensive to run (which hurts both its mixing rate\nand error at time T). We also observe that in the allowed\nrunning time SGFS-f has not reached its error \ufb02oor (both\nfor the mean and the covariance). SGFS-d is reaching its\nerror \ufb02oor only for the covariance (which is consistent with\nFigure 1 bottom) but still fares well in terms of the mean.\nFinally, for SGLD we clearly see that in order to obtain a\nhigh mixing rate (low ATUC) it has to pay the price of a\nlarge bias. These plots clearly illustrate the advantage of\nSGFS over both HMC as well as SGLD.\n5.2. SGFS on Neural Networks\nWe also applied our methods to a 3 layer neural network\n(NN) with logistic activation functions. Below we describe\nclassi\ufb01cation results for two datasets.\n10\n4\n10\n5\n10\n-0.35\n10\n-0.34\n10\n-0.33\n10\n-0.32\nIteration Number (Log)\nClassification Error (Log)\n \n \nSGD-Avg (\uf06c = 0)\nSGD-Avg (\uf06c = 0.1)\nSGLD-Avg (\uf06c = 0)\nSGLD-Avg (\uf06c = 0.1)\nSGFS-d-Avg (\uf06c = 0, \uf061 = 6)\nSGFS-d-Avg (\uf06c = 0.1, \uf061 = 6)\nSGD\nSGFS-d\nSGLD\n10\n0\n10\n2\n10\n4\n10\n-0.7\n10\n-0.5\n10\n-0.3\n10\n-0.1\nIteration Number (Log)\nClassification Error (Log)\n \n \nSGFS-f-Avg (\uf06c = 0.001, \uf061 = 2)\nSGFS-d-Avg (\uf06c = 0.001, \uf061 = 2)\nSGLD-Avg (\uf06c = 0.001, \uf065 = [10-3,10-7])\nSGD-Avg (\uf06c = 0.001, \uf065 = [10-3,10-7])\nSGD\nSGLD\nSGFS-f\nSGFS-d\nFigure 3. Test-set classi\ufb01cation error of NNs trained with SGFS-\nf, SGFS-d, SGLD and SGD on the HHP dataset (left) and the\nMNIST dataset (right)\n5.2.1. HERITAGE HEALTH PRIZE (HHP)\nThe goal of this competition is to predict how many days\nbetween [0 \u221215] a person will stay in a hospital given\nhis/her past three years of hospitalization records2.\nWe\nused the same features as the team market makers that won\nthe \ufb01rst milestone prize. Integrating the \ufb01rst and second\nyear data, we obtained 147,473 data-items with 139 fea-\nture dimensions and then used a randomly selected 70%\nfor training and the remainder for testing. NNs with 30\nhidden units were used because more hidden units did not\nnoticeably improve the results. Although we used \u03b1 = 6\nfor SGFS-d, there was no signi\ufb01cant difference for values\nin the range 3 \u2264\u03b1 \u22646. However, \u03b1 < 3 did not work for\nthis dataset due to the fact that many features had values 0.\nFor SGD, we used stepsizes from a polynomial anneal-\ning schedule a(b + t)\u2212\u03b4. Because the training error de-\ncreased slowly in a valid range \u03b4 = [0.5, 1], we used \u03b4 = 3,\na = 1014, b = 2.2 \u00d7 105 instead which was found optimal\nthrough cross-validation. (This setting reduced the stepsize\nfrom 10\u22122 to 10\u22126 during 1e+7 iterations). For SGLD,\na = 1, b = 104, and \u03b4 = 1 reducing the step size from\n10\u22124 to 10\u22126 was used. Figure 3 (left) shows the classi-\n\ufb01cation errors averaged over the posterior samples for two\nregularizer values, \u03bb = 0 and the best regularizer value \u03bb\nfound through cross-validation. First, we clearly see that\nSGD severely over\ufb01ts without a regularizer while SGLD\nand SGFS prevent it because they average predictions over\nsamples from a posterior mode. Furthermore, we see that\nwhen the best regularizer is used, SGFS (marginally) out-\nperforms both SGD and SGLD. The result from SGFS-d\nsubmitted to the actual competition leaderboard gave us an\nerror of 0.4635 which is comparable to 0.4632 obtained by\nthe milestone winner with a \ufb01ne-tuned Gradient Boosting\nMachine.\n2http://www.heritagehealthprize.com\nBayesian Posterior Sampling via Stochastic Gradient Fisher Scoring\n\u22122.2\n\u22122\n\u22121.8\n\u22121.6\n\u22121.4\n\u22121.2\n\u22121\n\u22120.8\n\u22120.6\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nSGFS\u2212f (BEST)\n\u22120.25\n\u22120.2\n\u22120.15\n\u22120.1\n\u22120.05\n0\n0.05\n0.1\n0.15\n\u22120.2\n\u22120.15\n\u22120.1\n\u22120.05\n0\n0.05\n0.1\n0.15\n0.2\nSGFS\u2212f (WORST)\n\u22122.2\n\u22122\n\u22121.8\n\u22121.6\n\u22121.4\n\u22121.2\n\u22121\n\u22120.8\n\u22120.6\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nSGLD (BEST)\n\u22122\n\u22121.5\n\u22121\n\u22120.5\n0\n0.5\n\u22124\n\u22123.5\n\u22123\n\u22122.5\n\u22122\n\u22121.5\n\u22121\n\u22120.5\n0\n0.5\nSGLD (WORST)\nFigure 4. 2-d marginal posterior distributions of DRBM. Grey\ncolors correspond to samples from SGFS/SGLD. Thick red solid\nlines correspond to iso-probability contours at two standard de-\nviations away from the mean computed from HMC samples.\nThin red solid lines correspond to HMC results based on sub-\nsets of the samples. The thick blue dashed lines correspond to\nSGFS-f (top) and SGLD (bottom) runs. Plots on the left repre-\nsent the 2-d marginals with the smallest difference between HMC\nand SGFS/SGLD while the plots on the right represent the 2-d\nmarginals with the largest difference.\n5.2.2. CHARACTER RECOGNITION\nWe also tested our methods on the MNIST dataset for 10\ndigit classi\ufb01cation which has 60,000 training instances and\n10,000 test instances. In order to test with SGFS-f, we used\ninputs from 20 dimensional random projections and 30 hid-\nden units so that the number of parameters equals 940.\nMoreover, we increased the mini-batch size to 2,000 to re-\nduce the time required to reach a good approximation of the\n940 \u00d7 940 covariance matrix. The classi\ufb01cation error aver-\naged over the samples is shown in Figure 3 (right). Here,\nwe used a small regularization parameter of \u03bb = 0.001\nfor all methods as over\ufb01tting was not an issue. For SGFS,\n\u03b1 = 2 is used while for both SGD and SGLD the stepsizes\nwere annealed from 10\u22123 to 10\u22127 using a = 1, b = 1000,\nand \u03b3 = 1.\n5.3. Discriminative Restricted Boltzmann Machine\n(DRBM)\nWe trained a DRBM (Larochelle & Bengio, 2008) on the\nKDD99 dataset which consists of 4,898,430 datapoints\nwith 40 features, belonging to a total of 23 classes. We\n\ufb01rst tested the classi\ufb01cation performance by training the\nDRBM using SGLD, SGFS-f, SGFS-d and SGD. For this\nexperiment the dataset was divided into a 90% training set,\n5% validation and 5% test set. We used 41 hidden units\ngiving us a total of 2647 parameters in the model. We used\n0.5\n1\n1.5\n2\n2.5\n3\n3.5\n4\n4.5\nx 10\n\u00ef5\n0.05\n0.1\n0.15\n0.2\n0.25\n0.3\n0.35\n0.4\nRel. Err. in Mean at 6790 sec.\nMixing Rate (1/ATUC)\nSGFS\u00eff\nSGLD\n0.5\n1\n1.5\n2\n2.5\n3\n3.5\n4\n4.5\nx 10\n\u00ef5\n1\n2\n3\n4\n5\n6\n7\nRel. Err. in Cov at 6790 sec.\nMixing Rate (1/ATUC)\nSGFS\u00eff\nSGLD\nFigure 5. Final error for DRBM at time T versus mixing rate for\nthe mean (left) and covariance (right) estimates after 6790 sec-\nonds of computation on a subset of KDD99.\nSGD\nSGLD\nSGFS-d\nSGFS-f\n8.010\u22124\n6.610\u22124\n4.210\u22124\n4.410\u22124\nTable 1. Final test error rate on the KDD99 dataset.\n\u03bb = 10 and B = \u03b3IN. We tried 6 different (\u03b1, \u03f5) com-\nbinations for SGFS-f and SGFS-d and tried 18 annealing\nschedules for SGD and SGLD, and used the validation set\nto pick the best one. The best results were obtained with an\n\u03b1 value of 8.95 for SGFS-f and SGFS-d, and [a = 0.1, b =\n100000, \u03b4 = 0.9] for SGD and SGLD. We ran all algorithms\nfor 100,000 iterations. Although we experimented with dif-\nferent burn-in iterations, the algorithms were insensitive to\nthis choice. The \ufb01nal error rates are given in table 1 from\nwhich we conclude that the samplers based on stochastic\ngradients can act as effective optimizers whereas HMC on\nthe full dataset becomes completely impractical because it\nhas to compute 11.7 billion gradients per iteration which\ntakes around 7.5 minutes per sample (4408587 datapoints\n\u00d7 2647 parameters).\nTo compare the quality of the samples drawn after burn-in,\nwe created a 10% subset of the original dataset. This time\nwe picked only the 6 most populous classes. We tested all\nalgorithms with 41, 10 and 5 hidden units, but since the\nposterior is highly multi-modal, the different algorithms\nended up sampling from different modes. In an attempt\nto get a meaningful comparison, we therefore reduced the\nnumber of hidden units to 2. This improved the situation to\nsome degree, but did not entirely get rid of the multi-modal\nand non-Gaussian structure of the posterior. We compare\nresults of SGFS-f/SGLD with 30 independent HMC runs,\neach providing 4000 samples for a total of 120,000 sam-\nples. Since HMC was very slow (even on the reduced set)\nwe initialized at a mode and used the Fisher information\nat the mode as a pre-conditioner. We used 1 leapfrog step\nand tuned the step-size to get an acceptance rate of 0.8.\nWe ran SGFS-f with \u03b1 = [2, 3, 4, 5, 10] and SGLD with\n\ufb01xed step sizes of [5e-4, 1e-4, 5e-5, 1e-5, 5e-6].\nBoth\nalgorithms were initialized at the same mode and ran for\n1 million iterations. We looked at the marginal distribu-\nBayesian Posterior Sampling via Stochastic Gradient Fisher Scoring\ntions of the top 25 pairs of variables which had the highest\ncorrelation coef\ufb01cient. In Figure 4 (top-left and bottom-\nleft) we show a set of parameters where both SGFS-f and\nSGLD obtained an accurate estimate of the marginal poste-\nrior. In 4 (top-right and bottom-right) we show an example\nwhere SGLD failed. The thin solid red lines correspond\nto HMC runs computed from various subsets of the sam-\nples, whereas the thick solid red line is computed using the\nall samples from all HMC runs. We have shown marginal\nposterior estimates of the SGFS-f/SGLD algorithms with a\nthick dashed blue ellipse. After inspection, it seemed that\nthe posterior structure was highly non-Gaussian with re-\ngions where the probability very sharply decreased. SGLD\nregularly stepped into these regions and then got catapulted\naway due to the large gradients there. SGFS-f presumably\navoided those regions by adapting to the local covariance\nstructure. We found that in this region even the HMC runs\nare not consistent with one another. Note that the SGFS-f\ncontours seem to agree with the HMC contours as much as\nthe HMC contours agree with the results of its own subsets,\nin both the easy and the hard case.\nFinally, we plot the error after 6790 seconds of computa-\ntion versus the mixing rate. Figure 5-left shows the results\nfor the mean and the right for the covariance (for an ex-\nplanation of the various quantities see discussion in section\n5.1). We note again that SGLD incurs a signi\ufb01cantly larger\napproximation bias at the same mixing rate as SGFS-f.\n6. Conclusions\nWe have introduced a novel method, \u201cStochastic Gradient\nFisher Scoring\u201d (SGFS) for approximate Bayesian learn-\ning. The main idea is to use stochastic gradients in the\nLangevin equation and leverage the central limit theorem\nto estimate the noise induced by the subsampling process.\nThis subsampling noise is combined with arti\ufb01cially in-\njected noise and multiplied by the estimated inverse Fisher\ninformation matrix to approximately sample from the pos-\nterior. This leads to the following desirable properties.\n\u2022 Unlike regular MCMC methods, SGFS is fast because it\nuses only stochastic gradients based on small mini-batches\nto draw samples.\n\u2022 Unlike stochastic gradient descent, SGFS samples (ap-\nproximately) from the posterior distribution.\n\u2022 Unlike SGLD, SGFS samples from a Gaussian approx-\nimation of the posterior distribution (that is correct for\nN \u2192\u221e) for large stepsizes.\n\u2022 By annealing the stepsize, SGFS becomes an any-\ntime method capturing more non-Gaussian structure with\nsmaller stepsizes but at the cost of slower mixing.\n\u2022 During its burn-in phase, SGFS is an ef\ufb01cient optimizer\nbecause like Fisher scoring and Gauss-Newton methods, it\nis based on the natural gradient.\nFor an appropriate annealing schedule, SGFS thus goes\nthrough three distinct phases: 1) during burn-in we use\na large stepsize and the method is similar to a stochastic\ngradient version of Fisher scoring, 2) when the stepsize\nis still large, but when we have reached the mode of the\ndistribution, SGFS samples from the asymptotic Gaussian\napproximation of the posterior, and 3) when the stepsize\nis further annealed, SGFS will behave like SGLD with a\npre-conditioning matrix and generate increasingly accurate\nsamples from the true posterior.\nAcknowledgements\nThis material is based upon work supported by the National Sci-\nence Foundation under Grant No. 0447903, 0914783, 0928427.\nReferences\nAndrieu, C. and Thoms, J. A tutorial on adaptive mcmc. Statistics\nand Computing, 18(4):343\u2013373, 2009.\nBorkar, V.S. Stochastic approximation with two time scales. Sys-\ntems and Control Letters, 29(5):291\u2013294, 1997.\nBottou, L. and Bousquet, O. The tradeoffs of large scale learn-\ning. In Advances in Neural Information Processing Systems,\nvolume 20, pp. 161\u2013168, 2008.\nGirolami, M. and Calderhead, B.\nRiemann manifold langevin\nand hamiltonian monte carlo. Journal of the Royal Statistical\nSociety B, 73 (2):1\u201337, 2010.\nLarochelle, H. and Bengio, Y. Classi\ufb01cation using discriminative\nRestricted Boltzmann Machines. In Proceedings of the 25th\nInternational Conference on Machine learning, pp. 536\u2013543.\nACM, 2008.\nLe Cam, L.M. Asymptotic methods in statistical decision theory.\nSpringer, 1986.\nNeal, R.M.\nProbabilistic inference using markov chain monte\ncarlo methods. Technical Report CRG-TR-93-1, University of\nToronto, Computer Science, 1993.\nSchraudolph, N. N., Yu, J., and G\u00a8unter, S. A stochastic quasi-\nNewton method for online convex optimization. In Meila, Ma-\nrina and Shen, Xiaotong (eds.), Proc. 11th Intl. Conf. Arti\ufb01cial\nIntelligence and Statistics (AIstats), pp. 436\u2013443, San Juan,\nPuerto Rico, 2007.\nScott, W.A. Maximum likelihood estimation using the empirical\n\ufb01sher information matrix. Journal of Statistical Computation\nand Simulation, 72(8):599\u2013611, 2002.\nSeeger, M.\nLow rank updates for the cholesky decompo-\nsition.\nTechnical report, University of California Berke-\nley, 2004.\nURL http://lapmal.epfl.ch/papers/\ncholupdate.shtml.\nWelling, M. and Teh, Y.W. Bayesian learning via stochastic gra-\ndient langevin dynamics. In Proceedings of the 28th Interna-\ntional Conference on Machine Learning (ICML), pp. 681\u2013688,\n2011.\n",
        "sentence": " MCMC methods are computationally expensive, and recent years have seen the introduction of biased MCMC procedures (see, e.g., Welling & Teh, 2011; Ahn et al., 2012; Korattikara et al., 2014) that exchange asymptotic correctness for increased sampling speed. , Welling & Teh, 2011; Ahn et al., 2012; Korattikara et al., 2014) that exchange asymptotic correctness for increased sampling speed. Since standard MCMC diagnostics, like mean and trace plots, pooled and within-chain variance measures, effective sample size, and asymptotic variance (Brooks et al., 2011), do not account for asymptotic bias, Gorham & Mackey (2015) defined a new family of sample quality measures \u2013 the Stein discrepancies \u2013 that measure how well EQn approximates EP while avoiding explicit integration under P . , Welling & Teh, 2011; Ahn et al., 2012; Korattikara et al., 2014) that exchange asymptotic correctness for increased sampling speed. Since standard MCMC diagnostics, like mean and trace plots, pooled and within-chain variance measures, effective sample size, and asymptotic variance (Brooks et al., 2011), do not account for asymptotic bias, Gorham & Mackey (2015) defined a new family of sample quality measures \u2013 the Stein discrepancies \u2013 that measure how well EQn approximates EP while avoiding explicit integration under P . Gorham & Mackey (2015); Mackey & Gorham (2016); Gorham et al. , Welling & Teh, 2011; Ahn et al., 2012; Korattikara et al., 2014) that exchange asymptotic correctness for increased sampling speed. Since standard MCMC diagnostics, like mean and trace plots, pooled and within-chain variance measures, effective sample size, and asymptotic variance (Brooks et al., 2011), do not account for asymptotic bias, Gorham & Mackey (2015) defined a new family of sample quality measures \u2013 the Stein discrepancies \u2013 that measure how well EQn approximates EP while avoiding explicit integration under P . Gorham & Mackey (2015); Mackey & Gorham (2016); Gorham et al. , Welling & Teh, 2011; Ahn et al., 2012; Korattikara et al., 2014) that exchange asymptotic correctness for increased sampling speed. Since standard MCMC diagnostics, like mean and trace plots, pooled and within-chain variance measures, effective sample size, and asymptotic variance (Brooks et al., 2011), do not account for asymptotic bias, Gorham & Mackey (2015) defined a new family of sample quality measures \u2013 the Stein discrepancies \u2013 that measure how well EQn approximates EP while avoiding explicit integration under P . Gorham & Mackey (2015); Mackey & Gorham (2016); Gorham et al. (2016) further showed that specific members of this family \u2013 the graph Stein discrepancies \u2013 were (a) efficiently computable by solving a linear program and (b) convergence-determining for large classes of targets P . , Welling & Teh, 2011; Ahn et al., 2012; Korattikara et al., 2014) that exchange asymptotic correctness for increased sampling speed. Since standard MCMC diagnostics, like mean and trace plots, pooled and within-chain variance measures, effective sample size, and asymptotic variance (Brooks et al., 2011), do not account for asymptotic bias, Gorham & Mackey (2015) defined a new family of sample quality measures \u2013 the Stein discrepancies \u2013 that measure how well EQn approximates EP while avoiding explicit integration under P . Gorham & Mackey (2015); Mackey & Gorham (2016); Gorham et al. (2016) further showed that specific members of this family \u2013 the graph Stein discrepancies \u2013 were (a) efficiently computable by solving a linear program and (b) convergence-determining for large classes of targets P . Building on the zero mean reproducing kernel theory of Oates et al. (2016b), Chwialkowski et al. , Welling & Teh, 2011; Ahn et al., 2012; Korattikara et al., 2014) that exchange asymptotic correctness for increased sampling speed. Since standard MCMC diagnostics, like mean and trace plots, pooled and within-chain variance measures, effective sample size, and asymptotic variance (Brooks et al., 2011), do not account for asymptotic bias, Gorham & Mackey (2015) defined a new family of sample quality measures \u2013 the Stein discrepancies \u2013 that measure how well EQn approximates EP while avoiding explicit integration under P . Gorham & Mackey (2015); Mackey & Gorham (2016); Gorham et al. (2016) further showed that specific members of this family \u2013 the graph Stein discrepancies \u2013 were (a) efficiently computable by solving a linear program and (b) convergence-determining for large classes of targets P . Building on the zero mean reproducing kernel theory of Oates et al. (2016b), Chwialkowski et al. (2016) and Liu et al. , Welling & Teh, 2011; Ahn et al., 2012; Korattikara et al., 2014) that exchange asymptotic correctness for increased sampling speed. Since standard MCMC diagnostics, like mean and trace plots, pooled and within-chain variance measures, effective sample size, and asymptotic variance (Brooks et al., 2011), do not account for asymptotic bias, Gorham & Mackey (2015) defined a new family of sample quality measures \u2013 the Stein discrepancies \u2013 that measure how well EQn approximates EP while avoiding explicit integration under P . Gorham & Mackey (2015); Mackey & Gorham (2016); Gorham et al. (2016) further showed that specific members of this family \u2013 the graph Stein discrepancies \u2013 were (a) efficiently computable by solving a linear program and (b) convergence-determining for large classes of targets P . Building on the zero mean reproducing kernel theory of Oates et al. (2016b), Chwialkowski et al. (2016) and Liu et al. (2016) later showed that other members of the Stein discrepancy family had a closed-form solution involving the sum of kernel evaluations over pairs of sample points. Selecting samplers Ahn et al. (2012) developed two biased MCMC samplers for accelerated posterior inference, both called Stochastic Gradient Fisher Scoring (SGFS). Selecting samplers Ahn et al. (2012) developed two biased MCMC samplers for accelerated posterior inference, both called Stochastic Gradient Fisher Scoring (SGFS). In the full version of SGFS (termed SGFS-f), a d\u00d7 d matrix must be inverted to draw each new sample point. Since this can be costly for large d, the authors developed a second sampler (termed SGFS-d) in which only a diagonal matrix must be inverted to draw each new sample point. Both samplers can be viewed as discrete-time approximations to a continuoustime Markov process that has the target P as its stationary distribution; however, because no Metropolis-Hastings correction is employed, neither sampler has the target as its stationary distribution. Hence we will use the KSD \u2013 a quality measure that accounts for asymptotic bias \u2013 to evaluate and choose between these samplers. Specifically, we evaluate the SGFS-f and SGFS-d samples produced in (Ahn et al., 2012, Sec. 5.1). The target P is a Bayesian logistic regression with a flat prior, conditioned on a dataset of 10 MNIST handwritten digit images. From each image, the authors extracted 50 random projections of the raw pixel values as covariates and a label indicating whether the image was a 7 or a 9. After discarding the first half of sample points as burn-in, we obtained regression coefficient samples with 5 \u00d7 10 points and d = 51 dimensions (including the intercept term). Figure 4 displays the IMQ KSD applied to the first n points in each sample. As external validation, we follow the protocol of Ahn et al. (2012) to find the bivariate marginal means and 95% confidence ellipses of each sample that align best and worst with those of a surrogate ground truth sample obtained from a",
        "context": "tonian Monte Carlo sampling (Neal, 1993) and to SGLD\n(Welling & Teh, 2011). For HMC, the \u201cleapfrogstep\u201d size\nwas adapted during burn-in so that the acceptance ratio was\naround 0.8. For SGLD we also used a range of \ufb01xed step-\nsizes.\n\u2022 Unlike regular MCMC methods, SGFS is fast because it\nuses only stochastic gradients based on small mini-batches\nto draw samples.\n\u2022 Unlike stochastic gradient descent, SGFS samples (ap-\nproximately) from the posterior distribution.\ndistribution, SGFS samples from the asymptotic Gaussian\napproximation of the posterior, and 3) when the stepsize\nis further annealed, SGFS will behave like SGLD with a\npre-conditioning matrix and generate increasingly accurate"
    },
    {
        "title": "Functional Analysis. Academic Press textbooks in mathematics",
        "author": [
            "G. Bachman",
            "L. Narici"
        ],
        "venue": "Dover Publications,",
        "citeRegEx": "Bachman and Narici,? \\Q1966\\E",
        "shortCiteRegEx": "Bachman and Narici",
        "year": 1966,
        "abstract": "",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "Integration of radial functions",
        "author": [
            "J. Baker"
        ],
        "venue": "Mathematics Magazine,",
        "citeRegEx": "Baker,? \\Q1999\\E",
        "shortCiteRegEx": "Baker",
        "year": 1999,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "Stein\u2019s method and Poisson process convergence",
        "author": [
            "A.D. Barbour"
        ],
        "venue": "J. Appl. Probab., (Special Vol. 25A):175\u2013184,",
        "citeRegEx": "Barbour,? \\Q1988\\E",
        "shortCiteRegEx": "Barbour",
        "year": 1988,
        "abstract": "",
        "full_text": "",
        "sentence": " Inspired by the generator method of Barbour (1988; 1990) and G\u00f6tze (1991), Gorham & Mackey (2015) generalized this operator to multiple dimensions. Inspired by the generator method of Barbour (1988; 1990) and G\u00f6tze (1991), Gorham & Mackey (2015) generalized this operator to multiple dimensions. Inspired by the generator method of Barbour (1988; 1990) and G\u00f6tze (1991), Gorham & Mackey (2015) generalized this operator to multiple dimensions. The resulting Langevin Stein operator (TP g)(x) , 1 p(x) \u3008\u2207, p(x)g(x)\u3009 = \u3008g(x), b(x)\u3009+ \u3008\u2207, g(x)\u3009 for functions g : R \u2192 R was independently developed, without connection to Stein\u2019s method, by Oates et al. (2016b) for the design of Monte Carlo control functionals. Inspired by the generator method of Barbour (1988; 1990) and G\u00f6tze (1991), Gorham & Mackey (2015) generalized this operator to multiple dimensions. The resulting Langevin Stein operator (TP g)(x) , 1 p(x) \u3008\u2207, p(x)g(x)\u3009 = \u3008g(x), b(x)\u3009+ \u3008\u2207, g(x)\u3009 for functions g : R \u2192 R was independently developed, without connection to Stein\u2019s method, by Oates et al. (2016b) for the design of Monte Carlo control functionals. Notably, the Langevin Stein operator depends on P only through its score function b = \u2207 log p and hence is computable even when the normalizing constant of p is not. While our work is compatible with other practical Stein operators, like the family of diffusion Stein operators defined in (Gorham et al., 2016), we will focus on the Langevin operator for the sake of brevity. Hereafter, we will let k : R\u00d7R \u2192 R be the reproducing kernel of a reproducing kernel Hilbert space (RKHS) Kk of functions from R \u2192 R. That is, Kk is a Hilbert space of functions such that, for all x \u2208 R, k(x, \u00b7) \u2208 Kk and f(x) = \u3008f, k(x, \u00b7)\u3009Kk whenever f \u2208 Kk. We let \u2016\u00b7\u2016Kk be the norm induced from the inner product on Kk. With this definition, we define our kernel Stein set Gk,\u2016\u00b7\u2016 as the set of vector-valued functions g = (g1, . . . , gd) such that each component function gj belongs toKk and the vector of their norms \u2016gj\u2016Kk belongs to the \u2016\u00b7\u2016 \u2217 unit ball:2 Gk,\u2016\u00b7\u2016 , {g = (g1, . . . , gd) | \u2016v\u2016 \u2217 \u2264 1 for vj , \u2016gj\u2016Kk}. The following result, proved in Section B, establishes that this is an acceptable domain for TP . Proposition 1 (Zero mean test functions). If k \u2208 C b and EP [\u2016\u2207 log p(Z)\u20162] <\u221e, then EP [(TP g)(Z)] = 0 for all g \u2208 Gk,\u2016\u00b7\u2016. Our analyses and algorithms support each gj belonging to a different RKHS Kkj , but we will not need that flexibility here. The Langevin Stein operator and kernel Stein set together define our quality measure of interest, the kernel Stein discrepancy (KSD) S(\u03bc, TP ,Gk,\u2016\u00b7\u2016). When \u2016\u00b7\u2016 = \u2016\u00b7\u20162, this definition recovers the KSD proposed by Chwialkowski et al. (2016) and Liu et al. Inspired by the generator method of Barbour (1988; 1990) and G\u00f6tze (1991), Gorham & Mackey (2015) generalized this operator to multiple dimensions. The resulting Langevin Stein operator (TP g)(x) , 1 p(x) \u3008\u2207, p(x)g(x)\u3009 = \u3008g(x), b(x)\u3009+ \u3008\u2207, g(x)\u3009 for functions g : R \u2192 R was independently developed, without connection to Stein\u2019s method, by Oates et al. (2016b) for the design of Monte Carlo control functionals. Notably, the Langevin Stein operator depends on P only through its score function b = \u2207 log p and hence is computable even when the normalizing constant of p is not. While our work is compatible with other practical Stein operators, like the family of diffusion Stein operators defined in (Gorham et al., 2016), we will focus on the Langevin operator for the sake of brevity. Hereafter, we will let k : R\u00d7R \u2192 R be the reproducing kernel of a reproducing kernel Hilbert space (RKHS) Kk of functions from R \u2192 R. That is, Kk is a Hilbert space of functions such that, for all x \u2208 R, k(x, \u00b7) \u2208 Kk and f(x) = \u3008f, k(x, \u00b7)\u3009Kk whenever f \u2208 Kk. We let \u2016\u00b7\u2016Kk be the norm induced from the inner product on Kk. With this definition, we define our kernel Stein set Gk,\u2016\u00b7\u2016 as the set of vector-valued functions g = (g1, . . . , gd) such that each component function gj belongs toKk and the vector of their norms \u2016gj\u2016Kk belongs to the \u2016\u00b7\u2016 \u2217 unit ball:2 Gk,\u2016\u00b7\u2016 , {g = (g1, . . . , gd) | \u2016v\u2016 \u2217 \u2264 1 for vj , \u2016gj\u2016Kk}. The following result, proved in Section B, establishes that this is an acceptable domain for TP . Proposition 1 (Zero mean test functions). If k \u2208 C b and EP [\u2016\u2207 log p(Z)\u20162] <\u221e, then EP [(TP g)(Z)] = 0 for all g \u2208 Gk,\u2016\u00b7\u2016. Our analyses and algorithms support each gj belonging to a different RKHS Kkj , but we will not need that flexibility here. The Langevin Stein operator and kernel Stein set together define our quality measure of interest, the kernel Stein discrepancy (KSD) S(\u03bc, TP ,Gk,\u2016\u00b7\u2016). When \u2016\u00b7\u2016 = \u2016\u00b7\u20162, this definition recovers the KSD proposed by Chwialkowski et al. (2016) and Liu et al. (2016). Our next result shows that, for any \u2016\u00b7\u2016, the KSD admits a closed-form solution.",
        "context": null
    },
    {
        "title": "Stein\u2019s method for diffusion approximations",
        "author": [
            "A.D. Barbour"
        ],
        "venue": "Probab. Theory Related Fields,",
        "citeRegEx": "Barbour,? \\Q1990\\E",
        "shortCiteRegEx": "Barbour",
        "year": 1990,
        "abstract": "",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "A consistent test for multivariate normality based on the empirical characteristic function",
        "author": [
            "L. Baringhaus",
            "N. Henze"
        ],
        "venue": "Metrika,",
        "citeRegEx": "Baringhaus and Henze,? \\Q1988\\E",
        "shortCiteRegEx": "Baringhaus and Henze",
        "year": 1988,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "Julia: A fresh approach to numerical computing",
        "author": [
            "J. Bezanson",
            "A. Edelman",
            "S. Karpinski",
            "V.B. Shah"
        ],
        "venue": "arXiv preprint arXiv:1411.1607,",
        "citeRegEx": "Bezanson et al\\.,? \\Q2014\\E",
        "shortCiteRegEx": "Bezanson et al\\.",
        "year": 2014,
        "abstract": "Bridging cultures that have often been distant, Julia combines expertise from\nthe diverse fields of computer science and computational science to create a\nnew approach to numerical computing. Julia is designed to be easy and fast.\nJulia questions notions generally held as \"laws of nature\" by practitioners of\nnumerical computing:\n  1. High-level dynamic programs have to be slow.\n  2. One must prototype in one language and then rewrite in another language\nfor speed or deployment, and\n  3. There are parts of a system for the programmer, and other parts best left\nuntouched as they are built by the experts.\n  We introduce the Julia programming language and its design --- a dance\nbetween specialization and abstraction. Specialization allows for custom\ntreatment. Multiple dispatch, a technique from computer science, picks the\nright algorithm for the right circumstance. Abstraction, what good computation\nis really about, recognizes what remains the same after differences are\nstripped away. Abstractions in mathematics are captured as code through another\ntechnique from computer science, generic programming.\n  Julia shows that one can have machine performance without sacrificing human\nconvenience.",
        "full_text": "Julia: A fresh approach to numerical computing\nJe\ufb00Bezanson\nAlan Edelman\nStefan Karpinski\nViral B. Shah\nMIT and Julia Computing\u2217\nJuly 7, 2015\nAbstract\nBridging cultures that have often been distant, Julia combines expertise from the diverse \ufb01elds\nof computer science and computational science to create a new approach to numerical computing.\nJulia is designed to be easy and fast. Julia questions notions generally held as \u201claws of nature\u201d by\npractitioners of numerical computing:\n1. High-level dynamic programs have to be slow,\n2. One must prototype in one language and then rewrite in another language for speed or deploy-\nment, and\n3. There are parts of a system for the programmer, and other parts best left untouched as they\nare built by the experts.\nWe introduce the Julia programming language and its design \u2014 a dance between specialization\nand abstraction. Specialization allows for custom treatment. Multiple dispatch, a technique from\ncomputer science, picks the right algorithm for the right circumstance.\nAbstraction, what good\ncomputation is really about, recognizes what remains the same after di\ufb00erences are stripped away.\nAbstractions in mathematics are captured as code through another technique from computer science,\ngeneric programming.\nJulia shows that one can have machine performance without sacri\ufb01cing human convenience.\n\u2217http://www.juliacomputing.com\n1\narXiv:1411.1607v4  [cs.MS]  19 Jul 2015\nContents\n1\nScienti\ufb01c computing languages: The Julia innovation\n3\n1.1\nComputing transcends communities\n. . . . . . . . . . . . . . . . . . . . . . . . . . .\n3\n1.2\nJulia architecture and language design philosophy . . . . . . . . . . . . . . . . . . . .\n3\n2\nA taste of Julia\n5\n2.1\nA brief tour . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n5\n2.2\nAn invaluable tool for numerical integrity\n. . . . . . . . . . . . . . . . . . . . . . . .\n9\n2.3\nThe Julia community . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n10\n3\nWriting programs with and without types\n11\n3.1\nThe balance between human and the computer . . . . . . . . . . . . . . . . . . . . .\n11\n3.2\nJulia\u2019s recognizable types\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n11\n3.3\nUser\u2019s own types are \ufb01rst class too . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n12\n3.4\nVectorization: Key Strengths and Serious Weaknesses\n. . . . . . . . . . . . . . . . .\n13\n3.5\nType inference rescues \u201cfor loops\u201d and so much more . . . . . . . . . . . . . . . . . .\n14\n4\nCode selection: Run the right code at the right time\n15\n4.1\nMultiple Dispatch\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n15\n4.2\nCode selection from bits to matrices\n. . . . . . . . . . . . . . . . . . . . . . . . . . .\n17\n4.2.1\nSumming Numbers: Floats and Ints\n. . . . . . . . . . . . . . . . . . . . . . .\n17\n4.2.2\nSumming Matrices: Dense and Sparse . . . . . . . . . . . . . . . . . . . . . .\n18\n4.3\nThe many levels of code selection . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n20\n4.4\nIs \u201ccode selection\u201d just traditional object oriented programming? . . . . . . . . . . .\n21\n4.5\nQuantifying the use of multiple dispatch . . . . . . . . . . . . . . . . . . . . . . . . .\n22\n4.6\nCase Study for Numerical Computing\n. . . . . . . . . . . . . . . . . . . . . . . . . .\n23\n4.6.1\nDeterminant: Simple Single Dispatch . . . . . . . . . . . . . . . . . . . . . . .\n24\n4.6.2\nA Symmetric Arrow Matrix Type . . . . . . . . . . . . . . . . . . . . . . . . .\n24\n5\nLeveraging language design for high performance libraries\n26\n5.1\nInteger arithmetic\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n26\n5.2\nA powerful approach to linear algebra\n. . . . . . . . . . . . . . . . . . . . . . . . . .\n28\n5.2.1\nMatrix factorizations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n28\n5.2.2\nUser-extensible wrappers for BLAS and LAPACK\n. . . . . . . . . . . . . . .\n29\n5.3\nHigh Performance Polynomials and Special Functions with Macros . . . . . . . . . .\n31\n5.4\nEasy and \ufb02exible parallelism . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n31\n5.5\nPerformance Recap . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n34\n6\nConclusion and Acknowledgments\n35\n2\n1\nScienti\ufb01c computing languages: The Julia innovation\nThe original numerical computing language was Fortran, short for \u201cFormula Translating System\u201d,\nreleased in 1957.\nSince those early days, scientists have dreamed of writing high-level, generic\nformulas and having them translated automatically into low-level, e\ufb03cient machine code, tailored to\nthe particular data types they need to apply the formulas to. Fortran made historic strides towards\nrealization of this dream, and its dominance in so many areas of high-performance computing is a\ntestament to its remarkable success.\nThe landscape of computing has changed dramatically over the years. Modern scienti\ufb01c com-\nputing environments such as Python [43], R [19], Mathematica [27], Octave [30], Matlab [28], and\nSciLab [16], to name some, have grown in popularity and fall under the general category known as\ndynamic languages or dynamically typed languages. In these programming languages, programmers\nwrite simple, high-level code without any mention of types like int, float or double that pervade\nstatically typed languages such as C and Fortran.\nMany researchers today do their day-to-day work in dynamic languages. Still, C and Fortran\nremain the gold standard for computationally-intensive problems for performance. In as much as\nthe dynamic language programmer has missed out on performance, the C and Fortran programmer\nhas missed out on productivity.\nAn unfortunate outcome of the currently popular languages is\nthat the most challenging areas of numerical computing have bene\ufb01ted the least from the increased\nabstraction and productivity o\ufb00ered by higher level languages. The consequences have been more\nserious than many realize.\nJulia\u2019s innovation is the very combination of productivity and performance. New users want\na quick explanation as to why Julia is fast, and whether somehow the same \u201cmagic dust\u201d could\nalso be sprinkled on their traditional scienti\ufb01c computing language. Julia is fast because of careful\nlanguage design and the right combination of the carefully chosen technologies that work very well\nwith each other. This paper demonstrates some of these technologies using a number of examples.\nWe invite the reader to follow along at http://juliabox.org using Jupyter notebooks [32, 36] or\nby downloading Julia http://julialang.org/downloads.\n1.1\nComputing transcends communities\nNumerical computing research has always lived on the boundary of computer science, engineer-\ning, mathematics, and computational sciences. Readers might enjoy trying to label the \u201cTop 10\nalgorithms\u201d[11] by \ufb01eld, and may quickly see that advances typically transcend any one \ufb01eld with\nbroader impacts to science and technology as a whole.\nComputing is more than using an overgrown calculator. It is a cross cutting communication\nmedium. Research into programming languages therefore breaks us out of our research boundaries.\nThe \ufb01rst decade of the 21st century saw a boost in such research with the High Productivity\nComputing Systems DARPA funded projects into the languages such as Chapel [6, 7], Fortress\n[37, 1]\nand X10 [34, 8].\nAlso contemporaneous has been a growing acceptance of Python.\nUp\nto around 2009 some of us were working on Star-P, an interactive high performance computing\nsystem for parallelizing various dynamic programming languages. Some excellent resources on Star-\nP that discuss these ideas are the Star-P user Guide [39], the Star-P Getting Started guide [40],\nand various papers [9, 14, 18, 10]. Julia continues our research into parallel computing, with the\nmost important lesson from our Star-P experience being that one cannot design a high performance\nparallel programming system without a programming language that works well sequentially.\n1.2\nJulia architecture and language design philosophy\nMany popular dynamic languages were not designed with the goal of high performance.\nAfter\nall, if you wanted really good performance you would use a static language, or so the popular\nwisdom would say. Only with the increased need in the day-to-day life of scienti\ufb01c programmers for\nsimultaneous productivity and performance in a single system has the need for high-performance\ndynamic languages become pressing. Unfortunately, retro\ufb01tting an existing slow dynamic language\n3\nfor high performance is almost impossible speci\ufb01cally in numerical computing ecosystems.\nThis\nis because numerical computing requires performance-critical numerical libraries, which invariably\ndepend on the details of the internal implementation of the high-level language, thereby locking in\nthose internal implementation details. For example, you can run Python code much faster than\nthe standard CPython implementation using the PyPy just-in-time compiler; but PyPy is currently\nincompatible with NumPy and the rest of SciPy.\nAnother important point is that just because a program is available in C or Fortran, it may\nnot run e\ufb03ciently from the high level language or be easy to \u201cglue\u201d it in.\nFor example when\nSteven Johnson tried to include his C erf function in Python, he reported that Pauli Virtane had\nto write glue code1 to vectorize the erf function over the native structures in Python in order to\nget good performance. Johnson also had to write similar glue code for Matlab, Octave, and Scilab.\nThe Julia e\ufb00ort was, by contrast, e\ufb00ortless.2 As another example, randn, Julia\u2019s normal random\nnumber generator was originally based on calling randmtzig, a C implementation. It turned out\nlater, that a pure Julia implementation of the same code actually ran faster, and is now the default\nimplementation. In some cases, \u201cglue\u201d can often lead to poor performance, even when the underlying\nlibraries being called are high performance.\nThe best path to a fast, high-level system for scienti\ufb01c and numerical computing is to make\nthe system fast enough that all of its libraries can be written in the high-level language in the \ufb01rst\nplace. The JUMP.jl [26] and the Convex.jl [42] packages are great examples of the success of this\napproach\u2014the entire library is written in Julia and uses many Julia language features described in\nthis paper.\nThe Two Language Problem: As long as the developers\u2019 language is harder than the users\u2019\nlanguage, numerical computing will always be hindered.\nThis is an essential part of the design\nphilosophy of Julia: all basic functionality must be possible to implement in Julia\u2014never force\nthe programmer to resort to using C or Fortran.\nJulia solves the two language problem.\nBasic\nfunctionality must be fast: integer arithmetic, for loops, recursion, \ufb02oating-point operations, calling\nC functions, manipulating C-like structs. While these are not only important for numerical pro-\ngrams, without them, you certainly cannot write fast numerical code. \u201cVectorization languages\u201d like\nPython+NumPy, R, and Matlab hide their for loops and integer operations, but they are still there,\ninside the C and Fortran, lurking behind the thin veneer. Julia removes this separation entirely,\nallowing high-level code to \u201cjust write a for loop\u201d if that happens to be the best way to solve a\nproblem.\nWe believe that the Julia programming language ful\ufb01lls much of the Fortran dream: automatic\ntranslation of formulas into e\ufb03cient executable code. It allows programmers to write clear, high-\nlevel, generic and abstract code that closely resembles mathematical formulas, as they have grown\naccustomed to in dynamic systems, yet produces fast, low-level machine code that has traditionally\nonly been generated by static languages.\nJulia\u2019s ability to combine these levels of performance and productivity in a single language stems\nfrom the choice of a number of features that work well with each other:\n1. An expressive type system, allowing optional type annotations (Section 3);\n2. Multiple dispatch using these types to select implementations (Section 4);\n3. Metaprogramming for code generation (Section 5.3);\n4. A data\ufb02ow type inference algorithm allowing types of most expressions to be inferred [3, 5];\n5. Aggressive code specialization against run-time types [3, 5];\n6. Just-In-Time (JIT) compilation [3, 5] using the LLVM compiler framework [23], which is also\nused by a number of other compilers such as Clang [12] and Apple\u2019s Swift [41]; and\n7. Julia\u2019s carefully written libraries that leverage the language design, i.e., points 1 through 6\nabove (Section 5).\n1https://github.com/scipy/scipy/commit/ed14bf0\n2Steven Johnson, personal communication. See http://ab-initio.mit.edu/wiki/index.php/Faddeeva_Package\n4\nPoints 1, 2, and 3 above are especially for the human, and the focus of this paper. On details\nabout the parts that are about language implementation and internals such as in points 4, 5, and\n6, we direct the reader to our earlier work( [3, 5]). Point 7 brings everything together to build high\nperformance computational libraries in Julia.\nAlthough a sophisticated type system is made available to the programmer, it remains unobtru-\nsive in the sense that one is never required to specify types, nor are type annotations necessary for\nperformance. Type information \ufb02ows naturally through the program due to data\ufb02ow type inference.\nIn what follows, we describe the bene\ufb01ts of Julia\u2019s language design for numerical computing,\nallowing programmers to more readily express themselves and obtain performance at the same time.\n2\nA taste of Julia\n2.1\nA brief tour\nIn[1]:\nA = rand(3,3) + eye(3) # Familiar Syntax\ninv(A)\nOut[1]:\n3x3 Array{Float64,2}:\n0.698106 -0.393074 -0.0480912\n-0.223584\n0.819635 -0.124946\n-0.344861\n0.134927\n0.601952\nThe output from the Julia prompt says that A is a two dimensional matrix of size 3 \u00d7 3, and\ncontains double precision \ufb02oating point numbers.\nIndexing of arrays is performed with brackets, and is 1-based. It is also possible to compute an\nentire array expression and then index into it, without assigning the expression to a variable:\nIn[2]:\nx = A[1,2]\ny = (A+2I)[3,3] # The [3,3] entry of A+2I\nOut[2]:\n2.601952\nIn Julia, I is a built-in representation of the identity matrix, without explicitly forming the\nidentity matrix as is commonly done using commands such as \u201ceye.\u201d (\u201ceye\u201d , a homonym of \u201cI\u201d, is\nused in such languages as Matlab, Octave, Go\u2019s matrix library, Python\u2019s Numpy, and Scilab.)\nJulia has symmetric tridiagonal matrices as a special type.\nFor example, we may de\ufb01ne Gil\nStrang\u2019s favorite matrix (the second order di\ufb00erence matrix) in a way that uses only O(n) memory.\nIn[3]:\nstrang(n) = SymTridiagonal(2*ones(n),-ones(n-1))\nstrang(7)\nOut[3]:\n7x7 SymTridiagonal{Float64}:\n2.0\n-1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n-1.0\n2.0\n-1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n-1.0\n2.0\n-1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n-1.0\n2.0\n-1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n-1.0\n2.0\n-1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n-1.0\n2.0\n-1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n-1.0\n2.0\nA commonly used notation to express the solution x to the equation Ax = b is A\\b. If Julia\n5\nFigure 1: Gil Strang\u2019s favorite matrix is strang(n) = SymTridiagonal(2*ones(n),-ones(n-1))\nJulia only stores the diagonal and o\ufb00-diagonal. (Picture taken in Gil Strang\u2019s classroom.)\nknows that A is a tridiagonal matrix, it uses an e\ufb03cient O(n) algorithm:\nIn[4]:\nstrang(8)\\ones(8)\nOut[4]:\n8-element Array{Float64,1}:\n4.0\n7.0\n9.0\n10.0\n10.0\n9.0\n7.0\n4.0\nNote the Array{ElementType,dims} syntax. In the above example, the elements are 64 bit \ufb02oats\nor Float64\u2019s. The 1 indicates it is a one dimensional vector.\nConsider the sorting of complex numbers. Sometimes it is handy to have a sort that generalizes\nthe real sort. This can be done by sorting \ufb01rst by the real part, and where there are ties, sort by\nthe imaginary part. Other times it is handy to use the polar representation, which sorts by radius\nthen angle. By default, complex numbers are incomparable in Julia.\nIf a numerical computing language \u201chard-wires\u201d its sort to be one or the other, it misses an\nopportunity. A sorting algorithm need not depend on details of what is being compared or how it is\nbeing compared. One can abstract away these details thereby reusing a sorting algorithm for many\ndi\ufb00erent situations. One can specialize later. Thus alphabetizing strings, sorting real numbers, or\nsorting complex numbers in two or more ways all run with the same code.\nIn Julia, one can turn a complex number w into an ordered pair of real numbers (a tuple of length\n2) such as the Cartesian form (real(w),imag(w)) or the polar form (abs(w),angle(w)). Tuples are\nthen compared lexicographically in Julia. The sort command takes an optional \u201cless-than\u201d operator,\nlt, which is used to compare elements when sorting. Note the compact function de\ufb01nition syntax\n6\navailable in Julia used in the example below and is of the form f(x,y,...) = <expression>.\nIn[5]:\n# Cartesian comparison sort of complex numbers\ncomplex compare1(w,z) = (real(w),imag(w)) < (real(z),imag(z))\nsort([-2,2,-1,im,1], lt = complex compare1 )\nOut[5]:\n5-element Array{Complex{Int64},1}:\n-2+0im\n-1+0im\n0+1im\n1+0im\n2+0im\nIn[6]:\n# Polar comparison sort of complex numbers\ncomplex compare2(w,z) = (abs(w),angle(w)) < (abs(z),angle(z))\nsort([-2,2,-1,im,1], lt = complex compare2)\nOut[6]:\n5-element Array{Complex{Int64},1}:\n1+0im\n0+1im\n-1+0im\n2+0im\n-2+0im\nTo be sure, experienced computer scientists tend to suspect there is nothing new under the sun.\nThe C function qsort() takes a compar function. Nothing really new there. Python also has custom\nsorting with a key. Matlab\u2019s sort is more basic. The real contribution of Julia, as will be \ufb02eshed out\nfurther in this paper, is that the design of Julia allows custom sorting to be high performance and\n\ufb02exible and comparable with implementations in other dynamic languages that are often written in\n7\nC.\nIn[7]:\nPkg.add(\"PyPlot\") # Download the PyPlot package\nusing PyPlot # load the functionality into Julia\nfor i=1:5\ny=cumsum(randn(500))\nplot(y)\nend\n0\n100\n200\n300\n400\n500\n30\n20\n10\n0\n10\n20\n30\nThe next example that we have chosen for the introductory taste of Julia is a quick plot of\nBrownian motion, in two ways.\nThe \ufb01rst such example uses the Python Matplotlib package for\ngraphics, which is popular for users coming from Python or Matlab.\nThe second example uses\nGad\ufb02y.jl, another very popular package for plotting. Gad\ufb02y was built by Daniel Jones completely\nin Julia and was in\ufb02uenced by the well admired Grammar of Graphics (see [45] and [44])3 Many\nJulia users \ufb01nd Gad\ufb02y more \ufb02exible and prefer its aesthetics. Julia plots can also be manipulated\ninteractively with sliders and buttons using Julia\u2019s Interact.jl package4. The Interact.jl package page\ncontains many examples of interactive visualizations5.\nIn[8]:\nPkg.add(\"Gadfly\") # Download the Gadfly package\nusing Gadfly # load the functionality into Julia\nn = 500\np = [layer(x=1:n, y=cumsum(randn(n)), color=[i], Geom.line)\nfor i in [\"First\",\"Second\",\"Third\"]]\nlabels=(Guide.xlabel(\"Time\"),Guide.ylabel(\"Value\"),\nGuide.Title(\"Brownian Motion Trials\"),Guide.colorkey(\"Trial\"))\nplot(p...,labels...)\n3See tutorial on http://gadflyjl.org\n4https://github.com/JuliaLang/Interact.jl\n5https://github.com/JuliaLang/Interact.jl/issues/36\n8\nTime\n0\n100\n200\n300\n400\n500\nFirst\nSecond\nThird\nTrial\nValue\n-60\n-40\n-20\n0\n20\nBrownian Motion Trials\nThe ellipses on the last line above is known as a splat operator. The elements of the vector p\nand the tuple labels are inserted individually as arguments to the plot function.\n2.2\nAn invaluable tool for numerical integrity\nOne popular feature of Julia is that it gives the user the ability to \u201ckick the tires\u201d of a numerical\ncomputation. We thank Velvel Kahan for the sage advice6 concerning the importance of this feature.\nThe idea is simple: a good engineer tests his or her code for numerical stability. In Julia this can\nbe done by changing IEEE rounding modes. There are \ufb01ve modes to choose from, yet most engineers\nsilently only choose the RoundNearest mode default available in many numerical computing systems.\nIf a di\ufb00erence is detected, one can also run the computation in higher precision. Kahan writes:\nCan the e\ufb00ects of roundo\ufb00upon a \ufb02oating-point computation be assessed without\nsubmitting it to a mathematically rigorous and (if feasible at all) time-consuming error-\nanalysis? In general, No.\n. . .\nThough far from foolproof, rounding every inexact arithmetic operation (but not con-\nstants) in the same direction for each of two or three directions besides the default To\nNearest is very likely to con\ufb01rm accidentally exposed hypersensitivity to roundo\ufb00. When\nfeasible, this scheme o\ufb00ers the best Bene\ufb01t/Cost ratio. [21]\nAs an example, we round a 15x15 Hilbert-like matrix, and take the [1,1] entry of the inverse\ncomputed in various round o\ufb00modes. The radically di\ufb00erent answers dramatically indicates the\nnumerical sensitivity to roundo\ufb00. We even noticed that slight changes to LAPACK give radically\ndi\ufb00erent answers. Very likely you will see di\ufb00erent numbers when you run this code due to the very\nhigh sensitivity to roundo\ufb00errors.\nIn[9]:\nh(n)=[1/(i+j+1) for i=1:n,j=1:n]\nOut[9]:\nh (generic function with 1 method)\n6Personal communication, January 2013, in the Kahan home, Berkeley, California\n9\nIn[10]:\nH=h(15);\nwith rounding(Float64,RoundNearest) do\ninv(H)[1,1]\nend\nOut[10]:\n154410.55589294434\nIn[11]:\nwith rounding(Float64,RoundUp) do\ninv(H)[1,1]\nend\nOut[11]:\n-49499.606132507324\nIn[12]:\nwith rounding(Float64,RoundDown) do\ninv(H)[1,1]\nend\nOut[12]:\n-841819.4371948242\nWith 300 bits of precision, we obtain\nIn[13]:\nwith bigfloat precision(300) do\ninv(big(H))[1,1]\nend\nOut[13]:\n-2.09397179250746270128280174214489516162708857703714959763232689047153\n50765882491054998376252e+03\nNote this is the [1,1] entry of the inverse of the rounded Hilbert-like matrix, not the inverse of the\nexact Hilbert-like matrix. Also, the Float64 results are senstive to the BLAS[24] and LAPACK[2],\nand may di\ufb00er on di\ufb00erent machines with di\ufb00erent versions of Julia. For extended precision, Julia\nuses the MPFR library[33].\n2.3\nThe Julia community\nJulia has been in development since 2009. A public release was announced in February of 2012. It is\nan active open source project with over 350 contributors and is available under the MIT License [25]\nfor open source software. Over 1.3 million unique visitors have visited the Julia website since then,\nand Julia has now been adopted as a teaching tool in dozens of universities around the world7.\nWhile it was nurtured at the Massachusetts Institute of Technology, it is really the contributions\nfrom experts around the world that make it a joy to use for numerical computing. It is also recognized\nas a general purpose computing language unlike traditional numerical computing systems, allowing it\nto be used not only to prototype numerical algorithms, but also to deploy those algorithms, and even\nserve results to the rest of the world. A great example of this is Shashi Gowda\u2019s Escher.jl package8,\nwhich makes it possible for Julia programmers to build beautiful interactive websites in Julia, and\nserve up the results of a Julia computation from the web server, without any knowledge of HTML\nor javascript. Another such example is the Sudoku as a service9, by Iain Dunning, where a Sudoku\npuzzle is solved using the optimization capabilities of the JUMP.jl Julia package [26] and made\navailable as a web service. This is exactly why Julia is being increasingly deployed in production\nenvironments in businesses, as seen in various talks at JuliaCon 10.\nThese use cases utilize not\n7http://julialang.org/community\n8https://github.com/shashi/Escher.jl\n9http://iaindunning.com/2013/sudoku-as-a-service.html\n10http://www.juliacon.org\n10\njust Julia\u2019s capabilities for mathematical computation, but also to build web APIs, database access,\nand much more. Perhaps most signi\ufb01cantly, a rapidly growing ecosystem of over 600 open source,\ncomposable packages 11, which include a mix of libraries for mathematical as well as general purpose\ncomputing, is leading to the adoption of Julia in research, businesses, and in government.\n3\nWriting programs with and without types\n3.1\nThe balance between human and the computer\nGraydon Hoare, author of the Rust programming language [35], in an essay on \u201cInteractive Scienti\ufb01c\nComputing\u201d [17] de\ufb01ned programming languages succinctly:\nProgramming languages are mediating devices, interfaces that try to strike a balance\nbetween human needs and computer needs.\nImplicit in that is the assumption that\nhuman and computer needs are equally important, or need mediating.\nA program consists of data and operations on data. Data is not just the input \ufb01le, but everything\nthat is held\u2014an array, a list, a graph, a constant\u2014during the life of the program. The more the\ncomputer knows about this data, the better it is at executing operations on that data.\nTypes\nare exactly this metadata. Describing this metadata, the types, takes real e\ufb00ort for the human.\nStatically typed languages such as C and Fortran are at one extreme, where all types must be\nde\ufb01ned and are statically checked during the compilation phase. The result is excellent performance.\nDynamically typed languages dispense with type de\ufb01nitions, which leads to greater productivity, but\nlower performance as the compiler and the runtime cannot bene\ufb01t from the type information that\nis essential to produce fast code. Can we strike a balance between the human\u2019s preference to avoid\ntypes and the computer\u2019s need to know?\n3.2\nJulia\u2019s recognizable types\nMany users of Julia may never need to know about types for performance. Julia\u2019s type inference\nsystem often does the work, giving performance without type declarations.\nJulia\u2019s design allows for the gradual learning of concepts, where users start in a manner that\nis familiar to them and over time, learn to structure programs in the \u201cJulian way\u201d \u2014 a term\nthat captures well-structured readable high performance Julia code. Julia users coming from other\nnumerical computing environments have a notion that data may be represented as matrices that\nmay be dense, sparse, symmetric, triangular, or of some other kind. They may also, though not\nalways, know that elements in these data structures may be single precision \ufb02oating point numbers,\ndouble precision, or integers of a speci\ufb01c width. In more general cases, the elements within data\nstructures may be other data structures. We introduce Julia\u2019s type system using matrices and their\nnumber types:\nIn[14]:\nrand(1,2,1)\nOut[14]:\n1x2x1 Array{Float64,3}:\n[ :, :, 1] =\n0.789166 0.652002\nIn[15]:\n[1 2; 3 4]\nOut[15]:\n2x2 Array{Int64,2}:\n1 2\n3 4\n11urlhttp://pkg.julialang.org\n11\nIn[16]:\n[true; false]\nOut[16]:\n2-element Array{Bool,1}:\ntrue\nfalse\nWe see a pattern in the examples above. Array{T,ndims} is the general form of the type of a\ndense array with ndims dimensions, whose elements themselves have a speci\ufb01c type T, which is of\ntype double precision \ufb02oating point in the \ufb01rst example, a 64-bit signed integer in the second, and\na boolean in the third example. Therefore Array{T,1} is a 1-d vector (\ufb01rst class objects in Julia)\nwith element type T and Array{T,2} is the type for 2-d matrices.\nIt is useful to think of arrays as a generic N-d object that may contain elements of any type\nT. Thus T is a type parameter for an array that can take on many di\ufb00erent values. Similarly, the\ndimensionality of the array ndims is also a parameter for the array type. This generality makes\nit possible to create arrays of arrays. For example, Using Julia\u2019s array comprehension syntax, we\ncreate a 2-element vector containing 2 \u00d7 2 identity matrices.\nIn[17]:\na = [eye(2) for i=1:2]\nOut[17]:\n2-element Array{Array{Float64,2},1}:\n3.3\nUser\u2019s own types are \ufb01rst class too\nMany dynamic languages for numerical computing have traditionally had an asymmetry, where\nbuilt-in types have much higher performance than any user-de\ufb01ned types. This is not the case with\nJulia, where there is no meaningful distinction between user-de\ufb01ned and \u201cbuilt-in\u201d types.\nWe have mentioned so far a few number types and two matrix types, Array{T,2} the dense\narray, with element type T and SymTridiagonal{T}, the symmetric tridiagonal with element type\nT. There are also other matrix types, for other structures including SparseMatrixCSC (Compressed\nSparse Columns), Hermitian, Triangular, Bidiagonal, and Diagonal. Julia\u2019s sparse matrix type has\nan added \ufb02exibility that it can go beyond storing just numbers as nonzeros, and instead store any\nother Julia type as well. The indices in SparseMatrixCSC can also be represented as integers of any\nwidth (16-bit, 32-bit or 64-bit). All these di\ufb00erent matrix types, although available as built-in types\nto a user downloading Julia, are implemented completely in Julia, and are in no way any more or\nless special than any other types one may de\ufb01ne in their own program.\nFor demonstration, we create a symmetric arrow matrix type that contains a diagonal and the\n\ufb01rst row A[1,2:n].\nIn[18]:\n# Type Parameter Example (Parameter T)\n# Define a Symmetric Arrow Matrix Type with elements of type T\ntype SymArrow{T}\ndv::Vector{T} # diagonal\nev::Vector{T} # 1st row[2:n]\nend\n# Create your first Symmetric Arrow Matrix\nS = SymArrow([1,2,3,4,5],[6,7,8,9])\nOut[18]:\nSymArrow{Int64}([1,2,3,4,5],[6,7,8,9])\nThe parameter in the array refers to the type of each element of the array. Code can and should\nbe written independently of the type of each element.\n12\nLater in Section 4.6.2, we develop the symmetric arrow example much further. The SymArrow\nmatrix type contains two vectors, one each for the diagonal and the \ufb01rst row, and these vector\ncontain elements of type T. In the type de\ufb01nition, the type SymArrow is parametrized by the type\nof the storage element T. By doing so, we have created a generic type, which refers to a universe of\nall arrow matrices containing elements of all types. The matrix S, is an example where T is Int64.\nWhen we write functions in Section 4.6.2 that operate on arrow matrices, those functions themselves\nwill be generic and applicable to the entire universe of arrow matrices we have de\ufb01ned here.\nJulia\u2019s type system allows for abstract types, concrete \u201cbits\u201d types, composite types, and im-\nmutable composite types. All of these can have parameters and users may even write programs\nusing unions of these di\ufb00erent types. We refer the reader to read all about Julia\u2019s type system in\nthe types chapter in the Julia manual12.\n3.4\nVectorization: Key Strengths and Serious Weaknesses\nUsers of traditional high level computing languages know that vectorization improves performance.\nDo most users know exactly why vectorization is so useful? It is precisely because, by vectorizing,\nthe user has promised the computer that the type of an entire vector of data matches the very \ufb01rst\nelement. This is an example where users are willing to provide type information to the computer\nwithout even knowing exactly that is what they are doing. Hence, it is an example of a strategy\nthat balances the computer\u2019s needs with the human\u2019s.\nFrom the computer\u2019s viewpoint, vectorization means that operations on data happen largely in\nsections of the code where types are known to the runtime system. When the runtime is operating\non arrays, it has no idea about the data contained in an array until it encounters the array. Once\nencountered, the type of the data within the array is known, and this knowledge is used to execute\nan appropriate high performance kernel. Of course what really occurs at runtime is that the system\n\ufb01gures out the type, and gets to reuse that information through the length of the array. As long as\nthe array is not too small, all the extra work in gathering type information and acting upon it at\nruntime is amortized over the entire operation.\nThe downside of this approach is that the user can achieve high performance only with built-in\ntypes, and user de\ufb01ned types end up being dramatically slower. The restructuring for vectorization\nis often unnatural, and at times not possible. We illustrate this with an example of the cumulative\nsum computation. Note that due to the size of the problem, the computation is memory bound, and\none does not observe the case with complex arithmetic to be twice as slower than the real case, even\nthough it is performing twice as many \ufb02oating point operations.\nIn[19]:\n# Sum prefix (cumsum) on vector w with elements of type T\nfunction prefix{T}(w::Vector{T})\nfor i=2:size(w,1)\nw[i]+=w[i-1]\nend\nw\nend\nWe execute this code on a vector of double precision numbers and double-precision complex\nnumbers and observe something that may seem remarkable: similar running times.\nIn[20]:\nx = ones(1 000 000)\n@time prefix(x)\ny = ones(1 000 000) + im*ones(1 000 000)\n@time prefix(y);\n12See the chapter on types in the Julia manual: http://docs.julialang.org/en/latest/manual/types/\n13\nOut[20]:\nelapsed time:\n0.003243692 seconds (80 bytes allocated)\nelapsed time:\n0.003290693 seconds (80 bytes allocated)\nThis simple example is di\ufb03cult to vectorize, and hence is often provided as a built-in function\nin many numerical computing systems. In Julia, the implementation is very similar to the snippet\nof code above, and runs at speeds similar to C. While Julia users can write vectorized programs like\nin any other dynamic language, vectorization is not a pre-requisite for performance. This is because\nJulia strikes a di\ufb00erent balance between the human and the computer when it comes to specifying\ntypes. Julia allows optional type annotations, which are essential when writing libraries that utilize\nmultiple dispatch, but not for end-user programs that are exploring algorithms or a dataset.\nGenerally, in Julia, type annotations are not for performance. They are purely for code selection.\n(See Section 4). If the programmer annotates their program with types, the Julia compiler will use\nthat information. But for the most part, user code often includes minimal or no type annotations,\nand the Julia compiler automatically infers the types.\n3.5\nType inference rescues \u201cfor loops\u201d and so much more\nA key component of Julia\u2019s ability to combine performance with productivity in a single language\nis its implementation of data\ufb02ow type inference [29],[22],[5]. Unlike type inference algorithms for\nstatic languages, this algorithm is tailored to the way dynamic languages work: the typing of code\nis determined by the \ufb02ow of data through it. The algorithm works by walking through a program,\nstarting with the types of its input values, and \u201cabstractly interpreting\u201d it: instead of applying the\ncode to values, it applies the code to types, following all branches concurrently and tracking all\npossible states the program could be in, including all the types each expression could assume.\nThe data\ufb02ow type inference algorithm allows programs to be automatically annotated with type\nbounds without forcing the programmer to explicitly specify types. Yet, in dynamic languages it\nis possible to write programs which inherently cannot be concretely typed. In such cases, data\ufb02ow\ntype inference provides what bounds it can, but these may be trivial and useless\u2014i.e. they may\nnot narrow down the set of possible types for an expression at all. However, the design of Julia\u2019s\nprogramming model and standard library are such that a majority of expressions in typical programs\ncan be concretely typed. Moreover, there is a positive correlation between the ability to concretely\ntype code and that code being performance-critical.\nA lesson of the numerical computing languages is that one must learn to vectorize to get perfor-\nmance. The mantra is \u201cfor loops\u201d are bad, vectorization is good. Indeed one can \ufb01nd the mantra\non p.72 of the \u201c1998 Getting Started with Matlab manual\u201d (and other editions):\nExperienced Matlab users like to say \u201cLife is too short to spend writing for loops.\u201d\nIt is not that \u201cfor loops\u201d are inherently slow by themselves. The slowness comes from the fact\nthat in the case of most dynamic languages, the system does not have access to the types of the\nvariables within a loop. Since programs often spend much of their time doing repeated computations,\nthe slowness of a particular operation due to lack of type information is magni\ufb01ed inside a loop.\nThis leads to users often talking about \u201cslow for loops\u201d or \u201cloop overhead\u201d.\nIn statically typed languages, full type information is always available at compile time, allowing\ncompilation of a loop into a few machine instructions. This is not the case in most dynamic languages,\nwhere the types are discovered at run time, and the cost of determining the types and selecting the\nright operation can run into hundreds or thousands of instructions.\nJulia has a transparent performance model. For example a Vector{Float64} as in our example\nhere, always has the same in-memory representation as it would in C or Fortran; one can take a\npointer to the \ufb01rst array element and pass it to a C library function using ccall and it will just\nwork. The programmer knows exactly how the data is represented and can reason about it. They\nknow that a Vector{Float64} does not require any additional heap allocation besides the Float64\nvalues and that arithmetic operations on these values will be machine arithmetic operations. In\nthe case of say, Complex128, Julia stores complex numbers in the same way as C or Fortran. Thus\ncomplex arrays are actually arrays of complex values, where the real and imaginary values are stored\n14\nconsecutively. Some systems have taken the path of storing the real and imaginary parts separately,\nwhich leads to some convenience for the user, at the cost of performance and interoperability. With\nthe immutable keyword, a programmer can also de\ufb01ne immutable data types, and enjoy the same\nbene\ufb01ts of performance for composite types as for the more primitive number types (bits types).\nThis approach is being used to de\ufb01ne many interesting data structures such as small arrays of \ufb01xed\nsizes, which can have much higher performance than the more general array data structure.\nThe transparency of the C data and performance models has been one of the major reasons for\nC\u2019s long-lived success. One of the design goals of Julia is to have similarly transparent data and\nperformance models. With a sophisticated type system and type inference, Julia achieves both.\n4\nCode selection: Run the right code at the right time\nCode selection or code specialization from one point of view is the opposite of code reuse enabled\nby abstraction. Ironically, viewed another way, it enables abstraction. Julia allows users to overload\nfunction names, and select code based on argument types. This can happen at the highest and\nlowest levels of the software stack. Code specialization lets us optimize for the details of the case at\nhand. Code abstraction lets calling codes, probably those not yet even written or perhaps not even\nimagined, work all the way through on structures that may not have been envisioned by the original\nprogrammer.\nWe see this as the ultimate realization of the famous 1908 quip that\nMathematics is the art of giving the same name to di\ufb00erent things.\nby noted mathematician Henri Poincar\u00b4e.13\nIn this upcoming section we provide examples of how plus can apply to so many objects. Some\nexamples are \ufb02oating point numbers, or integers. It can also apply to sparse and dense matrices.\nAnother example is the use of the same name, \u201cdet\u201d, for determinant, for the very di\ufb00erent algorithms\nthat apply to very di\ufb00erent matrix structures. The use of overloading not only for single argument\nfunctions, but for multiple argument functions is already a powerful abstraction.\n4.1\nMultiple Dispatch\nMultiple dispatch is the selection of a function implementation based on the types of each argument\nof the function. It is not only a nice notation to remove a long list of \u201ccase\u201d statements, but it is\npart of the reason for Julia\u2019s speed. It is expressed in Julia by annotating the type of a function\nargument in a function de\ufb01nition with the following syntax: argument::Type.\n13 A few versions of this quote are relevant to Julia\u2019s power of abstractions and numerical computing. They are worth\npondering:\nIt is the harmony of the di\ufb00erent parts, their symmetry, and their happy adjustment; it is, in a word, all that\nintroduces order, all that gives them unity, that enables us to obtain a clear comprehension of the whole as\nwell as of the parts. Elegance may result from the feeling of surprise caused by the unlooked-for occurrence of\nobjects not habitually associated. In this, again, it is fruitful, since it discloses thus relations that were until\nthen unrecognized. Mathematics is the art of giving the same names to di\ufb00erent things.\nhttp://www.nieuwarchief.nl/serie5/pdf/naw5-2012-13-3-154.pdf. and\nOne example has just shown us the importance of terms in mathematics; but I could quote many others. It\nis hardly possible to believe what economy of thought, as Mach used to say, can be e\ufb00ected by a well-chosen\nterm. I think I have already said somewhere that mathematics is the art of giving the same name\nto di\ufb00erent things. It is enough that these things, though di\ufb00ering in matter, should be similar in form,\nto permit of their being, so to speak, run in the same mould. When language has been well chosen, one is\nastonished to \ufb01nd that all demonstrations made for a known object apply immediately to many new objects:\nnothing requires to be changed, not even the terms, since the names have become the same.\nhttp://www-history.mcs.st-andrews.ac.uk/Extras/Poincare Future.html\n15\nFigure 2:\nGauss quote hanging from the ceiling of the longstanding old Boston Museum of Science\nMathematica Exhibit.\nMathematical notations that are often used in print are di\ufb03cult to employ in programs. For\nexample, we can teach the computer some natural ways to multiply numbers and functions. Suppose\nthat a and t are scalars, and f and g are functions, and we wish to de\ufb01ne\n1. Number x Function = scale output: a \u2217g is the function that takes x to a \u2217g(x)\n2. Function x Number = scale argument : f \u2217t is the function that takes x to f(tx) and\n3. Function x Function = composition of functions: f \u2217g is the function that takes x to\nf(g(x)).\nIf you are a mathematician who does not program, you would not see the fuss. If you thought\nhow you might implement this in your favorite computer language, you might immediately see the\nbene\ufb01t. In Julia, multiple dispatch makes all three uses of * easy to express:\nIn[21]:\n*(a::Number, g::Function)= x->a*g(x)\n# Scale output\n*(f::Function,t::Number) = x->f(t*x)\n# Scale argument\n*(f::Function,g::Function)= x->f(g(x)) # Function composition\nHere, multiplication is dispatched by the type of its \ufb01rst and second arguments. It goes the usual\nway if both are numbers, but there are three new ways if one, the other, or both are functions.\nThese de\ufb01nitions exist as part of a larger system of generic de\ufb01nitions, which can be reused\nby later de\ufb01nitions. Consider the case of the mathematician Gauss\u2019 preference for sin2 \u03c6 to refer\nto sin(sin(\u03c6)) and not sin(\u03c6)2 (writing \u201csin2(\u03c6) is odious to me, even though Laplace made use of\nit.\u201d(Figure 2).) By de\ufb01ning *(f::Function,g::Function)= x->f(g(x)), (f^2)(x) automatically\ncomputes f(f(x)) as Gauss wanted. This is a consequence of a generic de\ufb01nition that evaluates x^2\nas x*x no matter how x*x is de\ufb01ned.\nThis paradigm is a natural \ufb01t for numerical computing, since so many important operations\ninvolve interactions among multiple values or entities.\nBinary arithmetic operators are obvious\nexamples, but many other uses abound. The fact that the compiler can pick the sharpest matching\nde\ufb01nition of a function based on its input types helps achieve higher performance, by keeping the\ncode execution paths tight and minimal.\nWe have not seen this in the literature but it seems worthwhile to point out four possibilities:\n1. Static single dispatch (not done)\n2. Static multiple dispatch (frequent in static languages, e.g. C++ overloading)\n3. Dynamic single dispatch (Matlab\u2019s object oriented system might fall in this category though\nit has its own special characteristics)\n4. Dynamic multiple dispatch (usually just called multiple dispatch).\n16\nIn Section 4.4 we discuss the comparison with traditional object oriented approaches. Class-based\nobject oriented programming could reasonably be called dynamic single dispatch, and overloading\ncould reasonably be called static multiple dispatch. Julia\u2019s dynamic multiple dispatch approach is\nmore \ufb02exible and adaptable while still retaining powerful performance capabilities. Julia program-\nmers often \ufb01nd that dynamic multiple dispatch makes it easier to structure their programs in ways\nthat are closer to the underlying science.\n4.2\nCode selection from bits to matrices\nJulia uses the same mechanism for code selection at all levels, from the top to the bottom.\nf\nFunction\nOperand Types\nLow Level \u201c+\u201d\nAdd Numbers\n{Float , Int}\nHigh Level \u201c+\u201d\nAdd Matrices\n{Dense Matrix , Sparse Matrix}\n\u201c * \u201d\nScale or Compose\n{Function , Number }\n4.2.1\nSumming Numbers: Floats and Ints\nWe begin at the lowest level. Mathematically, integers are thought of as being special real numbers,\nbut on a computer, an Int and a Float have two very di\ufb00erent representations.\nIgnoring for a\nmoment that there are even many choices of Int and Float representations, if we add two numbers,\ncode selection based on numerical representation is taking place at a very low level. Most users are\nblissfully unaware of this code selection, because it is hidden somewhere that is usually o\ufb00-limits\nto the user. Nonetheless, one can follow the evolution of the high level code all the way down to\nthe assembler level which ultimately would reveal an ADD instruction for integer addition, and,\nfor example, the AVX14 instruction VADDSD15 for \ufb02oating point addition in the language of x86\nassembly level instructions. The point being these are ultimately two di\ufb00erent algorithms being\ncalled, one for a pair of Ints and one for a pair of Floats.\nFigure 3 takes a close look at what a computer must do to perform x+y depending on whether\n(x,y) is (Int,Int), (Float,Float), or (Int,Float) respectively. In the \ufb01rst case, an integer add is called,\nwhile in the second case a \ufb02oat add is called. In the last case, a promotion of the int to \ufb02oat is called\nthrough the x86 instruction VCVTSI2SD16, and then the \ufb02oat add follows.\nIt is instructive to build a Julia simulator in Julia itself.\nLet us de\ufb01ne the aforementioned\nassembler instructions using Julia.\nIn[22]:\n# Simulate the assembly level add, vaddsd, and vcvtsi2sd commands\nadd(x::Int\n,y::Int)\n= x+y\nvaddsd(x::Float64,y::Float64) = x+y\nvcvtsi2sd(x::Int)\n= float(x)\nIn[23]:\n# Simulate Julia\u2019s definition of + using \u2295\n# To type \u2295, type as in TeX, \\oplus and hit the <tab> key\n\u2295(x::Int,\ny::Int)\n= add(x,y)\n\u2295(x::Float64,y::Float64) = vaddsd(x,y)\n\u2295(x::Int,\ny::Float64) = vaddsd(vcvtsi2sd(x),y)\n\u2295(x::Float64,y::Int)\n= y \u2295x\nIn[24]:\nmethods(\u2295)\n14AVX: Adanced Vector eXtension to the x86 instruction set\n15VADDSD: Vector ADD Scalar Double-precision\n16VCVTSI2SD: Vector ConVerT Doubleword (Scalar) Integer to(2) Scalar Double Precision Floating-Point Value\n17\nOut[24]:\n4 methods for generic function \u2295:\n\u2295(x::Int64,y::Int64) at In[23]:3\n\u2295(x::Float64,y::Float64) at In[23]:4\n\u2295(x::Int64,y::Float64) at In[23]:5\n\u2295(x::Float64,y::Int64) at In[23]:6\n4.2.2\nSumming Matrices: Dense and Sparse\nWe now move to a much higher level: matrix addition. The versatile \u201c+\u201d symbol lets us add ma-\ntrices. Mathematically, sparse matrices are thought of as being special matrices with enough zero\nentries. On a computer, dense matrices are (usually) contiguous blocks of data with a few param-\neters attached, while sparse matrices (which may be stored in many ways) require storage of index\ninformation one way or another. If we add two matrices, code selection must take place depending\non whether the summands are (dense,dense), (dense,sparse), (sparse,dense) or (sparse,sparse).\nWhile this is at a much higher level, the basic pattern is unmistakably the same as that of Section\n4.2.1. We show how to use a dense algorithm in the implementation of \u2295when either A or B (or\nboth) are dense. A sparse algorithm is used when both A and B are sparse.\nIn[29]:\n# Dense + Dense\n\u2295(A::Matrix, B::Matrix) =\n[A[i,j]+B[i,j] for i in 1:size(A,1),j in 1:size(A,2)]\n# Dense + Sparse\n\u2295(A::Matrix, B::AbstractSparseMatrix) = A \u2295full(B)\n# Sparse + Dense\n\u2295(A::AbstractSparseMatrix,B::Matrix)\n= B \u2295A # Use Dense + Sparse\n# Sparse + Sparse is best written using the long form function definition:\nfunction \u2295(A::AbstractSparseMatrix, B::AbstractSparseMatrix)\nC=copy(A)\n(i,j)=findn(B)\nfor k=1:length(i)\nC[i[k],j[k]]+=B[i[k],j[k]]\nend\nreturn C\nend\nWe now have eight methods for the function \u2295, four for the low level sum, and four more for the\nhigh level sum.\nIn[30]:\nmethods(\u2295)\nOut[30]:\n8 methods for generic function \u2295:\n\u2295(x::Int64,y::Int64) at In[23]:3\n\u2295(x::Float64,y::Float64) at In[23]:4\n\u2295(x::Int64,y::Float64) at In[23]:5\n\u2295(x::Float64,y::Int64) at In[23]:6\n\u2295(A::Array{T,2},B::Array{T,2}) at In[29]:1\n\u2295(A::Array{T,2},B::AbstractSparseArray{Tv,Ti,2}) at In[29]:1\n\u2295(A::AbstractSparseArray{Tv,Ti,2},B::Array{T,2}) at In[29]:1\n\u2295(A::AbstractSparseArray{Tv,Ti,2},B::AbstractSparseArray{Tv,Ti,2}) at\nIn[29]:2\n18\nFigure 3:\nWhile assembly code may seem intimidating, Julia disassembles readily.\nArmed\nwith\nthe\ncode native\ncommand\nin\nJulia\nand\nperhaps\na\ngood\nlist\nof\nassembler\ncom-\nmands such as may be found on http://docs.oracle.com/cd/E36784 01/pdf/E36859.pdf or\nhttp://en.wikipedia.org/wiki/X86 instruction listings one can really learn to see the details\nof code selection in action at the lowest levels. More importantly one can begin to understand that Julia\nis fast because the assembly code produced is so tight.\nIn[25]:\nf(a,b) = a + b\nOut[25]:\nf (generic function with 1 method)\nIn[26]:\n# Ints add with the x86 add instruction\n@code_native f(2,3)\nOut[26]:\npush RBP\nmov RBP, RSP\nadd RDI, RSI\nmov RAX, RDI\npop RBP\nret\nIn[27]:\n# Floats add, for example, with the x86 vaddsd instruction\n@code_native f(1.0,3.0)\nOut[27]:\npush RBP\nmov RBP, RSP\nvaddsd XMM0, XMM0, XMM1\npop RBP\nret\nIn[28]:\n# Int + Float requires a convert to scalar double precision, hence\n# the x86 vcvtsi2sd instruction\n@code_native f(1.0,3)\nOut[28]:\npush RBP\nmov RBP, RSP\nvcvtsi2sd XMM1, XMM0, RDI\nvaddsd XMM0, XMM1, XMM0\npop RBP\nret\n19\n4.3\nThe many levels of code selection\nIn Julia as in mathematics, functions are as important as the data they operate on, their arguments.\nPerhaps even more so. We can create a new function foo and gave it six de\ufb01nitions depending on\nthe combination of types. In the following example we sensitize unfamiliar readers with terms from\ncomputer science language research. It is not critical that these terms be understood all at once.\nIn[31]:\n# Define a generic function with 6 methods.\nEach method is itself a\n# function.\nIn Julia generic functions are far more convenient than the\n# multitude of case statements seen in other languages.\nWhen Julia sees\n# foo, it decides which method to use, rather than first seeing and deciding\n# based on the type.\nfoo() = \"Empty input\"\nfoo(x::Int) = x\nfoo(S::String) = length(S)\nfoo(x::Int, S::String) = \"An Int and a String\"\nfoo(x::Float64,y::Float64) = sqrt(x^2+y^2)\nfoo(a::Any,b::String)= \"Something more general than an Int and a String\"\n# The function name foo is overloaded.\nThis is an example of polymorphism.\n# In the jargon of computer languages this is called ad-hoc polymorphism.\n# The multiple dynamic dispatch idea captures the notion that the generic\n# function is deciphered dynamically at runtime.\nOne of the six choices\n# will be made or an error will occur.\nOut[31]:\nfoo (generic function with 6 methods)\nAny one instance of foo is known as a method or function. The collection of six methods is\nreferred to as a generic function. The word \u201cpolymorphism\u201d refers to the use of the same name\n(foo, in this example) for functions with di\ufb00erent types.\nContemplating the Poincar\u00b4e quote in\nFootnote 5, it is handy to reason about everything that you are giving the same name. In real life\ncoding, one tends to use the same name when the abstraction makes a great deal of sense. That we\nuse \u201d+\u201d for ints,\ufb02oats, dense matrices, and sparse matrices is the same name for di\ufb00erent things.\nMethods are grouped into generic functions.\nWhile mathematics is the art of giving the same name to seemingly di\ufb00erent things, a computer\nhas to eventually execute the right program in the right circumstance. Julia\u2019s code selection operates\nat multiple levels in order to translate a user\u2019s abstract ideas into e\ufb03cient execution. A generic\nfunction can operate on several arguments, and the method with the most speci\ufb01c signature matching\nthe arguments is invoked. It is worth crystallizing some key aspects of this process:\n1. The same name can be used for di\ufb00erent functions in di\ufb00erent circumstances. For example,\nselect may refer to the selection algorithm for \ufb01nding the kth smallest element in a list, or\nto select records in a database query, or simply as a user-de\ufb01ned function in a user\u2019s own\nprogram. Julia\u2019s namespaces allow the usage of the same vocabulary in di\ufb00erent circumstances\nin a simple way that makes programs easy to read.\n2. A collection of functions that represent the same idea but operate on di\ufb00erent structures are\nnaturally referred to by the same name.\nWhich method is called is based entirely on the\ntypes of all the arguments - this is multiple dispatch. The function det may be de\ufb01ned for all\nmatrices at an abstract level. However, for reasons of e\ufb03ciency, Julia de\ufb01nes di\ufb00erent methods\nfor di\ufb00erent types of matrices, depending on whether they are dense or sparse, or if they have\na special structure such as diagonal or tridiagonal.\n3. Within functions that operate on the same structure, there may be further di\ufb00erences based\non the di\ufb00erent types of data contained within. For example, whether the input is a vector of\nFloat64 values or Int32 values, the norm is computed in the same exact way, with a common\n20\n\\* Polymorphic Java Example. Method de\ufb01ned by types of two arguments. *\\\npublic class OverloadedAddable {\npublic int\naddthem(int i, int f} {\nreturn i+f;\n}\npublic double addthem(int i, double f} {\nreturn i+f;\n}\npublic double addthem(double i, int f} {\nreturn i+f;\n}\npublic double addthem(double i,\ndouble\nf} {\nreturn i+f;\n}\n}\nFigure 4:\nAdvantages of Julia: It is true that the above Java code is polymorphic based on the types of\nthe two arguments. (\u201cPolymorphism\u201d is the use of the same name for a function that may have di\ufb00erent\ntype arguments.) However, in Java if the method addthem is called, the types of the arguments must be\nknown at compile time. This is static dispatch. Java is also encumbered by encapsulation: in this case\naddthem is encapsulated inside the OverloadedAddable class. While this is considered a safety feature\nin Java culture, it becomes a burden for numerical computing.\nbody of code, but the compiler is able to generate di\ufb00erent executable code from the abstract\nspeci\ufb01cation.\n4. Julia uses the same mechanism of code selection at the lowest and highest levels - whether\nit is performing operations on matrices or operations on bits. As a result, Julia is able to\noptimize the whole program, picking the right method at the right time, either at compile-time\nor run-time.\n4.4\nIs \u201ccode selection\u201d just traditional object oriented program-\nming?\nThe method to be executed in Julia is not chosen by only one argument, which is what happens in the\ncase of single dispatch, but through multiple dispatch that considers the types of all the arguments.\nJulia is not encumbered by the encapsulation restrictions (class based methods) of most object\noriented languages. The generic functions play a more important role than the data types. Some\ncall this \u201cverb\u201d based languages as opposed to most object oriented languages being \u201cnoun\u201d based.\nIn numerical computing, it is the concept of \u201csolve Ax = b\u201d that often feels more primary, at the\nhighest level, rather than whether the matrix A is full, sparse, or structured. Readers familiar with\nJava might think, \u201dSo what? One can easily create methods based on the types of the arguments\u201d.\nAn example is provided in Figure 4. However a moment\u2019s thought shows that the following dynamic\n21\nsituation in Julia is impossible to express in Java:\nIn[32]:\n# It is possible for a static compiler to know that x,y are Float\nx = rand(Bool) ?\n1.0 :\n2.0\ny = rand(Bool) ?\n1.0 :\n2.0\nx+y\n# It is impossible to know until runtime if x,y are Int or Float\nx = rand(Bool) ?\n1 :\n1.0\ny = rand(Bool) ?\n1 :\n1.0\nx+y\nReaders are familiar with single dispatch mechanism, as in Matlab. It is unusual in that it is not\ncompletely class based, as the code selection is based on Matlab\u2019s own custom hierarchy. In Matlab\nthe leftmost object has precedence, but user-de\ufb01ned classes have precedence over built-in classes.\nMatlab also has a mechanism to create a custom hierarchy.\nJulia generally shuns the notion of \u201cbuilt-in\u201d vs. \u201cuser-de\ufb01ned\u201d preferring to focus on the method\nto be performed based on the combination of types, and obtaining high performance as a byproduct.\nA high level library writer, which we do not distinguish from any user, has to match the best\nalgorithm for the best input structure. A sparse matrix would match to a sparse routine, a dense\nmatrix to a dense routine. A low level language designer has to make sure that integers are added\nwith an integer adder, and \ufb02oating points are added with a \ufb02oat adder. Despite the very di\ufb00erent\nlevels, the reader might recognize that deep down, these are both examples of code being selected\nto match the structure of the problem.\nReaders familiar with object-oriented paradigms such as C++ or Java are most likely familiar\nwith the approach of encapsulating methods inside classes. Julia\u2019s more general multiple dispatch\nmechanism (also known as generic functions, or multi-methods) is a paradigm where methods are\nde\ufb01ned on combinations of data types (classes) Julia has proven that this is remarkably well suited\nfor numerical computing.\nA class based language might express the sum of a sparse matrix with a full matrix as follows:\nA sparse matrix.plus(A full matrix). Similarly it might express indexing as\nA sparse matrix.sub(A full matrix) . If a tridiagonal were added to the system, one has to \ufb01nd\nthe method plus or sub which is encapsulated in the sparse matrix class, modify it and test it.\nSimilarly, one has to modify every full matrix method, etc. We believe that class-based methods,\nwhich can be taken quite far, are not su\ufb03ciently powerful to express the full gamut of abstractions\nin scienti\ufb01c computing.\nFurther, the burdens of encapsulation create a wall around objects and\nmethods that are counterproductive for numerical computing.\nThe generic function idea captures the notion that a method for a general operation on pairs\nof matrices may exist (e.g. \u201c+\u201d) but if a more speci\ufb01c operation is possible (e.g. \u201c+\u201d on sparse\nmatrices, or \u201c+\u201d on a special matrix structure like Bidiagonal), then the more speci\ufb01c operation is\nused. We also mention indexing as another example, Why should the indexee take precedence over\nthe index?\n4.5\nQuantifying the use of multiple dispatch\nIn [4] we performed an analysis to substantiate the claim that multiple dispatch, an esoteric idea for\nnumerical computing from computer languages, \ufb01nds its killer application in scienti\ufb01c computing.\nWe wanted to answer for ourselves the question of whether there was really anything di\ufb00erent about\nhow Julia uses multiple dispatch.\nTable 4.5 gives an answer in terms of Dispatch ratio (DR), Choice ratio (CR). and Degree of\nspecialization (DoS). While multiple dispatch is an idea that has been circulating for some time, its\napplication to numerical computing appears to have signi\ufb01cantly favorable characteristics compared\nto previous applications.\n22\nLanguage\nDR\nCR\nDoS\nGwydion\n1.74\n18.27\n2.14\nOpenDylan\n2.51\n43.84\n1.23\nCMUCL\n2.03\n6.34\n1.17\nSBCL\n2.37\n26.57\n1.11\nMcCLIM\n2.32\n15.43\n1.17\nVortex\n2.33\n63.30\n1.06\nWhirlwind\n2.07\n31.65\n0.71\nNiceC\n1.36\n3.46\n0.33\nLocStack\n1.50\n8.92\n1.02\nJulia\n5.86\n51.44\n1.54\nJulia operators\n28.13\n78.06\n2.01\nTable 1:\nA comparison of Julia (1208 functions exported from the Base library) to other languages\nwith multiple dispatch. The \u201cJulia operators\u201d row describes 47 functions with special syntax (binary\noperators, indexing, and concatenation). Data for other systems are from [31]. The results indicate that\nJulia is using multiple dispatch far more heavily than previous systems.\nTo quantify how heavily a language feature is used, we use the following metrics for evaluating\nthe extent of multiple dispatch [31]:\n1. Dispatch ratio (DR): The average number of methods in a generic function.\n2. Choice ratio (CR): For each method, the total number of methods over all generic functions it\nbelongs to, averaged over all methods. This is essentially the sum of the squares of the number\nof methods in each generic function, divided by the total number of methods. The intent of\nthis statistic is to give more weight to functions with a large number of methods.\n3. Degree of specialization (DoS): The average number of type-specialized arguments per method.\nTable 4.5 shows the mean of each metric over the entire Julia Base library, showing a high degree\nof multiple dispatch compared with corpora in other languages [31]. Compared to most multiple\ndispatch systems, Julia functions tend to have a large number of de\ufb01nitions. To see why this might\nbe, it helps to compare results from a biased sample of common operators. These functions are the\nmost obvious candidates for multiple dispatch, and as a result their statistics climb dramatically.\nJulia is focused on numerical computing, and so is likely to have a large proportion of functions with\nthis character.\n4.6\nCase Study for Numerical Computing\nThe complexity of linear algebra software has been nicely captured in the context of LAPACK and\nScaLAPACK by Demmel and Dongarra, et.al., [13] and reproduced verbatim here:\n(1) for all linear algebra problems\n(linear systems, eigenproblems, ...)\n(2)\nfor all matrix types\n(general, symmetric, banded, ...)\n(3)\nfor all data types\n(real, complex, single, double, higher precision)\n(4)\nfor all machine architectures\nand communication topologies\n(5)\nfor all programming interfaces\n(6)\nprovide the best algorithm(s) available in terms of\nperformance and accuracy (\u2018\u2018algorithms\" is plural\n23\nbecause sometimes no single one is always best)\nIn the language of Computer Science, code reuse is about taking advantage of polymorphism. In\nthe general language of mathematics it\u2019s about taking advantage of abstraction, or the sameness of\ntwo things. Either way, programs are e\ufb03cient, powerful, and maintainable if programmers are given\npowerful mechanisms to reuse code.\nIncreasingly, the applicability of linear algebra has gone well beyond the LAPACK world of\n\ufb02oating point numbers. These days linear algebra is being performed on, say, high precision numbers,\nintegers, elements of \ufb01nite \ufb01elds, or rational numbers. There will always be a special place for the\nBLAS, and the performance it provides for \ufb02oating point numbers.\nNonetheless, linear algebra\noperations transcend any one data type. One must be able to write a general implemenation and as\nlong as the necessary operations are available, the code should just work [20]. That is the power of\ncode reuse.\n4.6.1\nDeterminant: Simple Single Dispatch\nIn traditional numerical computing there were people with special skills known as library writers.\nMost users were, well, just users of libraries. In this case study, we show how anybody can dispatch\na new determinant function based solely on the type of the argument.\nFor triangular and diagonal structures the obvious formulas are used. For general matrices, the\nprogrammer will compute a QR decomposition of the matrix and \ufb01nd the determinant as the product\nof the diagonal elements of R.17 For symmetric tridiagonals the usual 3-term recurrence formula[38]\nis used. (The \ufb01rst four are de\ufb01ned as one line functions; the symmetric tridiagonal uses the long\nform.)\nIn[33]:\n# Simple determinants defined using the short form for functions\nnewdet(x::Number) = x\nnewdet(A::Diagonal ) = prod(diag(A))\nnewdet(A::Triangular) = prod(diag(A))\nnewdet(A::Matrix) = -prod(diag(qrfact(full(A))[:R]))*(-1)^size(A,1)\n# Tridiagonal determinant defined using the long form for functions\nfunction newdet(A::SymTridiagonal)\n# Assign c and d as a pair\nc,d = 1, A[1,1]\nfor i=2:size(A,1)\n# temp=d, d=the expression, c=temp\nc,d = d, d*A[i,i]-c*A[i,i-1]^2\nend\nd\nend\nWe have illustrated a mechanism to select a determinant formula at runtime based on the type of\nthe input argument. If Julia knows an argument type early, it can make use of this information for\nperformance. If it does not, code selection can still happen, at runtime. The reason why Julia can\nstill perform well is that once code selection based on type occurs, Julia can return to performing\nwell once inside the method.\n4.6.2\nA Symmetric Arrow Matrix Type\nIn the \ufb01eld of Matrix Computations, there are matrix structures and operations on these matrices.\nIn Julia, these structures exist as Julia types.\nJulia has a number of prede\ufb01ned matrix struc-\nture types: (dense) Matrix, (compressed sparse column) SparseMatrixCSC, Symmetric, Hermitian,\n17LU is more e\ufb03cient. We simply wanted to illustrate other ways are possible.\n24\nSymTridiagonal, Bidiagonal, Tridiagonal, Diagonal, and Triangular are all examples of Julia\u2019s\nmatrix structures.\nThe operations on these matrices exist as Julia functions. Familiar examples of operations are\nindexing, determinant, size, and matrix addition. Since matrix addition takes two arguments, it\nmay be necessary to reconcile two di\ufb00erent types when computing the sum.\nSome languages do not allow you to extend their built in functions and types. This ability is\nknown as external dispatch. In the following example, we illustrate how the user can add symmetric\narrow matrices to the system, and then add a specialized det method to compute the determinant of\na symmetric arrow matrix e\ufb03ciently. We build on the symmetric arrow type introduced in Section\n3.3.\nIn[34]:\n# Define a Symmetric Arrow Matrix Type\nimmutable SymArrow{T} <:\nAbstractMatrix{T}\ndv::Vector{T} # diagonal\nev::Vector{T} # 1st row[2:n]\nend\nIn[35]:\n# Define its size\nimportall Base\nsize(A::SymArrow, dim::Integer) = size(A.dv,1)\nsize(A::SymArrow)= size(A,1), size(A,1)\nOut[35]:\nsize (generic function with 52 methods)\nIn[36]:\n# Index into a SymArrow\nfunction getindex(A::SymArrow,i::Integer,j::Integer)\nif i==j; return A.dv[i]\nelseif i==1; return A.ev[j-1]\nelseif j==1; return A.ev[i-1]\nelse return zero(typeof(A.dv[1]))\nend\nend\nOut[36]:\ngetindex (generic function with 168 methods)\nIn[37]:\n# Dense version of SymArrow\nfull(A::SymArrow) =[A[i,j] for i=1:size(A,1), j=1:size(A,2)]\nOut[37]:\nfull (generic function with 17 methods)\nIn[38]:\n# An example\nS=SymArrow([1,2,3,4,5],[6,7,8,9])\nOut[38]:\n5x5 SymArrow{Int64}:\n1 6 7 8 9\n6 2 0 0 0\n7 0 3 0 0\n8 0 0 4 0\n9 0 0 0 5\n25\nIn[39]:\n# det for SymArrow (external dispatch example)\nfunction exc prod(v) # prod(v)/v[i]\n[prod(v[[1:(i-1),(i+1):end]]) for i=1:size(v,1)]\nend\n# det for SymArrow formula\ndet(A::SymArrow) = prod(A.dv)-sum(A.ev.^2.*exc prod(A.dv[2:end]))\nOut[39]:\ndet (generic function with 17 methods)\nThe above julia code uses the special formula\ndet(A) =\nn\nY\ni=1\ndi \u2212\nn\nX\ni=2\ne2\ni\nY\n2\u2264j\u0338=i\u2264n\ndj,\nvalid for symmetric arrow matrices with diagonal d and \ufb01rst row starting with the second entry e.\nIn some numerical computing languages, a function might begin with a lot of argument checking\nto pick which algorithm to use.\nIn Julia, one creates a number of methods.\nThus newdet on a\ndiagonal is one method for newdet, and newdet on a triangular matrix is a second method. det\non a SymArrow is a new method for det. (See Section 4.6.1.) Code is selected, in advance if the\ncompiler knows the type, otherwise the code is selected at run time. The selection of code is known\nas dispatch.\nWe have seen a number of examples of code selection for single dispatch, i.e., the selection of\ncode based on the type of a single argument. We can now turn to a powerful feature, Julia\u2019s multiple\ndispatch mechanism. Now that we have created a symmetric arrow matrix, we might want to add\nit to all possible matrices of all types. However, we might notice that a symmetric arrow plus a\ndiagonal does not require operations on full dense matrices.\nThe code below starts with the most general case, and then allows for specialization for the\nsymmetric arrow and diagonal sum:\nIn[40]:\n# SymArrow + Any Matrix:\n(Fallback:\nadd full dense arrays )\n+(A::SymArrow, B::Matrix) = full(A)+B\n+(B::Matrix, A::SymArrow) = A+B\n# SymArrow + Diagonal:\n(Special case:\nadd diagonals, copy off-diagonal)\n+(A::SymArrow, B::Diagonal) = SymArrow(A.dv+B.diag,A.ev)\n+(B::Diagonal, A::SymArrow) = A+B\n5\nLeveraging language design for high performance li-\nbraries\nSeemingly innocuous design choices in a language can have profound, pervasive performance im-\nplications. These are often overlooked in languages that were not designed from the beginning to\nbe able to deliver excellent performance. Other aspects of language and library design a\ufb00ect the\nusability, composability, and power of the provided functionality.\n5.1\nInteger arithmetic\nA simple but crucial example of a performance-critical language design choice is integer arithmetic.\nJulia uses machine arithmetic for integer computations.\n26\nFigure 5: Performance comparison of various language performing simple micro-benchmarks. Benchmark\nexecution time relative to C. (Smaller is better, C performance = 1.0).\n27\nConsider what happens if we make the number of loop iterations \ufb01xed:\nIn[41]:\n# 10 Iterations of f(k)=5k-1 on integers\nfunction g(k)\nfor i = 1:10\nk = f(k)\nend\nk\nend\nOut[41]:\ng (generic function with 2 methods)\nIn[42]:\ncode native(g,(Int,))\nOut[42]:\nSource line:\n3\npush RBP\nmov RBP, RSP\nSource line:\n3\nimul RAX, RDI, 9765625\nadd RAX, -2441406\nSource line:\n5\npop RBP\nret\nBecause the compiler knows that integer addition and multiplication are associative and that\nmultiplication distributes over addition it can optimize the entire loop down to just a multiply and\nan add. Indeed, if f(k) = 5k \u22121, it is true that the tenfold iterate f (10)(k) = \u22122441406 + 9765625k.\n5.2\nA powerful approach to linear algebra\nWe describe how the Julia language features have been used to provide a powerful approach to linear\nalgebra[20].\n5.2.1\nMatrix factorizations\nFor decades, orthogonal matrices have been represented internally as products of Householder ma-\ntrices stored in terms of vectors, and displayed for humans as matrix elements. LU factorizations are\noften performed in place, storing the L and U information together in the data locations originally\noccupied by A. All this speaks to the fact that matrix factorizations deserve to be \ufb01rst class objects\nin a linear algebra system.\nIn Julia, thanks to the contributions of Andreas Noack Jensen [20] and many others, these\nstructures are indeed \ufb01rst class objects. The structure QRCompactWY holds a compact Q and an R\nin memory. Similarly an LU holds an L and U in packed form in memory. Through the magic of\nmultiple dispatch, we can solve linear systems, extract the pieces, and do least squares directly on\nthese structures.\nThe QR example is even more fascinating. Suppose one computes QR of a 4 \u00d7 3 matrix. What\nis the size of Q? The right answer, of course, is that it depends: it could be 4 \u00d7 4 or 4 \u00d7 3. The\nunderlying representation is the same.\nIn Julia one can compute Aqr = qrfact(rand(4,3)). Then one extract Q from the factorization\nwith Q=Aqr[:Q]. This Q retains its clever underlying structure and therefore is e\ufb03cient and applicable\nwhen multiplying vectors of length 4 or length 3, contrary to the rules of freshman linear algebra,\n28\nbut welcome in numerical libraries for saving space and faster computations.\nIn[43]:\nA=[1 2 3\n1 2 1\n1 0 1\n1 0 -1]\nAqr = qrfact(A));\nQ = Aqr[:Q]\nOut[43]:\n4x4 QRCompactWYQ{Float64}:\n-0.5\n-0.5\n-0.5\n-0.5\n-0.5\n0.5\n-0.5\n0.5\n-0.5\n-0.5\n0.5\n0.5\nIn[44]:\nQ*[1,0,0,0]\nOut[44]:\n4-element Array{Float64,1}:\n-0.5\n-0.5\n-0.5\n-0.5\nIn[45]:\nQ*[1, 0, 0]\nOut[45]:\n4-element Array{Float64,1}:\n-0.5\n-0.5\n-0.5\n-0.5\n5.2.2\nUser-extensible wrappers for BLAS and LAPACK\nThe tradition in linear algebra is to leave the coding to LAPACK writers, and call LAPACK for\nspeed and accuracy. This has worked fairly well, but Julia exposes considerable opportunities for\nimprovement.\nFirstly, all of LAPACK is available to Julia users, not just the most common functions. All\nLAPACK wrappers are implemented fully in Julia code, using ccall18, which does not require a C\ncompiler, and can be called directly from the interactive Julia prompt. This makes it easy for users\nto contribute LAPACK functionality, and that is how Julia\u2019s LAPACK functionality has grown bit\nby bit. Wrappers for missing LAPACK functionality can also be added by users in their own code.\nConsider the following example that implements the Cholesky factorization by calling LAPACK\u2019s\nxPOTRF. It uses Julia\u2019s metaprogramming facilities to generate four functions, each corresponding to\nthe xPOTRF functions for Float32, Float64, Complex64, and Complex128 types. The actual call to\nthe Fortran functions is wrapped in ccall. Finally, the chol function provides a user-accessible way\n18http://docs.julialang.org/en/latest/manual/calling-c-and-fortran-code/\n29\nto compute the factorization. It is easy to modify the template below for any LAPACK call.\nIn[46]:\n# Generate calls to LAPACK\u2019s Cholesky for double, single, etc.\n# xPOTRF refers to POsitive definite TRiangular Factor\n# LAPACK signature:\nSUBROUTINE DPOTRF( UPLO, N, A, LDA, INFO )\n# LAPACK documentation:\n*\nUPLO\n(input) CHARACTER*1\n*\n= \u2019U\u2019:\nUpper triangle of A is stored;\n*\n= \u2019L\u2019:\nLower triangle of A is stored.\n*\nN\n(input) INTEGER\n*\nThe order of the matrix A.\nN >= 0.\n*\nA\n(input/output) DOUBLE PRECISION array, dimension (LDA,N)\n*\nOn entry, the symmetric matrix A.\nIf UPLO = \u2019U\u2019, the leading\n*\nN-by-N upper triangular part of A contains the upper\n*\ntriangular part of the matrix A, and the strictly lower\n*\ntriangular part of A is not referenced.\nIf UPLO = \u2019L\u2019, the\n*\nleading N-by-N lower triangular part of A contains the lower\n*\ntriangular part of the matrix A, and the strictly upper\n*\ntriangular part of A is not referenced.\n*\nOn exit, if INFO = 0, the factor U or L from the Cholesky\n*\nfactorization A = U**T*U or A = L*L**T.\n*\nLDA\n(input) INTEGER\n*\nThe leading dimension of the array A.\nLDA >= max(1,N).\n*\nINFO\n(output) INTEGER\n*\n= 0:\nsuccessful exit\n*\n< 0:\nif INFO = -i, the i-th argument had an illegal value\n*\n> 0:\nif INFO = i, the leading minor of order i is not\n*\npositive definite, and the factorization could not be\n*\ncompleted.\n# Generate Julia method potrf!\nfor\n(potrf,\nelty) in\n# Run through 4 element types\n((:dpotrf_,:Float64),\n(:spotrf_,:Float32),\n(:zpotrf_,:Complex128),\n(:cpotrf_,:Complex64))\n# Begin function potrf!\n@eval begin\nfunction potrf!(uplo::Char, A::StridedMatrix{$elty})\nlda = max(1,stride(A,2))\nlda==0 && return A, 0\ninfo = Array(Int, 1)\n# Call to LAPACK:ccall(LAPACKroutine,Void,PointerTypes,JuliaVariables)\nccall(($(string(potrf)),:liblapack), Void,\n(Ptr{Char}, Ptr{Int}, Ptr{$elty}, Ptr{Int}, Ptr{Int}),\n&uplo,\n&size(A,1),\nA,\n&lda,\ninfo)\nreturn A, info[1]\nend\nend\nend\nchol(A::Matrix) = potrf!(\u2019U\u2019, copy(A))\n30\n5.3\nHigh Performance Polynomials and Special Functions with Macros\nJulia has a macro system that provides easy custom code generation, bringing a level of performance\nthat is otherwise di\ufb03cult to achieve. A macro is a function that runs at parse-time, and takes parsed\nsymbolic expressions in and returns transformed symbolic expressions out, which are inserted into\nthe code for later compilation.\nFor example, a library developer implemented an @evalpoly macro that uses Horner\u2019s rule to\nevaluate polynomials e\ufb03ciently. Consider\nIn[47]:\n@evalpoly(10,3,4,5,6)\nwhich returns 6543 (the polynomial 3 + 4x + 5x2 + 6x3, evaluated at 10 with Horner\u2019s rule). Julia\nallows us to see the inline generated code with the command\nIn[48]:\nmacroexpand(:@evalpoly(10,3,4,5,6))\nWe reproduce the key lines below\nOut[48]:\n#471#t = 10 # Store 10 into a variable named #471#t\nBase.Math.+(3,Base.Math.*(#471#t,Base.Math.+(4,Base.Math.*\n(#471#t,Base.Math.+(5,Base.Math.*(#471#t,6)))) ))\nThis code-generating macro only needs to produce the correct symbolic structure, and Julia\u2019s\ncompiler handles the remaining details of fast native code generation. Since polynomial evaluation\nis so important for numerical library software it is critical that users can evaluate polynomials as\nfast as possible. The overhead of implementing an explicit loop, accessing coe\ufb03cients in an array,\nand possibly a subroutine call (if it is not inlined), is substantial compared to just inlining the whole\npolynomial evaluation.\nSteven Johnson reports in his EuroSciPy notebook19\nThis is precisely how er\ufb01nv is implemented in Julia (in single and double precision),\nand is 3 to 4 times faster than the compiled (Fortran?) code in Matlab, and 2 to 3 times\nfaster than the compiled (Fortran Cephes) code used in SciPy.\nThe di\ufb00erence (at least in Cephes) seems to be mainly that they have explicit arrays\nof polynomial coe\ufb03cients and call a subroutine for Horner\u2019s rule, versus inlining it via a\nmacro.\nJohnson also used the same trick in his implementation of the digamma special function for\ncomplex arguments20 following an idea of Knuth:\nAs described in Knuth TAOCP vol. 2, sec. 4.6.4, there is actually an algorithm even better\nthan Horner\u2019s rule for evaluating polynomials p(z) at complex arguments (but with real\ncoe\ufb03cients): you can save almost a factor of two for high degrees. It is so complicated\nthat it is basically only usable via code generation, so it would be especially nice to modify\nthe @horner macro to switch to this for complex arguments.\nNo sooner than this was proposed, the macro was rewritten to allow for this case giving a factor of\nfour performance improvement on all real polynomials evaluated at complex arguments.\n5.4\nEasy and \ufb02exible parallelism\nParallel computing remains an important research topic in numerical computing. Parallel computing\nhas yet to reach the level of richness and interactivity required for innovation that has been achieved\nwith sequential tools. The issues discussed in Section 3.1 on the balance between the human and\n19https://github.com/stevengj/Julia-EuroSciPy14/blob/master/Metaprogramming.ipynb\n20https://github.com/JuliaLang/julia/issues/7033\n31\nthe computer become more pronounced in the parallel setting. Part of the problem is that parallel\ncomputing means di\ufb00erent things to di\ufb00erent people:\n1. At the most basic level, one wants instruction level parallelism within a CPU, and expects the\ncompiler to discover such parallelism in the code. In Julia, this can be achieved explicitly with\nthe use of the @simd primitive. Beyond that,\n2. In order to utilize multicore and manycore CPUs on the same node, one wants some kind of\nmulti-threading. Currently, we have experimental multi-threading support in Julia, and this\nwill be the topic of a further paper. Julia currently does provide a SharedArray data structure\nwhere the same array in memory can be operated on by multiple di\ufb00erent Julia processes on\nthe same node.\n3. Then, there is distributed memory, often considered the most di\ufb03cult kind of parallelism. This\ncan mean running Julia on anything between half a dozen to thousands of nodes, each with\nmulticore CPUs.\nIn the fullness of time, there may be a uni\ufb01ed programming model that addresses this hierarchical\nnature of parallelism at di\ufb00erent levels, across di\ufb00erent memory hierarchies.\nOur experience with Star-P [9] taught us a valuable lesson. Star-P parallelism [40, 39] included\nglobal dense, sparse, and cell arrays that were distributed on parallel shared or distributed memory\ncomputers. Before the evolution of the cloud as we know it today, the user used a familiar front\nend (usually Matlab) as the client on a laptop or desktop, and connected seamlessly to a server\n(usually a large distributed computer). Blockbuster functions from sparse and dense linear algebra,\nparallel FFTs, parallel sorting, and many others were easily available and composable for the user.\nIn these cases Star-P called Fortran/MPI or C/MPI. Star-P also allowed a kind of parallel for loop\nthat worked on rows, planes or hyperplanes of an array. In these cases Star-P used copies of the\nclient language on the backend, usually Matlab, octave, python, or R.\nOur experience taught us that while we were able to get a useful parallel computing system\nthis way, bolting parallelism onto an existing language that was not designed for performance or\nparallelism is di\ufb03cult at best, and impossible at worst. One of our (not so secret) motivations to\nbuild Julia was to have the right language for parallel computing.\nJulia provides many facilities for parallelism, which are described in detail in the Julia manual21.\nDistributed memory programming in Julia is built on two primtives - remote calls that execute a\nfunction on a remote processor and remote references that are returned by the remote processor to the\ncaller. These primitives are implemented completely within Julia. On top of these, Julia provides\na distributed array data structure, a pmap implementation, and a way to parallelize independent\niterations of a loop with the @parallel macro - all of which can parallelize code in distributed\nmemory. These ideas are exploratory in nature, and will certainly evolve. We only discuss them\nhere to emphasize that well-designed programming language abstractions and primitives allow one to\nexpress and implement parallelism completely within the language, and explore a number of di\ufb00erent\nparallel programming models with ease. We hope to have a detailed discussion on Juila\u2019s approach\nto parallelism in a future paper.\n21http://docs.julialang.org/en/latest/manual/parallel-computing/\n32\nWe proceed with one example that demonstrates @parallel at work, and how one can impulsively\ngrab a large number of processors and explore their problem space quickly.\nIn[49]:\n@everywhere begin\n# define on every processor\nfunction stochastic(\u03b2=2,n=200)\nh=n^-(1/3)\nx=0:h:10\nN=length(x)\nd=(-2/h^2 .-x) +\n2sqrt(h*\u03b2)*randn(N) # diagonal\ne=ones(N-1)/h^2\n# subdiagonal\neigvals(SymTridiagonal(d,e))[N]\n# smallest negative eigenvalue\nend\nend\nIn[50]:\nt = 10000\nfor \u03b2=[1,2,4,10,20]\nhist([stochastic(\u03b2) for i=1:t], -4:.01:1)[2]\nplot(midpoints(-4:.01:1),z/sum(z)/.01)\nend\nSuppose we wish to perform a complicated histogram in parallel.\nWe use an example from\nRandom Matrix Theory, (but it could easily have been from \ufb01nance), the computation of the scaled\nlargest eigenvalue in magnitude of the so called stochastic Airy operator [15]\nd2\ndx2 \u2212x +\n1\n2\u221a\u03b2 dW.\nThis is just the usual \ufb01nite di\ufb00erence discretization of\nd2\ndx2 \u2212x with a \u201cnoisy\u201d diagonal.\nWe illustrate an example of the famous Tracy-Widom law being simulated with Monte Carlo\nexperiments for di\ufb00erent values of the inverse temperature parameter \u03b2. The code on 1 processor\nis fuzzy and unfocused, as compared to the same simulation on 1024 processors, which is sharp and\nfocused, and runs in exactly the same wall clock time as the sequential run. It is this ability of being\nable to perform scienti\ufb01c computation at the speed of thought conveniently without the traditional\nfuss associated with parallel computing, that we believe will make a new era of scienti\ufb01c discovery\n33\npossible.\nIn[51]:\n# Readily adding 1024 processors sharpens the Monte Carlo simulation in\n# the same time\naddprocs(1024)\nIn[52]:\nt = 10000\nfor \u03b2=[1,2,4,10,20]\nz = @parallel (+) for p=1:nprocs()\nhist([stochastic(\u03b2) for i=1:t], -4:.01:1)[2]\nend\nplot(midpoints(-4:.01:1),z/sum(z)/.01)\nend\nend\n5.5\nPerformance Recap\nIn the early days of high level numerical computing languages, the thinking was that the performance\nof the high level language did not matter so long as most of the time was spent inside the numerical\nlibraries. These libraries consisted of blockbuster algorithms that would be highly tuned, making\ne\ufb03cient use of computer memory, cache, and low level instructions.\nWhat the world learned was that only a few codes were spending a majority of their time in\nthe blockbusters. Real codes were being caught by interpreter overheads, stemming from processing\nmore aspects of a program at run time than are strictly necessary.\nAs we explored in Section 3, one of the hindrances of completing this analysis is type information.\nProgramming language design thus becomes an exercise in balancing incentives to the programmer\nto provide type information and the ability of the computer to infer type information. Vectorization\nis one such incentive system. Existing numerical computing languages would have us believe that\nthis is the only system, or even if there were others, that somehow this was the best system.\nVectorization at the software level can be elegant for some problems. There are many matrix\ncomputation problems that look beautiful vectorized. These programs should be vectorized. Other\nprograms require heroics and skill to vectorize sometimes producing unreadable code all in the name\nof performance. These are the ones that we object to vectorizing. Still other programs can not\nbe vectorized very well even with heroics.\nThe Julia message is to vectorize when it is natural,\nproducing nice code. Do not vectorize in the name of speed.\nSome users believe that vectorization is required to make use of special hardware capabilities\nsuch as SIMD instructions, multithreading, GPU units, and other forms of parallelism. This is not\nstrictly true, as compilers are increasingly able to apply these performance features to explicit loops.\nThe Julia message remains: vectorize when natural, when you feel it is right.\n34\n6\nConclusion and Acknowledgments\nWe built Julia to meet our needs for numerical computing, and it turns out that many others wanted\nexactly the same thing. At the time of writing, not a day goes by where we don\u2019t learn that someone\nelse has picked up Julia at universities and companies around the world, in \ufb01elds as diverse as\nengineering, mathematics, physical and social sciences, \ufb01nance, biotech, and many others. More\nthan just a language, Julia has become a place for programmers, physical scientists, social scientists,\ncomputational scientists, mathematicians, and others to pool their collective knowledge in the form\nof online discussions and in the form of code. Numerical computing is maturing and it is exciting to\nwatch!\nJulia would not have been possible without the enthusiasm and contributions of the Julia com-\nmunity22. We thank Michael La Croix for his beautiful Julia display macros. We are indebted at\nMIT to Jeremy Kepner, Chris Hill, Saman Amarasinghe, Charles Leiserson, Steven Johnson and\nGil Strang for their collegial support which not only allowed for the possibility of an academic re-\nsearch project to update technical computing, but made it more fun too. The authors gratefully\nacknowledge \ufb01nancial support from the MIT Deshpande center for numerical innovation, the Intel\nTechnology Science Center for Big Data, the DARPA Xdata program, the Singapore MIT Alliance,\nNSF Awards CCF-0832997 and DMS-1016125, VMWare Research, a DOE grant with Dr. Andrew\nGelman of Columbia University for petascale hierarchical modeling, grants from Aramco oil thanks\nto Ali Dogru and Shell oil thanks to Alon Arad, and a Citibank grant for High Performance Banking\nData Analysis, and the Gordon and Betty Moore foundation.\nReferences\n[1] Eric Allen, David Chase, Joe Hallett, Victor Luchangco, Jan-Willem Maessen, Sukyoung Ryu,\nGuy L. Steele Jr., and Sam Tobin-Hochstadt. The fortress language speci\ufb01cation version 1.0.\nhttp://research.sun.com/projects/plrg/fortress.pdf, 2008.\n[2] E. Anderson, Z. Bai, C. Bischof, S. Blackford, J. Demmel, J. Dongarra, J. Du Croz, A. Green-\nbaum, S. Hammarling, A. McKenney, and D. Sorensen. LAPACK Users\u2019 Guide. Society for\nIndustrial and Applied Mathematics, Philadelphia, PA, third edition, 1999.\n[3] Je\ufb00Bezanson. Abstraction in Technical Computing. PhD thesis, Massachusetts Institute of\nTechnology, 2015.\n[4] Je\ufb00Bezanson, Jiahao Chen, Stefan Karpinski, Viral B. Shah, and Alan Edelman. Array oper-\nators using multiple dispatch. ARRAY\u201914, 2014.\n[5] Je\ufb00Bezanson, Stefan Karpinski, Viral B. Shah, and Alan Edelman. Julia: a Fast Dynamic\nLanguage for Technical Computing. arXiv:1209.5145v1, 2012.\n[6] B.L. Chamberlain.\nA Brief Overview of Chapel.\nhttp://chapel.cray.com/papers/\nChapelCUG13.pdf, 2013.\n[7] B.L. Chamberlain, D. Callahan, and H.P. Zima.\nParallel programmability and the chapel\nlanguage. Int. J. High Perform. Comput. Appl., 21(3):291\u2013312, August 2007.\n[8] Philippe Charles, Christian Grotho\ufb00, Vijay Saraswat, Christopher Donawa, Allan Kielstra,\nKemal Ebcioglu, Christoph von Praun, and Vivek Sarkar. X10: An object-oriented approach\nto non-uniform cluster computing. SIGPLAN Not., 40(10):519\u2013538, October 2005.\n[9] Ron Choy and Alan Edelman. Parallel MATLAB: Doing it right. In Proceedings of the IEEE,\nvolume 93, pages 331\u2013341, 2005.\n[10] Ron Choy, Alan Edelman, John R. Gilbert, Viral Shah, and David Cheng. Star-P: High pro-\nductivity parallel computing.\nIn In 8th Annual Workshop on High-Performance Embedded\nComputing (HPEC 04), 2004.\n22https://github.com/JuliaLang/julia/graphs/contributors\n35\n[11] Barry A. Cipra. The best of the 20th century: Editors name top 10 algorithms. SIAM News.\nhttps://www.siam.org/pdf/news/637.pdf.\n[12] The Clang project. http://clang.llvm.org/.\n[13] James W. Demmel, Jack J. Dongarra, Beresford N. Parlett, William Kahan, Ming Gu, David S.\nBindel, Yozo Hida, Xiaoye S. Li, Osni A. Marques, E. Jason Riedy, Christof Vomel, Julien\nLangou, Piotr Luszczek, Jakub Kurzak, Alfredo Buttari, Julie Langou, and Stanimire Tomov.\nProspectus for the next LAPACK and ScaLAPACK libraries. Technical Report 181, LAPACK\nWorking Note, February 2007.\n[14] Alan Edelman, Parry Husbands, and Steve Leibman. Interactive supercomputing\u2019s star-p plat-\nform: Parallel matlab and mpi homework classroom study on high level language productivity.\nIn Proceedings of the 10th High Performance Embedded Computing Workshop (HPEC 2006),\n2006.\n[15] Alan Edelman and Brian Sutton. From Random Matrices to Stochastic Operators. Journal of\nStatistical Physics, 127:1121\u20131165, 2007.\n[16] Claude Gomez, editor. Engineering and Scienti\ufb01c Computing With Scilab. Birkh\u00a8auser, 1999.\n[17] Graydon Hoare. technicalities: interactive scienti\ufb01c computing #1 of 2, pythonic parts. http:\n//graydon2.dreamwidth.org/3186.html, 2014.\n[18] Parry Husbands, Charles L. Isbell, Jr., and Alan Edelman. Interactive supercomputing with\nmitmatlab, 1998.\n[19] R. Ihaka and R. Gentleman. R: A language for data analysis and graphics. Journal of Compu-\ntational and Graphical Statistics, 5:299\u2013314, 1996.\n[20] Andreas Noack Jensen. Fast and generic linear algebra in Julia. Technical report, MIT, 2015.\n[21] William Kahan. How futile are mindless assessments of roundo\ufb00in \ufb02oating-point computation?\nhttp://www.cs.berkeley.edu/~wkahan/Mindless.pdf, 2006.\n[22] Marc A. Kaplan and Je\ufb00rey D. Ullman. A scheme for the automatic inference of variable types.\nJournal of the ACM, 27(1):128\u2013145, January 1980.\n[23] Chris Lattner and Vikram Adve. LLVM: A compilation framework for lifelong program analysis\n& transformation. In Proceedings of the 2004 International Symposium on Code Generation and\nOptimization (CGO\u201904), pages 75\u201386, Palo Alto, California, Mar 2004.\n[24] C. L. Lawson, R. J. Hanson, D. R. Kincaid, and F. T. Krogh. Basic linear algebra subprograms\nfor fortran usage. ACM Trans. Math. Softw., 5(3):308\u2013323, September 1979.\n[25] MIT License. http://opensource.org/licenses/MIT.\n[26] Miles Lubin and Iain Dunning. Computing in Operations Research using Julia. INFORMS\nJournal on Computing, 27(2):238\u2013248, 2015.\n[27] Mathematica. http://www.mathematica.com.\n[28] Matlab. http://www.mathworks.com.\n[29] Markus Mohnen.\nA Graph-Free Approach to Data-\ufb02ow Analysis.\nIn R. Horspool, editor,\nCompiler Construction, volume 2304 of Lecture Notes in Computer Science, pages 185\u2013213.\nSpringer Berlin / Heidelberg, 2002.\n[30] Malcolm Murphy. Octave: A Free, High-Level Language for Mathematics. Linux J., 1997, July\n1997.\n[31] Radu Muschevici, Alex Potanin, Ewan Tempero, and James Noble. Multiple dispatch in prac-\ntice. In Proceedings of the 23rd ACM SIGPLAN Conference on Object-oriented Programming\nSystems Languages and Applications, OOPSLA \u201908, pages 563\u2013582, New York, NY, USA, 2008.\nACM.\n[32] The Jupyter Project. http://jupyter.org/.\n[33] The MPFR Project. http://www.mpfr.org/.\n36\n[34] The X10 project. http://x10-lang.org/.\n[35] Rust. http://www.rust-lang.org/.\n[36] Helen Shen. Interactive notebooks: Sharing the code, Nature Toolbox, Volume 515, Issue 7525,\nNov 2014.\nhttp://www.nature.com/news/interactive-notebooks-sharing-the-code-1.\n16261.\n[37] Guy Steele Jr. Parallel programming and code selection in fortress. In \u201dPPoPP \u201906 Proceedings\nof the eleventh ACM SIGPLAN symposium on Principles and practice of parallel programming\u201d,\npage 1, 2006.\n[38] Gilbert Strang. Introduction to Linear Algebra. Wellesley-Cambridge Press, 2003.\n[39] Interactive Supercomputing.\nStar-p user guide.\nhttp://www-math.mit.edu/~edelman/\npublications/star-p-user.pdf.\n[40] Interactive Supercomputing. Getting started with star-p; taking your \ufb01rst test-drive. http:\n//www-math.mit.edu/~edelman/publications.php, 2006.\n[41] Swift. https://developer.apple.com/swift/.\n[42] Madeleine Udell, Karanveer Mohan, David Zeng, Jenny Hong, Steven Diamond, and Stephen\nBoyd. Convex optimization in Julia. SC14 Workshop on High Performance Technical Computing\nin Dynamic Languages, 2014.\n[43] St\u00b4efan van der Walt, S. Chris Colbert, and Ga\u00a8el Varoquaux. The numpy array: a structure for\ne\ufb03cient numerical computation. CoRR, abs/1102.1523, 2011.\n[44] Hadley Wickham. ggplot2. http://ggplot2.org/.\n[45] Leland Wilkinson. The Grammar of Graphics (Statistics and Computing). Springer-Verlag New\nYork, Inc., Secaucus, NJ, USA, 2005.\n37\n",
        "sentence": " Code reproducing all experiments can be found on the Julia (Bezanson et al., 2014) package site https://jgorham.",
        "context": "with each other. This paper demonstrates some of these technologies using a number of examples.\nWe invite the reader to follow along at http://juliabox.org using Jupyter notebooks [32, 36] or\nby downloading Julia http://julialang.org/downloads.\n1.1\nto be used not only to prototype numerical algorithms, but also to deploy those algorithms, and even\nserve results to the rest of the world. A great example of this is Shashi Gowda\u2019s Escher.jl package8,\nan active open source project with over 350 contributors and is available under the MIT License [25]\nfor open source software. Over 1.3 million unique visitors have visited the Julia website since then,"
    },
    {
        "title": "Handbook of Markov chain Monte Carlo",
        "author": [
            "S. Brooks",
            "A. Gelman",
            "G. Jones",
            "Meng",
            "X.-L"
        ],
        "venue": "CRC press,",
        "citeRegEx": "Brooks et al\\.,? \\Q2011\\E",
        "shortCiteRegEx": "Brooks et al\\.",
        "year": 2011,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " When Bayesian inference and maximum likelihood estimation (Geyer, 1991) demand the evaluation of intractable expectations EP [h(Z)] = \u222b p(x)h(x)dx under a target distribution P , Markov chain Monte Carlo (MCMC) methods (Brooks et al., 2011) are often employed to approximate these integrals with asymptotically correct sample averages EQn [h(X)] = 1 n \u2211n i=1 h(xi). Since standard MCMC diagnostics, like mean and trace plots, pooled and within-chain variance measures, effective sample size, and asymptotic variance (Brooks et al., 2011), do not account for asymptotic bias, Gorham & Mackey (2015) defined a new family of sample quality measures \u2013 the Stein discrepancies \u2013 that measure how well EQn approximates EP while avoiding explicit integration under P . budget of 148000 likelihood evaluations, and plotted the median IMQ KSD and effective sample size (ESS, a standard sample quality measure based on asymptotic variance (Brooks et al., 2011)) in Figure 3.",
        "context": null
    },
    {
        "title": "Vector valued reproducing kernel hilbert spaces and universality",
        "author": [
            "C. Carmeli",
            "E. De Vito",
            "A. Toigo",
            "V. Umanit\u00e1"
        ],
        "venue": "Analysis and Applications,",
        "citeRegEx": "Carmeli et al\\.,? \\Q2010\\E",
        "shortCiteRegEx": "Carmeli et al\\.",
        "year": 2010,
        "abstract": "This paper is devoted to the study of vector valued reproducing kernel\nHilbert spaces. We focus on two aspects: vector valued feature maps and\nuniversal kernels. In particular we characterize the structure of translation\ninvariant kernels on abelian groups and we relate it to the universality\nproblem.",
        "full_text": "arXiv:0807.1659v1  [math.FA]  10 Jul 2008\nVector valued reproducing kernel Hilbert\nspaces and universality\nC. Carmeli\u2217, , E. De Vito\u2020, A. Toigo\u2021, V. Umanit`a\u00a7,\nOctober 22, 2018\nAbstract\nThis paper is devoted to the study of vector valued reproducing\nkernel Hilbert spaces. We focus on two aspects: vector valued feature\nmaps and universal kernels. In particular we characterize the structure\nof translation invariant kernels on abelian groups and we relate it to\nthe universality problem.\n1\nIntroduction\nIn learning theory, reproducing kernel Hilbert spaces (RKHS) are an impor-\ntant tool for designing learning algorithms, see for example [8, 29, 31] and\nthe book [9]. In the usual setting the elements of the RKHS are scalar func-\ntions. The mathematical theory for scalar RKHS has been established in the\nseminal paper [1]. For a standard reference see the book [25].\nIn machine learning there is an increasing interest for vector valued learn-\ning algorithms, see [20, 12, 4]. In this framework, the basic object is a Hilbert\nspace of functions f from a set X into a normed vector space Y with the\nproperty that, for any x \u2208X, \u2225f(x)\u2225\u2264Cx \u2225f\u2225for a positive constant Cx\n\u2217C. Carmeli, DIFI, Universit`a di Genova, and I.N.F.N., Sezione di Genova, Via Dode-\ncaneso 33, 16146 Genova, Italy. e-mail: carmeli@ge.infn.it\n\u2020E. De Vito, DSA., Universit`a di Genova, Stradone S. Agostino 37, 16123 Genova,\nItaly, and I.N.F.N., Sezione di Genova, Via Dodecaneso 33, 16146 Genova, Italy. e-mail:\ndevito@dima.unige.it\n\u2021A. Toigo, DISI, Universit`a di Genova, Via Dodecaneso 35, 16146 Genova, and I.N.F.N.,\nSezione di Genova, Via Dodecaneso 33, 16146 Genova, Italy. e-mail: toigo@ge.infn.it\n\u00a7V. Umanit`a, DISI, Universit`a di Genova, Via Dodecaneso 35, 16146 Genova, and\nDipartimento di Matematica \u201cF. Brioschi\u201d, Politecnico di Milano, Piazza Leonardo da\nVinci 32, I-20133 Milano, Italy. e-mail: veronica.umanita@polimi.it\n1\nindependent of f.\nThe theory of vector valued RKHS has been completely worked out in the\nseminal paper [27], devoted to the characterization of the Hilbert spaces that\nare continuously embedded into a locally convex topological vector space, see\n[23]. In the case Y is itself a Hilbert space, the theory can be simpli\ufb01ed as\nshown in [21, 6, 5]. As in the scalar case, a RKHS is completely characterize\nby a map K from X \u00d7X into the space of bounded operators on Y such that\nN\nX\ni,j=1\n\u27e8K(xi, xj)yj, yi\u27e9\u22650\nfor any x1, . . . , xN in X and y1, . . . , yN in Y. Such a map is called a Y-\nreproducing kernel and the corresponding RKHS is denoted by HK.\nThis paper focuses on three aspects of particular interest in vector valued\nlearning problems:\n\u2022 vector valued feature maps;\n\u2022 universal reproducing kernels;\n\u2022 translation invariant reproducing kernels.\nThe feature map approach is the standard way in which scalar RKHS are pre-\nsented in learning theory, see for example [26]. A feature map is a function\nmapping the input space X into an arbitrary Hilbert space H in such a way\nthat H can be identi\ufb01ed with a unique RKHS. Conversely, any RKHS can be\nrealized as a closed subspace of a concrete Hilbert space, called feature space,\nby means of a suitable feature map \u2013 typical examples of feature spaces are\n\u21132 and L2(X, \u00b5) for some measure \u00b5.\nThe concept of feature map is extended to the vector valued setting in [6, 5],\nwhere a feature map is de\ufb01ned as a function from X into the space of bounded\noperators between Y and the feature space H.\nIn the \ufb01rst part of our paper, Section 3 shows that sum, product and compo-\nsition with maps of RKHS can be easily described by suitable feature maps.\nIn particular we give an elementary proof of Schur lemma about the product\nof a scalar kernel with a vector valued kernel. Moreover, we present several\nexamples of vector valued RKHS, most of them considered in [22, 5]. For\neach one of them we exhibit a nice feature space. This allows to describe the\nimpact of these examples on some learning algorithms, like the regularized\nleast-squares [13].\nIn the second part of the paper, Section 4 discusses the problem of char-\nacterizing universal kernels. We say that a Y-reproducing kernel is universal\n2\nif the corresponding RKHS HK is dense in L2(X, \u00b5; Y) for any probability\nmeasure \u00b5 on the input space X. This de\ufb01nition is motivated observing that\nin learning theory the goal is to approximate a target function f \u2217by means\nof a prediction function fn \u2208HK, depending on the data, in such a way\nthe distance between f \u2217and fn goes to zero when the number of data n\ngoes to in\ufb01nity. In learning theory the \u201cright\u201d distance is given by the norm\nin L2(X, \u00b5; Y), where \u00b5 is the (unknown) probability distribution modeling\nthe sample of the input data, see [8]. The possibility of learning any target\nfunction f \u2217by means of functions in HK is precisely the density of HK in\nL2(X, \u00b5; Y). Since the probability measure \u00b5 is unknown, we require that\nthe above property holds for any choice of \u00b5 \u2013 compare with the de\ufb01nition of\nuniversal consistency for a learning algorithm [18]. Under the condition that\nthe elements of HK are continuous functions vanishing at in\ufb01nity, we prove\nthat universality of HK is equivalent to require that HK is dense in C0(X; Y),\nthe Banach space of continuous functions vanishing at in\ufb01nity with the uni-\nform norm. If X is compact and H = C, the density of HK in C0(X; Y) is\nprecisely the de\ufb01nition of universality given in [30, 32]. For arbitrary X and\nY, another de\ufb01nition of universality is suggested in [5] under the assumption\nthat the elements of HK are continuous functions. We show that this last\nnotion is equivalent to require that HK is dense in L2(X, \u00b5; Y) for any prob-\nability measure \u00b5 with compact support, or that HK is dense in C(X; Y),\nthe space of continuous functions with the compact-open topology. If X is\nnot compact, the two de\ufb01nitions of universality are not equivalent, as we\nshow in two examples. To avoid confusion we refer to the second notion as\ncompact-universality.\nWe characterize both universality and compact-universality in terms of the\ninjectivity of the integral operator on L2(X, \u00b5; Y) whose kernel is the repro-\nducing kernel K. For compact-universal kernels, this result is presented in a\nslightly di\ufb00erent form in [5] \u2013 compare Theorem 2 below with Theorem 11 of\n[5]. However, our statement of the theorem does not require a direct use of\nvector valued measures, our proof is simpler and it is based on the fact that\nany bounded linear functional T on C0(X; Y) is of the form\nT(f) =\nZ\nX\n\u27e8f(x), h(x)\u27e9d\u00b5(x),\nwhere \u00b5 is a probability measure and h is a bounded measurable function\nfrom X to Y \u2013 see Appendix A.\nNotice that, though in learning theory\nthe main issue is the density of the RKHS HK in L2(X, \u00b5; Y), however, our\nresults hold if, in the de\ufb01nition of universal kernels, we replace L2(X, \u00b5; Y)\nwith Lp(X, \u00b5; Y) for any 1 \u2264p < \u221e. In particular, we show that HK is\ndense in C0(X; Y) if and only if there exists 1 \u2264p < \u221esuch that HK is\n3\ndense in Lp(X, \u00b5; Y) for any probability measure \u00b5.\nIn that case, HK is\ndense in Lq(X, \u00b5; Y) for any 1 \u2264q < \u221e.\nIn the third part of the paper, under the assumption that X is a group,\nSection 5 studies translation invariant reproducing kernels, that is, the kernels\nsuch that K(x, t) = Ke(t\u22121x) for some operator valued function Ke : X \u2192\nL(Y) of completely positive type. In particular, we show that any translation\ninvariant kernel is of the form\nK(x, t) = A\u03c0x\u22121tA\u2217\nfor some unitary representation \u03c0 of X acting on a Hilbert space H, and a\nbounded operator A : H \u2192Y. If X is an abelian group, SNAG theorem [16]\nprovides a more explicit description of the reproducing kernel K, namely\nK(x, t) =\nZ\n\u02c6\nX\n\u03c7(t \u2212x)dQ(\u03c7),\nwhere \u02c6X is the dual group and Q is a positive operator valued measure\non \u02c6X. The above equation is precisely the content of Bochner theorem for\noperator valued functions of positive type [2, 15]. In particular, we show that\nthe corresponding RKHS HK can be always realized as a closed subspace of\nL2( \u02c6X, \u02c6\u03bd, Y) where \u02c6\u03bd is a suitable positive measure on \u02c6X. In this setting,\nwe give a su\ufb03cient condition ensuring that a translation invariant kernel is\nuniversal. This condition is also necessary if X is compact or Y = C. For\nscalar kernels and compact-universality this result is given in [22]. We end\nthe paper by discussing in Section 6 the universality of some of the examples\nintroduced in Section 3.\n2\nBackground\nIn this section we set the main notations and we recall some basic facts about\nvector valued reproducing kernels.\n2.1\nNotations and assumptions\nIn the following we \ufb01x a locally compact second countable topological space\nX and a complex separable Hilbert space Y, whose norm and scalar product\nare denoted by \u2225\u00b7\u2225and \u27e8\u00b7, \u00b7\u27e9respectively. Local compactness of X is needed in\norder to prove Theorem 7 in the appendix, which is at the root of Theorem 1.\nThe separability of X and Y will avoid some problems in measure theory.\nAll these assumptions are always satis\ufb01ed in learning theory.\n4\nWe denote by F(X; Y) the vector space of functions f : X \u2192Y, by\nC(X; Y) the subspace of continuous functions, and by C0(X; Y) the subspace\nof continuous functions vanishing at in\ufb01nity.\nIf Y = C, we set C(X) =\nC(X; C) and C0(X) = C0(X, C). If X is compact, C0(X; Y) = C(X; Y).\nWe regard C(X; Y) as a locally convex topological vector space by endowing\nit with the compact-open topology1 and C0(X; Y) as a Banach space with\nrespect to the uniform norm \u2225f\u2225\u221e= maxx\u2208X \u2225f(x)\u2225.\nLet B(X) be the Borel \u03c3-algebra of X. By a measure on X we mean\na \u03c3-additive map \u00b5 : B(X) \u2212\u2192[0, +\u221e] which is \ufb01nite on compact sets2.\nWe say that \u00b5 is a probability measure if \u00b5(X) = 1.\nFor 1 \u2264p < \u221e,\nLp(X, \u00b5; Y) denotes the Banach space of (equivalence classes of) measur-\nable3 functions f : X \u2192Y such that \u2225f\u2225p is \u00b5-integrable, with norm\n\u2225f\u2225p =\n\u0000R\nX \u2225f(x)\u2225p d\u00b5(x)\n\u00011/p.\nIf p = 2 we denote the scalar product in\nL2(X, \u00b5, Y) by \u27e8\u00b7, \u00b7\u27e92. For p = \u221e, L\u221e(X, \u00b5; Y) is the Banach space of \u00b5-\nessentially bounded measurable functions f : X \u2192Y with norm \u2225f\u2225\u00b5,\u221e=\n\u00b5\u2212ess supx\u2208X \u2225f(x)\u2225.\nIf \u00b5 is a probability measure, clearly\nC0(X; Y) \u2282Lp(X, \u00b5; Y) \u2282Lq(X, \u00b5; Y)\nfor all 1 \u2264q < p \u2264\u221e, each inclusion being continuous. Moreover, since X\nis locally compact and second countable, C0(X; Y) is dense in Lp(X, \u00b5; Y) for\nany 1 \u2264p < \u221e.\nIf H is an arbitrary (complex) Hilbert space we denote its scalar product\nby \u27e8\u00b7, \u00b7\u27e9H and its norm by \u2225\u00b7\u2225H. When H\u2032 is another Hilbert space, we denote\nby L(H; H\u2032) the Banach space of bounded operators from H to H\u2032 endowed\nwith the uniform norm. In the case H = H\u2032, we set L(H) = L(H; H).\nGiven w1, w2 \u2208H, we let w1 \u2297w2 be the rank one operator\n(w1 \u2297w2)v = \u27e8v, w2\u27e9H w1\nv \u2208H.\n2.2\nVector valued reproducing kernels\nWe brie\ufb02y recall the main properties of vector valued reproducing kernel\nHilbert spaces. Given X and Y as above, a map K : X \u00d7 X \u2212\u2192L(Y) is\n1This is the topology of uniform convergence on compact subsets de\ufb01ned by the family\nof seminorms \u2225f\u2225Z = maxx\u2208Z \u2225f(x)\u2225for Z varying over the compact subsets in X.\n2Since X is locally compact second countable, then \u00b5 is both inner and outer regular.\n3Since Y is separable, measurability is equivalent to the fact that \u27e8f(\u00b7), y\u27e9is measurable\nfor all y \u2208Y.\n5\ncalled a Y-reproducing kernel if\nN\nX\ni,j=1\n\u27e8K(xi, xj)yj, yi\u27e9\u22650\nfor any x1, . . . , xN in X, y1, . . . , yN in Y and N \u22651. Given x \u2208X, Kx : Y \u2192\nF(X; Y) denotes the linear operator whose action on a vector y \u2208Y is the\nfunction Kxy \u2208F(X; Y) de\ufb01ned by\n(Kxy)(t) = K(t, x)y\nt \u2208X.\n(1)\nGiven a Y-reproducing kernel K, there is a unique Hilbert space HK \u2282F(X; Y)\nsatisfying\nKx \u2208L(Y, HK)\nx \u2208X\n(2)\nf(x) = K\u2217\nxf\nx \u2208X, f \u2208HK,\n(3)\nwhere K\u2217\nx : HK \u2192Y is the adjoint of Kx, see Proposition 2.1 of [6]. The\nspace HK is called the reproducing kernel Hilbert space associated with K,\nthe corresponding scalar product and norm are denoted by \u27e8\u00b7, \u00b7\u27e9K and \u2225\u00b7\u2225K,\nrespectively. As a consequence of (3), we have that\nK(x, t) = K\u2217\nxKt\nx, t \u2208X\nHK = span {Kxy | x \u2208X, y \u2208Y} .\nAs discussed in the introduction, the space HK can be realized as a closed\nsubspace of some arbitrary Hilbert space by means of a suitable feature map,\nas shown by the next result, see Proposition 2.4 of [6].\nProposition 1. Let H be a Hilbert space and \u03b3 : X \u2212\u2192B(Y; H). Then the\noperator W : H \u2212\u2192F(X; Y) de\ufb01ned by\n(Wu)(x) = \u03b3\u2217\nxu,\nu \u2208H, x \u2208X,\n(4)\nis a partial isometry from H onto the reproducing kernel Hilbert space HK\nwith reproducing kernel\nK(x, t) = \u03b3\u2217\nx\u03b3t,\nx, t \u2208X.\n(5)\nMoreover, W \u2217W is the orthogonal projection onto\nker W \u22a5= span {\u03b3xy | x \u2208X, y \u2208Y} ,\nand\n\u2225f\u2225K = inf{\u2225u\u2225H | u \u2208H, Wu = f}.\n6\nThe map \u03b3 is usually called the feature map, W the feature operator and\nH the feature space. Since W is an isometry from ker W \u22a5onto HK, the map\nW allows us to identify HK with the closed subspace ker W \u22a5of H. With a\nmild abuse of notation, we say that HK is embedded into H by means of the\nfeature operator W.\nComparing (4) with (3), we notice that any RKHS HK admits a trivial feature\nmap, namely \u03b3x = Kx.\nIn this case the feature operator is the identity.\nConversely, if H is a Hilbert space of functions from X to Y such that \u2225f\u2225\u2264\nCx \u2225f\u2225H for some positive constant Cx, then there exists a bounded operator\n\u03b3x : Y \u2192H such that f(x) = \u03b3\u2217\nxf. Hence, the above proposition implies\nthat H is a RKHS with kernel given by (5) and that the feature operator is\nthe identity.\n2.3\nMercer and C0-kernels\nIn this paper, we mainly focus on reproducing kernel Hilbert spaces, whose\nelements are continuous functions. In particular we study the following two\nclasses of reproducing kernels.\nDe\ufb01nition 1. A reproducing kernel K : X \u00d7 X \u2192L(Y) is called\n(i) Mercer provided that HK is a subspace of C(X; Y);\n(ii) C0 provided that HK is a subspace of C0(X; Y).\nThe choice of C(X; Y) and C0(X; Y) is motivated in Section 4 where we\ndiscuss the universality problem.\nThe following proposition directly characterizes Mercer and C0-kernels in\nterms of properties of the kernels.\nProposition 2. Let K be a reproducing kernel.\n(i) The kernel K is Mercer i\ufb00the function x 7\u2212\u2192\u2225K(x, x)\u2225is locally\nbounded and Kxy \u2208C(X; Y) for all x \u2208X and y \u2208Y.\n(ii) The kernel K is C0 i\ufb00the function x 7\u2212\u2192\u2225K(x, x)\u2225is bounded and\nKxy \u2208C0(X; Y) for all x \u2208X and y \u2208Y.\nIf K is a Mercer kernel, the inclusion HK \u0592\u2192C(X; Y) is continuous. If K is\na C0-kernel the inclusion HK \u0592\u2192C0(X; Y) is continuous. In both cases, the\nspace HK is separable.\n7\nProof. We prove only (ii), since the other proof is similar \u2013 see Proposition 5.1\nof [6]. If HK \u2282C0(X; Y), it is clear that Kxy is an element of C0(X; Y).\nMoreover, since \u2225K\u2217\nxf\u2225= \u2225f(x)\u2225\u2264\u2225f\u2225\u221e\u2200f \u2208HK, by the principle of\nuniform boundedness there exists M < \u221esuch that \u2225K\u2217\nx\u2225\u2264M for all x.\nTherefore, \u2225K(x, x)\u2225= \u2225K\u2217\nx\u22252 \u2264M2 for all x.\nConversely, assume that the function x 7\u2212\u2192\u2225K(x, x)\u2225is bounded and Kxy \u2208\nC0(X; Y). Given f \u2208HK, we have\n\u2225f(x)\u2225\u2264\u2225f\u2225K \u2225K(x, x)\u22251/2 \u2264M \u2225f\u2225K .\nIn particular, convergence in HK implies uniform convergence, so that the\nclosure (in HK) of the linear span of {Kxy | x \u2208X, y \u2208Y} is contained in\nC0(X; Y), i.e. HK \u2286C0(X; Y).\nThe continuity of the inclusion of HK in C0(X; Y) follows from \u2225f\u2225\u221e\u2264\nM \u2225f\u2225HK. Finally HK is separable by Corollary 5.2 of [6].\nIf HK is de\ufb01ned by means of a feature map \u03b3, the above characterization\ncan be expressed in terms of \u03b3, as shown by the following result.\nCorollary 1. With the notations of Proposition 1 the following conditions\nare equivalent.\n(a) The kernel K is Mercer [resp. C0].\n(b) There is a total set S in H such that W(S) \u2282C(X; Y) [resp. W(S) \u2282\nC0(X; Y)] and the function x 7\u2212\u2192\u2225\u03b3x\u2225is locally bounded [resp. bounded].\nProof. We give the proof only in the case of a C0-kernel, the other case being\nsimpler. Suppose hence (a) holds true, i.e. HK \u2282C0, then W(S) \u2282ran W =\nHK \u2282C0(X; Y) for all subset S of H. Moreover, \u2225\u03b3x\u22252 = \u2225K(x, x)\u2225\u2264M by\nitem (ii) of Proposition 2. Conversely, if condition (b) holds, we have that\nfor all x \u2208X and u \u2208H\n\u2225(Wu)(x)\u2225= \u2225K\u2217\nx(Wu)\u2225\u2264\u2225K\u2217\nx\u2225\u2225W\u2225\u2225u\u2225H \u2264\u2225K(x, x)\u2225\n1\n2 \u2225u\u2225H \u2264M\n1\n2 \u2225u\u2225H ,\nwhere \u2225W\u2225\u22641 being W a partial isometry. Then W maps H into the space\nof bounded functions and W is continuous from H onto HK endowed with\nthe uniform norm. Since W(S) \u2282C0(X; Y) and C0(X; Y) is complete, then\nHK \u2282C0(X; Y).\n8\n2.4\nMercer theorem\nFor a Mercer kernel K, there is a canonical feature map, based on Mercer\ntheorem, which relates the spectral properties of the integral operator with\nkernel K, and the structure of the corresponding reproducing kernel Hilbert\nspace. This result will be also used in the examples.\nTo state this result for vector valued reproducing kernels, we need some\npreliminary facts. First of all, if K is a Mercer kernel and \u00b5 is a probability\nmeasure on X, the space HK is a subspace of L2(X, \u00b5; Y), provided that\n\u2225K(x, x)\u2225is bounded on the support of \u00b5.\nThis last condition is always\nsatis\ufb01ed if K is a C0-kernel or if \u00b5 has compact support. If HK is a subspace\nof L2(X, \u00b5; Y), we denote the canonical inclusion by\ni\u00b5 : HK \u0592\u2192L2(X, \u00b5; Y).\nNext lemma states some properties of i\u00b5 and its proof is a consequence of\nPropositions 3.3, 4.4 and 4.8 of [6].\nProposition 3. Let K be a Mercer kernel and \u00b5 a probability measure such\nthat K is bounded on the support of \u00b5. The inclusion i\u00b5 is a bounded operator,\nits adjoint i\u2217\n\u00b5 : L2(X, \u00b5; Y) \u2212\u2192HK is given by\n(i\u2217\n\u00b5f)(x) =\nZ\nX\nK(x, t)f(t)d\u00b5(t),\nwhere the integral converges in norm, and the composition i\u00b5i\u2217\n\u00b5 = L\u00b5 is the\nintegral operator on L2(X, \u00b5; Y) with kernel K\n(L\u00b5f)(x) =\nZ\nX\nK(x, t)f(t)d\u00b5(t).\nIn particular, if K(x, x) is a compact operator for all x \u2208X, then LK is a\ncompact operator.\nThe fact that LK is a compact operator implies that there is a family\n(fi)i\u2208I of eigenvectors in C(X; Y) and a family (\u03c3i)i\u2208I of eigenvalues in ]0, \u221e[\nsuch that (fi)i\u2208I is an orthonormal basis of ker L\u00b5\n\u22a5= ran L\u00b5 and\nL\u00b5fi = \u03c3ifi.\n(6)\nWith this notation we are ready to state Mercer Theorem for vector valued\nkernels. Its proof is consequence of Proposition 6.1 and Theorem 6.3 of [6].\n9\nProposition 4. Let \u00b5 be a probability measure with supp \u00b5 = X. Suppose\nK is a Mercer kernel such that supx\u2208X \u2225K(x, x)\u2225< \u221e, and K(x, x) is a\ncompact operator \u2200x \u2208X. With the notation of (6), we have that\nHK = {f \u2208C(X; Y) \u2229ker L\u00b5\n\u22a5|\nX\ni\u2208I\n| \u27e8f, fi\u27e92 |2\n\u03c3i\n< \u221e}\n(7)\n\u27e8f, g\u27e9K =\nX\ni\u2208I\n\u27e8f, fi\u27e92 \u27e8fi, g\u27e92\n\u03c3i\n(8)\nK(x, t) =\nX\ni\u2208I\n\u03c3ifi(x) \u2297fi(t)\n(9)\nwhere the last series converges in the strong operator topology of L(Y).\nEquations (7) and (8) imply that (\u221a\u03c3ifi)i\u2208I is an orthonormal basis in\nHK. In particular the vectors \u221a\u03c3ifi are \u21132-linearly independent in F(X; Y),\nnamely, if (ci)i\u2208I is a family such that P\ni\u2208I |ci|2 < \u221eand P\ni\u2208I ci\n\u221a\u03c3ifi(x) =\n0 for all x \u2208X, then ci = 0 for all i \u2208I.\nAs said at the beginning of Section 2.4, Proposition 4 gives a feature\noperator, which is often used in learning theory.\nExample 1. With the assumptions and notations of Proposition 4, the re-\nproducing kernel Hilbert space HK is unitarily equivalent to ker L\u00b5\n\u22a5= ran L\u00b5\nby means of the feature operator\n(Wf)(x) =\nX\ni\u2208I\n\u221a\u03c3ifi(x) \u27e8f, fi\u27e92 = (L\n1\n2\u00b5f)(x) ,\nf \u2208L2(X, \u00b5; Y) .\n(10)\nProof. Given x \u2208X, de\ufb01ne\n\u03b3x : Y \u2192L2(X, \u00b5; Y)\n\u03b3xy =\nX\ni\u2208I\n\u221a\u03c3i \u27e8y, fi(x)\u27e9fi,\nwhich is well de\ufb01ned since (fi)i\u2208I is orthonormal family of continuous func-\ntions and (9) ensures that P\ni\u2208I \u03c3i| \u27e8y, fi(x)\u27e9|2 < \u221e. Using (9) again, one\nchecks that \u03b3\u2217\nx\u03b3t = K(x, t). The fact that feature operator is given by (10)\nis clear by de\ufb01nition of \u03b3x. Since ker W = ker L\u00b5, W is a unitary operator\nfrom ker L\u00b5\n\u22a5onto HK.\n2.5\nTrivial examples\nWe give two examples of trivial vector valued kernels.\n10\nExample 2. Let B \u2208L(Y) be a positive operator and de\ufb01ne K(x, t) = B for\nall x, t \u2208X, then K is a Y-reproducing kernel, HK is unitarily equivalent to\nker B\u22a5= ran B by means of the feature operator\n(Wy)(x) = B\n1\n2y\nx \u2208X, y \u2208ker B\u22a5.\nThe kernel K is of Mercer type and it is a C0-kernel if and only if X is\ncompact.\nProof. Apply Proposition 1 with H = ker B\u22a5and \u03b3x = B\n1\n2.\nSince B is\ninjective on H, then W is unitary.\nThe claims about the continuity are\nclear.\nExample 3. Let f : X \u2192Y, f \u0338= 0. De\ufb01ne K(x, t) = f(x) \u2297f(t), then\nK is a reproducing kernel, HK is unitarily equivalent to C by means of the\nfeature operator\n(Wc)(x) = cf(x)\nx \u2208X, c \u2208C.\nIn particular K is Mercer [resp.C0] if and only if f \u2208C(X; Y) [resp. f \u2208\nC0(X; Y)].\nProof. Apply Proposition 1 with H = C and \u03b3xy = \u27e8y, f(x)\u27e9. Since f \u0338= 0,\nW is injective. The characterization about Mercer and C0 is trivial.\n3\nOperations with kernels\nIn this section we characterize reproducing kernel Hilbert spaces whose kernel\nis de\ufb01ned by algebraic operations, like sum, product and composition. Most\nof the results are well known for scalar kernels, whereas for vector valued ker-\nnels they are consequences of the theory developed in [27] in a more general\ncontext. We provide a direct and simple proof of these results, based on the\nuse of suitable feature maps. In some cases, our approach can be of interest\nalso in the scalar case, like, for example, in proving Schur lemma about the\nproduct of kernels.\nAs an application, we present a large supply of examples of vector valued re-\nproducing kernels and, for most of them, we realize the corresponding RKHS\nby elegant and simple structures. This characterization will be used to an-\nalyze some learning algorithm, like regularized least-squares, in the vector\nvalued setting.\n11\n3.1\nSum of kernels\nThe following result extends to vector valued kernels the relation between\nsum of kernels and sum of the corresponding reproducing kernel Hilbert\nspaces.\nProposition 5. Denote by I a countable set and let (Ki)i\u2208I be a family of\nY-reproducing kernels such that\nX\ni\u2208I\n\nKi(x, x)y, y\n\u000b\n< \u221e\n\u2200y \u2208Y and \u2200x \u2208X.\nGiven x, t \u2208X the series P\ni\u2208I Ki(x, t) converges to a bounded operator\nK(x, t) in the strong operator topology, and the map K : X \u00d7 X \u2192L(Y)\nde\ufb01ned by\nK(x, t)y =\nX\ni\u2208I\nKi(x, t)y\nis a Y-reproducing kernel.\nThe corresponding space HK is embedded in\nL\ni\u2208I HKi by means of the feature operator\nW(f)(x) =\nX\ni\u2208I\nfi(x)\nwhere f = \u2295i\u2208Ifi\nwhere the sum converges in norm.\nMoreover, if each Ki is a Mercer kernel [resp. C0-kernel] and x 7\u2192P\ni\u2208I \u2225Ki(x, x)\u2225\nis locally bounded [resp. bounded], then K is Mercer [resp. C0].\nProof. We apply Proposition 1. Letting H = L\ni\u2208I HKi, we regard each HKi\nas a closed subspace of H so that any two of them are orthogonal. Given\nx \u2208X, we de\ufb01ne the bounded operator \u03b3x : Y \u2192H by \u03b3x = P\ni\u2208I Ki\nx, where\nthe series converges in the strong operator topology since, given y \u2208Y,\nX\ni\u2208I\n\r\rKi\nxy\n\r\r2\nKi =\nX\ni\u2208I\n\nKi(x, x)y, y\n\u000b\n< \u221e\nby assumption, see [7]. Given i \u2208I and fi \u2208HKi, then\n\u27e8\u03b3\u2217\nxfi, y\u27e9=\n\nfi, Ki\nxy\n\u000b\nKi = \u27e8fi(x), y\u27e9\nby reproducing property (3), so that \u03b3\u2217\nxfi = fi(x). Since \u03b3\u2217\nx is continuous, for\nany f = \u2295i\u2208Ifi,\n(Wf)(x) = \u03b3\u2217\nxf =\nX\ni\u2208I\n\u03b3\u2217\nxfi =\nX\ni\u2208I\nfi(x)\n12\nwhere the series converges in norm.\nFinally, K(x, t)y = \u03b3\u2217\nx\u03b3ty = P\ni\u2208I(\u03b3ty)i(x) = P\ni\u2208I Ki(x, t)y, that is K(x, t) =\nP\ni\u2208I Ki(x, t) in the strong operator topology.\nThe second part is a consequence of Corollary 1 with S = S\ni\u2208I HKi.\nAs an application, we have the following example.\nExample 4. Let (fi)\u2208I a countable family of functions fi : X \u2192Y such that\nP\ni\u2208I | \u27e8fi(x), y\u27e9|2 is \ufb01nite for all x \u2208X and y \u2208Y . De\ufb01ne K : X \u00d7 X \u2192\nL(Y) as\nK(x, t) =\nX\ni\u2208I\nfi(x) \u2297fi(t) .\nThen, the sum converges in the strong operator topology, K is a reproducing\nkernel and\nHK = {f \u2208F(X; Y) | f(x) =\nX\ni\u2208I\ncifi(x),\nX\ni\u2208I\n|ci|2 < \u221e}.\n(11)\nIn particular (fi)i\u2208I is a normalized tight frame in HK. It is an orthonormal\nbasis if and only if (fi)i\u2208I is \u21132-linearly independent in F(X; Y).\nProof. Apply Proposition 5, with Ki(x, t) = fi(x) \u2297fi(t), observing that\nHKi = C by Example 3, so that \u2295i\u2208IHKi \u2243\u21132. The feature operator is\nexplicitly given by\nW(c)(x) =\nX\ni\u2208I\ncifi(x)\nwhere c = (ci)i\u2208I,\nX\ni\u2208I\n|ci|2 < \u221e,\nso that (11) is clear. If (ei)i\u2208I is the canonical orthonormal basis of \u21132, then\nWei = fi and, for any f \u2208HK,\n\u2225f\u22252\nK = \u2225W \u2217f\u22252\n\u21132 =\nX\ni\n| \u27e8W \u2217f, ei\u27e9\u21132 |2 =\nX\ni\n| \u27e8f, fi\u27e9K |2 ,\ni.e. (fi)i\u2208I is a normalized tight frame in HK. Clearly, it is an orthonormal\nbasis if and only if W is unitary, i.e. W is injective. This is precisely the\ncondition that (fi)i\u2208I is \u21132-linearly independent in F(X; Y).\nProposition 4 shows that any RKHS with a bounded compact Mercer\nkernel can be realized as in the above example, where the functions fi are the\neigenfunctions (with \u2225fi\u22252\n2 = \u03c3i) of the integral operator L\u00b5 with eigenvalues\n\u03c3i > 0, and \u00b5 is any probability measure with supp \u00b5 = X, see (6).\n13\n3.2\nComposition with maps\nWe now describe the reproducing kernel Hilbert spaces whose kernel is de\ufb01ned\nin terms of a mother kernel and suitable maps acting either on the input space\nX or on the output space Y. The following result characterizes the action of\na bounded operator on Y.\nProposition 6. Let K be a Y-reproducing kernel. Let Y\u2032 be another Hilbert\nspace and w : Y \u2192Y\u2032 be a bounded operator. De\ufb01ne\nKw : X \u00d7 X \u2192L(Y\u2032)\nKw(x, t) = wK(x, t)w\u2217,\nthen Kw is a Y\u2032 reproducing kernel and HKw is embedded in HK by means of\nW : HK \u2212\u2192HKw ,\n(Wf)(x) = wf(x)\nx \u2208X.\nIf w is injective, HKw is unitarily equivalent to HK. Moreover, if K is Mercer\n[resp. C0], then Kw is Mercer [resp. C0].\nProof. Let \u03b3x : Y\u2032 \u2192HK, \u03b3x = Kxw\u2217and apply Proposition 1 with H = HK.\nThe feature operator from HK onto HKw is explicitly given by (Wf)(x) =\n\u03b3\u2217\nxf = wf(x). If w is injective, then W is unitary. The second claim is\nevident.\nWe now study the action of an arbitrary map on X.\nProposition 7. Let K be a Y-reproducing kernel on X. Let T be another\nlocally compact second countable topological space, and \u03a8 : T \u2192X. De\ufb01ne\nK\u03a8 : T \u00d7 T \u2192L(Y)\nK\u03a8(t1, t2) = K(\u03a8(t1), \u03a8(t2))\nt1, t2 \u2208T.\nThen K\u03a8 is a Y-reproducing kernel on T, the space HK\u03a8 is unitarily equiv-\nalent to\nspan {Kxy | x \u2208ran \u03a8} = {f \u2208HK | f(x) = 0 \u2200x \u2208ran \u03a8}\u22a5\nby means of the feature operator\nW : HK \u2212\u2192HK\u03a8\nW(f)(t) = f(\u03a8(t))\nf \u2208HK, t \u2208T.\nIf K is a Mercer kernel and \u03a8 is continuous, then K\u03a8 is Mercer. If K is a\nC0-kernel and \u03a8 is continuous and proper, then K\u03a8 is C0.\nProof. Apply Proposition 1 with H = HK and, for any t \u2208T, \u03b3t = K\u03a8(t),\nobserving that ker W = {f \u2208HK | f(x) = 0 \u2200x \u2208ran \u03a8}.\nThe claims about Mercer and C0-kernels are clear.\n14\nIn the above proposition observe that ker W \u22a5can be identi\ufb01ed with the\nquotient space HK/ ker W, so that one has also the natural identi\ufb01cation\nHK\u03a8 \u2243{f|ran \u03a8 | f \u2208HK}\n(12)\nwhere, the r.h.s. is endowed with the norm\n\r\rf|ran \u03a8\n\r\r = inf{\u2225g\u2225K | g \u2208HK, g|ran \u03a8 = f|ran \u03a8}\nAs a consequence, we describe the relation between a kernel and its re-\nstriction to a subset.\nCorollary 2. Let X0 be a subset of X. Let KX0 be the restriction of K to\nX0 \u00d7 X0, then\nHKX0 = {f|X0 : f \u2208HK}.\nIf K is Mercer and X0 is locally closed, then KX0 is Mercer. If K is C0 and\nX0 is closed, then KX0 is C0.\nProof. Apply Proposition 7 and identi\ufb01cation (12), with \u03a8 the canonical\ninclusion of X0 in X.\nWe end this part by describing the reproducing kernel Hilbert space as-\nsociated with the kernel proposed in [5].\nProposition 8. Let \u03ba be a scalar reproducing kernel on X. Let T be an-\nother locally compact second countable topological space. Let \u03a81, . . . , \u03a8m be\nfunctions from T to X and de\ufb01ne K(t1, t2) as the m \u00d7 m-matrix\nK(t1, t2)ij = \u03ba(\u03a8i(t1), \u03a8j(t2))\ni, j = 1, . . . , m, t1, t2 \u2208T.\nThen K is a Cm-reproducing kernel on T, the space HK is embedded in H\u03ba\nby means of the feature operator\nW : H\u03ba \u2212\u2192HK\n(W(\u03d5)(t))i = \u03d5(\u03a8i(t))\n\u03d5 \u2208H\u03ba, t \u2208T.\nIf one of \u03a81, . . ., \u03a8m is surjective, then W is unitary.\nProof. Apply Proposition 1 with H = H\u03ba and \u03b3t : Cm \u2192H\u03ba,\n\u03b3t(y1, . . . , ym) =\nm\nX\ni=1\nyi\u03ba\u03a8i(t),\nso that \u03b3\u2217\nt (\u03d5)i = \u03d5(\u03a8i(t)).\nIf \u03a8i is surjective for some index i = 1, . . . , m, the condition \u03d5(\u03a8i(t)) = 0\nfor all t \u2208T implies that \u03d5(x) = 0 for all x \u2208X, that is, \u03d5 = 0. Hence W is\ninjective and, hence, unitary.\n15\n3.3\nProduct of kernels\nThe following proposition extends Schur lemma about products of reproduc-\ning kernels to the vector valued case.\nProposition 9. Let K be a Y-kernel and \u03ba a scalar kernel. De\ufb01ne\n(\u03baK)(x, t) = \u03ba(x, t)K(x, t)\nx, t \u2208X,\nthen \u03baK is a Y-reproducing kernel and H\u03baK is embedded into H\u03ba \u2297HK by\nmeans of the feature operator\nW(\u03d5 \u2297f)(x) = \u03d5(x)f(x)\n\u03d5 \u2208H\u03ba, f \u2208HK.\nIf both \u03ba and K are Mercer kernels, so is \u03baK, whereas if\nsup\nx\u2208X\n{\u03ba(x, x), \u2225K(x, x)\u2225} < \u221eand\n\uf8f1\n\uf8f2\n\uf8f3\n\u03bax \u2208C0(X) and Kxv \u2208C(X; Y)\nor\n\u03bax \u2208C(X) and Kxv \u2208C0(X; Y)\n(13)\nthen K is a C0 kernel.\nProof. Let H = H\u03ba \u2297HK.\nSince \u03ba is a scalar kernel, \u03bax \u2208H\u03ba.\nDe\ufb01ne\n\u03b3x : Y \u2192H by means of \u03b3xy = \u03bax \u2297Kxy, then \u03b3\u2217\nx(\u03d5 \u2297f) = \u03d5(x)f(x). First\nclaim is a consequence of Proposition 1.\nIf both \u03ba and K are Mercer kernels, clearly \u03baK is Mercer.\nTo prove that if (13) hold then K is C0, we apply Corollary 1 with S =\n{\u03d5 \u2297f | \u03d5 \u2208H\u03ba, f \u2208HK}, and observe that\n\u2225\u03b3x\u2225\u2264\u2225\u03bax\u2225\u03ba \u2225Kx\u2225\u2264C,\nby assumption.\nBased on the above results, we characterize the RKHS whose kernel is\ngiven in [5].\nExample 5. Let \u03ba be a scalar reproducing kernel and B a positive bounded\noperator on Y. De\ufb01ne K : X \u00d7 X \u2192L(Y) as\nK(x, t) = \u03ba(x, t)B\nx, t \u2208X\n(i) The map K is a Y-reproducing kernel and HK is unitarily equivalent\nto H\u03ba \u2297ker B\u22a5by means of the unitary operator\nW(\u03d5 \u2297y)(x) = \u03d5(x)B\n1\n2y.\n16\n(ii) If \u03ba is Mercer [resp. C0], then K is Mercer [resp. C0], too.\n(iii) If there is an orthonormal basis (yi)i\u2208I of ker B\u22a5such that Byi = \u03c3iyi\n(so that \u03c3i > 0 for all i \u2208I), then HK is unitarily equivalent to \u2295i\u2208IH\u03ba\nby means of the unitary operator\nf\nW(\u2295i\u2208I\u03d5i)(x) =\nX\ni\u2208I\n\u221a\u03c3i\u03d5i(x)yi ,\n(14)\nwhere the series converges in norm.\nProof. First two items are a consequence of Proposition 9 and Example 2.\nWe prove item (iii) in two steps. Apply \ufb01rst Proposition 6 with w : Y \u2192\u21132,\n(wy)i = \u27e8y, yi\u27e9, so that HKw is embedded in HK, by means of the feature\noperator Ww(f) = w \u25e6f for all f \u2208HK. The corresponding \u21132-kernel is\nKw(x, t) = \u03ba(x, t)wBw\u2217. By de\ufb01nition of w, the kernel Kw is diagonal with\nrespect to (ei)i\u2208I, the canonical basis of \u21132, namely\nKw(x, t) =\nX\ni\u2208I\n\u03c3i\u03ba(x, t)ei \u2297ei =:\nX\ni\u2208I\nKi(x, t),\nwhere the series converges in the strong operator topology.\nNow observe that, for each i \u2208I, ker(\u03c3iei \u2297ei)\u22a5= Cei, so that for item (i)\nof this example, the space HKi is unitarily equivalent to H\u03ba \u2297C ei \u2243H\u03ba,\nthrough the feature operator\nW i : H\u03ba \u2192HKi ,\nW i(\u03d5)(x) = \u03d5(x)\u221a\u03c3iei\nApplying Proposition 5 to the family (Ki)i\u2208I, we obtain a unitary operator\nW :\nM\ni\u2208I\nH\u03ba \u2212\u2192HKw ,\nW(\u2295i\u03d5i)(x) =\nX\ni\n\u03d5i(x)\u221a\u03c3iei,\n(the operator W is unitary since \u03c3i > 0 for all i \u2208I, so that W is injective).\nEquation (14) is \ufb01nally obtained letting f\nW = W \u2217\nwW.\nIf in Example 5, Y is a RKHS of scalar functions over some set X\u2032, then\nthere is a particular choice for the operator B, suggested by Example 1.\nExample 6. Let X and X\u2032 be two locally compact second countable topo-\nlogical spaces. Let \u03ba : X \u00d7 X \u2192C and \u03ba\u2032 : X\u2032 \u00d7 X\u2032 \u2192C be two scalar\nreproducing kernels on X and X\u2032, respectively.\n17\n(i) If I\u2032 denotes the identity operator on H\u03ba\u2032, de\ufb01ne\nK : X \u00d7 X \u2192L(H\u03ba)\nK(x, t) = \u03ba(x, t)I\u2032,\nthen K is a H\u03ba\u2032-reproducing kernel on X and the corresponding RKHS\nHK is unitarily equivalent to H\u03ba \u2297H\u03ba\u2032 by means of the feature operator\nW : H\u03ba \u2297H\u03ba\u2032 \u2212\u2192HK ,\nW(\u03d51 \u2297\u03d52)(x) = \u03d51(x)\u03d52 .\n(ii) De\ufb01ne \u03ba \u00d7 \u03ba\u2032 : (X \u00d7 X\u2032) \u00d7 (X \u00d7 X\u2032) \u2192C as\n(\u03ba \u00d7 \u03ba\u2032) (x, x\u2032; t, t\u2032) = \u03ba(x, t)\u03ba\u2032(x\u2032, t\u2032),\nthen \u03ba\u00d7\u03ba\u2032 is a scalar kernel on X\u00d7X\u2032 and H\u03ba\u00d7\u03ba\u2032 is unitarily equivalent\nto HK by means of the feature operator\nf\nW(f)(x, x\u2032) = [f(x)] (x\u2032) = \u27e8f(x), \u03ba\u2032\nx\u2032\u27e9\u03ba\u2032\nf \u2208HK.\nProof. The \ufb01rst part follows from Example 5 with Y = H\u03ba\u2032 and B = I\u2032,\nwhich is injective. The second part is a consequence of Proposition 1 applied\nto\n\u03b3 : X \u00d7 X\u2032 \u2212\u2192L(C; HK) \u2243HK ,\n(x, x\u2032) 7\u2212\u2192W(\u03bax \u2297\u03ba\u2032\nx\u2032) ,\ntaking into account the injectivity of W and the equalities\n\nW(\u03d51 \u2297\u03d52), \u03b3(x,x\u2032)\n\u000b\nK=\u27e8\u03d51 \u2297\u03d52, \u03bax \u2297\u03ba\u2032\nx\u2032\u27e9= \u03d51(x) \u27e8\u03d52, \u03ba\u2032\nx\u2032\u27e9\u03ba\u2032\n=\u27e8W(\u03d51 \u2297\u03d52)(x), \u03ba\u2032\nx\u2032\u27e9\u03ba\u2032 = f\nW(W(\u03d51 \u2297\u03d52))(x, x\u2032).\nBy using Proposition 4 on the space X\u2032, the above example can be realized\nin an alternative way.\nExample 7. Let X and X\u2032 be two locally compact second countable topo-\nlogical spaces. Let \u03ba : X \u00d7 X \u2192C and \u03ba\u2032 : X\u2032 \u00d7 X\u2032 \u2192C be two scalar\nC0-reproducing kernels on X and X\u2032, respectively. Let \u00b5\u2032 be a probability mea-\nsure on X\u2032 with supp \u00b5\u2032 = X\u2032 and L\u00b5\u2032 be the integral operator on L2(X\u2032, \u00b5\u2032)\nwith kernel \u03ba\u2032. De\ufb01ne\nbK : X \u00d7 X \u2192L(L2(X\u2032, \u00b5\u2032))\nbK(x, t) = \u03ba(x, t)L\u00b5\u2032,\nthen the kernel bK is a L2(X\u2032, \u00b5\u2032)-reproducing kernel and the space H b\nK is\nunitarily equivalent to H\u03ba \u2297H\u03ba\u2032 by means of\nc\nW(f \u2297g)(x) = f(x)i\u00b5\u2032(g)\nf \u2208H\u03ba, g \u2208H\u03ba\u2032,\nwhere i\u00b5\u2032 is the inclusion of H\u03ba\u2032 in L2(X\u2032, \u00b5). In particular, bK is a C0-kernel.\n18\nProof. Apply Proposition 6 with K = \u03baI\u2032, as in the previous example, and\nw = i\u00b5\u2032, which is injective. Clearly Kw = bK, so that H b\nK is unitarily equiva-\nlent to H\u03baI\u2032. The thesis follows immediately from Example 6.\nThe above example shows that HK and H b\nK are the same RKHS, where\nthe elements of HK are regarded as functions from X into H\u03ba\u2032, whereas the\nelements of H b\nK are regarded as functions from X into L2(X\u2032, \u00b5\u2032).\n3.4\nApplication to learning theory\nWe end this section considering an application of some of the above examples\nto vector valued regression problems. In learning theory, a popular algorithm\nis the minimization on a RKHS HK of the empirical error with a penalty term\nproportional to the square of the norm [13], namely\nf \u22c6= argmin\nf\u2208HK\n \n1\nn\nn\nX\n\u2113=1\n\r\ry\u2113\u2212f(x\u2113)\n\r\r2\nY + \u03bb \u2225f\u22252\nK\n!\n.\n(15)\nHere {(x1, y1), . . . , (xn, yn)} is the training set of n input-output pairs (x\u2113, y\u2113) \u2208\nX \u00d7 Y and \u03bb > 0 is the regularization parameter. If the reproducing kernel\nK is as in Example 5, then it can be checked that\nf \u22c6(x) =\nX\ni\u2208I\n\u03d5\u22c6\ni (x)yi\nwhere each \u03d5\u22c6\ni is given by\n\u03d5\u22c6\ni = argmin\n\u03d5\u2208H\u03ba\n \n1\nn\nn\nX\n\u2113=1\n|y\u2113\ni \u2212\u03d5(x\u2113)|2 + \u03bb\n\u03c3i\n\u2225\u03d5\u22252\n\u03ba\n!\n,\nand y\u2113\ni =\n\ny\u2113, yi\n\u000b\n.\nIn many applications Y = Cm so that B is a m \u00d7 m positive semi-de\ufb01nite\nmatrix. The above observation reduces the problem of computing the mini-\nmizer of (15) to |I| scalar problems, where the cardinality |I| is the rank of\nthe matrix B.\nWith the choice of K as in Proposition 8, let f \u22c6be the minimizer given\nby (15), where the n-examples in the training set are the pairs (t\u2113, y\u2113) \u2208\nT \u00d7 Rm. By using the fact that W is a partial surjective isometry, one can\ncheck that\nf \u22c6(t) = (\u03d5\u22c6(\u03a8i(t)), . . . , \u03d5\u22c6(\u03a8m(t)),\n19\nwhere \u03d5\u22c6is given by\n\u03d5\u22c6= argmin\n\u03d5\u2208H\u03ba\n \n1\nn\nn\nX\n\u2113=1\nm\nX\ni=1\n|y\u2113\ni \u2212\u03d5(x\u2113\ni)|2 + \u03bb \u2225\u03d5\u22252\n\u03ba\n!\n,\nwhere y\u2113\ni \u2208R are the components of the output y\u2113\u2208Rm and x\u2113\ni = \u03a8i(t\u2113) \u2208X.\nWith this choice the problem (15) is reduced to a minimization problem on\nthe scalar RKHS H\u03ba.\n4\nUniversal kernels: main results\nIn this section we address the problem of de\ufb01ning and characterizing the\nuniversality of a kernel K. As pointed out in the introduction, in learning\ntheory a necessary condition in order to have universally consistent algo-\nrithms is the assumption that the reproducing kernel Hilbert space HK is\ndense in L2(X, \u00b5; Y) for any probability measure \u00b5. From this point of view\nnext de\ufb01nition is very natural.\nDe\ufb01nition 2. Let K : X \u00d7 X \u2192L(Y) be a reproducing kernel.\n(i) A C0-kernel K is called universal if HK is dense in L2(X, \u00b5; Y) for\neach probability measure \u00b5.\n(ii) A Mercer kernel K is called compact-universal if HK is dense in\nL2(X, \u00b5; Y) for each probability measure \u00b5 with compact support.\nWe brie\ufb02y comment on the above de\ufb01nitions. In item (i) the assumption\nthat the kernel is C0 ensures both that HK is a subspace of L2(X, \u00b5; Y)\nand that universality is equivalent to the density of HK is C0(X; Y) (see\nTheorem 1).\nIn item (ii), since \u00b5 has compact support, it is enough to\nassume that K is a Mercer kernel in order to have HK \u2282L2(X, \u00b5; Y). This\nlast property turns out to be equivalent to the de\ufb01nition of universality given\nin [5].\nClearly a universal kernel is also compact-universal. Conversely, a C0-kernel\ncan be compact-universal but not universal, as shown by Examples 8 and 11.\nNotice that in De\ufb01nition 2 if we replace L2(X, \u00b5; Y) with Lp(X, \u00b5; Y)\nfor an arbitrary 1 \u2264p < \u221e, we have in principle a di\ufb00erent notion of\nuniversality. Nevertheless Theorem 1 clari\ufb01es that there is no di\ufb00erence. We\nstate the results for p = 2, since it is the natural choice in learning theory.\nThe following corollary shows that universality is preserved by restriction\nto a subset.\n20\nCorollary 3. Let X0 be a subset of X.\n(i) If X0 is closed and K is universal, then KX0 is universal.\n(ii) If X0 is locally closed and K is compact-universal, then KX0 is compact-\nuniversal.\nProof. We only prove (i). Since X0 is closed, Corollary 2 implies that KX0\nis a C0-kernel, and a function f belongs to HKX0 if and only if there exists\ng \u2208HK such that f = g|X0. Given a probability measure \u00b5 on X0, let \u03bd be\nthe probability measure on X, \u03bd(E) = \u00b5(E \u2229X0) for any Borel subset E of\nX. By universality of K, HK is dense in L2(X, \u03bd, Y) \u2243L2(X0, \u00b5, Y), where\nthe equivalence is given by the restriction from X to X0, so that HKX0 is\ndense in L2(X0, \u00b5, Y).\nThe converse is clearly not true. Notice that the compact-universal ker-\nnels are precisely the Mercer kernels such that KX0 is universal for any com-\npact subset X0 of X.\nIn the next subsections we discuss separately the two notions of univer-\nsality and then we make a comparison between them.\n4.1\nUniversality and C0-kernels\nIn this section we characterize the universal C0-kernels. First result shows\nthat the density of HK in L2(X, \u00b5; Y) for any probability measure \u00b5 is equiv-\nalent to the density in C0(X; Y) and that one can replace L2(X, \u00b5; Y) with\nLp(X, \u00b5; Y), 1 \u2264p < \u221e.\nTheorem 1. Suppose K is a C0-kernel. The following facts are equivalent.\n(a) The kernel K is universal.\n(b) The space HK is dense in C0(X; Y).\n(c) There is 1 \u2264p < \u221esuch that HK is dense in Lp(X, \u00b5; Y) for all\nprobability measures \u00b5 on X.\nProof. Clearly (a) implies (c). Since X is locally compact and second count-\nable, C0(X; Y) is dense in L2(X, \u00b5; Y) where the inclusion is continuous, so\nthat (b) implies (a).\nWe show that (c) implies (b).\nSuppose hence that HK is not dense in\nC0(X; Y). Then, there exists T \u2208C0(X; Y)\u2217, T \u0338= 0 such that T(f) = 0\nfor all f \u2208HK. By Theorem 7, there is a probability measure \u00b5 on X and\na function h \u2208L\u221e(X, \u00b5; Y) such that T(f) =\nR\nX \u27e8f(x), h(x)\u27e9d\u00b5(x). Since\n21\nT \u0338= 0, then h \u0338= 0.\nSince \u00b5 is a probability measure, h is a non-null element in Lp/(p\u22121)(X, \u00b5; Y) =\nLp(X, \u00b5; Y)\u2217(where we set 1/0 = \u221e) such that\nZ\nX\n\u27e8f(x), h(x)\u27e9d\u00b5(x) = 0\n\u2200f \u2208HK.\nIt follows that HK is not dense in Lp(X, \u00b5; Y).\nAs a consequence of the previous theorem, we have the following nice\ncorollary.\nCorollary 4. Let K be a C0- kernel. Given 1 \u2264p \u2264q < \u221e, the space HK is\ndense in Lp(X, \u00b5; Y) for all probability measures \u00b5 if and only if it is dense\nin Lq(X, \u00b5; Y) for all probability measures \u00b5.\nThe previous result is not trivial. Clearly, if q \u2265p, the space Lq(X, \u00b5; Y)\nis always a dense subspace of Lp(X, \u00b5; Y) and the inclusion is continuous.\nHence, if a RKHS HK is dense in Lq(X, \u00b5; Y), then HK is always dense in\nLp(X, \u00b5; Y). However, in general Lp(X, \u00b5; Y) is not contained in Lq(X, \u00b5; Y),\nso that, if HK is dense Lp(X, \u00b5; Y), the density of HK in Lq(X, \u00b5; Y) has to\nbe proved. Corollary 4 shows this result under the assumption that K is C0.\nNow, we give a characterisation of universality of K in terms of the injec-\ntivity property of the integral operators L\u00b5, for \u00b5 varying over the probability\nmeasures on X.\nTheorem 2. Suppose K is a C0-kernel. Then the following facts are equiv-\nalent.\n(a) The kernel K is universal.\n(b) The operator i\u2217\n\u00b5 : L2(X, \u00b5; Y) \u2192HK is an injective operator for all\nprobability measures \u00b5 on X.\n(c) The integral operator L\u00b5 : L2(X, \u00b5; Y) \u2192L2(X, \u00b5; Y) is injective for\nall probability measures \u00b5 on X.\nThe proof is an immediate consequence of Theorem 1 and the next propo-\nsition.\nProposition 10. Let K be a Mercer kernel and \u00b5 a \ufb01xed probability measure\non X such that K is bounded on the support of \u00b5. The following facts are\nequivalent.\n(a) The space HK is dense in L2(X, \u00b5; Y).\n22\n(b) The operator i\u2217\n\u00b5 is injective.\n(c) The integral operator L\u00b5 is injective.\nProof. The space HK is dense in L2(X, \u00b5; Y) if and only if the range of i\u00b5 is\ndense in L2(X, \u00b5; Y). This last condition is equivalent to the injectivity of\ni\u2217\n\u00b5, that is, (a) is equivalent to (b). Since L\u00b5 = i\u00b5i\u2217\n\u00b5 and ker L\u00b5 = ker i\u2217\n\u00b5, then\n(b) and (c) are equivalent.\n4.2\nCompact-universality\nIn this section, we characterize compact-universality of Mercer kernels and\nwe show that compact-universality is precisely what is called universality in\n[5].\nNext theorem characterizes compact-universality.\nTheorem 3. Suppose K is a Mercer kernel. The following facts are equiva-\nlent.\n(a) The kernel K is compact-universal.\n(b) The space HK is dense in C(X; Y) endowed with compact-open topology.\n(c) There is 1 \u2264p < \u221esuch that HK is dense in Lp(X, \u00b5; Y) for all\ncompactly supported probability measures.\nProof. Clearly (a) implies (c). We prove that (b) implies (a). Indeed, \ufb01xed\na probability measure \u00b5 with compact support Z, the fact that HK is dense\nin C(X; Y) implies that HK|Z := {f |Z | f \u2208HK} is dense in C(Z; Y), but\nC(Z; Y) is clearly dense in L2(Z, \u00b5; Y) \u2243L2(X, \u00b5; Y) with continuous injec-\ntion. Hence HK is dense in L2(X, \u00b5; Y). It only remains to prove that (c)\nimplies (b). For this, it is enough to prove that HK|Z is dense in C(Z; Y)\nwith the uniform norm, for all compact subset Z of X. But this is a simple\nconsequence of Theorem 1 since HK|Z is clearly dense in Lp(Z, \u00b5; Y) for all\nprobability measure \u00b5 on Z, and C(Z; Y) = C0(Z; Y).\nThe analog of theorem 2 also holds.\nTheorem 4. Suppose K is a Mercer kernel. Then the following facts are\nequivalent.\n(a) The kernel K is compact-universal.\n(b) The operator i\u2217\n\u00b5 : HK \u2192L2(X, \u00b5; Y) is an injective operator for all\ncompactly supported probability measures \u00b5 on X.\n23\n(c) The integral operator L\u00b5 : L2(X, \u00b5; Y) \u2192L2(X, \u00b5; Y) is injective for\nall probability measures \u00b5 on X with compact support.\nThe proof is a simple consequence of Proposition 10.\nClearly universality of a C0-kernel K implies compact-universality. The\nconverse is not true as shown by the following example, see also Example 11.\nThe reason of this phenomenon is the fact that C0(X; Y) endowed with the\ncompact-open topology is not continuously embedded in Lp(X, \u00b5; Y).\nExample 8. Let X = Z+, and let \u21132 be the Hilbert space of square summable\nsequences. Then, \u21132 is a RKHS of scalar functions on X with reproducing\nkernel K(i, j) = \u03b4i,j, where \u03b4i,j is the Kronecker delta. We \ufb01x the following\nsequence {fk}k\u2208Z+ in \u21132\nfk(j) = \u03b4j,k + e\u03b4j,k+1,\nand we let\nH \u02dc\nK = \u21132\u2212cl span {fk | k \u2208Z+}\n(16)\n(\u21132\u2212cl denotes the closure in \u21132). H \u02dc\nK is also a RKHS of scalar functions\non X, whose reproducing kernel we denote by \u02dcK. Since \u21132 \u2282c0 ( = the\nsequences going to 0 at in\ufb01nity), \u02dcK is a C0-reproducing kernel.\nFor all n \u2208Z+, let Zn = {1, 2 . . .n}. Zn is compact in X, and every\ncompact set Z \u2282X is contained in some Zn. Clearly,\nC(Zn) = span\n\b\n(fk)|Zn | k \u2264n\n\t\n,\nhence H \u02dc\nK is dense in C(X) with the topology of uniform convergence on\ncompact subsets.\nLet \u00b5 be the probability measure on X such that \u00b5({j}) = (e \u22121)e\u2212j.\nWe claim that H \u02dc\nK is not dense in L2(X, \u00b5). In fact, let f \u2208L2(X, \u00b5) be the\nfunction f(j) = (\u22121)j. We have \u27e8fk, f\u27e9L2(X,\u00b5) = 0 for all k. By (16) and\ncontinuity of the inclusion \u21132 \u0592\u2192L2(X, \u00b5), we see that f is in the orthogonal\ncomplement of H \u02dc\nK in L2(X, \u00b5). The claim then follows.\nA universal kernel is strictly positive de\ufb01nite, but the converse in general\nfails, as shown by the following corollary and example.\nCorollary 5. Suppose K is a compact-universal kernel. Then K is strictly\npositive de\ufb01nite, i.e. for all \ufb01nite subsets {x1, x2 . . . xN} of X such that xi \u0338=\nxj if i \u0338= j, the condition\nN\nX\ni,j=1\n\u27e8K(xi, xj)yj, yi\u27e9= 0\n(yi \u2208Y, i = 1 . . . N)\nimplies yi = 0 for all i = 1, . . . , N.\n24\nProof. Assume PN\ni,j=1 \u27e8K(xi, xj)yj, yi\u27e9= 0 for some \ufb01nite subset {x1, x2 . . . xN} \u2208\nX, xi \u0338= xj if i \u0338= j, and {y1, y2 . . . xN} in Y. Taking\n\u00b5 = 1\nN\nN\nX\ni=1\n\u03b4xi\nand\n\u03d5 =\nN\nX\ni=1\nyi\u03b4xi,\nwe obtain a probability measure \u00b5 on X with compact support and a function\n\u03d5 \u2208L2(X, \u00b5; Y) such that\n0 =\nN\nX\ni,j=1\n\u27e8K(xi, xj)yj, yi\u27e9= N2\nZ\nX\u00d7X\n\u27e8K(x, y)\u03d5(y), \u03d5(x)\u27e9d\u00b5(y) d\u00b5(x)\n= N2\nZ\nX\n\u27e8(L\u00b5\u03d5)(x), \u03d5(x)\u27e9d\u00b5(x) = N2 \u27e8L\u00b5\u03d5, \u03d5\u27e92 .\nSince L\u00b5 is positive and injective by Theorem 4, we have \u03d5(xi) = 0 for all\ni = 1, . . . , N. Since xi \u0338= xj if i \u0338= j, then yi = 0 for all i = 1, . . . , N.\nThe converse of the above corollary fails to be true, as shown by the\nfollowing example.\nExample 9. Let K : R \u00d7 R \u2192C be the kernel\nK (x, t)=\nZ 1\n\u22121\ne2\u03c0i(x\u2212t)pdp = sin 2\u03c0(x \u2212t)\n\u03c0(x \u2212t)\n.\nThe map K is a scalar C0-kernel, which is strictly positive de\ufb01nite, but not\nuniversal.\nProof. We show that it is strictly positive de\ufb01nite. Indeed, let x1, . . . xN \u2208X\nsuch that xi \u0338= xj if i \u0338= j, c1, . . . , cN \u2208C and suppose\n0 =\nN\nX\ni,j=1\ncicjK (xi, xj)=\nZ 1\n\u22121\n|\nN\nX\ni=1\ncie2\u03c0ixip|2dp\nSince p 7\u2192| P\ni cie2\u03c0ixip|2 is continuous, it follows that | P\ni cie2\u03c0ixip|2 = 0\nfor all p \u2208[\u22121, 1]. Observing that the functions fj(t) = e2\u03c0ixjt are linearly\nindependent on [\u22121, 1] since xi \u0338= xj, it follows that cj = 0 for all j. Clearly\nK is a C0-kernel, but it is not universal (see Example 11).\nIn the next remark we show that compact-universality is exactly what is\ncalled universality in [5].\n25\nRemark 1. In [5], a Mercer kernel K is said to be universal if, for each\ncompact set Z \u2286X\nC(Z; Y) = \u2225\u00b7\u2225Z \u2212cl span\n\b\nK (\u00b7, x) v|Z | x \u2208Z, v \u2208Y\n\t\n,\n(17)\nwhere \u2225\u00b7\u2225Z \u2212cl denotes the closure in C(Z; Y) with the uniform norm topol-\nogy. This is equivalent to require that HK is dense C(X; Y) with the compact-\nopen topology, that is, by Theorem 1 that K is compact-universal. Indeed,\nby de\ufb01nition of the compact-open topology, HK is dense in C(X; Y) if and\nonly if\nC(Z; Y) = \u2225\u00b7\u2225Z \u2212cl HK|Z\n(18)\nfor all compact Z \u2286X.\nClearly (17) implies (18). Suppose on the other hand that (18) holds true.\nDenote with eK the restriction of K to Z \u00d7 Z. Since convergence in H e\nK\nimplies uniform convergence we have\n\u2225\u00b7\u2225Z \u2212cl span\n\b\nK(\u00b7, x)v|Z | x \u2208Z, v \u2208Y\n\t\n\u2287H e\nK\nOn the other hand, H e\nK = HK|Z as a linear space of functions (see Corol-\nlary 2). Hence (18) implies (17).\n5\nTranslation invariant kernels and univer-\nsality\nIn this section we assume that X is a locally compact second countable\ntopological group with identity e and we study the reproducing kernels that\nare translation invariant, namely\nK(zx, zt) = K(x, t)\nfor all x, t, z \u2208X.\n(19)\nIn particular we characterize all the translation invariant kernels in terms\nof a unitary representation of X acting on an arbitrary Hilbert space H\nand an operator A : H \u2192Y. If X is an abelian group, we give a more\nexplicit characterization in Theorem 5 and Theorem 13 provides a su\ufb03cient\ncondition ensuring that the corresponding reproducing kernel Hilbert space\nis universal. This condition is also necessary if X is compact or Y = C. For\nscalar kernels on Rd our result has been already proved in [22].\nFor a representation \u03c0 of X on a vector space V we mean a group ho-\nmomorphism from X to the automorphisms of V . In particular, if V is a\nHilbert space, \u03c0 is unitary if it takes values in the group of unitary operators\n26\non V . In this framewok the representation is called continuous if \u03c0 is strongly\ncontinuous (see [16]).\nWe denote by \u03bb the left regular representation of X acting on F(X; Y),\nnamely\n(\u03bbxf)(t) = f(x\u22121t)\nt, x \u2208X, f \u2208F(X; Y).\nWe recall that a function \u0393 : X \u2192L(Y) is of completely positive type if\nN\nX\ni,j=1\n\n\u0393(x\u22121\nj xi)yj, yi\n\u000b\n\u22650\n(20)\nfor all \ufb01nite sequences {xi}i=1...N in X and {yi}i=1...N in Y.\nThe following facts are easy to prove.\nProposition 11. Let K : X \u00d7 X \u2192L(Y) be a reproducing kernel. The\nfollowing conditions are equivalent.\n(a) K is a translation invariant reproducing kernel.\n(b) There is a function Ke : X \u2192L(Y) of completely positive type such\nthat K(x, t) = Ke(t\u22121x).\nIf one the above conditions is satis\ufb01ed, then the representation \u03bb leaves in-\nvariant HK, its action on HK is unitary and\nK(x, t) = K\u2217\ne\u03bbx\u22121tKe\nx, t \u2208X\n(21)\n\u2225K(x, x)\u2225= \u2225Ke(e)\u2225\nx \u2208X\n(22)\nThe notation Ke for the function of completely positive type associated\nwith the reproducing kernel K is consistent with the de\ufb01nition given by (1)\nsince\n(Key)(x) = Ke(x)y\ny \u2208Y, x \u2208X.\nProof of Proposition 11. Assume (a). Given x, t \u2208X, (1) and (19) give\nKe(t\u22121x) = K(t\u22121x, e) = K(x, t).\nSince K is a reproducing kernel, Ke is of completely positive type, so that\n(b) holds true.\nAssume (b). Clearly K is a translation invariant reproducing kernel, so that\n(a) holds true.\nSuppose now that K is a translation invariant reproducing kernel. Ob-\nserve that, given t \u2208X and y \u2208Y,\n(\u03bbxKty)(z) = (Kty)(x\u22121z) = K(x\u22121z, t)y = K(z, xt)y = (Kxty)(z)\nx, z \u2208X,\n27\nthat is, \u03bbxKt = Kxt. Moreover\n\u27e8\u03bbxKt1y1, \u03bbxKt2y2\u27e9K = \u27e8Kxt1y1, Kxt2y2\u27e9K = \u27e8K(xt2, xt1)y1, y2\u27e9\n= \u27e8K(t2, t1)y1, y2\u27e9= \u27e8Kt1y1, Kt2y2\u27e9K .\nThis means that \u03bb leaves the set {Kxy | x \u2208X, y \u2208Y} invariant and its ac-\ntion is unitary. First two claims now follow recalling that {Kxy | x \u2208X, y \u2208Y}\nis total in HK. To prove (21) observe that\nK(x, t) = K\u2217\nxKt = K\u2217\ne\u03bb\u2217\nx\u03bbtKe = K\u2217\ne\u03bbx\u22121tKe\nfor all x, t \u2208X.\nNotice that, if K is a translation invariant kernel, (22) implies that the\nelements of HK are bounded functions. The following lemma characterizes\nthe translation invariant kernels that are Mercer or C0.\nLemma 1. Let Ke : X \u2192Y be a function of completely positive type and\nlet K be the corresponding translation invariant reproducing kernel.\nThe\nfollowing conditions are equivalent.\n(a) The map K is a Mercer kernel.\n(b) For all y \u2208Y, Ke(\u00b7)y \u2208C(X; Y).\n(c) The representation \u03bb is continuous on HK.\nMoreover, the map K is a C0-kernel if and only if Ke(\u00b7)y \u2208C0(X; Y) for all\ny \u2208Y.\nProof. The equivalence between (a) and (b) as well as the statement about\nC0-kernel is a consequence of Proposition 2, observing that (Kxy)(t) = Ke(x\u22121t)y\nand (22) holds.\nAssume that K is a Mercer kernel. Since \u03bb is a unitary representation and\nthe set {Kty | t \u2208X, y \u2208Y} is total in HK, it is enough to check that for\nany t \u2208X and y \u2208Y the function x 7\u2192\u03bbxKty is continuous at the identity.\nIndeed, observe that\n\u2225\u03bbxKty \u2212Kty\u22252\nK = \u2225Kxty \u2212Kty\u22252\nK\n= \u27e8(K(xt, xt) \u2212K(t, xt) \u2212K(xt, t) + K(t, t)) y, y\u27e9\n=\n\n\u00002Ke(e) \u2212Ke(t\u22121x\u22121t) \u2212Ke(t\u22121xt)\n\u0001\ny, y\n\u000b\n,\nwhich is continuous at the identity by assumption on Ke. Conversely, if \u03bb is\ncontinuous, (21) gives that\nKe(x)y = K(x, e)y = K\u2217\ne\u03bbx\u22121Key,\nso that Ke(\u00b7)y is continuous.\n28\nThe following theorem characterizes the translation invariant reproducing\nkernels.\nProposition 12. Let \u03c0 be a unitary representation of X acting on a separable\nHilbert space H and A : H \u2192Y a bounded operator. De\ufb01ne\nW : H \u2192F(X; Y) ,\n(Wv)(x) = A\u03c0x\u22121v .\n(23)\nW is a unitary map from ker W \u22a5onto the reproducing kernel Hilbert space\nHK with translation invariant kernel\nK(x, t) = A\u03c0x\u22121tA\u2217\nx, t \u2208X.\n(24)\nMoreover W intertwines the representations \u03c0 and \u03bb. Finally W is unitary\nif and only if the only \u03c0-invariant closed subspace of ker A is the null space.\nProof. De\ufb01ne \u03b3x : Y \u2192H as \u03b3x = \u03c0xA\u2217, so that (Wv)(x) = \u03b3\u2217\nxv = A\u03c0x\u22121v.\nThe claim is now consequence of Proposition 1, up the last statement. The\nfact that W intertwines \u03c0 with \u03bb is trivial. Finally, by Proposition 1, W is\nunitary if and only if is injective. By de\ufb01nition\nker W = {v \u2208H | \u03c0xv \u2208ker A \u2200x \u2208X}.\nHence ker W is a closed subspace of ker A invariant with respect to \u03c0. Con-\nversely any \u03c0-invariant closed subspace of ker A is contained in ker W.\nProposition 11 and 12 show that any translation invariant kernel is of\nthe form K(x, t) = A\u03c0x\u22121tA\u2217for some unitary representation \u03c0 acting on a\nHilbert space H and a bounded operator A : H \u2192Y. In particular, if \u03c0 is\na continuous representation, then K is a Mercer kernel and for any Mercer\nkernel \u03c0 can be assumed to be continuous and H separable. Moreover, the\nreproducing kernel Hilbert space HK is embedded in H by the feature oper-\nator W de\ufb01ned by (23). Observe that if the representation \u03c0 is irreducible\nor if A is injective, then W is unitary.\nIf Y = C, the operator A is of the form Av = \u27e8v, w\u27e9H for some w \u2208H, so\nthat (Wv)(x) = \u27e8v, \u03c0xw\u27e9H. This operator is well know in harmonic analysis\nas wavelet operator [17].\nRemark 2. Notice that any translation invariant kernel K is the sum of\ntranslation invariant kernels associated with cyclic representations. Indeed,\nlet \u03c0 be a unitary representation de\ufb01ning K by means of (24). Since any\nunitary representation is the direct sum of a family of cyclic representations,\nthen H = \u2295i\u2208IHi where each Hi is a closed \u03c0-invariant subspace and the\n29\naction of \u03c0 on Hi is cyclic. Denote by Pi the orthogonal projection on Hi,\nthen\nK(x, t) =\nX\ni\u2208I\nAPi\u03c0x\u22121tPiA\u2217=\nX\ni\u2208I\nKi(x, t),\nwhere the series converges in the strong operator topology and the reproduc-\ning kernels Ki are Ki(x, t) = Ai\u03c0i\nx\u22121tA\u2217\ni where \u03c0i and Ai are the restrictions\nof \u03c0 and A to Hi, respectively. Proposition 5 implies that HK = P\ni\u2208I HKi.\nFor scalar kernels, we can always assume that \u03c0 is cyclic itself. Indeed, the\nwavelet operator is (Wv)(x) = \u27e8v, \u03c0xw\u27e9H for some w \u2208H, so that the as-\nsociated kernel K is determined only by the cyclic subrepresentation of \u03c0\ncontaining w.\n5.1\nAbelian groups\nIn this section, we specialize the previous discussion to the case in which\nX is an abelian group. With this assumption, we can give a more explicit\nconstruction of translation invariant Mercer kernels, which is related to a\ngeneralization of Bochner theorem for scalar functions of positive type, [2, 15].\nWe denote the product in X additively and the identity by 0, since the\nmain example is Rd. We let \u02c6X be the dual group of X and we denote by dx\nthe Haar measure on X.\nNow, we brie\ufb02y recall the de\ufb01nition of Fourier transform, see for example\n[16]. If \u03c6 \u2208L1(X, dx; Y), its Fourier transform F(\u03c6) : \u02c6X \u2192Y is given by\nF(\u03c6)(\u03c7) =\nZ\nX\n\u03c7(x) \u03c6(x)dx.\nWe denote by d\u03c7 the Haar measure on \u02c6X normalized so that F extends to\na unitary operator from L2(X, dx; Y) onto L2( \u02c6X, d\u03c7; Y). If \u00b5 is a positive\nmeasure on X and \u03d5 \u2208L1(X, \u00b5; Y), let F(\u03d5\u00b5) : \u02c6X \u2192Y be given by\nF(\u03d5\u00b5)(\u03c7) =\nZ\nX\n\u03c7(x)\u03d5(x) d\u00b5(x).\nIf \u00b5 is a complex measure4 on X, we denote F(\u00b5) = F(h|\u00b5|) where |\u00b5| is the\ntotal variation of \u00b5 and h \u2208L1(X, |\u00b5|) is the density of \u00b5 with respect to |\u00b5|.\nBy general properties of Fourier transform, F(\u03c6) and F(\u00b5) are bounded\ncontinuous functions on \u02c6X (actually, F(\u03c6) \u2208C0(X; Y)). Moreover, F(\u03c6) = 0\n[respectively, F(\u00b5) = 0] if and only if \u03c6 = 0 in L1(X, dx; Y) [resp., \u00b5 = 0].\n4That is, a \u03c3-additive map \u00b5 : B(X) \u2192C.\n30\nWe recall that a positive operator valued measure (POVM) on \u02c6X with\nvalues in Y is a map Q : B( \u02c6X) \u2212\u2192L(Y) such that Q( \u02c6Z) \u22650 for all\n\u02c6Z \u2208B( \u02c6X), and\nX\ni\nQ( \u02c6Zi) = Q(\u222ai \u02c6Zi),\nfor every denumerable sequence of disjoint Borel sets { \u02c6Zi}i where the sum\nconverges in the weak operator topology. A positive operator valued measure\nQ is a projection valued measure if Q( \u02c6Z)2 = 1 for all \u02c6Z \u2208B( \u02c6X). If \u02c6f : \u02c6X \u2192\nC is a bounded measurable function,\nR\n\u02c6\nX \u02c6f(\u03c7)dQ(\u03c7) is the unique bounded\noperator \u02c6f(Q) de\ufb01ned by\nD\n\u02c6f(Q)y, y\u2032E\n=\nZ\n\u02c6\nX\n\u02c6f(\u03c7)dQy,y\u2032(\u03c7)\ny, y\u2032 \u2208Y,\nwhere Qy,y\u2032 is the complex measure on \u02c6X given by Qy,y\u2032( \u02c6Z) =\nD\nQ( \u02c6Z)y, y\u2032E\nfor all Borel subsets \u02c6Z.\nNext theorem shows that there is a one to one correspondence between\ntranslation invariant Mercer kernels on X and positive operator valued mea-\nsures on \u02c6X. For scalar kernels this result is Bochner theorem [2]. For vector\nvalued kernels, it is proved in [14, 15] under the weaker assumption that K0\nis a function of positive type, namely that\nN\nX\ni,j=1\ncicj \u27e8K0(xi \u2212xj)y, y\u27e9\u22650\n(25)\nfor all \ufb01nite sequences {xi}i=1...N in X, {ci}i=1...N in C and y \u2208Y. The\nfact that conditions (20) and (25) are equivalent for abelian groups is a\nconsequence of [10, Lemma 3.1]. In the following, assuming (20), we give a\nproof simpler than the one provided in [14, 15].\nTheorem 5. If Q : B( \u02c6X) \u2212\u2192L(Y) is a positive operator valued measure,\nthen\nK(x, t) =\nZ\n\u02c6\nX\n\u03c7(t \u2212x)dQ(\u03c7)\n(26)\nis a translation invariant Y-Mercer kernel on X. Conversely, if K is a trans-\nlation invariant Y-Mercer kernel on X, then there exists a unique positive\noperator valued measure Q such that (26) holds.\nWe say that Q in (26) is the positive operator valued measure associated\nto the translation invariant Mercer kernel K.\n31\nProof of Theorem 5. If Q : B( \u02c6X) \u2212\u2192L(Y) is a positive operator valued\nmeasure, by Neumark dilation theorem [24] there exist a separable Hilbert\nspace H, a projection valued measure P : B( \u02c6X) \u2212\u2192L(H) and a bounded\noperator A : H \u2212\u2192Y such that\nQ( \u02c6Z) = AP( \u02c6Z)A\u2217\n\u2200\u02c6Z \u2208B( \u02c6X).\n(27)\nLet \u03c0 be the continuous unitary representation of X acting on H given by\n\u03c0(x) =\nZ\n\u02c6\nX\n\u03c7(x)dP(\u03c7),\n(28)\nsee [16]. Eq. (26) then becomes K(x, t) = A\u03c0t\u2212xA\u2217, so that K is a translation\ninvariant Mercer kernel by Proposition 12 and Lemma 1.\nConversely, by Proposition 12 and Lemma 1, every translation invariant Mer-\ncer kernel is of the form K(x, t) = A\u03c0t\u2212xA\u2217for some continuous unitary\nrepresentation \u03c0 of X in a separable Hilbert space H and some bounded\noperator A : H \u2212\u2192Y. By SNAG theorem [16], there is then a projection\nvalued measure P : B( \u02c6X) \u2212\u2192L(H) such that (28) holds and (26) follows\nde\ufb01ning the POVM Q as in (27).\nFinally, uniqueness of Q follows from\n\u27e8K0(x)y, y\u2032\u27e9=\nZ\n\u02c6\nX\n\u03c7(x)dQy,y\u2032(\u03c7) = F(Qy,y\u2032)(x)\nby injectivity of Fourier transform of measures on \u02c6X.\nThe next proposition is a useful tool to construct translation invariant\nMercer kernels.\nTheorem 6. Let \u02c6\u03bd be a measure on \u02c6X and A : L2( \u02c6X, \u02c6\u03bd; Y) \u2192Y be a bounded\noperator. For all y, y\u2032 \u2208Y let\n\u27e8K(x, t)y, y\u2032\u27e9=\nZ\n\u02c6\nX\n\u03c7(t \u2212x) \u27e8(A\u2217y)(\u03c7), (A\u2217y\u2032)(\u03c7)\u27e9d\u02c6\u03bd(\u03c7).\n(29)\nThen K is a translation invariant Mercer kernel and the corresponding re-\nproducing kernel Hilbert space is embedded in L2( \u02c6X, \u02c6\u03bd; Y) by means of the\nfeature operator W : L2( \u02c6X, \u02c6\u03bd; Y) \u2192HK\n(W \u02c6f)(x) = A \u02c6f x\nwhere\n\u02c6f x(\u03c7) = \u03c7(x) \u02c6f(\u03c7)\n(30)\nD\n(W \u02c6f)(x), y\nE\n=\nZ\n\u02c6\nX\n\u03c7(x)\nD\n\u02c6f(\u03c7), (A\u2217y)(\u03c7)\nE\nd\u02c6\u03bd(\u03c7).\nConversely, any translation invariant Mercer kernel is of the above form for\nsome positive measure \u02c6\u03bd and bounded operator A : L2( \u02c6X, \u02c6\u03bd; Y) \u2192Y.\n32\nProof. If \u02c6\u03bd is a measure on \u02c6X and A : L2( \u02c6X, \u02c6\u03bd; Y) \u2192Y is a bounded operator,\nthen\nD\nQ( \u02c6Z)y, y\u2032E\n=\nZ\n\u02c6Z\n\u27e8(A\u2217y) (\u03c7), (A\u2217y\u2032) (\u03c7)\u27e9d\u02c6\u03bd(\u03c7)\n\u2200\u02c6Z \u2208B( \u02c6X), y, y\u2032 \u2208Y\nde\ufb01nes a positive operator valued measure Q : B( \u02c6X) \u2212\u2192L(Y), since Q( \u02c6Z) =\nAP( \u02c6Z)A\u2217where P( \u02c6Z) is the multiplication by the characteristic function of\n\u02c6Z. The kernel K given in (29) is then the translation invariant Mercer kernel\nassociated to Q by (26). To prove (30), set\n\u03b3x : Y \u2212\u2192L2( \u02c6X, \u02c6\u03bd; Y)\n(\u03b3xy) (\u03c7) = \u03c7(x)(A\u2217y)(\u03c7),\nso that K(x, t) = \u03b3\u2217\nx\u03b3t and\nD\n\u03b3\u2217\nx \u02c6f, y\nE\n=\nD\n\u02c6f, \u03b3xy\nE\n2 =\nZ\n\u02c6\nX\nD\n\u02c6f(\u03c7), \u03c7(x)(A\u2217y)(\u03c7)\nE\nd\u02c6\u03bd(\u03c7)\n=\nZ\n\u02c6\nX\n\u03c7(x)\nD\n\u02c6f(\u03c7), (A\u2217y)(\u03c7)\nE\nd\u02c6\u03bd(\u03c7) =\nD\nA \u02c6f x, y\nE\nfor all \u02c6f \u2208L2( \u02c6X, \u02c6\u03bd; Y).\nConversely, assume that K is a translation invariant Mercer kernel. We \ufb01rst\nconsider the case that Y is in\ufb01nite-dimensional. Propositions 11 and 12 show\nthat K is of the form K(x, t) = A\u03c0t\u2212xA\u2217for some unitary continuous repre-\nsentati\u00a1on \u03c0 acting on a separable Hilbert space H and a bounded operator\nA : H \u2192Y.\nA basic result of commutative harmonic analysis (see [16]) ensures that, for\neach n \u2208N\u2217:= N \u222a{\u221e}, there exist a complex separable Hilbert space Yn\nof dimension n, and a measurable subset \u02c6Xn of \u02c6X endowed with a positive\nmeasure \u02c6\u03bdn such that the \u02c6Xn are disjoint and cover \u02c6X. Without loss of gen-\nerality, we can assume that \u02c6\u03bdn( \u02c6Xn) \u22642\u2212n and \u02c6\u03bd\u221e( \u02c6X\u221e) \u22641. Moreover there\nexists a unitary operator U : H \u2192L\nn L2( \u02c6Xn, \u02c6\u03bdn, Yn) such that\n(U\u03c0xU\u2217\u02c6fn)(\u03c7) = \u03c7(x) \u02c6fn(\u03c7)\n\u02c6fn \u2208L2( \u02c6Xn, \u02c6\u03bdn, Yn) .\nFor each n \u2208N\u2217, let Jn : Yn \u2192Y be a \ufb01xed isometry, which always exists\nsince Y is in\ufb01nite dimensional, and consider the Hilbert space L2( \u02c6X, \u02c6\u03bd; Y),\nwhere \u02c6\u03bd = P\nn \u02c6\u03bdn, which is a bounded measure by assumption on \u02c6\u03bdn. De\ufb01ne\nthe isometry V : H \u2192L2( \u02c6X, \u02c6\u03bd; Y) as\n(V u)(\u03c7) = Jn(Uv)(\u03c7)\n\u03c7 \u2208\u02c6Xn.\nA simple calculation shows that\n\u03c0x = V \u2217\u02c6\u03bbxV\n33\nwhere (\u02c6\u03bbx \u02c6f)(\u03c7) = \u03c7(x) \u02c6f(\u03c7) is the diagonal representation on L2( \u02c6X, \u02c6\u03bd; Y).\nNow\nK(x, t) = A\u03c0t\u2212xA\u2217= AV \u2217\u02c6\u03bbt\u2212xV A\u2217.\nRede\ufb01ning A = AV \u2217, (29) is a consequence of the explicit form of \u02c6\u03bbx.\nIf Y is \ufb01nite dimensional, let (\u02c6\u03bd, B) be the pair associated to K as in\nProposition 13 below. Eq. (29) follows de\ufb01ning A : L2( \u02c6X, \u02c6\u03bd, Y) \u2192Y\nD\nA \u02c6f, y\nE\n=\nZ\n\u02c6\nX\nD\nB(\u03c7)\n1\n2 \u02c6f(\u03c7), y\nE\nd\u02c6\u03bd(\u03c7).\nIf Y = Cm, K(x, t) can be regarded as a m\u00d7m-matrix and A is uniquely\nde\ufb01ned by a family of functions \u02c6f1, . . . , \u02c6fm \u2208L2( \u02c6X, \u02c6\u03bd; Y) through A\u2217ei = \u02c6fi.\nHence, (29) becomes\nK(t \u2212x)ij =\nZ\n\u02c6\nX\n\u03c7(t \u2212x) \u27e8fj(\u03c7), fi(\u03c7)\u27e9d\u02c6\u03bd(\u03c7)\ni, j = 1, . . . , m.\n(31)\nAs an application, we give the following example that generalizes the one\ngiven in [5].\nExample 10. Let X = Rd, regarded as vector abelian group, and Y = Cm.\nThe dual group is isomorphic to Rd by means of \u03c7p(x) = ei2\u03c0x\u00b7p. Let \u02c6\u03bd = dp\nbe the Lebesgue measure on Rd and\n\u02c6fi(p) =\n1\n(2\u03c0)d/4 e\u2212\u03c32\ni\n|p|2\n2\nvi\nvi \u2208Y, \u03c3i > 0,\nthen the translation invariant Mercer kernel given by (31) is\nK(t \u2212x)ij =\nZ\nRd ei2\u03c0(t\u2212x)\u00b7p \u27e8fj(p), fi(p)\u27e9dp\n=\n1\n(\u03c32\ni + \u03c32\nj)d/2 e\n\u22122\u03c02 |x\u2212t|2\n\u03c32\ni +\u03c32\nj \u27e8vj, vi\u27e9.\nThe example in [5] corresponds to the choice vi = vj and \u03c3i = \u03c3j for any\ni, j = 1, . . . , m.\nTheorems 5 and 6 give two di\ufb00erent characterizations of a translation\ninvariant kernel K, but the POVM Q de\ufb01ning K through (26) is always\nunique, whereas there are many pairs (\u02c6\u03bd, A) de\ufb01ning the same K by (29).\nThese two descriptions are related observing that, given a pair (\u02c6\u03bd, A), the\nscalar bounded measure Qy,y\u2032 has density \u27e8(A\u2217y)(\u03c7), (A\u2217y\u2032)(\u03c7)\u27e9with respect\n34\nto \u02c6\u03bd for any y, y\u2032 \u2208Y. On the other hand, given the POVM Q, let \u02c6\u03bdQ be the\nbounded positive measure de\ufb01ned by\n\u02c6\u03bdQ( \u02c6Z) =\nX\nn\n2\u2212n \u2225yn\u2225\u22122n D\nQ( \u02c6Z)yn, yn\nE\n\u2200\u02c6Z \u2208B( \u02c6X)\n(32)\nwhere {yn}n\u2208N is a dense sequence in Y. Clearly, given \u02c6Z \u2208B( \u02c6X), \u02c6\u03bdQ( \u02c6Z) = 0\nif and only if Q( \u02c6Z) = 0, and \u02c6\u03bdQ is uniquely de\ufb01ned by Q up to an equivalence.\nMoreover, by Neumark dilation theorem, see (27), there exists an operator\nAQ : L2( \u02c6X, \u02c6\u03bdQ; Y) \u2192Y such that the pair (\u02c6\u03bdQ, AQ) gives the kernel K\nassociated with Q.\nWe notice that in general it is not true that the POVM Q has an operator\nvalued density. We recall that Q has operator density if there exists a map\nB : \u02c6X \u2212\u2192L(Y) and a positive measure \u02c6\u03bd such that \u27e8B(\u00b7)y, y\u2032\u27e9\u2208L1( \u02c6X, \u02c6\u03bd)\nfor all y, y\u2032 \u2208Y and\nZ\n\u02c6Z\n\u27e8B(\u03c7)y, y\u2032\u27e9d\u02c6\u03bd(\u03c7) = Qy,y\u2032( \u02c6Z)\n\u2200\u02c6Z \u2208B( \u02c6X).\n(33)\nThe following proposition will characterize the kernels having a POVM with\nan operator density. To prove the result, we need the following technical\nlemma.\nLemma 2. Let \u02c6\u03bd be a positive measure on \u02c6X and B : \u02c6X \u2212\u2192L(Y) such that\n\u27e8B(\u00b7)y, y\u2032\u27e9\u2208L1( \u02c6X, \u02c6\u03bd) for all y, y\u2032 \u2208Y. Then, the sesquilinear form\nY \u00d7 Y \u2192L1( \u02c6X, \u02c6\u03bd),\n(y, y\u2032) 7\u2192\u27e8B(\u00b7)y, y\u2032\u27e9\n(34)\nis continuous.\nProof. For \ufb01xed y \u2208Y [resp. y\u2032 \u2208Y] the map y\u2032 7\u2192\u27e8B(\u00b7)y, y\u2032\u27e9[resp. y 7\u2192\n\u27e8B(\u00b7)y, y\u2032\u27e9] is continuous from Y into L1( \u02c6X, \u02c6\u03bd) by the closed graph theorem,\ni.e. the application de\ufb01ned in (34) is separately continuous in y and y\u2032. So,\nthe closed graph theorem again assures the joint continuity.\nProposition 13. Let \u02c6\u03bd be a positive measure on \u02c6X and B : \u02c6X \u2212\u2192L(Y)\nsuch that \u27e8B(\u00b7)y, y\u2032\u27e9\u2208L1( \u02c6X, \u02c6\u03bd) for all y, y\u2032 \u2208Y and B(\u03c7) \u22650 for \u02c6\u03bd-almost\nall \u03c7. Then\nK(x, t) =\nZ\n\u02c6\nX\n\u03c7(t \u2212x)B(\u03c7) d\u02c6\u03bd(\u03c7),\n(35)\nis a translation invariant Mercer kernel, and the space HK is embedded in\nL2( \u02c6X, \u02c6\u03bd; Y) by means of the feature operator\n(W \u02c6f)(x) =\nZ\n\u02c6\nX\n\u03c7(x)B(\u03c7)\n1\n2 \u02c6f(\u03c7)d\u02c6\u03bd(\u03c7),\n(36)\n35\nwhere both the above integrals converge in the weak sense.\nIf Y is \ufb01nite dimensional or X is compact, any translation invariant kernel\nis of the above form for some pair (\u02c6\u03bd, B).\nIf Y = C, one can always assume that B = 1 and \u02c6\u03bd is a bounded positive\nmeasure.\nProof. Let \u02c6\u03bd and B as in the assumptions. Given a Borel subset \u02c6Z of \u02c6X\nde\ufb01ne Q( \u02c6Z) as the unique bounded operator satisfying\nD\nQ( \u02c6Z)y, y\u2032E\n=\nZ\n\u02c6Z\n\u27e8B(\u03c7)y, y\u2032\u27e9d\u02c6\u03bd(\u03c7).\nThe fact that Q( \u02c6Z) is a bounded operator follows from Lemma 2 and from\nthe continuity of the map L1( \u02c6X, \u02c6\u03bd) \u220b\u03c6 7\u2192\nR\n\u02c6Z \u03c6(\u03c7)d\u02c6\u03bd(\u03c7) \u2208C.\nClearly,\nQ( \u02c6Z) is a positive operator and monotone convergence theorem implies that\n\u02c6Z 7\u2192Q( \u02c6Z) is a POVM on \u02c6X. By construction K(x, t) =\nR\n\u02c6\nX \u03c7(t \u2212x)dQ(\u03c7),\nso K is a translation invariant Mercer kernel by Theorem 5. Setting\n\u03b3x : Y \u2212\u2192L2( \u02c6X, \u02c6\u03bd; Y)\n(\u03b3xy) (\u03c7) = \u03c7(x)B(\u03c7)1/2y,\nwe see that K(x, t) = \u03b3\u2217\nx\u03b3t and\nD\n\u03b3\u2217\nx \u02c6f, y\nE\n=\nD\n\u02c6f, \u03b3xy\nE\n2 =\nZ\n\u02c6\nX\nD\n\u02c6f(\u03c7), \u03c7(x)B(\u03c7)1/2y\nE\nd\u02c6\u03bd(\u03c7)\n=\nZ\n\u02c6\nX\n\u03c7(x)\nD\nB(\u03c7)1/2 \u02c6f(\u03c7), y\nE\nd\u02c6\u03bd(\u03c7)\nfor all \u02c6f \u2208L2( \u02c6X, \u02c6\u03bd; Y), from which (36) follows.\nAssume now that Y is \ufb01nite dimensional or X is compact and K is a trans-\nlation invariant Mercer kernel. Theorem 5 ensures that there exists a POVM\nQ on \u02c6X taking value in Y such that K(x, t) =\nR\n\u02c6\nX \u03c7(t \u2212x)dQ(\u03c7). If X is\ncompact, \u02c6X is discrete. Let \u02c6\u03bd be the counting measure and B(\u03c7) = Q({\u03c7})\nfor all \u03c7 \u2208\u02c6X, then (\u02c6\u03bd, B) satis\ufb01es the required properties.\nIf Y is \ufb01nite dimensional, choose \u02c6\u03bdQ as in (32).\nIt follows that for any\ny, y\u2032 \u2208Y, the complex measure Qy,y\u2032 has density by,y\u2032 \u2208L1( \u02c6X, \u02c6\u03bdQ) with\nrespect to \u02c6\u03bdQ.\nIn particular, by,y(\u03c7) \u22650 for \u02c6\u03bdQ-almost all \u03c7 \u2208\u02c6X.\nLet\ny1, . . . , yN be a basis of Y and by linearity extend byi,yj \u2208L1( \u02c6X, \u02c6\u03bdQ) to a map\nB : \u02c6X \u2192L(Y), which clearly satis\ufb01es the required properties.\nIf Y = C, the claim is clear.\nIf Y = C, Proposition 13 is already given in [22].\nWe end by showing a su\ufb03cient condition ensuring that a translation in-\nvariant Mercer kernel is of the form given in Proposition 13.\n36\nProposition 14. Let K be a translation invariant Mercer kernel. Suppose\nthat \u27e8K0(\u00b7)y, y\u2032\u27e9\u2208L1(X, dx) for all y, y\u2032 \u2208Y. Let\n\u27e8B(\u03c7)y, y\u2032\u27e9:=\nZ\nX\n\u03c7(x) \u27e8K0(x)y, y\u2032\u27e9dx\n\u2200y, y\u2032 \u2208Y.\n(37)\nThen\n(i) B(\u03c7) is a bounded nonnegative operator for all \u03c7 \u2208\u02c6X;\n(ii) \u27e8B(\u00b7)y, y\u2032\u27e9\u2208L1(X, dx) for all y, y\u2032 \u2208Y;\n(iii) for all x, t \u2208X,\nK(x, t) =\nZ\n\u02c6\nX\n\u03c7(t \u2212x)B(\u03c7)d\u03c7,\n(38)\nwhere the integral converges in the weak sense.\nProof. The operator B(\u03c7) de\ufb01ned in (37) is bounded as a consequence of\nLemma 2 (applied to K0) and of the continuity of the map L1(X, dx) \u220b\u03c6 7\u2192\nF(\u03c6)(\u03c7) \u2208C.\nSince \u27e8K0(\u00b7)y, y\u27e9is a function of positive type, by Fourier inversion theorem\n\u27e8B(\u00b7)y, y\u27e9\u2208L1( \u02c6X, d\u03c7), and\n\u27e8K0(x)y, y\u27e9=\nZ\n\u02c6\nX\n\u03c7(x) \u27e8B(\u03c7)y, y\u27e9d\u03c7,\nwhich is (38).\n5.2\nUniversality\nIn this section we study the universality problem for translation invariant ker-\nnels on an abelian group in terms of the characterization given by Theorem 5\nand Proposition 13. The assumptions and notations are as in Section 5.1.\nTo state the following result, we recall that the support of a POVM Q is the\ncomplement of the largest open subset U such that Q(U) = 0.\nProposition 15. Let K be a translation invariant Mercer kernel, and Q its\nassociated positive operator valued measure. If the RKHS HK is dense in\nL2(X, \u00b5; Y) for any probability measure \u00b5, then supp(Q) = \u02c6X.\nProof. Suppose there is an open set U \u2282\n\u02c6X such that Q(U) = 0.\nLet\n\u03c70 \u2208U, so that \u03c70U\u22121 is a neighborhood of the identity element of \u02c6X. Let\n37\n\u00b5 be a probability measure5 on X such that supp F(\u00b5) \u2282\u03c70U\u22121 and set\n\u03d5(x) = \u03c70(x)y with y \u2208Y \\ {0}. Then (26) gives\n\u27e8L\u00b5\u03d5, \u03d5\u27e9=\nZ\nX\nZ\nX\nZ\n\u02c6\nX\n\u03c7(t \u2212x)\u03c70(x)\u03c70(t)dQy,y(\u03c7)d\u00b5(x)d\u00b5(t)\n=\nZ\n\u02c6\nX\n\f\fF(\u00b5)(\u03c70\u03c7\u22121)\n\f\f2 dQy,y(\u03c7) = 0.\nThis shows that L\u00b5 is not injective, i.e. K is not universal.\nWe now characterize the universality of the kernels de\ufb01ned in terms of\nthe pair (\u02c6\u03bd, B) by means of (35).\nProposition 16. Given a positive measure \u02c6\u03bd on \u02c6X and B : \u02c6X \u2212\u2192L(Y)\nsuch that \u27e8B(\u00b7)y, y\u2032\u27e9\u2208L1( \u02c6X, \u02c6\u03bd) for all y, y\u2032 \u2208Y and B(\u03c7) \u22650 for \u02c6\u03bd-almost\nall \u03c7, let K be the translation invariant Mercer kernel given by (35).\n(i) If HK is dense in L2(X, \u00b5; Y) for any probability measure \u00b5, then both\nsupp \u02c6\u03bd = \u02c6X and supp B = \u02c6X .\n(ii) If supp \u02c6\u03bd = \u02c6X and B(\u03c7) is injective for \u02c6\u03bd-almost all \u03c7 \u2208\u02c6X, then HK\nis dense in L2(X, \u00b5; Y) for any probability measure \u00b5.\nIn the case X is compact also the converse holds true.\n(iii) If Y = C and B = 1, HK is dense in L2(X, \u00b5; Y) for any probability\nmeasure \u00b5 if and only if supp \u02c6\u03bd = \u02c6X.\nProof. Item (i) follows from Proposition 15 and (33).\nLet now \u00b5 be a probability measure on X. Using (35), we have\n\u27e8L\u00b5\u03d5, \u03d5\u27e9=\nZZZ\n\u03c7(t \u2212x) \u27e8B(\u03c7)\u03d5(t), \u03d5(x)\u27e9d\u02c6\u03bd(\u03c7)d\u00b5(x)d\u00b5(t)\n=\nZ\n\u02c6\nX\n\nB(\u03c7)F(\u03d5\u00b5)(\u03c7\u22121), F(\u03d5\u00b5)(\u03c7\u22121)\n\u000b\nd\u02c6\u03bd(\u03c7).\n(39)\n(ii) If B(\u03c7) is injective for almost all \u03c7 \u2208\u02c6X and supp \u02c6\u03bd = \u02c6X, then, by\nthe above equation, positivity of B(\u03c7) and the injectivity of Fourier\ntransform, L\u00b5\u03d5 \u0338= 0 if \u03d5 \u0338= 0 in L2(X, \u00b5; Y). Therefore, HK is dense in\nL2(X, \u00b5; Y) for any probability measure \u00b5.\nSuppose X is compact, so that\n\u02c6X is discrete.\nIf HK is dense in\n5For example, if V is a compact symmetric neighborood of the identity of \u02c6X such\nthat V 2 \u2282\u03c70U \u22121, let h = 1V \u22171V , so that (up to a constant) the measure d\u00b5(x) =\nF\u22121(h)(x)dx =\n\f\fF\u22121(1V )(x)\n\f\f2 dx has the required property.\n38\nL2(X, \u00b5; Y) for any probability measure \u00b5, supp \u02c6\u03bd = \u02c6X by item (i).\nIf \u03c70 \u2208\u02c6X and y \u2208ker B(\u03c70), choose d\u00b5(x) = dx and \u03d5(x) = \u03c70(x)y,\nso that F(\u03d5\u00b5)(\u03c7) = \u03b4\u03c7,\u03c7\u22121\n0 y. We thus have\n\u27e8L\u00b5\u03d5, \u03d5\u27e9= \u27e8B(\u03c70)y, y\u27e9\u02c6\u03bd(\u03c70) = 0.\nSince L\u00b5 is injective, this implies \u03d5 = 0, i.e. y = 0.\n(iii) Since B = 1, the \u2018if\u2019 part is clear from item (ii). The converse follows\nby item (i).\nBy inspecting the proofs of Propositions 15 and 16, one can easily replace\nL2(X, \u00b5; Y) with any Lp(X, \u00b5; Y), 1 \u2264p < \u221e, in the statements. The same\nholds for Corollary 6 below.\nRemark 3. If the translation invariant kernel K is C0, then Propositions 15\nand 16 characterize universality of K.\nRemark 4. If X = Rd, Y = C, and supp \u02c6\u03bd is a subset of \u02c6X = Rd such\nthat every entire function on Cd vanishing on it is identically zero, then K\nis \u03ba-universal (see [22, Proposition 14]). This follows by (39), taking into\naccount that, for compactly supported \u00b5, the Fourier transform of \u03d5\u00b5 can\nbe extended to an entire function de\ufb01ned on Cd.\nIn particular, if d = 1 a su\ufb03cient condition for \u03ba-universality is that supp \u02c6\u03bd\nhas an accumulation point.\nBased on the above remark, we give another example of compact-universal\nkernel, which is not universal, see also Example 8.\nExample 11. Let K : R \u00d7 R \u2192C be the C0-kernel\nK (x, t) =\nZ 1\n\u22121\ne2\u03c0i(t\u2212x)pdp = sin 2\u03c0(t \u2212x)\n\u03c0(t \u2212x)\n,\nwith \u02c6\u03bd the restriction of the Lebesgue measure to [\u22121, 1]. Since the support of\n\u02c6\u03bd admits an accumulation point, K is compact-universal by the last remark.\nOn the other hand since supp \u02c6\u03bd is not the whole R, K is not universal by\nProposition 16.\nWe now exhibit a particular case in which Proposition 16 applies.\nCorollary 6. Let K be a translation invariant Mercer kernel such that\n\u27e8K0(\u00b7)y, y\u2032\u27e9\u2208L1(X, dx) for all y, y\u2032 \u2208Y. Let B : \u02c6X \u2212\u2192L(Y) be as in\n(37). If B(\u03c7) is injective for d\u03c7-almost all \u03c7, then the reproducing kernel\nHilbert space HK is dense in L2(X, \u00b5; Y) for any probability measure \u00b5.\nProof. Since the support of the Haar measure d\u03c7 is \u02c6X, the claim is then a\nconsequence of Proposition 16.\n39\n6\nExamples of universal kernels\nIn this section we present various examples of universal kernels, some of them\nhas been already introduced in Section 3.\nWe start with the gaussian kernel, which is a well known example of uni-\nversal kernel. The \ufb01rst proof about universality is given [30] with a di\ufb00erent\ntechnique and in [22] by means of the Fourier transform. In both paper only\ncompact-universality is taken into account.\nExample 12. Let X be a closed subset of Rd, Y = C and\n\u03ba(x, t) = e\u2212\u2225x\u2212t\u22252\n2\u03c32\nx, t \u2208X,\nwhere \u03c3 > 0. Then K is a C0-universal kernel.\nProof. Assume \ufb01rst that X = Rd, regarded as abelian group, then \u03ba is trans-\nlation invariant kernel with \u03ba0 in C0(Rd) \u2229L1(Rd, dx). According to (37)\nB(p) =\np\n(2\u03c0\u03c32)d e\u22122\u03c02\u03c32\u2225p\u22252\nwhere the dual group is identi\ufb01ed with Rd by means of \u03c7p(x) = ei2\u03c0p\u00b7x. Since\nB(p) > 0 for all p \u2208Rd, universality is a consequence of Corollary 6.\nIf X is an arbitrary closed subset of Rd it is enough to apply Corollary 3.\nNext example is well known in functional analysis (see, for example, [3]).\nExample 13. Let X = R, Y = C and let\n\u03ba(x, t) = e\u2212\u03c0|x\u2212t| .\nThen the kernel \u03ba is a C0-universal kernel and H\u03ba = W 1 (R), the Sobolev\nspace of measurable complex functions f on R with \ufb01nite norm\n\u2225f\u22252\nW 1 =\nZ\nX\nh\n|f(x)|2 + |f \u2032(x)|2i\ndx,\nwhere f \u2032 is the weak derivative.\nProof. The same reasoning as above, observing that B(p) =\n2\n\u03c0+4\u03c0p2 > 0 for\nall p \u2208R.\nNext example characterizes universal kernels of the form K = \u03baB \u2013 see\nExample 5.\n40\nExample 14. Let \u03ba be a C0-scalar reproducing kernel and B a positive op-\nerator. The kernel K = \u03baB is universal if and only if \u03ba is universal and B\nis injective.\nProof. We have to show that, given a probability measure \u00b5, H\u03baB is dense\nin L2(X, \u00b5; Y). The space H\u03baB is unitarily equivalent to H\u03ba \u2297ker B\u22a5by\nmeans of W(\u03d5 \u2297y)(x) = \u03d5(x)B\n1\n2y, see Example 5. Hence, it is enough to\nprove that H\u03ba \u2297B\n1\n2Y is dense in L2(X, \u00b5) \u2297Y. This is the case if and only\nif H\u03ba is dense in L2(X, \u00b5) and B\n1\n2 has dense range, and this last condition is\nequivalent to the fact that B is injective since B is a positive operator.\nThe same result holds replacing C0-kernel with Mercer kernel and univer-\nsality with compact-universality.\nExample 15. Let \u03ba : X \u00d7 X \u2192C and \u03ba\u2032 : X\u2032 \u00d7 X\u2032 \u2192C be two scalar C0\nreproducing kernels on X and X\u2032, respectively. Let I\u2032 be the identity operator\non H\u03ba\u2032.\n(i) The H\u03ba\u2032-kernel K = \u03baI\u2032 if universal if and only if \u03ba is universal.\n(ii) Fixed a probability measure \u00b5\u2032 on X\u2032, the L2(X\u2032, \u00b5\u2032)-kernel bK = \u03baL\u00b5\u2032\nis universal if and only if \u03ba is universal and H\u03ba\u2032 is dense in L2(X\u2032, \u00b5\u2032).\n(iii) The scalar kernel \u03ba \u00d7 \u03ba\u2032 is universal if both \u03ba and \u03ba\u2032 are universal.\nProof. Items (i) and (ii) follow immediately from Example 14 and Propo-\nsition 10. Item (iii) is a consequence of Proposition 9 and the density of\nC0(X) \u2297C0(X\u2032) in C0(X \u00d7 X\u2032).\nThe following class of examples is considered in [5].\nExample 16. Let X be a locally compact second countable abelian group.\nLet {Bi}\nN\ni=1 be a \ufb01nite set of positive operators on Y and {\u03bai\n0}\nN\ni=1 be a \ufb01nite\nset of scalar functions of positive type in C0(X) \u2229L1(X, dx). The translation\ninvariant kernel K\nK(x, t) =\nN\nX\ni=1\n\u03bai\n0(x \u2212t)Bi\nis universal provided that \u2229ikerBi = {0} and, for each i = 1, . . . N, there is\nan open dense subset \u02c6Zi \u2282\u02c6X such that F(\u03bai\n0) > 0 on \u02c6Zi.\n41\nProof. Clearly, \u27e8K0(\u00b7)y, y\u2032\u27e9is in L1(X, dx). Moreover, according to (37), for\nall y \u2208Y and \u03c7 \u2208\u02c6X\nB(\u03c7)y =\nN\nX\ni=1\nF(\u03bai\n0)(\u03c7\u22121)Biy.\nEach bZi is open and dense, hence bZ = \u2229bZi is dense in \u02c6X. Let \u03c7 \u2208\u02c6Z and\ny \u2208Y such that B(\u03c7)y = 0; then Biy = 0 for all i = 1, . . . , N, since every\nBi is a positive operator and F(\u03bai\n0) > 0 on \u02c6Zi, so that by assumption y = 0.\nTherefore, K is universal by Corollary 6.\nA\nVector valued measures\nIn this appendix we describe the dual of C0(X; Y). For Y = C, it is a well\nknown result that C0(X)\u2217can be identi\ufb01ed with the Banach space of complex\nmeasures on X. For arbitrary Y, a similar result holds by considering the\nspace of vector measures. If X is compact, this result is due to [28] and we\nslightly extend it to X being only locally compact. The proof we give is\nsimpler than the original one also for X compact.\nMoreover, by using a version of Radon-Nikodym theorem for vector valued\nmeasures, it is possible to describe the dual of C0(X; Y) in a simpler way.\nIndeed, the following result holds.\nTheorem 7. Let T \u2208C0(X; Y)\u2217. There exists a unique probability measure\n\u00b5 on X and a unique function h \u2208L\u221e(X, \u00b5; Y) such that\nT(f) =\nZ\nX\n\u27e8f(x), h(x)\u27e9d\u00b5(x)\nf \u2208C0(X; Y)\n(40)\nwith \u2225h(x)\u2225= \u2225T\u2225for \u00b5-almost all x \u2208X.\nProof. It follows combining Theorems 8 and 9 below.\nObserve that, given \u00b5 and h as in the statement of the theorem, if we\nde\ufb01ne T by (40), then T \u2208C0(X; Y). Hence (40) completely characterizes\nthe dual of C0(X; Y) in terms of pairs (\u00b5, h).\nTo prove the theorem, we recall some basic facts from the theory of vector\nvalued measures (see [11, 19]). If A \u2208B(X), we denote by \u03a0(A) the family\nof partitions of A into \ufb01nite or denumerable disjoint Borel subsets.\nDe\ufb01nition 3. A vector measure on X with values in Y is a mapping M :\nB(X) \u2212\u2192Y such that\n42\n(i)\nsup\n{Ai}\u2208\u03a0(X)\nX\ni\n\u2225M(Ai)\u2225< \u221e;\n(ii) for all A \u2208B(X) and {Ai} \u2208\u03a0(A)\nM(A) =\nX\ni\nM(Ai)\nwhere the sum converges absolutely by item (i).\nIf M is a Y-valued vector measure on X, for all A \u2208B(X) we de\ufb01ne\n|M|(A) =\nsup\n{Ai}\u2208\u03a0(A)\nX\ni\u2208I\n\u2225M(Ai)\u2225.\nThen, |M| is a bounded positive measure on X, called the total variation of\nM.\nThe integration of a function f \u2208L1(X, |M|; Y) with respect to M is\nde\ufb01ned as it follows. Let St(X; Y) be the space of functions f = Pn\ni=1 1Aivi,\nwith Ai disjoint Borel sets and vi \u2208Y (1A is the characteristic function of\nthe set A). For such f\u2019s, de\ufb01ne\nZ\nX\n\u27e8f(x), dM(x)\u27e9:=\nn\nX\ni=1\n\u27e8vi, M(Ai)\u27e9.\n(41)\nSince \f\f\f\f\f\nn\nX\ni=1\n\u27e8vi, M(Ai)\u27e9\n\f\f\f\f\f \u2264\nX\ni\n\u2225vi\u2225\u2225M(Ai)\u2225\u2264\nX\ni\n|M|(Ai) \u2225vi\u2225= \u2225f\u22251 ,\nthe integral (41) extends to a bounded functional on L1(X, |M|; Y), which is\ndenoted again by\nR\nX \u27e8f(x), dM(x)\u27e9. By Theorem 4.1 in [19], then there exists\nh \u2208L\u221e(X, |M|; Y) such that\nZ\n\u27e8f(x), dM(x)\u27e9=\nZ\n\u27e8f(x), h(x)\u27e9d|M|(x)\n\u2200f \u2208L1(X, |M|; Y),\nand \u2225h(x)\u2225= 1 for |M|-almost all x. These facts are collected in the following\ntheorem.\nTheorem 8 (Radon-Nikodym). If M is a Y-valued vector measure on X,\nthere exists a unique |M|-measurable function h : X \u2212\u2192Y such that \u2225h(x)\u2225=\n1 for |M|-almost all x and\nZ\nX\n\u27e8f(x), dM(x)\u27e9=\nZ\nX\n\u27e8f(x), h(x)\u27e9d|M|(x)\n\u2200f \u2208L1(X, |M|; Y).\n43\nThe function h is called the density of M with respect to |M|.\nWe denote by M(X; Y) the space of Y-valued vector measures on X. The\nspace M(X; Y) is a Banach space with respect to the norm\n\u2225M\u2225= |M|(X)\n(see [11]). If Y = C, we let M(X) = M(X; C). The next duality theorem is\nshown in [28] for X compact \u2013 see also [11].\nTheorem 9. If C0(X; Y) is endowed with the Banach space topology induced\nby the uniform norm, then C0(X; Y)\u2217= M(X; Y), the duality being given by\n\u27e8f, M\u27e9=\nZ\nX\n\u27e8f(x), dM(x)\u27e9\n\u2200f \u2208C0(X; Y), M \u2208M(X; Y).\nProof. By Theorem 8, it is clear that, if M \u2208M(X; Y), then\nTM(f) =\nZ\nX\n\u27e8f(x), dM(x)\u27e9=\nZ\nX\n\u27e8f(x), h(x)\u27e9d|M|(x)\nde\ufb01nes a bounded functional TM on C0(X; Y).\nClearly \u2225TM\u2225\u2264\u2225M\u2225. To show that \u2225TM\u2225= \u2225M\u2225, \ufb01x by Lusin theorem\na function g \u2208C0(X; Y) such that g(x) = h(x) for x \u2208X \\ Z, Z being a\n|M|-measurable set with |M|(Z) < \u01eb, and \u2225g\u2225\u221e\u2264\u2225h\u2225|M|,\u221e= 1. For \u01eb small\nenough, we then have\n|M|(X) \u22122\u01eb < |M|(X \\ Z) \u2212|M|(Z) \u2264\n\f\f\f\f\nZ\nX\n\u27e8g(x), h(x)\u27e9d|M|(x)\n\f\f\f\f \u2264|M|(X).\nThis shows that \u2225TM\u2225= \u2225M\u2225.\nSuppose now T \u2208C0(X; Y)\u2217. For v \u2208Y, let iv : C0(X) \u2212\u2192C0(X; Y) be\nthe bounded operator given by\n[iv(\u03d5)](x) = \u03d5(x)v.\nSince Tiv \u2208C0(X)\u2217, by Riesz theorem there exists a measure \u00b5v \u2208M(X)\nsuch that\nTiv(\u03d5) =\nZ\nX\n\u03d5(x)d\u00b5v(x)\nand\n\u2225Tiv\u2225= \u2225\u00b5v\u2225.\nFor all A \u2208B(X), let M(A) be the vector in Y such that\n\u27e8v, M(A)\u27e9= \u00b5v(A)\n44\n(M(A) is well de\ufb01ned, since |\u00b5v(A)| \u2264\u2225\u00b5v\u2225= \u2225Tiv\u2225\u2264\u2225T\u2225\u2225v\u2225).\nWe now show that, if A \u2208B(X) and {Ai} \u2208\u03a0(A), then\nX\ni \u2225M(Ai)\u2225\u2264\u2225T\u2225,\nso that item (i) of De\ufb01nition 3 holds. It is enough to prove it for all \ufb01nite\npartitions {Ai}i=1...n. Let vi = M(Ai)/ \u2225M(Ai)\u2225(we set vi = 0 whenever\nM(Ai) = 0). We have\nX\ni \u2225M(Ai)\u2225=\nX\ni \u27e8vi, M(Ai)\u27e9=\nX\ni \u00b5vi(Ai).\nSet \u03bd = P\ni |\u00b5vi|, which is \u03bd a bounded positive measure, and every \u00b5vi has\ndensity with respect to \u03bd. For all i = 1 . . . n, \ufb01x a sequence {\u03d5(i)\nj }j\u2208N in Cc(X)\nsuch that limj \u03d5(i)\nj (x) = 1Ai(x) for \u03bd-almost all x. De\ufb01ne\n\u03c8j(x) =\n\"\n1 \u2228\nn\nX\nk=1\n\f\f\f\u03d5(k)\nj (x)\n\f\f\f\n#\u22121\nn\nX\ni=1\n\u03d5(i)\nj (x)vi.\nThen, \u03c8j \u2208Cc(X; Y), and \u2225\u03c8j(x)\u2225\u22641 for all x. Moreover,\n\f\f\f\f\f\f\n\"\n1 \u2228\nn\nX\nk=1\n\f\f\f\u03d5(k)\nj (x)\n\f\f\f\n#\u22121\n\u03d5(i)\nj (x)\n\f\f\f\f\f\f\n\u22641\n\u2200x, i\nand\nlim\nj\n\"\n1 \u2228\nn\nX\nk=1\n\f\f\f\u03d5(k)\nj (x)\n\f\f\f\n#\u22121\n\u03d5(i)\nj (x) = 1Ai(x)\nfor \u03bd-almost all x.\nTherefore\n\f\f\f\nX\ni \u2225M(Ai)\u2225\u2212T\u03c8j\n\f\f\f\n=\n\f\f\f\f\f\f\nX\ni\n\uf8f1\n\uf8f2\n\uf8f3\u00b5vi(Ai) \u2212Tivi\n\uf8eb\n\uf8ed\n\"\n1 \u2228\nX\nk\n\f\f\f\u03d5(k)\nj\n\f\f\f\n#\u22121\n\u03d5(i)\nj\n\uf8f6\n\uf8f8\n\uf8fc\n\uf8fd\n\uf8fe\n\f\f\f\f\f\f\n\u2264\nX\ni\n\f\f\f\f\f\f\nZ\nX\n\uf8f1\n\uf8f2\n\uf8f31Ai(x) \u2212\n\"\n1 \u2228\nX\nk\n\f\f\f\u03d5(k)\nj (x)\n\f\f\f\n#\u22121\n\u03d5(i)\nj (x)\n\uf8fc\n\uf8fd\n\uf8fed\u00b5vi(x)\n\f\f\f\f\f\f\nj\u2192\u221e\n\u2212\u21920\n45\nby dominated convergence theorem. On the other hand, |T\u03c8j| \u2264\u2225T\u2225\u2225\u03c8j\u2225\u221e\u2264\n\u2225T\u2225. It follows that Pn\ni=1 \u2225M(Ai)\u2225\u2264\u2225T\u2225, as claimed.\nWe now show that\nM(A) =\nX\ni\nM(Ai)\n(absolutely) for all A \u2208B(X) and {Ai} \u2208\u03a0(A). We have just proved that\nthe right hand side is absolutely convergent, and the equality follows by\n*\nv,\nX\ni\nM(Ai)\n+\n=\nX\ni\n\u00b5v(Ai) = \u00b5v(A) = \u27e8v, M(A)\u27e9\n\u2200v \u2208Y.\nTherefore, M is a Y-valued measure. It remains to show that T = TM.\nLet h and |M| be associated to M as in Radon-Nikodym theorem. Then, for\nany Borel set A \u2282X, we have \u00b5v(A) =\nR\nA \u27e8v, h(x)\u27e9d|M|(x), from which it\nfollows that \u00b5v has density \u27e8v, h(x)\u27e9with respect to |M|. For \u03d5 \u2208Cc(X) and\nv \u2208Y, we thus have\nT(\u03d5v) =\nZ\nX\n\u03d5(x)d\u00b5v(x) =\nZ\nX\n\u27e8\u03d5(x)v, h(x)\u27e9d|M|(x) = TM(\u03d5v).\nThen, T = TM by density of Cc(X) \u2297Y in C0(X; Y).\nAcknowledgment. This work has been partially supported by the FIRB\nproject RBIN04PARL and by the the EU Integrated Project Health-e-Child\nIST-2004-027749.\nReferences\n[1] N. Aronszajn. Theory of reproducing kernels. Trans. Amer. Math. Soc.,\n68:337\u2013404, 1950.\n[2] S. Bochner.\nLectures on Fourier integrals. With an author\u2019s supple-\nment on monotonic functions, Stieltjes integrals, and harmonic analysis.\nTranslated by Morris Tenenbaum and Harry Pollard. Annals of Math-\nematics Studies, No. 42. Princeton University Press, Princeton, N.J.,\n1959.\n[3] H. Br\u00b4ezis. Analyse fonctionnelle : th\u00b4eorie et applications. Dunod, Paris,\n1983.\n[4] A. Caponnetto and E. De Vito. Optimal rates for the regularized least-\nsquares algorithm. Found. Comput. Math., 7(3):331\u2013368, 2007.\n46\n[5] A.\nCaponnetto,\nC.\nA.\nMicchelli,\nM.\nPontil,\nand\nY.\nYing.\nUniversal\nkernels\nfor\nmulti-task\nlearning.\nJ.\nMach.\nLearn.\nRes.,\n2008\n(accepted).\nPreprint\navailable\nat\nhttp://eprints.pascal-network.org/archive/00003780/.\n[6] C. Carmeli, E. De Vito, and A. Toigo. Vector valued reproducing kernel\nHilbert spaces of integrable functions and Mercer theorem. Anal. Appl.\n(Singap.), 4(4):377\u2013408, 2006.\n[7] J. B. Conway. A course in functional analysis, volume 96 of Graduate\nTexts in Mathematics. Springer-Verlag, New York, second edition, 1990.\n[8] F. Cucker and S. Smale. On the mathematical foundations of learning.\nBull. Amer. Math. Soc. (N.S.), 39(1):1\u201349 (electronic), 2002.\n[9] F. Cucker and D.-X. Zhou.\nLearning theory: an approximation the-\nory viewpoint. Cambridge Monographs on Applied and Computational\nMathematics. Cambridge University Press, Cambridge, 2007. With a\nforeword by Stephen Smale.\n[10] E. B. Davies. Quantum theory of open systems. Academic Press [Har-\ncourt Brace Jovanovich Publishers], London, 1976.\n[11] J. Diestel and J. J. Uhl, Jr. Vector measures. American Mathemati-\ncal Society, Providence, R.I., 1977. With a foreword by B. J. Pettis,\nMathematical Surveys, No. 15.\n[12] T. Evgeniou, C. A. Micchelli, and M. Pontil. Learning multiple tasks\nwith kernel methods. J. Mach. Learn. Res., 6:615\u2013637 (electronic), 2005.\n[13] T. Evgeniou, M. Pontil, and T. Poggio. Regularization networks and\nsupport vector machines. Adv. Comput. Math., 13(1):1\u201350, 2000.\n[14] P. L. Falb. On a theorem of Bochner. Inst. Hautes \u00b4Etudes Sci. Publ.\nMath., 36:59\u201367, 1969.\n[15] P. L. Falb and U. Haussmann. Bochner\u2019s theorem in in\ufb01nite dimensions.\nPaci\ufb01c J. Math., 43:601\u2013618, 1972.\n[16] G. B. Folland. A course in abstract harmonic analysis. Studies in Ad-\nvanced Mathematics. CRC Press, Boca Raton, FL, 1995.\n[17] H. F\u00a8uhr. Abstract harmonic analysis of continuous wavelet transforms,\nvolume 1863 of Lecture Notes in Mathematics. Springer-Verlag, Berlin,\n2005.\n47\n[18] L. Gy\u00a8or\ufb01, M. Kohler, A. Krzy\u02d9zak, and H. Walk. A distribution-free the-\nory of nonparametric regression. Springer Series in Statistics. Springer-\nVerlag, New York, 2002.\n[19] S. Lang. Real and functional analysis, volume 142 of Graduate Texts in\nMathematics. Springer-Verlag, New York, third edition, 1993.\n[20] Y. Lee, Y. Lin, and G. Wahba. Multicategory support vector machines.\nIn Proceedings of the 33rd Symposium on the Interface, 2001.\n[21] C. A. Micchelli and M. Pontil.\nOn learning vector-valued functions.\nNeural Comput., 17(1):177\u2013204, 2005.\n[22] C. A. Micchelli, Y. Xu, and H. Zhang. Universal kernels.\nJ. Mach.\nLearn. Res., 7:2651\u20132667, 2006.\n[23] G. Pedrick. Theory of reproducing kernels for hilbert spaces of vector\nvalued functions. Technical report, Kansas Univ Lawrence, 1957.\n[24] F. Riesz and B. Sz.-Nagy. Functional analysis. Dover Books on Ad-\nvanced Mathematics. Dover Publications Inc., New York, 1990. Trans-\nlated from the second French edition by Leo F. Boron, Reprint of the\n1955 original.\n[25] S. Saitoh. Theory of reproducing kernels and its applications, volume 189\nof Pitman Research Notes in Mathematics Series. Longman Scienti\ufb01c &\nTechnical, Harlow, 1988.\n[26] B. Schoelkopf and J. Smola. Learning with Kernels. The MIT Press,\nCambridge, MA, 2002.\n[27] L. Schwartz. Sous-espaces hilbertiens d\u2019espaces vectoriels topologiques\net noyaux associ\u00b4es (noyaux reproduisants). J. Analyse Math., 13:115\u2013\n256, 1964.\n[28] I. Singer. Linear functionals on the space of continuous mappings of a\ncompact Hausdor\ufb00space into a Banach space. Rev. Math. Pures Appl.,\n2:301\u2013315, 1957.\n[29] S. Smale and D.-X. Zhou. Shannon sampling. II. Connections to learning\ntheory. Appl. Comput. Harmon. Anal., 19(3):285\u2013302, 2005.\n[30] I. Steinwart. On the in\ufb02uence of the kernel on the consistency of support\nvector machines. Journal of Machine Learning Research, 2:67\u201393, Nov.\n2001.\n48\n[31] H.-W. Sun and D.-X. Zhou. Reproducing kernel Hilbert spaces associ-\nated with analytic translation-invariant Mercer kernels. J. Fourier Anal.\nAppl., 14(1):89\u2013101, 2008.\n[32] D. X. Zhou. Density problem and approximation error in learning theory.\nTechnical report, City University of Hong Kong, 2003.\n49\n",
        "sentence": " 3) established that if k is C0universal (Carmeli et al., 2010, Defn. 4.1) or integrally strictly positive definite (ISPD, Stewart, 1976, Sec. 6) and E\u03bc[k0(X,X) + \u2016\u2207 log p(X) r(X)\u2016 2 2] < \u221e for k0 , \u2211d j=1 k j 0, then S(\u03bc, TP ,Gk) = 0 only if \u03bc = P . However, this property is insufficient to conclude that probability measures with small KSD are close to P in any traditional sense. Indeed, Gaussian and Mat\u00e9rn kernels are C0 universal and ISPD, but, by Theorem 6, their KSDs can be driven to zero by sequences not converging to P . On compact domains, where tightness is no longer an issue, the combined results of (Oates et al., 2016a, Lem. 4), (Fukumizu et al., 2007, Lem. 1), and (Simon-Gabriel & Sch\u00f6lkopf, 2016, Thm. 55) give conditions for a KSD to dominate weak convergence. While assessing sample quality was our chief objective, our results may hold benefits for other applications that make use of Stein discrepancies or Stein operators. In particular, our kernel recommendations could be incorporated into the Monte Carlo control functionals framework of Oates et al. (2016b); Oates & Girolami (2015), the variational inference approaches of Liu & Wang (2016); Liu & Feng (2016); Ranganath et al. 3) established that if k is C0universal (Carmeli et al., 2010, Defn. 4.1) or integrally strictly positive definite (ISPD, Stewart, 1976, Sec. 6) and E\u03bc[k0(X,X) + \u2016\u2207 log p(X) r(X)\u2016 2 2] < \u221e for k0 , \u2211d j=1 k j 0, then S(\u03bc, TP ,Gk) = 0 only if \u03bc = P . However, this property is insufficient to conclude that probability measures with small KSD are close to P in any traditional sense. Indeed, Gaussian and Mat\u00e9rn kernels are C0 universal and ISPD, but, by Theorem 6, their KSDs can be driven to zero by sequences not converging to P . On compact domains, where tightness is no longer an issue, the combined results of (Oates et al., 2016a, Lem. 4), (Fukumizu et al., 2007, Lem. 1), and (Simon-Gabriel & Sch\u00f6lkopf, 2016, Thm. 55) give conditions for a KSD to dominate weak convergence. While assessing sample quality was our chief objective, our results may hold benefits for other applications that make use of Stein discrepancies or Stein operators. In particular, our kernel recommendations could be incorporated into the Monte Carlo control functionals framework of Oates et al. (2016b); Oates & Girolami (2015), the variational inference approaches of Liu & Wang (2016); Liu & Feng (2016); Ranganath et al. 3) established that if k is C0universal (Carmeli et al., 2010, Defn. 4.1) or integrally strictly positive definite (ISPD, Stewart, 1976, Sec. 6) and E\u03bc[k0(X,X) + \u2016\u2207 log p(X) r(X)\u2016 2 2] < \u221e for k0 , \u2211d j=1 k j 0, then S(\u03bc, TP ,Gk) = 0 only if \u03bc = P . However, this property is insufficient to conclude that probability measures with small KSD are close to P in any traditional sense. Indeed, Gaussian and Mat\u00e9rn kernels are C0 universal and ISPD, but, by Theorem 6, their KSDs can be driven to zero by sequences not converging to P . On compact domains, where tightness is no longer an issue, the combined results of (Oates et al., 2016a, Lem. 4), (Fukumizu et al., 2007, Lem. 1), and (Simon-Gabriel & Sch\u00f6lkopf, 2016, Thm. 55) give conditions for a KSD to dominate weak convergence. While assessing sample quality was our chief objective, our results may hold benefits for other applications that make use of Stein discrepancies or Stein operators. In particular, our kernel recommendations could be incorporated into the Monte Carlo control functionals framework of Oates et al. (2016b); Oates & Girolami (2015), the variational inference approaches of Liu & Wang (2016); Liu & Feng (2016); Ranganath et al. 3) established that if k is C0universal (Carmeli et al., 2010, Defn. 4.1) or integrally strictly positive definite (ISPD, Stewart, 1976, Sec. 6) and E\u03bc[k0(X,X) + \u2016\u2207 log p(X) r(X)\u2016 2 2] < \u221e for k0 , \u2211d j=1 k j 0, then S(\u03bc, TP ,Gk) = 0 only if \u03bc = P . However, this property is insufficient to conclude that probability measures with small KSD are close to P in any traditional sense. Indeed, Gaussian and Mat\u00e9rn kernels are C0 universal and ISPD, but, by Theorem 6, their KSDs can be driven to zero by sequences not converging to P . On compact domains, where tightness is no longer an issue, the combined results of (Oates et al., 2016a, Lem. 4), (Fukumizu et al., 2007, Lem. 1), and (Simon-Gabriel & Sch\u00f6lkopf, 2016, Thm. 55) give conditions for a KSD to dominate weak convergence. While assessing sample quality was our chief objective, our results may hold benefits for other applications that make use of Stein discrepancies or Stein operators. In particular, our kernel recommendations could be incorporated into the Monte Carlo control functionals framework of Oates et al. (2016b); Oates & Girolami (2015), the variational inference approaches of Liu & Wang (2016); Liu & Feng (2016); Ranganath et al. 3) established that if k is C0universal (Carmeli et al., 2010, Defn. 4.1) or integrally strictly positive definite (ISPD, Stewart, 1976, Sec. 6) and E\u03bc[k0(X,X) + \u2016\u2207 log p(X) r(X)\u2016 2 2] < \u221e for k0 , \u2211d j=1 k j 0, then S(\u03bc, TP ,Gk) = 0 only if \u03bc = P . However, this property is insufficient to conclude that probability measures with small KSD are close to P in any traditional sense. Indeed, Gaussian and Mat\u00e9rn kernels are C0 universal and ISPD, but, by Theorem 6, their KSDs can be driven to zero by sequences not converging to P . On compact domains, where tightness is no longer an issue, the combined results of (Oates et al., 2016a, Lem. 4), (Fukumizu et al., 2007, Lem. 1), and (Simon-Gabriel & Sch\u00f6lkopf, 2016, Thm. 55) give conditions for a KSD to dominate weak convergence. While assessing sample quality was our chief objective, our results may hold benefits for other applications that make use of Stein discrepancies or Stein operators. In particular, our kernel recommendations could be incorporated into the Monte Carlo control functionals framework of Oates et al. (2016b); Oates & Girolami (2015), the variational inference approaches of Liu & Wang (2016); Liu & Feng (2016); Ranganath et al. (2016), and the Stein generative adversarial network approach of Wang & Liu (2016). 3) established that if k is C0universal (Carmeli et al., 2010, Defn. 4.1) or integrally strictly positive definite (ISPD, Stewart, 1976, Sec. 6) and E\u03bc[k0(X,X) + \u2016\u2207 log p(X) r(X)\u2016 2 2] < \u221e for k0 , \u2211d j=1 k j 0, then S(\u03bc, TP ,Gk) = 0 only if \u03bc = P . However, this property is insufficient to conclude that probability measures with small KSD are close to P in any traditional sense. Indeed, Gaussian and Mat\u00e9rn kernels are C0 universal and ISPD, but, by Theorem 6, their KSDs can be driven to zero by sequences not converging to P . On compact domains, where tightness is no longer an issue, the combined results of (Oates et al., 2016a, Lem. 4), (Fukumizu et al., 2007, Lem. 1), and (Simon-Gabriel & Sch\u00f6lkopf, 2016, Thm. 55) give conditions for a KSD to dominate weak convergence. While assessing sample quality was our chief objective, our results may hold benefits for other applications that make use of Stein discrepancies or Stein operators. In particular, our kernel recommendations could be incorporated into the Monte Carlo control functionals framework of Oates et al. (2016b); Oates & Girolami (2015), the variational inference approaches of Liu & Wang (2016); Liu & Feng (2016); Ranganath et al. (2016), and the Stein generative adversarial network approach of Wang & Liu (2016). In the future, we aim to leverage stochastic, low-rank, and sparse approximations of the kernel matrix and score function to produce KSDs that scale better with the number of sample and data points while still guaranteeing control over weak convergence. 3) established that if k is C0universal (Carmeli et al., 2010, Defn. 4.1) or integrally strictly positive definite (ISPD, Stewart, 1976, Sec. 6) and E\u03bc[k0(X,X) + \u2016\u2207 log p(X) r(X)\u2016 2 2] < \u221e for k0 , \u2211d j=1 k j 0, then S(\u03bc, TP ,Gk) = 0 only if \u03bc = P . However, this property is insufficient to conclude that probability measures with small KSD are close to P in any traditional sense. Indeed, Gaussian and Mat\u00e9rn kernels are C0 universal and ISPD, but, by Theorem 6, their KSDs can be driven to zero by sequences not converging to P . On compact domains, where tightness is no longer an issue, the combined results of (Oates et al., 2016a, Lem. 4), (Fukumizu et al., 2007, Lem. 1), and (Simon-Gabriel & Sch\u00f6lkopf, 2016, Thm. 55) give conditions for a KSD to dominate weak convergence. While assessing sample quality was our chief objective, our results may hold benefits for other applications that make use of Stein discrepancies or Stein operators. In particular, our kernel recommendations could be incorporated into the Monte Carlo control functionals framework of Oates et al. (2016b); Oates & Girolami (2015), the variational inference approaches of Liu & Wang (2016); Liu & Feng (2016); Ranganath et al. (2016), and the Stein generative adversarial network approach of Wang & Liu (2016). In the future, we aim to leverage stochastic, low-rank, and sparse approximations of the kernel matrix and score function to produce KSDs that scale better with the number of sample and data points while still guaranteeing control over weak convergence. A reader may also wonder for which distributions outside ofP the KSD dominates weak convergence. The following theorem, proved in Section J, shows that no KSD with aC0 kernel dominates weak convergence when the target has a bounded score function. Theorem 10 (KSD fails for bounded scores). If \u2207 log p is bounded and k \u2208 C 0 , then S(Qn, TP ,Gk) \u2192 0 does not imply Qn \u21d2 P . However, Gorham et al. (2016) developed convergencedetermining graph Stein discrepancies for heavy-tailed targets by replacing the Langevin Stein operator TP with diffusion Stein operators of the form (T g)(x) = 1 p(x) \u3008\u2207, p(x)(a(x) + c(x))g(x)\u3009.",
        "context": "lation invariant kernel with \u03ba0 in C0(Rd) \u2229L1(Rd, dx). According to (37)\nB(p) =\np\n(2\u03c0\u03c32)d e\u22122\u03c02\u03c32\u2225p\u22252\nwhere the dual group is identi\ufb01ed with Rd by means of \u03c7p(x) = ei2\u03c0p\u00b7x. Since\nB(p) > 0 for all p \u2208Rd, universality is a consequence of Corollary 6.\n(a) The kernel K is universal.\n(b) The space HK is dense in C0(X; Y).\n(c) There is 1 \u2264p < \u221esuch that HK is dense in Lp(X, \u00b5; Y) for all\nprobability measures \u00b5 on X.\nProof. Clearly (a) implies (c). Since X is locally compact and second count-\nall probability measures \u00b5 on X with compact support.\nThe proof is a simple consequence of Proposition 10.\nClearly universality of a C0-kernel K implies compact-universality. The\nconverse is not true as shown by the following example, see also Example 11."
    },
    {
        "title": "Nonnormal approximation by Stein\u2019s method of exchangeable pairs with application to the Curie-Weiss model",
        "author": [
            "S. Chatterjee",
            "Q. Shao"
        ],
        "venue": "Ann. Appl. Probab.,",
        "citeRegEx": "Chatterjee and Shao,? \\Q2011\\E",
        "shortCiteRegEx": "Chatterjee and Shao",
        "year": 2011,
        "abstract": "",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "Normal approximation by Stein\u2019s method. Probability and its Applications",
        "author": [
            "L. Chen",
            "L. Goldstein",
            "Q. Shao"
        ],
        "venue": null,
        "citeRegEx": "Chen et al\\.,? \\Q2011\\E",
        "shortCiteRegEx": "Chen et al\\.",
        "year": 2011,
        "abstract": "",
        "full_text": "",
        "sentence": " Selecting a Stein operator and a Stein set A standard, widely applicable univariate Stein operator is the density method operator (see Stein et al., 2004; Chatterjee & Shao, 2011; Chen et al., 2011; Ley et al., 2017), (T g)(x) , 1 p(x) d dx (p(x)g(x)) = g(x)b(x) + g \u2032(x).",
        "context": null
    },
    {
        "title": "A kernel test of goodness of fit",
        "author": [
            "K. Chwialkowski",
            "H. Strathmann",
            "A. Gretton"
        ],
        "venue": "In Proc. 33rd ICML,",
        "citeRegEx": "Chwialkowski et al\\.,? \\Q2016\\E",
        "shortCiteRegEx": "Chwialkowski et al\\.",
        "year": 2016,
        "abstract": "We propose a nonparametric statistical test for goodness-of-fit: given a set\nof samples, the test determines how likely it is that these were generated from\na target density function. The measure of goodness-of-fit is a divergence\nconstructed via Stein's method using functions from a Reproducing Kernel\nHilbert Space. Our test statistic is based on an empirical estimate of this\ndivergence, taking the form of a V-statistic in terms of the log gradients of\nthe target density and the kernel. We derive a statistical test, both for\ni.i.d. and non-i.i.d. samples, where we estimate the null distribution\nquantiles using a wild bootstrap procedure. We apply our test to quantifying\nconvergence of approximate Markov Chain Monte Carlo methods, statistical model\ncriticism, and evaluating quality of fit vs model complexity in nonparametric\ndensity estimation.",
        "full_text": "A Kernel Test of Goodness of Fit\nKacper Chwialkowski\u2217\nKACPER.CHWIALKOWSKI@GMAIL.COM\nHeiko Strathmann\u2217\nHEIKO.STRATHMANN@GMAIL.COM\nArthur Gretton\nARTHUR.GRETTON@GMAIL.COM\nGatsby Unit, University College London, United Kingdom\nAbstract\nWe propose a nonparametric statistical test for\ngoodness-of-\ufb01t: given a set of samples, the test\ndetermines how likely it is that these were gen-\nerated from a target density function. The meas-\nure of goodness-of-\ufb01t is a divergence construc-\nted via Stein\u2019s method using functions from a re-\nproducing kernel Hilbert space. Our test statistic\nis based on an empirical estimate of this diver-\ngence, taking the form of a V-statistic in terms\nof the gradients of the log target density and of\nthe kernel. We derive a statistical test, both for\ni.i.d. and non-i.i.d. samples, where we estim-\nate the null distribution quantiles using a wild\nbootstrap procedure. We apply our test to quanti-\nfying convergence of approximate Markov chain\nMonte Carlo methods, statistical model criticism,\nand evaluating quality of \ufb01t vs model complexity\nin nonparametric density estimation.\n1. Introduction\nStatistical tests of goodness-of-\ufb01t are a fundamental tool in\nstatistical analysis, dating back to the test of Kolmogorov\nand Smirnov (Kolmogorov, 1933; Smirnov, 1948). Given\na set of samples {Zi}n\ni=1 with distribution Zi \u223cq, our in-\nterest is in whether q matches some reference or target dis-\ntribution p, which we assume to be only known up to the\nnormalisation constant. Recently, in the multivariate set-\nting, Gorham & Mackey (2015) proposed an elegant meas-\nure of sample quality with respect to a target. This measure\nis a maximum discrepancy between empirical sample ex-\npectations and target expectations over a large class of test\nfunctions, constructed so as to have zero expectation over\nthe target distribution by use of a Stein operator. This op-\nerator depends only on the derivative of the log q: thus, the\nProceedings of the 33 rd International Conference on Machine\nLearning, New York, NY, USA, 2016. JMLR: W&CP volume\n48. Copyright 2016 by the author(s).\napproach can be applied very generally, as it does not re-\nquire closed-form integrals over the target distribution (or\nnumerical approximations of such integrals). By contrast,\nmany earlier discrepancy measures require integrals with\nrespect to the target (see below for a review). This is prob-\nlematic e.g. if the intention is to perform benchmarks for\nassessing Markov Chain Monte Carlo, since these integrals\nare certainly not known to the practitioner.\nA challenge in applying the approach of Gorham &\nMackey is the complexity of the function class used, which\nresults from applying the Stein operator to the W 2,\u221eSo-\nbolev space. Thus, their sample quality measure requires\nsolving a linear program that arises from a complicated\nconstruction of graph Stein discrepancies and geometric\nspanners. Their metric furthermore requires access to non-\ntrivial lower bounds that, despite being provided for log-\nconcave densities, are a largely open problem otherwise, in\nparticular for multivariate cases.\nAn important application of a goodness-of-\ufb01t measure is in\nstatistical testing, where it is desired to determine whether\nthe empirical discrepancy measure is large enough to reject\nthe null hypothesis (that the sample arises from the target\ndistribution). One approach is to establish the asymptotic\nbehaviour of the test statistic, and to set a test threshold at\na large quantile of the asymptotic distribution. The asymp-\ntotic behaviour of the W 2,\u221e-Sobolev Stein discrepancies\nremains a challenging open problem, due to the complex-\nity of the function class used. It is not clear how one would\ncompute p-values for this statistic, or determine when the\ngoodness of \ufb01t test allows to accept the null hypothesis at a\nuser-speci\ufb01ed test level.\nThe key contribution of this work is to de\ufb01ne a statistical\ntest of goodness-of-\ufb01t, based on a Stein discrepancy com-\nputed in a reproducing kernel Hilbert space (RKHS). To\nconstruct our test statistic, we use a function class de\ufb01ned\nby applying the Stein operator to a chosen space of RKHS\nfunctions, as proposed by (Oates et al., 2016).1 Our meas-\n1Oates et al. addressed the problem of variance reduction in\nMonte Carlo integration, using the Stein operator to avoid bias.\narXiv:1602.02964v4  [stat.ML]  27 Sep 2016\nA Kernel Test of Goodness of Fit\nure of goodness of \ufb01t is the largest discrepancy over this\nspace of functions between empirical sample expectations\nand target expectations (the latter being zero, due to the\neffect of the Stein operator). The approach is a natural\nextension to goodness-of-\ufb01t testing of the earlier kernel\ntwo-sample tests (Gretton et al., 2012) and independence\ntests (Gretton et al., 2007), which are based on the max-\nimum mean discrepancy, an integral probability metric. As\nwith these earlier tests, our statistic is a simple V-statistic,\nand can be computed in closed form and in quadratic time.\nMoreover, it is an unbiased estimate of the corresponding\npopulation discrepancy. As with all Stein-based discrepan-\ncies, only the gradient of the log target density is needed;\nwe do not require integrals with respect to the target density\n\u2013 including the normalisation constant. Given that our test\nstatistic is a V-statistic, we make use of the extensive lit-\nerature on asymptotics of V-statistics to formulate a hypo-\nthesis test (Ser\ufb02ing, 1980; Leucht & Neumann, 2013).2 We\nprovide statistical tests for both uncorrelated and correlated\nsamples, where the latter is essential if the test is used in as-\nsessing the quality of output of an MCMC procedure. An\nidentical test was obtained simultaneously in independent\nwork by Liu et al. (2016), for uncorrelated samples.\nSeveral alternative approaches exist in the statistics liter-\nature to goodness-of-\ufb01t testing. A \ufb01rst strategy is to par-\ntition the space, and to conduct the test on a histogram\nestimate of the distribution (Barron, 1989; Beirlant et al.,\n1994; Gy\u00f6r\ufb01& van der Meulen, 1990; Gy\u00f6r\ufb01& Vajda,\n2002). Such space partitioning approaches can have at-\ntractive theoretical properties (e.g.\ndistribution-free test\nthresholds) and work well in low dimensions, however they\nare much less powerful than alternatives once the dimen-\nsionality increases (Gretton & Gyor\ufb01, 2010). A second\npopular approach has been to use the smoothed L2 dis-\ntance between the empirical characteristic function of the\nsample, and the characteristic function of the target dens-\nity. This dates back to the test of Gaussianity of Baring-\nhaus & Henze (1988, Eq. 2.1), who used an exponentiated\nquadratic smoothing function. For this choice of smoothing\nfunction, their statistic is identical to the maximum mean\ndiscrepancy (MMD) with the exponentiated quadratic ker-\nnel, which can be shown using the Bochner representation\nof the kernel (Sriperumbudur et al., 2010, Corollary 4). It\nis essential in this case that the target distribution be Gaus-\nsian, since the convolution with the kernel (or in the Four-\nier domain, the smoothing function) must be available in\nclosed form. An L2 distance between Parzen window es-\ntimates can also be used (Bowman & Foster, 1993), giving\nthe same expression again, although the optimal choice of\n2An alternative linear-time test, based on differences in ana-\nlytic functions of the sample and following the recent work of\n(Chwialkowski et al., 2015), is provided in (Chwialkowski et al.,\n2016, Appendix 5.1)\nbandwidth for consistent Parzen window estimates may not\nbe a good choice for testing (Anderson et al., 1994). A dif-\nferent smoothing scheme in the frequency domain results\nin an energy distance statistic (this likewise being an MMD\nwith a particular choice of kernel; see Sejdinovic et al.,\n2013), which can be used in a test of normality (Sz\u00e9kely\n& Rizzo, 2005). The key point is that the required integ-\nrals are again computable in closed form for the Gaussian,\nalthough the reasoning may be extended to certain other\nfamilies of interest, e.g. (Rizzo, 2009). The requirement\nof computing closed-form integrals with respect to the test\ndistribution severely restricts this testing strategy. Finally,\na problem related to goodness-of-\ufb01t testing is that of model\ncriticism (Lloyd & Ghahramani, 2015).\nIn this setting,\nsamples generated from a \ufb01tted model are compared via the\nmaximum mean discrepancy with samples used to train the\nmodel, such that a small MMD indicates a good \ufb01t. There\nare two limitation to the method: \ufb01rst, it requires samples\nfrom the model (which might not be easy if this requires a\ncomplex MCMC sampler); second, the choice of number\nof samples from the model is not obvious, since too few\nsamples cause a loss in test power, and too many are com-\nputationally wasteful. Neither issue arises in our test, as we\ndo not require model samples.\nIn our experiments, a particular focus is on applying our\ngoodness-of-\ufb01t test to certify the output of approximate\nMarkov Chain Monte Carlo (MCMC) samplers (Korat-\ntikara et al., 2014; Welling & Teh, 2011; Bardenet et al.,\n2014). These methods use modi\ufb01cations to Markov trans-\nition kernels that improve mixing speed at the cost of intro-\nducing asymptotic bias. The resulting bias-variance trade-\noff can usually be tuned with parameters of the sampling al-\ngorithms. It is therefore important to test whether for a par-\nticular parameter setting and run-time, the samples are of\nthe desired quality. This question cannot be answered with\nclassical MCMC convergence statistics, such as the widely\nused potential scale reduction factor (R-factor) (Gelman &\nRubin, 1992) or the effective sample size, since these as-\nsume that the Markov chain reaches the true equilibrium\ndistribution i.e. absence of asymptotic bias. By contrast,\nour test exactly quanti\ufb01es the asymptotic bias of approxim-\nate MCMC.\nCode\ncan\nbe\nfound\nat\nht-\ntps://github.com/karlnapf/kernel_goodness_of_\ufb01t.\nPaper outline\nWe begin in section 2 with a high-level\nconstruction of the RKHS-based Stein discrepancy and as-\nsociated statistical test. In Section 3, we provide additional\ndetails and prove the main results. Section 4 contains ex-\nperimental illustrations on synthetic examples, statistical\nmodel criticism, bias-variance trade-offs in approximate\nMCMC, and convergence in non-parametric density estim-\nation.\nA Kernel Test of Goodness of Fit\n2. Test De\ufb01nition: Statistic and Threshold\nWe begin with a high-level construction of our divergence\nmeasure and the associated statistical test. While this sec-\ntion aims to outline the main ideas, we provide details and\nproofs in Section 3.\n2.1. Stein Operator in RKHS\nOur goal is to write the maximum discrepancy between\nthe target distribution p and observed sample distribution\nq in a modi\ufb01ed RKHS, such that functions have zero ex-\npectation under p. Denote by F the RKHS of real-valued\nfunctions on Rd with reproducing kernel k, and by Fd the\nproduct RKHS consisting of elements f := (f1, . . . , fd)\nwith fi \u2208F, and with a standard inner product \u27e8f, g\u27e9Fd =\nPd\ni=1 \u27e8fi, gi\u27e9F. We further assume that all measures con-\nsidered in this paper are supported on an open set, equal\nto zero on the border, and strictly positive3 (so logarithms\nare well de\ufb01ned). Similarly to Stein (1972); Gorham &\nMackey (2015); Oates et al. (2016), we begin by de\ufb01ning a\nStein operator Tp acting on f \u2208Fd\n(Tpf)(x) :=\nd\nX\ni=1\n\u0012\u2202log p(x)\n\u2202xi\nfi(x) + \u2202fi(x)\n\u2202xi\n\u0013\n.\nSuppose a random variable Z is distributed according to\na measure4 q and X is distributed according to the target\nmeasure p. As we will see, the operator can be expressed\nby de\ufb01ning a function that depends on gradient of the log-\ndensity and the kernel,\n\u03bep(x, \u00b7) := [\u2207log p(x)k(x, \u00b7) + \u2207k(x, \u00b7)] ,\n(1)\nwhose expected inner product with f gives exactly the ex-\npected value of the Stein operator,\nEqTpf(Z) = \u27e8f, Eq\u03bep(Z)\u27e9Fd =\nd\nX\ni=1\n\u27e8fi, Eq\u03bep,i(Z)\u27e9F,\nwhere \u03bep,i(x, \u00b7) is the i-th component of \u03bep(x, \u00b7). For X\nfrom the target measure, we have Ep(Tpf)(X) = 0, which\ncan be seen using integration by parts, c.f. Lemma 5.1 in\nthe supplement. We can now de\ufb01ne a Stein discrepancy\nand express it in the RKHS,\nSp(Z) := sup\n\u2225f\u2225<1\nEq(Tpf)(Z) \u2212Ep(Tpf)(X)\n= sup\n\u2225f\u2225<1\nEq(Tpf)(Z)\n= sup\n\u2225f\u2225<1\n\u27e8f, Eq\u03bep(Z)\u27e9Fd\n= \u2225Eq\u03bep(Z)\u2225Fd,\n3An example of such a space is the positive real line\n4Throughout the article, all occurrences of Z, e.g. Z\u2032, Zi, Z\u2665,\nare understood to be distributed according to q.\nThis makes it clear why Ep(Tpf)(X)\n=\n0 is a de-\nsirable property: we can compute Sp(Z) by computing\n\u2225Eq\u03bep(Z)\u2225, without the need to access X in the form of\nsamples from p. To state our \ufb01rst result we de\ufb01ne\nhp(x, y) := \u2207log p(x)\u22a4\u2207log p(y)k(x, y)\n+ \u2207log p(y)\u22a4\u2207xk(x, y)\n+ \u2207log p(x)\u22a4\u2207yk(x, y)\n+ \u27e8\u2207xk(x, \u00b7), \u2207yk(\u00b7, y)\u27e9Fd,\nwhere the last term can be written as a sum Pd\ni=1\n\u2202k(x,y)\n\u2202xi\u2202yi .\nThe following theorem gives a simple closed form expres-\nsion for \u2225Eq\u03bep(Z)\u2225Fd in terms of hp.\nTheorem 2.1. If Ehp(Z, Z)\n<\n\u221e, then S2\np(Z)\n=\n\u2225Eq\u03bep(Z)\u22252\nFd = Eqhp(Z, Z\u2032), where Z\u2032 is independent\nof Z with an identical distribution.\nThe second main result states that the discrepancy Sp(Z)\ncan be used to distinguish two distributions.\nTheorem 2.2. Let q, p be probability measures and Z \u223cq.\nIf the kernel k is C0-universal (Carmeli et al., 2010, De\ufb01-\nnition 4.1), Eqhq(Z, Z) < \u221e, and Eq\n\r\r\r\u2207\n\u0010\nlog p(Z)\nq(Z)\n\u0011\r\r\r\n2\n<\n\u221e, then Sp(Z) = 0 if and only if p = q.\nSection 3.1 contains all necessary proofs. We now proceed\nto construct an estimator for S(Z)2, and outline its asymp-\ntotic properties.\n2.2. Wild Bootstrap Testing\nIt is straightforward to estimate the squared Stein discrep-\nancy S(Z)2 from samples {Zi}n\ni=1: a quadratic time es-\ntimator is a V-Statistic, and takes the form\nVn = 1\nn2\nn\nX\ni,j=1\nhp(Zi, Zj).\nThe asymptotic null distribution of the normalised V-\nStatistic nVn, however, has no computable closed form.\nFurthermore, care has to be taken when the Zi exhibit cor-\nrelation structure, as the null distribution might signi\ufb01c-\nantly change, impacting test signi\ufb01cance. The wild boot-\nstrap technique (Shao, 2010; Leucht & Neumann, 2013;\nFromont et al., 2012) addresses both problems. First, it al-\nlows us to estimate quantiles of the null distribution in order\nto compute test thresholds. Second, it accounts for correl-\nation structure in the Zi by mimicking it with an auxiliary\nrandom process: a simple Markov chain taking values in\n{\u22121, 1}, starting from W1,n = 1,\nWt,n = 1(Ut > an)Wt\u22121,n \u22121(Ut < an)Wt\u22121,n,\nwhere the Ut are uniform (0, 1) i.i.d. random variables and\nan is the probability of Wt,n changing sign (for i.i.d. data\nwe set an = 0.5). This leads to a bootstrapped V-statistic\nA Kernel Test of Goodness of Fit\nBn = 1\nn2\nn\nX\ni,j=1\nWi,nWj,nhp(Zi,Zj).\nProposition 3.2 establishes that, under the null hypothesis,\nnBn is a good approximation of nVn, so it is possible to\napproximate quantiles of the null distribution by sampling\nfrom it. Under the alternative, Vn dominates Bn \u2013 resulting\nin almost sure rejection of the null hypothesis.\nWe propose the following test procedure for testing the null\nhypothesis that the Zi are distributed according to the target\ndistribution p.\n1. Calculate the test statistic Vn.\n2. Obtain wild bootstrap samples {Bn}D\ni=1 and estimate\nthe 1 \u2212\u03b1 empirical quantile of these samples.\n3. If Vn exceeds the quantile, reject.\n3. Proofs of the Main Results\nWe now prove the claims made in the previous section.\n3.1. Stein Operator in RKHS\nLemma 5.1 (in the Appendix) shows that the expected\nvalue of the Stein operator is zero on the target measure.\nProof of Theorem 2.1. \u03bep(x, \u00b7) is an element of the re-\nproducing kernel Hilbert space Fd \u2013 by Steinwart &\nChristmann (2008, Lemma 4.34) \u2207k(x, \u00b7)\n\u2208\nF, and\n\u2202log p(x)\n\u2202xi\nis just a scalar. We \ufb01rst show that hp(x, y) =\n\u27e8\u03bep(x, \u00b7), \u03bep(y, \u00b7)\u27e9. Using notations\n\u2207xk(x, \u00b7) =\n\u0012\u2202k(x, \u00b7)\n\u2202x1\n, \u00b7 \u00b7 \u00b7 , \u2202k(x, \u00b7)\n\u2202xd\n\u0013\n\u2207yk(\u00b7, y) =\n\u0012\u2202k(\u00b7, y)\n\u2202y1\n, \u00b7 \u00b7 \u00b7 , \u2202k(\u00b7, y)\n\u2202yd\n\u0013\n,\nwe calculate\n\u27e8\u03bep(x, \u00b7), \u03bep(y, \u00b7)\u27e9= \u2207log p(x)\u22a4\u2207log p(y)k(x, y)\n+ \u2207log p(y)\u22a4\u2207xk(x, y)\n+ \u2207log p(x)\u22a4\u2207yk(x, y)\n+ \u27e8\u2207xk(x, \u00b7), \u2207yk(\u00b7, y)\u27e9Fd.\nNext we show that \u03bep(x, \u00b7) is Bochner integrable (see Stein-\nwart & Christmann, 2008, De\ufb01nition A.5.20),\nEq\u2225\u03bep(Z)\u2225Fd \u2264\nq\nEq\u2225\u03bep(Z)\u22252\nFd =\nq\nEqhp(Z, Z) < \u221e.\nThis allows us to take the expectation inside the RKHS in-\nner product. We next relate the expected value of the Stein\noperator to the inner product of f and the expected value of\n\u03beq(Z),\nEqTpf(Z) = \u27e8f, Eq\u03bep(Z)\u27e9Fd =\nd\nX\ni=1\n\u27e8fi, Eq\u03bep,i(Z)\u27e9F.\n(2)\nWe check the claim for all dimensions,\n\u27e8fi, Eq\u03bep,i(Z)\u27e9F\n=\n\u001c\nfi, Eq\n\u0014\u2202log p(Z)\n\u2202xi\nk(Z, \u00b7) + \u2202k(Z, \u00b7)\n\u2202xi\n\u0015\u001d\nF\n= Eq\n\u001c\nfi, \u2202log p(Z)\n\u2202xi\nk(Z, \u00b7) + \u2202k(Z, \u00b7)\n\u2202xi\n\u001d\nF\n= Eq\n\u0014\u2202log p(Z)\n\u2202xi\nfi(Z) + \u2202fi(Z, \u00b7)\n\u2202xi\n\u0015\n.\nThe second equality follows from the fact that a linear oper-\nator \u27e8fi, \u00b7\u27e9F can be interchanged with the Bochner integral,\nand the fact that \u03bep is Bochner integrable. Using de\ufb01nition\nof S(Z), Lemma (5.1), and Equation (2), we have\nSp(Z) := sup\n\u2225f\u2225<1\nEq(Tpf)(Z) \u2212Ep(Tpf)(X)\n= sup\n\u2225f\u2225<1\nEq(Tpf)(Z)\n= sup\n\u2225f\u2225<1\n\u27e8f, Eq\u03bep(Z)\u27e9Fd\n= \u2225Eq\u03bep(Z)\u2225Fd.\nWe now calculate closed form expression for S2\np(Z),\nS2\np(Z) = \u27e8Eq\u03bep(Z), Eq\u03bep(Z)\u27e9Fd = Eq\u27e8\u03bep(Z), Eq\u03bep(Z)\u27e9Fd\n= Eq\u27e8\u03bep(Z), \u03bep(Z\u2032)\u27e9Fd = Eqhp(Z, Z\u2032),\nwhere Z\u2032 is an independent copy of Z.\nNext, we prove that the discrepancy S discriminates differ-\nent probability measures.\nProof of Theorem 2.2. If p = q then Sp(Z) is 0 by Lemma\n(5.1). Suppose p \u0338= q, but Sp(Z) = 0. If Sp(Z) = 0\nthen, by Theorem 2.1, Eq\u03bep(Z) = 0. In the following we\nsubstitute log p(Z) = log q(Z) + [log p(Z) \u2212log q(Z)],\nEq\u03bep(Z)\n= Eq (\u2207log p(Z)k(Z, \u00b7) + \u2207k(Z, \u00b7))\n= Eq\u03beq(Z) + Eq (\u2207[log p(Z) \u2212log q(Z)]k(Z, \u00b7))\n= Eq (\u2207[log p(Z) \u2212log q(Z)]k(Z, \u00b7))\nWe have used Theorem 2.1 and Lemma (5.1) to see that\nEq\u03beq(Z) = 0, since \u2225Eq\u03beq(Z)\u22252 = S2\nq(Z) = 0.\nA Kernel Test of Goodness of Fit\nWe recognise that the expected value of \u2207(log p(Z) \u2212\nlog q(Z))k(Z, \u00b7) is the mean embedding of a function\ng(y) = \u2207\n\u0010\nlog p(y)\nq(y)\n\u0011\nwith respect to the measure q. By\nthe assumptions the function g is square integrable; there-\nfore, since the kernel k is Co-universal, by Carmeli et al.\n(2010, Theorem 4.2 b) its embedding is zero if and only if\ng = 0. This implies that\n\u2207log p(y)\nq(y) = (0, \u00b7 \u00b7 \u00b7 , 0).\nA constant vector \ufb01eld of derivatives can only be generated\nby a constant function, so log p(y)\nq(y) = C, for some C, which\nimplies that p(y) = eCq(y). Since p and q both integrate to\none, C = 0 and thus p = q, which is a contradiction.\n3.2. Wild Bootstrap Testing\nThe two concepts required to derive the distribution of the\ntest statistic are: \u03c4-mixing (Dedecker et al., 2007; Leucht\n& Neumann, 2013), and V-statistics (Ser\ufb02ing, 1980).\nWe assume \u03c4-mixing as our notion of dependence within\nthe observations, since this is weak enough for most practi-\ncal applications. Trivially, i.i.d. observations are \u03c4-mixing.\nAs for Markov chains, whose convergence we study in the\nexperiments, the property of geometric ergodicity implies\n\u03c4-mixing (given that the stationary distribution has a \ufb01nite\nmoment of some order \u2013 see the Appendix for further dis-\ncussion). For further details on \u03c4-mixing, see (Dedecker &\nPrieur, 2005; Dedecker et al., 2007). For this work, we as-\nsume a technical condition P\u221e\nt=1 t2p\n\u03c4(t) \u2264\u221e. A direct\napplication of (Leucht, 2012, Theorem 2.1) characterises\nthe limiting behavior of nVn for \u03c4-mixing processes.\nProposition 3.1.\nIf h is Lipschitz continuous and\nEqhp(Z, Z) < \u221ethen, under the null hypothesis, nVn\nconverges weakly to some distribution.\nThe proof, which is a simple veri\ufb01cation of the relevant as-\nsumptions, can be found in the Appendix. Although a for-\nmula for a limit distribution of Vn can be derived explicitly\n(Theorem 2.1 Leucht, 2012), we do not provide it here. To\nour knowledge there are no methods of obtaining quantiles\nof a limit of Vn in closed form. The common solution is to\nestimate quantiles by a resampling method, as described in\nSection 2. The validity of this resampling method is guar-\nanteed by the following proposition (which follows from\nTheorem 2.1 of Leucht and a modi\ufb01cation of the Lemma 5\nof Chwialkowski et al. (2014)), proved in the supplement.\nProposition\n3.2.\nLet\nf(Z1,n, \u00b7 \u00b7 \u00b7 , Zt,n)\n=\nsupx |P(nBn\n>\nx|Z1,n, \u00b7 \u00b7 \u00b7 , Zt,n) \u2212P(nVn\n>\nx)|\nbe a difference between quantiles.\nIf h is Lipschitz\ncontinuous and Eqhp(Z, Z)2 < \u221ethen, under the null\n1.0\n5.0\n10.0\ninf\ndegrees of freedom\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\np values\nFigure 1. Large autocovariance, unsuitable bootstrap. The param-\neter an is too large and the bootstrapped V-statistics Bn are too\nlow on average. Therefore, it is very likely that Vn > Bn and the\ntest is too conservative.\nhypothesis, f(X1,n, \u00b7 \u00b7 \u00b7 , Xt,n) converges to zero in prob-\nability; under the alternative hypothesis, Bn converges to\nzero, while Vn converges to a positive constant.\nAs a consequence, if the null hypothesis is true, we can ap-\nproximate any quantile; while under the alternative hypo-\nthesis, all quantiles of Bn collapse to zero while P(Vn >\n0) \u21921. We discuss speci\ufb01c case of testing MCMC con-\nvergence in the Appendix.\n4. Experiments\nWe provide a number of experimental applications for our\ntest. We begin with a simple check to establish correct\ntest calibration on non-i.i.d. data, followed by a demon-\nstration of statistical model criticism for Gaussian process\n(GP) regression. We then apply the proposed test to quan-\ntify bias-variance trade-offs in MCMC, and demonstrate\nhow to use the test to verify whether MCMC samples are\ndrawn from the desired stationary distribution. In the \ufb01-\nnal experiment, we move away from the MCMC setting,\nand use the test to evaluate the convergence of a non-\nparametric density estimator.\nCode can be found at ht-\ntps://github.com/karlnapf/kernel_goodness_of_\ufb01t.\nSTUDENT\u2019S T VS. NORMAL\nIn our \ufb01rst task, we modify Experiment 4.1 from Gorham\n& Mackey (2015). The null hypothesis is that the observed\nsamples come from a standard normal distribution.\nWe\nstudy the power of the test against samples from a Stu-\ndent\u2019s t distribution. We expect to observe low p-values\nwhen testing against a Student\u2019s t distribution with few de-\ngrees of freedom. We consider 1, 5, 10 or \u221edegrees of\nfreedom, where \u221eis equivalent to sampling from a stan-\ndard normal distribution. For a \ufb01xed number of degrees of\nA Kernel Test of Goodness of Fit\n1.0\n5.0\n10.0\ninf\ndegrees of freedom\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\np values\nFigure 2. Large autocovariance, suitable bootstrap. The parame-\nter anis chosen suitably, but due to a large autocorrelation within\nthe samples, the power of the test is small (effective sample size\nis small).\nfreedom we draw 1400 samples and calculate the p-value.\nThis procedure is repeated 100 times, and the bar plots of\np-values are shown in Figures 1,2,3.\nOur twist on the original experiment 4.1 by Gorham &\nMackey is that the draws from the Student\u2019s t distribution\nexhibit temporal correlation. We generate samples using a\nMetropolis\u2013Hastings algorithm, with a Gaussian random\nwalk with variance 1/2.\nWe emphasise the need for an\nappropriate choice of the wild bootstrap process parame-\nter an. In Figure 1 we plot p-values for an being set to\n0.5. Such a high value of an is suitable for i.i.d. obser-\nvations, but results in p-values that are too conservative\nfor temporally correlated observations. In Figure 2, we set\nan = 0.02, which gives a well calibrated distribution of the\np-values under the null hypothesis, however the test power\nis reduced. Indeed, p-values for \ufb01ve degrees of freedom\nare already large. The solution that we recommend is a\nmixture of thinning and adjusting an, as presented in the\nFigure 3. We thin the observations by a factor of 20 and\nset an = 0.1, thus preserving both good statistical power\nand correct calibration of p-values under the null hypoth-\nesis. In a general, we recommend to thin a chain so that\nCor(Xt, Xt\u22121) < 0.5, set an = 0.1/k, and run test with at\nleast max(500k, d100) data points, where k < 10.\nCOMPARING TO A PARAMETRIC TEST IN INCREASING\nDIMENSIONS\nIn this experiment, we compare with the test proposed by\nBaringhaus & Henze (1988), which is essentially an MMD\ntest for normality, i.e. the null hypothesis is that Z is a d-\ndimensional standard normal random variable. We set the\nsample size to n = 500, 1000 and an = 0.5, generate\nZ \u223cN(0, Id)\nY \u223cU[0, 1],\n1.0\n5.0\n10.0\ninf\ndegrees of freedom\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\np values\nFigure 3. Thinned sample, suitable bootstrap. Most of the auto-\ncorrelation within the sample is canceled by thinning. To guar-\nantee that the remaining autocorrelation is handled properly, the\nwild bootstrap \ufb02ip probability is set at 0.1.\nd\n2\n5\n10\n15\n20\n25\nB&H\nn = 500\n1\n1\n1\n0.86\n0.29\n0.24\nStein\n1\n1\n0.86\n0.39\n0.05\n0.05\nB&H\nn = 1000\n1\n1\n1\n1\n0.87\n0.62\nStein\n1\n1\n1\n0.77\n0.25\n0.05\nTable 1. Test power vs. sample size for the test by Baringhaus &\nHenze (1988) (B&H) and our Stein based test.\nand modify Z0 \u2190Z0 + Y . Table 1 shows the power as a\nfunction of the sample size. We observe that for higher di-\nmensions, and where the expectation of the kernel exists in\nclosed form, an MMD-type test like (Baringhaus & Henze,\n1988) is a better choice.\nSTATISTICAL MODEL CRITICISM ON GAUSSIAN\nPROCESSES\nWe next apply our test to the problem of statistical model\ncriticism for GP regression. Our presentation and approach\nare similar to the non i.i.d. case in Section 6 of Lloyd &\nGhahramani (2015). We use the solar dataset, consisting\nof a d = 1 regression problem with N = 402 pairs (X, y).\nWe \ufb01t Ntrain = 361 data using a GP with an exponentiated\nquadratic kernel and a Gaussian noise model, and perform\nstandard maximum likelihood II on the hyperparameters\n(length-scale, overall scale, noise-variance). We then apply\nour test to the remaining Ntest = 41 data. The test attempts\nto falsify the null hypothesis that the solar dataset was\ngenerated from the plug-in predictive distribution (condi-\ntioned on training data and predicted position) of the GP.\nLloyd & Ghahramani refer to this setup as non-i.i.d., since\nthe predictive distribution is a different univariate Gaussian\nfor every predicted point. Our particular Ntrain, Ntest were\nchosen to make sure the GP \ufb01t has stabilised, i.e. adding\nmore data did not cause further model re\ufb01nement.\nA Kernel Test of Goodness of Fit\n\u22122.0 \u22121.5 \u22121.0 \u22120.5\n0.0\n0.5\n1.0\n1.5\n2.0\nX\n\u22122\n\u22121\n0\n1\n2\n3\n4\ny\nFigure 4. Fitted GP and data used to \ufb01t (blue) and to apply test\n(red).\nFigure 4 shows training and testing data, and the \ufb01tted GP.\nClearly, the Gaussian noise model is a poor \ufb01t for this par-\nticular dataset, e.g. around X = \u22121. Figure 5 shows the\ndistribution over D = 10000 bootstrapped V-statistics Bn\nwith n = Ntest. The test statistic lies in an upper quantile of\nthe bootstrapped null distribution, correctly indicating that\nit is unlikely the test points were generated by the \ufb01tted\nGP model, even for the low number of test data observed,\nn = 41.\nIn a second experiment, we compare against Lloyd &\nGhahramani: we compute the MMD statistic between test\ndata (Xtest, ytest) and (Xtest, yrep), where yrep are samples\nfrom the \ufb01tted GP. We draw 10000 samples from the null\ndistribution by repeatedly sampling new \u02dcyrep from the GP\nplug-in predictive posterior, and comparing (Xtest, \u02dcyrep) to\n(Xtest, yrep). When averaged over 100 repetitions of ran-\ndomly partitioned (X, y) for training and testing, our good-\nness of \ufb01t test produces a p-value that is statistically not\nsigni\ufb01cantly different from the MMD method (p \u22480.1,\nnote that this result is subject to Ntrain, Ntest).\nWe em-\nphasise, however, that Lloyd & Ghahramani\u2019s test requires\nto sample from the \ufb01tted model (here 10000 null samples\nwere required in order to achieve stable p-values). Our test\ndoes not sample from the GP at all and completely side-\nsteps this more costly approach.\nBIAS QUANTIFICATION IN APPROXIMATE MCMC\nWe now illustrate how to quantify bias-variance trade-offs\nin an approximate MCMC algorithm \u2013 austerity MCMC\n(Korattikara et al., 2013). For the purpose of illustration\nwe use a simple generative model from Gorham & Mackey\n0\n50\n100\n150\n200\n250\n300\nVn\n0.000\n0.005\n0.010\n0.015\n0.020\n0.025\n0.030\nFrequency\nVn test\nBootstrapped Bn\nFigure 5. Bootstrapped Bn distribution with the test statistic Vn\nmarked.\n0.001\n0.04\n0.08\n0.13\n0.17\nepsilon\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\np values\nFigure 6. Distribution of p-values as a function of \u03f5 for austerity\nMCMC.\n(2015); Welling & Teh (2011),\n\u03b81 \u223cN(0, 10); \u03b82 \u223cN(0, 1)\nXi \u223c1\n2N(\u03b81, 4) + 1\n2N(\u03b82 + \u03b81, 4).\nAusterity MCMC is a Monte Carlo procedure designed\nto reduce the number of likelihood evaluation in the accep-\ntance step of the Metropolis-Hastings algorithm. The crux\nof method is to look at only a subset of the data, and make\nan acceptance/rejection decision based on this subset. The\nprobability of making a wrong decision is proportional to\na parameter \u03f5 \u2208[0, 1] . This parameter in\ufb02uences the time\ncomplexity of austerity MCMC: when \u03f5 is larger, i.e., when\nthere is a greater tolerance for error, the expected computa-\ntional cost is lower. We simulate {Xi}1\u2264i\u2264400 points from\nthe model with \u03b81 = 0 and \u03b82 = 1. In our experiment,\nthere are two modes in the posterior distribution: one at\n(0, 1) and the other at (1, \u22121). We run the algorithm with\n\u03f5 varying over the range [0.001, 0.2]. For each \u03f5 we calcu-\nlate an individual thinning factor, such that correlation be-\ntween consecutive samples from the chains is smaller than\n0.5 (greater \u03f5 generally required more thinning). For each\n\u03f5 we test the hypothesis that {\u03b8i}1\u2264i\u2264500 is drawn from the\nA Kernel Test of Goodness of Fit\n0.00\n0.05\n0.10\n0.15\n0.20\nepsilon\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\nlikelihood evaluations\nFigure 7. Average number of likelihood evaluations a function of\n\u03f5 for austerity MCMC (the y-axis is in millions of evaluations).\n50\n100\n500\n1000\n2000\n5000\nN\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\np-value\nFigure 8. Density estimation: p-values for an increasing number\nof data N for the non-parametric model. Fixed n = 500.\ntrue stationary posterior, using our goodness of \ufb01t test. We\ngenerate 100 p-values for each \u03f5 , as shown in Figure 6.\nA good approximation of the true stationary distribution is\nobtained at \u03f5 = 0.4, which is still parsimonious in terms of\nlikelihood evaluations, as shown in Figure 7.\nCONVERGENCE IN NON-PARAMETRIC DENSITY\nESTIMATION\nIn our \ufb01nal experiment, we apply our goodness of \ufb01t test\nto measuring quality-of-\ufb01t in nonparametric density estim-\nation. We evaluate two density models: the in\ufb01nite dimen-\nsional exponential family (Sriperumbudur et al., 2014), and\na recent approximation to this model using random Fourier\nfeatures (Strathmann et al., 2015). Our implementation of\nthe model assumes the log density to take the form f(x),\nwhere f lies in an RKHS induced by a Gaussian kernel\nwith bandwidth 1. We \ufb01t the model using N observations\ndrawn from a standard Gaussian, and perform our quad-\nratic time test on a separate evaluation dataset of \ufb01xed size\nn = 500. Our goal is to identify N suf\ufb01ciently large that\n5\n10\n50\n100\n500\n2000\n5000\nm\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\np-value\nFigure 9. Approximate density estimation: p-values for an in-\ncreasing number of random features m. Fixed n = 500.\nthe goodness of \ufb01t test does not reject the null hypothesis\n(i.e., the model has learned the density suf\ufb01ciently well,\nbearing in mind that it is guaranteed to converge for suf-\n\ufb01ciently large N). Figure 8 shows how the distribution of\np-values evolves as a function of N; this distribution is uni-\nform for N = 5000, but at N = 500, the null hypothesis\nwould very rarely be rejected.\nWe next consider the random Fourier feature approxima-\ntion to this model, where the log pdf, f, is approximated us-\ning a \ufb01nite dictionary of random Fourier features (Rahimi\n& Recht, 2007). The natural question when using this ap-\nproximation is: \u201cHow many random features are needed?\u201d\nUsing the same test set size n = 500 as above, and a large\nnumber of samples, N = 5 \u00b7 104, Figure 9 shows the dis-\ntributions of p-values for an increasing number of random\nfeatures m. From m = 50, the null hypothesis would rarely\nbe rejected. Note, however, that the p-values do not have\na uniform distribution, even for a large number of random\nfeatures. This subtle effect is caused by over-smoothing\ndue to the regularisation approach taken by Strathmann\net al. (2015, KMC \ufb01nite), which would not otherwise have\nbeen detected.\nAcknowledgement.\nArthur Gretton has ORCID 0000-\n0003-3169-7624.\nA Kernel Test of Goodness of Fit\nReferences\nAnderson, N., Hall, P., and Titterington, D. Two-sample\ntest statistics for measuring discrepancies between two\nmultivariate probability density functions using kernel-\nbased density estimates. Journal of Multivariate Ana-\nlysis, 50:41\u201354, 1994.\nBardenet, R., Doucet, A., and Holmes, C. Towards scaling\nup Markov Chain Monte Carlo: an adaptive subsampling\napproach. In ICML, pp. 405\u2013413, 2014.\nBaringhaus, L. and Henze, N. A consistent test for mul-\ntivariate normality based on the empirical characteristic\nfunction. Metrika, 35:339\u2013348, 1988.\nBarron, A. R. Uniformly powerful goodness of \ufb01t tests.\nThe Annals of Statistics, 17:107\u2013124, 1989.\nBeirlant, J., Gy\u00f6r\ufb01, L., and Lugosi, G. On the asymptotic\nnormality of the l1- and l2-errors in histogram density\nestimation. Canadian Journal of Statistics, 22:309\u2013318,\n1994.\nBowman, A.W. and Foster, P.J. Adaptive smoothing and\ndensity based tests of multivariate normality. J. Amer.\nStatist. Assoc, 88:529\u2013537, 1993.\nBradley, R. et al. Basic properties of strong mixing con-\nditions. a survey and some open questions. Probability\nsurveys, 2(107-44):37, 2005.\nCarmeli, Claudio, De Vito, Ernesto, Toigo, Alessandro, and\nUmanit\u00e1, Veronica.\nVector valued reproducing kernel\nhilbert spaces and universality. Analysis and Applica-\ntions, 8(01):19\u201361, 2010.\nChwialkowski, Kacper, Ramdas, Aaditya, Sejdinovic,\nDino, and Gretton, Arthur.\nFast two-sample testing\nwith analytic representations of probability measures. In\nNIPS, pp. 1972\u20131980, 2015.\nChwialkowski, Kacper, Strathmann, Heiko, and Gretton,\nArthur. A kernel test of goodness of \ufb01t. arXiv preprint\narXiv:1602.02964, 2016.\nChwialkowski, Kacper P, Sejdinovic, Dino, and Gretton,\nArthur. A wild bootstrap for degenerate kernel tests. In\nAdvances in neural information processing systems, pp.\n3608\u20133616, 2014.\nDedecker, J., Doukhan, P., Lang, G., Louhichi, S., and\nPrieur, C. Weak dependence: with examples and applic-\nations, volume 190. Springer, 2007.\nDedecker, J\u00e9r\u00f4me and Prieur, Cl\u00e9mentine. New depend-\nence coef\ufb01cients. examples and applications to statistics.\nProbability Theory and Related Fields, 132(2):203\u2013236,\n2005.\nFromont, M., Laurent, B, Lerasle, M, and Reynaud-Bouret,\nP. Kernels based tests with non-asymptotic bootstrap ap-\nproaches for two-sample problems. In COLT, pp. 23.1\u2013\n23.22, 2012.\nGelman, A. and Rubin, D.B. Inference from iterative sim-\nulation using multiple sequences. Statistical science, pp.\n457\u2013472, 1992.\nGorham, J. and Mackey, L. Measuring sample quality with\nstein\u2019s method. In NIPS, pp. 226\u2013234, 2015.\nGretton, A. and Gyor\ufb01, L. Consistent nonparametric tests\nof independence.\nJournal of Machine Learning Re-\nsearch, 11:1391\u20131423, 2010.\nGretton, A., Fukumizu, K., Teo, C, Song, L., Sch\u00f6lkopf, B.,\nand Smola, A. A kernel statistical test of independence.\nIn NIPS, volume 20, pp. 585\u2013592, 2007.\nGretton, A., Borgwardt, K.M., Rasch, M.J., Sch\u00f6lkopf, B.,\nand Smola, A. A kernel two-sample test. J. Mach. Learn.\nRes., 13:723\u2013773, 2012.\nGy\u00f6r\ufb01, L. and Vajda, I. Asymptotic distributions for good-\nness of \ufb01t statistics in a sequence of multinomial models.\nStatistics and Probability Letters, 56:57\u201367, 2002.\nGy\u00f6r\ufb01, L. and van der Meulen, E. C. A consistent goodness\nof \ufb01t test based on the total variation distance. In Rous-\nsas, G. (ed.), Nonparametric Functional Estimation and\nRelated Topics, pp. 631\u2013645. Kluwer, Dordrecht, 1990.\nKolmogorov, A.\nSulla determinazione empirica di una\nlegge di distribuzione.\nG. Ist. Ital. Attuari, 4:83\u201391,\n1933.\nKorattikara, A., Chen, Y., and Welling, M. Austerity in\nMCMC Land: Cutting the Metropolis-Hastings Budget.\nIn ICML, pp. 181\u2013189, 2014.\nKorattikara, Anoop, Chen, Yutian, and Welling, Max. Aus-\nterity in mcmc land: Cutting the metropolis-hastings\nbudget. arXiv preprint arXiv:1304.5299, 2013.\nLeucht, A. Degenerate U- and V-statistics under weak de-\npendence: Asymptotic theory and bootstrap consistency.\nBernoulli, 18(2):552\u2013585, 2012.\nLeucht, A. and Neumann, M.H. Dependent wild bootstrap\nfor degenerate U- and V-statistics. Journal of Multivari-\nate Analysis, 117:257\u2013280, 2013. ISSN 0047-259X. doi:\n10.1016/j.jmva.2013.03.003.\nLiu, Q., Lee, J., and Jordan, M. I. A kernelized stein dis-\ncrepancy for goodness-of-\ufb01t tests and model evaluation.\nIn ICML, 2016.\nA Kernel Test of Goodness of Fit\nLloyd, James R and Ghahramani, Zoubin. Statistical model\ncriticism using kernel two sample tests.\nIn NIPS, pp.\n829\u2013837, 2015.\nOates, C., Girolami, M., and Chopin, N.\nControl func-\ntionals for monte carlo integration.\narXiv preprint\narXiv:1410.2392v4, 2016. To appear, JRSS B.\nRahimi, A. and Recht, B. Random features for large-scale\nkernel machines. In NIPS, pp. 1177\u20131184, 2007.\nRizzo, M. L. New goodness-of-\ufb01t tests for pareto distri-\nbutions. ASTIN Bulletin: Journal of the International\nAssociation of Actuaries, 39(2):691\u2013715, 2009.\nSejdinovic, D., Sriperumbudur, B., Gretton, A., and Fuku-\nmizu, K.\nEquivalence of distance-based and RKHS-\nbased statistics in hypothesis testing. Ann. Statist., 41\n(5):2263\u20132291, 2013.\nSer\ufb02ing, R.\nApproximation Theorems of Mathematical\nStatistics. Wiley, New York, 1980.\nShao, X. The dependent wild bootstrap. J. Amer. Statist.\nAssoc., 105(489):218\u2013235, 2010.\nSmirnov, N. Table for estimating the goodness of \ufb01t of em-\npirical distributions. Annals of Mathematical Statistics,\n19:279\u2013281, 1948.\nSriperumbudur, B., Gretton, A., Fukumizu, K., Lanckriet,\nG., and Sch\u00f6lkopf, B.\nHilbert space embeddings and\nmetrics on probability measures. J. Mach. Learn. Res.,\n11:1517\u20131561, 2010.\nSriperumbudur, B., Fukumizu, K., Kumar, R., Gretton,\nA., and Hyv\u00e4rinen, A.\nDensity Estimation in In\ufb01n-\nite Dimensional Exponential Families.\narXiv preprint\narXiv:1312.3516, 2014.\nStein, Charles. A bound for the error in the normal ap-\nproximation to the distribution of a sum of dependent\nrandom variables. In Proceedings of the Sixth Berkeley\nSymposium on Mathematical Statistics and Probability,\nVolume 2: Probability Theory, pp. 583\u2013602, Berkeley,\nCalif., 1972. University of California Press.\nSteinwart, I. and Christmann, A. Support vector machines.\nInformation Science and Statistics. Springer, New York,\n2008. ISBN 978-0-387-77241-7.\nStrathmann, H., Sejdinovic, D., Livingstone, S., Szabo, Z.,\nand Gretton, A. Gradient-free Hamiltonian Monte Carlo\nwith Ef\ufb01cient Kernel Exponential Families.\nIn NIPS,\n2015.\nSz\u00e9kely, G. J. and Rizzo, M. L. A new test for multivariate\nnormality. J. Multivariate Analysis, 93(1):58\u201380, 2005.\nWelling, M. and Teh, Y.W.\nBayesian Learning via\nStochastic Gradient Langevin Dynamics. In ICML, pp.\n681\u2013688, 2011.\nA Kernel Test of Goodness of Fit\nAppendix\n5. Proofs\nLemma 5.1. If a random variable X is distributed according to p, under conditions on the kernel\n0 =\nI\n\u2202X\nk(x, x\u2032)q(x)n(x)dS(x\u2032),\n0 =\nI\n\u2202X\n\u2207xk(x, x\u2032)\u22a4n(x\u2032)q(x\u2032)dS(x\u2032),\nand then for all f \u2208F, the expected value of T is zero, i.e. Ep(Tf)(X) = 0.\nProof. This result was proved on bounded domains X \u2282Rd by Oates et al. (2016, Lemma 1), where n(x) is the unit\nvector normal to the boundary at x, and\nH\n\u2202X is the surface integral over the boundary \u2202X. The case of unbounded domains\nwas discussed by Oates et al. (2016, Remark 2). Here we provide an alternative, elementary proof for the latter case. First\nwe show that the function p \u00b7 fi vanishes at in\ufb01nity, by which we mean that for all dimensions j\nlim\nxj\u2192\u221ep(x1, \u00b7 \u00b7 \u00b7 , xd) \u00b7 fi(x1, \u00b7 \u00b7 \u00b7 , xd) = 0.\nThe density function p vanishes at in\ufb01nity. The function f is bounded, which is implied by Cauchy-Schwarz inequality,\n|f(x)| \u2264\u2225f\u2225\np\nk(x, x). This implies that the function p \u00b7 fi vanishes at in\ufb01nity. We check that the expected value\nEp(Tp)f(X) is zero. For all dimensions i,\nEp(Tp)f(X)\n= Ep\n\u0012\u2202log p(X)\n\u2202xi\nfi(X) + \u2202fi(X)\n\u2202xi\n\u0013\n=\nZ\nRd\n\u0014\u2202log p(x)\n\u2202xi\nfi(x) + \u2202fi(x)\n\u2202xi\n\u0015\np(x)dx\n=\nZ\nRd\n\u0014 1\np(x)\n\u2202p(x)\n\u2202xi\nf(x) + \u2202f(x)\n\u2202xi\n\u0015\np(x)dx\n=\nZ\nRd\n\u0014\u2202p(x)\n\u2202xi\nfi(x) + \u2202fi(x)\n\u2202xi\np(x)\n\u0015\ndx\n(a)\n=\nZ\nRd\u22121\n \nlim\nR\u2192\u221ep(x)fi(x)\n\f\f\f\f\nxi=R\nxi=\u2212R\n!\ndx1 \u00b7 \u00b7 \u00b7 dxi\u22121 \u00b7 \u00b7 \u00b7 dxi+1 \u00b7 \u00b7 \u00b7 dxd\n=\nZ\nRd\u22121\n0dx1 \u00b7 \u00b7 \u00b7 dxi\u22121 \u00b7 \u00b7 \u00b7 dxi+1 \u00b7 \u00b7 \u00b7 dxd\n= 0.\nFor the equation (a) we have used integration by parts, the fact that p(x)fi(x) vanishes at in\ufb01nity, and the Fubini-\nToneli theorem to show that we can do iterated integration. The suf\ufb01cient condition for the Fubini-Toneli theorem is\nthat Eq\u27e8f, \u03bep(Z)\u27e92 < \u221e. This is true since Ep\u2225\u03bep(X)\u22252 \u2264Ephp(X, X) < \u221e.\nProof of proposition 3.1. We check assumptions of Theorem 2.1 from (Leucht, 2012). Condition A1, P\u221e\nt=1\np\n\u03c4(t) \u2264\u221e,\nis implied by assumption P\u221e\nt=1 t2p\n\u03c4(t) \u2264\u221ein Section 3. Condition A2 (iv), Lipschitz continuity of h, is assumed.\nConditions A2 i), ii) positive de\ufb01niteness, symmetry and degeneracy of h follow from the proof of Theorem (2.2). Indeed\nhp(x, y) = \u27e8\u03bep(x), \u03bep(y)\u27e9Fd\nA Kernel Test of Goodness of Fit\nso the statistic is an inner product and hence positive de\ufb01nite. Degeneracy under the null follows from the fact that, by\nTheorem 2.1, Eq\u03bep(Z) = 0. Finally, condition A2 (iii), Ephp(X, X) \u2264\u221e, is assumed.\nProof of proposition 3.2. We use Theorem 2.1 (Leucht, 2012) to see that, under the null hypothesis, f(Z1,n, \u00b7 \u00b7 \u00b7 , Zt,n)\nconverges to zero in probability. Condition A1, P\u221e\nt=1\np\n\u03c4(t) \u2264\u221e, is implied by assumption P\u221e\nt=1 t2p\n\u03c4(t) \u2264\u221e\nin Section 3. Condition A2 (iv), Lipschitz continuity of h, is assumed.. Assumption B1 is identical to our assumption\nP\u221e\nt=1 t2p\n\u03c4(t) \u2264\u221efrom Section 3. Finally we check assumption B2 (bootstrap assumption): {Wt,n}1\u2264t\u2264n is a row-\nwise strictly stationary triangular array independent of all Zt such that EWt,n = 0 and supn E|W 2+\u03c3\nt,n | = 1 < \u221efor some\n\u03c3 > 0. The autocovariance of the process is given by EWs,nWt,n = (1\u22122pn)\u2212|s\u2212t|, so the function \u03c1(x) = exp(\u2212x), and\nln = log(1 \u22122pn)\u22121. We verify that limu\u21920 \u03c1(u) = 1. If we set pn = w\u22121\nn\n, such that wn = o(n) and limn\u2192\u221ewn = \u221e,\nthen ln = O(wn) and Pn\u22121\nr=1 \u03c1(|r|/ln) =\n1\u2212(1\u22122pn)n+1\npn\n= O(wn) = O(ln). Under the alternative hypothesis, Bn\nconverges to zero - we use (Chwialkowski et al., 2014, Theorem 2), where the only assumption \u03c4(r) = o(r\u22124) is satis\ufb01ed\nsince P\u221e\nt=1 t2p\n\u03c4(t) \u2264\u221e. We check the assumption\nsup\nn\nsup\ni,j<n\nEqhp(Zi, Zj)2 < \u221e.\nWe have Eqhp(Z, Z\u2032)2 \u2264\n\u0000Eq\u2225\u03bep(Z)\u22252\u00012 = (Eqhp(Z, Z))2 < \u221e.\nWe show that under the alternative hypothesis, Vn converges to a positive constant \u2013 using (Chwialkowski et al.,\n2014, Theorem 3).\nThe zero comportment of h is positive since S2(Z)\np\n>\n0.\nWe checked the assumption\nsupn supi,j<n Eqhp(Zi, Zj)2 < \u221eabove.\n6. MCMC convergence testing\nStationary phase.\nIn the stationary phase there are number of results which might be used to show that the chain is\n\u03c4-mixing.\nStrong mixing coef\ufb01cients.\nStrong mixing is historically the most studied type of temporal dependence \u2013 a lot of models,\nincluding Markov Chains, are proved to be strongly mixing, therefore it\u2019s useful to relate weak mixing to strong mixing.\nFor a random variable X on a probability space (\u2126, F, PX) and M \u2282F we de\ufb01ne\n\u03b2(M, \u03c3(X)) = \u2225\nsup\nA\u2208B(R)\n|PX|M(A) \u2212PX(A)|\u2225.\nA process is called \u03b2-mixing or absolutely regular if\n\u03b2(r) = sup\nl\u2208N\n1\nl\nsup\nr\u2264i1\u2264...\u2264il\n\u03b2(F0, (Xi1, ..., Xil))\nr\u2192\u221e\n\u2212\u21920.\nDedecker & Prieur (2005)[Equation 7.6] relates \u03c4-mixing and \u03b2-mixing , as follows: if Qx is the generalized inverse of\nthe tail function\nQx(u) = inf\nt\u2208R{P(|X| > t) \u2264u},\nthen\n\u03c4(M, X) \u22642\nZ \u03b2(M,\u03c3(X))\n0\nQx(u)du.\nWhile this de\ufb01nition can be hard to interpret, it can be simpli\ufb01ed in the case E|X|p = M for some p > 1, since via\nMarkov\u2019s inequality P(|X| > t) \u2264M\ntp , and thus M\ntp \u2264u implies P(|X| > t) \u2264u. Therefore Q\u2032(u) =\nM\np\u221au \u2265Qx(u). As\na result, we have the inequality\npp\n\u03b2(M, \u03c3(X))\nM\n\u2265C\u03c4(M, X).\n(3)\nA Kernel Test of Goodness of Fit\nDedecker & Prieur (2005) provide examples of systems that are \u03c4-mixing. In particular, given that certain assumptions are\nsatis\ufb01ed, then causal functions of stationary sequences, iterated random functions, Markov chains, and expanding maps\nare all \u03c4-mixing.\nOf particular interest to this work are Markov chains. The assumptions provided by (Dedecker & Prieur, 2005), under which\nMarkov chains are \u03c4-mixing, are dif\ufb01cult to check. We can, however, use classical theorems about the absolute regularity\n(\u03b2-mixing). In particular (Bradley et al., 2005, Corollary 3.6) states that a Harris recurrent and aperiodic Markov chain\nsatis\ufb01es absolute regularity, and (Bradley et al., 2005, Theorem 3.7) states that geometric ergodicity implies geometric\ndecay of the \u03b2 coef\ufb01cient. Interestingly (Bradley et al., 2005, Theorem 3.2) describes situations in which a non-stationary\nchain \u03b2-mixes exponentially.\nUsing inequalities between \u03c4-mixing coef\ufb01cient and strong mixing coef\ufb01cients, one can use these classical theorems show\nthat e.g for p = 2 we have\np\n\u03b2(M, \u03c3(X)) \u2265\u03c4(M, X).\n",
        "sentence": "",
        "context": "surveys, 2(107-44):37, 2005.\nCarmeli, Claudio, De Vito, Ernesto, Toigo, Alessandro, and\nUmanit\u00e1, Veronica.\nVector valued reproducing kernel\nhilbert spaces and universality. Analysis and Applica-\ntions, 8(01):19\u201361, 2010.\nSteinwart, I. and Christmann, A. Support vector machines.\nInformation Science and Statistics. Springer, New York,\n2008. ISBN 978-0-387-77241-7.\nStrathmann, H., Sejdinovic, D., Livingstone, S., Szabo, Z.,\nrandom variables. In Proceedings of the Sixth Berkeley\nSymposium on Mathematical Statistics and Probability,\nVolume 2: Probability Theory, pp. 583\u2013602, Berkeley,\nCalif., 1972. University of California Press."
    },
    {
        "title": "Approximate slice sampling for Bayesian posterior inference",
        "author": [
            "C. DuBois",
            "A. Korattikara",
            "M. Welling",
            "P. Smyth"
        ],
        "venue": "In Proc. 17th AISTATS,",
        "citeRegEx": "DuBois et al\\.,? \\Q2014\\E",
        "shortCiteRegEx": "DuBois et al\\.",
        "year": 2014,
        "abstract": "",
        "full_text": "",
        "sentence": " Selecting sampler hyperparameters The approximate slice sampler of DuBois et al. (2014) is a biased MCMC procedure designed to accelerate inference when the target density takes the form p(x) \u221d \u03c0(x) \u220fL l=1 \u03c0(yl|x) for \u03c0(\u00b7) a prior distribution on R and \u03c0(yl|x) the likelihood of a datapoint yl. The score statistic of Fan et al. (2006) and the Gibbs sampler convergence criteria of Zellner & Min (1995) detect certain forms of non-convergence but fail to detect others due to the finite number of test functions tested. The score statistic of Fan et al. (2006) and the Gibbs sampler convergence criteria of Zellner & Min (1995) detect certain forms of non-convergence but fail to detect others due to the finite number of test functions tested.",
        "context": null
    },
    {
        "title": "Reflection couplings and contraction rates for diffusions",
        "author": [
            "A. Eberle"
        ],
        "venue": "Probab. Theory Related Fields, pp",
        "citeRegEx": "Eberle,? \\Q2015\\E",
        "shortCiteRegEx": "Eberle",
        "year": 2015,
        "abstract": "We consider contractivity for diffusion semigroups w.r.t. Kantorovich ($L^1$\nWasserstein) distances based on appropriately chosen concave functions. These\ndistances are inbetween total variation and usual Wasserstein distances. It is\nshown that by appropriate explicit choices of the underlying distance,\ncontractivity with rates of close to optimal order can be obtained in several\nfundamental classes of examples where contractivity w.r.t. standard Wasserstein\ndistances fails. Applications include overdamped Langevin diffusions with\nlocally non-convex potentials, products of these processes, and systems of\nweakly interacting diffusions, both of mean-field and nearest neighbour type.",
        "full_text": "arXiv:1305.1233v4  [math.PR]  19 Oct 2015\nPTRF manuscript No.\n(will be inserted by the editor)\nRe\ufb02ection couplings and contraction rates for di\ufb00usions\nAndreas Eberle\nReceived: date / Accepted: date\nAbstract We consider contractivity for di\ufb00usion semigroups w.r.t. Kantorovich (L1 Wasserstein)\ndistances based on appropriately chosen concave functions. These distances are inbetween total\nvariation and usual Wasserstein distances. It is shown that by appropriate explicit choices of the\nunderlying distance, contractivity with rates of close to optimal order can be obtained in several\nfundamental classes of examples where contractivity w.r.t. standard Wasserstein distances fails.\nApplications include overdamped Langevin di\ufb00usions with locally non-convex potentials, products\nof these processes, and systems of weakly interacting di\ufb00usions, both of mean-\ufb01eld and nearest\nneighbour type.\nKeywords Couplings of di\ufb00usion processes \u00b7 Wasserstein distances \u00b7 Absence of convexity \u00b7\nConcave distance functions \u00b7 Quantitative bounds for convergence to stationarity\nMathematics Subject Classi\ufb01cation (2010) 60J60 \u00b7 60H10\n1 Introduction\nConsider a di\ufb00usion process (Xt)t\u22650 in Rd de\ufb01ned by a stochastic di\ufb00erential equation\ndXt = b(Xt) dt + \u03c3 dBt.\n(1)\nHere (Bt)t\u22650 is a d-dimensional Brownian motion, \u03c3 \u2208Rd\u00d7d is a constant d \u00d7 d matrix with\ndet \u03c3 > 0, and b : Rd \u2192Rd is a locally Lipschitz continuous function. We assume that the unique\nstrong solution of (1) is non-explosive for any initial condition, which is essentially a consequence\nof the assumptions imposed further below. The transition kernels of the di\ufb00usion process on Rd\nde\ufb01ned by (1) will be denoted by pt(x, dy).\nContraction properties of the transition semigroup (pt)t\u22650 have been studied by various ap-\nproaches. In particular, L2 and entropy methods (e.g. spectral gap estimates, logarithmic Sobolev\nand transportation inequalities) yield bounds that both are relatively stable under perturbations\nand applicable in high dimensions, cf. e.g. [2\u20137,38,43]. On the other hand, coupling methods pro-\nvide a more intuitive probabilistic understanding of convergence to equilibrium [13,14, 16, 24, 25,\n34, 35, 41, 43]. In contrast to L2 and entropy methods, bounds resulting from coupling methods\ntypically hold for arbitrary initial values x0 \u2208Rd. In many applications, couplings are used to\nbound the total variation distances dT V (\u00b5pt, \u03bdpt) between the laws \u00b5pt and \u03bdpt of Xt w.r.t. two\ndi\ufb00erent initial distributions \u00b5 and \u03bd at a given time t \u22650 , cf. [34,35]. Typically, however, the\ntotal variation distance is decaying substantially only after a certain amount of time. This is also\nmanifested in cut-o\ufb00phenomena [12,19,20,33].\nAndreas Eberle\nInstitute for Applied Mathematics, University of Bonn, Endenicher Allee 60, 53115 Bonn, Germany\nE-mail: eberle@uni-bonn.de\n2\nAndreas Eberle\nAlternatively, it is well-known that synchronuous couplings (i.e., couplings given by the \ufb02ow\nof the s.d.e. (1)) can be used to show that the map \u00b5 7\u2192\u00b5pt is exponentially contractive w.r.t.\nLp Wasserstein distances W p for any p \u2208[1, \u221e) if, for example, (Xt) is an overdamped Langevin\ndi\ufb00usion with a strictly convex potential U \u2208C2(Rd), i.e., \u03c3 = Id and b = \u2212\u2207U/2, see e.g. [7].\nThis leads to an elegant and powerful approach to convergence to equilibrium and to many related\nresults if applicable. However, it has been pointed out in [37] that strict convexity of U is also a\nnecessary condition for exponential contractivity w.r.t. W p. This seems to limit the applicability\nsubstantially.\nHere, we are instead considering exponential contractivity w.r.t. Kantorovich (L1 Wasserstein)\ndistances Wf based on underlying distance functions of the form\ndf(x, y) = f(\u2225x \u2212y\u2225)\non Rd,\nand, more generally,\ndf(x, y) =\nn\nX\ni=1\nfi(\u2225xi \u2212yi\u2225)\non Rd1 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 Rdn,\nwhere f, fi : [0, \u221e) \u2192[0, \u221e) are strictly increasing concave functions, cf. Sections 2.1 and 3.1 below\nfor details. For proving exponential contractivity, we will apply a re\ufb02ection coupling on Rd and an\n(approximate) componentwise re\ufb02ection coupling on products of Euclidean spaces. It will become\nclear by the proofs below, that for distances based on concave functions f, fi, these couplings are\nsuperior to synchronuous couplings, whereas the synchronuous couplings are superior w.r.t. the\nWasserstein distances W p for p > 1, cf. e.g. Lemma 4.\nThe idea to study contraction properties w.r.t. Kantorovich distances based on concave distance\nfunctions appears in Chen and Wang [15, 16, 42] and Hairer and Mattingly [24]. In [16], similar\nmethods are applied to estimate spectral gaps of di\ufb00usion generators on Rd and on manifolds. In [24]\nand [25], Hairer, Mattingly and Scheutzow apply Wasserstein distances based on particular concave\ndistance functions to prove exponential ergodicity in in\ufb01nite dimensional situations. The key idea\nbelow is to obtain more quantitative results by \u201calmost\u201d optimizing the choice of the functions f\nand fi to obtain large contraction rates. In the case n = 1, this idea has also been exploited in [16]\nto derive lower bounds for spectral gaps. The novelty here is that we suggest a simple and very\nexplicit choice for f that leads to close to optimal results in several examples. Furthermore, by a\nnew extension to the product case based on an approximate componentwise re\ufb02ection coupling,\nwe obtain dimension free contraction results in product models and perturbations thereof without\nrelying on convexity.\nBefore stating the general results, we consider some examples illustrating the scope of the\napproach:\nExample 1 (Overdamped Langevin dynamics with locally non-convex potential) Suppose\nthat \u03c3 = Id and b(x) = \u22121\n2\u2207U(x) for a function U \u2208C2(Rd) that is strictly convex outside a given\nball B \u2282Rd. Then Z :=\n\u00b4\nexp(\u2212U(x))dx is \ufb01nite, and the probability measure\nd\u00b5 = Z\u22121 exp(\u2212U) dx\nis a stationary distribution for the di\ufb00usion process (Xt). Corollary 2 below yields exponential\ncontractivity for the transition semigroup (pt) with an explicit rate w.r.t. an appropriate Kan-\ntorovich distance Wf. As a consequence, we obtain dimension-independent upper bounds for the\nstandard L1 Wasserstein distances between the laws \u03bdpt of Xt and \u00b5 for arbitrary initial distri-\nbutions \u03bd and t \u22650. These bounds are of of optimal order in R, L \u2208[0, \u221e) and K \u2208(0, \u221e) if\n(x \u2212y) \u00b7 (\u2207U(x) \u2212\u2207U(y)) is bounded from below by \u2212L|x \u2212y|2 for |x \u2212y| < R and by K|x \u2212y|2\nfor |x \u2212y| \u2265R.\nRe\ufb02ection couplings and contraction rates\n3\nExample 2 (Product models) For a di\ufb00usion process Xt = (X1\nt , . . . , Xn\nt ) in Rn\u00b7d with indepen-\ndent Langevin di\ufb00usions X1, . . . , Xn as in Example 1, Theorem 7 below yields exponential con-\ntractivity in an appropriate Kantorovich distance with rate c = min(c1, . . . , cn) where c1, . . . , cn\nare the lower bounds obtained for the contraction rates of the components.\nExample 3 (Systems of interacting di\ufb00usions) More generally, consider a system\ndXi\nt = \u22121\n2\u2207U(Xi\nt) dt \u2212\u03b1\nn\nn\nX\nj=1\n\u2207V (Xi\nt \u2212Xj\nt ) dt + dBi\nt,\ni = 1, . . . , n,\nof n interacting di\ufb00usion processes in Rd where U \u2208C2(Rd) is strictly convex outside a ball,\nV \u2208C2(Rd) has bounded second derivatives, and B1, . . . , Bn are independent Brownian motions\nin Rd. Then Corollary 9 below shows that for \u03b1 su\ufb03ciently small, exponential contractivity holds\nin an appropriate Kantorovich distance with a rate that does not depend on n.\nWe now introduce brie\ufb02y the couplings to be considered in the proofs below:\nA coupling by re\ufb02ection of two solutions of (1) with initial distributions \u00b5 and \u03bd is a di\ufb00usion\nprocess (Xt, Yt) with values in R2d de\ufb01ned by (X0, Y0) \u223c\u03b7 where \u03b7 is a coupling of \u00b5 and \u03bd,\ndXt = b(Xt) dt + \u03c3 dBt\nfor t \u22650,\n(2)\ndYt = b(Yt) dt + \u03c3(I \u22122ete\u22a4\nt ) dBt\nfor t < T,\nYt = Xt for t \u2265T.\n(3)\nHere ete\u22a4\nt is the orthogonal projection onto the unit vector\net := \u03c3\u22121(Xt \u2212Yt)/|\u03c3\u22121(Xt \u2212Yt)|,\nand T = inf{t \u22650 : Xt = Yt} is the coupling time, i.e., the \ufb01rst hitting time of the diagonal\n\u2206= {(x, y) \u2208R2d : x = y}, cf. [14, 35]. The re\ufb02ection coupling can be realized as a di\ufb00usion\nprocess in R2d, and the marginal processes (Xt)t\u22650 and (Yt)t\u22650 are solutions of (1) w.r.t. the\nBrownian motions Bt and\n\u02c7Bt =\n\u02c6 t\n0\n(Id \u22122I{s<T }ese\u22a4\ns ) dBs.\nNotice that by L\u00b4evy\u2019s characterization, \u02c7B is indeed a Brownian motion since the process Id \u2212\n2I{s<T }ese\u22a4\ns takes values in the orthogonal matrices. The di\ufb00erence vector\nZt := Xt \u2212Yt\nsolves the s.d.e.\ndZt = (b(Xt) \u2212b(Yt)) dt + 2|\u03c3\u22121Zt|\u22121Zt dWt\nfor t < T,\n(4)\nZt = 0\nfor t \u2265T,\nw.r.t. the one-dimensional Brownian motion\nWt =\n\u02c6 t\n0\ne\u22a4\ns dBs.\nA synchronuous coupling of two solutions of (1) is de\ufb01ned correspondingly with et \u22610, i.e.,\nthe same noise is applied both to Xt and Yt. Below we will also consider mixed couplings that are\nre\ufb02ection couplings for certain values of Zt, synchronuous couplings for other values of Zt, and\nmixtures of both types of couplings for Zt in an intermediate region. Notice that the standard\nre\ufb02ection coupling introduced above is a synchronuous coupling for t \u2265T, i.e., if Zt = 0 !\nMore generally, we will consider couplings for di\ufb00usion processes on product spaces (such as\nin Examples 2 and 3) that are approximately componentwise re\ufb02ection couplings, i.e., the i-th\ncomponent (Xi\nt, Y i\nt ) of the coupling (Xt, Yt) is de\ufb01ned similarly to (3) provided |Xi\nt \u2212Y i\nt | \u2265\u03b4 for\na given constant \u03b4 > 0, cf. Section 6 below.\n4\nAndreas Eberle\nFor di\ufb00usion processes with non-constant di\ufb00usion matrix \u03c3(x), the re\ufb02ection coupling should\nbe replaced by the Kendall-Cranston coupling w.r.t. the intrinsic Riemannian metric G(x) =\n\u0000\u03c3(x)\u03c3(x)T\u0001\u22121 induced by the di\ufb00usion coe\ufb03cients, cf. [17, 28, 31, 43]. Here, we restrict ourselves\nto the case of constant di\ufb00usion matrices where the Kendall-Cranston coupling coincides with the\nstandard coupling by re\ufb02ection.\nThe main results of this paper are stated in Section 2 for re\ufb02ection coupling, and in Section 3\nfor componentwise re\ufb02ection coupling on product spaces. The proofs are contained in Sections 4, 5\nand 6. A part of the results in Section 2 have been announced in the Comptes Rendus Note [21].\n2 Main results for re\ufb02ection coupling\n2.1 Re\ufb02ection couplings and contractivity on Rd\nLindvall and Rogers [35] introduced coupling by re\ufb02ection in order to derive upper bounds for the\ntotal variation distance of the distributions of Xt and Yt at a given time t \u22650. Here we are instead\nconsidering the Kantorovich-Rubinstein (L1-Wasserstein) distances\nWf(\u00b5, \u03bd) = inf\n\u03b7\n\u02c6\ndf(x, y) \u03b7(dx dy),\ndf(x, y) = f(\u2225x \u2212y\u2225)\n(x, y \u2208Rd),\n(5)\nof probability measures \u00b5, \u03bd on Rd, where the in\ufb01mum is over all couplings \u03b7 of \u00b5 and \u03bd, f : [0, \u221e) \u2192\n[0, \u221e) is an appropriately chosen concave increasing function with f(0) = 0, and \u2225z\u2225=\n\u221a\nz \u00b7 Gz\nwith G \u2208Rd\u00d7d symmetric and strictly positive de\ufb01nite. Typical choices for the norm are the\nEuclidean norm \u2225z\u2225= |z| and the intrinsic metric \u2225z\u2225= |\u03c3\u22121z| corresponding to G = Id and\nG = (\u03c3\u03c3\u22a4)\u22121 respectively.\nRemark 1 (Interpolating between total variation and Wasserstein distances) For the\nchoice of the function f there are two extreme cases with minimal and maximal concavity:\n1. Choosing f(x) = x yields the standard Kantorovich (L1 Wasserstein) distance Wf = W 1. In\nthis case it is well known that if, for example, G = \u03c3 = Id and b(x) = \u2212\u2207U(x)/2, then the\ntransition kernels pt(x, dy) of the di\ufb00usion process (Xt) satisfy\nWf(\u00b5pt, \u03bdpt) \u2264e\u2212Kt/2 Wf(\u00b5, \u03bd)\nfor any \u00b5, \u03bd and t \u22650,\nprovided \u22072U \u2265K \u00b7 Id holds globally. This condition is also sharp in the sense that if U is not\nglobally strictly convex, then contractivity of pt w.r.t. Wf does not hold, cf. Sturm and von\nRenesse [37].\n2. On the other hand, choosing f(x) = I(0,\u221e)(x) yields the total variation distance Wf = dT V . In\nthis case,\nWf(\u00b5pt, \u03bdpt) \u2264P[T > t]\nfor any \u00b5, \u03bd and t \u22650,\nbut there is no strict contractivity of pt w.r.t. dT V in general. Indeed, in many applica-\ntions dT V (\u00b5pt, \u03bdpt) only decreases substantially after a certain amount of time (\u201ccut-o\ufb00phe-\nnomenon\u201d).\nBy choosing for f an appropriate concave function, exponential contractivity w.r.t. Wf may\nhold even without global convexity, cf. [16]. We now explain how the function f can be chosen in\na very explicit way such that the obtained exponential decay rate w.r.t. the Kantorovich distance\nWf di\ufb00ers from the maximal decay rate that we can achieve by our approach based on re\ufb02ection\ncoupling only by a constant factor.\nAt \ufb01rst, similarly to Lindvall and Rogers [35], let us de\ufb01ne for r \u2208(0, \u221e):\n\u03ba(r) = inf\n\u001a\n\u22122 |\u03c3\u22121(x \u2212y)|2\n\u2225x \u2212y\u22252\n(x \u2212y) \u00b7 G(b(x) \u2212b(y))\n\u2225x \u2212y\u22252\n: x, y \u2208Rd s.t. \u2225x \u2212y\u2225= r\n\u001b\n,\nRe\ufb02ection couplings and contraction rates\n5\ni.e., \u03ba(r) is the largest constant such that\n(x \u2212y) \u00b7 G(b(x) \u2212b(y)) \u2264\u22121\n2\u03ba(r)\u2225x \u2212y\u22254/|\u03c3\u22121(x \u2212y)|2\n(6)\nholds for any x, y \u2208Rd with \u2225x \u2212y\u2225= r. Notice that if \u2225\u00b7 \u2225is the intrinsic metric then the factor\n|\u03c3\u22121(x \u2212y)|2/\u2225x \u2212y\u22252 equals 1 . In Example 1 with G = Id, we have\n\u03ba(r) = inf\n\u001a\u02c6 1\n0\n\u22022\n(x\u2212y)/|x\u2212y|U((1 \u2212t)x + ty) dt : x, y \u2208Rd s.t. |x \u2212y| = r\n\u001b\n.\nWe assume from now on that \u03ba(r) is a continuous function on (0, \u221e) satisfying\nlim inf\nr\u2192\u221e\u03ba(r) > 0\nand\n\u02c6 1\n0\nr\u03ba(r)\u2212dr < \u221e.\n(7)\nIn Example 1 with G = Id, this assumption is satis\ufb01ed if U is strictly convex outside a ball.\nNext, we de\ufb01ne constants R0, R1 \u2208[0, \u221e) with R0 \u2264R1 by\nR0 = inf{R \u22650 : \u03ba(r) \u22650 \u2200r \u2265R},\n(8)\nR1 = inf{R \u2265R0 : \u03ba(r)R(R \u2212R0) \u22658 \u2200r \u2265R},\n(9)\nNotice that by (7), both constants are \ufb01nite. We now consider the particular distance function\ndf(x, y) = f(\u2225x \u2212y\u2225) given by\nf(r) =\nr\n\u02c6\n0\n\u03d5(s)g(s) ds,\nwhere\n(10)\n\u03d5(r) = exp\n\uf8eb\n\uf8ed\u22121\n4\nr\n\u02c6\n0\ns\u03ba(s)\u2212ds\n\uf8f6\n\uf8f8,\n\u03a6(r) =\n\u02c6 r\n0\n\u03d5(s) ds,\ng(r) = 1 \u22121\n2\nr\u2227R1\n\u02c6\n0\n\u03a6(s)\n\u03d5(s) ds\n, R1\n\u02c6\n0\n\u03a6(s)\n\u03d5(s) ds.\nLet us summarize some basic properties of the functions \u03d5, g and f:\n\u2013 \u03d5 is decreasing, \u03d5(0) = 1, and \u03d5(r) = \u03d5(R0) for any r \u2265R0,\n\u2013 g is decreasing, g(0) = 1, and g(r) = 1\n2 for any r \u2265R1,\n\u2013 f is concave, f(0) = 0, f \u2032(0) = 1, and\n\u03a6(r)/2 \u2264f(r) \u2264\u03a6(r)\nfor any r \u22650.\n(11)\nThe last statement shows that df and d\u03a6 as well as Wf and W\u03a6 di\ufb00er at most by a factor 2.\nWe will explain in Section 4 below how the choice of f is obtained by trying to maximize the\nexponential decay rate. Let us now state our \ufb01rst main result which will be proven in Section 4.\nTheorem 1 (Exponential contractivity of re\ufb02ection coupling) Let \u03b1 := sup{|\u03c3\u22121z|2 : z \u2208\nRd with \u2225z\u2225= 1}, and de\ufb01ne c \u2208(0, \u221e) by\n1\nc = \u03b1\nR1\n\u02c6\n0\n\u03a6(s)\u03d5(s)\u22121 ds = \u03b1\nR1\n\u02c6\n0\ns\n\u02c6\n0\nexp\n\uf8eb\n\uf8ed1\n4\ns\n\u02c6\nt\nu\u03ba(u)\u2212du\n\uf8f6\n\uf8f8dt ds .\n(12)\nThen for the distance df given by (5) and (10), the function t 7\u2192ectE[df(Xt, Yt)] is decreasing on\n[0, \u221e).\n6\nAndreas Eberle\nThe theorem yields exponential contractivity at rate c > 0 for the transition kernels pt of (1) w.r.t.\nthe Kantorovich distance Wf. Moreover, it implies upper bounds for the standard Kantorovich\n(L1 Wasserstein) distance W 1 = Wid w.r.t. the distance function d(x, y) = \u2225x \u2212y\u2225:\nCorollary 2 For any t \u22650 and any probability measures \u00b5, \u03bd on Rd,\nWf(\u00b5pt, \u03bdpt) \u2264exp(\u2212ct) Wf(\u00b5, \u03bd),\nand\n(13)\nW 1(\u00b5pt, \u03bdpt) \u22642\u03d5(R0)\u22121 exp(\u2212ct) W 1(\u00b5, \u03bd).\n(14)\nNote that the second estimate follows from the \ufb01rst, since by the properties of \u03d5 and g stated\nabove, \u03d5(R0)/2 \u2264f \u2032 \u22641, and hence\n\u03d5(R0)\u2225x \u2212y\u2225/2 \u2264df(x, y) \u2264\u2225x \u2212y\u2225\nfor any x, y \u2208Rd.\n(15)\nThe corollary yields an upper bound for mixing times w.r.t. the Kantorovich distance W 1. For\n\u03b5 > 0 let\n\u03c4W 1(\u03b5) := inf{t \u22650 : W 1(\u00b5pt, \u03bdpt) \u2264\u03b5W 1(\u00b5, \u03bd) \u2200\u00b5, \u03bd \u2208M1(Rd)}.\nThen by Corollary 2,\n\u03c4W 1(\u03b5) \u2264c\u22121 log(2/(\u03b5\u03d5(R0)))\nfor any \u03b5 > 0.\nThe proofs of Theorem 1 and Corollary 2 are given in Section 4 below.\nRemark 2 (Non-constant di\ufb00usion coe\ufb03cients) The methods and results presented above have\nnatural extensions to di\ufb00usion processes with smooth non-constant di\ufb00usion matrices. In that case,\none possibility is to use an ad hoc coupling as in [35], but this leads to restrictive assumptions and\nbounds that are far from optimal. A better approach is to switch to a Riemannian setup where\nthe metric is the intrinsic metric G(x) = (\u03c3(x)\u03c3(x)T)\u22121 given by the di\ufb00usion coe\ufb03cients. The\ndi\ufb00usion process (Xt) can then be represented in the form\ndXt = \u03b2(Xt) dt + dBG\nt\n(16)\nwhere (BG\nt ) is a Brownian motion on the Riemannian manifold (Rd, G), and \u03b2 is a modi\ufb01ed\ndrift vector \ufb01eld. Now, by replacing the re\ufb02ection coupling by the corresponding Kendall-Cranston\ncoupling on (Rd, G), one can expect similar results as above with \u03ba de\ufb01ned as\n\u03ba(r) = 2r\u22121 inf\n\u001a\n\u2212\u27e8\u03b3\u2032\ny,x(r), \u03b2(x)\u27e9+ \u27e8\u03b3\u2032\ny,x(0), \u03b2(y)\u27e9+\n\u02c6 r\n0\nRic(\u03b3\u2032\ny,x(s), \u03b3\u2032\ny,x(s))ds : \u2225x \u2212y\u2225= r\n\u001b\n,\nwhere \u03b3y,x : [0, r] \u2192Rd is the unit speed geodesic from y to x and Ric denotes the Ricci curvature\non (Rd, G), cf. [17,43].\nRemark 3 (Di\ufb00usions with re\ufb02ection on smooth convex domains) The results above also\napply to di\ufb00usion processes on a smooth bounded domain D \u2286Rd with normal re\ufb02ection at the\nboundary [1,10,18,36,40]. In that case the SDE (1) is replaced by\ndXt = b(Xt) dt + n(Xt) d\u2113t + \u03c3 dBt,\n(17)\nwhere n(x) is the interior normal vector at a boundary point x, and (\u2113t) is the local time of (Xt)\non the boundary \u2202D, i.e., t 7\u2192\u2113t is a non-decreasing process that increases only at times when\nXt \u2208\u2202D. Consequently, in the Equation (4) for the coupling di\ufb00erence Zt = Xt \u2212Yt, additional\ndrift terms in the directions n(Xt) and \u2212n(Yt) occur when one of the two copies is at the boundary.\nSince for a convex domain, both Zt \u00b7 n(Xt) \u22640 and \u2212Zt \u00b7 n(Yt) \u22640, the re\ufb02ection at the boundary\nimproves the upper bounds for \u2225Zt\u2225in the proofs below when choosing G = Id. Therefore, the\nassertions of Theorem 1 and Corollary 2 hold true without further change if we take the in\ufb01mum\nin the de\ufb01nition of \u03ba only over x, y \u2208D and choose R0, R1 respectively equal to the diameter of\nD in case the in\ufb01ma in (8) or (9) are over empty sets.\nRe\ufb02ection couplings and contraction rates\n7\n2.2 Consequences\nWe summarize some important consequences of exponential contractivity w.r.t. Kantorovich dis-\ntances as stated in Corollary 2. These consequences are essentially well-known, cf. e.g. Joulin [29],\nJoulin and Ollivier [30], and Komorowski and Walczuk [32] for related results. For the reader\u2019s\nconvenience, the proofs are nevertheless included in Section 4 below. We assume that \u2225z\u2225= |\u03c3\u22121z|\nis the intrinsic metric, b is in C1(Rd, Rd), and\n\u02c6\n|z| pt(x0, dz) < \u221e\n(18)\nholds for some x0 \u2208Rd and any t \u22650. Then, equivalently to (13), Theorem 1 implies Lipschitz\ncontractivity for the transition semigroup\n(ptg)(x) =\n\u02c6\ng(z) pt(x, dz)\nw.r.t. the metric df, i.e.,\n\u2225ptg\u2225Lip(f) \u2264exp(\u2212ct) \u2225g\u2225Lip(f)\n(19)\nholds for any t \u22650 and any Lipschitz continuous function g : Rd \u2192R, where\n\u2225g\u2225Lip(f) = sup\n\u001a|g(x) \u2212g(y)|\ndf(x, y)\n: x, y \u2208Rd s.t. x \u0338= y\n\u001b\ndenotes the Lipschitz semi-norm w.r.t. df. An immediate consequence is the existence of a unique\nstationary distribution \u00b5 with \ufb01nite second moments:\nCorollary 3 (Convergence to equilibrium) There exists a unique stationary distribution \u00b5 of\n(pt)t\u22650 satisfying \u00b4 |y| \u00b5(dy) < \u221eand\nVar\u00b5(g) \u2264(2c)\u22121\u2225g\u22252\nLip(f) for any Lipschitz continuous g : Rd \u2192R.\n(20)\nMoreover, for any probability measure \u03bd on Rd,\nWf(\u00b5, \u03bdpt) \u2264exp(\u2212ct) Wf(\u00b5, \u03bd)\nfor any t \u22650.\n(21)\nWe refer to [7, 11] for other recent results on convergence to equilibrium of di\ufb00usion processes\nin Wasserstein distances.\nFurther important consequences of (19) are quantitative non-asymptotic bounds for the decay\nof correlations and the bias and variance of ergodic averages. Let x0 \u2208Rd and suppose that (X, P)\nis a solution of (1) with initial condition X0 = x0.\nCorollary 4 (Decay of correlations) For any Lipschitz continuous functions g, h : Rd \u2192R and\ns, t \u22650,\nCov (g(Xt), h(Xt+s)) \u22641 \u2212e\u22122ct\n2c\ne\u2212cs \u2225g\u2225Lip(f) \u2225h\u2225Lip(f).\n(22)\nCorollary 5 (Bias and variance of ergodic averages) For any Lipschitz continuous function\ng : Rd \u2192R and t \u2208(0, \u221e),\n\f\f\f\fE\n\u00121\nt\n\u02c6 t\n0\ng(Xs) ds \u2212\n\u02c6\ng d\u00b5\n\u0013\f\f\f\f \u22641 \u2212e\u2212ct\nct\n\u2225g\u2225Lip(f)\n\u02c6\ndf(x0, y) \u00b5(dy),\nand\nVar\n\u00121\nt\n\u02c6 t\n0\ng(Xs) ds\n\u0013\n\u2264\n1\nc2t \u2225g\u22252\nLip(f).\nIn the variance estimate in Corollary 5, one of the factors 1/c is due to the variance bound\n(20) w.r.t. the stationary distribution, whereas the second factor 1/c bounds the decay rate for the\ncorrelations. Short proofs of Corollaries 3, 4, and 5 are included in Section 4.\nRemark 4 (CLT, Gaussian deviation inequality) The contractivity w.r.t. Wf can also be used\nto prove a central limit theorem for the ergodic averages [32] and a Gaussian deviation inequality\nstrengthening Corollary 5, cf. Remark 2.10 in [29].\n8\nAndreas Eberle\n2.3 Examples\nIn order to illustrate the quality of the bounds given in Theorem 1 and in Corollary 2, we esti-\nmate the constant c de\ufb01ned by (12) in di\ufb00erent scenarios, and we study the behaviour of c under\nperturbations of the drift b.\nWe \ufb01rst consider the situation where \u03ba is bounded from below by a negative constant for any\nr, and by a positive constant for large r:\nLemma 1 (Contractivity under lower bounds on \u03ba) Suppose that\n\u03ba(r) \u2265\u2212L for r \u2264R,\nand \u03ba(r) \u2265K for r > R\n(23)\nhold with constants R, L \u2208[0, \u221e) and K \u2208(0, \u221e). If LR2\n0 \u22648 then\n\u03b1\u22121c\u22121 \u2264e \u22121\n2\nR2 + e\n\u221a\n8K\u22121 R + 4K\u22121 \u22643e\n2 max(R2, 8K\u22121),\n(24)\nand if LR2\n0 \u22658 then\n\u03b1\u22121c\u22121 \u22648\n\u221a\n2\u03c0R\u22121L\u22121/2(L\u22121 + K\u22121) exp\n\u0012LR2\n8\n\u0013\n+ 32R\u22122K\u22122.\n(25)\nFor di\ufb00usions with re\ufb02ection on a smooth convex domain corresponding bounds with K = \u221e\nhold if R is the diameter of the domain, cf. Remark 3 above.\nRemark 5 If L = 0 then the bound in (24) improves to\n\u03b1\u22121c\u22121 \u22642 max(R2, 2K\u22121).\n(26)\nThe proofs of Lemma 1 and Remark 5 are given in Section 5 below.\nIn the \ufb01rst case considered in the lemma, the constant c is at least of order min(R\u22122, K). Even\nif L = 0 (convex case), this order can not be improved as one-dimensional Langevin di\ufb00usions\nwith potential U(x) = Kx2/2, or, respectively, with vanishing drift on (\u2212R/2,R/2) demonstrate.\nIn particular, for U(x) = Kx2/2 with K > 0, the distance Wf is equivalent to W 1, and the\nexact decay rate is K/2. This di\ufb00ers from the bounds in (26) and (24) only by a factor 2, 6e\nrespectively. Thus, if LR2\n0 is not too large, the contractivity properties are not a\ufb00ected substantially\nby non-convexity !\nIn the second case (LR2\n0 \u22658), if K \u2265const. \u00b7 L then the upper bound for c\u22121 is of order\nL\u22123/2R\u22121 exp(LR2/8). By the next example, this order in R and L is again optimal:\nExample 4 (Double-well potential with U \u2032\u2032(x) = \u2212L for |x| \u2264R/2) Consider a Langevin dif-\nfusion in R1 with a symmetric potential U \u2208C2(R) satisfying U(x) = \u2212Lx2/2 for x \u2208[\u2212R/2, R/2],\nU \u2032\u2032 \u2265\u2212L, and lim inf|x|\u2192\u221eU \u2032\u2032(x) > 0. If \u2225\u00b7 \u2225is the Euclidean norm then \u03ba(r) = \u2212L for r \u2208(0, R].\nOn the other hand, let \u03c40 = inf{t \u22650 : Xt = 0} denote the \ufb01rst hitting time of 0. Then for any\ninitial condition x0 > 0,\nlim\nt\u2192\u221et\u22121 log Px0[\u03c40 > t] = \u2212\u03bb1(0, \u221e)\n(27)\nwhere \u2212\u03bb1(0, \u221e) is the \ufb01rst Dirichlet eigenvalue of the generator Lv = (v\u2032\u2032 \u2212U \u2032v\u2032)/2 on (0, \u221e),\ncf. [23] or see Section 5 below for a short proof of the corresponding lower bound that is relevant here.\nIf LR2 \u22654 then by inserting the function g(x) = min(\n\u221a\nLx, 1) into the variational characterization\nof the Dirichlet eigenvalue, we obtain the upper bound\n\u03bb1(0, \u221e) \u22643\n4e1/2L3/2R exp(\u2212LR2/8),\n(28)\ncf. Section 5 below. The estimates (27) and (28) seem to indicate that for x0 > 0, the Kantorovich\ndistance W 1(\u03b4\u2212x0pt, \u03b4x0pt) decays at most with a rate of order L3/2R exp(\u2212LR2/8). Indeed, under\nappropriate growth assumptions on U(x) for |x| \u2265R, one can prove that\nPR [\u03c40 > t] \u22653/4\nfor any t \u2264\u03bb1(0, \u221e)\u22121/4,\nRe\ufb02ection couplings and contraction rates\n9\ncf. Section 5. Hence for t \u22643\u22121e\u22121/2L\u22123/2R\u22121 exp(LR2/8), the Kantorovich distance W 1(\u03b4Rpt, \u00b5)\nbetween \u03b4Rpt and the stationary distribution \u00b5 is bounded from below by a strictly positive constant\nthat does not depend on L and R if LR2 \u22654.\nFor analyzing the behaviour of c under perturbations of the drift, we assume that \u2225z\u2225= |\u03c3\u22121z|\nis the intrinsic metric corresponding to the di\ufb00usion matrix, i.e., G = (\u03c3\u03c3T )\u22121. Suppose that\nb(x) = b0(x) + \u03b3(x)\nfor any x \u2208R\n(29)\nwith locally Lipschitz continuous functions b0, \u03b3 : Rd \u2192Rd. For r > 0 let\n\u03ba0(r) = inf\n\u001a\n\u22122 (x \u2212y) \u00b7 G(b0(x) \u2212b0(y))\n\u2225x \u2212y\u22252\n: x, y \u2208Rd s.t. \u2225x \u2212y\u2225= r\n\u001b\n(30)\nbe de\ufb01ned analogously to \u03ba(r) with b replaced by b0. We assume that \u03ba0 satis\ufb01es the assumptions\n(7) imposed on \u03ba above, and we de\ufb01ne R0 and R1 similarly to (8) and (9) but with \u03ba replaced by\n\u03ba0. Now suppose that there exists a constant R \u2264R0 such that\n(x \u2212y) \u00b7 (\u03b3(x) \u2212\u03b3(y)) \u22640\nfor any x, y \u2208Rd s.t. \u2225x \u2212y\u2225\u2265R.\n(31)\nThen \u03ba(r) \u2265\u03ba0(r) for r \u2265R, and hence the constants R0 and R1 de\ufb01ned w.r.t. b are smaller than\nthe corresponding constants de\ufb01ned w.r.t. b0. In this situation, we can compare the lower bounds\nc and c0 for the contraction rates w.r.t. b and b0 given by (12):\nLemma 2 (Bounded and Lipschitz perturbations) Suppose that the drift b : Rd \u2192Rd is\ngiven by (29) with b0 and \u03b3 satisfying the assumptions stated above, and let c and c0 denote the\nlower bounds for the contraction rates w.r.t. b and b0 given by (12).\n1. If \u03b3 is bounded and (31) holds for a constant R \u2208[0, R0] then\nc \u2265c0 exp(\u2212R sup \u2225\u03b3\u2225).\n(32)\n2. If \u03b3 satis\ufb01es the one-sided Lipschitz condition\n(x \u2212y) \u00b7 G(\u03b3(x) \u2212\u03b3(y)) \u2264L \u00b7 \u2225x \u2212y\u22252\n\u2200x, y \u2208Rd\n(33)\nwith a \ufb01nite constant L \u2208[0, \u221e) and (31) holds for a constant R \u2208[0, R0] then\nc \u2265c0 exp(\u2212LR2/4).\n(34)\nRemark 6 The condition R \u2264R0 is required in Lemma 2. If (31) does not hold for x, y \u2208Rd with\n\u2225x \u2212y\u2225\u2265R0 then the constants R0(b) and R1(b) de\ufb01ned w.r.t. b are in general greater than the\ncorresponding constants de\ufb01ned w.r.t. b0, i.e., the region of non-convexity increases by adding the\ndrift \u03b3. This will also a\ufb00ect the bound in (12) signi\ufb01cantly.\nThe proof of Lemma 2 is given in Section 5.\n2.4 Local contractivity and a high-dimensional example\nConsider again the setup in Section 2.1. In some applications, the condition lim infr\u2192\u221e\u03ba(r) > 0\nimposed above is not satis\ufb01ed, but the di\ufb00usion process will stay inside a ball B \u2282Rd for a long\ntime with high probability. In this case, one can still prove exponential contractivity up to an error\nterm that is determined by the exit probabilities from the ball. Corresponding estimates are useful\nto prove non-asymptotic error bounds, i.e., for \ufb01xed t \u2208(0, \u221e), cf. e.g. [8,9,22].\nFix R \u2208(0, \u221e) and let WfR denote the Kantorovich distance based on the distance function\ndfR(x, y) = fR(\u2225x \u2212y\u2225) given by\nfR(r) =\n\u02c6 r\n0\n\u03d5(s)gR(s) ds\nfor r \u22650,\n(35)\n10\nAndreas Eberle\nwhere \u03d5 and \u03a6 are de\ufb01ned by (10), and\ngR(r) = 1 \u2212\n\u02c6 r\u2227R\n0\n\u03a6(s)\n\u03d5(s) ds\n\u001e\u02c6 R\n0\n\u03a6(s)\n\u03d5(s) ds .\n(36)\nNotice that\ngR(r) = 0\nand\nfR(r) = fR(R)\nfor any r \u2265R,\ni.e., we have cut the distance at fR(R).\nTheorem 6 (Local exponential contractivity) Suppose that the assumptions from Section 2.1\nare satis\ufb01ed except for the condition lim infr\u2192\u221e\u03ba(r) > 0. Then for any t, R \u22650 and any probability\nmeasures \u00b5, \u03bd on Rd,\nWfR(\u00b5pt, \u03bdpt) \u2264exp(\u2212cRt) WfR(\u00b5, \u03bd)\n+ R \u00b7\n\u0000P\u00b5[\u03c4R/2 \u2264t] + P\u03bd[\u03c4R/2 \u2264t]\n\u0001\n,\n(37)\nwhere (Xt, P\u00b5) is a di\ufb00usion process satisfying (1) with initial distribution \u00b5, \u03c4R/2 = inf{t \u22650 :\n\u2225Xt\u2225> R/2} denotes the \ufb01rst exit time from the ball of radius R/2 around 0, and\n1\ncR\n= \u03b1\nR\n\u02c6\n0\n\u03a6(s)\u03d5(s)\u22121 ds = \u03b1\nR\n\u02c6\n0\ns\n\u02c6\n0\nexp\n\uf8eb\n\uf8ed1\n4\ns\n\u02c6\nt\nu\u03ba(u)\u2212du\n\uf8f6\n\uf8f8dt ds.\n(38)\nThe proof of the theorem is given in Section 5. In applications, the exit probabilities are typically\nestimated by using appropriate Lyapunov functions.\nExample 5 (Stochastic heat equation) We consider the di\ufb00usion in Rd\u22121 given by X0\nt \u2261Xd\nt \u22610\nand\ndXi\nt =\nh\nd2 (Xi+1\nt\n\u22122Xi\nt + Xi\u22121\nt\n) + V \u2032(Xi\nt)\ni\ndt +\n\u221a\nd dBi\nt,\n(39)\ni = 1, . . . , d \u22121, where V : R \u2192R is a C2 function such that V \u2032\u2032 \u2265\u2212L for a \ufb01nite constant L \u2208R.\nThe equation (39) is a spatial discretization at the grid points i/d (i = 0, 1, . . . , d) of the stochastic\nheat equation with space-time white noise and Dirichlet boundary conditions on the interval [0, 1]\ngiven by\ndu =\n\u0000\u2206Diru + V \u2032(u)\n\u0001\ndt + dW\n(40)\nwith the Dirichlet Laplacian \u2206Dir on the interval [0, 1] and a cylindrical Wiener process (Wt)t\u22650\nover the Hilbert space L2(0, 1). We observe that (39) is of the form (1) with \u03c3 =\n\u221a\ndId\u22121 and\nb = \u2212d\u2207U where\nU(x) = d\n2\nd\nX\ni=1\n\f\f\fxi \u2212xi\u22121\f\f\f\n2\n+ 1\nd\nd\nX\ni=0\nV (xi)\nfor x = (x1, . . . , xd\u22121) \u2208Rd\u22121 and x0 = xd = 0. By the discrete Poincar\u00b4e inequality,\nd\nX\ni=1\n\f\f\fxi \u2212xi\u22121\f\f\f\n2\n\u22652 (1 \u2212cos(\u03c0/d))\nd\u22121\nX\ni=1\n\f\f\fxi\f\f\f\n2\n.\nHence for any x, \u03be \u2208Rd\u22121 and x0 = xd = \u03be0 = \u03bed = 0, the lower bound\n\u22022\n\u03be\u03beU(x) = d\nd\nX\ni=1\n\f\f\f\u03bei \u2212\u03bei\u22121\f\f\f\n2\n+ 1\nd\nd\u22121\nX\ni=1\nV \u2032\u2032(xi)\n\f\f\f\u03bei\f\f\f\n2\n\u22651\ndKd\nd\u22121\nX\ni=1\n\f\f\f\u03bei\f\f\f\n2\nholds with Kd = 2 d2 (1 \u2212cos(\u03c0/d)) \u2212L, and thus\n(x \u2212y) \u00b7 (b(x) \u2212b(y)) = \u2212d (x \u2212y) \u00b7 (\u2207U(x) \u2212\u2207U(y)) \u2264\u2212Kd |x \u2212y|2\nRe\ufb02ection couplings and contraction rates\n11\nfor any x, y \u2208Rd\u22121 where | \u00b7 | denotes the Euclidean norm. Choosing for \u2225\u00b7 \u2225the intrinsic metric\n\u2225x\u2225= d\u22121/2|x|, we obtain\n\u03ba(r) \u22652 Kd\nfor any r > 0.\nIn particular, the function \u03ba is bounded from below uniformly by a real constant that does not\ndepend on the dimension d since\nlim\nd\u2192\u221eKd = \u03c02 \u2212L > \u2212\u221e.\n(41)\nTheorem 6 now shows that for any R > 0, local exponential contractivity in the sense of (37) holds\non the ball\nBR/2 = {x \u2208Rd\u22121 : \u2225x\u2225\u2264R/2} = {x \u2208Rd\u22121 : |x| \u2264d1/2R/2}\nwith rate cR satisfying\n1\ncR\n\u22644\u221a\u03c0R\u22121|Kd|\u22123/2 exp(\u2212KdR2/4)\nfor KdR2 \u2264\u22124,\n1\ncR\n\u2264(e \u22121)R2/2\nfor \u22124 \u2264KdR2 < 0,\n1\ncR\n\u2264R2/2\nfor Kd = 0 respectively.\nHere the explicit upper bounds are obtained analogously as in the proof of Lemma 1. For Kd > 0,\nstrict convexity holds, and we obtain global exponential contractivity with a dimension-independent\nrate. We remark that because of (41), the bounds also carry over to the limiting SPDE (40) for\nwhich they imply local exponential contractivity on balls w.r.t. the L2 norm.\n3 Main results for componentwise re\ufb02ection couplings\n3.1 Componentwise re\ufb02ection couplings and contractivity on product spaces\nWe now consider a system\ndXi\nt = bi(Xt) dt + dBi\nt,\ni = 1, . . . , n,\n(42)\nof n interacting di\ufb00usion processes taking values in Rdi, di \u2208N. Here Bi, i = 1, . . . , n, are indepen-\ndent Brownian motions in Rdi, X = (X1, . . . , Xn) is a di\ufb00usion process taking values in Rd where\nd = Pn\ni=1 di, and bi : Rd \u2192Rdi are locally Lipschitz continuous functions. We will assume that\nbi(x) = bi\n0(xi) + \u03b3i(x),\ni = 1, . . . , n,\n(43)\nwhere the functions bi\n0 : Rdi \u2192Rdi are locally Lipschitz continuous, and \u03b3i : Rd \u2192Rdi are\n\u201csu\ufb03ciently small\u201d perturbations, cf. Theorem 7 below. In particular, for \u03b3i \u22610 the components\nX1, . . . , Xn are independent.\nTo analyse contraction properties of the process X, one could use a re\ufb02ection coupling on Rd\nand apply the results above based on a distance function of the form df(x, y) = f(|x \u2212y|). In\nsome applications, this approach does indeed provide dimension-free bounds, cf. Example 5 above.\nHowever, in the product case \u03b3i \u22610 it leads in general to lower bounds for contraction rates that\ndegenerate rapidly as n \u2192\u221e, even though one would expect exponential contractivity with the\nminimum of the contraction rates for the components. The reason is that the approach requires\nconvexity outside a Euclidean ball in Rd whereas in corresponding product models, in general\nconvexity only holds if all components are outside given balls in Rdi.\nInstead, we now consider contractivity w.r.t. Kantorovich distances Wf,w based on distance\nfunctions on Rd = Rd1+\u00b7\u00b7\u00b7+dn of the form\ndf,w(x, y) =\nn\nX\ni=1\nfi(|xi \u2212yi|) wi .\n(44)\n12\nAndreas Eberle\nHere fi : [0, \u221e) \u2192[0, \u221e), 1 \u2264i \u2264n, are strictly increasing concave C1 functions with fi(0) = 0\nand f \u2032\ni(0) = 1 that are obtained from bi\n0 in the same way as f has been obtained from b above,\nand wi \u2208(0, 1] are positive weights. In many applications, one can choose wi = 1 for any i. The\ncorresponding distance will then be denoted by d1,f. Notice that d1,f is bounded from above by\nthe \u21131 distance\nd\u21131(x, y) =\nn\nX\ni=1\n|xi \u2212yi|.\nHence W1,f is bounded from above by the Kantorovich distance W\u21131 based on d\u21131.\nFor r \u2208(0, \u221e) let\n\u03bai(r) = r\u22122 inf\nn\n\u22122 (x \u2212y) \u00b7 (bi\n0(x) \u2212bi\n0(y)) : x, y \u2208Rd s.t. |x \u2212y| = r\no\n.\n(45)\nSimilarly as above, we assume that for 1 \u2264i \u2264n,\n\u03bai : (0, \u221e) \u2192R is continuous with lim inf\nr\u2192\u221e\u03bai(r) > 0.\n(46)\nMoreover, we assume\nlim\nr\u21920 r\u03bai(r) = 0.\n(47)\nLet Ri\n0, Ri\n1, gi(r), \u03d5i(r), fi(r) and \u03a6i(r) =\n\u00b4 r\n0 \u03d5i(s) ds be de\ufb01ned analogously to (8), (9) and (10)\nwith \u03ba replaced by \u03bai. Moreover, we de\ufb01ne ci \u2208(0, \u221e) by\n1\nci\n=\nRi\n1\n\u02c6\n0\n\u03a6i(s)\u03d5i(s)\u22121 ds =\nRi\n1\n\u02c6\n0\ns\n\u02c6\n0\nexp\n\uf8eb\n\uf8ed1\n4\ns\n\u02c6\nt\nu\u03bai(u)\u2212du\n\uf8f6\n\uf8f8dt ds .\n(48)\nRecall that by Theorem 1 and Corollary 2, ci is a lower bound for the contraction rate of the\ndi\ufb00usion process e\nXi on Rdi satisfying the s.d.e. d e\nXi\nt = bi\n0( e\nXi\nt) dt + dBi\nt.\nLet pt(x, dy) denote the transition kernels of the di\ufb00usion process Xt = (X1\nt , . . . , Xd\nt ) on Rd\nsatisfying (42). We now state our second main result:\nTheorem 7 (Exponential contractivity on product spaces) Suppose that (46) and (47) hold,\nand suppose that there exist constants \u03b5i \u2208[0, ci), 1 \u2264i \u2264n, such that for any x, y \u2208Rd,\nn\nX\ni=1\n|\u03b3i(x) \u2212\u03b3i(y)| wi \u2264\nn\nX\ni=1\n\u03b5i fi(|xi \u2212yi|) wi.\n(49)\nThen for any t \u22650 and any probability measures \u00b5, \u03bd on Rd,\nWf,w(\u00b5pt, \u03bdpt) \u2264exp(\u2212ct) Wf,w(\u00b5, \u03bd),\nand\n(50)\nW\u21131(\u00b5pt, \u03bdpt) \u2264A exp(\u2212ct) W\u21131(\u00b5, \u03bd),\n(51)\nwhere c =\nmin\ni=1,...,n(ci \u2212\u03b5i)\nand\nA = 2\n\u001e\nmin\ni=1,...,n(\u03d5i(Ri\n0)wi) .\nExample 6 (Product model) In the product case, \u03b3i \u22610 for any i. Hence Condition (49) is\nsatis\ufb01ed with \u03b5i = 0, and, therefore,\nWf,w(\u00b5pt, \u03bdpt) \u2264exp(\u2212ct) Wf,w(\u00b5, \u03bd)\nholds with c = min ci for any choice of the weights w1, . . . , wn.\nMore generally than in the example, suppose now that \u03b3 = (\u03b31, . . . , \u03b3n) satis\ufb01es an \u21131-Lipschitz\ncondition\nn\nX\ni=1\n|\u03b3i(x) \u2212\u03b3i(y)| \u2264\u03bb\nn\nX\ni=1\n|xi \u2212yi|\n\u2200x, y \u2208Rd.\n(52)\nThen exponential contractivity holds for the perturbed product model provided \u03bb < ci\u03d5(Ri\n0)/2 for\nany i:\nRe\ufb02ection couplings and contraction rates\n13\nCorollary 8 (Perturbations of product models) Suppose that (43), (46), (47) and (52) hold\nwith \u03bb \u2208[0, \u221e). Then for any t \u22650 and any probability measures \u00b5, \u03bd on Rd,\nWf,1(\u00b5pt, \u03bdpt) \u2264exp(\u2212ct) Wf,1(\u00b5, \u03bd),\nand\n(53)\nW\u21131(\u00b5pt, \u03bdpt) \u2264A exp(\u2212ct) W\u21131(\u00b5, \u03bd),\n(54)\nwhere c =\nmin\ni=1,...n(ci \u22122\u03bb\u03d5i(Ri\n0)\u22121) and A = 2 max\ni=1,...n \u03d5i(Ri\n0)\u22121.\nThe inituitive idea of proof for Theorem 7 is to construct a coupling (Xt, Yt) of two solutions of\n(42) by applying a re\ufb02ection coupling individually for each component (Xi\nt, Y i\nt ) if Xi\nt \u0338= Y i\nt , and a\nsynchronuous coupling if Xi\nt = Y i\nt . In the product case this just means that Xi\nt = Y i\nt for any t \u2265\u03c4 i\nwhere \u03c4 i = inf{t \u22650 : Xi\nt = Y i\nt } is the coupling time for the i-th component. In the non-product\ncase, however, Xi\nt and Y i\nt can move apart again after the time \u03c4 i due to interactions with other\ncomponents. In that case it is not clear how to de\ufb01ne a coupling as described above rigorously.\nInstead we will use a regularized version where re\ufb02ection coupling is applied to the i-th component\nwhenever |Xi\nt \u2212Y i\nt | \u2265\u03b4 for a given constant \u03b4 > 0, and synchronuous coupling is applied whenever\n|Xi\nt \u2212Y i\nt | \u2264\u03b4/2. A precise description of the coupling and the proofs of Theorem 7 and Corollary\n8 are given in Sections 6 and 7 below.\n3.2 Consequences\nThe contractivity results in Theorem 7 and Corollary 8 have corresponding consequences as the\ncontractivity results in the non-product case, cf. Section 2.2 above. An important di\ufb00erence to be\nnoted is, however, that on product spaces,\ndf,w(x, y) \u2264\nn\nX\ni=1\n|xi \u2212yi| \u2264n1/2 |x \u2212y|\nby the Cauchy-Schwarz inequality. Therefore, an additional factor n occurs in the variance bounds\nfrom Corollaries 3, 4 and 5 on product spaces. Apart from this additional factor, all results in\nSection 2.2 carry over to the setup considered in Section 3.1.\n3.3 Interacting Langevin di\ufb00usions\nAs an illustration of the results in Section 3.1, we consider a system\ndXi\nt = \u22121\n2\u2207U(Xi\nt) dt \u2212\nn\nX\nj=1\naij \u2207V (Xi\nt \u2212Xj\nt ) dt + dBi\nt\n(55)\nof n interacting overdamped Langevin di\ufb00usions taking values in Rk for some k \u2208N. Here B1, . . . , Bn\nare independent Brownian motions in Rk, U \u2208C2(Rk) is strictly convex outside a given ball, the\ninteraction potential V is in C2(Rk) with bounded second derivatives, and aij, 1 \u2264i, j \u2264n, are \ufb01-\nnite real constants. For example, we are interested in nearest-neighbour interactions and mean-\ufb01eld\ninteractions given by\naij =\n\u001a \u03b1/2\nif i \u2212j \u22611 mod n or i \u2212j \u2261\u22121 mod n,\n0\notherwise,\n(56)\naij = \u03b1 n\u22121\nrespectively,\n(57)\nwhere \u03b1 \u2208R is a \ufb01nite coupling constant.\nChoosing bi\n0(xi) = \u2212\u2207U(xi)/2 and \u03b3i(x) = \u2212Pn\nj=1 aij\u2207V (xi \u2212xj), we observe that the\nfunction\n\u03bai(r) = inf\n\u001a\u02c6 1\n0\n\u22022\n(x\u2212y)/|x\u2212y|U((1 \u2212t)x + ty) dt : x, y \u2208Rk s.t. |x \u2212y| = r\n\u001b\n14\nAndreas Eberle\ndoes not depend on i. Let \u03d5 and f be the corresponding functions given by (10), and consider the\ndistance\nd1,f(x, y) =\nn\nX\ni=1\nf(|xi \u2212yi|).\nMorover, let c be given by (12) with \u03b1 = 1, i.e., c is the lower bound for the contraction rate of\nthe di\ufb00usion process Y in Rk satisfying dY = \u22121\n2\u2207U(Y ) dt + dB. We note that \u03b3 satis\ufb01es the \u21131\nLipschitz condition (52) with\n\u03bb = M \u00b7 max\ni\nn\nX\nj=1\n(|aij| + |aji|)\nwhere M = sup \u2225\u22072V \u2225. Therefore, if\nn\nX\nj=1\n(|aij| + |aji|) \u2264c \u03d5(R0) M \u22121\nthen by Corollary 8, contractivity in the sense of (53) holds with contraction rate\n\u00afc = c \u22122\u03bb\u03d5(R0)\u22121 > 0.\nIn particular, in the nearest neighbour and mean \ufb01eld case, we obtain contractivity with a rate\nthat does not depend on the dimension if \u03b1 is small:\nCorollary 9 (Mean \ufb01eld and nearest neighbour interactions) Let pt, t \u22650, denote the tran-\nsition kernels of the di\ufb00usion process on Rnk solving (55). Suppose that sup \u2225\u22072V \u2225< \u221eand that\naij is given by (56) or by (57) with \u03b1 \u2208R. Then there exist \ufb01nite constants c, \u03b8, A \u2208(0, \u221e) that do\nnot depend on the dimension n such that\nWf,1(\u00b5pt, \u03bdpt) \u2264e(\u03b8\u03b1\u2212c)t Wf,1(\u00b5, \u03bd),\nand\n(58)\nW\u21131(\u00b5pt, \u03bdpt) \u2264A e(\u03b8\u03b1\u2212c)t W\u21131(\u00b5, \u03bd),\n(59)\nhold for any t \u22650 and any probability measures \u00b5, \u03bd on Rnk. In particular, exponential contractivity\nholds for \u03b1 < c/\u03b8.\nThe bounds in (58) and (59) are not sharp. However, it is known that for example in mean\n\ufb01eld models where U is a double-well potential and V is quadratic, exponential contractivity with\na rate independent of the dimension can not be expected to hold for large \u03b1. Indeed, in this case\nthe corresponding McKean-Vlasov process has several stationary distributions if \u03b1 > \u03b11 for some\ncritical parameter \u03b11 \u2208(0, \u221e), cf. [26,27].\n4 Proofs for Re\ufb02ection Coupling\nIn this section, we \ufb01rst motivate our particular choice of the function f, and we prove Theorem 1.\nAfterwards, we prove Corollaries 2, 3, 4 and 5.\nLet rt = \u2225Xt \u2212Yt\u2225where (X, Y ) is a re\ufb02ection coupling of two solutions of (1). Our goal is to\n\ufb01nd an explicit concave increasing function f : [0, \u221e) \u2192[0, \u221e) with f(0) = 0 and f \u2032(0) = 1 such\nthat ectf(rt) is a (local) supermartingale for t less than the coupling time T with a constant c > 0\nthat we are trying to maximize by the choice of f.\nAn application of It\u02c6o\u2019s formula to the s.d.e. (4) satis\ufb01ed by the di\ufb00erence process Zt = Xt \u2212Yt\nshows that the following It\u02c6o equations hold almost surely for t < T whenever f is C1 and f \u2032 is\nabsolutely continuous:\nd\u2225Zt\u22252 = 4 |\u03c3\u22121Zt|\u22121\u2225Zt\u22252 dWt\n+ 2 Zt \u00b7 G(b(Xt) \u2212b(Yt)) dt + 4 |\u03c3\u22121Zt|\u22122\u2225Zt\u22252 dt,\ndrt = 2 |\u03c3\u22121Zt|\u22121rt dWt + r\u22121\nt\nZt \u00b7 G(b(Xt) \u2212b(Yt)) dt,\nand\ndf(rt) = 2 |\u03c3\u22121Zt|\u22121rt f \u2032(rt) dWt\n+ r\u22121\nt\nZt \u00b7 G(b(Xt) \u2212b(Yt))f \u2032(rt) dt + 2 |\u03c3\u22121Zt|\u22122r2\nt f \u2032\u2032(rt) dt.\n(60)\nRe\ufb02ection couplings and contraction rates\n15\nBy de\ufb01nition of the function \u03ba, the drift term on the right hand side of (60) is bounded from above\nby\n\u03b2t := 2 |\u03c3\u22121Zt|\u22122r2\nt \u00b7\n\u0012\nf \u2032\u2032(rt) \u22121\n4 rt \u03ba(rt)f \u2032(rt)\n\u0013\n.\n(61)\nHence the process ectf(rt) is a supermartingale for t < T if \u03b2t \u2264\u2212cf(rt). Since\n|\u03c3\u22121z|2 \u2264\u03b1\u2225z\u22252\nfor any z \u2208Rd\n(62)\nwith \u03b1 de\ufb01ned as in Theorem 1, a su\ufb03cient condition is\nf \u2032\u2032(r) \u22121\n4r\u03ba(r)f \u2032(r) \u2264\u2212\u03b1c\n2 f(r)\nfor a.e. r > 0.\n(63)\nWe now \ufb01rst observe that this equation holds with c = 0 (i.e., f(rt) is a supermartingale for t < T)\nif f is chosen such that f \u2032(r) = \u03d5(r) = exp(\u2212\u00b4 r\n0 s\u03ba(s)\u2212ds/4). Indeed, f(r) = \u00b4 r\n0 \u03d5(s) ds is the least\nconcave among all concave functions f satisfying \u03b2t \u22640.\nTo satisfy the stronger condition \u03b2t \u2264\u2212cf(rt) with c > 0, we make the ansatz\nf \u2032(r) = \u03d5(r) g(r)\n(64)\nwith a decreasing absolutely continuous function g \u22651/2 such that g(0) = 1. Note that the\ncondition g \u22650 is required to ensure that f is non-decreasing. By replacing this condition by the\nstronger condition g \u22651/2, we are loosing at most a factor 2 in the estimates below. On the other\nhand, the condition 1/2 \u2264g \u22641 has the huge advantage of ensuring that\n\u03a6/2 \u2264f \u2264\u03a6\n(65)\nwhere \u03a6(r) = \u00b4 r\n0 \u03d5(s) ds. The ansatz (64) yields\nf \u2032\u2032 = \u22121\n4 r\u03ba\u2212f + \u03d5g\u2032 \u22641\n4 r\u03baf + \u03d5g\u2032,\ni.e., Condition (63) is satis\ufb01ed if\ng\u2032 \u2264\u2212\u03b1c\n2 f/\u03d5 .\nalmost surely.\n(66)\nWe will see in the proof below that for r \u2265R1, Condition (63) is automatically satis\ufb01ed since \u03ba is\nsu\ufb03ciently positive. Therefore, it is enough to assume that (66) holds on (0, R1).\nNow on the one hand, if (66) is satis\ufb01ed on (0, R1) then\ng(R1) \u22641 \u2212\u03b1c\n2\n\u02c6 R1\n0\nf(s)\u03d5(s)\u22121 ds \u22641 \u2212\u03b1c\n4\n\u02c6 R1\n0\n\u03a6(s)\u03d5(s)\u22121 ds.\nThis condition can only be satis\ufb01ed with a function g taking values in [1/2,1] if\n\u03b1 c \u22642\n\u001e\u02c6 R1\n0\n\u03a6(s)\u03d5(s)\u22121 ds .\nOn the other hand, by choosing\ng\u2032(r) = \u2212\u03a6(r)\n2\u03d5(r)\n\u001e\u02c6 R1\n0\n\u03a6(s)\n\u03d5(s) ds\nfor r < R1,\n(67)\nCondition (66) is satis\ufb01ed with the constant\n\u03b1 c = 1\n\u001e\u02c6 R1\n0\n\u03a6(s)\u03d5(s)\u22121 ds .\nThis shows that up to a factor 2, choosing g as in (67) is the best we can do under the assumptions\nthat we have made.\n16\nAndreas Eberle\nThe considerations above explain the particular choice of the function f made in (10). Once\nthis choice has been made, the proof of Theorem 1 is almost straightforward:\nProof of Theorem 1. As remarked above, the drift in the s.d.e. (60) for f(rt) is bounded\nfrom above by \u03b2t de\ufb01ned by (61). We now show that by our choice of f in (10), this expression is\nsmaller than \u2212cf(rt) where c is given by (12). Indeed, for r < R1,\nf \u2032\u2032(r) = \u22121\n4r\u03ba(r)\u2212\u03d5(r)g(r) \u22121\n2\u03a6(r)\n, R1\n\u02c6\n0\n\u03a6(s)\u03d5(s)\u22121 ds\n(68)\n\u22641\n4r\u03ba(r)f \u2032(r) \u22121\n2f(r)\n, R1\n\u02c6\n0\n\u03a6(s)\u03d5(s)\u22121 ds .\nFor r > R1, we have f \u2032(r) = \u03d5(r)/2 = \u03d5(R0)/2 and \u03ba(r)R1(R1 \u2212R0) \u22658 by de\ufb01nition of R1,\nwhence\nf \u2032\u2032(r) \u22121\n4r\u03ba(r)f \u2032(r) = \u22121\n8r\u03ba(r)\u03d5(R0) \u2264\u2212\u03d5(R0)\nR1 \u2212R0 \u00b7 r\nR1\n\u2264\u2212\u03d5(R0)\nR1 \u2212R0 \u00b7 \u03a6(r)\n\u03a6(R1) \u2264\u22121\n2\u03a6(r)\n\u001e\u02c6 R1\nR0\n\u03a6(s)\u03d5(s)\u22121 ds\n(69)\n\u2264\u22121\n2f(r)\n\u001e\u02c6 R1\n0\n\u03a6(s)\u03d5(s)\u22121 ds .\nHere we have used that for r \u2265R0, the function \u03d5(r) is constant, and, therefore, \u03a6(r) = \u03a6(R0) +\n(r \u2212R0) \u03d5(R0), and\n\u02c6 R1\nR0\n\u03a6(s)\u03d5(s)\u22121 ds =\n\u02c6 R1\nR0\n(\u03a6(R0) + (s \u2212R0)\u03d5(R0))\u03d5(R0)\u22121 ds\n= \u03a6(R0)\u03d5(R0)\u22121(R1 \u2212R0) + (R1 \u2212R0)2/2\n\u2265(R1 \u2212R0) (\u03a6(R0) + (R1 \u2212R0)\u03d5(R0)) \u03d5(R0)\u22121/2\n= (R1 \u2212R0)\u03a6(R1)\u03d5(R0)\u22121/2.\nBy (68) and (69), we conclude that \u03b2t \u2264\u2212cf(rt). Optional stopping in (60) at Tk = inf{t \u22650 :\nrt \u0338\u2208(k\u22121, k)} now implies\nE[f(rt) ; t < Tk] \u2264\u2212c\n\u02c6 t\n0\nE[f(rs) ; s < Tk] ds\nfor any k \u2208N and t \u22650. The assertion follows for k \u2192\u221esince rt = 0 for t \u2265T, and T = sup Tk\nby non-explosiveness. \u25a1\nProof of Corollary 2. Let (X, Y ) be a re\ufb02ection coupling of two solutions of (1) with joint\ninitial distribution (X0, Y0) \u223c\u03b7. Then by Theorem 1,\nWf(\u00b5pt, \u03bdpt) \u2264E [df(Xt, Yt)] \u2264e\u2212ct E [df(X0, Y0)]\n= e\u2212ct\n\u02c6\ndf(x, y) \u03b7(dx dy)\nfor any t \u22650. The estimate (13) now follows by taking the in\ufb01mum over all couplings \u03b7 of two\ngiven probability measures \u00b5 and \u03bd on Rd. Moreover, (14) follows from (13) by (15). \u25a1\nNext, we are going to prove the results in Section 2.2. Suppose that (18) holds, \u2225z\u2225= |\u03c3\u22121z| is\nthe intrinsic metric, and b is in C1. Corollary 2 implies\n\u02c6\n|y| pt(x, dy) \u2264\n\u02c6\n|y| pt(x0, dy) + W 1(pt(x, \u00b7), pt(x0, \u00b7)) < \u221e\nRe\ufb02ection couplings and contraction rates\n17\nfor any t \u22650 and any x \u2208Rd. In particular, (ptg)(x) =\n\u00b4\ng(y) pt(x, dy) is de\ufb01ned for any Lipschitz\ncontinuous function g : Rd \u2192R, and\n|(ptg)(x) \u2212(ptg)(y)| = |E[g(Xt) \u2212g(Yt)]| \u2264\u2225g\u2225Lip(f)E[df(Xt, Yt)]\nfor any coupling (Xt, Yt) of pt(x, \u00b7) and pt(y, \u00b7). Hence by Theorem 1,\n|(ptg)(x) \u2212(ptg)(y)| \u2264e\u2212ct \u2225g\u2225Lip(f) df(x, y),\n(70)\ni.e., pt satis\ufb01es the exponential contractivity condition (19) w.r.t. \u2225\u00b7 \u2225Lip(f). If ptg is C1 then by\n(70) and since\ndf(x, y) \u2264\u2225x \u2212y\u2225= |\u03c3\u22121(x \u2212y)|\n\u2200x, y \u2208Rd,\nwe obtain the uniform gradient bound\nsup\n\f\f\f\u03c3T \u2207ptg\n\f\f\f \u2264e\u2212ct \u2225g\u2225Lip(f)\n\u2200t \u22650.\n(71)\nIt is well-known that this bound can be used to control variances w.r.t. the measures pt(x, \u00b7):\nLemma 3 For any t \u22650, x \u2208Rd, and any Lipschitz continuous g : Rd \u2192R,\nVarpt(x,\u00b7)(g) \u22641 \u2212exp(\u22122ct)\n2c\n\u2225g\u22252\nLip(f).\n(72)\nProof. We may assume g \u2208C2(Rd) and t > 0. Then, by standard elliptic regularity results,\n(t, x) 7\u2192(ptg)(x) is di\ufb00erentiable in t and x, and\nd\ndt ptg = Lptg = ptLg\nwhere L = 1\n2\nP aij\n\u22022\n\u2202xi\u2202xj +b(x)\u00b7\u2207, a = \u03c3\u03c3T , is the generator of (Xt), cf. e.g. [38,39]. In particular,\nfor s \u2208(0, t),\nd\nds ps(pt\u2212sg)2 = ps\n\u0010\nL(pt\u2212sg)2 \u22122pt\u2212sg Lpt\u2212sg\n\u0011\n= ps\n\f\f\f\u03c3T \u2207pt\u2212sg\n\f\f\f\n2\n\u2264e\u22122c(t\u2212s)\u2225g\u22252\nLip(f)\nby (71). Integrating w.r.t. s, we obtain\nptg2 \u2212(ptg)2 \u22641 \u2212exp(\u22122ct)\n2c\n\u2225g\u22252\nLip(f),\nwhich is equivalent to (72). \u25a1\nBy Lemma 3 and (70), we can now easily prove Corollaries 3, 4 and 5:\nProof of Corollary 3.\nExistence and uniqueness of a stationary distribution \u00b5 for (pt)t\u22650\nsatisfying\n\u00b4\n|y| \u00b5(dy) < \u221efollows easily as in [32], Section 3: By Corollary 2, the map \u03bd 7\u2192\u03bdp1\nis a contraction w.r.t. the distance Wf (equivalent to W 1) on the complete metric space P1 of all\nprobability measures \u03bd on (Rd, B(Rd)) satisfying\n\u00b4\n|y| \u00b5(dy) < \u221e. Hence by the Banach \ufb01xed point\ntheorem, there exists a unique probability measure \u00b50 such that \u00b50p1 = \u00b50. It is then elementary\nto verify that the measure \u00b5 =\n\u00b4 1\n0 \u00b50ps ds satis\ufb01es \u00b5pt = \u00b5 for any t \u2208[0, 1], and hence for any\nt \u2208[0, \u221e). Moreover, by Corollary 2,\nWf(\u00b5, \u03bdpt) = Wf(\u00b5pt, \u03bdpt) \u2264e\u2212ct Wf(\u00b5, \u03bd)\nfor any \u03bd \u2208P1. In particular, as t \u2192\u221e, pt(x, \u00b7) \u2192\u00b5 in P1 for any x \u2208Rd. The variance bound\nfor \u00b5 now follows from the corresponding bound for pt(x, \u00b7) in Lemma 3. \u25a1\n18\nAndreas Eberle\nProof of Corollary 4. By Lemma 3,\nCov (g(Xt), h(Xt+s)) = E [g(Xt) h(Xt+s)] \u2212E [g(Xt)] E [h(Xt+s)]\n= E [(g psh)(Xt)] \u2212E [g(Xt)] E [(psh)(Xt)] = Covpt(x0,\u00b7)(g, psh)\n\u2264(1 \u2212exp(\u22122ct)) (2c)\u22121 \u2225g\u2225Lip(f)\u2225psh\u2225Lip(f)\nfor any s, t \u22650. The assertion now follows by (70). \u25a1\nProof of Corollary 5. The bound for the bias follows immediately from (70), since\n\f\f\f\fE\n\u00141\nt\n\u02c6 t\n0\ng(Xs) ds \u2212\n\u02c6\ng d\u00b5\n\u0015\f\f\f\f =\n\f\f\f\f\n1\nt\n\u02c6 t\n0\n\u02c6\n(psg(x0) \u2212psg(y))\u00b5(dy) ds\n\f\f\f\f\n\u22641\nt\n\u02c6 t\n0\ne\u2212cs ds \u2225g\u2225Lip(f)\n\u02c6\ndf(x0, y) \u00b5(dy).\nMoreover, by Corollary 4,\nVar\n\u00121\nt\n\u02c6 t\n0\ng(Xs) ds\n\u0013\n= Cov\n\u00121\nt\n\u02c6 t\n0\ng(Xs) ds , 1\nt\n\u02c6 t\n0\ng(Xs) ds\n\u0013\n= 2\nt2\n\u02c6 t\n0\n\u02c6 t\ns\nCov (g(Xs), g(Xu)) du ds\n\u2264\n1\nct2\n\u02c6 t\n0\n(1 \u2212e\u22122cs)\n\u02c6 t\ns\ne\u2212c(u\u2212s) du ds \u2225g\u22252\nLip(f)\n\u2264\n1\nc2t \u2225g\u22252\nLip(f).\n\u25a1\n5 Examples\nWe now prove the results in Sections 2.3 and 2.4, including in particular Lemma 1, Lemma 2 and\nTheorem 6.\nProof of Lemma 1 and Remark 5. We \ufb01rst prove the lower bounds on the exponential decay\nrate c in (12) stated in (26), (24) and (25). Notice that the constant c de\ufb01ned by (12) increases if\n\u03ba(r) is replaced by a greater function. Indeed, for r \u22650,\n\u03a6(r)\u03d5(r)\u22121 =\nr\n\u02c6\n0\n\u03d5(t)\u03d5(r)\u22121 dt =\nr\n\u02c6\n0\nexp\n\uf8eb\n\uf8ed1\n4\nr\n\u02c6\nt\ns\u03ba(s)\u2212ds\n\uf8f6\n\uf8f8dt,\n(73)\nwhence R0, R1 and c\u22121 = \u03b1\nR1\n\u00b4\n0\n\u03a6(s)\u03d5(s)\u22121 ds are decreasing functions of \u03ba.\nConvex Case. Suppose \ufb01rst that \u03ba(r) \u22650 for any r \u22650 and \u03ba(r) \u2265K for r \u2265R with constants\nK \u2208(0, \u221e) and R \u2208[0, \u221e). Then R0 = 0, R1 \u2264max(R,\np\n8/K), \u03d5 \u22611, and hence\nc = (\u03b1R2\n1/2)\u22121 \u2265\u03b1\u22121 min(R\u22122/2, K/4).\nLocally non-convex case. Now suppose that \u03ba(r) \u2265\u2212L for r \u2264R and \u03ba(r) \u2265K for r > R with\nconstants K, L \u2208(0, \u221e) and R \u2208[0, \u221e]. Since \u03d5(r) = \u03d5(R0) and \u03a6(r) = \u03a6(R0) + (r \u2212R0)\u03d5(R0)\nfor r \u2265R0, we have\n\u03b1\u22121c\u22121 =\nR1\n\u02c6\n0\n\u03a6(s)\u03d5(s)\u22121 ds\n=\nR0\n\u02c6\n0\n\u03a6(s)\u03d5(s)\u22121 ds + (R1 \u2212R0)\u03a6(R0)\u03d5(R0)\u22121 + (R1 \u2212R0)2/2.\n(74)\nRe\ufb02ection couplings and contraction rates\n19\nThe lower curvature bounds imply the upper bounds\nR0 \u2264R,\nR1 \u2212R0 \u2264min(8/(KR0),\np\n8/K),\nand\n(75)\n\u03a6(r)\u03d5(r)\u22121 \u2264\nr\n\u02c6\n0\nexp(L(r2 \u2212t2)/8) dt\n\u2264min(\np\n2\u03c0/L, r) exp(Lr2/8)\nfor r \u2264R0.\n(76)\nSince exp x \u22641 + (e \u22121)x for x \u2208[0, 1] and\n\u02c6 x\n0\nexp(u2) du \u2264e +\n\u02c6 x\n1\n(2 \u2212u\u22122) exp(u2) du = x\u22121 exp(x2)\nfor x \u22651,\nwe can conclude that\nR0\n\u02c6\n0\n\u03a6(r)\u03d5(r)\u22121 dr \u2264\n\u02c6 R0\n0\nr exp(Lr2/8) dr = 4L\u22121(exp(LR2\n0/8) \u22121)\n\u2264(e \u22121)R2\n0/2\nif LR2\n0/8 \u22641,\nand\nR0\n\u02c6\n0\n\u03a6(r)\u03d5(r)\u22121 dr \u2264\nr\n2\u03c0\nL\n\u02c6 R0\n0\nexp(Lr2\n8 ) dr =\nr\n8 \u00b7 2\u03c0\nL2\n\u02c6 \u221a\nLR2\n0/8\n0\nexp(u2) du\n\u22648\n\u221a\n2\u03c0L\u22123/2R\u22121\n0\nexp(LR2\n0/8)\nif LR2\n0/8 \u22651.\nCombining these estimates, we obtain by (74), (75) and (76),\n\u03b1\u22121c\u22121 \u2264(e \u22121)R2/2 + e\np\n8/KR + 4/K\nif LR2\n0/8 \u22641,\nand\n\u03b1\u22121c\u22121 \u22648\n\u221a\n2\u03c0R\u22121L\u22121/2(L\u22121 + K\u22121) exp(LR2/8) + 32R\u22122K\u22122 if LR2\n0/8 \u22651,\nwhere we have used that the function x 7\u2192x\u22121 exp(x2) is increasing for x \u22651. \u25a1\nProofs for Example 4. Consider the one-dimensional Langevin di\ufb00usion (Xt) with drift\n\u2212\u2207U(x)/2 and generator\nLv = 1\n2(v\u2032\u2032 \u2212U \u2032v\u2032) = 1\n2 eU \u0010\ne\u2212Uv\u2032\u0011\u2032\n.\n(77)\nThe assumption lim inf|x|\u2192\u221eU \u2032\u2032(x) > 0 implies that there is a unique strictly positive bounded\neigenfunction v1 \u2208C2(0, \u221e) \u2229C([0,\u221e)) satisfying v1(0) = 0, v\u2032\n1(0) = 1 and Lv1 = \u2212\u03bb1v1, where\n\u03bb1 = \u03bb1(0, \u221e) =\ninf\nv\u2208C\u221e\n0 (0,\u221e)\n1\n2\n\u00b4 \u221e\n0\nv\u2032(x)2 exp(\u2212U(x)) dx\n\u00b4 \u221e\n0\nv(x)2 exp(\u2212U(x))dx\nis the in\ufb01mum of the spectrum of the self-adjoint realization of \u2212L with Dirichlet boundary con-\nditions on (0, \u221e). Since Lv1 = \u2212\u03bb1v1 and v1 is bounded, the process Mt = exp(\u03bb1t)v1(Xt) is a\nmartingale. Optional stopping applied to the di\ufb00usion with initial condition X0 = x0 shows that\nv1(x0) = Ex0 [M0] = Ex0 [M\u03c40\u2227t] = Ex0 [exp(\u03bb1t)v1(Xt); \u03c40 > t]\n\u2264exp(\u03bb1t) Px0 [\u03c40 > t] sup v1\n(78)\nfor any x0 > 0 and t \u22650. Since v1(x0) > 0 and sup v1 < \u221e, the estimate (78) implies the\nasymptotic lower bound\nlim inf\nt\u2192\u221et\u22121 log Px0[\u03c40 > t] \u2265\u2212\u03bb1(0, \u221e).\n(79)\nMoreover, for any \ufb01xed t \u2264\u03bb\u22121\n1 /4,\nPR [\u03c40 > t] \u2265e\u22121/4 v1(R)/ sup v1 \u22653/4\n20\nAndreas Eberle\nprovided v1(R) \u2265\n3\n4e1/4 sup v1 = 0.96 . . . \u00b7 sup v1. By the eigenfunction equation eU(e\u2212Uv\u2032\n1)\u2032 =\n\u2212\u03bb1v1, one veri\ufb01es that the latter condition is satis\ufb01ed whenever U is growing fast enough on\n[R, \u221e).\nFor bounding \u03bb1(0, \u221e) from above let\nv(x) = min(\n\u221a\nLx, 1) =\n\u001a \u221a\nLx if x \u22641/\n\u221a\nL,\n1\nif x \u22651/\n\u221a\nL.\nBy the assumptions on U, the function v is contained in the weighted Sobolev space H1,2\n0\n((0,\u221e), e\u2212U dx)\n(closure of C\u221e\n0 (0, \u221e) w.r.t. the norm \u2225w\u22252 =\n\u00b4 \u221e\n0 (w2 + (w\u2032)2) e\u2212U dx). Therefore, if LR2/4 \u22651\nthen (28) holds, since\n\u03bb1 \u2264\n1\n2\n\u00b4\nv\u2032(x)2 exp(\u2212U(x)) dx\n\u00b4\nv(x)2 exp(\u2212U(x)) dx\n\u2264\n\u00b4 1/\n\u221a\nL\n0\nL exp(Lx2/2) dx\n\u00b4 R/2\n0\nv(x)2 exp(Lx2/2) dx\n= L\n2\n\u00b4 1\n0 exp(y2/2) dy\n\u00b4 \u221a\nLR2/4\n0\nmin(y, 1)2 exp(y2/2) dy\n\u22643Le1/2\n2\nr\nLR2\n4\nexp\n\u0012LR2\n8\n\u0013\n.\nHere we have used that by assumption, U(x) \u2265\u2212Lx2/2 for any x \u2208R with equality for |x| < R/2,\nand for x \u22651,\n\u02c6 x\n0\nmin(y, 1)2ey2/2 dy =\n\u02c6 1\n0\n. . . +\n\u02c6 x\n1\n. . . \u22651\n3 + 1\nxex2/2 \u22121 \u2265\n1\n3xex2/2\nas (x\u22121ex2/2)\u2032 = (1 \u2212x\u22122)ex2/2 \u2264ex2/2. \u25a1\nProof of Lemma 2. Since b = b0 + \u03b3, we have\n(x \u2212y) \u00b7 G(b(x) \u2212b(y)) = (x \u2212y) \u00b7 G(b0(x) \u2212b0(y)) + (x \u2212y) \u00b7 G(\u03b3(x) \u2212\u03b3(y))\nfor any x, y \u2208Rd. Therefore, by (31) and by de\ufb01nition of \u03ba and \u03ba0,\n\u03ba(r)\u2212\u2264\u03ba0(r)\u2212\nfor any r \u2264R, and\n(80)\n\u03ba(r)\u2212\u2264\u03ba0(r)\u2212+ 4r\u22121 sup \u2225\u03b3\u2225\nfor any r \u2208(0, \u221e).\n(81)\nIn particular, if \u03b3 is bounded then \u03ba satis\ufb01es the conditions in (7). Since the constant R1(b) de\ufb01ned\nw.r.t. b is smaller than the corresponding constant R1 de\ufb01ned w.r.t. b0, we obtain\n1\nc \u2264\n\u02c6 R1\n0\n\u02c6 s\n0\nexp\n\u00121\n4\n\u02c6 s\nt\nu\u03ba(u)\u2212du\n\u0013\ndt ds\n\u2264\n\u02c6 R1\n0\n\u02c6 s\n0\nexp\n\u00121\n4\n\u02c6 s\nt\nu\u03ba0(u)\u2212du\n\u0013\nexp (R sup \u2225\u03b3\u2225) dt ds\n\u22641\nc0 \u00b7 exp (R sup \u2225\u03b3\u2225),\ni.e., (32) holds.\nSimilarly, if \u03b3 satis\ufb01es the one-sided Lipschitz condition (33) then\n\u03ba(r)\u2212\u2264\u03ba0(r)\u2212+ 2L\nfor any r \u2208(0, \u221e).\n(82)\nHence again the conditions in (7) are satis\ufb01ed, and we obtain\n1\nc \u2264\n1\nc0 \u00b7 exp\n\u0012L\n2\n\u02c6 R\n0\nr dr\n\u0013\nsimilarly as above, i.e., (34) holds. \u25a1\nRe\ufb02ection couplings and contraction rates\n21\nProof of Theorem 6. Fix R > 0 and probability measures \u00b5, \u03bd on Rd. By de\ufb01nition of fR,\nf \u2032\u2032\nR(r) \u22641\n4r\u03ba(r)f \u2032\nR(r) \u2212fR(r)\n\u001e\u02c6 R\n0\n\u03a6(s)\n\u03d5(s) ds\nfor any r < R. Therefore, similarly to the proof of Theorem 1, Equation (60) shows that the process\necRtfR(rt) is a local supermartingale for t < \u02c6\u03c4R where\n\u02c6\u03c4R = inf{t \u22650 : rt > R}.\nHere rt = \u2225Xt \u2212Yt\u2225again denotes the distance process for a re\ufb02ection coupling (Xt, Yt) of two\nsolutions of (1) with initial distribution given by a coupling \u03b7 of \u00b5 and \u03bd. By optional stopping and\nFatou\u2019s lemma, we thus obtain\nE[fR(rt); \u02c6\u03c4R > t] \u2264E[fR(rt\u2227\u02c6\u03c4R)] \u2264exp(\u2212cRt) E[fR(r0)]\nfor any t \u22650, and hence\nE[fR(rt)] \u2264exp(\u2212cRt)E[fR(r0)] + P[\u02c6\u03c4R \u2264t]\n\u2264e\u2212cRt\n\u02c6\nfR(\u2225x \u2212y\u2225\u03b7(dx dy) + P\u00b5[\u03c4R/2 \u2264t] + P\u03bd[\u03c4R/2 \u2264t].\nThe assertion now follows as in the proof of Corollary 2 by minimizing over all couplings \u03b7 of \u00b5\nand \u03bd. \u25a1\n6 Couplings on product spaces\nLet d = Pn\ni=1 di with n, d1, . . . , dn \u2208N. We now consider \u201ccomponentwise\u201d couplings for di\ufb00usion\nprocesses Xt = (X1\nt , . . . , Xn\nt ) and Yt = (Y 1\nt , . . . , Y n\nt ) on Rd satisfying the s.d.e.\ndXi\nt = bi(Xt) dt + dBi\nt,\ni = 1, . . . , n,\n(83)\nwith initial conditions X0 \u223c\u00b5 and Y0 \u223c\u03bd. Here Bi, i = 1, . . . , n, are independent Brownian\nmotions on Rdi, and bi : Rdi \u2192Rdi are locally Lipschitz continuous functions such that the unique\nstrong solution of (83) is non-explosive for any given initial condition.\nLet \u03b4 > 0. Suppose that \u03bbi, \u03c0i : Rd \u2192[0, 1], i = 1, . . . , n, are Lipschitz continuous functions\nsuch that\n\u03bbi(z)2 + \u03c0i(z)2 = 1\nfor any z \u2208Rd,\nand\n(84)\n\u03bbi(z) = 0\nif |zi| \u2264\u03b4/2,\n(85)\nand let Bi and eBi, 1 \u2264i \u2264n, be independent Brownian motions on Rdi. Then a coupling of two\nsolutions of (83) with initial distributions \u00b5 and \u03bd is given by a strong solution of the system\ndXi\nt = bi(Xt) dt + \u03bbi(Zt) dBi\nt + \u03c0i(Zt) d eBi\nt,\n(86)\ndY i\nt = bi(Yt) dt + \u03bbi(Zt) (I \u22122ei\ntei,T\nt\n) dBi\nt + \u03c0i(Zt) d eBi\nt,\n1 \u2264i \u2264n, with initial distribution (X0, Y0) \u223c\u03b7 where \u03b7 is a coupling of \u00b5 and \u03bd. Here we use the\nnotation\nZt = Xt \u2212Yt,\nand ei\nt is a measurable process taking values in the unit sphere in Rdi such that\nei\nt =\n\u001a Zi\nt/|Zi\nt|\nif Zi\nt \u0338= 0,\nui\nif Zi\nt = 0,\nwhere ui is an arbitrary \ufb01xed unit vector in Rdi. Notice that by (85), the choice of ui is not\nrelevant for (86), which is a standard It\u02c6o s.d.e. in R2d with locally Lipschitz continuous coe\ufb03cients.\n22\nAndreas Eberle\nTo see that (86) de\ufb01nes a coupling, we observe that (Xt) and (Yt) satisfy (83) w.r.t. the processes\n\u02c6Bt = ( \u02c6B1\nt , . . . , \u02c6Bn\nt ) and \u02c7Bt = ( \u02c7B1\nt , . . . , \u02c7Bn\nt ) de\ufb01ned by\n\u02c6Bi\nt =\n\u02c6 t\n0\n\u03bbi(Zs) dBi\ns +\n\u02c6 t\n0\n\u03c0i(Zs) d eBi\ns,\n\u02c7Bi\nt =\n\u02c6 t\n0\n\u03bbi(Zs) (I \u22122ei\nsei,T\ns\n) dBi\ns +\n\u02c6 t\n0\n\u03c0i(Zs) d eBi\ns.\nBy L\u00b4evy\u2019s characterization and by (84), both \u02c6B and \u02c7B are indeed Brownian motions in Rd, cp. the\ncorresponding argument for re\ufb02ection coupling.\nRemark 7 (1) By Condition (85) and non-explosiveness of (83), the coupling process (Xt, Yt) is\nde\ufb01ned for any t \u22650.\n(2) By choosing \u03bbi \u22610 and \u03c0i \u22611 we recover the synchronuous coupling, i.e., the same noise is\napplied to both processes X and Y .\n(3) A componentwise re\ufb02ection coupling would be informally given by choosing \u03bbi(z) = 1 if zi \u0338= 0\nand \u03bbi(z) = 0 if zi = 0. As this function is not continuous and ei(z) = zi/|zi| also has a discontinuity\nat zero, it is not obvious how to make sense of this coupling rigorously. Instead, we will use below\nan approximate componentwise re\ufb02ection coupling where \u03bbi(z) = 1 if |zi| \u2265\u03b4 and \u03bbi(z) = 0 if\n|zi| \u2264\u03b4/2 for a small positive constant \u03b4.\nBy subtracting the equations for X and Y in (86), we see that the di\ufb00erence process Z = X \u2212Y\nsatis\ufb01es the s.d.e.\ndZi\nt = (bi(Xt) \u2212bi(Yt)) dt + 2\u03bbi(Zt) ei\nt dW i\nt ,\n(87)\ni = 1, . . . , n, where the processes\nW i\nt =\n\u02c6 t\n0\nei\nt \u00b7 dBi\nt,\n1 \u2264i \u2264n,\nare independent one-dimensional Brownian motions.\nLet ri\nt = |Xi\nt \u2212Y i\nt | denote the Euclidean norm of Zi\nt. The next lemma is crucial for quantifying\ncontraction properties of the coupling given by (86):\nLemma 4 Suppose that f : [0, \u221e) \u2192[0, \u221e) is a strictly increasing concave function in C1([0,\u221e))\nsuch that f \u2032 is absolutely continuous on (0, \u221e). Then for any i = 1, . . . , n, the process f(ri\nt) satis\ufb01es\nthe It\u02c6o equation\nf(ri\nt) = f(ri\n0) + 2\n\u02c6 t\n0\n\u03bbi(Xs \u2212Ys) f \u2032(ri\ns) dW i\ns\n+\n\u02c6 t\n0\nn\nei\ns \u00b7 (bi(Xs) \u2212bi(Ys)) f \u2032(ri\ns) + 2\u03bbi(Xs \u2212Ys)2 f \u2032\u2032(ri\ns)\no\nds.\n(88)\nRemark 8 The lemma shows in particular that the process ri\nt satis\ufb01es\ndri\nt = ei\nt \u00b7 (bi(Xt) \u2212bi(Yt)) dt + 2\u03bbi(Xt \u2212Yt) dW i\nt .\n(89)\nNotice that in this equation, the drift term does not depend on the choice of \u03bb.\nProof of Lemma 4. Recall that ei\nt = Zi\nt/|Zi\nt| if ri\nt = |Zi\nt| \u0338= 0. Since the function y 7\u2192y/|y|\nis smooth on Rdi \\ {0} and x 7\u2192\u221ax is smooth on (0, \u221e), we can apply It\u02c6o\u2019s formula and (87) to\nshow that the It\u02c6o equations\nd|Zi|2 = 2Zi \u00b7 (bi(X) \u2212bi(Y )) dt + 4 \u03bbi(Z)2 dt + 4\u03bbi(Z) |Zi| dW i ,\ndri =\n1\n2ri d|Zi|2 \u2212\n1\n8(ri)3 d[|Zi|2]\n= ei \u00b7 (bi(X) \u2212bi(Y )) dt + 2\u03bbi(X \u2212Y ) dW i\n(90)\nRe\ufb02ection couplings and contraction rates\n23\nhold almost surely on any stochastic interval [\u03c41, \u03c42] such that Zi\nt \u0338= 0 a.s. for \u03c41 \u2264t \u2264\u03c42.\nOn the other hand, suppose that |Zi| < \u03b4/2 a.s. on a stochastic interval [\u03c43, \u03c44]. Then on [\u03c43, \u03c44],\n\u03bb(Z) \u22610 by (85), and hence Zi is almost surely absolutely continuous with\ndZi/dt = bi(X) \u2212bi(Y )\na.e. on [\u03c43, \u03c44].\nThis implies that ri = |Zi| is almost surely absolutely continuous on [\u03c43, \u03c44] as well with\ndri/dt = ei \u00b7 (bi(X) \u2212bi(Y ))\na.e. on [\u03c43, \u03c44],\n(91)\nwhich is equivalent to (89) on [\u03c43, \u03c44]. Note that the value of ei for Zi = 0 is not relevant here,\nsince Zi can only stay at 0 for a positive amount of time if bi(X) \u2212bi(Y ) vanishes during that time\ninterval.\nSince R+ is the union of countably many stochastic intervals of the \ufb01rst and second type\nconsidered above, the It\u02c6o equation (89) holds almost surely on R+. The assertion (88) now follows\nfrom (89) by another application of It\u02c6o\u2019s formula. Here it is enough to assume that f is C1 on\n[0, \u221e) and f \u2032 is absolutely continuous on (0, \u221e) because \u03bbi(Xs \u2212Ys) vanishes for ri\ns < \u03b4/2. \u25a1\nWe now \ufb01x weights w1, . . . wn \u2208[0, \u221e) and strictly increasing concave functions f1, . . . , fn \u2208\nC1([0, \u221e)) \u2229C2((0,\u221e)) such that fi(0) = 0 for any i. Consider\n\u03c1t =\nn\nX\ni=1\nfi(ri\nt) wi = df,w(Xt, Yt)\n(92)\nwhere df,w is de\ufb01ned by (44). By Lemma 4,\nd\u03c1t =\nn\nX\ni=1\n\u0010\nei\nt \u00b7 (bi(Xt) \u2212bi(Yt)) f \u2032\ni(ri\nt) + 2\u03bbi(Xt \u2212Yt)2 f \u2032\u2032\ni (ri\nt)\n\u0011\nwi dt\n+2\nn\nX\ni=1\n\u03bbi(Xt \u2212Yt) f \u2032\ni(ri\nt) dW i\nt .\n(93)\nNotice that the last term on the right hand side is a martingale since \u03bbi and f \u2032\ni are bounded. This\nenables us to control the expectation E[\u03c1t] if we can bound the drift in (93) by m\u2212c\u03c1t for constants\nm, c \u2208(0, \u221e):\nLemma 5 Let m, c \u2208(0, \u221e) and suppose that\nn\nX\ni=1\n\u0012\ncfi(ri) + (xi \u2212yi) \u00b7 (bi(x) \u2212bi(y)) f \u2032\ni(ri)\nri\n+ 2\u03bbi(x \u2212y)2 f \u2032\u2032\ni (ri)\n\u0013\nwi \u2264m\n(94)\nholds for any x, y \u2208Rd with ri := |xi \u2212yi| > 0 \u2200i \u2208{1, . . . n}. Then\nE[\u03c1t] \u2264e\u2212ct E[\u03c10] + m (1 \u2212e\u2212ct)/c\nfor any t \u22650.\n(95)\nProof. We \ufb01rst note that by continuity of bi and f \u2032\ni, (94) implies that\nn\nX\ni=1\n\u0010\ncfi(ri) + ei \u00b7 (bi(x) \u2212bi(y)) f \u2032\ni(ri) + 2\u03bbi(x \u2212y)2 f \u2032\u2032\ni (ri)\n\u0011\nwi \u2264m\n(96)\nholds for any x, y \u2208Rd (even if xi \u2212yi = 0) provided ei = (xi \u2212yi)/ri if ri > 0 and ei is an\narbitrary unit vector if ri = 0. Indeed, we obtain (96) by applying (94) with xi replaced by xi +hei\nwhenever xi \u2212yi = 0 and taking the limit as h \u21930. In particular, by (96), the drift term \u03b2t in (93)\nis bounded from above by\n\u03b2t \u2264m \u2212\nn\nX\ni=1\ncfi(ri\nt)wi = m \u2212c\u03c1t.\n24\nAndreas Eberle\nTherefore by (93) and by the It\u02c6o product rule,\nd(ect\u03c1) = ect d\u03c1 + cect\u03c1 dt \u2264ectm dt + dM\nwhere M is a martingale, and thus\nE[ect\u03c1t] \u2264E[\u03c10] + m\n\u02c6 t\n0\necs ds\nfor any t \u22650.\n\u25a1\nSince f \u2032\u2032\ni \u22640, the process \u03c1t is decreasing more rapidly (or growing more slowly) if \u03bbi takes\nlarger values. In particular, the decay properties of \u03c1t would be optimized when \u03bbi(z) = 1 for any z\nwith zi \u0338= 0. This optimal choice of \u03bb1, . . . , \u03bbn would correspond to a componentwise re\ufb02ection cou-\npling, but it violates Condition (85). It is perhaps possible to construct a corresponding coupling\nprocess by an approximation argument. For our purpose of bounding the Kantorovich distance\nWf,w(\u00b5pt, \u03bdpt) this is not necessary. Indeed, it will be su\ufb03cient to consider approximate compo-\nnentwise re\ufb02ection couplings where (84) and (85) are satis\ufb01ed and \u03bbi(z) = 1 whenever |zi| > \u03b4.\nThe limit \u03b4 \u21930 will then be considered for the resulting estimates of the Kantorovich distance but\nnot for the coupling processes.\n7 Application to interacting di\ufb00usions\nWe will now apply the couplings introduced in Section 6 to prove the contraction properties for\nsystems of interacting di\ufb00usions stated in Theorem 7 and Corollary 8. We consider the setup\ndescribed in Section 3.1, i.e.,\nbi(x) = bi\n0(xi) + \u03b3i(x)\nfor i = 1, . . . , n\n(97)\nwith bi\n0 : Rdi \u2192Rdi locally Lipschitz such that \u03bai de\ufb01ned by (45) is continuous on (0, \u221e) with\nlim inf\nr\u2192\u221e\u03bai(r) > 0 and\nlim\nr\u21920 r\u03bai(r) = 0 for any 1 \u2264i \u2264n.\n(98)\nThe functions fi are de\ufb01ned via \u03bai, and ci is the corresponding contraction rate given by (48).\nProof of Theorem 7. We \ufb01x \u03b4 > 0 and Lipschitz continuous functions \u03bbi, \u00b5i : Rd \u2192[0, 1],\n1 \u2264i \u2264n, such that (84) and (85) hold and \u03bbi(z) = 1 if |zi| \u2265\u03b4. Let (Xt, Yt) denote a corresponding\napproximate componentwise re\ufb02ection coupling of two solutions of (42) given by (86), and let\n\u03c1t = df,w(Xt, Yt). We will apply Lemma 5 which requires bounding the right hand side in (94).\nFor this purpose recall that fi and ci have been chosen in such a way that\n2f \u2032\u2032\ni (r) \u22121\n2r\u03bai(r)f \u2032\ni(r) \u2264\u2212ci fi(r)\n\u2200r > 0,\ncf. (68) and (69). Therefore, by (97) and by de\ufb01nition of \u03bai,\n(xi \u2212yi) \u00b7 (bi(x) \u2212bi(y))f \u2032\ni(ri)/ri + 2\u03bbi(x \u2212y)2 f \u2032\u2032\ni (ri)\n\u2264\u22121\n2ri\u03bai(ri)f \u2032\ni(ri) + |\u03b3i(x) \u2212\u03b3i(y)|f \u2032\ni(ri) + 2\u03bbi(x \u2212y)2 f \u2032\u2032\ni (ri)\n\u2264\u2212\u03bbi(x \u2212y)2cifi(ri) + |\u03b3i(x) \u2212\u03b3i(y)| \u22121\n2(1 \u2212\u03bbi(x \u2212y)2) ri\u03bai(ri)f \u2032\ni(ri)\n(99)\n\u2264\u2212cifi(ri) + |\u03b3i(x) \u2212\u03b3i(y)| + ci\u03b4 + 1\n2 sup\nr<\u03b4\n\u0010\nr\u03bai(r)\u2212\u0011\nRe\ufb02ection couplings and contraction rates\n25\nfor any x, y \u2208Rd with ri = |xi \u2212yi| > 0. Here we have used that 0 \u2264f \u2032\ni \u22641, and that \u03bbi(x\u2212y) \u0338= 1\nonly if ri < \u03b4. In this case, fi(ri) \u2264ri \u2264\u03b4. By (99) and by the assumption (49) on \u03b3i, we obtain\nn\nX\ni=1\n\u0010\n(xi \u2212yi) \u00b7 (bi(x) \u2212bi(y)) f \u2032\ni(ri)/ri + 2\u03bbi(x \u2212y)2 f \u2032\u2032\ni (ri)\n\u0011\nwi\n\u2264m(\u03b4) +\nn\nX\ni=1\n(\u2212ci + \u03b5i)fi(ri)wi \u2264m(\u03b4) \u2212c\nn\nX\ni=1\nfi(ri)wi\nfor x, y as above, where\nm(\u03b4) =\nn\nX\ni=1\n(ci\u03b4 + 1\n2 sup\nr<\u03b4\n(r\u03bai(r)\u2212)\nis a \ufb01nite constant by (98), and c = mini=1,...n(ci \u2212\u03b5i). Hence (94) is satis\ufb01ed with c and m(\u03b4)\nand, therefore,\nE[\u03c1t] \u2264e\u2212ct E[\u03c10] + m(\u03b4) (1 \u2212e\u2212ct)/c.\n(100)\nBy choosing the coupling process (Xt, Yt) with initial distribution given by a coupling \u03b7 of proba-\nbility measures \u00b5 and \u03bd on Rd, we conclude that\nWf,w(\u00b5pt, \u03bdpt) \u2264E [df,w(Xt, Yt)] = E[\u03c1t]\n\u2264e\u2212ct\n\u02c6\ndf,w(x, y) \u03b7(dx dy) + m(\u03b4) (1 \u2212e\u2212ct)/c\n(101)\nfor any t \u22650. Moreover, by (47), m(\u03b4) \u21920 as \u03b4 \u21930. Hence the assertion (50) follows from (101)\nby taking the limit as \u03b4 \u21930 and minimizing over all couplings \u03b7 of \u00b5 and \u03bd. Finally, (51) follows\nfrom (50) since \u03d5(Ri\n0)r/2 \u2264fi(r) \u2264r implies\nA\u22121 d\u21131(x, y) \u2264df,w(x, y) =\nX\nfi(|xi \u2212yi|) wi \u2264d\u21131(x, y). \u25a1\nProof of Corollary 8.\nThe \u21131-Lipschitz condition (52) for \u03b3 implies that (49) holds with\nwi = 1 for any i, and\n\u03bb\u03b5\u22121\ni\n= inf\nr>0 fi(r) = f \u2032\ni(Ri\n1) = \u03d5i(Ri\n0)/2,\ni.e., \u03b5i = 2\u03bb/\u03d5i(Ri\n0). The assertion now follows from Theorem 7.\n\u25a1\nAcknowledgements I would like to thank the referees for helpful comments. Financial support from the German\nScience Foundation through the Hausdor\ufb00Center for Mathematics is gratefully acknowledged.\nReferences\n1. Andres, S.: Pathwise di\ufb00erentiability for SDEs in a smooth domain with re\ufb02ection. Electron. J. Probab. 16,\nno. 28, 845\u2013879 (2011)\n2. An\u00b4e, C., Blach`ere, S., Chafa\u00a8\u0131, D., Foug`eres, P., Gentil, I., Malrieu, F., Roberto, C., Sche\ufb00er, G.: Sur les\nin\u00b4egalit\u00b4es de Sobolev logarithmiques, Panoramas et Synth`eses [Panoramas and Syntheses], vol. 10. Soci\u00b4et\u00b4e\nMath\u00b4ematique de France, Paris (2000). With a preface by Dominique Bakry and Michel Ledoux\n3. Bakry, D.: L\u2019hypercontractivit\u00b4e et son utilisation en th\u00b4eorie des semigroupes. In: Lectures on probability\ntheory (Saint-Flour, 1992), Lecture Notes in Math., vol. 1581, pp. 1\u2013114. Springer, Berlin (1994)\n4. Bakry, D., Cattiaux, P., Guillin, A.: Rate of convergence for ergodic continuous Markov processes: Lyapunov\nversus Poincar\u00b4e. J. Funct. Anal. 254(3), 727\u2013759 (2008)\n5. Bakry, D., \u00b4Emery, M.: Di\ufb00usions hypercontractives.\nIn: S\u00b4eminaire de probabilit\u00b4es, XIX, 1983/84, Lecture\nNotes in Math., vol. 1123, pp. 177\u2013206. Springer, Berlin (1985)\n6. Bakry, D., Gentil, I., Ledoux, M.: Analysis and geometry of Markov di\ufb00usion operators, Grundlehren der\nMathematischen Wissenschaften [Fundamental Principles of Mathematical Sciences], vol. 348.\nSpringer,\nCham (2014)\n26\nAndreas Eberle\n7. Bolley, F., Gentil, I., Guillin, A.: Convergence to equilibrium in Wasserstein distance for Fokker-Planck\nequations. J. Funct. Anal. 263(8), 2430\u20132457 (2012)\n8. Bou-Rabee, N., Hairer, M.: Nonasymptotic mixing of the MALA algorithm. IMA J. Numer. Anal. 33(1),\n80\u2013110 (2013)\n9. Bou-Rabee, N., Vanden-Eijnden, E.: A patch that imparts unconditional stability to explicit integrators for\nLangevin-like equations. J. Comput. Phys. 231(6), 2565\u20132580 (2012)\n10. Burdzy, K., Chen, Z.Q., Jones, P.: Synchronous couplings of re\ufb02ected Brownian motions in smooth domains.\nIllinois J. Math. 50(1-4), 189\u2013268 (electronic) (2006)\n11. Cattiaux, P., Guillin, A.: Semi log-concave Markov di\ufb00usions. In: S\u00b4eminaire de Probabilit\u00b4es XLVI, Lecture\nNotes in Math., vol. 2123, pp. 231\u2013292. Springer, Cham (2014)\n12. Chen, G.Y., Salo\ufb00-Coste, L.: The cuto\ufb00phenomenon for ergodic Markov processes. Electron. J. Probab. 13,\nno. 3, 26\u201378 (2008)\n13. Chen, M.F.: From Markov chains to nonequilibrium particle systems. World Scienti\ufb01c Publishing Co. Inc.,\nRiver Edge, NJ (1992)\n14. Chen, M.F., Li, S.F.: Coupling methods for multidimensional di\ufb00usion processes. Ann. Probab. 17(1), 151\u2013177\n(1989)\n15. Chen, M.F., Wang, F.Y.: Estimation of the \ufb01rst eigenvalue of second order elliptic operators. J. Funct. Anal.\n131(2), 345\u2013363 (1995)\n16. Chen, M.F., Wang, F.Y.: Estimation of spectral gap for elliptic operators. Trans. Amer. Math. Soc. 349(3),\n1239\u20131267 (1997)\n17. Cranston, M.: Gradient estimates on manifolds using coupling. J. Funct. Anal. 99(1), 110\u2013124 (1991)\n18. Cranston, M., Le Jan, Y.: Noncoalescence for the Skorohod equation in a convex domain of R2. Probab.\nTheory Related Fields 87(2), 241\u2013252 (1990)\n19. Diaconis, P.: The cuto\ufb00phenomenon in \ufb01nite Markov chains. Proc. Nat. Acad. Sci. U.S.A. 93(4), 1659\u20131664\n(1996)\n20. Diaconis, P., Salo\ufb00-Coste, L.: Separation cut-o\ufb00s for birth and death chains.\nAnn. Appl. Probab. 16(4),\n2098\u20132122 (2006)\n21. Eberle, A.: Re\ufb02ection coupling and Wasserstein contractivity without convexity. C. R. Math. Acad. Sci. Paris\n349(19-20), 1101\u20131104 (2011)\n22. Eberle, A.: Error bounds for Metropolis\u2013Hastings algorithms applied to perturbations of Gaussian measures\nin high dimensions. Ann. Appl. Probab. 24(1), 337\u2013377 (2014)\n23. Freidlin, M.: Functional integration and partial di\ufb00erential equations, Annals of Mathematics Studies, vol.\n109. Princeton University Press, Princeton, NJ (1985)\n24. Hairer, M., Mattingly, J.C.: Spectral gaps in Wasserstein distances and the 2D stochastic Navier-Stokes\nequations. Ann. Probab. 36(6), 2050\u20132091 (2008)\n25. Hairer, M., Mattingly, J.C., Scheutzow, M.: Asymptotic coupling and a general form of Harris\u2019 theorem with\napplications to stochastic delay equations. Probab. Theory Related Fields 149(1-2), 223\u2013259 (2011)\n26. Herrmann, S., Tugaut, J.: Non-uniqueness of stationary measures for self-stabilizing processes. Stochastic\nProcess. Appl. 120(7), 1215\u20131246 (2010)\n27. Herrmann, S., Tugaut, J.: Self-stabilizing processes: uniqueness problem for stationary measures and conver-\ngence rate in the small-noise limit. ESAIM Probab. Stat. 16, 277\u2013305 (2012)\n28. Hsu, E.P.: Stochastic analysis on manifolds, Graduate Studies in Mathematics, vol. 38. American Mathemat-\nical Society, Providence, RI (2002)\n29. Joulin, A.: A new Poisson-type deviation inequality for Markov jump processes with positive Wasserstein\ncurvature. Bernoulli 15(2), 532\u2013549 (2009)\n30. Joulin, A., Ollivier, Y.: Curvature, concentration and error estimates for Markov chain Monte Carlo. Ann.\nProbab. 38(6), 2418\u20132442 (2010)\n31. Kendall, W.S.: Coupled Brownian motions and partial domain monotonicity for the Neumann heat kernel.\nJ. Funct. Anal. 86(2), 226\u2013236 (1989)\n32. Komorowski, T., Walczuk, A.: Central limit theorem for Markov processes with spectral gap in the Wasserstein\nmetric. Stochastic Process. Appl. 122(5), 2155\u20132184 (2012)\n33. Levin, D.A., Luczak, M.J., Peres, Y.: Glauber dynamics for the mean-\ufb01eld Ising model: cut-o\ufb00, critical power\nlaw, and metastability. Probab. Theory Related Fields 146(1-2), 223\u2013265 (2010)\n34. Lindvall, T.: Lectures on the coupling method.\nDover Publications Inc., Mineola, NY (2002).\nCorrected\nreprint of the 1992 original\n35. Lindvall, T., Rogers, L.C.G.: Coupling of multidimensional di\ufb00usions by re\ufb02ection.\nAnn. Probab. 14(3),\n860\u2013872 (1986)\n36. Lions, P.L., Sznitman, A.S.: Stochastic di\ufb00erential equations with re\ufb02ecting boundary conditions. Comm.\nPure Appl. Math. 37(4), 511\u2013537 (1984)\n37. von Renesse, M.K., Sturm, K.T.: Transport inequalities, gradient estimates, entropy, and Ricci curvature.\nComm. Pure Appl. Math. 58(7), 923\u2013940 (2005)\n38. Royer, G.: An initiation to logarithmic Sobolev inequalities, SMF/AMS Texts and Monographs, vol. 14.\nAmerican Mathematical Society, Providence, RI (2007). Translated from the 1999 French original by Donald\nBabbitt\n39. Stroock, D.W.: Partial di\ufb00erential equations for probabilists, Cambridge Studies in Advanced Mathematics,\nvol. 112. Cambridge University Press, Cambridge (2012). Paperback edition of the 2008 original\n40. Tanaka, H.: Stochastic di\ufb00erential equations with re\ufb02ecting boundary condition in convex regions. Hiroshima\nMath. J. 9(1), 163\u2013177 (1979)\nRe\ufb02ection couplings and contraction rates\n27\n41. Thorisson, H.: Coupling, stationarity, and regeneration.\nProbability and its Applications (New York).\nSpringer-Verlag, New York (2000)\n42. Wang, F.Y.: Application of coupling methods to the Neumann eigenvalue problem. Probab. Theory Related\nFields 98(3), 299\u2013306 (1994)\n43. Wang, F.Y.: Functional inequalities, Markov semigoups and spectral theory. Science Press (2005)\n",
        "sentence": " Definition 4 (Distant dissipativity (Eberle, 2015; Gorham et al., 2016)).",
        "context": "Cham (2014)\n26\nAndreas Eberle\n7. Bolley, F., Gentil, I., Guillin, A.: Convergence to equilibrium in Wasserstein distance for Fokker-Planck\nequations. J. Funct. Anal. 263(8), 2430\u20132457 (2012)\n6. Bakry, D., Gentil, I., Ledoux, M.: Analysis and geometry of Markov di\ufb00usion operators, Grundlehren der\nMathematischen Wissenschaften [Fundamental Principles of Mathematical Sciences], vol. 348.\nSpringer,\nCham (2014)\n26\nAndreas Eberle\nrelevant for (86), which is a standard It\u02c6o s.d.e. in R2d with locally Lipschitz continuous coe\ufb03cients.\n22\nAndreas Eberle\nTo see that (86) de\ufb01nes a coupling, we observe that (Xt) and (Yt) satisfy (83) w.r.t. the processes\n\u02c6Bt = ( \u02c6B1\nt , . . . , \u02c6Bn"
    },
    {
        "title": "Output assessment for Monte Carlo simulations via the score statistic",
        "author": [
            "Y. Fan",
            "S.P. Brooks",
            "A. Gelman"
        ],
        "venue": "J. Comp. Graph. Stat.,",
        "citeRegEx": "Fan et al\\.,? \\Q2006\\E",
        "shortCiteRegEx": "Fan et al\\.",
        "year": 2006,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " For example, when P = N (0, 1), the score statistic (Fan et al., 2006) only monitors sample means and variances.",
        "context": null
    },
    {
        "title": "Kernel measures of conditional dependence",
        "author": [
            "K. Fukumizu",
            "A. Gretton",
            "X. Sun",
            "B. Sch\u00f6lkopf"
        ],
        "venue": "In NIPS,",
        "citeRegEx": "Fukumizu et al\\.,? \\Q2007\\E",
        "shortCiteRegEx": "Fukumizu et al\\.",
        "year": 2007,
        "abstract": "",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "Markov chain Monte Carlo maximum likelihood",
        "author": [
            "C.J. Geyer"
        ],
        "venue": "Computer Science and Statistics: Proc. 23rd Symp. Interface,",
        "citeRegEx": "Geyer,? \\Q1991\\E",
        "shortCiteRegEx": "Geyer",
        "year": 1991,
        "abstract": "",
        "full_text": "",
        "sentence": " When Bayesian inference and maximum likelihood estimation (Geyer, 1991) demand the evaluation of intractable expectations EP [h(Z)] = \u222b p(x)h(x)dx under a target distribution P , Markov chain Monte Carlo (MCMC) methods (Brooks et al.",
        "context": null
    },
    {
        "title": "Measuring sample quality with Stein\u2019s method",
        "author": [
            "J. Gorham",
            "L. Mackey"
        ],
        "venue": "Adv. NIPS",
        "citeRegEx": "Gorham and Mackey,? \\Q2015\\E",
        "shortCiteRegEx": "Gorham and Mackey",
        "year": 2015,
        "abstract": "",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "Measuring sample quality with diffusions",
        "author": [
            "J. Gorham",
            "A. Duncan",
            "S. Vollmer",
            "L. Mackey"
        ],
        "venue": null,
        "citeRegEx": "Gorham et al\\.,? \\Q2016\\E",
        "shortCiteRegEx": "Gorham et al\\.",
        "year": 2016,
        "abstract": "Stein's method for measuring convergence to a continuous target distribution\nrelies on an operator characterizing the target and Stein factor bounds on the\nsolutions of an associated differential equation. While such operators and\nbounds are readily available for a diversity of univariate targets, few\nmultivariate targets have been analyzed. We introduce a new class of\ncharacterizing operators based on Ito diffusions and develop explicit\nmultivariate Stein factor bounds for any target with a fast-coupling Ito\ndiffusion. As example applications, we develop computable and\nconvergence-determining diffusion Stein discrepancies for log-concave,\nheavy-tailed, and multimodal targets and use these quality measures to select\nthe hyperparameters of biased Markov chain Monte Carlo (MCMC) samplers, compare\nrandom and deterministic quadrature rules, and quantify bias-variance tradeoffs\nin approximate MCMC. Our results establish a near-linear relationship between\ndiffusion Stein discrepancies and Wasserstein distances, improving upon past\nwork even for strongly log-concave targets. The exposed relationship between\nStein factors and Markov process coupling may be of independent interest.",
        "full_text": "MEASURING SAMPLE QUALITY WITH DIFFUSIONS\nBy Jackson Gorham, Andrew B. Duncan, Sebastian J.\nVollmer, and Lester Mackey\nStanford University, Imperial College London, University of Warwick, and\nMicrosoft Research New England\nStein\u2019s method for measuring convergence to a continuous target\ndistribution relies on an operator characterizing the target and Stein\nfactor bounds on the solutions of an associated di\ufb00erential equation.\nWhile such operators and bounds are readily available for a diversity\nof univariate targets, few multivariate targets have been analyzed. We\nintroduce a new class of characterizing operators based on It\u02c6o di\ufb00u-\nsions and develop explicit multivariate Stein factor bounds for any\ntarget with a fast-coupling It\u02c6o di\ufb00usion. As example applications, we\ndevelop computable and convergence-determining di\ufb00usion Stein dis-\ncrepancies for log-concave, heavy-tailed, and multimodal targets and\nuse these quality measures to select the hyperparameters of biased\nMarkov chain Monte Carlo (MCMC) samplers, compare random and\ndeterministic quadrature rules, and quantify bias-variance tradeo\ufb00s\nin approximate MCMC. Our results establish a near-linear relation-\nship between di\ufb00usion Stein discrepancies and Wasserstein distances,\nimproving upon past work even for strongly log-concave targets. The\nexposed relationship between Stein factors and Markov process cou-\npling may be of independent interest.\n1. Introduction.\nConsider a target probability distribution P with \ufb01-\nnite mean, continuously di\ufb00erentiable density p, and support on all of Rd.\nWe will name the set of all such distributions P1. We assume that p can be\nevaluated up to its normalizing constant but that exact expectations under\nP are unattainable for most functions of interest. We will therefore use a\nweighted sample, represented as a discrete distribution Qn = Pn\ni=1 q(xi)\u03b4xi,\nto approximate intractable expectations EP [h(Z)] with tractable sample es-\ntimates EQn[h(X)] = Pn\ni=1 q(xi)h(xi). Here, the support of Qn is a collection\nof distinct sample points x1, . . . , xn \u2208Rd, and the weight q(xi) associated\nwith each point is governed by a probability mass function q. We assume\nnothing about the process generating the sample points, so they may be the\nproduct of any random or deterministic mechanism.\nMSC 2010 subject classi\ufb01cations: Primary 60J60; 62-04; 62E17; 60E15; 65C60; sec-\nondary 62-07; 65C05; 68T05\nKeywords and phrases: multivariate Stein factors, It\u02c6o di\ufb00usion, Stein\u2019s method, Stein\ndiscrepancy, sample quality, Wasserstein decay, Markov chain Monte Carlo\n1\narXiv:1611.06972v6  [stat.ML]  13 Nov 2018\n2\nJ. GORHAM, A.B. DUNCAN, S.J. VOLLMER, AND L. MACKEY\nOur ultimate goal is to develop a computable quality measure suitable\nfor comparing any two samples approximating the same target distribution.\nMore precisely, we seek to quantify how well EQn approximates EP in a\nmanner that, at the very least, (i) indicates when a sample sequence is\nconverging to P, (ii) identi\ufb01es when a sample sequence is not converging\nto P, and (iii) is computationally tractable. A natural starting point is to\nconsider the maximum error incurred by the sample approximation over a\nclass of scalar test functions H,\ndH(Qn, P) \u225csup\nh\u2208H\n|EP [h(Z)] \u2212EQn[h(X)]|.\n(1)\nWhen H is convergence determining, the measure (1) is an integral probabil-\nity metric (IPM) [67], and dH(Qn, P) converges to zero only if the sample\nsequence (Qn)n\u22651 converges in distribution to P.\nWhile a variety of standard probability metrics are representable as IPMs\n[67], the intractability of integration under P precludes us from computing\nmost of these candidate quality measures. Recently, Gorham and Mackey\n[35] sidestepped this issue by constructing a class of test functions h known\na priori to have zero mean under P. Their resulting quality measure \u2013 the\nLangevin graph Stein discrepancy \u2013 satis\ufb01ed our computability and conver-\ngence detection requirements (Desiderata (i) and (iii)) and detected sample\nsequence non-convergence (Desideratum (ii)) for strongly log concave tar-\ngets with bounded third and fourth derivatives [63].\nOur \ufb01rst contribution is to show that the Langevin Stein discrepancy\nin fact determines convergence for all smooth, distantly dissipative target\ndistributions by explicitly lower and upper bounding the Langevin Stein\ndiscrepancy by standard Wasserstein distances. Distant dissipativity is a\nsubstantial relaxation of log concavity that covers a variety of common non-\nlog concave targets like Gaussian mixtures and robust Student\u2019s t regression\nposteriors. This contribution greatly extends the range of applicability of\nthe Langevin Stein discrepancy.\nBecause heavy-tailed distributions are never distantly dissipative, as a\nsecond contribution, we extend the computable Stein discrepancy framework\nof [35] to accommodate heavy-tailed target distributions by introducing a\nnew class of multivariate Stein operators based on general It\u02c6o di\ufb00usions.\nThese operators can be used as drop-in replacements for the commonly used\nLangevin operator in applications.\nAs a third contribution, we establish a near linear relationship between\nthe introduced di\ufb00usion Stein discrepancies S(Qn, T , G\u2225\u00b7\u2225) and standard Ls\n3\nWasserstein distances Ws,\u2225\u00b7\u2225(Qn, P) \u225cinfX\u223cQn,Z\u223cP E[\u2225X \u2212Z\u2225s]1/s. Namely,\nW1,\u2225\u00b7\u2225(Qn, P) \u2264C1 S(Qn, T , G\u2225\u00b7\u2225) max(1, log(1/S(Qn,T ,G\u2225\u00b7\u2225)))\nand\nS(Qn, T , G\u2225\u00b7\u2225) \u2264C2 W2,\u2225\u00b7\u2225(Qn, P)\nfor constants C1, C2 > 0 determined by Theorem 7 and Proposition 8. This\nimproves upon prior analyses even in the case of strongly log concave targets.\nOur primary contribution underlies these three advances. By relating\nStein\u2019s method to Markov process coupling rates in Section 2, we prove\nthat every su\ufb03ciently fast coupling It\u02c6o di\ufb00usion gives rise to explicit, uni-\nform multivariate Stein factor bounds on the derivatives of Stein equation\nsolutions. Stein factor bounds are central to Stein\u2019s method of measuring\ndistributional convergence, and while a wealth of bounds are available for\nunivariate targets (see, e.g., [88, 10, 11] for explicit bounds or [55] for a\nrecent review), Stein factors for continuous multivariate distributions have\nlargely been relegated to Gaussian [4, 37, 79, 9, 66, 68, 30], Dirichlet [29], and\nstrongly log-concave [63] target distributions. Our approach, which exposes\na general relationship between Stein factors and Markov process coupling\ntimes, extends the reach of Stein\u2019s method to the stationary distributions of\nall fast coupling It\u02c6o di\ufb00usions.\nIn Section 3, we provide examples of practically checkable su\ufb03cient condi-\ntions for fast coupling and illustrate the process of verifying these conditions\nfor canonical log-concave, heavy-tailed, and multimodal targets. Section 4\ndescribes a practical algorithm for computing di\ufb00usion Stein discrepancies\nusing a geometric spanner and linear programming. In Section 5, we com-\nplement the principal theoretical contributions of this work with several\nsimple numerical examples illustrating how di\ufb00usion Stein discrepancies can\nbe deployed in practice. In particular, we use our discrepancies to select\nthe hyperparameters of biased samplers, compare random and determinis-\ntic quadrature rules, and quantify bias-variance tradeo\ufb00s in approximate\nMarkov chain Monte Carlo. A discussion of related and future work follows\nin Section 6, and all proofs are deferred to the appendices.\nNotation For r \u2208[1, \u221e], let \u2225\u00b7\u2225r denote the \u2113r norm on Rd. We will use\n\u2225\u00b7\u2225as a generic norm on Rd satisfying \u2225\u00b7\u2225\u2265\u2225\u00b7\u22252 and de\ufb01ne the associated\ndual norms, \u2225v\u2225\u2217\u225csupu\u2208Rd:\u2225u\u2225=1 \u27e8u, v\u27e9for vectors v \u2208Rd and \u2225W\u2225\u2217\u225c\nsupu\u2208Rd:\u2225u\u2225=1 \u2225Wu\u2225\u2217for matrices W \u2208Rd\u00d7d. Let ej be the j-th standard\nbasis vector, \u2207j be the partial derivative\n\u2202\n\u2202xj , and \u03bbmin(\u00b7) and \u03bbmax(\u00b7) be the\nsmallest and largest eigenvalues of a symmetric matrix. For any real vector\nv and tensor T, let \u2225v\u2225op \u225c\u2225v\u22252 and \u2225T\u2225op \u225csup\u2225u\u22252=1 \u2225T[u]\u2225op. For each\nsu\ufb03ciently di\ufb00erentiable vector- or matrix-valued function g, we de\ufb01ne the\n4\nJ. GORHAM, A.B. DUNCAN, S.J. VOLLMER, AND L. MACKEY\nbound M0(g) \u225csupx\u2208Rd \u2225g(x)\u2225op and the k-th order H\u00a8older coe\ufb03cients\nMk(g) \u225csupx,y\u2208Rd,x\u0338=y\n\u2225\u2207\u2308k\u2309\u22121g(x)\u2212\u2207\u2308k\u2309\u22121g(y)\u2225op\n\u2225x\u2212y\u2225{k}\n2\nfor\nk > 0,\nwhere {k} \u225ck \u2212\u2308k \u22121\u2309and \u22070 is the identity operator. For each di\ufb00eren-\ntiable matrix-valued function a, we let \u27e8\u2207, a(x)\u27e9= P\nj ej\nP\nk \u2207kajk(x) rep-\nresent the divergence operator applied to each row of a and de\ufb01ne the Lips-\nchitz coe\ufb03cients Fk(a) \u225csupx\u2208Rd,\u2225v1\u22252=1,...,\u2225vk\u22252=1 \u2225\u2207ka(x)[v1, . . . , vk]\u2225F for\n\u2225\u00b7\u2225F the Frobenius norm. Finally, when the domain and range of a function\nf can be inferred from context, we write f \u2208Ck to indicate that f has k\ncontinuous derivatives.\n2. Stein\u2019s method.\nIn the early 1970s, Charles Stein [87] introduced\na powerful three-step approach to upper-bounding a reference IPM dH:\n1. First, identify an operator T that maps input functions1 g : Rd \u2192Rd\nin a domain G into mean-zero functions under P, i.e.,\nEP [(T g)(Z)] = 0\nfor all\ng \u2208G.\nThe operator T and its domain G de\ufb01ne the Stein discrepancy [35],\nS(Qn, T , G) \u225csup\ng\u2208G\n|EQn[(T g)(X)]|\n= sup\ng\u2208G\n|EQn[(T g)(X)] \u2212EP [(T g)(Z)]| = dT G(Qn, P),\n(2)\na quality measure which takes the form of an integral probability met-\nric while avoiding explicit integration under P.\n2. Next, prove that, for each test function h in the reference class H, the\nStein equation\nh(x) \u2212EP [h(Z)] = (T gh)(x)\n(3)\nadmits a solution gh \u2208G. This step ensures that the reference metric\ndH lower bounds the Stein discrepancy (Desideratum (ii)) and, in\npractice, can be carried out simultaneously for large classes of target\ndistributions.\n3. Finally, use whatever means necessary to upper bound the Stein dis-\ncrepancy and thereby establish convergence to zero under appropriate\nconditions (Desideratum (i)). Our general result, Proposition 8, suf-\n\ufb01ces for this purpose.\n1Real-valued g are also common, but Rd-valued g are more convenient for our purposes.\n5\nWhile Stein\u2019s method is traditionally used as analytical tool to establish\nrates of distributional convergence, we aim, following [35], to develop the\nmethod into a practical computational tool for measuring the quality of a\nsample. We begin by assessing the convergence properties of a broad class\nof Stein operators derived from It\u02c6o di\ufb00usions. Our e\ufb00orts will culminate in\nSection 4, where we show how to explicitly compute the Stein discrepancy (2)\ngiven any sample measure Qn and appropriate choices of T and G.\n2.1. Identifying a Stein operator.\nTo identify an operator T that gener-\nates mean-zero functions under P, we will appeal to the elegant and widely\napplicable generator method construction of Barbour [3, 4] and G\u00a8otze [37].\nThese authors note that if (Zt)t\u22650 is a Feller process with invariant measure\nP, then the in\ufb01nitesimal generator A of the process, de\ufb01ned pointwise by\n(Au)(x) = lim\nt\u21920 (E[u(Zt) | Z0 = x] \u2212u(x))/t\n(4)\nsatis\ufb01es EP [(Au)(Z)] = 0 under very mild restrictions on u and A. Gorham\nand Mackey [35] developed a Langevin Stein operator based on the generator\na speci\ufb01c Markov process \u2013 the Langevin di\ufb00usion described in (D1). Here,\nwe will consider a broader class of continuous Markov processes known as\nIt\u02c6o di\ufb00usions.\nDefinition 1 (It\u02c6o di\ufb00usion [70, Def. 7.1.1]).\nA (time-homogeneous) It\u02c6o\ndi\ufb00usion with starting point x \u2208Rd, Lipschitz drift coe\ufb03cient b : Rd \u2192Rd,\nand Lipschitz di\ufb00usion coe\ufb03cient \u03c3 : Rd \u2192Rd\u00d7m is a stochastic process\n(Zt,x)t\u22650 solving the It\u02c6o stochastic di\ufb00erential equation\ndZt,x = b(Zt,x) dt + \u03c3(Zt,x) dWt\nwith\nZ0,x = x \u2208Rd,\n(5)\nwhere (Wt)t\u22650 is an m-dimensional Wiener process.\nAs the next theorem, distilled from [62, Thm. 2] and [74, Sec. 4.6], shows,\nit is straightforward to construct It\u02c6o di\ufb00usions with a given invariant mea-\nsure P (see also [50, 47]).\nTheorem 2 ([62, Thm. 2] and [74, Sec. 4.6]).\nFix an It\u02c6o di\ufb00usion with\nC1 drift and di\ufb00usion coe\ufb03cients b and \u03c3, and de\ufb01ne its covariance coe\ufb03-\ncient a(x) \u225c\u03c3(x)\u03c3(x)\u22a4. P \u2208P1 is an invariant measure of this di\ufb00usion if\nand only if b(x) = 1\n2\n1\np(x)\u27e8\u2207, p(x)a(x)\u27e9+f(x) for a non-reversible component\nf \u2208C1 satisfying \u27e8\u2207, p(x)f(x)\u27e9= 0 for all x \u2208Rd. If f is P-integrable, then\nb(x) = 1\n2\n1\np(x)\u27e8\u2207, p(x)(a(x) + c(x))\u27e9\n(6)\n6\nJ. GORHAM, A.B. DUNCAN, S.J. VOLLMER, AND L. MACKEY\nfor c a di\ufb00erentiable P-integrable skew-symmetric d\u00d7d matrix\u2013valued func-\ntion termed the stream coe\ufb03cient [16, 54]. In this case, for all u \u2208C2 \u2229\ndom(A), the in\ufb01nitesimal generator (4) of the di\ufb00usion takes the form\n(Au)(x) = 1\n2\n1\np(x)\u27e8\u2207, p(x)(a(x) + c(x))\u2207u(x)\u27e9.2\n(7)\nRemarks.\nTheorem 2 does not require Lipschitz assumptions on b or\n\u03c3. An example of a non-reversible component which is not P-integrable is\nf(x) = v/p(x) for any constant vector v \u2208Rd. Prominent examples of P-\ntargeted di\ufb00usions include\n(D1) the (overdamped) Langevin di\ufb00usion (also known as the Brownian or\nSmoluchowski dynamics) [74, Secs. 6.5 and 4.5], where a \u2261I and c \u22610;\n(D2) the preconditioned Langevin di\ufb00usion [89], where c \u22610 and a \u2261\u03c3\u03c3\u22a4\nfor a constant di\ufb00usion coe\ufb03cient \u03c3 \u2208Rd\u00d7m ;\n(D3) the Riemannian Langevin di\ufb00usion [50, 83, 33], where c \u22610 and a is\nnot constant;\n(D4) the non-reversible preconditioned Langevin di\ufb00usion [see, e.g., 62, 20,\n80], where a \u2261\u03c3\u03c3\u22a4for \u03c3 \u2208Rd\u00d7m constant and c not identically 0;\n(D5) and the second-order or underdamped Langevin di\ufb00usion [43], where\nwe target the joint distribution P \u2297N(0, I) on R2d with\na \u22612\n\u00120\n0\n0\nI\n\u0013\nand c \u22612\n\u00120\n\u2212I\nI\n0\n\u0013\n.\nWe will present detailed examples making use of these di\ufb00usion classes in\nSections 3 and 5.\nTheorem 2 forms the basis for our Stein operator of choice, the di\ufb00usion\nStein operator T , de\ufb01ned by substituting g for 1\n2\u2207u in the generator (7):\n(T g)(x) =\n1\np(x)\u27e8\u2207, p(x)(a(x) + c(x))g(x)\u27e9.\n(8)\nT is an appropriate choice for our setting as it depends on P only through\n\u2207log p and is therefore computable even when the normalizing constant of\n2We have chosen an atypical form for the in\ufb01nitesimal generator in (7), as it will give\nrise to a \ufb01rst-order di\ufb00erential operator (8) with more desirable properties. One can check,\nfor instance, that the \ufb01rst order operator (T g)(x) = 2\u27e8b(x), g(x)\u27e9+ \u27e8a(x), \u2207g(x)\u27e9derived\nfrom the standard form of the generator, (Au)(x) = \u27e8b(x), \u2207u(x)\u27e9+ 1\n2\u27e8a(x), \u22072u(x)\u27e9, fails\nto satisfy Proposition 3 whenever the non-reversible component f(x) \u0338\u22610.\n7\np is unavailable. One suitable domain for T is the classical Stein set [35] of\n1-bounded functions with 1-bounded, 1-Lipschitz derivatives:\nG\u2225\u00b7\u2225\u225c\n\u001a\ng : Rd \u2192Rd\n\f\f\f\f\nsup\nx\u0338=y\u2208Rd max\n\u0010\n\u2225g(x)\u2225\u2217, \u2225\u2207g(x)\u2225\u2217, \u2225\u2207g(x)\u2212\u2207g(y)\u2225\u2217\n\u2225x\u2212y\u2225\n\u0011\n\u22641\n\u001b\n.\nIndeed, our next proposition, proved in Section A, shows that, on this do-\nmain, the di\ufb00usion Stein operator generates mean-zero functions under P.\nProposition 3.\nIf T is the di\ufb00usion Stein operator (8) for P \u2208P1 with\na, c \u2208C1 and a, c, b (6) P-integrable, then EP [(T g)(Z)] = 0 for all g \u2208G\u2225\u00b7\u2225.\nTogether, T and G\u2225\u00b7\u2225give rise to the classical di\ufb00usion Stein discrepancy\nS(Qn, T , G\u2225\u00b7\u2225), our primary object of study in Sections 2.2 and 2.3.\n2.2. Lower bounding the di\ufb00usion Stein discrepancy.\nTo establish that\nthe classical di\ufb00usion Stein discrepancy detects non-convergence (Desidera-\ntum (ii)), we will lower bound the discrepancy in terms of the L1 Wasser-\nstein distance, dW\u2225\u00b7\u22252 = W1,\u2225\u00b7\u22252, a standard reference IPM generated by\nH = W\u2225\u00b7\u22252 \u225c{h : Rd \u2192R | supx\u0338=y\u2208Rd |h(x) \u2212h(y)| \u2264\u2225x \u2212y\u22252}.\nThe \ufb01rst step is to show that, for each h \u2208W\u2225\u00b7\u22252, the solution gh to the\nStein equation (3) with di\ufb00usion Stein operator (8) has low-order derivatives\nuniformly bounded by target-speci\ufb01c constants called Stein factors.\nExplicit Langevin di\ufb00usion (D1) Stein factor bounds are readily available\nfor a wide variety of univariate targets3 (see, e.g., [88, 10, 11] for explicit\nbounds or [55] for a recent review). In contrast, in the multivariate setting,\ne\ufb00orts to establish Stein factors have focused on Gaussian [4, 37, 79, 9, 66, 68,\n30], Dirichlet [29], and strongly log-concave [63] targets with preconditioned\nLangevin (D2) operators. To extend the reach of the literature, we will derive\nmultivariate Stein factors for targets with fast-coupling It\u02c6o di\ufb00usions. Our\nmeasure of coupling speed is the Wasserstein decay rate.\nDefinition 4 (Wasserstein decay rate).\nLet (Pt)t\u22650 be the transition\nsemigroup of an It\u02c6o di\ufb00usion (Zt,x)t\u22650 de\ufb01ned via\n(Ptf)(x) \u225cE[f(Zt,x)]\nfor all measurable f,\nx \u2208Rd,\nand\nt \u22650.\nFor any non-increasing integrable function r : R\u22650 \u2192R, we say that (Pt)t\u22650\nhas Wasserstein decay rate r if\ndW\u2225\u00b7\u22252(\u03b4xPt, \u03b4yPt) \u2264r(t) dW\u2225\u00b7\u22252(\u03b4x, \u03b4y)\nfor all x, y \u2208Rd and t \u22650,\n(9)\nwhere \u03b4xPt denotes the distribution of Zt,x.\n3The Langevin operator recovers Stein\u2019s density method operator [88] when d = 1.\n8\nJ. GORHAM, A.B. DUNCAN, S.J. VOLLMER, AND L. MACKEY\nOur next result, proved in Section B, shows that the smoothness of a\nsolution gh to a Stein equation is controlled by the rate of Wasserstein de-\ncay and hence by how quickly two di\ufb00usions with distinct starting points\ncouple. The Stein factor bounds on the derivatives of uh and gh may be of\nindependent interest for establishing rates of distributional convergence.\nTheorem 5 (Stein factors from Wasserstein decay).\nFix any Lipschitz\nh. If an It\u02c6o di\ufb00usion has invariant measure P \u2208P1, transition semigroup\n(Pt)t\u22650, Wasserstein decay rate r, and in\ufb01nitesimal generator A (4), then\nuh \u225c\nR \u221e\n0 EP [h(Z)] \u2212Pth dt\n(10)\nis twice continuously di\ufb00erentiable and satis\ufb01es\nM1(uh) \u2264M1(h)\nR \u221e\n0 r(t) dt\nand\nh \u2212EP [h(Z)] = Auh.\nHence, gh \u225c1\n2\u2207uh solves the Stein equation (3) with di\ufb00usion Stein opera-\ntor (8) whenever A has the form (7). If the drift and di\ufb00usion coe\ufb03cients\nb and \u03c3 have locally Lipschitz second derivatives and a right inverse \u03c3\u22121(x)\nfor each x \u2208Rd and h \u2208C2 with bounded second derivatives, then\n(11)\nM2(uh) \u2264M1(h)(\u03b21 + \u03b22),\nwhere\n\u03b21 = r(0)(2M0(\u03c3\u22121) + r(0)M1(\u03c3)M0(\u03c3\u22121) + r(0)\u221a\u03b1),\nand\n\u03b22 = r(0)(e\u03b32M0(\u03c3\u22121) + e\u03b32M1(\u03c3)M0(\u03c3\u22121) + 2\n3e\u03b34\u221a\u03b1)\nR \u221e\n0 r(t) dt\nfor \u03b3\u03c1 \u225c\u03c1M1(b) + \u03c12\u22122\u03c1\n2\nM1(\u03c3)2 + \u03c1\n2F1(\u03c3)2, \u03b1 \u225c\nM2(b)2\n2M1(b)+4M1(\u03c3)2 + 2F2(\u03c3)2.\nIf, additionally, \u22073b and \u22073\u03c3 are locally Lipschitz and h \u2208C3 with bounded\nthird derivatives, then, for all \u03b9 \u2208(0, 1),\nM3\u2212\u03b9(uh) \u2264M1(h) 1\nK\n\u0000 1\n\u03b9 +\nR \u221e\n0 r(t) dt\n\u0001\n(12)\nfor K > 0 a constant depending only on M1:3(\u03c3), M1:3(b), M0(\u03c3\u22121), and r.\nRemark.\nThms. 1 and 2 of Pardoux and Veretennikov [72] also bound\nthe solutions of the Stein equation (3). However, for generic Lipschitz h,\n[72, Thms. 1 and 2] provide inexplicit constants; only guarantee the polyno-\nmial growth of gh and its derivatives, not uniform boundedness; and require\nbounded \u03c3, a strong assumption which rules out the heavy-tailed examples\nof Section 3.\n9\nA \ufb01rst consequence of Theorem 5, proved in Section D, concerns Stein\noperators (8) with constant covariance and stream matrices a and c. In this\nsetting, fast Wasserstein decay implies that the di\ufb00usion Stein discrepancy\nconverges to zero only if the Wasserstein distance does (Desideratum (ii)).\nTheorem 6 (Stein discrepancy lower bound: constant a and c).\nCon-\nsider an It\u02c6o di\ufb00usion with di\ufb00usion Stein operator T (8) for P \u2208P1,\nWasserstein decay rate r, constant covariance and stream matrices a and\nc, and Lipschitz drift b(x) = 1\n2(a + c)\u2207log p(x). If sr \u225c\nR \u221e\n0 r(t) dt, then\ndW\u2225\u00b7\u22252(Qn, P)\n(13)\n\u22643sr max\n\u0010\nS(Qn, T , G\u2225\u00b7\u2225),\n3q\nS(Qn, T , G\u2225\u00b7\u2225)\n\u221a\n2 E[\u2225G\u22252]2(2M1(b) + 1\nsr )2\n\u0011\n,\nwhere G \u2208Rd is a standard normal vector and M1(b) \u22641\n2\u2225a + c\u2225opM2(log p).\nTheorem 6 in fact provides an explicit upper bound on the Wasserstein\ndistance in terms of the Stein discrepancy and the Wasserstein decay rate.\nUnder additional smoothness assumptions on the coe\ufb03cients, the explicit\nrelationship between Stein discrepancy and Wasserstein distance can be im-\nproved and extended to di\ufb00usions with non-constant di\ufb00usion coe\ufb03cient, as\nour next result, proved in Section E, shows.\nTheorem 7 (Stein discrepancy lower bound: non-constant a and c).\nConsider an It\u02c6o di\ufb00usion for P \u2208P1 with di\ufb00usion Stein operator T (8),\nWasserstein decay rate r, and Lipschitz drift and di\ufb00usion coe\ufb03cients b (6)\nand \u03c3 with locally Lipschitz second derivatives. If sr \u225c\nR \u221e\n0 r(t) dt, then\ndW\u2225\u00b7\u22252(Qn, P)\n\u22642 max\n\u0012\nS(Qn, T , G\u2225\u00b7\u2225) max(sr, \u03b21 + \u03b22),\nq\nS(Qn, T , G\u2225\u00b7\u2225)\np\n2/\u03c0(\u03b21 + \u03b22)\u03b6\n\u0013\n,\nfor \u03b21, \u03b22 de\ufb01ned in Theorem 5 and\n\u03b6 \u225cE[\u2225G\u22252](1 + 2M1(b)sr + M\u2217\n1 (m)(\u03b21 + \u03b22))\nwhere G \u2208Rd is a standard normal vector, m \u225ca + c, and M\u2217\n1 (m) \u225c\nsupx\u0338=y \u2225m(x) \u2212m(y)\u2225\u2217\nop/\u2225x \u2212y\u22252.\nIf, additionally, \u22073b and \u22073\u03c3 are locally Lipschitz, then\ndW\u2225\u00b7\u22252(Qn, P) \u22642S(Qn, T , G\u2225\u00b7\u2225) max\n\u0012\nmax(sr, \u03b21 + \u03b22),\ne max\n\u0010\nd1/4\u221a\u03b6\n\u221a\nK ,\n\u221a\nd\nK\n\u0011\n(sr + max(log(1/S(Qn,T ,G\u2225\u00b7\u2225)), 1))\n\u0013\n,\n(14)\n10\nJ. GORHAM, A.B. DUNCAN, S.J. VOLLMER, AND L. MACKEY\nfor a constant K > 0 depending only on M1:3(\u03c3), M1:3(b), M0(\u03c3\u22121), and r.\nRemark.\nThe log(1/S(Qn,T ,G\u2225\u00b7\u2225)) term in (14) re\ufb02ects the potential non-\nsmoothness of the Stein equation solution gh studied in Theorem 5. Indeed,\nfor d \u22652 and standard multivariate Gaussian P, there exist Lipschitz h with\nin\ufb01nite M2(gh) [77, Remark 2].\nIn Section 3, we will present practically checkable conditions implying fast\nWasserstein decay and discuss both broad families and speci\ufb01c di\ufb00usion-\ntarget pairings covered by this theory.\n2.3. Upper bounding the di\ufb00usion Stein discrepancy.\nIn upper bounding\nthe Stein discrepancy, one classically aims to establish rates of convergence to\nP for speci\ufb01c sequences (Qn)\u221e\nn=1. Since our interest is in explicitly computing\nStein discrepancies for arbitrary sample sequences, our general upper bound\nin Proposition 8 serves principally to provide su\ufb03cient conditions under\nwhich the classical di\ufb00usion Stein discrepancy converges to zero.\nProposition 8 (Stein discrepancy upper bound).\nLet T be the di\ufb00usion\nStein operator (8) for P \u2208P1. If m \u225ca + c and b (6) are P-integrable,\nS(Qn, T , G\u2225\u00b7\u2225) \u2264\ninf\nX\u223cQn,Z\u223cP (E[2\u2225b(X) \u2212b(Z)\u2225+ \u2225m(X) \u2212m(Z)\u2225]\n+ E[(2\u2225b(Z)\u2225+ \u2225m(Z)\u2225) min(\u2225X \u2212Z\u2225, 2)])\n\u2264Ws,\u2225\u00b7\u2225(Qn, P)(2M\u2225\u00b7\u2225\n1 (b) + M\u2225\u00b7\u2225\n1 (m))\n+ Ws,\u2225\u00b7\u2225(Qn, P)t 21\u2212t E[(2\u2225b(Z)\u2225+ \u2225m(Z)\u2225)s/(s\u2212t)](s\u2212t)/s\nfor any s \u22651 and t \u2208(0, 1]. Moreover, for \u00b50 \u225cE\n\u0002\ne2\u2225b(Z)\u2225+\u2225m(Z)\u2225\u0003\n,\nS(Qn, T , G\u2225\u00b7\u2225) \u2264W1,\u2225\u00b7\u2225(Qn, P)(2M\u2225\u00b7\u2225\n1 (b) + M\u2225\u00b7\u2225\n1 (m))\n+ min(W1,\u2225\u00b7\u2225(Qn, P), 2) log((e\u00b50)/min(W1,\u2225\u00b7\u2225(Qn, P), 2)).\nThis result, proved in Section F, complements the Wasserstein distance\nlower bounds of Section 2.2 and implies that, for Lipschitz and su\ufb03ciently\nintegrable m and b, the di\ufb00usion Stein discrepancy converges to zero when-\never Qn converges to P in Wasserstein distance.\n2.4. Extension to non-uniform Stein sets.\nFor any c1, c2, c3 > 0, our\nanalyses and algorithms readily accommodate the non-uniform Stein set\nGc1:3\n\u2225\u00b7\u2225\u225c\n\u001a\ng : Rd \u2192Rd\n\f\f\f\f\nsup\nx\u0338=y\u2208Rd max\n\u0010\n\u2225g(x)\u2225\u2217\nc1\n, \u2225\u2207g(x)\u2225\u2217\nc2\n, \u2225\u2207g(x)\u2212\u2207g(y)\u2225\u2217\nc3\u2225x\u2212y\u2225\n\u0011\n\u22641\n\u001b\n.\n11\nThis added \ufb02exibility can be valuable when tight upper bounds on a refer-\nence IPM, like the Wasserstein distance, are available for a particular choice\nof Stein factors (c1, c2, c3). When such Stein factors are unknown or di\ufb03-\ncult to compute, we recommend the parameter-free classical Stein set and\ngraph Stein set of the sequel as practical defaults, since the classical Stein\ndiscrepancy is strongly equivalent to any non-uniform Stein discrepancy:\nProposition 9 (Equivalence of non-uniform Stein discrepancies).\nFor\nany c1, c2, c3 > 0,\nmin(c1, c2, c3)S(Qn, T , G\u2225\u00b7\u2225) \u2264S(Qn, T , Gc1:3\n\u2225\u00b7\u2225) \u2264max(c1, c2, c3)S(Qn, T , G\u2225\u00b7\u2225).\nRemark.\nThe short proof follows exactly as in [35, Prop. 4].\n3. Su\ufb03cient conditions for Wasserstein decay.\nSince the Stein dis-\ncrepancy lower bounds of Section 2 depend on the Wasserstein decay (9) of\nthe chosen di\ufb00usion, we next provide examples of practically checkable su\ufb03-\ncient conditions for Wasserstein decay and illustrate the process of verifying\nthese conditions for a selection of di\ufb00usion-target pairings. These pedagog-\nical examples serve to succinctly illustrate the process of verifying our as-\nsumptions and do not represent the full scope of applicability.\n3.1. Uniform dissipativity.\nIt is well known [see, e.g., 7, Eq. 7] that\nthe Langevin di\ufb00usion (D1) enjoys exponential Wasserstein decay when-\never log p is k-strongly log concave, i.e., when the drift b = 1\n2\u2207log p satis\ufb01es\n\u27e8b(x) \u2212b(y), x \u2212y\u27e9\u2264\u2212k\n2\u2225x \u2212y\u22252\n2 for k > 0. An analogous uniform dissipa-\ntivity condition gives explicit exponential decay for a generic It\u02c6o di\ufb00usion:\nTheorem 10 (Wasserstein decay: uniform dissipativity).\nFix k > 0 and\nG \u227b0, and let \u2225w\u22252\nG \u225c\u27e8w, Gw\u27e9, for any vector or matrix w \u2208Rd\u00d7d\u2032, d\u2032 \u22651.\nAn It\u02c6o di\ufb00usion with drift and di\ufb00usion coe\ufb03cients b and \u03c3 satisfying\n2\u27e8b(x) \u2212b(y), G(x \u2212y)\u27e9+ \u2225\u03c3(x) \u2212\u03c3(y)\u22252\nG \u2264\u2212k\u2225x \u2212y\u22252\nG for all x, y \u2208Rd\nhas Wasserstein decay rate (9) r(t) = e\u2212kt/2p\n\u03bbmax(G)/\u03bbmin(G).\nRemark.\nThe proof of Theorem 10 in Section G holds even when the\ndrift b is not Lipschitz, yields the same decay rate for W2,\u2225\u00b7\u22252, and relies on\na synchronous coupling of It\u02c6o di\ufb00usions, mimicking [7, Sec. 1].\nHence, if the drift b of an It\u02c6o di\ufb00usion is \u2212k/2-one-sided Lipschitz, i.e.,\n2\u27e8b(x) \u2212b(y), G(x \u2212y)\u27e9\u2264\u2212k\u2225x \u2212y\u22252\nG\nfor all\nx, y \u2208Rd\n(15)\n12\nJ. GORHAM, A.B. DUNCAN, S.J. VOLLMER, AND L. MACKEY\nand some G \u227b0, and the di\ufb00usion coe\ufb03cient \u03c3 is\n\u221a\nk\u2032-Lipschitz, that is,\n\u2225\u03c3(x) \u2212\u03c3(y)\u22252\nG \u2264k\u2032\u2225x \u2212y\u22252\nG\nfor all\nx, y \u2208Rd,\nthen, whenever k\u2032 < k, the di\ufb00usion exhibits exponential Wasserstein decay.\nwith rate e\u2212(k\u2212k\u2032)t/2p\n\u03bbmax(G)/\u03bbmin(G).\nExample 1 (Bayesian logistic regression with Gaussian prior).\nA one-\nsided Lipschitz drift arises naturally in the setting of Bayesian logistic regres-\nsion [32], a canonical model of binary outcomes y \u2208{\u22121, 1} given measured\ncovariates v \u2208Rd. Consider the log density of a Bayesian logistic regression\nposterior based on a dataset of L observations (vl, yl) and a N(\u00b5, \u03a3) prior:\nlog p(\u03b2) = \u22121\n2\u2225\u03a3\u22121/2(\u03b2 \u2212\u00b5)\u22252\n2\n|\n{z\n}\nmultivariate Gaussian prior\n\u2212PL\nl=1 log(1 + exp(\u2212yl\u27e8vl, \u03b2\u27e9))\n|\n{z\n}\nlogistic regression likelihood\n+ const.\nHere, our inferential target is the unobserved parameter vector \u03b2 \u2208Rd. Since\n\u2212\u03a3\u22121 \u227d\u22072 log p(\u03b2) = \u2212\u03a3\u22121\u2212PL\nl=1\neyl\u27e8vl,\u03b2\u27e9\n(1+eyl\u27e8vl,\u03b2\u27e9)2 vlv\u22a4\nl \u227d\u2212\u03a3\u22121\u22121\n4\nPL\nl=1 vlv\u22a4\nl ,\nthe P-targeted preconditioned Langevin di\ufb00usion (D2) drift b(\u03b2) = 1\n2\u03a3\u2207log p(\u03b2)\nsatis\ufb01es (15) with k = 1 and G = \u03a3\u22121 and M1(b) \u22641\n2\u2225I + 1\n4\u03a3 PL\nl=1 vlv\u22a4\nl \u2225op.\nHence, the di\ufb00usion enjoys geometric Wasserstein decay (Theorem 10) and\na Wasserstein lower bound on the Stein discrepancy (Theorem 6).\nExample 2 (Bayesian Huber regression with Gaussian prior).\nHuber\u2019s\nleast favorable distribution provides a robust error model for the regression of\na continuous response y \u2208R onto a vector of measured covariates v \u2208Rd [44].\nGiven L observations (vl, yl) and a N(\u00b5, \u03a3) prior on an unknown parameter\nvector \u03b2 \u2208Rd, the Bayesian Huber regression log posterior takes the form\nlog p(\u03b2) = \u22121\n2\u2225\u03a3\u22121/2(\u03b2 \u2212\u00b5)\u22252\n2\n|\n{z\n}\nmultivariate Gaussian prior\n\u2212PL\nl=1 \u03c1c(yl \u2212\u27e8vl, \u03b2\u27e9)\n|\n{z\n}\nHuber\u2019s least favorable likelihood\n+ const.\nwhere \u03c1c(r) \u225c\n1\n2r2I[|r| \u2264c] + c(|r| \u22121\n2c)I[|r| > c] for \ufb01xed c > 0. Since\n\u03c1\u2032\nc(r) = min(max(r, \u2212c), c) is 1-Lipschitz and convex, and the Hessian of\nthe log prior is \u2212\u03a3\u22121, the P-targeted preconditioned Langevin di\ufb00usion\n(D2) drift b(\u03b2) =\n1\n2\u03a3\u2207log p(\u03b2) satis\ufb01es (15) with k = 1 and G = \u03a3\u22121\nand M1(b) \u22641\n2\u2225I + \u03a3 PL\nl=1 vlv\u22a4\nl \u2225op. This is again su\ufb03cient for exponential\nWasserstein decay and a Wasserstein lower bound on the Stein discrepancy.\n13\n3.2. Distant dissipativity, constant \u03c3.\nWhen the di\ufb00usion coe\ufb03cient \u03c3 is\nconstant with a \u225c1\n2\u03c3\u03c3\u22a4invertible, Eberle [22] showed that a distant dissipa-\ntivity condition is su\ufb03cient for exponential Wasserstein decay. Speci\ufb01cally,\nif we de\ufb01ne a one-sided Lipschitz constant conditioned on a distance r > 0,\n\u2212\u03ba(r) = sup{2(b(x) \u2212b(y))\u22a4a\u22121(x \u2212y)/r2 : (x \u2212y)\u22a4a\u22121(x \u2212y) = r2},\nthen [22, Cor. 2] establishes exponential Wasserstein decay whenever \u03ba is\ncontinuous with lim infr\u2192\u221e\u03ba(r) > 0 and\nR 1\n0 r\u03ba(r)\u2212dr < \u221e. For a Lipschitz\ndrift, this holds whenever b is dissipative at large distances, that is, whenever,\nfor some k > 0, we have \u03ba(r) \u2265k for all r su\ufb03ciently large [22, Lem. 1].\nExample 3 (Gaussian mixture with common covariance).\nConsider an\nm-component mixture density p(x) = Pm\nj=1 wj\u03c6j(x), where the component\nweights wj \u22650 sum to one and \u03c6j is the density of a N(\u00b5j, \u03a3) distribution on\nRd. Fix any x, y \u2208Rd. If \u2225\u03a3\u22121/2(x \u2212y)\u22252 = r, the P-targeted preconditioned\nLangevin di\ufb00usion (D2) with drift b(z) = 1\n2a\u2207log p(z) and a = \u03a3 satis\ufb01es\n2(b(x) \u2212b(y))\u22a4a\u22121(x \u2212y) = (\u2207log p(x) \u2212\u2207log p(y))\u22a4(x \u2212y)\n= \u2212r2 + \u27e8\u03a3\u22121/2(\u00b5(x) \u2212\u00b5(y)), \u03a3\u22121/2(x \u2212y)\u27e9\u2264\u2212r2 + r\u2206,\nby Cauchy-Schwarz and Jensen\u2019s inequality, for \u2206\u225csupj,k \u2225\u03a3\u22121/2(\u00b5j \u2212\u00b5k)\u22252,\n\u00b5(x) \u225cPm\nj=1 \u03c0j(x)\u00b5j, and \u03c0j(x) \u225cwj\u03c6j(x)\np(x) . Moreover, by the mean value\ntheorem, Cauchy-Schwarz, and Jensen\u2019s inequality, we have, for each v \u2208Rd,\n2\u27e8\u03a3\u22121/2(b(x) \u2212b(y)), v\u27e9= \u27e8\u03a3\u22121/2(\u2207\u00b5(z) \u2212I)(x \u2212y), v\u27e9\n= \u27e8(\u03a3\u22121/2S(z)\u03a3\u22121/2 \u2212I)\u03a3\u22121/2(x \u2212y), v\u27e9\u2264\u2225v\u22252\u2225\u03a3\u22121/2(x \u2212y)\u22252 L,\nfor some z \u2208Rd, S(x) \u225c1\n2\nPm\nj=1\nPm\nk=1 \u03c0j(x)\u03c0k(x)(\u00b5j \u2212\u00b5k)(\u00b5j \u2212\u00b5k)\u22a4, and\nL \u225csupj,k |1 \u2212\u2225\u03a3\u22121/2(\u00b5j \u2212\u00b5k)\u22252\n2/2|. Hence, b is Lipschitz, and \u03ba(r) \u22651\n2\nwhen r > 2\u2206, so our di\ufb00usion enjoys exponential Wasserstein decay [22,\nLem. 1] and a Stein discrepancy upper bound on the Wasserstein distance.\n3.3. Distant dissipativity, non-constant \u03c3.\nUsing a combination of syn-\nchronous and re\ufb02ection couplings, Wang [93, Thm. 2.6] showed that di\ufb00u-\nsions satisfying a distant dissipativity condition exhibit exponential Wasser-\nstein decay, even when the di\ufb00usion coe\ufb03cient \u03c3 is non-constant. In Sec-\ntion H, we combine the coupling strategy of [93, Thm. 2.6] with the ap-\nproach of [22] for di\ufb00usions with constant \u03c3 to obtain the following explicit\nWasserstein decay rate for distantly dissipative di\ufb00usions with bounded \u03c3\u22121.\n14\nJ. GORHAM, A.B. DUNCAN, S.J. VOLLMER, AND L. MACKEY\nTheorem 11 (Wasserstein decay: distant dissipativity).\nLet (Pt)t\u22650 be\nthe transition semigroup of an It\u02c6o di\ufb00usion with drift and di\ufb00usion coe\ufb03-\ncients b and \u03c3. De\ufb01ne the truncated di\ufb00usion coe\ufb03cient\n\u03c30(x) = (\u03c3(x)\u03c3(x)\u22a4\u2212\u03bb2\n0I)1/2\nfor some\n\u03bb0 \u2208[0, 1/M0(\u03c3\u22121)]\nand the distance-conditional dissipativity function\n\u03ba(r) = inf{ \u22122\u03b1(\u27e8b(x) \u2212b(y), x \u2212y\u27e9+ 1\n2\u2225\u03c30(x) \u2212\u03c30(y)\u22252\nF\n(16)\n\u22121\n2\u2225(\u03c30(x) \u2212\u03c30(y))\u22a4(x \u2212y)\u22252\n2/r2)/r2 : \u2225x \u2212y\u22252 = r}\nfor any m0 \u2264infx\u0338=y\n\u2225(\u03c30(x)\u2212\u03c30(y))\u22a4(x\u2212y)\u22252\n\u2225x\u2212y\u22252\nand\n\u03b1 \u225c1/(\u03bb2\n0 + m2\n0/4).\nIf the constants R0 = inf{R \u22650 : \u03ba(r) \u22650, \u2200r \u2265R} and R1 = inf{R \u2265\nR0 : \u03ba(r)R(R \u2212R0) \u22658, \u2200r \u2265R} satisfy R0 \u2264R1 < \u221e, then\ndW\u2225\u00b7\u22252(\u03b4xPt, \u03b4yPt) \u22642\u03d5(R0)\u22121e\u2212ct dW\u2225\u00b7\u22252(\u03b4x, \u03b4y)\n(17)\nfor all x, y \u2208Rd and t \u22650, where 1\nc = \u03b1\nR R1\n0\nR s\n0 exp( 1\n4\nR s\nt u\u03ba\u2212(u) du) dt ds,\n\u03d5(r) = e\u22121\n4\nR r\n0 s\u03ba\u2212(s) ds, and \u03ba\u2212(s) = max(\u2212\u03ba(s), 0).\nRemark.\nTheorem 11 holds even when the drift b is not Lipschitz.\nThe Wasserstein decay rate (17) in Theorem 11 has a simple form when\nthe di\ufb00usion is dissipative at large distances and \u03ba is bounded below. These\nrates follow exactly as in [22, Lem. 1].\nCorollary 12.\nUnder the conditions of Theorem 11, suppose that, for\nR, L \u22650 and K > 0, \u03ba(r) \u2265\u2212L for r \u2264R and \u03ba(r) \u2265K for r > R. Then\n\u03b1\u22121c\u22121 \u2264\n( e\u22121\n2 R2 + e\n\u221a\n8K\u22121 R + 4K\u22121\nif LR2\n0 \u22648\n8\n\u221a\n2\u03c0\nRL1/2 (L\u22121 + K\u22121) exp\n\u0010\nLR2\n8\n\u0011\n+ 32R\u22122K\u22122\nif LR2\n0 > 8.\nExample 4 (Multivariate Student\u2019s t regression with pseudo-Huber prior).\nThe multivariate Student\u2019s t distribution is also commonly employed as a\nrobust error model for the linear regression of continuous responses y \u2208RL\nonto measured covariates V \u2208RL\u00d7d [94, 56]. Under a pseudo-Huber prior\n[42], a Bayesian multivariate Student\u2019s t regression posterior takes the form\np(\u03b2) \u221dexp(\u03b42(1 \u2212\np\n1 + \u2225\u03b2/\u03b4\u22252\n2))\n|\n{z\n}\npseudo-Huber prior\n(1 + 1\n\u03bd (y \u2212V \u03b2)\u22a4\u03a3\u22121(y \u2212V \u03b2))\u2212(\u03bd+L)/2\n|\n{z\n}\nmultivariate Student\u2019s t likelihood\n15\nfor \ufb01xed \u03b4, \u03bd > 0 and \u03a3 \u227b0. Introduce the shorthand \u03c8\u03bb(r) \u225c2\np\n1 + r2/\u03b42\u2212\n\u03bb2 for each \u03bb \u2208[0,\n\u221a\n2) and \u03be(\u03b2) \u225c1 + 1\n\u03bd (y \u2212V \u03b2)\u22a4\u03a3\u22121(y \u2212V \u03b2). Since\n\u2207log p(\u03b2) = \u22122\u03b2/\u03c80(\u2225\u03b2\u22252) + (1 + \u03bd\nL)V \u22a4\u03a3\u22121(y \u2212V \u03b2)/\u03be(\u03b2)\nis bounded, no P-targeted preconditioned Langevin di\ufb00usion (D2) will sat-\nisfy the distant dissipativity conditions of Section 3.2. However, we will\nshow that whenever V \u22a4V \u227b0, the Riemannian Langevin di\ufb00usion (D3) with\n\u03c3(\u03b2) =\np\n\u03c80(\u2225\u03b2\u22252)I \u2208Rd\u00d7d, a(\u03b2) = 1\n2\u03c80(\u2225\u03b2\u22252)I, and b(\u03b2) = a(\u03b2)\u2207log p(\u03b2)+\n\u27e8\u2207, a(\u03b2)\u27e9satis\ufb01es the Wasserstein decay preconditions of Corollary 12.\nIndeed, \ufb01x any \u03bb0 \u2208(0, 1/M0(\u03c3\u22121)) = (0,\n\u221a\n2). Since M1(\u221a\u03c8\u03bb) \u2264\n1\n\u03b4\n\u221a\n2\u2212\u03bb2 ,\nM1(\u03c8\u03bb) \u22642\n\u03b4, and M2(\u03c8\u03bb) \u22642\n\u03b42 , \u03c30, \u03c3, a, and \u2207a are all Lipschitz. The drift\nb is also Lipschitz, since \u2207log p and the product of a(\u03b2) and\n\u22072 log p(\u03b2) = \u22122I/\u03c80(\u2225\u03b2\u22252) + 8\u03b2\u03b2\u22a4/(\u03b42\u03c83\n0(\u2225\u03b2\u22252))\n+\n\u00001 + \u03bd\nL\n\u0001\n(2V \u22a4\u03a3\u22121(y \u2212V \u03b2)(y \u2212V \u03b2)\u22a4\u03a3\u22121V /\u03be2(\u03b2) \u2212V \u22a4\u03a3\u22121V /\u03be(\u03b2)).\nare bounded. Hence, \u03ba (16) is bounded below. Moreover, the the H\u00a8older\ncontinuity of x 7\u2192\u221ax, Cauchy-Schwarz, and the triangle inequality imply\n\u03ba(r) \u2265\ninf\n\u2225\u03b2\u2212\u03b2\u2032\u22252=r\n2\u03b1\nr2 (\u27e8b(\u03b2\u2032) \u2212b(\u03b2), \u03b2 \u2212\u03b2\u2032\u27e9\u2212d\u22121\n2 |\np\n\u03c8\u03bb0(\u2225\u03b2\u22252) \u2212\np\n\u03c8\u03bb0(\u2225\u03b2\u2032\u22252)|2)\n\u22652\u03b1 \u22122\u03b1\nr ( d\u22121\n\u03b4\n+ M1(\u03c80) + sup\u03b2 (1 + \u03bd\nL)\u03c80(\u2225\u03b2\u22252)\u2225V \u22a4\u03a3\u22121(y \u2212V \u03b2)\u22252/\u03be(\u03b2))\n\u22652\u03b1 \u22122\u03b1\nr\n\u0010\nd+1\n\u03b4\n+ sups (1 + \u03bd\nL)\n2(1+s/\u03b4)(\u2225V \u22a4\u03a3\u22121y\u22252+s\u2225V \u22a4\u03a3\u22121V \u2225op)\n1+ 1\n\u03bd max(0,s/\u2225(V \u22a4\u03a3\u22121V )\u22121\u2225op\u2212\u2225\u03a3\u22121y\u22252)2\n\u0011\n.\nLetting \u03b6 represent the supremum in the \ufb01nal inequality, we see that \u03ba(r) \u2265\n\u03b1 = 1/\u03bb2\n0 whenever r \u22652( d+1\n\u03b4 +\u03b6). Hence, Corollary 12 delivers exponential\nWasserstein decay. A Wasserstein lower bound on the Stein discrepancy\nnow follows from Theorem 7, since M2(\u221a\u03c80) \u2264\n1\n\u221a\n2\u03b42 , M3(\u03c80) \u2264\n96\n25\n\u221a\n5\u03b43 , and\na(\u03b2)\u22072 log p(\u03b2) is Lipschitz, and hence M2(\u03c3) and M2(b) are bounded.\n4. Computing Stein discrepancies.\nIn this section, we introduce a\ncomputationally tractable Stein discrepancy that inherits the favorable con-\nvergence properties established in Sections 2 and 3. We will directly port\nthe spanner discrepancy methodology developed and detailed in [35] and\nuse our new di\ufb00usion operators as drop-in replacements for the overdamped\nLangevin operators advocated in [35]. While we only explicitly discuss tar-\nget distributions supported on all of Rd, constrained domains of the form\n(\u03b11, \u03b21) \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 (\u03b1d, \u03b2d) where \u2212\u221e\u2264\u03b1i < \u03b2i \u2264\u221efor all 1 \u2264i \u2264d can be\nhandled by introducing boundary constraints as in [35, Section 4.4].\n16\nJ. GORHAM, A.B. DUNCAN, S.J. VOLLMER, AND L. MACKEY\n4.1. Spanner Stein discrepancies.\nFor any sample Qn, Stein operator T ,\nand Stein set G, the Stein discrepancy S(Qn, T , G) is recovered by solving\nan optimization problem over functions g \u2208G. For example, if we write\nm \u225ca + c and b(x) \u225c\n1\n2\n1\np(x)\u27e8\u2207, p(x)m(x)\u27e9, the classical di\ufb00usion Stein\ndiscrepancy is the value\nS(Qn, T , G\u2225\u00b7\u2225) = supg\nPn\ni=1q(xi)(2\u27e8b(xi), g(xi)\u27e9+ \u27e8m(xi), \u2207g(xi)\u27e9)\ns.t. max(\u2225g(x)\u2225\u2217, \u2225\u2207g(x)\u2225\u2217, \u2225\u2207g(x)\u2212\u2207g(y)\u2225\u2217\n\u2225x\u2212y\u2225\n) \u22641, \u2200x, y \u2208Rd.\nFor all Stein sets, the di\ufb00usion Stein discrepancy objective is linear in g and\nonly queries g and \u2207g at the n sample points underlying Qn. However, the\nclassical Stein set G\u2225\u00b7\u2225constrains g at all points in its domain, resulting in\nan in\ufb01nite-dimensional optimization problem.4\nTo obtain a \ufb01nite-dimensional problem that is convergence-determining\nand straightforward to optimize, we will make use of the graph Stein sets of\n[35]. For a given graph G = (V, E) with V = supp(Qn), the graph Stein set,\nG\u2225\u00b7\u2225,Qn,G =\nn\ng : max(\u2225g(v)\u2225\u2217, \u2225\u2207g(v)\u2225\u2217, \u2225g(x)\u2212g(y)\u2225\u2217\n\u2225x\u2212y\u2225\n, \u2225\u2207g(x)\u2212\u2207g(y)\u2225\u2217\n\u2225x\u2212y\u2225\n) \u22641,\n\u2225g(x)\u2212g(y)\u2212\u2207g(x)(x\u2212y)\u2225\u2217\n1\n2 \u2225x\u2212y\u22252\n\u22641, \u2225g(x)\u2212g(y)\u2212\u2207g(y)(x\u2212y)\u2225\u2217\n1\n2 \u2225x\u2212y\u22252\n\u22641, \u2200(x, y) \u2208E, v \u2208V\n\u001b\n,\nimposes boundedness constraints only at sample points and smoothness con-\nstraints only at pairs of sample points enumerated in the edge set E. The\ngraph is termed a t-spanner [14, 75] if each edge (x, y) \u2208E is assigned\nthe weight \u2225x \u2212y\u2225, and, for all x\u2032 \u0338= y\u2032 \u2208V , there exists a path between\nx\u2032 and y\u2032 in the graph with total path weight no greater than t\u2225x\u2032 \u2212y\u2032\u2225.\nRemarkably, for any linear Stein operator T , a spanner Stein discrepancy\nS(Qn, T , G\u2225\u00b7\u2225,Qn,Gt) based on a t-spanner Gt is equivalent to the classical\nStein discrepancy in the following strong sense, implying Desiderata (i) and\n(ii).\nProposition 13 (Equivalence of classical and spanner Stein discrepan-\ncies).\nIf Gt = (supp(Qn), E) is a t-spanner for t \u22651, then\nS(Qn, T , G\u2225\u00b7\u2225) \u2264S(Qn, T , G\u2225\u00b7\u2225,Qn,Gt) \u2264\u03badt2 S(Qn, T , G\u2225\u00b7\u2225)\nwhere \u03bad is independent of (Qn, P, T , Gt) and depends only on d and \u2225\u00b7\u2225.\nRemark.\nThe proof relies on the Whitney-Glaeser extension theorem\n[85, Thm. 1.4] of Glaeser [34] and follows exactly as in [35, Prop. 5 and 6].\n4When d = 1, the problem reduces to a \ufb01nite-dimensional convex quadratically con-\nstrained quadratic program with linear objective as in [35, Thm. 9].\n17\nAlgorithm 1 Spanner di\ufb00usion Stein discrepancy, S(Qn, T , G\u2225\u00b7\u22251,Qn,G2)\ninput: sample Qn, target score \u2207log p, covariance coe\ufb03cient a, stream coe\ufb03cient c\nG2 \u21902-spanner of V = supp(Qn)\nfor j = 1 to d do (in parallel)\n\u03c4j \u2190Optimal value of j-th coordinate linear program (18) with graph G2\nreturn Pd\nj=1 \u03c4j\nWhen d = 1, a t-spanner with exactly n\u22121 edges is obtained in O(n log n)\ntime for all t \u22651 by introducing edges just between sample points that\nare adjacent in sorted order. More generally, if \u2225\u00b7\u2225is an \u2113p norm, one can\nconstruct a 2-spanner with O(\u03ba\u2032\ndn) edges in O(\u03ba\u2032\ndn log(n)) expected time\nwhere \u03ba\u2032\nd is a constant that depends only on the norm \u2225\u00b7\u2225and the dimension\nd [41]. Hence, a spanner Stein discrepancy can be computed by solving a\n\ufb01nite-dimensional convex optimization problem with a linear objective, O(n)\nvariables, and O(\u03ba\u2032\ndn) convex constraints, making it an appealing choice for\na computable quality measure (Desideratum (iii)).\n4.2. Decoupled linear programs.\nMoreover, if we choose the norm \u2225\u00b7\u2225=\n\u2225\u00b7\u22251, the graph Stein discrepancy optimization problem decouples into d\nindependent linear programs (LPs) that can be solved in parallel using o\ufb00-\nthe-shelf solvers. Indeed, for any G = (supp(Qn), E), S(Qn, T , G\u2225\u00b7\u22251,Qn,G)\nequals\nPd\nj=1\nsup\n\u03c8j\u2208Rn,\u03a8j\u2208Rd\u00d7n\nPn\ni=1q(xi)(2bj(xi)\u03c8ji + Pd\nk=1mjk(xi)\u03a8jki)\n(18)\ns.t. \u2225\u03c8j\u2225\u221e\u22641, \u2225\u03a8j\u2225\u221e\u22641, and for all i \u0338= l, (xi, xl) \u2208E\nmax\n\u0000 |\u03c8ji\u2212\u03c8jl|\n\u2225xi\u2212xl\u22251 , \u2225\u03a8j(ei\u2212ek)\u2225\u221e\n\u2225xi\u2212xl\u22251\n, |\u03c8ji\u2212\u03c8jl\u2212\u27e8\u03a8jei,xi\u2212xl\u27e9|\n1\n2 \u2225xi\u2212xl\u22252\n1\n, |\u03c8ji\u2212\u03c8jl\u2212\u27e8\u03a8jei,xl\u2212xi\u27e9|\n1\n2 \u2225xi\u2212xl\u22252\n1\n\u0001\n\u22641,\nwhere \u03c8ji and \u03a8jki represent the values gj(xi) and \u2207kgj(xi) respectively.\nTherefore, our recommended quality measure is the 2-spanner di\ufb00usion Stein\ndiscrepancy with \u2225\u00b7\u2225= \u2225\u00b7\u22251. Its computation is summarized in Algorithm 1.\nAn e\ufb03cient implementation of Algorithm 1, integrated with 11 linear pro-\ngram solver options, is publicly available via our Julia package.5\n5. Numerical illustrations.\nIn this section, we complement the prin-\ncipal theoretical contributions of this work with several simple numerical il-\nlustrations demonstrating how di\ufb00usion Stein discrepancies can be deployed\nin practice. We will use our proposed quality measures to select hyperpa-\nrameters for biased samplers, to quantify a bias-variance trade-o\ufb00for ap-\nproximate MCMC, and to compare deterministic and random quadrature\n5https://jgorham.github.io/SteinDiscrepancy.jl/\n18\nJ. GORHAM, A.B. DUNCAN, S.J. VOLLMER, AND L. MACKEY\n\u2206= 1\n\u2206= 3\n\u2206= 5\nG\nG G G\nG\nG\nG\nGGG\nG\nG G\nGG\nG\nGGG\nG\nG\nGGGGGG\nG\nG\nG\nG\nG G\nGGGGG\nGG\nG G\nG\nGGGGGG\nG G\nGGGGGGG\nG G\nG\nG GGGG\nGG\nGG\nG\nG\nGG\nGGGGG\nG G\nGG\nGGGGG\nG\nG\nGG\nG\n0.010\n0.032\n0.100\n0.316\n1.000\n101\n102\n103\n104\n101\n102\n103\n104\n101\n102\n103\n104\n105\nNumber of sample points, n\nLangevin spanner\nStein discrepancy\nSample\nG i.i.d. from P\ni.i.d. from single mode\nFig 1: Stein discrepancy for normal mixture target P with \u2206mode separation (Section 5.1).\nrules. In each case, we choose experimental settings in which a notion of sur-\nrogate ground truth is available for external validation. We solve all linear\nprograms using Julia for Mathematical Programming [60] with the Gurobi\n6.0.4 solver [71] and use the C++ greedy spanner implementation of Bouts\net al. [5] to compute our 2-spanners. Our timings were obtained on a single\ncore of an Intel Xeon CPU E5-2650 v2 @ 2.60GHz. Code reconstructing all\nexperiments is available on the Julia package site.5\n5.1. A simple example.\nWe \ufb01rst present a simple example to illustrate\nseveral Stein discrepancy properties. For a Gaussian mixture target P (Ex-\nample 3) with p(x) \u221de\u22121\n2 (x\u2212\u2206\n2 )2 + e\u22121\n2 (x+ \u2206\n2 )2 and \u2206> 0, we simulate one\ni.i.d. sequence of sample points from P and a second i.i.d. sequence from\nN(\u2212\u2206\n2 , 1), which represents only one component of P. For various mode\nseparations \u2206, Figure 1 shows that the Langevin spanner Stein discrepancy\n(D1) applied to the \ufb01rst n Gaussian mixture sample points decreases to zero\nat a n\u22121/2 rate, while the discrepancy applied to the single mode sequence\nstays bounded away from zero. However, Figure 1 also indicates that larger\nsample sizes are needed to distinguish between the mixture and single mode\nsample sequences when \u2206is large. This accords with our theory (see Exam-\nple 3, Corollary 12, and Theorem 6), which implies that both the Langevin\ndi\ufb00usion Wasserstein decay rate and the bound relating Stein to Wasserstein\ndegrade as the mixture mode separation \u2206increases.\n5.2. Selecting sampler hyperparameters.\nStochastic Gradient Riemannian\nLangevin Dynamics (SGRLD) [73] with a constant step size \u03f5 is an approx-\nimate MCMC procedure designed to accelerate posterior inference. Unlike\nasymptotically correct MCMC algorithms, SGRLD has a stationary distri-\nbution that deviates increasingly from its target P as its step size \u03f5 grows.\nOn the other hand, if \u03f5 is too small, SGRLD fails to explore the sample\nspace su\ufb03ciently quickly. Hence, an appropriate setting of \u03f5 is paramount\nfor accurate inference.\nTo demonstrate the value of di\ufb00usion Stein discrepancies for hyperpa-\n19\nG\nG\nG\nG\nG\nG\nESS\nRiemannian Stein discrepancy\nSurrogate ground truth\n2\n4\n6\n6.6\n7.0\n7.4\n\u22122\n\u22121\n0\n1\n10\u22127\n10\u22126\n10\u22125\n10\u22124\n10\u22123\n10\u22122\n10\u22127\n10\u22126\n10\u22125\n10\u22124\n10\u22123\n10\u22122\n10\u22127\n10\u22126\n10\u22125\n10\u22124\n10\u22123\n10\u22122\nStep size, \u03b5\nLog diagnostic\n(a) Step size selection criteria and surrogate ground truth (median marginal Wasserstein).\nESS maximized at \u03f5 = 10\u22122. Stein discrepancy and ground truth minimized at \u03f5 = 10\u22124.\nx = \u03b21\ny = \u03b22\nx = \u03b21\ny = \u03b23\nx = \u03b21\ny = \u03b24\nx = \u03b22\ny = \u03b23\nx = \u03b22\ny = \u03b24\nx = \u03b23\ny = \u03b24\n0\n4\n8\n0\n4\n8\n0\n4\n8\n0\n4\n8\n\u03b5 = 10\u22122\n\u03b5 = 10\u22124\n\u03b5 = 10\u22127\nMARLA\nSGRLD\nSGRLD\nSGRLD\n\u22122\n0\n2\n\u22122\n0\n2\n\u22122\n0\n2\n4\n6\n8 10\n4\n6\n8\n10\n\u22122\n0\n2\nx\ny\nDensity\n0.004\n0.008\n0.012\n0.016\n(b) Bivariate hexbin plots. Top row: surrogate ground truth sample (2\u00d7108 MARLA points).\nBottom 3 rows: 2, 000 SGRLD sample points for various step sizes \u03f5.\nFig 2: Step size selection, stochastic gradient Riemannian Langevin dynamics (Section 5.2).\nrameter selection, we analyzed a biometric dataset of L = 202 athletes\nfrom the Australian Institute of Sport that was previously the focus of a\nheavy-tailed regression analysis [86]. In the notation of Example 4, we used\nSGRLD to conduct a Bayesian multivariate Student\u2019s t regression (\u03bd = 10,\n\u03a3 = I) of athlete lean body mass onto red blood count, white blood count,\nplasma ferritin concentration, and a constant regressor of value 1/\n\u221a\nL with\na pseudo-Huber prior (\u03b4 = 0.1) on the unknown parameter vector \u03b2 \u2208R4.\nAfter standardizing the output variable and non-constant regressors and\ninitializing each chain with an approximate posterior mode found by L-\nBFGS started at the origin, we ran SGRLD with minibatch size 30, metric\nG(\u03b2) = 1/(2\np\n1 + \u2225\u03b2/\u03b4\u22252\n2)I, and a variety of step sizes \u03f5 to produce sam-\nple sequences of length 200, 000 thinned to length 2, 000. We then selected\nthe step size that delivered the highest quality sample \u2013 either the maxi-\nmum e\ufb00ective sample size (ESS, a popular MCMC mixing diagnostic based\n20\nJ. GORHAM, A.B. DUNCAN, S.J. VOLLMER, AND L. MACKEY\non asymptotic variance [6]) or the minimum Riemannian Langevin spanner\nStein discrepancy with a(\u03b2) = G\u22121(\u03b2). The longest discrepancy computa-\ntion consumed 6s for spanner construction and 65s to solve a coordinate\noptimization problem. As a surrogate measure of ground truth, we also gen-\nerated a sample Q\u2217of size 2\u00d7108 from the Metropolis-adjusted Riemannian\nLangevin Algorithm (MARLA) [33] with metric G and compute the median\nbivariate marginal Wasserstein distance dW\u2225\u00b7\u22251 between each SGRLD sample\nand Q\u2217thinned to 5, 000 points [39].\nFigure 2a shows that ESS, which does not account for stationary dis-\ntribution bias, selects the largest step size available, \u03f5 = 10\u22122. As seen in\nFigure 2b, this choice results in samples that are greatly overdispersed when\ncompared with the ground truth MARLA sample Q\u2217. At the other extreme,\nthe selection \u03f5 = 10\u22127 produces greatly underdispersed samples due to slow\nmixing. The Stein discrepancy chooses an intermediate value, \u03f5 = 10\u22124. The\nsame value minimizes the surrogate ground truth Wasserstein measure and\nproduces samples that most closely resemble the Q\u2217in Figure 2b.\n5.3. Quantifying a bias-variance trade-o\ufb00.\nApproximate random walk\nMetropolis-Hastings (ARWMH) [52] with tolerance parameter \u03f5 is a biased\nMCMC procedure that accelerates posterior inference by approximating the\nstandard MH correction. Qualitatively, a smaller setting of \u03f5 produces a\nmore faithful approximation of the MH correction and less bias between\nthe chain\u2019s stationary distribution and the target distribution of interest.\nA larger setting of \u03f5 leads to faster sampling and a more rapid reduction\nof Monte Carlo variance, as fewer datapoint likelihoods are computed per\nsampling step. We will quantify this bias-variance trade-o\ufb00as a function of\nsampling time using the Langevin spanner Stein discrepancy.\nIn the notation of Example 2, we conduct a Bayesian Huber regression\nanalysis (c = 1) of the log radon levels in 1, 190 Minnesota households [31]\nas a function of the log amount of uranium in the county, an indicator of\nwhether the radon reading was performed in a basement, and an intercept\nterm. A N(0, I) prior is placed on the coe\ufb03cient vector \u03b2. We run ARWMH\nwith minibatch size 5 and two settings of the tolerance threshold \u03f5 (0.1 and\n0.2) for 107 likelihood evaluations, discard the sample points from the \ufb01rst\n105 evaluations, and thin the remaining points to sequences of length 1, 000.\nFigure 3 displays the Langevin spanner Stein discrepancy applied to the \ufb01rst\nn points in each sequence as a function of the likelihood evaluation count,\nwhich serves as a proxy for sampling time. As expected, the higher tolerance\nsample (\u03f5 = 0.2) is of higher Stein quality for a small computational budget\nbut is eventually overtaken by the \u03f5 = 0.1 sample with smaller asymptotic\n21\nG\nG G\nG\nG\nGG\nG\nG\nG\nGGG\nGGGG\nG\nGGGGGGGG\nGGGGGG\nGGGG\nGGGGGGG\nGGG\nGGGGG\nG\nGGGGG\nG\nG\nG\nG\nG\nG\nGG\nGG\nG\nGG\nG\nGGG\nG\nG\nGGGG\nGGGG\nGGGGGGGG\nGGGGGGGG\nG\nGGGGGGGGGGGGGGG\nG\nG G\nG\nG\nG\nG\nGGG\nGGG\nG\nG\nGG\nG\nGG\nGGG\nGGGGG\nGGG\nGG\nGGGGGGGG\nGGGGGGGGGGGGGGGG\nG\nG\nG\nG G\nG\nG\nG\nG\nGGGGG\nG\nG\nGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGG\nLangevin Stein discrepancy\nNormalized predictive error\nMean error\nSecond moment error\n10\n15\n20\n25\n0.025\n0.050\n0.075\n0.100\n0.03\n0.06\n0.09\n0.1\n0.2\n0.3\n3e+05 1e+06 3e+06 1e+07\n3e+05 1e+06 3e+06 1e+07\n3e+05 1e+06 3e+06 1e+07\n3e+05 1e+06 3e+06 1e+07\nNumber of likelihood evaluations\nDiscrepancy\nHyperparameter\nG\n\u03b5 = 0.1\n\u03b5 = 0.2\nFig 3: Bias-variance trade-o\ufb00curves for approximate random walk MH (Section 5.3).\nbias. The longest discrepancy computation consumed 0.8s for the spanner\nand 20.1s for a coordinate LP.\nTo provide external support for the Stein discrepancy quanti\ufb01cation, we\ngenerate a Metropolis-adjusted Langevin chain [82] of length 108 as a surro-\ngate Q\u2217for the target P and display several measures of expectation error\nbetween X \u223cQn and Z \u223cQ\u2217in Figure 3: the normalized predictive error\nmaxl |E[\u27e8X \u2212Z, vl/\u2225vl\u2225\u221e\u27e9]| for vl the l-th datapoint covariate vector, the\nmean error maxj |E[Xj\u2212Zj]|\nmaxj |EQ\u2217[Zj]| , and the second moment error maxj,k |E[XjXk\u2212ZjZk]|\nmaxj,k |EQ\u2217[ZjZk]|\n.\nWe see that the Stein discrepancy provides comparable results without the\nneed for an additional surrogate chain.\n5.4. Comparing quadrature rules.\nStein discrepancies can also measure\nthe quality of deterministic sample sequences designed to improve upon\nMonte Carlo sampling. For the Gaussian mixture target of Section 5.1,\nFigure 4 compares the median quality of 50 sample sequences generated\nfrom four quadrature rules recently studied in [53, Sec. 4.1]: i.i.d. sampling\nfrom P, Quasi-Monte Carlo (QMC) sampling using a deterministic quasir-\nandom number generator, Frank-Wolfe (FW) kernel herding [13, 2], and\nfully-corrective Frank-Wolfe (FCFW) kernel herding [53]. The quality judg-\nments of the Langevin spanner Stein discrepancy (D1) closely mimic those of\nthe L1 Wasserstein distance dW\u2225\u00b7\u2225, which is computable for simple univariate\ntargets [91]. Each Stein discrepancy was computed in under 0.03s.\nUnder both diagnostics and as previously observed in other metrics [53],\nthe i.i.d. samples are typically of lower median quality than their deter-\nministic counterparts. More suprisingly and in contrast to past work fo-\ncused on very smooth function classes [53], FCFW underperforms FW and\nQMC in our diagnostics for larger sample sizes. Apparently FCFW, which\nis heavily optimized for smooth function integration, has sacri\ufb01ced approx-\nimation quality for less smooth test functions. For example, Figure 4 shows\nthat QMC o\ufb00ers a better quadrature estimate than FCFW for h1(x) =\nmax{0, 1\u2212minj\u2208{1,2} |x\u2212\u00b5j|}, a 1-Lipschitz approximation to the indicator\n22\nJ. GORHAM, A.B. DUNCAN, S.J. VOLLMER, AND L. MACKEY\n\u2206= 1\n\u2206= 3\n\u2206= 5\nG\nG\nG\nG\nG G\nG\nG GGGGGGGGGGGG\nG\nG\nG\nG\nG G\nG\nG GGGGGGGGGGGG\nG\nG\nG\nG\nG G G\nG GGGGGGGGGGGG\nG\nG\nG\nG\nG G G G GGGGGGGGGGGG\nG\nG\nG\nG\nG\nG G\nG GGG\nGGGGGGGGG\nG\nG\nG\nG\nG G G G GGGGGGGGG\nGGG\nG\nG\nG\nG\nG G G G GGGGGGGGGGGG\nG\nG\nG\nG\nG\nG G\nG GGGGGGG\nGGGGG\nG\nG\nG\nG\nG G G G GGGGGG\nG\nGGGGG\n0.10\n0.32\n1.00\n0.03\n0.10\n0.32\n1e\u221204\n1e\u221203\n1e\u221202\n1e\u221201\nLangevin Stein\nWasserstein\nEP h1(Z) \u2212EQ h1(X)\n10\n30\n100\n10\n30\n100\n10\n30\n100\nNumber of sample points, n\nMedian diagnostic\nMetric\nG IID\nFW\nFCFW\nQMC\n\u2206= 1\n\u2206= 3\n\u2206= 5\n0\n1\n\u22125.0 \u22122.5\n0.0\n2.5\n5.0\u22125.0 \u22122.5\n0.0\n2.5\n5.0\u22125.0 \u22122.5\n0.0\n2.5\n5.0\nx\nh1(x)\nIID\nFW\nFCFW\nQMC\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n\u22124\n0\n4\n\u22124\n0\n4\nx\ndensity\nGG\nGG\nGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGG\nGG GGG GGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGG GGGG\nGGGGG\nG\nG\nG\nG\nGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGG\nG\nG\nG\nGGGGGGG GGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGG GG\nG\nGG GGGGG\nG\nGGGGGGGG\nG\nG\nGGGGG\nG\nG\nGG\nGGG\nG\nG\nGG\nG\nG\nG\nGG GG\nGG\nG\nG\nGG\nG\nGG\nGG\nG\nGGGG\nG\nG\nG\nGGG\nGGGGGGGGGG\nG\nG\nG\nG\nG\nG\nG\nGGGG\nG\nG\nG\nGG GG\nG\nG\nG\nG\nG\nGGGGGG\nG\nG\nG\nG\nGGG\nG\nGG\nG\nG\nG\nGG\nG\nG\nGGGGGGGGGGGGGGGGGGGGGGGG\nGGGGGGGGG\nG\nG\nG\nG\nGGG\nG\nGG\nGG\nG G\nG\nGGGGGGGGGGGGGGG\nGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGG\nG\nG\nGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGGG\nIID\nFW\nFCFW\nQMC\n\u22120.04\n\u22120.02\n0.00\n0.02\n\u22120.04\n\u22120.02\n0.00\n0.02\n\u22124\n0\n4\n\u22124\n0\n4\nx\nh* (x) = (T g*)(x)\nFig 4: Left: Quadrature rule quality comparison for Gaussian mixture targets P with\nmode separation \u2206(Section 5.4). Right: (Top) Sample histograms with p overlaid (\u2206= 5,\nn = 200). (Bottom) Optimal discriminating test functions h\u2217= T g\u2217from Stein program.\nof being within one standard deviation of a mode.\nIn addition to providing a sample quality score, the Stein discrepancy op-\ntimization problem produces an optimal Stein function g\u2217and an associated\ntest function h\u2217= T g\u2217that is mean zero under P and best distinguishes\nthe sample Qn from the target P. Figure 4 gives examples of these maxi-\nmally discriminatve functions h\u2217for a target mode separation of \u2206= 5 and\nlength 200 sequences from each quadrature rule. We also display the asso-\nciated sample histograms with overlaid target density. The optimal FCFW\nfunction re\ufb02ects the jagged nature of the FCFW histogram.\n6. Connections and conclusions.\nWe developed quality measures\nsuitable for comparing the \ufb01delity of arbitrary \u201co\ufb00-target\u201d sample sequences\nby generating in\ufb01nite collections of known target expectations.\nAlternative quality measures.\nThe score statistic of Fan et al. [25] and the\nGibbs sampler convergence criteria of Zellner and Min [95] account for some\nsample biases but sacri\ufb01ce di\ufb00erentiating power by exploiting only a \ufb01nite\nnumber of known target expectations. For example, when P = N(0, 1), the\nscore statistic [25] cannot di\ufb00erentiate two samples with the same means and\nvariances. Maximum mean discrepancies (MMDs) over characteristic repro-\nducing kernel Hilbert spaces [38] do detect arbitrary distributional biases but\n23\nare only computable when the chosen kernel functions can be integrated un-\nder the target. In practice, one often approximates MMD using a sample\nfrom the target, but this requires a separate trustworthy sample from P.\nWhile we have focused on the graph and classical Stein sets of [35], our\ndi\ufb00usion Stein operators can also be paired with the reproducing kernel\nHilbert space unit balls advocated in [69, 15, 59, 36] to form tractable kernel\ndi\ufb00usion Stein discrepancies or with the random feature functions advocated\nin [46] to form random feature di\ufb00usion Stein discrepancies. We have also\nrestricted our attention to Stein operators arising from di\ufb00usion generators.\nThese take the form (T g)(x) =\n1\np(x)\u27e8\u2207, p(x)m(x)g(x)\u27e9with m = a + c for\na(x) positive semide\ufb01nite and c(x) skew-symmetric. More generally, if the\nmatrix m possesses eigenvalues having a negative real part, then the resulting\noperator need not correspond to a di\ufb00usion process. Such operators fall into\nthe class of pseudo-Fokker Planck operators which have been studied in\nthe context of quantum optics [81]. As noted in [18, 19] it is possible to\nobtain corresponding stochastic dynamics in an extended state space by\nintroducing complex-valued noise terms; these operators may merit further\nstudy in future work.\nAlternative inferential tasks.\nWhile our chief motivation is sample quality\nmeasurement, our work is also directly applicable to a variety of inferential\ntasks that currently rely on the Langevin operator introduced by [35, 69],\nincluding control variate design [69], goodness-of-\ufb01t testing [15, 59], varia-\ntional inference [58, 78, 12], and importance sampling [57]. The Stein factor\nbounds of Theorem 5 can also be used, in the manner of [65, 48, 40], to\ncharacterize the error of numerical discretizations of di\ufb00usions. These works\nconvert bounds on the solutions of Poisson equations \u2013 Stein factors \u2013 into\ncentral limit theorems for EQn[h(X)] \u2212EP [h(Z)], con\ufb01dence intervals for\nEP [h(Z)], and mean-squared error bounds for the estimate EQn[h(X)]. Teh\net al. [90] and Vollmer et al. [92] extended these approaches to obtain error\nestimates for approximate discretizations of the Langevin di\ufb00usion on Rd,\nwhile, independently of our work, Huggins and Zou [45] established error\nestimates for It\u02c6o di\ufb00usion approximations with biased drifts and constant\ndi\ufb00usion coe\ufb03cients. By Theorem 5, their results also hold for It\u02c6o di\ufb00usions\nwith non-constant di\ufb00usion coe\ufb03cients. Following the release of the present\npaper and with the aim of analyzing discretization error for the overdamped\nLangevin di\ufb00usion, Fang et al. [26, Thm. 3.1] derived multivariate Stein fac-\ntor bounds for a class of strongly log-concave distributions. Our Theorem 5\nwith the choice \u03b9 = 1/ log(1/\u03f5) provides Stein factors of the same form but\napplies also to non-log-concave targets and more general di\ufb00usions.\n24\nJ. GORHAM, A.B. DUNCAN, S.J. VOLLMER, AND L. MACKEY\nAlternative targets.\nOur exposition has focused on the Wasserstein distance\ndW\u2225\u00b7\u2225, which is only de\ufb01ned for distributions with \ufb01nite means. A parallel\ndevelopment could be made for the Dudley metric [67] to target distributions\nwith unde\ufb01ned mean. The work of Cerrai [8] also suggests that the Lipschitz\ncondition on our drift and di\ufb00usion coe\ufb03cients can be relaxed.\nAPPENDIX A: PROOF OF PROPOSITION 3\nFix any g \u2208G\u2225\u00b7\u2225. Since g and \u2207g are bounded and b, a, and c are P-\nintegrable, EP [(T g)(Z)] is \ufb01nite. De\ufb01ne the ball Br = {x \u2208Rd : \u2225x\u22252 \u2264r}\nwith nr(z) the outward facing unit normal vector for each z on the boundary\n\u2202Br. Since z 7\u2192p(z)(a(z) + c(z))g(z) is in C1, we may apply the dominated\nconvergence theorem and then the divergence theorem to obtain\nEP [(T g)(Z)] = lim\nr\u2192\u221e\nR\nBr\u27e8\u2207, p(z)(a(z) + c(z))g(z)\u27e9dz\n= lim\nr\u2192\u221e\nR\n\u2202Br\u27e8nr(z), (a(z) + c(z))g(z)p(z)\u27e9dz.\nLet f(r) = M0(g)\nR\n\u2202Br \u2225a(z) + c(z)\u2225op p(z) dz. Since g and nr are bounded,\nR\n\u2202Br\u27e8nr(z), (a(z) + c(z))g(z)p(z)\u27e9dz \u2264f(r).\nThe coarea formula [1] and the integrability of a and c further imply that\nR \u221e\n0 f(r) dr =\nR\nRd M0(g)\u2225a(z) + c(z)\u2225op p(z) dz < \u221e.\nHence, lim infr\u2192\u221ef(r) = 0, and therefore EP [(T g)(Z)] = 0.\nAPPENDIX B: PROOF OF THEOREM 5\nFix any x \u2208Rd and h \u2208W\u2225\u00b7\u22252 with EP [h(Z)] = 0. Since the drift and dif-\nfusion coe\ufb03cients are Lipschitz, [51, Thm. 3.4] guarantees that the di\ufb00usion\n(Zt,x)t\u22650 is well-de\ufb01ned. Using the shorthand sr \u225c\nR \u221e\n0 r(t) dt, we will show\nthat the posited function uh (10) exists and solves the Poisson equation\nh = Auh\n(19)\nwith in\ufb01nitesimal generator A, that uh is Lipschitz, that uh has a continu-\nous Hessian, that uh has a bounded and H\u00a8older continuous Hessian under\nadditional smoothness assumptions.\n25\nExistence of uh and solving the Poisson equation (19).\nConsider the set\nL \u225c(1 + \u2225x\u22252\n2)C0(Rd) = {(1 + \u2225x\u22252\n2)f : f \u2208C0(Rd)}, where C0(Rd) is the\nset of continuous functions vanishing at in\ufb01nity. Equipped with the norm\n\u2225f\u2225L = supx\u2208Rd |f(x)|/(1 + \u2225x\u22252\n2), the set L is a Banach space [84]. As\nnoted in [17], the space L can also be characterized as the closure of the set of\nbounded continuous functions, Cb(Rd), in the set {f : Rd \u2192R : \u2225f\u2225L < \u221e}.\nTo discuss the well-posedness of the Poisson equation (19), we \ufb01rst show that\nthe transition semigroup of an It\u02c6o di\ufb00usion is strongly continuous on L.\nProposition 14.\nThe transition semigroup (Pt)t\u22650 of an It\u02c6o di\ufb00usion\nwith Lipschitz drift and di\ufb00usion coe\ufb03cients is strongly continuous on L.\nProof. Fix any f \u2208L and x \u2208Rd. We \ufb01rst show that (Ptf)(x) converges\npointwise to f(x) as t \u21920+. Since the associated It\u02c6o process (Zt,x)t\u22650 is\nalmost surely pathwise continuous [51, Thm. 3.4] and f is continuous in a\nneighborhood of x, it follows that f(Zt,x) \u2192f(x) as t \u21920+, almost surely.\nMoreover, [28, Sec. 5, Cor. 1.2] implies that\nE\n\u0002\nsup0\u2264t\u22641 |f(Zt,x)|\n\u0003\n\u2264\u2225f\u2225L(1 + E\n\u0002\nsup0\u2264t\u22641 \u2225Zt,x\u22252\n2\n\u0003\n) \u2264C\u2225f\u2225L(1 + \u2225x\u22252\n2),\nfor some C > 0 depending only on M1(b) and M1(\u03c3). The dominated con-\nvergence theorem now yields the desired pointwise convergence.\nTo prove the strong continuity of (Pt)t\u22650, it su\ufb03ces, by [23, Thm. I.5.8, p.\n40], to verify that (Pt)t\u22650 is weakly continuous, i.e., that l(Ptf) \u2192l(f), as\nt \u21920+, for all elements l of the dual space L\u2217. To this end, \ufb01x any l \u2208L\u2217.\nBy the Riesz-Markov theorem for L [17, Theorem 2.4], there exists a \ufb01nite\nsigned Radon measure \u00b5 such that\n(20)\nl(f) =\nR\nRd f(x)\u00b5(dx) and\nR\nRd(1 + \u2225x\u22252\n2)|\u00b5|(dx) = \u2225l\u2225L\u2217,\nfor \u2225\u00b7\u2225L\u2217the dual norm. By Jensen\u2019s inequality and [28, Sec. 5, Cor. 1.2],\n\u2200t, \u2225(Ptf)(x)\u22252 \u2264E[|f(Zt,x)|] \u2264\u2225f\u2225LE\n\u0002\n1 + \u2225Zt,x\u22252\n2\n\u0003\n\u2264C\u2225f\u2225L(1 + \u2225x\u22252\n2).\nSince 1 + \u2225x\u22252\n2 is |\u00b5|-integrable by (20), dominated convergence gives\nlimt\u21920+ l(Ptf) = limt\u21920+\nR\nRd(Ptf)(x)\u00b5(dx) =\nR\nRd f(x)\u00b5(dx) = l(f),\nyielding the result.\nConsider the in\ufb01nitesimal generator A of the semigroup (Pt)t\u22650 on L with\ndom(A) =\n\b\nf \u2208L : limt\u21920+ Ptf\u2212f\nt\nexists in the \u2225\u00b7\u2225L norm\n\t\n.\n26\nJ. GORHAM, A.B. DUNCAN, S.J. VOLLMER, AND L. MACKEY\nSince Pt is strongly continuous on L and h \u2208L with M1(h) \u22641 and\nEP [h(Z)] = 0, [24, Prop. 1.5] implies that\nh \u2212Pth = \u2212A\nR t\n0 Psh ds = Auh,t\nfor\nuh,t \u225c\u2212\nR t\n0 Psh ds.\nThe stationarity of P and the de\ufb01nitions of dW\u2225\u00b7\u22252 and r imply that\n\u2225Pth\u2225L = \u2225Pth \u2212EP [h]\u2225L = supx\u2208Rd |EP [Pth(x) \u2212Pth(Z)]|/(1 + \u2225x\u22252\n2)\n\u2264supx\u2208Rd\nEP [dW\u2225\u00b7\u22252 (\u03b4xPt,\u03b4ZPt)]\n1+\u2225x\u22252\n2\n\u2264r(t) supx\u2208Rd EP [\u2225x\u2212Z\u22252]\n1+\u2225x\u22252\n2\n,\nand hence \u2225Pth\u2225L \u21920 as t \u2192\u221e, since P has a \ufb01nite mean, and r(t) \u21920\nas t \u2192\u221eas r is integrable and monotonic. Arguing similarly,\n\u2225uh,t \u2212uh,t\u2032\u2225L \u2264\u2225\nR t\u2032\nt EP [dW\u2225\u00b7\u22252(\u03b4xPs, \u03b4ZPs)]ds\u2225L\n\u2264supx\u2208Rd EP [\u2225x\u2212Z\u22252]\n1+\u2225x\u22252\n2\nR t\u2032\nt r(s) ds.\nThus, it follows that (uh,t)t>0 is a Cauchy sequence in L with limit uh =\nR \u221e\n0 Psh ds \u2208L. Thus, (h \u2212Pth, uh,t) \u2192(h, uh) in the graph norm on L \u00d7 L,\nand since A is closed [24, Cor. 1.6], uh \u2208dom(A) and h = Auh.\nRemark.\nThe choice of the Banach space is crucial for the argument\nabove. As noted in [64] and contrary to the claim in [4], the semigroup (Pt)t\u22650\nfails to be strongly continuous over the Banach space eL \u225c(1 + \u2225x\u22252\n2)Cb(Rd)\nwhen (Zt,x)t\u22650 is an Ornstein-Uhlenbeck process, i.e., a Langevin di\ufb00usion\n(D1) with a multivariate Gaussian invariant measure.\nLipschitz continuity of uh.\nTo demonstrate that uh is Lipschitz, we choose\nan arbitrary v \u2208Rd, and apply the de\ufb01nition of the Wasserstein distance,\nthe assumed decay rate, and the integrability of r to obtain\n\u2225uh(x + v) \u2212uh(x)\u22252 \u2264\nR \u221e\n0 \u2225E[h(Zt,x) \u2212h(Zt,x+v)]\u22252 dt\n\u2264\nR \u221e\n0 dW\u2225\u00b7\u2225(\u03b4xPt, \u03b4x+vPt) dt \u2264dW\u2225\u00b7\u2225(\u03b4x, \u03b4x+v) sr = \u2225v\u22252 sr < \u221e.\nContinuity of \u22072uh.\nSince uh \u2208dom(A) is a continuous solution of the\nPoisson equation (19), and since the in\ufb01nitesimal generator agrees with the\ncharacteristic operator of a di\ufb00usion when both are de\ufb01ned [70, p. 129],\nThm. 5.9 of [21] implies that uh \u2208C2.\n27\nBoundedness of \u22072uh.\nInstantiate the additional preconditions of (11), and\nassume that M0(\u03c3\u22121), F2(\u03c3), M2(b) < \u221e, or else (11) is vacuous. Lemma 15,\nestablished in Section C, shows that the semigroup Pth admits a bounded\ncontinuous Hessian, which is integrable in t.\nLemma 15 (Semigroup Hessian estimate).\nSuppose that the drift and\ndi\ufb00usion coe\ufb03cients b and \u03c3 of an It\u02c6o di\ufb00usion are Lipschitz with Lipschitz\ngradients and locally Lipschitz second derivatives. If the transition semigroup\n(Pt)t\u22650 has Wasserstein decay rate r, and \u03c3(x) has a right inverse \u03c3\u22121(x)\nfor each x \u2208Rd, then, for all t > 0 and any f \u2208C2 with bounded \ufb01rst and\nsecond derivatives, Ptf is twice continuously di\ufb00erentiable with\nM1(Ptf) \u2264M1(f)r(t)\nand\n(21)\nM2(Ptf) \u2264\ninf\nt0\u2208(0,t] M1(f)r(t \u2212t0)\nq\n1\nt0 et0\u03b32M0(\u03c3\u22121)\n(22)\n+ M1(f)r(t \u2212t0)r(0)et0\u03b32M1(\u03c3)M0(\u03c3\u22121)\n+ M1(f)r(t \u2212t0)\u221at0 r(0)et0\u03b34 2\n3\n\u221a\u03b1\nfor \u03b3\u03c1 \u225c\u03c1M1(b) + \u03c12\u22122\u03c1\n2\nM1(\u03c3)2 + \u03c1\n2F1(\u03c3)2, \u03b1 \u225c\nM2(b)2\n2M1(b)+4M1(\u03c3)2 + 2F2(\u03c3)2.\nThe dominated convergence theorem now implies that the Hessian of uh\nis obtained by di\ufb00erentiating twice under the integral sign. The advertised\nbound (11) on \u22072uh follows by replacing the in\ufb01mum on the right-hand side\nof the semigroup bound (22) with the selection t0 = min(t, 1), applying the\nbound emin(t,1)\u03b3\u03c1 \u2264e\u03b3\u03c1 for each \u03b3\u03c1 and t, and integrating the result over t.\nH\u00a8older continuity of \u22072uh.\nFinally, instantiate the additional precondi-\ntions of (12), and \ufb01x any \u03b9 \u2208(0, 1). The integral representation (10) of uh,\nthe dominated convergence theorem, and Jensen\u2019s inequality imply\nM1\u2212\u03b9(\u22072uh) = M1\u2212\u03b9\n\u0000\u2212\nR \u221e\n0 \u22072Pth dt\n\u0001\n\u2264\nR \u221e\n0 M1\u2212\u03b9(\u22072Pth) dt.\nWhen t \u22641, a seminorm interpolation lemma (Lemma 19 in the supple-\nment), a semigroup third derivative estimate (Lemma 20 in the supple-\nment) with t0 = min(t, 1), and the semigroup second derivative estimate\nof Lemma 15 with t0 = min(t, 1) imply\nM1\u2212\u03b9(\u22072Pth) \u2264M1(h)2\u03b9M0(\u22072Pth)\u03b9M1(\u22072Pth)1\u2212\u03b9 \u2264M1(h)t\u03b9/2\u22121/K1\n28\nJ. GORHAM, A.B. DUNCAN, S.J. VOLLMER, AND L. MACKEY\nfor some constant K1 > 0 depending only on M1:3(b), M1:3(\u03c3), M0(\u03c3\u22121), and\nr. Thus\nR 1\n0 M1\u2212\u03b9(\u22072Pth) dt \u22642M1(h)\nK1\u03b9 . For t > 1, Lemmas 19, 20, and 15 and\nthe integrability of r yield\nR \u221e\n1 M1\u2212\u03b9(\u22072Pth) dt \u2264M1(h) 2\nK2\nR \u221e\n1 r(t \u22121) dt = M1(h) 2\nK2 sr\nfor a constant K2 > 0 again depending only on M1:3(b), M1:3(\u03c3), M0(\u03c3\u22121),\nand r. Combining these bounds and choosing K = min(K1, K2)/2 completes\nthe proof. An explicit constant K can be obtained by tracing constants\nthrough the proof of Lemma 20.\nAPPENDIX C: PROOF OF LEMMA 15\nFix any x \u2208Rd and f : Rd \u2192R in C2 with bounded \ufb01rst and sec-\nond derivatives, and let (Zt,x)t\u22650 be an It\u02c6o di\ufb00usion solving the stochastic\ndi\ufb00erential equation (5) with starting point Z0,x = x, underlying Wiener\nprocess (Wt)t\u22650, and transition semigroup (Pt)t\u22650. Our proof is divided into\n\ufb01ve pieces establishing, for each t > 0, the Lipschitz continuity of Ptf, the\nLipschitz continuity of \u2207Ptf, the continuity of \u22072Ptf, an initial bound on\n\u22072Ptf, and the in\ufb01mal bound (22) on \u22072Ptf.\nLipschitz continuity of Ptf.\nThe semigroup gradient bound (21) follows\nfrom the Lipschitz continuity of f and the de\ufb01nitions of the Wasserstein\ndecay rate and the Wasserstein distance, as, for any y \u2208Rd and t \u22650,\n(Ptf)(x) \u2212(Ptf)(y) = E[f(Zt,x) \u2212f(Zt,y)] \u2264M1(f)dW\u2225\u00b7\u22252(\u03b4xPt, \u03b4yPt)\n\u2264M1(f)r(t) dW\u2225\u00b7\u22252(\u03b4x, \u03b4y) = M1(f)r(t)\u2225x \u2212y\u22252.\nLipschitz continuity of \u2207Ptf.\nFix any v, v\u2032 \u2208Rd. Under our smoothness\nassumptions on b and \u03c3, [76, Theorem V.40] implies that (Zt,x)t\u22650 is twice\ncontinuously di\ufb00erentiable in x. The \ufb01rst directional derivative \ufb02ow (Vt,v)t\u22650\nsolves the \ufb01rst variation equation,\ndVt,v = \u2207b(Zt,x)Vt,v dt + \u2207\u03c3(Zt,x)Vt,v dWt\nwith\nV0,v = v,\n(23)\nobtained by formally di\ufb00erentiating the equation (5) de\ufb01ning (Zt,x)t\u22650 with\nrespect to x in the direction v. The second directional derivative \ufb02ow\n(Ut,v,v\u2032)t\u22650 solves the second variation equation,\ndUt,v,v\u2032 = (\u2207b(Zt,x)Ut,v,v\u2032 + \u22072b(Zt,x)[Vt,v\u2032]Vt,v) dt\n+ (\u2207\u03c3(Zt,x)Ut,v,v\u2032 + \u22072\u03c3(Zt,x)[Vt,v\u2032]Vt,v) dWt with U0,v,v\u2032 = 0,\n(24)\nobtained by di\ufb00erentiating (23) with respect to x in the direction v\u2032.\n29\nSince f has bounded \ufb01rst and second derivatives, the dominated conver-\ngence theorem implies that, for each t \u22650, Ptf is twice di\ufb00erentiable with\n\u27e8\u2207(Ptf)(x), v\u27e9= E[\u27e8\u2207f(Zt,x), Vt,v\u27e9]\nand\nv\u2032\u22a4\u22072(Ptf)(x)v = E\nh\nV \u22a4\nt,v\u2032\u22072f(Zt,x)Vt,v + \u27e8\u2207f(Zt,x), Ut,v,v\u2032\u27e9\ni\n(25)\nobtained by di\ufb00erentiating under the integral sign. Lemma 16, proved in\nSection C.1, justi\ufb01es the exchanges of derivative and expectation by ensuring\nthat the derivative \ufb02ows have moments bounded uniformly in x.\nLemma 16 (Derivative \ufb02ow bounds).\nSuppose that (Zt,x)t\u22650 is an It\u02c6o\ndi\ufb00usion with starting point Z0,x = x \u2208Rd, driving Wiener process (Wt)t\u22650,\nand Lipschitz drift and di\ufb00usion coe\ufb03cients b and \u03c3 with Lipschitz gradients\nand locally Lipschitz second derivatives. If (Vt,v)t\u22650 and (Ut,v,v\u2032)t\u22650 respec-\ntively solve the stochastic di\ufb00erential equations (23) and (24) for v, v\u2032 \u2208Rd,\nthen, for any \u03c1 \u22652,\nE[\u2225Vt,v\u2225\u03c1\n2] \u2264\u2225v\u2225\u03c1\n2 et\u03b3\u03c1\nand\n(26)\nE\n\u0002\n\u2225Ut,v,v\u2032\u22252\n2\n\u0003\n\u2264\u03b1\u2225v\u22252\n2\u2225v\u2032\u22252\n2tet\u03b34\n(27)\nfor \u03b3\u03c1 \u225c\u03c1M1(b)+ \u03c12\u22122\u03c1\n2\nM1(\u03c3)2+ \u03c1\n2F1(\u03c3)2 and \u03b1 \u225c\nM2(b)2\n2M1(b)+4M1(\u03c3)2 +2F2(\u03c3)2.\nSince \u2207f and \u22072f are bounded, and (Vt,v)t\u22650, (Vt,v\u2032)t\u22650, and (Ut,v,v\u2032)t\u22650\nhave second moments bounded uniformly in x by Lemma 16, the Hessian\nformula (25) implies that \u22072Ptf is bounded and hence that \u2207Ptf is Lipschitz\ncontinuous for each t \u22650.\nContinuity of \u22072Ptf.\nHereafter we assume that M0(\u03c3\u22121) < \u221e, as the\nsemigroup Hessian bound (22) is otherwise vacuous.\nThe Lipschitz continuity of f and the It\u02c6o di\ufb00usion moment bound of [51,\nThm. 3.4, part 4] together imply that\nE\n\u0002\nf(Zt,x)2\u0003\n\u2264E\n\u0002\n(|f(x)| + \u2225Zt,x \u2212x\u22252M1(f))2\u0003\n< \u221e\nfor all t \u22650. Since \u03c3\u22121 is bounded, and \u2207b and \u2207\u03c3 are bounded and Lip-\nschitz, [27, Prop. 3.2] gives the following Bismut-Elworthy-Li-type formula\nfor the directional derivative of Ptf for each t > 0:\n\u27e8\u2207(Ptf)(x), v\u27e9= 1\nt E\nh\nf(Zt,x)\nR t\n0\u27e8\u03c3\u22121(Zs,x)Vs,v, dWs\u27e9\ni\n,\n30\nJ. GORHAM, A.B. DUNCAN, S.J. VOLLMER, AND L. MACKEY\nBy interchanging derivative and integral, the dominated convergence theo-\nrem now delivers the Hessian expression\nv\u2032\u22a4\u22072(Ptf)(x)v = E[J1,x + J2,x + J3,x]\nfor\n(28)\nJ1,x \u225c1\nt \u27e8\u2207f(Zt,x), Vt,v\u2032\u27e9\nR t\n0\u27e8\u03c3\u22121(Zs,x)Vs,v, dWs\u27e9,\nJ2,x \u225c1\nt f(Zt,x)\nR t\n0\u27e8\u2207\u03c3\u22121(Zs,x)[Vs,v\u2032]Vs,v, dWs\u27e9,\nand\nJ3,x \u225c1\nt f(Zt,x)\nR t\n0\u27e8\u03c3\u22121(Zs,x)Us,v,v\u2032, dWs\u27e9,\nfor each t > 0, provided that J1,x, J2,x, and J3,x are continuous in x. The\nrequisite continuity follows from the Lipschitz continuity of \u2207f and f, the\nboundedness of \u03c3\u22121, \u2207\u03c3, and \u22072\u03c3, and the controlled moment growth and\nH\u00a8older continuity of (Zt,x)t\u22650, (Vt,v)t\u22650, (Vt,v\u2032)t\u22650, and (Ut,v,v\u2032)t\u22650 as func-\ntions of x [76, Theorem V.40]. The dominated convergence theorem further\nimplies that \u22072Ptf is continuous for each t > 0.\nInitial bound on \u22072Ptf.\nNow, we \ufb01x any t > 0 and turn to bounding \u22072Ptf\nin terms of M1(f), by bounding the expectations of J1,x, J2,x, and J3,x of\n(28) in turn.\nTo control E[J1,x], we apply Cauchy-Schwarz, the It\u02c6o isometry [28, Eqs.\n7.1 and 7.2], the derivative \ufb02ow bound (26), and the fact es\u03b32 \u2264et\u03b32 for all\ns \u2264t to obtain\nE[J1,x] \u22641\nt\nq\nE\n\u0002\n\u27e8\u2207f(Zt,x), Vt,v\u2032\u27e92\u0003\nE[(\nR t\n0\u27e8\u03c3\u22121(Zs,x)Vs,v, dWs\u27e9)2]\n\u22641\nt M1(f)\nq\nE\n\u0002\n\u2225Vt,v\u2032\u22252\n2\n\u0003 R t\n0 E\n\u0002\n\u2225\u03c3\u22121(Zs,x)Vs,v\u22252\n2\n\u0003\nds\n\u22641\nt M1(f)M0(\u03c3\u22121)\nq\nE\n\u0002\n\u2225Vt,v\u2032\u22252\n2\n\u0003 R t\n0 E\n\u0002\n\u2225Vs,v\u22252\n2\n\u0003\nds\n\u22641\nt M1(f)M0(\u03c3\u22121)\u2225v\u2032\u22252\u2225v\u22252\nq\net\u03b32 R t\n0 es\u03b32 ds\n\u2264\nq\n1\nt et\u03b32M1(f)M0(\u03c3\u22121)\u2225v\u2032\u22252\u2225v\u22252,\nwhere we have adopted the de\ufb01nition of \u03b3\u03c1 given in Lemma 16.\nTo control E[J2,x], we will \ufb01rst rewrite the unbounded quantity f(Zt,x) in\nterms of more manageable semigroup gradients. To this end, we note that,\nsince Pt\u2212sf \u2208C2 for all s \u2208[0, t], we may apply It\u02c6o\u2019s formula [28, Thm. 7.1]\nto (s, x) 7\u2192Pt\u2212sf(x) to obtain the identity\nf(Zt,x) = (Ptf)(x) +\nR t\n0\u27e8\u2207(Pt\u2212sf)(Zs,x), \u03c3(Zs,x) dWs\u27e9.\n(29)\n31\nNow we may rewrite E[J2,x] as\nE[J2,x] = 1\nt E\nh\n(Ptf)(x)\nR t\n0\u27e8\u2207\u03c3\u22121(Zs,x)[Vs,v\u2032]Vs,v, dWs\u27e9\n+\nR t\n0\u27e8\u2207(Pt\u2212sf)(Zs,x), \u03c3(Zs,x) dWs\u27e9\nR t\n0\u27e8\u2207\u03c3\u22121(Zs,x)[Vs,v\u2032]Vs,v, dWs\u27e9\ni\n= 1\nt E\nhR t\n0\u27e8\u2207(Pt\u2212sf)(Zs,x), \u03c3(Zs,x)\u2207\u03c3\u22121(Zs,x)[Vs,v\u2032]Vs,v\u27e9ds\ni\n= \u22121\nt E\nhR t\n0\u27e8\u2207(Pt\u2212sf)(Zs,x), \u2207\u03c3(Zs,x)[Vs,v\u2032]\u03c3\u22121(Zs,x)Vs,v\u27e9ds\ni\n,\nwhere we have used Dynkin\u2019s formula [28, Eq. 7.11], the It\u02c6o isometry, and\nthe chain rule,\n(30)\n\u2207\u03c3\u22121(x)[v] = \u2212\u03c3\u22121(x)\u2207\u03c3(x)[v]\u03c3\u22121(x).\nFinally, we bound E[J2,x] using Cauchy-Schwarz, the semigroup gradient\nbound (21), the derivative \ufb02ow bound (26), and the fact that s 7\u2192r(t\u2212s)es\u03b32\nis increasing:\nE[J2,x] \u22641\nt M1(\u03c3)M0(\u03c3\u22121)\nR t\n0 M1(Pt\u2212sf)E\n\u0002\n\u2225Vs,v\u2032\u22252\u2225Vs,v\u22252\n\u0003\nds\n\u22641\nt M1(\u03c3)M0(\u03c3\u22121)\nR t\n0 M1(Pt\u2212sf)\nq\nE\n\u0002\n\u2225Vs,v\u2032\u22252\n2\n\u0003\nE\n\u0002\n\u2225Vs,v\u22252\n2\n\u0003\nds\n\u22641\nt M1(\u03c3)M0(\u03c3\u22121)M1(f)\u2225v\u2032\u22252\u2225v\u22252\nR t\n0 r(t \u2212s)es\u03b32 ds\n\u2264r(0)et\u03b32M1(\u03c3)M0(\u03c3\u22121)M1(f)\u2225v\u2032\u22252\u2225v\u22252.\nTo control E[J3,x], we again appeal to Dynkin\u2019s formula and the It\u02c6o isom-\netry to obtain\nE[J3,x] = 1\nt E\nh\n(Ptf)(x)\nR t\n0\u27e8\u03c3\u22121(Zs,x)Us,v,v\u2032, dWs\u27e9\n+\nR t\n0\u27e8\u2207(Pt\u2212sf)(Zs,x), \u03c3(Zs,x) dWs\u27e9\nR t\n0\u27e8\u03c3\u22121(Zs,x)Us,v,v\u2032, dWs\u27e9\ni\n= E\nhR t\n0\u27e8\u2207(Pt\u2212sf)(Zs,x), Us,v,v\u2032\u27e9ds\ni\n,\nand we bound this expression using Cauchy-Schwarz, Jensen\u2019s inequality,\nthe semigroup gradient bound (21), the second derivative \ufb02ow bound (27),\nand the fact that s 7\u2192r(t \u2212s)es\u03b34 is increasing:\nE[J3,x] \u22641\nt\nR t\n0 M1(Pt\u2212sf)E\n\u0002\n\u2225Us,v,v\u2032\u22252\n\u0003\nds \u22641\nt\nR t\n0 M1(Pt\u2212sf)\nq\nE\n\u0002\n\u2225Us,v,v\u2032\u22252\n2\n\u0003\nds\n\u22641\nt M1(f)\u221a\u03b1\u2225v\u2032\u22252\u2225v\u22252\nR t\n0 r(t \u2212s)\u221ases\u03b34 ds\n\u22642\n3\n\u221a\nt r(0)et\u03b34M1(f)\u221a\u03b1\u2225v\u2032\u22252\u2225v\u22252,\nwhere \u03b1 is de\ufb01ned in Lemma 16. The advertised result (22) for t0 = t follows\nby summing the bounds developed for E[J1,x], E[J2,x], and E[J3,x].\n32\nJ. GORHAM, A.B. DUNCAN, S.J. VOLLMER, AND L. MACKEY\nIn\ufb01mal bound on \u22072Ptf.\nTo obtain the in\ufb01mum over t0 \u2208(0, t] in (22), we\nadapt an argument of [8, Prop. 1.5.1]. Speci\ufb01cally, \ufb01x any t0 \u2208(0, t]. Our\nwork thus far shows that v\u2032\u22a4\u22072(Pt0 \u02dcf)(x)v \u2264M1( \u02dcf)\u03b6(t0) for a real-valued\nfunction \u03b6 and \u02dcf \u2208C2 with bounded \ufb01rst and second derivatives. Since we\nnow know that Pt\u2212t0f \u2208C2 with bounded \ufb01rst and second derivatives, the\nMarkov property of the di\ufb00usion and the \ufb01rst derivative bound (21) yield\nv\u2032\u22a4\u22072(Ptf)(x)v = v\u2032\u22a4\u22072(Pt0Pt\u2212t0f)(x)v\n\u2264M1(Pt\u2212t0f)\u03b6(t0) \u2264M1(f)r(t \u2212t0)\u03b6(t0).\nC.1. Proof of Lemma 16: Derivative \ufb02ow bounds.\nFix any \u03c1 \u22652\nand v \u2208Rd. Since Dynkin\u2019s formula and Cauchy-Schwarz give\nE[\u2225Vt,v\u2225\u03c1\n2] = \u2225v\u2225\u03c1\n2 + E\nhR t\n0 \u03c1\u27e8Vs,v\u2225Vs,v\u2225\u03c1\u22122\n2\n, \u2207b(Zs,x)Vs,v\u27e9\n+ \u03c1\n2\u2225Vs,v\u2225\u03c1\u22124\n2\n((\u03c1 \u22122)\u2225V \u22a4\ns,v\u2207\u03c3(Zs,x)[Vs,v]\u22252\n2 + \u2225Vs,v\u22252\n2\u2225\u2207\u03c3(Zs,x)[Vs,v]\u22252\nF ) ds\n\u0003\n\u2264\u2225v\u2225\u03c1\n2 +\nR t\n0(\u03c1M1(b) + \u03c12\u22122\u03c1\n2\nM1(\u03c3)2 + \u03c1\n2F1(\u03c3)2)E[\u2225Vs,v\u2225\u03c1\n2] ds,\nthe advertised result (26) follows from Gr\u00a8onwall\u2019s inequality.\nNow \ufb01x any v, v\u2032 \u2208Rd, and de\ufb01ne Ut \u225cUt,v,v\u2032. Dynkin\u2019s formula and\nmultiple applications of Cauchy-Schwarz and Young\u2019s inequality give\nE\n\u0002\n\u2225Ut\u22252\n2\n\u0003\n= E\nhR t\n0 2\u27e8Us, \u2207b(Zs,x)Us + \u22072b(Zs,x)[Vs,v\u2032]Vs,v\u27e9\n+ \u2225\u2207\u03c3(Zs,x)[Us] + \u22072\u03c3(Zs,x)[Vs,v\u2032]Vs,v\u22252\nF ds\n\u0003\n\u2264E\nhR t\n0 2\u2225Us\u22252\n2M1(b) + 2\u2225Us\u22252\u2225Vs,v\u22252\u2225Vs,v\u2032\u22252M2(b)\n+ 2\u2225\u2207\u03c3(Zs,x)[Us]\u22252\nF + 2\u2225\u22072\u03c3(Zs,x)[Vs,v\u2032]Vs,v\u22252\nF ds\n\u0003\n\u2264\nR t\n0(2M1(b) + 2F1(\u03c3)2 + \u03f5)E\n\u0002\n\u2225Us\u22252\n2\n\u0003\n+ (M2(b)2/\u03f5 + 2F2(\u03c3)2)E\n\u0002\n\u2225Vs,v\u22252\n2\u2225Vs,v\u2032\u22252\n2\n\u0003\nds\nfor any \u03f5 > 0. Letting \u03b3\u03c1 = \u03c1M1(b) + \u03c12\u22122\u03c1\n2\nM1(\u03c3)2 + \u03c1\n2F1(\u03c3)2, we see that,\nby Cauchy-Schwarz and our derivative \ufb02ow bound (26),\nR t\n0 E\n\u0002\n\u2225Vs,v\u22252\n2\u2225Vs,v\u2032\u22252\n2\n\u0003\nds \u2264\nR t\n0\nq\nE\n\u0002\n\u2225Vs,v\u22254\n2\n\u0003\nE\n\u0002\n\u2225Vs,v\u2032\u22254\n2\n\u0003\nds\n\u2264\nR t\n0 \u2225v\u22252\n2\u2225v\u2032\u22252\n2 es\u03b34 ds = \u2225v\u22252\n2\u2225v\u2032\u22252\n2\net\u03b34\u22121\n\u03b34\n.\nHence, if we choose \u03f5 = \u03b34 \u2212(2M1(b) + 2F1(\u03c3)2) and de\ufb01ne \u03b1 = M2(b)2/\u03f5 +\n2F2(\u03c3)2 we may write\nE\n\u0002\n\u2225Ut\u22252\n2\n\u0003\n\u2264\u03b1\u2225v\u22252\n2\u2225v\u2032\u22252\n2\net\u03b34\u22121\n\u03b34\n+\nR t\n0 \u03b34E\n\u0002\n\u2225Us\u22252\n2\n\u0003\nds.\n33\nGronwall\u2019s inequality now yields the result (27) via\nE\n\u0002\n\u2225Ut\u22252\n2\n\u0003\n\u2264\u03b1\u2225v\u22252\n2\u2225v\u2032\u22252\n2\n\u0010\net\u03b34\u22121\n\u03b34\n+\nR t\n0\nes\u03b34\u22121\n\u03b34\n\u03b34e(t\u2212s)\u03b34 ds\n\u0011\n= \u03b1\u2225v\u22252\n2\u2225v\u2032\u22252\n2tet\u03b34.\nAPPENDIX D: PROOF OF THEOREM 6\nWe \ufb01rst derive the result for \u2225\u00b7\u2225= \u2225\u00b7\u22252. Without loss of generality, as-\nsume h \u2208W\u2225\u00b7\u22252 with EP [h(Z)] = 0. Our high-level strategy is to relate the\nWasserstein distance to the Stein discrepancy via the Stein equation (3) with\ndi\ufb00usion Stein operator T (8). Since the in\ufb01nitesimal generator A (4) has\nthe form (7) by Theorem 2, Theorem 5 implies that there exists a contin-\nuously di\ufb00erentiable solution gh to the the Stein equation h(x) = (T gh)(x)\nsatisfying M0(gh) \u2264srM1(h) \u2264sr. Since boundedness alone is insu\ufb03cient\nto declare that gh falls into a scaled copy of the classical Stein set G\u2225\u00b7\u2225, we\nwill develop a smoothed version of the Stein solution with greater regularity.\nSince a and c are constant, b(x) = 1\n2(a + c)\u2207log p(x). Fix any s > 0 and\nconsider the convolution gh,s(x) \u225cE[gh(x + sG)]. If the smoothing level s is\nsmall, the Lipschitz continuity of h implies that that (T gh,s)(x) provides a\nclose approximation to h(x) for each x \u2208Rd:\nh(x) \u2264E[h(x + sG)] + M1(h)sE[\u2225G\u22252]\n(31)\n\u2264E\nh\n1\np(x+sG)\u27e8\u2207, p(x + sG)(a + c)gh(x + sG)\u27e9\ni\n+ sE[\u2225G\u22252]\n\u22642E[\u27e8b(x + sG), gh(x + sG)\u27e9] + E[\u27e8a + c, \u2207gh(x + sG)\u27e9] + sE[\u2225G\u22252]\n\u2264(T gh,s)(x) + sE[\u2225G\u22252](1 + 2M1(b)M0(gh)).\nMoreover, by our next lemma, proved in Section D.1, the smoothed Stein\nsolution admits a bounded Lipschitz gradient \u2207gh,s(x) = E[\u2207gh(x + sG)].\nLemma 17 (Smoothing by Gaussian convolution).\nLet G \u2208Rd be a\nstandard normal random vector, and \ufb01x s > 0. If f : Rd \u2192R is bounded\nand measurable, and fs(x) \u225cE[f(x + sG)], then\nM0(fs) \u2264M0(f),\nM1(fs) \u2264\nq\n2\n\u03c0\nM0(f)\ns\n,\nand\nM2(fs) \u2264\n\u221a\n2 M0(f)\ns2\n.\nIf, additionally, f \u2208C1, then \u2207fs(x) = E[\u2207f(x + sG)].\nIndeed, for each non-zero w \u2208Rd, we may apply Lemma 17 to the function\nfw(x) \u225c\u27e8w, gh(x)\u27e9/\u2225w\u22252 with convolution fw,s(x) = \u27e8w, gh,s(x)\u27e9/\u2225w\u22252 to\nobtain the bounds\nM0(gh,s) = supw\u0338=0 M0(fw,s) \u2264supw\u0338=0 M0(fw) = M0(gh) \u2264sr,\nM1(gh,s) = supw\u0338=0 M1(fw,s) \u2264supw\u0338=0\nq\n2\n\u03c0\nM1(fw)\ns\n=\nq\n2\n\u03c0\nM1(fw)\ns\n\u2264\nq\n2\n\u03c0\nsr\ns ,\n34\nJ. GORHAM, A.B. DUNCAN, S.J. VOLLMER, AND L. MACKEY\nand M2(gh,s) = supw\u0338=0 M2(fw,s) \u2264supw\u0338=0\n\u221a\n2M2(fw)\ns2\n=\n\u221a\n2M2(fw)\ns2\n\u2264\n\u221a\n2 sr\ns2 .\nHence, since our choice of h was arbitrary, and\n\u03bas \u225cmax\n\u0010\n1, 1\ns\nq\n2\n\u03c0,\n\u221a\n2\ns2\n\u0011\n= max\n\u0010\n1,\n\u221a\n2\ns2\n\u0011\n\u2265max(M0(gh,s),M1(gh,s),M2(gh,s))\nsr\n,\nwe may take expectation under Qn and supremum over h in (31) to reach\ndW\u2225\u00b7\u22252(\u00b5, \u03bd) \u2264inf\ns>0 S(Qn, T , G\u2225\u00b7\u22252)sr\u03bas + sE[\u2225G\u22252](1 + 2M1(b)sr)\n\u2264max\n\u0000S(Qn, T , G\u2225\u00b7\u22252)sr, \u03b7\n\u0001\n+ 2\u03b7 \u22643 max\n\u0000S(Qn, T , G\u2225\u00b7\u22252)sr, \u03b7\n\u0001\n,\nwhere we de\ufb01ne \u03b7 =\n3q\nS(Qn, T , G\u2225\u00b7\u22252)\n\u221a\n2sr E[\u2225G\u22252]2(1 + 2M1(b)sr)2 and\nselect s =\n3q\nS(Qn, T , G\u2225\u00b7\u22252)2\n\u221a\n2sr/(E[\u2225G\u22252](1 + 2M1(b)sr)) to produce the\nsecond inequality. The generic norm result now follows from the assumed\nnorm domination property \u2225\u00b7\u2225\u2265\u2225\u00b7\u22252, which implies G\u2225\u00b7\u22252 \u2286G\u2225\u00b7\u2225.\nD.1. Proof of Lemma 17: Smoothing by Gaussian convolution.\nThe conclusion M0(fs) \u2264M0(f) follows from H\u00a8older\u2019s inequality. Now, \ufb01x\nany x and non-zero v1, v2 \u2208Rd. Since fs = f \u22c6\u03c6s, where \u03c6s \u2208C\u221eis the\ndensity of sG and \u22c6is the convolution operator, Leibniz\u2019s rule implies that\n\u27e8v1, \u2207fs(x)\u27e9= \u27e8v1, (f \u22c6\u2207\u03c6s)(x)\u27e9= 1\ns2\nR\nf(x \u2212y)\u27e8v1, y\u27e9\u03c6s(y)dy\n\u2264M0(f)\ns2\nR\n|\u27e8v1, y\u27e9|\u03c6s(y) dy =\nq\n2\n\u03c0\nM0(f)\ns\n\u2225v1\u22252,\nas \u27e8v1, G\u27e9/\u2225v1\u22252 has a standard normal distribution. Leibniz\u2019s rule also gives\n\u22072fs(x)[v1, v2] = (f \u22c6\u22072\u03c6s)(x)[v1, v2]\n\u2264M0(f)\ns2\nR\nRd\n\f\f\u27e8v1, zz\u22a4v2\u27e9/s2 \u2212\u27e8v1, v2\u27e9\n\f\f\u03c6s(z) dz\n\u2264M0(f)\ns2\nqR\nRd|\u27e8v1, zz\u22a4v2\u27e9/s2 \u2212\u27e8v1, v2\u27e9|2\u03c6s(z) dz\n= M0(f)\ns2\np\n\u27e8v1, v2\u27e92 + \u2225v1\u22252\n2\u2225v2\u22252\n2 \u2264\n\u221a\n2M0(f)\ns2\n\u2225v1\u22252\u2225v2\u22252,\nwhere the last equality follows by Isserlis\u2019 theorem. Finally, when f \u2208C1,\nLeibniz\u2019s rule gives \u2207fs = \u2207f \u22c6\u03c6s.\nAPPENDIX E: PROOF OF THEOREM 7\nWe will derive each inequality for \u2225\u00b7\u2225= \u2225\u00b7\u22252; the generic norm results will\nthen follow from the property \u2225\u00b7\u2225\u2265\u2225\u00b7\u22252, which implies G\u2225\u00b7\u22252 \u2286G\u2225\u00b7\u2225.\n35\nFix any h \u2208H = {h : Rd \u2192R | h \u2208C3, M1(h) \u22641, M2(h) < \u221e, M3(h) <\n\u221e} with EP [h(Z)] = 0. We assume that M1(b), M2(b), M1(\u03c3), F2(\u03c3),\nM\u2217\n1 (m), and M0(\u03c3\u22121) are all \ufb01nite, or else the results are vacuous. Our\nhigh-level strategy is to relate the Wasserstein distance to the Stein dis-\ncrepancy via the Stein equation (3) with di\ufb00usion Stein operator T (8). By\nTheorem 5, we know that there exists a Lipschitz solution gh to the the\nStein equation h(x) = (T gh)(x) satisfying M0(gh) \u2264srM1(h) \u2264sr and\nM1(gh) \u2264\u03b2M1(h) \u2264\u03b2, for \u03b2 \u225c\u03b21 + \u03b22, where \u03b21 and \u03b22 are de\ufb01ned in\nTheorem 5. Since a Lipschitz gradient is also needed to declare that gh falls\ninto a scaled copy of the classical Stein set G\u2225\u00b7\u2225, we will develop a smoothed\nversion of the Stein solution with greater regularity.\nFor this purpose, \ufb01x any s > 0 and consider the convolution gh,s(x) \u225c\nE[gh(x + sG)]. If the smoothing level s is small, the Lipschitz continuity of\nm and h implies that (T gh,s)(x) closely approximates h(x) for each x \u2208Rd:\nh(x) \u2264E[h(x + sG)] + M1(h)sE[\u2225G\u22252]\n(32)\n\u22642E[\u27e8b(x + sG), gh(x + sG)\u27e9+ \u27e8m(x + sG), \u2207gh(x + sG)\u27e9] + sE[\u2225G\u22252]\n\u2264(T gh,s)(x) + s\u03b6.\nE.1. Proof of the \ufb01rst inequality.\nMoreover, by an argument mir-\nroring that of Theorem 6, Lemma 17 shows that gh,s admits a Lipschitz\ngradient \u2207gh,s(x) = E[\u2207gh(x + sG)] and satis\ufb01es the derivative bounds\nM0(gh,s) \u2264M0(gh) \u2264sr,\n(33)\nM1(gh,s) = M0(\u2207gh,s) \u2264M0(\u2207gh) \u2264\u03b2,\nand\nM2(gh,s) = M1(\u2207gh,s) \u2264\nq\n2\n\u03c0\nM0(\u2207gh)\ns\n\u2264\nq\n2\n\u03c0\n\u03b2\ns .\nLet \u03b7 \u225cs\u2217\u03b6 for s\u2217=\nq\nS(Qn, T , G\u2225\u00b7\u22252)\np\n2/\u03c0\u03b2/\u03b6. Since H is dense in W\u2225\u00b7\u22252,\nwe may take expectation under Qn and supremum over h in (32) to reach\ndW\u2225\u00b7\u22252(\u00b5, \u03bd) \u2264inf\ns>0 S(Qn, T , G\u2225\u00b7\u22252) max\n\u0010\nsr, \u03b2,\nq\n2\n\u03c0\n\u03b2\ns\n\u0011\n+ s\u03b6\n\u2264max(S(Qn, T , G\u2225\u00b7\u22252) max(sr, \u03b2), \u03b7) + \u03b7\n\u22642 max(S(Qn, T , G\u2225\u00b7\u22252) max(sr, \u03b2), \u03b7).\nE.2. Proof of the second inequality.\nAssume now that \u22073b and\n\u22073\u03c3 are bounded and locally Lipschitz. Fix any \u03b9 \u2208(0, 1). Lemma 17 and\nan auxiliary smoothing lemma (Lemma 18 in the supplement) imply that\nM2(gh,s) = M1(\u2207gh,s) \u2264\n\u221a\nd M1\u2212\u03b9(\u2207gh)\ns\u03b9\n. This improved dependence on s will\n36\nJ. GORHAM, A.B. DUNCAN, S.J. VOLLMER, AND L. MACKEY\nallow us to establish a near-linear relationship between the Stein discrepancy\nand the Wasserstein distance. By Theorem 5, M1\u2212\u03b9(\u2207gh) \u2264\n1\nK ( 1\n\u03b9 + sr) for\nK depending only on M1:3(\u03c3), M1:3(b), M0(\u03c3\u22121), and r. Hence, M2(gh,s) \u2264\nC\u03b9/s\u03b9 for C\u03b9 \u225c\n\u221a\nd\nK ( 1\n\u03b9 + sr). Following the derivation in Section E.1 and\nchoosing s\u2217=\n\u0010 \u03b9C\u03b9S(Qn,T ,G\u2225\u00b7\u22252)\n\u03b6\n\u0011\n1\n\u03b9+1 and \u03b7 \u225c\u03b6\n\u03b9 s\u2217, we obtain\ndW\u2225\u00b7\u22252(P, Qn) \u2264infs>0 S(Qn, T , G\u2225\u00b7\u22252) max(sr, \u03b2, C\u03b9s\u2212\u03b9) + s\u03b6\n(34)\n\u2264max(S(Qn, T , G\u2225\u00b7\u22252) max(sr, \u03b2), \u03b7) + \u03b7\u03b9\n\u22642 max(S(Qn, T , G\u2225\u00b7\u22252) max(sr, \u03b2), \u03b7).\nNow consider the case in which S(Qn, T , G\u2225\u00b7\u2225) < e\u22121 and the choice \u03b9 =\n1/ log(1/S(Qn,T ,G\u2225\u00b7\u2225)) \u2208(0, 1). Since x1/(log x\u22121) \u2264e for all x \u2208(0, e\u22121),\n1\n\u03b9 S(Qn, T , G\u2225\u00b7\u2225)\n1\n1+\u03b9 = log(1/S(Qn,T ,G\u2225\u00b7\u2225))S(Qn, T , G\u2225\u00b7\u2225)1+1/(log S(Qn,T ,G\u2225\u00b7\u2225)\u22121)\n\u2264e log(1/S(Qn,T ,G\u2225\u00b7\u2225))S(Qn, T , G\u2225\u00b7\u2225).\nIntroduce the shorthand c0 =\n\u221a\nd\nK\u03b6 . Since 1/1+\u03b9 \u2208(1/2, 1), we have c\n1\n1+\u03b9\n0\n\u2264\nmax(\u221ac0, c0). Similarly, 1 + sr\u03b9 > 1, so (1 + \u03b9sr)\n1\n1+\u03b9 \u22641 + \u03b9sr. Therefore,\n\u03b6\n\u03b9 S(Qn, T , G\u2225\u00b7\u2225)\n1\n1+\u03b9 ( 1+\u03b9sr\nK\u03b6/\n\u221a\nd)\n1\n1+\u03b9\n\u2264e\u03b6S(Qn, T , G\u2225\u00b7\u2225) log(1/S(Qn,T ,G\u2225\u00b7\u2225)) max\n\u0010\nd1/4\n\u221aK\u03b6 ,\n\u221a\nd\nK\u03b6\n\u0011\u0010\n1 +\nsr\nlog(1/S(Qn,T ,G\u2225\u00b7\u2225))\n\u0011\n= e S(Qn, T , G\u2225\u00b7\u2225) max\n\u0010\nd1/4\u221a\u03b6\n\u221a\nK ,\n\u221a\nd\nK\n\u0011\n(sr + log(1/S(Qn,T ,G\u2225\u00b7\u2225))).\nNext, \ufb01x any \u03b9 \u2208(0, 1) and consider the case in which S(Qn, T , G\u2225\u00b7\u2225) \u2265e\u22121\nso that S(Qn, T , G\u2225\u00b7\u2225)\n1\n1+\u03b9 \u2264S(Qn, T , G\u2225\u00b7\u2225)e\n\u03b9\n\u03b9+1 . Because 1\n\u03b9 e\n\u03b9\n\u03b9+1 \u22641\n2e1/2 < e\nand (1 + \u03b9sr)\n1/1+\u03b9 \u22641 + sr, we conclude that\n\u03b6\n\u03b9 S(Qn, T , G\u2225\u00b7\u2225)\n1\n1+\u03b9 ( 1+\u03b9sr\nK\u03b6/\n\u221a\nd)\n1\n1+\u03b9 \u2264e S(Qn, T , G\u2225\u00b7\u2225) max\n\u0010\nd1/4\u221a\u03b6\n\u221a\nK ,\n\u221a\nd\nK\n\u0011\n(sr + 1).\nThe result follows from estimates of these two cases and the bound (34).\nAPPENDIX F: PROOF OF PROPOSITION 8\nFix any g \u2208G\u2225\u00b7\u2225. Since EP [(T g)(Z)] = 0 by Proposition 3, we may write\n|EQn[(T g)(X)]| = |EQn[(T g)(X)] \u2212EP [(T g)(Z)]|\n= |2E[\u27e8b(X) \u2212b(Z), g(X)\u27e9+ \u27e8b(Z), g(X) \u2212g(Z)\u27e9]\n+ E[\u27e8m(X) \u2212m(Z), \u2207g(X)\u27e9+ \u27e8m(Z), \u2207g(X) \u2212\u2207g(Z)\u27e9]|.\n(35)\n37\nfor any coupling of X and Z. We obtain the \ufb01rst advertised inequality by\nrepeatedly applying the Fenchel-Young inequality for dual norms, invoking\nthe boundedness and Lipschitz constraints on g and \u2207g, and taking a supre-\nmum over g \u2208G\u2225\u00b7\u2225. The second inequality follows from the \ufb01rstby invoking\nJensen\u2019s inequality, the fact min(x, y) \u2264xty1\u2212t for all x, y \u22650, H\u00a8older\u2019s\ninequality, and \ufb01nally the de\ufb01nition of Ws,\u2225\u00b7\u2225.\nWe prove the \ufb01nal claim by bounding the \ufb01rst advertised inequality in a\nsecond manner. Let (X, Z) be coupled so that c \u225cmin(W1,\u2225\u00b7\u2225(Qn, P), 2) =\nmin(E[\u2225X \u2212Z\u2225], 2), A = 2\u2225b(Z)\u2225+ \u2225m(Z)\u2225, and B = min(\u2225X \u2212Z\u2225, 2).\nThe Fenchel-Young inequality (xy \u2264ex \u2212y + y log y for y \u22650, x \u2208R), the\nconcavity of x 7\u2192min(x, 2), and Jensen\u2019s inequality now yield the result as\nE[AB] = E[(A \u2212log(\u00b50/c))B] + E[B] log(\u00b50/c)\n\u2264E\n\u0002\neA\u2212log(\u00b50/c) \u2212B + B log(B)\n\u0003\n+ E[B] log(\u00b50/c)\n= c \u2212E[B log(e/B)] + E[B] log(\u00b50/c) \u2264c + c log(\u00b50/c) = c log(e\u00b50/c).\nAPPENDIX G: PROOF OF THEOREM 10\nFix any x, y \u2208Rd, and de\ufb01ne two It\u02c6o di\ufb00usions solving dZt,x = b(Zt,x) dt+\n\u03c3(Zt,x) dWt with Z0,x = x and dZt,y = b(Zt,y) dt+\u03c3(Zt,y) dWt with Z0,y = y,\nfor (Wt)t\u22650 a shared Wiener process. Applying Dynkin\u2019s formula to the\nfunction f(t, x) = ekt\u2225x\u22252\nG for the di\ufb00erence process Zt,x \u2212Zt,y yields\nE[f(t, Zt,x \u2212Zt,y)] = \u2225x \u2212y\u22252\nG + E[\nR t\n0 keks\u2225Zs,x \u2212Zs,y\u22252\nG ds]\n+ E[\nR t\n0 eks\u0000\u2225\u03c3(Zs,x) \u2212\u03c3(Zs,y)\u22252\nG + 2\u27e8b(Zs,x) \u2212b(Zs,y), G(Zs,x \u2212Zs,y)\u27e9\n\u0001\nds]\nBy the uniform dissipativity assumption, the right-hand side is at most\n\u2225x \u2212y\u22252\nG = dW\u2225\u00b7\u2225G(\u03b4x, \u03b4y)2. For the transition semigroup (Pt)t\u22650,\nE[f(t, Zt,x \u2212Zt,y)] = ektE\n\u0002\n\u2225Zt,x \u2212Zt,y\u22252\nG\n\u0003\n\u2265ektdW\u2225\u00b7\u2225G(\u03b4xPt, \u03b4yPt)2,\nby Cauchy-Schwarz. The result now follows from the fact that \u03bbmin(G1) \u2264\n\u2225z\u22252\nG/\u2225z\u22252\n2 \u2264\u03bbmax(G1) for all z \u0338= 0.\nAPPENDIX H: PROOF OF THEOREM 11\nAs in the proof of [93, Thm. 2.6], we \ufb01x two arbitrary starting points\nx, y \u2208Rd and de\ufb01ne a pair of coupled It\u02c6o di\ufb00usions (Zt,x)t\u22650 and (Zt,y)t\u22650,\neach with associated marginal semigroup (Pt)t\u22650. Speci\ufb01cally, we set Z0,x =\nx and Z0,y = y and let (Zt,x)t\u22650 and (Zt,y)t\u22650 solve the equations\ndZt,x = b(Zt,x) dt + \u03c30(Zt,x) dW \u2032\nt + \u03bb0 dW \u2032\u2032\nt\ndZt,y = b(Zt,y) dt + \u03c30(Zt,y) dW \u2032\nt + \u03bb0\n\u0000I \u22122\nZt,x\u2212Zt,y\n\u2225Zt,x\u2212Zt,y\u22252\nZt,x\u2212Z\u22a4\nt,y\n\u2225Zt,x\u2212Zt,y\u22252\n\u0001\ndW \u2032\u2032\nt ,\n38\nJ. GORHAM, A.B. DUNCAN, S.J. VOLLMER, AND L. MACKEY\nwhere (W \u2032\nt)t\u22650 is an m-dimensional Wiener process and (W \u2032\u2032\nt )t\u22650 is an inde-\npendent d-dimensional Wiener process.\nFollowing the argument of Eberle [22, Sec. 4], we de\ufb01ne the di\ufb00erence\nprocess Yt = Zt,x \u2212Zt,y, its norm rt = \u2225Yt\u22252, and the one-dimensional\nWiener process Wt =\nR t\n0\u27e8Ys/rs, dW \u2032\u2032\ns \u27e9, and apply the generalized It\u02c6o formula\n[49, Thm. 22.5] to obtain the stochastic di\ufb00erential equations\nd\u2225Yt\u22252\n2 = (2\u27e8Yt, b(Zt,x) \u2212b(Zt,y)\u27e9+ \u2225\u03c30(Zt,x) \u2212\u03c30(Zt,y)\u22252\nF + 4\u03bb2\n0) dt\n+ 2\u27e8Yt, (\u03c30(Zt,x) \u2212\u03c30(Zt,y)) dW \u2032\nt\u27e9+ 4\u03bb0\u2225Yt\u22252 dWt\nand\ndf(rt) = f\u2032(rt)/(rt)\u27e8Yt, (\u03c30(Zt,x) \u2212\u03c30(Zt,y)) dW \u2032\nt\u27e9+ 2\u03bb0f\u2032(rt) dWt\n+(f\u2032\u2032(rt)(2\u03bb2\n0 + 1\n2\u2225(\u03c30(Zt,x) \u2212\u03c30(Zt,y))\u22a4Yt\u22252\n2/r2\nt ) \u22121\n2\u03b1f\u2032(rt)\u03ba(rt)rt) dt\nfor any concave increasing f : [0, \u221e) 7\u2192[0, \u221e) with absolutely continuous\nderivative, f(0) = 0, and f\u2032(0) = 1. Since the drift term in the latter equation\nis bounded above by \u03b2t \u225c(2/\u03b1)(f\u2032\u2032(rt) \u2212(1/4)f\u2032(rt)\u03ba(rt)rt), the argument\nof [22, p. 15] shows that the results of [22, Thm. 1 and Cor. 2] hold for our\nchoice of \u03b1 and \u03ba.\nACKNOWLEDGMENTS\nWe thank Simon Lacoste-Julien for sharing his quadrature code, Martin\nHairer for discussing interpolation inequalities, Andreas Eberle for reading\nan earlier version of this manuscript, and Murat Erdogdu for identifying an\nimportant typographical error in an earlier version of this manuscript. This\nmaterial is based upon work supported by the grant EPSRC EP/N000188/1,\nthe National Science Foundation DMS RTG Grant No. 1501767, the Na-\ntional Science Foundation Graduate Research Fellowship under Grant No.\nDGE-114747, the Frederick E. Terman Fellowship, and the Lloyd\u2019s Regis-\nter Foundation programme on Data Centric engineering at the Alan Turing\nInstitute, UK.\nREFERENCES\n[1] L. Ambrosio, N. Fusco, and D. Pallara.\nFunctions of bounded variation and free\ndiscontinuity problems. Oxford University Press, 2000.\n[2] F. Bach, S. Lacoste-Julien, and G. Obozinski. On the equivalence between herding\nand conditional gradient algorithms. In Proc. 29th ICML, ICML\u201912, 2012.\n[3] A. D. Barbour. Stein\u2019s method and Poisson process convergence. J. Appl. Probab.,\n(Special Vol. 25A):175\u2013184, 1988. ISSN 0021-9002. A celebration of applied proba-\nbility.\n[4] A. D. Barbour. Stein\u2019s method for di\ufb00usion approximations. Probab. Theory Related\nFields, 84(3):297\u2013322, 1990. ISSN 0178-8051.\n[5] Q. W. Bouts, A. P. ten Brink, and K. Buchin.\nA framework for Computing the\nGreedy Spanner. In Proc. of 30th SOCG, pages 11:11\u201311:19, New York, NY, 2014.\nACM.\n39\n[6] S. Brooks, A. Gelman, G. Jones, and X.-L. Meng. Handbook of Markov chain Monte\nCarlo. CRC press, 2011.\n[7] P. Cattiaux and A. Guillin. Semi log-concave Markov di\ufb00usions. In S\u00b4eminaire de\nProbabilit\u00b4es XLVI, volume 2123 of Lecture Notes in Math., pages 231\u2013292. Springer,\nCham, 2014.\n[8] S. Cerrai. Second order PDE\u2019s in \ufb01nite and in\ufb01nite dimension: a probabilistic ap-\nproach, volume 1762. Springer Science & Business Media, 2001.\n[9] S. Chatterjee and E. Meckes. Multivariate normal approximation using exchangeable\npairs. ALEA Lat. Am. J. Probab. Math. Stat., 4:257\u2013283, 2008. ISSN 1980-0436.\n[10] S. Chatterjee and Q. Shao.\nNonnormal approximation by Stein\u2019s method of ex-\nchangeable pairs with application to the Curie-Weiss model. Ann. Appl. Probab., 21\n(2):464\u2013483, 2011. ISSN 1050-5164.\n[11] L. Chen, L. Goldstein, and Q. Shao. Normal approximation by Stein\u2019s method. Prob-\nability and its Applications. Springer, Heidelberg, 2011. ISBN 978-3-642-15006-7.\n[12] W. Y. Chen, L. Mackey, J. Gorham, F.-X. Briol, and C. Oates. Stein points. In Proc.\n35th ICML, ICML\u201918, 2018.\n[13] Y. Chen, M. Welling, and A. Smola. Super-samples from kernel herding. In UAI,\n2010.\n[14] P. Chew. There is a Planar Graph Almost As Good As the Complete Graph. In\nProc. 2nd SOCG, pages 169\u2013177, New York, NY, 1986. ACM.\n[15] K. Chwialkowski, H. Strathmann, and A. Gretton. A kernel test of goodness of \ufb01t.\nIn Proc. 33rd ICML, ICML, 2016.\n[16] C. Conca and M. Vanninathan. Periodic homogenization problems in incompressible\n\ufb02uid equations. Handbook of Mathematical Fluid Dynamics, 4:649\u2013698, 2007.\n[17] P. Doersek and J. Teichmann. A Semigroup Point Of View On Splitting Schemes For\nStochastic (Partial) Di\ufb00erential Equations. ArXiv e-prints, Nov. 2010.\n[18] P. D. Drummond and C. W. Gardiner. Generalised P-representations in quantum\noptics. J. Phys. A, 13(7):2353, 1980.\n[19] P. D. Drummond and D. F. Walls. Quantum theory of optical bistability. I. Nonlinear\npolarisability model. J. Phys. A, 13(2):725, 1980.\n[20] A. B. Duncan, T. Lelievre, and G. Pavliotis. Variance reduction using nonreversible\nLangevin samplers. J. Stat. Phys., 163(3):457\u2013491, 2016.\n[21] E. Dynkin. Markov Processes : Volume 1. Springer Berlin Heidelberg, Berlin, Hei-\ndelberg, 1965. ISBN 978-3-662-00033-5.\n[22] A. Eberle. Re\ufb02ection couplings and contraction rates for di\ufb00usions. Probab. Theory\nRelated Fields, pages 1\u201336, 2015.\n[23] K. Engel and R. Nagel. One-parameter semigroups for linear evolution equations,\nvolume 194 of Graduate Texts in Mathematics. Springer-Verlag, New York, 2000.\nISBN 0-387-98463-1.\n[24] S. N. Ethier and T. G. Kurtz. Markov processes. Wiley Series in Probability and\nMathematical Statistics: Probability and Mathematical Statistics. John Wiley &\nSons, Inc., New York, 1986. ISBN 0-471-08186-8.\n[25] Y. Fan, S. P. Brooks, and A. Gelman. Output assessment for Monte Carlo simulations\nvia the score statistic. J. Comp. Graph. Stat., 15(1), 2006.\n[26] X. Fang, Q.-M. Shao, and L. Xu. Multivariate approximations in wasserstein distance\nby steins method and bismuts formula. Probability Theory and Related Fields, pages\n1\u201335, 2018.\n[27] E. Fourni\u00b4e, J. Lasry, J. Lebuchoux, P. Lions, and N. Touzi. Applications of Malliavin\ncalculus to Monte Carlo methods in \ufb01nance. Fin. Stochastics, 3(4):391\u2013412, 1999.\nISSN 0949-2984.\n40\nJ. GORHAM, A.B. DUNCAN, S.J. VOLLMER, AND L. MACKEY\n[28] A. Friedman. Stochastic di\ufb00erential equations and applications. Vol. 1. Academic\nPress [Harcourt Brace Jovanovich, Publishers], New York-London, 1975. Probability\nand Mathematical Statistics, Vol. 28.\n[29] H. L. Gan, A. R\u00a8ollin, and N. Ross. Dirichlet approximation of equilibrium distri-\nbutions in Cannings models with mutation. Advances in Applied Probability, 49(3):\n927\u2013959, 2017.\n[30] R. E. Gaunt. Rates of convergence in normal approximation under moment conditions\nvia new bounds on solutions of the stein equation. J. Theoret. Probab., 29(1):231\u2013247,\n2016.\n[31] A. Gelman. Multilevel (hierarchical) modeling: what it can and cannot do. Techno-\nmetrics, 2012.\n[32] A. Gelman, J. Carlin, H. Stern, D. Dunson, A. Vehtari, and D. Rubin. Bayesian\ndata analysis. Texts in Statistical Science Series. CRC Press, Boca Raton, FL, third\nedition, 2014. ISBN 978-1-4398-4095-5.\n[33] M. Girolami and B. Calderhead. Riemann Manifold Langevin and Hamiltonian Monte\nCarlo methods. J. R. Stat. Soc. Ser. B, 73(2):123\u2013214, 2011.\n[34] G. Glaeser.\n\u00b4Etude de quelques alg`ebres tayloriennes.\nJ. Analyse Math., 6:1\u2013124;\nerratum, insert to 6 (1958), no. 2, 1958.\n[35] J. Gorham and L. Mackey. Measuring sample quality with Stein\u2019s method. In Adv.\nNIPS 28, pages 226\u2013234. 2015.\n[36] J. Gorham and L. Mackey. Measuring sample quality with kernels. In Proc. of 34st\nICML, ICML\u201917, 2017.\n[37] F. G\u00a8otze. On the rate of convergence in the multivariate CLT. Ann. Probab., 19(2):\n724\u2013739, 1991.\n[38] A. Gretton, K. Borgwardt, M. Rasch, B. Sch\u00a8olkopf, and A. Smola. A kernel method\nfor the two-sample-problem. In Adv. NIPS 19, pages 513\u2013520, 2006.\n[39] J. Gudmundsson, O. Klein, C. Knauer, and M. Smid. Small Manhattan Networks\nand Algorithmic Applications for the Earth Movers Distance. In Proc. 23rd EuroCG,\npages 174\u2013177, 2007.\n[40] M. Hairer, A. Stuart, and S. Vollmer. Spectral gaps for a Metropolis\u2013Hastings algo-\nrithm in in\ufb01nite dimensions. Ann. Appl. Probab., 2014.\n[41] S. Har-Peled and M. Mendel. Fast construction of nets in low-dimensional metrics\nand their applications. SIAM J. Comput., 35(5):1148\u20131184, 2006.\n[42] R. Hartley and A. Zisserman. Multiple View Geometry in Computer Vision. Cam-\nbridge University Press, ISBN: 0521540518, second edition, 2004.\n[43] A. Horowitz. The second order Langevin equation and numerical simulations. Nucl.\nPhys. B, 280:510\u2013522, 1987.\n[44] P. Huber and E. Ronchetti. Robust statistics. Wiley Series in Probability and Statis-\ntics. John Wiley & Sons, Inc., Hoboken, NJ, second edition, 2009. ISBN 978-0-470-\n12990-6.\n[45] J. Huggins and J. Zou.\nQuantifying the accuracy of approximate di\ufb00usions and\nMarkov chains. In Proc. 20th AISTATS, pages 382\u2013391, 2017.\n[46] J. H. Huggins and L. Mackey. Random feature Stein discrepancies. In Adv. NIPS\n31, 2018.\n[47] C. Hwang, S. Hwang-Ma, and S. Sheu. Accelerating Gaussian di\ufb00usions. Ann. Appl.\nProbab., pages 897\u2013913, 1993.\n[48] A. Joulin and Y. Ollivier. Curvature, concentration and error estimates for Markov\nchain Monte Carlo. Ann. Probab., 38(6):2418\u20132442, 11 2010.\n[49] O. Kallenberg. Foundations of modern probability. Probability and its Applications.\nSpringer-Verlag, New York, second edition, 2002. ISBN 0-387-95313-2.\n41\n[50] J. Kent. Time-reversible di\ufb00usions. Ann. Appl. Probab., pages 819\u2013835, 1978.\n[51] R. Khasminskii. Stochastic stability of di\ufb00erential equations, volume 66 of Stochastic\nModelling and Applied Probability. Springer, Heidelberg, second edition, 2012. ISBN\n978-3-642-23279-4. With contributions by G. N. Milstein and M. B. Nevelson.\n[52] A. Korattikara, Y. Chen, and M. Welling. Austerity in MCMC land: Cutting the\nMetropolis-Hastings budget. In Proc. of 31st ICML, ICML\u201914, 2014.\n[53] S. Lacoste-Julien, F. Lindsten, and F. Bach. Sequential kernel herding: Frank-Wolfe\noptimization for particle \ufb01ltering. In AISTATS, 2015.\n[54] C. Landim, S. Olla, and H. Yau.\nConvection\u2013di\ufb00usion equation with space\u2013time\nergodic random \ufb02ow. Probability theory and related \ufb01elds, 112(2):203\u2013220, 1998.\n[55] C. Ley, G. Reinert, and Y. Swan. Stein\u2019s method for comparison of univariate distri-\nbutions. Probab. Surveys, 14:1\u201352, 2017.\n[56] C. Liu. Bayesian robust multivariate linear regression with incomplete data. JASA,\n91(435):1219\u20131227, 1996. ISSN 01621459.\n[57] Q. Liu and J. Lee. Black-box Importance Sampling. In Proc. 20th AISTATS, pages\n952\u2013961, 2017.\n[58] Q. Liu and D. Wang. Stein variational gradient descent: A general purpose bayesian\ninference algorithm. In Adv. NIPS 29, pages 2378\u20132386, 2016.\n[59] Q. Liu, J. Lee, and M. Jordan. A kernelized Stein discrepancy for goodness-of-\ufb01t\ntests. In Proc. of 33rd ICML, volume 48 of ICML, pages 276\u2013284, 2016.\n[60] M. Lubin and I. Dunning. Computing in operations research using Julia. INFORMS\nJournal on Computing, 27(2):238\u2013248, 2015.\n[61] A. Lunardi. An introduction to interpolation theory. 2007.\n[62] Y. Ma, T. Chen, and E. Fox. A complete recipe for stochastic gradient MCMC. In\nAdv. NIPS 28, pages 2899\u20132907, 2015.\n[63] L. Mackey and J. Gorham.\nMultivariate Stein factors for a class of strongly log-\nconcave distributions. Electron. Commun. Probab., 21:14 pp., 2016.\n[64] L. Manca. Kolmogorov operators in spaces of continuous functions and equations for\nmeasures. PhD thesis, Scuola Normale Superiore di Pisa, 2008.\n[65] J. Mattingly, A. Stuart, and M. Tretyakov. Convergence of numerical time-averaging\nand stationary measures via Poisson equations. SIAM J. Numer. Anal., 48(2):552\u2013\n577, 2010.\n[66] E. Meckes.\nOn Stein\u2019s method for multivariate normal approximation.\nIn High\ndimensional probability V: the Luminy volume, volume 5 of Inst. Math. Stat. Collect.,\npages 153\u2013178. Inst. Math. Statist., Beachwood, OH, 2009.\n[67] A. M\u00a8uller. Integral probability metrics and their generating classes of functions. Ann.\nAppl. Probab., 29(2):pp. 429\u2013443, 1997.\n[68] I. Nourdin, G. Peccati, and A. R\u00b4eveillac. Multivariate normal approximation using\nStein\u2019s method and Malliavin calculus. Ann. Inst. Henri Poincar\u00b4e Probab. Stat., 46\n(1):45\u201358, 2010. ISSN 0246-0203.\n[69] C. J. Oates, M. Girolami, and N. Chopin. Control functionals for Monte Carlo inte-\ngration. Journal of the Royal Statistical Society: Series B (Statistical Methodology),\npages n/a\u2013n/a, 2016. ISSN 1467-9868.\n[70] B. Oksendal.\nStochastic di\ufb00erential equations: an introduction with applications.\nSpringer Science & Business Media, 6 edition, 2013.\n[71] G. Optimization.\nGurobi optimizer reference manual, 2015.\nURL http://www.\ngurobi.com.\n[72] E. Pardoux and A. Veretennikov. On the Poisson equation and di\ufb00usion approxima-\ntion. i. Ann. Probab., pages 1061\u20131085, 2001.\n[73] S. Patterson and Y. Teh. Stochastic gradient Riemannian langevin dynamics on the\n42\nJ. GORHAM, A.B. DUNCAN, S.J. VOLLMER, AND L. MACKEY\nprobability simplex. In Adv. NIPS 26, pages 3102\u20133110, 2013.\n[74] G. A. Pavliotis. Stochastic processes and applications, volume 60 of Texts in Applied\nMathematics. Springer, New York, 2014. ISBN 978-1-4939-1322-0; 978-1-4939-1323-7.\nDi\ufb00usion processes, the Fokker-Planck and Langevin equations.\n[75] D. Peleg and A. Sch\u00a8a\ufb00er. Graph spanners. J. Graph Theory, 13(1):99\u2013116, 1989.\n[76] P. Protter. Stochastic integration and di\ufb00erential equations, volume 21 of Stochastic\nModelling and Applied Probability. Springer-Verlag, Berlin, 2005. ISBN 3-540-00313-\n4. Second edition. Version 2.1, Corrected third printing.\n[77] M. Rai\u02c7c. A multivariate CLT for decomposable random vectors with \ufb01nite second\nmoments. Journal of Theoretical Probability, 17(3):573\u2013603, 2004.\n[78] R. Ranganath, D. Tran, J. Altosaar, and D. Blei. Operator variational inference. In\nAdvances in Neural Information Processing Systems, pages 496\u2013504, 2016.\n[79] G. Reinert and A. R\u00a8ollin. Multivariate normal approximation with Stein\u2019s method\nof exchangeable pairs under a general linearity condition. Ann. Probab., 37(6):2150\u2013\n2173, 2009. ISSN 0091-1798.\n[80] L. Rey-Bellet and K. Spiliopoulos. Irreversible Langevin samplers and variance re-\nduction: a large deviation approach. arXiv preprint arXiv:1404.0105, 2014.\n[81] H. Risken.\nThe Fokker-Planck Equation: Methods of Solutions and Applications.\nSpringer Series in Synergetics. Springer, 2nd ed. 1989. 3rd printing edition, Sept.\n1996. ISBN 354061530X.\n[82] G. Roberts and R. Tweedie. Exponential convergence of Langevin distributions and\ntheir discrete approximations. Bernoulli, 2(4):341\u2013363, 1996. ISSN 1350-7265.\n[83] G. O. Roberts and O. Stramer. Langevin di\ufb00usions and Metropolis-Hastings algo-\nrithms. 4(4):337\u2013357, 2002.\n[84] M. R\u00a8ockner, Z. Sobol, et al.\nKolmogorov equations in in\ufb01nite dimensions: well-\nposedness and regularity of solutions, with applications to stochastic generalized\nburgers equations. Ann. Probab., 34(2):663\u2013727, 2006.\n[85] P. Shvartsman. The Whitney extension problem and Lipschitz selections of set-valued\nmappings in jet-spaces. Trans. Amer. Math. Soc., 360(10):5529\u20135550, 2008.\n[86] G. So\ufb00ritti and G. Galimberti. Multivariate linear regression with non-normal errors:\na solution based on mixture models. Statistics and Computing, 21(4):523\u2013536, 2011.\n[87] C. Stein. A bound for the error in the normal approximation to the distribution of a\nsum of dependent random variables. In Proc. 6th Berkeley Symposium on Mathemat-\nical Statistics and Probability (Univ. California, Berkeley, Calif., 1970/1971), Vol.\nII: Probability theory, pages 583\u2013602. Univ. California Press, Berkeley, Calif., 1972.\n[88] C. Stein, P. Diaconis, S. Holmes, and G. Reinert.\nUse of exchangeable pairs in\nthe analysis of simulations. In Stein\u2019s method: expository lectures and applications,\nvolume 46 of IMS Lecture Notes Monogr. Ser., pages 1\u201326. Inst. Math. Statist.,\nBeachwood, OH, 2004.\n[89] A. Stuart, J. Voss, P. Wilberg, et al. Conditional path sampling of SDEs and the\nLangevin MCMC method. Commun. Math. Sci., 2(4):685\u2013697, 2004.\n[90] Y. Teh, A. Thi\u00b4ery, and S. Vollmer. Consistency and \ufb02uctuations for stochastic gra-\ndient Langevin dynamics. arXiv:1409.0578, 2014.\n[91] S. Vallender. Calculation of the Wasserstein distance between probability distribu-\ntions on the line. Theory Probab. Appl., 18(4):784\u2013786, 1974.\n[92] S. Vollmer, K. Zygalakis, and Y. Teh. Exploration of the (non-)asymptotic bias and\nvariance of stochastic gradient langevin dynamics. J. Mach. Learn. Res., 17(159):\n1\u201348, 2016.\n[93] F. Wang. Exponential Contraction in Wasserstein Distances for Di\ufb00usion Semigroups\nwith Negative Curvature. ArXiv e-prints, Mar. 2016.\n43\n[94] A. Zellner. Bayesian and Non-Bayesian analysis of the regression model with multi-\nvariate student-t error terms. JASA, 71(354):400\u2013405, 1976.\n[95] A. Zellner and C. Min. Gibbs sampler convergence criteria. JASA, 90(431):921\u2013927,\n1995.\n44\nJ. GORHAM, A.B. DUNCAN, S.J. VOLLMER, AND L. MACKEY\nSUPPLEMENTARY APPENDIX I: SMOOTHING AND INTERPOLATION\nWe present in this section two essentially standard results on smoothing by convolution and\nseminorm interpolation [see, e.g., 61, Ex. 1.1.8] which support the proof of Theorem 7. Throughout,\nwe let G \u2208Rd be a standard normal vector and \u03c6 \u2208C\u221ebe its probability density. For any s > 0\nand function f : Rd \u2192R we de\ufb01ne\nfs(x) \u225cE[f(x + sG)] = s\u2212d R\nf(y)\u03c6\n\u0000 x\u2212y\ns\n\u0001\ndy.\nThe \ufb01rst result bounds the Lipschitz constant of fs in terms of the H\u00a8older continuity of f.\nLemma 18 (Smoothing by convolution II).\nFix \u03b9 \u2208(0, 1) and consider any f : Rd \u2192R with\nM1\u2212\u03b9(f) < \u221e. For all s > 0,\nM1(fs) \u2264E\n\u0002\n\u2225G\u22252\u22122\u03b9\n2\n\u00031/2M1\u2212\u03b9(f)s\u2212\u03b9.\nProof. Fix any \u2225v\u22252 \u22641 and x \u2208Rd. Leibniz\u2019s rule implies that\n\u27e8\u2207fs(x), v\u27e9= s\u2212d\u22121 R\nf(y)\u27e8\u2207\u03c6\n\u0000 x\u2212y\ns\n\u0001\n, v\u27e9dy.\nBecause s\u2212d R\n\u2207\u27e8\u03c6( x\u2212y\ns ), v\u27e9dy = 0 for any v \u2208Rd, we also have\n|\u27e8\u2207fs(x), v\u27e9| = |s\u2212d\u22121 R\nf(y)\u27e8\u2207\u03c6\n\u0000 x\u2212y\ns\n\u0001\n, v\u27e9dy| = |s\u2212d\u22121 R\n[f(y) \u2212f(x)]\u27e8\u2207\u03c6\n\u0000 x\u2212y\ns\n\u0001\n, v\u27e9dy|\n= |s\u2212d\u22121 R\n[f(x \u2212z) \u2212f(x)]\u27e8\u2207\u03c6\n\u0000 z\ns\n\u0001\n, v\u27e9dz|\n\u2264s\u2212d\u22121 R\nM1\u2212\u03b9(f)\u2225z\u22251\u2212\u03b9\n2\n|\u27e8\u2207\u03c6\n\u0000 z\ns\n\u0001\n, v\u27e9| dz\n= M1\u2212\u03b9(f)s\u2212\u03b9 R\n\u2225\u03c9\u22251\u2212\u03b9\n2\n|\u27e8\u2207\u03c6(\u03c9), v\u27e9| d\u03c9,\nwhere we have used substitutions z \u225cx \u2212y and \u03c9 \u225cz/s. Finally, as \u2207\u03c6(\u03c9) = \u2212\u03c9\u03c6(\u03c9) for all\n\u03c9 \u2208Rd, we can use the spherical symmetry of the standard normal and Cauchy-Schwarz to yield\nR\n\u2225\u03c9\u22251\u2212\u03b9\n2\n|\u27e8\u2207\u03c6(\u03c9), v\u27e9| d\u03c9 = E\n\u0002\n\u2225G\u22251\u2212\u03b9\n2\n|\u27e8G, v\u27e9|\n\u0003\n\u2264E\n\u0002\n\u2225G\u22252\u22122\u03b9\n2\n\u00031/2E\n\u0002\n|\u27e8G, v\u27e9|2\u00031/2\n= E\n\u0002\n\u2225G\u22252\u22122\u03b9\n2\n\u00031/2E\n\u0002\nG2\n1\n\u00031/2 = E\n\u0002\n\u2225G\u22252\u22122\u03b9\n2\n\u00031/2,\nconcluding the lemma.\nThe second result provides interpolation bounds for the H\u00a8older seminorm Mk where k \u0338\u2208N.\nLemma 19 (Seminorm interpolation).\nLet k > 0 and f \u2208C\u2308k\u2309(Rd). Then we have that\nMk(f) \u226421\u2212{k}\u0000M\u2308k\u2309\u22121(f)\n\u00011\u2212{k}\u0000M\u2308k\u2309(f)\n\u0001{k}.\n45\nProof. For m \u2208N, let Vm = {(v1, . . . , vm) : \u2225vi\u22252 \u22641 for each i \u2208{1, . . . , m}}. Using the\nfundamental theorem of calculus we obtain\nsupV\u2308k\u2309\u22121\n\f\f\f\u2207\u2308k\u2309\u22121f(x)[v1, v2, . . . , v\u2308k\u2309\u22121] \u2212\u2207\u2308k\u2309\u22121f(y)[v1, v2, . . . , v\u2308k\u2309\u22121]\n\f\f\f\n= supV\u2308k\u2309\u22121\n\f\f\f\nR 1\n0 \u2207\u2308k\u2309f(x + s(y \u2212x))[v1, v2, . . . , v\u2308k\u2309\u22121, y \u2212x]ds\n\f\f\f\n\u2264supV\u2308k\u2309\u22121\n\f\f\f supz \u2207\u2308k\u2309f(z)[v1, v2, . . . , v\u2308k\u2309\u22121, y \u2212x]\n\f\f\f\n\u2264supz \u2225\u2207\u2308k\u2309f(z)\u2225op\u2225x \u2212y\u22252.\nAn application of the triangle inequality gives rise to\nsupV\u2308k\u2309\u22121\n\f\f\u2207\u2308k\u2309\u22121f(x)[v1, v2, . . . , v\u2308k\u2309\u22121] \u2212\u2207\u2308k\u2309\u22121f(y)[v1, v2, . . . , v\u2308k\u2309\u22121]\n\f\f \u22642 supz \u2225\u2207\u2308k\u2309\u22121f(z)\u2225op.\nThere we obtain\nMk(f) = supx,y\u2208Rd;x\u0338=y\n\u2225\u2207\u2308k\u2309\u22121f(x)\u2212\u2207\u2308k\u2309\u22121f(y)\u2225op\n\u2225x\u2212y\u2225{k}\n2\n\u2264supx,y\u2208Rd;x\u0338=y\n21\u2212{k}(supz \u2225\u2207\u2308k\u2309f(z)\u2225op)\n{k}(supz \u2225\u2207\u2308k\u2309\u22121f(z)\u2225op)\n1\u2212{k}\u2225x\u2212y\u2225{k}\n2\n\u2225x\u2212y\u2225{k}\n2\n\u226421\u2212{k}\u0000supz \u2225\u2207\u2308k\u2309f(z)\u2225op\n\u0001{k}\u0000supz \u2225\u2207\u2308k\u2309\u22121f(z)\u2225op\n\u00011\u2212{k}\n\u226421\u2212{k}\u0000M\u2308k\u2309(f)\n\u0001{k}\u0000M\u2308k\u2309\u22121(f)\n\u00011\u2212{k}\nthus proving the statement.\nSUPPLEMENTARY APPENDIX J: SEMIGROUP THIRD DERIVATIVE ESTIMATE\nLemma 20 (Semigroup third derivative estimate).\nSuppose that the drift and di\ufb00usion coe\ufb03-\ncients b and \u03c3 of an It\u02c6o di\ufb00usion have bounded, locally Lipschitz \ufb01rst, second, and third derivatives.\nIf the transition semigroup (Pt)t\u22650 has Wasserstein decay rate r, \u03c3(x) has a right inverse \u03c3\u22121(x)\nfor each x \u2208Rd, and M0(\u03c3\u22121) < \u221e, then, for all t > 0 and any f \u2208C3 with bounded second and\nthird derivatives,\nM3(Ptf) \u2264inft0\u2208(0,t] M1(f)r(t \u2212t0) c\nt0 eCt0\n(36)\nfor constants c, C depending only on M1:3(\u03c3), M1:3(b), M0(\u03c3\u22121), and r.\nProof. Our proof closely follows that of Lemma 15 in Section C, and we will only highlight the\nimportant di\ufb00erences. Throughout, c and C will represent arbitrary constants depending only on\nM1:3(\u03c3), M1:3(b), M0(\u03c3\u22121), and r that may change from expression to expression.\nFix any v, v\u2032, v\u2032\u2032 with unit Euclidean norms in Rd and, without loss of generality, \ufb01x any f : Rd \u2192R\nin C3 with bounded \ufb01rst, second, and third derivatives. Let (Zt,x)t\u22650 be an It\u02c6o di\ufb00usion solving the\n46\nJ. GORHAM, A.B. DUNCAN, S.J. VOLLMER, AND L. MACKEY\nstochastic di\ufb00erential equation (5) with starting point Z0,x = x, underlying Wiener process (Wt)t\u22650,\nand transition semigroup (Pt)t\u22650. Under our smoothness assumptions on b and \u03c3, [76, Theorem V.40]\nimplies that (Zt,x)t\u22650 is thrice continuously di\ufb00erentiable in x with third directional derivative \ufb02ow\n(Yt,v,v\u2032,v\u2032\u2032)t\u22650 solving the third variation equation,\ndYt,v,v\u2032,v\u2032\u2032 = \u2207b(Zt,x)Yt,v,v\u2032,v\u2032\u2032 dt + \u22072b(Zt,x)[Ut,v,v\u2032]Vt,v\u2032\u2032 dt\n(37)\n+ \u22073b(Zt,x)[Vt,v, Vt,v\u2032, Vt,v\u2032\u2032] dt + \u22072b(Zt,x)[Ut,v\u2032,v\u2032\u2032]Vt,v dt\n+ \u22072b(Zt,x)[Ut,v,v\u2032\u2032]Vt,v\u2032 dt\n+ \u2207\u03c3(Zt,x)Yt,v,v\u2032,v\u2032\u2032 dWt + \u22072\u03c3(Zt,x)[Ut,v,v\u2032]Vt,v\u2032\u2032 dWt\n+ \u22073\u03c3(Zt,x)[Vt,v, Vt,v\u2032, Vt,v\u2032\u2032] dWt + \u22072\u03c3(Zt,x)[Ut,v\u2032,v\u2032\u2032]Vt,v dWt\n+ \u22072\u03c3(Zt,x)[Ut,v,v\u2032\u2032]Vt,v\u2032 dWt\nwith\nY0,v,v\u2032,v\u2032\u2032 = 0,\nobtained by di\ufb00erentiating (24) with respect to x in the direction v\u2032\u2032.\nIn a manner analogous to the derivation of (28) in proof of Lemma 15, we can derive an expression\nfor the third derivative of the semi-group,\n\u22073(Ptf)(x)[v, v\u2032, v\u2032\u2032] = E\nhP\ni,j Ji,j,x\ni\nfor\n(38)\nJ1,1,x\u225c1\nt \u27e8\u22072f(Zt,x)Vt,v\u2032\u2032, Vt,v\u2032\u27e9\nR t\n0\u27e8\u03c3\u22121(Zs,x)Vs,v, dWs\u27e9,\nJ1,2,x\u225c1\nt \u27e8\u2207f(Zt,x), Ut,v\u2032,v\u2032\u2032\u27e9\nR t\n0\u27e8\u03c3\u22121(Zs,x)Vs,v, dWs\u27e9,\nJ1,3,x\u225c1\nt \u27e8\u2207f(Zt,x), Vt,v\u2032\u27e9\nR t\n0\u27e8\u2207\u03c3\u22121(Zs,x)[Vs,v\u2032\u2032]Vs,v, dWs\u27e9,\nJ2,1x\u225c1\nt \u27e8\u2207f(Zt,x), Vt,v\u2032\u2032\u27e9\nR t\n0\u27e8\u2207\u03c3\u22121(Zs,x)[Vs,v\u2032]Vs,v, dWs\u27e9,\nJ2,2,x\u225c1\nt f(Zt,x)\nR t\n0\u27e8\u22072\u03c3\u22121(Zs,x)[Vs,v\u2032\u2032][Vs,v\u2032]Vs,v, dWs\u27e9,\nJ2,3,x\u225c1\nt f(Zt,x)\nR t\n0\u27e8\u2207\u03c3\u22121(Zs,x)[Us,v\u2032,v\u2032\u2032]Vs,v, dWs\u27e9,\nJ2,4,x\u225c1\nt f(Zt,x)\nR t\n0\u27e8\u2207\u03c3\u22121(Zs,x)[Vs,v\u2032]Us,v,v\u2032\u2032, dWs\u27e9,\nJ3,1,x\u225c1\nt \u27e8\u2207f(Zt,x), Vt,v\u2032\u2032\u27e9\nR t\n0\u27e8\u03c3\u22121(Zs,x)Us,v,v\u2032, dWs\u27e9,\nJ3,2,x\u225c1\nt f(Zt,x)\nR t\n0\u27e8\u2207\u03c3\u22121(Zs,x)[Vs,v\u2032\u2032]Us,v,v\u2032, dWs\u27e9,\nJ3,3,x\u225c1\nt f(Zt,x)\nR t\n0\u27e8\u03c3\u22121(Zs,x)Ys,v,v\u2032,v\u2032\u2032, dWs\u27e9,\nJ3,4,x\u225c1\nt \u27e8\u2207f(Zt,x), Vt,v\u2032\u27e9\nR t\n0\u27e8\u03c3\u22121(Zs,x)Us,v,v\u2032\u2032, dWs\u27e9.\nWe will bound each term Ji,j,x in (38) in turn.\nJ.1. The J1,\u00b7,x terms.\nWe will provide a step-by-step calculation for the \ufb01rst term. By Cauchy-\nSchwarz,\nE[J1,1,x]= 1\nt E\nh\n\u27e8\u22072f(Zt,x)Vt,v\u2032\u2032, Vt,v\u2032\u27e9\nR t\n0\u27e8\u03c3\u22121(Zs,x)Vs,v, dWs\u27e9\ni\n\u22641\nt\nq\nE\n\u0002\n\u27e8\u22072f(Zt,x)Vt,v\u2032\u2032, Vt,v\u2032\u27e92\u0003\nE[(\nR t\n0\u27e8\u03c3\u22121(Zs,x)Vs,v, dWs\u27e9)2].\n47\nWe use the derivative \ufb02ow bounds of Lemma 16 to realize\nq\nE\n\u0002\n\u2225Vt,v\u2032\u22252\n2\u2225Vt,v\u2032\u2032\u22252\n2\n\u0003\n\u2264\n4q\nE\u2225Vt,v\u2032\u22254\n2E\u2225Vt,v\u2032\u2032\u22254\n2 \u2264\u2225v\u2032\u22252\u2225v\u2032\u2032\u22252e\n1\n2 t\u03b34.\nCauchy-Schwarz, the It\u02c6o isometry [28, Eqs. 7.1 and 7.2], and Lemma 16 now yield\nE[J1,1,x] \u22641\nt M2(f)\nr\nE\n\u0002\n\u2225Vt,v\u2032\u22252\n2\u2225Vt,v\u2032\u2032\u22252\n2\n\u0003\nE\nhR t\n0 \u2225\u03c3\u22121(Zs,x)Vs,v\u22252\n2 ds\ni\n\u22641\nt M2(f)\u2225v\u2032\u22252\u2225v\u2032\u2032\u22252\u2225v\u22252e\n1\n2 t\u03b34M0(\u03c3\u22121)\n\u0010\n1\n\u03b32 (e\u03b32t \u22121)\n\u0011 1\n2 \u2264M2(f) c\n\u221a\nteCt.\nSimilar reasoning yields\nE[J1,2,x]= E 1\nt \u27e8\u2207f(Zt,x), Ut,v\u2032,v\u2032\u2032\u27e9\nR t\n0\u27e8\u03c3\u22121(Zs,x)Vs,v, dWs\u27e9\n\u22641\nt M1(f)\n\u221a\nte\n1\n4 t\u03b34M0(\u03c3\u22121)\n\u0010\n1\n\u03b32 (e\u03b32t \u22121)\n\u0011 1\n2 \u2264M1(f)ceCt\nand using equation (30)\nE[J1,3,x] \u22641\nt M1(f)e\n1\n2 \u03b32t\u2225v\u2032\u22252\nq\nE\nR t\n0 M1(\u03c3\u22121)2\u2225Vs,v\u22252\n2\u2225Vs,v\u2032\u2032\u22252\n2 ds\n\u22641\nt M1(f)e\n1\n2 \u03b32t\u2225v\u2032\u22252M1(\u03c3\u22121)\u2225v\u22252\u2225v\u2032\u2032\u22252\n\u0010R t\n0 e\u03b34sds\n\u0011 1\n2\n\u2264t\u22121\n2 M1(f)e\u03b32t/2\u2225v\u2032\u22252M0(\u03c3\u22121)2M1(\u03c3)\u2225v\u22252\u2225v\u2032\u2032\u22252e\u03b34t/2 \u2264M1(f) c\n\u221a\nteCt.\nJ.2. The J2,\u00b7,x terms.\nThe bound E[J2,1,x] \u2264M1(f) c\n\u221a\nteCt follows exactly as it did for J1,3,x.\nTo tackle the remaining J2,\u00b7,x terms, we will rewrite the unbounded quantity f(Zt,x) using (29). We\nobtain the bound\nEJ2,2,x= E 1\nt f(Zt,x)\nR t\n0\u27e8\u22072\u03c3\u22121(Zs,x)[Vs,v\u2032\u2032][Vs,v\u2032]Vs,v, dWs\u27e9\n= E 1\nt\nR t\n0\u27e8\u2207(Pt\u2212sf)(Zs,x), \u03c3(Zs,x) dWs\u27e9.\nR t\n0\u27e8\u22072\u03c3\u22121(Zs,x)[Vs,v\u2032\u2032][Vs,v\u2032]Vs,v, dWs\u27e9\n= 1\nt E\nR t\n0\u27e8\u2207(Pt\u2212sf)(Zs,x), \u03c3(Zs,x)\u22072\u03c3\u22121(Zs,x)[Vs,v\u2032\u2032][Vs,v\u2032]Vs,v\u27e9ds\n\u22641\nt M1(f)r(0)\n\u00002M1(\u03c3)2M0(\u03c3\u22121)2 + M2(\u03c3)M0(\u03c3\u22121)\n\u0001 R t\n0 E\n\u0002\n\u2225Vs,v\u2032\u2032\u22252\u2225Vs,v\u2032\u22252\u2225Vs,v\u2032\u22252\n\u0003\nds.\n\u2264M1(f)r(0)\n\u00002M1(\u03c3)2M0(\u03c3\u22121)2 + M2(\u03c3)M0(\u03c3\u22121)\n\u0001\ne\u03b33t\u2225v\u2032\u2032\u22252\u2225v\u2032\u22252\u2225v\u22252 \u2264M1(f)ceCt,\nwhere we used the chain rule expression\n\u22072\u03c3\u22121(x)[v][v\u2032] = \u2212\u03c3(x)\u22121\u0010\n\u2212\u2207\u03c3(x)[v]\u03c3(x)\u22121\u2207\u03c3[v\u2032](x) \u2212\u2207\u03c3(x)[v\u2032]\u03c3(x)\u22121\u2207\u03c3(x)[v]\n+ \u22072\u03c3(x)[v][v\u2032]\n\u0011\n\u03c3(x)\u22121\n48\nJ. GORHAM, A.B. DUNCAN, S.J. VOLLMER, AND L. MACKEY\nto rewrite \u03c3(Zs,x)\u22072\u03c3\u22121(Zs,x). The next term satis\ufb01es\nE[J2,3,x] = E 1\nt\nR t\n0\u27e8\u2207(Pt\u2212sf)(Zs,x), \u03c3(Zs,x) dWs\u27e9\nR t\n0\u27e8\u2207\u03c3\u22121(Zs,x)[Us,v\u2032,v\u2032\u2032]Vs,v, dWs\u27e9\n= E 1\nt\nR t\n0\u27e8\u2207(Pt\u2212sf)(Zs,x), \u03c3(Zs,x)\u2207\u03c3\u22121(Zs,x)[Us,v\u2032,v\u2032\u2032]Vs,v\u27e9ds\n\u22641\nt M1(f)M0(\u03c3\u22121)M1(\u03c3)r(x)\nR t\n0 E\u2225Us,v\u2032,v\u2032\u2032\u2225\u2225Vs,v\u2225ds\n\u2264M1(f)M0(\u03c3\u22121)M1(\u03c3)r(x) 1\nt\nR t\n0 \u2225v\u2225\u2225v\u2032\u2225\u2225v\u2032\u2032\u2225(\u03b1se\u03b34s)\n1\n2 es\u03b32/2ds \u2264M1(f)ceCt.\nThe term E[J2,4,x] can be bounded in the same way by swapping the roles of v and v\u2032.\nJ.3. The J3,\u00b7 terms.\nCauchy-Schwarz and the It\u02c6o isometry [28, Eqs. 7.1 and 7.2] yield\nE[J3,1,x] \u22641\nt M1(f)M0(\u03c3\u22121)\nr\nE\n\u0002\n\u2225Vt,v\u2032\u2032\u22252\n2\n\u0003\nE\nhR t\n0 \u2225Us,v,v\u2032\u22252\n2 ds\ni\n\u22641\nt M1(f)M0(\u03c3\u22121)e\n1\n2 t\u03b32\n\u0010\n1\n\u03b32 (e\u03b32t \u22121)\n\u0011 1\n2 \u2264M1(f) c\n\u221a\nteCt.\nThe bound for E[J3,4,x] is identical, and the bound E[J3,2,x] \u2264M1(f)ceCt follows exactly as it did\nfor J2,3,x. Now we consider the last term\nJ3,3,x= 1\nt f(Zt,x)\nR t\n0\u27e8\u03c3\u22121(Zs,x)Ys,v,v\u2032,v\u2032\u2032, dWs\u27e9,\nUsing (29), Lemma 15, and the non-increasing property of r, we \ufb01nd that\nE[J3,3,x] = 1\nt E\nR t\n0\u27e8\u2207(Pt\u2212sf)(Zs,x), Ys,v,v\u2032,v\u2032\u2032\u27e9ds \u22641\nt\nR t\n0 M1(Pt\u2212sf)E\u2225Ys,v,v\u2032,v\u2032\u2032\u22252ds\n\u22641\nt\nR t\n0 M1(Pt\u2212sf)\n\u0000E\u2225Ys,v,v\u2032,v\u2032\u2032\u22252\n2\n\u0001 1\n2 ds \u2264M1(f) 1\nt\nR t\n0 r(t \u2212s)\n\u0000E\u2225Ys,v,v\u2032,v\u2032\u2032\u22252\n2\n\u0001 1\n2 ds\n\u2264M1(f)r(0) 1\nt\nR t\n0\n\u0000E\u2225Ys,v,v\u2032,v\u2032\u2032\u22252\n2\n\u0001 1\n2 ds.\nThis \ufb01nal expression is bounded by M1(f)ceCt provided that E\u2225Ys,v,v\u2032,v\u2032\u2032\u22252\n2 \u2264ceCs. We will establish\nsuch a bound for the third directional derivative \ufb02ow in Section J.5.\nJ.4. Semigroup third derivative bound.\nBy combining the bounds for each Ji,j,x term,\nadapting the argument of [8, Prop. 1.5.1], and invoking the semigroup gradient bound and Hessian\nbound M2(Psf) \u2264M1(f)r(s \u2212s0) c\u2032\n\u221as0 eC\u2032s0 of Lemma 15, we obtain, for any t0 \u2208(0, t] and s0 = t0/2\n\r\r\u22073Ptf[v, v\u2032, v\u2032\u2032]\n\r\r\nop =\n\r\r\u22073Pt0/2\n\u0000Pt\u2212t0/2f\n\u0001\n[v, v\u2032, v\u2032\u2032]\n\r\r\nop\n(39)\n\u2264(M1(Pt\u2212t0/2f) + M2(Pt\u2212t0/2f))\nc\np\nt0/2\neCt0/2\n\u2264M1(f)(r(t \u2212t0/2) + r(t \u2212t0/2 \u2212s0) c\u2032\n\u221as0\neC\u2032s0)\nc\np\nt0/2\neCt0/2.\n\u2264M1(f)(r(t \u2212t0/2) + r(t \u2212t0)\nc\u2032\np\nt0/2\neC\u2032t0/2)\nc\np\nt0/2\neCt0/2.\n49\nJ.5. Third derivative \ufb02ow bound.\nIntroduce the shorthand (Yt)t\u22650 for (Yt,v,v\u2032,v\u2032\u2032)t\u22650 solving\nthe third variation equation (37). Dynkin\u2019s formula gives E\u2225Yt\u22252\n2 =\nR t\n0 T1 + T2 ds for\nT1 \u225cE2\nD\nYs, \u2207b(Zs,x)Ys + \u22072b(Zs,x)[Us,v,v\u2032]Vs,v\u2032\u2032 + \u22073b(Zs,x)[Vs,v, Vs,v\u2032, Vs,v\u2032\u2032]\n+ \u22072b(Zs,x)[Us,v\u2032,v\u2032\u2032]Vs,v + \u22072b(Zt,x)[Ut,v,v\u2032\u2032]Vt,v\u2032\nE\nT2 \u225cE\n\r\r\r\u2207\u03c3(Zs,x)[Ys] + \u22072\u03c3(Zs,x)[Us,v,v\u2032]Vs,v\u2032\u2032 + \u22073\u03c3(Zs,x)[Vs,v, Vs,v\u2032, Vs,v\u2032\u2032]\n+ \u22072\u03c3(Zs,x)[Us,v\u2032,v\u2032\u2032]Vs,v + \u22072\u03c3(Zs,x)[Us,v,v\u2032\u2032]Vs,v\u2032\n\r\r\r\n2\nF .\nWe have by Cauchy-Schwarz and Young\u2019s inequality\nT1\n2 \u2264E\n\u0000\u2225Ys\u22252\n2M1(b) + M2(b)\u2225Ys\u22252 \u2225Us,v,v\u2032\u22252\u2225Vs,v\u2032\u2032\u22252\n|\n{z\n}\n+2 permutations\n+M3(b)\u2225Ys\u22252\u2225Vs,v\u22252\u2225Vs,v\u2032\u22252\u2225Vs,v\u2032\u2032\u22252\n\u0001\n\u2264E\n\u0000\u2225Ys\u22252\n2\n\u0000M1(b) + M2\n2 (b) + M2\n3 (b)\n\u0001\n+ M2\n2 (b)\u2225Us,v,v\u2032\u22252\n2\u2225Vs,v\u2032\u2032\u22252\n2\n|\n{z\n}\n+2 permutations\n+ M3(b)2\u0000\u2225Vs,v\u22252\u2225Vs,v\u2032\u22252\u2225Vs,v\u2032\u2032\u22252\n\u00012\u0001\nand\nT2\n4 \u2264E\u2225Y2\u22252\n2\n\u0000M1(\u03c3)2 + \u2225\u22072\u03c3\u22252\nF3 + \u2225\u22073\u03c3\u22252\nF3\n\u0001\n+ \u2225\u22073\u03c3\u22252\nF3E(\u2225Vs\u2225\u2225V \u2032\ns\u2225\u2225V \u2032\u2032\ns \u2225)2 + \u2225\u22072\u03c3\u22252\nF3 E\n\u0002\n\u2225Us,v,v\u2032\u22252\n2\u2225Vs,v\u2032\u2032\u22252\n2\n\u0003\n|\n{z\n}\n+2 permutations\n.\nProvided that we establish a bound of E\u2225Us,v,v\u2032\u22254\n2 \u2264cteCt, we have that overall\nE\u2225Yt\u22252\n2 \u2264\nR t\n0 cE\u2225Ys\u22252\n2ds + ceCt.\nWe can conclude using Gronwall\u2019s inequality that\nE\u2225Yt\u22252\n2 \u2264ceCt.\n(40)\nIt remains to establish bounds on E\u2225Ut,v,v\u2032\u2225\u03c1\n2 for \u03c1 > 2. Recall that the second derivative \ufb02ow\nsolves (24). Applying Ito\u2019s formula to f(Ut,v,v\u2032) = \u2225Ut,v,v\u2032\u2225\u03c1\n2, taking expectations, and introducing\n50\nJ. GORHAM, A.B. DUNCAN, S.J. VOLLMER, AND L. MACKEY\nthe shorthand Ut = Ut,v,v\u2032, we obtain\nE[\u2225Ut\u2225\u03c1\n2] = \u2225U0\u2225\u03c1\n2 + E\nh R t\n0 \u03c1\u27e8Us\u2225Us,\u2225\u03c1\u22122\n2\n, \u2207b(Zs,x)Us + \u22072b(Zs,x)[Vs,v\u2032]Vs,v)\u27e9\n+ \u03c1\n2\u2225Us\u2225\u03c1\u22124\n2\n((\u03c1 \u22122)\u2225U \u22a4\ns \u2207\u03c3(Zs,x)[Us,v] + U \u22a4\ns \u22072\u03c3(Zs,x)[Vs,v\u2032]Vs,v\u22252\n2\n+ \u2225Us,v\u22252\n2\u2225\u2207\u03c3(Zs,x)[Us,] + \u22072\u03c3(Zs,x)[Vs,v\u2032]Vs,v\u22252\nF ) ds\ni\n\u2264\u2225U0\u2225\u03c1\n2 +\nR t\n0 \u03c1M1(b)\u2225Us\u2225\u03c1\n2 + \u03c1M2(b)\u2225Us\u2225\u03c1\u22121\n2\n\u2225Vs\u22252\u2225V \u2032\ns\u22252\n+ \u03c12\u2212\u03c1\n2\n\u0010\nM1(\u03c3)2\u2225Us\u2225\u03c1\n2 + M2(\u03c3)2\u2225Us\u2225\u03c1\u22122\n2\n\u2225Vs\u22252\u2225V \u2032\ns\u22252\n\u0011\n+ \u03c1\n2\n\u0010\nF1(\u03c3)2\u2225Us\u2225\u03c1\n2 + F2(\u03c3)2\u2225Us\u2225\u03c1\u22122\n2\n\u2225Vs\u22252\u2225V \u2032\ns\u22252\n\u0011\nds\n\u2264\u2225U0\u2225\u03c1\n2 +\nR t\n0 E[\u2225Us\u2225\u03c1\n2](\u03c1M1(b) + (\u03c1 \u22121)M2(b) + M1(\u03c3)2 \u03c12\u2212\u03c1\n2\n+ M2(\u03c3)2 (\u03c1\u22121)2\n2\n+ F2(\u03c3) \u03c1\u22121\n2 )ds\n+\nR t\n0\n\u0010\nM2(b) + \u03c1\u22121\n2 M2(\u03c3)2 + 1\n2\n\u0011\nE[(\u2225Vs\u22252\u2225V \u2032\ns\u22252)\u03c1]ds\n\u2264\u2225U0\u2225\u03c1\n2 +\nR t\n0 E[\u2225Us\u2225\u03c1\n2](\u03c1M1(b) + (\u03c1 \u22121)M2(b) + M1(\u03c3)2 \u03c12\u2212\u03c1\n2\n+ M2(\u03c3)2 (\u03c1\u22121)2\n2\n+ F2(\u03c3) \u03c1\u22121\n2 )ds\n+\nR t\n0\n\u0010\nM2(b) + \u03c1\u22121\n2 M2(\u03c3)2 + 1\n2\n\u0011\n(\u2225v\u22252\u2225v\u2032\u22252)\u03c1e\u03b32\u03c1sds\nwhere we use that, by Young\u2019s inequality,\n\u2225Us\u2225\u03c1\u22121\n2\n\u2225Vs\u22252\u2225V \u2032\ns\u22252 \u2264\u03c1\u22121\n\u03c1 \u2225Us\u2225\u03c1\n2 + 1\n\u03c1\u2225Vs\u2225\u03c1\n2\u2225V \u2032\ns\u2225\u03c1\n2,\nand similarly\n\u2225Us\u2225\u03c1\u22122\n2\n\u2225Vs\u22252\u2225V \u2032\ns\u22252 \u2264\u03c1\u22122\n\u03c1 \u2225Us\u2225\u03c1\n2 + 2\n\u03c1\u2225Vs\u2225\u03c1/2\n2\n\u2225V \u2032\ns\u2225\u03c1/2\n2\n.\nFollowing the arguments of Section C.1, Gr\u00a8onwall\u2019s inequality gives\nE[\u2225Ut\u2225\u03c1\n2] \u2264\n\u0010\nM2(b) + \u03c1\u22121\n2 M2(\u03c3)2 + 1\n2\n\u0011\n(\u2225v\u22252\u2225v\u2032\u22252)\u03c1e\u03b32\u03c1tt exp(\u03b3\u03c1t).\n",
        "sentence": " To address this shortcoming, we develop a theory of weak convergence for the kernel Stein discrepancies analogous to that of (Gorham & Mackey, 2015; Mackey & Gorham, 2016; Gorham et al., 2016) and design a class of kernel Stein discrepancies that provably control weak convergence for a large class of target distributions. While Stein\u2019s method is principally used as a mathematical tool to prove convergence in distribution, we seek, in the spirit of (Gorham & Mackey, 2015; Gorham et al., 2016), to harness the Stein discrepancy as a practical tool for measuring sample quality. While our work is compatible with other practical Stein operators, like the family of diffusion Stein operators defined in (Gorham et al., 2016), we will focus on the Langevin operator for the sake of brevity. Recently, Gorham et al. (2016) showed that the Langevin graph Stein discrepancy dominates convergence in distribution whenever P belongs to the class P of distantly dissipative distributions with Lipschitz score function b: Definition 4 (Distant dissipativity (Eberle, 2015; Gorham et al., 2016)).",
        "context": "Our \ufb01rst contribution is to show that the Langevin Stein discrepancy\nin fact determines convergence for all smooth, distantly dissipative target\ndistributions by explicitly lower and upper bounding the Langevin Stein\nposteriors. This contribution greatly extends the range of applicability of\nthe Langevin Stein discrepancy.\nBecause heavy-tailed distributions are never distantly dissipative, as a\nsecond contribution, we extend the computable Stein discrepancy framework\n[35] J. Gorham and L. Mackey. Measuring sample quality with Stein\u2019s method. In Adv.\nNIPS 28, pages 226\u2013234. 2015.\n[36] J. Gorham and L. Mackey. Measuring sample quality with kernels. In Proc. of 34st\nICML, ICML\u201917, 2017."
    },
    {
        "title": "On the rate of convergence in the multivariate CLT",
        "author": [
            "F. G\u00f6tze"
        ],
        "venue": "Ann. Probab.,",
        "citeRegEx": "G\u00f6tze,? \\Q1991\\E",
        "shortCiteRegEx": "G\u00f6tze",
        "year": 1991,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "A kernel two-sample test",
        "author": [
            "A. Gretton",
            "K. Borgwardt",
            "M. Rasch",
            "B. Sch\u00f6lkopf",
            "A. Smola"
        ],
        "venue": "J. Mach. Learn. Res.,",
        "citeRegEx": "Gretton et al\\.,? \\Q2012\\E",
        "shortCiteRegEx": "Gretton et al\\.",
        "year": 2012,
        "abstract": "",
        "full_text": "",
        "sentence": " Each term wj in Proposition 2 can also be viewed as an instance of the maximum mean discrepancy (MMD) (Gretton et al., 2012) between \u03bc and P measured with respect to the Stein kernel k 0.",
        "context": null
    },
    {
        "title": "The Plancherel formula, the Plancherel theorem, and the Fourier transform of orbital integrals",
        "author": [
            "R. Herb",
            "P.J. Sally Jr."
        ],
        "venue": "In Representation Theory and Mathematical Physics: Conference in Honor of Gregg Zuckerman\u2019s 60th Birthday,",
        "citeRegEx": "Herb and Jr.,? \\Q2009\\E",
        "shortCiteRegEx": "Herb and Jr.",
        "year": 2009,
        "abstract": "We discuss various forms of the Plancherel Formula and the Plancherel Theorem\non reductive groups over local fields.",
        "full_text": "arXiv:1101.3753v1  [math.RT]  19 Jan 2011\nTHE PLANCHEREL FORMULA, THE PLANCHEREL\nTHEOREM, AND THE FOURIER TRANSFORM OF ORBITAL\nINTEGRALS\nREBECCA A. HERB (UNIVERSITY OF MARYLAND) AND PAUL J. SALLY, JR.\n(UNIVERSITY OF CHICAGO)\nAbstract. We discuss various forms of the Plancherel Formula and the Plancherel\nTheorem on reductive groups over local \ufb01elds.\nDedicated to Gregg Zuckerman on his 60th birthday\n1. Introduction\nThe classical Plancherel Theorem proved in 1910 by Michel Plancherel can be\nstated as follows:\nTheorem 1.1. Let f \u2208L2(R) and de\ufb01ne \u03c6n : R \u2192C for n \u2208N by\n\u03c6n(y) =\n1\n\u221a\n2\u03c0\nZ n\n\u2212n\nf(x)eiyxdx.\nThe sequence \u03c6n is Cauchy in L2(R) and we write \u03c6 = limn\u2192\u221e\u03c6n (in L2). De\ufb01ne\n\u03c8n : R \u2192C for n \u2208N by\n\u03c8n(x) =\n1\n\u221a\n2\u03c0\nZ n\n\u2212n\n\u03c6(y)e\u2212iyxdy.\nThe sequence \u03c8n is Cauchy in L2(R) and we write \u03c8 = limn\u2192\u221e\u03c8n (in L2). Then,\n\u03c8 = f almost everywhere, and\nZ\nR\n|f(x)|2 dx =\nZ\nR\n|\u03c6(y)|2 dy.\nThis theorem is true in various forms for any locally compact abelian group. It\nis often proved by starting with f \u2208L1(R)\u2229L2(R), but it is really a theorem about\nsquare integrable functions.\nThere is also a \u201csmooth\u201d version of Fourier analysis on R, motivated by the work\nof Laurent Schwartz, that leads to the Plancherel Theorem.\nDe\ufb01nition 1.2 (The Schwartz Space). The Schwartz space, S(R), is the collection\nof complex-valued functions f on R satisfying:\n(1) f \u2208C\u221e(R).\n(2) f and all its derivatives vanish at in\ufb01nity faster than any polynomial. That\nis, lim|x|\u2192\u221e|x|kf (m)(x) = 0 for all k, m \u2208N.\nFact 1.3. The Schwartz space has the following properties:\nDate: January 20, 2011.\n1\n2\nHERB\u2013SALLY\n(1) The space S(R) is dense in Lp(R) for 1 \u2264p < \u221e.\n(2) The space S(R) is not dense in L\u221e(R).\n(3) The space S(R) is a vector space over C.\n(4) The space S(R) is an algebra under both pointwise multiplication and con-\nvolution.\n(5) The space S(R) is invariant under translation.\nFor f \u2208S(R), we de\ufb01ne the Fourier transform as usual by\nbf(y) =\n1\n\u221a\n2\u03c0\nZ\nR\nf(x)eiyxdx.\nOf course, there are no convergence problems here, and we have\nf(x) =\n1\n\u221a\n2\u03c0\nZ\nR\nbf(y)e\u2212iyxdy.\nThis leads to the Plancherel Theorem for functions in S(R) by setting ef(x) =\nf(\u2212x) and considering f \u2217ef at 0. Using the fact that the Fourier transform carries\nconvolution product to function product, we have\n\u2225f\u22252 =\nh\nf \u2217ef\ni\n(0) =\n1\n\u221a\n2\u03c0\nZ\nR\n[\nf \u2217ef(y)dy =\n\r\r\r bf\n\r\r\r\n2\n.\nIt is often simpler to work on the space C\u221e\nc (R) of complex-valued, compactly\nsupported, in\ufb01nitely di\ufb00erentiable functions on R. However, nonzero functions in\nC\u221e\nc (R) do not have Fourier transforms in C\u221e\nc (R). On the other hand, the Fourier\ntransform is an isometric isomorphism from S(R) to S(R).\nThe spaces C\u221e\nc (R) and S(R) can be turned into topological vector spaces so\nthat the embedding from C\u221e\nc (R) into S(R) is continuous. However, the topology\non C\u221e\nc (R) is not the relative topology from S(R). A continuous linear functional on\nC\u221e\nc (R) is a distribution on R, and this distribution is tempered if it can be extended\nto a continuous linear functional on S(R) with the appropriate topology.\nThis\nsituation will arise again in our discussion of the Plancherel Formula on reductive\ngroups.\nWork on the Plancherel Formula for non-abelian groups began in earnest in the\nlate 1940s. There were two distinct approaches. The \ufb01rst, for separable, locally\ncompact, unimodular groups, was pursued by Mautner [63], Segal [82], and others.\nThe second, for semisimple Lie groups, was followed by Gel\u2032fand\u2013Naimark [23], and\nHarish\u2013Chandra [24], along with others. Segal\u2019s paper [82] and Mautner\u2019s paper\n[63] led eventually to the following statement (see [21], Theorem 7.44).\nTheorem 1.4. Let G be a separable, unimodular, type I group, and let dx be a \ufb01xed\nHaar measure on G. There exists a positive measure \u00b5 on bG (determined uniquely\nup to a constant that depends only on dx) such that, for f \u2208L1(G) \u2229L2(G), \u03c0(f)\nis a Hilbert\u2013Schmidt operator for \u00b5-almost all \u03c0 \u2208bG, and\nZ\nG\n|f(x)|2 dx =\nZ\nb\nG\n\u2225\u03c0(f)\u22252\nHS d\u00b5(\u03c0).\nHere, of course, bG denotes the set of equivalence classes of irreducible unitary\nrepresentations of G.\nAt about the same time, Harish-Chandra stated the following theorem in his\npaper Plancherel Formula for Complex Semisimple Lie Groups.\nTHE PLANCHEREL FORMULA\n3\nTheorem 1.5. Let G be a connected, complex, semisimple Lie group. Then, for\nf \u2208C\u221e\nc (G),\nf(1) = lim\nH\u21920\nY\n\u03b1\u2208P\nD\u03b1D\u03b1\n\u0014\ne\u03c1(H)+\u03c1(H)\nZ\nK\u00d7N\nf\n\u0000u exp(H)nu\u22121\u0001\ndudn\n\u0015\n.\nAn explanation of the notation here can be found in [24].\nWe do note two\nthings. First of all, f is taken to be in C\u221e\nc (G), and the formula for f(1) is the limit\nof a di\ufb00erential operator applied to what may be regarded as a Fourier inversion\nformula for the orbital integral over a conjugacy class of exp(H) in G. It should\nalso be mentioned that not all irreducible unitary representations are contained\nin the support of the Plancherel measure for complex semisimple Lie groups. In\nparticular, the complementary series are omitted.\nIn this note, we will trace the evolution of the Plancherel Formula over the past\nsixty years. For real groups, we observe that the original Plancherel Formula and the\nFourier inversion formula ultimately became a decomposition of the Schwartz space\ninto orthogonal components indexed by conjugacy classes of Cartan subgroups.\nWhile this distinction might not have been clear for real semisimple Lie groups, it\ncertainly appeared in the development of the Plancherel Theorem for reductive p-\nadic groups by Harish-Chandra in his paper The Plancherel Formula for Reductive\np-adic Groups in [40]. See also the papers of Waldspurger [93] and Silberger [89],\n[90]. For p-adic groups, the lack of information about irreducible characters and\nsuitable techniques for Fourier inversion has made the derivation of an explicit\nPlancherel Formula very di\ufb03cult.\nIn this paper, the authors have drawn extensively on the perceptive description of\nHarish-Chandra\u2019s work by R. Howe, V. S. Varadarajan, and N. Wallach (see [39]).\nThe authors would like to thank Jonathan Gleason and Nick Ramsey for their\nassistance in preparing this paper. We also thank David Vogan for his valuable\ncomments on the \ufb01rst draft.\n2. Orbital Integrals and the Plancherel Formula\nLet G be a reductive group over a local \ufb01eld. For \u03b3 \u2208G, let G\u03b3 be the centralizer\nof \u03b3 in G. Assume G\u03b3 is unimodular. For f \u201csmooth\u201d on G, de\ufb01ne\n\u039b\u03b3(f) =\nZ\nG/G\u03b3\nf\n\u0000x\u03b3x\u22121\u0001\nd \u02d9x,\nwith d \u02d9x a G-invariant measure on G/G\u03b3.\nThen, \u039b\u03b3 is an invariant distribution on G, that is, \u039b\u03b3(f) = \u039b\u03b3(yf) where\nyf(x) = f\n\u0000yxy\u22121\u0001\nfor y \u2208G. A major problem in harmonic analysis on reductive\ngroups is to \ufb01nd the Fourier transform of the invariant distribution \u039b\u03b3. That is,\n\ufb01nd a linear functional c\n\u039b\u03b3 such that\n\u039b\u03b3(f) = c\n\u039b\u03b3\n\u0010\n\u02c6f\n\u0011\n,\nwhere \u02c6f is a function de\ufb01ned on the space of tempered invariant \u201ceigendistributions\u201d\non G. This space should include the tempered irreducible characters of G along\nwith other invariant distributions. For example, if \u03a0 is an admissible representation\nof G with character \u0398\u03a0, then\n\u02c6f(\u03a0) = tr(\u03a0(f)) =\nZ\nG\nf(x)\u0398\u03a0(x)dx.\n4\nHERB\u2013SALLY\nThe nature of the other distributions is an intriguing problem. The hope is that\nthe Plancherel Formula for G can be obtained through some limiting process for\n\u039b\u03b3.\nFor example, if G = SU(1, 1) \u223c= SL(2, R), we let\n\u03b3 =\n\u0014\nei\u03b80\n0\n0\ne\u2212i\u03b80\n\u0015\n, \u03b80 \u0338= 0, \u03c0.\nThen, \u03b3 is a regular element in G, and\nG\u03b3 = T =\n\u001a\u0014ei\u03b8\n0\n0\ne\u2212i\u03b8\n\u0015\f\f\f\f 0 \u2264\u03b8 < 2\u03c0\n\u001b\n.\nAfter a simple computation, we get\nF T\nf (\u03b3) =\n\f\fei\u03b80 \u2212e\u2212i\u03b80\f\f \u039b\u03b3(f)\n= \u22121\n2\n\u0010\n\u03c0(+,+)(f) \u2212\u03c0(+,\u2212)(f)\n\u0011\n\u2212\nX\nn\u0338=0\nsgn(n)\u03c7\u03c9(n)(f)e\u2212in\u03b80\n+ i\n4\n\u0014Z\nR\n\u03c0(+,\u03bd)(f)sinh (\u03bd(\u03b80 \u2212\u03c0/2))\nsinh(\u03bd\u03c0/2)\nd\u03bd \u2212\nZ\nR\n\u03c0(\u2212,\u03bd)(f)cosh (\u03bd(\u03b80 \u2212\u03c0/2))\ncosh(\u03bd\u03c0/2)\nd\u03bd\n\u0015\n.\nThe parameter n \u0338= 0 indexes the discrete series and the parameter \u03bd indexes the\nprincipal series representations of G. The terms \u03c0(+,+)(f) and \u03c0(+,\u2212)(f) represent\nthe characters of the irreducible components of the reducible principal series, and\nwe obtain a \u201csingular invariant eigendistribution\u201d on G by subtracting one from the\nother and dividing by 2. This is exactly the invariant distribution that makes har-\nmonic analysis work. It is called a supertempered distribution by Harish-Chandra.\nThis leads directly to the Plancherel Formula. By a theorem of Harish-Chandra,\nit follows that\nlim\n\u03b8\u21920\n\u00141\ni\nd\nd\u03b8\n\u0002\nF T\nf (\u03b3)\n\u0003\u0015\n= 8\u03c0f(1)\n=\nX\nn\u2208Z\n|n|\u03c7\u03c9(n)(f) + 1/2\nZ \u221e\n0\n\u03c0(+,\u03bd)(f)\u03bd coth(\u03c0/2\u03bd)d\u03bd\n+ 1/2\nZ \u221e\n0\n\u03c0(\u2212,\u03bd)(f)\u03bd tanh(\u03c0/2\u03bd)d\u03bd.\nThe representations of SL(2, R) were \ufb01rst determined by Bargmann [8].\nIn\nhis 1952 paper [25], Harish-Chandra gave hints to the entire picture for Fourier\nanalysis on real groups.\nHe constructed the unitary representations, computed\ntheir characters, found the Fourier transform of orbital integrals, and deduced the\nPlancherel Formula. This was done in about four and one-half pages.\nWe mention again that the support of the Fourier transform of the tempered\ninvariant distribution \u039b\u03b3 contains not only the characters of the principal series\nand the discrete series, but also the tempered invariant distribution\n1\n2\n\u0010\n\u03c0(+,+) \u2212\u03c0(+,\u2212)\u0011\n.\nThis singular invariant eigendistribution (appropriately normalized) is equal to 1\non the elliptic set and 0 o\ufb00the elliptic set, thereby having no e\ufb00ect on harmonic\nanalysis of the principal series.\nTHE PLANCHEREL FORMULA\n5\nThrough the 1950s, along with an intensive study of harmonic analysis on semisim-\nple Lie groups, Harish-Chandra analyzed invariant distributions, their Fourier trans-\nforms, and limit formulas related to these. This was mainly with reference to dis-\ntributions on C\u221e\nc (G). He showed that G has discrete series i\ufb00G has a compact\nCartan subgroup. For the rest of this section, we will assume that G has discrete se-\nries. He also suspected quite early that the irreducible unitary representations that\noccurred in the Plancherel Formula would be indexed by a series of representations\nparameterized by characters of conjugacy classes of Cartan subgroups.\nIn the 1960s, Harish-Chandra proved deep results about the character theory of\nsemisimple Lie groups, in particular, the discrete series characters. In developing\nthe Fourier analysis on a semisimple Lie group, he had to work with the smooth\nmatrix coe\ufb03cients of the discrete series. These matrix coe\ufb03cients vanish rapidly at\nin\ufb01nity, but are not compactly supported. This led to the de\ufb01nition of the Schwartz\nspace C(G) [27]. The Schwartz space was designed to include matrix coe\ufb03cients of\nthe discrete series and slightly more. The Schwartz space is dense in L2(G), but is\nnot contained in L1(G). Moreover, the Schwartz space C(G) does not contain the\nsmooth matrix coe\ufb03cients of parabolically induced representations. Nonetheless,\nthe matrix coe\ufb03cients of these parabolically induced representations are tempered\ndistributions, that is, if m is such a matrix coe\ufb03cient and f \u2208C(G), then\nR\nG fm\nconverges. Hence, one can consider the orthogonal complement of these matrix\ncoe\ufb03cients in C(G).\nThe collection of parabolically induced representations is indexed by non-compact\nCartan subgroups of G. If H is a Cartan subgroup of G with split component A,\nthen the centralizer L of A is a Levi subgroup of G. Now the representations cor-\nresponding to H are induced from parabolic subgroups with Levi component L,\nand the subspace CH(G) is generated by so called wave packets associated to these\ninduced representations. Thus, we have an orthogonal decomposition\nC(G) =\nM\nCH(G),\nwhere H runs over conjugacy classes of Cartan subgroups. When H is the compact\nCartan subgroup of G, CH(G) is the space of cusp forms in C(G). This decomposi-\ntion of the Schwartz space is a version of the Plancherel Theorem for G, and it is\nin this form that the Plancherel Theorem appears for reductive p-adic groups.\nAs he approached his \ufb01nal version of the Plancherel Theorem and Formula\nfor real semisimple Lie groups, Harish-Chandra presented a development of the\nPlancherel Formula for functions in C\u221e\nc (G) in his paper Two Theorems on Semisim-\nple Lie Groups [28]. Here, he shows exactly how irreducible tempered characters\ndecompose the \u03b4 distribution. In particular, for G of real rank 1, he gives an ex-\nplicit formula for the Fourier transform of an elliptic orbital integral, and derives\nthe Plancherel Formula from this. To understand the Plancherel Theorem for real\ngroups in complete detail, one should consult the three papers [34], [35], [36], and\nthe expository renditions of this material [30], [31], [32].\n3. The Fourier Transform of Orbital Integrals, the Plancherel\nFormula, and Supertempered Distributions\nIn a paper in Acta Mathematica in 1973 [79], Sally and Warner re-derived, by\nsomewhat di\ufb00erent methods, the inversion formula that Harish-Chandra proved in\nhis \u201cTwo Theorems\u201d paper [28]. The purpose of the Sally\u2013Warner paper was to\n6\nHERB\u2013SALLY\nexplore the support of the Fourier transform of an elliptic orbital integral. To quote:\n\u201cIn this paper, we give explicit formulas for the Fourier transform of \u039by, that is,\nwe determine a linear functional c\n\u039by such that\n\u039by(f) = c\n\u039by\n\u0010\n\u02c6f\n\u0011\n, f \u2208C\u221e\nc (G).\nHere, we regard \u02c6f as being de\ufb01ned on the space of tempered invariant eigendistri-\nbutions on G. This space contains the characters of the principal series and the\ndiscrete series for G along with some \u2018singular\u2019 invariant eigendistributions whose\ncharacter-theoretic nature has not yet been completely determined.\u201d\nIn fact, the character theoretic nature of these singular invariant eigendistribu-\ntions was determined in a paper [42] by Herb and Sally in 1977. In this paper,\nthe present authors used results of Hirai [53], Knapp\u2013Zuckerman [58], Schmid [81],\nand Zuckerman [96] to show that, as in the case of SU(1, 1), these distributions\nare alternating sums of characters of limits of discrete series representations which\ncan be embedded as the irreducible components of certain reducible principal se-\nries. In his \ufb01nal published paper [38], Harish-Chandra developed a comprehensive\nversion of these singular invariant eigendistributions, and he called them \u201csupertem-\npered distributions.\u201d These supertempered distributions include the characters of\ndiscrete series along with some \ufb01nite linear combinations of irreducible tempered\nelliptic characters that arise from components of reducible generalized principal se-\nries. This situation has already been illustrated for SL(2, R) in Section 2 of this\npaper. One notable fact about supertempered distributions is that they appear\ndiscretely in the Fourier transforms of elliptic orbital integrals; hence they play an\nessential role in the study of invariant harmonic analysis. For the remainder of\nthis section, we present a collection of results of the \ufb01rst author related to Fourier\ninversion and the Plancherel Theorem for real groups.\nIn order to explain the steps needed to derive the Fourier transform for orbital\nintegrals in general, we \ufb01rst look in more detail at the case that G has real rank one.\nIn this case G has at most two non-conjugate Cartan subgroups: a non-compact\nCartan subgroup H with vector part of dimension one, and possibly a compact\nCartan subgroup T . We assume for simplicity that G is acceptable, that is, the half-\nsum of positive roots (denoted \u03c1) exponentiates to give a well de\ufb01ned character on\nT . The characters \u0398T\n\u03c4 of the discrete series representations are indexed by \u03c4 \u2208\u02c6T \u2032,\nthe set of regular characters of T , and the characters \u0398H\n\u03c7 of the principal series\nare indexed by characters \u03c7 \u2208\u02c6H. In addition, for f \u2208C\u221e\nc (G) we have invariant\nintegrals F T\nf (t), t \u2208T , and F H\nf (a), a \u2208H. These are normalized versions of the\norbital integrals \u039b\u03b3(f), \u03b3 \u2208G, which have better properties as functions on the\nCartan subgroups.\nThe analysis on the non-compact Cartan subgroup is elementary. First, as func-\ntions on G\u2032, the set of regular elements of G, the principal series characters are\nsupported on conjugates of H. In addition, for \u03c7 \u2208\u02c6H, a \u2208H\u2032 = H \u2229G\u2032, \u0398H\n\u03c7 (a)\nis given by a simple formula in terms of \u03c7(a). As a result it is easy to show that\nthe abelian Fourier transform \u02c6F H\nf (\u03c7), \u03c7 \u2208\u02c6H, is equal up to a constant to \u0398H\n\u03c7 (f),\nthe principal series character evaluated at f. Finally, F H\nf\n\u2208C\u221e\nc (H), and so the\nabelian Fourier inversion formula on A yields an expansion\n(3.1)\nF H\nf (a) = cH\nZ\n\u02c6\nH\n\u01eb(\u03c7)\u03c7(a)\u0398H\n\u03c7 (f)d\u03c7, a \u2208H,\nTHE PLANCHEREL FORMULA\n7\nwhere cH is a constant depending on normalizations of measures and \u01eb(\u03c7) = \u00b11.\nThe situation on the compact Cartan subgroup is more complicated. There are\nthree main di\ufb00erences. First, for \u03c4 \u2208\u02c6T \u2032, t \u2208T \u2032 = T \u2229G\u2032, \u0398T\n\u03c4 (t) is given by a simple\nformula in terms of the character \u03c4(t). However, \u0398T\n\u03c4 is also non-zero on H\u2032. Thus for\n\u03c4 \u2208\u02c6T \u2032, f \u2208C\u221e\nc (G), the abelian Fourier coe\ufb03cient \u02c6F T\nf (\u03c4) is equal up to a constant\nto \u0398T\n\u03c4 (f) plus an error term which is an integral over H of F H\nf times the numerator\nof \u0398T\n\u03c4 . Second, the singular characters \u03c40 \u2208\u02c6T do not correspond to discrete series\ncharacters.\nThey do however parameterize singular invariant eigendistributions\n\u0398T\n\u03c40, and \u02c6F T\nf (\u03c40) can be given in terms of \u0398T\n\u03c40(f). Finally, F T\nf is smooth on T \u2032, but\nhas jump discontinuities at singular elements. Because of this there are convergence\nissues when the abelian Fourier inversion formula is used to expand F T\nf in terms of\nits Fourier coe\ufb03cients.\nSally and Warner were able to compute the explicit Fourier transform of F T\nf in\nthe rank one situation where discrete series character formulas on the non-compact\nCartan subgroup were known. The resulting formula is very similar to the one\nfor the special case of SU(1, 1) given in the previous section. The discrete series\ncharacters and singular invariant eigendistributions occur discretely in a sum over\n\u02c6T and the principal series characters occur in an integral over \u02c6A with hyperbolic\nsine and cosine factors. They were also able to di\ufb00erentiate the resulting formula\nto obtain the Plancherel Formula.\nThe key to computing an explicit Fourier transform for orbital integrals in\nthe general case is an understanding of discrete series character formulas on non-\ncompact Cartan subgroups. Thus we brie\ufb02y review some of these formulas. The\nresults are valid for any connected reductive Lie group, but we assume for simplicity\nof notation that G is acceptable. A detailed expository account of all results about\ndiscrete series characters presented in this section is given in [51].\nAssume that G has discrete series representations, and hence a compact Cartan\nsubgroup T , and identify the character group of T with a lattice L \u2282E = it\u2217. For\neach \u03bb \u2208E, let W(\u03bb) = {w \u2208W : w\u03bb = \u03bb} where W is the full complex Weyl\ngroup, and let E\u2032 = {\u03bb \u2208E : W(\u03bb) = {1}}. Then \u03bb \u2208L\u2032 = L \u2229E\u2032 is regular,\nand corresponds to a discrete series character \u0398T\n\u03bb. For t \u2208T \u2032, we have the simple\ncharacter formula\n(3.2)\n\u0398T\n\u03bb (t) = \u01eb(E+)\u2206(t)\u22121 X\nw\u2208WK\ndet(w)ew\u03bb(t),\nwhere \u2206is the Weyl denominator, WK is the subgroup of W generated by re\ufb02ections\nin the compact roots, and \u01eb(E+) = \u00b11 depends only on the connected component\n(Weyl chamber) E+ of E\u2032 containing \u03bb.\nNow assume that H is a non-compact Cartan subgroup of G, and let H+ be a\nconnected component of H\u2032. Then for h \u2208H+,\n(3.3)\n\u0398T\n\u03bb (h) = c(H+)\u01eb(E+)\u2206(h)\u22121 X\nw\u2208W\ndet(w)c(w: E+ : H+)\u03bew,\u03bb(h),\nwhere c(H+) is an explicit constant given as a quotient of certain Weyl groups and\nthe c(w: E+ : H+) are integer constants depending only on the data shown in the\nnotation. The sum is over the full complex Weyl group W, and for w such that\nc(w: E+ : H+) is potentially non-zero, \u03bew,\u03bb is a character of H obtained from w\nand \u03bb using a Cayley transform. This formula is a restatement of results of Harish-\nChandra in [26]. In that paper, Harish-Chandra gave properties of the constants\n8\nHERB\u2013SALLY\nc(w: E+ : H+) which characterize them completely. These properties can in theory\nbe used to determine the constants by induction on the dimension of the vector\ncomponent of H. This easily yields formulas when this dimension is one or two,\nbut quickly becomes cumbersome for higher dimensions.\nWith the above notation, it is easy to describe the singular invariant eigendis-\ntributions corresponding to \u03bb \u2208Ls = L\\L\u2032. Let \u03bb0 \u2208Ls, and let E+ be a chamber\nwith \u03bb0 \u2208Cl(E+). The exponential terms \u03bew,\u03bb0(h), h \u2208H+, still make sense, and\nthe \u201climit of discrete series\u201d \u0398T\n\u03bb0,E+ = lim\u03bb\u2192\u03bb0,\u03bb\u2208L\u2229E+ \u0398T\n\u03bb is given by (3.3) using\nthe constants from E+. Zuckerman [96] showed that the limits of discrete series\nare the characters of tempered unitary representations of G. The singular invariant\neigendistribution corresponding to \u03bb0 is the alternating sum of the limits of discrete\nseries taken over all chambers with closures containing \u03bb0.\n(3.4)\n\u0398T\n\u03bb0 = [W(\u03bb0)]\u22121\nX\nw\u2208W(\u03bb0)\ndet w \u0398T\n\u03bb0,wE+.\nThe main results of [43] are as follows. Let \u03a6(\u03bb0) denote the roots of T which are\northogonal to \u03bb0. Then \u0398T\n\u03bb0 vanishes if \u03a6(\u03bb0) contains any compact roots. Thus\nwe may as well assume that all roots in \u03a6(\u03bb0) are non-compact. By using Cayley\ntransforms with respect to the roots of \u03a6(\u03bb0) we obtain a Cartan subgroup H and\ncorresponding cuspidal Levi subgroup M. Because the Cayley transform of \u03bb0 is\nregular with respect to the roots of H in M, it determines a discrete series character\nof M, which can then be parabolically induced to obtain a unitary principal series\ncharacter \u0398H\n\u03bb0 of G.\nTheorem 3.5 (Herb\u2013Sally). \u0398H\n\u03bb0 = P\nw\u2208W(\u03bb0) \u0398T\n\u03bb0,wE+.\nIt follows from Knapp [57] that \u0398H\n\u03bb0 has at most [W(\u03bb0)] irreducible components.\nThus each limit of discrete series character is irreducible, and \u0398T\n\u03bb0 is the alternating\nsum of the characters of the irreducible constituents of \u0398H\n\u03bb0.\nIn [44], Herb used the methods of Sally and Warner, and the discrete series\ncharacter formulas of Harish-Chandra, to obtain a Fourier inversion formula for\norbital integrals for groups of arbitrary real rank. As in the rank one case, for any\nCartan subgroup H of G we have normalized orbital integrals F H\nf (h), h \u2208H, f \u2208\nC\u221e\nc (G). We also have characters \u0398H\n\u03c7 , \u03c7 \u2208\u02c6H. If H is compact, these are discrete\nseries characters for regular \u03c7 and singular invariant eigendistributions for singular\n\u03c7. If H is non-compact, corresponding to the Levi subgroup M, then they are\nparabolically induced from discrete series or singular invariant eigendistributions\non M. Using standard character formulas for parabolic induction, these characters\ncan also be written using Harish-Chandra\u2019s discrete series formulas for M.\nFix a Cartan subgroup H0. The goal is to \ufb01nd a formula\n(3.6)\nF H0\nf\n(h0) =\nX\nH\nZ\n\u02c6\nH\n\u0398H\n\u03c7 (f)KH(h0, \u03c7)d\u03c7, h0 \u2208H\u2032\n0,\nwhere H runs over a set of representatives of conjugacy classes of Cartan subgroups\nof G, d\u03c7 is Haar measure on \u02c6H, and KH(h0, \u03c7) is a function depending on h0 and\n\u03c7. The problem is to compute the functions KH(h0, \u03c7), or at least show they exist.\nAs in the rank one case, for \u03c70 \u2208\u02c6H0, f \u2208C\u221e\nc (G), the abelian Fourier coe\ufb03cient\n\u02c6F H0\nf\n(\u03c70) is equal up to a constant to \u0398H0\n\u03c70 (f) plus an error term for each of the other\nCartan subgroups. The error term corresponding to H is an integral over H of the\nTHE PLANCHEREL FORMULA\n9\nnumerator of \u0398H0\n\u03c70 times F H\nf . Because \u0398H0\n\u03c70 is parabolically induced, its character is\nnon-zero only on Cartan subgroups of G which are conjugate to Cartan subgroups\nof M0, the corresponding Levi subgroup. Thus the error term will be identically\nzero unless H can be conjugated into M0, but is not conjugate to H0. This implies\nin particular that the vector dimension of H is strictly greater than that of H0.\nThus if H0 is maximally split in G there are no error terms. However, if H0 = T\nis compact, then M0 = G and all non-compact Cartan subgroups contribute error\nterms.\nLet H be a Cartan subgroup of M0 which is not conjugate to H0 and let M be the\ncorresponding Levi subgroup. In analyzing the error term corresponding to H, we\nobtain a primary term involving the characters \u0398H\n\u03c7 (f), \u03c7 \u2208\u02c6H, plus secondary error\nterms, one for each Cartan subgroup of M not conjugate to H. This leads to messy\nbookkeeping, but the process eventually terminates since the vector dimension of\nthe Cartan subgroups with non-zero error terms increases strictly at each step.\nIn particular, if H is a Cartan subgroup of G not conjugate to a Cartan subgroup\nof M0, then it never occurs in a non-zero error term and KH is identically zero.\nOur original Cartan subgroup H0 also is not involved in any error term, and we\nhave\n(3.7)\nKH0(h0, \u03c70) = cH0\u01eb(\u03c70)\u03c70(h0), h0 \u2208H\u2032\n0, \u03c70 \u2208\u02c6H0.\nThe formulas for KH become progressively more complicated as the vector dimen-\nsion of H increases.\nIn particular, if H is maximally split in G, then KH has\ncontributions from error terms at many di\ufb00erent steps.\nAside from the proliferation of error terms, the analysis which will lead to the\nfunctions KH(h0, \u03c7) involves two main problems that do not occur in real rank\none. The main problem is that the \ufb01nal formulas contain the unknown integer\nconstants c(w: E+ : H+) appearing in discrete series character formulas. These\noccur in complicated expressions which can be interpreted as Fourier series in several\nvariables. These series are not absolutely convergent and have no obvious closed\nform. Thus although [43] showed the existence of the functions KH(h0, \u03c7), it does\nnot result in a formula which is suitable for applications. In particular, it cannot be\ndi\ufb00erentiated to obtain the Plancherel Formula for G. Second, in the rank one case\nthe analysis can be carried out for any h \u2208H\u2032. However there are cases in higher\nrank, for example the real symplectic group of real rank three, in which certain\nintegrals diverge for some elements h \u2208H\u2032. However, the analysis is valid on a\ndense open subset of H\u2032.\nIn order to improve these results and obtain a satisfactory Fourier inversion\nformula similar to that of Sally and Warner for rank one groups, it was necessary\nto have more information about the discrete series constants. The \ufb01rst of these\nimprovements came from a consideration of stable discrete series characters and\nstable orbital integrals.\nAssume that G has a compact Cartan subgroup T , and use the notation from\nthe earlier discussion of discrete series characters. For \u03bb \u2208L we de\ufb01ne\n(3.8)\n\u0398T,st\n\u03bb\n= [WK]\u22121 X\nw\u2208W\n\u0398T\nw\u03bb.\n10\nHERB\u2013SALLY\nIf \u03bb \u2208L\u2032, then \u0398T,st\n\u03bb\nis called a stable discrete series character. For \u03bb \u2208Ls, we\nhave \u0398T,st\n\u03bb\n= 0. Similarly we de\ufb01ne the stable orbital integral\n(3.9)\n\u039bst\nt (f) =\nX\nw\u2208W\n\u039bwt(f), f \u2208C\u221e\nc (G), t \u2208T \u2032.\nIf we normalize the orbital integral as usual, we have\n(3.10)\nF T,st\nf\n(t) = \u2206(t)\u039bst\nt (f) =\nX\nw\u2208W\ndet(w)F T\nf (wt).\nSimilarly, for any Cartan subgroup H with corresponding Levi subgroup M there\nis a series of stable characters \u0398H,st\n\u03c7\n, \u03c7 \u2208\u02c6H, induced from stable discrete series\ncharacters of M.\nWe also obtain stable orbital integrals by averaging over the\ncomplex Weyl group of H in M.\nRecall that there is a di\ufb00erential operator \u03a0 such that\n(3.11)\nf(1) =\nlim\nt\u21921,t\u2208T \u2032 \u03a0F T\nf (t).\nSince the di\ufb00erential operator \u03a0 transforms by the sign character of W, it follows\nimmediately that we also have\n(3.12)\nf(1) = [W]\u22121\nlim\nt\u21921,t\u2208T \u2032 \u03a0F T,st\nf\n(t).\nThe advantage of stabilizing is that the formulas for the stable discrete series\ncharacters on the non-compact Cartan subgroups are simpler than those of the\nindividual discrete series characters. The Fourier inversion formula for stable orbital\nintegrals involves only these stable characters and has the general form\n(3.13)\nF T,st\nf\n(t) =\nX\nH\nZ\n\u02c6\nH\n\u0398H,st\n\u03c7\n(f)KH,st(t, \u03c7)d\u03c7, t \u2208T \u2032.\nWhen G has real rank one the Fourier inversion formulas for the stable orbital\nintegrals are no simpler than those obtained by Sally and Warner. However when\nG has real rank two there is already signi\ufb01cant simpli\ufb01cation, and Sally\u2019s student\nChao [13] was able to obtain expressions for the functions KH,st(t, \u03c7) in closed form\nand di\ufb00erentiate them to obtain the Plancherel Formula.\nHerb [45], [46] then developed the theory of two-structures and showed that the\nconstants occurring in stable discrete series character formulas for any group can\nbe expressed in terms of stable discrete constants for the group SL(2, R) and the\nrank two symplectic group Sp(4, R). As a consequence she was able to write each\nfunction KH,st(t, \u03c7) occurring in (3.13) as a product of factors which occur in the\ncorresponding formulas for SL(2, R) and Sp(4, R).\nThis formula can be di\ufb00erentiated to yield the Plancherel Formula. However, the\nFourier inversion formulas for stable orbital integrals are of independent interest,\nand much of the complexity of these distributions is lost when they are di\ufb00erentiated\nand evaluated at t = 1. In particular the functions occurring in the Plancherel\nFormula, which had already been obtained by di\ufb00erent methods by Harish-Chandra\n[36], reduce to a product of rank one factors which occur in the Plancherel Formula\nfor SL(2, R). The discrete series character formulas and Fourier inversion formula\nfor F T,st\nf\n(t) require both SL(2, R) and Sp(4, R) type factors coming from the theory\nof two-structures.\nIn [47] Herb was able to use Shelstad\u2019s ideas on endoscopy to obtain explicit\nFourier inversion formulas for the individual (not stabilized) orbital integrals. The\nTHE PLANCHEREL FORMULA\n11\nidea is that certain weighted sums of orbital integrals, \u039b\u03ba\n\u03b3(f), correspond to stable\norbital integrals on endoscopic groups. Thus their Fourier inversion formulas can be\ncomputed as in [46]. This is done for su\ufb03ciently many weights \u03ba that the original\norbital integrals \u039b\u03b3(f) can be recovered. Again, the theory of two-structures was\nimportant, and the functions KH(h0, \u03c7) occurring in (3.6) can be given in closed\nform using products of terms coming from the groups SL(2, R) and Sp(4, R).\nAlthough this gave a satisfactory Fourier inversion formula, the derivation is com-\nplicated by the use of stability and endoscopy. Stability and endoscopy also com-\nbined to yield explicit, but cumbersome, formulas for the discrete series constants\nc(w: E+ : H+) occurring in (3.3). In [50], Herb found simpler formulas for these\nconstants that bypass the theories of stability and endoscopy, and are easier to prove\nindependently of these results. Using special two-structures called two-structures of\nnon-compact type, she obtained a formula for the constants c(w: E+ : H+) directly\nin terms of constants occurring in discrete series character formulas for SL(2, R)\nand Sp(4, R). These formulas could be used to give a direct and simpler proof of\nthe Fourier inversion formulas for orbital integrals given in [47].\n4. The p-adic Case\nWe now focus on the representation theory and harmonic analysis of reductive\np-adic groups. Since the 1960s, there has been a \ufb02urry of activity related to these\ngroups. Some of this has been generated by the so-called \u201cLanglands Program\u201d (see\nJacquet\u2013Langlands [55] and Langlands [61]). However, a number of results in rep-\nresentation theory and harmonic analysis were completed well before this activity\nrelated to the Langlands Program by Bruhat [9], Satake [80], Gel\u2032fand\u2013Graev [22],\nand Macdonald [62]. Of particular interest were the results of Mautner [64] that\ngave the \ufb01rst construction of supercuspidal representations. Here, a supercuspidal\nrepresentation is an in\ufb01nite-dimensional, irreducible, unitary representation with\ncompactly supported matrix coe\ufb03cients (mod the center). In the mid-1960s, for\na p-adic \ufb01eld F with odd residual characteristic, all supercuspidal representations\nfor SL(2, F) were constructed by Shalika [86], and for PGL(2, F) by Silberger [88].\nThese two were Mautner\u2019s Ph.D. students. At roughly the same time, Shintani [87]\nconstructed some supercuspidal representations for the group of n\u00d7n matrices over\nF whose determinant is a unit in the ring of integers of F. Shintani also proved the\nexistence of a Frobenius-type formula for computing supercuspidal characters as\ninduced characters. Incidentally, in 1967\u20131968, the name \u201csupercuspidal\u201d had not\nemerged, and these representations were called \u201cabsolutely cuspidal,\u201d \u201ccompactly\nsupported discrete series,\u201d and other illustrative titles.\nWe also note that, in this same period, Sally and Shalika computed the characters\nof the discrete series of SL(2, F) as induced characters [75] (see also [2]), derived the\nPlancherel Formula for SL(2, F) [76], and developed an explicit Fourier transform\nfor elliptic orbital integrals in SL(2, F) [78]. This Fourier transform led directly\nto the Plancherel Formula through the use of the Shalika germ expansion [85].\nThe guide for this progression of results was the 1952 paper of Harish-Chandra on\nSL(2, R) [25].\nIn the autumn of 1969, Harish-Chandra presented his \ufb01rst complete set of notes\non reductive p-adic groups [29]. These are known as the \u201cvan Dijk Notes\u201d. These\nnotes appear to be the origin of the terms \u201csupercusp form\u201d and \u201csupercuspidal\nrepresentation\u201d.\nThey present a wealth of information about supercusp forms,\n12\nHERB\u2013SALLY\ndiscrete series characters, and other related topics. At the end of the introduction,\nHarish-Chandra states the following: \u201cOf course the main goal here is the Plancherel\nFormula. However, I hope that a correct understanding of this question would lead\nus in a natural way to the discrete series for G. (This is exactly what happens\nin the real case. But the p-adic case seems to be much more di\ufb03cult here.)\u201d It\nseems that that Harish-Chandra favored the pre\ufb01x \u201csuper\u201d as in \u201csupercusp form,\u201d\n\u201csupertempered distribution,\u201d etc.\nWe now proceed to the description of Harish-Chandra\u2019s Plancherel Theorem (see\n[40]) and Waldspurger\u2019s exposition of Harish-Chandra\u2019s ideas [93]. We then give\nan outline of the current state of the discrete series of reductive p-adic groups and\ntheir characters. Finally, we give details (as currently known) of the Plancherel\nFormula and the Fourier transform of orbital integrals.\nThe background for Harish-Chandra\u2019s Plancherel Theorem was developed in his\nWilliamstown lectures [33]. He showed that, using the philosophy of cusp forms,\none could prove a formula similar to that for real groups that we outlined in Section\n2. He was able to do this despite the lack of information about the discrete series\nand their characters.\nFollowing the model of real groups, for each special torus A, Harish-Chandra\nconstructed a subspace CA(G) from the matrix coe\ufb03cients of representations cor-\nresponding to A.\nThese representations are parabolically induced from relative\ndiscrete series representations of M, the centralizer of A. There are two notable\ndi\ufb00erences between the real case and the p-adic case. First of all, because, in the\np-adic case, there are discrete series that are not supercuspidal (for example, the\nSteinberg representation of SL(2, F)), the theory of the constant term must be\nmodi\ufb01ed. Second, because of a compactness condition on the dual of A, it is not\nnecessary to consider the asymptotics of the Plancherel measure that are required\nin the real case because of non-compactness.\nThus, even though the understanding of the discrete series and their charac-\nters for p-adic groups is quite rudimentary, Harish-Chandra succeeded in proving\na version of the Plancherel Theorem.\nThis version, as stated by Howe [39], is:\n\u201cThe (Schwartz) space C(G) is the orthogonal direct sum of wave packets formed\nfrom series of representations induced unitarily from discrete series of (the Levi\ncomponents of) parabolic subgroups P. Moreover if two such series of induced rep-\nresentations yield the same subspace of C(G), then the parabolics from which they\nare induced are associate, and the representations of the Levi components are con-\njugate.\u201d Equivalently, as stated by Harish-Chandra (Lemma 5 of The Plancherel\nFormula for Reductive p-adic Groups in [40]), if G is a connected reductive p-adic\ngroup and C(G) is the Schwartz space of G, then\nC(G) =\nX\nA\u2208S\nCA(G)\nwhere S is the set of conjugacy classes of special tori in G and the sum is orthogonal.\nIn 2002, Waldspurger produced a carefully designed version of Harish-Chandra\u2019s\nPlancherel Theorem.\nThis work is executed with remarkable precision, and we\nquote here from Waldspurger\u2019s introduction (the translation here is that of the\nauthors of the present article).\nTHE PLANCHEREL FORMULA\n13\n\u201cThe Plancherel formula is an essential tool of invariant harmonic analysis on\nreal or p-adic reductive groups. Harish-Chandra dedicated several articles to it.\nHe \ufb01rst treated the case of real groups, his last article on this subject being [36].\nA little later, he proved the formula in the p-adic case. But he published only a\nsummary of these results [40]. The complete proof was to be found in a hand-\nwritten manuscript that was hardly publishable in that state. Several years ago, L.\nClozel and the present author conceived of a project to publish these notes. This\nproject was not realized, but the preparatory work done on that occasion has now\nbecome the text that follows. It is a redaction of Harish-Chandra\u2019s proof, based on\nthe unpublished manuscript.\n. . .\nAs this article is appearing more than \ufb01fteen years after Harish-Chandra\u2019s man-\nuscript, we had the choice between scrupulously respecting the original or intro-\nducing several modi\ufb01cations taking account of the evolution of the subject in the\nmeantime. We have chosen the latter option. As this choice is debatable and the\nfashion in which we observe the subject to have evolved is rather subjective, let us\nattempt to explain the modi\ufb01cations that we have wrought.\nThere are several changes of notation: we have used those which seemed to us\nto be the most common and which have been used since Arthur\u2019s work on the trace\nformula. We work on a base \ufb01eld of any characteristic, positive characteristic caus-\ning only the slightest disturbance. We have eliminated the notion of the Eisenstein\nintegral in favor of the equivalent and more popular coe\ufb03cient of the induced rep-\nresentation. We have used the algebraic methods introduced by Bernstein. They\nallow us to demonstrate more naturally that certain functions are polynomial or\nrational, where Harish-Chandra proved their holomorphy or meromorphy. At the\nend of the article, we have slightly modi\ufb01ed the method of extending the results\nobtained for semi-simple groups to reductive groups, in particular, the manner in\nwhich one treats the center. In fact, the principal change concerns the \u2018constant\nterms\u2019 and the intertwining operators. Harish-Chandra began with the study of\nthe \u2018constant terms\u2019 of the coe\ufb03cients of the induced representations and deduced\nfrom this study the properties of the intertwining operators. These latter having\nseemed to us more popular than the \u2018constant terms,\u2019 we have inverted the order,\n\ufb01rst studying the intertwining operators, in particular their rational extension, and\nhaving deduced from this the properties of the \u2018constant terms.\u2019 All of these mod-\ni\ufb01cations remain, nevertheless, minor and concern above all the preliminaries. The\nproof of the Plancherel formula itself (sections VI, VII and VIII below) has not\nbeen altered and is exactly that of Harish-Chandra.\u201d\nIt remains to address the current status of the three central problems of harmonic\nanalysis on reductive p-adic groups.\nThese are the construction of the discrete\nseries, the determination of the characters of the discrete series, and the derivation\nof the Fourier transform of orbital integrals as linear functionals on the space of\nsupertempered distributions.\nThere is a long list of authors who have attacked the construction of discrete\nseries of p-adic groups over the past forty years. We limit ourselves to a few of the\nmajor stepping stones. The work of Howe [54] on GL(n) in the tame case set the\nstage for a great deal of the future work. Howe\u2019s supercuspidal representations for\n14\nHERB\u2013SALLY\nGL(n) were proved to be exhaustive by Moy in [68]. Further work in the direction\nof tame supercuspidals may be found in the papers [66] and [67] of L. Morris.\nIn the mid 1980s, Bushnell and Kutzko attacked GL(n) in the wild case. Their\nmain weapon was the theory of types, and the de\ufb01nitive results for GL(n) and\nSL(n) were published in [10], [11], and [12]. While in the tame case, one gets a\nreasonable parameterization in terms of characters of tori, it does not seem that\nsuch a parameterization can be expected in the wild case. It is di\ufb03cult to associate\ncertain characters with any particular torus, as well as to tell when representations\nconstructed from di\ufb00erent tori are distinct. We also mention the work of Corwin\non division algebras in both the tame [14] and the wild [15] case.\nA big breakthrough came in J.-K. Yu\u2019s construction of tame supercuspidal rep-\nresentations for a wide class of groups in [95]. In this paper, Yu points to the fact\nthat he was guided by the results of Adler [1] at the beginning of this undertaking.\nUnder certain restrictions on p, Yu\u2019s supercuspidal representations were proved to\nbe exhaustive by Ju-Lee Kim [56] using tools from harmonic analysis in a remark-\nable way. Throughout this period, the work of Moy\u2013Prasad [69], [70] was quite\nin\ufb02uential. Also, Stevens [92] succeeded in applying the Bushnell\u2013Kutzko methods\nto the classical groups to obtain all their supercuspidal representations as induced\nrepresentations when the underlying \ufb01eld has odd residual characteristic. Finally,\nmajor results have been obtained by M\u0153glin and Tadic for non-supercuspidal dis-\ncrete series in [65]. There is still much work to be done, but considerable progress\nhas been made.\nThe theory of characters has been slower in its development. There are two\navenues of approach that have been cultivated. The \ufb01rst is the local character ex-\npansion of Harish-Chandra. If O is a G-orbit in g, then O carries a G-invariant\nmeasure denoted by \u00b5O (see, for example, [74]). The Fourier transform of the dis-\ntribution f 7\u2192\u00b5O(f) is represented by a function c\n\u00b5O on g that is locally summable\non the set of regular elements g\u2032 in g. The local character expansion is:\nTheorem 4.1. Let \u03c0 be an irreducible smooth representation of G.\nThere are\ncomplex numbers cO(\u03c0), indexed by nilpotent orbits O, such that\n\u0398\u03c0(exp Y ) =\nX\nO\ncO(\u03c0) c\n\u00b5O(Y )\nfor Y su\ufb03ciently near 0 in g\u2032.\nThis result is presented in Harish-Chandra\u2019s Queen\u2019s Notes [37] and is fully\nexplicated in [41]. The local character expansion could be a very valuable tool if\nthree problems are overcome. These are: (1) determine the functions c\n\u00b5O, (2) \ufb01nd\nthe constants cO(\u03c0), and (3) determine the domain of validity of the expansion. For\nprogress in these directions, see Murnaghan [71], [72], Waldspurger [94], DeBacker\u2013\nSally [19], and DeBacker [17].\nThe second approach is the direct use of the Frobenius formula for induced\ncharacters to produce full character formulas on the regular elements in G. See\nHarish-Chandra [29] (p. 94), Sally [77], and Rader\u2013Silberger [73]. This approach\nhas been used by DeBacker for GL(\u2113), \u2113a prime [20], and Spice for SL(\u2113), \u2113a prime\n[91]. Recent work of Adler and Spice [3] and DeBacker and Reeder [18] shows some\npromise in this direction, but their results are still quite limited. The paper [3] of\nAdler and Spice gives an interesting report on the development and current status\nof character theory on reductive p-adic groups. For additional results on the theory\nTHE PLANCHEREL FORMULA\n15\nof characters, consult the papers of Cunningham and Gordon [16] and Kutzko and\nPantoja [59].\nWe \ufb01nish this paper with an update on the Plancherel Theorem, the Plancherel\nFormula, and the Fourier transform of orbital integrals in the p-adic case. As re-\ngards the Plancherel Theorem, it seems that some \ufb02esh is beginning to appear on\nthe bones. Thus, for some special cases, an explicit Plancherel measure related to\nthe components in the Schwartz space decomposition has been found (see Shahidi\n[83], [84], Kutzko\u2013Morris [60], and Aubert\u2013Plymen [6], [7]). The results seem to be\napplicable mainly to GL(n) and SL(n). In some cases, restrictions on the resid-\nual characteristic have been completely avoided. These methods seem to a great\nextent to be independent of explicit character formulas. It would be interesting\nto determine how far these techniques can be carried for general reductive p-adic\ngroups.\nIt is one of the purposes of this paper to point out the nature of the Plancherel\nFormula in the theory of harmonic analysis on reductive p-adic groups. As was the\ncase originally with Harish-Chandra, the Plancherel Formula should be considered\nas the Fourier transform of the \u03b4 distribution regarded as an invariant distribution\non a space of smooth functions on the underlying group. This is achieved in the\nreal case by determining the Fourier transform of an elliptic orbital integral and\napplying a limit formula involving di\ufb00erential operators to deduce an expression\nfor f(1) as a linear functional on the space of tempered invariant distributions.\nThis space is directly connected to the space of tempered irreducible characters of\nG along with some additional supertempered virtual characters. It appears to be\nthe case that, to accomplish this goal, one has to have a full understanding of the\nirreducible tempered characters of G. This, of course, requires a detailed knowledge\nof the discrete series. This is exactly the approach that was detailed in Section 3.\nAs pointed out by Harish-Chandra, a complete knowledge of the discrete series\nand their characters would yield the Plancherel measure for p-adic groups exactly\nas in the real case. In the p-adic case, the role of di\ufb00erential operators in the limit\nformula to obtain f(1) is assumed by the Shalika germ expansion.\nShalika Germs\nFor a connected semi-simple p-adic group G, Shalika de\ufb01nes in [85]\nIf(x) =\nZ\nG(x)\nfd\u00b5,\nwhere x is a regular element in G, G(x) is its conjugacy class, \u00b5 is a G-invariant\nmeasure on G(x), and f \u2208C\u221e\nc (G). Shalika shows that If(x) has an asymptotic\nexpansion in terms of the integrals\n\u039bO(f) =\nZ\nO\nfd\u00b5\nof f over the unipotent conjugacy classes O. Here, for O = {1}, we take \u039bO(f) =\nf(1).\nThe coe\ufb03cients CO(x) occurring in this expansion are called the Shalika\ngerms.\nWe start with G = SL(2, F) where F has odd residual characteristic, and then\nuse Shalika germs to produce the Plancherel Formula for G. This result of Sally\nand Shalika was proved in 1969 and is presented in detail in [78]. We repeat it\n16\nHERB\u2013SALLY\nhere to indicate the role that such a formula can play in the harmonic analysis on\na reductive p-adic group.\nLet T be a compact Cartan subgroup of G. For each nontrivial unipotent orbit\nO, there is a subset TO of the set of regular elements in T such that the following\nasymptotic expansion holds.\nF T\nf (t) = |D(t)|1/2If(t) \u223c\u2212AT |D(t)|1/2f(1) + BT\nX\ndim O>0\nCO(t)\u039bO(f)\nwhere the Shalika germ CO(t) is the characteristic function of TO. The constants\nAT and BT depend on normalization of measures and whether T is rami\ufb01ed or\nunrami\ufb01ed.\nBy summing products of characters, we are led to the following expression.\n\u00b5(T )If(t)\n=\nX\n\u03a0\u2208D\n\u03c7\u03a0(t) \u02c6f(\u03a0) + 1\n2\nX\n\u03a0\u2208RPSV\n\u03c7\u03a0(t) \u02c6f(\u03a0)\n\u2212\nq + 1\n2q \u00b5(A1)\nZ\n\u03be \u2208d\nF \u00d7\n\u03be|Ah0+1 = 1\n|\u0393(\u03be)|\u22122 \u02c6f(\u03be)d\u03be\n+\nq\n2\u00b5(A1)\u03baT |D(t)|\u22121/2\nZ\n\u03be \u2208d\nF \u00d7\n\u03be|Ah0+1 = 1\n\u02c6f(\u03be)d\u03be\nThis is the Fourier transform of the elliptic orbital integral corresponding to the\nregular element t. Note the occurrence of the characters of the reducible principal\nseries, denoted RPSV , corresponding to the three sgn characters on F \u00d7. As in the\ncase of SL(2, R), each represents the di\ufb00erence of two characters divided by 2, and\nthat di\ufb00erence is 0 except on the compact Cartan subgroups corresponding to the\nsgn character associated to the quadratic extension V . So again, these singular\ntempered invariant distributions (see [52]) appear in the Fourier transform of an\nelliptic orbit.\nUsing Shalika germs, we are led directly to the Plancherel Formula for SL(2, F).\n\u00b5(K)f(1) =\nX\n\u03a0\u2208D\n\u02c6f(\u03a0)d(\u03a0) + 1\n2\n\u0012q2 \u22121\nq\n\u0013\n\u00b5(A1)\nZ\n\u03be\u2208d\nF \u00d7 |\u0393(\u03be)|\u22122 \u02c6f(\u03be)d\u03be\nIt is clear that a complete theory of the Fourier transform of orbital integrals\nwould lead to direct results about lifting, matching, and transferring orbital inte-\ngrals. Such a theory would entail a deep understanding of discrete series characters\nand their properties. A start in this direction may be found in papers of Arthur\n[4], [5] and Herb [48], [49]. We expect to return to this subject in the near future.\nReferences\n[1] Je\ufb00rey D. Adler. Re\ufb01ned anisotropic K-types and supercuspidal representations. Paci\ufb01c J.\nMath., 185(1):1\u201332, 1998.\n[2] Je\ufb00rey D. Adler, Stephen DeBacker, Paul J. Sally, Jr., and Loren Spice. Supercuspidal char-\nacters of SL2 over a p-adic \ufb01eld. To appear in Harmonic Analysis on reductive, p-adic groups\n(Contemp. Math.).\n[3] Je\ufb00rey D. Adler and Loren Spice. Supercuspidal characters of reductive p-adic groups. Amer.\nJ. Math., 131(4):1137\u20131210, 2009.\n[4] James Arthur. On elliptic tempered characters. Acta Math., 171(1):73\u2013138, 1993.\n[5] James Arthur. On the Fourier transforms of weighted orbital integrals. J. Reine Angew.\nMath., 452:163\u2013217, 1994.\nTHE PLANCHEREL FORMULA\n17\n[6] Anne-Marie Aubert and Roger Plymen. Explicit Plancherel formula for the p-adic group\nGL(n). C. R. Math. Acad. Sci. Paris, 338(11):843\u2013848, 2004.\n[7] Anne-Marie Aubert and Roger Plymen. Plancherel measure for GL(n, F ) and GL(m, D):\nexplicit formulas and Bernstein decomposition. J. Number Theory, 112(1):26\u201366, 2005.\n[8] V. Bargmann. Irreducible unitary representations of the Lorentz group. Ann. of Math. (2),\n48:568\u2013640, 1947.\n[9] Fran\u00b8cois Bruhat. Sur les r\u00b4epr\u00b4esentations des groupes classiques P -adiques. I, II. Amer. J.\nMath., 83:321\u2013338, 343\u2013368, 1961.\n[10] Colin J. Bushnell and Philip C. Kutzko. The admissible dual of GL(N) via compact open sub-\ngroups, volume 129 of Annals of Mathematics Studies. Princeton University Press, Princeton,\nNJ, 1993.\n[11] Colin J. Bushnell and Philip C. Kutzko. The admissible dual of SL(N). I. Ann. Sci. \u00b4Ecole\nNorm. Sup. (4), 26(2):261\u2013280, 1993.\n[12] Colin J. Bushnell and Philip C. Kutzko. The admissible dual of SL(N). II. Proc. London\nMath. Soc. (3), 68(2):317\u2013379, 1994.\n[13] Wen-Min Chao. Fourier inversion and Plancherel formula for semisimple Lie groups of real\nrank two. University of Chicago Thesis, 1977.\n[14] Lawrence Corwin. Representations of division algebras over local \ufb01elds. Advances in Math.,\n13:259\u2013267, 1974.\n[15] Lawrence Corwin. The unitary dual for the multiplicative group of arbitrary division algebras\nover local \ufb01elds. J. Amer. Math. Soc., 2(3):565\u2013598, 1989.\n[16] Clifton Cunningham and Julia Gordon. Motivic proof of a character formula for SL(2). Ex-\nperiment. Math., 18(1):11\u201344, 2009.\n[17] Stephen Debacker. Homogeneity results for invariant distributions of a reductive p-adic group.\nAnn. Sci. \u00b4Ecole Norm. Sup. (4), 35(3):391\u2013422, 2002.\n[18] Stephen DeBacker and Mark Reeder. Depth-zero supercuspidal L-packets and their stability.\nAnn. of Math. (2), 169(3):795\u2013901, 2009.\n[19] Stephen DeBacker and Paul J. Sally, Jr. Germs, characters, and the Fourier transforms of\nnilpotent orbits. In The mathematical legacy of Harish-Chandra (Baltimore, MD, 1998),\nvolume 68 of Proc. Sympos. Pure Math., pages 191\u2013221. Amer. Math. Soc., Providence, RI,\n2000.\n[20] Stephen M. DeBacker. On supercuspidal characters of GL\u2113, \u2113a prime. University of Chicago\nThesis, 1997.\n[21] Gerald B. Folland. A course in abstract harmonic analysis. Studies in Advanced Mathematics.\nCRC Press.\n[22] I. M. Gel\u2032fand and M. I. Graev. Representations of the group of second-order matrices with\nelements in a locally compact \ufb01eld and special functions on locally compact \ufb01elds. Uspehi\nMat. Nauk, 18(4 (112)):29\u201399, 1963.\n[23] I. M. Gel\u2032fand and M. A. Na\u02d8\u0131mark. Unitarnye predstavleniya klassi\u02c7ceskih grupp. Trudy Mat.\nInst. Steklov., vol. 36. Izdat. Nauk SSSR, Moscow-Leningrad, 1950.\n[24] Harish-Chandra. Plancherel formula for complex semi-simple Lie groups. Proc. Nat. Acad.\nSci. U. S. A., 37:813\u2013818, 1951.\n[25] Harish-Chandra. Plancherel formula for the 2 \u00d7 2 real unimodular group. Proc. Nat. Acad.\nSci. U. S. A., 38:337\u2013342, 1952.\n[26] Harish-Chandra. Discrete series for semisimple Lie groups. I. Construction of invariant\neigendistributions. Acta Math., 113:241\u2013318, 1965.\n[27] Harish-Chandra. Discrete series for semisimple Lie groups. II. Explicit determination of the\ncharacters. Acta Math., 116:1\u2013111, 1966.\n[28] Harish-Chandra. Two theorems on semi-simple Lie groups. Ann. of Math. (2), 83:74\u2013128,\n1966.\n[29] Harish-Chandra. Harmonic analysis on reductive p-adic groups. Lecture Notes in Mathemat-\nics, Vol. 162. Springer-Verlag, Berlin, 1970. Notes by G. van Dijk.\n[30] Harish-Chandra. Harmonic analysis on semisimple Lie groups. Bull. Amer. Math. Soc.,\n76:529\u2013551, 1970.\n[31] Harish-Chandra. Some applications of the Schwartz space of a semisimple Lie group. In\nLectures in Modern Analysis and Applications. II, Lecture Notes in Mathematics, Vol. 140,\npages 1\u20137. Springer, Berlin, 1970.\n18\nHERB\u2013SALLY\n[32] Harish-Chandra. On the theory of the Eisenstein integral. In Conference on Harmonic Anal-\nysis (Univ. Maryland, College Park, Md., 1971), pages 123\u2013149. Lecture Notes in Math., Vol\n. 266. Springer, Berlin, 1972.\n[33] Harish-Chandra. Harmonic analysis on reductive p-adic groups. In Harmonic analysis on\nhomogeneous spaces (Proc. Sympos. Pure Math., Vol. XXVI, Williams Coll., Williamstown,\nMass., 1972), pages 167\u2013192. Amer. Math. Soc., Providence, R.I., 1973.\n[34] Harish-Chandra. Harmonic analysis on real reductive groups. I. The theory of the constant\nterm. J. Functional Analysis, 19:104\u2013204, 1975.\n[35] Harish-Chandra. Harmonic analysis on real reductive groups. II. Wavepackets in the Schwartz\nspace. Invent. Math., 36:1\u201355, 1976.\n[36] Harish-Chandra. Harmonic analysis on real reductive groups. III. The Maass-Selberg relations\nand the Plancherel formula. Ann. of Math. (2), 104(1):117\u2013201, 1976.\n[37] Harish-Chandra. Admissible invariant distributions on reductive p-adic groups. In Lie theories\nand their applications (Proc. Ann. Sem. Canad. Math. Congr., Queen\u2019s Univ., Kingston,\nOnt., 1977), pages 281\u2013347. Queen\u2019s Papers in Pure Appl. Math., No. 48. Queen\u2019s Univ.,\nKingston, Ont., 1978.\n[38] Harish-Chandra. Supertempered distributions on real reductive groups. In Studies in applied\nmathematics, volume 8 of Adv. Math. Suppl. Stud., pages 139\u2013153. Academic Press, New\nYork, 1983.\n[39] Harish-Chandra. Collected papers. Vol. I. Springer-Verlag, New York, 1984. 1944\u20131954,\nEdited and with an introduction by V. S. Varadarajan, With introductory essays by Nolan\nR. Wallach and Roger Howe.\n[40] Harish-Chandra. Collected papers. Vol. IV. Springer-Verlag, New York, 1984. 1970\u20131983,\nEdited by V. S. Varadarajan.\n[41] Harish-Chandra. Admissible invariant distributions on reductive p-adic groups, volume 16\nof University Lecture Series. American Mathematical Society, Providence, RI, 1999. Preface\nand notes by Stephen DeBacker and Paul J. Sally, Jr.\n[42] R. A. Herb and P. J. Sally, Jr. Singular invariant eigendistributions as characters. Bull. Amer.\nMath. Soc., 83(2):252\u2013254, 1977.\n[43] R. A. Herb and P. J. Sally, Jr. Singular invariant eigendistributions as characters in the\nFourier transform of invariant distributions. J. Funct. Anal., 33(2):195\u2013210, 1979.\n[44] Rebecca A. Herb. Fourier inversion of invariant integrals on semisimple real Lie groups. Trans.\nAmer. Math. Soc., 249(2):281\u2013302, 1979.\n[45] Rebecca A. Herb. Fourier inversion and the Plancherel theorem. In Noncommutative har-\nmonic analysis and Lie groups (Marseille, 1980), volume 880 of Lecture Notes in Math.,\npages 197\u2013210. Springer, Berlin, 1981.\n[46] Rebecca A. Herb. Fourier inversion and the Plancherel theorem for semisimple real Lie groups.\nAmer. J. Math., 104(1):9\u201358, 1982.\n[47] Rebecca A. Herb. Discrete series characters and Fourier inversion on semisimple rea l Lie\ngroups. Trans. Amer. Math. Soc., 277(1):241\u2013262, 1983.\n[48] Rebecca A. Herb. Elliptic representations for Sp(2n) and SO(n). Paci\ufb01c J. Math., 161(2):347\u2013\n358, 1993.\n[49] Rebecca A. Herb. Supertempered virtual characters. Compositio Math., 93(2):139\u2013154, 1994.\n[50] Rebecca A. Herb. Discrete series characters and two-structures. Trans. Amer. Math. Soc.,\n350(8):3341\u20133369, 1998.\n[51] Rebecca A. Herb. Two-structures and discrete series character formulas. In The mathematical\nlegacy of Harish-Chandra (Baltimore, MD, 1998), volume 68 of Proc. Sympos. Pure Math.,\npages 285\u2013319. Amer. Math. Soc., Providence, RI, 2000.\n[52] Rebecca A. Herb, Nick Ramsey, and Paul J. Sally, Jr. Some remarks on the representations\nof p-adic SL2. To appear.\n[53] Takeshi Hirai. Invariant eigendistributions of Laplace operators on real simple Lie groups. III.\nMethods of construction for semisimple Lie groups. Japan. J. Math. (N.S.), 2(2):269\u2013341,\n1976.\n[54] Roger E. Howe. Tamely rami\ufb01ed supercuspidal representations of Gln. Paci\ufb01c J. Math.,\n73(2):437\u2013460, 1977.\n[55] H. Jacquet and R. P. Langlands. Automorphic forms on GL(2). Lecture Notes in Mathemat-\nics, Vol. 114. Springer-Verlag, Berlin, 1970.\nTHE PLANCHEREL FORMULA\n19\n[56] Ju-Lee Kim. Supercuspidal representations: an exhaustion theorem. J. Amer. Math. Soc.,\n20(2):273\u2013320 (electronic), 2007.\n[57] A. W. Knapp. Commutativity of intertwining operators. II. Bull. Amer. Math. Soc.,\n82(2):271\u2013273, 1976.\n[58] A. W. Knapp and Gregg Zuckerman. Classi\ufb01cation of irreducible tempered representations\nof semi-simpl e Lie groups. Proc. Nat. Acad. Sci. U.S.A., 73(7):2178\u20132180, 1976.\n[59] Phil Kutzko and Jos\u00b4e Pantoja. Character formulas for supercuspidal representations of the\ngroups GL2, SL2. Comm. Algebra, 26(6):1679\u20131697, 1998.\n[60] Philip Kutzko and Lawrence Morris. Explicit Plancherel theorems for H(q1, q2) and SL2(F ).\nPure Appl. Math. Q., 5(1):435\u2013467, 2009.\n[61] R. P. Langlands. Problems in the theory of automorphic forms. In Lectures in modern analysis\nand applications, III, pages 18\u201361. Lecture Notes in Math., Vol. 170. Springer, Berlin, 1970.\n[62] I. G. Macdonald. Spherical functions on a p-adic Chevalley group. Bull. Amer. Math. Soc.,\n74:520\u2013525, 1968.\n[63] F. I. Mautner. Unitary representations of locally compact groups. II. Ann. of Math. (2),\n52:528\u2013556, 1950.\n[64] F. I. Mautner. Spherical functions over p-adic \ufb01elds. II. Amer. J. Math., 86:171\u2013200, 1964.\n[65] Colette M\u0153glin and Marko Tadi\u00b4c. Construction of discrete series for classical p-adic groups.\nJ. Amer. Math. Soc., 15(3):715\u2013786 (electronic), 2002.\n[66] Lawrence Morris. Tamely rami\ufb01ed supercuspidal representations of classical groups. I. Fil-\ntrations. Ann. Sci. \u00b4Ecole Norm. Sup. (4), 24(6):705\u2013738, 1991.\n[67] Lawrence Morris. Tamely rami\ufb01ed supercuspidal representations of classical groups. II. Rep-\nresentation theory. Ann. Sci. \u00b4Ecole Norm. Sup. (4), 25(3):233\u2013274, 1992.\n[68] Allen Moy. Local constants and the tame Langlands correspondence. Amer. J. Math.,\n108(4):863\u2013930, 1986.\n[69] Allen Moy and Gopal Prasad. Unre\ufb01ned minimal K-types for p-adic groups. Invent. Math.,\n116(1-3):393\u2013408, 1994.\n[70] Allen Moy and Gopal Prasad. Jacquet functors and unre\ufb01ned minimal K-types. Comment.\nMath. Helv., 71(1):98\u2013121, 1996.\n[71] Fiona Murnaghan. Characters of supercuspidal representations of classical groups. Ann. Sci.\n\u00b4Ecole Norm. Sup. (4), 29(1):49\u2013105, 1996.\n[72] Fiona Murnaghan. Local character expansions and Shalika germs for GL(n). Math. Ann.,\n304(3):423\u2013455, 1996.\n[73] Cary Rader and Allan Silberger. Some consequences of Harish-Chandra\u2019s submersion princi-\nple. Proc. Amer. Math. Soc., 118(4):1271\u20131279, 1993.\n[74] R. Ranga Rao. Orbital integrals in reductive groups. Ann. of Math., 96:505\u2013510, 1972.\n[75] P. J. Sally, Jr. and J. A. Shalika. Characters of the discrete series of representations of SL(2)\nover a local \ufb01eld. Proc. Nat. Acad. Sci. U.S.A., 61:1231\u20131237, 1968.\n[76] P. J. Sally, Jr. and J. A. Shalika. The Plancherel formula for SL(2) over a local \ufb01eld. Proc.\nNat. Acad. Sci. U.S.A., 63:661\u2013667, 1969.\n[77] Paul J. Sally, Jr. Some remarks on discrete series characters for reductive p-adic groups.\nIn Representations of Lie groups, Kyoto, Hiroshima, 1986 , volume 14 of Adv. Stud. Pure\nMath., pages 337\u2013348. Academic Press, Boston, MA, 1988.\n[78] Paul J. Sally, Jr. and Joseph A. Shalika. The Fourier transform of orbital integrals on SL2\nove r a p-adic \ufb01eld. In Lie group representations, II (College Park, Md., 1982/1983), volume\n1041 of Lecture Notes in Math., pages 303\u2013340. Springer, Berlin, 1984.\n[79] Paul J. Sally, Jr. and Garth Warner. The Fourier transform on semisimple Lie groups of real\nrank one. Acta Math., 131:1\u201326, 1973.\n[80] Ichir\u02c6o Satake. Theory of spherical functions on reductive algebraic groups over p-adic \ufb01elds.\nInst. Hautes \u00b4Etudes Sci. Publ. Math., (18):5\u201369, 1963.\n[81] Wilfried Schmid. On the characters of the discrete series. The Hermitian symmetri c case.\nInvent. Math., 30(1):47\u2013144, 1975.\n[82] I. E. Segal. An extension of Plancherel\u2019s formula to separable unimodular groups. Ann. of\nMath. (2), 52:272\u2013292, 1950.\n[83] Freydoon Shahidi. Fourier transforms of intertwining operators and Plancherel measures for\nGL(n). Amer. J. Math., 106(1):67\u2013111, 1984.\n[84] Freydoon Shahidi. A proof of Langlands\u2019 conjecture on Plancherel measures; complementary\nseries for p-adic groups. Ann. of Math. (2), 132(2):273\u2013330, 1990.\n20\nHERB\u2013SALLY\n[85] J. A. Shalika. A theorem on semi-simple P -adic groups. Ann. of Math. (2), 95:226\u2013242, 1972.\n[86] Joseph A. Shalika. Representation of the two by two unimodular group over local \ufb01elds.\nIn Contributions to automorphic forms, geometry, and number theory, pages 1\u201338. Johns\nHopkins Univ. Press, Baltimore, MD, 2004.\n[87] Takuro Shintani. On certain square-integrable irreducible unitary representations of some\np-adic linear groups. J. Math. Soc. Japan, 20:522\u2013565, 1968.\n[88] Allan J. Silberger. PGL2 over the p-adics:\nits representations, spherical functions, and\nFourier analysis. Lecture Notes in Mathematics, Vol. 166. Springer-Verlag, Berlin, 1970.\n[89] Allan J. Silberger. Harish-Chandra\u2019s Plancherel theorem for p-adic groups. Trans. Amer.\nMath. Soc., 348(11):4673\u20134686, 1996.\n[90] Allan J. Silberger. Correction to: \u201cHarish-Chandra\u2019s Plancherel theorem for p-adic groups\u201d\n[Trans. Amer. Math. Soc. 348 (1996), no. 11, 4673\u20134686; MR1370652 (99c:22026)]. Trans.\nAmer. Math. Soc., 352(4):1947\u20131949, 2000.\n[91] Loren Spice. Supercuspidal characters of SLl over a p-adic \ufb01eld, l a prime. Amer. J. Math.,\n127(1):51\u2013100, 2005.\n[92] Shaun Stevens. The supercuspidal representations of p-adic classical groups. Invent. Math.,\n172(2):289\u2013352, 2008.\n[93] J.-L. Waldspurger. La formule de Plancherel pour les groupes p-adiques (d\u2019apr`es Harish-\nChandra). J. Inst. Math. Jussieu, 2(2):235\u2013333, 2003.\n[94] Jean-Loup Waldspurger. Int\u00b4egrales orbitales nilpotentes et endoscopie pour les groupes clas-\nsiques non rami\ufb01\u00b4es. Ast\u00b4erisque, (269):vi+449, 2001.\n[95] Jiu-Kang Yu. Construction of tame supercuspidal representations. J. Amer. Math. Soc.,\n14(3):579\u2013622 (electronic), 2001.\n[96] Gregg Zuckerman. Tensor products of \ufb01nite and in\ufb01nite dimensional representations o f\nsemisimple Lie groups. Ann. Math. (2), 106(2):295\u2013308, 1977.\n",
        "sentence": "",
        "context": "semisimple Lie groups. Ann. Math. (2), 106(2):295\u2013308, 1977.\nmathematics, volume 8 of Adv. Math. Suppl. Stud., pages 139\u2013153. Academic Press, New\nYork, 1983.\n[39] Harish-Chandra. Collected papers. Vol. I. Springer-Verlag, New York, 1984. 1944\u20131954,\n116(1-3):393\u2013408, 1994.\n[70] Allen Moy and Gopal Prasad. Jacquet functors and unre\ufb01ned minimal K-types. Comment.\nMath. Helv., 71(1):98\u2013121, 1996.\n[71] Fiona Murnaghan. Characters of supercuspidal representations of classical groups. Ann. Sci."
    },
    {
        "title": "Austerity in MCMC land: Cutting the Metropolis-Hastings budget",
        "author": [
            "A. Korattikara",
            "Y. Chen",
            "M. Welling"
        ],
        "venue": "In Proc. of 31st ICML,",
        "citeRegEx": "Korattikara et al\\.,? \\Q2014\\E",
        "shortCiteRegEx": "Korattikara et al\\.",
        "year": 2014,
        "abstract": "",
        "full_text": "",
        "sentence": " MCMC methods are computationally expensive, and recent years have seen the introduction of biased MCMC procedures (see, e.g., Welling & Teh, 2011; Ahn et al., 2012; Korattikara et al., 2014) that exchange asymptotic correctness for increased sampling speed.",
        "context": null
    },
    {
        "title": "Stein\u2019s method for comparison of univariate distributions",
        "author": [
            "C. Ley",
            "G. Reinert",
            "Y. Swan"
        ],
        "venue": "Probab. Surveys,",
        "citeRegEx": "Ley et al\\.,? \\Q2017\\E",
        "shortCiteRegEx": "Ley et al\\.",
        "year": 2017,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " Selecting a Stein operator and a Stein set A standard, widely applicable univariate Stein operator is the density method operator (see Stein et al., 2004; Chatterjee & Shao, 2011; Chen et al., 2011; Ley et al., 2017), (T g)(x) , 1 p(x) d dx (p(x)g(x)) = g(x)b(x) + g \u2032(x).",
        "context": null
    },
    {
        "title": "Two methods for wild variational inference",
        "author": [
            "Q. Liu",
            "Y. Feng"
        ],
        "venue": "arXiv preprint arXiv:1612.00081,",
        "citeRegEx": "Liu and Feng,? \\Q2016\\E",
        "shortCiteRegEx": "Liu and Feng",
        "year": 2016,
        "abstract": "Variational inference provides a powerful tool for approximate probabilistic\nin- ference on complex, structured models. Typical variational inference\nmethods, however, require to use inference networks with computationally\ntractable proba- bility density functions. This largely limits the design and\nimplementation of vari- ational inference methods. We consider wild variational\ninference methods that do not require tractable density functions on the\ninference networks, and hence can be applied in more challenging cases. As an\nexample of application, we treat stochastic gradient Langevin dynamics (SGLD)\nas an inference network, and use our methods to automatically adjust the step\nsizes of SGLD, yielding significant improvement over the hand-designed step\nsize schemes",
        "full_text": "TWO METHODS FOR WILD VARIATIONAL INFERENCE\nQiang Liu\nYihao Feng\nComputer Science, Dartmouth College\nHanover, NH, 03755\n{qiang.liu, yihao.feng.gr}@dartmouth.edu\nABSTRACT\nVariational inference provides a powerful tool for approximate probabilistic in-\nference on complex, structured models. Typical variational inference methods,\nhowever, require to use inference networks with computationally tractable proba-\nbility density functions. This largely limits the design and implementation of vari-\national inference methods. We consider wild variational inference methods that\ndo not require tractable density functions on the inference networks, and hence\ncan be applied in more challenging cases. As an example of application, we treat\nstochastic gradient Langevin dynamics (SGLD) as an inference network, and use\nour methods to automatically adjust the step sizes of SGLD, yielding signi\ufb01cant\nimprovement over the hand-designed step size schemes.\n1\nINTRODUCTION\nProbabilistic modeling provides a principled approach for reasoning under uncertainty, and has been\nincreasingly dominant in modern machine learning where highly complex, structured probabilistic\nmodels are often the essential components for solving complex problems with increasingly larger\ndatasets. A key challenge, however, is to develop computationally ef\ufb01cient Bayesian inference\nmethods to approximate, or draw samples from the posterior distributions. Variational inference\n(VI) provides a powerful tool for scaling Bayesian inference to complex models and big data. The\nbasic idea of VI is to approximate the true distribution with a simpler distribution by minimizing the\nKL divergence, transforming the inference problem into an optimization problem, which is often\nthen solved ef\ufb01ciently using stochastic optimization techniques (e.g., Hoffman et al., 2013; Kingma\n& Welling, 2013). However, the practical design and application of VI are still largely restricted by\nthe requirement of using simple approximation families, as we explain in the sequel.\nLet p(z) be a distribution of interest, such as the posterior distribution in Bayesian inference. VI\napproximates p(z) with a simpler distribution q\u2217(z) found in a set Q = {q\u03b7(z)} of distributions\nindexed by parameter \u03b7 by minimizing the KL divergence objective:\nmin\n\u03b7\n\b\nKL(q\u03b7 || p) \u2261Ez\u223cq\u03b7[log(q\u03b7(z)/p(z))]\n\t\n,\n(1)\nwhere we can get exact result p = q\u2217if Q is chosen to be broad enough to actually include p. In\npractice, however, Q should be chosen carefully to make the optimization in (1) computationally\ntractable; this casts two constraints on Q:\n1. A minimum requirement is that we should be able to sample from q\u03b7 ef\ufb01ciently, which allows us\nto make estimates and predictions based on q\u03b7 in placement of the more intractable p. The samples\nfrom q\u03b7 can also be used to approximate the expectation Eq[\u00b7] in (1) during optimization. This means\nthat there should exist some computable function f(\u03b7; \u03be), called the inference network, which takes\na random seed \u03be, whose distribution is denoted by q0, and outputs a random variable z = f(\u03b7; \u03be)\nwhose distribution is q\u03b7.\n2. We should also be able to calculate the density q\u03b7(z) or it is derivative in order to optimize the\nKL divergence in (1). This, however, casts a much more restrictive condition, since it requires us to\nuse only simple inference network f(\u03b7; \u03be) and input distributions q0 to ensure a tractable form for\nthe density q\u03b7 of the output z = f(\u03b7; \u03be).\nIn fact, it is this requirement of calculating q\u03b7(z) that has been the major constraint for the design\nof state-of-the-art variational inference methods. The traditional VI methods are often limited to\n1\narXiv:1612.00081v2  [stat.ML]  30 Oct 2017\nGiven distribution\nInference network\nSamples\nFigure 1: Wild variational inference allows us to train general stochastic neural inference networks to learn to\ndraw (approximate) samples from the target distributions, without restriction on the computational tractability\nof the density function of the neural inference networks.\nusing simple mean \ufb01eld, or Gaussian-based distributions as q\u03b7 and do not perform well for approx-\nimating complex target distributions. There is a line of recent work on variational inference with\nrich approximation families (e.g., Rezende & Mohamed, 2015b; Tran et al., 2015; Ranganath et al.,\n2015, to name only a few), all based on handcrafting special inference networks to ensure the com-\nputational tractability of q\u03b7(z) while simultaneously obtaining high approximation accuracy. These\napproaches require substantial mathematical insights and research effects, and can be dif\ufb01cult to\nunderstand or use for practitioners without a strong research background in VI. Methods that allow\nus to use arbitrary inference networks without substantial constraints can signi\ufb01cantly simplify the\ndesign and applications of VI methods, allowing practical users to focus more on choosing proposals\nthat work best with their speci\ufb01c tasks.\nWe use the term wild variational inference to refer to variants of variational methods working with\ngeneral inference networks f(\u03b7, \u03be) without tractability constraints on its output density q\u03b7(z); this\nshould be distinguished with the black-box variational inference (Ranganath et al., 2014) which\nrefers to methods that work for generic target distributions p(z) without signi\ufb01cant model-by-model\nconsideration (but still require to calculate the proposal density q\u03b7(z)). Essentially, wild variational\ninference makes it possible to \u201clearn to draw samples\u201d, constructing black-box neural samplers for\ngiven distributions. This enables more adaptive and automatic design of ef\ufb01cient Bayesian infer-\nence procedures, replacing the hand-designed inference algorithms with more ef\ufb01cient ones that can\nimprove their ef\ufb01ciency adaptively over time based on past tasks they performed.\nIn this work, we discuss two methods for wild variational inference, both based on recent works that\ncombine kernel techniques with Stein\u2019s method (e.g., Liu & Wang, 2016; Liu et al., 2016). The \ufb01rst\nmethod, also discussed in Wang & Liu (2016), is based on iteratively adjusting parameter \u03b7 to make\nthe random output z = f(\u03b7; \u03be) mimic a Stein variational gradient direction (SVGD) (Liu & Wang,\n2016) that optimally decreases its KL divergence with the target distribution. The second method is\nbased on minimizing a kernelized Stein discrepancy, which, unlike KL divergence, does not require\nto calculate density q\u03b7(z) for the optimization thanks to its special form.\nAnother critical problem is to design good network architectures well suited for Bayesian infer-\nence. Ideally, the network design should leverage the information of the target distribution p(z)\nin a convenient way. One useful perspective is that we can view the existing MC/MCMC meth-\nods as (hand-designed) stochastic neural networks which can be used to construct native inference\nnetworks for given target distributions. On the other hand, using existing MC/MCMC methods as\ninference networks also allow us to adaptively adjust the hyper-parameters of these algorithms; this\nenables amortized inference which leverages the experience on past tasks to accelerate the Bayesian\ncomputation, providing a powerful approach for designing ef\ufb01cient algorithms in settings when a\nlarge number of similar tasks are needed.\nAs an example, we leverage stochastic gradient Langevin dynamics (SGLD) (Welling & Teh, 2011)\nas the inference network, which can be treated as a special deep residential network (He et al.,\n2016), in which important gradient information \u2207z log p(z) is fed into each layer to allow ef\ufb01cient\napproximation for the target distribution p(z). In our case, the network parameter \u03b7 are the step sizes\nof SGLD, and our method provides a way to adaptively improve the step sizes, providing speed-up\non future tasks with similar structures. We show that the adaptively estimated step sizes signi\ufb01cantly\noutperform the hand-designed schemes such as Adagrad.\nRelated Works\nThe idea of amortized inference (Gershman & Goodman, 2014) has been recently\napplied in various domains of probabilistic reasoning, including both amortized variational inference\n2\n(e.g., Kingma & Welling, 2013; Rezende & Mohamed, 2015a) and date-driven designs of Monte\nCarlo based methods (e.g., Paige & Wood, 2016), to name only a few. Most of these methods,\nhowever, require to explicitly calculate q\u03b7(z) (or its gradient).\nOne well exception is a very recent work (Ranganath et al., 2016) that also avoids calculating q\u03b7(z)\nand hence works for general inference networks; their method is based on a similar idea related\nto Stein discrepancy (Liu et al., 2016; Oates et al., 2017; Chwialkowski et al., 2016; Gorham &\nMackey, 2015), for which we provide a more detailed discussion in Section 3.2.\nThe auxiliary variational inference methods (e.g., Agakov & Barber, 2004) provide an alternative\nway when the variational distribution q\u03b7(z) can be represented as a hidden variable model. In\nparticular, Salimans et al. (2015) used the auxiliary variational approach to leverage MCMC as a\nvariational approximation. These approaches, however, still require to write down the likelihood\nfunction on the augmented spaces, and need to introduce an additional inference network related to\nthe auxiliary variables.\nThere is a large literature on traditional adaptive MCMC methods (e.g., Andrieu & Thoms, 2008;\nRoberts & Rosenthal, 2009) which can be used to adaptively adjust the proposal distribution of\nMCMC by exploiting the special theoretical properties of MCMC (e.g., by minimizing the auto-\ncorrelation). Our method is simpler, more generic, and works ef\ufb01ciently in practice thanks to the\nuse of gradient-based back-propagation. Finally, connections between stochastic gradient descent\nand variational inference have been discussed and exploited in Mandt et al. (2016); Maclaurin et al.\n(2015).\nOutline\nSection 2 introduces background on Stein discrepancy and Stein variational gradient de-\nscent. Section 3 discusses two methods for wild variational inference. Section 4 discuss using\nstochastic gradient Langevin dynamics (SGLD) as the inference network. Empirical results are\nshown in Section 5.\n2\nSTEIN\u2019S IDENTITY, STEIN DISCREPANCY, STEIN VARIATIONAL GRADIENT\nStein\u2019s identity\nStein\u2019s identity plays a fundamental role in our framework. Let p(z) be a positive\ndifferentiable density on Rd, and \u03c6(z) = [\u03c61(z), \u00b7 \u00b7 \u00b7 , \u03c6d(z)]\u22a4is a differentiable vector-valued\nfunction. De\ufb01ne \u2207z \u00b7 \u03c6 = P\ni \u2202zi\u03c6. Stein\u2019s identity is\nEz\u223cp[\u27e8\u2207z log p(z), \u03c6(z)\u27e9+ \u2207z \u00b7 \u03c6(z)] =\nZ\nX\n\u2207z \u00b7 (p(z)\u03c6(z))dx = 0,\n(2)\nwhich holds once p(z)\u03c6(z) vanishes on the boundary of X by integration by parts or Stokes\u2019 theo-\nrem; It is useful to rewrite Stein\u2019s identity in a more compact way:\nEz\u223cp[Tp\u03c6(z)] = 0,\nwith\nTp\u03c6\ndef\n= \u27e8\u2207z log p, \u03c6\u27e9+ \u2207z \u00b7 \u03c6,\n(3)\nwhere Tp is called a Stein operator, which acts on function \u03c6 and returns a zero-mean function\nTp\u03c6(z) under z \u223cp. A key computational advantage of Stein\u2019s identity and Stein operator is\nthat they depend on p only through the derivative of the log-density \u2207z log p(z), which does not\ndepend on the cumbersome normalization constant of p, that is, when p(z) = \u00afp(z)/Z, we have\n\u2207z log p(z) = \u2207z log \u00afp(z), independent of the normalization constant Z. This property makes\nStein\u2019s identity a powerful practical tool for handling unnormalized distributions widely appeared in\nmachine learning and statistics.\nStein Discrepancy\nAlthough Stein\u2019s identity ensures that Tp\u03c6 has zero expectation under p, its\nexpectation is generally non-zero under a different distribution q. Instead, for p \u0338= q, there must exist\na \u03c6 which distinguishes p and q in the sense that Ez\u223cq[Tp\u03c6(z)] \u0338= 0. Stein discrepancy leverages\nthis fact to measure the difference between p and q by considering the \u201cmaximum violation of Stein\u2019s\nidentity\u201d for \u03c6 in certain function set F:\nD(q || p) = max\n\u03c6\u2208F\n\b\nEz\u223cq[Tp\u03c6(z)]\n\t\n,\n(4)\nwhere F is the set of functions \u03c6 that we optimize over, and decides both the discriminative power\nand computational tractability of Stein discrepancy. Kernelized Stein discrepancy (KSD) is a special\n3\nStein discrepancy that takes F to be the unit ball of vector-valued reproducing kernel Hilbert spaces\n(RKHS), that is,\nF = {\u03c6 \u2208Hd : ||\u03c6||Hd \u22641},\n(5)\nwhere H is a real-valued RKHS with kernel k(z, z\u2032). This choice of F makes it possible to get a\nclosed form solution for the optimization in (4) (Liu et al., 2016; Chwialkowski et al., 2016; Oates\net al., 2017):\nD(q || p) = max\n\u03c6\u2208Hd\n\b\nEz\u223cq[Tp\u03c6(z)],\ns.t.\n||\u03c6||Hd \u22641\n\t\n,\n(6)\n=\nq\nEz,z\u2032\u223cq[\u03bap(z, z\u2032)],\n(7)\nwhere \u03bap(z, z\u2032) is a positive de\ufb01nite kernel obtained by applying Stein operator on k(z, z\u2032) twice:\n\u03bap(z, z\u2032) = T z\u2032\np (T z\np \u2297k(z, z\u2032)),\n= sp(z)sp(z\u2032)k(z, z\u2032) + sp(z)\u2207z\u2032k(z, z\u2032) + sp(z\u2032)\u2207zk(z, z\u2032) + \u2207z \u00b7 (\u2207z\u2032k(z, z\u2032)),\n(8)\nwhere sp(z) = \u2207z log p(z) and T z\np and T z\np denote the Stein operator when treating k(z, z\u2032) as a\nfunction of z and z\u2032, respectively; here we de\ufb01ned T z\np \u2297k(z, z\u2032) = \u2207x log p(x)k(z, z\u2032)+\u2207xk(z, z\u2032)\nwhich returns a d \u00d7 1 vector-valued function. It can be shown that D(q || p) = 0 if and only if q = p\nwhen k(z, z\u2032) is strictly positive de\ufb01nite in a proper sense (Liu et al., 2016; Chwialkowski et al.,\n2016). D(q || p) can treated as a variant of maximum mean discrepancy equipped with kernel\n\u03bap(z, z\u2032) which depends on p (which makes D(q || p) asymmetric on q and p).\nThe form of KSD in (6) allows us to estimate the discrepancy between a set of sample {zi} (e.g.,\ndrawn from q) and a distribution p speci\ufb01ed by \u2207z log p(z),\n\u02c6D2\nu({zi} || p) =\n1\nn(n \u22121)\nX\ni\u0338=j\n[\u03bap(zi, zj)],\n\u02c6D2\nv({zi} || p) = 1\nn2\nX\ni,j\n[\u03bap(zi, zj)],\n(9)\nwhere \u02c6D2\nu(q || p) provides an unbiased estimator (hence called a U-statistic) for D2(q || p), and\n\u02c6D2\nv(q || p), called V -statistic, provides a biased estimator but is guaranteed to be always non-\nnegative: \u02c6D2\nv({zi} || p) \u22650.\nStein Variational Gradient Descent (SVGD)\nStein operator and Stein discrepancy have a close\nconnection with KL divergence, which is exploited in Liu & Wang (2016) to provide a general\npurpose deterministic approximate sampling method. Assume that {zi}n\ni=1 is a sample (or a set\nof particles) drawn from q, and we want to update {zi}n\ni=1 to make it \u201cmove closer\u201d to the target\ndistribution p to improve the approximation quality. We consider updates of form\nzi \u2190zi + \u03f5\u03c6\u2217(zi),\n\u2200i = 1, . . . , n,\n(10)\nwhere \u03c6\u2217is a perturbation direction, or velocity \ufb01eld, chosen to maximumly decrease the KL diver-\ngence between the distribution of updated particles and the target distribution, in the sense that\n\u03c6\u2217= arg max\n\u03c6\u2208F\n\u001a\n\u2212d\nd\u03f5KL(q[\u03f5\u03c6] || p)\n\f\f\n\u03f5=0\n\u001b\n,\n(11)\nwhere q[\u03f5\u03c6] denotes the density of the updated particle z\u2032 = z + \u03f5\u03c6(z) when the density of the\noriginal particle z is q, and F is the set of perturbation directions that we optimize over. A key ob-\nservation (Liu & Wang, 2016) is that the optimization in (11) is in fact equivalent to the optimization\nfor KSD in (4); we have\n\u2212d\nd\u03f5KL(q[\u03f5\u03c6] || p)\n\f\f\n\u03f5=0 = Ez\u223cq[Tp\u03c6(z)],\n(12)\nthat is, the Stein operator transforms the perturbation \u03c6 on the random variable (the particles) to the\nchange of the KL divergence. Taking F to be unit ball of Hd as in (5), the optimal solution \u03c6\u2217of\n(11) equals that of (6), which is shown to be (e.g., Liu et al., 2016)\n\u03c6\u2217(z\u2032) \u221dEz\u223cq[T z\np k(z, z\u2032)] = Ez\u223cq[\u2207z log p(z)k(z, z\u2032) + \u2207zk(z, z\u2032)].\n4\nAlgorithm 1 Amortized SVGD and KSD Minimization for Wild Variational Inference\nfor iteration t do\n1. Draw random {\u03bei}n\ni=1, calculate zi = f(\u03b7; \u03bei), and the Stein variational gradient \u2206zi in\n(13).\n2. Update parameter \u03b7 using (14) or (15) for amortized SVGD, or (17) for KSD minimization.\nend for\nBy approximating the expectation under q with the empirical mean of the current particles {zi}n\ni=1,\nSVGD admits a simple form of update that iteratively moves the particles towards the target distri-\nbution,\nzi \u2190zi + \u03f5\u2206zi,\n\u2200i = 1, . . . , n,\n\u2206zi = \u02c6Ez\u2208{zi}n\ni=1[\u2207z log p(z)k(z, zi) + \u2207zk(z, zi)],\n(13)\nwhere \u02c6Ez\u223c{zi}n\ni=1[f(z)] = P\ni f(zi)/n. The two terms in \u2206zi play two different roles: the term\nwith the gradient \u2207z log p(z) drives the particles towards the high probability regions of p(z),\nwhile the term with \u2207zk(z, zi) serves as a repulsive force to encourage diversity; to see this, con-\nsider a stationary kernel k(z, z\u2032) = k(z \u2212z\u2032), then the second term reduces to \u02c6Ez\u2207zk(z, zi) =\n\u2212\u02c6Ez\u2207zik(z, zi), which can be treated as the negative gradient for minimizing the average similarity\n\u02c6Ezk(z, zi) in terms of zi.\nIt is easy to see from (13) that \u2206zi reduces to the typical gradient \u2207z log p(zi) when there is only a\nsingle particle (n = 1) and \u2207zk(z, zi) when z = zi, in which case SVGD reduces to the standard\ngradient ascent for maximizing log p(z) (i.e., maximum a posteriori (MAP)).\n3\nTWO METHODS FOR WILD VARIATIONAL INFERENCE\nSince the direct parametric optimization of the KL divergence (1) requires calculating q\u03b7(z), there\nare two essential ways to avoid calculating q\u03b7(z): either using alternative (approximate) optimiza-\ntion approaches, or using different divergence objective functions. We discuss two possible ap-\nproaches in this work: one based on \u201camortizing SVGD\u201d (Wang & Liu, 2016) which trains the\ninference network f(\u03b7, \u03be) so that its output mimic the SVGD dynamics in order to decrease the KL\ndivergence; another based on minimizing the KSD objective (9) which does not require to evaluate\nq(z) thanks to its special form.\n3.1\nAMORTIZED SVGD\nSVGD provides an optimal updating direction to iteratively move a set of particles {zi} towards the\ntarget distribution p(z). We can leverage it to train an inference network f(\u03b7; \u03be) by iteratively ad-\njusting \u03b7 so that the output of f(\u03b7; \u03be) changes along the Stein variational gradient direction in order\nto maximumly decrease its KL divergence with the target distribution. By doing this, we \u201camortize\u201d\nSVGD into a neural network, which allows us to leverage the past experience to adaptively improve\nthe computational ef\ufb01ciency and generalize to new tasks with similar structures. Amortized SVGD\nis also presented in Wang & Liu (2016); here we present some additional discussion.\nTo be speci\ufb01c, assume {\u03bei} are drawn from q0 and zi = f(\u03b7; \u03bei) the corresponding random output\nbased on the current estimation of \u03b7. We want to adjust \u03b7 so that zi changes along the Stein vari-\national gradient direction \u2206zi in (13) so as to maximumly decrease the KL divergence with target\ndistribution. This can be done by updating \u03b7 via\n\u03b7 \u2190arg min\n\u03b7\nn\nX\ni=1\n||f(\u03b7; \u03bei) \u2212zi \u2212\u03f5\u2206zi||2\n2.\n(14)\nEssentially, this projects the non-parametric perturbation direction \u2206zi to the change of the \ufb01nite\ndimensional network parameter \u03b7. If we take the step size \u03f5 to be small, then the updated \u03b7 by (14)\nshould be very close to the old value, and a single step of gradient descent of (14) can provide a\n5\ngood approximation for (14). This gives a simpler update rule:\n\u03b7 \u2190\u03b7 + \u03f5\nX\ni\n\u2202\u03b7f(\u03b7; \u03bei)\u2206zi,\n(15)\nwhich can be intuitively interpreted as a form of chain rule that back-propagates the SVGD gradient\nto the network parameter \u03b7. In fact, when we have only one particle, (15) reduces to the stan-\ndard gradient ascent for max\u03b7 log p(f(\u03b7; \u03be)), in which f\u03b7 is trained to \u201clearn to optimize\u201d (e.g.,\nAndrychowicz et al., 2016), instead of \u201clearn to sample\u201d p(z). Importantly, as we have more than\none particles, the repulsive term \u2207zk(z, zi) in \u2206zi becomes active, and enforces an amount of di-\nversity on the network output that is consistent with the variation in p(z). The full algorithm is\nsummarized in Algorithm 1.\nAmortized SVGD can be treated as minimizing the KL divergence using a rather special algorithm:\nit leverages the non-parametric SVGD which can be treated as approximately solving the in\ufb01nite\ndimensional optimization minq KL(q || p) without explicitly assuming a parametric form on q, and\niteratively projecting the non-parametric update back to the \ufb01nite dimensional parameter space of\n\u03b7. It is an interesting direction to extend this idea to \u201camortize\u201d other MC/MCMC-based inference\nalgorithms. For example, given a MCMC with transition probability T(z\u2032|z) whose stationary dis-\ntribution is p(z), we may adjust \u03b7 to make the network output move towards the updated values z\u2032\ndrawn from the transition probability T(z\u2032|z). The advantage of using SVGD is that it provides a\ndeterministic gradient direction which we can back-propagate conveniently and is particle ef\ufb01cient\nin that it reduces to \u201clearning to optimize\u201d with a single particle. We have been using the simple\nL2 loss in (14) mainly for convenience; it is possible to use other two-sample discrepancy measures\nsuch as maximum mean discrepancy.\n3.2\nKSD VARIATIONAL INFERENCE\nAmortized SVGD attends to minimize the KL divergence objective, but can not be interpreted as\na typical \ufb01nite dimensional optimization on parameter \u03b7. Here we provide an alternative method\nbased on directly minimizing the kernelized Stein discrepancy (KSD) objective, for which, thanks\nto its special form, the typical gradient-based optimization can be performed without needing to\nestimate q(z) explicitly.\nTo be speci\ufb01c, take q\u03b7 to be the density of the random output z = f(\u03b7; \u03be) when \u03be \u223cq0, and we\nwant to \ufb01nd \u03b7 to minimize D(q\u03b7 || p). Assuming {\u03bei} is i.i.d. drawn from q0, we can approximate\nD2(q\u03b7 || p) unbiasedly with a U-statistics:\nD2(q\u03b7 || p) \u2248\n1\nn(n \u22121)\nX\ni\u0338=j\n\u03bap(f(\u03b7; \u03bei), f(\u03b7; \u03bej)),\n(16)\nfor which a standard gradient descent can be derived for optimizing \u03b7:\n\u03b7 \u2190\u03b7 \u2212\u03f5\n2\nn(n \u22121)\nX\ni\u0338=j\n\u2202\u03b7f(\u03b7; \u03bei)\u2207zi\u03bap(zi, zj),\nwhere\nzi = f(\u03b7; \u03bei).\n(17)\nThis enables a wild variational inference method based on directly minimizing \u03b7 with standard\n(stochastic) gradient descent. See Algorithm 1. Note that (17) is similar to (15) in form, but replaces\n\u2206zi with a \u02dc\u2206zi \u221d\u2212P\nj : i\u0338=j \u2207zi\u03bap(zi, zj). It is also possible to use the V -statistic in (9), but\nwe \ufb01nd that the U-statistic performs much better in practice, possibly because of its unbiasedness\nproperty.\nMinimizing KSD can be viewed as minimizing a constrastive divergence objective function. To see\nthis, recall that q[\u03f5\u03c6] denotes the density of z\u2032 = z + \u03f5\u03c6(z) when z \u223cq. Combining (11) and (6),\nwe can show that\nD2(q || p) \u22481\n\u03f5 (KL(q || p) \u2212KL(q[\u03f5\u03c6] || p)).\nThat is, KSD measures the amount of decrease of KL divergence when we update the particles\nalong the optimal SVGD perturbation direction \u03c6 given by (11). If q = p, then the decrease of KL\n6\ndivergence equals zero and D2(q || p) equals zero. In fact, as shown in Liu & Wang (2016) KSD can\nbe explicitly represented as the magnitude of a functional gradient of KL divergence:\nD(q || p) =\n\f\f\f\n\f\f\f d\nd\u03c6KL(q[\u03c6] || p)\n\f\f\n\u03c6=0\n\f\f\f\n\f\f\f\nHd,\nwhere q[\u03c6] is the density of z = z + \u03c6(z) when z \u223cq, and\nd\nd\u03c6F(\u03c6) denotes the functional gradient\nof functional F(\u03c6) w.r.t. \u03c6 de\ufb01ned in RKHS Hd, and\nd\nd\u03c6F(\u03c6) is also an element in Hd. Therefore,\nKSD variational inference can be treated as explicitly minimizing the magnitude of the gradient of\nKL divergence, in contract with amortized SVGD which attends to minimize the KL divergence\nobjective itself.\nThis idea is also similar to the contrastive divergence used for learning restricted Boltzmann ma-\nchine (RBM) (Hinton, 2002) (which, however, optimizes p with \ufb01xed q). It is possible to extend this\napproach by replacing z\u2032 = z + \u03f5\u03c6(z) with other transforms, such as these given by a transition\nprobability of a Markov chain whose stationary distribution is p. In fact, according the so called gen-\nerator method for constructing Stein operator (Barbour, 1988), any generator of a Markov process\nde\ufb01nes a Stein operator that can be used to de\ufb01ne a corresponding Stein discrepancy.\nThis idea is related to a very recent work by Ranganath et al. (2016), which is based on directly\nminimizing the variational form of Stein discrepancy in (4); Ranganath et al. (2016) assumes F\nconsists of a neural network \u03c6\u03c4(z) parametrized by \u03c4, and \ufb01nd \u03b7 by solving the following min-max\nproblem:\nmin\n\u03b7\nmax\n\u03c4\nEz\u223cq[Tp\u03c6\u03c4(z)].\nIn contrast, our method leverages the closed form solution by taking F to be an RKHS and hence\nobtains an explicit optimization problem, instead of a min-max problem that can be computationally\nmore expensive, or have dif\ufb01culty in achieving convergence.\nBecause \u03bap(x, x\u2032) (de\ufb01ned in (8)) depends on the derivative \u2207x log p(x) of the target distribution,\nthe gradient in (17) depends on the Hessian matrix \u22072\nx log p(x) and is hence less convenient to im-\nplement compared with amortized SVGD (the method by Ranganath et al. (2016) also has the same\nproblem). However, this problem can be alleviated using automatic differentiation tools, which be\nused to directly take the derivative of the objective in (16) without manually deriving its derivatives.\n4\nLANGEVIN INFERENCE NETWORK\nWith wild variational inference, we can choose more complex inference network structures to obtain\nbetter approximation accuracy. Ideally, the best network structure should leverage the special prop-\nerties of the target distribution p(z) in a convenient way. One way to achieve this by viewing existing\nMC/MCMC methods as inference networks with hand-designed (and hence potentially suboptimal)\nparameters, but good architectures that take the information of the target distribution p(z) into ac-\ncount. By applying wild variational inference on networks constructed based on existing MCMC\nmethods, we effectively provide an hyper-parameter optimization for these existing methods. This\nallows us to fully optimize the potential of existing Bayesian inference methods, signi\ufb01cantly im-\nproving the result with less computation cost, and decreasing the need for hyper-parameter tuning\nby human experts. This is particularly useful when we need to solve a large number of similar tasks,\nwhere the computation cost spent on optimizing the hyper-parameters can signi\ufb01cantly improve the\nperformance on the future tasks.\nWe take the stochastic gradient Langevin dynamics (SGLD) algorithm (Welling & Teh, 2011) as an\nexample. SGLD starts with a random initialization z0, and perform iterative update of form\nzt+1 \u2190zt + \u03b7t \u2299\u2207z log \u02c6p(zt; Mt) +\np\n2\u03b7t \u2299\u03bet,\n\u2200t = 1, \u00b7 \u00b7 \u00b7 T,\n(18)\nwhere log \u02c6p(zt; Mt) denotes an approximation of log p(zt) based on, e.g., a random mini-batch\nMt of observed data at t-th iteration, and \u03bet is a standard Gaussian random vector of the same size\nas z, and \u03b7t denotes a (vector) step-size at t-th iteration; here \u201c\u2299\u201d denotes element-wise product.\nWhen running SGLD for T iterations, we can treat zT as the output of a T-layer neural network\nparametrized by the collection of step sizes \u03b7 = {\u03b7t}T\nt=1, whose random inputs include the random\ninitialization z0, the mini-batch Mt and Gaussian noise \u03bet at each iteration t. We can see that this\n7\n(a) Initialization\n(b) Amortized SVGD\n(c) KSD Minimization\n(d) Constant Stepsize\n(e) Power Decay Stepsize\nFigure 2: Results on a 1D Gaussian mixture when training the step sizes of SGLD with T = 20\niterations. The target distribution p(x) is shown by the red dashed line. (a) The distribution of\nthe initialization z0 of SGLD (the green line), visualized by kernel density estimator. (b)-(d) The\ndistribution of the \ufb01nal output zT (green line) given by different types of step sizes, visualized by\nkernel density estimator.\nde\ufb01nes a rather complex network structure with several different types of random inputs (z0, Mt\nand \u03bet). This makes it intractable to explicitly calculate the density of zT and traditional variational\ninference methods can not be applied directly. But wild variational inference can still allow us to\nadaptively improve the optimal step-size \u03b7 in this case.\n5\nEMPIRICAL RESULTS\nWe test our algorithm with Langevin inference network on both a toy Gaussian mixture model and\na Bayesian logistic regression example. We \ufb01nd that we can adaptively learn step sizes that signi\ufb01-\ncantly outperform the existing hand-designed step size schemes, and hence save computational cost\nin the testing phase. In particular, we compare with the following step size schemes, for all of which\nwe report the best results (testing accuracy in Figure 3(a); testing likelihood in Figure 3(b)) among\na range of hyper-parameters:\n1. Constant Step Size. We select a best constant step size in {1, 2, 23, . . . , 229} \u00d7 10\u22126.\n2.\nPower Decay Step Size.\nWe consider \u03f5t = 10a \u00d7 (b + t)\u2212\u03b3 where \u03b3 = 0.55, a \u2208\n{\u22126, \u22125, . . . , 1, 2}, b \u2208{0, 1, . . . , 9}.\n3. Adagrad, Rmsprop, Adadelta, all with the master step size selected in {1, 2, 23, . . . , 229} \u00d7 10\u22126,\nwith the other parameters chosen by default values.\nGaussian Mixture\nWe start with a simple 1D Gaussian mixture example shown in Figure 2 where\nthe target distribution p(z) is shown by the red dashed curve. We use amortized SVGD and KSD\nto optimize the step size parameter of the Langevin inference network in (18) with T = 20 layers\n(i.e., SGLD with T = 20 iterations), with an initial z0 drawn from a q0 far away from the target\ndistribution (see the green curve in Figure 2(a)); this makes it critical to choose a proper step size\nto achieve close approximation within T = 20 iterations. We \ufb01nd that amortized SVGD and KSD\nallow us to achieve good performance with 20 steps of SGLD updates (Figure 2(b)-(c)), while the\nresult of the best constant step size and power decay step-size are much worse (Figure 2(d)-(e)).\nBayesian Logistic Regression\nWe consider Bayesian logistic regression for binary classi\ufb01cation\nusing the same setting as Gershman et al. (2012), which assigns the regression weights w with a\nGaussian prior p0(w|\u03b1) = N(w, \u03b1\u22121) and p0(\u03b1) = Gamma(\u03b1, 1, 0.01). The inference is applied\non the posterior of z = [w, log \u03b1]. We test this model on the binary Covertype dataset1 with 581,012\ndata points and 54 features.\nTo demonstrate that our estimated learning rate can work well on new datasets never seen by the\nalgorithm. We partition the dataset into mini-datasets of size 50, 000, and use 80% of them for\ntraining and 20% for testing. We adapt our amortized SVGD/KSD to train on the whole population\nof the training mini-datasets by randomly selecting a mini-dataset at each iteration of Algorithm 1,\nand evaluate the performance of the estimated step sizes on the remaining 20% testing mini-datasets.\n1https://www.csie.ntu.edu.tw/\u02dccjlin/libsvmtools/datasets/binary.html\n8\nSteps\n10\n50\n100\nAccuracy\n0.5\n0.55\n0.6\n0.65\n0.7\n0.75\n0.8\nSteps\n10\n50\n100\nLog Likelihood\n-0.7\n-0.65\n-0.6\n-0.55\n-0.5\nAmortized SVGD\nKSD U-statistic\nAdadelta\nConstant Rate\nPower Decay Rate\nRMSprop\nAdagrad\nSGLD(fully converged)\nSVGD(fully converged)\n(a)\n(b)\nFigure 3: The testing accuracy (a) and testing likelihood (b) when training Langevin inference net-\nwork with T \u2208{10, 50, 100} layers, respectively. The results reported here are the performance of\nthe \ufb01nal result zT outputted by the last layer of the network. We \ufb01nd that both amortized SVGD\nand KSD minimization (with U-statistics) outperform all the hand-designed learning rates. Results\naveraged on 100 random trails.\nFigure 3 reports the testing accuracy and likelihood on the 20% testing mini-datasets when we\ntrain the Langevin network with T = 10, 50, 100 layers, respectively. We \ufb01nd that our methods\noutperform all the hand-designed learning rates, and allow us to get performance closer to the fully\nconverged SGLD and SVGD with a small number T of iterations.\nFigure 4 shows the testing accuracy and testing likelihood of all the intermediate results when train-\ning Langevin network with T = 100 layers. It is interesting to observe that amortized SVGD and\nKSD learn rather different behavior: KSD tends to increase the performance quickly at the \ufb01rst few\niterations but saturate quickly, while amortized SVGD tends to increase slowly in the beginning\nand boost the performance quickly in the last few iterations. Note that both algorithms are set up\nto optimize the performance of the last layers, while need to decide how to make progress on the\nintermediate layers to achieve the best \ufb01nal performance.\nIntermediate Steps\n0\n50\n100\nAccuracy\n0.5\n0.55\n0.6\n0.65\n0.7\n0.75\nIntermediate Steps\n0\n50\n100\nLog Likelihood\n-0.7\n-0.65\n-0.6\n-0.55\n-0.5\nAmortized SVGD\nKSD U-statistic\nAdadelta\nConstant Rate\nPower Decay Rate\nRMSprop\nAdagrad\n(a)\n(b)\nFigure 4: The testing accuracy (a) and testing likelihood (b) of the outputs of the intermediate layers\nwhen training the Langevin network with T = 100 layers. Note that both amortized SVGD and\nKSD minimization target to optimize the performance of the last layer, but need to optimize the\nprogress of the intermediate steps in order to achieve the best \ufb01nal results.\n6\nCONCLUSION\nWe consider two methods for wild variational inference that allows us to train general inference net-\nworks with intractable density functions, and apply it to adaptively estimate step sizes of stochastic\ngradient Langevin dynamics. More studies are needed to develop better methods, more applications\nand theoretical understandings for wild variational inference, and we hope that the two methods we\ndiscussed in the paper can motivate more ideas and studies in the \ufb01eld.\n9\nREFERENCES\nAgakov, Felix V and Barber, David. An auxiliary variational method. In International Conference on Neural\nInformation Processing, pp. 561\u2013566. Springer, 2004.\nAndrieu, Christophe and Thoms, Johannes. A tutorial on adaptive mcmc. Statistics and Computing, 18(4):\n343\u2013373, 2008.\nAndrychowicz, Marcin, Denil, Misha, Gomez, Sergio, Hoffman, Matthew W, Pfau, David, Schaul, Tom,\nand de Freitas, Nando.\nLearning to learn by gradient descent by gradient descent.\narXiv preprint\narXiv:1606.04474, 2016.\nBarbour, Andrew D. Stein\u2019s method and poisson process convergence. Journal of Applied Probability, pp.\n175\u2013184, 1988.\nChwialkowski, Kacper, Strathmann, Heiko, and Gretton, Arthur. A kernel test of goodness of \ufb01t. In Proceedings\nof the International Conference on Machine Learning (ICML), 2016.\nGershman, Samuel, Hoffman, Matt, and Blei, David. Nonparametric variational inference. In Proceedings of\nthe International Conference on Machine Learning (ICML), 2012.\nGershman, Samuel J and Goodman, Noah D. Amortized inference in probabilistic reasoning. In Proceedings\nof the 36th Annual Conference of the Cognitive Science Society, 2014.\nGorham, Jack and Mackey, Lester. Measuring sample quality with Stein\u2019s method. In Advances in Neural\nInformation Processing Systems (NIPS), pp. 226\u2013234, 2015.\nHe, Kaiming, Zhang, Xiangyu, Ren, Shaoqing, and Sun, Jian. Deep residual learning for image recognition. In\nCVPR, 2016.\nHinton, Geoffrey E. Training products of experts by minimizing contrastive divergence. Neural computation,\n14(8):1771\u20131800, 2002.\nHoffman, Matthew D, Blei, David M, Wang, Chong, and Paisley, John. Stochastic variational inference. JMLR,\n2013.\nKingma, Diederik P and Welling, Max. Auto-encoding variational Bayes. In Proceedings of the International\nConference on Learning Representations (ICLR), 2013.\nLiu, Qiang and Wang, Dilin. Stein variational gradient descent: A general purpose bayesian inference algo-\nrithm. arXiv preprint arXiv:1608.04471, 2016.\nLiu, Qiang, Lee, Jason D, and Jordan, Michael I. A kernelized Stein discrepancy for goodness-of-\ufb01t tests. In\nProceedings of the International Conference on Machine Learning (ICML), 2016.\nMaclaurin, Dougal, Duvenaud, David, and Adams, Ryan P. Early stopping is nonparametric variational infer-\nence. arXiv preprint arXiv:1504.01344, 2015.\nMandt, Stephan, Hoffman, Matthew D, and Blei, David M. A variational analysis of stochastic gradient algo-\nrithms. arXiv preprint arXiv:1602.02666, 2016.\nOates, Chris J, Girolami, Mark, and Chopin, Nicolas. Control functionals for Monte Carlo integration. Journal\nof the Royal Statistical Society, Series B, 2017.\nPaige, Brooks and Wood, Frank. Inference networks for sequential monte carlo in graphical models. arXiv\npreprint arXiv:1602.06701, 2016.\nRanganath, R., Altosaar, J., Tran, D., and Blei, D.M. Operator variational inference. 2016.\nRanganath, Rajesh, Gerrish, Sean, and Blei, David M. Black box variational inference. In Proceedings of the\nInternational Conference on Arti\ufb01cial Intelligence and Statistics (AISTATS), 2014.\nRanganath, Rajesh, Tran, Dustin, and Blei, David M.\nHierarchical variational models.\narXiv preprint\narXiv:1511.02386, 2015.\nRezende, Danilo Jimenez and Mohamed, Shakir. Variational inference with normalizing \ufb02ows. In Proceedings\nof the International Conference on Machine Learning (ICML), 2015a.\nRezende, Danilo Jimenez and Mohamed, Shakir. Variational inference with normalizing \ufb02ows. arXiv preprint\narXiv:1505.05770, 2015b.\n10\nRoberts, Gareth O and Rosenthal, Jeffrey S. Examples of adaptive mcmc. Journal of Computational and\nGraphical Statistics, 18(2):349\u2013367, 2009.\nSalimans, Tim et al. Markov chain monte carlo and variational inference: Bridging the gap. In International\nConference on Machine Learning, 2015.\nTran, Dustin, Ranganath, Rajesh, and Blei, David M.\nVariational gaussian process.\narXiv preprint\narXiv:1511.06499, 2015.\nWang, Dilin and Liu, Qiang. Learning to draw samples: With application to amortized mle for generative\nadversarial learning. arXiv preprint arXiv:1611.01722, 2016.\nWelling, Max and Teh, Yee W. Bayesian learning via stochastic gradient Langevin dynamics. In Proceedings\nof the International Conference on Machine Learning (ICML), 2011.\n11\n",
        "sentence": "",
        "context": "Gershman, Samuel J and Goodman, Noah D. Amortized inference in probabilistic reasoning. In Proceedings\nof the 36th Annual Conference of the Cognitive Science Society, 2014.\nGorham, Jack and Mackey, Lester. Measuring sample quality with Stein\u2019s method. In Advances in Neural\nInformation Processing Systems (NIPS), pp. 226\u2013234, 2015.\nto Stein discrepancy (Liu et al., 2016; Oates et al., 2017; Chwialkowski et al., 2016; Gorham &\nMackey, 2015), for which we provide a more detailed discussion in Section 3.2."
    },
    {
        "title": "Variational Gradient Descent: A General Purpose",
        "author": [
            "Q. Liu",
            "Wang",
            "D. Stein"
        ],
        "venue": "Bayesian Inference Algorithm",
        "citeRegEx": "Liu et al\\.,? \\Q2016\\E",
        "shortCiteRegEx": "Liu et al\\.",
        "year": 2016,
        "abstract": "",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "A kernelized Stein discrepancy for goodness-of-fit tests",
        "author": [
            "Q. Liu",
            "J. Lee",
            "M. Jordan"
        ],
        "venue": "In Proc. of 33rd ICML, volume 48 of ICML,",
        "citeRegEx": "Liu et al\\.,? \\Q2016\\E",
        "shortCiteRegEx": "Liu et al\\.",
        "year": 2016,
        "abstract": "",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "Multivariate Stein factors for a class of strongly log-concave distributions",
        "author": [
            "L. Mackey",
            "J. Gorham"
        ],
        "venue": "Electron. Commun. Probab.,",
        "citeRegEx": "Mackey and Gorham,? \\Q2016\\E",
        "shortCiteRegEx": "Mackey and Gorham",
        "year": 2016,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "Integral probability metrics and their generating classes of functions",
        "author": [
            "A. M\u00fcller"
        ],
        "venue": "Ann. Appl. Probab.,",
        "citeRegEx": "M\u00fcller,? \\Q1997\\E",
        "shortCiteRegEx": "M\u00fcller",
        "year": 1997,
        "abstract": "We consider probability metrics of the following type: for a class  of functions and probability measures P, Q we define  A unified study of such integral probability metrics is given. We characterize the maximal class of functions that generates such a metric. Further, we show how some interesting properties of these probability metrics arise directly from conditions on the generating class of functions. The results are illustrated by several examples, including the Kolmogorov metric, the Dudley metric and the stop-loss metric.",
        "full_text": "",
        "sentence": " In this case, we call (1) an integral probability metric (IPM) (M\u00fcller, 1997). In this case, we call (1) an integral probability metric (IPM) (M\u00fcller, 1997). For example, when H = BL\u2016\u00b7\u20162 , {h : R \u2192 R |M0(h) + M1(h) \u2264 1}, the IPM dBL\u2016\u00b7\u20162 is called the bounded Lipschitz or Dudley metric and exactly metrizes convergence in distribution. Alternatively, when H = W\u2016\u00b7\u20162 , {h : R d \u2192 R |M1(h) \u2264 1} is the set of 1-Lipschitz functions, the IPM dW\u2016\u00b7\u2016 in (1) is known as the Wasserstein metric. An apparent practical problem with using the IPM dH as a sample quality measure is that EP [h(Z)] may not be computable for h \u2208 H. However, if H were chosen such that EP [h(Z)] = 0 for all h \u2208 H, then no explicit integration under P would be necessary. To generate such a class of test functions and to show that the resulting IPM still satisfies our desiderata, we follow the lead of Gorham & Mackey (2015) and consider Charles Stein\u2019s method for characterizing distributional convergence.",
        "context": null
    },
    {
        "title": "Control functionals for QuasiMonte Carlo integration",
        "author": [
            "C. Oates",
            "M. Girolami"
        ],
        "venue": null,
        "citeRegEx": "Oates and Girolami,? \\Q2015\\E",
        "shortCiteRegEx": "Oates and Girolami",
        "year": 2015,
        "abstract": "",
        "full_text": "",
        "sentence": " Our Stein set choice was motivated by the work of Oates et al. (2016b) who used the sum of Stein kernels k0 = \u2211d j=1 k j 0 to develop nonparametric control variates.",
        "context": null
    },
    {
        "title": "Convergence rates for a class of estimators based on steins method",
        "author": [
            "C. Oates",
            "J. Cockayne",
            "F. Briol",
            "M. Girolami"
        ],
        "venue": "arXiv preprint arXiv:1603.03220,",
        "citeRegEx": "Oates et al\\.,? \\Q2016\\E",
        "shortCiteRegEx": "Oates et al\\.",
        "year": 2016,
        "abstract": "",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "Control functionals for Monte Carlo integration",
        "author": [
            "C.J. Oates",
            "M. Girolami",
            "N. Chopin"
        ],
        "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology), pp. n/a\u2013n/a,",
        "citeRegEx": "Oates et al\\.,? \\Q2016\\E",
        "shortCiteRegEx": "Oates et al\\.",
        "year": 2016,
        "abstract": "A non-parametric extension of control variates is presented. These leverage\ngradient information on the sampling density to achieve substantial variance\nreduction. It is not required that the sampling density be normalised. The\nnovel contribution of this work is based on two important insights; (i) a\ntrade-off between random sampling and deterministic approximation and (ii) a\nnew gradient-based function space derived from Stein's identity. Unlike\nclassical control variates, our estimators achieve super-root-$n$ convergence,\noften requiring orders of magnitude fewer simulations to achieve a fixed level\nof precision. Theoretical and empirical results are presented, the latter\nfocusing on integration problems arising in hierarchical models and models\nbased on non-linear ordinary differential equations.",
        "full_text": "Control Functionals for Monte Carlo Integration\nChris J. Oates1,2,\u2217, Mark Girolami3,4 and Nicolas Chopin5\n1University of Technology Sydney, Australia\n2Australian Research Council Centre for Excellence in Mathematical\nand Statistical Frontiers\n3University of Warwick, Coventry, UK\n4Alan Turing Institute\n5CREST-LS and ENSAE, Paris, France\nApril 5, 2016\nAbstract\nA non-parametric extension of control variates is presented. These leverage gradient\ninformation on the sampling density to achieve substantial variance reduction. It is\nnot required that the sampling density be normalised. The novel contribution of this\nwork is based on two important insights; (i) a trade-o\ufb00between random sampling and\ndeterministic approximation and (ii) a new gradient-based function space derived from\nStein\u2019s identity. Unlike classical control variates, our estimators achieve super-root-n\nconvergence, often requiring orders of magnitude fewer simulations to achieve a \ufb01xed\nlevel of precision. Theoretical and empirical results are presented, the latter focusing\non integration problems arising in hierarchical models and models based on non-linear\nordinary di\ufb00erential equations.\nKeywords: control variates, non-parametric, reproducing kernel, Stein\u2019s identity, variance\nreduction\n1\nIntroduction\nStatistical methods are increasingly being employed to analyse complex models of physical\nphenomena (e.g. in climate forecasting or simulations of molecular dynamics; Slingo et al.,\n2009; Angelikopoulos et al., 2012). Analytic intractability of complex models has inspired the\ndevelopment of sophisticated Monte Carlo methodologies to facilitate computation (Robert\n\u2217Address for correspondence: School of Mathematical and Physical Sciences, University of Technology\nSydney, NSW 2007, Australia. E-mail: christopher.oates@uts.edu.au\n1\narXiv:1410.2392v5  [stat.ME]  2 Apr 2016\nand Casella, 2004). In their most basic form, Monte Carlo estimators converge as the re-\nciprocal of root-n where n is the number of random samples. For complex models it may\nonly be feasible to obtain a limited number of samples (e.g. a recent Met O\ufb03ce model for\nfuture climate simulations required the order of 106 core-hours per simulation; Mizielinski\net al., 2014). In these situations, root-n convergence is too slow and leads in practice to\nhigh-variance estimation. Our contribution is motivated by resolving this issue and provides\nnovel methodology that is both formal and general.\nThe focus of this paper is the estimation of an expectation \u00b5(f) =\nR\nf(x)\u03c0(x)dx, where\nf is a test function of interest and \u03c0 is a probability density associated with a random\nvariable X. Provided that f(X) has variance \u03c32(f) < \u221e, the arithmetic mean estimator\n1\nn\nn\nX\ni=1\nf(xi),\nbased on n independent and identically distributed (IID) samples {xi}n\ni=1 of the random\nvariable, satis\ufb01es the central limit theorem and converges to \u00b5(f) at the rate OP(n\u22121/2), or\nsimply at \u201croot-n\u201d. When working with complex models, root-n convergence can be prob-\nlematic, as highlighted in e.g. Ba and Joseph (2012). A model is considered complex when\neither (i) X is expensive to simulate, or (ii) f is expensive to evaluate, in each case relative to\nthe required estimator precision. Both situations are prevalent in scienti\ufb01c and engineering\napplications (e.g. Kohlho\ufb00et al., 2014; Higdon et al., 2015). This paper introduces a class\nof estimators that converge more quickly than root-n. The signi\ufb01cance of our contribution\nis made clear in the comparative overview below.\nGeneric approaches to reduction of variance are well-known in both statistics and numer-\nical analysis. These include (i) importance sampling and its extensions (Cornuet et al., 2012;\nLi et al., 2013), (ii) strati\ufb01ed sampling and related techniques (Rubinstein and Kroese, 2011),\n(iii) antithetic variables (Green and Han, 1992) and more generally (randomised) quasi-\nMonte Carlo (QMC/RQMC; Dick and Pillichshammer, 2010), (iv) Rao-Blackwellisation\n(Robert and Casella, 2004; Douc and Robert, 2011; Ghosh and Clyde, 2011; Olsson and\nRyden, 2011), (v) Riemann sums (Philippe, 1997), (vi) control variates (Glasserman, 2004;\nMira et al., 2013; Li et al., 2016), (vii) multi-level Monte Carlo and related techniques (e.g.\nHeinrich, 1995; Giles, 2013; Giles and Szpruch, 2014), (viii) Bayesian Monte Carlo (BMC;\nO\u2019Hagan, 1991; Rasmussen and Ghahramani, 2003; Briol et al., 2015), and (ix) a plethora\nof sophisticated Markov chain Monte Carlo sampling schemes (MCMC;  Latuszy\u00b4nski et al.,\n2015). Classical introductions to many of the above techniques include Robert and Casella\n(2004, Chap. 4) and Rubinstein and Kroese (2011, Chap. 5).\nMotivated by contemporary statistical applications, we state four desiderata for a vari-\nance reduction technique: (I) Unbiased estimation: Monte Carlo (MC) methods based on IID\nsamples produce unbiased estimators, whilst techniques such as MCMC generally produce\nbiased estimators. (II) Compatibility with an un-normalised density \u03c0: An \u201cun-normalised\u201d\ndensity is known only up to proportionality so that, for example, MCMC techniques are\nrequired for sampling. (III) Super-root-n convergence (for su\ufb03ciently regular f): The con-\nvergence rates of (R)QMC are well studied and can be super-root-n. Riemann sums can\n2\nEstimation Method\nUnbiased\nUn-normalised \u03c0\nSuper-root-n\nPost-hoc\nMC(/MCMC) + Arithmetic Mean\n\u2713(/\u00d7)\n\u00d7(/\u2713)\n\u00d7\n\u00d7\nMC + Importance Sampling\n\u2713(/\u00d7)\n\u00d7(/\u2713)\n\u00d7\n\u00d7\nMC + Antithetic Variables\n\u2713\n\u00d7\n\u00d7\n\u00d7\nMC(/MCMC) + Strati\ufb01ed Sampling\n\u2713(/\u00d7)\n\u00d7(/\u2713)\n\u00d7\n\u00d7\nQuasi-MC (QMC)\n\u00d7\n\u00d7\n\u2713\n\u00d7\nRandomised QMC (RQMC)\n\u2713\n\u00d7\n\u2713\n\u00d7\nMC(/MCMC) + Rao-Blackwellisation\n\u2713(/\u00d7)\n\u00d7(/\u2713)\n\u00d7\n\u2713\nMC(/MCMC) + Control Variates\n\u2713(/\u00d7)\n\u00d7(/\u2713)\n\u00d7\n\u2713\nMC(/MCMC) + Riemann Sums\n\u00d7\n\u00d7(/\u2713)\n\u2713\n\u2713\nBayesian MC (BMC)\n\u00d7\n\u00d7\n\u2713\n\u2713\nMC(/MCMC) + Control Functionals\n\u2713(/\u00d7)\n\u00d7(/\u2713)\n\u2713\n\u2713\nTable 1: A comparison of estimation methods for integrals. [\u201cUnbiased\u201d = the estimator is\nunbiased for \u00b5(f). \u201cUn-normalised \u03c0\u201d = the estimator can handle sampling densities that\nare only available up to proportionality. \u201cSuper-root-n\u201d = the estimator converges faster\nthan root-n. \u201cPost-hoc\u201d = the estimator places no restriction on how the samples xi are\ngenerated, i.e. requires no modi\ufb01cation to computer code for sampling. Estimator properties\nmay change in order to handle un-normalised densities \u03c0; these are shown in parentheses.]\nalso achieve super-root-n rates and Briol et al. (2016) showed the same holds for BMC. (IV)\nPost-hoc schemes: Rao-Blackwellisation, Riemann sums, BMC and control variates can all\nbe conceived as post-hoc schemes; i.e.\nschemes that can be applied retrospectively after\nsamples have been obtained. In contrast, the remaining methods require modi\ufb01cation to\ncomputer code for the sampling process itself. The former are appealing from both a the-\noretical and a practical perspective since they separate the challenge of sampling from the\nchallenge of variance reduction.\nTable 1 summarises existing techniques in relation to these desiderata; note that no\ntechnique ful\ufb01ls all four criteria. In contrast, the method proposed here, called \u201ccontrol\nfunctionals\u201d, is able to satisfy all four desiderata. Control functionals appear to be similar,\nin this sense, to Riemann sums i.e. they are a super-root-n, post-hoc approach that applies\nto un-normalised sampling densities. However, Riemann sums are rarely used in practice\ndue to (i) the fact that estimators are biased at \ufb01nite sample sizes, and (ii) there is a\nprohibitive increase in methodological complexity for multi-dimensional state spaces. Control\nfunctionals do not posses either of these drawbacks.\nThe control functional method that we develop below can be intuitively considered as a\nnon-parametric development of control variates. In control variate schemes one seeks a basis\n{si}m\ni=1, m \u2208N, that have expectation \u00b5(si) = 0. Then a surrogate function \u02dcf = f \u2212a1s1 \u2212\n\u00b7 \u00b7 \u00b7 \u2212amsm is constructed such that \u00b5( \u02dcf) = \u00b5(f) and, for suitably chosen a1, . . . , am \u2208R, a\nvariance reduction \u03c32( \u02dcf) < \u03c32(f) is obtained (see e.g. Rubinstein and Marcus, 1985). The\nstatistics si are known as control variates and the variance \u03c32( \u02dcf) can be reduced to zero\nif and only if there is perfect canonical correlation between f and the basis {si}m\ni=1. For\nestimation based on Markov chains, control variates for the discrete state space case were\n3\nprovided by Andrad\u00b4ottir et al. (1993). For continuous state spaces, statistics relating to\nthe chain can be used as control variates (Hammer and Tjelmeland, 2008; Dellaportas and\nKontoyiannis, 2012; Li et al., 2016). Alternatively control variates can be constructed based\non gradient information (Assaraf and Ca\ufb00arel, 1999; Mira et al., 2013).\nThe control variates described above are solving a misspeci\ufb01ed regression problem, since\nin general f will not be a linear combination of the si basis functions. As such they achieve\nat most a constant factor reduction in estimator variance. Intuitively, one would like to\nincrease the number m of basis functions to increase in line with the number n. Mijatovi\u00b4c\nand Vogrinc (2015) explored this approach within the Metropolis-Hastings method. However,\ntheir solution requires the user to partition of the state space, which limits its wider appeal.\nThis paper introduces a powerful new perspective on variance reduction that fully resolves\nthese issues, satisfying all the desiderata described above. To realise our method we developed\na gradient-based function space that leads to closed-form estimators whose convergence can\nbe guaranteed. The functional analysis perspective works \u201cout of the box\u201d, without requiring\nthe user to partition the state space. Extensive empirical support is provided in favour of\nthe proposed method, including applications to hierarchical models and models based on\nnon-linear di\ufb00erential equations. In each case state-of-the-art estimation is achieved.\nAll results can be reproduced using MATLAB R2015a code that is available to download\nfrom http://warwick.ac.uk/control_functionals.\n2\nMethodology\n2.1\nSet-up and notation\nConsider a random vector X taking values in an open set \u2126\u2286Rd. Assume X admits\na positive density on \u2126with respect to d-dimensional Lebesgue measure, written \u03c0(x) >\n0. For bounded \u2126with boundary \u2202\u2126, we assume \u2202\u2126is piecewise smooth (i.e. in\ufb01nitely\ndi\ufb00erentiable). Write L2(\u03c0) for the space of measurable functions g : \u2126\u2192R for which\nR\n\u2126g(x)2\u03c0(x)dx is \ufb01nite. Write Ck(\u2126, Rj) for the space of (measurable) functions from \u2126to\nRj with continuous partial derivatives up to order k. Consider a test function f : \u2126\u2192R\nof interest, assume f \u2208L2(\u03c0) and write \u00b5(f) :=\nR\n\u2126f(x)\u03c0(x)dx, \u03c32(f) :=\nR\n\u2126(f(x) \u2212\n\u00b5(f))2\u03c0(x)dx.\nDenote by D = {xi}n\ni=1 a collection of states xi \u2208\u2126. At each state xi the corresponding\nfunction values f(xi) and gradients \u2207x log \u03c0(xi) are assumed to have been pre-computed\nand cached. The method that we develop does not then require any further recourse to\nthe statistical model \u03c0, nor any further evaluations of the function f, and is in this sense a\nwidely-applicable post-hoc scheme.\n4\n2.2\nFrom control variates to control functionals\n2.2.1\nDeterministic approximation\nOur starting point is establish a trade-o\ufb00between random sampling and deterministic ap-\nproximation, as suggested on several separate occasions by authors including Bakhvalov\n(1959); Heinrich (1995); Speight (2009); Giles (2013).\nConsider a dichotomy of available states D = {xi}n\ni=1 into two disjoint subsets D0 =\n{xi}m\ni=1 and D1 = {xi}n\ni=m+1, where 1 \u2264m < n. Although m, n are \ufb01xed, we will be\ninterested in the asymptotic regime where m = O(n\u03b3) for some \u03b3 \u2208[0, 1]. Consider surrogate\nfunctions of the form\nfD0(x) := f(x) \u2212sf,D0(x) + \u00b5(sf,D0),\nwhere sf,D0 \u2208L2(\u03c0) is an approximation to f, based on D0, whose expectation \u00b5(sf,D0) is\nanalytically tractable. By construction fD0 \u2208L2(\u03c0), \u00b5(fD0) = \u00b5(f) and \u03c32(fD0) = \u03c32(f \u2212\nsf,D0). We study estimators of the form\n\u02c6\u00b5(D0, D1; f) :=\n\u001a\n1\nn\u2212m\nPn\ni=m+1 fD0(xi)\nfor m < n\n\u00b5(sf,D0)\nfor m = n.\nFor theoretical purposes the second subset D1 is assumed to be an IID sample from \u03c0, statisti-\ncally independent from D0. Then, for m < n, we have unbiasedness, i.e. ED1[\u02c6\u00b5(D0, D1; f)] =\n\u00b5(f), where the expectation here is with respect to the sampling distribution \u03c0 of the n \u2212m\nrandom variables that constitute D1, and is conditional on D0.\nThe corresponding esti-\nmator variance, conditional on D0, is VD1[\u02c6\u00b5(D0, D1; f)] = (n \u2212m)\u22121\u03c32(f \u2212sf,D0). This\nformulation encompasses control variates as the special case where sf,D0 is constrained to a\n\ufb01nite-dimensional space.\nThe insight required to go beyond control variates and achieve super-root-n convergence\nis that we can use an in\ufb01nite-dimensional space to construct an increasingly accurate ap-\nproximations sf,D0 as m \u2192\u221e. We allow for the possibility that the \ufb01rst subset D0 are also\nrandom and write ED0 to denote an expectation with respect to the (marginal) distribution\nof these m random variables.\nProposition 1. Assume m = O(n\u03b3) for some \u03b3 \u2208[0, 1] and that the expected functional\napproximation error (EFAE) vanishes as\nED0[\u03c32(f \u2212sf,D0)] = O(m\u2212\u03b4)\nfor some \u03b4 \u22650. Then ED0ED1[(\u02c6\u00b5(D0, D1; f) \u2212\u00b5(f))2] = O(n\u22121\u2212\u03b3\u03b4).\nAll proofs are reserved for Appendix A.\nRemark 1. Taking \u03b3 = 1 optimises the rate in Prop. 1 and we therefore assume in the\nsequel that m/n \u2192r \u2208(0, 1).\n5\n2.2.2\nControl variates based on Stein\u2019s identity\nTo construct approximations sf,D0 whose integrals \u00b5(sf,D0) are analytically tractable, we\nmake the assumption\n(A1) The density \u03c0 belongs to C1(\u2126, R).\nDenote the gradient function by u(x) := \u2207x log \u03c0(x) where \u2207x := [\u2202/\u2202x1, . . . , \u2202/\u2202xd]T,\nwell-de\ufb01ned by (A1). We study approximations of the form\nsf,D0(x)\n:=\nc + \u03c8(x)\n\u03c8(x)\n:=\n\u2207x \u00b7 \u03c6(x) + \u03c6(x) \u00b7 u(x)\n(1)\nwhere c \u2208R is a constant and \u03c6 \u2208C1(\u2126, Rd). Eqn. 1 appears in Stein\u2019s classical test for\napproximate normality (Stein, 1970) and related to (but simpler than) control variates pro-\nposed by Assaraf and Ca\ufb00arel (1999); Mira et al. (2013). We make the following assumption\n(c.f. e.g. Eqn. 9 of Mira et al., 2013):\n(A2) Let n(x) be the unit normal to the boundary \u2202\u2126of the state space \u2126. Then\nI\n\u2202\u2126\n\u03c0(x)\u03c6(x) \u00b7 n(x)S(dx) = 0.\n(The notation\nH\n\u2202\u2126denotes a surface integral over \u2202\u2126and S(dx) denotes the surface element\nat x \u2208\u2202\u2126.) Stein\u2019s identity implies that this class of approximations has integrals that are\nanalytically tractable:\nProposition 2. Assume (A1,2). Then \u00b5(\u03c8) = 0 and so \u00b5(sf,D0) = c.\nWhen \u2126is unbounded, all surface integrals are interpreted as tail conditions. i.e. (A2)\nshould be replaced with\nH\n\u0393r\u2229\u2126\u03c0(x)\u03c6(x) \u00b7 n(x)S(dx) \u21920, where \u0393r \u2282Rd is the sphere of\nradius r centred at the origin and n(x) is the unit normal to the surface of \u0393r.\nThe statistic \u03c8 is recognised as a control variate. These control variates were explored\nin the case where \u03c6 is a (gradient of a low-degree) polynomial by Assaraf and Ca\ufb00arel\n(1999), Assaraf and Ca\ufb00arel (2003) and Mira et al. (2013). This paper takes the innovative\nstep of setting \u03c6 within a function space to enable fully non-parametric approximation.\nThe functional approximation perspective di\ufb00ers fundamentally from the control variate\napproach, in which the estimation problem is formally mis-speci\ufb01ed (i.e. \u03c6 is restricted to a\nlow dimensional parametric family that does not contain the \u201ctrue\u201d function). We emphasise\nthis key conceptual distinction by referring to \u03c8 as a control functional (CF; re\ufb02ecting the\nuse of terminology from functional analysis).\n2.3\nTheory\nThis section establishes \u03c8 as belonging to a Hilbert space H0 \u2282L2(\u03c0). This allows us to\nformulate and solve a functional approximation problem that targets the EFAE.\n6\n2.3.1\nA Hilbert space of control functionals\nSpeci\ufb01cation of \u03c8 is equivalent to speci\ufb01cation of \u03c6. We decide to restrict each component\nfunction \u03c6i : \u2126\u2192R to a Hilbert space H \u2282L2(\u03c0) \u2229C1(\u2126, R) with inner product \u27e8\u00b7, \u00b7\u27e9H :\nH \u00d7 H \u2192R. Moreover we insist that H is a reproducing kernel Hilbert space. This implies\nthat there exists a symmetric positive de\ufb01nite function k : \u2126\u00d7 \u2126\u2192R such that (i) for all\nx \u2208\u2126we have k(\u00b7, x) \u2208H and (ii) for all x \u2208\u2126and h \u2208H we have h(x) = \u27e8h, k(\u00b7, x)\u27e9H\n(Berlinet and Thomas-Agnan, 2004, Def. 1, p7, and Def. 2, p10). The vector-valued function\n\u03c6 : \u2126\u2192Rd is de\ufb01ned in the Cartesian product space Hd := H \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 H, itself a Hilbert\nspace with the inner product \u27e8\u03c6, \u03c6\u2032\u27e9Hd = Pd\ni=1\u27e8\u03c6i, \u03c6\u2032\ni\u27e9H.\nWe make an assumption on k that will be enforced by construction:\n(A3) The kernel k belongs to C2(\u2126\u00d7 \u2126, R).\nNow we can analyse the class of CFs induced by k:\nTheorem 1. Assume \u03c6 \u2208Hd and (A1,3). Then \u03c8 belongs to H0, the reproducing kernel\nHilbert space with kernel k0(x, x\u2032) := \u2207x\u00b7\u2207x\u2032k(x, x\u2032)+u(x)\u00b7\u2207x\u2032k(x, x\u2032)+u(x\u2032)\u00b7\u2207xk(x, x\u2032)+\nu(x) \u00b7 u(x\u2032)k(x, x\u2032).\nTo gain some intuition for H0 we strengthen (A2) as follows:\n(A2\u2019) For \u03c0-almost all x \u2208\u2126the kernel k satis\ufb01es\nI\n\u2202\u2126\nk(x, x\u2032)\u03c0(x\u2032)n(x\u2032)S(dx\u2032) = 0\nand\nI\n\u2202\u2126\n\u2207xk(x, x\u2032)\u03c0(x\u2032) \u00b7 n(x\u2032)S(dx\u2032) = 0.\nWhile (A2\u2019) must be veri\ufb01ed on a case-by-case basis, it can in principle always be enforced\nwith a suitable choice of k.\nLemma 1. Under (A1,2\u2019,3), the gradient-based kernel k0 satis\ufb01es\nZ\n\u2126\nk0(x, x\u2032)\u03c0(x\u2032)dx\u2032 = 0\nfor \u03c0-almost all x \u2208\u2126.\nLemma 1 generalises Eqn. 1 of Mira et al. (2013) and implies that H0 consists of only valid\nCFs, i.e. \u03c8 \u2208H0 =\u21d2\u00b5(\u03c8) = 0. These ideas are illustrated in Fig. 1.\n(A4) The gradient-based kernel k0 satis\ufb01es\nZ\n\u2126\nk0(x, x)\u03c0(x)dx < \u221e.\nLemma 2. Under (A1,2\u2019,3,4) we have H0 \u2282L2(\u03c0).\nRemark 2. In general (A4) must be veri\ufb01ed on a case-by-case basis. (A4) is easily veri\ufb01ed\nfor all examples in this paper.\n7\n\u22123\n\u22122\n\u22121\n0\n1\n2\n3\n\u22121.5\n\u22121\n\u22120.5\n0\n0.5\n1\nx\n\u03c6(x)\nElements of H\n\u22123\n\u22122\n\u22121\n0\n1\n2\n3\n\u22124\n\u22122\n0\n2\n4\nx\n\u03c8(x)\nAssociated control functionals in H0 \n\u22123\n\u22122\n\u22121\n0\n1\n2\n3\n0\n0.5\n1\nx\n\u03c0(x)\nFigure 1: Constructing control functionals (in dimension d = 1): Representative elements\n\u03c6 from the reproducing kernel Hilbert space H (left panel) are plotted, along with their\nassociated control functionals \u03c8 = \u2207\u03c6 + \u03c6\u2207log \u03c0 in H0 (right, top panel).\nEach \u03c6 is\nunconstrained in expectation, but the corresponding control functional \u03c8 is automatically\nconstrained to have expectation zero with respect to the (possibly un-normalised) probability\ndensity \u03c0 (right, bottom panel).\n2.3.2\nConsistent approximation and asymptotics\nNow we establish theoretical results for consistent approximation of f by sf,D0. Write C for\nthe reproducing kernel Hilbert space of constant functions with kernel kC(x, x\u2032) = 1 for all\nx, x\u2032 \u2208\u2126. Denote the norms associated to C and H0 respectively by \u2225\u00b7 \u2225C and \u2225\u00b7 \u2225H0. Write\nH+ = C + H0 for the set {c + \u03c8 : c \u2208C, \u03c8 \u2208H0}. Equip H+ with the structure of a vector\nspace, with addition operator (c + \u03c8) + (c\u2032 + \u03c8\u2032) = (c + c\u2032) + (\u03c8 + \u03c8\u2032) and multiplication\noperator \u03bb(c + \u03c8) = (\u03bbc) + (\u03bb\u03c8), each well-de\ufb01ned due to uniqueness of the representation\nf = c + \u03c8, f \u2032 = c\u2032 + \u03c8\u2032 with c, c\u2032 \u2208C and \u03c8, \u03c8\u2032 \u2208H0. In addition, equip H+ with the norm\n\u2225f\u22252\nH+ := \u2225c\u22252\nC +\u2225\u03c8\u22252\nH0, again, well-de\ufb01ned by uniqueness of representation. It can be shown\nthat H+ is a reproducing kernel Hilbert space with kernel k+(x, x\u2032) := kC(x, x\u2032) + k0(x, x\u2032)\n(Berlinet and Thomas-Agnan, 2004, Thm. 5, p24).\nFor the analysis we assume a basic well-posedness condition:\n(A5) f \u2208H+. i.e. f = c + \u03c8 for some c \u2208C and \u03c8 \u2208H0.\nRemark 3. (A5) is equivalent to the existence of a solution \u03c6 \u2208Hd to the partial di\ufb00erential\nequation\n\u2207x \u00b7 [\u03c0(x)\u03c6(x)] = [f(x) \u2212\u00b5(f)]\u03c0(x),\n8\ncalled the \u201cfundamental equation\u201d in Assaraf and Ca\ufb00arel (1999, Eqn. 5; see also Eqn. 4 in\nMira et al. (2013)). With no initial or boundary conditions to satisfy, it is easy to show that\nthere exist in\ufb01nitely many solutions to the fundamental equation, so (A5) is automatically\nsatis\ufb01ed by choosing k such that H is big enough to contain at least one solution.\nTo realise the CF method we consider the regularised least-squares (RLS) functional\napproximation given by\nsf,D0 := arg min\ng\u2208H+\n(\n1\nm\nm\nX\nj=1\n(f(xj) \u2212g(xj))2 + \u03bb\u2225g\u22252\nH+\n)\nwhere \u03bb > 0.\nFor the special case where m = n, the CF estimator can be interpreted\nas kernel quadrature (Sommariva and Vianello, 2006) and also as empirical interpolation\n(Kristo\ufb00ersen, 2013).\nThe distinguishing feature of CFs from these methods is that the\nStein construction is compatible with un-normalised \u03c0.\nBelow we will establish that the RLS estimate produces vanishing EFAE under a strength-\nening of (A4):\n(A4\u2019) supx\u2208\u2126k0(x, x) < \u221e.\nRemark 4. (A4\u2019) would follow from (A3) and compactness of the state space \u2126, but we do\nnot assume compactness here. All experiments in this paper have at worst u(x) = O(\u2225x\u22252),\nso that (A4\u2019) is automatically satis\ufb01ed, for example, when we choose k(x, x\u2032) = (1+\u03b11\u2225x\u22252\n2+\n\u03b11\u2225x\u2032\u22252\n2)\u22121 exp(\u2212(2\u03b12\n2)\u22121\u2225x \u2212x\u2032\u22252\n2) for some \u03b11, \u03b12 > 0.\nWe can now state our main result:\nTheorem 2. Assume (A1,2\u2019,3,4\u2019,5) and take a RLS estimate with \u03bb = O(m\u22121/2). When\nD0 are IID samples from \u03c0, the estimator \u02c6\u00b5(D0, D1; f) is an unbiased estimator of \u00b5(f) with\nED0ED1[(\u02c6\u00b5(D0, D1; f) \u2212\u00b5(f))2] = O(n\u22127/6).\nCFs based on RLS therefore improve upon the Monte Carlo rate. The hypotheses on \u03c0\nare weak, only requiring that \u03c0 be continuously di\ufb00erentiable. Empirical evidence (below)\nindicates stronger rates hold in more regular examples. Indeed we can prove sharper results\nunder stronger conditions that include boundedness of \u2126. Details are reserved for a future\npublication (Oates et al., 2016).\nImportantly, the RLS estimate leads to a convenient closed-form expression for the CF\nestimator:\nLemma 3. Assume (A1,3). The CF estimator based on RLS is\n\u02c6\u00b5(D0, D1; f) =\n1\nn \u2212m1T(f1 \u2212\u02c6f1)\n|\n{z\n}\n(\u2217)\n+\n1T(K0 + \u03bbmI)\u22121f0\n1 + 1T(K0 + \u03bbmI)\u221211\n|\n{z\n}\n(\u2217\u2217)\n(2)\n9\nwhere f0 = [f(x1), . . . , f(xm)]T, f1 = [f(xm+1), . . . , f(xn)]T, 1 = [1, . . . , 1]T, (K0)i,j =\nk0(xi, xj) and the vector\n\u02c6f1 := K1,0(K0 + \u03bbmI)\u22121f0 + (1 \u2212K1,0(K0 + \u03bbmI)\u221211)\n\u0012 1T(K0 + \u03bbmI)\u22121f0\n1 + 1T(K0 + \u03bbmI)\u221211\n\u0013\ncontains predictions for f1 based only on D0, with (K1,0)i,j = k0(xm+i, xj).\nRemark 5. The estimator is a weighted combination of function values f = [f T\n0 , f T\n1 ]T\nwith weights summing to one. Estimates are readily obtained using standard matrix algebra.\nMoreover the weights are independent of the test function f and can be re-used to estimate\nmultiple expectations \u00b5(fj) for a collection {fj}.\nRemark 6. The samples D1 enter only through the term (\u2217) in Eqn. 2, which vanishes in\nprobability as m \u2192\u221e. Thus any randomness due to D1 vanishes and this gives another\nperspective on the source of super-root-n convergence of the estimator.\nRemark 7. The term (\u2217\u2217) in Eqn. 2 is algebraically equivalent to BMC based on H+. i.e.\n(\u2217\u2217) is the posterior mean for \u00b5(f) based on a Gaussian process (GP) prior f \u223cGP(0, k+)\nand data D0 (Rasmussen and Ghahramani, 2003, Eqn. 9). Our general construction in\nLemma 3 therefore \u201cheals\u201d BMC in the sense of (i) de-biasing the BMC estimator, (ii) gen-\neralising BMC to un-normalised densities and (iii) remaining agnostic to statistical paradigm\n(e.g. frequentist vs. Bayesian).\n2.3.3\nNon-asymptotic bounds\nThe naive computational complexity associated with the RLS estimate is O(m3) due to the\nsolution of an m \u00d7 m linear system. In situations where X is expensive to simulate or f is\nexpensive to evaluate, m is necessarily small and this additional computational cost will be\nnegligible relative to model-based computation. In such scenarios we are more interested in\nnon-asymptotic behaviour:\nTheorem 3. Assume (A1,2\u2019,3,5). Let \u03bb \u21980 to simplify presentation. Then\n|\u02c6\u00b5(D0, D1; f) \u2212\u00b5(f)| \u2264D(D0, D1)1/2\u2225f\u2225H+\nwhere\nD(D0, D1) =\n1\n(n \u2212m)2\n\u0014(1TK1,0K\u22121\n0 1)2\n1 + 1TK\u22121\n0 1\n\u22121TK1,0K\u22121\n0 K0,11 + 1TK11\n\u0015\n.\nHere (K1)i,j = k0(xm+i, xm+j) and K0,1 = KT\n1,0.\nTheorem 3 provides an explicit error bound for f \u2208H+, mimicking the approach of\n(R)QMC (Dick and Pillichshammer, 2010, Sec. 2.3.3). This o\ufb00ers a principled approach to\n10\nselection of the design points D0 since, writing VD0,D1 for the variance with respect to the\njoint distribution of D0 and D1, we have\nVD0,D1[\u02c6\u00b5(D0, D1; f)] \u2264ED0ED1[D(D0, D1)]\u2225f\u22252\nH+.\nIn the extreme case where x \u0338= x\u2032 =\u21d2k0(x, x\u2032) = 0, the discrepancy D(D0, D1) reduces to\n(n \u2212m)\u22121 and we recover the usual root-n rate. A similar bound forms the basis for recent\nwork on two-sample testing by Chwialkowski et al. (2016); Liu et al. (2016).\n2.3.4\nImplementation\nSeveral randomly chosen splits of the samples D into subsets D0 and D1 may be averaged over\nto reduce estimator variance. We note that a multi-splitting estimator remains unbiased.\nAs an alternative to multi-splitting, for applications where consistency su\ufb03ces and unbiased\nestimation is not essential, we also propose the simpli\ufb01ed estimator (\u2217\u2217) in Eqn. 2 with\nm = n. Empirical results below show that bias is negligible for practical purposes and, to\npre-empt our conclusions, we recommend this simpli\ufb01ed estimator for use in applications\ndue to its reduced variance compared to the multi-splitting estimator.\nIn all cases the\nregularisation parameter \u03bb was taken to be the smallest power of 10 such that the kernel\nmatrix K0 + \u03bbI has condition number lower than 1010.\nA kernel k(x, x\u2032; \u03b1) typically involves hyper-parameters \u03b1 that must be speci\ufb01ed. Selec-\ntion of \u03b1 can proceed via cross-validation, under the assumption that D0 are independent\nsamples from \u03c0. Speci\ufb01cally, we randomly split the samples D0 into m\u2032 training samples D0,0\nand m \u2212m\u2032 test samples D0,1. Then we propose to select \u03b1 to minimise \u2225f(0,1) \u2212\u02c6f(0,1)\u22252\nwhere f(0,1) is a vector of values fi for xi \u2208D0,1, and \u02c6f(0,1) are the corresponding predicted\nvalues. In this way we are targeting the EFAE that re\ufb02ects the variance of the CF estimator.\nWe emphasise that the cross-validated estimator does not require additional sampling and\nwill remain unbiased provided that preliminary cross-validation is performed only using D0.\nIn this paper we employed the kernel de\ufb01ned in Remark 4, with hyper-parameters \u03b11, \u03b12.\nFull pseudocode is provided in the supplement.\n2.4\nIllustration\nTo illustrate the method we begin with simple, tractable examples. Consider the synthetic\nproblem of estimating the expectation of f(X) = sin( \u03c0\nd\nPd\ni=1 Xi) where X is a d-dimensional\nstandard Gaussian random variable. By symmetry the true expectation is \u00b5(f) = 0. Initially\nwe take n = 50 IID samples and consider the scalar case d = 1. Cross-validation was used\nto select tuning parameters. Speci\ufb01cally: (i) We selected the hyper-parameters \u03b11 = 0.1,\n\u03b12 = 1 on the basis that this approximately minimised the cross-validation error (Fig. S1a).\n(ii) We found that estimator variance due to sample-splitting was minimised when at least\nhalf of the samples were allocated to D0 (Fig. S1b). We therefore set a conservative default\nm = \u2308n/2\u2309. (iii) Empirical results showed that little additional variance reduction occurs\nfrom employing multiple splits (Fig. S1c), so we chose to just use a single split. (iv) Finally,\n11\n\u22120.5\n0\n0.5\n10\n20\n30\n40\n50\n60\n70\nNum samples n\nEstimator Distribution\nArithmetic Mean\n\u22120.5\n0\n0.5\n10\n20\n30\n40\n50\n60\n70\nNum samples n\nEstimator Distribution\nRiemann Sums\n\u22120.5\n0\n0.5\n10 20 30 40 50 60 70\nNum samples n\nEstimator Distribution\nZV Control Variates\n\u22120.5\n0\n0.5\n10\n20\n30\n40\n50\n60\n70\nNum samples n\nEstimator Distribution\nControl Functionals\n\u22120.5\n0\n0.5\n10\n20\n30\n40\n50\n60\n70\nNum samples n\nEstimator Distribution\nControl Functionals (Simplified)\n\u22120.01\n0\n0.01\n\u22120.01\n0\n0.01\nFigure 2: Illustration on a synthetic integration problem in d = 1 dimension. Here we display\nthe empirical sampling distribution of Monte Carlo estimators, based on n samples and 100\nindependent realisations. [The settings for all methods were as described in the main text.]\nwe found that the bias of the simpli\ufb01ed estimator was negligible (<\u223c10\u22123) compared to\nMonte Carlo error (\u223c10\u22122) (Fig. S1d). This is in line with an analogous result for classical\ncontrol variates, where estimator bias vanishes asymptotically with respect to Monte Carlo\nerror (Glasserman, 2004, p.200).\nIn Fig.\n2 we summarise the sampling distribution of both the sample-splitting and\nsimpli\ufb01ed CF estimators as the number of samples n is varied. The alternative approaches\nof the arithmetic mean, Riemann sums and \u201czero variance\u201d (ZV) control variates are also\nshown, the latter being based on quadratic polynomials (Mira et al., 2013). It is visually\napparent that CFs enjoy the lowest variance at all samples sizes considered. We note that\nin this synthetic example, where there are essentially no computational restrictions, the\nCF framework is unnecessary and gains in precision come with comparable increases in\ncomputational cost. However we emphasise that, in the serious applications that follow, the\nCF calculations requires negligible computational resources in comparison to simulation from\nthe model. Super-root-n convergence could perhaps be achieved by employing polynomials\nof increasing degree in the ZV method, but our implementation of this approach did not\nprovide stable estimates in this example (full details in the supplement).\nSince the performance of CF is so pronounced, in order to more clearly visualise the\nresults for all sample sizes, in Fig. 3 we plot the estimator mean square error (MSE) scaled\n12\n10\n20\n30\n40\n50\n60\n70\n10\n\u22125\n10\n\u22124\n10\n\u22123\n10\n\u22122\n10\n\u22121\n10\n0\n10\n1\nNum samples n\nMSE x n\n \n \nArithmetic Mean\nRiemann Sums\nZV Control Variates\nControl Functionals\nControl Functionals (Simplified)\nFigure 3: Illustration on a synthetic integration problem in d = 1 dimension (continued).\nEmpirical assessment of asymptotic properties.\nby n, so that root-n convergence corresponds to a horizontal line. Empirical results here are\nconsistent with theory, showing that the arithmetic mean and control variates all achieve\na constant factor variance reduction, whereas Riemann sums and CFs achieve super-root-n\nconvergence. In this example CFs signi\ufb01cantly outperformed Riemann sums, the latter being\nbased on piecewise linear approximations. We plot results for both the sample-splitting CF\nestimator and the simpli\ufb01ed CF estimator, observing that the latter has lower variance.\nTo assess the generality of our conclusions we considered going beyond the scalar case to\nexamples with dimensions d = 3 and d = 5. The analogous results in Figs. S2a, S2b show\nthat, whilst increasing dimensionality presents fundamental challenges for all the variance\nreduction methods, CF continues to out-perform alternatives. Going further we considered\na variety of alternative problems, varying both the test function f and the density \u03c0. These\ninclude several pathological cases, with results summarised in Table S1. The results marked\n(b) echo the conclusions of Mira et al. (2013), that ZV control variates are e\ufb00ective in many\ncases where f is well-approximated by a low-degree polynomial and \u03c0 is a Gaussian or gamma\ndensity. However, when f is not well-approximated by a low-degree polynomial, or when \u03c0\ntakes a more complex form, as in cases marked (c), ZV control variates can be outperformed\nby CFs, which have the potential to decrease variance dramatically. We then investigated\nhow CFs can fail when theoretical assumptions are violated (see examples marked \u201cCF \u00d7\u201d).\nAs expected, violation of (A2) and (A5) in (e), (g) respectively led to poor performance of\nthe CF estimator. Interestingly, violation of di\ufb00erentiability in example (f) did not lead to\npoor estimation, though this may be because \u03c0 was only non-di\ufb00erentiable at a single point.\n13\nWe have not reported computational times for these experiments. Our work is motivated\nby settings in which either simulation from \u03c0 or evaluation of f (or both) are computationally\nprohibitive, so that additional e\ufb00ort required to implement CFs is negligible by comparison;\nwe illustrate this with two realistic applications the next section.\n3\nApplications\nTwo applications are considered that together present many of the challenges associated\nwith complex models. Firstly we consider marginalisation of hyper-parameters in hierarchi-\ncal models, focussing on a non-parametric prediction problem. Here evaluation of f forms a\ncomputational bottleneck due to the required inversion of a large matrix. For this problem,\nCFs are shown to o\ufb00er signi\ufb01cant computational savings. Secondly we consider computa-\ntion of normalising constants for models based on non-linear ordinary di\ufb00erential equations\n(ODEs). Evaluation of the likelihood function requires numerical integration of a system of\nODEs and dominates computational expenditure in both sampling from \u03c0 and evaluation of\nf. Here CFs combine with gradient-based population MCMC and thermodynamic integra-\ntion in order to deliver a state-of-the-art technique for low-variance estimation of normalising\nconstants.\n3.1\nMarginalisation in hierarchical models\nA fully Bayesian treatment of hierarchical models aims to marginalise over hyper-parameters,\nbut this often entails a prohibitive level of computation. Here we explore the e\ufb03cacy of CFs\nin such situations.\n3.1.1\nA hierarchical GP model\nThe marginalisation of hyper-parameters is a common problem in spatial statistics and\nBayesian statistics in general (Besag and Green, 1993; Agapiou et al., 2014; Filippone and\nGirolami, 2014).\nHere we consider one such model that is based on p-dimensional GP\nregression.\nDenote by Yi \u2208R a measured response variable at state zi \u2208Rp, assumed\nto satisfy Yi = g(zi) + \u03f5i where \u03f5i \u223cN(0, \u03c32) are independent for i = 1, . . . , N and\n\u03c3 > 0 will be assumed known. In order to use training data (yi, zi)n\ni=1 to make predic-\ntions regarding an unseen test point z\u2217, we place a GP prior g \u223cGP(0, c(z, z\u2032; \u03b8)) where\nc(z, z\u2032; \u03b8) = \u03b81 exp(\u22121\n2\u03b82\n2 \u2225z \u2212z\u2032\u22252\n2). Here \u03b8 = (\u03b81, \u03b82) are hyper-parameters that control how\ntraining samples are used to predict the response at a new test point. In the fully-Bayesian\nframework these are assigned hyper-priors, say \u03b81 \u223c\u0393(\u03b1, \u03b2), \u03b82 \u223c\u0393(\u03b3, \u03b4) in the shape/scale\nparametrisation, which we write jointly as \u03c0(\u03b8).\n14\n3.1.2\nMarginalising the GP hyper-parameters\nWe are interested in predicting the value of the response Y\u2217corresponding to an unseen state\nvector z\u2217. Our estimator will be the Bayesian posterior mean given by\n\u02c6Y\u2217:= E[Y\u2217|y] =\nZ\nE[Y\u2217|y, \u03b8]\u03c0(\u03b8)d\u03b8,\n(3)\nwhere we implicitly condition on the covariates z1, . . . , zN, z\u2217. Eqn. 3 is unavailable in closed\nform and we therefore naive a Monte Carlo estimate by sampling \u03b81, . . . , \u03b8n independently\nfrom the prior \u03c0(\u03b8) (more e\ufb03cient QMC estimates are considered later). Phrasing in terms\nof our previous notation, the function of interest is\nf(\u03b8) = E[Y\u2217|y, \u03b8] = C\u2217,N(CN + \u03c32IN\u00d7N)\u22121y\nwhere (CN)i,j = c(zi, zj; \u03b8) and (C\u2217,N)1,j = c(z\u2217, zj; \u03b8) and the underlying distribution is\n\u03c0(\u03b8). Each evaluation of the integrand f(\u03b8) requires O(N 3) operations due to the matrix\ninversion; this can be reduced by employing a \u201csubset of regressors\u201d approximation\nf(\u03b8) \u2248C\u2217,N\u2032(CN\u2032,NCN,N\u2032 + \u03c32CN\u2032)\u22121CN\u2032,Ny\n(4)\nwhere N \u2032 < N denotes a subset of the full data (see Sec. 8.3.1 of Rasmussen and Williams,\n2006, for full details). To facilitate the illustration below, which investigates the sampling\ndistribution of estimators, we take a random subset of N = 1, 000 training points and a\nsubset of regressors approximation with N \u2032 = 100. However we emphasise that evaluation\nof Eqn. 4 will typically be based on much larger N and N \u2032 and will be extremely expensive\nin general. In applications we would therefore have to proceed with Monte Carlo estimation\nbased on only a small number n of these function evaluations.\n3.1.3\nSARCOS robot arm\nWe used the hierarchical GP model in Sec. 3.1.2 to estimate the inverse dynamics of a\nseven degrees-of-freedom SARCOS anthropomorphic robot arm. The task, as described in\nRasmussen and Williams (2006, Sec. 8.3.1), is to map from a 21-dimensional input space\n(7 positions, 7 velocities, 7 accelerations) to the corresponding 7 joint torques using the\nhierarchical GP model described in Sec. 3.1.1. Following Rasmussen and Williams (2006)\nwe present results below on just one of the mappings, from the 21 input variables to the\n\ufb01rst of the seven torques. The dataset consists of 48,933 input-output pairs, of which 44,484\nwere used as a training set and the remaining 4,449 were used as a test set. The inputs\nwere translated and scaled to have mean zero and unit variance on the training set. The\noutputs were centred so as to have mean zero on the training set. Here \u03c3 = 0.1, \u03b1 = \u03b3 = 25,\n\u03b2 = \u03b4 = 0.04, so that each hyper-parameter \u03b8i has a prior mean of 1 and a prior standard\ndeviation of 0.2.\nFor each test point z\u2217we estimated the sampling standard deviation of \u02c6Y\u2217over 10 in-\ndependent realisations of the Monte Carlo sampling procedure. For CF we took default\n15\n0\n0.02\n0.04\n0.06\n0.08\n0\n0.02\n0.04\n0.06\n0.08\nStd[Control Functionals]\nStd[Arithmetic Mean]\nn = 50\n0\n0.02\n0.04\n0.06\n0.08\n0\n0.02\n0.04\n0.06\n0.08\nStd[Control Functionals]\nStd[Arithmetic Mean]\nn = 75\n0\n0.02\n0.04\n0.06\n0.08\n0\n0.02\n0.04\n0.06\n0.08\nStd[Control Functionals]\nStd[Arithmetic Mean]\nn = 100\n0\n0.01\n0.02\n0.03\n0\n0.01\n0.02\n0.03\nStd[Control Functionals]\nStd[ZV Control Variates]\nn = 50\n0\n0.01\n0.02\n0.03\n0\n0.01\n0.02\n0.03\nStd[Control Functionals]\nStd[ZV Control Variates]\nn = 75\n0\n0.005 0.01 0.015 0.02\n0\n0.005\n0.01\n0.015\n0.02\nStd[Control Functionals]\nStd[ZV Control Variates]\nn = 100\nFigure 4: Marginalisation of hyper-parameters in hierarchical models. [Here we display the\nsampling standard deviation of Monte Carlo estimators for the posterior predictive mean\nE[Y\u2217|y] in the SARCOS robot arm example, computed over 10 independent realisations.\nEach point, representing one Monte Carlo integration problem, is represented by a cross.]\nhyper-parameters \u03b11 = 0.1, \u03b12 = 1, the latter re\ufb02ecting the fact that the training data were\nstandardised. The estimator standard deviations were estimated in this way for all 4,449\ntest samples and the full results are shown in Fig. 4. Note that each test sample corresponds\nto a di\ufb00erent function f and thus these results are quite objective, encompassing thousands\nof di\ufb00erent Monte Carlo integration problems. Results show that, for the vast majority of\nintegration problems, CF achieves a lower estimator variance compared with both the arith-\nmetic mean estimator and ZV control variates. Here the cost of post-processing the Monte\nCarlo samples (using either ZV control variates or CF) is negligible in comparison to the\ncost of evaluating the function f, even once. Indeed, CF requires that we invert a n \u00d7 n\nmatrix once, where n is no larger than N \u2032 in this example.\nIn the supplement we investigate an extension that draws design points D0 using RQMC.\nResults show that CFs+RQMC outperforms RQMC alone.\n3.2\nNormalising constants for non-linear ODE models\nOur second application concerns the estimation of normalising constants for non-linear ODE\nmodels (e.g. Calderhead and Girolami, 2009). Recent empirical investigations recommend\n16\nthermodynamic integration (TI) for this task (e.g. Friel and Wyse, 2012).\nThe control\nvariate method of Mira et al. (2003) was recently applied to TI by Oates et al. (2016),\nwho found that this \u201ccontrolled thermodynamic integral\u201d (CTI) was extremely e\ufb00ective for\nstandard regression models, but only moderately e\ufb00ective in complex models including non-\nlinear ODEs due to poor approximation by low-degree polynomials. Below we study the\napplication of CFs to TI in this setting where CTI is less e\ufb00ective.\n3.2.1\nThermodynamic integration\nConditional on an inverse temperature parameter t, the \u201cpower posterior\u201d for parameters\n\u03b8 given data y is de\ufb01ned as p(\u03b8|y, t) \u221dp(y|\u03b8)tp(\u03b8) (Friel and Pettitt, 2008).\nVarying\nt \u2208[0, 1] produces a continuous path between the prior p(\u03b8) and the posterior p(\u03b8|y) and it\nis assumed here that all intermediate distributions exist and are well-de\ufb01ned. The standard\nthermodynamic identity is\nlog p(y) =\nZ 1\n0\nE\u03b8|y,t[log p(y|\u03b8)]dt,\n(5)\nwhere the expectation in the integrand is with respect to the power posterior. In TI, the one-\ndimensional integral in Eqn. 5 is evaluated numerically using a quadrature approximation\nover a discrete temperature ladder 0 = t0 < t1 < \u00b7 \u00b7 \u00b7 < tm = 1. Here we use the second-order\nquadrature recommended by Friel et al. (2014):\nlog p(y) \u2248\nm\u22121\nX\ni=0\n(ti+1 \u2212ti)\n2\n(\u02c6\u00b5i + \u02c6\u00b5i+1) \u2212(ti+1 \u2212ti)2\n12\n(\u02c6\u03bdi+1 \u2212\u02c6\u03bdi),\nwhere \u02c6\u00b5i, \u02c6\u03bdi are Monte Carlo estimates of the posterior mean and variance respectively of\nlog p(y|\u03b8) when \u03b8 arises from p(\u03b8|y, ti). CTI uses ZV control variates to reduce the variance\nof these estimates.\nHowever, in complex models log p(y|\u03b8) will be poorly approximated\nby a low-degree polynomial and p(\u03b8|y, t) will be non-Gaussian; this explains the mediocre\nperformance of CTI in these cases. In contrast, CFs should still be able to deliver gains in\nestimation.\n3.2.2\nNon-linear ODE models\nThe approach is illustrated by computing the marginal likelihood for a non-linear ODE\nmodel (the van der Pol oscillator), described in full in the supplement. For TI, a temperature\nschedule ti = (i/30)5 was used, following the recommendation by Calderhead and Girolami\n(2009). The power posterior is not available in closed form, precluding the straight-forward\ngeneration of IID samples. Instead, samples from each of the power posteriors p(\u03b8|y, ti)\nwere obtained using population MCMC, involving both (i) \u201cwithin-temperature\u201d proposals\nproduced by the (simpli\ufb01ed) m-MALA algorithm of Girolami and Calderhead (2011), and\n(ii) \u201cbetween-temperature\u201d proposals, as described previously by Calderhead and Girolami\n(2009).\nGradient information is thus pre-computed in the sampling scheme and can be\n17\n25.3\n25.4\n25.5\n25.6\n25.7\nEstimator Distribution\nStandard TI\nNum samples n\n20\n40\n60\n80\n100\n25.3\n25.4\n25.5\n25.6\n25.7\nEstimator Distribution\nControlled TI (JASA 2016)\nNum samples n\n20\n40\n60\n80\n100\n25.3\n25.4\n25.5\n25.6\n25.7\nEstimator Distribution\nTI + Control Functionals\nNum samples n\n20\n40\n60\n80\n100\nFigure 5: Estimation of normalising constants for non-linear ordinary di\ufb00erential equations\nusing thermodynamic integration (TI); van der Pol oscillator example. [Here we show the\ndistribution of 100 independent realisations of each estimator for log p(y). \u201cStandard TI\u201d is\nbased on arithmetic means. \u201cControlled TI\u201d is based on ZV control variates.]\nleveraged \u201cfor free\u201d, as noted by Papamarkou et al. (2014).\nWe denote the number of\nsamples by n, such that for each of the 31 temperatures we obtained n samples (a total of\n31\u00d7n occasions where the system of ODEs was integrated numerically). Both sampling and\nevaluation of the integrand are computationally expensive, requiring the numerical solution\nof a system of ODEs.\nResults in Fig. 5 show that the CTI estimator improves upon the standard TI estimator,\nbut a more substantial reduction in estimator variance results from using the CF method.\nFor the CF computation we have used the simpli\ufb01ed but biased CF estimator, since TI in any\ncase produces a biased estimate for the normalising constant due to numerical quadrature.\nThe hyper-parameters \u03b11 = 0.1, \u03b12 = 3 were selected on the basis of cross-validation. The\nadditional cost of using CF is essentially zero relative to running the population MCMC\nsampler, the latter requiring repeated solution of the ODE system.\n4\nDiscussion\nThis paper developed a novel and general approach to integration that achieves super-root-n\nconvergence. An important feature of CFs is that variance reduction is formulated as a post-\nhoc step. This has several advantages: (i) No modi\ufb01cation is required to existing computer\ncode associated with either the sampling process or the model itself. (ii) Speci\ufb01c implemen-\ntational choices, e.g. for the kernel, can be made after performing expensive simulations.\nThrough exploitation of recent results in functional analysis we were able to realise our gen-\neral framework and construct estimators with an analytic form. Empirical results evidenced\nthe practical utility of CF estimators in settings where gradient information is available and\n18\nthe dimensionality of the problem is not too large (e.g. \u226410). The paper concludes below\nby suggesting directions for further research.\nIn terms of methodology: (i) The estimates we presented here are not parameterisation-\ninvariant. Likewise the speci\ufb01cation of f and \u03c0 is not unique, as we can employ an importance\nsampling transformation f 7\u2192(f\u03c0)/\u03c0\u2032, \u03c0 7\u2192\u03c0\u2032. It would therefore be interesting to elicit\ne\ufb00ective parametrisations as an additional post-hoc step. (ii) The version of CFs presented\nhere is limited in terms of the dimension of the problems for which it is e\ufb00ective. Techniques\nfor high-dimensional functional approximation should be applicable in the context of CFs\n(e.g. Dick et al., 2013) and this forms part of our ongoing research.\nIn terms of theory: (i) For bounded \u2126, sharper asymptotics are provided in a sequel, Oates\net al. (2016). These account for various levels of smoothness of both f and \u03c0 and help to\nexplain the strong empirical results presented here. However the case of unbounded \u2126seems\nconsiderably more challenging to characterise. (ii) For problems involving un-normalised\ndensities \u03c0, sampling is naturally facilitated by MCMC. The analysis of CFs is carried out in\nOates et al. (2016) under a uniform ergodicity assumption. For unbounded \u2126this condition\nis too strong and future work will aim to relax this constraint.\nIn terms of application: (i) Our methods were motivated by the un-normalised den-\nsities arising in Bayesian computation.\nAn extension should be possible to models with\nunknown, parameter-dependent normalising constants, which include e.g. Markov random\n\ufb01elds (Everitt, 2012) and random network models (Friel et al., 2015). (ii) An interesting\ndirection would be to use the discrepancy D(D0, D1) as a tool for assessment of MCMC con-\nvergence, providing a reproducing kernel Hilbert space alternative to Gorham and Mackey\n(2015).\nFinally we note that Oates and Girolami (2016) provide a complementary study of CF\nstrategies in the QMC setting.\nAcknowledgements:\nThe authors are grateful to the editor and referees, whose valuable\nfeedback helped to improve the paper. The authors bene\ufb01ted from discussions with Sergios\nAgapiou, Michel Ca\ufb00arel, Adam Johansen, Christian Robert, Daniel Simpson and Tim Sul-\nlivan. CJO was supported by EPSRC [EP/D002060/1] and the ARC Centre for Excellence\nin Mathematical and Statistical Frontiers. MG was supported by EPSRC [EP/J016934/1],\nEU [EU/259348], an EPSRC Established Career Fellowship and a Royal Society Wolfson\nResearch Merit Award. NC was supported by the ANR (Agence Nationale de la Recherche)\ngrant Labex ECODEC ANR [11-LABEX-0047].\nA\nProofs\nProof of Proposition 1. We exploit the unbiasedness property ED1[\u02c6\u00b5(D0, D1; f) \u2212\u00b5(f)] = 0\nto show that\nED0ED1[(\u02c6\u00b5(D0, D1; f) \u2212\u00b5(f))2]\n=\nED0VD1[\u02c6\u00b5(D0, D1; f)]\n=\n(n \u2212m)\u22121ED0[\u03c32(f \u2212sf,D0)]\n19\nwhere (n \u2212m)\u22121 = O(n\u22121) and by hypothesis ED0[\u03c32(f \u2212sf,D0)] = O(m\u2212\u03b4). Thus using\nm = O(n\u03b3) produces an overall rate O(n\u22121\u2212\u03b3\u03b4), as required.\nProof of Proposition 2. (A1) ensures \u03c8 is well-de\ufb01ned. Since \u2202\u2126is piecewise smooth we can\napply the divergence theorem (e.g. Bourne and Kendall, 1977, p.159) to obtain\n\u00b5(\u03c8) =\nZ\n\u2126\n\u03c8(x)\u03c0(x)dx =\nZ\n\u2126\n\u2207x \u00b7 [\u03c6(x)\u03c0(x)]dx =\nI\n\u2202\u2126\n\u03c0(x)\u03c6(x) \u00b7 n(x)S(dx),\nwhich is zero by (A2). The use of this identity in statistical applications is often attributed\nto Stein (1970). Thus \u00b5(sf,D0) = \u00b5(c) + \u00b5(\u03c8) = c, as required.\nProof of Theorem 1. Stage 1: We begin by de\ufb01ning the set of CFs H0. Given a reproducing\nkernel k : \u2126\u00d7 \u2126\u2192R for the reproducing kernel Hilbert space (RKHS) H, de\ufb01ne the\ncanonical feature map \u03a6 : \u2126\u2192H by \u03a6(x) = k(\u00b7, x). Under (A1) the gradient function\nu : \u2126\u2192R is well-de\ufb01ned. Under (A3) k has mixed \ufb01rst order partial derivatives; it follows\nthat all elements \u03c6i \u2208H are di\ufb00erentiable and thus (\u2202/\u2202xi)\u03c6i(x) is well-de\ufb01ned (Steinwart\nand Christmann, 2008, Cor. 4.36, p131). We then have that\n\u03c8(x)\n=\nd\nX\ni=1\n(\u2202/\u2202xi)\u03c6i(x) + ui(x)\u03c6i(x)\n=\nd\nX\ni=1\n(\u2202/\u2202xi)\u27e8\u03c6i, \u03a6(x)\u27e9H + ui(x)\u27e8\u03c6i, \u03a6(x)\u27e9H\n=\nd\nX\ni=1\n\u27e8\u03c6i, (\u2202/\u2202xi)\u03a6(x) + ui(x)\u03a6(x)\u27e9H =\nd\nX\ni=1\n\u27e8\u03c6i, \u03a6\u2217\ni (x)\u27e9H,\nwhere we have used the notation \u03a6\u2217\ni (x) = (\u2202/\u2202xi)\u03a6(x) + ui(x)\u03a6(x). Write \u03a6\u2217: \u2126d \u2192Rd\nfor the derived feature map with ith component \u03a6\u2217\ni . De\ufb01ne the set of all CFs \u03c8 of this form\nas\nH0 = {\u03c8 : \u2126\u2192R such that \u2200x \u2208\u2126, \u03c8(x) = \u27e8\u03c6, \u03a6\u2217(x)\u27e9Hd for some \u03c6 \u2208Hd}.\nClearly H0 is a vector space with addition and multiplication de\ufb01ned pointwise; (\u03bb\u03c8 +\n\u03bb\u2032\u03c8\u2032)(x) = \u03bb\u03c8(x) + \u03bb\u2032\u03c8\u2032(x).\nStage 2: We now show that H0 can be endowed with the structure of a RKHS. To this\nend, de\ufb01ne a norm on H0 by\n\u2225\u03c8\u2225H0 := inf\n\u03c6\u2208Hd{\u2225\u03c6\u2225Hd such that \u2200x \u2208\u2126, \u03c8(x) = \u27e8\u03c6, \u03a6\u2217(x)\u27e9Hd}.\nTheorem 4.21 (p121) of Steinwart and Christmann (2008) immediately gives that the normed\nspace (H0, \u2225\u00b7 \u2225H0) is a RKHS whose kernel k0 satis\ufb01es k0(x, x\u2032) = \u27e8\u03a6\u2217(x), \u03a6\u2217(x\u2032)\u27e9Hd. Thus\n20\nwe can directly calculate\nk0(x, x\u2032)\n=\n\u27e8\u03a6\u2217(x), \u03a6\u2217(x\u2032)\u27e9Hd\n=\nd\nX\ni=1\n\u27e8(\u2202/\u2202xi)\u03a6(x) + ui(x)\u03a6(x), (\u2202/\u2202x\u2032\ni)\u03a6(x\u2032) + ui(x\u2032)\u03a6(x\u2032)\u27e9H\n=\nd\nX\ni=1\n(\u2202/\u2202xi)(\u2202/\u2202x\u2032\ni)k(x, x\u2032) + ui(x)(\u2202/\u2202x\u2032\ni)k(x, x\u2032)\n+ui(x\u2032)(\u2202/\u2202xi)k(x, x\u2032) + ui(x)ui(x\u2032)k(x, x\u2032),\nwhere the interchange of derivative and inner product is justi\ufb01ed by (A3) and Lemma 4.34\n(p130) in Steinwart and Christmann (2008). This completes the proof.\nProof of Lemma 1. (A1,3) ensure the kernel k0 is well-de\ufb01ned. Then\nZ\n\u2126\nk0(x, x\u2032)\u03c0(x\u2032)dx\u2032\n=\nZ\n\u2126\n[\u2207x \u00b7 \u2207x\u2032k(x, x\u2032)]\u03c0(x\u2032)dx\u2032 +\nZ\n\u2126\n[u(x) \u00b7 \u2207x\u2032k(x, x\u2032)]\u03c0(x\u2032)dx\u2032\n+\nZ\n\u2126\n[u(x\u2032) \u00b7 \u2207xk(x, x\u2032)]\u03c0(x\u2032)dx\u2032 +\nZ\n\u2126\n[u(x) \u00b7 u(x\u2032)k(x, x\u2032)]\u03c0(x\u2032)dx\u2032\n=\nZ\n\u2126\n[\u2207x\u2032 \u00b7 \u2207xk(x, x\u2032)]\u03c0(x\u2032) + [\u2207xk(x, x\u2032)] \u00b7 [\u2207x\u2032\u03c0(x\u2032)]dx\u2032\n+u(x) \u00b7\nZ\n\u2126\n[\u2207x\u2032k(x, x\u2032)]\u03c0(x\u2032) + k(x, x\u2032)[\u2207x\u2032\u03c0(x\u2032)]dx\u2032\n=\nZ\n\u2126\n\u2207x\u2032 \u00b7 {[\u2207xk(x, x\u2032)]\u03c0(x\u2032)} dx\u2032 + u(x) \u00b7\nZ\n\u2126\n\u2207x\u2032 {k(x, x\u2032)\u03c0(x)} dx\u2032.\nNow using the divergence theorem (Bourne and Kendall, 1977, p.159) we obtain\n=\nI\n\u2202\u2126\n\u2207xk(x, x\u2032)\u03c0(x\u2032) \u00b7 n(x\u2032)S(dx\u2032)\n|\n{z\n}\n=0 \u03c0-a.e. from (A2\u2019)\n+u(x) \u00b7\nI\n\u2202\u2126\nk(x, x\u2032)\u03c0(x\u2032)n(x\u2032)S(dx\u2032)\n|\n{z\n}\n=0 \u03c0-a.e. from (A2\u2019)\n,\nproving the claim.\nProof of Lemma 2. From Theorem 1, (A1,3) ensure H0 is well-de\ufb01ned.\nMoreover, from\nLemma 1, (A1,2\u2019,3) imply that \u00b5(\u03c8) = 0 and thus\n\u03c32(\u03c8) =\nZ\n\u2126\n\u03c8(x)2\u03c0(x)dx.\nNow, given \u03c8 \u2208H0, we need to show \u03c32(\u03c8) < \u221e. By the reproducing property followed by\nthe Cauchy-Schwarz inequality, we have\n|\u03c8(x)| = |\u27e8\u03c8, k0(\u00b7, x)\u27e9H0| \u2264\u2225\u03c8\u2225H0\u2225k0(\u00b7, x)\u2225H0.\n21\nUsing the reproducing property again, we have \u2225k0(\u00b7, x)\u22252\nH0 = k0(x, x) and it follows from\n(A4) that\n\u03c32(\u03c8) =\nZ\n\u03c8(x)2\u03c0(x)dx \u2264\nZ\n\u2126\n\u2225\u03c8\u22252\nH0k0(x, x)\u03c0(x)dx = \u2225\u03c8\u22252\nH0\nZ\n\u2126\nk0(x, x)\u03c0(x)dx < \u221e,\nas required.\nProof of Theorem 2. From Theorem 1, (A1,3) ensure H+ is well-de\ufb01ned. Unbiasedness fol-\nlows from (A1,2\u2019,3) and Lemma 2. Below we employ the standard notation\nL2(\u03c0) = L2(\u03c0) \\ {f such that f = 0 \u03c0-almost everywhere}\nand denote the standard norm on this space by \u2225\u00b7 \u2225L2(\u03c0).\nFor the remainder we appeal to the relatively recent work of Sun and Wu (2009), who\nconsidered convergence in a general setting where (i) \u2126\u222a\u2202\u2126is not required to be compact\nin Rd, and (ii) only weak assumptions are required on the kernel k+, which can be easily\nsatis\ufb01ed in our setting. To this end, de\ufb01ne the integral operator\n(Tg)(x) :=\nZ\n\u2126\nk+(x, x\u2032)g(x\u2032)\u03c0(x\u2032)dx\u2032,\nx \u2208\u2126, g \u2208L2(\u03c0).\nIn the well-posed setting of (A5), Theorem 1.1 of Sun and Wu (2009) establishes that if\n(i) supx\u2208\u2126k+(x, x) < \u221eand (ii) T \u22121/2f \u2208L2(\u03c0), then with a RLS estimator based on\n\u03bb = O(m\u22121/2) we have ED0[\u03c32(f \u2212sf,D0)] = O(m\u22121/6). Inserting this rate into Proposition\n1 with \u03b3 = 1, \u03b4 = 1/6 would produce a MSE ED0ED1[(\u02c6\u00b5(D0, D1; f) \u2212\u00b5(f))2] = O(n\u22121\u2212\u03b3\u03b4) =\nO(n\u22127/6).\nIt therefore remains to prove requirements (i) and (ii) above are satis\ufb01ed. For (i) we\nhave that supx\u2208\u2126k+(x, x\u2032) = 1 + supx\u2208\u2126k0(x, x), where the second term is \ufb01nite by (A4\u2019).\nFor (ii), Prop. 3.3 of Sun and Wu (2009) (which does not depend on Theorem 1.1 of the\nsame paper) shows that, when (i) holds, we have \u2225T \u22121/2h\u2225L2(\u03c0) = \u2225h\u2225H+ for all h \u2208H+.\nSince f \u2208H+ by (A5) we thus have \u2225T \u22121/2f\u2225L2(\u03c0) = \u2225f\u2225H+ < \u221eand so T \u22121/2f \u2208L2(\u03c0), as\nrequired.\nProof of Lemma 3. From Theorem 1, (A1,3) ensure H0 is well-de\ufb01ned. The interpolation\nproblem is equivalently expressed as sf,D0 = \u02c6c + \u02c6\u03c8 where\n(\u02c6c, \u02c6\u03c8) := arg min\nc\u2208C, \u03c8\u2208H0\n\u2225c\u22252\nC + \u2225\u03c8\u22252\nH0 s.t. f(xj) = c + \u03c8(xj) for j = 1, ..., m.\nFor \ufb01xed c \u2208C, the representer theorem (Steinwart and Christmann, 2008, Thm. 5.5, p168)\ntells us that the solution\n\u02c6\u03c8 = arg min\n\u03c8\u2208H0\n\u2225\u03c8\u2225H0 s.t. f(xj) = c + \u03c8(xj) for j = 1, ..., m\n22\ntakes the form \u03c8(x) = Pm\ni=1 \u03b2ik0(xi, x) where, due to the reproducing property, \u2225\u03c8\u22252\nH0 =\n\u03b2TK0\u03b2. Thus writing \u03b2 = [\u03b21, . . . , \u03b2m]T reduces the problem to\n(\u02c6c, \u02c6\u03b2) = arg min\nc\u2208C,\u03b2\u2208Rmc2 s.t. f(xj) = c +\nm\nX\ni=1\n\u03b2ik0(xi, xj) for j = 1, ..., m\nDi\ufb00erentiating with respect to c and \u03b2 leads, via the Woodbury matrix inversion identity,\nto the solution\n\u02c6c =\n1TK\u22121\n0 f0\n1 + 1TK\u22121\n0 1,\n\u02c6\u03b2 = K\u22121\n0 (f0 \u2212\u02c6c1)\nand associated \ufb01tted values \u02c6f1 = \u02c6c1 + K1,0 \u02c6\u03b2 at the points D1. Putting this together, we\nhave\n\u02c6\u00b5(D0, D1; f) =\n1\nn \u2212m\nn\nX\ni=m+1\nfD0(xi)\n=\n1\nn \u2212m\nn\nX\ni=m+1\nf(xi) \u2212sf,D0(xi) + \u00b5(sf,D0)\n=\n1\nn \u2212m1T(f1 \u2212\u02c6f1) + \u02c6c.\nThis completes the proof.\nProof of Theorem 3. From Theorem 1, (A1,3) ensure H0 is well-de\ufb01ned. The CF estimator\ntakes the form\n\u02c6\u00b5(D0, D1; f) =\nn\nX\ni=1\nwif(xi) = \u02c6c +\nn\nX\ni=1\nwi\u03c8(xi)\nwhere, by Lemma 3, the vector of weights w = [w1, . . . , wn]T is given by\nw =\n\"\n\u2212(K0+\u03bbmI)\u22121K0,11\nn\u2212m\n+\n1\nn\u2212m\n(1T (K0+\u03bbmI)\u22121K0,11)(K0+\u03bbmI)\u221211\n1+1T (K0+\u03bbmI)\u221211\n1\nn\u2212m1\n#\n(6)\nand satis\ufb01es 1Tw = 1. Using the reproducing property, the estimation error is\n\u02c6\u00b5(D0, D1; f) \u2212\u00b5(f)\n=\nn\nX\ni=1\nwif(xi) \u2212\nZ\n\u2126\nf(x)\u03c0(x)dx\n=\nn\nX\ni=1\nwi\u03c8(xi) \u2212\nZ\n\u2126\n\u03c8(x)\u03c0(x)dx\n=\nD\n\u03c8,\nn\nX\ni=1\nwik0(\u00b7, xi) \u2212\nZ\n\u2126\nk0(\u00b7, x)\u03c0(x)dx\n|\n{z\n}\n=0 from Lemma 1\nE\nH0.\nIt follows from the Cauchy-Schwarz inequality that\n|\u02c6\u00b5(D0, D1; f) \u2212\u00b5(f)| \u2264\u2225\u03c8\u2225H0\n\r\r\r\r\r\nn\nX\ni=1\nwik0(\u00b7, xi)\n\r\r\r\r\r\nH0\n.\n23\nThe \ufb01rst term satis\ufb01es \u2225\u03c8\u22252\nH0 \u2264\u02c6c2 + \u2225\u03c8\u22252\nH0 = \u2225f\u22252\nH+ and, from the reproducing property,\nthe second term satis\ufb01es\n\r\r\r\r\r\nn\nX\ni=1\nwik0(\u00b7, xi)\n\r\r\r\r\r\n2\nH0\n= wTKw,\nK =\n\u0014 K0\nK0,1\nK1,0\nK1\n\u0015\n.\n(7)\nFinally, upon substituting Eqn. 6 into Eqn. 7 we obtain the required result with D(D0, D1) =\nwTKw. The special case \u03bb = 0 is reported in the statement of the theorem.\nReferences\nAgapiou, S., Bardsley, J. M., Papaspiliopoulos, O. and Stuart, A. M. (2014) Analysis of the\nGibbs sampler for hierarchical inverse problems. SIAM/ASA J. Uncertainty Quanti\ufb01ca-\ntion, 2(1), 511-544.\nAndrad\u00b4ottir, S., Heyman, D. P. and Ott, T. J. (1993) Variance reduction through smoothing\nand control variates for Markov Chain simulations. ACM T. M. Comput. S., 3, 167-189.\nAngelikopoulos, P., Papadimitriou, C. and Koumoutsakos, P. (2012) Bayesian uncertainty\nquanti\ufb01cation and propagation in molecular dynamics simulations: A high performance\ncomputing framework. J. Chem. Phys., 137, 144103.\nAssaraf, R. and Ca\ufb00arel, M. (1999) Zero-Variance Principle for Monte Carlo Algorithms.\nPhys. Rev. Lett., 83(23), 4682-4685.\nAssaraf, R. and Ca\ufb00arel, M. (2003) Zero-Variance Zero-Bias Principle for Observables in\nquantum Monte Carlo: Application to Forces. J. Chem. Phys., 119, 10536.\nBa, S. and Joseph, V. R. (2012) Composite Gaussian process models for emulating expensive\nfunctions. Ann. Appl. Stat., 6(4), 1838-1860.\nBakhvalov, N.S. (1959) On the approximate calculation of multiple integrals (in Russian).\nVestnik MGU, Ser. Math. Mech. Astron. Phys. Chem., 4, 3-18.\nBerlinet, A. and Thomas-Agnan, C. (2004) Reproducing kernel Hilbert spaces in probability\nand statistics. Kluwer Academic, Boston.\nBesag, J. and Green, P. J. (1993) Spatial statistics and Bayesian computation. J. R. Statist.\nSoc. B, 55, 25-37.\nBourne, D. E. and Kendall, P. C. (1977) Vector analysis and Cartesian tensors (2nd ed.).\nNelson and Sons, UK.\nBriol, F.X., Oates, C.J., Girolami, M., Osborne, M.A. (2016) Frank-Wolfe Bayesian Quadra-\nture: Probabilistic Integration with Theoretical Guarantees. Adv. Neur. In., 28.\n24\nBriol, F.X., Oates, C.J., Girolami, M., Osborne, M.A., Sejdinovic, D. (2016) Probabilistic\nIntegration: A Role for Statisticians in Numerical Analysis? arXiv:1512.00933.\nCalderhead, B. and Girolami, M. (2009) Estimating Bayes factors via thermodynamic inte-\ngration and population MCMC. Comput. Stat. Data An., 53, 4028-4045.\nChwialkowski, K., Strathmann, H. and Gretton, A. (2016) A Kernel Test of Goodness of\nFit. arXiv:1602.02964.\nCornuet, J.-M., Marin, J.-M., Mira, A. and Robert, C. P. (2012) Adaptive Multiple Impor-\ntance Sampling. Scand. J. Stat., 39, 798-812.\nDellaportas, P. and Kontoyiannis, I. (2012) Control variates for estimation based on re-\nversible Markov chain Monte Carlo samplers. J. R. Statist. Soc. B, 74, 133-161.\nDick, J. and Pillichshammer, F. (2010) Discrepancy theory and quasi-Monte Carlo integra-\ntion. Springer, Berlin.\nDick, J., Kuo, F. Y. and Sloan, I. H. (2013) High-dimensional integration: the quasi-Monte\nCarlo way. Acta Numer., 22, 133-288.\nDouc, R. and Robert, C. P. (2011) A vanilla Rao-Blackwellization of Metropolis-Hastings\nalgorithms. Ann. Stat., 39(1), 261-277.\nEveritt, R. G. (2012) Bayesian parameter estimation for latent Markov random \ufb01elds and\nsocial networks. J. Comp. Graph. Stat., 21(4), 940-960.\nFilippone, M. and Girolami, M. (2014) Pseudo-marginal Bayesian inference for Gaussian\nprocesses. IEEE T. Pattern Anal., 36(11), 2214-2226.\nFriel, N. and Pettitt, A. N. (2008) Marginal likelihood estimation via power posteriors. J.\nR. Statist. Soc. B, 70, 589-607.\nFriel, N. and Wyse, J. (2012) Estimating the statistical evidence - a review. Stat. Neerl., 66,\n288-308.\nFriel, N., Hurn, M. A. and Wyse, J. (2014) Improving power posterior estimation of statistical\nevidence. Stat. Comp., 24, 709-723.\nFriel, N., Mira, A. and Oates, C. J. (2015) Exploiting Multi-Core Architectures for Reduced-\nVariance Estimation with Intractable Likelihoods. Baysian Anal., 11(1), 215-245.\nGhosh, J. and Clyde, M. A. (2011) Rao-Blackwellization for Bayesian variable selection and\nmodel averaging in linear and binary regression: A novel data augmentation approach. J.\nAm. Stat. Assoc., 106(495), 1041-1052.\nGiles, M. B. (2013) Multilevel Monte Carlo methods. In Monte Carlo and Quasi-Monte Carlo\nMethods (pp. 83-103). Springer, Berlin Heidelberg.\n25\nGiles, M. B. and Szpruch, L. (2014) Antithetic multilevel Monte Carlo estimation for multi-\ndimensional SDEs without L\u00b4evy area simulation. Ann. Appl. Prob., 24(4), 1585-1620.\nGirolami, M. and Calderhead, B. (2011) Riemann manifold Langevin and Hamiltonian Monte\nCarlo methods. J. R. Statist. Soc. B, 73, 1-37.\nGlasserman, P. (2004) Monte Carlo methods in \ufb01nancial engineering. Springer, New York.\nGorham, J. and Mackey, L. (2015) Measuring Sample Quality with Stein\u2019s Method. Adv.\nNeur. In. 28, 226-234.\nGreen, P. and Han, X. (1992) Metropolis methods, Gaussian proposals, and antithetic vari-\nables. Lect. Notes Stat., 74, 142-164.\nHammer, H. and Tjelmeland, H. (2008) Control variates for the Metropolis-Hastings algo-\nrithm. Scand. J. Stat., 35, 400-414.\nHeinrich, S. (1995) Variance reduction for Monte Carlo methods by means of deterministic\nnumerical computation. Monte Carlo Methods and Applications, 1(4), 251-278.\nHigdon, D., McDonnell, J. D., Schunck, N., Sarich, J. and Wild, S. M. (2015) A Bayesian ap-\nproach for parameter estimation and prediction using a computationally intensive model.\nJ. Phys. G: Nucl. Part. Phys., 42, 034009.\nKohlho\ufb00, K. J., Shukla, D., Lawrenz, M., Bowman, G. R., Konerding, D. E., Belov, D.,\nAltman, R. B. and Pande, V. S. (2014) Cloud-based simulations on Google Exacycle\nreveal ligand modulation of GPCR activation pathways. Nat. Chem., 6(1), 15-21.\nKristo\ufb00ersen, S. (2013) The empirical interpolation method. Master\u2019s thesis, Department of\nMathematical Sciences, Norwegian University of Science and Technology.\n Latuszy\u00b4nski, K., Green, P., Pereyra, M. and Robert, C. P. (2015) Bayesian computation: a\nperspective on the current state, and sampling backwards and forwards. Stat. Comput.,\n25(4), 835-862.\nLi, W., Tan, Z. and Chen, R. (2013) Two-Stage Importance Sampling With Mixture Pro-\nposals. J. Am. Stat. Assoc., 108(504), 1350-1365.\nLi, W., Chen, R. and Tan, Z. (2016) E\ufb03cient Sequential Monte Carlo with Multiple Proposals\nand Control Variates. J. Am. Stat. Assoc., to appear.\nLiu, Q., Lee, J. D. and Jordan, M. (2016) A Kernelized Stein Discrepancy for Goodness-of-\ufb01t\nTests and Model Evaluation. arXiv:1602.03253.\nMira, A., Tenconi, P. and Bressanini, D. (2003) Variance reduction for MCMC. Technical\nReport 2003/29, Universit\u00b4a degli Studi dell\u2019 Insubria, Italy.\n26\nMira, A., Solgi, R. and Imparato, D. (2013) Zero Variance Markov Chain Monte Carlo for\nBayesian Estimators. Stat. Comput., 23, 653-662.\nMizielinski, M. S., Roberts, M. J., Vidale, P. L., Schiemann, R., Demory, M. E., Strachan, J.,\nEdwards, T., Stephens, A., Lawrence, B. N., Pritchard, M., Chiu, P., Iwi, A., Churchill, J.,\ndel Cano Novales, C., Kettleborough, J., Roseblade, W., Selwood, P., Foster, M., Glover,\nM. and Malcolm, A. (2014) High-resolution global climate modelling: the UPSCALE\nproject, a large-simulation campaign. Geosci. Model Dev., 7(4), 1629-1640.\nMijatovi\u00b4c, A. and Vogrinc, J. (2015) On the Poisson equation for Metropolis-Hastings chains.\narXiv:1511.07464.\nO\u2019Hagan, A. (1991) Bayes-Hermite Quadrature. J. Stat. Plan. Infer., 29, 245-260.\nOates, C. J., Papamarkou, T. and Girolami, M. (2016) The Controlled Thermodynamic\nIntegral for Bayesian Model Evidence Evaluation. J. Am. Stat. Assoc., to appear.\nOates, C. J. and Girolami, M. (2016) Control Functionals for Quasi Monte Carlo Inte-\ngration. In: Proc. 19th International Conference on Arti\ufb01cial Intelligence and Statistics\n(AISTATS), to appear.\nOates, C. J., Cockayne, J., Briol, F.-X. and Girolami, M. (2016) Convergence Rates for a\nClass of Estimators Based on Stein\u2019s Identity. arXiv:1603.03220.\nOlsson, J. and Ryden, T. (2011) Rao-Blackwellization of particle Markov chain Monte Carlo\nmethods using forward \ufb01ltering backward sampling. IEEE T. Signal Proces., 59(10), 4606-\n4619.\nPapamarkou, T., Mira, A. and Girolami, M. (2014) Zero Variance Di\ufb00erential Geometric\nMarkov Chain Monte Carlo Algorithms. Bayesian Anal., 9:97-128,\nPhilippe, A. (1997) Processing simulation output by Riemann sums. J. Statist. Comput.\nSimul., 59, 295-314.\nRasmussen, C. E. and Ghahramani, Z. (2003) Bayesian Monte Carlo. Adv. Neur. Inf., 17,\n505-512.\nRasmussen, C.E. and Williams, C.K. (2006) Gaussian Processes for Machine Learning. MIT\nPress.\nRobert, C. and Casella, G. (2004) Monte Carlo Statistical Methods (2nd ed.). Springer-\nVerlag, New York.\nRubinstein, R. Y. and Marcus, R. (1985) E\ufb03ciency of Multivariate Control Variates in Monte\nCarlo Simulation. Oper. Res., 33, 661-677.\nRubinstein, R. Y. and Kroese, D. P. (2011) Simulation and the Monte Carlo method. John\nWiley and Sons, New Jersey.\n27\nSlingo, J., Bates, K., Nikiforakis, N., Piggott, M., Roberts, M., Sha\ufb00rey, L., Stevens, L.,\nVidale, P. L. and Weller, H. (2009) Developing the next-generation climate system models:\nchallenges and achievements. Philos. T. R. Soc. A, 367, 815-831.\nSommariva, A. and Vianello, M. (2006) Numerical cubature on scattered data by radial basis\nfunctions. Computing, 76(3-4), 295-310.\nSpeight, A. (2009) A multilevel approach to control variates. J. Comp. Finance, 12, 1-25.\nStein, C. (1970) A bound for the error in the normal approximation to the distribution of\na sum of dependent random variables. Proc. 6th Berkeley Symp. Math. Statist. Prob. 2,\n583-602.\nSteinwart, I. and Christmann, A. (2008) Support Vector Machines. Springer, New York.\nSun, H. and Wu, Q. (2009) Application of integral operator for regularized least-square\nregression. Math. Comput. Model., 49(1), 276-285.\n28\n",
        "sentence": "",
        "context": "Edwards, T., Stephens, A., Lawrence, B. N., Pritchard, M., Chiu, P., Iwi, A., Churchill, J.,\ndel Cano Novales, C., Kettleborough, J., Roseblade, W., Selwood, P., Foster, M., Glover,\nKohlho\ufb00, K. J., Shukla, D., Lawrenz, M., Bowman, G. R., Konerding, D. E., Belov, D.,\nAltman, R. B. and Pande, V. S. (2014) Cloud-based simulations on Google Exacycle\nreveal ligand modulation of GPCR activation pathways. Nat. Chem., 6(1), 15-21.\n\u2217Address for correspondence: School of Mathematical and Physical Sciences, University of Technology\nSydney, NSW 2007, Australia. E-mail: christopher.oates@uts.edu.au\n1\narXiv:1410.2392v5  [stat.ME]  2 Apr 2016"
    },
    {
        "title": "Operator variational inference",
        "author": [
            "R. Ranganath",
            "D. Tran",
            "J. Altosaar",
            "D. Blei"
        ],
        "venue": "In Advances in Neural Information Processing Systems,",
        "citeRegEx": "Ranganath et al\\.,? \\Q2016\\E",
        "shortCiteRegEx": "Ranganath et al\\.",
        "year": 2016,
        "abstract": "Variational inference is an umbrella term for algorithms which cast Bayesian\ninference as optimization. Classically, variational inference uses the\nKullback-Leibler divergence to define the optimization. Though this divergence\nhas been widely used, the resultant posterior approximation can suffer from\nundesirable statistical properties. To address this, we reexamine variational\ninference from its roots as an optimization problem. We use operators, or\nfunctions of functions, to design variational objectives. As one example, we\ndesign a variational objective with a Langevin-Stein operator. We develop a\nblack box algorithm, operator variational inference (OPVI), for optimizing any\noperator objective. Importantly, operators enable us to make explicit the\nstatistical and computational tradeoffs for variational inference. We can\ncharacterize different properties of variational objectives, such as objectives\nthat admit data subsampling---allowing inference to scale to massive data---as\nwell as objectives that admit variational programs---a rich class of posterior\napproximations that does not require a tractable density. We illustrate the\nbenefits of OPVI on a mixture model and a generative model of images.",
        "full_text": "Operator Variational Inference\nRajesh Ranganath\nPrinceton University\nJaan Altosaar\nPrinceton University\nDustin Tran\nColumbia University\nDavid M. Blei\nColumbia University\nAbstract\nVariational inference is an umbrella term for algorithms which cast Bayesian infer-\nence as optimization. Classically, variational inference uses the Kullback-Leibler\ndivergence to de\ufb01ne the optimization. Though this divergence has been widely\nused, the resultant posterior approximation can su\ufb00er from undesirable statistical\nproperties. To address this, we reexamine variational inference from its roots as\nan optimization problem. We use operators, or functions of functions, to design\nvariational objectives. As one example, we design a variational objective with a\nLangevin-Stein operator. We develop a black box algorithm, operator variational\ninference (opvi), for optimizing any operator objective. Importantly, operators en-\nable us to make explicit the statistical and computational tradeo\ufb00s for variational\ninference. We can characterize di\ufb00erent properties of variational objectives, such\nas objectives that admit data subsampling\u2014allowing inference to scale to massive\ndata\u2014as well as objectives that admit variational programs\u2014a rich class of pos-\nterior approximations that does not require a tractable density. We illustrate the\nbene\ufb01ts of opvi on a mixture model and a generative model of images.\n1\nIntroduction\nVariational inference is an umbrella term for algorithms that cast Bayesian inference as optimiza-\ntion [12]. Originally developed in the 1990s, recent advances in variational inference have scaled\nBayesian computation to massive data [9], provided black box strategies for generic inference in\nmany models [23], and enabled more accurate approximations of a model\u2019s posterior without sac-\nri\ufb01cing e\ufb03ciency [25, 24]. These innovations have both scaled Bayesian analysis and removed the\nanalytic burdens that have traditionally taxed its practice.\nGiven a model of latent and observed variables p(x, z), variational inference posits a family of dis-\ntributions over its latent variables and then \ufb01nds the member of that family closest to the posterior,\np(z | x). This is typically formalized as minimizing a Kullback-Leibler (kl) divergence from the\napproximating family q(\u00b7) to the posterior p(\u00b7). However, while the kl(q \u2225p) objective o\ufb00ers many\nbene\ufb01cial computational properties, it is ultimately designed for convenience; it sacri\ufb01ces many de-\nsirable statistical properties of the resultant approximation.\nWhen optimizing kl, there are two issues with the posterior approximation that we highlight. First,\nit typically underestimates the variance of the posterior. Second, it can result in degenerate solutions\nthat zero out the probability of certain con\ufb01gurations of the latent variables. While both of these is-\nsues can be partially circumvented by using more expressive approximating families, they ultimately\nstem from the choice of the objective. Under the kl divergence, we pay a large price when q(\u00b7) is\nbig where p(\u00b7) is tiny; this price becomes in\ufb01nite when q(\u00b7) has larger support than p(\u00b7).\nIn this paper, we revisit variational inference from its core principle as an optimization problem. We\nuse operators\u2014mappings from functions to functions\u2014to design variational objectives, explicitly\ntrading o\ufb00computational properties of the optimization with statistical properties of the approxima-\ntion. We use operators to formalize the basic properties needed for variational inference algorithms.\nWe further outline how to use them to de\ufb01ne new variational objectives; as one example, we design\na variational objective using a Langevin-Stein operator.\n30th Conference on Neural Information Processing Systems (NIPS 2016), Barcelona, Spain.\narXiv:1610.09033v3  [stat.ML]  15 Mar 2018\nWe develop operator variational inference (opvi), a black box algorithm that optimizes any operator\nobjective. In the context of opvi, we show that the Langevin-Stein objective enjoys two good prop-\nerties. First, it is amenable to data subsampling, which allows inference to scale to massive data.\nSecond, it permits rich approximating families, called variational programs, which do not require\nanalytically tractable densities. This greatly expands the class of variational families and the \ufb01delity\nof the resulting approximation. (We note that the traditional kl is not amenable to using variational\nprograms.) We study opvi with the Langevin-Stein objective on a mixture model and a generative\nmodel of images.\nRelated Work. There are several threads of research in variational inference with alternative diver-\ngences. An early example is expectation propagation (ep) [18]. ep promises approximate minimiza-\ntion of the inclusive kl divergence kl(p||q) to \ufb01nd overdispersed approximations to the posterior. ep\nhinges on local minimization with respect to subsets of data and connects to work on \u03b1-divergence\nminimization [19, 8]. However, it does not have convergence guarantees and typically does not min-\nimize kl or an \u03b1-divergence because it is not a global optimization method. We note that these\ndivergences can be written as operator variational objectives, but they do not satisfy the tractability\ncriteria and thus require further approximations. Li and Turner [16] present a variant of \u03b1-divergences\nthat satisfy the full requirements of opvi. Score matching [11], a method for estimating models by\nmatching the score function of one distribution to another that can be sampled, also falls into the\nclass of objectives we develop.\nHere we show how to construct new objectives, including some not yet studied. We make explicit the\nrequirements to construct objectives for variational inference. Finally, we discuss further properties\nthat make them amenable to both scalable and \ufb02exible variational inference.\n2\nOperator Variational Objectives\nWe de\ufb01ne operator variational objectives and the conditions needed for an objective to be useful\nfor variational inference. We develop a new objective, the Langevin-Stein objective, and show how\nto place the classical kl into this class. In the next section, we develop a general algorithm for\noptimizing operator variational objectives.\n2.1\nVariational Objectives\nConsider a probabilistic model p(x, z) of data x and latent variables z. Given a data set x, approxi-\nmate Bayesian inference seeks to approximate the posterior distribution p(z | x), which is applied in\nall downstream tasks. Variational inference posits a family of approximating distributions q(z) and\noptimizes a divergence function to \ufb01nd the member of the family closest to the posterior.\nThe divergence function is the variational objective, a function of both the posterior and the ap-\nproximating distribution. Useful variational objectives hinge on two properties: \ufb01rst, optimizing the\nfunction yields a good posterior approximation; second, the problem is tractable when the posterior\ndistribution is known up to a constant.\nThe classic construction that satis\ufb01es these properties is the evidence lower bound (elbo),\nEq(z)[log p(x, z) \u2212log q(z)].\n(1)\nIt is maximized when q(z) = p(z | x) and it only depends on the posterior distribution up to a\ntractable constant, log p(x, z). The elbo has been the focus in much of the classical literature. Max-\nimizing the elbo is equivalent to minimizing the kl divergence to the posterior, and the expectations\nare analytic for a large class of models [5].\n2.2\nOperator Variational Objectives\nWe de\ufb01ne a new class of variational objectives, operator variational objectives. An operator ob-\njective has three components. The \ufb01rst component is an operator Op,q that depends on p(z | x) and\nq(z). (Recall that an operator maps functions to other functions.) The second component is a family\nof test functions F, where each f(z) \u2208F maps realizations of the latent variables to real vectors\nRd. In the objective, the operator and a function will combine in an expectation Eq(z)[(Op,q f)(z)],\ndesigned such that values close to zero indicate that q is close to p. The third component is a distance\n2\nfunction t(a) : R \u2192[0, \u221e), which is applied to the expectation so that the objective is non-negative.\n(Our example uses the square function t(a) = a2.)\nThese three components combine to form the operator variational objective. It is a non-negative\nfunction of the variational distribution,\nL(q; Op,q, F, t) = sup\nf\u2208F\nt(Eq(z)[(Op,q f)(z)]).\n(2)\nIntuitively, it is the worst-case expected value among all test functions f \u2208F. Operator variational\ninference seeks to minimize this objective with respect to the variational family q \u2208Q.\nWe use operator objectives for posterior inference. This requires two conditions on the operator and\nfunction family.\n1. Closeness. The minimum of the variational objective is at the posterior, q(z) = p(z | x). We\nmeet this condition by requiring that Ep(z | x)[(Op,p f)(z)] = 0 for all f \u2208F. Thus, optimizing\nthe objective will produce p(z | x) if it is the only member of Q with zero expectation (otherwise\nit will produce a distribution in the equivalence class: q \u2208Q with zero expectation). In practice,\nthe minimum will be the closest member of Q to p(z | x).\n2. Tractability. We can calculate the variational objective up to a constant without involving the\nexact posterior p(z | x). In other words, we do not require calculating the normalizing constant\nof the posterior, which is typically intractable. We meet this condition by requiring that the oper-\nator Op,q\u2014originally in terms of p(z | x) and q(z)\u2014can be written in terms of p(x, z) and q(z).\nTractability also imposes conditions on F: it must be feasible to \ufb01nd the supremum. Below, we\nsatisfy this by de\ufb01ning a parametric family for F that is amenable to stochastic optimization.\nEq.2 and the two conditions provide a mechanism to design meaningful variational objectives for\nposterior inference. Operator variational objectives try to match expectations with respect to q(z) to\nthose with respect to p(z | x).\n2.3\nUnderstanding Operator Variational Objectives\nConsider operators where Eq(z)[(Op,q f)(z)] only takes positive values. In this case, distance to zero\ncan be measured with the identity t(a) = a, so tractability implies the operator need only be known\nup to a constant. This family includes tractable forms of familiar divergences like the kl divergence\n(elbo), R\u00e9nyi\u2019s \u03b1-divergence [16], and the \u03c7-divergence [21].\nWhen the expectation can take positive or negative values, operator variational objectives are closely\nrelated to Stein divergences [2]. Consider a family of scalar test functions F\u2217that have expectation\nzero with respect to the posterior, Ep(z | x)[f \u2217(z)] = 0. Using this family, a Stein divergence is\nDStein(p, q) = sup\nf \u2217\u2208F\u2217|Eq(z)[f \u2217(z)] \u2212Ep(z | x)[f \u2217(z)]|.\nNow recall the operator objective of Eq.2. The closeness condition implies that\nL(q; Op,q, F, t) = sup\nf\u2208F\nt(Eq(z)[(Op,q f)(z)] \u2212Ep(z | x)[(Op,p f)(z)]).\nIn other words, operators with positive or negative expectations lead to Stein divergences with a more\ngeneralized notion of distance.\n2.4\nLangevin-Stein Operator Variational Objective\nWe developed the operator variational objective. It is a class of tractable objectives, each of which\ncan be optimized to yield an approximation to the posterior. An operator variational objective is\nbuilt from an operator, function class, and distance function to zero. We now use this construction\nto design a new type of variational objective.\nAn operator objective involves a class of functions that has known expectations with respect to an\nintractable distribution. There are many ways to construct such classes [1, 2]. Here, we construct an\noperator objective from the generator Stein\u2019s method applied to the Langevin di\ufb00usion.\n3\nLet \u2207\u22a4f denote the divergence of a vector-valued function f, that is, the sum of its individual gradi-\nents. Applying the generator method of Barbour [2] to Langevin di\ufb00usion gives the operator\n(Op\nls f)(z) = \u2207z log p(x, z)\u22a4f(z) + \u2207\u22a4f.\n(3)\nWe call this the Langevin-Stein (ls) operator. See also Mira et al. [20], Oates et al. [22], Gorham\nand Mackey [7] for related derivations. We obtain the corresponding variational objective by using\nthe squared distance function and substituting Eq.3 into Eq.2,\nL(q; Op\nls, F) = sup\nf\u2208F\n(Eq[\u2207z log p(x, z)\u22a4f(z) + \u2207\u22a4f])2.\n(4)\nThe ls operator satis\ufb01es both conditions. First, it satis\ufb01es closeness because it has expectation zero\nunder the posterior (Appendix A) and its unique minimizer is the posterior (Appendix B). Second, it\nis tractable because it requires only the joint distribution. The functions f will also be a parametric\nfamily, which we detail later.\nAdditionally, while the kl divergence \ufb01nds variational distributions that underestimate the variance,\nthe ls objective does not su\ufb00er from that pathology. The reason is that kl is in\ufb01nite when the support\nof q is larger than p; here this is not the case.\nWe provided one example of a variational objectives using operators, which is speci\ufb01c to continu-\nous variables. In general, operator objectives are not limited to continuous variables; Appendix C\ndescribes an operator for discrete variables.\n2.5\nThe KL Divergence as an Operator Variational Objective\nFinally, we demonstrate how classical variational methods fall inside the operator family. For exam-\nple, traditional variational inference minimizes the kl divergence from an approximating family to\nthe posterior [12]. This can be construed as an operator variational objective,\n(Op,q\nKL f)(z) = log q(z) \u2212log p(z|x)\n\u2200f \u2208F.\n(5)\nThis operator does not use the family of functions\u2014it trivially maps all functions f to the same\nfunction. Further, because kl is strictly positive, we use the identity distance t(a) = a.\nThe operator satis\ufb01es both conditions. It satis\ufb01es closeness because KL(p||p) = 0. It satis\ufb01es\ntractability because it can be computed up to a constant when used in the operator objective of Eq.2.\nTractability comes from the fact that log p(z | x) = log p(z, x) \u2212log p(x).\n3\nOperator Variational Inference\nWe described operator variational objectives, a broad class of objectives for variational inference. We\nnow examine how it can be optimized. We develop a black box algorithm [32, 23] based on Monte\nCarlo estimation and stochastic optimization. Our algorithm applies to a general class of models and\nany operator objective.\nMinimizing the operator objective involves two optimizations: minimizing the objective with respect\nto the approximating family Q and maximizing the objective with respect to the function class F\n(which is part of the objective).\nWe index the family Q with variational parameters \u03bb and require that it satis\ufb01es properties typically\nassumed by black box methods [23]: the variational distribution q(z; \u03bb) has a known and tractable\ndensity; we can sample from q(z; \u03bb); and we can tractably compute the score function \u2207\u03bb log q(z; \u03bb).\nWe index the function class F with parameters \u03b8, and require that f\u03b8(\u00b7) is di\ufb00erentiable. In the\nexperiments, we use neural networks, which are \ufb02exible enough to approximate a general family of\ntest functions [10].\nGiven parameterizations of the variational family and test family, operator variational inference\n(opvi) seeks to solve a minimax problem,\n\u03bb\u2217= inf\n\u03bb sup\n\u03b8\nt(E\u03bb[(Op,qf\u03b8)(z)]).\n(6)\nWe will use stochastic optimization [27, 14]. In principle, we can \ufb01nd stochastic gradients of \u03bb\nby rewriting the objective in terms of the optimized value of \u03b8, \u03b8\u2217(\u03bb). In practice, however, we\n4\nAlgorithm 1: Operator variational inference\nInput : Model log p(x, z), variational approximation q(z; \u03bb)\nOutput\n:\nVariational parameters \u03bb\nInitialize \u03bb and \u03b8 randomly.\nwhile not converged do\nCompute unbiased estimates of \u2207\u03bbL\u03b8 from Eq.7.\nCompute unbiased esimates of \u2207\u03b8L\u03bb from Eq.8.\nUpdate \u03bb, \u03b8 with unbiased stochastic gradients.\nend\nsimultaneously solve the maximization and minimization. Though computationally bene\ufb01cial, this\nproduces saddle points. In our experiments we found it to be stable enough. We derive gradients for\nthe variational parameters \u03bb and test function parameters \u03b8. (We \ufb01x the distance function to be the\nsquare t(a) = a2; the identity t(a) = a also readily applies.)\nGradient with respect to \u03bb. For a \ufb01xed test function with parameters \u03b8, denote the objective\nL\u03b8 = t(E\u03bb[(Op,q f\u03b8)(z)]).\nThe gradient with respect to variational parameters \u03bb is\n\u2207\u03bbL\u03b8 = 2 E\u03bb[(Op,q f\u03b8)(z)] \u2207\u03bbE\u03bb[(Op,q f\u03b8)(z)].\nNow write the second expectation with the score function gradient [23]. This gradient is\n\u2207\u03bbL\u03b8 = 2 E\u03bb[(Op,q f\u03b8)(z)] E\u03bb[\u2207\u03bb log q(z; \u03bb)(Op,q f\u03b8)(z) + \u2207\u03bb(Op,q f\u03b8)(z)].\n(7)\nEq.7 lets us calculate unbiased stochastic gradients. We \ufb01rst generate two sets of independent sam-\nples from q; we then form Monte Carlo estimates of the \ufb01rst and second expectations. For the second\nexpectation, we can use the variance reduction techniques developed for black box variational infer-\nence, such as Rao-Blackwellization [23].\nWe described the score gradient because it is general. An alternative is to use the reparameterization\ngradient for the second expectation [13, 26]. It requires that the operator be di\ufb00erentiable with respect\nto z and that samples from q can be drawn as a transformation r of a parameter-free noise source \u03f5,\nz = r(\u03f5, \u03bb). In our experiments, we use the reparameterization gradient.\nGradient with respect to \u03b8. Mirroring the notation above, the operator objective for \ufb01xed varia-\ntional \u03bb is\nL\u03bb = t(E\u03bb[(Op,q f\u03b8)(z)]).\nThe gradient with respect to test function parameters \u03b8 is\n\u2207\u03b8L\u03bb = 2 E\u03bb[(Op,qf\u03b8)(z)] E\u03bb[\u2207\u03b8Op,q f\u03b8(z)].\n(8)\nAgain, we can construct unbiased stochastic gradients with two sets of Monte Carlo estimates. Note\nthat gradients for the test function do not require score gradients (or reparameterization gradients)\nbecause the expectation does not depend on \u03b8.\nAlgorithm. Algorithm 1 outlines opvi. We simultaneously minimize the variational objective with\nrespect to the variational family q\u03bb while maximizing it with respect to the function class f\u03b8. Given\na model, operator, and function class parameterization, we can use automatic di\ufb00erentiation to calcu-\nlate the necessary gradients [3]. Provided the operator does not require model-speci\ufb01c computation,\nthis algorithm satis\ufb01es the black box criteria.\n3.1\nData Subsampling and opvi\nWith stochastic optimization, data subsampling scales up traditional variational inference to massive\ndata [9, 30]. The idea is to calculate noisy gradients by repeatedly subsampling from the data set,\nwithout needing to pass through the entire data set for each gradient.\n5\nAn as illustration, consider hierarchical models. Hierarchical models consist of global latent vari-\nables \u03b2 that are shared across data points and local latent variables zi each of which is associated to\na data point xi. The model\u2019s log joint density is\nlog p(x1:n, z1:n, \u03b2) = log p(\u03b2) +\nn\nX\ni=1\nh\nlog p(xi | zi, \u03b2) + log p(zi | \u03b2)\ni\n.\nHo\ufb00man et al. [9] calculate unbiased estimates of the log joint density (and its gradient) by subsam-\npling data and appropriately scaling the sum.\nWe can characterize whether opvi with a particular operator supports data subsampling. opvi relies\non evaluating the operator and its gradient at di\ufb00erent realizations of the latent variables (Eq.7 and\nEq.8). Thus we can subsample data to calculate estimates of the operator when it derives from\nlinear operators of the log density, such as di\ufb00erentiation and the identity. This follows as a linear\noperator of sums is a sum of linear operators, so the gradients in Eq.7 and Eq.8 decompose into\na sum. The Langevin-Stein and kl operator are both linear in the log density; both support data\nsubsampling.\n3.2\nVariational Programs\nGiven an operator and variational family, Algorithm 1 optimizes the corresponding operator objec-\ntive. Certain operators require the density of q. For example, the kl operator (Eq.5) requires its\nlog density. This potentially limits the construction of rich variational approximations for which the\ndensity of q is di\ufb03cult to compute.1\nSome operators, however, do not depend on having a analytic density; the Langevin-Stein (ls) op-\nerator (Eq.3) is an example. These operators can be used with a much richer class of variational\napproximations, those that can be sampled from but might not have analytically tractable densities.\nWe call such approximating families variational programs.\nInference with a variational program requires the family to be reparameterizable [13, 26]. (Otherwise\nwe need to use the score function, which requires the derivative of the density.) A reparameteriz-\nable variational program consists of a parametric deterministic transformation R of random noise \u03f5.\nFormally, let\n\u03f5 \u223cNormal(0, 1),\nz = R(\u03f5; \u03bb).\n(9)\nThis generates samples for z, is di\ufb00erentiable with respect to \u03bb, and its density may be intractable. For\noperators that do not require the density of q, it can be used as a powerful variational approximation.\nThis is in contrast to the standard Kullback-Leibler (kl) operator.\nAs an example, consider the following variational program for a one-dimensional random variable.\nLet \u03bbi denote the ith dimension of \u03bb and make the corresponding de\ufb01nition for \u03f5:\nz = (\u03f53 > 0)R(\u03f51; \u03bb1) \u2212(\u03f53 \u22640)R(\u03f52; \u03bb2).\n(10)\nWhen R outputs positive values, this separates the parametrization of the density to the positive\nand negative halves of the reals; its density is generally intractable. In Section 4, we will use this\ndistribution as a variational approximation.\nEq.9 contains many densities when the function class R can approximate arbitrary continuous func-\ntions. We state it formally.\nTheorem 1. Consider a posterior distribution p(z | x) with a \ufb01nite number of latent variables and\ncontinuous quantile function. Assume the operator variational objective has a unique root at the\nposterior p(z | x) and that R can approximate continuous functions. Then there exists a sequence\nof parameters \u03bb1, \u03bb2 . . . , in the variational program, such that the operator variational objective\nconverges to 0, and thus q converges in distribution to p(z | x).\nThis theorem says that we can use variational programs with an appropriate q-independent operator\nto approximate continuous distributions. The proof is in Appendix D.\n1It is possible to construct rich approximating families with kl(q||p), but this requires the introduction of\nan auxiliary distribution [17].\n6\n4\nEmpirical Study\nWe evaluate operator variational inference on a mixture of Gaussians, comparing di\ufb00erent choices\nin the objective. We then study logistic factor analysis for images.\n4.1\nMixture of Gaussians\nConsider\na\none-dimensional\nmixture\nof\nGaussians\nas\nthe\nposterior\nof\ninterest,\np(z)\n=\n1\n2Normal(z; \u22123, 1) +\n1\n2Normal(z; 3, 1).\nThe posterior contains multiple modes.\nWe seek to approximate it with three variational objectives: Kullback-Leibler (kl) with a Gaussian\napproximating family, Langevin-Stein (ls) with a Gaussian approximating family, and ls with a\nvariational program.\n\u00005\n0\n5\nValue of Latent Variable z\nKL\nTruth\n\u00005\n0\n5\nValue of Latent Variable z\nLangevin-Stein\nTruth\n\u00005\n0\n5\nValue of Latent Variable z\nVariational Program\nTruth\nFigure 1: The true posterior is a mixture of two Gaussians, in green. We approximate it with a Gaus-\nsian using two operators (in blue). The density on the far right is a variational program given in Eq.10\nand using the Langevin-Stein operator; it approximates the truth well. The density of the variational\nprogram is intractable. We plot a histogram of its samples and compare this to the histogram of the\ntrue posterior.\nFigure 1 displays the posterior approximations. We \ufb01nd that the kl divergence and ls divergence\nchoose a single mode and have slightly di\ufb00erent variances. These operators do not produce good\nresults because a single Gaussian is a poor approximation to the mixture. The remaining distribution\nin Figure 1 comes from the toy variational program described by Eq.10 with the ls operator. Because\nthis program captures di\ufb00erent distributions for the positive and negative half of the real line, it is\nable to capture the posterior.\nIn general, the choice of an objective balances statistical and computational properties of variational\ninference. We highlight one tradeo\ufb00: the ls objective admits the use of a variational program;\nhowever, the objective is more di\ufb03cult to optimize than the kl.\n4.2\nLogistic Factor Analysis\nLogistic factor analysis models binary vectors xi with a matrix of parameters W and biases b,\nzi \u223cNormal(0, 1)\nxi,k \u223cBernoulli(\u03c3(w\u22a4\nk zi + bk)),\nwhere zi has \ufb01xed dimension K and \u03c3 is the sigmoid function. This model captures correlations of\nthe entries in xi through W.\nWe apply logistic factor analysis to analyze the binarized MNIST data set [28], which contains 28x28\nbinary pixel images of handwritten digits. (We set the latent dimensionality to 10.) We \ufb01x the model\nparameters to those learned with variational expectation-maximization using the kl divergence, and\nfocus on comparing posterior inferences.\nWe compare the kl operator to the ls operator and study two choices of variational models: a fully\nfactorized Gaussian distribution and a variational program. The variational program generates sam-\nples by transforming a K-dimensional standard normal input with a two-layer neural network, using\nrecti\ufb01ed linear activation functions and a hidden size of twice the latent dimensionality. Formally,\n7\nInference method\nCompleted data log-likelihood\nMean-\ufb01eld Gaussian + kl\n-59.3\nMean-\ufb01eld Gaussian + ls\n-75.3\nVariational Program + ls\n-58.9\nTable 1: Benchmarks on logistic factor analysis for binarized MNIST. The same variational approx-\nimation with ls performs worse than kl on likelihood performance. The variational program with\nls performs better without directly optimizing for likelihoods.\nthe variational program we use generates samples of z as follows:\nz0 \u223cNormal(0, I)\nh0 = ReLU(Wq\n0\n\u22a4z0 + bq\n0)\nh1 = ReLU(Wq\n1\n\u22a4h0 + bq\n1)\nz = Wq\n2\n\u22a4h1 + bq\n2.\nThe variational parameters are the weights Wq and biases bq. For f, we use a three-layer neural net-\nwork with the same hidden size as the variational program and hyperbolic tangent activations where\nunit activations were bounded to have norm two. Bounding the unit norm bounds the divergence.\nWe used the Adam optimizer [? ] with learning rates 2\u00d710\u22124 for f and 2\u00d710\u22125 for the variational\napproximation.\nThere is no standard for evaluating generative models and their inference algorithms [29]. Following\nRezende et al. [26], we consider a missing data problem. We remove half of the pixels in the test set\n(at random) and reconstruct them from a \ufb01tted posterior predictive distribution. Table 1 summarizes\nthe results on 100 test images; we report the log-likelihood of the completed image. ls with the\nvariational program performs best. It is followed by kl and the simpler ls inference. The ls performs\nbetter than kl even though the model parameters were learned with kl.\n5\nSummary\nWe present operator variational objectives, a broad yet tractable class of optimization problems for\napproximating posterior distributions. Operator objectives are built from an operator, a family of\ntest functions, and a distance function. We outline the connection between operator objectives and\nexisting divergences such as the KL divergence, and develop a new variational objective using the\nLangevin-Stein operator. In general, operator objectives produce new ways of posing variational\ninference.\nGiven an operator objective, we develop a black box algorithm for optimizing it and show which\noperators allow scalable optimization through data subsampling. Further, unlike the popular evidence\nlower bound, not all operators explicitly depend on the approximating density. This permits \ufb02exible\napproximating families, called variational programs, where the distributional form is not tractable.\nWe demonstrate this approach on a mixture model and a factor model of images.\nThere are several possible avenues for future directions such as developing new variational objectives,\nadversarially learning [6] model parameters with operators, and learning model parameters with\noperator variational objectives.\nAcknowledgments.\nThis work is supported by NSF IIS-1247664, ONR N00014-11-1-0651,\nDARPA FA8750-14-2-0009, DARPA N66001-15-C-4032, Adobe, NSERC PGS-D, Porter Ogden\nJacobus Fellowship, Seibel Foundation, and the Sloan Foundation. The authors would like to thank\nDawen Liang, Ben Poole, Stephan Mandt, Kevin Murphy, Christian Naesseth, and the anonymous\nreviews for their helpful feedback and comments.\nReferences\n[1] Assaraf, R. and Ca\ufb00arel, M. (1999). Zero-variance principle for monte carlo algorithms. In Phys. Rev. Let.\n[2] Barbour, A. D. (1988). Stein\u2019s method and poisson process convergence. Journal of Applied Probability.\n8\n[3] Carpenter, B., Ho\ufb00man, M. D., Brubaker, M., Lee, D., Li, P., and Betancourt, M. (2015). The Stan Math\nLibrary: Reverse-mode automatic di\ufb00erentiation in C++. arXiv preprint arXiv:1509.07164.\n[4] Cinlar, E. (2011). Probability and Stochastics. Springer.\n[5] Ghahramani, Z. and Beal, M. (2001). Propagation algorithms for variational Bayesian learning. In NIPS\n13, pages 507\u2013513.\n[6] Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., and Bengio,\nY. (2014). Generative adversarial nets. In Neural Information Processing Systems.\n[7] Gorham, J. and Mackey, L. (2015). Measuring sample quality with stein\u2019s method. In Advances in Neural\nInformation Processing Systems, pages 226\u2013234.\n[8] Hern\u00e1ndez-Lobato, J. M., Li, Y., Rowland, M., Hern\u00e1ndez-Lobato, D., Bui, T., and Turner, R. E. (2015).\nBlack-box \u03b1-divergence Minimization. arXiv.org.\n[9] Ho\ufb00man, M., Blei, D., Wang, C., and Paisley, J. (2013). Stochastic variational inference. Journal of\nMachine Learning Research, 14(1303\u20131347).\n[10] Hornik, K., Stinchcombe, M., and White, H. (1989). Multilayer feedforward networks are universal ap-\nproximators. Neural networks, 2(5):359\u2013366.\n[11] Hyv\u00e4rinen, A. (2005). Estimation of non-normalized statistical models by score matching. Journal of\nMachine Learning Research, 6(Apr):695\u2013709.\n[12] Jordan, M., Ghahramani, Z., Jaakkola, T., and Saul, L. (1999). Introduction to variational methods for\ngraphical models. Machine Learning, 37:183\u2013233.\n[13] Kingma, D. and Welling, M. (2014). Auto-encoding variational bayes. In (ICLR).\n[14] Kushner, H. and Yin, G. (1997). Stochastic Approximation Algorithms and Applications. Springer New\nYork.\n[15] Ley, C. and Swan, Y. (2011). Discrete stein characterizations and discrete information distances. arXiv\npreprint arXiv:1201.0143.\n[16] Li, Y. and Turner, R. E. (2016). R\u00e9nyi divergence variational inference. arXiv preprint arXiv:1602.02311.\n[17] Maal\u00f8e, L., S\u00f8nderby, C. K., S\u00f8nderby, S. K., and Winther, O. (2016). Auxiliary deep generative models.\narXiv preprint arXiv:1602.05473.\n[18] Minka, T. P. (2001). Expectation propagation for approximate Bayesian inference. In UAI.\n[19] Minka, T. P. (2004). Power EP. Technical report, Microsoft Research, Cambridge.\n[20] Mira, A., Solgi, R., and Imparato, D. (2013). Zero variance markov chain monte carlo for bayesian esti-\nmators. Statistics and Computing, pages 1\u201310.\n[21] Nielsen, F. and Nock, R. (2013). On the chi square and higher-order chi distances for approximating\nf-divergences. arXiv preprint arXiv:1309.3029.\n[22] Oates, C. J., Girolami, M., and Chopin, N. (2014). Control functionals for monte carlo integration. arXiv\npreprint arXiv:1410.2392.\n[23] Ranganath, R., Gerrish, S., and Blei, D. (2014). Black Box Variational Inference. In AISTATS.\n[24] Ranganath, R., Tran, D., and Blei, D. M. (2016). Hierarchical variational models. In International Con-\nference on Machine Learning.\n[25] Rezende, D. J. and Mohamed, S. (2015). Variational inference with normalizing \ufb02ows. In Proceedings of\nthe 31st International Conference on Machine Learning (ICML-15).\n[26] Rezende, D. J., Mohamed, S., and Wierstra, D. (2014). Stochastic backpropagation and approximate\ninference in deep generative models. In International Conference on Machine Learning.\n[27] Robbins, H. and Monro, S. (1951). A stochastic approximation method. The Annals of Mathematical\nStatistics, 22(3):pp. 400\u2013407.\n[28] Salakhutdinov, R. and Murray, I. (2008). On the quantitative analysis of deep belief networks. In Interna-\ntional Conference on Machine Learning.\n[29] Theis, L., van den Oord, A., and Bethge, M. (2016). A note on the evaluation of generative models. In\nInternational Conference on Learning Representations.\n9\n[30] Titsias, M. and L\u00e1zaro-Gredilla, M. (2014). Doubly stochastic variational bayes for non-conjugate infer-\nence. In Proceedings of the 31st International Conference on Machine Learning (ICML-14), pages 1971\u2013\n1979.\n[31] Tran, D., Ranganath, R., and Blei, D. M. (2016). The variational Gaussian process. In ICLR.\n[32] Wingate, D. and Weber, T. (2013). Automated variational inference in probabilistic programming. ArXiv\ne-prints.\nA\nTechnical Conditions for Langevin-Stein Operators\nHere we establish the conditions needed on the function class F or the posterior distribution short-\nhanded p for the operators to have expectation zero for all f \u2208F. W derive properties using inte-\ngration by parts for supports that are bounded open sets. Then we extend the result to unbounded\nsupports using limits. We start with the Langevin-Stein operator. Let S be the set over which we\nintegrate and let B be its boundary. Let v be the unit normal to the surface B, and vi be the ith\ncomponent of the surface normal (which is d dimensional). Then we have that\nZ\nS\np(Op\nLS f)dS =\nZ\nS\np\u2207z log p\u22a4f + p\u2207\u22a4fdS\n=\nd\nX\ni=1\nZ\nS\n\u2202\n\u2202zi\n[p]fi + p \u2202\n\u2202zi\n[fi]dS\n=\nd\nX\ni=1\nZ\nS\n\u2202\n\u2202zi\n[p]fidS +\nZ\nB\nfipvidB \u2212\nZ\nS\n\u2202\n\u2202zi\n[p]fidS\n=\nZ\nB\nv\u22a4fpdB.\nA su\ufb03cient condition for this expectation to be zero is that either p goes to zero at its boundary or\nthat the vector \ufb01eld f is zero at the boundary.\nFor unbounded sets, the result can be written as a limit for a sequence of increasing sets Sn \u2192S\nand a set of boundaries Bn \u2192B using the dominated convergence theorem [4]. To use dominated\nconvergence, we establish absolute integrability. Su\ufb03cient conditions for absolute integrability of\nthe Langevin-Stein operator are for the gradient of log p to be bounded and the vector \ufb01eld f and its\nderivatives to be bounded. Via dominated convergence, we get that limn\nR\nBn v\u22a4fpdB = 0 for the\nLangevin-Stein operator to have expectation zero.\nB\nCharacterizing the zeros of the Langevin-Stein Operators\nWe provide analysis on how to characterize the equivalence class of distributions de\ufb01ned as\n(Op,qf)(z) = 0. One general condition for equality in distribution comes from equality in probabil-\nity on all Borel sets. We can build functions that have expectation zero with respect to the posterior\nthat test this equality. Formally, for any Borel set A with \u03b4A being the indicator, these functions on\nA have the form:\n\u03b4A(z) \u2212\nZ\nA\np(y)dy\nWe show that if the Langevin-Stein operator satis\ufb01es L(q; Op\nLS, F) = 0, then q is equivalent to p\nin distribution. We do this by showing the above functions are in the span of Op\nLS. Expanding the\nLangevin-Stein operator we have\n(Op\nLS f) = p\u22121\u2207zp\u22a4f + \u2207\u22a4f = p\u22121\nd\nX\ni=1\n\u2202fip\n\u2202zi\n.\nSetting this equal to the desired function above yields the di\ufb00erential equation\n\u03b4A(z) \u2212\nZ\nA\np(y)dy = p\u22121(z)\nd\nX\ni=1\n\u2202fip\n\u2202zi\n(z).\n10\nTo solve this, set fi = 0 for all but i = 1. This yields\n\u03b4A(z) \u2212\nZ\nA\np(y)dy = p\u22121(z)\u2202f1p\n\u2202z1\n(z),\nwhich is an ordinary di\ufb00erential equation with solution for f1\nf A\n1 (z) =\n1\np(z)\nz1\nZ\n\u2212\u221e\np(a, z2...d)\n\u0012\n\u03b4A(a, z2...d) \u2212\nZ\nA\np(y)dy\n\u0013\nda.\nThis function is di\ufb00erentiable with respect to z1, so this gives the desired result. Plugging the function\nback into the operator variational objective gives\nEq\n\u0014\n\u03b4A(z) \u2212\nZ\nA\np(y)dy\n\u0015\n= 0 \u21d0\u21d2Eq[\u03b4A(z)] = Ep[\u03b4A(z)],\nfor all Borel measurable A. This implies the induced distance captures total variation.\nC\nOperators for Discrete Variables\nSome operators based on Stein\u2019s method are applicable only for latent variables in a continuous\nspace. There are Stein operators that work with discrete variables [1, 15]. We present one amenable to\noperator variational objectives based on a discrete analogue to the Langevin-Stein operator developed\nin [15]. For simplicity, consider a one-dimensional discrete posterior with support {0, ..., c}. Let f\nbe a function such that f(0) = 0, then an operator can be de\ufb01ned as\n(Op\ndiscrete f)(z) = f(z + 1)p(z + 1, x) \u2212f(z)p(z, x)\np(z, x)\n.\nSince the expectation of this operator with respect to the posterior p(z | x) is a telescoping sum with\nboth endpoints 0, it has expectation zero.\nThis relates to the Langevin-Stein operator in the following. The Langevin-Stein operator in one\ndimension can be written as\n(Op\nLS f) =\nd\ndz[fp]\np\n.\nThis operator is the discrete analogue as the di\ufb00erential is replaced by a discrete di\ufb00erence. We can\nextend this operator to multiple dimensions by an ordered indexing. For example, binary numbers\nof length n would work for n binary latent variables.\nD\nProof of Universal Representations\nConsider the optimal form of R such that transformations of standard normal draws are equal in\ndistribution to exact draws from the posterior. This means\nR(\u03f5; \u03bb) = P \u22121(\u03a6(\u03f5)),\nwhere \u03a6(\u03f5) squashes the draw from a standard normal such that it is equal in distribution to a uniform\nrandom variable. The posterior\u2019s inverse cumulative distribution function P \u22121 is applied to the\nuniform draws. The transformed samples are now equivalent to exact samples from the posterior. For\na rich-enough parameterization of R, we may hope to su\ufb03ciently approximate this function.\nIndeed, as in the universal approximation theorem of Tran et al. [31] there exists a sequence of pa-\nrameters {\u03bb1, \u03bb2, . . .} such that the operator variational objective goes to zero, but the function class\nis no longer limited to local interpolation. Universal approximators like neural networks [10] also\nwork. Further, under the assumption that p is the unique root and by satisfying the conditions de-\nscribed in Section B for equality in distribution, this implies that the variational program given by\ndrawing \u03f5 \u223cN(0, I) and applying R(\u03f5) converges in distribution to p(z | x).\n11\n",
        "sentence": "",
        "context": "Dawen Liang, Ben Poole, Stephan Mandt, Kevin Murphy, Christian Naesseth, and the anonymous\nreviews for their helpful feedback and comments.\nReferences\nDARPA FA8750-14-2-0009, DARPA N66001-15-C-4032, Adobe, NSERC PGS-D, Porter Ogden\nJacobus Fellowship, Seibel Foundation, and the Sloan Foundation. The authors would like to thank\n13, pages 507\u2013513.\n[6] Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., and Bengio,\nY. (2014). Generative adversarial nets. In Neural Information Processing Systems."
    },
    {
        "title": "Kernel distribution embeddings: Universal kernels, characteristic kernels and kernel metrics on distributions",
        "author": [
            "C. Simon-Gabriel",
            "B. Sch\u00f6lkopf"
        ],
        "venue": "arXiv preprint arXiv:1604.05251,",
        "citeRegEx": "Simon.Gabriel and Sch\u00f6lkopf,? \\Q2016\\E",
        "shortCiteRegEx": "Simon.Gabriel and Sch\u00f6lkopf",
        "year": 2016,
        "abstract": "Kernel mean embeddings have recently attracted the attention of the machine\nlearning community. They map measures $\\mu$ from some set $M$ to functions in a\nreproducing kernel Hilbert space (RKHS) with kernel $k$. The RKHS distance of\ntwo mapped measures is a semi-metric $d_k$ over $M$. We study three questions.\n(I) For a given kernel, what sets $M$ can be embedded? (II) When is the\nembedding injective over $M$ (in which case $d_k$ is a metric)? (III) How does\nthe $d_k$-induced topology compare to other topologies on $M$? The existing\nmachine learning literature has addressed these questions in cases where $M$ is\n(a subset of) the finite regular Borel measures. We unify, improve and\ngeneralise those results. Our approach naturally leads to continuous and\npossibly even injective embeddings of (Schwartz-) distributions, i.e.,\ngeneralised measures, but the reader is free to focus on measures only. In\nparticular, we systemise and extend various (partly known) equivalences between\ndifferent notions of universal, characteristic and strictly positive definite\nkernels, and show that on an underlying locally compact Hausdorff space, $d_k$\nmetrises the weak convergence of probability measures if and only if $k$ is\ncontinuous and characteristic.",
        "full_text": "arXiv:1604.05251v2  [stat.ML]  17 Dec 2019\nKernel Distribution Embeddings:\nUniversal Kernels, Characteristic Kernels and Kernel Metrics on\nDistributions\nCarl-Johann Simon-Gabriel\u22171 and Bernhard Sch\u00a8olkopf\u20201\n1Max Planck Institute for Intelligent Systems, Department of Empirical Inference,\nSpemanstra\u00dfe 38, 72076 T\u00a8ubingen, Germany\nAbstract\nKernel mean embeddings have recently attracted the attention of the machine learning\ncommunity. They map measures \u00b5 from some set M to functions in a reproducing kernel\nHilbert space (RKHS) with kernel k. The RKHS distance of two mapped measures is a semi-\nmetric dk over M. We study three questions.\n(I) For a given kernel, what sets M can be embedded?\n(II) When is the embedding injective over M (in which case dk is a metric)?\n(III) How does the dk-induced topology compare to other topologies on M?\nThe existing machine learning literature has addressed these questions in cases where M is\n(a subset of) the \ufb01nite regular Borel measures. We unify, improve and generalise those re-\nsults. Our approach naturally leads to continuous and possibly even injective embeddings of\n(Schwartz-)distributions, i.e., generalised measures, but the reader is free to focus on measures\nonly.\nIn particular, we systemise and extend various (partly known) equivalences between\ndi\ufb00erent notions of universal, characteristic and strictly positive de\ufb01nite kernels, and show\nthat on an underlying locally compact Hausdor\ufb00space, dk metrises the weak convergence of\nprobability measures if and only if k is continuous and characteristic.\nKeywords:\nkernel\nmean\nembedding,\nuniversal\nkernel,\ncharacteristic\nkernel,\nSchwartz-\ndistributions, kernel metrics on distributions, metrisation of the weak topology\nThe following preprint is an older and longer version of the paper with same title published in\nJMLR (Simon-Gabriel and Sch\u00a8olkopf, 2018). The JMLR version is completely restructured and\nhopefully easier to follow. We leave this older version online for citation consistency and some\nadditional content, but we advise to start with the JMLR version and refer to this older version\nonly if needed. Also, please note that the proofs of Theorems 37 and 40 are \ufb02awed and have not\nbeen \ufb01xed.\n1\nIntroduction\nDuring the past two decades, reproducing kernel Hilbert spaces (RKHS) have risen to a major\ntool in various areas of machine learning.\n\u2217cjsimon@tuebingen.mpg.de\n\u2020bs@tuebingen.mpg.de\n1\nThey o\ufb00er a variety of nice function spaces for dimensionality-reduction, regression and clas-\nsi\ufb01cation algorithms, such as kernel PCA, kernel regression and SVMs. For those algorithms, the\nfunction space should be su\ufb03ciently large to approximate well the unknown target function, and\nits norm su\ufb03ciently strong to avoid over\ufb01tting. These two requirements lead to the question how\na given RKHS Hk compares with other well-known function spaces F. The question is two-fold.\nWhen is Hk continuously contained in F? And when is it dense in F? During the past two\ndecades, the machine learning community focused on the second, more di\ufb03cult question. Kernels\nk : X \u00d7 X \u2192C whose RKHS is dense in F were called universal, with slight variations depending\non F. The latter was usually a space of continuous functions. But we will try to \u201cnarrow the\ngap\u201d between Hk and F by choosing a smaller space F, containing for example only continuously\ndi\ufb00erentiable functions; with far-reaching consequences for kernel mean embeddings (KME).\nThese KMEs only recently caught the attention of the machine learning community. They\nembed \ufb01nite Borel measures into Hk via the map \u03a6k : \u00b5 7\u2192\nR\nX k(., x) d\u00b5(x). This gives access to\nthe RKHS\u2019s computational handiness to handle measures numerically, and led to new homogene-\nity (Gretton et al., 2007), distribution comparison (Gretton et al., 2007, 2012) and (conditional)\nindependence tests (Gretton et al., 2005a, 2008; Fukumizu et al., 2008; Gretton and Gy\u00a8or\ufb01, 2010;\nLopez-Paz et al., 2013). For those algorithms, it is interesting when \u03a6k is de\ufb01ned and injective\nover large sets M, in which case the kernel is said characteristic over M. It turned out that\nmany usual kernels are characteristic over (subsets of) \ufb01nite Borel measures (Micchelli et al.,\n2006; Fukumizu et al., 2008, 2009a,b; Sriperumbudur et al., 2008, 2010a, 2011).\nNevertheless,\nKMEs are in general not surjective, leaving \u201ca gap\u201d between M and Hk that we will try to nar-\nrow by extending \u03a6k to generalised measures, i.e. Schwartz-distributions (thereafter simply called\ndistributions, as opposed to measures).\nNarrowing the gaps between Hk and F and between M and Hk may seem like unrelated\nproblems, but they are actually linked by duality. If Hk continuously embeds into F, Corollary 9\nwill show that the KME is well-de\ufb01ned over the dual F\u2032 and Theorem 12 that the kernel is universal\nover F if and only if (i\ufb00) it is characteristic to F\u2032. For wisely chosen spaces F, F\u2032 identi\ufb01es with\na space of measures; for others, it becomes a space of distributions. Corollary 9 and Theorem 12\nthus \ufb01nally unify and systemise various known links between di\ufb00erent notions of universal and\ncharacteristic kernels catalogued in an overview paper by Sriperumbudur et al. (2011); and they\ngive a straightforward method to extend KMEs from measures to distributions! Proposition 16\ncomplements these results by noticing that kernels are characteristic to a given vector space i\ufb00\nthey are strictly positive de\ufb01nite (s.p.d.) over this space. This systemises the various connections\n(also reviewed by Sriperumbudur et al., 2011) between universal and characteristic kernels and\ndi\ufb00erent notions of s.p.d. kernels, such as conditionally s.p.d. or integrally s.p.d. kernels. Table 1\nsummarises these connections.\nUsing (integrally) strict positive de\ufb01niteness, Sriperumbudur et al. (2010a) gave handy condi-\ntions to check whether a stationary (also known as translation-invariant) kernel is characteristic.\nWe improve and extend these results to distributions. Surprisingly, it turns out that any smooth\nand stationary kernel that is characteristic to \ufb01nite measures with compact support is also char-\nacteristic to the larger space of compactly supported distributions (Proposition 42)!\nThe RKHS distance between two mapped measures (or distributions) \u00b5 and \u03bd de\ufb01nes a semi-\nmetric dk over M: dk(\u00b5, \u03bd) := \u2225\u03a6k(\u00b5) \u2212\u03a6k(\u03bd)\u2225k. It is only natural to compare it with other\ntopologies on M. Table 2 summarises these comparisons. They have important consequences.\nFor one thing, they guarantee continuity of KMEs when M is a dual equipped with its strong\ndual topology, the most common topology on duals (Proposition 43). For another, they prove\n2\nthat our extension of KMEs to distributions with compact support is the only continuous linear\nextension of usual KMEs (Proposition 47). And most importantly, they \ufb01nalise a series of recent\nresults (Sriperumbudur et al., 2010b; Sriperumbudur, 2013) by showing that on locally compact\nHausdor\ufb00spaces, dk metrises the narrow1 convergence of probability measures i\ufb00k is continuous\nand characteristic to probability measures (Theorem 55). Metrics metrising the narrow conver-\ngence are of prime importance for convergence results in probability theory. Many such metrics\nexist, for example the Dudley, L\u00b4evy-Prohorov and Wasserstein (or Kantorovich) metrics. But\ndk has numerous advantages, particularly for applications. First, it underlies various machine\nlearning algorithms, including almost all applications of KMEs, as well as kernel independent\ncomponent analysis (Bach and Jordan, 2002; Gretton et al., 2005b; Shen et al., 2009) and kernel\nbased dimensionality reduction for supervised learning (Fukumizu et al., 2004). Second, it is easy\nto estimate, even with \ufb01nite samples only, because \u2225\u03a6k(\u00b5)\u22252\nk =\nRR\nk(x, y) d\u00b5(x) d\u00af\u00b5(y) (which also\nholds for distributions!). And third, it depends only on the kernel and can thus be de\ufb01ned over\narbitrary input domains X. For further details, see introduction of Sriperumbudur et al. (2010b).\nOverall, our main contribution is to unify and \ufb01nalise many results on KMEs that were discov-\nered and used by the machine learning community during the past decade. Contrary to probability\nmeasures, which often need extra care and are given special attention in this paper, embeddings of\ndistributions are rather a byproduct of our systematic uni\ufb01cation. For generality we phrase many\nresults in terms of distributions, but readers may focus on measures only, if they systematically:\n\u22b2replace the word \u201cdistribution\u201d by \u201cmeasure\u201d;\n\u22b2set the parameter m to 0 (see notations in Section 2);\n\u22b2remember that D0\nL1 = Mf and E0 = Mc.\nSection 4.3 however will lose most of its substance, as it focuses on distributional derivatives:\nthe essence of distribution theory. As an appetiser however, let us show how these derivatives may\nappear and relate to KMEs. Consider an input space X = R, and let \u20d7\u03d5k(x) := \u03a6k(\u03b4x) = k(., x).\nWhen k is continuously di\ufb00erentiable, then \u20d7\u03d5k, as a function of x with values in Hk, is di\ufb00erentiable\nand its derivative \u2202\u20d7\u03d5k belongs to Hk (Steinwart and Christmann, 2008, Lemma 4.34). It is thus\ntempting to write\n[\u2202\u20d7\u03d5k](x) := lim\nh\u21920 \u03a6k\n\u0012\u03b4x+h \u2212\u03b4x\nh\n\u0013\n?= \u03a6k\n\u0012\nlim\nh\u21920\n\u03b4x+h \u2212\u03b4x\nh\n\u0013\n= \u03a6k(\u2212\u2202\u03b4x) = \u2212\u03a6k(\u2202\u03b4x) ,\n(1)\nwhere \u2202\u03b4x := limh\u21920\n\u03b4x\u2212h\u2212\u03b4x\nh\nwould denote the distributional derivative of \u03b4x. Thus, the function\n\u2202\u20d7\u03d5k does not seem to be the image of a measure, but the image of the \ufb01rst-order Schwartz-\ndistribution \u2212\u2202\u03b4x, which in physics is called a dipole.\nEquation (1) will be made precise in\nSection 4.3.\nThe structure of this paper roughly follows questions (I)\u2013(III) of the abstract. After \ufb01xing\nde\ufb01nitions and notations in Section 2, we de\ufb01ne the KME of distributions in Section 3. This\ngives answers to (I) and immediately yields Theorem 12 which links universal and characteristic\nkernels. To answer (II), we \ufb01rst need basic calculus rules for embedded distributions, together\nwith some results speci\ufb01c to KMEs of probability measures. Those are covered by Sections 4 and\n5 respectively. Section 6 answers (II) by giving necessary and su\ufb03cient conditions for kernels to\nbe characteristic to some distribution spaces. In particular, it shows that kernels which injectively\nembed large spaces of distributions do exist and provides examples. Section 7 addresses (III)\n1The narrow convergence is also known as the weak or weak-* convergence in probability theory. Except for\nthe abstract, we call it narrow to distinguish it from the weak (dual) topology from functional analysis.\n3\nby focusing on the induced kernel semi-metric dk, with a special emphasis on the metrisation of\nthe narrow convergence of \ufb01nite measures. Section 8 gives a brief overview of related work and\nconcludes.\n2\nDe\ufb01nitions and Notations\nMost of our notations are fairly standard. An informed reader might skip to Section 3 and refer\nto Section 2 only as needed.\nInput and output space.\nLet X be the input set of all considered kernels and functions.\nWhenever referring to di\ufb00erentiable functions or to distributions of order \u22651, we will implicitly\ntake X = \u2126\u2282Rd, where d is a strictly positive integer, R the set of real numbers, and \u2126\nan open subset of Rd. Otherwise, when referring to functions which need not be di\ufb00erentiable\nor to distributions of order 0 (i.e. measures), X will simply be a locally compact paracompact\nHausdor\ufb00set. Note that a Hausdor\ufb00set is paracompact if and only if (i\ufb00) it admits partitions of\nunity subordinate to any open cover. For many results, the paracompactness assumption will be\nsuper\ufb02uous, but it is handy to keep it for ease of discussions. We will explicitly lift this assumptions\nwhen we think it is worth it. X will be equipped with its Borel \u03c3-algebra. All considered functions,\nmeasures and distributions will take their values in C, the complex numbers.\nKernel.\nIn this paper, a kernel k : X \u00d7 X \u2192C will be a positive de\ufb01nite function, meaning\nthat for all n \u2208N, all \u03bb1, . . . , \u03bbn \u2208C, and all x1, x2, . . . xn \u2208X,\nn\nX\ni,j=1\n\u03bbik(xi, xj)\u03bbj \u22650.\nDi\ufb00erentiation.\nUnless stated otherwise, m will always designate a positive, possibly in\ufb01nite\ninteger: m \u2208N \u222a{\u221e}. Let p = (p1, p2, . . . , pd) \u2208Nd, where N is the set of non-negative integers.\nWe note |p| := Pd\ni=1 pi.\nFor a given function f : X \u2192C, we de\ufb01ne \u2202pf :=\n\u2202|p|f\n\u2202p\n1x1\u2202p\n2 x2\u00b7\u00b7\u00b7\u2202p\ndxd ,\nwhenever the right-hand-side is well de\ufb01ned. f will be said m-times continuously di\ufb00erentiable, if\nfor any p such that |p| = m, \u2202pf exists and is continuous. Similarly, for kernels k : \u2126\u00d7 \u2126\u2192C, we\nwill write \u2202(p,q)k, where p, q \u2208Nd. A kernel will be said (m, m)-times continuously di\ufb00erentiable\nif, for all p such that |p| = m, \u2202(p,p)k exists and is continuous. In particular, \u2202(p,q)k then exists\nand is continuous for any |p|, |q| \u2264m. The same notations will be used for the distributional\nderivative.\nTopological subsets.\nLet S1 and S2 be two topological sets such that S1 \u2282S2. If the canonical\nembedding of S1 into S2 is continuous, S1 is said to embed continuously into S2 or to be continuously\ncontained in S2. In that case, we write\nS1 \u0592\u2192S2\nor\nS2 \u2190\u0593 S1 .\nIn other words, S1 \u0592\u2192S2 means: S1 is contained in S2 and carries a stronger topology than the\nrelative topology induced by S2.\n4\nSpaces of functions.\nThe letter F will always designate a locally convex (loc. cv.) topological\nvector space (TVS) of functions. For (a few) reminders on loc. cv. TVSs, see Appendix D. All\nfunction spaces encountered in this paper are loc. cv. TVSs. Let m be a possibly in\ufb01nite, non-\nnegative integer, and let q \u2208R, with 1 \u2264q \u2264\u221e. Whenever it is de\ufb01ned, we note \u2225f\u2225\u221e:=\nsupx\u2208X |f(x)|.\nWe will consider the following spaces of functions over X.\nExcept Cm\nb , they\nwill always be equipped with their usual, natural topology. The reader need not know all these\ntopologies to understand the paper. They are stated here solely for completeness and rigour.\nCX\nthe space of all functions from X to C, equipped with the pointwise convergence topology.\nCm\nthe space of m-times continuously di\ufb00erentiable functions.\nIt is equipped with the\ntopology of uniform convergence on compact subsets of the functions and of their\nderivatives of order \u2264m.\nThis topology is generated by the family of semi-norms\n{\u2225.\u2225p,K | |p| \u2264m, K \u2282X, K compact}, where \u2225f\u2225p,K := maxK \u2202p|f|.\n(Cm\nb )c\nthe space of m-times continuously di\ufb00erentiable functions f, such that all their derivatives\nup to order m be bounded. The natural topology would be the uniform convergence of\nthe functions and of all their derivatives up to order m. This topology, however, will be\ntoo strong for our purposes. Instead, we will equip Cm\nb\nwith a weaker (LCv) topology\n(see Section 5.2) \u2014which we mark by the index c in (Cm\nb )c\u2014 such that the dual of (Cm\nb )c\nbe the same as the dual of Cm\n0 (see next item).\nCm\n0\nthe space of m-times continuously di\ufb00erentiable functions that vanish at in\ufb01nity, as well\nas all their derivatives of order \u2264m. Note that a function f \u2208CX is said to vanish at\nin\ufb01nity, if, for any \u01eb > 0, there exists a compact K \u2282X such that |f(X\\K)| \u2264\u01eb. The\nspace Cm\n0 is equipped with the topology of uniform convergence of the functions and of\ntheir derivatives of order \u2264m. This topology is generated by the family of semi-norms\n{\u2225.\u2225p | |p| \u2264m}, where \u2225f\u2225p := max \u2202p|f|.\nCm\nc\nthe space of m-times continuously di\ufb00erentiable functions with compact support. It will\nbe equipped with its usual limit Fr\u00b4echet topology (see Schwartz, 1978; Treves, 1967). In\nthis topology, a sequence (fn)n of functions converges to f i\ufb00\n(i) there exists a compact K \u2282X such that fn, f \u2208Cm(K) and\n(ii) fn \u2192f in Cm(K).\nLq\nwith its usual Lq-norm \u2225.\u2225Lq.\nW m,q := {f \u2208Lq | \u2200|p| \u2264m, \u2202pf \u2208Lq}\nHere, \u2202pf is the distributional derivative of f. The topology on these so-called Sobolev\nspaces is generated by the family of semi-norms f 7\u2192\u2225\u2202pf\u2225Lq.\nW m,q\n0\nthe closure of C\u221e\nc\nin W m,q. When q < \u221eand m = 0, or when q < \u221eand X = Rd, then\nit can be shown that W m,q\n0\n= W m,q. This does not hold for any arbitrary X = \u2126.\nWhenever m = 0, we may drop the superscript m. Also, we may write C0(Rd) if we want to\nspecify that the input space X is speci\ufb01cally Rd. We will write Cm\n\u2217if we want to designate either\nCm or Cm\n0 without specifying which one of them. Note that CX \u2190\u0593 Cm \u2190\u0593 (Cm\nb )c \u2190\u0593 Cm\n0 \u2190\u0593 Cm\nc .\nWe will write k \u2208C(m,m) (resp. k \u2208C(m,m)\n0\n) to express that k is (m, m)-times continuously\ndi\ufb00erentiable (resp. (m, m)-times continuously di\ufb00erentiable and, for all |p| \u2264m and x \u2208X,\n\u2202(p,p)k(., x) \u2208C0 and supx\u2208X \u2202(p,p)k(x, x) < \u221e).\n5\nSpaces of measures.\nWe will only consider regular Borel measures. This won\u2019t be repeated in\nthe sequel. The letter M will designate a generic set of measures. We will encounter the following\nspaces (or sets) of (signed) measures.\nM\u03b4\nthe space of measures with \ufb01nite support: M\u03b4 := span{\u03b4x | x \u2208X}.\nMc\nthe \ufb01nite measures with compact support.\nMf\nthe \ufb01nite measures.\nP\nthe set of probability measures. It is the set of positive measures in Mr that sum to 1.\nMr\nthe space of locally \ufb01nite measures, also known as Radon measures. By locally \ufb01nite, we\nmean that, on every compact K \u2282X, the measure takes \ufb01nite values.\nWe will also brie\ufb02y encounter the following subsets of Mf: M+, the \ufb01nite positive measures; M\n\u2264c\nf ,\nthe \ufb01nite measures with total variation \u2264c; M\n\u2264c\n+ := M+ \u2229M\n\u2264c\nf ; and Pc := Mc \u2229P. Note that\nM\u03b4 \u2282Mc \u2282Mf \u2282Mr. Given a measure \u00b5 \u2208Mr, we note |\u00b5| its absolute value. In particular,\n|\u00b5|(X) equals its (possibly in\ufb01nite) total variation. Given a set F of \u00b5-integrable functions, for\nf \u2208F, we will write \u00b5(f) :=\nR\nf d\u00b5. In particular, note that \u00b5 de\ufb01nes a linear form over F. This\nleads us to\nDual spaces and spaces of measures.\nThe (topological) dual F\u2032 of a TVS F is the space of\ncontinuous linear forms over F. On locally compact Hausdor\ufb00spaces X, some spaces of measures\nM are known to identify, algebraically and topologically,2 with the dual F\u2032 of speci\ufb01c function\nspaces F via the map M \u2192F\u2032, \u00b5 7\u2192(f 7\u2192\u00b5(f)) (see Riesz-Markov-Kakutani Representation\nTheorem in Appendix D). In particular: M\u03b4 = (CX)\u2032, Mc = C\u2032, Mf = C\u2032\n0 and Mr = C\u2032\nc. When\nX = Rd, these are special cases of the following\nSpaces of distributions.\nThe letter D will always be used as a generic to designate a set of\ndistributions. We now list the respective duals of the previously de\ufb01ned spaces of functions.3\nM\u03b4\nwhich can be shown to be the dual of CX.\nEm\nthe space of distributions of order m with compact support. It can be shown that, for\nm \u2264\u221e: Em = {P\n|p|\u2264n \u2202p\u00b5p | n < m + 1, \u00b5p \u2208Mc}. In particular, E0 = Mc.\nDm\nL1\nthe space of summable or integrable distributions of order m. It can be shown that, for\nm \u2264\u221e: Dm\nL1 = {P\n|p|\u2264n \u2202p\u00b5p | n < m + 1, \u00b5p \u2208Mf}. In particular, D0\nL1 = Mf.\nDm\nL1\nsame space as before.\nDm\nthe space of Schwartz-distributions of order m. We will refer to D\u221esimply as the space\nof distributions. It can be shown that, for m < \u221e: Dm = {P\n|p|\u2264m \u2202p\u00b5p | \u00b5p \u2208Mr}. In\nparticular, D0 = Mr.\nLq\u2032\nwhere for 1 \u2264q < \u221e,\n1\nq\u2032 + 1\nq = 1. Note that the dual of L1 is L\u221e, but the converse is\nnot true.\n2when F\u2032 is equipped with its strong dual topology\n3Note that our notations di\ufb00er from those of L. Schwartz in that we omit the additional prime he puts to every\nspace of distribution. For him, D (without prime) would rather designate a space of functions.\n6\n/\nWe will not consider the duals of W m,q.\nW \u2212m,q\u2032\n0\n:= (W m,q\n0\n)\u2032 (q < \u221e)\nFor a characterisation of these spaces of distributions, see Adams (1975, Theorem 3.10).\nWe will call any element of one of these dual spaces a distribution.\nNote that they can all\nbe seen as an element of D\u221e.\nIn particular: M\u03b4 \u2282Em \u2282W \u2212m\u22121,q\u2032\n0\n\u2282Dm\nL1 \u2282Dm \u2282D\u221e\nand Lq\u2032 \u2282W \u2212m,q\n0\n\u2282D\u221e.\nThese inclusions generalise those noted for measures.\nA diagram\nsummarising all inclusions of functions and of their duals can be found in Appendix E.\nTopologies on duals.\nWhen A is a set of linear forms over a TVS F, we note b(A, F) or\nb(F\u2032, F)\u2229A (resp. w(A, F) or w(F\u2032, F)\u2229A) the topology of bounded (resp. pointwise) convergence\nover F. We will call b(F\u2032, F) (resp. w(F\u2032, F)) the strong dual (resp. weak dual, or weak-star)\ntopology on F\u2032. When F = Cb, we will call w(A, Cb) the narrow topology over A and use the\nletter \u03c3 rather than w: \u03c3(A, Cb) := w(A, Cb). Finally, we call w(A, Cc) the vague topology over\nA.\nWe will write (F\u2032)b (resp. (F\u2032)w, (F\u2032)\u03c3, (F\u2032)T) to specify that F\u2032 carries its strong dual\ntopology (resp. its weak dual topology, its narrow topology, a given topology T). By default, F\u2032\nwill be equipped with its strong topology. When F is a Banach space, the strong topology of F\u2032\nis the topology induced by the dual norm. A topological subspace of distributions \u2014or topological\ndistribution space\u2014 is any subspace of D\u221ethat embeds continuously into D\u221e. If a LCv TVS\nF1 is densely and continuously contained in another one F2, then F\u2032\n2 is continuously in F\u2032\n1 (see\nSection 7.2). In particular, all dual spaces listed above are topological distribution spaces: they\nverify the following continuous inclusions.\nM\u03b4 \u0592\u2192Em \u0592\u2192W \u2212m\u22121,q\u2032\n0\n\u0592\u2192Dm\nL1 \u0592\u2192Dm \u0592\u2192D\u221e\nand\nLq\u2032 \u0592\u2192W \u2212m,q\n0\n\u0592\u2192D\u221e.\nOn Dm\nL1, we will only consider the strong and weak topologies b(Dm\nL1, Cm\n0 ) and w(Dm\nL1, Cm\n0 ).\nFunction\nand\ndistribution\nrelated\nnotations.\nIn\naccordance\nwith\nthe\nnotation\n\u00b5(f) :=\nR\nf d\u00b5, we may also designate D(f) by\nZ\nf dD\nor\nZ\nf(x) dD(x)\nor\nDx(f)\nor\nDx(f(\u02c6x)) .\nNote that, given a function f (or functional D), it will sometimes be handy to specify the name\nof its input variable. To do so, we will write f(\u02c6x) (or D( \u02c6f)), meaning f (or D), where x (or f) is\nthe dummy input variable. In particular k(., \u02c6x) = (x 7\u2192k(., x)). Finally, we note \u00aff the complex\nconjugate function f(\u02c6x) of f.\nSupports.\nThe support supp\u03d5 of a function \u03d5 \u2208CX is the closure of the set of points x \u2208X\nsuch that \u03d5(x) \u0338= 0. The support suppD of a distribution D is the greatest closed set S \u2282X,\nsuch that for any \u03d5 \u2208C\u221e\nc\nwith support in X\\S, D(\u03d5) = 0. When identifying function \u03d5 with\nthe distribution f 7\u2192\nR\nf\u03d5, the two de\ufb01nitions coincide.\nLet us now turn to some de\ufb01nitions concerning kernels.\n7\n3\nCharacteristicness: a Dual Counterpart to Universality\nIn this section, we show how to embed general distribution spaces into an RKHS. The link between\nuniversal and characteristic kernels will then be straightforward. But before we go into detail, let\nus brie\ufb02y sketch the idea of a distribution embedding.\nLike any other Hilbert space, an RKHS Hk identi\ufb01es with its conjugate dual Hk\n\u2032 via the Riesz\nrepresentation map Hk \u2192Hk\n\u2032, f 7\u2192\u27e8\u02c6\u03d5 | f\u27e9k (see Appendix D)4. For the KME\nR\nk(., x) d\u00b5(x) of\na measure \u00b5 (and under suitable assumptions on the kernel k and the measure \u00b5), this associated\nlinear form is simply \u00af\u00b5\n\f\f\nHk, because\n\u001c\n\u02c6\u03d5 |\nZ\nk(., x) d\u00b5(x)\n\u001d\nk\n=\nZ\n\u27e8\u02c6\u03d5 | k(., x)\u27e9k d\u00af\u00b5(x) = \u00af\u00b5( \u02c6\u03d5) .\nThus, whenever a measure \u00b5 de\ufb01nes a continuous linear form over Hk, its KME is the Riesz\nrepresenter of \u00af\u00b5\n\f\f\nHk. But when is \u00b5 continuous over Hk? It is, as soon as \u00b5 belongs to a dual F\u2032\nsuch that Hk embeds continuously into F. This holds not only for measures, but for all elements of\nF\u2032. And depending on the choice of F, those elements can be distributions that are not measures.\nNow the details. We start with reminders on function spaces and universal kernels, continue\nwith the de\ufb01nition of distribution embeddings and characteristic kernels, and \ufb01nish with the\nnatural link between universal and characteristic kernels.\n3.1\nUniversal Kernels\nThe literature distinguishes various kinds of universal kernels k, such as c-, cc\u2212or c0-universal\nkernels. They are all special cases of the following unifying de\ufb01nition.\nDe\ufb01nition 1 (Universal Kernels). Let F be a loc. cv. TVS and k a kernel such that Hk \u0592\u2192F. k\nis said universal over F if Hk is dense in F. When F equals Cm\n0 (resp. Cm\n0 ), k will be said cm-\n(resp. cm\n0 -) universal. When m = 0, we may drop the superscript m.\nRemark 2. c-universality usually refers to the special case where X is compact, the general case\nbeing known as cc-universality (Sriperumbudur et al., 2010a,b). We \ufb01nd this an unnatural dis-\ntinction and stick to c-universal for both.\nIn the literature, the hypothesis that Hk be not only contained, but continuously contained in\nF is usually replaced by (necessary and) su\ufb03cient smoothness assumptions on the kernel, so that\nthis inclusion holds. For example (see proofs in Appendices A.1 and A.2):\nProposition 3 (Characterisation of Hk \u0592\u2192C\u22c6). Hk \u2282C0 (resp. Hk \u2282Cb, resp. Hk \u2282C) i\ufb00the\ntwo following conditions hold.\n(i) For all x \u2208X, K(., x) \u2208C0 (resp. K(., x) \u2208Cb, resp. K(., x) \u2208C).\n(ii) x 7\u2192k(x, x) is bounded (resp. bounded, resp. locally bounded, meaning that, for each y \u2208X,\nthere exists a (compact) neighbourhood of y on which x 7\u2192k(x, x) is bounded.).\nIf so, then Hk \u0592\u2192C0 (resp. Hk \u0592\u2192Cb , thus Hk \u0592\u2192(Cb)c , resp. Hk \u0592\u2192C).\n4In this paper, the inner products are chosen linear on the left, and anti-linear on the right.\n8\nProposition 4 (Su\ufb03cient condition for Hk \u0592\u2192Cm\n\u22c6).\nIf k \u2208C(m,m) , then Hk \u0592\u2192Cm.\nIf k \u2208C(m,m)\n0\n, then Hk \u0592\u2192Cm\n0 .\nIf k \u2208C(m,m)\nb\n, then Hk \u0592\u2192Cm\nb , thus Hk \u0592\u2192(Cm\nb )c.\nProposition 3 shows that Hk \u2282C\u2217i\ufb00Hk \u0592\u2192C\u2217. As there exist non-continuous kernels whose\nRKHS functions are all continuous, the hypothesises in Proposition 4 is su\ufb03cient but not necessary.\nWe do not know whether, in general, Hk \u2282Cm\n\u2217\nimplies Hk \u0592\u2192Cm\n\u2217. However, if Hk \u2282Cm+1,\nthen k \u2208C(m,m) (Grothendieck, 1953, cited by Schwartz, 1954, Lemme 2), thus Hk \u0592\u2192Cm. In\nparticular, if Hk \u2282C\u221e, then Hk \u0592\u2192C\u221e.\n3.2\nDistribution Embeddings and Characteristic Kernels\nBefore de\ufb01ning characteristic kernels, we must de\ufb01ne the KME of a distribution. The trick resides\nin the interpretation of the integral\nR\nk(., x) d\u00b5(x).\nFor a bounded measurable kernel, the KME of a \ufb01nite measure \u00b5 is de\ufb01ned as the RKHS\nfunction given by\nR\nk(., x) d\u00b5(x). However, as k(., \u02c6x) : x 7\u2192k(., x) takes its values in a possibly\nin\ufb01nite dimensional vector space, one must agree on the de\ufb01nition of the integral\nR\nk(., x) d\u00b5(x).\nUsually one uses the Bochner-integral, which is convenient, because k(., \u02c6x) is Bochner-integrable\nwith respect to (w.r.t.) \u00b5 i\ufb00\nR\n\u2225k(., x)\u2225k d|\u00b5|(x) < \u221e.\nUnfortunately, the Bochner-integral is\ndi\ufb03cult to generalise to arbitrary distributions. So we will use a more general integral: the weak-\n(or Pettis-) integral.\nRemark 5. Alternatively, one may be tempted to see\nR\nk(\u02c6s, x) d\u00b5(x) as a parametric integral with\nparameter s. (This corresponds to replacing the condition \u2200f \u2208Hk in the upcoming Equations 2\nand 3 by the condition \u2200k(., s), s \u2208X.) However, even though the integral may be de\ufb01ned for\nany parameter value s, the resulting function need not be in the RKHS (see Appendix B). Thus\nthis notion of integrability is too weak for KMEs.\nDe\ufb01nition 6 (Weak Integral and KME). Let D be a linear form over a loc. cv. TVS of functions\nF. Let \u20d7\u03d5 : X \u2192Hk be an RKHS-valued function such that for any f \u2208Hk, \u27e8f | \u20d7\u03d5(\u02c6x)\u27e9k \u2208F.\nThen \u20d7\u03d5 : X \u2192Hk is said to be weakly integrable w.r.t. D if there exists a function in Hk, noted\nR\n\u20d7\u03d5(x) dD(x) or D(\u20d7\u03d5), such that\n\u2200f \u2208Hk,\n\u001c\nf |\nZ\n\u20d7\u03d5(x) dD(x)\n\u001d\nk\n=\nZ\n\u27e8f | \u20d7\u03d5(x)\u27e9k d \u00afD(x) .\n(2)\nIf \u20d7\u03d5 is weakly integrable w.r.t. D (or w.r.t. any D in a set of linear forms D), we say that D\n(resp. D) embeds \u2014or is embeddable\u2014 into Hk via \u20d7\u03d5.\nIf \u20d7\u03d5(\u02c6x) = k(., \u02c6x), we omit the \u201cvia\nk(., \u02c6x)\u201d and call\nR\nk(., x) d\u00b5(x) the kernel mean embedding (KME) of \u00b5. We note \u03a6\u20d7\u03d5 the map\n\u03a6\u20d7\u03d5 : D \u2192Hk, D 7\u2192\nR\n\u20d7\u03d5(x) dD(x). It is linear, whenever D is a vector space.\nThis de\ufb01nition extends the usual Bochner-integral: if \u20d7\u03d5 is Bochner-integrable w.r.t. a Radon\nmeasure \u00b5, then, for any f \u2208Hk, the function \u27e8f | \u20d7\u03d5(\u02c6x)\u27e9k is \u00b5-(Lebesgue-)integrable and \u20d7\u03d5 is\nweakly integrable w.r.t. \u00b5 (Schwabik, 2005, Proposition 2.3.1).\nIn principle, F could be any loc. cv. TVS of functions and D any linear form. For us however,\nF will be one of the function spaces de\ufb01ned in Section 2, and D a distribution. Furthermore,\nto de\ufb01ne KMEs, we will focus on \u20d7\u03d5(\u02c6x) = k(., \u02c6x). The condition that \u27e8f | \u20d7\u03d5(\u02c6x)\u27e9k \u2208F for any\n9\nf \u2208Hk then simply becomes Hk \u2282F. And, remembering that by de\ufb01nition\nR\nf(x) dD(x) stands\nfor D(f), Equation (2) now simply reads:\n\u2200f \u2208Hk,\n\u001c\nf |\nZ\nk(., x) dD(x)\n\u001d\nk\n= \u00afD(f) .\n(3)\nThe left side being continuous w.r.t. f, so is the right-side. Thus \u00afD and D are continuous linear\nforms over Hk: D \u2208H\u2032\nk. Conversely, if D is a continuous linear form over Hk, then, by the Riesz\nrepresentation theorem (see Appendix D), there exists a unique element\nR\nk(., x) dD(x) \u2208Hk,\ncalled the Riesz representer of \u00afD, such that (3) be satis\ufb01ed. We just proved\nLemma 7. A linear form D over a loc. cv. TVS of functions F embeds into Hk i\ufb00Hk \u2282F and\nif the restriction D\n\f\f\nHk of D to Hk is a continuous linear form over Hk. In that case \u00afD\n\f\f\nHk is also\na continuous linear form over Hk and\nR\nk(., x) dD(x) is its Riesz representer.\nThus, for an embeddable space D, the embedding map \u03a6k is the composition of the following\ntwo linear maps:\n\u03a6k :\n\uf8f1\n\uf8f4\n\uf8f2\n\uf8f4\n\uf8f3\nD\n\u2212\u2192\nHk\n\u2032\n\u2212\u2192\nHk\nConjugate restriction\nRiesz representer\nD\n7\u2212\u2192\n\u00afD\n\f\f\nHk\n7\u2212\u2192\nR\nk(., x) dD(x)\n,\n(4)\nwhere Hk\n\u2032 is the conjugate space of H\u2032\nk.\nIt is the space H\u2032\nk, where the scalar multiplication\n(\u03bb, f \u2032) 7\u2192\u03bbf \u2032 is replaced by (\u03bb, f \u2032) 7\u2192\u00af\u03bbf \u2032. Using Hk\n\u2032 instead of H\u2032\nk is simply a trick for both the\nconjugate restriction and the Riesz representation maps to become linear instead of anti-linear.\nHk\n\u2032 contains exactly the same elements as H\u2032\nk. When Hk is identi\ufb01ed with its dual Hk\n\u2032, the KME\n\u03a6k is the complex conjugate of the restriction map (in short: conjugate restriction map) of D to\nHk.5 Furthermore, we have the following obvious but crucial lemma.\nLemma 8. Let F be a loc. cv. TVS of functions.\nIf Hk \u2282F, then the KME is simply the\nconjugate \u0131\u22c6of the (algebraic) transpose \u0131\u22c6of the canonical embedding \u0131 : Hk \u2192F, f 7\u2192f.\nThe continuity assumption appearing in Lemma 7 may seem unpalatable.\nUnfortunately,\nwhenever Hk is in\ufb01nite dimensional, non-continuous linear forms over Hk do exist, and those\nhave no Riesz representer. Thus the continuity assumption is necessary. Now we may wonder,\nhow to know if a linear form D, for example a distribution, is continuous over Hk. In practice,\nwe will use the following easy but important result.\nCorollary 9. Let F be a loc. cv. TVS of functions. If Hk \u0592\u2192F, then F\u2032 embeds into Hk. In\nparticular:\n(i) If Hk \u2282C (resp. Hk \u2282C0, resp. Hk \u2282Cb ) then Mc (resp. Mf, resp. Mf) embeds into Hk.\n(ii) If k \u2208C(m,m) (resp. k \u2208C(m,m)\n0\n, resp. k \u2208C(m,m)\nb\n), then Em (resp. Dm\nL1, resp. Dm\nL1) is\nembeddable into Hk.\nProof. The \ufb01rst line is a straightforward corollary of lemma 8, but may as well be proven directly.\nIndeed, suppose that Hk \u0592\u2192F. Let D \u2208F\u2032 and let f, f1, f2, . . . \u2208Hk. If fn \u2192f in Hk then\nfn \u2192f in F, thus D(fn) \u2192D(f).\nThus D is a continuous linear form over Hk.\nThe \u201cin\nparticular\u201d part then follows from Propositions 3 and 4.\n5Had we chosen to identify Hk, not with Hk\n\u2032, but with H\u2032\nk via the anti-linear Riesz representation map, then\nwe would end up identifying \u03a6k, which is a linear map, with an anti-linear map!\n10\nRemark 10. Anticipating on Section 7, let us already note that if Hk \u0592\u2192F, meaning that the\ncanonical embedding map \u0131 is continuous, then (the conjugate of) its transpose is also continuous:\nthe KME is thus continuous over F\u2032.\nFor some machine learning algorithms, it is interesting when KMEs are injective. This leads\nto\nDe\ufb01nition 11 (Characteristic Kernel). Let D be a set of linear forms that embed into\nan RKHS Hk.\nThe kernel k will be said\ncharacteristic to D if the embedding \u03a6k\n:\nD \u2192Hk, D 7\u2192\nR\nk(., x) dD(x) is injective. When D = P, k is simply said characteristic.\nDiagram (4) shows that a kernel is characteristic to an embeddable set D i\ufb00the conjugate\nrestriction map D 7\u2192\u00afD\n\f\f\nHk (or equivalently simply the restriction map D 7\u2192D\n\f\f\nHk) is injective.\nWhen D is a dual of a space F that contains Hk, it suggests to characterise characteristic kernels\nover D by checking whether there are \u201cenough\u201d functions in Hk to uniquely de\ufb01ne a distribution\nD \u2208D by its values taken on Hk. \u201cEnough\u201d will be when Hk is dense in F.\n3.3\nDuality Links Universal and Characteristic Kernels\nSuppose that Hk \u0592\u2192F.\nIf Hk is dense in F (i.e., if k is universal over F), any D \u2208F\u2032 is\ncompletely determined by its values taken over Hk.\nThus the (conjugate) restriction map is\ninjective. Conversely, if Hk is not dense in F (k is not universal over F), then, by de\ufb01nition,\nits closure f\nHk in F is strictly smaller than F. Then, as a direct corollary6 of the Hahn-Banach\nTheorem, one can construct a distribution D \u2208F\u2032 that is identically null on f\nHk and non-null\noutside: D \u0338= 0 but \u00afD\n\f\f\nHk = 0. So the (conjugate) restriction map is not injective. We just proved\nTheorem 12 (Universal and Characteristic Kernels). Let F be a loc. cv. TVS such that Hk \u0592\u2192F.\nThe kernel k is universal over F i\ufb00it is characteristic to F\u2032.\nIn particular, if Hk is continuously contained in Cm\n0\n(resp. Cm), then k is cm\n0 - (resp. cm-)\nuniversal i\ufb00it is characteristic to Dm\nL1 (resp. Em). Theorem 12 generalises Theorems 3 and 16\nfrom Sriperumbudur et al. (2010a), which respectively correspond to the case F = C0 and F = C.\nTable 1 illustrates the wide range of applications of Theorem 12.\nIn general, the assumption that Hk \u0592\u2192F cannot be dropped \u2014at least, not the assumption\nHk \u2282F. Indeed, if k is a universal kernel over C0, then the kernel k + 1 is characteristic to Mf\n(because integrally strictly positive de\ufb01nite (\nR\ns.p.d.), see Section 4.2). But Hk is not contained\nin C0.\nTheorem 37 and its corollary will show that cm\n0 -universal kernels do exist. Their proof use\nresults from sections to come, which establish basic calculus rules for embedded distributions.\n4\nDistributional Calculus\nThis section\u2019s primary goal is to show the following calculus rules and a few implications. For any\nembeddable distributions D and T and any distribution D such that \u2202pD is embeddable,\n6It is in this corollary that one uses the local convexity of F. See Corollary 3 of the Hahn-Banach Theorem by\nTreves (1967).\n11\nUniversal\nCharacteristic S.P.D.\nUsual Name\nProof\nF\nF\u2032\nF\u2032\n/\nTh.12 & P.16\nCX\nM\u03b4\nM\u03b4\ns.p.d.\nTh.12 & P.16\nCX/1\nM0\n\u03b4\nM0\n\u03b4\nconditionally s.p.d.\nP.33 & R.34 & P.16\nC\nMc\nMc\nc-universal (or cc-universal)\nTh.12 & P.16\nC0\nMf\nMf\nc0-universal\nTh.12 & P.16\n(Cb)c\nMf\nMf\nR\nspd\nTh.12 & P.16\n((Cb)c)/1\nP (or M0\nf )\nM0\nf\ncharacteristic\nL.27 & P.33\nLq\nLq\u2032\nLq\u2032\n/\nTh.12 & P.16\nCm\nEm\nEm\ncm-universal\nTh.12 & P.16\nCm\n0\nDm\nL1\nDm\nL1\ncm\n0 -universal\nTh.12 & P.16\n(Cm\nb )c\nDm\nL1\nDm\nL1\n/\nTh.12 & P.16\nW m,q\n0\nW \u2212m,q\u2032\n0\nW \u2212m,q\u2032\n0\n/\nTh.12 & P.16\nTable 1: Equivalence between the notions of universal, characteristic and s.p.d. kernels. This table\nis to be read: \u201cIf Hk continuously embeds into Column 1, then k is universal over Column 1 i\ufb00\nit is characteristic to Column 2 and i\ufb00it is s.p.d. over Column 3. See Column 4.\u201d Here, q < \u221e,\nm \u2208N \u222a{\u221e}, and Th,P,R stand for Theorem, Proposition and Remark. The case of P (last row)\nis not directly covered by Theorem 12 and Proposition 16, because it is not even a vector space.\n(R1)\n\nf |\nR\nk(., x) dD(x)\n\u000b\nk =\nR\n\u27e8f | k(., x)\u27e9k dD(x) (De\ufb01nition of embedding / weak integral)\n(R2) \u27e8\u03a6k(D) | \u03a6k(T)\u27e9k =\nR\nk(x, y) dD(y) d \u00afT (x) (Fubini)\n(R3) \u03a6k(\u2202pS) = (\u22121)|p| R\n\u2202(0,p)k(., x) dS(x) (Di\ufb00erentiation).\nRule (R1) holds by de\ufb01nition of an embedding. So let us move directly to Rule (R2).\n4.1\nInner Product and Norm of Embeddings\nIf D and T are two embeddable distributions for a kernel k, we write:\n\u27e8D | T\u27e9k := \u27e8\u03a6k(D) | \u03a6k(T)\u27e9k\n\u2225D\u2225k := \u2225\u03a6k(D)\u2225k .\nNote that this induces a new metric on the set of embeddable distributions (see Section 7).\nTheorem 13 (Fubini). Let D, T be two embeddable distributions into Hk. Then:\n\u27e8D | T\u27e9k =\nZZ\nk(x, y) dD(y) d \u00afT (x) =\nZZ\nk(x, y) d \u00afT (x) dD(y)\n(5)\n\u2225D\u22252\nk =\nZZ\nk(x, y) dD(y) d \u00afD(x) =\nZZ\nk(x, y) d \u00afD(x) dD(y) ,\nwhere\nRR\nk(x, y) dD(y) d \u00afT (x) is to be interpreted as \u00afTx(I\u02c6x) with Ix = Dy(k(x, \u02c6y)).\n12\nProof. See Appendix A.3.\nWe will also use the tensor product notation\n[ \u00afT \u2297D](k) := [ \u00afTx \u2297Dy](k) :=\nZZ\nk(x, y) d \u00afT (x) dD(y) = \u27e8D | T\u27e9k .\n4.2\nApplication to Strict Positive De\ufb01niteness\nDe\ufb01nition 14 (S.P.D.). A kernel k is said s.p.d. over a set of distributions D if D is embeddable\ninto Hk and if for any D \u2208D,\n\u2225D\u22252\nk = 0\n\u21d2\nD = 0 .\n(6)\nFollowing the tradition, when D = M\u03b4, we simply call k s.p.d. When D = Mf, k is said integrally\nstrictly positive de\ufb01nite (\nR\ns.p.d.).\nRemark 15. By de\ufb01nition, a kernel is s.p.d. over D i\ufb00the semi-norm induced by k in D is a proper\nnorm. Also note that, using Theorem 13 (Fubini), we may rewrite Equation (6) as follows. For\nany D \u2208D,\nZ\nk(x, y) dD(y) d \u00afD(x) = 0\n\u21d2\nD = 0.\nRemembering that by de\ufb01nition, for any embeddable D, \u2225D\u2225k := \u2225\u03a6k(D)\u2225k, we immediately\nget:\nProposition 16. For any embeddable vector space of distributions D, k is characteristic to D i\ufb00\nit is s.p.d. over D.\nIn particular, saying that a kernel k is s.p.d. (resp.\nR\ns.p.d.) is equivalent to saying it is char-\nacteristic to CX (resp. Mf). Note also that the proposition does not apply to the case where D\nis the set of probability measures P. However, Lemma 27 will show that a kernel is characteristic\nto P i\ufb00it is s.p.d. over M0\nf := {\u00b5 \u2208Mf |\nR\nd\u00b5 = 0}.\n4.3\nDi\ufb00erentiation\nLet us now turn towards di\ufb00erentiation.\nWe start with an easy, but useful lemma proved in\nAppendix A.4.\nLemma 17. Let k \u2208C(m,m) and \u20d7\u03d5k(\u02c6x) = k(., \u02c6x). The RKHS-valued function \u20d7\u03d5k is m-times\ncontinuously di\ufb00erentiable and \u2202p\u20d7\u03d5k = \u2202(0,p)k.\nThis lemma is used in Remark 19 to give a concise interpretation of the following theorem\n(shown in Appendix A.5).\nTheorem 18. Let k \u2208C(m,m) and p \u2208Nd such that |p| \u2264m. A distribution D embeds into Hk\nvia \u2202(0,p)k i\ufb00\u2202pD embeds into Hk via k. In that case,\n\u03a6k(\u2202pD) = (\u22121)|p|\nZ\n[\u2202(0,p)k](., x) dD(x) = (\u22121)|p| \u03a6\u2202(0,p)k(D) .\n13\nRemark 19. Noting \u20d7\u03d5k(\u02c6x) = k(., \u02c6x), Equation (18) reduces to \u2202pD(\u20d7\u03d5k) = (\u22121)|p|D(\u2202p\u20d7\u03d5k) . A\nsu\ufb03cient condition for this equality to hold is D \u2208Cm\u2212|p|\n\u2217\n. Taking D = \u03b4x proves Equation (1)\nfrom the introduction: by de\ufb01nition, \u03b4x(\u2202p\u20d7\u03d5k) = \u2202p\u20d7\u03d5k(x) and \u2202p\u03b4x(\u20d7\u03d5k) = \u03a6k(\u2202p\u03b4x).\nThis result has two important practical applications.\nFirst and foremost, it gives a concrete way to numerically calculate the KME of a distribution\nD. Indeed, consider the case where D is a measure \u00b5. Equation (18) shows we may compute the\nembedding of \u2202p\u00b5 using only integration w.r.t. \u00b5. In other words, we have reduced the computation\nof an integral w.r.t. the distribution \u2202p\u00b5 to an integral w.r.t. the measure \u00b5. And this trick applies\nintegrable to any embeddable and integrable distribution, because (Schwartz, 1954, around p.100):\nProposition 20. For any m \u2264\u221eand any distribution in D \u2208(Cm\n0 )\u2032 (resp. D \u2208(Cm)\u2032) there\nexists a \ufb01nite family of measures \u00b5p \u2208Mf (resp. \u00b5 \u2208Mc), with p \u2208Nd, |p| \u2264m, such that\nD = P\n|p|\u2264m \u2202p\u00b5p.\nA second application is the following. Suppose we could easily measure the values of a function\nf (say the location of a particle), but not of its derivative \u2202f (the speed). But we want the KME\nof \u2202f. Then Theorem 18 asserts that, under suitable assumptions on f, instead of measuring \u2202f\nand embed it, we can measure f, embed it and di\ufb00erentiate the embedding.\nLet us now show how embeddings in Hk relate to those in H\u2202(p,p)k. The following proposition,\nproved in Appendix A.6, shows that H\u2202(p,p)k is isometrical to a subspace of Hk.\nProposition 21. Let k \u2208C(m,m) and p \u2208Nd with |p| \u2264m. De\ufb01ne H\u2202(0,p)k to be the closed\nsubspace in Hk generated by \u2202(0,p)k(., x) when x \u2208X:\nH\u2202(0,p)k := span{\u2202(0,p)k(., x) | x \u2208X} \u2282Hk .\nThen the following de\ufb01nes a bijective isometry from H\u2202(0,p)k onto H\u2202(p,p)k:\n\u2202p :H\u2202(0,p)k\u2212\u2192H\u2202(p,p)k\nf\n7\u2212\u2192\u2202pf\n.\nObviously H\u2202(0,p)k is a closed subset of Hk, but is it a proper subset? Let us focus on the\ncase where k \u2208C(m,m)\n0\n. Assume that |p| = 1, say p = (p1, 0, . . . , 0) \u2208Nd. As, for any f \u2208Hk,\n\u27e8f | \u2202pk(., x)\u27e9k = \u2202pf(x), the orthogonal complement H\u22a5\n\u2202(0,p)k of H\u2202(0,p)k is contained in the set of\nfunctions that do not depend on the \ufb01rst coordinate. Thus, for example, if X is 1-dimensional\n(d = 1), then H\u22a5\n\u2202(0,p)k is also at most 1-dimensional. And obviously, if Hk \u2282Cm\n0 , then it is even\n{0}. And by recurrence on |p|, we get\nTheorem 22. Let k \u2208C(m,m)\n0\nand p \u2208Nd such that |p| \u2264m. Then H\u2202(0,p)k = Hk. Hk and\nH\u2202(p,p)k are then isometrically isomorphic via \u2202p, the partial derivation operator.\nRemark 23. In geostatistics, one often considers the subspace H0\nk of Hk de\ufb01ned as the closure\nof {Pn\ni=1 \u03bbik(., xi) | n \u2208N, xi \u2208X, \u03bbi \u2208C, Pn\ni=1 \u03bbi = 0}.\nIf k \u2208C(m,m), it is not di\ufb03cult\nto see that H\u2202(0,p)k \u2282H0\nk. Thus, if k \u2208C(m,m)\n0\n, H0\nk = Hk. So when doing so-called intrinsic\nkriging, as opposed to simple kriging (i.e. with a known mean, see specialised literature such as\nChil`es and Del\ufb01ner, 2012 and Wackernagel, 2003), we solve two di\ufb00erent minimisation problems,\nyet, when Hk \u2282C0, we look for solutions that lie in the same set of functions Hk.\n14\nProposition 24 (Switches of Derivation and Integration). Let k \u2208C(m,m), p \u2208Nd such that\n|p| \u2264m and let D be a distribution. Consider the following three statements.\n(i) \u2202pD embeds into Hk via k.\n(ii) D embeds into Hk via \u2202(0,p)k.\n(iii) D embeds into H\u2202(p,p)k via \u2202(p,p)k.\nConditions (i) and (ii) are equivalent, and they imply (iii). If they are satis\ufb01ed, then\n(\u22121)|p|\u2202p\u03a6k(\u2202pD) = \u2202p\u03a6\u2202(0,p)k(D) = \u03a6\u2202(p,p)k(D) .\n(7)\nAdditionally, if k \u2208C(m,m)\n0\n, then all three conditions are equivalent.\nThis proposition, proved in Appendix A.7, calls for some comments. First, Equation (7) states\nin particular, that the integration and derivation signs may be switched:\n\u2202p[\nZ\n\u2202(0,p)k(., x) dD(x)] =\nZ\n\u2202(p,p)k(., x) dD(x) .\nSecond, when k \u2208C(m,m)\n0\n, the function \u03a6\u2202(0,p)k(D) is the only primitive of \u03a6\u2202(p,p)k(D) contained\nin Hk: it is the primitive that converges to 0 at in\ufb01nity. This con\ufb01rms what we already knew: \u2202p\nis injective. Third, in general, even if the conditions (i)-(iii) are met, the distribution D may not\nembed into Hk via k. Because even if a distribution \u2202pD is in Dm\u2212|p|\nL1\n, the distribution D, or any\nother primitive of order p of \u2202pD, may not be in Dm\nL1.\nExample 1. Consider the Gaussian kernel k(\u02c6x, \u02c6y) = e(\u02c6x\u2212\u02c6y)2 over X = R and the measure D =\narctan x dx. Then D \u2208D1\nL1, \u2202D =\ndx\n1+x2 \u2208D0\nL1 and \u2202D embeds into Hk; but D does not. For if it\ndid, the function y 7\u2192\nR\nk(y, x) arctan x dx would vanish at in\ufb01nity, because Hk \u2282C0. But, when\ny \u2192+\u221e, it converges to a strictly positive real.\nNoting that for stationary kernels, \u2202(0,p)k(\u02c6x, \u02c6y) = (\u22121)|p| \u2202p\u03c8(\u02c6x \u2212\u02c6y), we get the following\nrelation between the embedding of the derivative and the derivative of the embedding.\nCorollary 25 (For Stationary Kernels). Suppose that the kernel k \u2208C(m,m) be stationary and\nthat assumptions (i)-(iii) of Proposition 24 are met. Then\n\u03a6k(\u2202pD) = \u2202p[\u03a6k(D)].\nAnother corollary, proved in Appendix A.8, is the following.\nCorollary 26. Let k \u2208C(m,m) and p \u2208Nd such that |p| \u2264m. Let D be a set of distributions such\nthat the set \u2202pD := {\u2202pD | D \u2208D} embeds into Hk. Then the kernel \u2202(p,p)k is characteristic to\nD i\ufb00it is characteristic to \u2202pD.\nWhen k \u2208C(m,m)\n0\n(resp. k \u2208C(m,m)), this corollary applies in particular to any subset D of\nDm\u2212|p|\nL1\n(resp. Em\u2212|p|). This will play an important role in the proofs of Theorems 37 and 40, that\nshow the existence of c\u221e\n0 - and c\u221e-universal kernels.\n15\n5\nThe Set of Probability Measures P\nSo far, we focused on the properties of distribution embeddings in general, putting a special\nemphasis on duals of function spaces, such as Mf or its generalisations Dm\nL1. In most applications,\nhowever, one only wants to embed probability measures. And, although the set of probability\nmeasures P is not a vector space, the same questions as before arise. In particular: when is a\nkernel characteristic, not over all Mf, but solely over the set of probability measures P?\nAs\nP \u2282Mf, it is clear that if k is characteristic to Mf \u2014i.e., if k is\nR\ns.p.d.\u2014 then it is characteristic\nto P. In Section 5.1 we show that the converse does not hold and give an astonishingly simple\ncharacterisation of characteristic kernels.\nSection 5.2 then gives a second, more theoretical\ncharacterisation that complements Section 3.3.\nBefore we start, we make some important preliminary observations about characteristic kernels.\nThe set of probability measures lies in the (closed) a\ufb03ne hyperplane M1\nf of Mf given by the\nequation \u00b5(1) = 1, where\n1 is the constant function that equals 1: M1\nf := {\u00b5 \u2208Mf | \u00b5(1) = 1}.\nP is not equal to M1\nf .\nBut when translated by, say, \u2212\u03b4x (x \u2208X) to lie in the hyperplane\nM0\nf := {\u00b5 \u2208Mf | \u00b5(1) = 0}, its linear span spans the whole hyperplane M0\nf : span(P \u2212\u03b4x) = M0\nf .\nThus:\nLemma 27 (Important!). A kernel k0 is characteristic to P (resp. Pc := P \u2229Mc) i\ufb00it is\ncharacteristic to the closed hyperplane M0\nf := {\u00b5 \u2208Mf | \u00b5(1) = 0} in Mf (resp. M0\nc := M0\nf \u2229Mc\nin Mc).\nBeing given a kernel k is equivalent to being given its KME \u03a6k (over a set containing at least\nM\u03b4): the former de\ufb01nes the latter by construction, and the latter de\ufb01nes the former because\nk(x, y) = \u27e8\u03a6k(\u03b4y) | \u03a6k(\u03b4x)\u27e9k =: \u27e8\u03b4y | \u03b4x\u27e9k .\nThus, informally speaking, Lemma 27 shows that a characteristic kernel k0 is \u201calmost\u201d\nR\ns.p.d.\nBoth the KME of a characteristic and the KME of an\nR\ns.p.d. kernel are injective over M0\nf . But\nthe \ufb01rst need not be injective over Mf: there might be one single line in Mf, not contained in M0\nf\nand mapped to the null function of Hk0. This suggests that, from an\nR\ns.p.d. kernel k, it is easy\nto construct a characteristic kernel k0 that is not\nR\ns.p.d. anymore (Proposition 28): consider its\nembedding \u03a6k, choose a direction \u03bd0 \u2208Mf\\M0\nf , and construct a new embedding \u03a6k0 that equals\n\u03a6k on M0\nf , but maps \u03bd0 to the null-function. The kernel associated to this new embedding is\ncharacteristic but not\nR\ns.p.d. Conversely, it also suggests that, given a characteristic kernel k0\nthat is not already\nR\ns.p.d., one may easily construct an\nR\ns.p.d. kernel (Proposition 30): add 1\ndimension to Hk0, and construct an embedding \u03a6k that coincides with \u03a6k0 on M0\nf but maps \u03bd0 to\nthe additional new dimension. Thus, contrary to \u03a6k0, \u03a6k does not identify all measures parallel\nto \u03bd0 and is thus injective over Mf. By de\ufb01nition, its associated kernel k is then\nR\ns.p.d. These\nconsiderations will lead to the characterisation of characteristic kernels given in Theorem 32.\n5.1\nS.P.D. Kernels and Kernels Characteristic to P\nProposition 28 (Characteristic \u0338\u21d2\nR\ns.p.d.). Let k be an\nR\ns.p.d. kernel or, equivalently, a char-\nacteristic kernel over Mf. Let \u03bd0 \u2208Mf such that \u03bd0(1) \u0338= 0. De\ufb01ne, for any x, y \u2208X, the kernel\nk0(x, y) := \u27e8\u03b4x \u2212\u03bd0 | \u03b4y \u2212\u03bd0\u27e9k. Then k0 embeds Mf, but is not characteristic to Mf. However, k0\n16\nand k induce the same semi-metric dk in M0\nf and P. Thus k0 is a kernel that is characteristic to\nP, but not over Mf.\nFor instance, taking z0 \u2208X and \u03bd0 = \u03b4z0 leads to k0(x, y) = k(x, y) \u2212k(x, z0) \u2212k(z0, y) +\nk(z0, z0). As \u03a6k0(\u03b4z0) = 0, this example shows the following\nCorollary 29 (Characteristic \u0338\u21d2s.p.d.). There exist characteristic kernels, which are not strictly\npositive de\ufb01nite.\nProof. (of Proposition 28) k0 is a kernel, because it is of the form \u27e8\u20d7\u03d5(x) | \u20d7\u03d5(y)\u27e9k, for the map\n\u20d7\u03d5 : X \u2192Hk, x 7\u2192\u03a6k(\u03b4x \u2212\u03bd0). As \u03a6k0(\u03bd0) = 0, it is not characteristic to Mf. To prove that\nk and k0 induce the same semi-metric in M0\nf (and thus in P by Lemma 27), simply notice the\nfollowing. The kernel k0 veri\ufb01es: k0(x, y) = k(x, y) + \u03c8(x) + \u00af\u03c8(y) for some function \u03c8 : X \u2192C.\nUsing Theorem 13 (Fubini), we thus get, for any \u00b5 \u2208M0\nf\n\u2225\u00b5\u2225k0 = [\u00af\u00b5x \u2297\u00b5y](k(x, y) + \u03c8(x) + \u00af\u03c8(y)) = [\u00af\u00b5 \u2297\u00b5](k) + 0 + 0 = \u2225\u00b5\u2225k .\nWith their Example 1, Sriperumbudur et al. (2011) already exhibited a kernel that is char-\nacteristic but not s.p.d. and a fortiori not\nR\ns.p.d. But their construction may seem somewhat\nabstruse. Proposition 28 on the contrary not only gives an easy and systematic way to construct\na characteristic but non-\nR\ns.p.d. kernel, but the next proposition \u2014the converse\u2014 shows that all\nsuch kernels can be constructed with this procedure!\nProposition 30 (Characteristic is \u201calmost\u201d\nR\ns.p.d.). Let k0 be a characteristic kernel to P. Then\nthere exists a kernel k and a measure \u03bd0 \u2208Mf such that, k0(\u02c6x, \u02c6y) = \u27e8\u03b4\u02c6x \u2212\u03bd0 | \u03b4\u02c6y \u2212\u03bd0\u27e9k.\nRemark 31. Both in Proposition 28 and 30 we may replace \u201ccharacteristic (to P or M0\nf )\u201d and\n\u201c\nR\ns.p.d.\u201d (i.e. \u201ccharacteristic to Mf\u201d) by respectively \u201ccharacteristic to M0\u201d and \u201ccharacteristic\nto M\u201d, where M is M\u03b4 or any embeddable Em or Dm\nL1, and M0 := {\u00b5 \u2208M |\nR\nd\u00b5 = 0}. Note\nthat k0 and k de\ufb01ne the same equivalence class of conditionally positive de\ufb01nite (c.p.d.) kernels\n(see Remark 74 in Appendix C).\nWe prove Proposition 30 in Appendix A.9. Minor changes in this proof (see Appendix A.10)\nyield the following, seemingly more general and astonishingly simple characterisation of charac-\nteristic kernels. Surprisingly, we could not \ufb01nd any similar statement in the literature.\nTheorem 32 (Characterisation of Characteristic Kernels). Let k0 be a kernel. The following\nconditions are equivalent.\n(i) k0 is characteristic to P.\n(ii) k0 is characteristic to M0\nf .\n(iii) There exists \u01eb \u2208R such that the kernel k(\u02c6x, \u02c6y) := k0(\u02c6x, \u02c6y) + \u01eb2 is characteristic to Mf (i.e.\nR\ns.p.d. over Mf).\n(iv) For all \u01eb \u2208R\\{0}, the kernel k(\u02c6x, \u02c6y) := k0(\u02c6x, \u02c6y) + \u01eb2 is characteristic to Mf.\n(v) There exists an RKHS Hk with kernel k and a measure \u03bd0 \u2208Mf\\M0\nf such that k is charac-\nteristic to Mf, and k0(\u02c6x, \u02c6y) = \u27e8\u03b4\u02c6x \u2212\u03bd0 | \u03b4\u02c6y \u2212\u03bd0\u27e9k.\nIf these conditions are met, then k0 and k induce the same semi-metric dk in M0\nf and in P.\n17\n5.2\nCharacteristic Kernels to P: Characterisation in Terms of Universality\nThis section is a complement to Theorem 12. The latter applies to dual spaces but not necessarily\nto strict subsets.\nIn particular, Theorem 12 does not directly apply to probability measures.\nNevertheless, we will now characterise characteristic kernels (to P) in terms of universality.\nTo do so, consider the space of bounded continuous functions Cb. Following Schwartz (1954),\nwe equip Cb with a topology Tc that is weaker than its usual one.7 Let T1 be the topology induced\nby C in Cb and T2 the topology of uniform convergence over the compact sets of Mf. Then we\nde\ufb01ne Tc as the smallest topology containing T1 and T2. We note8 (Cb)c the space Cb equipped\nwith Tc. In this space, a sequence (fn)n\u2208N converges to f i\ufb00\n(i) fn \u2192f in C (i.e., on every compact set \u2225fn \u2212f\u2225\u221e\u21920) and\n(ii) for any compact set A \u2282Mf, sup\u00b5\u2208A |\u00b5(fn \u2212f)| \u21920.\nThe space (Cb)c is complete and its dual is Mf. The advantage of using (Cb)c instead of C0 as a\npredual of Mf is that Cb contains the constant unit function\n1. Thus we can de\ufb01ne the quotient\nspace ((Cb)c/1) and may state (and prove in Appendix A.11)\nProposition 33. Let k be such that Hk \u0592\u2192(Cb)c. Then k is characteristic to P i\ufb00k is universal\nover the quotient space ((Cb)c/1).\nRemark 34. Supposing that Hk \u0592\u2192((Cb)c/1) is su\ufb03cient. Additionally, Proposition 33 also holds\nif, in this whole section, one systematically replaces C, Cb, Mf, P and \u2225f\u2225\u221eby Cm, Cm\nb , Dm\nL1,\n(Dm\nL1)0 := {D \u2208Dm\nL1 | D(1) = 0} and \u2200|p| \u2264m, \u2225\u2202pf\u2225\u221erespectively. It also holds with CX and\nM0\n\u03b4 instead of Cb and P.\nApplying Theorem 12 to F = (Cm\nb )c, we get the following, in practice very useful lemma.\nLemma 35. Let k be such that Hk \u0592\u2192(Cm\nb )c.\n(For example, let k \u2208C(m,m)\nb\n.)\nThen k is\ncharacteristic to Dm\nL1 i\ufb00k is universal over (Cm\nb )c.\nWe close this section with a remark on an unfruitful, but maybe enlightening attempt to\ncharacterise characteristic kernels.\nRemark 36. Let X = R. For any \u00b5 \u2208Mf, if \u2202\u00b5 \u2208Mf, then \u2202\u00b5 \u2208M0\nf . In short: \u2202Mf \u2229Mf \u2282M0\nf .\nIn light of Proposition 24 and its Corollary 26, we were tempted to formulate a proposition like:\n\u201cLet k \u2208C(1,1)\n0\nbe a kernel such that \u2202(1,1)k is characteristic to Mf. Then k is characteristic to\nM0\nf (i.e. to P).\u201d Unfortunately \u2202Mf \u2229Mf \u0338= M0\nf , thus we failed to conclude.\n6\ncm- and cm\n0 -Universal Kernels\nIn this section, we will give su\ufb03cient conditions for a kernel to be cm\n0 - (Theorem 37) or cm-universal\n(Theorem 41). This will \ufb01nally show that c\u221e\n0 - and c\u221e-universal kernels do exist. Furthermore,\nstationary kernels k(x, y) := \u03c8(x \u2212y) will behave particularly beautifully: as soon as they are\nsmooth enough, cm\n0 - (resp. cm-) and c0- (resp. c\u2212) universal kernels turn out to be one and the\nsame thing (Corollary 38, resp. Proposition 42). Thus, machine learners need not change their\nhabits: if they show, as usual, that a smooth enough, stationary kernel is c0-universal, they get\ncm\n0 -universality for free, even without any prerequisites in distribution theory!\n7An alternative would be to equip C0\nb with the so-called strict topology (cf. Fremlin et al., 1972).\n8Schwartz (1954) uses the symbol Bm\nc\nrather than (Cb)c.\n18\n6.1\ncm\n0 -Universal Kernels\nTheorem 37. PROOF FLAWED. Let k be a kernel in C(m,m)\nb\n. If, for any p \u2208Nd with |p| \u2264m,\n\u2202(p,p)k is characteristic to Mf, then k is characteristic to Dm\nL1.\nProof. We proceed by recurrence on |p|. For ease of notation however, we restrict ourselves to the\nstep from |p| = 0 (which obviously holds) to |p| = 1.\nLet p \u2208Nd with |p| = 1. Suppose there exist two di\ufb00erent distribution D, T \u2208D1\nL1 such that\n\u03a6k(D) = \u03a6k(T). As \u2202(p,p)k is characteristic to Mf, k is characteristic to \u2202pMf (Corollary 26).\nThus k is characteristic to Mf and over \u2202pMf. Consequently D and T cannot be both in Mf or\nboth in \u2202pMf. Thus, one of them, say D, is in Mf\\(\u2202pMf). Now, \u03a6k(D) = \u03a6k(T) implies that,\nfor any f \u2208Hk: D(f) = T(f). But D being a measure, and Hk being dense in C0\n0, D de\ufb01nes a\nunique element in Mf by its values taken on Hk. In particular, it is uniquely de\ufb01ned on C1\n0 \u2282C0\n0\nby its values taken on Hk. Thus so is T. Thus D = T. Contradiction. Thus k is characteristic\nto Mf \u222a(\u2202pMf). For any other q \u2208Nd with |q| = 1, one now shows in a similar fashion that k is\ncharacteristic to (Mf \u222a(\u2202pMf))\u222a\u2202qMf. Continuing this process, one \ufb01nds that k is characteristic\nto \u222a|p|\u22641\u2202pMf. In other words: k is characteristic to D1\nL1 (Proposition 20).\nAs a beautiful corollary, we get:\nCorollary 38. 9 Let k(\u02c6x, \u02c6y) = \u03c8(\u02c6x \u2212\u02c6y) \u2208C(m,m)\nb\nbe a stationary kernel over X = Rd. The\nfollowing are equivalent.\n(i) k is characteristic (to P).\n(ii) k is characteristic to Mf (i.e.\nR\ns.p.d.).\n(iii) k is characteristic to Dm\nL1.\n(iv) supp F\u03c8 = X, meaning that the distributional Fourier transform F\u03c8 of \u03c8 has full support.\nIn particular, if \u03c8 \u2208Cm\n0 , then k is cm\n0 -universal i\ufb00it is c0-universal.\nRemark 39. Modulo a possible normalisation or scaling factor that depend on the chosen con-\nvention in the de\ufb01nition of the Fourier transform, F\u03c8 is but the (positive) measure \u039b that the\nBochner theorem associates to the stationary kernel k.\nProof. The following proof relies on Theorem 37 and is therefore \ufb02awed. Use the proof given in\nSimon-Gabriel and Sch\u00a8olkopf (2018) instead (which is a shortened version of the alternative proof\ngiven in Appendix A.13).\nThat (i) and (iv) are equivalent is Theorem 9 of Sriperumbudur et al. (2010b). Looking at\nthe proof, it is straightforward to see that they are also equivalent to (ii). It is clear that (iii)\nimplies (ii). Thus we only need to show that (iv) implies (iii).\nSuppose (iv) holds.\nThe Fourier transforms of \u2202(p,p)k(\u02c6x, \u02c6y) = (\u22121)p\u22022p\u03c8(\u02c6x \u2212\u02c6y) satisfy\nF( \u22022p\u03c8) = (\u2212i)|p|\u02c6\u03bep F\u03c8.\nThus they do also have full support.\nSo for each |p| \u2264m, \u2202(p,p)k\nis characteristic to Mf. Thus Theorem 37 applies and k is characteristic to Dm\nL1.\n9The result of this corollary still holds, despite the proof of Theorem 37 being \ufb02awed. The alternative proof\ngiven in Appendix A.13 and shortened in the JMLR version of this paper is independent of Theorem 37.\n19\nWe give an alternative proof that is independent of Theorem 37 in Appendix A.13.\nThis result calls for comments. First, it shows that the popular Gaussian kernel k(x, y) =\nCe\u2212(x\u2212y)2/\u03c32 (where C, \u03c32 > 0) is c\u221e\n0 -universal. Now, informally, what does it mean when a KME\nis injective over D\u221e\nL1? A distribution D is by de\ufb01nition determined by all the values D(\u03c8) it takes,\nwhen \u03c8 runs over the space of test functions C\u221e\nc (Rd). Even better: let \u03c8\u01eb \u22650 be a function in\nC\u221e\nc\nwith support contained in a compact set of diameter \u2264\u01eb and such that\nR\n\u03c8\u01eb(y) dy = 1. Then\nthe convolutional product [D \u2217\u03c8\u01eb](x) := Dy(\u03c8\u01eb(x \u2212y)) is a function in C\u221e\nc\nthat converges in D\u221e\nto D when \u01eb \u21920. This often leads physicists to interpret Dy(\u03c8\u01eb(x \u2212y)) as the measurement at\npoint x of some natural phenomenon (modelled by) D by an imprecise instrument \u03c8\u01eb. They see\nDy(\u03c8\u01eb(x \u2212y)) as a \u201cweighted mean\u201d of D on a small compact set (of diameter at most \u01eb) around\nx. The more precise the measurement device \u03c8\u01eb, the smaller the \u01eb, the better the measurement.\nIn particular, a physicist need not de\ufb01ne a natural phenomenon D by its exact values in every\npoint x, but rather by all the possible measurements that one could do of D at every point x. This\nbrings us back to c\u221e\n0 -universality. For it implies that you do not need a whole family of functions\n(\u03c8\u01eb)\u01eb>0 (and their translations) to characterise a distribution in D\u221e\nL1. A single function \u03c8 (and its\ntranslations) su\ufb03ces: indeed, \u03c8 having compact support, supp F\u03c8 = Rd, thus the kernel \u03c8(\u02c6x\u2212\u02c6y) is\nc\u221e\n0 -universal. So any distribution D \u2208D\u221e\nL1 is uniquely characterised by Dy(\u03c8(\u02c6x\u2212y)), the KME of\nD w.r.t. \u03c8. Informally speaking, it means we do not need in\ufb01nite measuring devices: one device\n(translated over the input space) su\ufb03ces10 to distinguish D from all other natural phenomena\nmodelled by D\u221e\nL1. And we are back to usual functions: a function f is characterised by the graph\nx 7\u2192f(x); an integrable distribution D is characterised by the graph x 7\u2192Dy(\u03c8(x \u2212y)).\nWe now return to more mathematically rigorous statements and discuss cm-universality.\n6.2\ncm-Universal Kernels\nWe start with the analogue of Theorem 37, but for characteristic kernels over Em. Its proof is\nliterally the same, with Mf and DL1 being replaced by Mc and E respectively.\nTheorem 40. PROOF FLAWED. Let k be a kernel in C(m,m). If, for any p \u2208Nd with |p| \u2264m,\n\u2202(p,p)k is characteristic to Mc, then k is characteristic to Em.\nObviously, any characteristic kernel over Dm\nL1 is also characteristic to Em; or if one prefers:\nany cm\n0 -universal kernel is also cm-universal. Now, in Corollary 38, the condition that the support\nof F\u03c8 be all Rd may seem quite non-restrictive. But it actually prevents some usual kernels from\nbeing c0-universal. An example in X = R: the kernel sinc(\u02c6x \u2212\u02c6y) (where sinc(x) := sin(x)/x for\nany x \u0338= 0 and 1 otherwise). However, the next theorem and the next proposition will show that\nthe sinc kernel, as the vast majority of stationary continuous kernels used in practice, is c-universal\n(and even c\u221e-universal). We thereby re\ufb01ne Proposition 17 of Sriperumbudur et al. (2010a) and\nprovide a converse. For the proof, see Appendix A.14.\n10The physicist however is not out of the woods, for if he wants to perfectly reconstruct D, he now needs to\nperfectly know his magnifying glass \u03c8. And anyone working in deconvolution knows how hard this can be!\n20\nTheorem 41. Let k(\u02c6x, \u02c6y) = \u03c8(\u02c6x \u2212\u02c6y) \u2208C(m,m) be a stationary kernel with X = R. For a set S,\nwe note #S its cardinal. The following two statements are equivalent.\n(i) k is not characteristic to Dm\nL1.\n(ii) The support S of the distributional Fourier transform F\u03c8 is at most countable and, there\nexists a constant M \u2208R such that, for any r > 0\n#{s \u2208S | |s| \u2264r} \u2264Mr .\n(8)\nEquation (8) upper bounds the average density of the support S of F\u03c8 in R. In other words,\nto prevent a stationary and smooth enough kernel k from being characteristic to Dm\nL1, the support\nof F\u03c8 needs not only be countable, but its density in R should also be su\ufb03ciently low. Such\nkernels \u03c8 include for example any periodic or any \ufb01nite sum of periodic stationary kernels.\nTheorem 41 accounts only for the case X = R. When X = Rd, we expect a similar statement\nto hold, with Mr replaced by Mrd. This yet remains to be proven. However, we may state and\nprove (in Appendix A.15) the following proposition.\nProposition 42. Let k(\u02c6x, \u02c6y) = \u03c8(\u02c6x \u2212\u02c6y) \u2208C(m,m) be a stationary kernel with X = Rd.\n(i) The kernel is characteristic to Em i\ufb00it is characteristic to Mc.\n(ii) If the kernel is not characteristic to Em, then the support of the Fourier transform of \u03c8 is\nat most countable.\nAs a \ufb01nal remark before moving to the next section, note that one can give su\ufb03cient conditions\n(for a kernel to be characteristic to Em) that apply to a slightly more general class of kernels\nthan the stationary ones. They apply to so-called conditionally positive de\ufb01nite (c.p.d.) kernels,\nand their proof uses a generalisation of the Bochner theorem.\nAs this needs too much of an\nintroduction to c.p.d. kernels, with rather small bene\ufb01ts in the end, we move these discussions\nto Appendix C. Among other things, these su\ufb03cient conditions prove that the Brownian motion\nkernel k(\u02c6x, \u02c6y) := min(|\u02c6x|, |\u02c6y|) (x, y \u2208R) is characteristic to the set of measures with compact\nsupport.\n7\nTopology Induced by k\nSo far, we embedded various spaces of distributions into H\u2032\nk and studied whether these embeddings\nare injective. We now inquire their continuity properties w.r.t. various topologies.\nEach distribution space D de\ufb01ned in Section 2 can be endowed with its strong dual, its weak\ndual or any other topology. Naturally, when embedding D into Hk one may ask: does it preserve\nthe topological structure of D? More precisely:\n(Q1) If a sequence Dn \u2208D converges to D \u2208D, does \u03a6k(Dn) converge to \u03a6k(D)? Or more\nabstractly: is \u03a6k continuous?\n(Q2) If \u03a6k(Dn) converges to \u03a6k(D) in Hk, does Dn converge to D?\nIn this section we answer those questions for various sets of distributions D (and various topolo-\ngies). Table 2 summarises these answers.\nPrevious studies already addressed those questions (Guilbart, 1978 or more recently\nSriperumbudur et al., 2010b; Sriperumbudur, 2013).\nHowever, all of them focused on Mf or\n21\nF\u2032\nDm\nL1\nEm\nE\u221e\nMf\nMc\nM+\nP\nb\n\u2283P.43\n\u2283P.43\n\u2283P.43\n\u2283P.43\n\u2283P.43\n\u2283P.43\n\u2283P.43\n\u2283P.43\nb \u2229B\n\u2283P.43\n\u2283P.43\n\u2283P.43\n=C.50\n\u2283P.43\n\u2283P.43\n\u2283P.43\n\u2283P.43\nw\n\u0338\u2283S.7.3.1\n\u0338\u2283S.7.3.1\n\u0338\u2283S.7.3.1\n=Th.55\nw \u2229B\n\u2282Th.48\n=C.50\n=C.50\n\u0338\u2283S.7.3.1\n=C.50\n\u0338\u2283S.7.3.1\n=Th.55\n\u03c3\nn.d.\nn.d.\nn.d.\nn.d.\n\u2283P.53\n\u0338\u2283P.53\n=Th.55\n\u03c3 \u2229B\nn.d.\nn.d.\nn.d.\nn.d.\n\u2283P.53\n=R.51 (ii)\n\u0338\u2283P.53\n=Th.55\nTable 2: Relations between topology Tk, induced by the kernel metric dk, and other topologies.\nLetters b, w and \u03c3 designate the strong dual, the weak dual and the narrow topologies. Th, P, C,\nS, R and n.d. stand for Theorem, Proposition, Corollary, Section, Remark and not de\ufb01ned. For\nexample, Column 3 Line 3 reads: \u201cOver any bounded subset B of Dm\nL1, and under the assumptions\nof Proposition 43, the strong topology of Dm\nL1 dominates Tk: b(Dm\nL1, Cm\n0 )\u2229B \u2283Tk \u2229B\u201d. Column 3\nLine 4 reads: \u201cThe weak topology of Dm\nL1 need not be stronger than Tk.\nSee discussions in\nSection 7.3.1\u201d. The two last lines of last column feature this section\u2019s main theorem: On a locally\ncompact Hausdor\ufb00space X, a bounded kernel k metrises the weak convergence of probability\nmeasures i\ufb00k is continuous and characteristic to P.\non subsets like M+ (positive \ufb01nite measures) or P. Sriperumbudur et al. (2010b) for example\ninquire on which conditions a kernel k metrises the narrow convergence topology over P. In other\nwords, they seek positive answers to (Q1) and (Q2) with D = P, when it is equipped with its\nnarrow topology and Hk with its usual norm. We, on the contrary, start with the general case\nwhere D is a subset of Em or Dm\nL1 and only then focus on subsets of Mf.\nAfter a few preliminary remarks and notations (Section 7.1), we show that, under very general\nassumptions on D, the answer to (Q1) is positive for all embeddable distribution sets D, when\nboth D and Hk are equipped with their strong (in short: b \u2192b) or with their weak (w \u2192w) dual\ntopology (Section 7.2). We then re\ufb01ne those results by analysing the case w \u2192b (Section 7.3).\nWe then address (Q2), starting with the general case where D could be any subset of Em or\nDm\nL1. Quickly, however, we focus on bounded subsets of Em and Dm\nL1. In Section 7.5, we then\nconcentrate on the case where D is Mf or P, equipped with the narrow topology. We \ufb01nish with\na short investigation in Section 7.6 of the questions: \u201cWhen is the KME surjective? And when\ndoes Hk identify with a (topological) subspace of distributions?\u201d.\n7.1\nPreliminary Remarks and Notations\nIn all this section, D designates an embeddable set of distributions. For a subset B of a set S with\ntopology T, we note T \u2229B the relative topology induced by D in B.\nIdenti\ufb01cation of Hk, Hk\n\u2032 and H\u2032\nk.\nHk and Hk\n\u2032 are isomorphic, algebraically and topologically,\nvia the Riesz representation map (see Appendix D). As depicted in Diagram (4), when Hk and\nHk\n\u2032 are identi\ufb01ed, the KME \u03a6k is the complex conjugate of the restriction map D 7\u2192D\n\f\f\nHk.\nBut in this section, we will only be studying its continuity properties: they are the same, with\n22\nor without the complex conjugation. So we will drop all complex conjugation bars and simply\nconsider that the KME \u03a6k is the restriction map (without complex conjugation) from D into H\u2032\nk.\nMathematically speaking, we identify Hk\n\u2032 and H\u2032\nk via the complex conjugation map f 7\u2192\u00aff. In\npractice, it simply means that instead of writing for instance b(Hk\n\u2032, Hk), w(Hk\n\u2032, Hk) or \u00afD\u03b1 \u2192\u00afD,\nwe will write b(H\u2032\nk, Hk), w(H\u2032\nk, Hk) and D\u03b1 \u2192D. It will be useful for this section to always\nremember that, when Hk continuously embeds into F, then the KME \u03a6k is the transpose \u0131\u22c6of\nthe canonical embedding map \u0131 : Hk \u2192F, f 7\u2192f and coincides with the restriction map of F\u2032 to\nHk. In short: if Hk \u0592\u2192F, then \u03a6k = \u0131\u22c6.\nMetric and strong topology induced by k.\nThe strong topology induced by k in D is simply\nthe topology induced by the semi-metric dk de\ufb01ned by: for any D, T \u2208D,\ndk(D, T) := \u2225\u03a6k(D) \u2212\u03a6k(T)\u2225k .\nWe will call dk the kernel(-induced) (semi-)metric. When D is a vector space, this semi-metric is\ninduced by the semi-inner product\n\u27e8D | T\u27e9k := \u27e8\u03a6k(D) | \u03a6k(T)\u27e9k .\nThe \u201csemi\u201d means that the topology induced by dk may not be Hausdor\ufb00: dk(D, T) = 0 may\nnot imply D = T. dk is a (non-degenerate) metric i\ufb00the KME is injective. If it is, we note\nb(H\u2032\nk, Hk) \u2229D the induced topology. A net11 (D\u03b1)\u03b1\u2208A in D converges to D \u2208D for the kernel\nsemi-metric i\ufb00\u2225D\u03b1 \u2212D\u2225k \u21920.\nWeak topology induced by k.\nWe de\ufb01ne the weak topology induced by k over an embeddable\nset of distributions D by its convergent nets. A net D\u03b1 in D converges to D in D for the weak\ntopology induced by k i\ufb00for any f \u2208Hk, D\u03b1(f) \u2192D(f). Said di\ufb00erently, the weak topology\ninduced by k over D is the topology of pointwise convergence (with points) in Hk. Here again,\nthis topology is Hausdor\ufb00i\ufb00the KME is injective over D. If it is, we note w(H\u2032\nk, Hk) \u2229D this\ntopology.\nInduced topologies, continuity of \u03a6k and metrisation.\nRemember that the strong (resp.\nweak) topology induced by k is weaker than the original topology T of D i\ufb00the KME \u03a6k is\ncontinuous when D is equipped with T and Hk with its RKHS norm (resp. with its weak topology).\nWe then write \u03a6k : DT \u0592\u2192(H\u2032\nk)b (resp. \u03a6k : DT \u0592\u2192(H\u2032\nk)w ). When the topology induced by dk in\nD equals the original topology of D, k is said to metrise the topology of D. In that case, a net\nD\u03b1 converges to D in D for the original topology of D i\ufb00\u2225D\u03b1 \u2212D\u2225k \u21920.\nContinuity and sequential continuity.\nLet X and Y be two arbitrary topological spaces and\nf a function from X to Y . The function f is continuous i\ufb00for any net x\u03b1 that converges to x\nin X, f(x\u03b1) converges to f(x) in Y . In particular, if f is continuous, then f is also sequentially\ncontinuous: for any sequence xn of X, if xn \u2192x, then f(xn) \u2192f(x). If X is a so-called se-\nquential space, then sequential continuity also implies continuity. But in general, the converse is\nwrong. Fortunately, many spaces are sequential. For example, any metric space \u2014in particular,\nany Banach or Fr\u00b4echet space\u2014 is sequential. Unfortunately, loc. cv. TVSs need not be sequential.\n11A net is a generalisation of a sequence, in which the directed index set A need not be countable.\n23\nFor instance, Dm\nL1 is sequential i\ufb00m is \ufb01nite. Thus, in all this section, we will carefully distin-\nguish continuity and sequential continuity. In particular, question (Q1) asks whether the KME is\nsequentially continuous over a space D. For practical purposes, that is all what matters. Never-\ntheless, we will mainly establish (non-sequential) continuity properties of the KME. The reader\nshould keep in mind that they imply sequential continuity. In short: if \u03a6k : DT \u0592\u2192Hk, then (Q1)\nis answered positively. A similar remark holds for (Q2). Indeed, let T1 and T2 be two topologies\nover a space X. Obviously, when T1 is stronger than T2, then T1 is also sequentially stronger\nthan T2: if xn \u2192x in T1 then xn \u2192x in T2. From metric spaces, we are used to the converse\nbeing true: if T1 is sequentially stronger than T2, then T1 is stronger than T2. However, when T2\nis not sequential (meaning that X equipped with T2 is not a sequential space), this converse may\nfail to hold: sequentially stronger need not imply stronger. Coming back to question (Q2), we see\nit asks whether the topology induced in D by \u03a6k is sequentially stronger than the initial topology\nof D. There again, for practical purposes, that is all what matters. Nevertheless, we will mainly\nestablish statements such as: T1 is stronger than (we also say dominates) T2, in which case we\nwrite T1 \u2283T2.\n7.2\nEmbeddings of Dual Spaces are Continuous\nTo start, let us remind how one usually embeds Mf into the space of distributions D\u221e. One\nsimply notes that the space C\u221e\nc\nis continuously contained and dense in C0. Thus the transpose \u0131\u22c6\nof the canonical embedding map \u0131 : f 7\u2192f de\ufb01nes an injective linear map from the dual Mf of C0\ninto the dual D\u221eof C\u221e\n0 . (Note that \u0131\u22c6is the restriction map \u00b5 7\u2192\u00b5\n\f\f\nC\u221e\nc .) Thus identifying Mf\nwith its image in D\u221e, one says that Mf is a subspace of D\u221e. But not only is \u0131\u22c6injective, it is also\ncontinuous, when both duals are equipped with their strong or with their weak topology. (That\nis why, when Mf is equipped with its strong topology, the total variation norm, Mf is not only a\nspace of distributions, it is also a topological space of distributions.) This is simply an instance\nof a more general fact: if a loc. cv. TVS F1 embeds continuously into another F2 via a linear\nmap \u0131, then the transpose \u0131\u22c6is a continuous linear embedding from F\u2032\n2 into F\u2032\n1, when F\u2032\n1 and F\u2032\n2\ncarry either both their weak or both their strong dual topology (Treves, 1967, Proposition 19.5\nand Corollary). Applying this to F2 = Hk yields:\nProposition 43. Let X be any topological space (not necessarily locally compact and Hausdor\ufb00).\nIf Hk continuously embeds into F, then the KME \u03a6k : F\u2032 \u2192H\u2032\nk is continuous, when F\u2032 and H\u2032\nk\ncarry either both their strong or both their weak topologies. In short:\nHk \u0592\u2192F\n=\u21d2\n\u001a\u03a6k : F\u2032\nb \u0592\u2192(H\u2032\nk)b\n\u03a6k : F\u2032\nw \u0592\u2192(H\u2032\nk)w .\n(9)\nThe conclusion of Proposition 43 concerning continuity w.r.t. weak topologies is actually trivial!\nIt states that for a net D\u03b1 and D in F\u2032, if D\u03b1(f) \u2192D(f) for any f \u2208F, then D\u03b1(f) \u2192D(f)\nfor any f \u2208Hk (which is clear, because Hk \u2282F !). The conclusion concerning strong continuity\nis more interesting, although not much more di\ufb03cult to prove. It says: if D\u03b1 \u2192D in F\u2032\nb, then\n\u2225D\u03b1 \u2212D\u2225k \u21920. Thus, when Hk embeds continuously into F, then Proposition 43 answers (Q1)\npositively, when F\u2032 and H\u2032\nk are equipped both with their strong or both with their weak topologies.\nExample 2. We illustrate Proposition 43 on spaces F that are not listed in Section 2. Let k be\na bounded kernel. Equip X with the topology induced by the kernel-induced semi-distance \u03c1k,\nde\ufb01ned as: for any x, y \u2208X, \u03c1k(x, y) := dk(\u03b4x, \u03b4y) = \u2225\u03b4x \u2212\u03b4y\u2225k. (X may or may not be locally\ncompact and Hausdor\ufb00.) Then Hk continuously embeds into any of the following spaces:\n24\nCb\nthe space of bounded \u03c1k-continuous functions, equipped with the supremum norm \u2225f\u2225\u221e.\nCL := {f \u2208CX | \u2225f\u2225L < \u221e}\nthe space of Lipschitz continuous functions, equipped with the Lipschitz-norm \u2225f\u2225L :=\nsup{|f(x) \u2212f(y)|/\u03c1k(x, y) | x, y \u2208X , \u03c1k(x, y) \u0338= 0}.\nCBL := {f \u2208CX | \u2225f\u2225BL < \u221e}\nthe space of bounded and Lipschitz continuous functions, equipped with the BL-norm\n\u2225f\u2225BL := \u2225f\u2225\u221e+ \u2225f\u2225L.\nThe duals of these spaces contain all probability measures. The respective metrics induced by\ntheir dual norms on P are: the total variation metric, the Kantorovich metric (which coincides\nwith the Wasserstein metric when X is separable) and the Dudley metric. Proposition 43 shows,\nwithout any computation, that all these metrics dominate the kernel-induced semi-metric dk.\n7.3\nWhen Does F\u2032\nw Continuously Embed into H\u2032\nk?\nProposition 43 answers (Q1) positively when F embeds continuously into Hk, and when both\nF\u2032 and H\u2032\nk are equipped with their strong (resp. weak) dual topology. The weak dual topology\nbeing weaker than the strong one, under the assumption of Proposition 43 we obviously also have\n\u03a6k : F\u2032 \u0592\u2192(H\u2032\nk)w. How about \u03a6k : F\u2032\nw \u0592\u2192H\u2032\nk ? This will be the focus now.\nThe answer depends on the speci\ufb01c space F\u2032, for the weak or strong dual topologies may vary\nfrom one dual to another. The weak (resp. strong) dual topology of Em, for instance, is strictly\nstronger than the weak (resp. strong) topology it carries when seen as a subspace of Dm\nL1 (resp.\n(Dm\nL1)w). As we will see, it will also depend on some assumptions made on the kernel.\n7.3.1\nDoes (Dm\nL1)w Continuously Embed into H\u2032\nk?\nIn general no: (Dm\nL1)w need not continuously embed into H\u2032\nk. For instance, suppose k is stationary\nand take a sequence xn \u2208X such that xn \u2192\u221e.\nThen \u03b4xn converges to 0 in (Dm\nL1)w, but\n\u2225\u03b4xn\u22252\nk = k(xn, xn) = k(x1, x1). Thus \u03b4xn does not converge in the strong topology induced by k.\n7.3.2\nDoes (Em)w Continuously Embed into H\u2032\nk?\nUnder mild smoothness assumptions on k, yes: (Em)w continuously embeds into H\u2032\nk \u2014at least\nsequentially. More precisely, inspired by a remark from Matheron (1973), we get the following\ntwo theorems.\nTheorem 44. The following six conditions are equivalent.\n(i) k is continuous.\n(ii) k(., \u02c6x) : X \u2192H\u2032\nk, x 7\u2192\u03a6k(\u03b4x) = k(., x) is continuous.\n(iii) \u03a6k : (Mc)w \u2192H\u2032\nk is continuous over the bounded sets of (Mc)w.\n(iv) \u03a6k : (M\u03b4 \u2229M+)\u03c3 \u2192H\u2032\nk is continuous.\n(v) \u03a6k : (P)\u03c3 \u2192H\u2032\nk is continuous.\n(vi) \u03a6k : (M+)\u03c3 \u2192H\u2032\nk is continuous.\nIf these conditions are satis\ufb01ed, all these maps are sequentially continuous.\n25\nThe \u03c3 in (v) and (vi) refer to the narrow topology \u03c3(Mf, Cb), which will be studied in more\ndetails in Section 7.5. For the general case D = Em, we state:\nTheorem 45. If k is (m, m)-times continuously di\ufb00erentiable, then the embedding \u03a6k\n:\n(Em)w \u2192H\u2032\nk is continuous over the bounded sets of (Em)w. In particular, \u03a6k is sequentially\ncontinuous over (Em)w.\nProof. [of Theorems 44 and 45] First, a word on Theorem 44. That (i) and (ii) are equivalent\nis proven for example by Steinwart and Christmann (2008, Theorem 4.29).\nFurthermore, it is\nclear that any of the points (iii) to (vi) imply (ii) and that (vi) \u21d2(v) \u21d2(iv). Thus, we are left\nwith proving that (i) implies both (iii) and (vi). The proof of these implications as well as of\nTheorem 45 follow the same proof-schema, which we illustrate on the proof of (i) \u21d2(iii).\n(i) Let \u00b5\u03b1 be a net contained in a bounded subset B of Em such that \u00b5\u03b1 \u21920 in w(Mc, C) \u2229B.\n(ii) Show that, because k \u2208C(X \u00d7 X), the map\nB :(Mc(X))w \u00d7 (Mc(X))w\u2212\u2192(Mc(X \u00d7 X))w\n(\u00b5, \u03bd)\n7\u2212\u2192\u00b5 \u2297\u03bd\nis (weakly) continuous when restricted to B \u00d7 B.\n(iii) Conclude that \u2225\u00b5\u03b1\u22252\nk = [\u00b5\u03b1 \u2297\u00af\u00b5\u03b1](k) \u21920.\nAppendix A details how to adapt the input and output sets of B to prove each remaining impli-\ncation. The continuity of each such map B is also proven there. Finally, continuity over bounded\nsets implies sequential continuity (over the entire space D), because any convergent sequence is\nbounded. (In general, however, it need not imply continuity over unbounded sets.)\nRemark 46. Theorem 44 shows that even if Mc embeds into H\u2032\nk, the embedding may not be\ncontinuous from (Mc)w into H\u2032\nk. Indeed, there exist non-continuous kernels such that Hk \u2282C\n(see Lehto, 1952). With Proposition 3, this implies Hk \u0592\u2192C, thus Mc embeds into H\u2032\nk. But as k\nis not continuous, (Mc)w \u0592\u2192H\u2032\nk does not hold.\nIn Theorem 44, one may replace (Mc)w by its dense subset M\u03b4 equipped with the induced\nweak topology w(Mc, C) \u2229M\u03b4. This shows we could have de\ufb01ned the KME as follows.\n(i) Embed M\u03b4 into Hk ;\n(ii) Note that this embedding is continuous when M\u03b4 is equipped with the weak topology induced\nby Mc;\n(iii) Extend this embedding by continuity to Mc. Theorem 44 then guarantees that this extension\ncoincides with our previously de\ufb01ned KME.\nThis technique of continuous extension has often been used as a starting point to embed measures\ninto Hk (Matheron, 1973; Guilbart, 1978 or Berlinet and Thomas-Agnan, 2004, Chapter 4). The\nfollowing proposition shows that for E\u221e, this continuous extension technique leads to the same\nKME de\ufb01nition as ours. Indeed, M\u03b4 is dense in E\u221e, when equipped with the weak or strong\ntopology. Thus Theorem 45 immediately yields\nProposition 47. Let k \u2208C(\u221e,\u221e). The KME of E\u221eis the unique continuous linear extension of\nthe KME of M\u03b4, when M\u03b4 is equipped with the (weak or strong) topology induced by E\u221e.\n26\n7.4\nWhen does k metrise the topology of F\u2032?\nThe two preceding sections dealt solely with question (Q1). They inquired under which conditions\nthe weak and strong topologies induced by k in a given dual F\u2032 are weaker than the original\ntopology. To answer question (Q2), we now investigate under which conditions they are stronger.\nFirst, the kernel must be characteristic to F\u2032. If it was not, the weak and strong kernel induced\ntopologies would not be Hausdor\ufb00(and the semi-metric dk not be a metric), thus could not be\nstronger than any topology we will consider. So from now on, we assume that k is characteristic\nto the set to embed.\nSecond, we will suppose that F is barrelled (see De\ufb01nition in Appendix D). This is merely a\ntechnical assumption used to apply the Banach-Steinhaus theorem (see Appendix D, Theorem 79)\nin the proof of Theorem 48. In practice, it is very unrestrictive: any Banach, Fr\u00b4echet or Limit-\nFr\u00b4echet12 space is barrelled. In particular, all function spaces de\ufb01ned in Section 2 are barrelled,\nexcept (Cm\nb )c and perhaps CX.\nThird, we will focus on bounded subsets of F\u2032. Let us justify why. First, if a sequence converges\nin a Hausdor\ufb00topology, then it is bounded. Thus, two topologies that coincide on every bounded\nset de\ufb01ne the same convergent sequences. Second, one of the most used sets of distributions is\nbounded: the set of probability measures P. (Indeed, the topology of Mf is given by the total\nvariation norm \u2225.\u2225TV and, for any P \u2208P, \u2225P\u2225TV =\nR\ndP = 1.) But would P still be bounded,\nhad Mf been equipped with its weak or with its narrow topology? Yes. This leads to a third good\nreason to consider bounded sets: if F is barrelled, then the Banach-Steinhaus theorem implies\nthat all topologies stronger than the weak but weaker than the strong dual topology de\ufb01ne the same\nbounded sets over F\u2032. In other words, for any such topology, if F is barrelled, a subset B of F\u2032 is\nbounded i\ufb00\n\u2200f \u2208F ,\nsup\nD\u2208B\n|D(f)| < \u221e.\n(10)\nThe narrow topology is clearly such a topology: it is stronger than the weak-star, but weaker than\nthe strong dual topology.\nTo introduce our next theorem, consider a space of \ufb01nite Borel measures de\ufb01ned on a locally\ncompact Hausdor\ufb00space X. There are two common options to construct Mf as a dual space.\nOne is to consider the space Cc endowed with the topology of uniform convergence. The other\nis to consider its completion, C0. Their duals both identify with Mf. But do their weak dual\ntopologies coincide? If X is not compact, they do not.13 The Banach-Steinhaus theorem, however,\nguarantees that, at least, they coincide on all bounded subsets of Mf. The following theorem,\nproved in Appendix A.12, states essentially the same, but with Hk and F instead of Cc and C0.\nTheorem 48. Let F be a barrelled space such that Hk \u0592\u2192F and that k be universal over F. On\nany bounded subset B of F\u2032, the weak-star topology coincides with the weak topology induced by\nk. Thus the topology induced by the semi-metric dk dominates the weak-star topology over any\nbounded subset of F\u2032 and\nb(F\u2032, F) \u2229B \u2283b(H\u2032\nk, Hk) \u2229B \u2283w(F\u2032, F) \u2229B = w(H\u2032\nk, Hk) \u2229B .\n(11)\nTheorem 48 may seem abstract, but for sequences, it simply means the following.\nCorollary 49. Let F and k be as in Theorem 48. Let Dn be a bounded sequence in F\u2032, and\nD \u2208F\u2032.\n12Example of Limit-Fr\u00b4echet spaces which are not Fr\u00b4echet spaces are Cm\nc\nand E\u221e(Schwartz, 1978).\n13Proof: Equipped with w(Mf, C0), the dual of Mf is C0. But equipped with w(Mf, Cc) it is Cc.\n27\n(i) Dn(f) \u2192D(f) for any f \u2208F i\ufb00\u00afDn(f) \u2192\u00afD(f) for any f \u2208Hk.\n(ii) If \u2225Dn \u2212D\u2225k \u21920 , then Dn(f) \u2192D(f) for any f \u2208F.\n(iii) If Dn \u2192D in b(F\u2032, F), then \u2225Dn \u2212D\u2225k \u21920.\nWith F = Cm, we get the following important and stunning corollary.\nCorollary 50. Let k be a kernel in C(m,m), and let B be a bounded subset of Em. Then\nb(F\u2032, F) \u2229B \u2283b(H\u2032\nk, Hk) \u2229B = w(F\u2032, F) \u2229B = w(H\u2032\nk, Hk) \u2229B .\n(12)\nIn particular, on bounded subsets of Em, the following statements are equivalent.\n(i) For any f \u2208F , Dn(f) \u2192D(f).\n(ii) For any f \u2208Hk , Dn(f) \u2192D(f).\n(iii) \u2225Dn \u2212D\u2225k \u21920.\nMoreover, when m = \u221e, the inclusion in (12) is an equality and (i) - (iii) are equivalent to:\n(iv) Dn \u2192D in b(E\u221e, C\u221e).\nProof. Equality (12) is a straightforward consequence of Theorems 45 and 48. The statements\nconcerning the special case m = \u221estem from the fact that, because E\u221eis a Montel space, on\nbounded subsets, its weak and strong dual topologies coincide.\nBefore closing this subsection, we group a few remarks.\nRemark 51.\n(i) Two topologies that coincide on every bounded subset of a topological space X need not\ncoincide on the whole space X. However, as any convergent sequence is bounded, they both\nde\ufb01ne the same convergent sequences. (Thus, if both topologies are sequential and coincide\non the bounded subsets of X, then they coincide on the whole space X.)\n(ii) A subset B of Mf is bounded i\ufb00it is uniformly bounded for the total variation norm, which\nis equivalent to the seemingly weaker condition (10). It is bounded in Mc i\ufb00it is bounded\nin Mf (or even Mr) and if all its elements have their support in a common compact subset\nof X. In that case, w(Mc, C) \u2229B = \u03c3(Mc, Cb) \u2229B. More generally, B is bounded in Em i\ufb00\nit is bounded in Dm\nL1 (or even Dm) and if the support of its elements are all contained in a\ncommon compact subset of X.\n(iii) In Theorem 48, F continuously embeds into Hk, thus F\u2032 continuously embeds into H\u2032\nk.\nSo bounded subsets of F\u2032 are also bounded for the kernel-induced metric. In general, the\nconverse is false: if a subset B of F\u2032 is dk-bounded, it need not be bounded in the original\ntopology of F\u2032. Consider for example an unbounded sequence xn in X = Rd and a bounded\nkernel k. Then \u03a6k(\u03b4xn) is bounded in Hk , but \u03b4xn is not bounded in Mc , because the\nsupports of the measures \u03b4xn are not contained in a common compact subset of X.\n(iv) Corollary 50 states that on bounded subsets B of Em, the weak and strong topologies of\nHk coincide. This may seem surprising, as, in an in\ufb01nite dimensional Hilbert space, even\non bounded sets, weak and strong topologies in general do not coincide.\nFor example,\nBessel\u2019s inequality shows that any in\ufb01nite orthonormal sequence converges weakly to 0. But\nit certainly does not converge strongly to 0. However, there again, B is not any bounded\nsubset of Hk : it is both included in Em and bounded for its topology (which is stronger\nthan that of Hk !).\n28\nSo far, we focused on the two most common dual topologies: the weak and the strong dual\ntopologies. Probability theory, however, certainly uses the narrow topology \u03c3(Mf, Cb) even more.\nSo let us see how it compares with the topologies induced by k.\n7.5\nWhen does k Metrise the Narrow Convergence?\nA kernel k metrises the narrow topology on a set B \u2282Mf(X) if the strong topology induced by\nk coincides on B with the narrow topology; i.e., if: b(H\u2032\nk, Hk) \u2229B = \u03c3(Mf, Cb) \u2229B. When B\nis a bounded set consisting of positive measures over a metrisable and separable (not necessarily\nlocally compact) domain X, the narrow convergence topology is metrisable14. In that case, it is\nentirely de\ufb01ned on B by its convergent sequences, and we can equivalently de\ufb01ne: k metrises the\nnarrow topology on a bounded set B \u2282Mf of positive measures i\ufb00: for any \u00b5, \u00b51, \u00b52, . . . \u2208Mf(X),\n\u00b5n \u2192\u00b5 narrowly i\ufb00\u03a6k(\u00b5n) \u2192\u03a6k(\u00b5) in RKHS norm.\nThe quest for kernels that do metrise the narrow topology dates back at least to the 1970s.\nCriteria were already given by Guilbart (1978) (see also Berlinet and Thomas-Agnan, 2004, The-\norems 100 and 101). More recently, Sriperumbudur et al. (2010b, Theorem 23)15 pointed out that\non compact domains X, any c0-universal kernel metrises the narrow convergence topology on P.\nPutting together Theorems 44 and 48 immediately yields a similar result (Corollary 52). But\nthings become trickier on unbounded domains. As we will see, the narrow topology is in general\nstronger than (the topology of) the kernel-induced metric dk (Proposition 53), and often strictly\nstronger (Proposition 54). But there is an important exception: when focusing on probability\nmeasures (Proposition 71)!\nWe start with the case when the input set X is compact.\nThen Mf(X) = Mc(X) and\nw(Mf, C0) = w(Mc, C). Thus Theorems 44 and 48 give:\nCorollary 52. Let X be a compact Hausdor\ufb00space. A kernel k metrises the narrow topology on\nany bounded set of Mf i\ufb00it is continuous and c0-universal. In particular, a continuous c0-universal\nkernel metrises the narrow topology over P.\nProof. If k is continuous and bounded, then Hk \u0592\u2192C. Thus, Theorem 48 shows that the kernel\nmetric dk is stronger than the narrow convergence topology, and Theorem 44 that it is weaker. So\nboth topologies coincide. Conversely, if both topologies coincide, then k must be characteristic to\nMf (for dk to be metric) and the implication (v) \u21d2(i) of Theorem 44 shows that k is continuous.\nWe now turn to the general case, where X is a locally compact Hausdor\ufb00set. Reformulating\nTheorem 44 (which also holds, even if X is not paracompact), we immediately get:\nProposition 53 ((Mf)\u03c3 \u0592\u2192Hk). Let X be a locally compact Hausdor\ufb00space and let k be a\nbounded continuous kernel on X. The topology induced by the semi-metric dk in Mf is weaker\nthan the narrow topology.\nThere exists an analogue of Proposition 53 when, instead of being locally compact, X is a\nseparable metric space (see Appendix A.17).\nSummarising, we see that, whether X be a locally compact Hausdor\ufb00or a separable metric\nspace, the narrow topology is in general stronger than the kernel-induced metric. Conversely,\n14They are for example metrised by the Dudley or the Prokhorov metrics (see Dudley, 2002).\n15Note that they call \u201cweak\u201d what we call \u201cnarrow\u201d.\n29\nGuilbart (1978) showed that, on any separable metric space, there exist kernels that do metrise\nthe narrow topology over the set of \ufb01nite positive measures M+. However, the following proposition\n(proved in Appendix A.18) shows that being continuous and characteristic to Mf is not su\ufb03cient.\nProposition 54. Let X = Rd and let k be a continuous stationary kernel k(\u02c6x, \u02c6y) = \u03c8(\u02c6x\u2212\u02c6y) such\nthat \u03c8 \u2208C0. Then k does not metrise the narrow convergence topology over any subset of Mf(Rd)\nthat contains P \u222a{0}.\nExamples of such sets include Mf(Rd), M\n\u22641\nf (Rd), M\n\u22641\n+ (Rd), but not P(Rd). We do not know\nwhether this result extends to any (possibly non-stationary) kernel such that Hk \u0592\u2192C0(Rd).\nProposition 54 is somewhat disappointing. On the one side, kernels satisfying its assump-\ntions \u2014Gaussian kernels for example\u2014 are among the most used kernels in machine learning and\ngeostatistics. They have several nice properties. For instance, the norm associated to a measure\ndoes not depend on the choice of origin in X. On the other side, they do not metrise the narrow\ntopology, probably the most wide-spread topology in probability theory and statistics.\nThese\n\ufb01elds, however, focus on a very speci\ufb01c subset of Mf: probability measures. For those, we will now\nshow that any continuous and characteristic kernel metrises the narrow convergence. We consider\nthis the second main result of this paper.\nTheorem 55. A bounded kernel k over a locally compact Hausdor\ufb00space X metrises the narrow\ntopology over P i\ufb00it is continuous and characteristic (to P). In particular, if k is continuous\nand\nR\ns.p.d. (or continuous and c0-universal), then k metrises the narrow topology over P.\nProof. [Theorem 55] If k metrises the narrow topology over P, then k is obviously characteristic,\nand Theorem 44 shows that k is continuous. Conversely, if k is continuous, then by Theorem 44,\nthe kernel metric on P is weaker than the narrow topology. Thus it su\ufb03ces to show that w(Mf, Hk)\nis stronger than the narrow topology over P. Consider the kernel k1 := k + 1. From Theorem 32\nit follows that k1 is\nR\ns.p.d., thus Lemma 69 (see Appendix A.19) shows that k1 metrises the vague\ntopology w(P, Cc). But on P, the vague and the narrow topology coincide (Berg et al., 1984,\nChapter 2, Corollary 4.3). Thus k1 metrises the narrow topology over P. And k1 and k induce\nthe same metric in P.\nTo the best of our knowledge, so far, the most general su\ufb03cient conditions for a kernel to\nmetrise the narrow topology over P were given by Theorem 2 of Sriperumbudur (2013). It actually\ndeals with families of kernels, but when applied to a single kernel, it states: on a Polish16, locally\ncompact (Hausdor\ufb00) space, any continuous, c0-universal kernel metrises the narrow convergence\ntopology over P. Let us compare it to Theorem 55. A kernel k is continuous and c0-universal\ni\ufb00k belongs to C(0,0)\n0\nand is characteristic to Mf.\nBoth these assumptions are now relaxed:\ninstead of vanishing at in\ufb01nity, k only needs to be bounded; instead of characteristic to Mf, it\nonly needs to be characteristic to P. Thanks to these weaker assumptions, Theorem 55 can be\nstated as an i\ufb00statement. The other major di\ufb00erence is that we relax the assumption that X\nbe Polish, and keep only the local compact Hausdor\ufb00assumption. Doing so, we align with the\ntrend promoted by Bourbaki to do probability theory on locally compact Hausdor\ufb00rather than\nPolish spaces. Wether we could have relaxed the local compact Hausdor\ufb00and kept the Polish\nassumption instead remains the main and most important open question of this paper.\n16A Polish space is a complete, separable, metric space.\n30\n7.6\nIs H\u2032\nk a Space of Distributions?\nSo far, we focused on the two (complementary) questions: when is the KME injective over a set\nof distributions D? And when does k metrise the topology of D? We now brie\ufb02y examine the\ntwo converse questions:\n(Q3) When is the KME surjective?\n(Q4) And if it is, does k metrise the topology of D?\nBoth these questions have important practical applications. Indeed, just reason in terms of se-\nquences. A positive answer to the \ufb01rst (resp. second) question would yield: \u201cIf \u03a6k(Dn) converges\nto g \u2208Hk, then there exists a distribution D such that g = \u03a6k(D) (resp. and Dn \u2192D in D\u221e).\u201d\nThe di\ufb00erence with (Q1) and (Q2) is huge: they assume that the limit D is the KME of a dis-\ntribution. Instead, if the KME was surjective, then whatever the convergent sequence of KMEs\nand whatever its limit (in RKHS norm at least), we would know that this limit is the KME of a\ndistribution. Nevertheless, our answers to (Q3) and (Q4) will unfortunately apply only to very\nspeci\ufb01c cases.\nWe concentrate on (Q4). Recall that D is a topological space of distributions if it is a subspace\nof D\u221ethat carries a stronger topology TD than the topology induced by D\u221e. In particular, TD is\nalways Hausdor\ufb00. Consequently, if D embeds into an RKHS Hk and if the induced kernel-metric\ndominates the initial topology TD, then \u03a6k must be injective. So if \u03a6k is also surjective, then it is\nbijective. We may then identify D and Hk algebraically. And if we endow D \u2261Hk with the kernel\nmetric, then Hk (or rather Hk\n\u2032) becomes a topological space of distributions. Asking (Q4) thus\nboils down to asking: when does Hk (or Hk\n\u2032) identify with a topological space of distributions?\nSo, to answer (Q4), we may use any known criterion to check whether a space of linear forms\nis a topological space of distributions. The most common criterion is the following: if C\u221e\nc\nis\ncontinuously contained and dense in a loc. cv. TVS F, then F\u2032 is continuously contained in D\u221e.\nSaid di\ufb00erently, F\u2032 is then a topological space of distributions.\nApplying this with F = C0,\nfor example, is how one identi\ufb01es Mf as a (topological) subspace of D\u221e. And applying it with\nF = Hk yields Proposition 56, proved in Appendix A.21.\nProposition 56. If C\u221e\nc\nis continuously contained and dense in Hk, then Hk identi\ufb01es via the\nKME with a topological space of distributions via the KME.\nIn practice, the assumptions of Proposition 56 seem cumbersome to verify. Both examples of\nRKHSs that do and that do not verify these assumptions exist. Any Sobolev space W m,2\n0\n(Rd)\ndoes. And it is an RKHS whenever m > 1/2 (Berlinet and Thomas-Agnan, 2004, Theorem 121).\n(But honestly: no need to use Proposition 56 to re-discover that the duals of these Sobolev spaces\nare topological spaces of distributions.) On the other hand, Gaussian kernels do not verify the\nassumptions, because, contrary to C\u221e\nc , all functions of a Gaussian RKHS are extendable to entire\nfunctions over C (Steinwart and Christmann, 2008, Corollary 4.40).\nInstead of wondering on what conditions Hk identi\ufb01es with a subset of distributions, we may\nwonder, whether it might be a superset of distributions. Could there be any RKHS such that all\ndistributions D\u221ewould embed into Hk? If Hk was continuously included in C\u221e\nc , yes. But the\nfollowing lemma, proved in Appendix A.22, often excludes this inclusion.\nLemma 57. If X is non-compact subset of Rd and if k is s.p.d., then Hk contains functions with\nnon-compact support.\n31\nTwo comments on the assumptions of this result. First, the assumption that k be s.p.d. is not\nvery restrictive, for the identi\ufb01cation of Hk with a sub- or superset of D\u221ewould only make sense,\nif the canonical embedding of one into the other was injective. And this requires an s.p.d. kernel.\nAs for the other assumption, that X be non-compact, it is also rather natural, because we already\nknow that, as soon as k is smooth, Hk can embed all distributions with compact support.\nSummarising, we answered (Q4) positively in some cases (when C\u221e\nc\nembeds continuously into\nHk), but left many others unanswered. Lemma 57 is good news: it forti\ufb01es the presumption that\nRKHSs are at least not bigger than D\u221e. Whether they can always be seen as the KME of a\ndistribution space remains, however, an open question.\n8\nConclusion\nWe \ufb01rst discuss how this work relates and contributes to existing machine learning literature and\nthen conclude with future work and closing remarks.\nComparison with Related Machine Learning Literature.\nUniversal and characteristic\nkernels play an important role in learning theory and machine learning. Universal kernels en-\nsure the consistency of some RKHS-based estimators in the context of regression and classi-\n\ufb01cation (Steinwart, 2001), whereas characteristic kernels have found applications in the con-\ntext of two-sample tests (Gretton et al., 2007, 2012), independence tests (Gretton et al., 2008;\nGretton and Gy\u00a8or\ufb01, 2010; Fukumizu et al., 2008) and kernel density estimators (Sriperumbudur,\n2013). The terminology is surprisingly recent: in the past \ufb01fteen years, the machine learning\ncommunity introduced various notions of universality (Steinwart, 2001; Micchelli et al., 2006;\nCarmeli et al., 2006; Caponnetto et al., 2008), accompanied by su\ufb03cient conditions on ker-\nnels to guarantee their universality.\nMeanwhile, Fukumizu et al. (2004) introduced the notion\nof a characteristic kernel to P followed by a series of articles (Fukumizu et al., 2004, 2008,\n2009a,b; Gretton et al., 2007; Sriperumbudur et al., 2008, 2010a,b) to characterise this notion.\nGretton et al. (2007) were \ufb01rst to link universal and characteristic kernels, by showing that char-\nacteristic implies universal. Fukumizu et al. (2009b) were \ufb01rst to explicitly link\nR\ns.p.d. and char-\nacteristic kernels, by showing that one implies the other. Sriperumbudur et al. (2010b) then used\nthis link to prove handy su\ufb03cient criteria for a kernel to be characteristic. In their overview arti-\ncle, Sriperumbudur et al. (2011) catalogue known (and complete some missing) relations between\nthe various notions of universal, characteristic and s.p.d. kernels that had appeared. However,\nthey do not clearly systemise their equivalence. Theorem 12 and Proposition 16 \ufb01ll this gap, by\nshowing that, if Hk continuously embeds into F, then k is universal over F i\ufb00it is characteristic\n(or, equivalently, s.p.d.) over D = F\u2032. This is surprisingly easy to prove, simple to remember\nand covers almost any case encountered in practice... except the most common one: when the\nkernel is characteristic to P. Theorem 32 and Proposition 33 palliate this shortage and complete\nrespectively Theorem 12 and Proposition 16.\nAlthough introduced much earlier in the mathematical community (Guilbart, 1978), the ques-\ntion whether a given kernel metrises the narrow topology was introduced and studied in the\nmachine learning community by Sriperumbudur et al. (2010b). They provided su\ufb03cient condi-\ntions, later on improved and used by Sriperumbudur (2013) to upper bound convergence rates of\nkernel density estimators. Theorem 55 generalises these su\ufb03cient conditions and \ufb01nally provides\na converse on locally compact Hausdor\ufb00spaces.\n32\nThe key observation underlying this paper is the dual link between the embedding of Hk into\nF and the KME of F\u2032, which is captured in Lemma 8 and Corollary 9. We do not claim originality\nfor this insight but hope that this work will convince the machine learning community of its use-\nfulness. Not only does it underly the link between universal, characteristic and s.p.d. kernels, but\nit straightforwardly leads to the extension of KMEs from measures to distributions. Although we\ndiscuss possible applications (see discussions after Theorem 18), we have not yet speci\ufb01cally used\nthis extension in a machine learning problem. But the calculus rules of Section 4 are promising,\nparticularly the link between derivatives of embeddings and embeddings of derivatives. And con-\nsidering the importance of Schwartz-distributions even in such applied problems as the resolution\nof (partial) di\ufb00erential equations, we are con\ufb01dent that the extension of KMEs will \ufb01nd useful\nmachine learning applications. Meanwhile, it can be seen as a curious byproduct of Lemma 8\nthat completes the theory of KMEs and o\ufb00ers memorable results (Fubini Theorem, Corollary 38,\nProposition 42, Proposition 47).\nFuture Work and Closing Remark.\nExcept when explicitly stated otherwise, we always\nassumed that X is locally compact. This hypothesis seems more and more unadapted to the\ngeneral trend in probability theory which is to use Polish spaces, such as separable Banach spaces.\nBut a Banach space is locally compact i\ufb00it is \ufb01nite dimensional.\nWhether Theorem 55 also\nholds for any Polish space is certainly the most important open question of this paper.\nOur\nchoice to work with locally compact spaces is all the more questionable, seeing that every kernel\nover X de\ufb01nes a very natural (semi-) metric over X, namely \u03c1k(\u02c6x, \u02c6y) := \u2225k(., \u02c6x) \u2212k(., \u02c6y)\u2225k.\nTopologies over graphs, for example, are often de\ufb01ned via such kernel-induced metrics. However,\nlocal compactness naturally \ufb01ts distributions in general, and more speci\ufb01cally measures, when\nde\ufb01ned via the Riesz representation theorem.\nOverall, we think our work o\ufb00ers good insights in the nature of the functions that appear\nwhen completing H\npre\nk\nto Hk. Indeed, recall that H\npre\nk\nis the set of \ufb01nite linear combinations of\nk(., x). Thus the question: when completing H\npre\nk , what else could appear than functions that are\n\u201cin\ufb01nite linear combinations of k(., x)\u201d? When saying \u201cin\ufb01nite linear combinations of k(., x)\u201d,\none might \ufb01rst think of continuous sums of weighted in\ufb01nitesimals k(., x) d\u00b5(x), meaning the\nKME\nR\nk(., x) d\u00b5(x) of a measure \u00b5. One may, however, reasonably argue that distributions are\nsimilar to measures, but allow for more general weighting schemes. So it is only natural that,\nunder suitable assumptions on k, they may also embed into Hk; in general not all of them, but\nat least all those with compact support as soon as k is smooth. In some cases, the embeddable\ndistributions even su\ufb03ce to cover the whole space Hk (for example, when Hk is a Sobolev space\nW m,2\n0\n(Rd) with m > 1/2). However, remember that H\u2032\nk identi\ufb01es with Hk via the anti-linear\nRiesz representer map. And after all, with weak-integration, this is just saying that Hk contains\nexactly all those functions\nR\nk(., x) d\u00b5(x) where \u00b5 might not be a continuous linear form over C\u221e\nc\n(in which case it would be a distribution), but is at least a continuous linear form over Hk.\nAcknowledgements\nWe are deeply indebted to Alphabet for their support via a Google European Fellowship in Causal\nInference accorded to Carl-Johann Simon-Gabriel.\nWe thank Bharath Sriperumbudur for en-\nlightening remarks, Jonas Peters for his kind and helpful advice, and Ilya Tolstikhin for useful\ndiscussions and encouragements.\n33\nA\nProofs\nIn this section, we gather all the complements to non fully proved theorems, propositions, corol-\nlaries or lemmas appearing in the main text.\nA.1\nProof of Proposition 3\nProof. Suppose that Hk \u2282C0. (i) clearly holds. Suppose (ii) was not met. Then let xn \u2208X\nsuch that k(xn, xn) = \u2225k(., xn)\u22252\nk \u2192\u221e. Thus k(., xn) is unbounded. But \u27e8f | k(., xn)\u27e9k = f(xn) is\nbounded for any f \u2208Hk, thus k(., xn) is bounded (Banach-Steinhaus Theorem). Contradiction.\nThus (ii) is met.\nConversely, suppose that (i) and (ii) hold. Let H\npre\nk\n:= span{k(., x) | x \u2208X}. Then, H\npre\nk\n\u2282C0,\nand for any f, g \u2208Hk, \u2225f \u2212g\u2225\u221e\u2264\u2225f \u2212g\u2225k \u2225k\u2225\u221e. Thus H\npre\nk\ncontinuously embeds into the closed\nC0, thus so does its \u2225.\u2225k-closure, Hk. The proof of the cases Hk \u2282C and Hk \u2282Cb are similar\n(see also Berlinet and Thomas-Agnan, 2004, Theorem 17).\nA.2\nProof of Proposition 4\nProof. Suppose that k \u2208C(m,m)\nb\n.\nThen H\npre\nk\n\u2282Cm\nb\n(Steinwart and Christmann, 2008, Corol-\nlary 4.36) and for any x \u2208X, f \u2208H\npre\nk , and |p| \u2264m, we have \u2225\u2202pf\u2225\u221e\u2264\u2225f\u2225k\n\r\r\r\n\u221a\n\u2202(p,p)k\n\r\r\r\n\u221e. Thus\nH\npre\nk\ncontinuously embeds into the closed space Cm\nb , thus so does its \u2225.\u2225k-closure, Hk. But, almost\nby de\ufb01nition of (Cm\nb )c (see Section 5.2), we have Cm\nb \u0592\u2192(Cm\nb )c. Thus Hk \u0592\u2192(Cm\nb )c\nA.3\nProof of Theorem 13\nProof. The de\ufb01nition of the embedding of a distribution, together with the kernel\u2019s property that\nk(y, x) = k(x, y) leads to:\n\u27e8D | T\u27e9k =\nZ\nx\n\u001cZ\ny\nk(., y) dD(y) | k(., x)\n\u001d\nk\nd \u00afT(x)\n=\nZ\nx\n\u001c\nk(., x) |\nZ\ny\nk(., y) dD(y)\n\u001d\nk\nd \u00afT(x)\n=\nZ\nx\nZ\ny\n\u27e8k(., x) | k(., y)\u27e9k d \u00afD(y) d \u00afT(x)\n=\nZZ\nk(x, y) dD(y) d \u00afT (x).\nUse \u27e8D | T\u27e9k = \u27e8T | D\u27e9k to prove the right-most part of (5).\nA.4\nProof of Lemma 17\nProof. That\n\u20d7\u03d5k\nis\ncontinuously\ndi\ufb00erentiable\nfollows\nfrom\nCorollary\n4.36\nof\nSteinwart and Christmann (2008).\nSuppose now that |p| = 1.\nThen, for any x, y \u2208X,\n34\nh \u2208R:\n[\u20d7\u03d5k(x)](y) = \u27e8\u2202p\u20d7\u03d5k(x) | k(., y)\u27e9k\n(\u2217)\n= lim\nh\u21920\n\u001c \u20d7\u03d5k(x + h p) \u2212\u20d7\u03d5k(x)\nh\n| k(., y)\n\u001d\nk\n= lim\nh\u21920\nk(y, x + h p) \u2212k(y, x)\nh\n= \u2202(0,p)k(y, x),\nwhere (\u2217) holds because strong RKHS-convergence implies weak convergence. We then iterate the\nprocess by increasing |p| step by step, until we get the announced result: \u2202p\u20d7\u03d5k = \u2202(0,p)k.\nA.5\nProof of Theorem 18\nProof. The proof holds in the following equalities. For any f \u2208Hk,\n\u001c\nf |\nZ\nk(., x) d[\u2202pD](x)\n\u001d\nk\n=\nZ\n\u27e8f | k(., x)\u27e9k d[\u2202p \u00afD](x) = [\u2202p \u00afD](f)\n(13)\n= (\u22121)|p| \u00afD(\u2202pf)\n(14)\n= (\u22121)|p| \u00afD(\nD\nf | \u2202(0,p)k(., \u02c6x)\nE\nk)\n(15)\n=\n\u001c\nf | (\u22121)|p|\nZ\n\u2202(0,p)k(., x) dD(x)\n\u001d\nk\n.\n(16)\nEquation (14) uses the de\ufb01nition of the distributional derivative (see Schwartz, 1978); (15) uses\nthe fact that, for any x \u2208X, \u2202pf(x) =\n\nf | \u2202(0,p)k(., x)\n\u000b\nk (see Steinwart and Christmann, 2008,\nproof of Corollary 4.36). And as the left-hand-side of (13) is a continuous linear form over Hk, it\nsu\ufb03ces to apply the weak integral\u2019s de\ufb01nition to (15) to get Equation (16).\nA.6\nProof of Proposition 21\nProof. The linear span H\npre\n\u2202(0,p) of {\u2202(0,p)k(., x) | x \u2208X} and that of {\u2202(p,p)k(., x) | x \u2208X} are\ndense in H\u2202(0,p)k and H\u2202(p,p)k respectively. And for any x, y \u2208X (Steinwart and Christmann,\n2008, Lemma 4.29),\nD\n\u2202(0,p)k(., x) | \u2202(0,p)k(., y)\nE\nk = \u2202(p,p)k(y, x) =\nD\n\u2202(p,p)k(., x) | \u2202(p,p)k(., y)\nE\n\u2202(p,p)k .\nThus H\u2202(0,p)k and H\u2202(p,p)k are congruent (i.e. isometrically isomorphic) via the isometry\nf\n\u2202p :\nH\u2202(0,p)k\n\u2212\u2192H\u2202(p,p)k\n\u2202(0,p)k(., x)7\u2212\u2192\u2202(p,p)k(., x)\n\u2200x \u2208X\n.\nThe maps f\n\u2202p and \u2202p coincide on H\npre\n\u2202(0,p). Let now f \u2208H\u2202(0,p)k and fn \u2208H\npre\n\u2202(0,p) such that fn \u2192f.\nThen, strong convergence implying weak convergence: for any x \u2208X,\n\u2202pfn(x)=\n\nfn | \u2202(0,p)k(., x)\n\u000b\nk\n\u2212\u2192\n\nf | \u2202(0,p)k(., x)\n\u000b\nk\n= \u2202pf(x)\n\u2202pfn(x)=\n\n\u2202p(fn) | \u2202p(\u2202(0,p)k(., x))\n\u000b\n\u2212\u2192\n\n\u2202p(f) | \u2202p(\u2202(0,p)k(., x))\n\u000b\nk=\u2202p(f)(x) .\nThus \u2202p(f) = \u2202pf. Thus f\n\u2202p = \u2202p.\n35\nA.7\nProof of Proposition 24\nProof. Conditions (i) and (ii) are equivalent by Theorem 18. Let now f \u2208H\u2202(0,p)k, and let D be\na distribution such that D(\u2202pf) is well de\ufb01ned. Using Proposition 21, we may then write\nD\n\u0010D\nf | \u2202(0,p)k(., x)\nE\nk\n\u0011\n= D\n\u0010D\n\u2202p f | \u2202p[\u2202(0,p)k(., x)]\nE\nk\n\u0011\n= D\n\u0010D\n\u2202pf | \u2202(p,p)k(., x)\nE\n\u2202(p,p)k\n\u0011\n.\n(17)\nAs \u2202p is an isometry from H\u2202(0,p)k onto H\u2202(p,p)k, we see that the left-hand-side de\ufb01nes a continuous\nlinear form over H\u2202(0,p)k i\ufb00the the right-hand-side de\ufb01nes a continuous linear form over H\u2202(p,p)k.\nThis both shows that (ii) implies (iii) (because H\u2202(0,p)k \u2282Hk), and that (ii) and (iii) are equivalent\nwhen k \u2208C(m,m)\n0\n(because then H\u2202(0,p)k = Hk). Let us now suppose that (ii) is met (which covers\nboth preceding cases). Then the embedding \u03a6\u2202(0,p)k(D) belongs to H\u2202(0,p)k, because, for any f in\nthe orthogonal of H\u2202(0,p)k, we have\n\u27e8f | \u03a6\u2202(0,p)k(D)\u27e9k = \u00afD\n\u0010D\nf | \u2202(0,p)k(., x)\nE\nk\n\u0011\n= 0 .\nAnd rewriting Equation (17) with KMEs, we get, for any f \u2208H\u2202(0,p)k\n\u27e8f | \u03a6\u2202(0,p)k(D)\u27e9k =\nD\n\u2202p f | \u2202p \u03a6\u2202(0,p)k(.,x)(D)\nE\n\u2202(p,p)k =\nD\n\u2202pf | \u03a6\u2202(p,p)k(.,x)(D)\nE\n\u2202(p,p)k .\nThus \u2202p\u03a6\u2202(0,p)k(D) = \u03a6\u2202(p,p)k(D), which proves Equation (7).\nA.8\nProof of Corollary 26\nProof. As the conditions (i)-(iii) from Proposition 24 are met, Equation (7) applies. But \u03a6k(\u2202pD)\nequals \u03a6\u2202(0,p)k(D), which belongs to H\u2202(0,p)k. Thus Equation (7) shows that \u03a6\u2202(p,p)k(D) is simply\nthe image of (\u22121)|p|\u03a6k(\u2202pD) via the isometry \u2202p between H\u2202(0,p)k and H\u2202(p,p)k, which concludes.\nA.9\nProof of Proposition 30\nProof. If k0 is characteristic to Mf, then k0 = k and \u03bd0 = 0 ful\ufb01l the requirements. Thus, for the\nrest of the proof, let us suppose that k0 is not characteristic to Mf. Then there exists a non zero\nmeasure \u03bd0 \u2208Mf such that \u03a6(\u03bd0) = 0, and without loss of generality, we can choose \u03bd0(1) = 1.\nThe proof now proceeds as follows.\n(a) Show that if k0 is characteristic to Mf, then the constant function\n1 \u0338\u2208Hk.\n(b) Construct a new Hilbert space of functions of the form Hk = span\n1 \u2295Hk0 that ful\ufb01ls the\nrequirements.\n(c) Show that it has a reproducing kernel k.\n(d) Show that k ful\ufb01ls the requirements.\n(a) Suppose\nthat\n1\n\u2208\nHk0.\nThen\n1\n=\n\u00af\u03bd0(1)\n=\nR\n\u27e81 | k0(., x)\u27e9k0 d\u00af\u03bd0(x)\n(\u2217)\n=\n\n1 |\nR\nk0(., x) d\u03bd0(x)\n\u000b\nk0 = \u27e81 | \u03a6k0(\u03bd0)\u27e9k0 = 0, where in (\u2217) we use Equation (2) (weak inte-\ngral characterisation). Contradiction. Thus\n1 \u0338\u2208Hk0.\n36\n(b) De\ufb01ne H := span\n1 \u2295Hk0 and equip it with the inner product \u27e8. | .\u27e9that extends the inner\nproduct of Hk0 so, that\n1 \u22a5Hk0\nand\n\u22251\u2225= 1 .\n(18)\nIn other words, for any f = cf\n1+f \u22a5\u2208H and any g = cg\n1+g\u22a5\u2208H: \u27e8f | g\u27e9:=\n\nf \u22a5| g\u22a5\u000b\nk0 +\ncf \u00afcg. Obviously H is a Hilbert space of functions.\n(c) We now construct a function k by constructing \ufb01rst a new embedding \u03a6 and then taking\nthe associated kernel. We then show that k is indeed a reproducing kernel and that \u03a6 is its\nassociated KME.\nDe\ufb01ne the hyperplane M0\nf := {\u00b5 \u2208Mf | \u00b5(1) = 0}. Each measure \u00b5 \u2208Mf can be decom-\nposed uniquely in a sum: \u00b5 = \u00b5\u22a5+ \u00b5(1)\u03bd0 where \u00b5\u22a5= \u00b5 \u2212\u00b5(1)\u03bd0 \u2208M0\nf . Thus we may\nde\ufb01ne the linear embedding \u03a6 : Mf \u2192H such that\n\u03a6(\u00b5) :=\n\u001a\u03a6k0(\u00b5)if \u00b5 \u2208M0\nf\n1\nif \u00b5 = \u03bd0\n.\nSaid di\ufb00erently: \u03a6(\u00b5) := \u03a6k0(\u00b5\u22a5)+\u00b5(1)1 = \u03a6k0(\u00b5)+\u00b5(1)1. By construction, \u03a6 is injective.\nNow de\ufb01ne k(x, y) := \u27e8\u03a6(\u03b4y) | \u03a6(\u03b4x)\u27e9. Then, for any f \u2208H, and any x \u2208X,\n\u27e8f | \u03a6(\u03b4x)\u27e9=\nD\nf \u22a5| \u03b4\u22a5\nx\nE\nk0 + cf = f \u22a5(x) + cf\n1(x) = f(x) .\n(19)\nIn particular, taking f = \u03a6(\u03b4y), we get k(., y) = \u03a6(\u03b4y). Thus (19) may be rewritten:\n\u2200f \u2208H, \u2200x \u2208X,\n\u27e8f | k(., x)\u27e9= f(x).\nThus H is an RKHS with reproducing kernel k(x, y) = k0(x, y) + 1, and \u03a6 is its associated\nKME \u03a6k.\n(d) As \u03a6 is injective over Mf, k is characteristic to Mf. To conclude, note that k0(x, y) =\n\u27e8\u03b4y \u2212\u03bd0 | \u03b4x \u2212\u03bd0\u27e9.\nA.10\nProof of Theorem 32\nIn the proof of Proposition 30, we started with a characteristic kernel k0 and ended up showing\nthat k(x, y) := k0(x, y) + 1 is\nR\ns.p.d. To prove Theorem 32, it su\ufb03ces, in (18), to choose \u22251\u2225k = \u01eb\nfor some real \u01eb > 0 instead of \u22251\u2225k = 1. This then shows that k(x, y) := k0(x, y) + \u01eb2 is\nR\ns.p.d.\nThe converse is clear.\nA.11\nProof of Proposition 33\nProof. As the vector space N spanned by\n1 is closed (because \ufb01nite dimensional), the transpose of\nthe canonical homomorphism (Cb)c \u2192(Cb/1) is a bijective linear map from the dual ((Cb)c/1)\u2032\nonto the polar of N, which is: M0\nf := {\u00b5 \u2208Mf | \u00b5(1) = 0} (Treves, 1967, Proposition 35.5). In\nother words, we may identify the dual of (Cb)c/1 with the set of measures M0\nf that sum to 0.\nThus k is characteristic to M0\nf i\ufb00k is universal over (Cb)c/1. Lemma 27 then concludes.\n37\nA.12\nProof of Theorem 48\nProof. That both weak topologies w(F\u2032, F) and w(H\u2032\nk, Hk) coincide on bounded sets is the di-\nrect application of a known functional analysis result which can be stated as follows.\nOn an\nequicontinuous set of a dual space F\u2032 (of a Hausdor\ufb00TVS F), the weak-star topology \u2014or topol-\nogy of pointwise convergence in F\u2014 coincides with the topology of pointwise convergence in a\ndense subset of F \u2014such as Hk, when k is universal over F. The crux is then that, in a bar-\nrelled space, the equicontinuous sets in F\u2032 are precisely its bounded sets. For more details, see\nTreves (1967, Proposition 32.5 and Theorem 33.1, \u201cBanach-Steinhaus Theorem\u201d). This proves\nthat w(F\u2032, F) \u2229B = w(H\u2032\nk, Hk) \u2229B. The rest of Equation (11) uses Proposition 43 and the fact\nthat the weak topology of H\u2032\nk is weaker than its strong topology.\nA.13\nA Second Proof of Corollary 38\nProof. [Second Proof] Theorem 9 of Sriperumbudur et al. (2010b) shows that we only need\nto prove that (iv) implies (iii).\nThe idea of this second proof is to generalise the proof of\nSriperumbudur et al. so as to directly show that (iv) implies (iii) instead of showing that (iv)\nimplies (i).\nThe justi\ufb01cation of equalities (a) and (b) are rather long: we prove them in two\nseparate proofs.\nSuppose that k be c0-universal. We will show that, for any D \u2208Dm\nL1, \u2225D\u22252\nk = [D \u2297\u00afD](k) =\n0 \u21d2D = 0. Let \u039b := F\u03c8, which by Bochner\u2019s Theorem is a positive, \ufb01nite (Borel) measure.\nThen:\n[D \u2297\u00afD](k) = [Dx \u2297\u00afDy]\n\u0012Z\ne\u2212i(x\u2212y)\u03be d\u039b(\u03be)\n\u0013\n(a)\n=\nZ\n[Dx \u2297\u00afDy](e\u2212i(x\u2212y)\u03be) d\u039b(\u03be)\n=\nZ\n|Dx(e\u2212ix\u03be)|2 d\u039b(\u03be).\nAs \u039b has full support over X, this implies Dx(e\u2212ix\u03be)\n(b)\n= [FDx](\u03be) = 0 for all \u03be \u2208X.\nThus\nD = 0.\nBefore justifying (a) and (b), here is some necessary background material.\nBoth justi\ufb01ca-\ntions essentially boil down to the question, whether the two (or three) distributions of a tensor\nproduct commute.\nFor example, in (a), we have to justify that [Dx \u2297\u00afDy \u2297\u039b\u03be](ei(x\u2212y)\u03be) =\n[\u039b \u2297Dx \u2297\u00afDy](ei(x\u2212y)\u03be). Now, usually (for example in Schwartz, 1978), the tensor product of dis-\ntributions Sx \u2208D\u221e\nx and Ty \u2208D\u221e\ny\nis de\ufb01ned as a new distribution Sx \u2297Ty in D\u221e\nx,y. (Here, and in\nall this proof, the indices such as x, y specify the underlying dummy variables.) This distribution\nacts on test functions in C\u221e\nc,x,y, and for those functions \u03d5, one knows that Sx\u2297Ty(\u03d5) = Ty \u2297Sx(\u03d5).\nHere however, we only used the tensor product notation as a proxy for successive parametric inte-\ngrations: [Dx \u2297\u00afDy \u2297\u039b\u03be](ei(x\u2212y)\u03be) means that we \ufb01rst integrate the function ei(x\u2212y)\u03be w.r.t. \u039b, then\nw.r.t. Dy, then w.r.t. Dx. If ei(x\u2212y)\u03be was in C\u221e\nc,x,y,\u03be, this would be the same as evaluating the usual\ntensor product at a given test function. But ei(x\u2212y)\u03be is not in C\u221e\nc,x,y,\u03be so, a priori, we cannot inter-\npret our successive parametric integrations as a usual tensor product of distributions. However,\nnotice that, if Dx and Ty are in Dm\nL1, we know that they can (be uniquely extended to) integrate\nnot only test functions in C\u221e\nc , but also any function in Cm\nb . Thus their tensor product Dx \u2297Ty\nshould also be uniquely extendable to a larger set of functions than C\u221e\nc,x,y, which we call Hm,m\nx,y .\n38\nThis is done by Schwartz (1954). Furthermore, on these spaces Hm,m\nx,y , Schwartz shows that the\ntensor product commutes and can still be evaluated by successive parametric integrations. Thus,\nif ei\u02c6x\u02c6y happened to be in Hm,m\nx,y , we see the quantity [Sx\u2297Ty](eixy), seen as a successive parametric\nintegration, is actually the tensor product of Sx and Ty evaluated in eixy. And it commutes.\nWe now proceed with the justi\ufb01cations of (a) and (b). They will mostly be about identifying\nthis space Hm,m\nx,y\n(or its analogue for the tensor product of three distributions) and verifying that\nthe integrand belongs to it.\nProof. [Justi\ufb01cation\nof\n(a)]\nWe\nwant\nto\nshow\nthat\n[Dx \u2297\n\u00afDy \u2297\u039b\u03be](ei(x\u2212y)\u03be)\n=\n[\u039b \u2297Dx \u2297\u00afDy](ei(x\u2212y)\u03be) (where the right-hand side is not yet proven to be well-de\ufb01ned).\nAs\njust mentioned, the tensor product of distributions is usually de\ufb01ned to act on test functions in\nC\u221e\nc . But ei(\u02c6x\u2212\u02c6y)\u02c6\u03be is obviously not such a test function. So we start by extending the domain\nof de\ufb01nition of certain tensor products. Proposition 14 of Schwartz (1954), and its two preced-\ning paragraphs (applied with Ll\nx = (Cm\nb,x)c, Mm\ny\n= (Cm\nb,y)c and E = (C0\nb,\u03be)c) show that, for any\ndistributions Sx, Ty, Q\u03be in (Dm\nL1)x, (Dm\nL1)y and (Mf)\u03be, their tensor product Sx \u2297Ty \u2297Q\u03be can be\nextended to a continuous linear form over a much wider class of functions than C\u221e\nc,x,y,\u03be, which we\ncall Hm,m,0\nx,y,\u03be . Precisely, Hm,m,0\nx,y,z\nis the tensor product of (Cm\nb,x)c, (Cm\nb,y)c and (C0\nb,\u03be)c, completed for\nGrothendieck\u2019s \u01eb-topology. We write:\nHm,m,0\nx,y,\u03be\n:= (Cm\nb,x)c \u02c6\u2297\u01eb(Cm\nb,y)c \u02c6\u2297\u01eb(C0\nb,\u03be)c .\nAs the tensor products of Sx, Ty and Q\u03be commute on the algebraic (non-completed) tensor product\n(Cm\nb,x)c \u2297(Cm\nb,y)c \u2297(C0\nb,\u03be)c, which is dense in Hm,m,0\nx,y,\u03be , they commute also on Hm,m,0\nx,y,\u03be . Now, the\nproof would be \ufb01nished, if ei(\u02c6x\u2212\u02c6y)\u03be belonged to Hm,m,0\nx,y,\u03be . But when m \u0338= 0, this is wrong. Indeed,\nProposition 13 of Schwartz (1954) shows that Hm,m,0\nx,y,\u03be\ncontains exactly all the functions \u03d5(\u02c6x, \u02c6y, \u02c6\u03be)\nsuch that, for all |p|, |q| \u2264m, the partial derivatives \u2202p\nx\u2202q\nx\u03d5 exist and are continuous and bounded.\nThus ei(\u02c6x\u2212\u02c6y)\u02c6\u03be \u0338\u2208Hm,m,0\nx,y,\u03be .\nTo solve this issue, we use the following trick. First, write\n[Dx \u2297\u00afDy \u2297\u039b\u03be]\n\u0010\nei(x\u2212y)\u03be\u0011\n= [Dx \u2297\u00afDy \u2297e\u039b\u03be]\n \nei(x\u2212y)\u03be\n1 + |\u03be|2m\n!\n,\nwhere e\u039b := (1 + |\u02c6\u03be|2m)\u039b. Now the function ei(x\u2212y)\u03be/(1 + |\u03be|2m) is obviously in Hm,m,0\nx,y,\u03be . And to\n\ufb01nish the proof, we show that e\u039b \u2208Mf. Indeed, remember that \u039b is the (distributional) Fourier\ntransform of the kernel k(x, y) = \u03c8(x \u2212y): \u039b = F\u03c8. But k is in C(m,m)\nx,y\n. Thus, for any |p| \u2264m,\n\u2202(p,p)k(x, y) = (\u22121)p\u22022p\u03c8(x \u2212y) is a continuous kernel. So, using Bochner\u2019s theorem, we see\nthat the Fourier transform of \u22022p\u03c8, (\u22121)|p| F(\u22022p\u03c8)(\u02c6\u03be) = \u02c6\u03be2p F\u03c8(\u02c6\u03be) = \u02c6\u03be2p\u039b(\u02c6\u03be) is a \ufb01nite positive\nmeasure. Being a positive linear combination of such measures, e\u039b also belongs to Mf.\nProof. [Justi\ufb01cation of (b)] Using Th\u00b4eor`eme 2 of Schwartz (1954), applied with Hm = (Cm\nb )c and\nE = (C0\nb)c, we see that the function f : \u03be 7\u2192Dx(e\u2212ix\u03be) is well de\ufb01ned and continuous. (Note that,\nhaving justi\ufb01ed step (a) already shows that f is well-de\ufb01ned and integrable w.r.t. \u039b.) Now, let\nT f : C\u221e\nc\n\u2192C, \u03d5 7\u2192\nR\n\u03d5f be the distribution associated to f. We want to show that T f = FD.\nTo do so, let \u03d5 \u2208D\u221eand let T \u03d5 be the distribution associated to \u03d5. The following equality\nconcludes.\nFD(\u03d5) := D(F\u03d5) = D\u03be(\nZ\neix\u03be\u03d5(x) dx) = [D\u03be \u2297T \u03d5\nx ]\n\u0010\neix\u03be\u0011\n= [T \u03d5\nx \u2297D\u03be]\n\u0010\neix\u03be\u0011\n= T f(\u03d5) .\n39\nHere, the justi\ufb01cation for the swap of tensor products is essentially the same as the justi\ufb01cation\nof (a). ([D\u03be \u2297T \u03d5\nx ] is a continuous linear form over Hm,0\nx,\u03be := Cm\nb,x \u02c6\u2297\u01eb C0\n\u03be; it commutes on the dense\nsubspace Cm\nb,x \u2297C0\n\u03be thus also on Hm,0\nx,\u03be ; and ei\u02c6x\u02c6\u03be belongs to Hm,0\nx,\u03be .)\nThis ends the proof of Theorem 38.\nA.14\nProof of Theorem 41\nProof. [of Theorem 41] We recall that an entire (or holomorphic) function f : Cd \u2192C is said of\nexponential type i\ufb00|f(z)| \u2264Me\u03c4|z| for some positive numbers M and \u03c4, and su\ufb03ciently large z.\nWe also remind that the zeroes of an entire function are necessarily isolated: they are thus at most\ncountable. Our proof now relies on the following three theorems. The \ufb01rst is proved by Schwartz\n(1978, Chapitre VII, Th\u00b4eor`eme XVI) (see also Treves, 1967, Theorem 29.2); the second, which\nwe will only state for the particular case of entire functions of exponential type, can be found in\nthe book by Boas (1954, Theorem 2.10.1); the third is the compilation of Theorem 2.6.5 of Boas\n(1954) and the remarks just preceding it.\nTheorem 58 (Paley-Wiener-Schwartz). The (distributional) Fourier transform of a distribution\nover X = Rd with compact support is the restriction to Rd of an entire function of exponential\ntype de\ufb01ned over Cd; and vice-versa.\nTheorem 59 (Lindel\u00a8of). Let f : C \u2192C be an entire function. Let (zn)n be its zeroes. The\nfunction f is of exponential type i\ufb00there exists a constant M > 0, such that, for any r > 0\n|\nX\n|zn|\u2264r\n1\nzn\n| \u2264M\nand\n#{|zn| \u2264r} \u2264Mr .\n(20)\nTheorem 60 (Weierstrass). For any integer p and any z \u2208C, we de\ufb01ne the Weierstrass primary\nfactor E(z; p) as\nE(z; p) =\n(\n1 \u2212z\nif p = 0\n(1 \u2212z) exp(z + z2\n2 + \u00b7 \u00b7 \u00b7 + zp\np )if p > 0 .\nLet now (zn)n be a possibly \ufb01nite complex sequence, ordered by increasing modulus, with z0 \u0338= 0,\nand such that\n\u2200\u01eb > 0,\nX\nn\n1\n|zn|p+\u01eb < \u221e.\n(21)\nThen the products Q\nn E( z\nzn ; p) converge pointwise to an entire function P(\u02c6z), whose zeroes are\nexactly the zn\u2019s.\nWe now prove Theorem 41. First, note that, as k is in C(m,m), the distributions with compact\nsupport of order m, Em, indeed embed into Hk. Let now D \u2208Em. As proven in the second proof\nof Corollary 38, we have\n\u2225D\u22252\nk = [Dx \u2297Dy]\n\u0012Z\ne\u2212i(x\u2212y)\u03be d\u039b(\u03be)\n\u0013\n(\u2217)\n=\nZ\n[Dx \u2297Dy](e\u2212i(x\u2212y)\u03be) d\u039b(\u03be)\n=\nZ\n|Dx(e\u2212ix\u03be)|2 d\u039b(\u03be) ,\n(22)\n40\nwhere \u039b = F\u03c8.\nIn particular, we had shown that the function Dx(e\u2212ix\u02c6\u03be) coincides with the\ndistributional Fourier-transform FD of D. This time, however, as D has compact support, the\nPaley-Wiener-Schwartz theorem even shows that FD is an entire function of exponential type.\nAnd Lindel\u00a8of\u2019s theorem shows that its zeroes (zn)n verify Equation (20).\nFor the direct part, suppose that k is not characteristic to Dm\nL1. Then we may as well suppose\nthat \u2225D\u2225k = 0.\nLet us now decompose \u039b into its atomic part \u039b\u03b4 and non-atomic part \u039bs:\n\u039b = \u039b\u03b4 + \u039bs (Dudley, 2002, Theorem 3.5.1). As \u039b is a positive measure (Bochner theorem), so\nare \u039b\u03b4 and \u039bs. Lindel\u00a8of\u2019s theorem and Equation 22 show that the support {xn} of the atomic\npart must satisfy #{|zn| \u2264r} \u2264Mr for any r > 0 and a \ufb01xed M > 0. As for \u039bs, we claim it is\nnull, which will prove the direct part.\nIndeed, suppose that \u039bs \u0338= 0. Let C be a compact subset of its support that contains no zero\nzn of FD. Then, for all \u03be \u2208C, | FD(\u03be)|2 \u2265minC | FD |2 > 0 and\n\u2225D\u22252\nk =\nZ\n|Dx(e\u2212ix\u03be)|2 d\u039b(\u03be) \u2265min\nC | FD |2\nZ\nC\nd\u039b(\u03be) > 0 .\n(23)\nContradiction. Thus \u039bs = 0, which proves the direct part.\nConversely, suppose that the Fourier transform \u039b of \u03c8 has a countable support (xn)n and that\nthere exists M > 0 such that, for any r > 0\n#{|xn| \u2264r} \u2264Mr .\n(24)\nWe will now construct an entire function of exponential type that cancels in each xn.\nConsider a complex sequence zn, ordered by increasing modulus, such that {zn} = ({xn} \u222a\n{\u2212xn})\\{0}. As (xn)n satis\ufb01es Equation (24), (zn)n satis\ufb01es Equation (21). Thus Weierstrass\u2019\ntheorem applies and give an entire function P(z) := z Q\nn E( z\nzn ; 1) that cancels on each xn. And,\nas for any r > 0, | P\n|zn|\u2264r z\u22121\nn | = 0, Lindel\u00a8of\u2019s theorem shows that P is of exponential type. And\nthe Paley-Wiener-Schwartz theorem shows that P is the Fourier transform of a distribution T\nwith compact support: T \u2208E\u221e. T, however, need not be of order m. Thus, instead, we consider\na distribution D := \u03c6\u22c6T, where \u03c6 is any \ufb01xed smooth function with compact support and where \u22c6\ndenotes the convolutional product operator. Then D is a smooth function with compact support,\nso D \u2208Em. And the Fourier transform FD of D is the product of the two entire functions F\u03c6\nand FT, thus it cancels on every xn. So \u2225D\u2225k = 0, showing that k is not characteristic to Em.\nRemark 61. We did not \ufb01nd an analogue of Lindel\u00a8of\u2019s theorem when X is Rd instead of R. That\nis why we restricted this Theorem 41 to the one-dimensional case.\nTo prove the converse part, we constructed a distribution D \u2208Em whose Fourier transform\ncancels on the support (xn)n of F\u03c8. To do so, we used Weierstrass\u2019 theorem. Here is an alternative,\nif, instead of assuming that (xn)n satis\ufb01es Equation (24), we suppose that is veri\ufb01es the stronger\ncondition\nX\nn\n1\n|xn| < \u221e.\n(25)\nWe will construct D as the limit of the sequence of convolutional products of the following\nfunctions:\nfn(x) :=\n\u001a|xn|\n2 if |x| \u2264|xn|\n0 otherwise\n.\n41\nIf the sequence (xn)n has \ufb01nite cardinal N, then D can be chosen as the N-fold convolutional\nproduct D := f1 \u2217f2 \u2217\u00b7 \u00b7 \u00b7 fN. That D then satis\ufb01es all the requirements is clear. Thus, from\nnow on, we suppose that (xn)n\u22651 is not a \ufb01nite sequence. This time, D will be the limit of the\nconvolutional products Fn := f1 \u2217f2 \u2217\u00b7 \u00b7 \u00b7 \u2217fn when n grows inde\ufb01nitely. To show that these\nconvolutional products do converge to a probability measure D, we use Levy\u2019s continuity theorem\n(Dudley, 2002, Theorem 9.8.2). Indeed:\n\u22b2[Ff n](\u02c6x) = sinc(\u02c6x/|xn|), where sinc(\u02c6x) :=\nsin \u02c6x\n\u02c6x .\nFor any x \u2208R, | sinc x| \u22641, thus\n|[FF n](x)| = |[Ff1](x)[Ff 2](x) \u00b7 \u00b7 \u00b7 [Ff n](x)| decreases with n, thus converges.\nAnd, as\nsoon as |x| \u2264|xm|, [Ff m](x) \u22650. Thus, for any n \u2265m the sign of FF n stays constant,\nthus converges. Thus FF n converges pointwise to a function we denote \u03c6.\n\u22b2Let us now show that \u03c6 is continuous in 0. Let |x| < |x1|. Then 1 \u2265sinc\nx\n|xn| \u22651 \u2212\nx2\n6|xn|2.\nThus\n0 = ln \u03c6(0) \u2265ln \u03c6(x) \u2265\nX\nn\nln(1 \u2212\nx2\n|xn|2 ) \u2265\u2212x2\n6\nX\nn\n1\n|xn|2\nx\u21920\n\u2212\u2212\u2212\u21920 .\nThus \u03c6 is continuous in 0.\nThus \u03c6 is the Fourier transform FD of a probability measure D. Finally, note that for any n,\nsuppFn \u2282\n\"\n\u2212\nX\nn\n1\n|xn|,\nX\nn\n1\n|xn|\n#\n=: S .\nThus P has also its support in the compact set S. Finally, by construction, for any n, [FP](xn) =\n0, thus \u2225P\u22252\nk =\nR\n| FP |2 d F\u03c8 = 0, although P \u0338= 0. Thus \u03c8(\u02c6x\u2212\u02c6y) is not characteristic to P\u2229Em.\nA.15\nProof of Proposition 42\nProof. The proof is completely similar to the direct part of the preceding proof (of Theorem 41).\nTo show Statement (ii), suppose that k is not characteristic to Em and take D \u2208Em such that\n\u2225D\u2225k = 0. Write\n\u2225D\u22252\nk =\nZ\n|Dx(e\u2212ix\u03be)|2 d\u039b(\u03be) .\n(26)\nNotice that Dx(e\u2212ix\u03be) has at most a countable number of zeroes. Deduce that the support of the\natomic part of \u039b is at most countable. Conclude by showing that \u039b is an atomic measure. This\nproves (ii).\nTo prove Statement (i), do the same, and consider now the convolutional product P of D with\na smooth, compactly supported function \u03d5 (P := D \u22c6\u03d5, with \u03d5 \u2208C\u221e\nc ). Then P is also a smooth,\ncompactly supported function, so de\ufb01nes a measure in Mc: P \u2208Mc. But any zero of FD is also a\nzero of FP, because FP = F\u03d5 \u00b7 FD. Thus they also contain the support of \u039b, which shows that\n\u2225P\u2225k = 0, and that k is not characteristic to Mc either. By contraposition, we proved that if k is\ncharacteristic to Mc, then it is characteristic to Em. The converse is clear.\nA.16\nProof of Theorems 44 and 45\nWhat was left, was to show the continuity of appropriate tensor product maps (D, T) 7\u2192D \u2297T.\nThese maps will be given in Lemma 62, 63 and 65. They respectively prove (iii)\u21d2(i), (iv)\u21d2(i)\nand Theorem 45.\n42\nLemma 62. Let X, Y be locally compact Hausdor\ufb00spaces. The bilinear map\nB :(M+(X))\u03c3 \u00d7 (M+(Y))\u03c3\u2212\u2192(M+(X \u00d7 Y))w\n(\u00b5, \u03bd)\n7\u2212\u2192\u00b5 \u2297\u03bd\nis continuous (where \u03c3 denotes the narrow convergence topology).\nProof. See Chapter 2, Theorem 3.3 of Berg et al. (1984).\nLemma 63. Let X, Y be locally compact Hausdor\ufb00spaces. The bilinear map\nB :(Mc(X))w \u00d7 (Mc(Y))w\u2212\u2192(Mc(X \u00d7 Y))w\n(\u00b5, \u03bd)\n7\u2212\u2192\u00b5 \u2297\u03bd\nis continuous over any sets B \u00d7 C, where B, C are bounded subsets of Mc(X) and Mc(Y).\nProof. As B is bounded in Mc(X), there exists a compact K0 \u2282X such that, for any \u00b5 \u2208B,\nthe support of \u00b5 is contained in K0 (Schwartz, 1978, p. 90). Let K be a compact neighbourhood\nof K0 (which exists, because X is locally compact Hausdor\ufb00).\nLet\n1K0 be a function with\nsupport contained in K, that equals 1 on a neighbourhood of K0. Then, for any \u00b5 \u2208B and any\nf \u2208C(X),1K0f \u2208Cc and \u00b5(f) = \u00b5(1K0f). Thus, on B, the weak topology w(Mc, C) coincides\nwith the vague topology w(Mc, Cc). The same holds for C and for B(B \u00d7 C) (because X \u00d7 Y is\nalso locally compact Hausdor\ufb00and supp\u00b5 \u2297\u03bd = supp \u00b5 \u00d7 supp\u03bd). Thus the result follows from\nProposition 6 of Bourbaki (2007a, Chapitre 3, \u00a74, n\u00b03) and its subsequent remark.\nThe proof of Lemma 65 needs some preparation. Let X and Y be open subsets of Rd and let\nC(m,m)(X \u00d7 Y) be the the of functions f such that, for any p, q \u2208N \u222a{\u221e}, with |p|, |q| \u2264m, the\nderivative \u2202(p,q)f exists and is continuous. Endow C(m,m)(X \u00d7 Y) with the topology of uniform\nconvergence of f and of all its derivatives up to the order (m, m) over the compacts of X \u00d7 Y. It\nis the topology generated by the family\nVp, q, K, U :=\nn\n\u03d5 \u2208C(m,m)(X \u00d7 Y) | [\u2202(p,q)\u20d7\u03d5](K) \u2282U\no\n,\nwhere |p| \u2264m, |q| \u2264m, U is an open subset of R and K a compact in X\u00d7Y. Note E(m,m)(X\u00d7Y)\nits dual. Now, let Cm(X; Cm(Y)) be the space of functions over X with values in Cm(Y) which\nare m-times continuously di\ufb00erentiable. Similarly to Cm(X; R), equip Cm(X; Cm(Y)) with the\nthe topology generated by\nVp, K, U := {\u20d7\u03d5 \u2208Cm(X; Cm(Y)) | [\u2202p\u20d7\u03d5](K) \u2282U} ,\nwhere |p| \u2264m, U is an open subset of Cm(Y) and K a compact in X. Then\nLemma 64. The map\nC(m,m)(X \u00d7 Y)\u2212\u2192Cm(X; Cm(Y))\n\u03d5\n7\u2212\u2192\u20d7\u03d5 : x 7\u2192\u03d5(x, .)\nis a bijective isomorphism for the TVS structures.\nProof. Combine Proposition 9 and 12 of Schwartz (1954).\nNow comes the actual lemma of interest for the proof of Theorem 45.\n43\nLemma 65. Let X and Y be open subset of Rd and let m \u2208N \u222a{\u221e}. The bilinear map\nB :(Em(X))w \u00d7 (Em(Y))w\u2212\u2192(E(m,m)(X \u00d7 Y))w\n(Dx, Ty)\n7\u2212\u2192Dx \u2297Ty\nis continuous over any sets B \u00d7 C, where B, C are bounded subsets of Em(X) and Em(Y).\nActually, we will prove the following strengthened version of the lemma.\nLemma 66. Let X and Y be open subsets of Rd and let m \u2208N \u222a{\u221e}. Let (Em(X))c (and\nsimilarly (Em(Y))c and (E(m,m)(X \u00d7 Y))c) denote the space Em(X) equipped with the topology of\nuniform convergence over the compact subsets of Cm(X). The bilinear map\neB :(Em(X))c \u00d7 (Em(Y))c\u2212\u2192(E(m,m)(X \u00d7 Y))c\n(Dx, Ty)\n7\u2212\u2192Dx \u2297Ty\nis hypocontinuous w.r.t. the bounded subsets of Em(X) and Em(Y).\nProof. [Lemma 66] Let K be a compact subset of C(m,m)(X \u00d7 Y) and let B be a bounded subset\nof Em(Y). We need to show that, when Dx converges to 0 uniformly over the compacts of Cm(X)\nwhile Ty varies in B, then Dx \u2297Ty converges to 0 uniformly over K. Because Cm(X) is barrelled,\nbounded sets of its dual, such as B, are relatively weakly compact and equicontinuous. But the\nbilinear map\nF :(Em(X))c \u00d7 Cm(X; Cm(Y))\u2212\u2192Cm(Y)\n(Ty, \u20d7\u03d5)\n7\u2212\u2192Ty(\u03d5(\u02c6x, y))\nis hypocontinuous w.r.t. equicontinuous subsets of Em and relatively compact subsets of Cm\n(Schwartz, 1954, Corollaire 2 of Proposition 19). Thus its restriction to B \u00d7 K (where we identify\nCm(X; Cm(Y)) and C(m,m)(X \u00d7 Y) using Lemma 64) is continuous. But seen that B is equicon-\ntinuous, the weak and the compact convergence topology coincide. Thus, being relatively weakly\ncompact, K is also a compact subset of (Em(Y))c. Thus the image of B \u00d7 K via F is a compact\nsubset of Cm(Y). Thus Dx(Ty(\u03d5)) converges uniformly to 0 when \u03d5 varies in K and Ty varies in\nB.\nProof. [Lemma 65] First, note that the bounded subsets B and C of Em(X) and Em(Y) are\nalso bounded for the compact convergence topology. But the map eB being hypocontinuous w.r.t.\nbounded subsets of Em(X) and Em(Y), its restriction to B \u00d7 C is continuous. And Cm being\nbarrelled, on bounded subsets of Em, the weak and the compact convergence topology coincide\n(Treves, 1967, Proposition 32.5 and Theorem 33.2).\nRemark 67. Note that when m = \u221e, then C(\u221e,\u221e) = C\u221eand E(\u221e,\u221e) = E\u221e. In this case, Schwartz\n(1978, Chapitre IV, Th\u00b4eor`eme VI) even proves that B is continuous when E\u221e(X), E\u221e(Y) and\nE\u221e(X\u00d7Y) are equipped with their strong topology. This implies Lemma 65, because on bounded\nsets of E\u221e, weak and strong topologies coincide.\nA.17\nProposition 53 for Separable Metric Spaces\nProposition 68 ((Mf)\u03c3\nseq\n\u0592\u2192Hk). Let X be any topological space with a countable dense subset,\nand let k be a bounded continuous kernel on X. The semi-inner product topology induced by k in\nMf is sequentially weaker than the narrow topology.\n44\nIn other words: for any \u00b5, \u00b51, \u00b52, ... \u2208Mf, if \u00b5n(f) \u2192\u00b5(f) for any f \u2208Cb, then \u03a6k(\u00b5n) \u2192\n\u03a6k(\u00b5) in \u2225.\u2225k.\nNote that on bounded sets of positive measures \u2014such as P\u2014, and when X\nis a separable metric space, then the narrow convergence topology is metrisable. In that case,\n\u201csequentially weaker\u201d implies \u201cweaker\u201d.\nProof. Let Bk := {f \u2208Hk | \u2225f\u2225k \u22641} and C := supx\u2208X k(x, x)1/2. Let \u00b5n, \u00b5 \u2208Mf such that\n\u00b5n \u2192\u00b5 narrowly. De\ufb01ne the possibly degenerate metric d(\u02c6x, \u02c6y) := \u2225k(., \u02c6x) \u2212k(., \u02c6y)\u2225k. First, note\nthat the underlying topology T is stronger than the topology induced by d. (Use the fact that k\nbeing continuous, so is k(., \u02c6x).) Now, for any f \u2208Bk,\n|f(x) \u2212f(y)| \u2264\u2225f\u2225k \u2225k(., x) \u2212k(., y)\u2225k \u2264d(x, y) .\nThus Bk is a d-equicontinuous set of functions. But T being stronger than d, Bk is also a T-\nequicontinuous. And it is uniformly bounded, since |f(x)| \u2264C for any x \u2208X and any f \u2208Bk.\nThus Corollary 11.3.4. of Dudley (2002) applies and gives: \u2225\u00b5n \u2212\u00b5\u2225k = supf\u2208Bk\nR\nf d(\u00b5n \u2212\u00b5) \u2192\n0.\nA.18\nProof of Proposition 54\nB\nB\nX\nX\nnd\nnd\nA\n0\nFigure 1: The 2d-dimensional hypercube Cn and its subparts A and B (see Proof of Proposition 54)\nProof. Consider the sequence of probability measures Pn :=\n1\nnd \u03bb[0,n]d, where [0, n]d denotes the\nd-dimensional hypercube with edge of length n. The sequence Pn does not converge narrowly (as\nit is not uniformly tight). But we will show that Pn converges to the null measure in RKHS-norm,\nwhich concludes. Indeed, let \u01eb > 0 and \u03b1 > 0 such that, for any h \u2208Rd with |h| > \u03b1, |\u03c8(h)| \u2264\u01eb.\nConsider the 2d-dimensional hypercube Cn in Rd \u00d7 Rd, with edges of length n \u2208N. Divide the\nhypercube into 2 parts A and B as schematised on Figure 1. More precisely, A are all points of\nCn at distance \u2264\u03b1 from the \u201cdiagonal\u201d {(x, x) \u2208(X \u00d7 X) \u2229Cn}; B is the rest of Cn. Note that\nthere exist constants a and b, independent of \u01eb, \u03b1 and n, such that the volumes VA, VB of A and\nB verify: VA \u2264a\u03b1 nd and VB \u2264bn2d. Furthermore, by construction sup(x,y)\u2208B |k(x, y)| \u2264\u01eb. Thus\n\u2225Pn\u22252\nk =\n1\nn2d\nZZ\nA \u222aB\nk(x, y) dx dy\n\u2264\n1\nn2d\n\u0010\nVA \u2225k\u2225\u221e+ \u01eb VB)\n\u2264a\u03b1 \u2225k\u2225\u221e\nnd\n+ \u01ebb .\nFor n su\ufb03ciently large, \u2225Pn\u22252\nk \u22642b\u01eb, thus Pn converges to 0 in the RKHS-norm.\n45\nA.19\nStatement and Proof of Lemma 69\nThe proof of Theorem 55 is based on the following lemma.\nLemma 69. Let k be a continuous,\nR\ns.p.d. kernel. Let c > 0 and M\n\u2264c\n+ := {\u00b5 \u2208M+ | \u00b5(X) \u2264c}.\nThe following topologies coincide on M\n\u2264c\n+ :\n(i) the weak topology w(M\n\u2264c\n+ , Hk) induced by k, or topology of pointwise convergence in Hk ;\n(ii) the vague topology w(M\n\u2264c\n+ , Cc), or topology of pointwise convergence in Cc .\n(iii) the topology w(M\n\u2264c\n+ , C0), or topology of pointwise convergence in Cc .\nRemark 70. When k is continuous and c0-universal, then the equality w(Mf, C0) \u2229M\n\u2264c\n+\n=\nw(H\u2032\nk, Hk) \u2229M\n\u2264c\n+ from Lemma 69 is a direct consequence of Theorem 48.\nProof. [Lemma 69] The topologies of (ii) and (iii) are equivalent because Cc is dense in C0 and\nM\n\u2264c\n+ is a bounded subset of their dual Mf. We now show the equality between the topologies (i)\nand (ii).\nThe kernel k being continuous, its induced metric on M\n\u2264c\n+ is weaker than the narrow topology\n(Theorem 44). Thus it su\ufb03ces to show that w(Mf, Hk) is stronger than the narrow topology\nover M\n\u2264c\n+ . As any \ufb01lter is the intersection of its \ufb01ner ultra\ufb01lters (Bourbaki, 2007b, Chapter I, \u00a76,\nn\u00b05, Proposition 7), it su\ufb03ces to show that any ultra\ufb01lter U on M\n\u2264c\n+ that converges to \u00b50 \u2208Mf\nin w(Mf, Hk) also converges to \u00b50 in w(Mf, Cc).\nThus, let U be such an ultra\ufb01lter and let\nK be a compact subset of X.\nBecause k is\nR\ns.p.d., Hk is dense in (Cb)c (Theorem 48).\nIn\nparticular, there exists h \u2208Hk such that h > 0 on K.\nFor any f \u2208Cc(X) and \u00b5 \u2208M\n\u2264c\n+ ,\n|[h.\u00b5](f)| = |\u00b5(hf)| \u2264c \u2225h\u2225\u221e\u2225f\u2225\u221e. Thus h.M\n\u2264c\n+ is bounded in Mf, thus relatively compact in\nw(Mf, Cc). Thus the ultra\ufb01lter generated by Uh := h.U converges in w(Mf, Cc) to a measure\n\u00b51 \u2208M\n\u2264c\n+ . We have now shown in particular that, for any compact K \u2282X,\n\u2200g \u2208Cc(K),\nlim\n\u00b5\u2208U \u00b5(gh) = \u00b51(gh) .\n(27)\nBut the map Cc(K) \u2192Cc(K), g 7\u2192gh is bijective (because h > 0 on K). Thus Equation (27)\nactually reads: for all g \u2208Cc(K), lim\u00b5\u2208U \u00b5(g) = \u00b51(g). Thus U converges to \u00b50 in w(Mf, Cc). But\nHk and Cc are both dense in (Cb)c. Thus \u00b50 = \u00b51.\nA.20\nAlternative Proof for a Weaker Version of Theorem 55\nIn the proof of Theorem 55, a key equality was w(Mf, C0) \u2229P = w(H\u2032\nk, Hk) \u2229P, which was given\nby Lemma 69. As noted in Remark 70, this equality can also be obtained from Theorem 48, when\nHk \u0592\u2192C0. Here, we show a weaker version of Theorem 55 where we add the constrain Hk \u2282C0,\nso that its proof relies now only on Theorem 48.\nProposition 71. Let X be a locally compact Hausdor\ufb00space and suppose that Hk \u2282C0(X).\nThe kernel k metrises the narrow topology over P i\ufb00it is continuous and characteristic (to P).\nIn particular, any c0-universal kernel metrises the narrow topology over P.\nProof. The only if part is clear.\nConversely, if k is continuous and characteristic to P, then\nProposition 53 shows that the metric induced by k is weaker than the narrow topology. And if,\nadditionally to being characteristic, we suppose that k be even c0-universal, then Theorem 48,\napplied to the bounded set B = P shows that the metric induced by k on P is stronger than the\n46\nweak topology w(Mf, C0) \u2229P. As P is bounded in Mf, this weak topology coincides with the\nvague topology w(Mf, Cc) \u2229P, which is known to coincide with the narrow topology (Berg et al.,\n1984, Chapter 2, Corollary 4.3). Thus narrow and kernel induced topology coincide on P. The\nremaining of the proof now consists in treating the case where k is characteristic, but not c0-\nuniversal. The idea is to use k to construct a new kernel k\u03d5 that be c0-universal, and such that,\non P, the topologies induced by k and by k\u03d5 coincide.\nSuppose now that Hk \u0592\u2192C0, but without k being c0-universal. Let \u03bd \u0338= 0 be a \ufb01nite measure\nsuch that \u03a6k(\u03bd) = 0. Let \u03d5 \u2208C0 such that \u03bd(\u03d5) = 1 and de\ufb01ne the kernel k\u03d5(x, y) := \u03d5(x) \u00af\u03d5(y) +\nk(x, y). The kernel k\u03d5 is continuous and c0-universal. Thus, the \ufb01rst part of the proof shows that\nit metrises the vague topology w(Mf, Cc) \u2229P over P.\nLet now M\u03d5\nf and M0\nf be the hyperplanes of Mf de\ufb01ned respectively by \u00b5(\u03d5) = 0 and \u00b5(1) = 0.\nOn M\u03d5\nf , k and k\u03d5 induce the same metric. Thus k metrises the vague topology w(Mf, Cc) \u2229M\u03d5\nf\nover M\u03d5\nf . But M\u03d5\nf and M0\nf are two non-identical hyperplanes in Mf. Thus\nM0\nf = (M0\nf \u2229M\u03d5\nf ) \u2295span \u03bd0\nM\u03d5\nf = (M0\nf \u2229M\u03d5\nf ) \u2295span \u03bd ,\nwhere \u03bd0 \u2208M0\nf \\M\u03d5\nf such that \u03bd0(\u03d5) = 1. Thus M0\nf and M\u03d5\nf are isomorphic, algebraically and\ntopologically, via\n\u0131 :\u00b57\u2212\u2192\n\u001a\u00b5if \u00b5 \u2208M0\nf \u2229M\u03d5\nf\n\u03bdif \u00b5 = \u03bd0\n.\nLet now P, P1, P2, . . . \u2208P. We decompose Pn (and P) on (M0\nf \u2229M\u03d5\nf )\u2295span \u03bd0: Pn = Pn(\u03d5)\u03bd0+pn.\nSuppose that \u2225Pn \u2212P\u2225k \u21920. Then, on one side, both Pn(\u03d5) \u2192P(\u03d5) and \u2225pn \u2212p\u2225k \u21920. On\nthe other, \u2225\u0131(Pn \u2212P)\u2225k \u21920, which shows that \u0131(Pn) converges vaguely to \u0131(P). Said di\ufb00erently:\nboth pn \u2192p and Pn(\u03d5)\u03bd0 \u2192P(\u03d5)\u03bd0 vaguely. Thus both pn \u2192p and Pn(\u03d5)\u03bd \u2192P(\u03d5)\u03bd vaguely.\nThus Pn \u2192P vaguely.\nAnd being probability measures, they also converge narrowly, which\nconcludes.\nRemark 72. Note that, following Theorem 32, one is tempted to choose k\u03d5 = k +\n1. k\u03d5 is then\nknown to be characteristic to Mf, and the metrics induced by k and k\u03d5 would obviously coincide\non P. However, this approach fails because, when Hk \u2282C0, then Hk+1 \u0338\u2282C0.\nA.21\nProof of Proposition 56\nProof. Indeed, if C\u221e\nc\nembeds continuously and densely into Hk via the canonical embedding\n\uf6be: C\u221e\nc\n\u2192Hk, f 7\u2192f, then H\u2032\nk embeds continuously and injectively into D\u221evia the transpose\n\uf6be\u22c6of \uf6be. Thus Hk\n\u2032 \u2261Hk embeds continuously and injectively into D\u221evia the complex conjugate\nmap of \uf6be\u22c6: said di\ufb00erently, it identi\ufb01es with a topological space of distributions.\nA.22\nProof of Lemma 57\nProof. Let (xn)n\u2208N be a sequence in X that is not included in any compact. Consider then the\nsequence fn := Pn\nk\nak\n22k\nk(.,xk)\n\u2225k(.,xk)\u2225k where an = 1 if n = 0 or if fn\u22121(xn) \u22650, and an = \u22121 otherwise.\n47\nThen (fn)n obviously converges to a function f in Hk, and f has non-compact support, because\n|f(xn)| \u2265|fn(xn)| \u2212\n\f\f\f\f\f\nX\nk>n\nak\n22k\nk(., xk)\n\u2225k(., xk)\u2225k\n\f\f\f\f\f \u2265\n\u2265\n1\n22n\nk(xn, xn)\n\u2225k(., xn)\u2225k\n\u2212\nX\nk>n\n\u2225k(., xn)\u2225k\n22k\n= 4\n3\n\u2225k(., xn)\u2225k\n22n\n> 0 .\nB\nThe Parametric Integral is not Enough\nIn this section, we give examples of kernels k and measures \u00b5 over X = Rd, such that the function\ny 7\u2192\nR\nk(y, x) d\u00b5(x), although being well-de\ufb01ned for any x \u2208X, is not contained in Hk.\nFirst, remember that if k \u2208C0, then all functions in Hk vanish at in\ufb01nity. Let now k(\u02c6x, \u02c6y) =\n\u03c8(\u02c6x \u2212\u02c6y) be any continuous stationary kernel over X = Rd such that \u03c8 is Lebesgue-integrable\n(\u03c8 \u2208L1) and\nR\n\u03c8(x) dx \u0338= 0. (Take for example a gaussian kernel.) Then, for any y \u2208Rd, the\nintegral\nR\nk(y, x) dx is well-de\ufb01ned. But the function y 7\u2192\nR\nk(y, x) dx is non-zero and constant.\nSo it does not vanish at in\ufb01nity and cannot be contained in Hk. Thus the Lebesgue measure does\nnot embed into Hk, although it can parametrically integrate the kernel k. This is why we used\nweak-integration, and not parametric integration.\nC\nC.P.D. Kernels and Characteristic Kernels to P\nThe two most relevant criteria we gave for characteristicness (Corollary 38 and Theorem 41)\napply only to stationary kernels, because their proofs rely on Bochner\u2019s theorem. But there is\nan equivalent of Bochner\u2019s theorem that holds for a larger class of stationary kernels, namely\nthe stationary conditionally positive de\ufb01nite kernels (see De\ufb01nition 73). Using this generalised\nBochner theorem (Theorem 75), we get an analogue of Theorem 41 that holds for this more general\nclass of kernels.\nIn particular, these results show that the Brownian motion kernel k(\u02c6x, \u02c6y) =\nmin(|\u02c6x|, |\u02c6y|) is characteristic to the probability measures with compact support P \u2229Mc.\nWe\nstart with reminders on c.p.d. kernels and refer to the geostatistics literature (Matheron, 1973;\nChil`es and Del\ufb01ner, 2012) or Au\ufb00ray and Barbillon (2009) for many more details.\nDe\ufb01nition 73. A conditionally positive de\ufb01nite (c.p.d.) kernel is a function kc : X \u00d7 X \u2192C\nsuch that, for any n \u2208N, x1, . . . , xn \u2208X, \u03bb1, . . . , \u03bbn \u2208C,\nn\nX\ni=1\n\u03bbi = 0\n\u21d2\nn\nX\ni,j=1\n\u03bbi\u03bbjkc(xi, xj) \u22650 .\nRemark 74. In the geostatistics literature, one likes to identify all c.p.d. kernels kc that cannot be\ndistinguished by the values Pn\ni,j=1 \u03bbi\u03bbjkc(xi, xj) when the \u03bbi vary in the hyperplane de\ufb01ned by\nPn\ni=1 \u03bbi = 0. All such functions de\ufb01ne an equivalence class of c.p.d. kernels. Note that for any\nc.p.d. kernel kc, and any function \u03c8 : X \u2192C, the c.p.d. kernels kc and kc(\u02c6x, \u02c6y) + \u03c8(\u02c6x) + \u03c8(\u02c6y) are\nin the same equivalence class.\n48\nSimilarly to p.d. kernels, we may associate to each c.p.d. kernel kc a map\n\u03a6kc :M\u03b4\u2212\u2192CX\n\u03b4x 7\u2212\u2192kc(., x) =\nR\nkc(., s) d\u03b4x(s)\n.\nThis time however, we focus on\nM0\n\u03b4 : =\n( n\nX\ni=1\n\u03bbi\u03b4xi | n \u2208N, \u03bbi \u2208C, xi \u2208X,\nn\nX\ni=1\n\u03bbi = 0\n)\n=\n\u001a\n\u00b5 \u2208M\u03b4 |\nZ\nd\u00b5 = 0\n\u001b\n= M\u03b4 \u2229M0\nf\nand the induced space of functions H\n0,pre\nkc\n:= \u03a6kc(M0\n\u03b4). H\n0,pre\nkc\nis equipped with the inner product\nde\ufb01ned, for any f = \u03a6kc(\u00b5) and g = \u03a6kc(\u03bd) in H\n0,pre\nkc\n(\u00b5 = P\ni \u00b5i\u03b4xi, \u03bd = P\nj \u00b5j\u03b4yj \u2208M0\n\u03b4), as17\n\u27e8f | g\u27e9kc :=\nX\ni,j\n\u00b5ik(xi, xj)\u03bdj =\nZ\nkc(x, y) d\u00b5(x) d\u00af\u03bd(y) ,\nwhich we use to de\ufb01ne the induced semi-inner product on M0\n\u03b4:\n\u2200\u00b5, \u03bd \u2208M0\n\u03b4,\n\u27e8\u00b5 | \u03bd\u27e9kc := \u27e8\u03a6kc(\u00b5) | \u03a6kc(\u03bd)\u27e9kc .\nIt is a (proper) inner product over M0\n\u03b4 i\ufb00, for any \u00b5 \u2208M0\n\u03b4:\n\u2225\u00b5\u22252\nkc =\nn\nX\ni,j=1\n\u00b5ik(xi, xj)\u00b5j = 0\n\u21d2\n\u00b5 = 0,\nthat is i\ufb00kc is characteristic to M0\n\u03b4. In that case, kc is said conditionally strictly positive de\ufb01nite.\nNext we complete H\n0,pre\nkc\nto a Hilbert space of functions H0\nkc and extend the embedding \u03a6kc to\nlarger sets of measures or distributions. We again focus only on sets D0 that are contained in the\n(closed) hyperplane of D\u221e\nL1 de\ufb01ned by the equation\nR\ndD = 0: for example M0\nc, M0\nf , (Dm\nL1)0. We\nproceed as for the p.d. case: if H0\nkc \u0592\u2192Cm (resp. Cm\n0 ), then any distribution in D \u2208(Em)0 (resp.\n(Dm\nL1)0) de\ufb01nes a continuous linear form over H0\nkc. Thus it de\ufb01nes an element in the dual (H0\nkc)\u2032,\nwhose Riesz representer\nR\nkc(., x) dD(x) we call the KME of D in H0\nkc.\nObviously, each p.d. kernel is also c.p.d. Conversely, given any c.p.d. kernel kc, the function\nde\ufb01ned for a given z0 \u2208X and any x, y \u2208X as k(x, y) := \u27e8\u03b4x \u2212\u03b4z0 | \u03b4y \u2212\u03b4z0\u27e9kc, is a p.d. kernel\n\u201cthat coincides with kc on M0\n\u03b4 \u201d (read: \u201csuch that \u03a6kc = \u03a6k on M0\n\u03b4 \u201d). Thus any metric we\ninduce in M0\n\u03b4 (or M0\nc, M0\nf , ...) using a c.p.d. kernel could also have been induced by a p.d. kernel.\nSo why bother about c.p.d. kernels? Because there exist p.d. kernels k that are not stationary,\nbut coincide on M0\n\u03b4 with a stationary c.p.d. kernel. The class of stationary c.p.d. kernels is thus\nstrictly bigger than the class of c.p.d. kernels that coincide with a stationary p.d. kernel on M0\n\u03b4.\nExample 3. Let z0 \u2208X = R and k1(x, y) := e\u2212|x\u2212y| \u2212e\u2212|x\u2212z0| \u2212e\u2212|z0\u2212y| + 1 and k2(x, y) :=\nmin(|x|, |y|) (Brownian motion (or Wiener) kernel). Both k1 and k2 are non stationary p.d. kernels,\nbut on M0\n\u03b4, they coincide with the c.p.d. kernels kc,1(x, y) = e\u2212|x\u2212y| and kc,2(x, y) = \u2212|x \u2212y|\nrespectively. As it happens, kc,1 is p.d., but kc,2 is not. And one may prove (Matheron, 1973;\nChil`es and Del\ufb01ner, 2012) that there exists no stationary p.d. kernel that coincides with k2 on\nM0\n\u03b4.\n17As for p.d. kernels, one easily veri\ufb01es that if f = \u03a6kc(\u00b5) = \u03a6kc(\u03bb), with \u00b5, \u03bb \u2208M0\n\u03b4, then the right hand side\ndoes not change if we replace \u00b5 by \u03bb.\n49\nFor ease of presentation, we now restrict ourselves to real-valued measures and functions. The\nfollowing theorem extends Bochner\u2019s theorem to c.p.d. kernels (Matheron, 1973, Theorem 2.1):\nTheorem 75 (Generalised Bochner Theorem). A continuous symmetrical function \u03c8c(\u02c6x\u2212\u02c6y) over\nRd \u00d7 Rd is a real-valued stationary c.p.d. kernel i\ufb00there exists an even c.p.d. polynomial P0 of\norder \u22642 and a positive (necessarily unique and symmetric) Radon measure \u03c7 without atom at\nthe origin such that\nR\nd\u03c7(\u03be)\n1+|\u03be|2 < +\u221eand, for any h \u2208Rd,\n\u03c8c(h) =\nZ cos \u27e8h | \u03be\u27e9Rd \u22121\n|\u03be|2\nd\u03c7(\u03be) + P0(h) .\nNow remember: Corollary 38 and Theorem 41 characterised characteristic stationary p.d.\nkernels \u03c8(\u02c6x, \u02c6y) using the support of F\u03c8, where F\u03c8 is precisely the positive measure from the\nusual Bochner theorem.\nIt is only natural to try to extend these critera to stationary c.p.d.\nkernels, using measure \u03c7 from the generalised Bochner theorem instead of F\u03c8.\nWe will however only address characteristicness to M0\nc, because M0\nf is actually already covered\nby Corollary 38. Indeed, if we do not impose that the c.p.d. kernel kc be bounded, M0\nf need not\nembed into Hk. But if we do impose boundedness, then Matheron (1973) showed that there exists\na stationary p.d. kernel k that di\ufb00ers from kc only by an additive constant, so induces the same\nnorm than kc in M0\nf . And for stationary p.d. kernels, being characteristic to M0\nf is equivalent to\nbeing characteristic to Mf. So to study whether kc is characteristic to M0\nf , it su\ufb03ces to apply\nCorollary 38 to k.\nProposition 76. Let kc(\u02c6x, \u02c6y) = \u03c8c(\u02c6x \u2212\u02c6y) be a real-valued continuous stationary c.p.d. kernel\nover Rd \u00d7 Rd in C. Let \u03c7 be the measure de\ufb01ned in Theorem 75. Then M0\nc embeds into H0\nkc. If\nthe support of \u03c7 has an accumulation point in Rd, then kc embeds and is characteristic to M0\nc.\nProof. For any \u00b5 \u2208Mc,\nR\n\u2225kc(., x)\u2225kc d|\u00b5|(x) < \u221e. Thus kc is Bochner-integrable w.r.t. to any\nmeasure in Mc, so M0\nc embeds into H0\nkc. For any \u00b5 \u2208M0\nc with \u00b5 \u0338= 0,\n\u2225\u00b5\u22252\nkc =\nZ\nkc(x, y) d\u00b5(x) d\u00b5(y)\n=\nZ\n(x,y)\n\u0012Z\n\u03be\ncos \u27e8x \u2212y | \u03be\u27e9Rd \u22121\n|\u03be|2\nd\u03c7(\u03be) + P0(x \u2212y)\n\u0013\nd\u00b5(x) d\u00b5(y)\n=\nZ\n\u03be\n| F\u00b5(\u03be)|2\n|\u03be|2\nd\u03c7(\u03be) + |C\u00b5|2\nwhere we noted F\u00b5(\u03be) :=\nR\ne\u2212i\u27e8x | \u03be\u27e9Rd d\u00b5(x) the Fourier transform of \u00b5 and |C\u00b5|2 :=\nR\n(x,y) P0(x \u2212\ny) d\u00b5(x) d\u00b5(y) (the right hand-side is positive because P0 is c.p.d.). The switch of integral follows\nfrom Fubini\u2019s theorem, because (cos \u27e8x \u2212y | \u03be\u27e9Rd \u22121)/|\u03be|2 is integrable w.r.t. \u03c7 \u2297|\u00b5| \u2297|\u00b5|. As\n\u00b5 has compact support, its Fourier-transform is a holomorphic function (Paley-Wiener-Schwartz\nTheorem, see Treves, 1967, Theorem 29.2). Thus its zeroes (zn)n have no accumulation point in\nRd.\nSuppose now that the support of \u03c7 has an accumulation point in Rd. Let then C be a compact\nneighbourhood of an accumulation such that C contains no zero zn of F\u00b5.\nAs \u00b5(Rd) = 0,\nF\u00b5(0) = 0. But F\u00b5 being holomorphic, the function | F\u00b5(\u02c6\u03be)|2/|\u02c6\u03be|2 is continuous, even in 0. Thus\n\u2225\u00b5\u22252\nkc \u2265\nZ\nC\n| F\u00b5(\u03be)|\n|\u03be|2\nd\u03c7(\u03be) \u2265\u00b5(C) min\n\u03be\u2208C\n| F\u00b5(\u03be)|\n|\u03be|2\n> 0 .\n50\nThus any non-null measure in M0\nc has a strictly positive norm. So kc is characteristic.\nApplying Proposition 76 and Lemma 27 yields: the Brownian motion kernel (see Example 3)\nis characteristic to Pc , the probability measures with compact support.18 And being continuous,\nit metrises the narrow convergence topology over Pc (Theorem 44).\nD\nBackground Material\nLet us start with the de\ufb01nition of a barrelled set. In a normed space (E, \u2225.\u2225), the sets T := {f \u2208\nE | \u2225f\u2225\u2264C} where C > 0 are the closed balls centred on the origin of E. A normed space is a\nparticular case of a topological vector space (TVS). In a general locally convex (loc. cv.) TVS E,\nthe topology might not be given by a single norm, but by a family of semi-norms (\u2225.\u2225\u03b1)\u03b1\u2208I (where\nthe index set I can be uncountable). A so-called barrel of E is then any closed ball centred on the\norigin and associated to a norm \u2225.\u2225\u03b1, \u03b1 \u2208I. More abstractly, a barrel can be de\ufb01ned as follows.\nDe\ufb01nition 77 (Barrel). A subset T of a TVS E is called a barrel if it is\n(i) absorbing: for any f \u2208E, there exists cf > 0 such that f \u2208cfT;\n(ii) balanced: for any f \u2208E, if f \u2208T then \u03bbf \u2208T for any \u03bb \u2208C with |\u03bb| \u22641 ;\n(iii) convex ;\n(iv) closed.\nIn any loc. cv. space, there exists a basis of neighbourhoods of the origin consisting only of\nbarrels. However, in general, there may be barrels that are not a neighbourhood of 0. This leads\nto\nDe\ufb01nition 78 (Barrelled spaces). A TVS is barrelled if any barrel is a neighbourhood of the\norigin.\nAlthough many authors include local convexity in the de\ufb01nition, in general, a barrelled space\nneed not be loc. cv. Barrelled spaces were introduced by Bourbaki, because they were well-suited\nfor the following generalisation of the celebrated Banach-Steinhaus theorem.\nTheorem 79 (Banach-Steinhaus). Let E be a barrelled TVS, F be a loc. cv. TVS, and let L(E, F)\nbe the set of continuous linear maps form E to F. For any H \u2282L(E, F) the following properties\nare equivalent:\n(i) H is equicontinuous.\n(ii) H is bounded for the topology of pointwise convergence.\n(iii) H is bounded for the topology of bounded convergence.\nWhen E is a normed space and F = C, then L(E, F) is by de\ufb01nition E\u2032. With \u2225.\u2225E\u2032 being the\ndual norm in E\u2032, the equivalence of (ii) and (iii) states that\n\u2200f \u2208E, sup\nh\u2208H\n|h(f)| < \u221e\n=\u21d2\nsup\nh\u2208H\n\u2225h\u2225E\u2032 < \u221e.\n18This result cannot be obtained directly from Corollary 38. However, for this particular kernel, it is actually\nquicker to simply notice that the RKHS HBM associated to a Brownian motion kernel consists of the functions\n\u03d5 \u2208W 2,2\n0\nsuch that \u03d5(0) = 0. Thus the Brownian motion kernel is characteristic to Pc.\n51\nObviously, to understand the content of the Banach-Steinhaus theorem, one needs the de\ufb01ni-\ntion of a bounded set. Let us de\ufb01ne them now.\nWhen E is a normed space, then a subset B of E is called bounded if supf\u2208B \u2225f\u2225E < \u221e. In a\nmore general loc. cv. TVS E, where the topology is given by a family of semi-norms (\u2225.\u2225\u03b1)\u03b1\u2208I, a\nsubset B of E is called bounded if, for any \u03b1 \u2208I, supf\u2208B \u2225f\u2225\u03b1 < \u221e. This can be shown equivalent\nto the following, more usual de\ufb01nition.\nDe\ufb01nition 80 (Bounded Sets in a TVS). A subset B of a TVS E is bounded, if, for any open\nset U \u2282E, there exists a real cB > 0 such that B \u2282cBU.\nWe now move on to an unrelated topic: the Riesz Representation theorem for Hilbert spaces.\nMost of this paper relies on this one theorem.\nTheorem 81 (Riesz Representation Theorem for Hilbert Spaces). A Hilbert space H and its\ntopological dual H\u2032 are isometrically (anti-) isomorphic via the Riesz representer map\n\u0131 :H\u2212\u2192H\u2032\nf 7\u2212\u2192Df :=\n\u001aH\u2212\u2192\nC\ng 7\u2212\u2192\u27e8g | f\u27e9\n.\nIn particular, for any continuous linear form D \u2208H\u2032, there exists a unique element f \u2208H, called\nthe Riesz representer of D, such that\n\u2200g \u2208H,\nD(g) = \u27e8g | f\u27e9.\nNote that \u201canti\u201d in \u201canti-isomorphic\u201d simply means that, instead of being linear, \u0131 is anti-\nlinear: for any \u03bb \u2208C and f \u2208H, \u0131(\u03bbf) = \u00af\u03bb \u0131(f). Often, we prefer to say that H is isometrically\nisomorphic to H\n\u2032, where H\n\u2032 denotes the conjugate of H, where the scalar multiplication is replaced\nby (\u03bb, f) 7\u2192\u00af\u03bbf. H\u2032\nk and Hk\n\u2032 are obviously isomorphic via the complex conjugation map D 7\u2192\u00afD.\nThe Riesz representation theorem for Hilbert spaces is not to be confounded with the following\ntheorem, also known as the Riesz \u2014or Riesz-Markov-Kakutani\u2014 representation theorem. In this\npaper, we always refer to the latter as the Riesz-Markov-Kakutani representation theorem. This\ntheorem has numerous variants, depending on which dual pair (E, E\u2032) one uses. Here we state it\nfor E = C0.\nTheorem 82 (Riesz-Markov-Kakutani). Let X be a locally compact Hausdor\ufb00space. The spaces\nMf(X) and (C0(X))\u2032 are isomorphic, both algebraically and topologically via the map\n\u0131 :Mf(X)\u2212\u2192(C0(X))\u2032\n\u00b5\n7\u2212\u2192D\u00b5 :=\n\u001aC0\u2212\u2192\nC\n\u03d5 7\u2212\u2192\nR\n\u03d5 d\u00b5\n.\nIn other words, for any continuous linear form D over C0(X), there exists a unique \ufb01nite\nBorel measure \u00b5 \u2208Mf such that, for any test function \u03d5 \u2208C0(X), D(\u03d5) =\nR\n\u03d5 d\u00b5. Moreover,\nsup\u2225\u03d5\u2225\u221e\u22641 D(\u03d5) = |\u00b5|(X), or in short: \u2225D\u2225(C0)\u2032 = \u2225\u00b5\u2225TV , where \u2225\u00b5\u2225TV denotes the total variation\nnorm of \u00b5. This is why, in this paper, we identify Mf \u2014a space of \u03c3-additive set functions\u2014 with\nMf \u2014a space of linear functionals.\nIn this paper, to embed a space of measures into an RKHS Hk we successively apply both Riesz\nrepresentation theorems: If Hk embeds continuously into C0, then (C0)\u2032 embeds continuously into\nHk\n\u2032, via the embedding map \u03a6k. But (C0)\u2032 = Mf (Riesz-Markov-Kakutani Representation) and\nHk\n\u2032 = Hk (Riesz Representation). Thus \u03a6k may also be seen as an embedding of Mf into Hk.\n52\nE\nFunction Spaces and their Duals\nThe following two diagrams depict how some function spaces (\ufb01rst diagram) and their corre-\nsponding duals (second diagram) embed one into another. The shaded line in the \ufb01rst diagram\nhighlights those function spaces, for which we do not know any RKHS that they continuously and\ndensely contain. Likewise, we do not know whether their duals can be continuously embedded via\na KME. Lemma 57 might hint, that such KMEs do not exist.\n\u2190\u2212\u2212\u2212\u0593\nLq \u2190\u0593 W m,q\n0\n\u2190\u0593 W \u221e,q\n0\n\u2190\u0593Hgauss\n\u0592\u2192\n\u0592\u2192\n\u0592\u2192\nCc \u2190\u0593 Cm\nc\n\u2190\u0593 C\u221e\nc\n=\n\u0592\u2192\n\u0592\u2192\n\u0592\u2192\nCX\nC0 \u2190\u0593 Cm\n0\n\u2190\u0593 C\u221e\n0\n\u2190\u0593Hgauss\n\u2190\u2212\u2212\u2212\u0593\n\u0592\u2192\n\u0592\u2192\n\u0592\u2192\n(Cb)c\u2190\u0593(Cm\nb )c\u2190\u0593(C\u221e\nb )c\n\u0592\u2192\n\u0592\u2192\n\u0592\u2192\nC \u2190\u0593 Cm \u2190\u0593 C\u221e\n\u0592\u2212\u2212\u2212\u2192\nLq\u2032 \u0592\u2192W \u2212m,q\u2032\n0\n\u0592\u2192W \u2212\u221e,q\u2032\n0\n\u0592\u2192H\u2032\ngauss\n\u0592\u2192\n\u0592\u2192\n\u0592\u2192\nMr\u0592\u2192\nDm\n\u0592\u2192\nD\u221e\n=\n\u0592\u2192\n\u0592\u2192\n\u0592\u2192\nM\u03b4\nMf \u0592\u2192Dm\nL1\n\u0592\u2192\nD\u221e\nL1\n\u0592\u2192H\u2032\ngauss\n\u0592\u2212\u2212\u2212\u2192\n\u0592\u2192\n\u0592\u2192\n\u0592\u2192\nMf \u0592\u2192Dm\nL1\n\u0592\u2192\nD\u221e\nL1\n\u0592\u2192\n\u0592\u2192\n\u0592\u2192\nMc\u0592\u2192\nEm\n\u0592\u2192\nE\u221e\nReferences\nR. A. Adams. Sobolev Spaces. Academic Press, 1975.\nYves Au\ufb00ray and Pierre Barbillon. Conditionally positive de\ufb01nite kernels: theoritical contribution,\napplication to interpolation and approximation. Technical report, INRIA, 2009.\nF. Bach and M. Jordan. Kernel independent component analysis. Journal of Machine Learning\nResearch, 3:1\u201348, 2002. URL http://www.jmlr.org/papers/v3/bach02a.\nC. Berg, J. P. R. Christensen, and P. Ressel. Harmonic Analysis on Semigroups Theory of Positive\nDe\ufb01nite and Related Functions. Springer, 1984.\nA. Berlinet and C. Thomas-Agnan. Reproducing Kernel Hilbert Spaces in Probability and Statistics.\nSpringer, 2004.\nR. P. Boas. Entire Functions. Academic Press, 1954.\n53\nN Bourbaki. Int\u00b4egration - Chapitres 1-4. Springer, reprint of the 1965 original edition, 2007a.\nN. Bourbaki. Topologie G\u00b4en\u00b4erale - Chapitres 1-4. Springer, reprint of the 1971 original edition,\n2007b.\nA. Caponnetto, C. Micchelli, M. Pontil, and Y Ying. Universal multi-task kernels. Journal of\nMachine Learning Research, 9:1615\u20131646, 2008.\nC. Carmeli, E. De Vito, and A. Toigo. Vector valued reproducing kernel hilbert spaces of integrable\nfunctions and mercer theorem. Analysis and Applications, 4(4):377\u2013408, 2006.\nJ.-P. Chil`es and P. Del\ufb01ner. Geostatistics: Modeling Spatial Uncertainty. Wiley, 2nd edition, 2012.\nR. M. Dudley. Real Analysis and Probability. Cambridge University Press, 2002.\nD. H. Fremlin, D. J. H. Garling, and R. G. Haydon. Bounded measures on topological spaces.\nProceedings of the London Mathematical Society, s3-25(1):115\u2013136, 1972.\nK. Fukumizu, F. Bach, and M. Jordan. Kernel dimensionality reduction for supervised learning.\nJournal of Machine Learning Research, 5:73\u201399, 2004.\nK. Fukumizu, A. Gretton, X. Sun, and B. Sch\u00a8olkopf. Kernel measures of conditional dependence.\nIn Neural Information Processing Systems, pages 489\u2013496, 2008.\nK. Fukumizu, A. Gretton, B. Sch\u00a8olkopf, and B. Sriperumbudur. Characteristic kernels on groups\nand semigroups. In Neural Information Processing Systems, pages 473\u2013480, 2009a.\nK. Fukumizu, G. R. Gretton, A.and Lanckriet, B. Sch\u00a8olkopf, and B. K. Sriperumbudur. Ker-\nnel Choice and Classi\ufb01ability for RKHS Embeddings of Probability Distributions. In Neural\nInformation Processing Systems, pages 1750\u20131758. Curran Associates, Inc., 2009b.\nA. Gretton and L. Gy\u00a8or\ufb01. Consistent nonparametric tests of independence. Journal of Machine\nLearning Research, 11:1391\u20131423, 2010.\nA. Gretton, O. Bousquet, A. J. Smola, and B. Sch\u00a8olkopf. Measuring statistical dependence with\nHilbert-Schmidt norms. In Algorithmic Learning Theory, pages 63\u201378, 2005a.\nA. Gretton, R. Herbrich, A. Smola, O. Bousquet, and B. Sch\u00a8olkopf. Kernel methods for measuring\nindependence. Journal of Machine Learning Research, 6:2075\u20132129, 2005b.\nA. Gretton, K. M. Borgwardt, M. Rasch, B. Sch\u00a8olkopf, and A. J. Smola. A kernel method for the\ntwo-sample-problem. In Neural Information Processing Systems, pages 513\u2013520, 2007.\nA. Gretton, K. Fukumizu, C. Teo, L. Song, B. Sch\u00a8olkopf, and A. Smola. A kernel ttatistical test\nof independence. In Neural Information Processing Systems, pages 585\u2013592, 2008.\nA. Gretton, K. M. Borgwardt, M. J. Rasch, B. Sch\u00a8olkopf, and A. Smola. A kernel two-sample\ntest. Journal of Machine Learning Research, 13:723\u2013773, 2012.\nA. Grothendieck. Sur certains espaces de fonctions holomorphes. I. Journal f\u00a8ur die reine und\nangewandte Mathematik, 192:35\u201364, 1953.\nC. Guilbart. Etude des Produits Scalaires sur l\u2019Espace des Mesures: Estimation par Projections.\nPhD thesis, Universit\u00b4e des Sciences et Techniques de Lille, 1978.\n54\nO. Lehto. Some remarks on the kernel function in Hilbert function space. Annales Academiae\nScientiarum Fennicae, 109:6, 1952.\nD. Lopez-Paz, P. Hennig, and B. Sch\u00a8olkopf. The randomized dependence coe\ufb03cient. In Neural\nInformation Processing Systems, pages 1\u20139, 2013.\nG. Matheron. The intrinsic random functions and their applications. Advances in Applied Proba-\nbility, 5(3):439\u2013468, 1973.\nC. A. Micchelli, Y. Xu, and H. Zhang. Universal kernels. Journal of Machine Learning Research,\n7:2651\u20132667, 2006.\n\u02c7S. Schwabik. Topics in Banach Space Integration. Number 10 in Series in Real Analysis. World\nScienti\ufb01c, 2005.\nL. Schwartz.\nEspaces de fonctions di\ufb00\u00b4erentiables `a valeurs vectorielles.\nJournal d\u2019Analyse\nMath\u00b4ematique, 4(1):88\u2013148, 1954.\nL. Schwartz. Th\u00b4eorie des Distributions. Hermann, 1978.\nH. Shen, S. Jegelka, and A. Gretton. Fast kernel-based independent component analysis. IEEE\nTransactions on Signal Processing, 57(9):3498\u20133511, 2009.\nC.-J. Simon-Gabriel and B. Sch\u00a8olkopf. Kernel distribution embeddings: Universal kernels, char-\nacteristic kernels and kernel metrics on distributions. Journal of Machine Learning Research,\n19(44):1\u201329, 2018.\nB. K. Sriperumbudur. On the optimal estimation of probability measures in weak and strong\ntopologies. arXiv:1310.8240, 2013.\nB. K. Sriperumbudur, A. Gretton, K. Fukumizu, G. R. G. Lanckriet, and B. Sch\u00a8olkopf. Injective\nHilbert space embeddings of probability measures. In Conference On Learning Theory, pages\n111\u2013122, 2008.\nB. K. Sriperumbudur, K. Fukumizu, and G. Lanckriet.\nOn the relation between universality,\ncharacteristic kernels and RKHS embedding of measures. In ICAIS, pages 773\u2013780, 2010a.\nB. K. Sriperumbudur, A. Gretton, K. Fukumizu, B. Sch\u00a8olkopf, and G. R.G. Lanckriet. Hilbert\nspace embeddings and metrics on probability measures. Journal of Machine Learning Research,\n11:1517\u20131561, 2010b.\nB. K. Sriperumbudur, K. Fukumizu, and G. R. G. Lanckriet. Universality, characteristic kernels\nand RKHS embedding of measures. Journal of Machine Learning Research, 12:2389\u20132410, 2011.\nI. Steinwart. On the in\ufb02uence of the kernel on the consistency of support vector machines. Journal\nof Machine Learning Research, 2:67\u201393, 2001.\nI. Steinwart and A. Christmann. Support Vector Machines. Information Science and Statistics.\nSpringer, 2008.\nF. Treves. Topological Vector Spaces, Distributions and Kernels. Academic Press, 1967.\nH. Wackernagel. Multivariate Geostatistics - An Introduction with Applications. Springer, 3rd\nedition, 2003.\n55\n",
        "sentence": "",
        "context": "Inference accorded to Carl-Johann Simon-Gabriel.\nWe thank Bharath Sriperumbudur for en-\nlightening remarks, Jonas Peters for his kind and helpful advice, and Ilya Tolstikhin for useful\ndiscussions and encouragements.\n33\nA\nProofs\nbility, 5(3):439\u2013468, 1973.\nC. A. Micchelli, Y. Xu, and H. Zhang. Universal kernels. Journal of Machine Learning Research,\n7:2651\u20132667, 2006.\n\u02c7S. Schwabik. Topics in Banach Space Integration. Number 10 in Series in Real Analysis. World\nScienti\ufb01c, 2005.\n\u0592\u2192\nMr\u0592\u2192\nDm\n\u0592\u2192\nD\u221e\n=\n\u0592\u2192\n\u0592\u2192\n\u0592\u2192\nM\u03b4\nMf \u0592\u2192Dm\nL1\n\u0592\u2192\nD\u221e\nL1\n\u0592\u2192H\u2032\ngauss\n\u0592\u2212\u2212\u2212\u2192\n\u0592\u2192\n\u0592\u2192\n\u0592\u2192\nMf \u0592\u2192Dm\nL1\n\u0592\u2192\nD\u221e\nL1\n\u0592\u2192\n\u0592\u2192\n\u0592\u2192\nMc\u0592\u2192\nEm\n\u0592\u2192\nE\u221e\nReferences\nR. A. Adams. Sobolev Spaces. Academic Press, 1975."
    },
    {
        "title": "On the optimal estimation of probability measures in weak and strong",
        "author": [
            "B. Sriperumbudur"
        ],
        "venue": "topologies. Bernoulli,",
        "citeRegEx": "Sriperumbudur,? \\Q2016\\E",
        "shortCiteRegEx": "Sriperumbudur",
        "year": 2016,
        "abstract": "",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "Hilbert space embeddings and metrics on probability measures",
        "author": [
            "B. Sriperumbudur",
            "A. Gretton",
            "K. Fukumizu",
            "B. Sch\u00f6lkopf",
            "G. Lanckriet"
        ],
        "venue": "J. Mach. Learn. Res.,",
        "citeRegEx": "Sriperumbudur et al\\.,? \\Q2010\\E",
        "shortCiteRegEx": "Sriperumbudur et al\\.",
        "year": 2010,
        "abstract": "A Hilbert space embedding for probability measures has recently been\nproposed, with applications including dimensionality reduction, homogeneity\ntesting, and independence testing. This embedding represents any probability\nmeasure as a mean element in a reproducing kernel Hilbert space (RKHS). A\npseudometric on the space of probability measures can be defined as the\ndistance between distribution embeddings: we denote this as $\\gamma_k$, indexed\nby the kernel function $k$ that defines the inner product in the RKHS.\n  We present three theoretical properties of $\\gamma_k$. First, we consider the\nquestion of determining the conditions on the kernel $k$ for which $\\gamma_k$\nis a metric: such $k$ are denoted {\\em characteristic kernels}. Unlike\npseudometrics, a metric is zero only when two distributions coincide, thus\nensuring the RKHS embedding maps all distributions uniquely (i.e., the\nembedding is injective). While previously published conditions may apply only\nin restricted circumstances (e.g. on compact domains), and are difficult to\ncheck, our conditions are straightforward and intuitive: bounded continuous\nstrictly positive definite kernels are characteristic. Alternatively, if a\nbounded continuous kernel is translation-invariant on $\\bb{R}^d$, then it is\ncharacteristic if and only if the support of its Fourier transform is the\nentire $\\bb{R}^d$. Second, we show that there exist distinct distributions that\nare arbitrarily close in $\\gamma_k$. Third, to understand the nature of the\ntopology induced by $\\gamma_k$, we relate $\\gamma_k$ to other popular metrics\non probability measures, and present conditions on the kernel $k$ under which\n$\\gamma_k$ metrizes the weak topology.",
        "full_text": "arXiv:0907.5309v3  [stat.ML]  30 Jan 2010\nHilbert Space Embedding and Characteristic Kernels\nHilbert Space Embeddings and Metrics on Probability\nMeasures\nBharath K. Sriperumbudur\nbharathsv@ucsd.edu\nDepartment of Electrical and Computer Engineering\nUniversity of California, San Diego\nLa Jolla, CA 92093-0407, USA\nArthur Gretton\narthur@tuebingen.mpg.de\nMPI for Biological Cybernetics\nSpemannstra\u00dfe 38\n72076, T\u00a8ubingen, Germany\nKenji Fukumizu\nfukumizu@ism.ac.jp\nInstitute of Statistical Mathematics\n4-6-7 Minami-Azabu, Minato-ku\nTokyo 106-8569, Japan\nBernhard Sch\u00a8olkopf\nbernhard.schoelkopf@tuebingen.mpg.de\nMPI for Biological Cybernetics\nSpemannstra\u00dfe 38\n72076, T\u00a8ubingen, Germany\nGert R. G. Lanckriet\ngert@ece.ucsd.edu\nDepartment of Electrical and Computer Engineering\nUniversity of California, San Diego\nLa Jolla, CA 92093-0407, USA\nEditor:\nAbstract\nA Hilbert space embedding for probability measures has recently been proposed, with appli-\ncations including dimensionality reduction, homogeneity testing, and independence testing.\nThis embedding represents any probability measure as a mean element in a reproducing\nkernel Hilbert space (RKHS). A pseudometric on the space of probability measures can be\nde\ufb01ned as the distance between distribution embeddings: we denote this as \u03b3k, indexed by\nthe kernel function k that de\ufb01nes the inner product in the RKHS.\nWe present three theoretical properties of \u03b3k. First, we consider the question of de-\ntermining the conditions on the kernel k for which \u03b3k is a metric: such k are denoted\ncharacteristic kernels. Unlike pseudometrics, a metric is zero only when two distributions\ncoincide, thus ensuring the RKHS embedding maps all distributions uniquely (i.e., the em-\nbedding is injective). While previously published conditions may apply only in restricted\ncircumstances (e.g. on compact domains), and are di\ufb03cult to check, our conditions are\nstraightforward and intuitive: integrally strictly positive de\ufb01nite kernels are characteristic.\nAlternatively, if a bounded continuous kernel is translation-invariant on Rd, then it is char-\nacteristic if and only if the support of its Fourier transform is the entire Rd. Second, we\nshow that the distance between distributions under \u03b3k results from an interplay between\nthe properties of the kernel and the distributions, by demonstrating that distributions are\n1\nSriperumbudur, Gretton, Fukumizu, Sch\u00a8olkopf and Lanckriet\nclose in the embedding space when their di\ufb00erences occur at higher frequencies. Third, to\nunderstand the nature of the topology induced by \u03b3k, we relate \u03b3k to other popular metrics\non probability measures, and present conditions on the kernel k under which \u03b3k metrizes\nthe weak topology.\nKeywords:\nProbability metrics, Homogeneity tests, Independence tests, Kernel methods,\nUniversal kernels, Characteristic kernels, Hilbertian metric, Weak topology.\n1. Introduction\nThe concept of distance between probability measures is a fundamental one and has found\nmany applications in probability theory, information theory and statistics (Rachev, 1991;\nRachev and R\u00a8uschendorf, 1998; Liese and Vajda, 2006).\nIn statistics, distances between\nprobability measures are used in a variety of applications, including hypothesis tests (ho-\nmogeneity tests, independence tests, and goodness-of-\ufb01t tests), density estimation, Markov\nchain monte carlo, etc. As an example, homogeneity testing, also called the two-sample\nproblem, involves choosing whether to accept or reject a null hypothesis H0 : P = Q versus\nthe alternative H1 : P \u0338= Q, using random samples {Xj}m\nj=1 and {Yj}n\nj=1 drawn i.i.d. from\nprobability distributions P and Q on a topological space (M, A). It is easy to see that solving\nthis problem is equivalent to testing H0 : \u03b3(P, Q) = 0 versus H1 : \u03b3(P, Q) > 0, where \u03b3 is a\nmetric (or, more generally, a semi-metric1) on the space of all probability measures de\ufb01ned\non M. The problems of testing independence and goodness-of-\ufb01t can be posed in an analo-\ngous form. In non-parametric density estimation, \u03b3(pn, p0) can be used to study the quality\nof the density estimate, pn, that is based on the samples {Xj}n\nj=1 drawn i.i.d. from p0. Pop-\nular examples for \u03b3 in these statistical applications include the Kullback-Leibler divergence,\nthe total variation distance, the Hellinger distance (Vajda, 1989) \u2014 these three are spe-\nci\ufb01c instances of the generalized \u03c6-divergence (Ali and Silvey, 1966; Csisz\u00b4ar, 1967) \u2014 the\nKolmogorov distance (Lehmann and Romano, 2005, Section 14.2), the Wasserstein distance\n(del Barrio et al., 1999), etc.\nIn probability theory, the distance between probability measures is used in studying\nlimit theorems, the popular example being the central limit theorem. Another application\nis in metrizing the weak convergence of probability measures on a separable metric space,\nwhere the L\u00b4evy-Prohorov distance (Dudley, 2002, Chapter 11) and dual-bounded Lipschitz\ndistance (also called the Dudley metric) (Dudley, 2002, Chapter 11) are commonly used.\nIn the present work, we will consider a particular pseudometric1 on probability distribu-\ntions which is an instance of an integral probability metric (IPM) (M\u00a8uller, 1997). Denoting\nP the set of all Borel probability measures on (M, A), the IPM between P \u2208P and Q \u2208P\nis de\ufb01ned as\n\u03b3F(P, Q) = sup\nf\u2208F\n\f\f\f\f\nZ\nM\nf dP \u2212\nZ\nM\nf dQ\n\f\f\f\f ,\n(1)\n1. Given a set M, a metric for M is a function \u03c1 : M \u00d7 M \u2192R+ such that (i) \u2200x, \u03c1(x, x) = 0, (ii)\n\u2200x, y, \u03c1(x, y) = \u03c1(y, x), (iii) \u2200x, y, z, \u03c1(x, z) \u2264\u03c1(x, y) + \u03c1(y, z), and (iv) \u03c1(x, y) = 0 \u21d2x = y.\nA\nsemi-metric only satis\ufb01es (i), (ii) and (iv). A pseudometric only satis\ufb01es (i)-(iii) of the properties of a\nmetric. Unlike a metric space (M, \u03c1), points in a pseudometric space need not be distinguishable: one\nmay have \u03c1(x, y) = 0 for x \u0338= y.\nNow, in the two-sample test, though we mentioned that \u03b3 is a metric/semi-metric, it is su\ufb03cient\nthat \u03b3 satis\ufb01es (i) and (iv).\n2\nHilbert Space Embedding and Characteristic Kernels\nwhere F is a class of real-valued bounded measurable functions on M. In addition to the gen-\neral application domains discussed earlier for metrics on probabilities, IPMs have been used\nin proving central limit theorems using Stein\u2019s method (Stein, 1972; Barbour and Chen,\n2005), and are popular in empirical process theory (van der Vaart and Wellner, 1996). Since\nmost of the applications listed above require \u03b3F to be a metric on P, the choice of F is\ncritical (note that irrespective of F, \u03b3F is a pseudometric on P). The following are some\nexamples of F for which \u03b3F is a metric.\n(a) F = Cb(M), the space of bounded continuous functions on (M, \u03c1), where \u03c1 is a metric\n(Shorack, 2000, Chapter 19, De\ufb01nition 1.1).\n(b) F = Cbu(M), the space of bounded \u03c1-uniformly continuous functions on (M, \u03c1) \u2014\nPortmonteau theorem (Shorack, 2000, Chapter 19, Theorem 1.1).\n(c) F = {f : \u2225f\u2225\u221e\u22641} =: FTV , where \u2225f\u2225\u221e= supx\u2208M |f(x)|. \u03b3F is called the total\nvariation distance (Shorack, 2000, Chapter 19, Proposition 2.2), which we denote as\nTV , i.e., \u03b3FT V =: TV .\n(d) F = {f : \u2225f\u2225L \u22641} =: FW , where \u2225f\u2225L := sup{|f(x) \u2212f(y)|/\u03c1(x, y) : x \u0338= y in M}.\n\u2225f\u2225L is the Lipschitz semi-norm of a real-valued function f on M and \u03b3F is called the\nKantorovich metric. If (M, \u03c1) is separable, then \u03b3F equals the Wasserstein distance\n(Dudley, 2002, Theorem 11.8.2), denoted as W := \u03b3FW .\n(e) F = {f : \u2225f\u2225BL \u22641} =: F\u03b2, where \u2225f\u2225BL := \u2225f\u2225L + \u2225f\u2225\u221e. \u03b3F is called the Dudley\nmetric (Shorack, 2000, Chapter 19, De\ufb01nition 2.2), denoted as \u03b2 := \u03b3F\u03b2.\n(f) F = {1(\u2212\u221e,t] : t \u2208Rd} =: FKS. \u03b3F is called the Kolmogorov distance (Shorack, 2000,\nTheorem 2.4).\n(g) F = {e\n\u221a\u22121\u27e8\u03c9,\u00b7\u27e9: \u03c9 \u2208Rd} =: Fc. This choice of F results in the maximal di\ufb00erence\nbetween the characteristic functions of P and Q. That \u03b3Fc is a metric on P follows\nfrom the uniqueness theorem for characteristic functions (Dudley, 2002, Theorem\n9.5.1).\nRecently, Gretton et al. (2007) and Smola et al. (2007) considered F to be the unit ball\nin a reproducing kernel Hilbert space (RKHS) H (Aronszajn, 1950), with k as its reproduc-\ning kernel (r.k.), i.e., F = {f : \u2225f\u2225H \u22641} =: Fk (also see Chapter 4 of Berlinet and Thomas-Agnan\n(2004) and references therein for related work): we denote \u03b3Fk =: \u03b3k. While we have seen\nmany possible F for which \u03b3F a metric, Fk has a number of important advantages:\n\u2022 Estimation of \u03b3F: In applications such as hypothesis testing, P and Q are known\nonly through the respective random samples {Xj}m\nj=1 and {Yj}n\nj=1 drawn i.i.d. from\neach, and \u03b3F(P, Q) is estimated based on these samples. One approach is to com-\npute \u03b3F(P, Q) using the empirical measures Pm = 1\nm\nPm\nj=1 \u03b4Xj and Qn = 1\nn\nPn\nj=1 \u03b4Yj,\nwhere \u03b4x represents a Dirac measure at x.\nIt can be shown that choosing F as\nCb(M), Cbu(M), FTV or Fc results in this approach not yielding consistent estimates\nof \u03b3F(P, Q) for all P and Q (Devroye and Gy\u00a8or\ufb01, 1990). Although choosing F = FW\nor F\u03b2 yields consistent estimates of \u03b3F(P, Q) for all P and Q when M = Rd, the rates\n3\nSriperumbudur, Gretton, Fukumizu, Sch\u00a8olkopf and Lanckriet\nof convergence are dependent on d and become slow for large d (Sriperumbudur et al.,\n2009b). On the other hand, \u03b3k(Pm, Qn) is a\np\nmn/(m + n)-consistent estimator of\n\u03b3k(P, Q) if k is measurable and bounded, for all P and Q. If k is translation invariant\non M = Rd, the rate is independent of d (Gretton et al., 2007; Sriperumbudur et al.,\n2009b), an important property when dealing with high dimensions. Moreover, \u03b3F is\nnot straightforward to compute when F is Cb(M), Cbu(M), FW or F\u03b2 (Weaver, 1999,\nSection 2.3): by contrast, \u03b32\nk(P, Q) is simply a sum of expectations of the kernel k (see\nTheorem 1 and (13)).\n\u2022 Comparison to \u03c6-divergences: Instead of using \u03b3F in statistical applications, one\ncan also use \u03c6-divergences. However, the estimators of \u03c6-divergences (especially the\nKullback-Leibler divergence) exhibit arbitrarily slow rates of convergence depend-\ning on the distributions (see Wang et al. (2005); Nguyen et al. (2008) and references\ntherein for details), while, as noted above, \u03b3k(Pm, Qn) exhibits good convergence be-\nhavior.\n\u2022 Structured domains: Since \u03b3k is dependent only on the kernel (see Theorem 1) and\nkernels can be de\ufb01ned on arbitrary domains M (Aronszajn, 1950), choosing F = Fk\nprovides the \ufb02exibility of measuring the distance between probability measures de\ufb01ned\non structured domains (Borgwardt et al., 2006) like graphs, strings, etc., unlike F =\nFKS or Fc, which can handle only M = Rd.\nThe distance measure \u03b3k has appeared in a wide variety of applications.\nThese in-\nclude statistical hypothesis testing, of homogeneity (Gretton et al., 2007), independence\n(Gretton et al., 2008), and conditional independence (Fukumizu et al., 2008); as well as in\nmachine learning applications including kernel independent component analysis (Bach and Jordan,\n2002; Gretton et al., 2005) and kernel based dimensionality reduction for supervised learn-\ning (Fukumizu et al., 2004). In these applications, kernels o\ufb00er a linear approach to deal\nwith higher order statistics: given the problem of homogeneity testing, for example, di\ufb00er-\nences in higher order moments are encoded as di\ufb00erences in the means of nonlinear features\nof the variables. To capture all nonlinearities that are relevant to the problem at hand, the\nembedding RKHS therefore has to be \u201csu\ufb03ciently large\u201d that di\ufb00erences in the embeddings\ncorrespond to di\ufb00erences of interest in the distributions. Thus, a natural question is how\nto guarantee k provides a su\ufb03ciently rich RKHS so as to detect any di\ufb00erence in distri-\nbutions. A second problem is to determine what properties of distributions result in their\nbeing proximate or distant in the embedding space. Finally, we would like to compare \u03b3k to\nthe classical integral probability metrics listed earlier, when used to measure convergence of\ndistributions. In the following section, we describe the contributions of the present paper,\naddressing each of these three questions in turn.\n1.1 Contributions\nThe contributions in this paper are three-fold and explained in detail below.\n1.1.1 When is H characteristic?\nRecently, Fukumizu et al. (2008) introduced the concept of a characteristic kernel, i.e., a\nreproducing kernel for which \u03b3k(P, Q) = 0 \u21d4P = Q, P, Q \u2208P, i.e., \u03b3k is a metric on P.\n4\nHilbert Space Embedding and Characteristic Kernels\nThe corresponding RKHS, H is referred to as a characteristic RKHS. The following are two\ncharacterizations for characteristic RKHSs that have already been studied in literature:\n1. When M is compact, Gretton et al. (2007) showed that H is characteristic if k is\nuniversal in the sense of Steinwart (2001, De\ufb01nition 4), i.e., H is dense in the Banach\nspace of bounded continuous functions with respect to the supremum norm. Examples\nof such H include those induced by the Gaussian and Laplacian kernels on every\ncompact subset of Rd.\n2. Fukumizu et al. (2008, 2009a) extended this characterization to non-compact M and\nshowed that H is characteristic if and only if the direct sum of H and R is dense in\nthe Banach space of r-integrable (for some r \u22651) functions. Using this characteri-\nzation, they showed that the RKHSs induced by the Gaussian and Laplacian kernels\n(supported on the entire Rd) are characteristic.\nIn the present study, we provide alternative conditions for characteristic RKHSs which\naddress several limitations of the foregoing. First, it can be di\ufb03cult to verify the conditions\nof denseness in both of the above characterizations. Second, universality is in any case an\noverly restrictive condition because universal kernels assume M to be compact, i.e., they\ninduce a metric only on the space of probability measures that are supported on compact\nM. In addition, there are compactly supported kernels which are not universal, e.g., B2n+1-\nsplines (Steinwart, 2001), which can be shown to be characteristic.\nIn Section 3.1, we present the simple characterization that integrally strictly positive\nde\ufb01nite (pd) kernels (see Section 1.2 for the de\ufb01nition) are characteristic, i.e., the induced\nRKHS is characteristic (also see Sriperumbudur et al., 2009a, Theorem 4). This condition\nis more natural \u2014 strict pd is a natural property of interest for kernels, unlike the denseness\ncondition \u2014 and much easier to understand than the characterizations mentioned above.\nExamples of integrally strictly pd kernels on Rd include the Gaussian, Laplacian, inverse\nmultiquadratics, Mat\u00b4ern kernel family, B2n+1-splines, etc.\nAlthough the above characterization of integrally strictly pd kernels being characteristic\nis simple to understand, it is only a su\ufb03cient condition and does not provide an answer for\nkernels that are not integrally strictly pd,2 e.g., a Dirichlet kernel. Therefore, in Section 3.2,\nwe provide an easily checkable condition, after making some assumptions on the kernel. We\npresent a complete characterization of characteristic kernels when the kernel is translation\ninvariant on Rd. We show that a bounded continuous translation invariant kernel on Rd\nis characteristic if and only if the support of the Fourier transform of the kernel is the\nentire Rd.\nThis condition is easy to check compared to the characterizations described\nabove. An earlier version of this result was provided by Sriperumbudur et al. (2008): by\ncomparison, we now present a simpler and more elegant proof.\nWe also show that all\ncompactly supported translation invariant kernels on Rd are characteristic. Note, however,\nthat the characterization of integral strict positive de\ufb01niteness in Section 3.1 does not\nassume M to be Rd nor k to be translation invariant.\nWe extend the result of Section 3.2 to M being a d-Torus, i.e., Td = S1\u00d7\nd. . . \u00d7S1 \u2261\n[0, 2\u03c0)d, where S1 is a circle. In Section 3.3, we show that a translation invariant kernel on\n2. It can be shown that integrally strictly pd kernels are strictly pd (see footnote 4). Therefore, examples\nof kernels that are not integrally strictly pd include those kernels that are not strictly pd.\n5\nSriperumbudur, Gretton, Fukumizu, Sch\u00a8olkopf and Lanckriet\nTd is characteristic if and only if the Fourier series coe\ufb03cients of the kernel are positive,\ni.e., the support of the Fourier spectrum is the entire Zd. The proof of this result is similar\nin \ufb02avor to the one in Section 3.2. As examples, the Poisson kernel can be shown to be\ncharacteristic, while the Dirichlet kernel is not.\nBased on the discussion so far, it is clear that the characteristic property of k is char-\nacterized in many ways. Given these characterizations, we would like to understand the\nrelation betweeen them. For example, we know that if k is universal, then it is characteristic.\nIs the converse true? Similarly, as we mentioned before, integrally strictly pd kernels are\ncharacteristic and are also also strictly pd. Then what is the relation between characteristic\nand strictly pd kernels? In Section 3.4, we address these questions by exploring the relation\nbetween these characterizations, which are summarized in Figure 1.\n1.1.2 Dissimilar distributions with small \u03b3k\nAs we have seen, the characteristic property of a kernel is critical in distinguishing between\ndistinct probability measures. Suppose, however, that for a given characteristic kernel k and\nfor any \u03b5 > 0, there exist P and Q, P \u0338= Q, such that \u03b3k(P, Q) < \u03b5. Though k distinguishes\nbetween such P and Q, it can be di\ufb03cult to tell the distributions apart in applications (even\nwith characteristic kernels), since P and Q are then replaced with \ufb01nite samples, and the\ndistance between them may not be statistically signi\ufb01cant (Gretton et al., 2007). Therefore,\ngiven a characteristic kernel, it is of interest to determine the properties of distributions\nP and Q that will cause their embeddings to be close. To this end, in Section 4, we show\nthat given a kernel k (see Theorem 19 for conditions on the kernel), for any \u03b5 > 0, there\nexists P \u0338= Q (with non-trivial di\ufb00erences between them) such that \u03b3k(P, Q) < \u03b5. These\ndistributions are constructed so as to di\ufb00er at a su\ufb03ciently high frequency, which is then\npenalized by the RKHS norm when computing \u03b3k.\n1.1.3 When does \u03b3k metrize the weak topology on P?\nGiven \u03b3k, which is a metric on P, a natural question of theoretical and practical importance\nto ask is \u201chow is \u03b3k related to other probability metrics, such as the Dudley metric (\u03b2),\nWasserstein distance (W), total variation metric (TV ), etc?\u201d For example, in applications\nlike density estimation, wherein the unknown density is estimated based on \ufb01nite samples\ndrawn i.i.d. from it, the quality of the estimate is measured by computing the distance\nbetween the true density and the estimated density. In such a setting, given two probability\nmetrics, \u03c11 and \u03c12, one might want to use the stronger3 of the two to determine this distance,\nas the convergence of the estimated density to the true density in the stronger metric implies\nthe convergence in the weaker metric, while the converse is not true. On the other hand, one\nmight need to use a metric of weaker topology (i.e., coarser topology) to show convergence\nof some estimators, as the convergence might not occur w.r.t. a metric of strong topology.\nClarifying and comparing the topology of a metric on the probabilities is, thus, important\n3. Two metrics \u03c11 : Y \u00d7Y \u2192R+ and \u03c12 : Y \u00d7Y \u2192R+ are said to be equivalent if \u03c11(x, y) = 0 \u21d4\u03c12(x, y) =\n0, \u2200x, y \u2208Y . On the other hand, \u03c11 is said to be stronger than \u03c12 if \u03c11(x, y) = 0 \u21d2\u03c12(x, y) = 0, \u2200x, y \u2208\nY but not vice-versa. If \u03c11 is stronger than \u03c12, then we say \u03c12 is weaker than \u03c11. Note that if \u03c11 is\nstronger (resp. weaker) than \u03c12, then the topology induced by \u03c11 is \ufb01ner (resp. coarser) than the one\ninduced by \u03c12.\n6\nHilbert Space Embedding and Characteristic Kernels\nin the analysis of density estimation. Based on this motivation, in Section 5, we analyze\nthe relation between \u03b3k and other probability metrics, and show that \u03b3k is weaker than all\nthese other metrics.\nIt is well known in probability theory that \u03b2 is weaker than W and TV , and it metrizes\nthe weak topology (we will provide formal de\ufb01nitions in Section 5) on P (Shorack, 2000;\nGibbs and Su, 2002). Since \u03b3k is weaker than all these other probability metrics, i.e., the\ntopology induced by \u03b3k is coarser than the one induced by these metrics, the next inter-\nesting question to answer would be, \u201cWhen does \u03b3k metrize the weak topology on P?\u201d In\nother words, for what k, does the topology induced by \u03b3k coincides with the weak topology?\nAnswering this question would show that \u03b3k is equivalent to \u03b2, while it is weaker than W\nand TV . In probability theory, the metrization of weak topology is of prime importance in\nproving results related to the weak convergence of probability measures. Therefore, knowing\nthe answer to the above question will help in using \u03b3k as a theoretical tool in probability\ntheory. To this end, in Section 5, we show that universal kernels on compact (M, \u03c1) metrize\nthe weak topology on P. For the non-compact setting, we assume M = Rd and provide\nsu\ufb03cient conditions on the kernel such that \u03b3k metrizes the weak topology on P.\nIn the following section, we introduce the notation and some de\ufb01nitions that are used\nthroughout the paper. Supplementary results used in proofs are collected in Appendix A.\n1.2 De\ufb01nitions and notation\nFor M \u2282Rd and \u00b5 a Borel measure on M, Lr(M, \u00b5) denotes the Banach space of r-power\n(r \u22651) \u00b5-integrable functions. We will also use Lr(M) for Lr(M, \u00b5) and dx for d\u00b5(x) if\n\u00b5 is the Lebesgue measure on M. Cb(M) denotes the space of all bounded, continuous\nfunctions on M. The space of all r-continuously di\ufb00erentiable functions on M is denoted\nby Cr(M), 0 \u2264r \u2264\u221e. For x \u2208C, x represents the complex conjugate of x. We denote as\ni the imaginary unit \u221a\u22121.\nFor a measurable function f and a signed measure P, Pf :=\nR\nf dP =\nR\nM f(x) dP(x).\n\u03b4x represents the Dirac measure at x. The symbol \u03b4 is overloaded to represent the Dirac\nmeasure, the Dirac-delta distribution, and the Kronecker-delta, which should be distinguish-\nable from the context. For M = Rd, the characteristic function, \u03c6P of P \u2208P is de\ufb01ned as\n\u03c6P(\u03c9) :=\nR\nRd ei\u03c9T x dP(x), \u03c9 \u2208Rd.\nVanishing at in\ufb01nity and C0(M): A complex function f on a locally compact Hausdor\ufb00\nspace M is said to vanish at in\ufb01nity if for every \u01eb > 0 there exists a compact set K \u2282M\nsuch that |f(x)| < \u01eb for all x /\u2208K. The class of all continuous f on M which vanish at\nin\ufb01nity is denoted as C0(M).\nHolomorphic and entire functions: Let D \u2282Cd be an open subset and f : D \u2192C be\na function. f is said to be holomorphic at the point z0 \u2208D if\nf \u2032(z0) := lim\nz\u2192z0\nf(z0) \u2212f(z)\nz0 \u2212z\n(2)\nexists. Moreover, f is called holomorphic if it is holomorphic at every z0 \u2208D. f is called\nan entire function if f is holomorphic and D = Cd.\n7\nSriperumbudur, Gretton, Fukumizu, Sch\u00a8olkopf and Lanckriet\nPositive de\ufb01nite and strictly positive de\ufb01nite: A function k : M \u00d7 M \u2192R is called\npositive de\ufb01nite (pd) if, for all n \u2208N, \u03b11, . . . , \u03b1n \u2208R and all x1, . . . , xn \u2208M, we have\nn\nX\ni,j=1\n\u03b1i\u03b1jk(xi, xj) \u22650.\n(3)\nFurthermore, k is said to be strictly pd if, for mutually distinct x1, . . . , xn \u2208X, equality in\n(3) only holds for \u03b11 = \u00b7 \u00b7 \u00b7 = \u03b1n = 0. \u03c8 is said to be a positive de\ufb01nite function on Rd if\nk(x, y) = \u03c8(x \u2212y) is positive de\ufb01nite.\nIntegrally strictly positive de\ufb01nite: Let M be a topological space. A measurable and\nbounded kernel, k is said to be integrally strictly positive de\ufb01nite if\nZZ\nM\nk(x, y) d\u00b5(x) d\u00b5(y) > 0\n(4)\nfor all \ufb01nite non-zero signed Borel measures, \u00b5 de\ufb01ned on M.\nThe above de\ufb01nition is a generalization of integrally strictly positive de\ufb01nite functions\n(Stewart, 1976, Section 6):\nRR\nM k(x, y)f(x)f(y) dx dy > 0 for all f \u2208L2(Rd), which is the\nstrictly positive de\ufb01niteness of the integral operator given by the kernel. Note that the\nabove de\ufb01nition is not equivalent to the de\ufb01nition of strictly pd kernels: if k is integrally\nstrictly pd, then it is strictly pd, while the converse is not true.4\nFourier transform in Rd: For f \u2208L1(Rd), bf and f \u2228represent the Fourier transform and\ninverse Fourier transform of f respectively, de\ufb01ned as\nbf(y) :=\n1\n(2\u03c0)d/2\nZ\nRd e\u2212iyT xf(x) dx, y \u2208Rd,\n(5)\nf \u2228(x) :=\n1\n(2\u03c0)d/2\nZ\nRd eixT yf(y) dy, x \u2208Rd.\n(6)\nConvolution: If f and g are complex functions in Rd, their convolution f \u2217g is de\ufb01ned by\n(f \u2217g)(x) :=\nZ\nRd f(y)g(x \u2212y) dy,\n(7)\nprovided that the integral exists for almost all x \u2208Rd, in the Lebesgue sense. Let \u00b5 be a\n\ufb01nite Borel measure on Rd and f be a bounded measurable function on Rd. The convolution\nof f and \u00b5, f \u2217\u00b5, which is a bounded measurable function, is de\ufb01ned by\n(f \u2217\u00b5)(x) :=\nZ\nRd f(x \u2212y) d\u00b5(y).\n(8)\n4. Suppose k is not strictly pd. This means for all n \u2208N and for all mutually distinct x1, . . . , xn \u2208M,\nthere exists R \u220b\u03b1j \u0338= 0 for some j \u2208{1, . . . , n} such that Pn\nj,l=1 \u03b1j\u03b1lk(xj, xl) = 0.\nBy de\ufb01ning\n\u00b5 = Pn\nj=1 \u03b1j\u03b4xj, it is easy to see that there exists \u00b5 \u0338= 0 such that\nRR\nM k(x, y) d\u00b5(x) d\u00b5(y) = 0, which\nmeans k is not integrally strictly pd.\nTherefore, if k is integrally strictly pd, then it is strictly pd.\nHowever, the converse is not true. See Steinwart and Christmann (2008, Proposition 4.60, Theorem\n4.62) for an example.\n8\nHilbert Space Embedding and Characteristic Kernels\nRapidly decaying functions, Dd and Sd: Let Dd be the space of compactly supported\nin\ufb01nitely di\ufb00erentiable functions on Rd, i.e., Dd = {f \u2208C\u221e(Rd) | supp(f) is bounded},\nwhere supp(f) = cl\n\u0000{x \u2208Rd | f(x) \u0338= 0}\n\u0001\n. A function f : Rd \u2192C is said to decay rapidly,\nor be rapidly decreasing, if for all N \u2208N,\nsup\n\u2225\u03b1\u22251\u2264N\nsup\nx\u2208Rd(1 + \u2225x\u22252\n2)N|(T\u03b1f)(x)| < \u221e,\n(9)\nwhere \u03b1 = (\u03b11, . . . , \u03b1d) is an ordered d-tuple of non-negative \u03b1j, \u2225\u03b1\u22251 = Pd\nj=1 \u03b1j and\nT\u03b1 =\n\u0010\n1\ni\n\u2202\n\u2202x1\n\u0011\u03b11 \u00b7 \u00b7 \u00b7\n\u0010\n1\ni\n\u2202\n\u2202xd\n\u0011\u03b1d. Sd, called the Schwartz class, denotes the vector space of\nrapidly decreasing functions. Note that Dd \u2282Sd. It can be shown that for any f \u2208Sd,\nbf \u2208Sd and f \u2228\u2208Sd (see Folland (1999, Chapter 9) and Rudin (1991, Chapter 6) for\ndetails).\nDistributions, tempered distributions, D\u2032\nd and S \u2032\nd: A linear functional on Dd which is\ncontinuous with respect to the Fr\u00b4echet topology (see Rudin, 1991, De\ufb01nition 6.3) is called a\ndistribution in Rd. The space of all distributions in Rd is denoted by D\u2032\nd. A linear continuous\nfunctional over the space Sd is called a tempered distribution and the space of all tempered\ndistributions in Rd is denoted by S \u2032\nd.\nSupport of a distribution: For an open set U \u2282Rd, Dd(U) denotes the subspace of Dd\nconsisting of the functions with support contained in U. Suppose D \u2208D\u2032\nd. If U is an open\nset of Rd and if D(\u03d5) = 0 for every \u03d5 \u2208Dd(U), then D is said to vanish or be null in U.\nLet W be the union of all open U \u2282Rd in which D vanishes. The complement of W is the\nsupport of D.\nFor complete details on distribution theory and Fourier transforms of distributions, we\nrefer the reader to Folland (1999, Chapter 9) and Rudin (1991, Chapter 6).\n2. Hilbert Space Embedding of Probability Measures\nWe previously mentioned that \u03b3k is related to the theory of RKHS embedding of probability\nmeasures described in Gretton et al. (2007); Smola et al. (2007), and originally introduced\nand studied in the late 70\u2019s and early 80\u2019s (see Berlinet and Thomas-Agnan (2004, Chap-\nter 4) and references therein for details). The following result shows how such embedding\ncan be obtained through an alternative representation for \u03b3k.\nTheorem 1 Let Pk := {P \u2208P :\nR\nM\np\nk(x, x) dP(x) < \u221e}, where k is measurable on M.\nThen for any P, Q \u2208Pk,\n\u03b3k(P, Q) =\n\r\r\r\r\nZ\nM\nk(\u00b7, x) dP(x) \u2212\nZ\nM\nk(\u00b7, x) dQ(x)\n\r\r\r\r\nH\n=: \u2225Pk \u2212Qk\u2225H,\n(10)\nwhere H is the RKHS generated by k.\nProof\nLet TP : H \u2192R be the linear functional de\ufb01ned as TP[f] :=\nR\nM f(x) dP(x) with\n\u2225TP\u2225:= supf\u2208H,f\u0338=0\n|TP[f]|\n\u2225f\u2225H . Consider\n|TP[f]| =\n\f\f\f\f\nZ\nM\nf dP\n\f\f\f\f \u2264\nZ\nM\n|f(x)| dP(x) =\nZ\nM\n|\u27e8f, k(\u00b7, x)\u27e9H| dP(x) \u2264\nZ\nM\np\nk(x, x)\u2225f\u2225H dP(x),\n9\nSriperumbudur, Gretton, Fukumizu, Sch\u00a8olkopf and Lanckriet\nwhich implies \u2225TP\u2225< \u221e, \u2200P \u2208Pk, i.e., TP is a bounded linear functional on H. There-\nfore, by the Riesz representation theorem (Reed and Simon, 1972, Theorem II.4), for each\nP \u2208Pk, there exists a unique \u03bbP \u2208H such that TP[f] = \u27e8f, \u03bbP\u27e9H, \u2200f \u2208H.\nLet\nf = k(\u00b7, u) for some u \u2208M.\nThen, TP[k(\u00b7, u)] = \u27e8k(\u00b7, u), \u03bbP\u27e9H = \u03bbP(u), which implies\n\u03bbP =\nR\nM k(\u00b7, x) dP(x) =: Pk. Therefore, with\n|Pf \u2212Qf| = |TP[f] \u2212TQ[f]| = |\u27e8f, \u03bbP\u27e9H \u2212\u27e8f, \u03bbQ\u27e9H| = |\u27e8f, \u03bbP \u2212\u03bbQ\u27e9H| ,\nwe have\n\u03b3k(P, Q) =\nsup\n\u2225f\u2225H\u22641\n|Pf \u2212Qf| = \u2225\u03bbP \u2212\u03bbQ\u2225H = \u2225Pk \u2212Qk\u2225H.\nNote that this holds for any P, Q \u2208Pk.\nGiven a kernel, k, (10) holds for all P \u2208Pk. However, in practice, especially in statistical\ninference applications, it is not possible to check whether P \u2208Pk as P is not known.\nTherefore, one would prefer to have a kernel such that\nZ\nM\np\nk(x, x) dP(x) < \u221e, \u2200P \u2208P.\n(11)\nThe following proposition shows that (11) is equivalent to the kernel being bounded. There-\nfore, combining Theorem 1 and Proposition 2 shows that if k is measurable and bounded,\nthen \u03b3k(P, Q) = \u2225Pk \u2212Qk\u2225H for any P, Q \u2208P.\nProposition 2 Let f be a measurable function on M. Then\nR\nM f(x) dP(x) < \u221efor all\nP \u2208P if and only if f is bounded.\nProof One direction is straightforward because if f is bounded, then\nR\nM f(x) dP(x) < \u221e\nfor all P \u2208P.\nLet us consider the other direction.\nSuppose f is not bounded. Then\nthere exists a sequence {xn} \u2282M such that f(xn) n\u2192\u221e\n\u2212\u2192\u221e. By taking a subsequence, if\nnecessary, we can assume f(xn) > n2 for all n. Then, A := P\u221e\nn=1\n1\nf(xn) < \u221e. De\ufb01ne a\nprobability measure P on M by P = P\u221e\nn=1\n1\nAf(xn) \u03b4xn, where \u03b4xn is a Dirac measure at xn.\nThen,\nR\nM f(x) dP(x) =\n1\nA\nP\u221e\nn=1\nf(xn)\nf(xn) = \u221e, which means if f is not bounded, then there\nexists a P \u2208P such that\nR\nM f(x) dP(x) = \u221e.\nThe representation of \u03b3k in (10) yields the embedding,\n\u03a0 : P \u2192H\nP 7\u2192\nZ\nM\nk(\u00b7, x) dP(x),\n(12)\nas proposed by Berlinet and Thomas-Agnan (2004, Chapter 4, Section 1.1) and Smola et al.\n(2007).\nBerlinet and Thomas-Agnan (2004) derived this embedding as a generalization\nof \u03b4x 7\u2192k(\u00b7, x) (see Chapter 4 of Berlinet and Thomas-Agnan (2004) for details), while\nGretton et al. (2007) arrived at this embedding by choosing F = Fk in (1). Since \u03b3k(P, Q) =\n\u2225\u03a0[P] \u2212\u03a0[Q]\u2225H, the question \u201cWhen is \u03b3k a metric on P?\u201d is equivalent to the question\n\u201cWhen is \u03a0 injective?\u201d. Addressing these questions is the central focus of the paper and is\ndiscussed in Section 3.\nBefore proceeding further, we present some other, equivalent representations of \u03b3k which\nwill not only improve our understanding of \u03b3k, but also be helpful in its computation. First,\n10\nHilbert Space Embedding and Characteristic Kernels\nnote that by exploiting the reproducing property of k, \u03b3k can be equivalently represented\nas\n\u03b32\nk(P, Q) =\n\r\r\r\r\nZ\nM\nk(\u00b7, x) dP(x) \u2212\nZ\nM\nk(\u00b7, x) dQ(x)\n\r\r\r\r\n2\nH\n=\n\u001cZ\nM\nk(\u00b7, x) dP(x) \u2212\nZ\nM\nk(\u00b7, x) dQ(x),\nZ\nM\nk(\u00b7, y) dP(y) \u2212\nZ\nM\nk(\u00b7, y) dQ(y)\n\u001d\nH\n=\n\u001cZ\nM\nk(\u00b7, x) dP(x),\nZ\nM\nk(\u00b7, y) dP(y)\n\u001d\nH\n+\n\u001cZ\nM\nk(\u00b7, x) dQ(x),\nZ\nM\nk(\u00b7, y) dQ(y)\n\u001d\nH\n\u22122\n\u001cZ\nM\nk(\u00b7, x) dP(x),\nZ\nM\nk(\u00b7, y) dQ(y)\n\u001d\nH\n(a)\n=\nZZ\nM\nk(x, y) dP(x) dP(y) +\nZZ\nM\nk(x, y) dQ(x) dQ(y)\n\u22122\nZZ\nM\nk(x, y) dP(x) dQ(y)\n(13)\n=\nZZ\nM\nk(x, y) d(P \u2212Q)(x) d(P \u2212Q)(y),\n(14)\nwhere (a) follows from the fact that\nR\nM f(x) dP(x) = \u27e8f,\nR\nM k(\u00b7, x) dP(x)\u27e9H for all f \u2208H,\nP \u2208P (see proof of Theorem 1), applied with f =\nR\nM k(\u00b7, y) dP(y).\nAs motivated in\nSection 1, \u03b32\nk is a straightforward sum of expectations of k, and can be computed easily,\ne.g., using (13) either in closed form or using numerical integration techniques, depending\non the choice of k, P and Q. It is easy to show that, if k is a Gaussian kernel with P\nand Q being normal distributions on Rd, then \u03b3k can be computed in a closed form (see\nSriperumbudur et al. (2009b, Section III-C) for examples). In the following corollary to\nTheorem 1, we prove three results which provide a nice interpretation for \u03b3k when M = Rd\nand k is translation invariant, i.e., k(x, y) = \u03c8(x\u2212y), where \u03c8 is a positive de\ufb01nite function.\nWe provide a detailed explanation for Corollary 4 in Remark 5. Before stating the results,\nwe need a famous result due to Bochner, that characterizes \u03c8. We quote this result from\nWendland (2005, Theorem 6.6).\nTheorem 3 (Bochner) A continuous function \u03c8 : Rd \u2192R is positive de\ufb01nite if and only\nif it is the Fourier transform of a \ufb01nite nonnegative Borel measure \u039b on Rd, i.e.,\n\u03c8(x) =\nZ\nRd e\u2212ixT \u03c9 d\u039b(\u03c9), x \u2208Rd.\n(15)\nCorollary 4 (Di\ufb00erent interpretations of \u03b3k) (i) Let M = Rd and k(x, y) = \u03c8(x \u2212y),\nwhere \u03c8 : M \u2192R is a bounded, continuous positive de\ufb01nite function. Then for any P, Q \u2208\nP,\n\u03b3k(P, Q) =\nsZ\nRd |\u03c6P(\u03c9) \u2212\u03c6Q(\u03c9)|2 d\u039b(\u03c9) =: \u2225\u03c6P \u2212\u03c6Q\u2225L2(Rd,\u039b),\n(16)\nwhere \u03c6P and \u03c6Q represent the characteristic functions of P and Q respectively.\n11\nSriperumbudur, Gretton, Fukumizu, Sch\u00a8olkopf and Lanckriet\n(ii) Suppose \u03b8 \u2208L1(Rd) is a continuous bounded positive de\ufb01nite function and\nR\nRd \u03b8(x) dx =\n1. Let \u03c8(x) := \u03c8t(x) = t\u2212d\u03b8(t\u22121x). Assume that p and q are bounded uniformly continuous\nRadon-Nikodym derivatives of P and Q w.r.t. the Lebesgue measure, i.e., dP = p dx and\ndQ = q dx. Then,\nlim\nt\u21920 \u03b3k(P, Q) = \u2225p \u2212q\u2225L2(Rd).\n(17)\nIn particular, if |\u03b8(x)| \u2264C(1+\u2225x\u22252)\u2212d\u2212\u03b5 for some C, \u03b5 > 0, then (17) holds for all bounded\np and q (not necessarily uniformly continuous).\n(iii) Suppose \u03c8 \u2208L1(Rd) and\nq\nb\u03c8 \u2208L1(Rd). Then,\n\u03b3k(P, Q) = (2\u03c0)\u2212d/4\u2225\u03a6 \u2217P \u2212\u03a6 \u2217Q\u2225L2(Rd),\n(18)\nwhere \u03a6 :=\n\u0012q\nb\u03c8\n\u0013\u2228\nand d\u039b = (2\u03c0)\u2212d/2 b\u03c8 d\u03c9. Here, \u03a6 \u2217P represents the convolution of \u03a6\nand P.\nProof (i) Let us consider (14) with k(x, y) = \u03c8(x \u2212y). Then, we have\n\u03b32\nk(P, Q) =\nZZ\nRd \u03c8(x \u2212y) d(P \u2212Q)(x) d(P \u2212Q)(y)\n(a)\n=\nZZZ\nRd e\u2212i(x\u2212y)T \u03c9 d\u039b(\u03c9) d(P \u2212Q)(x) d(P \u2212Q)(y)\n(b)\n=\nZZ\nRd e\u2212ixT \u03c9 d(P \u2212Q)(x)\nZ\nRd eiyT \u03c9 d(P \u2212Q)(y) d\u039b(\u03c9)\n=\nZ\nRd (\u03c6P(\u03c9) \u2212\u03c6Q(\u03c9))\n\u0010\n\u03c6P(\u03c9) \u2212\u03c6Q(\u03c9)\n\u0011\nd\u039b(\u03c9)\n=\nZ\nRd |\u03c6P(\u03c9) \u2212\u03c6Q(\u03c9)|2 d\u039b(\u03c9),\nwhere Bochner\u2019s theorem (Theorem 3) is invoked in (a), while Fubini\u2019s theorem (Folland,\n1999, Theorem 2.37) is invoked in (b).\n(ii) Consider (13) with k(x, y) = \u03c8t(x \u2212y),\n\u03b32\nk(P, Q) =\nZZ\nRd \u03c8t(x \u2212y)p(x)p(y) dx dy +\nZZ\nRd \u03c8t(x \u2212y)q(x)q(y) dx dy\n\u22122\nZZ\nRd \u03c8t(x \u2212y)p(x)q(y) dx dy\n=\nZ\nRd(\u03c8t \u2217p)(x)p(x) dx +\nZ\nRd(\u03c8t \u2217q)(x)q(x) dx \u22122\nZ\nRd(\u03c8t \u2217q)(x)p(x) dx. (19)\nNote that limt\u21920\nR\nRd(\u03c8t \u2217p)(x)p(x) dx =\nR\nRd limt\u21920(\u03c8t \u2217p)(x)p(x) dx, by invoking the dom-\ninated convergence theorem. Since p is bounded and uniformly continuous, by Theorem 25\n(see Appendix A), we have p \u2217\u03c8t \u2192p uniformly as t \u21920, which means limt\u21920\nR\nRd(\u03c8t \u2217\np)(x)p(x) dx =\nR\nRd p2(x) dx. Using this in (19), we have\nlim\nt\u21920 \u03b32\nk(P, Q) =\nZ\nRd(p2(x) + q2(x) \u22122p(x)q(x)) dx = \u2225p \u2212q\u22252\nL2(Rd).\n12\nHilbert Space Embedding and Characteristic Kernels\nSuppose |\u03b8(x)| \u2264(1 + \u2225x\u22252)\u2212d\u2212\u03b5 for some C, \u03b5 > 0. Since p \u2208L1(Rd), by Theorem 26\n(see Appendix A), we have (p \u2217\u03c8t)(x) \u2192p(x) as t \u21920 for almost every x. Therefore\nlimt\u21920\nR\nRd(\u03c8t \u2217p)(x)p(x) dx =\nR\nRd p2(x) dx and the result follows.\n(iii) Since \u03c8 is positive de\ufb01nite, b\u03c8 is nonnegative and therefore\nq\nb\u03c8 is valid. Since\nq\nb\u03c8 \u2208\nL1(Rd), \u03a6 exists. De\ufb01ne \u03c6P,Q := \u03c6P \u2212\u03c6Q. Now, consider\n\u2225\u03a6 \u2217P \u2212\u03a6 \u2217Q\u22252\nL2(Rd) =\nZ\nRd |(\u03a6 \u2217(P \u2212Q))(x)|2 dx\n=\nZ\nRd\n\f\f\f\f\nZ\nRd \u03a6(x \u2212y) d(P \u2212Q)(y)\n\f\f\f\f\n2\ndx\n=\n1\n(2\u03c0)d\nZ\nRd\n\f\f\f\f\nZZ\nRd\nq\nb\u03c8(\u03c9) ei(x\u2212y)T \u03c9 d\u03c9 d(P \u2212Q)(y)\n\f\f\f\f\n2\ndx\n(c)\n=\n1\n(2\u03c0)d\nZ\nRd\n\f\f\f\f\nZ\nRd\nq\nb\u03c8(\u03c9)(\u03c6P(\u03c9) \u2212\u03c6Q(\u03c9)) eixT \u03c9 d\u03c9\n\f\f\f\f\n2\ndx\n=\n1\n(2\u03c0)d\nZZZ\nRd\nq\nb\u03c8(\u03c9)\nq\nb\u03c8(\u03be) \u03c6P,Q(\u03c9) \u03c6P,Q(\u03be) ei(\u03c9\u2212\u03be)T x d\u03c9 d\u03be dx\n(d)\n=\nZZ\nRd\nq\nb\u03c8(\u03c9)\nq\nb\u03c8(\u03be) \u03c6P,Q(\u03c9) \u03c6P,Q(\u03be)\n\u0014\n1\n(2\u03c0)d\nZ\nRd ei(\u03c9\u2212\u03be)T x dx\n\u0015\nd\u03c9 d\u03be\n=\nZZ\nRd\nq\nb\u03c8(\u03c9)\nq\nb\u03c8(\u03be) \u03c6P,Q(\u03c9) \u03c6P,Q(\u03be) \u03b4(\u03c9 \u2212\u03be) d\u03c9 d\u03be\n=\nZ\nRd\nb\u03c8(\u03c9) |\u03c6P(\u03c9) \u2212\u03c6Q(\u03c9)|2 d\u03c9\n= (2\u03c0)d/2\u03b32\nk(P, Q),\nwhere (c) and (d) are obtained by invoking Fubini\u2019s theorem.\nRemark 5 (a) (16) shows that \u03b3k is the L2-distance between the characteristic functions\nof P and Q computed w.r.t. the non-negative \ufb01nite Borel measure, \u039b, which is the Fourier\ntransform of \u03c8. If \u03c8 \u2208L1(Rd), then (16) is a rephrase of the well known fact (Wendland,\n2005, Theorem 10.12): for any f \u2208H,\n\u2225f\u22252\nH =\nZ\nRd\n| bf(\u03c9)|2\nb\u03c8(\u03c9)\nd\u03c9.\n(20)\nChoosing f = (P \u2212Q) \u2217\u03c8 in (20) yields bf = (\u03c6P \u2212\u03c6Q) b\u03c8 and therefore the result in (16).\n(b) Suppose d\u039b(\u03c9) = (2\u03c0)\u2212d d\u03c9. Assume P and Q have p and q as Radon-Nikodym deriva-\ntives w.r.t. the Lebesgue measure, i.e., dP = p dx and dQ = q dx. Using these in (16), it\ncan be shown that \u03b3k(P, Q) = \u2225p \u2212q\u2225L2(Rd). However, this result should be interpreted in\na limiting sense as mentioned in Corollary 4(ii) because the choice of d\u039b(\u03c9) = (2\u03c0)\u2212d d\u03c9\nimplies \u03c8(x) = \u03b4(x), which does not satisfy the conditions of Corollary 4(i). It can be shown\nthat \u03c8(x) = \u03b4(x) is obtained in a limiting sense (Folland, 1999, Proposition 9.1): \u03c8t \u2192\u03b4\nin D\u2032\nd as t \u21920.\n13\nSriperumbudur, Gretton, Fukumizu, Sch\u00a8olkopf and Lanckriet\n(c) Choosing \u03b8(x) = (2\u03c0)\u2212d/2e\u2212\u2225x\u22252\n2/2 in Corollary 4(ii) corresponds to \u03c8t being a Gaus-\nsian kernel (with appropriate normalization such that\nR\nRd \u03c8t(x) dx = 1). Therefore, (17)\nshows that as the bandwidth, t of the Gaussian kernel approaches zero, \u03b3k approaches the\nL2-distance between the densities p and q. The same result also holds for choosing \u03c8t as\nthe Laplacian kernel, B2n+1-spline, inverse multiquadratic, etc. Therefore, \u03b3k(P, Q) can be\nseen as a generalization of the L2-distance between probability measures, P and Q.\n(d) The result in (17) holds if p and q are bounded and uniformly continuous. Since any\ncondition on P and Q is usually di\ufb03cult to check in statistical applications, it is better to\nimpose conditions on \u03c8 rather than on P and Q. In Corollary 4(ii), by imposing addi-\ntional conditions on \u03c8t, the result in (17) is shown to hold for all P and Q with bounded\ndensities p and q. The condition, |\u03b8(x)| \u2264C(1 + \u2225x\u22252)\u2212d\u2212\u03b5 for some C, \u03b5 > 0, is, e.g.,\nsatis\ufb01ed by the inverse multiquadratic kernel, \u03b8(x) = eC(1+\u2225x\u22252\n2)\u2212\u03c4, x \u2208Rd, \u03c4 > d/2, where\neC =\n\u0000R\nRd(1 + \u2225x\u22252\n2)\u2212\u03c4 dx\n\u0001\u22121.\n(e) The result in Corollary 4(ii) has connections to the kernel density estimation in L2-sense\nusing Parzen windows (Rosenblatt, 1975), where \u03c8 can be chosen as the Parzen window.\n(f) (18) shows that \u03b3k is proportional to the L2-distance between \u03a6 \u2217P and \u03a6 \u2217Q. Let \u03a6\nbe such that \u03a6 is nonnegative and \u03a6 \u2208L1(Rd). Then, de\ufb01ning e\u03a6 :=\n\u0000R\nRd \u03a6(x) dx\n\u0001\u22121 \u03a6 =\n\u03a6/\nq\nb\u03c8(0) =\n\u0000R\nRd \u03c8(x) dx\n\u0001\u22121/2 \u03a6 and using this in (18), we have\n\u03b3k(P, Q) = (2\u03c0)\u2212d/4\nq\nb\u03c8(0)\n\r\r\re\u03a6 \u2217P \u2212e\u03a6 \u2217Q\n\r\r\r\nL2(Rd) .\n(21)\nThe r.h.s. of (21) can be interpreted as follows. Let X, Y and N be independent random\nvariables such that X \u223cP, Y \u223cQ and N \u223ce\u03a6. This means \u03b3k is proportional to the\nL2-distance computed between the densities associated with the perturbed random variables,\nX +N and Y +N. Note that \u2225p\u2212q\u2225L2(Rd) is the L2-distance between the densities of X and\nY . Examples of \u03c8 that satisfy the conditions in Corollary 4(iii) in addition to the conditions\non \u03a6 as mentioned here include the Gaussian and Laplacian kernels on Rd. The result in\n(18) holds even if\nq\nb\u03c8 /\u2208L1(Rd) as the proof of (iii) can be handled using distribution\ntheory. However, we assumed\nq\nb\u03c8 \u2208L1(Rd) to keep the proof simple, without delving into\ndistribution theory.\nAlthough we will not be using all the results of Corollary 4 in deriving our main results in\nthe following sections, Corollary 4 was presented to provide a better intuitive understanding\nof \u03b3k. To summarize, the core results of this section are Theorem 1 (combined with Propo-\nsition 2), which provides a closed form expression for \u03b3k in terms of the measurable and\nbounded k, and Corollary 4(i), which provides an alternative representation for \u03b3k when k\nis bounded, continuous and translation invariant on Rd.\n3. Conditions for Characteristic Kernels\nIn this section, we address the question \u201cWhen is \u03b3k a metric on P?\u201d. In other words,\n\u201cWhen is \u03a0 injective?\u201d or \u201cUnder what conditions is k characteristic?\u201d. To this end, we\nstart with the de\ufb01nition of characteristic kernels and provide some examples where k is such\n14\nHilbert Space Embedding and Characteristic Kernels\nSummary of Main Results\nDomain\nProperty\nQ\nCharacteristic\nReference\nM\nk is integrally strictly pd\nP\nYes\nTheorem 7\nRd\n\u2126= Rd\nP\nYes\nTheorem 9\nRd\nsupp(\u03c8) is compact\nP\nYes\nCorollary 10\nRd\n\u2126\u228aRd, int(\u2126) \u0338= \u2205\nP1\nYes\nTheorem 12\nRd\n\u2126\u228aRd\nP\nNo\nTheorem 9\nTd\nA\u03c8(0) \u22650, A\u03c8(n) > 0, \u2200n \u0338= 0\nP\nYes\nTheorem 14\nTd\n\u2203n \u0338= 0 | A\u03c8(n) = 0\nP\nNo\nTheorem 14\nTable 1: The table should be read as: If \u201cProperty\u201d is satis\ufb01ed on \u201cDomain\u201d, then k is\ncharacteristic (or not) to Q. P is the set of all Borel probability measures on a\ntopological space, M. See Section 1.2 for the de\ufb01nition of integrally strictly pd\nkernels. When M = Rd, k(x, y) = \u03c8(x \u2212y), where \u03c8 is a bounded, continuous\npositive de\ufb01nite function on Rd. \u03c8 is the Fourier transform of a \ufb01nite nonnegative\nBorel measure, \u039b, and \u2126:= supp(\u039b) (see Theorem 3 and footnote 5 for details).\nP1 := {P \u2208P : \u03c6P \u2208L1(Rd)\u222aL2(Rd), P \u226a\u03bb and supp(P) is compact}, where \u03c6P\nis the characteristic function of P and \u03bb is the Lebesgue measure. P \u226a\u03bb denotes\nthat P is absolutely continuous w.r.t. \u03bb. When M = Td, k(x, y) = \u03c8(x \u2212y),\nwhere \u03c8 is a bounded, continuous positive de\ufb01nite function on Td. {A\u03c8(n)}\u221e\nn=\u2212\u221e\nare the Fourier series coe\ufb03cients of \u03c8 which are nonnegative and summable (see\nTheorem 13 for details).\nthat \u03b3k is not a metric on P. As discussed in Section 1.1.1, although some characterizations\nare available for k so that \u03b3k is a metric on P, they are di\ufb03cult to check in practice. So, in\nSection 3.1, we provide the characterization that if k is integrally strictly pd, then \u03b3k is a\nmetric on P. In Section 3.2, we present more easily checkable conditions wherein we show\nthat if supp(\u039b) = Rd (see footnote 5 for the de\ufb01nition of the support of a Borel measure),\nthen \u03b3k is a metric on P. This result is extended in a straightforward way to Td (d-Torus)\nin Section 3.3. The main results of this section are summarized in Table 1.\nWe start by de\ufb01ning characteristic kernels.\n15\nSriperumbudur, Gretton, Fukumizu, Sch\u00a8olkopf and Lanckriet\nDe\ufb01nition 6 (Characteristic kernel) A bounded measurable positive de\ufb01nite kernel k is\ncharacteristic to a set Q \u2282P of probability measures de\ufb01ned on (M, A) if for P, Q \u2208Q,\n\u03b3k(P, Q) = 0 \u21d4P = Q. k is simply said to be characteristic if it is characteristic to P.\nThe RKHS, H induced by such a k is called a characteristic RKHS.\nAs mentioned before, the injectivity of \u03a0 is related to the characteristic property of k.\nIf k is characteristic, then \u03b3k(P, Q) = \u2225\u03a0[P] \u2212\u03a0[Q]\u2225H = 0 \u21d2P = Q, which means\nP 7\u2192\nR\nM k(\u00b7, x) dP(x), i.e., \u03a0 is injective.\nTherefore, when M = Rd, the embedding of\na distribution to a characteristic RKHS can be seen as a generalization of the charac-\nteristic function, \u03c6P =\nR\nRd ei\u27e8\u00b7,x\u27e9dP(x).\nThis is because, by the uniqueness theorem for\ncharacteristic functions (Dudley, 2002, Theorem 9.5.1), \u03c6P = \u03c6Q \u21d2P = Q, which means\nP 7\u2192\nR\nRd ei\u27e8\u00b7,x\u27e9dP(x) is injective. So, in this context, intuitively ei\u27e8y,x\u27e9can be treated as the\ncharacteristic kernel, k, although, formally, this is not true as ei\u27e8y,x\u27e9is not a pd kernel.\nBefore we get to the characterization of characteristic kernels, the following examples\nshow that there exist bounded measurable kernels that are not characteristic.\nExample 1 (Trivial kernel) Let k(x, y) = \u03c8(x \u2212y) = C, \u2200x, y \u2208Rd with C > 0. Using\nthis in (13), we have \u03b32\nk(P, Q) = C + C \u22122C = 0 for any P, Q \u2208P, which means k is not\ncharacteristic.\nExample 2 (Dot product kernel) Let k(x, y) = xT y, x, y \u2208Rd. Using this in (13), we\nhave\n\u03b32\nk(P, Q) = \u00b5T\nP \u00b5P + \u00b5T\nQ\u00b5Q \u22122\u00b5T\nP \u00b5Q = \u2225\u00b5P \u2212\u00b5Q\u22252\n2,\nwhere \u00b5P and \u00b5Q represent the means associated with P and Q respectively, i.e., \u00b5P :=\nR\nRd x dP(x). It is clear that k is not characteristic as \u03b3k(P, Q) = 0 \u21d2\u00b5P = \u00b5Q \u21cfP = Q\nfor all P, Q \u2208P.\nExample 3 (Polynomial kernel of order 2) Let k(x, y) = (1+xT y)2, x, y \u2208Rd. Using\nthis in (14), we have\n\u03b32\nk(P, Q) =\nZZ\nRd(1 + 2xT y + xT yyT x) d(P \u2212Q)(x) d(P \u2212Q)(y)\n= 2\u2225\u00b5P \u2212\u00b5Q\u22252\n2 + \u2225\u03a3P \u2212\u03a3Q + \u00b5P\u00b5T\nP \u2212\u00b5Q\u00b5T\nQ\u22252\nF ,\nwhere \u03a3P and \u03a3Q represent the covariance matrices associated with P and Q respectively,\ni.e., \u03a3P :=\nR\nRd xxT dP(x) \u2212\u00b5P\u00b5T\nP . \u2225\u00b7 \u2225F represents the Frobenius norm. Since \u03b3k(P, Q) =\n0 \u21d2(\u00b5P = \u00b5Q and \u03a3P = \u03a3Q) \u21cfP = Q for all P, Q \u2208P, k is not characteristic.\nIn the following sections, we address the question of when k is characteristic, i.e., for what\nk is \u03b3k a metric on P?\n3.1 Integrally strictly positive de\ufb01nite kernels are characteristic\nCompared to the existing characterizations in literature (Gretton et al., 2007; Fukumizu et al.,\n2008, 2009a), the following result provides a more natural and easily understandable char-\nacterization for characteristic kernels, which shows that integrally strictly pd kernels are\ncharacteristic to P.\n16\nHilbert Space Embedding and Characteristic Kernels\nTheorem 7 (Integrally strictly pd kernels are characteristic) If k is integrally strictly\npositive de\ufb01nite on a topological space, M, then k is characteristic to P.\nBefore proving Theorem 7, we provide a supplementary result in Lemma 8 that provides\nnecessary and su\ufb03cient conditions for a kernel not to be characteristic.\nWe show that\nchoosing k to be integrally strictly pd violates the conditions in Lemma 8, and k is therefore\ncharacteristic to P.\nLemma 8 Let k be measurable and bounded on a topological space, M.\nThen \u2203P \u0338=\nQ, P, Q \u2208P such that \u03b3k(P, Q) = 0 if and only if there exists a \ufb01nite non-zero signed\nBorel measure \u00b5 that satis\ufb01es:\n(i)\nRR\nM k(x, y) d\u00b5(x) d\u00b5(y) = 0,\n(ii) \u00b5(M) = 0.\nProof\n( \u21d0) Suppose there exists a \ufb01nite non-zero signed Borel measure, \u00b5 that satis\ufb01es\n(i) and (ii) in Lemma 8. By the Jordan decomposition theorem (Dudley, 2002, Theorem\n5.6.1), there exist unique positive measures \u00b5+ and \u00b5\u2212such that \u00b5 = \u00b5+ \u2212\u00b5\u2212and \u00b5+ \u22a5\u00b5\u2212\n(\u00b5+ and \u00b5\u2212are singular). By (ii), we have \u00b5+(M) = \u00b5\u2212(M) =: \u03b1. De\ufb01ne P = \u03b1\u22121\u00b5+ and\nQ = \u03b1\u22121\u00b5\u2212. Clearly, P \u0338= Q, P, Q \u2208P. Then, by (14), we have\n\u03b32\nk(P, Q) =\nZZ\nM\nk(x, y) d(P \u2212Q)(x) d(P \u2212Q)(y) = \u03b1\u22122\nZZ\nM\nk(x, y) d\u00b5(x) d\u00b5(y)\n(a)\n= 0,\nwhere (a) is obtained by invoking (i). So, we have constructed P \u0338= Q such that \u03b3k(P, Q) = 0.\n( \u21d2) Suppose \u2203P \u0338= Q, P, Q \u2208P such that \u03b3k(P, Q) = 0. Let \u00b5 = P \u2212Q. Clearly \u00b5 is a\n\ufb01nite non-zero signed Borel measure that satis\ufb01es \u00b5(M) = 0. Note that by (14),\n\u03b32\nk(P, Q) =\nZZ\nM\nk(x, y) d(P \u2212Q)(x) d(P \u2212Q)(y) =\nZZ\nM\nk(x, y) d\u00b5(x) d\u00b5(y),\nand therefore (i) follows.\nProof (of Theorem 7) Since k is integrally strictly pd on M, we have\nZZ\nM\nk(x, y) d\u03b7(x)d\u03b7(y) > 0,\nfor any \ufb01nite non-zero signed Borel measure \u03b7. This means there does not exist a \ufb01nite\nnon-zero signed Borel measure that satis\ufb01es (i) in Lemma 8. Therefore, by Lemma 8, there\ndoes not exist P \u0338= Q, P, Q \u2208P such that \u03b3k(P, Q) = 0, which implies k is characteristic.\nExamples of integrally strictly pd kernels on Rd include the Gaussian, exp(\u2212\u03c3\u2225x\u2212y\u22252\n2), \u03c3 >\n0; the Laplacian, exp(\u2212\u03c3\u2225x \u2212y\u22251), \u03c3 > 0; inverse multiquadratics, (\u03c32 + \u2225x \u2212y\u22252\n2)\u2212c, c >\n0, \u03c3 > 0, etc, which are translation invariant kernels on Rd. A translation variant integrally\nstrictly pd kernel, ek, can be obtained from a translation invariant integrally strictly pd\nkernel, k, as ek(x, y) = f(x)k(x, y)f(y), where f : M \u2192R is a bounded continuous function.\nA simple example of a translation variant integrally strictly pd kernel on Rd is ek(x, y) =\nexp(\u03c3xT y), \u03c3 > 0, where we have chosen f(.) = exp(\u03c3\u2225.\u22252\n2/2) and k(x, y) = exp(\u2212\u03c3\u2225x \u2212\n17\nSriperumbudur, Gretton, Fukumizu, Sch\u00a8olkopf and Lanckriet\ny\u22252\n2/2), \u03c3 > 0. Clearly, this kernel is characteristic on compact subsets of Rd. The same\nresult can also be obtained from the fact that ek is universal on compact subsets of Rd\n(Steinwart, 2001, Section 3, Example 1).\nAlthough the condition for characteristic k in Theorem 7 is easy to understand com-\npared to other characterizations in literature, it is not always easy to check for integral\nstrict positive de\ufb01niteness of k. In the following section, we assume M = Rd and k to be\ntranslation invariant and present a complete characterization for characteristic k which is\nsimple to check.\n3.2 Characterization for translation invariant k on Rd\nThe complete, detailed proofs of the main results in this section are provided in Section 3.5.\nCompared to Sriperumbudur et al. (2008), we now present simple proofs for these results\nwithout resorting to distribution theory. Let us start with the following assumption.\nAssumption 1 k(x, y) = \u03c8(x \u2212y) where \u03c8 is a bounded continuous real-valued positive\nde\ufb01nite function on M = Rd.\nThe following theorem characterizes all translation invariant kernels in Rd that are charac-\nteristic.\nTheorem 9 Suppose k satis\ufb01es Assumption 1.\nThen k is characteristic if and only if\nsupp(\u039b) = Rd, where \u039b is de\ufb01ned as in (15).5\nFirst, note that the condition supp(\u039b) = Rd is easy to check compared to all other, afore-\nmentioned characterizations for characteristic k.\nTable 2 shows some popular transla-\ntion invariant kernels on R along with their Fourier spectra, b\u03c8 and its support: Gaus-\nsian, Laplacian, B2n+1-spline6 (Sch\u00a8olkopf and Smola, 2002) and Sinc kernels are aperiodic\nwhile Poisson (Br\u00b4emaud, 2001; Steinwart, 2001; Vapnik, 1998), Dirichlet (Br\u00b4emaud, 2001;\nSch\u00a8olkopf and Smola, 2002), F\u00b4ejer (Br\u00b4emaud, 2001) and cosine kernels are periodic. Al-\nthough the Gaussian and Laplacian kernels are shown to be characteristic by all the char-\nacterizations we have mentioned so far, the case of B2n+1-splines is addressed only by\nTheorem 9, which shows them to be characteristic (note that B2n+1-splines being integrally\nstrictly pd also follow from Theorem 9). In fact, one can provide a more general result on\ncompactly supported translation invariant kernels, which we do later in Corollary 10. The\nMat\u00b4ern class of kernels (Rasmussen and Williams, 2006, Section 4.2.1), given by\nk(x, y) = \u03c8(x \u2212y) = 21\u2212\u03bd\n\u0393(\u03bd)\n \u221a\n2\u03bd\u2225x \u2212y\u22252\n\u03c3\n!\u03bd\nK\u03bd\n \u221a\n2\u03bd\u2225x \u2212y\u22252\n\u03c3\n!\n, \u03bd > 0, \u03c3 > 0,\n(22)\n5. For a \ufb01nite regular measure \u00b5, there is a largest open set U with \u00b5(U) = 0. The complement of U is\ncalled the support of \u00b5, denoted by supp(\u00b5).\n6. A B2n+1-spline is a Bn-spline of odd order. Only B2n+1-splines are admissible, i.e. Bn splines of odd\norder are positive de\ufb01nite kernels whereas the ones of even order have negative components in their\nFourier spectrum, b\u03c8 and, therefore, are not admissible kernels. In Table 2, the symbol \u2217(2n+2)\n1\nrepresents\nthe (2n + 2)-fold convolution. An important point to be noted with the B2n+1-spline kernel is that its\nFourier spectrum, b\u03c8 has vanishing points at \u03c9 = 2\u03c0\u03b1, \u03b1 \u2208Z\\{0} unlike Gaussian and Laplacian kernels\nwhich do not have any vanishing points in their Fourier spectrum. Nevertheless, the spectrum of all\nthese kernels has support R.\n18\nHilbert Space Embedding and Characteristic Kernels\nKernel\n\u03c8(x)\nb\u03c8(\u03c9)\nsupp( b\u03c8)\nGaussian\nexp\n\u0010\n\u2212x2\n2\u03c32\n\u0011\n\u03c3 exp\n\u0010\n\u2212\u03c32\u03c92\n2\n\u0011\nR\nLaplacian\nexp(\u2212\u03c3|x|)\nq\n2\n\u03c0\n\u03c3\n\u03c32+\u03c92\nR\nB2n+1-spline\n\u2217(2n+2)\n1\n1[\u22121\n2 , 1\n2](x)\n4n+1\n\u221a\n2\u03c0\nsin2n+2( \u03c9\n2 )\n\u03c92n+2\nR\nSinc\nsin(\u03c3x)\nx\np \u03c0\n2 1[\u2212\u03c3,\u03c3](\u03c9)\n[\u2212\u03c3, \u03c3]\nPoisson\n1\u2212\u03c32\n\u03c32\u22122\u03c3 cos(x)+1, 0 < \u03c3 < 1\n\u221a\n2\u03c0 P\u221e\nj=\u2212\u221e\u03c3|j| \u03b4(\u03c9 \u2212j)\nZ\nDirichlet\nsin (2n+1)x\n2\nsin x\n2\n\u221a\n2\u03c0 Pn\nj=\u2212n \u03b4(\u03c9 \u2212j)\n{0, \u00b11, . . ., \u00b1n}\nF\u00b4ejer\n1\nn+1\nsin2 (n+1)x\n2\nsin2 x\n2\n\u221a\n2\u03c0 Pn\nj=\u2212n\n\u0010\n1 \u2212\n|j|\nn+1\n\u0011\n\u03b4(\u03c9 \u2212j)\n{0, \u00b11, . . ., \u00b1n}\nCosine\ncos(\u03c3x)\np \u03c0\n2 [\u03b4(\u03c9 \u2212\u03c3) + \u03b4(\u03c9 + \u03c3)]\n{\u2212\u03c3, \u03c3}\nTable 2: Translation invariant kernels on R de\ufb01ned by \u03c8, their spectra, b\u03c8 and its support,\nsupp( b\u03c8). The \ufb01rst four are aperiodic kernels while the last four are periodic. The\ndomain is considered to be R for simplicity.\nFor x \u2208Rd, the above formulae\ncan be extended by computing \u03c8(x) = Qd\nj=1 \u03c8(xj) where x = (x1, . . . , xd) and\nb\u03c8(\u03c9) = Qd\nj=1 b\u03c8(\u03c9j) where \u03c9 = (\u03c91, . . . , \u03c9d). \u03b4 represents the Dirac-delta function.\nis characteristic as the Fourier spectrum of \u03c8, given by\nb\u03c8(\u03c9) = 2d+\u03bd\u03c0d/2\u0393(\u03bd + d/2)\u03bd\u03bd\n\u0393(\u03bd)\u03c32\u03bd\n\u00122\u03bd\n\u03c32 + 4\u03c02\u2225\u03c9\u22252\n2\n\u0013\u2212(\u03bd+d/2)\n, \u03c9 \u2208Rd,\n(23)\nis positive for any \u03c9 \u2208Rd. Here, \u0393 is the Gamma function, K\u03bd is the modi\ufb01ed Bessel\nfunction of the second kind of order \u03bd, where \u03bd controls the smoothness of k. The case\nof \u03bd = 1\n2 in the Mat\u00b4ern class gives the exponential kernel, k(x, y) = exp(\u2212\u2225x \u2212y\u22252/\u03c3),\nwhile \u03bd \u2192\u221egives the Gaussian kernel. Note that b\u03c8(x \u2212y) in (23) is actually the inverse\nmultiquadratic kernel, which is characteristic both by Theorem 7 and Theorem 9.\nBy Theorem 9, the Sinc kernel in Table 2 is not characteristic, which is not easy to show\nusing other characterizations. By combining Theorem 7 with Theorem 9, it can be shown\nthat the Sinc, Poisson, Dirichlet, F\u00b4ejer and cosine kernels are not integrally strictly pd.\nTherefore, for translation invariant kernels on Rd, the integral strict positive de\ufb01niteness of\nthe kernel (or the lack of it) can be tested using Theorems 7 and 9.\nWe note that, of all the kernels shown in Table 2, only the Gaussian, Laplacian and\nB2n+1-spline kernels are integrable and their corresponding b\u03c8 are computed using (5). The\n19\nSriperumbudur, Gretton, Fukumizu, Sch\u00a8olkopf and Lanckriet\nother kernels shown in Table 2 are not integrable and their corresponding b\u03c8 have to be\ntreated as distributions (see Folland (1999, Chapter 9) and Rudin (1991, Chapter 6) for\ndetails), except for the Sinc kernel whose Fourier transform can be computed in the L2\nsense.7\nProof (Theorem 9) We provide an outline of the complete proof, which is presented in\nSection 3.5. The su\ufb03cient condition in Theorem 9 is simple to prove and follows from Corol-\nlary 4(i), whereas we need a supplementary result to prove its necessity, which is presented\nin Lemma 16 (see Section 3.5). Proving the necessity of Theorem 9 is equivalent to showing\nthat if supp(\u039b) \u228aRd, then \u2203P \u0338= Q, P, Q \u2208P such that \u03b3k(P, Q) = 0. In Lemma 16,\nwe present equivalent conditions for the existence of P \u0338= Q such that \u03b3k(P, Q) = 0 if\nsupp(\u039b) \u228aRd, using which we prove the necessity of Theorem 9.\nThe whole family of compactly supported translation invariant continuous bounded\nkernels on Rd is characteristic, as shown by the following corollary to Theorem 9.\nCorollary 10 Suppose k \u0338= 0 satis\ufb01es Assumption 1 and supp(\u03c8) is compact. Then k is\ncharacteristic.\nProof\nSince supp(\u03c8) is compact in Rd, by the Paley-Wiener theorem (Theorem 29 in\nAppendix A) and Lemma 30 (see Appendix A), we deduce that supp(\u039b) = Rd. Therefore,\nthe result follows from Theorem 9.\nThe above result is interesting in practice because of the computational advantage in dealing\nwith compactly supported kernels. Note that proving such a general result for compactly\nsupported kernels on Rd is not straightforward (maybe not even possible) with the other\ncharacterizations.\nAs a corollary to Theorem 9, the following result provides a method to construct new\ncharacteristic kernels from a given one.\nCorollary 11 Let k, k1 and k2 satisfy Assumption 1.\nSuppose k is characteristic and\nk2 \u0338= 0. Then k + k1 and k \u00b7 k2 are characteristic.\nProof Since k, k1 and k2 satisfy Assumption 1, k +k1 and k2 \u00b7k also satisfy Assumption 1.\nIn addition,\n(k + k1)(x, y) := k(x, y) + k1(x, y) = \u03c8(x \u2212y) + \u03c81(x \u2212y) =\nZ\nRd e\u2212i(x\u2212y)T \u03c9 d(\u039b + \u039b1)(\u03c9),\n(k \u00b7 k2)(x, y) := k(x, y)k2(x, y) = \u03c8(x \u2212y)\u03c82(x \u2212y) =\nZZ\nRd e\u2212i(x\u2212y)T (\u03c9+\u03be) d\u039b(\u03c9) d\u039b2(\u03be)\n(a)\n=:\nZ\nRd e\u2212i(x\u2212y)T \u03c9 d(\u039b \u2217\u039b2)(\u03c9),\nwhere (a) follows from the de\ufb01nition of convolution of measures (see Rudin (1991, Section\n9.14) for details). Since k is characteristic, i.e., supp(\u039b) = Rd, and supp(\u039b) \u2282supp(\u039b+\u039b1),\n7. If f \u2208L2(Rd), the Fourier transform \u0325[f] := bf of f is de\ufb01ned to be the limit, in the L2-norm, of the\nsequence { bfn} of Fourier transforms of any sequence {fn} of functions belonging to Sd, such that fn\nconverges in the L2-norm to the given function f \u2208L2(Rd), as n \u2192\u221e. The function bf is de\ufb01ned almost\neverywhere on Rd and belongs to L2(Rd). Thus, \u0325 is a linear operator, mapping L2(Rd) into L2(Rd).\nSee Gasquet and Witomski (1999, Chapter IV, Lesson 22) for details.\n20\nHilbert Space Embedding and Characteristic Kernels\nwe have supp(\u039b+\u039b1) = Rd and therefore k+k1 is characteristic. Similarly, since supp(\u039b) \u2282\nsupp(\u039b \u2217\u039b2), we have supp(\u039b \u2217\u039b2) = Rd and therefore, k \u00b7 k2 is characteristic.\nNote that in the above result, we do not need k1 or k2 to be characteristic. Therefore,\none can generate all sorts of kernels that are characteristic by starting with a characteristic\nkernel, k.\nSo far, we have considered characterizations for k such that it is characteristic to P.\nWe showed in Theorem 9 that kernels with supp(\u039b) \u228aRd are not characteristic to P.\nNow, we can question whether such kernels can be characteristic to some proper subset\nQ of P.\nThe following result addresses this.\nNote that these kernels, i.e., the kernels\nwith supp(\u039b) \u228aRd are usually not useful in practice, especially in statistical inference\napplications, because the conditions on Q are usually not easy to check.\nOn the other\nhand, the following result is of theoretical interest: along with Theorem 9, it completes the\ncharacterization of characteristic kernels that are translation invariant on Rd. Before we\nstate the result, we denote P \u226aQ to mean that P is absolutely continuous w.r.t. Q.\nTheorem 12 Let P1 := {P \u2208P : \u03c6P \u2208L1(Rd)\u222aL2(Rd), P \u226a\u03bb and supp(P) is compact},\nwhere \u03bb is the Lebesgue measure. Suppose k satis\ufb01es Assumption 1 and supp(\u039b) \u228aRd has\na non-empty interior, where \u039b is de\ufb01ned as in (15). Then k is characteristic to P1.\nProof See Section 3.5.\nAlthough, by Theorem 9, the kernels with supp(\u039b) \u228aRd are not characteristic to P,\nTheorem 12 shows that there exists a subset of P to which a subset of these kernels are\ncharacteristic. This type of result is not available for the previously mentioned character-\nizations. An example of a kernel that satis\ufb01es the conditions in Theorem 12 is the Sinc\nkernel, \u03c8(x) = sin(\u03c3x)\nx\nwhich has supp(\u039b) = [\u2212\u03c3, \u03c3]. The condition that supp(\u039b) \u228aRd has a\nnon-empty interior is important for Theorem 12 to hold. If supp(\u039b) has an empty interior\n(examples include periodic kernels), then one can construct P \u0338= Q, P, Q \u2208P1 such that\n\u03b3k(P, Q) = 0. This is illustrated in Example 5, which is deferred to Section 3.5.\nSo far, we have characterized the characteristic property of kernels that satisfy (a)\nsupp(\u039b) = Rd or (b) supp(\u039b) \u228aRd with int(supp(\u039b)) \u0338= \u2205. In the following section, we\ninvestigate kernels that have supp(\u039b) \u228aRd with int(supp(\u039b)) = \u2205, examples of which\ninclude periodic kernels on Rd. This discussion uses the fact that a periodic function on Rd\ncan be treated as a function on Td, the d-Torus.\n3.3 Characterization for translation invariant k on Td\nLet M = \u00d7d\nj=1[0, \u03c4j) and \u03c4 := (\u03c41, . . . , \u03c4d). A function de\ufb01ned on M with periodic boundary\nconditions is equivalent to considering a periodic function on Rd with period \u03c4. With no\nloss of generality, we can choose \u03c4j = 2\u03c0, \u2200j which yields M = [0, 2\u03c0)d =: Td, called the\nd-Torus. The results presented here hold for any 0 < \u03c4j < \u221e, \u2200j but we choose \u03c4j = 2\u03c0\nfor simplicity. Similar to Assumption 1, we now make the following assumption.\nAssumption 2 k(x, y) = \u03c8((x \u2212y)mod 2\u03c0), where \u03c8 is a continuous real-valued positive\nde\ufb01nite function on M = Td.\nSimilar to Theorem 3, we now state Bochner\u2019s theorem on M = Td.\n21\nSriperumbudur, Gretton, Fukumizu, Sch\u00a8olkopf and Lanckriet\nTheorem 13 (Bochner) A continuous function \u03c8 : Td \u2192R is positive de\ufb01nite if and\nonly if\n\u03c8(x) =\nX\nn\u2208Zd\nA\u03c8(n) eixT n, x \u2208Td,\n(24)\nwhere A\u03c8 : Zd \u2192R+, A\u03c8(\u2212n) = A\u03c8(n) and P\nn\u2208Zd A\u03c8(n) < \u221e. A\u03c8 are called the Fourier\nseries coe\ufb03cients of \u03c8.\nExamples for \u03c8 include the Poisson, Dirichlet, F\u00b4ejer and cosine kernels, which are shown in\nTable 2. We now state the result that de\ufb01nes characteristic kernels on Td.\nTheorem 14 Suppose k satis\ufb01es Assumption 2. Then k is characteristic (to the set of all\nBorel probability measures on Td) if and only if A\u03c8(0) \u22650, A\u03c8(n) > 0, \u2200n \u0338= 0.\nThe proof is provided in Section 3.5 and the idea is similar to that of Theorem 9. Based on\nthe above result, one can generate characteristic kernels by constructing an in\ufb01nite sequence\nof positive numbers that are summable and then using them in (24). It can be seen from\nTable 2 that the Poisson kernel on T is characteristic while the Dirichlet, F\u00b4ejer and cosine\nkernels are not. Some examples of characteristic kernels on T are:\n(1) k(x, y) = e\u03b1 cos(x\u2212y) cos(\u03b1 sin(x \u2212y)), 0 < \u03b1 \u22641 \u2194A\u03c8(0) = 1, A\u03c8(n) = \u03b1|n|\n2|n|!, \u2200n \u0338= 0.\n(2) k(x, y) = \u2212log(1 \u22122\u03b1 cos(x \u2212y) + \u03b12), |\u03b1| < 1 \u2194A\u03c8(0) = 0, A\u03c8(n) = \u03b1n\nn , \u2200n \u0338= 0.\n(3) k(x, y) = (\u03c0 \u2212(x \u2212y)mod 2\u03c0)2 \u2194A\u03c8(0) = \u03c02\n3 , A\u03c8(n) =\n2\nn2, \u2200n \u0338= 0.\n(4) k(x, y) =\nsinh \u03b1\ncosh \u03b1\u2212cos(x\u2212y), \u03b1 > 0 \u2194A\u03c8(0) = 1, A\u03c8(n) = e\u2212\u03b1|n|, \u2200n \u0338= 0.\n(5) k(x, y) = \u03c0 cosh(\u03b1(\u03c0\u2212(x\u2212y)mod 2\u03c0))\n\u03b1 sinh(\u03c0\u03b1)\n\u2194A\u03c8(0) =\n1\n\u03b12 , A\u03c8(n) =\n1\nn2+\u03b12 , \u2200n \u0338= 0.\nThe following result relates characteristic kernels and universal kernels de\ufb01ned on Td.\nCorollary 15 Let k be a characteristic kernel satisfying Assumption 2 with A\u03c8(0) > 0.\nThen k is also universal.\nProof\nSince k is characteristic with A\u03c8(0) > 0, we have A\u03c8(n) > 0, \u2200n. Therefore, by\nCorollary 11 of Steinwart (2001), k is universal.\nSince k being universal implies that it is characteristic, the above result shows that the\nconverse is not true (though almost true except that A\u03c8(0) can be zero for characteristic\nkernels). The condition on A\u03c8 in Theorem 14, i.e., A\u03c8(0) \u22650, A\u03c8(n) > 0, \u2200n \u0338= 0 can be\nequivalently written as supp(A\u03c8) = Zd. Therefore, Theorems 9 and 14 are of similar \ufb02avor.\nIn fact, these results can be generalized to locally compact Abelian groups. Fukumizu et al.\n(2009b) shows that a bounded continuous translation invariant kernel on a locally compact\nAbelian group, G is characteristic to the set of all probability measures on G if and only if\nthe support of the Fourier transform of the translation invariant kernel is the dual group of\nG. In our case, (Rd, +) and (Td, +) are locally compact Abelian groups with (Rd, +) and\n(Zd, +) as their respective dual groups. In Fukumizu et al. (2009b), these results are also\nextended to translation invariant kernels on non-Abelian compact groups and the semigroup\nRd\n+.\n22\nHilbert Space Embedding and Characteristic Kernels\nFigure 1: Summary of the relationship between various characterizations is shown along\nwith the reference. The letters \u201cC\u201d, \u201cF\u201d, and \u201cT\u201d refer to Corollary, Footnote and\nTheorem respectively. For example, T. 7 refers to Theorem 7. The implications\nwhich are open problems are shown with \u201c?\u201d. A D B indicates that A is a dense\nsubset of B. Refer to Section 3.4 for details.\n3.4 Relation between various characterizations of characteristic kernels\nSo far, we have presented various characterizations of characteristic kernels, which are easily\ncheckable compared to the characterizations proposed in literature (Gretton et al., 2007;\nFukumizu et al., 2008, 2009b). Now, it is of interest to understand the relation between\nthese characterizations. A summary of the relationship between these characterizations is\nshown in Figure 1, which is discussed below.\nCharacteristic kernels vs. Integrally strictly pd kernels: It is clear from Theorem 7\nthat integrally strictly pd kernels on a topological space, M are characteristic, while it is\nnot clear whether the converse is true or not. However, when k is translation invariant\non Rd, then the converse holds. This is because if k is characteristic, then by Theorem 9,\nsupp(\u039b) = Rd, where \u039b is de\ufb01ned as in (15). It is easy to check that if supp(\u039b) = Rd, then\nk is integrally strictly pd.\nIntegrally strictly pd kernels vs.\nStrictly pd kernels: The relation between in-\ntegrally strictly pd and strictly pd kernels shown in Figure 1 is straightforward, as one\ndirection follows from footnote 4, while the other direction is not true, which follows from\nSteinwart and Christmann (2008, Proposition 4.60, Theorem 4.62).\nHowever, if M is a\n\ufb01nite set, then k being strictly pd also implies it is integrally strictly pd.\nCharacteristic kernels vs. Strictly pd kernels: Since integrally strictly pd kernels are\ncharacteristic and are also strictly pd, a natural question to ask is, \u201cWhat is the relation\nbetween characteristic and strictly pd kernels?\u201d It can be seen that strictly pd kernels need\nnot be characteristic because the sinc-squared kernel, k(x, y) = sin2(\u03c3(x\u2212y))\n(x\u2212y)2\non R, which\nhas supp(\u039b) = [\u2212\u03c3, \u03c3] \u228aR is strictly pd (Wendland, 2005, Theorem 6.11), while it is not\ncharacteristic by Theorem 9. However, for any general M, it is not clear whether k being\ncharacteristic implies that it is strictly pd. As a special case, if M = Rd or M = Td, then\nby Theorems 9 and 12, it follows that a translation invariant k being characteristic also\nimplies that it is strictly pd.\n23\nSriperumbudur, Gretton, Fukumizu, Sch\u00a8olkopf and Lanckriet\nUniversal kernels vs. Characteristic kernels: Gretton et al. (2007) have shown that\nif k is universal in the sense of Steinwart (2001), then it is characteristic. As mentioned in\nSection 3.3, the converse is not true, i.e., if a kernel is characteristic, then it need not be\nuniversal, which follows from Corollary 15. Note that in this case, M is assumed to be a\ncompact metric space. The notion of universality of kernels was extended to non-compact\ndomains by Micchelli et al. (2006): k is said to universal on a non-compact Hausdro\ufb00space,\nM, if for any compact Z \u2282M, the set K(Z) := span{k(\u00b7, y) : y \u2208Z} is dense in C(Z) w.r.t.\nthe supremum norm, where C(Z) represents the space of continuous functions de\ufb01ned on\nZ. It is to be noted that when M is compact, this notion of universality is same as that of\nSteinwart (2001). Micchelli et al. (2006, Proposition 15) have provided a characterization of\nuniversality for translation invariant kernels on Rd: k is universal if \u03bb(supp(\u039b)) > 0, where\n\u03bb is the Lebesgue measure and \u039b is de\ufb01ned as in (15). This means, if a translation invariant\nkernel on Rd is characteristic, then it is also universal in the sense of Micchelli et al. (2006),\nwhile the converse is not true. However, the relation between these notions for a general\nnon-compact Hausdor\ufb00space, M is not clear.\nFukumizu et al. (2008, 2009b) have shown that k is characteristic if and only if H+R is\ndense in Lr(M, P) for all P \u2208P and for some r \u2208[1, \u221e). This means, if k is characteristic,\nthen H+R D Lr(M, P), which implies H D Lr(M, P) for all P \u2208P and for some r \u2208[1, \u221e).\nClearly, the converse is not true (refer to Figure 1 for the de\ufb01nition of D). However, if\nconstant functions are included in H, then it is easy to see that the converse is also true.\nUniversal kernels vs. Strictly pd kernels: If a kernel is universal, then it is strictly\npd, which follows from Steinwart and Christmann (2008, De\ufb01nition 4.53, Proposition 4.54,\nExercise 4.11). On the other hand, if a kernel is strictly pd, then it need not be univer-\nsal, which follows from the results due to Dahmen and Micchelli (1987) and Pinkus (2004)\nfor Taylor kernels (Steinwart and Christmann, 2008, Lemma 4.8, Corollary 4.57). Refer to\nSteinwart and Christmann (2008, Section 4.7, p. 161) for more details.\nRecently, in Sriperumbudur et al. (2010a,b), we carried out a thorough study of relating\ncharacteristic kernels to various notions of universality, wherein we addressed some open\nquestions mentioned in the above discussion and Figure 1. This is done by relating uni-\nversality to the injective embedding of regular Borel measures into an RKHS, which can\ntherefore be seen as a generalization of the notion of characteristic kernels, as the latter\ndeal with the injective RKHS embedding of probability measures.\n3.5 Proofs\nFirst, we present a supplementary result in Lemma 16 that will be used to prove Theorem 9.\nThe idea of Lemma 16 is to characterize the equivalent conditions for the existence of\nP \u0338= Q such that \u03b3k(P, Q) = 0 when supp(\u039b) \u228aRd. Its proof relies on the properties of\ncharacteristic functions, which we have collected in Theorem 27 in Appendix A.\nLemma 16 Let P0 := {P \u2208P : \u03c6P \u2208L1(Rd) \u222aL2(Rd) and P \u226a\u03bb}, where \u03bb is the\nLebesgue measure. Suppose k satis\ufb01es Assumption 1 and supp(\u039b) \u228aRd, where \u039b is de\ufb01ned\nas in (15). Then, for any Q \u2208P0, \u2203P \u0338= Q, P \u2208P0 such that \u03b3k(P, Q) = 0 if and only if\nthere exists a non-zero function \u03b8 : Rd \u2192C that satis\ufb01es the following conditions:\n24\nHilbert Space Embedding and Characteristic Kernels\n(i) \u03b8 \u2208(L1(Rd)\u222aL2(Rd))\u2229Cb(Rd) is conjugate symmetric8, i.e., \u03b8(x) = \u03b8(\u2212x), \u2200x \u2208Rd,\n(ii) \u03b8\u2228\u2208L1(Rd) \u2229(L2(Rd) \u222aCb(Rd)),\n(iii)\nR\nRd |\u03b8(x)|2 d\u039b(x) = 0,\n(iv) \u03b8(0) = 0,\n(v) infx\u2208Rd{\u03b8\u2228(x) + q(x)} \u22650.\nProof De\ufb01ne L1 := L1(Rd), L2 := L2(Rd) and Cb := Cb(Rd).\n( \u21d0) Suppose there exists a non-zero function \u03b8 satisfying (i) \u2013 (v). For any Q \u2208P0, we\nhave \u03c6Q \u2208(L1 \u222aL2) \u2229Cb. When \u03c6Q \u2208L1 \u2229Cb, the Riemann-Lebesgue lemma (Lemma 28\nin Appendix A) implies that q = [\u03c6Q]\u2228\u2208L1 \u2229Cb, where q is the Radon-Nikodym derivative\nof Q w.r.t. \u03bb. When \u03c6Q \u2208L2 \u2229Cb, the Fourier transform in the L2 sense (see footnote 7)\nimplies that q = [\u03c6Q]\u2228\u2208L1 \u2229L2. Therefore, q \u2208L1 \u2229(L2 \u222aCb). De\ufb01ne p := q +\u03b8\u2228. Clearly\np \u2208L1 \u2229(L2 \u222aCb). In addition, \u03c6P = bp = bq + c\n\u03b8\u2228= \u03c6Q + \u03b8 \u2208(L1 \u222aL2) \u2229Cb. Since \u03b8 is\nconjugate symmetric, \u03b8\u2228is real valued and so is p. Consider\nZ\nRd p(x) dx =\nZ\nRd q(x) dx +\nZ\nRd \u03b8\u2228(x) dx = 1 + \u03b8(0) = 1.\n(v) implies that p is non-negative.\nTherefore, p is the Radon-Nikodym derivative of a\nprobability measure P w.r.t. \u03bb, where P is such that P \u0338= Q and P \u2208P0. By (16), we have\n\u03b32\nk(P, Q) =\nZ\nRd |\u03c6P(x) \u2212\u03c6Q(x)|2 d\u039b(x) =\nZ\nRd |\u03b8(x)|2 d\u039b(x) = 0.\n( \u21d2) Suppose that there exists P \u0338= Q, P, Q \u2208P0 such that \u03b3k(P, Q) = 0. De\ufb01ne \u03b8 := \u03c6P \u2212\n\u03c6Q. We need to show that \u03b8 satis\ufb01es (i) \u2013 (v). P, Q \u2208P0 implies \u03c6P, \u03c6Q \u2208(L1\u222aL2)\u2229Cb and\np, q \u2208L1\u2229(L2\u222aCb). Therefore, \u03b8 = \u03c6P\u2212\u03c6Q \u2208(L1\u222aL2)\u2229Cb and \u03b8\u2228= p\u2212q \u2208L1\u2229(L2\u222aCb).\nBy Theorem 27 (see Appendix A), \u03c6P and \u03c6Q are conjugate symmetric and so is \u03b8. Therefore\n\u03b8 satis\ufb01es (i) and \u03b8\u2228satis\ufb01es (ii). \u03b8 satis\ufb01es (iv) as\n\u03b8(0) =\nZ\nRd \u03b8\u2228(x) dx =\nZ\nRd(p(x) \u2212q(x)) dx = 0.\nNon-negativity of p yields (v). By (16), \u03b3k(P, Q) = 0 implies (iii).\nRemark 17 Note that the dependence of \u03b8 on the kernel appears in the form of (iii) in\nLemma 16. This condition shows that \u03bb(supp(\u03b8) \u2229supp(\u039b)) = 0, i.e., the supports of \u03b8 and\n\u039b are disjoint w.r.t. the Lebesgue measure, \u03bb. In other words, supp(\u03b8) \u2282cl(Rd\\supp(\u039b)).\nSo, the idea is to introduce the perturbation, \u03b8 over an open set, U where \u039b(U) = 0. The\nremaining conditions characterize the nature of this perturbation so that the constructed\nmeasure, p = q + \u03b8\u2228, is a valid probability measure. Conditions (i), (ii) and (iv) simply\nfollow from \u03b8 = \u03c6P \u2212\u03c6Q, while (v) ensures that p(x) \u22650, \u2200x.\n8. Note that Re[\u03b8] and Im[\u03b8] are even and odd functions in Rd.\n25\nSriperumbudur, Gretton, Fukumizu, Sch\u00a8olkopf and Lanckriet\nUsing Lemma 16, we now present the proof of Theorem 9.\nProof (Theorem 9) The su\ufb03ciency follows from (16): if supp(\u039b) = Rd, then \u03b32\nk(P, Q) =\nR\nRd |\u03c6P(x) \u2212\u03c6Q(x)|2 d\u039b(x) = 0 \u21d2\u03c6P = \u03c6Q, a.e., which implies P = Q and therefore k is\ncharacteristic. To prove necessity, we need to show that if supp(\u039b) \u228aRd, then there exists\nP \u0338= Q, P, Q \u2208P such that \u03b3k(P, Q) = 0. By Lemma 16, this is equivalent to showing\nthat there exists a non-zero \u03b8 satisfying the conditions in Lemma 16. Below, we provide a\nconstructive procedure for such a \u03b8 when supp(\u039b) \u228aRd, thereby proving the result.\nConsider the following function, f\u03b2,\u03c90 \u2208C\u221e(Rd) supported in [\u03c90 \u2212\u03b2, \u03c90 + \u03b2],\nf\u03b2,\u03c90(\u03c9) =\nd\nY\nj=1\nh\u03b2j,\u03c90,j(\u03c9j) with ha,b(y) := 1[\u2212a,a](y \u2212b) e\n\u2212\na2\na2\u2212(y\u2212b)2 ,\n(25)\nwhere \u03c9 = (\u03c91, . . . , \u03c9d), \u03c90 = (\u03c90,1, . . . , \u03c90,d), \u03b2 = (\u03b21, . . . , \u03b2d), a \u2208R++, b \u2208R and y \u2208R.\nSince supp(\u039b) \u228aRd, there exists an open set U \u2282Rd such that \u039b(U) = 0. So, there exists\n\u03b2 \u2208Rd\n++ and \u03c90 > \u03b2 (element-wise inequality) such that [\u03c90 \u2212\u03b2, \u03c90 + \u03b2] \u2282U. Let\n\u03b8 = \u03b1(f\u03b2,\u03c90 + f\u03b2,\u2212\u03c90), \u03b1 \u2208R\\{0},\n(26)\nwhich implies supp(\u03b8) = [\u2212\u03c90\u2212\u03b2, \u2212\u03c90+\u03b2]\u222a[\u03c90\u2212\u03b2, \u03c90+\u03b2] is compact. Clearly \u03b8 \u2208Dd \u2282Sd\nwhich implies \u03b8\u2228\u2208Sd \u2282L1(Rd) \u2229L2(Rd). Therefore, by construction, \u03b8 satis\ufb01es (i) \u2013 (iv)\nin Lemma 16. Since\nR\nRd \u03b8\u2228(x) dx = \u03b8(0) = 0 (by construction), \u03b8\u2228will take negative values,\nso we need to show that there exists Q \u2208P0 such that (v) in Lemma 16 holds. Let Q be\nsuch that it has a density given by\nq(x) = Cl\nd\nY\nj=1\n1\n(1 + |xj|2)l , l \u2208N where Cl =\nd\nY\nj=1\n\u0012Z\nR\n(1 + |xj|2)\u2212l dxj\n\u0013\u22121\n,\n(27)\nand x = (x1, . . . , xd). It can be veri\ufb01ed that choosing \u03b1 such that\n0 < |\u03b1| \u2264\nCl\n2 supx\n\f\f\f\nQd\nj=1 h\u2228\n\u03b2j,0(xj)(1 + |xj|2)l cos(\u03c9T\n0 x)\n\f\f\f\n< \u221e,\nensures that \u03b8 satis\ufb01es (v) in Lemma 16. The existence of \ufb01nite \u03b1 is guaranteed as ha,0 \u2208\nD1 \u2282S1 which implies h\u2228\na,0 \u2208S1, \u2200a. We conclude there exists a non-zero \u03b8 as claimed\nearlier, which completes the proof.\nTo elucidate the necessity part in the above proof, in the following, we present a simple\nexample that provides an intuitive understanding about the construction of \u03b8 such that for\na given Q, P \u0338= Q can be constructed with \u03b3k(P, Q) = 0.\nExample 4 Let Q be a Cauchy distribution in R, i.e., q(x) =\n1\n\u03c0(1+x2) with characteristic\nfunction, \u03c6Q(\u03c9) =\n1\n\u221a\n2\u03c0e\u2212|\u03c9| in L1(R). Let \u03c8 be a Sinc kernel, i.e., \u03c8(x) =\nq\n2\n\u03c0\nsin(\u03b2x)\nx\nwith\nFourier transform given by b\u03c8(\u03c9) = 1[\u2212\u03b2,\u03b2](\u03c9) and supp( b\u03c8) = [\u2212\u03b2, \u03b2] \u228aR. Let \u03b8 be\n\u03b8(\u03c9) = \u03b1\n2i\nh\n\u2217N\n1 1[\u2212\u03b2\n2 , \u03b2\n2](\u03c9)\ni\n\u2217[\u03b4(\u03c9 \u2212\u03c90) \u2212\u03b4(\u03c9 + \u03c90)] ,\n(28)\n26\nHilbert Space Embedding and Characteristic Kernels\nwhere |\u03c90| \u2265\n\u0000N+2\n2\n\u0001\n\u03b2, N \u22652 and \u03b1 \u0338= 0. \u2217N\n1 represents the N-fold convolution. Note that \u03b8\nis such that supp(\u03b8)\u2229supp( b\u03c8) is a null set w.r.t. the Lebesgue measure, which satis\ufb01es (iii)\nin Lemma 16. It is easy to verify that \u03b8 \u2208L1(R) \u2229L2(R) \u2229Cb(R) also satis\ufb01es conditions\n(i) and (iv) in Lemma 16. \u03b8\u2228can be computed as\n\u03b8\u2228(x) = 2N\u03b1\n\u221a\n2\u03c0\nsin(\u03c90x)\nsinN \u0010\n\u03b2x\n2\n\u0011\nxN\n,\n(29)\nand \u03b8\u2228\u2208L1(R) \u2229L2(R) \u2229Cb(R) satis\ufb01es (ii) in Lemma 16. Choose\n0 < |\u03b1| \u2264\n\u221a\n2\n\u221a\u03c0\u03b2N supx\n\f\f\f(1 + x2) sin(\u03c90x)sincN \u0010\n\u03b2x\n2\u03c0\n\u0011\f\f\f\n,\n(30)\nwhere sinc(x) := sin(\u03c0x)\n\u03c0x\n. De\ufb01ne g(x) := sin(\u03c90x)sincN \u0010\n\u03b2x\n2\u03c0\n\u0011\n. Since g \u2208S1, 0 < supx |(1 +\nx2)g(x)| < \u221eand, therefore, \u03b1 is a \ufb01nite non-zero number. It is easy to see that \u03b8 satis\ufb01es\n(v) of Lemma 16. Then, by Lemma 16, there exists P \u0338= Q, P \u2208P0, given by\np(x) =\n1\n\u03c0(1 + x2) + 2N\u03b1\n\u221a\n2\u03c0\nsin(\u03c90x)\nsinN \u0010\n\u03b2x\n2\n\u0011\nxN\n,\n(31)\nwith \u03c6P = \u03c6Q + \u03b8 = \u03c6Q + i\u03b8I where \u03b8I = Im[\u03b8] and \u03c6P \u2208L1(R). So, we have constructed\nP \u0338= Q, such that \u03b3k(P, Q) = 0. Figure 2 shows the plots of \u03c8, b\u03c8, \u03b8, \u03b8\u2228, q, \u03c6Q, p and |\u03c6P|\nfor \u03b2 = 2\u03c0, N = 2, \u03c90 = 4\u03c0 and \u03b1 =\n1\n50.\nWe now prove Theorem 12.\nProof (Theorem 12) Suppose \u2203P \u0338= Q, P, Q \u2208P1 such that \u03b3k(P, Q) = 0. Since any\npositive Borel measure on Rd is a distribution (Rudin, 1991, p.\n157), P and Q can be\ntreated as distributions with compact support. By the Paley-Wiener theorem (Theorem 29\nin Appendix A), \u03c6P and \u03c6Q are entire on Cd. Let \u03b8 := \u03c6P \u2212\u03c6Q. Since \u03b3k(P, Q) = 0, we\nhave from (16) that\nR\nRd |\u03b8(\u03c9)|2 d\u039b(\u03c9) = 0. From Remark 17, it follows that supp(\u03b8) \u2282\ncl(Rd\\supp(\u039b)). Since supp(\u039b) has a non-empty interior, we have supp(\u03b8) \u228aRd. Thus,\nthere exists an open set, U \u2282Rd such that \u03b8(x) = 0, \u2200x \u2208U. Therefore, by Lemma 30 (see\nAppendix A), \u03b8 = 0, which means \u03c6P = \u03c6Q \u21d2P = Q, leading to a contradiction. So, there\ndoes not exist P \u0338= Q, P, Q \u2208P1 such that \u03b3k(P, Q) = 0 and k is therefore characteristic to\nP1.\nThe condition that supp(\u039b) has a non-empty interior is important for Theorem 12 to hold.\nIn the following, we provide a simple example to show that P \u0338= Q, P, Q \u2208P1 can be\nconstructed such that \u03b3k(P, Q) = 0, if k is a periodic translation invariant kernel for which\nint(supp(\u039b)) = \u2205.\nExample 5 Let Q be a uniform distribution on [\u2212\u03b2, \u03b2] \u2282R, i.e., q(x) =\n1\n2\u03b21[\u2212\u03b2,\u03b2](x) with\nits characteristic function, \u03c6Q(\u03c9) =\n1\n\u03b2\n\u221a\n2\u03c0\nsin(\u03b2\u03c9)\n\u03c9\n\u2208L2(R). Let \u03c8 be the Dirichlet kernel with\n27\nSriperumbudur, Gretton, Fukumizu, Sch\u00a8olkopf and Lanckriet\n\u22125\n0\n5\n\u22121\n0\n1\n2\n3\n4\n5\nx\n\u03c8(x)\n(a)\n\u03c8\u2227(\u03c9)\n\u22122\u03c0\n0\n2\u03c0\n0\n1\n(a\u2032)\n\u22125\n0\n5\n\u22120.3\n\u22120.2\n\u22120.1\n0\n0.1\n0.2\n0.3\nx\n\u03b8v(x)\n(b)\ni\u03b8(\u03c9)\n\u22126\u03c0 \u22124\u03c0 \u22122\u03c0\n0\n2\u03c0\n4\u03c0\n6\u03c0\n\u2212\u03c0/50\n0\n\u03c0/50\n(b\u2032)\nq(x)\n\u22125\n5\n0\n1/\u03c0\n\u22125\n0\n5\n0\n1/\u03c0\n(c)\n\u03c6Q(\u03c9)\n\u22125\n0\n5\n0\n(1/2\u03c0)1/2\n(c\u2032)\n\u22125\n0\n5\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\nx\np(x)\n(d)\n\u221220\n\u221210\n0\n10\n20\n0\n0.1\n0.2\n0.3\n0.4\n\u03c9\n|\u03c6P(\u03c9)|\n(d\u2032)\nFigure 2: (a-a\u2032) \u03c8 and its Fourier spectrum b\u03c8, (b-b\u2032) \u03b8\u2228and i\u03b8, (c-c\u2032) the Cauchy distri-\nbution, q and its characteristic function \u03c6Q, and (d-d\u2032) p = q + \u03b8\u2228and |\u03c6P|. See\nExample 4 for details.\n28\nHilbert Space Embedding and Characteristic Kernels\nperiod \u03c4, where \u03c4 \u2264\u03b2, i.e., \u03c8(x) =\nsin (2l+1)\u03c0x\n\u03c4\nsin \u03c0x\n\u03c4\nand b\u03c8(\u03c9) =\n\u221a\n2\u03c0 Pl\nj=\u2212l \u03b4\n\u0010\n\u03c9 \u22122\u03c0j\n\u03c4\n\u0011\nwith\nsupp( b\u03c8) = {2\u03c0j\n\u03c4 , j \u2208{0, \u00b11, . . . , \u00b1l}}. Clearly, supp( b\u03c8) has an empty interior. Let \u03b8 be\n\u03b8(\u03c9) = 8\n\u221a\n2\u03b1\ni\u221a\u03c0 sin\n\u0010\u03c9\u03c4\n2\n\u0011 sin2 \u0000 \u03c9\u03c4\n4\n\u0001\n\u03c4\u03c92\n,\n(32)\nwith \u03b1 \u2264\n1\n2\u03b2. It is easy to verify that \u03b8 \u2208L1(R) \u2229L2(R) \u2229Cb(R), so \u03b8 satis\ufb01es (i) in\nLemma 16. Since \u03b8(\u03c9) = 0 at \u03c9 = 2\u03c0l\n\u03c4 , l \u2208Z, supp(\u03b8) \u2229supp( b\u03c8) \u2282supp( b\u03c8) is a set of\nLebesgue measure zero, so (iii) and (iv) in Lemma 16 are satis\ufb01ed. \u03b8\u2228is given by\n\u03b8\u2228(x) =\n\uf8f1\n\uf8f4\n\uf8f2\n\uf8f4\n\uf8f3\n2\u03b1|x+ \u03c4\n2|\n\u03c4\n\u2212\u03b1,\n\u2212\u03c4 \u2264x \u22640\n\u03b1 \u2212\n2\u03b1|x\u2212\u03c4\n2|\n\u03c4\n,\n0 \u2264x \u2264\u03c4\n0,\notherwise,\n(33)\nwhere \u03b8\u2228\u2208L1(R) \u2229L2(R) \u2229Cb(R) satis\ufb01es (ii) in Lemma 16. Now, consider p = q + \u03b8\u2228,\nwhich is given as\np(x) =\n\uf8f1\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f2\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f3\n1\n2\u03b2,\nx \u2208[\u2212\u03b2, \u2212\u03c4] \u222a[\u03c4, \u03b2]\n2\u03b1|x+ \u03c4\n2|\n\u03c4\n+ 1\n2\u03b2 \u2212\u03b1,\nx \u2208[\u2212\u03c4, 0]\n\u03b1 + 1\n2\u03b2 \u2212\n2\u03b1|x\u2212\u03c4\n2|\n\u03c4\n,\nx \u2208[0, \u03c4]\n0,\notherwise.\n(34)\nClearly, p(x) \u22650, \u2200x and\nR\nR p(x) dx = 1. \u03c6P = \u03c6Q + \u03b8 = \u03c6Q + i\u03b8I where \u03b8I = Im[\u03b8] and\n\u03c6P \u2208L2(R). We have therefore constructed P \u0338= Q, such that \u03b3k(P, Q) = 0, where P and\nQ are compactly supported in R with characteristic functions in L2(R), i.e., P, Q \u2208P1.\nFigure 3 shows the plots of \u03c8, b\u03c8, \u03b8, \u03b8\u2228, q, \u03c6Q, p and |\u03c6P| for \u03c4 = 2, l = 2, \u03b2 = 3 and\n\u03b1 = 1\n8.\nWe now present the proof of Theorem 14, which is similar to that of Theorem 9.\nProof (Theorem 14) ( \u21d0) From (14), we have\n\u03b32\nk(P, Q) =\nZZ\nTd \u03c8(x \u2212y) d(P \u2212Q)(x) d(P \u2212Q)(y)\n(a)\n=\nZZ\nTd\nX\nn\u2208Zd\nA\u03c8(n) ei(x\u2212y)T n d(P \u2212Q)(x) d(P \u2212Q)(y)\n(b)\n=\nX\nn\u2208Zd\nA\u03c8(n)\n\f\f\f\f\nZ\nTd e\u2212ixT n d(P \u2212Q)(x)\n\f\f\f\f\n2\n(c)\n= (2\u03c0)2d X\nn\u2208Zd\nA\u03c8(n) |AP(n) \u2212AQ(n)|2 ,\n(35)\nwhere we have invoked Bochner\u2019s theorem (Theorem 13) in (a), Fubini\u2019s theorem in (b) and\nAP(n) :=\n1\n(2\u03c0)d\nZ\nTd e\u2212inT x dP(x), n \u2208Z,\n(36)\n29\nSriperumbudur, Gretton, Fukumizu, Sch\u00a8olkopf and Lanckriet\n\u22125\n0\n5\n\u22121\n0\n1\n2\n3\n4\n5\nx\n\u03c8(x)\n(a)\n(2\u03c0)\u22121/2\u03c8\u2227(\u03c9)\n\u22124\u03c0\n\u22123\u03c0\n\u22122\u03c0\n\u2212\u03c0\n0\n\u03c0\n2\u03c0\n3\u03c0\n4\u03c0\n0\n1\n(a\u2032)\n\u22126\n\u22123 \u22122 \u22121\n0\n1\n2\n3\n6\n\u22121/8\n0    \n1/8\nx\n\u03b8v(x)\n(b)\n\u22120.08\n\u22120.04\n0\n0.04\n0.08\ni\u03b8(\u03c9)\n\u22124\u03c0\n\u22123\u03c0\n\u22122\u03c0\n\u2212\u03c0\n0\n\u03c0\n2\u03c0\n3\u03c0\n4\u03c0\n(b\u2032)\n\u22126\n\u22123 \u22122\n\u22121\n0\n1\n2\n3\n6\n0  \n1/6\nx\nq(x)\n(c)\n\u22120.1\n0\n0.1\n0.2\n0.3\n0.4\n\u03c6Q(\u03c9)\n\u22124\u03c0\n\u22123\u03c0\n\u22122\u03c0\n\u2212\u03c0\n0\n\u03c0\n2\u03c0\n3\u03c0\n4\u03c0\n(c\u2032)\n\u22126\n\u22123 \u22122\n\u22121\n0\n1\n2\n3\n6\n0   \n1/24\n1/6\n7/24\nx\np(x)\n(d)\n0\n0.1\n0.2\n0.3\n0.4\n|\u03c6P(\u03c9)|\n\u22124\u03c0\n\u22123\u03c0\n\u22122\u03c0\n\u2212\u03c0\n0\n\u03c0\n2\u03c0\n3\u03c0\n4\u03c0\n(d\u2032)\nFigure 3: (a-a\u2032) \u03c8 and its Fourier spectrum b\u03c8, (b-b\u2032) \u03b8\u2228and i\u03b8, (c-c\u2032) the uniform distri-\nbution, q and its characteristic function \u03c6Q, and (d-d\u2032) p = q + \u03b8\u2228and |\u03c6P|. See\nExample 5 for details.\n30\nHilbert Space Embedding and Characteristic Kernels\nin (c). AP is the Fourier transform of P in Td. Since A\u03c8(0) \u22650 and A\u03c8(n) > 0, \u2200n \u0338= 0, we\nhave AP(n) = AQ(n), \u2200n. Therefore, by the uniqueness theorem of Fourier transform, we\nhave P = Q.\n( \u21d2) Proving the necessity is equivalent to proving that if A\u03c8(0) \u22650, A\u03c8(n) > 0, \u2200n \u0338= 0\nis violated, then k is not characteristic, which is equivalent to showing that \u2203P \u0338= Q such\nthat \u03b3k(P, Q) = 0. Let Q be a uniform probability measure with q(x) =\n1\n(2\u03c0)d , \u2200x \u2208Td.\nLet k be such that A\u03c8(n) = 0 for some n = n0 \u0338= 0. De\ufb01ne\nAP(n) :=\n\u001a\nAQ(n),\nn \u0338= \u00b1n0\nAQ(n) + \u03b8(n),\nn = \u00b1n0 ,\n(37)\nwhere AQ(n) =\n1\n(2\u03c0)d \u03b4n0 and \u03b8(\u2212n0) = \u03b8(n0). So,\np(x) =\nX\nn\u2208Zd\nAP (n)eixT n =\n1\n(2\u03c0)d + \u03b8(n0)eixT n0 + \u03b8(\u2212n0)e\u2212ixT n0.\n(38)\nChoose \u03b8(n0) = i\u03b1, \u03b1 \u2208R. Then, p(x) =\n1\n(2\u03c0)d \u22122\u03b1 sin(xT n0). It is easy to check that p\nintegrates to one. Choosing |\u03b1| \u2264\n1\n2(2\u03c0)d ensures that p(x) \u22650, \u2200x \u2208Td. By using AP(n) in\n(35), it is clear that \u03b3k(P, Q) = 0. Therefore, \u2203P \u0338= Q such that \u03b3k(P, Q) = 0, which means\nk is not characteristic.\n4. Dissimilar Distributions with Small \u03b3k\nSo far, we have studied di\ufb00erent characterizations for the kernel k such that \u03b3k is a metric\non P. As mentioned in Section 1, the metric property of \u03b3k is crucial in many statistical\ninference applications like hypothesis testing. Therefore, in practice, it is important to use\ncharacteristic kernels. However, in this section, we show that characteristic kernels, while\nguaranteeing \u03b3k to be a metric on P, may nonetheless have di\ufb03culty in distinguishing\ncertain distributions on the basis of \ufb01nite samples. More speci\ufb01cally, in Theorem 19 we\nshow that for a given kernel, k and for any \u03b5 > 0, there exist P \u0338= Q such that \u03b3k(P, Q) < \u03b5.\nBefore proving the result, we motivate it through the following example.\nExample 6 Let P be absolutely continuous w.r.t.\nthe Lebesgue measure on R with the\nRadon-Nikodym derivative de\ufb01ned as\np(x) = q(x) + \u03b1q(x) sin(\u03bd\u03c0x),\n(39)\nwhere q is the Radon-Nikodym derivative of Q w.r.t. the Lebesgue measure satisfying q(x) =\nq(\u2212x), \u2200x and \u03b1 \u2208[\u22121, 1]\\{0}, \u03bd \u2208R\\{0}. It is obvious that P \u0338= Q. The characteristic\nfunction of P is given as\n\u03c6P(\u03c9) = \u03c6Q(\u03c9) \u2212i\u03b1\n2 [\u03c6Q(\u03c9 \u2212\u03bd\u03c0) \u2212\u03c6Q(\u03c9 + \u03bd\u03c0)] , \u03c9 \u2208R,\n(40)\nwhere \u03c6Q is the characteristic function associated with Q. Note that with increasing |\u03bd|, p\nhas higher frequency components in its Fourier spectrum and therefore appears more noisy\n31\nSriperumbudur, Gretton, Fukumizu, Sch\u00a8olkopf and Lanckriet\nas shown in Figure 4. In Figure 4, (a-c) show the plots of p when q = U[\u22121, 1] (uniform\ndistribution) and (a\u2032-c\u2032) show the plots of p when q = N(0, 2) (zero mean normal distribution\nwith variance 2) for \u03bd = 0, 2 and 7.5 with \u03b1 = 1\n2.\nConsider the B1-spline kernel on R given by k(x, y) = \u03c8(x \u2212y) where\n\u03c8(x) =\n\u001a 1 \u2212|x|,\n|x| \u22641\n0,\notherwise ,\n(41)\nwith its Fourier transform given by\nb\u03c8(\u03c9) = 2\n\u221a\n2\n\u221a\u03c0\nsin2 \u03c9\n2\n\u03c92\n.\n(42)\nSince \u03c8 is characteristic to P, \u03b3k(P, Q) > 0 (see Theorem 9).\nHowever, it would be\nof interest to study the behavior of \u03b3k(P, Q) as a function of \u03bd.\nWe study the behav-\nior of \u03b32\nk(P, Q) through its unbiased, consistent estimator,9 \u03b32\nk,u(m, m) as considered by\nGretton et al. (2007, Lemma 7).\nFigure 5(a) shows the behavior of \u03b32\nk,u(m, m) as a function of \u03bd for q = U[\u22121, 1] and\nq = N(0, 2) using the B1-spline kernel in (41). Since the Gaussian kernel, k(x, y) = e\u2212(x\u2212y)2\nis also a characteristic kernel, its e\ufb00ect on the behavior of \u03b32\nk,u(m, m) is shown in Figure 5(b)\nin comparison to that of the B1-spline kernel.\nIn Figure 5, we observe two circumstances under which \u03b32\nk may be small. First, \u03b32\nk,u(m, m)\ndecays with increasing |\u03bd|, and can be made as small as desired by choosing a su\ufb03ciently\nlarge |\u03bd|. Second, in Figure 5(a), \u03b32\nk,u(m, m) has troughs at \u03bd = \u03c90\n\u03c0 where \u03c90 = {\u03c9 : b\u03c8(\u03c9) =\n0}. Since \u03b32\nk,u(m, m) is a consistent estimate of \u03b32\nk(P, Q), one would expect similar behavior\nfrom \u03b32\nk(P, Q). This means that, although the B1-spline kernel is characteristic to P, in\npractice, it becomes harder to distinguish between P and Q with \ufb01nite samples, when P is\nconstructed as in (39) with \u03bd = \u03c90\n\u03c0 . In fact, one can observe from a straightforward spectral\nargument that the troughs in \u03b32\nk(P, Q) can be made arbitrarily deep by widening q, when q\nis Gaussian.\nFor characteristic kernels, although \u03b3k(P, Q) > 0 when P \u0338= Q, Example 6 demonstrates that\none can construct distributions such that \u03b32\nk,u(m, m) is indistinguishable from zero with high\nprobability, for a given sample size m. Below, in Theorem 19, we explicitly construct P \u0338= Q\nsuch that |P\u03d5l \u2212Q\u03d5l| is large for some large l, but \u03b3k(P, Q) is arbitrarily small, making\nit hard to detect a non-zero value of \u03b3k(P, Q) based on \ufb01nite samples. Here, \u03d5l \u2208L2(M)\nrepresents the bounded orthonormal eigenfunctions of a positive de\ufb01nite integral operator\nassociated with k. Based on this theorem, e.g., in Example 6, the decay mode of \u03b3k for\nlarge |\u03bd| can be investigated.\nConsider the formulation of \u03b3F with F = Fk in (1). The construction of P for a given\nQ such that \u03b3k(P, Q) is small, though not zero, can be intuitively understood by re-writing\n9. Let {Xj}m\nj=1 and {Yj}m\nj=1 be random samples drawn i.i.d. from P and Q respectively. An unbiased\nempirical estimate of \u03b32\nk(P, Q), denoted as \u03b32\nk,u(m, m) is given by \u03b32\nk,u(m, m) =\n1\nm(m\u22121)\nPm\nl\u0338=j h(Zl, Zj),\nwhich is a one-sample U-statistic with h(Zl, Zj) := k(Xl, Xj) + k(Yl, Yj) \u2212k(Xl, Yj) \u2212k(Xj, Yl), where\nZ1, . . . , Zm are m i.i.d. random variables with Zj := (Xj, Yj). See Gretton et al. (2007, Lemma 7) for\ndetails.\n32\nHilbert Space Embedding and Characteristic Kernels\n\u22122\n\u22121\n0\n1\n2\n0  \n0.5\nx\nq(x)\n(a)\n\u22122\n\u22121\n0\n1\n2\n0\n0.2\n0.4\n0.6\n0.8\nx\np(x)\n(b)\n\u22122\n\u22121\n0\n1\n2\n0\n0.2\n0.4\n0.6\n0.8\nx\np(x)\n(c)\n\u22125\n0\n5\n0\n0.1\n0.2\n0.3\nx\nq(x)\n(a\u2032)\n\u22125\n0\n5\n0\n0.1\n0.2\n0.3\n0.4\nx\np(x)\n(b\u2032)\n\u22125\n0\n5\n0\n0.1\n0.2\n0.3\n0.4\nx\np(x)\n(c\u2032)\nFigure 4: (a) q = U[\u22121, 1], (a\u2032) q = N(0, 2). (b-c) and (b\u2032-c\u2032) denote p(x) computed as\np(x) = q(x) + 1\n2q(x) sin(\u03bd\u03c0x) with q = U[\u22121, 1] and q = N(0, 2) respectively. \u03bd is\nchosen to be 2 in (b,b\u2032) and 7.5 in (c,c\u2032). See Example 6 for details.\n\u22128\n\u22126\n\u22124\n\u22122\n0\n2\n4\n6\n8\n10\n\u22124\n10\n\u22123\n10\n\u22122\n10\n\u22121\n\u03bd\n\u03b32\nk,u(m,m)\n \n \nUniform\nGaussian\n(a)\n\u22128\n\u22126\n\u22124\n\u22122\n0\n2\n4\n6\n8\n0.05\n0.1\n0.15\n0.2\n0.25\n0.3\n\u03bd\n\u03b32\nk,u(m,m)\n \n \nUniform\nGaussian\n(b)\nFigure 5: Behavior of the empirical estimate of \u03b32\nk(P, Q) w.r.t.\n\u03bd for (a) the B1-spline\nkernel and (b) the Gaussian kernel. P is constructed from Q as de\ufb01ned in (39).\n\u201cUniform\u201d corresponds to Q = U[\u22121, 1] and \u201cGaussian\u201d corresponds to Q =\nN(0, 2). m = 1000 samples are generated from P and Q to estimate \u03b32\nk(P, Q)\nthrough \u03b32\nk,u(m, m). This is repeated 100 times and the average \u03b32\nk,u(m, m) is\nplotted in both \ufb01gures. Since the quantity of interest is the average behavior of\n\u03b32\nk,u(m, m), we omit the error bars. See Example 6 for details.\n(1) as\n\u03b3k(P, Q) = sup\nf\u2208H\n|Pf \u2212Qf|\n\u2225f\u2225H\n.\n(43)\n33\nSriperumbudur, Gretton, Fukumizu, Sch\u00a8olkopf and Lanckriet\nWhen P \u0338= Q, |Pf \u2212Qf| can be large for some f \u2208H. However, \u03b3k(P, Q) can be made\nsmall by selecting P such that the maximization of |Pf\u2212Qf|\n\u2225f\u2225H\nover H requires an f with large\n\u2225f\u2225H. More speci\ufb01cally, higher order eigenfunctions of the kernel (\u03d5l for large l) have large\nRKHS norms, so, if they are prominent in P and Q (i.e., highly non-smooth distributions),\none can expect \u03b3k(P, Q) to be small even when there exists an l for which |P\u03d5l \u2212Q\u03d5l| is\nlarge. To this end, we need the following lemma, which we quote from Gretton et al. (2004,\nLemma 6).\nLemma 18 (Gretton et al. (2004)) Let F be the unit ball in an RKHS (H, k) de\ufb01ned\non a compact topological space, M, with k being measurable. Let \u03d5l \u2208L2(M, \u00b5) be absolutely\nbounded orthonormal eigenfunctions and \u03bbl be the corresponding eigenvalues (arranged in\na decreasing order for increasing l) of a positive de\ufb01nite integral operator associated with\nk and a \u03c3-\ufb01nite measure, \u00b5. Assume \u03bb\u22121\nl\nincreases superlinearly with l. Then, for f \u2208F\nwhere f(x) = P\u221e\nj=1 efj\u03d5j(x), efj := \u27e8f, \u03d5j\u27e9L2(M,\u00b5), we have P\u221e\nj=1 | efj| < \u221eand for every\n\u03b5 > 0, \u2203l0 \u2208N such that | efl| < \u03b5 if l > l0.\nTheorem 19 (P \u0338= Q can have arbitrarily small \u03b3k) Assume the conditions in Lemma 18\nhold. Then, there exist probability measures P \u0338= Q de\ufb01ned on M such that \u03b3k(P, Q) < \u03b5 for\nany arbitrarily small \u03b5 > 0.\nProof Suppose q be the Radon-Nikodym derivative associated with Q w.r.t. the \u03c3-\ufb01nite\nmeasure, \u00b5 (see Lemma 18). Let us construct p(x) = q(x) + \u03b1le(x) + \u03c4\u03d5l(x) where e(x) =\n1M(x). For P to be a probability measure, the following conditions need to be satis\ufb01ed:\nZ\nM\n[\u03b1le(x) + \u03c4\u03d5l(x)] d\u00b5(x) = 0,\n(44)\nmin\nx\u2208M [q(x) + \u03b1le(x) + \u03c4\u03d5l(x)] \u22650.\n(45)\nExpanding e(x) and f(x) in the orthonormal basis {\u03d5l}\u221e\nl=1, we get e(x) = P\u221e\nl=1 eel\u03d5l(x) and\nf(x) = P\u221e\nl=1 efl\u03d5l(x), where eel := \u27e8e, \u03d5l\u27e9L2(M,\u00b5) and efl := \u27e8f, \u03d5l\u27e9L2(M,\u00b5). Therefore,\nPf \u2212Qf\n=\nZ\nM\nf(x) [\u03b1le(x) + \u03c4\u03d5l(x)] d\u00b5(x)\n=\nZ\nM\n\uf8ee\n\uf8f0\u03b1l\n\u221e\nX\nj=1\neej\u03d5j(x) + \u03c4\u03d5l(x)\n\uf8f9\n\uf8fb\n\" \u221e\nX\nt=1\neft\u03d5t(x)\n#\nd\u00b5(x)\n=\n\u03b1l\n\u221e\nX\nj=1\neej efj + \u03c4 efl,\n(46)\nwhere we used the fact that10 \u27e8\u03d5j, \u03d5t\u27e9L2(M,\u00b5) = \u03b4jt. Rewriting (44) and substituting for e(x)\ngives\nZ\nM\n[\u03b1le(x) + \u03c4\u03d5l(x)] d\u00b5(x) =\nZ\nM\ne(x)[\u03b1le(x) + \u03c4\u03d5l(x)] d\u00b5(x) = \u03b1l\n\u221e\nX\nj=1\nee2\nj + \u03c4eel = 0,\n10. Here, \u03b4 is used in the Kronecker sense.\n34\nHilbert Space Embedding and Characteristic Kernels\nwhich implies\n\u03b1l = \u2212\n\u03c4eel\nP\u221e\nj=1 ee2\nj\n.\n(47)\nNow, let us consider P\u03d5t \u2212Q\u03d5t = \u03b1leet + \u03c4\u03b4tl. Substituting for \u03b1l gives\nP\u03d5t \u2212Q\u03d5t = \u03c4\u03b4tl \u2212\u03c4\neeteel\nP\u221e\nj=1 ee2\nj\n= \u03c4\u03b4tl \u2212\u03c4\u03c1tl,\n(48)\nwhere \u03c1tl :=\neeteel\nP\u221e\nj=1 ee2\nj . By Lemma 18, P\u221e\nl=1 |eel| < \u221e\u21d2P\u221e\nj=1 ee2\nj < \u221e, and choosing large\nenough l gives |\u03c1tl| < \u03b7, \u2200t, for any arbitrary \u03b7 > 0. Therefore, |P\u03d5t \u2212Q\u03d5t| > \u03c4 \u2212\u03b7 for\nt = l and |P\u03d5t \u2212Q\u03d5t| < \u03b7 for t \u0338= l, which means P \u0338= Q. In the following, we prove that\n\u03b3k(P, Q) can be arbitrarily small, though non-zero.\nRecall that \u03b3k(P, Q) = sup\u2225f\u2225H\u22641 |Pf \u2212Qf|. Substituting for \u03b1l in (46) and replacing\n|Pf \u2212Qf| by (46) in \u03b3k(P, Q), we have\n\u03b3k(P, Q) =\nsup\n{ efj}\u221e\nj=1\n\uf8f1\n\uf8f2\n\uf8f3\u03c4\n\u221e\nX\nj=1\n\u03bdjl efj :\n\u221e\nX\nj=1\nef 2\nj\n\u03bbj\n\u22641\n\uf8fc\n\uf8fd\n\uf8fe,\n(49)\nwhere we used the de\ufb01nition of RKHS norm as \u2225f\u2225H := P\u221e\nj=1\nef2\nj\n\u03bbj and \u03bdjl := \u03b4jl \u2212\u03c1jl. (49) is\na convex quadratically constrained quadratic program in { efj}\u221e\nj=1. Solving the Lagrangian\nyields efj =\n\u03bdjl\u03bbj\nqP\u221e\nj=1 \u03bd2\njl\u03bbj . Therefore,\n\u03b3k(P, Q) = \u03c4\nv\nu\nu\nt\n\u221e\nX\nj=1\n\u03bd2\njl\u03bbj = \u03c4\nv\nu\nu\nt\u03bbl \u22122\u03c1ll\u03bbl +\n\u221e\nX\nj=1\n\u03c12\njl\u03bbj\nl\u2192\u221e\n\u2212\u21920,\n(50)\nbecause (i) by choosing su\ufb03ciently large l, |\u03c1jl| < \u03b5, \u2200j, for any arbitrary \u03b5 > 0, and (ii)\n\u03bbl \u21920 as l \u2192\u221e(Sch\u00a8olkopf and Smola, 2002, Theorem 2.10). Therefore, we have con-\nstructed P \u0338= Q such that \u03b3k(P, Q) < \u03b5 for any arbitrarily small \u03b5 > 0.\n5. Metrization of the Weak Topology\nSo far, we have shown that a characteristic kernel, k induces a metric, \u03b3k on P.\nAs\nmotivated in Section 1.1.3, an important question to consider that is useful both in theory\nand practice would be: \u201cHow strong or weak is \u03b3k related to other metrics on P?\u201d This\nquestion is addressed in Theorem 21, wherein we compared \u03b3k to other metrics on P like\nthe Dudley metric (\u03b2), Wasserstein distance (W), total variation distance (TV ) and showed\nthat \u03b3k is weaker than all these metrics (see footnote 3 for the de\ufb01nition of \u201cstrong\u201d and\n\u201cweak\u201d metrics). Since \u03b3k is weaker than the Dudley metric, which is well known to induce\na topology on P that coincides with the standard topology on P, called the weak-\u2217(weak-\nstar) topology (usually called the weak topology in probability theory), the next question\nwe are interested in is to understand the topology that is being induced by \u03b3k. In particular,\nwe are interested in determining the conditions on k for which the topology induced by \u03b3k\n35\nSriperumbudur, Gretton, Fukumizu, Sch\u00a8olkopf and Lanckriet\ncoincides with the weak topology on P. This is answered in Theorems 23 and 24, wherein\nTheorem 23 deals with compact M and Theorem 24 provides a su\ufb03cient condition on k\nwhen M = Rd.\nThe proofs of all these results are provided in Section 5.1.\nBefore we\nmotivate the need for this study and its implications, we present some preliminaries.\nThe weak topology on P is the weakest topology such that the map P 7\u2192\nR\nM f dP is\ncontinuous for all f \u2208Cb(M). For a metric space, (M, \u03c1), a sequence Pn of probability\nmeasures is said to converge weakly to P, written as Pn\nw\u2192P, if and only if\nR\nM f dPn \u2192\nR\nM f dP for every f \u2208Cb(M). A metric \u03b3 on P is said to metrize the weak topology if\nthe topology induced by \u03b3 coincides with the weak topology, which is de\ufb01ned as follows: if,\nfor P, P1, P2, . . . \u2208P, (Pn\nw\u2192P \u21d4\u03b3(Pn, P) n\u2192\u221e\n\u2212\u21920) holds, then the topology induced by \u03b3\ncoincides with the weak topology.\nIn the following, we collect well-known results on the relation between various metrics on\nP, which will be helpful to understand the behavior of these metrics in relation to others.\nLet (M, \u03c1) be a separable metric space. The Prohorov metric on (M, \u03c1), de\ufb01ned as\n\u03c2(P, Q) := inf{\u01eb > 0 : P(A) \u2264Q(A\u01eb) + \u01eb, \u2200Borel sets A},\n(51)\nmetrizes the weak topology on P (Dudley, 2002, Theorem 11.3.3), where P, Q \u2208P and\nA\u01eb := {y \u2208M : \u03c1(x, y) < \u01eb for some x \u2208A}. Since the Dudley metric is related to the\nProhorov metric as\n1\n2\u03b2(P, Q) \u2264\u03c2(P, Q) \u22642\np\n\u03b2(P, Q),\n(52)\nit also metrizes the weak topology on P (Dudley, 2002, Theorem 11.3.3). The Wasserstein\ndistance and total variation distance are related to the Prohorov metric as\n\u03c22(P, Q) \u2264W(P, Q) \u2264(diam(M) + 1)\u03c2(P, Q),\n(53)\nand\n\u03c2(P, Q) \u2264TV (P, Q),\n(54)\nwhere diam(M) := sup{\u03c1(x, y) : x, y \u2208M} (Gibbs and Su, 2002, Theorem 2). This means\nW and TV are stronger than \u03c2, while W and \u03c2 are equivalent (i.e., induce the same topology)\nwhen M is bounded. By Theorem 4 in Gibbs and Su (2002), TV and W are related as\nW(P, Q) \u2264diam(M)TV (P, Q),\n(55)\nwhich means W and TV are comparable if M is bounded. See Shorack (2000, Chapter 19,\nTheorem 2.4) and Gibbs and Su (2002) for the relationship between various metrics on P.\nNow, let us consider a sequence of of probability measures on R, Pn :=\n\u00001 \u22121\nn\n\u0001\n\u03b40 + 1\nn\u03b4n\nand let P := \u03b40. It can be shown that \u03b2(Pn, P) \u21920 as n \u2192\u221ewhich means Pn\nw\u2192P, while\nW(Pn, P) = 1 and TV (Pn, P) = 1 for all n. \u03b3k(Pn, P) can be computed as\n\u03b32\nk(Pn, P) = 1\nn2\nZZ\nR\nk(x, y) d(\u03b40 \u2212\u03b4n)(x) d(\u03b40 \u2212\u03b4n)(y) = k(0, 0) + k(n, n) \u22122k(0, n)\nn2\n. (56)\nIf k is, e.g., a Gaussian, Laplacian or inverse multiquadratic kernel, then \u03b3k(Pn, P) \u21920 as\nn \u2192\u221e. This example shows that \u03b3k is weaker than W and TV . It also shows that \u03b3k\nbehaves similar to \u03b2 and leads to several questions we want to answer: Does \u03b3k metrize\n36\nHilbert Space Embedding and Characteristic Kernels\nthe weak topology on P? What is the general behavior of \u03b3k compared to other metrics?\nIn other words, depending on k, how weak or strong is \u03b3k compared to other metrics on\nP? Understanding the answer to these questions is important both in theory and practice.\nIf k is characterized such that \u03b3k metrizes the weak topology on P, then it can be used\nas a theoretical tool in probability theory, similar to the Prohorov and Dudley metrics.\nOn the other hand, the answer to these questions is critical in applications as it will have\na bearing on the choice of kernels to be used. In applications like density estimation, one\nwould need a strong metric to ascertain that the density estimate is a good representation of\nthe true underlying density. For this reason, usually, the total variation distance, Hellinger\ndistance or Kullback-Leibler distance are used. Studying the relation of \u03b3k to these metrics\nwill provide an understanding about the choice of kernels to be used, depending on the\napplication.\nWith the above motivation, in the following, we \ufb01rst compare \u03b3k to \u03b2, W and TV .\nSince \u03b2 is equivalent to \u03c2, we do not compare \u03b3k to \u03c2. Before we provide the main result\nin Theorem 21 that compares \u03b3k to other metrics, we present an upper bound on \u03b3k in\nterms of the coupling formulation (Dudley, 2002, Section 11.8), which is not only useful in\nderiving the main result but also interesting in its own right.\nProposition 20 (Coupling bound) Let k be measurable and bounded on M. Then, for\nany P, Q \u2208P,\n\u03b3k(P, Q) \u2264\ninf\n\u00b5\u2208L(P,Q)\nZZ\nM\n\u2225k(\u00b7, x) \u2212k(\u00b7, y)\u2225H d\u00b5(x, y),\n(57)\nwhere L(P, Q) represents the set of all laws on M \u00d7 M with marginals P and Q.\nProof For any \u00b5 \u2208L(P, Q), we have\n\f\f\f\f\nZ\nM\nf d(P \u2212Q)\n\f\f\f\f =\n\f\f\f\f\nZZ\nM\n(f(x) \u2212f(y)) d\u00b5(x, y)\n\f\f\f\f \u2264\nZZ\nM\n|f(x) \u2212f(y)| d\u00b5(x, y)\n=\nZZ\nM\n|\u27e8f, k(\u00b7, x) \u2212k(\u00b7, y)\u27e9H| d\u00b5(x, y)\n\u2264\u2225f\u2225H\nZZ\nM\n\u2225k(\u00b7, x) \u2212k(\u00b7, y)\u2225H d\u00b5(x, y).\n(58)\nTaking the supremum over f \u2208Fk and the in\ufb01mum over \u00b5 \u2208L(P, Q) in (58), where\nP, Q \u2208P, gives the result in (57).\nWe now present the main result that compares \u03b3k to \u03b2, W and TV .\nTheorem 21 (Comparison of \u03b3k to \u03b2, W and TV ) Assume supx\u2208M k(x, x) \u2264C < \u221e,\nwhere k is measurable on M. Let\ne\u03c1(x, y) = \u2225k(\u00b7, x) \u2212k(\u00b7, y)\u2225H.\n(59)\nThen, for any P, Q \u2208P,\n(i) \u03b3k(P, Q) \u2264W(P, Q) \u2264\nq\n\u03b32\nk(P, Q) + 4C if (M, e\u03c1) is separable.\n(ii)\n\u03b3k(P,Q)\n(1+\n\u221a\nC) \u2264\u03b2(P, Q) \u22642(\u03b32\nk(P, Q) + 4C)\n1\n3 if (M, e\u03c1) is separable.\n37\nSriperumbudur, Gretton, Fukumizu, Sch\u00a8olkopf and Lanckriet\n(iii) \u03b3k(P, Q) \u2264\n\u221a\nC TV (P, Q).\nThe proof is provided in Section 5.1. Below are some remarks on Theorem 21.\nRemark 22 (a) First, note that, since k is bounded, (M, e\u03c1) is a bounded metric space. In\naddition, the metric, e\u03c1, which depends on the kernel as in (59), is a Hilbertian metric11\n(Berg et al., 1984, Chapter 3, Section 3) on M. A popular example of such a metric is\ne\u03c1(x, y) = \u2225x \u2212y\u22252, which can be obtained by choosing M to be a compact subset of Rd and\nk(x, y) = xT y.\n(b) Theorem 21 shows that \u03b3k is weaker than \u03b2, W and TV for the assumptions being made\non k and e\u03c1. Note that the result holds irrespective of whether the kernel is characteristic\nor not, as we have not assumed anything about the kernel except it being measurable and\nbounded. Also, it is important to remember that the result holds when e\u03c1 is Hilbertian, as\nmentioned in (59) (see Remark 22(d)).\n(c) Apart from showing that \u03b3k is weaker than \u03b2, W and TV , the result in Theorem 21\ncan be used to bound these metrics in terms of \u03b3k. For \u03b2, which is primarily of theoretical\ninterest, we do not know a closed form expression, whereas a closed form expression to\ncompute W is known only for R (Vallander, 1973).12 Since \u03b3k is easy to compute (see (13)\nand (14)), bounds on W can be obtained from Theorem 21 in terms of \u03b3k. A closed form\nexpression for TV is available if P and Q have Radon-Nikodym derivatives w.r.t. a \u03c3-\ufb01nite\nmeasure. However, from Theorem 21, a simple lower bound can be obtained on TV in terms\nof \u03b3k for any P, Q \u2208P.\n(d) In Theorem 21, the kernel is \ufb01xed and e\u03c1 is de\ufb01ned as in (59), which is a Hilbertian\nmetric. On the other hand, suppose a Hilbertian metric, e\u03c1 is given. Then, the associated\nkernel, k can be obtained from e\u03c1 (Berg et al., 1984, Chapter 3, Lemma 2.1) as\nk(x, y) = 1\n2[e\u03c12(x, x0) + e\u03c12(y, x0) \u2212e\u03c12(x, y)], x, y, x0 \u2208M,\n(60)\nwhich can then be used to compute \u03b3k.\nThe discussion so far has been devoted to relating \u03b3k to \u03b2, W and TV to understand the\nstrength or weakness of \u03b3k w.r.t. these metrics. In a next step, we address the other question\nof when \u03b3k metrizes the weak topology on P. This question would have been answered\nhad the result in Theorem 21 shown that under some conditions on k, \u03b3k is equivalent to \u03b2.\nSince Theorem 21 does not throw light on the question we are interested in, we approach the\nproblem di\ufb00erently. In the following, we provide two results related to this question. The\n\ufb01rst result states that when (M, \u03c1) is compact, \u03b3k induced by universal kernels metrizes the\nweak topology. In the second result, we relax the assumption of compactness but restrict\nourselves to M = Rd and provide a su\ufb03cient condition on k such that \u03b3k metrizes the weak\ntopology on P. The proofs of both theorems are provided in Section 5.1.\n11. A metric \u03c1 on M is said to be Hilbertian if there exists a Hilbert space, H and a mapping \u03a6 such that\n\u03c1(x, y) = \u2225\u03a6(x) \u2212\u03a6(y)\u2225H, \u2200x, y \u2208M. In our case, H = H and \u03a6 : M \u2192H, x 7\u2192k(\u00b7, x).\n12. The explicit form for the Wasserstein distance is known for (M, \u03c1(x, y)) = (R, |x \u2212y|), which is given as\nW (P,Q) =\nR\nR |FP(x) \u2212FQ(x)| dx, where FP(x) = P((\u2212\u221e, x]). It is easy to show that this explicit form\ncan be extended to (Rd, \u2225\u00b7 \u22251).\n38\nHilbert Space Embedding and Characteristic Kernels\nTheorem 23 (Weak convergence-I) Let (M, \u03c1) be a compact metric space. If k is uni-\nversal, then \u03b3k metrizes the weak topology on P.\nFrom Theorem 23, it is clear that \u03b3k is equivalent to \u03c2, \u03b2 and W (see (52) and (53)) when\nM is compact and k is universal.\nTheorem 24 (Weak convergence-II) Let M = Rd and k(x, y) = \u03c8(x \u2212y), where \u03c8 \u2208\nC0(Rd) \u2229L1(Rd) is a real-valued bounded strictly positive de\ufb01nite function. If there exists\nan l \u2208N such that\nZ\nRd\n1\nb\u03c8(\u03c9)(1 + \u2225\u03c9\u22252)l d\u03c9 < \u221e,\n(61)\nthen \u03b3k metrizes the weak topology on P.\nThe entire Mat\u00b4ern class of kernels in (22) satis\ufb01es the conditions of Theorem 24 and,\ntherefore, the corresponding \u03b3k metrizes the weak topology on P. Note that Gaussian\nkernels on Rd do not satisfy the condition in Theorem 24. The characterization of k for\ngeneral non-compact domains M (not necessarily Rd), such that \u03b3k metrizes the weak\ntopology on P, still remains an open problem.\n5.1 Proofs\nWe now present the proofs of Theorems 21, 23 and 24.\nProof (Theorem 21) (i) When (M, \u03c1) is separable, W(P, Q) has a coupling formulation\n(Dudley, 2002, p. 420), given as\nW(P, Q) =\ninf\n\u00b5\u2208L(P,Q)\nZZ\nM\n\u03c1(x, y) d\u00b5(x, y),\n(62)\nwhere P, Q \u2208{P \u2208P :\nR\nM \u03c1(x, y) dP(y) < \u221e, \u2200x \u2208M}. In our case \u03c1(x, y) = \u2225k(\u00b7, x) \u2212\nk(\u00b7, y)\u2225H. In addition, (M, \u03c1) is bounded, which means (62) holds for all P, Q \u2208P. The\nlower bound therefore follows from (57).\nThe upper bound can be obtained as follows.\nConsider W(P, Q) = inf\u00b5\u2208L(P,Q)\nRR\nM \u2225k(\u00b7, x) \u2212k(\u00b7, y)\u2225H d\u00b5(x, y), which can be bounded as\nW(P, Q) \u2264\nZZ\nM\n\u2225k(\u00b7, x) \u2212k(\u00b7, y)\u2225H dP(x) dQ(y)\n(a)\n\u2264\n\u0014ZZ\nM\n\u2225k(\u00b7, x) \u2212k(\u00b7, y)\u22252\nH dP(x) dQ(y)\n\u0015 1\n2\n\u2264\n\u0014Z\nM\nk(x, x) d(P + Q)(x) \u22122\nZZ\nM\nk(x, y) dP(x) dQ(y)\n\u0015 1\n2\n\u2264\n\u0014\n\u03b32\nk(P, Q) +\nZZ\nM\n(k(x, x) \u2212k(x, y)) d(P \u2297P + Q \u2297Q)(x, y)\n\u0015 1\n2\n\u2264\nq\n\u03b32\nk(P, Q) + 4C,\n(63)\nwhere we have used Jensen\u2019s inequality (Folland, 1999, p. 109) in (a).\n39\nSriperumbudur, Gretton, Fukumizu, Sch\u00a8olkopf and Lanckriet\n(ii) Let F := {f : \u2225f\u2225H < \u221e} and G := {f : \u2225f\u2225BL < \u221e}. For f \u2208F, we have\n\u2225f\u2225BL = sup\nx\u0338=y\n|f(x) \u2212f(y)|\n\u03c1(x, y)\n+ sup\nx\u2208M\n|f(x)| = sup\nx\u0338=y\n|\u27e8f, k(\u00b7, x) \u2212k(\u00b7, y)\u27e9H|\n\u2225k(\u00b7, x) \u2212k(\u00b7, y)\u2225H\n+ sup\nx\u2208M\n|\u27e8f, k(\u00b7, x)\u27e9H|\n\u2264(1 +\n\u221a\nC)\u2225f\u2225H < \u221e,\n(64)\nwhich implies f \u2208G and, therefore, F \u2282G. For any P, Q \u2208P,\n\u03b3k(P, Q) = sup{|Pf \u2212Qf| : f \u2208Fk}\n\u2264sup{|Pf \u2212Qf| : \u2225f\u2225BL \u2264(1 +\n\u221a\nC), f \u2208F}\n\u2264sup{|Pf \u2212Qf| : \u2225f\u2225BL \u2264(1 +\n\u221a\nC), f \u2208G}\n= (1 +\n\u221a\nC)\u03b2(P, Q).\nThe upper bound is obtained as follows. For any P, Q \u2208P, by Markov\u2019s inequality (Folland,\n1999, Theorem 6.17), for all \u01eb > 0, we have\n\u01eb2\u00b5(\u2225k(\u00b7, X) \u2212k(\u00b7, Y )\u2225H > \u01eb) \u2264\nZZ\nM\n\u2225k(\u00b7, x) \u2212k(\u00b7, y)\u22252\nH d\u00b5(x, y),\nwhere X and Y are distributed as P and Q respectively. Choose \u01eb such that \u01eb3 =\nRR\nM \u2225k(\u00b7, x)\u2212\nk(\u00b7, y)\u22252\nH d\u00b5(x, y), such that \u00b5(\u2225k(\u00b7, X) \u2212k(\u00b7, Y )\u2225H > \u01eb) \u2264\u01eb. From the proof of Theorem\n11.3.5 of Dudley (2002), when (M, \u03c1) is separable, we have\n\u00b5(\u03c1(X, Y ) \u2265\u01eb) < \u01eb \u21d2\u03c2(P, Q) \u2264\u01eb,\nwhich implies that\n\u03c2(P, Q) \u2264\n\u0012\ninf\n\u00b5\u2208L(P,Q)\nZZ\nM\n\u2225k(\u00b7, x) \u2212k(\u00b7, y)\u22252\nH d\u00b5(x, y)\n\u0013 1\n3\n\u2264\n\u0012ZZ\nM\n\u2225k(\u00b7, x) \u2212k(\u00b7, y)\u22252\nH dP(x) dQ(y)\n\u0013 1\n3\n(b)\n\u2264\n\u0000\u03b32\nk(P, Q) + 4C\n\u0001 1\n3 ,\n(65)\nwhere (b) follows from (63). The result follows from (52).\n(iii) The proof of this result was presented in Sriperumbudur et al. (2009b) and is provided\nhere for completeness. To prove the result, we use (57) and the coupling formulation for\nTV (Lindvall, 1992, p. 19), given as\n1\n2TV (P, Q) =\ninf\n\u00b5\u2208L(P,Q) \u00b5(X \u0338= Y ),\n(66)\nwhere L(P, Q) is the set of all measures on M \u00d7 M with marginals P and Q. Here, X and\nY are distributed as P and Q respectively. Consider\n\u2225k(\u00b7, x) \u2212k(\u00b7, y)\u2225H \u22641{x\u0338=y}\u2225k(\u00b7, x) \u2212k(\u00b7, y)\u2225H \u22642\n\u221a\nC1{x\u0338=y}.\n(67)\n40\nHilbert Space Embedding and Characteristic Kernels\nTaking expectations w.r.t. \u00b5 and the in\ufb01mum over \u00b5 \u2208L(P, Q) on both sides of (67) gives\nthe desired result, which follows from (57).\nProof (Theorem 23) We need to show that for measures P, P1, P2, . . . \u2208P, Pn\nw\u2192P if and\nonly if \u03b3k(Pn, P) \u21920 as n \u2192\u221e. One direction is trivial as Pn\nw\u2192P implies \u03b3k(Pn, P) \u21920\nas n \u2192\u221e. We prove the other direction as follows. Since k is universal, H is dense in\nCb(M), the space of bounded continuous functions, w.r.t. the uniform norm, i.e., for any\nf \u2208Cb(M) and every \u01eb > 0, there exists a g \u2208H such that \u2225f \u2212g\u2225\u221e\u2264\u01eb. Therefore,\n|Pnf \u2212Pf| = |Pn(f \u2212g) + P(g \u2212f) + (Png \u2212Pg)|\n\u2264Pn|f \u2212g| + P|f \u2212g| + |Png \u2212Pg|\n\u22642\u01eb + |Png \u2212Pg| \u22642\u01eb + \u2225g\u2225H\u03b3k(Pn, P).\n(68)\nSince \u03b3k(Pn, P) \u21920 as n \u2192\u221eand \u01eb is arbitrary, |Pnf \u2212Pf| \u21920 for any f \u2208Cb(M).\nProof (Theorem 24) As mentioned in the proof of Theorem 23, one direction of the\nproof is straightforward: Pn\nw\u2192P \u21d2\u03b3k(Pn, P) \u21920 as n \u2192\u221e. Let us consider the other\ndirection. Since \u03c8 \u2208C0(Rd) \u2229L1(Rd) is a strictly positive de\ufb01nite function, any f \u2208H\nsatis\ufb01es (Wendland, 2005, Theorem 10.12)\nZ\nRd\n| bf(\u03c9)|2\nb\u03c8(\u03c9)\nd\u03c9 < \u221e.\n(69)\nAssume that\nsup\n\u03c9\u2208Rd(1 + \u2225\u03c9\u22252)l| bf(\u03c9)|2 < \u221e,\n(70)\nfor any l \u2208N, which means f \u2208Sd. Let (61) be satis\ufb01ed for some l = l0. Then,\nZ\nRd\n| bf(\u03c9)|2\nb\u03c8(\u03c9)\nd\u03c9 =\nZ\nRd\n| bf(\u03c9)|2(1 + \u2225\u03c9\u22252)l0\nb\u03c8(\u03c9)(1 + \u2225\u03c9\u22252)l0\nd\u03c9\n\u2264sup\n\u03c9\u2208Rd(1 + \u2225\u03c9\u22252)l0| bf(\u03c9)|2\nZ\nRd\n1\nb\u03c8(\u03c9)(1 + \u2225\u03c9\u22252)l0 d\u03c9 < \u221e,\nwhich means f \u2208H, i.e., if f \u2208Sd, then f \u2208H, which implies Sd \u2282H. Note that S (Rd)\nis dense in C0(Rd). Since \u03c8 \u2208C0(Rd), we have H \u2282C0(Rd) and, therefore, H is dense in\nC0(Rd) w.r.t. the uniform norm. Suppose P, P1, P2, . . . \u2208P. Using a similar analysis as in\nthe proof of Theorem 23, it can be shown that for any f \u2208C0(Rd) and every \u01eb > 0, there\nexists a g \u2208H such that |Pnf \u2212Pf| \u22642\u01eb+|Png\u2212Pg|. Since \u01eb is arbitrary and \u03b3k(Pn, P) \u21920\nas n \u2192\u221e, the result follows.\n6. Conclusion and Discussion\nIn this paper, we have studied various properties associated with a pseudometric, \u03b3k on\nP, which is based on the Hilbert space embedding of probability measures.\nFirst, we\nstudied the conditions on the kernel (called the characteristic kernel) under which \u03b3k is a\nmetric and showed that, apart from universal kernels, a large family of bounded continuous\n41\nSriperumbudur, Gretton, Fukumizu, Sch\u00a8olkopf and Lanckriet\nkernels induce a metric on P: (a) integrally strictly pd kernels and (b) translation invariant\nkernels on Rd and Td that have the support of their Fourier transform to be Rd and Zd\nrespectively. Next, we showed that there exist distinct distributions which will be considered\nclose according to \u03b3k (whether or not the kernel is characteristic), and thus may be hard\nto distinguish based on \ufb01nite samples. Finally, we compared \u03b3k to other metrics on P and\nexplicitly presented the conditions under which it induces a weak topology on P. These\nresults together provide a strong theoretical foundation for using the \u03b3k metric in both\nstatistics and machine learning applications.\nNow, we discuss two topics related to \u03b3k, one about the choice of kernel parameter and\nthe other about kernels de\ufb01ned on P.\nAn important question that we did not discuss in this paper is how to choose a charac-\nteristic kernel. Let us consider the following setting: M = Rd and k\u03c3(x, y) = exp(\u2212\u03c3\u2225x \u2212\ny\u22252\n2), \u03c3 \u2208R+, a Gaussian kernel with \u03c3 as the bandwidth parameter. {k\u03c3 : \u03c3 \u2208R+} is the\nfamily of Gaussian kernels and {\u03b3k\u03c3 : \u03c3 \u2208R+} is the associated family of distance measures\nindexed by the kernel parameter, \u03c3. Note that k\u03c3 is characteristic for any \u03c3 \u2208R++ and,\ntherefore, \u03b3k\u03c3 is a metric on P for any \u03c3 \u2208R++. In practice, one would prefer a single\nnumber that de\ufb01nes the distance between P and Q. The question therefore to be addressed\nis how to choose an appropriate \u03c3. Note that as \u03c3 \u21920, k\u03c3 \u21921 and as \u03c3 \u2192\u221e, k\u03c3 \u21920 a.e.,\nwhich means \u03b3k\u03c3(P, Q) \u21920 as \u03c3 \u21920 or \u03c3 \u2192\u221efor all P, Q \u2208P. This behavior is also\nexhibited by k\u03c3(x, y) = exp(\u2212\u03c3\u2225x \u2212y\u22251), \u03c3 > 0 and k\u03c3(x, y) = \u03c32/(\u03c32 + \u2225x \u2212y\u22252\n2), \u03c3 > 0,\nwhich are also characteristic. This means choosing su\ufb03ciently small or su\ufb03ciently large \u03c3\n(depending on P and Q) makes \u03b3k\u03c3(P, Q) arbitrarily small. Therefore, \u03c3 must be chosen\nappropriately in applications to e\ufb00ectively distinguish between P and Q.\nTo this end, one can consider the following modi\ufb01cation to \u03b3k, which yields a pseudo-\nmetric on P,\n\u03b3(P, Q) = sup{\u03b3k(P, Q) : k \u2208K} = sup{\u2225Pk \u2212Qk\u2225H : k \u2208K}.\n(71)\nNote that \u03b3 is the maximal RKHS distance between P and Q over a family, K of measurable\nand bounded positive de\ufb01nite kernels. It is easy to check that, if any k \u2208K is characteristic,\nthen \u03b3 is a metric on P. Examples for K include:\n1. Kg := {e\u2212\u03c3\u2225x\u2212y\u22252\n2, x, y \u2208Rd : \u03c3 \u2208R+}.\n2. Kl := {e\u2212\u03c3\u2225x\u2212y\u22251, x, y \u2208Rd : \u03c3 \u2208R+}.\n3. K\u03c8 := {e\u2212\u03c3\u03c8(x,y), x, y \u2208M : \u03c3 \u2208R+}, where \u03c8 : M \u00d7 M \u2192R is a negative de\ufb01nite\nkernel (Berg et al., 1984, Chapter 3).\n4. Krbf := {\nR \u221e\n0\ne\u2212\u03bb\u2225x\u2212y\u22252\n2 d\u00b5\u03c3(\u03bb), x, y \u2208Rd, \u00b5\u03c3 \u2208M + : \u03c3 \u2208\u03a3 \u2282Rd}, where M + is the\nset of all \ufb01nite nonnegative Borel measures, \u00b5\u03c3 on R+ that are not concentrated at\nzero, etc.\n5. Klin := {k\u03bb = Pl\nj=1 \u03bbjkj | k\u03bb is pd, Pl\nj=1 \u03bbj = 1}, which is the linear combination of\npd kernels {kj}l\nj=1.\n6. Kcon := {k\u03bb = Pl\nj=1 \u03bbjkj | \u03bbj \u22650, Pl\nj=1 \u03bbj = 1}, which is the convex combination of\npd kernels {kj}l\nj=1.\n42\nHilbert Space Embedding and Characteristic Kernels\nThe idea and validity behind the proposal of \u03b3 in (71) can be understood from a Bayesian\nperspective, where we de\ufb01ne a non-negative \ufb01nite measure \u03bb over K, and average \u03b3k over\nthat measure, i.e., \u03b1(P, Q) :=\nR\nK \u03b3k(P, Q) d\u03bb(k). This also yields a pseudometric on P.\nThat said, \u03b1(P, Q) \u2264\u03bb(K)\u03b3(P, Q), \u2200P, Q, which means that, if P and Q can be distinguished\nby \u03b1, then they can be distinguished by \u03b3, but not vice-versa. In this sense, \u03b3 is stronger\nthan \u03b1 and therefore studying \u03b3 makes sense. One further complication with the Bayesian\napproach is in de\ufb01ning a sensible \u03bb over K. Note that \u03b3k0 can be obtained by de\ufb01ning\n\u03bb(k) = \u03b4(k\u2212k0) in \u03b1(P, Q). Future work will include analyzing \u03b3 and investigating its utility\nin applications compared to that of \u03b3k (with a \ufb01xed kernel, k). Refer to Sriperumbudur et al.\n(2009a) for some preliminary work, wherein we showed that \u03b3(Pm, Qn) is a\np\nmn/(m + n)-\nconsistent estimator of \u03b3(P, Q), for the class K of kernels shown in the previous page.\nWe now discuss how kernels on P can be obtained from \u03b3k. As discussed in the paper,\n\u03b3k is a Hilbertian metric on P. Therefore, using (60), the associated kernel can be easily\ncomputed as\nK(P, Q) =\n\u001cZ\nM\nk(\u00b7, x) dP(x),\nZ\nM\nk(\u00b7, x) dQ(x)\n\u001d\nH\n=\nZZ\nM\nk(x, y) dP(x) dQ(y),\nwhere K : P \u00d7 P \u2192R is a positive de\ufb01nite kernel, which can be seen as the dot-product\nkernel on P. Using the results in Berg et al. (1984, Chapter 3, Theorems 2.2 and 2.3),\nGaussian and inverse multi-quadratic kernels on P can be de\ufb01ned as\nK(P, Q) = exp(\u2212\u03c3\u03b32\nk(P, Q)), \u03c3 > 0 and K(P, Q) = (\u03c3 + \u03b32\nk(P, Q))\u22121, \u03c3 > 0\nrespectively.\nBroadly, this relates to the work on Hilbertian metrics and positive de\ufb01-\nnite kernels on probability measures by Hein and Bousquet (2005) and Fuglede and Tops\u00f8e\n(2003).\nAcknowledgments\nB. K. S. wishes to acknowledge support from the Max Planck Institute (MPI) for Biological\nCybernetics, the National Science Foundation (grant dms-mspa 0625409), the Fair Isaac\nCorporation and the University of California MICRO program.\nPart of this work was\ndone while the \ufb01rst author was an intern at the MPI, and part was done while A. G. was\na project scientist at CMU, under grants DARPA IPTO FA8750-09-1-0141, ONR MURI\nN000140710747, and ARO MURI W911NF0810242. This work is also supported by the\nIST Program of the EC, under the FP7 Network of Excellence, ICT-216886-NOE. B. K. S.\nwishes to thank Agnes Radl for her comments on the manuscript.\nAppendix A. Supplementary Results\nFor completeness, we present the supplementary results that were used to prove the results\nin this paper. The following result is quoted from Folland (1999, Theorem 8.14).\nTheorem 25 Suppose \u03c6 \u2208L1(Rd),\nR\n\u03c6(x) dx = a and \u03c6t(x) = t\u2212d\u03c6(t\u22121x) for t > 0. If f\nis bounded and uniformly continuous on Rd, then f \u2217\u03c6t \u2192af uniformly as t \u21920.\n43\nSriperumbudur, Gretton, Fukumizu, Sch\u00a8olkopf and Lanckriet\nBy imposing slightly stronger conditions on \u03c6, the following result quoted from Folland\n(1999, Theorem 8.15) shows that f \u2217\u03c6t \u2192af almost everywhere for f \u2208Lr(Rd).\nTheorem 26 Suppose |\u03c6(x)| \u2264C(1 + \u2225x\u22252)\u2212d\u2212\u03b5 for some C, \u03b5 > 0, and\nR\n\u03c6(x) dx = a. If\nf \u2208Lr(Rd) (1 \u2264r \u2264\u221e), then f \u2217\u03c6t(x) \u2192af(x) as t \u21920 for every x in the Lebesgue set\nof f \u2014 in particular, for almost every x, and for every x at which f is continuous.\nTheorem 27 (Fourier transform of a measure) Let \u00b5 be a \ufb01nite Borel measure on Rd.\nThe Fourier transform of \u00b5 is given by\nb\u00b5(\u03c9) =\nZ\nRd e\u2212i\u03c9T x d\u00b5(x), \u03c9 \u2208Rd,\n(72)\nwhich is a bounded, uniformly continuous function on Rd.\nIn addition, b\u00b5 satis\ufb01es the\nfollowing properties:\n(i) b\u00b5(\u03c9) = b\u00b5(\u2212\u03c9), \u2200\u03c9 \u2208Rd, i.e., b\u00b5 is conjugate symmetric,\n(ii) b\u00b5(0) = 1.\nThe following result, called the Riemann-Lebesgue lemma, is quoted from Rudin (1991,\nTheorem 7.5).\nLemma 28 (Riemann-Lebesgue) If f \u2208L1(Rd), then bf \u2208C0(Rd), and \u2225bf\u2225\u221e\u2264\u2225f\u22251.\nThe following theorem is a version of the Paley-Wiener theorem for distributions, and is\nproved in Rudin (1991, Theorem 7.23).\nTheorem 29 (Paley-Wiener) If f \u2208D\u2032\nd has compact support , then bf is entire.\nThe following lemma provides a property of entire functions, which is quoted from Rudin\n(1991, Lemma 7.21).\nLemma 30 If f is an entire function in Cd that vanishes on Rd, then f = 0.\nReferences\nS. M. Ali and S. D. Silvey. A general class of coe\ufb03cients of divergence of one distribution\nfrom another. Journal of the Royal Statistical Society, Series B (Methodological), 28:\n131\u2013142, 1966.\nN. Aronszajn. Theory of reproducing kernels. Trans. Amer. Math. Soc., 68:337\u2013404, 1950.\nF. R. Bach and M. I. Jordan. Kernel independent component analysis. Journal of Machine\nLearning Research, 3:1\u201348, 2002.\nA. D. Barbour and L. H. Y. Chen. An Introduction to Stein\u2019s Method. Singapore University\nPress, Singapore, 2005.\nC. Berg, J. P. R. Christensen, and P. Ressel. Harmonic Analysis on Semigroups. Spring\nVerlag, New York, 1984.\n44\nHilbert Space Embedding and Characteristic Kernels\nA. Berlinet and C. Thomas-Agnan. Reproducing Kernel Hilbert Spaces in Probability and\nStatistics. Kluwer Academic Publishers, London, UK, 2004.\nK. M. Borgwardt, A. Gretton, M. Rasch, H.-P. Kriegel, B. Sch\u00a8olkopf, and A. J. Smola. Inte-\ngrating structured biological data by kernel maximum mean discrepancy. Bioinformatics,\n22(14):e49\u2013e57, 2006.\nP. Br\u00b4emaud. Mathematical Principles of Signal Processing. Springer-Verlag, New York,\n2001.\nI. Csisz\u00b4ar. Information-type measures of di\ufb00erence of probability distributions and indirect\nobservations. Studia Scientiarium Mathematicarum Hungarica, 2:299\u2013318, 1967.\nW. Dahmen and C. A. Micchelli. Some remarks on ridge functions. Approx. Theory Appl.,\n3:139\u2013143, 1987.\nE. del Barrio, J. A. Cuesta-Albertos, C. Matr\u00b4an, and J. M. Rodr\u00b4\u0131guez-Rodr\u00b4\u0131guez. Testing of\ngoodness of \ufb01t based on the L2-Wasserstein distance. Annals of Statistics, 27:1230\u20131239,\n1999.\nL. Devroye and L. Gy\u00a8or\ufb01.\nNo empirical probability measure can converge in the total\nvariation sense for all distributions. Annals of Statistics, 18(3):1496\u20131499, 1990.\nR. M. Dudley. Real Analysis and Probability. Cambridge University Press, Cambridge, UK,\n2002.\nG. B. Folland.\nReal Analysis:\nModern Techniques and Their Applications.\nWiley-\nInterscience, New York, 1999.\nB. Fuglede and F. Tops\u00f8e. Jensen-Shannon divergence and Hilbert space embedding, 2003.\nPreprint.\nK. Fukumizu, F. R. Bach, and M. I. Jordan.\nDimensionality reduction for supervised\nlearning with reproducing kernel Hilbert spaces. Journal of Machine Learning Research,\n5:73\u201399, 2004.\nK. Fukumizu, A. Gretton, X. Sun, and B. Sch\u00a8olkopf. Kernel measures of conditional de-\npendence. In J.C. Platt, D. Koller, Y. Singer, and S. Roweis, editors, Advances in Neural\nInformation Processing Systems 20, pages 489\u2013496, Cambridge, MA, 2008. MIT Press.\nK. Fukumizu, F. R. Bach, and M. I. Jordan. Kernel dimension reduction in regression.\nAnnals of Statistics, 37(5):1871\u20131905, 2009a.\nK. Fukumizu, B. K. Sriperumbudur, A. Gretton, and B. Sch\u00a8olkopf. Characteristic kernels\non groups and semigroups. In D. Koller, D. Schuurmans, Y. Bengio, and L. Bottou,\neditors, Advances in Neural Information Processing Systems 21, pages 473\u2013480, 2009b.\nC. Gasquet and P. Witomski. Fourier Analysis and Applications. Springer-Verlag, New\nYork, 1999.\n45\nSriperumbudur, Gretton, Fukumizu, Sch\u00a8olkopf and Lanckriet\nA. L. Gibbs and F. E. Su. On choosing and bounding probability metrics. International\nStatistical Review, 70(3):419\u2013435, 2002.\nA. Gretton, A. Smola, O. Bousquet, R. Herbrich, B. Sch\u00a8olkopf, and N. Logothetis. Be-\nhaviour and convergence of the constrained covariance. Technical Report 130, MPI for\nBiological Cybernetics, 2004.\nA. Gretton, R. Herbrich, A. Smola, O. Bousquet, and B. Sch\u00a8olkopf. Kernel methods for\nmeasuring independence. Journal of Machine Learning Research, 6:2075\u20132129, December\n2005.\nA. Gretton, K. M. Borgwardt, M. Rasch, B. Sch\u00a8olkopf, and A. Smola. A kernel method for\nthe two sample problem. In B. Sch\u00a8olkopf, J. Platt, and T. Ho\ufb00man, editors, Advances in\nNeural Information Processing Systems 19, pages 513\u2013520. MIT Press, 2007.\nA. Gretton, K. Fukumizu, C. H. Teo, L. Song, B. Sch\u00a8olkopf, and A. J. Smola. A kernel\nstatistical test of independence. In J. Platt, D. Koller, Y. Singer, and S. Roweis, editors,\nAdvances in Neural Information Processing Systems 20, pages 585\u2013592. MIT Press, 2008.\nM. Hein and O. Bousquet. Hilbertian metrics and positive de\ufb01nite kernels on probability\nmeasures. In AISTATS, 2005.\nE. L. Lehmann and J. P. Romano. Testing Statistical Hypothesis. Springer-Verlag, New\nYork, 2005.\nF. Liese and I. Vajda. On divergences and informations in statistics and information theory.\nIEEE Trans. Information Theory, 52(10):4394\u20134412, 2006.\nT. Lindvall. Lectures on the Coupling Method. John Wiley & Sons, New York, 1992.\nC. A. Micchelli, Y. Xu, and H. Zhang. Universal kernels. Journal of Machine Learning\nResearch, 7:2651\u20132667, 2006.\nA. M\u00a8uller. Integral probability metrics and their generating classes of functions. Advances\nin Applied Probability, 29:429\u2013443, 1997.\nX. Nguyen, M. J. Wainwright, and M. I. Jordan. Estimating divergence functionals and\nthe likelihood ratio by convex risk minimization. Technical Report 764, Department of\nStatistics, University of California, Berkeley, 2008.\nA. Pinkus. Strictly positive de\ufb01nite functions on a real inner product space. Adv. Comput.\nMath., 20:263\u2013271, 2004.\nS. T. Rachev. Probability Metrics and the Stability of Stochastic Models. John Wiley &\nSons, Chichester, 1991.\nS. T. Rachev and L. R\u00a8uschendorf. Mass transportation problems. Vol. I Theory, Vol. II\nApplications. Probability and its Applications. Springer-Verlag, Berlin, 1998.\nC. E. Rasmussen and C. K. I. Williams. Gaussian Processes for Machine Learning. MIT\nPress, Cambridge, MA, 2006.\n46\nHilbert Space Embedding and Characteristic Kernels\nM. Reed and B. Simon. Functional Analysis. Academic Press, New York, 1972.\nM. Rosenblatt. A quadratic measure of deviation of two-dimensional density estimates and\na test of independence. Annals of Statistics, 3(1):1\u201314, 1975.\nW. Rudin. Functional Analysis. McGraw-Hill, USA, 1991.\nB. Sch\u00a8olkopf and A. J. Smola. Learning with Kernels. MIT Press, Cambridge, MA, 2002.\nG. R. Shorack. Probability for Statisticians. Springer-Verlag, New York, 2000.\nA. J. Smola, A. Gretton, L. Song, and B. Sch\u00a8olkopf. A Hilbert space embedding for distri-\nbutions. In Proc. 18th International Conference on Algorithmic Learning Theory, pages\n13\u201331. Springer-Verlag, Berlin, Germany, 2007.\nB. K. Sriperumbudur, A. Gretton, K. Fukumizu, G. R. G. Lanckriet, and B. Sch\u00a8olkopf.\nInjective Hilbert space embeddings of probability measures. In R. Servedio and T. Zhang,\neditors, Proc. of the 21st Annual Conference on Learning Theory, pages 111\u2013122, 2008.\nB. K. Sriperumbudur, K. Fukumizu, A. Gretton, G. R. G. Lanckriet, and B. Sch\u00a8olkopf.\nKernel choice and classi\ufb01ability for RKHS embeddings of probability distributions. In\nY. Bengio, D. Schuurmans, J. La\ufb00erty, C. K. I. Williams, and A. Culotta, editors, Ad-\nvances in Neural Information Processing Systems 22, pages 1750\u20131758. MIT Press, 2009a.\nB.\nK.\nSriperumbudur,\nK.\nFukumizu,\nA.\nGretton,\nB.\nSch\u00a8olkopf,\nand G.\nR. G.\nLanckriet.\nOn integral probability metrics, \u03c6-divergences and binary classi\ufb01cation.\nhttp://arxiv.org/abs/0901.2698v4, October 2009b.\nB. K. Sriperumbudur, K. Fukumizu, and G. R. G. Lanckriet.\nOn the relation between\nuniversality, characteristic kernels and RKHS embedding of measures. 2010a. Submitted\nto AISTATS.\nB. K. Sriperumbudur, K. Fukumizu, and G. R. G. Lanckriet. Universality, characteristic\nkernels and RKHS embedding of measures. 2010b. In preparation.\nC. Stein. A bound for the error in the normal approximation to the distribution of a sum of\ndependent random variables. In Proc. of the Sixth Berkeley Symposium on Mathematical\nStatistics and Probability, 1972.\nI. Steinwart. On the in\ufb02uence of the kernel on the consistency of support vector machines.\nJournal of Machine Learning Research, 2:67\u201393, 2001.\nI. Steinwart and A. Christmann. Support Vector Machines. Springer, 2008.\nJ. Stewart.\nPositive de\ufb01nite functions and generalizations, an historical survey.\nRocky\nMountain Journal of Mathematics, 6(3):409\u2013433, 1976.\nI. Vajda. Theory of Statistical Inference and Information. Kluwer Academic Publishers,\nBoston, 1989.\n47\nSriperumbudur, Gretton, Fukumizu, Sch\u00a8olkopf and Lanckriet\nS. S. Vallander. Calculation of the Wasserstein distance between probability distributions\non the line. Theory Probab. Appl., 18:784\u2013786, 1973.\nA. W. van der Vaart and J. A. Wellner.\nWeak Convergence and Empirical Processes.\nSpringer-Verlag, New York, 1996.\nV. N. Vapnik. Statistical Learning Theory. Wiley, New York, 1998.\nQ. Wang, S. R. Kulkarni, and S. Verd\u00b4u. Divergence estimation of continuous distributions\nbased on data-dependent partitions. IEEE Trans. Information Theory, 51(9):3064\u20133074,\n2005.\nN. Weaver. Lipschitz Algebras. World Scienti\ufb01c Publishing Company, 1999.\nH. Wendland. Scattered Data Approximation.\nCambridge University Press, Cambridge,\nUK, 2005.\n48\n",
        "sentence": "",
        "context": "UK, 2005.\n48\npendence. In J.C. Platt, D. Koller, Y. Singer, and S. Roweis, editors, Advances in Neural\nInformation Processing Systems 20, pages 489\u2013496, Cambridge, MA, 2008. MIT Press.\na project scientist at CMU, under grants DARPA IPTO FA8750-09-1-0141, ONR MURI\nN000140710747, and ARO MURI W911NF0810242. This work is also supported by the\nIST Program of the EC, under the FP7 Network of Excellence, ICT-216886-NOE. B. K. S."
    },
    {
        "title": "A bound for the error in the normal approximation to the distribution of a sum of dependent random variables",
        "author": [
            "C. Stein"
        ],
        "venue": "In Proc. 6th Berkeley Symposium on Mathematical Statistics and Probability (Univ. California, Berkeley,",
        "citeRegEx": "Stein,? \\Q1971\\E",
        "shortCiteRegEx": "Stein",
        "year": 1971,
        "abstract": "",
        "full_text": "",
        "sentence": " For any such Stein operator and Stein set G, Gorham & Mackey (2015) defined the Stein discrepancy as Comparing discrepancies Our first, simple experiment is designed to illustrate several properties of the IMQ KSD and to compare its behavior with that of two preexisting discrepancy measures, the Wasserstein distance dW\u2016\u00b7\u20162 , which can be computed for simple univariate targets (Vallender, 1974), and the spanner graph Stein discrepancy of Gorham & Mackey (2015). We adopt a bimodal Gaussian mixture with p(x) \u221d e\u2212 1 2\u2016x+\u2206e1\u2016 2 2 + e\u2212 1 2\u2016x\u2212\u2206e1\u2016 2 2 and \u2206 = 1. Comparing discrepancies Our first, simple experiment is designed to illustrate several properties of the IMQ KSD and to compare its behavior with that of two preexisting discrepancy measures, the Wasserstein distance dW\u2016\u00b7\u20162 , which can be computed for simple univariate targets (Vallender, 1974), and the spanner graph Stein discrepancy of Gorham & Mackey (2015). We adopt a bimodal Gaussian mixture with p(x) \u221d e\u2212 1 2\u2016x+\u2206e1\u2016 2 2 + e\u2212 1 2\u2016x\u2212\u2206e1\u2016 2 2 and \u2206 = 1.5 as our target P and generate a first sample point sequence i.i.d. from the target and a second sequence i.i.d. from one component of the mixture, N (\u2212\u2206e1, Id). As seen in the left panel of Figure 1 where d = 1, the IMQ KSD decays at an n\u22120.51 rate when applied to the first n points in the target sample and remains bounded away from zero when applied to the to the single component sample. This desirable behavior is closely mirrored by the Wasserstein distance and the graph Stein discrepancy. The middle panel of Figure 1 records the time consumed by the graph and kernel Stein discrepancies applied to the i.i.d. sample points from P . Each method is given access to d cores when working in d dimensions, and we use the released code of Gorham & Mackey (2015) with the default Gurobi 6.",
        "context": null
    },
    {
        "title": "Use of exchangeable pairs in the analysis of simulations. In Stein\u2019s method: expository lectures and applications, volume 46 of IMS Lecture Notes Monogr",
        "author": [
            "C. Stein",
            "P. Diaconis",
            "S. Holmes",
            "G. Reinert"
        ],
        "venue": "Ser., pp. 1\u201326. Inst. Math. Statist., Beachwood,",
        "citeRegEx": "Stein et al\\.,? \\Q2004\\E",
        "shortCiteRegEx": "Stein et al\\.",
        "year": 2004,
        "abstract": "",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "Positive definite functions and generalizations, an historical survey",
        "author": [
            "J. Stewart"
        ],
        "venue": "Rocky Mountain J. Math.,",
        "citeRegEx": "Stewart,? \\Q1976\\E",
        "shortCiteRegEx": "Stewart",
        "year": 1976,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "Calculation of the Wasserstein distance between probability distributions on the line",
        "author": [
            "S. Vallender"
        ],
        "venue": "Theory Probab. Appl.,",
        "citeRegEx": "Vallender,? \\Q1974\\E",
        "shortCiteRegEx": "Vallender",
        "year": 1974,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " Comparing discrepancies Our first, simple experiment is designed to illustrate several properties of the IMQ KSD and to compare its behavior with that of two preexisting discrepancy measures, the Wasserstein distance dW\u2016\u00b7\u20162 , which can be computed for simple univariate targets (Vallender, 1974), and the spanner graph Stein discrepancy of Gorham & Mackey (2015).",
        "context": null
    },
    {
        "title": "High-dimensional statistics: A non-asymptotic viewpoint. 2017. URL http: //www.stat.berkeley.edu/ \u0303wainwrig/ nachdiplom/Chap5_Sep10_2015.pdf",
        "author": [
            "M. Wainwright"
        ],
        "venue": null,
        "citeRegEx": "Wainwright,? \\Q2015\\E",
        "shortCiteRegEx": "Wainwright",
        "year": 2015,
        "abstract": "",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "Learning to Draw Samples: With Application to Amortized MLE for Generative Adversarial Learning",
        "author": [
            "D. Wang",
            "Q. Liu"
        ],
        "venue": null,
        "citeRegEx": "Wang and Liu,? \\Q2016\\E",
        "shortCiteRegEx": "Wang and Liu",
        "year": 2016,
        "abstract": "We propose a simple algorithm to train stochastic neural networks to draw\nsamples from given target distributions for probabilistic inference. Our method\nis based on iteratively adjusting the neural network parameters so that the\noutput changes along a Stein variational gradient that maximumly decreases the\nKL divergence with the target distribution. Our method works for any target\ndistribution specified by their unnormalized density function, and can train\nany black-box architectures that are differentiable in terms of the parameters\nwe want to adapt. As an application of our method, we propose an amortized MLE\nalgorithm for training deep energy model, where a neural sampler is adaptively\ntrained to approximate the likelihood function. Our method mimics an\nadversarial game between the deep energy model and the neural sampler, and\nobtains realistic-looking images competitive with the state-of-the-art results.",
        "full_text": "Under review as a conference paper at ICLR 2017\nLEARNING TO DRAW SAMPLES: WITH APPLICATION\nTO AMORTIZED MLE FOR GENERATIVE ADVERSAR-\nIAL LEARNING\nDilin Wang, Qiang Liu\nDepartment of Computer Science, Dartmouth College\n{dilin.wang.gr, qiang.liu}@dartmouth.edu\nABSTRACT\nWe propose a simple algorithm to train stochastic neural networks to draw samples\nfrom given target distributions for probabilistic inference. Our method is based\non iteratively adjusting the neural network parameters so that the output changes\nalong a Stein variational gradient (Liu & Wang, 2016) that maximumly decreases\nthe KL divergence with the target distribution. Our method works for any target\ndistribution speci\ufb01ed by their unnormalized density function, and can train any\nblack-box architectures that are differentiable in terms of the parameters we want to\nadapt. As an application of our method, we propose an amortized MLE algorithm\nfor training deep energy model, where a neural sampler is adaptively trained to\napproximate the likelihood function. Our method mimics an adversarial game\nbetween the deep energy model and the neural sampler, and obtains realistic-\nlooking images competitive with the state-of-the-art results.\n1\nINTRODUCTION\nModern machine learning increasingly relies on highly complex probabilistic models to reason\nabout uncertainty. A key computational challenge is to develop ef\ufb01cient inference techniques to\napproximate, or draw samples from complex distributions. Currently, most inference methods,\nincluding MCMC and variational inference, are hand-designed by researchers or domain experts.\nThis makes it dif\ufb01cult to fully optimize the choice of different methods and their parameters, and\nexploit the structures in the problems of interest in an automatic way. The hand-designed algorithm\ncan also be inef\ufb01cient when it requires to make fast inference repeatedly on a large number of different\ndistributions with similar structures. This happens, for example, when we need to reason about a\nnumber of observed datasets in settings like online learning, or need fast inference as inner loops for\nother algorithms such as maximum likelihood training. Therefore, it is highly desirable to develop\nmore intelligent probabilistic inference systems that can adaptively improve its own performance to\nfully the optimize computational ef\ufb01ciency, and generalize to new tasks with similar structures.\nSpeci\ufb01cally, denote by p(x) a probability density of interest speci\ufb01ed up to the normalization constant,\nwhich we want to draw sample from, or marginalize to estimate its normalization constant. We want\nto study the following problem:\nProblem 1. Given a distribution with density p(x) and a function f(\u03b7; \u03be) with parameter \u03b7 and\nrandom input \u03be, for which we only have assess to draws of the random input \u03be (without knowing its\ntrue distribution q0), and the output values of f(\u03b7; \u03be) and its derivative \u2202\u03b7f(\u03b7; \u03be) given \u03b7 and \u03be. We\nwant to \ufb01nd an optimal parameter \u03b7 so that the density of the random output variable x = f(\u03b7; \u03be)\nwith \u03be \u223cq0 closely matches the target density p(x).\nBecause we have no assumption on the structure of f(\u03b7; \u03be) and the distribution of random input,\nwe can not directly calculate the actual distribution of the output random variable x = f(\u03b7; \u03be); this\n1\narXiv:1611.01722v2  [stat.ML]  26 Nov 2016\nUnder review as a conference paper at ICLR 2017\nmakes it dif\ufb01cult to solve Problem 1 using the traditional variational inference (VI) methods. Recall\nthat traditional VI approximates p(x) using simple proposal distributions q\u03b7(x) indexed by parameter\n\u03b7, and \ufb01nds the optimal \u03b7 by minimizing KL divergence KL(q\u03b7 || p) = Eq\u03b7[log(q\u03b7/p)], which\nrequires to calculate the density q\u03b7(x) or its derivative that is not computable by our assumption\n(even when the Monte Carlo gradient estimation and the reparametrization trick (Kingma & Welling,\n2013) are applied).\nIn fact, it is this requirement of calculating q\u03b7(x) that has been the major constraint for the designing of\nstate-of-the-art variational inference methods with rich approximation families; the recent successful\nalgorithms (e.g., Rezende & Mohamed, 2015b; Tran et al., 2015; Ranganath et al., 2015, to name only\na few) have to handcraft special variational families to ensure the computational tractability of q\u03b7(x)\nand simultaneously obtain high approximation accuracy, which require substantial mathematical\ninsights and research effects. Methods that do not require to explicitly calculate q\u03b7(x) can signi\ufb01cantly\nsimplify the design and applications of VI methods, allowing practical users to focus more on choosing\nproposals that work best with their speci\ufb01c tasks. We will use the term wild variational inference\nto refer to new variants of variational methods that require no tractability q\u03b7(x), to distinguish with\nthe black-box variational inference (Ranganath et al., 2014) which refers to methods that work for\ngeneric target distributions p(x) without signi\ufb01cant model-by-model consideration (but still require\nto calculate the proposal density q\u03b7(x)).\nA similar problem also appears in importance sampling (IS), where it requires to calculate the IS\nproposal density q(x) in order to calculate the importance weight w(x) = p(x)/q(x). However, there\nexist methods that use no explicit information of q(x), which, seemingly counter-intuitively, give\nbetter asymptotic variance or converge rates than the typical IS that uses the proposal information\n(e.g., Liu & Lee, 2016; Briol et al., 2015; Henmi et al., 2007; Delyon & Portier, 2014). Discussions\non this phenomenon dates back to O\u2019Hagan (1987), who argued that \u201cMonte Carlo (that uses\nthe proposal information) is fundamentally unsound\u201d for violating the Likelihood Principle, and\ndeveloped Bayesian Monte Carlo (O\u2019Hagan, 1991) as an example that uses no information on q(x),\nyet gives better convergence rate than the typical Monte Carlo O(n\u22121/2) rate (Briol et al., 2015).\nDespite the substantial difference between IS and VI, these results intuitively suggest the possibility\nof developing ef\ufb01cient variational inference without calculating q(x) explicitly.\nIn this work, we propose a simple algorithm for Problem 1 by iteratively adjusting the network\nparameter \u03b7 to make its output random variable changes along a Stein variational gradient direction\n(SVGD) (Liu & Wang, 2016) that optimally decreases its KL divergence with the target distribution.\nCritically, the SVGD gradient includes a repulsive term to ensure that the generated samples have\nthe right amount of variability that matches p(x). In this way, we \u201camortize SVGD\u201d using a neural\nnetwork, which makes it possible for our method to adaptively improve its own ef\ufb01ciency by\nleveraging fast experience, especially in cases when it needs to perform fast inference repeatedly\non a large number of similar tasks. As an application, we use our method to amortize the MLE\ntraining of deep energy models, where a neural sampler is adaptively trained to approximate the\nlikelihood function. Our method, which we call SteinGAN, mimics an adversarial game between\nthe energy model and the neural sampler, and obtains realistic-looking images competitive with the\nstate-of-the-art results produced by generative adversarial networks (GAN) (Goodfellow et al., 2014;\nRadford et al., 2015).\nRelated Work\nThe idea of amortized inference (Gershman & Goodman, 2014) has been recently\napplied in various domains of probabilistic reasoning, including both amortized variational infer-\nence (e.g., Kingma & Welling, 2013; Rezende & Mohamed, 2015a), and data-driven proposals for\n(sequential) Monte Carlo methods (e.g., Paige & Wood, 2016), to name only a few. Most of these\nmethods, however, require to explicitly calculate q(x) (or its gradient). One exception is a very recent\npaper (Ranganath et al., 2016) that avoids calculating q(x) using an idea related to Stein discrepancy\n(Gorham & Mackey, 2015; Liu et al., 2016; Oates et al., 2014; Chwialkowski et al., 2016). There is\nalso a raising interest recently on a similar problem of \u201clearning to optimize\u201d (e.g., Andrychowicz\net al., 2016; Daniel et al., 2016; Li & Malik, 2016), which is technically easier than the more general\n2\nUnder review as a conference paper at ICLR 2017\nproblem of \u201clearning to sample\u201d. In fact, we show that our algorithm reduces to \u201clearning to optimize\u201d\nwhen only one particle is used in SVGD.\nGenerative adversarial network (GAN) and its variants have recently gained remarkable success on\ngenerating realistic-looking images (Goodfellow et al., 2014; Salimans et al., 2016; Radford et al.,\n2015; Li et al., 2015; Dziugaite et al., 2015; Nowozin et al., 2016). All these methods are set up to\ntrain latent variable models (the generator) under the assistant of the discriminator. Our SteinGAN\ninstead performs traditional MLE training for a deep energy model, with the help of a neural sampler\nthat learns to draw samples from the energy model to approximate the likelihood function; this admits\nan adversarial interpretation: we can view the neural sampler as a generator that attends to fool the\ndeep energy model, which in turn serves as a discriminator that distinguishes the real samples and\nthe simulated samples given by the neural sampler. This idea of training MLE with neural samplers\nwas \ufb01rst discussed by Kim & Bengio (2016); one of the key differences is that the neural sampler\nin Kim & Bengio (2016) is trained with the help of a heuristic diversity regularizer based on batch\nnormalization, while SVGD enforces the diversity in a more principled way. Another method by\nZhao et al. (2016) also trains an energy score to distinguish real and simulated samples, but within a\nnon-probabilistic framework (see Section 5 for more discussion). Other more traditional approaches\nfor training energy-based models (e.g., Ngiam et al., 2011; Xie et al., 2016) are often based on\nvariants of MCMC-MLE or contrastive divergence (Geyer, 1991; Hinton, 2002; Tieleman, 2008),\nand have dif\ufb01culty generating realistic-looking images from scratch.\n2\nSTEIN VARIATIONAL GRADIENT DESCENT (SVGD)\nStein variational gradient descent (SVGD) (Liu & Wang, 2016) is a general purpose Bayesian\ninference algorithm motivated by Stein\u2019s method (Stein, 1972; Barbour & Chen, 2005) and kernelized\nStein discrepancy (Liu et al., 2016; Chwialkowski et al., 2016; Oates et al., 2014). It uses an ef\ufb01cient\ndeterministic gradient-based update to iteratively evolve a set of particles {xi}n\ni=1 to minimize the\nKL divergence with the target distribution. SVGD has a simple form that reduces to the typical\ngradient descent for maximizing log p when using only one particle (n = 1), and hence can be easily\ncombined with the successful tricks for gradient optimization, including stochastic gradient, adaptive\nlearning rates (such as adagrad), and momentum.\nTo give a quick overview of the main idea of SVGD, let p(x) be a positive density function on Rd\nwhich we want to approximate with a set of particles {xi}n\ni=1. SVGD initializes the particles by\nsampling from some simple distribution q0, and updates the particles iteratively by\nxi \u2190xi + \u03f5\u03c6(xi),\n\u2200i = 1, . . . , n,\n(1)\nwhere \u03f5 is a step size, and \u03c6(x) is a \u201cparticle gradient direction\u201d chosen to maximumly decrease the\nKL divergence between the distribution of particles and the target distribution, in the sense that\n\u03c6 = arg max\n\u03c6\u2208F\n\u001a\n\u2212d\nd\u03f5KL(q[\u03f5\u03c6] || p)\n\f\f\n\u03f5=0\n\u001b\n,\n(2)\nwhere q[\u03f5\u03c6] denotes the density of the updated particle x\u2032 = x + \u03f5\u03c6(x) when the density of the\noriginal particle x is q, and F is the set of perturbation directions that we optimize over. We choose\nF to be the unit ball of a vector-valued reproducing kernel Hilbert space (RKHS) Hd = H \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 H\nwith each H associating with a positive de\ufb01nite kernel k(x, x\u2032); note that H is dense in the space of\ncontinuous functions with universal kernels such as the Gaussian RBF kernel.\nCritically, the gradient of KL divergence in (2) equals a simple linear functional of \u03c6, allowing us to\nobtain a closed form solution for the optimal \u03c6. Liu & Wang (2016) showed that\n\u2212d\nd\u03f5KL(q[\u03f5\u03c6] || p)\n\f\f\n\u03f5=0 = Ex\u223cq[Tp\u03c6(x)],\n(3)\nwith\nTp\u03c6(x) = \u2207x log p(x)\u22a4\u03c6(x) + \u2207x \u00b7 \u03c6(x),\n(4)\n3\nUnder review as a conference paper at ICLR 2017\nAlgorithm 1 Amortized SVGD for Problem 1\nSet batch size m, step-size scheme {\u03f5t} and kernel k(x, x\u2032). Initialize \u03b70.\nfor iteration t do\nDraw random {\u03bei}m\ni=1, calculate xi = f(\u03b7t; \u03bei), and the Stein variational gradient \u2206xi in (7).\nUpdate parameter \u03b7 using (8), (9) or (10).\nend for\nwhere Tp is considered as a linear operator acting on function \u03c6 and is called the Stein operator in\nconnection with Stein\u2019s identity which shows that the RHS of (3) equals zero if p = q:\nEp[Tp\u03c6] = Ep[\u2207x log p\u22a4\u03c6 + \u2207x \u00b7 \u03c6] = 0.\n(5)\nThis is a result of integration by parts assuming the value of p(x)\u03c6(x) vanishes on the boundary of\nthe integration domain.\nTherefore, the optimization in (2) reduces to\nD(q || p)\ndef\n= max\n\u03c6\u2208Hd{Ex\u223cq[Tp\u03c6(x)] s.t.\n||\u03c6||Hd \u22641},\n(6)\nwhere D(q || p) is the kernelized Stein discrepancy de\ufb01ned in Liu et al. (2016), which equals zero if\nand only if p = q under mild regularity conditions. Importantly, the optimal solution of (6) yields a\nclosed form\n\u03c6\u2217(x\u2032) \u221dEx\u223cq[\u2207x log p(x)k(x, x\u2032) + \u2207xk(x, x\u2032)].\nBy approximating the expectation under q with the empirical average of the current particles {xi}n\ni=1,\nSVGD admits a simple form of update:\nxi \u2190xi + \u03f5\u2206xi,\n\u2200i = 1, . . . , n,\nwhere\n\u2206xi = \u02c6Ex\u2208{xi}n\ni=1[\u2207x log p(x)k(x, xi) + \u2207xk(x, xi)],\n(7)\nand \u02c6Ex\u223c{xi}n\ni=1[f(x)] = P\ni f(xi)/n. The two terms in \u2206xi play two different roles: the term with\nthe gradient \u2207x log p(x) drives the particles toward the high probability regions of p(x), while the\nterm with \u2207xk(x, xi) serves as a repulsive force to encourage diversity; to see this, consider a station-\nary kernel k(x, x\u2032) = k(x \u2212x\u2032), then the second term reduces to \u02c6Ex\u2207xk(x, xi) = \u2212\u02c6Ex\u2207xik(x, xi),\nwhich can be treated as the negative gradient for minimizing the average similarity \u02c6Exk(x, xi) in\nterms of xi. Overall, this particle update produces diverse points for distributional approximation and\nuncertainty assessment, and also has an interesting \u201cmomentum\u201d effect in which the particles move\ncollaboratively to escape the local optima.\nIt is easy to see from (7) that \u2206xi reduces to the typical gradient \u2207x log p(xi) when there is only a\nsingle particle (n = 1) and \u2207xk(x, xi) when x = xi, in which case SVGD reduces to the standard\ngradient ascent for maximizing log p(x) (i.e., maximum a posteriori (MAP)).\n3\nAMORTIZED SVGD: TOWARDS AN AUTOMATIC NEURAL SAMPLER\nSVGD and other particle-based methods become inef\ufb01cient when we need to repeatedly infer a large\nnumber different target distributions for multiple tasks, including online learning or inner loops of\nother algorithms, because they can not improve based on the experience from the past tasks, and may\nrequire a large memory to restore a large number of particles. We propose to \u201camortize SVGD\u201d by\ntraining a neural network f(\u03b7; \u03be) to mimic the SVGD dynamics, yielding a solution for Problem 1.\nOne straightforward way to achieve this is to run SVGD to convergence and train f(\u03b7; \u03be) to \ufb01t the\nSVGD results. This, however, requires to run many epochs of fully converged SVGD and can be\nslow in practice. We instead propose an incremental approach in which \u03b7 is iteratively adjusted so\n4\nUnder review as a conference paper at ICLR 2017\nthat the network outputs x = f(\u03b7; \u03be) changes along the Stein variational gradient direction in (7) in\norder to decrease the KL divergence between the target and approximation distribution.\nTo be speci\ufb01c, denote by \u03b7t the estimated parameter at the t-th iteration of our method; each iteration\nof our method draws a batch of random inputs {\u03bei}m\ni=1 and calculate their corresponding output\nxi = f(\u03b7; \u03bei) based on \u03b7t; here m is a mini-batch size (e.g., m = 100). The Stein variational\ngradient \u2206xi in (7) would then ensure that x\u2032\ni = xi + \u03f5\u2206xi forms a better approximation of the target\ndistribution p. Therefore, we should adjust \u03b7 to make its output matches {x\u2032\ni}, that is, we want to\nupdate \u03b7 by\n\u03b7t+1 \u2190arg min\n\u03b7\nm\nX\ni=1\n||f(\u03b7; \u03bei) \u2212x\u2032\ni||2\n2,\nwhere\nx\u2032\ni = xi + \u03f5\u2206xi.\n(8)\nSee Algorithm 1 for the summary of this procedure. If we assume \u03f5 is very small, then (8) reduces\nto a least square optimization. To see this, note that f(\u03b7; \u03bei) \u2248f(\u03b7t; \u03bei) + \u2202\u03b7f(\u03b7t; \u03bei)(\u03b7 \u2212\u03b7t) by\nTaylor expansion. Since xi = f(\u03b7t; \u03bei), we have\n||f(\u03b7; \u03bei) \u2212x\u2032\ni||2\n2 \u2248||\u2202\u03b7f(\u03b7t; \u03bei)(\u03b7 \u2212\u03b7t) \u2212\u03f5\u2206xi||2\n2.\nAs a result, (8) reduces to the following least square optimization:\n\u03b7t+1 \u2190\u03b7t + \u03f5\u2206\u03b7t,\nwhere\n\u2206\u03b7t = arg min\n\u03b4\nm\nX\ni=1\n||\u2202\u03b7f(\u03b7t; \u03bei)\u03b4 \u2212\u2206xi||2\n2.\n(9)\nUpdate (9) can still be computationally expensive because of the matrix inversion. We can derive a\nfurther approximation by performing only one step of gradient descent of (8) (or (9)), which gives\n\u03b7t+1 \u2190\u03b7t + \u03f5\nm\nX\ni=1\n\u2202\u03b7f(\u03b7t; \u03bei)\u2206xi.\n(10)\nAlthough update (10) is derived as an approximation of (8)-(9), it is computationally faster and we\n\ufb01nd it works very effectively in practice; this is because when \u03f5 is small, one step of gradient update\ncan be suf\ufb01ciently close to the optimum.\nUpdate (10) also has a simple and intuitive form: (10) can be thought as a \u201cchain rule\u201d that back-\npropagates the Stein variational gradient to the network parameter \u03b7. This can be justi\ufb01ed by\nconsidering the special case when we use only a single particle (n = 1) in which case \u2206xi in\n(7) reduces to the typical gradient \u2207x log p(xi) of log p(x), and update (10) reduces to the typical\ngradient ascent for maximizing\nE\u03be[log p(f(\u03b7; \u03be))],\nin which case f(\u03b7; \u03be) is trained to maximize log p(x) (that is, learning to optimize), instead of\nlearning to draw samples from p for which it is crucial to use Stein variational gradient \u2206xi to\ndiversify the network outputs.\nUpdate (10) also has a close connection with the typical variational inference with the reparameteri-\nzation trick (Kingma & Welling, 2013). Let q\u03b7(x) be the density function of x = f(\u03b7; \u03be), \u03be \u223cq0.\nUsing the reparameterization trick, the gradient of KL(q\u03b7 || p) w.r.t. \u03b7 can be shown to be\n\u2207\u03b7KL(q\u03b7 || p) = \u2212E\u03be\u223cq0[\u2202\u03b7f(\u03b7; \u03be)(\u2207x log p(x) \u2212\u2207x log q\u03b7(x))].\nWith {\u03bei} i.i.d. drawn from q0 and xi = f(\u03b7; \u03bei), \u2200i, the standard stochastic gradient descent for\nminimizing the KL divergence is\n\u03b7t+1 \u2190\u03b7t +\nX\ni\n\u2202\u03b7f(\u03b7t; \u03bei) \u02dc\u2206xi,\nwhere\n\u02dc\u2206xi = \u2207x log p(xi) \u2212\u2207x log q\u03b7(xi).\n(11)\n5\nUnder review as a conference paper at ICLR 2017\nThis is similar with (10), but replaces the Stein gradient \u2206xi de\ufb01ned in (7) with \u02dc\u2206xi. The advantage\nof using \u2206xi is that it does not require to explicitly calculate q\u03b7, and hence admits a solution to\nProblem 1 in which q\u03b7 is not computable for complex network f(\u03b7; \u03be) and unknown input distribution\nq0. Further insights can be obtained by noting that\n\u2206xi \u2248Ex\u223cq[\u2207x log p(x)k(x, xi) + \u2207xk(x, xi)]\n= Ex\u223cq[(\u2207x log p(x) \u2212\u2207x log q(x))k(x, xi)]\n(12)\n= Ex\u223cq[( \u02dc\u2206x)k(x, xi)],\nwhere (12) is obtained by using Stein\u2019s identity (5). Therefore, \u2206xi can be treated as a kernel\nsmoothed version of \u02dc\u2206xi.\n4\nAMORTIZED MLE FOR GENERATIVE ADVERSARIAL TRAINING\nOur method allows us to design ef\ufb01cient approximate sampling methods adaptively and automatically,\nand enables a host of novel applications. In this paper, we apply it in an amortized MLE method for\ntraining deep generative models.\nMaximum likelihood estimator (MLE) provides a fundamental approach for learning probabilistic\nmodels from data, but can be computationally prohibitive on distributions for which drawing samples\nor computing likelihood is intractable due to the normalization constant. Traditional methods such as\nMCMC-MLE use hand-designed methods (e.g., MCMC) to approximate the intractable likelihood\nfunction but do not work ef\ufb01ciently in practice. We propose to adaptively train a generative neural\nnetwork to draw samples from the distribution during MLE training, which not only provides\ncomputational advantage, and also allows us to generate realistic-looking images competitive with,\nor better than the state-of-the-art generative adversarial networks (GAN) (Goodfellow et al., 2014;\nRadford et al., 2015) (see Figure 1-5).\nTo be speci\ufb01c, denote by {xi,obs} a set of observed data. We consider the maximum likelihood\ntraining of energy-based models of form\np(x|\u03b8) = exp(\u2212\u03c6(x, \u03b8) \u2212\u03a6(\u03b8)),\n\u03a6(\u03b8) = log\nZ\nexp(\u2212\u03c6(x, \u03b8))dx,\nwhere \u03c6(x; \u03b8) is an energy function for x indexed by parameter \u03b8 and \u03a6(\u03b8) is the log-normalization\nconstant. The log-likelihood function of \u03b8 is\nL(\u03b8) = 1\nn\nn\nX\ni=1\nlog p(xi,obs|\u03b8),\nwhose gradient is\n\u2207\u03b8L(\u03b8) = \u2212\u02c6Eobs[\u2202\u03b8\u03c6(x; \u03b8)] + E\u03b8[\u2202\u03b8\u03c6(x; \u03b8)],\nwhere \u02c6Eobs[\u00b7] and E\u03b8[\u00b7] denote the empirical average on the observed data {xi,obs} and the expectation\nunder model p(x|\u03b8), respectively. The key computational dif\ufb01culty is to approximate the model\nexpectation E\u03b8[\u00b7]. To address this problem, we use a generative neural network x = f(\u03b7; \u03be) trained\nby Algorithm 1 to approximately sample from p(x|\u03b8), yielding a gradient update for \u03b8 of form\n\u03b8 \u2190\u03b8 + \u03f5 \u02c6\u2207\u03b8L(\u03b8),\n\u02c6\u2207\u03b8L(\u03b8) = \u2212\u02c6Eobs[\u2202\u03b8\u03c6(x; \u03b8)] + \u02c6E\u03b7[\u2202\u03b8\u03c6(x; \u03b8)],\n(13)\nwhere \u02c6E\u03b7 denotes the empirical average on {xi} where xi = f(\u03b7; \u03bei), {\u03bei} \u223cq0. As \u03b8 is updated by\ngradient ascent, \u03b7 is successively updated via Algorithm 1 to follow p(x|\u03b8). See Algorithm 2.\nWe call our method SteinGAN, because it can be intuitively interpreted as an adversarial game between\nthe generative network f(\u03b7; \u03be) and the energy model p(x|\u03b8) which serves as a discriminator: The\n6\nUnder review as a conference paper at ICLR 2017\nAlgorithm 2 Amortized MLE as Generative Adversarial Learning\nGoal: MLE training for energy model p(x|\u03b8) = exp(\u2212\u03c6(x, \u03b8) \u2212\u03a6(\u03b8)).\nInitialize \u03b7 and \u03b8.\nfor iteration t do\nUpdating \u03b7: Draw \u03bei \u223cq0, xi = f(\u03b7; \u03bei); update \u03b7 using (8), (9) or (10) with p(x) = p(x|\u03b8).\nRepeat several times when needed.\nUpdating \u03b8: Draw a mini-batch of observed data {xi,obs}, and simulated data xi = f(\u03b7; \u03bei),\nupdate \u03b8 by (13).\nend for\nMLE gradient update of p(x|\u03b8) effectively decreases the energy of the training data and increases\nthe energy of the simulated data from f(\u03b7; \u03be), while the SVGD update of f(\u03b7; \u03be) decreases the\nenergy of the simulated data to \ufb01t better with p(x|\u03b8). Compared with the traditional methods based\non MCMC-MLE or contrastive divergence, we amortize the sampler as we train, which gives much\nfaster speed and simultaneously provides a high quality generative neural network that can generate\nrealistic-looking images; see Kim & Bengio (2016) for a similar idea and discussions.\n5\nEMPIRICAL RESULTS\nWe evaluated our SteinGAN on four datasets, MNIST, CIFAR-10, CelebA (Liu et al., 2015), and\nLarge-scale Scene Understanding (LSUN) (Yu et al., 2015), on which we \ufb01nd our method tends\nto generate realistic-looking images competitive with, sometimes better than DCGAN (Radford\net al., 2015) (see Figure 2 - Figure 3). Our code is available at https://github.com/DartML/\nSteinGAN.\nModel Setup\nIn order to generate realistic-looking images, we de\ufb01ne our energy model based on\nan autoencoder:\np(x|\u03b8) \u221dexp(\u2212||x \u2212D(E(x; \u03b8); \u03b8)||),\n(14)\nwhere x denotes the image. This choice is motivated by Energy-based GAN (Zhao et al., 2016) in\nwhich the autoencoder loss is used as a discriminator but without a probabilistic interpretation. We\nassume f(\u03b7; \u03be) to be a neural network whose input \u03be is a 100-dimensional random vector drawn by\nUniform([\u22121, 1]). The positive de\ufb01nite kernel in SVGD is de\ufb01ned by the RBF kernel on the hidden\nrepresentation obtained by the autoencoder in (14), that is,\nk(x, x\u2032) = exp(\u22121\nh2 ||E(x; \u03b8) \u2212E(x\u2032; \u03b8)||2).\nAs it is discussed in Section 3, the kernel provides a repulsive force to produce an amount of variability\nrequired for generating samples from p(x). This is similar to the heuristic repelling regularizer in\nZhao et al. (2016) and the batch normalization based regularizer in Kim & Bengio (2016), but is\nderived in a more principled way. We take the bandwidth to be h = 0.5 \u00d7 med, where med is the\nmedian of the pairwise distances between E(x) on the image simulated by f(\u03b7; \u03be). This makes the\nkernel change adaptively based on both \u03b8 (through E(x; \u03b8)) and \u03b7 (through bandwidth h).\nSome datasets include both images x and their associated discrete labels y. In these cases, we train\na joint energy model on (x, y) to capture both the inner structure of the images and its predictive\nrelation with the label, allowing us to simulate images with a control on which category it belongs to.\nOur joint energy model is de\ufb01ned to be\np(x, y|\u03b8) \u221dexp\n\b\n\u2212||x \u2212D(E(x; \u03b8); \u03b8)|| \u2212max[m, \u03c3(y, E(x; \u03b8))]\n\t\n,\n(15)\nwhere \u03c3(\u00b7, \u00b7) is the cross entropy loss function of a fully connected output layer. In this case, our\nneural sampler \ufb01rst draws a label y randomly according to the empirical counts in the dataset, and\nthen passes y into a neural network together with a 100 \u00d7 1 random vector \u03be to generate image x.\nThis allows us to generate images for particular categories by controlling the value of input y.\n7\nUnder review as a conference paper at ICLR 2017\nStabilization\nIn practice, we \ufb01nd it is useful to modify (13) to be\n\u03b8 \u2190\u03b8 \u2212\u03f5\u02c6Eobs[\u2207\u03b8\u03c6(x, \u03b8)] + \u03f5(1 \u2212\u03b3)\u02c6E\u03b7[\u2207\u03b8\u03c6(x, \u03b8)].\n(16)\nwhere \u03b3 is a discount factor (which we take to be \u03b3 = 0.7). This is equivalent to maximizing a\nregularized likelihood:\nmax\n\u03b8 {log p(x|\u03b8) + \u03b3\u03a6(\u03b8)}\nwhere \u03a6(\u03b8) is the log-partition function; note that exp(\u03b3\u03a6(\u03b8)) is a conjugate prior of p(x|\u03b8).\nWe initialize the weights of both the generator and discriminator from Gaussian distribution\nN(0, 0.02), and train them using Adam (Kingma & Ba, 2014) with a learning rate of 0.001 for\nthe generator and 0.0001 for the energy model (the discriminator). In order to keep the generator\nand discriminator approximately aligned during training, we speed up the MLE update (16) of the\ndiscriminator (by increasing its learning rate to 0.0005) when the energy of the real data batch is\nlarger than the energy of the simulated images, while slow down it (by freezing the MLE update\nof \u03b8 in (16)) if the magnitude of the energy difference between the real images and the simulated\nimages goes above a threshold of 0.5. We used the bag of architecture guidelines for stable training\nsuggested in DCGAN (Radford et al., 2015).\nDiscussion\nThe MNIST dataset has a training set of 60, 000 examples. Both DCGAN and our\nmodel produce high quality images, both visually indistinguishable from real images; see \ufb01gure 1.\nCIFAR-10 is very diverse, and with only 50,000 training examples. Figure 2 shows examples of\nsimulated images by DCGAN and SteinGAN generated conditional on each category, which look\nequally well visually. We also provide quantitively evaluation using a recently proposed inception\nscore (Salimans et al., 2016), as well as the classi\ufb01cation accuracy when training ResNet using\n50, 000 simulated images as train sets, evaluated on a separate held-out testing set never seen by the\nGAN models. Besides DCGAN and SteinGAN, we also evaluate another simple baseline obtained by\nsubsampling 500 real images from the training set and duplicating them 100 times. We observe that\nthese scores capture rather different perspectives of image generation: The inception score favors\nimages that look realistic individually and have uniformly distributed labels; as a result, the inception\nscore of the duplicated 500 images is almost as high as the real training set. We \ufb01nd that the inception\nscore of SteinGAN is comparable, or slightly lower than that of DCGAN. On the other hand, the\nclassi\ufb01cation accuracy measures the amount information captured in the simulated image sets; we\n\ufb01nd that SteinGAN achieves the highest classi\ufb01cation accuracy, suggesting that it captures more\ninformation in the training set.\nFigure 3 and 4 visualize the results on CelebA (with more than 200k face images) and LSUN (with\nnearly 3M bedroom images), respectively. We cropped and resized both dataset images into 64 \u00d7 64.\nDCGAN\nSteinGAN\nFigure 1: MNIST images generated by DCGAN and our SteinGAN. We use the joint model in (15)\nto allow us to generate images for each digit. We set m = 0.2.\n8\nUnder review as a conference paper at ICLR 2017\nairplane\nautomobile\nbird\ncat\ndeer\ndog\nfrog\nhorse\nship\ntruck\nDCGAN\nSteinGAN\nInception Score\nReal Training Set\n500 Duplicate\nDCGAN\nSteinGAN\nModel Trained on ImageNet\n11.237\n11.100\n6.581\n6.351\nModel Trained on CIFAR-10\n9.848\n9.807\n7.368\n7.428\nTesting Accuracy\nReal Training Set\n500 Duplicate\nDCGAN\nSteinGAN\n92.58 %\n44.96 %\n44.78 %\n63.81 %\nFigure 2: Results on CIFAR-10. \u201c500 Duplicate\u201d denotes 500 images randomly subsampled from the\ntraining set, each duplicated 100 times. Upper: images simulated by DCGAN and SteinGAN (based\non joint model (15)) conditional on each category. Middle: inception scores for samples generated by\nvarious methods (all with 50,000 images) on inception models trained on ImageNet and CIFAR-10,\nrespectively. Lower: testing accuracy on real testing set when using 50,000 simulated images to train\nResNets for classi\ufb01cation. SteinGAN achieves higher testing accuracy than DCGAN. We set m = 1\nand \u03b3 = 0.8.\n6\nCONCLUSION\nWe propose a new method to train neural samplers for given distributions, together with a new\nSteinGAN method for generative adversarial training. Future directions involve more applications\nand theoretical understandings for training neural samplers.\n9\nUnder review as a conference paper at ICLR 2017\nDCGAN\nSteinGAN\nFigure 3: Results on CelebA. Upper: images generated by DCGAN and our SteinGAN. Lower:\nimages generated by SteinGAN when performing a random walk \u03be \u2190\u03be + 0.01 \u00d7 Uniform([\u22121, 1])\non the random input \u03be; we can see that a man with glasses and black hair gradually changes to a\nwoman with blonde hair. See Figure 5 for more examples.\nDCGAN\nSteinGAN\nFigure 4: Images generated by DCGAN and our SteinGAN on LSUN.\n10\nUnder review as a conference paper at ICLR 2017\nREFERENCES\nAndrychowicz, Marcin, Denil, Misha, Gomez, Sergio, Hoffman, Matthew W, Pfau, David, Schaul, Tom, and\nde Freitas, Nando. Learning to learn by gradient descent by gradient descent. arXiv preprint arXiv:1606.04474,\n2016.\nBarbour, Andrew D and Chen, Louis Hsiao Yun. An introduction to Stein\u2019s method, volume 4. World Scienti\ufb01c,\n2005.\nBriol, Franc\u00b8ois-Xavier, Oates, Chris, Girolami, Mark, Osborne, Michael A, Sejdinovic, Dino, et al. Probabilistic\nintegration: A role for statisticians in numerical analysis? arXiv preprint http://arxiv.org/abs/1512.00933,\n2015.\nChwialkowski, Kacper, Strathmann, Heiko, and Gretton, Arthur. A kernel test of goodness of \ufb01t. In Proceedings\nof the International Conference on Machine Learning (ICML), 2016.\nDaniel, Christian, Taylor, Jonathan, and Nowozin, Sebastian. Learning step size controllers for robust neural\nnetwork training. In Thirtieth AAAI Conference on Arti\ufb01cial Intelligence, 2016.\nDelyon, Bernard and Portier, Franc\u00b8ois.\nIntegral approximation by kernel smoothing.\narXiv preprint\narXiv:1409.0733, 2014.\nDziugaite, Gintare Karolina, Roy, Daniel M., and Ghahramani, Zoubin. Training generative neural networks\nvia maximum mean discrepancy optimization. In Conference on Uncertainty in Arti\ufb01cial Intelligence (UAI),\n2015.\nGershman, Samuel J and Goodman, Noah D. Amortized inference in probabilistic reasoning. In Proceedings of\nthe 36th Annual Conference of the Cognitive Science Society, 2014.\nGeyer, Charles J. Markov chain Monte Carlo maximum likelihood. In Computing Science and Statistics: Proc.\n23rd Symp. Interface, pp. 156\u2013163, 1991.\nGoodfellow, Ian, Pouget-Abadie, Jean, Mirza, Mehdi, Xu, Bing, Warde-Farley, David, Ozair, Sherjil, Courville,\nAaron, and Bengio, Yoshua. Generative adversarial nets. In Advances in Neural Information Processing\nSystems, pp. 2672\u20132680, 2014.\nGorham, Jack and Mackey, Lester. Measuring sample quality with Stein\u2019s method. In Advances in Neural\nInformation Processing Systems (NIPS), pp. 226\u2013234, 2015.\nHenmi, Masayuki, Yoshida, Ryo, and Eguchi, Shinto. Importance sampling via the estimated sampler. Biometrika,\n94(4):985\u2013991, 2007.\nHinton, Geoffrey E. Training products of experts by minimizing contrastive divergence. Neural computation, 14\n(8):1771\u20131800, 2002.\nKim, Taesup and Bengio, Yoshua. Deep directed generative models with energy-based probability estimation.\narXiv preprint arXiv:1606.03439, 2016.\nKingma, Diederik and Ba, Jimmy. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980,\n2014.\nKingma, Diederik P and Welling, Max. Auto-encoding variational Bayes. In Proceedings of the International\nConference on Learning Representations (ICLR), 2013.\nLi, Ke and Malik, Jitendra. Learning to optimize. arXiv preprint arXiv:1606.01885, 2016.\nLi, Yujia, Swersky, Kevin, and Zemel, Rich. Generative moment matching networks. In Proceedings of the\nInternational Conference on Machine Learning (ICML), 2015.\nLiu, Qiang and Lee, Jason D. Black-box importance sampling. https://arxiv.org/abs/1610.05247, 2016.\nLiu, Qiang and Wang, Dilin. Stein variational gradient descent: A general purpose bayesian inference algorithm.\narXiv preprint arXiv:1608.04471, 2016.\nLiu, Qiang, Lee, Jason D, and Jordan, Michael I. A kernelized Stein discrepancy for goodness-of-\ufb01t tests. In\nProceedings of the International Conference on Machine Learning (ICML), 2016.\n11\nUnder review as a conference paper at ICLR 2017\nLiu, Ziwei, Luo, Ping, Wang, Xiaogang, and Tang, Xiaoou. Deep learning face attributes in the wild. In\nProceedings of International Conference on Computer Vision (ICCV), 2015.\nNgiam, Jiquan, Chen, Zhenghao, Koh, Pang W, and Ng, Andrew Y. Learning deep energy models. In Proceedings\nof the International Conference on Machine Learning (ICML), pp. 1105\u20131112, 2011.\nNowozin, Sebastian, Cseke, Botond, and Tomioka, Ryota. f-gan: Training generative neural samplers using\nvariational divergence minimization. arXiv preprint arXiv:1606.00709, 2016.\nOates, Chris J, Girolami, Mark, and Chopin, Nicolas. Control functionals for Monte Carlo integration. Journal\nof the Royal Statistical Society, Series B, 2014.\nO\u2019Hagan, Anthony. Monte Carlo is fundamentally unsound. Journal of the Royal Statistical Society. Series D\n(The Statistician), 36(2/3):247\u2013249, 1987.\nO\u2019Hagan, Anthony. Bayes\u2013hermite quadrature. Journal of statistical planning and inference, 29(3):245\u2013260,\n1991.\nPaige, Brooks and Wood, Frank. Inference networks for sequential monte carlo in graphical models. arXiv\npreprint arXiv:1602.06701, 2016.\nRadford, Alec, Metz, Luke, and Chintala, Soumith. Unsupervised representation learning with deep convolutional\ngenerative adversarial networks. arXiv preprint arXiv:1511.06434, 2015.\nRanganath, R., Altosaar, J., Tran, D., and Blei, D.M. Operator variational inference. 2016.\nRanganath, Rajesh, Gerrish, Sean, and Blei, David M. Black box variational inference. In Proceedings of the\nInternational Conference on Arti\ufb01cial Intelligence and Statistics (AISTATS), 2014.\nRanganath, Rajesh, Tran, Dustin, and Blei, David M.\nHierarchical variational models.\narXiv preprint\narXiv:1511.02386, 2015.\nRezende, Danilo Jimenez and Mohamed, Shakir. Variational inference with normalizing \ufb02ows. In Proceedings\nof the International Conference on Machine Learning (ICML), 2015a.\nRezende, Danilo Jimenez and Mohamed, Shakir. Variational inference with normalizing \ufb02ows. arXiv preprint\narXiv:1505.05770, 2015b.\nSalimans, Tim, Goodfellow, Ian, Zaremba, Wojciech, Cheung, Vicki, Radford, Alec, and Chen, Xi. Improved\ntechniques for training gans. arXiv preprint arXiv:1606.03498, 2016.\nStein, Charles. A bound for the error in the normal approximation to the distribution of a sum of dependent\nrandom variables. In Proceedings of the Sixth Berkeley Symposium on Mathematical Statistics and Probability,\nVolume 2: Probability Theory, pp. 583\u2013602, 1972.\nTieleman, Tijmen. Training restricted boltzmann machines using approximations to the likelihood gradient. In\nProceedings of the 25th international conference on Machine learning, pp. 1064\u20131071. ACM, 2008.\nTran, Dustin, Ranganath, Rajesh, and Blei, David M.\nVariational gaussian process.\narXiv preprint\narXiv:1511.06499, 2015.\nXie, Jianwen, Lu, Yang, Zhu, Song-Chun, and Wu, Ying Nian. A theory of generative convnet. arXiv preprint\narXiv:1602.03264, 2016.\nYu, Fisher, Seff, Ari, Zhang, Yinda, Song, Shuran, Funkhouser, Thomas, and Xiao, Jianxiong. Lsun: Construction\nof a large-scale image dataset using deep learning with humans in the loop. arXiv preprint arXiv:1506.03365,\n2015.\nZhao, Junbo, Mathieu, Michael, and LeCun, Yann. Energy-based generative adversarial network. arXiv preprint\narXiv:1609.03126, 2016.\n12\nUnder review as a conference paper at ICLR 2017\nFigure 5: More images generated by SteinGAN on CelebA.\n13\n",
        "sentence": "",
        "context": "on the random input \u03be; we can see that a man with glasses and black hair gradually changes to a\nwoman with blonde hair. See Figure 5 for more examples.\nDCGAN\nSteinGAN\nFigure 4: Images generated by DCGAN and our SteinGAN on LSUN.\n10\n10\nUnder review as a conference paper at ICLR 2017\nREFERENCES\nAndrychowicz, Marcin, Denil, Misha, Gomez, Sergio, Hoffman, Matthew W, Pfau, David, Schaul, Tom, and\nLiu, Ziwei, Luo, Ping, Wang, Xiaogang, and Tang, Xiaoou. Deep learning face attributes in the wild. In\nProceedings of International Conference on Computer Vision (ICCV), 2015."
    },
    {
        "title": "Bayesian learning via stochastic gradient Langevin dynamics",
        "author": [
            "M. Welling",
            "Y. Teh"
        ],
        "venue": "In ICML,",
        "citeRegEx": "Welling and Teh,? \\Q2011\\E",
        "shortCiteRegEx": "Welling and Teh",
        "year": 2011,
        "abstract": "",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "Scattered data approximation, volume 17",
        "author": [
            "H. Wendland"
        ],
        "venue": "Cambridge university press,",
        "citeRegEx": "Wendland,? \\Q2004\\E",
        "shortCiteRegEx": "Wendland",
        "year": 2004,
        "abstract": "",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "Proof of Proposition 2: KSD closed form Our proof generalizes that of (Chwialkowski et al., 2016",
        "author": [
            "C. EP [(TP g)(Z"
        ],
        "venue": "Thm. 2.1). For each dimension j \u2208 {1,",
        "citeRegEx": "0.,? \\Q2016\\E",
        "shortCiteRegEx": "0.",
        "year": 2016,
        "abstract": "",
        "full_text": "",
        "sentence": " IMPROVING SAMPLE QUALITY Liu & Lee (2016) recently used the KSD S(Qn, TP ,Gk) as a means of improving the quality of a sample. Specifically, given an initial sample Qn supported on x1, . . . , xn, they minimize S(Q\u0303n, TP ,Gk) over all measures Q\u0303n supported on the same sample points to obtain a new sample that better approximates P over the class of test functions H = TPGk. In all experiments, Liu & Lee (2016) employ a Gaussian kernel k(x, y) = e\u2212 1 h\u2016x\u2212y\u2016 2 2 with bandwidth h selected to be the median of the squared Euclidean distance between pairs of sample points.",
        "context": null
    },
    {
        "title": "x)kb(x, y) and \u2207r(x) = \u2212xr(x). Thus for any x, by (Steinwart & Christmann, 2008, Corollary 4.36) we have \u2016\u2207h(x)\u20162 \u2264 Kk\u0303b \u3008\u2207x,\u2207yk\u0303b(x",
        "author": [
            "\u2207xkb(x"
        ],
        "venue": null,
        "citeRegEx": "\u2207xkb.x and y,? \\Q2008\\E",
        "shortCiteRegEx": "\u2207xkb.x and y",
        "year": 2008,
        "abstract": "",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "When bounds on R and F are known, the final expression can be optimized over and \u03b4 to produce rates of convergence in dBL\u2016\u00b7\u20162 . Consider now a sequence of probability measures (\u03bcm)m\u22651 that is uniformly tight. This implies that lim supmR(\u03bcm, ) < \u221e for all",
        "author": [
            "Gorham"
        ],
        "venue": null,
        "citeRegEx": "Gorham,? \\Q2016\\E",
        "shortCiteRegEx": "Gorham",
        "year": 2016,
        "abstract": "",
        "full_text": "",
        "sentence": "",
        "context": null
    },
    {
        "title": "Taking \u2192 0 yields dBL\u2016\u00b7\u20162 (\u03bcm, P )\u2192",
        "author": [
            "\u03b8d MP"
        ],
        "venue": null,
        "citeRegEx": "..,? \\Q2016\\E",
        "shortCiteRegEx": "..",
        "year": 2016,
        "abstract": "",
        "full_text": "",
        "sentence": "",
        "context": null
    }
]