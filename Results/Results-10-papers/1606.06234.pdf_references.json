[
    {
        "title": "Deep leaning",
        "author": [
            "G.H. Yann LeCun",
            "Yoshua Bengio"
        ],
        "venue": "Nature, vol. 521, pp. 436\u2013444, May 2015.",
        "citeRegEx": "1",
        "shortCiteRegEx": null,
        "year": 2015,
        "abstract": "",
        "full_text": "",
        "sentence": " Consequently, deep learning has become a research hotspot in research organizations and the companies[1].",
        "context": null
    },
    {
        "title": "Optimizing fpga-based accelerator design for deep convolutional neural networks",
        "author": [
            "C. Zhang",
            "P. Li",
            "G. Sun",
            "Y. Guan",
            "B. Xiao",
            "J. Cong"
        ],
        "venue": "FPGA \u201915, pp. 161\u2013170, 2015.",
        "citeRegEx": "2",
        "shortCiteRegEx": null,
        "year": 2015,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " The state-of-the-art means for accelerating deep learning algorithms are Field-Programmable Gate Array (FPGA) [2], Application Specific Integrated Circuit (ASIC) [3], and Graphic Processing Unit (GPU) [4]. [2] explores the bandwidth for the parameters facing the limitation of an FPGA chip.",
        "context": null
    },
    {
        "title": "Diannao: A small-footprint high-throughput accelerator for ubiquitous machine-learning",
        "author": [
            "T. Chen",
            "Z. Du",
            "N. Sun",
            "J. Wang",
            "C. Wu",
            "Y. Chen",
            "O. Temam"
        ],
        "venue": "ASPLOS \u201914, pp. 269\u2013284, 2014.",
        "citeRegEx": "3",
        "shortCiteRegEx": null,
        "year": 2014,
        "abstract": "",
        "full_text": "",
        "sentence": " The state-of-the-art means for accelerating deep learning algorithms are Field-Programmable Gate Array (FPGA) [2], Application Specific Integrated Circuit (ASIC) [3], and Graphic Processing Unit (GPU) [4]. Diannao [3] is one of the pioneers works solidifying the neural networks on the hardware circuits.",
        "context": null
    },
    {
        "title": "Neural acceleration for gpu throughput processors",
        "author": [
            "A. Yazdanbakhsh",
            "J. Park",
            "H. Sharma",
            "P. Lotfi-Kamran",
            "H. Esmaeilzadeh"
        ],
        "venue": "MI- CRO\u201915, pp. 482\u2013493, 2015.",
        "citeRegEx": "4",
        "shortCiteRegEx": null,
        "year": 2015,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " The state-of-the-art means for accelerating deep learning algorithms are Field-Programmable Gate Array (FPGA) [2], Application Specific Integrated Circuit (ASIC) [3], and Graphic Processing Unit (GPU) [4].",
        "context": null
    },
    {
        "title": "Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding",
        "author": [
            "S. Han",
            "H. Mao",
            "W.J. Dally"
        ],
        "venue": "ICLR\u201916, 2015.",
        "citeRegEx": "5",
        "shortCiteRegEx": null,
        "year": 2015,
        "abstract": "Neural networks are both computationally intensive and memory intensive,\nmaking them difficult to deploy on embedded systems with limited hardware\nresources. To address this limitation, we introduce \"deep compression\", a three\nstage pipeline: pruning, trained quantization and Huffman coding, that work\ntogether to reduce the storage requirement of neural networks by 35x to 49x\nwithout affecting their accuracy. Our method first prunes the network by\nlearning only the important connections. Next, we quantize the weights to\nenforce weight sharing, finally, we apply Huffman coding. After the first two\nsteps we retrain the network to fine tune the remaining connections and the\nquantized centroids. Pruning, reduces the number of connections by 9x to 13x;\nQuantization then reduces the number of bits that represent each connection\nfrom 32 to 5. On the ImageNet dataset, our method reduced the storage required\nby AlexNet by 35x, from 240MB to 6.9MB, without loss of accuracy. Our method\nreduced the size of VGG-16 by 49x from 552MB to 11.3MB, again with no loss of\naccuracy. This allows fitting the model into on-chip SRAM cache rather than\noff-chip DRAM memory. Our compression method also facilitates the use of\ncomplex neural networks in mobile applications where application size and\ndownload bandwidth are constrained. Benchmarked on CPU, GPU and mobile GPU,\ncompressed network has 3x to 4x layerwise speedup and 3x to 7x better energy\nefficiency.",
        "full_text": "Published as a conference paper at ICLR 2016\nDEEP COMPRESSION: COMPRESSING DEEP NEURAL\nNETWORKS WITH PRUNING, TRAINED QUANTIZATION\nAND HUFFMAN CODING\nSong Han\nStanford University, Stanford, CA 94305, USA\nsonghan@stanford.edu\nHuizi Mao\nTsinghua University, Beijing, 100084, China\nmhz12@mails.tsinghua.edu.cn\nWilliam J. Dally\nStanford University, Stanford, CA 94305, USA\nNVIDIA, Santa Clara, CA 95050, USA\ndally@stanford.edu\nABSTRACT\nNeural networks are both computationally intensive and memory intensive, making\nthem dif\ufb01cult to deploy on embedded systems with limited hardware resources. To\naddress this limitation, we introduce \u201cdeep compression\u201d, a three stage pipeline:\npruning, trained quantization and Huffman coding, that work together to reduce\nthe storage requirement of neural networks by 35\u00d7 to 49\u00d7 without affecting their\naccuracy. Our method \ufb01rst prunes the network by learning only the important\nconnections. Next, we quantize the weights to enforce weight sharing, \ufb01nally, we\napply Huffman coding. After the \ufb01rst two steps we retrain the network to \ufb01ne\ntune the remaining connections and the quantized centroids. Pruning, reduces the\nnumber of connections by 9\u00d7 to 13\u00d7; Quantization then reduces the number of\nbits that represent each connection from 32 to 5. On the ImageNet dataset, our\nmethod reduced the storage required by AlexNet by 35\u00d7, from 240MB to 6.9MB,\nwithout loss of accuracy. Our method reduced the size of VGG-16 by 49\u00d7 from\n552MB to 11.3MB, again with no loss of accuracy. This allows \ufb01tting the model\ninto on-chip SRAM cache rather than off-chip DRAM memory. Our compression\nmethod also facilitates the use of complex neural networks in mobile applications\nwhere application size and download bandwidth are constrained. Benchmarked on\nCPU, GPU and mobile GPU, compressed network has 3\u00d7 to 4\u00d7 layerwise speedup\nand 3\u00d7 to 7\u00d7 better energy ef\ufb01ciency.\n1\nINTRODUCTION\nDeep neural networks have evolved to the state-of-the-art technique for computer vision tasks\n(Krizhevsky et al., 2012)(Simonyan & Zisserman, 2014). Though these neural networks are very\npowerful, the large number of weights consumes considerable storage and memory bandwidth. For\nexample, the AlexNet Caffemodel is over 200MB, and the VGG-16 Caffemodel is over 500MB\n(BVLC). This makes it dif\ufb01cult to deploy deep neural networks on mobile system.\nFirst, for many mobile-\ufb01rst companies such as Baidu and Facebook, various apps are updated via\ndifferent app stores, and they are very sensitive to the size of the binary \ufb01les. For example, App\nStore has the restriction \u201capps above 100 MB will not download until you connect to Wi-Fi\u201d. As a\nresult, a feature that increases the binary size by 100MB will receive much more scrutiny than one\nthat increases it by 10MB. Although having deep neural networks running on mobile has many great\n1\narXiv:1510.00149v5  [cs.CV]  15 Feb 2016\nPublished as a conference paper at ICLR 2016\nTrain Connectivity\nPrune Connections\nTrain Weights\nCluster the Weights\nGenerate Code Book\nQuantize the Weights \nwith Code Book\nRetrain Code Book\nPruning: less number of weights\nQuantization: less bits per weight\noriginal\n   size\n   9x-13x \nreduction\n  27x-31x \nreduction\n   same \naccuracy\n   same \naccuracy\noriginal \nnetwork\nEncode Weights\nEncode Index\nHuffman Encoding\n  35x-49x \nreduction\n   same \naccuracy\nFigure 1: The three stage compression pipeline: pruning, quantization and Huffman coding. Pruning\nreduces the number of weights by 10\u00d7, while quantization further improves the compression rate:\nbetween 27\u00d7 and 31\u00d7. Huffman coding gives more compression: between 35\u00d7 and 49\u00d7. The\ncompression rate already included the meta-data for sparse representation. The compression scheme\ndoesn\u2019t incur any accuracy loss.\nfeatures such as better privacy, less network bandwidth and real time processing, the large storage\noverhead prevents deep neural networks from being incorporated into mobile apps.\nThe second issue is energy consumption. Running large neural networks require a lot of memory\nbandwidth to fetch the weights and a lot of computation to do dot products\u2014 which in turn consumes\nconsiderable energy. Mobile devices are battery constrained, making power hungry applications such\nas deep neural networks hard to deploy.\nEnergy consumption is dominated by memory access. Under 45nm CMOS technology, a 32 bit\n\ufb02oating point add consumes 0.9pJ, a 32bit SRAM cache access takes 5pJ, while a 32bit DRAM\nmemory access takes 640pJ, which is 3 orders of magnitude of an add operation. Large networks\ndo not \ufb01t in on-chip storage and hence require the more costly DRAM accesses. Running a 1 billion\nconnection neural network, for example, at 20fps would require (20Hz)(1G)(640pJ) = 12.8W just\nfor DRAM access - well beyond the power envelope of a typical mobile device.\nOur goal is to reduce the storage and energy required to run inference on such large networks so they\ncan be deployed on mobile devices. To achieve this goal, we present \u201cdeep compression\u201d: a three-\nstage pipeline (Figure 1) to reduce the storage required by neural network in a manner that preserves\nthe original accuracy. First, we prune the networking by removing the redundant connections, keeping\nonly the most informative connections. Next, the weights are quantized so that multiple connections\nshare the same weight, thus only the codebook (effective weights) and the indices need to be stored.\nFinally, we apply Huffman coding to take advantage of the biased distribution of effective weights.\nOur main insight is that, pruning and trained quantization are able to compress the network without\ninterfering each other, thus lead to surprisingly high compression rate. It makes the required storage\nso small (a few megabytes) that all weights can be cached on chip instead of going to off-chip DRAM\nwhich is energy consuming. Based on \u201cdeep compression\u201d, the EIE hardware accelerator Han et al.\n(2016) was later proposed that works on the compressed model, achieving signi\ufb01cant speedup and\nenergy ef\ufb01ciency improvement.\n2\nNETWORK PRUNING\nNetwork pruning has been widely studied to compress CNN models. In early work, network pruning\nproved to be a valid way to reduce the network complexity and over-\ufb01tting (LeCun et al., 1989;\nHanson & Pratt, 1989; Hassibi et al., 1993; Str\u00a8om, 1997). Recently Han et al. (2015) pruned state-\nof-the-art CNN models with no loss of accuracy. We build on top of that approach. As shown on\nthe left side of Figure 1, we start by learning the connectivity via normal network training. Next, we\nprune the small-weight connections: all connections with weights below a threshold are removed\nfrom the network. Finally, we retrain the network to learn the \ufb01nal weights for the remaining sparse\nconnections. Pruning reduced the number of parameters by 9\u00d7 and 13\u00d7 for AlexNet and VGG-16\nmodel.\n2\nPublished as a conference paper at ICLR 2016\nFigure 2: Representing the matrix sparsity with relative index. Padding \ufb01ller zero to prevent over\ufb02ow.\n2.09\n-0.98\n1.48\n0.09\n0.05\n-0.14\n-1.08\n2.12\n-0.91\n1.92\n0\n-1.03\n1.87\n0\n1.53\n1.49\n3\n0\n1\n1\n1\n1\n0\n3\n0\n3\n1\n0\n3\n1\n2\n2\n-0.03\n-0.01\n0.03\n0.02\n-0.01\n0.01\n-0.02\n0.12\n-0.01\n0.02\n0.04\n0.01\n-0.07\n-0.02\n0.01\n-0.02\n0.04\n0.02\n0.04\n-0.03\n-0.03\n0.12\n0.02\n-0.07\n0.03\n0.01\n0.02\n-0.01\n0.01\n0.04\n -0.01\n-0.02\n-0.01\n0.01\ncluster\n   weights \n(32 bit float)\ncentroids\ngradient\n3\n0\n2\n1\n1\n1\n0\n3\n0\n3\n1\n0\n3\n1\n2\n2\ncluster index\n  (2 bit uint)\n2.00\n1.50\n0.00\n-1.00\n-0.02\n-0.02\ngroup by\nfine-tuned \ncentroids\nreduce\n1.96\n1.48\n-0.04\n-0.97\n1:\nlr\n0:\n2:\n3:\nFigure 3: Weight sharing by scalar quantization (top) and centroids \ufb01ne-tuning (bottom).\nWe store the sparse structure that results from pruning using compressed sparse row (CSR) or\ncompressed sparse column (CSC) format, which requires 2a + n + 1 numbers, where a is the number\nof non-zero elements and n is the number of rows or columns.\nTo compress further, we store the index difference instead of the absolute position, and encode this\ndifference in 8 bits for conv layer and 5 bits for fc layer. When we need an index difference larger\nthan the bound, we the zero padding solution shown in Figure 2: in case when the difference exceeds\n8, the largest 3-bit (as an example) unsigned number, we add a \ufb01ller zero.\n3\nTRAINED QUANTIZATION AND WEIGHT SHARING\nNetwork quantization and weight sharing further compresses the pruned network by reducing the\nnumber of bits required to represent each weight. We limit the number of effective weights we need to\nstore by having multiple connections share the same weight, and then \ufb01ne-tune those shared weights.\nWeight sharing is illustrated in Figure 3. Suppose we have a layer that has 4 input neurons and 4\noutput neurons, the weight is a 4 \u00d7 4 matrix. On the top left is the 4 \u00d7 4 weight matrix, and on the\nbottom left is the 4 \u00d7 4 gradient matrix. The weights are quantized to 4 bins (denoted with 4 colors),\nall the weights in the same bin share the same value, thus for each weight, we then need to store only\na small index into a table of shared weights. During update, all the gradients are grouped by the color\nand summed together, multiplied by the learning rate and subtracted from the shared centroids from\nlast iteration. For pruned AlexNet, we are able to quantize to 8-bits (256 shared weights) for each\nCONV layers, and 5-bits (32 shared weights) for each FC layer without any loss of accuracy.\nTo calculate the compression rate, given k clusters, we only need log2(k) bits to encode the index. In\ngeneral, for a network with n connections and each connection is represented with b bits, constraining\nthe connections to have only k shared weights will result in a compression rate of:\nr =\nnb\nnlog2(k) + kb\n(1)\nFor example, Figure 3 shows the weights of a single layer neural network with four input units and\nfour output units. There are 4\u00d74 = 16 weights originally but there are only 4 shared weights: similar\nweights are grouped together to share the same value. Originally we need to store 16 weights each\n3\nPublished as a conference paper at ICLR 2016\n0.10\n0.05\n0.00\n0.05\n0.10\nweight value\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\ncummulative distribution\nCDF\nPDF\ndensity initialization\nlinear initialization\nrandom initialization\n0.04\n0.02\n0.00\n0.02\n0.04\n0.06\nweight value\n0\n5000\n10000\n15000\n20000\ndensity\nlinear quantization\nnonlinear quantization by\nclustring and finetuning\nFigure 4: Left: Three different methods for centroids initialization. Right: Distribution of weights\n(blue) and distribution of codebook before (green cross) and after \ufb01ne-tuning (red dot).\nhas 32 bits, now we need to store only 4 effective weights (blue, green, red and orange), each has 32\nbits, together with 16 2-bit indices giving a compression rate of 16 \u221732/(4 \u221732 + 2 \u221716) = 3.2\n3.1\nWEIGHT SHARING\nWe use k-means clustering to identify the shared weights for each layer of a trained network, so that\nall the weights that fall into the same cluster will share the same weight. Weights are not shared across\nlayers. We partition n original weights W = {w1, w2, ..., wn} into k clusters C = {c1, c2, ..., ck},\nn \u226bk, so as to minimize the within-cluster sum of squares (WCSS):\narg min\nC\nk\nX\ni=1\nX\nw\u2208ci\n|w \u2212ci|2\n(2)\nDifferent from HashNet (Chen et al., 2015) where weight sharing is determined by a hash function\nbefore the networks sees any training data, our method determines weight sharing after a network is\nfully trained, so that the shared weights approximate the original network.\n3.2\nINITIALIZATION OF SHARED WEIGHTS\nCentroid initialization impacts the quality of clustering and thus affects the network\u2019s prediction\naccuracy. We examine three initialization methods: Forgy(random), density-based, and linear\ninitialization. In Figure 4 we plotted the original weights\u2019 distribution of conv3 layer in AlexNet\n(CDF in blue, PDF in red). The weights forms a bimodal distribution after network pruning. On the\nbottom it plots the effective weights (centroids) with 3 different initialization methods (shown in blue,\nred and yellow). In this example, there are 13 clusters.\nForgy (random) initialization randomly chooses k observations from the data set and uses these as\nthe initial centroids. The initialized centroids are shown in yellow. Since there are two peaks in the\nbimodal distribution, Forgy method tend to concentrate around those two peaks.\nDensity-based initialization linearly spaces the CDF of the weights in the y-axis, then \ufb01nds the\nhorizontal intersection with the CDF, and \ufb01nally \ufb01nds the vertical intersection on the x-axis, which\nbecomes a centroid, as shown in blue dots. This method makes the centroids denser around the two\npeaks, but more scatted than the Forgy method.\nLinear initialization linearly spaces the centroids between the [min, max] of the original weights.\nThis initialization method is invariant to the distribution of the weights and is the most scattered\ncompared with the former two methods.\nLarger weights play a more important role than smaller weights (Han et al., 2015), but there are fewer\nof these large weights. Thus for both Forgy initialization and density-based initialization, very few\ncentroids have large absolute value which results in poor representation of these few large weights.\nLinear initialization does not suffer from this problem. The experiment section compares the accuracy\n4\nPublished as a conference paper at ICLR 2016\n0\n25000\n50000\n75000\n100000\n1\n3\n5\n7\n9\n11\n13\n15\n17\n19\n21\n23\n25\n27\n29\n31\nCount\nWeight Index (32 Effective Weights)\n0\n55000\n110000\n165000\n220000\n1\n3\n5\n7\n9\n11\n13\n15\n17\n19\n21\n23\n25\n27\n29\n31\nCount\nSparse Matrix Location Index (Max Diff is 32)\nFigure 5: Distribution for weight (Left) and index (Right). The distribution is biased.\nof different initialization methods after clustering and \ufb01ne-tuning, showing that linear initialization\nworks best.\n3.3\nFEED-FORWARD AND BACK-PROPAGATION\nThe centroids of the one-dimensional k-means clustering are the shared weights. There is one level\nof indirection during feed forward phase and back-propagation phase looking up the weight table.\nAn index into the shared weight table is stored for each connection. During back-propagation, the\ngradient for each shared weight is calculated and used to update the shared weight. This procedure is\nshown in Figure 3.\nWe denote the loss by L, the weight in the ith column and jth row by Wij, the centroid index of\nelement Wi,j by Iij, the kth centroid of the layer by Ck. By using the indicator function 1(.), the\ngradient of the centroids is calculated as:\n\u2202L\n\u2202Ck\n=\nX\ni,j\n\u2202L\n\u2202Wij\n\u2202Wij\n\u2202Ck\n=\nX\ni,j\n\u2202L\n\u2202Wij\n1(Iij = k)\n(3)\n4\nHUFFMAN CODING\nA Huffman code is an optimal pre\ufb01x code commonly used for lossless data compression(Van Leeuwen,\n1976). It uses variable-length codewords to encode source symbols. The table is derived from the\noccurrence probability for each symbol. More common symbols are represented with fewer bits.\nFigure 5 shows the probability distribution of quantized weights and the sparse matrix index of the\nlast fully connected layer in AlexNet. Both distributions are biased: most of the quantized weights are\ndistributed around the two peaks; the sparse matrix index difference are rarely above 20. Experiments\nshow that Huffman coding these non-uniformly distributed values saves 20% \u221230% of network\nstorage.\n5\nEXPERIMENTS\nWe pruned, quantized, and Huffman encoded four networks: two on MNIST and two on ImageNet\ndata-sets. The network parameters and accuracy-1 before and after pruning are shown in Table 1. The\ncompression pipeline saves network storage by 35\u00d7 to 49\u00d7 across different networks without loss\nof accuracy. The total size of AlexNet decreased from 240MB to 6.9MB, which is small enough to\nbe put into on-chip SRAM, eliminating the need to store the model in energy-consuming DRAM\nmemory.\nTraining is performed with the Caffe framework (Jia et al., 2014). Pruning is implemented by adding\na mask to the blobs to mask out the update of the pruned connections. Quantization and weight\nsharing are implemented by maintaining a codebook structure that stores the shared weight, and\ngroup-by-index after calculating the gradient of each layer. Each shared weight is updated with all\nthe gradients that fall into that bucket. Huffman coding doesn\u2019t require training and is implemented\nof\ufb02ine after all the \ufb01ne-tuning is \ufb01nished.\n5.1\nLENET-300-100 AND LENET-5 ON MNIST\nWe \ufb01rst experimented on MNIST dataset with LeNet-300-100 and LeNet-5 network (LeCun et al.,\n1998). LeNet-300-100 is a fully connected network with two hidden layers, with 300 and 100\n1Reference model is from Caffe model zoo, accuracy is measured without data augmentation\n5\nPublished as a conference paper at ICLR 2016\nTable 1: The compression pipeline can save 35\u00d7 to 49\u00d7 parameter storage with no loss of accuracy.\nNetwork\nTop-1 Error\nTop-5 Error\nParameters\nCompress\nRate\nLeNet-300-100 Ref\n1.64%\n-\n1070 KB\nLeNet-300-100 Compressed\n1.58%\n-\n27 KB\n40\u00d7\nLeNet-5 Ref\n0.80%\n-\n1720 KB\nLeNet-5 Compressed\n0.74%\n-\n44 KB\n39\u00d7\nAlexNet Ref\n42.78%\n19.73%\n240 MB\nAlexNet Compressed\n42.78%\n19.70%\n6.9 MB\n35\u00d7\nVGG-16 Ref\n31.50%\n11.32%\n552 MB\nVGG-16 Compressed\n31.17%\n10.91%\n11.3 MB\n49\u00d7\nTable 2: Compression statistics for LeNet-300-100. P: pruning, Q:quantization, H:Huffman coding.\nLayer\n#Weights\nWeights%\n(P)\nWeight\nbits\n(P+Q)\nWeight\nbits\n(P+Q+H)\nIndex\nbits\n(P+Q)\nIndex\nbits\n(P+Q+H)\nCompress\nrate\n(P+Q)\nCompress\nrate\n(P+Q+H)\nip1\n235K\n8%\n6\n4.4\n5\n3.7\n3.1%\n2.32%\nip2\n30K\n9%\n6\n4.4\n5\n4.3\n3.8%\n3.04%\nip3\n1K\n26%\n6\n4.3\n5\n3.2\n15.7%\n12.70%\nTotal\n266K\n8%(12\u00d7)\n6\n5.1\n5\n3.7\n3.1% (32\u00d7)\n2.49% (40\u00d7)\nTable 3: Compression statistics for LeNet-5. P: pruning, Q:quantization, H:Huffman coding.\nLayer\n#Weights\nWeights%\n(P)\nWeight\nbits\n(P+Q)\nWeight\nbits\n(P+Q+H)\nIndex\nbits\n(P+Q)\nIndex\nbits\n(P+Q+H)\nCompress\nrate\n(P+Q)\nCompress\nrate\n(P+Q+H)\nconv1\n0.5K\n66%\n8\n7.2\n5\n1.5\n78.5%\n67.45%\nconv2\n25K\n12%\n8\n7.2\n5\n3.9\n6.0%\n5.28%\nip1\n400K\n8%\n5\n4.5\n5\n4.5\n2.7%\n2.45%\nip2\n5K\n19%\n5\n5.2\n5\n3.7\n6.9%\n6.13%\nTotal\n431K\n8%(12\u00d7)\n5.3\n4.1\n5\n4.4\n3.05% (33\u00d7)\n2.55% (39\u00d7)\nneurons each, which achieves 1.6% error rate on Mnist. LeNet-5 is a convolutional network that\nhas two convolutional layers and two fully connected layers, which achieves 0.8% error rate on\nMnist. Table 2 and table 3 show the statistics of the compression pipeline. The compression rate\nincludes the overhead of the codebook and sparse indexes. Most of the saving comes from pruning\nand quantization (compressed 32\u00d7), while Huffman coding gives a marginal gain (compressed 40\u00d7)\n5.2\nALEXNET ON IMAGENET\nWe further examine the performance of Deep Compression on the ImageNet ILSVRC-2012 dataset,\nwhich has 1.2M training examples and 50k validation examples. We use the AlexNet Caffe model as\nthe reference model, which has 61 million parameters and achieved a top-1 accuracy of 57.2% and a\ntop-5 accuracy of 80.3%. Table 4 shows that AlexNet can be compressed to 2.88% of its original size\nwithout impacting accuracy. There are 256 shared weights in each CONV layer, which are encoded\nwith 8 bits, and 32 shared weights in each FC layer, which are encoded with only 5 bits. The relative\nsparse index is encoded with 4 bits. Huffman coding compressed additional 22%, resulting in 35\u00d7\ncompression in total.\n5.3\nVGG-16 ON IMAGENET\nWith promising results on AlexNet, we also looked at a larger, more recent network, VGG-16 (Si-\nmonyan & Zisserman, 2014), on the same ILSVRC-2012 dataset. VGG-16 has far more convolutional\nlayers but still only three fully-connected layers. Following a similar methodology, we aggressively\ncompressed both convolutional and fully-connected layers to realize a signi\ufb01cant reduction in the\nnumber of effective weights, shown in Table5.\nThe VGG16 network as a whole has been compressed by 49\u00d7. Weights in the CONV layers are\nrepresented with 8 bits, and FC layers use 5 bits, which does not impact the accuracy. The two largest\nfully-connected layers can each be pruned to less than 1.6% of their original size. This reduction\n6\nPublished as a conference paper at ICLR 2016\nTable 4: Compression statistics for AlexNet. P: pruning, Q: quantization, H:Huffman coding.\nLayer\n#Weights\nWeights%\n(P)\nWeight\nbits\n(P+Q)\nWeight\nbits\n(P+Q+H)\nIndex\nbits\n(P+Q)\nIndex\nbits\n(P+Q+H)\nCompress\nrate\n(P+Q)\nCompress\nrate\n(P+Q+H)\nconv1\n35K\n84%\n8\n6.3\n4\n1.2\n32.6%\n20.53%\nconv2\n307K\n38%\n8\n5.5\n4\n2.3\n14.5%\n9.43%\nconv3\n885K\n35%\n8\n5.1\n4\n2.6\n13.1%\n8.44%\nconv4\n663K\n37%\n8\n5.2\n4\n2.5\n14.1%\n9.11%\nconv5\n442K\n37%\n8\n5.6\n4\n2.5\n14.0%\n9.43%\nfc6\n38M\n9%\n5\n3.9\n4\n3.2\n3.0%\n2.39%\nfc7\n17M\n9%\n5\n3.6\n4\n3.7\n3.0%\n2.46%\nfc8\n4M\n25%\n5\n4\n4\n3.2\n7.3%\n5.85%\nTotal\n61M\n11%(9\u00d7)\n5.4\n4\n4\n3.2\n3.7% (27\u00d7)\n2.88% (35\u00d7)\nTable 5: Compression statistics for VGG-16. P: pruning, Q:quantization, H:Huffman coding.\nLayer\n#Weights\nWeights%\n(P)\nWeigh\nbits\n(P+Q)\nWeight\nbits\n(P+Q+H)\nIndex\nbits\n(P+Q)\nIndex\nbits\n(P+Q+H)\nCompress\nrate\n(P+Q)\nCompress\nrate\n(P+Q+H)\nconv1 1\n2K\n58%\n8\n6.8\n5\n1.7\n40.0%\n29.97%\nconv1 2\n37K\n22%\n8\n6.5\n5\n2.6\n9.8%\n6.99%\nconv2 1\n74K\n34%\n8\n5.6\n5\n2.4\n14.3%\n8.91%\nconv2 2\n148K\n36%\n8\n5.9\n5\n2.3\n14.7%\n9.31%\nconv3 1\n295K\n53%\n8\n4.8\n5\n1.8\n21.7%\n11.15%\nconv3 2\n590K\n24%\n8\n4.6\n5\n2.9\n9.7%\n5.67%\nconv3 3\n590K\n42%\n8\n4.6\n5\n2.2\n17.0%\n8.96%\nconv4 1\n1M\n32%\n8\n4.6\n5\n2.6\n13.1%\n7.29%\nconv4 2\n2M\n27%\n8\n4.2\n5\n2.9\n10.9%\n5.93%\nconv4 3\n2M\n34%\n8\n4.4\n5\n2.5\n14.0%\n7.47%\nconv5 1\n2M\n35%\n8\n4.7\n5\n2.5\n14.3%\n8.00%\nconv5 2\n2M\n29%\n8\n4.6\n5\n2.7\n11.7%\n6.52%\nconv5 3\n2M\n36%\n8\n4.6\n5\n2.3\n14.8%\n7.79%\nfc6\n103M\n4%\n5\n3.6\n5\n3.5\n1.6%\n1.10%\nfc7\n17M\n4%\n5\n4\n5\n4.3\n1.5%\n1.25%\nfc8\n4M\n23%\n5\n4\n5\n3.4\n7.1%\n5.24%\nTotal\n138M\n7.5%(13\u00d7)\n6.4\n4.1\n5\n3.1\n3.2% (31\u00d7)\n2.05% (49\u00d7)\nis critical for real time image processing, where there is little reuse of these layers across images\n(unlike batch processing). This is also critical for fast object detection algorithms where one CONV\npass is used by many FC passes. The reduced layers will \ufb01t in an on-chip SRAM and have modest\nbandwidth requirements. Without the reduction, the bandwidth requirements are prohibitive.\n6\nDISCUSSIONS\n6.1\nPRUNING AND QUANTIZATION WORKING TOGETHER\nFigure 6 shows the accuracy at different compression rates for pruning and quantization together\nor individually. When working individually, as shown in the purple and yellow lines, accuracy of\npruned network begins to drop signi\ufb01cantly when compressed below 8% of its original size; accuracy\nof quantized network also begins to drop signi\ufb01cantly when compressed below 8% of its original\nsize. But when combined, as shown in the red line, the network can be compressed to 3% of original\nsize with no loss of accuracy. On the far right side compared the result of SVD, which is inexpensive\nbut has a poor compression rate.\nThe three plots in Figure 7 show how accuracy drops with fewer bits per connection for CONV layers\n(left), FC layers (middle) and all layers (right). Each plot reports both top-1 and top-5 accuracy.\nDashed lines only applied quantization but without pruning; solid lines did both quantization and\npruning. There is very little difference between the two. This shows that pruning works well with\nquantization.\nQuantization works well on pruned network because unpruned AlexNet has 60 million weights to\nquantize, while pruned AlexNet has only 6.7 million weights to quantize. Given the same amount of\ncentroids, the latter has less error.\n7\nPublished as a conference paper at ICLR 2016\nFigure 6: Accuracy v.s. compression rate under different compression methods. Pruning and\nquantization works best when combined.\nFigure 7: Pruning doesn\u2019t hurt quantization. Dashed: quantization on unpruned network. Solid:\nquantization on pruned network; Accuracy begins to drop at the same number of quantization bits\nwhether or not the network has been pruned. Although pruning made the number of parameters less,\nquantization still works well, or even better(3 bits case on the left \ufb01gure) as in the unpruned network.\nFigure 8: Accuracy of different initialization methods. Left: top-1 accuracy. Right: top-5 accuracy.\nLinear initialization gives best result.\nThe \ufb01rst two plots in Figure 7 show that CONV layers require more bits of precision than FC layers.\nFor CONV layers, accuracy drops signi\ufb01cantly below 4 bits, while FC layer is more robust: not until\n2 bits did the accuracy drop signi\ufb01cantly.\n6.2\nCENTROID INITIALIZATION\nFigure 8 compares the accuracy of the three different initialization methods with respect to top-1\naccuracy (Left) and top-5 accuracy (Right). The network is quantized to 2 \u223c8 bits as shown on\nx-axis. Linear initialization outperforms the density initialization and random initialization in all\ncases except at 3 bits.\nThe initial centroids of linear initialization spread equally across the x-axis, from the min value to the\nmax value. That helps to maintain the large weights as the large weights play a more important role\nthan smaller ones, which is also shown in network pruning Han et al. (2015). Neither random nor\ndensity-based initialization retains large centroids. With these initialization methods, large weights are\nclustered to the small centroids because there are few large weights. In contrast, linear initialization\nallows large weights a better chance to form a large centroid.\n8\nPublished as a conference paper at ICLR 2016\nFigure 9: Compared with the original network, pruned network layer achieved 3\u00d7 speedup on CPU,\n3.5\u00d7 on GPU and 4.2\u00d7 on mobile GPU on average. Batch size = 1 targeting real time processing.\nPerformance number normalized to CPU.\nFigure 10: Compared with the original network, pruned network layer takes 7\u00d7 less energy on CPU,\n3.3\u00d7 less on GPU and 4.2\u00d7 less on mobile GPU on average. Batch size = 1 targeting real time\nprocessing. Energy number normalized to CPU.\n6.3\nSPEEDUP AND ENERGY EFFICIENCY\nDeep Compression is targeting extremely latency-focused applications running on mobile, which\nrequires real-time inference, such as pedestrian detection on an embedded processor inside an\nautonomous vehicle. Waiting for a batch to assemble signi\ufb01cantly adds latency. So when bench-\nmarking the performance and energy ef\ufb01ciency, we consider the case when batch size = 1. The cases\nof batching are given in Appendix A.\nFully connected layer dominates the model size (more than 90%) and got compressed the most by\nDeep Compression (96% weights pruned in VGG-16). In state-of-the-art object detection algorithms\nsuch as fast R-CNN (Girshick, 2015), upto 38% computation time is consumed on FC layers on\nuncompressed model. So it\u2019s interesting to benchmark on FC layers, to see the effect of Deep\nCompression on performance and energy. Thus we setup our benchmark on FC6, FC7, FC8 layers of\nAlexNet and VGG-16. In the non-batched case, the activation matrix is a vector with just one column,\nso the computation boils down to dense / sparse matrix-vector multiplication for original / pruned\nmodel, respectively. Since current BLAS library on CPU and GPU doesn\u2019t support indirect look-up\nand relative indexing, we didn\u2019t benchmark the quantized model.\nWe compare three different off-the-shelf hardware: the NVIDIA GeForce GTX Titan X and the Intel\nCore i7 5930K as desktop processors (same package as NVIDIA Digits Dev Box) and NVIDIA Tegra\nK1 as mobile processor. To run the benchmark on GPU, we used cuBLAS GEMV for the original\ndense layer. For the pruned sparse layer, we stored the sparse matrix in in CSR format, and used\ncuSPARSE CSRMV kernel, which is optimized for sparse matrix-vector multiplication on GPU. To\nrun the benchmark on CPU, we used MKL CBLAS GEMV for the original dense model and MKL\nSPBLAS CSRMV for the pruned sparse model.\nTo compare power consumption between different systems, it is important to measure power at a\nconsistent manner (NVIDIA, b). For our analysis, we are comparing pre-regulation power of the\nentire application processor (AP) / SOC and DRAM combined. On CPU, the benchmark is running on\nsingle socket with a single Haswell-E class Core i7-5930K processor. CPU socket and DRAM power\nare as reported by the pcm-power utility provided by Intel. For GPU, we used nvidia-smi\nutility to report the power of Titan X. For mobile GPU, we use a Jetson TK1 development board and\nmeasured the total power consumption with a power-meter. We assume 15% AC to DC conversion\nloss, 85% regulator ef\ufb01ciency and 15% power consumed by peripheral components (NVIDIA, a) to\nreport the AP+DRAM power for Tegra K1.\n9\nPublished as a conference paper at ICLR 2016\nTable 6: Accuracy of AlexNet with different aggressiveness of weight sharing and quantization. 8/5\nbit quantization has no loss of accuracy; 8/4 bit quantization, which is more hardware friendly, has\nnegligible loss of accuracy of 0.01%; To be really aggressive, 4/2 bit quantization resulted in 1.99%\nand 2.60% loss of accuracy.\n#CONV bits / #FC bits\nTop-1 Error\nTop-5 Error\nTop-1 Error\nIncrease\nTop-5 Error\nIncrease\n32bits / 32bits\n42.78%\n19.73%\n-\n-\n8 bits / 5 bits\n42.78%\n19.70%\n0.00%\n-0.03%\n8 bits / 4 bits\n42.79%\n19.73%\n0.01%\n0.00%\n4 bits / 2 bits\n44.77%\n22.33%\n1.99%\n2.60%\nThe ratio of memory access over computation characteristic with and without batching is different.\nWhen the input activations are batched to a matrix the computation becomes matrix-matrix multipli-\ncation, where locality can be improved by blocking. Matrix could be blocked to \ufb01t in caches and\nreused ef\ufb01ciently. In this case, the amount of memory access is O(n2), and that of computation is\nO(n3), the ratio between memory access and computation is in the order of 1/n.\nIn real time processing when batching is not allowed, the input activation is a single vector and the\ncomputation is matrix-vector multiplication. In this case, the amount of memory access is O(n2), and\nthe computation is O(n2), memory access and computation are of the same magnitude (as opposed\nto 1/n). That indicates MV is more memory-bounded than MM. So reducing the memory footprint\nis critical for the non-batching case.\nFigure 9 illustrates the speedup of pruning on different hardware. There are 6 columns for each\nbenchmark, showing the computation time of CPU / GPU / TK1 on dense / pruned network. Time is\nnormalized to CPU. When batch size = 1, pruned network layer obtained 3\u00d7 to 4\u00d7 speedup over the\ndense network on average because it has smaller memory footprint and alleviates the data transferring\noverhead, especially for large matrices that are unable to \ufb01t into the caches. For example VGG16\u2019s\nFC6 layer, the largest layer in our experiment, contains 25088 \u00d7 4096 \u00d7 4 Bytes \u2248400MB data,\nwhich is far from the capacity of L3 cache.\nIn those latency-tolerating applications , batching improves memory locality, where weights could\nbe blocked and reused in matrix-matrix multiplication. In this scenario, pruned network no longer\nshows its advantage. We give detailed timing results in Appendix A.\nFigure 10 illustrates the energy ef\ufb01ciency of pruning on different hardware. We multiply power\nconsumption with computation time to get energy consumption, then normalized to CPU to get\nenergy ef\ufb01ciency. When batch size = 1, pruned network layer consumes 3\u00d7 to 7\u00d7 less energy over\nthe dense network on average. Reported by nvidia-smi, GPU utilization is 99% for both dense\nand sparse cases.\n6.4\nRATIO OF WEIGHTS, INDEX AND CODEBOOK\nPruning makes the weight matrix sparse, so extra space is needed to store the indexes of non-zero\nelements. Quantization adds storage for a codebook. The experiment section has already included\nthese two factors. Figure 11 shows the breakdown of three different components when quantizing\nfour networks. Since on average both the weights and the sparse indexes are encoded with 5 bits,\ntheir storage is roughly half and half. The overhead of codebook is very small and often negligible.\nFigure 11: Storage ratio of weight, index and codebook.\n10\nPublished as a conference paper at ICLR 2016\nTable 7: Comparison with other compression methods on AlexNet. (Collins & Kohli, 2014) reduced\nthe parameters by 4\u00d7 and with inferior accuracy. Deep Fried Convnets(Yang et al., 2014) worked\non fully connected layers and reduced the parameters by less than 4\u00d7. SVD save parameters but\nsuffers from large accuracy loss as much as 2%. Network pruning (Han et al., 2015) reduced the\nparameters by 9\u00d7, not including index overhead. On other networks similar to AlexNet, (Denton\net al., 2014) exploited linear structure of convnets and compressed the network by 2.4\u00d7 to 13.4\u00d7\nlayer wise, with 0.9% accuracy loss on compressing a single layer. (Gong et al., 2014) experimented\nwith vector quantization and compressed the network by 16\u00d7 to 24\u00d7, incurring 1% accuracy loss.\nNetwork\nTop-1 Error\nTop-5 Error\nParameters\nCompress\nRate\nBaseline Caffemodel (BVLC)\n42.78%\n19.73%\n240MB\n1\u00d7\nFastfood-32-AD (Yang et al., 2014)\n41.93%\n-\n131MB\n2\u00d7\nFastfood-16-AD (Yang et al., 2014)\n42.90%\n-\n64MB\n3.7\u00d7\nCollins & Kohli (Collins & Kohli, 2014)\n44.40%\n-\n61MB\n4\u00d7\nSVD (Denton et al., 2014)\n44.02%\n20.56%\n47.6MB\n5\u00d7\nPruning (Han et al., 2015)\n42.77%\n19.67%\n27MB\n9\u00d7\nPruning+Quantization\n42.78%\n19.70%\n8.9MB\n27\u00d7\nPruning+Quantization+Huffman\n42.78%\n19.70%\n6.9MB\n35\u00d7\n7\nRELATED WORK\nNeural networks are typically over-parametrized, and there is signi\ufb01cant redundancy for deep learning\nmodels(Denil et al., 2013). This results in a waste of both computation and memory usage. There\nhave been various proposals to remove the redundancy: Vanhoucke et al. (2011) explored a \ufb01xed-\npoint implementation with 8-bit integer (vs 32-bit \ufb02oating point) activations. Hwang & Sung\n(2014) proposed an optimization method for the \ufb01xed-point network with ternary weights and 3-bit\nactivations. Anwar et al. (2015) quantized the neural network using L2 error minimization and\nachieved better accuracy on MNIST and CIFAR-10 datasets.Denton et al. (2014) exploited the linear\nstructure of the neural network by \ufb01nding an appropriate low-rank approximation of the parameters\nand keeping the accuracy within 1% of the original model.\nThe empirical success in this paper is consistent with the theoretical study of random-like sparse\nnetworks with +1/0/-1 weights (Arora et al., 2014), which have been proved to enjoy nice properties\n(e.g. reversibility), and to allow a provably polynomial time algorithm for training.\nMuch work has been focused on binning the network parameters into buckets, and only the values in\nthe buckets need to be stored. HashedNets(Chen et al., 2015) reduce model sizes by using a hash\nfunction to randomly group connection weights, so that all connections within the same hash bucket\nshare a single parameter value. In their method, the weight binning is pre-determined by the hash\nfunction, instead of being learned through training, which doesn\u2019t capture the nature of images. Gong\net al. (2014) compressed deep convnets using vector quantization, which resulted in 1% accuracy\nloss. Both methods studied only the fully connected layer, ignoring the convolutional layers.\nThere have been other attempts to reduce the number of parameters of neural networks by replacing\nthe fully connected layer with global average pooling. The Network in Network architecture(Lin et al.,\n2013) and GoogLenet(Szegedy et al., 2014) achieves state-of-the-art results on several benchmarks by\nadopting this idea. However, transfer learning, i.e. reusing features learned on the ImageNet dataset\nand applying them to new tasks by only \ufb01ne-tuning the fully connected layers, is more dif\ufb01cult with\nthis approach. This problem is noted by Szegedy et al. (2014) and motivates them to add a linear\nlayer on the top of their networks to enable transfer learning.\nNetwork pruning has been used both to reduce network complexity and to reduce over-\ufb01tting. An\nearly approach to pruning was biased weight decay (Hanson & Pratt, 1989). Optimal Brain Damage\n(LeCun et al., 1989) and Optimal Brain Surgeon (Hassibi et al., 1993) prune networks to reduce\nthe number of connections based on the Hessian of the loss function and suggest that such pruning\nis more accurate than magnitude-based pruning such as weight decay. A recent work (Han et al.,\n2015) successfully pruned several state of the art large scale networks and showed that the number of\nparameters could be reduce by an order of magnitude. There are also attempts to reduce the number\nof activations for both compression and acceleration Van Nguyen et al. (2015).\n11\nPublished as a conference paper at ICLR 2016\n8\nFUTURE WORK\nWhile the pruned network has been benchmarked on various hardware, the quantized network with\nweight sharing has not, because off-the-shelf cuSPARSE or MKL SPBLAS library does not support\nindirect matrix entry lookup, nor is the relative index in CSC or CSR format supported. So the full\nadvantage of Deep Compression that \ufb01t the model in cache is not fully unveiled. A software solution\nis to write customized GPU kernels that support this. A hardware solution is to build custom ASIC\narchitecture specialized to traverse the sparse and quantized network structure, which also supports\ncustomized quantization bit width. We expect this architecture to have energy dominated by on-chip\nSRAM access instead of off-chip DRAM access.\n9\nCONCLUSION\nWe have presented \u201cDeep Compression\u201d that compressed neural networks without affecting accuracy.\nOur method operates by pruning the unimportant connections, quantizing the network using weight\nsharing, and then applying Huffman coding. We highlight our experiments on AlexNet which\nreduced the weight storage by 35\u00d7 without loss of accuracy. We show similar results for VGG-16\nand LeNet networks compressed by 49\u00d7 and 39\u00d7 without loss of accuracy. This leads to smaller\nstorage requirement of putting convnets into mobile app. After Deep Compression the size of these\nnetworks \ufb01t into on-chip SRAM cache (5pJ/access) rather than requiring off-chip DRAM memory\n(640pJ/access). This potentially makes deep neural networks more energy ef\ufb01cient to run on mobile.\nOur compression method also facilitates the use of complex neural networks in mobile applications\nwhere application size and download bandwidth are constrained.\nREFERENCES\nAnwar, Sajid, Hwang, Kyuyeon, and Sung, Wonyong. Fixed point optimization of deep convolutional\nneural networks for object recognition. In Acoustics, Speech and Signal Processing (ICASSP),\n2015 IEEE International Conference on, pp. 1131\u20131135. IEEE, 2015.\nArora, Sanjeev, Bhaskara, Aditya, Ge, Rong, and Ma, Tengyu. Provable bounds for learning some\ndeep representations. In Proceedings of the 31th International Conference on Machine Learning,\nICML 2014, pp. 584\u2013592, 2014.\nBVLC. Caffe model zoo. URL http://caffe.berkeleyvision.org/model_zoo.\nChen, Wenlin, Wilson, James T., Tyree, Stephen, Weinberger, Kilian Q., and Chen, Yixin. Compress-\ning neural networks with the hashing trick. arXiv preprint arXiv:1504.04788, 2015.\nCollins, Maxwell D and Kohli, Pushmeet. Memory bounded deep convolutional networks. arXiv\npreprint arXiv:1412.1442, 2014.\nDenil, Misha, Shakibi, Babak, Dinh, Laurent, de Freitas, Nando, et al. Predicting parameters in deep\nlearning. In Advances in Neural Information Processing Systems, pp. 2148\u20132156, 2013.\nDenton, Emily L, Zaremba, Wojciech, Bruna, Joan, LeCun, Yann, and Fergus, Rob. Exploiting linear\nstructure within convolutional networks for ef\ufb01cient evaluation. In Advances in Neural Information\nProcessing Systems, pp. 1269\u20131277, 2014.\nGirshick, Ross. Fast r-cnn. arXiv preprint arXiv:1504.08083, 2015.\nGong, Yunchao, Liu, Liu, Yang, Ming, and Bourdev, Lubomir. Compressing deep convolutional\nnetworks using vector quantization. arXiv preprint arXiv:1412.6115, 2014.\nHan, Song, Pool, Jeff, Tran, John, and Dally, William J. Learning both weights and connections for\nef\ufb01cient neural networks. In Advances in Neural Information Processing Systems, 2015.\nHan, Song, Liu, Xingyu, Mao, Huizi, Pu, Jing, Pedram, Ardavan, Horowitz, Mark A, and Dally,\nWilliam J. EIE: Ef\ufb01cient inference engine on compressed deep neural network. arXiv preprint\narXiv:1602.01528, 2016.\n12\nPublished as a conference paper at ICLR 2016\nHanson, Stephen Jos\u00b4e and Pratt, Lorien Y. Comparing biases for minimal network construction with\nback-propagation. In Advances in neural information processing systems, pp. 177\u2013185, 1989.\nHassibi, Babak, Stork, David G, et al. Second order derivatives for network pruning: Optimal brain\nsurgeon. Advances in neural information processing systems, pp. 164\u2013164, 1993.\nHwang, Kyuyeon and Sung, Wonyong. Fixed-point feedforward deep neural network design using\nweights+ 1, 0, and- 1. In Signal Processing Systems (SiPS), 2014 IEEE Workshop on, pp. 1\u20136.\nIEEE, 2014.\nJia, Yangqing, Shelhamer, Evan, Donahue, Jeff, Karayev, Sergey, Long, Jonathan, Girshick, Ross,\nGuadarrama, Sergio, and Darrell, Trevor. Caffe: Convolutional architecture for fast feature\nembedding. arXiv preprint arXiv:1408.5093, 2014.\nKrizhevsky, Alex, Sutskever, Ilya, and Hinton, Geoffrey E. Imagenet classi\ufb01cation with deep\nconvolutional neural networks. In NIPS, pp. 1097\u20131105, 2012.\nLeCun, Yann, Denker, John S, Solla, Sara A, Howard, Richard E, and Jackel, Lawrence D. Optimal\nbrain damage. In NIPs, volume 89, 1989.\nLeCun, Yann, Bottou, Leon, Bengio, Yoshua, and Haffner, Patrick. Gradient-based learning applied\nto document recognition. Proceedings of the IEEE, 86(11):2278\u20132324, 1998.\nLin, Min, Chen, Qiang, and Yan, Shuicheng. Network in network. arXiv:1312.4400, 2013.\nNVIDIA. Technical brief: NVIDIA jetson TK1 development kit bringing GPU-accelerated computing\nto embedded systems, a. URL http://www.nvidia.com.\nNVIDIA. Whitepaper: GPU-based deep learning inference: A performance and power analysis, b.\nURL http://www.nvidia.com/object/white-papers.html.\nSimonyan, Karen and Zisserman, Andrew. Very deep convolutional networks for large-scale image\nrecognition. arXiv preprint arXiv:1409.1556, 2014.\nStr\u00a8om, Nikko. Phoneme probability estimation with dynamic sparsely connected arti\ufb01cial neural\nnetworks. The Free Speech Journal, 1(5):1\u201341, 1997.\nSzegedy, Christian, Liu, Wei, Jia, Yangqing, Sermanet, Pierre, Reed, Scott, Anguelov, Dragomir,\nErhan, Dumitru, Vanhoucke, Vincent, and Rabinovich, Andrew. Going deeper with convolutions.\narXiv preprint arXiv:1409.4842, 2014.\nVan Leeuwen, Jan. On the construction of huffman trees. In ICALP, pp. 382\u2013410, 1976.\nVan Nguyen, Hien, Zhou, Kevin, and Vemulapalli, Raviteja. Cross-domain synthesis of medical\nimages using ef\ufb01cient location-sensitive deep network. In Medical Image Computing and Computer-\nAssisted Intervention\u2013MICCAI 2015, pp. 677\u2013684. Springer, 2015.\nVanhoucke, Vincent, Senior, Andrew, and Mao, Mark Z. Improving the speed of neural networks on\ncpus. In Proc. Deep Learning and Unsupervised Feature Learning NIPS Workshop, 2011.\nYang, Zichao, Moczulski, Marcin, Denil, Misha, de Freitas, Nando, Smola, Alex, Song, Le, and\nWang, Ziyu. Deep fried convnets. arXiv preprint arXiv:1412.7149, 2014.\n13\nPublished as a conference paper at ICLR 2016\nA\nAPPENDIX: DETAILED TIMING / POWER REPORTS OF DENSE & SPARSE\nNETWORK LAYERS\nTable 8: Average time on different layers. To avoid variance, we measured the time spent on each\nlayer for 4096 input samples, and averaged the time regarding each input sample. For GPU, the time\nconsumed by cudaMalloc and cudaMemcpy is not counted. For batch size = 1, gemv is used;\nFor batch size = 64, gemm is used. For sparse case, csrmv and csrmm is used, respectively.\nTime (us)\nAlexNet\nFC6\nAlexNet\nFC7\nAlexNet\nFC8\nVGG16\nFC6\nVGG16\nFC7\nVGG16\nFC8\nTitan X\ndense (batch=1)\n541.5\n243.0\n80.5\n1467.8\n243.0\n80.5\nsparse (batch=1)\n134.8\n65.8\n54.6\n167.0\n39.8\n48.0\ndense (batch=64)\n19.8\n8.9\n5.9\n53.6\n8.9\n5.9\nsparse (batch=64)\n94.6\n51.5\n23.2\n121.5\n24.4\n22.0\nCore\ni7-5930k\ndense (batch=1)\n7516.2\n6187.1\n1134.9\n35022.8\n5372.8\n774.2\nsparse (batch=1)\n3066.5\n1282.1\n890.5\n3774.3\n545.1\n777.3\ndense (batch=64)\n318.4\n188.9\n45.8\n1056.0\n188.3\n45.7\nsparse (batch=64)\n1417.6\n682.1\n407.7\n1780.3\n274.9\n363.1\nTegra K1\ndense (batch=1)\n12437.2\n5765.0\n2252.1\n35427.0\n5544.3\n2243.1\nsparse (batch=1)\n2879.3\n1256.5\n837.0\n4377.2\n626.3\n745.1\ndense (batch=64)\n1663.6\n2056.8\n298.0\n2001.4\n2050.7\n483.9\nsparse (batch=64)\n4003.9\n1372.8\n576.7\n8024.8\n660.2\n544.1\nTable 9: Power consumption of different layers. We measured the Titan X GPU power with\nnvidia-smi, Core i7-5930k CPU power with pcm-power and Tegra K1 mobile GPU power with\nan external power meter (scaled to AP+DRAM, see paper discussion). During power measurement,\nwe repeated each computation multiple times in order to get stable numbers. On CPU, dense matrix\nmultiplications consume 2x energy than sparse ones because it is accelerated with multi-threading.\nPower (Watts)\nAlexNet\nFC6\nAlexNet\nFC7\nAlexNet\nFC8\nVGG16\nFC6\nVGG16\nFC7\nVGG16\nFC8\nTitanX\ndense (batch=1)\n157\n159\n159\n166\n163\n159\nsparse (batch=1)\n181\n183\n162\n189\n166\n162\ndense (batch=64)\n168\n173\n166\n173\n173\n167\nsparse (batch=64)\n156\n158\n163\n160\n158\n161\nCore\ni7-5930k\ndense (batch=1)\n83.5\n72.8\n77.6\n70.6\n74.6\n77.0\nsparse (batch=1)\n42.3\n37.4\n36.5\n38.0\n37.4\n36.0\ndense (batch=64)\n85.4\n84.7\n101.6\n83.1\n97.1\n87.5\nsparse (batch=64)\n37.2\n37.1\n38\n39.5\n36.6\n38.2\nTegra K1\ndense (batch=1)\n5.1\n5.1\n5.4\n5.3\n5.3\n5.4\nsparse (batch=1)\n5.9\n6.1\n5.8\n5.6\n6.3\n5.8\ndense (batch=64)\n5.6\n5.6\n6.3\n5.4\n5.6\n6.3\nsparse (batch=64)\n5.0\n4.6\n5.1\n4.8\n4.7\n5.0\n14\n",
        "sentence": " The neural network model has been an emerging field during the past few years [5].",
        "context": "There have been other attempts to reduce the number of parameters of neural networks by replacing\nthe fully connected layer with global average pooling. The Network in Network architecture(Lin et al.,\n19.70%\n6.9MB\n35\u00d7\n7\nRELATED WORK\nNeural networks are typically over-parametrized, and there is signi\ufb01cant redundancy for deep learning\nmodels(Denil et al., 2013). This results in a waste of both computation and memory usage. There\nCPU, GPU and mobile GPU, compressed network has 3\u00d7 to 4\u00d7 layerwise speedup\nand 3\u00d7 to 7\u00d7 better energy ef\ufb01ciency.\n1\nINTRODUCTION\nDeep neural networks have evolved to the state-of-the-art technique for computer vision tasks"
    },
    {
        "title": "Building high-level features using large scale unsupervised learning",
        "author": [
            "Q. Le",
            "M. Ranzato",
            "R. Monga",
            "M. Devin",
            "K. Chen",
            "G. Corrado",
            "J. Dean",
            "A. Ng"
        ],
        "venue": "ICML\u201912, 2012.",
        "citeRegEx": "6",
        "shortCiteRegEx": null,
        "year": 2012,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " Distributed computing platforms have been widely recognized as the scalable, and easy-to-deploy measures [6].",
        "context": null
    },
    {
        "title": "Project adam: Building an efficient and scalable deep learning training system",
        "author": [
            "T. Chilimbi",
            "Y. Suzue",
            "J. Apacible",
            "K. Kalyanaraman"
        ],
        "venue": "OSDI\u201914, pp. 571\u2013582, 2014.",
        "citeRegEx": "7",
        "shortCiteRegEx": null,
        "year": 2014,
        "abstract": "",
        "full_text": "",
        "sentence": " Project Adam [7] describes the design and implementation of a distributed system comprised of commodity server machines to train large-scale deep learning models.",
        "context": null
    },
    {
        "title": "Singa: A distributed deep learning platform",
        "author": [
            "B.C. Ooi",
            "K.-L. Tan",
            "S. Wang",
            "W. Wang",
            "Q. Cai",
            "G. Chen",
            "J. Gao",
            "Z. Luo",
            "A.K. Tung",
            "Y. Wang",
            "Z. Xie",
            "M. Zhang",
            "K. Zheng"
        ],
        "venue": "MM \u201915, pp. 685\u2013688, 2015.",
        "citeRegEx": "8",
        "shortCiteRegEx": null,
        "year": 2015,
        "abstract": "",
        "full_text": "",
        "sentence": " SINGA [8] is a distributed deep learning system for training big models over large datasets.",
        "context": null
    },
    {
        "title": "Large scale distributed deep networks",
        "author": [
            "J. Dean",
            "G. Corrado",
            "R. Monga",
            "K. Chen",
            "M. Devin",
            "M. Mao",
            "M. Ranzato",
            "A. Senior",
            "P. Tucker",
            "K. Yang",
            "Q.V. Le",
            "A.Y. Ng"
        ],
        "venue": "NIPS\u201912, pp. 1232\u20131240, 2012.",
        "citeRegEx": "9",
        "shortCiteRegEx": null,
        "year": 2012,
        "abstract": "",
        "full_text": "",
        "sentence": " DistBelief [9] is a software framework that can utilize computing clusters with a good number of machines to train large models.",
        "context": null
    },
    {
        "title": "Deep learning with cots hpc systems",
        "author": [
            "A. Coates",
            "B. Huval",
            "T. Wang",
            "D. Wu",
            "B. Catanzaro",
            "N. Andrew"
        ],
        "venue": "ICML\u201913, vol. 28, pp. 1337\u2013 1345, May 2013.",
        "citeRegEx": "10",
        "shortCiteRegEx": null,
        "year": 2013,
        "abstract": "",
        "full_text": "",
        "sentence": " al [10] present a high-performance computing system with a cluster of GPU servers, using Infiniband interconnects and MPI.",
        "context": null
    },
    {
        "title": "Neural acceleration for gpu throughput processors",
        "author": [
            "A. Yazdanbakhsh",
            "J. Park",
            "H. Sharma",
            "P. Lotfi-Kamran",
            "H. Esmaeilzadeh"
        ],
        "venue": "MI- CRO\u201915, pp. 482\u2013493, 2015.",
        "citeRegEx": "11",
        "shortCiteRegEx": null,
        "year": 2015,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " NGPU [11] brings GPU accelerators together without hindering SIMT execution or adding excessive hardware overhead.",
        "context": null
    },
    {
        "title": "Large scale recurrent neural network on gpu",
        "author": [
            "B. Li",
            "E. Zhou",
            "B. Huang",
            "J. Duan",
            "Y. Wang",
            "N. Xu",
            "J. Zhang",
            "H. Yang"
        ],
        "venue": "IJCNN\u201914, pp. 4062\u20134069, 2014.",
        "citeRegEx": "12",
        "shortCiteRegEx": null,
        "year": 2014,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " [12] propose an efficient GPU implementation of the large-scale recurrent neural network and demonstrate the power of scaling up the recurrent neural network with GPUs.",
        "context": null
    },
    {
        "title": "Fast convolutional nets with fbfft: A gpu performance evaluation",
        "author": [
            "N. Vasilache",
            "J. Johnson",
            "M. Mathieu",
            "S. Chintala",
            "S. Piantino",
            "Y. LeCun"
        ],
        "venue": "ICLR\u201915, 2015.",
        "citeRegEx": "13",
        "shortCiteRegEx": null,
        "year": 2015,
        "abstract": "We examine the performance profile of Convolutional Neural Network training\non the current generation of NVIDIA Graphics Processing Units. We introduce two\nnew Fast Fourier Transform convolution implementations: one based on NVIDIA's\ncuFFT library, and another based on a Facebook authored FFT implementation,\nfbfft, that provides significant speedups over cuFFT (over 1.5x) for whole\nCNNs. Both of these convolution implementations are available in open source,\nand are faster than NVIDIA's cuDNN implementation for many common convolutional\nlayers (up to 23.5x for some synthetic kernel configurations). We discuss\ndifferent performance regimes of convolutions, comparing areas where\nstraightforward time domain convolutions outperform Fourier frequency domain\nconvolutions. Details on algorithmic applications of NVIDIA GPU hardware\nspecifics in the implementation of fbfft are also provided.",
        "full_text": "arXiv:1412.7580v3  [cs.LG]  10 Apr 2015\nPublished as a conference paper at ICLR 2015\nFAST CONVOLUTIONAL NETS WITH fbfft :\nA GPU PERFORMANCE EVALUATION\nNicolas Vasilache, Jeff Johnson, Michael Mathieu,\nSoumith Chintala, Serkan Piantino & Yann LeCun\nFacebook AI Research\n770 Broadway, New York, NY 10003, USA\n{ntv,jhj,myrhev,soumith,spiantino,yann}@fb.com\nABSTRACT\nWe examine the performance pro\ufb01le of Convolutional Neural Network (CNN)\ntraining on the current generation of NVIDIA Graphics Processing Units (GPUs).\nWe introduce two new Fast Fourier Transform convolution implementations: one\nbased on NVIDIA\u2019s cuFFT library, and another based on a Facebook authored\nFFT implementation, fbfft, that provides signi\ufb01cant speedups over cuFFT (over\n1.5\u00d7) for whole CNNs. Both of these convolution implementations are avail-\nable in open source, and are faster than NVIDIA\u2019s cuDNN implementation for\nmany common convolutional layers (up to 23.5\u00d7 for a synthetic kernel con\ufb01gura-\ntion). We discuss different performance regimes of convolutions, comparing areas\nwhere straightforward time domain convolutions outperform Fourier frequency\ndomain convolutions. Details on algorithmic applications of NVIDIA GPU hard-\nware speci\ufb01cs in the implementation of fbfft are also provided.\n1\nINTRODUCTION\nDeep convolutional neural networks (CNNs) have emerged as one of the most promising techniques\nto tackle large scale learning problems, whether in image and face recognition, audio and speech\nprocessing or natural language understanding. A convolutional layer within these networks pro-\nvides useful properties such as translation equivariance of activations. A limiting factor for use of\nconvolutional nets on large data sets was, until recently, their computational expense.\nKrizhevsky et al. (2012) demonstrated that training of large CNNs with millions of weights and\nmassive data sets is tractable when graphics processing units (GPUs) are properly put to use. Since\nthen, renewed interest in CNNs insuf\ufb02ated a fresh breath in various frameworks and implemen-\ntations, including Torch (Collobert et al. (2011a)), Theano (Bergstra et al. (2010)), cuda-convnet\n(Krizhevsky (2014)) and Caffe (Jia et al. (2014)). Many of these frameworks are based around\ncodes for NVIDIA GPUs using CUDA (Garland et al. (2008)).\nWe discuss our contributions to convolution performance on these GPUs, namely using Fast Fourier\nTransform (FFT) implementations within the Torch framework. We summarize the theory behind\ntraining convolutional layers both in the time and frequency domain in Section 2. We then detail\nour implementations. The \ufb01rst is based on NVIDIA\u2019s cuFFT and cuBLAS libraries (Section 3). We\nevaluate our relative performance to NVIDIA\u2019s cuDNN library (Chetlur et al. (2014)) on over 8, 000\ndifferent con\ufb01gurations (Section 4). We signi\ufb01cantly outperform cuDNN and other time domain\nconvolution implementations for a wide range of problem sizes.\nOur second implementation is motivated by limitations in using a black box library such as cuFFT in\nour application domain, which we describe. In reaction, we implemented a from-scratch open-\nsource implementation of batched 1-D FFT and batched 2-D FFT, called Facebook FFT (fbfft),\nwhich achieves over 1.5\u00d7 speedup over cuFFT for the sizes of interest in our application domain.\nThis implementation achieves GPU ef\ufb01ciency ratios of over 75% in certain cases. We describe an on-\ngoing effort to further improve the performance of our solution based on algorithmic tiling (Section\n6) before we conclude. Our implementation is released as part of the fbcuda and fbcunn open-\nsource libraries at http://github.com/facebook.\n1\nPublished as a conference paper at ICLR 2015\n2\nCONVOLUTION\nDiscrete convolution and cross-correlation are used in CNNs. We quickly summarize these and their\nimplementation, with a formulation mirroring Mathieu et al. (2013). Forward propagation (fprop)\ninputs are a set f of input feature planes xi, i \u2208f. These are cross-correlated1 with f \u2032 \u00d7 f different\n\ufb01lter kernel weights w(j,i), j \u2208f \u2032, i \u2208f, producing output feature planes yj, j \u2208f \u2032. Each input\nand output feature can be part of a minibatch S, so we have x(s,i) and y(s,j), i \u2208f, j \u2208f \u2032, s \u2208S:\ny(s,j) =\nX\ni\u2208f\nx(s,i) \u22c6w(j,i)\nThe feature planes f are reduced (summed) pointwise. For back-propagation (bprop), the gradient\nof the loss with respect to outputs are convolved with the kernels:\n\u2202L\n\u2202x(s,i)\n=\nX\nj\u2208f \u2032\n\u2202L\n\u2202y(s,j)\n\u2217w(j,i)\nReduction is over f \u2032 here. Finally, the kernel weights are updated using the gradient of the loss with\nrespect to the weights (accGrad):\n\u2202L\n\u2202w(j,i)\n=\nX\ns\u2208S\n\u2202L\n\u2202y(s,j)\n\u22c6x(s,i)\nReduction is over S here. For purposes of this paper, we use set symbols interchangeably to refer to\ntheir size: each input plane is a 2-D matrix of size h \u00d7 w, and each \ufb01lter kernel is a 2-D matrix of\nsize kh \u00d7 kw2. The output planes y(s,i) are of size (h \u2212kh + 1) \u00d7 (w \u2212kw + 1), and implement\nvalid-only convolution, as per MATLAB terminology. Input zero padding and input mirror padding\naround the margins of the input (ph, pw) can be optionally added.3\nA popular convolution implementation is to unroll the data until the computation is in the form of a\nlarge matrix multiplication (Chellapilla et al. (2006)). This is the strategy followed by many imple-\nmentors, since matrix multiplication is a well-tuned linear algebra primitive available on virtually\nany platform. While it is possible to provide instances of direct calculation that are faster than matrix\nunrolling (e.g., for large S, Krizhevsky (2014)), it is challenging to provide an implementation that\nis faster for more than just a small subset of possible convolution problems.\nIntroducing strides in this form of convolution (i.e., performing the convolution at every dh, dw-th\noffset) is a popular way to reduce the computational cost at the expense of precision. The memory\naccesses required are very similar but with fewer reuse opportunities. On the other hand, by the\nconvolution theorem, a convolution of two discrete signals can be performed with lower asymptotic\ncomplexity by performing the multiplication in the frequency domain. Applied to the forward pass,\nit becomes:\ny(s,j) =\nX\ni\u2208f\nx(s,i) \u22c6w(j,i) =\nX\ni\u2208f\nF\u22121 \u0000F(x(s,i)) \u25e6F(w(j,i))\u2217\u0001\nwhere \u2217denotes complex conjugation and \u25e6is the pointwise product. The discrete Fourier basis\nused is the largest of the two components convolved and the output.4 Linearity of the DFT allows\none to perform the sum above in the Fourier domain if desired. Applying the FFT then yields\na O(Sff \u2032n2 + (Sf + ff \u2032 + Sf \u2032)n2 log n) procedure in lieu of the original O(Sff \u2032n2k2), n =\nh = w, k = kh = kw. Similar transformations apply for the other two passes. We call this\na frequency domain convolution, in contrast to time domain convolution via direct computation.\n1Torch practice is that the forward pass is cross-correlation, hence the \u22c6.\n22-D can be extended to n-D, n \u22651.\n3Input size (h + ph) \u00d7 (w + pw), output size (h + ph \u2212kh + 1) \u00d7 (w + pw \u2212kw + 1).\n4(h \u00d7 w)-dimensional or even bigger for performance (Section 3.2).\n2\nPublished as a conference paper at ICLR 2015\nStrided convolutions via FFT can be implemented ef\ufb01ciently to obtain good performance Brosch &\nTam (2015). We do not consider those in this paper.\n3\nCUFFT CONVOLUTION IMPLEMENTATION\nIn this section we discuss implementation strategies using the NVIDIA cuFFT libraries and their\nef\ufb01ciency.\n3.1\nFFT CONVOLUTION DETAILS\nWe described the general formulation for the three types of convolutions in section 2. Here, we\nborrow the Torch naming convention: input for x(s,i); weight for w(j,i); output for y(s,j); gradOutput\nfor \u2202L/\u2202y(s,j); gradInput for \u2202L/\u2202x(s,i); and gradWeight for \u2202L/\u2202w(j,i). All are stored as single-\nprecision \ufb02oating point 4-D tensors in row-major layout, and are stored in memory using the so-\ncalled BDHW format. This is explicit in the expression InS\u00d7f\u00d7h\u00d7w, with input image row data as\nthe innermost or most varying dimension.\nTable 1 describes the in-order operations for FFT computation of the forward pass, using the\nFFT 2D and IFFT 2D operators and Cgemm matrix multiplication. Similar implementations fol-\nlow for the other two passes. The G pre\ufb01x denotes gradients. The F suf\ufb01x denotes C-valued fre-\nquency domain tensors; the rest are over R. The T suf\ufb01x denotes transposed tensors.\nTable 1: Implementation detail for forward propagation\nINPUT\nOUTPUT\nInS\u00d7f\u00d7h\u00d7w\nF F T 2D\n\u2212\u2212\u2212\u2212\u2212\u2192\nInFS\u00d7f\u00d7(h+ph)\u00d7(\u230aw+pw\n2\n\u230b+1)\nWeif \u2032\u00d7f\u00d7kh\u00d7kw\nF F T 2D\n\u2212\u2212\u2212\u2212\u2212\u2192\nWeiFf \u2032\u00d7f\u00d7(h+ph)\u00d7(\u230aw+pw\n2\n\u230b+1)\nInFS\u00d7f\u00d7(h+ph)\u00d7(\u230aw+pw\n2\n\u230b+1)\nT rans2D\n\u2212\u2212\u2212\u2212\u2212\u2212\u2192\nInFT(h+ph)\u00d7(\u230aw+pw\n2\n\u230b+1)\u00d7S\u00d7f\nWeiFf \u2032\u00d7f\u00d7(h+ph)\u00d7(\u230aw+pw\n2\n\u230b+1)\nT rans2D\n\u2212\u2212\u2212\u2212\u2212\u2212\u2192\nWeiFT(h+ph)\u00d7(\u230aw+pw\n2\n\u230b+1)\u00d7f \u2032\u00d7f\n(\nInFT(h+ph)\u00d7(\u230aw+pw\n2\n\u230b+1)\u00d7S\u00d7f\nWeiFT \u2217\n(h+ph)\u00d7(\u230aw+pw\n2\n\u230b+1)\u00d7f \u2032\u00d7f\nCgemm\n\u2212\u2212\u2212\u2192\nOutFT(h+ph)\u00d7(\u230aw+pw\n2\n\u230b+1)\u00d7S\u00d7f \u2032\nOutFT(h+ph)\u00d7(\u230aw+pw\n2\n\u230b+1)\u00d7S\u00d7f \u2032\nT rans2D\n\u2212\u2212\u2212\u2212\u2212\u2212\u2192\nOutFS\u00d7f \u2032\u00d7(h+ph)\u00d7(\u230aw+pw\n2\n\u230b+1)\nOutFS\u00d7f \u2032\u00d7(h+ph)\u00d7(\u230aw+pw\n2\n\u230b+1)\nIF F T 2D\n\u2212\u2212\u2212\u2212\u2212\u2212\u2192\nOutS\u00d7f \u2032\u00d7(h\u2212kh+1)\u00d7(w\u2212kw+1)\nExact tensor dimensions are also given above. By taking advantage of the Hermitian symmetry\nproperty of the 2-D DFT for R-valued inputs we only store about half the complex entries; the\nremaining can be obtained by complex conjugation. This results in array sizes such as \u230aw+pw\n2\n\u230b+ 1.\nWe also perform interpolation by zero-padding, which serves multiple purposes. First, it is necessary\nto handle boundary conditions.5 Second, it is required to interpolate all operands over the same\nFourier basis.6 Finally, padding has an impact on the FFT algorithm used in practice, as well as on\nthe \ufb02oating point operation count of non-FFT operations (Section 3.2).\nFollowing the conversion into frequency domain, we perform transpositions to prepare the tensors\nfor Cgemm matrix multiplication library calls. The transposition converts the BDHW layout into\nHWBD. The transposition is currently out-of-place and implemented using the Cgeam routine; we\nare also considering our own, in-place transposition routine. Cgemm library calls are performed\non transposed tensors in the frequency domain. Casting the operation as a Cgemm call allows us\nto bene\ufb01t from the heavily tuned cuBLAS routine. Eventually, we transpose the result back into\nthe BDHW format and perform a 2-D inverse FFT. At this point, the resulting real tensor, always\n5In this case, we typically have ph = \u230akh\n2 \u230band pw = \u230akw\n2 \u230b.\n6All tensors are zero-padded to (h + ph) \u00d7 (w + pw) before FFT 2D.\n3\nPublished as a conference paper at ICLR 2015\n(h + ph) \u00d7 (w + pw), is clipped to the appropriate \ufb01nal size: (h \u2212kh + 1) \u00d7 (w \u2212kw + 1) for fprop,\nh \u00d7 w for bprop, kh \u00d7 kw for accGrad.\n3.2\nCUFFT DESIGN SPACE\nWe now discuss implementation aspects we explored. Multiple factors in\ufb02uence the computational\nef\ufb01ciency of FFTs: transform size n, n\u2019s prime factor decomposition, and whether batched or it-\nerated single transforms are applied. In the deep learning domain, it is commonplace to deal with\nsmall sizes, n \u0338= 2k. If n has undesirable properties, ef\ufb01ciency can drop by an order of magnitude.7\ncuFFT implements FFTs with the ubiquitous Cooley-Tukey algorithm (Cooley & Tukey (1965))\nwhich takes advantage of trigonometric equalities to recursively decompose and reuse computa-\ntions. This is further discussed in the Supplement. Decomposition is built on specialized kernels of\n\ufb01xed sizes which correspond to the prime factor decomposition of n. cuFFT implements specialized\nbuilding blocks for radix sizes 2, 3, 5, 7, and for sizes n where 4|n, it can use more ef\ufb01cient kernels\nexploiting the conjugate symmetry property. When n does not admit a prime factor decomposition\nusing those radices only, the expensive Bluestein algorithm is used (Bluestein (1970)). Because our\nresults are used in the time domain, we can in fact zero-pad the image and kernel to perform the FFT\nat any larger size that may be handled more ef\ufb01ciently. Exploiting more ef\ufb01cient, larger sizes should\nbe balanced against the extra cost introduced in the subsequent transposition and matrix multiplica-\ntion steps. Table 4\u2019s last case is one in which the best tradeoff is not easily guessed. cuFFT also has\nbatched mode optimizations when multiple FFTs of the same size are being performed.\n3.3\nCUBLAS DESIGN SPACE\nThe cuBLAS library also comes with different implementations for batched and single operation\nmodes. We had the choice between 3 implementation options:\n\u2022 for larger batches over small matrices, the cublasCgemmBatched library call;\n\u2022 for smaller batches over larger matrices, multiple cublasCgemm calls from the host;\n\u2022 for intermediate batch and matrix sizes, devices of compute capability 3.5 and higher sup-\nport dynamic parallelism which allows CUDA kernels to launch other kernels. This can be\nbene\ufb01cial for many launches over small matrices.\nNote that the discussion above applies to multiplications after transposition. So the matrix size is\neither S \u00d7f, S \u00d7f \u2032 or f \u00d7f \u2032 and the number of such matrices is h\u00d7w. Vendor libraries are usually\noptimized for throughput and not latency, so we expect it to be more ef\ufb01cient for larger sizes along\ncritical dimensions (i.e., image size for the batch case and S \u00d7 f, S \u00d7 f \u2032 or f \u00d7 f \u2032 for the multiple\nkernel case). Due to build system limitations we were not able to experiment with the dynamic\nparallelism strategy; we leave this for future work.\nAt the system level, we use CUDA streams and buffering of all CUDA resources and intermediate\nbuffers to remove synchronization points across convolutions. We are mindful of memory consump-\ntion; to address this we keep one single buffered copy of each type of tensor involved. This behavior\nis tailored for a bulk synchronous execution of layers on a GPU and is not adapted for multiple\nasynchronous convolutions on the same GPU. The buffers are automatically expanded as required\nand reused as much as possible.\n3.4\nAUTOTUNING\nWe combine the above implementation with a simple autotuning strategy. We devise a strategy\nselection mechanism that runs once for each problem size and caches the fastest strategy out of a\nfew dozen for later reuse. The autotuning strategy explores different possible Fourier basis sizes that\ncan be decomposed in powers for which cuFFT has an ef\ufb01cient implementation. In other words, for\nan FFT dimension of size n, we explore the sizes i \u2208[n, 2\u230alog2 n\u230b] where i = 2a3b5c7d. When the\ninput size is a power of 2, the search space is reduced to a single point. In addition to Fourier basis\nsizes, we weigh in various cuBLAS calls and asynchronous modes.\n7http://docs.nvidia.com/cuda/cufft/index.html#accuracy-and-performance\n4\nPublished as a conference paper at ICLR 2015\n4\nCUFFT CONVOLUTION PERFORMANCE\n4.1\nPERFORMANCE VERSUS CUDNN: 8,232 CONFIGURATIONS\nWe compare our cuFFT convolution results against NVIDIA\u2019s cuDNN 1.0 library (Chetlur et al.\n(2014)), which contains one of the fastest, general purpose convolution methods for the GPU, using\nmatrix unrolling. It has decent performance for many problem sizes thanks to heavy autotuning of\ncuBLAS codes for different problems. It is a strong baseline for this reason.\nImage CNNs to date have for the most part used square input images and \ufb01lters, though rectan-\ngular \ufb01lters are valid for other problems (notably text CNNs, Collobert et al. (2011b)). Thus, we\nrestrict ourselves to a 5-D problem domain {S, f, f \u2032, n(= h = w), k(= kh = kw)}. Much of this\nspace is not used in practice. Some areas are perhaps over-emphasized (large S, small k) due to\ncurrent engineering concerns. We evaluate cuDNN vs cuFFT-based convolution for Table 2\u2019s 8, 232\ncon\ufb01gurations.8\nTable 2: Con\ufb01guration elements evaluated\nDIMENSION\nSIZES EVALUATED\nMinibatch size (S)\n1, 16, 64, 128\nInput \ufb01lters (f)\n1, 4, 16, 64, 96, 128, 256\nOutput \ufb01lters (f \u2032)\n1, 4, 16, 64, 96, 128, 256\nKernel h/w (k = kh = kw)\n3, 5, 7, 9, 11, 13\nOutput h/w (y = h \u2212kh + 1 = w \u2212kw + 1)\n1, 2, 4, 8, 16, 32, 64\nFigures 1-6 are performance summaries of cuFFT convolution versus cuDNN on a NVIDIA Tesla\nK40m, averaged across all three passes. The y-axis problem size corresponds to the minibatch size\nmultiplied by number of input and output planes (Sff \u2032); each one of these is a pass reduction\ndimension. Many possible combinations of S, f, f \u2032 may map to the same problem size. cuDNN per-\nformance varies to a greater degree than cuFFT across passes. This is due to the asymmetry of\nconvolution sizes in each pass, and the fact that a larger convolution kernel (as seen with gradient\naccumulation) is essentially free in the Fourier domain. Averaging the three passes together pro-\nvides a proxy for overall performance. The x-axis corresponds to output height/width. For deeper\nlayers in image CNNs, output size will decrease while f, f \u2032 will increase, so depth corresponds to\nmoving from the upper right to the lower left of the graph. Black areas in the chart are due to failed\ncuFFT runs, due to memory pressure or undetermined potential cuFFT 6.5 issues.\nFFT convolutions make large kernel sizes inexpensive, which make the performance of all three\npasses roughly equal (Table 4). On the other hand, zero-padding kh \u00d7kw to h\u00d7w penalizes smaller\nkernels compared to cuDNN. For 3 \u00d7 3 kernels (Figure 1), cuFFT performance is poor compared\nto cuDNN. The overhead of multiple kernel launches, streaming memory in and out multiple times,\nand zero-padding to the input size often outweigh the algorithmic advantage of FFT. However, for\nthe largest problem sizes, 3\u00d73 convolution via FFT can still be advantageous, with top speed 1.84\u00d7\nfaster than cuDNN. 5\u00d75 kernels (Figure 2) show an increasing dominance of the FFT strategy, with\ntop speed 5.33\u00d7 faster. The tendency is con\ufb01rmed for larger kernel sizes: at 13 \u00d7 13, maximum\nspeedup is 23.54\u00d7 over cuDNN.\n8Parameterized on output rather than input size h, w because the implied h = y + kh \u22121, w = y + kw \u22121\nwill be valid for any choice of kh, kw.\n5\nPublished as a conference paper at ICLR 2015\n1\n2\n4\n8\n16\n32\n64\n8388608\n4194304\n3145728\n2097152\n1572864\n1179648\n1048576\n786432\n589824\n524288\n393216\n262144\n196608\n147456\n131072\n98304\n65536\n49152\n32768\n12288\n4096\n512\n96\n1\noutput size\nproblem size\n1/16x\n16x\nspeedup\nFigure 1: 3 \u00d7 3 kernel (K40m)\n1\n2\n4\n8\n16\n32\n64\n8388608\n4194304\n3145728\n2097152\n1572864\n1179648\n1048576\n786432\n589824\n524288\n393216\n262144\n196608\n147456\n131072\n98304\n65536\n49152\n32768\n12288\n4096\n512\n96\n1\noutput size\nproblem size\nFigure 2: 5 \u00d7 5 kernel (K40m)\n1\n2\n4\n8\n16\n32\n64\n8388608\n4194304\n3145728\n2097152\n1572864\n1179648\n1048576\n786432\n589824\n524288\n393216\n262144\n196608\n147456\n131072\n98304\n65536\n49152\n32768\n12288\n4096\n512\n96\n1\noutput size\nproblem size\nFigure 3: 7 \u00d7 7 kernel (K40m)\n1\n2\n4\n8\n16\n32\n64\n8388608\n4194304\n3145728\n2097152\n1572864\n1179648\n1048576\n786432\n589824\n524288\n393216\n262144\n196608\n147456\n131072\n98304\n65536\n49152\n32768\n12288\n4096\n512\n96\n1\noutput size\nproblem size\nFigure 4: 9 \u00d7 9 kernel (K40m)\n1\n2\n4\n8\n16\n32\n64\n8388608\n4194304\n3145728\n2097152\n1572864\n1179648\n1048576\n786432\n589824\n524288\n393216\n262144\n196608\n147456\n131072\n98304\n65536\n49152\n32768\n12288\n4096\n512\n96\n1\noutput size\nproblem size\nFigure 5: 11 \u00d7 11 kernel (K40m)\n1\n2\n4\n8\n16\n32\n64\n8388608\n4194304\n3145728\n2097152\n1572864\n1179648\n1048576\n786432\n589824\n524288\n393216\n262144\n196608\n147456\n131072\n98304\n65536\n49152\n32768\n12288\n4096\n512\n96\n1\noutput size\nproblem size\nFigure 6: 13 \u00d7 13 kernel (K40m)\n6\nPublished as a conference paper at ICLR 2015\n4.2\nCNN PERFORMANCE\nIn table 3, we show performance for real CNNs, AlexNet (Krizhevsky et al. (2012)) and OverFeat\nfast (Sermanet et al. (2014)), comparing against cuDNN and cuda-convnet2 (ccn2) kernels in Torch.\nThe \ufb01rst layer uses cuDNN for the cuFFT runs because it is strided, but all other layers use cuFFT.\nThe timings include all convolutional layers of the network.\nTable 3: AlexNet and OverFeat fast performance (K40, ms)\nNETWORK\nKERNEL\nFPROP\nBPROP\nACCGRAD\nTOTAL\nAlexNet\ncuFFT\n94.34\n96.69\n93.20\n284.23\ncuDNN\n147.32\n167.79\n153.96\n469.07\nccn2\n99.03\n104.59\n103.29\n306.91\nOverFeat fast\ncuFFT\n375.65\n460.48\n397.85\n1233.98\ncuDNN\n459.06\n634.26\n508.02\n1601.35\nccn2\n433.11\n398.87\n450.82\n1282.80\nTable 4 shows the performance of the cuDNN and our cuFFT convolution implementation for some\nrepresentative layer sizes, assuming all the data is present on the GPU. Our speedups range from\n1.4\u00d7 to 14.5\u00d7 over cuDNN. Unsurprisingly, larger h, w, smaller S, f, f \u2032, kh, kw all contribute to\nreduced ef\ufb01ciency with the FFT. More surprisingly, we experience noticeable speedups on small\n3 \u00d7 3 kernels as long as the input tensor remains of small size. The optimal FFT sizes that au-\ntotuning \ufb01nds are reported in columns 2 and 3; note L5 padding being found by the autotuner.\nColumn 7 has the trillion equivalent time-domain reductions per second (single-precision \ufb02oating\npoint multiply-adds) achieved by our implementation on a NVIDIA Tesla K40m on CUDA 6.5.\nThis number represents the throughput a time-domain kernel needs to achieve in order to match our\nimplementation; it is computed as (Sff \u2032khkw(h \u2212kh + 1)(w \u2212kw + 1))/time. This is a metric\nto compare relative ef\ufb01ciency across problem and padding sizes. In the cases L2, L3 and L4, a time\ndomain convolution would need to exceed the K40m peak of 4.29 T\ufb02op/sec in order to match our\nthroughput.\n5\nfbfft IMPLEMENTATION\nThis section presumes familiarity with GPU architecture. Refer to the Supplement for details.\nWhen designing high-performance libraries, multiple objectives must be balanced against each\nother: memory latency/bandwidth tradeoffs, maximizing locality without sacri\ufb01cing too much par-\nallelism, good instruction mix, register usage and mapping strategy of computation and data to\nmemories and compute elements. A key principle is to design a set of leaf kernels with well-tuned\nin-register performance and reduce the larger problem to a combination of these kernels by data and\nloop tiling (Irigoin & Triolet (1988)) and recursive decompositions (Gunnels et al. (2001)). Since\nvendors have to sustain high performance for a large class of application domains, there exist pa-\nrameter con\ufb01gurations for which a carefully tuned approach signi\ufb01cantly outperforms vendor-tuned\nlibraries (Shin et al. (2010)). For common deep learning use, convolutional layers consist of many\nbatched small 2-D convolutions. These are tiny relative to DSP and HPC standards and put us in\na regime where (a) we fall outside of the highly tuned regime, (b) feature dimensions are often\nsmaller than GPU warp sizes and can often \ufb01t exclusively in registers rather than in shared memory\n(SMEM), and (c) we are very sensitive to latencies. We determined that it is possible to obtain better\nef\ufb01ciency than the existing batched cuFFT mode for CNNs.\n5.1\nLIMITATIONS OF CUFFT\nBecause the cuFFT library is a black box, zero-padding9 has to be explicitly embedded in the input\nand output arrays. The consequence is that one may need to allocate a duplicate, larger memory\n9This is different from the FFTW compatibility padding mode for in-place transforms.\n7\nPublished as a conference paper at ICLR 2015\nTable 4: Representative layer performance (S = 128, K40m)\nLAYER\nh + ph\nw + pw\ncuDNN\ncuFFT\nSPEEDUP\nTRED/s\nL1\nParams: f = 3, f \u2032 = 96, h = w = 128, kh = kw = 11\nfprop\n128\n128\n125.11 ms\n80.98 ms\n1.54\u00d7\n0.9\nbprop\n128\n128\n153.39 ms\n66.49 ms\n2.30\u00d7\n1.1\naccGrad\n128\n128\n155.07 ms\n69.63 ms\n2.22\u00d7\n1.05\nL2\nParams: f = 64, f \u2032 = 64, h = w = 64, kh = kw = 9\nfprop\n64\n64\n354.83 ms\n46.44 ms\n7.64\u00d7\n7.49\nbprop\n64\n64\n579.37 ms\n46.25 ms\n12.5\u00d7\n7.52\naccGrad\n64\n64\n416.34 ms\n47.03 ms\n8.85\u00d7\n7.40\nL3\nParams: f = 128, f \u2032 = 128, h = w = 32, kh = kw = 9\nfprop\n32\n32\n130.89 ms\n17.77 ms\n7.36\u00d7\n9.90\nbprop\n32\n32\n245.57 ms\n16.97 ms\n14.5\u00d7\n10.37\naccGrad\n32\n32\n154.96 ms\n17.00 ms\n9.29\u00d7\n10.34\nL4\nParams: f = 128, f \u2032 = 128, h = w = 16, kh = kw = 7\nfprop\n16\n16\n15.13 ms\n4.88 ms\n3.10\u00d7\n5.54\nbprop\n16\n16\n20.80 ms\n4.71 ms\n4.41\u00d7\n5.76\naccGrad\n16\n16\n18.17 ms\n4.70 ms\n3.86\u00d7\n5.75\nL5\nParams: f = 384, f \u2032 = 384, h = w = 13, kh = kw = 3\nfprop\n13\n14\n39.82 ms\n21.35 ms\n1.86\u00d7\n1.34\nbprop\n13\n14\n28.33 ms\n20.22 ms\n1.40\u00d7\n1.42\naccGrad\n13\n14\n47.84 ms\n21.26 ms\n2.25\u00d7\n1.35\nregion (only once) and copy data from non-padded tensors to padded tensors. This memory con-\nsumption and spurious copies affect latency signi\ufb01cantly. Instead, we devised an implementation for\nbatched 1-D FFT and 2-D FFT of sizes 2-256 and reaches up to 78% ef\ufb01ciency at 97.5% occupancy.\nWe also implemented an IFFT kernel based on our FFT kernel.\nIn our implementation we use clipping to conditionally load a value if reading within bounds or a\nconstant (0) otherwise. This is an approach used in automatic code generation tools such as Halide\n(Ragan-Kelley et al. (2013)) and relies on aggressive if-conversion properties of the CUDA compiler.\nIt allows for more ef\ufb01cient control \ufb02ow rather than using explicit loop prologues and epilogues. This\nmechanism does not require any additional memory allocation and is zero-copy; this is particularly\ndesirable in the latency sensitive mode.\nAdditionally, since cuFFT and cuBLAS are closed source, it is impossible to take advantage of algo-\nrithmic simpli\ufb01cations that may be available. For instance, in the forward pass of our computation as\nshown in Table 1, the result of the \ufb01rst cuFFT call is of the form S\u00d7f\u00d7(h+ph)\u00d7(\u230a(w+pw)/2\u230b+1).\nWith fbfft we return it in the form S \u00d7 f \u00d7 (\u230a(w + pw)/2\u230b+ 1) \u00d7 (h + ph) where the two inner-\nmost data dimensions are transposed. This allows us to remove a full data transposition from each\nof the FFT kernels. Another domain-speci\ufb01c optimization we have yet to explore is eliminating bit\nreversal portions of the FFT and IFFT. This can be done by performing the FFT with decimation in\nfrequency (DIF) and the IFFT with decimation in time (DIT), discussed in the Supplement.\n5.2\nWARP-LEVEL 1-D FFT AND 2-D FFT FOR SIZE n \u226432\nFor batched FFT of power of two sizes we view a single warp as a small distributed system with\nlockstep collective communication capabilities and we program it in a bulk-synchronous fashion\n(Valiant (1990)). We implement DIF and enforce the following invariants for the log2 n steps:\n\u2022 each warp thread originally loads one real element of the input vector and locally computes\none complex twiddle factor (i.e. a root of unity);\n\u2022 at each step, all warp threads exchange data with another thread in the warp in parallel and\nproduce a new value;\n8\nPublished as a conference paper at ICLR 2015\n\u2022 then, all warp threads exchange twiddle factors with another thread in the warp in parallel,\nand produce a new value.\nThe two bulk-synchronous exchanges can be written each with one warp-wide instruction. After\nthe log2 n steps, the FFT is computed and stored in a distributed and bit reversed manner within 1\nregister across a warp. For sizes n \u226432, bit reversal can be implemented with a single warp shuf\ufb02e.\nWe either load twiddle factors from device memory or compute them with the sincosf function\nonly once, and subsequently swap them within registers. This greatly reduces the reliance on either\nmemory bandwidth or on the special functional unit at the expense of a few additional registers.\nThe decision between explicitly loading twiddle factors from device memory or computing them is\na tradeoff between arithmetic intensity and memory bandwidth. For sizes 16 and 32 the arithmetic\npipeline is the bottleneck. Loading twiddle factors from memory for these two special sizes results\nin a performance increase of 15% and 20% respectively.\nThe discussion above applies to 1-D FFT and to each independent FFT within a larger 2-D FFT.\nA n-D Fourier transform is separable and can be implemented with sets of multiple 1-D FFT with\ntranspositions between each of these sets. In 2-D FFT R-to-C, the \ufb01rst set comprises n FFTs and the\nsecond set comprises n/2 + 1 FFTs by Hermitian symmetry. Following standard techniques Lyons\n(1996) we further pack 2 real FFTs into a single complex FFT . The extra 1 term in the quantity n/2+\n1 makes the computation ill-balanced and can bring down performance by lowering occupancy. We\nchose to dimension our kernels to have size n\u00d7(n/2) and introduce additional control \ufb02ow to handle\nthe border case. This results in 30% additional performance. We implement the transposition in\nSMEM across warps following Ruetsch & Micikevicius (2009). Data is already resident in registers\nso our main concerns are limiting SMEM usage to keep occupancy high, and limiting load/stores by\nusing vector instructions to avoid saturating the load-store unit (LSU).\n5.3\n1-D FFT AND 2-D FFT FOR SIZE 32 < n \u2264256\nWith size 32 as our building block, we extend our strategy to larger sizes. We use the same single\nwarp approach to compute a full 1-D FFT. The main difference is that the computation is now dis-\ntributed across multiple registers across threads in a warp (\u2308n/32\u2309Fourier coef\ufb01cients and twiddle\nfactors in registers per thread). Because we perform a full FFT per warp, a performance cross-over\nwhere cuFFT wins happens after register usage limits occupancy too much. We outperform 1-D\ncuFFT for n \u2264256, with a hard register limit at n = 512 (128 and 256 similarly for 2-D FFT). This\nis still well within our application domain. The following modi\ufb01cations handle multiple registers\nper thread:\n\u2022 Hermitian symmetry allows us to perform half the computation. There is a tradeoff be-\ntween adding control-\ufb02ow divergence and performing less work. At n \u226564, bene\ufb01ts from\nreduced computations dominate divergence losses;\n\u2022 we take advantage of trigonometric symmetries and twiddle factor distribution to compute\nonly a fraction of the roots of unity needed for each FFT, distributed with register to register\ncopies;\n\u2022 twiddle factor re-balancing across a warp and across registers requires a different imple-\nmentation. We managed to implement it fully within registers;\n\u2022 bit reversal occurs across registers and across warps. The high-order bits represent the reg-\nister while the low-order bits represent the warp. Without a sophisticated implementation,\nthis results in indirect addressing of registers which is costly. We implement a simple bit\nreversal in SMEM, which is an occupancy bottleneck at n \u2265256 for 1-D FFT.\nIn the 2-D FFT case, the intermediate transpose becomes signi\ufb01cantly more expensive. We exper-\nimented with various strategies to keep occupancy high, including partial transpositions within a\nwarp to use minimal amounts of SMEM.\n5.4\nDISCUSSION\nWe report the relative performance of our implementation fbfft compared to cuFFT for various\nbatch and input sizes of interest. The number of batches to consider depends on the dimension of\n9\nPublished as a conference paper at ICLR 2015\nCNN layers as well as any multi-GPU parallelization strategy that may be involved. At typical sizes\nof interest, fbfft is between 1.5\u00d7 and 5\u00d7 faster. We tried up to 4 million batches and at larger\nsizes gains stabilize around 1.4\u00d7 but ef\ufb01ciency goes down as more and more memory is used.\n4\n32\n128\n1024\n4096\n16384\n65536\nNumber of batches\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\n4.0\n4.5\nFBFFT Speedup\nFBFFT-1D Speedup at various sizes and batches\n8\n16\n32\n64\n128\n256\n4\n32\n128\n1024\n4096\n16384\n65536\nNumber of batches\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\n4.0\n4.5\nFBIFFT Speedup\nFBIFFT-1D Speedup at various sizes and batches\n8\n16\n32\n64\n128\n256\nFigure 7: fbfft-1D FFT and IFFT (K40m, cuFFT 6.5 @ 1x)\n4\n32\n128\n1024\n4096\n16384\n65536\nNumber of batches\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\n4.0\n4.5\nFBFFT Speedup\nFBFFT-2D Speedup at various sizes and batches\n8\n16\n32\n64\n128\n4\n32\n128\n1024\n4096\n16384\n65536\nNumber of batches\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\n4.0\n4.5\nFBIFFT Speedup\nFBIFFT-2D Speedup at various sizes and batches\n8\n16\n32\n64\n128\nFigure 8: fbfft-2D FFT and IFFT (K40m, cuFFT 6.5 @ 1x)\nFigure 7 shows the performance in the 1-D case. These numbers do not exercise our implicit zero-\ncopy padding, so we expect additional gains when we incorporate our FFT in the convolution. Our\nimplementation outperforms cuFFT for all cases of interest, more dramatically so for smaller batch\nsizes. Small batch sizes also correspond to the latency sensitive regime in Figures 1-6 for which\nthe cuFFT based implementation performs quite worse than cuDNN. We achieve 78% ef\ufb01ciency at\n97.5% occupancy for size 64 at batch size 16, 384, as reported by nvvp.\nFigure 8 shows the performance in the 2-D case. Relative performance gains for sizes 64 are more\nmodest than in the 1-D case, even losing to cuFFT at size 128 and small batch sizes. The magnitude\nof the relative gains at various batch sizes drops faster than in the 1-D case. Looking at the perfor-\nmance of the 32 \u00d7 32 FFT, we obtain 1.6\u00d7 speedup over cuFFT at 1, 024 batches. The same ratio is\nnot obtained until 16, 384 batches in 1-D FFT.10 When coupled with the tiling strategy in Section 6,\nwe emphasize that the sizes of interest are actually 8-64, and depend on kh, kw but not input h, w.\nBatch sizes can vary on the whole spectrum.\nWe interfaced fbfft into our convolution module and ran experiments with 3 \u00d7 3 kernels for the\n3 different convolution passes over inputs of sizes x = h = w, x \u2208{13, 16, 27, 32, 57, 64}. For\nproblem size, we used p = S = f = f \u2032, p \u2208{16, 32, 64, 128}. By swapping our FFT implemen-\ntation we observed an overall mean speedup of 1.51\u00d7 with standard deviation 0.21 and geometric\nmean 1.49\u00d7. The minimum speedup was 1.21\u00d7, despite sometimes performing more computations\n10This is not unexpected because these two computations perform the same number of \ufb02ops when accounting\nfor Hermitian symmetry, plus the fact that the ef\ufb01ciency of cuFFT increases while fbfft remains high but\nalmost constant.\n10\nPublished as a conference paper at ICLR 2015\nwith fbfft which can only interpolate to a power of 2. These experiments exercise the zero-copy\npadding and lower memory footprints of fbfft compared to cuFFT but do not yet re\ufb02ect additional\noptimizations such as tiling and bit twiddling elision.\n6\nCURRENT LIMITATIONS AND FUTURE WORK\nIn our current implementation, fbfft heavily relies on shuf\ufb02e instructions. In spite of a good\nef\ufb01ciency, we only utilize 60% of the available memory bandwidth. This is due to the load and store\ninstructions in our kernel competing with the shuf\ufb02e instructions for the Load-Store Unit (LSU). As\na consequence, our \ufb01rst bottleneck is the number of instructions issued on the LSU. For instance, on\nKepler (capability 3.5), the throughput for 32-bit \ufb02oating point multiply-add operations is 192 per\ncycle but the throughput for shuf\ufb02es is only 32. In the future we will investigate and release faster\nimplementations as they become available.\nTemporary memory overhead requirements are a common issue when performing convolutions in\nthe Fourier domain. In this \ufb01rst implementation, we introduced the following memory buffers to\nsupport our implementation:\n\u2022 for each of input, output and weight tensors we store 1 buffer for the frequency array and\n1 buffer for its complex transpose. These buffers store the Fourier representation and are\ngenerally limited by the weight tensor which is independent of the mini-batch size. Because\nof the global memory pressure we introduce, we reuse buffers at each layer and pass on the\nopportunity to (1) reuse 2 FFT results in each hidden layer, reducing the cost of forward\nFFTs by 33%; and (2) asynchronously precompute FFTs of the weight tensors and their\ngradients to better \ufb01ll the gpu utilization pipeline,\n\u2022 when using cuFFT we additionally pad the input, weight and output tensors explicitly to\nthe best performing common fft size\n\u2022 when using cuFFT additional temporary memory is reserved by each cufftPlan\n\u2022 with fbfft padding is implicit but and no temporary memory buffer is needed until we\nreach size 64. On the other hand, fbfft only supports square convolutions whose size is\na power of 2. As a consequence, too much padding could occur and adversely affect both\nperformance and memory consumption. The tiling strategy we describe next is a good way\nto circumvent the problem.\nAdditionally, we recently developed an in-place transposed batched CGEMM which permits the\nremoval of the complex transposed buffer. For this problem, a tool like MaxAS Lavin (2015) could\nbe valuable.\nfbfft provides the most gains over cuFFT at sizes 8-64. A tiling strategy for the input can be used\nto exploit this advantage. When the kernel is signi\ufb01cantly smaller than the input, we can decompose\na large convolution into several smaller ones. For simplicity, we consider 1D convolution on a\nsingle input plane, as it can trivially be extended. Let x be an input of size n, c a kernel of size w and\ny = x \u22c6c. We write x[i,j] for the vector formed by contiguous elements of x: {xi, xi+1, ..., xj\u22121}.\nLet d \u2264n. From the de\ufb01nition of the convolution, we have:\ny[i,i+d] = x[i,i+d+w] \u22c6c\nSo the convolution of the input of size n can be computed with \u230an/d\u230bconvolutions with inputs of\nsize d + w. The cost of the convolution goes down from O(n log(n)) to O(\u230an/d\u230b(d + w) log(d +\nw)) = O((n + w/d) log(d + w)). From this formula, we see that the optimal d is of the order of\nw, to get the complexity O(n log(w)). This strategy allows us to speed up forward and backward\npropagation. Tiling can also be used to reduce memory cost for temporary storage by not running all\nthe tiles in parallel (just the tiles which do run in parallel need their scratch space), at the potential\nexpense of parallelism or ef\ufb01ciency.\nFor the gradient accumulation, we cannot reuse this strategy, since it involves a larger convolution\nbetween an input x of size n and a kernel z = \u2202L\n\u2202y of size n \u2212w + 1. However, we have a similar\nformula:\n11\nPublished as a conference paper at ICLR 2015\n\u0012\u2202L\n\u2202c\n\u0013\nj\n=\nn\u22121\nX\ni=0\nxj+i \u00b7 zi =\n\u230an/d\u230b\u22121\nX\nk=0\nd\u22121\nX\ni=0\nxj+i+kd \u00b7 zi+kd +\nn\u22121\nX\ni=d\u230an/d\u230b\nxj+i \u00b7 zi\nAnd so\n\u0012\u2202L\n\u2202c\n\u0013\n=\n\u230an/d\u230b\u22121\nX\nk=0\nx[dk,(d+1)k+w\u22121] \u22c6z[dk,(d+1)k] + x[d\u230an/d\u230b,n] \u22c6z[d\u230an/d\u230b,n\u2212w+1]\nWe have a few other optimizations that are planned as well. Since much of the data we have is al-\nready available in registers or in shared memory, we are implementing our own in-place, in-register\ntranspose via recursive decomposition. The pointwise multiplications in the Fourier domain, es-\npecially with tiling, are rather small, so our own matrix multiplication routines integrated with the\nrest of the convolution kernel code might win over cuBLAS, and prevent the need for multiple\nCUDA kernel launches and their associated overhead. Finally, as mentioned earlier, bit reversal\nportions can be eliminated with the FFT using DIF and the IFFT using DIT.\n7\nCONCLUSION\nTo summarize, we achieve signi\ufb01cant gains in CNNs using FFTs, with a cuFFT convolution im-\nplementation achieving 1.4 \u00d7 \u221214.5\u00d7 speedups over cuDNN for common sizes. In reaction to\ncuFFT and cuBLAS limitations in the context of our speci\ufb01c application domain, we developed\nour own FFT implementation, fbfft, which is more suited to deep learning problem sizes (large\nbatches, small feature planes). fbfft itself is \u22651.4\u00d7 faster than cuFFT transforms for these prob-\nlems of interest. For convolution, it is faster than the cuFFT as well, with a mean of 1.51\u00d7 for sizes\nthat we wish to exploit.\nGiven our new ef\ufb01cient primitive for size 8-64 convolution, we are continuing work on bit twiddling,\ntransposition and pointwise multiplication optimizations, and continuing work on tiling to make the\ncomputational advantage at that size apply to larger convolution problems. These will all allow for\nreduced training time and use of ever larger and deeper CNNs.\nACKNOWLEDGMENTS\nWe would like to thank Julien Demouth from NVIDIA who suggested further improvements are still\npossible by virtue of the current implementation being LSU throughput-bound rather than memory-\nbound.\nREFERENCES\nBergstra, James, Breuleux, Olivier, Bastien, Fr\u00b4ed\u00b4eric, Lamblin, Pascal, Pascanu, Razvan, Des-\njardins, Guillaume, Turian, Joseph, Warde-Farley, David, and Bengio, Yoshua. Theano: a CPU\nand GPU math expression compiler. In Proceedings of the Python for Scienti\ufb01c Computing Con-\nference (SciPy), June 2010. Oral Presentation.\nBluestein, Leo I. A linear \ufb01ltering approach to the computation of discrete Fourier transform. Audio\nand Electroacoustics, IEEE Transactions on, 18(4):451\u2013455, December 1970. ISSN 0018-9278.\nBrosch, Tom and Tam, Roger C. Ef\ufb01cient training of convolutional deep belief networks in the\nfrequency domain for application to high-resolution 2d and 3d images. Neural Computation, 27\n(1):211\u2013227, 2015. doi: 10.1162/NECO a 00682. URL http://dx.doi.org/10.1162/\nNECO_a_00682.\nBurrus, C. Sidney.\nFast fourier transforms, 2008.\nURL http://cnx.org/contents/\n16e8e5e8-4f22-4b53-9cd6-a15b14f01ce4@5.6:16/Fast_Fourier_\nTransforms_(6x9_V.\nChellapilla, Kumar, Puri, Sidd, and Simard, Patrice. High Performance Convolutional Neural Net-\nworks for Document Processing. In Lorette, Guy (ed.), Tenth International Workshop on Frontiers\n12\nPublished as a conference paper at ICLR 2015\nin Handwriting Recognition, La Baule (France), October 2006. Universit\u00b4e de Rennes 1, Suvisoft.\nURL https://hal.inria.fr/inria-00112631. http://www.suvisoft.com.\nChetlur, Sharan, Woolley, Cliff, Vandermersch, Philippe, Cohen, Jonathan, Tran, John, Catanzaro,\nBryan, and Shelhamer, Evan. cudnn: Ef\ufb01cient primitives for deep learning. CoRR, abs/1410.0759,\n2014. URL http://arxiv.org/abs/1410.0759.\nCollobert, R., Kavukcuoglu, K., and Farabet, C. Torch7: A matlab-like environment for machine\nlearning. In BigLearn, NIPS Workshop, 2011a.\nCollobert, Ronan, Weston, Jason, Bottou, L\u00b4eon, Karlen, Michael, Kavukcuoglu, Koray, and Kuksa,\nPavel. Natural language processing (almost) from scratch. J. Mach. Learn. Res., 12:2493\u20132537,\nNovember 2011b. ISSN 1532-4435. URL http://dl.acm.org/citation.cfm?id=\n1953048.2078186.\nCooley, James W. and Tukey, John W. An algorithm for the machine calculation of complex fourier\nseries. Mathematics of computation, 19(90):297\u2013301, 1965.\nGarland, Michael, Le Grand, Scott, Nickolls, John, Anderson, Joshua, Hardwick, Jim, Morton,\nScott, Phillips, Everett, Zhang, Yao, and Volkov, Vasily. Parallel computing experiences with\ncuda. IEEE Micro, 28(4):13\u201327, July 2008. ISSN 0272-1732. doi: 10.1109/MM.2008.57. URL\nhttp://dx.doi.org/10.1109/MM.2008.57.\nGiles, Mike.\nCourse on cuda programming on nvidia gpus, lecture 3, 2014.\nURL http:\n//people.maths.ox.ac.uk/gilesm/cuda/lecs/lec3.pdf.\nGunnels, John A., Henry, Greg M., and van de Geijn, Robert A. A family of high-performance\nmatrix multiplication algorithms. In Proceedings of the International Conference on Computa-\ntional Sciences-Part I, ICCS \u201901, pp. 51\u201360, London, UK, UK, 2001. Springer-Verlag. ISBN\n3-540-42232-3. URL http://dl.acm.org/citation.cfm?id=645455.653765.\nIrigoin, F. and Triolet, R. Supernode partitioning. In Proceedings of the 15th ACM SIGPLAN-\nSIGACT Symposium on Principles of Programming Languages, POPL \u201988, pp. 319\u2013329, New\nYork, NY, USA, 1988. ACM. ISBN 0-89791-252-7. doi: 10.1145/73560.73588. URL http:\n//doi.acm.org/10.1145/73560.73588.\nJia, Yangqing, Shelhamer, Evan, Donahue, Jeff, Karayev, Sergey, Long, Jonathan, Girshick, Ross,\nGuadarrama, Sergio, and Darrell, Trevor. Caffe: Convolutional architecture for fast feature em-\nbedding. arXiv preprint arXiv:1408.5093, 2014.\nKrizhevsky, Alex.\ncuda-convnet2, 2014.\nURL https://code.google.com/p/cuda-\nconvnet2/.\nKrizhevsky, Alex, Sutskever, Ilya, and Hinton, Geoffrey E.\nImagenet classi\ufb01cation with deep\nconvolutional neural networks.\nIn Pereira, F., Burges, C.J.C., Bottou, L., and Weinberger,\nK.Q. (eds.), Advances in Neural Information Processing Systems 25, pp. 1097\u20131105. Cur-\nran Associates, Inc., 2012. URL http://papers.nips.cc/paper/4824-imagenet-\nclassification-with-deep-convolutional-neural-networks.pdf.\nLavin, Andrew. maxdnn: An ef\ufb01cient convolution kernel for deep learning with maxwell gpus.\nCoRR, abs/1501.06633, 2015. URL http://arxiv.org/abs/1501.06633.\nLyons, Richard G. Understanding Digital Signal Processing. Addison-Wesley Longman Publishing\nCo., Inc., Boston, MA, USA, 1st edition, 1996. ISBN 0201634678.\nMathieu, Micha\u00a8el, Henaff, Mikael, and LeCun, Yann.\nFast training of convolutional networks\nthrough ffts. CoRR, abs/1312.5851, 2013. URL http://arxiv.org/abs/1312.5851.\nRagan-Kelley, Jonathan, Barnes, Connelly, Adams, Andrew, Paris, Sylvain, Durand, Fr\u00b4edo, and\nAmarasinghe, Saman P. Halide: a language and compiler for optimizing parallelism, locality,\nand recomputation in image processing pipelines. In ACM SIGPLAN Conference on Program-\nming Language Design and Implementation, PLDI \u201913, Seattle, WA, USA, June 16-19, 2013, pp.\n519\u2013530, 2013. doi: 10.1145/2462156.2462176. URL http://doi.acm.org/10.1145/\n2462156.2462176.\n13\nPublished as a conference paper at ICLR 2015\nRuetsch, Greg and Micikevicius, Paulius. Optimizing matrix transpose in cuda. Technical report,\nNVIDIA Corp., January 2009.\nSermanet, Pierre, Eigen, David, Zhang, Xiang, Mathieu, Michael, Fergus, Rob, and LeCun, Yann.\nOverfeat: Integrated recognition, localization and detection using convolutional networks. In\nInternational Conference on Learning Representations (ICLR 2014). CBLS, April 2014. URL\nhttp://openreview.net/document/d332e77d-459a-4af8-b3ed-55ba.\nShin, Jaewook, Hall, Mary W., Chame, Jacqueline, Chen, Chun, Fischer, Paul F., and Hovland,\nPaul D. Speeding up nek5000 with autotuning and specialization. In Proceedings of the 24th\nACM International Conference on Supercomputing, ICS \u201910, pp. 253\u2013262, New York, NY, USA,\n2010. ACM. ISBN 978-1-4503-0018-6.\nValiant, Leslie G. A bridging model for parallel computation. Commun. ACM, 33(8):103\u2013111,\nAugust 1990. ISSN 0001-0782. doi: 10.1145/79173.79181. URL http://doi.acm.org/\n10.1145/79173.79181.\nVolkov, V. Better performance at lower occupancy. In GPU Technology Conference, 2010. URL\nhttp://www.cs.berkeley.edu/\u02dcvolkov/volkov10-GTC.pdf.\n14\nPublished as a conference paper at ICLR 2015\n8\nSUPPLEMENT\n8.1\nCUFFT CONVOLUTION PERFORMANCE BREAKDOWN\nWe show a breakdown of cuFFT convolution performance for the steps indicated in Table 1. The\ntimings do not add up to 100% of the reported performance in the previous table because we do not\nreport additional copies needed for zero-padding here. We also enforce force extra synchronizations\nto isolate the contribution of each operation. Abstracting from these details, the FFT and IFFT take\nup a signi\ufb01cant amount of compute resources, which we address in Section 5.\nTable 5: cuFFT convolution performance breakdown (K40m, ms)\nLAYER\nFFT A\nTRANS. A\nFFT B\nTRANS. B\nCGEMM\nTRANS. C\nIFFT C\nL1\nfprop\n0.86\n0.24\n1.13\n0.32\n15.13\n12.67\n36.46\nbprop\n0.86\n0.24\n34.55\n10.26\n12.62\n0.39\n1.19\naccGrad\n1.14\n0.32\n34.60\n10.26\n12.37\n0.26\n0.91\nL2\nfprop\n2.99\n0.98\n5.91\n2.03\n8.92\n1.67\n6.24\nbprop\n2.99\n0.98\n5.92\n2.03\n8.85\n1.67\n6.23\naccGrad\n5.94\n2.04\n5.93\n2.02\n8.38\n0.83\n3.15\nL3\nfprop\n3.07\n0.89\n3.08\n0.89\n4.40\n0.87\n3.49\nbprop\n3.08\n0.89\n3.07\n0.90\n4.05\n0.86\n3.48\naccGrad\n3.07\n0.89\n3.06\n0.89\n4.03\n0.87\n3.48\nL4\nfprop\n0.84\n0.24\n0.83\n0.24\n1.21\n0.24\n0.95\nbprop\n0.83\n0.24\n0.83\n0.24\n1.13\n0.23\n0.94\naccGrad\n0.84\n0.24\n0.82\n0.24\n1.10\n0.24\n0.95\nL5\nfprop\n7.07\n1.58\n2.39\n0.51\n6.23\n0.50\n2.54\nbprop\n7.07\n1.59\n2.40\n0.51\n5.59\n0.51\n2.54\naccGrad\n2.40\n0.51\n2.38\n0.52\n6.18\n1.54\n7.51\nIn the particular case of L1, the FFTs take more than 50% of the runtime. This is due to the wasteful\ninterpolation of the kernel tensor from a 11 \u00d7 11 up to 128 \u00d7 128, which is the minimal size to\ncompute the FFT of the input array without interpolation loss. In such cases, the tiling strategy we\nare developing (see section 6) will result in large additional performance gains.\n8.2\nFFT : DECIMATION IN TIME VS FREQUENCY\nA Fourier transform projects R and C-valued functions onto a harmonic orthogonal basis. The\ndiscrete Fourier transform of a vector {xk}, k \u2208[0, n \u22121] is the vector:\n{Xk} =\n\uf8eb\n\uf8ed\nn\u22121\nX\nj=0\nxjwkj\nn\n\uf8f6\n\uf8f8, k \u2208[0, n \u22121]\nwhere wj\nn = e\u22122\u03c0ij/n is the jth n-root of unity. The traditional radix-2 Cooley-Tukey algorithm\nrecursively decomposes the computation between an odd and even part:\n{Xk} =\n\uf8eb\n\uf8ed\n(n\u22121)/2\nX\nj=0\nxjwk(2j)\nn\n+\n(n\u22121)/2\nX\nj=0\nx2j+1wk(2j+1)\nn\n\uf8f6\n\uf8f8, k \u2208[1, n]\n15\nPublished as a conference paper at ICLR 2015\nFigure 9: DIT output ordered (left); DIF input ordered (right) (Burrus (2008))\nThis decomposition is called decimation in time (DIT). An alternate decomposition performs deci-\nmation in frequency (DIF):\n{Xk} =\n\uf8eb\n\uf8ed\n(n\u22121)/2\nX\nj=0\nxjwkj\nn +\nn\nX\nj=(n\u22121)/2\nxjwkj\nn\n\uf8f6\n\uf8f8, k \u2208[1, n]\nWhen n is a power of 2, both decimations recursively decompose into a perfectly balanced tree and\ntake advantage of the symmetry properties of the roots of unity. The data\ufb02ow graph for the radix-2\nFFT has a butter\ufb02y shape and is a good way of visualizing the computations. There is a symmetry\nbetween DIT and DIF in both the order of operations applied and in whether the input or the output\norder is shuf\ufb02ed (Figure 9).\n8.3\nGPU PROGRAMMING\nThere are a variety of references available that describe CUDA and NVIDIA\u2019s various GPU architec-\ntures (Garland et al. (2008)) which we won\u2019t discuss in detail, but the implementation of fbfft very\nmuch depends upon speci\ufb01cs of the Kepler GPU architecture.\nNVIDIA GPUs execute code at the granularity of a warp which is de\ufb01ned as a set of 32 threads\nin all existing architectures; each thread is assigned a lane within the warp. These threads execute\nin a SIMT (single instruction, multiple thread) fashion, meaning that a warp is an atomic unit of\nexecution. It holds a single program counter (PC) and can thus only execute a single instruction\nat a time across all of its threads. Collections of warps are brought together in blocks or CTAs,\nwhich together share a region of fast shared memory resident on chip. Blocks themselves can only\nexchange data via much slower global memory, resident on the GPU or in the host CPU\u2019s address\nspace.\nIndividual threads within a warp are free to take divergent paths, but since a single PC is present,\neach branch in the execution will be serialized. Threads that aren\u2019t participating in the branch in\nquestion are disabled. In other words, if all 32 threads were to take divergent code paths, we would\nobtain only 1/32\u00d7 of the computational ef\ufb01ciency.\nDivergent code paths are hard to avoid, but the NVIDIA instruction set has means to reduce their\ncost (Giles (2014)). One is with predicated instructions, which are used for small branches, in which\nall warp threads execute both parts of the branch, with non-participating threads having no side\neffects.\nBlock threads have access to a register \ufb01le, with up to 255 registers per thread for Kepler. Registers\nare allocated statically by the CUDA compiler. An important performance factor when writing\nCUDA kernels is that data should be kept in registers as much as possible to avoid communications.\n16\nPublished as a conference paper at ICLR 2015\nRegisters in CUDA are \u201caddressable\u201d: it is possible to declare a static array within registers and\noperate on its elements. The limitation is that all addressing should be performed using statically\ndetermined constants so the compiler can translate these accesses to a register number known at\ncompile time. Indirect addressing is also supported but results in copies to a local region within\nglobal memory, which essentially constitutes register spilling. Even with the presence of caches,\nusing local memory usually comes with a performance hit.11 As a consequence, we design our\nkernels using aggressive inlining, template parameters and unrolling directives to make all register\naccesses statically addressable.\nThe Kepler architecture introduced specialized shuf\ufb02e instructions to exchange data between regis-\nters within a warp synchronously, which avoids round-trips to shared or global memory. Interest-\ningly, these shuf\ufb02e instructions allow the dynamic indexing of an array held in registers, as long as\nthe array is distributed in a cyclic fashion across registers in each thread within a warp.\nfloat arr[3];\n...\n// This simulates a linear array float realArr[96]:\n// arr[0] holds elements 0-31 (lane i holds element i)\n// arr[1] holds elements 32-63 (lane i holds element 32 + i)\n// arr[2] holds elements 64-95 (lane i holds element 64 + i)\n// Example: all warp threads read value held at realArr[34]\nfloat val = __shfl(arr[1], 2); // \u20181\u2018 must be statically known\n// \u20182\u2018 can be dynamic\nMany warps run in parallel and can be switched by the GPU hardware at each cycle. When enough\nparallelism is available (measured in occupancy of the GPU as a \ufb01rst approximation), long latency\noperations are hidden thanks to fast context switching. Registers and shared memory come in \ufb01nite\nquantities on each GPU compute multiprocessor. These limited resources are partitioned by the\ncompiler and the hardware amongst computations at the level of a CUDA kernel. Increased usage\nof registers or of shared memory can reduce GPU occupancy, which limits the ability to hide long\nlatency operations. Reduced occupancy does not necessarily result in performance loss (Volkov\n(2010)). There are often non-obvious performance tradeoffs in increasing or decreasing threads per\nblock, shared memory per block or registers per thread that are hard to discover. This problem is\none of the many reasons why designing a one-size-\ufb01ts-all implementation that aims to be ef\ufb01cient\nfor any problem is dif\ufb01cult.\n11There are bleeding edge cases where a little local memory consumption helps performance; for instance,\nwhen restricting the number of registers per thread to increase occupancy.\n17\n",
        "sentence": " examine the performance profile using fbfft of CNN training on the current generation of GPU [13].",
        "context": "Facebook AI Research\n770 Broadway, New York, NY 10003, USA\n{ntv,jhj,myrhev,soumith,spiantino,yann}@fb.com\nABSTRACT\nWe examine the performance pro\ufb01le of Convolutional Neural Network (CNN)\ntraining on the current generation of NVIDIA Graphics Processing Units (GPUs).\nWe introduce two new Fast Fourier Transform convolution implementations: one\nbased on NVIDIA\u2019s cuFFT library, and another based on a Facebook authored\nCNN layers as well as any multi-GPU parallelization strategy that may be involved. At typical sizes\nof interest, fbfft is between 1.5\u00d7 and 5\u00d7 faster. We tried up to 4 million batches and at larger"
    },
    {
        "title": "Optimized deep belief networks on cuda gpus",
        "author": [
            "T. Li",
            "Y. Dou",
            "J. Jiang",
            "Y. Wang",
            "Q. Lv"
        ],
        "venue": "IJCNN\u201915, pp. 1\u20138, July 2015.",
        "citeRegEx": "14",
        "shortCiteRegEx": null,
        "year": 2015,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " describe an efficient DBN implementation on the GPU, including the pre-training and fine-tuning processes [14].",
        "context": null
    },
    {
        "title": "Geeps: Scalable deep learning on distributed gpus with a gpu-specialized parameter server",
        "author": [
            "H. Cui",
            "H. Zhang",
            "G.R. Ganger",
            "P.B. Gibbons",
            "E.P. Xing"
        ],
        "venue": "EuroSys \u201916, pp. 4:1\u20134:16, 2016.",
        "citeRegEx": "15",
        "shortCiteRegEx": null,
        "year": 2016,
        "abstract": "",
        "full_text": "",
        "sentence": " Recently, GeePS is a scalable deep learning architecture on distributed GPUs with specific parameters [15].",
        "context": null
    },
    {
        "title": "14.1 a 126.1mw real-time natural ui/ux processor with embedded deep-learning core for low-power smart glasses",
        "author": [
            "S. Park",
            "S. Choi",
            "J. Lee",
            "M. Kim",
            "J. Park",
            "H.J. Yoo"
        ],
        "venue": "ISSCC\u201916, pp. 254\u2013255, Jan 2016.",
        "citeRegEx": "16",
        "shortCiteRegEx": null,
        "year": 2016,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " To overcome the power consumption issue of the GPU and Cloud based frameworks, many developers seek solutions at hardware level [16], [17], [18].",
        "context": null
    },
    {
        "title": "14.5 eyeriss: An energy-efficient reconfigurable accelerator for deep convolutional neural networks",
        "author": [
            "Y.H. Chen",
            "T. Krishna",
            "J. Emer",
            "V. Sze"
        ],
        "venue": "ISSCC\u201916, pp. 262\u2013263, Jan 2016.",
        "citeRegEx": "17",
        "shortCiteRegEx": null,
        "year": 2016,
        "abstract": "",
        "full_text": "",
        "sentence": " To overcome the power consumption issue of the GPU and Cloud based frameworks, many developers seek solutions at hardware level [16], [17], [18].",
        "context": null
    },
    {
        "title": "14.6 a 1.42tops/w deep convolutional neural network recognition processor for intelligent ioe systems",
        "author": [
            "J. Sim",
            "J.S. Park",
            "M. Kim",
            "D. Bae",
            "Y. Choi",
            "L.S. Kim"
        ],
        "venue": "ISSCC\u201916, pp. 264\u2013265, Jan 2016.",
        "citeRegEx": "18",
        "shortCiteRegEx": null,
        "year": 2016,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " To overcome the power consumption issue of the GPU and Cloud based frameworks, many developers seek solutions at hardware level [16], [17], [18].",
        "context": null
    },
    {
        "title": "Origami: A convolutional network accelerator",
        "author": [
            "L. Cavigelli",
            "D. Gschwend",
            "C. Mayer",
            "S. Willi",
            "B. Muheim",
            "L. Benini"
        ],
        "venue": "GLSVLSI \u201915, pp. 199\u2013204, 2015.",
        "citeRegEx": "19",
        "shortCiteRegEx": null,
        "year": 2015,
        "abstract": "",
        "full_text": "",
        "sentence": " Origami [19] present a tape-out accelerator with silicon measurements of power-, area- and I/O efficiency.",
        "context": null
    },
    {
        "title": "Deep learning on fpgas: Past, present, and future",
        "author": [
            "G. Lacey",
            "G.W. Taylor",
            "S. Areibi"
        ],
        "venue": "arXiv preprint arXiv:1602.04283, 2016.",
        "citeRegEx": "20",
        "shortCiteRegEx": null,
        "year": 2016,
        "abstract": "The rapid growth of data size and accessibility in recent years has\ninstigated a shift of philosophy in algorithm design for artificial\nintelligence. Instead of engineering algorithms by hand, the ability to learn\ncomposable systems automatically from massive amounts of data has led to\nground-breaking performance in important domains such as computer vision,\nspeech recognition, and natural language processing. The most popular class of\ntechniques used in these domains is called deep learning, and is seeing\nsignificant attention from industry. However, these models require incredible\namounts of data and compute power to train, and are limited by the need for\nbetter hardware acceleration to accommodate scaling beyond current data and\nmodel sizes. While the current solution has been to use clusters of graphics\nprocessing units (GPU) as general purpose processors (GPGPU), the use of field\nprogrammable gate arrays (FPGA) provide an interesting alternative. Current\ntrends in design tools for FPGAs have made them more compatible with the\nhigh-level software practices typically practiced in the deep learning\ncommunity, making FPGAs more accessible to those who build and deploy models.\nSince FPGA architectures are flexible, this could also allow researchers the\nability to explore model-level optimizations beyond what is possible on fixed\narchitectures such as GPUs. As well, FPGAs tend to provide high performance per\nwatt of power consumption, which is of particular importance for application\nscientists interested in large scale server-based deployment or\nresource-limited embedded applications. This review takes a look at deep\nlearning and FPGAs from a hardware acceleration perspective, identifying trends\nand innovations that make these technologies a natural fit, and motivates a\ndiscussion on how FPGAs may best serve the needs of the deep learning community\nmoving forward.",
        "full_text": "arXiv:1602.04283v1  [cs.DC]  13 Feb 2016\nDeep Learning on FPGAs: Past, Present, and Future\nGrif\ufb01n Lacey\nUniversity of Guelph\n50 Stone Rd E\nGuelph, Ontario\nlaceyg@uoguelph.ca\nGraham Taylor\nUniversity of Guelph\n50 Stone Rd E\nGuelph, Ontario\ngwtaylor@uoguelph.ca\nShawki Areibi\nUniversity of Guelph\n50 Stone Rd E\nGuelph, Ontario\nsareibi@uoguelph.ca\nABSTRACT\nThe rapid growth of data size and accessibility in recent\nyears has instigated a shift of philosophy in algorithm de-\nsign for arti\ufb01cial intelligence. Instead of engineering algo-\nrithms by hand, the ability to learn composable systems au-\ntomatically from massive amounts of data has led to ground-\nbreaking performance in important domains such as com-\nputer vision, speech recognition, and natural language pro-\ncessing. The most popular class of techniques used in these\ndomains is called deep learning, and is seeing signi\ufb01cant\nattention from industry. However, these models require in-\ncredible amounts of data and compute power to train, and\nare limited by the need for better hardware acceleration\nto accommodate scaling beyond current data and model\nsizes. While the current solution has been to use clusters\nof graphics processing units (GPU) as general purpose pro-\ncessors (GPGPU), the use of \ufb01eld programmable gate arrays\n(FPGA) provide an interesting alternative. Current trends\nin design tools for FPGAs have made them more compatible\nwith the high-level software practices typically practiced in\nthe deep learning community, making FPGAs more accessi-\nble to those who build and deploy models. Since FPGA ar-\nchitectures are \ufb02exible, this could also allow researchers the\nability to explore model-level optimizations beyond what is\npossible on \ufb01xed architectures such as GPUs. As well, FP-\nGAs tend to provide high performance per watt of power\nconsumption, which is of particular importance for appli-\ncation scientists interested in large scale server-based de-\nployment or resource-limited embedded applications. This\nreview takes a look at deep learning and FPGAs from a\nhardware acceleration perspective, identifying trends and\ninnovations that make these technologies a natural \ufb01t, and\nmotivates a discussion on how FPGAs may best serve the\nneeds of the deep learning community moving forward.\n1.\nINTRODUCTION\nThe e\ufb00ects of machine learning on our everyday life are\nfar-reaching.\nWhether you are clicking through personal-\nized recommendations on websites, using speech to commu-\nnicate with your smart-phone, or using face-detection to get\nthe perfect picture on your digital camera, some form of\narti\ufb01cial intelligence is involved. This new wave of arti\ufb01-\ncial intelligence is accompanied by a shift in philosophy for\nalgorithm design.\nWhere past attempts at learning from\ndata involved much \u201cfeature engineering\u201d by hand using ex-\npert domain-speci\ufb01c knowledge, the ability to learn compos-\nable feature extraction systems automatically from massive\namounts of example data has led to ground-breaking per-\nformance in important domains such as computer vision,\nspeech recognition, and natural language processing. The\nstudy of these data-driven techniques is called deep learn-\ning, and is seeing signi\ufb01cant attention from two important\ngroups of the technology community: researchers, who are\ninterested in exploring and training these models to achieve\ntop performance across tasks, and application scientists, who\nare interested in deploying these models for novel, real world\napplications. However, both of these groups are limited by\nthe need for better hardware acceleration to accommodate\nscaling beyond current data and algorithm sizes.\nThe current state of hardware acceleration for deep learn-\ning is largely dominated by using clusters of graphics pro-\ncessing units (GPU) as general purpose processors (GPGPU)\n[18]. GPUs have orders of magnitude more computational\ncores compared to traditional general purpose processors\n(GPP), and allow a greater ability to perform parallel com-\nputations. In particular, the NVIDIA CUDA platform for\nGPGPU programming is most dominant, with major deep\nlearning tools utilizing this platform to access GPU accel-\neration [16, 26, 13, 19].\nMore recently, the open parallel\nprogramming standard OpenCL has gained traction as an\nalternative tool for heterogeneous hardware programming,\nwith interest from these popular tools gaining momentum.\nOpenCL, while trailing CUDA in terms of support in the\ndeep learning community, has two unique features which dis-\ntinguish itself from CUDA. First is the open source, royalty-\nfree standard for development, as opposed to the single ven-\ndor support of CUDA. The second is the support for a wide\nvariety of alternative hardware including GPUs, GPPs, \ufb01eld\nprogrammable gate-arrays (FPGA), and digital signal pro-\ncessors (DSP).\n1.1\nThe Case for FPGAs\nThe imminent support for alternative hardware is espe-\ncially important for FPGAs, a strong competitor to GPUs\nfor algorithm acceleration. Unlike GPUs, these devices have\na \ufb02exible hardware con\ufb01guration, and often provide better\nperformance per watt than GPUs for subroutines important\nto deep learning, such as sliding-windows computation [24].\nHowever, programming of these devices requires hardware\nspeci\ufb01c knowledge that many researchers and application\nscientists may not possess, and as such, FPGAs have been\noften considered a specialist architecture. Recently, FPGA\ntools have adopted software-level programming models, in-\ncluding OpenCL, which has made them a more attractive\noption for users trained in mainstream software development\npractices.\nFor researchers considering a variety of design tools, the\nselection criteria is typically related to having user-friendly\nsoftware development tools, \ufb02exible and upgradeable ways\nto design models, and fast computation to reduce the train-\ning time of large models.\nDeep learning researchers will\nbene\ufb01t from the use of FPGAs given the trend of higher\nabstraction design tools which are making FPGAs easier to\nprogram, the recon\ufb01gurability which allows customized ar-\nchitectures, and the large degree of parallelism which will\naccelerate execution speeds.\nFor application scientists, while similar tool level prefer-\nences exist, the emphasis for hardware selection is to maxi-\nmize performance per watt of power consumption, reducing\ncosts for large scale operations. Deep learning application\nscientists will bene\ufb01t from the use of FPGAs given the strong\nperformance per watt that typically accompanies the ability\nto customize the architecture for a particular application.\nFPGAs serve as a logical design choice which appeal to\nthe needs of these two important audiences.\nThis review\ntakes a look at the current state of deep learning on FP-\nGAs, as well as current developments which serve to bridge\nthese two technologies.\nAs such, this review serves three\nimportant purposes. First, it identi\ufb01es the opportunity that\nexists within the deep learning community for exploring new\nhardware acceleration platforms, and shows FPGAs as an\nideal choice. Next, it outlines the current state of FPGA\nsupport for deep learning, identifying potential limitations.\nFinally, it makes key recommendations of future directions\nfor FPGA hardware acceleration that would help in solving\nthe deep learning problems of tomorrow.\n2.\nDEEP LEARNING\nConventional approaches to arti\ufb01cial intelligence focused\non using computation to solve problems analytically, requir-\ning explicit knowledge about a given domain [10]. For sim-\nple problems this approach was adequate, as the programs\nengineered by hand were small, and domain experts could\ncarefully transform the modest amount of raw data into use-\nful representations for learning. However, advances in arti-\n\ufb01cial intelligence created interest in solving more complex\nproblems, where knowledge is not easily expressed explicitly.\nExpert knowledge about problems such as face recognition,\nspeech transcription, and medical diagnosis is di\ufb03cult to\nexpress formally, and conventional approaches to arti\ufb01cial\nintelligence failed to account for the implicit information\nstored in the raw data.\nMoreover, tremendous growth in\ndata acquisition and storage means that using this implicit\ninformation is more important than ever. Recently, these\ntypes of applications are seeing state-of-the-art performance\nfrom a class of techniques called deep learning, where this\nimplicit information is discovered automatically by learn-\ning task-relevant features from raw data.\nInterest in this\nresearch area has led to several recent reviews [29, 34, 9].\nThe \ufb01eld of deep learning emerged around 2006 after a\nlong period of relative disinterest around neural networks\nresearch. Interestingly, the early successes in the \ufb01eld were\ndue to unsupervised learning\u2013 techniques that can learn\nfrom unlabeled data. Speci\ufb01cally, unsupervised learning was\nused to \u201cpre-train\u201d (initialize) the layers of deep neural net-\nworks, which were thought at the time to be too di\ufb03cult\nto train with the usual methods, i.e. gradient backpropaga-\ntion. However, with the introduction of GPGPU computing\nand the availability of larger datasets towards the end of\nthe 2000\u2019s and into the current decade, focus has shifted al-\nmost exclusively to supervised learning. In particular, there\nare two types of neural network architectures that have re-\nceived most of the attention both in research and industry.\nThese are multi-layer perceptrons (MLP) and convolutional\nneural networks (CNN). Essentially all of the research on\nFPGA-based deep learning has focused on one of these ar-\nchitectures, and therefore we brie\ufb02y describe them below.\nBefore describing any speci\ufb01c architecture, however, it is\nworth noting several characteristics of most deep learning\nmodels and applications that, in general, make them well-\nsuited for parallellization using hardware accelerators.\nData parallelism \u2013 The parallelism inherent in pixel-based\nsensory input (e.g. images and video) manifests itself in op-\nerations that apply concurrently to all pixels or local regions.\nAs well, the most popular way of training models is not by\npresenting it with a single example at a time, but by pro-\ncessing \u201cminibatches\u201d of typically hundreds or thousands of\nexamples. Each example in a minibatch can be processed\nindependently.\nModel parallelism \u2013 These biologically-inspired models\nare composed of redundant processing units which can be\ndistributed in hardware and updated in parallel.\nRecent\nwork on accelerating CNNs using multiple GPUs has used\nvery sophisticated strategies to balance data and model-\nbased parallism such that di\ufb00erent parts of the architecture\nare parallelized in di\ufb00erent, but optimal ways [27].\nPipeline Parallelism \u2013 The feed-forward nature of com-\nputation in architectures like MLPs and CNNs means that\nhardware which is well suited to exploit pipeline parallelism\n(e.g. FPGAs) can o\ufb00er a particular advantage. While GPPs\nand GPUs rely on executing parallel threads on multiple\ncores, FPGAs can create customized hardware circuits which\nare deeply pipelined and inherently multithreaded.\n2.1\nMulti-layer Perceptrons\nSimple feed-forward deep networks are known as multi-\nlayer perceptrons (MLP), and are the backbone of deep\nlearning [11]. To describe these models using neural network\nterminology, we refer to the examples fed to these models\nas inputs, the predictions produced from these models as\noutputs, each modular sub-function as a layer with hidden\nlayers referring to those layers between the \ufb01rst (input) layer\nand last (output) layer, each scalar output of one of these\nlayers as a unit (analogous to a neuron in the biological\ninspiration of these models), and each connection between\nunits as a weight (analogous to a synapse), which de\ufb01ne the\nfunction of the model as they are the parameters that are\nadjusted during training [9]. Collections of units are some-\ntimes referred to as features, as to draw similarities to the\ntraditional idea of features in conventional machine learn-\ning, which were designed by domain experts.\nTo prevent\nthe entire network from collapsing to a linear transforma-\ntion, each unit applies an element-wise nonlinear operation\nto its input, with the most popular choice being the recti\ufb01ed\nlinear unit (ReLU). A basic MLP is illustrated in Figure 1.\n2.2\nConvolutional Neural Networks\nDeep convolutional neural networks (CNN) are currently\nthe most popular deep learning architectures, especially for\npixel-based visual recognition tasks. More formally, these\nnetworks are designed for data that has a measure of spatial\nor temporal continuity. This inspiration is drawn largely on\nFigure 1: Di\ufb00erences and similarities between MLPs and CNNs.\nwork from Hubel and Wiesel, who described the function of\na cat\u2019s visual cortex as being sensitive to small sub-regions of\nthe visual \ufb01eld [25]. Commonly, spatial continuity in data\nis found in images where a pixel at location (i, j) shares\nsimilar intensity or color properties to its neighbours in a\nlocal region of the image.\nCNNs are composed of various combinations of a few im-\nportant layer types. These layers, in comparison to MLPs,\nare constructed as a 2D arrangement of units called feature\nmaps. Convolution layers, analogous to the linear feature ex-\ntraction operation of MLPs, are parameterized by learnable\n\ufb01lters (kernels), which have local connections to a small re-\nceptive \ufb01eld of the input feature map and shared at all loca-\ntions of the input. Feature extraction in a CNN amounts to\nconvolution with these \ufb01lters. Pooling layers apply a simple\nreduction operation (e.g. a max or average) to local regions\nof the feature maps.\nThis reduces the size of the feature\nmaps, which is favorable to computation and reducing pa-\nrameters, but also yields a small amount of shift-invariance.\nFinally, in recognition applications CNNs typically apply\none or more fully connected layers (the same layers used in\nMLPs) towards the output layer in order to reduce the spa-\ntially and/or temporally organized information in feature\nmaps to a decision, such as a classi\ufb01cation or regression.\n3.\nFPGAs\nTraditionally, when evaluating hardware platforms for ac-\nceleration, one must inevitably consider the trade-o\ufb00be-\ntween \ufb02exibility and performance. On one end of the spec-\ntrum, general purpose processors (GPP) provide a high de-\ngree of \ufb02exibility and ease of use, but perform relatively inef-\n\ufb01ciently. These platforms tend to be more readily accessible,\ncan be produced cheaply, and are appropriate for a wide va-\nriety of uses and reuses. On the other end of the spectrum,\napplication speci\ufb01c integrated circuits (ASIC) provide high\nperformance at the cost of being in\ufb02exible and more di\ufb03cult\nto produce. These circuits are dedicated to a speci\ufb01c appli-\ncation, and are expensive and time consuming to produce.\nFPGAs serve as a compromise between these two extremes.\nThey belong to a more general class of programmable logic\ndevices (PLD) and are, in the most simple sense, a recon\ufb01g-\nurable integrated circuit. As such, they provide the perfor-\nmance bene\ufb01ts of integrated circuits, with the recon\ufb01gurable\n\ufb02exibility of GPPs. At a low-level, FPGAs can implement\nsequential logic through the use of \ufb02ip-\ufb02ops (FF) and com-\nbinational logic through the use of look-up tables (LUT).\nModern FPGAs also contain hardened components for com-\nmonly used functions such as full processor cores, communi-\ncation cores, arithmetic cores, and block RAM (BRAM). In\naddition, current FPGA trends are tending toward a system-\non-chip (SoC) design approach, where ARM coprocessors\nand FPGAs are commonly found on the same fabric. The\ncurrent FPGA market is dominated by Xilinx and Altera,\naccounting for a combined 85 percent market share [8]. In\naddition, FPGAs are rapidly replacing ASICs and applica-\ntion speci\ufb01c standard products (ASSP) for \ufb01xed function\nlogic. The FPGA market is expected to reach the $10 bil-\nlion mark by 2016 [8].\nFor deep learning, FPGAs provide an obvious potential\nfor acceleration above and beyond what is possible on tra-\nditional GPPs. Software-level execution on GPPs rely on\nthe traditional Von Neumann architecture, which stores in-\nstructions and data in external memory to be fetched when\nneeded. This is the motivation for caches, which alleviate\nmuch of the expensive external memory operations [8]. The\nbottleneck in this architecture is the processor and memory\ncommunication, which severely cripples GPP performance,\nespecially for the memory-bound techniques frequently re-\nquired in deep learning. In comparison, the programmable\nlogic cells on FPGAs can be used to implement the data\nand control path found in common logic functions, which\ndo not rely on the Von Neumann architecture.\nThey are\nalso capable of exploiting distributed on-chip memory, as\nwell as large degrees of pipeline parallelism, which \ufb01t nat-\nurally with the feed-forward nature deep learning methods.\nModern FPGAs also support partial dynamic recon\ufb01gura-\ntion, where part of the FPGA can be reprogrammed while\nanother part of the FPGA is being used. This can have im-\nplications for large deep learning models, where individual\nlayers could be recon\ufb01gured on the FPGA while not dis-\nrupting ongoing computation in other layers.\nThis would\naccommodate models which may be too large to \ufb01t on a sin-\ngle FPGA, and also alleviate expensive global memory reads\nby keeping intermediate results in local memory.\nMost importantly, when compared to GPUs, FPGAs o\ufb00er\na di\ufb00erent perspective on what it means to accelerate designs\non hardware. With GPUs and other \ufb01xed architectures, a\nsoftware execution model is followed, and structured around\nexecuting tasks in parallel on independent compute units.\nAs such, the goal in developing deep learning techniques for\nGPUs is to adapt algorithms to follow this model, where\ncomputation is done in parallel, and data interdependence\nis ensured. In contrast, FPGA architecture is tailored for\nthe application. When developing deep learning techniques\nfor FPGAs, there is less emphasis on adapting algorithms\nfor a \ufb01xed computational structure, allowing more freedom\nto explore algorithm level optimizations. Techniques which\nrequire many complex low-level hardware control operations\nwhich cannot be easily implemented in high-level software\nlanguages are especially attractive for FPGA implementa-\ntions.\nHowever, this \ufb02exibility comes at the cost of large\ncompile (place and route) times, which is often problematic\nfor researchers who need to quickly iterate through design\ncycles.\nIn addition to compile time, the problem of attracting\nresearchers and application scientists, who tend to favour\nhigh-level programming languages, to develop for FPGAs\nhas been especially di\ufb03cult. While it is often the case that\nbeing \ufb02uent in one software language means one can eas-\nily learn another, the same cannot be said for translating\nskills to hardware languages. The most popular languages\nfor FPGAs have been Verilog and VHDL, both examples of\nhardware description languages (HDL). The main di\ufb00erence\nbetween these languages and traditional software languages,\nis that HDL is simply describing hardware, whereas software\nlanguages such as C are describing sequential instructions\nwith no need to understand hardware level implementation\ndetails. Describing hardware e\ufb03ciently requires a working\nknowledge of digital design and circuits, and while some of\nthe low level implementation decisions can be left to auto-\nmatic synthesizer tools, this does not always result in e\ufb03-\ncient designs. As a result, researchers and application sci-\nentists tend to opt for a software design experience, which\nhas matured to support a large assortment of abstractions\nand conveniences that increase the productivity of program-\nmers. These trends have pushed the FPGA community to\nnow favour design tools with a high-level of abstraction.\n3.1\nHigh-Level Abstraction Tools\nBoth Xilinx and Altera have favoured the use of high-\nlevel design tools which abstract away many of the chal-\nlenges of low level hardware programming. These tools are\ncommonly termed high-level synthesis (HLS) tools, which\ntranslate high-level designs into low-level register-transfer\nlevel (RTL) or HDL code. A good overview of HLS tools is\npresented in [8], where they are grouped into \ufb01ve main cat-\negories: model-based frameworks, high-level language based\nframeworks, HDL-like languages, C-based frameworks, and\nparallel computing frameworks (i.e. CUDA/OpenCL). While\nit is important to understand these di\ufb00erent types of ab-\nstraction tools, this review focuses on parallel computing\nframeworks, as they provide the most sensible path to join\ndeep learning and FPGAs.\n3.2\nOpenCL\nOpenCL is an open source, standardized framework for\nalgorithm acceleration on heterogeneous architectures. As a\nC-based language (C99), programs written in OpenCL can\nbe executed transparently on GPPs, GPUs, DSPs, and FP-\nGAs. Similar to CUDA, OpenCL provides a standard frame-\nwork for parallel programming, as well as low-level access to\nhardware. While both CUDA and OpenCL provide similar\nfunctionality to programmers, key di\ufb00erences between them\nhave left most people divided. Since CUDA is the current\nchoice for most popular deep learning tools, it is impor-\ntant to discuss these di\ufb00erences in detail, in the interest of\ndemonstrating how OpenCL could be used for deep learning\nmoving forward.\nThe major di\ufb00erence between OpenCL and CUDA is in\nterms of ownership. CUDA is a proprietary framework cre-\nated by the hardware manufacturer NVIDIA, known for\nmanufacturing high performance GPUs. OpenCL is open-\nsource, royality-free, and is maintained by the Khronos group.\nThis gives OpenCL a unique capability compared to CUDA:\nFigure 2: Proposed deployment \ufb02ow for image classi\ufb01cation using FPGA for acceleration.\nOpenCL can support programming a wide variety of hard-\nware platforms, including FPGAs.\nHowever, this \ufb02exibil-\nity comes at a cost, where all supported platforms are not\nguaranteed to support all OpenCL functions. In the case\nof FPGAs, only a subset of OpenCL functions are currently\nsupported.\nWhile a detailed comparison of OpenCL and\nCUDA is outside the scope of this paper, performance of\nboth frameworks has been shown to be very similar for given\napplications [22].\nBeginning in late 2013, both Altera and Xilinx started to\nadopt OpenCL SDKs for their FPGAs [21, 7]. This move\nallowed a much wider audience of software developers to\ntake advantage of the high-performance and low power ben-\ne\ufb01ts that come with designing for FPGAs.\nConveniently,\nboth companies have taken similar approaches to adopting\nOpenCL for their devices.\n3.3\nProposed Design Flow for Deep Learning\nDevelopment\nTo successfully integrate FPGAs into deep learning de-\nsign \ufb02ows, the needs of researchers and application scien-\ntists, who are familiar with GPU design \ufb02ows, need to be\nconsidered.\nWhile this is a challenge given the architec-\ntural di\ufb00erences of FPGAs and GPUs, we believe this goal\nis achievable.\nThe main challenge is related to design compile time.\nBoth Altera and Xilinx support primarily o\ufb04ine compil-\ning for OpenCL kernels. The main reason for this is that\nOpenCL kernel compilation time for both vendors is on the\norder of tens of minutes to hours, whereas compiling generic\nOpenCL kernels for GPPs/GPUs is on the order of millisec-\nonds to seconds. Obviously, this makes iterating through the\ndesign phase challenging if the compilation time is hours\nfor each design iteration. However, this is not necessarily\nfutile for deep learning, as deep learning tools often reuse\nthe same pre-compiled kernels during the design phase. As\nwell, deep learning tools supported by CUDA use a similar\nmethodology, as CUDA employs a just-in-time compiling\napproach.\nAs such, doing one-time o\ufb04ine compilation of\ncommonly used deep learning kernels is a reasonable com-\npromise that does not limit application scientists who are\nusually not designing these kernels, but just interested in\nusing them. Figure 2 shows an example \ufb02ow for deployment\nof an image classi\ufb01cation model.\nFor researchers who do have an interest in kernel design,\nboth Altera and Xilinx support integrated kernel pro\ufb01ling,\ndebugging, and optimization in their OpenCL tools. These\nfeatures give researchers the ability to speed up and opti-\nmize the development of kernels. As well, both companies\nhave support for kernel simulation in software, which can cir-\ncumvent the hassle of dealing with long compile times when\ndebugging non-hardware issues such as semantic or syntax\nerrors.\n4.\nA REVIEW OF CNNS ON FPGAS\nOne of the most limiting hardware realizations for deep\nlearning techniques on FPGAs is design size.\nThe trade-\no\ufb00between design recon\ufb01gurability and density means that\nFPGA circuits are often considerably less dense than hard-\nware alternatives, and so implementing large neural net-\nworks has not always been possible. However, as modern\nFPGAs continue to exploit smaller feature sizes to increase\ndensity, and incorporate hardened computational units along-\nside generic FPGA fabric, deep networks have started being\nFigure 3: Timeline of important events in FPGA deep learning research.\nimplemented on single FPGA systems. A brief timeline of\nimportant events in FPGA deep learning research is seen in\nFigure 3.\nThe \ufb01rst FPGA implementations of neural networks be-\ngan appearing in the early 1990\u2019s, with the \ufb01rst implemen-\ntation credited to Cox et al.\nin 1992 [20].\nHowever, the\n\ufb01rst FPGA implementations of CNNs began appearing a\nfew years later. Cloutier et al. were among the \ufb01rst to ex-\nplore these e\ufb00orts, but were strongly limited by FPGA size\nconstraints at the time, leading to the use of low-precision\narithmetic [17]. In addition, because FPGAs at this time did\nnot contain the dense hardened multiply-accumulate (MAC)\nunits that are present in today\u2019s FPGAs, arithmetic was also\nvery slow in addition to being resource expensive. Since this\ntime, FPGA technology has changed signi\ufb01cantly. Most no-\ntably, there has been a large increase in the density of FPGA\nfabric, motivated by the decreasing feature (transistor) size,\nas well as an increase in the number of hardened compu-\ntational units present in FPGAs.\nState-of-the-art FPGA\nimplementations of CNNs take advantage of both of these\ndesign improvements.\nTo the best of our knowledge, state-of-the-art performance\nfor forward propagation of CNNs on FPGAs was achieved\nby a team at Microsoft.\nOvtcharov et al. have reported\na throughput of 134 images/second on the ImageNet 1K\ndataset [28], which amounts to roughly 3x the throughput\nof the next closest competitor, while operating at 25 W on a\nStratix V D5 [30]. This performance is projected to increase\nby using top-of-the-line FPGAs, with an estimated through-\nput of roughly 233 images/second while consuming roughly\nthe same power on an Arria 10 GX1150.\nThis is com-\npared to high-performing GPU implementations (Ca\ufb00e +\ncuDNN), which achieve 500-824 images/second, while con-\nsuming 235 W. Interestingly, this was achieved using Microsoft-\ndesigned FPGA boards and servers, an experimental project\nwhich integrates FPGAs into datacenter applications. This\nproject has claimed to improve large-scale search engine per-\nformance by a factor of two, showing promise for this type\nof FPGA application [4].\nOther strong e\ufb00orts include the design proposed by Zhang\net al., referenced above as the closest competitor achieving a\nthroughput of 46 images/second on a Virtex 7 485T, with an\nunreported power consumption [36]. In this paper, Zhang et\nal. show their work to outperform most of the main strong\ncompetitors in this \ufb01eld, including [14, 15, 23, 32, 33]. Most\nof these implementations contain architecturally similar de-\nsigns, commonly using o\ufb00-chip memory access, con\ufb01gurable\nsoftware layers, bu\ufb00ered input and output, and many paral-\nlel processing elements implemented on FPGA fabric (com-\nmonly used to perform convolution). However, important\nFPGA speci\ufb01c di\ufb00erences exist, such as using di\ufb00erent mem-\nory sub-systems, data transfer mechanisms, soft-cores, LUT\ntypes, operation frequencies, and entirely di\ufb00erent FPGAs,\nto name a few. As a result, it is hard to determine speci\ufb01c\noptimal architecture decisions, as more research is needed.\nSince pre-trained CNNs are algorithmically simple and\ncomputationally e\ufb03cient, most FPGA e\ufb00orts have involved\naccelerating the forward propagation of these models, and\nreporting on the achieved throughput. This is often of most\nimportance to application engineers who wish to use pre-\ntrained networks to process large amounts of data as quickly\nand e\ufb03ciently as possible. However, this only represents one\naspect of CNN design considerations for FPGAs, as accel-\nerating backward propagation on FPGAs is also an area of\ninterest. Paul et al. were the \ufb01rst to completely parallelize\nthe learning phase on a Virtex E FPGA in 2006 [31].\n5.\nLOOKING FORWARD\nThe future of deep learning on FPGAs, and in general,\nis largely dependant on scalability. For these techniques to\nsucceed on the problems of tomorrow, they must scale to\naccommodate data sizes and architectures that continue to\ngrow at an incredible rate. FPGA technology is adapting to\nsupport this trend, as the hardware is headed toward larger\nmemory, smaller feature sizes, and interconnect improve-\nments to accommodate multi-FPGA con\ufb01gurations.\nThe\nIntel acquisition of Altera, along with the partnership of\nIBM and Xilinx, indicate a change in the FPGA landscape\nwhich may also see the integration of FPGAs in consumer\nand data center applications in the very near future.\nIn\naddition, design tools will likely tend toward higher levels\nof abstraction and software-like experiences, in an e\ufb00ort to\nattract a wider technical range of users.\n5.1\nPopular Deep Learning Software Tools\nOf the most popular software packages for deep learning,\nseveral have began to take notice of the need for OpenCL\nsupport in addition to CUDA support. This support will\nmake FPGAs more accessible for the purposes of deep learn-\ning. While, to our knowledge, no deep learning tools exist\nyet which explicitly support FPGAs, the following list (sum-\nmarized in Table 1) details some of the notable OpenCL\ne\ufb00orts which move these tools in that direction:\n\u2022 Ca\ufb00e, developed by the Berkeley Vision and Learning\nTable 1: Overview of Deep Learning Frameworks with OpenCL Support\nTool\nCore Language\nBindings\nOpenCL\nUser Base\nCa\ufb00e\nC++\nPython\nMATLAB\nPartial Support\nLarge\nTorch\nLua\n-\nPartial Support\nLarge\nTheano\nPython\n-\nMinimal Support\nLarge\nDeepCL\nC++\nPython\nLua\nFull Support\nModerate\nCenter, has uno\ufb03cial support for OpenCL under the\nname project GreenTea [2].\nThere is also an AMD\nversion of Ca\ufb00e that supports OpenCL [1].\n\u2022 Torch, a scienti\ufb01c computing framework written in\nLua, is widely used and has uno\ufb03cial support for OpenCL\nunder the project CLTorch [6].\n\u2022 Theano, developed by the University of Montreal, has\nuno\ufb03cial support for OpenCL under the work-in-progress\ngpuarray backend [5].\n\u2022 DeepCL is an OpenCL library to train deep convo-\nlutional neural networks, developed by Hugh Perkins\n[3].\nFor those new to this \ufb01eld who are looking to choose be-\ntween these tools, our recommendation is to start with Ca\ufb00e\ndue to its popularity, support, and easy to use interface. As\nwell, using Ca\ufb00e\u2019s \u201cmodel zoo\u201d repository, it is easy to ex-\nperiment with popular pre-trained models.\n5.2\nIncreasing Degrees of Freedom for Train-\ning\nWhile one may expect the process of training machine\nlearning algorithms to be fully autonomous, in practice there\nare tunable hyper-parameters that need to be adjusted. This\nis especially true for deep learning, where complexity of the\nmodel in terms of number of parameters is often accompa-\nnied by many possible combinations of hyper-parameters.\nThe number of training iterations, the learning rate, mini-\nbatch size, number of hidden units, and number of layers are\nall examples of hyper-parameters that can be adjusted. The\nact of tuning these values is equivalent to selecting which\nmodel, among the set of all possible models, is best for a par-\nticular problem. Traditionally, hyper-parameters have been\nset by experience or systematically by grid search or more\ne\ufb00ectively, random search [12].\nVery recently, researchers\nhave turned to adaptive methods, which exploit the results\nof hyper-parameter attempts. Among these, Bayesian Opti-\nmization [35] is the most popular.\nRegardless of the method selected to tune hyper-parameters,\ncurrent training procedures using \ufb01xed architectures are some-\nwhat limited in their ability to grow these sets of possi-\nble models, meaning that we may be viewing the solution\nspace through a very narrow lens. Fixed architectures make\nit much easier to explore hyper-parameter settings within\nmodels (e.g. number of hidden units, number of layers) but\ndi\ufb03cult to explore settings between models (i.e. di\ufb00erent\ntypes of models) as the training of models which do not\nconveniently conform to a particular \ufb01xed architecture may\nbe very slow. The \ufb02exible architecture of FPGAs, however,\nmay be better suited for these types of optimizations, as a\ncompletely di\ufb00erent hardware structure can be programmed\nand accelerated at runtime.\n5.3\nLow Power Compute Clusters\nOne of the most intriguing aspects of deep learning models\nis the ability to scale. Whether the purpose is to discover\ncomplex high level features in data, or to increase perfor-\nmance for data center applications, deep learning techniques\nare often scaled up across multi-node computing infrastruc-\ntures. Current solutions to this problem involve using clus-\nters of GPUs with In\ufb01niband interconnects and MPI to allow\nhigh levels of parallel computing power and fast data trans-\nfer between nodes [18]. However, as the workloads of these\nlarge scale applications become increasingly heterogeneous,\nthe use of FPGAs may prove to be a superior alternative.\nThe programmability of FPGAs would allow recon\ufb01gurabil-\nity based on the application and workload, and FPGAs pro-\nvide an attractive performance/watt that would lower costs\nfor the next generation of data centers.\n6.\nCONCLUSION\nWhen addressing the hardware needs of deep learning,\nFPGAs provide an attractive alternative to GPUs and GPPs.\nIn particular, the ability to exploit pipeline parallelism and\nachieve an e\ufb03cient rate of power consumption give FPGAs\na unique advantage over GPUs and GPPs for common deep\nlearning practices. As well, design tools have matured to\na point where integrating FPGAs into popular deep learn-\ning frameworks is now possible. Looking forward, FPGAs\ncan e\ufb00ectively accommodate the trends of deep learning and\nprovide architectural freedom for exploration and research.\n7.\nREFERENCES\n[1] Ca\ufb00e-OpenCL.\nhttps://github.com/amd/OpenCL-caffe/wiki, 2015.\n[2] Ca\ufb00e: project greentea.\nhttps://github.com/BVLC/caffe/pull/2195, 2015.\n[3] DeepCL. https://github.com/hughperkins/DeepCL,\n2015.\n[4] Microsoft Research: catapult, 2015.\n[5] Theano: gpuarray backend. http://deeplearning.\nnet/software/libgpuarray/index.html, 2015.\n[6] Torch: cltorch.\nhttps://github.com/hughperkins/cltorch, 2015.\n[7] Xilinx: sdaccel, 2015.\n[8] D. F. Bacon, R. Rabbah, and S. Shukla. Fpga\nprogramming for the masses. Communications of the\nACM, 56(4):56\u201363, 2013.\n[9] Y. Bengio. Learning deep architectures for ai.\nFoundations and trends R\n\u20ddin Machine Learning,\n2(1):1\u2013127, 2009.\n[10] Y. Bengio and O. Delalleau. On the expressive power\nof deep architectures. In Algorithmic Learning Theory,\npages 18\u201336. Springer, 2011.\n[11] Y. Bengio, I. J. Goodfellow, and A. Courville. Deep\nlearning. Book in preparation for MIT Press, 2015.\n[12] J. Bergstra and Y. Bengio. Random search for\nhyper-parameter optimization. The Journal of\nMachine Learning Research, 13(1):281\u2013305, 2012.\n[13] J. Bergstra, O. Breuleux, F. Bastien, P. Lamblin,\nR. Pascanu, G. Desjardins, J. Turian,\nD. Warde-Farley, and Y. Bengio. Theano: A cpu and\ngpu math compiler in python. In Proc. 9th Python in\nScience Conf, pages 1\u20137, 2010.\n[14] S. Cadambi, A. Majumdar, M. Becchi, S. Chakradhar,\nand H. P. Graf. A programmable parallel accelerator\nfor learning and classi\ufb01cation. In Proceedings of the\n19th international conference on Parallel architectures\nand compilation techniques, pages 273\u2013284. ACM,\n2010.\n[15] S. Chakradhar, M. Sankaradas, V. Jakkula, and\nS. Cadambi. A dynamically con\ufb01gurable coprocessor\nfor convolutional neural networks. In ACM SIGARCH\nComputer Architecture News, volume 38, pages\n247\u2013257. ACM, 2010.\n[16] S. Chetlur, C. Woolley, P. Vandermersch, J. Cohen,\nJ. Tran, B. Catanzaro, and E. Shelhamer. cudnn:\nE\ufb03cient primitives for deep learning. arXiv preprint\narXiv:1410.0759, 2014.\n[17] J. Cloutier, S. Pigeon, F. R. Boyer, E. Cosatto, and\nP. Y. Simard. Vip: An fpga-based processor for image\nprocessing and neural networks. In microneuro, page\n330. IEEE, 1996.\n[18] A. Coates, B. Huval, T. Wang, D. Wu, B. Catanzaro,\nand N. Andrew. Deep learning with cots hpc systems.\nIn Proceedings of the 30th international conference on\nmachine learning, pages 1337\u20131345, 2013.\n[19] R. Collobert, S. Bengio, and J. Mari\u00b4ethoz. Torch: a\nmodular machine learning software library. Technical\nreport, IDIAP, 2002.\n[20] C. E. Cox and W. E. Blanz. Ganglion-a fast\n\ufb01eld-programmable gate array implementation of a\nconnectionist classi\ufb01er. Solid-State Circuits, IEEE\nJournal of, 27(3):288\u2013299, 1992.\n[21] T. S. Czajkowski, U. Aydonat, D. Denisenko,\nJ. Freeman, M. Kinsner, D. Neto, J. Wong,\nP. Yiannacouras, and D. P. Singh. From opencl to\nhigh-performance hardware on fpgas. In Field\nProgrammable Logic and Applications (FPL), 2012\n22nd International Conference on, pages 531\u2013534.\nIEEE, 2012.\n[22] J. Fang, A. L. Varbanescu, and H. Sips. A\ncomprehensive performance comparison of cuda and\nopencl. In Parallel Processing (ICPP), 2011\nInternational Conference on, pages 216\u2013225. IEEE,\n2011.\n[23] C. Farabet, C. Poulet, J. Y. Han, and Y. LeCun. Cnp:\nAn fpga-based processor for convolutional networks.\nIn Field Programmable Logic and Applications, 2009.\nFPL 2009. International Conference on, pages 32\u201337.\nIEEE, 2009.\n[24] J. Fowers, G. Brown, P. Cooke, and G. Stitt. A\nperformance and energy comparison of fpgas, gpus,\nand multicores for sliding-window applications. In\nProceedings of the ACM/SIGDA international\nsymposium on Field Programmable Gate Arrays, pages\n47\u201356. ACM, 2012.\n[25] D. H. Hubel and T. N. Wiesel. Receptive \ufb01elds and\nfunctional architecture of monkey striate cortex. The\nJournal of physiology, 195(1):215\u2013243, 1968.\n[26] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev,\nJ. Long, R. Girshick, S. Guadarrama, and T. Darrell.\nCa\ufb00e: Convolutional architecture for fast feature\nembedding. arXiv preprint arXiv:1408.5093, 2014.\n[27] A. Krizhevsky. One weird trick for parallelizing\nconvolutional neural networks. CoRR, abs/1404.5997,\n2014.\n[28] A. Krizhevsky, I. Sutskever, and G. E. Hinton.\nImagenet classi\ufb01cation with deep convolutional neural\nnetworks. In Advances in neural information\nprocessing systems, pages 1097\u20131105, 2012.\n[29] Y. LeCun, Y. Bengio, and G. Hinton. Deep learning.\nNature, 521:436\u2013444, 2015. Nature Publishing Group,\na division of Macmillan Publishers Limited. All Rights\nReserved.\n[30] K. Ovtcharov, O. Ruwase, J.-Y. Kim, J. Fowers,\nK. Strauss, and E. S. Chung. Accelerating deep\nconvolutional neural networks using specialized\nhardware. Microsoft Research Whitepaper, 2, 2015.\n[31] K. Paul and S. Rajopadhye. Back-propagation\nalgorithm achieving 5 gops on the virtex-e. In FPGA\nImplementations of Neural Networks, pages 137\u2013165.\nSpringer, 2006.\n[32] M. Peemen, A. Setio, B. Mesman, H. Corporaal, et al.\nMemory-centric accelerator design for convolutional\nneural networks. In Computer Design (ICCD), 2013\nIEEE 31st International Conference on, pages 13\u201319.\nIEEE, 2013.\n[33] M. Sankaradas, V. Jakkula, S. Cadambi,\nS. Chakradhar, I. Durdanovic, E. Cosatto, and H. P.\nGraf. A massively parallel coprocessor for\nconvolutional neural networks. In Application-speci\ufb01c\nSystems, Architectures and Processors, 2009. ASAP\n2009. 20th IEEE International Conference on, pages\n53\u201360. IEEE, 2009.\n[34] J. Schmidhuber. Deep learning in neural networks: An\noverview. Neural Networks, 61:85\u2013117, 2015. Published\nonline 2014; based on TR arXiv:1404.7828 [cs.NE].\n[35] J. Snoek, H. Larochelle, and R. P. Adams. Practical\nbayesian optimization of machine learning algorithms.\nIn Advances in neural information processing systems,\npages 2951\u20132959, 2012.\n[36] C. Zhang, P. Li, G. Sun, Y. Guan, B. Xiao, and\nJ. Cong. Optimizing fpga-based accelerator design for\ndeep convolutional neural networks. In Proceedings of\nthe 2015 ACM/SIGDA International Symposium on\nField-Programmable Gate Arrays, pages 161\u2013170.\nACM, 2015.\n",
        "sentence": " Therefore, it can fit changing applications and parameters in neural networks [20], [21].",
        "context": "due to its popularity, support, and easy to use interface. As\nwell, using Ca\ufb00e\u2019s \u201cmodel zoo\u201d repository, it is easy to ex-\nperiment with popular pre-trained models.\n5.2\nIncreasing Degrees of Freedom for Train-\ning\nRecent\nwork on accelerating CNNs using multiple GPUs has used\nvery sophisticated strategies to balance data and model-\nbased parallism such that di\ufb00erent parts of the architecture\nare parallelized in di\ufb00erent, but optimal ways [27].\nmay be better suited for these types of optimizations, as a\ncompletely di\ufb00erent hardware structure can be programmed\nand accelerated at runtime.\n5.3\nLow Power Compute Clusters\nOne of the most intriguing aspects of deep learning models"
    },
    {
        "title": "fpgaconvnet: A framework for mapping convolutional neural networks on fpgas",
        "author": [
            "S.I. Venieris",
            "C.-S. Bouganis"
        ],
        "venue": "FCCM\u201916, pp. 40\u201347, 2016.",
        "citeRegEx": "21",
        "shortCiteRegEx": null,
        "year": 2016,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " Therefore, it can fit changing applications and parameters in neural networks [20], [21].",
        "context": null
    },
    {
        "title": "Throughput-optimized opencl-based fpga accelerator for large-scale convolutional neural networks",
        "author": [
            "N. Suda",
            "V. Chandra",
            "G. Dasika",
            "A. Mohanty",
            "Y. Ma",
            "S. Vrudhula",
            "J.-s. Seo",
            "Y. Cao"
        ],
        "venue": "FPGA, pp. 16\u201325, 2016.",
        "citeRegEx": "22",
        "shortCiteRegEx": null,
        "year": 2016,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " [22] presents a design space exploration method OpenCL programming model approach, which can explore the trade-offs the parameters in the network topologies.",
        "context": null
    },
    {
        "title": "Memristive boltzmann machine: A hardware accelerator for combinatorial optimization and deep learning",
        "author": [
            "M.N. Bojnordi",
            "E. Ipek"
        ],
        "venue": "HPCA\u201916, pp. 1\u201313, March 2016.",
        "citeRegEx": "23",
        "shortCiteRegEx": null,
        "year": 2016,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " Besides the ASIC and FPGA-based accelerators, there have been numerous directions using emerging hardware technologies, such as Memristive Boltzmann Machine [23], and Processing-in-Memory techniques [24].",
        "context": null
    },
    {
        "title": "A reconfigurable digital neuromorphic processor with memristive synaptic crossbar for cognitive computing",
        "author": [
            "Y. Kim",
            "Y. Zhang",
            "P. Li"
        ],
        "venue": "J. Emerg. Technol. Comput. Syst., vol. 11, pp. 38:1\u201338:25, Apr. 2015.",
        "citeRegEx": "24",
        "shortCiteRegEx": null,
        "year": 2015,
        "abstract": "\n            This article presents a brain-inspired reconfigurable digital neuromorphic processor (DNP) architecture for large-scale spiking neural networks. The proposed architecture integrates an arbitrary number of\n            N\n            digital leaky integrate-and-fire (LIF) silicon neurons to mimic their biological counterparts and on-chip learning circuits to realize spike-timing-dependent plasticity (STDP) learning rules. We leverage memristor nanodevices to build an\n            N\n            \u00d7\n            N\n            crossbar array to store not only multibit synaptic weight values but also network configuration data with significantly reduced area overhead. Additionally, the crossbar array is designed to be accessible both column- and row-wise to expedite the synaptic weight update process for learning. The proposed digital pulse width modulator (PWM) produces binary pulses with various durations for reading and writing the multilevel memristive crossbar. The proposed column based analog-to-digital conversion (ADC) scheme efficiently accumulates the presynaptic weights of each neuron and reduces silicon area overhead by using a shared arithmetic unit to process the LIF operations of all\n            N\n            neurons. With 256 silicon neurons, learning circuits and 64K synapses, the power dissipation and area of our DNP are 6.45 mW and 1.86 mm\n            2\n            , respectively, when implemented in a 90-nm CMOS technology. The functionality of the proposed DNP architecture is demonstrated by realizing an unsupervised-learning based character recognition system.\n          ",
        "full_text": "",
        "sentence": " Besides the ASIC and FPGA-based accelerators, there have been numerous directions using emerging hardware technologies, such as Memristive Boltzmann Machine [23], and Processing-in-Memory techniques [24].",
        "context": null
    },
    {
        "title": "Eie: Efficient inference engine on compressed deep neural network",
        "author": [
            "S. Han",
            "X. Liu",
            "H. Mao",
            "J. Pu",
            "A. Pedram",
            "M.A. Horowitz",
            "W.J. Dally"
        ],
        "venue": "ISCA\u201916, 2016.",
        "citeRegEx": "25",
        "shortCiteRegEx": null,
        "year": 2016,
        "abstract": "State-of-the-art deep neural networks (DNNs) have hundreds of millions of\nconnections and are both computationally and memory intensive, making them\ndifficult to deploy on embedded systems with limited hardware resources and\npower budgets. While custom hardware helps the computation, fetching weights\nfrom DRAM is two orders of magnitude more expensive than ALU operations, and\ndominates the required power.\n  Previously proposed 'Deep Compression' makes it possible to fit large DNNs\n(AlexNet and VGGNet) fully in on-chip SRAM. This compression is achieved by\npruning the redundant connections and having multiple connections share the\nsame weight. We propose an energy efficient inference engine (EIE) that\nperforms inference on this compressed network model and accelerates the\nresulting sparse matrix-vector multiplication with weight sharing. Going from\nDRAM to SRAM gives EIE 120x energy saving; Exploiting sparsity saves 10x;\nWeight sharing gives 8x; Skipping zero activations from ReLU saves another 3x.\nEvaluated on nine DNN benchmarks, EIE is 189x and 13x faster when compared to\nCPU and GPU implementations of the same DNN without compression. EIE has a\nprocessing power of 102GOPS/s working directly on a compressed network,\ncorresponding to 3TOPS/s on an uncompressed network, and processes FC layers of\nAlexNet at 1.88x10^4 frames/sec with a power dissipation of only 600mW. It is\n24,000x and 3,400x more energy efficient than a CPU and GPU respectively.\nCompared with DaDianNao, EIE has 2.9x, 19x and 3x better throughput, energy\nefficiency and area efficiency.",
        "full_text": "EIE: Ef\ufb01cient Inference Engine on Compressed Deep Neural Network\nSong Han\u2217Xingyu Liu\u2217Huizi Mao\u2217Jing Pu\u2217Ardavan Pedram\u2217\nMark A. Horowitz\u2217William J. Dally\u2217\u2020\n\u2217Stanford University, \u2020NVIDIA\n{songhan,xyl,huizi,jingpu,perdavan,horowitz,dally}@stanford.edu\nAbstract\u2014State-of-the-art deep neural networks (DNNs)\nhave hundreds of millions of connections and are both compu-\ntationally and memory intensive, making them dif\ufb01cult to de-\nploy on embedded systems with limited hardware resources and\npower budgets. While custom hardware helps the computation,\nfetching weights from DRAM is two orders of magnitude more\nexpensive than ALU operations, and dominates the required\npower.\nPreviously proposed \u2018Deep Compression\u2019 makes it possible\nto \ufb01t large DNNs (AlexNet and VGGNet) fully in on-chip\nSRAM. This compression is achieved by pruning the redundant\nconnections and having multiple connections share the same\nweight. We propose an energy ef\ufb01cient inference engine (EIE)\nthat performs inference on this compressed network model and\naccelerates the resulting sparse matrix-vector multiplication\nwith weight sharing. Going from DRAM to SRAM gives EIE\n120\u00d7 energy saving; Exploiting sparsity saves 10\u00d7; Weight\nsharing gives 8\u00d7; Skipping zero activations from ReLU saves\nanother 3\u00d7. Evaluated on nine DNN benchmarks, EIE is\n189\u00d7 and 13\u00d7 faster when compared to CPU and GPU\nimplementations of the same DNN without compression. EIE\nhas a processing power of 102 GOPS/s working directly on\na compressed network, corresponding to 3 TOPS/s on an\nuncompressed network, and processes FC layers of AlexNet at\n1.88\u00d7104 frames/sec with a power dissipation of only 600mW.\nIt is 24,000\u00d7 and 3,400\u00d7 more energy ef\ufb01cient than a CPU\nand GPU respectively. Compared with DaDianNao, EIE has\n2.9\u00d7, 19\u00d7 and 3\u00d7 better throughput, energy ef\ufb01ciency and\narea ef\ufb01ciency.\nKeywords-Deep Learning; Model Compression; Hardware\nAcceleration; Algorithm-Hardware co-Design; ASIC;\nI. INTRODUCTION\nNeural networks have become ubiquitous in applications\nincluding computer vision [1]\u2013[3], speech recognition [4],\nand natural language processing [4]. In 1998, Lecun et\nal. classi\ufb01ed handwritten digits with less than 1M parame-\nters [5], while in 2012, Krizhevsky et al. won the ImageNet\ncompetition with 60M parameters [1]. Deepface classi\ufb01ed\nhuman faces with 120M parameters [6]. Neural Talk [7]\nautomatically converts image to natural language with 130M\nCNN parameters and 100M RNN parameters. Coates et\nal. scaled up a network to 10 billion parameters on HPC\nsystems [8].\nLarge DNN models are very powerful but consume large\namounts of energy because the model must be stored in\nexternal DRAM, and fetched every time for each image,\n4-bit \u2028\nRelative Index\n4-bit \u2028\nVirtual weight\n16-bit  \nReal weight\n16-bit \u2028\nAbsolute Index\nEncoded Weight \nRelative Index \nSparse Format \nALU\nMem\nCompressed \nDNN Model\nWeight  \nLook-up\nIndex  \nAccum\nPrediction\nInput \nImage\nResult\nFigure 1.\nEf\ufb01cient inference engine that works on the compressed deep\nneural network model for machine learning applications.\nword, or speech sample. For embedded mobile applications,\nthese resource demands become prohibitive. Table I shows\nthe energy cost of basic arithmetic and memory operations\nin a 45nm CMOS process [9]. It shows that the total energy\nis dominated by the required memory access if there is\nno data reuse. The energy cost per fetch ranges from 5pJ\nfor 32b coef\ufb01cients in on-chip SRAM to 640pJ for 32b\ncoef\ufb01cients in off-chip LPDDR2 DRAM. Large networks do\nnot \ufb01t in on-chip storage and hence require the more costly\nDRAM accesses. Running a 1G connection neural network,\nfor example, at 20Hz would require (20Hz)(1G)(640pJ) =\n12.8W just for DRAM accesses, which is well beyond the\npower envelope of a typical mobile device.\nPrevious work has used specialized hardware to accelerate\nDNNs [10]\u2013[12]. However, these efforts focus on acceler-\nating dense, uncompressed models - limiting their utility\nto small models or to cases where the high energy cost\nof external DRAM access can be tolerated. Without model\ncompression, it is only possible to \ufb01t very small neural\nnetworks, such as Lenet-5, in on-chip SRAM [12].\nEf\ufb01cient implementation of convolutional layers in CNN\nhas been intensively studied, as its data reuse and manipu-\nlation is quite suitable for customized hardware [10]\u2013[15].\nHowever, it has been found that fully-connected (FC) layers,\nwidely used in RNN and LSTMs, are bandwidth limited\non large networks [14]. Unlike CONV layers, there is no\nparameter reuse in FC layers. Data batching has become\nan ef\ufb01cient solution when training networks on CPUs or\nGPUs, however, it is unsuitable for real-time applications\nwith latency requirements.\nNetwork compression via pruning and weight sharing\n[16] makes it possible to \ufb01t modern networks such as\nAlexNet (60M parameters, 240MB), and VGG-16 (130M\nparameters, 520MB) in on-chip SRAM. Processing these\narXiv:1602.01528v2  [cs.CV]  3 May 2016\nTable I\nENERGY TABLE FOR 45NM CMOS PROCESS [9]. DRAM ACCESS USES\nTHREE ORDERS OF MAGNITUDE MORE ENERGY THAN SIMPLE\nARITHMETIC AND 128X MORE THAN SRAM.\nOperation\nEnergy [pJ]\nRelative Cost\n32 bit int ADD\n0.1\n1\n32 bit \ufb02oat ADD\n0.9\n9\n32 bit int MULT\n3.1\n31\n32 bit \ufb02oat MULT\n3.7\n37\n32 bit 32KB SRAM\n5\n50\n32 bit DRAM\n640\n6400\ncompressed models, however, is challenging. With pruning,\nthe matrix becomes sparse and the indicies become relative.\nWith weight sharing, we store only a short (4-bit) index for\neach weight. This adds extra levels of indirection that cause\ncomplexity and inef\ufb01ciency on CPUs and GPUs.\nTo ef\ufb01ciently operate on compressed DNN models, we\npropose EIE, an ef\ufb01cient inference engine, a specialized\naccelerator that performs customized sparse matrix vector\nmultiplication and handles weight sharing with no loss of\nef\ufb01ciency. EIE is a scalable array of processing elements\n(PEs) Every PE stores a partition of network in SRAM and\nperforms the computations associated with that part. It takes\nadvantage of dynamic input vector sparsity, static weight\nsparsity, relative indexing, weight sharing and extremely\nnarrow weights (4bits).\nIn our design, each PE holds 131K weights of the\ncompressed model, corresponding to 1.2M weights of the\noriginal dense model, and is able to perform 800 million\nweight calculations per second. In 45nm CMOS technology,\nan EIE PE has an area of 0.638mm2 and dissipates 9.16mW\nat 800MHz. The fully-connected layers of AlexNet \ufb01t into\n64PEs that consume a total of 40.8mm2 and operate at\n1.88 \u00d7 104 frames/sec with a power dissipation of 590mW.\nCompared with CPU (Intel i7-5930k) GPU (GeForce TITAN\nX) and mobile GPU (Tegra K1), EIE achieves 189\u00d7, 13\u00d7\nand 307\u00d7 acceleration, while saving 24, 000\u00d7, 3, 400\u00d7 and\n2, 700\u00d7 energy, respectively.\nThis paper makes the following contributions:\n1) We present the \ufb01rst accelerator for sparse and weight\nsharing neural networks. Operating directly on com-\npressed networks enables the large neural network mod-\nels to \ufb01t in on-chip SRAM, which results in 120\u00d7 better\nenergy savings compared to accessing from external\nDRAM.\n2) EIE is the \ufb01rst accelerator that exploits the dynamic\nsparsity of activations to save computation. EIE saves\n65.16% energy by avoiding weight references and arith-\nmetic for the 70% of activations that are zero in a\ntypical deep learning applications.\n3) We describe a method of both distributed storage and\ndistributed computation to parallelize a sparsi\ufb01ed layer\nacross multiple PEs, which achieves load balance and\ngood scalability.\n4) We evaluate EIE on a wide range of deep learning\nmodels, including CNN for object detection and LSTM\nfor natural language processing and image captioning.\nWe also compare EIE to CPU, GPU, FPGA, and other\nASIC accelerators.\nIn Section II we describe the motivation of accelerating\ncompressed networks. Section III reviews the compression\ntechnique that EIE uses and how to parallelize the workload.\nThe hardware architecture in each PE to perform inference\nis described in Section IV. In Section V we describe our\nevaluation methodology. Then we report our experimental\nresults in Section VI, followed by discussions, comparison\nwith related work and conclusions.\nII. MOTIVATION\nMatrix-vector multiplication (M\u00d7V) is a basic building\nblock in a wide range of neural networks and deep learning\napplications. In convolutional neural network (CNN), fully\nconnected layers are implemented with M\u00d7V, and more\nthan 96% of the connections are in the FC layers [1]. In\nobject detection algorithms, an FC layer is required to run\nmultiple times on all proposal regions, taking up to 38%\ncomputation time [17]. In recurrent neural network (RNN),\nM\u00d7V operations are performed on the new input and the\nhidden state at each time step, producing a new hidden state\nand the output. Long-Short-Term-Memory (LSTM) [18] is\na widely used structure of RNN cell that provides more\ncomplex hidden unit computation. Each LSTM cell can\nbe decomposed into eight M\u00d7V operations, two for each:\ninput gate, forget gate, output gate, and one temporary\nmemory cell. RNN, including LSTM, is widely used in\nimage captioning [7], speech recognition [19] and natural\nlanguage processing [4].\nDuring M\u00d7V, the memory access is usually the bottle-\nneck [14] especially when the matrix is larger than the\ncache capacity. There is no reuse of the input matrix, thus\na memory access is needed for every operation. On CPUs\nand GPUs, this problem is usually solved by batching, i.e.,\ncombining multiple vectors into a matrix to reuse the param-\neters. However, such a strategy is unsuitable for real-time\napplications that are latency-sensitive, such as pedestrian\ndetection in an autonomous vehicle [20], because batching\nsubstantially increases latency. Therefore it is preferable to\ncreate an ef\ufb01cient method of executing large neural networks\nwithout the latency cost of batching.\nBecause memory access is the bottleneck in large layers,\ncompressing the neural network offers a solution. Though\ncompression reduces the total number of operations, the\nirregular pattern caused by compression hinders the effective\nacceleration on CPUs and GPUs, as illustrated in Table IV.\nA compressed network is not ef\ufb01cient on previous accel-\nerators either. Previous SPMV accelerators can only exploit\nthe static weight sparsity. They are unable to exploit dynamic\nactivation sparsity [21]. Previous DNN accelerators cannot\nexploit either form of sparsity and must expand the network\nto dense form before operation [11]. Neither is able to\nexploit weight sharing. This motivates building a special\nengine that can operate on a compressed network.\nIII. DNN COMPRESSION AND PARALLELIZATION\nA. Computation\nA FC layer of a DNN performs the computation\nb = f(Wa + v)\n(1)\nWhere a is the input activation vector, b is the output\nactivation vector, v is the bias, W is the weight matrix, and\nf is the non-linear function, typically the Recti\ufb01ed Linear\nUnit(ReLU) [22] in CNN and some RNN. Sometimes v\nwill be combined with W by appending an additional one\nto vector a, therefore we neglect the bias in the following\nparagraphs.\nFor a typical FC layer like FC7 of VGG-16 or AlexNet,\nthe activation vectors are 4K long, and the weight matrix is\n4K \u00d7 4K (16M weights). Weights are represented as single-\nprecision \ufb02oating-point numbers so such a layer requires\n64MB of storage. The output activations of Equation (1) are\ncomputed element-wise as:\nbi = ReLU\n\uf8eb\n\uf8ed\nn\u22121\nX\nj=0\nWijaj\n\uf8f6\n\uf8f8\n(2)\nDeep Compression [23] describes a method to compress\nDNNs without loss of accuracy through a combination of\npruning and weight sharing. Pruning makes matrix W sparse\nwith density D ranging from 4% to 25% for our benchmark\nlayers. Weight sharing replaces each weight Wij with a four-\nbit index Iij into a shared table S of 16 possible weight\nvalues.\nWith deep compression, the per-activation computation of\nEquation (2) becomes\nbi = ReLU\n\uf8eb\n\uf8ed\nX\nj\u2208Xi\u2229Y\nS[Iij]aj\n\uf8f6\n\uf8f8\n(3)\nWhere Xi is the set of columns j for which Wij \u0338= 0, Y\nis the set of indices j for which aj \u0338= 0, Iij is the index\nto the shared weight that replaces Wij, and S is the table\nof shared weights. Here Xi represents the static sparsity of\nW and Y represents the dynamic sparsity of a. The set Xi\nis \ufb01xed for a given model. The set Y varies from input to\ninput.\nAccelerating Equation (3) is needed to accelerate a com-\npressed DNN. We perform the indexing S[Iij] and the\nmultiply-add only for those columns for which both Wij\nand aj are non-zero, so that both the sparsity of the matrix\nand the vector are exploited. This results in a dynamically ir-\nregular computation. Performing the indexing itself involves\n\u20d7a \u00000\n0\na2\n0\na4\na5\n0\na7\n\u0001\n\u00d7\n\u20d7b\nPE0\nPE1\nPE2\nPE3\n\uf8eb\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ed\nw0,0\n0\nw0,2\n0\nw0,4 w0,5 w0,6\n0\n0\nw1,1\n0\nw1,3\n0\n0\nw1,6\n0\n0\n0\nw2,2\n0\nw2,4\n0\n0\nw2,7\n0\nw3,1\n0\n0\n0\nw0,5\n0\n0\n0\nw4,1\n0\n0\nw4,4\n0\n0\n0\n0\n0\n0\nw5,4\n0\n0\n0\nw5,7\n0\n0\n0\n0\nw6,4\n0\nw6,6\n0\nw7,0\n0\n0\nw7,4\n0\n0\nw7,7\n0\nw8,0\n0\n0\n0\n0\n0\n0\nw8,7\nw9,0\n0\n0\n0\n0\n0\nw9,6 w9,7\n0\n0\n0\n0\nw10,4\n0\n0\n0\n0\n0\nw11,2\n0\n0\n0\n0\nw11,7\nw12,0\n0\nw12,2\n0\n0\nw12,5\n0\nw12,7\nw13,0w13,2\n0\n0\n0\n0\nw13,6\n0\n0\n0\nw14,2w14,3w14,4w14,5\n0\n0\n0\n0\nw15,2w15,3\n0\nw15,5\n0\n0\n\uf8f6\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f8\n=\n\uf8eb\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ed\nb0\nb1\n\u2212b2\nb3\n\u2212b4\nb5\nb6\n\u2212b7\n\u2212b8\n\u2212b9\nb10\n\u2212b11\n\u2212b12\nb13\nb14\n\u2212b15\n\uf8f6\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f8\nReLU\n\u21d2\n\uf8eb\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ed\nb0\nb1\n0\nb3\n0\nb5\nb6\n0\n0\n0\nb10\n0\n0\nb13\nb14\n0\n\uf8f6\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f8\n1\nFigure 2.\nMatrix W and vectors a and b are interleaved over 4 PEs.\nElements of the same color are stored in the same PE.\nVirtual \nWeight\nW0,0 W8,0 W12,0 W4,1 W0,2 W12,2 W0,4 W4,4 W0,5 W12,5 W0,6 W8,7 W12,7\nRelative \u2028\nRow Index\n0\n1\n0\n1\n0\n2\n0\n0\n0\n2\n0\n2\n0\nColumn \nPointer\n0\n3\n4\n6\n6\n8\n10\n11\n13\n \n \n \nFigure 3.\nMemory layout for the relative indexed, indirect weighted and\ninterleaved CSC format, corresponding to PE0 in Figure 2.\nbit manipulations to extract four-bit Iij and an extra load\n(which is almost assured a cache hit).\nB. Representation\nTo exploit the sparsity of activations we store our encoded\nsparse weight matrix W in a variation of compressed sparse\ncolumn (CSC) format [24].\nFor each column Wj of matrix W we store a vector v\nthat contains the non-zero weights, and a second, equal-\nlength vector z that encodes the number of zeros before\nthe corresponding entry in v. Each entry of v and z is\nrepresented by a four-bit value. If more than 15 zeros appear\nbefore a non-zero entry we add a zero in vector v. For\nexample, we encode the following column\n[0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3]\nas v = [1, 2, 0, 3], z = [2, 0, 15, 2]. v and z of all columns\nare stored in one large pair of arrays with a pointer vector p\npointing to the beginning of the vector for each column. A\n\ufb01nal entry in p points one beyond the last vector element so\nthat the number of non-zeros in column j (including padded\nzeros) is given by pj+1 \u2212pj.\nStoring the sparse matrix by columns in CSC format\nmakes it easy to exploit activation sparsity. We simply\nmultiply each non-zero activation by all of the non-zero\nelements in its corresponding column.\nPointer Read \nAct R/W \nAct Queue \nSparse Matrix Access \nSparse \nMatrix \nSRAM \n                      Arithmetic Unit \nRegs \nCol \nStart/\nEnd \nAddr \nAct Index \nWeight \nDecoder \nAddress \nAccum \nDest \nAct \nRegs \nAct \nSRAM \nAct Value \nEncoded \nWeight \nRelative \nIndex \nSrc \nAct \nRegs \nAbsolute Address \nBypass \nLeading \nNZero \nDetect \nEven Ptr SRAM Bank \nOdd Ptr SRAM Bank \nReLU \n(a) \n(b) \nFrom NE \nFrom SE \nFrom SW \nLeading \nNzero \nDetect \nAct0 \nAct1 \nAct3 \nAct Value \ns0 \ns1 \ns3 \nFrom NW \nAct2 s2 \nNzero Index \nAct0,1,2,3 \nFigure 4.\n(a) The architecture of Leading Non-zero Detection Node. (b) The architecture of Processing Element.\nC. Parallelizing Compressed DNN\nWe distribute the matrix and parallelize our matrix-vector\ncomputation by interleaving the rows of the matrix W over\nmultiple processing elements (PEs). With N PEs, PEk holds\nall rows Wi, output activations bi, and input activations ai\nfor which i (mod N) = k. The portion of column Wj in\nPEk is stored in the CSC format described in Section III-B\nbut with the zero counts referring only to zeros in the subset\nof the column in this PE. Each PE has its own v, x, and p\narrays that encode its fraction of the sparse matrix.\nFigure 2 shows an example multiplying an input activation\nvector a (of length 8) by a 16\u00d78 weight matrix W yielding\nan output activation vector b (of length 16) on N = 4 PEs.\nThe elements of a, b, and W are color coded with their PE\nassignments. Each PE owns 4 rows of W, 2 elements of a,\nand 4 elements of b.\nWe perform the sparse matrix \u00d7 sparse vector operation\nby scanning vector a to \ufb01nd its next non-zero value aj\nand broadcasting aj along with its index j to all PEs.\nEach PE then multiplies aj by the non-zero elements in\nits portion of column Wj \u2014 accumulating the partial sums\nin accumulators for each element of the output activation\nvector b. In the CSC representation these non-zeros weights\nare stored contiguously so each PE simply walks through its\nv array from location pj to pj+1 \u22121 to load the weights.\nTo address the output accumulators, the row number i\ncorresponding to each weight Wij is generated by keeping\na running sum of the entries of the x array.\nIn the example of Figure 2, the \ufb01rst non-zero is a2 on\nPE2. The value a2 and its column index 2 is broadcast\nto all PEs. Each PE then multiplies a2 by every non-\nzero in its portion of column 2. PE0 multiplies a2 by\nW0,2 and W12,2; PE1 has all zeros in column 2 and so\nperforms no multiplications; PE2 multiplies a2 by W2,2\nand W14,2, and so on. The result of each product is summed\ninto the corresponding row accumulator. For example PE0\ncomputes b0 = b0 + W0,2a2 and b12 = b12 + W12,2a2.\nThe accumulators are initialized to zero before each layer\ncomputation.\nThe interleaved CSC representation facilitates exploitation\nof both the dynamic sparsity of activation vector a and\nthe static sparsity of the weight matrix W. We exploit\nactivation sparsity by broadcasting only non-zero elements\nof input activation a. Columns corresponding to zeros in a\nare completely skipped. The interleaved CSC representation\nallows each PE to quickly \ufb01nd the non-zeros in each column\nto be multiplied by aj. This organization also keeps all of the\ncomputation except for the broadcast of the input activations\nlocal to a PE. The interleaved CSC representation of matrix\nin Figure 2 is shown in Figure 3.\nThis process may suffer load imbalance because each PE\nmay have a different number of non-zeros in a particular\ncolumn. We will see in Section IV how this load imbalance\ncan be reduced by queuing.\nIV. HARDWARE IMPLEMENTATION\nFigure 4 shows the architecture of EIE. A Central Control\nUnit (CCU) controls an array of PEs that each computes one\nslice of the compressed network. The CCU also receives\nnon-zero input activations from a distributed leading non-\nzero detection network and broadcasts these to the PEs.\nAlmost all computation in EIE is local to the PEs except\nfor the collection of non-zero input activations that are\nbroadcast to all PEs. However, the timing of the activation\ncollection and broadcast is non-critical as most PEs take\nmany cycles to consume each input activation.\nActivation Queue and Load Balancing. Non-zero ele-\nments of the input activation vector aj and their correspond-\ning index j are broadcast by the CCU to an activation queue\nin each PE. The broadcast is disabled if any PE has a full\nqueue. At any point in time each PE processes the activation\nat the head of its queue.\nThe activation queue allows each PE to build up a backlog\nof work to even out load imbalance that may arise because\nthe number of non zeros in a given column j may vary\nfrom PE to PE. In Section VI we measure the sensitivity of\nperformance to the depth of the activation queue.\nPointer Read Unit. The index j of the entry at the head\nof the activation queue is used to look up the start and end\npointers pj and pj+1 for the v and x arrays for column j.\nTo allow both pointers to be read in one cycle using single-\nported SRAM arrays, we store pointers in two SRAM banks\nand use the LSB of the address to select between banks. pj\nand pj+1 will always be in different banks. EIE pointers are\n16-bits in length.\nSparse Matrix Read Unit. The sparse-matrix read unit\nuses pointers pj and pj+1 to read the non-zero elements (if\nany) of this PE\u2019s slice of column Ij from the sparse-matrix\nSRAM. Each entry in the SRAM is 8-bits in length and\ncontains one 4-bit element of v and one 4-bit element of x.\nFor ef\ufb01ciency (see Section VI) the PE\u2019s slice of encoded\nsparse matrix I is stored in a 64-bit-wide SRAM. Thus eight\nentries are fetched on each SRAM read. The high 13 bits\nof the current pointer p selects an SRAM row, and the low\n3-bits select one of the eight entries in that row. A single\n(v, x) entry is provided to the arithmetic unit each cycle.\nArithmetic Unit. The arithmetic unit receives a (v, x)\nentry from the sparse matrix read unit and performs the\nmultiply-accumulate operation bx = bx + v \u00d7 aj. Index\nx is used to index an accumulator array (the destination\nactivation registers) while v is multiplied by the activation\nvalue at the head of the activation queue. Because v is stored\nin 4-bit encoded form, it is \ufb01rst expanded to a 16-bit \ufb01xed-\npoint number via a table look up. A bypass path is provided\nto route the output of the adder to its input if the same\naccumulator is selected on two adjacent cycles.\nActivation Read/Write. The Activation Read/Write Unit\ncontains two activation register \ufb01les that accommodate the\nsource and destination activation values respectively during\na single round of FC layer computation. The source and\ndestination register \ufb01les exchange their role for next layer.\nThus no additional data transfer is needed to support multi-\nlayer feed-forward computation.\nEach activation register \ufb01le holds 64 16-bit activations.\nThis is suf\ufb01cient to accommodate 4K activation vectors\nacross 64 PEs. Longer activation vectors can be accommo-\ndated with the 2KB activation SRAM. When the activation\nvector has a length greater than 4K, the M\u00d7V will be\ncompleted in several batches, where each batch is of length\n4K or less. All the local reduction is done in the register\n\ufb01le. The SRAM is read only at the beginning and written at\nthe end of the batch.\nDistributed Leading Non-Zero Detection. Input acti-\nvations are hierarchically distributed to each PE. To take\nadvantage of the input vector sparsity, we use leading non-\nzero detection logic to select the \ufb01rst non-zero result. Each\ngroup of 4 PEs does a local leading non-zero detection on\ntheir input activation. The result is sent to a Leading Non-\nzero Detection Node (LNZD Node) illustrated in Figure 4.\nEach LNZD node \ufb01nds the next non-zero activation across\nits four children and sends this result up the quadtree. The\nquadtree is arranged so that wire lengths remain constant as\nwe add PEs. At the root LNZD Node, the selected non-zero\nactivation is broadcast back to all the PEs via a separate\nwire placed in an H-tree.\nCentral Control Unit. The Central Control Unit (CCU)\nis the root LNZD Node. It communicates with the master,\nfor example a CPU, and monitors the state of every PE by\nsetting the control registers. There are two modes in the\nSpMat\nSpMat\nPtr_Even\nPtr_Odd\nArithm\nAct_0\nAct_1\nFigure 5.\nLayout of one PE in EIE under TSMC 45nm process.\nTable II\nTHE IMPLEMENTATION RESULTS OF ONE PE IN EIE AND THE\nBREAKDOWN BY COMPONENT TYPE (LINE 3-7), BY MODULE (LINE\n8-13). THE CRITICAL PATH OF EIE IS 1.15 NS\nPower\n(%)\nArea\n(%)\n(mW)\n(\u00b5m2)\nTotal\n9.157\n638,024\nmemory\n5.416\n(59.15%)\n594,786\n(93.22%)\nclock network\n1.874\n(20.46%)\n866\n(0.14%)\nregister\n1.026\n(11.20%)\n9,465\n(1.48%)\ncombinational\n0.841\n(9.18%)\n8,946\n(1.40%)\n\ufb01ller cell\n23,961\n(3.76%)\nAct queue\n0.112\n(1.23%)\n758\n(0.12%)\nPtrRead\n1.807\n(19.73%)\n121,849\n(19.10%)\nSpmatRead\n4.955\n(54.11%)\n469,412\n(73.57%)\nArithmUnit\n1.162\n(12.68%)\n3,110\n(0.49%)\nActRW\n1.122\n(12.25%)\n18,934\n(2.97%)\n\ufb01ller cell\n23,961\n(3.76%)\nCentral Unit: I/O and Computing. In the I/O mode, all of\nthe PEs are idle while the activations and weights in every\nPE can be accessed by a DMA connected with the Central\nUnit. This is one time cost. In the Computing mode, the\nCCU repeatedly collects a non-zero value from the LNZD\nquadtree and broadcasts this value to all PEs. This process\ncontinues until the input length is exceeded. By setting the\ninput length and starting address of pointer array, EIE is\ninstructed to execute different layers.\nV. EVALUATION METHODOLOGY\nSimulator, RTL and Layout. We implemented a custom\ncycle-accurate C++ simulator for the accelerator aimed to\nmodel the RTL behavior of synchronous circuits. Each\nhardware module is abstracted as an object that implements\ntwo abstract methods: propagate and update, corresponding\nto combination logic and the \ufb02ip-\ufb02op in RTL. The simulator\nis used for design space exploration. It also serves as a\nchecker for RTL veri\ufb01cation.\nTo measure the area, power and critical path delay, we\nimplemented the RTL of EIE in Verilog. The RTL is veri\ufb01ed\nagainst the cycle-accurate simulator. Then we synthesized\nEIE using the Synopsys Design Compiler (DC) under the\nTSMC 45nm GP standard VT library with worst case PVT\ncorner. We placed and routed the PE using the Synopsys IC\ncompiler (ICC). We used Cacti [25] to get SRAM area and\n1x\n1x\n1x\n1x\n1x\n1x\n1x\n1x\n1x\n1x\n2x\n5x\n1x\n9x\n10x\n1x\n2x\n3x\n2x\n3x\n14x\n25x\n14x\n24x\n22x\n10x\n9x\n15x\n9x\n15x\n56x\n94x\n21x\n210x\n135x\n16x\n34x\n33x\n25x\n48x\n0.6x\n1.1x\n0.5x\n1.0x\n1.0x\n0.3x\n0.5x\n0.5x\n0.5x\n0.6x\n3x\n5x\n1x\n8x\n9x\n1x\n3x\n2x\n1x\n3x\n248x\n507x\n115x\n1018x\n618x\n92x\n63x\n98x\n60x\n189x\n0.1x\n1x\n10x\n100x\n1000x\nAlex-6\nAlex-7\nAlex-8\nVGG-6\nVGG-7\nVGG-8\nNT-We\nNT-Wd\nNT-LSTM\nGeo Mean\nSpeedup\nCPU Dense (Baseline)\nCPU Compressed\nGPU Dense\nGPU Compressed\nmGPU Dense\nmGPU Compressed\nEIE\nFigure 6.\nSpeedups of GPU, mobile GPU and EIE compared with CPU running uncompressed DNN model. There is no batching in all cases.\n1x\n1x\n1x\n1x\n1x\n1x\n1x\n1x\n1x\n1x\n5x\n9x\n3x\n17x\n20x\n2x\n6x\n6x\n4x\n6x\n7x\n12x\n7x\n10x\n10x\n5x\n6x\n6x\n5x\n7x\n26x\n37x\n10x\n78x\n61x\n8x\n25x\n14x\n15x\n23x\n10x\n15x\n7x\n13x\n14x\n5x\n8x\n7x\n7x\n9x\n37x\n59x\n18x\n101x\n102x\n14x\n39x\n25x\n20x\n36x\n34,522x\n61,533x\n14,826x\n119,797x\n76,784x\n11,828x\n9,485x\n10,904x\n8,053x\n24,207x\n1x\n10x\n100x\n1000x\n10000x\n100000x\nAlex-6\nAlex-7\nAlex-8\nVGG-6\nVGG-7\nVGG-8\nNT-We\nNT-Wd\nNT-LSTM\nGeo Mean\nEnergy Efficiency\nCPU Dense (Baseline)\nCPU Compressed\nGPU Dense\nGPU Compressed\nmGPU Dense\nmGPU Compressed\nEIE\nFigure 7.\nEnergy ef\ufb01ciency of GPU, mobile GPU and EIE compared with CPU running uncompressed DNN model. There is no batching in all cases.\nenergy numbers. We annotated the toggle rate from the RTL\nsimulation to the gate-level netlist, which was dumped to\nswitching activity interchange format (SAIF), and estimated\nthe power using Prime-Time PX.\nComparison Baseline. We compare EIE with three dif-\nferent off-the-shelf computing units: CPU, GPU and mobile\nGPU.\n1) CPU. We use Intel Core i-7 5930k CPU, a Haswell-E\nclass processor, that has been used in NVIDIA Digits Deep\nLearning Dev Box as a CPU baseline. To run the benchmark\non CPU, we used MKL CBLAS GEMV to implement the\noriginal dense model and MKL SPBLAS CSRMV for the\ncompressed sparse model. CPU socket and DRAM power\nare as reported by the pcm-power utility provided by Intel.\n2) GPU. We use NVIDIA GeForce GTX Titan X GPU,\na state-of-the-art GPU for deep learning as our baseline\nusing nvidia-smi utility to report the power. To run\nthe benchmark, we used cuBLAS GEMV to implement\nthe original dense layer. For the compressed sparse layer,\nwe stored the sparse matrix in in CSR format, and used\ncuSPARSE CSRMV kernel, which is optimized for sparse\nmatrix-vector multiplication on GPUs.\n3) Mobile GPU. We use NVIDIA Tegra K1 that has\n192 CUDA cores as our mobile GPU baseline. We used\ncuBLAS GEMV for the original dense model and cuS-\nPARSE CSRMV for the compressed sparse model. Tegra K1\ndoesn\u2019t have software interface to report power consumption,\nso we measured the total power consumption with a power-\nmeter, then assumed 15% AC to DC conversion loss, 85%\nregulator ef\ufb01ciency and 15% power consumed by peripheral\ncomponents [26], [27] to report the AP+DRAM power for\nTegra K1.\nBenchmarks. We compare the performance on two sets\nof models: uncompressed DNN model and the compressed\nDNN model. The uncompressed DNN model is obtained\nfrom Caffe model zoo [28] and NeuralTalk model zoo [7];\nThe compressed DNN model is produced as described\nTable III\nBENCHMARK FROM STATE-OF-THE-ART DNN MODELS\nLayer\nSize\nWeight%\nAct%\nFLOP%\nDescription\nAlex-6\n9216,\n9%\n35.1%\n3%\nCompressed\n4096\nAlexNet [1] for\nAlex-7\n4096,\n9%\n35.3%\n3%\nlarge scale image\n4096\nclassi\ufb01cation\nAlex-8\n4096,\n25%\n37.5%\n10%\n1000\nVGG-6\n25088,\n4%\n18.3%\n1%\nCompressed\n4096\nVGG-16 [3] for\nVGG-7\n4096,\n4%\n37.5%\n2%\nlarge scale image\n4096\nclassi\ufb01cation and\nVGG-8\n4096,\n23%\n41.1%\n9%\nobject detection\n1000\nNT-We\n4096,\n10%\n100%\n10%\nCompressed\n600\nNeuralTalk [7]\nNT-Wd\n600,\n11%\n100%\n11%\nwith RNN and\n8791\nLSTM for\nNTLSTM\n1201,\n10%\n100%\n11%\nautomatic\n2400\nimage captioning\nin [16], [23]. The benchmark networks have 9 layers in total\nobtained from AlexNet, VGGNet, and NeuralTalk. We use\nthe Image-Net dataset [29] and the Caffe [28] deep learning\nframework as golden model to verify the correctness of the\nhardware design.\nVI. EXPERIMENTAL RESULTS\nFigure 5 shows the layout (after place-and-route) of\nan EIE processing element. The power/area breakdown is\nshown in Table II. We brought the critical path delay down\nto 1.15ns by introducing 4 pipeline stages to update one\nactivation: codebook lookup and address accumulation (in\nparallel), output activation read and input activation multiply\n(in parallel), shift and add, and output activation write. Ac-\ntivation read and write access a local register and activation\nbypassing is employed to avoid a pipeline hazard. Using\n64 PEs running at 800MHz yields a performance of 102\nGOP/s. Considering 10\u00d7 weight sparsity and 3\u00d7 activation\nsparsity, this requires a dense DNN accelerator 3TOP/s to\nhave equivalent application throughput.\nTable IV\nWALL CLOCK TIME COMPARISON BETWEEN CPU, GPU, MOBILE GPU AND EIE. UNIT: \u00b5S\nPlatform\nBatch\nMatrix\nAlexNet\nVGG16\nNT-\nSize\nType\nFC6\nFC7\nFC8\nFC6\nFC7\nFC8\nWe\nWd\nLSTM\nCPU\n1\ndense\n7516.2\n6187.1\n1134.9\n35022.8\n5372.8\n774.2\n605.0\n1361.4\n470.5\n(Core\nsparse\n3066.5\n1282.1\n890.5\n3774.3\n545.1\n777.3\n261.2\n437.4\n260.0\ni7-5930k)\n64\ndense\n318.4\n188.9\n45.8\n1056.0\n188.3\n45.7\n28.7\n69.0\n28.8\nsparse\n1417.6\n682.1\n407.7\n1780.3\n274.9\n363.1\n117.7\n176.4\n107.4\nGPU\n1\ndense\n541.5\n243.0\n80.5\n1467.8\n243.0\n80.5\n65\n90.1\n51.9\n(Titan X)\nsparse\n134.8\n65.8\n54.6\n167.0\n39.8\n48.0\n17.7\n41.1\n18.5\n64\ndense\n19.8\n8.9\n5.9\n53.6\n8.9\n5.9\n3.2\n2.3\n2.5\nsparse\n94.6\n51.5\n23.2\n121.5\n24.4\n22.0\n10.9\n11.0\n9.0\nmGPU\n1\ndense\n12437.2\n5765.0\n2252.1\n35427.0\n5544.3\n2243.1\n1316\n2565.5\n956.9\n(Tegra K1)\nsparse\n2879.3\n1256.5\n837.0\n4377.2\n626.3\n745.1\n240.6\n570.6\n315\n64\ndense\n1663.6\n2056.8\n298.0\n2001.4\n2050.7\n483.9\n87.8\n956.3\n95.2\nsparse\n4003.9\n1372.8\n576.7\n8024.8\n660.2\n544.1\n236.3\n187.7\n186.5\nEIE\nTheoretical Time\n28.1\n11.7\n8.9\n28.1\n7.9\n7.3\n5.2\n13.0\n6.5\nActual Time\n30.3\n12.2\n9.9\n34.4\n8.7\n8.4\n8.0\n13.9\n7.5\n0%\n20%\n40%\n60%\n80%\n100%\nAlex-6\nAlex-7\nAlex-8\nVGG-6\nVGG-7\nVGG-8\nNT-We\nNT-Wd\nNT-LSTM\nLoad Balance \nFIFO=1\nFIFO=2\nFIFO=4\nFIFO=8\nFIFO=16\nFIFO=32\nFIFO=64\nFIFO=128\nFIFO=256\nFigure 8.\nLoad ef\ufb01ciency improves as FIFO size increases. When FIFO deepth>8, the marginal gain quickly diminishes. So we choose FIFO depth=8.\nThe total SRAM capacity (Spmat+Ptr+Act) of each EIE\nPE is 162KB. The activation SRAM is 2KB storing ac-\ntivations. The Spmat SRAM is 128KB storing the com-\npressed weights and indices. Each weight is 4bits, each\nindex is 4bits. Weights and indices are grouped to 8bits and\naddressed together. The Spmat access width is optimized\nat 64bits. The Ptr SRAM is 32KB storing the pointers in\nthe CSC format. In the steady state, both Spmat SRAM\nand Ptr SRAM are accessed every 64/8 = 8 cycles. The\narea and power is dominated by SRAM, the ratio is 93%\nand 59% respectively. Each PE is 0.638mm2 consuming\n9.157mW. Each group of 4 PEs needs a LNZD unit for\nnonzero detection. A total of 21 LNZD units are needed for\n64 PEs (16+4+1 = 21). Synthesized result shows that one\nLNZD unit takes only 0.023mW and an area of 189um2,\nless than 0.3% of a PE.\nA. Performance\nWe compare EIE against CPU, desktop GPU and the\nmobile GPU on 9 benchmarks selected from AlexNet, VGG-\n16 and Neural Talk. The overall results are shown in Fig-\nure 6. There are 7 columns for each benchmark, comparing\nthe computation time of EIE on compressed network over\nCPU / GPU / TK1 on uncompressed / compressed network.\nTime is normalized to CPU. EIE signi\ufb01cantly outperforms\nthe general purpose hardware and is, on average, 189\u00d7, 13\u00d7,\n307\u00d7 faster than CPU, GPU and mobile GPU respectively.\nEIE\u2019s theoretical computation time is calculated by divid-\ning workload GOPs by peak throughput. The actual compu-\ntation time is around 10% more than the theoretical compu-\ntation time due to load imbalance. In Fig. 6, the comparison\nwith CPU / GPU / TK1 is reported using actual computation\ntime. The wall clock time of CPU / GPU / TK1/ EIE for all\nbenchmarks are shown in Table IV.\nEIE is targeting extremely latency-focused applications,\nwhich require real-time inference. Since assembling a batch\nadds signi\ufb01cant amounts of latency, we consider the case\nwhen batch size = 1 when benchmarking the performance\nand energy ef\ufb01ciency with CPU and GPU as shown in\nFigure 6. As a comparison, we also provided the result for\nbatch size = 64 in Table IV. EIE outperforms most of the\nplatforms and is comparable to desktop GPU in the batching\ncase.\nThe GOP/s required for EIE to achieve the same appli-\ncation throughput (Frames/s) is much lower than competing\napproaches because EIE exploits sparsity to eliminate 97%\nof the GOP/s performed by dense approaches. 3 TOP/s on\nan uncompressed network requires only 100 GOP/s on a\ncompressed network. EIE\u2019s throughput is scalable to over\n256 PEs. Without EIE\u2019s dedicated logic, however, model\ncompression by itself applied on a CPU/GPU yields only\n3\u00d7 speedup.\nB. Energy\nIn Figure 7, we report the energy ef\ufb01ciency comparisons\nof M\u00d7V on different benchmarks. There are 7 columns\nfor each benchmark, comparing the energy ef\ufb01ciency of\nEIE on compressed network over CPU / GPU / TK1 on\nuncompressed / compressed network. Energy is obtained by\nmultiplying computation time and total measured power as\ndescribed in section V.\nEIE consumes on average, 24, 000\u00d7, 3, 400\u00d7, and\n2, 700\u00d7 less energy compared to CPU, GPU and the mobile\nGPU respectively. This is a 3-order of magnitude energy sav-\n45\n90\n135\n180\n225\nK\n125K\n250K\n375K\n500K\n32 bit\n64 bit\n128 bit 256 bit 512 bit\nRead Energy (pJ)\n# Read\nSRAM Width\nEnergy per Read (pJ)\n# Read\n0\n2100\n4200\n6300\n8400\n32bit\n64bit\n128bit\n256bit\n512bit\nEnergy (nJ) \nSRAM width\nAlex-6\nAlex-7\nAlex-8\nVGG-6\nVGG-7\nVGG-8\nNT-We\nNT-Wd\nNT-LSTM\nFigure 9.\nLeft: SRAM read energy and number of reads benchmarked on AlexNet. Right: Multiplying the two curves in the left gives the total energy\nconsumed by SRAM read.\n0.0\n1.0\n2.0\n3.0\n4.0\n0%\n23%\n45%\n68%\n90%\n32b Float\n32b Int\n16b Int\n8b Int\nMul Energy (pJ)\nAccuracy\nArithmetic Precision\nMultiply Energy (pJ)\nPrediction Accuracy\nFigure 10.\nPrediction accuracy and multiplier energy with different\narithmetic precision.\ning from three places: \ufb01rst, the required energy per memory\nread is saved (SRAM over DRAM): using a compressed\nnetwork model enables state-of-the-art neural networks to \ufb01t\nin on-chip SRAM, reducing energy consumption by 120\u00d7\ncompared to fetching a dense uncompressed model from\nDRAM (Figure 7). Second, the number of required memory\nreads is reduced. The compressed DNN model has 10% of\nthe weights where each weight is quantized by only 4 bits.\nLastly, taking advantage of vector sparsity saved 65.14%\nredundant computation cycles. Multiplying those factors\n120\u00d710\u00d78\u00d73 gives 28, 800\u00d7 theoretical energy saving. Our\nactual savings are about 10\u00d7 less than this number because\nof index overhead and because EIE is implemented in 45nm\ntechnology compared to the 28nm technology used by the\nTitan-X GPU and the Tegra K1 mobile GPU.\nC. Design Space Exploration\nQueue Depth. The activation FIFO queue deals with\nload imbalance between the PEs. A deeper FIFO queue can\nbetter decouple producer and consumer, but with diminish-\ning returns, as shown in our experiment in Figure 8. We\nvaried the FIFO queue depth from from 1 to 256 in powers\nof 2 across 9 benchmarks using 64 PEs, and measured\nthe load balance ef\ufb01ciency. This ef\ufb01ciency is de\ufb01ned as:\n1 \u2212bubble cycles (due to starvation) divided by total\ncomputation cycles. At FIFO size = 1, around half of the\ntotal cycles are idle and the accelerator suffers from severe\nload imbalance. Load imbalance is reduced as FIFO depth\nis increased but with diminishing returns beyond a depth of\n8. Thus, we choose 8 as the optimal queue depth.\nNotice the NT-We benchmark has poorer load balance\nef\ufb01ciency compared with others. This is because that it has\nonly 600 rows. Divided by 64 PEs and considering the 11%\nsparsity, each PE on average gets a single entry, which is\nhighly susceptible to variation among PEs, leading to load\nimbalance. Such small matrices are more ef\ufb01ciently executed\non 32 or fewer PEs.\nSRAM Width. We choose an SRAM with a 64-bit\ninterface to store the sparse matrix (Spmat) since it mini-\nmized the total energy. Wider SRAM interfaces reduce the\nnumber of total SRAM accesses, but increase the energy\ncost per SRAM read. The experimental trade-off is shown\nin Figure 9. SRAM energy is modeled using Cacti [25]\nunder 45nm process. SRAM access times are measured by\nthe cycle-accurate simulator on AlexNet benchmark. As the\ntotal energy is shown on the right, the minimum total access\nenergy is achieved when SRAM width is 64 bits. For larger\nSRAM widths, read data is wasted: the typical number of\nactivation elements of FC layer is 4K [1], [3] so assuming\n64 PEs and 10% density [16], each column in a PE will\nhave 6.4 elements on average. This matches a 64-bit SRAM\ninterface that provides 8 elements. If more elements are\nfetched and the next column corresponds to a zero activation,\nthose elements are wasted.\nArithmetic Precision. We use 16-bit \ufb01xed-point arith-\nmetic. As shown in Figure 10, 16-bit \ufb01xed-point multiplica-\ntion consumes 5\u00d7 less energy than 32-bit \ufb01xed-point and\n6.2\u00d7 less energy than 32-bit \ufb02oating-point. At the same\ntime, using 16-bit \ufb01xed-point arithmetic results in less than\n0.5% loss of prediction accuracy: 79.8% compared with\n80.3% using 32-bit \ufb02oating point arithmetic. With 8-bits\n\ufb01xed-point, however, the accuracy dropped to only 53%,\nwhich becomes intolerable. The accuracy is measured on\nImageNet dataset [29] with AlexNet [1], and the energy is\nobtained from synthesized RTL under 45nm process.\n\u2018Deep Compression\u2019 does not affect accuracy [16] [23]\n[30], but using 16-bit arithmetic degrades accuracy by\nonly 0.5%. In order to have absolute no loss of accuracy,\nswitching to 32 bit arithmetic would not substantially affect\nthe power or area of EIE. Only the 16-entry codebook,\nthe arithmetic units, and the activation register \ufb01les would\nchange in width. The index stored in SRAM would remain\n4-bits. The majority of power/area is taken by SRAM, not\narithmetic. The area used by \ufb01ller cells (used to \ufb01ll blank\narea) is suf\ufb01cient to double the area of the arithmetic units\nand activation registers (Table II).\n1\n10\n100\nAlex-6\nAlex-7\nAlex-8\nVGG-6\nVGG-7\nVGG-8\nNT-We\nNT-Wd\nNT-LSTM\nSpeedup\n1PE\n2PEs\n4PEs\n8PEs\n16PEs\n32PEs\n64PEs\n128PEs\n256PEs\nFigure 11.\nSystem scalability. It measures the speedups with different numbers of PEs. The speedup is near-linear.\nVII. DISCUSSION\nMany engines have been proposed for Sparse Matrix-\nVector multiplication (SPMV) and the existing trade-offs\non the targeted platforms are studied [21], [31]. There\nare typically three approaches to partition the workload\nfor matrix-vector multiplication. The combination of these\nmethods with storage format of the Matrix creates a design\nspace trade-off.\nA. Workload Partitioning\nThe \ufb01rst approach is to distribute matrix columns to PEs.\nEach PE handles the multiplication between its columns of\nW and corresponding element of a to get a partial sum\nof the output vector b. The bene\ufb01t of this solutions is that\neach element of a is only associated with one PE \u2014 giving\nfull locality for vector a. The drawback is that a reduction\noperation between PEs is required to obtain the \ufb01nal result.\nA second approach (ours) is to distribute matrix rows to\nPEs. A central unit broadcasts one vector element aj to all\nPEs. Each PE computes a number of output activations bi by\nperforming inner products of the corresponding row of W,\nWj that is stored in the PE with vector a. The bene\ufb01t of this\nsolutions is that each element of b is only associated with\none PE \u2014 giving full locality for vector b. The drawback is\nthat vector a needs to be broadcast to all PEs.\nA third approach combines the previous two approaches\nby distributing blocks of W to the PEs in 2D fashion.\nThis solution is more scalable for distributed systems where\ncommunication latency cost is signi\ufb01cant [32]. This way\nboth of the collective communication operations \u201dBroadcast\u201d\nand \u201dReduction\u201d are exploited but in a smaller scale and\nhence this solution is more scalable.\nThe nature of our target class of application and its\nsparsity pattern affects the constraints and therefore our\nchoice of partitioning and storage. The density of W is\n\u224810%, and the density of a is \u224830%, both with random\ndistribution. Vector a is stored in normal dense format and\ncontains 70% the zeros in the memory, because for different\ninput, aj\u2019s sparsity pattern differs. We want to utilize the\nsparsity of both W and a.\nThe \ufb01rst solution suffers from load imbalance given that\nvector a is also sparse. Each PE is responsible for a column.\nPEj will be completely idle if their corresponding element\naj is zero. On top of the Idle PEs, this solution requires\nacross-PE reduction and extra level of synchronization.\nSince the SPMV engine, has a limited number of PEs,\nthere won\u2019t be a scalability issue to worry about. However,\nthe hybrid solution will suffer from inherent complexity and\nstill possible load imbalance since multiple PEs sharing the\nsame column might remain idle.\nWe build our solution based on the second distribution\nscheme taking the 30% density of vector a into account. Our\nsolution aims to perform computations by in-order look-up\nof nonzeros in a. Each PE gets all the non-zero elements of\na in order and performs the inner products by looking-up\nthe matching element that needs to be multiplied by aj, Wj.\nThis requires the matrix W being stored in CSC format so\nthe PE can multiply all the elements in the j-th column of\nW by aj.\nB. Scalability\nAs the matrix gets larger, the system can be scaled up by\nadding more PEs. Each PE has local SRAM storing distinct\nrows of the matrix without duplication, so the SRAM is\nef\ufb01ciently utilized.\nWire delay increases with the square root of the number of\nPEs, however, this is not a problem in our architecture. Since\nEIE only requires one broadcast over the computation of the\nentire column, which takes many cycles. Consequently, the\nbroadcast is not on the critical path and can be pipelined\nbecause FIFOs decouple producer and consumer.\nFigure 11 shows EIE achieves good scalability on all\nbenchmarks except NT-We. NT-We is very small (4096 \u00d7\n600). Dividing the columns of size 600 and sparsity 10% to\n64 or more PEs causes serious load imbalance.\nFigure 12 shows the number of padding zeros with\ndifferent number PEs. Padding zero occur when the jump\nbetween two consecutive non-zero element in the sparse\nmatrix is larger than 16, the largest number that 4 bits can\nencode. Padding zeros are considered non-zero and lead\nto wasted computation. Using more PEs reduces padding\nzeros, because the distance between non-zero elements get\nsmaller due to matrix partitioning, and 4-bits encoding a\nmax distance of 16 will more likely be enough.\nFigure 13 shows the load balance with different number\nof PEs, measured with FIFO depth equal to 8. With more\nPEs, load balance becomes worse, but padding zero overhead\ndecreases, which yields ef\ufb01ciency for most benchmarks\nremain constant. The scalability result is plotted in \ufb01gure\n11.\n0%\n20%\n40%\n60%\n80%\n100%\nAlex-6\nAlex-7\nAlex-8\nVGG-6\nVGG-7\nVGG-8\nNT-We\nNT-Wd\nNT-LSTM\nReal Work /\nTotal Work\n1PE\n2PEs\n4PEs\n8PEs\n16PEs\n32PEs\n64PEs\n128PEs\n256PEs\nFigure 12.\nAs the number of PEs goes up, the number of padding zeros decreases, leading to less padding zeros and less redundant work, thus better\ncompute ef\ufb01ciency.\n0%\n20%\n40%\n60%\n80%\n100%\nAlex-6\nAlex-7\nAlex-8\nVGG-6\nVGG-7\nVGG-8\nNT-We\nNT-Wd\nNT-LSTM\nLoad Balance\n1PE\n2PEs\n4PEs\n8PEs\n16PEs\n32PEs\n64PEs\n128PEs\n256PEs\nFigure 13.\nLoad ef\ufb01ciency is measured by the ratio of stalled cycles over total cycles in ALU. More PEs lead to worse load balance, but less padding\nzeros and more useful computation.\nC. Flexibility\nEIE is designed for large neural networks. The weights\nand input/ouput of most layers can be easily \ufb01t into EIE\u2019s\nstorage. For those with extremely large input/output sizes\n(for example, FC6 layer of VGG-16 has an input size of\n25088), EIE is still able to execute them with 64PEs.\nEIE can assist general-purpose processors for sparse neu-\nral network acceleration or other tasks related to SPMV. One\ntype of neural network structure can be decomposed into\ncertain control sequence so that by writing control sequence\nto the registers of EIE, a network could be executed.\nEIE has the potential to support 1x1 convolution and 3x3\nWinograd convolution by turning the channel-wise reduction\ninto an M \u00d7V . Winograd convolution saves 2.25\u00d7 multipli-\ncations than naive convolution [33], and for each Winograd\npatch the 16 M \u00d7 V can be scheduled on an EIE.\nVIII. COMPARISON WITH RELATED WORK\nWe compare the results of performance, power and area\non M \u00d7 V in Table V. The performance is evaluated on\nthe FC7 layer of AlexNet1. We compared six platforms\nfor neural networks: Core-i7 (CPU), Titan X (GPU), Tegra\nK1 (mobile GPU), A-Eye (FPGA), DaDianNao (ASIC),\nTrueNorth (ASIC). All other four platforms suffer from low-\nef\ufb01ciency during matrix-vector multiplication. A-Eye is opti-\nmized for CONV layers and all of the parameters are fetched\nfrom the external DDR3 memory, making it extremely sen-\nsitive to bandwidth problem. DaDianNao distributes weights\non 16 tiles, each tile with 4 eDRAM banks, thus has a peak\nmemory bandwidth of 16 \u00d7 4 \u00d7 (1024bit/8) \u00d7 606MHz =\n4964GB/s. Its performance on M\u00d7V is estimated based on\nthe peak memory bandwidth because M \u00d7 V is completely\nmemory bound. In contrast, EIE maintains a high throughput\nfor M \u00d7 V because after compression, all weights \ufb01t in\non-chip SRAM, even for very large-scale networks. With\n1Except for TrueNorth where FC7 result is not provided, using TIMIT\nLSTM result for comparison instead (different benchmarks differ < 2\u00d7).\n256PEs, EIE has 3.25\u00d7 more throughput than 64PEs and\ncan hold 336 million parameters, even larger than VGGnet.\nThe right column projected EIE to the same technology\n(28nm) as the other platforms, with 256PEs, EIE has 2.9\u00d7\nthroughput, 3\u00d7 area ef\ufb01ciency and 19\u00d7 power ef\ufb01ciency\nthan DaDianNao.\nModel Compression. DNN model compression is widely\nused to reduce the storage required by DNN models. In early\nwork, network pruning proved to be a promising approach to\nreducing the network complexity and over-\ufb01tting [34]\u2013[36].\nRecently Han et al. [16] pruned connections in large scale\nneural networks and achieved 9\u00d7 and 13\u00d7 pruning rate for\nAlexNet and VGG-16 with no loss of accuracy on ImageNet.\nThe followup work \u2018Deep Compression\u2019 [23] compressed\nDNN by pruning, weight sharing and Huffman coding,\npushing the compression ratio to 35-49\u00d7. SqueezeNet [30]\nfurther pushes this ratio to 510\u00d7: it combines new ConvNet\narchitecture and Deep Compression, its model size is only\n470KB-660KB and has the same prediction accuracy as\nAlexNet. SqueezeNet makes it easy to \ufb01t DNN model fully\nin SRAM and is available online2. SVD is frequently used\nto reduce model size [37] [38]. Minerva [39] also uses data\nquantization to save memory energy.\nModel compression is crucial for reducing memory en-\nergy. However, model compression by itself applied on\na GPU yields only 3\u00d7 energy savings [23], while EIE\nincreases this to 3000\u00d7 by tailoring an architecture to exploit\nthe irregularity and decoding created by model compression.\nDNN accelerator. Many custom accelerators have been\nproposed for DNNs. DianNao [10] implements an array of\nmultiply-add units to map large DNN onto its core architec-\nture. Due to limited SRAM resource, the off-chip DRAM\ntraf\ufb01c dominates the energy consumption. DaDianNao [11]\nand ShiDianNao [12] eliminate the DRAM access by having\nall weights on-chip (eDRAM or SRAM).\n2The 660KB version of SqueezeNet model trained on Imagenet is\navailable at: http://songhan.github.io/SqueezeNet-Deep-Compression\nTable V\nCOMPARISON WITH EXISTING HARDWARE PLATFORMS FOR DNNS.\nPlatform\nCore-i7\n5930K\nGeForce\nTitan X\nTegra\nK1\nA-Eye\n[14]\nDa-\nDianNao\n[11]\nTrue-\nNorth\n[40]\nEIE\n(ours,\n64PE)\nEIE\n(28nm,\n256PE)\nYear\n2014\n2015\n2014\n2015\n2014\n2014\n2016\n2016\nPlatform Type\nCPU\nGPU\nmGPU\nFPGA\nASIC\nASIC\nASIC\nASIC\nTechnology\n22nm\n28nm\n28nm\n28nm\n28nm\n28nm\n45nm\n28nm\nClock (MHz)\n3500\n1075\n852\n150\n606\nAsync\n800\n1200\nMemory type\nDRAM\nDRAM\nDRAM\nDRAM\neDRAM\nSRAM\nSRAM\nSRAM\nMax DNN model size (#Params)\n<16G\n<3G\n<500M\n<500M\n18M\n256M\n84M\n336M\nQuantization Stategy\n32-bit\n\ufb02oat\n32-bit\n\ufb02oat\n32-bit\n\ufb02oat\n16-bit\n\ufb01xed\n16-bit\n\ufb01xed\n1-bit\n\ufb01xed\n4-bit\n\ufb01xed\n4-bit\n\ufb01xed\nArea (mm2)\n356\n601\n-\n-\n67.7\n430\n40.8\n63.8\nPower (W)\n73\n159\n5.1\n9.63\n15.97\n0.18\n0.59\n2.36\nM\u00d7V Throughput (Frames/s)\n162\n4,115\n173\n33\n147,938\n1,989\n81,967\n426,230\nArea Ef\ufb01ciency ( Frames/s/mm2)\n0.46\n6.85\n-\n-\n2,185\n4.63\n2,009\n6,681\nEnergy Ef\ufb01ciency (Frames/J)\n2.22\n25.9\n33.9\n3.43\n9,263\n10,839\n138,927\n180,606\nIn both architectures, the weights are uncompressed and\nstored in the dense format. As a result, ShiDianNao can\nonly handle very small DNN models up to 64K parameters,\nwhich is 3 orders of magnitude smaller than the 60 Mil-\nlion parameter AlexNet by only containing 128KB on-chip\nRAM. Such large networks are impossible to \ufb01t on chip on\nShiDianNao without compression.\nDaDianNao stores the uncompressed model in eDRAM\ntaking 6.12W memory power and 15.97W total power. EIE\nstores compressed model in SRAM taking only 0.35W\nmemory power and only 0.59W total power; DaDianNao\ncannot exploit the sparsity from weights and activations\nand they must expand the network to dense form before\noperation. It can not exploit weight sharing either. Using just\nthe compression (and decompressing the data before com-\nputation) would reduce DaDianNao total power to around\n10W in 28nm, compared to EIEs power of 0.58W in 45nm.\nPrevious DNN accelerators targeting ASIC and FPGA\nplatforms [10] [41] used mostly CONV layer as benchmarks,\nbut have few dedicated experiments on FC layers, which\nhas signi\ufb01cant bandwidth bottlenecks, and is widely used\nin RNN and LSTMs. Loading weights from the external\nmemory for the FC layer may signi\ufb01cantly degrade the\noverall performance of the network [14].\nSparse\nMatrix-Vector\nMultiplication\nAccelerator.\nThere is research effort on the implementation of sparse\nmatrix-vector multiplication (SPMV) on general-purpose\nprocessors. Monakov et al. [42] proposed a matrix storage\nformat that improves locality, which has low memory foot-\nprint and enables automatic parameter tuning on GPU. Bell\net al. [43] implemented data structures and algorithms for\nSPMV on GeForce GTX 280 GPU and achieved perfor-\nmance of 36 GFLOP/s in single precision and 16 GFLOP/s\nin double precision. [44] developed SPMV techniques that\nutilizes large percentages of peak bandwidth for throughput-\noriented architectures like the GPU. They achieved over an\norder of magnitude performance improvement over a quad-\ncore Intel Clovertown system.\nTo pursue a better computational ef\ufb01ciency, several recent\nworks focus on using FPGA as an accelerator for SPMV.\nZhuo et al. [31] proposed an FPGA-based design on Virtex-\nII Pro for SPMV. Their design outperforms general-purpose\nprocessors, but the performance is limited by memory band-\nwidth. Fowers et al. [45] proposed a novel sparse matrix\nencoding and an FPGA-optimized architecture for SPMV.\nWith lower bandwidth, it achieves 2.6\u00d7 and 2.3\u00d7 higher\npower ef\ufb01ciency over CPU and GPU respectively while\nhaving lower performance due to lower memory bandwidth.\nDorrance et al. [21] proposed a scalable SMVM kernel on\nVirtex-5 FPGA. It outperforms CPU and GPU counterparts\nwith>300\u00d7 computational ef\ufb01ciency and has 38-50\u00d7 im-\nprovement in energy ef\ufb01ciency.\nFor compressed deep networks, previously proposed\nSPMV accelerators can only exploit the static weight spar-\nsity. They are unable to exploit dynamic activation spar-\nsity (3\u00d7), and they are unable to exploit weight sharing (8\u00d7),\naltogether 24\u00d7 energy saving is lost.\nIX. CONCLUSION\nFully-connected layers of deep neural networks perform a\nmatrix-vector multiplication. For real-time networks where\nbatching cannot be employed to improve re-use, these layers\nare memory limited. To improve the ef\ufb01ciency of these\nlayers, one must reduce the energy needed to fetch their\nparameters.\nThis paper presents EIE, an energy-ef\ufb01cient engine opti-\nmized to operate on compressed deep neural networks. By\nleveraging sparsity in both the activations and the weights,\nand taking advantage of weight sharing and quantization,\nEIE reduces the energy needed to compute a typical FC\nlayer by 3,400\u00d7 compared with GPU. This energy saving\ncomes from four main factors: the number of parameters is\npruned by 10\u00d7; Weight-sharing reduced the weights to only\n4 bits. Then the smaller model can be fetched from SRAM\nand not DRAM, giving a 120\u00d7 energy advantage; and since\nthe activation vector is also sparse, only 30% of the matrix\ncolumns need to be fetched for a \ufb01nal 3\u00d7 savings. These\nsavings enable an EIE PE to do 1.6 GOPS in an area of\n0.64mm2 and dissipate only 9mW. 64 PEs can process FC\nlayers of AlexNet at 1.88\u00d7104 frames/sec. The architecture\nis scalable from one PE to over 256 PEs with nearly linear\nscaling of energy and performance. On 9 fully-connected\nlayer benchmarks, EIE outperforms CPU, GPU and mobile\nGPU by factors of 189\u00d7, 13\u00d7 and 307\u00d7, and consumes\n24, 000\u00d7, 3, 400\u00d7 and 2, 700\u00d7 less energy than CPU, GPU\nand mobile GPU, respectively.\nREFERENCES\n[1] A. Krizhevsky, I. Sutskever, and G. E. Hinton, \u201cImagenet classi\ufb01ca-\ntion with deep convolutional neural networks,\u201d in NIPS, 2012.\n[2] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov,\nD. Erhan, V. Vanhoucke, and A. Rabinovich, \u201cGoing deeper with\nconvolutions,\u201d arXiv:1409.4842, 2014.\n[3] K. Simonyan and A. Zisserman, \u201cVery deep convolutional networks\nfor large-scale image recognition,\u201d arXiv:1409.1556, 2014.\n[4] T. Mikolov, M. Kara\ufb01\u00b4at, L. Burget, J. Cernock`y, and S. Khudan-\npur, \u201cRecurrent neural network based language model.\u201d in INTER-\nSPEECH, September 26-30, 2010, 2010, pp. 1045\u20131048.\n[5] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner, \u201cGradient-based\nlearning applied to document recognition,\u201d Proceedings of the IEEE,\nvol. 86, no. 11, pp. 2278\u20132324, 1998.\n[6] Y. Taigman, M. Yang, M. Ranzato, and L. Wolf, \u201cDeepface: Closing\nthe gap to human-level performance in face veri\ufb01cation,\u201d in CVPR.\nIEEE, 2014, pp. 1701\u20131708.\n[7] A. Karpathy and L. Fei-Fei, \u201cDeep visual-semantic alignments for\ngenerating image descriptions,\u201d arXiv:1412.2306, 2014.\n[8] A. Coates, B. Huval, T. Wang, D. Wu, B. Catanzaro, and N. Andrew,\n\u201cDeep learning with cots hpc systems,\u201d in 30th ICML, 2013.\n[9] M. Horowitz. Energy table for 45nm process, Stanford VLSI wiki.\n[Online]. Available: https://sites.google.com/site/seecproject\n[10] T. Chen, Z. Du, N. Sun, J. Wang, C. Wu, Y. Chen, and O. Temam,\n\u201cDiannao: a small-footprint high-throughput accelerator for ubiqui-\ntous machine-learning,\u201d in ASPLOS, 2014.\n[11] Y. Chen, T. Luo, S. Liu, S. Zhang, L. He, J. Wang, L. Li, T. Chen,\nZ. Xu, N. Sun, and O. Temam, \u201cDadiannao: A machine-learning\nsupercomputer,\u201d in MICRO, December 2014.\n[12] Z. Du, R. Fasthuber, T. Chen, P. Ienne, L. Li, T. Luo, X. Feng,\nY. Chen, and O. Temam, \u201cShidiannao: shifting vision processing\ncloser to the sensor,\u201d in ISCA.\nACM, 2015, pp. 92\u2013104.\n[13] C. Farabet, C. Poulet, J. Y. Han, and Y. LeCun, \u201cCnp: An fpga-based\nprocessor for convolutional networks,\u201d in FPL, 2009.\n[14] J. Qiu, J. Wang, S. Yao, K. Guo, B. Li, E. Zhou, J. Yu, T. Tang, N. Xu,\nS. Song, Y. Wang, and H. Yang, \u201cGoing deeper with embedded fpga\nplatform for convolutional neural network,\u201d in FPGA, 2016.\n[15] A. Sha\ufb01ee and et al., \u201cISAAC: A convolutional neural network\naccelerator with in-situ analog arithmetic in crossbars,\u201d ISCA, 2016.\n[16] S. Han, J. Pool, J. Tran, and W. J. Dally, \u201cLearning both weights\nand connections for ef\ufb01cient neural networks,\u201d in Proceedings of\nAdvances in Neural Information Processing Systems, 2015.\n[17] R. Girshick, \u201cFast R-CNN,\u201d arXiv:1504.08083, 2015.\n[18] S. Hochreiter and J. Schmidhuber, \u201cLong short-term memory,\u201d Neural\ncomputation, 1997.\n[19] A. Graves and J. Schmidhuber, \u201cFramewise phoneme classi\ufb01cation\nwith bidirectional lstm and other neural network architectures,\u201d\nNeural Networks, 2005.\n[20] N. D. Lane and P. Georgiev, \u201cCan deep learning revolutionize mobile\nsensing?\u201d in International Workshop on Mobile Computing Systems\nand Applications.\nACM, 2015, pp. 117\u2013122.\n[21] Richard Dorrance and Fengbo Ren and Dejan Markovi\u00b4c, \u201cA Scal-\nable Sparse Matrix-vector Multiplication Kernel for Energy-ef\ufb01cient\nSparse-blas on FPGAs,\u201d in FPGA, 2014.\n[22] V. Nair and G. E. Hinton, \u201cRecti\ufb01ed linear units improve restricted\nboltzmann machines,\u201d in ICML, 2010.\n[23] S. Han, H. Mao, and W. J. Dally, \u201cDeep compression: Compressing\ndeep neural networks with pruning, trained quantization and huffman\ncoding,\u201d International Conference on Learning Representations 2016.\n[24] R. W. Vuduc, \u201cAutomatic performance tuning of sparse matrix\nkernels,\u201d Ph.D. dissertation, UC Berkeley, 2003.\n[25] N. Muralimanohar, R. Balasubramonian, and N. P. Jouppi, \u201cCacti 6.0:\nA tool to model large caches,\u201d HP Laboratories, pp. 22\u201331, 2009.\n[26] NVIDIA. Technical brief: NVIDIA jetson TK1 development kit\nbringing GPU-accelerated computing to embedded systems.\n[27] NVIDIA. Whitepaper: GPU-based deep learning inference: A perfor-\nmance and power analysis.\n[28] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick,\nS. Guadarrama, and T. Darrell, \u201cCaffe: Convolutional architecture for\nfast feature embedding,\u201d arXiv:1408.5093, 2014.\n[29] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei,\n\u201cImagenet: A large-scale hierarchical image database,\u201d in Computer\nVision and Pattern Recognition. 2009.\n[30] F. N. Iandola, S. Han, M. W. Moskewicz, K. Ashraf, W. J. Dally,\nand K. Keutzer, \u201cSqueezenet: Alexnet-level accuracy with 50x fewer\nparameters and\u00a1 0.5mb model size,\u201d arXiv:1602.07360, 2016.\n[31] Ling Zhuo and Viktor K. Prasanna, \u201cSparse Matrix-Vector Multipli-\ncation on FPGAs,\u201d in FPGA, 2005.\n[32] V. Eijkhout, LAPACK working note 50: Distributed sparse data\nstructures for linear algebra operations, 1992.\n[33] A. Lavin, \u201cFast algorithms for convolutional neural networks,\u201d\narXiv:1509.09308, 2015.\n[34] S. J. Hanson and L. Y. Pratt, \u201cComparing biases for minimal network\nconstruction with back-propagation,\u201d in NIPS, 1989.\n[35] Y. LeCun, J. S. Denker, S. A. Solla, R. E. Howard, and L. D. Jackel,\n\u201cOptimal brain damage.\u201d in NIPs, vol. 89, 1989.\n[36] B. Hassibi, D. G. Stork et al., \u201cSecond order derivatives for network\npruning: Optimal brain surgeon,\u201d Advances in neural information\nprocessing systems, pp. 164\u2013164, 1993.\n[37] E. L. Denton, W. Zaremba, J. Bruna, Y. LeCun, and R. Fergus, \u201cEx-\nploiting linear structure within convolutional networks for ef\ufb01cient\nevaluation,\u201d in NIPS 2014.\n[38] X. Zhang, J. Zou, X. Ming, K. He, and J. Sun, \u201cEf\ufb01cient\nand accurate approximations of nonlinear convolutional networks,\u201d\narXiv:1411.4229, 2014.\n[39] B. Reagen, P. Whatmough, R. Adolf, S. Rama, H. Lee, S. K. Lee,\nJ. M. Hernndez-Lobato, G.-Y. Wei, and D. Brooks, \u201cMinerva: En-\nabling low-power, highly-accurate deep neural network accelerators,\u201d\nISCA, 2016.\n[40] S. K. Esser and et al., \u201cConvolutional networks for fast, energy-\nef\ufb01cient neuromorphic computing,\u201d arXiv:1603.08270, 2016.\n[41] C. Zhang, P. Li, G. Sun, Y. Guan, B. Xiao, and J. Cong, \u201cOpti-\nmizing fpga-based accelerator design for deep convolutional neural\nnetworks,\u201d in FPGA, 2015.\n[42] Alexander Monakov and Anton Lokhmotov and Arutyun Avetisyan,\n\u201cAutomatically tuning sparse matrix-vector multiplication for GPU\narchitectures,\u201d in HiPEAC, 2010.\n[43] N. Bell and M. Garland, \u201cEf\ufb01cient sparse matrix-vector multiplication\non cuda,\u201d Nvidia Technical Report NVR-2008-004, Tech. Rep., 2008.\n[44] Bell, Nathan and Garland, Michael, \u201cImplementing Sparse Matrix-\nvector Multiplication on Throughput-oriented Processors,\u201d in High\nPerformance Computing Networking, Storage and Analysis, 2009.\n[45] J. Fowers and K. Ovtcharov and K. Strauss and E.S. Chung and G.\nStitt, \u201cA high memory bandwidth fpga accelerator for sparse matrix-\nvector multiplication,\u201d in FCCM, 2014.\n",
        "sentence": " Energy efficient inference engine (EIE) uses compression by pruning the redundant connections and having multiple connections share the same weight [25].",
        "context": "to \ufb01t large DNNs (AlexNet and VGGNet) fully in on-chip\nSRAM. This compression is achieved by pruning the redundant\nconnections and having multiple connections share the same\nweight. We propose an energy ef\ufb01cient inference engine (EIE)\nmized to operate on compressed deep neural networks. By\nleveraging sparsity in both the activations and the weights,\nand taking advantage of weight sharing and quantization,\nEIE reduces the energy needed to compute a typical FC\nEIE: Ef\ufb01cient Inference Engine on Compressed Deep Neural Network\nSong Han\u2217Xingyu Liu\u2217Huizi Mao\u2217Jing Pu\u2217Ardavan Pedram\u2217\nMark A. Horowitz\u2217William J. Dally\u2217\u2020\n\u2217Stanford University, \u2020NVIDIA\n{songhan,xyl,huizi,jingpu,perdavan,horowitz,dally}@stanford.edu"
    }
]