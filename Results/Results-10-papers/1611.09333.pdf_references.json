[
    {
        "title": "A Wavelet Tour of Signal Processing: The Sparse Way, 3rd ed",
        "author": [
            "S. Mallat"
        ],
        "venue": null,
        "citeRegEx": "1",
        "shortCiteRegEx": "1",
        "year": 2008,
        "abstract": "",
        "full_text": "",
        "sentence": " Sparse representation models [1], [2], [3], [4] and related machine learning algorithms [5], [6] have proven remarkably successful at extracting useful information from complex highdimensional signals, for example in the context of denoising [7], under-determined source separation [8], [9], compressed sensing [10], super-resolution sensing [11], and classification [12].",
        "context": null
    },
    {
        "title": "From sparse solutions of systems of equations to sparse modeling of signals and images",
        "author": [
            "A. Bruckstein",
            "D. Donoho",
            "M. Elad"
        ],
        "venue": "SIAM Review, vol. 51, no. 1, pp. 34\u201381, 2009.",
        "citeRegEx": "2",
        "shortCiteRegEx": null,
        "year": 2009,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " Sparse representation models [1], [2], [3], [4] and related machine learning algorithms [5], [6] have proven remarkably successful at extracting useful information from complex highdimensional signals, for example in the context of denoising [7], under-determined source separation [8], [9], compressed sensing [10], super-resolution sensing [11], and classification [12]. Since that time several probabilistic dictionary learning and sparse coding methods have been developed [2], [5], aiming for a dictionary that either maximizes the",
        "context": null
    },
    {
        "title": "Sparse and redundant representations: from theory to applications in signal and image processing",
        "author": [
            "M. Elad"
        ],
        "venue": null,
        "citeRegEx": "3",
        "shortCiteRegEx": "3",
        "year": 2010,
        "abstract": "",
        "full_text": "",
        "sentence": " Sparse representation models [1], [2], [3], [4] and related machine learning algorithms [5], [6] have proven remarkably successful at extracting useful information from complex highdimensional signals, for example in the context of denoising [7], under-determined source separation [8], [9], compressed sensing [10], super-resolution sensing [11], and classification [12].",
        "context": null
    },
    {
        "title": "The cosparse analysis model and algorithms",
        "author": [
            "S. Nam",
            "M. Davies",
            "M. Elad",
            "R. Gribonval"
        ],
        "venue": "Applied and Computational Harmonic Analysis, vol. 34, no. 1, pp. 30\u201356, 2013.",
        "citeRegEx": "4",
        "shortCiteRegEx": null,
        "year": 2013,
        "abstract": "After a decade of extensive study of the sparse representation synthesis\nmodel, we can safely say that this is a mature and stable field, with clear\ntheoretical foundations, and appealing applications. Alongside this approach,\nthere is an analysis counterpart model, which, despite its similarity to the\nsynthesis alternative, is markedly different. Surprisingly, the analysis model\ndid not get a similar attention, and its understanding today is shallow and\npartial. In this paper we take a closer look at the analysis approach, better\ndefine it as a generative model for signals, and contrast it with the synthesis\none. This work proposes effective pursuit methods that aim to solve inverse\nproblems regularized with the analysis-model prior, accompanied by a\npreliminary theoretical study of their performance. We demonstrate the\neffectiveness of the analysis model in several experiments.",
        "full_text": "arXiv:1106.4987v1  [math.NA]  24 Jun 2011\nThe Cosparse Analysis Model and Algorithms\u2729\nS. Nama, M. E. Daviesb, M. Eladc, R. Gribonvala\naCentre de Recherche INRIA Rennes - Bretagne Atlantique, Campus de Beaulieu, F-35042\nRennes, France\nbSchool of Engineering and Electronics, The University of Edinburgh, Edinburgh, EH9 3JL,\nUK\ncDepartment of Computer Science, The Technion, Haifa 32000, Israel\nAbstract\nAfter a decade of extensive study of the sparse representation synthesis model,\nwe can safely say that this is a mature and stable \ufb01eld, with clear theoretical\nfoundations, and appealing applications. Alongside this approach, there is an\nanalysis counterpart model, which, despite its similarity to the synthesis alter-\nnative, is markedly di\ufb00erent. Surprisingly, the analysis model did not get a\nsimilar attention, and its understanding today is shallow and partial.\nIn this paper we take a closer look at the analysis approach, better de\ufb01ne it\nas a generative model for signals, and contrast it with the synthesis one. This\nwork proposes e\ufb00ective pursuit methods that aim to solve inverse problems regu-\nlarized with the analysis-model prior, accompanied by a preliminary theoretical\nstudy of their performance. We demonstrate the e\ufb00ectiveness of the analysis\nmodel in several experiments.\nKeywords: Synthesis, Analysis, Sparse Representations, Union of Subspaces,\nPursuit Algorithms, Greedy Algorithms, Compressed-Sensing.\n\u2729This work was supported in part by the EU through the project SMALL (Sparse Models,\nAlgorithms and Learning for Large-Scale data), FET-Open programme, under grant number:\n225913\nEmail addresses: sangnam.nam@inria.fr (S. Nam), Mike.Davies@ed.ac.uk\n(M. E. Davies), elad@cs.technion.ac.il (M. Elad), remi.gribonval@inria.fr\n(R. Gribonval)\nPreprint submitted to Elsevier\nOctober 10, 2018\n1. Introduction\nSituated at the heart of signal and image processing, data models are fun-\ndamental for stabilizing the solution of inverse problems, and enabling various\nother tasks, such as compression, detection, separation, sampling, and more.\nWhat are those models? Essentially, a model poses a set of mathematical prop-\nerties that the data is believed to satisfy. Choosing these properties (i.e. the\nmodel) carefully and wisely may lead to a highly e\ufb00ective treatment of the\nsignals in question and consequently to successful applications.\nThroughout the years, a long series of models has been proposed and used,\nexhibiting an evolution of ideas and improvements. In this context, the past\ndecade has been certainly the era of sparse and redundant representations, a\nnovel synthesis model for describing signals [21, 5, 33, 40].\nHere is a brief\ndescription of this model:\nAssume that we are to model the signal x \u2208Rd. The sparse and redundant\nsynthesis model suggests that this signal could be described as x = Dz, where\nD \u2208Rd\u00d7n is a possibly redundant dictionary (n \u2265d), and z \u2208Rn, the signal\u2019s\nrepresentation, is assumed to be sparse. Measuring the cardinality of non-zeros\nof z using the \u2018\u21130-norm\u2019, such that \u2225z\u22250 is the count of the non-zeros in z, we\nexpect \u2225z\u22250 to be much smaller than n. Thus, the model essentially assumes that\nany signal from the family of interest could be described as a linear combination\nof few columns from the dictionary D. The name \u201csynthesis\u201d comes from the\nrelation x = Dz, with the obvious interpretation that the model describes a\nway to synthesize a signal.\nThis model has been the focus of many papers, studying its core theoretical\nproperties by exploring practical numerical algorithms for using it in practice\n(e.g.\n[10, 32, 7, 11]), evaluating theoretically these algorithms\u2019 performance\nguarantees (e.g. [25, 16, 41, 42, 2]), addressing ways to obtain the dictionary\nfrom a bulk of data (e.g.\n[22, 1, 30, 38]), and beyond all these, attacking\na long series of applications in signal and image processing with this model,\ndemonstrating often state-of-the-art results (e.g. [20, 18, 28, 34]). Today, after\na decade of an extensive study along the above lines, with nearly 4000 papers1\nwritten on this model and related issues, we can safely say that this is a mature\nand stable \ufb01eld, with clear theoretical foundations, and appealing applications.\nInterestingly, the synthesis model has a \u201ctwin\u201d that takes an analysis point of\nview. This alternative assumes that for a signal of interest, the analyzed vector\n\u2126x is expected to be sparse, where \u2126\u2208Rp\u00d7d is a possibly redundant analysis\noperator (p \u2265d). Thus, we consider a signal as belonging to the analysis model\nif \u2225\u2126x\u22250 is small enough. Common examples of analysis operators include: the\nshift invariant wavelet transform \u2126WT [33]; the \ufb01nite di\ufb00erence operator \u2126DIF,\nwhich concatenates the horizontal and vertical derivatives of an image and is\n1This is a crude estimate,\nobtained using ISI-Web-of-Science.\nBy \ufb01rst searching\nTopic=(sparse and representation and (dictionary or pursuit or sensing)), 240 papers are\nobtained. Then we consider all the papers that cite the above-found, and this results with\n\u22483900 papers.\n2\nclosely connected to total variation [36]; the curvelet transform [39], and more.\nEmpirically, analysis models have been successfully used for a variety of signal\nprocessing tasks such as denoising, deblurring, and most recently compressed\nsensing, but this has been done with little theoretical justi\ufb01cation.\nIt is well known by now [19] that for a square and invertible dictionary, the\nsynthesis and the analysis models are the same with D = \u2126\u22121. The models\nremain similar for more general dictionaries, although then the gap between\nthem is unexplored. Despite the close-proximity between the two \u2013 synthesis\nand analysis \u2013 models, the \ufb01rst has been studied extensively while the second\nhas been left aside almost untouched. In this paper we aim to bring justice to\nthe analysis model by addressing the following set of topics:\n1. Cosparsity: In Section 2 we start our discussion with a closer look at the\nsparse analysis model in order to better de\ufb01ne it as a generative model\nfor signals. We show that, while the synthesis model puts an emphasis on\nthe non-zeros of the representation vector z, the analysis model draws its\nstrength from the zeros in the analysis vector \u2126x.\n2. Union of Subspaces: Section 2 is also devoted to a comparison between\nthe synthesis model and the analysis one. We know that the synthesis\nmodel described above is an instance of a wider family of models, built\nas a \ufb01nite union of subspaces [29].\nBy choosing all the sub-groups of\ncolumns from D that could be combined linearly to generate signals, we\nget an exponentially large family of low-dimensional subspaces that cover\nthe signals of interest. Adopting this perspective, the analysis model can\nobtain a similar interpretation. How are the two related to each other?\nSection 2 considers this question and proposes a few answers.\n3. Uniqueness:\nWe know that the spark of the dictionary governs the\nuniqueness properties of sparse solutions of the underdetermined linear\nsystem Dz = x [16]. Can we derive a similar relation for the analysis\ncase? As a platform for studying the analysis uniqueness properties, we\nconsider an inverse problem of the form y = Mx, where M \u2208Rm\u00d7d and\nm < d, and y \u2208Rm is a measurement vector. Put roughly (and this will\nbe better de\ufb01ned later on), assuming that x comes from the sparse anal-\nysis model, could we claim that there is only one possible solution x that\ncan explain the measurement vector y? Section 3 presents this uniqueness\nstudy.\n4. Pursuit Algorithms: Armed with a deeper understanding of the anal-\nysis model, we may ask how to e\ufb03ciently \ufb01nd x for the above-described\nlinear inverse problem. As in the synthesis case, we can consider either\nrelaxation-based methods or greedy ones. In Section 4 we present two\nnumerical approximation algorithms: a greedy algorithm termed \u201cGreedy\nAnalysis Pursuit\u201d (GAP) that resembles the Orthogonal Matching Pur-\nsuit (OMP) [32] \u2013 adapted to the analysis model \u2013, and the previously\nconsidered \u21131-minimization approach [19, 37, 9]. Section 5 accompanies\nthe presentation of GAP with a theoretical study of its performance guar-\nantee, deriving a condition that resembles the ERC obtained for OMP\n3\n[41]. Similarly, we study the terms of success of the \u21131-minimization ap-\nproach for the analysis model, deriving a condition that is similar to the\none obtained for the synthesis sparse model [41].\n5. Tests: In Section 6 we demonstrate the e\ufb00ectiveness of the analysis model\nand the pursuit algorithms proposed in several experiments, starting from\nsynthetic ones and going all the way to a compressed-sensing test for an\nimage based on the analysis model: the Shepp Logan phantom.\nWe believe that with the above set of contributions, the cosparse analysis model\nbecomes a well-de\ufb01ned and competitive model to the synthesis counterpart,\nequipped with all the necessary ingredients for its practical use. Furthermore,\nthis work leads to a series of new questions that are parallel to those studied for\nthe synthesis model \u2013 developing novel pursuit methods, a theoretical study of\npursuit algorithms for handling other inverse problems, training \u2126just as done\nfor D, and more. We discuss these and other topics in Section 7.\nRelated Work. Several works exist in the literature that are related to the anal-\nysis model. The work by Elad et. al. [19] was the \ufb01rst to observe the dichotomy\nof analysis and synthesis models for signals. Their study, done in the context of\nthe Maximum-A-Posteriori Probability estimation, presented the two alterna-\ntives and explored cases of equivalence between the two. They demonstrated a\nsuperiority of the analysis-based approach in signal denoising. Further empirical\nevidence of the e\ufb00ectiveness of the analysis-based approach can be found in [35]\nand [37] for signal and image restoration. In [37] it was noted that the nonzero\ncoe\ufb03cients play a di\ufb00erent role in the analysis and synthesis forms but the im-\nportance of the zero coe\ufb03cients for the analysis model \u2013 which is reminiscent of\nsignal characterizations through the zero-crossings of their undecimated wavelet\ntransform [31] \u2013 was not explicitly identi\ufb01ed.\nMore recently, Cand`es et al. [9] provided a theoretical study on the error\nwhen the analysis-based \u21131-minimization is used in the context of compressed\nsensing. Our work is closely related to these contributions in various ways, and\nwe shall return to these papers when diving into the details of our study.\n2. A Closer Look at the Cosparse Analysis Model\nWe start our discussion with the introduction of the sparse analysis model,\nand the notion of cosparsity that is fundamental for its de\ufb01nition.\nWe also\ndescribe how to interpret the analysis model as a generative one (just like the\nsynthesis counterpart).\nFinally, we consider the interpretation of the sparse\nanalysis and synthesis models as two manifestations of union-of-subspaces mod-\nels, and show how they are related.\n2.1. Introducing Cosparsity\nAs described in the introduction, a conceptually simple model for data\nwould be to assume that each signal we consider can be expressed (i.e., well-\napproximated) as a combination of a few building atoms. Once we take this\n4\nview, a simple synthesis model can be thought of: First, there is a collection\nof the atomic signals {dj}n\nj=1 \u2208Rd that we concatenate as the columns of a\ndictionary, denoted by D \u2208Rd\u00d7n. Here, typically n \u2265d, implying that the\ndictionary is redundant. Second, the signal x \u2208Rd can be expressed as a linear\ncombination of some atoms of D, thus there exists z \u2208Rn such that x = Dz.\nThird and most importantly, x must lie in a low dimensional subspace, and in\norder to ensure this, very few atoms are used in the expression x = Dz, i.e., the\nnumber of non-zeros \u2225z\u22250 is very small. By the observation that \u2225z\u22250 is small,\nwe say that x has a sparse representation in D. The number k = \u2225z\u22250 is the\nsparsity of x.\nOften, the validity of the above described sparse synthesis model is demon-\nstrated by applying a linear transform to a class of signals to be processed and\nobserving that most of the coe\ufb03cients are close to zero, exhibiting sparsity.\nIn signal and image processing, discrete transforms such as wavelet, Gabor,\ncurvelet, contourlet, shearlet, and others [33, 39, 13, 27], are of interest, and\nthis empirical observation seems to give a good support for the sparse syn-\nthesis model. Indeed, when aiming to claim optimality of a given transform,\nthis is exactly the approach taken \u2013 show that for a (theoretically-modeled)\nclass of signals of interest, the transform coe\ufb03cients tend to exhibit a strong\ndecay. However, one cannot help but noticing that this approach of validat-\ning the synthesis model seems to actually validate another \u2018similar\u2019 model; we\nare considering a model where the signals of interest have sparse analysis rep-\nresentations. This point is especially pronounced when the transform used is\nover-complete or redundant.\nLet us now look more carefully at the above mentioned model that seems\nto be similar to the sparse synthesis one.\nFirst, let \u2126\u2208Rp\u00d7d be a signal\ntransformation or an analysis operator. Its rows are the row vectors {\u03c9j}p\nj=1\nthat will be applied to the signals. Applying \u2126to x, we obtain the (analysis)\nrepresentation \u2126x of x. To capture various aspects of the information in x, we\ntypically have p \u2265d.\nFor simplicity, unless stated otherwise, we shall assume hereafter that all the\nrows of \u2126are in general position, i.e., there are no non-trivial linear dependen-\ncies among the rows.2\nClearly, unless x = 0, no representation \u2126x can be \u2018very sparse\u2019, since at\nleast p \u2212d of the coe\ufb03cients of \u2126x are necessarily non-zeros. We shall put our\nemphasis on the number of zeros in the representation, a quantity we will call\ncosparsity.\nDe\ufb01nition 1. The cosparsity of a signal x \u2208Rd with respect to \u2126\u2208Rp\u00d7d (or\nsimply the cosparsity of x) is de\ufb01ned to be:\nCosparsity :\n\u2113:= p \u2212\u2225\u2126x\u22250\n(1)\n2Put di\ufb00erently, we assume that the spark of the matrix \u2126T is full, implying that every\nset of d rows from \u2126are linearly independent.\n5\nThe index set of the zero entries of \u2126x is called the cosupport of x. We say that\nx has cosparse representation or x is cosparse when the cosparsity of x is large,\nwhere by large we mean that \u2113is close to d. We will see that, while \u2113\u2264d for an\nanalysis operator in general position, there are speci\ufb01c examples where \u2113may\nexceed d.\nAt \ufb01rst sight the replacement of sparsity by cosparsity might appear to be\nmere semantics. However we will see that this is not the case. In the synthesis\nmodel it is the columns dj, j \u2208T associated with the index set T of nonzero\ncoe\ufb03cients that de\ufb01ne the signal subspace. Removing columns from D not in\nT leaves this subspace unchanged. In contrast, it is the rows \u03c9j associated with\nthe index set \u039b such that \u27e8\u03c9j, x\u27e9= 0, j \u2208\u039b that de\ufb01ne the analysis subspace.\nIn this case removing rows from \u2126for which \u27e8\u03c9j, x\u27e9\u0338= 0 leaves the subspace\nunchanged.\nFrom this perspective, the cosparse model is rather related to signal charac-\nterizations from the zero-crossings of their undecimated wavelet transform [31]\nthan to sparse wavelet expansions.\n2.2. Sparse Analysis Model as a Generative Model\nIn a Bayesian context, one can think of data models as generators for random\nsignals from a pre-speci\ufb01ed probability density function. In that context, the\nsignals that satisfy the k-sparse synthesis model can be generated as follows:\nFirst, choose k columns of the dictionary D at random (e.g. assuming a uniform\nprobability). We denote the index set chosen by T , and clearly |T | = k. Second,\nform a coe\ufb03cient vector z that is k-sparse, with zeros outside the support T .\nThe k non-zeros in z can be chosen at random as well (e.g. Gaussian iid entries).\nFinally, the signal is created by multiplying D to the resulting sparse coe\ufb03cient\nvector z.\nCould we adopt a similar view for the cosparse analysis model? The answer\nis positive. Similar to the above, one can produce an \u2113-cosparse signal in the\nfollowing way: First, choose \u2113rows of the analysis operator \u2126at random, and\nthose are denoted by an index set \u039b (thus, |\u039b| = \u2113). Second, form an arbitrary\nsignal v in Rd \u2013 e.g., a random vector with Gaussian iid entries. Then, project\nv to the orthogonal complement of the subspace generated by the rows of \u2126\nthat are indexed by \u039b, this way getting the cosparse signal x. Alternatively,\none could \ufb01rst \ufb01nd a basis for the orthogonal complement and then generate a\nrandom coe\ufb03cient vector for the basis.\nThis way, both models can be considered as generators of signals that have\na special structure, and clearly, the two signal generators are di\ufb00erent. It is now\ntime to ask how those two families of signals inter-relate. In order to answer\nthis question, we take the union-of-subspaces point of view.\n2.3. Union-of-Subspaces Models\nIt is well known that the sparse synthesis model is a special instance of a\nwider family of models called union-of-subspaces [29, 4]. Given a dictionary D,\na vector z that is exactly k-sparse with support T leads to a signal x = Dz =\n6\nDT zT , a linear combination of k columns from D. The notation DT denotes\nthe sub-matrix of D containing only the columns indexed by T .\nDenoting\nthe subspace spanned by these columns by VT := span(dj, j \u2208T ), the sparse\nsynthesis signals belong to the union of all\n\u0000n\nk\n\u0001\npossible subspaces of dimension\nk,\nSparse Synthesis Model:\nx \u2208\u222aT :|T |=k VT .\n(2)\nSimilarly, the analysis model is associated to a union of subspaces model\nas well. Given an analysis operator \u2126, a signal that is exactly \u2113-cosparse with\nrespect to the rows \u039b from \u2126is simply in the orthogonal complement to these\n\u2113rows. Thus, we have3 \u2126\u039bx = 0, which implies that x \u2208W\u039b, where W\u039b :=\nspan(\u03c9j, j \u2208\u039b)\u22a5= {x, \u27e8\u03c9j, x\u27e9= 0, \u2200j \u2208\u039b}.\nPut di\ufb00erently, we may write\nW\u039b = Range(\u2126T\n\u039b)\u22a5= Null(\u2126\u039b). Hence, cosparse analysis signals x belong to\nthe union of all the\n\u0000p\n\u2113\n\u0001\npossible such subspaces of dimension d \u2212\u2113,\nCosparse Analysis Model:\nx \u2208\u222a\u039b:|\u039b|=\u2113W\u039b.\n(3)\nThe following table summarizes these two unions of subspaces, where we recall\nthat we consider \u2126and D in general position.\nModel\nSubspaces\nNo. of Subspaces\nSubspace dimension\nSynthesis\nVT := span(dj, j \u2208T )\n\u0000n\nk\n\u0001\nk\nAnalysis\nW\u039b := span(\u03c9j, j \u2208\u039b)\u22a5\n\u0000p\n\u2113\n\u0001\nd \u2212\u2113\nWhat is the relation between these two union of subspaces, as described in\nEquations (2) and (3)? In general, the answer is that the two are di\ufb00erent. An\ninteresting way to compare between the two models is to consider an \u2113-cosparse\nanalysis model and a corresponding (d \u2212\u2113)-sparse synthesis model, so that the\ntwo have the same dimension in their subspaces.\nFollowing this guideline, we consider \ufb01rst a special case where \u2113= d \u22121. In\nsuch a case, the dimension of the analysis subspaces is d \u2212\u2113= 1, and there are\n\u0000p\n\u2113\n\u0001\nof those. An equivalent synthesis union of subspaces can be created, where\nk = 1. We should construct a dictionary D with n =\n\u0000p\n\u2113\n\u0001\natoms dj, where\neach atom is the orthogonal complement to one of the sets of \u2113rows from \u2126.\nWhile the two models become equivalent in this case, clearly n \u226bp in general,\nimplying that the sparse synthesis model becomes untractable since D becomes\ntoo large.\nBy further assuming that p = d, we get that there are exactly\n\u0000p\n\u2113\n\u0001\n=\n\u0000 d\nd\u22121\n\u0001\n=\nd subspaces in the analysis union, and in this case n = p = d as well. Further-\nmore, it is not hard to see that in this case the synthesis atoms are obtained\ndirectly by a simple inversion, D = \u2126\u22121.\n3Note that the notation \u2126\u039b refers to restricting rows from \u2126indexed by \u039b, whereas in\nthe synthesis case we have taken the columns. We shall use this convention throughout this\npaper, where from the context it should be clear whether rows or columns are extracted.\n7\nAdopting a similar approach, considering the general case where \u2113is a general\nvalue (and not necessarily d \u22121), one could always construct a synthesis model\nthat is equivalent to the analysis one. We can compose the synthesis dictionary\nby simply concatenating all the bases for the orthogonal complements to the\nsubspaces W\u039b. The obtained dictionary will have at most (d \u2212\u2113)\n\u0000p\n\u2113\n\u0001\natoms.\nHowever, not all supports of size k are allowed in the obtained synthesis model,\nsince otherwise the new sparse synthesis model will strictly contain the cosparse\nanalysis one. As such, the cosparse analysis model may be viewed as a sparse\nsynthesis model with some structure.\nFurther on the comparison between the two models, it would be of bene\ufb01t to\nconsider again the case d \u2212\u2113= k (i.e., having the same dimensionality), assume\nthat p = n (i.e., having the same overcompleteness, for example with \u2126= DT ),\nand compare the number of subspaces amalgamated in each model. For the\nsake of simplicity we consider a mild overcompleteness of p = n = 2d. Denoting\nH(t) := \u2212t log2 t \u2212(1 \u2212t) log2(1 \u2212t), 0 < t < 1, the number of subspaces of\nlow dimension k \u226ad = n/2 in each data model, from Stirling\u2019s approximation,\nroughly satis\ufb01es for large d:\nSynthesis:\nlog2\n\u0012n\nk\n\u0013\n\u2248n \u00b7 H\n\u0012k\nn\n\u0013\n\u2248k \u00b7 log2\nn\nk\nAnalysis:\nlog2\n\u0012p\n\u2113\n\u0013\n\u2248n \u00b7 H\n\u0012d \u2212k\nn\n\u0013\n\u2248n \u00b7 H(0.5) = n.\nMore generally, unless d/n \u22481, there are much fewer low-dimensional synthesis\nsubspaces than the number of analysis subspaces of the same dimension. This\nis illustrated on Figure 2.3 when n = p = 2d. This indicates a strong di\ufb00erence\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\n0\n0.2\n0.4\n0.6\n0.8\n1\nk/d\nlog2(\u266fsubpaces)/d\nNumber of subspaces of dimension k in Rd\n \n \nSynthesis model, n/d=2\nAnalysis model, p/d =2 l=d\u2212k\nFigure 1: Number of subspaces of a given dimension, for n = p = 2d. The solid blue curve\nshows the log number of subspaces for the synthesis model as the dimension of subspaces vary,\nwhile the dashed red curve shows that for the analysis model.\nin the structure of the two models: The synthesis model includes very few\nlow-dimensional subspaces, and an increasingly large number of subspaces of\nhigher dimension; and the analysis model contains a combinatorial number of\nlow-dimensional subspaces, with fewer high dimensional subspaces.\n8\nComment: One must keep in mind that the huge number of low-dimensional\nsubspaces, though rich in terms of its descriptive power, makes it very di\ufb03cult to\nrecover algorithmically signals that belong to the union of those low-dimensional\nsubspaces or to e\ufb03ciently code/sample those signals (see the experimental re-\nsults in Section 6.1). This stems from the fact that in general, it is not possible\nto get cosparsity d \u2264\u2113< p: any vector x that is orthogonal to d linearly inde-\npendent rows of \u2126must be the zero vector, leading to an uninformative model.\nOne may, however, get cosparsities in the range d \u2264\u2113< p when the analysis\noperator \u2126displays certain linear dependencies. Therefore it appears to be de-\nsirable, in the cosparse analysis model, to have analysis operators that exhibit\nhighly linearly dependent structure. We will see in Section 3.4 that a leading\nexample of such operators is the \ufb01nite di\ufb00erence analysis operator.\nAnother interesting point of view towards the di\ufb00erence between the two\nmodels is the following: While a synthesis signal is characterized by the support\nof the non-zeros in its representation in order to de\ufb01ne the subspace it belong\nto, a signal from the analysis model is characterized by the locations of the zeros\nin its representation \u2126x. The fact that this representation may contain many\nnon-zeroes (and especially so when p \u226bd) should be of no consequence to the\ne\ufb03ciency of the analysis model.\n2.4. Comparison with the Traditional Sparse Analysis model\nPrevious work using analysis representations, both theoretical and algorith-\nmic, has focussed on gauging performance in terms of the more traditional\nsparsity perspective. For example, in the context of compressed sensing, recent\ntheoretical work [9] has provided performance guarantees for minimum \u21131-norm\nanalysis representations in this light.\nThe analysis operator is generally viewed as the dual frame for a redundant\nsynthesis dictionary so that \u2126= D\u2020. This means that the analysis coe\ufb03cients\n\u2126x provide a consistent synthesis representation for x in terms of the dictionary\nD, implying that the representation \u2126x is a feasible solution to the linear system\nof equations Dz = x.\nFurthermore, if \u2225\u2126x\u22250 = p \u2212\u2113, then \u2126x must be an element of the k-sparse\nsynthesis model, S\nT :|T |=k VT , with k = p \u2212\u2113. Hence:\n{0} \u2286\n[\n\u039b:|\u039b|=p\u2212k\nW\u039b \u2286\n[\nT :|T |=k\nVT \u2286Rd.\n(4)\nOf course, \u2126x is not guaranteed to be the sparsest representation of x in terms\nof D. Hence the two subspace models are not equivalent.\nNote that while in Section 2.3 the sparsity k was matched to d \u2212\u2113, here it\nis matched to p \u2212\u2113. The former was used to get the same dimensions in the\nresulting subspaces, while the match discussed here considers the vector \u2126x as\na candidate k-sparse representation.\nSuch a perspective treats the analysis operator as a poor man\u2019s sparse syn-\nthesis representation. That is, for certain signals x, the representation \u2126x may\n9\nbe reasonably sparse but is unlikely to be as sparse as, for example, the minimum\n\u21131-norm synthesis representation4.\nIn the context of linear inverse problems, it is tempting to try to exploit\nthe nesting property (4) in order to derive identi\ufb01ability guarantees in terms of\nthe sparsity of the analysis coe\ufb03cients \u2126x. For example, in [9], the compressed\nsensing recovery guarantees exploit the nesting property (4) by assuming a su\ufb03-\ncient number of observations to achieve a stable embedding (restricted isometry\nproperty) for the k-sparse synthesis union of subspaces, which in turn implies a\nstable embedding of the (p \u2212k)-cosparse analysis union of subspaces.\nWhile such an approach is of course valid, it misses a crucial di\ufb00erence\nbetween the analysis and synthesis representations: they do not correspond to\nequivalent signal models. Treating the two models as equivalent hides the fact\nthat they may be composed of subspaces with markedly di\ufb00erent dimensions.\nThe di\ufb00erence between these models is highlighted in the following examples.\n2.4.1. Example: generic analysis operators, p = 2d\nAssuming the rows of \u2126are in general position, then when p \u22652d the\nnesting property (4) is trivial but rather useless! Indeed, if k < d, then the only\nanalysis signal for which \u2225\u2126x\u22250 = k = p \u2212\u2113is x = 0. Alternatively, if k \u2265d,\nthe synthesis model is trivially the full space: S\nT :|T |=k VT = Rd.\n2.4.2. Example: shift invariant wavelet transform\nThe shift invariant wavelet transform is a popular analysis transform in\nsignal processing. It is particularly good for processing piecewise smooth signals.\nIts inverse transform has a synthesis interpretation as the redundant wavelet\ndictionary consisting of wavelet atoms with all possible shifts.\nThe shift invariant wavelet transform [33] provides a nice example of an\nanalysis operator that has signi\ufb01cant dependencies due to the \ufb01nite support of\nthe individual wavelets. Such nontrivial dependencies within the rows of \u2126WT\nmean that the dimensions of the (analysis or synthesis) signal subspaces are not\neasily characterised by either the sparsity k or the cosparsity \u2113. However the\nbehaviour of the model is still driven by the zero coe\ufb03cients not the nonzero\nones, i.e., by the zero-crossings of the wavelet transform [31]. By considering\na particular support set of an analysis representation \u2126WTx with the shift\ninvariant wavelet transform we can illustrate the dramatic di\ufb00erence between\nthe analysis and synthesis interpretations of the coe\ufb03cients.\nFigure 2 shows the support set of the nonzero analysis coe\ufb03cients, associated\nwith the cone of in\ufb02uence around a discontinuity in a piecewise polynomial signal\nof length 128-samples [17], using a shift-invariant Daubechies wavelet transform\nwith s = 3 vanishing moments [33]. For such a signal, the cone of in\ufb02uence at\n4When measuring sparsity with an \u2113p norm, 0 < p \u22641, rather than with p = 0, it has been\nshown [26] that for so-called localized frames the analysis coe\ufb03cients \u2126x obtained with \u2126=\nD\u2020 the canonical dual frame of D are near optimally sparse: \u2225\u2126x\u2225p \u2264Cp minz|Dz=x \u2225z\u2225p,\nwhere the constant Cp does not depend on x.\n10\nlevel J in a shift invariant wavelet transform contains Lj \u22121 nonzero coe\ufb03cients\nwhere Lj is the length of the wavelet \ufb01lter at level j. Note though, the nonzero\ncoe\ufb03cients are not linearly independent and can be elegantly described through\nthe notion of wavelet footprints [17].\nSynthesis perspective. Interpreting the support set within the synthesis\nmodel implies that the signal is not particularly sparse and needs a signi\ufb01cant\nnumber of wavelet atoms to describe it: in Figure 2 the size of the support\nset, excluding coe\ufb03cients of scaling functions, is 122. Could the support set\nbe signi\ufb01cantly reduced by using a better support selection strategy such as \u21131\nminimization? In practice, using \u21131 minimization, a support set of 30 can be\nobtained, again ignoring scaling coe\ufb03cients.\nAnalysis perspective. The analysis interpretation of the shift invariant\nwavelet representation relies on the examination of the size of the analysis sub-\nspace associated with the cosupport set. From the theory of wavelet footprints,\nthe dimension of this subspace is equal to the number of vanishing moments of\nthe wavelet \ufb01lter, which in this example is only . . . 3, providing a much lower\ndimensional signal model.\nWe therefore see that the analysis model has a much lower number of degrees\nof freedom for this support set, leading to a signi\ufb01cantly more parsimonious\nmodel.\n20\n40\n60\n80\n100\n120\n1\n2\n3\n4\nFigure 2: The support set for the wavelet coe\ufb03cients of a piecewise quadratic signal using\na J = 4 level shift invariant Daubechies wavelet transform with s = 3 vanishing moments.\nScaling coe\ufb03cients are not shown. The support set contains 122 coe\ufb03cients out of a possible\n512, yet the analysis subspace has a dimension of only 3.\n2.5. Hybrid Analysis/Synthesis models?\nIn this section we have demonstrated that while both the cosparse analysis\nmodel and the sparse synthesis model can be described by a union of subspaces\nthese models are typically very di\ufb00erent. We do not argue that one is inevitably\nbetter than the other. The value of the model will very much depend on the\nproblem instance. Indeed the intrinsic di\ufb00erence between the models also sug-\ngests that it might be fruitful to explore building other union of subspace models\nfrom hybrid compositions of analysis and synthesis operators. For example, one\ncould imagine a signal model where x = Dz through a redundant synthesis\n11\ndictionary but instead of imposing sparsity on z we restrict z through an ad-\nditional analysis operator: \u2225\u2126z\u22250 \u2264k. In such a case there will still be an\nunderlying union of subspace model but with the subspaces de\ufb01ned by a combi-\nnation of atoms and analysis operator constraints. A special case of this is the\nsplit analysis model suggested in [9].\n3. Uniqueness Properties\nIn the synthesis model, if a dictionary D is redundant, then a given signal x\ncan admit many synthesis representations \u02dcz, i.e., \u02dcz with D\u02dcz = x. This makes the\nfollowing type of problem interesting in the context of the sparse signal recovery:\nWhen a signal has a sparse representation z, can there be another representation\nthat is equally sparse or sparser? This problem is well-understood in terms of\nthe so-called spark of D [16], the smallest number of columns from D that are\nlinearly dependent.\nUnlike in the synthesis model, if the signal is known, then its analysis repre-\nsentation \u2126x with respect to an analysis operator \u2126is completely determined.\nHence, there is no inherent question of uniqueness for the cosparse analysis\nmodel. The uniqueness question we want to consider in this paper is in the\ncontext of the noiseless linear inverse problem,\ny = Mx,\n(5)\nwhere M \u2208Rm\u00d7d, and m < d, implying that the measurement vector y \u2208Rm is\nnot su\ufb03cient to fully characterize the original signal x \u2208Rd. For this problem we\nask: when can we assert that a solution x with cosparsity \u2113is the only solution\nwith that cosparsity or more? The problem (5) (especially, with additive noise)\narises ubiquitously in many applications, and we shall focus on this problem\nthroughout this paper as a platform for introducing the cosparse analysis model,\nits properties and behavior. Not to complicate matters unnecessarily, we assume\nthat all the rows of M are linearly independent, and we omit noise, leaving\nrobustness analysis to further work.\nFor completeness of our discussion, let us return for a moment to the synthe-\nsis model and consider the uniqueness property for the inverse problem posed in\nEquation (5). Assuming that the signal\u2019s sparse representation satis\ufb01es x = Dz,\nwe have that y = Mx = MDz. Had we known the support T of z, this linear\nsystem would have reduced to y = MDT zT , a system of m equations with k\nunknowns. Thus, recovery of x from y is possible only if k \u2264m.\nWhen the support of z is unknown, it is the spark of the compound matrix\nMD that governs whether the cardinality of zT is su\ufb03cient to ensure uniqueness\n\u2013 if k = \u2225z\u22250 is smaller than half the spark of MD, then necessarily z is the\nsignal\u2019s sparsest representation. At best, spark(MD) = m + 1, and then we\nrequire that the number of measurements is at least twice the cardinality k. Put\nformally, we require\nk = \u2225z\u22250 < 1\n2spark(MD) \u2264m + 1\n2\n.\n(6)\n12\nIt will be interesting to contrast this requirement with the one we will derive\nhereafter for the analysis model.\n3.1. Uniqueness When the Cosupport is Known\nBefore we tackle the uniqueness problem for the analysis model, let us con-\nsider an easier question: Given the observations y obtained via a measurement\nmatrix M, and assuming that the cosupport \u039b of the signal x is known, what\nare the su\ufb03cient conditions for the recovery of x? The answer to this question\nis straightforward since x satis\ufb01es the linear equation\n\u0014\ny\n0\n\u0015\n=\n\u0014\nM\n\u2126\u039b\n\u0015\nx = Ax.\n(7)\nTo be able to uniquely identify x from Equation (7), the matrix A must have a\nzero null space. This is equivalent to the requirement\nNull(\u2126\u039b) \u2229Null(M) = W\u039b \u2229Null(M) = {0}.\n(8)\nLet us now assume that M and \u2126are mutually independent, in the sense\nthat there are no nontrivial linear dependencies among the rows of M and\n\u2126; this is a reasonable assumption because \ufb01rst, one should not be measuring\nsomething that may be already available from \u2126, and second, for a \ufb01xed \u2126,\nmutual independency holds true for almost all M (in the Lebesgue measure).\nThen, (8) would be satis\ufb01ed as soon as dim(W\u039b) + dim(Null(M)) \u2264d, or\ndim(W\u039b) \u2264m, since dim(Null(M)) = d \u2212m. This motivates us to de\ufb01ne\n\u03ba\u2126(\u2113) := max\n|\u039b|\u2265\u2113\ndim(W\u039b).\n(9)\nThe quantity \u03ba\u2126(\u2113) plays an important role in determining the necessary\nand su\ufb03cient cosparsity level for the identi\ufb01cation of cosparse signals. Indeed,\nunder the assumption of the mutual independence of \u2126and M, a necessary\nand su\ufb03cient condition for the uniqueness of every cosparse signal given the\nknowledge of its cosupport \u039b of size \u2113is\n\u03ba\u2126(\u2113) \u2264m.\n(10)\n3.2. Uniqueness When the Cosupport is Unknown\nThe uniqueness question that we answered above refers to the case where\nthe cosupport is known, but of course, in general this is not the case.\nWe\nshall assume that we may only know the cosparsity level \u2113, which means that\nour uniqueness question now becomes: what cosparsity level \u2113guarantees that\nthere can be only one signal x matching a given observation y?\nAs we have seen, the cosparse analysis model is a special case of a general\nunion of subspaces model. Uniqueness guarantees for missing data problems\nsuch as (5) with general union of subspace models are covered in [29, 4]. In\nparticular [29] shows that M is invertible on the union of subspaces \u222a\u03b3\u2208\u0393S\u03b3 if\nand only if M is invertible on all subspaces S\u03b3 + S\u03b8 for all \u03b3, \u03b8 \u2208\u0393. In the\ncontext of the analysis model this gives the following result whose proof is a\ndirect consequence of the results in [29]:\n13\nProposition 2 ([29]). Let \u222a\u039bW\u039b, |\u039b| = \u2113be the union of \u2113-cosparse analysis\nsubspaces induced by the analysis operator \u2126. Then the following statements\nare equivalent:\n1. If the linear system y = Mx admits an \u2113-cosparse solution, then this is\nthe unique \u2113-cosparse solution;\n2. M is invertible on \u222a\u039bW\u039b;\n3. (W\u039b1 + W\u039b2) \u2229Null(M) = 0 for any |\u039b1|, |\u039b2| \u2265\u2113;\nProposition 2 answers the question of uniqueness for cosparse signals in the\ncontext of linear inverse problems. Unfortunately, the answer we obtained still\nleaves us in the dark in terms of the necessary cosparsity level or necessary\nnumber of measurements. In order to pose a clearer condition, we use Propo-\nsition 2 from [29] that poses a sharp condition on the number of measurements\nto guarantee uniqueness (when M and \u2126are mutually independent):\nm \u2265\u02dc\u03ba\u2126(\u2113),\nwhere \u02dc\u03ba\u2126(\u2113) := max {dim(W\u039b1 + W\u039b2) : |\u039bi| \u2265\u2113, i = 1, 2}\n(11)\nInterestingly, a su\ufb03cient condition can also be obtained using the quantity \u03ba\u2126\nde\ufb01ned in (9) above, which was observed to play an important role in the unique-\nness result when the cosupport is assumed to be known. Namely, we have the\nfollowing result.\nProposition 3. Assume that \u03ba\u2126(\u2113) \u2264\nm\n2 . Then for almost all M (wrt the\nLebesgue measure), the linear inverse problem y = Mx has at most one \u2113-\ncosparse solution.\nProof. Assuming the mutual independence of \u2126and M, which holds for almost\nall M, we note that the uniqueness of \u2113cosparse solutions holds if and only\nif: dim (W\u039b1 + W\u039b2) \u2264m, whenever |\u039bi| \u2265\u2113, i = 1, 2. Assume that \u03ba\u2126(\u2113) \u2264\nm/2. By de\ufb01nition of \u03ba\u2126, if |\u039bi| \u2265\u2113, i = 1, 2, then dim(W\u039bi) \u2264\nm\n2 , hence\ndim (W\u039b1 + W\u039b2) \u2264m.\nIn the synthesis model the degree to which columns are interdependent can\nbe partially characterized by the spark of D [16] de\ufb01ned as the the smallest\nnumber of columns of D that are linearly dependent. Here the function \u03ba\u2126\nplays a similar role in quantifying the interdependence between rows in the\nanalysis model.\nRemark 4. The condition \u03ba\u2126(\u2113) \u2264m\n2 is in general not necessary while condi-\ntion (11) is.\nThere are two classes of analysis operators for which the function \u03ba\u2126is\nwell-understood: analysis operators in general position and the \ufb01nite di\ufb00erence\noperators. We discuss the uniqueness results for these two classes in the follow-\ning subsections.\n14\n3.3. Analysis Operators in General Position\nIt can be easily checked that \u03ba\u2126(\u2113) = max(d \u2212\u2113, 0). This enables us to\nquantify the exact level of cosparsity necessary for the uniqueness guarantees:\nCorollary 5. Let \u2126\u2208Rp\u00d7d be an analysis operator in general position. Then,\nfor almost all m \u00d7 d matrix M, the following hold:\n\u2022 Based on Eq. (10), if m \u2265d \u2212\u2113, then the equation y = Mx has at most\none solution with known cosupport \u039b (of cosparsity at least \u2113);\n\u2022 Based on Proposition 2, if m \u22652(d \u2212\u2113), then the equation y = Mx has\nat most one solution with cosparsity at least \u2113.\n3.4. The Finite Di\ufb00erence Operator\nAn interesting class of analysis operators with signi\ufb01cant linear dependencies\nis the family of \ufb01nite di\ufb00erence operators on graphs, \u2126DIF. These are strongly\nrelated to TV norm minimization, popular in image processing applications [36],\nand has the added bene\ufb01t that we are able to quantify the function \u03ba\u2126and hence\nthe uniqueness properties of the cosparse signal model under \u2126DIF.\nWe begin by considering \u2126DIF on an arbitrary graph before restricting our\ndiscussion to the 2D lattice associated with image pixels.\nConsider a non-\noriented graph with vertices V and edges E \u2282V 2. An edge e is a pair e = (v1, v2)\nof connected vertices.\nFor any vector of coe\ufb03cients de\ufb01ned on the vertices,\nx \u2208RV , the \ufb01nite di\ufb00erence analysis operator \u2126DIF computes the collection of\ndi\ufb00erences (x(v1)\u2212x(v2)) between end-points, for all edges in the graph. Thus,\nan edge e \u2208E may be viewed as a \ufb01nite di\ufb00erence on RV .\nCan we estimate the function \u03ba\u2126DIF(\u2113)?\nThe following shows that it is\nintimately related to topological properties of the graph. For each sub-collection\n\u039b \u2282E of edges, we can de\ufb01ne its vertex-set V (\u039b) \u2282V as the collection of\nvertices covered by at least one edge in \u039b. The support set V (\u039b) of \u039b can be\ndecomposed into J(\u039b) connected components (a connected component is a set\nof vertices connected to one another by a walk through vertices in \u039b). It is easy\nto check that a vector x belongs to the space W\u039b = Null(\u2126\u039b) if and only if its\nvalues are constant on each connected component. As a result, the dimension\nof this subspace is given by\ndim(W\u039b) = |V | \u2212|V (\u039b)| + J(\u039b)\nwhere the |V | \u2212|V (\u039b)| vertices out of V are associated to arbitrary values in\nx that are distinct from all their neighbors, while all entries from each of the\nJ(\u039b) connected components have an arbitrary common value. It follows that\n\u03ba\u2126(\u2113) = max\n|\u039b|\u2265\u2113\nn\n|V | \u2212|V (\u039b)| + J(\u039b)\no\n= |V | \u2212min\n|\u039b|\u2265\u2113\nn\n|V (\u039b)| \u2212J(\u039b)\no\n(12)\nBecause of the nesting of the subspaces W\u039b, the minimum on the right hand\nside is achieved when |\u039b| = \u2113.\n15\nUniqueness Condition for Cosparse Images with respect to the 2D \u2126DIF. In the\nabstract context of general graph the characterization (12) may remain obscure,\nbut can we get more concrete estimates by specializing to the 2D regular graph\nassociated to the pixels of an N \u00d7 N image? It turns out that one can obtain\nrelatively simple upper and lower bounds for \u03ba\u2126DIF and hence derive an easily\ninterpretable uniqueness condition (see Appendix C for a proof):\nProposition 6. Let \u2126DIF be the \ufb01nite di\ufb00erence analysis operator that com-\nputes horizontal and vertical discrete derivatives of a d = N \u00d7 N image. For\nany \u2113we have\nd \u2212\u2113\n2 \u2212\nr\n\u2113\n2 \u22121 \u2264\u03ba\u2126DIF(\u2113) \u2264d \u2212\u2113\n2.\n(13)\nAs a result, assuming that M is \u2019mutually independent\u2019 from \u2126DIF, we have:\n\u2022 Based on Eq. (10), if m \u2265d \u2212\u2113/2, that is to say\n\u2113\u22652d \u22122m,\n(14)\nthen the equation y = Mx has at most one solution with known cosupport\n\u039b (of cosparsity at least \u2113);\n\u2022 Based on Proposition 2, if m \u22652(d \u2212\u2113/2) = 2d \u2212\u2113, that is to say\n\u2113\u22652d \u2212m,\n(15)\nthen the equation y = Mx has at most one solution with cosparsity at\nleast \u2113.\nNote that as soon as the matrix M is associated to an underdetermined linear\nsystem, i.e., when m < d, we need \u2113\u22652d \u2212m > d to exploit the uniqueness\nguarantee (15).\nThe 2D \u2126DIF, Piecewise Constant Images, and the TV norm. The 2D \ufb01nite\ndi\ufb00erence operator is closely related to the TV norm [36]: the discrete TV norm\nof x is essentially a mixed \u21132 \u2212\u21131 norm of \u2126DIFx. Just like its close cousin TV\nnorm minimization, the minimization of \u2225\u2126x\u22250 is particularly good at inducing\npiecewise constant images. We illustrate this through a worked example.\nConsider the popular Shepp Logan phantom image shown in left hand side\nof Figure 3. This particular image has 14 distinct connected regions of constant\nintensity. The number of non-zero coe\ufb03cients in the \ufb01nite di\ufb00erence representa-\ntion is determined by the total length (Manhattan distance) of the boundaries\nbetween these regions. For the Shepp Logan phantom this length is 2546 pixel\nwidths and thus the cosparsity is \u2113= 130560\u22122546 = 128014. Furthermore, as\nthere are no isolated pixels with any other intensity, all pixels belong to a con-\nstant intensity region so that |V (\u039b)| = |V | and the cosupport has an associated\nsubspace dimension of:\ndim(W\u039b) = (|V | \u2212|V (\u039b)|) + J(\u039b)\n= 14\n16\nFigure 3: An example of a piecewise constant image: the 256 \u00d7 256 Shepp Logan phantom\n(left); and an image with the same cosparsity, \u2113= 128014, but whose cosupport is associated\nwith an empirically maximum subspace dimension (right).\nIn order to determine when the Shepp Logan image is the unique solution\nto y = Mx with maximum cosparsity it is necessary to consider the maximum\nsubspace dimension of all possible support sets with the same cosparsity. This is\nthe quantity measured by \u03ba\u2126DIF(\u2113). The right hand image in Figure 3 shows an\nimage with equal copsparsity but whose support is associated with the highest\ndimensional subspace we could \ufb01nd: dim(W\u039b) = 1276. Comparing this to the\nbounds given in (13) of Proposition 6\n1270 \u2264\u03ba\u2126DIF(\u2113) \u22641524,\nsuggests that the lower bound is reasonably tight in this instance. Note, as\nexplained in Appendix C, this image has a single connected subgraph, \u039b, which\nis nearly square. The uniqueness result from Proposition 6 then tells us that\na su\ufb03cient number of measurements to uniquely determine the Shepp Logan\nimage is given by m = 2\u03ba\u2126DIF(128014) which is somewhere between 2552 (if our\nempirical estimate is accurate) and 3048 (worst case).\nWe will revisit this again in Subsection 6.2 where we investigate the empirical\nrecovery performance of some practical reconstruction algorithms.\n3.5. Overview of cosparse vs sparse models for inverse problems\nTo conclude this section, Figure 4 provides a schematic overview of analy-\nsis cosparse models vs synthesis sparse models in the context of linear inverse\nproblems such as compressed sensing. In the synthesis model, the signal x is\na projection (through the dictionary D) of a high-dimensional vector z living\nin the union of sparse coe\ufb03cient subspaces; in the analysis model, the signal\nlives in the pre-image by the analysis operator \u2126of the intersection between\nthe range of \u2126and this union of subspaces. For a given sparsity of z, this is\nusually a set of much smaller dimensionality.\n4. Pursuit algorithms\nHaving a theoretical foundation for the uniqueness of the problem\n\u02c6x = arg min\nx\n\u2225\u2126x\u22250 subject to Mx = y,\n(16)\n17\nFigure 4: A schematic overview of analysis cosparse vs synthesis sparse models in relation\nwith compressed sensing.\nwe turn now to the question of how to solve it: algorithms. In this section we\npresent two algorithms, both targeting the solution of problem (16). As in the\nuniqueness discussion, we assume that M \u2208Rm\u00d7d, where m < d. This implies\nthat the equation Mx = y has in\ufb01nitely many possible solutions, and the term\n\u2225\u2126x\u22250 introduces the analysis model to regularize the problem.\nThe \ufb01rst algorithm we present, the analysis \u21131-minimization, is well-known\nand widely used already in practice, see e.g. [20, 40].\nThe other algorithm\nwe discuss is a variant of well-known greedy pursuit algorithm used for the\nsynthesis model \u2013 the Orthogonal Matching Pursuit (OMP) algorithm. Similar\nto the synthesis case, our goal is to detect the informative support of \u2126x \u2013\nas discussed in Section 3.1, in the analysis case, this amounts to the locations\nof the zeros in the vector \u2126x, so as to introduce additional constraints to the\nunderdetermined system Mx = y. Note that for obtaining a solution, one needs\nto detect at least d \u2212m of these zeros, and thus if \u2113> d \u2212m, detection of the\ncomplete set of zeros is not mandatory. Of course, there can be many more\npossibilities to solve (16) or to \ufb01nd approximate solutions of it. We mention a\nfew works where some of such methods can be found: [35, 37, 6].\n4.1. The Analysis \u21131-minimization\nSolving (16) can be quite di\ufb03cult. In fact, the synthesis counterpart of (16)\nis known to be NP-hard in general. As is well-known, a very e\ufb00ective way to\nremedy this situation is to modify (16) and to solve:\n\u02c6x = arg min\nx\n\u2225\u2126x\u22251 subject to Mx = y.\n(17)\n18\nThe attractiveness of this approach comes from that (17) is a convex problem\nand hence admits computationally tractable algorithms to solve it, and that the\n\u21131-norm promotes high cosparsity in the solution \u02c6x. An algorithm that targets\nthe solution of (17) and its convergence analysis can be found in [6].\n4.2. The Greedy Analysis Pursuit Algorithm (GAP)\nThe algorithm we present in this section is named Greedy Analysis Pursuit\n(GAP). As mentioned at the beginning of the section and as the name suggests,\nthis algorithm aims to \ufb01nd the cosupports of cosparse signals in a greedy fashion.\nAn obvious way to \ufb01nd the cosupport of a cosparse signal would proceed\nas follows: First, obtain a reasonable estimate of the signal from the given\ninformation. Using the initial estimate, select a location as belonging to the\ncosupport. Having this estimated part of the cosupport, we can obtain a new\nestimate. One can now see that by alternating the two previous steps, we will\nhave identi\ufb01ed enough locations of the cosupport to get the \ufb01nal estimate.\nHowever, the GAP works in an opposite direction and aims to detect the\nelements outside the set \u039b, this way carving its way towards the detection of\nthe desired cosupport. Therefore, the cosupport \u02c6\u039b is initialized to be the whole\nset {1, 2, 3, . . . , p}, and through the iterations it is reduced towards a set of\nsize \u2113(or less, d \u2212m).\nLet us discuss the algorithm with some detail.\nFirst, the GAP uses the\nfollowing initial estimate:\n\u02c6x0 = arg min\nx \u2225\u2126x\u22252\n2\nsubject to\ny = Mx.\n(18)\nNot knowing the locations of the cosupport but knowing that many entries of\n\u2126x0 are zero, this is a reasonable \ufb01rst estimate of x0. Once we have \u02c6x0, we\ncan view \u2126\u02c6x0 as an estimate of \u2126x0. Hence, we \ufb01nd the location of the largest\nentries (in absolute value) of \u2126\u02c6x0 and regard them as not belonging to the\ncosupport. After this, we remove the corresponding rows from \u2126and work with\na reduced \u2126. A detailed description of the algorithm is given in Figure 5.\nSome readers may notice that the GAP has similar \ufb02avors to the FOCUSS\n[23] and the IRLS [12]. This is certainly true in the sense that the GAP solves\nconstrained least squares problems and adjusts weights as it iterates. However,\nthe weight adjustment in the GAP is more aggressive (removal of rows) and\nbinary in nature.\nStopping criterion / targeted sparsity. In GAP, we debate between using the\nfull \u2113zeros in the product \u2126x versus a minimal and su\ufb03cient set of d \u2212m\nzeros. In between these two values, and assuming that the proper elements of\n\u039b have been detected, we expect the solution obtained by the algorithms to be\nthe same, with a slightly better numerical stability for a larger number of zeros.\nThus, an alternative stopping criterion for the GAP could be to detect\nwhether the solution is static or the analysis coe\ufb03cients of the solution are\nsmall. This way, even if the GAP made an error and removed from \u02c6\u039bk an index\nthat belongs to the true cosupport \u039b, the tendency of the solution to stabilize\n19\n\u2022 Task: Approximate the solution of (16).\n\u2022 Parameters: Given are the matrices M, \u2126, the vector y, the target\nnumber of zeros \u2113, and a selection factor t \u2208(0, 1].\n\u2022 Initialization: Set k = 0 and perform the following steps:\n\u2013 Initialize Cosupport: \u02c6\u039bk = {1, 2, 3, . . . , p},\n\u2013 Initialize Solution:\n\u02c6xk = arg min\nx \u2225\u2126\u02c6\u039bkx\u22252\n2\nsubject to\ny = Mx.\n\u2022 GAP Iterations: Increment k by 1 and perform the following steps:\n\u2013 Project: Compute \u03b1 = \u2126\u02c6xk\u22121,\n\u2013 Find largest entries: \u0393k = {i : |\u03b1i| \u2265t maxj |\u03b1j|},\n\u2013 Update Support: \u02c6\u039bk = \u02c6\u039bk\u22121 \\ \u0393k, and\n\u2013 Update Solution:\n\u02c6xk = arg min\nx \u2225\u2126\u02c6\u039bkx\u22252\n2\nsubject to\ny = Mx.\n\u2013 Stopping Criterion: If k \u2265p \u2212d + m (or k \u2265p \u2212\u2113), stop.\n\u2022 Output: The proposed solution is \u02c6xGAP = \u02c6xk obtained after k iterations.\nFigure 5: Greedy Analysis Pursuit Algorithm (GAP)\ncould help in preventing the algorithm to incorporate this error into the solution.\nIn fact, this criterion is used in the experiment in Section 6.\nMultiple selections.. The selection factor 0 < t \u22641 allow the selection of mul-\ntiple rows at once, to accelerate the algorithm by reducing the number of iter-\nations.\nSolving the required least squares problems. The solution of Eq. (18) (and of\nthe adjusted problems with reduced \u2126at subsequent steps of the algorithm) is\ngiven analytically by\n\u02c6x0 =\n\u0014\nM\n\u2126\u02c6\u039b\n\u0015\u2020 \u0014\ny\n0\n\u0015\n= (MT M + \u2126T\n\u02c6\u039b\u2126\u02c6\u039b)\u22121MT y.\nIn practice, instead of (18), we compute\n\u02c6x0 = arg min\nx\n\b\n\u2225y \u2212Mx\u22252\n2 + \u03bb\u2225\u2126x\u22252\n2\n\t\n= arg min\nx\n\r\r\r\r\n\u0014y\n0\n\u0015\n\u2212\n\u0014 M\n\u221a\n\u03bb\u2126\n\u0015\nx\n\r\r\r\r\n2\n2\n20\nfor a small \u03bb > 0, yielding the solution\n\u02c6x0 =\n\u0014 M\n\u221a\n\u03bb\u2126\n\u0015\u2020 \u0014\ny\n0\n\u0015\n= (MT M + \u03bb\u2126T \u2126)\u22121MT y.\n5. Theoretical analysis\nSo far, we have introduced the cosparse analysis data model, provided unique-\nness results in the context of linear inverse problems for the model, and described\nsome algorithms that may be used to solve such linear inverse problems to re-\ncover cosparse signals.\nBefore validating the algorithms and the model pro-\nposed with experimental results, we \ufb01rst investigate theoretically under what\nconditions the proposed algorithms to solve cosparse signal recovery (16) are\nguaranteed to work. After that, we discuss the nature of the condition derived\nby contrasting it to that for the synthesis model. Further discussion including\nsome desirable properties of \u2126and M can be found in Appendix D.\n5.1. A Su\ufb03cient Condition for the Success of the \u21131-minimization\nIn the sparse synthesis framework, there is a well-known necessary and suf-\n\ufb01cient condition called the null space property (NSP) [15] that guarantees the\nsuccess of the synthesis \u21131-minimization\n\u02c6z0 := argmin\nz\n\u2225z\u22251\nsubject to\ny = \u03a6z\n(19)\nto recover the sparsest solution, say z0, to y = \u03a6z. To elaborate, in the case\nof a \ufb01xed support T , the \u21131-minimization (19) recovers every sparse coe\ufb03cient\nvector z0 supported on T if and only if\n\u2225zT \u22251 < \u2225zT c\u22251,\n\u2200z \u2208Null(\u03a6), z \u0338= 0.\n(20)\nThe NSP (20) cannot easily be checked but some \u2018simpler\u2019 su\ufb03cient conditions\ncan be derived from it; for example, one can get a recovery condition of [41]\ncalled the Exact Recovery Condition (ERC):\n\u2225|\u03a6\u2020\nT\u03a6T c|\u22251\u21921 < 1,\n(21)\nwhich also implies the success of greedy algorithms such as OMP [41]. Note that\nhere we used the symbol \u03a6 for an object which may be viewed as a dictionary or\na measurement matrix. Separating the data model and sampling, we can write\n\u03a6 = MD as was done in Section 3.\nOne may naturally wonder: is there a condition for the cosparse analysis\nmodel that is similar to (20) and (21)? The answer to this question seems to be\na\ufb03rmative with some quali\ufb01cation as the following two results show (the proofs\nare in Appendix A):\n21\nTheorem 7. Let \u039b be a \ufb01xed cosupport. The analysis \u21131-minimization\n\u02c6x0 := argmin\nx\n\u2225\u2126x\u22251\nsubject to\ny := Mx0 = Mx\n(22)\nrecovers every x0 with cosupport \u039b as a unique minimizer if, and only if,\nsup\nx\u039b:\u2126\u039bx\u039b=0\n|\u27e8\u2126\u039bcz, sign(\u2126\u039bcx\u039b)\u27e9| < \u2225\u2126\u039bz\u22251,\n\u2200z \u2208Null(M), z \u0338= 0.\n(23)\nCorollary 8. Let NT be any d\u00d7(d\u2212m) basis matrix for the null space Null(M),\nand \u039b be a \ufb01xed cosupport such that the \u2113\u00d7 (d \u2212m) matrix \u2126\u039bNT is of full\nrank d \u2212m. If\nsup\nx\u039b:\u2126\u039bx\u039b=0\n\u2225(N\u2126T\n\u039b)\u2020N\u2126T\n\u039bc sign(\u2126\u039bcx\u039b)\u2225\u221e< 1,\n(24)\nthen the analysis \u21131-minimization (22) recovers every x0 with cosupport \u039b.\nMoreover, if\n\u2225|(N\u2126T\n\u039b)\u2020N\u2126T\n\u039bc|\u2225\u221e\u2192\u221e= \u2225|\u2126\u039bcNT (\u2126\u039bNT )\u2020|\u22251\u21921 < 1\n(25)\nthen condition (24) holds true.\nThere is an apparent similarity between the analysis ERC condition (25)\nabove and its standard synthesis counterpart (21), yet there are some subtle\ndi\ufb00erences between the two that will be highlighted in Section 5.3.\n5.2. A Su\ufb03cient Condition for the Success of the GAP\nThere is an interesting parallel between the synthesis ERC (21) and its anal-\nysis version in Corollary 8; namely, the analysis ERC condition (25) also implies\nthe success of the GAP algorithm, as we will now show.\nFrom the way GAP algorithm works, we can guarantee that it will perform\na correct elimination at the \ufb01rst step if the largest analysis coe\ufb03cients of \u2126\u039bc \u02c6x0\nof the \ufb01rst estimate \u02c6x0 are larger than the largest of \u2126\u039b\u02c6x0 where \u039b denotes\nthe true cosupport of x0. This observation suggests that we can hope to \ufb01nd\na condition for success if we can \ufb01nd some relation between \u2126\u039bc \u02c6x0 and \u2126\u039b\u02c6x0.\nThe following result provides such a relation:\nLemma 9. Let NT be any d \u00d7 (d \u2212m) basis matrix for the null space Null(M)\nand \u039b be a \ufb01xed cosupport such that the \u2113\u00d7 (d \u2212m) matrix \u2126\u039bNT is of full\nrank d \u2212m. Let a signal x0 with \u2126\u039bx0 = 0 and its observation y = Mx0 be\ngiven. Then the estimate \u02c6x0 in (18) satis\ufb01es\n\u2126\u039b\u02c6x0 = \u2212(N\u2126T\n\u039b)\u2020N\u2126T\n\u039bc\u2126\u039bc \u02c6x0.\n(26)\nHaving obtained a relation between \u2126\u039b\u02c6x0 and \u2126\u039bc \u02c6x0, we can derive a suf-\n\ufb01cient condition which guarantees the success of GAP for recovering the true\ntarget signal x0:\n22\nTheorem 10. Let NT be any d\u00d7(d\u2212m) basis matrix for the null space Null(M)\nand \u039b be a \ufb01xed cosupport such that the \u2113\u00d7 (d \u2212m) matrix \u2126\u039bNT is of full\nrank d \u2212m. Let a signal x0 with \u2126\u039bx0 = 0 and an observation y = Mx0 be\ngiven. Suppose that the analysis ERC (25) holds true. Then, when applied to\nsolve (16), GAP with selections factor t \u2265\u2225|(N\u2126T\n\u039b)\u2020N\u2126T\n\u039bc|\u2225\u221e\u2192\u221ewill recover\nx0 after at most |\u039bc| iterations.\nProof. At the \ufb01rst iteration, GAP is doing the correct thing if it removes a row\nfrom \u2126\u039bc. Clearly, this happens when\n\u2225\u2126\u039b\u02c6x0\u2225\u221e< t\u2225\u2126\u039bc \u02c6x0\u2225\u221e.\n(27)\nIn view of (26), if (25) holds and t \u2265\u2225|(N\u2126T\n\u039b)\u2020N\u2126T\n\u039bc|\u2225\u221e\u2192\u221e, then (27) is\nguaranteed. Therefore, GAP successfully removes a row from \u2126\u039bc at the \ufb01rst\nstep.\nNow suppose that (25) was true and GAP has removed a row from \u2126\u039bc at\nthe \ufb01rst iteration. Then, at the next iteration, we have the same \u2126\u039b and, in\nthe place of \u2126\u039bc, a submatrix \u02dc\u2126\u039bc of \u2126\u039bc (with one fewer row). Thus, we can\ninvoke Lemma 9 again and we have\n\u2126\u039b\u02c6x1 = \u2212\n\u0000N\u2126T\n\u039b\n\u0001\u2020 N \u02dc\u2126T\n\u039bc \u02dc\u2126\u039bc \u02c6x1.\nLet R0 :=\n\u0000N\u2126T\n\u039b\n\u0001\u2020 N\u2126T\n\u039bc and R1 :=\n\u0000N\u2126T\n\u039b\n\u0001\u2020 N \u02dc\u2126T\n\u039bc. We observe that R1 is a\nsubmatrix of R0 obtained by removing one column. Therefore,\n\u2225|R1|\u2225\u221e\u2192\u221e< \u2225|R0|\u2225\u221e\u2192\u221e\u2264t.\nBy the same logic as for the \ufb01rst step, the success of the second step is guaran-\nteed. Repeating the same argument, we obtain the conclusion.\nRemark 11. As pointed out at the beginning of the subsection, the Exact Re-\ncovery Condition (25) for the cosparse signal recovery guarantees the success of\nboth the GAP and the analysis \u21131-minimization.\n5.3. Analysis vs synthesis exact recovery conditions\nWhen \u03a6 is written as MD, the exact recovery condition (21) for the sparse\nsynthesis model is equivalent to\n\u2225|(MDT )\u2020MDT c|\u22251\u21921 < 1.\n(28)\nHere, T is the support of the sparsest representation of the target signal. At\n\ufb01rst glance, the two conditions (28) and (25):\n\u2225|\u2126\u039bcNT (\u2126\u039bNT )\u2020|\u22251\u21921 < 1\nlook similar; that is, for both cases, one needs to understand the characteristics\nof a single matrix, \u2126NT for the cosparse model, and MD for the sparse model.\nMoreover, the expressions involving these matrices have similar forms.\n23\nHowever, upon closer inspection, there is a crucial di\ufb00erence in the structures\nof the two expressions. In the synthesis case, the operator norm in question\ndepends only on how the columns of MD are related, since a more explicit\nwriting of the pseudo-inverse shows that the matrix to consider is\n(DT\nT MT MDT )\u22121(MDT )T MDT c\nThis fact allows us to obtain more easily characterizable conditions like inco-\nherence assumptions [41] that ensure condition (28).\nTo the contrary, in the analysis case, more complicated relations among the\nrows and the columns of \u2126NT have to be taken into account. The matrix to\nconsider being\n\u2126\u039bcNT \u0000N\u2126T\n\u039b\u2126\u039bNT \u0001\u22121 N\u2126T\n\u039b,\nthe inner expression N\u2126T\n\u039b\u2126\u039bNT is connected with how the columns of \u2126NT\nare related. However, because the matrices \u2126\u039bcNT and N\u2126T\n\u039b appear outside,\nit also becomes relevant how the rows of \u2126NT are related.\nThere is also an interesting distinction in terms of the sharpness of these\nexact recovery conditions. Namely, the violation of (28) implies the failure of\nthe OMP in the sense that there exist a sparse vector x = DT zT for which the\n\ufb01rst step of OMP picks up an atom which is not indexed by T . To the opposite,\nthe violation of (25) does not seem to imply the necessary \u201cfailure\u201d of GAP in\na similar sense.\nNote however that both conditions are not essential for the success of the\nalgorithms. One of the reasons is that the violation of the conditions does not\nguarantee that the algorithms would select wrong atoms. Furthermore, even if\nthe GAP or the OMP \u201cfails\u201d in one step, that does not necessarily mean that\nthe algorithms fail in the end: further steps may still enable them to achieve an\naccurate estimate of the vector x0.\n5.4. Relation to the Work by Cand`es et. al. [9]\nBefore moving onto experimental results, we discuss the recovery guarantee\nresult of Cand`es et al. [9] for the algorithm\n\u02c6x = argmin\n\u02c6x\u2208Rd \u2225DT \u02c6x\u22251\nsubject to\n\u2225M\u02c6x \u2212y\u22252 \u2264\u01eb\n(29)\nwhen partial noisy observation y = Mx + w with \u2225w\u22252 \u2264\u01eb is given for an\nunknown target signal x.\nIn order to derive the result, the concept of D-RIP is introduced [9]: A\nmeasurement matrix M satis\ufb01es D-RIP adapted to D with constant \u03b4D\ns if\n(1 \u2212\u03b4D\ns )\u2225v\u22252\n2 \u2264\u2225Mv\u22252\n2 \u2264(1 + \u03b4D\ns )\u2225v\u22252\n2\nholds for all v that can be expressed as a linear combination of s columns of D.\nWith this de\ufb01nition of D-RIP, the main result of [9] can be stated as follows:\n24\nFor an arbitrary tight frame D and a measurement matrix M satisfying D-RIP\nwith \u03b4D\n7s < 0.6, the solution \u02c6x to (29) satis\ufb01es\n\u2225\u02c6x \u2212x\u22252 \u2264C0\u01eb + C1\n\u2225DT x \u2212(DT x)s\u22251\n\u221as\n(30)\nwhere the constants C0 and C1 may depend only on \u03b4D\n7s, and the notation (c)s\nrepresents a sequence obtained from a sequence c by keeping the s-largest values\nof c in magnitude (and setting the other to zero).\nThe above recovery guarantee is one of the few\u2014very likely the only\u2014results\nexisting in the literature on (29). However, we observe that there is much room\nfor improving the result. We now discuss why we hold this view. For clarity\nand for the purpose of comparison to our result, we consider only the case \u01eb = 0\nfor (29).\nFirst, we note that [9] implicitly uses the estimate of type \u2225\u2126\u039bcz\u22251 < \u2225\u2126\u039bz\u22251\nfor (23). Hence, the main result of [9] cannot be sharp in general due to the\nfact that the sign patterns of (23) are ignored5\nSecond, the quality of the bound \u2225DT x \u2212(DT x)s\u22251/\u221as in (30) is measured\nin terms of how e\ufb00ective DT x is in sparsifying the signal x with respect to the\ndictionary D. To explain, let us consider the synthesis \u21131-minimization\n\u22061(x) := argmin\nz\u2208Rn \u2225z\u22251\nsubject to\nMDz = Mx\n(31)\nand let \u22060(x) be the sparsest representation of x. Applying the standard result\nfor the synthesis \u21131-minimization, we have\n\u2225\u22061(x) \u2212\u22060(x)\u22252 \u2264C2\n\u2225\u22060(x) \u2212(\u22060(x))s\u22251\n\u221as\nprovided that MD satis\ufb01es the standard RIP with, e.g., \u03b42s <\n\u221a\n2 \u22121 \u22480.414.\nSince D is a tight frame, it is equivalent to\n\u2225D\u22061(x) \u2212x\u22252 \u2264C2\n\u2225\u22060(x) \u2212(\u22060(x))s\u22251\n\u221as\n.\n(32)\nNote that both \u22060(x) and DT x are legitimate representations of x since D\u22060(x) =\nx = DDT x.\nThus, \u22060(x) is sparser than DT x in general; in this sense,\nDT x is not e\ufb00ective in sparsifying x.\nGiven this, we expect that \u2225\u22060(x) \u2212\n(\u22060(x))s\u22251/\u221as is smaller than \u2225DT x \u2212(DT x)s\u22251/\u221as. We now see that (30)\nwith \u01eb = 0 and (32) are of the same form. Furthermore, given the degree of\nrestriction on the RIP constants (\u03b4D\n7s < 0.6 vs. \u03b42s < 0.414), we can only ex-\npect that the constant C2 is smaller than C1. From these considerations, (30)\n5Note that the same lack of sharpness holds true for our results based on (25), yet we will\nsee that these can actually provide cosparse signal recovery guarantees in simple but nontrivial\ncases.\n25\nonly lets us to conclude that analysis \u21131-minimization (17) performs on par with\nsynthesis \u21131-minimization (31), or tends to perform worse.\nThird, the nature of the formulation in (30) takes the view that the cosparse\nsignals are the same as the sparse synthesis signals as described in Section 2.4.\nDue to this, the only way for (30) to explain that the cosparse signals are\nperfectly recovered by analysis \u21131-minimization is to show that DT x is exactly\ns-sparse for some s > 0 with D-RIP constant \u03b4D\n7s < 0.6. Unfortunately, we\ncan quickly observe that the situation becomes hopeless even for moderately\novercomplete D; for example, let D be a 1.15-times overcomplete random tight\nframe for Rd and consider recovering (d \u22121)-cosparse signals for the operator\nDT . Note that (d\u22121)-cosparse signals x lead to (0.15d+1)-sparse representation\nDT x. This means that we need \u03b4D\n7(0.15d+1) = \u03b4D\n1.05d+7 to be smaller than 0.6 to\nshow that x can be recovered with analysis \u21131, which of course cannot happen\nsince \u03b4D\nd \u22651. By taking the synthesis view of the signals, (30) cannot explain\nthe recovery of the simplest cosparse signals (cosparsity d \u22121) no matter what\nM is (as long as it is under-determined).\nWe also observe that the result of [9] cannot say much about the recovery of\ncosparse signals with respect to the \ufb01nite di\ufb00erence operators \u2126DIF discussed\nin Section 3. This is due to the fact that \u2126T\nDIF is not a tight frame. How does\nour recovery result (25) fare in this regard? For illustration, we took \u2126to be\nthe \ufb01nite di\ufb00erence operator \u2126DIF for 32 \u00d7 32 images (thus, d = 1024). As a\ntest image, we took x to be constant in the region {(i, j) : i, j = 1, . . . , 16} and\n{(i, j) : i, j = 1, . . . , 16}c. For this admittedly simple test image, we computed\nthe operator norm in (25) for random measurement matrices M \u2208R640\u00d71024.\nWhen the operator norm was computed for 100 instances M, it was observed to\nbe less than 0.726. Hence, our result does give the guarantee of cosparse signal\nrecovery in simple cases.\n6. Experiments\nEmpirical performance of the proposed algorithms is presented in this sec-\ntion. First, we show how the algorithms perform in synthetic cosparse recovery\nproblems. Second, experimental results for an analysis-based compressed sens-\ning are presented.\n6.1. Performance of analysis algorithms\nIn this section, we apply the algorithms described in Section 4 to synthetic\ncosparse recovery problems. In the experiment, the entries of M \u2208Rm\u00d7d were\ndrawn independently from the normal distribution. For the analysis operator\n\u2126\u2208Rp\u00d7d, it was constructed so that its transpose is a random tight frame with\nunit norm columns\u2014we will simply say that \u2126is a random tight frame in this\ncase.6 Next, the co-sparsity \u2113was chosen, and the true or target signal x was\n6One could also construct \u2126by simply drawing the rows of it randomly and independently\nfrom Sd\u22121 without the tight frame constraint. We have run the experiment for such operators\n26\ngenerated randomly as described in Section 2.2. The observation was obtained\nby y = Mx.\nWe have used Matlab cvx package [24] with the precision set to best for\nthe analysis-\u21131. For the \ufb01nal results, we used the estimate \u02c6x from \u21131 solver to\nobtain an estimate of the cosupport\u2014the cosupport estimate was obtained by\ntaking the indices for which the corresponding analysis coe\ufb03cient is of size less\nthan 10\u22126\u2014and then using this cosupport and the observation y to compute\nthe \ufb01nal estimate of x (this process can be considered as de-biasing.).\n\u03b4\n\u03c1\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\n\u03b4\n\u03c1\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\n\u03b4\n\u03c1\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\n\u03b4\n\u03c1\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\n\u03b4\n\u03c1\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\n\u03b4\n\u03c1\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nFigure 6: Recovery Rate of Analysis Algorithms for d = 200. The \ufb01gures correspond to GAP\n(top) and L1 (bottom) with \u03c3 = 1 (left), \u03c3 = 1.2 (center) and \u03c3 = 2 (right).\nFigure 6 shows the results. In all cases, the signal dimension d is set to 200.\nWe then varied the number m of measurements, the co-sparsity \u2113of the target\nsignal, and the operator size p according to the following formulae:\nm = \u03b4d,\n\u2113= d \u2212\u03c1m,\np = \u03c3d.\nwhich is consistent with Donoho & Tanner\u2019s notations for phase transition dia-\ngrams [14]: \u03b4 = m/d is the undersampling ratio, and \u03c1 = (d \u2212\u2113)/m measures\nthe relative dimension of the \u2113-cosparse subspaces compared to the number of\nmeasures. For every \ufb01xed parameter triplet (\u03c3, \u03b4, \u03c1), the experiment was re-\npeated 50 times. A relative error of size less than 10\u22126 was counted as perfect\nrecovery. Each pixel in the diagrams corresponds to a triplet (\u03c3, \u03b4, \u03c1) and the\npixel intensity represents the ratio of the signals recovered perfectly with white\nbeing the 100% success.\nThe \ufb01gures show that the GAP can be a viable option when it comes to the\ncosparse signal recovery. What is a bit unexpected is that GAP performs better\nthan \u21131-minimization, especially for overcomplete \u2126\u2019s. Yet, it should be clear\nand observed that the result was similar.\n27\nfrom its description that GAP has polynomial complexity, and it is tractable in\npractice.\nAn interesting phenomenon observed in the plots for overcomplete \u2126is that\nthere seems to be some threshold \u03b4\u2217such that if the observation to dimension\nratio \u03b4 is less than \u03b4\u2217, one could not recover any signal however cosparse it\nmay be. We may explain this heuristically as follows: If m measurements are\navailable, then the amount of information we have for the signal is c1m where c1\nis the number of bits each observation represent. In order to recover a cosparse\nsignal, we need \ufb01rst to identify which subspace the signal belongs to out of\n\u0000p\n\u2113\n\u0001\n,\nand then to obtain the d \u2212\u2113coe\ufb03cients for the signal with respect to a basis of\nthe d \u2212\u2113dimensional subspace. Therefore, roughly speaking, one may hope to\nrecover the signal when\nc1m \u2265log2\n\u0012p\n\u2113\n\u0013\n+ c1(d \u2212\u2113) = log2\n\u0012p\n\u2113\n\u0013\n+ \u03c1c1m.\nThus, the recovery is only possible when (1 \u2212\u03c1)\u03b4 \u2265log2\n\u0000p\nd\n\u0001\n/(c1d).\nUsing\nthe relation p = \u03c3d and Stirling\u2019s approximation, this leads to an asymptotic\nrelation\n\u03b4 \u2265(1 \u2212\u03c1)\u03b4 \u2265\u03c3 log \u03c3 \u2212(\u03c3 \u22121) log(\u03c3 \u22121)\nc1\n,\nwhich explains the phenomenon.\nThe calculation above and the experimental evidence from the \ufb01gures con-\n\ufb01rm the intuition we had in Section 2.3: The combinatorial number of low-\ndimensional cosparse subspaces arising from analysis operators in general po-\nsition is not desirable. This strengthens our view on the necessity of design-\ning/learning analysis operators with high linear dependencies.\n6.2. Analysis-based Compressed Sensing\nWe observed in Section 6.1 that the cosparse analysis model facilitates e\ufb00ec-\ntive algorithms to recover partially observed cosparse signals. In this section,\nwe demonstrate the e\ufb00ectiveness of GAP algorithm on a standard toy problem:\nthe Shepp Logan phantom recovery problem.\nWe consider the following problem that is related to computed tomography\n(CT): There is an image, say of size n\u00d7n, which we are interested in but cannot\nobserve directly. It can only be observed indirectly by means of its 2D Fourier\ntransform coe\ufb03cients.\nHowever, due to high cost of measurements or some\nphysical limitation, the Fourier coe\ufb03cients can only be observed along a few\nradial lines. These limited observations or the locations thereof can be modeled\nby a measurement matrix M, and with the obtained observation we want to\nrecover the original image. As an ideal example, we consider the Shepp Logan\nphantom. One can easily see that this image is a good example of cosparse\nsignals in \u2126DIF which consists of all the vertical and horizontal gradients (or\none step di\ufb00erences). This image has been used extensively as an example in\nthe literature in the context of compressed sensing (see, e.g., [8, 3]).\n28\nFigure 7 is the result obtained using GAP. The number of measurements\nthat corresponds to 12 radial lines is m = 3032.\nCompared to the number\nof pixels in the image d = 65536, it is approximately 4.63%. The number of\nanalysis atoms that give non-zero coe\ufb03cients is p \u2212\u2113= 2546.\nThe size of\n\u2126DIF is roughly twice the image size d = 65536, namely p = 130560. At \ufb01rst\nglance, this corresponds to very high co-sparsity level (\u2113= 130560 \u22122546), or\nput di\ufb00erently, given the high cosparsity level \u2113= 128014, we seem to have\nrequired too many measurements. However, using the conjectured near optimal\nnecessary condition for uniqueness guarantee (13), we may have uniqueness\nguarantee when m \u22652551. Also, using the su\ufb03cient condition (15), one would\nwant to have m \u22653058 measurements. In view of this, the fact that GAP\nrecovered the signal perfectly for 3032 measurements is remarkable!\nFigure 7: Recovery of 256 \u00d7 256 Shepp Logan phantom image. From top to bottom, left to\nright: (a) Original Image. (b) Sampling locations of Fourier coe\ufb03cients. (c) Reconstructed\nimage. (d) Locations where one-step di\ufb00erence of the original image is non-zero. Upper half\ncorresponds to the horizontal di\ufb00erences and lower half the vertical di\ufb00erences. (e) Locations\nthat GAP identi\ufb01ed/eliminated to be the ones where the di\ufb00erences are likely non-zero. (f)\nLocations that GAP failed to identify as non-zero locations. Blank black \ufb01gure indicates that\nnone of the non-zero locations were missed (perfect reconstruction).\nWe have also ran the GAP algorithm for a larger sized 512 \u00d7 512 problem.\nThe results (not shown here) are visually similar to Figure 7.\nIn this case,\nthe number of measurements (m = 7112) represents approximately 2.71% of\nthe image size (d = 262144). The number of non-zero analysis coe\ufb03cients is\np \u2212\u2113= 5104. The su\ufb03cient uniqueness condition (15) gives m \u22656126 as a\n29\nnumber of measurements for the uniqueness.\nRemark 12. Due to the large size of these problems, GAP algorithm as de-\nscribed in Section 4 had to be modi\ufb01ed: We used numerical optimization to\ncompute pseudo-inverses. Also, due to high computational cost, we eliminated\nmany rows at each iteration (super greedy) instead of one. Although this was not\nimplemented using a selection factor, this can be interpreted as using varying\nselection factors 0 < tk < 1 along the iterations.\nTo conclude this section, we have repeated the 256\u00d7256 Shepp Logan phan-\ntom image recovery problem for several algorithms while varying the number of\nradial observation lines. Given that we know the minimal theoretical number\nand a theoretically su\ufb03cient number of radial observation lines for the unique-\nness guarantee, the experimental result gives us an insight on how various al-\ngorithms actually perform in the recovery problem in relation to the amount of\nobservation available. Figure 8 shows the outcome. The algorithms used in the\nexperiment are the GAP, the TV-minimization from l1magic, the AIHT from\n[3], and the back-projection algorithm.7 The GAP and l1magic can be viewed\nas analysis-based reconstruction algorithms while the AIHT is a synthesis-based\nreconstruction algorithm. The AIHT is seen to use Haar wavelets as the synthe-\nsis dictionary, hence the algorithm implicitly assumes that the phantom image\nhas sparse representation in that dictionary. We remark that while Figure 8\ngives an impression that the AIHT does not have any improvement over the\nbaseline back-projection algorithm, perfect reconstructions were observed for\nthe former when su\ufb03cient measurements were available, which is not the case\nfor the latter.\n10\n12\n14\n16\n18\n20\n22\n24\n0\n20\n40\n60\n80\n100\nNumber of Radial Lines\n \n \nSNR\nGAP\nL1 MAGIC\nBACK\u2212PROJ\nAIHT\nFigure 8: SNR vs the number of radial observation lines in 256 \u00d7 256 Shepp Logan phantom\nimage recovery. The output line for the GAP is clipped due to high SNR value.\n7The code for l1magic was downloaded from http://www.acm.caltech.edu/l1magic/ and\nthe one for AIHT from http://www.personal.soton.ac.uk/tb1m08/sparsify/AIHT Paper Code.zip.\nThe result for the back-projection was obtained using the code for AIHT.\n30\nRemark 13. It must be noted that in our experiment, each radial line consists\nof N pixels for an N \u00d7 N image; this is in contrast to the fact that the radial\nlines in the existing codes, e.g. l1magic, have N \u22121 pixels. We have made\nappropriate changes for our experiment. The radial lines with N \u22121 pixels do\nmake the recovery problem more di\ufb03cult and more observations were required\nfor perfect recovery for the GAP.\n7. Conclusions and Further Work\nIn this work, we have described the cosparse analysis data model as an al-\nternative to the popular sparse synthesis model. By the description, we have\nshown that the cosparse analysis model is distinctly di\ufb00erent from the sparse\nsynthesis one in spite of their apparent similarities. In particular, treating the\ncosparse model as the synthesis model by assuming that the analysis representa-\ntions of cosparse signals are sparse was demonstrated to be not very meaningful.\nHaving had presented the model, we have stated conditions that guarantee the\nuniqueness of cosparse solutions in the context of linear inverse problems based\non the work [29]. We then presented some algorithms for the cosparse recovery\nproblem and provided some theoretical result for the analysis \u21131-minimization\nand the newly proposed GAP. Lastly, the model and the proposed algorithm\nwere validated via experimental results.\nAlthough our work in this paper shows that the cosparse analysis model\ntogether with algorithms based on the model is an interesting subject to study\nand viable for practical applications, there are much more to be learned about\nthe model.\nAmong possible future avenues for related research, we list the\nfollowing: 1) The stability of measurement matrices M on the analysis union\nof subspaces \u222a\u039bW\u039b; 2) The e\ufb00ect of noise on the cosparse analysis model and\nassociated algorithms; 3) The designing / learning of analysis operators for\nclasses of signals of interest; 4) More concrete and/or optimal theoretical success\nguarantees for algorithms, with a better understanding of the role of linear\ndependencies between rows of the analysis operator.\nAppendix A. Proof of Theorem 7 and Corollary 8\nLet us begin with the simplest case. For a \ufb01xed x0 with cosupport \u039b, the\nanalysis \u21131-minimization (22) recovers x0 as the unique minimizer if and only if\n|\u27e8\u2126\u039bcz, sign(\u2126\u039bcx0)\u27e9| < \u2225\u2126\u039bz\u22251,\n\u2200z \u2208Null(M), z \u0338= 0.\nThis follows from two facts: a) the above condition characterizes strict local\nminima of the optimization problem; b) the optimization problem is convex and\ncan have at most one strict local minimum, which must be the unique global\noptimum. From this, we derive the following: The analysis \u21131-minimization (22)\nrecovers x0 as a unique minimizer for any x0 with cosupport \u039b, if and only if\nsup\nx\u039b:\u2126\u039bx\u039b=0\n|\u27e8\u2126\u039bcz, sign(\u2126\u039bcx\u039b)\u27e9| < \u2225\u2126\u039bz\u22251,\n\u2200z \u2208Null(M), z \u0338= 0\n31\nand the proof of Theorem 7 is complete.\nTo obtain Corollary 8, observe that we can remove the constraint z \u2208\nNull(M) by writing z = NT \u03b1 where NT is an d \u00d7 (d \u2212m) basis matrix for\nNull(M) and \u03b1 \u2208Rd\u2212m is an appropriate coe\ufb03cient sequence. Thus, the nec-\nessary and su\ufb03cient condition becomes\nsup\nx\u039b:\u2126\u039bx\u039b=0\n\f\f\u27e8\u2126\u039bcNT \u03b1, sign(\u2126\u039bcx\u039b)\u27e9\n\f\f < \u2225\u2126\u039bNT \u03b1\u22251,\n\u2200\u03b1 \u2208Rd\u2212m, \u03b1 \u0338= 0.\n(A.1)\nSince the \u2113\u00d7 (d \u2212m) matrix \u2126\u039bNT is thin (\u2113\u2265d \u2212m) and full-rank, de\ufb01ning\n\u03b2 := \u2126\u039bNT \u03b1, we have \u03b1 = (\u2126\u039bNT )\u2020\u03b2. Therefore, a su\ufb03cient (but no longer\nnecessary) recovery condition for analysis \u21131-minimization is\nsup\nx\u039b:\u2126\u039bx\u039b=0\n\f\f\u27e8\u2126\u039bcNT (\u2126\u039bNT )\u2020\u03b2, sign(\u2126\u039bcx\u039b)\u27e9\n\f\f < \u2225\u03b2\u22251,\n\u2200\u03b2 \u2208R\u2113, \u03b2 \u0338= 0.\n(A.2)\nEquivalently, for all x\u039b with \u2126\u039bx\u039b = 0,\nsup\n\u2225\u03b2\u22251=1\n|\u27e8\u03b2, (N\u2126T\n\u039b)\u2020N\u2126T\n\u039bc sign(\u2126\u039bcx\u039b)\u27e9| < 1\n(A.3)\nthat is to say\nsup\nx\u039b:\u2126\u039bx\u039b=0\n\u2225(N\u2126T\n\u039b)\u2020N\u2126T\n\u039bc sign(\u2126\u039bcx\u039b)\u2225\u221e< 1.\n(A.4)\nCondition (24) follows from the above. To conclude the proof of Corollary 8,\nwe note that since \u2225sign(\u2126\u039bcx\u039b)\u2225\u221e= 1, the left hand side of (A.4) is bounded\nabove by\n\u2225|(N\u2126T\n\u039b)\u2020N\u2126T\n\u039bc|\u2225\u221e\u2192\u221e= \u2225|\u2126\u039bcNT (\u2126\u039bNT )\u2020|\u22251\u21921.\nTherefore, condition (25) implies (24) and the proof is complete.\nAppendix B. Proof of Lemma 9\nSince \u02c6x0 is the solution of arg minx \u2225\u2126x\u22252\n2 subject to y = Mx, applying the\nLagrange multiplier method, we observe that \u02c6x0 satis\ufb01es\n\u2126T \u2126\u02c6x0 = MT v\nand\nM\u02c6x0 = y,\nfor some v \u2208Rm.\nFrom the \ufb01rst equation, we obtain v = (MT )\u2020\u2126T \u2126\u02c6x0.\nPutting this back in, one gets\n\u0000Id \u2212MT (MT )\u2020\u0001\n\u2126T \u2126\u02c6x0 = 0. The last equation\ncan be written as (NT )\u2020N\u2126T \u2126\u02c6x0 = 0, where (NT )\u2020 is the pseudo-inverse of\nNT . Thus,\nN\u2126T \u2126\u02c6x0 = 0.\n32\nNow, we split \u2126T \u2126= \u2126T\n\u039b\u2126\u039b + \u2126T\n\u039bc\u2126\u039bc and write\nN\u2126T\n\u039b\u2126\u039b\u02c6x0 = \u2212N\u2126T\n\u039bc\u2126\u039bc \u02c6x0.\nSince \u2126\u039bx0 = 0, we can also write\nN\u2126T\n\u039b\u2126\u039bu = \u2212N\u2126T\n\u039bc\u2126\u039bc \u02c6x0\n(B.1)\nwith u = \u02c6x0 \u2212x0. On the other hand, from M\u02c6x0 = y = Mx0, we have Mu = 0.\nThis means that u can be expressed as u =: NT w for some w. Plugging this\ninto (B.1), we have\nN\u2126T\n\u039b\u2126\u039bNT w = \u2212N\u2126T\n\u039bc\u2126\u039bc \u02c6x0.\nHence, w = \u2212\n\u0000N\u2126T\n\u039b\u2126\u039bNT \u0001\u22121 N\u2126T\n\u039bc\u2126\u039bc \u02c6x0. This gives us\n\u02c6x0 \u2212x0 = u = \u2212NT \u0000N\u2126T\n\u039b\u2126\u039bNT \u0001\u22121 N\u2126T\n\u039bc\u2126\u039bc \u02c6x0.\nAgain, using \u2126\u039bx0 = 0, we have\n\u2126\u039b\u02c6x0 = \u2212\u2126\u039bNT \u0000N\u2126T\n\u039b\u2126\u039bNT \u0001\u22121 N\u2126T\n\u039bc\u2126\u039bc \u02c6x0 = \u2212(N\u2126T\n\u039b)\u2020N\u2126T\n\u039bc\u2126\u039bc \u02c6x0.\nAppendix C. Proof of Proposition 6\nAll the statements in this section are about a 2D regular graph consisting of\nd = N \u00d7 N vertices (V ) and the vertical and horizontal edges (E) connecting\nthese vertices. To prove the proposition, we will start with two simple lemmas.\nLemma 14. For a \ufb01xed \u2113, the value\n\u03b1(\u2113) :=\nmin\n\u039b\u2282E:|\u039b|\u2265\u2113{|V (\u039b)| \u2212J(\u039b)}\nis achieved for a subgraph (V (\u039b), \u039b)\u2014we will simply identify \u039b with the subgraph\nfrom here on\u2014satisfying |\u039b| = \u2113and J(\u039b) = 1.\nProof. It is not di\ufb03cult to check that the minimum if achieved for \u039b with |\u039b| = \u2113.\nThus, we will assume |\u039b| = \u2113.\nNow, we need to show that there is also a \u039b with J(\u039b) = 1. Suppose that\n\u02dc\u039b with |\u02dc\u039b| = \u2113achieves \u03b1(\u2113) and J(\u02dc\u039b) > 1. We will show that we can obtain\n\u039b from \u02dc\u039b that also achieves the value \u03b1(\u2113), and |\u039b| = \u2113and J(\u039b) = 1. For\nsimplicity, we will consider the case J(\u02dc\u039b) = 2 only; one can deal with other\ncases by the repetition of the same argument.\nLet \u02dc\u039b1 and \u02dc\u039b2 be the two connected components of \u02dc\u039b.\nNote that on a\n2D regular graph, we can shift a subgraph horizontally or vertically unless the\nsubgraph has vertices on all four boundaries of V . Since \u02dc\u039b1 and \u02dc\u039b2 are discon-\nnected, not all of them can have vertices on all four boundaries of V . Therefore,\none of them, say \u02dc\u039b1, can be shifted towards the other. Let us consider the \ufb01rst\nmoment when they touched each other. Let t be the number of vertices that\n33\ncoincided. Then, at most t \u22121 edges must have coincided. Thus, denoting the\nnumber of edges coincided by s < t, the resulting subgraph \u02dc\u039b\u2032 has |V (\u02dc\u039b)| \u2212t\nvertices and |\u02dc\u039b| \u2212s edges and one connected components. Now let \u039b be a sub-\ngraph obtained from \u02dc\u039b\u2032 by adding s additional edges that are connected to \u02dc\u039b\u2032.\nThen,\n|V (\u039b)| \u2264|V (\u02dc\u039b\u2032)| + s \u2264|V (\u02dc\u039b)| \u2212t + s,\n|\u039b| = |\u02dc\u039b| = \u2113, and J(\u039b) = 1. Hence,\n|V (\u039b)| \u2212J(\u039b) \u2264|V (\u02dc\u039b)| \u2212t + s \u22121 = |V (\u02dc\u039b)| \u2212J(\u02dc\u039b) \u2212t + s + 1 \u2264|V (\u02dc\u039b)| \u2212J(\u02dc\u039b),\nwhich is what we wanted to show.\nFor the next lemma, let us de\ufb01ne the degree \u03b4\u039b(v) of a vertex v \u2208V (\u039b):\n\u03b4\u039b(v) := |{e \u2208\u039b : v \u2208e}|\nwhere v \u2208e signi\ufb01es that v is a vertex of the edge e. That is, \u03b4\u039b(v) is the\nnumber of edges in \u039b that start/end at v.\nLemma 15. For a non-empty \u039b \u2282E,\n4|V (\u039b)| \u2265\nX\nv\u2208V (\u039b)\n\u03b4\u039b(v) + 4\nholds.\nProof. On a 2D regular grid, \u03b4\u039b(v) \u22644. Therefore, we have\n4|V (\u039b)| \u2265\nX\nv\u2208V (\u039b)\n\u03b4\u039b(v).\nSince the equality above would hold if and only if \u03b4\u039b(v) = 4 for all v \u2208V (\u039b),\nthe claim of the lemma can be proved by showing that there are at least two\nvertices v with \u03b4\u039b(v) \u22642. For this, we consider two \u2018extreme corner points\u2019 of\n\u039b. Let vNW be the north-west corner point of \u039b in the sense that 1) there is no\nvertex v \u2208V (\u039b) that is above it, and 2) there is no vertex v \u2208V (\u039b) that is left\nof vNW and on the same level (height). Let vSE be the south-east corner point\nof \u039b de\ufb01ned similarly. By de\ufb01nition, \u03b4\u039b(vNW) \u22642 and \u03b4\u039b(vSE) \u22642, and vNW\nand vSE are distinct vertices if \u039b \u0338= \u2205.\nProof of Proposition 6. We will \ufb01rst prove the upper bound. Clearly,\nX\nv\u2208V (\u039b)\n\u03b4\u039b(v) = 2|\u039b|.\nBy Lemma 15, we also have\n4|V (\u039b)| \u2265\nX\nv\u2208V (\u039b)\n\u03b4\u039b(v) + 4\n34\nHence, we have\n|V (\u039b)| \u2265|\u039b|\n2 + 1\n(C.1)\nBy Lemma 14, the value of \u03ba\u2126DIF(\u2113) given by Eq. (12) is attained for \u039b with\nJ(\u039b) = 1 and |\u039b| = \u2113. Combining this with (12) and (C.1) we get\n\u03ba\u2126DIF(\u2113) \u2264|V | \u2212(|V (\u039b)| \u22121) \u2264|V | \u2212|\u039b|/2 = d \u2212\u2113\n2.\nThe proof of the lower bound is given in Lemma 16.\nBefore moving on to Lemma 16, we give a brief motivation for it. Our goal\nis to obtain not just a lower bound on \u03ba\u2126DIF but a lower bound that is close\nto optimal. By Lemma 14, \u03ba\u2126DIF(\u2113) is achieved for connected \u039b, so we will\nconsider such \u039b\u2019s only (J(\u039b) = 1). With J(\u039b) = 1, the formula (12) tells us to\nlook for the cases when |V (\u039b)| is minimal in order to compute \u03ba\u2126DIF(\u2113).\nWhat is the shape of the collection of edges \u039b yielding the minimum ?\nRecalling Euler\u2019s formula for graphs on plane:\n|V (\u039b)| \u2212|\u039b| + |F(\u039b)| = 2,\n(C.2)\nwhere F(\u039b) is the faces of \u039b which includes the \u2018unbounded one\u2019, we see that\nwe are seeking \u039b such that |F(\u039b)| is maximal, i.e., there is maximum number\nof faces. By intuition, we conjecture that this happens when \u039b consists of all\nthe edges in an almost square, by which we mean V (\u039b) is an r \u00d7 r or r \u00d7 (r + 1)\nrectangular grid or the inbetweens (e.g., an r\u00d7r grid of pixels to which 1 \u2264j \u2264r\npixels have been added on one side). These considerations lead to the following:\nLemma 16.\n\u03ba\u2126DIF(\u2113) \u2265d \u2212\u2113\n2 \u2212\nr\n\u2113\n2 \u22121\nfor \u2113\u22655.\nProof. For r \u22652, we consider a subgraph corresponding to an r\u00d7r square (solid\nlines) and consider graphs obtained by adding additional edges in the fashion\ndepicted in Figure C.9.\nFigure C.9: Add dashed edges (from longer to shorter dashed) to r \u00d7 r square subgraph (solid\nlines).\n35\nWe \ufb01nd that for the square \u039b, |\u039b| = 2(r2 \u2212r) and |V (\u039b)| = r2, for the graph\n\u039b with one additional edge, |\u039b| = 2(r2\u2212r)+1 and |V (\u039b)| = r2+1, for the graph\n\u039b with two additional edges, |\u039b| = 2(r2 \u2212r)+2 and |V (\u039b)| = r2 +2, and for the\ngraph \u039b with three additional edges, |\u039b| = 2(r2 \u2212r)+ 3 and |V (\u039b)| = r2 + 2. In\nfact, we observe that two edges can be added while adding one additional vertex\nuntil \u039b corresponds to r \u00d7 (r + 1) rectangle. Summarizing all these, a graph \u039b\nthat is constructed as above, is contained r \u00d7 (r + 1) rectangle (included), and\ncontains r\u00d7r square; satis\ufb01es either |\u039b| = 2(r2\u2212r)+2j or |\u039b| = 2(r2\u2212r)+2j+1,\nand |V (\u039b)| = r2 + j + 1, for j = 1, . . . , r \u22121\u2014this holds for j = r as well.\n(Here, the case |\u039b| = 2(r2 \u2212r) + 1 is not stated.) By a similar observation, we\nobserve that a graph \u039b that is constructed similarly as above, is contained in\n(r + 1) \u00d7 (r + 1) square (included), and contains r \u00d7 (r + 1) square; satis\ufb01es\neither |\u039b| = 2r2 \u22121 + 2j or |\u039b| = 2r2 \u22121 + 2j + 1, and |V (\u039b)| = r2 + r + j + 1,\nfor j = 1, . . . , r\u2014this holds for j = r + 1 as well.\nThe above observation leads to the following inequalities\u2014which we conjec-\nture to be in fact equalities:\n\u03ba\u2126DIF(2(r2 \u2212r) + 2j) \u2265d \u2212(r2 + j),\nj = 1, . . . , r,\n\u03ba\u2126DIF(2(r2 \u2212r) + 2j + 1) \u2265d \u2212(r2 + j),\nj = 1, . . . , r,\n\u03ba\u2126DIF(2r2 \u22121 + 2j) \u2265d \u2212(r2 + r + j),\nj = 1, . . . , r + 1,\n\u03ba\u2126DIF(2r2 \u22121 + 2j + 1) \u2265d \u2212(r2 + r + j),\nj = 1, . . . , r + 1.\nWe will now express these in a simpler form in terms of |\u039b| = \u2113. In the \ufb01rst\ncase, letting \u2113= 2(r2 \u2212r) + 2j, we have\nd \u2212(r2 + j) = d \u2212\u2113\n2 \u2212r.\nSince\n2(r2 \u22122r + 1) \u22642(r2 \u2212r + 1) \u2264\u2113\u22642r2,\nwe have r \u22121 \u2264\nq\n\u2113\n2 \u2264r. Hence, we can write \u03ba\u2126DIF(\u2113) \u2265d \u2212\u2113\n2 \u2212\nq\n\u2113\n2 \u22121. The\nother three cases can be treated similarly and we obtain\n\u03ba\u2126DIF(\u2113) \u2265d \u2212\u2113\n2 \u2212\nr\n\u2113\n2,\n\u03ba\u2126DIF(\u2113) \u2265d \u2212\u2113\n2 \u2212\nr\n\u2113\n2 \u22121\n2,\n\u03ba\u2126DIF(\u2113) \u2265d \u2212\u2113\n2 \u2212\nr\n\u2113\n2\nTherefore, for all \u2113\u22655, we have \u03ba\u2126DIF(\u2113) \u2265d \u2212\u2113\n2 \u2212\nq\n\u2113\n2 \u22121.\nAppendix D. Discussion on the analysis exact recovery condition\nWe observe that the analysis ERC condition (25) is not sharp in general,\nespecially for the redundant \u2126.\nIn the case of GAP, tracing the arguments\n36\nof Lemma 9 and Theorem 10, we conclude that in order for (25) to be sharp,\nthere must exist a cosparse signal x0 such that \u2126\u039bc \u02c6x0 matches the exact sign\npattern of the row of (N\u2126T\n\u039b)\u2020N\u2126T\n\u039bc with the largest \u21131-norm and is of constant\nmagnitude in absolute value. We remind that \u02c6x0 is the initial estimate that\nappears in the algorithm.\nSince the collection of \u2126\u039bc \u02c6x0 may not span the\nwhole R\u039bc, especially when \u2126is over-complete, it is unreasonable to expect the\nexistence of such an x0. Similarly, in the case of analysis \u21131, we know that\n(25) is obtained from (24) in a crude way without taking into account the sign\npatterns of \u2126\u039bcx\u039b, which is not sharp in general for redundant \u2126.\nAverage case performance guarantees?. Can we think of a way to obtain a more\nrealistic success guarantee? We have a partial answer for this question in the\nsense that we can derive a condition\u2014which is not a guarantee\u2014that re\ufb02ects\nempirical results more faithfully. The idea is, instead of obtaining an upper\nbound of the left hand side of (24) by disregarding (or considering the worst\ncase of) sign patterns, to model the e\ufb00ects of the sign patterns by estimating\nthe size of the left hand side in terms of the maximum \u21132-norm of the rows\nof\n\u0000N\u2126T\n\u039b\n\u0001\u2020 N\u2126T\n\u039bc (up to some constants). Though further investigation is de-\nsirable, we have empirically observed that the condition derived in this way\nre\ufb02ected better the success rates of GAP and \u21131-minimization.\nDesirable properties for \u2126and M. At this point, one may ask a practical ques-\ntion: what are desirable properties of \u2126and M that would help the perfor-\nmance of GAP or \u21131-minimization? Can we gain some insights from our the-\noretical result?\nFor this, we look for scenarios where the entries of R0 :=\n\u2126\u039bNT \u0000N\u2126T\n\u039b\u2126\u039bNT \u0001\u22121 N\u2126T\n\u039bc are small (hence, it is likely that condition (25)\nis satis\ufb01ed). We start with the inner expression\n\u0000N\u2126T\n\u039b\u2126\u039bNT \u0001\u22121. The larger\nthe minimum singular value of N\u2126T\n\u039b\u2126\u039bNT , the smaller the entries of R0. First,\nassuming that the rows of \u2126are normalized, we note that the minimum singular\nvalue is larger when the size \u039b is larger. Second, the closer the minimum sin-\ngular value is to the maximum one (this is in some sense an RIP-like condition\nfor \u2126), the larger it is. These two observations tell us that \u2126should have high\nlinear dependencies (to allow large cosupport \u039b) and the rows of \u2126should be\nclose to uniformly distributed on Sd\u22121.\nSuppose that \u2126has the properties described above. Then, R0 is well ap-\nproximated by R1 := \u03b3\u2126\u039bNT N\u2126T\n\u039bc for some \u03b3 > 0. Therefore, we ask when\nthe entries of \u2126\u039bNT N\u2126T\n\u039bc are small. Each entry of \u2126\u039bNT N\u2126T\n\u039bc can be guar-\nanteed to be small if a) N satis\ufb01es an RIP condition for the space spanned by\ntwo rows of \u2126and the rows of \u2126are incoherent. In summary, it is desirable\nthat:\n\u2022 The rows of \u2126are close to uniformly distributed in Sd\u22121.\n\u2022 \u2126is highly redundant and have highly linearly dependent structure.\n\u2022 M is \u2018independent\u2019 from \u2126. This has to do with the RIP-like properties.\n37\n\u2022 The rows of \u2126are incoherent.\n\u2022 The cosparsity \u2113are large.\nRemark 17. The 2D \ufb01nite di\ufb00erence operator \u2126DIF may be considered inco-\nherent even though the coherence is relatively large (1/4). This is because the\nmajority of pairs of rows of \u2126DIF are in fact uncorrelated.\nHeuristic comparison of success guarantees for analysis-\u21131 and GAP. We point\nout that one can obtain from (26) a condition for the GAP that is similar to\n(24). For this, we observe from (26) that\n\u2225\u2126\u039b\u02c6x0\u2225\u221e= \u2225(N\u2126T\n\u039b)\u2020N\u2126T\n\u039bc\u2126\u039bc \u02c6x0\u2225\u221e= \u2225[\u2126\u039bcNT (\u2126\u039bNT )\u2020]T \u2126\u039bc \u02c6x0\u2225\u221e.\nSince \u2225\u2126\u039b\u02c6x0\u2225\u221e< \u2225\u2126\u039bc\u02c6x0\u2225\u221eis the necessary and su\ufb03cient condition for the\n(one-step) success of the GAP, we can derive a necessary and su\ufb03cient condition:\n\u2225[\u2126\u039bcNT (\u2126\u039bNT )\u2020]T \u2126\u039bc \u02c6x0\u2225\u221e< \u2225\u2126\u039bc \u02c6x0\u2225\u221e\nwhere x0 is varied over all signals with cosupport \u039b and \u02c6x0 is the signal resulting\nfrom the \ufb01rst step of GAP . The above condition can be rewritten in a form\nsimilar to (24):\nsup\nx0\n\u2225[\u2126\u039bcNT (\u2126\u039bNT )\u2020]T (sign(\u2126\u039bc \u02c6x0) \u2299v)\u2225\u221e< 1\n(D.1)\nwhere v := |\u2126\u039bc \u02c6x0|/\u2225\u2126\u039bc \u02c6x0\u2225\u221e, i.e., v is obtained from \u2126\u039bc \u02c6x0 by taking\nelement-wise absolute values and normalizing it to a unit \u2113\u221e-norm, and \u2299de-\nnotes the element-wise multiplication of vectors. Condition (D.1) and (24) are\nin a similar form, but there are two di\ufb00erences between the two: First, for (D.1),\nthe signal \u02c6x0 that apears is not in general a vector with cosupport \u039b. It is rather\na signal that arises from an approximation. Second, there is a \u2018weight\u2019 vector\nv in (D.1). One can heuristically deduce that such a v favours condition (D.1)\nto hold true since the size of most entries of v likely be smaller than 1. Beside\nthese di\ufb00erences, one should keep in mind that condition (D.1) is only for one\nstep.\nReferences\n[1] M. Aharon, M. Elad, and A.M. Bruckstein.\nK-SVD: An algorithm for\ndesigning of overcomplete dictionaries for sparse representation.\nSignal\nProcessing, IEEE Transactions on, 54:4311 4322, 2006.\n[2] Zvika Ben-Haim, Yonina C. Eldar, and Michael Elad. Coherence-based\nperformance guarantees for estimating a sparse vector under random noise.\nIEEE Transactions on Signal Processing, 58(10):5030\u20135043, 2010.\n[3] Thomas Blumensath. Accelerated iterative hard thresholding. preprint,\n2011.\n38\n[4] Thomas Blumensath and Michael E. Davies. Sampling theorems for signals\nfrom the union of \ufb01nite-dimensional linear subspaces. IEEE Transactions\non Information Theory, 55(4):1872\u20131882, 2009.\n[5] A.M. Bruckstein, D.L. Donoho, and M. Elad.\nFrom sparse solutions of\nsystems of equations to sparse modeling of signals and images.\nSIAM\nReview, 51(1):34 \u201381, February 2009.\n[6] Jian-Feng Cai, Stanley Osher, and Zuowei Shen. Split bregman methods\nand frame based image restoration.\nMultiscale Modeling & Simulation,\n8(2):337\u2013369, 2009.\n[7] E. Cand`es and T. Tao. The Dantzig selector: statistical estimation when\np is much larger than n. Annals of Statistics, 35(6):2313\u20132351, 2007.\n[8] E. J. Cand`es, J. Romberg, and T. Tao.\nRobust uncertainty principles:\nexact signal reconstruction from highly incomplete frequency information.\nIEEE Trans. Info. Theory, 52(2):489\u2013509, February 2006.\n[9] Emmanuel J. Cand`es, Yonina C. Eldar, Deanna Needell, and Paige Randall.\nCompressed sensing with coherent and redundant dictionaries. Applied and\nComputational Harmonic Analysis, In Press, Corrected Proof:\u2013, 2010.\n[10] S.S. Chen, D.L. Donoho, and M.A. Saunders. Atomic decomposition by\nbasis pursuit. SIAM J. Sci. Comput., 20:33 \u201361, 1998.\n[11] Wei Dai and O. Milenkovic. Subspace pursuit for compressive sensing signal\nreconstruction. Information Theory, IEEE Transactions on, 55(5):2230 \u2013\n2249, may 2009.\n[12] Ingrid Daubechies, Ronald DeVore, Massimo Fornasier, and C. Sinan\nG\u00a8unt\u00a8urk. Iteratively reweighted least squares minimization for sparse re-\ncovery. Communications on Pure and Applied Mathematics, 63:1\u201338, 2010.\n[13] M.N. Do and M. Vetterli. The contourlet transform: An e\ufb03cient directional\nmultiresolution image representation. Image Processing, IEEE Transac-\ntions on, 14:2091 2106, 2005.\n[14] David Donoho and Jared Tanner. Counting faces of randomly-projected\npolytopes when the projection radically lowers dimension. Journal of the\nAMS, 22(1):1\u201353, January 2009.\n[15] David L. Donoho and Xiaoming Huo.\nUncertainty principles and ideal\natomic decomposition. IEEE Transactions on Information Theory, 2001.\n[16] D.L. Donoho and M. Elad.\nOptimally sparse representation in general\n(nonorthogonal) dictionaries via \u21131 minimization.\nProc. Nat. Aca. Sci.,\n100(5):2197\u20132202, March 2003.\n39\n[17] P.L. Dragotti and M. Vetterli. Wavelet footprints: theory, algorithms, and\napplications. Signal Processing, IEEE Transactions on, 51(5):1306 \u2013 1323,\nmay 2003.\n[18] M. Elad and M. Aharon. Image denoising via sparse and redundant repre-\nsentations over learned dictionaries. Image Processing, IEEE Transactions\non, 15:3736 3745, 2006.\n[19] M. Elad, P. Milanfar, and R. Rubinstein. Analysis versus synthesis in signal\npriors. Inverse Problems, 23(3):947\u2013968, June 2007.\n[20] M. Elad, J.-L. Starck, P. Querre, and D.L. Donoho.\nSimultaneous car-\ntoon and texture image inpainting using morphological component analysis\n(mca). Appl. Comput. Harmon. Anal., 19:340 358, 2005.\n[21] Michael Elad. Sparse and Redundant Representations - From Theory to\nApplications in Signal and Image Processing. Springer, 2010.\n[22] K. Engan, S.O. Aase, and J.H. Husoy. Multi-frame compression: Theory\nand design. Signal Processing, 80:2121 2140, 2000.\n[23] Irina F. Gorodnitsky and Bhaskar D. Rao. Sparse signal reconstruction\nfrom limited data using focuss: A re-weighted minimum norm algorithm.\nIEEE Trans. Signal Processing, pages 600\u2013616, 1997.\n[24] Michael Grant, Stephen Boyd, and Yinyu Ye. CVX: Matlab Software for\nDisciplined Convex Programming, August 2008.\n[25] R\u00b4emi Gribonval and Morten Nielsen. Sparse representations in unions of\nbases. IEEE Trans. Inform. Theory, 49(12):3320\u20133325, December 2003.\n[26] R\u00b4emi Gribonval and Morten Nielsen. Highly sparse representations from\ndictionaries are unique and independent of the sparseness measure. Applied\nand Computational Harmonic Analysis, 22(3):335\u2013355, May 2007.\n[27] Demetrio Labate, Wang-Q Lim, Gitta Kutyniok, and Guido Weiss. Sparse\nmultidimensional representation using shearlets.\nIn Wavelets XI (San\nDiego, CA, 2005), 254-262, SPIE Proc. 5914, pages 254\u2013262, Bellingham,\nWA, 2005.\n[28] Anna Llagostera Casanovas, Gianluca Monaci, Pierre Vandergheynst, and\nR\u00b4emi Gribonval. Blind audiovisual source separation based on sparse rep-\nresentations.\nIEEE Transactions on Multimedia, 12(5):358\u2013371, August\n2010.\n[29] Y.M. Lu and M.N. Do.\nA theory for sampling signals from a union of\nsubspaces.\nSignal Processing, IEEE Transactions on, 56(6):2334 \u20132345,\nJune 2008.\n40\n[30] Julien Mairal, Francis Bach, Jean Ponce, and Guillermo Sapiro. Online\nlearning for matrix factorization and sparse coding. J. Mach. Learn. Res.,\n11:19\u201360, March 2010.\n[31] S. Mallat.\nZero-crossings of a wavelet transform.\nInformation Theory,\nIEEE Transactions on, 37(4):1019 \u20131033, jul 1991.\n[32] S. Mallat and Z. Zhang. Matching pursuits with time-frequency dictionar-\nies. Signal Processing, IEEE Transactions on, 41:3397 3415, 1993.\n[33] St\u00b4ephane Mallat. A Wavelet Tour of Signal Processing, Third Edition: The\nSparse Way. Academic Press, 3rd edition, 2008.\n[34] Mark D. Plumbley, Thomas Blumensath, Laurent Daudet, R\u00b4emi Gribonval,\nand M. E. Davies. Sparse representations in audio and music: from coding\nto source separation. Proceedings of the IEEE, 98(6):995\u20131005, June 2010.\n[35] Javier Portilla.\nImage restoration through l0 analysis-based sparse op-\ntimization in tight frames.\nProceedings of the 16th IEEE International\nConference on Image Processing, pages 3865\u20133868, 2009.\n[36] L. Rudin, Stanley Osher, and E. Fatemi. Nonlinear total variation based\nnoise removal algorithms. Physica D, 60:259\u2013268, 1992.\n[37] I. W. Selesnick and M. A. T. Figueiredo. Signal restoration with overcom-\nplete wavelet transforms: Comparison of analysis and synthesis priors. In\nProceedings of SPIE, 7446 (Wavelets XIII), August 2009.\n[38] Karl Skretting and Kjersti Engan. Recursive least squares dictionary learn-\ning algorithm. IEEE Transactions on Signal Processing, 58(4):2121\u20132130,\n2010.\n[39] J.-L. Starck, E.J. Cand`es, and D.L. Donoho. The curvelet transform for\nimage denoising. Image Processing, IEEE Transactions on, 11(11):670 684,\nNovember 2002.\n[40] Jean-Luc Starck, Fionn Murtagh, and Mohamed-Jalal Fadili. Sparse Im-\nage and Signal Processing - Wavelets, Curvelets, Morphological Diversity.\nCambridge University Press, 2010.\n[41] Joel A. Tropp. Greed is good: Algorithmic results for sparse approximation.\nIEEE Trans. Inform. Theory, 50:2231\u20132242, 2004.\n[42] Joel A. Tropp. Just relax: Convex programming methods for subset selec-\ntion and sparse approximation. IEEE Transactions on Information Theory,\n51:1030\u20131051, 2006.\n41\n",
        "sentence": " Sparse representation models [1], [2], [3], [4] and related machine learning algorithms [5], [6] have proven remarkably successful at extracting useful information from complex highdimensional signals, for example in the context of denoising [7], under-determined source separation [8], [9], compressed sensing [10], super-resolution sensing [11], and classification [12].",
        "context": "Empirically, analysis models have been successfully used for a variety of signal\nprocessing tasks such as denoising, deblurring, and most recently compressed\nsensing, but this has been done with little theoretical justi\ufb01cation.\nPrevious work using analysis representations, both theoretical and algorith-\nmic, has focussed on gauging performance in terms of the more traditional\nsparsity perspective. For example, in the context of compressed sensing, recent\napplications. Signal Processing, IEEE Transactions on, 51(5):1306 \u2013 1323,\nmay 2003.\n[18] M. Elad and M. Aharon. Image denoising via sparse and redundant repre-\nsentations over learned dictionaries. Image Processing, IEEE Transactions"
    },
    {
        "title": "Dictionaries for sparse representation modeling",
        "author": [
            "R. Rubinstein",
            "A. Bruckstein",
            "M. Elad"
        ],
        "venue": "Proceedings of the IEEE, vol. 98, no. 6, pp. 1045\u20131057, June 2010.",
        "citeRegEx": "5",
        "shortCiteRegEx": null,
        "year": 2010,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " Sparse representation models [1], [2], [3], [4] and related machine learning algorithms [5], [6] have proven remarkably successful at extracting useful information from complex highdimensional signals, for example in the context of denoising [7], under-determined source separation [8], [9], compressed sensing [10], super-resolution sensing [11], and classification [12]. Since that time several probabilistic dictionary learning and sparse coding methods have been developed [2], [5], aiming for a dictionary that either maximizes the",
        "context": null
    },
    {
        "title": "Dictionary learning",
        "author": [
            "I. Tosic",
            "P. Frossard"
        ],
        "venue": "Signal Processing Magazine, IEEE, vol. 28, no. 2, pp. 27\u201338, March 2011.",
        "citeRegEx": "6",
        "shortCiteRegEx": null,
        "year": 2011,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " Sparse representation models [1], [2], [3], [4] and related machine learning algorithms [5], [6] have proven remarkably successful at extracting useful information from complex highdimensional signals, for example in the context of denoising [7], under-determined source separation [8], [9], compressed sensing [10], super-resolution sensing [11], and classification [12].",
        "context": null
    },
    {
        "title": "Gradient pursuits",
        "author": [
            "T. Blumensath",
            "M. Davies"
        ],
        "venue": "IEEE Transactions on Signal Processing, vol. 56, no. 6, pp. 2370\u20132382, 2008.",
        "citeRegEx": "7",
        "shortCiteRegEx": null,
        "year": 2008,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " Sparse representation models [1], [2], [3], [4] and related machine learning algorithms [5], [6] have proven remarkably successful at extracting useful information from complex highdimensional signals, for example in the context of denoising [7], under-determined source separation [8], [9], compressed sensing [10], super-resolution sensing [11], and classification [12].",
        "context": null
    },
    {
        "title": "Blind source separation by sparse decomposition in a signal dictionary",
        "author": [
            "M. Zibulevsky",
            "B.A. Pearlmutter"
        ],
        "venue": "Neural Computation, vol. 13, no. 4, pp. 863\u2013882, 2016/02/04 2001. [Online]. Available: http://dx.doi.org/10.1162/089976601300014385",
        "citeRegEx": "8",
        "shortCiteRegEx": null,
        "year": 2016,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " Sparse representation models [1], [2], [3], [4] and related machine learning algorithms [5], [6] have proven remarkably successful at extracting useful information from complex highdimensional signals, for example in the context of denoising [7], under-determined source separation [8], [9], compressed sensing [10], super-resolution sensing [11], and classification [12].",
        "context": null
    },
    {
        "title": "Underdetermined blind source separation using sparse representations",
        "author": [
            "P. Bofill",
            "M. Zibulevsky"
        ],
        "venue": "Signal Processing, vol. 81, no. 11, pp. 2353 \u2013 2362, 2001. [Online]. Available: http://www.sciencedirect.com/ science/article/pii/S0165168401001207",
        "citeRegEx": "9",
        "shortCiteRegEx": null,
        "year": 2001,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " Sparse representation models [1], [2], [3], [4] and related machine learning algorithms [5], [6] have proven remarkably successful at extracting useful information from complex highdimensional signals, for example in the context of denoising [7], under-determined source separation [8], [9], compressed sensing [10], super-resolution sensing [11], and classification [12].",
        "context": null
    },
    {
        "title": "Compressed sensing",
        "author": [
            "D.L. Donoho"
        ],
        "venue": "IEEE Trans. Inform. Theory, vol. 52, pp. 1289\u20131306, 2006.",
        "citeRegEx": "10",
        "shortCiteRegEx": null,
        "year": 2006,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " Sparse representation models [1], [2], [3], [4] and related machine learning algorithms [5], [6] have proven remarkably successful at extracting useful information from complex highdimensional signals, for example in the context of denoising [7], under-determined source separation [8], [9], compressed sensing [10], super-resolution sensing [11], and classification [12].",
        "context": null
    },
    {
        "title": "Towards a mathematical theory of super-resolution",
        "author": [
            "E.J. Cands",
            "C. Fernandez-Granda"
        ],
        "venue": "Communications on Pure and Applied Mathematics, vol. 67, no. 6, pp. 906\u2013956, 2014. [Online]. Available: http://dx.doi.org/10.1002/cpa.21455",
        "citeRegEx": "11",
        "shortCiteRegEx": null,
        "year": 2014,
        "abstract": "",
        "full_text": "",
        "sentence": " Sparse representation models [1], [2], [3], [4] and related machine learning algorithms [5], [6] have proven remarkably successful at extracting useful information from complex highdimensional signals, for example in the context of denoising [7], under-determined source separation [8], [9], compressed sensing [10], super-resolution sensing [11], and classification [12].",
        "context": null
    },
    {
        "title": "Supervised dictionary learning",
        "author": [
            "J. Mairal",
            "J. Ponce",
            "G. Sapiro",
            "A. Zisserman",
            "F.R. Bach"
        ],
        "venue": "Advances in Neural Information Processing Systems 21, D. Koller, D. Schuurmans, Y. Bengio, and L. Bottou, Eds. Curran Associates, Inc., 2009, pp. 1033\u20131040. [Online]. Available: http://papers.nips.cc/paper/3448-supervised-dictionary-learning.pdf",
        "citeRegEx": "12",
        "shortCiteRegEx": null,
        "year": 2009,
        "abstract": "It is now well established that sparse signal models are well suited to\nrestoration tasks and can effectively be learned from audio, image, and video\ndata. Recent research has been aimed at learning discriminative sparse models\ninstead of purely reconstructive ones. This paper proposes a new step in that\ndirection, with a novel sparse representation for signals belonging to\ndifferent classes in terms of a shared dictionary and multiple class-decision\nfunctions. The linear variant of the proposed model admits a simple\nprobabilistic interpretation, while its most general variant admits an\ninterpretation in terms of kernels. An optimization framework for learning all\nthe components of the proposed model is presented, along with experimental\nresults on standard handwritten digit and texture classification tasks.",
        "full_text": "arXiv:0809.3083v1  [cs.CV]  18 Sep 2008\napport \r\n\r\nde recherche\r\nISSN 0249-6399\nISRN INRIA/RR--6652--FR+ENG\nTh\u00e8me COG\nINSTITUT NATIONAL DE RECHERCHE EN INFORMATIQUE ET EN AUTOMATIQUE\nSupervised Dictionary Learning\nJulien Mairal \u2014 Francis Bach \u2014 Jean Ponce \u2014 Guillermo Sapiro \u2014 Andrew Zisserman\nN\u00b0 6652\nSeptember 2008\nCentre de recherche INRIA Paris \u2013 Rocquencourt\nDomaine de Voluceau, Rocquencourt, BP 105, 78153 Le Chesnay Cedex\nT\u00e9l\u00e9phone : +33 1 39 63 55 11 \u2014 T\u00e9l\u00e9copie : +33 1 39 63 53 30\nSupervised Dictionary Learning\nJulien Mairal\u2217\u2020, Francis Bach\u2217\u2020 , Jean Ponce\u2021\u2020 , Guillermo Sapiro\u00a7\n, Andrew Zisserman\u00b6\u2020\nTh`eme COG \u2014 Syst`emes cognitifs\n\u00b4Equipes-Projets Willow\nRapport de recherche n\u00b0 6652 \u2014 September 2008 \u2014 15 pages\nAbstract:\nIt is now well established that sparse signal models are well suited\nto restoration tasks and can e\ufb00ectively be learned from audio, image, and video\ndata. Recent research has been aimed at learning discriminative sparse models\ninstead of purely reconstructive ones. This paper proposes a new step in that\ndirection, with a novel sparse representation for signals belonging to di\ufb00erent\nclasses in terms of a shared dictionary and multiple class-decision functions. The\nlinear variant of the proposed model admits a simple probabilistic interpretation,\nwhile its most general variant admits an interpretation in terms of kernels. An\noptimization framework for learning all the components of the proposed model\nis presented, along with experimental results on standard handwritten digit and\ntexture classi\ufb01cation tasks.\nKey-words:\nsparsity, classi\ufb01cation\n\u2217INRIA\n\u2020 WILLOW project-team, Laboratoire d\u2019Informatique de l\u2019Ecole Normale Sup\u00b4erieure,\nENS/INRIA/CNRS UMR 8548\n\u2021 Ecole Normale Sup\u00b4erieure\n\u00a7 University of Minnesota, Department of Electrical Engineering\n\u00b6 University of Oxford\nApprentissage de dictionnaires supervis\u00b4e\nR\u00b4esum\u00b4e : Il est maintenant bien \u00b4etabli que les repr\u00b4esentations parcimonieuses\nde signaux sont bien adapt\u00b4ees `a des taches de restauration d\u2019image, de sons ou de\nvideo. De recherches r\u00b4ecentes ont eu pour but d\u2019apprendre des repr\u00b4esentations\ndiscriminantes au lieu de seulement reconstructives.\nCe travail propose un\nnouveau cadre pour repr\u00b4esenter des signaux appartenant `a plusieurs classes\ndi\ufb00\u00b4erentes, en apprenant de fa\u00b8con simultan\u00b4ee un dictionnaire partag\u00b4e et de\nmultiples fonctions de d\u00b4ecision. On montre que la variante lin\u00b4eaire de ce cadre\nadmet une interpr\u00b4etation probabilistique simple, tandis que la version plus\ng\u00b4en\u00b4erale peut s\u2019interpr\u00b4eter en terme de noyaux. Nous proposons une m\u00b4ethode\nd\u2019optimisation e\ufb03cace et nous \u00b4evaluons le mod`ele sur un probl`eme de recon-\nnaissance de chi\ufb00res manuscrits et de classi\ufb01cation de textures.\nMots-cl\u00b4es :\nparcimonie, sparsit\u00b4e, classi\ufb01cation\nSupervised Dictionary Learning\n3\n1\nIntroduction\nSparse and overcomplete image models were \ufb01rst introduced in [13] for modeling\nthe spatial receptive \ufb01elds of simple cells in the human visual system. The linear\ndecomposition of a signal using a few atoms of a learned dictionary, instead of\nprede\ufb01ned ones\u2013such as wavelets\u2013has recently led to state-of-the-art results for\nnumerous low-level image processing tasks such as denoising [5], showing that\nsparse models are well adapted to natural images. Unlike principal component\nanalysis decompositions, these models are most ofen overcomplete, with a num-\nber of basis elements greater than the dimension of the data. Recent research\nhas shown that sparsity helps to capture higher-order correlation in data: In\n[9, 21], sparse decompositions are used with prede\ufb01ned dictionaries for face and\nsignal recognition. In [14], dictionaries are learned for a reconstruction task,\nand the sparse decompositions are then used a posteriori within a classi\ufb01er.\nIn [12], a discriminative method is introduced for various classi\ufb01cation tasks,\nlearning one dictionary per class; the classi\ufb01cation process itself is based on the\ncorresponding reconstruction error, and does not exploit the actual decompo-\nsition coe\ufb03cients. In [17], a generative model for document representation is\nlearned at the same time as the parameters of a deep network structure. The\nframework we present in this paper extends these approaches by learning si-\nmultaneously a single shared dictionary as well as multiple decision functions\nfor di\ufb00erent signal classes in a mixed generative and discriminative formulation\n(see also [18], where a di\ufb00erent discrimination term is added to the classical\nreconstructive one for supervised dictionary learning via class supervised simul-\ntaneous orthogonal matching pursuit).. Similar joint generative/discriminative\nframeworks have started to appear in probabilistic approaches to learning, e.g.,\n[2, 8, 10, 15, 19, 20], but not, to the best of our knowledge, in the sparse dictio-\nnary learning framework. Section 2 presents the formulation and Section 3 its\ninterpretation in term of probability and kernel frameworks. The optimization\nprocedure is detailed in Section 4, and experimental results are presented in\nSection 5.\n2\nSupervised dictionary learning\nWe present in this section the core of the proposed model. We start by describ-\ning how to perform sparse coding in a supervised fashion, then show how to\nsimultaneously learn a discriminative/reconstructive dictionary and a classi\ufb01er.\n2.1\nSupervised Sparse Coding\nIn classical sparse coding tasks, one considers a signal x in Rn and a \ufb01xed\ndictionary D = [d1, . . . , dk] in Rn\u00d7k (allowing k > n, making the dictionary\novercomplete). In this setting, sparse coding with an \u21131 regularization1 amounts\nto computing\nR\u22c6(x, D) = min\n\u03b1\u2208Rk ||x \u2212D\u03b1||2\n2 + \u03bb1||\u03b1||1.\n(1)\n1The \u2113p regularization term of a vector x for p \u22650 is de\ufb01ned as ||x||p\np = (Pn\ni=1 |x[i]|p).\n||.||p is a norm when p \u22651. When p = 0, it counts the number of non-zeros elements in the\nvector.\nRR n\u00b0 6652\n4\nMairal, Bach, Ponce, Sapiro & Zisserman\nIt is well known in the statistics, optimization, and compressed sensing commu-\nnities that the \u21131 penalty yields a sparse solution, very few non-zero coe\ufb03cients\nin \u03b1,\n[3], although there is no explicit analytic link between the value of \u03bb1\nand the e\ufb00ective sparsity that this model yields. Other sparsity penalties using\nthe \u21130 (or more generally \u2113p) regularization can be used as well. Since it uses\na proper norm, the \u21131 formulation of sparse coding is a convex problem, which\nmakes the optimization tractable with algorithms such as those introduced in\n[4, 7], and has proven in our proposed framework to be more stable than its\n\u21130 counterpart, in the sense that the resulting decompositions are less sensi-\ntive to small perturbations of the input signal x. Note that sparse coding with\nan \u21130 penalty is an NP-hard problem and is often approximated using greedy\nalgorithms.\nIn this paper, we consider a di\ufb00erent setting, where the signal may belong to\nany of p di\ufb00erent classes. We model the signal x using a single shared dictionary\nD and a set of p decision functions gi(x, \u03b1, \u03b8) (i = 1, . . . , p) acting on x and\nits sparse code \u03b1 over D. The function gi should be positive for any signal in\nclass i and negative otherwise. The vector \u03b8 parametrizes the model and will be\njointly learned with D. In the following, we will consider two kinds of decision\nfunctions:\n(i) linear in \u03b1: gi(x, \u03b1, \u03b8) = wT\ni \u03b1 + bi, where \u03b8 = {wi \u2208Rk, bi \u2208R}p\ni=1,\nand the vectors wi (i = 1, . . . , p) can be thought of as p linear models for the\ncoe\ufb03cients \u03b1, with the scalars bi acting as biases;\n(ii) bilinear in x and \u03b1: gi(x, \u03b1, \u03b8) = xT Wi\u03b1 + bi, where \u03b8 = {Wi \u2208\nRn\u00d7k, bi \u2208R}p\ni=1. Note that the number of parameters in (ii) is greater than in\n(i), which allows for richer models. One can interpret Wi as a \ufb01lter encoding\nthe input signal x into a model for the coe\ufb03cients \u03b1, which has a role similar\nto the encoder in [16] but for a discriminative task.\nLet us de\ufb01ne softmax discriminative cost functions as\nCi(x1, ..., xp) = log(\np\nX\nj=1\nexj\u2212xi)\nfor i = 1, . . . , p. These are multiclass versions of the logistic function, enjoying\nproperties similar to that of the hinge loss from the SVM literature, while being\ndi\ufb00erentiable. Given some input signal x and \ufb01xed (for now) dictionary D and\nparameters \u03b8, the supervised sparse coding problem for the class p can be de\ufb01ned\nas computing\nS\u22c6\ni (x, D, \u03b8) = min\n\u03b1 Si(\u03b1, x, D, \u03b8),\n(2)\nwhere\nSi(\u03b1, x, D, \u03b8) = Ci({gj(x, \u03b1, \u03b8)}p\nj=1) + \u03bb0||x \u2212D\u03b1||2\n2 + \u03bb1||\u03b1||1.\n(3)\nNote the explicit incorporation of the classi\ufb01cation and discriminative compo-\nnent into sparse coding, in addition to the classical reconstructive term (see [18]\nfor a di\ufb00erent classi\ufb01caiton component). In turn, any solution to this problem\nprovides a straightforward classi\ufb01cation procedure, namely:\ni\u22c6(x, D, \u03b8) = arg min\ni=1,...,p\nS\u22c6\ni (x, D, \u03b8).\n(4)\nCompared with earlier work using one dictionary per class [12], this model\nhas the advantage of letting multiple classes share some features, and uses the\nINRIA\nSupervised Dictionary Learning\n5\ncoe\ufb03cients \u03b1 of the sparse representations as part of the classi\ufb01cation procedure,\nthereby following the works from [9, 14, 21], but with learned representations\noptimized for the classi\ufb01cation task similar to [2, 18].\nAs shown in Section\n3, this formulation has a straightforward probabilistic interpretation, but let us\n\ufb01rst see how to learn the dictionary D and the parameters \u03b8 from training data.\n2.2\nSDL: Supervised Dictionary Learning\nLet us assume that we are given p sets of training data Ti, i = 1, . . . , p, such\nthat all samples in Ti belong to class i. The most direct method for learning D\nand \u03b8 is to minimize with respect to these variables the mean value of S\u22c6\ni , with\nan \u21132 regularization term to prevent over\ufb01tting:\nmin\nD,\u03b8\n\u0010\np\nX\ni=1\nX\nj\u2208Ti\nS\u22c6\ni (xj, D, \u03b8)\n\u0011\n+ \u03bb2||\u03b8||2\n2,\ns.t. \u2200i = 1, . . . , k,\n||di||2 \u22641.\n(5)\nSince the reconstruction errors ||x \u2212D\u03b1||2\n2 are invariant to scaling simultane-\nously D by a scalar and \u03b1 by its inverse, constraining the \u21132 norm of columns\nof D prevents any transfer of energy between these two variables, which would\nhave the e\ufb00ect of overcoming the sparsity penalty. Such a constraint is classical\nin sparse coding [5]. We will refer later to this model as SDL-G (supervised\ndictionary learning, generative).\nNevertheless, since the classi\ufb01cation procedure from Eq. (4) will compare the\ndi\ufb00erent residuals S\u22c6\ni of a given signal for i = 1, . . . , p, a more discriminative\napproach is to not only make the S\u22c6\ni small for signals with label i, as in (5), but\nalso make the value of S\u22c6\nj greater than S\u22c6\ni for j di\ufb00erent than i, which is the\npurpose of the softmax function Ci. This leads to:\nmin\nD,\u03b8\n\u0010\np\nX\ni=1\nX\nj\u2208Ti\nCi({S\u22c6\nl (xj, D, \u03b8)}p\nl=1)\n\u0011\n+ \u03bb2||\u03b8||2\n2 s.t. \u2200i = 1, . . . , k, ||di||2 \u22641.\n(6)\nAs detailed below, this problem is more di\ufb03cult to solve than Eq. (5), and\ntherefore we adopt instead a mixed formulation between the minimization of\nthe generative Eq. (5) and its discriminative version (6), [15]\u2014that is,\nmin\nD,\u03b8\n\u0010\np\nX\ni=1\nX\nj\u2208Ti\n\u00b5Ci({S\u22c6\nl (xj, D, \u03b8)}p\nl=1) + (1 \u2212\u00b5)S\u22c6\ni (xj, D, \u03b8)\n\u0011\n+ \u03bb2||\u03b8||2\n2\ns.t. \u2200i,\n||di||2 \u22641,\n(7)\nwhere \u00b5 controls the trade-o\ufb00between reconstruction from Eq. (5) and discrim-\nination from Eq. (6). This is the proposed generative/discriminative model for\nsparse signal representation and classi\ufb01cation from learned dictionary D and\nmodel \u03b8. We will refer to this mixed model as SDL-D, (supervised dictionary\nlearning, discriminative).\nBefore presenting the proposed optimization procedure, we provide below\ntwo interpretations of the linear and bilinear versions of our formulation in\nterms of a probabilistic graphical model and a kernel.\nRR n\u00b0 6652\n6\nMairal, Bach, Ponce, Sapiro & Zisserman\nW\nD\nxj\n\u03b1j\nyj\nj = 1, . . ., m\nFigure 1: Graphical model for the proposed generative/discriminative learning\nframework.\n3\nInterpreting the model\n3.1\nA probabilistic interpretation of the linear model\nLet us \ufb01rst construct a graphical model which gives a probabilistic interpretation\nto the training and classi\ufb01cation criteria given above when using a linear model\nwith zero bias (no constant term) on the coe\ufb03cients\u2014that is, gi(x, \u03b1, \u03b8) =\nwT\ni \u03b1. This model consists of the following components (Figure 1):\n\u2022 The matrices D and W are parameters of the problem, with a Gaussian prior\non W, p(W) \u221de\u2212\u03bb2||W||2\n2, and on the columns of D, p(D) \u221dQk\nl=1 e\u2212\u03b3l||dl||2\n2,\nwhere the \u03b3l\u2019s are the Gaussian parameters. All the dl\u2019s are considered inde-\npendent of each other.\n\u2022 The coe\ufb03cients \u03b1j are latent variables with a Laplace prior, p(\u03b1j) \u221de\u2212\u03bb1||\u03b1j||1.\n\u2022 The signals xj are generated according to a Gaussian probability distribution\nconditioned on D and \u03b1j, p(xj|\u03b1j, D) \u221de\u2212\u03bb0||xj\u2212D\u03b1j||2\n2. All the xj\u2019s are con-\nsidered independent from each other.\n\u2022 The labels yj are generated according to a probability distribution conditioned\non W and \u03b1j, and given by p(yj = i|\u03b1j, W) = e\u2212WT\ni \u03b1j/Pp\nl=1 e\u2212WT\nl \u03b1j. Given\nD and W, all the triplets (\u03b1j, xj, yj) are independent.\nWhat is commonly called \u201cgenerative training\u201d in the literature (e.g., [10,\n15]), amounts to \ufb01nding the maximum likelihood for D and W according to\nthe joint distribution p({xj, yj}m\nj=1, D, W), where the xj\u2019s and the yj\u2019s are re-\nspectively the training signals and their labels. It can easily be shown (details\nomitted due to space limitations) that there is an equivalence between this gen-\nerative training and our formulation in Eq. (5) under MAP approximations.2\nAlthough joint generative modeling of x and y through a shared representa-\ntion, e.g., [2], has shown great promise, we show in this paper that a more\ndiscriminative approach is desirable. \u201cDiscriminative training\u201d is slightly dif-\nferent and amounts to maximizing p({yj}m\nj=1, D, W|{xj}m\nj=1) with respect to\nD and W: Given some input data, one \ufb01nds the best parameters that will\npredict the labels of the data. The same kind of MAP approximation relates\nthis discriminative training formulation to the discriminative model of Eq. (6)\n2We are also investigating how to properly estimate D by marginalizing over \u03b1 instead of\nmaximizing with respect to that parameter.\nINRIA\nSupervised Dictionary Learning\n7\n(again, details omitted due to space limitations). The mixed approach from Eq.\n(7) is a classical trade-o\ufb00between generative and discriminative (e.g., [10, 15]),\nwhere generative components are often added to discriminative frameworks to\nadd robustness, e.g., to noise and occlusions (see examples of this for the model\nin [18]).\n3.2\nA kernel interpretation of the bilinear model\nOur bilinear model with gi(x, \u03b1, \u03b8) = xT Wi\u03b1 + bi does not admit a straight-\nforward probabilistic interpretation. On the other hand, it can easily be inter-\npreted in terms of kernels: Given two signals x1 and x2, with coe\ufb03cients \u03b11 and\n\u03b12, using the kernel K(x1, x2) = \u03b1T\n1 \u03b12xT\n1 x2 in a logistic regression classi\ufb01er\namounts to \ufb01nding a decision function of the same form as (ii). It is a product of\ntwo linear kernels, one on the \u03b1\u2019s and one on the input signals x. Interestingly,\nRaina et al. [14] learn a dictionary adapted to reconstruction on a training set,\nthen train an SVM a posteriori on the decomposition coe\ufb03cients \u03b1. They derive\nand use a Fisher kernel, which can be written as K\u2032(x1, x2) = \u03b1T\n1 \u03b12rT\n1 r2 in this\nsetting, where the r\u2019s are the residuals of the decompositions. Experimentally,\nwe have observed that the kernel K, where the signals x replace the residuals\nr, generally yields a level of performance similar to K\u2032, and often actually does\nbetter when the number of training samples is small or the data are noisy.\n4\nOptimization procedure\nClassical dictionary learning techniques (e.g., [1, 13, 14]), address the problem\nof learning a reconstructive dictionary D in Rn\u00d7k well adapted to a training set\nT as\nmin\nD,\u03b1\nX\nj\u2208T\n||xj \u2212D\u03b1j||2\n2 + \u03bb1||\u03b1j||1,\n(8)\nwhich is not jointly convex in (D, \u03b1), but convex with respect to each unknown\nwhen the other one is \ufb01xed. This is why block coordinate descent on D and\n\u03b1 performs reasonably well [1, 13, 14], although not necessarily providing the\nglobal optimum. Training when \u00b5 = 0 (generative case), i.e., from Eq. (5),\nenjoys similar properties and can be addressed with the same optimization pro-\ncedure. Equation (5) can be rewritten as:\nmin\nD,\u03b8,\u03b1\n\u0010\np\nX\ni=1\nX\nj\u2208Ti\nSi(xj, \u03b1j, D, \u03b8)\n\u0011\n+ \u03bb2||\u03b8||2\n2,\ns.t. \u2200i = 1, . . . , k,\n||di||2 \u22641.\n(9)\nBlock coordinate descent consists therefore of iterating between supervised sparse\ncoding, where D and \u03b8 are \ufb01xed and one optimizes with respect to the \u03b1\u2019s and\nsupervised dictionary update, where the coe\ufb03cients \u03b1j\u2019s are \ufb01xed, but D and \u03b8\nare updated. Details on how to solve these two problems are given in Section\n4.1 and 4.2.\nThe discriminative version of SDL from Eq. (6) is more problematic. The\nminimization of the term Ci({Sl(\u03b1jl, xj, D, \u03b8)}p\nl=1) with respect to D and \u03b8\nwhen the \u03b1jl\u2019s are \ufb01xed, is not convex in general, and does not necessarily\ndecrease the \ufb01rst term of Eq. (6), i.e., Ci({S\u22c6\nl (xj, D, \u03b8)}p\nl=1). To reach a lo-\ncal minimum for this di\ufb03cult problem, we have chosen a continuation method,\nRR n\u00b0 6652\n8\nMairal, Bach, Ponce, Sapiro & Zisserman\nInput: p (number of classes); n (signal dimensions); {Ti}p\ni=1 (training sig-\nnals); k (size of the dictionary); \u03bb0, \u03bb1, \u03bb2 (parameters); 0 \u2264\u00b51 \u2264\u00b52 \u2264. . . \u2264\n\u00b5m \u22641 (increasing sequence);\nOutput: D \u2208Rn\u00d7k (dictionary); \u03b8 (parameters).\nInitialization: Set D to a random Gaussian matrix. Set \u03b8 to zero.\nLoop: For \u00b5 = \u00b51, . . . , \u00b5m,\nLoop: Repeat until convergence (or a \ufb01xed number of iterations),\n\u2022 Supervised sparse coding: Compute, for all i = 1, . . . , p, all j in Ti, and\nall l = 1, . . . , p,\n\u03b1\u22c6\njl = arg min\n\u03b1\u2208Rk Sl(\u03b1, xj, D, \u03b8).\n(10)\n\u2022 Dictionary update: Solve, under the constraint ||dl|| \u22641 for all l =\n1, . . . , k\nmin\nD,\u03b8\n\u0010\np\nX\ni=1\nX\nj\u2208Ti\n\u00b5Ci({Sl(\u03b1\u22c6\njl, xj, D, \u03b8)}p\nl=1)+(1\u2212\u00b5)Si(\u03b1\u22c6\nji, xj, D, \u03b8)\n\u0011\n+\u03bb2||\u03b8||2\n2.\n(11)\nFigure 2: SDL: Supervised dictionary learning algorithm.\nstarting from the generative case and ending with the discriminative one as in\n[12]. The algorithm is presented on Figure 2, and details on the hyperparame-\nters\u2019 settings are given in Section 5.\n4.1\nSupervised sparse coding\nThe supervised sparse coding problem from Eq. (10) (D and \u03b8 are \ufb01xed in\nthis step), amounts to minimizing a convex function under an \u21131 penalty. The\n\ufb01xed-point continuation method (FPC) from [7] achieves state-of-the-art results\nin terms of convergence speed for this class of problems. It has proven in our\nexperiments to be simple, e\ufb03cient, and well adapted to our supervised sparse\ncoding problem. Algorithmic details are given in [7]. For our speci\ufb01c problem,\ndenoting by f the convex function to minimize, this method only requires \u2207f\nand a bound on the spectral norm of its Hessian Hf. Since the we have chosen\ndecision functions gi in Eq. (10) which are linear in \u03b1, there exists, for each\nsignal x to be sparsely represented, a matrix A in Rk\u00d7p and a vector b in Rp\nsuch that\n(\nf(\u03b1) =\nCi(AT \u03b1 + b) + \u03bb0||x \u2212D\u03b1||2\n2,\n\u2207f(\u03b1) =\nA\u2207Ci(AT \u03b1 + b) \u22122\u03bb0DT (x \u2212D\u03b1),\nand it can be shown that, if ||U||2 denotes the spectral norm of a matrix\nU (which is the magnitude of its largest eigenvalue), then ||Hf||2 \u2264(1 \u2212\n1\np)||AT A||2\n2+2\u03bb0||DT D||2. In the case where p = 2 (only two classes), we can ob-\ntain a tighter bound, ||Hf(\u03b1)||2 \u2264e\u2212C1(AT\u03b1)\u2212C2(AT\u03b1)||a2\u2212a1||2\n2+2\u03bb0||DT D||2,\nwhere a1 and a2 are the \ufb01rst and second columns of A.\nINRIA\nSupervised Dictionary Learning\n9\n4.2\nDictionary update\nThe problem of updating D and \u03b8 in Eq. (11) is not convex in general (except\nwhen \u00b5 is close to 0), but a local minimum can be obtained using projected\ngradient descent (as in the general literature on dictionary learning, this local\nminimum has experimentally been found to be good enough for our formulation).\nDenoting E(D, \u03b8) the function we want to minimize in Eq. (11), we just need\nthe partial derivatives of E with respect to D and the parameters \u03b8. Details\nwhen using the linear model for the \u03b1\u2019s, gi(x, \u03b1, \u03b8) = wT\ni \u03b1+bi, and \u03b8 = {W \u2208\nRk\u00d7p, b \u2208Rp}, are\n\uf8f1\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f2\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f3\n\u2202E\n\u2202D =\n\u22122\u03bb0\n\u0000p\nX\ni=1\nX\nj\u2208Ti\np\nX\nl=1\n\u03c9jl(xj \u2212D\u03b1\u22c6\njl)\u03b1\u22c6T\njl\n\u0001\n,\n\u2202E\n\u2202W =\np\nX\ni=1\nX\nj\u2208Ti\np\nX\nl=1\n\u03c9jl\u03b1\u22c6\njl\u2207CT\nl (WT \u03b1\u22c6\njl + b),\n\u2202E\n\u2202b =\np\nX\ni=1\nX\nj\u2208Ti\np\nX\nl=1\n\u03c9jl\u2207Cl(WT \u03b1\u22c6\njl + b),\n(12)\nwhere\n\u03c9jl = \u00b5\u2207Ci({Sm(\u03b1\u22c6\njm, xj, D, \u03b8)}p\nm=1)[l] + (1 \u2212\u00b5)1l=i.\n(13)\nPartial derivatives when using our model with the bilinear decision functions\ngi(x, \u03b1, \u03b8) = xT Wi\u03b1 + bi are not given in this paper because of space limita-\ntions.\n5\nExperimental validation\nWe compare in this section a reconstructive approach, dubbed REC, which con-\nsists of learning a reconstructive dictionary D as in [14] and then learning the\nparameters \u03b8 a posteriori; SDL with generative training (dubbed SDL-G); and\nSDL with discriminative learning (dubbed SDL-D). We also compare the per-\nformance of the linear (L) and bilinear (BL) decision functions.\nBefore presenting experimental results, let us brie\ufb02y discuss the choice of the\n\ufb01ve model parameters \u03bb0, \u03bb1, \u03bb2, \u00b5 and k (size of the dictionary). Tuning all of\nthem using cross-validation is cumbersome and unnecessary since some simple\nchoices can be made, some of which can be done sequentially. We de\ufb01ne \ufb01rst\nthe sparsity parameter \u03ba = \u03bb1\n\u03bb0 , which dictates how sparse the decompositions\nare. When the input data points have unit \u21132 norm, choosing \u03ba = 0.15 was\nempirically found to be a good choice. The number of parameters to learn is\nlinear in k, the number of elements in the dictionary D. For reconstructive tasks,\nk = 256 is a typical value often used in the literature (e.g., [1]). Nevertheless,\nfor discriminative tasks, increasing the number of parameters is likely to allow\nover\ufb01tting, and smaller values like k = 64 or k = 32 are preferred. The scalar \u03bb2\nis a regularization parameter for preventing the model to over\ufb01t the input data.\nAs in logistic regression or support vector machines, this parameter is crucial\nwhen the number of training samples is small. Performing cross validation with\nthe fast method REC quickly provides a reasonable value for this parameter,\nwhich can be used afterward for SDL-G or SDL-D.\nRR n\u00b0 6652\n10\nMairal, Bach, Ponce, Sapiro & Zisserman\nOnce \u03ba, k and \u03bb2 are chosen, let us see how to \ufb01nd \u03bb0. In logistic regression,\na projection matrix maps input data onto a softmax function, and its shape and\nscale are adapted so that it becomes discriminative according to an underlying\nprobabilistic model. In the model we are proposing, the functions S\u22c6\ni are also\nmapped onto a softmax function, and the parameters D and \u03b8 are adapted\n(learned) in such a way that S\u22c6\ni becomes discriminative. However, for a \ufb01xed \u03ba,\nthe second and third terms of S\u22c6\ni , namely \u03bb0||x \u2212D\u03b1||2\n2 and \u03bb0\u03ba||\u03b1||1, are not\nfreely scalable when adapting D and \u03b8, since their magnitudes are bounded.\n\u03bb0 plays the important role of controlling the trade-o\ufb00between reconstruction\nand discrimination in Eq.\n(3).\nFirst, we perform cross-validation for a few\niterations with \u00b5 = 0 to \ufb01nd a good value for SDL-G. Then, a scale factor\nmaking the S\u22c6\ni \u2019s discriminative for \u00b5 > 0 can be chosen during the optimization\nprocess: Given a set of S\u22c6\ni \u2019s, one can compute a scale factor \u03b3 such that \u03b3 =\narg min\u03b3\nPp\ni=1\nP\nj\u2208Ti Ci({\u03b3S\u22c6\nl (xj, D, W)}). We therefore propose the following\nstrategy, which has proven to be e\ufb03cient during our experiments: Starting from\nsmall values for \u03bb0 and a \ufb01xed \u03ba, we apply the algorithm in Figure 2, and after\na supervised sparse coding step, we compute the best scale factor \u03b3, and replace\n\u03bb0 and \u03bb1 by \u03b3\u03bb0 and \u03b3\u03bb1. Typically, applying this procedure during the \ufb01rst\n10 iterations has proven to lead to reasonable values for this parameter.\nSince we are following a continuation path starting from \u00b5 = 0 to \u00b5 = 1,\nthe optimal value of \u00b5 is found along the path by measuring the classi\ufb01cation\nperformance of the model on a validation set during the optimization.\n5.1\nDigits recognition\nIn this section, we present experiments on the popular MNIST [11] and USPS\nhandwritten digit datasets. MNIST is composed of 70 000 images of 28 \u00d7 28\npixels, 60 000 for training, 10 000 for testing, each of them containing a hand-\nwritten digit. USPS is composed of 7291 training images and 2007 test images.\nAs it is often done in classi\ufb01cation, we have chosen to learn pairwise binary\nclassi\ufb01ers, one for each pair of digits.\nAlthough we have presented a multi-\nclass framework, pairwise binary classi\ufb01ers have proven to o\ufb00er a slightly better\nperformance in practice. Five-fold cross validation has been performed to \ufb01nd\nthe best pair (k, \u03ba). The tested values for k are {24, 32, 48, 64, 96}, and for \u03ba,\n{0.13, 0.14, 0.15, 0.16, 0.17}. Then, we have kept the three best pairs of param-\neters and used them to train three sets of pairwise classi\ufb01ers. For a given patch\nx, the test procedure consists of selecting the class which receives the most votes\nfrom the pairwise classi\ufb01ers. All the other parameters are obtained using the\nprocedure explained above. Classi\ufb01cation results are presented on Table 1 when\nusing the linear model. We see that for the linear model L, SDL-D L performs\nthe best. REC BL o\ufb00ers a larger feature space and performs better than REC\nL. Nevertheless, we have observed no gain by using SDL-G BL or SDL-D BL\ninstead of REC BL. Since the linear model is already performing very well, one\nside e\ufb00ect of using BL instead of L is to increase the number of free parame-\nters and thus to cause over\ufb01tting. Note that the best error rates published on\nthese datasets (without any modi\ufb01cation of the training set) are 0.60% [16] for\nMNIST and 2.4% [6] for USPS, using methods tailored to these tasks, whereas\nours is generic and has not been tuned to the handwritten digit classi\ufb01cation\ndomain.\nINRIA\nSupervised Dictionary Learning\n11\nREC L\nSDL-G L\nSDL-D L\nREC BL\nk-NN, \u21132\nSVM-Gauss\nMNIST\n4.33\n3.56\n1.05\n3.41\n5.0\n1.4\nUSPS\n6.83\n6.67\n3.54\n4.38\n5.2\n4.2\nTable 1: Error rates on MNIST and USPS datasets in percents from the REC,\nSDL-G L and SDL-D L approaches, compared with k-nearest neighbor and SVM\nwith a Gaussian kernel [11].\nThe purpose of our second experiment is not to measure the raw performance\nof our algorithm, but to answer the question \u201care the obtained dictionaries D\ndiscriminative per se or is the pair (D,\u03b8) discriminative?\u201d. To do so, we have\ntrained on the USPS dataset 10 binary classi\ufb01ers, one per digit in a one vs all\nfashion on the training set. For a given value of \u00b5, we obtain 10 dictionaries D\nand 10 sets of parameters \u03b8, learned by the SDL-D L model.\nTo evaluate the discriminative power of the dictionaries D, we discard the\nlearned parameters \u03b8 and use the dictionaries as if they had been learned in\na reconstructive REC model: For each dictionary, we decompose each image\nfrom the training set by solving the simple sparse reconstruction problem from\nEq. (1) instead of using supervised sparse coding. This provides us with some\ncoe\ufb03cients \u03b1, which we use as features in a linear SVM. Repeating the sparse\ndecomposition procedure on the test set permits us to evaluate the performance\nof these learned linear SVM. We plot the average error rate of these classi\ufb01ers\non Figure 3 for each value of \u00b5. We see that using the dictionaries obtained\nwith discrimative learning (\u00b5 > 0, SDL-D L) dramatically improves the perfor-\nmance of the basic linear classi\ufb01er learned a posteriori on the \u03b1\u2019s, showing that\nour learned dictionaries are discriminative per se. Figure 4 shows a dictionary\nadapted to the reconstruction of the MNIST dataset and a discriminative one,\nadapted to \u201c9 vs all\u201d.\n0\n0.5\n1.0\n1.5\n2.0\n2.5\n0\n0.2\n0.4\n0.6\n0.8\n1.0\nFigure 3: Average error rate in percents obtained by our dictionaries learned\nin a discriminative framework (SDL-D L) for various values of \u00b5, when used in\nused at test time in a reconstructive framework (REC-L). See text for details.\n5.2\nTexture classi\ufb01cation\nIn the digit recognition task, our BL bilinear framework did not perform better\nthan L and we believe that one of the main reasons is due to the simplicity of the\nRR n\u00b0 6652\n12\nMairal, Bach, Ponce, Sapiro & Zisserman\n(a) REC, MNIST\n(b) SDL-D, MNIST\nFigure 4: On the left, a reconstructive dictionary, on the right a discriminative\none for the task \u201c9 vs all\u201d.\nM\nREC L\nSDL-G L\nSDL-D L\nREC BL\nSDL-G BL\nSDL-D BL\nGain\n300\n48.84\n47.34\n44.84\n26.34\n26.34\n26.34\n0%\n1500\n46.8\n46.3\n42\n22.7\n22.3\n22.3\n2%\n3000\n45.17\n45.1\n40.6\n21.99\n21.22\n21.22\n4%\n6000\n45.71\n43.68\n39.77\n19.77\n18.75\n18.61\n6%\n15000\n47.54\n46.15\n38.99\n18.2\n17.26\n15.48\n15%\n30000\n47.28\n45.1\n38.3\n18.99\n16.84\n14.26\n25%\nTable 2: Error rates for the texture classi\ufb01cation task using various frameworks\nand sizes M of training set. The last column indicates the gain between the\nerror rate of REC BL and SDL-D BL.\ntask, where a linear model is rich enough. The purpose of our next experiment\nis to answer the question \u201cWhen is BL worth using?\u201d.\nWe have chosen to\nconsider two texture images from the Brodatz dataset, presented in Figure 5,\nand to build two classes, composed of 12 \u00d7 12 patches taken from these two\ntextures. We have compared the classi\ufb01cation performance of all our methods,\nincluding BL, for a dictionary of size k = 64 and \u03ba = 0.15. The training set\nwas composed of patches from the left half of each texture and the test sets\nof patches from the right half, so that there is no overlap between them in the\ntraining and test set. Error rates are reported for varying sizes of the training\nset. This experiment shows that in some cases, the linear model completely\nfails and BL is necessary. Discrimination helps especially when the size of the\ntraining set is particularly valuable for large training sets. Note that we did\nnot perform any cross-validation to optimize the parameters k and \u03ba for this\nexperiment. Dictionaries obtained with REC and SDL-D BL are presented in\nFigure 5. Note that though they are visually quite similar, they lead to very\ndi\ufb00erent performance.\nINRIA\nSupervised Dictionary Learning\n13\n(a) Texture 1\n(b) Texture 2\n(c) REC\n(d) SDL-D BL\nFigure 5: Top: Test textures. Bottom left: reconstructive dictionary. Bottom\nright: discriminative dictionary.\nRR n\u00b0 6652\n14\nMairal, Bach, Ponce, Sapiro & Zisserman\n6\nConclusion\nWe have introduced in this paper a discriminative approach to supervised dictio-\nnary learning that e\ufb00ectively exploits the corresponding sparse signal decompo-\nsitions in image classi\ufb01cation tasks, and a\ufb00ords an e\ufb00ective method for learning\na shared dictionary and multiple (linear or bilinear) decision functions. Future\nwork will be devoted to adapting the proposed framework to shift-invariant\nmodels that are standard in image processing tasks, but not readily generalized\nto the sparse dictionary learning setting. We are also investigating extensions to\nunsupervised and semi-supervised learning and applications into natural image\nclassi\ufb01cation.\nReferences\n[1] M. Aharon, M. Elad, and A. M. Bruckstein.\nThe K-SVD: An algorithm for\ndesigning of overcomplete dictionaries for sparse representations. IEEE Trans.\nSP, 54(11):4311\u20134322, November 2006.\n[2] D. Blei and J. McAuli\ufb00e. Supervised topic models. In Adv. NIPS, 2007.\n[3] D. L. Donoho. Compressive sampling. IEEE Trans. IT, 52(4):1289\u20131306, April\n2006.\n[4] B. Efron, T. Hastie, I. Johnstone, and R. Tibshirani. Least angle regression. Ann.\nStatist., 32(2):407\u2013499, 2004.\n[5] M. Elad and M. Aharon. Image denoising via sparse and redundant represen-\ntations over learned dictionaries. IEEE Trans. IP, 54(12):3736\u20133745, December\n2006.\n[6] B. Haasdonk and D. Keysers. Tangent distant kernels for support vector machines.\nIn Proc. ICPR, 2002.\n[7] E. T. Hale, W. Yin, and Y. Zhang.\nA \ufb01xed-point continuation method for\nl1-regularized minimization with applications to compressed sensing.\nTech-\nnical report,\nRice University,,\n2007.\nCAAM Technical Report TR07-07,\nhttp://www.caam.rice.edu/\u223coptimization/L1/fpc/.\n[8] A. Holub and P. Perona. A discriminative framework for modeling object classes.\nIn Proc. IEEE CVPR, 2005.\n[9] K. Huang and S. Aviyente. Sparse representation for signal classi\ufb01cation. In Adv.\nNIPS, 2006.\n[10] J.A. Lasserre, C.M. Bishop, and T.P. Minka. Principled hybrids of generative\nand discriminative models. In Proc. IEEE CVPR, 2006.\n[11] Y. LeCun, L. Bottou, Y. Bengio, and P. Ha\ufb00ner. Gradient-based learning applied\nto document recognition. Proceedings of the IEEE, 86(11):2278\u20132324, November\n1998.\n[12] J. Mairal, F. Bach, J. Ponce, G. Sapiro, and A. Zisserman. Learning discriminative\ndictionaries for local image analysis. In Proc. IEEE CVPR, 2008.\n[13] B. A. Olshausen and D. J. Field. Sparse coding with an overcomplete basis set:\nA strategy employed by v1? Vision Research, 37:3311\u20133325, 1997.\n[14] R. Raina, A. Battle, H. Lee, B. Packer, and A. Y. Ng.\nSelf-taught learning:\ntransfer learning from unlabeled data. In ICML, 2007.\n[15] R. Raina, Y. Shen, A. Y. Ng, and A. McCallum.\nClassi\ufb01cation with hybrid\ngenerative/discriminative models. In Adv. NIPS, 2004.\nINRIA\nSupervised Dictionary Learning\n15\n[16] M. Ranzato, C. Poultney, S. Chopra, and Y. LeCun. E\ufb03cient learning of sparse\nrepresentations with an energy-based model. In Adv. NIPS, 2006.\n[17] M. Ranzato and M. Szummer. Semi-supervised learning of compact document\nrepresentations with deep networks. In ICML, 2008.\n[18] F. Rodriguez and G. Sapiro.\nSparse representations for image classi\ufb01cation:\nLearning discriminative and reconstructive non-parametric dictionaries.\nTech-\nnical report, University of Minnesota, December 2007. IMA Preprint 2213.\n[19] R. R. Salakhutdinov and G. E. Hinton.\nLearning a non-linear embedding by\npreserving class neighbourhood structure. In AI and Statistics, 2007.\n[20] J. Winn, A. Criminisi, and T. Minka. Object categorization by learned universal\nvisual dictionary. In Proc. IEEE ICCV, 2005.\n[21] J. Wright, A. Y. Yang, A. Ganesh, S. Sastry, and Y. Ma.\nRobust face\nrecognition via sparse representation.\nIEEE Trans. PAMI, 2008.\nto appear,\nhttp://perception.csl.uiuc.edu/recognition/Home.html.\nRR n\u00b0 6652\nCentre de recherche INRIA Paris \u2013 Rocquencourt\nDomaine de Voluceau - Rocquencourt - BP 105 - 78153 Le Chesnay Cedex (France)\nCentre de recherche INRIA Bordeaux \u2013 Sud Ouest : Domaine Universitaire - 351, cours de la Lib\u00e9ration - 33405 Talence Cedex\nCentre de recherche INRIA Grenoble \u2013 Rh\u00f4ne-Alpes : 655, avenue de l\u2019Europe - 38334 Montbonnot Saint-Ismier\nCentre de recherche INRIA Lille \u2013 Nord Europe : Parc Scienti\ufb01que de la Haute Borne - 40, avenue Halley - 59650 Villeneuve d\u2019Ascq\nCentre de recherche INRIA Nancy \u2013 Grand Est : LORIA, Technop\u00f4le de Nancy-Brabois - Campus scienti\ufb01que\n615, rue du Jardin Botanique - BP 101 - 54602 Villers-l\u00e8s-Nancy Cedex\nCentre de recherche INRIA Rennes \u2013 Bretagne Atlantique : IRISA, Campus universitaire de Beaulieu - 35042 Rennes Cedex\nCentre de recherche INRIA Saclay \u2013 \u00cele-de-France : Parc Orsay Universit\u00e9 - ZAC des Vignes : 4, rue Jacques Monod - 91893 Orsay Cedex\nCentre de recherche INRIA Sophia Antipolis \u2013 M\u00e9diterran\u00e9e : 2004, route des Lucioles - BP 93 - 06902 Sophia Antipolis Cedex\n\u00c9diteur\nINRIA - Domaine de Voluceau - Rocquencourt, BP 105 - 78153 Le Chesnay Cedex (France)\nhttp://www.inria.fr\nISSN 0249-6399\n",
        "sentence": " Sparse representation models [1], [2], [3], [4] and related machine learning algorithms [5], [6] have proven remarkably successful at extracting useful information from complex highdimensional signals, for example in the context of denoising [7], under-determined source separation [8], [9], compressed sensing [10], super-resolution sensing [11], and classification [12].",
        "context": "numerous low-level image processing tasks such as denoising [5], showing that\nsparse models are well adapted to natural images. Unlike principal component\nanalysis decompositions, these models are most ofen overcomplete, with a num-\nSparse representations for image classi\ufb01cation:\nLearning discriminative and reconstructive non-parametric dictionaries.\nTech-\nnical report, University of Minnesota, December 2007. IMA Preprint 2213.\n[19] R. R. Salakhutdinov and G. E. Hinton.\nIn Proc. IEEE CVPR, 2005.\n[9] K. Huang and S. Aviyente. Sparse representation for signal classi\ufb01cation. In Adv.\nNIPS, 2006.\n[10] J.A. Lasserre, C.M. Bishop, and T.P. Minka. Principled hybrids of generative"
    },
    {
        "title": "On the computational intractability of exact and approximate dictionary learning",
        "author": [
            "A.M. Tillmann"
        ],
        "venue": "IEEE Signal Processing Letters, vol. 22, no. 1, pp. 45\u201349, Jan 2015.",
        "citeRegEx": "13",
        "shortCiteRegEx": null,
        "year": 2015,
        "abstract": "The efficient sparse coding and reconstruction of signal vectors via linear\nobservations has received a tremendous amount of attention over the last\ndecade. In this context, the automated learning of a suitable basis or\novercomplete dictionary from training data sets of certain signal classes for\nuse in sparse representations has turned out to be of particular importance\nregarding practical signal processing applications. Most popular dictionary\nlearning algorithms involve NP-hard sparse recovery problems in each iteration,\nwhich may give some indication about the complexity of dictionary learning but\ndoes not constitute an actual proof of computational intractability. In this\ntechnical note, we show that learning a dictionary with which a given set of\ntraining signals can be represented as sparsely as possible is indeed NP-hard.\nMoreover, we also establish hardness of approximating the solution to within\nlarge factors of the optimal sparsity level. Furthermore, we give NP-hardness\nand non-approximability results for a recent dictionary learning variation\ncalled the sensor permutation problem. Along the way, we also obtain a new\nnon-approximability result for the classical sparse recovery problem from\ncompressed sensing.",
        "full_text": "arXiv:1405.6664v2  [cs.IT]  3 Aug 2014\nON THE COMPUTATIONAL INTRACTABILITY OF EXACT AND APPROXIMATE DICTIONARY LEARNING\n1\nOn the Computational Intractability of\nExact and Approximate Dictionary Learning\nAndreas M. Tillmann\nAbstract\u2014The ef\ufb01cient sparse coding and reconstruction of\nsignal vectors via linear observations has received a tremendous\namount of attention over the last decade. In this context, the\nautomated learning of a suitable basis or overcomplete dictionary\nfrom training data sets of certain signal classes for use in sparse\nrepresentations has turned out to be of particular importance\nregarding practical signal processing applications. Most popular\ndictionary learning algorithms involve NP-hard sparse recovery\nproblems in each iteration, which may give some indication about\nthe complexity of dictionary learning but does not constitute\nan actual proof of computational intractability. In this technical\nnote, we show that learning a dictionary with which a given set\nof training signals can be represented as sparsely as possible\nis indeed NP-hard. Moreover, we also establish hardness of\napproximating the solution to within large factors of the optimal\nsparsity level. Furthermore, we give NP-hardness and non-\napproximability results for a recent dictionary learning variation\ncalled the sensor permutation problem. Along the way, we also\nobtain a new non-approximability result for the classical sparse\nrecovery problem from compressed sensing.\nIndex\nTerms\u2014(SAS-MALN,\nMLSAS-SPARSE)\nMachine\nLearning, Compressed Sensing, Computational Complexity\nI. INTRODUCTION\nA\nS a central problem in compressed sensing (CS) [1], [2],\n[3], the task of \ufb01nding a sparsest exact or approximate\nsolution to an underdetermined system of linear equations\nhas been a strong focus of research during the past decade.\nDenoting by \u2225x\u22250 the so-called \u21130-norm, i.e., the number of\nnonzero entries in x, the sparse recovery problem reads\nmin \u2225x\u22250\ns.t.\n\u2225Dx \u2212y\u22252 \u2264\u03b4,\n(P\u03b4\n0)\nfor a given matrix D \u2208Rm\u00d7n with m \u2264n and an estimate\n\u03b4 \u22650 of the amount of error contained in the measurements\ny \u2208Rm. Both the noisefree problem (P0) := (P0\n0) and the\nerror-tolerant variant (P\u03b4\n0) with \u03b4 > 0 are well-known to be\nNP-hard in the strong sense, cf. [4, problem MP5] and [5],\nand also dif\ufb01cult to approximate [6], [7].\nGroundbreaking results from CS theory include quali\ufb01ca-\ntory conditions (on the dictionary D and the solution sparsity\nlevel) which yield ef\ufb01cient solvability of the generally hard\nproblems (P\u03b4\n0) by greedy methods\u2014e.g., orthogonal matching\npursuit (OMP) [8]\u2014or (convex) relaxations such as Basis\nPursuit [9] (replacing the \u21130-norm by the \u21131-norm); see [2],\n[3], [10] for overviews. Subsequently, numerous optimization\nalgorithms have been tailored to sparse recovery tasks, and\nA. M. Tillmann is with the Research Group Optimization at TU Darmstadt,\nDolivostr. 15, 64293 Darmstadt, Germany (phone: +49-6151-1670868, e-mail:\ntillmann@mathematik.tu-darmstadt.de).\nThis work has been accepted by the IEEE for publication. Copyright may\nbe transferred without notice, after which this version may no longer be\naccessible.\nvarious types of dictionaries were shown or designed to exhibit\nfavorable recoverability properties. In particular, the essential\nassumption of (exact or approximate) sparse representability\nof certain signal classes using speci\ufb01c dictionaries has been\nempirically veri\ufb01ed in many practical signal processing appli-\ncations; for instance, natural images are known to admit sparse\napproximations over discrete cosine or wavelet bases [2].\nNevertheless, a predetermined setup typically cannot fully\ncapture the true structure of real-world signals; thus, using a\n\ufb01xed dictionary D naturally restricts the achievable sparsity\nlevels of the representations. Indeed, the simultaneous search\nfor both dictionary and sparse representations of a set of train-\ning signals\u2014commonly referred to as dictionary learning\u2014\nwas demonstrated to allow for signi\ufb01cantly improved sparsity\nlevels using the learned dictionary instead of an analytical,\nstructured or random one. Successful applications of dictionary\nlearning include diverse tasks such as image inpainting [11],\n[12], denoising [13], [14] and deblurring [15], or audio and\nspeech signal representation [16], [17], to name but a few.\nSomewhat informally, the dictionary learning (DL) prob-\nlem can be stated as: Given a collection of training data\nvectors y1, . . . , yp \u2208Rm and a positive integer n, \ufb01nd a\nmatrix D \u2208Rm\u00d7n that allows for the sparsest possible\nrepresentations xj such that Dxj = yj (for all j). This\ntask can be formalized in different ways, and there exist\nmany variants seeking dictionaries with further properties such\nas incoherence [18] or union-of-bases [19]; see also, e.g.,\n[20], [21], [12]. Moreover, several DL algorithms have been\ndeveloped over the past years; the frequently encountered hard\nsparse recovery subproblems are typically treated by classical\nmethods from CS. We refer to [22], [23], [24], [25], [26], [12],\n[27], [28] (and references therein) for a broader overview of\nwell-established DL techniques and some more recent results.\nIn this paper, we are concerned with the computational\ncomplexity of dictionary learning. Due to its combinatorial\nnature, it is widely believed to be a very challenging prob-\nlem, but to the best of our knowledge, a formal proof of\nthis intractability claim was missing. We contribute to the\ntheoretical understanding of the problem by providing an\nNP-hardness proof as well as a strong non-approximability\nresult for DL, see Section II. Furthermore, we prove NP-\nhardness and non-approximability of an interesting new DL\nvariant\u2014the sensor permutation problem, where the sought\ndictionary is constrained to be related to a given sensing\nmatrix via unknown row permutations; see Section III for the\ndetails. As a byproduct, we also obtain a new NP-hardness of\napproximation result for the sparse recovery problem (P\u03b4\n0).\nRemark 1: Recall\nthat\nNP-hardness\nimplies\nthat\nno\npolynomial-time solution algorithm can exist, under the\n2\nON THE COMPUTATIONAL INTRACTABILITY OF EXACT AND APPROXIMATE DICTIONARY LEARNING\nmost-widely believed theoretical complexity assumption that\nP\u0338=NP [4]. Further, strong NP-hardness can be understood,\nin a nutshell, as an indication that a problem\u2019s intractability\ndoes not depend on ill-conditioning of the input coef\ufb01cients.\nThis additionally implies that (unless P=NP) there cannot\nexist a pseudo-polynomial-time exact algorithm and not even a\nfully polynomial-time approximation scheme (FPTAS), i.e., an\nalgorithm that solves a minimization problem within a factor\nof (1+\u03b5) of the optimal value in polynomial time with respect\nto the input size and 1/\u03b5, see [4]. For a thorough and detailed\ntreatment of complexity theory, we refer to [4], [29].\nII. THE COMPLEXITY OF DICTIONARY LEARNING\nAs mentioned in the introduction, different philosophies\nor goals lead to different formulations of dictionary learning\nproblems, which are usually captured by the general form\nmin\nD,X f(D, X; Y ) + g(D) + h(X),\n(1)\nwhere the variables are the dictionary D \u2208Rm\u00d7n (for an a\npriori chosen n) and the matrix X \u2208Rn\u00d7p, whose columns\nare the representation vectors xj of the given training signals\nyj (w.r.t. the linear model assumption Dxj \u2248yj), collected in\nY \u2208Rm\u00d7p as its columns; the functions f, g and h express a\ndata \ufb01delity term, and constraints or penalties/regularizers for\nthe dictionary and the representation coef\ufb01cient vectors, resp.\nIn the (ideal) noiseless case, the usual approach (see, e.g.,\n[11], [21], [25]) is\nmin\nD,X \u2225X\u22250\ns.t.\nDX = Y ,\n(2)\nwhich \ufb01ts the framework (1) by setting f(D, X; Y ) :=\n\u03c7{DX=Y }(D, X) (where \u03c7 is the indicator function, i.e.,\nf(D, X; Y ) = 0 if DX = Y and \u221eotherwise), g(D) := 0\nand h(X) := \u2225X\u22250 (extending the usual notation to matrices,\n\u2225X\u22250 counts the nonzero entries in X). This problem is a\nnatural extension of (P0), and can also be seen as a matrix-\nfactorization problem. To mitigate scaling ambiguities, one of-\nten sets g(D) := \u03c7{\u2225Dj\u22252\u22641 \u2200j=1,...,n}(D), i.e., the columns\nof D are required to have bounded norms; cf. [26], [16].\nNote that if n is not \ufb01xed a priori to a value smaller than p,\nthe dictionary learning task becomes trivial: Then, we could\njust take D = [y1, . . . , yp] and exactly represent every yi\nusing only one column. (Clearly, this also holds for variants\nwhich allow representation errors, e.g., \u2225Dxj \u2212yj\u22252 \u2264\u03b4 for\nsome \u03b4 > 0, or minimize such errors under hard sparsity limits\n\u2225xj\u22250 \u2264k for some k \u22651.) Thus, requiring n < p is hardly\nrestrictive, in particular since the training data set (and hence,\np) is usually very large\u2014intuitively, the more samples of a\ncertain signal class are available for learning the dictionary,\nthe better the outcome will be adapted to that signal class\u2014\nand with respect to storage aspects and ef\ufb01cient (algorithmic)\napplicability of the learned dictionary, settling for a smaller\nnumber of dictionary atoms is well-justi\ufb01ed. Similarly, m \u2264n\nis a natural assumption, since sparsity of the coef\ufb01cient vectors\nis achieved via appropriate representation bases or redundancy\n(overcompleteness) of the dictionary; also, at least for large p,\none can expect rank(Y ) = m, in which case rank(D) =\nm \u2264n becomes necessary to maintain DX = Y .\nA. NP-Hardness\nAs the following results show, \ufb01nding a dictionary with\nwhich the training signals can be represented with optimal\nsparsity is indeed a computationally intractable problem.\nTheorem 2: Solving the dictionary learning problem (2) is\nNP-hard in the strong sense, even when restricting n = m.\nProof: We reduce from the matrix sparsi\ufb01cation (MS)\nproblem: Given a full-rankmatrix M \u2208Qm\u00d7p (m < p), \ufb01nd\na regular matrix B \u2208Rm\u00d7m such that BM has as few\nnonzero entries as possible. (The full-rank assumption is not\nmandatory, but can be made w.l.o.g.: If rank(M) = k < m,\nm \u2212k rows can be zeroed in polynomial time by elemen-\ntary row operations, reducing the problem to sparsifying the\nremaining k-row submatrix.) The MS problem was shown\nto be NP-hard in [30, Theorem 3.2.1] (see also [31], [10]),\nby a reduction from simple max cut, cf. [4, problem ND16];\nsince this reduction constructs a binary matrix (of dimensions\npolynomially bounded by the cut problem\u2019s input graph size),\nNP-hardness of MS in fact holds in the strong sense, and we\nmay even assume w.l.o.g. that M \u2208{0, 1}m\u00d7p.\nFrom an instance of MS, we obtain an equivalent instance\nof (2) as follows: Set n := m and let Y\n:= M. Then,\nthe task (2) is to \ufb01nd D \u2208Rm\u00d7m and X \u2208Rm\u00d7p such\nthat DX = Y and \u2225X\u22250 is minimal. (Note that the sought\ndictionary in fact constitutes a basis for Rm, since M has\nfull row-rank m, thus requiring this of the dictionary as well,\nas discussed above.) Clearly, an optimal solution (D\u2217, X\u2217)\nof this dictionary learning instance gives an optimal solution\nB\u2217= D\u22121\n\u2217\nof MS, with B\u2217M = X\u2217. It remains to note that\nthe reduction is indeed polynomial, since the matrix inversion\ncan be performed in strongly polynomial time by Gaussian\nelimination, cf. [32]. Thus, (2) is strongly NP-hard.\nRemark 3: The above NP-hardness result easily extends to\nvariants of (2) with the additional constraint that, for some con-\nstant c > 0, \u2225Dj\u22252 \u2264c for all j, or \u2225D\u22252\nF = tr(D\u22a4D) \u2264c\n(as treated in [16]): Since the discrete objectives are invariant\nto scaling in both the dictionary learning and the MS problem,\nthere is always also an optimal D\u2032\n\u2217(achieving the same\nnumber of nonzeros in the corresponding X\u2032\n\u2217) that obeys the\nnorm constraints and yields an associated optimal solution\nB\u2032\n\u2217= (D\u2032\n\u2217)\u22121 of the MS problem. (Clearly, this argument\nremains valid for a host of similar norm constraints as well.)\nIt is not known whether the decision version of the MS\nproblem is contained in NP (and thus not only NP-hard but\nNP-complete) [30]. Similarly, we do not know if the decision\nproblem associated with (2)\u2014\u201cgiven Y \u2208Qm\u00d7p and positive\nintegers k and n, decide whether there exist D \u2208Rm\u00d7n\nand X \u2208Rn\u00d7p such that DX = Y and \u2225X\u22250 \u2264k\u201d\u2014is\ncontained in NP, even in the square case n = m.\nB. Non-Approximability\nSince for NP-hard problems, the existence of ef\ufb01cient\n(polynomial-time) general exact solution algorithms is deemed\nimpossible, it is natural to search for good approximation\nmethods. Indeed, virtually all well-known dictionary learning\nalgorithms can be interpreted (in a vague sense) as \u201capprox-\nimation schemes\u201d since, e.g., the \u21130-norm is convexi\ufb01ed to\nON THE COMPUTATIONAL INTRACTABILITY OF EXACT AND APPROXIMATE DICTIONARY LEARNING\n3\nthe \u21131-norm, constraints may be turned to penalty terms in\nthe objective (regularization), etc. However, even disregarding\nthe computational costs of the algorithms, little is known\nabout the quality of the obtained approximations; several\nrecent works along these lines started investigating theoretical\nrecovery properties and error guarantees of dictionary learning\nalgorithms, see, e.g., [33], [34], [28]; in particular, [34] shows\nthe importantance of a good dictionary initialization.\nThe non-existence of an FPTAS (cf. Remark 1) itself does\nnot generally rule out the existence of an ef\ufb01cient algorithm\nwith some constant approximation guarantee. However, we\nshow below that it is almost-NP-hard to approximate the\ndictionary learning problem (2) to within large factors of the\noptimal achievable sparsity of representations. Almost-NP-\nhardness means that no polynomial-time algorithm (here, to\nachieve the desired approximation ratio) can exist so long as\nNP\u0338\u2286DTIME(N poly(log N)), where N measures the input size\n(usually, dimension); cf. [35]. This complexity assumption is\nstronger than P\u0338=NP, but also \ufb01rmly believed (cf., e.g., [36],\n[37], [38]); it essentially amounts to the claim that not all NP-\nhard problems admit a quasi-polynomial-time deterministic so-\nlution algorithm. Many of the best known non-approximability\nresults are based on this assumption (see, e.g., [39], [36]).\nTheorem 4: For any \u03b5 > 0, the dictionary learning prob-\nlem (2) cannot be approximated within a factor of 2log1\u2212\u03b5 m\nin polynomial time, unless NP\u2286DTIME(mpoly(log m)).\nProof: In [40], almost-NP-hardness of approximating the\noptimal value of the matrix sparsi\ufb01cation problem1 to within\na factor of 2log1/2\u2212o(1) m (i.e., 2log1/2\u2212\u03b5 m for any \u03b5 > 0)\nwas shown, based on results from [35] for the problem of\nminimizing the number of violated equations in an infeasible\nlinear equation system (see also [6], where this is called\nMinULR). A closer inspection of [40, Section 3] and [35,\nTheorems 7 and 8] reveals that this non-approximability result\nin fact holds up to factors of 2log1\u2212\u03b5 m for any \u03b5 > 0. Since\nour reduction in the proof of Theorem 2 is cost-preserving,\nthis result carries over directly.\nThis shows that it is extremely unlikely to ef\ufb01ciently learn\na dictionary that yields provably good approximations of the\nsought sparse representations of the training data.\nRemark 5: The extensions of problem (2) that incorporate\nnorm bounds on D are equally hard to approximate since the\nrespective objectives do not differ from the original matrix\nsparsi\ufb01cation problem\u2019s objective, cf. Remark 3. Moreover, the\nchain of reductions ending in the above result and starting with\n[35, Theorem 7], maintains a polynomial relationship between\nthe dimensions (here, m and p); thus, almost-NP-hardness also\nholds for approximation to within 2log1\u2212\u03b5 p.\nRemark 6: One may also be interested in learning an anal-\nysis dictionary \u2126, minimizing \u2225\u2126x\u22250 (for given x), see,\ne.g., [41], [42]. Imposing that \u2126has full rank excludes the\ntrivial solution \u2126= 0 and, in fact, the square case then is\ncompletely equivalent to the MS problem, showing strong\nNP-hardness and almost-NP-hardness of approximation for\nanalysis dictionary learning; Remarks 3 and 5 apply similarly.\n1The MS problem in [40] is de\ufb01ned precisely in transposed form compared\nto the present paper, i.e., there, one seeks to sparsify a full-rank matrix with\nmore rows than columns by right-multiplication with an invertible matrix.\nIII. SPARSE CODING WITH UNKNOWN SENSOR\nLOCATIONS\nRecently, an interesting new problem was introduced in\n[43] and dubbed the \u201csensor permutation problem\u201d (for short,\nSP). Here, it is assumed that the dictionary D is known\nup to a permutation of its rows, and one wishes to obtain\nthe sparsest representations of the observations Y achievable\nvia permuting these rows\u2014or equivalently, the measurement\nentries. This approach can model, e.g., faulty wiring in the\nmeasurement system setup [43]. Formally, the SP problem can\nbe stated as\nmin\nP ,X \u2225X\u22250\ns.t.\nAX = P Y , P \u2208Pm,\n(3)\nwhere A \u2208Rm\u00d7n is a known dictionary, Y \u2208Rm\u00d7p and\nPm := {P \u2208{0, 1}m\u00d7m : \u2225P \u22251 = \u2225P \u2225\u221e= 1, P \u22a4P = I}\ndenotes the set of all m \u00d7 m permutation matrices. (3) can\nalso be seen as a special case of the general dictionary learn-\ning framework (1), with f(D, X; Y ) = \u03c7{DX=Y }(D, X),\ng(D) = \u03c7{D=P \u22a4A : P \u2208Pm}(D) and h(X) = \u2225X\u22250.\nAs our following results show, the sensor permutation\nproblem is computationally intractable, even for \u201cnice\u201d input\nthat does not contain numbers of highly varying sizes.\nTheorem 7: Problem (3) is NP-hard in the strong sense,\neven if A and Y are binary and p = 1. Moreover, for any\n\u03b1 \u2208(0, 1) and any \u03b5 > 0, there is no polynomial-time\nalgorithm to approximate (3) within a factor of (1 \u2212\u03b1) ln(m)\nunless P=NP, or to within a factor of 2log1\u2212\u03b5 m unless\nNP\u2286DTIME(mpoly(log m)). These results remain valid when\nAX = P Y is relaxed to \u2225AX \u2212P Y \u22252 \u2264\u03b4 for 0 < \u03b4 \u2208R,\nand/or m is replaced by n.\nFor the proof, recall the well-known strongly NP-hard Set\nCover problem (SC, cf. [4, problem SP5]): \u201cGiven a set S\nand a collection C of subsets of S, \ufb01nd a cover of minimum\ncardinality, i.e., a subcollection C\u2032 of as few sets from C as\npossible such that S\nC\u2208C\u2032 C = S\u201d. A cover C\u2032 is called exact\nif C \u2229D = \u2205for all C, D \u2208C\u2032 (in other words, if every\nelement of S is contained in exactly one set from C\u2032).\nWe will employ the following very recent result:\nProposition 8 ([44, Theorem 2]): For every 0 < \u03b1 < 1,\nthere exists a polynomial-time reduction from an arbitrary\ninstance of the strongly NP-complete satis\ufb01ability problem\n(SAT, cf. [4, problem LO1]) to an SC instance (S, C) with\na parameter k \u2208N such that if the input SAT instance is\nsatis\ufb01able, there is an exact cover of size k (and no smaller\ncovers), whereas otherwise, every cover has size at least\n(1 \u2212\u03b1) ln(|S|) k.\nRecall also that, for any \u03b5 > 0, approximating the sparse\nrecovery problem (P\u03b4\n0) (with any \u03b4 \u22650) to within factors\n2log1\u2212\u03b5 n is almost-NP-hard, by [7, Theorem 3]. (In fact,\nalthough it clearly goes through for \u03b4 = 0 as well, [7] states\nthe proof of this only for \u03b4 > 0, because the corresponding\nresult for (P0) had already been shown in [6] before.) The\nproof of [7, Theorem 3] is based on a special SC instance\nconstruction from [45] (see also [35, Proposition 6]) similar\nto that from Proposition 8.\nRemark 9: In the special SC instances underlying the above\nresults, it holds that |C| and |S| are polynomially related, so\n4\nON THE COMPUTATIONAL INTRACTABILITY OF EXACT AND APPROXIMATE DICTIONARY LEARNING\nthat all non-approximability results stated in this section also\nhold with m (= |S|) replaced by n (= |C|).\nWe are now ready to prove the main result of this section.\nProof of Theorem 7:\nLet (S, C, k, \u03b1) be a Set Cover\ninstance as in Proposition 8, and let n = |C|, m = |S|.\nFollowing the proof of [7, Theorem 3], we \ufb01rst transform the\ntask of \ufb01nding a minimum-cardinality set cover to the sparse\nrecovery problem (P0): De\ufb01ne D \u2208{0, 1}m\u00d7n by setting\nDij = 1 if and only if the i-th element of S is contained in\nthe j-th set from C, and set y := 1, i.e., the all-ones vector of\nlength m. It is easily seen that the support of every solution\nx of Dx = y induces a set cover (if some element was\nnot covered, at least one row of the equality system would\nevaluate to 0 = 1, contradicting Dx = y). Conversely, every\nexact cover induces a solution of the same \u21130-norm as the\ncover size (put xC = 1 for the sets C contained in the exact\ncover, and zero in the remaining components). Thus, if there\nis an exact cover of size k, there is a k-sparse solution of\nDx = y. Conversely, if all set covers have size at least\n(1 \u2212\u03b1) ln(m) k, then necessarily all x with Dx = y have\n\u2225x\u22250 \u2265(1 \u2212\u03b1) ln(m) k (because otherwise, the support of x\nwould yield a set cover of size smaller than (1 \u2212\u03b1) ln(m) k).\nThis instance of (P0) is now easily transformed into one\nof the sensor permutation problem (3): We set A := D,\nY\n:= y (thus, p = 1). Now, since Y\n= 1, P Y\n= Y\nfor all P \u2208Pm and the choice of P has no in\ufb02uence on\nthe solution. Thus, indeed, the SP problem (3) for these A\nand Y has precisely the same solution value as the above-\nconstructed instance of (P0). Since solving the original Set\nCover instance is (strongly) NP-hard (by Proposition 8), and\nall constructed numbers and their encoding lengths remain\npolynomially bounded by the input parameter m (and n), this\nimmediately shows the claimed strong NP-hardness result. In\nfact, could we approximate, in polynomial time, the optimal\nsolution value of (3) to within a factor of (1 \u2212\u03b1) ln(m),\nthen we could also decide the SAT instance underlying the\nSC problem from Proposition 8 in polynomial time, which is\nimpossible unless P=NP. Therefore, for any 0 < \u03b1 < 1, even\napproximating (3) to within factors (1\u2212\u03b1) ln(m) is NP-hard.\nFor the second non-approximability result of Theorem 7, it\nsuf\ufb01ces to note that the construction above is cost-preserving\nand that the (P0) instance in the proof of [7, Theorem 3]\nalso has y = 1. Hence, we can directly transfer the non-\napproximability properties, and conclude that there is no\npolynomial-time algorithm approximating (3) to within factors\n2log1\u2212\u03b5 n (for any \u03b5 > 0), unless NP\u2286DTIME(npoly(log n)).\nFinally, the above results extend to the noise-aware\nSP problem variant by treating the relaxed constraints\n\u2225AX \u2212P Y \u22252 \u2264\u03b4 for \u03b4 > 0 completely analogously to\nthe proof of [7, Theorem 3] (we omit the details) and, by\nRemark 9, remain valid w.r.t. either m or n.\nRemark 10: The decision version of (3) is easily seen to be\nin NP (for rational input), and hence NP-complete.\nNote that the \ufb01rst part of the above proof yields a new\nnon-approximability result for sparse recovery:\nCorollary 11: For any \u03b1 \u2208(0, 1), it is NP-hard to approx-\nimate (P\u03b4\n0) to within a factor of (1 \u2212\u03b1)ln(n).\nThis complements the previously known results from [6,\nTheorem 7] and [7, Theorem 3]: For n large enough (and some\n\ufb01xed pair \u03b1, \u03b5), 2log1\u2212\u03b5 n > (1 \u2212\u03b1) ln(n), but the assumption\nP\u0338=NP is weaker than NP\u0338\u2286DTIME(npoly(log n)).\nIV. CONCLUDING REMARKS\nIn this note, we gave formal proofs for NP-hardness and\nnon-approximability of several dictionary learning problems.\nWhile perhaps not very surprising, these results provide\na complexity-theoretical justi\ufb01cation for the common ap-\nproaches to tackle dictionary learning tasks by inexact methods\nand heuristics without performance guarantees.\nWhile preparing this manuscript, we became aware of a\nrelated result presented at ICASSP 2014, see [46]. In that\nwork, the authors claim NP-hardness of approximating\nmin\nD,X \u2225DX \u2212Y \u22252\nF\ns.t.\n\u2225Xj\u22250 \u2264k \u2200j = 1, . . . , p, (4)\nto within a given additive error w.r.t. the objective (i.e., not\nwithin a factor of the optimal value), for the case in which Y\ncontains only two columns and k is \ufb01xed to 1. Unfortunately,\n[46] does not contain a proof, and at the time of writing,\nwe could not locate it elsewhere. Note also that, clearly, (4)\nis also a special case of the general formulation (1)\u2014using\nf(D, X; Y ) = \u2225DX \u2212Y \u22252\nF, h(X) = \u03c7{\u2225Xj\u22250\u2264k \u2200j}(X)\nand g(D) = 0\u2014but that the results from the present paper\nand from [46] nevertheless pertain to different problems, both\nof which are often referred to as \u201cdictionary learning\u201d.\nFuture research closely related to the present work could\ninclude investigating the potential use of matrix sparsi\ufb01cation\nbased heuristics for dictionary learning purposes (e.g., when\nlearning a union-of-bases dictionary as in [19]).\nNote also that the reduction from [40] does not admit trans-\nferring the NP-hardness of approximating MinULR to within\nany constant factor (see [35, Theorem 5]) to the MS problem.\n(Similarly, the reduction to MS in [30] apparently does not pre-\nserve approximation ratios.) Such non-approximability results\nunder the slightly weaker P\u0338=NP assumption hence remain\nopen for problem (2) (and its norm-constrained variants).\nAlso, the complexities of dictionary learning with \u21131-objective\nand/or noise-awareness (e.g., constraints \u2225DX \u2212Y \u2225F \u2264\u03b4 for\n\u03b4 > 0) remain important open problems.\nOn the other hand, one may wish to focus on \u201cgood\nnews\u201d, e.g., by designing ef\ufb01cient approximation algorithms\nthat give performance guarantees not too much worse than our\nintractability thresholds, or by identifying special cases which\nare notably easier to solve. Also, it would be interesting to\ndevelop further \u201chybrid algorithms\u201d that combine relaxation\nmethods and tools from combinatorial optimization, such as\nthe branch & bound procedure from [43].\nACKNOWLEDGMENTS\nThe author would like to thank Yonina Eldar and Julien\nMairal for enticing him to look into dictionary learning, R\u00e9mi\nGribonval for bringing the sensor permutation problem to his\nattention, as well as Imke Joormann, Marc Pfetsch and two\nanonymous referees for their valuable comments on an earlier\nversion of the manuscript.\nON THE COMPUTATIONAL INTRACTABILITY OF EXACT AND APPROXIMATE DICTIONARY LEARNING\n5\nREFERENCES\n[1] D. L. Donoho, \u201cCompressed Sensing,\u201d IEEE Trans. Inform. Theory,\nvol. 52, no. 4, pp. 1289\u20131306, 2006.\n[2] S. Foucart and H. Rauhut, A Mathematical Introduction to Compressive\nSensing, ser. Applied and Numerical Harmonic Analysis.\nBirkh\u00e4user,\n2013.\n[3] G. Kutyniok and Y. C. Eldar, Eds., Compressed Sensing: Theory and\nApplications.\nCambridge University Press, 2012.\n[4] M. R. Garey and D. S. Johnson, Computers and Intractability. A Guide\nto the Theory of NP-completeness. W. H. Freeman and Company, 1979.\n[5] B. K. Natarajan, \u201cSparse Approximate Solutions to Linear Systems,\u201d\nSIAM J. Comput., vol. 24, no. 2, pp. 227\u2013234, 1995.\n[6] E. Amaldi and V. Kann, \u201cOn the Approximability of Minimizing\nNonzero Variables or Unsatis\ufb01ed Relations in Linear Systems,\u201d Theor.\nComput. Sci., vol. 209, no. 1\u20132, pp. 237\u2013260, 1998.\n[7] E. Amaldi, \u201cOn the complexity of designing compact perceptrons and\nsome consequences,\u201d in El. Proc. 5th Internat. Symp. on Arti\ufb01cial\nIntelligence and Math., 1999.\n[8] Y. C. Pati, R. Rezaiifar, and P. S. Krishnaprasad, \u201cOrthogonal Match-\ning Pursuit: Recursive Function Approximation with Applications to\nWavelet Decomposition,\u201d in Proc. 27th Ann. Asilomar Conference on\nSignals, Systems and Computers.\nIEEE Computer Society Press, 1993,\nvol. 1, pp. 40\u201344.\n[9] S. S. Chen, D. L. Donoho, and M. A. Saunders, \u201cAtomic Decomposition\nby Basis Pursuit,\u201d SIAM J. Sci. Comput., vol. 20, no. 1, pp. 33\u201361, 1998.\n[10] A. M. Tillmann, \u201cComputational Aspects of Compressed Sensing,\u201d\nDoctoral dissertation, TU Darmstadt, Germany, 2013.\n[11] M. Aharon, M. Elad, and A. M. Bruckstein, \u201cOn the uniqueness of\novercomplete dictionaries, and a practical way to retrieve them,\u201d Linear\nAlgebra Appl., vol. 416, no. 1, pp. 48\u201367, 2006.\n[12] J. Mairal, F. Bach, J. Ponce, and G. Shapiro, \u201cOnline Learning for Matrix\nFactorization and Sparse Coding,\u201d J. Mach. Learn. Res., vol. 11, pp. 19\u2013\n60, 2010.\n[13] M. Elad and M. Aharon, \u201cImage Denoising Via Sparse and Redundant\nRepresentations Over Learned Dictionaries,\u201d IEEE Trans. Image Pro-\ncess., vol. 15, no. 12, pp. 3736\u20133745, 2006.\n[14] S. Beckouche, J. L. Starck, and J. Fadili, \u201cAstronomical image denoising\nusing dictionary learning,\u201d Astron. Astrophys., vol. 556, no. A132, 2013.\n[15] F. Couzinie-Devy, J. Mairal, F. Bach, and J. Ponce, \u201cDictionary Learning\nfor Deblurring and Digital Zoom,\u201d arXiv:1110.0957 [cs.LG], 2011.\n[16] M. Yaghoobi, T. Blumensath, and M. Davies, \u201cRegularized Dictionary\nLearning for Sparse Approximation,\u201d in Proc. EUSIPCO\u201908, 2008.\n[17] M. G. Jafari and M. D. Plumbley, \u201cFast Dictionary Learning for Sparse\nRepresentation of Speech Signals,\u201d IEEE J. Sel. Top. Signa., vol. 5,\nno. 5, pp. 1025\u20131031, 2011.\n[18] D. Barchiesi and M. D. Plumbley, \u201cLearning Incoherent Dictionaries for\nSparse Approximation Using Iterative Projections and Rotations,\u201d IEEE\nTrans. Signal Process., vol. 61, no. 8, pp. 2055\u20132065, 2013.\n[19] S. Lesage, R. Gribonval, F. Bimbot, and L. Benaroya, \u201cLearning unions\nof orthonormal bases with thresholded singular value decompositon,\u201d in\nProc. IEEE ICASSP\u201905, 2005, vol. 5, pp. v/293\u2013v/296.\n[20] R. Rubinstein, M. Zibulevsky, and M. Elad, \u201cDouble Sparsity: Learning\nSparse Dictionaries for Sparse Signal Approximation,\u201d IEEE Trans.\nSignal Process., vol. 58, no. 3, pp. 1553\u20131564, 2010.\n[21] M. D. Plumbley, \u201cDictionary Learning for L1-Exact Sparse Coding,\u201d in\nIndependent Component Analysis and Signal Separation (Proc. ICA\u201907),\nser. Lect. Notes Comput. Sc.\nSpringer, 2007, vol. 4666, pp. 406\u2013413.\n[22] K. Engan, S. O. Aase, and J. H. Hus\u00f8y, \u201cMethod of Optimal Directions\nfor Frame Design,\u201d in Proc. IEEE ICASSP\u201999, 1999, vol. 5, pp. 2443\u2013\n2446.\n[23] M. Aharon, \u201cOvercomplete Dictionaries for Sparse Representation of\nSignals,\u201d Ph.D. dissertation, Technion \u2013 Israel Institute of Technology,\nHaifa, Israel, 2006.\n[24] M. Aharon, M. Elad, and A. M. Bruckstein, \u201cK-SVD: An Algorithm\nfor Designing of Overcomplete Dictionaries for Sparse Representations,\u201d\nIEEE Trans. Signal Process., vol. 54, no. 11, pp. 4311\u20134322, 2006.\n[25] R. Gribonval and K. Schnass, \u201cDictionary Identi\ufb01cation \u2013 Sparse Matrix-\nFactorization via \u21131-Minimization,\u201d IEEE Trans. Inform. Theory, vol. 56,\nno. 7, pp. 3523\u20133539, 2010.\n[26] K. Kreutz-Delgado, J. F. Murray, B. D. Rao, K. Engan, T.-W. Lee, and\nT. J. Sejnowski, \u201cDictionary Learning Algorithms for Sparse Represen-\ntation,\u201d Neural Comput., vol. 15, no. 2, pp. 349\u2013396, 2003.\n[27] A. Rakotomamonjy, \u201cDirect Optimization of the Dictionary Learning\nProblem,\u201d IEEE Trans. Signal Process., vol. 61, no. 22, pp. 5495\u20135506,\n2013.\n[28] S. Arora, A. Bhaskara, R. Ge, and T.Ma, \u201cMore Algorithms for Provable\nDictionary Learning,\u201d arXiv:1401.0579 [cs.DS], 2014.\n[29] B. Korte and J. Vygen, Combinatorial Optimization. Theory and Algo-\nrithms, 5th ed., ser. Algorithms and Combinatorics.\nSpringer, 2011,\nvol. 21.\n[30] S. T. McCormick, \u201cA Combinatorial Approach to some Sparse Matrix\nProblems,\u201d Ph.D. dissertation, Stanford University, CA, USA, 1983.\n[31] T. F. Coleman and A. Pothen, \u201cThe Sparse Null Space Basis Problem,\u201d\nCornell University, Ithaca, NY, USA, Tech. Rep. TR 84-598, 1984.\n[32] M. Gr\u00f6tschel, L. Lov\u00e1sz, and A. Schrijver, Geometric Algorithms and\nCombinatorial Optimization, 2nd ed., ser. Algorithms and Combina-\ntorics.\nSpringer, 1993, vol. 2.\n[33] D. A. Spielman, H. Wang, and J. Wright, \u201cExact Recovery of Sparsely-\nUsed Dictionaries,\u201d J. Mach. Learn. Res., vol. 23, pp. 37.1\u201337.18, 2012.\n[34] A. Agarwal, A. Anandkumar, P. Jain, P. Netrapalli, and R. Tandon,\n\u201cLearning Sparsely Used Overcomplete Dictionaries via Alternating\nMinimization,\u201d arXiv:1310.7991 [cs.LG], 2013.\n[35] S. Arora, L. Babai, J. Stern, and Z. Sweedyk, \u201cThe Hardness of Approx-\nimate Optima in Lattices, Codes, and Systems of Linear Equations,\u201d J.\nComput. Syst. Sci., vol. 54, no. 2, pp. 317\u2013331, 1997.\n[36] L. Trevisan, \u201cInapproximability of Combinatorial Optimization Prob-\nlems,\u201d in Paradigms of Combinatorial Optimizations: Problems and New\nApproaches, V. T. Paschos, Ed.\nJohn Wiley & Sons, 2013, vol. 2, pp.\n381\u2013434.\n[37] H. Buhrman and S. Homer, \u201cSuperpolynomial Circuits, Almost Sparse\nOracles and the Exponential Hierarchy,\u201d in Proc. FSTTCS 12, ser. Lect.\nNotes Comput. Sci.\nSpringer, 1992, vol. 652, pp. 116\u2013127.\n[38] R. Impagliazzo and R. Paturi, \u201cOn the Complexity of k-SAT,\u201d J. Comput.\nSyst. Sci., vol. 62, pp. 367\u2013375, 2001.\n[39] S. Arora and C. Lund, \u201cHardness of Approximations,\u201d in Approximation\nAlgorithms for NP-hard Problems, D. Hochbaum, Ed. PWS Publishing,\n1996, pp. 399\u2013446.\n[40] L.-A. Gottlieb and T. Neylon, \u201cMatrix Sparsi\ufb01cation and the Sparse Null\nSpace Problem,\u201d in Approximation, Randomization, and Combinatorial\nOptimization. Algorithms and Techniques (Proc. APPROX\u201910 and RAN-\nDOM\u201910), ser. Lect. Notes Comput. Sci.\nSpringer, 2010, vol. 6302,\npp. 205\u2013218.\n[41] R. Rubinstein, T. Peleg, and M. Elad, \u201cAnalysis K-SVD: A Dictionary-\nLearning Algorithm for the Analysis Sparse Model,\u201d IEEE Trans. Signal\nProcess., vol. 61, no. 3, pp. 661\u2013677, 2013.\n[42] M. Yaghoobi, S. Nam, R. Gribonval, and M. E. Davies, \u201cAnalysis\nOperator Learning for Overcomplete Cosparse Representations,\u201d in Proc.\nEUSIPCO\u201911, 2011.\n[43] V. Emiya, A. Bonnefoy, L. Daudet, and R. Gribonval, \u201cCompressed\nSensing with Unknown Sensor Permutation,\u201d in Proc. IEEE ICASSP\u201914,\n2014, pp. 1040\u20131044.\n[44] D. Moshkovitz, \u201cThe Projection Games Conjecture and the NP-Hardness\nof ln n-Approximating Set-Cover,\u201d Preprint, 2014. [Online]. Available:\nhttp://people.csail.mit.edu/dmoshkov/papers/set-cover/set-cover-full.pdf\n[45] M. Bellare, S. Goldwasser, C. Lung, and A. Russell, \u201cEf\ufb01cient Proba-\nbilistically Checkable Proofs with Applications to Approximation Prob-\nlems,\u201d in Proc. 25th ACM Symp. Theory Comput., 1993, pp. 294\u2013304.\n[46] M. Razaviyayn, H.-W. Tseng, and Z.-Q. Luo, \u201cDictionary Learning\nfor Sparse Representation: Complexity and Algorithms,\u201d in Proc. IEEE\nICASSP\u201914, 2014, pp. 5247\u20135251.\n",
        "sentence": " However, the dictionary learning problem is NP-hard and it is also hard to find approximate solutions near the optimal sparsity level [13]. This problem is NP-hard [13] and cannot be solved explicitly.",
        "context": "which the training signals can be represented with optimal\nsparsity is indeed a computationally intractable problem.\nTheorem 2: Solving the dictionary learning problem (2) is\nNP-hard in the strong sense, even when restricting n = m.\nwith some constant approximation guarantee. However, we\nshow below that it is almost-NP-hard to approximate the\ndictionary learning problem (2) to within large factors of the\noptimal achievable sparsity of representations. Almost-NP-\napproximating the solution to within large factors of the optimal\nsparsity level. Furthermore, we give NP-hardness and non-\napproximability results for a recent dictionary learning variation\ncalled the sensor permutation problem. Along the way, we also"
    },
    {
        "title": "Emergence of simple-cell receptive field properties by learning a sparse code for natural images",
        "author": [
            "B. Olshausen",
            "D. Field"
        ],
        "venue": "Nature, vol. 381, pp. 607\u2013609, 1996.",
        "citeRegEx": "14",
        "shortCiteRegEx": null,
        "year": 1996,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " The development of dictionary learning methods was stimulated by results presented in the mid \u201990s by Olshausen and Field [14], [15], which demonstrate that atoms similar to the receptive fields of cells in visual cortex can be learned from natural images by imposing a few general optimization conditions, including sparsity and statistical independence of atoms.",
        "context": null
    },
    {
        "title": "Learning overcomplete representations",
        "author": [
            "M.S. Lewicki",
            "T.J. Sejnowski"
        ],
        "venue": "Neural Computation, vol. 12, no. 2, pp. 337\u2013365, March 2000.",
        "citeRegEx": "16",
        "shortCiteRegEx": null,
        "year": 2000,
        "abstract": " In an overcomplete basis, the number of basis vectors is greater than the dimensionality of the input, and the representation of an input is not a unique combination of basis vectors. Overcomplete representations have been advocated because they have greater robustness in the presence of noise, can be sparser, and can have greater flexibility in matching structure in the data. Overcomplete codes have also been proposed as a model of some of the response properties of neurons in primary visual cortex. Previous work has focused on finding the best representation of a signal using a fixed overcomplete basis (or dictionary). We present an algorithm for learning an overcomplete basis by viewing it as probabilistic model of the observed data. We show that overcomplete bases can yield a better approximation of the underlying statistical distribution of the data and can thus lead to greater coding efficiency. This can be viewed as a generalization of the technique of independent component analysis and provides a method for Bayesian reconstruction of signals in the presence of noise and for blind source separation when there are more sources than mixtures. ",
        "full_text": "",
        "sentence": " likelihood of the data, as for example in [16], or the posterior probability of the dictionary, as in [17]. The starting point is the dictionary learning method introduced by Smith and Lewicki [28], where the sparse code is generated with Matching Pursuit (MP) [29], [30] and the shiftinvariant dictionary is updated with probabilistic gradient ascent on the likelihood of the data [16]. that is generated by the matching pursuit [16], [28], [29].",
        "context": null
    },
    {
        "title": "Dictionary learning algorithms for sparse representation",
        "author": [
            "K. Kreutz-Delgado",
            "J.F. Murray",
            "B.D. Rao",
            "K. Engan",
            "T.-W. Lee",
            "T.J. Sejnowski"
        ],
        "venue": "Neural Computation, vol. 15, no. 2, pp. 349\u2013396, 2016/02/03 2003. [Online]. Available: http://dx.doi.org/10. 1162/089976603762552951",
        "citeRegEx": "17",
        "shortCiteRegEx": null,
        "year": 2016,
        "abstract": " Algorithms for data-driven learning of domain-specific overcomplete dictionaries are developed to obtain maximum likelihood and maximum a posteriori dictionary estimates based on the use of Bayesian models with concave/Schur-concave (CSC) negative log priors. Such priors are appropriate for obtaining sparse representations of environmental signals within an appropriately chosen (environmentally matched) dictionary. The elements of the dictionary can be interpreted as concepts, features, or words capable of succinct expression of events encountered in the environment (the source of the measured signals). This is a generalization of vector quantization in that one is interested in a description involving a few dictionary entries (the proverbial \u201c25 words or less\u201d), but not necessarily as succinct as one entry. To learn an environmentally adapted dictionary capable of concise expression of signals generated by the environment, we develop algorithms that iterate between a representative set of sparse representations found by variants of FOCUSS and an update of the dictionary using these sparse representations.  Experiments were performed using synthetic data and natural images. For complete dictionaries, we demonstrate that our algorithms have improved performance over other independent component analysis (ICA) methods, measured in terms of signal-to-noise ratios of separated sources. In the overcomplete case, we show that the true underlying dictionary and sparse sources can be accurately recovered. In tests with natural images, learned overcomplete dictionaries are shown to have higher coding efficiency than complete dictionaries; that is, images encoded with an overcomplete dictionary have both higher compression (fewer bits per pixel) and higher accuracy (lower mean square error). ",
        "full_text": "",
        "sentence": " likelihood of the data, as for example in [16], or the posterior probability of the dictionary, as in [17].",
        "context": null
    },
    {
        "title": "An online algorithm for distributed dictionary learning",
        "author": [
            "S. Chouvardas",
            "Y. Kopsinis",
            "S. Theodoridis"
        ],
        "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2015 IEEE International Conference on, April 2015, pp. 3292\u20133296.",
        "citeRegEx": "18",
        "shortCiteRegEx": null,
        "year": 2015,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " Recent developments include extensions of dictionary learning methods to distributed systems [18] and low-power hardware [19].",
        "context": null
    },
    {
        "title": "On-chip sparse learning acceleration with cmos and resistive synaptic devices",
        "author": [
            "J. sun Seo",
            "B. Lin",
            "M. Kim",
            "P.-Y. Chen",
            "D. Kadetotad",
            "Z. Xu",
            "A. Mohanty",
            "S. Vrudhula",
            "S. Yu",
            "J. Ye",
            "Y. Cao"
        ],
        "venue": "Nanotechnology, IEEE Transactions on, vol. 14, no. 6, pp. 969\u2013979, Nov 2015.",
        "citeRegEx": "19",
        "shortCiteRegEx": null,
        "year": 2015,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " Recent developments include extensions of dictionary learning methods to distributed systems [18] and low-power hardware [19].",
        "context": null
    },
    {
        "title": "Adaptation of the simple or complex nature of v1 receptive fields to visual statistics",
        "author": [
            "J. Fournier",
            "C. Monier",
            "M. Pananceau",
            "Y. Fregnac"
        ],
        "venue": "Nature Neuroscience, vol. 14, no. 8, pp. 1053\u20131060, Aug 2011.",
        "citeRegEx": "20",
        "shortCiteRegEx": null,
        "year": 2011,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " For instance, neurons demonstrate a form of homeostatic adaptation of the functional properties of the network to the ongoing changes in the statistical structure of the sensory input [20], [21], [22], which may be related to optimal encoding [23] of the input by dynamic adaptation of the receptive fields.",
        "context": null
    },
    {
        "title": "Spatiotemporal receptive fields of barrel cortex revealed by reverse correlation of synaptic input",
        "author": [
            "A. Ramirez",
            "E.A. Pnevmatikakis",
            "J. Merel",
            "L. Paninski",
            "K.D. Miller",
            "R.M. Bruno"
        ],
        "venue": "Nature Neuroscience, vol. 17, no. 6, pp. 866\u2013875, Jun 2014.",
        "citeRegEx": "21",
        "shortCiteRegEx": null,
        "year": 2014,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " For instance, neurons demonstrate a form of homeostatic adaptation of the functional properties of the network to the ongoing changes in the statistical structure of the sensory input [20], [21], [22], which may be related to optimal encoding [23] of the input by dynamic adaptation of the receptive fields.",
        "context": null
    },
    {
        "title": "Hidden complexity of synaptic receptive fields in cat v1",
        "author": [
            "J. Fournier",
            "C. Monier",
            "M. Levy",
            "O. Marre",
            "K. Sri",
            "Z.F. Kisvrday",
            "Y. Frgnac"
        ],
        "venue": "The Journal of Neuroscience, vol. 34, no. 16, pp. 5515\u20135528, 2014.",
        "citeRegEx": "22",
        "shortCiteRegEx": null,
        "year": 2014,
        "abstract": "In the primary visual cortex (V1), Simple and Complex receptive fields (RFs) are usually characterized on the basis of the linearity of the cell spiking response to stimuli of opposite contrast. Whether or not this classification reflects a functional dichotomy in the synaptic inputs to Simple and Complex cells is still an open issue. Here we combined intracellular membrane potential recordings in cat V1 with 2D dense noise stimulation to decompose the Simple-like and Complex-like components of the subthreshold RF into a parallel set of functionally distinct subunits. Results show that both Simple and Complex RFs exhibit a remarkable diversity of excitatory and inhibitory Complex-like contributions, which differ in orientation and spatial frequency selectivity from the linear RF, even in layer 4 and layer 6 Simple cells. We further show that the diversity of Complex-like contributions recovered at the subthreshold level is expressed in the cell spiking output. These results demonstrate that the Simple or Complex nature of V1 RFs does not rely on the diversity of Complex-like components received by the cell from its synaptic afferents but on the imbalance between the weights of the Simple-like and Complex-like synaptic contributions.",
        "full_text": "",
        "sentence": " For instance, neurons demonstrate a form of homeostatic adaptation of the functional properties of the network to the ongoing changes in the statistical structure of the sensory input [20], [21], [22], which may be related to optimal encoding [23] of the input by dynamic adaptation of the receptive fields.",
        "context": null
    },
    {
        "title": "Visual adaptation as optimal information transmission",
        "author": [
            "M.J. Wainwright"
        ],
        "venue": "Vision Research, vol. 39, no. 23, pp. 3960 \u2013 3974, 1999.",
        "citeRegEx": "23",
        "shortCiteRegEx": null,
        "year": 1999,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " For instance, neurons demonstrate a form of homeostatic adaptation of the functional properties of the network to the ongoing changes in the statistical structure of the sensory input [20], [21], [22], which may be related to optimal encoding [23] of the input by dynamic adaptation of the receptive fields.",
        "context": null
    },
    {
        "title": "The criticality hypothesis: how local cortical networks might optimize information processing",
        "author": [
            "J.M. Beggs"
        ],
        "venue": "Philosophical Transactions of the Royal Society of London A: Mathematical, Physical and Engineering Sciences, vol. 366, no. 1864, pp. 329\u2013343, 2008. [Online]. Available: http://rsta.royalsocietypublishing.org/content/366/1864/329",
        "citeRegEx": "24",
        "shortCiteRegEx": null,
        "year": 1864,
        "abstract": "Early theoretical and simulation work independently undertaken by Packard, Langton and Kauffman suggested that adaptability and computational power would be optimized in systems at the \u2018edge of chaos\u2019, at a critical point in a phase transition between total randomness and boring order. This provocative hypothesis has received much attention, but biological experiments supporting it have been relatively few. Here, we review recent experiments on networks of cortical neurons, showing that they appear to be operating near the critical point. Simulation studies capture the main features of these data and suggest that criticality may allow cortical networks to optimize information processing. These simulations lead to predictions that could be tested in the near future, possibly providing further experimental evidence for the criticality hypothesis.",
        "full_text": "",
        "sentence": " Furthermore, there is a notion that homeostatic mechanisms serve to maintain the dynamics of cortical networks at a critical point [24], [25] where the dynamic ar X iv :1 61 1.",
        "context": null
    },
    {
        "title": "Being critical of criticality in the brain",
        "author": [
            "J.M. Beggs",
            "N. Timme"
        ],
        "venue": "Frontiers in Physiology, vol. 3, no. 163, 2012. [Online]. Available: http://www.frontiersin.org/fractal physiology/10.3389/fphys. 2012.00163/abstract",
        "citeRegEx": "25",
        "shortCiteRegEx": null,
        "year": 2012,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " Furthermore, there is a notion that homeostatic mechanisms serve to maintain the dynamics of cortical networks at a critical point [24], [25] where the dynamic ar X iv :1 61 1.",
        "context": null
    },
    {
        "title": "The functional benefits of criticality in the cortex",
        "author": [
            "W.L. Shew",
            "D. Plenz"
        ],
        "venue": "The Neuroscientist, vol. 19, no. 1, pp. 88\u2013100, 2013. [Online]. Available: http://nro.sagepub.com/content/19/1/88.abstract",
        "citeRegEx": "26",
        "shortCiteRegEx": null,
        "year": 2013,
        "abstract": " Rapidly growing empirical evidence supports the hypothesis that the cortex operates near criticality. Although the confirmation of this hypothesis would mark a significant advance in fundamental understanding of cortical physiology, a natural question arises: What functional benefits are endowed to cortical circuits that operate at criticality? In this review, we first describe an introductory-level thought experiment to provide the reader with an intuitive understanding of criticality. Second, we discuss some practical approaches for investigating criticality. Finally, we review quantitative evidence that three functional properties of the cortex are optimized at criticality: 1) dynamic range, 2) information transmission, and 3) information capacity. We focus on recently reported experimental evidence and briefly discuss the theory and history of these ideas. ",
        "full_text": "",
        "sentence": " range and information processing capacity are optimal [26], [27].",
        "context": null
    },
    {
        "title": "Quasicritical brain dynamics on a nonequilibrium widom line",
        "author": [
            "R.V. Williams-Garc\u0131\u0301a",
            "M. Moore",
            "J.M. Beggs",
            "G. Ortiz"
        ],
        "venue": "Phys. Rev. E, vol. 90, p. 062714, Dec 2014. [Online]. Available: http://link.aps.org/doi/10.1103/PhysRevE.90.062714",
        "citeRegEx": "27",
        "shortCiteRegEx": null,
        "year": 2014,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " range and information processing capacity are optimal [26], [27].",
        "context": null
    },
    {
        "title": "Efficient auditory coding",
        "author": [
            "E. Smith",
            "M.S. Lewicki"
        ],
        "venue": "Nature, no. 7079, pp. 978\u2013982, 02.",
        "citeRegEx": "28",
        "shortCiteRegEx": null,
        "year": 0,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " The starting point is the dictionary learning method introduced by Smith and Lewicki [28], where the sparse code is generated with Matching Pursuit (MP) [29], [30] and the shiftinvariant dictionary is updated with probabilistic gradient ascent on the likelihood of the data [16]. that is generated by the matching pursuit [16], [28], [29]. Under these assumptions the atoms can be optimized by performing gradient ascent on the approximate log data probability [28]. We extend a well-known dictionary learning and sparse representation model [28] with a basic homeostatic regulation mechanism.",
        "context": null
    },
    {
        "title": "Efficient coding of time-relative structure using spikes",
        "author": [
            "E. Smith",
            "M. Lewicki"
        ],
        "venue": "Neural Computation, vol. 17, no. 1, pp. 19\u201345, 2005.",
        "citeRegEx": "29",
        "shortCiteRegEx": null,
        "year": 2005,
        "abstract": " Nonstationary acoustic features provide essential cues for many auditory tasks, including sound localization, auditory stream analysis, and speech recognition. These features can best be characterized relative to a precise point in time, such as the onset of a sound or the beginning of a harmonic periodicity. Extracting these types of features is a difficult problem. Part of the difficulty is that with standard block-based signal analysis methods, the representation is sensitive to the arbitrary alignment of the blocks with respect to the signal. Convolutional techniques such as shift-invariant transformations can reduce this sensitivity, but these do not yield a code that is efficient, that is, one that forms a nonredundant representation of the underlying structure. Here, we develop a non-block-based method for signal representation that is both time relative and efficient. Signals are represented using a linear superposition of time-shiftable kernel functions, each with an associated magnitude and temporal position. Signal decomposition in this method is a non-linear process that consists of optimizing the kernel function scaling coefficients and temporal positions to form an efficient, shift-invariant representation. We demonstrate the properties of this representation for the purpose of characterizing structure in various types of nonstationary acoustic signals. The computational problem investigated here has direct relevance to the neural coding at the auditory nerve and the more general issue of how to encode complex, time-varying signals with a population of spiking neurons. ",
        "full_text": "",
        "sentence": " The starting point is the dictionary learning method introduced by Smith and Lewicki [28], where the sparse code is generated with Matching Pursuit (MP) [29], [30] and the shiftinvariant dictionary is updated with probabilistic gradient ascent on the likelihood of the data [16]. that is generated by the matching pursuit [16], [28], [29].",
        "context": null
    },
    {
        "title": "Matching pursuits with time-frequency dictionaries",
        "author": [
            "S. Mallat",
            "Z. Zhang"
        ],
        "venue": "IEEE T Signal Proces, 1993.",
        "citeRegEx": "30",
        "shortCiteRegEx": null,
        "year": 1993,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " The starting point is the dictionary learning method introduced by Smith and Lewicki [28], where the sparse code is generated with Matching Pursuit (MP) [29], [30] and the shiftinvariant dictionary is updated with probabilistic gradient ascent on the likelihood of the data [16]. In addition to MP we consider dictionary learning with Orthogonal MP (OMP) [30], [31], which in the case of shift-invariant dictionaries is applicable to high-dimensional signals in the form of local OMP [32]. The atom selection rule defined above is optimal for sparse decomposition of a signal with a constant dictionary [30], but it does not imply optimal dictionary learning. The sparse representations are generated with Matching Pursuit (MP) [30], Local Orthogonal MP (OMP) [32] and the homeostatic extensions Equiprobable MP (E\u2013MP) and Equiprobable OMP (E\u2013OMP) introduced here.",
        "context": null
    },
    {
        "title": "Orthogonal matching pursuit: recursive function approximation with applications to wavelet decomposition",
        "author": [
            "Y. Pati",
            "R. Rezaiifar",
            "P. Krishnaprasad"
        ],
        "venue": "Signals, Systems and Computers, 1993. 1993 Conference Record of The Twenty-Seventh Asilomar Conference on, Nov 1993, pp. 40\u201344 vol.1.",
        "citeRegEx": "31",
        "shortCiteRegEx": null,
        "year": 1993,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " In addition to MP we consider dictionary learning with Orthogonal MP (OMP) [30], [31], which in the case of shift-invariant dictionaries is applicable to high-dimensional signals in the form of local OMP [32].",
        "context": null
    },
    {
        "title": "A low complexity orthogonal matching pursuit for sparse signal approximation with shift-invariant dictionaries",
        "author": [
            "B. Mailhe",
            "R. Gribonval",
            "F. Bimbot",
            "P. Vandergheynst"
        ],
        "venue": "Acoustics, Speech and Signal Processing, 2009. ICASSP 2009. IEEE International Conference on, April 2009, pp. 3445\u20133448.",
        "citeRegEx": "32",
        "shortCiteRegEx": null,
        "year": 2009,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " In addition to MP we consider dictionary learning with Orthogonal MP (OMP) [30], [31], which in the case of shift-invariant dictionaries is applicable to high-dimensional signals in the form of local OMP [32]. MP optimizes the parameters \u03c4i,j and ai,j of the most recently selected atom, while local OMP [32] re-optimizes all ai,j for selected atoms with overlapping support in each iteration (OMP compensates for the interference between atom instances). Our implementation of MP and OMP are based on efficient computational methods like those described in [32] and [34]. The resulting four matching pursuits are summarized in Table I and Algorithm 1, which is a straightforward extension of the local OMP algorithm presented in [32] to equiprobable atom selection. The sparse representations are generated with Matching Pursuit (MP) [30], Local Orthogonal MP (OMP) [32] and the homeostatic extensions Equiprobable MP (E\u2013MP) and Equiprobable OMP (E\u2013OMP) introduced here.",
        "context": null
    },
    {
        "title": "Probabilistic framework for the adaptation and comparison of image codes",
        "author": [
            "M.S. Lewicki",
            "B.A. Olshausen"
        ],
        "venue": "J. Opt. Soc. Am. A, vol. 16, no. 7, pp. 1587\u20131601, Jul 1999. [Online]. Available: http://josaa.osa.org/abstract.cfm?URI=josaa-16-7-1587",
        "citeRegEx": "33",
        "shortCiteRegEx": null,
        "year": 1999,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " two-step iteration process is used [33]: A) Encoding step; optimize the sparse representation of the signal x(t) with MP or OMP and a constant dictionary \u03a6.",
        "context": null
    },
    {
        "title": "MPTK: Matching Pursuit made tractable",
        "author": [
            "S. Krstulovic",
            "R. Gribonval"
        ],
        "venue": "Proc. Int. Conf. Acoust. Speech Signal Process. (ICASSP\u201906), vol. 3, May 2006, pp. III\u2013496 \u2013 III\u2013499.",
        "citeRegEx": "34",
        "shortCiteRegEx": null,
        "year": 2006,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " Our implementation of MP and OMP are based on efficient computational methods like those described in [32] and [34].",
        "context": null
    },
    {
        "title": "After the last",
        "author": [
            "The Red Thread"
        ],
        "venue": "licensed under CC BY-NC 3.0. [Online]. Available: http://freemusicarchive.org",
        "citeRegEx": "35",
        "shortCiteRegEx": null,
        "year": 0,
        "abstract": "",
        "full_text": "",
        "sentence": " 1 kHz rock music track with lyrics [35] and the second signal is a 26 seconds long 48 kHz recording of Zebra Finch song phrases [36].",
        "context": null
    },
    {
        "title": "Song phrases of Zebra Finch",
        "author": [
            "M. Anderson"
        ],
        "venue": "cat. nr. XC287103. [Online]. Available: www.xeno-canto.org/287103/download",
        "citeRegEx": "36",
        "shortCiteRegEx": null,
        "year": 2871,
        "abstract": "",
        "full_text": "",
        "sentence": " 1 kHz rock music track with lyrics [35] and the second signal is a 26 seconds long 48 kHz recording of Zebra Finch song phrases [36].",
        "context": null
    },
    {
        "title": "Inhibitory interneurons decorrelate excitatory cells to drive sparse code formation in a spiking model of v1",
        "author": [
            "P.D. King",
            "J. Zylberberg",
            "M.R. DeWeese"
        ],
        "venue": "Journal of Neuroscience, vol. 33, no. 13, pp. 5475\u20135485, 2013.",
        "citeRegEx": "37",
        "shortCiteRegEx": null,
        "year": 2013,
        "abstract": "Sparse coding models of natural scenes can account for several physiological properties of primary visual cortex (V1), including the shapes of simple cell receptive fields (RFs) and the highly kurtotic firing rates of V1 neurons. Current spiking network models of pattern learning and sparse coding require direct inhibitory connections between the excitatory simple cells, in conflict with the physiological distinction between excitatory (glutamatergic) and inhibitory (GABAergic) neurons (Dale's Law). At the same time, the computational role of inhibitory neurons in cortical microcircuit function has yet to be fully explained. Here we show that adding a separate population of inhibitory neurons to a spiking model of V1 provides conformance to Dale's Law, proposes a computational role for at least one class of interneurons, and accounts for certain observed physiological properties in V1. When trained on natural images, this excitatory\u2013inhibitory spiking circuit learns a sparse code with Gabor-like RFs as found in V1 using only local synaptic plasticity rules. The inhibitory neurons enable sparse code formation by suppressing predictable spikes, which actively decorrelates the excitatory population. The model predicts that only a small number of inhibitory cells is required relative to excitatory cells and that excitatory and inhibitory input should be correlated, in agreement with experimental findings in visual cortex. We also introduce a novel local learning rule that measures stimulus-dependent correlations between neurons to support \u201cexplaining away\u201d mechanisms in neural coding.",
        "full_text": "",
        "sentence": " that the information entropy of such sparse representations is sub-optimal by construction, and partially also by the central role of homeostatic regulation in cortical networks and spiking neural network models of sensory areas, see for example [37], [38].",
        "context": null
    },
    {
        "title": "Nonlinear hebbian learning as a unifying principle in receptive field formation",
        "author": [
            "C.S.N. Brito",
            "W. Gerstner"
        ],
        "venue": "2016. [Online]. Available: http://arxiv.org/abs/1601.00701",
        "citeRegEx": "38",
        "shortCiteRegEx": null,
        "year": 2016,
        "abstract": "The development of sensory receptive fields has been modeled in the past by a\nvariety of models including normative models such as sparse coding or\nindependent component analysis and bottom-up models such as spike-timing\ndependent plasticity or the Bienenstock-Cooper-Munro model of synaptic\nplasticity. Here we show that the above variety of approaches can all be\nunified into a single common principle, namely Nonlinear Hebbian Learning. When\nNonlinear Hebbian Learning is applied to natural images, receptive field shapes\nwere strongly constrained by the input statistics and preprocessing, but\nexhibited only modest variation across different choices of nonlinearities in\nneuron models or synaptic plasticity rules. Neither overcompleteness nor sparse\nnetwork activity are necessary for the development of localized receptive\nfields. The analysis of alternative sensory modalities such as auditory models\nor V2 development lead to the same conclusions. In all examples, receptive\nfields can be predicted a priori by reformulating an abstract model as\nnonlinear Hebbian learning. Thus nonlinear Hebbian learning and natural\nstatistics can account for many aspects of receptive field formation across\nmodels and sensory modalities.",
        "full_text": "Nonlinear Hebbian learning as a unifying principle\nin receptive \ufb01eld formation\nCarlos S. N. Brito*, Wulfram Gerstner\nSchool of Computer and Communication Sciences and School of Life Sciences,\nEcole Polytechnique Federale de Lausanne (EPFL), Switzerland\n*Corresponding author: carlos.stein@ep\ufb02.ch\nAbstract\nThe development of sensory receptive \ufb01elds has been modeled in the past by a\nvariety of models including normative models such as sparse coding or indepen-\ndent component analysis and bottom-up models such as spike-timing dependent\nplasticity or the Bienenstock-Cooper-Munro model of synaptic plasticity. Here we\nshow that the above variety of approaches can all be uni\ufb01ed into a single common\nprinciple, namely Nonlinear Hebbian Learning. When Nonlinear Hebbian Learning\nis applied to natural images, receptive \ufb01eld shapes were strongly constrained by\nthe input statistics and preprocessing, but exhibited only modest variation across\ndifferent choices of nonlinearities in neuron models or synaptic plasticity rules.\nNeither overcompleteness nor sparse network activity are necessary for the de-\nvelopment of localized receptive \ufb01elds. The analysis of alternative sensory modal-\nities such as auditory models or V2 development lead to the same conclusions.\nIn all examples, receptive \ufb01elds can be predicted a priori by reformulating an ab-\nstract model as nonlinear Hebbian learning. Thus nonlinear Hebbian learning and\nnatural statistics can account for many aspects of receptive \ufb01eld formation across\nmodels and sensory modalities.\n1\narXiv:1601.00701v1  [q-bio.NC]  4 Jan 2016\nIntroduction\nNeurons in sensory areas of the cortex are optimally driven by stimuli with characteristic features\nthat de\ufb01ne the \u2019receptive \ufb01eld\u2019 of the cell. While receptive \ufb01elds of simple cells in primary visual\ncortex (V1) are localized in visual space and sensitive to the orientation of light contrast1, those\nof auditory neurons are sensitive to speci\ufb01c time-frequency patterns in sounds2. The concept of a\nreceptive \ufb01eld is also useful when studying higher-order sensory areas, for instance when analyzing\nthe degree of selectivity and invariance of neurons to stimulus properties3,4.\nThe characteristic receptive \ufb01elds of simple cells in V1 have been related to statistical properties\nof natural images5.\nThese \ufb01ndings inspired various models, based on principles as diverse as\nsparse sensory representations6, optimal information transmission7, or synaptic plasticity8. Several\nstudies highlighted possible connections between biological and normative justi\ufb01cations of sensory\nreceptive \ufb01elds9\u201312, not only in V1, but also in other sensory areas13, such as auditory14,15 and\nsecondary visual cortex (V2)16.\nSince disparate models appear to achieve similar results, the question arises whether there ex-\nists a general underlying concept in unsupervised learning models15,17. Here we show that the\nprinciple of nonlinear Hebbian learning is suf\ufb01cient for receptive \ufb01eld development under rather gen-\neral conditions. The nonlinearity is de\ufb01ned by the neuron\u2019s f-I curve combined with the nonlinearity\nof the plasticity function. The outcome of such nonlinear learning is equivalent to projection pur-\nsuit18\u201320, which focuses on features with non-trivial statistical structure, and therefore links receptive\n\ufb01eld development to optimality principles.\nHere we unify and broaden the above concepts and show that plastic neural networks, sparse\ncoding models and independent component analysis can all be reformulated as nonlinear Hebbian\nlearning. For natural images as sensory input, we \ufb01nd that a broad class of nonlinear Hebbian rules\nlead to orientation selective receptive \ufb01elds, and explain how seemingly disparate approaches may\nlead to similar receptive \ufb01elds. The theory predicts the diversity of receptive \ufb01eld shapes obtained\nin simulations for several different families of nonlinearities. The robustness to model assumptions\nalso applies to alternative sensory modalities, implying that the statistical properties of the input\nstrongly constrain the type of receptive \ufb01elds that can be learned. Since the conclusions are robust\nto speci\ufb01c properties of neurons and plasticity mechanisms, our results support the idea that synaptic\nplasticity can be interpreted as nonlinear Hebbian learning, implementing a statistical optimization\n2\nof the neuron\u2019s receptive \ufb01eld properties.\nResults\nThe effective Hebbian nonlinearity\nIn classic rate models of sensory development6,8,21, a \ufb01rst layer of neurons, representing the sensory\ninput x, is connected to a downstream neuron with activity y, through synaptic connections with\nweights w (Fig. 1a). The response to a speci\ufb01c input is y = g(wTx), where g is the frequency-\ncurrent (f-I) curve. In most models of Hebbian plasticity22,23, synaptic changes \u2206w of the connection\nweights depend on pre- and post-synaptic activity, with a linear dependence on the pre-synaptic and\na nonlinear dependence on the post-synaptic activity, \u2206w \u221dx h(y), in accordance with models of\npairing experiments10,24. The learning dynamics arise from a combination of the neuronal f-I curve\ny = g(wTx) and the Hebbian plasticity function \u2206w \u221dx h(y):\n\u2206w \u221dx h(g(wTx)) = x f(wTx)\n(1)\nwhere we de\ufb01ne the effective Hebbian nonlinearity f := h \u25e6g as the composition of the nonlinearity\nin the plasticity rule and the neuron\u2019s f-I curve. In an experimental setting, the pre-synaptic activity x\nis determined by the set of sensory stimuli (in\ufb02uenced by, e.g., the rearing conditions during sensory\ndevelopment25). Therefore, the evolution of synaptic strength, Eq. 1, is determined by the effective\nnonlinearity f and the statistics of the input x.\nMany existing models can be formulated in the framework of Eq. 1. For instance, in a clas-\nsic study of simple-cell formation8, the Bienenstock-Cooper-Munro (BCM) model22 has a quadratic\nplasticity nonlinearity, h(y) = y(y \u2212\u03b8), with a variable plasticity threshold \u03b8, and a sigmoidal f-I\ncurve, \u03c3(wTx), which combine into nonlinear Hebbian learning dynamics, \u2206w \u221dx h(\u03c3(wTx)).\nMore realistic cortical networks have dynamical properties which are not accounted for by rate\nmodels.\nBy analyzing state-of-the-art models of cortical neurons and synaptic plasticity, we in-\nspected whether plastic spiking networks can be reduced to nonlinear Hebbian learning. We consid-\nered a generalized leaky integrate-and-\ufb01re model (GIF), which includes adaptation, stochastic \ufb01ring\n3\nand predicts experimental spikes with high accuracy26, and we approximate its f-I curve by a linear\nrecti\ufb01er, g(u) = a(u \u2212\u03b8)+, with slope a and threshold \u03b8 (Fig. 1b).\nAs a phenomenological model of synaptic plasticity grounded on experimental data27, we imple-\nmented triplet spike-timing dependent plasticity (STDP)24. In this STDP model, the dependence of\nlong-term potentiation (LTP) upon two post-synaptic spikes induces in the corresponding rate model\na quadratic dependence on the post-synaptic rate, while long-term depression (LTD) is linear. The\nresulting rate plasticity24 is h(y) = y2 \u2212by, with an LTD factor b (post-synaptic activity threshold\nseparating LTD from LTP, Fig. 1c), similar to the classic BCM model8,22.\nComposing the f-I curve of the GIF with the h(y) for the triplet plasticity model, we have an\napproximation of the effective learning nonlinearity f = h \u25e6g in cortical spiking neurons (Fig 1d),\nthat can be described as a quadratic recti\ufb01er, with LTD threshold given by \u03b81 = \u03b8 and LTP threshold\ngiven by \u03b82 = \u03b8 + b/a. Interestingly, the f-I slope a and LTD factor b are redundant parameters of the\nlearning dynamics: only their ratio counts in nonlinear Hebbian plasticity. Metaplasticity can control\nthe LTD factor24,28, thus regulating the LTP threshold.\nIf one considers a linear STDP model29,30 instead of the triplet STDP24, the plasticity curve is\nlinear23, as in standard Hebbian learning, and the effective nonlinearity is shaped by the properties\nof the f-I curve (Fig. 2a).\nSparse coding as nonlinear Hebbian learning\nBeyond phenomenological modeling, normative principles that explain receptive \ufb01elds development\nhave been one of the goals of theoretical neuroscience31. Sparse coding6 starts from the assump-\ntions that V1 aims at maximizing the sparseness of the activity in the sensory representation, and\nbecame a well-known normative model to develop orientation selective receptive \ufb01elds9,12,13. We\ndemonstrate that the algorithm implemented in the sparse coding model is in fact a particular exam-\nple of nonlinear Hebbian learning.\nThe sparse coding model aims at minimizing an input reconstruction error E = 1\n2||x \u2212Wy||2 +\n\u03bbS(y), under a sparsity constraint S with relative importance \u03bb > 0. For K hidden neurons yj, such\na model implicitly assumes that the vector wj of feed-forward weights onto neuron j are mirrored by\nhypothetical \"reconstruction weights\", W = [w1 . . . wK]. The resulting encoding algorithm can be\nrecast as a neural model32, if neurons are embedded in a feedforward model with lateral inhibition,\n4\nf := h \u25e6g\n}\nwi\nxi\ny = g(wT x)\n\u2206wi \u221dxi h(g(wT x))\na\nEffective Hebbian nonlinearity\nb\nd\nFiring rate (Hz)\nMean input (nA)\nMean input (nA)\nWeight change\nWeight change\nc\nFiring rate (Hz)\nFigure 1: The effective Hebbian nonlinearity of plastic cortical networks. (a) Receptive \ufb01eld\ndevelopment between an input layer of neurons with activities xi, connected by synaptic projections\nwi to a neuron with \ufb01ring rate y, given by an f-I curve y = g(wTx)). Synaptic connections change\naccording to a Hebbian rule \u2206wi \u221dxi h(y). (b) f-I curve (blue) of a GIF model26 of a pyramidal\nneurons in response to step currents of 500 ms duration (dashed line: piece-wise linear \ufb01t, with\nslope a = 143 Hz/nA and threshold \u03b8 = 0.08 nA). (c) Plasticity function of the triplet STDP model24\n(blue), \ufb01tted to visual cortex plasticity data24,27, showing the weight change \u2206wi as a function of\nthe post-synaptic rate y, under a constant pre-synaptic stimulation xi (dashed line: \ufb01t by quadratic\nfunction, with LTD factor b = 22.1 Hz). (d) The combination of the f-I curve and plasticity function\ngenerates the effective Hebbian nonlinearity (dashed line: quadratic nonlinearity with LTD threshold\n\u03b81 = 0.08 nA, LTP threshold \u03b82 = 0.23 nA).\ny = g(wTx \u2212vTy), where v are inhibitory recurrent synaptic connections (see Methods). In the\ncase of a single output neuron, its \ufb01ring rate is simply y = g(wTx). The nonlinearity g of the f-I curve\nis threshold-like, and determined by the choice of the sparsity constraint32, such as the Cauchy, L0\n, or L1 constraints (Fig 2a, see Methods).\nIf weights are updated through gradient descent so as to minimize E, the resulting plasticity\nrule is Oja\u2019s learning rule33, \u2206w \u221dx y \u2212w y2. The second term \u2212w y2 has a multiplicative\neffect on the strength of synapses projecting onto the same neuron (weight rescaling), but does not\naffect the receptive \ufb01eld shape, whereas the \ufb01rst term x y drives feature selectivity and receptive\n\ufb01eld formation. Together, these derivations imply that the one-unit sparse coding algorithm can be\nimplemented by an effective nonlinear Hebbian rule combined with weight normalization. Although\nthe plasticity mechanism is linear, \u2206w \u221dx y, a nonlinearity arises from the f-I curve, y = g(wTx),\n5\nso that the effective plasticity is\n\u2206w \u221dx g(wTx)\n(2)\nThis analysis reveals an equivalence between sparse coding models and neural networks with\nlinear plasticity mechanisms, where the sparsity constraint is determined by the f-I curve g.\nSimilarly, algorithms performing independent component analysis (ICA), a model class closely\nrelated to sparse coding, also perform effective nonlinear Hebbian learning, albeit inversely, with lin-\near neurons and a nonlinear plasticity rule34. For variants of ICA based on information maximization7\nor kurtosis34 different nonlinearities arise (Fig. 2a), but Eq. 2 applies equally well. Hence, various\ninstantiations of sparse coding and ICA models not only relate to each other in their normative as-\nsumptions35, but when implemented as iterative gradient update rules, they all employ nonlinear\nHebbian learning.\nSimple cell development for a large class of nonlinearities\nSince the models described above can be implemented by similar plasticity rules, we hypothesized\nnonlinear Hebbian learning to be a general principle that explains the development of receptive \ufb01eld\nselectivity. Nonlinear Hebbian learning with an effective nonlinearity f is linked to an optimization\nprinciple with a function F =\n\u00b4\nf 19,20. For an input ensemble x, optimality is achieved by weights \u02dcw\nthat maximize \u27e8F( \u02dcwTx)\u27e9, where angular brackets denote the average over the input statistics. Non-\nlinear Hebbian learning is a stochastic gradient ascent implementation of this optimization process,\ncalled projection pursuit18\u201320:\n\u02dcw = maxw\u27e8F(wTx)\u27e9=\u21d2\u2206w \u221dx f(wTx)\n(3)\nMotivated by results from ICA theory36 and statistical properties of whitened natural images5, we\nselected diverse Hebbian nonlinearities f (Fig. 2a) and calculated the corresponding optimization\nvalue \u27e8F(wTx)\u27e9for different features of interest that we consider as candidate RF shapes, with a\nwhitened ensemble of patches extracted from natural images as input (see Methods). These include\na random connectivity pattern, a non-local oriented edge (as in principal components of natural\nimages) and localized oriented edges (as in cat and monkey simple cells in the visual cortex), shown\n6\nin Fig.\n2b.\nThe relative value of \u27e8F(wTx)\u27e9between one feature and another was remarkably\nconsistent across various choices of the nonlinearity f, with localized orientation-selective receptive\n\ufb01elds as maxima (Fig. 2b). Furthermore, we also searched for the maxima through gradient ascent,\nso as to con\ufb01rm that the maxima are orientation selective (Fig. 2c, left). Our results indicate that\nreceptive \ufb01eld development of simple cells is mainly governed by the statistical properties of natural\nimages, while robust to speci\ufb01c model assumptions.\nThe relevant property of natural image statistics is that the distribution of a feature derived from\ntypical localized oriented patterns has high kurtosis5,6,37. Thus to establish a quantitative measure\nwhether a nonlinearity is suitable for feature learning, we de\ufb01ne a selectivity index (SI), which mea-\nsures the relative value of \u27e8F(.)\u27e9between a variable l with a Laplacian distribution and a variable g\nwith Gaussian distribution36: SI = (\u27e8F(l)\u27e9\u2212\u27e8F(g)\u27e9)/\u03c3F (see Methods). The Laplacian variable has\nhigher kurtosis than the Gaussian variable, serving as a prototype of a kurtotic distribution. Since\nvalues obtained by \ufb01ltering natural images with localized oriented patterns have a distribution with\nlonger tails than other patterns5, as does the Laplacian variable compared to the Gaussian, positive\nvalues SI > 0 indicate good candidate functions for learning simple cell-like receptive \ufb01elds from\nnatural images. We \ufb01nd that each model has an appropriate parameter range where SI > 0 (Fig.\n3). For example the quadratic recti\ufb01er nonlinearity needs an LTP threshold \u03b82 below some critical\nlevel, so as to be useful for feature learning (Fig. 3a).\nA sigmoidal function with threshold at zero has negative SI, but a negative sigmoid, as used\nin ICA studies7, has SI > 0. More generally, whenever an effective nonlinearity f is not suited\nfor feature learning, its opposite \u2212f should be, since its SI will have the opposite sign (Fig. 2c).\nThis implies that, in general, half of the function space could be suitable for feature learning36,\ni.e. it \ufb01nds weights w such that the distribution of the feature wTx has a long tail, indicating high\nkurtosis (\"kurtotic feature\"). The other half of the function space learns the least kurtotic features\n(e.g. random connectivity patterns for natural images, Fig. 2b,c).\nThis universality strongly constrains the possible shape of receptive \ufb01elds that may arise during\ndevelopment for a given input dataset. For whitened natural images, a learnable receptive \ufb01eld is in\ngeneral either a localized edge detector or a non-localized random connectivity pattern.\nAn important special case is an effective linear curve, f(u) = u, which arises when both f-I\nand plasticity curves are linear21. Because the linear model maximizes variance \u27e8(wTx)2\u27e9, it can\nperform principal component analysis33, but does not have any feature selectivity on whitened input\n7\na\nb\nInput\nWeight change\nOptimization value\nReceptive field shape\nc\nFigure 2: Simple cell development from natural images regardless of speci\ufb01c effective Heb-\nbian nonlinearity. (a) Effective nonlinearity of \ufb01ve common models (arbitrary units): quadratic rec-\nti\ufb01er (green, as in cortical and BCM models, \u03b81 = 1., \u03b82 = 2.), linear recti\ufb01er (dark blue, as in L1\nsparse coding or networks with linear STDP, \u03b8 = 3.), Cauchy sparse coding nonlinearity (light blue,\n\u03bb = 3.), L0 sparse coding nonlinearity (orange, \u03bb = 3.), and negative sigmoid (purple, as in ICA\nmodels). (b) Relative optimization value \u27e8F(wTx)\u27e9for each of the \ufb01ve models in a, for different pres-\nelected features w, averaged over natural image patches x. Candidate features are represented as\ntwo-dimensional receptive \ufb01elds. For all models, the optimum is achieved at the localized oriented\nreceptive \ufb01eld. Inset: Example of natural image and image patch (red square) used as sensory\ninput. (c) Receptive \ufb01elds learned in four trials for ten effective Hebbian functions f (from top: the\n\ufb01ve functions considered above, u3, \u2212sin(u), u, (|u|\u22122)+, \u2212cos(u)) (left column), and their oppo-\nsites \u2212f (right column). The \ufb01rst seven functions (above the dashed line) lead to localized oriented\n\ufb01lters, while a sign-\ufb02ip leads to random patterns. Linear or symmetric functions are exceptions and\ndo not develop oriented \ufb01lters (bottom rows).\n8\ndatasets, where variance is constant (Fig. 2c).\nSymmetric effective nonlinearities, f(u) = f(\u2212u), are also exceptions, since their corresponding\noptimization functions are asymmetric, F(u) = \u2212F(\u2212u), so that for datasets with symmetric statis-\ntical distributions, P(x) = P(\u2212x), the optimization value will be zero, \u27e8Fasym.(wTxsym.)\u27e9= 0. As\nnatural images are not completely symmetric, localized receptive \ufb01elds do develop, though without\norientation selectivity, as illustrated by a cosine function and a symmetric piece-wise linear function\nas effective nonlinearities (Fig. 2c, bottom rows).\nLTP threshold\nSelectivity index\n Selectivity index\nF-I threshold\nSelectivity index\nSelectivity index\na\nb\nc\nd\nSparsity penalty\nSigmoid center\nFigure 3: Selectivity index for different effective nonlinearities. (a) Quadratic recti\ufb01er (small\ngraphic, three examples with different LTP thresholds) with LTD threshold at \u03b81 = 1: LTP threshold\nmust be below 3.5 to secure positive selectivity index (green region, main Fig) and learn localized\noriented receptive \ufb01elds (inset). A negative selectivity index (red region) leads to a random connec-\ntivity pattern (inset) (b) Linear recti\ufb01er: activation threshold must be above zero. (c) Sigmoid: center\nmust be below a = \u22121.2 or, for a stronger effect, above a = +1.2. The opposite conditions apply\nto the negative sigmoid. (d) Cauchy sparse coding nonlinearity: positive but weak feature selectivity\nfor any sparseness penalty \u03bb > 0. Insets show the nonlinearities for different choices of parameters.\n9\nReceptive \ufb01eld diversity\nSensory neurons display a variety of receptive \ufb01eld shapes38, and recent modeling efforts9,12 have\nattempted to understand the properties that give rise to the speci\ufb01c receptive \ufb01elds seen in experi-\nments. We show here that the shape diversity of a model can be predicted by our projection pursuit\nanalysis, and is primarily determined by the statistics of input representation, while relatively robust\nto the speci\ufb01c effective nonlinearity.\nWe studied a model with multiple neurons in the second layer, which compete with each other\nfor the representation of speci\ufb01c features of the input. Each neuron had a piece-wise linear f-I curve\nand a quadratic recti\ufb01er plasticity function (see Methods) and projected inhibitory connections v onto\nall others. These inhibitory connections are learned by anti-Hebbian plasticity and enforce decorre-\nlation of neurons, so that receptive \ufb01elds represent different positions, orientations and shapes39\u201341.\nFor 50 neurons, the resulting receptive \ufb01elds became diversi\ufb01ed (Fig. 4a-c, colored dots). In an\novercomplete network of 1000 neurons, the diversity further increased (Fig. 4d-f, colored dots).\nFor the analysis of the simulation results, we re\ufb01ned our inspection of optimal oriented receptive\n\ufb01elds for natural images by numerical evaluation of the optimality criterion \u27e8F(wTx)\u27e9for receptive\n\ufb01elds w = wGabor, described as Gabor functions of variable length, width and spatial frequency. For\nall tested nonlinearities, the optimization function for single-neuron receptive \ufb01elds varies smoothly\nwith these parameters (Fig 4, grey-shaded background). The single-neuron optimality landscape\nwas then used to analyze the multi-neuron simulation results. We found that receptive \ufb01elds are\nlocated in the area where the single-neuron optimality criterion is near its maximum, but spread out\nso as to represent different features of the input (Fig. 4). Thus the map of optimization values,\ncalculated from the theory of effective nonlinearity, enables us to qualitatively predict the shape\ndiversity of receptive \ufb01elds.\nAlthough qualitatively similar, there are differences in the receptive \ufb01elds developed for each\nmodel, such as smaller lengths for the L0 sparse coding model (Fig. 4c). While potentially sig-\nni\ufb01cant, these differences across models may be overwhelmed by differences due to other model\nproperties, including different network sizes or input representations. This is illustrated by observing\nthat receptive \ufb01eld diversity for a given model differ substantially across network sizes (Fig. 4), and\nthe difference is even greater from simulations with an input that is not completely white (Fig. 5c).\nThus our results suggests that efforts to model receptive \ufb01eld shapes observed experimentally9,12,38\n10\nshould focus on network size and input representation, which potentially have a stronger effect than\nthe nonlinear properties of the speci\ufb01c model under consideration.\nc\na\nd\nLength\nWidth\nLength\nWidth\nb\ne\nf\n50 neurons\n50 neurons\n50 neurons\n1000 neurons\nLength\nLength\nLength\nLength\nWidth\nWidth\nWidth\nWidth\n1000 neurons\n1000 neurons\nLength\nWidth\nFigure 4: Optimal receptive \ufb01eld shapes in model networks induce diversity. (a-f) Gray level\nindicates the optimization value for different lengths and widths (see inset in a) of oriented receptive\n\ufb01elds for natural images, for the quadratic recti\ufb01er (left, see Fig. 2a), linear recti\ufb01er (middle) and L0\nsparse coding (right). Optima marked with a black cross. (a-c) Colored circles indicate the receptive\n\ufb01elds of different shapes developed in a network of 50 neurons with lateral inhibitory connections.\nInsets on the right show example receptive \ufb01elds developed during simulation. (d-f) Same for a\nnetwork of 1000 neurons.\nWe also studied the variation of receptive \ufb01eld position and orientation. For all \ufb01ve nonlineari-\nties considered, the optimization value is equal for different positions of the receptive \ufb01eld centers,\ncon\ufb01rming the translation invariance in the image statistics, as long as the receptive \ufb01eld is not too\nclose to the border of the anatomically allowed fan-in of synaptic connections (Fig. 6b). Also, all\nnonlinearities reveal the same bias towards the horizontal and vertical orientations (Fig. 6c). These\n11\na\nb\nc\nWidth\nWidth\nLength\nLength\n4 neurons\n50 neurons\n1000 neurons\nLength\nWidth\nFigure 5: Receptive \ufb01elds for non-whitened natural images.Images were preprocessed as in the\noriginal sparse coding study35. We simulated linear recti\ufb01er neurons (\u03b8 = 0.5) with a quadratic\nplasticity nonlinearity (b = 0.5).\n(a) Multiple-neuron simulations, with 4 neurons.\nThe principal\ncomponents dominate the optimization and receptive \ufb01elds are not local, since they extend over\nmost of the image patch. With 50 (b) and 1000 (c) neurons, lateral inhibition promotes diversity,\nand more localized receptive \ufb01eld are formed. (insets) Sample receptive \ufb01elds developed for each\nsimulation.\noptimality predictions are con\ufb01rmed in single neuron simulations, which lead mostly to either hor-\nizontal or vertical orientations, at random positions (Fig. 6d). When the network is expanded to\n50 neurons, recurrent inhibition forces receptive \ufb01elds to cover different positions, though excluding\nborder positions, and some neurons have non-cardinal orientations (Fig. 6e). With 1000 neurons,\nreceptive \ufb01elds diversify to many possible combinations of position, orientation and length (Fig. 6f).\nBeyond V1 simple cells\nNonlinear Hebbian learning is not limited to explaining simple cells in V1. We investigated if the\nsame learning principles apply to receptive \ufb01eld development in other visual or auditory areas or\nunder different rearing conditions.\nFor auditory neurons14, we used segments of speech as input (Fig. 7a) and observed the de-\nvelopment of spectrotemporal receptive \ufb01elds localized in both frequency and time2 (Fig. 7d). The\nstatistical distribution of input patterns aligned with the learned receptive \ufb01elds had longer tails than\nfor random or non-local receptive \ufb01elds, indicating temporal sparsity of responses (Fig. 7d). Simi-\nlar to our simple cell results, the learned receptive \ufb01elds show higher optimization value for all \ufb01ve\n12\nPostition (pixels)\nOrientation (rad)\nReceptive field size\nd\ne\nf\nb\nc\nOptimization value\nOptimization value\n50 neurons\n1 neuron\n1000 neurons\na\nOptimization value\nFigure 6: Diversity of receptive \ufb01eld size, position and orientation. (a) The optimization value\nof localized oriented receptive \ufb01elds, within a 16x16 pixel patch of sensors, as a function of size\n(see Methods), for \ufb01ve nonlinearities (colors as in Fig. 2a). Optimal size is a receptive \ufb01eld of width\naround 3 to 4 pixels (\ufb01lled triangles). (b) The optimization value as a function of position of the\nreceptive \ufb01eld center, for a receptive \ufb01eld width of 4 pixels, indicates invariance to position within the\n16x16 patch, except near the borders. (c) The optimization value as a function of orientation shows\npreference toward horizontal and vertical directions, for all \ufb01ve nonlinearities. (d) Receptive \ufb01eld\nposition, orientation and length (colored bars) learned for 50 single-neuron trials. The color code\nindicates different orientations. (e) Receptive \ufb01eld positions and orientations learned in a 50 neuron\nnetwork reveal diversi\ufb01cation of positions, except at the borders. (f) With 1000 neurons, positions\nand orientations cover the full range of combinations (top). Selecting 50 randomly chosen receptive\n\ufb01elds highlights the diversi\ufb01cation of position, orientation and size (bottom). Receptive \ufb01elds were\nlearned through the quadratic recti\ufb01er nonlinearity (\u03b81 = 1., \u03b82 = 2.).\n13\neffective nonlinearities (Fig 7g).\nFor a study of receptive \ufb01eld development in the secondary visual cortex (V2)16, we used natural\nimages and the standard energy model42 of V1 complex cells to generate input to V2 (Fig. 7b). The\nlearned receptive \ufb01eld was selective to a single orientation over neighboring positions, indicating a\nhigher level of translation invariance. When inputs were processed with this receptive \ufb01eld, we found\nlonger tails in the feature distribution than with random features or receptive \ufb01elds without orientation\ncoherence (Fig 7e), and the learned receptive \ufb01eld had a higher optimization value for all choices of\nnonlinearity (Fig 7h).\nAnother important constraint for developmental models are characteristic deviations, such as\nstrabismus, caused by abnormal sensory rearing. Under normal binocular rearing conditions, the\nfan-in of synaptic input from the left and right eyes overlap in visual space (Fig 7c). In this case,\nbinocular receptive \ufb01elds with similar features for left and right eyes develop.\nIn the strabismic\ncondition, the left and right eyes are not aligned, modeled as binocular rearing with non-overlapping\ninput from each eye (Fig. 7c). In this scenario, a monocular simple cell-like receptive \ufb01eld developed\n(Fig. 7f), as observed in experiments and earlier models43. The statistical distributions con\ufb01rm that\nfor disparate inputs the monocular receptive \ufb01eld is more kurtotic than a binocular one, explaining its\nformation in diverse models44 (Fig 7f,i).\nOur results demonstrate the generality of the theory across multiple cortical areas. Selecting\na relevant feature space for an extensive analysis, as we have done with simple cells and natural\nimages, may not be possible in general. Nonetheless, nonlinear Hebbian learning helps to explain\nwhy some features (and not others) are learnable in network models15.\nDiscussion\nHistorically, a variety of models have been proposed to explain the development and distribution of\nreceptive \ufb01elds. We have shown that nonlinear Hebbian learning is a parsimonious principle which is\nimplicitly or explicitly present in many developmental models6\u201312,24,36,39,44. The fact that receptive \ufb01eld\ndevelopment is robust to the speci\ufb01c nonlinearity highlights a functional relation between different\nmodels. It also uni\ufb01es feature learning across sensory modalities: receptive \ufb01elds form around\nfeatures with a long-tailed distribution.\n14\nd\ne\nProbability\nProbability\nOptimization value\nOptimization value\nInput\nInput\nSpeech signal\nProbability\nInput\nOptimization value\nf\ng\ni\nh\nTime (ms)\nFreq. (kHz)\n0\n400\n0.2\n4.\nLeft\nRight\nPosition x\nPosition y\na\nFreq. decomposition\n4 kHz\n0.2 kHz\nTime (ms)\n0\n2000\n400 ms\nb\nLeft\nLeft\nRight\nRight\nNormal rearing\nStrabismic rearing\nc\n+\n+\n...\nComplex cells model\n2 kHz\n3 kHz\n1 kHz\nFigure 7: Nonlinear Hebbian learning across sensory modalities. (a) The auditory input is mod-\neled as segments over time and frequency (red) of the spectrotemporal representation of speech\nsignals. (b) The V2 input is assembled from the output of modeled V1 complex cells at different\npositions and orientations. Receptive \ufb01elds are represented by bars with size proportional to the\nconnection strength to the complex cell with the respective position and orientation. (c) Strabismic\nrearing is modeled as binocular stimuli with non-overlapping left and right eye input patches (red).\n(d-f) Statistical distribution (log scale) of the input projected onto three different features for speech\n(d), V2 (e) and strabismus (f). In all three cases, the learned receptive \ufb01eld (blue, inset) is character-\nized by a longer tailed distribution (arrows) than the random (red) and comparative (green) features.\n(g-i) Relative optimization value for \ufb01ve nonlinearities (same as in Fig. 2), for the three selected pat-\nterns (insets). The receptive \ufb01elds learned with the quadratic recti\ufb01er nonlinearity (\u03b81 = 1., \u03b82 = 2.)\nare the maxima among the three patterns, for all \ufb01ve nonlinearities, for all three datasets.\n15\nRelation to previous studies\nEarlier studies have already placed developmental models side by side, comparing their normative\nassumptions, algorithmic implementation or receptive \ufb01elds developed. Though consistent with their\n\ufb01ndings, our results lead to revised interpretations and predictions.\nThe similarities between sparse coding and ICA are clear from their normative correspondence35.\nNevertheless, the additional constraint in ICA, of having at most as many features as inputs, makes\nit an easier problem to solve, allowing for a range of suitable algorithms34. These differ from algo-\nrithms derived for sparse coding, in which the inference step is dif\ufb01cult due to overcompleteness. We\nhave shown that regardless of the speci\ufb01c normative assumptions, it is the common implementation\nof nonlinear Hebbian learning that explains similarities in their learning properties.\nIn contrast to the idea that in sparse coding algorithms overcompleteness is required for devel-\nopment of localized oriented edges35, we have demonstrated that a sparse coding model with a\nsingle neuron is mathematically equivalent to nonlinear Hebbian learning and learns localized \ufb01lters\nin a setting that is clearly \"undercomplete\". Thus differences observed in receptive \ufb01eld shapes\nbetween sparse coding and ICA models38 are likely due to differences in network size and input\npreprocessing. For instance, the original sparse coding model35 applied a preprocessing \ufb01lter that\ndid not completely whiten the input, leading to larger receptive \ufb01elds (Fig. 5).\nStudies that derive spiking models from normative theories often interpret the development of\noriented receptive \ufb01elds as a consequence of its normative assumptions11,12. In a recent example, a\nspiking network has been related to the sparse coding model12, using neural properties de\ufb01ned ad\nhoc. Our results suggest that many other choices of neural activations would have given qualitatively\nsimilar receptive \ufb01elds, independent of the sparse coding assumption. While in sparse coding the\neffective nonlinearity derives from a linear plasticity rule combined with a nonlinear f-I curve, our\nresults indicate that a nonlinear plasticity rule combined with a linear neuron model would give the\nsame outcome.\nIn order to distinguish between different normative assumptions, or particular neural implemen-\ntations, the observation of \"oriented \ufb01lters\" is not suf\ufb01cient and additional constraints are needed.\nSimilarly receptive shape diversity, another important experimental constraint, should also be con-\nsidered with care, since it cannot easily distinguish between models either. Studies that confront\nthe receptive \ufb01eld diversity of a model to experimental data9,12,38 should also take into account input\n16\npreprocessing choices and how the shape changes with an increasing network size, since we have\nobserved that these aspects may have a larger effect on receptive \ufb01eld shape than the particulars of\nthe learning model.\nEmpirical studies of alternative datasets, including abnormal visual rearing44, tactile and auditory\nstimuli15, have also observed that different unsupervised learning algorithms lead to comparable\nreceptive \ufb01elds shapes. Our results offer a plausible theoretical explanation for these \ufb01ndings.\nPast investigations on nonlinear Hebbian learning20,36 demonstrated that many nonlinearities\nwere capable of solving the cocktail party problem. Since it is a speci\ufb01c toy model, that asks for the\nunmixing of linearly mixed independent features, it is not clear a priori whether the same conclusions\nwould hold in other settings. We have shown that the results of Fyfe and Baddeley 20 and Hyvarinen\nand Oja 36 generalize in two directions. First, the effective nonlinear Hebbian learning mechanism\nis also behind other models beyond ICA, such as sparse coding models and plastic spiking net-\nworks. Second, the robustness to the choice of nonlinearity is not limited to a toy example, but also\nholds in multiple real world data. Together, these insights explain and predict the outcome of many\ndevelopmental models, in diverse applications.\nRobustness to normative assumptions\nMany theoretical studies start from normative assumptions7,9,11,35, such as a statistical model of the\nsensory input or a functional objective, and derive neural and synaptic dynamics from them. Our\nclaim of universality of feature learning indicates that details of normative assumptions may be of\nlower importance.\nFor instance, in sparse coding one assumes features with a speci\ufb01c statistical prior9,35. After\nlearning, this prior is expected to match the posterior distribution of the neuron\u2019s \ufb01ring activity9,35.\nNevertheless, we have shown that receptive \ufb01eld learning is largely unaffected by the choice of\nprior. Thus, one cannot claim that the features were learned because they match the assumed prior\ndistribution, and indeed in general they do not. For a coherent statistical interpretation, one could\nsearch for a prior that would match the feature statistics. However, since the outcome of learning\nis largely unaffected by the choice of prior, such a statistical approach would have limited predictive\npower.\nGenerally, kurtotic prior assumptions enable feature learning, but the speci\ufb01c priors are\nnot as decisive as one might expect. Because normative approaches have assumptions, such as\n17\nindependence of hidden features, that are not generally satis\ufb01ed by the data they are applied to, the\nactual algorithm that is used for optimization becomes more critical than the formal statistical model.\nThe concept of sparseness of neural activity is used with two distinct meanings. The \ufb01rst one\nis a single-neuron concept and speci\ufb01cally refers to the long-tailed distribution statistics of neural\nactivity, indicating a \"kurtotic\" distribution. The second notion of sparseness is an ensemble concept\nand refers to the very low \ufb01ring rate of neurons, observed in cortical activity45, which may arise\nfrom lateral competition in overcomplete representations. Overcompleteness of ensembles makes\nsparse coding different from ICA35. We have shown here that competition between multiple neurons\nis fundamental for receptive \ufb01eld diversity, whereas it is not required for simple cell formation per se.\nKurtotic features can be learned even by a single neuron with nonlinear Hebbian learning, and with\nno restrictions on the sparseness of its \ufb01ring activity.\nInteraction of selectivity with preprocessing and homeostasis\nThe concept of nonlinear Hebbian learning also clari\ufb01es the interaction of feature selectivity with\npreprocessing mechanisms. We have assumed whitened data throughout the study, except Fig. 5.\nSince after whitening second-order correlations are uninformative, neurons can develop sensitivity\nto higher order features. While whitened data is formally not required for our analysis, second-order\ncorrelations may dominate the optimization for non-white input, so that principal components will be\nlearned (Fig. 5a). Only when multiple neurons are added and receptive \ufb01elds diversify, are localized\nsimple cells formed with an input that is not completely white35 (Fig. 5c).\nIn studies of spiking networks, the input is restricted to positive rates, possibly through an on/off\nrepresentation, as observed in the LGN46. While the center-surround properties of LGN contributes\nto a partial decorrelation of neuronal activity47, in such alternative representations, trivial receptive\n\ufb01elds may develop, such as a single non-zero synapse, and additional mechanisms, such as hard\nbounds on each synaptic strength, a \u2264wj \u2264b, may be necessary to restrict the optimization space\nto desirable features10.\nInstead of constraining the synaptic weights, one may implement a synaptic decay as in Oja\u2019s\nplasticity rule33, \u2206w \u221dx \u00b7 y \u2212w \u00b7 y2 (see also48). Because of its multiplicative effect, the decay term\ndoes not alter the receptive \ufb01eld, but only scales its strength. Thus, it is equivalent to rescaling the\ninput in the f-I curve, so as to shift it to the appropriate range (Fig. 3). Similar scaling effects arise\n18\nfrom f-I changes due to intrinsic plasticity11,28,49. The precise relation between nonlinear Hebbian\nlearning, spiking representations and homeostasis in the cortex is an important topic for further\nstudies.\nUniversality supports biological instantiation\nThe principle of nonlinear Hebbian learning has a direct correspondence to biological neurons and\nis compatible with a large variety of plasticity mechanisms. It is not uncommon for biological systems\nto have diverse implementations with comparable functional properties50. Different species, or brain\nareas, could have different neural and plasticity characteristics, and still have similar feature learning\nproperties51,52. The generality of the results discussed in this paper reveals learning simple cell-like\nreceptive \ufb01elds from natural images to be much easier than previously thought. It implies that a\nbiological interpretation of models is possible even if some aspects of a model appear simpli\ufb01ed or\neven wrong in some biological aspects. Universality also implies that the study of receptive \ufb01eld\ndevelopment is not suf\ufb01cient to distinguish between different models.\nThe relation of nonlinear Hebbian learning to projection pursuit endorses the interpretation of\ncortical plasticity as an optimization process. Under the rate coding assumptions considered here,\nthe crucial property is an effective synaptic change linear in the pre-synaptic rate, and nonlinear\nin the post-synaptic input. Pairing experiments with random \ufb01ring and independently varying pre-\nand post-synaptic rates would be valuable to investigate these properties27,53,54. Altogether, the\nrobustness to details in both input modality and neural implementation suggests nonlinear Hebbian\nlearning as a fundamental principle underlying the development of sensory representations.\nMethods\nSpiking model. A generalized leaky integrate-and-\ufb01re neuron26 was used as spiking model, which\nincludes power-law spike-triggered adaptation and stochastic \ufb01ring, with parameters26 \ufb01tted to pyra-\nmidal neurons. The f-I curve g(I) was estimated by injecting step currents and calculating the trial\naverage of the spike count over the \ufb01rst 500 ms. The minimal triplet-STDP model24 was imple-\nmented, in which synaptic changes follow\n19\nd\ndtw(t) = A+y(t)\u00afy+(t)\u00afx+(t) \u2212A\u2212x(t)\u00afy\u2212(t)\n(4)\nwhere y(t) and x(t) are the post- and pre-synaptic spike trains, respectively: y(t) = P\nf \u03b4(t \u2212tf),\nwhere tf are the \ufb01ring times and \u03b4 denotes the Dirac \u03b4-function; x(t) is a vector with components\nxi(t) = P\nf \u03b4(t\u2212tf\ni ), where tf\ni are the \ufb01ring times of pre-synaptic neuron i; w is a vector comprising\nthe synaptic weights wi connecting a pre-synaptic neuron i to a post-synaptic cell. A+ = 6.5 \u00b7 10\u22123\nand A\u2212= 5.3 \u00b7 10\u22123 are constants, and \u00afy+, \u00afx+ and \u00afy\u2212are moving averages, implemented by\nintegration (e.g. \u03c4 \u2202\u00afy\n\u2202t = \u2212\u00afy +y), with time scales 114.0 ms, 16.8 ms and 33.7 ms, respectively24. For\nestimating the nonlinearity h(y) of the plasticity, pre- and post-synaptic spike trains were generated\nas Poisson processes, with the pre-synaptic rate set to 20 Hz.\nA linear recti\ufb01er g(x) = a(x \u2212b)+ was \ufb01tted to the f-I curve of the spiking neuron model by\nsquared error optimization.\nSimilarly, a quadratic function h(x) = a(x2 \u2212bx) was \ufb01tted to the\nnonlinearity of the triplet STDP model. The combination of these two \ufb01tted functions was plotted as\n\ufb01t for the effective nonlinearity f(x) = h(g(x)).\nSparse coding analysis. A sparse coding model, with K neurons y1, . . . , yK, has a nonlinear\nHebbian learning formulation. The sparse coding model minimizes a least square reconstruction\nerror between the vector of inputs x and the reconstruction vector Wy, where W = [w1 . . . wK],\nand y = (y1, . . . , yK) is the vector of neuronal activities, with yj \u22650 for 1 \u2264j \u2264K. The total error\nE combines a sparsity constraint S with weight \u03bb and the reconstruction error, E = 1\n2||x \u2212Wy||2 +\n\u03bb P S(yk). E has to be minimal, averaged across all input samples, under the constraint yj \u22650 for\nall j.\nThe minimization problem is solved by a two-step procedure. In the \ufb01rst step, for each input\n20\nsample, one minimizes E with respect to all hidden units yj\nd\ndyj\nE = 0 \u21d0\u21d2wj(x \u2212Wy) \u2212\u03bbS\u2032(yj) = 0\n\u21d0\u21d2wjx \u2212\nX\nk\u0338=j\n(wT\nj wk)yk \u2212||wj||2yj \u2212\u03bbS\u2032(yj) = 0\n\u21d0\u21d2yj + \u03bbS\u2032(yj) = wT\nj x \u2212\nX\nk\u0338=j\n(wT\nj wk)yk\n\u21d0\u21d2yj = g(wT\nj x \u2212\nX\nk\u0338=j\nvjkyk)\n(5)\nwhere we constrained the vector wj of synapses projecting onto unit yj by ||wj||2 = 1, de\ufb01ned the\nactivation function g(.) = T \u22121(.), the inverse of T(y) = (y +\u03bbS\u2032(y)), and de\ufb01ned recurrent synaptic\nweights vjk = wT\nj wk. For each input sample x, this equation shall be iterated until convergence.\nThe equation can be interpreted as a recurrent neural network, where each neuron has an activation\nfunction g, and the input is given by the sum of the feedforward drive wT\nj x and a recurrent inhibition\nterm \u2212P\nk\u0338=j vjkyk. To avoid instability, we implement a smooth membrane potential uj, which has\nthe same convergence point32\n\u03c4u\nd\ndtuj(t) = \u2212uj(t) + (wT\nj x \u2212\nX\nk\u0338=j\nvjkyk(t))\nyj(t) = g(uj(t))\n(6)\ninitialized with uj(t) = 0.\nThe second step is a standard gradient descent implementation of the least square regression\noptimization, leading to an learning rule\n\u2206wj \u221d\nd\ndwj\nE = (x \u2212WTy) yj = x yj \u2212wj y2\nj \u2212\nX\nk\u0338=j\nwkykyj\nThe decay term wj y2\nj has no effect, since the norm is constrained to ||wj|| = 1 at each step.\nFor a single unit y, the model simpli\ufb01es to a nonlinear Hebbian formulation, \u2206w \u221dx g(wT\nj x). For\nmultiple units, it can be interpreted as projection pursuit on an effective input, not yet represented by\n21\nother neurons, \u02dcxj = x \u2212P\nk\u0338=j wkyk, which simpli\ufb01es to \u2206wj \u221d\u02dcxj \u00b7 g(wT\nj \u02dcxj) .\nThere are two non-local terms that need to be implemented by local mechanisms so as to be\nbiologically plausible. First, the recurrent weights depend on the overlap between receptive \ufb01elds,\nwT\nj wk, which is non-local. The sparse coding model assumes independent hidden neurons, which\nimplies that after learning neurons should be pair-wise uncorrelated, cov(yj, yk) = 0. As an aside we\nnote that the choice vjk = wT\nj wk does not automatically guarantee decorrelation. Decorrelation may\nbe enforced through plastic lateral connections, following an anti-Hebbian rule12,39, \u2206vjk \u221d(yj \u2212\n\u27e8yj\u27e9)\u00b7yk, where \u27e8yj\u27e9is a moving average (we use \u03c4 = 1000 input samples). Thus by substituting \ufb01xed\nrecurrent connections by anti-Hebbian plasticity, convergence \u2206vjk = 0 implies cov(yj, yk) = 0.\nWhile this implementation does not guarantee vjk = wT\nj wk after convergence, neither does vjk =\nwT\nj wk guarantee decorrelation cov(yj, yk) = 0, it does lead to optimal decorrelation, which is the\nbasis of the normative assumption. Additionally we constrain vjk \u22650 to satisfy Dale\u2019s law. Although\nsome weights would converge to negative values otherwise, most neuron pairs have correlated\nreceptive \ufb01elds, and thus positive recurrent weights.\nSecond, we ignore the non-local term P\nk\u0338=j wkykyj in the update rule. Although this approxima-\ntion is not theoretically justi\ufb01ed, we observed in simulations that receptive \ufb01elds do not qualitatively\ndiffer when this term is removed.\nThe resulting Hebbian formulation can be summarized as\nyj = g(wT\nj x \u2212\nX\nk\u0338=j\nvjkyk)\n\u2206wj \u221dx yj\n\u2206vjk \u221d(yj \u2212\u27e8yj\u27e9) \u00b7 yk\n(7)\nThis derivation uni\ufb01es previous results on the biological implementation of sparse coding: the\nrelation of the sparseness constraint to a speci\ufb01c activation function32, the derivation of a Hebbian\nlearning rule from quadratic error minimization33, and the possibility of approximating lateral interac-\ntion terms by learned lateral inhibition12,39.\nNonlinearities and optimization value. The optimization value for a given effective nonlin-\nearity f, synaptic weights w, and input samples x, is given by R = \u27e8F(wTx)\u27e9, where F =\n\u00b4\nf\nand angular brackets indicate the ensemble average over x. Relative optimization values in Figs.\n22\n2b and 6 were normalized to [0, 1], relative to the minimum and maximum values among the con-\nsidered choice of features w, R\u2217= (R \u2212Rmin)/(Rmax \u2212Rmin). The selectivity index of a non-\nlinearity f is de\ufb01ned as SI = (\u27e8F(l)\u27e9\u2212\u27e8F(g)\u27e9)/\u03c3F , where l and g are Laplacian and Gaus-\nsian variables respectively, normalized to unit variance. \u03c3F = \u221a\u03c3F(l)\u03c3F(g) is a normalization fac-\ntor, with \u03c3F(.) =\np\n\u27e8F(.)2\u27e9. The selectivity of an effective nonlinearity f is not altered by multi-\nplicative scaling, \u02dcf(u) = \u03b1f(u), neither by additive constants when the input distribution is sym-\nmetric, \u02dcf(u) = \u03b1f(u) + \u03b2.\nThe effective nonlinearities in Fig.\n2 included the linear recti\ufb01er\nf(u) =\n\uf8f1\n\uf8f2\n\uf8f3\n0,\nif u < \u03b8\nu \u2212\u03b8,\nif u \u2265\u03b8\n, the quadratic recti\ufb01er f(u) =\n\uf8f1\n\uf8f2\n\uf8f3\n0,\nif u < \u03b8\n(u \u2212\u03b8)(u \u2212\u03b8 \u2212b),\nif u \u2265\u03b8\n, the L0\nsparse coding nonlinearity f(u) =\n\uf8f1\n\uf8f2\n\uf8f3\n0,\nif u < \u03bb\nu,\nif u \u2265\u03bb\n, the Cauchy sparse coding nonlinearity f = T \u22121,\nwhere T(y) =\n\uf8f1\n\uf8f2\n\uf8f3\n0,\nif y < 0\ny + 2\u03bby/(1 + y2),\nif y \u22650\n, the negative sigmoid f(u) = 1 \u22122/(1 + e\u22122u), a\npolynomial function f(u) = u3, trigonometric functions sin(u) and cos(u), a symmetric piece-wise\nlinear function f(u) =\n\uf8f1\n\uf8f2\n\uf8f3\n0,\nif |u| < \u03b8\n|u| \u2212\u03b8,\nif |u| \u2265\u03b8\n, as well as, for comparison, a linear function f(u) = u.\nReceptive \ufb01eld learning. Natural image patches (16 by 16 pixel windows) were sampled from\na standard dataset6 (106 patches). Patches were randomly rotated by \u00b190\u25e6degrees to avoid biases\nin orientation. The dataset was whitened by mean subtraction and a standard linear transformation\nx\u2217= Mx, where M = RD\u22121/2RT and \u27e8xxT\u27e9= RDRT is the eigenvalue decomposition of the\ninput correlation matrix. In Fig. 5, we used images preprocessed as in Olshausen and Field 6,\n\ufb01ltered in the spatial frequency domain by M(f) = f e\u2212(f/f0)4. The exponential factor is a low-\npass \ufb01lter that attenuates high-frequency spatial noise, with f0 = 200 cycles per image. The linear\nfactor f was designed to whiten the images by canceling the approximately 1/f power law spatial\ncorrelation observed in natural images37. But since the exponent of the power law for this particular\ndataset has an exponent closer to 1.2, the preprocessed image exhibit higher variance at lower\nspatial frequencies.\nSynaptic weights were initialized randomly (normal distribution with zero mean) and, for an ef-\nfective nonlinearity f, evolved through wk+1 = wk + \u03b7 x f(wT\nk xk), for each input sample xk, with\na small learning rate \u03b7. We enforced normalized weights at each time step, ||w||2 = 1, through\n23\nmultiplicative normalization, implicitly assuming rapid homeostatic mechanisms28,55. For multiple\nneurons, the neural version of the sparse coding model described in Eq 7 was implemented. In Fig 4\nand 5, the learned receptive \ufb01elds were \ufb01tted to Gabor \ufb01lters by least square optimization. Receptive\n\ufb01elds with less than 0.6 variance explained were rejected (less than 5% of all receptive \ufb01elds).\nReceptive \ufb01eld selection. In Fig. 2b, the \ufb01ve selected candidate patterns are: random con-\nnectivity \ufb01lter (weights sampled independently from the normal distribution with zero mean), high-\nfrequency Fourier \ufb01lter (with equal horizontal and vertical spatial periods, Tx = Ty = 8 pixels),\ndifference of Gaussians \ufb01lter (\u03c31 = 3., \u03c32 = 4.), low-frequency Fourier \ufb01lter (Tx = 16, Ty = 32),\nand centered localized Gabor \ufb01lter (\u03c3x = 1.5, \u03c3y = 2.0, f = 0.2, \u03b8 = \u03c0/3, \u03c6 = \u03c0/2). Fourier \ufb01lters\nwere modeled as wab = sin(2\u03c0a/Tx)\u2217cos(2\u03c0b/Ty); difference of Gaussians \ufb01lters as the difference\nbetween two centered 2D Gaussians with same amplitude and standard deviations \u03c31 and \u03c32; and\nwe considered standard Gabor \ufb01lters, with center (xc, yc), spatial frequency f, width \u03c3x, length \u03c3y,\nphase \u03c6 and angle \u03b8. In Fig 4 and 5 we de\ufb01ne the Gabor width and length in pixels as 2.5 times\nthe standard deviation of the respective Gaussian envelopes, \u03c3x and \u03c3y. In Fig. 6a, a Gabor \ufb01lter of\nsize s had parameters \u03c3x = 0.3 \u00b7 s, \u03c3y = 0.6 \u00b7 s, f = 1/s and \u03b8 = \u03c0/3. In Fig. 6b-c, the Gabor \ufb01lter\nparameters were \u03c3x = 1.2, \u03c3y = 2.4, f = 0.25. All receptive \ufb01elds were normalized to ||w||2 = 1. In\nFig. 4 and 5, the background optimization value was calculated for Gabor \ufb01lters of different widths,\nlengths, frequencies, phases \u03c6 = 0 and \u03c6 = \u03c0/2. For each width and length, the maximum value\namong frequencies and phases was plotted.\nAdditional datasets. For the strabismus model, two independent natural image patches were\nconcatenated, representing non-overlapping left and right eye inputs, forming a dataset with 16\nby 32 patches43. For the binocular receptive \ufb01eld in the strabismus statistical analysis (Fig. 7a),\na receptive \ufb01eld was learned with a binocular input with same input from left and right eyes. As\nV2 input, V1 complex cell responses were obtained from natural images as in standard energy\nmodels42, modeled as the sum of the squared responses of simple cells with alternated phases.\nThese simple cells were modeled as linear neurons with Gabor receptive \ufb01elds (\u03c3x = 1.2, \u03c3y = 2.4,\nf = 0.3), with centers placed on a 8 by 8 grid (3.1 pixels spacing), with 8 different orientations at\neach position (total of 512 input dimensions). For the non-orientation selective receptive \ufb01eld in the\nV2 statistical analysis (Fig. 7d), the orientations of the input complex cells for the learned receptive\n\ufb01eld were randomized. As auditory input, spectrotemporal segments were sampled from utterances\nspoken by a US English male speaker (CMU US BDL ARCTIC database, Kominek and Black 56). For\n24\nthe frequency decomposition14, each audio segment was \ufb01ltered by gammatone kernels, absolute\nand log value taken and downsampled to 50 Hz. Each sample was 20 time points long (400 ms\nsegment) and 20 frequency points wide (equally spaced between 0.2 kHz and 4.0 kHz). For the\nnon-local receptive \ufb01eld in the auditory statistical analysis (Fig. 7g), a Fourier \ufb01lter was used (Tt =\nTf = 10). For all datasets, the input ensemble was whitened after the preprocessing steps, by\nthe same linear transformation described above for natural images, and all receptive \ufb01elds were\nnormalized to ||w||2 = 1.\nAcknowledgments\nWe thank C. Pozzorini and J. Brea for valuable comments, and D.S. Corneil for critical reading of the\nmanuscript. This research was supported by the European Research Council under grant agreement\nno. 268689 (MultiRules).\nReferences\n[1] David H. Hubel and Torsten N. Wiesel. Receptive \ufb01elds of single neurones in the cat\u2019s striate\ncortex. The Journal of physiology, 148(3):574, 1959.\n[2] Lee M. Miller, Monty A. Escabi, Heather L. Read, and Christoph E. Schreiner. Spectrotemporal\nreceptive \ufb01elds in the lemniscal auditory thalamus and cortex. Journal of neurophysiology, 87\n(1):516\u2013527, 2002.\n[3] James J. DiCarlo, Davide Zoccolan, and Nicole C. Rust. How does the brain solve visual object\nrecognition? Neuron, 73(3):415\u2013434, 2012.\n[4] Jeremy Freeman and Eero P Simoncelli. Metamers of the ventral stream. Nat Neurosci, 14(9):\n1195\u20131201, 2011.\n[5] David Field. What is the goal of sensory coding? Neural computation, 6(4):559\u2013601, 1994.\n[6] Bruno A. Olshausen and David J. Field. Emergence of simple-cell receptive \ufb01eld properties by\nlearning a sparse code for natural images. Nature, 381(6583):607\u2013609, 1996.\n25\n[7] Anthony J. Bell and Terrence J. Sejnowski. The \u201cindependent components\u201d of natural scenes\nare edge \ufb01lters. Vision Research, 37(23):3327\u20133338, 1997.\n[8] C C Law and L N Cooper. Formation of receptive \ufb01elds in realistic visual environments accord-\ning to the bienenstock, cooper, and munro (BCM) theory. Proceedings of the National Academy\nof Sciences, 91(16):7797\u20137801, 1994.\n[9] M. Rehn and F. T Sommer. A network that uses few active neurones to code visual input predicts\nthe diverse shapes of cortical receptive \ufb01elds. Journal of Computational Neuroscience, 22(2):\n135\u2013146, 2007.\n[10] C. Clopath, L. Busing, E. Vasilaki, and W. Gerstner. Connectivity re\ufb02ects coding: a model of\nvoltage-based STDP with homeostasis. Nature Neuroscience, 13(3):344\u2013352, 2010.\n[11] Cristina Savin, Prashant Joshi, and Jochen Triesch. Independent component analysis in spiking\nneurons. PLoS computational biology, 6(4):e1000757, 2010.\n[12] Joel Zylberberg, Jason Timothy Murphy, and Michael Robert DeWeese. A sparse coding model\nwith synaptically local plasticity and spiking neurons can account for the diverse shapes of v1\nsimple cell receptive \ufb01elds. PLoS Comput Biol, 7(10):e1002250, 2011.\n[13] Bruno A Olshausen and David J Field. Sparse coding of sensory inputs. Current Opinion in\nNeurobiology, 14(4):481\u2013487, 2004.\n[14] Evan C. Smith and Michael S. Lewicki. Ef\ufb01cient auditory coding. Nature, 439(7079):978\u2013982,\n2006.\n[15] Andrew Saxe, Maneesh Bhand, Ritvik Mudur, Bipin Suresh, and Andrew Y. Ng. Unsupervised\nlearning models of primary cortical receptive \ufb01elds and receptive \ufb01eld plasticity. Advances in\nneural information processing systems, pages 1971\u20131979, 2011.\n[16] H. Lee, C. Ekanadham, and A. Ng. Sparse deep belief net model for visual area v2. Advances\nin neural information processing systems, 20, 2007.\n[17] Daniel LK Yamins, Ha Hong, Charles F. Cadieu, Ethan A. Solomon, Darren Seibert, and\nJames J. DiCarlo.\nPerformance-optimized hierarchical models predict neural responses in\n26\nhigher visual cortex. Proceedings of the National Academy of Sciences, 111(23):8619\u20138624,\n2014.\n[18] Jerome H. Friedman. Exploratory projection pursuit. Journal of the American Statistical Asso-\nciation, 82(397):249\u201366, 1987.\n[19] E. Oja, H. Ogawa, and J. Wangviwattana. Learning in nonlinear constrained hebbian networks.\nArti\ufb01cial Neural Networks, 1991.\n[20] Colin Fyfe and Roland Baddeley. Non-linear data structure extraction using simple hebbian\nnetworks. Biological Cybernetics, 72(6):533\u2013541, 1995.\n[21] K. D. Miller, J. B. Keller, and M. P. Stryker. Ocular dominance column development: analysis\nand simulation. Science, 245(4918):605\u2013615, 1989.\n[22] E. L. Bienenstock, L. N. Cooper, and P. W. Munro.\nTheory for the development of neuron\nselectivity: orientation speci\ufb01city and binocular interaction in visual cortex.\nThe Journal of\nNeuroscience, 2(1):32\u201348, 1982.\n[23] Wulfram Gerstner, Werner M. Kistler, Richard Naud, and Liam Paninski. Neuronal Dynamics:\nFrom Single Neurons to Networks and Models of Cognition. Cambridge University Press, 2014.\n[24] J. P P\ufb01ster and W. Gerstner. Triplets of spikes in a model of spike timing-dependent plasticity.\nThe Journal of Neuroscience, 26(38):9673\u20139682, 2006.\n[25] Torsten N. Wiesel and David H. Hubel. Single-cell responses in striate cortex of kittens deprived\nof vision in one eye. J Neurophysiol, 26(6):1003\u20131017, 1963.\n[26] Christian Pozzorini, Richard Naud, Skander Mensi, and Wulfram Gerstner. Temporal whitening\nby power-law adaptation in neocortical neurons. Nature Neuroscience, 16(7):942\u2013948, 2013.\n[27] P. J Sjostrom, G. G Turrigiano, and S. B Nelson. Rate, timing, and cooperativity jointly determine\ncortical synaptic plasticity. Neuron, 32(6):1149\u20131164, 2001.\n[28] G. Turrigiano. Too many cooks? intrinsic and synaptic homeostatic mechanisms in cortical\ncircuit re\ufb01nement. Annual review of neuroscience, 34:89\u2013103, 2011.\n27\n[29] S. Song, K. D Miller, and L. F Abbott.\nCompetitive hebbian learning through spike-timing-\ndependent synaptic plasticity. Nature Neuroscience, 3(9):919\u2013926, 2000.\n[30] W. Gerstner, R. Kempter, and J. L Van Hemmen. A neuronal learning rule for sub-millisecond\ntemporal coding. Nature, 383(6595):76\u201378, 1996.\n[31] P. Dayan and L. F. Abbott. Theoretical neuroscience, volume 31. MIT press Cambridge, MA,\n2001.\n[32] Christopher J. Rozell, Don H. Johnson, Richard G. Baraniuk, and Bruno A. Olshausen. Sparse\ncoding via thresholding and local competition in neural circuits. Neural Computation, 20(10):\n2526\u20132563, 2008.\n[33] Erkki Oja. Simpli\ufb01ed neuron model as a principal component analyzer. Journal of mathematical\nbiology, 15(3):267\u2013273, 1982.\n[34] A. Hyvarinen and E. Oja. Independent component analysis: algorithms and applications. Neural\nnetworks, 13(4-5):411\u2013430, 2000.\n[35] B. A Olshausen and D. J Field. Sparse coding with an overcomplete basis set: A strategy\nemployed by v1? Vision research, 37(23):3311\u20133325, 1997.\n[36] Aapo Hyvarinen and Erkki Oja. Independent component analysis by general nonlinear hebbian-\nlike learning rules. Signal Processing, 64(3):301\u2013313, 1998.\n[37] Daniel L. Ruderman and William Bialek. Statistics of natural images: Scaling in the woods.\nPhysical Review Letters, 73(6):814\u2013817, 1994.\n[38] Dario L. Ringach. Spatial structure and symmetry of simple-cell receptive \ufb01elds in macaque\nprimary visual cortex. Journal of Neurophysiology, 88(1):455\u2013463, 2002.\n[39] P. Foldiak. Forming sparse representations by local anti-hebbian learning. Biological cybernet-\nics, 64(2):165\u2013170, 1990.\n[40] T. P. Vogels, Henning Sprekeler, Friedemann Zenke, Claudia Clopath, and Wulfram Gerstner.\nInhibitory plasticity balances excitation and inhibition in sensory pathways and memory net-\nworks. Science, 334(6062):1569\u20131573, 2011.\n28\n[41] Paul D. King, Joel Zylberberg, and Michael R. DeWeese. Inhibitory interneurons decorrelate\nexcitatory cells to drive sparse code formation in a spiking model of v1. The Journal of Neuro-\nscience, 33(13):5475\u20135485, 2013.\n[42] A. Hyvarinen, J. Hurri, and P. O. Hoyer. Natural Image Statistics: A Probabilistic Approach to\nEarly Computational Vision., volume 39. Springer, 2009.\n[43] Leon N. Cooper, Nathan Intrator, Brian S. Blais, and Harel Z. Shouval.\nTheory of Cortical\nPlasticity. World Scienti\ufb01c Pub Co Inc, 2004.\n[44] Jonathan J. Hunt, Peter Dayan, and Geoffrey J. Goodhill. Sparse coding can predict primary\nvisual cortex receptive \ufb01eld changes induced by abnormal visual input. PLoS Comput Biol, 9\n(5):e1003005, 2013.\n[45] Alison L. Barth and James F.A. Poulet. Experimental evidence for sparse \ufb01ring in the neocortex.\nTrends in Neurosciences, 35(6):345\u2013355, 2012.\n[46] Kenneth D. Miller. A model for the development of simple cell receptive \ufb01elds and the ordered\narrangement of orientation columns through activity-dependent competition between on-and\noff-center inputs. Journal of Neuroscience, 14:409\u2013409, 1994.\n[47] Yang Dan, Joseph J. Atick, and R. Clay Reid. Ef\ufb01cient coding of natural scenes in the lateral\ngeniculate nucleus: Experimental test of a computational theory. The Journal of Neuroscience,\n16(10):3351\u20133362, 1996.\n[48] Jen-Yung Chen, Peter Lonjers, Christopher Lee, Marina Chistiakova, Maxim Volgushev, and\nMaxim Bazhenov. Heterosynaptic plasticity prevents runaway synaptic dynamics. The Journal\nof Neuroscience, 33(40):15915\u201315929, 2013.\n[49] Terry Elliott. Sparseness, antisparseness and anything in between: The operating point of a\nneuron determines its computational repertoire. Neural computation, pages 1\u201349, 2014.\n[50] Astrid A. Prinz, Dirk Bucher, and Eve Marder. Similar network activity from disparate circuit\nparameters. Nature Neuroscience, 7(12):1345\u20131352, 2004.\n29\n[51] Jitendra Sharma, Alessandra Angelucci, and Mriganka Sur. Induction of visual orientation mod-\nules in auditory cortex. Nature, 404(6780):841\u2013847, 2000.\n[52] Matthias Kaschube, Michael Schnabel, Siegrid Lowel, David M. Coppola, Leonard E. White,\nand Fred Wolf. Universality in the evolution of orientation columns in the visual cortex. Science,\n330(6007):1113\u20131116, 2010.\n[53] P. Jesper Sjostrom, Ede A. Rancz, Arnd Roth, and Michael Hausser. Dendritic excitability and\nsynaptic plasticity. Physiol. Rev., 88(2):769\u2013840, 2008.\n[54] Michael Graupner and Nicolas Brunel. Calcium-based plasticity model explains sensitivity of\nsynaptic changes to spike pattern, rate, and dendritic location. Proceedings of the National\nAcademy of Sciences, 109(10):3991\u20133996, 2012.\n[55] Friedemann Zenke, Guillaume Hennequin, and Wulfram Gerstner. Synaptic plasticity in neural\nnetworks needs homeostasis with a fast rate detector.\nPLoS computational biology, 9(11):\ne1003330, 2013.\n[56] John Kominek and Alan W. Black. The CMU Arctic speech databases. 2004.\n30\n",
        "sentence": " that the information entropy of such sparse representations is sub-optimal by construction, and partially also by the central role of homeostatic regulation in cortical networks and spiking neural network models of sensory areas, see for example [37], [38].",
        "context": "activity, indicating a \"kurtotic\" distribution. The second notion of sparseness is an ensemble concept\nand refers to the very low \ufb01ring rate of neurons, observed in cortical activity45, which may arise\nprimary visual cortex. Journal of Neurophysiology, 88(1):455\u2013463, 2002.\n[39] P. Foldiak. Forming sparse representations by local anti-hebbian learning. Biological cybernet-\nics, 64(2):165\u2013170, 1990.\nlearning, spiking representations and homeostasis in the cortex is an important topic for further\nstudies.\nUniversality supports biological instantiation\nThe principle of nonlinear Hebbian learning has a direct correspondence to biological neurons and"
    },
    {
        "title": "Dropout: A simple way to prevent neural networks from overfitting",
        "author": [
            "N. Srivastava",
            "G. Hinton",
            "A. Krizhevsky",
            "I. Sutskever",
            "R. Salakhutdinov"
        ],
        "venue": "Journal of Machine Learning Research, vol. 15, pp. 1929\u20131958, 2014.",
        "citeRegEx": "39",
        "shortCiteRegEx": null,
        "year": 1929,
        "abstract": "",
        "full_text": "",
        "sentence": " In principle the equiprobable selection mechanism resembles a dropout [39] mechanism where the probability of dropout dynamically depends on the selection rate of each atom.",
        "context": null
    }
]