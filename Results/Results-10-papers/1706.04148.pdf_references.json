[
    {
        "title": "Random search for hyperparameter optimization",
        "author": [
            "James Bergstra",
            "Yoshua Bengio"
        ],
        "venue": "J. Mach. Learn. Res.,",
        "citeRegEx": "2",
        "shortCiteRegEx": "2",
        "year": 2012,
        "abstract": "",
        "full_text": "",
        "sentence": " We tuned the hyper-parameters of each model (baselines included) on the validation set using random search [2].",
        "context": null
    },
    {
        "title": "On the properties of neural machine translation: Encoder\u2013decoder approaches",
        "author": [
            "Kyunghyun Cho",
            "Bart van Merri\u00ebnboer",
            "Dzmitry Bahdanau",
            "Yoshua Bengio"
        ],
        "venue": "In SSST-8: 8th Workshop on Syntax, Semantics and Structure in Statistical Translation,",
        "citeRegEx": "3",
        "shortCiteRegEx": "3",
        "year": 2014,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " Slightly simplified version of LSTM \u2013 that still maintains all their properties \u2013 are Gated Recurrent Units (GRUs) [3] which we use in this work.",
        "context": null
    },
    {
        "title": "Collaborative filtering with recurrent neural networks",
        "author": [
            "Robin Devooght",
            "Hugues Bersini"
        ],
        "venue": "arXiv preprint arXiv:1608.07400,",
        "citeRegEx": "4",
        "shortCiteRegEx": "4",
        "year": 2016,
        "abstract": "We show that collaborative filtering can be viewed as a sequence prediction\nproblem, and that given this interpretation, recurrent neural networks offer\nvery competitive approach. In particular we study how the long short-term\nmemory (LSTM) can be applied to collaborative filtering, and how it compares to\nstandard nearest neighbors and matrix factorization methods on movie\nrecommendation. We show that the LSTM is competitive in all aspects, and\nlargely outperforms other methods in terms of item coverage and short term\npredictions.",
        "full_text": "arXiv:1608.07400v2  [cs.IR]  3 Jan 2017\nCollaborative Filtering with Recurrent Neural Networks\nRobin Devooght\nIRIDIA\nUniversit\u00e9 Libre de Bruxelles\n1050 Brussels, Belgium\nrobin.devooght@ulb.ac.be\nHugues Bersini\nIRIDIA\nUniversit\u00e9 Libre de Bruxelles\n1050 Brussels, Belgium\nbersini@ulb.ac.be\nAbstract\nWe show that collaborative \ufb01ltering can be viewed as a\nsequence prediction problem, and that given this interpre-\ntation, recurrent neural networks o\ufb00er very competitive\napproach. In particular we study how the long short-term\nmemory (LSTM) can be applied to collaborative \ufb01ltering,\nand how it compares to standard nearest neighbors and\nmatrix factorization methods on movie recommendation.\nWe show that the LSTM is competitive in all aspects,\nand largely outperforms other methods in terms of item\ncoverage and short term predictions.\nKeywords.\nCollaborative \ufb01ltering, recommendation\nsystems, recurrent neural network, LSTM, deep learning.\n1\nIntroduction\nCollaborative \ufb01ltering is the problem of recommending\nitems to users based on past interactions between users\nand items. The intuition is that if two users have had\nsimilar interactions in the past, they should look to each\nother for recommendations. The nearest neighbors ap-\nproaches (KNN) are based on that idea, and o\ufb00er mul-\ntiple mechanisms to identify similar users and pool rec-\nommendations. Next to KNN, the other main approach\nfor collaborative \ufb01ltering is matrix factorization (MF),\nwhich frames recommendation as a dimensionality reduc-\ntion problem. MF attempts to represents users and items\nas points in a feature space according to an optimiza-\ntion criterion based on the interactions between users\nand items (i.e. the users should be close to the items\nthat they liked).\nAlthough useful, those methods are\nfar from perfect and improvements are coming more and\nmore slowly. Moreover, they are unadapted to capture\nthe temporal aspects of recommendations, such as evolv-\ning taste or context-dependent interests.\nWe explore a new approach to collaborative \ufb01ltering,\nbased on recurrent neural networks. Modern recurrent\nneural networks such as the LSTM are powerful tools for\nsequence prediction problems and are well-suited to cap-\nture the evolution of users taste. In order to apply them\nto recommendations, we need to reframe collaborative\n\ufb01ltering as a sequence prediction problem.\nIn the following sections, we will describe collaborative\n\ufb01ltering as a sequence prediction problem, then show how\nthe LSTM can be trained on such a problem and compare\nit to traditional collaborative \ufb01ltering approaches. We\nwill then explore some of the many design choices of the\nLSTM.\nOur\ncode\nis\navailable\nin\ngithub.com/rdevooght/sequence-based-recommendations.\n2\nCollaborative \ufb01ltering as a se-\nquence prediction problem\nIn a typical top-N recommendation problem, we consider\na user i at a time t, with St\u2212\ni\nbeing the set of items that\nthe user consumed before t and St+\ni\nthe set of items that\nhe consumed after t. The goal of the recommendation\nsystem is to predict the items in St+\ni\nas a function of\nSt\u2212\ni . In this setting, that we call static, the order in which\nitems are consumed is irrelevant for the recommendation\nsystem.\nInstead, we suggest that the recommendations should\nbe based not only on the set of items that the user con-\nsumed, but on the order in which he consumed them.\nThe user is represented by its sequence of action: he con-\nsumed x1, then x2, then x3, and the goal is to predict his\nnext actions (x4, x5, etc.) based on the beginning of the\nsequence.\nFraming collaborative as a sequence prediction prob-\nlem naturally leads to a distinction between what we call\nshort-term and long-term predictions. A short-term pre-\ndiction aims to predict which item will the user consume\nnext (i.e.\nright after the last one), while a long term\nprediction aims to predict which items will the user con-\nsume eventually.\nIn the static setting, this distinction\ndoes not make sense because the order of items in St+\ni\nis\nignored, and predictions in a static setting are equivalent\nto long term prediction. In sequence prediction however,\nthe models are usually trained to produce short-term pre-\ndiction.\nIt is hard to argue for which type of prediction is more\ninteresting in recommender systems, and ultimately, this\nchoice lies with the user, but it is useful to remember that\na sequence prediction approach is better suited for short-\n1\nterm predictions. Moreover, we will show in Section 3\nthat training for short-prediction is likely to improve the\ndiversity of recommendations. In the following experi-\nments, we will report performances on both short-term\nand long-term predictions.\nThe basic motivation supporting this work is that,\nwhile neglected so far, the information contained in the\nsequence of action could be of great importance for pro-\nducing better recommendations.\nFor example, this se-\nquence can reveal the evolution of a user\u2019s taste. It might\nhelp to identify which items became irrelevant with re-\ngards to the current user\u2019s interest, or which items make\npart of a vanishing interest. It might also help to identify\nwhich items are more in\ufb02uential in changing users taste.\nMethods based on sequence prediction should produce\nricher models. As a matter of fact, a static recommenda-\ntion system models users and items as points in a feature\nspace, frozen in time. It will recommend items that are\nclose to the user in that speci\ufb01c poorly informative space.\nGiven a sequence prediction approach, users are rather\nrepresented by trajectories in the feature space, entailing\na more accurate prediction of the evolution of the users\ninterest (because pursuing this same trajectory).\n3\nSequence prediction favors di-\nversity in recommendations\nAs said in the previous section, methods that ignore the\nsequence of events are trained to produce long-term pre-\ndictions, while methods based on sequence prediction are\nusually trained to produce short-term predictions. The\ntypical metrics to evaluate long term predictions are the\nprecision and recall. If Pi are the predictions based on\nSt\u2212\ni , then the precision is |Pi \u2229St+\ni |/|Pi| and the recall is\n|Pi \u2229St+\ni |/|St+\ni | (we usually compute the precision and\nrecall \u201cat k\u201d, where k|Pi|). In order to evaluate the qual-\nity of short-term prediction, we propose a simple metric\nthat we will call the \u201csequence prediction success at k\u201d\n(sps@k) : |Pi \u2229{xi}|, where xi is the \ufb01rst item of the\nsequence St+\ni .\nAn interesting side-e\ufb00ect of training recommendation\nsystems to optimize short-term rather than long term\npredictions, is that it increases the diversity of the rec-\nommendations. Although we do not have a formal proof,\nthe reasoning is as follows: the correct short term pre-\ndictions are a subset of the correct long term predictions;\nbecause of that, any given item will be a correct predic-\ntion for more users in terms of long term prediction than\nin terms of short term prediction; the result is that it\ntakes fewer items to make correct long term predictions\nfor a given percentage of users than it takes to make cor-\nrect short term predictions.\nWe illustrate that with a simple experiment.\nCon-\nsider an oracle (i.e. a perfect recommendation system)\nthat can only recommend items within the t most pop-\nular ones; Figure 1 shows how the rec@10, prec@10 and\nsps@10 increase as t increases on two real datasets. The\nrec@10 and prec@10 converge very fast, they reach 80%\nof their maximum value with a small fraction of the items,\nand each new item brings only a marginal improvement.\nThe sps@10 on the other hand has a much slower con-\nvergence and requires therefore a higher diversity of rec-\nommendation to reach 80% of its maximum value; we\ntherefore expect that optimizing a recommendation sys-\ntem for short term prediction will force it to produce\nmore diverse recommendations.\n4\nMethods comparison\nRecurrent neural networks (RNNs) might be the most\nversatile methods of sequence prediction, and their re-\ncent success in speech recognition, translation and other\ndomains [6, 17] make them good candidates for the rec-\nommendation problem. In this section we evaluate RNN\non a problem of item recommendation for two movie rec-\nommendation datasets, and compare it to collaborative\n\ufb01ltering methods (that makes no use of the sequence in-\nformation).\n4.1\nDatasets\nOur major constraint in \ufb01nding datasets is the presence\nof information on the order of events (often in the form of\na timestamp). This information is unfortunately missing\nin most collaborative \ufb01ltering dataset, but is available in\ntwo well known datasets of movie recommendation:\n\u2022 Movielens 1M: a rather small version of the Movie-\nlens dataset, with 6040 users and 1,000,209 ratings\nover 3706 movies.\nThis dataset has extra infor-\nmation about the users (age, sex, occupation) and\nabout the movies (year, genre) that will be exploited\nin Section 5.\n\u2022 Net\ufb02ix: A much larger dataset, with about 480k\nusers and 100M ratings over 17770 movies.\nEach set has been splitted into training, validation and\ntest subsets. These subsets where obtained by dividing\nthe users into 3 groups: N randomly chosen users and\nall their ratings to constitute the test set, N others to\nconstitute the validation set and all the remaining users\nfor the training set. We used N = 500 for Movielens 1M\nand N = 1000 for Net\ufb02ix.\nIn those datasets, any item can appear only once in\nthe rating history of any user. We therefore helped all\nthe methods by forcing them to recommend items that\nthe user had not yet seen.\nAlthough the datasets have explicit feedback of users\nin the form of ratings, none of the methods presented\nhere used the values of the rating to build their model\nor make predictions, they just used the fact that an item\nwas rated or not by a user.\n2\n 0\n 0.2\n 0.4\n 0.6\n 0.8\n 1\n 1\n 10\n 100\n 1000\n 10000\nMetric value\nNumber of movies known by the recommender\nprec@10\nrec@10\nsps@10\n(a) Movielens 1M\n 0\n 0.2\n 0.4\n 0.6\n 0.8\n 1\n 1\n 10\n 100\n 1000\n 10000\nNumber of movies known by the recommender\nprec@10\nrec@10\nsps@10\n(b) Net\ufb02ix\nFigure 1: Evolution of the rec@10, prec@10 and sps@10 for a recommendation that can only recommend the top t\nmost popular items. The maximum value of the rec@10 is actually much smaller than 1 but we normalized the recall\nhere by dividing it by its maximum value to make the graph more readable.\n4.2\nRecurrent neural networks\nRNNs are commonly used for language modeling, where\nthey are trained to learn sequences of words [11]. We\ntook a similar approach by considering each item as a\nword, the catalog of items as the full vocabulary, and\nthe history of each user as a sample sequence. The RNN\nruns through the sequence of items consumed by a user,\nitem by item. At each time step, the input is the one-hot\nencoding of the current item, and the output is a softmax\nlayer with a neuron for each item in the catalog. The k\nitems whose neurons are activated the most are used as\nthe k recommendations.\nThe state-of-the-art in recurrent neural networks is\nwhat are called \u201cgated\u201d RNNs, where the internal state\nof the RNN is controled by one or more small neural\nnetworks called gates. The original gated RNN is the\nLSTM[9], but it has spawned multiple variants [3, 7].\nWe trained the RNN to minimize the categorical cross-\nentropy loss, with the only correct item being the next\nitem in the sequence. As said earlier, this approach trains\nthe RNN to focus on short-term prediction so that we can\nunsurprisingly expect to have a high sps.\nFor the following experiments, we used a unidirectional\nsingle-layered LSTM, we tuned the number of hidden neu-\nrons and the learning mechanism over the validation set.\nThe results shown here are obtained using Adagrad [4]\nwith a learning rate of 0.1, 20 hidden neurons for the\nMovielens dataset and 100 for the Net\ufb02ix dataset.\n4.3\nCompeting methods\nWe compare the RNN with two static methods of top-\nN recommendation: one based on nearest neighbors and\none on matrix factorization. We also compare the RNN\nwith a simple Markov chain that can be seen as a baseline\nfor the sequence prediction approach.\n4.3.1\nMarkov chain\nIn this simple method, the users\u2019 behavior is modelled\nby a Markov chain whose states are the di\ufb00erent items.\nThe transition probabilities between items are inferred\nfrom the transition frequencies observed in the training\nset. At any time the state of a user corresponds to the\nlast item that he consumed, and the recommendations\nfor that user will be the k items with the highest tran-\nsition probabilities from that state. In other words, if\nthe last item consumed by a user is the item j, the k\n\ufb01rst recommendations of the Markov model will be the\nk items that followed most often the item j in the se-\nquences coming from other users. This is equivalent to a\nbigram model in language modeling in which the words\nbecome the items.\n4.3.2\nUser-based nearest neighbors\nUser-based nearest neighbors or user KNN is one of the\noldest method of collaborative \ufb01ltering, yet still a strong\nbaseline for top-N recommendation. A score sij is com-\nputed between a user i and an item j:\nsij =\nX\nu\u2208Nk(i)\nciu1(j \u2208Su)\n(1)\nWhere ciu is the similarity between users i and u, Nk(i)\nis the set k users closest to i according to the similar-\nity measure c, and 1(j \u2208Su) is the indicator function\nthat evaluates to 1 if item j belongs to the sequence of\nitems of user u, or else evaluates to 0. We used the co-\nsine similarity measure, which is usualy prefered for item\nrecommendation:\nciu = |Si \u2229Su|/\np\n|Si||Su|\n(2)\nThe size of the neighborhood (k) was optimized by means\nof a validation set.\n4.3.3\nBPR-MF\nBPR-MF\nis\na\nstate-of-the-art\nmatrix\nfactorization\nmethod for top-N recommendation devised by [14]. It\n3\nis based on the Bayesian personalized ranking: an objec-\ntive function similar to the AUC (area under the ROC\ncurve) that can be trained trough stochastic gradient de-\nscent. We used the original implementation of BPR-MF,\navailable in the MyMediaLite framework [5]. BPR-MF\nhas many parameters; we selected the most important\n(number of features and number of iterations) on the ba-\nsis of a validation set, and kept the default values of\nMyMediaLite for the others.\n4.4\nMetrics\nWe compare the di\ufb00erent methods through a range of\nmetrics \ufb01nely designed to capture various qualities of the\nrecommendation systems.\n\u2022 sps.\nThe Short-term Prediction Success captures\nthe ability of the method to predict the next item.\nIt is 1 if the next item is present in the recommen-\ndations, 0 else.\n\u2022 Recall. The usual metrics for top-N recommenda-\ntion captures the ability of the method to do long\nterm predictions.\n\u2022 User coverage. The fraction of users who received\nat least one correct recommendation.\nAverage re-\ncall (and precision) hide the distribution of success\namong users.\nA high recall could still mean that\nmany users do not receive any good recommenda-\ntion.\nThis metrics captures the generality of the\nmethod.\n\u2022 Item coverage. The number of distinct items that\nwere correctly recommended. It captures the capac-\nity of the method to make diverse, successful, recom-\nmendations.\nAll those metrics are computed \u201cat 10\u201d, i.e. in a setting\nwhere the recommendation systems produces ten recom-\nmendations for each user.\n4.5\nTesting procedure\nFor each user of the test set, the method can base its rec-\nommendations on the \ufb01rst half of the user\u2019s ratings and\non the model previously built on the training set (or on\nthe training set directly, in the case of the nearest neigh-\nbours methods). Those recommendations are evaluated\nagainst the second half of the user\u2019s rating, using the\nmetrics described in Section 4.4. The metrics are then\naveraged over all test users.\nThe only method that needs the ratings of a user dur-\ning the construction of the model is BPR-MF. For that\nreason, BPR-MF is trained on a larger training set than\nthe other methods, that includes the \ufb01rst half of the rat-\nings of each of the test users.\nThis is an unfair but\nunfortunately unavoidable advantage for the BPR-MF\nmethod.\nSince BPR-MF and RNN produce stochastic models,\ntable 1 gives the average and the standard deviation of\nthe results over ten models.\n4.6\nAnalysis\nThe results are shown in Table 1. The methods using the\nsequence information are impressively much better than\nthe others in terms of sps. It is worth underlying the\nquality of those results: given ten trials (i.e. ten recom-\nmendations), the RNN is able to predict the next movie\nseen by 33% of the users of Movielens and 40% of the\nusers of Net\ufb02ix, while methods not based on the sequence\nare below 15%. As predicted in Section 3, methods with\nthe best sps also have a larger item coverage, which is\nan important, but often overlooked aspect of recommen-\ndations. In particular, the item coverage of the RNN is\nmore than twice the one of the User KNN.\nAs a global observation, the RNN proves to be a very\npromising method for all considered metrics.\nIt domi-\nnates the other methods in every aspects on the Movie-\nlens dataset and is only beaten in terms of recall by BPR-\nMF and User KNN on Net\ufb02ix.\n5\nVariations of the recurrent neu-\nral networks\nAs we apply RNN to a new problem, it is worth exploring\nhow to adapt it to the speci\ufb01cities of the new problem. In\nthis section we study technical details of the implementa-\ntion such as the learning rate and the number of hidden\nneurons, then we present a modi\ufb01cation of the loss func-\ntion that leads to more diverse recommendations, and we\nstudy how the RNN can use extra information such as\nusers\u2019 age or ratings value to built better models.\n5.1\nLearning method\nModern neural networks are rarely trained using the\nvanilla SGD approach: a set of mechanisms have been\ndevelopped to speed up learning and schedule the de-\ncrease of the learning rate. Two mechanisms have been\nfound to be generally useful:\n\u2022 Momentum: smooths the gradients variations over\ntime in order to avoid \u201czig-zags\u201d during learning[13].\n\u2022 Adaptive learning: decrease the learning rate for fre-\nquently updated parameter (Adagrad[4], rmsprop).\nSome methods, such as Adadelta[18] and Adam[10] com-\nbine both approaches.\nWe observed that in this context, adaptive learning\nseems more important than momentum, with the meth-\nods Adagrad, rmsprop and Adam working particularly\nwell. Adagrad is especially appealing because it requires\nto tune only one parameter: the initial learning rate,\n4\nTable 1: Comparison of top-N recommendation methods on Movielens 1M and Net\ufb02ix\nMetrics (@10)\nMethod\nsps (%)\nItem coverage\nUser coverage (%)\nrec (%)\nMovielens\nBPR-MF\n12.18 \u00b1 0.35\n388.4 \u00b1 7.91\n82.56 \u00b1 0.48\n5.64 \u00b1 0.07\nUser KNN\n14.40\n277\n80.8\n6.31\nMC\n29.20\n518\n77.0\n4.90\nRNN\n33.69 \u00b1 0.58\n649.22 \u00b1 7.88\n86.62 \u00b1 0.38\n7.63 \u00b1 0.1\nNet\ufb02ix\nBPR-MF\n9.76 \u00b1 0.35\n589.22 \u00b1 7.13\n79.96 \u00b1 0.36\n6.92 \u00b1 0.09\nUser KNN\n13.04\n383\n80.84\n8.49\nMC\n32.50\n594\n64.50\n3.17\nRNN\n40.62 \u00b1 0.70\n769.56 \u00b1 8.91\n80.59 \u00b1 0.36\n6.10 \u00b1 0.10\n 0\n 0.05\n 0.1\n 0.15\n 0.2\n 0.25\n 0.3\n 0.35\n 1\n 10\n 100\n 1000\nsps@10\nNumber of epochs\n\u03b7 = 0.02\n\u03b7 = 0.05\n\u03b7 = 0.1\n\u03b7 = 0.2\n\u03b7 = 0.5\nFigure 2: In\ufb02uence of the learning rate parameter with\nthe adagrad scheduler on Movielens. The sps is computed\non the validation set during learning.\nwhile rmsprop requires two, and Adam three. The Ada-\ngrad updates are computed in the following way:\n\u03b8t+1 = \u03b8t \u2212\n\u03b7\n\u221aGt + \u01eb \u2299gt\n(3)\nWhere \u03b8 is the vector of parameters, \u03b7 is the initial learn-\ning rate, \u01eb is a small value to avoid division by zero, gt\nis the gradient of the objective function with regards to\nthe parameters at time t, and Gt = Pt\ni=0 g2\ni is the sum\nof previous square gradients. All parameters start with\nthe same learning rate, but during training the learning\nrate decreases faster for the parameters that are often\nassociated with large gradients. Those parameters will\ntherefore be \ufb01xed early during training, making it easier\nto learn the other parameters, which have more subtle\nin\ufb02uences on the objective function. Figure 2 shows the\nin\ufb02uence of the learning rate with the Adagrad method.\nLearning rates around 0.01 gave the best result, and sim-\nilar observations where made on the Net\ufb02ix dataset.\n5.2\nIn\ufb02uence of the cell size\nAnother important parameter of the of the LSTM is the\nnumber of neurons in the cell. The LSTM will not be able\ntoo learn a rich enough model if the number of neurons is\ntoo small, but more neurons lead to a slower learning and\nit may increase the risk of over-\ufb01tting. A good \ufb01rst choice\nfor the number of neurons in the LSTM cell is the number\nof feature you would use to solve the same problem with\nmatrix factorization. Figure 3 shows the validation error\nduring training for several cell size.\nWe observe that\nwith 10 neurons the model is severely limited, and the\nhighest sps seems to be reached with around 100 neurons.\nHowever, we also observe a faster learning for the smaller\ncell of 20 neurons.\n5.3\nArchitecture\nAlthough we used a vanilla LSTM in all our experiments,\nmany other RNN architectures could have been used. In\nthis section we very brie\ufb02y explore some of the other\npossible types of RNNs:\n\u2022 Bidirectional LSTM: two LSTM are used in parallel,\none reading the inputs in chronological order and\nthe other in the reverse order. The output of both\nLSTM is fed into the last (softmax) output layer.\n\u2022 2-layered LSTM: RNN can be stacked, the output\nof one LSTM feeding the next one.\nWe tried the\nsimplest version, with two layers of LSTM, the \ufb01rst\nreading the initial input, and the second reading the\noutput of the previous layer and producing the pre-\ndictions.\n\u2022 GRU: the gated recurrent unit is simpler than the\nLSTM, with fewer gates and fewer parameters to\ntune[3].\nFigure 4 compares the di\ufb00erent architectures with the\nvanilla LSTM. We report the evolution of the validation\nerror during training, both we regards to the number of\nepochs and to the training time because the choice of ar-\nchitecture has a signi\ufb01cant impact on the speed at which\nthe network is able to process a sequence. In particular,\nthe GRU is faster than the LSTM-based RNNs, and the\ngain in speed does not seem to impact the quality of the\nprediction. On the long run however, the di\ufb00erence be-\ntween GRU and LSTM is negligible. The bidirectional\nand the 2-layered LSTM slightly under-perform, but the\n5\n 0\n 0.05\n 0.1\n 0.15\n 0.2\n 0.25\n 0.3\n 0.35\n 0.4\n 0.01\n 0.1\n 1\n 10\n 100\n 1000\nsps@10\nNumber of epochs\n10 neurons\n20 neurons\n50 neurons\n100 neurons\n200 neurons\n 0\n 0.05\n 0.1\n 0.15\n 0.2\n 0.25\n 0.3\n 0.35\n 0.4\n 100\n 1000\n 10000\n 100000\nsps@10\nTime (s)\n10 neurons\n20 neurons\n50 neurons\n100 neurons\n200 neurons\nFigure 3: In\ufb02uence of the number of neurons on Movielens. The sps is computed on the validation set during learning.\nThe right \ufb01gure shows the sps in function of the training time on one core of an Intel(R) Xeon(R) CPU E5-2620 v3 @\n2.40GHz.\n 0.05\n 0.1\n 0.15\n 0.2\n 0.25\n 0.3\n 0.35\n 0.1\n 1\n 10\n 100\n 1000\nsps@10\nNumber of epochs\nLSTM\nbidirectional LSTM\n2-layered LSTM\nGRU\n 0.05\n 0.1\n 0.15\n 0.2\n 0.25\n 0.3\n 0.35\n 100\n 1000\n 10000\n 100000\nsps@10\nTime (s)\nLSTM\nbidirectional LSTM\n2-layered LSTM\nGRU\nFigure 4: In\ufb02uence of the architecture of the RNN on Movielens. Each type (LSTM, bidirectional LSTM, 2-layered\nLSTM and GRU) have a cell size of 20 neurons. The sps is computed on the validation set during learning. The right\n\ufb01gure shows the sps in function of the training time on one core of an Intel(R) Xeon(R) CPU E5-2620 v3 @ 2.40GHz.\n6\noverall e\ufb00ect of the choice of architecture appears to be\nsmall.\n5.4\nDiversity bias\nAs expressed earlier, diversity is a key challenge of rec-\nommendation systems. The di\ufb03culty comes mainly from\nthe fact that the distribution of popularity among items\nis usually very skewed, in the presence of very few pop-\nular items and much more rarer items. Naturally, any\nmodel trained to optimize the chance of correct recom-\nmendations will learn to often propose the most popular\nitems. Unfortunately, most of the times, those popular\nitems turn out to be trivial, useless recommendations.\nWe present here a small modi\ufb01cation of the objective\nfunction of the RNN, aimed to increase the diversity of\nthe recommendation while not loosing too much preci-\nsion. In Section 4 we used the categorical cross-entropy\nas the objective function of the RNN: L = \u2212log(ocorrect),\nwhere ocorrect is the value of the output neuron corre-\nsponding to the correct item.\nWe test here a slightly\ndi\ufb00erent objective function that lowers the error associ-\nated with mispredicting the most popular items.\nThe\nunderlying rational is that insisting on the rarest items\nshould counteract the bias toward popular items caused\nby the skewed distribution of popularity.\nOur objective function is thus transformed as follows:\nL\u03b4 = \u2212log(ocorrect)/e\u03b4pcorrect\n(4)\nWhere \u03b4 \u2208[0, inf) is the diversity bias parameter and\npcorrect is a measure of popularity associated with the cor-\nrect item. p could be for example the number of views\nor the number of purchases of the item. In our exper-\niment, we constructed p by dividing the items into ten\nbins of logarithmic size, the smaller bin containing the\nmost popular items and the largest bin containing the\nleast popular (in terms of number of ratings). Then we\nset p = 1 for all the items in the largest bin, p = 2 for the\nitems in the second largest, etc. When \u03b4 = 0, the objec-\ntive function is reduced to the categorical cross-entropy,\nand increasing \u03b4 increases the bias towards infrequent\nitems.\nFigure 5 shows that the diversity bias o\ufb00ers an easy\nway to trade precision for item coverage. For small val-\nues of \u03b4 (under 0.2) we can signi\ufb01cantly increase the item\ncoverage with a small loss in sps. With larger values of\n\u03b4, the RNN keeps producing more diverse recommenda-\ntions, but the precision becomes so low that the item cov-\nerage actually decreases (the item coverage is the number\nof distinct items correctly recommended).\n5.5\nExtra features\nThe input of the RNN used in Section 4 consisted only\nin the sequence of items (in the form of a one hot encod-\ning of the last item at each time step). However, many\ndatasets possess much richer information and those same\n 12\n 16\n 20\n 24\n 28\n 32\n 36\n 0\n 0.2\n 0.4\n 0.6\n 0.8\n 1\nsps@10 (%)\n\u03b4\n 500\n 550\n 600\n 650\n 700\n 750\n 0\n 0.2\n 0.4\n 0.6\n 0.8\n 1\nitem coverage@10\n\u03b4\nFigure 5: In\ufb02uence of the diversity bias on the sps and\nthe item coverage.\ninformation might improve the model discovered by the\nRNN. We distinguish three types of extra information:\nthe ones that concern a user (age, sex, etc.), the ones\nthat concern an item (category, price, etc.) and the ones\nthat concern a speci\ufb01c user-item interaction (rating, re-\nview, etc.). Movielens 1M has some of each category:\n\u2022 users\u2019 features:\nage (approximated by seven age\nranges), sex, and occupation (chosen among twenty-\none options)\n\u2022 items\u2019 features: year of release (we used only the\ndecade) and genre (chosen among eighteen cate-\ngories)\n\u2022 interactions\u2019 features: rating value (ten possible val-\nues)\nWe represented each feature with a one hot encoding\nand added them to the normal input. In other words,\nthe input of the RNN that uses the sequence plus, for\ninstance, the users\u2019 features, consists in 3736 neurons:\n3706 for the one hot encoding of the movie, 7 for the age\nrange, 2 for the sex and 21 for the occupation.\nTable 2 shows the in\ufb02uence of using the users\u2019 features,\nthe items\u2019 features or the interactions\u2019 features separately,\nfollowed at last by their combined e\ufb00ect. Interestingly,\nthe gain is only signi\ufb01cant when all the extra features are\ncombined, and even then this gain remains small. This\nsuggests that the sequence of actions already contains\nmost of the information and those extra features are in\na way or another already implicit in these sequences.\n6\nRelated Work\nDeep learning techniques are getting more and more at-\ntention in the recommender system community, but we\nknow of only two attempts to use recurrent neural net-\nwork for collaborative \ufb01ltering. Spotify might have been\n7\nTable 2: E\ufb00ect of extra features for the RNN on Movielens 1M\nMetrics (@10)\nFeatures\nsps (%)\nItem coverage\nUser coverage (%)\nrec (%)\nno extra features\n33.69 \u00b1 0.58\n649.22 \u00b1 7.88\n86.62 \u00b1 0.38\n7.63 \u00b1 0.1\nusers\u2019 features\n33.78 \u00b1 0.74\n656.44 \u00b1 6.28\n87.89 \u00b1 0.48\n7.79 \u00b1 0.06\nitem\u2019s features\n34.07 \u00b1 0.63\n656.67 \u00b1 5.07\n87.9 \u00b1 0.3\n7.69 \u00b1 0.03\ninteractions\u2019 features\n33.04 \u00b1 0.61\n652.56 \u00b1 9.37\n86.89 \u00b1 0.36\n7.56 \u00b1 0.06\nall features\n34.97 \u00b1 0.81\n666.17 \u00b1 3.87\n87.33 \u00b1 0.59\n7.87 \u00b1 0.07\nusing it as far back as 2014 [1] to build playlists. They\nhowever do not seem to be using gated RNN, and they\nare using a hierachical softmax output in order to deal\nwith the very large number of items (they have much\nmore songs than net\ufb02ix or movielens have movies). More\nrecently, [8] has applied gated RNN to session based col-\nlaborative \ufb01ltering. Interestingly, they have used other\nobjective functions than the categorical cross-entropy,\nnamely the Bayesian personalized ranking (the same used\nby BPR-MF) and an other ranking loss called TOP1 that\nthey devised for the task.\nSome earlier works have framed collaborative \ufb01lter-\ning as a sequence prediction problem and used simpler\nMarkov chain methods to solve it. In the early 2000s, [19]\nused a simple Markov model and tested it for web-page\nrecommendation. [12] adopted a similar approach, using\nsequential pattern mining. Both showed the superiority\nof methods based on sequence over nearest-neighbors ap-\nproaches. In [16, 2], Brafman et al. defended the view of\nrecommendation systems as a Markov decision process,\nand although the predictive model was not their main\nfocus, they did present in [16] a Markov chain approach,\nimproved by some heuristics such as skipping and clus-\ntering.\nMore recently, [15] introduced a rather fair approach\nto build personalized Markov chain, exploiting matrix\nfactorization to \ufb01ght the sparsity problem. Their method\nis mainly designed for the next basket recommendation\nproblem, but it would be of great interest to adapt it for\na more general recommendation problem.\n7\nConclusion\nWe explored the use of recurrent neural network, and\nin particular the LSTM, for the collaborative \ufb01ltering\nproblem. Using RNNs requires the re-frame collaborative\n\ufb01ltering as a sequence prediction problem, and it could\nlead to richer models, taking the evolution of users\u2019 taste\ninto account. Our experiments showed that the LSTM\nproduces very good results on the Movielens and Net\ufb02ix\ndatasets, and is especially good in terms of short term\nprediction and item coverage.\nBetter performance still could be achieve by designing\nRNNs speci\ufb01cally for the collaborative \ufb01ltering task, es-\npecially at the level of the objective function, but the fact\nthat standard LSTM works already so well is yet another\nproof of its ability to tackle general problems.\nAcknowledgments\nR. Devooght is supported by the Belgian Fonds pour\nla Recherche dans l\u2019Industrie et l\u2019Agriculture (FRIA,\n1.E041.14).\nReferences\n[1] E. Bernhardsson. Recurrent neural networks for col-\nlaborative \ufb01ltering, 2014. [Online; accessed 20-Mai-\n2016].\n[2] R. I. Brafman, D. Heckerman, and G. Shani. Recom-\nmendation as a stochastic sequential decision prob-\nlem. In ICAPS, pages 164\u2013173, 2003.\n[3] K. Cho, B. Van Merri\u00ebnboer, D. Bahdanau, and\nY. Bengio.\nOn the properties of neural machine\ntranslation:\nEncoder-decoder approaches.\narXiv\npreprint arXiv:1409.1259, 2014.\n[4] J. Duchi, E. Hazan, and Y. Singer. Adaptive subgra-\ndient methods for online learning and stochastic op-\ntimization. Journal of Machine Learning Research,\n12(Jul):2121\u20132159, 2011.\n[5] Z. Gantner,\nS. Rendle,\nC. Freudenthaler,\nand\nL. Schmidt-Thieme.\nMyMediaLite: A free recom-\nmender system library.\nIn Proceedings of the 5th\nACM Conference on Recommender Systems (RecSys\n2011), 2011.\n[6] A. Graves, A.-r. Mohamed, and G. Hinton. Speech\nrecognition with deep recurrent neural networks. In\nAcoustics, Speech and Signal Processing (ICASSP),\n2013 IEEE International Conference on,\npages\n6645\u20136649. IEEE, 2013.\n[7] K. Gre\ufb00, R. K. Srivastava, J. Koutn\u00edk, B. R. Steune-\nbrink, and J. Schmidhuber. Lstm: A search space\nodyssey. arXiv preprint arXiv:1503.04069, 2015.\n[8] B. Hidasi,\nA. Karatzoglou,\nL. Baltrunas,\nand\nD. Tikk. Session-based recommendations with recur-\nrent neural networks. CoRR, abs/1511.06939, 2015.\n8\n[9] S. Hochreiter and J. Schmidhuber. Long short-term\nmemory. Neural computation, 9(8):1735\u20131780, 1997.\n[10] D. Kingma and J. Ba. Adam: A method for stochas-\ntic optimization.\narXiv preprint arXiv:1412.6980,\n2014.\n[11] T. Mikolov, M. Kara\ufb01\u00e1t, L. Burget, J. Cernock`y,\nand S. Khudanpur. Recurrent neural network based\nlanguage model.\nIn INTERSPEECH, volume 2,\npage 3, 2010.\n[12] B. Mobasher, H. Dai, T. Luo, and M. Nakagawa. Us-\ning sequential and non-sequential patterns in predic-\ntive web usage mining tasks. In Data Mining, 2002.\nICDM 2003. Proceedings. 2002 IEEE International\nConference on, pages 669\u2013672. IEEE, 2002.\n[13] N. Qian.\nOn the momentum term in gradi-\nent descent learning algorithms.\nNeural networks,\n12(1):145\u2013151, 1999.\n[14] S. Rendle,\nC. Freudenthaler,\nZ. Gantner,\nand\nL. Schmidt-Thieme.\nBpr:\nBayesian personalized\nranking from implicit feedback. In Proceedings of\nthe twenty-\ufb01fth conference on uncertainty in arti\ufb01-\ncial intelligence, pages 452\u2013461. AUAI Press, 2009.\n[15] S. Rendle,\nC. Freudenthaler,\nand L. Schmidt-\nThieme. Factorizing personalized markov chains for\nnext-basket recommendation. In Proceedings of the\n19th international conference on World wide web,\npages 811\u2013820. ACM, 2010.\n[16] G. Shani, R. I. Brafman, and D. Heckerman. An\nmdp-based recommender system. In Proceedings of\nthe Eighteenth conference on Uncertainty in arti\ufb01-\ncial intelligence, pages 453\u2013460. Morgan Kaufmann\nPublishers Inc., 2002.\n[17] I. Sutskever, O. Vinyals, and Q. V. Le. Sequence\nto sequence learning with neural networks. In Ad-\nvances in neural information processing systems,\npages 3104\u20133112, 2014.\n[18] M. D. Zeiler. Adadelta: an adaptive learning rate\nmethod. arXiv preprint arXiv:1212.5701, 2012.\n[19] A. Zimdars, D. M. Chickering, and C. Meek. Us-\ning temporal data for making recommendations. In\nProceedings of the Seventeenth conference on Uncer-\ntainty in arti\ufb01cial intelligence, pages 580\u2013588. Mor-\ngan Kaufmann Publishers Inc., 2001.\n9\n 0\n 0.05\n 0.1\n 0.15\n 0.2\n 0.25\n 0.3\n 0.35\n 1\n 10\n 100\n 1000\nsps@10\nNumber of epochs\n\u03b7 = 0.02\n\u03b7 = 0.05\n\u03b7 = 0.1\n\u03b7 = 0.2\n\u03b7 = 0.5\n",
        "sentence": " RNN\u2019s have also been used in more standard user-item collaborative filtering settings where the aim is to model the evolution of the user and items factors [20] [4] where the results are though less impressive, with the proposed methods barely outperforming standard matrix factorization methods.",
        "context": "4.3\nCompeting methods\nWe compare the RNN with two static methods of top-\nN recommendation: one based on nearest neighbors and\none on matrix factorization. We also compare the RNN\nwith a simple Markov chain that can be seen as a baseline\ndomains [6, 17] make them good candidates for the rec-\nommendation problem. In this section we evaluate RNN\non a problem of item recommendation for two movie rec-\nommendation datasets, and compare it to collaborative\nbased on recurrent neural networks. Modern recurrent\nneural networks such as the LSTM are powerful tools for\nsequence prediction problems and are well-suited to cap-\nture the evolution of users taste. In order to apply them"
    },
    {
        "title": "Adaptive subgradient methods for online learning and stochastic optimization",
        "author": [
            "John Duchi",
            "Elad Hazan",
            "Yoram Singer"
        ],
        "venue": "The Journal of Machine Learning Research,",
        "citeRegEx": "5",
        "shortCiteRegEx": "5",
        "year": 2011,
        "abstract": "",
        "full_text": "",
        "sentence": " We optimize the neural models for TOP1 loss using AdaGrad [5] with momentum for 10 epochs3.",
        "context": null
    },
    {
        "title": "Fast ALS-based tensor factorization for context-aware recommendation from implicit feedback. In ECML- PKDD\u201912",
        "author": [
            "B. Hidasi",
            "D. Tikk"
        ],
        "venue": "Part II,",
        "citeRegEx": "6",
        "shortCiteRegEx": "6",
        "year": 2012,
        "abstract": "",
        "full_text": "",
        "sentence": " This is an accurate model for certain practical scenarios where no recommendation is highlighted and their absolute order does not matter, and strongly correlates with important KPIs such as CTR [6].",
        "context": null
    },
    {
        "title": "Session-based recommendations with recurrent neural networks",
        "author": [
            "Bal\u00e1zs Hidasi",
            "Alexandros Karatzoglou",
            "Linas Baltrunas",
            "Domonkos Tikk"
        ],
        "venue": "CoRR, abs/1511.06939,",
        "citeRegEx": "7",
        "shortCiteRegEx": "7",
        "year": 2015,
        "abstract": "",
        "full_text": "",
        "sentence": " Recurrent Neural Networks (RNN\u2019s) have been recently used for the purpose of session-based recommendations [7] outperforming item-based methods by 15% to 30% in terms of ranking metrics. RNNs were first used to model session data in [7]. Our model is based on the session-based Recurrent Neural Network (RNN henceforth) model presented in [7]. The network can be trained with several ranking loss functions such as cross-entropy, BPR [14] and TOP1 [7]. \u2219 RNN adopts the same model described in [7]. This is in line with past results over similar datasets [7, 8].",
        "context": null
    },
    {
        "title": "Parallel recurrent neural network architectures for feature-rich session-based recommendations",
        "author": [
            "Bal\u00e1zs Hidasi",
            "Massimo Quadrana",
            "Alexandros Karatzoglou",
            "Domonkos Tikk"
        ],
        "venue": "In Proceedings of the 10th ACM Conference on Recommender Systems, RecSys",
        "citeRegEx": "8",
        "shortCiteRegEx": "8",
        "year": 2016,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " RNNs were also used to jointly model the content or features of items together with click-sequence interactions [8]. For the sake of efficiency in training, we have edited the session-parallel mini-batch mechanism described in [8] to account for user identifiers during training (see Figure 2). This is in line with past results over similar datasets [7, 8]. To speed up evaluation, we computed the rank of the relevant item compared to the 50,000 most supported items, as done in [8].",
        "context": null
    },
    {
        "title": "Long short-term memory",
        "author": [
            "Sepp Hochreiter",
            "J\u00fcrgen Schmidhuber"
        ],
        "venue": "Neural computation,",
        "citeRegEx": "9",
        "shortCiteRegEx": "9",
        "year": 1997,
        "abstract": "",
        "full_text": "",
        "sentence": " Long Short-Term Memory (LSTM) [9] networks are a type of RNNs that have been shown to work particularly well, it includes additional gates that regulate when and how much to take the input into account and when to reset the hidden state.",
        "context": null
    },
    {
        "title": "Towards scalable and accurate item-oriented recommendations",
        "author": [
            "Noam Koenigstein",
            "Yehuda Koren"
        ],
        "venue": "In Proceedings of the 7th ACM Conference on Recommender Systems,",
        "citeRegEx": "10",
        "shortCiteRegEx": "10",
        "year": 2013,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " Providing recommendations in these domains poses unique challenges that until recently have been mainly tackled by applying conventional recommender algorithms [10] on either the last interaction or the last session (session-based recommenders).",
        "context": null
    },
    {
        "title": "Amazon.com recommendations: Item-to-item collaborative filtering",
        "author": [
            "G. Linden",
            "B. Smith",
            "J. York"
        ],
        "venue": "Internet Computing,",
        "citeRegEx": "11",
        "shortCiteRegEx": "11",
        "year": 2003,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " recommendation approach [11, 16].",
        "context": null
    },
    {
        "title": "A critical review of recurrent neural networks for sequence learning",
        "author": [
            "Zachary C Lipton",
            "John Berkowitz",
            "Charles Elkan"
        ],
        "venue": null,
        "citeRegEx": "12",
        "shortCiteRegEx": "12",
        "year": 2015,
        "abstract": "Countless learning tasks require dealing with sequential data. Image\ncaptioning, speech synthesis, and music generation all require that a model\nproduce outputs that are sequences. In other domains, such as time series\nprediction, video analysis, and musical information retrieval, a model must\nlearn from inputs that are sequences. Interactive tasks, such as translating\nnatural language, engaging in dialogue, and controlling a robot, often demand\nboth capabilities. Recurrent neural networks (RNNs) are connectionist models\nthat capture the dynamics of sequences via cycles in the network of nodes.\nUnlike standard feedforward neural networks, recurrent networks retain a state\nthat can represent information from an arbitrarily long context window.\nAlthough recurrent neural networks have traditionally been difficult to train,\nand often contain millions of parameters, recent advances in network\narchitectures, optimization techniques, and parallel computation have enabled\nsuccessful large-scale learning with them. In recent years, systems based on\nlong short-term memory (LSTM) and bidirectional (BRNN) architectures have\ndemonstrated ground-breaking performance on tasks as varied as image\ncaptioning, language translation, and handwriting recognition. In this survey,\nwe review and synthesize the research that over the past three decades first\nyielded and then made practical these powerful learning models. When\nappropriate, we reconcile conflicting notation and nomenclature. Our goal is to\nprovide a self-contained explication of the state of the art together with a\nhistorical perspective and references to primary research.",
        "full_text": "A Critical Review of Recurrent Neural Networks\nfor Sequence Learning\nZachary C. Lipton\nzlipton@cs.ucsd.edu\nJohn Berkowitz\njaberkow@physics.ucsd.edu\nCharles Elkan\nelkan@cs.ucsd.edu\nJune 5th, 2015\nAbstract\nCountless learning tasks require dealing with sequential data. Image\ncaptioning, speech synthesis, and music generation all require that a model\nproduce outputs that are sequences. In other domains, such as time series\nprediction, video analysis, and musical information retrieval, a model must\nlearn from inputs that are sequences. Interactive tasks, such as translat-\ning natural language, engaging in dialogue, and controlling a robot, often\ndemand both capabilities. Recurrent neural networks (RNNs) are connec-\ntionist models that capture the dynamics of sequences via cycles in the\nnetwork of nodes. Unlike standard feedforward neural networks, recurrent\nnetworks retain a state that can represent information from an arbitrarily\nlong context window. Although recurrent neural networks have tradition-\nally been di\ufb03cult to train, and often contain millions of parameters, recent\nadvances in network architectures, optimization techniques, and paral-\nlel computation have enabled successful large-scale learning with them.\nIn recent years, systems based on long short-term memory (LSTM) and\nbidirectional (BRNN) architectures have demonstrated ground-breaking\nperformance on tasks as varied as image captioning, language translation,\nand handwriting recognition.\nIn this survey, we review and synthesize\nthe research that over the past three decades \ufb01rst yielded and then made\npractical these powerful learning models. When appropriate, we reconcile\ncon\ufb02icting notation and nomenclature.\nOur goal is to provide a self-\ncontained explication of the state of the art together with a historical\nperspective and references to primary research.\n1\nIntroduction\nNeural networks are powerful learning models that achieve state-of-the-art re-\nsults in a wide range of supervised and unsupervised machine learning tasks.\n1\narXiv:1506.00019v4  [cs.LG]  17 Oct 2015\nThey are suited especially well for machine perception tasks, where the raw un-\nderlying features are not individually interpretable. This success is attributed\nto their ability to learn hierarchical representations, unlike traditional meth-\nods that rely upon hand-engineered features [Farabet et al., 2013]. Over the\npast several years, storage has become more a\ufb00ordable, datasets have grown\nfar larger, and the \ufb01eld of parallel computing has advanced considerably. In\nthe setting of large datasets, simple linear models tend to under-\ufb01t, and often\nunder-utilize computing resources. Deep learning methods, in particular those\nbased on deep belief networks (DNNs), which are greedily built by stacking re-\nstricted Boltzmann machines, and convolutional neural networks, which exploit\nthe local dependency of visual information, have demonstrated record-setting\nresults on many important applications.\nHowever, despite their power, standard neural networks have limitations.\nMost notably, they rely on the assumption of independence among the training\nand test examples. After each example (data point) is processed, the entire state\nof the network is lost. If each example is generated independently, this presents\nno problem. But if data points are related in time or space, this is unaccept-\nable. Frames from video, snippets of audio, and words pulled from sentences,\nrepresent settings where the independence assumption fails. Additionally, stan-\ndard networks generally rely on examples being vectors of \ufb01xed length. Thus\nit is desirable to extend these powerful learning tools to model data with tem-\nporal or sequential structure and varying length inputs and outputs, especially\nin the many domains where neural networks are already the state of the art.\nRecurrent neural networks (RNNs) are connectionist models with the ability to\nselectively pass information across sequence steps, while processing sequential\ndata one element at a time. Thus they can model input and/or output con-\nsisting of sequences of elements that are not independent. Further, recurrent\nneural networks can simultaneously model sequential and time dependencies on\nmultiple scales.\nIn the following subsections, we explain the fundamental reasons why recur-\nrent neural networks are worth investigating. To be clear, we are motivated by\na desire to achieve empirical results. This motivation warrants clari\ufb01cation be-\ncause recurrent networks have roots in both cognitive modeling and supervised\nmachine learning. Owing to this di\ufb00erence of perspectives, many published pa-\npers have di\ufb00erent aims and priorities. In many foundational papers, generally\npublished in cognitive science and computational neuroscience journals, such as\n[Hop\ufb01eld, 1982, Jordan, 1986, Elman, 1990], biologically plausible mechanisms\nare emphasized.\nIn other papers [Schuster and Paliwal, 1997, Socher et al.,\n2014, Karpathy and Fei-Fei, 2014], biological inspiration is downplayed in favor\nof achieving empirical results on important tasks and datasets.\nThis review\nis motivated by practical results rather than biological plausibility, but where\nappropriate, we draw connections to relevant concepts in neuroscience. Given\nthe empirical aim, we now address three signi\ufb01cant questions that one might\nreasonably want answered before reading further.\n2\n1.1\nWhy model sequentiality explicitly?\nIn light of the practical success and economic value of sequence-agnostic models,\nthis is a fair question. Support vector machines, logistic regression, and feedfor-\nward networks have proved immensely useful without explicitly modeling time.\nArguably, it is precisely the assumption of independence that has led to much\nrecent progress in machine learning. Further, many models implicitly capture\ntime by concatenating each input with some number of its immediate prede-\ncessors and successors, presenting the machine learning model with a sliding\nwindow of context about each point of interest. This approach has been used\nwith deep belief nets for speech modeling by Maas et al. [2012].\nUnfortunately, despite the usefulness of the independence assumption, it\nprecludes modeling long-range dependencies.\nFor example, a model trained\nusing a \ufb01nite-length context window of length 5 could never be trained to answer\nthe simple question, \u201cwhat was the data point seen six time steps ago?\u201d For\na practical application such as call center automation, such a limited system\nmight learn to route calls, but could never participate with complete success\nin an extended dialogue. Since the earliest conception of arti\ufb01cial intelligence,\nresearchers have sought to build systems that interact with humans in time. In\nAlan Turing\u2019s groundbreaking paper Computing Machinery and Intelligence, he\nproposes an \u201cimitation game\u201d which judges a machine\u2019s intelligence by its ability\nto convincingly engage in dialogue [Turing, 1950]. Besides dialogue systems,\nmodern interactive systems of economic importance include self-driving cars\nand robotic surgery, among others. Without an explicit model of sequentiality\nor time, it seems unlikely that any combination of classi\ufb01ers or regressors can\nbe cobbled together to provide this functionality.\n1.2\nWhy not use Markov models?\nRecurrent neural networks are not the only models capable of representing time\ndependencies.\nMarkov chains, which model transitions between states in an\nobserved sequence, were \ufb01rst described by the mathematician Andrey Markov\nin 1906. Hidden Markov models (HMMs), which model an observed sequence\nas probabilistically dependent upon a sequence of unobserved states, were de-\nscribed in the 1950s and have been widely studied since the 1960s [Stratonovich,\n1960]. However, traditional Markov model approaches are limited because their\nstates must be drawn from a modestly sized discrete state space S. The dynamic\nprogramming algorithm that is used to perform e\ufb03cient inference with hidden\nMarkov models scales in time O(|S|2) [Viterbi, 1967]. Further, the transition\ntable capturing the probability of moving between any two time-adjacent states\nis of size |S|2. Thus, standard operations become infeasible with an HMM when\nthe set of possible hidden states grows large. Further, each hidden state can\ndepend only on the immediately previous state. While it is possible to extend a\nMarkov model to account for a larger context window by creating a new state\nspace equal to the cross product of the possible states at each time in the win-\ndow, this procedure grows the state space exponentially with the size of the\n3\nwindow, rendering Markov models computationally impractical for modeling\nlong-range dependencies [Graves et al., 2014].\nGiven the limitations of Markov models, we ought to explain why it is rea-\nsonable that connectionist models, i.e., arti\ufb01cial neural networks, should fare\nbetter. First, recurrent neural networks can capture long-range time depen-\ndencies, overcoming the chief limitation of Markov models. This point requires\na careful explanation. As in Markov models, any state in a traditional RNN\ndepends only on the current input as well as on the state of the network at the\nprevious time step.1 However, the hidden state at any time step can contain\ninformation from a nearly arbitrarily long context window. This is possible be-\ncause the number of distinct states that can be represented in a hidden layer of\nnodes grows exponentially with the number of nodes in the layer. Even if each\nnode took only binary values, the network could represent 2N states where N is\nthe number of nodes in the hidden layer. When the value of each node is a real\nnumber, a network can represent even more distinct states. While the potential\nexpressive power of a network grows exponentially with the number of nodes,\nthe complexity of both inference and training grows at most quadratically.\n1.3\nAre RNNs too expressive?\nFinite-sized RNNs with nonlinear activations are a rich family of models, capa-\nble of nearly arbitrary computation. A well-known result is that a \ufb01nite-sized\nrecurrent neural network with sigmoidal activation functions can simulate a uni-\nversal Turing machine [Siegelmann and Sontag, 1991]. The capability of RNNs\nto perform arbitrary computation demonstrates their expressive power, but one\ncould argue that the C programming language is equally capable of expressing\narbitrary programs. And yet there are no papers claiming that the invention of\nC represents a panacea for machine learning. A fundamental reason is there is\nno simple way of e\ufb03ciently exploring the space of C programs. In particular,\nthere is no general way to calculate the gradient of an arbitrary C program to\nminimize a chosen loss function. Moreover, given any \ufb01nite dataset, there exist\ncountless programs which over\ufb01t the dataset, generating desired training output\nbut failing to generalize to test examples.\nWhy then should RNNs su\ufb00er less from similar problems? First, given any\n\ufb01xed architecture (set of nodes, edges, and activation functions), the recurrent\nneural networks with this architecture are di\ufb00erentiable end to end. The deriva-\ntive of the loss function can be calculated with respect to each of the parameters\n(weights) in the model. Thus, RNNs are amenable to gradient-based training.\nSecond, while the Turing-completeness of RNNs is an impressive property, given\na \ufb01xed-size RNN with a speci\ufb01c architecture, it is not actually possible to re-\nproduce any arbitrary program. Further, unlike a program composed in C, a\nrecurrent neural network can be regularized via standard techniques that help\n1 While traditional RNNs only model the dependence of the current state on the previous\nstate, bidirectional recurrent neural networks (BRNNs) [Schuster and Paliwal, 1997] extend\nRNNs to model dependence on both past states and future states.\n4\nprevent over\ufb01tting, such as weight decay, dropout, and limiting the degrees of\nfreedom.\n1.4\nComparison to prior literature\nThe literature on recurrent neural networks can seem impenetrable to the unini-\ntiated. Shorter papers assume familiarity with a large body of background lit-\nerature, while diagrams are frequently underspeci\ufb01ed, failing to indicate which\nedges span time steps and which do not. Jargon abounds, and notation is in-\nconsistent across papers or overloaded within one paper. Readers are frequently\nin the unenviable position of having to synthesize con\ufb02icting information across\nmany papers in order to understand just one. For example, in many papers sub-\nscripts index both nodes and time steps. In others, h simultaneously stands for a\nlink function and a layer of hidden nodes. The variable t simultaneously stands\nfor both time indices and targets, sometimes in the same equation. Many excel-\nlent research papers have appeared recently, but clear reviews of the recurrent\nneural network literature are rare.\nAmong the most useful resources are a recent book on supervised sequence\nlabeling with recurrent neural networks [Graves, 2012] and an earlier doctoral\nthesis [Gers, 2001]. A recent survey covers recurrent neural nets for language\nmodeling [De Mulder et al., 2015]. Various authors focus on speci\ufb01c technical\naspects; for example Pearlmutter [1995] surveys gradient calculations in contin-\nuous time recurrent neural networks. In the present review paper, we aim to\nprovide a readable, intuitive, consistently notated, and reasonably comprehen-\nsive but selective survey of research on recurrent neural networks for learning\nwith sequences. We emphasize architectures, algorithms, and results, but we\naim also to distill the intuitions that have guided this largely heuristic and\nempirical \ufb01eld. In addition to concrete modeling details, we o\ufb00er qualitative ar-\nguments, a historical perspective, and comparisons to alternative methodologies\nwhere appropriate.\n2\nBackground\nThis section introduces formal notation and provides a brief background on\nneural networks in general.\n2.1\nSequences\nThe input to an RNN is a sequence, and/or its target is a sequence. An input\nsequence can be denoted (x(1), x(2), ..., x(T )) where each data point x(t) is a real-\nvalued vector. Similarly, a target sequence can be denoted (y(1), y(2), ..., y(T )).\nA training set typically is a set of examples where each example is an (input\nsequence, target sequence) pair, although commonly either the input or the\noutput may be a single data point. Sequences may be of \ufb01nite or countably\nin\ufb01nite length. When they are \ufb01nite, the maximum time index of the sequence\n5\nis called T. RNNs are not limited to time-based sequences. They have been\nused successfully on non-temporal sequence data, including genetic data [Baldi\nand Pollastri, 2003]. However, in many important applications of RNNs, the\nsequences have an explicit or implicit temporal aspect. While we often refer to\ntime in this survey, the methods described here are applicable to non-temporal\nas well as to temporal tasks.\nUsing temporal terminology, an input sequence consists of data points x(t)\nthat arrive in a discrete sequence of time steps indexed by t. A target sequence\nconsists of data points y(t). We use superscripts with parentheses for time, and\nnot subscripts, to prevent confusion between sequence steps and indices of nodes\nin a network. When a model produces predicted data points, these are labeled\n\u02c6y(t).\nThe time-indexed data points may be equally spaced samples from a con-\ntinuous real-world process.\nExamples include the still images that comprise\nthe frames of videos or the discrete amplitudes sampled at \ufb01xed intervals that\ncomprise audio recordings. The time steps may also be ordinal, with no exact\ncorrespondence to durations. In fact, RNNs are frequently applied to domains\nwhere sequences have a de\ufb01ned order but no explicit notion of time. This is\nthe case with natural language. In the word sequence \u201cJohn Coltrane plays the\nsaxophone\u201d, x(1) = John, x(2) = Coltrane, etc.\n2.2\nNeural networks\nNeural networks are biologically inspired models of computation.\nGenerally,\na neural network consists of a set of arti\ufb01cial neurons, commonly referred to\nas nodes or units, and a set of directed edges between them, which intuitively\nrepresent the synapses in a biological neural network. Associated with each\nneuron j is an activation function lj(\u00b7), which is sometimes called a link function.\nWe use the notation lj and not hj, unlike some other papers, to distinguish the\nactivation function from the values of the hidden nodes in a network, which, as\na vector, is commonly notated h in the literature.\nAssociated with each edge from node j\u2032 to j is a weight wjj\u2032. Following\nthe convention adopted in several foundational papers [Hochreiter and Schmid-\nhuber, 1997, Gers et al., 2000, Gers, 2001, Sutskever et al., 2011], we index\nneurons with j and j\u2032, and wjj\u2032 denotes the \u201cto-from\u201d weight corresponding to\nthe directed edge to node j from node j\u2032. It is important to note that in many\nreferences the indices are \ufb02ipped and wj\u2032j \u0338= wjj\u2032 denotes the \u201cfrom-to\u201d weight\non the directed edge from the node j\u2032 to the node j, as in lecture notes by Elkan\n[2015] and in Wikipedia [2015].\nThe value vj of each neuron j is calculated by applying its activation function\nto a weighted sum of the values of its input nodes (Figure 1):\nvj = lj\n\uf8eb\n\uf8edX\nj\u2032\nwjj\u2032 \u00b7 vj\u2032\n\uf8f6\n\uf8f8.\nFor convenience, we term the weighted sum inside the parentheses the incoming\n6\nFigure 1: An arti\ufb01cial neuron computes a nonlinear function of a weighted sum\nof its inputs.\nactivation and notate it as aj.\nWe represent this computation in diagrams\nby depicting neurons as circles and edges as arrows connecting them. When\nappropriate, we indicate the exact activation function with a symbol, e.g., \u03c3 for\nsigmoid.\nCommon choices for the activation function include the sigmoid \u03c3(z) =\n1/(1 + e\u2212z) and the tanh function \u03c6(z) = (ez \u2212e\u2212z)/(ez + e\u2212z). The latter has\nbecome common in feedforward neural nets and was applied to recurrent nets by\nSutskever et al. [2011]. Another activation function which has become prominent\nin deep learning research is the recti\ufb01ed linear unit (ReLU) whose formula is\nlj(z) = max(0, z). This type of unit has been demonstrated to improve the\nperformance of many deep neural networks [Nair and Hinton, 2010, Maas et al.,\n2012, Zeiler et al., 2013] on tasks as varied as speech processing and object\nrecognition, and has been used in recurrent neural networks by Bengio et al.\n[2013].\nThe activation function at the output nodes depends upon the task. For mul-\nticlass classi\ufb01cation with K alternative classes, we apply a softmax nonlinearity\nin an output layer of K nodes. The softmax function calculates\n\u02c6yk =\neak\nPK\nk\u2032=1 eak\u2032 for k = 1 to k = K.\nThe denominator is a normalizing term consisting of the sum of the numerators,\nensuring that the outputs of all nodes sum to one. For multilabel classi\ufb01cation\nthe activation function is simply a point-wise sigmoid, and for regression we\ntypically have linear output.\n7\nFigure 2: A feedforward neural network. An example is presented to the network\nby setting the values of the blue (bottom) nodes. The values of the nodes in each\nlayer are computed successively as a function of the prior layers until output is\nproduced at the topmost layer.\n2.3\nFeedforward networks and backpropagation\nWith a neural model of computation, one must determine the order in which\ncomputation should proceed. Should nodes be sampled one at a time and up-\ndated, or should the value of all nodes be calculated at once and then all updates\napplied simultaneously? Feedforward networks (Figure 2) are a restricted class\nof networks which deal with this problem by forbidding cycles in the directed\ngraph of nodes. Given the absence of cycles, all nodes can be arranged into\nlayers, and the outputs in each layer can be calculated given the outputs from\nthe lower layers.\nThe input x to a feedforward network is provided by setting the values of\nthe lowest layer. Each higher layer is then successively computed until output\nis generated at the topmost layer \u02c6y. Feedforward networks are frequently used\nfor supervised learning tasks such as classi\ufb01cation and regression. Learning is\naccomplished by iteratively updating each of the weights to minimize a loss\nfunction, L(\u02c6y, y), which penalizes the distance between the output \u02c6y and the\ntarget y.\nThe most successful algorithm for training neural networks is backpropaga-\ntion, introduced for this purpose by Rumelhart et al. [1985]. Backpropagation\nuses the chain rule to calculate the derivative of the loss function L with respect\nto each parameter in the network. The weights are then adjusted by gradi-\nent descent. Because the loss surface is non-convex, there is no assurance that\nbackpropagation will reach a global minimum. Moreover, exact optimization is\nknown to be an NP-hard problem. However, a large body of work on heuristic\n8\npre-training and optimization techniques has led to impressive empirical success\non many supervised learning tasks. In particular, convolutional neural networks,\npopularized by Le Cun et al. [1990], are a variant of feedforward neural network\nthat holds records since 2012 in many computer vision tasks such as object\ndetection [Krizhevsky et al., 2012].\nNowadays, neural networks are usually trained with stochastic gradient de-\nscent (SGD) using mini-batches. With batch size equal to one, the stochastic\ngradient update equation is\nw \u2190w \u2212\u03b7\u2207wFi\nwhere \u03b7 is the learning rate and \u2207wFi is the gradient of the objective function\nwith respect to the parameters w as calculated on a single example (xi, yi).\nMany variants of SGD are used to accelerate learning. Some popular heuristics,\nsuch as AdaGrad [Duchi et al., 2011], AdaDelta [Zeiler, 2012], and RMSprop\n[Tieleman and Hinton, 2012], tune the learning rate adaptively for each feature.\nAdaGrad, arguably the most popular, adapts the learning rate by caching the\nsum of squared gradients with respect to each parameter at each time step.\nThe step size for each feature is multiplied by the inverse of the square root of\nthis cached value. AdaGrad leads to fast convergence on convex error surfaces,\nbut because the cached sum is monotonically increasing, the method has a\nmonotonically decreasing learning rate, which may be undesirable on highly non-\nconvex loss surfaces. RMSprop modi\ufb01es AdaGrad by introducing a decay factor\nin the cache, changing the monotonically growing value into a moving average.\nMomentum methods are another common SGD variant used to train neural\nnetworks. These methods add to each update a decaying sum of the previous\nupdates.\nWhen the momentum parameter is tuned well and the network is\ninitialized well, momentum methods can train deep nets and recurrent nets\ncompetitively with more computationally expensive methods like the Hessian-\nfree optimizer of Sutskever et al. [2013].\nTo calculate the gradient in a feedforward neural network, backpropagation\nproceeds as follows. First, an example is propagated forward through the net-\nwork to produce a value vj at each node and outputs \u02c6y at the topmost layer.\nThen, a loss function value L(\u02c6yk, yk) is computed at each output node k. Sub-\nsequently, for each output node k, we calculate\n\u03b4k = \u2202L(\u02c6yk, yk)\n\u2202\u02c6yk\n\u00b7 l\u2032\nk(ak).\nGiven these values \u03b4k, for each node in the immediately prior layer we calculate\n\u03b4j = l\u2032(aj)\nX\nk\n\u03b4k \u00b7 wkj.\nThis calculation is performed successively for each lower layer to yield \u03b4j for\nevery node j given the \u03b4 values for each node connected to j by an outgoing\nedge. Each value \u03b4j represents the derivative \u2202L/\u2202aj of the total loss function\nwith respect to that node\u2019s incoming activation. Given the values vj calculated\n9\nduring the forward pass, and the values \u03b4j calculated during the backward pass,\nthe derivative of the loss L with respect a given parameter wjj\u2032 is\n\u2202L\n\u2202wjj\u2032 = \u03b4jvj\u2032.\nOther methods have been explored for learning the weights in a neural net-\nwork. A number of papers from the 1990s [Belew et al., 1990, Gruau et al.,\n1994] championed the idea of learning neural networks with genetic algorithms,\nwith some even claiming that achieving success on real-world problems only by\napplying many small changes to the weights of a network was impossible. De-\nspite the subsequent success of backpropagation, interest in genetic algorithms\ncontinues. Several recent papers explore genetic algorithms for neural networks,\nespecially as a means of learning the architecture of neural networks, a problem\nnot addressed by backpropagation [Bayer et al., 2009, Harp and Samad, 2013].\nBy architecture we mean the number of layers, the number of nodes in each, the\nconnectivity pattern among the layers, the choice of activation functions, etc.\nOne open question in neural network research is how to exploit sparsity in\ntraining. In a neural network with sigmoidal or tanh activation functions, the\nnodes in each layer never take value exactly zero.\nThus, even if the inputs\nare sparse, the nodes at each hidden layer are not. However, recti\ufb01ed linear\nunits (ReLUs) introduce sparsity to hidden layers [Glorot et al., 2011]. In this\nsetting, a promising path may be to store the sparsity pattern when computing\neach layer\u2019s values and use it to speed up computation of the next layer in the\nnetwork. Some recent work shows that given sparse inputs to a linear model\nwith a standard regularizer, sparsity can be fully exploited even if regularization\nmakes the gradient be not sparse [Carpenter, 2008, Langford et al., 2009, Singer\nand Duchi, 2009, Lipton and Elkan, 2015].\n3\nRecurrent neural networks\nRecurrent neural networks are feedforward neural networks augmented by the\ninclusion of edges that span adjacent time steps, introducing a notion of time\nto the model. Like feedforward networks, RNNs may not have cycles among\nconventional edges. However, edges that connect adjacent time steps, called\nrecurrent edges, may form cycles, including cycles of length one that are self-\nconnections from a node to itself across time. At time t, nodes with recurrent\nedges receive input from the current data point x(t) and also from hidden node\nvalues h(t\u22121) in the network\u2019s previous state. The output \u02c6y(t) at each time t\nis calculated given the hidden node values h(t) at time t. Input x(t\u22121) at time\nt \u22121 can in\ufb02uence the output \u02c6y(t) at time t and later by way of the recurrent\nconnections.\nTwo equations specify all calculations necessary for computation at each time\nstep on the forward pass in a simple recurrent neural network as in Figure 3:\nh(t) = \u03c3(Whxx(t) + Whhh(t\u22121) + bh)\n10\nFigure 3: A simple recurrent network. At each time step t, activation is passed\nalong solid edges as in a feedforward network. Dashed edges connect a source\nnode at each time t to a target node at each following time t + 1.\n\u02c6y(t) = softmax(Wyhh(t) + by).\nHere Whx is the matrix of conventional weights between the input and the\nhidden layer and Whh is the matrix of recurrent weights between the hidden\nlayer and itself at adjacent time steps. The vectors bh and by are bias parameters\nwhich allow each node to learn an o\ufb00set.\nThe dynamics of the network depicted in Figure 3 across time steps can\nbe visualized by unfolding it as in Figure 4. Given this picture, the network\ncan be interpreted not as cyclic, but rather as a deep network with one layer\nper time step and shared weights across time steps. It is then clear that the\nunfolded network can be trained across many time steps using backpropagation.\nThis algorithm, called backpropagation through time (BPTT), was introduced\nby Werbos [1990]. All recurrent networks in common current use apply it.\n3.1\nEarly recurrent network designs\nThe foundational research on recurrent networks took place in the 1980s. In\n1982, Hop\ufb01eld introduced a family of recurrent neural networks that have pat-\ntern recognition capabilities [Hop\ufb01eld, 1982]. They are de\ufb01ned by the values\nof the weights between nodes and the link functions are simple thresholding at\nzero. In these nets, a pattern is placed in the network by setting the values of\nthe nodes. The network then runs for some time according to its update rules,\nand eventually another pattern is read out. Hop\ufb01eld networks are useful for\nrecovering a stored pattern from a corrupted version and are the forerunners of\nBoltzmann machines and auto-encoders.\n11\nFigure 4: The recurrent network of Figure 3 unfolded across time steps.\nAn early architecture for supervised learning on sequences was introduced\nby Jordan [1986]. Such a network (Figure 5) is a feedforward network with a\nsingle hidden layer that is extended with special units.2 Output node values\nare fed to the special units, which then feed these values to the hidden nodes\nat the following time step. If the output values are actions, the special units\nallow the network to remember actions taken at previous time steps. Several\nmodern architectures use a related form of direct transfer from output nodes;\nSutskever et al. [2014] translates sentences between natural languages, and when\ngenerating a text sequence, the word chosen at each time step is fed into the\nnetwork as input at the following time step.\nAdditionally, the special units\nin a Jordan network are self-connected. Intuitively, these edges allow sending\ninformation across multiple time steps without perturbing the output at each\nintermediate time step.\nThe architecture introduced by Elman [1990] is simpler than the earlier\nJordan architecture. Associated with each unit in the hidden layer is a context\nunit. Each such unit j\u2032 takes as input the state of the corresponding hidden\nnode j at the previous time step, along an edge of \ufb01xed weight wj\u2032j = 1. This\nvalue then feeds back into the same hidden node j along a standard edge. This\narchitecture is equivalent to a simple RNN in which each hidden node has a\nsingle self-connected recurrent edge. The idea of \ufb01xed-weight recurrent edges\nthat make hidden nodes self-connected is fundamental in subsequent work on\nLSTM networks [Hochreiter and Schmidhuber, 1997].\nElman [1990] trains the network using backpropagation and demonstrates\nthat the network can learn time dependencies.\nThe paper features two sets\nof experiments. The \ufb01rst extends the logical operation exclusive or (XOR) to\n2 Jordan [1986] calls the special units \u201cstate units\u201d while Elman [1990] calls a corresponding\nstructure \u201ccontext units.\u201d In this paper we simplify terminology by using only \u201ccontext units\u201d.\n12\nFigure 5: A recurrent neural network as proposed by Jordan [1986]. Output\nunits are connected to special units that at the next time step feed into them-\nselves and into hidden units.\nthe time domain by concatenating sequences of three tokens. For each three-\ntoken segment, e.g. \u201c011\u201d, the \ufb01rst two tokens (\u201c01\u201d) are chosen randomly and\nthe third (\u201c1\u201d) is set by performing xor on the \ufb01rst two. Random guessing\nshould achieve accuracy of 50%. A perfect system should perform the same as\nrandom for the \ufb01rst two tokens, but guess the third token perfectly, achieving\naccuracy of 66.7%. The simple network of Elman [1990] does in fact approach\nthis maximum achievable score.\n3.2\nTraining recurrent networks\nLearning with recurrent networks has long been considered to be di\ufb03cult. Even\nfor standard feedforward networks, the optimization task is NP-complete Blum\nand Rivest [1993]. But learning with recurrent networks can be especially chal-\nlenging due to the di\ufb03culty of learning long-range dependencies, as described\nby Bengio et al. [1994] and expanded upon by Hochreiter et al. [2001]. The\nproblems of vanishing and exploding gradients occur when backpropagating er-\nrors across many time steps. As a toy example, consider a network with a single\ninput node, a single output node, and a single recurrent hidden node (Figure 7).\nNow consider an input passed to the network at time \u03c4 and an error calculated\nat time t, assuming input of zero in the intervening time steps. The tying of\nweights across time steps means that the recurrent edge at the hidden node j\nalways has the same weight. Therefore, the contribution of the input at time \u03c4\nto the output at time t will either explode or approach zero, exponentially fast,\nas t \u2212\u03c4 grows large. Hence the derivative of the error with respect to the input\nwill either explode or vanish.\n13\nFigure 6: A recurrent neural network as described by Elman [1990]. Hidden\nunits are connected to context units, which feed back into the hidden units at\nthe next time step.\nWhich of the two phenomena occurs depends on whether the weight of the\nrecurrent edge |wjj| > 1 or |wjj| < 1 and on the activation function in the\nhidden node (Figure 8). Given a sigmoid activation function, the vanishing gra-\ndient problem is more pressing, but with a recti\ufb01ed linear unit max(0, x), it is\neasier to imagine the exploding gradient. Pascanu et al. [2012] give a thorough\nmathematical treatment of the vanishing and exploding gradient problems, char-\nacterizing exact conditions under which these problems may occur. Given these\nconditions, they suggest an approach to training via a regularization term that\nforces the weights to values where the gradient neither vanishes nor explodes.\nTruncated backpropagation through time (TBPTT) is one solution to the\nexploding gradient problem for continuously running networks [Williams and\nZipser, 1989]. With TBPTT, some maximum number of time steps is set along\nwhich error can be propagated. While TBPTT with a small cuto\ufb00can be used\nto alleviate the exploding gradient problem, it requires that one sacri\ufb01ce the\nability to learn long-range dependencies.\nThe LSTM architecture described\nbelow uses carefully designed nodes with recurrent edges with \ufb01xed unit weight\nas a solution to the vanishing gradient problem.\nThe issue of local optima is an obstacle to e\ufb00ective training that cannot\nbe dealt with simply by modifying the network architecture. Optimizing even\na single hidden-layer feedforward network is an NP-complete problem [Blum\nand Rivest, 1993]. However, recent empirical and theoretical studies suggest\nthat in practice, the issue may not be as important as once thought. Dauphin\net al. [2014] show that while many critical points exist on the error surfaces of\nlarge neural networks, the ratio of saddle points to true local minima increases\nexponentially with the size of the network, and algorithms can be designed to\n14\nFigure 7: A simple recurrent net with one input unit, one output unit, and one\nrecurrent hidden unit.\nFigure 8: A visualization of the vanishing gradient problem, using the network\ndepicted in Figure 7, adapted from Graves [2012].\nIf the weight along the\nrecurrent edge is less than one, the contribution of the input at the \ufb01rst time\nstep to the output at the \ufb01nal time step will decrease exponentially fast as a\nfunction of the length of the time interval in between.\n15\nescape from saddle points.\nOverall, along with the improved architectures explained below, fast imple-\nmentations and better gradient-following heuristics have rendered RNN training\nfeasible. Implementations of forward and backward propagation using GPUs,\nsuch as the Theano [Bergstra et al., 2010] and Torch [Collobert et al., 2011]\npackages, have made it straightforward to implement fast training algorithms.\nIn 1996, prior to the introduction of the LSTM, attempts to train recurrent nets\nto bridge long time gaps were shown to perform no better than random guess-\ning [Hochreiter and Schmidhuber, 1996]. However, RNNs are now frequently\ntrained successfully.\nFor some tasks, freely available software can be run on a single GPU and\nproduce compelling results in hours [Karpathy, 2015]. Martens and Sutskever\n[2011] reported success training recurrent neural networks with a Hessian-free\ntruncated Newton approach, and applied the method to a network which learns\nto generate text one character at a time in [Sutskever et al., 2011]. In the paper\nthat describes the abundance of saddle points on the error surfaces of neural\nnetworks [Dauphin et al., 2014], the authors present a saddle-free version of New-\nton\u2019s method. Unlike Newton\u2019s method, which is attracted to critical points,\nincluding saddle points, this variant is specially designed to escape from them.\nExperimental results include a demonstration of improved performance on re-\ncurrent networks. Newton\u2019s method requires computing the Hessian, which is\nprohibitively expensive for large networks, scaling quadratically with the num-\nber of parameters. While their algorithm only approximates the Hessian, it is\nstill computationally expensive compared to SGD. Thus the authors describe\na hybrid approach in which the saddle-free Newton method is applied only in\nplaces where SGD appears to be stuck.\n4\nModern RNN architectures\nThe most successful RNN architectures for sequence learning stem from two pa-\npers published in 1997. The \ufb01rst paper, Long Short-Term Memory by Hochre-\niter and Schmidhuber [1997], introduces the memory cell, a unit of computation\nthat replaces traditional nodes in the hidden layer of a network. With these\nmemory cells, networks are able to overcome di\ufb03culties with training encoun-\ntered by earlier recurrent networks. The second paper, Bidirectional Recurrent\nNeural Networks by Schuster and Paliwal [1997], introduces an architecture in\nwhich information from both the future and the past are used to determine the\noutput at any point in the sequence. This is in contrast to previous networks,\nin which only past input can a\ufb00ect the output, and has been used success-\nfully for sequence labeling tasks in natural language processing, among others.\nFortunately, the two innovations are not mutually exclusive, and have been suc-\ncessfully combined for phoneme classi\ufb01cation [Graves and Schmidhuber, 2005]\nand handwriting recognition [Graves et al., 2009]. In this section we explain the\nLSTM and BRNN and we describe the neural Turing machine (NTM), which\nextends RNNs with an addressable external memory [Graves et al., 2014].\n16\nFigure 9: One LSTM memory cell as proposed by Hochreiter and Schmidhuber\n[1997]. The self-connected node is the internal state s. The diagonal line indi-\ncates that it is linear, i.e. the identity link function is applied. The blue dashed\nline is the recurrent edge, which has \ufb01xed unit weight. Nodes marked \u03a0 output\nthe product of their inputs. All edges into and from \u03a0 nodes also have \ufb01xed\nunit weight.\n4.1\nLong short-term memory (LSTM)\nHochreiter and Schmidhuber [1997] introduced the LSTM model primarily in\norder to overcome the problem of vanishing gradients. This model resembles\na standard recurrent neural network with a hidden layer, but each ordinary\nnode (Figure 1) in the hidden layer is replaced by a memory cell (Figure 9).\nEach memory cell contains a node with a self-connected recurrent edge of \ufb01xed\nweight one, ensuring that the gradient can pass across many time steps without\nvanishing or exploding. To distinguish references to a memory cell and not an\nordinary node, we use the subscript c.\nThe term \u201clong short-term memory\u201d comes from the following intuition.\nSimple recurrent neural networks have long-term memory in the form of weights.\nThe weights change slowly during training, encoding general knowledge about\nthe data.\nThey also have short-term memory in the form of ephemeral ac-\ntivations, which pass from each node to successive nodes. The LSTM model\nintroduces an intermediate type of storage via the memory cell. A memory cell\nis a composite unit, built from simpler nodes in a speci\ufb01c connectivity pattern,\nwith the novel inclusion of multiplicative nodes, represented in diagrams by the\nletter \u03a0. All elements of the LSTM cell are enumerated and described below.\nNote that when we use vector notation, we are referring to the values of the\n17\nnodes in an entire layer of cells. For example, s is a vector containing the value\nof sc at each memory cell c in a layer. When the subscript c is used, it is to\nindex an individual memory cell.\n\u2022 Input node: This unit, labeled gc, is a node that takes activation in the\nstandard way from the input layer x(t) at the current time step and (along\nrecurrent edges) from the hidden layer at the previous time step h(t\u22121).\nTypically, the summed weighted input is run through a tanh activation\nfunction, although in the original LSTM paper, the activation function is\na sigmoid.\n\u2022 Input gate: Gates are a distinctive feature of the LSTM approach. A gate\nis a sigmoidal unit that, like the input node, takes activation from the\ncurrent data point x(t) as well as from the hidden layer at the previous\ntime step. A gate is so-called because its value is used to multiply the\nvalue of another node. It is a gate in the sense that if its value is zero,\nthen \ufb02ow from the other node is cut o\ufb00. If the value of the gate is one, all\n\ufb02ow is passed through. The value of the input gate ic multiplies the value\nof the input node.\n\u2022 Internal state: At the heart of each memory cell is a node sc with lin-\near activation, which is referred to in the original paper as the \u201cinternal\nstate\u201d of the cell. The internal state sc has a self-connected recurrent edge\nwith \ufb01xed unit weight. Because this edge spans adjacent time steps with\nconstant weight, error can \ufb02ow across time steps without vanishing or ex-\nploding. This edge is often called the constant error carousel. In vector\nnotation, the update for the internal state is s(t) = g(t) \u2299i(t) + s(t\u22121)\nwhere \u2299is pointwise multiplication.\n\u2022 Forget gate: These gates fc were introduced by Gers et al. [2000]. They\nprovide a method by which the network can learn to \ufb02ush the contents\nof the internal state.\nThis is especially useful in continuously running\nnetworks. With forget gates, the equation to calculate the internal state\non the forward pass is\ns(t) = g(t) \u2299i(t) + f (t) \u2299s(t\u22121).\n\u2022 Output gate: The value vc ultimately produced by a memory cell is the\nvalue of the internal state sc multiplied by the value of the output gate\noc. It is customary that the internal state \ufb01rst be run through a tanh\nactivation function, as this gives the output of each cell the same dynamic\nrange as an ordinary tanh hidden unit. However, in other neural network\nresearch, recti\ufb01ed linear units, which have a greater dynamic range, are\neasier to train. Thus it seems plausible that the nonlinear function on the\ninternal state might be omitted.\nIn the original paper and in most subsequent work, the input node is labeled\ng. We adhere to this convention but note that it may be confusing as g does\n18\nFigure 10: LSTM memory cell with a forget gate as described by Gers et al.\n[2000].\nnot stand for gate. In the original paper, the gates are called yin and yout but\nthis is confusing because y generally stands for output in the machine learning\nliterature. Seeking comprehensibility, we break with this convention and use i,\nf, and o to refer to input, forget and output gates respectively, as in Sutskever\net al. [2014].\nSince the original LSTM was introduced, several variations have been pro-\nposed. Forget gates, described above, were proposed in 2000 and were not part\nof the original LSTM design. However, they have proven e\ufb00ective and are stan-\ndard in most modern implementations. That same year, Gers and Schmidhuber\n[2000] proposed peephole connections that pass from the internal state directly\nto the input and output gates of that same node without \ufb01rst having to be\nmodulated by the output gate. They report that these connections improve\nperformance on timing tasks where the network must learn to measure precise\nintervals between events. The intuition of the peephole connection can be cap-\ntured by the following example. Consider a network which must learn to count\nobjects and emit some desired output when n objects have been seen. The net-\nwork might learn to let some \ufb01xed amount of activation into the internal state\nafter each object is seen. This activation is trapped in the internal state sc by\nthe constant error carousel, and is incremented iteratively each time another\nobject is seen. When the nth object is seen, the network needs to know to let\nout content from the internal state so that it can a\ufb00ect the output. To accom-\nplish this, the output gate oc must know the content of the internal state sc.\nThus sc should be an input to oc.\nPut formally, computation in the LSTM model proceeds according to the\n19\nFigure 11: A recurrent neural network with a hidden layer consisting of two\nmemory cells. The network is shown unfolded across two time steps.\nfollowing calculations, which are performed at each time step. These equations\ngive the full algorithm for a modern LSTM with forget gates:\ng(t) = \u03c6(Wgxx(t) + Wghh(t\u22121) + bg)\ni(t) = \u03c3(Wixx(t) + Wihh(t\u22121) + bi)\nf (t) = \u03c3(Wfxx(t) + Wfhh(t\u22121) + bf)\no(t) = \u03c3(Woxx(t) + Wohh(t\u22121) + bo)\ns(t) = g(t) \u2299i(i) + s(t\u22121) \u2299f (t)\nh(t) = \u03c6(s(t)) \u2299o(t).\nThe value of the hidden layer of the LSTM at time t is the vector h(t), while\nh(t\u22121) is the values output by each memory cell in the hidden layer at the pre-\nvious time. Note that these equations include the forget gate, but not peephole\nconnections. The calculations for the simpler LSTM without forget gates are\nobtained by setting f (t) = 1 for all t. We use the tanh function \u03c6 for the input\nnode g following the state-of-the-art design of Zaremba and Sutskever [2014].\nHowever, in the original LSTM paper, the activation function for g is the sigmoid\n\u03c3.\nIntuitively, in terms of the forward pass, the LSTM can learn when to let\nactivation into the internal state. As long as the input gate takes value zero,\nno activation can get in.\nSimilarly, the output gate learns when to let the\n20\nFigure 12: A bidirectional recurrent neural network as described by Schuster\nand Paliwal [1997], unfolded in time.\nvalue out. When both gates are closed, the activation is trapped in the memory\ncell, neither growing nor shrinking, nor a\ufb00ecting the output at intermediate\ntime steps. In terms of the backwards pass, the constant error carousel enables\nthe gradient to propagate back across many time steps, neither exploding nor\nvanishing. In this sense, the gates are learning when to let error in, and when\nto let it out. In practice, the LSTM has shown a superior ability to learn long-\nrange dependencies as compared to simple RNNs. Consequently, the majority of\nstate-of-the-art application papers covered in this review use the LSTM model.\nOne frequent point of confusion is the manner in which multiple memory\ncells are used together to comprise the hidden layer of a working neural network.\nTo alleviate this confusion, we depict in Figure 11 a simple network with two\nmemory cells, analogous to Figure 4. The output from each memory cell \ufb02ows\nin the subsequent time step to the input node and all gates of each memory cell.\nIt is common to include multiple layers of memory cells [Sutskever et al., 2014].\nTypically, in these architectures each layer takes input from the layer below at\nthe same time step and from the same layer in the previous time step.\n4.2\nBidirectional recurrent neural networks (BRNNs)\nAlong with the LSTM, one of the most used RNN architectures is the bidirec-\ntional recurrent neural network (BRNN) (Figure 12) \ufb01rst described by Schuster\nand Paliwal [1997]. In this architecture, there are two layers of hidden nodes.\nBoth hidden layers are connected to input and output. The two hidden layers\nare di\ufb00erentiated in that the \ufb01rst has recurrent connections from the past time\n21\nsteps while in the second the direction of recurrent of connections is \ufb02ipped,\npassing activation backwards along the sequence. Given an input sequence and\na target sequence, the BRNN can be trained by ordinary backpropagation after\nunfolding across time. The following three equations describe a BRNN:\nh(t) = \u03c3(Whxx(t) + Whhh(t\u22121) + bh)\nz(t) = \u03c3(Wzxx(t) + Wzzz(t+1) + bz)\n\u02c6y(t) = softmax(Wyhh(t) + Wyzz(t) + by)\nwhere h(t) and z(t) are the values of the hidden layers in the forwards and\nbackwards directions respectively.\nOne limitation of the BRNN is that cannot run continuously, as it requires\na \ufb01xed endpoint in both the future and in the past.\nFurther, it is not an\nappropriate machine learning algorithm for the online setting, as it is implausible\nto receive information from the future, i.e., to know sequence elements that\nhave not been observed. But for prediction over a sequence of \ufb01xed length, it\nis often sensible to take into account both past and future sequence elements.\nConsider the natural language task of part-of-speech tagging. Given any word\nin a sentence, information about both the words which precede and those which\nfollow it is useful for predicting that word\u2019s part-of-speech.\nThe LSTM and BRNN are in fact compatible ideas. The former introduces a\nnew basic unit from which to compose a hidden layer, while the latter concerns\nthe wiring of the hidden layers, regardless of what nodes they contain. Such an\napproach, termed a BLSTM has been used to achieve state of the art results on\nhandwriting recognition and phoneme classi\ufb01cation [Graves and Schmidhuber,\n2005, Graves et al., 2009].\n4.3\nNeural Turing machines\nThe neural Turing machine (NTM) extends recurrent neural networks with an\naddressable external memory [Graves et al., 2014]. This work improves upon the\nability of RNNs to perform complex algorithmic tasks such as sorting. The au-\nthors take inspiration from theories in cognitive science, which suggest humans\npossess a \u201ccentral executive\u201d that interacts with a memory bu\ufb00er [Baddeley\net al., 1996].\nBy analogy to a Turing machine, in which a program directs\nread heads and write heads to interact with external memory in the form of a\ntape, the model is named a Neural Turing Machine. While technical details of\nthe read/write heads are beyond the scope of this review, we aim to convey a\nhigh-level sense of the model and its applications.\nThe two primary components of an NTM are a controller and memory ma-\ntrix. The controller, which may be a recurrent or feedforward neural network,\ntakes input and returns output to the outside world, as well as passing instruc-\ntions to and reading from the memory. The memory is represented by a large\nmatrix of N memory locations, each of which is a vector of dimension M. Ad-\nditionally, a number of read and write heads facilitate the interaction between\n22\nthe controller and the memory matrix. Despite these additional capabilities, the\nNTM is di\ufb00erentiable end-to-end and can be trained by variants of stochastic\ngradient descent using BPTT.\nGraves et al. [2014] select \ufb01ve algorithmic tasks to test the performance of the\nNTM model. By algorithmic we mean that for each task, the target output for a\ngiven input can be calculated by following a simple program, as might be easily\nimplemented in any universal programming language. One example is the copy\ntask, where the input is a sequence of \ufb01xed length binary vectors followed by a\ndelimiter symbol. The target output is a copy of the input sequence. In another\ntask, priority sort, an input consists of a sequence of binary vectors together\nwith a distinct scalar priority value for each vector. The target output is the\nsequence of vectors sorted by priority. The experiments test whether an NTM\ncan be trained via supervised learning to implement these common algorithms\ncorrectly and e\ufb03ciently. Interestingly, solutions found in this way generalize\nreasonably well to inputs longer than those presented in the training set. In\ncontrast, the LSTM without external memory does not generalize well to longer\ninputs. The authors compare three di\ufb00erent architectures, namely an LSTM\nRNN, an NTM with a feedforward controller, and an NTM with an LSTM\ncontroller. On each task, both NTM architectures signi\ufb01cantly outperform the\nLSTM RNN both in training set performance and in generalization to test data.\n5\nApplications of LSTMs and BRNNs\nThe previous sections introduced the building blocks from which nearly all state-\nof-the-art recurrent neural networks are composed. This section looks at several\napplication areas where recurrent networks have been employed successfully. Be-\nfore describing state of the art results in detail, it is appropriate to convey a\nconcrete sense of the precise architectures with which many important tasks\ncan be expressed clearly as sequence learning problems with recurrent neural\nnetworks. Figure 13 demonstrates several common RNN architectures and as-\nsociates each with corresponding well-documented tasks.\nIn the following subsections, we \ufb01rst introduce the representations of natural\nlanguage used for input and output to recurrent neural networks and the com-\nmonly used performance metrics for sequence prediction tasks. Then we survey\nstate-of-the-art results in machine translation, image captioning, video caption-\ning, and handwriting recognition. Many applications of RNNs involve processing\nwritten language. Some applications, such as image captioning, involve gener-\nating strings of text. Others, such as machine translation and dialogue systems,\nrequire both inputting and outputting text. Systems which output text are more\ndi\ufb03cult to evaluate empirically than those which produce binary predictions or\nnumerical output. As a result several methods have been developed to assess\nthe quality of translations and captions. In the next subsection, we provide the\nbackground necessary to understand how text is represented in most modern\nrecurrent net applications. We then explain the commonly reported evaluation\nmetrics.\n23\nFigure 13: Recurrent neural networks have been used successfully to model both\nsequential inputs and sequential outputs as well as mappings between single data\npoints and sequences (in both directions). This \ufb01gure, based on a similar \ufb01gure\nin Karpathy [2015] shows how numerous tasks can be modeled with RNNs with\nsequential inputs and/or sequential outputs. In each sub\ufb01gure, blue rectangles\ncorrespond to inputs, red rectangles to outputs and green rectangles to the en-\ntire hidden state of the neural network. (a) This is the conventional independent\ncase, as assumed by standard feedforward networks. (b) Text and video classi-\n\ufb01cation are tasks in which a sequence is mapped to one \ufb01xed length vector. (c)\nImage captioning presents the converse case, where the input image is a single\nnon-sequential data point. (d) This architecture has been used for natural lan-\nguage translation, a sequence-to-sequence task in which the two sequences may\nhave varying and di\ufb00erent lengths. (e) This architecture has been used to learn\na generative model for text, predicting at each step the following character.\n24\n5.1\nRepresentations of natural language inputs and out-\nputs\nWhen words are output at each time step, generally the output consists of a\nsoftmax vector y(t) \u2208RK where K is the size of the vocabulary. A softmax\nlayer is an element-wise logistic function that is normalized so that all of its com-\nponents sum to one. Intuitively, these outputs correspond to the probabilities\nthat each word is the correct output at that time step.\nFor application where an input consists of a sequence of words, typically the\nwords are fed to the network one at a time in consecutive time steps. In these\ncases, the simplest way to represent words is a one-hot encoding, using binary\nvectors with a length equal to the size of the vocabulary, so \u201c1000\u201d and \u201c0100\u201d\nwould represent the \ufb01rst and second words in the vocabulary respectively. Such\nan encoding is discussed by Elman [1990] among others. However, this encoding\nis ine\ufb03cient, requiring as many bits as the vocabulary is large. Further, it o\ufb00ers\nno direct way to capture di\ufb00erent aspects of similarity between words in the\nencoding itself.\nThus it is common now to model words with a distributed\nrepresentation using a meaning vector. In some cases, these meanings for words\nare learned given a large corpus of supervised data, but it is more usual to\ninitialize the meaning vectors using an embedding based on word co-occurrence\nstatistics. Freely available code to produce word vectors from these statistics\ninclude GloVe [Pennington et al., 2014], and word2vec [Goldberg and Levy,\n2014], which implements a word embedding algorithm from Mikolov et al. [2013].\nDistributed representations for textual data were described by Hinton [1986],\nused extensively for natural language by Bengio et al. [2003], and more recently\nbrought to wider attention in the deep learning community in a number of\npapers describing recursive auto-encoder (RAE) networks [Socher et al., 2010,\n2011a,b,c]. For clarity we point out that these recursive networks are not recur-\nrent neural networks, and in the present survey the abbreviation RNN always\nmeans recurrent neural network. While they are distinct approaches, recurrent\nand recursive neural networks have important features in common, namely that\nthey both involve extensive weight tying and are both trained end-to-end via\nbackpropagation.\nIn many experiments with recurrent neural networks [Elman, 1990, Sutskever\net al., 2011, Zaremba and Sutskever, 2014], input is fed in one character at a\ntime, and output generated one character at a time, as opposed to one word at\na time. While the output is nearly always a softmax layer, many papers omit\ndetails of how they represent single-character inputs. It seems reasonable to\ninfer that characters are encoded with a one-hot encoding. We know of no cases\nof paper using a distributed representation at the single-character level.\n5.2\nEvaluation methodology\nA serious obstacle to training systems well to output variable length sequences\nof words is the \ufb02aws of the available performance metrics. In the case of cap-\ntioning or translation, there maybe be multiple correct translations. Further,\n25\na labeled dataset may contain multiple reference translations for each example.\nComparing against such a gold standard is more fraught than applying standard\nperformance measure to binary classi\ufb01cation problems.\nOne commonly used metric for structured natural language output with\nmultiple references is BLEU score. Developed in 2002, BLEU score is related\nto modi\ufb01ed unigram precision [Papineni et al., 2002]. It is the geometric mean\nof the n-gram precisions for all values of n between 1 and some upper limit\nN. In practice, 4 is a typical value for N, shown to maximize agreement with\nhuman raters. Because precision can be made high by o\ufb00ering excessively short\ntranslations, the BLEU score includes a brevity penalty B. Where c is average\nthe length of the candidate translations and r the average length of the reference\ntranslations, the brevity penalty is\nB =\n(\n1\nif c > r\ne(1\u2212r/c)\nif c \u2264r .\nThen the BLEU score is\nBLEU = B \u00b7 exp\n \n1\nN\nN\nX\nn=1\nlog pn\n!\nwhere pn is the modi\ufb01ed n-gram precision, which is the number of n-grams\nin the candidate translation that occur in any of the reference translations,\ndivided by the total number of n-grams in the candidate translation. This is\ncalled modi\ufb01ed precision because it is an adaptation of precision to the case of\nmultiple references.\nBLEU scores are commonly used in recent papers to evaluate both transla-\ntion and captioning systems. While BLEU score does appear highly correlated\nwith human judgments, there is no guarantee that any given translation with a\nhigher BLEU score is superior to another which receives a lower BLEU score.\nIn fact, while BLEU scores tend to be correlated with human judgement across\nlarge sets of translations, they are not accurate predictors of human judgement\nat the single sentence level.\nMETEOR is an alternative metric intended to overcome the weaknesses of\nthe BLEU score [Banerjee and Lavie, 2005]. METEOR is based on explicit word\nto word matches between candidates and reference sentences. When multiple\nreferences exist, the best score is used. Unlike BLEU, METEOR exploits known\nsynonyms and stemming. The \ufb01rst step is to compute an F-score\nF\u03b1 =\nP \u00b7 R\n\u03b1 \u00b7 P + (1 \u2212\u03b1) \u00b7 R\nbased on single word matches where P is the precision and R is the recall.\nThe next step is to calculate a fragmentation penalty M \u221dc/m where c is\nthe smallest number of chunks of consecutive words such that the words are\nadjacent in both the candidate and the reference, and m is the total number of\nmatched unigrams yielding the score. Finally, the score is\nMETEOR = (1 \u2212M) \u00b7 F\u03b1.\n26\nEmpirically, this metric has been found to agree with human raters more than\nBLEU score.\nHowever, METEOR is less straightforward to calculate than\nBLEU. To replicate the METEOR score reported by another party, one must\nexactly replicate their stemming and synonym matching, as well as the calcula-\ntions. Both metrics rely upon having the exact same set of reference translations.\nEven in the straightforward case of binary classi\ufb01cation, without sequential\ndependencies, commonly used performance metrics like F1 give rise to optimal\nthresholding strategies which may not accord with intuition about what should\nconstitute good performance [Lipton et al., 2014]. Along the same lines, given\nthat performance metrics such as the ones above are weak proxies for true\nobjectives, it may be di\ufb03cult to distinguish between systems which are truly\nstronger and those which most over\ufb01t the performance metrics in use.\n5.3\nNatural language translation\nTranslation of text is a fundamental problem in machine learning that resists\nsolutions with shallow methods. Some tasks, like document classi\ufb01cation, can\nbe performed successfully with a bag-of-words representation that ignores word\norder. But word order is essential in translation. The sentences \u201cScientist killed\nby raging virus\u201d and \u201cVirus killed by raging scientist\u201d have identical bag-of-\nwords representations.\nSutskever et al. [2014] present a translation model using two multilayered\nLSTMs that demonstrates impressive performance translating from English to\nFrench. The \ufb01rst LSTM is used for encoding an input phrase from the source\nlanguage and the second LSTM for decoding the output phrase in the target\nlanguage. The model works according to the following procedure (Figure 14):\n\u2022 The source phrase is fed to the encoding LSTM one word at a time, which\ndoes not output anything. The authors found that signi\ufb01cantly better\nresults are achieved when the input sentence is fed into the network in\nreverse order.\n\u2022 When the end of the phrase is reached, a special symbol that indicates the\nbeginning of the output sentence is sent to the decoding LSTM. Addition-\nally, the decoding LSTM receives as input the \ufb01nal state of the \ufb01rst LSTM.\nThe second LSTM outputs softmax probabilities over the vocabulary at\neach time step.\n\u2022 At inference time, beam search is used to choose the most likely words\nfrom the distribution at each time step, running the second LSTM until\nthe end-of-sentence (EOS) token is reached.\nFor training, the true inputs are fed to the encoder, the true translation\nis fed to the decoder, and loss is propagated back from the outputs of the\ndecoder across the entire sequence to sequence model. The network is trained\nto maximize the likelihood of the correct translation of each sentence in the\ntraining set. At inference time, a left to right beam search is used to determine\n27\nFigure 14: Sequence to sequence LSTM model of Sutskever et al. [2014]. The\nnetwork consists of an encoding model (\ufb01rst LSTM) and a decoding model\n(second LSTM). The input blocks (blue and purple) correspond to word vectors,\nwhich are fully connected to the corresponding hidden state. Red nodes are\nsoftmax outputs.\nWeights are tied among all encoding steps and among all\ndecoding time steps.\n28\nwhich words to output. A few among the most likely next words are chosen\nfor expansion after each time step. The beam search ends when the network\noutputs an end-of-sentence (EOS) token. Sutskever et al. [2014] train the model\nusing stochastic gradient descent without momentum, halving the learning rate\ntwice per epoch, after the \ufb01rst \ufb01ve epochs. The approach achieves a BLEU\nscore of 34.81, outperforming the best previous neural network NLP systems,\nand matching the best published results for non-neural network approaches,\nincluding systems that have explicitly programmed domain expertise. When\ntheir system is used to rerank candidate translations from another system, it\nachieves a BLEU score of 36.5.\nThe implementation which achieved these results involved eight GPUS. Nev-\nertheless, training took 10 days to complete. One GPU was assigned to each\nlayer of the LSTM, and an additional four GPUs were used simply to calculate\nsoftmax. The implementation was coded in C++, and each hidden layer of the\nLSTM contained 1000 nodes. The input vocabulary contained 160,000 words\nand the output vocabulary contained 80,000 words. Weights were initialized\nuniformly randomly in the range between \u22120.08 and 0.08.\nAnother RNN approach to language translation is presented by Auli et al.\n[2013]. Their RNN model uses the word embeddings of Mikolov and a lattice\nrepresentation of the decoder output to facilitate search over the space of pos-\nsible translations. In the lattice, each node corresponds to a sequence of words.\nThey report a BLEU score of 28.5 on French-English translation tasks. Both\npapers provide results on similar datasets but Sutskever et al. [2014] only report\non English to French translation while Auli et al. [2013] only report on French\nto English translation, so it is not possible to compare the performance of the\ntwo models.\n5.4\nImage captioning\nRecently, recurrent neural networks have been used successfully for generating\nsentences that describe photographs [Vinyals et al., 2015, Karpathy and Fei-Fei,\n2014, Mao et al., 2014]. In this task, a training set consists of input images x\nand target captions y. Given a large set of image-caption pairs, a model is\ntrained to predict the appropriate caption for an image.\nVinyals et al. [2015] follow up on the success in language to language trans-\nlation by considering captioning as a case of image to language translation.\nInstead of both encoding and decoding with LSTMs, they introduce the idea of\nencoding an image with a convolutional neural network, and then decoding it\nwith an LSTM. Mao et al. [2014] independently developed a similar RNN image\ncaptioning network, and achieved then state-of-the-art results on the Pascal,\nFlickr30K, and COCO datasets.\nKarpathy and Fei-Fei [2014] follows on this work, using a convolutional net-\nwork to encode images together with a bidirectional network attention mech-\nanism and standard RNN to decode captions, using word2vec embeddings as\nword representations. They consider both full-image captioning and a model\nthat captures correspondences between image regions and text snippets.\nAt\n29\ninference time, their procedure resembles the one described by Sutskever et al.\n[2014], where sentences are decoded one word at a time. The most probable\nword is chosen and fed to the network at the next time step. This process is\nrepeated until an EOS token is produced.\nTo convey a sense of the scale of these problems, Karpathy and Fei-Fei [2014]\nfocus on three datasets of captioned images: Flickr8K, Flickr30K, and COCO, of\nsize 50MB (8000 images), 200MB (30,000 images), and 750MB (328,000 images)\nrespectively. The implementation uses the Ca\ufb00e library [Jia et al., 2014], and\nthe convolutional network is pretrained on ImageNet data. In a revised version,\nthe authors report that LSTMs outperform simpler RNNs and that learning\nword representations from random initializations is often preferable to word2vec\nembeddings. As an explanation, they say that word2vec embeddings may cluster\nwords like colors together in the embedding space, which can be not suitable\nfor visual descriptions of images.\n5.5\nFurther applications\nHandwriting recognition is an application area where bidirectional LSTMs have\nbeen used to achieve state of the art results. In work by Liwicki et al. [2007] and\nGraves et al. [2009], data is collected from an interactive whiteboard. Sensors\nrecord the (x, y) coordinates of the pen at regularly sampled time steps. In\nthe more recent paper, they use a bidirectional LSTM model, outperforming an\nHMM model by achieving 81.5% word-level accuracy, compared to 70.1% for\nthe HMM.\nIn the last year, a number of papers have emerged that extend the success\nof recurrent networks for translation and image captioning to new domains.\nAmong the most interesting of these applications are unsupervised video en-\ncoding [Srivastava et al., 2015], video captioning [Venugopalan et al., 2015],\nand program execution [Zaremba and Sutskever, 2014].\nVenugopalan et al.\n[2015] demonstrate a sequence to sequence architecture that encodes frames\nfrom a video and decode words. At each time step the input to the encoding\nLSTM is the topmost hidden layer of a convolutional neural network. At de-\ncoding time, the network outputs probabilities over the vocabulary at each time\nstep.\nZaremba and Sutskever [2014] experiment with networks which read com-\nputer programs one character at a time and predict their output. They focus\non programs which output integers and \ufb01nd that for simple programs, including\nadding two nine-digit numbers, their network, which uses LSTM cells in several\nstacked hidden layers and makes a single left to right pass through the program,\ncan predict the output with 99% accuracy.\n6\nDiscussion\nOver the past thirty years, recurrent neural networks have gone from models\nprimarily of interest for cognitive modeling and computational neuroscience, to\n30\npowerful and practical tools for large-scale supervised learning from sequences.\nThis progress owes to advances in model architectures, training algorithms, and\nparallel computing. Recurrent networks are especially interesting because they\novercome many of the restrictions placed on input and output data by tradi-\ntional machine learning approaches. With recurrent networks, the assumption\nof independence between consecutive examples is broken, and hence also the\nassumption of \ufb01xed-dimension inputs and outputs.\nWhile LSTMs and BRNNs have set records in accuracy on many tasks in\nrecent years, it is noteworthy that advances come from novel architectures rather\nthan from fundamentally novel algorithms. Therefore, automating exploration\nof the space of possible models, for example via genetic algorithms or a Markov\nchain Monte Carlo approach, could be promising. Neural networks o\ufb00er a wide\nrange of transferable and combinable techniques.\nNew activation functions,\ntraining procedures, initializations procedures, etc. are generally transferable\nacross networks and tasks, often conferring similar bene\ufb01ts. As the number of\nsuch techniques grows, the practicality of testing all combinations diminishes.\nIt seems reasonable to infer that as a community, neural network researchers are\nexploring the space of model architectures and con\ufb01gurations much as a genetic\nalgorithm might, mixing and matching techniques, with a \ufb01tness function in the\nform of evaluation metrics on major datasets of interest.\nThis inference suggests two corollaries. First, as just stated, this body of\nresearch could bene\ufb01t from automated procedures to explore the space of mod-\nels. Second, as we build systems designed to perform more complex tasks, we\nwould bene\ufb01t from improved \ufb01tness functions. A BLEU score inspires less con-\n\ufb01dence than the accuracy reported on a binary classi\ufb01cation task. To this end,\nwhen possible, it seems prudent to individually test techniques \ufb01rst with classic\nfeedforward networks on datasets with established benchmarks before applying\nthem to recurrent networks in settings with less reliable evaluation criteria.\nLastly, the rapid success of recurrent neural networks on natural language\ntasks leads us to believe that extensions of this work to longer texts would be\nfruitful. Additionally, we imagine that dialogue systems could be built along\nsimilar principles to the architectures used for translation, encoding prompts\nand generating responses, while retaining the entirety of conversation history as\ncontextual information.\n7\nAcknowledgements\nThe \ufb01rst author\u2019s research is funded by generous support from the Division\nof Biomedical Informatics at UCSD, via a training grant from the National\nLibrary of Medicine. This review has bene\ufb01ted from insightful comments from\nVineet Bafna, Julian McAuley, Balakrishnan Narayanaswamy, Stefanos Poulis,\nLawrence Saul, Zhuowen Tu, and Sharad Vikram.\n31\nReferences\nMichael Auli, Michel Galley, Chris Quirk, and Geo\ufb00rey Zweig. Joint language\nand translation modeling with recurrent neural networks. In EMNLP, pages\n1044\u20131054, 2013.\nAlan Baddeley, Sergio Della Sala, and T.W. Robbins. Working memory and\nexecutive control [and discussion]. Philosophical Transactions of the Royal\nSociety B: Biological Sciences, 351(1346):1397\u20131404, 1996.\nPierre Baldi and Gianluca Pollastri. The principled design of large-scale re-\ncursive neural network architectures\u2013DAG-RNNs and the protein structure\nprediction problem. The Journal of Machine Learning Research, 4:575\u2013602,\n2003.\nSatanjeev Banerjee and Alon Lavie. METEOR: An automatic metric for MT\nevaluation with improved correlation with human judgments. In Proceedings\nof the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for\nMachine Translation and/or Summarization, pages 65\u201372, 2005.\nJustin Bayer, Daan Wierstra, Julian Togelius, and J\u00a8urgen Schmidhuber. Evolv-\ning memory cell structures for sequence learning.\nIn Arti\ufb01cial Neural\nNetworks\u2013ICANN 2009, pages 755\u2013764. Springer, 2009.\nRichard K. Belew, John McInerney, and Nicol N. Schraudolph. Evolving net-\nworks: Using the genetic algorithm with connectionist learning. In In. Cite-\nseer, 1990.\nYoshua Bengio, Patrice Simard, and Paolo Frasconi. Learning long-term depen-\ndencies with gradient descent is di\ufb03cult. Neural Networks, IEEE Transactions\non, 5(2):157\u2013166, 1994.\nYoshua Bengio, R\u00b4ejean Ducharme, Pascal Vincent, and Christian Janvin. A\nneural probabilistic language model. The Journal of Machine Learning Re-\nsearch, 3:1137\u20131155, 2003.\nYoshua Bengio, Nicolas Boulanger-Lewandowski, and Razvan Pascanu.\nAd-\nvances in optimizing recurrent networks.\nIn Acoustics, Speech and Signal\nProcessing (ICASSP), 2013 IEEE International Conference on, pages 8624\u2013\n8628. IEEE, 2013.\nJames Bergstra, Olivier Breuleux, Fr\u00b4ed\u00b4eric Bastien, Pascal Lamblin, Razvan\nPascanu, Guillaume Desjardins, Joseph Turian, David Warde-Farley, and\nYoshua Bengio. Theano: a CPU and GPU math expression compiler. In\nProceedings of the Python for Scienti\ufb01c Computing Conference (SciPy), vol-\nume 4, page 3. Austin, TX, 2010.\nAvrim L. Blum and Ronald L. Rivest. Training a 3-node neural network is NP-\ncomplete. In Machine Learning: From Theory to Applications, pages 9\u201328.\nSpringer, 1993.\n32\nBob Carpenter. Lazy sparse stochastic gradient descent for regularized multi-\nnomial logistic regression. Alias-i, Inc., Tech. Rep, pages 1\u201320, 2008.\nRonan Collobert, Koray Kavukcuoglu, and Cl\u00b4ement Farabet. Torch7: A matlab-\nlike environment for machine learning. In BigLearn, NIPS Workshop, 2011.\nYann N Dauphin, Razvan Pascanu, Caglar Gulcehre, Kyunghyun Cho, Surya\nGanguli, and Yoshua Bengio.\nIdentifying and attacking the saddle point\nproblem in high-dimensional non-convex optimization. In Advances in Neural\nInformation Processing Systems, pages 2933\u20132941, 2014.\nWim De Mulder, Steven Bethard, and Marie-Francine Moens.\nA survey on\nthe application of recurrent neural networks to statistical language modeling.\nComputer Speech & Language, 30(1):61\u201398, 2015.\nJohn Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for\nonline learning and stochastic optimization. The Journal of Machine Learning\nResearch, 12:2121\u20132159, 2011.\nCharles Elkan. Learning meanings for sentences. http://cseweb.ucsd.edu/\n~elkan/250B/learningmeaning.pdf, 2015. Accessed: 2015-05-18.\nJe\ufb00rey L. Elman. Finding structure in time. Cognitive science, 14(2):179\u2013211,\n1990.\nClement Farabet, Camille Couprie, Laurent Najman, and Yann LeCun. Learn-\ning hierarchical features for scene labeling. Pattern Analysis and Machine\nIntelligence, IEEE Transactions on, 35(8):1915\u20131929, 2013.\nFelix A. Gers.\nLong short-term memory in recurrent neural networks.\nUn-\npublished PhD dissertation, \u00b4Ecole Polytechnique F\u00b4ed\u00b4erale de Lausanne, Lau-\nsanne, Switzerland, 2001.\nFelix A. Gers and J\u00a8urgen Schmidhuber. Recurrent nets that time and count. In\nNeural Networks, 2000. IJCNN 2000, Proceedings of the IEEE-INNS-ENNS\nInternational Joint Conference on, volume 3, pages 189\u2013194. IEEE, 2000.\nFelix A. Gers, J\u00a8urgen Schmidhuber, and Fred Cummins. Learning to forget:\nContinual prediction with LSTM.\nNeural computation, 12(10):2451\u20132471,\n2000.\nXavier Glorot, Antoine Bordes, and Yoshua Bengio. Deep sparse recti\ufb01er net-\nworks. In Proceedings of the 14th International Conference on Arti\ufb01cial In-\ntelligence and Statistics. JMLR W&CP Volume, volume 15, pages 315\u2013323,\n2011.\nYoav Goldberg and Omer Levy. word2vec explained: deriving Mikolov et al.\u2019s\nnegative-sampling word-embedding method. arXiv preprint arXiv:1402.3722,\n2014.\n33\nAlex Graves. Supervised sequence labelling with recurrent neural networks, vol-\nume 385. Springer, 2012.\nAlex Graves and J\u00a8urgen Schmidhuber. Framewise phoneme classi\ufb01cation with\nbidirectional LSTM and other neural network architectures. Neural Networks,\n18(5):602\u2013610, 2005.\nAlex Graves, Marcus Liwicki, Santiago Fern\u00b4andez, Roman Bertolami, Horst\nBunke, and J\u00a8urgen Schmidhuber. A novel connectionist system for uncon-\nstrained handwriting recognition. Pattern Analysis and Machine Intelligence,\nIEEE Transactions on, 31(5):855\u2013868, 2009.\nAlex Graves, Greg Wayne, and Ivo Danihelka. Neural Turing machines. arXiv\npreprint arXiv:1410.5401, 2014.\nFrdric Gruau, L\u2019universite Claude Bernard lyon I, Of A Diplome De Doctorat,\nM. Jacques Demongeot, Examinators M. Michel Cosnard, M. Jacques Ma-\nzoyer, M. Pierre Peretto, and M. Darell Whitley. Neural network synthesis\nusing cellular encoding and the genetic algorithm., 1994.\nSteven A. Harp and Tariq Samad. Optimizing neural networks with genetic\nalgorithms. In Proceedings of the 54th American Power Conference, Chicago,\nvolume 2, 2013.\nGeo\ufb00rey E. Hinton. Learning distributed representations of concepts, 1986.\nSepp Hochreiter and Jurgen Schmidhuber. Bridging long time lags by weight\nguessing and \u201clong short-term memory\u201d. Spatiotemporal Models in Biological\nand Arti\ufb01cial Systems, 37:65\u201372, 1996.\nSepp Hochreiter and J\u00a8urgen Schmidhuber. Long short-term memory. Neural\nComputation, 9(8):1735\u20131780, 1997.\nSepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and J\u00a8urgen Schmidhuber. Gra-\ndient \ufb02ow in recurrent nets: the di\ufb03culty of learning long-term dependencies.\nA \ufb01eld guide to dynamical recurrent neural networks, 2001.\nJohn J. Hop\ufb01eld. Neural networks and physical systems with emergent collective\ncomputational abilities. Proceedings of the National Academy of Sciences, 79\n(8):2554\u20132558, 1982.\nYangqing Jia, Evan Shelhamer, Je\ufb00Donahue, Sergey Karayev, Jonathan Long,\nRoss Girshick, Sergio Guadarrama, and Trevor Darrell. Ca\ufb00e: Convolutional\narchitecture for fast feature embedding.\narXiv preprint arXiv:1408.5093,\n2014.\nMichael I. Jordan.\nSerial order: A parallel distributed processing approach.\nTechnical Report 8604, Institute for Cognitive Science, University of Califor-\nnia, San Diego, 1986.\n34\nAndrej Karpathy.\nThe unreasonable e\ufb00ectiveness of recurrent neural net-\nworks.\nhttp://karpathy.github.io/2015/05/21/rnn-effectiveness/,\n2015. Accessed: 2015-08-13.\nAndrej Karpathy and Li Fei-Fei. Deep visual-semantic alignments for generating\nimage descriptions. arXiv preprint arXiv:1412.2306, 2014.\nAlex Krizhevsky, Ilya Sutskever, and Geo\ufb00rey E. Hinton. ImageNet classi\ufb01cation\nwith deep convolutional neural networks. In Advances in Neural Information\nProcessing Systems, pages 1097\u20131105, 2012.\nJohn Langford, Lihong Li, and Tong Zhang. Sparse online learning via truncated\ngradient. In Advances in Neural Information Processing Systems, pages 905\u2013\n912, 2009.\nYann Le Cun, B. Boser, John S. Denker, D. Henderson, Richard E. Howard,\nW. Hubbard, and Lawrence D. Jackel. Handwritten digit recognition with\na back-propagation network. In Advances in Neural Information Processing\nSystems. Citeseer, 1990.\nZachary C. Lipton and Charles Elkan. E\ufb03cient elastic net regularization for\nsparse linear models. CoRR, abs/1505.06449, 2015. URL http://arxiv.\norg/abs/1505.06449.\nZachary C. Lipton, Charles Elkan, and Balakrishnan Naryanaswamy. Optimal\nthresholding of classi\ufb01ers to maximize F1 measure. In Machine Learning and\nKnowledge Discovery in Databases, pages 225\u2013239. Springer, 2014.\nMarcus Liwicki, Alex Graves, Horst Bunke, and J\u00a8urgen Schmidhuber. A novel\napproach to on-line handwriting recognition based on bidirectional long short-\nterm memory networks. In Proc. 9th Int. Conf. on Document Analysis and\nRecognition, volume 1, pages 367\u2013371, 2007.\nAndrew L. Maas, Quoc V. Le, Tyler M. O\u2019Neil, Oriol Vinyals, Patrick Nguyen,\nand Andrew Y. Ng. Recurrent neural networks for noise reduction in robust\nASR. In INTERSPEECH. Citeseer, 2012.\nJunhua Mao, Wei Xu, Yi Yang, Jiang Wang, and Alan Yuille. Deep caption-\ning with multimodal recurrent neural networks (m-RNN).\narXiv preprint\narXiv:1412.6632, 2014.\nJames Martens and Ilya Sutskever. Learning recurrent neural networks with\nHessian-free optimization. In Proceedings of the 28th International Conference\non Machine Learning (ICML-11), pages 1033\u20131040, 2011.\nTomas Mikolov, Kai Chen, Greg Corrado, and Je\ufb00rey Dean. E\ufb03cient estimation\nof word representations in vector space. arXiv preprint arXiv:1301.3781, 2013.\nVinod Nair and Geo\ufb00rey E. Hinton. Recti\ufb01ed linear units improve restricted\nBoltzmann machines. In Proceedings of the 27th International Conference on\nMachine Learning (ICML-10), pages 807\u2013814, 2010.\n35\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.\nBLEU: a\nmethod for automatic evaluation of machine translation. In Proceedings of\nthe 40th annual meeting on association for computational linguistics, pages\n311\u2013318. Association for Computational Linguistics, 2002.\nRazvan Pascanu, Tomas Mikolov, and Yoshua Bengio.\nOn the di\ufb03culty of\ntraining recurrent neural networks. arXiv preprint arXiv:1211.5063, 2012.\nBarak A. Pearlmutter. Gradient calculations for dynamic recurrent neural net-\nworks: A survey. Neural Networks, IEEE Transactions on, 6(5):1212\u20131228,\n1995.\nJe\ufb00rey Pennington, Richard Socher, and Christopher D. Manning.\nGlove:\nGlobal vectors for word representation. Proceedings of the Empirical Methods\nin Natural Language Processing (EMNLP 2014), 12, 2014.\nDavid E. Rumelhart, Geo\ufb00rey E. Hinton, and Ronald J. Williams. Learning\ninternal representations by error propagation. Technical report, DTIC Docu-\nment, 1985.\nMike Schuster and Kuldip K. Paliwal. Bidirectional recurrent neural networks.\nSignal Processing, IEEE Transactions on, 45(11):2673\u20132681, 1997.\nHava T. Siegelmann and Eduardo D. Sontag. Turing computability with neural\nnets. Applied Mathematics Letters, 4(6):77\u201380, 1991.\nYoram Singer and John C. Duchi. E\ufb03cient learning using forward-backward\nsplitting. In Advances in Neural Information Processing Systems, pages 495\u2013\n503, 2009.\nRichard Socher, Christopher D. Manning, and Andrew Y. Ng. Learning con-\ntinuous phrase representations and syntactic parsing with recursive neural\nnetworks. In Proceedings of the NIPS-2010 Deep Learning and Unsupervised\nFeature Learning Workshop, pages 1\u20139, 2010.\nRichard Socher, Eric H. Huang, Je\ufb00rey Pennin, Christopher D. Manning, and\nAndrew Y. Ng. Dynamic pooling and unfolding recursive autoencoders for\nparaphrase detection. In Advances in Neural Information Processing Systems,\npages 801\u2013809, 2011a.\nRichard Socher, Cli\ufb00C. Lin, Chris Manning, and Andrew Y. Ng. Parsing natural\nscenes and natural language with recursive neural networks. In Proceedings\nof the 28th international conference on machine learning (ICML-11), pages\n129\u2013136, 2011b.\nRichard Socher, Je\ufb00rey Pennington, Eric H. Huang, Andrew Y. Ng, and Christo-\npher D. Manning. Semi-supervised recursive autoencoders for predicting sen-\ntiment distributions. In Proceedings of the Conference on Empirical Methods\nin Natural Language Processing, pages 151\u2013161. Association for Computa-\ntional Linguistics, 2011c.\n36\nRichard Socher, Andrej Karpathy, Quoc V. Le, Christopher D. Manning, and\nAndrew Y. Ng. Grounded compositional semantics for \ufb01nding and describing\nimages with sentences. Transactions of the Association for Computational\nLinguistics, 2:207\u2013218, 2014.\nNitish Srivastava, Elman Mansimov, and Ruslan Salakhutdinov.\nUnsu-\npervised learning of video representations using LSTMs.\narXiv preprint\narXiv:1502.04681, 2015.\nRuslan L. Stratonovich. Conditional markov processes. Theory of Probability &\nIts Applications, 5(2):156\u2013178, 1960.\nIlya Sutskever, James Martens, and Geo\ufb00rey E. Hinton. Generating text with\nrecurrent neural networks. In Proceedings of the 28th International Conference\non Machine Learning (ICML-11), pages 1017\u20131024, 2011.\nIlya Sutskever, James Martens, George Dahl, and Geo\ufb00rey E. Hinton. On the\nimportance of initialization and momentum in deep learning. In Proceedings\nof the 30th International Conference on Machine Learning (ICML-13), pages\n1139\u20131147, 2013.\nIlya Sutskever, Oriol Vinyals, and Quoc V. Le. Sequence to sequence learning\nwith neural networks. In Advances in Neural Information Processing Systems,\npages 3104\u20133112, 2014.\nTijmen Tieleman and Geo\ufb00rey E. Hinton.\nLecture 6.5- RMSprop:\nDivide\nthe gradient by a running average of its recent magnitude. https://www.\nyoutube.com/watch?v=LGA-gRkLEsI, 2012.\nAlan M. Turing. Computing machinery and intelligence. Mind, pages 433\u2013460,\n1950.\nSubhashini Venugopalan, Marcus Rohrbach, Je\ufb00Donahue, Raymond Mooney,\nTrevor Darrell, and Kate Saenko. Sequence to sequence\u2013video to text. arXiv\npreprint arXiv:1505.00487, 2015.\nOriol Vinyals, Alexander Toshev, Samy Bengio, and Dumitru Erhan. Show and\ntell: A neural image caption generator. In Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition, pages 3156\u20133164, 2015.\nAndrew J. Viterbi. Error bounds for convolutional codes and an asymptotically\noptimum decoding algorithm. Information Theory, IEEE Transactions on,\n13(2):260\u2013269, 1967.\nPaul J. Werbos. Backpropagation through time: what it does and how to do it.\nProceedings of the IEEE, 78(10):1550\u20131560, 1990.\nWikipedia. Backpropagation \u2014 Wikipedia, the free encyclopedia, 2015. URL\nhttp://en.wikipedia.org/wiki/Backpropagation. [Online; accessed 18-\nMay-2015].\n37\nRonald J. Williams and David Zipser.\nA learning algorithm for continually\nrunning fully recurrent neural networks. Neural Computation, 1(2):270\u2013280,\n1989.\nWojciech Zaremba and Ilya Sutskever.\nLearning to execute.\narXiv preprint\narXiv:1410.4615, 2014.\nMatthew D. Zeiler. Adadelta: an adaptive learning rate method. arXiv preprint\narXiv:1212.5701, 2012.\nMatthew D. Zeiler, M. Ranzato, Rajat Monga, M. Mao, K. Yang, Quoc V.\nLe, Patrick Nguyen, A. Senior, Vincent Vanhoucke, Je\ufb00rey Dean, et al. On\nrecti\ufb01ed linear units for speech processing. In Acoustics, Speech and Signal\nProcessing (ICASSP), 2013 IEEE International Conference on, pages 3517\u2013\n3521. IEEE, 2013.\n38\n",
        "sentence": " RNNs are the deep models of choice when dealing with sequential data [12].",
        "context": "Along with the LSTM, one of the most used RNN architectures is the bidirec-\ntional recurrent neural network (BRNN) (Figure 12) \ufb01rst described by Schuster\nand Paliwal [1997]. In this architecture, there are two layers of hidden nodes.\ndemand both capabilities. Recurrent neural networks (RNNs) are connec-\ntionist models that capture the dynamics of sequences via cycles in the\nnetwork of nodes. Unlike standard feedforward neural networks, recurrent\n30\npowerful and practical tools for large-scale supervised learning from sequences.\nThis progress owes to advances in model architectures, training algorithms, and\nparallel computing. Recurrent networks are especially interesting because they"
    },
    {
        "title": "Improving pairwise learning for item recommendation from implicit feedback",
        "author": [
            "Steffen Rendle",
            "Christoph Freudenthaler"
        ],
        "venue": "In Proceedings of the 7th ACM International Conference on Web Search and Data Mining,",
        "citeRegEx": "13",
        "shortCiteRegEx": "13",
        "year": 2014,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " Both properties are known to be beneficial for pairwise learning with implicit user feedback [13].",
        "context": null
    },
    {
        "title": "Bpr: Bayesian personalized ranking from implicit feedback",
        "author": [
            "Steffen Rendle",
            "Christoph Freudenthaler",
            "Zeno Gantner",
            "Lars Schmidt-Thieme"
        ],
        "venue": "In Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence,",
        "citeRegEx": "14",
        "shortCiteRegEx": "14",
        "year": 2009,
        "abstract": "Item recommendation is the task of predicting a personalized ranking on a set\nof items (e.g. websites, movies, products). In this paper, we investigate the\nmost common scenario with implicit feedback (e.g. clicks, purchases). There are\nmany methods for item recommendation from implicit feedback like matrix\nfactorization (MF) or adaptive knearest-neighbor (kNN). Even though these\nmethods are designed for the item prediction task of personalized ranking, none\nof them is directly optimized for ranking. In this paper we present a generic\noptimization criterion BPR-Opt for personalized ranking that is the maximum\nposterior estimator derived from a Bayesian analysis of the problem. We also\nprovide a generic learning algorithm for optimizing models with respect to\nBPR-Opt. The learning method is based on stochastic gradient descent with\nbootstrap sampling. We show how to apply our method to two state-of-the-art\nrecommender models: matrix factorization and adaptive kNN. Our experiments\nindicate that for the task of personalized ranking our optimization method\noutperforms the standard learning techniques for MF and kNN. The results show\nthe importance of optimizing models for the right criterion.",
        "full_text": "BPR: Bayesian Personalized Ranking from Implicit Feedback\nSte\ufb00en Rendle, Christoph Freudenthaler, Zeno Gantner and Lars Schmidt-Thieme\n{srendle, freudenthaler, gantner, schmidt-thieme}@ismll.de\nMachine Learning Lab, University of Hildesheim\nMarienburger Platz 22, 31141 Hildesheim, Germany\nAbstract\nItem recommendation is the task of predict-\ning a personalized ranking on a set of items\n(e.g.\nwebsites, movies, products).\nIn this\npaper, we investigate the most common sce-\nnario with implicit feedback (e.g.\nclicks,\npurchases).\nThere are many methods for\nitem recommendation from implicit feedback\nlike matrix factorization (MF) or adaptive k-\nnearest-neighbor (kNN). Even though these\nmethods are designed for the item predic-\ntion task of personalized ranking, none of\nthem is directly optimized for ranking.\nIn\nthis paper we present a generic optimization\ncriterion BPR-Opt for personalized ranking\nthat is the maximum posterior estimator de-\nrived from a Bayesian analysis of the prob-\nlem. We also provide a generic learning al-\ngorithm for optimizing models with respect\nto BPR-Opt. The learning method is based\non stochastic gradient descent with bootstrap\nsampling. We show how to apply our method\nto two state-of-the-art recommender models:\nmatrix factorization and adaptive kNN. Our\nexperiments indicate that for the task of per-\nsonalized ranking our optimization method\noutperforms the standard learning techniques\nfor MF and kNN. The results show the im-\nportance of optimizing models for the right\ncriterion.\n1\nIntroduction\nRecommending content is an important task in many\ninformation systems.\nFor example online shopping\nwebsites like Amazon give each customer personalized\nrecommendations of products that the user might be\ninterested in. Other examples are video portals like\nYouTube that recommend movies to customers. Per-\nsonalization is attractive both for content providers,\nwho can increase sales or views, and for customers,\nwho can \ufb01nd interesting content more easily. In this\npaper, we focus on item recommendation. The task of\nitem recommendation is to create a user-speci\ufb01c rank-\ning for a set of items. Preferences of users about items\nare learned from the user\u2019s past interaction with the\nsystem \u2013 e.g. his buying history, viewing history, etc.\nRecommender systems are an active topic of research.\nMost recent work is on scenarios where users provide\nexplicit feedback, e.g.\nin terms of ratings.\nNever-\ntheless, in real-world scenarios most feedback is not\nexplicit but implicit. Implicit feedback is tracked au-\ntomatically, like monitoring clicks, view times, pur-\nchases, etc. Thus it is much easier to collect, because\nthe user has not to express his taste explicitly. In fact\nimplicit feedback is already available in almost any in-\nformation system \u2013 e.g. web servers record any page\naccess in log \ufb01les.\nIn this paper we present a generic method for learning\nmodels for personalized ranking. The contributions of\nthis work are:\n1. We present the generic optimization criterion\nBPR-Opt derived from the maximum posterior\nestimator for optimal personalized ranking. We\nshow the analogies of BPR-Opt to maximization\nof the area under ROC curve.\n2. For\nmaximizing\nBPR-Opt,\nwe\npropose\nthe\ngeneric learning algorithm LearnBPR that is\nbased on stochastic gradient descent with boot-\nstrap sampling of training triples. We show that\nour algorithm is superior to standard gradient de-\nscent techniques for optimizing w.r.t. BPR-Opt.\n3. We show how to apply LearnBPR to two state-\nof-the-art recommender model classes.\n4. Our experiments empirically show that for the\ntask of of personalized ranking, learning a model\nwith BPR outperforms other learning methods.\nRENDLE ET AL.\n452\nUAI 2009\n2\nRelated Work\nThe most popular model for recommender systems is\nk-nearest neighbor (kNN) collaborative \ufb01ltering [2].\nTraditionally the similarity matrix of kNN is com-\nputed by heuristics \u2013 e.g. the Pearson correlation \u2013\nbut in recent work [8] the similarity matrix is treated\nas model parameters and is learned speci\ufb01cally for the\ntask. Recently, matrix factorization (MF) has become\nvery popular in recommender systems both for im-\nplicit and explicit feedback.\nIn early work [13] sin-\ngular value decomposition (SVD) has been proposed\nto learn the feature matrices. MF models learned by\nSVD have shown to be very prone to over\ufb01tting. Thus\nregularized learning methods have been proposed. For\nitem prediction Hu et al. [5] and Pan et al. [10] pro-\npose a regularized least-square optimization with case\nweights (WR-MF). The case weights can be used to\nreduce the impact of negative examples. Hofmann [4]\nproposes a probabilistic latent semantic model for item\nrecommendation.\nSchmidt-Thieme [14] converts the\nproblem into a multi-class problem and solves it with\na set of binary classi\ufb01ers. Even though all the work on\nitem prediction discussed above is evaluated on per-\nsonalized ranking datasets, none of these methods di-\nrectly optimizes its model parameters for ranking. In-\nstead they optimize to predict if an item is selected\nby a user or not. In our work we derive an optimiza-\ntion criterion for personalized ranking that is based\non pairs of items (i.e. the user-speci\ufb01c order of two\nitems). We will show how state-of-the-art models like\nMF or adaptive kNN can be optimized with respect to\nthis criterion to provide better ranking quality than\nwith usual learning methods. A detailed discussion of\nthe relationship between our approach and the WR-\nMF approach of Hu et al. [5] and Pan et al. [10] as\nwell as maximum margin matrix factorization [15] can\nbe found in Section 5. In Section 4.1.1, we will also\ndiscuss the relations of our optimization criterion to\nAUC optimization like in [3].\nIn this paper, we focus on o\ufb04ine learning of the model\nparameters. Extending the learning method to online\nlearning scenarios \u2013 e.g. a new user is added and his\nhistory increases from 0 to 1, 2, . . . feedback events \u2013\nhas already been studied for MF for the related task of\nrating prediction [11]. The same fold-in strategy can\nbe used for BPR.\nThere is also related work on learning to rank with\nnon-collaborative models. One direction is to model\ndistributions on permutations [7, 6]. Burges et al. [1]\noptimize a neural network model for ranking using gra-\ndient descent.\nAll these approaches learn only one\nranking \u2013 i.e. they are non-personalized. In contrast\nto this, our models are collaborative models that learn\npersonalized rankings, i.e. one individual ranking per\nuser. In our evaluation, we show empirically that in\ntypical recommender settings our personalized BPR\nmodel outperforms even the theoretical upper bound\nfor non-personalized ranking.\n3\nPersonalized Ranking\nThe task of personalized ranking is to provide a user\nwith a ranked list of items. This is also called item\nrecommendation. An example is an online shop that\nwants to recommend a personalized ranked list of items\nthat the user might want to buy.\nIn this paper we\ninvestigate scenarios where the ranking has to be in-\nferred from the implicit behavior (e.g. purchases in the\npast) of the user. Interesting about implicit feedback\nsystems is that only positive observations are avail-\nable. The non-observed user-item pairs \u2013 e.g. a user\nhas not bought an item yet \u2013 are a mixture of real\nnegative feedback (the user is not interested in buying\nthe item) and missing values (the user might want to\nbuy the item in the future).\n3.1\nFormalization\nLet U be the set of all users and I the set of all items.\nIn our scenario implicit feedback S \u2286U \u00d7I is available\n(see left side of Figure 1). Examples for such feedback\nare purchases in an online shop, views in a video portal\nor clicks on a website. The task of the recommender\nsystem is now to provide the user with a personalized\ntotal ranking >u\u2282I2 of all items, where >u has to\nmeet the properties of a total order:\n\u2200i, j \u2208I : i \u0338= j \u21d2i >u j \u2228j >u i\n(totality)\n\u2200i, j \u2208I : i >u j \u2227j >u i \u21d2i = j\n(antisymmetry)\n\u2200i, j, k \u2208I : i >u j \u2227j >u k \u21d2i >u k\n(transitivity)\nFor convenience we also de\ufb01ne:\nI+\nu := {i \u2208I : (u, i) \u2208S}\nU +\ni := {u \u2208U : (u, i) \u2208S}\n3.2\nAnalysis of the problem setting\nAs we have indicated before, in implicit feedback sys-\ntems only positive classes are observed. The remain-\ning data is a mixture of actually negative and missing\nvalues. The most common approach for coping with\nthe missing value problem is to ignore all of them but\nthen typical machine learning models are unable to\nlearn anything, because they cannot distinguish be-\ntween the two levels anymore.\nThe usual approach for item recommenders is to pre-\ndict a personalized score \u02c6xui for an item that re\ufb02ects\nRENDLE ET AL.\nUAI 2009\n453\n? +\n+ ?\n+ ? ? +\n+\n+ ? ?\n? ? +\n+\n? ? + ?\ni1\u00a0\u00a0\u00a0i2\u00a0\u00a0\u00a0\u00a0i3\u00a0\u00a0\u00a0i4\nitem\nuser\nu1\nu2\nu3\nu4\nu5\n0\n1\n1\n0\n1\n0\n0\n1\n1\n1\n0\n0\n0\n0\n1\n1\n0\n0\n1\n0\ni1\u00a0\u00a0\u00a0i2\u00a0\u00a0\u00a0\u00a0i3\u00a0\u00a0\u00a0i4\nitem\nuser\nu1\nu2\nu3\nu4\nu5\nFigure 1: On the left side, the observed data S is\nshown. Learning directly from S is not feasible as only\npositive feedback is observed. Usually negative data is\ngenerated by \ufb01lling the matrix with 0 values.\nthe preference of the user for the item. Then the items\nare ranked by sorting them according to that score.\nMachine learning approaches for item recommenders\n[5, 10] typically create the training data from S by\ngiving pairs (u, i) \u2208S a positive class label and all\nother combinations in (U \u00d7 I) \\ S a negative one (see\nFigure 1). Then a model is \ufb01tted to this data. That\nmeans the model is optimized to predict the value 1 for\nelements in S and 0 for the rest. The problem with this\napproach is that all elements the model should rank in\nthe future ((U \u00d7 I) \\ S) are presented to the learning\nalgorithm as negative feedback during training. That\nmeans a model with enough expressiveness (that can\n\ufb01t the training data exactly) cannot rank at all as it\npredicts only 0s. The only reason why such machine\nlearning methods can predict rankings are strategies\nto prevent over\ufb01tting, like regularization.\nWe use a di\ufb00erent approach by using item pairs as\ntraining data and optimize for correctly ranking item\npairs instead of scoring single items as this better rep-\nresents the problem than just replacing missing values\nwith negative ones. From S we try to reconstruct for\neach user parts of >u. If an item i has been viewed\nby user u \u2013 i.e.\n(u, i) \u2208S \u2013 then we assume that\nthe user prefers this item over all other non-observed\nitems. E.g. in Figure 2 user u1 has viewed item i2 but\nnot item i1, so we assume that this user prefers item\ni2 over i1: i2 >u i1. For items that have both been\nseen by a user, we cannot infer any preference. The\nsame is true for two items that a user has not seen yet\n(e.g. item i1 and i4 for user u1). To formalize this we\ncreate training data DS : U \u00d7 I \u00d7 I by:\nDS := {(u, i, j)|i \u2208I+\nu \u2227j \u2208I \\ I+\nu }\nThe semantics of (u, i, j) \u2208DS is that user u is as-\nsumed to prefer i over j. As >u is antisymmetric, the\nnegative cases are regarded implicitly.\nOur approach has two advantages:\n1. Our training data consists of both positive and\n? +\n+ ?\n+ ? ? +\n+\n+ ? ?\n? ? +\n+\n? ? + ?\ni1\u00a0\u00a0\u00a0i2\u00a0\u00a0\u00a0\u00a0i3\u00a0\u00a0\u00a0i4\nitem\nuser\nu1\nu2\nu3\nu4\nu5\n+\n+ ?\n\u00ad\n?\n\u00ad\n\u00ad\n?\n\u00ad\n? +\n+\ni1\u00a0\u00a0\u00a0i2\u00a0\u00a0\u00a0\u00a0i3\u00a0\u00a0\u00a0i4\nj1\nj2\nj3\nj4\nitem\nitem\nu1:\u00a0\u00a0\u00a0i\u00a0>u1\u00a0j\n? + ?\n?\n+ ?\n\u00ad\n\u00ad\n\u00ad\n? ? +\ni1\u00a0\u00a0\u00a0i2\u00a0\u00a0\u00a0\u00a0i3\u00a0\u00a0\u00a0i4\nj1\nj2\nj3\nj4\nitem\nitem\nu5:\u00a0\u00a0\u00a0i\u00a0>u5\u00a0j\n...\nFigure 2: On the left side, the observed data S is\nshown.\nOur approach creates user speci\ufb01c pairwise\npreferences i >u j between a pair of items. On the\nright side, plus (+) indicates that a user prefers item i\nover item j; minus (\u2013) indicates that he prefers j over i.\nnegative pairs and missing values. The missing\nvalues between two non-observed items are ex-\nactly the item pairs that have to be ranked in\nthe future. That means from a pairwise point of\nview the training data DS and the test data is\ndisjoint.\n2. The training data is created for the actual objec-\ntive of ranking, i.e. the observed subset DS of >u\nis used as training data.\n4\nBayesian Personalized Ranking\n(BPR)\nIn this section we derive a generic method for solv-\ning the personalized ranking task. It consists of the\ngeneral optimization criterion for personalized rank-\ning, BPR-Opt, which will be derived by a Bayesian\nanalysis of the problem using the likelihood function\nfor p(i >u j|\u0398) and the prior probability for the model\nparameter p(\u0398). We show the analogies to the ranking\nstatistic AUC (area under the ROC curve). For learn-\ning models with respect to BPR-Opt, we propose the\nalgorithm LearnBPR. Finally, we show how BPR-\nOpt and LearnBPR can be applied to two state-of-\nthe-art recommender algorithms, matrix factorization\nand adaptive kNN. Optimized with BPR these mod-\nels are able to generate better rankings than with the\nusual training methods.\nRENDLE ET AL.\n454\nUAI 2009\n4.1\nBPR Optimization Criterion\nThe Bayesian formulation of \ufb01nding the correct per-\nsonalized ranking for all items i \u2208I is to maximize\nthe following posterior probability where \u0398 represents\nthe parameter vector of an arbitrary model class (e.g.\nmatrix factorization).\np(\u0398| >u) \u221dp(>u |\u0398) p(\u0398)\nHere, >u is the desired but latent preference structure\nfor user u. All users are presumed to act independently\nof each other. We also assume the ordering of each\npair of items (i, j) for a speci\ufb01c user is independent\nof the ordering of every other pair. Hence, the above\nuser-speci\ufb01c likelihood function p(>u |\u0398) can \ufb01rst be\nrewritten as a product of single densities and second\nbe combined for all users u \u2208U.\nY\nu\u2208U\np(>u |\u0398) =\nY\n(u,i,j)\u2208U\u00d7I\u00d7I\np(i >u j|\u0398)\u03b4((u,i,j)\u2208DS)\n\u00b7 (1 \u2212p(i >u j|\u0398))\u03b4((u,j,i)\u0338\u2208DS)\nwhere \u03b4 is the indicator function:\n\u03b4(b) :=\n(\n1\nif b is true,\n0\nelse\nDue to the totality and antisymmetry of a sound pair-\nwise ordering scheme the above formula can be simpli-\n\ufb01ed to:\nY\nu\u2208U\np(>u |\u0398) =\nY\n(u,i,j)\u2208DS\np(i >u j|\u0398)\nSo far it is generally not guaranteed to get a person-\nalized total order. In order to establish this, the al-\nready mentioned sound properties (totality, antisym-\nmetry and transitivity) need to be ful\ufb01lled. To do so,\nwe de\ufb01ne the individual probability that a user really\nprefers item i to item j as:\np(i >u j|\u0398) := \u03c3(\u02c6xuij(\u0398))\nwhere \u03c3 is the logistic sigmoid:\n\u03c3(x) :=\n1\n1 + e\u2212x\nHere \u02c6xuij(\u0398) is an arbitrary real-valued function of\nthe model parameter vector \u0398 which captures the spe-\ncial relationship between user u, item i and item j.\nIn other words, our generic framework delegates the\ntask of modeling the relationship between u, i and j\nto an underlying model class like matrix factorization\nor adaptive kNN, which are in charge of estimating\n\u02c6xuij(\u0398).\nHence, it becomes feasible to statistically\n\u22126\n\u22124\n\u22122\n0\n2\n4\n6\n\u22121.5\n\u22120.5\n0.5\n1.0\nLoss functions\nx\nf(x)\nHeaviside\nHinge\nSigmoid\nln Sigmoid\nFigure 3:\nLoss functions for optimizing the AUC.\nThe non-di\ufb00erentiable Heaviside H(x) is often approx-\nimated by the sigmoid \u03c3(x). Our MLE derivation sug-\ngests to use ln \u03c3(x) instead.\nmodel a personalized total order >u. For convenience,\nin the following we will skip the argument \u0398 from \u02c6xuij.\nSo far, we have only discussed the likelihood function.\nIn order to complete the Bayesian modeling approach\nof the personalized ranking task, we introduce a gen-\neral prior density p(\u0398) which is a normal distribution\nwith zero mean and variance-covariance matrix \u03a3\u0398.\np(\u0398) \u223cN(0, \u03a3\u0398)\nIn the following, to reduce the number of unknown\nhyperparameters we set \u03a3\u0398 = \u03bb\u0398I. Now we can for-\nmulate the maximum posterior estimator to derive our\ngeneric optimization criterion for personalized ranking\nBPR-Opt.\nBPR-Opt := ln p(\u0398| >u)\n= ln p(>u |\u0398) p(\u0398)\n= ln\nY\n(u,i,j)\u2208DS\n\u03c3(\u02c6xuij) p(\u0398)\n=\nX\n(u,i,j)\u2208DS\nln \u03c3(\u02c6xuij) + ln p(\u0398)\n=\nX\n(u,i,j)\u2208DS\nln \u03c3(\u02c6xuij) \u2212\u03bb\u0398||\u0398||2\nwhere \u03bb\u0398 are model speci\ufb01c regularization parameters.\n4.1.1\nAnalogies to AUC optimization\nWith this formulation of the Bayesian Personalized\nRanking (BPR) scheme, it is now easy to grasp the\nanalogy between BPR and AUC. The AUC per user is\nRENDLE ET AL.\nUAI 2009\n455\nusually de\ufb01ned as:\nAUC(u) :=\n1\n|I+\nu | |I \\ I+\nu |\nX\ni\u2208I+\nu\nX\nj\u2208|I\\I+\nu |\n\u03b4(\u02c6xuij > 0)\nHence the average AUC is:\nAUC :=\n1\n|U|\nX\nu\u2208U\nAUC(u)\nWith our notation of DS this can be written as:\nAUC(u) =\nX\n(u,i,j)\u2208DS\nzu \u03b4(\u02c6xuij > 0)\n(1)\nwhere zu is the normalizing constant:\nzu =\n1\n|U| |I+\nu | |I \\ I+\nu |\nThe analogy between (1) and BPR-Opt is obvious.\nBesides the normalizing constant zu they only di\ufb00er in\nthe loss function. The AUC uses the non-di\ufb00erentiable\nloss \u03b4(x > 0) which is identical to the Heaviside func-\ntion:\n\u03b4(x > 0) = H(x) :=\n(\n1,\nx > 0\n0,\nelse\nInstead we use the di\ufb00erentiable loss ln \u03c3(x). It is com-\nmon practice to replace the non-di\ufb00erentiable Heav-\niside function when optimizing for AUC [3].\nOften\nthe choice of the substitution is heuristic and a simi-\nlarly shaped function like \u03c3 is used (see \ufb01gure 3). In\nthis paper, we have derived the alternative substitu-\ntion ln \u03c3(x) that is motivated by the MLE.\n4.2\nBPR Learning Algorithm\nIn the last section we have derived an optimization\ncriterion for personalized ranking.\nAs the criterion\nis di\ufb00erentiable, gradient descent based algorithms\nare an obvious choice for maximization.\nBut as we\nwill see, standard gradient descent is not the right\nchoice for our problem. To solve this issue we propose\nLearnBPR, a stochastic gradient-descent algorithm\nbased on bootstrap sampling of training triples (see\n\ufb01gure 4).\nFirst of all the gradient of BPR-Opt with respect to\nthe model parameters is:\n\u2202BPR-Opt\n\u2202\u0398\n=\nX\n(u,i,j)\u2208DS\n\u2202\n\u2202\u0398 ln \u03c3(\u02c6xuij) \u2212\u03bb\u0398\n\u2202\n\u2202\u0398||\u0398||2\n\u221d\nX\n(u,i,j)\u2208DS\n\u2212e\u2212\u02c6xuij\n1 + e\u2212\u02c6xuij \u00b7 \u2202\n\u2202\u0398 \u02c6xuij \u2212\u03bb\u0398\u0398\n1: procedure LearnBPR(DS, \u0398)\n2:\ninitialize \u0398\n3:\nrepeat\n4:\ndraw (u, i, j) from DS\n5:\n\u0398 \u2190\u0398 + \u03b1\n\u0010\ne\u2212\u02c6xuij\n1+e\u2212\u02c6xuij \u00b7\n\u2202\n\u2202\u0398 \u02c6xuij + \u03bb\u0398 \u00b7 \u0398\n\u0011\n6:\nuntil convergence\n7:\nreturn \u02c6\u0398\n8: end procedure\nFigure 4: Optimizing models for BPR with bootstrap-\nping based stochastic gradient descent. With learning\nrate \u03b1 and regularization \u03bb\u0398.\nThe two most common algorithms for gradient descent\nare either full or stochastic gradient descent. In the\n\ufb01rst case, in each step the full gradient over all training\ndata is computed and then the model parameters are\nupdated with the learning rate \u03b1:\n\u0398 \u2190\u0398 \u2212\u03b1\u2202BPR-Opt\n\u2202\u0398\nIn general this approach leads to a descent in the \u2018cor-\nrect\u2019 direction, but convergence is slow. As we have\nO(|S| |I|) training triples in DS, computing the full\ngradient in each update step is not feasible. Further-\nmore, for optimizing BPR-Opt with full gradient de-\nscent also the skewness in the training pairs leads to\npoor convergence. Imagine an item i that is often pos-\nitive. Then we have many terms of the form \u02c6xuij in the\nloss because for many users u the item i is compared\nagainst all negative items j (the dominating class).\nThus the gradient for model parameters depending on\ni would dominate largely the gradient. That means\nvery small learning rates would have to be chosen. Sec-\nondly, regularization is di\ufb03cult as the gradients di\ufb00er\nmuch.\nThe other popular approach is stochastic gradient de-\nscent.\nIn this case for each triple (u, i, j) \u2208DS an\nupdate is performed.\n\u0398 \u2190\u0398 + \u03b1\n\u0012\ne\u2212\u02c6xuij\n1 + e\u2212\u02c6xuij \u00b7 \u2202\n\u2202\u0398 \u02c6xuij + \u03bb\u0398\u0398\n\u0013\nIn general this is a good approach for our skew problem\nbut the order in which the training pairs are traversed\nis crucial. A typical approach that traverses the data\nitem-wise or user-wise will lead to poor convergence\nas there are so many consecutive updates on the same\nuser-item pair \u2013 i.e. for one user-item pair (u, i) there\nare many j with (u, i, j) \u2208DS.\nTo solve this issue we suggest to use a stochastic gra-\ndient descent algorithm that chooses the triples ran-\ndomly (uniformly distributed).\nWith this approach\nthe chances to pick the same user-item combination\nRENDLE ET AL.\n456\nUAI 2009\n0e+00\n2e+09\n4e+09\n6e+09\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nConvergence on Rossmann dataset\nNumber of single updates\nAUC\nLearnBPR\nuser\u2212wise stochastic gradient descent\nFigure 5: Empirical comparison of the convergence\nof typical user-wise stochastic gradient descent to our\nLearnBPR algorithm with bootstrap sampling.\nin consecutive update steps is small. We suggest to\nuse a bootstrap sampling approach with replacement\nbecause stopping can be performed at any step. Aban-\ndoning the idea of full cycles through the data is es-\npecially useful in our case as the number of examples\nis very large and for convergence often a fraction of a\nfull cycle is su\ufb03cient. We choose the number of sin-\ngle steps in our evaluation linearly depending on the\nnumber of observed positive feedback S.\nFigure 5 shows a comparison1 of a typical user-\nwise stochastic gradient descent to our approach\nLearnBPR with bootstrapping. The model is BPR-\nMF with 16 dimensions. As you can see LearnBPR\nconverges much faster than user-wise gradient descent.\n4.3\nLearning models with BPR\nIn the following we describe two state-of-the-art model\nclasses for item recommendation and how we can learn\nthem with our proposed BPR methods. We have cho-\nsen the two diverse model classes of matrix factoriza-\ntion [5, 12] and learned k-nearest-neighbor [8]. Both\nclasses try to model the hidden preferences of a user\non an item. Their prediction is a real number \u02c6xul per\nuser-item-pair (u, l).\nBecause in our optimization we have triples (u, i, j) \u2208\nDS, we \ufb01rst decompose the estimator \u02c6xuij and de\ufb01ne\nit as:\n\u02c6xuij := \u02c6xui \u2212\u02c6xuj\n1Details about the dataset and evaluation method can\nbe found in Section 6.\nNow we can apply any standard collaborative \ufb01ltering\nmodel that predicts \u02c6xul.\nIt is important to note that even though we use the\nsame models as in other work, we optimize them\nagainst another criterion. This will lead to a better\nranking because our criterion is optimal for the rank-\ning task. Our criterion does not try to regress a single\npredictor \u02c6xul to a single number but instead tries to\nclassify the di\ufb00erence of two predictions \u02c6xui \u2212\u02c6xuj.\n4.3.1\nMatrix Factorization\nThe problem of predicting \u02c6xui can be seen as the task\nof estimating a matrix X : U \u00d7 I. With matrix fac-\ntorization the target matrix X is approximated by the\nmatrix product of two low-rank matrices W : |U| \u00d7 k\nand H : |I| \u00d7 k:\n\u02c6X := WHt\nwhere k is the dimensionality/rank of the approxima-\ntion. Each row wu in W can be seen as a feature vector\ndescribing a user u and similarly each row hi of H de-\nscribes an item i. Thus the prediction formula can also\nbe written as:\n\u02c6xui = \u27e8wu, hi\u27e9=\nk\nX\nf=1\nwuf \u00b7 hif\nBesides the dot product \u27e8\u00b7, \u00b7\u27e9in general any kernel can\nbe used like in [11]. The model parameters for matrix\nfactorization are \u0398 = (W, H). The model parameters\ncan also be seen as latent variables, modeling the non-\nobserved taste of a user and the non-observed proper-\nties of an item.\nIn general the best approximation of \u02c6X to X with re-\nspect to least-square is achieved by the singular value\ndecomposition (SVD). For machine learning tasks, it is\nknown that SVD over\ufb01ts and therefore many other ma-\ntrix factorization methods have been proposed, includ-\ning regularized least square optimization, non-negative\nfactorization, maximum margin factorization, etc.\nFor the task of ranking, i.e.\nestimating whether\na user prefers one item over another, a better ap-\nproach is to optimize against the BPR-Opt crite-\nrion. This can be achieved by using our proposed al-\ngorithm LearnBPR. As stated before for optimizing\nwith LearnBPR, only the gradient of \u02c6xuij with re-\nspect to every model parameter \u03b8 has to be known.\nFor the matrix factorization model the derivatives are:\n\u2202\n\u2202\u03b8 \u02c6xuij =\n\uf8f1\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f2\n\uf8f4\n\uf8f4\n\uf8f4\n\uf8f3\n(hif \u2212hjf)\nif \u03b8 = wuf,\nwuf\nif \u03b8 = hif,\n\u2212wuf\nif \u03b8 = hjf,\n0\nelse\nRENDLE ET AL.\nUAI 2009\n457\nFurthermore, we use three regularization constants:\none \u03bbW for the user features W; for the item features\nH we have two regularization constants, \u03bbH+ that is\nused for positive updates on hif, and \u03bbH\u2212for negative\nupdates on hjf.\n4.3.2\nAdaptive k-Nearest-Neighbor\nNearest-neighbor methods are very popular in collab-\norative \ufb01ltering. They rely on a similarity measure be-\ntween either items (item-based) or users (user-based).\nIn the following we describe item-based methods as\nthey usually provide better results, but user-based\nmethods work analogously. The idea is that the pre-\ndiction for a user u and an item i depends on the\nsimilarity of i to all other items the user has seen in\nthe past \u2013 i.e. I+\nu . Often only the k most similar items\nof I+\nu are regarded \u2013 the k-nearest neighbors. If the\nsimilarities between items are chosen carefully, one can\nalso compare to all items in I+\nu . For item prediction\nthe model of item-based k-nearest-neighbor is:\n\u02c6xui =\nX\nl\u2208I+\nu \u2227l\u0338=i\ncil\nwhere C : I \u00d7 I is the symmetric item-correlation/\nitem-similarity matrix. Hence the model parameters\nof kNN are \u0398 = C.\nThe common approach for choosing C is by applying\na heuristic similarity measure, e.g. cosine vector simi-\nlarity:\nccosine\ni,j\n:=\n|U +\ni \u2229U +\nj |\nq\n|U +\ni | \u00b7 |U +\nj |\nA better strategy is to adapt the similarity measure C\nto the problem by learning it. This can be either done\nby using C directly as model parameters or if the num-\nber of items is too large, one can learn a factorization\nHHt of C with H : I \u00d7 k. In the following and also in\nour evaluation we use the \ufb01rst approach of learning C\ndirectly without factorizing it.\nAgain for optimizing the kNN model for ranking, we\napply the BPR optimization criterion and use the\nLearnBPR algorithm. For applying the algorithm,\nthe gradient of \u02c6xuij with respect to the model param-\neters C is:\n\u2202\n\u2202\u03b8 \u02c6xuij =\n\uf8f1\n\uf8f4\n\uf8f2\n\uf8f4\n\uf8f3\n+1\nif \u03b8 \u2208{cil, cli} \u2227l \u2208I+\nu \u2227l \u0338= i,\n\u22121\nif \u03b8 \u2208{cjl, clj} \u2227l \u2208I+\nu \u2227l \u0338= j,\n0\nelse\nWe have two regularization constants, \u03bb+ for updates\non cil, and \u03bb\u2212for updates on cjl.\n5\nRelations to other methods\nWe discuss the relations of our proposed methods for\nranking to two further item prediction models.\n5.1\nWeighted Regularized Matrix\nFactorization (WR-MF)\nBoth Pan et al. [10] and Hu et al. [5] have presented a\nmatrix factorization method for item prediction from\nimplicit feedback. Thus the model class is the same as\nwe described in Section 4.3.1, i.e. \u02c6X := WHt with the\nmatrices W : |U| \u00d7 k and H : |U| \u00d7 k. The optimiza-\ntion criterion and learning method di\ufb00er substantially\nfrom our approach. Their method is an adaption of\na SVD, which minimizes the square-loss.\nTheir ex-\ntensions are regularization to prevent over\ufb01tting and\nweights in the error function to increase the impact of\npositive feedback. In total their optimization criterion\nis:\nX\nu\u2208U\nX\ni\u2208I\ncui(\u27e8wu, hi\u27e9\u22121)2 + \u03bb||W||2\nf + \u03bb||H||2\nf\nwhere cui are not model parameters but apriori given\nweights for each tuple (u, i). Hu et al. have additional\ndata to estimate cui for positive feedback and they set\ncui = 1 for the rest. Pan et al. suggest to set cui = 1\nfor positive feedback and choose lower constants for\nthe rest.\nFirst of all, it is obvious that this optimization is on in-\nstance level (one item) instead of pair level (two items)\nas BPR. Apart from this, their optimization is a least-\nsquare which is known to correspond to the MLE for\nnormally distributed random variables. However, the\ntask of item prediction is actually not a regression\n(quantitative), but a classi\ufb01cation (qualitative) one,\nso the logistic optimization is more appropriate.\nA strong point of WR-MF is that it can be learned in\nO(iter (|S| k2 +k3 (|I|+|U|))) provided that cui is con-\nstant for non-positive pairs. Our evaluation indicates\nthat LearnBPR usually converges after a subsample\nof m \u00b7 |S| single update steps even though there are\nmuch more triples to learn from.\n5.2\nMaximum Margin Matrix Factorization\n(MMMF)\nWeimer et al.\n[15] use the maximum margin ma-\ntrix factorization method (MMMF) for ordinal rank-\ning. Their MMMF is designed for scenarios with ex-\nplicit feedback in terms of ratings. Even though their\nranking MMMF is not intended for implicit feedback\ndatasets, one could apply it in our scenario by giving\nall non-observed items the \u2018rating\u2019 0 and the observed\nones a 1 (see Figure 1). With these modi\ufb01cations their\nRENDLE ET AL.\n458\nUAI 2009\noptimization criterion to be minimized would be quite\nsimilar to BPR applied for matrix factorization:\nX\n(u,i,j)\u2208Ds\nmax(0, 1\u2212\u27e8wu, hi \u2212hj\u27e9)+\u03bbw||W||2\nf+\u03bbh||H||2\nf\nOne di\ufb00erence is that the error functions di\ufb00er \u2013 our\nhinge loss is smooth and motivated by the MLE. Ad-\nditionally, our BPR-Opt criterion is generic and can\nbe applied to several models, whereas their method is\nspeci\ufb01c for MF.\nBesides this, their learning method for MMMF di\ufb00ers\nfrom our generic approach LearnBPR. Their learning\nmethod is designed to work with sparse explicit data,\ni.e. they assume that there are many missing values\nand thus they assume to have much less pairs than in\nan implicit setting. But when their learning method\nis applied to implicit feedback datasets, the data has\nto be densi\ufb01ed like described above and the number\nof training pairs DS is in O(|S| |I|).\nOur method\nLearnBPR can handle this situation by bootstrap-\nping from DS (see Section 4.2).\n6\nEvaluation\nIn our evaluation we compare learning with BPR to\nother learning approaches. We have chosen the two\npopular model classes of matrix factorization (MF)\nand k-nearest-neighbor (kNN). MF models are known\nto outperform [12] many other models including the\nBayesian models URP [9] and PLSA [4] for the related\ntask of collaborative rating prediction.\nIn our eval-\nuation, the matrix factorization models are learned\nby three di\ufb00erent methods, i.e.\nSVD-MF, WR-MF\n[5, 10] and our BPR-MF. For kNN, we compare cosine\nvector similarity (Cosine-kNN) to a model that has\nbeen optimized using our BPR method (BPR-kNN).\nAdditionally, we report results for the baseline most-\npopular, that weights each item user-independent, e.g.:\n\u02c6xmost-pop\nui\n:= |U +\ni |.\nFurthermore, we give the theo-\nretical upper bound on AUC (npmax) for any non-\npersonalized ranking method.\n6.1\nDatasets\nWe use two datasets of two di\ufb00erent applications. The\nRossmann dataset is from an online shop. It contains\nthe buying history of 10, 000 users on 4000 items. In\ntotal 426, 612 purchases are recorded. The task is to\npredict a personalized list of the items the user wants\nto buy next. The second dataset is the DVD rental\ndataset of Net\ufb02ix.\nThis dataset contains the rating\nbehavior of users, where a user provides explicit ratings\n1 to 5 stars for some movies. As we want to solve an\nimplicit feedback task, we removed the rating scores\nfrom the dataset. Now the task is to predict if a user\nis likely to rate a movie. Again we are interested in a\npersonalized ranked list starting with the movie that\nis most likely to be rated. For Net\ufb02ix we have created\na subsample of 10, 000 users, 5000 items containing\n565, 738 rating actions. We draw the subsample such\nthat every user has at least 10 items (\u2200u \u2208U : |I+\nu | \u2265\n10) and each item has at least 10 users: \u2200i \u2208I : |U +\ni | \u2265\n10.\n6.2\nEvaluation Methodology\nWe use the leave one out evaluation scheme, where we\nremove for each user randomly one action (one user-\nitem pair) from his history, i.e. we remove one entry\nfrom I+\nu per user u. This results in a disjoint train set\nStrain and test set Stest. The models are then learned\non Strain and their predicted personalized ranking is\nevaluated on the test set Stest by the average AUC\nstatistic:\nAUC =\n1\n|U|\nX\nu\n1\n|E(u)|\nX\n(i,j)\u2208E(u)\n\u03b4(\u02c6xui > \u02c6xuj)\n(2)\nwhere the evaluation pairs per user u are:\nE(u) := {(i, j)|(u, i) \u2208Stest \u2227(u, j) \u0338\u2208(Stest \u222aStrain)}\nA higher value of the AUC indicates a better quality.\nThe trivial AUC of a random guess method is 0.5 and\nthe best achievable quality is 1.\nWe repeated all experiments 10 times by drawing new\ntrain/test splits in each round. The hyperparameters\nfor all methods are optimized via grid search in the\n\ufb01rst round and afterwards are kept constant in the\nremaining 9 repetitions.\n6.3\nResults and Discussion\nFigure 6 shows the AUC quality of all models on the\ntwo datasets. First of all, you can see that the two\nBPR optimized methods outperform all other meth-\nods in prediction quality. Comparing the same mod-\nels among each other one can see the importance of\nthe optimization method. For example all MF meth-\nods (SVD-MF, WR-MF and BPR-MF) share exactly\nthe same model, but their prediction quality di\ufb00ers a\nlot. Even though SVD-MF is known to yield the best\n\ufb01t on the training data with respect to element-wise\nleast square, it is a poor prediction method for machine\nlearning tasks as it results in over\ufb01tting. This can be\nseen as the quality of SVD-MF decreases with an in-\ncreasing number of dimensions.\nWR-MF is a more\nsuccessful learning method for the task of ranking.\nDue to regularization its performance does not drop\nbut steadily rises with an increasing number of dimen-\nsions. But BPR-MF outperforms WR-MF clearly for\nRENDLE ET AL.\nUAI 2009\n459\n10\n20\n50\n100\n0.75\n0.80\n0.85\n0.90\nOnline shopping: Rossmann\nnumber of dimensions\nAUC\nG\nG\nG\nG\nG\nG\nBPR\u2212MF\nBPR\u2212kNN\nWR\u2212MF\nSVD\u2212MF\nCosine\u2212kNN\nmost popular\nnpmax\n10\n20\n50\n100\n0.80\n0.82\n0.84\n0.86\n0.88\n0.90\n0.92\nVideo Rental: Netflix\nnumber of dimensions\nAUC\nG\nG\nG\nG\nG\nG\nBPR\u2212MF\nBPR\u2212kNN\nWR\u2212MF\nSVD\u2212MF\nCosine\u2212kNN\nmost popular\nnpmax\nFigure 6: Area under the ROC curve (AUC) prediction quality for the Rossmann dataset and a Net\ufb02ix subsample.\nOur BPR optimizations for matrix factorization BPR-MF and k-nearest neighbor BPR-kNN are compared\nagainst weighted regularized matrix factorization (WR-MF) [5, 10], singular value decomposition (SVD-MF),\nk-nearest neighbor (Cosine-kNN) [2] and the most-popular model.\nFor the factorization methods BPR-MF,\nWR-MF and SVD-MF, the model dimensions are increased from 8 to 128 dimensions. Finally, npmax is the\ntheoretical upper bound for any non-personalized ranking method.\nthe task of ranking on both datasets. For example on\nNet\ufb02ix a MF model with 8 dimensions optimized by\nBPR-MF achieves comparable quality as a MF model\nwith 128 dimensions optimized by WR-MF.\nTo summarize, our results show the importance of op-\ntimizing model parameters to the right criterion. The\nempirical results indicate that our BPR-Opt criterion\nlearned by LearnBPR outperforms the other state-\nof-the-art methods for personalized ranking from im-\nplicit feedback. The results are justi\ufb01ed by the analy-\nsis of the problem (section 3.2) and by the theoretical\nderivation of BPR-Opt from the MLE.\n6.4\nNon-personalized ranking\nFinally, we compare the AUC quality of our per-\nsonalized ranking methods to the best possible non-\npersonalized ranking method. In contrast to our per-\nsonalized ranking methods, a non-personalized rank-\ning method creates the same ranking > for all users.\nWe compute the theoretical upper-bound npmax for\nany non-personalized ranking method by optimizing\nthe ranking > on the test set Stest2. Figure 6 shows\n2We computed a real upper-bound but non-tight es-\ntimate on the AUC score.\nPlease note that ranking by\nmost-popular on test is not an upper bound on AUC. But\nthat even simple personalized methods like Cosine-\nkNN outperform the upper-bound npmax \u2014 and thus\nalso all non-personalized methods \u2014 largely.\n7\nConclusion\nIn this paper we have presented a generic optimiza-\ntion criterion and learning algorithm for personal-\nized ranking. The optimization criterion BPR-Opt\nis the maximum posterior estimator that is derived\nfrom a Bayesian analysis of the problem. For learn-\ning models with respect to BPR-Opt we have pre-\nsented the generic learning algorithm LearnBPR that\nis based on stochastic gradient descent with bootstrap\nsampling.\nWe have demonstrated how this generic\nmethod can be applied to the two state-of-the-art rec-\nommender models of matrix factorization and adap-\ntive kNN. In our evaluation we show empirically that\nfor the task of personalized ranking, models learned\nby BPR outperform the same models that are opti-\nmized with respect to other criteria. Our results show\nthat the prediction quality does not only depend on\nthe model but also largely on the optimization crite-\nin our experiments both AUC scores are quite similar, e.g.\non Net\ufb02ix with most-popular on test 0.8794 vs. our upper\nbound of 0.8801.\nRENDLE ET AL.\n460\nUAI 2009\nrion. Both our theoretical and empirical results indi-\ncate that the BPR optimization method is the right\nchoice for the important task of personalized ranking.\nAcknowledgements\nThe\nauthors\ngratefully\nacknowledge\nthe\npar-\ntial\nco-funding\nof\ntheir\nwork\nthrough\nthe\nEu-\nropean\nCommission\nFP7\nproject\nMyMedia\n(www.mymediaproject.org) under the grant agree-\nment no. 215006. For your inquiries please contact\ninfo@mymediaproject.org.\nReferences\n[1] C. Burges, T. Shaked, E. Renshaw, A. Lazier,\nM. Deeds, N. Hamilton, and G. Hullender. Learn-\ning to rank using gradient descent.\nIn ICML\n\u201905: Proceedings of the 22nd international con-\nference on Machine learning, pages 89\u201396, New\nYork, NY, USA, 2005. ACM Press.\n[2] M. Deshpande and G. Karypis. Item-based top-n\nrecommendation algorithms. ACM Transactions\non Information Systems. Springer-Verlag, 22/1,\n2004.\n[3] A. Herschtal and B. Raskutti. Optimising area\nunder the roc curve using gradient descent.\nIn\nICML \u201904: Proceedings of the twenty-\ufb01rst inter-\nnational conference on Machine learning, page 49,\nNew York, NY, USA, 2004. ACM.\n[4] T. Hofmann. Latent semantic models for collabo-\nrative \ufb01ltering. ACM Trans. Inf. Syst., 22(1):89\u2013\n115, 2004.\n[5] Y. Hu, Y. Koren, and C. Volinsky. Collaborative\n\ufb01ltering for implicit feedback datasets. In IEEE\nInternational Conference on Data Mining (ICDM\n2008), pages 263\u2013272, 2008.\n[6] J. Huang, C. Guestrin, and L. Guibas. E\ufb03cient\ninference for distributions on permutations.\nIn\nJ. Platt, D. Koller, Y. Singer, and S. Roweis,\neditors, Advances in Neural Information Process-\ning Systems 20, pages 697\u2013704, Cambridge, MA,\n2008. MIT Press.\n[7] R. Kondor, A. Howard, and T. Jebara.\nMulti-\nobject tracking with representations of the sym-\nmetric group. In Proceedings of the Eleventh In-\nternational Conference on Arti\ufb01cial Intelligence\nand Statistics, San Juan, Puerto Rico, March\n2007.\n[8] Y. Koren. Factorization meets the neighborhood:\na multifaceted collaborative \ufb01ltering model.\nIn\nKDD \u201908: Proceeding of the 14th ACM SIGKDD\ninternational conference on Knowledge discovery\nand data mining, pages 426\u2013434, New York, NY,\nUSA, 2008. ACM.\n[9] B. Marlin. Modeling user rating pro\ufb01les for col-\nlaborative \ufb01ltering.\nIn S. Thrun, L. Saul, and\nB. Sch\u00a8olkopf, editors, Advances in Neural Infor-\nmation Processing Systems 16, Cambridge, MA,\n2004. MIT Press.\n[10] R. Pan, Y. Zhou, B. Cao, N. N. Liu, R. M. Lukose,\nM. Scholz, and Q. Yang. One-class collaborative\n\ufb01ltering.\nIn IEEE International Conference on\nData Mining (ICDM 2008), pages 502\u2013511, 2008.\n[11] S. Rendle and L. Schmidt-Thieme.\nOnline-\nupdating regularized kernel matrix factorization\nmodels for large-scale recommender systems. In\nRecSys \u201908: Proceedings of the 2008 ACM confer-\nence on Recommender systems. ACM, 2008.\n[12] J. D. M. Rennie and N. Srebro.\nFast maxi-\nmum margin matrix factorization for collabora-\ntive prediction. In ICML \u201905: Proceedings of the\n22nd international conference on Machine learn-\ning, pages 713\u2013719, New York, NY, USA, 2005.\nACM.\n[13] B. Sarwar, G. Karypis, J. Konstan, and J. Riedl.\nIncremental singular value decomposition algo-\nrithms for highly scalable recommender systems.\nIn Proceedings of the 5th International Conference\nin Computers and Information Technology, 2002.\n[14] L. Schmidt-Thieme.\nCompound classi\ufb01cation\nmodels for recommender systems. In IEEE In-\nternational Conference on Data Mining (ICDM\n2005), pages 378\u2013385, 2005.\n[15] M. Weimer, A. Karatzoglou, and A. Smola. Im-\nproving maximum margin matrix factorization.\nMachine Learning, 72(3):263\u2013276, 2008.\nRENDLE ET AL.\nUAI 2009\n461\n",
        "sentence": " The network can be trained with several ranking loss functions such as cross-entropy, BPR [14] and TOP1 [7].",
        "context": "be used for BPR.\nThere is also related work on learning to rank with\nnon-collaborative models. One direction is to model\ndistributions on permutations [7, 6]. Burges et al. [1]\noptimize a neural network model for ranking using gra-\ndient descent.\n7\nConclusion\nIn this paper we have presented a generic optimiza-\ntion criterion and learning algorithm for personal-\nized ranking. The optimization criterion BPR-Opt\nis the maximum posterior estimator that is derived\ngeneral optimization criterion for personalized rank-\ning, BPR-Opt, which will be derived by a Bayesian\nanalysis of the problem using the likelihood function\nfor p(i >u j|\u0398) and the prior probability for the model"
    },
    {
        "title": "Neurocomputing: Foundations of research. chapter Learning Internal Representations by Error Propagation, pages 673\u2013695",
        "author": [
            "D.E. Rumelhart",
            "G.E. Hinton",
            "R.J. Williams"
        ],
        "venue": null,
        "citeRegEx": "15",
        "shortCiteRegEx": "15",
        "year": 1988,
        "abstract": "",
        "full_text": "",
        "sentence": " , Nm \u2212 1 (5) The model is trained end-to-end using back-propagation [15].",
        "context": null
    },
    {
        "title": "Item-based collaborative filtering recommendation algorithms",
        "author": [
            "Badrul Sarwar",
            "George Karypis",
            "Joseph Konstan",
            "John Riedl"
        ],
        "venue": "Int. Conf. on World Wide Web,",
        "citeRegEx": "16",
        "shortCiteRegEx": "16",
        "year": 2001,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " recommendation approach [11, 16].",
        "context": null
    },
    {
        "title": "A hierarchical recurrent encoder-decoder for generative context-aware query suggestion",
        "author": [
            "Alessandro Sordoni",
            "Yoshua Bengio",
            "Hossein Vahabi",
            "Christina Lioma",
            "Jakob Grue Simonsen",
            "Jian-Yun Nie"
        ],
        "venue": "In Proceedings of the 24th ACM International on Conference on Information and Knowledge Management,",
        "citeRegEx": "17",
        "shortCiteRegEx": "17",
        "year": 2015,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " Finally a sequence to sequence model with a version of Hierarchical Recurrent Neural Networks was used for generative context-aware query suggestion in [17]. 1To simplify the explanation we use a notation similar to [17].",
        "context": null
    },
    {
        "title": "Dropout: A simple way to prevent neural networks from overfitting",
        "author": [
            "Nitish Srivastava",
            "Geoffrey Hinton",
            "Alex Krizhevsky",
            "Ilya Sutskever",
            "Ruslan Salakhutdinov"
        ],
        "venue": "J. Mach. Learn. Res.,",
        "citeRegEx": "18",
        "shortCiteRegEx": "18",
        "year": 2014,
        "abstract": "",
        "full_text": "",
        "sentence": " We used dropout regularization [18] on the hidden states of RNN and HRNN .",
        "context": null
    },
    {
        "title": "Improved recurrent neural networks for session-based recommendations",
        "author": [
            "Yong Kiam Tan",
            "Xinxing Xu",
            "Yong Liu"
        ],
        "venue": "In Proceedings of the 1st Workshop on Deep Learning for Recommender Systems,",
        "citeRegEx": "19",
        "shortCiteRegEx": "19",
        "year": 2016,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " In [19] proposed data augmentation techniques to improve the performance of the RNN for session-based recommendations, these techniques have though the side effect of increasing training times as a single session is split into several sub-sessions for training.",
        "context": null
    },
    {
        "title": "Recurrent recommender networks",
        "author": [
            "Chao-Yuan Wu",
            "Amr Ahmed",
            "Alex Beutel",
            "Alexander J. Smola",
            "How Jing"
        ],
        "venue": "In Proceedings of the Tenth ACM International Conference on Web Search and  Data Mining,",
        "citeRegEx": "20",
        "shortCiteRegEx": "20",
        "year": 2017,
        "abstract": "Abstract n\u00e3o dispon\u00edvel",
        "full_text": "",
        "sentence": " RNN\u2019s have also been used in more standard user-item collaborative filtering settings where the aim is to model the evolution of the user and items factors [20] [4] where the results are though less impressive, with the proposed methods barely outperforming standard matrix factorization methods.",
        "context": null
    }
]