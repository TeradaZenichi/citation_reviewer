[
    {
        "title": "Representation learning: A review and new perspectives",
        "author": [
            "Bengio",
            "Yoshua",
            "Courville",
            "Aaron",
            "Vincent",
            "Pascal"
        ],
        "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,",
        "citeRegEx": "Bengio et al\\.,? \\Q2013\\E",
        "shortCiteRegEx": "Bengio et al\\.",
        "year": 2013,
        "abstract": "The success of machine learning algorithms generally depends on data\nrepresentation, and we hypothesize that this is because different\nrepresentations can entangle and hide more or less the different explanatory\nfactors of variation behind the data. Although specific domain knowledge can be\nused to help design representations, learning with generic priors can also be\nused, and the quest for AI is motivating the design of more powerful\nrepresentation-learning algorithms implementing such priors. This paper reviews\nrecent work in the area of unsupervised feature learning and deep learning,\ncovering advances in probabilistic models, auto-encoders, manifold learning,\nand deep networks. This motivates longer-term unanswered questions about the\nappropriate objectives for learning good representations, for computing\nrepresentations (i.e., inference), and the geometrical connections between\nrepresentation learning, density estimation and manifold learning.",
        "full_text": "1\nRepresentation Learning: A Review and New\nPerspectives\nYoshua Bengio†, Aaron Courville, and Pascal Vincent†\nDepartment of computer science and operations research, U. Montreal\n† also, Canadian Institute for Advanced Research (CIFAR)\n!\nAbstract—\nThe success of machine learning algorithms generally depends on\ndata representation, and we hypothesize that this is because different\nrepresentations can entangle and hide more or less the different ex-\nplanatory factors of variation behind the data. Although speciﬁc domain\nknowledge can be used to help design representations, learning with\ngeneric priors can also be used, and the quest for AI is motivating\nthe design of more powerful representation-learning algorithms imple-\nmenting such priors. This paper reviews recent work in the area of\nunsupervised feature learning and deep learning, covering advances\nin probabilistic models, auto-encoders, manifold learning, and deep\nnetworks. This motivates longer-term unanswered questions about the\nappropriate objectives for learning good representations, for computing\nrepresentations (i.e., inference), and the geometrical connections be-\ntween representation learning, density estimation and manifold learning.\nIndex Terms—Deep learning, representation learning, feature learning,\nunsupervised learning, Boltzmann Machine, autoencoder, neural nets\n1\nINTRODUCTION\nThe performance of machine learning methods is heavily\ndependent on the choice of data representation (or features)\non which they are applied. For that reason, much of the actual\neffort in deploying machine learning algorithms goes into the\ndesign of preprocessing pipelines and data transformations that\nresult in a representation of the data that can support effective\nmachine learning. Such feature engineering is important but\nlabor-intensive and highlights the weakness of current learning\nalgorithms: their inability to extract and organize the discrimi-\nnative information from the data. Feature engineering is a way\nto take advantage of human ingenuity and prior knowledge to\ncompensate for that weakness. In order to expand the scope\nand ease of applicability of machine learning, it would be\nhighly desirable to make learning algorithms less dependent\non feature engineering, so that novel applications could be\nconstructed faster, and more importantly, to make progress\ntowards Artiﬁcial Intelligence (AI). An AI must fundamentally\nunderstand the world around us, and we argue that this can\nonly be achieved if it can learn to identify and disentangle the\nunderlying explanatory factors hidden in the observed milieu\nof low-level sensory data.\nThis paper is about representation learning, i.e., learning\nrepresentations of the data that make it easier to extract useful\ninformation when building classiﬁers or other predictors. In\nthe case of probabilistic models, a good representation is often\none that captures the posterior distribution of the underlying\nexplanatory factors for the observed input. A good representa-\ntion is also one that is useful as input to a supervised predictor.\nAmong the various ways of learning representations, this paper\nfocuses on deep learning methods: those that are formed by\nthe composition of multiple non-linear transformations, with\nthe goal of yielding more abstract – and ultimately more useful\n– representations. Here we survey this rapidly developing area\nwith special emphasis on recent progress. We consider some\nof the fundamental questions that have been driving research\nin this area. Speciﬁcally, what makes one representation better\nthan another? Given an example, how should we compute its\nrepresentation, i.e. perform feature extraction? Also, what are\nappropriate objectives for learning good representations?\n2\nWHY SHOULD WE CARE ABOUT LEARNING\nREPRESENTATIONS?\nRepresentation learning has become a ﬁeld in itself in the\nmachine learning community, with regular workshops at the\nleading conferences such as NIPS and ICML, and a new\nconference dedicated to it, ICLR1, sometimes under the header\nof Deep Learning or Feature Learning. Although depth is an\nimportant part of the story, many other priors are interesting\nand can be conveniently captured when the problem is cast as\none of learning a representation, as discussed in the next sec-\ntion. The rapid increase in scientiﬁc activity on representation\nlearning has been accompanied and nourished by a remarkable\nstring of empirical successes both in academia and in industry.\nBelow, we brieﬂy highlight some of these high points.\nSpeech Recognition and Signal Processing\nSpeech was one of the early applications of neural networks,\nin particular convolutional (or time-delay) neural networks 2.\nThe recent revival of interest in neural networks, deep learning,\nand representation learning has had a strong impact in the\narea of speech recognition, with breakthrough results (Dahl\net al., 2010; Deng et al., 2010; Seide et al., 2011a; Mohamed\net al., 2012; Dahl et al., 2012; Hinton et al., 2012) obtained\nby several academics as well as researchers at industrial labs\nbringing these algorithms to a larger scale and into products.\nFor example, Microsoft has released in 2012 a new version\nof their MAVIS (Microsoft Audio Video Indexing Service)\n1. International Conference on Learning Representations\n2. See Bengio (1993) for a review of early work in this area.\narXiv:1206.5538v3  [cs.LG]  23 Apr 2014\n2\nspeech system based on deep learning (Seide et al., 2011a).\nThese authors managed to reduce the word error rate on\nfour major benchmarks by about 30% (e.g. from 27.4% to\n18.5% on RT03S) compared to state-of-the-art models based\non Gaussian mixtures for the acoustic modeling and trained on\nthe same amount of data (309 hours of speech). The relative\nimprovement in error rate obtained by Dahl et al. (2012) on a\nsmaller large-vocabulary speech recognition benchmark (Bing\nmobile business search dataset, with 40 hours of speech) is\nbetween 16% and 23%.\nRepresentation-learning algorithms have also been applied\nto music, substantially beating the state-of-the-art in poly-\nphonic transcription (Boulanger-Lewandowski et al., 2012),\nwith relative error improvement between 5% and 30% on a\nstandard benchmark of 4 datasets. Deep learning also helped\nto win MIREX (Music Information Retrieval) competitions,\ne.g. in 2011 on audio tagging (Hamel et al., 2011).\nObject Recognition\nThe beginnings of deep learning in 2006 have focused on\nthe MNIST digit image classiﬁcation problem (Hinton et al.,\n2006; Bengio et al., 2007), breaking the supremacy of SVMs\n(1.4% error) on this dataset3. The latest records are still held\nby deep networks: Ciresan et al. (2012) currently claims the\ntitle of state-of-the-art for the unconstrained version of the task\n(e.g., using a convolutional architecture), with 0.27% error,\nand Rifai et al. (2011c) is state-of-the-art for the knowledge-\nfree version of MNIST, with 0.81% error.\nIn the last few years, deep learning has moved from\ndigits to object recognition in natural images, and the latest\nbreakthrough has been achieved on the ImageNet dataset4\nbringing down the state-of-the-art error rate from 26.1% to\n15.3% (Krizhevsky et al., 2012).\nNatural Language Processing\nBesides speech recognition, there are many other Natural\nLanguage Processing (NLP) applications of representation\nlearning. Distributed representations for symbolic data were\nintroduced by\nHinton (1986), and ﬁrst developed in the\ncontext of statistical language modeling by Bengio et al.\n(2003) in so-called neural net language models (Bengio,\n2008). They are all based on learning a distributed repre-\nsentation for each word, called a word embedding. Adding a\nconvolutional architecture, Collobert et al. (2011) developed\nthe SENNA system5 that shares representations across the\ntasks of language modeling, part-of-speech tagging, chunking,\nnamed entity recognition, semantic role labeling and syntactic\nparsing. SENNA approaches or surpasses the state-of-the-art\non these tasks but is simpler and much faster than traditional\npredictors. Learning word embeddings can be combined with\nlearning image representations in a way that allow to associate\ntext and images. This approach has been used successfully to\nbuild Google’s image search, exploiting huge quantities of data\nto map images and queries in the same space (Weston et al.,\n3. for the knowledge-free version of the task, where no image-speciﬁc prior\nis used, such as image deformations or convolutions\n4. The 1000-class ImageNet benchmark, whose results are detailed here:\nhttp://www.image-net.org/challenges/LSVRC/2012/results.html\n5. downloadable from http://ml.nec-labs.com/senna/\n2010) and it has recently been extended to deeper multi-modal\nrepresentations (Srivastava and Salakhutdinov, 2012).\nThe neural net language model was also improved by\nadding recurrence to the hidden layers (Mikolov et al., 2011),\nallowing it to beat the state-of-the-art (smoothed n-gram\nmodels) not only in terms of perplexity (exponential of the\naverage negative log-likelihood of predicting the right next\nword, going down from 140 to 102) but also in terms of\nword error rate in speech recognition (since the language\nmodel is an important component of a speech recognition\nsystem), decreasing it from 17.2% (KN5 baseline) or 16.9%\n(discriminative language model) to 14.4% on the Wall Street\nJournal benchmark task. Similar models have been applied\nin statistical machine translation (Schwenk et al., 2012; Le\net al., 2013), improving perplexity and BLEU scores. Re-\ncursive auto-encoders (which generalize recurrent networks)\nhave also been used to beat the state-of-the-art in full sentence\nparaphrase detection (Socher et al., 2011a) almost doubling the\nF1 score for paraphrase detection. Representation learning can\nalso be used to perform word sense disambiguation (Bordes\net al., 2012), bringing up the accuracy from 67.8% to 70.2%\non the subset of Senseval-3 where the system could be applied\n(with subject-verb-object sentences). Finally, it has also been\nsuccessfully used to surpass the state-of-the-art in sentiment\nanalysis (Glorot et al., 2011b; Socher et al., 2011b).\nMulti-Task and Transfer Learning, Domain Adaptation\nTransfer learning is the ability of a learning algorithm to\nexploit commonalities between different learning tasks in order\nto share statistical strength, and transfer knowledge across\ntasks. As discussed below, we hypothesize that representation\nlearning algorithms have an advantage for such tasks because\nthey learn representations that capture underlying factors, a\nsubset of which may be relevant for each particular task, as\nillustrated in Figure 1. This hypothesis seems conﬁrmed by a\nnumber of empirical results showing the strengths of repre-\nsentation learning algorithms in transfer learning scenarios.\nraw input x \ntask 1  \noutput y1 \ntask 3  \noutput y3 \ntask 2 \noutput y2 \nTask%A%\nTask%B%\nTask%C%\n%output%\n%input%\n%shared%\nsubsets%of%\nfactors%\nFig. 1.\nIllustration of representation-learning discovering ex-\nplanatory factors (middle hidden layer, in red), some explaining\nthe input (semi-supervised setting), and some explaining target\nfor each task. Because these subsets overlap, sharing of statis-\ntical strength helps generalization.\n.\nMost impressive are the two transfer learning challenges\nheld in 2011 and won by representation learning algorithms.\nFirst, the Transfer Learning Challenge, presented at an ICML\n2011 workshop of the same name, was won using unsuper-\nvised layer-wise pre-training (Bengio, 2011; Mesnil et al.,\n2011). A second Transfer Learning Challenge was held the\n3\nsame year and won by Goodfellow et al. (2011). Results\nwere presented at NIPS 2011’s Challenges in Learning Hier-\narchical Models Workshop. In the related domain adaptation\nsetup, the target remains the same but the input distribution\nchanges (Glorot et al., 2011b; Chen et al., 2012). In the\nmulti-task learning setup, representation learning has also been\nfound advantageous Krizhevsky et al. (2012); Collobert et al.\n(2011), because of shared factors across tasks.\n3\nWHAT MAKES A REPRESENTATION GOOD?\n3.1\nPriors for Representation Learning in AI\nIn Bengio and LeCun (2007), one of us introduced the\nnotion of AI-tasks, which are challenging for current machine\nlearning algorithms, and involve complex but highly structured\ndependencies. One reason why explicitly dealing with repre-\nsentations is interesting is because they can be convenient to\nexpress many general priors about the world around us, i.e.,\npriors that are not task-speciﬁc but would be likely to be useful\nfor a learning machine to solve AI-tasks. Examples of such\ngeneral-purpose priors are the following:\n• Smoothness: assumes the function to be learned f is s.t.\nx ≈y generally implies f(x) ≈f(y). This most basic prior\nis present in most machine learning, but is insufﬁcient to get\naround the curse of dimensionality, see Section 3.2.\n• Multiple explanatory factors: the data generating distribu-\ntion is generated by different underlying factors, and for the\nmost part what one learns about one factor generalizes in many\nconﬁgurations of the other factors. The objective to recover\nor at least disentangle these underlying factors of variation is\ndiscussed in Section 3.5. This assumption is behind the idea of\ndistributed representations, discussed in Section 3.3 below.\n• A hierarchical organization of explanatory factors: the\nconcepts that are useful for describing the world around us\ncan be deﬁned in terms of other concepts, in a hierarchy, with\nmore abstract concepts higher in the hierarchy, deﬁned in\nterms of less abstract ones. This assumption is exploited with\ndeep representations, elaborated in Section 3.4 below.\n• Semi-supervised learning: with inputs X and target Y to\npredict, a subset of the factors explaining X’s distribution\nexplain much of Y , given X. Hence representations that are\nuseful for P(X) tend to be useful when learning P(Y |X),\nallowing sharing of statistical strength between the unsuper-\nvised and supervised learning tasks, see Section 4.\n• Shared factors across tasks: with many Y ’s of interest or\nmany learning tasks in general, tasks (e.g., the corresponding\nP(Y |X, task)) are explained by factors that are shared with\nother tasks, allowing sharing of statistical strengths across\ntasks, as discussed in the previous section (Multi-Task and\nTransfer Learning, Domain Adaptation).\n• Manifolds: probability mass concentrates near regions that\nhave a much smaller dimensionality than the original space\nwhere the data lives. This is explicitly exploited in some\nof the auto-encoder algorithms and other manifold-inspired\nalgorithms described respectively in Sections 7.2 and 8.\n• Natural clustering: different values of categorical variables\nsuch as object classes are associated with separate manifolds.\nMore precisely, the local variations on the manifold tend to\npreserve the value of a category, and a linear interpolation\nbetween examples of different classes in general involves\ngoing through a low density region, i.e., P(X|Y = i) for\ndifferent i tend to be well separated and not overlap much. For\nexample, this is exploited in the Manifold Tangent Classiﬁer\ndiscussed in Section 8.3. This hypothesis is consistent with the\nidea that humans have named categories and classes because\nof such statistical structure (discovered by their brain and\npropagated by their culture), and machine learning tasks often\ninvolves predicting such categorical variables.\n• Temporal and spatial coherence: consecutive (from a se-\nquence) or spatially nearby observations tend to be associated\nwith the same value of relevant categorical concepts, or result\nin a small move on the surface of the high-density manifold.\nMore generally, different factors change at different temporal\nand spatial scales, and many categorical concepts of interest\nchange slowly. When attempting to capture such categorical\nvariables, this prior can be enforced by making the associated\nrepresentations slowly changing, i.e., penalizing changes in\nvalues over time or space. This prior was introduced in Becker\nand Hinton (1992) and is discussed in Section 11.3.\n• Sparsity: for any given observation x, only a small fraction\nof the possible factors are relevant. In terms of representation,\nthis could be represented by features that are often zero (as\ninitially proposed by Olshausen and Field (1996)), or by the\nfact that most of the extracted features are insensitive to small\nvariations of x. This can be achieved with certain forms of\npriors on latent variables (peaked at 0), or by using a non-\nlinearity whose value is often ﬂat at 0 (i.e., 0 and with a\n0 derivative), or simply by penalizing the magnitude of the\nJacobian matrix (of derivatives) of the function mapping input\nto representation. This is discussed in Sections 6.1.1 and 7.2.\n• Simplicity of Factor Dependencies: in good high-level\nrepresentations, the factors are related to each other through\nsimple, typically linear dependencies. This can be seen in\nmany laws of physics, and is assumed when plugging a linear\npredictor on top of a learned representation.\nWe can view many of the above priors as ways to help the\nlearner discover and disentangle some of the underlying (and\na priori unknown) factors of variation that the data may reveal.\nThis idea is pursued further in Sections 3.5 and 11.4.\n3.2\nSmoothness and the Curse of Dimensionality\nFor AI-tasks, such as vision and NLP, it seems hopeless to\nrely only on simple parametric models (such as linear models)\nbecause they cannot capture enough of the complexity of in-\nterest unless provided with the appropriate feature space. Con-\nversely, machine learning researchers have sought ﬂexibility in\nlocal6 non-parametric learners such as kernel machines with\na ﬁxed generic local-response kernel (such as the Gaussian\nkernel). Unfortunately, as argued at length by Bengio and\nMonperrus (2005); Bengio et al. (2006a); Bengio and LeCun\n(2007); Bengio (2009); Bengio et al. (2010), most of these\nalgorithms only exploit the principle of local generalization,\ni.e., the assumption that the target function (to be learned)\nis smooth enough, so they rely on examples to explicitly\nmap out the wrinkles of the target function. Generalization\n6. local in the sense that the value of the learned function at x depends\nmostly on training examples x(t)’s close to x\n4\nis mostly achieved by a form of local interpolation between\nneighboring training examples. Although smoothness can be\na useful assumption, it is insufﬁcient to deal with the curse\nof dimensionality, because the number of such wrinkles (ups\nand downs of the target function) may grow exponentially\nwith the number of relevant interacting factors, when the data\nare represented in raw input space. We advocate learning\nalgorithms that are ﬂexible and non-parametric7 but do not\nrely exclusively on the smoothness assumption. Instead, we\npropose to incorporate generic priors such as those enumerated\nabove into representation-learning algorithms. Smoothness-\nbased learners (such as kernel machines) and linear models\ncan still be useful on top of such learned representations. In\nfact, the combination of learning a representation and kernel\nmachine is equivalent to learning the kernel, i.e., the feature\nspace. Kernel machines are useful, but they depend on a prior\ndeﬁnition of a suitable similarity metric, or a feature space\nin which naive similarity metrics sufﬁce. We would like to\nuse the data, along with very generic priors, to discover those\nfeatures, or equivalently, a similarity function.\n3.3\nDistributed representations\nGood\nrepresentations\nare\nexpressive,\nmeaning\nthat\na\nreasonably-sized learned representation can capture a huge\nnumber of possible input conﬁgurations. A simple counting\nargument helps us to assess the expressiveness of a model pro-\nducing a representation: how many parameters does it require\ncompared to the number of input regions (or conﬁgurations) it\ncan distinguish? Learners of one-hot representations, such as\ntraditional clustering algorithms, Gaussian mixtures, nearest-\nneighbor algorithms, decision trees, or Gaussian SVMs all re-\nquire O(N) parameters (and/or O(N) examples) to distinguish\nO(N) input regions. One could naively believe that one cannot\ndo better. However, RBMs, sparse coding, auto-encoders or\nmulti-layer neural networks can all represent up to O(2k) input\nregions using only O(N) parameters (with k the number of\nnon-zero elements in a sparse representation, and k = N in\nnon-sparse RBMs and other dense representations). These are\nall distributed 8 or sparse9 representations. The generalization\nof clustering to distributed representations is multi-clustering,\nwhere either several clusterings take place in parallel or the\nsame clustering is applied on different parts of the input,\nsuch as in the very popular hierarchical feature extraction for\nobject recognition based on a histogram of cluster categories\ndetected in different patches of an image (Lazebnik et al.,\n2006; Coates and Ng, 2011a). The exponential gain from\ndistributed or sparse representations is discussed further in\nsection 3.2 (and Figure 3.2) of\nBengio (2009). It comes\nabout because each parameter (e.g. the parameters of one of\nthe units in a sparse code, or one of the units in a Restricted\n7. We understand non-parametric as including all learning algorithms\nwhose capacity can be increased appropriately as the amount of data and its\ncomplexity demands it, e.g. including mixture models and neural networks\nwhere the number of parameters is a data-selected hyper-parameter.\n8. Distributed representations: where k out of N representation elements\nor feature values can be independently varied, e.g., they are not mutually\nexclusive. Each concept is represented by having k features being turned on\nor active, while each feature is involved in representing many concepts.\n9. Sparse representations: distributed representations where only a few of\nthe elements can be varied at a time, i.e., k < N.\nBoltzmann Machine) can be re-used in many examples that are\nnot simply near neighbors of each other, whereas with local\ngeneralization, different regions in input space are basically\nassociated with their own private set of parameters, e.g., as\nin decision trees, nearest-neighbors, Gaussian SVMs, etc. In\na distributed representation, an exponentially large number of\npossible subsets of features or hidden units can be activated\nin response to a given input. In a single-layer model, each\nfeature is typically associated with a preferred input direction,\ncorresponding to a hyperplane in input space, and the code\nor representation associated with that input is precisely the\npattern of activation (which features respond to the input,\nand how much). This is in contrast with a non-distributed\nrepresentation such as the one learned by most clustering\nalgorithms, e.g., k-means, in which the representation of a\ngiven input vector is a one-hot code identifying which one of\na small number of cluster centroids best represents the input 10.\n3.4\nDepth and abstraction\nDepth is a key aspect to representation learning strategies we\nconsider in this paper. As we will discuss, deep architectures\nare often challenging to train effectively and this has been\nthe subject of much recent research and progress. However,\ndespite these challenges, they carry two signiﬁcant advantages\nthat motivate our long-term interest in discovering successful\ntraining strategies for deep architectures. These advantages\nare: (1) deep architectures promote the re-use of features, and\n(2) deep architectures can potentially lead to progressively\nmore abstract features at higher layers of representations\n(more removed from the data).\nFeature re-use. The notion of re-use, which explains the\npower of distributed representations, is also at the heart of the\ntheoretical advantages behind deep learning, i.e., constructing\nmultiple levels of representation or learning a hierarchy of\nfeatures. The depth of a circuit is the length of the longest\npath from an input node of the circuit to an output node of\nthe circuit. The crucial property of a deep circuit is that its\nnumber of paths, i.e., ways to re-use different parts, can grow\nexponentially with its depth. Formally, one can change the\ndepth of a given circuit by changing the deﬁnition of what\neach node can compute, but only by a constant factor. The\ntypical computations we allow in each node include: weighted\nsum, product, artiﬁcial neuron model (such as a monotone non-\nlinearity on top of an afﬁne transformation), computation of a\nkernel, or logic gates. Theoretical results clearly show families\nof functions where a deep representation can be exponentially\nmore efﬁcient than one that is insufﬁciently deep (H˚astad,\n1986; H˚astad and Goldmann, 1991; Bengio et al., 2006a;\nBengio and LeCun, 2007; Bengio and Delalleau, 2011). If\nthe same family of functions can be represented with fewer\n10. As discussed in (Bengio, 2009), things are only slightly better when\nallowing continuous-valued membership values, e.g., in ordinary mixture\nmodels (with separate parameters for each mixture component), but the\ndifference in representational power is still exponential (Montufar and Morton,\n2012). The situation may also seem better with a decision tree, where each\ngiven input is associated with a one-hot code over the tree leaves, which\ndeterministically selects associated ancestors (the path from root to node).\nUnfortunately, the number of different regions represented (equal to the\nnumber of leaves of the tree) still only grows linearly with the number of\nparameters used to specify it (Bengio and Delalleau, 2011).\n5\nparameters (or more precisely with a smaller VC-dimension),\nlearning theory would suggest that it can be learned with\nfewer examples, yielding improvements in both computational\nefﬁciency (less nodes to visit) and statistical efﬁciency (less\nparameters to learn, and re-use of these parameters over many\ndifferent kinds of inputs).\nAbstraction and invariance. Deep architectures can lead\nto abstract representations because more abstract concepts can\noften be constructed in terms of less abstract ones. In some\ncases, such as in the convolutional neural network (LeCun\net al., 1998b), we build this abstraction in explicitly via a\npooling mechanism (see section 11.2). More abstract concepts\nare generally invariant to most local changes of the input. That\nmakes the representations that capture these concepts generally\nhighly non-linear functions of the raw input. This is obviously\ntrue of categorical concepts, where more abstract representa-\ntions detect categories that cover more varied phenomena (e.g.\nlarger manifolds with more wrinkles) and thus they potentially\nhave greater predictive power. Abstraction can also appear in\nhigh-level continuous-valued attributes that are only sensitive\nto some very speciﬁc types of changes in the input. Learning\nthese sorts of invariant features has been a long-standing goal\nin pattern recognition.\n3.5\nDisentangling Factors of Variation\nBeyond being distributed and invariant, we would like our rep-\nresentations to disentangle the factors of variation. Different\nexplanatory factors of the data tend to change independently\nof each other in the input distribution, and only a few at a time\ntend to change when one considers a sequence of consecutive\nreal-world inputs.\nComplex data arise from the rich interaction of many\nsources. These factors interact in a complex web that can\ncomplicate AI-related tasks such as object classiﬁcation. For\nexample, an image is composed of the interaction between one\nor more light sources, the object shapes and the material prop-\nerties of the various surfaces present in the image. Shadows\nfrom objects in the scene can fall on each other in complex\npatterns, creating the illusion of object boundaries where there\nare none and dramatically effect the perceived object shape.\nHow can we cope with these complex interactions? How can\nwe disentangle the objects and their shadows? Ultimately,\nwe believe the approach we adopt for overcoming these\nchallenges must leverage the data itself, using vast quantities\nof unlabeled examples, to learn representations that separate\nthe various explanatory sources. Doing so should give rise to\na representation signiﬁcantly more robust to the complex and\nrichly structured variations extant in natural data sources for\nAI-related tasks.\nIt is important to distinguish between the related but distinct\ngoals of learning invariant features and learning to disentangle\nexplanatory factors. The central difference is the preservation\nof information. Invariant features, by deﬁnition, have reduced\nsensitivity in the direction of invariance. This is the goal of\nbuilding features that are insensitive to variation in the data\nthat are uninformative to the task at hand. Unfortunately, it\nis often difﬁcult to determine a priori which set of features\nand variations will ultimately be relevant to the task at hand.\nFurther, as is often the case in the context of deep learning\nmethods, the feature set being trained may be destined to\nbe used in multiple tasks that may have distinct subsets of\nrelevant features. Considerations such as these lead us to the\nconclusion that the most robust approach to feature learning\nis to disentangle as many factors as possible, discarding as\nlittle information about the data as is practical. If some form\nof dimensionality reduction is desirable, then we hypothesize\nthat the local directions of variation least represented in the\ntraining data should be ﬁrst to be pruned out (as in PCA,\nfor example, which does it globally instead of around each\nexample).\n3.6\nGood criteria for learning representations?\nOne of the challenges of representation learning that distin-\nguishes it from other machine learning tasks such as classi-\nﬁcation is the difﬁculty in establishing a clear objective, or\ntarget for training. In the case of classiﬁcation, the objective\nis (at least conceptually) obvious, we want to minimize the\nnumber of misclassiﬁcations on the training dataset. In the\ncase of representation learning, our objective is far-removed\nfrom the ultimate objective, which is typically learning a\nclassiﬁer or some other predictor. Our problem is reminiscent\nof the credit assignment problem encountered in reinforcement\nlearning. We have proposed that a good representation is one\nthat disentangles the underlying factors of variation, but how\ndo we translate that into appropriate training criteria? Is it even\nnecessary to do anything but maximize likelihood under a good\nmodel or can we introduce priors such as those enumerated\nabove (possibly data-dependent ones) that help the representa-\ntion better do this disentangling? This question remains clearly\nopen but is discussed in more detail in Sections 3.5 and 11.4.\n4\nBUILDING DEEP REPRESENTATIONS\nIn 2006, a breakthrough in feature learning and deep learning\nwas initiated by Geoff Hinton and quickly followed up in\nthe same year (Hinton et al., 2006; Bengio et al., 2007;\nRanzato et al., 2007), and soon after by Lee et al. (2008)\nand many more later. It has been extensively reviewed and\ndiscussed in Bengio (2009). A central idea, referred to as\ngreedy layerwise unsupervised pre-training, was to learn a\nhierarchy of features one level at a time, using unsupervised\nfeature learning to learn a new transformation at each level\nto be composed with the previously learned transformations;\nessentially, each iteration of unsupervised feature learning adds\none layer of weights to a deep neural network. Finally, the set\nof layers could be combined to initialize a deep supervised pre-\ndictor, such as a neural network classiﬁer, or a deep generative\nmodel, such as a Deep Boltzmann Machine (Salakhutdinov\nand Hinton, 2009).\nThis paper is mostly about feature learning algorithms\nthat can be used to form deep architectures. In particular, it\nwas empirically observed that layerwise stacking of feature\nextraction often yielded better representations, e.g., in terms\nof classiﬁcation error (Larochelle et al., 2009; Erhan et al.,\n2010b), quality of the samples generated by a probabilistic\nmodel (Salakhutdinov and Hinton, 2009) or in terms of the\ninvariance properties of the learned features (Goodfellow\n6\net al., 2009). Whereas this section focuses on the idea of\nstacking single-layer models, Section 10 follows up with a\ndiscussion on joint training of all the layers.\nAfter greedy layerwise unsuperivsed pre-training, the re-\nsulting deep features can be used either as input to a standard\nsupervised machine learning predictor (such as an SVM) or as\ninitialization for a deep supervised neural network (e.g., by ap-\npending a logistic regression layer or purely supervised layers\nof a multi-layer neural network). The layerwise procedure can\nalso be applied in a purely supervised setting, called the greedy\nlayerwise supervised pre-training (Bengio et al., 2007). For\nexample, after the ﬁrst one-hidden-layer MLP is trained, its\noutput layer is discarded and another one-hidden-layer MLP\ncan be stacked on top of it, etc. Although results reported\nin Bengio et al. (2007) were not as good as for unsupervised\npre-training, they were nonetheless better than without pre-\ntraining at all. Alternatively, the outputs of the previous layer\ncan be fed as extra inputs for the next layer (in addition to the\nraw input), as successfully done in Yu et al. (2010). Another\nvariant (Seide et al., 2011b) pre-trains in a supervised way all\nthe previously added layers at each step of the iteration, and\nin their experiments this discriminant variant yielded better\nresults than unsupervised pre-training.\nWhereas combining single layers into a supervised model\nis straightforward, it is less clear how layers pre-trained by\nunsupervised learning should be combined to form a better\nunsupervised model. We cover here some of the approaches\nto do so, but no clear winner emerges and much work has to\nbe done to validate existing proposals or improve them.\nThe ﬁrst proposal was to stack pre-trained RBMs into a\nDeep Belief Network (Hinton et al., 2006) or DBN, where\nthe top layer is interpreted as an RBM and the lower layers\nas a directed sigmoid belief network. However, it is not clear\nhow to approximate maximum likelihood training to further\noptimize this generative model. One option is the wake-sleep\nalgorithm (Hinton et al., 2006) but more work should be done\nto assess the efﬁciency of this procedure in terms of improving\nthe generative model.\nThe second approach that has been put forward is to\ncombine the RBM parameters into a Deep Boltzmann Machine\n(DBM), by basically halving the RBM weights to obtain\nthe DBM weights (Salakhutdinov and Hinton, 2009). The\nDBM can then be trained by approximate maximum likelihood\nas discussed in more details later (Section 10.2). This joint\ntraining has brought substantial improvements, both in terms\nof likelihood and in terms of classiﬁcation performance of\nthe resulting deep feature learner (Salakhutdinov and Hinton,\n2009).\nAnother early approach was to stack RBMs or auto-\nencoders into a deep auto-encoder (Hinton and Salakhutdi-\nnov, 2006). If we have a series of encoder-decoder pairs\n(f (i)(·), g(i)(·)), then the overall encoder is the composition of\nthe encoders, f (N)(. . . f (2)(f (1)(·))), and the overall decoder\nis its “transpose” (often with transposed weight matrices as\nwell), g(1)(g(2)(. . . f (N)(·))). The deep auto-encoder (or its\nregularized version, as discussed in Section 7.2) can then\nbe jointly trained, with all the parameters optimized with\nrespect to a global reconstruction error criterion. More work\non this avenue clearly needs to be done, and it was probably\navoided by fear of the challenges in training deep feedfor-\nward networks, discussed in the Section 10 along with very\nencouraging recent results.\nYet another recently proposed approach to training deep\narchitectures (Ngiam et al., 2011) is to consider the iterative\nconstruction of a free energy function (i.e., with no explicit\nlatent variables, except possibly for a top-level layer of hidden\nunits) for a deep architecture as the composition of transforma-\ntions associated with lower layers, followed by top-level hid-\nden units. The question is then how to train a model deﬁned by\nan arbitrary parametrized (free) energy function. Ngiam et al.\n(2011) have used Hybrid Monte Carlo (Neal, 1993), but other\noptions include contrastive divergence (Hinton, 1999; Hinton\net al., 2006), score matching (Hyv¨arinen, 2005; Hyv¨arinen,\n2008), denoising score matching (Kingma and LeCun, 2010;\nVincent, 2011), ratio-matching (Hyv¨arinen, 2007) and noise-\ncontrastive estimation (Gutmann and Hyvarinen, 2010).\n5\nSINGLE-LAYER LEARNING MODULES\nWithin the community of researchers interested in representa-\ntion learning, there has developed two broad parallel lines of\ninquiry: one rooted in probabilistic graphical models and one\nrooted in neural networks. Fundamentally, the difference be-\ntween these two paradigms is whether the layered architecture\nof a deep learning model is to be interpreted as describing a\nprobabilistic graphical model or as describing a computation\ngraph. In short, are hidden units considered latent random\nvariables or as computational nodes?\nTo date, the dichotomy between these two paradigms has\nremained in the background, perhaps because they appear to\nhave more characteristics in common than separating them.\nWe suggest that this is likely a function of the fact that\nmuch recent progress in both of these areas has focused\non single-layer greedy learning modules and the similarities\nbetween the types of single-layer models that have been\nexplored: mainly, the restricted Boltzmann machine (RBM)\non the probabilistic side, and the auto-encoder variants on the\nneural network side. Indeed, as shown by one of us (Vincent,\n2011) and others (Swersky et al., 2011), in the case of\nthe restricted Boltzmann machine, training the model via\nan inductive principle known as score matching (Hyv¨arinen,\n2005) (to be discussed in sec. 6.4.3) is essentially identical\nto applying a regularized reconstruction objective to an auto-\nencoder. Another strong link between pairs of models on\nboth sides of this divide is when the computational graph for\ncomputing representation in the neural network model corre-\nsponds exactly to the computational graph that corresponds to\ninference in the probabilistic model, and this happens to also\ncorrespond to the structure of graphical model itself (e.g., as\nin the RBM).\nThe connection between these two paradigms becomes more\ntenuous when we consider deeper models where, in the case\nof a probabilistic model, exact inference typically becomes\nintractable. In the case of deep models, the computational\ngraph diverges from the structure of the model. For example,\nin the case of a deep Boltzmann machine, unrolling variational\n(approximate) inference into a computational graph results in\n7\na recurrent graph structure. We have performed preliminary\nexploration (Savard, 2011) of deterministic variants of deep\nauto-encoders whose computational graph is similar to that of\na deep Boltzmann machine (in fact very close to the mean-\nﬁeld variational approximations associated with the Boltzmann\nmachine), and that is one interesting intermediate point to ex-\nplore (between the deterministic approaches and the graphical\nmodel approaches).\nIn the next few sections we will review the major de-\nvelopments in single-layer training modules used to support\nfeature learning and particularly deep learning. We divide these\nsections between (Section 6) the probabilistic models, with\ninference and training schemes that directly parametrize the\ngenerative – or decoding – pathway and (Section 7) the typ-\nically neural network-based models that directly parametrize\nthe encoding pathway. Interestingly, some models, like Pre-\ndictive Sparse Decomposition (PSD) (Kavukcuoglu et al.,\n2008) inherit both properties, and will also be discussed (Sec-\ntion 7.2.4). We then present a different view of representation\nlearning, based on the associated geometry and the manifold\nassumption, in Section 8.\nFirst, let us consider an unsupervised single-layer represen-\ntation learning algorithm spaning all three views: probabilistic,\nauto-encoder, and manifold learning.\nPrincipal Components Analysis\nWe will use probably the oldest feature extraction algorithm,\nprincipal components analysis (PCA), to illustrate the proba-\nbilistic, auto-encoder and manifold views of representation-\nlearning. PCA learns a linear transformation h = f(x) =\nW T x + b of input x ∈Rdx, where the columns of dx × dh\nmatrix W form an orthogonal basis for the dh orthogonal\ndirections of greatest variance in the training data. The result\nis dh features (the components of representation h) that\nare decorrelated. The three interpretations of PCA are the\nfollowing: a) it is related to probabilistic models (Section 6)\nsuch as probabilistic PCA, factor analysis and the traditional\nmultivariate Gaussian distribution (the leading eigenvectors of\nthe covariance matrix are the principal components); b) the\nrepresentation it learns is essentially the same as that learned\nby a basic linear auto-encoder (Section 7.2); and c) it can be\nviewed as a simple linear form of linear manifold learning\n(Section 8), i.e., characterizing a lower-dimensional region\nin input space near which the data density is peaked. Thus,\nPCA may be in the back of the reader’s mind as a common\nthread relating these various viewpoints. Unfortunately the\nexpressive power of linear features is very limited: they cannot\nbe stacked to form deeper, more abstract representations since\nthe composition of linear operations yields another linear\noperation. Here, we focus on recent algorithms that have\nbeen developed to extract non-linear features, which can be\nstacked in the construction of deep networks, although some\nauthors simply insert a non-linearity between learned single-\nlayer linear projections (Le et al., 2011c; Chen et al., 2012).\nAnother rich family of feature extraction techniques that this\nreview does not cover in any detail due to space constraints is\nIndependent Component Analysis or ICA (Jutten and Herault,\n1991; Bell and Sejnowski, 1997). Instead, we refer the reader\nto Hyv¨arinen et al. (2001a); Hyv¨arinen et al. (2009). Note that,\nwhile in the simplest case (complete, noise-free) ICA yields\nlinear features, in the more general case it can be equated\nwith a linear generative model with non-Gaussian independent\nlatent variables, similar to sparse coding (section 6.1.1), which\nresult in non-linear features. Therefore, ICA and its variants\nlike Independent and Topographic ICA (Hyv¨arinen et al.,\n2001b) can and have been used to build deep networks (Le\net al., 2010, 2011c): see section 11.2. The notion of obtaining\nindependent components also appears similar to our stated\ngoal of disentangling underlying explanatory factors through\ndeep networks. However, for complex real-world distributions,\nit is doubtful that the relationship between truly independent\nunderlying factors and the observed high-dimensional data can\nbe adequately characterized by a linear transformation.\n6\nPROBABILISTIC MODELS\nFrom the probabilistic modeling perspective, the question of\nfeature learning can be interpreted as an attempt to recover\na parsimonious set of latent random variables that describe\na distribution over the observed data. We can express as\np(x, h) a probabilistic model over the joint space of the latent\nvariables, h, and observed data or visible variables x. Feature\nvalues are conceived as the result of an inference process to\ndetermine the probability distribution of the latent variables\ngiven the data, i.e. p(h | x), often referred to as the posterior\nprobability. Learning is conceived in term of estimating a set\nof model parameters that (locally) maximizes the regularized\nlikelihood of the training data. The probabilistic graphical\nmodel formalism gives us two possible modeling paradigms\nin which we can consider the question of inferring latent\nvariables, directed and undirected graphical models, which\ndiffer in their parametrization of the joint distribution p(x, h),\nyielding major impact on the nature and computational costs\nof both inference and learning.\n6.1\nDirected Graphical Models\nDirected latent factor models separately parametrize the con-\nditional likelihood p(x | h) and the prior p(h) to construct\nthe joint distribution, p(x, h) = p(x | h)p(h). Examples of\nthis decomposition include: Principal Components Analysis\n(PCA) (Roweis, 1997; Tipping and Bishop, 1999), sparse\ncoding (Olshausen and Field, 1996), sigmoid belief net-\nworks (Neal, 1992) and the newly introduced spike-and-slab\nsparse coding model (Goodfellow et al., 2011).\n6.1.1\nExplaining Away\nDirected models often leads to one important property: ex-\nplaining away, i.e., a priori independent causes of an event can\nbecome non-independent given the observation of the event.\nLatent factor models can generally be interpreted as latent\ncause models, where the h activations cause the observed x.\nThis renders the a priori independent h to be non-independent.\nAs a consequence, recovering the posterior distribution of h,\np(h | x) (which we use as a basis for feature representation),\nis often computationally challenging and can be entirely\nintractable, especially when h is discrete.\nA classic example that illustrates the phenomenon is to\nimagine you are on vacation away from home and you receive\na phone call from the security system company, telling you that\n8\nthe alarm has been activated. You begin worrying your home\nhas been burglarized, but then you hear on the radio that a\nminor earthquake has been reported in the area of your home.\nIf you happen to know from prior experience that earthquakes\nsometimes cause your home alarm system to activate, then\nsuddenly you relax, conﬁdent that your home has very likely\nnot been burglarized.\nThe example illustrates how the alarm activation rendered\ntwo otherwise entirely independent causes, burglarized and\nearthquake, to become dependent – in this case, the depen-\ndency is one of mutual exclusivity. Since both burglarized\nand earthquake are very rare events and both can cause\nalarm activation, the observation of one explains away the\nother. Despite the computational obstacles we face when\nattempting to recover the posterior over h, explaining away\npromises to provide a parsimonious p(h | x), which can\nbe an extremely useful characteristic of a feature encoding\nscheme. If one thinks of a representation as being composed\nof various feature detectors and estimated attributes of the\nobserved input, it is useful to allow the different features to\ncompete and collaborate with each other to explain the input.\nThis is naturally achieved with directed graphical models, but\ncan also be achieved with undirected models (see Section 6.2)\nsuch as Boltzmann machines if there are lateral connections\nbetween the corresponding units or corresponding interaction\nterms in the energy function that deﬁnes the probability model.\nProbabilistic Interpretation of PCA. PCA can be given\na natural probabilistic interpretation (Roweis, 1997; Tipping\nand Bishop, 1999) as factor analysis:\np(h)\n=\nN(h; 0, σ2\nhI)\np(x | h)\n=\nN(x; Wh + µx, σ2\nxI),\n(1)\nwhere x ∈Rdx, h ∈Rdh, N(v; µ, Σ) is the multivariate\nnormal density of v with mean µ and covariance Σ, and\ncolumns of W span the same space as leading dh principal\ncomponents, but are not constrained to be orthonormal.\nSparse Coding. Like PCA, sparse coding has both a proba-\nbilistic and non-probabilistic interpretation. Sparse coding also\nrelates a latent representation h (either a vector of random\nvariables or a feature vector, depending on the interpretation)\nto the data x through a linear mapping W, which we refer\nto as the dictionary. The difference between sparse coding\nand PCA is that sparse coding includes a penalty to ensure a\nsparse activation of h is used to encode each input x. From\na non-probabilistic perspective, sparse coding can be seen as\nrecovering the code or feature vector associated with a new\ninput x via:\nh∗= f(x) = argmin\nh\n∥x −Wh∥2\n2 + λ∥h∥1,\n(2)\nLearning the dictionary W can be accomplished by optimizing\nthe following training criterion with respect to W:\nJSC =\nX\nt\n∥x(t) −Wh∗(t)∥2\n2,\n(3)\nwhere x(t) is the t-th example and h∗(t) is the corresponding\nsparse code determined by Eq. 2. W is usually constrained to\nhave unit-norm columns (because one can arbitrarily exchange\nscaling of column i with scaling of h(t)\ni , such a constraint is\nnecessary for the L1 penalty to have any effect).\nThe probabilistic interpretation of sparse coding differs from\nthat of PCA, in that instead of a Gaussian prior on the latent\nrandom variable h, we use a sparsity inducing Laplace prior\n(corresponding to an L1 penalty):\np(h)\n=\ndh\nY\ni\nλ\n2 exp(−λ|hi|)\np(x | h)\n=\nN(x; Wh + µx, σ2\nxI).\n(4)\nIn the case of sparse coding, because we will seek a sparse\nrepresentation (i.e., one with many features set to exactly zero),\nwe will be interested in recovering the MAP (maximum a\nposteriori value of h: i.e. h∗= argmaxh p(h | x) rather than\nits expected value E [h|x]. Under this interpretation, dictionary\nlearning proceeds as maximizing the likelihood of the data\ngiven these MAP values of h∗: argmaxW\nQ\nt p(x(t) | h∗(t))\nsubject to the norm constraint on W. Note that this pa-\nrameter learning scheme, subject to the MAP values of\nthe latent h, is not standard practice in the probabilistic\ngraphical model literature. Typically the likelihood of the\ndata p(x) = P\nh p(x | h)p(h) is maximized directly. In the\npresence of latent variables, expectation maximization is em-\nployed where the parameters are optimized with respect to\nthe marginal likelihood, i.e., summing or integrating the joint\nlog-likelihood over the all values of the latent variables under\ntheir posterior P(h | x), rather than considering only the\nsingle MAP value of h. The theoretical properties of this\nform of parameter learning are not yet well understood but\nseem to work well in practice (e.g. k-Means vs Gaussian\nmixture models and Viterbi training for HMMs). Note also\nthat the interpretation of sparse coding as a MAP estimation\ncan be questioned (Gribonval, 2011), because even though the\ninterpretation of the L1 penalty as a log-prior is a possible\ninterpretation, there can be other Bayesian interpretations\ncompatible with the training criterion.\nSparse coding is an excellent example of the power of\nexplaining away. Even with a very overcomplete dictionary11,\nthe MAP inference process used in sparse coding to ﬁnd h∗\ncan pick out the most appropriate bases and zero the others,\ndespite them having a high degree of correlation with the input.\nThis property arises naturally in directed graphical models\nsuch as sparse coding and is entirely owing to the explaining\naway effect. It is not seen in commonly used undirected prob-\nabilistic models such as the RBM, nor is it seen in parametric\nfeature encoding methods such as auto-encoders. The trade-\noff is that, compared to methods such as RBMs and auto-\nencoders, inference in sparse coding involves an extra inner-\nloop of optimization to ﬁnd h∗with a corresponding increase\nin the computational cost of feature extraction. Compared\nto auto-encoders and RBMs, the code in sparse coding is a\nfree variable for each example, and in that sense the implicit\nencoder is non-parametric.\nOne might expect that the parsimony of the sparse coding\nrepresentation and its explaining away effect would be advan-\ntageous and indeed it seems to be the case. Coates and Ng\n(2011a) demonstrated on the CIFAR-10 object classiﬁcation\ntask (Krizhevsky and Hinton, 2009) with a patch-base feature\nextraction pipeline, that in the regime with few (< 1000)\n11. Overcomplete: with more dimensions of h than dimensions of x.\n9\nlabeled training examples per class, the sparse coding repre-\nsentation signiﬁcantly outperformed other highly competitive\nencoding schemes. Possibly because of these properties, and\nbecause of the very computationally efﬁcient algorithms that\nhave been proposed for it (in comparison with the general\ncase of inference in the presence of explaining away), sparse\ncoding enjoys considerable popularity as a feature learning and\nencoding paradigm. There are numerous examples of its suc-\ncessful application as a feature representation scheme, includ-\ning natural image modeling (Raina et al., 2007; Kavukcuoglu\net al., 2008; Coates and Ng, 2011a; Yu et al., 2011), audio\nclassiﬁcation (Grosse et al., 2007), NLP (Bagnell and Bradley,\n2009), as well as being a very successful model of the early\nvisual cortex (Olshausen and Field, 1996). Sparsity criteria can\nalso be generalized successfully to yield groups of features that\nprefer to all be zero, but if one or a few of them are active then\nthe penalty for activating others in the group is small. Different\ngroup sparsity patterns can incorporate different forms of prior\nknowledge (Kavukcuoglu et al., 2009; Jenatton et al., 2009;\nBach et al., 2011; Gregor et al., 2011).\nSpike-and-Slab Sparse Coding. Spike-and-slab sparse cod-\ning (S3C) is one example of a promising variation on sparse\ncoding for feature learning (Goodfellow et al., 2012). The\nS3C model possesses a set of latent binary spike variables\ntogether with a a set of latent real-valued slab variables. The\nactivation of the spike variables dictates the sparsity pattern.\nS3C has been applied to the CIFAR-10 and CIFAR-100 object\nclassiﬁcation tasks (Krizhevsky and Hinton, 2009), and shows\nthe same pattern as sparse coding of superior performance in\nthe regime of relatively few (< 1000) labeled examples per\nclass (Goodfellow et al., 2012). In fact, in both the CIFAR-\n100 dataset (with 500 examples per class) and the CIFAR-\n10 dataset (when the number of examples is reduced to a\nsimilar range), the S3C representation actually outperforms\nsparse coding representations. This advantage was revealed\nclearly with S3C winning the NIPS’2011 Transfer Learning\nChallenge (Goodfellow et al., 2011).\n6.2\nUndirected Graphical Models\nUndirected graphical models, also called Markov random\nﬁelds (MRFs), parametrize the joint p(x, h) through a product\nof unnormalized non-negative clique potentials:\np(x, h) = 1\nZθ\nY\ni\nψi(x)\nY\nj\nηj(h)\nY\nk\nνk(x, h)\n(5)\nwhere ψi(x), ηj(h) and νk(x, h) are the clique potentials de-\nscribing the interactions between the visible elements, between\nthe hidden variables, and those interaction between the visible\nand hidden variables respectively. The partition function Zθ\nensures that the distribution is normalized. Within the context\nof unsupervised feature learning, we generally see a particular\nform of Markov random ﬁeld called a Boltzmann distribution\nwith clique potentials constrained to be positive:\np(x, h) = 1\nZθ exp (−Eθ(x, h)) ,\n(6)\nwhere Eθ(x, h) is the energy function and contains the inter-\nactions described by the MRF clique potentials and θ are the\nmodel parameters that characterize these interactions.\nThe Boltzmann machine was originally deﬁned as a network\nof symmetrically-coupled binary random variables or units.\nThese stochastic units can be divided into two groups: (1) the\nvisible units x ∈{0, 1}dx that represent the data, and (2) the\nhidden or latent units h ∈{0, 1}dh that mediate dependencies\nbetween the visible units through their mutual interactions. The\npattern of interaction is speciﬁed through the energy function:\nEBM\nθ\n(x, h) = −1\n2xT Ux −1\n2hT V h −xT Wh −bT x −dT h,\n(7)\nwhere θ = {U, V, W, b, d} are the model parameters which\nrespectively encode the visible-to-visible interactions, the\nhidden-to-hidden interactions, the visible-to-hidden interac-\ntions, the visible self-connections, and the hidden self-\nconnections (called biases). To avoid over-parametrization, the\ndiagonals of U and V are set to zero.\nThe Boltzmann machine energy function speciﬁes the prob-\nability distribution over [x, h], via the Boltzmann distribution,\nEq. 6, with the partition function Zθ given by:\nZθ =\nx1=1\nX\nx1=0\n· · ·\nxdx =1\nX\nxdx =0\nh1=1\nX\nh1=0\n· · ·\nhdh =1\nX\nhdh =0\nexp\n\u0010\n−EBM\nθ\n(x, h; θ)\n\u0011\n.\n(8)\nThis joint probability distribution gives rise to the set of\nconditional distributions of the form:\nP(hi | x, h\\i) = sigmoid\n\nX\nj\nWjixj +\nX\ni′̸=i\nVii′hi′ + di\n\n\n(9)\nP(xj | h, x\\j) = sigmoid\n\nX\ni\nWjixj +\nX\nj′̸=j\nUjj′xj′ + bj\n\n.\n(10)\nIn general, inference in the Boltzmann machine is intractable.\nFor example, computing the conditional probability of hi given\nthe visibles, P(hi | x), requires marginalizing over the rest of\nthe hiddens, which implies evaluating a sum with 2dh−1 terms:\nP(hi | x) =\nh1=1\nX\nh1=0\n· · ·\nhi−1=1\nX\nhi−1=0\nhi+1=1\nX\nhi+1=0\n· · ·\nhdh =1\nX\nhdh =0\nP(h | x)\n(11)\nHowever with some judicious choices in the pattern of inter-\nactions between the visible and hidden units, more tractable\nsubsets of the model family are possible, as we discuss next.\nRestricted Boltzmann Machines (RBMs). The RBM\nis likely the most popular subclass of Boltzmann ma-\nchine (Smolensky, 1986). It is deﬁned by restricting the\ninteractions in the Boltzmann energy function, in Eq. 7, to\nonly those between h and x, i.e. ERBM\nθ\nis EBM\nθ\nwith U = 0\nand V = 0. As such, the RBM can be said to form a bipartite\ngraph with the visibles and the hiddens forming two layers\nof vertices in the graph (and no connection between units of\nthe same layer). With this restriction, the RBM possesses the\nuseful property that the conditional distribution over the hidden\nunits factorizes given the visibles:\nP(h | x) =\nQ\ni P(hi | x)\nP(hi = 1 | x) =\nsigmoid\n\u0010P\nj Wjixj + di\n\u0011\n.\n(12)\nLikewise, the conditional distribution over the visible units\ngiven the hiddens also factorizes:\nP(x | h) =\nY\nj\nP(xj | h)\nP(xj = 1 | h) = sigmoid\n X\ni\nWjihi + bj\n!\n.\n(13)\n10\nThis makes inferences readily tractable in RBMs. For exam-\nple, the RBM feature representation is taken to be the set of\nposterior marginals P(hi | x), which, given the conditional\nindependence described in Eq. 12, are immediately available.\nNote that this is in stark contrast to the situation with popular\ndirected graphical models for unsupervised feature extraction,\nwhere computing the posterior probability is intractable.\nImportantly, the tractability of the RBM does not extend\nto its partition function, which still involves summing an\nexponential number of terms. It does imply however that we\ncan limit the number of terms to min{2dx, 2dh}. Usually this is\nstill an unmanageable number of terms and therefore we must\nresort to approximate methods to deal with its estimation.\nIt is difﬁcult to overstate the impact the RBM has had to\nthe ﬁelds of unsupervised feature learning and deep learning.\nIt has been used in a truly impressive variety of applica-\ntions, including fMRI image classiﬁcation (Schmah et al.,\n2009), motion and spatial transformations (Taylor and Hinton,\n2009; Memisevic and Hinton, 2010), collaborative ﬁltering\n(Salakhutdinov et al., 2007) and natural image modeling\n(Ranzato and Hinton, 2010; Courville et al., 2011b).\n6.3\nGeneralizations of the RBM to Real-valued data\nImportant progress has been made in the last few years in\ndeﬁning generalizations of the RBM that better capture real-\nvalued data, in particular real-valued image data, by better\nmodeling the conditional covariance of the input pixels. The\nstandard RBM, as discussed above, is deﬁned with both binary\nvisible variables v ∈{0, 1} and binary latent variables h ∈\n{0, 1}. The tractability of inference and learning in the RBM\nhas inspired many authors to extend it, via modiﬁcations of its\nenergy function, to model other kinds of data distributions. In\nparticular, there has been multiple attempts to develop RBM-\ntype models of real-valued data, where x ∈Rdx. The most\nstraightforward approach to modeling real-valued observations\nwithin the RBM framework is the so-called Gaussian RBM\n(GRBM) where the only change in the RBM energy function\nis to the visible units biases, by adding a bias term that is\nquadratic in the visible units x. While it probably remains\nthe most popular way to model real-valued data within the\nRBM framework, Ranzato and Hinton (2010) suggest that the\nGRBM has proved to be a somewhat unsatisfactory model of\nnatural images. The trained features typically do not represent\nsharp edges that occur at object boundaries and lead to latent\nrepresentations that are not particularly useful features for\nclassiﬁcation tasks. Ranzato and Hinton (2010) argue that\nthe failure of the GRBM to adequately capture the statistical\nstructure of natural images stems from the exclusive use of the\nmodel capacity to capture the conditional mean at the expense\nof the conditional covariance. Natural images, they argue, are\nchieﬂy characterized by the covariance of the pixel values,\nnot by their absolute values. This point is supported by the\ncommon use of preprocessing methods that standardize the\nglobal scaling of the pixel values across images in a dataset\nor across the pixel values within each image.\nThese kinds of concerns about the ability of the GRBM\nto model natural image data has lead to the development of\nalternative RBM-based models that each attempt to take on this\nobjective of better modeling non-diagonal conditional covari-\nances. (Ranzato and Hinton, 2010) introduced the mean and\ncovariance RBM (mcRBM). Like the GRBM, the mcRBM is a\n2-layer Boltzmann machine that explicitly models the visible\nunits as Gaussian distributed quantities. However unlike the\nGRBM, the mcRBM uses its hidden layer to independently\nparametrize both the mean and covariance of the data through\ntwo sets of hidden units. The mcRBM is a combination of the\ncovariance RBM (cRBM) (Ranzato et al., 2010a), that models\nthe conditional covariance, with the GRBM that captures the\nconditional mean. While the GRBM has shown considerable\npotential as the basis of a highly successful phoneme recogni-\ntion system (Dahl et al., 2010), it seems that due to difﬁculties\nin training the mcRBM, the model has been largely superseded\nby the mPoT model. The mPoT model (mean-product of\nStudent’s T-distributions model)\n(Ranzato et al., 2010b) is\na combination of the GRBM and the product of Student’s T-\ndistributions model (Welling et al., 2003). It is an energy-based\nmodel where the conditional distribution over the visible units\nconditioned on the hidden variables is a multivariate Gaussian\n(non-diagonal covariance) and the complementary conditional\ndistribution over the hidden variables given the visibles are a\nset of independent Gamma distributions.\nThe PoT model has recently been generalized to the mPoT\nmodel (Ranzato et al., 2010b) to include nonzero Gaussian\nmeans by the addition of GRBM-like hidden units, similarly to\nhow the mcRBM generalizes the cRBM. The mPoT model has\nbeen used to synthesize large-scale natural images (Ranzato\net al., 2010b) that show large-scale features and shadowing\nstructure. It has been used to model natural textures (Kivinen\nand Williams, 2012) in a tiled-convolution conﬁguration (see\nsection 11.2).\nAnother recently introduced RBM-based model with the\nobjective of having the hidden units encode both the mean\nand covariance information is the spike-and-slab Restricted\nBoltzmann Machine (ssRBM) (Courville et al., 2011a,b).\nThe ssRBM is deﬁned as having both a real-valued “slab”\nvariable and a binary “spike” variable associated with each\nunit in the hidden layer. The ssRBM has been demonstrated\nas a feature learning and extraction scheme in the context\nof CIFAR-10 object classiﬁcation (Krizhevsky and Hinton,\n2009) from natural images and has performed well in the\nrole (Courville et al., 2011a,b). When trained convolutionally\n(see Section 11.2) on full CIFAR-10 natural images, the model\ndemonstrated the ability to generate natural image samples\nthat seem to capture the broad statistical structure of natural\nimages better than previous parametric generative models, as\nillustrated with the samples of Figure 2.\nThe mcRBM, mPoT and ssRBM each set out to model\nreal-valued data such that the hidden units encode not only\nthe conditional mean of the data but also its conditional\ncovariance. Other than differences in the training schemes, the\nmost signiﬁcant difference between these models is how they\nencode their conditional covariance. While the mcRBM and\nthe mPoT use the activation of the hidden units to enforce con-\nstraints on the covariance of x, the ssRBM uses the hidden unit\nto pinch the precision matrix along the direction speciﬁed by\nthe corresponding weight vector. These two ways of modeling\n11\nFig. 2.\n(Top) Samples from convolutionally trained µ-ssRBM\nfrom Courville et al. (2011b). (Bottom) Images in CIFAR-10 train-\ning set closest (L2 distance with contrast normalized training\nimages) to corresponding model samples on top. The model\ndoes not appear to be overﬁtting particular training examples.\nconditional covariance diverge when the dimensionality of the\nhidden layer is signiﬁcantly different from that of the input.\nIn the overcomplete setting, sparse activation with the ssRBM\nparametrization permits variance only in the select directions\nof the sparsely activated hidden units. This is a property the\nssRBM shares with sparse coding models (Olshausen and\nField, 1996; Grosse et al., 2007). On the other hand, in\nthe case of the mPoT or mcRBM, an overcomplete set of\nconstraints on the covariance implies that capturing arbitrary\ncovariance along a particular direction of the input requires\ndecreasing potentially all constraints with positive projection\nin that direction. This perspective would suggest that the mPoT\nand mcRBM do not appear to be well suited to provide a sparse\nrepresentation in the overcomplete setting.\n6.4\nRBM parameter estimation\nMany of the RBM training methods we discuss here are ap-\nplicable to more general undirected graphical models, but are\nparticularly practical in the RBM setting. Freund and Haussler\n(1994) proposed a learning algorithm for harmoniums (RBMs)\nbased on projection pursuit. Contrastive Divergence (Hinton,\n1999; Hinton et al., 2006) has been used most often to train\nRBMs, and many recent papers use Stochastic Maximum\nLikelihood (Younes, 1999; Tieleman, 2008).\nAs discussed in Sec. 6.1, in training probabilistic models\nparameters are typically adapted in order to maximize the like-\nlihood of the training data (or equivalently the log-likelihood,\nor its penalized version, which adds a regularization term).\nWith T training examples, the log likelihood is given by:\nT\nX\nt=1\nlog P(x(t); θ) =\nT\nX\nt=1\nlog\nX\nh∈{0,1}dh\nP(x(t), h; θ).\n(14)\nGradient-based optimization requires its gradient, which for\nBoltzmann machines, is given by:\n∂\n∂θi\nT\nX\nt=1\nlog p(x(t))\n=\n−\nT\nX\nt=1\nEp(h|x(t))\n\u0014 ∂\n∂θi EBM\nθ\n(x(t), h)\n\u0015\n+\nT\nX\nt=1\nEp(x,h)\n\u0014 ∂\n∂θi EBM\nθ\n(x, h)\n\u0015\n,\n(15)\nwhere we have the expectations with respect to p(h(t) | x(t))\nin the “clamped” condition (also called the positive phase),\nand over the full joint p(x, h) in the “unclamped” condition\n(also called the negative phase). Intuitively, the gradient acts\nto locally move the model distribution (the negative phase\ndistribution) toward the data distribution (positive phase dis-\ntribution), by pushing down the energy of (h, x(t)) pairs (for\nh ∼P(h|x(t))) while pushing up the energy of (h, x) pairs\n(for (h, x) ∼P(h, x)) until the two forces are in equilibrium,\nat which point the sufﬁcient statistics (gradient of the energy\nfunction) have equal expectations with x sampled from the\ntraining distribution or with x sampled from the model.\nThe RBM conditional independence properties imply that\nthe expectation in the positive phase of Eq. 15 is tractable.\nThe negative phase term – arising from the partition func-\ntion’s contribution to the log-likelihood gradient – is more\nproblematic because the computation of the expectation over\nthe joint is not tractable. The various ways of dealing with the\npartition function’s contribution to the gradient have brought\nabout a number of different training algorithms, many trying\nto approximate the log-likelihood gradient.\nTo approximate the expectation of the joint distribution in\nthe negative phase contribution to the gradient, it is natural to\nagain consider exploiting the conditional independence of the\nRBM in order to specify a Monte Carlo approximation of the\nexpectation over the joint:\nEp(x,h)\n\u0014 ∂\n∂θi ERBM\nθ\n(x, h)\n\u0015\n≈1\nL\nL\nX\nl=1\n∂\n∂θi ERBM\nθ\n(˜x(l), ˜h(l)),\n(16)\nwith the samples (˜x(l), ˜h(l)) drawn by a block Gibbs MCMC\n(Markov chain Monte Carlo) sampling procedure:\n˜x(l)\n∼\nP(x | ˜h(l−1))\n˜h(l)\n∼\nP(h | ˜x(l)).\nNaively, for each gradient update step, one would start a\nGibbs sampling chain, wait until the chain converges to the\nequilibrium distribution and then draw a sufﬁcient number of\nsamples to approximate the expected gradient with respect\nto the model (joint) distribution in Eq. 16. Then restart the\nprocess for the next step of approximate gradient ascent on\nthe log-likelihood. This procedure has the obvious ﬂaw that\nwaiting for the Gibbs chain to “burn-in” and reach equilibrium\nanew for each gradient update cannot form the basis of a prac-\ntical training algorithm. Contrastive Divergence (Hinton, 1999;\nHinton et al., 2006), Stochastic Maximum Likelihood (Younes,\n1999; Tieleman, 2008) and fast-weights persistent contrastive\ndivergence or FPCD (Tieleman and Hinton, 2009) are all ways\nto avoid or reduce the need for burn-in.\n6.4.1\nContrastive Divergence\nContrastive divergence (CD) estimation (Hinton, 1999; Hinton\net al., 2006) estimates the negative phase expectation (Eq. 15)\nwith a very short Gibbs chain (often just one step) initialized\n12\nat the training data used in the positive phase. This reduces\nthe variance of the gradient estimator and still moves in a\ndirection that pulls the negative chain samples towards the as-\nsociated positive chain samples. Much has been written about\nthe properties and alternative interpretations of CD and its\nsimilarity to auto-encoder training, e.g. Carreira-Perpi˜nan and\nHinton (2005); Yuille (2005); Bengio and Delalleau (2009);\nSutskever and Tieleman (2010).\n6.4.2\nStochastic Maximum Likelihood\nThe Stochastic Maximum Likelihood (SML) algorithm (also\nknown as persistent contrastive divergence or PCD) (Younes,\n1999; Tieleman, 2008) is an alternative way to sidestep an\nextended burn-in of the negative phase Gibbs sampler. At each\ngradient update, rather than initializing the Gibbs chain at the\npositive phase sample as in CD, SML initializes the chain at\nthe last state of the chain used for the previous update. In\nother words, SML uses a continually running Gibbs chain (or\noften a number of Gibbs chains run in parallel) from which\nsamples are drawn to estimate the negative phase expectation.\nDespite the model parameters changing between updates, these\nchanges should be small enough that only a few steps of Gibbs\n(in practice, often one step is used) are required to maintain\nsamples from the equilibrium distribution of the Gibbs chain,\ni.e. the model distribution.\nA troublesome aspect of SML is that it relies on the Gibbs\nchain to mix well (especially between modes) for learning to\nsucceed. Typically, as learning progresses and the weights of\nthe RBM grow, the ergodicity of the Gibbs sample begins to\nbreak down12. If the learning rate ϵ associated with gradient\nascent θ ←θ + ϵˆg (with E[ˆg] ≈∂log pθ(x)\n∂θ\n) is not reduced\nto compensate, then the Gibbs sampler will diverge from the\nmodel distribution and learning will fail. Desjardins et al.\n(2010); Cho et al. (2010); Salakhutdinov (2010b,a) have all\nconsidered various forms of tempered transitions to address\nthe failure of Gibbs chain mixing, and convincing solutions\nhave not yet been clearly demonstrated. A recently introduced\npromising avenue relies on depth itself, showing that mixing\nbetween modes is much easier on deeper layers (Bengio et al.,\n2013) (Sec.9.4).\nTieleman and Hinton (2009) have proposed quite a dif-\nferent approach to addressing potential mixing problems of\nSML with their fast-weights persistent contrastive divergence\n(FPCD), and it has also been exploited to train Deep Boltz-\nmann Machines (Salakhutdinov, 2010a) and construct a pure\nsampling algorithm for RBMs (Breuleux et al., 2011). FPCD\nbuilds on the surprising but robust tendency of Gibbs chains\nto mix better during SML learning than when the model\nparameters are ﬁxed. The phenomenon is rooted in the form of\nthe likelihood gradient itself (Eq. 15). The samples drawn from\nthe SML Gibbs chain are used in the negative phase of the\ngradient, which implies that the learning update will slightly\nincrease the energy (decrease the probability) of those samples,\nmaking the region in the neighborhood of those samples\n12. When weights become large, the estimated distribution is more peaky,\nand the chain takes very long time to mix, to move from mode to mode, so\nthat practically the gradient estimator can be very poor. This is a serious\nchicken-and-egg problem because if sampling is not effective, nor is the\ntraining procedure, which may seem to stall, and yields even larger weights.\nless likely to be resampled and therefore making it more\nlikely that the samples will move somewhere else (typically\ngoing near another mode). Rather than drawing samples from\nthe distribution of the current model (with parameters θ),\nFPCD exaggerates this effect by drawing samples from a local\nperturbation of the model with parameters θ∗and an update\nθ∗\nt+1 = (1 −η)θt+1 + ηθ∗\nt + ϵ∗∂\n∂θi\n T\nX\nt=1\nlog p(x(t))\n!\n,\n(17)\nwhere ϵ∗is the relatively large fast-weight learning rate\n(ϵ∗> ϵ) and 0 < η < 1 (but near 1) is a forgetting factor\nthat keeps the perturbed model close to the current model.\nUnlike tempering, FPCD does not converge to the model\ndistribution as ϵ and ϵ∗go to 0, and further work is necessary\nto characterize the nature of its approximation to the model\ndistribution. Nevertheless, FPCD is a popular and apparently\neffective means of drawing approximate samples from the\nmodel distribution that faithfully represent its diversity, at the\nprice of sometimes generating spurious samples in between\ntwo modes (because the fast weights roughly correspond to a\nsmoothed view of the current model’s energy function). It has\nbeen applied in a variety of applications (Tieleman and Hinton,\n2009; Ranzato et al., 2011; Kivinen and Williams, 2012) and\nit has been transformed into a sampling algorithm (Breuleux\net al., 2011) that also shares this fast mixing property with\nherding (Welling, 2009), for the same reason, i.e., introducing\nnegative correlations between consecutive samples of the\nchain in order to promote faster mixing.\n6.4.3\nPseudolikelihood, Ratio-matching and More\nWhile CD, SML and FPCD are by far the most popular meth-\nods for training RBMs and RBM-based models, all of these\nmethods are perhaps most naturally described as offering dif-\nferent approximations to maximum likelihood training. There\nexist other inductive principles that are alternatives to maxi-\nmum likelihood that can also be used to train RBMs. In partic-\nular, these include pseudo-likelihood (Besag, 1975) and ratio-\nmatching (Hyv¨arinen, 2007). Both of these inductive principles\nattempt to avoid explicitly dealing with the partition function,\nand their asymptotic efﬁciency has been analyzed (Marlin and\nde Freitas, 2011). Pseudo-likelihood seeks to maximize the\nproduct of all one-dimensional conditional distributions of the\nform P(xd|x\\d), while ratio-matching can be interpreted as\nan extension of score matching (Hyv¨arinen, 2005) to discrete\ndata types. Both methods amount to weighted differences of\nthe gradient of the RBM free energy13 evaluated at a data\npoint and at neighboring points. One potential drawback of\nthese methods is that depending on the parametrization of\nthe energy function, their computational requirements may\nscale up to O(nd) worse than CD, SML, FPCD, or denoising\nscore matching (Kingma and LeCun, 2010; Vincent, 2011),\ndiscussed below. Marlin et al. (2010) empirically compared all\nof these methods (except denoising score matching) on a range\nof classiﬁcation, reconstruction and density modeling tasks and\nfound that, in general, SML provided the best combination of\noverall performance and computational tractability. However,\nin a later study, the same authors (Swersky et al., 2011)\n13. The free energy F(x; θ) is the energy associated with the data marginal\nprobability, F(x; θ) = −log P(x) −log Zθ and is tractable for the RBM.\n13\nfound denoising score matching to be a competitive inductive\nprinciple both in terms of classiﬁcation performance (with\nrespect to SML) and in terms of computational efﬁciency (with\nrespect to analytically obtained score matching). Denoising\nscore matching is a special case of the denoising auto-encoder\ntraining criterion (Section 7.2.2) when the reconstruction error\nresidual equals a gradient, i.e., the score function associated\nwith an energy function, as shown in (Vincent, 2011).\nIn the spirit of the Boltzmann machine gradient (Eq. 15)\nseveral approaches have been proposed to train energy-based\nmodels. One is noise-contrastive estimation (Gutmann and Hy-\nvarinen, 2010), in which the training criterion is transformed\ninto a probabilistic classiﬁcation problem: distinguish between\n(positive) training examples and (negative) noise samples\ngenerated by a broad distribution (such as the Gaussian).\nAnother family of approaches, more in the spirit of Contrastive\nDivergence, relies on distinguishing positive examples (of\nthe training distribution) and negative examples obtained by\nperturbations of the positive examples (Collobert and Weston,\n2008; Bordes et al., 2012; Weston et al., 2010).\n7\nDIRECTLY LEARNING A PARAMETRIC MAP\nFROM INPUT TO REPRESENTATION\nWithin the framework of probabilistic models adopted in\nSection 6, the learned representation is always associated with\nlatent variables, speciﬁcally with their posterior distribution\ngiven an observed input x. Unfortunately, this posterior dis-\ntribution tends to become very complicated and intractable if\nthe model has more than a couple of interconnected layers,\nwhether in the directed or undirected graphical model frame-\nworks. It then becomes necessary to resort to sampling or\napproximate inference techniques, and to pay the associated\ncomputational and approximation error price. If the true pos-\nterior has a large number of modes that matter then current\ninference techniques may face an unsurmountable challenge or\nendure a potentially serious approximation. This is in addition\nto the difﬁculties raised by the intractable partition function in\nundirected graphical models. Moreover a posterior distribution\nover latent variables is not yet a simple usable feature vector\nthat can for example be fed to a classiﬁer. So actual feature\nvalues are typically derived from that distribution, taking the\nlatent variable’s expectation (as is typically done with RBMs),\ntheir marginal probability, or ﬁnding their most likely value\n(as in sparse coding). If we are to extract stable deterministic\nnumerical feature values in the end anyway, an alternative\n(apparently) non-probabilistic feature learning paradigm that\nfocuses on carrying out this part of the computation, very efﬁ-\nciently, is that of auto-encoders and other directly parametrized\nfeature or representation functions. The commonality between\nthese methods is that they learn a direct encoding, i.e., a\nparametric map from inputs to their representation.\nRegularized auto-encoders, discussed next, also involve\nlearning a decoding function that maps back from represen-\ntation to input space. Sections 8.1 and 11.3 discuss direct\nencoding methods that do not require a decoder, such as semi-\nsupervised embedding (Weston et al., 2008) and slow feature\nanalysis (Wiskott and Sejnowski, 2002).\n7.1\nAuto-Encoders\nIn the auto-encoder framework (LeCun, 1987; Bourlard and\nKamp, 1988; Hinton and Zemel, 1994), one starts by ex-\nplicitly deﬁning a feature-extracting function in a speciﬁc\nparametrized closed form. This function, that we will denote\nfθ, is called the encoder and will allow the straightforward\nand efﬁcient computation of a feature vector h = fθ(x)\nfrom an input x. For each example x(t) from a data set\n{x(1), . . . , x(T )}, we deﬁne\nh(t) = fθ(x(t))\n(18)\nwhere h(t) is the feature-vector or representation or code com-\nputed from x(t). Another closed form parametrized function\ngθ, called the decoder, maps from feature space back into\ninput space, producing a reconstruction r = gθ(h). Whereas\nprobabilistic models are deﬁned from an explicit probability\nfunction and are trained to maximize (often approximately) the\ndata likelihood (or a proxy), auto-encoders are parametrized\nthrough their encoder and decoder and are trained using a\ndifferent training principle. The set of parameters θ of the\nencoder and decoder are learned simultaneously on the task\nof reconstructing as well as possible the original input, i.e.\nattempting to incur the lowest possible reconstruction error\nL(x, r) – a measure of the discrepancy between x and its\nreconstruction r – over training examples. Good generalization\nmeans low reconstruction error at test examples, while having\nhigh reconstruction error for most other x conﬁgurations. To\ncapture the structure of the data-generating distribution, it\nis therefore important that something in the training crite-\nrion or the parametrization prevents the auto-encoder from\nlearning the identity function, which has zero reconstruction\nerror everywhere. This is achieved through various means in\nthe different forms of auto-encoders, as described below in\nmore detail, and we call these regularized auto-encoders. A\nparticular form of regularization consists in constraining the\ncode to have a low dimension, and this is what the classical\nauto-encoder or PCA do.\nIn summary, basic auto-encoder training consists in ﬁnding\na value of parameter vector θ minimizing reconstruction error\nJAE(θ)\n=\nX\nt\nL(x(t), gθ(fθ(x(t))))\n(19)\nwhere x(t) is a training example. This minimization is usually\ncarried out by stochastic gradient descent as in the training\nof Multi-Layer-Perceptrons (MLPs). Since auto-encoders were\nprimarily developed as MLPs predicting their input, the most\ncommonly used forms for the encoder and decoder are afﬁne\nmappings, optionally followed by a non-linearity:\nfθ(x)\n=\nsf(b + Wx)\n(20)\ngθ(h)\n=\nsg(d + W ′h)\n(21)\nwhere sf and sg are the encoder and decoder activation\nfunctions (typically the element-wise sigmoid or hyperbolic\ntangent non-linearity, or the identity function if staying linear).\nThe set of parameters of such a model is θ = {W, b, W ′, d}\nwhere b and d are called encoder and decoder bias vectors,\nand W and W ′ are the encoder and decoder weight matrices.\nThe choice of sg and L depends largely on the input domain\nrange and nature, and are usually chosen so that L returns a\nnegative log-likelihood for the observed value of x. A natural\nchoice for an unbounded domain is a linear decoder with a\n14\nsquared reconstruction error, i.e. sg(a) = a and L(x, r) =\n∥x −r∥2. If inputs are bounded between 0 and 1 however,\nensuring a similarly-bounded reconstruction can be achieved\nby using sg = sigmoid. In addition if the inputs are of a binary\nnature, a binary cross-entropy loss14 is sometimes used.\nIf both encoder and decoder use a sigmoid non-linearity,\nthen fθ(x) and gθ(h) have the exact same form as the\nconditionals P(h | v) and P(v | h) of binary RBMs (see\nSection 6.2). This similarity motivated an initial study (Bengio\net al., 2007) of the possibility of replacing RBMs with auto-\nencoders as the basic pre-training strategy for building deep\nnetworks, as well as the comparative analysis of auto-encoder\nreconstruction error gradient and contrastive divergence up-\ndates (Bengio and Delalleau, 2009).\nOne notable difference in the parametrization is that RBMs\nuse a single weight matrix, which follows naturally from their\nenergy function, whereas the auto-encoder framework allows\nfor a different matrix in the encoder and decoder. In practice\nhowever, weight-tying in which one deﬁnes W ′ = W T may\nbe (and is most often) used, rendering the parametrizations\nidentical. The usual training procedures however differ greatly\nbetween the two approaches. A practical advantage of training\nauto-encoder variants is that they deﬁne a simple tractable\noptimization objective that can be used to monitor progress.\nIn the case of a linear auto-encoder (linear encoder and\ndecoder) with squared reconstruction error, minimizing Eq. 19\nlearns the same subspace15 as PCA. This is also true when\nusing a sigmoid nonlinearity in the encoder (Bourlard and\nKamp, 1988), but not if the weights W and W ′ are tied\n(W ′ = W T ), because W cannot be forced into being small\nand W ′ large to achieve a linear encoder.\nSimilarly, Le et al. (2011b) recently showed that adding a\nregularization term of the form P\nt\nP\nj s3(Wjx(t)) to a linear\nauto-encoder with tied weights, where s3 is a nonlinear convex\nfunction, yields an efﬁcient algorithm for learning linear ICA.\n7.2\nRegularized Auto-Encoders\nLike PCA, auto-encoders were originally seen as a dimen-\nsionality reduction technique and thus used a bottleneck, i.e.\ndh < dx. On the other hand, successful uses of sparse\ncoding and RBM approaches tend to favour overcomplete\nrepresentations, i.e. dh\n> dx. This can allow the auto-\nencoder to simply duplicate the input in the features, with\nperfect reconstruction without having extracted more mean-\ningful features. Recent research has demonstrated very suc-\ncessful alternative ways, called regulrized auto-encoders, to\n“constrain” the representation, even when it is overcomplete.\nThe effect of a bottleneck or of this regularization is that the\nauto-encoder cannot reconstruct well everything, it is trained\nto reconstruct well the training examples and generalization\nmeans that reconstruction error is also small on test examples.\nAn interesting justiﬁcation (Ranzato et al., 2008) for the\nsparsity penalty (or any penalty that restricts in a soft way\n14. L(x, r) = −Pdx\ni=1 xi log(ri) + (1 −xi) log(1 −ri)\n15. Contrary to traditional PCA loading factors, but similarly to the\nparameters learned by probabilistic PCA, the weight vectors learned by a\nlinear auto-encoder are not constrained to form an orthonormal basis, nor to\nhave a meaningful ordering. They will however span the same subspace.\nthe volume of hidden conﬁgurations easily accessible by the\nlearner) is that it acts in spirit like the partition function of\nRBMs, by making sure that only few input conﬁgurations can\nhave a low reconstruction error.\nAlternatively, one can view the objective of the regulariza-\ntion applied to an auto-encoder as making the representation\nas “constant” (insensitive) as possible with respect to changes\nin input. This view immediately justiﬁes two variants of\nregularized auto-encoders described below: contractive auto-\nencoders reduce the number of effective degrees of freedom of\nthe representation (around each point) by making the encoder\ncontractive, i.e., making the derivative of the encoder small\n(thus making the hidden units saturate), while the denoising\nauto-encoder makes the whole mapping “robust”, i.e., insen-\nsitive to small random perturbations, or contractive, making\nsure that the reconstruction cannot stay good when moving in\nmost directions around a training example.\n7.2.1\nSparse Auto-Encoders\nThe earliest uses of single-layer auto-encoders for building\ndeep architectures by stacking them (Bengio et al., 2007)\nconsidered the idea of tying the encoder weights and decoder\nweights to restrict capacity as well as the idea of introducing a\nform of sparsity regularization (Ranzato et al., 2007). Sparsity\nin the representation can be achieved by penalizing the hidden\nunit biases (making these additive offset parameters more\nnegative) (Ranzato et al., 2007; Lee et al., 2008; Goodfellow\net al., 2009; Larochelle and Bengio, 2008) or by directly\npenalizing the output of the hidden unit activations (making\nthem closer to their saturating value at 0) (Ranzato et al.,\n2008; Le et al., 2011a; Zou et al., 2011). Penalizing the bias\nruns the danger that the weights could compensate for the\nbias, which could hurt numerical optimization. When directly\npenalizing the hidden unit outputs, several variants can be\nfound in the literature, but a clear comparative analysis is\nstill lacking. Although the L1 penalty (i.e., simply the sum\nof output elements hj in the case of sigmoid non-linearity)\nwould seem the most natural (because of its use in sparse cod-\ning), it is used in few papers involving sparse auto-encoders.\nA close cousin of the L1 penalty is the Student-t penalty\n(log(1+h2\nj)), originally proposed for sparse coding (Olshausen\nand Field, 1996). Several papers penalize the average output\n¯hj (e.g. over a minibatch), and instead of pushing it to 0,\nencourage it to approach a ﬁxed target, either through a mean-\nsquare error penalty, or maybe more sensibly (because hj\nbehaves like a probability), a Kullback-Liebler divergence\nwith respect to the binomial distribution with probability ρ:\n−ρ log ¯hj −(1−ρ) log(1−¯hj)+constant, e.g., with ρ = 0.05.\n7.2.2\nDenoising Auto-Encoders\nVincent et al. (2008, 2010) proposed altering the training ob-\njective in Eq. 19 from mere reconstruction to that of denoising\nan artiﬁcially corrupted input, i.e. learning to reconstruct the\nclean input from a corrupted version. Learning the identity\nis no longer enough: the learner must capture the structure\nof the input distribution in order to optimally undo the effect\nof the corruption process, with the reconstruction essentially\nbeing a nearby but higher density point than the corrupted\ninput. Figure 3 illustrates that the Denoising Auto-Encoder\n15\n(DAE) is learning a reconstruction function that corresponds\nto a vector ﬁeld pointing towards high-density regions (the\nmanifold where examples concentrate).\nCorrupted input \nCorrupted input \nprior:&examples&concentrate&\nnear&a&lower&dimensional&\n“manifold”&&\noriginal  \ninput \nFig. 3.\nWhen data concentrate near a lower-dimensional\nmanifold, the corruption vector is typically almost orthogonal to\nthe manifold, and the reconstruction function learns to denoise,\nmap from low-probability conﬁgurations (corrupted inputs) to\nhigh-probability ones (original inputs), creating a vector ﬁeld\naligned with the score (derivative of the estimated density).\n.\nFormally, the objective optimized by a DAE is:\nJDAE\n=\nX\nt\nEq(˜x|x(t))\nh\nL(x(t), gθ(fθ(˜x)))\ni\n(22)\nwhere Eq(˜x|x(t)) [·] averages over corrupted examples ˜x drawn\nfrom corruption process q(˜x|x(t)). In practice this is optimized\nby stochastic gradient descent, where the stochastic gradient is\nestimated by drawing one or a few corrupted versions of x(t)\neach time x(t) is considered. Corruptions considered in Vin-\ncent et al. (2010) include additive isotropic Gaussian noise,\nsalt and pepper noise for gray-scale images, and masking\nnoise (salt or pepper only), e.g., setting some randomly chosen\ninputs to 0 (independently per example). Masking noise has\nbeen used in most of the simulations. Qualitatively better\nfeatures are reported with denoising, resulting in improved\nclassiﬁcation, and DAE features performed similarly or better\nthan RBM features. Chen et al. (2012) show that a simpler\nalternative with a closed form solution can be obtained when\nrestricting to a linear auto-encoder and have successfully\napplied it to domain adaptation.\nVincent (2011) relates DAEs to energy-based probabilistic\nmodels: DAEs basically learn in r(˜x) −˜x a vector pointing\nin the direction of the estimated score ∂log p(˜x)\n∂˜x\n(Figure 3). In\nthe special case of linear reconstruction and squared error,\nVincent (2011) shows that training an afﬁne-sigmoid-afﬁne\nDAE amounts to learning an energy-based model, whose\nenergy function is very close to that of a GRBM. Training uses\na regularized variant of the score matching parameter estima-\ntion technique (Hyv¨arinen, 2005; Hyv¨arinen, 2008; Kingma\nand LeCun, 2010) termed denoising score matching (Vincent,\n2011). Swersky (2010) had shown that training GRBMs with\nscore matching is equivalent to training a regular auto-encoder\nwith an additional regularization term, while, following up on\nthe theoretical results in Vincent (2011), Swersky et al. (2011)\nshowed the practical advantage of denoising to implement\nscore matching efﬁciently. Finally Alain and Bengio (2012)\ngeneralize Vincent (2011) and prove that DAEs of arbitrary\nparametrization with small Gaussian corruption noise are\ngeneral estimators of the score.\n7.2.3\nContractive Auto-Encoders\nContractive Auto-Encoders (CAE), proposed by Rifai et al.\n(2011a), follow up on Denoising Auto-Encoders (DAE) and\nshare a similar motivation of learning robust representations.\nCAEs achieve this by adding an analytic contractive penalty\nto Eq. 19: the Frobenius norm of the encoder’s Jacobian,\nand results in penalizing the sensitivity of learned features to\ninﬁnitesimal input variations. Let J(x) = ∂fθ\n∂x (x) the Jacobian\nmatrix of the encoder at x. The CAE’s training objective is\nJCAE\n=\nX\nt\nL(x(t), gθ(fθ(x(t)))) + λ\n\r\r\rJ(x(t))\n\r\r\r\n2\nF\n(23)\nwhere λ is a hyper-parameter controlling the strength of the\nregularization. For an afﬁne sigmoid encoder, the contractive\npenalty term is easy to compute:\nJj(x)\n=\nfθ(x)j(1 −fθ(x)j)Wj\n∥J(x)∥2\nF\n=\nX\nj\n(fθ(x)j(1 −fθ(x)j))2∥Wj∥2\n(24)\nThere are at least three notable differences with DAEs, which\nmay be partly responsible for the better performance that CAE\nfeatures seem to empirically demonstrate: (a) the sensitivity of\nthe features is penalized16 rather than that of the reconstruc-\ntion; (b) the penalty is analytic rather than stochastic: an efﬁ-\nciently computable expression replaces what might otherwise\nrequire dx corrupted samples to size up (i.e. the sensitivity in\ndx directions); (c) a hyper-parameter λ allows a ﬁne control of\nthe trade-off between reconstruction and robustness (while the\ntwo are mingled in a DAE). Note however that there is a tight\nconnection between the DAE and the CAE: as shown in (Alain\nand Bengio, 2012) a DAE with small corruption noise can be\nseen (through a Taylor expansion) as a type of contractive\nauto-encoder where the contractive penalty is on the whole\nreconstruction function rather than just on the encoder17.\nA potential disadvantage of the CAE’s analytic penalty is\nthat it amounts to only encouraging robustness to inﬁnitesimal\ninput variations. This is remedied in Rifai et al. (2011b) with\nthe CAE+H, that penalizes all higher order derivatives, in an\nefﬁcient stochastic manner, by adding a term that encourages\nJ(x) and J(x + ϵ) to be close:\nJCAE+H\n=\nX\nt\nL(x(t), gθ(x(t))) + λ\n\r\r\rJ(x(t))\n\r\r\r\n2\nF\n+γEϵ\n\u0002\n∥J(x) −J(x + ϵ)∥2\nF\n\u0003\n(25)\nwhere ϵ ∼N(0, σ2I), and γ is the associated regularization\nstrength hyper-parameter.\nThe DAE and CAE have been successfully used to win\nthe ﬁnal phase of the Unsupervised and Transfer Learning\nChallenge (Mesnil et al., 2011). The representation learned\nby the CAE tends to be saturated rather than sparse, i.e.,\nmost hidden units are near the extremes of their range (e.g. 0\nor 1), and their derivative ∂hi(x)\n∂x\nis near 0. The non-saturated\nunits are few and sensitive to the inputs, with their associated\nﬁlters (weight vectors) together forming a basis explaining\nthe local changes around x, as discussed in Section 8.2.\nAnother way to get saturated (nearly binary) units is semantic\nhashing (Salakhutdinov and Hinton, 2007).\n16. i.e., the robustness of the representation is encouraged.\n17. but note that in the CAE, the decoder weights are tied to the encoder\nweights, to avoid degenerate solutions, and this should also make the decoder\ncontractive.\n16\n7.2.4\nPredictive Sparse Decomposition\nSparse coding (Olshausen and Field, 1996) may be viewed as a\nkind of auto-encoder that uses a linear decoder with a squared\nreconstruction error, but whose non-parametric encoder fθ\nperforms the comparatively non-trivial and relatively costly\niterative minimization of Eq. 2. A practically successful variant\nof sparse coding and auto-encoders, named Predictive Sparse\nDecomposition or PSD (Kavukcuoglu et al., 2008) replaces\nthat costly and highly non-linear encoding step by a fast\nnon-iterative approximation during recognition (computing the\nlearned features). PSD has been applied to object recognition\nin images and video (Kavukcuoglu et al., 2009, 2010; Jarrett\net al., 2009), but also to audio (Henaff et al., 2011), mostly\nwithin the framework of multi-stage convolutional deep archi-\ntectures (Section 11.2). The main idea can be summarized\nby the following equation for the training criterion, which\nis simultaneously optimized with respect to hidden codes\n(representation) h(t) and with respect to parameters (W, α):\nJPSD =\nX\nt\nλ∥h(t)∥1 + ∥x(t) −Wh(t)∥2\n2 + ∥h(t) −fα(x(t))∥2\n2 (26)\nwhere x(t) is the input vector for example t, h(t) is the\noptimized hidden code for that example, and fα(·) is the\nencoding function, the simplest variant being\nfα(x(t)) = tanh(b + W T x(t))\n(27)\nwhere encoding weights are the transpose of decoding\nweights. Many variants have been proposed, including the\nuse of a shrinkage operation instead of the hyperbolic tan-\ngent (Kavukcuoglu et al., 2010). Note how the L1 penalty on\nh tends to make them sparse, and how this is the same criterion\nas sparse coding with dictionary learning (Eq. 3) except for the\nadditional constraint that one should be able to approximate\nthe sparse codes h with a parametrized encoder fα(x). One can\nthus view PSD as an approximation to sparse coding, where we\nobtain a fast approximate encoder. Once PSD is trained, object\nrepresentations fα(x) are used to feed a classiﬁer. They are\ncomputed quickly and can be further ﬁne-tuned: the encoder\ncan be viewed as one stage or one layer of a trainable multi-\nstage system such as a feedforward neural network.\nPSD can also be seen as a kind of auto-encoder where\nthe codes h are given some freedom that can help to further\nimprove reconstruction. One can also view the encoding\npenalty added on top of sparse coding as a kind of regularizer\nthat forces the sparse codes to be nearly computable by a\nsmooth and efﬁcient encoder. This is in contrast with the codes\nobtained by complete optimization of the sparse coding crite-\nrion, which are highly non-smooth or even non-differentiable,\na problem that motivated other approaches to smooth the\ninferred codes of sparse coding (Bagnell and Bradley, 2009),\nso a sparse coding stage could be jointly optimized along with\nfollowing stages of a deep architecture.\n8\nREPRESENTATION\nLEARNING\nAS\nMANI-\nFOLD LEARNING\nAnother important perspective on representation learning is\nbased on the geometric notion of manifold. Its premise is\nthe manifold hypothesis, according to which real-world data\npresented in high dimensional spaces are expected to con-\ncentrate in the vicinity of a manifold M of much lower\ndimensionality dM, embedded in high dimensional input space\nRdx. This prior seems particularly well suited for AI tasks\nsuch as those involving images, sounds or text, for which\nmost uniformly sampled input conﬁgurations are unlike natural\nstimuli. As soon as there is a notion of “representation”\nthen one can think of a manifold by considering the vari-\nations in input space, which are captured by or reﬂected\n(by corresponding changes) in the learned representation.\nTo ﬁrst approximation, some directions are well preserved\n(the tangent directions of the manifold) while others aren’t\n(directions orthogonal to the manifolds). With this perspec-\ntive, the primary unsupervised learning task is then seen as\nmodeling the structure of the data-supporting manifold18. The\nassociated representation being learned can be associated with\nan intrinsic coordinate system on the embedded manifold. The\narchetypal manifold modeling algorithm is, not surprisingly,\nalso the archetypal low dimensional representation learning\nalgorithm: Principal Component Analysis, which models a\nlinear manifold. It was initially devised with the objective\nof ﬁnding the closest linear manifold to a cloud of data\npoints. The principal components, i.e. the representation fθ(x)\nthat PCA yields for an input point x, uniquely locates its\nprojection on that manifold: it corresponds to intrinsic co-\nordinates on the manifold. Data manifold for complex real\nworld domains are however expected to be strongly non-\nlinear. Their modeling is sometimes approached as patchworks\nof locally linear tangent spaces (Vincent and Bengio, 2003;\nBrand, 2003). The large majority of algorithms built on\nthis geometric perspective adopt a non-parametric approach,\nbased on a training set nearest neighbor graph (Sch¨olkopf\net al., 1998; Roweis and Saul, 2000; Tenenbaum et al., 2000;\nBrand, 2003; Belkin and Niyogi, 2003; Donoho and Grimes,\n2003; Weinberger and Saul, 2004; Hinton and Roweis, 2003;\nvan der Maaten and Hinton, 2008). In these non-parametric\napproaches, each high-dimensional training point has its own\nset of free low-dimensional embedding coordinates, which are\noptimized so that certain properties of the neighborhood graph\ncomputed in original high dimensional input space are best\npreserved. These methods however do not directly learn a\nparametrized feature extraction function fθ(x) applicable to\nnew test points19, which seriously limits their use as feature\nextractors, except in a transductive setting. Comparatively few\nnon-linear manifold learning methods have been proposed,\nthat learn a parametric map that can directly compute a\nrepresentation for new points; we will focus on these.\n8.1\nLearning a parametric mapping based on a\nneighborhood graph\nSome of the above non-parametric manifold learning al-\ngorithms can be modiﬁed to learn a parametric mapping\nfθ, i.e., applicable to new points: instead of having free\n18. Actually, data points need not strictly lie on the “manifold”, but the\nprobability density is expected to fall off sharply as one moves away from it,\nand it may actually be constituted of several possibly disconnected manifolds\nwith different intrinsic dimensionality.\n19. For several of these techniques, representations for new points can\nbe computed using the Nystr¨om approximation as has been proposed as\nan extension in (Bengio et al., 2004), but this remains cumbersome and\ncomputationally expensive.\n17\nlow-dimensional embedding coordinate “parameters” for each\ntraining point, these coordinates are obtained through an\nexplicitly parametrized function, as with the parametric vari-\nant (van der Maaten, 2009) of t-SNE (van der Maaten and\nHinton, 2008).\nInstead, Semi-Supervised Embedding (Weston et al., 2008)\nlearns a direct encoding while taking into account the manifold\nhypothesis through a neighborhood graph. A parametrized\nneural network architecture simultaneously learns a manifold\nembedding and a classiﬁer. The training criterion encourages\ntraining set neigbhors to have similar representations.\nThe reduced and tightly controlled number of free param-\neters in such parametric methods, compared to their pure\nnon-parametric counterparts, forces models to generalize the\nmanifold shape non-locally (Bengio et al., 2006b), which can\ntranslate into better features and ﬁnal performance (van der\nMaaten and Hinton, 2008). However, basing the modeling of\nmanifolds on training set neighborhood relationships might\nbe risky statistically in high dimensional spaces (sparsely\npopulated due to the curse of dimensionality) as e.g. most\nEuclidean nearest neighbors risk having too little in common\nsemantically. The nearest neighbor graph is simply not enough\ndensely populated to map out satisfyingly the wrinkles of\nthe target manifold (Bengio and Monperrus, 2005; Bengio\net al., 2006b; Bengio and LeCun, 2007). It can also become\nproblematic computationally to consider all pairs of data\npoints20, which scales quadratically with training set size.\n8.2\nLearning to represent non-linear manifolds\nCan we learn a manifold without requiring nearest neighbor\nsearches? Yes, for example, with regularized auto-encoders or\nPCA. In PCA, the sensitivity of the extracted components (the\ncode) to input changes is the same regardless of position x.\nThe tangent space is the same everywhere along the linear\nmanifold. By contrast, for a non-linear manifold, the tangent\nof the manifold changes as we move on the manifold, as\nillustrated in Figure 6. In non-linear representation-learning\nalgorithms it is convenient to think about the local variations\nin the representation as the input x is varied on the manifold,\ni.e., as we move among high-probability conﬁgurations. As\nwe discuss below, the ﬁrst derivative of the encoder therefore\nspeciﬁes the shape of the manifold (its tangent plane) around\nan example x lying on it. If the density was really concentrated\non the manifold, and the encoder had captured that, we\nwould ﬁnd the encoder derivatives to be non-zero only in the\ndirections spanned by the tangent plane.\nLet us consider sparse coding in this light: parameter matrix\nW may be interpreted as a dictionary of input directions from\nwhich a different subset will be picked to model the local\ntangent space at an x on the manifold. That subset corresponds\nto the active, i.e. non-zero, features for input x. Non-zero\ncomponent hi will be sensitive to small changes of the input\nin the direction of the associated weight vector W:,i, whereas\ninactive features are more likely to be stuck at 0 until a\nsigniﬁcant displacement has taken place in input space.\n20. Even if pairs are picked stochastically, many must be considered before\nobtaining one that weighs signiﬁcantly on the optimization objective.\nThe Local Coordinate Coding (LCC) algorithm (Yu et al.,\n2009) is very similar to sparse coding, but is explicitly derived\nfrom a manifold perspective. Using the same notation as that\nof sparse coding in Eq. 2, LCC replaces regularization term\n∥h(t)∥1 = P\nj |h(t)\nj | yielding objective\nJLCC =\nX\nt\n \n∥x(t) −Wh(t)∥2\n2 + λ\nX\nj\n|h(t)\nj |∥W:,j −x(t)∥1+p\n!\n(28)\nThis is identical to sparse coding when p = −1, but with\nlarger p it encourages the active anchor points for x(t) (i.e.\nthe codebook vectors W:,j with non-negligible |h(t)\nj | that\nare combined to reconstruct x(t)) to be not too far from\nx(t), hence the local aspect of the algorithm. An important\ntheoretical contribution of Yu et al. (2009) is to show that\nthat any Lipschitz-smooth function φ : M →R deﬁned on a\nsmooth nonlinear manifold M embedded in Rdx can be well\napproximated by a globally linear function with respect to the\nresulting coding scheme (i.e. linear in h), where the accuracy\nof the approximation and required number dh of anchor points\ndepend on dM rather than dx. This result has been further\nextended with the use of local tangent directions (Yu and\nZhang, 2010), as well as to multiple layers (Lin et al., 2010).\nLet us now consider the efﬁcient non-iterative “feed-\nforward” encoders fθ, used by PSD and the auto-encoders\nreviewed in Section 7.2, that are in the form of Eq. 20 or\n27.The computed representation for x will be only signiﬁ-\ncantly sensitive to input space directions associated with non-\nsaturated hidden units (see e.g. Eq. 24 for the Jacobian of a\nsigmoid layer). These directions to which the representation\nis signiﬁcantly sensitive, like in the case of PCA or sparse\ncoding, may be viewed as spanning the tangent space of the\nmanifold at training point x.\n1\"\nMNIST\"\nInput\"Point\"\nTangents\"\nFig. 4.\nThe tangent vectors to the high-density manifold as\nestimated by a Contractive Auto-Encoder (Rifai et al., 2011a).\nThe original input is shown on the top left. Each tangent vector\n(images on right side of ﬁrst row) corresponds to a plausible\nadditive deformation of the original input, as illustrated on the\nsecond row, where a bit of the 3rd singular vector is added to\nthe original, to form a translated and deformed image. Unlike\nin PCA, the tangent vectors are different for different inputs,\nbecause the estimated manifold is highly non-linear.\n. Rifai et al. (2011a) empirically analyze in this light the\nsingular value spectrum of the Jacobian (derivatives of rep-\nresentation vector with respect to input vector) of a trained\nCAE. Here the SVD provides an ordered orthonormal basis of\nmost sensitive directions. The spectrum is sharply decreasing,\nindicating a relatively small number of signiﬁcantly sensi-\ntive directions. This is taken as empirical evidence that the\nCAE indeed modeled the tangent space of a low-dimensional\nmanifold. The leading singular vectors form a basis for the\ntangent plane of the estimated manifold, as illustrated in\nFigure 4. The CAE criterion is believed to achieve this thanks\nto its two opposing terms: the isotropic contractive penalty,\n18\nthat encourages the representation to be equally insensitive to\nchanges in any input directions, and the reconstruction term,\nthat pushes different training points (in particular neighbors) to\nhave a different representation (so they may be reconstructed\naccurately), thus counteracting the isotropic contractive pres-\nsure only in directions tangent to the manifold.\nAnalyzing learned representations through the lens of the\nspectrum of the Jacobian and relating it to the notion of tangent\nspace of a manifold is feasible, whenever the mapping is\ndifferentiable, and regardless of how it was learned, whether\nas direct encoding (as in auto-encoder variants), or derived\nfrom latent variable inference (as in sparse coding or RBMs).\nExact low dimensional manifold models (like PCA) would\nyield non-zero singular values associated to directions along\nthe manifold, and exact zeros for directions orthogonal to the\nmanifold. But in smooth models like the CAE or the RBM we\nwill instead have large versus relatively small singular values\n(as opposed to non-zero versus exactly zero).\n8.3\nLeveraging the modeled tangent spaces\nThe local tangent space, at a point along the manifold, can\nbe thought of capturing locally valid transformations that\nwere prominent in the training data. For example Rifai et al.\n(2011c) examine the tangent directions extracted with an SVD\nof the Jacobian of CAEs trained on digits, images, or text-\ndocument data: they appear to correspond to small transla-\ntions or rotations for images or digits, and to substitutions\nof words within a same theme for documents. Such very\nlocal transformations along a data manifold are not expected\nto change class identity. To build their Manifold Tangent\nClassiﬁer (MTC), Rifai et al. (2011c) then apply techniques\nsuch as tangent distance (Simard et al., 1993) and tangent\npropagation (Simard et al., 1992), that were initially developed\nto build classiﬁers that are insensitive to input deformations\nprovided as prior domain knowledge. Now these techniques\nare applied using the local leading tangent directions extracted\nby a CAE, i.e. not using any prior domain knowledge (except\nthe broad prior about the existence of a manifold). This\napproach set a new record for MNIST digit classiﬁcation\namong prior-knowledge free approaches21.\n9\nCONNECTIONS\nBETWEEN PROBABILISTIC\nAND DIRECT ENCODING MODELS\nThe standard likelihood framework for probabilistic mod-\nels decomposes the training criterion for models with pa-\nrameters θ in two parts: the log-likelihood log P(x|θ) (or\nlog P(x|h, θ) with latent variables h), and the prior log P(θ)\n(or log P(h|θ) + log P(θ) with latent variables).\n9.1\nPSD: a probabilistic interpretation\nIn the case of the PSD algorithm, a connection can be made\nbetween the above standard probabilistic view and the direct\nencoding computation graph. The probabilistic model of PSD\nis the same directed generative model P(x|h) of sparse coding\n(Section 6.1.1), which only accounts for the decoder. The\nencoder is viewed as an approximate inference mechanism to\n21. It yielded 0.81% error rate using the full MNIST training set, with no\nprior deformations, and no convolution.\nguess P(h|x) and initialize a MAP iterative inference (where\nthe sparse prior P(h) is taken into account). However, in\nPSD, the encoder is trained jointly with the decoder, rather\nthan simply taking the end result of iterative inference as a\ntarget to approximate. An interesting view22 to reconcile these\nfacts is that the encoder is a parametric approximation for\nthe MAP solution of a variational lower bound on the joint\nlog-likelihood. When MAP learning is viewed as a special\ncase of variational learning (where the approximation of the\njoint log-likelihood is with a dirac distribution located at the\nMAP solution), the variational recipe tells us to simultaneously\nimprove the likelihood (reduce reconstruction error) and im-\nprove the variational approximation (reduce the discrepancy\nbetween the encoder output and the latent variable value).\nHence PSD sits at the intersection of probabilistic models\n(with latent variables) and direct encoding methods (which\ndirectly parametrize the mapping from input to representation).\nRBMs also sit at the intersection because their particular\nparametrization includes an explicit mapping from input to\nrepresentation, thanks to the restricted connectivity between\nhidden units. However, this nice property does not extend\nto their natural deep generalizations, i.e., Deep Boltzmann\nMachines, discussed in Section 10.2.\n9.2\nRegularized\nAuto-Encoders\nCapture\nLocal\nStructure of the Density\nCan we also say something about the probabilistic interpreta-\ntion of regularized auto-encoders? Their training criterion does\nnot ﬁt the standard likelihood framework because this would\ninvolve a data-dependent “prior”. An interesting hypothesis\nemerges to answer that question, out of recent theoretical\nresults (Vincent, 2011; Alain and Bengio, 2012): the training\ncriterion of regularized auto-encoders, instead of being a form\nof maximum likelihood, corresponds to a different inductive\nprinciple, such as score matching. The score matching con-\nnection is discussed in Section 7.2.2 and has been shown for\na particular parametrization of DAE and equivalent Gaussian\nRBM (Vincent, 2011). The work in Alain and Bengio (2012)\ngeneralizes this idea to a broader class of parametrizations (ar-\nbitrary encoders and decoders), and shows that by regularizing\nthe auto-encoder so that it be contractive, one obtains that the\nreconstruction function and its derivative estimate ﬁrst and\nsecond derivatives of the underlying data-generative density.\nThis view can be exploited to successfully sample from auto-\nencoders, as shown in Rifai et al. (2012); Bengio et al. (2012).\nThe proposed sampling algorithms are MCMCs similar to\nLangevin MCMC, using not just the estimated ﬁrst derivative\nof the density but also the estimated manifold tangents so as\nto stay close to manifolds of high density.\nThis interpretation connects well with the geometric per-\nspective introduced in Section 8. The regularization effects\n(e.g., due to a sparsity regularizer, a contractive regularizer,\nor the denoising criterion) asks the learned representation to\nbe as insensitive as possible to the input, while minimiz-\ning reconstruction error on the training examples forces the\nrepresentation to contain just enough information to distin-\n22. suggested by Ian Goodfellow, personal communication\n19\nx\"\nr(x)\"\nx1\"\nx2\"\nx3\"\nFig. 5.\nReconstruction function r(x) (green) learned by a\nhigh-capacity autoencoder on 1-dimensional input, minimizing\nreconstruction error at training examples x(t) (r(x(t)) in red)\nwhile trying to be as constant as possible otherwise. The dotted\nline is the identity reconstruction (which might be obtained\nwithout the regularizer). The blue arrows shows the vector ﬁeld\nof r(x)−x pointing towards high density peaks estimated by the\nmodel, and estimating the score (log-density derivative).\n.\nguish them. The solution is that variations along the high-\ndensity manifolds are preserved while other variations are\ncompressed: the reconstruction function should be as constant\nas possible while reproducing training examples, i.e., points\nnear a training example should be mapped to that training\nexample (Figure 5). The reconstruction function should map\nan input towards the nearest point manifold, i.e., the difference\nbetween reconstruction and input is a vector aligned with\nthe estimated score (the derivative of the log-density with\nrespect to the input). The score can be zero on the manifold\n(where reconstruction error is also zero), at local maxima of\nthe log-density, but it can also be zero at local minima. It\nmeans that we cannot equate low reconstruction error with\nhigh estimated probability. The second derivatives of the log-\ndensity corresponds to the ﬁrst derivatives of the reconstruction\nfunction, and on the manifold (where the ﬁrst derivative is 0),\nthey indicate the tangent directions of the manifold (where the\nﬁrst derivative remains near 0).\nFig. 6. Sampling from regularized auto-encoders (Rifai et al.,\n2012; Bengio et al., 2012): each MCMC step adds to current\nstate x the noise δ mostly in the directions of the estimated man-\nifold tangent plane H and projects back towards the manifold\n(high-density regions) by performing a reconstruction step.\n.\nAs illustrated in Figure 6, the basic idea of the auto-encoder\nsampling algorithms in Rifai et al. (2012); Bengio et al. (2012)\nis to make MCMC moves where one (a) moves toward the\nmanifold by following the density gradient (i.e., applying a\nreconstruction) and (b) adds noise in the directions of the\nleading singular vectors of the reconstruction (or encoder)\nJacobian, corresponding to those associated with smallest\nsecond derivative of the log-density.\n9.3\nLearning Approximate Inference\nLet us now consider from closer how a representation is\ncomputed in probabilistic models with latent variables, when\niterative inference is required. There is a computation graph\n(possibly with random number generation in some of the\nnodes, in the case of MCMC) that maps inputs to repre-\nsentation, and in the case of deterministic inference (e.g.,\nMAP inference or variational inference), that function could\nbe optimized directly. This is a way to generalize PSD that has\nbeen explored in recent work on probabilistic models at the\nintersection of inference and learning (Bagnell and Bradley,\n2009; Gregor and LeCun, 2010b; Grubb and Bagnell, 2010;\nSalakhutdinov and Larochelle, 2010; Stoyanov et al., 2011;\nEisner, 2012), where a central idea is that instead of using a\ngeneric inference mechanism, one can use one that is learned\nand is more efﬁcient, taking advantage of the speciﬁcs of the\ntype of data on which it is applied.\n9.4\nSampling Challenges\nA troubling challenge with many probabilistic models with\nlatent variables like most Boltzmann machine variants is that\ngood MCMC sampling is required as part of the learning\nprocedure, but that sampling becomes extremely inefﬁcient (or\nunreliable) as training progresses because the modes of the\nlearned distribution become sharper, making mixing between\nmodes very slow. Whereas initially during training a learner as-\nsigns mass almost uniformly, as training progresses, its entropy\ndecreases, approaching the entropy of the target distribution as\nmore examples and more computation are provided. According\nto our Manifold and Natural Clustering priors of Section 3.1,\nthe target distribution has sharp modes (manifolds) separated\nby extremely low density areas. Mixing then becomes more\ndifﬁcult because MCMC methods, by their very nature, tend\nto make small steps to nearby high-probability conﬁgurations.\nThis is illustrated in Figure 7.\n1\"\nFig. 7. Top: early during training, MCMC mixes easily between\nmodes because the estimated distribution has high entropy\nand puts enough mass everywhere for small-steps movements\n(MCMC) to go from mode to mode. Bottom: later on, training\nrelying on good mixing can stall because estimated modes are\nseparated by wide low-density deserts.\n.\nBengio et al. (2013) suggest that deep representations could\nhelp mixing between such well separated modes, based on\nboth theoretical arguments and on empirical evidence. The\nidea is that if higher-level representations disentangle better\n20\nthe underlying abstract factors, then small steps in this abstract\nspace (e.g., swapping from one category to another) can easily\nbe done by MCMC. The high-level representations can then\nbe mapped back to the input space in order to obtain input-\nlevel samples, as in the Deep Belief Networks (DBN) sampling\nalgorithm (Hinton et al., 2006). This has been demonstrated\nboth with DBNs and with the newly proposed algorithm for\nsampling from contracting and denoising auto-encoders (Rifai\net al., 2012; Bengio et al., 2012). This observation alone does\nnot sufﬁce to solve the problem of training a DBN or a DBM,\nbut it may provide a crucial ingredient, and it makes it possible\nto consider successfully sampling from deep models trained\nby procedures that do not require an MCMC, like the stacked\nregularized auto-encoders used in Rifai et al. (2012).\n9.5\nEvaluating and Monitoring Performance\nIt is always possible to evaluate a feature learning algorithm\nin terms of its usefulness with respect to a particular task (e.g.\nobject classiﬁcation), with a predictor that is fed or initialized\nwith the learned features. In practice, we do this by saving\nthe features learned (e.g. at regular intervals during training,\nto perform early stopping) and training a cheap classiﬁer on\ntop (such as a linear classiﬁer). However, training the ﬁnal\nclassiﬁer can be a substantial computational overhead (e.g.,\nsupervised ﬁne-tuning a deep neural network takes usually\nmore training iterations than the feature learning itself), so\nwe may want to avoid having to train a classiﬁer for ev-\nery training iteration of the unsupervised learner and every\nhyper-parameter setting. More importantly this may give an\nincomplete evaluation of the features (what would happen for\nother tasks?). All these issues motivate the use of methods to\nmonitor and evaluate purely unsupervised performance. This\nis rather easy with all the auto-encoder variants (with some\ncaution outlined below) and rather difﬁcult with the undirected\ngraphical models such as the RBM and Boltzmann machines.\nFor auto-encoder and sparse coding variants, test set re-\nconstruction error can readily be computed, but by itself may\nbe misleading because larger capacity (e.g., more features,\nmore training time) tends to systematically lead to lower\nreconstruction error, even on the test set. Hence it cannot be\nused reliably for selecting most hyper-parameters. On the other\nhand, denoising reconstruction error is clearly immune to this\nproblem, so that solves the problem for DAEs. Based on the\nconnection between DAEs and CAEs uncovered in\nBengio\net al. (2012); Alain and Bengio (2012), this immunity can be\nextended to DAEs, but not to the hyper-parameter controlling\nthe amount of noise or of contraction.\nFor RBMs and some (not too deep) Boltzmann machines,\none option is the use of Annealed Importance Sampling (Mur-\nray and Salakhutdinov, 2009) in order to estimate the partition\nfunction (and thus the test log-likelihood). Note that this esti-\nmator can have high variance and that it becomes less reliable\n(variance becomes too large) as the model becomes more\ninteresting, with larger weights, more non-linearity, sharper\nmodes and a sharper probability density function (see our\nprevious discussion in Section 9.4). Another interesting and\nrecently proposed option for RBMs is to track the partition\nfunction during training (Desjardins et al., 2011), which could\nbe useful for early stopping and reducing the cost of ordinary\nAIS. For toy RBMs (e.g., 25 hidden units or less, or 25\ninputs or less), the exact log-likelihood can also be computed\nanalytically, and this can be a good way to debug and verify\nsome properties of interest.\n10\nGLOBAL TRAINING OF DEEP MODELS\nOne of the most interesting challenges raised by deep archi-\ntectures is: how should we jointly train all the levels? In the\nprevious section and in Section 4 we have only discussed\nhow single-layer models could be combined to form a deep\nmodel. Here we consider joint training of all the levels and\nthe difﬁculties that may arise.\n10.1\nThe Challenge of Training Deep Architectures\nHigher-level abstraction means more non-linearity. It means\nthat two nearby input conﬁgurations may be interpreted very\ndifferently because a few surface details change the underlying\nsemantics, whereas most other changes in the surface details\nwould not change the underlying semantics. The representa-\ntions associated with input manifolds may be complex because\nthe mapping from input to representation may have to unfold\nand distort input manifolds that generally have complicated\nshapes into spaces where distributions are much simpler, where\nrelations between factors are simpler, maybe even linear or\ninvolving many (conditional) independencies. Our expectation\nis that modeling the joint distribution between high-level\nabstractions and concepts should be much easier in the sense\nof requiring much less data to learn. The hard part is learning a\ngood representation that does this unfolding and disentangling.\nThis may be at the price of a more difﬁcult training problem,\npossibly involving ill-conditioning and local minima.\nIt is only since 2006 that researchers have seriously inves-\ntigated ways to train deep architectures, to the exception of\nthe convolutional networks (LeCun et al., 1998b). The ﬁrst\nrealization (Section 4) was that unsupervised or supervised\nlayer-wise training was easier, and that this could be taken\nadvantage of by stacking single-layer models into deeper ones.\nIt is interesting to ask why does the layerwise unsuper-\nvised pre-training procedure sometimes help a supervised\nlearner (Erhan et al., 2010b). There seems to be a more\ngeneral principle at play 23 of guiding the training of inter-\nmediate representations, which may be easier than trying to\nlearn it all in one go. This is nicely related to the curriculum\nlearning idea (Bengio et al., 2009), that it may be much easier\nto learn simpler concepts ﬁrst and then build higher-level\nones on top of simpler ones. This is also coherent with the\nsuccess of several deep learning algorithms that provide some\nsuch guidance for intermediate representations, like Semi-\nSupervised Embedding (Weston et al., 2008).\nThe question of why unsupervised pre-training could be\nhelpful was extensively studied (Erhan et al., 2010b), trying\nto dissect the answer into a regularization effect and an\noptimization effect. The regularization effect is clear from\nthe experiments where the stacked RBMs or denoising auto-\nencoders are used to initialize a supervised classiﬁcation neural\nnetwork (Erhan et al., 2010b). It may simply come from the\n23. First suggested to us by Leon Bottou\n21\nuse of unsupervised learning to bias the learning dynamics\nand initialize it in the basin of attraction of a “good” local\nminimum (of the training criterion), where “good” is in terms\nof generalization error. The underlying hypothesis exploited\nby this procedure is that some of the features or latent factors\nthat are good at capturing the leading variations in the input\ndistribution are also good at capturing the variations in the\ntarget output random variables of interest (e.g., classes). The\noptimization effect is more difﬁcult to tease out because the\ntop two layers of a deep neural net can just overﬁt the training\nset whether the lower layers compute useful features or not,\nbut there are several indications that optimizing the lower\nlevels with respect to a supervised training criterion can be\nchallenging.\nOne such indication is that changing the numerical con-\nditions of the optimization procedure can have a profound\nimpact on the joint training of a deep architecture, for ex-\nample by changing the initialization range and changing the\ntype of non-linearity used (Glorot and Bengio, 2010), much\nmore so than with shallow architectures. One hypothesis to\nexplain some of the difﬁculty in the optimization of deep\narchitectures is centered on the singular values of the Jacobian\nmatrix associated with the transformation from the features\nat one level into the features at the next level (Glorot and\nBengio, 2010). If these singular values are all small (less than\n1), then the mapping is contractive in every direction and\ngradients would vanish when propagated backwards through\nmany layers. This is a problem already discussed for recurrent\nneural networks (Bengio et al., 1994), which can be seen as\nvery deep networks with shared parameters at each layer, when\nunfolded in time. This optimization difﬁculty has motivated\nthe exploration of second-order methods for deep architectures\nand recurrent networks, in particular Hessian-free second-\norder methods (Martens, 2010; Martens and Sutskever, 2011).\nUnsupervised pre-training has also been proposed to help\ntraining recurrent networks and temporal RBMs (Sutskever\net al., 2009), i.e., at each time step there is a local signal to\nguide the discovery of good features to capture in the state\nvariables: model with the current state (as hidden units) the\njoint distribution of the previous state and the current input.\nNatural gradient (Amari, 1998) methods that can be applied to\nnetworks with millions of parameters (i.e. with good scaling\nproperties) have also been proposed (Le Roux et al., 2008b;\nPascanu and Bengio, 2013). Cho et al. (2011) proposes to use\nadaptive learning rates for RBM training, along with a novel\nand interesting idea for a gradient estimator that takes into\naccount the invariance of the model to ﬂipping hidden unit bits\nand inverting signs of corresponding weight vectors. At least\none study indicates that the choice of initialization (to make\nthe Jacobian of each layer closer to 1 across all its singular\nvalues) could substantially reduce the training difﬁculty of\ndeep networks (Glorot and Bengio, 2010) and this is coherent\nwith the success of the initialization procedure of Echo State\nNetworks (Jaeger, 2007), as recently studied by Sutskever\n(2012). There are also several experimental results (Glorot and\nBengio, 2010; Glorot et al., 2011a; Nair and Hinton, 2010)\nshowing that the choice of hidden units non-linearity could\ninﬂuence both training and generalization performance, with\nparticularly interesting results obtained with sparse rectifying\nunits (Jarrett et al., 2009; Nair and Hinton, 2010; Glorot\net al., 2011a; Krizhevsky et al., 2012). An old idea regarding\nthe ill-conditioning issue with neural networks is that of\nsymmetry breaking: part of the slowness of convergence may\nbe due to many units moving together (like sheep) and all\ntrying to reduce the output error for the same examples.\nBy initializing with sparse weights (Martens, 2010) or by\nusing often saturated non-linearities (such as rectiﬁers as max-\npooling units), gradients only ﬂow along a few paths, which\nmay help hidden units to specialize more quickly. Another\npromising idea to improve the conditioning of neural network\ntraining is to nullify the average value and slope of each\nhidden unit output (Raiko et al., 2012), and possibly locally\nnormalize magnitude as well (Jarrett et al., 2009). The debate\nstill rages between using online methods such as stochastic\ngradient descent and using second-order methods on large\nminibatches (of several thousand examples) (Martens, 2010;\nLe et al., 2011a), with a variant of stochastic gradient descent\nrecently winning an optimization challenge 24.\nFinally, several recent results exploiting large quantities\nof\nlabeled\ndata\nsuggest\nthat\nwith\nproper\ninitialization\nand choice of non-linearity, very deep purely supervised\nnetworks can be trained successfully without any layerwise\npre-training (Ciresan et al., 2010; Glorot et al., 2011a; Seide\net al., 2011a; Krizhevsky et al., 2012). Researchers report\nthan in such conditions, layerwise unsupervised pre-training\nbrought little or no improvement over pure supervised\nlearning from scratch when training for long enough. This\nreinforces\nthe\nhypothesis\nthat\nunsupervised\npre-training\nacts as a prior, which may be less necessary when very\nlarge quantities of labeled data are available, but begs the\nquestion of why this had not been discovered earlier. The\nlatest results reported in this respect (Krizhevsky et al.,\n2012) are particularly interesting because they allowed to\ndrastically reduce the error rate of object recognition on a\nbenchmark (the 1000-class ImageNet task) where many more\ntraditional computer vision approaches had been evaluated\n(http://www.image-net.org/challenges/LSVRC/2012/results.html).\nThe main techniques that allowed this success include the\nfollowing: efﬁcient GPU training allowing one to train\nlonger (more than 100 million visits of examples), an aspect\nﬁrst reported by Lee et al. (2009a); Ciresan et al. (2010),\nlarge number of labeled examples, artiﬁcially transformed\nexamples (see Section 11.1), a large number of tasks (1000\nor 10000 classes for ImageNet), convolutional architecture\nwith max-pooling (see section 11 for these latter two\ntechniques),\nrectifying\nnon-linearities\n(discussed\nabove),\ncareful initialization (discussed above), careful parameter\nupdate and adaptive learning rate heuristics, layerwise\nfeature normalization (across features), and a new dropout\ntrick based on injecting strong binary multiplicative noise on\nhidden units. This trick is similar to the binary noise injection\nused at each layer of a stack of denoising auto-encoders.\nFuture work is hopefully going to help identify which of\nthese elements matter most, how to generalize them across\n24. https://sites.google.com/site/nips2011workshop/optimization-challenges\n22\na large variety of tasks and architectures, and in particular\ncontexts where most examples are unlabeled, i.e., including\nan unsupervised component in the training criterion.\n10.2\nJoint Training of Deep Boltzmann Machines\nWe now consider the problem of joint training of all layers of\na speciﬁc unsupervised model, the Deep Boltzmann Machine\n(DBM). Whereas much progress (albeit with many unan-\nswered questions) has been made on jointly training all the\nlayers of deep architectures using back-propagated gradients\n(i.e., mostly in the supervised setting), much less work has\nbeen done on their purely unsupervised counterpart, e.g. with\nDBMs25. Note however that one could hope that the successful\ntechniques described in the previous section could be applied\nto unsupervised learning algorithms.\nLike the RBM, the DBM is another particular subset of\nthe Boltzmann machine family of models where the units\nare again arranged in layers. However unlike the RBM, the\nDBM possesses multiple layers of hidden units, with units in\nodd-numbered layers being conditionally independent given\neven-numbered layers, and vice-versa. With respect to the\nBoltzmann energy function of Eq. 7, the DBM corresponds\nto setting U = 0 and a sparse connectivity structure in both V\nand W. We can make the structure of the DBM more explicit\nby specifying its energy function. For the model with two\nhidden layers it is given as:\nEDBM\nθ\n(v, h(1), h(2); θ) = −vT Wh(1) −h(1) T V h(2)−\nd(1) T h(1) −d(2) T h(2) −bT v,\n(29)\nwith θ = {W, V, d(1), d(2), b}. The DBM can also be char-\nacterized as a bipartite graph between two sets of vertices,\nformed by odd and even-numbered layers (with v := h(0)).\n10.2.1\nMean-ﬁeld approximate inference\nA key point of departure from the RBM is that the pos-\nterior distribution over the hidden units (given the visibles)\nis no longer tractable, due to the interactions between the\nhidden units. Salakhutdinov and Hinton (2009) resort to a\nmean-ﬁeld approximation to the posterior. Speciﬁcally, in\nthe case of a model with two hidden layers, we wish to\napproximate P\n\u0000h(1), h(2) | v\n\u0001\nwith the factored distribution\nQv(h(1), h(2)) = QN1\nj=1 Qv\n\u0010\nh(1)\nj\n\u0011 QN2\ni=1 Qv\n\u0010\nh(2)\ni\n\u0011\n, such\nthat the KL divergence KL\n\u0000P\n\u0000h(1), h(2) | v\n\u0001\n∥Qv(h1, h2)\n\u0001\nis minimized or equivalently, that a lower bound to the log\nlikelihood is maximized:\nlog P(v) > L(Qv) ≡\nX\nh(1)\nX\nh(2)\nQv(h(1), h(2)) log\n\u0012P(v, h(1), h(2))\nQv(h(1), h(2))\n\u0013\n(30)\nMaximizing this lower-bound with respect to the mean-ﬁeld\ndistribution Qv(h1, h2) (by setting derivatives to zero) yields\nthe following mean ﬁeld update equations:\nˆh(1)\ni\n←sigmoid\n X\nj\nWjivj +\nX\nk\nVikˆh(2)\nk\n+ d(1)\ni\n!\n(31)\nˆh(2)\nk\n←sigmoid\n X\ni\nVikˆh(1)\ni\n+ d(2)\nk\n!\n(32)\n25. Joint training of all the layers of a Deep Belief Net is much more\nchallenging because of the much harder inference problem involved.\nNote how the above equations ostensibly look like a ﬁxed\npoint recurrent neural network, i.e., with constant input. In\nthe same way that an RBM can be associated with a simple\nauto-encoder, the above mean-ﬁeld update equations for the\nDBM can be associated with a recurrent auto-encoder. In that\ncase the training criterion involves the reconstruction error at\nthe last or at consecutive time steps. This type of model has\nbeen explored by Savard (2011) and Seung (1998) and shown\nto do a better job at denoising than ordinary auto-encoders.\nIterating Eq. (31-32) until convergence yields the Q param-\neters of the “variational positive phase” of Eq. 33:\nL(Qv) =EQv\nh\nlog P(v, h(1), h(2)) −log Qv(h(1), h(2))\ni\n=EQv\nh\n−EDBM\nθ\n(v, h(1), h(2)) −log Qv(h(1), h(2))\ni\n−log Zθ\n∂L(Qv)\n∂θ\n= −EQv\n\u0014∂EDBM\nθ\n(v, h(1), h(2))\n∂θ\n\u0015\n+ EP\n\u0014∂EDBM\nθ\n(v, h(1), h(2))\n∂θ\n\u0015\n(33)\nThis variational learning procedure leaves the “negative\nphase” untouched, which can thus be estimated through SML\nor Contrastive Divergence (Hinton, 2000) as in the RBM case.\n10.2.2\nTraining Deep Boltzmann Machines\nThe major difference between training a DBM and an RBM\nis that instead of maximizing the likelihood directly, we\ninstead choose parameters to maximize the lower-bound on\nthe likelihood given in Eq. 30. The SML-based algorithm for\nmaximizing this lower-bound is as follows:\n1) Clamp the visible units to a training example.\n2) Iterate over Eq. (31-32) until convergence.\n3) Generate negative phase samples v−, h(1)−and h(2)−\nthrough SML.\n4) Compute ∂L(Qv) /∂θ using the values obtained in steps\n2-3.\n5) Finally, update the model parameters with a step of\napproximate stochastic gradient ascent.\nWhile the above procedure appears to be a simple extension\nof the highly effective SML scheme for training RBMs, as we\ndemonstrate in Desjardins et al. (2012), this procedure seems\nvulnerable to falling in poor local minima which leave many\nhidden units effectively dead (not signiﬁcantly different from\nits random initialization with small norm).\nThe failure of the SML joint training strategy was noted\nby Salakhutdinov and Hinton (2009). As an alternative, they\nproposed a greedy layer-wise training strategy. This procedure\nconsists in pre-training the layers of the DBM, in much the\nsame way as the Deep Belief Network: i.e. by stacking RBMs\nand training each layer to independently model the output of\nthe previous layer. A ﬁnal joint “ﬁne-tuning” is done following\nthe above SML-based procedure.\n11\nBUILDING-IN INVARIANCE\nIt is well understood that incorporating prior domain knowl-\nedge helps machine learning. Exploring good strategies for\ndoing so is a very important research avenue. However, if we\nare to advance our understanding of core machine learning\nprinciples, it is important that we keep comparisons between\npredictors fair and maintain a clear awareness of the prior\n23\ndomain knowledge used by different learning algorithms,\nespecially when comparing their performance on benchmark\nproblems. We have so far only presented algorithms that\nexploited only generic inductive biases for high dimensional\nproblems, thus making them potentially applicable to any\nhigh dimensional problem. The most prevalent approach to\nincorporating prior knowledge is to hand-design better features\nto feed a generic classiﬁer, and has been used extensively in\ncomputer vision (e.g. (Lowe, 1999)). Here, we rather focus\non how basic domain knowledge of the input, in particular\nits topological structure (e.g. bitmap images having a 2D\nstructure), may be used to learn better features.\n11.1\nGenerating transformed examples\nGeneralization performance is usually improved by providing\na larger quantity of representative data. This can be achieved\nby generating new examples by applying small random defor-\nmations to the original training examples, using deformations\nthat are known not to change the target variables of interest,\ne.g., an object class is invariant to small transformations of\nimages such as translations, rotations, scaling, or shearing.\nThis old approach (Baird, 1990) has been recently applied with\ngreat success in the work of Ciresan et al. (2010) who used an\nefﬁcient GPU implementation (40× speedup) to train a stan-\ndard but large deep multilayer Perceptron on deformed MNIST\ndigits. Using both afﬁne and elastic deformations (Simard\net al., 2003), with plain old stochastic gradient descent, they\nreach a record 0.32% classiﬁcation error rate.\n11.2\nConvolution and pooling\nAnother powerful approach is based on even more basic\nknowledge of merely the topological structure of the input\ndimensions. By this we mean e.g., the 2D layout of pixels\nin images or audio spectrograms, the 3D structure of videos,\nthe 1D sequential structure of text or of temporal sequences\nin general. Based on such structure, one can deﬁne local\nreceptive ﬁelds (Hubel and Wiesel, 1959), so that each low-\nlevel feature will be computed from only a subset of the input:\na neighborhood in the topology (e.g. a sub-image at a given\nposition). This topological locality constraint corresponds to a\nlayer having a very sparse weight matrix with non-zeros only\nallowed for topologically local connections. Computing the\nassociated matrix products can of course be made much more\nefﬁcient than having to handle a dense matrix, in addition\nto the statistical gain from a much smaller number of free\nparameters. In domains with such topological structure, similar\ninput patterns are likely to appear at different positions, and\nnearby values (e.g. consecutive frames or nearby pixels) are\nlikely to have stronger dependencies that are also important to\nmodel the data. In fact these dependencies can be exploited\nto discover the topology (Le Roux et al., 2008a), i.e. recover\na regular grid of pixels out of a set of vectors without any\norder information, e.g. after the elements have been arbitrarily\nshufﬂed in the same way for all examples. Thus a same local\nfeature computation is likely to be relevant at all translated po-\nsitions of the receptive ﬁeld. Hence the idea of sweeping such\na local feature extractor over the topology: this corresponds to\na convolution, and transforms an input into a similarly shaped\nfeature map. Equivalently to sweeping, this may be seen as\nstatic but differently positioned replicated feature extractors\nthat all share the same parameters. This is at the heart of\nconvolutional networks (LeCun et al., 1989, 1998b) which\nhave been applied both to object recognition and to image\nsegmentation (Turaga et al., 2010). Another hallmark of the\nconvolutional architecture is that values computed by the same\nfeature detector applied at several neighboring input locations\nare then summarized through a pooling operation, typically\ntaking their max or their sum. This confers the resulting pooled\nfeature layer some degree of invariance to input translations,\nand this style of architecture (alternating selective feature\nextraction and invariance-creating pooling) has been the ba-\nsis of convolutional networks, the Neocognitron (Fukushima,\n1980) and HMAX (Riesenhuber and Poggio, 1999) models,\nand argued to be the architecture used by mammalian brains\nfor object recognition (Riesenhuber and Poggio, 1999; Serre\net al., 2007; DiCarlo et al., 2012). The output of a pooling\nunit will be the same irrespective of where a speciﬁc feature\nis located inside its pooling region. Empirically the use of\npooling seems to contribute signiﬁcantly to improved classi-\nﬁcation accuracy in object classiﬁcation tasks (LeCun et al.,\n1998b; Boureau et al., 2010, 2011). A successful variant of\npooling connected to sparse coding is L2 pooling (Hyv¨arinen\net al., 2009; Kavukcuoglu et al., 2009; Le et al., 2010),\nfor which the pool output is the square root of the possibly\nweighted sum of squares of ﬁlter outputs. Ideally, we would\nlike to generalize feature-pooling so as to learn what features\nshould be pooled together, e.g. as successfully done in several\npapers (Hyv¨arinen and Hoyer, 2000; Kavukcuoglu et al., 2009;\nLe et al., 2010; Ranzato and Hinton, 2010; Courville et al.,\n2011b; Coates and Ng, 2011b; Gregor et al., 2011). In this\nway, the pool output learns to be invariant to the variations\ncaptured by the span of the features pooled.\nPatch-based training\nThe simplest approach for learning a convolutional layer in an\nunsupervised fashion is patch-based training: simply feeding\na generic unsupervised feature learning algorithm with local\npatches extracted at random positions of the inputs. The\nresulting feature extractor can then be swiped over the input to\nproduce the convolutional feature maps. That map may be used\nas a new input for the next layer, and the operation repeated\nto thus learn and stack several layers. Such an approach\nwas recently used with Independent Subspace Analysis (Le\net al., 2011c) on 3D video blocks, reaching the state-of-the-art\non Hollywood2, UCF, KTH and YouTube action recognition\ndatasets. Similarly (Coates and Ng, 2011a) compared several\nfeature learners with patch-based training and reached state-\nof-the-art results on several classiﬁcation benchmarks. Inter-\nestingly, in this work performance was almost as good with\nvery simple k-means clustering as with more sophisticated\nfeature learners. We however conjecture that this is the case\nonly because patches are rather low dimensional (compared\nto the dimension of a whole image). A large dataset might\nprovide sufﬁcient coverage of the space of e.g. edges prevalent\nin 6 × 6 patches, so that a distributed representation is not\nabsolutely necessary. Another plausible explanation for this\n24\nsuccess is that the clusters identiﬁed in each image patch are\nthen pooled into a histogram of cluster counts associated with\na larger sub-image. Whereas the output of a regular clustering\nis a one-hot non-distributed code, this histogram is itself a\ndistributed representation, and the “soft” k-means (Coates and\nNg, 2011a) representation allows not only the nearest ﬁlter but\nalso its neighbors to be active.\nConvolutional and tiled-convolutional training\nIt is possible to directly train large convolutional layers using\nan unsupervised criterion. An early approach (Jain and Seung,\n2008) trained a standard but deep convolutional MLP on\nthe task of denoising images, i.e. as a deep, convolutional,\ndenoising auto-encoder. Convolutional versions of the RBM\nor its extensions have also been developed (Desjardins and\nBengio, 2008; Lee et al., 2009a; Taylor et al., 2010) as well as\na probabilistic max-pooling operation built into Convolutional\nDeep Networks (Lee et al., 2009a,b; Krizhevsky, 2010). Other\nunsupervised feature learning approaches that were adapted to\nthe convolutional setting include PSD (Kavukcuoglu et al.,\n2009, 2010; Jarrett et al., 2009; Henaff et al., 2011), a\nconvolutional version of sparse coding called deconvolutional\nnetworks (Zeiler et al., 2010), Topographic ICA (Le et al.,\n2010), and mPoT that Kivinen and Williams (2012) applied\nto modeling natural textures. Gregor and LeCun (2010a);\nLe et al. (2010) also demonstrated the technique of tiled-\nconvolution, where parameters are shared only between feature\nextractors whose receptive ﬁelds are k steps away (so the\nones looking at immediate neighbor locations are not shared).\nThis allows pooling units to be invariant to more than just\ntranslations, and is a hybrid between convolutional networks\nand earlier neural networks with local connections but no\nweight sharing (LeCun, 1986, 1989).\nAlternatives to pooling\nAlternatively, one can also use explicit knowledge of the\nexpected invariants expressed mathematically to deﬁne trans-\nformations that are robust to a known family of input defor-\nmations, using so-called scattering operators (Mallat, 2012;\nBruna and Mallat, 2011), which can be computed in a way\ninterestingly analogous to deep convolutional networks and\nwavelets. Like convolutional networks, the scattering operators\nalternate two types of operations: convolution and pooling\n(as a norm). Unlike convolutional networks, the proposed\napproach keeps at each level all of the information about the\ninput (in a way that can be inverted), and automatically yields\na very sparse (but very high-dimensional) representation. An-\nother difference is that the ﬁlters are not learned but instead\nset so as to guarantee that a priori speciﬁed invariances are\nrobustly achieved. Just a few levels were sufﬁcient to achieve\nimpressive results on several benchmark datasets.\n11.3\nTemporal coherence and slow features\nThe principle of identifying slowly moving/changing factors in\ntemporal/spatial data has been investigated by many (Becker\nand Hinton, 1992; Wiskott and Sejnowski, 2002; Hurri and\nHyv¨arinen, 2003; K¨ording et al., 2004; Cadieu and Olshausen,\n2009) as a principle for ﬁnding useful representations. In\nparticular this idea has been applied to image sequences and as\nan explanation for why V1 simple and complex cells behave\nthe way they do. A good overview can be found in Hurri and\nHyv¨arinen (2003); Berkes and Wiskott (2005).\nMore recently, temporal coherence has been successfully\nexploited in deep architectures to model video (Mobahi et al.,\n2009). It was also found that temporal coherence discov-\nered visual features similar to those obtained by ordinary\nunsupervised feature learning (Bergstra and Bengio, 2009),\nand a temporal coherence penalty has been combined with a\ntraining criterion for unsupervised feature learning (Zou et al.,\n2011), sparse auto-encoders with L1 regularization, in this\ncase, yielding improved classiﬁcation performance.\nThe temporal coherence prior can be expressed in several\nways, the simplest being the squared difference between\nfeature values at times t and t + 1. Other plausible tempo-\nral coherence priors include the following. First, instead of\npenalizing the squared change, penalizing the absolute value\n(or a similar sparsity penalty) would state that most of the\ntime the change should be exactly 0, which would intuitively\nmake sense for the real-life factors that surround us. Second,\none would expect that instead of just being slowly changing,\ndifferent factors could be associated with their own different\ntime scale. The speciﬁcity of their time scale could thus\nbecome a hint to disentangle explanatory factors. Third, one\nwould expect that some factors should really be represented by\na group of numbers (such as x, y, and z position of some object\nin space and the pose parameters of\nHinton et al. (2011))\nrather than by a single scalar, and that these groups tend\nto move together. Structured sparsity penalties (Kavukcuoglu\net al., 2009; Jenatton et al., 2009; Bach et al., 2011; Gregor\net al., 2011) could be used for this purpose.\n11.4\nAlgorithms to Disentangle Factors of Variation\nThe goal of building invariant features is to remove sensitivity\nof the representation to directions of variance in the data that\nare uninformative to the task at hand. However it is often the\ncase that the goal of feature extraction is the disentangling or\nseparation of many distinct but informative factors in the data,\ne.g., in a video of people: subject identity, action performed,\nsubject pose relative to the camera, etc. In this situation,\nthe methods of generating invariant features, such as feature-\npooling, may be inadequate.\nThe process of building invariant features can be seen as\nconsisting of two steps. First, low-level features are recovered\nthat account for the data. Second, subsets of these low level\nfeatures are pooled together to form higher-level invariant\nfeatures, exempliﬁed by the pooling and subsampling layers\nof convolutional neural networks. The invariant representation\nformed by the pooling features offers an incomplete window\non the data as the detailed representation of the lower-level\nfeatures is abstracted away in the pooling procedure. While\nwe would like higher-level features to be more abstract and\nexhibit greater invariance, we have little control over what\ninformation is lost through pooling. What we really would like\nis for a particular feature set to be invariant to the irrelevant\nfeatures and disentangle the relevant features. Unfortunately,\nit is often difﬁcult to determine a priori which set of features\nwill ultimately be relevant to the task at hand.\n25\nAn interesting approach to taking advantage of some of\nthe factors of variation known to exist in the data is the\ntransforming auto-encoder (Hinton et al., 2011): instead of a\nscalar pattern detector (e.g,. corresponding to the probability\nof presence of a particular form in the input) one can think\nof the features as organized in groups that include both a\npattern detector and pose parameters that specify attributes\nof the detected pattern. In (Hinton et al., 2011), what is\nassumed a priori is that pairs of examples (or consecutive ones)\nare observed with an associated value for the corresponding\nchange in the pose parameters. For example, an animal that\ncontrols its eyes knows what changes to its ocular motor\nsystem were applied when going from one image on its retina\nto the next. In that work, it is also assumed that the pose\nchanges are the same for all the pattern detectors, and this\nmakes sense for global changes such as image translation and\ncamera geometry changes. Instead, we would like to discover\nthe pose parameters and attributes that should be associated\nwith each feature detector, without having to specify ahead of\ntime what they should be, force them to be the same for all\nfeatures, and having to necessarily observe the changes in all\nof the pose parameters or attributes.\nThe approach taken recently in the Manifold Tangent Clas-\nsiﬁer, discussed in section 8.3, is interesting in this respect.\nWithout any supervision or prior knowledge, it ﬁnds prominent\nlocal factors of variation (tangent vectors to the manifold,\nextracted from a CAE, interpreted as locally valid input ”defor-\nmations”). Higher-level features are subsequently encouraged\nto be invariant to these factors of variation, so that they must\ndepend on other characteristics. In a sense this approach is\ndisentangling valid local deformations along the data manifold\nfrom other, more drastic changes, associated to other factors\nof variation such as those that affect class identity.26\nOne solution to the problem of information loss that would\nﬁt within the feature-pooling paradigm, is to consider many\noverlapping pools of features based on the same low-level\nfeature set. Such a structure would have the potential to\nlearn a redundant set of invariant features that may not cause\nsigniﬁcant loss of information. However it is not obvious\nwhat learning principle could be applied that can ensure\nthat the features are invariant while maintaining as much\ninformation as possible. While a Deep Belief Network or a\nDeep Boltzmann Machine (as discussed in sections 4 and 10.2\nrespectively) with two hidden layers would, in principle, be\nable to preserve information into the “pooling” second hidden\nlayer, there is no guarantee that the second layer features\nare more invariant than the “low-level” ﬁrst layer features.\nHowever, there is some empirical evidence that the second\nlayer of the DBN tends to display more invariance than the\nﬁrst layer (Erhan et al., 2010a).\nA more principled approach, from the perspective of en-\nsuring a more robust compact feature representation, can\nbe conceived by reconsidering the disentangling of features\nthrough the lens of its generative equivalent – feature com-\nposition. Since many unsupervised learning algorithms have a\n26. The changes that affect class identity might, in input space, actually be\nof similar magnitude to local deformations, but not follow along the manifold,\ni.e. cross zones of low density.\ngenerative interpretation (or a way to reconstruct inputs from\ntheir high-level representation), the generative perspective can\nprovide insight into how to think about disentangling fac-\ntors. The majority of the models currently used to construct\ninvariant features have the interpretation that their low-level\nfeatures linearly combine to construct the data.27 This is a\nfairly rudimentary form of feature composition with signiﬁcant\nlimitations. For example, it is not possible to linearly combine\na feature with a generic transformation (such as translation) to\ngenerate a transformed version of the feature. Nor can we even\nconsider a generic color feature being linearly combined with\na gray-scale stimulus pattern to generate a colored pattern. It\nwould seem that if we are to take the notion of disentangling\nseriously we require a richer interaction of features than that\noffered by simple linear combinations.\n12\nCONCLUSION\nThis review of representation learning and deep learning has\ncovered three major and apparently disconnected approaches:\nthe probabilistic models (both the directed kind such as\nsparse coding and the undirected kind such as Boltzmann\nmachines), the reconstruction-based algorithms related to auto-\nencoders, and the geometrically motivated manifold-learning\napproaches. Drawing connections between these approaches\nis currently a very active area of research and is likely to\ncontinue to produce models and methods that take advantage\nof the relative strengths of each paradigm.\nPractical Concerns and Guidelines. One of the criticisms\naddressed to artiﬁcial neural networks and deep learning algo-\nrithms is that they have many hyper-parameters and variants\nand that exploring their conﬁgurations and architectures is an\nart. This has motivated an earlier book on the “Tricks of the\nTrade” (Orr and Muller, 1998) of which LeCun et al. (1998a)\nis still relevant for training deep architectures, in particular\nwhat concerns initialization, ill-conditioning and stochastic\ngradient descent. A good and more modern compendium of\ngood training practice, particularly adapted to training RBMs,\nis provided in Hinton (2010), while a similar guide oriented\nmore towards deep neural networks can be found in Bengio\n(2013), both of which are part of a novel version of the\nabove book. Recent work on automating hyper-parameter\nsearch (Bergstra and Bengio, 2012; Bergstra et al., 2011;\nSnoek et al., 2012) is also making it more convenient, efﬁcient\nand reproducible.\nIncorporating Generic AI-level Priors. We have covered\nmany high-level generic priors that we believe could bring\nmachine learning closer to AI by improving representation\nlearning. Many of these priors relate to the assumed existence\nof multiple underlying factors of variation, whose variations\nare in some sense orthogonal to each other. They are expected\nto be organized at multiple levels of abstraction, hence the\nneed for deep architectures, which also have statistical advan-\ntages because they allow to re-use parameters in a combi-\nnatorially efﬁcient way. Only a few of these factors would\n27. As an aside, if we are given only the values of the higher-level pooling\nfeatures, we cannot accurately recover the data because we do not know how to\napportion credit for the pooling feature values to the lower-level features. This\nis simply the generative version of the consequences of the loss of information\ncaused by pooling.\n26\ntypically be relevant for any particular example, justifying\nsparsity of representation. These factors are expected to be\nrelated to simple (e.g., linear) dependencies, with subsets of\nthese explaining different random variables of interest (inputs,\ntasks) and varying in structured ways in time and space\n(temporal and spatial coherence). We expect future successful\napplications of representation learning to reﬁne and increase\nthat list of priors, and to incorporate most of them instead of\nfocusing on only one. Research in training criteria that better\ntake these priors into account are likely to move us closer to\nthe long-term objective of discovering learning algorithms that\ncan disentangle the underlying explanatory factors.\nInference. We anticipate that methods based on directly\nparametrizing a representation function will incorporate more\nand more of the iterative type of computation one ﬁnds in the\ninference procedures of probabilistic latent-variable models.\nThere is already movement in the other direction, with prob-\nabilistic latent-variable models exploiting approximate infer-\nence mechanisms that are themselves learned (i.e., producing a\nparametric description of the representation function). A major\nappeal of probabilistic models is that the semantics of the\nlatent variables are clear and this allows a clean separation\nof the problems of modeling (choose the energy function),\ninference (estimating P(h|x)), and learning (optimizing the\nparameters), using generic tools in each case. On the other\nhand, doing approximate inference and not taking that approxi-\nmation into account explicitly in the approximate optimization\nfor learning could have detrimental effects, hence the appeal\nof learning approximate inference. More fundamentally, there\nis the question of the multimodality of the posterior P(h|x).\nIf there are exponentially many probable conﬁgurations of\nvalues of the factors hi that can explain x, then we seem\nto be stuck with very poor inference, either focusing on a\nsingle mode (MAP inference), assuming some kind of strong\nfactorization (as in variational inference) or using an MCMC\nthat cannot visit enough modes of P(h|x). What we propose\nas food for thought is the idea of dropping the requirement\nof an explicit representation of the posterior and settle for\nan implicit representation that exploits potential structure in\nP(h|x) in order to represent it compactly: even though P(h|x)\nmay have an exponential number of modes, it may be possible\nto represent it with a small set of numbers. For example,\nconsider computing a deterministic feature representation f(x)\nthat implicitly captures the information about a highly multi-\nmodal P(h|x), in the sense that all the questions (e.g. making\nsome prediction about some target concept) that can be asked\nfrom P(h|x) can also be answered from f(x).\nOptimization. Much remains to be done to better under-\nstand the successes and failures of training deep architectures,\nboth in the supervised case (with many recent successes) and\nthe unsupervised case (where much more work needs to be\ndone). Although regularization effects can be important on\nsmall datasets, the effects that persist on very large datasets\nsuggest some optimization issues are involved. Are they more\ndue to local minima (we now know there are huge numbers\nof them) and the dynamics of the training procedure? Or\nare they due mostly to ill-conditioning and may be handled\nby approximate second-order methods? These basic questions\nremain unanswered and deserve much more study.\nAcknowledgments\nThe author would like to thank David Warde-Farley, Razvan\nPascanu and Ian Goodfellow for useful feedback, as well as\nNSERC, CIFAR and the Canada Research Chairs for funding.\nREFERENCES\nAlain, G. and Bengio, Y. (2012). What regularized auto-encoders\nlearn from the data generating distribution. Technical Report Arxiv\nreport 1211.4246, Universit´e de Montr´eal.\nAmari, S. (1998).\nNatural gradient works efﬁciently in learning.\nNeural Computation, 10(2), 251–276.\nBach, F., Jenatton, R., Mairal, J., and Obozinski, G. (2011). Struc-\ntured sparsity through convex optimization. CoRR, abs/1109.2397.\nBagnell, J. A. and Bradley, D. M. (2009).\nDifferentiable sparse\ncoding. In NIPS’2009, pages 113–120.\nBaird, H. (1990). Document image defect models. In IAPR Workshop,\nSyntactic & Structural Patt. Rec., pages 38–46.\nBecker, S. and Hinton, G. (1992). A self-organizing neural network\nthat discovers surfaces in random-dot stereograms. Nature, 355,\n161–163.\nBelkin, M. and Niyogi, P. (2003). Laplacian eigenmaps for dimen-\nsionality reduction and data representation. Neural Computation,\n15(6), 1373–1396.\nBell, A. and Sejnowski, T. J. (1997). The independent components\nof natural scenes are edge ﬁlters. Vision Research, 37, 3327–3338.\nBengio, Y. (1993). A connectionist approach to speech recognition.\nInternational Journal on Pattern Recognition and Artiﬁcial Intel-\nligence, 7(4), 647–668.\nBengio, Y. (2008). Neural net language models. Scholarpedia, 3(1).\nBengio, Y. (2009). Learning deep architectures for AI. Foundations\nand Trends in Machine Learning, 2(1), 1–127. Also published as\na book. Now Publishers, 2009.\nBengio, Y. (2011). Deep learning of representations for unsupervised\nand transfer learning. In JMLR W&CP: Proc. Unsupervised and\nTransfer Learning.\nBengio, Y. (2013).\nPractical recommendations for gradient-based\ntraining of deep architectures. In K.-R. M¨uller, G. Montavon, and\nG. B. Orr, editors, Neural Networks: Tricks of the Trade. Springer.\nBengio, Y. and Delalleau, O. (2009).\nJustifying and generalizing\ncontrastive divergence. Neural Computation, 21(6), 1601–1621.\nBengio, Y. and Delalleau, O. (2011). On the expressive power of\ndeep architectures. In ALT’2011.\nBengio, Y. and LeCun, Y. (2007). Scaling learning algorithms towards\nAI. In L. Bottou, O. Chapelle, D. DeCoste, and J. Weston, editors,\nLarge Scale Kernel Machines. MIT Press.\nBengio, Y. and Monperrus, M. (2005). Non-local manifold tangent\nlearning. In NIPS’2004, pages 129–136. MIT Press.\nBengio, Y., Simard, P., and Frasconi, P. (1994). Learning long-term\ndependencies with gradient descent is difﬁcult. IEEE Transactions\non Neural Networks, 5(2), 157–166.\nBengio, Y., Ducharme, R., Vincent, P., and Jauvin, C. (2003).\nA\nneural probabilistic language model. JMLR, 3, 1137–1155.\nBengio, Y., Paiement, J.-F., Vincent, P., Delalleau, O., Le Roux,\nN., and Ouimet, M. (2004). Out-of-sample extensions for LLE,\nIsomap, MDS, Eigenmaps, and Spectral Clustering. In NIPS’2003.\nBengio, Y., Delalleau, O., and Le Roux, N. (2006a). The curse of\nhighly variable functions for local kernel machines. In NIPS’2005.\nBengio, Y., Larochelle, H., and Vincent, P. (2006b).\nNon-local\nmanifold Parzen windows. In NIPS’2005. MIT Press.\nBengio, Y., Lamblin, P., Popovici, D., and Larochelle, H. (2007).\nGreedy layer-wise training of deep networks. In NIPS’2006.\nBengio, Y., Louradour, J., Collobert, R., and Weston, J. (2009).\nCurriculum learning. In ICML’09.\nBengio, Y., Delalleau, O., and Simard, C. (2010).\nDecision trees\ndo not generalize to new variations. Computational Intelligence,\n26(4), 449–467.\n27\nBengio, Y., Alain, G., and Rifai, S. (2012). Implicit density esti-\nmation by local moment matching to sample from auto-encoders.\nTechnical report, arXiv:1207.0057.\nBengio, Y., Mesnil, G., Dauphin, Y., and Rifai, S. (2013). Better\nmixing via deep representations. In ICML’2013.\nBergstra, J. and Bengio, Y. (2009). Slow, decorrelated features for\npretraining complex cell-like networks. In NIPS’2009.\nBergstra, J. and Bengio, Y. (2012).\nRandom search for hyper-\nparameter optimization. J. Machine Learning Res., 13, 281–305.\nBergstra, J., Bardenet, R., Bengio, Y., and K´egl, B. (2011). Algo-\nrithms for hyper-parameter optimization. In NIPS’2011.\nBerkes, P. and Wiskott, L. (2005). Slow feature analysis yields a\nrich repertoire of complex cell properties. Journal of Vision, 5(6),\n579–602.\nBesag, J. (1975).\nStatistical analysis of non-lattice data.\nThe\nStatistician, 24(3), 179–195.\nBordes, A., Glorot, X., Weston, J., and Bengio, Y. (2012).\nJoint\nlearning of words and meaning representations for open-text\nsemantic parsing. AISTATS’2012.\nBoulanger-Lewandowski, N., Bengio, Y., and Vincent, P. (2012).\nModeling temporal dependencies in high-dimensional sequences:\nApplication to polyphonic music generation and transcription. In\nICML’2012.\nBoureau, Y., Ponce, J., and LeCun, Y. (2010). A theoretical analysis\nof feature pooling in vision algorithms. In ICML’10.\nBoureau, Y., Le Roux, N., Bach, F., Ponce, J., and LeCun, Y. (2011).\nAsk the locals: multi-way local pooling for image recognition. In\nICCV’11.\nBourlard, H. and Kamp, Y. (1988). Auto-association by multilayer\nperceptrons and singular value decomposition. Biological Cyber-\nnetics, 59, 291–294.\nBrand, M. (2003). Charting a manifold. In NIPS’2002, pages 961–\n968. MIT Press.\nBreuleux, O., Bengio, Y., and Vincent, P. (2011). Quickly generating\nrepresentative samples from an RBM-derived process.\nNeural\nComputation, 23(8), 2053–2073.\nBruna, J. and Mallat, S. (2011).\nClassiﬁcation with scattering\noperators. In ICPR’2011.\nCadieu, C. and Olshausen, B. (2009).\nLearning transformational\ninvariants from natural movies.\nIn NIPS’2009, pages 209–216.\nMIT Press.\nCarreira-Perpi˜nan, M. A. and Hinton, G. E. (2005). On contrastive\ndivergence learning. In AISTATS’2005, pages 33–40.\nChen, M., Xu, Z., Winberger, K. Q., and Sha, F. (2012). Marginalized\ndenoising autoencoders for domain adaptation. In ICML’2012.\nCho, K., Raiko, T., and Ilin, A. (2010). Parallel tempering is efﬁcient\nfor learning restricted Boltzmann machines. In IJCNN’2010.\nCho, K., Raiko, T., and Ilin, A. (2011).\nEnhanced gradient and\nadaptive learning rate for training restricted Boltzmann machines.\nIn ICML’2011, pages 105–112.\nCiresan, D., Meier, U., and Schmidhuber, J. (2012). Multi-column\ndeep neural networks for image classiﬁcation. Technical report,\narXiv:1202.2745.\nCiresan, D. C., Meier, U., Gambardella, L. M., and Schmidhuber,\nJ. (2010).\nDeep big simple neural nets for handwritten digit\nrecognition. Neural Computation, 22, 1–14.\nCoates, A. and Ng, A. Y. (2011a). The importance of encoding versus\ntraining with sparse coding and vector quantization. In ICML’2011.\nCoates, A. and Ng, A. Y. (2011b). Selecting receptive ﬁelds in deep\nnetworks. In NIPS’2011.\nCollobert, R. and Weston, J. (2008).\nA uniﬁed architecture for\nnatural language processing: Deep neural networks with multitask\nlearning. In ICML’2008.\nCollobert, R., Weston, J., Bottou, L., Karlen, M., Kavukcuoglu, K.,\nand Kuksa, P. (2011). Natural language processing (almost) from\nscratch. Journal of Machine Learning Research, 12, 2493–2537.\nCourville, A., Bergstra, J., and Bengio, Y. (2011a). A spike and slab\nrestricted Boltzmann machine. In AISTATS’2011.\nCourville, A., Bergstra, J., and Bengio, Y. (2011b). Unsupervised\nmodels of images by spike-and-slab RBMs. In ICML’2011.\nDahl, G. E., Ranzato, M., Mohamed, A., and Hinton, G. E. (2010).\nPhone recognition with the mean-covariance restricted Boltzmann\nmachine. In NIPS’2010.\nDahl, G. E., Yu, D., Deng, L., and Acero, A. (2012).\nContext-\ndependent pre-trained deep neural networks for large vocabulary\nspeech recognition.\nIEEE Transactions on Audio, Speech, and\nLanguage Processing, 20(1), 33–42.\nDeng, L., Seltzer, M., Yu, D., Acero, A., Mohamed, A., and Hinton,\nG. (2010). Binary coding of speech spectrograms using a deep\nauto-encoder. In Interspeech 2010, Makuhari, Chiba, Japan.\nDesjardins, G. and Bengio, Y. (2008).\nEmpirical evaluation of\nconvolutional RBMs for vision.\nTechnical Report 1327, Dept.\nIRO, U. Montr´eal.\nDesjardins, G., Courville, A., Bengio, Y., Vincent, P., and Delalleau,\nO. (2010). Tempered Markov chain Monte Carlo for training of\nrestricted Boltzmann machine. In AISTATS’2010, volume 9, pages\n145–152.\nDesjardins, G., Courville, A., and Bengio, Y. (2011). On tracking the\npartition function. In NIPS’2011.\nDesjardins, G., Courville, A., and Bengio, Y. (2012). On training\ndeep Boltzmann machines. Technical Report arXiv:1203.4416v1,\nUniversit´e de Montr´eal.\nDiCarlo, J., Zoccolan, D., and Rust, N. (2012). How does the brain\nsolve visual object recognition? Neuron.\nDonoho, D. L. and Grimes, C. (2003).\nHessian eigenmaps: new\nlocally linear embedding techniques for high-dimensional data.\nTechnical Report 2003-08, Dept. Statistics, Stanford University.\nEisner, J. (2012).\nLearning approximate inference policies for\nfast prediction.\nKeynote talk at ICML Workshop on Inferning:\nInteractions Between Search and Learning.\nErhan, D., Courville, A., and Bengio, Y. (2010a).\nUnderstanding\nrepresentations learned in deep architectures.\nTechnical Report\n1355, Universit´e de Montr´eal/DIRO.\nErhan, D., Bengio, Y., Courville, A., Manzagol, P.-A., Vincent, P.,\nand Bengio, S. (2010b). Why does unsupervised pre-training help\ndeep learning? Journal of Machine Learning Research, 11, 625–\n660.\nFreund, Y. and Haussler, D. (1994).\nUnsupervised learning of\ndistributions on binary vectors using two layer networks. Technical\nReport UCSC-CRL-94-25, University of California, Santa Cruz.\nFukushima, K. (1980).\nNeocognitron: A self-organizing neural\nnetwork model for a mechanism of pattern recognition unaffected\nby shift in position. Biological Cybernetics, 36, 193–202.\nGlorot, X. and Bengio, Y. (2010). Understanding the difﬁculty of\ntraining deep feedforward neural networks. In AISTATS’2010.\nGlorot, X., Bordes, A., and Bengio, Y. (2011a). Deep sparse rectiﬁer\nneural networks. In AISTATS’2011.\nGlorot, X., Bordes, A., and Bengio, Y. (2011b). Domain adaptation\nfor large-scale sentiment classiﬁcation: A deep learning approach.\nIn ICML’2011.\nGoodfellow, I., Le, Q., Saxe, A., and Ng, A. (2009).\nMeasuring\ninvariances in deep networks. In NIPS’2009, pages 646–654.\nGoodfellow, I., Courville, A., and Bengio, Y. (2011).\nSpike-and-\nslab sparse coding for unsupervised feature discovery. In NIPS\nWorkshop on Challenges in Learning Hierarchical Models.\nGoodfellow, I. J., Courville, A., and Bengio, Y. (2012).\nSpike-\nand-slab\nsparse\ncoding\nfor\nunsupervised\nfeature\ndiscovery.\narXiv:1201.3382.\nGregor, K. and LeCun, Y. (2010a). Emergence of complex-like cells\nin a temporal product network with local receptive ﬁelds. Technical\nreport, arXiv:1006.0448.\nGregor, K. and LeCun, Y. (2010b). Learning fast approximations of\nsparse coding. In ICML’2010.\nGregor, K., Szlam, A., and LeCun, Y. (2011).\nStructured sparse\ncoding via lateral inhibition. In NIPS’2011.\nGribonval, R. (2011). Should penalized least squares regression be\ninterpreted as Maximum A Posteriori estimation? IEEE Transac-\ntions on Signal Processing, 59(5), 2405–2410.\n28\nGrosse, R., Raina, R., Kwong, H., and Ng, A. Y. (2007).\nShift-\ninvariant sparse coding for audio classiﬁcation. In UAI’2007.\nGrubb, A. and Bagnell, J. A. D. (2010). Boosted backpropagation\nlearning for training deep modular networks. In ICML’2010.\nGutmann, M. and Hyvarinen, A. (2010). Noise-contrastive estimation:\nA new estimation principle for unnormalized statistical models. In\nAISTATS’2010.\nHamel, P., Lemieux, S., Bengio, Y., and Eck, D. (2011). Temporal\npooling and multiscale learning for automatic annotation and\nranking of music audio. In ISMIR.\nH˚astad, J. (1986).\nAlmost optimal lower bounds for small depth\ncircuits. In STOC’86, pages 6–20.\nH˚astad, J. and Goldmann, M. (1991). On the power of small-depth\nthreshold circuits. Computational Complexity, 1, 113–129.\nHenaff, M., Jarrett, K., Kavukcuoglu, K., and LeCun, Y. (2011).\nUnsupervised learning of sparse features for scalable audio clas-\nsiﬁcation. In ISMIR’11.\nHinton, G., Krizhevsky, A., and Wang, S. (2011). Transforming auto-\nencoders. In ICANN’2011.\nHinton, G., Deng, L., Dahl, G. E., Mohamed, A., Jaitly, N., Senior, A.,\nVanhoucke, V., Nguyen, P., Sainath, T., and Kingsbury, B. (2012).\nDeep neural networks for acoustic modeling in speech recognition.\nIEEE Signal Processing Magazine, 29(6), 82–97.\nHinton, G. E. (1986). Learning distributed representations of con-\ncepts. In Proc. 8th Conf. Cog. Sc. Society, pages 1–12.\nHinton, G. E. (1999). Products of experts. In ICANN’1999.\nHinton, G. E. (2000). Training products of experts by minimizing\ncontrastive divergence.\nTechnical Report GCNU TR 2000-004,\nGatsby Unit, University College London.\nHinton, G. E. (2010).\nA practical guide to training restricted\nBoltzmann machines.\nTechnical Report UTML TR 2010-003,\nDepartment of Computer Science, University of Toronto.\nHinton, G. E. and Roweis, S. (2003). Stochastic neighbor embedding.\nIn NIPS’2002.\nHinton, G. E. and Salakhutdinov, R. (2006). Reducing the dimension-\nality of data with neural networks. Science, 313(5786), 504–507.\nHinton, G. E. and Zemel, R. S. (1994).\nAutoencoders, minimum\ndescription length, and helmholtz free energy. In NIPS’1993.\nHinton, G. E., Osindero, S., and Teh, Y. (2006).\nA fast learning\nalgorithm for deep belief nets. Neural Computation, 18, 1527–\n1554.\nHubel, D. H. and Wiesel, T. N. (1959). Receptive ﬁelds of single\nneurons in the cat’s striate cortex. Journal of Physiology, 148,\n574–591.\nHurri, J. and Hyv¨arinen, A. (2003).\nTemporal coherence, natural\nimage sequences, and the visual cortex. In NIPS’2002.\nHyv¨arinen, A. (2005).\nEstimation of non-normalized statistical\nmodels using score matching. J. Machine Learning Res., 6.\nHyv¨arinen, A. (2007). Some extensions of score matching. Compu-\ntational Statistics and Data Analysis, 51, 2499–2512.\nHyv¨arinen, A. (2008). Optimal approximation of signal priors. Neural\nComputation, 20(12), 3087–3110.\nHyv¨arinen, A. and Hoyer, P. (2000).\nEmergence of phase and\nshift invariant features by decomposition of natural images into\nindependent feature subspaces. Neural Computation, 12(7).\nHyv¨arinen, A., Karhunen, J., and Oja, E. (2001a).\nIndependent\nComponent Analysis. Wiley-Interscience.\nHyv¨arinen, A., Hoyer, P. O., and Inki, M. (2001b).\nTopographic\nindependent component analysis.\nNeural Computation, 13(7),\n1527–1558.\nHyv¨arinen, A., Hurri, J., and Hoyer, P. O. (2009). Natural Image\nStatistics: A probabilistic approach to early computational vision.\nSpringer-Verlag.\nJaeger, H. (2007). Echo state network. Scholarpedia, 2(9), 2330.\nJain, V. and Seung, S. H. (2008).\nNatural image denoising with\nconvolutional networks. In NIPS’2008.\nJarrett, K., Kavukcuoglu, K., Ranzato, M., and LeCun, Y. (2009).\nWhat is the best multi-stage architecture for object recognition?\nIn ICCV’09.\nJenatton, R., Audibert, J.-Y., and Bach, F. (2009).\nStructured\nvariable selection with sparsity-inducing norms. Technical report,\narXiv:0904.3523.\nJutten, C. and Herault, J. (1991). Blind separation of sources, part I:\nan adaptive algorithm based on neuromimetic architecture. Signal\nProcessing, 24, 1–10.\nKavukcuoglu, K., Ranzato, M., and LeCun, Y. (2008). Fast inference\nin sparse coding algorithms with applications to object recognition.\nCBLL-TR-2008-12-01, NYU.\nKavukcuoglu, K., Ranzato, M.-A., Fergus, R., and LeCun, Y. (2009).\nLearning invariant features through topographic ﬁlter maps.\nIn\nCVPR’2009.\nKavukcuoglu, K., Sermanet, P., Boureau, Y.-L., Gregor, K., Math-\nieu, M., and LeCun, Y. (2010).\nLearning convolutional feature\nhierarchies for visual recognition. In NIPS’2010.\nKingma, D. and LeCun, Y. (2010). Regularized estimation of image\nstatistics by score matching. In NIPS’2010.\nKivinen, J. J. and Williams, C. K. I. (2012).\nMultiple texture\nBoltzmann machines. In AISTATS’2012.\nK¨ording, K. P., Kayser, C., Einh¨auser, W., and K¨onig, P. (2004).\nHow are complex cell properties adapted to the statistics of natural\nstimuli? J. Neurophysiology, 91.\nKrizhevsky, A. (2010).\nConvolutional deep belief networks on\nCIFAR-10. Technical report, U. Toronto.\nKrizhevsky, A. and Hinton, G. (2009). Learning multiple layers of\nfeatures from tiny images. Technical report, U. Toronto.\nKrizhevsky, A., Sutskever, I., and Hinton, G. (2012). ImageNet clas-\nsiﬁcation with deep convolutional neural networks. In NIPS’2012.\nLarochelle, H. and Bengio, Y. (2008). Classiﬁcation using discrimi-\nnative restricted Boltzmann machines. In ICML’2008.\nLarochelle, H., Bengio, Y., Louradour, J., and Lamblin, P. (2009).\nExploring strategies for training deep neural networks. Journal of\nMachine Learning Research, 10, 1–40.\nLazebnik, S., Schmid, C., and Ponce, J. (2006).\nBeyond bags of\nfeatures: Spatial pyramid matching for recognizing natural scene\ncategories. In CVPR’2006.\nLe, H.-S., Oparin, I., Allauzen, A., Gauvin, J.-L., and Yvon, F. (2013).\nStructured output layer neural network language models for speech\nrecognition. IEEE Trans. Audio, Speech & Language Processing.\nLe, Q., Ngiam, J., Chen, Z., hao Chia, D. J., Koh, P. W., and Ng, A.\n(2010). Tiled convolutional neural networks. In NIPS’2010.\nLe, Q., Ngiam, J., Coates, A., Lahiri, A., Prochnow, B., and Ng,\nA. (2011a).\nOn optimization methods for deep learning.\nIn\nICML’2011.\nLe, Q. V., Karpenko, A., Ngiam, J., and Ng, A. Y. (2011b). ICA with\nreconstruction cost for efﬁcient overcomplete feature learning. In\nNIPS’2011.\nLe, Q. V., Zou, W. Y., Yeung, S. Y., and Ng, A. Y. (2011c). Learning\nhierarchical spatio-temporal features for action recognition with\nindependent subspace analysis. In CVPR’2011.\nLe Roux, N., Bengio, Y., Lamblin, P., Joliveau, M., and Kegl, B.\n(2008a). Learning the 2-D topology of images. In NIPS’07.\nLe Roux, N., Manzagol, P.-A., and Bengio, Y. (2008b). Topmoumoute\nonline natural gradient algorithm. In NIPS’07.\nLeCun, Y. (1986). Learning processes in an asymmetric threshold\nnetwork.\nIn Disordered Systems and Biological Organization,\npages 233–240. Springer-Verlag.\nLeCun, Y. (1987). Mod`eles connexionistes de l’apprentissage. Ph.D.\nthesis, Universit´e de Paris VI.\nLeCun, Y. (1989). Generalization and network design strategies. In\nConnectionism in Perspective. Elsevier Publishers.\nLeCun, Y., Boser, B., Denker, J. S., Henderson, D., Howard, R. E.,\nHubbard, W., and Jackel, L. D. (1989). Backpropagation applied\nto handwritten zip code recognition. Neural Computation.\nLeCun, Y., Bottou, L., Orr, G. B., and M¨uller, K. (1998a). Efﬁcient\nbackprop. In Neural Networks, Tricks of the Trade.\nLeCun, Y., Bottou, L., Bengio, Y., and Haffner, P. (1998b). Gradient\nbased learning applied to document recognition. Proc. IEEE.\nLee, H., Ekanadham, C., and Ng, A. (2008). Sparse deep belief net\n29\nmodel for visual area V2. In NIPS’07.\nLee, H., Grosse, R., Ranganath, R., and Ng, A. Y. (2009a). Convolu-\ntional deep belief networks for scalable unsupervised learning of\nhierarchical representations. In ICML’2009.\nLee, H., Pham, P., Largman, Y., and Ng, A. (2009b). Unsupervised\nfeature learning for audio classiﬁcation using convolutional deep\nbelief networks. In NIPS’2009.\nLin, Y., Tong, Z., Zhu, S., and Yu, K. (2010). Deep coding network.\nIn NIPS’2010.\nLowe, D. (1999).\nObject recognition from local scale invariant\nfeatures. In ICCV’99.\nMallat, S. (2012). Group invariant scattering. Communications on\nPure and Applied Mathematics.\nMarlin, B. and de Freitas, N. (2011).\nAsymptotic efﬁciency of\ndeterministic estimators for discrete energy-based models: Ratio\nmatching and pseudolikelihood. In UAI’2011.\nMarlin, B., Swersky, K., Chen, B., and de Freitas, N. (2010).\nInductive principles for restricted Boltzmann machine learning. In\nAISTATS’2010, pages 509–516.\nMartens, J. (2010). Deep learning via Hessian-free optimization. In\nICML’2010, pages 735–742.\nMartens, J. and Sutskever, I. (2011).\nLearning recurrent neural\nnetworks with Hessian-free optimization. In ICML’2011.\nMemisevic, R. and Hinton, G. E. (2010). Learning to represent spatial\ntransformations with factored higher-order Boltzmann machines.\nNeural Comp., 22(6).\nMesnil, G., Dauphin, Y., Glorot, X., Rifai, S., Bengio, Y., Goodfellow,\nI., Lavoie, E., Muller, X., Desjardins, G., Warde-Farley, D., Vin-\ncent, P., Courville, A., and Bergstra, J. (2011). Unsupervised and\ntransfer learning challenge: a deep learning approach. In JMLR\nW&CP: Proc. Unsupervised and Transfer Learning, volume 7.\nMikolov, T., Deoras, A., Kombrink, S., Burget, L., and Cernocky,\nJ. (2011).\nEmpirical evaluation and combination of advanced\nlanguage modeling techniques. In INTERSPEECH’2011.\nMobahi, H., Collobert, R., and Weston, J. (2009). Deep learning from\ntemporal coherence in video. In ICML’2009.\nMohamed, A., Dahl, G., and Hinton, G. (2012). Acoustic modeling\nusing deep belief networks. IEEE Trans. on Audio, Speech and\nLanguage Processing, 20(1), 14–22.\nMontufar, G. F. and Morton, J. (2012).\nWhen does a mixture\nof products contain a product of mixtures?\nTechnical report,\narXiv:1206.0387.\nMurray, I. and Salakhutdinov, R. (2009).\nEvaluating probabilities\nunder high-dimensional latent variable models.\nIn NIPS’2008,\npages 1137–1144.\nNair, V. and Hinton, G. E. (2010). Rectiﬁed linear units improve\nrestricted Boltzmann machines. In ICML’10.\nNeal, R. M. (1992).\nConnectionist learning of belief networks.\nArtiﬁcial Intelligence, 56, 71–113.\nNeal, R. M. (1993).\nProbabilistic inference using Markov chain\nMonte-Carlo methods. Technical Report CRG-TR-93-1, Dept. of\nComputer Science, University of Toronto.\nNgiam, J., Chen, Z., Koh, P., and Ng, A. (2011).\nLearning deep\nenergy models. In Proc. ICML’2011. ACM.\nOlshausen, B. A. and Field, D. J. (1996).\nEmergence of simple-\ncell receptive ﬁeld properties by learning a sparse code for natural\nimages. Nature, 381, 607–609.\nOrr, G. and Muller, K.-R., editors (1998). Neural networks: tricks of\nthe trade. Lect. Notes Comp. Sc. Springer-Verlag.\nPascanu, R. and Bengio, Y. (2013).\nNatural gradient revisited.\nTechnical report, arXiv:1301.3584.\nRaiko, T., Valpola, H., and LeCun, Y. (2012). Deep learning made\neasier by linear transformations in perceptrons. In AISTATS’2012.\nRaina, R., Battle, A., Lee, H., Packer, B., and Ng, A. Y. (2007).\nSelf-taught learning: transfer learning from unlabeled data.\nIn\nICML’2007.\nRanzato, M. and Hinton, G. H. (2010). Modeling pixel means and\ncovariances using factorized third-order Boltzmann machines. In\nCVPR’2010, pages 2551–2558.\nRanzato, M., Poultney, C., Chopra, S., and LeCun, Y. (2007). Efﬁcient\nlearning of sparse representations with an energy-based model. In\nNIPS’2006.\nRanzato, M., Boureau, Y., and LeCun, Y. (2008).\nSparse feature\nlearning for deep belief networks. In NIPS’2007.\nRanzato, M., Krizhevsky, A., and Hinton, G. (2010a). Factored 3-\nway restricted Boltzmann machines for modeling natural images.\nIn AISTATS’2010, pages 621–628.\nRanzato, M., Mnih, V., and Hinton, G. (2010b). Generating more\nrealistic images using gated MRF’s. In NIPS’2010.\nRanzato, M., Susskind, J., Mnih, V., and Hinton, G. (2011). On deep\ngenerative models with applications to recognition. In CVPR’2011.\nRiesenhuber, M. and Poggio, T. (1999). Hierarchical models of object\nrecognition in cortex. Nature Neuroscience.\nRifai, S., Vincent, P., Muller, X., Glorot, X., and Bengio, Y. (2011a).\nContractive auto-encoders: Explicit invariance during feature ex-\ntraction. In ICML’2011.\nRifai, S., Mesnil, G., Vincent, P., Muller, X., Bengio, Y., Dauphin,\nY., and Glorot, X. (2011b). Higher order contractive auto-encoder.\nIn ECML PKDD.\nRifai, S., Dauphin, Y., Vincent, P., Bengio, Y., and Muller, X. (2011c).\nThe manifold tangent classiﬁer. In NIPS’2011.\nRifai, S., Bengio, Y., Dauphin, Y., and Vincent, P. (2012).\nA\ngenerative process for sampling contractive auto-encoders.\nIn\nICML’2012.\nRoweis, S. (1997). EM algorithms for PCA and sensible PCA. CNS\nTechnical Report CNS-TR-97-02, Caltech.\nRoweis, S. and Saul, L. K. (2000).\nNonlinear dimensionality\nreduction by locally linear embedding. Science, 290(5500).\nSalakhutdinov, R. (2010a). Learning deep Boltzmann machines using\nadaptive MCMC. In ICML’2010.\nSalakhutdinov, R. (2010b). Learning in Markov random ﬁelds using\ntempered transitions. In NIPS’2010.\nSalakhutdinov, R. and Hinton, G. E. (2007). Semantic hashing. In\nSIGIR’2007.\nSalakhutdinov, R. and Hinton, G. E. (2009).\nDeep Boltzmann\nmachines. In AISTATS’2009, pages 448–455.\nSalakhutdinov, R. and Larochelle, H. (2010). Efﬁcient learning of\ndeep Boltzmann machines. In AISTATS’2010.\nSalakhutdinov, R., Mnih, A., and Hinton, G. E. (2007). Restricted\nBoltzmann machines for collaborative ﬁltering. In ICML 2007.\nSavard, F. (2011). R´eseaux de neurones `a relaxation entraˆın´es par\ncrit`ere d’autoencodeur d´ebruitant. Master’s thesis, U. Montr´eal.\nSchmah, T., Hinton, G. E., Zemel, R., Small, S. L., and Strother,\nS. (2009). Generative versus discriminative training of RBMs for\nclassiﬁcation of fMRI images. In NIPS’2008, pages 1409–1416.\nSch¨olkopf, B., Smola, A., and M¨uller, K.-R. (1998).\nNonlinear\ncomponent analysis as a kernel eigenvalue problem.\nNeural\nComputation, 10, 1299–1319.\nSchwenk, H., Rousseau, A., and Attik, M. (2012). Large, pruned or\ncontinuous space language models on a gpu for statistical machine\ntranslation. In Workshop on the future of language modeling for\nHLT.\nSeide, F., Li, G., and Yu, D. (2011a).\nConversational speech\ntranscription using context-dependent deep neural networks.\nIn\nInterspeech 2011, pages 437–440.\nSeide, F., Li, G., and Yu, D. (2011b).\nFeature engineering in\ncontext-dependent deep neural networks for conversational speech\ntranscription. In ASRU’2011.\nSerre, T., Wolf, L., Bileschi, S., and Riesenhuber, M. (2007). Robust\nobject recognition with cortex-like mechanisms.\nIEEE Trans.\nPattern Anal. Mach. Intell., 29(3), 411–426.\nSeung, S. H. (1998).\nLearning continuous attractors in recurrent\nnetworks. In NIPS’1997.\nSimard, D., Steinkraus, P. Y., and Platt, J. C. (2003). Best practices\nfor convolutional neural networks. In ICDAR’2003.\nSimard, P., Victorri, B., LeCun, Y., and Denker, J. (1992). Tangent\nprop - A formalism for specifying selected invariances in an\nadaptive network. In NIPS’1991.\n30\nSimard, P. Y., LeCun, Y., and Denker, J. (1993). Efﬁcient pattern\nrecognition using a new transformation distance. In NIPS’92.\nSmolensky, P. (1986). Information processing in dynamical systems:\nFoundations of harmony theory.\nIn D. E. Rumelhart and J. L.\nMcClelland, editors, Parallel Distributed Processing, volume 1,\nchapter 6, pages 194–281. MIT Press, Cambridge.\nSnoek, J., Larochelle, H., and Adams, R. P. (2012). Practical bayesian\noptimization of machine learning algorithms. In NIPS’2012.\nSocher, R., Huang, E. H., Pennington, J., Ng, A. Y., and Manning,\nC. D. (2011a). Dynamic pooling and unfolding recursive autoen-\ncoders for paraphrase detection. In NIPS’2011.\nSocher, R., Pennington, J., Huang, E. H., Ng, A. Y., and Manning,\nC. D. (2011b). Semi-supervised recursive autoencoders for pre-\ndicting sentiment distributions. In EMNLP’2011.\nSrivastava, N. and Salakhutdinov, R. (2012). Multimodal learning\nwith deep boltzmann machines. In NIPS’2012.\nStoyanov, V., Ropson, A., and Eisner, J. (2011).\nEmpirical risk\nminimization of graphical model parameters given approximate\ninference, decoding, and model structure. In AISTATS’2011.\nSutskever, I. (2012).\nTraining Recurrent Neural Networks.\nPh.D.\nthesis, Departement of computer science, University of Toronto.\nSutskever, I. and Tieleman, T. (2010). On the Convergence Properties\nof Contrastive Divergence. In AISTATS’2010.\nSutskever, I., Hinton, G., and Taylor, G. (2009).\nThe recurrent\ntemporal restricted Boltzmann machine. In NIPS’2008.\nSwersky, K. (2010).\nInductive Principles for Learning Restricted\nBoltzmann Machines.\nMaster’s thesis, University of British\nColumbia.\nSwersky, K., Ranzato, M., Buchman, D., Marlin, B., and de Freitas,\nN. (2011).\nOn score matching for energy based models: Gen-\neralizing autoencoders and simplifying deep learning.\nIn Proc.\nICML’2011. ACM.\nTaylor, G. and Hinton, G. (2009).\nFactored conditional restricted\nBoltzmann machines for modeling motion style. In ICML’2009.\nTaylor, G., Fergus, R., LeCun, Y., and Bregler, C. (2010). Convolu-\ntional learning of spatio-temporal features. In ECCV’10.\nTenenbaum, J., de Silva, V., and Langford, J. C. (2000). A global\ngeometric framework for nonlinear dimensionality reduction. Sci-\nence, 290(5500), 2319–2323.\nTieleman, T. (2008). Training restricted Boltzmann machines using\napproximations to the likelihood gradient. In ICML’2008, pages\n1064–1071.\nTieleman, T. and Hinton, G. (2009). Using fast weights to improve\npersistent contrastive divergence. In ICML’2009.\nTipping, M. E. and Bishop, C. M. (1999).\nProbabilistic principal\ncomponents analysis. J. Roy. Stat. Soc. B, (3).\nTuraga, S. C., Murray, J. F., Jain, V., Roth, F., Helmstaedter, M.,\nBriggman, K., Denk, W., and Seung, H. S. (2010). Convolutional\nnetworks can learn to generate afﬁnity graphs for image segmen-\ntation. Neural Computation, 22, 511–538.\nvan der Maaten, L. (2009).\nLearning a parametric embedding by\npreserving local structure. In AISTATS’2009.\nvan der Maaten, L. and Hinton, G. E. (2008). Visualizing data using\nt-SNE. J. Machine Learning Res., 9.\nVincent, P. (2011).\nA connection between score matching and\ndenoising autoencoders. Neural Computation, 23(7).\nVincent, P. and Bengio, Y. (2003). Manifold Parzen windows. In\nNIPS’2002. MIT Press.\nVincent, P., Larochelle, H., Bengio, Y., and Manzagol, P.-A. (2008).\nExtracting and composing robust features with denoising autoen-\ncoders. In ICML 2008.\nVincent, P., Larochelle, H., Lajoie, I., Bengio, Y., and Manzagol,\nP.-A. (2010).\nStacked denoising autoencoders: Learning useful\nrepresentations in a deep network with a local denoising criterion.\nJ. Machine Learning Res., 11.\nWeinberger, K. Q. and Saul, L. K. (2004). Unsupervised learning of\nimage manifolds by semideﬁnite programming. In CVPR’2004,\npages 988–995.\nWelling, M. (2009). Herding dynamic weights for partially observed\nrandom ﬁeld models. In UAI’2009.\nWelling, M., Hinton, G. E., and Osindero, S. (2003). Learning sparse\ntopographic representations with products of Student-t distribu-\ntions. In NIPS’2002.\nWeston, J., Ratle, F., and Collobert, R. (2008). Deep learning via\nsemi-supervised embedding. In ICML 2008.\nWeston, J., Bengio, S., and Usunier, N. (2010). Large scale image\nannotation: learning to rank with joint word-image embeddings.\nMachine Learning, 81(1), 21–35.\nWiskott, L. and Sejnowski, T. (2002). Slow feature analysis: Un-\nsupervised learning of invariances.\nNeural Computation, 14(4),\n715–770.\nYounes, L. (1999).\nOn the convergence of Markovian stochastic\nalgorithms with rapidly decreasing ergodicity rates. Stochastics\nand Stochastic Reports, 65(3), 177–228.\nYu, D., Wang, S., and Deng, L. (2010). Sequential labeling using\ndeep-structured conditional random ﬁelds. IEEE Journal of Se-\nlected Topics in Signal Processing.\nYu, K. and Zhang, T. (2010). Improved local coordinate coding using\nlocal tangents. In ICML’2010.\nYu, K., Zhang, T., and Gong, Y. (2009). Nonlinear learning using\nlocal coordinate coding. In NIPS’2009.\nYu, K., Lin, Y., and Lafferty, J. (2011). Learning image representa-\ntions from the pixel level via hierarchical sparse coding. In CVPR.\nYuille, A. L. (2005). The convergence of contrastive divergences. In\nNIPS’2004, pages 1593–1600.\nZeiler, M., Krishnan, D., Taylor, G., and Fergus, R. (2010). Decon-\nvolutional networks. In CVPR’2010.\nZou, W. Y., Ng, A. Y., and Yu, K. (2011). Unsupervised learning\nof visual invariance with temporal coherence.\nIn NIPS 2011\nWorkshop on Deep Learning and Unsupervised Feature Learning.\n",
        "sentence": " As argued by Bengio et al. (2013), the use of deep networks can offer both computational and statistical efficiency for complex tasks."
    },
    {
        "title": "A committee of neural networks for traffic sign classification",
        "author": [
            "Ciresan",
            "Dan",
            "Meier",
            "Ueli",
            "Masci",
            "Jonathan",
            "Schmidhuber",
            "Jürgen"
        ],
        "venue": "In Neural Networks (IJCNN), The 2011 International Joint Conference on,",
        "citeRegEx": "Ciresan et al\\.,? \\Q2011\\E",
        "shortCiteRegEx": "Ciresan et al\\.",
        "year": 2011,
        "abstract": "Abstract não disponível",
        "full_text": "",
        "sentence": ""
    },
    {
        "title": "Learning to forget: Continual prediction with LSTM",
        "author": [
            "Gers",
            "Felix A",
            "Schmidhuber",
            "Jürgen",
            "Cummins",
            "Fred"
        ],
        "venue": "In Artificial Neural Networks,",
        "citeRegEx": "Gers et al\\.,? \\Q1999\\E",
        "shortCiteRegEx": "Gers et al\\.",
        "year": 1999,
        "abstract": " Long short-term memory (LSTM; Hochreiter &amp; Schmidhuber, 1997) can solve numerous tasks not solvable by previous learning algorithms for recurrent neural networks (RNNs). We identify a weakness of LSTM networks processing continual input streams that are not a priori segmented into subsequences with explicitly marked ends at which the network's internal state could be reset. Without resets, the state may grow indefinitely and eventually cause the network to break down. Our remedy is a novel, adaptive “forget gate” that enables an LSTM cell to learn to reset itself at appropriate times, thus releasing internal resources. We review illustrative benchmark problems on which standard LSTM outperforms other RNN algorithms. All algorithms (including LSTM) fail to solve continual versions of these problems. LSTM with forget gates, however, easily solves them, and in an elegant way. ",
        "full_text": "",
        "sentence": " This scheme is strongly inspired by the proposal of Gers et al. (1999) to initially bias the gates in a Long Short-Term Memory recurrent network to help bridge long-term temporal dependencies early in learning."
    },
    {
        "title": "Understanding the difficulty of training deep feedforward neural networks",
        "author": [
            "Glorot",
            "Xavier",
            "Bengio",
            "Yoshua"
        ],
        "venue": "In International Conference on Artificial Intelligence and Statistics,",
        "citeRegEx": "Glorot et al\\.,? \\Q2010\\E",
        "shortCiteRegEx": "Glorot et al\\.",
        "year": 2010,
        "abstract": "",
        "full_text": "",
        "sentence": ""
    },
    {
        "title": "Computational limitations of small-depth circuits",
        "author": [
            "Håstad",
            "Johan"
        ],
        "venue": "MIT press,",
        "citeRegEx": "Håstad and Johan.,? \\Q1987\\E",
        "shortCiteRegEx": "Håstad and Johan.",
        "year": 1987,
        "abstract": "",
        "full_text": "",
        "sentence": ""
    },
    {
        "title": "On the power of small-depth threshold circuits",
        "author": [
            "Håstad",
            "Johan",
            "Goldmann",
            "Mikael"
        ],
        "venue": "Computational Complexity,",
        "citeRegEx": "Håstad et al\\.,? \\Q1991\\E",
        "shortCiteRegEx": "Håstad et al\\.",
        "year": 1991,
        "abstract": "Abstract não disponível",
        "full_text": "",
        "sentence": ""
    },
    {
        "title": "Delving deep into rectifiers: Surpassing human-level performance on ImageNet classification",
        "author": [
            "He",
            "Kaiming",
            "Zhang",
            "Xiangyu",
            "Ren",
            "Shaoqing",
            "Sun",
            "Jian"
        ],
        "venue": "[cs],",
        "citeRegEx": "He et al\\.,? \\Q2015\\E",
        "shortCiteRegEx": "He et al\\.",
        "year": 2015,
        "abstract": "Abstract não disponível",
        "full_text": "",
        "sentence": " Optimization of deep networks has proven to be considerably more difficult, leading to research on initialization schemes (Glorot & Bengio, 2010; Saxe et al., 2013; He et al., 2015), techniques of training networks in multiple stages (Simonyan & Zisserman, 2014; Romero et al. For up to 100 layers we compare their training behavior to that of traditional networks with normalized initialization (Glorot & Bengio, 2010; He et al., 2015). For up to 100 layers we compare their training behavior to that of traditional networks with normalized initialization (Glorot & Bengio, 2010; He et al., 2015). We show that optimization of highway networks is virtually independent of depth, while for traditional networks it suffers significantly as the number of layers increases. We also show that architectures comparable to those recently presented by Romero et al. (2014) can be directly trained to obtain similar test ar X iv :1 50 5. For plain deep networks, training with SGD stalls at the beginning unless a specific weight initialization scheme is used such that the variance of the signals during forward and backward propagation is preserved initially (Glorot & Bengio, 2010; He et al., 2015). Very deep plain networks become difficult to optimize even if using the variance-preserving initialization scheme form (He et al., 2015). All other weights were initialized following the scheme introduced by (He et al., 2015)."
    },
    {
        "title": "Long short term memory",
        "author": [
            "Hochreiter",
            "Sepp",
            "Schmidhuber",
            "Jürgen"
        ],
        "venue": "Technical Report FKI-207-95,",
        "citeRegEx": "Hochreiter et al\\.,? \\Q1995\\E",
        "shortCiteRegEx": "Hochreiter et al\\.",
        "year": 1995,
        "abstract": "",
        "full_text": "",
        "sentence": ""
    },
    {
        "title": "Imagenet classification with deep convolutional neural networks",
        "author": [
            "Krizhevsky",
            "Alex",
            "Sutskever",
            "Ilya",
            "Hinton",
            "Geoffrey E"
        ],
        "venue": "In Advances in Neural Information Processing Systems,",
        "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E",
        "shortCiteRegEx": "Krizhevsky et al\\.",
        "year": 2012,
        "abstract": "We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5% and 17.0%, respectively, which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully connected layers we employed a recently developed regularization method called \"dropout\" that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3%, compared to 26.2% achieved by the second-best entry.",
        "full_text": "",
        "sentence": " For instance, the top-5 image classification accuracy on the 1000-class ImageNet dataset has increased from ∼84% (Krizhevsky et al., 2012) to ∼95% (Szegedy et al."
    },
    {
        "title": "URL http://jmlr.org/ proceedings/papers/v38/lee15a.html",
        "author": [
            "Lee",
            "Chen-Yu",
            "Xie",
            "Saining",
            "Gallagher",
            "Patrick",
            "Zhang",
            "Zhengyou",
            "Tu",
            "Zhuowen"
        ],
        "venue": "Deeply-supervised nets. pp",
        "citeRegEx": "Lee et al\\.,? \\Q2015\\E",
        "shortCiteRegEx": "Lee et al\\.",
        "year": 2015,
        "abstract": "",
        "full_text": "",
        "sentence": " , 2014) or with temporary companion loss functions attached to some of the layers (Szegedy et al., 2014; Lee et al., 2015)."
    },
    {
        "title": "FitNets: Hints for thin deep nets",
        "author": [
            "Romero",
            "Adriana",
            "Ballas",
            "Nicolas",
            "Kahou",
            "Samira Ebrahimi",
            "Chassang",
            "Antoine",
            "Gatta",
            "Carlo",
            "Bengio",
            "Yoshua"
        ],
        "venue": "[cs],",
        "citeRegEx": "Romero et al\\.,? \\Q2014\\E",
        "shortCiteRegEx": "Romero et al\\.",
        "year": 2014,
        "abstract": "While depth tends to improve network performances, it also makes\ngradient-based training more difficult since deeper networks tend to be more\nnon-linear. The recently proposed knowledge distillation approach is aimed at\nobtaining small and fast-to-execute models, and it has shown that a student\nnetwork could imitate the soft output of a larger teacher network or ensemble\nof networks. In this paper, we extend this idea to allow the training of a\nstudent that is deeper and thinner than the teacher, using not only the outputs\nbut also the intermediate representations learned by the teacher as hints to\nimprove the training process and final performance of the student. Because the\nstudent intermediate hidden layer will generally be smaller than the teacher's\nintermediate hidden layer, additional parameters are introduced to map the\nstudent hidden layer to the prediction of the teacher hidden layer. This allows\none to train deeper students that can generalize better or run faster, a\ntrade-off that is controlled by the chosen student capacity. For example, on\nCIFAR-10, a deep student network with almost 10.4 times less parameters\noutperforms a larger, state-of-the-art teacher network.",
        "full_text": "arXiv:1412.6550v4  [cs.LG]  27 Mar 2015\nPublished as a conference paper at ICLR 2015\nFITNETS: HINTS FOR THIN DEEP NETS\nAdriana Romero1, Nicolas Ballas2, Samira Ebrahimi Kahou3, Antoine Chassang2,\nCarlo Gatta4 & Yoshua Bengio2†\n1Universitat de Barcelona, Barcelona, Spain.\n2Universit´e de Montr´eal, Montr´eal, Qu´ebec, Canada. †CIFAR Senior Fellow.\n3 ´Ecole Polytechnique de Montr´eal, Montr´eal, Qu´ebec, Canada.\n4Centre de Visi´o per Computador, Bellaterra, Spain.\nABSTRACT\nWhile depth tends to improve network performances, it also makes gradient-based\ntraining more difﬁcult since deeper networks tend to be more non-linear. The re-\ncently proposed knowledge distillation approach is aimed at obtaining small and\nfast-to-execute models, and it has shown that a student network could imitate the\nsoft output of a larger teacher network or ensemble of networks. In this paper,\nwe extend this idea to allow the training of a student that is deeper and thinner\nthan the teacher, using not only the outputs but also the intermediate represen-\ntations learned by the teacher as hints to improve the training process and ﬁnal\nperformance of the student. Because the student intermediate hidden layer will\ngenerally be smaller than the teacher’s intermediate hidden layer, additional pa-\nrameters are introduced to map the student hidden layer to the prediction of the\nteacher hidden layer. This allows one to train deeper students that can generalize\nbetter or run faster, a trade-off that is controlled by the chosen student capacity.\nFor example, on CIFAR-10, a deep student network with almost 10.4 times less\nparameters outperforms a larger, state-of-the-art teacher network.\n1\nINTRODUCTION\nDeep networks have recently exhibited state-of-the-art performance in computer vision tasks such\nas image classiﬁcation and object detection (Simonyan & Zisserman, 2014; Szegedy et al., 2014).\nHowever, top-performing systems usually involve very wide and deep networks, with numerous\nparameters. Once learned, a major drawback of such wide and deep models is that they result\nin very time consuming systems at inference time, since they need to perform a huge number of\nmultiplications. Moreover, having large amounts of parameters makes the models high memory\ndemanding. For these reasons, wide and deep top-performing networks are not well suited for\napplications with memory or time limitations.\nThere have been several attempts in the literature to tackle the problem of model compression to\nreduce the computational burden at inference time. In Bucila et al. (2006), authors propose to train\na neural network to mimic the output of a complex and large ensemble. The method uses the ensem-\nble to label unlabeled data and trains the neural network with the data labeled by the ensemble, thus\nmimicking the function learned by the ensemble and achieving similar accuracy. The idea has been\nrecently adopted in Ba & Caruana (2014) to compress deep and wide networks into shallower but\neven wider ones, where the compressed model mimics the function learned by the complex model, in\nthis case, by using data labeled by a deep (or an ensemble of deep) networks. More recently, Knowl-\nedge Distillation (KD) (Hinton & Dean, 2014) was introduced as a model compression framework,\nwhich eases the training of deep networks by following a student-teacher paradigm, in which the\nstudent is penalized according to a softened version of the teacher’s output. The framework com-\npresses an ensemble of deep networks (teacher) into a student network of similar depth. To do so,\nthe student is trained to predict the output of the teacher, as well as the true classiﬁcation labels. All\nprevious works related to Convolutional Neural Networks focus on compressing a teacher network\nor an ensemble of networks into either networks of similar width and depth or into shallower and\nwider ones; not taking advantage of depth.\n1\nPublished as a conference paper at ICLR 2015\nDepth is a fundamental aspect of representation learning, since it encourages the re-use of features,\nand leads to more abstract and invariant representations at higher layers (Bengio et al., 2013). The\nimportance of depth has been veriﬁed (1) theoretically: deep representations are exponentially more\nexpressive than shallow ones for some families of functions (Montufar et al., 2014); and (2) empiri-\ncally: the two top-performers of ImageNet use deep convolutional networks with 19 and 22 layers,\nrespectively (Simonyan & Zisserman, 2014) and (Szegedy et al., 2014).\nNevertheless, training deep architectures has proven to be challenging (Larochelle et al., 2007;\nErhan et al., 2009), since they are composed of successive non-linearities and, thus result in highly\nnon-convex and non-linear functions. Signiﬁcant effort has been devoted to alleviate this optimiza-\ntion problem. On the one hand, pre-training strategies, whether unsupervised (Hinton et al., 2006;\nBengio et al., 2007) or supervised (Bengio et al., 2007) train the network parameters in a greedy lay-\nerwise fashion in order to initialize the network parameters in a potentially good basin of attraction.\nThe layers are trained one after the other according to an intermediate target. Similarly, semi-\nsupervised embedding (Weston et al., 2008) provides guidance to an intermediate layer to help learn\nvery deep networks. Along this line of reasoning, (Cho et al., 2012) ease the optimization problem\nof DBM by borrowing the activations of another model every second layer in a purely unsupervised\nscenario. More recently, (Chen-Yu et al., 2014; Szegedy et al., 2014; Gulcehre & Bengio, 2013)\nshowed that adding supervision to intermediate layers of deep architectures assists the training of\ndeep networks. Supervision is introduced by stacking a supervised MLP with a softmax layer on top\nof intermediate hidden layers to ensure their discriminability w.r.t. labels. Alternatively, Curriculum\nLearning strategies (CL) (Bengio, 2009) tackle the optimization problem by modifying the training\ndistribution, such that the learner network gradually receives examples of increasing and appropriate\ndifﬁculty w.r.t. the already learned concepts. As a result, curriculum learning acts like a continuation\nmethod, speeds up the convergence of the training process and ﬁnds potentially better local minima\nof highly non-convex cost functions.\nIn this paper, we aim to address the network compression problem by taking advantage of depth.\nWe propose a novel approach to train thin and deep networks, called FitNets, to compress wide and\nshallower (but still deep) networks. The method is rooted in the recently proposed Knowledge Dis-\ntillation (KD) (Hinton & Dean, 2014) and extends the idea to allow for thinner and deeper student\nmodels. We introduce intermediate-level hints from the teacher hidden layers to guide the training\nprocess of the student, i.e., we want the student network (FitNet) to learn an intermediate repre-\nsentation that is predictive of the intermediate representations of the teacher network. Hints allow\nthe training of thinner and deeper networks. Results conﬁrm that having deeper models allow us to\ngeneralize better, whereas making these models thin help us reduce the computational burden sig-\nniﬁcantly. We validate the proposed method on MNIST, CIFAR-10, CIFAR-100, SVHN and AFLW\nbenchmark datasets and provide evidence that our method matches or outperforms the teacher’s\nperformance, while requiring notably fewer parameters and multiplications.\n2\nMETHOD\nIn this section, we detail the proposed student-teacher framework to train FitNets from shallower\nand wider nets. First, we review the recently proposed KD. Second, we highlight the proposed hints\nalgorithm to guide the FitNet throughout the training process. Finally, we describe how the FitNet\nis trained in a stage-wise fashion.\n2.1\nREVIEW OF KNOWLEDGE DISTILLATION\nIn order to obtain a faster inference, we explore the recently proposed compression framework\n(Hinton & Dean, 2014), which trains a student network, from the softened output of an ensemble of\nwider networks, teacher network. The idea is to allow the student network to capture not only the\ninformation provided by the true labels, but also the ﬁner structure learned by the teacher network.\nThe framework can be summarized as follows.\nLet T be a teacher network with an output softmax PT = softmax(aT ) where aT is the vector of\nteacher pre-softmax activations, for some example. In the case where the teacher model is a single\nnetwork, aT represents the weighted sums of the output layer, whereas if the teacher model is the\nresult of an ensemble either PT or aT are obtained by averaging outputs from different networks\n(respectively for arithmetic or geometric averaging). Let S be a student network with parameters\n2\nPublished as a conference paper at ICLR 2015\nWS and output probability PS = softmax(aS), where aS is the student’s pre-softmax output. The\nstudent network will be trained such that its output PS is similar to the teacher’s output PT, as well\nas to the true labels ytrue. Since PT might be very close to the one hot code representation of the\nsample’s true label, a relaxation τ > 1 is introduced to soften the signal arising from the output\nof the teacher network, and thus, provide more information during training1. The same relaxation\nis applied to the output of the student network (Pτ\nS), when it is compared to the teacher’s softened\noutput (Pτ\nT):\nPτ\nT = softmax\n\u0010aT\nτ\n\u0011\n,\nPτ\nS = softmax\n\u0010aS\nτ\n\u0011\n.\n(1)\nThe student network is then trained to optimize the following loss function:\nLKD(WS) = H(ytrue, PS) + λH(Pτ\nT, Pτ\nS),\n(2)\nwhere H refers to the cross-entropy and λ is a tunable parameter to balance both cross-entropies.\nNote that the ﬁrst term in Eq. (2) corresponds to the traditional cross-entropy between the output of\na (student) network and labels, whereas the second term enforces the student network to learn from\nthe softened output of the teacher network.\nTo the best of our knowledge, KD is designed such that student networks mimic teacher architectures\nof similar depth. Although we found the KD framework to achieve encouraging results even when\nstudent networks have slightly deeper architectures, as we increase the depth of the student network,\nKD training still suffers from the difﬁculty of optimizing deep nets (see Section 4.1).\n2.2\nHINT-BASED TRAINING\nIn order to help the training of deep FitNets (deeper than their teacher), we introduce hints from the\nteacher network. A hint is deﬁned as the output of a teacher’s hidden layer responsible for guiding\nthe student’s learning process. Analogously, we choose a hidden layer of the FitNet, the guided\nlayer, to learn from the teacher’s hint layer. We want the guided layer to be able to predict the output\nof the hint layer. Note that having hints is a form of regularization and thus, the pair hint/guided\nlayer has to be chosen such that the student network is not over-regularized. The deeper we set the\nguided layer, the less ﬂexibility we give to the network and, therefore, FitNets are more likely to\nsuffer from over-regularization. In our case, we choose the hint to be the middle layer of the teacher\nnetwork. Similarly, we choose the guided layer to be the middle layer of the student network.\nGiven that the teacher network will usually be wider than the FitNet, the selected hint layer may\nhave more outputs than the guided layer. For that reason, we add a regressor to the guided layer,\nwhose output matches the size of the hint layer. Then, we train the FitNet parameters from the ﬁrst\nlayer up to the guided layer as well as the regressor parameters by minimizing the following loss\nfunction:\nLHT (WGuided, Wr) = 1\n2||uh(x; WHint) −r(vg(x; WGuided); Wr)||2,\n(3)\nwhere uh and vg are the teacher/student deep nested functions up to their respective hint/guided\nlayers with parameters WHint and WGuided, r is the regressor function on top of the guided layer\nwith parameters Wr. Note that the outputs of uh and r have to be comparable, i.e., uh and r must\nbe the same non-linearity.\nNevertheless, using a fully-connected regressor increases the number of parameters and the memory\nconsumption dramatically in the case where the guided and hint layers are convolutional. Let Nh,1×\nNh,2 and Oh be the teacher hint’s spatial size and number of channels, respectively. Similarity, let\nNg,1 ×Ng,2 and Og be the FitNet guided layer’s spatial size and number of channels. The number of\nparameters in the weight matrix of a fully connected regressor is Nh,1×Nh,2×Oh×Ng,1×Ng,2×Og.\nTo mitigate this limitation, we use a convolutional regressor instead. The convolutional regressor\nis designed such that it considers approximately the same spatial region of the input image as the\nteacher hint. Therefore, the output of the regressor has the same spatial size as the teacher hint.\nGiven a teacher hint of spatial size Nh,1 × Nh,2, the regressor takes the output of the Fitnet’s guided\n1For example, as argued by Hinton & Dean (2014), with softened outputs, more information is provided\nabout the relative similarity of the input to classes other than the one with the highest probability.\n3\nPublished as a conference paper at ICLR 2015\n.\n.\n.\n.\n.\n.\nWT\n1\nWT\n2\nWT\n1\nWT\nh\nWT\nL\n.\n.\n.\n.\n.\n.\nWS\n2\nWS\n1\nWS\ng\nWS\nM\nWHint\nWGuided\nTeacher Network\nFitNet\nWT\nWS\n(a) Teacher and Student Networks\nWHint\nWGuided\nWr\nW*\nGuided = \n(WGuided, Wr)\n(b) Hints Training\nWT\nW\nS\nW*\nS = \n(W\nS)\nW*\nGuided\n(c) Knowledge Distillation\nFigure 1: Training a student network using hints.\nlayer of size Ng,1 × Ng,2 and adapts its kernel shape k1 × k2 such that Ng,i −ki + 1 = Nh,i,\nwhere i ∈{1, 2}. The number of parameters in the weight matrix of a the convolutional regressor is\nk1 × k2 × Oh × Og, where k1 × k2 is signiﬁcantly lower than Nh,1 × Nh,2 × Ng,1 × Ng,2.\n2.3\nFITNET STAGE-WISE TRAINING\nWe train the FitNet in a stage-wise fashion following the student/teacher paradigm. Figure 1 sum-\nmarizes the training pipeline. Starting from a trained teacher network and a randomly initialized\nFitNet (Fig. 1 (a)), we add a regressor parameterized by Wr on top of the FitNet guided layer and\ntrain the FitNet parameters WGuided up to the guided layer to minimize Eq. (3) (see Fig. 1 (b)).\nFinally, from the pre-trained parameters, we train the parameters of whole FitNet WS to minimize\nEq. (2) (see Fig. 1 (c)). Algorithm 1 details the FitNet training process.\nAlgorithm 1 FitNet Stage-Wise Training.\nThe algorithm receives as input the trained parameters WT of a teacher, the randomly initialized\nparameters WS of a FitNet, and two indices h and g corresponding to hint/guided layers, respec-\ntively. Let WHint be the teacher’s parameters up to the hint layer h. Let WGuided be the FitNet’s\nparameters up to the guided layer g. Let Wr be the regressor’s parameters. The ﬁrst stage consists in\npre-training the student network up to the guided layer, based on the prediction error of the teacher’s\nhint layer (line 4). The second stage is a KD training of the whole network (line 6).\nInput: WS, WT, g, h\nOutput: W∗\nS\n1: WHint ←{WT\n1, . . . , WT\nh}\n2: WGuided ←{WS\n1, . . . , WS\ng}\n3: Intialize Wr to small random values\n4: W∗\nGuided ←argmin\nWGuided\nLHT (WGuided, Wr)\n5: {WS1, . . . , WSg} ←{WGuided\n∗1, . . . , WGuided\n∗g}\n6: W∗\nS ←argmin\nWS\nLKD(WS)\n2.4\nRELATION TO CURRICULUM LEARNING\nIn this section, we argue that our hint-based training with KD can be seen as a particular form of\nCurriculum Learning (Bengio, 2009). Curriculum learning has proven to accelerate the training\nconvergence as well as potentially improve the model generalization by properly choosing a se-\nquence of training distributions seen by the learner: from simple examples to more complex ones.\nA curriculum learning extension (Gulcehre & Bengio, 2013) has also shown that by using guidance\nhints on an intermediate layer during the training, one could considerably ease training. However,\nBengio (2009) uses hand-deﬁned heuristics to measure the “simplicity” of an example in a sequence\nand Gulcehre & Bengio (2013)’s guidance hints require some prior knowledge of the end-task. Both\nof these curriculum learning strategies tend to be problem-speciﬁc.\nOur approach alleviates this issue by using a teacher model. Indeed, intermediate representations\nlearned by the teacher are used as hints to guide the FitNet optimization procedure. In addition, the\nteacher conﬁdence provides a measure of example “simplicity” by means of teacher cross-entropy\n4\nPublished as a conference paper at ICLR 2015\nterm in Eq. (2). This term ensures that examples with a high teacher conﬁdence have a stronger\nimpact than examples with low teacher conﬁdence: the latter correspond to probabilities closer to\nthe uniform distribution, which exert less of a push on the student parameters. In other words, the\nteacher penalizes the training examples according to its conﬁdence. Note that parameter λ in Eq.\n(2) controls the weight given to the teacher cross-entropy, and thus, the importance given to each\nexample. In order to promote the learning of more complex examples (examples with lower teacher\nconﬁdence), we gradually anneal λ during the training with a linear decay. The curriculum can be\nseen as composed of two stages: ﬁrst learn intermediate concepts via the hint/guided layer transfer,\nthen train the whole student network jointly, annealing λ, which allows easier examples (on which\nthe teacher is very conﬁdent) to initially have a stronger effect, but progressively decreasing their\nimportance as λ decays. Therefore, the hint-based training introduced in the paper is a generic\ncurriculum learning approach, where prior information about the task-at-hand is deduced purely\nfrom the teacher model.\nAlgorithm\n# params\nAccuracy\nCompression\nFitNet\n∼2.5M\n91.61%\nTeacher\n∼9M\n90.18%\nMimic single\n∼54M\n84.6%\nMimic single\n∼70M\n84.9%\nMimic ensemble\n∼70M\n85.8%\nState-of-the-art methods\nMaxout\n90.65%\nNetwork in Network\n91.2%\nDeeply-Supervised Networks\n91.78%\nDeeply-Supervised Networks (19)\n88.2%\nTable 1: Accuracy on CIFAR-10\nAlgorithm\n# params\nAccuracy\nCompression\nFitNet\n∼2.5M\n64.96%\nTeacher\n∼9M\n63.54%\nState-of-the-art methods\nMaxout\n61.43%\nNetwork in Network\n64.32%\nDeeply-Supervised Networks\n65.43%\nTable 2: Accuracy on CIFAR-100\n3\nRESULTS ON BENCHMARK DATASETS\nIn this section, we show the results on several benchmark datasets2. The architectures of all networks\nas well as the training details are reported in the supplementary material.\n3.1\nCIFAR-10 AND CIFAR-100\nThe CIFAR-10 and CIFAR-100 datasets (Krizhevsky & Hinton, 2009) are composed of 32x32 pixel\nRGB images belonging to 10 and 100 different classes, respectively. They both contain 50K training\nimages and 10K test images. CIFAR-10 has 1000 samples per class, whereas CIFAR-100 has 100\nsamples per class. Like Goodfellow et al. (2013b), we normalized the datasets for contrast normal-\nization and applied ZCA whitening.\nCIFAR-10: To validate our approach, we trained a teacher network of maxout convolutional lay-\ners as reported in Goodfellow et al. (2013b) and designed a FitNet with 17 maxout convolutional\nlayers, followed by a maxout fully-connected layer and a top softmax layer, with roughly 1/3 of\nthe parameters. The 11th layer of the student network was trained to mimic the 2nd layer of the\nteacher network. Like in Goodfellow et al. (2013b); Chen-Yu et al. (2014), we augmented the data\nwith random ﬂipping during training. Table 1 summarizes the obtained results. Our student model\noutperforms the teacher model, while requiring notably fewer parameters, suggesting that depth is\ncrucial to achieve better representations. When compared to network compression methods, our\nalgorithm achieves outstanding results; i.e., the student network achieves an accuracy of 91.61%,\nwhich is signiﬁcantly higher than the top-performer 85.8% of Ba & Caruana (2014), while requir-\ning roughly 28 times fewer parameters. When compared to state-of-the-art methods, our algorithm\nmatches the best performers.\nOne could argue the choice of hinting the inner layers with the hidden state of a wide teacher net-\nwork. A straightforward alternative would be to hint them with the desired output. This could be\naddressed in a few different ways: (1) Stage-wise training, where stage 1 optimizes the 1st half of\nthe network w.r.t. classiﬁcation targets and stage 2 optimizes the whole network w.r.t. classiﬁcation\n2Code to reproduce the experiments publicly available: https://github.com/adri-romsor/FitNets\n5\nPublished as a conference paper at ICLR 2015\ntargets. In this case, stage 1 set the network parameters in a good local minima but such initializa-\ntion did not seem to help stage 2 sufﬁciently, which failed to learn. To further assist the training\nof the thin and deep student network, we could add extra hints with the desired output at different\nhidden layers. Nevertheless, as observed in (Bengio et al., 2007), with supervised pre-training the\nguided layer may discard some factors from the input, which require more layers and non-linearity\nbefore they can be exploited to predict the classes. (2) Stage-wise training with KD, where stage 1\noptimizes the 1st half of the net w.r.t. classiﬁcation targets and stage 2 optimizes the whole network\nw.r.t. Eq. (2). As in the previous case, stage 1 set the network parameters in a good local minima\nbut such initialization did not seem to help stage 2 sufﬁciently, which failed to learn. (3) Jointly\noptimizing both stages w.r.t. the sum of the supervised hint for the guided layer and classiﬁcation\ntarget for the output layer. We performed this experiment, tried different initializations and learning\nrates with RMSprop (Tieleman & Hinton, 2012) but we could not ﬁnd any combination to make the\nnetwork learn. Note that we could ease the training by adding hints to each layer and optimizing\njointly as in Deeply Supervised Networks (DSN). Therefore, we built the above-mentioned 19-layer\narchitecture and trained it by means of DSN, achieving a test performance of 88.2%, which is sig-\nniﬁcantly lower than the performance obtained by the FitNets hint-based training (91.61%). Such\nresult suggests that using a very discriminative hint w.r.t. classiﬁcation at intermediate layers might\nbe too aggressive; using a smoother hint (such as the guidance from a teacher network) offers better\ngeneralization. (4) Jointly optimizing both stages w.r.t. the sum of supervised hint for the guided\nlayer and Eq. (2) for the output layer. Adding supervised hints to the middle layer of the network\ndid not ease the training of such a thin and deep network, which failed to learn.\nCIFAR-100: To validate our approach, we trained a teacher network of maxout convolutional layers\nas reported in Goodfellow et al. (2013b) and used the same FitNet architecture as in CIFAR-10.\nAs in Chen-Yu et al. (2014), we augmented the data with random ﬂipping during training. Table 2\nsummarizes the obtained results. As in the previous case, our FitNet outperforms the teacher model,\nreducing the number of parameters by a factor of 3 and, when compared to state-of-the-art methods,\nthe FitNet provides near state-of-the-art performance.\n3.2\nSVHN\nThe SVHN dataset (Netzer et al., 2011) is composed by 32 × 32 color images of house numbers\ncollected by GoogleStreet View. There are 73,257 images in the training set, 26,032 images in the\ntest set and 531,131 less difﬁcult examples. We follow the evaluation procedure of Goodfellow et al.\n(2013b) and use their maxout network as teacher. We trained a 13-layer FitNet composed of 11\nmaxout convolutional layers, a fully-connected layer and a softmax layer.\nAlgorithm\n# params\nMisclass\nCompression\nFitNet\n∼1.5M\n2.42%\nTeacher\n∼4.9M\n2.38%\nState-of-the-art methods\nMaxout\n2.47%\nNetwork in Network\n2.35%\nDeeply-Supervised Networks\n1.92%\nTable 3: SVHN error\nAlgorithm\n# params\nMisclass\nCompression\nTeacher\n∼361K\n0.55%\nStandard backprop\n∼30K\n1.9%\nKD\n∼30K\n0.65%\nFitNet\n∼30K\n0.51%\nState-of-the-art methods\nMaxout\n0.45%\nNetwork in Network\n0.47%\nDeeply-Supervised Networks\n0.39%\nTable 4: MNIST error\nTable 3 shows that our FitNet achieves comparable accuracy than the teacher despite using only 32%\nof teacher capacity. Our FitNet is comparable in terms of performance to other state-of-art methods,\nsuch as Maxout and Network in Network.\n3.3\nMNIST\nAs a sanity check for the training procedure, we evaluated the proposed method on the MNIST\ndataset (LeCun et al., 1998). MNIST is a dataset of handwritten digits (from 0 to 9) composed of\n28x28 pixel greyscale images, with 60K training images and 10K test images. We trained a teacher\nnetwork of maxout convolutional layers as reported in Goodfellow et al. (2013b) and designed a\n6\nPublished as a conference paper at ICLR 2015\nFitNet twice as deep as the teacher network and with roughly 8% of the parameters. The 4th layer\nof the student network was trained to mimic the 2nd layer of the teacher network.\nTable 4 reports the obtained results. To verify the inﬂuence of using hints, we trained the FitNet\narchitecture using either (1) standard backprop (w.r.t. classiﬁcation labels), (2) KD or (3) Hint-\nbased Training (HT). When training the FitNet with standard backprop from the softmax layer, the\ndeep and thin architecture achieves 1.9% misclassiﬁcation error. Using KD, the very same network\nachieves 0.65%, which conﬁrms the potential of the teacher network; and when adding hints, the\nerror still decreases to 0.51%. Furthermore, the student network achieves slightly better results than\nthe teacher network, while requiring 12 times less parameters.\n3.4\nAFLW\nAFLW (Koestinger et al., 2011) is a real-world face database, containing 25K annotated images. In\norder to evaluate the proposed framework in a face recognition setting, we extracted positive samples\nby re-sizing the annotated regions of the images to ﬁt 16x16 pixels patches. Similarly, we extracted\n25K 16x16 pixels patches not containing faces from ImageNet (Russakovsky et al., 2014) dataset,\nas negative samples. We used 90% of the extracted patches to train the network.\nIn this experiment, we aimed to evaluate the method on a different kind of architecture. Therefore,\nwe trained a teacher network of 3 ReLU convolutional layers and a sigmoid output layer. We de-\nsigned a ﬁrst FitNet (FitNet 1) with 15 times fewer multiplications than the teacher network, and a\nsecond FitNet (FitNet 2) with 2.5 times fewer multiplications than the teacher network. Both FitNets\nhave 7 ReLU convolutional layers and a sigmoid output layer.\nThe teacher network achieved 4.21% misclassiﬁcation error on the validation set. We trained both\nFitNets by means of KD and HT. On the one hand, we report a misclassiﬁcation error of 4.58%\nwhen training FitNet 1 with KD and a misclassiﬁcation error of when 2.55% when training it with\nHT. On the other hand, we report a missclassifation error of 1.95% when training FitNet 2 with KD\nand a misclassiﬁcation error of 1.85% when training it with HT. These results show how the method\nis extensible to different kind of architectures and highlight the beneﬁts of using hints, especially\nwhen dealing with thinner architectures.\n4\nANALYSIS OF EMPIRICAL RESULTS\nWe empirically investigate the beneﬁts of our approach by comparing various networks trained us-\ning standard backpropagation (cross-entropy w.r.t. labels), KD or Hint-based Training (HT). Exper-\niments are performed on CIFAR-10 dataset (Krizhevsky & Hinton, 2009).\nWe compare networks of increasing depth given a ﬁxed computational budget. Each network is com-\nposed of successive convolutional layers of kernel size 3 × 3, followed by a maxout non-linearity\nand a non-overlapping 2 × 2 max-pooling. The last max-pooling takes the maximum over all re-\nmaining spatial dimensions leading to a 1 × 1 vector representation. We only change the depth and\nthe number of channels per convolution between different networks, i.e. the number of channels per\nconvolutional layer decreases as a network depth increases to respect a given computational budget.\n4.1\nASSISTING THE TRAINING OF DEEP NETWORKS\nIn this section, we investigate the impact of HT. We consider two computational budgets of ap-\nproximately 30M and 107M operations, corresponding to the multiplications needed in an image\nforward propagation. For each computational budget, we train networks composed of 3, 5, 7 and\n9 convolutional layers, followed by a fully-connected layer and a softmax layer. We compare their\nperformances when they are trained with standard backpropagation, KD and HT. Figure 2 reports\ntest on CIFAR-10 using early stopping on the validation set, i.e. we do not retrain our models on the\ntraining plus validation sets.\nDue to their depth and small capacity, FitNets are hard to train. As shown in Figure 2(a), we could\nnot train 30M multiplications networks with more than 5 layers with standard backprop. When\nusing KD, we succesfully trained networks up to 7 layers. Adding KD’s teacher cross-entropy to\nthe training objective (Eq. (2)) gives more importance to easier examples, i.e. samples for which\nthe teacher network is conﬁdent and, can lead to a smoother version of the training cost (Bengio,\n2009). Despite some optimization beneﬁts, it is worth noticing that KD training still suffers from\n7\nPublished as a conference paper at ICLR 2015\n5\n7\n9\n11\nNumber of Layers\n83.0\n83.5\n84.0\n84.5\n85.0\n85.5\n86.0\n86.5\n87.0\n87.5\n88.0\n88.5\nTest Accuracy\nBack Propagation\nKnowledge Distillation\nHint Training\n(a) 30M Multiplications\n5\n7\n9\n11\nNumber of Layers\n85.5\n86.0\n86.5\n87.0\n87.5\n88.0\n88.5\n89.0\n89.5\n90.0\n90.5\n91.0\nTest Accuracy\nBack Propagation\nKnowledge Distillation\nHint Training\n(b) 107M Multiplications\nFigure 2: Comparison of Standard Back-Propagation, Knowledge Distillation and Hint-based Train-\ning on CIFAR-10.\nNetwork\n# layers\n# params\n# mult\nAcc\nSpeed-up\nCompression rate\nTeacher\n5\n∼9M\n∼725M\n90.18%\n1\n1\nFitNet 1\n11\n∼250K\n∼30M\n89.01%\n13.36\n36\nFitNet 2\n11\n∼862K\n∼108M\n91.06%\n4.64\n10.44\nFitNet 3\n13\n∼1.6M\n∼392M\n91.10%\n1.37\n5.62\nFitNet 4\n19\n∼2.5M\n∼382M\n91.61%\n1.52\n3.60\nTable 5: Accuracy/Speed Trade-off on CIFAR-10.\nthe increasing depth and reaches its limits for 7-layer networks. HT tends to ease these optimization\nissues and is able to train 13-layer networks of 30M multiplications. The only difference between\nHT and KD is the starting point in the parameter space: either random or obtained by means of the\nteacher’s hint. On the one hand, the proliferation of local minima and especially saddle points in\nhighly non-linear functions such as very deep networks highlights the difﬁculty of ﬁnding a good\nstarting point in the parameter space at random (Dauphin et al., 2014). On the other hand, results in\nFigure 2(a) indicate that HT can guide the student to a better initial position in the parameter space,\nfrom which we can minimize the cost through stochastic gradient descent. Therefore, HT provides\nbeneﬁts from an optimization point of view. Networks trained with HT also tend to yield better\ntest performances than the other training methods when we ﬁx the capacity and number of layers.\nFor instance, in Figure 2(b), the 7-layers network, trained with hints, obtains a +0.7% performance\ngain on the test set compared to the model that does not use any hints (the accuracy increases from\n89.45% to 90.1%). As pointed by Erhan et al. (2009), pre-training strategies can act as regularizers.\nThese results suggest that HT is a stronger regularizer than KD, since it leads to better generalization\nperformance on the test set. Finally, Figure 2 highlights that deep models have better performances\nthan shallower ones given a ﬁxed computational budget. Indeed, considering networks that are\ntrained with hints, an 11-layer network outperforms a 5-layer network by an absolute improvement\nof 4.11% for 107M multiplications and of 3.4% for 30M multiplications. Therefore, the experiments\nvalidate our hypothesis that given a ﬁxed number of computations, we leverage depth in a model to\nachieve faster computation and better generalization.\nIn summary, this experiment shows that (1) using HT, we are able to train deeper models than with\nstandard back-propagation and KD; and (2) given a ﬁxed capacity, deeper models performed better\nthan shallower ones.\n4.2\nTRADE-OFF BETWEEN MODEL PERFORMANCE AND EFFICIENCY\nTo evaluate FitNets efﬁciency, we measure their total inference times required for processing\nCIFAR-10 test examples on a GPU as well as their parameter compression. Table 5 reports both\nthe speed-up and compression rate obtained by various FitNets w.r.t. the teacher model along with\ntheir number of layers, capacity and accuracies. In this experiment, we retrain our FitNets on train-\ning plus validation sets as in Goodfellow et al. (2013b), for fair comparison with the teacher.\n8\nPublished as a conference paper at ICLR 2015\nFitNet 1, our smallest network, with 36× less capacity than the teacher, is one order of magnitude\nfaster than the teacher and only witnesses a minor performance decrease of 1.3%. FitNet 2, slightly\nincreasing the capacity, outperforms the teacher by 0.9%, while still being faster by a strong 4.64\nfactor. By further increasing network capacity and depth in FitNets 3 and 4, we improve the perfor-\nmance gain, up to 1.6%, and still remain faster than the teacher. Although a trade-off between speed\nand accuracy is introduced by the compression rate, FitNets tend to be signiﬁcantly faster, matching\nor outperforming their teacher, even when having low capacity.\nA few works such as matrix factorization (Jaderberg et al., 2014; Denton et al., 2014) focus on\nspeeding-up deep networks’ convolutional layers at the expense of slightly deteriorating their per-\nformance. Such approaches are complementary to FitNets and could be used to further speed-up the\nFitNet’s convolutional layers.\nOther works related to quantization schemes (Chen et al., 2010; J´egou et al., 2011; Gong et al.,\n2014) aim at reducing storage requirements. Unlike FitNets, such approaches witness a little de-\ncrease in performance when compressing the network parameters. Exploiting depth allows FitNets\nto obtain performance improvements w.r.t. their teachers, even when reducing the number of param-\neters 10×. However, we believe that quantization approaches are also complementary to FitNets and\ncould be used to further reduce the storage requirements. It would be interesting to compare how\nmuch redundancy is present in the ﬁlters of the teacher networks w.r.t. the ﬁlters of the FitNet and,\ntherefore, how much FitNets ﬁlters could be compressed without witnessing signiﬁcant performance\ndrop. This analysis is out of the scope of the paper and is left as future work.\n5\nCONCLUSION\nWe proposed a novel framework to compress wide and deep networks into thin and deeper ones, by\nintroducing intermediate-level hints from the teacher hidden layers to guide the training process of\nthe student. We are able to use these hints to train very deep student models with less parameters,\nwhich can generalize better and/or run faster than their teachers. We provided empirical evidence\nthat hinting the inner layers of a thin and deep network with the hidden state of a teacher network\ngeneralizes better than hinting them with the classiﬁcation targets. Our experiments on benchmark\ndatasets emphasize that deep networks with low capacity are able to extract feature representations\nthat are comparable or even better than networks with as much as 10 times more parameters. The\nhint-based training suggests that more efforts should be devoted to explore new training strategies\nto leverage the power of deep networks.\nACKNOWLEDGMENTS\nWe thank the developers of Theano (Bastien et al., 2012) and Pylearn2 (Goodfellow et al., 2013a)\nand the computational resources provided by Compute Canada and Calcul Qu´ebec. This work has\nbeen partially supported by NSERC, CIFAR, and Canada Research Chairs, Project TIN2013-41751,\ngrant 2014-SGR-221 and Spanish MINECO grant TIN2012-38187-C03.\nREFERENCES\nBa, J. and Caruana, R. Do deep nets really need to be deep? In NIPS, pp. 2654–2662. 2014.\nBastien, F., Lamblin, P., Pascanu, R., Bergstra, J., Goodfellow, I., Bergeron, A., Bouchard, N., Warde-Farley,\nD., and Bengio, Y. Theano: new features and speed improvements. Deep Learning & Unsupervised Feature\nLearning Workshop, NIPS, 2012.\nBengio, Y. Learning deep architectures for AI. Foundations and trends in Machine Learning, 2009.\nBengio, Y., Lamblin, P., Popovici, D., and Larochelle, H. Greedy layer-wise training of deep networks. In\nNIPS, pp. 153–160, 2007.\nBengio, Y., Courville, A., and Vincent, P. Representation learning: A review and new perspectives. TPAMI,\n2013.\nBucila, C., Caruana, R., and Niculescu-Mizil, A. Model compression. In KDD, pp. 535–541, 2006.\nChen, Yongjian, Guan, Tao, and Wang, Cheng. Approximate nearest neighbor search by residual vector quan-\ntization. Sensors, 10(12):11259–11273, 2010.\n9\nPublished as a conference paper at ICLR 2015\nChen-Yu, L., Saining, X., Patrick, G., Zhengyou, Z., and Zhuowen, T.\nDeeply-supervised nets.\nCoRR,\nabs/1409.5185, 2014.\nCho, Kyunghyun, Raiko, Tapani, Ilin, Alexander, and Karhunen, Juha. A two-stage pretraining algorithm for\ndeep Boltzmann machines. In NIPS Workshop on Deep Learning and Unsupervised Feature Learning, 2012.\nDauphin, Y., Pascanu, R., Gulcehre, C., Cho, K., Ganguli, S., and Bengio, Y. Identifying and attacking the\nsaddle point problem in high-dimensional non-convex optimization. In NIPS, 2014.\nDenton, Emily L, Zaremba, Wojciech, Bruna, Joan, LeCun, Yann, and Fergus, Rob. Exploiting linear structure\nwithin convolutional networks for efﬁcient evaluation. In NIPS, pp. 1269–1277. 2014.\nErhan, D., Manzagol, P.A., Bengio, Y., Bengio, S., and Vincent, P. The difﬁculty of training deep architectures\nand the effect of unsupervised pre-training. In AISTATS, pp. 153–160, 2009.\nGong, Yunchao, Liu, Liu, Yang, Min, and Bourdev, Lubomir. Compressing deep convolutional networks using\nvector quantization. CoRR, abs/1412.6115, 2014.\nGoodfellow, I. J., Warde-Farley, D., Lamblin, P., Dumoulin, V., Mirza, M., Pascanu, R., Bergstra, J., Bastien,\nF., and Bengio, Y. Pylearn2: a machine learning research library. arXiv preprint arXiv:1308.4214, 2013a.\nGoodfellow, I.J., Warde-Farley, D., Mirza, M., Courville, A., and Bengio, Y. Maxout networks. In ICML,\n2013b.\nGulcehre, C. and Bengio, Y. Knowledge matters: Importance of prior information for optimization. In ICLR,\n2013.\nHinton, G. E., Osindero, S., and Teh, Y.-W. A fast learning algorithm for deep belief nets. Neural Computation,\n18(7):1527–1554, 2006.\nHinton, G. Vinyals, O. and Dean, J. Distilling knowledge in a neural network. In Deep Learning and Repre-\nsentation Learning Workshop, NIPS, 2014.\nJaderberg, M., Vedaldi, A., and Zisserman, A. Speeding up convolutional neural networks with low rank\nexpansions. In BMVC, 2014.\nJ´egou, Herv´e, Douze, Matthijs, and Schmid, Cordelia. Product quantization for nearest neighbor search. IEEE\nTPAMI, 33(1):117–128, 2011.\nKoestinger, M., Wohlhart, P., Roth, P.M., and Bischof, H. Annotated facial landmarks in the wild: A large-scale,\nreal-world database for facial landmark localization. In First IEEE International Workshop on Benchmarking\nFacial Image Analysis Technologies, 2011.\nKrizhevsky, A. and Hinton, G. Learning multiple layers of features from tiny images. Master’s thesis, Depart-\nment of Computer Science, University of Toronto, 2009.\nLarochelle, H., Erhan, D., Courville, A., Bergstra, J., and Bengio, Y. An empirical evaluation of deep architec-\ntures on problems with many factors of variation. In ICML, pp. 473–480, 2007.\nLeCun, Y., Bottou, L., Bengio, Y., and Haffner, P. Gradient-based learning applied to document recognition.\nProceedings of the IEEE, 86(11):2278–2324, November 1998.\nMontufar, G.F., Pascanu, R., Cho, K., and Bengio, Y. On the number of linear regions of deep neural networks.\nIn NIPS. 2014.\nNetzer, Y., Wang, T., Coates, A., Bissacco, A., Wu, B., and Ng, A. Reading digits in natural images with\nunsupervised feature learning. In Deep Learning & Unsupervised Feature Learning Workshop, NIPS, 2011.\nRussakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Karpathy, A., Khosla, A.,\nBernstein, M., Berg, A. C., and Fei-Fei, L. ImageNet Large Scale Visual Recognition Challenge, 2014.\nSimonyan, K. and Zisserman, A. Very deep convolutional networks for large-scale image recognition. CoRR,\nabs/1409.1556, 2014.\nSzegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., D.A., Erhan, D., Vanhoucke, V., and Rabinovich, A. Going\ndeeper with convolutions. CoRR, abs/1409.4842, 2014.\nTieleman, T. and Hinton, G. Lecture 6.5—RmsProp: Divide the gradient by a running average of its recent\nmagnitude. COURSERA: Neural Networks for Machine Learning, 2012.\nWeston, J., Ratle, F., and Collobert, R. Deep learning via semi-supervised embedding. In ICML, 2008.\n10\nPublished as a conference paper at ICLR 2015\nA\nSUPPLEMENTARY MATERIAL: NETWORK ARCHITECTURES AND\nTRAINING PROCEDURES\nIn the supplementary material, we describe all network architectures and hyper-parameters used\nthroughout the paper.\nA.1\nCIFAR-10/CIFAR-100\nIn this section, we describe the teacher and FitNet architectures as well as hyper-parameters used in\nboth CIFAR-10/CIFAR-100 experiments.\nA.1.1\nTEACHERS\nWe used the CIFAR-10/CIFAR-100 maxout convolutional networks reported in Goodfellow et al.\n(2013b) as teachers. Both teachers have the same architecture, composed of 3 convolutional hidden\nlayers of 96-192-192 units, respectively. Each convolutional layer is followed by a maxout non-\nlinearity (with 2 linear pieces) and a max-pooling operator with respective windows sizes of 4x4,\n4x4 and 2x2 pixels. All max-pooling units have an overlap of 2x2 pixels. The third convolutional\nlayer is followed by a fully-connected maxout layer of 500 units (with 5 linear pieces) and a top\nsoftmax layer. The CIFAR-10/CIFAR-100 teachers are trained using stochastic gradient descent\nand momentum. Please refer to Goodfellow et al. (2013b) for more details.\nA.1.2\nFITNETS\nHere, we describe the FitNet architectures used in the Section 3 and Section 4. Each FitNet is\ncomposed of successive zero-padded convolutional layers of kernel size 3×3, followed by a maxout\nnon-linearity with two linear pieces. A non-overlapping 2 × 2 max-pooling follows some of the\nconvolutional layers; each network has a total of 3 max-pooling units. The last max-pooling takes\nthe maximum over all remaining spatial dimensions, leading to a 1 × 1 vector representation. The\nlast convolutional layer is followed by a fully-connected and a softmax layer, as the ones on CIFAR-\n10/100 teachers.\nTable 6 describes the architectures used for the depth experiment in Figure 2. Table 7 describes the\narchitectures for the efﬁciency-performance trade-off experiment in Table 5. The results reported in\nTable 1, Table 2 and Table 3 correspond to the FitNet 4 architecture.\nAll FitNet parameters were initialized randomly in U(-0.005,0.005). We used stochastic gradient\ndescent with RMSProp (Tieleman & Hinton, 2012) to train the FitNets, with an initial learning rate\n0.005 and a mini-batch size of 128. Parameter λ in Eq. (2) was initialized to 4 and decayed linearly\nduring 500 epochs reaching λ = 1. The relaxation term τ was set to 3.\nOn CIFAR-10, we divided the training set into 40K training examples and 10K validation examples.\nWe trained stage 1 by minimizing Eq. (3) and stopped the training after 100 epochs of no validation\nerror improvement, performing a maximum of 500 epochs. After that, we trained stage 2 by mini-\nmizing Eq. (2) using RMSprop, the same stopping criterion and the same hyper-parameters as stage\n1. We picked the optimal number of epochs according to the above-mentioned stopping criterion\nand retrained the FitNet on the whole 50K training examples (training + validation sets).\nOn CIFAR-100, we trained directly on the whole training set using stochastic gradient descent with\nRMSprop, the same hyper-parameters as CIFAR-10 FitNets and the number of epochs determined\nby CIFAR-10 stopping criterion.\nA.2\nMNIST\nIn this section, we describe the teacher and FitNet architectures as well as the hyper-parameters used\nin the MNIST experiments.\nWe trained a teacher network of maxout convolutional layers as reported in Goodfellow et al.\n(2013b). The teacher architecture has three convolutional maxout hidden layers (with 2 linear pieces\neach) of 48-48-24 units, respectively, followed by a spatial max-pooling of 4x4-4x4-2x2 pixels, with\n11\nPublished as a conference paper at ICLR 2015\n5 Layer\n7 Layer\n9 Layer\n11 Layer\nconv 3x3x64 (3x3x128)\nconv 3x3x16 (3x3x32)\nconv 3x3x16 (3x3x32)\nconv 3x3x16 (3x3x16)\npool 2x2\nconv 3x3x32 (3x3x64)\nconv 3x3x32 (3x3x32)\nconv 3x3x16 (3x3x32)\npool 2x2\npool 2x2\nconv 3x3x16 (3x3x32)\npool 2x2\nconv 3x3x64 (3x3x128)\nconv 3x3x32 (3x3x80)\nconv 3x3x32 (3x3x64)\nconv 3x3x32 (3x3x48)\npool 2x2\nconv 3x3x64 (3x3x80)\nconv 3x3x32 (3x3x80)\nconv 3x3x32 (3x3x64)\npool 2x2\nconv 3x3x32 (3x3x80)\nconv 3x3x32 (3x3x80)\npool 2x2\npool 2x2\nconv 3x3x64 (3x3x128)\nconv 3x3x64 (3x3x128)\nconv 3x3x48 (3x3x96)\nconv 3x3x48 (3x3x96)\npool 8x8\npool 8x8\nconv 3x3x64 (3x3x128)\nconv 3x3x48 (3x3x96)\npool 8x8\nconv 3x3x64 (3x3x128)\npool 8x8\nfc\nfc\nfc\nfc\nsoftmax\nsoftmax\nsoftmax\nsoftmax\nhint: 2←2\nhint: 4←2\nhint: 5←2\nhint: 7←2\nTable 6: Fitnet architectures with a computational budget of 30M (or 107M) of multiplications:\nconv sx × sy × c is a convolution of kernel size sx × sy with c outputs channels; pool sx × sy is\na non-overlapping pooling of size sx × sy; fc stands for fully connected. hint: FitNet ←teacher\nspeciﬁes the hint and guided layers used for hint-based training, respectively.\nFitNet 1\nFitNet 2\nFitNet 3\nFitNet 4\nconv 3x3x16\nconv 3x3x16\nconv 3x3x32\nconv 3x3x32\nconv 3x3x16\nconv 3x3x32\nconv 3x3x48\nconv 3x3x32\nconv 3x3x16\nconv 3x3x32\nconv 3x3x64\nconv 3x3x32\npool 2x2\npool 2x2\nconv 3x3x64\nconv 3x3x48\npool 2x2\nconv 3x3x48\npool 2x2\nconv 3x3x32\nconv 3x3x48\nconv 3x3x80\nconv 3x3x80\nconv 3x3x32\nconv 3x3x64\nconv 3x3x80\nconv 3x3x80\nconv 3x3x32\nconv 3x3x80\nconv 3x3x80\nconv 3x3x80\npool 2x2\npool 2x2\nconv 3x3x80\nconv 3x3x80\npool 2x2\nconv 3x3x80\nconv 3x3x80\npool 2x2\nconv 3x3x48\nconv 3x3x96\nconv 3x3x128\nconv 3x3x128\nconv 3x3x48\nconv 3x3x96\nconv 3x3x128\nconv 3x3x128\nconv 3x3x64\nconv 3x3x128\nconv 3x3x128\nconv 3x3x128\npool 8x8\npool 8x8\npool 8x8\nconv 3x3x128\nconv 3x3x128\nconv 3x3x128\npool 8x8\nfc\nfc\nfc\nfc\nsoftmax\nsoftmax\nsoftmax\nsoftmax\nhint: 6←2\nhint: 6←2\nhint: 8←2\nhint: 11←2\nTable 7: Performance-Efﬁciency FitNet architectures.\n12\nPublished as a conference paper at ICLR 2015\nan overlap of 2x2 pixels. The 3rd hidden layer is followed by a fully-connected softmax layer. As is\nGoodfellow et al. (2013b), we added zero padding to the second convolutional layer.\nWe designed a FitNet twice as deep as the teacher network and with roughly 8% of the parameters.\nThe student architecture has 6 maxout convolutional hidden layers (with 2 linear pieces each) of\n16-16-16-16-12-12 units, respectively. Max-pooling is only applied every second layer in regions of\n4x4-4x4-2x2 pixels, with an overlap of 2x2 pixels. The 6th convolutional hidden layer is followed\nby a fully-connected softmax layer.\nThe teacher network was trained as described in Goodfellow et al. (2013b). The FitNet was trained\nin a stage-wise fashion as described in Section 2. We divided the training set into a training set of\n50K samples and a validation set of 10K samples.\nAll network parameters where initialized randomly in U(-0.005,0.005). In the ﬁrst stage, the 4th\nlayer of the FitNet was trained to mimic the 2nd layer of the teacher network, by minimizing Eq.\n(3) through stochastic gradient descent. We used a mini-batch size of 128 samples and ﬁxed the\nlearning rate to 0.0005. We initialized λ to 4 and decayed it for the ﬁrst 150 epochs until it reached\n1. The training was stopped according to the following criterion: after 100 epochs of no validation\nerror improvement and performning a maximum of 500 epochs. We used the same mini-batch size,\nlearning rate and stopping criterion to train the second stage. The relaxation term τ was set to 3.\nA.3\nSVHN\nIn this section, we describe the teacher and FitNet architectures as well as the hyper-parameters used\nin the SVHN experiments.\nWe used SVHN maxout convolutional network described in as Goodfellow et al. (2013b) teacher.\nThe network is composed of 3 convolutional hidden layers of 64-128-128 units, respectively, fol-\nlowed by a fully-connected maxout layer of 400 units and a top softmax layer. The teacher training\nwas carried out as in Goodfellow et al. (2013b).\nWe used the FitNet 4 architecture outlined in Table 7, initializing the network parameters randomly\nin U(-0.005,0.005) and training with the same hyper-parameters as in CIFAR-10. In this case, we\nused the same early stopping as in CIFAR-10, but we did not retrain the FitNet on the whole training\nset (training + validation). The same hyper-parameters where used for both stages.\nA.4\nAFLW\nIn this section, we describe the teacher and FitNet architectures as well as the hyper-parameters used\nin the AFLW experiments.\nWe trained a teacher network of 3 ReLU convolutional layers of 128-512-512 units, respectively,\nfollowed by a sigmoid layer. Non-overlapping max-pooling of size 2 × 2 was performed after the\nﬁrst convolutional layer. We used receptive ﬁelds of 3-2-5 for each layer, respectively.\nWe designed two FitNets of 7 ReLU convolutional layers. Fitnet 1’s layers have 16-32-32-32-32-\n32-32-32 units, respectively, followed by a sigmoid layer. Fitnet 2’s layers have 32-64-64-64-64-\n64-64-64 units, respectively, followed by a sigmoid layer. In both cases, we used receptive ﬁelds of\n3 × 3 and, due to the really small image resolution, we did not perform any max-pooling.\nAll network parameters of both FitNets where initialized randomly in U(-0.05,0.05). Both FitNets\nwere trained in the stage-wise fashion described in Section 2. We used 90% of the data for training.\nIn the ﬁrst stage, the 5th layer of the FitNets were trained to mimic the 3rd layer of the teacher\nnetwork, by minimizing Eq. (3) through stochastic gradient descent. We used a mini-batch size of\n128 samples and initialized the learning rate to 0.001 and decayed it for the ﬁrst 100 epochs until\nreaching 0.01. We also used momentum. We initialized momentum to 0.1 and saturated it to 0.9 at\nepoch 100. We picked the best validation value after a 500 epochs. We used the same mini-batch\nsize, learning rate and stopping criterion to train the second stage. The relaxation term τ was set to\n3.\n13\n",
        "sentence": " , 2015), techniques of training networks in multiple stages (Simonyan & Zisserman, 2014; Romero et al., 2014) or with temporary companion loss functions attached to some of the layers (Szegedy et al. Deep highway networks are easy to optimize, but are they also beneficial for supervised learning where we are interested in generalization performance on a test set? To address this question, we compared highway networks to the thin and deep architectures termed Fitnets proposed recently by Romero et al. (2014) on the CIFAR-10 dataset augmented with random translations. 25M parameter both perform similar to the teacher network of Romero et al. (2014). Network Number of Layers Number of Parameters Accuracy Fitnet Results reported by Romero et al. (2014) Teacher 5 ∼9M 90. For comparison, results reported by Romero et al. (2014) using maxout networks are also shown."
    },
    {
        "title": "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks",
        "author": [
            "Saxe",
            "Andrew M",
            "McClelland",
            "James L",
            "Ganguli",
            "Surya"
        ],
        "venue": "URL http://arxiv.org/abs/1312",
        "citeRegEx": "Saxe et al\\.,? \\Q2013\\E",
        "shortCiteRegEx": "Saxe et al\\.",
        "year": 2013,
        "abstract": "Despite the widespread practical success of deep learning methods, our\ntheoretical understanding of the dynamics of learning in deep neural networks\nremains quite sparse. We attempt to bridge the gap between the theory and\npractice of deep learning by systematically analyzing learning dynamics for the\nrestricted case of deep linear neural networks. Despite the linearity of their\ninput-output map, such networks have nonlinear gradient descent dynamics on\nweights that change with the addition of each new hidden layer. We show that\ndeep linear networks exhibit nonlinear learning phenomena similar to those seen\nin simulations of nonlinear networks, including long plateaus followed by rapid\ntransitions to lower error solutions, and faster convergence from greedy\nunsupervised pretraining initial conditions than from random initial\nconditions. We provide an analytical description of these phenomena by finding\nnew exact solutions to the nonlinear dynamics of deep learning. Our theoretical\nanalysis also reveals the surprising finding that as the depth of a network\napproaches infinity, learning speed can nevertheless remain finite: for a\nspecial class of initial conditions on the weights, very deep networks incur\nonly a finite, depth independent, delay in learning speed relative to shallow\nnetworks. We show that, under certain conditions on the training data,\nunsupervised pretraining can find this special class of initial conditions,\nwhile scaled random Gaussian initializations cannot. We further exhibit a new\nclass of random orthogonal initial conditions on weights that, like\nunsupervised pre-training, enjoys depth independent learning times. We further\nshow that these initial conditions also lead to faithful propagation of\ngradients even in deep nonlinear networks, as long as they operate in a special\nregime known as the edge of chaos.",
        "full_text": "Exact solutions to the nonlinear dynamics of learning in\ndeep linear neural networks\nAndrew M. Saxe (asaxe@stanford.edu)\nDepartment of Electrical Engineering\nJames L. McClelland (mcclelland@stanford.edu)\nDepartment of Psychology\nSurya Ganguli (sganguli@stanford.edu)\nDepartment of Applied Physics\nStanford University, Stanford, CA 94305 USA\nAbstract\nDespite the widespread practical success of deep learning methods, our theoretical under-\nstanding of the dynamics of learning in deep neural networks remains quite sparse. We\nattempt to bridge the gap between the theory and practice of deep learning by systemati-\ncally analyzing learning dynamics for the restricted case of deep linear neural networks.\nDespite the linearity of their input-output map, such networks have nonlinear gradient de-\nscent dynamics on weights that change with the addition of each new hidden layer. We\nshow that deep linear networks exhibit nonlinear learning phenomena similar to those seen\nin simulations of nonlinear networks, including long plateaus followed by rapid transitions\nto lower error solutions, and faster convergence from greedy unsupervised pretraining ini-\ntial conditions than from random initial conditions. We provide an analytical description\nof these phenomena by ﬁnding new exact solutions to the nonlinear dynamics of deep\nlearning. Our theoretical analysis also reveals the surprising ﬁnding that as the depth of\na network approaches inﬁnity, learning speed can nevertheless remain ﬁnite: for a special\nclass of initial conditions on the weights, very deep networks incur only a ﬁnite, depth\nindependent, delay in learning speed relative to shallow networks. We show that, under\ncertain conditions on the training data, unsupervised pretraining can ﬁnd this special class\nof initial conditions, while scaled random Gaussian initializations cannot. We further ex-\nhibit a new class of random orthogonal initial conditions on weights that, like unsupervised\npre-training, enjoys depth independent learning times. We further show that these initial\nconditions also lead to faithful propagation of gradients even in deep nonlinear networks,\nas long as they operate in a special regime known as the edge of chaos.\nDeep learning methods have realized impressive performance in a range of applications, from visual object\nclassiﬁcation [1, 2, 3] to speech recognition [4] and natural language processing [5, 6]. These successes have\nbeen achieved despite the noted difﬁculty of training such deep architectures [7, 8, 9, 10, 11]. Indeed, many\nexplanations for the difﬁculty of deep learning have been advanced in the literature, including the presence of\nmany local minima, low curvature regions due to saturating nonlinearities, and exponential growth or decay\nof back-propagated gradients [12, 13, 14, 15]. Furthermore, many neural network simulations have observed\n1\narXiv:1312.6120v3  [cs.NE]  19 Feb 2014\nstrikingly nonlinear learning dynamics, including long plateaus of little apparent improvement followed by\nalmost stage-like transitions to better performance. However, a quantitative, analytical understanding of the\nrich dynamics of deep learning remains elusive. For example, what determines the time scales over which\ndeep learning unfolds? How does training speed retard with depth? Under what conditions will greedy\nunsupervised pretraining speed up learning? And how do the ﬁnal learned internal representations depend\non the statistical regularities inherent in the training data?\nHere we provide an exact analytical theory of learning in deep linear neural networks that quantitatively\nanswers these questions for this restricted setting. Because of its linearity, the input-output map of a deep\nlinear network can always be rewritten as a shallow network. In this sense, a linear network does not gain ex-\npressive power from depth, and hence will underﬁt and perform poorly on complex real world problems. But\nwhile it lacks this important aspect of practical deep learning systems, a deep linear network can nonetheless\nexhibit highly nonlinear learning dynamics, and these dynamics change with increasing depth. Indeed, the\ntraining error, as a function of the network weights, is non-convex, and gradient descent dynamics on this\nnon-convex error surface exhibits a subtle interplay between different weights across multiple layers of the\nnetwork. Hence deep linear networks provide an important starting point for understanding deep learning\ndynamics.\nTo answer these questions, we derive and analyze a set of nonlinear coupled differential equations describing\nlearning dynamics on weight space as a function of the statistical structure of the inputs and outputs. We\nﬁnd exact time-dependent solutions to these nonlinear equations, as well as ﬁnd conserved quantities in the\nweight dynamics arising from symmetries in the error function. These solutions provide intuition into how\na deep network successively builds up information about the statistical structure of the training data and\nembeds this information into its weights and internal representations. Moreover, we compare our analytical\nsolutions of learning dynamics in deep linear networks to numerical simulations of learning dynamics in\ndeep non-linear networks, and ﬁnd that our analytical solutions provide a reasonable approximation. Our\nsolutions also reﬂect nonlinear phenomena seen in simulations, including alternating plateaus and sharp pe-\nriods of rapid improvement. Indeed, it has been shown previously [16] that this nonlinear learning dynamics\nin deep linear networks is sufﬁcient to qualitatively capture aspects of the progressive, hierarchical differ-\nentiation of conceptual structure seen in infant development. Next, we apply these solutions to investigate\nthe commonly used greedy layer-wise pretraining strategy for training deep networks [17, 18], and recover\nconditions under which such pretraining speeds learning. We show that these conditions are approximately\nsatisﬁed for the MNIST dataset, and that unsupervised pretraining therefore confers an optimization advan-\ntage for deep linear networks applied to MNIST. Finally, we exhibit a new class of random orthogonal initial\nconditions on weights that, in linear networks, provide depth independent learning times, and we show that\nthese initial conditions also lead to faithful propagation of gradients even in deep nonlinear networks. We\nfurther show that these initial conditions also lead to faithful propagation of gradients even in deep nonlinear\nnetworks, as long as they operate in a special regime known as the edge of chaos. In this regime, synaptic\ngains are tuned so that linear ampliﬁcation due to propagation of neural activity through weight matrices\nexactly balances dampening of activity due to saturating nonlinearities. In particular, we show that even in\nnonlinear networks, operating in this special regime, Jacobians that are involved in backpropagating error\nsignals act like near isometries.\n1\nGeneral learning dynamics of gradient descent\nW 21\nW 32\nx ∈RN1\nh ∈RN2\ny ∈RN3\nFigure 1: The three layer network analyzed\nin this section.\nWe begin by analyzing learning in a three layer network (in-\nput, hidden, and output) with linear activation functions (Fig\n1). We let Ni be the number of neurons in layer i. The input-\noutput map of the network is y = W 32W 21x. We wish to\ntrain the network to learn a particular input-output map from\n2\na set of P training examples {xµ, yµ} , µ = 1, . . . , P. Training is accomplished via gradient descent on the\nsquared error PP\nµ=1\n\r\ryµ −W 32W 21xµ\r\r2 between the desired feature output, and the network’s feature\noutput. This gradient descent procedure yields the batch learning rule\n∆W 21 = λ\nP\nX\nµ=1\nW 32T \u0000yµxµT −W 32W 21xµxµT \u0001\n,\n∆W 32 = λ\nP\nX\nµ=1\n\u0000yµxµT −W 32W 21xµxµT \u0001\nW 21T ,\n(1)\nwhere λ is a small learning rate. As long as λ is sufﬁciently small, we can take a continuous time limit to\nobtain the dynamics,\nτ d\ndtW 21 = W 32T \u0000Σ31 −W 32W 21Σ11\u0001\n,\nτ d\ndtW 32 =\n\u0000Σ31 −W 32W 21Σ11\u0001\nW 21T ,\n(2)\nwhere Σ11 ≡PP\nµ=1 xµxµT is an N1 × N1 input correlation matrix, Σ31 ≡PP\nµ=1 yµxµT is an N3 × N1\ninput-output correlation matrix, and τ ≡1\nλ. Here t measures time in units of iterations; as t varies from 0\nto 1, the network has seen P examples corresponding to one iteration. Despite the linearity of the network’s\ninput-output map, the gradient descent learning dynamics given in Eqn (2) constitutes a complex set of\ncoupled nonlinear differential equations with up to cubic interactions in the weights.\n1.1\nLearning dynamics with orthogonal inputs\nOur fundamental goal is to understand the dynamics of learning in (2) as a function of the input statistics\nΣ11 and input-output statistics Σ31. In general, the outcome of learning will reﬂect an interplay between\ninput correlations, described by Σ11, and the input-output correlations described by Σ31. To begin, though,\nwe further simplify the analysis by focusing on the case of orthogonal input representations where Σ11 = I.\nThis assumption will hold exactly for whitened input data, a widely used preprocessing step.\nBecause we have assumed orthogonal input representations (Σ11 = I), the input-output correlation matrix\ncontains all of the information about the dataset used in learning, and it plays a pivotal role in the learning\ndynamics. We consider its singular value decomposition (SVD)\nΣ31 = U 33S31V 11T = PN1\nα=1 sαuαvT\nα,\n(3)\nwhich will be central in our analysis. Here V 11 is an N1 × N1 orthogonal matrix whose columns contain\ninput-analyzing singular vectors vα that reﬂect independent modes of variation in the input, U 33 is an N3 ×\nN3 orthogonal matrix whose columns contain output-analyzing singular vectors uα that reﬂect independent\nmodes of variation in the output, and S31 is an N3 × N1 matrix whose only nonzero elements are on the\ndiagonal; these elements are the singular values sα, α = 1, . . . , N1 ordered so that s1 ≥s2 ≥· · · ≥sN1.\nNow, performing the change of variables on synaptic weight space, W 21 = W\n21V 11T , W 32 = U 33W\n32,\nthe dynamics in (2) simplify to\nτ d\ndtW\n21 = W\n32T\n(S31 −W\n32W\n21),\nτ d\ndtW\n32 = (S31 −W\n32W\n21)W\n21T\n.\n(4)\nTo gain intuition for these equations, note that while the matrix elements of W 21 and W 32 connected neurons\nin one layer to neurons in the next layer, we can think of the matrix element W\n21\niα as connecting input mode\nvα to hidden neuron i, and the matrix element W\n32\nαi as connecting hidden neuron i to output mode uα. Let\naα be the αth column of W\n21, and let bαT be the αth row of W\n32. Intuitively, aα is a column vector of N2\nsynaptic weights presynaptic to the hidden layer coming from input mode α, and bα is a column vector of\n3\nN2 synaptic weights postsynaptic to the hidden layer going to output mode α. In terms of these variables,\nor connectivity modes, the learning dynamics in (4) become\nτ d\ndtaα = (sα −aα · bα) bα −\nX\nγ̸=α\nbγ (aα · bγ),\nτ d\ndtbα = (sα −aα · bα) aα −\nX\nγ̸=α\naγ (bα · aγ). (5)\nNote that sα = 0 for α > N1. These dynamics arise from gradient descent on the energy function\nE = 1\n2τ\nX\nα\n(sα −aα · bα)2 + 1\n2τ\nX\nα̸=β\n(aα · bβ)2,\n(6)\nand display an interesting combination of cooperative and competitive interactions. Consider the ﬁrst terms\nin each equation. In these terms, the connectivity modes from the two layers, aα and bα associated with the\nsame input-output mode of strength sα, cooperate with each other to drive each other to larger magnitudes\nas well as point in similar directions in the space of hidden units; in this fashion these terms drive the\nproduct of connectivity modes aα · bα to reﬂect the input-output mode strength sα. The second terms\ndescribe competition between the connectivity modes in the ﬁrst (aα) and second (bβ) layers associated with\ndifferent input modes α and β. This yields a symmetric, pairwise repulsive force between all distinct pairs of\nﬁrst and second layer connectivity modes, driving the network to a decoupled regime in which the different\nconnectivity modes become orthogonal.\n1.2\nThe ﬁnal outcome of learning\nThe ﬁxed point structure of gradient descent learning in linear networks was worked out in [19]. In the\nlanguage of the connectivity modes, a necessary condition for a ﬁxed point is aα ·bβ = sαδαβ, while aα and\nbα are zero whenever sα = 0. To satisfy these relations for undercomplete hidden layers (N2 < N1, N2 <\nN3), aα and bα can be nonzero for at most N2 values of α. Since there are rank(Σ31) ≡r nonzero values\nof sα, there are\n\u0012\nr\nN2\n\u0013\nfamilies of ﬁxed points. However, all of these ﬁxed points are unstable, except for\nthe one in which only the ﬁrst N2 strongest modes, i.e. aα and bα for α = 1, . . . , N2 are active. Thus\nremarkably, the dynamics in (5) has only saddle points and no non-global local minima [19]. In terms of the\noriginal synaptic variables W 21 and W 32, all globally stable ﬁxed points satisfy\nW 32W 21 = PN2\nα=1 sαuαvT\nα.\n(7)\n−2\n−1\n0\n1\n2\n−2\n−1.5\n−1\n−0.5\n0\n0.5\n1\n1.5\n2\na\nb\nFigure 2: Vector ﬁeld (blue), stable\nmanifold (red) and two solution tra-\njectories (green) for the two dimen-\nsional dynamics of a and b in (8),\nwith τ = 1, s = 1.\nHence when learning has converged, the network will represent the\nclosest rank N2 approximation to the true input-output correlation\nmatrix. In this work, we are interested in understanding the dynami-\ncal weight trajectories and learning time scales that lead to this ﬁnal\nﬁxed point.\n1.3\nThe time course of learning\nIt is difﬁcult though to exactly solve (5) starting from arbitrary initial\nconditions because of the competitive interactions between different\ninput-output modes. Therefore, to gain intuition for the general dy-\nnamics, we restrict our attention to a special class of initial conditions\nof the form aα and bα ∝rα for α = 1, . . . , N2, where rα · rβ = δαβ,\nwith all other connectivity modes aα and bα set to zero (see [20] for\n4\n0\n500\n1000\n0\n20\n40\n60\n80\nt (Epochs)\nmode strength\n \n \n0\n5\n10\n15\n20\n25\n30\n−0.1\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\nInput−output mode\n(thalf−tanaly)/tanaly\n \n \nLinear\nTanh\nFigure 3:\nLeft: Dynamics of learning in a three layer neural network. Curves show the strength of the\nnetwork’s representation of seven modes of the input-output correlation matrix over the course of learning.\nRed traces show analytical curves from Eqn. 12. Blue traces show simulation of full dynamics of a linear\nnetwork (Eqn. (2)) from small random initial conditions. Green traces show simulation of a nonlinear\nthree layer network with tanh activation functions. To generate mode strengths for the nonlinear network,\nwe computed the nonlinear network’s evolving input-output correlation matrix, and plotted the diagonal\nelements of U 33T Σ31tanhV 11 over time. The training set consists of 32 orthogonal input patterns, each\nassociated with a 1000-dimensional feature vector generated by a hierarchical diffusion process described\nin [16] with a ﬁve level binary tree and ﬂip probability of 0.1. Modes 1, 2, 3, 5, 12, 18, and 31 are plotted\nwith the rest excluded for clarity. Network training parameters were λ = 0.5e−3, N2 = 32, u0 = 1e−6.\nRight: Delay in learning due to competitive dynamics and sigmoidal nonlinearities. Vertical axis shows the\ndifference between simulated time of half learning and the analytical time of half learning, as a fraction of\nthe analytical time of half learning. Error bars show standard deviation from 100 simulations with random\ninitializations.\nsolutions to a partially overlapping but distinct set of initial condi-\ntions, further discussed in Supplementary Appendix A). Here rα is a ﬁxed collection of N2 vectors that\nform an orthonormal basis for synaptic connections from an input or output mode onto the set of hidden\nunits. Thus for this set of initial conditions, aα and bα point in the same direction for each alpha and differ\nonly in their scalar magnitudes, and are orthogonal to all other connectivity modes. Such an initialization\ncan be obtained by computing the SVD of Σ31 and taking W 32 = U 33DaRT , W 21 = RDbV 11T where\nDa, Db are diagonal, and R is an arbitrary orthogonal matrix; however, as we show\nin subsequent experiments, the solutions we ﬁnd are also excellent approximations to trajectories from small\nrandom initial conditions. It is straightforward to verify that starting from these initial conditions, aα and bα\nwill remain parallel to rα for all future time. Furthermore, because the different active modes are orthogonal\nto each other, they do not compete, or even interact with each other (all dot products in the second terms of\n(5)-(6) are 0).\nThus this class of conditions deﬁnes an invariant manifold in weight space where the modes evolve indepen-\ndently of each other.\nIf we let a = aα · rα, b = bα · rα, and s = sα, then the dynamics of the scalar projections (a, b) obeys,\nτ d\ndta = b (s −ab),\nτ d\ndtb = a (s −ab).\n(8)\nThus our ability to decouple the connectivity modes yields a dramatically simpliﬁed two dimensional non-\nlinear system. These equations can by solved by noting that they arise from gradient descent on the error,\nE(a, b) =\n1\n2τ (s −ab)2.\n(9)\nThis implies that the product ab monotonically approaches the ﬁxed point s from its initial value. Moreover,\nE(a, b) satisﬁes a symmetry under the one parameter family of scaling transformations a →λa, b →\nb\nλ.\nThis symmetry implies, through Noether’s theorem, the existence of a conserved quantity, namely a2 −b2,\n5\nwhich is a constant of motion. Thus the dynamics simply follows hyperbolas of constant a2 −b2 in the (a, b)\nplane until it approaches the hyperbolic manifold of ﬁxed points, ab = s. The origin a = 0, b = 0 is also a\nﬁxed point, but is unstable. Fig. 2 shows a typical phase portrait for these dynamics.\nAs a measure of the timescale of learning, we are interested in how long it takes for ab to approach s from\nany given initial condition. The case of unequal a and b is treated in the Supplementary Appendix A due to\nspace constraints. Here we pursue an explicit solution with the assumption that a = b, a reasonable limit\nwhen starting with small random initial conditions. We can then track the dynamics of u ≡ab, which from\n(8) obeys\nτ d\ndtu = 2u(s −u).\n(10)\nThis equation is separable and can be integrated to yield\nt = τ\nZ uf\nu0\ndu\n2u(s −u) = τ\n2s ln uf(s −u0)\nu0(s −uf).\n(11)\nHere t is the time it takes for u to travel from u0 to uf. If we assume a small initial condition u0 = ϵ, and\nask when uf is within ϵ of the ﬁxed point s, i.e. uf = s −ϵ, then the learning timescale in the limit ϵ →0\nis t = τ/s ln (s/ϵ) = O(τ/s) (with a weak logarithmic dependence on the cutoff). This yields a key result:\nthe timescale of learning of each input-output mode α of the correlation matrix Σ31 is inversely proportional\nto the correlation strength sα of the mode. Thus the stronger an input-output relationship, the quicker it is\nlearned.\nWe can also ﬁnd the entire time course of learning by inverting (11) to obtain\nuf(t) =\nse2st/τ\ne2st/τ −1 + s/u0\n.\n(12)\nThis time course describes the temporal evolution of the product of the magnitudes of all weights from an\ninput mode (with correlation strength s) into the hidden layers, and from the hidden layers to the same output\nmode. If this product starts at a small value u0 < s, then it displays a sigmoidal rise which asymptotes\nto s as t →∞. This sigmoid can exhibit sharp transitions from a state of no learning to full learning.\nThis analytical sigmoid learning curve is shown in Fig. 3 to yield a reasonable approximation to learning\ncurves in linear networks that start from random initial conditions that are not on the orthogonal, decoupled\ninvariant manifold–and that therefore exhibit competitive dynamics between connectivity modes–as well as\nin nonlinear networks solving the same task. We note that though the nonlinear networks behaved similarly\nto the linear case for this particular task, this is likely to be problem dependent.\n2\nDeeper multilayer dynamics\nThe network analyzed in Section 1 is the minimal example of a multilayer net, with just a single layer of\nhidden units. How does gradient descent act in much deeper networks? We make an initial attempt in this\ndirection based on initial conditions that yield particularly simple gradient descent dynamics.\nIn a linear neural network with Nl layers and hence Nl−1 weight matrices indexed by W l, l = 1, · · · , Nl−1,\nthe gradient descent dynamics can be written as\nτ d\ndtW l =\n Nl−1\nY\ni=l+1\nW i\n!T \"\nΣ31 −\n Nl−1\nY\ni=1\nW i\n!\nΣ11\n#  l−1\nY\ni=1\nW i\n!T\n,\n(13)\nwhere Qb\ni=a W i = W bW (b−1) · · · W (a−1)W a with the special case that Qb\ni=a W i = I, the identity, if\na > b.\n6\nTo describe the initial conditions, we suppose that there are Nl orthogonal matrices Rl that diagonalize\nthe starting weight matrices, that is, RT\nl+1Wl(0)Rl = Dl for all l, with the special case that R1 = V 11\nand RNl = U 33. This requirement essentially demands that the output singular vectors of layer l be the\ninput singular vectors of the next layer l + 1, so that a change in mode strength at any layer propagates to\nthe output without mixing into other modes. We note that this formulation does not restrict hidden layer\nsize; each hidden layer can be of a different size, and may be undercomplete or overcomplete. Making the\nchange of variables Wl = Rl+1W lRT\nl along with the assumption that Σ11 = I leads to a set of decoupled\nconnectivity modes that evolve independently of each other. In analogy to the simpliﬁcation occurring in the\nthree layer network from (2) to (8), each connectivity mode in the Nl layered network can be described by\nNl −1 scalars a1, . . . , aNl−1, whose dynamics obeys gradient descent on the energy function (the analog of\n(9)),\nE(a1, · · · , aNl−1) = 1\n2τ\n \ns −\nNl−1\nY\ni=1\nai\n!2\n.\n(14)\nThis dynamics also has a set of conserved quantities a2\ni −a2\nj arising from the energetic symmetry w.r.t. the\ntransformation ai →λai, aj →aj\nλ , and hence can be solved exactly. We focus on the invariant submanifold\nin which ai(t = 0) = a0 for all i, and track the dynamics of u = QNl−1\ni=1\nai, the overall strength of this\nmode, which obeys (i.e. the generalization of (10)),\nτ d\ndtu = (Nl −1)u2−2/(Nl−1)(s −u).\n(15)\nThis can be integrated for any positive integer Nl, though the expression is complicated. Once the overall\nstrength increases sufﬁciently, learning explodes rapidly.\nEqn. (15) lets us study the dynamics of learning as depth limits to inﬁnity. In particular, as Nl →∞we\nhave the dynamics\nτ d\ndtu = Nlu2(s −u)\n(16)\nwhich can be integrated to obtain\nt = τ\nNl\n\u0014 1\ns2 log\n\u0012uf(u0 −s)\nu0(uf −s)\n\u0013\n+\n1\nsu0\n−\n1\nsuf\n\u0015\n.\n(17)\nRemarkably this implies that, for a ﬁxed learning rate, the learning time as measured by the num-\nber of iterations required tends to zero as Nl goes to inﬁnity.\nThis result depends on the con-\ntinuous time formulation, however.\nAny implementation will operate in discrete time and must\nchoose a ﬁnite learning rate that yields stable dynamics.\nAn estimate of the optimal learn-\ning rate can be derived from the maximum eigenvalue of the Hessian over the region of interest.\n0\n50\n100\n0\n50\n100\n150\n200\n250\nNl (Number of layers)\nLearning time (Epochs)\n0\n50\n100\n0\n0.2\n0.4\n0.6\n0.8\n1\n1.2\nx 10\n−4\nOptimal learning rate\nNl (Number of layers)\nFigure 4:\nLeft: Learning time as a function of depth on MNIST. Right:\nEmpirically optimal learning rates as a function of depth.\nFor linear networks with ai =\naj = a, this optimal learn-\ning rate αopt decays with\ndepth as O\n\u0010\n1\nNls2\n\u0011\nfor large\nNl (see Supplementary Ap-\npendix B). Incorporating this\ndependence of the learning\nrate on depth, the learning\ntime as depth approaches in-\nﬁnity still surprisingly re-\nmains ﬁnite:\nwith the opti-\nmal learning rate, the differ-\nence between learning times\n7\nfor an Nl = 3 network and an Nl = ∞network is t∞−t3 ∼O (s/ϵ) for small ϵ (see Supplementary\nAppendix B.1). We emphasize that our analysis of learning speed is based on the number of iterations re-\nquired, not the amount of computation–computing one iteration of a deep network will require more time\nthan doing so in a shallow network.\nTo verify these predictions, we trained deep linear networks on the MNIST classiﬁcation task with depths\nranging from Nl = 3 to Nl = 100. We used hidden layers of size 1000, and calculated the iteration at\nwhich training error fell below a ﬁxed threshold corresponding to nearly complete learning. We optimized\nthe learning rate separately for each depth by training each network with twenty rates logarithmically spaced\nbetween 10−4 and 10−7 and picking the fastest. See Supplementary Appendix C for full experimental\ndetails. Networks were initialized with decoupled initial conditions and starting initial mode strength u0 =\n0.001. Fig. 4 shows the resulting learning times, which saturate, and the empirically optimal learning rates,\nwhich scale like O(1/Nl) as predicted.\nThus learning times in deep linear networks that start with decoupled initial conditions are only a ﬁnite\namount slower than a shallow network regardless of depth. Moreover, the delay incurred by depth scales\ninversely with the size of the initial strength of the association. Hence ﬁnding a way to initialize the mode\nstrengths to large values is crucial for fast deep learning.\n3\nFinding good weight initializations: on greediness and randomness\nThe previous subsection revealed the existence of a decoupled submanifold in weight space in which con-\nnectivity modes evolve independently of each other during learning, and learning times can be independent\nof depth, even for arbitrarily deep networks, as long as the initial composite, end to end mode strength,\ndenoted by u above, of every connectivity mode is O(1). What numerical weight initilization procedures\ncan get us close to this weight manifold, so that we can exploit its rapid learning properties?\nA breakthrough in training deep neural networks started with the discovery that greedy layer-wise unsu-\npervised pretraining could substantially speed up and improve the generalization performance of standard\ngradient descent [17, 18]. Unsupervised pretraining has been shown to speed the optimization of deep\nnetworks, and also to act as a special regularizer towards solutions with better generalization performance\n[18, 12, 13, 14]. At the same time, recent results have obtained excellent performance starting from carefully-\nscaled random initializations, though interestingly, pretrained initializations still exhibit faster convergence\n[21, 13, 22, 3, 4, 1, 23] (see Supplementary Appendix D for discussion). Here we examine analytically how\nunsupervised pretraining achieves an optimization advantage, at least in deep linear networks, by ﬁnding\nthe special class of orthogonalized, decoupled initial conditions in the previous section that allow for rapid\nsupervised deep learning, for input-output tasks with a certain precise structure. Subsequently, we analyze\nthe properties of random initilizations.\nWe consider the following pretraining and ﬁnetuning procedure: First, using autoencoders as the unsuper-\nvised pretraining module [18, 12], the network is trained to produce its input as its output (yµ\npre = xµ).\nSubsequently, the network is ﬁnetuned on the ultimate input-output task of interest (e.g., a classiﬁcation\ntask). In the following we consider the case N2 = N1 for simplicity.\nDuring the pretraining phase, the input-output correlation matrix Σ31pre is simply the input correlation matrix\nΣ11. Hence the SVD of Σ31pre is PCA on the input correlation matrix, since Σ31pre = Σ11 = QΛQT ,\nwhere Q are eigenvectors of Σ11 and Λ is a diagonal matrix of variances. Our analysis of the learning\ndynamics in Section 1.1 does not directly apply, because here the input correlation matrix is not white. In\nSupplementary Appendix E we generalize our results to handle this case. During pretraining, the weights\napproach W 32W 21 = Σ31(Σ31)−1, but since they do not reach the ﬁxed point in ﬁnite time, they will\nend at W 32W 21 = QMQT where M is a diagonal matrix that is approaching the identity matrix during\n8\n0\n100\n200\n300\n400\n500\n1\n1.5\n2\n2.5\n3\nx 10\n4\nEpoch\nError\n \n \nPretrain\nRandom\n \n \n2000\n4000\n6000\n8000\n \n \n0\n5\n10\n15\nx 10\n5\nFigure 5: MNIST satisﬁes the consistency condition for greedy pretraining. Left: Submatrix from the raw\nMNIST input correlation matrix Σ11. Center: Submatrix of V 11Σ11V 11T which is approximately diagonal\nas required. Right: Learning curves on MNIST for a ﬁve layer linear network starting from random (black)\nand pretrained (red) initial conditions. Pretrained curve starts with a delay due to pretraining time. The small\nrandom initial conditions correspond to all weights chosen i.i.d. from a zero mean Gaussian with standard\ndeviation 0.01.\nlearning. Hence in general, W 32 = QM 1/2C−1 and W 21 = CM 1/2QT where C is any invertible matrix.\nWhen starting from small random weights, though, each weight matrix will end up with a roughly balanced\ncontribution to the overall map. This corresponds to having C ≈R2 where R2 is orthogonal. Hence at\nthe end of the pretraining phase, the input-to-hidden mapping will be W 21 = R2M 1/2QT where R2 is an\narbitrary orthogonal matrix.\nNow consider the ﬁne-tuning phase. Here the weights are trained on the ultimate task of interest with\ninput-output correlations Σ31 = U 33S31V 11. The matrix W 21 begins from the pretrained initial condition\nW 21 = R2M 1/2QT . For the ﬁne-tuning task, a decoupled initial condition for W 21 is one that can be\nwritten as W 21 = R2D1V 11T (see Section 2). Clearly, this will be possible only if\nQ = V 11.\n(18)\nThen the initial condition obtained from pretraining will also be a decoupled initial condition for the ﬁnetun-\ning phase, with initial mode strengths D1 = M 1/2 near one. Hence we can state the underlying condition\nrequired for successful greedy pretraining in deep linear networks: the right singular vectors of the ultimate\ninput-ouput task of interest V 11 must be similar to the principal components of the input data Q. This is a\nquantitatively precise instantiation of the intuitive idea that unsupervised pretraining can help in a subsequent\nsupervised learning task if (and only if) the statistical structure of the input is consistent with the structure\nof input-output map to be learned. Moreover, this quantitative instantiation of this intuitive idea gives a\nsimple empirical criterion that can be evaluated on any new dataset: given the input-output correlation Σ31\nand input correlation Σ11, compute the right singular vectors V 11 of Σ31 and check that V 11Σ11V 11T is\napproximately diagonal. If the condition in Eqn. (18) holds, autoencoder pretraining will have properly set\nup decoupled initial conditions for W 21, with an appreciable initial association strength near 1. This argu-\nment also goes through straightforwardly for layer-wise pretraining of deeper networks. Fig. 5 shows that\nthis consistency condition empirically holds on MNIST, and that a pretrained deep linear neural network\nlearns faster than one started from small random initial conditions, even accounting for pretraining time (see\nSupplementary Appendix F for experimental details). We note that this analysis is unlikely to carry over\ncompletely to nonlinear networks. Some nonlinear networks are approximately linear (e.g., tanh nonlin-\nearities) after initialization with small random initializations, and hence our solutions may describe these\ndynamics well early in learning. However as the network enters its nonlinear regime, our solutions should\nnot be expected to remain accurate.\nAs an alternative to greedy layerwise pre-training, [13] proposed choosing appropriately scaled initial condi-\ntions on weights that would preserve the norm of typical error vectors as they were backpropagated through\nthe deep network. In our context, the appropriate norm-preserving scaling for the initial condition of an\nN by N connectivity matrix W between any two layers corresponds to choosing each weight i.i.d. from a\n9\nFigure 6: A Left: Learning time (on MNIST using the same architecture and parameters as in Fig. 4) as a\nfunction of depth for different initial conditions on weights (scaled i.i.d. uniform weights chosen to preserve\nthe norm of propagated gradients as proposed in [13] (blue), greedy unsupervised pre-training (green) and\nrandom orthogonal matrices (red). The red curve lies on top of the green curve. Middle: Optimal learning\nrates as a function of depth for different weight initilizations. Right: The eigenvalue spectrum, in the\ncomplex plane, of a random 100 by 100 orthogonal matrix. B Histograms of the singular values of products\nof Nl −1 independent random Gaussian N by N matrices whose elements themselves are chosen i.i.d. from\na zero mean Gaussian with standard deviation 1/\n√\nN. In all cases, N = 1000, and histograms are taken over\n500 realizations of such random product matrices, yielding a total 5 · 105 singular values in each histogram.\nC Histograms of the eigenvalue distributions on the complex plane of the same product matrices in B. The\nbin width is 0.1, and, for visualization purposes, the bin containing the origin has been removed in each case;\nthis bin would otherwise dominate the histogram in the middle and right plots, as it contains 32% and 94%\nof the eigenvalues respectively.\nzero mean Gaussian with standard deviation 1/\n√\nN. With this choice, ⟨vT W T Wv⟩W = vT v, where ⟨·⟩W\ndenotes an average over distribution of the random matrix W. Moreover, the distribution of vT W T Wv con-\ncentrates about its mean for large N. Thus with this scaling, in linear networks, both the forward propagation\nof activity, and backpropagation of gradients is typically norm-preserving. However, with this initialization,\nthe learning time with depth on linear networks trained on MNIST grows with depth (Fig. 6A, left, blue).\nThis growth is in distinct contradiction with the theoretical prediction, made above, of depth independent\nlearning times starting from the decoupled submanifold of weights with composite mode strength O(1).\nThis suggests that the scaled random initialization scheme, despite its norm-preserving nature, does not ﬁnd\nthis submanifold in weight space. In contrast, learning times with greedy layerwise pre-training do not grow\nwith depth (Fig. 6A, left, green curve hiding under red curve), consistent with the predictions of our theory\n(as a technical point: note that learning times under greedy pre-training initialization in Fig. 6A are faster\nthan those obtained in Fig. 4 by explicitly choosing a point on the decoupled submanifold, because there\nthe initial mode strength was chosen to be small (u = 0.001) whereas greedy pre-training ﬁnds a composite\nmode strength closer to 1).\n10\nIs there a simple random initialization scheme that does enjoy the rapid learning properties of greedy-\nlayerwise pre-training? We empirically show (Fig. 6A, left, red curve) that if we choose the initial weights in\neach layer to be a random orthogonal matrix (satisifying W T W = I), instead of a scaled random Gaussian\nmatrix, then this orthogonal random initialization condition yields depth independent learning times just like\ngreedy layerwise pre-training (indeed the red and green curves are indistinguishable). Theoretically, why do\nrandom orthogonal initializations yield depth independent learning times, but not scaled random Gaussian\ninitializations, despite their norm preserving nature?\nThe answer lies in the eigenvalue and singular value spectra of products of Gaussian versus orthgonal random\nmatrices. While a single random orthogonal matrix has eigenvalue spectra lying exactly on the unit circle\nin the complex plane (Fig. 6A right), the eigenvalue spectra of random Gaussian matrices, whose elements\nhave variance 1/N, form a uniform distribution on a solid disk of radius 1 the complex plane (Fig. 6C left).\nMoreover the singular values of an orthogonal matrix are all exactly 1, while the squared singular values\nof a scaled Gaussian random matrix have the well known Marcenko-Pasteur distribution, with a nontrivial\nspread even as N →∞, (Fig. 6B left shows the distribution of singular values themselves). Now consider a\nproduct of these matrices across all Nl layers, representing the total end to end propagation of activity across\na deep linear network:\nWTot =\nNl−1\nY\ni=1\nW (i+1,i).\n(19)\nDue to the random choice of weights in each layer, WTot is itself a random matrix. On average, it preserves\nthe norm of a typical vector v no matter whether the matrices in each layer are Gaussian or orthogonal.\nHowever, the singular value spectra of WTot differ markedly in the two cases. Under random orthogonal\ninitilization in each layer, WTot is itself an orthogonal matrix and therefore has all singular values equal to\n1. However, under random Gaussian initialization in each layer, there is as of yet no complete theoretical\ncharacterization of the singular value distribution of WTot. We have computed it numerically as a function\nof different depths in Fig. 6B, and we ﬁnd that it develops a highly kurtotic nature as the depth increases.\nMost of the singular values become vanishingly small, while a long tail of very large singular values remain.\nThus WTot preserves the norm of a typical, randomly chosen vector v, but in a highly anisotropic manner,\nby strongly amplifying the projection of v onto a very small subset of singular vectors and attenuating v\nin all other directions. Intuitively WTot, as well as the linear operator W T\nTot that would be closely related\nto backpropagation of gradients to early layers, act as amplifying projection operators at large depth Nl.\nIn contrast, all of the eigenvalues of WTot in the scaled Gaussian case concentrate closer to the origin as\ndepth increases. This discrepancy between the behavior of the eigenvalues and singular values of WTot, a\nphenomenon that could occur only if the eigenvectors of WTot are highly non-orthogonal, reﬂects the highly\nnon-normal nature of products of random Gaussian matrices (a non-normal matrix is by deﬁnition a matrix\nwhose eigenvectors are non-orthogonal).\nWhile the combination of ampliﬁcation and projection in WTot can preserve norm, it is clear that it is not a\ngood way to backpropagate errors; the projection of error vectors onto a high dimensional subspace corre-\nsponding to small singular values would be strongly attenuated, yielding vanishingly small gradient signals\ncorresponding to these directions in the early layers. This effect, which is not present for random orthogonal\ninitializations or greedy pretraining, would naturally explain the long learning times starting from scaled\nrandom Gaussian initial conditions relative to the other initilizations in Fig. 6A left. For both linear and\nnonlinear networks, a more likely appropriate condition on weights for generating fast learning times would\nbe that of dynamical isometry. By this we mean that the product of Jacobians associated with error signal\nbackpropagation should act as a near isometry, up to some overall global O(1) scaling, on a subspace of\nas high a dimension as possible. This is equivalent to having as many singular values of the product of\nJacobians as possible within a small range around an O(1) constant, and is closely related to the notion of\nrestricted isometry in compressed sensing and random projections. Preserving norms is a necessary but not\nsufﬁcient condition for achieving dynamical isometry at large depths, as demonstrated in Fig. 6B, and we\n11\nhave shown that for linear networks, orthogonal initializations achieve exact dynamical isometry with all\nsingular values at 1, while greedy pre-training achieves it approximately.\nWe note that the discrepancy in learning times between the scaled Gaussian initialization and the orthogonal\nor pre-training initializations is modest for the depths of around 6 used in large scale applications, but is\nmagniﬁed at larger depths (Fig. 6A left). This may explain the modest improvement in learning times with\ngreedy pre-training versus random scaled Gaussian initializations observed in applications (see discussion in\nSupplementary Appendix D). We predict that this modest improvement will be magniﬁed at higher depths,\neven in nonlinear networks. Finally, we note that in recurrent networks, which can be thought of as inﬁnitely\ndeep feed-forward networks with tied weights, a very promising approach is a modiﬁcation to the training\nobjective that partially promotes dynamical isometry for the set of gradients currently being back-propagated\n[24].\n4\nAchieving approximate dynamical isometry in nonlinear networks\nWe have shown above that deep random orthogonal linear networks achieve perfect dynamical isometry.\nHere we show that nonlinear versions of these networks can also achieve good dynamical isometry proper-\nties. Consider the nonlinear feedforward dynamics\nxl+1\ni\n=\nX\nj\ng W (l+1,l)\nij\nφ(xl\nj),\n(20)\nwhere xl\ni denotes the activity of neuron i in layer l, W (l+1,l)\nij\nis a random orthogonal connectivity matrix from\nlayer l to l + 1, g is a scalar gain factor, and φ(x) is any nonlinearity that saturates as x →±∞. We show\nin Supplementary appendix G that there exists a critical value gc of the gain g such that if g < gc, activity\nwill decay away to zero as it propagates through the layers, while if g > gc, the strong linear positive gain\nwill combat the damping due to the saturating nonlinearity, and activity will propagate indeﬁnitely without\ndecay, no matter how deep the network is. When the nonlinearity is odd (φ(x) = −φ(−x)), so that the mean\nactivity in each layer is approximately 0, these dynamical properties can be quantitatively captured by the\nneural population variance in layer l,\nql ≡1\nN\nN\nX\ni=1\n(xl\ni)2.\n(21)\nThus liml→∞ql →0 for g < gc and liml→∞ql →q∞(g) > 0 for g > gc. When φ(x) = tanh(x), we\ncompute gc = 1 and numerically compute q∞(g) in Fig. 8 in Supplementary appendix G. Thus these non-\nlinear feedforward networks exhibit a phase-transition at the critical gain; above the critical gain, inﬁnitely\ndeep networks exhibit chaotic percolating activity propagation, so we call the critical gain gc the edge of\nchaos, in analogy with terminology for recurrent networks.\nNow we are interested in how errors at the ﬁnal layer Nl backpropagate back to earlier layers, and whether\nor not these gradients explode or decay with depth. To quantify this, for simplicity we consider the end to\nend Jacobian\nJNl,1\nij\n(xNl) ≡∂xNl\ni\n∂x1\nj\n\f\f\f\f\nxNl\n,\n(22)\nwhich captures how input perturbations propagate to the output. If the singular value distribution of this\nJacobian is well-behaved, with few extremely large or small singular values, then the backpropagation of\ngradients will also be well-behaved, and exhibit little explosion or decay. The Jacobian is evaluated at a\nparticular point xNl in the space of output layer activations, and this point is in turn obtained by iterating\n(20) starting from an initial input layer activation vector x1. Thus the singular value distribution of the\n12\n0\n1\n2\n3\nx 10\n−5\n0\n50\n100\nq = 0.2\ng = 0.9\n0\n2\n4\n6\nx 10\n−3\n0\n20\n40\n60\ng = 0.95\n0\n0.1\n0.2\n0.3\n0.4\n0\n10\n20\n30\n40\ng = 1\n0\n0.5\n1\n1.5\n2\n0\n50\n100\ng = 1.05\n0\n2\n4\n6\n0\n100\n200\n300\n400\ng = 1.1\n0\n1\n2\n3\nx 10\n−5\n0\n10\n20\n30\n40\nq = 1\n0\n1\n2\n3\n4\nx 10\n−3\n0\n10\n20\n30\n40\n0\n0.1\n0.2\n0.3\n0.4\n0\n10\n20\n30\n40\n0\n0.5\n1\n1.5\n0\n50\n100\n0\n1\n2\n3\n4\n0\n100\n200\n300\n400\n0\n1\n2\n3\nx 10\n−5\n0\n10\n20\n30\n40\nq = 4\n0\n1\n2\n3\n4\nx 10\n−3\n0\n10\n20\n30\n0\n0.1\n0.2\n0.3\n0.4\n0\n10\n20\n30\n40\n0\n0.5\n1\n1.5\n0\n50\n100\n150\n0\n1\n2\n3\n0\n200\n400\n600\nFigure 7: Singular value distribution of the end to end Jacobian, deﬁned in (22), for various values of the\ngain g in (20) and the input layer population variance q = q1 in (21). The network architecture consists of\nNl = 100 layers with N = 1000 neurons per layer, as in the linear case in Fig. 6B.\nJacobian will depend not only on the gain g, but also on the initial condition x1. By rotational symmetry,\nwe expect this distribution to depend on x1, only through its population variance q1. Thus for large N, the\nsingular value distribution of the end-to-end Jacobian in (22) (the analog of WTot in (19) in the linear case),\ndepends on only two parameters: gain g and input population variance q1.\nWe have numerically computed this singular value distribution as a function of these two parameters in Fig.\n7, for a single random orthogonal nonlinear network with N = 1000 and Nl = 100. These results are\ntypical; replotting the results for different random networks and different initial conditions (with the same\ninput variance) yield very similar results. We see that below the edge of chaos, when g < 1, the linear\ndampening over many layers yields extremely small singular values. Above the edge of chaos, when g > 1,\nthe combination of positive linear ampliﬁcation, and saturating nonlinear dampening yields an anisotropic\ndistribution of singular values. At the edge of chaos, g = 1, an O(1) fraction of the singular value distribu-\n13\ntion is concentrated in a range that remains O(1) despite 100 layers of propagation, reﬂecting appoximate\ndynamical isometry. Moreover, this nice property at g = 1 remains valid even as the input variance q1 is\nincreased far beyond 1, where the tanh function enters its nonlinear regime. Thus the right column of Fig.\n7 at g near 1 indicates that the useful dynamical isometry properties of random orthogonal linear networks\ndescribed above survives in nonlinear networks, even when activity patterns enter deeply into the nonlinear\nregime in the input layers. Interestingly, the singular value spectrum is more robust to perturbations that\nincrease g from 1 relative to those that decrease g. Indeed, the anisotropy in the singular value distribution at\ng = 1.1 is relatively mild compared to that of random linear networks with scaled Gaussian initial conditions\n(compare the bottom row of Fig. 7 with the right column of panel B in Fig. 6). Thus overall, these numerical\nresults suggest that being just beyond the edge of orthogonal chaos may be a good regime for learning in\ndeep nonlinear networks.\n5\nDiscussion\nIn summary, despite the simplicity of their input-output map, the dynamics of learning in deep linear net-\nworks reveals a surprising amount of rich mathematical structure, including nonlinear hyperbolic dynamics,\nplateaus and sudden performance transitions, a proliferation of saddle points, symmetries and conserved\nquantities, invariant submanifolds of independently evolving connectivity modes subserving rapid learning,\nand most importantly, a sensitive but computable dependence of learning time scales on input statistics, ini-\ntial weight conditions, and network depth. With the right initial conditions, deep linear networks can be only\na ﬁnite amount slower than shallow networks, and unsupervised pretraining can ﬁnd these initial conditions\nfor tasks with the right structure. Moreover, we introduce a mathematical condition for faithful backprop-\nagation of error signals, namely dynamical isometry, and show, surprisingly that random scaled Gaussian\ninitializations cannot achieve this condition despite their norm-preserving nature, while greedy pre-training\nand random orthogonal initialization can, thereby achieving depth independent learning times. Finally, we\nshow that the property of dynamical isometry survives to good approximation even in extremely deep non-\nlinear random orthogonal networks operating just beyond the edge of chaos. At the cost of expressivity, deep\nlinear networks gain theoretical tractability and may prove fertile for addressing other phenomena in deep\nlearning, such as the impact of carefully-scaled initializations [13, 23], momentum [23], dropout regulariza-\ntion [1], and sparsity constraints [2]. While a full analytical treatment of learning in deep nonlinear networks\ncurrently remains open, one cannot reasonably hope to move towards such a theory without ﬁrst completely\nunderstanding the linear case. In this sense, our work fulﬁlls an essential pre-requisite for progress towards\na general, quantitative theory of deep learning.\nReferences\n[1] A. Krizhevsky, I. Sutskever, and G.E. Hinton. ImageNet Classiﬁcation with Deep Convolutional Neural\nNetworks. In Advances in Neural Information Processing Systems 25, 2012.\n[2] Q.V. Le, M.A. Ranzato, R. Monga, M. Devin, K. Chen, G.S. Corrado, J. Dean, and A.Y. Ng. Build-\ning high-level features using large scale unsupervised learning. In 29th International Conference on\nMachine Learning, 2012.\n[3] D. Ciresan, U. Meier, and J. Schmidhuber. Multi-column Deep Neural Networks for Image Classiﬁca-\ntion. In IEEE Conf. on Computer Vision and Pattern Recognition, pages 3642–3649, 2012.\n[4] A. Mohamed, G.E. Dahl, and G. Hinton. Acoustic Modeling Using Deep Belief Networks. IEEE\nTransactions on Audio, Speech, and Language Processing, 20(1):14–22, January 2012.\n[5] R. Collobert and J. Weston. A Uniﬁed Architecture for Natural Language Processing: Deep Neural\nNetworks with Multitask Learning. In Proceedings of the 25th International Conference on Machine\nLearning, 2008.\n14\n[6] R. Socher, J. Bauer, C.D. Manning, and A.Y. Ng. Parsing with Compositional Vector Grammars. In\nAssociation for Computational Linguistics Conference, 2013.\n[7] S. Hochreiter. Untersuchungen zu dynamischen neuronalen Netzen. PhD thesis, TU Munich, 1991.\n[8] Y. Bengio, P. Simard, and P. Frasconi. Learning Long-Term Dependencies with Gradient Descent is\nDifﬁcult. IEEE Transactions on Neural Networks, 5(2):157–166, 1994.\n[9] Y. LeCun, L. Bottou, G.B. Orr, and K.R. M¨uller. Efﬁcient BackProp. Neural networks: Tricks of the\ntrade, 1998.\n[10] Y. Bengio and Y. LeCun. Scaling learning algorithms towards AI. In L. Bottou, O. Chapelle, D. De-\nCoste, and J. Weston, editors, Large-Scale Kernel Machines, number 1, pages 1–41. MIT Press, 2007.\n[11] D. Erhan, P.A. Manzagol, Y. Bengio, S. Bengio, and P. Vincent. The Difﬁculty of Training Deep Ar-\nchitectures and the Effect of Unsupervised Pre-Training. In 12th International Conference on Artiﬁcial\nIntelligence and Statistics, volume 5, 2009.\n[12] Y. Bengio. Learning Deep Architectures for AI. 2009.\n[13] X. Glorot and Y. Bengio. Understanding the difﬁculty of training deep feedforward neural networks.\n13th International Conference on Artiﬁcial Intelligence and Statistics, 2010.\n[14] D. Erhan, Y. Bengio, A. Courville, P.A. Manzagol, and P. Vincent. Why does unsupervised pre-training\nhelp deep learning? Journal of Machine Learning Research, 11:625–660, 2010.\n[15] Y.N. Dauphin and Y. Bengio. Big Neural Networks Waste Capacity. In International Conference on\nLearning Representations, 2013.\n[16] A.M. Saxe, J.L. McClelland, and S. Ganguli. Learning hierarchical category structure in deep neural\nnetworks. In Proceedings of the 35th Annual Conference of the Cognitive Science Society, 2013.\n[17] G.E. Hinton and R.R. Salakhutdinov. Reducing the dimensionality of data with neural networks. Sci-\nence, 313(5786):504–7, July 2006.\n[18] Y. Bengio, P. Lamblin, D. Popovici, and H. Larochelle. Greedy Layer-Wise Training of Deep Net-\nworks. Advances in Neural Information Processing Systems 20, 2007.\n[19] P. Baldi and K. Hornik. Neural networks and principal component analysis: Learning from examples\nwithout local minima. Neural Networks, 2(1):53–58, January 1989.\n[20] K. Fukumizu. Effect of Batch Learning In Multilayer Neural Networks. In Proceedings of the 5th\nInternational Conference on Neural Information Processing, pages 67–70, 1998.\n[21] J. Martens. Deep learning via Hessian-free optimization. In Proceedings of the 27th International\nConference on Machine Learning, 2010.\n[22] O. Chapelle and D. Erhan. Improved Preconditioner for Hessian Free Optimization. In NIPS Workshop\non Deep Learning and Unsupervised Feature Learning, 2011.\n[23] I. Sutskever, J. Martens, G. Dahl, and G.E. Hinton. On the importance of initialization and momentum\nin deep learning. In 30th International Conference on Machine Learning, 2013.\n[24] Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. On the difﬁculty of training recurrent neural\nnetworks. Technical report, Universite de Montreal, 2012.\n[25] P. Lamblin and Y. Bengio. Important gains from supervised ﬁne-tuning of deep architectures on large\nlabeled sets. In NIPS Workshop on Deep Learning and Unsupervised Feature Learning, 2010.\n15\nSupplementary Material\nA\nHyperbolic dynamics of learning\nIn Section 1.3 of the main text we treat the dynamics of learning in three layer networks where mode\nstrengths in each layer are equal, i.e, a = b, a reasonable limit when starting with small random initial\nconditions. More generally, though, we are interested in how long it takes for ab to approach s from any\ngiven initial condition. To access this, given the hyperbolic nature of the dynamics, it is useful to make the\nhyperbolic change of coordinates,\na = √c0 cosh θ\n2\nb = √c0 sinh θ\n2\nfor a2 > b2\n(23)\na = √c0 sinh θ\n2\nb = √c0 cosh θ\n2\nfor a2 < b2.\n(24)\nThus θ parametrizes the dynamically invariant manifolds a2 −b2 = ±c0. For any c0 and θ, this coordinate\nsystem covers the region a + b > 0, which is the basin of attraction of the upper right component of the\nhyperbola ab = s. A symmetric situation exists for a+b < 0, which is attracted to the lower left component\nof ab = s. We use θ as a coordinate to follow the dynamics of the product ab, and using the relations\nab = c0 sinh θ and a2 + b2 = c0 cosh θ, we obtain\nτ dθ\ndt = s −c0 sinh θ.\n(25)\nThis differential equation is separable in θ and t and can be integrated to yield\nt = τ\nZ θf\nθ0\ndθ\ns −c0 sinh θ =\nτ\np\nc2\n0 + s2\n\"\nln\np\nc2\n0 + s2 + c0 + s tanh θ\n2\np\nc2\n0 + s2 −c0 −s tanh θ\n2\n#θf\nθ0\n.\n(26)\nHere t is the amount of time it takes to travel from θ0 to θf along the hyperbola a2 −b2 = ±c0. The ﬁxed\npoint lies at θ = sinh−1 s/c0, but the dynamics cannot reach the ﬁxed point in ﬁnite time. Therefore we\nintroduce a cutoff ϵ to mark the endpoint of learning, so that θf obeys sinh θf = (1 −ϵ)s/c0 (i.e. ab is\nclose to s by a factor 1 −ϵ). We can then average over the initial conditions c0 and θ0 to obtain the expected\nlearning time of an input-output relation that has a correlation strength s. Rather than doing this, it is easier\nto obtain a rough estimate of the timescale of learning under the assumption that the initial weights are small,\nso that c0 and θ0 are close to 0. In this case t = O(τ/s) (with a weak logarithmic dependence on the cutoff\n(i.e. ln(1/ϵ)). This modestly generalizes the result given in the main text: the timescale of learning of each\ninput-output mode α of the correlation matrix Σ31 is inversely proportional to the correlation strength sα of\nthe mode even when a and b differ slightly, i.e., c0 small. This is not an unreasonable limit for random initial\nconditions because |c0| = |a · a −b · b| where a and b are random vectors of N2 synaptic weights into and\nout of the hidden units. Thus we expect the lengths of the two random vectors to be approximately equal\nand therefore c0 will be small relative to the length of each vector.\nThese solutions are distinctly different from solutions for learning dynamics in three layer networks found\nin [20]. In our notation, in [20], it was shown that if the initial vectors aα and bα satisfy the matrix identity\nP\nα aαaαT = P\nα bαbαT then the dynamics of learning becomes equivalent to a matrix Riccatti equation.\nHowever, the hyperbolic dynamics derived here arises from a set of initial conditions that do not satisfy the\nrestrictions of [20] and therefore do not arise through a solution to a matrix Ricatti equation. Moreover,\nin going beyond a statement of the matrix Riccatti solution, our analysis provides intuition about the time-\nscales over which the learning dynamics unfolds, and crucially, our methods extend beyond the three layer\ncase to the arbitrary Nl layer case, not studied in [20].\n16\nB\nOptimal discrete time learning rates\nIn Section 2 we state results on the optimal learning rate as a function of depth in a deep linear network,\nwhich we derive here. Starting from the decoupled initial conditions given in the main text, the dynamics\narise from gradient descent on\nE(a1, · · · , aNl−1) = 1\n2τ\n \ns −\nNl−1\nY\nk=1\nak\n!\n.\n(27)\nHence for each ai we have\n∂E\n∂ai\n= −1\nτ\n \ns −\nNl−1\nY\nk=1\nak\n! \n\nNl−1\nY\nk̸=i\nak\n\n≡f(ai)\n(28)\nThe elements of the Hessian are thus\n∂2E\n∂aiaj\n=\n1\nτ\n\n\nNl−1\nY\nk̸=j\nak\n\n\n\n\nNl−1\nY\nk̸=i\nak\n\n−1\nτ\n \ns −\nNl−1\nY\nk=1\nak\n! \n\nNl−1\nY\nk̸=i,j\nak\n\n\n(29)\n≡\ng(ai, aj)\n(30)\nfor i ̸= j, and\n∂2E\n∂a2\ni\n= 1\nτ\n\n\nNl−1\nY\nk̸=i\nak\n\n\n2\n≡h(ai)\n(31)\nfor i = j.\nWe now assume that we start on the symmetric manifold, such that ai = aj = a for all i, j. Thus we have\nE(a)\n=\n1\n2τ\n\u0000s −aNl−1\u0001\n,\n(32)\nf(a)\n=\n−1\nτ\n\u0000s −aNl−1\u0001\naNl−2,\n(33)\ng(a)\n=\n2\nτ a2Nl−4 −1\nτ saNl−3\n(34)\nh(a)\n=\n1\nτ a2Nl−4\n(35)\nThe Hessian is\nH(a) =\n\n\nh\ng\n· · ·\ng\ng\ng\nh\n· · ·\ng\ng\n...\n...\n...\ng\ng\n· · ·\nh\ng\ng\ng\n· · ·\ng\nh\n\n\n.\n(36)\nOne eigenvector is v1 = [11 · · · 1]T with eigenvalue λ1 = h + (Nl −2)g, or\nλ1 = (2Nl −3)1\nτ a2Nl−4 −(Nl −2)1\nτ saNl−3.\n(37)\n17\nNow consider the second order update (Newton-Raphson) (here we use 1 to denote a vector of ones)\nat+11\n=\nat1 −H−1f(at)1\n(38)\n=\nat1 −f(at)H−11\n(39)\nat+1\n=\nat −f(at)/λ1(at)\n(40)\nNote that the basin of attraction does not include small initial conditions, because for small a the Hessian is\nnot positive deﬁnite.\nTo determine the optimal learning rate for ﬁrst order gradient descent, we compute the maximum of λ1 over\nthe range of mode strengths that can be visited during learning, i.e., a ∈[0, s1/(Nl−1)]. This occurs at the\noptimum, aopt = s1/(Nl−1). Hence substituting this into (37) we have\nλ1(aopt) = (Nl −1)1\nτ s\n2Nl−4\nNl−1 .\n(41)\nThe optimal learning rate α is proportional to 1/λ1(aopt), and hence scales as\nα ∼O\n\u0012\n1\nNls2\n\u0013\n(42)\nfor large Nl.\nB.1\nLearning speeds with optimized learning rate\nHow does the optimal learning rate impact learning speeds? We compare the three layer learning time to the\ninﬁnite depth limit learning time, with learning rate set inversely proportional to Eqn. (41) with proportion-\nality constant c.\nThis yields a three layer learning time t3 of\nt3 = c ln uf(s −u0)\nu0(s −uf)\n(43)\nand an inﬁnite layer learning time t∞of\nt∞= c\n\u0014\nlog\n\u0012uf(u0 −s)\nu0(uf −s)\n\u0013\n+ s\nu0\n−s\nuf\n\u0015\n,\n(44)\nHence the difference is\nt∞−t3 = cs\nu0\n−cs\nuf\n≈cs\nϵ\n(45)\nwhere the ﬁnal approximation is for u0 = ϵ, uf = s −ϵ, and ϵ small. Thus very deep networks incur only a\nﬁnite delay relative to shallow networks.\nC\nExperimental setup for MNIST depth experiment\nWe trained deep linear networks on the MNIST dataset with ﬁfteen different depths Nl\n=\n{3, 5, 8, 10, 14, 20, 28, 36, 44, 54, 64, 74, 84, 94, 100}. Given a 784-dimensional input example, the network\ntried to predict a 10-dimensional output vector containing a 1 in the index for the correct class, and zeros\nelsewhere. The network was trained using batch gradient descent via Eqn. (13) on the 50,000 sample MNIST\ntraining dataset. We note that Eqn. (13) makes use of the linearity of the network to speed training and re-\nduce memory requirements. Instead of forward propagating all 50,000 training examples, we precompute\n18\nΣ31 and forward propagate only it. This enables experiments on very deep networks that otherwise would\nbe computationally infeasible. Experiments were accelerated on GPU hardware using the GPUmat package.\nWe used overcomplete hidden layers of size 1000. Here the overcompleteness is simply to demonstrate the\napplicability of the theory to this case; overcompleteness does not improve the representational power of\nthe network. Networks were initialized with decoupled initial conditions and starting initial mode strength\nu0 = 0.001, as described in the text. The random orthogonal matrices Rl were selected by generating ran-\ndom Gaussian matrices and computing a QR decomposition to obtain an orthogonal matrix. Learning times\nwere calculated as the iteration at which training error fell below a ﬁxed threshold of 1.3×104 corresponding\nto nearly complete learning. Note that this level of performance is grossly inferior to what can be obtained\nusing nonlinear networks, which reﬂects the limited capacity of a linear network. We optimized the learning\nrate λ separately for each depth by training each network with twenty rates logarithmically spaced between\n10−4 and 10−7 and picking the one that yielded the minimum learning time according to our threshold crite-\nrion. The range 10−4 and 10−7 was selected via preliminary experiments to ensure that the optimal learning\nrate always lay in the interior of the range for all depths.\nD\nEfﬁcacy of unsupervised pretraining\nRecently high performance has been demonstrated in deep networks trained from random initial conditions\n[21, 13, 22, 3, 4, 1, 23], suggesting that deep networks may not be as hard to train as previously thought.\nThese results show that pretraining is not necessary to obtain state-of-the-art performance, and to achieve\nthis they make use of a variety of techniques including carefully-scaled random initializations, more sophis-\nticated second order or momentum-based optimization methods, and specialized convolutional architectures.\nIt is therefore important to evaluate whether unsupervised pretraining is still useful, even if it is no longer\nnecessary, for training deep networks. In particular, does pretraining still confer an optimization advantage\nand generalization advantage when used in conjunction with these new techniques? Here we review results\nfrom a variety of papers, which collectively show that unsupervised pretraining still confers an optimization\nadvantage and a generalization advantage.\nD.1\nOptimization advantage\nThe optimization advatage of pretraining refers to faster convergence to the local optimum (i.e., faster learn-\ning speeds) when starting from pretrained initializations as compared to random initializations. Faster learn-\ning speeds starting from pretrained initial conditions have been consistently found with Hessian free opti-\nmization [21, 22]. This ﬁnding holds for two carefully-chosen random initialization schemes, the sparse\nconnectivity scheme of [21], and the dense scaled scheme of [13] (as used by [22]). Hence pretraining\nstill confers a convergence speed advantage with second order methods. Pretrained initial conditions also\nresult in faster convergence than carefully-chosen random initializations when optimizing with stochastic\ngradient descent [22, 13]. In light of this, it appears that pretrained initial conditions confer an optimization\nadvantage beyond what can be obtained currently with carefully-scaled random initializations, regardless of\noptimization technique. If run to convergence, second order methods and well-chosen scalings can erase the\ndiscrepancy between the ﬁnal objective value obtained on the training set for pretrained relative to random\ninitializations [21, 22]. The optimization advantage is thus purely one of convergence speed, not of ﬁnding\na better local minimum. This coincides with the situation in linear networks, where all methods will even-\ntually attain the same global minimum, but the rate of convergence can vary. Our analysis shows why this\noptimization advantage due to pretraining persists over well-chosen random initializations.\nFinally, we note that Sutskever et al. show that careful random initialization paired with carefully-tuned\nmomentum can achieve excellent performance [23], but these experiments did not try pretrained initial\nconditions. Krizhevsky et al. used convolutional architectures and did not attempt pretraining [1]. Thus\n19\nthe possible utility of pretraining in combination with momentum, and in combination with convolutional\narchitectures, dropout, and large supervised datasets, remains unclear.\nD.2\nGeneralization advantage\nPretraining can also act as a special regularizer, improving generalization error in certain instances. This\ngeneralization advantage appears to persist with new second order methods [21, 22], and in comparison to\ngradient descent with careful random initializations [13, 22, 25, 4]. An analysis of this effect in deep linear\nnetworks is out of the scope of this work, though promising tools have been developed for the three layer\nlinear case [20].\nE\nLearning dynamics with task-aligned input correlations\nIn the main text we focused on orthogonal input correlations (Σ11 = I) for simplicity, and to draw out\nthe main intuitions. However our analysis can be extended to input correlations with a very particular\nstructure. Recall that we decompose the input output correlations using the SVD as Σ31 = U 33S31V 11T .\nWe can generalize our solutions to allow input correlations of the form Σ11 = V 11DV 11T . Intuitively, this\ncondition requires the axes of variation in the input to coincide with the axes of variation in the input-output\ntask, though the variances may differ. If we take D = I then we recover the whitened case Σ11 = I, and\nif we take D = Λ, then we can treat the autoencoding case. The ﬁnal ﬁxed points of the weights are given\nby the best rank N2 approximation to Σ31(Σ11)−1. Making the same change of variables as in Eqn. (4) we\nnow obtain\nτ d\ndtW\n21 = W\n32T\n(S31 −W\n32W\n21D),\nτ d\ndtW\n32 = (S31 −W\n32W\n21D)W\n21T\n.\n(46)\nwhich, again, is decoupled if W\n32 and W\n21 begin diagonal. Based on this it is straightforward to generalize\nour results for the learning dynamics.\nF\nMNIST pretraining experiment\nWe trained networks of depth 5 on the MNIST classiﬁcation task with 200 hidden units per layer, starting\neither from small random initial conditions with each weight drawn independently from a Gaussian distribu-\ntion with standard deviation 0.01, or from greedy layerwise pretrained initial conditions. For the pretrained\nnetwork, each layer was trained to reconstruct the output of the next lower layer. In the ﬁnetuning stage, the\nnetwork tried to predict a 10-dimensional output vector containing a 1 in the index for the correct class, and\nzeros elsewhere. The network was trained using batch gradient descent via Eqn. (13) on the 50,000 sam-\nple MNIST training dataset. Since the network is linear, pretraining initializes the network with principal\ncomponents of the input data, and, to the extent that the consistency condition of Eqn. (18) holds, decouples\nthese modes throughout the deep network, as described in the main text.\nG\nAnalysis of Neural Dynamics in Nonlinear Orthogonal Networks\nWe can derive a simple, analytical recursion relation for the propagation of neural population variance ql,\ndeﬁned in (21), across layers l under the nonlinear dynamics (20). We have\nql+1 = 1\nN\nN\nX\ni=1\n(xl+1\ni\n)2 = g2 1\nN\nN\nX\ni=1\nφ(xl\ni)2,\n(47)\n20\ndue to the dynamics in (20) and the orthogonality of W (l+1,l). Now we know that by deﬁnition, the layer\nl population xl\ni has normalized variance ql. If we further assume that the distribution of activity across\nneurons in layer l is well approximated by a Gaussian distribution, we can replace the sum over neurons i\nwith an integral over a zero mean unit variance Gaussian variable z:\nql+1 = g2\nZ\nDz φ\n\u0000p\nqlz\n\u00012,\n(48)\nwhere Dz ≡\n1\n√\n2πe−1\n2 z2 dz is the standard Gaussian measure. This map from input to output variance\n0\n0.5\n1\n1.5\n2\n0\n0.2\n0.4\n0.6\n0.8\nqin\nqout\nInput to Output Variance Map at g=1\n0\n0.5\n1\n1.5\n0\n0.5\n1\n1.5\n2\ng\nq∞(g)\nStable population variance versus gain\nFigure 8: Left: The map from variance in the input layer qin = ql to variance in the output layer qout = ql+1\nin (48) for g = 1 and φ(x) = tanh(x). Right: The stable ﬁxed points of this map, q∞(g), as a function of\nthe gain g. The red curve is the analytic theory obtained by numerically solving (49). The blue points are\nobtained via numerical simulations of the dynamics in (20) for networks of depth Nl = 30 with N = 1000\nneurons per layer. The asymptotic population variance q∞is obtained by averaging the population variance\nin the last 5 layers.\nis numerically computed for g = 1 and φ(x) = tanh(x) in Fig. 8, left (other values of g yield a simple\n21\nmultiplicative scaling of this map). This recursion relation has a stable ﬁxed point q∞(g) obtained by solving\nthe nonlinear ﬁxed point equation\nq∞= g2\nZ\nDz φ\n\u0000√q∞z\n\u00012.\n(49)\nGraphically, solving this equation corresponds to scaling the curve in Fig. 8 left by g2 and looking for\nintersections with the line of unity. For g < 1, the only solution is q∞= 0. For g > 1, this solution remains,\nbut it is unstable under the recurrence (48). Instead, for g > 1, a new stable solution appears for some\nnonzero value of q∞. The entire set of stable solutions as a function of g is shown as the red curve in Fig.\n8 right. It constitutes a theoretical prediction of the population variance at the deepest layers of a nonlinear\nnetwork as the depth goes to inﬁnity. It matches well for example, the empirical population variance obtained\nfrom numerical simulations of nonlinear networks of depth 30 (blue points in Fig. 8 right).\nOverall, these results indicate a dynamical phase transition in neural activity propagation through the non-\nlinear network as g crosses the critical value gc = 1. When g > 1, activity propagates in a chaotic manner,\nand so g = 1 constitutes the edge of chaos.\n22\n",
        "sentence": " Optimization of deep networks has proven to be considerably more difficult, leading to research on initialization schemes (Glorot & Bengio, 2010; Saxe et al., 2013; He et al., 2015), techniques of training networks in multiple stages (Simonyan & Zisserman, 2014; Romero et al."
    },
    {
        "title": "Very deep convolutional networks for large-scale image recognition",
        "author": [
            "Simonyan",
            "Karen",
            "Zisserman",
            "Andrew"
        ],
        "venue": "[cs],",
        "citeRegEx": "Simonyan et al\\.,? \\Q2014\\E",
        "shortCiteRegEx": "Simonyan et al\\.",
        "year": 2014,
        "abstract": "",
        "full_text": "",
        "sentence": ""
    },
    {
        "title": "Understanding locally competitive networks",
        "author": [
            "Srivastava",
            "Rupesh Kumar",
            "Masci",
            "Jonathan",
            "Gomez",
            "Faustino",
            "Schmidhuber",
            "Jürgen"
        ],
        "venue": "In International Conference on Learning Representations,",
        "citeRegEx": "Srivastava et al\\.,? \\Q2015\\E",
        "shortCiteRegEx": "Srivastava et al\\.",
        "year": 2015,
        "abstract": "Recently proposed neural network activation functions such as rectified\nlinear, maxout, and local winner-take-all have allowed for faster and more\neffective training of deep neural architectures on large and complex datasets.\nThe common trait among these functions is that they implement local competition\nbetween small groups of computational units within a layer, so that only part\nof the network is activated for any given input pattern. In this paper, we\nattempt to visualize and understand this self-modularization, and suggest a\nunified explanation for the beneficial properties of such networks. We also\nshow how our insights can be directly useful for efficiently performing\nretrieval over large datasets using neural networks.",
        "full_text": "Published as a conference paper at ICLR 2015\nUNDERSTANDING LOCALLY COMPETITIVE\nNETWORKS\nRupesh Kumar Srivastava, Jonathan Masci, Faustino Gomez & J¨urgen Schmidhuber\nIstituto Dalle Molle di studi sull’Intelligenza Artiﬁciale (IDSIA)\nScuola universitaria professionale della Svizzera italiana (SUPSI)\nUniversit`a della Svizzera italiana (USI)\nLugano, Switzerland\n{rupesh, jonathan, tino, juergen}@idsia.ch\nABSTRACT\nRecently proposed neural network activation functions such as rectiﬁed linear,\nmaxout, and local winner-take-all have allowed for faster and more effective train-\ning of deep neural architectures on large and complex datasets. The common trait\namong these functions is that they implement local competition between small\ngroups of computational units within a layer, so that only part of the network\nis activated for any given input pattern. In this paper, we attempt to visualize\nand understand this self-modularization, and suggest a uniﬁed explanation for the\nbeneﬁcial properties of such networks. We also show how our insights can be\ndirectly useful for efﬁciently performing retrieval over large datasets using neural\nnetworks.\n1\nINTRODUCTION\nRecently proposed activation functions for neural networks such as rectiﬁed linear (ReL (Glorot\net al., 2011)), maxout (Goodfellow et al., 2013a) and LWTA (Srivastava et al., 2013) are quite\nunlike sigmoidal activation functions. These functions depart from the conventional wisdom in that\nthey are not continuously differentiable (and sometimes non-continuous) and are piecewise linear.\nNevertheless, many researchers have found that such networks can be trained faster and better than\nsigmoidal networks, and they are increasingly in use for learning from large and complex datasets\n(Krizhevsky et al., 2012; Zeiler et al., 2013). Past research has shown observational evidence that\nsuch networks have beneﬁcial properties such as not requiring unsupervised training for weight\ninitialization (Glorot et al., 2011), better gradient ﬂow (Goodfellow et al., 2013a) and mitigation of\ncatastrophic forgetting (Srivastava et al., 2013; Goodfellow et al., 2014). Recently, the expressive\npower of deep networks with such functions has been theoretically analyzed (Pascanu et al., 2013).\nHowever, we are far from a complete understanding of their behavior and advantages over sigmoidal\nnetworks, especially during learning. This paper sheds additional light on the properties of such\nnetworks by interpreting them as models of models.\nA common theme among the ReL, maxout and LWTA activation functions is that they are locally\ncompetitive. Maxout and LWTA utilize explicit competition between units in small groups within a\nlayer, while in the case of the rectiﬁed linear function, the weighted input sum competes with a ﬁxed\nvalue of 0. Networks with such functions are often trained with the dropout regularization technique\n(Hinton et al., 2012) for improved generalization.\nWe start from the observation that in locally competitive networks, a subnetwork of units has non-\nzero activations for each input pattern. Instead of treating a neural network as a complex function\napproximator, the expressive power of the network can be interpreted to be coming from its ability\nto activate different subsets of linear units for different patterns. We hypothesize that the network\nacts as a model that can switch between “submodels” (subnetworks) such that similar submodels\nrespond to similar patterns. As evidence of this behavior, we analyze the activated subnetworks for\na large subset of a dataset (which is not used for training) and show that the subnetworks activated for\ndifferent examples exhibit a structure consistent with our hypothesis. These observations provide a\nuniﬁed explanation for improved credit assignment in locally competitive networks during training,\nwhich is believed to be the main reason for their success. Our new point of view suggests a link\nbetween these networks and competitive learning approaches of the past decades. We also show that\n1\narXiv:1410.1165v3  [cs.NE]  9 Apr 2015\nPublished as a conference paper at ICLR 2015\nFigure 1: Comparison of rectiﬁed linear units\n(ReLUs), local winner-take-all (LWTA), and\nmaxout activation functions. The pre- and post-\nsynaptic activations of the units are shown on\nthe left and right side of the units respectively.\nThe shaded units are ‘active’ – non-zero activa-\ntions and errors ﬂow through them. The main\ndifference between maxout and LWTA is that\nthe post-synaptic activation can ﬂow through\nconnections with different weight depending on\nthe winning unit in LWTA. For maxout, the out-\ngoing weight is the same for all units in a block.\n0\n20\n40\n60\n80\n100\nExamples\n10\n9\n8\n7\n6\n5\n4\n3\n2\n1\nUnits\nFigure 2: Subnetworks for 100 examples for 10\nReLUs. The examples activate many different\npossible subsets of the units, shown in dark. In\nthis case, unit number 3 is inactive for all exam-\nples.\na simple encoding of which units in a layer are activated for a given example (its subnetwork) can be\nused to represent the example for retrieval tasks. Experiments on MNIST, CIFAR-10, CIFAR-100\nand the ImageNet dataset show that promising results are obtained for datasets of varying size and\ncomplexity.\n2\nLOCALLY COMPETITIVE NEURAL NETWORKS\nNeural networks with activation functions like rectiﬁed linear, maxout and LWTA are locally com-\npetitive. This means that local competition among units in the network decides which parts of it get\nactivated or trained for a particular input example. For each unit, the total input or presynaptic acti-\nvation z is ﬁrst computed as z = wx + b, where x is the vector of inputs to the unit, w is a trainable\nweight vector, and b is a trainable bias. For the rectiﬁed linear function, the output or postsynaptic\nactivation of each unit is simply max(z, 0), which can be interpreted as competition with a ﬁxed\nvalue of 0. For LWTA, the units in a layer are considered to be divided into blocks of a ﬁxed size.\nThen the output of each unit is Iz where I is an indicator which is 1 if the unit has the maximum z\nin its group and 0 otherwise. In maxout, the inputs from a few units compete using a max operation,\nand the block output is the maximum z among the units1. A maxout block can also be interpreted\nas an LWTA block with shared outgoing weights among the units. A comparison of the 3 activation\nfunctions is shown in Figure 1.\nIn each of the three cases, there is a local gating mechanism which allows non-zero activations (and\nerrors during training) to propagate only through part of the network, i.e. a subnetwork. Consider the\nactivation of a neural network with rectiﬁed linear units (ReLUs) in a single hidden layer. For each\ninput pattern, the subset of units with non-zero activations in the hidden layer form a subnetwork,\nand an examination of the subnetworks activated for several examples shows that a large number of\ndifferent subnetworks are activated (Figure 2). The result of training the network can interpreted in\nthe following way: when training a single network with a local gating mechanism, a large number\nof linear subnetworks are trained on the dataset such that different examples are gated to different\nsubnetworks, each getting trained to produce the desired output. At test time, the system generalizes\nin the sense that the appropriate subnetwork for a given example is activated.\n3\nSUBNETWORK ANALYSIS\nThis section investigates how the model of models that is implemented though local competition\nself-organizes due to training. In order to visualize the organization of subnetworks as a result\nof training, they are encoded as bit strings called submasks. For the input pattern i, the submask\n1In our terminology, the terms unit and block correspond to the terms ﬁlter and units in Goodfellow et al.\n(2013a).\n2\nPublished as a conference paper at ICLR 2015\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n(a)\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n(b)\nFigure 3: 2-D visualization of submasks from the penultimate layer of a 3 hidden layer network\nwith ReLUs on the MNIST test set. (a) shows the submasks from an untrained network layer which\nlacks any discernable structure. (b) shows submasks from a trained network layer, showing clearly\ndemarcated clusters relevant to the supervised learning task. ‘Mistakes’ made by the network can\nalso be observed, such as mistaking ‘4’s for ‘9’s.\nsi ∈{0, 1}u, where u is the number of units in the full network, represents the corresponding\nsubnetwork by having a 0 in position j, j = 1..u, if the corresponding unit has zero activation,\nand 1 otherwise. The submasks uniquely and compactly encode each subnetwork in a format that\nis amenable to analysis through clustering, and, as we show in Section 4.2, facilitates efﬁcient data\nretrieval.\nIn what follows, the subnetworks that emerge during training are ﬁrst visualized using the t-\nSNE (Van der Maaten & Hinton, 2008) algorithm. This dimensionality reduction technique en-\nables a good visualization of the relationship between submasks for several examples in a dataset\nby preserving the local structure. Later in this section, we examine the evolution of subnetworks\nduring training, and show that the submasks obtained from a trained network can directly be used\nfor classiﬁcation using a simple nearest neighbors approach. All experiments in this section are\nperformed on the MNIST (LeCun et al., 1998) dataset. This familiar dataset was chosen because\nit is relatively easy, and therefore provides a tractable setting in which to verify the repeatability\nof our results. Larger, more interesting datasets are used in section 4 to demonstrate the utility of\ntechniques developed in this section for classiﬁcation and retrieval.\n3.1\nVISUALIZATION THROUGH DIMENSIONALITY REDUCTION\nFor visualizing the relationship between submasks for a large number of input patterns, we trained\nmultiple networks with different activation functions on the MNIST training set, stopping when the\nerror on a validation set did not improve. The submasks for the entire test set (10K examples) were\nthen extracted and visualized using t-SNE. Since the competition between subnetworks is local and\nnot global, subsets of units in deeper (closer to the output) layers are activated based on information\nextracted in the shallow layers. Therefore, like unit activations, submasks from deeper layers are\nexpected to be better related to the task since deeper layers code for higher level abstractions. For\nthis reason, we use only submasks extracted from the penultimate network layers in this paper, which\nconsiderably reduces the size of submasks to consider.\nFigure 3b shows a 2D visualization of the submasks from a 3 hidden layer ReL network. Each\nsubmask is a bitstring of length 1000 (the size of the network’s penultimate layer). Ten distinct\nclusters are present corresponding to the ten MNIST classes. It is remarkable that, irrespective of\nthe actual activation values, the subnetworks which are active for the testing examples can be used\nto visually predict class memberships based on their similarity to each other. The visualization\nconﬁrms that the subnetworks active for examples of the same class are much more similar to each\nother compared to the ones activated for the examples of different classes.\nVisualization of submasks from the same layer of a randomly initialized network does not show any\nstructure (Figure 3a), but we observed some structure for the untrained ﬁrst hidden layer (Appendix\nA.1). For trained networks, similar clustering is observed in the submasks from shallow layers in\n3\nPublished as a conference paper at ICLR 2015\n0\n10\n20\n30\n40\n50\nEpochs\n0.00\n0.05\n0.10\n0.15\n0.20\nAverage ﬂips per unit\nFigure 4: The plot shows mean of the fraction of ex-\namples (total 10K) for which units in the layer ﬂip\n(turn from being active to inactive or vice-versa) af-\nter every pass through the training set. The units ﬂip\nfor upto 20% of the examples on average in the ﬁrst\nfew epochs, but quickly settle down to less than 5%.\nNetwork\nNo. test errors\nSoftmax\nkNN\nReL (no dropout)\n161\n158\nLWTA (dropout)\n142\n154\nMaxout (dropout)\n116\n131\nTable 1: Some examples of classiﬁcation re-\nsults on the permutation invariant MNIST\ntest set using softmax layer outputs vs. kNN\non the submasks.\nAll submasks are ex-\ntracted from the penultimate layer. kNN re-\nsults are close to the softmax results in each\ncase. The maxout network was additionally\ntrained on the validation set. Results vary\nslightly across experimental runs and were\nnot cherry-picked for reporting.\nthe network, though the clusters appear to be less separated and tight. The visualization also shows\nmany instances where the network makes mistakes. The submasks for some examples lie in the\ncluster of submasks for the wrong class, indicating that the ‘wrong’ subnetwork was selected for\nthese examples. The experiments in the next sections show that the organization of subnetworks is\nindicative of the classiﬁcation performance of the full network.\nOther locally competitive activation functions such as LWTA and maxout result in similar clustering\nof submasks (visualizations included in Appendix A.1). For LWTA layers, the submasks can be\ndirectly constructed from the activations because there is no subsampling when going from presy-\nnaptic to postsynaptic activations, and it is reasonable to expect a subnetwork organization similar to\nthat of ReL layers. Indeed, in a limited qualitative analysis, it has been shown previously (Srivastava\net al., 2013) that in trained LWTA nets there are more units in common between subnetworks for\nexamples of the same class than those for different class examples.\nFor maxout layers, the situation is trickier at a ﬁrst glance. The unit activations get pooled before\nbeing propagated to the next layer, so it is possible that the maximum activation value plays a much\nmore important role than the identity of the winning units. However, using the same basic principle\nof credit assignment to subnetworks, we can construct submasks from maxout layers by binarizing\nthe unit activations such that only the units producing the maximum activation are represented by\na 1. Separation of subnetworks is necessary to gain the advantages of local competition during\nlearning, and the visualization of the generated submasks produces results similar to those for ReLU\nand LWTA (included in Appendix A.1).\n3.2\nBEHAVIOR DURING TRAINING\nIn order to measure how the subnetworks evolve over the course of training, the submasks of each\nsample in the training set were recorded at each epoch. Figure 4 characterizes the change in the\nsubnets over time by counting the number of input patterns for which a unit ﬂips from being on to\nbeing off, or vice-versa, from one epoch to the next. The curve in the ﬁgure shows the fraction of\npatterns for which an inter-epoch ﬂip occurred, averaged across all units in the network. Higher\nvalues indicate that the assignment of subnets to patterns is not stable. The batch size for this\nexperiment was 100, which means that each pass over the training set consists of 500 weight updates.\nFor the run shown, the average fraction of ﬂips starts at 0.2, but falls quickly below 0.05 and keeps\nfalling as training proceeds, indicating that, the assignment of subnetworks to individual examples\nstabilizes quickly. In this case, after a brief (∼3 epochs) transient period, a ﬁne-tuning period follows\nwhere the assigned subnetworks keep getting trained on their corresponding examples with little re-\nassignment.\n3.3\nEVALUATING SUBMASKS\nSince the visualization of submasks for the test set shows task-relevant structure, it is natural to\nask: how well can the submask represent the data that produced it? If the submasks for similar\nexamples are similar, perhaps they can be used as data descriptors for tasks such as similarity-based\n4\nPublished as a conference paper at ICLR 2015\nretrieval. Sparse binary codes enable efﬁcient storage and retrieval for large and complex datasets\ndue to which learning to produce them is an active research area (Gong et al., 2013; Masci et al.,\n2014b;a; Grauman & Fergus, 2013). This would make representative submasks very attractive since\nno explicit training for retrieval would be required to generate them.\nTo evaluate if examples producing similar binary codes are indeed similar, we train locally competi-\ntive networks for classiﬁcation and use a simple k nearest neighbors (kNN) algorithm for classifying\ndata using the generated submasks. This approach is a simple way to examine the amount of infor-\nmation contained in the submasks (without utilizing the actual activation values).\nWe trained networks with fully connected layers on the MNIST training set, and selected the value\nof k with the lowest validation error to perform classiﬁcation on the test set. Results are shown in\nTable 1. In each case, the kNN classiﬁcation results are close to the classiﬁcation result obtained\nusing the network’s softmax layer. If we use the (non-pooled) unit activations from the maxout\nnetwork instead of submasks for kNN classiﬁcation, we obtain 121 errors.\nSubmasks can also be obtained from convolutional layers. Using a convolutional maxout network,\nwe obtained 52 errors on the MNIST test set when we reproduced the model from Goodfellow et al.\n(2013a). Since the penultimate layer in this model is convolutional, the submasks were constructed\nusing the presynaptic unit activations from this layer for all convolutional maps. Visualization of\nthese submasks showed similar structure to that obtained from fully connected layers, kNN classi-\nﬁcation on the submasks resulted in 65 errors. As seen before, for a well-trained network the kNN\nperformance is close to the performance of the network’s softmax layer.\n3.4\nEFFECT OF DROPOUT\nThe dropout (Hinton et al., 2012) regularization technique has proven to be very useful and efﬁcient\nat improving generalization for large models, and is often used in combination with locally com-\npetitive activation functions (Krizhevsky et al., 2012; Goodfellow et al., 2013a; Zeiler et al., 2013).\nWe found that networks which were trained with dropout (and thus produced lower test set error)\nalso yielded better submasks in terms of kNN classiﬁcation performance. To observe the effect of\ndropout in more detail, we trained a 3 hidden layer network with 800 ReLUs in each hidden layer\nwithout dropout on MNIST starting from 5 different initializations until the validation set error did\nnot improve. The networks were then trained again from the same initialization with dropout until\nthe validation error matched or fell below the lowest validation error from the non-dropout case. In\nboth cases, minibatch gradient descent with momentum was used for training the networks. A com-\nparison of kNN classiﬁcation error for the dropout and non-dropout cases showed that when dropout\ntraining is stopped at a point when validation error is similar to a no-dropout network, the submasks\nfrom both cases give similar results, but as dropout training continues (lowers validation set error),\nthe submasks yield improved results. This supports the interpretation of dropout as a regularization\ntechnique which prevents “co-adaptation of feature detectors” (units) (Hinton et al., 2012), leading\nto better representation of data by the subnetworks. Another way to look at this effect can be that\ndropout improves generalization by injecting noise in the organization of subnetworks, making them\nmore robust.\n4\nEXPERIMENTAL RESULTS\nThe following experiments apply the methods described in the previous section to more challenging\nbenchmark problems: CIFAR-10, CIFAR-100, and ImageNet. For the CIFAR experiments, we used\nthe models described in Goodfellow et al. (2013a) since they use locally competitive activations\n(maxout), are trained with dropout, and good hyperparameter settings for them are available (Good-\nfellow et al., 2013b). We report the classiﬁcation error on the test set obtained using the softmax\noutput layer, as well kNN classiﬁcation on the penultimate layer unit activations and submasks. The\nbest value of k is obtained using a validation set, though we found that k = 5 with distance weighting\nusually worked well.\n4.1\nCIFAR-10 & CIFAR-100\nCIFAR-10 and CIFAR-100 are datasets of 32×32 color images of 10 classes. The results obtained\non the test sets for these datasets are summarized in Table 2. We ﬁnd that when comparing nearest\nneighbor classiﬁcation performance with submasks to unit activation values, we lose an accuracy of\n1.25% on the CIFAR-10 dataset, and 2.26% on the CIFAR-100 dataset on average. Figure 5a shows\nthe 2-D visualization of the test set submasks for CIFAR-10. Some classes can be seen to have\n5\nPublished as a conference paper at ICLR 2015\n(a)\n(b)\nFigure 5: 2-D visualizations of the submasks from the penultimate layer of the trained maxout\nnetworks reported in Goodfellow et al. (2013a). (a) The CIFAR10 test set. The 10-cluster structure\nis visible, although the clusters are not as well separated as in the case of MNIST. This corresponds\nwith the higher error rates obtained using both kNN and the full network. (b) The CIFAR100 test\nset. It is difﬁcult to visualize any dataset with 100 classes, but several clusters are still visible. The\nseparation between clusters is much worse, which is reﬂected in the high classiﬁcation error.\nDataset\nNetwork error\nkNN\n(activations)\nkNN\n(pre-activations)\nkNN\n(submasks)\nCIFAR-10\n9.94 ± 0.31%\n9.63 ± 0.21%\n10.11 ± 0.16%\n11.36 ± 0.22%\nCIFAR-100\n34.49 ± 0.22%\n37.54 ± 0.14%\n41.37 ± 0.26%\n43.63 ± 0.18%\nTable 2: Classiﬁcation errors on CIFAR datasets comparing maxout network performance, kNN on\nactivation values, kNN on pre-activations (before maximum pooling) and kNN on binary submasks.\nResults are reported over 5 runs.\nhighly representative submasks, while confusion between classes in the lower half is observed. The\nclusters of subnetworks are not as well-separated as in the case of MNIST, reﬂecting the relatively\nworse classiﬁcation performance of the full network. Submask visualization for CIFAR-100 (Figure\n5b) reﬂects the high error rate for this dataset. Although any visualization with 100 classes can be\nhard to interpret, many small clusters of submasks can still be observed.\n4.2\nIMAGENET\nThe results of kNN classiﬁcation and t-SNE visualization using submasks on small datasets of vary-\ning complexities show that the submasks contain substantial information relevant for image clas-\nsiﬁcation. In this section, the utility of the submasks obtained for a large convolutional network\ntrained on the ImageNet Large Scale Visual Recognition Challenge 2012 (ILSVRC-2012) (Deng\net al., 2012) dataset is evaluated.\nOur results show that submasks retain a large amount of information on this difﬁcult large scale\ntask, while greatly improving storage efﬁciency. For instance, 4096-dimensional submasks for the\nfull ILSVRC-2012 training set can be stored in about 0.5 GB. Our experiments also indicate that\nsubmasks obtained from a better trained network result in better performance (Table 3). Krizhevsky\net al. (2012) suggested that the activations from a trained convolutional network can be compressed\nto binary codes using auto-encoders. We show here that the submasks can be directly utilized for\nefﬁcient retrieval of data based on high level similarity even though no pair-wise loss was used\nduring training.\nWe compare to DiffHash, a supervised similarity-preserving hashing approach proposed by Strecha\net al. (2012), trained on the non-binarized features from the network. Supervision is represented\nin terms of similar and dissimilar pairs of points, for which a ground-truth similarity measure is\nknown, i.e. sharing the same class or not. While it is beyond the scope of this paper to provide an\n2https://github.com/torontodeeplearning/convnet/\n6\nPublished as a conference paper at ICLR 2015\nRecall\n0\n0.2\n0.4\n0.6\n0.8\n1\nPrecision\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nDiffHash\nSubmasks\nFigure 6: Comparison of precision-recall curves\non ILSVRC-2012 when using binary codes ob-\ntained using different techniques.\nThe perfor-\nmance of submasks is competitive and decays\nonly for high recall values where supervised hash-\ning obtains a better ranking of the results due to\nthe pair-wise supervision.\nNetwork\nNetwork error\nkNN on submasks\nDeCAF\n19.2%\n29.2%\nConvnet2\n13.5%\n20.38%\nTable 3: Top-5 Classiﬁcation accuracy on validation\nset when performance of two different networks on\nImageNet is compared to performance of submasks\nobtained from each of them. Note that as network\naccuracy improves by about 6%, submask accuracy\nimproves by about 10%.\nTechnique\nmAP@5\nmAP@10\nmAP@100\nSubmasks\n58.3\n56.7\n46.7\nDiffhash\n61.0\n59.3\n49.5\nTable 4: Comparison of mean average precisions\nat various thresholds using binary codes obtained\nusing different techniques on the ILSVRC-2012\ndataset.\nSubmasks are obtained directly from net-\nworks trained for classiﬁcation without any further\ntraining. Up to mAP@100 the submasks show a con-\nstant performance degradation of about 3 points.\nexhaustive comparison or to introduce a new approach to supervised hashing, we nevertheless show\nvery competitive performance w.r.t. a dedicated algorithm devised for this task. Precision-recall\ncurves are shown in Figure 6 while Table 4 reports results for mean average precision; mAP =\nPR\nr=1 P(r) · rel(r), where rel(r) indicates the relevance of a result at a given rank r, P(r) the\nprecision at r, and R the number of retrieved results. DiffHash learns a linear projection, which\nis one of the reason we decided to use it to limit impact of supervision. Thus we attribute the\nsmall performance gap to the input features already being very discriminative which left little room\nfor improvement. For the purpose of this comparison, we did not investigate more sophisticated\ntechniques which would have steered the focus to conventional hashing approaches. Sample retrieval\nresults for examples from the ILSVRC-2012 dataset are shown in Figure 7.\n5\nDISCUSSION\nTraining a system of many networks on a dataset such that they specialize to solve simpler tasks\ncan be quite difﬁcult without combining them into a single network with locally competitive units.\nWithout such local competition, one needs to have a global gating mechanism as in Jacobs et al.\n(1991). The training algorithm and the objective function also need modiﬁcations such that compe-\ntition between networks is encouraged, and the system becomes hard to train. On the other hand,\na locally competitive neural network can behave like a model composed of many subnetworks, and\nmassive sharing of parameters between subnetworks enables better training. Stochastic gradient de-\nscent can be used to minimize the desired loss function, and the implementation is so simple that\none does not even realize that a model of models is being trained.\nFigure 4 suggests that during optimization, the subnetworks get organized during an early transient\nphase such that subnetworks responding to similar examples have more parameters in common than\nthose responding to dissimilar examples. This allows for better training of subnetworks due to\nreduced interference from dissimilar examples and shared parameters for similar examples. In the\nlater ﬁne-tuning phase, the parameters of subnetworks get adjusted to improve classiﬁcation and\nmuch less re-assignment of subnetworks is needed. In this way, the gating mechanism induced by\nlocally competitive activation functions accomplishes the purpose of global competition efﬁciently\nand no modiﬁcations to the error function are required.\nWe believe that due to above advantages locally competitive networks have allowed easier and faster\ntraining on complex pattern recognition tasks compared to networks with sigmoidal or similar ac-\ntivation functions. These ﬁndings provide indirect evidence that low interference between subnet-\nworks is a beneﬁcial property for training large networks. The nature of organization of subnetworks\nis reminiscent of the data manifold hypothesis for classiﬁcation (Rifai et al., 2011). Just like data\n7\nPublished as a conference paper at ICLR 2015\nFigure 7: Retrieval based on subnetworks on the ILSVRC-2012 dataset. The ﬁrst image in each row\nis the query image; the remaining 5 are the responses retrieved using submasks.\npoints of different classes are expected to concentrate along sub-manifolds, we expect that the orga-\nnization of subnetworks that respond to the data points reﬂects the data manifold being modeled.\nAn important take-away from these results is the unifying theme between locally competitive archi-\ntectures, which is related to past work on competitive learning. Insights from past literature on this\ntopic may be utilized to develop improved learning algorithms and neural architectures. This paper,\nto the best of our knowledge, is the ﬁrst to show that useful binary data descriptors can be obtained\ndirectly from a neural network trained for classiﬁcation without any additional training. These de-\nscriptors are not just results of a thresholding trick or unique to a particular activation function, but\narise as a direct result of the credit assignment process. Our experiments on datasets of increasing\ncomplexity show that when the network performance improves, the performance gap to submask-\nbased classiﬁcation closes. This suggests that in the near future, as training techniques continue to\nadvance and yield lower errors on larger datasets, submasks will perform as well as activation values\nfor retrieval and transfer learning tasks. Importantly, these binary representations will always be far\nmore efﬁcient for storage and retrieval than continuous activation vectors.\nREFERENCES\nBatra, D., Agrawal, H., Banik, P., Chavali, N., and Alfadda, A. CloudCV: Large-Scale distributed computer\nvision as a cloud service. http://www.cloudcv.org, September 2013. URL http://www.cloudcv.org.\nDeng, Jia, Berg, Alex, Satheesh, Sanjeev, Hao, Su, Khosla, Aditya, and Li, Fei-Fei. ImageNet large scale visual\nrecognition competition 2012 (ILSVRC2012). http://www.image-net.org/challenges/LSVRC/2012/, 2012.\nURL http://www.image-net.org/challenges/LSVRC/2012/.\nDonahue, Jeff, Jia, Yangqing, Vinyals, Oriol, Hoffman, Judy, Zhang, Ning, Tzeng, Eric, and Darrell, Trevor.\nDeCAF: a deep convolutional activation feature for generic visual recognition. arXiv:1310.1531 [cs], Octo-\nber 2013. URL http://arxiv.org/abs/1310.1531.\nGlorot, Xavier, Bordes, Antoine, and Bengio, Yoshua. Deep sparse rectiﬁer networks. In Proceedings of\nthe 14th International Conference on Artiﬁcial Intelligence and Statistics. JMLR W&CP, volume 15, pp.\n315–323, 2011. URL http://eprints.pascal-network.org/archive/00008596/.\n8\nPublished as a conference paper at ICLR 2015\nGong, Yunchao, Kumar, Sanjiv, Rowley, Henry A., and Lazebnik, Svetlana. Learning binary codes for high-\ndimensional data using bilinear projections. In Computer Vision and Pattern Recognition (CVPR), 2013\nIEEE Conference on, pp. 484–491. IEEE, 2013. URL http://ieeexplore.ieee.org/xpls/abs_\nall.jsp?arnumber=6618913.\nGoodfellow, Ian, Warde-Farley, David, Mirza, Mehdi, Courville, Aaron, and Bengio, Yoshua. Maxout net-\nworks. In Proceedings of The 30th International Conference on Machine Learning, pp. 1319–1327, 2013a.\nURL http://jmlr.org/proceedings/papers/v28/goodfellow13.html.\nGoodfellow, Ian J., Warde-Farley, David, Lamblin, Pascal, Dumoulin, Vincent, Mirza, Mehdi, Pascanu, Raz-\nvan, Bergstra, James, Bastien, Fr´ed´eric, and Bengio, Yoshua. Pylearn2: a machine learning research library.\narXiv:1308.4214 [cs, stat], August 2013b. URL http://arxiv.org/abs/1308.4214.\nGoodfellow, Ian J., Mirza, Mehdi, Da, Xiao, Courville, Aaron, and Bengio, Yoshua. An empirical investiga-\ntion of catastrophic forgetting in gradient-based neural networks. In International Conference on Learning\nRepresentations, 2014. URL http://arxiv.org/abs/1312.6211.\nGrauman, Kristen and Fergus, Rob. Learning binary hash codes for large-scale image search. In Machine\nLearning for Computer Vision, pp. 49–87. Springer, 2013.\nURL http://link.springer.com/\nchapter/10.1007/978-3-642-28661-2_3.\nHinton, Geoffrey E., Srivastava, Nitish, Krizhevsky, Alex, Sutskever, Ilya, and Salakhutdinov, Ruslan R. Im-\nproving neural networks by preventing co-adaptation of feature detectors. arXiv:1207.0580 [cs], July 2012.\nURL http://arxiv.org/abs/1207.0580.\nJacobs, Robert A., Jordan, Michael I., Nowlan, Steven J., and Hinton, Geoffrey E. Adaptive mixtures of local\nexperts. Neural computation, 3(1):79–87, 1991. URL http://www.mitpressjournals.org/doi/\nabs/10.1162/neco.1991.3.1.79.\nKrizhevsky, Alex and Hinton, Geoffrey. Learning multiple layers of features from tiny images. Computer\nScience Department, University of Toronto, Tech. Rep, 2009. URL http://citeseerx.ist.psu.\nedu/viewdoc/download?doi=10.1.1.222.9220&rep=rep1&type=pdf.\nKrizhevsky, Alex, Sutskever, Ilya, and Hinton, Geoffrey E. Imagenet classiﬁcation with deep convolutional\nneural networks. In Advances in Neural Information Processing Systems, 2012. URL http://books.\nnips.cc/papers/files/nips25/NIPS2012_0534.pdf.\nLeCun, Yann, Bottou, L´eon, Bengio, Yoshua, and Haffner, Patrick. Gradient-based learning applied to doc-\nument recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998. URL http://ieeexplore.\nieee.org/xpls/abs_all.jsp?arnumber=726791.\nMasci, Jonathan, Bronstein, Alex M., Bronstein, Michael M., and Schmidhuber, J¨urgen. Multimodal Similarity-\nPreserving Hashing. IEEE Transactions on Pattern Analysis and Machine Intelligence, 36(4), 2014a.\nMasci, Jonathan, Bronstein, Alex M., Bronstein, Michael M., Sprechmann, Pablo, and Sapiro, Guillermo.\nSparse similarity-preserving hashing. In International Conference on Learning Representations, 2014b.\nURL http://arxiv.org/abs/1312.5479. arXiv: 1312.5479.\nPascanu, Razvan, Montufar, Guido, and Bengio, Yoshua. On the number of response regions of deep feed\nforward networks with piece-wise linear activations. arXiv:1312.6098 [cs], December 2013. URL http:\n//arxiv.org/abs/1312.6098. arXiv: 1312.6098.\nRifai, Salah, Dauphin, Yann, Vincent, Pascal, Bengio, Yoshua, and Muller, Xavier. The manifold tangent\nclassiﬁer. In Advances in Neural Information Processing Systems, pp. 2294–2302, 2011. URL https:\n//papers.nips.cc/paper/4409-the-manifold-tangent-classifier.pdf.\nSalakhutdinov, Ruslan and Hinton, Geoffrey.\nSemantic hashing.\nInternational Journal of Approximate\nReasoning, 50(7):969–978, July 2009. doi: 10.1016/j.ijar.2008.11.006. URL http://linkinghub.\nelsevier.com/retrieve/pii/S0888613X08001813.\nSrivastava, Rupesh K., Masci, Jonathan, Kazerounian, Sohrob, Gomez, Faustino, and Schmidhuber, J¨urgen.\nCompete to compute. In Advances in Neural Information Processing Systems, pp. 2310–2318, 2013. URL\nhttp://papers.nips.cc/paper/5059-compete-to-compute.\nStrecha, Christop, Bronstein, Alex M., Bronstein, Michael M., and Fua, Pascal. LDAHash: Improved Matching\nwith Smaller Descriptors. IEEE Transactions on Pattern Analysis and Machine Intelligence, 34(1), 2012.\nVan der Maaten, Laurens and Hinton, Geoffrey. Visualizing data using t-SNE. Journal of Machine Learning\nResearch, 9(11), 2008.\nZeiler, Matthew D., Ranzato, M., Monga, R., Mao, M., Yang, K., Le, Q. V., Nguyen, P., Senior, A.,\nVanhoucke, V., and Dean, J.\nOn rectiﬁed linear units for speech processing.\nIn IEEE International\nConference on Acoustics, Speech and Signal Processing (ICASSP), pp. 3517–3521. IEEE, 2013.\nURL\nhttp://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6638312.\n9\nPublished as a conference paper at ICLR 2015\nA\nSUPPLEMENTARY MATERIALS\nA.1\nEXTRA VISUALIZATIONS\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n(a) Trained LWTA layer.\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n(b) Trained Maxout layer.\nFigure 8: 2-D visualization of submasks from the penultimate layer of 3 hidden layer LWTA and\nmaxout networks on MNIST test set. Organization of submasks into distinct class speciﬁc clusters\nsimilar to ReL networks is observed.\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n(a) Untrained 1st LWTA layer.\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n(b) Untrained 1st ReL layer.\nFigure 9: 2-D visualization of submasks obtained before training from the 1st (closest to the input)\nhidden layer of 3 hidden layer LWTA and ReL networks on MNIST test set.\nB\nDATASET DESCRIPTIONS\nB.1\nCIFAR-10 AND CIFAR-100\nCIFAR-10 is a dataset of 32×32 color images of 10 classes split into a training set of size 50,000 and testing\nset of size 10,000 (6000 images per class) (Krizhevsky & Hinton, 2009). CIFAR-100 is a similar dataset of\ncolor images but with 100 classes and 600 images per class, making it more challenging. The models from\nGoodfellow et al. (2013a) for these dataset utilize preprocessing using global contrast normalization and ZCA\nwhitening as well as data augmentation using translational and horizontal reﬂections.\nB.2\nIMAGENET (ILSVRC-2012)\nILSVRC-2012 is a dataset of over a million natural images split into 1000 classes. An implementation of\nthe network in Krizhevsky et al. (2012), with some minor differences (Donahue et al., 2013), is available\npublicly. For the experiments in this section, the penultimate-layer activations obtained using this model were\ndownloaded from CloudCV Batra et al. (2013). The activations were obtained using the center-only option,\nmeaning that only the activations for the central, 224×224 crop of each image were used.\n10\nPublished as a conference paper at ICLR 2015\nFor each validation set example, 100 examples from the training set with the closest submasks were weighted\nby the inverse of the distance, then the classes with top-1 or top-5 weighted sums were returned as predictions.\nC\nNOTE ON SIGMOIDAL NETWORKS\nIn this paper we focused on improving our understanding of neural networks with locally competitive activation\nfunctions. We also obtained binary codes for efﬁcient retrieval directly from neural networks trained for clas-\nsiﬁcation, but this was not the primary aim of our study. When this is the aim, we note here that it possible to\nuse sigmoidal activation functions to obtain binary codes by thresholding the activation values after supervised\nor unsupervised (Salakhutdinov & Hinton, 2009) training. However it should be noted that:\n• The thresholding is somewhat arbitrary and the best threshold needs to be selected by trying various\nvalues. For locally competitive networks, the binarization is natural and inherent to the nature of\ncredit assignment in these networks.\n• Since sigmoidal networks are hard and slow to train, the approach of thresholding their activations\nis impractical for large datasets which are common for retrieval tasks. Locally competitive networks\nhave been crucial for the successful application of neural networks to such datasets.\n11\n",
        "sentence": " Learning to route information through neural networks has helped to scale up their application to challenging problems by improving credit assignment and making training easier (Srivastava et al., 2015)."
    },
    {
        "title": "Going deeper with convolutions",
        "author": [
            "Szegedy",
            "Christian",
            "Liu",
            "Wei",
            "Jia",
            "Yangqing",
            "Sermanet",
            "Pierre",
            "Reed",
            "Scott",
            "Anguelov",
            "Dragomir",
            "Erhan",
            "Dumitru",
            "Vanhoucke",
            "Vincent",
            "Rabinovich",
            "Andrew"
        ],
        "venue": "[cs],",
        "citeRegEx": "Szegedy et al\\.,? \\Q2014\\E",
        "shortCiteRegEx": "Szegedy et al\\.",
        "year": 2014,
        "abstract": "We propose a deep convolutional neural network architecture codenamed\n\"Inception\", which was responsible for setting the new state of the art for\nclassification and detection in the ImageNet Large-Scale Visual Recognition\nChallenge 2014 (ILSVRC 2014). The main hallmark of this architecture is the\nimproved utilization of the computing resources inside the network. This was\nachieved by a carefully crafted design that allows for increasing the depth and\nwidth of the network while keeping the computational budget constant. To\noptimize quality, the architectural decisions were based on the Hebbian\nprinciple and the intuition of multi-scale processing. One particular\nincarnation used in our submission for ILSVRC 2014 is called GoogLeNet, a 22\nlayers deep network, the quality of which is assessed in the context of\nclassification and detection.",
        "full_text": "Going deeper with convolutions\nChristian Szegedy\nGoogle Inc.\nWei Liu\nUniversity of North Carolina, Chapel Hill\nYangqing Jia\nGoogle Inc.\nPierre Sermanet\nGoogle Inc.\nScott Reed\nUniversity of Michigan\nDragomir Anguelov\nGoogle Inc.\nDumitru Erhan\nGoogle Inc.\nVincent Vanhoucke\nGoogle Inc.\nAndrew Rabinovich\nGoogle Inc.\nAbstract\nWe propose a deep convolutional neural network architecture codenamed Incep-\ntion, which was responsible for setting the new state of the art for classiﬁcation\nand detection in the ImageNet Large-Scale Visual Recognition Challenge 2014\n(ILSVRC14). The main hallmark of this architecture is the improved utilization\nof the computing resources inside the network. This was achieved by a carefully\ncrafted design that allows for increasing the depth and width of the network while\nkeeping the computational budget constant. To optimize quality, the architectural\ndecisions were based on the Hebbian principle and the intuition of multi-scale\nprocessing. One particular incarnation used in our submission for ILSVRC14 is\ncalled GoogLeNet, a 22 layers deep network, the quality of which is assessed in\nthe context of classiﬁcation and detection.\n1\nIntroduction\nIn the last three years, mainly due to the advances of deep learning, more concretely convolutional\nnetworks [10], the quality of image recognition and object detection has been progressing at a dra-\nmatic pace. One encouraging news is that most of this progress is not just the result of more powerful\nhardware, larger datasets and bigger models, but mainly a consequence of new ideas, algorithms and\nimproved network architectures. No new data sources were used, for example, by the top entries in\nthe ILSVRC 2014 competition besides the classiﬁcation dataset of the same competition for detec-\ntion purposes. Our GoogLeNet submission to ILSVRC 2014 actually uses 12× fewer parameters\nthan the winning architecture of Krizhevsky et al [9] from two years ago, while being signiﬁcantly\nmore accurate. The biggest gains in object-detection have not come from the utilization of deep\nnetworks alone or bigger models, but from the synergy of deep architectures and classical computer\nvision, like the R-CNN algorithm by Girshick et al [6].\nAnother notable factor is that with the ongoing traction of mobile and embedded computing, the\nefﬁciency of our algorithms – especially their power and memory use – gains importance. It is\nnoteworthy that the considerations leading to the design of the deep architecture presented in this\npaper included this factor rather than having a sheer ﬁxation on accuracy numbers. For most of the\nexperiments, the models were designed to keep a computational budget of 1.5 billion multiply-adds\nat inference time, so that the they do not end up to be a purely academic curiosity, but could be put\nto real world use, even on large datasets, at a reasonable cost.\n1\narXiv:1409.4842v1  [cs.CV]  17 Sep 2014\nIn this paper, we will focus on an efﬁcient deep neural network architecture for computer vision,\ncodenamed Inception, which derives its name from the Network in network paper by Lin et al [12]\nin conjunction with the famous “we need to go deeper” internet meme [1]. In our case, the word\n“deep” is used in two different meanings: ﬁrst of all, in the sense that we introduce a new level of\norganization in the form of the “Inception module” and also in the more direct sense of increased\nnetwork depth. In general, one can view the Inception model as a logical culmination of [12]\nwhile taking inspiration and guidance from the theoretical work by Arora et al [2]. The beneﬁts\nof the architecture are experimentally veriﬁed on the ILSVRC 2014 classiﬁcation and detection\nchallenges, on which it signiﬁcantly outperforms the current state of the art.\n2\nRelated Work\nStarting with LeNet-5 [10], convolutional neural networks (CNN) have typically had a standard\nstructure – stacked convolutional layers (optionally followed by contrast normalization and max-\npooling) are followed by one or more fully-connected layers. Variants of this basic design are\nprevalent in the image classiﬁcation literature and have yielded the best results to-date on MNIST,\nCIFAR and most notably on the ImageNet classiﬁcation challenge [9, 21]. For larger datasets such\nas Imagenet, the recent trend has been to increase the number of layers [12] and layer size [21, 14],\nwhile using dropout [7] to address the problem of overﬁtting.\nDespite concerns that max-pooling layers result in loss of accurate spatial information, the same\nconvolutional network architecture as [9] has also been successfully employed for localization [9,\n14], object detection [6, 14, 18, 5] and human pose estimation [19]. Inspired by a neuroscience\nmodel of the primate visual cortex, Serre et al. [15] use a series of ﬁxed Gabor ﬁlters of different sizes\nin order to handle multiple scales, similarly to the Inception model. However, contrary to the ﬁxed\n2-layer deep model of [15], all ﬁlters in the Inception model are learned. Furthermore, Inception\nlayers are repeated many times, leading to a 22-layer deep model in the case of the GoogLeNet\nmodel.\nNetwork-in-Network is an approach proposed by Lin et al. [12] in order to increase the representa-\ntional power of neural networks. When applied to convolutional layers, the method could be viewed\nas additional 1×1 convolutional layers followed typically by the rectiﬁed linear activation [9]. This\nenables it to be easily integrated in the current CNN pipelines. We use this approach heavily in our\narchitecture. However, in our setting, 1 × 1 convolutions have dual purpose: most critically, they\nare used mainly as dimension reduction modules to remove computational bottlenecks, that would\notherwise limit the size of our networks. This allows for not just increasing the depth, but also the\nwidth of our networks without signiﬁcant performance penalty.\nThe current leading approach for object detection is the Regions with Convolutional Neural Net-\nworks (R-CNN) proposed by Girshick et al. [6]. R-CNN decomposes the overall detection problem\ninto two subproblems: to ﬁrst utilize low-level cues such as color and superpixel consistency for\npotential object proposals in a category-agnostic fashion, and to then use CNN classiﬁers to identify\nobject categories at those locations. Such a two stage approach leverages the accuracy of bound-\ning box segmentation with low-level cues, as well as the highly powerful classiﬁcation power of\nstate-of-the-art CNNs. We adopted a similar pipeline in our detection submissions, but have ex-\nplored enhancements in both stages, such as multi-box [5] prediction for higher object bounding\nbox recall, and ensemble approaches for better categorization of bounding box proposals.\n3\nMotivation and High Level Considerations\nThe most straightforward way of improving the performance of deep neural networks is by increas-\ning their size. This includes both increasing the depth – the number of levels – of the network and its\nwidth: the number of units at each level. This is as an easy and safe way of training higher quality\nmodels, especially given the availability of a large amount of labeled training data. However this\nsimple solution comes with two major drawbacks.\nBigger size typically means a larger number of parameters, which makes the enlarged network more\nprone to overﬁtting, especially if the number of labeled examples in the training set is limited.\nThis can become a major bottleneck, since the creation of high quality training sets can be tricky\n2\n(a) Siberian husky\n(b) Eskimo dog\nFigure 1: Two distinct classes from the 1000 classes of the ILSVRC 2014 classiﬁcation challenge.\nand expensive, especially if expert human raters are necessary to distinguish between ﬁne-grained\nvisual categories like those in ImageNet (even in the 1000-class ILSVRC subset) as demonstrated\nby Figure 1.\nAnother drawback of uniformly increased network size is the dramatically increased use of compu-\ntational resources. For example, in a deep vision network, if two convolutional layers are chained,\nany uniform increase in the number of their ﬁlters results in a quadratic increase of computation. If\nthe added capacity is used inefﬁciently (for example, if most weights end up to be close to zero),\nthen a lot of computation is wasted. Since in practice the computational budget is always ﬁnite, an\nefﬁcient distribution of computing resources is preferred to an indiscriminate increase of size, even\nwhen the main objective is to increase the quality of results.\nThe fundamental way of solving both issues would be by ultimately moving from fully connected\nto sparsely connected architectures, even inside the convolutions. Besides mimicking biological\nsystems, this would also have the advantage of ﬁrmer theoretical underpinnings due to the ground-\nbreaking work of Arora et al. [2]. Their main result states that if the probability distribution of\nthe data-set is representable by a large, very sparse deep neural network, then the optimal network\ntopology can be constructed layer by layer by analyzing the correlation statistics of the activations\nof the last layer and clustering neurons with highly correlated outputs. Although the strict math-\nematical proof requires very strong conditions, the fact that this statement resonates with the well\nknown Hebbian principle – neurons that ﬁre together, wire together – suggests that the underlying\nidea is applicable even under less strict conditions, in practice.\nOn the downside, todays computing infrastructures are very inefﬁcient when it comes to numerical\ncalculation on non-uniform sparse data structures. Even if the number of arithmetic operations is\nreduced by 100×, the overhead of lookups and cache misses is so dominant that switching to sparse\nmatrices would not pay off. The gap is widened even further by the use of steadily improving,\nhighly tuned, numerical libraries that allow for extremely fast dense matrix multiplication, exploit-\ning the minute details of the underlying CPU or GPU hardware [16, 9]. Also, non-uniform sparse\nmodels require more sophisticated engineering and computing infrastructure. Most current vision\noriented machine learning systems utilize sparsity in the spatial domain just by the virtue of em-\nploying convolutions. However, convolutions are implemented as collections of dense connections\nto the patches in the earlier layer. ConvNets have traditionally used random and sparse connection\ntables in the feature dimensions since [11] in order to break the symmetry and improve learning, the\ntrend changed back to full connections with [9] in order to better optimize parallel computing. The\nuniformity of the structure and a large number of ﬁlters and greater batch size allow for utilizing\nefﬁcient dense computation.\nThis raises the question whether there is any hope for a next, intermediate step: an architecture\nthat makes use of the extra sparsity, even at ﬁlter level, as suggested by the theory, but exploits our\n3\ncurrent hardware by utilizing computations on dense matrices. The vast literature on sparse matrix\ncomputations (e.g. [3]) suggests that clustering sparse matrices into relatively dense submatrices\ntends to give state of the art practical performance for sparse matrix multiplication. It does not\nseem far-fetched to think that similar methods would be utilized for the automated construction of\nnon-uniform deep-learning architectures in the near future.\nThe Inception architecture started out as a case study of the ﬁrst author for assessing the hypothetical\noutput of a sophisticated network topology construction algorithm that tries to approximate a sparse\nstructure implied by [2] for vision networks and covering the hypothesized outcome by dense, read-\nily available components. Despite being a highly speculative undertaking, only after two iterations\non the exact choice of topology, we could already see modest gains against the reference architec-\nture based on [12]. After further tuning of learning rate, hyperparameters and improved training\nmethodology, we established that the resulting Inception architecture was especially useful in the\ncontext of localization and object detection as the base network for [6] and [5]. Interestingly, while\nmost of the original architectural choices have been questioned and tested thoroughly, they turned\nout to be at least locally optimal.\nOne must be cautious though: although the proposed architecture has become a success for computer\nvision, it is still questionable whether its quality can be attributed to the guiding principles that have\nlead to its construction. Making sure would require much more thorough analysis and veriﬁcation:\nfor example, if automated tools based on the principles described below would ﬁnd similar, but\nbetter topology for the vision networks. The most convincing proof would be if an automated\nsystem would create network topologies resulting in similar gains in other domains using the same\nalgorithm but with very differently looking global architecture. At very least, the initial success of\nthe Inception architecture yields ﬁrm motivation for exciting future work in this direction.\n4\nArchitectural Details\nThe main idea of the Inception architecture is based on ﬁnding out how an optimal local sparse\nstructure in a convolutional vision network can be approximated and covered by readily available\ndense components. Note that assuming translation invariance means that our network will be built\nfrom convolutional building blocks. All we need is to ﬁnd the optimal local construction and to\nrepeat it spatially. Arora et al. [2] suggests a layer-by layer construction in which one should analyze\nthe correlation statistics of the last layer and cluster them into groups of units with high correlation.\nThese clusters form the units of the next layer and are connected to the units in the previous layer. We\nassume that each unit from the earlier layer corresponds to some region of the input image and these\nunits are grouped into ﬁlter banks. In the lower layers (the ones close to the input) correlated units\nwould concentrate in local regions. This means, we would end up with a lot of clusters concentrated\nin a single region and they can be covered by a layer of 1×1 convolutions in the next layer, as\nsuggested in [12]. However, one can also expect that there will be a smaller number of more\nspatially spread out clusters that can be covered by convolutions over larger patches, and there\nwill be a decreasing number of patches over larger and larger regions. In order to avoid patch-\nalignment issues, current incarnations of the Inception architecture are restricted to ﬁlter sizes 1×1,\n3×3 and 5×5, however this decision was based more on convenience rather than necessity. It also\nmeans that the suggested architecture is a combination of all those layers with their output ﬁlter\nbanks concatenated into a single output vector forming the input of the next stage. Additionally,\nsince pooling operations have been essential for the success in current state of the art convolutional\nnetworks, it suggests that adding an alternative parallel pooling path in each such stage should have\nadditional beneﬁcial effect, too (see Figure 2(a)).\nAs these “Inception modules” are stacked on top of each other, their output correlation statistics\nare bound to vary: as features of higher abstraction are captured by higher layers, their spatial\nconcentration is expected to decrease suggesting that the ratio of 3×3 and 5×5 convolutions should\nincrease as we move to higher layers.\nOne big problem with the above modules, at least in this na¨ıve form, is that even a modest number of\n5×5 convolutions can be prohibitively expensive on top of a convolutional layer with a large number\nof ﬁlters. This problem becomes even more pronounced once pooling units are added to the mix:\ntheir number of output ﬁlters equals to the number of ﬁlters in the previous stage. The merging of\nthe output of the pooling layer with the outputs of convolutional layers would lead to an inevitable\n4\n1x1 convolutions\n3x3 convolutions\n5x5 convolutions\nFilter \nconcatenation\nPrevious layer\n3x3 max pooling\n(a) Inception module, na¨ıve version\n1x1 convolutions\n3x3 convolutions\n5x5 convolutions\nFilter \nconcatenation\nPrevious layer\n3x3 max pooling\n1x1 convolutions\n1x1 convolutions\n1x1 convolutions\n(b) Inception module with dimension reductions\nFigure 2: Inception module\nincrease in the number of outputs from stage to stage. Even while this architecture might cover the\noptimal sparse structure, it would do it very inefﬁciently, leading to a computational blow up within\na few stages.\nThis leads to the second idea of the proposed architecture: judiciously applying dimension reduc-\ntions and projections wherever the computational requirements would increase too much otherwise.\nThis is based on the success of embeddings: even low dimensional embeddings might contain a lot\nof information about a relatively large image patch. However, embeddings represent information in\na dense, compressed form and compressed information is harder to model. We would like to keep\nour representation sparse at most places (as required by the conditions of [2]) and compress the\nsignals only whenever they have to be aggregated en masse. That is, 1×1 convolutions are used to\ncompute reductions before the expensive 3×3 and 5×5 convolutions. Besides being used as reduc-\ntions, they also include the use of rectiﬁed linear activation which makes them dual-purpose. The\nﬁnal result is depicted in Figure 2(b).\nIn general, an Inception network is a network consisting of modules of the above type stacked upon\neach other, with occasional max-pooling layers with stride 2 to halve the resolution of the grid. For\ntechnical reasons (memory efﬁciency during training), it seemed beneﬁcial to start using Inception\nmodules only at higher layers while keeping the lower layers in traditional convolutional fashion.\nThis is not strictly necessary, simply reﬂecting some infrastructural inefﬁciencies in our current\nimplementation.\nOne of the main beneﬁcial aspects of this architecture is that it allows for increasing the number of\nunits at each stage signiﬁcantly without an uncontrolled blow-up in computational complexity. The\nubiquitous use of dimension reduction allows for shielding the large number of input ﬁlters of the\nlast stage to the next layer, ﬁrst reducing their dimension before convolving over them with a large\npatch size. Another practically useful aspect of this design is that it aligns with the intuition that\nvisual information should be processed at various scales and then aggregated so that the next stage\ncan abstract features from different scales simultaneously.\nThe improved use of computational resources allows for increasing both the width of each stage\nas well as the number of stages without getting into computational difﬁculties. Another way to\nutilize the inception architecture is to create slightly inferior, but computationally cheaper versions\nof it. We have found that all the included the knobs and levers allow for a controlled balancing of\ncomputational resources that can result in networks that are 2 −3× faster than similarly performing\nnetworks with non-Inception architecture, however this requires careful manual design at this point.\n5\nGoogLeNet\nWe chose GoogLeNet as our team-name in the ILSVRC14 competition. This name is an homage to\nYann LeCuns pioneering LeNet 5 network [10]. We also use GoogLeNet to refer to the particular\nincarnation of the Inception architecture used in our submission for the competition. We have also\nused a deeper and wider Inception network, the quality of which was slightly inferior, but adding it\nto the ensemble seemed to improve the results marginally. We omit the details of that network, since\nour experiments have shown that the inﬂuence of the exact architectural parameters is relatively\n5\ntype\npatch size/\nstride\noutput\nsize\ndepth\n#1×1\n#3×3\nreduce\n#3×3\n#5×5\nreduce\n#5×5\npool\nproj\nparams\nops\nconvolution\n7×7/2\n112×112×64\n1\n2.7K\n34M\nmax pool\n3×3/2\n56×56×64\n0\nconvolution\n3×3/1\n56×56×192\n2\n64\n192\n112K\n360M\nmax pool\n3×3/2\n28×28×192\n0\ninception (3a)\n28×28×256\n2\n64\n96\n128\n16\n32\n32\n159K\n128M\ninception (3b)\n28×28×480\n2\n128\n128\n192\n32\n96\n64\n380K\n304M\nmax pool\n3×3/2\n14×14×480\n0\ninception (4a)\n14×14×512\n2\n192\n96\n208\n16\n48\n64\n364K\n73M\ninception (4b)\n14×14×512\n2\n160\n112\n224\n24\n64\n64\n437K\n88M\ninception (4c)\n14×14×512\n2\n128\n128\n256\n24\n64\n64\n463K\n100M\ninception (4d)\n14×14×528\n2\n112\n144\n288\n32\n64\n64\n580K\n119M\ninception (4e)\n14×14×832\n2\n256\n160\n320\n32\n128\n128\n840K\n170M\nmax pool\n3×3/2\n7×7×832\n0\ninception (5a)\n7×7×832\n2\n256\n160\n320\n32\n128\n128\n1072K\n54M\ninception (5b)\n7×7×1024\n2\n384\n192\n384\n48\n128\n128\n1388K\n71M\navg pool\n7×7/1\n1×1×1024\n0\ndropout (40%)\n1×1×1024\n0\nlinear\n1×1×1000\n1\n1000K\n1M\nsoftmax\n1×1×1000\n0\nTable 1: GoogLeNet incarnation of the Inception architecture\nminor. Here, the most successful particular instance (named GoogLeNet) is described in Table 1 for\ndemonstrational purposes. The exact same topology (trained with different sampling methods) was\nused for 6 out of the 7 models in our ensemble.\nAll the convolutions, including those inside the Inception modules, use rectiﬁed linear activation.\nThe size of the receptive ﬁeld in our network is 224×224 taking RGB color channels with mean sub-\ntraction. “#3×3 reduce” and “#5×5 reduce” stands for the number of 1×1 ﬁlters in the reduction\nlayer used before the 3×3 and 5×5 convolutions. One can see the number of 1×1 ﬁlters in the pro-\njection layer after the built-in max-pooling in the pool proj column. All these reduction/projection\nlayers use rectiﬁed linear activation as well.\nThe network was designed with computational efﬁciency and practicality in mind, so that inference\ncan be run on individual devices including even those with limited computational resources, espe-\ncially with low-memory footprint. The network is 22 layers deep when counting only layers with\nparameters (or 27 layers if we also count pooling). The overall number of layers (independent build-\ning blocks) used for the construction of the network is about 100. However this number depends on\nthe machine learning infrastructure system used. The use of average pooling before the classiﬁer is\nbased on [12], although our implementation differs in that we use an extra linear layer. This enables\nadapting and ﬁne-tuning our networks for other label sets easily, but it is mostly convenience and\nwe do not expect it to have a major effect. It was found that a move from fully connected layers to\naverage pooling improved the top-1 accuracy by about 0.6%, however the use of dropout remained\nessential even after removing the fully connected layers.\nGiven the relatively large depth of the network, the ability to propagate gradients back through all the\nlayers in an effective manner was a concern. One interesting insight is that the strong performance\nof relatively shallower networks on this task suggests that the features produced by the layers in the\nmiddle of the network should be very discriminative. By adding auxiliary classiﬁers connected to\nthese intermediate layers, we would expect to encourage discrimination in the lower stages in the\nclassiﬁer, increase the gradient signal that gets propagated back, and provide additional regulariza-\ntion. These classiﬁers take the form of smaller convolutional networks put on top of the output of\nthe Inception (4a) and (4d) modules. During training, their loss gets added to the total loss of the\nnetwork with a discount weight (the losses of the auxiliary classiﬁers were weighted by 0.3). At\ninference time, these auxiliary networks are discarded.\nThe exact structure of the extra network on the side, including the auxiliary classiﬁer, is as follows:\n• An average pooling layer with 5×5 ﬁlter size and stride 3, resulting in an 4×4×512 output\nfor the (4a), and 4×4×528 for the (4d) stage.\n6\ninput\nConv\n7x7+2(S)\nMaxPool\n3x3+2(S)\nLocalRespNorm\nConv\n1x1+1(V)\nConv\n3x3+1(S)\nLocalRespNorm\nMaxPool\n3x3+2(S)\nConv\n1x1+1(S)\nConv\n1x1+1(S)\nConv\n1x1+1(S)\nMaxPool\n3x3+1(S)\nDepthConcat\nConv\n3x3+1(S)\nConv\n5x5+1(S)\nConv\n1x1+1(S)\nConv\n1x1+1(S)\nConv\n1x1+1(S)\nConv\n1x1+1(S)\nMaxPool\n3x3+1(S)\nDepthConcat\nConv\n3x3+1(S)\nConv\n5x5+1(S)\nConv\n1x1+1(S)\nMaxPool\n3x3+2(S)\nConv\n1x1+1(S)\nConv\n1x1+1(S)\nConv\n1x1+1(S)\nMaxPool\n3x3+1(S)\nDepthConcat\nConv\n3x3+1(S)\nConv\n5x5+1(S)\nConv\n1x1+1(S)\nConv\n1x1+1(S)\nConv\n1x1+1(S)\nConv\n1x1+1(S)\nMaxPool\n3x3+1(S)\nAveragePool\n5x5+3(V)\nDepthConcat\nConv\n3x3+1(S)\nConv\n5x5+1(S)\nConv\n1x1+1(S)\nConv\n1x1+1(S)\nConv\n1x1+1(S)\nConv\n1x1+1(S)\nMaxPool\n3x3+1(S)\nDepthConcat\nConv\n3x3+1(S)\nConv\n5x5+1(S)\nConv\n1x1+1(S)\nConv\n1x1+1(S)\nConv\n1x1+1(S)\nConv\n1x1+1(S)\nMaxPool\n3x3+1(S)\nDepthConcat\nConv\n3x3+1(S)\nConv\n5x5+1(S)\nConv\n1x1+1(S)\nConv\n1x1+1(S)\nConv\n1x1+1(S)\nConv\n1x1+1(S)\nMaxPool\n3x3+1(S)\nAveragePool\n5x5+3(V)\nDepthConcat\nConv\n3x3+1(S)\nConv\n5x5+1(S)\nConv\n1x1+1(S)\nMaxPool\n3x3+2(S)\nConv\n1x1+1(S)\nConv\n1x1+1(S)\nConv\n1x1+1(S)\nMaxPool\n3x3+1(S)\nDepthConcat\nConv\n3x3+1(S)\nConv\n5x5+1(S)\nConv\n1x1+1(S)\nConv\n1x1+1(S)\nConv\n1x1+1(S)\nConv\n1x1+1(S)\nMaxPool\n3x3+1(S)\nDepthConcat\nConv\n3x3+1(S)\nConv\n5x5+1(S)\nConv\n1x1+1(S)\nAveragePool\n7x7+1(V)\nFC\nConv\n1x1+1(S)\nFC\nFC\nSoftmaxActivation\nsoftmax0\nConv\n1x1+1(S)\nFC\nFC\nSoftmaxActivation\nsoftmax1\nSoftmaxActivation\nsoftmax2\nFigure 3: GoogLeNet network with all the bells and whistles\n7\n• A 1×1 convolution with 128 ﬁlters for dimension reduction and rectiﬁed linear activation.\n• A fully connected layer with 1024 units and rectiﬁed linear activation.\n• A dropout layer with 70% ratio of dropped outputs.\n• A linear layer with softmax loss as the classiﬁer (predicting the same 1000 classes as the\nmain classiﬁer, but removed at inference time).\nA schematic view of the resulting network is depicted in Figure 3.\n6\nTraining Methodology\nOur networks were trained using the DistBelief [4] distributed machine learning system using mod-\nest amount of model and data-parallelism. Although we used CPU based implementation only, a\nrough estimate suggests that the GoogLeNet network could be trained to convergence using few\nhigh-end GPUs within a week, the main limitation being the memory usage. Our training used\nasynchronous stochastic gradient descent with 0.9 momentum [17], ﬁxed learning rate schedule (de-\ncreasing the learning rate by 4% every 8 epochs). Polyak averaging [13] was used to create the ﬁnal\nmodel used at inference time.\nOur image sampling methods have changed substantially over the months leading to the competition,\nand already converged models were trained on with other options, sometimes in conjunction with\nchanged hyperparameters, like dropout and learning rate, so it is hard to give a deﬁnitive guidance\nto the most effective single way to train these networks. To complicate matters further, some of\nthe models were mainly trained on smaller relative crops, others on larger ones, inspired by [8].\nStill, one prescription that was veriﬁed to work very well after the competition includes sampling\nof various sized patches of the image whose size is distributed evenly between 8% and 100% of the\nimage area and whose aspect ratio is chosen randomly between 3/4 and 4/3. Also, we found that the\nphotometric distortions by Andrew Howard [8] were useful to combat overﬁtting to some extent. In\naddition, we started to use random interpolation methods (bilinear, area, nearest neighbor and cubic,\nwith equal probability) for resizing relatively late and in conjunction with other hyperparameter\nchanges, so we could not tell deﬁnitely whether the ﬁnal results were affected positively by their\nuse.\n7\nILSVRC 2014 Classiﬁcation Challenge Setup and Results\nThe ILSVRC 2014 classiﬁcation challenge involves the task of classifying the image into one of\n1000 leaf-node categories in the Imagenet hierarchy. There are about 1.2 million images for training,\n50,000 for validation and 100,000 images for testing. Each image is associated with one ground\ntruth category, and performance is measured based on the highest scoring classiﬁer predictions.\nTwo numbers are usually reported: the top-1 accuracy rate, which compares the ground truth against\nthe ﬁrst predicted class, and the top-5 error rate, which compares the ground truth against the ﬁrst\n5 predicted classes: an image is deemed correctly classiﬁed if the ground truth is among the top-5,\nregardless of its rank in them. The challenge uses the top-5 error rate for ranking purposes.\nWe participated in the challenge with no external data used for training. In addition to the training\ntechniques aforementioned in this paper, we adopted a set of techniques during testing to obtain a\nhigher performance, which we elaborate below.\n1. We independently trained 7 versions of the same GoogLeNet model (including one wider\nversion), and performed ensemble prediction with them. These models were trained with\nthe same initialization (even with the same initial weights, mainly because of an oversight)\nand learning rate policies, and they only differ in sampling methodologies and the random\norder in which they see input images.\n2. During testing, we adopted a more aggressive cropping approach than that of Krizhevsky et\nal. [9]. Speciﬁcally, we resize the image to 4 scales where the shorter dimension (height or\nwidth) is 256, 288, 320 and 352 respectively, take the left, center and right square of these\nresized images (in the case of portrait images, we take the top, center and bottom squares).\nFor each square, we then take the 4 corners and the center 224×224 crop as well as the\n8\nTeam\nYear\nPlace\nError (top-5)\nUses external data\nSuperVision\n2012\n1st\n16.4%\nno\nSuperVision\n2012\n1st\n15.3%\nImagenet 22k\nClarifai\n2013\n1st\n11.7%\nno\nClarifai\n2013\n1st\n11.2%\nImagenet 22k\nMSRA\n2014\n3rd\n7.35%\nno\nVGG\n2014\n2nd\n7.32%\nno\nGoogLeNet\n2014\n1st\n6.67%\nno\nTable 2: Classiﬁcation performance\nNumber of models\nNumber of Crops\nCost\nTop-5 error\ncompared to base\n1\n1\n1\n10.07%\nbase\n1\n10\n10\n9.15%\n-0.92%\n1\n144\n144\n7.89%\n-2.18%\n7\n1\n7\n8.09%\n-1.98%\n7\n10\n70\n7.62%\n-2.45%\n7\n144\n1008\n6.67%\n-3.45%\nTable 3: GoogLeNet classiﬁcation performance break down\nsquare resized to 224×224, and their mirrored versions. This results in 4×3×6×2 = 144\ncrops per image. A similar approach was used by Andrew Howard [8] in the previous year’s\nentry, which we empirically veriﬁed to perform slightly worse than the proposed scheme.\nWe note that such aggressive cropping may not be necessary in real applications, as the\nbeneﬁt of more crops becomes marginal after a reasonable number of crops are present (as\nwe will show later on).\n3. The softmax probabilities are averaged over multiple crops and over all the individual clas-\nsiﬁers to obtain the ﬁnal prediction. In our experiments we analyzed alternative approaches\non the validation data, such as max pooling over crops and averaging over classiﬁers, but\nthey lead to inferior performance than the simple averaging.\nIn the remainder of this paper, we analyze the multiple factors that contribute to the overall perfor-\nmance of the ﬁnal submission.\nOur ﬁnal submission in the challenge obtains a top-5 error of 6.67% on both the validation and\ntesting data, ranking the ﬁrst among other participants. This is a 56.5% relative reduction compared\nto the SuperVision approach in 2012, and about 40% relative reduction compared to the previous\nyear’s best approach (Clarifai), both of which used external data for training the classiﬁers. The\nfollowing table shows the statistics of some of the top-performing approaches.\nWe also analyze and report the performance of multiple testing choices, by varying the number of\nmodels and the number of crops used when predicting an image in the following table. When we\nuse one model, we chose the one with the lowest top-1 error rate on the validation data. All numbers\nare reported on the validation dataset in order to not overﬁt to the testing data statistics.\n8\nILSVRC 2014 Detection Challenge Setup and Results\nThe ILSVRC detection task is to produce bounding boxes around objects in images among 200\npossible classes. Detected objects count as correct if they match the class of the groundtruth and\ntheir bounding boxes overlap by at least 50% (using the Jaccard index). Extraneous detections count\nas false positives and are penalized. Contrary to the classiﬁcation task, each image may contain\n9\nTeam\nYear\nPlace\nmAP\nexternal data\nensemble\napproach\nUvA-Euvision\n2013\n1st\n22.6%\nnone\n?\nFisher vectors\nDeep Insight\n2014\n3rd\n40.5%\nImageNet 1k\n3\nCNN\nCUHK DeepID-Net\n2014\n2nd\n40.7%\nImageNet 1k\n?\nCNN\nGoogLeNet\n2014\n1st\n43.9%\nImageNet 1k\n6\nCNN\nTable 4: Detection performance\nTeam\nmAP\nContextual model\nBounding box regression\nTrimps-Soushen\n31.6%\nno\n?\nBerkeley Vision\n34.5%\nno\nyes\nUvA-Euvision\n35.4%\n?\n?\nCUHK DeepID-Net2\n37.7%\nno\n?\nGoogLeNet\n38.02%\nno\nno\nDeep Insight\n40.2%\nyes\nyes\nTable 5: Single model performance for detection\nmany objects or none, and their scale may vary from large to tiny. Results are reported using the\nmean average precision (mAP).\nThe approach taken by GoogLeNet for detection is similar to the R-CNN by [6], but is augmented\nwith the Inception model as the region classiﬁer. Additionally, the region proposal step is improved\nby combining the Selective Search [20] approach with multi-box [5] predictions for higher object\nbounding box recall. In order to cut down the number of false positives, the superpixel size was\nincreased by 2×. This halves the proposals coming from the selective search algorithm. We added\nback 200 region proposals coming from multi-box [5] resulting, in total, in about 60% of the pro-\nposals used by [6], while increasing the coverage from 92% to 93%. The overall effect of cutting the\nnumber of proposals with increased coverage is a 1% improvement of the mean average precision\nfor the single model case. Finally, we use an ensemble of 6 ConvNets when classifying each region\nwhich improves results from 40% to 43.9% accuracy. Note that contrary to R-CNN, we did not use\nbounding box regression due to lack of time.\nWe ﬁrst report the top detection results and show the progress since the ﬁrst edition of the detection\ntask. Compared to the 2013 result, the accuracy has almost doubled. The top performing teams all\nuse Convolutional Networks. We report the ofﬁcial scores in Table 4 and common strategies for each\nteam: the use of external data, ensemble models or contextual models. The external data is typically\nthe ILSVRC12 classiﬁcation data for pre-training a model that is later reﬁned on the detection data.\nSome teams also mention the use of the localization data. Since a good portion of the localization\ntask bounding boxes are not included in the detection dataset, one can pre-train a general bounding\nbox regressor with this data the same way classiﬁcation is used for pre-training. The GoogLeNet\nentry did not use the localization data for pretraining.\nIn Table 5, we compare results using a single model only. The top performing model is by Deep\nInsight and surprisingly only improves by 0.3 points with an ensemble of 3 models while the\nGoogLeNet obtains signiﬁcantly stronger results with the ensemble.\n9\nConclusions\nOur results seem to yield a solid evidence that approximating the expected optimal sparse structure\nby readily available dense building blocks is a viable method for improving neural networks for\ncomputer vision. The main advantage of this method is a signiﬁcant quality gain at a modest in-\ncrease of computational requirements compared to shallower and less wide networks. Also note that\nour detection work was competitive despite of neither utilizing context nor performing bounding box\n10\nregression and this fact provides further evidence of the strength of the Inception architecture. Al-\nthough it is expected that similar quality of result can be achieved by much more expensive networks\nof similar depth and width, our approach yields solid evidence that moving to sparser architectures\nis feasible and useful idea in general. This suggest promising future work towards creating sparser\nand more reﬁned structures in automated ways on the basis of [2].\n10\nAcknowledgements\nWe would like to thank Sanjeev Arora and Aditya Bhaskara for fruitful discussions on [2]. Also\nwe are indebted to the DistBelief [4] team for their support especially to Rajat Monga, Jon Shlens,\nAlex Krizhevsky, Jeff Dean, Ilya Sutskever and Andrea Frome. We would also like to thank to Tom\nDuerig and Ning Ye for their help on photometric distortions. Also our work would not have been\npossible without the support of Chuck Rosenberg and Hartwig Adam.\nReferences\n[1] Know your meme: We need to go deeper.\nhttp://knowyourmeme.com/memes/\nwe-need-to-go-deeper. Accessed: 2014-09-15.\n[2] Sanjeev Arora, Aditya Bhaskara, Rong Ge, and Tengyu Ma. Provable bounds for learning\nsome deep representations. CoRR, abs/1310.6343, 2013.\n[3] ¨Umit V. C¸ ataly¨urek, Cevdet Aykanat, and Bora Uc¸ar. On two-dimensional sparse matrix par-\ntitioning: Models, methods, and a recipe. SIAM J. Sci. Comput., 32(2):656–683, February\n2010.\n[4] Jeffrey Dean, Greg Corrado, Rajat Monga, Kai Chen, Matthieu Devin, Mark Mao,\nMarc’aurelio Ranzato, Andrew Senior, Paul Tucker, Ke Yang, Quoc V. Le, and Andrew Y.\nNg. Large scale distributed deep networks. In P. Bartlett, F.c.n. Pereira, C.j.c. Burges, L. Bot-\ntou, and K.q. Weinberger, editors, Advances in Neural Information Processing Systems 25,\npages 1232–1240. 2012.\n[5] Dumitru Erhan, Christian Szegedy, Alexander Toshev, and Dragomir Anguelov. Scalable ob-\nject detection using deep neural networks. In Computer Vision and Pattern Recognition, 2014.\nCVPR 2014. IEEE Conference on, 2014.\n[6] Ross B. Girshick, Jeff Donahue, Trevor Darrell, and Jitendra Malik. Rich feature hierarchies\nfor accurate object detection and semantic segmentation.\nIn Computer Vision and Pattern\nRecognition, 2014. CVPR 2014. IEEE Conference on, 2014.\n[7] Geoffrey E. Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhut-\ndinov. Improving neural networks by preventing co-adaptation of feature detectors. CoRR,\nabs/1207.0580, 2012.\n[8] Andrew G. Howard. Some improvements on deep convolutional neural network based image\nclassiﬁcation. CoRR, abs/1312.5402, 2013.\n[9] Alex Krizhevsky, Ilya Sutskever, and Geoff Hinton. Imagenet classiﬁcation with deep con-\nvolutional neural networks. In Advances in Neural Information Processing Systems 25, pages\n1106–1114, 2012.\n[10] Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard, W. Hubbard, and L. D. Jackel.\nBackpropagation applied to handwritten zip code recognition. Neural Comput., 1(4):541–551,\nDecember 1989.\n[11] Yann LeCun, L´eon Bottou, Yoshua Bengio, and Patrick Haffner.\nGradient-based learning\napplied to document recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998.\n[12] Min Lin, Qiang Chen, and Shuicheng Yan. Network in network. CoRR, abs/1312.4400, 2013.\n[13] B. T. Polyak and A. B. Juditsky. Acceleration of stochastic approximation by averaging. SIAM\nJ. Control Optim., 30(4):838–855, July 1992.\n[14] Pierre Sermanet, David Eigen, Xiang Zhang, Micha¨el Mathieu, Rob Fergus, and Yann Le-\nCun. Overfeat: Integrated recognition, localization and detection using convolutional net-\nworks. CoRR, abs/1312.6229, 2013.\n11\n[15] Thomas Serre, Lior Wolf, Stanley M. Bileschi, Maximilian Riesenhuber, and Tomaso Poggio.\nRobust object recognition with cortex-like mechanisms. IEEE Trans. Pattern Anal. Mach.\nIntell., 29(3):411–426, 2007.\n[16] Fengguang Song and Jack Dongarra.\nScaling up matrix computations on shared-memory\nmanycore systems with 1000 cpu cores. In Proceedings of the 28th ACM International Con-\nference on Supercomputing, ICS ’14, pages 333–342, New York, NY, USA, 2014. ACM.\n[17] Ilya Sutskever, James Martens, George E. Dahl, and Geoffrey E. Hinton. On the importance\nof initialization and momentum in deep learning. In Proceedings of the 30th International\nConference on Machine Learning, ICML 2013, Atlanta, GA, USA, 16-21 June 2013, volume 28\nof JMLR Proceedings, pages 1139–1147. JMLR.org, 2013.\n[18] Christian Szegedy, Alexander Toshev, and Dumitru Erhan. Deep neural networks for object\ndetection.\nIn Christopher J. C. Burges, L´eon Bottou, Zoubin Ghahramani, and Kilian Q.\nWeinberger, editors, Advances in Neural Information Processing Systems 26: 27th Annual\nConference on Neural Information Processing Systems 2013. Proceedings of a meeting held\nDecember 5-8, 2013, Lake Tahoe, Nevada, United States., pages 2553–2561, 2013.\n[19] Alexander Toshev and Christian Szegedy. Deeppose: Human pose estimation via deep neural\nnetworks. CoRR, abs/1312.4659, 2013.\n[20] Koen E. A. van de Sande, Jasper R. R. Uijlings, Theo Gevers, and Arnold W. M. Smeulders.\nSegmentation as selective search for object recognition. In Proceedings of the 2011 Interna-\ntional Conference on Computer Vision, ICCV ’11, pages 1879–1886, Washington, DC, USA,\n2011. IEEE Computer Society.\n[21] Matthew D. Zeiler and Rob Fergus. Visualizing and understanding convolutional networks. In\nDavid J. Fleet, Tom´as Pajdla, Bernt Schiele, and Tinne Tuytelaars, editors, Computer Vision\n- ECCV 2014 - 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Pro-\nceedings, Part I, volume 8689 of Lecture Notes in Computer Science, pages 818–833. Springer,\n2014.\n12\n",
        "sentence": " , 2012) to ∼95% (Szegedy et al., 2014; Simonyan & Zisserman, 2014) through the use of ensembles of deeper architectures and smaller receptive fields (Ciresan et al. , 2014) or with temporary companion loss functions attached to some of the layers (Szegedy et al., 2014; Lee et al., 2015)."
    }
]