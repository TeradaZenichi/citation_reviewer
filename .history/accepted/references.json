[
    {
        "title": "ImageNet Large Scale Visual Recognition Challenge",
        "author": [
            "Olga Russakovsky",
            "Jia Deng",
            "Hao Su"
        ],
        "venue": "International Journal of Computer Vision (IJCV),",
        "citeRegEx": "1",
        "shortCiteRegEx": "1",
        "year": 2015,
        "abstract": "The ImageNet Large Scale Visual Recognition Challenge is a benchmark in\nobject category classification and detection on hundreds of object categories\nand millions of images. The challenge has been run annually from 2010 to\npresent, attracting participation from more than fifty institutions.\n  This paper describes the creation of this benchmark dataset and the advances\nin object recognition that have been possible as a result. We discuss the\nchallenges of collecting large-scale ground truth annotation, highlight key\nbreakthroughs in categorical object recognition, provide a detailed analysis of\nthe current state of the field of large-scale image classification and object\ndetection, and compare the state-of-the-art computer vision accuracy with human\naccuracy. We conclude with lessons learned in the five years of the challenge,\nand propose future directions and improvements.",
        "full_text": "Noname manuscript No.\n(will be inserted by the editor)\nImageNet Large Scale Visual Recognition Challenge\nOlga Russakovsky* · Jia Deng* · Hao Su · Jonathan Krause ·\nSanjeev Satheesh · Sean Ma · Zhiheng Huang · Andrej Karpathy ·\nAditya Khosla · Michael Bernstein · Alexander C. Berg · Li Fei-Fei\nReceived: date / Accepted: date\nAbstract The ImageNet Large Scale Visual Recogni-\ntion Challenge is a benchmark in object category classi-\nﬁcation and detection on hundreds of object categories\nand millions of images. The challenge has been run an-\nnually from 2010 to present, attracting participation\nfrom more than ﬁfty institutions.\nThis paper describes the creation of this benchmark\ndataset and the advances in object recognition that\nhave been possible as a result. We discuss the chal-\nO. Russakovsky*\nStanford University, Stanford, CA, USA\nE-mail: olga@cs.stanford.edu\nJ. Deng*\nUniversity of Michigan, Ann Arbor, MI, USA\n(* = authors contributed equally)\nH. Su\nStanford University, Stanford, CA, USA\nJ. Krause\nStanford University, Stanford, CA, USA\nS. Satheesh\nStanford University, Stanford, CA, USA\nS. Ma\nStanford University, Stanford, CA, USA\nZ. Huang\nStanford University, Stanford, CA, USA\nA. Karpathy\nStanford University, Stanford, CA, USA\nA. Khosla\nMassachusetts Institute of Technology, Cambridge, MA, USA\nM. Bernstein\nStanford University, Stanford, CA, USA\nA. C. Berg\nUNC Chapel Hill, Chapel Hill, NC, USA\nL. Fei-Fei\nStanford University, Stanford, CA, USA\nlenges of collecting large-scale ground truth annotation,\nhighlight key breakthroughs in categorical object recog-\nnition, provide a detailed analysis of the current state\nof the ﬁeld of large-scale image classiﬁcation and ob-\nject detection, and compare the state-of-the-art com-\nputer vision accuracy with human accuracy. We con-\nclude with lessons learned in the ﬁve years of the chal-\nlenge, and propose future directions and improvements.\nKeywords Dataset · Large-scale · Benchmark ·\nObject recognition · Object detection\n1 Introduction\nOverview. The ImageNet Large Scale Visual Recogni-\ntion Challenge (ILSVRC) has been running annually\nfor ﬁve years (since 2010) and has become the standard\nbenchmark for large-scale object recognition.1 ILSVRC\nfollows in the footsteps of the PASCAL VOC chal-\nlenge (Everingham et al., 2012), established in 2005,\nwhich set the precedent for standardized evaluation of\nrecognition algorithms in the form of yearly competi-\ntions. As in PASCAL VOC, ILSVRC consists of two\ncomponents: (1) a publically available dataset, and (2)\nan annual competition and corresponding workshop. The\ndataset allows for the development and comparison of\ncategorical object recognition algorithms, and the com-\npetition and workshop provide a way to track the progress\nand discuss the lessons learned from the most successful\nand innovative entries each year.\n1 In this paper, we will be using the term object recogni-\ntion broadly to encompass both image classiﬁcation (a task\nrequiring an algorithm to determine what object classes are\npresent in the image) as well as object detection (a task requir-\ning an algorithm to localize all objects present in the image).\narXiv:1409.0575v3  [cs.CV]  30 Jan 2015\n2\nOlga Russakovsky* et al.\nThe publically released dataset contains a set of\nmanually annotated training images. A set of test im-\nages is also released, with the manual annotations with-\nheld.2 Participants train their algorithms using the train-\ning images and then automatically annotate the test\nimages. These predicted annotations are submitted to\nthe evaluation server. Results of the evaluation are re-\nvealed at the end of the competition period and authors\nare invited to share insights at the workshop held at the\nInternational Conference on Computer Vision (ICCV)\nor European Conference on Computer Vision (ECCV)\nin alternate years.\nILSVRC annotations fall into one of two categories:\n(1) image-level annotation of a binary label for the pres-\nence or absence of an object class in the image, e.g.,\n“there are cars in this image” but “there are no tigers,”\nand (2) object-level annotation of a tight bounding box\nand class label around an object instance in the image,\ne.g., “there is a screwdriver centered at position (20,25)\nwith width of 50 pixels and height of 30 pixels”.\nLarge-scale challenges and innovations. In creating the\ndataset, several challenges had to be addressed. Scal-\ning up from 19,737 images in PASCAL VOC 2010 to\n1,461,406 in ILSVRC 2010 and from 20 object classes to\n1000 object classes brings with it several challenges. It\nis no longer feasible for a small group of annotators to\nannotate the data as is done for other datasets (Fei-Fei\net al., 2004; Criminisi, 2004; Everingham et al., 2012;\nXiao et al., 2010). Instead we turn to designing novel\ncrowdsourcing approaches for collecting large-scale an-\nnotations (Su et al., 2012; Deng et al., 2009, 2014).\nSome of the 1000 object classes may not be as easy\nto annotate as the 20 categories of PASCAL VOC: e.g.,\nbananas which appear in bunches may not be as easy\nto delineate as the basic-level categories of aeroplanes\nor cars. Having more than a million images makes it in-\nfeasible to annotate the locations of all objects (much\nless with object segmentations, human body parts, and\nother detailed annotations that subsets of PASCAL VOC\ncontain). New evaluation criteria have to be deﬁned to\ntake into account the facts that obtaining perfect man-\nual annotations in this setting may be infeasible.\nOnce the challenge dataset was collected, its scale\nallowed for unprecedented opportunities both in evalu-\nation of object recognition algorithms and in developing\nnew techniques. Novel algorithmic innovations emerge\nwith the availability of large-scale training data. The\nbroad spectrum of object categories motivated the need\nfor algorithms that are even able to distinguish classes\nwhich are visually very similar. We highlight the most\n2\nIn 2010, the test annotations were later released publicly;\nsince then the test annotation have been kept hidden.\nsuccessful of these algorithms in this paper, and com-\npare their performance with human-level accuracy.\nFinally, the large variety of object classes in ILSVRC\nallows us to perform an analysis of statistical properties\nof objects and their impact on recognition algorithms.\nThis type of analysis allows for a deeper understand-\ning of object recognition, and for designing the next\ngeneration of general object recognition algorithms.\nGoals. This paper has three key goals:\n1. To discuss the challenges of creating this large-scale\nobject recognition benchmark dataset,\n2. To highlight the developments in object classiﬁca-\ntion and detection that have resulted from this ef-\nfort, and\n3. To take a closer look at the current state of the ﬁeld\nof categorical object recognition.\nThe paper may be of interest to researchers working\non creating large-scale datasets, as well as to anybody\ninterested in better understanding the history and the\ncurrent state of large-scale object recognition.\nThe collected dataset and additional information\nabout ILSVRC can be found at:\nhttp://image-net.org/challenges/LSVRC/\n1.1 Related work\nWe brieﬂy discuss some prior work in constructing bench-\nmark image datasets.\nImage classiﬁcation datasets. Caltech 101 (Fei-Fei et al.,\n2004) was among the ﬁrst standardized datasets for\nmulti-category image classiﬁcation, with 101 object classes\nand commonly 15-30 training images per class. Caltech\n256 (Griﬃn et al., 2007) increased the number of object\nclasses to 256 and added images with greater scale and\nbackground variability. The TinyImages dataset (Tor-\nralba et al., 2008) contains 80 million 32x32 low resolu-\ntion images collected from the internet using synsets in\nWordNet (Miller, 1995) as queries. However, since this\ndata has not been manually veriﬁed, there are many\nerrors, making it less suitable for algorithm evaluation.\nDatasets such as 15 Scenes (Oliva and Torralba, 2001;\nFei-Fei and Perona, 2005; Lazebnik et al., 2006) or re-\ncent Places (Zhou et al., 2014) provide a single scene\ncategory label (as opposed to an object category).\nThe ImageNet dataset (Deng et al., 2009) is the\nbackbone of ILSVRC. ImageNet is an image dataset\norganized according to the WordNet hierarchy (Miller,\n1995). Each concept in WordNet, possibly described by\nmultiple words or word phrases, is called a “synonym\nImageNet Large Scale Visual Recognition Challenge\n3\nset” or “synset”. ImageNet populates 21,841 synsets of\nWordNet with an average of 650 manually veriﬁed and\nfull resolution images. As a result, ImageNet contains\n14,197,122 annotated images organized by the semantic\nhierarchy of WordNet (as of August 2014). ImageNet is\nlarger in scale and diversity than the other image clas-\nsiﬁcation datasets. ILSVRC uses a subset of ImageNet\nimages for training the algorithms and some of Ima-\ngeNet’s image collection protocols for annotating addi-\ntional images for testing the algorithms.\nImage parsing datasets. Many datasets aim to provide\nricher image annotations beyond image-category labels.\nLabelMe (Russell et al., 2007) contains general pho-\ntographs with multiple objects per image. It has bound-\ning polygon annotations around objects, but the ob-\nject names are not standardized: annotators are free\nto choose which objects to label and what to name\neach object. The SUN2012 (Xiao et al., 2010) dataset\ncontains 16,873 manually cleaned up and fully anno-\ntated images more suitable for standard object detec-\ntion training and evaluation. SIFT Flow (Liu et al.,\n2011) contains 2,688 images labeled using the LabelMe\nsystem. The LotusHill dataset (Yao et al., 2007) con-\ntains very detailed annotations of objects in 636,748\nimages and video frames, but it is not available for free.\nSeveral datasets provide pixel-level segmentations: for\nexample, MSRC dataset (Criminisi, 2004) with 591 im-\nages and 23 object classes, Stanford Background Dataset\n(Gould et al., 2009) with 715 images and 8 classes,\nand the Berkeley Segmentation dataset (Arbelaez et al.,\n2011) with 500 images annotated with object bound-\naries. OpenSurfaces segments surfaces from consumer\nphotographs and annotates them with surface proper-\nties, including material, texture, and contextual infor-\nmation (Bell et al., 2013) .\nThe closest to ILSVRC is the PASCAL VOC dataset\n(Everingham et al., 2010, 2014), which provides a stan-\ndardized test bed for object detection, image classiﬁ-\ncation, object segmentation, person layout, and action\nclassiﬁcation. Much of the design choices in ILSVRC\nhave been inspired by PASCAL VOC and the simi-\nlarities and diﬀerences between the datasets are dis-\ncussed at length throughout the paper. ILSVRC scales\nup PASCAL VOC’s goal of standardized training and\nevaluation of recognition algorithms by more than an\norder of magnitude in number of object classes and im-\nages: PASCAL VOC 2012 has 20 object classes and\n21,738 images compared to ILSVRC2012 with 1000 ob-\nject classes and 1,431,167 annotated images.\nThe recently released COCO dataset (Lin et al.,\n2014b) contains more than 328,000 images with 2.5 mil-\nlion object instances manually segmented. It has fewer\nobject categories than ILSVRC (91 in COCO versus\n200 in ILSVRC object detection) but more instances\nper category (27K on average compared to about 1K\nin ILSVRC object detection). Further, it contains ob-\nject segmentation annotations which are not currently\navailable in ILSVRC. COCO is likely to become another\nimportant large-scale benchmark.\nLarge-scale annotation. ILSVRC makes extensive use\nof Amazon Mechanical Turk to obtain accurate annota-\ntions (Sorokin and Forsyth, 2008). Works such as (Welin-\nder et al., 2010; Sheng et al., 2008; Vittayakorn and\nHays, 2011) describe quality control mechanisms for\nthis marketplace. (Vondrick et al., 2012) provides a de-\ntailed overview of crowdsourcing video annotation. A\nrelated line of work is to obtain annotations through\nwell-designed games, e.g. (von Ahn and Dabbish, 2005).\nOur novel approaches to crowdsourcing accurate image\nannotations are in Sections 3.1.3, 3.2.1 and 3.3.3.\nStandardized challenges. There are several datasets with\nstandardized online evaluation similar to ILSVRC: the\naforementioned PASCAL VOC (Everingham et al., 2012),\nLabeled Faces in the Wild (Huang et al., 2007) for\nunconstrained face recognition, Reconstruction meets\nRecognition (Urtasun et al., 2014) for 3D reconstruc-\ntion and KITTI (Geiger et al., 2013) for computer vi-\nsion in autonomous driving. These datasets along with\nILSVRC help benchmark progress in diﬀerent areas of\ncomputer vision. Works such as (Torralba and Efros,\n2011) emphasize the importance of examining the bias\ninherent in any standardized dataset.\n1.2 Paper layout\nWe begin with a brief overview of ILSVRC challenge\ntasks in Section 2. Dataset collection and annotation\nare described at length in Section 3. Section 4 discusses\nthe evaluation criteria of algorithms in the large-scale\nrecognition setting. Section 5 provides an overview of\nthe methods developed by ILSVRC participants.\nSection 6 contains an in-depth analysis of ILSVRC\nresults: Section 6.1 documents the progress of large-\nscale recognition over the years, Section 6.2 concludes\nthat ILSVRC results are statistically signiﬁcant, Sec-\ntion 6.3 thoroughly analyzes the current state of the\nﬁeld of object recognition, and Section 6.4 compares\nstate-of-the-art computer vision accuracy with human\naccuracy. We conclude and discuss lessons learned from\nILSVRC in Section 7.\n4\nOlga Russakovsky* et al.\n2 Challenge tasks\nThe goal of ILSVRC is to estimate the content of pho-\ntographs for the purpose of retrieval and automatic\nannotation. Test images are presented with no initial\nannotation, and algorithms have to produce labelings\nspecifying what objects are present in the images. New\ntest images are collected and labeled especially for this\ncompetition and are not part of the previously pub-\nlished ImageNet dataset (Deng et al., 2009).\nILSVRC over the years has consisted of one or more\nof the following tasks (years in parentheses):3\n1. Image classiﬁcation (2010-2014): Algorithms pro-\nduce a list of object categories present in the image.\n2. Single-object localization (2011-2014): Algorithms\nproduce a list of object categories present in the im-\nage, along with an axis-aligned bounding box indi-\ncating the position and scale of one instance of each\nobject category.\n3. Object detection (2013-2014): Algorithms produce\na list of object categories present in the image along\nwith an axis-aligned bounding box indicating the\nposition and scale of every instance of each object\ncategory.\nThis section provides an overview and history of each\nof the three tasks. Table 1 shows summary statistics.\n2.1 Image classiﬁcation task\nData for the image classiﬁcation task consists of pho-\ntographs collected from Flickr4 and other search en-\ngines, manually labeled with the presence of one of\n1000 object categories. Each image contains one ground\ntruth label.\nFor each image, algorithms produce a list of object\ncategories present in the image. The quality of a label-\ning is evaluated based on the label that best matches\nthe ground truth label for the image (see Section 4.1).\nConstructing ImageNet was an eﬀort to scale up\nan image classiﬁcation dataset to cover most nouns in\nEnglish using tens of millions of manually veriﬁed pho-\ntographs (Deng et al., 2009). The image classiﬁcation\ntask of ILSVRC came as a direct extension of this ef-\nfort. A subset of categories and images was chosen and\nﬁxed to provide a standardized benchmark while the\nrest of ImageNet continued to grow.\n3 In addition, ILSVRC in 2012 also included a taster ﬁne-\ngrained classiﬁcation task, where algorithms would classify\ndog photographs into one of 120 dog breeds (Khosla et al.,\n2011). Fine-grained classiﬁcation has evolved into its own\nFine-Grained classiﬁcation challenge in 2013 (Berg et al.,\n2013), which is outside the scope of this paper.\n4 www.flickr.com\n2.2 Single-object localization task\nThe single-object localization task, introduced in 2011,\nbuilt oﬀof the image classiﬁcation task to evaluate the\nability of algorithms to learn the appearance of the tar-\nget object itself rather than its image context.\nData for the single-object localization task consists\nof the same photographs collected for the image classi-\nﬁcation task, hand labeled with the presence of one of\n1000 object categories. Each image contains one ground\ntruth label. Additionally, every instance of this category\nis annotated with an axis-aligned bounding box.\nFor each image, algorithms produce a list of object\ncategories present in the image, along with a bounding\nbox indicating the position and scale of one instance\nof each object category. The quality of a labeling is\nevaluated based on the object category label that best\nmatches the ground truth label, with the additional re-\nquirement that the location of the predicted instance is\nalso accurate (see Section 4.2).\n2.3 Object detection task\nThe object detection task went a step beyond single-\nobject localization and tackled the problem of localizing\nmultiple object categories in the image. This task has\nbeen a part of the PASCAL VOC for many years on\nthe scale of 20 object categories and tens of thousands\nof images, but scaling it up by an order of magnitude\nin object categories and in images proved to be very\nchallenging from a dataset collection and annotation\npoint of view (see Section 3.3).\nData for the detection tasks consists of new pho-\ntographs collected from Flickr using scene-level queries.\nThe images are annotated with axis-aligned bounding\nboxes indicating the position and scale of every instance\nof each target object category. The training set is ad-\nditionally supplemented with (a) data from the single-\nobject localization task, which contains annotations for\nall instances of just one object category, and (b) nega-\ntive images known not to contain any instance of some\nobject categories.\nFor each image, algorithms produce bounding boxes\nindicating the position and scale of all instances of all\ntarget object categories. The quality of labeling is eval-\nuated by recall, or number of target object instances\ndetected, and precision, or the number of spurious de-\ntections produced by the algorithm (see Section 4.3).\nImageNet Large Scale Visual Recognition Challenge\n5\nTask\nImage\nclassiﬁcation\nSingle-object\nlocalization\nObject\ndetection\nManual labeling\non training set\nNumber of object classes\nannotated per image\n1\n1\n1 or more\nLocations of\nannotated classes\n—\nall instances\non some images\nall instances\non all images\nManual labeling\non validation\nand test sets\nNumber of object classes\nannotated per image\n1\n1\nall target classes\nLocations of\nannotated classes\n—\nall instances\non all images\nall instances\non all images\nTable 1 Overview of the provided annotations for each of the tasks in ILSVRC.\n3 Dataset construction at large scale\nOur process of constructing large-scale object recogni-\ntion image datasets consists of three key steps.\nThe ﬁrst step is deﬁning the set of target object\ncategories. To do this, we select from among the ex-\nisting ImageNet (Deng et al., 2009) categories. By us-\ning WordNet as a backbone (Miller, 1995), ImageNet\nalready takes care of disambiguating word meanings\nand of combining together synonyms into the same ob-\nject category. Since the selection of object categories\nneeds to be done only once per challenge task, we use a\ncombination of automatic heuristics and manual post-\nprocessing to create the list of target categories appro-\npriate for each task. For example, for image classiﬁca-\ntion we may include broader scene categories such as\na type of beach, but for single-object localization and\nobject detection we want to focus only on object cate-\ngories which can be unambiguously localized in images\n(Sections 3.1.1 and 3.3.1).\nThe second step is collecting a diverse set of can-\ndidate images to represent the selected categories. We\nuse both automatic and manual strategies on multiple\nsearch engines to do the image collection. The process is\nmodiﬁed for the diﬀerent ILSVRC tasks. For example,\nfor object detection we focus our eﬀorts on collecting\nscene-like images using generic queries such as “African\nsafari” to ﬁnd pictures likely to contain multiple ani-\nmals in one scene (Section 3.3.2).\nThe third (and most challenging) step is annotat-\ning the millions of collected images to obtain a clean\ndataset. We carefully design crowdsourcing strategies\ntargeted to each individual ILSVRC task. For example,\nthe bounding box annotation system used for localiza-\ntion and detection tasks consists of three distinct parts\nin order to include automatic crowdsourced quality con-\ntrol (Section 3.2.1). Annotating images fully with all\ntarget object categories (on a reasonable budget) for\nobject detection requires an additional hierarchical im-\nage labeling system (Section 3.3.3).\nWe describe the data collection and annotation pro-\ncedure for each of the ILSVRC tasks in order: image\nclassiﬁcation (Section 3.1), single-object localization (Sec-\ntion 3.2), and object detection (Section 3.3), focusing\non the three key steps for each dataset.\n3.1 Image classiﬁcation dataset construction\nThe image classiﬁcation task tests the ability of an algo-\nrithm to name the objects present in the image, without\nnecessarily localizing them.\nWe describe the choices we made in constructing\nthe ILSVRC image classiﬁcation dataset: selecting the\ntarget object categories from ImageNet (Section 3.1.1),\ncollecting a diverse set of candidate images by using\nmultiple search engines and an expanded set of queries\nin multiple languages (Section 3.1.2), and ﬁnally ﬁlter-\ning the millions of collected images using the carefully\ndesigned crowdsourcing strategy of ImageNet (Deng et al.,\n2009) (Section 3.1.3).\n3.1.1 Deﬁning object categories for the image\nclassiﬁcation dataset\nThe 1000 categories used for the image classiﬁcation\ntask were selected from the ImageNet (Deng et al.,\n2009) categories. The 1000 synsets are selected such\nthat there is no overlap between synsets: for any synsets\ni and j, i is not an ancestor of j in the ImageNet hierar-\nchy. These synsets are part of the larger hierarchy and\nmay have children in ImageNet; however, for ILSVRC\nwe do not consider their child subcategories. The synset\nhierarchy of ILSVRC can be thought of as a “trimmed”\nversion of the complete ImageNet hierarchy. Figure 1\nvisualizes the diversity of the ILSVRC2012 object cat-\negories.\nThe exact 1000 synsets used for the image classiﬁca-\ntion and single-object localization tasks have changed\nover the years. There are 639 synsets which have been\nused in all ﬁve ILSVRC challenges so far. In the ﬁrst\nyear of the challenge synsets were selected randomly\nfrom the available ImageNet synsets at the time, fol-\nlowed by manual ﬁltering to make sure the object cat-\negories were not too obscure. With the introduction of\n6\nOlga Russakovsky* et al.\nFig. 1 The diversity of data in the ILSVRC image classiﬁcation and single-object localization tasks. For each of the eight\ndimensions, we show example object categories along the range of that property. Object scale, number of instances and image\nclutter for each object category are computed using the metrics deﬁned in Section 3.2.2 and in Appendix B. The other properties\nwere computed by asking human subjects to annotate each of the 1000 object categories (Russakovsky et al., 2013).\nImageNet Large Scale Visual Recognition Challenge\n7\nthe object localization challenge in 2011 there were 321\nsynsets that changed: categories such as “New Zealand\nbeach” which were inherently diﬃcult to localize were\nremoved, and some new categories from ImageNet con-\ntaining object localization annotations were added. In\nILSVRC2012, 90 synsets were replaced with categories\ncorresponding to dog breeds to allow for evaluation of\nmore ﬁne-grained object classiﬁcation, as shown in Fig-\nure 2. The synsets have remained consistent since year\n2012. Appendix A provides the complete list of object\ncategories used in ILSVRC2012-2014.\n3.1.2 Collecting candidate images for the image\nclassiﬁcation dataset\nImage collection for ILSVRC classiﬁcation task is the\nsame as the strategy employed for constructing Ima-\ngeNet (Deng et al., 2009). Training images are taken\ndirectly from ImageNet. Additional images are collected\nfor the ILSVRC using this strategy and randomly par-\ntitioned into the validation and test sets.\nWe brieﬂy summarize the process; (Deng et al., 2009)\ncontains further details. Candidate images are collected\nfrom the Internet by querying several image search en-\ngines. For each synset, the queries are the set of Word-\nNet synonyms. Search engines typically limit the num-\nber of retrievable images (on the order of a few hundred\nto a thousand). To obtain as many images as possi-\nble, we expand the query set by appending the queries\nwith the word from parent synsets, if the same word\nappears in the glossary of the target synset. For exam-\nple, when querying “whippet”, according to WordNet’s\nglossary a “small slender dog of greyhound type de-\nveloped in England”, we also use “whippet dog” and\n“whippet greyhound.” To further enlarge and diversify\nthe candidate pool, we translate the queries into other\nlanguages, including Chinese, Spanish, Dutch and Ital-\nian. We obtain accurate translations using WordNets in\nthose languages.\n3.1.3 Image classiﬁcation dataset annotation\nAnnotating images with corresponding object classes\nfollows the strategy employed by ImageNet (Deng et al.,\n2009). We summarize it brieﬂy here.\nTo collect a highly accurate dataset, we rely on hu-\nmans to verify each candidate image collected in the\nprevious step for a given synset. This is achieved by us-\ning Amazon Mechanical Turk (AMT), an online plat-\nform on which one can put up tasks for users for a\nmonetary reward. With a global user base, AMT is par-\nticularly suitable for large scale labeling. In each of our\nlabeling tasks, we present the users with a set of can-\ndidate images and the deﬁnition of the target synset\n(including a link to Wikipedia). We then ask the users\nto verify whether each image contains objects of the\nsynset. We encourage users to select images regardless\nof occlusions, number of objects and clutter in the scene\nto ensure diversity.\nWhile users are instructed to make accurate judg-\nment, we need to set up a quality control system to\nensure this accuracy. There are two issues to consider.\nFirst, human users make mistakes and not all users fol-\nlow the instructions. Second, users do not always agree\nwith each other, especially for more subtle or confus-\ning synsets, typically at the deeper levels of the tree.\nThe solution to these issues is to have multiple users\nindependently label the same image. An image is con-\nsidered positive only if it gets a convincing majority of\nthe votes. We observe, however, that diﬀerent categories\nrequire diﬀerent levels of consensus among users. For\nexample, while ﬁve users might be necessary for obtain-\ning a good consensus on Burmese cat images, a much\nsmaller number is needed for cat images. We develop a\nsimple algorithm to dynamically determine the number\nof agreements needed for diﬀerent categories of images.\nFor each synset, we ﬁrst randomly sample an initial\nsubset of images. At least 10 users are asked to vote\non each of these images. We then obtain a conﬁdence\nscore table, indicating the probability of an image being\na good image given the consensus among user votes. For\neach of the remaining candidate images in this synset,\nwe proceed with the AMT user labeling until a pre-\ndetermined conﬁdence score threshold is reached.\nEmpirical evaluation. Evaluation of the accuracy of the\nlarge-scale crowdsourced image annotation system was\ndone on the entire ImageNet (Deng et al., 2009). A to-\ntal of 80 synsets were randomly sampled at every tree\ndepth of the mammal and vehicle subtrees. An inde-\npendent group of subjects veriﬁed the correctness of\neach of the images. An average of 99.7% precision is\nachieved across the synsets. We expect similar accuracy\non ILSVRC image classiﬁcation dataset since the im-\nage annotation pipeline has remained the same. To ver-\nify, we manually checked 1500 ILSVRC2012-2014 image\nclassiﬁcation test set images (the test set has remained\nunchanged in these three years). We found 5 annotation\nerrors, corresponding as expected to 99.7% precision.\n3.1.4 Image classiﬁcation dataset statistics\nUsing the image collection and annotation procedure\ndescribed in previous sections, we collected a large-\nscale dataset used for ILSVRC classiﬁcation task. There\n8\nOlga Russakovsky* et al.\nPASCAL\nILSVRC\nbirds\n· · ·\ncats\n· · ·\ndogs\n· · ·\nFig. 2 The ILSVRC dataset contains many more ﬁne-grained classes compared to the standard PASCAL VOC benchmark;\nfor example, instead of the PASCAL “dog” category there are 120 diﬀerent breeds of dogs in ILSVRC2012-2014 classiﬁcation\nand single-object localization tasks.\nare 1000 object classes and approximately 1.2 million\ntraining images, 50 thousand validation images and 100\nthousand test images. Table 2 (top) documents the size\nof the dataset over the years of the challenge.\n3.2 Single-object localization dataset construction\nThe single-object localization task evaluates the ability\nof an algorithm to localize one instance of an object\ncategory. It was introduced as a taster task in ILSVRC\n2011, and became an oﬃcial part of ILSVRC in 2012.\nThe key challenge was developing a scalable crowd-\nsourcing method for object bounding box annotation.\nOur three-step self-verifying pipeline is described in Sec-\ntion 3.2.1. Having the dataset collected, we perform\ndetailed analysis in Section 3.2.2 to ensure that the\ndataset is suﬃciently varied to be suitable for evalu-\nation of object localization algorithms.\nObject classes and candidate images. The object classes\nfor single-object localization task are the same as the\nobject classes for image classiﬁcation task described\nabove in Section 3.1. The training images for localiza-\ntion task are a subset of the training images used for\nimage classiﬁcation task, and the validation and test\nimages are the same between both tasks.\nBounding box annotation. Recall that for the image\nclassiﬁcation task every image was annotated with one\nobject class label, corresponding to one object that is\npresent in an image. For the single-object localization\ntask, every validation and test image and a subset of the\ntraining images are annotated with axis-aligned bound-\ning boxes around every instance of this object.\nEvery bounding box is required to be as small as\npossible while including all visible parts of the object\ninstance. An alternate annotation procedure could be\nto annotate the full (estimated) extent of the object:\ne.g., if a person’s legs are occluded and only the torso\nis visible, the bounding box could be drawn to include\nthe likely location of the legs. However, this alterna-\ntive procedure is inherently ambiguous and ill-deﬁned,\nleading to disagreement among annotators and among\nresearchers (what is the true “most likely” extent of\nthis object?). We follow the standard protocol of only\nannotating visible object parts (Russell et al., 2007; Ev-\neringham et al., 2010).5\n3.2.1 Bounding box object annotation system\nWe summarize the crowdsourced bounding box anno-\ntation system described in detail in (Su et al., 2012).\nThe goal is to build a system that is fully automated,\n5 Some datasets such as PASCAL VOC (Everingham et al.,\n2010) and LabelMe (Russell et al., 2007) are able to provide\nmore detailed annotations: for example, marking individual\nobject instances as being truncated. We chose not to provide\nthis level of detail in favor of annotating more images and\nmore object instances.\nImageNet Large Scale Visual Recognition Challenge\n9\nImage classiﬁcation annotations (1000 object classes)\nYear\nTrain images (per class)\nVal images (per class)\nTest images (per class)\nILSVRC2010\n1,261,406 (668-3047)\n50,000 (50)\n150,000 (150)\nILSVRC2011\n1,229,413 (384-1300)\n50,000 (50)\n100,000 (100)\nILSVRC2012-14\n1,281,167 (732-1300)\n50,000 (50)\n100,000 (100)\nAdditional annotations for single-object localization (1000 object classes)\nYear\nTrain images with\nbbox annotations\n(per class)\nTrain bboxes\nannotated\n(per class)\nVal images with\nbbox annotations\n(per class)\nVal bboxes\nannotated\n(per class)\nTest images with\nbbox annotations\nILSVRC2011\n315,525 (104-1256)\n344,233 (114-1502)\n50,000 (50)\n55,388 (50-118)\n100,000\nILSVRC2012-14\n523,966 (91-1268)\n593,173 (92-1418)\n50,000 (50)\n64,058 (50-189)\n100,000\nTable 2 Scale of ILSVRC image classiﬁcation task (top) and single-object localization task (bottom). The numbers in paren-\ntheses correspond to (minimum per class - maximum per class). The 1000 classes change from year to year but are consistent\nbetween image classiﬁcation and single-object localization tasks in the same year. All images from the image classiﬁcation task\nmay be used for single-object localization.\nhighly accurate, and cost-eﬀective. Given a collection\nof images where the object of interest has been veri-\nﬁed to exist, for each image the system collects a tight\nbounding box for every instance of the object.\nThere are two requirements:\n– Quality Each bounding box needs to be tight, i.e.\nthe smallest among all bounding boxes that contains\nall visible parts of the object. This facilitates the\nobject detection learning algorithms by providing\nthe precise location of each object instance;\n– Coverage Every object instance needs to have a\nbounding box. This is important for training local-\nization algorithms because it tells the learning algo-\nrithms with certainty what is not the object.\nThe core challenge of building such a system is ef-\nfectively controlling the data quality with minimal cost.\nOur key observation is that drawing a bounding box is\nsigniﬁcantly more diﬃcult and time consuming than\ngiving answers to multiple choice questions. Thus qual-\nity control through additional veriﬁcation tasks is more\ncost-eﬀective than consensus-based algorithms. This leads\nto the following workﬂow with simple basic subtasks:\n1. Drawing A worker draws one bounding box around\none instance of an object on the given image.\n2. Quality veriﬁcation A second worker checks if the\nbounding box is correctly drawn.\n3. Coverage veriﬁcation A third worker checks if all\nobject instances have bounding boxes.\nThe sub-tasks are designed following two principles.\nFirst, the tasks are made as simple as possible. For ex-\nample, instead of asking the worker to draw all bound-\ning boxes on the same image, we ask the worker to draw\nonly one. This reduces the complexity of the task. Sec-\nond, each task has a ﬁxed and predictable amount of\nwork. For example, assuming that the input images are\nclean (object presence is correctly veriﬁed) and the cov-\nerage veriﬁcation tasks give correct results, the amount\nof work of the drawing task is always that of providing\nexactly one bounding box.\nQuality control on Tasks 2 and 3 is implemented\nby embedding “gold standard” images where the cor-\nrect answer is known. Worker training for each of these\nsubtasks is described in detail in (Su et al., 2012).\nEmpirical evaluation. The system is evaluated on 10\ncategories with ImageNet (Deng et al., 2009): balloon,\nbear, bed, bench, beach, bird, bookshelf, basketball hoop,\nbottle, and people. A subset of 200 images are ran-\ndomly sampled from each category. On the image level,\nour evaluation shows that 97.9% images are completely\ncovered with bounding boxes. For the remaining 2.1%,\nsome bounding boxes are missing. However, these are\nall diﬃcult cases: the size is too small, the boundary is\nblurry, or there is strong shadow.\nOn the bounding box level, 99.2% of all bound-\ning boxes are accurate (the bounding boxes are visi-\nbly tight). The remaining 0.8% are somewhat oﬀ. No\nbounding boxes are found to have less than 50% inter-\nsection over union overlap with ground truth.\nAdditional evaluation of the overall cost and an anal-\nysis of quality control can be found in (Su et al., 2012).\n3.2.2 Single-object localization dataset statistics\nUsing the annotation procedure described above, we\ncollect a large set of bounding box annotations for the\nILSVRC single-object classiﬁcation task. All 50 thou-\nsand images in the validation set and 100 thousand im-\nages in the test set are annotated with bounding boxes\naround all instances of the ground truth object class\n(one object class per image). In addition, in ILSVRC2011\n25% of training images are annotated with bounding\n10\nOlga Russakovsky* et al.\nboxes the same way, yielding more than 310 thousand\nannotated images with more than 340 thousand anno-\ntated object instances. In ILSVRC2012 40% of training\nimages are annotated, yielding more than 520 thousand\nannotated images with more than 590 thousand anno-\ntated object instances. Table 2 (bottom) documents the\nsize of this dataset.\nIn addition to the size of the dataset, we also ana-\nlyze the level of diﬃculty of object localization in these\nimages compared to the PASCAL VOC benchmark. We\ncompute statistics on the ILSVRC2012 single-object lo-\ncalization validation set images compared to PASCAL\nVOC 2012 validation images.\nReal-world scenes are likely to contain multiple in-\nstances of some objects, and nearby object instances are\nparticularly diﬃcult to delineate. The average object\ncategory in ILSVRC has 1.61 target object instances\non average per positive image, with each instance hav-\ning on average 0.47 neighbors (adjacent instances of\nthe same object category). This is comparable to 1.69\ninstances per positive image and 0.52 neighbors per in-\nstance for an average object class in PASCAL.\nAs described in (Hoiem et al., 2012), smaller ob-\njects tend to be signiﬁcantly more diﬃcult to local-\nize. In the average object category in PASCAL the ob-\nject occupies 24.1% of the image area, and in ILSVRC\n35.8%. However, PASCAL has only 20 object categories\nwhile ILSVRC has 1000. The 537 object categories of\nILSVRC with the smallest objects on average occupy\nthe same fraction of the image as PASCAL objects:\n24.1%. Thus even though on average the object in-\nstances tend to be bigger in ILSVRC images, there are\nmore than 25 times more object categories than in PAS-\nCAL VOC with the same average object scale.\nAppendix B and (Russakovsky et al., 2013) have\nadditional comparisons.\n3.3 Object detection dataset construction\nThe ILSVRC task of object detection evaluates the abil-\nity of an algorithm to name and localize all instances of\nall target objects present in an image. It is much more\nchallenging than object localization because some ob-\nject instances may be small/occluded/diﬃcult to accu-\nrately localize, and the algorithm is expected to locate\nthem all, not just the one it ﬁnds easiest.\nThere are three key challenges in collecting the ob-\nject detection dataset. The ﬁrst challenge is selecting\nthe set of common objects which tend to appear in clut-\ntered photographs and are well-suited for benchmarking\nobject detection performance. Our approach relies on\nstatistics of the object localization dataset and the tra-\ndition of the PASCAL VOC challenge (Section 3.3.1).\nClass name in\nClosest class in\nAvg object scale (%)\nPASCAL VOC\nILSVRC-DET\nPASCAL\nILSVRC-\n(20 classes)\n(200 classes)\nVOC\nDET\naeroplane\nairplane\n29.7\n22.4\nbicycle\nbicycle\n29.3\n14.3\nbird\nbird\n15.9\n20.1\nboat\nwatercraft\n15.2\n16.5\nbottle\nwine bottle\n7.3\n10.4\nbus\nbus\n29.9\n22.1\ncar\ncar\n14.0\n13.4\ncat\ndomestic cat\n46.8\n29.8\nchair\nchair\n12.8\n10.1\ncow\ncattle\n19.3\n13.5\ndining table\ntable\n29.1\n30.3\ndog\ndog\n37.0\n28.9\nhorse\nhorse\n29.5\n18.5\nmotorbike\nmotorcyle\n32.0\n20.7\nperson\nperson\n17.5\n19.3\npotted plant\nﬂower pot\n12.3\n8.1\nsheep\nsheep\n12.2\n17.3\nsofa\nsofa\n41.7\n44.4\ntrain\ntrain\n35.4\n35.1\ntv/monitor\ntv or monitor\n14.6\n11.2\nTable 3 Correspondences between the object classes in the\nPASCAL VOC (Everingham et al., 2010) and the ILSVRC\ndetection task. Object scale is the fraction of image area (re-\nported in percent) occupied by an object instance. It is com-\nputed on the validation sets of PASCAL VOC 2012 and of\nILSVRC-DET. The average object scale is 24.1% across the\n20 PASCAL VOC categories and 20.3% across the 20 corre-\nsponding ILSVRC-DET categories. Section 3.3.4 reports ad-\nditional dataset statistics.\nThe second challenge is obtaining a much more var-\nied set of scene images than those used for the image\nclassiﬁcation and single-object localization datasets. Sec-\ntion 3.3.2 describes the procedure for utilizing as much\ndata from the single-object localization dataset as pos-\nsible and supplementing it with Flickr images queried\nusing hundreds of manually designed high-level queries.\nThe third, and biggest, challenge is completely an-\nnotating this dataset with all the objects. This is done\nin two parts. Section 3.3.3 describes the ﬁrst part: our\nhierarchical strategy for obtaining the list of all target\nobjects which occur within every image. This is nec-\nessary since annotating in a straight-forward way by\ncreating a task for every (image, object class) pair is\nno longer feasible at this scale. Appendix E describes\nthe second part: annotating the bounding boxes around\nthese objects, using the single-object localization bound-\ning box annotation pipeline of Section 3.2.1 along with\nextra veriﬁcation to ensure that every instance of the\nobject is annotated with exactly one bounding box.\nImageNet Large Scale Visual Recognition Challenge\n11\n3.3.1 Deﬁning object categories for the object detection\ndataset\nThere are 200 object classes hand-selected for the de-\ntection task, eacg corresponding to a synset within Im-\nageNet. These were chosen to be mostly basic-level ob-\nject categories that would be easy for people to identify\nand label. The rationale is that the object detection\nsystem developed for this task can later be combined\nwith a ﬁne-grained classiﬁcation model to further clas-\nsify the objects if a ﬁner subdivision is desired.6 As with\nthe 1000 classiﬁcation classes, the synsets are selected\nsuch that there is no overlap: for any synsets i and j, i\nis not an ancestor of j in the ImageNet hierarchy.\nThe selection of the 200 object detection classes in\n2013 was guided by the ILSVRC 2012 classiﬁcation and\nlocalization dataset. Starting with 1000 object classes\nand their bounding box annotations we ﬁrst eliminated\nall object classes which tended to be too “big” in the\nimage (on average the object area was greater than\n50% of the image area). These were classes such as\nT-shirt, spiderweb, or manhole cover. We then man-\nually eliminated all classes which we did not feel were\nwell-suited for detection, such as hay, barbershop, or\nponcho. This left 494 object classes which were merged\ninto basic-level categories: for example, diﬀerent species\nof birds were merged into just the “bird” class. The\nclasses remained the same in ILSVRC2014. Appendix D\ncontains the complete list of object categories used in\nILSVRC2013-2014 (in the context of the hierarchy de-\nscribed in Section 3.3.3).\nStaying mindful of the tradition of the PASCAL\nVOC dataset we also tried to ensure that the set of\n200 classes contains as many of the 20 PASCAL VOC\nclasses as possible. Table 3 shows the correspondences.\nThe changes that were done were to ensure more accu-\nrate and consistent crowdsourced annotations. The ob-\nject class with the weakest correspondence is “potted\nplant” in PASCAL VOC, corresponding to “ﬂower pot”\nin ILSVRC. “Potted plant” was one of the most chal-\nlenging object classes to annotate consistently among\nthe PASCAL VOC classes, and in order to obtain accu-\nrate annotations using crowdsourcing we had to restrict\nthe deﬁnition to a more concrete object.\n3.3.2 Collecting images for the object detection dataset\nMany images for the detection task were collected dif-\nferently than the images in ImageNet and the classiﬁca-\n6 Some of the training objects are actually annotated with\nmore detailed classes: for example, one of the 200 object\nclasses is the category “dog,” and some training instances\nare annotated with the speciﬁc dog breed.\nFig. 3 Summary of images collected for the detection task.\nImages in green (bold) boxes have all instances of all 200 de-\ntection object classes fully annotated. Table 4 lists the com-\nplete statistics.\ntion and single-object localization tasks. Figure 3 sum-\nmarizes the types of images that were collected. Ideally\nall of these images would be scene images fully anno-\ntated with all target categories. However, given budget\nconstraints our goal was to provide as much suitable de-\ntection data as possible, even if the images were drawn\nfrom a few diﬀerent sources and distributions.\nThe validation and test detection set images come\nfrom two sources (percent of images from each source\nin parentheses). The ﬁrst source (77%) is images from\nILSVRC2012 single-object localization validation and\ntest sets corresponding to the 200 detection classes (or\ntheir children in the ImageNet hierarchy). Images where\nthe target object occupied more than 50% of the image\narea were discarded, since they were unlikely to con-\ntain other objects of interest. The second source (23%)\nis images from Flickr collected speciﬁcally for detection\ntask. We queried Flickr using a large set of manually de-\nﬁned queries, such as “kitchenette” or “Australian zoo”\nto retrieve images of scenes likely to contain several ob-\njects of interest. Appendix C contains the full list. We\nalso added pairwise queries, or queries with two tar-\nget object names such as “tiger lion,” which also often\nreturned cluttered scenes.\nFigure 4 shows a random set of both types of val-\nidation images. Images were randomly split, with 33%\ngoing into the validation set and 67% into the test set.7\nThe training set for the detection task comes from\nthree sources of images (percent of images from each\nsource in parentheses). The ﬁrst source (63%) is all\ntraining images from ILSVRC2012 single-object local-\nization task corresponding to the 200 detection classes\n(or their children in the ImageNet hierarchy). We did\nnot ﬁlter by object size, allowing teams to take advan-\n7 The validation/test split is consistent with ILSVRC2012:\nvalidation images of ILSVRC2012 remained in the validation\nset of ILSVRC2013, and ILSVRC2012 test images remained\nin ILSVRC2013 test set.\n12\nOlga Russakovsky* et al.\nFig. 4 Random selection of images in ILSVRC detection validation set. The images in the top 4 rows were taken from\nILSVRC2012 single-object localization validation set, and the images in the bottom 4 rows were collected from Flickr using\nscene-level queries.\ntage of all the positive examples available. The second\nsource (24%) is negative images which were part of the\noriginal ImageNet collection process but voted as neg-\native: for example, some of the images were collected\nfrom Flickr and search engines for the ImageNet synset\n“animals” but during the manual veriﬁcation step did\nnot collect enough votes to be considered as containing\nan “animal.” These images were manually re-veriﬁed\nfor the detection task to ensure that they did not in\nfact contain the target objects. The third source (13%)\nis images collected from Flickr speciﬁcally for the de-\ntection task. These images were added for ILSVRC2014\nfollowing the same protocol as the second type of images\nin the validation and test set. This was done to bring\nthe training and testing distributions closer together.\nImageNet Large Scale Visual Recognition Challenge\n13\nFig. 5 Consider the problem of binary multi-label annota-\ntion. For each input (e.g., image) and each label (e.g., object),\nthe goal is to determine the presence or absense (+ or -) of the\nlabel (e.g., decide if the object is present in the image). Multi-\nlabel annotation becomes much more eﬃcient when consid-\nering real-world structure of data: correlation between labels,\nhierarchical organization of concepts, and sparsity of labels.\n3.3.3 Complete image-object annotation for the object\ndetection dataset\nThe key challenge in annotating images for the object\ndetection task is that all objects in all images need to\nbe labeled. Suppose there are N inputs (images) which\nneed to be annotated with the presence or absence of\nK labels (objects). A na¨ıve approach would query hu-\nmans for each combination of input and label, requiring\nNK queries. However, N and K can be very large and\nthe cost of this exhaustive approach quickly becomes\nprohibitive. For example, annotating 60, 000 validation\nand test images with the presence or absence of 200 ob-\nject classes for the detection task na¨ıvely would take 80\ntimes more eﬀort than annotating 150, 000 validation\nand test images with 1 object each for the classiﬁcation\ntask – and this is not even counting the additional cost\nof collecting bounding box annotations around each ob-\nject instance. This quickly becomes infeasible.\nIn (Deng et al., 2014) we study strategies for scal-\nable multilabel annotation, or for eﬃciently acquiring\nmultiple labels from humans for a collection of items.\nWe exploit three key observations for labels in real\nworld applications (illustrated in Figure 5):\n1. Correlation. Subsets of labels are often highly cor-\nrelated. Objects such as a computer keyboard, mouse\nand monitor frequently co-occur in images. Simi-\nlarly, some labels tend to all be absent at the same\ntime. For example, all objects that require electricity\nare usually absent in pictures taken outdoors. This\nsuggests that we could potentially ﬁll in the values\nof multiple labels by grouping them into only one\nquery for humans. Instead of checking if dog, cat,\nrabbit etc. are present in the photo, we just check\nabout the “animal” group If the answer is no, then\nthis implies a no for all categories in the group.\n2. Hierarchy. The above example of grouping dog,\ncat, rabbit etc. into animal has implicitly assumed\nthat labels can be grouped together and humans\ncan eﬃciently answer queries about the group as a\nwhole. This brings up our second key observation:\nhumans organize semantic concepts into hierarchies\nand are able to eﬃciently categorize at higher se-\nmantic levels (Thorpe et al., 1996), e.g. humans can\ndetermine the presence of an animal in an image as\nfast as every type of animal individually. This leads\nto substantial cost savings.\n3. Sparsity. The values of labels for each image tend\nto be sparse, i.e. an image is unlikely to contain more\nthan a dozen types of objects, a small fraction of the\nhundreds of object categories. This enables rapid\nelimination of many objects by quickly ﬁlling in no.\nWith a high degree of sparsity, an eﬃcient algorithm\ncan have a cost which grows logarithmically with the\nnumber of objects instead of linearly.\nWe propose algorithmic strategies that exploit the\nabove intuitions. The key is to select a sequence of\nqueries for humans such that we achieve the same label-\ning results with only a fraction of the cost of the na¨ıve\napproach. The main challenges include how to mea-\nsure cost and utility of queries, how to construct good\nqueries, and how to dynamically order them. A detailed\ndescription of the generic algorithm, along with theo-\nretical analysis and empirical evaluation, is presented\nin (Deng et al., 2014).\nApplication of the generic multi-class labeling algorithm\nto our setting. The generic algorithm automatically se-\nlects the most informative queries to ask based on ob-\nject label statistics learned from the training set. In\nour case of 200 object classes, since obtaining the train-\ning set was by itself challenging we chose to design the\nqueries by hand. We created a hierarchy of queries of\nthe type “is there a... in the image?” For example, one\nof the high-level questions was “is there an animal in\nthe image?” We ask the crowd workers this question\nabout every image we want to label. The children of\nthe “animal” question would correspond to speciﬁc ex-\namples of animals: for example, “is there a mammal in\nthe image?” or “is there an animal with no legs?” To\nannotate images eﬃciently, these questions are asked\nonly on images determined to contain an animal. The\n200 leaf node questions correspond to the 200 target ob-\njects, e.g., “is there a cat in the image?”. A few sample\niterations of the algorithm are shown in Figure 6.\nAlgorithm 1 is the formal algorithm for labeling an\nimage with the presence or absence of each target object\n14\nOlga Russakovsky* et al.\nImage\nObject Presence\nIs there\nan animal?\nIs there\na mammal?\nIs there\na cat?\nFig. 6 Our algorithm dynamically selects the next query to\neﬃciently determine the presence or absence of every object\nin every image. Green denotes a positive annotation and red\ndenotes a negative annotation. This toy example illustrates a\nsample progression of the algorithm for one label (cat) on a\nset of images.\ncategory. With this algorithm in mind, the hierarchy of\nquestions was constructed following the principle that\nfalse positives only add extra cost whereas false nega-\ntives can signiﬁcantly aﬀect the quality of the labeling.\nThus, it is always better to stick with more general but\nless ambiguous questions, such as “is there a mammal\nin the image?” as opposed to asking overly speciﬁc but\npotentially ambiguous questions, such as “is there an\nanimal that can climb trees?” Constructing this hierar-\nchy was a surprisingly time-consuming process, involv-\ning multiple iterations to ensure high accuracy of label-\ning and avoid question ambiguity. Appendix D shows\nthe constructed hierarchy.\nBounding box annotation. Once all images are labeled\nwith the presence or absence of all object categories we\nuse the bounding box system described in Section 3.2.1\nalong with some additional modiﬁcations of Appendix E\nto annotate the location of every instance of every present\nobject category.\n3.3.4 Object detection dataset statistics\nUsing the procedure described above, we collect a large-\nscale dataset for ILSVRC object detection task. There\nare 200 object classes and approximately 450K training\nimages, 20K validation images and 40K test images. Ta-\nble 4 documents the size of the dataset over the years of\nthe challenge. The major change between ILSVRC2013\nand ILSVRC2014 was the addition of 60,658 fully an-\nnotated training images.\nPrior to ILSVRC, the object detection benchmark\nwas the PASCAL VOC challenge (Everingham et al.,\n2010). ILSVRC has 10 times more object classes than\nPASCAL VOC (200 vs 20), 10.6 times more fully an-\nnotated training images (60,658 vs 5,717), 35.2 times\nmore training objects (478,807 vs 13,609), 3.5 times\nmore validation images (20,121 vs 5823) and 3.5 times\nmore validation objects (55,501 vs 15,787). ILSVRC has\n2.8 annotated objects per image on the validation set,\ncompared to 2.7 in PASCAL VOC. The average ob-\nject in ILSVRC takes up 17.0% of the image area and\nin PASCAL VOC takes up 20.7%; Table 3 contains\nInput: Image i, queries Q, directed graph G over Q\nOutput: Labels L : Q →{“yes”, “no”}\nInitialize labels L(q) = ∅∀q ∈Q;\nInitialize candidates C = {q: q ∈Root(G)};\nwhile C not empty do\nObtain answer A to query q∗∈C;\nL(q∗) = A; C = C\\{q∗};\nif A is “yes” then\nChldr = {q ∈Children(q∗, G): L(q) = ∅};\nC = C ∪Chldr;\nelse\nDes = {q ∈Descendants(q∗, G): L(q) = ∅};\nL(q) = “no′′ ∀q ∈Des;\nC = C\\Des;\nend\nend\nAlgorithm 1: The algorithm for complete multi-class\nannotation. This is a special case of the algorithm de-\nscribed in (Deng et al., 2014). A hierarchy of ques-\ntions G is manually constructed. All root questions\nare asked on every image. If the answer to query q∗\non image i is “no” then the answer is assumed to be\n“no” for all queries q such that q is a descendant of\nq∗in the hierarchy. We continue asking the queries\nuntil all queries are answered. For images taken from\nthe single-object localization task we used the known\nobject label to initialize L.\nper-class comparisons. Additionally, ILSVRC contains\na wide variety of objects, including tiny objects such as\nsunglasses (1.3% of image area on average), ping-pong\nballs (1.5% of image area on average) and basketballs\n(2.0% of image area on average).\n4 Evaluation at large scale\nOnce the dataset has been collected, we need to deﬁne a\nstandardized evaluation procedure for algorithms. Some\nmeasures have already been established by datasets such\nas the Caltech 101 (Fei-Fei et al., 2004) for image clas-\nsiﬁcation and PASCAL VOC (Everingham et al., 2012)\nfor both image classiﬁcation and object detection. To\nadapt these procedures to the large-scale setting we had\nto address three key challenges. First, for the image\nclassiﬁcation and single-object localization tasks only\none object category could be labeled in each image due\nto the scale of the dataset. This created potential ambi-\nguity during evaluation (addressed in Section 4.1). Sec-\nond, evaluating localization of object instances is inher-\nently diﬃcult in some images which contain a cluster\nof objects (addressed in Section 4.2). Third, evaluating\nlocalization of object instances which occupy few pixels\nin the image is challenging (addressed in Section 4.3).\nIn this section we describe the standardized eval-\nuation criteria for each of the three ILSVRC tasks.\nImageNet Large Scale Visual Recognition Challenge\n15\nObject detection annotations (200 object classes)\nYear\nTrain images\n(per class)\nTrain bboxes annotated\n(per class)\nVal images\n(per class)\nVal bboxes annotated\n(per class )\nTest\nimages\nILSVRC2013\n395909\n(417-561-66911 pos,\n185-4130-10073 neg)\n345854\n(438-660-73799)\n21121\n(23-58-5791 pos,\nrest neg)\n55501\n(31-111-12824)\n40152\nILSVRC2014\n456567\n(461-823-67513 pos,\n42945-64614-70626 neg)\n478807\n(502-1008-74517)\n21121\n(23-58-5791 pos,\nrest neg)\n55501\n(31-111-12824)\n40152\nTable 4 Scale of ILSVRC object detection task. Numbers in parentheses correspond to (minimum per class - median per\nclass - maximum per class).\nWe elaborate further on these and other more minor\nchallenges with large-scale evaluation. Appendix F de-\nscribes the submission protocol and other details of run-\nning the competition itself.\n4.1 Image classiﬁcation\nThe scale of ILSVRC classiﬁcation task (1000 categories\nand more than a million of images) makes it very ex-\npensive to label every instance of every object in every\nimage. Therefore, on this dataset only one object cate-\ngory is labeled in each image. This creates ambiguity in\nevaluation. For example, an image might be labeled as\na “strawberry” but contain both a strawberry and an\napple. Then an algorithm would not know which one\nof the two objects to name. For the image classiﬁcation\ntask we allowed an algorithm to identify multiple (up\nto 5) objects in an image and not be penalized as long\nas one of the objects indeed corresponded to the ground\ntruth label. Figure 7(top row) shows some examples.\nConcretely, each image i has a single class label Ci.\nAn algorithm is allowed to return 5 labels ci1, . . . ci5,\nand is considered correct if cij = Ci for some j.\nLet the error of a prediction dij = d(cij, Ci) be 1\nif cij ̸= Ci and 0 otherwise. The error of an algorithm\nis the fraction of test images on which the algorithm\nmakes a mistake:\nerror = 1\nN\nN\nX\ni=1\nmin\nj\ndij\n(1)\nWe used two additional measures of error. First, we\nevaluated top-1 error. In this case algorithms were pe-\nnalized if their highest-conﬁdence output label ci1 did\nnot match ground truth class Ci. Second, we evaluated\nhierarchical error. The intuition is that confusing two\nnearby classes (such as two diﬀerent breeds of dogs) is\nnot as harmful as confusing a dog for a container ship.\nFor the hierarchical criteria, the cost of one misclassiﬁ-\ncation, d(cij, Ci), is deﬁned as the height of the lowest\ncommon ancestor of cij and Ci in the ImageNet hier-\narchy. The height of a node is the length of the longest\npath to a leaf node (leaf nodes have height zero).\nHowever, in practice we found that all three mea-\nsures of error (top-5, top-1, and hierarchical) produced\nthe same ordering of results. Thus, since ILSVRC2012\nwe have been exclusively using the top-5 metric which\nis the simplest and most suitable to the dataset.\n4.2 Single-object localization\nThe evaluation for single-object localization is similar\nto object classiﬁcation, again using a top-5 criteria to al-\nlow the algorithm to return unannotated object classes\nwithout penalty. However, now the algorithm is con-\nsidered correct only if it both correctly identiﬁes the\ntarget class Ci and accurately localizes one of its in-\nstances. Figure 7(middle row) shows some examples.\nConcretely, an image is associated with object class\nCi, with all instances of this object class annotated with\nbounding boxes Bik. An algorithm returns {(cij, bij)}5\nj=1\nof class labels cij and associated locations bij. The error\nof a prediction j is:\ndij = max(d(cij, Ci), min\nk d(bij, Bik))\n(2)\nHere d(bij, Bik) is the error of localization, deﬁned as 0\nif the area of intersection of boxes bij and Bik divided\nby the areas of their union is greater than 0.5, and 1\notherwise. (Everingham et al., 2010) The error of an\nalgorithm is computed as in Eq. 1.\nEvaluating localization is inherently diﬃcult in some\nimages. Consider a picture of a bunch of bananas or a\ncarton of apples. It is easy to classify these images as\ncontaining bananas or apples, and even possible to lo-\ncalize a few instances of each fruit. However, in order\nfor evaluation to be accurate every instance of banana\nor apple needs to be annotated, and that may be impos-\nsible. To handle the images where localizing individual\nobject instances is inherently ambiguous we manually\ndiscarded 3.5% of images since ILSVRC2012. Some ex-\namples of discarded images are shown in Figure 8.\n16\nOlga Russakovsky* et al.\nFig. 7 Tasks in ILSVRC. The ﬁrst column shows the ground truth labeling on an example image, and the next three show\nthree sample outputs with the corresponding evaluation score.\nFig. 8 Images marked as “diﬃcult” in the ILSVRC2012 single-object localization validation set. Please refer to Section 4.2\nfor details.\n4.3 Object detection\nThe criteria for object detection was adopted from PAS-\nCAL VOC (Everingham et al., 2010). It is designed to\npenalize the algorithm for missing object instances, for\nduplicate detections of one instance, and for false posi-\ntive detections. Figure 7(bottom row) shows examples.\nFor each object class and each image Ii, an algo-\nrithm returns predicted detections (bij, sij) of predicted\nlocations bij with conﬁdence scores sij. These detec-\ntions are greedily matched to the ground truth boxes\n{Bik} using Algorithm 2. For every detection j on im-\nage i the algorithm returns zij = 1 if the detection is\nmatched to a ground truth box according to the thresh-\nold criteria, and 0 otherwise. For a given object class,\nlet N be the total number of ground truth instances\nacross all images. Given a threshold t, deﬁne recall as\nthe fraction of the N objects detected by the algorithm,\nand precision as the fraction of correct detections out\nof the total detections returned by the algorithm. Con-\ncretely,\nRecall(t) =\nP\nij 1[sij ≥t]zij\nN\n(3)\nPrecision(t) =\nP\nij 1[sij ≥t]zij\nP\nij 1[sij ≥t]\n(4)\nImageNet Large Scale Visual Recognition Challenge\n17\nInput: Bounding box predictions with conﬁdence\nscores {(bj, sj)}M\nj=1 and ground truth boxes B\non image I for a given object class.\nOutput: Binary results {zj}M\nj=1 of whether or not\nprediction j is a true positive detection\nLet U = B be the set of unmatched objects;\nOrder {(bj, sj)}M\nj=1 in descending order of sj;\nfor j=1 . . . M do\nLet C = {Bk ∈U : IOU(Bk, bj) ≥thr(Bk)};\nif C ̸= ∅then\nLet k∗= arg max{k : Bk∈C} IOU(Bk, bj);\nSet U = U\\Bk∗;\nSet zj = 1 since true positive detection;\nelse\nSet zj = 0 since false positive detection;\nend\nend\nAlgorithm 2: The algorithm for greedily matching\nobject detection outputs to ground truth labels. The\nstandard thr(Bk) = 0.5 (Everingham et al., 2010).\nILSVRC computes thr(Bk) using Eq. 5 to better han-\ndle low-resolution objects.\nThe ﬁnal metric for evaluating an algorithm on a\ngiven object class is average precision over the diﬀerent\nlevels of recall achieved by varying the threshold t. The\nwinner of each object class is then the team with the\nhighest average precision, and then winner of the chal-\nlenge is the team that wins on the most object classes.8\nDiﬀerence with PASCAL VOC. Evaluating localization\nof object instances which occupy very few pixels in the\nimage is challenging. The PASCAL VOC approach was\nto label such instances as “diﬃcult” and ignore them\nduring evaluation. However, since ILSVRC contains a\nmore diverse set of object classes including, for exam-\nple, “nail” and “ping pong ball” which have many very\nsmall instances, it is important to include even very\nsmall object instances in evaluation.\nIn Algorithm 2, a predicted bounding box b is con-\nsidered to have properly localized by a ground truth\nbounding box B if IOU(b, B) ≥thr(B). The PASCAL\nVOC metric uses the threshold thr(B) = 0.5. However,\nfor small objects even deviations of a few pixels would\nbe unacceptable according to this threshold. For exam-\nple, consider an object B of size 10 × 10 pixels, with a\ndetection window of 20 × 20 pixels which fully contains\nthat object. This would be an error of approximately 5\npixels on each dimension, which is average human an-\nnotation error. However, the IOU in this case would be\n100/400 = 0.25, far below the threshold of 0.5. Thus\n8 In this paper we focus on the mean average precision\nacross all categories as the measure of a team’s performance.\nThis is done for simplicity and is justiﬁed since the ordering\nof teams by mean average precision was always the same as\nthe ordering by object categories won.\nfor smaller objects we loosen the threshold in ILSVRC\nto allow for the annotation to extend up to 5 pixels on\naverage in each direction around the object. Concretely,\nif the ground truth box B is of dimensions w × h then\nthr(B) = min\n\u0012\n0.5,\nwh\n(w + 10)(h + 10)\n\u0013\n(5)\nIn practice, this changes the threshold only on objects\nwhich are smaller than approximately 25 × 25 pixels,\nand aﬀects 5.5% of objects in the detection validation\nset.\nPractical consideration. One additional practical con-\nsideration for ILSVRC detection evaluation is subtle\nand comes directly as a result of the scale of ILSVRC.\nIn PASCAL, algorithms would often return many de-\ntections per class on the test set, including ones with\nlow conﬁdence scores. This allowed the algorithms to\nreach the level of high recall at least in the realm of\nvery low precision. On ILSVRC detection test set if\nan algorithm returns 10 bounding boxes per object per\nimage this would result in 10×200×40K = 80M detec-\ntions. Each detection contains an image index, a class\nindex, 4 bounding box coordinates, and the conﬁdence\nscore, so it takes on the order of 28 bytes. The full set of\ndetections would then require 2.24Gb to store and sub-\nmit to the evaluation server, which is impractical. This\nmeans that algorithms are implicitly required to limit\ntheir predictions to only the most conﬁdent locations.\n5 Methods\nThe ILSVRC dataset and the competition has allowed\nsigniﬁcant algorithmic advances in large-scale image recog-\nnition and retrieval.\n5.1 Challenge entries\nThis section is organized chronologically, highlighting\nthe particularly innovative and successful methods which\nparticipated in the ILSVRC each year. Tables 5, 6 and 7\nlist all the participating teams. We see a turning point\nin 2012 with the development of large-scale convolu-\ntional neural networks.\nILSVRC2010. The ﬁrst year the challenge consisted\nof just the classiﬁcation task. The winning entry from\nNEC team (Lin et al., 2011) used SIFT (Lowe, 2004)\nand LBP (Ahonen et al., 2006) features with two non-\nlinear coding representations (Zhou et al., 2010; Wang\n18\nOlga Russakovsky* et al.\net al., 2010) and a stochastic SVM. The honorable men-\ntion XRCE team (Perronnin et al., 2010) used an im-\nproved Fisher vector representation (Perronnin and Dance,\n2007) along with PCA dimensionality reduction and\ndata compression followed by a linear SVM. Fisher vector-\nbased methods have evolved over ﬁve years of the chal-\nlenge and continued performing strongly in every ILSVRC\nfrom 2010 to 2014.\nILSVRC2011. The winning classiﬁcation entry in 2011\nwas the 2010 runner-up team XRCE, applying high-\ndimensional image signatures (Perronnin et al., 2010)\nwith compression using product quantization (Sanchez\nand Perronnin, 2011) and one-vs-all linear SVMs. The\nsingle-object localization competition was held for the\nﬁrst time, with two brave entries. The winner was the\nUvA team using a selective search approach to gener-\nate class-independent object hypothesis regions (van de\nSande et al., 2011b), followed by dense sampling and\nvector quantization of several color SIFT features (van de\nSande et al., 2010), pooling with spatial pyramid match-\ning (Lazebnik et al., 2006), and classifying with a his-\ntogram intersection kernel SVM (Maji and Malik, 2009)\ntrained on a GPU (van de Sande et al., 2011a).\nILSVRC2012. This was a turning point for large-scale\nobject recognition, when large-scale deep neural net-\nworks entered the scene. The undisputed winner of both\nthe classiﬁcation and localization tasks in 2012 was the\nSuperVision team. They trained a large, deep convolu-\ntional neural network on RGB values, with 60 million\nparameters using an eﬃcient GPU implementation and\na novel hidden-unit dropout trick (Krizhevsky et al.,\n2012; Hinton et al., 2012). The second place in image\nclassiﬁcation went to the ISI team, which used Fisher\nvectors (Sanchez and Perronnin, 2011) and a stream-\nlined version of Graphical Gaussian Vectors (Harada\nand Kuniyoshi, 2012), along with linear classiﬁers us-\ning Passive-Aggressive (PA) algorithm (Crammer et al.,\n2006). The second place in single-object localization\nwent to the VGG, with an image classiﬁcation sys-\ntem including dense SIFT features and color statis-\ntics (Lowe, 2004), a Fisher vector representation (Sanchez\nand Perronnin, 2011), and a linear SVM classiﬁer, plus\nadditional insights from (Arandjelovic and Zisserman,\n2012; Sanchez et al., 2012). Both ISI and VGG used\n(Felzenszwalb et al., 2010) for object localization; Su-\nperVision used a regression model trained to predict\nbounding box locations. Despite the weaker detection\nmodel, SuperVision handily won the object localization\ntask. A detailed analysis and comparison of the Super-\nVision and VGG submissions on the single-object local-\nization task can be found in (Russakovsky et al., 2013).\nThe inﬂuence of the success of the SuperVision model\ncan be clearly seen in ILSVRC2013 and ILSVRC2014.\nILSVRC2013. There were 24 teams participating in the\nILSVRC2013 competition, compared to 21 in the pre-\nvious three years combined. Following the success of the\ndeep learning-based method in 2012, the vast majority\nof entries in 2013 used deep convolutional neural net-\nworks in their submission. The winner of the classiﬁca-\ntion task was Clarifai, with several large deep convolu-\ntional networks averaged together. The network archi-\ntectures were chosen using the visualization technique\nof (Zeiler and Fergus, 2013), and they were trained\non the GPU following (Zeiler et al., 2011) using the\ndropout technique (Krizhevsky et al., 2012).\nThe winning single-object localization OverFeat sub-\nmission was based on an integrated framework for us-\ning convolutional networks for classiﬁcation, localiza-\ntion and detection with a multiscale sliding window\napproach (Sermanet et al., 2013). They were the only\nteam tackling all three tasks.\nThe winner of object detection task was UvA team,\nwhich utilized a new way of eﬃcient encoding (van de\nSande et al., 2014) densely sampled color descriptors (van de\nSande et al., 2010) pooled using a multi-level spatial\npyramid in a selective search framework (Uijlings et al.,\n2013). The detection results were rescored using a full-\nimage convolutional network classiﬁer.\nILSVRC2014. 2014 attracted the most submissions, with\n36 teams submitting 123 entries compared to just 24\nteams in 2013 – a 1.5x increase in participation.9 As\nin 2013 almost all teams used convolutional neural net-\nworks as the basis for their submission. Signiﬁcant progress\nhas been made in just one year: image classiﬁcation er-\nror was almost halved since ILSVRC2013 and object\ndetection mean average precision almost doubled com-\npared to ILSVRC2013. Please refer to Section 6.1 for\ndetails.\nIn 2014 teams were allowed to use outside data for\ntraining their models in the competition, so there were\nsix tracks: provided and outside data tracks in each\nof image classiﬁcation, single-object localization, and\nobject detection tasks.\nThe winning image classiﬁcation with provided data\nteam was GoogLeNet, which explored an improved con-\nvolutional neural network architecture combining the\nmulti-scale idea with intuitions gained from the Heb-\nbian principle. Additional dimension reduction layers\nallowed them to increase both the depth and the width\n9 Table 7 omits 4 teams which submitted results but chose\nnot to oﬃcially participate in the challenge.\nImageNet Large Scale Visual Recognition Challenge\n19\nILSVRC 2010\nCodename\nCLS\nLOC\nInsitutions\nContributors and references\nHminmax\n54.4\nMassachusetts Institute of Technology\nJim Mutch, Sharat Chikkerur, Hristo Paskov, Ruslan Salakhutdinov, Stan Bileschi, Hueihan Jhuang\nIBM\n70.1\nIBM research†, Georgia Tech‡\nLexing Xie†, Hua Ouyang‡, Apostol Natsev†\nISIL\n44.6\nIntelligent Systems and Informatics Lab., The University of\nTokyo\nTatsuya Harada, Hideki Nakayama, Yoshitaka Ushiku, Yuya Yamashita, Jun Imura, Yasuo Kuniyoshi\nITNLP\n78.7\nHarbin Institute of Technology\nDeyuan Zhang, Wenfeng Xuan, Xiaolong Wang, Bingquan Liu, Chengjie Sun\nLIG\n60.7\nLaboratoire d’Informatique de Grenoble\nGeorges Qu´enot\nNEC\n28.2\nNEC\nLabs\nAmerica†,\nUniversity\nof\nIllinois\nat\nUrbana-\nChampaign‡, Rutgers∓\nYuanqing Lin†, Fengjun Lv†, Shenghuo Zhu†, Ming Yang†, Timothee Cour†, Kai Yu†, LiangLiang Cao‡,\nZhen Li‡, Min-Hsuan Tsai‡, Xi Zhou‡, Thomas Huang‡, Tong Zhang∓\n(Lin et al., 2011)\nNII\n74.2\nNational Institute of Informatics, Tokyo,Japan†, Hefei Nor-\nmal Univ. Heifei, China‡\nCai-Zhi Zhu†, Xiao Zhou‡, Shin´ıchi Satoh†\nNTU\n58.3\nCeMNet, SCE, NTU, Singapore\nZhengxiang Wang, Liang-Tien Chia\nRegularities\n75.1\nSRI International\nOmid Madani, Brian Burns\nUCI\n46.6\nUniversity of California Irvine\nHamed Pirsiavash, Deva Ramanan, Charless Fowlkes\nXRCE\n33.6\nXerox Research Centre Europe\nJorge Sanchez, Florent Perronnin, Thomas Mensink\n(Perronnin et al., 2010)\nILSVRC 2011\nCodename\nCLS\nLOC\nInstitutions\nContributors and references\nISI\n36.0\n-\nIntelligent Systems and Informatics lab, University of Tokyo\nTatsuya Harada, Asako Kanezaki, Yoshitaka Ushiku, Yuya Yamashita, Sho Inaba, Hiroshi Muraoka, Yasuo\nKuniyoshi\nNII\n50.5\n-\nNational Institute of Informatics, Japan\nDuy-Dinh Le, Shin´ıchi Satoh\nUvA\n31.0\n42.5\nUniversity of Amsterdam†, University of Trento‡\nKoen E. A. van de Sande†, Jasper R. R. Uijlings‡, Arnold W. M. Smeulders†, Theo Gevers†, Nicu Sebe‡,\nCees Snoek†\n(van de Sande et al., 2011b)\nXRCE\n25.8\n56.5\nXerox Research Centre Europe†, CIII‡\nFlorent Perronnin†, Jorge Sanchez†‡\n(Sanchez and Perronnin, 2011)\nILSVRC 2012\nCodename\nCLS\nLOC\nInstitutions\nContributors and references\nISI\n26.2\n53.6\nUniversity of Tokyo†, JST PRESTO‡\nNaoyuki\nGunji†,\nTakayuki\nHiguchi†,\nKoki\nYasumoto†,\nHiroshi\nMuraoka†,\nYoshitaka\nUshiku†,\nTatsuya\nHarada†‡, Yasuo Kuniyoshi†\n(Harada and Kuniyoshi, 2012)\nLEAR\n34.5\n-\nLEAR\nINRIA\nGrenoble†,\nTVPA\nXerox\nResearch\nCentre\nEurope‡\nThomas Mensink†‡, Jakob Verbeek†, Florent Perronnin‡, Gabriela Csurka‡\n(Mensink et al., 2012)\nVGG\n27.0\n50.0\nUniversity of Oxford\nKaren Simonyan, Yusuf Aytar, Andrea Vedaldi, Andrew Zisserman\n(Arandjelovic and Zisserman, 2012; Sanchez et al., 2012)\nSuperVision\n16.4\n34.2\nUniversity of Toronto\nAlex Krizhevsky, Ilya Sutskever, Geoffrey Hinton\n(Krizhevsky et al., 2012)\nUvA\n29.6\n-\nUniversity of Amsterdam\nKoen E. A. van de Sande, Amir Habibian, Cees G. M. Snoek\n(Sanchez and Perronnin, 2011; Scheirer et al., 2012)\nXRCE\n27.1\n-\nXerox Research Centre Europe†, LEAR INRIA ‡\nFlorent Perronnin†, Zeynep Akata†‡, Zaid Harchaoui‡, Cordelia Schmid‡\n(Perronnin et al., 2012)\nTable 5 Teams participating in ILSVRC2010-2012, ordered alphabetically. Each method is identiﬁed with a codename used in the text. We report ﬂat top-5 classiﬁcation\nand single-object localization error, in percents (lower is better). For teams which submitted multiple entries we report the best score. In 2012, SuperVision also submitted\nentries trained with the extra data from the ImageNet Fall 2011 release, and obtained 15.3% classiﬁcation error and 33.5% localization error. Key references are provided\nwhere available. More details about the winning entries can be found in Section 5.1.\n20\nOlga Russakovsky* et al.\nILSVRC 2013\nCodename\nCLS\nLOC\nDET\nInsitutions\nContributors and references\nAdobe\n15.2\n-\n-\nAdobe†, University of Illinois at Urbana-Champaign‡\nHailin Jin†, Zhe Lin†, Jianchao Yang†, Tom Paine‡\n(Krizhevsky et al., 2012)\nAHoward\n13.6\n-\n-\nAndrew Howard Consulting\nAndrew Howard\nBUPT\n25.2\n-\n-\nBeijing University of Posts and Telecommunications†, Orange Labs\nInternational Center Beijing‡\nChong Huang†, Yunlong Bian†, Hongliang Bai‡, Bo Liu†, Yanchao Feng†, Yuan Dong†\nClarifai\n11.7\n-\n-\nClarifai\nMatthew Zeiler\n(Zeiler and Fergus, 2013; Zeiler et al., 2011)\nCogVision\n16.1\n-\n-\nMicrosoft Research†, Harbin Institute of Technology‡\nKuiyuan Yang†, Yalong Bai†, Yong Rui‡\ndecaf\n19.2\n-\n-\nUniversity of California Berkeley\nYangqing Jia, Jeff Donahue, Trevor Darrell\n(Donahue et al., 2013)\nDeep Punx\n20.9\n-\n-\nSaint Petersburg State University\nEvgeny Smirnov, Denis Timoshenko, Alexey Korolev\n(Krizhevsky et al., 2012; Wan et al., 2013; Tang, 2013)\nDelta\n-\n-\n6.1\nNational Tsing Hua University\nChe-Rung Lee, Hwann-Tzong Chen, Hao-Ping Kang, Tzu-Wei Huang, Ci-Hong Deng, Hao-\nChe Kao\nIBM\n20.7\n-\n-\nUniversity of Illinois at Urbana-Champaign†, IBM Watson Re-\nsearch Center‡, IBM Haifa Research Center∓\nZhicheng Yan†, Liangliang Cao‡, John R Smith‡, Noel Codella‡,Michele Merler‡, Sharath\nPankanti‡, Sharon Alpert∓, Yochay Tzur∓,\nMIL\n24.4\n-\n-\nUniversity of Tokyo\nMasatoshi Hidaka, Chie Kamada, Yusuke Mukuta, Naoyuki Gunji, Yoshitaka Ushiku, Tat-\nsuya Harada\nMinerva\n21.7\nPeking\nUniversity†,\nMicrosoft\nResearch‡,\nShanghai\nJiao\nTong\nUniversity∓, XiDian University§, Harbin Institute of Technologyς\nTianjun Xiao†‡, Minjie Wang∓‡, Jianpeng Li§‡, Yalong Baiς ‡, Jiaxing Zhang‡, Kuiyuan\nYang‡, Chuntao Hong‡, Zheng Zhang‡\n(Wang et al., 2014)\nNEC\n-\n-\n19.6\nNEC Labs America†, University of Missouri ‡\nXiaoyu Wang†, Miao Sun‡, Tianbao Yang†, Yuanqing Lin†, Tony X. Han‡, Shenghuo Zhu†\n(Wang et al., 2013)\nNUS\n13.0\nNational University of Singapore\nMin Lin*, Qiang Chen*, Jian Dong, Junshi Huang, Wei Xia, Shuicheng Yan (* = equal\ncontribution)\n(Krizhevsky et al., 2012)\nOrange\n25.2\nOrange Labs International Center Beijing†, Beijing University of\nPosts and Telecommunications‡\nHongliang BAI†, Lezi Wang‡, Shusheng Cen‡, YiNan Liu‡, Kun Tao†, Wei Liu†, Peng Li†,\nYuan Dong†\nOverFeat\n14.2\n30.0\n(19.4)\nNew York University\nPierre Sermanet, David Eigen, Michael Mathieu, Xiang Zhang, Rob Fergus, Yann LeCun\n(Sermanet et al., 2013)\nQuantum\n82.0\n-\n-\nSelf-employed†, Student in Troy High School, Fullerton, CA‡\nHenry Shu†, Jerry Shu‡\n(Batra et al., 2013)\nSYSU\n-\n-\n10.5\nSun Yat-Sen University, China.\nXiaolong Wang\n(Felzenszwalb et al., 2010)\nToronto\n-\n-\n11.5\nUniversity of Toronto\nYichuan Tang*, Nitish Srivastava*, Ruslan Salakhutdinov (* = equal contribution)\nTrimps\n26.2\n-\n-\nThe Third Research Institute of the Ministry of Public Security,\nP.R. China\nJie Shao, Xiaoteng Zhang, Yanfeng Shang, Wenfei Wang, Lin Mei, Chuanping Hu\nUCLA\n-\n-\n9.8\nUniversity of California Los Angeles\nYukun Zhu, Jun Zhu, Alan Yuille\nUIUC\n-\n-\n1.0\nUniversity of Illinois at Urbana-Champaign\nThomas Paine, Kevin Shih, Thomas Huang\n(Krizhevsky et al., 2012)\nUvA\n14.3\n-\n22.6\nUniversity of Amsterdam, Euvision Technologies\nKoen E. A. van de Sande, Daniel H. F. Fontijne, Cees G. M. Snoek, Harro M. G. Stokman,\nArnold W. M. Smeulders\n(van de Sande et al., 2014)\nVGG\n15.2\n46.4\n-\nVisual Geometry Group, University of Oxford\nKaren Simonyan, Andrea Vedaldi, Andrew Zisserman\n(Simonyan et al., 2013)\nZF\n13.5\n-\n-\nNew York University\nMatthew D Zeiler, Rob Fergus\n(Zeiler and Fergus, 2013; Zeiler et al., 2011)\nTable 6 Teams participating in ILSVRC2013, ordered alphabetically. Each method is identiﬁed with a codename used in the text. For classiﬁcaton and single-object\nlocalization we report ﬂat top-5 error, in percents (lower is better). For detection we report mean average precision, in percents (higher is better). Even though the winner\nof the challenge was determined by the number of object categories won, this correlated strongly with mAP. Parentheses indicate the team used outside training data\nand was not part of the oﬃcial competition. Some competing teams also submitted entries trained with outside data: Clarifai with 11.2% classiﬁcation error, NEC with\n20.9% detection mAP. Key references are provided where available. More details about the winning entries can be found in Section 5.1.\nImageNet Large Scale Visual Recognition Challenge\n21\nILSVRC 2014\nCodename\nCLS\nCLSo LOC\nLOCo DET\nDETo Insitutions\nContributors and references\nAdobe\n-\n11.6\n-\n30.1\n-\n-\nAdobe†, UIUC‡\nHailin Jin†, Zhaowen Wang‡, Jianchao Yang†, Zhe Lin†\nAHoward\n8.1\n-\n◦\n-\n-\n-\nHoward Vision Technologies\nAndrew Howard (Howard, 2014)\nBDC\n11.3\n-\n◦\n-\n-\n-\nInstitute for Infocomm Research†, Uni-\nversit Pierre et Marie Curie‡\nOlivier Morre†‡, Hanlin Goh†, Antoine Veillard‡, Vijay Chandrasekhar†(Krizhevsky et al., 2012)\nBerkeley\n-\n-\n-\n-\n-\n34.5\nUC Berkeley\nRoss Girshick, Jeff Donahue, Sergio Guadarrama, Trevor Darrell, Jitendra Malik (Girshick et al., 2013,\n2014)\nBREIL\n16.0\n-\n◦\n-\n-\n-\nKAIST department of EE\nJun-Cheol Park, Yunhun Jang, Hyungwon Choi, JaeYoung Jun (Chatfield et al., 2014; Jia, 2013)\nBrno\n17.6\n-\n52.0\n-\n-\n-\nBrno University of Technology\nMartin Kol´aˇr, Michal Hradiˇs, Pavel Svoboda (Krizhevsky et al., 2012; Mikolov et al., 2013; Jia, 2013)\nCASIA-2\n-\n-\n-\n-\n28.6\n-\nChinese Academy of Science†, South-\neast University‡\nPeihao\nHuang†,\nYongzhen\nHuang†,\nFeng\nLiu‡,\nZifeng\nWu†,\nFang\nZhao†,\nLiang\nWang†,\nTieniu\nTan†(Girshick et al., 2014)\nCASIAWS\n-\n11.4\n-\n◦\n-\n-\nCRIPAC, CASIA\nWeiqiang Ren, Chong Wang, Yanhua Chen, Kaiqi Huang, Tieniu Tan (Arbel´aez et al., 2014)\nCldi\n13.9\n-\n46.9\n-\n-\n-\nKAIST†, Cldi Inc.‡\nKyunghyun Paeng†, Donggeun Yoo†, Sunggyun Park†, Jungin Lee‡, Anthony S. Paek‡, In So Kweon†,\nSeong Dae Kim†(Krizhevsky et al., 2012; Perronnin et al., 2010)\nCUHK\n-\n-\n-\n-\n-\n40.7\nThe Chinese University of Hong Kong\nWanli Ouyang, Ping Luo, Xingyu Zeng, Shi Qiu, Yonglong Tian, Hongsheng Li, Shuo Yang, Zhe Wang,\nYuanjun Xiong, Chen Qian, Zhenyao Zhu, Ruohui Wang, Chen-Change Loy, Xiaogang Wang, Xiaoou\nTang (Ouyang et al., 2014; Ouyang and Wang, 2013)\nDeepCNet\n17.5\n-\n◦\n-\n-\n-\nUniversity of Warwick\nBen Graham (Graham, 2013; Schmidhuber, 2012)\nDeepInsight\n-\n-\n-\n-\n-\n40.5\nNLPR†, HKUST‡\nJunjie Yan†, Naiyan Wang‡, Stan Z. Li†, Dit-Yan Yeung‡(Girshick et al., 2014)\nFengjunLv\n17.4\n-\n◦\n-\n-\n-\nFengjun Lv Consulting\nFengjun Lv (Krizhevsky et al., 2012; Harel et al., 2007)\nGoogLeNet\n6.7\n-\n26.4\n-\n-\n43.9\nGoogle\nChristian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Drago Anguelov, Dumitru Erhan,\nAndrew Rabinovich (Szegedy et al., 2014)\nHKUST\n-\n-\n-\n-\n28.9\n-\nHong Kong U. of Science and Tech.†,\nChinese U. of H. K.‡, Stanford U.∓\nCewu Lu†, Hei Law*†, Hao Chen*‡, Qifeng Chen*∓, Yao Xiao*†Chi Keung Tang†(Uijlings et al., 2013;\nGirshick et al., 2013; Perronnin et al., 2010; Felzenszwalb et al., 2010)\nlibccv\n16.0\n-\n◦\n-\n-\n-\nlibccv.org\nLiu Liu (Zeiler and Fergus, 2013)\nMIL\n18.3\n-\n33.7\n-\n-\n30.4\nThe\nUniversity\nof\nTokyo†,\nIIT\nGuwahati‡\nSenthil\nPurushwalkam†‡,\nYuichiro\nTsuchiya†,\nAtsushi\nKanehira†,\nAsako\nKanezaki†,\nTatsuya\nHarada†(Kanezaki et al., 2014; Girshick et al., 2013)\nMPG UT\n-\n-\n-\n-\n-\n26.4\nThe University of Tokyo\nRiku Togashi, Keita Iwamoto, Tomoaki Iwase, Hideki Nakayama (Girshick et al., 2014)\nMSRA\n8.1\n-\n35.5\n-\n35.1\n-\nMicrosoft\nResearch†,\nXi’an\nJiaotong\nU.‡, U. of Science and Tech. of China∓\nKaiming He†, Xiangyu Zhang‡, Shaoqing Ren∓, Jian Sun†(He et al., 2014)\nNUS\n-\n-\n-\n-\n37.2\n-\nNational\nUniversity\nof\nSingapore†,\nIBM Research Australia‡\nJian Dong†, Yunchao Wei†, Min Lin†, Qiang Chen‡, Wei Xia†, Shuicheng Yan†(Lin et al., 2014a; Chen\net al., 2014)\nNUS-BST\n9.8\n-\n◦\n-\n-\n-\nNational Univ. of Singapore†, Beijing\nSamsung Telecom R&D Center†\nMin Lin†, Jian Dong†, Hanjiang Lai†, Junjun Xiong‡, Shuicheng Yan†(Lin et al., 2014a; Howard, 2014;\nKrizhevsky et al., 2012)\nOrange\n15.2\n14.8\n42.8\n42.7\n-\n27.7\nOrange Labs Beijing†, BUPT China‡\nHongliang Bai†, Yinan Liu†, Bo Liu‡, Yanchao Feng‡, Kun Tao†, Yuan Dong†(Girshick et al., 2014)\nPassBy\n16.7\n-\n◦\n-\n-\n-\nLENOVO†, HKUST‡, U. of Macao∓\nLin Sun†‡, Zhanghui Kuang†, Cong Zhao†, Kui Jia∓, Oscar C.Au‡(Jia, 2013; Krizhevsky et al., 2012)\nSCUT\n18.8\n-\n◦\n-\n-\n-\nSouth China Univ. of Technology\nGuo Lihua, Liao Qijun, Ma Qianli, Lin Junbin\nSoutheast\n-\n-\n-\n-\n30.5\n-\nSoutheast U.†, Chinese A. of Sciences‡\nFeng Liu†, Zifeng Wu‡, Yongzhen Huang‡\nSYSU\n14.4\n-\n31.9\n-\n-\n-\nSun Yat-Sen University\nLiliang Zhang, Tianshui Chen, Shuye Zhang, Wanglan He, Liang Lin, Dengguang Pang, Lingbo Liu\nTrimps\n-\n11.5\n-\n42.2\n-\n33.7\nThe Third Research Institute of the\nMinistry of Public Security\nJie Shao, Xiaoteng Zhang, JianYing Zhou, Jian Wang, Jian Chen, Yanfeng Shang, Wenfei Wang, Lin\nMei, Chuanping Hu (Girshick et al., 2014; Manen et al., 2013; Howard, 2014)\nTTIC\n10.2\n-\n48.3\n-\n-\n-\nToyota\nTechnological\nInstitute\nat\nChicago†, Ecole Centrale Paris‡\nGeorge Papandreou†, Iasonas Kokkinos‡(Papandreou, 2014; Papandreou et al., 2014; Jojic et al., 2003;\nKrizhevsky et al., 2012; Sermanet et al., 2013; Dubout and Fleuret, 2012; Iandola et al., 2014)\nUI\n99.5\n-\n◦\n-\n-\n-\nUniversity of Isfahan\nFatemeh Shafizadegan, Elham Shabaninia (Yang et al., 2009)\nUvA\n12.1\n-\n◦\n-\n32.0\n35.4\nU. of Amsterdam and Euvision Tech.\nKoen van de Sande, Daniel Fontijne, Cees Snoek, Harro Stokman, Arnold Smeulders (van de Sande et al.,\n2014)\nVGG\n7.3\n-\n25.3\n-\n-\n-\nUniversity of Oxford\nKaren Simonyan, Andrew Zisserman (Simonyan and Zisserman, 2014)\nXYZ\n11.2\n-\n◦\n-\n-\n-\nThe University of Queensland\nZhongwen Xu and Yi Yang (Krizhevsky et al., 2012; Jia, 2013; Zeiler and Fergus, 2013; Lin et al., 2014a)\nTable 7\nTeams participating in ILSVRC2014, ordered alphabetically. Each method is identiﬁed with a codename used in the text. For classiﬁcaton and single-object\nlocalization we report ﬂat top-5 error, in percents (lower is better). For detection we report mean average precision, in percents (higher is better). CLSo,LOCo,DETo\ncorresponds to entries using outside training data (oﬃcially allowed in ILSVRC2014). ◦means localization error greater than 60% (localization submission was required\nwith every classiﬁcation submission). Key references are provided where available. More details about the winning entries can be found in Section 5.1.\n22\nOlga Russakovsky* et al.\nof the network signiﬁcantly without incurring signiﬁ-\ncant computational overhead. In the image classiﬁca-\ntion with external data track, CASIAWS won by using\nweakly supervised object localization from only clas-\nsiﬁcation labels to improve image classiﬁcation. MCG\nregion proposals (Arbel´aez et al., 2014) pretrained on\nPASCAL VOC 2012 data are used to extract region\nproposals, regions are represented using convolutional\nnetworks, and a multiple instance learning strategy is\nused to learn weakly supervised object detectors to rep-\nresent the image.\nIn the single-object localization with provided data\ntrack, the winning team was VGG, which explored the\neﬀect of convolutional neural network depth on its ac-\ncuracy by using three diﬀerent architectures with up to\n19 weight layers with rectiﬁed linear unit non-linearity,\nbuilding oﬀof the implementation of Caﬀe (Jia, 2013).\nFor localization they used per-class bounding box re-\ngression similar to OverFeat (Sermanet et al., 2013). In\nthe single-object localization with external data track,\nAdobe used 2000 additional ImageNet classes to train\nthe classiﬁers in an integrated convolutional neural net-\nwork framework for both classiﬁcation and localization,\nwith bounding box regression. At test time they used\nk-means to ﬁnd bounding box clusters and rank the\nclusters according to the classiﬁcation scores.\nIn the object detection with provided data track, the\nwinning team NUS used the RCNN framework (Gir-\nshick et al., 2013) with the network-in-network method\n(Lin et al., 2014a) and improvements of (Howard, 2014).\nGlobal context information was incorporated follow-\ning (Chen et al., 2014). In the object detection with\nexternal data track, the winning team was GoogLeNet\n(which also won image classiﬁcation with provided data).\nIt is truly remarkable that the same team was able to\nwin at both image classiﬁcation and object detection,\nindicating that their methods are able to not only clas-\nsify the image based on scene information but also accu-\nrately localize multiple object instances. Just like most\nteams participating in this track, GoogLeNet used the\nimage classiﬁcation dataset as extra training data.\n5.2 Large scale algorithmic innovations\nILSVRC over the past ﬁve years has paved the way for\nseveral breakthroughs in computer vision.\nThe ﬁeld of categorical object recognition has dra-\nmatically evolved in the large-scale setting. Section 5.1\ndocuments the progress, starting from coded SIFT fea-\ntures and evolving to large-scale convolutional neural\nnetworks dominating at all three tasks of image classiﬁ-\ncation, single-object localization, and object detection.\nWith the availability of so much training data (along\nwith an eﬃcient algorithmic implementation and GPU\ncomputing resources) it became possible to learn neural\nnetworks directly from the image data, without need-\ning to create multi-stage hand-tuned pipelines of ex-\ntracted features and discriminative classiﬁers. The ma-\njor breakthrough came in 2012 with the win of the Su-\nperVision team on image classiﬁcation and single-object\nlocalization tasks (Krizhevsky et al., 2012), and by 2014\nall of the top contestants were relying heavily on con-\nvolutional neural networks.\nFurther, over the past few years there has been a\nlot of focus on large-scale recognition in the computer\nvision community . Best paper awards at top vision con-\nferences in 2013 were awarded to large-scale recognition\nmethods: at CVPR 2013 to ”Fast, Accurate Detection\nof 100,000 Object Classes on a Single Machine” (Dean\net al., 2013) and at ICCV 2013 to ”From Large Scale\nImage Categorization to Entry-Level Categories” (Or-\ndonez et al., 2013). Additionally, several inﬂuential lines\nof research have emerged, such as large-scale weakly\nsupervised localization work of (Kuettel et al., 2012)\nwhich was awarded the best paper award in ECCV 2012\nand large-scale zero-shot learning, e.g., (Frome et al.,\n2013).\n6 Results and analysis\n6.1 Improvements over the years\nState-of-the-art accuracy has improved signiﬁcantly from\nILSVRC2010 to ILSVRC2014, showcasing the massive\nprogress that has been made in large-scale object recog-\nnition over the past ﬁve years. The performance of the\nwinning ILSVRC entries for each task and each year are\nshown in Figure 9. The improvement over the years is\nclearly visible. In this section we quantify and analyze\nthis improvement.\n6.1.1 Image classiﬁcation and single-object localization\nimprovement over the years\nThere has been a 4.2x reduction in image classiﬁcation\nerror (from 28.2% to 6.7%) and a 1.7x reduction in\nsingle-object localization error (from 42.5% to 25.3%)\nsince the beginning of the challenge. For consistency,\nhere we consider only teams that use the provided train-\ning data. Even though the exact object categories have\nchanged (Section 3.1.1), the large scale of the dataset\nhas remained the same (Table 2), making the results\ncomparable across the years. The dataset has not changed\nsince 2012, and there has been a 2.4x reduction in image\nclassiﬁcation error (from 16.4% to 6.7%) and a 1.3x in\nImageNet Large Scale Visual Recognition Challenge\n23\nFig. 9 Performance of winning entries in the ILSVRC2010-\n2014 competitions in each of the three tasks (details about\nthe entries and numerical results are in Section 5.1). There is\na steady reduction of error every year in object classiﬁcation\nand single-object localization tasks, and a 1.9x improvement\nin mean average precision in object detection. There are two\nconsiderations in making these comparisons. (1) The object\ncategories used in ISLVRC changed between years 2010 and\n2011, and between 2011 and 2012. However, the large scale\nof the data (1000 object categories, 1.2 million training im-\nages) has remained the same, making it possible to compare\nresults. Image classiﬁcation and single-object localization en-\ntries shown here use only provided training data. (2) The\nsize of the object detection training data has increased signif-\nicantly between years 2013 and 2014 (Section 3.3). Section 6.1\ndiscusses the relative eﬀects of training data increase versus\nalgorithmic improvements.\nsingle-object localization error (from 33.5% to 25.3%)\nin the past three years.\n6.1.2 Object detection improvement over the years\nObject detection accuracy as measured by the mean\naverage precision (mAP) has increased 1.9x since the in-\ntroduction of this task, from 22.6% mAP in ILSVRC2013\nto 43.9% mAP in ILSVRC2014. However, these results\nare not directly comparable for two reasons. First, the\nsize of the object detection training data has increased\nsigniﬁcantly from 2013 to 2014 (Section 3.3). Second,\nthe 43.9% mAP result was obtained with the addition\nof the image classiﬁcation and single-object localiza-\ntion training data. Here we attempt to understand the\nrelative eﬀects of the training set size increase versus\nalgorithmic improvements. All models are evaluated on\nthe same ILSVRC2013-2014 object detection test set.\nFirst, we quantify the eﬀects of increasing detec-\ntion training data between the two challenges by com-\nparing the same model trained on ILSVRC2013 de-\ntection data versus ILSVRC2014 detection data. The\nUvA team’s framework from 2013 achieved 22.6% with\nILSVRC2013 data (Table 6) and 26.3% with ILSVRC2014\ndata and no other modiﬁcations.10 The absolute in-\ncrease in mAP was 3.7%. The RCNN model achieved\n31.4% mAP with ILSVRC2013 detection plus image\nclassiﬁcation data (Girshick et al., 2013) and 34.5%\nmAP with ILSVRC2014 detection plus image classiﬁ-\ncation data (Berkeley team in Table 7). The absolute\n10 Personal communication with members of the UvA team.\nincrease in mAP by expanding ILSVRC2013 detection\ndata to ILSVRC2014 was 3.1%.\nSecond, we quantify the eﬀects of adding in the ex-\nternal data for training object detection models. The\nNEC model in 2013 achieved 19.6% mAP trained on\nILSVRC2013 detection data alone and 20.9% mAP trained\non ILSVRC2013 detection plus classiﬁcation data (Ta-\nble 6). The absolute increase in mAP was 1.3%. The\nUvA team’s best entry in 2014 achieved 32.0% mAP\ntrained on ILSVRC2014 detection data and 35.4% mAP\ntrained on ILSVRC2014 detection plus classiﬁcation\ndata. The absolute increase in mAP was 3.4%.\nThus, we conclude based on the evidence so far\nthat expanding the ILSVRC2013 detection set to the\nILSVRC2014 set, as well as adding in additional train-\ning data from the classiﬁcation task, all account for\napproximately 1 −4% in absolute mAP improvement\nfor the models. For comparison, we can also attempt\nto quantify the eﬀect of algorithmic innovation. The\nUvA team’s 2013 framework achieved 26.3% mAP on\nILSVRC2014 data as mentioned above, and their im-\nproved method in 2014 obtained 32.0% mAP (Table 7).\nThis is 5.8% absolute increase in mAP over just one\nyear from algorithmic innovation alone.\nIn summary, we conclude that the absolute 21.3%\nincrease in mAP between winning entries of ILSVRC2013\n(22.6% mAP) and of ILSVRC2014 (43.9% mAP) is\nthe result of impressive algorithmic innovation and not\njust a consequence of increased training data. However,\nincreasing the ISLVRC2014 object detection training\ndataset further is likely to produce additional improve-\nments in detection accuracy for current algorithms.\n6.2 Statistical signiﬁcance\nOne important question to ask is whether results of dif-\nferent submissions to ILSVRC are statistically signiﬁ-\ncantly diﬀerent from each other. Given the large scale,\nit is no surprise that even minor diﬀerences in accuracy\nare statistically signiﬁcant; we seek to quantify exactly\nhow much of a diﬀerence is enough.\nFollowing the strategy employed by PASCAL VOC\n(Everingham et al., 2014), for each method we obtain\na conﬁdence interval of its score using bootstrap sam-\npling. During each bootstrap round, we sample N im-\nages with replacement from all the available N test\nimages and evaluate the performance of the algorithm\non those sampled images. This can be done very eﬃ-\nciently by precomputing the accuracy on each image.\nGiven the results of all the bootstrapping rounds we\ndiscard the lower and the upper α fraction. The range\nof the remaining results represents the 1 −2α conﬁ-\n24\nOlga Russakovsky* et al.\nImage classiﬁcation\nYear\nCodename\nError (percent)\n99.9% Conf Int\n2014\nGoogLeNet\n6.66\n6.40 - 6.92\n2014\nVGG\n7.32\n7.05 - 7.60\n2014\nMSRA\n8.06\n7.78 - 8.34\n2014\nAHoward\n8.11\n7.83 - 8.39\n2014\nDeeperVision\n9.51\n9.21 - 9.82\n2013\nClarifai†\n11.20\n10.87 - 11.53\n2014\nCASIAWS†\n11.36\n11.03 - 11.69\n2014\nTrimps†\n11.46\n11.13 - 11.80\n2014\nAdobe†\n11.58\n11.25 - 11.91\n2013\nClarifai\n11.74\n11.41 - 12.08\n2013\nNUS\n12.95\n12.60 - 13.30\n2013\nZF\n13.51\n13.14 - 13.87\n2013\nAHoward\n13.55\n13.20 - 13.91\n2013\nOverFeat\n14.18\n13.83 - 14.54\n2014\nOrange†\n14.80\n14.43 - 15.17\n2012\nSuperVision†\n15.32\n14.94 - 15.69\n2012\nSuperVision\n16.42\n16.04 - 16.80\n2012\nISI\n26.17\n25.71 - 26.65\n2012\nVGG\n26.98\n26.53 - 27.43\n2012\nXRCE\n27.06\n26.60 - 27.52\n2012\nUvA\n29.58\n29.09 - 30.04\nSingle-object localization\nYear\nCodename\nError (percent)\n99.9% Conf Int\n2014\nVGG\n25.32\n24.87 - 25.78\n2014\nGoogLeNet\n26.44\n25.98 - 26.92\n2013\nOverFeat\n29.88\n29.38 - 30.35\n2014\nAdobe†\n30.10\n29.61 - 30.58\n2014\nSYSU\n31.90\n31.40 - 32.40\n2012\nSuperVision†\n33.55\n33.05 - 34.04\n2014\nMIL\n33.74\n33.24 - 34.25\n2012\nSuperVision\n34.19\n33.67 - 34.69\n2014\nMSRA\n35.48\n34.97 - 35.99\n2014\nTrimps†\n42.22\n41.69 - 42.75\n2014\nOrange†\n42.70\n42.18 - 43.24\n2013\nVGG\n46.42\n45.90 - 46.95\n2012\nVGG\n50.03\n49.50 - 50.57\n2012\nISI\n53.65\n53.10 - 54.17\n2014\nCASIAWS†\n61.96\n61.44 - 62.48\nObject detection\nYear\nCodename\nAP (percent)\n99.9% Conf Int\n2014\nGoogLeNet†\n43.93\n42.92 - 45.65\n2014\nCUHK†\n40.67\n39.68 - 42.30\n2014\nDeepInsight†\n40.45\n39.49 - 42.06\n2014\nNUS\n37.21\n36.29 - 38.80\n2014\nUvA†\n35.42\n34.63 - 36.92\n2014\nMSRA\n35.11\n34.36 - 36.70\n2014\nBerkeley†\n34.52\n33.67 - 36.12\n2014\nUvA\n32.03\n31.28 - 33.49\n2014\nSoutheast\n30.48\n29.70 - 31.93\n2014\nHKUST\n28.87\n28.03 - 30.20\n2013\nUvA\n22.58\n22.00 - 23.82\n2013\nNEC†\n20.90\n20.40 - 22.15\n2013\nNEC\n19.62\n19.14 - 20.85\n2013\nOverFeat†\n19.40\n18.82 - 20.61\n2013\nToronto\n11.46\n10.98 - 12.34\n2013\nSYSU\n10.45\n10.04 - 11.32\n2013\nUCLA\n9.83\n9.48 - 10.77\nTable 8 We use bootstrapping to construct 99.9% conﬁ-\ndence intervals around the result of up to top 5 submissions\nto each ILSVRC task in 2012-2014. †means the entry used\nexternal training data. The winners using the provided data\nfor each track and each year are bolded. The diﬀerence be-\ntween the winning method and the runner-up each year is\nsigniﬁcant even at the 99.9% level.\nFig. 10 For each object class, we consider the best perfor-\nmance of any entry submitted to ILSVRC2012-2014, includ-\ning entries using additional training data. The plots show\nthe distribution of these “optimistic” per-class results. Perfor-\nmance is measured as accuracy for image classiﬁcation (left)\nand for single-object localization (middle), and as average\nprecision for object detection (right). While the results are\nvery promising in image classiﬁcation, the ILSVRC datasets\nare far from saturated: many object classes continue to be\nchallenging for current algorithms.\ndence interval. We run a large number of bootstrap-\nping rounds (from 20,000 until convergence). Table 8\nshows the results of the top entries to each task of\nILSVRC2012-2014. The winning methods are statis-\ntically signiﬁcantly diﬀerent from the other methods,\neven at the 99.9% level.\n6.3 Current state of categorical object recognition\nBesides looking at just the average accuracy across hun-\ndreds of object categories and tens of thousands of im-\nages, we can also delve deeper to understand where\nmistakes are being made and where researchers’ eﬀorts\nshould be focused to expedite progress.\nTo do so, in this section we will be analyzing an\n“optimistic” measurement of state-of-the-art recogni-\ntion performance instead of focusing on the diﬀerences\nin individual algorithms. For each task and each object\nclass, we compute the best performance of any entry\nsubmitted to any ILSVRC2012-2014, including meth-\nods using additional training data. Since the test sets\nhave remained the same, we can directly compare all\nthe entries in the past three years to obtain the most\n“optimistic” measurement of state-of-the-art accuracy\non each category.\nFor consistency with the object detection metric\n(higher is better), in this section we will be using image\nclassiﬁcation and single-object localization accuracy in-\nstead of error, where accuracy = 1 −error.\n6.3.1 Range of accuracy across object classes\nFigure 10 shows the distribution of accuracy achieved\nby the “optimistic” models across the object categories.\nThe image classiﬁcation model achieves 94.6% accu-\nracy on average (or 5.4% error), but there remains a\n41.0% absolute diﬀerence inaccuracy between the most\nImageNet Large Scale Visual Recognition Challenge\n25\nand least accurate object class. The single-object local-\nization model achieves 81.5% accuracy on average (or\n18.5% error), with a 77.0% range in accuracy across\nthe object classes. The object detection model achieves\n44.7% average precision, with an 84.7% range across the\nobject classes. It is clear that the ILSVRC dataset is far\nfrom saturated: performance on many categories has re-\nmained poor despite the strong overall performance of\nthe models.\n6.3.2 Qualitative examples of easy and hard classes\nFigures 11 and 12 show the easiest and hardest classes\nfor each task, i.e., classes with the best and worst results\nobtained with the “optimistic” models.\nFor image classiﬁcation, 121 out of 1000 object classes\nhave 100% image classiﬁcation accuracy according to\nthe optimistic estimate. Figure 11 (top) shows a ran-\ndom set of 10 of them. They contain a variety of classes,\nsuch as mammals like “red fox” and animals with dis-\ntinctive structures like “stingray”. The hardest classes\nin the image classiﬁcation task, with accuracy as low as\n59.0%, include metallic and see-through man-made ob-\njects, such as “hook” and “water bottle,” the material\n“velvet” and the highly varied scene class “restaurant.”\nFor single-object localization, the 10 easiest classes\nwith 99.0 −100% accuracy are all mammals and birds.\nThe hardest classes include metallic man-made objects\nsuch as “letter opener” and “ladle”, plus thin structures\nsuch as “pole” and “spacebar” and highly varied classes\nsuch as “wing”. The most challenging class “spacebar”\nhas a only 23.0% localization accuracy.\nObject detection results are shown in Figure 12. The\neasiest classes are living organisms such as “dog” and\n“tiger”, plus “basketball” and “volleyball” with distinc-\ntive shape and color, and a somewhat surprising “snow-\nplow.” The easiest class “butterﬂy” is not yet perfectly\ndetected but is very close with 92.7% AP. The hard-\nest classes are as expected small thin objects such as\n“ﬂute” and “nail”, and the highly varied “lamp” and\n“backpack” classes, with as low as 8.0% AP.\n6.3.3 Per-class accuracy as a function of image\nproperties\nWe now take a closer look at the image properties to\ntry to understand why current algorithms perform well\non some object classes but not others. One hypothesis\nis that variation in accuracy comes from the fact that\ninstances of some classes tend to be much smaller in\nimages than instances of other classes, and smaller ob-\njects may be harder for computers to recognize. In this\nsection we argue that while accuracy is correlated with\nobject scale in the image, not all variation in accuracy\ncan be accounted for by scale alone.\nFor every object class, we compute its average scale,\nor the average fraction of image area occupied by an in-\nstance of the object class on the ILSVRC2012-2014 val-\nidation set. Since the images and object classes in the\nimage classiﬁcation and single-object localization tasks\nare the same, we use the bounding box annotations of\nthe single-object localization dataset for both tasks. In\nthat dataset the object classes range from “swimming\ntrunks” with scale of 1.5% to “spider web” with scale\nof 85.6%. In the object detection validation dataset\nthe object classes range from “sunglasses” with scale\nof 1.3% to “sofa” with scale of 44.4%.\nFigure 13 shows the performance of the “optimistic”\nmethod as a function of the average scale of the object\nin the image. Each dot corresponds to one object class.\nWe observe a very weak positive correlation between ob-\nject scale and image classiﬁcation accuracy: ρ = 0.14.\nFor single-object localization and object detection the\ncorrelation is stronger, at ρ = 0.40 and ρ = 0.41 re-\nspectively. It is clear that not all variation in accuracy\ncan be accounted for by scale alone. Nevertheless, in\nthe next section we will normalize for object scale to\nensure that this factor is not aﬀecting our conclusions.\n6.3.4 Per-class accuracy as a function of object\nproperties.\nBesides considering image-level properties we can also\nobserve how accuracy changes as a function of intrin-\nsic object properties. We deﬁne three properties in-\nspired by human vision: the real-world size of the ob-\nject, whether it’s deformable within instance, and how\ntextured it is. For each property, the object classes are\nassigned to one of a few bins (listed below). These prop-\nerties are illustrated in Figure 1.\nHuman subjects annotated each of the 1000 im-\nage classiﬁcation and single-object localization object\nclasses from ILSVRC2012-2014 with these properties. (Rus-\nsakovsky et al., 2013). By construction (see Section 3.3.1),\neach of the 200 object detection classes is either also\none of 1000 object classes or is an ancestor of one or\nmore of the 1000 classes in the ImageNet hierarchy. To\ncompute the values of the properties for each object de-\ntection class, we simply average the annotated values of\nthe descendant classes.\nIn this section we draw the following conclusions\nabout state-of-the-art recognition accuracy as a func-\ntion of these object properties:\n– Real-world size: XS for extra small (e.g. nail),\nsmall (e.g. fox), medium (e.g. bookcase), large (e.g.\ncar) or XL for extra large (e.g. church)\n26\nOlga Russakovsky* et al.\nImage classiﬁcation\nEasiest classes\nHardest classes\nSingle-object localization\nEasiest classes\nHardest classes\nFig. 11 For each object category, we take the best performance of any entry submitted to ILSVRC2012-2014 (including\nentries using additional training data). Given these “optimistic” results we show the easiest and harder classes for each task.\nThe numbers in parentheses indicate classiﬁcation and localization accuracy. For image classiﬁcation the 10 easiest classes are\nrandomly selected from among 121 object classes with 100% accuracy. Object detection results are shown in Figure 12.\nImageNet Large Scale Visual Recognition Challenge\n27\nObject detection\nEasiest classes\nHardest classes\nFig. 12 For each object category, we take the best performance of any entry submitted to ILSVRC2012-2014 (including entries\nusing additional training data). Given these “optimistic” results we show the easiest and harder classes for the object detection\ntask, i.e., classes with best and worst results. The numbers in parentheses indicate average precision. Image classiﬁcation and\nsingle-object localization results are shown in Figure 11.\nFig. 13 Performance of the “optimistic” method as a function of object scale in the image, on each task. Each dot corresponds\nto one object class. Average scale (x-axis) is computed as the average fraction of the image area occupied by an instance of\nthat object class on the ILSVRC2014 validation set. “Optimistic” performance (y-axis) corresponds to the best performance\non the test set of any entry submitted to ILSVRC2012-2014 (including entries with additional training data). The test set\nhas remained the same over these three years. We see that accuracy tends to increase as the objects get bigger in the image.\nHowever, it is clear that far from all the variation in accuracy on these classes can be accounted for by scale alone.\n28\nOlga Russakovsky* et al.\nThe image classiﬁcation and single-object localiza-\ntion “optimistic” models performs better on large\nand extra large real-world objects than on smaller\nones. The “optimistic” object detection model sur-\nprisingly performs better on extra small objects than\non small or medium ones.\n– Deformability within instance: Rigid (e.g., mug)\nor deformable (e.g., water snake)\nThe “optimistic” model on each of the three tasks\nperforms statistically signiﬁcantly better on deformable\nobjects compared to rigid ones. However, this ef-\nfect disappears when analyzing natural objects sep-\narately from man-made objects.\n– Amount of texture: none (e.g. punching bag), low\n(e.g. horse), medium (e.g. sheep) or high (e.g. hon-\neycomb)\nThe “optimistic” model on each of the three tasks\nis signiﬁcantly better on objects with at least low\nlevel of texture compared to untextured objects.\nThese and other ﬁndings are justiﬁed and discussed in\ndetail below.\nExperimental setup. We observed in Section 6.3.3 that\nobjects that occupy a larger area in the image tend to\nbe somewhat easier to recognize. To make sure that\ndiﬀerences in object scale are not inﬂuencing results in\nthis section, we normalize each bin by object scale. We\ndiscard object classes with the largest scales from each\nbin as needed until the average object scale of object\nclasses in each bin across one property is the same (or\nas close as possible). For real-world size property for\nexample, the resulting average object scale in each of\nthe ﬁve bins is 31.6%−31.7% in the image classiﬁcation\nand single-object localization tasks, and 12.9%−13.4%\nin the object detection task.11\nFigure 14 shows the average performance of the “op-\ntimistic” model on the object classes that fall into each\nbin for each property. We analyze the results in detail\nbelow. Unless otherwise speciﬁed, the reported accura-\ncies below are after the scale normalization step.\nTo evaluate statistical signiﬁcance, we compute the\n95% conﬁdence interval for accuracy using bootstrap-\nping: we repeatedly sample the object classes within\nthe bin with replacement, discard some as needed to\nnormalize by scale, and compute the average accuracy\nof the “optimistic” model on the remaining classes. We\nreport the 95% conﬁdence intervals (CI) in parentheses.\n11 For rigid versus deformable objects, the average scale in\neach bin is 34.1% −34.2% for classiﬁcation and localization,\nand 13.5%−13.7% for detection. For texture, the average scale\nin each of the four bins is 31.1%−31.3% for classiﬁcation and\nlocalization, and 12.7% −12.8% for detection.\nReal-world size. In Figure 14(top, left) we observe that\nin the image classiﬁcation task the “optimistic” model\ntends to perform signiﬁcantly better on objects which\nare larger in the real-world. The classiﬁcation accuracy\nis 93.6% −93.9% on XS, S and M objects compared to\n97.0% on L and 96.4% on XL objects. Since this is after\nnormalizing for scale and thus can’t be explained by\nthe objects’ size in the image, we conclude that either\n(1) larger real-world objects are easier for the model to\nrecognize, or (2) larger real-world objects usually occur\nin images with very distinctive backgrounds.\nTo distinguish between the two cases we look Fig-\nure 14(top, middle). We see that in the single-object\nlocalization task, the L objects are easy to localize at\n82.4% localization accuracy. XL objects, however, tend\nto be the hardest to localize with only 73.4% localiza-\ntion accuracy. We conclude that the appearance of L\nobjects must be easier for the model to learn, while\nXL objects tend to appear in distinctive backgrounds.\nThe image background make these XL classes easier for\nthe image-level classiﬁer, but the individual instances\nare diﬃcult to accurately localize. Some examples of L\nobjects are “killer whale,” “schooner,” and “lion,” and\nsome examples of XL objects are “boathouse,” “mosque,”\n“toyshop” and “steel arch bridge.”\nIn Figure 14(top,right) corresponding to the object\ndetection task, the inﬂuence of real-world object size is\nnot as apparent. One of the key reasons is that many of\nthe XL and L object classes of the image classiﬁcation\nand single-object localization datasets were removed in\nconstructing the detection dataset (Section 3.3.1) since\nthey were not basic categories well-suited for detection.\nThere were only 3 XL object classes remaining in the\ndataset (“train,” “airplane” and “bus”), and none af-\nter scale normalization.We omit them from the analy-\nsis. The average precision of XS, S, M objects (44.5%,\n39.0%, and 38.5% mAP respectively) is statistically in-\nsigniﬁcant from average precision on L objects: 95%\nconﬁdence interval of L objects is 37.5% −59.5%. This\nmay be due to the fact that there are only 6 L object\nclasses remaining after scale normalization; all other\nreal-world size bins have at least 18 object classes.\nFinally, it is interesting that performance on XS ob-\njects of 44.5% mAP (CI 40.5% −47.6%) is statistically\nsigniﬁcantly better than performance on S or M ob-\njects with 39.0% mAP and 38.5% mAP respectively.\nSome examples of XS objects are “strawberry,” “bow\ntie” and “rugby ball.”\nDeformability within instance. In Figure 14(second row)\nit is clear that the “optimistic” model performs statis-\ntically signiﬁcantly worse on rigid objects than on de-\nformable objects. Image classiﬁcation accuracy is 93.2%\nImageNet Large Scale Visual Recognition Challenge\n29\nReal-world size\nDeformability within instance\nAmount of texture\nFig. 14 Performance of the “optimistic” computer vision model as a function of object properties. The x-axis corresponds to\nobject properties annotated by human labelers for each object class (Russakovsky et al., 2013) and illustrated in Figure 1. The\ny-axis is the average accuracy of the “optimistic” model. Note that the range of the y-axis is diﬀerent for each task to make\nthe trends more visible. The black circle is the average accuracy of the model on all object classes that fall into each bin. We\ncontrol for the eﬀects of object scale by normalizing the object scale within each bin (details in Section 6.3.4). The color bars\nshow the model accuracy averaged across the remaining classes. Error bars show the 95% conﬁdence interval obtained with\nbootstrapping. Some bins are missing color bars because less than 5 object classes remained in the bin after scale normalization.\nFor example, the bar for XL real-world object detection classes is missing because that bin has only 3 object classes (airplane,\nbus, train) and after normalizing by scale no classes remain.\n30\nOlga Russakovsky* et al.\non rigid objects (CI 92.6%−93.8%), much smaller than\n95.7% on deformable ones. Single-object localization ac-\ncuracy is 76.2% on rigid objects (CI 74.9% −77.4%),\nmuch smaller than 84.7% on deformable ones. Object\ndetection mAP is 40.1% on rigid objects (CI 37.2% −\n42.9%), much smaller than 44.8% on deformable ones.\nWe can further analyze the eﬀects of deformabil-\nity after separating object classes into “natural” and\n“man-made” bins based on the ImageNet hierarchy. De-\nformability is highly correlated with whether the object\nis natural or man-made: 0.72 correlation for image clas-\nsiﬁcation and single-object localization classes, and 0.61\nfor object detection classes. Figure 14(third row) shows\nthe eﬀect of deformability on performance of the model\nfor man-made and natural objects separately.\nMan-made classes are signiﬁcantly harder than nat-\nural classes: classiﬁcation accuracy 92.8% (CI 92.3% −\n93.3%) for man-made versus 97.0% for natural, localiza-\ntion accuracy 75.5% (CI 74.3% −76.5%) for man-made\nversus 88.5% for natural, and detection mAP 38.7% (CI\n35.6 −41.3%) for man-made versus 50.9% for natural.\nHowever, whether the classes are rigid or deformable\nwithin this subdivision is no longer signiﬁcant in most\ncases. For example, the image classiﬁcation accuracy is\n92.3% (CI 91.4% −93.1%) on man-made rigid objects\nand 91.8% on man-made deformable objects – not sta-\ntistically signiﬁcantly diﬀerent.\nThere are two cases where the diﬀerences in per-\nformance are statistically signiﬁcant. First, for single-\nobject localization, natural deformable objects are eas-\nier than natural rigid objects: localization accuracy of\n87.9% (CI 85.9% −90.1%) on natural deformable ob-\njects is higher than 85.8% on natural rigid objects –\nfalling slightly outside the 95% conﬁdence interval. This\ndiﬀerence in performance is likely because deformable\nnatural animals tend to be easier to localize than rigid\nnatural fruit.\nSecond, for object detection, man-made rigid ob-\njects are easier than man-made deformable objects: 38.5%\nmAP (CI 35.2% −41.7%) on man-made rigid objects is\nhigher than 33.0% mAP on man-made deformable ob-\njects. This is because man-made rigid objects include\nclasses like “traﬃc light” or “car” whereas the man-\nmade deformable objects contain challenging classes like\n“plastic bag,” “swimming trunks” or “stethoscope.”\nAmount of texture. Finally, we analyze the eﬀect that\nobject texture has on the accuracy of the “optimistic”\nmodel. Figure 14(fourth row) demonstrates that the\nmodel performs better as the amount of texture on the\nobject increases. The most signiﬁcant diﬀerence is be-\ntween the performance on untextured objects and the\nperformance on objects with low texture. Image clas-\nsiﬁcation accuracy is 90.5% on untextured objects (CI\n89.3% −91.6%), lower than 94.6% on low-textured ob-\njects. Single-object localization accuracy is 71.4% on\nuntextured objects (CI 69.1%−73.3%), lower than 80.2%\non low-textured objects. Object detection mAP is 33.2%\non untextured objects (CI 29.5% −35.9%), lower than\n42.9% on low-textured objects.\nTexture is correlated with whether the object is nat-\nural or man-made, at 0.35 correlation for image classi-\nﬁcation and single-object localization, and 0.46 corre-\nlation for object detection. To determine if this is a\ncontributing factor, in Figure 14(bottom row) we break\nup the object classes into natural and man-made and\nshow the accuracy on objects with no texture versus\nobjects with low texture. We observe that the model\nis still statistically signiﬁcantly better on low-textured\nobject classes than on untextured ones, both on man-\nmade and natural object classes independently.12\n6.4 Human accuracy on large-scale image classiﬁcation\nRecent improvements in state-of-the-art accuracy on\nthe ILSVRC dataset are easier to put in perspective\nwhen compared to human-level accuracy. In this sec-\ntion we compare the performance of the leading large-\nscale image classiﬁcation method with the performance\nof humans on this task.\nTo support this comparison, we developed an inter-\nface that allowed a human labeler to annotate images\nwith up to ﬁve ILSVRC target classes. We compare hu-\nman errors to those of the winning ILSRC2014 image\nclassiﬁcation model, GoogLeNet (Section 5.1). For this\nanalysis we use a random sample of 1500 ILSVRC2012-\n2014 image classiﬁcation test set images.\nAnnotation interface. Our web-based annotation inter-\nface consists of one test set image and a list of 1000\nILSVRC categories on the side. Each category is de-\nscribed by its title, such as “cowboy boot.” The cate-\ngories are sorted in the topological order of the Ima-\ngeNet hierarchy, which places semantically similar con-\ncepts nearby in the list. For example, all motor vehicle-\nrelated classes are arranged contiguously in the list. Ev-\nery class category is additionally accompanied by a row\nof 13 examples images from the training set to allow for\nfaster visual scanning. The user of the interface selects 5\ncategories from the list by clicking on the desired items.\n12 Natural object detection classes are removed from this\nanalysis because there are only 3 and 13 natural untextured\nand low-textured classes respectively, and none remain after\nscale normalization. All other bins contain at least 9 object\nclasses after scale normalization.\nImageNet Large Scale Visual Recognition Challenge\n31\nSince our interface is web-based, it allows for natural\nscrolling through the list, and also search by text.\nAnnotation protocol. We found the task of annotating\nimages with one of 1000 categories to be an extremely\nchallenging task for an untrained annotator. The most\ncommon error that an untrained annotator is suscepti-\nble to is a failure to consider a relevant class as a pos-\nsible label because they are unaware of its existence.\nTherefore, in evaluating the human accuracy we re-\nlied primarily on expert annotators who learned to rec-\nognize a large portion of the 1000 ILSVRC classes. Dur-\ning training, the annotators labeled a few hundred val-\nidation images for practice and later switched to the\ntest set images.\n6.4.1 Quantitative comparison of human and computer\naccuracy on large-scale image classiﬁcation\nWe report results based on experiments with two ex-\npert annotators. The ﬁrst annotator (A1) trained on\n500 images and annotated 1500 test images. The sec-\nond annotator (A2) trained on 100 images and then\nannotated 258 test images. The average pace of label-\ning was approximately 1 image per minute, but the dis-\ntribution is strongly bimodal: some images are quickly\nrecognized, while some images (such as those of ﬁne-\ngrained breeds of dogs, birds, or monkeys) may require\nmultiple minutes of concentrated eﬀort.\nThe results are reported in Table 9.\nAnnotator 1. Annotator A1 evaluated a total of 1500\ntest set images. The GoogLeNet classiﬁcation error on\nthis sample was estimated to be 6.8% (recall that the\nerror on full test set of 100,000 images is 6.7%, as shown\nin Table 7). The human error was estimated to be 5.1%.\nThus, annotator A1 achieves a performance superior to\nGoogLeNet, by approximately 1.7%. We can analyze\nthe statistical signiﬁcance of this result under the null\nhypothesis that they are from the same distribution. In\nparticular, comparing the two proportions with a z-test\nyields a one-sided p-value of p = 0.022. Thus, we can\nconclude that this result is statistically signiﬁcant at\nthe 95% conﬁdence level.\nAnnotator 2. Our second annotator (A2) trained on a\nsmaller sample of only 100 images and then labeled 258\ntest set images. As seen in Table 9, the ﬁnal classiﬁca-\ntion error is signiﬁcantly worse, at approximately 12.0%\nTop-5 error. The majority of these errors (48.8%) can\nbe attributed to the annotator failing to spot and con-\nsider the ground truth label as an option.\nRelative Confusion\nA1\nA2\nHuman succeeds, GoogLeNet succeeds\n1352\n219\nHuman succeeds, GoogLeNet fails\n72\n8\nHuman fails, GoogLeNet succeeds\n46\n24\nHuman fails, GoogLeNet fails\n30\n7\nTotal number of images\n1500\n258\nEstimated GoogLeNet classiﬁcation error\n6.8%\n5.8%\nEstimated human classiﬁcation error\n5.1%\n12.0%\nTable 9 Human classiﬁcation results on the ILSVRC2012-\n2014 classiﬁcation test set, for two expert annotators A1 and\nA2. We report top-5 classiﬁcation error.\nThus, we conclude that a signiﬁcant amount of train-\ning time is necessary for a human to achieve compet-\nitive performance on ILSVRC. However, with a suﬃ-\ncient amount of training, a human annotator is still\nable to outperform the GoogLeNet result (p = 0.022)\nby approximately 1.7%.\nAnnotator comparison. We also compare the prediction\naccuracy of the two annotators. Of a total of 204 images\nthat both A1 and A2 labeled, 174 (85%) were correctly\nlabeled by both A1 and A2, 19 (9%) were correctly\nlabeled by A1 but not A2, 6 (3%) were correctly labeled\nby A2 but not A1, and 5 (2%) were incorrectly labeled\nby both. These include 2 images that we consider to be\nincorrectly labeled in the ground truth.\nIn particular, our results suggest that the human\nannotators do not exhibit strong overlap in their pre-\ndictions. We can approximate the performance of an\n“optimistic” human classiﬁer by assuming an image to\nbe correct if at least one of A1 or A2 correctly labeled\nthe image. On this sample of 204 images, we approxi-\nmate the error rate of an “optimistic” human annotator\nat 2.4%, compared to the GoogLeNet error rate of 4.9%.\n6.4.2 Analysis of human and computer errors on\nlarge-scale image classiﬁcation\nWe manually inspected both human and GoogLeNet\nerrors to gain an understanding of common error types\nand how they compare. For purposes of this section, we\nonly discuss results based on the larger sample of 1500\nimages that were labeled by annotator A1. Examples\nof representative mistakes are in Figure 15. The anal-\nysis and insights below were derived speciﬁcally from\nGoogLeNet predictions, but we suspect that many of\nthe same errors may be present in other methods.\nTypes of errors in both computer and human annota-\ntions:\n1. Multiple objects. Both GoogLeNet and humans\nstruggle with images that contain multiple ILSVRC\n32\nOlga Russakovsky* et al.\nFig. 15 Representative validation images that highlight common sources of error. For each image, we display the ground truth\nin blue, and top 5 predictions from GoogLeNet follow (red = wrong, green = right). GoogLeNet predictions on the validation\nset images were graciously provided by members of the GoogLeNet team. From left to right: Images that contain multiple\nobjects, images of extreme closeups and uncharacteristic views, images with ﬁlters, images that signiﬁcantly beneﬁt from the\nability to read text, images that contain very small and thin objects, images with abstract representations, and example of a\nﬁne-grained image that GoogLeNet correctly identiﬁes but a human would have signiﬁcant diﬃculty with.\nclasses (usually many more than ﬁve), with little in-\ndication of which object is the focus of the image.\nThis error is only present in the Classiﬁcation set-\nting, since every image is constrained to have ex-\nactly one correct label. In total, we attribute 24\n(24%) of GoogLeNet errors and 12 (16%) of human\nerrors to this category. It is worth noting that hu-\nmans can have a slight advantage in this error type,\nsince it can sometimes be easy to identify the most\nsalient object in the image.\n2. Incorrect annotations. We found that approxi-\nmately 5 out of 1500 images (0.3%) were incorrectly\nannotated in the ground truth. This introduces an\napproximately equal number of errors for both hu-\nmans and GoogLeNet.\nTypes of errors that the computer is more susceptible to\nthan the human:\n1. Object small or thin. GoogLeNet struggles with\nrecognizing objects that are very small or thin in\nthe image, even if that object is the only object\npresent. Examples of this include an image of a\nstanding person wearing sunglasses, a person hold-\ning a quill in their hand, or a small ant on a stem of a\nﬂower. We estimate that approximately 22 (21%) of\nGoogLeNet errors fall into this category, while none\nof the human errors do. In other words, in our sam-\nple of images, no image was mislabeled by a human\nbecause they were unable to identify a very small\nor thin object. This discrepancy can be attributed\nto the fact that a human can very eﬀectively lever-\nage context and aﬀordances to accurately infer the\nidentity of small objects (for example, a few barely\nvisible feathers near person’s hand as very likely be-\nlonging to a mostly occluded quill).\n2. Image ﬁlters. Many people enhance their photos\nwith ﬁlters that distort the contrast and color dis-\ntributions of the image. We found that 13 (13%)\nof the images that GoogLeNet incorrectly classiﬁed\ncontained a ﬁlter. Thus, we posit that GoogLeNet is\nnot very robust to these distortions. In comparison,\nonly one image among the human errors contained\na ﬁlter, but we do not attribute the source of the\nerror to the ﬁlter.\n3. Abstract representations. GoogLeNet struggles\nwith images that depict objects of interest in an ab-\nstract form, such as 3D-rendered images, paintings,\nsketches, plush toys, or statues. An example is the\nabstract shape of a bow drawn with a light source in\nnight photography, a 3D-rendered robotic scorpion,\nor a shadow on the ground, of a child on a swing.\nWe attribute approximately 6 (6%) of GoogLeNet\nerrors to this type of error and believe that humans\nare signiﬁcantly more robust, with no such errors\nseen in our sample.\n4. Miscellaneous sources. Additional sources of er-\nror that occur relatively infrequently include ex-\ntreme closeups of parts of an object, unconventional\nviewpoints such as a rotated image, images that\ncan signiﬁcantly beneﬁt from the ability to read\ntext (e.g. a featureless container identifying itself as\n“face powder”), objects with heavy occlusions, and\nimages that depict a collage of multiple images. In\ngeneral, we found that humans are more robust to\nall of these types of error.\nTypes of errors that the human is more susceptible to\nthan the computer:\n1. Fine-grained recognition. We found that humans\nare noticeably worse at ﬁne-grained recognition (e.g.\ndogs, monkeys, snakes, birds), even when they are\nImageNet Large Scale Visual Recognition Challenge\n33\nin clear view. To understand the diﬃculty, consider\nthat there are more than 120 species of dogs in the\ndataset. We estimate that 28 (37%) of the human\nerrors fall into this category, while only 7 (7%) of\nGoogLeNet errors do.\n2. Class unawareness. The annotator may sometimes\nbe unaware of the ground truth class present as a\nlabel option. When pointed out as an ILSVRC class,\nit is usually clear that the label applies to the im-\nage. These errors get progressively less frequent as\nthe annotator becomes more familiar with ILSVRC\nclasses. Approximately 18 (24%) of the human er-\nrors fall into this category.\n3. Insuﬃcient training data. Recall that the anno-\ntator is only presented with 13 examples of a class\nunder every category name. However, 13 images are\nnot always enough to adequately convey the allowed\nclass variations. For example, a brown dog can be\nincorrectly dismissed as a “Kelpie” if all examples of\na “Kelpie” feature a dog with black coat. However,\nif more than 13 images were listed it would have\nbecome clear that a “Kelpie” may have brown coat.\nApproximately 4 (5%) of human errors fall into this\ncategory.\n6.4.3 Conclusions from human image classiﬁcation\nexperiments\nWe investigated the performance of trained human an-\nnotators on a sample of 1500 ILSVRC test set images.\nOur results indicate that a trained human annotator is\ncapable of outperforming the best model (GoogLeNet)\nby approximately 1.7% (p = 0.022).\nWe expect that some sources of error may be rela-\ntively easily eliminated (e.g. robustness to ﬁlters, rota-\ntions, collages, eﬀectively reasoning over multiple scales),\nwhile others may prove more elusive (e.g. identifying\nabstract representations of objects). On the other hand,\na large majority of human errors come from ﬁne-grained\ncategories and class unawareness. We expect that the\nformer can be signiﬁcantly reduced with ﬁne-grained\nexpert annotators, while the latter could be reduced\nwith more practice and greater familiarity with ILSVRC\nclasses. Our results also hint that human errors are not\nstrongly correlated and that human ensembles may fur-\nther reduce human error rate.\nIt is clear that humans will soon outperform state-\nof-the-art ILSVRC image classiﬁcation models only by\nuse of signiﬁcant eﬀort, expertise, and time. One inter-\nesting follow-up question for future investigation is how\ncomputer-level accuracy compares with human-level ac-\ncuracy on more complex image understanding tasks.\n7 Conclusions\nIn this paper we described the large-scale data collec-\ntion process of ILSVRC, provided a summary of the\nmost successful algorithms on this data, and analyzed\nthe success and failure modes of these algorithms. In\nthis section we discuss some of the key lessons we learned\nover the years of ILSVRC, strive to address the key\ncriticisms of the datasets and the challenges we encoun-\ntered over the years, and conclude by looking forward\ninto the future.\n7.1 Lessons learned\nThe key lesson of collecting the datasets and running\nthe challenges for ﬁve years is this: All human in-\ntelligence tasks need to be exceptionally well-\ndesigned. We learned this lesson both when annotat-\ning the dataset using Amazon Mechanical Turk workers\n(Section 3) and even when trying to evaluate human-\nlevel image classiﬁcation accuracy using expert label-\ners (Section 6.4). The ﬁrst iteration of the labeling in-\nterface was always bad – generally meaning completely\nunusable. If there was any inherent ambiguity in the\nquestions posed (and there almost always was), work-\ners found it and accuracy suﬀered. If there is one piece\nof advice we can oﬀer to future research, it is to very\ncarefully design, continuously monitor, and extensively\nsanity-check all crowdsourcing tasks.\nThe other lesson, already well-known to large-scale\nresearchers, is this: Scaling up the dataset always\nreveals unexpected challenges. From designing com-\nplicated multi-step annotation strategies (Section 3.2.1)\nto having to modify the evaluation procedure (Section 4),\nwe had to continuously adjust to the large-scale setting.\nOn the plus side, of course, the major breakthroughs in\nobject recognition accuracy (Section 5) and the analysis\nof the strength and weaknesses of current algorithms as\na function of object class properties ( Section 6.3) would\nnever have been possible on a smaller scale.\n7.2 Criticism\nIn the past ﬁve years, we encountered three major crit-\nicisms of the ILSVRC dataset and the corresponding\nchallenge: (1) the ILSVRC dataset is insuﬃciently chal-\nlenging, (2) the ILSVRC dataset contains annotation\nerrors, and (3) the rules of ILSVRC competition are\ntoo restrictive. We discuss these in order.\nThe ﬁrst criticism is that the objects in the dataset\ntend to be large and centered in the images, making\nthe dataset insuﬃciently challenging. In Sections 3.2.2\n34\nOlga Russakovsky* et al.\nand 3.3.4 we tried to put those concerns to rest by an-\nalyzing the statistics of the ILSVRC dataset and con-\ncluding that it is comparable with, and in many cases\nmuch more challenging than, the long-standing PAS-\nCAL VOC benchmark (Everingham et al., 2010).\nThe second is regarding the errors in ground truth\nlabeling. We went through several rounds of in-house\npost-processing of the annotations obtained using crowd-\nsourcing, and corrected many common sources of errors\n(e.g., Appendix E). The major remaining source of an-\nnotation errors stem from ﬁne-grained object classes,\ne.g., labelers failing to distinguish diﬀerent species of\nbirds. This is a tradeoﬀthat had to be made: in order\nto annotate data at this scale on a reasonable budget,\nwe had to rely on non-expert crowd labelers. However,\noverall the dataset is encouragingly clean. By our esti-\nmates, 99.7% precision is achieved in the image classi-\nﬁcation dataset (Sections 3.1.3 and 6.4) and 97.9% of\nimages that went through the bounding box annota-\ntion system have all instances of the target object class\nlabeled with bounding boxes (Section 3.2.1).\nThe third criticism we encountered is over the rules\nof the competition regarding using external training\ndata. In ILSVRC2010-2013, algorithms had to only use\nthe provided training and validation set images and an-\nnotations for training their models. With the growth of\nthe ﬁeld of large-scale unsupervised feature learning,\nhowever, questions began to arise about what exactly\nconstitutes “outside” data: for example, are image fea-\ntures trained on a large pool of “outside” images in an\nunsupervised fashion allowed in the competition? Af-\nter much discussion, in ILSVRC2014 we took the ﬁrst\nstep towards addressing this problem. We followed the\nPASCAL VOC strategy and created two tracks in the\ncompetition: entries using only “provided” data and en-\ntries using “outside” data, meaning any images or an-\nnotations not provided as part of ILSVRC training or\nvalidation sets. However, in the future this strategy will\nlikely need to be further revised as the computer vision\nﬁeld evolves. For example, competitions can consider\nallowing the use of any image features which are pub-\nlically available, even if these features were learned on\nan external source of data.\n7.3 The future\nGiven the massive algorithmic breakthroughs over the\npast ﬁve years, we are very eager to see what will hap-\npen in the next ﬁve years. There are many potential\ndirections of improvement and growth for ILSVRC and\nother large-scale image datasets.\nFirst, continuing the trend of moving towards richer\nimage understanding (from image classiﬁcation to single-\nobject localization to object detection), the next chal-\nlenge would be to tackle pixel-level object segmenta-\ntion. The recently released large-scale COCO dataset (Lin\net al., 2014b) is already taking a step in that direction.\nSecond, as datasets grow even larger in scale, it may\nbecome impossible to fully annotate them manually.\nThe scale of ILSVRC is already imposing limits on the\nmanual annotations that are feasible to obtain: for ex-\nample, we had to restrict the number of objects labeled\nper image in the image classiﬁcation and single-object\nlocalization datasets. In the future, with billions of im-\nages, it will become impossible to obtain even one clean\nlabel for every image. Datasets such as Yahoo’s Flickr\nCreative Commons 100M,13 released with weak human\ntags but no centralized annotation, will become more\ncommon.\nThe growth of unlabeled or only partially labeled\nlarge-scale datasets implies two things. First, algorithms\nwill have to rely more on weakly supervised training\ndata. Second, even evaluation might have to be done\nafter the algorithms make predictions, not before. This\nmeans that rather than evaluating accuracy (how many\nof the test images or objects did the algorithm get right)\nor recall (how many of the desired images or objects did\nthe algorithm manage to ﬁnd), both of which require\na fully annotated test set, we will be focusing more on\nprecision: of the predictions that the algorithm made,\nhow many were deemed correct by humans.\nWe are eagerly awaiting the future development of\nobject recognition datasets and algorithms, and are grate-\nful that ILSVRC served as a stepping stone along this\npath.\nAcknowledgements We thank Stanford University, UNC\nChapel Hill, Google and Facebook for sponsoring the chal-\nlenges, and NVIDIA for providing computational resources\nto participants of ILSVRC2014. We thank our advisors over\nthe years: Lubomir Bourdev, Alexei Efros, Derek Hoiem, Ji-\ntendra Malik, Chuck Rosenberg and Andrew Zisserman. We\nthank the PASCAL VOC organizers for partnering with us\nin running ILSVRC2010-2012. We thank all members of the\nStanford vision lab for supporting the challenges and putting\nup with us along the way. Finally, and most importantly, we\nthank all researchers that have made the ILSVRC eﬀort a suc-\ncess by competing in the challenges and by using the datasets\nto advance computer vision.\nAppendix A\nILSVRC2012-2014 image\nclassiﬁcation and single-object localization\nobject categories\nabacus, abaya, academic gown, accordion, acorn, acorn squash, acoustic gui-\ntar, admiral, affenpinscher, Afghan hound, African chameleon, African crocodile,\n13 http://webscope.sandbox.yahoo.com/catalog.php?\ndatatype=i&did=67\nImageNet Large Scale Visual Recognition Challenge\n35\nAfrican elephant, African grey, African hunting dog, agama, agaric, aircraft car-\nrier, Airedale, airliner, airship, albatross, alligator lizard, alp, altar, ambulance,\nAmerican alligator, American black bear, American chameleon, American coot,\nAmerican egret, American lobster, American Staffordshire terrier, amphibian,\nanalog clock, anemone fish, Angora, ant, apiary, Appenzeller, apron, Arabian\ncamel, Arctic fox, armadillo, artichoke, ashcan, assault rifle, Australian terrier,\naxolotl, baboon, backpack, badger, bagel, bakery, balance beam, bald eagle, bal-\nloon, ballplayer, ballpoint, banana, Band Aid, banded gecko, banjo, bannister,\nbarbell, barber chair, barbershop, barn, barn spider, barometer, barracouta, bar-\nrel, barrow, baseball, basenji, basketball, basset, bassinet, bassoon, bath towel,\nbathing cap, bathtub, beach wagon, beacon, beagle, beaker, bearskin, beaver,\nBedlington terrier, bee, bee eater, beer bottle, beer glass, bell cote, bell pepper,\nBernese mountain dog, bib, bicycle-built-for-two, bighorn, bikini, binder, binoc-\nulars, birdhouse, bison, bittern, black and gold garden spider, black grouse, black\nstork, black swan, black widow, black-and-tan coonhound, black-footed ferret,\nBlenheim spaniel, bloodhound, bluetick, boa constrictor, boathouse, bobsled,\nbolete, bolo tie, bonnet, book jacket, bookcase, bookshop, Border collie, Border\nterrier, borzoi, Boston bull, bottlecap, Bouvier des Flandres, bow, bow tie, box\nturtle, boxer, Brabancon griffon, brain coral, brambling, brass, brassiere, break-\nwater, breastplate, briard, Brittany spaniel, broccoli, broom, brown bear, bub-\nble, bucket, buckeye, buckle, bulbul, bull mastiff, bullet train, bulletproof vest,\nbullfrog, burrito, bustard, butcher shop, butternut squash, cab, cabbage butter-\nfly, cairn, caldron, can opener, candle, cannon, canoe, capuchin, car mirror, car\nwheel, carbonara, Cardigan, cardigan, cardoon, carousel, carpenter’s kit, car-\nton, cash machine, cassette, cassette player, castle, catamaran, cauliflower, CD\nplayer, cello, cellular telephone, centipede, chain, chain mail, chain saw, chain-\nlink fence, chambered nautilus, cheeseburger, cheetah, Chesapeake Bay retriever,\nchest, chickadee, chiffonier, Chihuahua, chime, chimpanzee, china cabinet, chi-\nton, chocolate sauce, chow, Christmas stocking, church, cicada, cinema, cleaver,\ncliff, cliff dwelling, cloak, clog, clumber, cock, cocker spaniel, cockroach, cocktail\nshaker, coffee mug, coffeepot, coho, coil, collie, colobus, combination lock, comic\nbook, common iguana, common newt, computer keyboard, conch, confectionery,\nconsomme, container ship, convertible, coral fungus, coral reef, corkscrew, corn,\ncornet, coucal, cougar, cowboy boot, cowboy hat, coyote, cradle, crane, crane,\ncrash helmet, crate, crayfish, crib, cricket, Crock Pot, croquet ball, crossword\npuzzle, crutch, cucumber, cuirass, cup, curly-coated retriever, custard apple,\ndaisy, dalmatian, dam, damselfly, Dandie Dinmont, desk, desktop computer,\ndhole, dial telephone, diamondback, diaper, digital clock, digital watch, dingo,\ndining table, dishrag, dishwasher, disk brake, Doberman, dock, dogsled, dome,\ndoormat, dough, dowitcher, dragonfly, drake, drilling platform, drum, drumstick,\ndugong, dumbbell, dung beetle, Dungeness crab, Dutch oven, ear, earthstar,\nechidna, eel, eft, eggnog, Egyptian cat, electric fan, electric guitar, electric lo-\ncomotive, electric ray, English foxhound, English setter, English springer, enter-\ntainment center, EntleBucher, envelope, Eskimo dog, espresso, espresso maker,\nEuropean fire salamander, European gallinule, face powder, feather boa, fid-\ndler crab, fig, file, fire engine, fire screen, fireboat, flagpole, flamingo, flat-\ncoated retriever, flatworm, flute, fly, folding chair, football helmet, forklift, foun-\ntain, fountain pen, four-poster, fox squirrel, freight car, French bulldog, French\nhorn, French loaf, frilled lizard, frying pan, fur coat, gar, garbage truck, gar-\nden spider, garter snake, gas pump, gasmask, gazelle, German shepherd, Ger-\nman short-haired pointer, geyser, giant panda, giant schnauzer, gibbon, Gila\nmonster, go-kart, goblet, golden retriever, goldfinch, goldfish, golf ball, golfcart,\ngondola, gong, goose, Gordon setter, gorilla, gown, grand piano, Granny Smith,\ngrasshopper, Great Dane, great grey owl, Great Pyrenees, great white shark,\nGreater Swiss Mountain dog, green lizard, green mamba, green snake, green-\nhouse, grey fox, grey whale, grille, grocery store, groenendael, groom, ground\nbeetle, guacamole, guenon, guillotine, guinea pig, gyromitra, hair slide, hair\nspray, half track, hammer, hammerhead, hamper, hamster, hand blower, hand-\nheld computer, handkerchief, hard disc, hare, harmonica, harp, hartebeest, har-\nvester, harvestman, hatchet, hay, head cabbage, hen, hen-of-the-woods, hermit\ncrab, hip, hippopotamus, hog, hognose snake, holster, home theater, honeycomb,\nhook, hoopskirt, horizontal bar, hornbill, horned viper, horse cart, hot pot, hot-\ndog, hourglass, house finch, howler monkey, hummingbird, hyena, ibex, Ibizan\nhound, ice bear, ice cream, ice lolly, impala, Indian cobra, Indian elephant, in-\ndigo bunting, indri, iPod, Irish setter, Irish terrier, Irish water spaniel, Irish\nwolfhound, iron, isopod, Italian greyhound, jacamar, jack-o’-lantern, jackfruit,\njaguar, Japanese spaniel, jay, jean, jeep, jellyfish, jersey, jigsaw puzzle, jinrik-\nisha, joystick, junco, keeshond, kelpie, Kerry blue terrier, killer whale, kimono,\nking crab, king penguin, king snake, kit fox, kite, knee pad, knot, koala, Ko-\nmodo dragon, komondor, kuvasz, lab coat, Labrador retriever, lacewing, ladle,\nladybug, Lakeland terrier, lakeside, lampshade, langur, laptop, lawn mower, leaf\nbeetle, leafhopper, leatherback turtle, lemon, lens cap, Leonberg, leopard, lesser\npanda, letter opener, Lhasa, library, lifeboat, lighter, limousine, limpkin, liner,\nlion, lionfish, lipstick, little blue heron, llama, Loafer, loggerhead, long-horned\nbeetle, lorikeet, lotion, loudspeaker, loupe, lumbermill, lycaenid, lynx, macaque,\nmacaw, Madagascar cat, magnetic compass, magpie, mailbag, mailbox, mail-\nlot, maillot, malamute, malinois, Maltese dog, manhole cover, mantis, maraca,\nmarimba, marmoset, marmot, mashed potato, mask, matchstick, maypole, maze,\nmeasuring cup, meat loaf, medicine chest, meerkat, megalith, menu, Mexican\nhairless, microphone, microwave, military uniform, milk can, miniature pinscher,\nminiature poodle, miniature schnauzer, minibus, miniskirt, minivan, mink, mis-\nsile, mitten, mixing bowl, mobile home, Model T, modem, monarch, monastery,\nmongoose, monitor, moped, mortar, mortarboard, mosque, mosquito net, mo-\ntor scooter, mountain bike, mountain tent, mouse, mousetrap, moving van, mud\nturtle, mushroom, muzzle, nail, neck brace, necklace, nematode, Newfoundland,\nnight snake, nipple, Norfolk terrier, Norwegian elkhound, Norwich terrier, note-\nbook, obelisk, oboe, ocarina, odometer, oil filter, Old English sheepdog, or-\nange, orangutan, organ, oscilloscope, ostrich, otter, otterhound, overskirt, ox,\noxcart, oxygen mask, oystercatcher, packet, paddle, paddlewheel, padlock, paint-\nbrush, pajama, palace, panpipe, paper towel, papillon, parachute, parallel bars,\npark bench, parking meter, partridge, passenger car, patas, patio, pay-phone,\npeacock, pedestal, Pekinese, pelican, Pembroke, pencil box, pencil sharpener,\nperfume, Persian cat, Petri dish, photocopier, pick, pickelhaube, picket fence,\npickup, pier, piggy bank, pill bottle, pillow, pineapple, ping-pong ball, pinwheel,\npirate, pitcher, pizza, plane, planetarium, plastic bag, plate, plate rack, platy-\npus, plow, plunger, Polaroid camera, pole, polecat, police van, pomegranate,\nPomeranian, poncho, pool table, pop bottle, porcupine, pot, potpie, potter’s\nwheel, power drill, prairie chicken, prayer rug, pretzel, printer, prison, proboscis\nmonkey, projectile, projector, promontory, ptarmigan, puck, puffer, pug, punch-\ning bag, purse, quail, quill, quilt, racer, racket, radiator, radio, radio telescope,\nrain barrel, ram, rapeseed, recreational vehicle, red fox, red wine, red wolf, red-\nbacked sandpiper, red-breasted merganser, redbone, redshank, reel, reflex cam-\nera, refrigerator, remote control, restaurant, revolver, rhinoceros beetle, Rhode-\nsian ridgeback, rifle, ringlet, ringneck snake, robin, rock beauty, rock crab, rock\npython, rocking chair, rotisserie, Rottweiler, rubber eraser, ruddy turnstone,\nruffed grouse, rugby ball, rule, running shoe, safe, safety pin, Saint Bernard,\nsaltshaker, Saluki, Samoyed, sandal, sandbar, sarong, sax, scabbard, scale, schip-\nperke, school bus, schooner, scoreboard, scorpion, Scotch terrier, Scottish deer-\nhound, screen, screw, screwdriver, scuba diver, sea anemone, sea cucumber, sea\nlion, sea slug, sea snake, sea urchin, Sealyham terrier, seashore, seat belt, sewing\nmachine, Shetland sheepdog, shield, Shih-Tzu, shoe shop, shoji, shopping bas-\nket, shopping cart, shovel, shower cap, shower curtain, siamang, Siamese cat,\nSiberian husky, sidewinder, silky terrier, ski, ski mask, skunk, sleeping bag,\nslide rule, sliding door, slot, sloth bear, slug, snail, snorkel, snow leopard, snow-\nmobile, snowplow, soap dispenser, soccer ball, sock, soft-coated wheaten ter-\nrier, solar dish, sombrero, sorrel, soup bowl, space bar, space heater, space\nshuttle, spaghetti squash, spatula, speedboat, spider monkey, spider web, spin-\ndle, spiny lobster, spoonbill, sports car, spotlight, spotted salamander, squirrel\nmonkey, Staffordshire bullterrier, stage, standard poodle, standard schnauzer,\nstarfish, steam locomotive, steel arch bridge, steel drum, stethoscope, stingray,\nstinkhorn, stole, stone wall, stopwatch, stove, strainer, strawberry, street sign,\nstreetcar, stretcher, studio couch, stupa, sturgeon, submarine, suit, sulphur but-\nterfly, sulphur-crested cockatoo, sundial, sunglass, sunglasses, sunscreen, suspen-\nsion bridge, Sussex spaniel, swab, sweatshirt, swimming trunks, swing, switch,\nsyringe, tabby, table lamp, tailed frog, tank, tape player, tarantula, teapot,\nteddy, television, tench, tennis ball, terrapin, thatch, theater curtain, thimble,\nthree-toed sloth, thresher, throne, thunder snake, Tibetan mastiff, Tibetan ter-\nrier, tick, tiger, tiger beetle, tiger cat, tiger shark, tile roof, timber wolf, titi,\ntoaster, tobacco shop, toilet seat, toilet tissue, torch, totem pole, toucan, tow\ntruck, toy poodle, toy terrier, toyshop, tractor, traffic light, trailer truck, tray,\ntree frog, trench coat, triceratops, tricycle, trifle, trilobite, trimaran, tripod, tri-\numphal arch, trolleybus, trombone, tub, turnstile, tusker, typewriter keyboard,\numbrella, unicycle, upright, vacuum, valley, vase, vault, velvet, vending machine,\nvestment, viaduct, vine snake, violin, vizsla, volcano, volleyball, vulture, waffle\niron, Walker hound, walking stick, wall clock, wallaby, wallet, wardrobe, war-\nplane, warthog, washbasin, washer, water bottle, water buffalo, water jug, water\nouzel, water snake, water tower, weasel, web site, weevil, Weimaraner, Welsh\nspringer spaniel, West Highland white terrier, whippet, whiptail, whiskey jug,\nwhistle, white stork, white wolf, wig, wild boar, window screen, window shade,\nWindsor tie, wine bottle, wing, wire-haired fox terrier, wok, wolf spider, wom-\nbat, wood rabbit, wooden spoon, wool, worm fence, wreck, yawl, yellow lady’s\nslipper, Yorkshire terrier, yurt, zebra, zucchini\nAppendix B\nAdditional single-object\nlocalization dataset statistics\nWe consider two additional metrics of object localiza-\ntion diﬃculty: chance performance of localization and\nthe level of clutter. We use these metrics to compare\nILSVRC2012-2014 single-object localization dataset to\nthe PASCAL VOC 2012 object detection benchmark.\nThe measures of localization diﬃculty are computed\non the validation set of both datasets. According to\nboth of these measures of diﬃculty there is a subset of\nILSVRC which is as challenging as PASCAL but more\nthan an order of magnitude greater in size. Figure 16\nshows the distributions of diﬀerent properties (object\nscale, chance performance of localization and level of\nclutter) across the diﬀerent classes in the two datasets.\nChance performance of localization (CPL). Chance per-\nformance on a dataset is a common metric to con-\nsider. We deﬁne the CPL measure as the expected ac-\ncuracy of a detector which ﬁrst randomly samples an\nobject instance of that class and then uses its bounding\nbox directly as the proposed localization window on all\nother images (after rescaling the images to the same\nsize). Concretely, let B1, B2, . . . , BN be all the bound-\ning boxes of the object instances within a class, then\nCPL =\nP\ni\nP\nj̸=i IOU(Bi, Bj) ≥0.5\nN(N −1)\n(6)\nSome of the most diﬃcult ILSVRC categories to lo-\ncalize according to this metric are basketball, swim-\nming trunks, ping pong ball and rubber eraser, all with\nless than 0.2% CPL. This measure correlates strongly\n(ρ = 0.9) with the average scale of the object (fraction\nof image occupied by object). The average CPL across\n36\nOlga Russakovsky* et al.\nthe 1000 ILSVRC categories is 20.8%. The 20 PASCAL\ncategories have an average CPL of 8.7%, which is the\nsame as the CPL of the 562 most diﬃcult categories of\nILSVRC.\nClutter. Intuitively, even small objects are easy to lo-\ncalize on a plain background. To quantify clutter we\nemploy the objectness measure of (Alexe et al., 2012),\nwhich is a class-generic object detector evaluating how\nlikely a window in the image contains a coherent ob-\nject (of any class) as opposed to background (sky, wa-\nter, grass). For every image m containing target ob-\nject instances at positions Bm\n1 , Bm\n2 , . . . , we use the pub-\nlicly available objectness software to sample 1000 win-\ndows W m\n1 , W m\n2 , . . . W m\n1000, in order of decreasing proba-\nbility of the window containing any generic object. Let\nobj(m) be the number of generic object-looking win-\ndows sampled before localizing an instance of the target\ncategory, i.e., obj(m) = min{k : maxi iou(W m\nk , Bm\ni ) ≥\n0.5}. For a category containing M images, we compute\nthe average number of such windows per image and de-\nﬁne\nClutter = log2( 1\nM\nP\nm obj(m))\n(7)\nThe higher the clutter of a category, the harder the\nobjects are to localize according to generic cues. If an\nobject can’t be localized with the ﬁrst 1000 windows (as\nis the case for 1% of images on average per category in\nILSVRC and 5% in PASCAL), we set obj(m) = 1001.\nThe fact that more than 95% of objects can be local-\nized with these windows imply that the objectness cue is\nalready quite strong, so objects that require many win-\ndows on average will be extremely diﬃcult to detect:\ne.g., ping pong ball (clutter of 9.57, or 758 windows\non average), basketball (clutter of 9.21), puck (clutter\nof 9.17) in ILSVRC. The most diﬃcult object in PAS-\nCAL is bottle with clutter score of 8.47. On average,\nILSVRC has clutter score of 3.59. The most diﬃcult\nsubset of ILSVRC with 250 object categories has an\norder of magnitude more categories and the same aver-\nage amount of clutter (of 5.90) as the PASCAL dataset.\nAppendix C\nManually curated queries for\nobtaining object detection scene images\nIn Section 3.3.2 we discussed three types of queries we\nused for collecting the object detection images: (1) sin-\ngle object category name or a synonym; (2) a pair of\nobject category names; (3) a manual query, typically\ntargetting one or more object categories with insuﬃ-\ncient data. Here we provide a list of the 129 manually\ncurated queries:\nafternoon tea, ant bridge building, armadillo race, armadillo yard, artist studio,\nauscultation, baby room, banjo orchestra, banjo rehersal, banjo show, califone\nheadphones & media player sets, camel dessert, camel tourist, carpenter drilling,\ncarpentry, centipede wild, coffee shop, continental breakfast toaster, continen-\ntal breakfast waffles, crutch walking, desert scorpion, diner, dining room, din-\ning table, dinner, dragonfly friendly, dragonfly kid, dragonfly pond, dragonfly\nwild, drying hair, dumbbell curl, fan blow wind, fast food, fast food restau-\nrant, firewood chopping, flu shot, goldfish aquarium, goldfish tank, golf cart\non golf course, gym dumbbell, hamster drinking water, harmonica orchestra,\nharmonica rehersal, harmonica show, harp ensemble, harp orchestra, harp re-\nhersal, harp show, hedgehog cute, hedgehog floor, hedgehog hidden, hippo bird,\nhippo friendly, home improvement diy drill, horseback riding, hotel coffee ma-\nchine, hotel coffee maker, hotel waffle maker, jellyfish scuba, jellyfish snorkling,\nkitchen, kitchen counter coffee maker, kitchen counter toaster, kitchenette, koala\nfeed, koala tree, ladybug flower, ladybug yard, laundromat, lion zebra friendly,\nlunch, mailman, making breakfast, making waffles, mexican food, motorcycle\nracing, office, office fan, opossum on tree branch, orchestra, panda play, panda\ntree, pizzeria, pomegranate tree, porcupine climbing trees, power drill carpenter,\npurse shop, red panda tree, riding competition, riding motor scooters, school sup-\nplies, scuba starfish, sea lion beach, sea otter, sea urchin habitat, shopping for\nschool supplies, sitting in front of a fan, skunk and cat, skunk park, skunk wild,\nskunk yard, snail flower, snorkling starfish, snowplow cleanup, snowplow pile,\nsnowplow winter, soccer game, south american zoo, starfish sea world, starts\nshopping, steamed artichoke, stethoscope doctor, strainer pasta, strainer tea,\nsyringe doctor, table with food, tape player, tiger circus, tiger pet, using a can\nopener, using power drill, waffle iron breakfast, wild lion savana, wildlife pre-\nserve animals, wiping dishes, wombat petting zoo, zebra savana, zoo feeding, zoo\nin australia\nAppendix D\nHierarchy of questions for full\nimage annotation\nThe following is a hierarchy of questions manually con-\nstructed for crowdsourcing full annotation of images\nwith the presence or absence of 200 object detection\ncategories in ILSVRC2013 and ILSVRC2014. All ques-\ntions are of the form “is there a ... in the image?” Ques-\ntions marked with • are asked on every image. If the\nanswer to a question is determined to be “no” then the\nanswer to all descendant questions is assumed to be\n“no”. The 200 numbered leaf nodes correspond to the\n200 object detection categories.\nThe goal in the hierarchy construction is to save\ncost (by asking as few questions as possible on every\nimage) while avoiding any ambiguity in questions which\nwould lead to false negatives during annotation. This\nhierarchy is not tree-structured; some questions have\nmultiple parents.\nHierarchy of questions:\n• first aid/ medical items\n◦(1) stethoscope\n◦(2) syringe\n◦(3) neck brace\n◦(4) crutch\n◦(5) stretcher\n◦(6) band aid: an adhesive bandage to cover small cuts or blisters\n• musical instruments\n◦(7) accordion (a portable box-shaped free-reed instrument; the reeds are\nmade to vibrate by air from the bellows controlled by the player)\n◦(8) piano, pianoforte, forte-piano\n◦percussion instruments: chimes, maraccas, drums, etc\n◦(9) chime: a percussion instrument consisting of a set of tuned bells that\nare struck with a hammer; used as an orchestral instrument\n◦(10) maraca\n◦(11) drum\n◦stringed instrument\n◦(12) banjo, the body of a banjo is round. please do not confuse with guitar\n◦(13) cello: a large stringed instrument; seated player holds it upright while\nplaying\n◦(14) violin: bowed stringed instrument that has four strings, a hollow\nbody, an unfretted fingerboard and is played with a bow. please do not\nconfuse with cello, which is held upright while playing\n◦(15) harp\n◦(16) guitar, please do not confuse with banjo. the body of a banjo is round\n◦wind instrument: a musical instrument in which the sound is produced by an\nenclosed column of air that is moved by the breath (such as trumpet, french\nhorn, harmonica, flute, etc)\n◦(17) trumpet: a brass musical instrument with a narrow tube and a flared\nbell, which is played by means of valves. often has 3 keys on top\n◦(18) french horn: a brass musical instrument consisting of a conical tube\nthat is coiled into a spiral, with a flared bell at the end\n◦(19) trombone: a brass instrument consisting of a long tube whose length\ncan be varied by a u-shaped slide\n◦(20) harmonica\nImageNet Large Scale Visual Recognition Challenge\n37\n1000 classes of ILSVRC2012-2014 single-object localization (dark green)\nversus 20 classes of PASCAL 2012 (light blue)\n→more diﬃult\nmore diﬃult ←\nmore diﬃult ←\n→more diﬃult\n200 hardest classes of ILSVRC2012-2014 single-object localization (dark green)\nversus 20 classes of PASCAL 2012 (light blue)\n→more diﬃult\nmore diﬃult ←\nmore diﬃult ←\n→more diﬃult\nFig. 16 Distribution of various measures of localization diﬃculty on the ILSVRC2012-2014 single-object localization (dark\ngreen) and PASCAL VOC 2012 (light blue) validation sets. Object scale is fraction of image area occupied by an average\nobject instance. Chance performance of localization and level of clutter are deﬁned in Appendix B. The plots on top contain\nthe full ILSVRC validation set with 1000 classes; the plots on the bottom contain 200 ILSVRC classes with the lowest chance\nperformance of localization. All plots contain all 20 classes of PASCAL VOC.\n◦(21) flute: a high-pitched musical instrument that looks like a straight\ntube and is usually played sideways (please do not confuse with oboes, which\nhave a distinctive straw-like mouth piece and a slightly flared end)\n◦(22) oboe: a slender musical instrument roughly 65cm long with metal\nkeys, a distinctive straw-like mouthpiece and often a slightly flared end\n(please do not confuse with flutes)\n◦(23) saxophone: a musical instrument consisting of a brass conical tube,\noften with a u-bend at the end\n• food: something you can eat or drink (includes growing fruit, vegetables and\nmushrooms, but does not include living animals)\n◦food with bread or crust: pretzel, bagel, pizza, hotdog, hamburgers, etc\n◦(24) pretzel\n◦(25) bagel, beigel\n◦(26) pizza, pizza pie\n◦(27) hotdog, hot dog, red hot\n◦(28) hamburger, beefburger, burger\n◦(29) guacamole\n◦(30) burrito\n◦(31) popsicle (ice cream or water ice on a small wooden stick)\n◦fruit\n◦(32) fig\n◦(33) pineapple, ananas\n◦(34) banana\n◦(35) pomegranate\n◦(36) apple\n◦(37) strawberry\n◦(38) orange\n◦(39) lemon\n◦vegetables\n◦(40) cucumber, cuke\n◦(41) artichoke, globe artichoke\n◦(42) bell pepper\n◦(43) head cabbage\n◦(44) mushroom\n• items that run on electricity (plugged in or using batteries); including clocks,\nmicrophones, traffic lights, computers, etc\n◦(45) remote control, remote\n◦electronics that blow air\n◦(46) hair dryer, blow dryer\n◦(47) electric fan: a device for creating a current of air by movement of a\nsurface or surfaces (please do not consider hair dryers)\n◦electronics that can play music or amplify sound\n◦(48) tape player\n◦(49) iPod\n◦(50) microphone, mike\n◦computer and computer peripherals: mouse, laptop, printer, keyboard, etc\n◦(51) computer mouse\n◦(52) laptop, laptop computer\n◦(53) printer (please do not consider typewriters to be printers)\n◦(54) computer keyboard\n◦(55) lamp\n◦electric cooking appliance (an appliance which generates heat to cook food\nor boil water)\n◦(56) microwave, microwave oven\n◦(57) toaster\n◦(58) waffle iron\n◦(59) coffee maker: a kitchen appliance used for brewing coffee automati-\ncally\n◦(60) vacuum, vacuum cleaner\n◦(61) dishwasher, dish washer, dishwashing machine\n◦(62) washer, washing machine: an electric appliance for washing clothes\n◦(63) traffic light, traffic signal, stoplight\n◦(64) tv or monitor: an electronic device that represents information in visual\nform\n◦(65) digital clock: a clock that displays the time of day digitally\n• kitchen items: tools,utensils and appliances usually found in the kitchen\n◦electric cooking appliance (an appliance which generates heat to cook food\nor boil water)\n◦(56) microwave, microwave oven\n◦(57) toaster\n◦(58) waffle iron\n◦(59) coffee maker: a kitchen appliance used for brewing coffee automati-\ncally\n◦(61) dishwasher, dish washer, dishwashing machine\n◦(66) stove\n◦things used to open cans/bottles: can opener or corkscrew\n◦(67) can opener (tin opener)\n◦(68) corkscrew\n◦(69) cocktail shaker\n◦non-electric item commonly found in the kitchen: pot, pan, utensil, bowl,\netc\n◦(70) strainer\n◦(71) frying pan (skillet)\n◦(72) bowl: a dish for serving food that is round, open at the top, and has\nno handles (please do not confuse with a cup, which usually has a handle\nand is used for serving drinks)\n◦(73) salt or pepper shaker: a shaker with a perforated top for sprinkling\nsalt or pepper\n◦(74) plate rack\n◦(75) spatula: a turner with a narrow flexible blade\n◦(76) ladle: a spoon-shaped vessel with a long handle; frequently used to\ntransfer liquids from one container to another\n◦(77) refrigerator, icebox\n• furniture (including benches)\n38\nOlga Russakovsky* et al.\n◦(78) bookshelf: a shelf on which to keep books\n◦(79) baby bed: small bed for babies, enclosed by sides to prevent baby from\nfalling\n◦(80) filing cabinet: office furniture consisting of a container for keeping\npapers in order\n◦(81) bench (a long seat for several people, typically made of wood or stone)\n◦(82) chair: a raised piece of furniture for one person to sit on; please do not\nconfuse with benches or sofas, which are made for more people\n◦(83) sofa, couch: upholstered seat for more than one person; please do not\nconfuse with benches (which are made of wood or stone) or with chairs (which\nare for just one person)\n◦(84) table\n• clothing, article of clothing: a covering designed to be worn on a person’s body\n◦(85) diaper: Garment consisting of a folded cloth drawn up between the legs\nand fastened at the waist; worn by infants to catch excrement\n◦swimming attire: clothes used for swimming or bathing (swim suits, swim\ntrunks, bathing caps)\n◦(86) swimming trunks: swimsuit worn by men while swimming\n◦(87) bathing cap, swimming cap: a cap worn to keep hair dry while swim-\nming or showering\n◦(88) maillot: a woman’s one-piece bathing suit\n◦necktie: a man’s formal article of clothing worn around the neck (including\nbow ties)\n◦(89) bow tie: a man’s tie that ties in a bow\n◦(90) tie: a long piece of cloth worn for decorative purposes around the\nneck or shoulders, resting under the shirt collar and knotted at the throat\n(NOT a bow tie)\n◦headdress, headgear: clothing for the head (hats, helmets, bathing caps, etc)\n◦(87) bathing cap, swimming cap: a cap worn to keep hair dry while swim-\nming or showering\n◦(91) hat with a wide brim\n◦(92) helmet: protective headgear made of hard material to resist blows\n◦(93) miniskirt, mini: a very short skirt\n◦(94) brassiere, bra: an undergarment worn by women to support their breasts\n◦(95) sunglasses\n• living organism (other than people): dogs, snakes, fish, insects, sea urchins,\nstarfish, etc.\n◦living organism which can fly\n◦(96) bee\n◦(97) dragonfly\n◦(98) ladybug\n◦(99) butterfly\n◦(100) bird\n◦living organism which cannot fly (please don’t include humans)\n◦living organism with 2 or 4 legs (please don’t include humans):\n◦mammals (but please do not include humans)\n◦feline (cat-like) animal: cat, tiger or lion\n◦(101) domestic cat\n◦(102) tiger\n◦(103) lion\n◦canine (dog-like animal): dog, hyena, fox or wolf\n◦(104) dog, domestic dog, canis familiaris\n◦(105) fox: wild carnivorous mammal with pointed muzzle and ears\nand a bushy tail (please do not confuse with dogs)\n◦animals with hooves: camels, elephants, hippos, pigs, sheep, etc\n◦(106) elephant\n◦(107) hippopotamus, hippo\n◦(108) camel\n◦(109) swine: pig or boar\n◦(110) sheep: woolly animal, males have large spiraling horns (please\ndo not confuse with antelope which have long legs)\n◦(111) cattle: cows or oxen (domestic bovine animals)\n◦(112) zebra\n◦(113) horse\n◦(114) antelope: a graceful animal with long legs and horns directed\nupward and backward\n◦(115) squirrel\n◦(116) hamster: short-tailed burrowing rodent with large cheek pouches\n◦(117) otter\n◦(118) monkey\n◦(119) koala bear\n◦(120) bear (other than pandas)\n◦(121) skunk (mammal known for its ability fo spray a liquid with a\nstrong odor; they may have a single thick stripe across back and tail,\ntwo thinner stripes, or a series of white spots and broken stripes\n◦(122) rabbit\n◦(123) giant panda: an animal characterized by its distinct black and\nwhite markings\n◦(124) red panda: Reddish-brown Old World raccoon-like carnivore\n◦(125) frog, toad\n◦(126) lizard: please do not confuse with snake (lizards have legs)\n◦(127) turtle\n◦(128) armadillo\n◦(129) porcupine, hedgehog\n◦living organism with 6 or more legs: lobster, scorpion, insects, etc.\n◦(130) lobster: large marine crustaceans with long bodies and muscular\ntails; three of their five pairs of legs have claws\n◦(131) scorpion\n◦(132) centipede: an arthropod having a flattened body of 15 to 173\nsegments each with a pair of legs, the foremost pair being modified as\nprehensors\n◦(133) tick (a small creature with 4 pairs of legs which lives on the blood\nof mammals and birds)\n◦(134) isopod: a small crustacean with seven pairs of legs adapted for\ncrawling\n◦(135) ant\n◦living organism without legs: fish, snake, seal, etc. (please don’t include\nplants)\n◦living organism that lives in water: seal, whale, fish, sea cucumber, etc.\n◦(136) jellyfish\n◦(137) starfish, sea star\n◦(138) seal\n◦(139) whale\n◦(140) ray: a marine animal with a horizontally flattened body and\nenlarged winglike pectoral fins with gills on the underside\n◦(141) goldfish: small golden or orange-red fishes\n◦living organism that slides on land: worm, snail, snake\n◦(142) snail\n◦(143) snake: please do not confuse with lizard (snakes do not have\nlegs)\n• vehicle: any object used to move people or objects from place to place\n◦a vehicle with wheels\n◦(144) golfcart, golf cart\n◦(145) snowplow: a vehicle used to push snow from roads\n◦(146) motorcycle (or moped)\n◦(147) car, automobile (not a golf cart or a bus)\n◦(148) bus: a vehicle carrying many passengers; used for public transport\n◦(149) train\n◦(150) cart: a heavy open wagon usually having two wheels and drawn by\nan animal\n◦(151) bicycle, bike: a two wheeled vehicle moved by foot pedals\n◦(152) unicycle, monocycle\n◦a vehicle without wheels (snowmobile, sleighs)\n◦(153) snowmobile: tracked vehicle for travel on snow\n◦(154) watercraft (such as ship or boat): a craft designed for water trans-\nportation\n◦(155) airplane: an aircraft powered by propellers or jets\n• cosmetics: toiletry designed to beautify the body\n◦(156) face powder\n◦(157) perfume, essence (usually comes in a smaller bottle than hair spray\n◦(158) hair spray\n◦(159) cream, ointment, lotion\n◦(160) lipstick, lip rouge\n• carpentry items: items used in carpentry, including nails, hammers, axes,\nscrewdrivers, drills, chain saws, etc\n◦(161) chain saw, chainsaw\n◦(162) nail: pin-shaped with a head on one end and a point on the other\n◦(163) axe: a sharp tool often used to cut trees/ logs\n◦(164) hammer: a blunt hand tool used to drive nails in or break things apart\n(please do not confuse with axe, which is sharp)\n◦(165) screwdriver\n◦(166) power drill: a power tool for drilling holes into hard materials\n• school supplies: rulers, erasers, pencil sharpeners, pencil boxes, binders\n◦(167) ruler,rule: measuring stick consisting of a strip of wood or metal or\nplastic with a straight edge that is used for drawing straight lines and mea-\nsuring lengths\n◦(168) rubber eraser, rubber, pencil eraser\n◦(169) pencil sharpener\n◦(170) pencil box, pencil case\n◦(171) binder, ring-binder\n• sports items: items used to play sports or in the gym (such as skis, raquets,\ngymnastics bars, bows, punching bags, balls)\n◦(172) bow: weapon for shooting arrows, composed of a curved piece of re-\nsilient wood with a taut cord to propel the arrow\n◦(173) puck, hockey puck: vulcanized rubber disk 3 inches in diameter that\nis used instead of a ball in ice hockey\n◦(174) ski\n◦(175) racket, racquet\n◦gymnastic equipment: parallel bars, high beam, etc\n◦(176) balance beam: a horizontal bar used for gymnastics which is raised\nfrom the floor and wide enough to walk on\n◦(177) horizontal bar, high bar: used for gymnastics; gymnasts grip it with\ntheir hands (please do not confuse with balance beam, which is wide enough\nto walk on)\n◦ball\n◦(178) golf ball\n◦(179) baseball\n◦(180) basketball\n◦(181) croquet ball\n◦(182) soccer ball\n◦(183) ping-pong ball\n◦(184) rugby ball\n◦(185) volleyball\n◦(186) tennis ball\n◦(187) punching bag, punch bag, punching ball, punchball\n◦(188) dumbbell: An exercising weight; two spheres connected by a short bar\nthat serves as a handle\n• liquid container: vessels which commonly contain liquids such as bottles, cans,\netc.\n◦(189) pitcher: a vessel with a handle and a spout for pouring\n◦(190) beaker: a flatbottomed jar made of glass or plastic; used for chemistry\n◦(191) milk can\n◦(192) soap dispenser\n◦(193) wine bottle\n◦(194) water bottle\n◦(195) cup or mug (usually with a handle and usually cylindrical)\n• bag\n◦(196) backpack: a bag carried by a strap on your back or shoulder\n◦(197) purse: a small bag for carrying money\n◦(198) plastic bag\n• (199) person\n• (200) flower pot: a container in which plants are cultivated\nAppendix E\nModiﬁcation to bounding box\nsystem for object detection\nThe bounding box annotation system described in Sec-\ntion 3.2.1 is used for annotating images for both the\nsingle-object localization dataset and the object de-\ntection dataset. However, two additional manual post-\nprocessing are needed to ensure accuracy in the object\ndetection scenario:\nImageNet Large Scale Visual Recognition Challenge\n39\nAmbiguous objects. The ﬁrst common source of error\nwas that workers were not able to accurately diﬀerenti-\nate some object classes during annotation. Some com-\nmonly confused labels were seal and sea otter, backpack\nand purse, banjo and guitar, violin and cello, brass in-\nstruments (trumpet, trombone, french horn and brass),\nﬂute and oboe, ladle and spatula. Despite our best ef-\nforts (providing positive and negative example images\nin the annotation task, adding text explanations to alert\nthe user to the distinction between these categories)\nthese errors persisted.\nIn the single-object localization setting, this prob-\nlem was not as prominent for two reasons. First, the\nway the data was collected imposed a strong prior on\nthe object class which was present. Second, since only\none object category needed to be annotated per image,\nambiguous images could be discarded: for example, if\nworkers couldn’t agree on whether or not a trumpet was\nin fact present, this image could simply be removed. In\ncontrast, for the object detection setting consensus had\nto be reached for all target categories on all images.\nTo ﬁx this problem, once bounding box annota-\ntions were collected we manually looked through all\ncases where the bounding boxes for two diﬀerent object\nclasses had signiﬁcant overlap with each other (about\n3% of the collected boxes). About a quarter of these\nboxes were found to correspond to incorrect objects\nand were removed. Crowdsourcing this post-processing\nstep (with very stringent accuracy constraints) would\nbe possible but it occurred in few enough cases that it\nwas faster (and more accurate) to do this in-house.\nDuplicate annotations. The second common source of\nerror were duplicate bounding boxes drawn on the same\nobject instance. Despite instructions not to draw more\nthan one bounding box around the same object instance\nand constraints in the annotation UI enforcing at least\na 5 pixel diﬀerence between diﬀerent bounding boxes,\nthese errors persisted. One reason was that sometimes\nthe initial bounding box was not perfect and subsequent\nlabelers drew a slightly improved alternative.\nThis type of error was also present in the single-\nobject localization scenario but was not a major cause\nfor concern. A duplicate bounding box is a slightly per-\nturbed but still correct positive example, and single-\nobject localization is only concerned with correctly lo-\ncalizing one object instance. For the detection task algo-\nrithms are evaluated on the ability to localize every ob-\nject instance, and penalized for duplicate detections, so\nit is imperative that these labeling errors are corrected\n(even if they only appear in about 0.6% of cases).\nApproximately 1% of bounding boxes were found\nto have signiﬁcant overlap of more than 50% with an-\nother bounding box of the same object class.We again\nmanually veriﬁed all of these cases in-house. In approx-\nimately 40% of the cases the two bounding boxes cor-\nrectly corresponded to diﬀerent people in a crowd, to\nstacked plates, or to musical instruments nearby in an\norchestra. In the other 60% of cases one of the boxes\nwas randomly removed.\nThese veriﬁcation steps complete the annotation pro-\ncedure of bounding boxes around every instance of ev-\nery object class in validation, test and a subset of train-\ning images for the detection task.\nTraining set annotation. With the optimized algorithm\nof Section 3.3.3 we fully annotated the validation and\ntest sets. However, annotating all training images with\nall target object classes was still a budget challenge.\nPositive training images taken from the single-object\nlocalization dataset already had bounding box annota-\ntions of all instances of one object class on each im-\nage. We extended the existing annotations to the de-\ntection dataset by making two modiﬁcation. First, we\ncorrected any bounding box omissions resulting from\nmerging ﬁne-grained categories: i.e., if an image be-\nlonged to the ”dalmatian” category and all instances of\n”dalmatian” were annotated with bounding boxes for\nsingle-object localization, we ensured that all remain-\ning ”dog” instances are also annotated for the object\ndetection task. Second, we collected signiﬁcantly more\ntraining data for the person class because the existing\nannotation set was not diverse enough to be representa-\ntive (the only people categories in the single-object lo-\ncalization task are scuba diver, groom, and ballplayer).\nTo compensate, we additionally annotated people in a\nlarge fraction of the existing training set images.\nAppendix F\nCompetition protocol\nCompetition format. At the beginning of the competi-\ntion period each year we release the new training/validation/test\nimages, training/validation annotations, and competi-\ntion speciﬁcation for the year. We then specify a dead-\nline for submission, usually approximately 4 months af-\nter the release of data. Teams are asked to upload a\ntext ﬁle of their predicted annotations on test images\nby this deadline to a provided server. We then evaluate\nall submissions and release the results.\nFor every task we released code that takes a text ﬁle\nof automatically generated image annotations and com-\npares it with the ground truth annotations to return a\nquantitative measure of algorithm accuracy. Teams can\nuse this code to evaluate their performance on the val-\nidation data.\n40\nOlga Russakovsky* et al.\nAs described in (Everingham et al., 2014), there are\nthree options for measuring performance on test data:\n(i) Release test images and annotations, and allow par-\nticipants to assess performance themselves; (ii) Release\ntest images but not test annotations – participants sub-\nmit results and organizers assess performance; (iii) Nei-\nther test images nor annotations are released – partic-\nipants submit software and organizers run it on new\ndata and assess performance. In line with the PASCAL\nVOC choice, we opted for option (ii). Option (i) allows\ntoo much leeway in overﬁtting to the test data; option\n(iii) is infeasible, especially given the scale of our test\nset (40K-100K images).\nWe released ILSVRC2010 test annotations for the\nimage classiﬁcation task, but all other test annotations\nhave remained hidden to discourage ﬁne-tuning results\non the test data.\nEvaluation protocol after the challenge. After the chal-\nlenge period we set up an automatic evaluation server\nthat researchers can use throughout the year to con-\ntinue evaluating their algorithms against the ground\ntruth test annotations. We limit teams to 2 submis-\nsions per week to discourage parameter tuning on the\ntest data, and in practice we have never had a problem\nwith researchers abusing the system.\n*Bibliography\nAhonen, T., Hadid, A., and Pietikinen, M. (2006). Face\ndescription with local binary patterns: Application to\nface recognition. PAMI, 28.\nAlexe, B., Deselares, T., and Ferrari, V. (2012). Mea-\nsuring the objectness of image windows. In PAMI.\nArandjelovic, R. and Zisserman, A. (2012).\nThree\nthings everyone should know to improve object re-\ntrieval. In CVPR.\nArbelaez, P., Maire, M., Fowlkes, C., and Malik, J.\n(2011). Contour detection and hierarchical image seg-\nmentation. IEEE TPAMI, 33.\nArbel´aez, P., Pont-Tuset, J., Barron, J., Marques,\nF., and Malik, J. (2014). Multiscale combinatorial\ngrouping. In Computer Vision and Pattern Recogni-\ntion.\nBatra, D., Agrawal, H., Banik, P., Chavali, N., Math-\nialagan, C. S., and Alfadda, A. (2013).\nCloudcv:\nLarge-scale distributed computer vision as a cloud\nservice.\nBell, S., Upchurch, P., Snavely, N., and Bala, K. (2013).\nOpenSurfaces: A richly annotated catalog of sur-\nface appearance. In ACM Transactions on Graphics\n(SIGGRAPH).\nBerg, A., Farrell, R., Khosla, A., Krause, J., Fei-Fei, L.,\nLi, J., and Maji, S. (2013). Fine-Grained Competi-\ntion. https://sites.google.com/site/fgcomp2013/.\nChatﬁeld, K., Simonyan, K., Vedaldi, A., and Zisser-\nman, A. (2014).\nReturn of the devil in the de-\ntails: Delving deep into convolutional nets. CoRR,\nabs/1405.3531.\nChen, Q., Song, Z., Huang, Z., Hua, Y., and Yan, S.\n(2014). Contextualizing object detection and classi-\nﬁcation. volume PP.\nCrammer, K., Dekel, O., Keshet, J., Shalev-Shwartz,\nS., and Singer, Y. (2006). Online passive-aggressive\nalgorithms. Journal of Machine Learning Research,\n7:551–585.\nCriminisi, A. (2004).\nMicrosoft Research Cambridge\n(MSRC) object recognition image database (version\n2.0).\nhttp://research.microsoft.com/vision/\ncambridge/recognition.\nDean, T., Ruzon, M., Segal, M., Shlens, J., Vijaya-\nnarasimhan, S., and Yagnik, J. (2013). Fast, accu-\nrate detection of 100,000 object classes on a single\nmachine. In CVPR.\nDeng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and\nFei-Fei, L. (2009). ImageNet: a large-scale hierarchi-\ncal image database. In CVPR.\nDeng, J., Russakovsky, O., Krause, J., Bernstein, M.,\nBerg, A. C., and Fei-Fei, L. (2014). Scalable multi-\nlabel annotation. In CHI.\nDonahue, J., Jia, Y., Vinyals, O., Hoﬀman, J., Zhang,\nN., Tzeng, E., and Darrell, T. (2013).\nDecaf: A\ndeep convolutional activation feature for generic vi-\nsual recognition. CoRR, abs/1310.1531.\nDubout, C. and Fleuret, F. (2012). Exact acceleration\nof linear object detectors. In Proceedings of the Eu-\nropean Conference on Computer Vision (ECCV).\nEveringham, M., , Eslami, S. M. A., Van Gool, L.,\nWilliams, C. K. I., Winn, J., and Zisserman, A.\n(2014).\nThe Pascal Visual Object Classes (VOC)\nchallenge - a Retrospective. IJCV.\nEveringham, M., Gool, L. V., Williams, C., Winn, J.,\nand Zisserman, A. (2005-2012). PASCAL Visual Ob-\nject Classes Challenge (VOC). http://www.pascal-\nnetwork.org/challenges/VOC/voc2012/workshop/index.html.\nEveringham, M., Van Gool, L., Williams, C. K. I.,\nWinn, J., and Zisserman, A. (2010).\nThe Pas-\ncal Visual Object Classes (VOC) challenge. IJCV,\n88(2):303–338.\nFei-Fei, L., Fergus, R., and Perona, P. (2004). Learn-\ning generative visual models from few examples: an\nincremental bayesian approach tested on 101 object\ncategories. In CVPR.\nFei-Fei, L. and Perona, P. (2005). A bayesian hierar-\nchical model for learning natural scene categories. In\nCVPR.\nFelzenszwalb, P., Girshick, R., McAllester, D., and Ra-\nmanan, D. (2010). Object detection with discrimina-\nImageNet Large Scale Visual Recognition Challenge\n41\ntively trained part based models. PAMI, 32.\nFrome, A., Corrado, G., Shlens, J., Bengio, S., Dean,\nJ., Ranzato, M., and Mikolov, T. (2013). Devise: A\ndeep visual-semantic embedding model. In Advances\nIn Neural Information Processing Systems, NIPS.\nGeiger, A., Lenz, P., Stiller, C., and Urtasun, R. (2013).\nVision meets robotics: The kitti dataset.\nInterna-\ntional Journal of Robotics Research (IJRR).\nGirshick, R., Donahue, J., Darrell, T., and Malik., J.\n(2014). Rich feature hierarchies for accurate object\ndetection and semantic segmentation. In CVPR.\nGirshick, R. B., Donahue, J., Darrell, T., and Malik, J.\n(2013). Rich feature hierarchies for accurate object\ndetection and semantic segmentation (v4). CoRR.\nGould, S., Fulton, R., and Koller, D. (2009). Decom-\nposing a scene into geometric and semantically con-\nsistent regions. In ICCV.\nGraham, B. (2013). Sparse arrays of signatures for on-\nline character recognition. CoRR.\nGriﬃn, G., Holub, A., and Perona, P. (2007). Caltech-\n256 object category dataset. Technical Report 7694,\nCaltech.\nHarada, T. and Kuniyoshi, Y. (2012). Graphical gaus-\nsian vector for image categorization. In NIPS.\nHarel, J., Koch, C., and Perona, P. (2007). Graph-based\nvisual saliency. In NIPS.\nHe, K., Zhang, X., Ren, S., , and Su, J. (2014). Spatial\npyramid pooling in deep convolutional networks for\nvisual recognition. In ECCV.\nHinton,\nG.\nE.,\nSrivastava,\nN.,\nKrizhevsky,\nA.,\nSutskever, I., and Salakhutdinov, R. (2012). Improv-\ning neural networks by preventing co-adaptation of\nfeature detectors. CoRR, abs/1207.0580.\nHoiem, D., Chodpathumwan, Y., and Dai, Q. (2012).\nDiagnosing error in object detectors. In ECCV.\nHoward, A. (2014). Some improvements on deep con-\nvolutional neural network based image classiﬁcation.\nICLR.\nHuang, G. B., Ramesh, M., Berg, T., and Learned-\nMiller, E. (2007).\nLabeled faces in the wild: A\ndatabase for studying face recognition in uncon-\nstrained environments. Technical Report 07-49, Uni-\nversity of Massachusetts, Amherst.\nIandola, F. N., Moskewicz, M. W., Karayev, S., Gir-\nshick, R. B., Darrell, T., and Keutzer, K. (2014).\nDensenet: Implementing eﬃcient convnet descriptor\npyramids. CoRR.\nJia, Y. (2013).\nCaﬀe: An open source convolutional\narchitecture for fast feature embedding.\nhttp://\ncaffe.berkeleyvision.org/.\nJojic, N., Frey, B. J., and Kannan, A. (2003). Epitomic\nanalysis of appearance and shape. In ICCV.\nKanezaki, A., Inaba, S., Ushiku, Y., Yamashita, Y.,\nMuraoka, H., Kuniyoshi, Y., and Harada, T. (2014).\nHard negative classes for multiple object detection.\nIn ICRA.\nKhosla, A., Jayadevaprakash, N., Yao, B., and Fei-Fei,\nL. (2011). Novel dataset for ﬁne-grained image cat-\negorization. In First Workshop on Fine-Grained Vi-\nsual Categorization, CVPR.\nKrizhevsky, A., Sutskever, I., and Hinton, G. (2012).\nImageNet classiﬁcation with deep convolutional neu-\nral networks. In NIPS.\nKuettel, D., Guillaumin, M., and Ferrari, V. (2012).\nSegmentation Propagation in ImageNet. In eccv.\nLazebnik, S., Schmid, C., and Ponce, J. (2006).\nBe-\nyond bags of features: Spatial Pyramid Matching for\nrecognizing natural scene categories. In CVPR.\nLin, M., Chen, Q., and Yan, S. (2014a). Network in\nnetwork. ICLR.\nLin, T.-Y., Maire, M., Belongie, S., Hays, J., Perona, P.,\nRamanan, D., Dollr, P., and Zitnick, C. L. (2014b).\nMicrosoft COCO: Common Objects in Context. In\nECCV.\nLin, Y., Lv, F., Cao, L., Zhu, S., Yang, M., Cour, T.,\nYu, K., and Huang, T. (2011). Large-scale image clas-\nsiﬁcation: Fast feature extraction and SVM training.\nIn CVPR.\nLiu, C., Yuen, J., and Torralba, A. (2011). Nonparamet-\nric scene parsing via label transfer. IEEE Transac-\ntions on Pattern Analysis and Machine Intelligence,\n33(12).\nLowe, D. G. (2004). Distinctive image features from\nscale-invariant keypoints. IJCV, 60(2):91–110.\nMaji, S. and Malik, J. (2009). Object detection using\na max-margin hough transform. In CVPR.\nManen, S., Guillaumin, M., and Van Gool, L. (2013).\nPrime Object Proposals with Randomized Prim’s Al-\ngorithm. In ICCV.\nMensink, T., Verbeek, J., Perronnin, F., and Csurka,\nG. (2012).\nMetric Learning for Large Scale Image\nClassiﬁcation: Generalizing to New Classes at Near-\nZero Cost. In ECCV.\nMikolov, T., Chen, K., Corrado, G., and Dean, J.\n(2013). Eﬃcient estimation of word representations\nin vector space. ICLR.\nMiller, G. A. (1995). Wordnet: A lexical database for\nenglish. Commun. ACM, 38(11).\nOliva, A. and Torralba, A. (2001). Modeling the shape\nof the scene: A holistic representation of the spatial\nenvelope. IJCV.\nOrdonez, V., Deng, J., Choi, Y., Berg, A. C., and Berg,\nT. L. (2013). From large scale image categorization\nto entry-level categories. In IEEE International Con-\nference on Computer Vision (ICCV).\n42\nOlga Russakovsky* et al.\nOuyang, W., Luo, P., Zeng, X., Qiu, S., Tian, Y., Li,\nH., Yang, S., Wang, Z., Xiong, Y., Qian, C., Zhu,\nZ., Wang, R., Loy, C. C., Wang, X., and Tang, X.\n(2014). Deepid-net: multi-stage and deformable deep\nconvolutional neural networks for object detection.\nCoRR, abs/1409.3505.\nOuyang, W. and Wang, X. (2013). Joint deep learning\nfor pedestrian detection. In ICCV.\nPapandreou, G. (2014). Deep epitomic convolutional\nneural networks. CoRR.\nPapandreou, G., Chen, L.-C., and Yuille, A. L. (2014).\nModeling image patches with a generic dictionary of\nmini-epitomes.\nPerronnin, F., Akata, Z., Harchaoui, Z., and Schmid, C.\n(2012). Towards good practice in large-scale learning\nfor image classiﬁcation. In CVPR.\nPerronnin, F. and Dance, C. R. (2007). Fisher kernels\non visual vocabularies for image categorization. In\nCVPR.\nPerronnin, F., S´anchez, J., and Mensink, T. (2010). Im-\nproving the ﬁsher kernel for large-scale image classi-\nﬁcation. In ECCV (4).\nRussakovsky, O., Deng, J., Huang, Z., Berg, A., and Fei-\nFei, L. (2013). Detecting avocados to zucchinis: what\nhave we done, and where are we going? In ICCV.\nRussell, B., Torralba, A., Murphy, K., and Freeman,\nW. T. (2007). LabelMe: a database and web-based\ntool for image annotation. IJCV.\nSanchez, J. and Perronnin, F. (2011). High-dim. signa-\nture compression for large-scale image classiﬁcation.\nIn CVPR.\nSanchez, J., Perronnin, F., and de Campos, T. (2012).\nModeling spatial layout of images beyond spatial\npyramids. In PRL.\nScheirer, W., Kumar, N., Belhumeur, P. N., and Boult,\nT. E. (2012). Multi-attribute spaces: Calibration for\nattribute fusion and similarity search. In CVPR.\nSchmidhuber, J. (2012). Multi-column deep neural net-\nworks for image classiﬁcation. In CVPR.\nSermanet, P., Eigen, D., Zhang, X., Mathieu, M., Fer-\ngus, R., and LeCun, Y. (2013). Overfeat: Integrated\nrecognition, localization and detection using convo-\nlutional networks. CoRR, abs/1312.6229.\nSheng, V. S., Provost, F., and Ipeirotis, P. G. (2008).\nGet another label? Improving data quality and data\nmining using multiple, noisy labelers. In SIGKDD.\nSimonyan, K., Vedaldi, A., and Zisserman, A. (2013).\nDeep ﬁsher networks for large-scale image classiﬁca-\ntion. In NIPS.\nSimonyan, K. and Zisserman, A. (2014). Very deep con-\nvolutional networks for large-scale image recognition.\nCoRR, abs/1409.1556.\nSorokin, A. and Forsyth, D. (2008). Utility data anno-\ntation with Amazon Mechanical Turk. In InterNet08.\nSu, H., Deng, J., and Fei-Fei, L. (2012). Crowdsourc-\ning annotations for visual object detection. In AAAI\nHuman Computation Workshop.\nSzegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S.,\nAnguelov, D., Erhan, D., and Rabinovich, A. (2014).\nGoing deeper with convolutions. Technical report.\nTang, Y. (2013). Deep learning using support vector\nmachines. CoRR, abs/1306.0239.\nThorpe, S., Fize, D., Marlot, C., et al. (1996). Speed\nof processing in the human visual system.\nnature,\n381(6582):520–522.\nTorralba, A. and Efros, A. A. (2011). Unbiased look at\ndataset bias. In CVPR’11.\nTorralba, A., Fergus, R., and Freeman, W. (2008). 80\nmillion tiny images: A large data set for nonparamet-\nric object and scene recognition. In PAMI.\nUijlings, J., van de Sande, K., Gevers, T., and Smeul-\nders, A. (2013). Selective search for object recogni-\ntion. International Journal of Computer Vision.\nUrtasun, R., Fergus, R., Hoiem, D., Torralba, A.,\nGeiger, A., Lenz, P., Silberman, N., Xiao, J.,\nand Fidler, S. (2013-2014).\nReconstruction meets\nrecognition challenge. http://ttic.uchicago.edu/\n~rurtasun/rmrc/.\nvan de Sande, K. E. A., Gevers, T., and Snoek, C.\nG. M. (2010). Evaluating color descriptors for object\nand scene recognition. IEEE Transactions on Pattern\nAnalysis and Machine Intelligence, 32(9):1582–1596.\nvan de Sande, K. E. A., Gevers, T., and Snoek, C. G. M.\n(2011a). Empowering visual categorization with the\ngpu. IEEE Transactions on Multimedia, 13(1):60–70.\nvan de Sande, K. E. A., Snoek, C. G. M., and Smeul-\nders, A. W. M. (2014). Fisher and vlad with ﬂair.\nIn Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition.\nvan de Sande, K. E. A., Uijlings, J. R. R., Gevers, T.,\nand Smeulders, A. W. M. (2011b). Segmentation as\nselective search for object recognition. In ICCV.\nVittayakorn, S. and Hays, J. (2011). Quality assessment\nfor crowdsourced object annotations. In BMVC.\nvon Ahn, L. and Dabbish, L. (2005). Esp: Labeling im-\nages with a computer game. In AAAI Spring Sympo-\nsium: Knowledge Collection from Volunteer Contrib-\nutors.\nVondrick, C., Patterson, D., and Ramanan, D. (2012).\nEﬃciently scaling up crowdsourced video annotation.\nInternational Journal of Computer Vision.\nWan, L., Zeiler, M., Zhang, S., LeCun, Y., and Fergus,\nR. (2013). Regularization of neural networks using\ndropconnect. In Proc. International Conference on\nMachine learning (ICML’13).\nImageNet Large Scale Visual Recognition Challenge\n43\nWang, J., Yang, J., Yu, K., Lv, F., Huang, T., and\nGong, Y. (2010). Locality-constrained Linear Coding\nfor image classiﬁcation. In CVPR.\nWang, M., Xiao, T., Li, J., Hong, C., Zhang, J., and\nZhang, Z. (2014). Minerva: A scalable and highly ef-\nﬁcient training platform for deep learning. In APSys.\nWang, X., Yang, M., Zhu, S., and Lin, Y. (2013). Re-\ngionlets for generic object detection. In ICCV.\nWelinder, P., Branson, S., Belongie, S., and Perona, P.\n(2010). The multidimensional wisdom of crowds. In\nNIPS.\nXiao, J., Hays, J., Ehinger, K., Oliva, A., and Torralba.,\nA. (2010). SUN database: Large-scale scene recogni-\ntion from Abbey to Zoo. CVPR.\nYang, J., Yu, K., Gong, Y., and Huang, T. (2009). Lin-\near spatial pyramid matching using sparse coding for\nimage classiﬁcation. In CVPR.\nYao, B., Yang, X., and Zhu, S.-C. (2007). Introduction\nto a large scale general purpose ground truth dataset:\nmethodology, annotation tool, and benchmarks.\nZeiler, M. D. and Fergus, R. (2013).\nVisualizing\nand understanding convolutional networks. CoRR,\nabs/1311.2901.\nZeiler, M. D., Taylor, G. W., and Fergus, R. (2011).\nAdaptive deconvolutional networks for mid and high\nlevel feature learning. In ICCV.\nZhou, B., Lapedriza, A., Xiao, J., Torralba, A., and\nOliva, A. (2014). Learning deep features for scene\nrecognition using places database. NIPS.\nZhou, X., Yu, K., Zhang, T., and Huang, T. (2010).\nImage classiﬁcation using super-vector coding of local\nimage descriptors. In ECCV.\n",
        "sentence": " In recent years, Deep Neural Networks (DNNs), especially Convolutional Neural Networks (CNNs), have demonstrated highly competitive results on object recognition and image classification [1, 2, 3, 4]. 3 million-image ILSVRC 2012 ImageNet dataset [1] using the Caffe-framework [21]. We select several source images from the ILSVRC 2012 challenge [1] validation data as examples for the inversion task, and choose a monkey image as the reference image to build the stacked ranVGG3. [1 3] on V G G Night Starry Der Schrei Photograph Picasso Woman with a Hat Meadow with Poplars"
    },
    {
        "title": "Imagenet classification with deep convolutional neural networks",
        "author": [
            "Alex Krizhevsky",
            "Ilya Sutskever",
            "Geoffrey E. Hinton"
        ],
        "venue": "In NIPS,",
        "citeRegEx": "2",
        "shortCiteRegEx": "2",
        "year": 2012,
        "abstract": "We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5% and 17.0%, respectively, which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully connected layers we employed a recently developed regularization method called \"dropout\" that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3%, compared to 26.2% achieved by the second-best entry.",
        "full_text": "",
        "sentence": " In recent years, Deep Neural Networks (DNNs), especially Convolutional Neural Networks (CNNs), have demonstrated highly competitive results on object recognition and image classification [1, 2, 3, 4]. [7] use the pretrained CNN AlexNet [2] and define a squared Euclidean loss on the activations to capture the representation differences and reconstruct the image. use a reference network AlexNet [2] which contains 8 layers of trained weights (5 convolutional layers and 3 fully connected layers), plus 3 pooling layers."
    },
    {
        "title": "Very deep convolutional networks for large-scale image recognition",
        "author": [
            "K. Simonyan",
            "A. Zisserman"
        ],
        "venue": "arXiv preprint arXiv:1409.1556,",
        "citeRegEx": "3",
        "shortCiteRegEx": "3",
        "year": 2014,
        "abstract": "",
        "full_text": "",
        "sentence": " In recent years, Deep Neural Networks (DNNs), especially Convolutional Neural Networks (CNNs), have demonstrated highly competitive results on object recognition and image classification [1, 2, 3, 4]. [8, 12] define a squared loss on the correlations between feature maps of some layers and synthesize natural textures of high perceptual quality using the pretrained CNN called VGG [3]. VGG-19 [3] is a convolutional neural network trained on the 1. [1 3] on V G G Night Starry Der Schrei Photograph Picasso Woman with a Hat Meadow with Poplars"
    },
    {
        "title": "Deep residual learning for image recognition",
        "author": [
            "Kaiming He",
            "Xiangyu Zhang",
            "Shaoqing Ren",
            "Jian Sun"
        ],
        "venue": "arXiv preprint arXiv:1512.03385,",
        "citeRegEx": "4",
        "shortCiteRegEx": "4",
        "year": 2015,
        "abstract": "Deeper neural networks are more difficult to train. We present a residual\nlearning framework to ease the training of networks that are substantially\ndeeper than those used previously. We explicitly reformulate the layers as\nlearning residual functions with reference to the layer inputs, instead of\nlearning unreferenced functions. We provide comprehensive empirical evidence\nshowing that these residual networks are easier to optimize, and can gain\naccuracy from considerably increased depth. On the ImageNet dataset we evaluate\nresidual nets with a depth of up to 152 layers---8x deeper than VGG nets but\nstill having lower complexity. An ensemble of these residual nets achieves\n3.57% error on the ImageNet test set. This result won the 1st place on the\nILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100\nand 1000 layers.\n  The depth of representations is of central importance for many visual\nrecognition tasks. Solely due to our extremely deep representations, we obtain\na 28% relative improvement on the COCO object detection dataset. Deep residual\nnets are foundations of our submissions to ILSVRC & COCO 2015 competitions,\nwhere we also won the 1st places on the tasks of ImageNet detection, ImageNet\nlocalization, COCO detection, and COCO segmentation.",
        "full_text": "Deep Residual Learning for Image Recognition\nKaiming He\nXiangyu Zhang\nShaoqing Ren\nJian Sun\nMicrosoft Research\n{kahe, v-xiangz, v-shren, jiansun}@microsoft.com\nAbstract\nDeeper neural networks are more difﬁcult to train. We\npresent a residual learning framework to ease the training\nof networks that are substantially deeper than those used\npreviously. We explicitly reformulate the layers as learn-\ning residual functions with reference to the layer inputs, in-\nstead of learning unreferenced functions. We provide com-\nprehensive empirical evidence showing that these residual\nnetworks are easier to optimize, and can gain accuracy from\nconsiderably increased depth. On the ImageNet dataset we\nevaluate residual nets with a depth of up to 152 layers—8×\ndeeper than VGG nets [41] but still having lower complex-\nity. An ensemble of these residual nets achieves 3.57% error\non the ImageNet test set. This result won the 1st place on the\nILSVRC 2015 classiﬁcation task. We also present analysis\non CIFAR-10 with 100 and 1000 layers.\nThe depth of representations is of central importance\nfor many visual recognition tasks. Solely due to our ex-\ntremely deep representations, we obtain a 28% relative im-\nprovement on the COCO object detection dataset. Deep\nresidual nets are foundations of our submissions to ILSVRC\n& COCO 2015 competitions1, where we also won the 1st\nplaces on the tasks of ImageNet detection, ImageNet local-\nization, COCO detection, and COCO segmentation.\n1. Introduction\nDeep convolutional neural networks [22, 21] have led\nto a series of breakthroughs for image classiﬁcation [21,\n50, 40]. Deep networks naturally integrate low/mid/high-\nlevel features [50] and classiﬁers in an end-to-end multi-\nlayer fashion, and the “levels” of features can be enriched\nby the number of stacked layers (depth). Recent evidence\n[41, 44] reveals that network depth is of crucial importance,\nand the leading results [41, 44, 13, 16] on the challenging\nImageNet dataset [36] all exploit “very deep” [41] models,\nwith a depth of sixteen [41] to thirty [16]. Many other non-\ntrivial visual recognition tasks [8, 12, 7, 32, 27] have also\n1http://image-net.org/challenges/LSVRC/2015/\nand\nhttp://mscoco.org/dataset/#detections-challenge2015.\n0\n1\n2\n3\n4\n5\n6\n0 \n10\n20\niter. (1e4)\ntraining error (%)\n \n \n0\n1\n2\n3\n4\n5\n6\n0\n10\n20\niter. (1e4)\ntest error (%)\n \n \n56-layer\n20-layer\n56-layer\n20-layer\nFigure 1. Training error (left) and test error (right) on CIFAR-10\nwith 20-layer and 56-layer “plain” networks. The deeper network\nhas higher training error, and thus test error. Similar phenomena\non ImageNet is presented in Fig. 4.\ngreatly beneﬁted from very deep models.\nDriven by the signiﬁcance of depth, a question arises: Is\nlearning better networks as easy as stacking more layers?\nAn obstacle to answering this question was the notorious\nproblem of vanishing/exploding gradients [1, 9], which\nhamper convergence from the beginning.\nThis problem,\nhowever, has been largely addressed by normalized initial-\nization [23, 9, 37, 13] and intermediate normalization layers\n[16], which enable networks with tens of layers to start con-\nverging for stochastic gradient descent (SGD) with back-\npropagation [22].\nWhen deeper networks are able to start converging, a\ndegradation problem has been exposed: with the network\ndepth increasing, accuracy gets saturated (which might be\nunsurprising) and then degrades rapidly.\nUnexpectedly,\nsuch degradation is not caused by overﬁtting, and adding\nmore layers to a suitably deep model leads to higher train-\ning error, as reported in [11, 42] and thoroughly veriﬁed by\nour experiments. Fig. 1 shows a typical example.\nThe degradation (of training accuracy) indicates that not\nall systems are similarly easy to optimize. Let us consider a\nshallower architecture and its deeper counterpart that adds\nmore layers onto it. There exists a solution by construction\nto the deeper model: the added layers are identity mapping,\nand the other layers are copied from the learned shallower\nmodel. The existence of this constructed solution indicates\nthat a deeper model should produce no higher training error\nthan its shallower counterpart. But experiments show that\nour current solvers on hand are unable to ﬁnd solutions that\n1\narXiv:1512.03385v1  [cs.CV]  10 Dec 2015\nidentity\nweight layer\nweight layer\nrelu\nrelu\nF(x)\u0001+\u0001x\nx\nF(x)\nx\nFigure 2. Residual learning: a building block.\nare comparably good or better than the constructed solution\n(or unable to do so in feasible time).\nIn this paper, we address the degradation problem by\nintroducing a deep residual learning framework.\nIn-\nstead of hoping each few stacked layers directly ﬁt a\ndesired underlying mapping, we explicitly let these lay-\ners ﬁt a residual mapping. Formally, denoting the desired\nunderlying mapping as H(x), we let the stacked nonlinear\nlayers ﬁt another mapping of F(x) := H(x)−x. The orig-\ninal mapping is recast into F(x)+x. We hypothesize that it\nis easier to optimize the residual mapping than to optimize\nthe original, unreferenced mapping. To the extreme, if an\nidentity mapping were optimal, it would be easier to push\nthe residual to zero than to ﬁt an identity mapping by a stack\nof nonlinear layers.\nThe formulation of F(x)+x can be realized by feedfor-\nward neural networks with “shortcut connections” (Fig. 2).\nShortcut connections [2, 34, 49] are those skipping one or\nmore layers. In our case, the shortcut connections simply\nperform identity mapping, and their outputs are added to\nthe outputs of the stacked layers (Fig. 2). Identity short-\ncut connections add neither extra parameter nor computa-\ntional complexity. The entire network can still be trained\nend-to-end by SGD with backpropagation, and can be eas-\nily implemented using common libraries (e.g., Caffe [19])\nwithout modifying the solvers.\nWe present comprehensive experiments on ImageNet\n[36] to show the degradation problem and evaluate our\nmethod. We show that: 1) Our extremely deep residual nets\nare easy to optimize, but the counterpart “plain” nets (that\nsimply stack layers) exhibit higher training error when the\ndepth increases; 2) Our deep residual nets can easily enjoy\naccuracy gains from greatly increased depth, producing re-\nsults substantially better than previous networks.\nSimilar phenomena are also shown on the CIFAR-10 set\n[20], suggesting that the optimization difﬁculties and the\neffects of our method are not just akin to a particular dataset.\nWe present successfully trained models on this dataset with\nover 100 layers, and explore models with over 1000 layers.\nOn the ImageNet classiﬁcation dataset [36], we obtain\nexcellent results by extremely deep residual nets. Our 152-\nlayer residual net is the deepest network ever presented on\nImageNet, while still having lower complexity than VGG\nnets [41].\nOur ensemble has 3.57% top-5 error on the\nImageNet test set, and won the 1st place in the ILSVRC\n2015 classiﬁcation competition. The extremely deep rep-\nresentations also have excellent generalization performance\non other recognition tasks, and lead us to further win the\n1st places on: ImageNet detection, ImageNet localization,\nCOCO detection, and COCO segmentation in ILSVRC &\nCOCO 2015 competitions. This strong evidence shows that\nthe residual learning principle is generic, and we expect that\nit is applicable in other vision and non-vision problems.\n2. Related Work\nResidual Representations. In image recognition, VLAD\n[18] is a representation that encodes by the residual vectors\nwith respect to a dictionary, and Fisher Vector [30] can be\nformulated as a probabilistic version [18] of VLAD. Both\nof them are powerful shallow representations for image re-\ntrieval and classiﬁcation [4, 48]. For vector quantization,\nencoding residual vectors [17] is shown to be more effec-\ntive than encoding original vectors.\nIn low-level vision and computer graphics, for solv-\ning Partial Differential Equations (PDEs), the widely used\nMultigrid method [3] reformulates the system as subprob-\nlems at multiple scales, where each subproblem is respon-\nsible for the residual solution between a coarser and a ﬁner\nscale. An alternative to Multigrid is hierarchical basis pre-\nconditioning [45, 46], which relies on variables that repre-\nsent residual vectors between two scales. It has been shown\n[3, 45, 46] that these solvers converge much faster than stan-\ndard solvers that are unaware of the residual nature of the\nsolutions. These methods suggest that a good reformulation\nor preconditioning can simplify the optimization.\nShortcut Connections. Practices and theories that lead to\nshortcut connections [2, 34, 49] have been studied for a long\ntime. An early practice of training multi-layer perceptrons\n(MLPs) is to add a linear layer connected from the network\ninput to the output [34, 49]. In [44, 24], a few interme-\ndiate layers are directly connected to auxiliary classiﬁers\nfor addressing vanishing/exploding gradients. The papers\nof [39, 38, 31, 47] propose methods for centering layer re-\nsponses, gradients, and propagated errors, implemented by\nshortcut connections. In [44], an “inception” layer is com-\nposed of a shortcut branch and a few deeper branches.\nConcurrent with our work, “highway networks” [42, 43]\npresent shortcut connections with gating functions [15].\nThese gates are data-dependent and have parameters, in\ncontrast to our identity shortcuts that are parameter-free.\nWhen a gated shortcut is “closed” (approaching zero), the\nlayers in highway networks represent non-residual func-\ntions.\nOn the contrary, our formulation always learns\nresidual functions; our identity shortcuts are never closed,\nand all information is always passed through, with addi-\ntional residual functions to be learned. In addition, high-\n2\nway networks have not demonstrated accuracy gains with\nextremely increased depth (e.g., over 100 layers).\n3. Deep Residual Learning\n3.1. Residual Learning\nLet us consider H(x) as an underlying mapping to be\nﬁt by a few stacked layers (not necessarily the entire net),\nwith x denoting the inputs to the ﬁrst of these layers. If one\nhypothesizes that multiple nonlinear layers can asymptoti-\ncally approximate complicated functions2, then it is equiv-\nalent to hypothesize that they can asymptotically approxi-\nmate the residual functions, i.e., H(x) −x (assuming that\nthe input and output are of the same dimensions).\nSo\nrather than expect stacked layers to approximate H(x), we\nexplicitly let these layers approximate a residual function\nF(x) := H(x) −x. The original function thus becomes\nF(x)+x. Although both forms should be able to asymptot-\nically approximate the desired functions (as hypothesized),\nthe ease of learning might be different.\nThis reformulation is motivated by the counterintuitive\nphenomena about the degradation problem (Fig. 1, left). As\nwe discussed in the introduction, if the added layers can\nbe constructed as identity mappings, a deeper model should\nhave training error no greater than its shallower counter-\npart.\nThe degradation problem suggests that the solvers\nmight have difﬁculties in approximating identity mappings\nby multiple nonlinear layers. With the residual learning re-\nformulation, if identity mappings are optimal, the solvers\nmay simply drive the weights of the multiple nonlinear lay-\ners toward zero to approach identity mappings.\nIn real cases, it is unlikely that identity mappings are op-\ntimal, but our reformulation may help to precondition the\nproblem. If the optimal function is closer to an identity\nmapping than to a zero mapping, it should be easier for the\nsolver to ﬁnd the perturbations with reference to an identity\nmapping, than to learn the function as a new one. We show\nby experiments (Fig. 7) that the learned residual functions in\ngeneral have small responses, suggesting that identity map-\npings provide reasonable preconditioning.\n3.2. Identity Mapping by Shortcuts\nWe adopt residual learning to every few stacked layers.\nA building block is shown in Fig. 2. Formally, in this paper\nwe consider a building block deﬁned as:\ny = F(x, {Wi}) + x.\n(1)\nHere x and y are the input and output vectors of the lay-\ners considered.\nThe function F(x, {Wi}) represents the\nresidual mapping to be learned. For the example in Fig. 2\nthat has two layers, F = W2σ(W1x) in which σ denotes\n2This hypothesis, however, is still an open question. See [28].\nReLU [29] and the biases are omitted for simplifying no-\ntations. The operation F + x is performed by a shortcut\nconnection and element-wise addition. We adopt the sec-\nond nonlinearity after the addition (i.e., σ(y), see Fig. 2).\nThe shortcut connections in Eqn.(1) introduce neither ex-\ntra parameter nor computation complexity. This is not only\nattractive in practice but also important in our comparisons\nbetween plain and residual networks. We can fairly com-\npare plain/residual networks that simultaneously have the\nsame number of parameters, depth, width, and computa-\ntional cost (except for the negligible element-wise addition).\nThe dimensions of x and F must be equal in Eqn.(1).\nIf this is not the case (e.g., when changing the input/output\nchannels), we can perform a linear projection Ws by the\nshortcut connections to match the dimensions:\ny = F(x, {Wi}) + Wsx.\n(2)\nWe can also use a square matrix Ws in Eqn.(1). But we will\nshow by experiments that the identity mapping is sufﬁcient\nfor addressing the degradation problem and is economical,\nand thus Ws is only used when matching dimensions.\nThe form of the residual function F is ﬂexible. Exper-\niments in this paper involve a function F that has two or\nthree layers (Fig. 5), while more layers are possible. But if\nF has only a single layer, Eqn.(1) is similar to a linear layer:\ny = W1x + x, for which we have not observed advantages.\nWe also note that although the above notations are about\nfully-connected layers for simplicity, they are applicable to\nconvolutional layers. The function F(x, {Wi}) can repre-\nsent multiple convolutional layers. The element-wise addi-\ntion is performed on two feature maps, channel by channel.\n3.3. Network Architectures\nWe have tested various plain/residual nets, and have ob-\nserved consistent phenomena. To provide instances for dis-\ncussion, we describe two models for ImageNet as follows.\nPlain Network. Our plain baselines (Fig. 3, middle) are\nmainly inspired by the philosophy of VGG nets [41] (Fig. 3,\nleft). The convolutional layers mostly have 3×3 ﬁlters and\nfollow two simple design rules: (i) for the same output\nfeature map size, the layers have the same number of ﬁl-\nters; and (ii) if the feature map size is halved, the num-\nber of ﬁlters is doubled so as to preserve the time com-\nplexity per layer. We perform downsampling directly by\nconvolutional layers that have a stride of 2. The network\nends with a global average pooling layer and a 1000-way\nfully-connected layer with softmax. The total number of\nweighted layers is 34 in Fig. 3 (middle).\nIt is worth noticing that our model has fewer ﬁlters and\nlower complexity than VGG nets [41] (Fig. 3, left). Our 34-\nlayer baseline has 3.6 billion FLOPs (multiply-adds), which\nis only 18% of VGG-19 (19.6 billion FLOPs).\n3\n7x7 conv, 64, /2\npool, /2\n3x3 conv, 64\n3x3 conv, 64\n3x3 conv, 64\n3x3 conv, 64\n3x3 conv, 64\n3x3 conv, 64\n3x3 conv, 128, /2\n3x3 conv, 128\n3x3 conv, 128\n3x3 conv, 128\n3x3 conv, 128\n3x3 conv, 128\n3x3 conv, 128\n3x3 conv, 128\n3x3 conv, 256, /2\n3x3 conv, 256\n3x3 conv, 256\n3x3 conv, 256\n3x3 conv, 256\n3x3 conv, 256\n3x3 conv, 256\n3x3 conv, 256\n3x3 conv, 256\n3x3 conv, 256\n3x3 conv, 256\n3x3 conv, 256\n3x3 conv, 512, /2\n3x3 conv, 512\n3x3 conv, 512\n3x3 conv, 512\n3x3 conv, 512\n3x3 conv, 512\navg pool\nfc 1000\nimage\n3x3 conv, 512\n3x3 conv, 64\n3x3 conv, 64\npool, /2\n3x3 conv, 128\n3x3 conv, 128\npool, /2\n3x3 conv, 256\n3x3 conv, 256\n3x3 conv, 256\n3x3 conv, 256\npool, /2\n3x3 conv, 512\n3x3 conv, 512\n3x3 conv, 512\npool, /2\n3x3 conv, 512\n3x3 conv, 512\n3x3 conv, 512\n3x3 conv, 512\npool, /2\nfc 4096\nfc 4096\nfc 1000\nimage\noutput \nsize: 112\noutput \nsize: 224\noutput \nsize: 56\noutput \nsize: 28\noutput \nsize: 14\noutput \nsize: 7\noutput \nsize: 1\nVGG-19\n34-layer plain\n7x7 conv, 64, /2\npool, /2\n3x3 conv, 64\n3x3 conv, 64\n3x3 conv, 64\n3x3 conv, 64\n3x3 conv, 64\n3x3 conv, 64\n3x3 conv, 128, /2\n3x3 conv, 128\n3x3 conv, 128\n3x3 conv, 128\n3x3 conv, 128\n3x3 conv, 128\n3x3 conv, 128\n3x3 conv, 128\n3x3 conv, 256, /2\n3x3 conv, 256\n3x3 conv, 256\n3x3 conv, 256\n3x3 conv, 256\n3x3 conv, 256\n3x3 conv, 256\n3x3 conv, 256\n3x3 conv, 256\n3x3 conv, 256\n3x3 conv, 256\n3x3 conv, 256\n3x3 conv, 512, /2\n3x3 conv, 512\n3x3 conv, 512\n3x3 conv, 512\n3x3 conv, 512\n3x3 conv, 512\navg pool\nfc 1000\nimage\n34-layer residual\nFigure 3. Example network architectures for ImageNet. Left: the\nVGG-19 model [41] (19.6 billion FLOPs) as a reference. Mid-\ndle: a plain network with 34 parameter layers (3.6 billion FLOPs).\nRight: a residual network with 34 parameter layers (3.6 billion\nFLOPs). The dotted shortcuts increase dimensions. Table 1 shows\nmore details and other variants.\nResidual Network. Based on the above plain network, we\ninsert shortcut connections (Fig. 3, right) which turn the\nnetwork into its counterpart residual version. The identity\nshortcuts (Eqn.(1)) can be directly used when the input and\noutput are of the same dimensions (solid line shortcuts in\nFig. 3). When the dimensions increase (dotted line shortcuts\nin Fig. 3), we consider two options: (A) The shortcut still\nperforms identity mapping, with extra zero entries padded\nfor increasing dimensions. This option introduces no extra\nparameter; (B) The projection shortcut in Eqn.(2) is used to\nmatch dimensions (done by 1×1 convolutions). For both\noptions, when the shortcuts go across feature maps of two\nsizes, they are performed with a stride of 2.\n3.4. Implementation\nOur implementation for ImageNet follows the practice\nin [21, 41]. The image is resized with its shorter side ran-\ndomly sampled in [256, 480] for scale augmentation [41].\nA 224×224 crop is randomly sampled from an image or its\nhorizontal ﬂip, with the per-pixel mean subtracted [21]. The\nstandard color augmentation in [21] is used. We adopt batch\nnormalization (BN) [16] right after each convolution and\nbefore activation, following [16]. We initialize the weights\nas in [13] and train all plain/residual nets from scratch. We\nuse SGD with a mini-batch size of 256. The learning rate\nstarts from 0.1 and is divided by 10 when the error plateaus,\nand the models are trained for up to 60 × 104 iterations. We\nuse a weight decay of 0.0001 and a momentum of 0.9. We\ndo not use dropout [14], following the practice in [16].\nIn testing, for comparison studies we adopt the standard\n10-crop testing [21]. For best results, we adopt the fully-\nconvolutional form as in [41, 13], and average the scores\nat multiple scales (images are resized such that the shorter\nside is in {224, 256, 384, 480, 640}).\n4. Experiments\n4.1. ImageNet Classiﬁcation\nWe evaluate our method on the ImageNet 2012 classiﬁ-\ncation dataset [36] that consists of 1000 classes. The models\nare trained on the 1.28 million training images, and evalu-\nated on the 50k validation images. We also obtain a ﬁnal\nresult on the 100k test images, reported by the test server.\nWe evaluate both top-1 and top-5 error rates.\nPlain Networks. We ﬁrst evaluate 18-layer and 34-layer\nplain nets. The 34-layer plain net is in Fig. 3 (middle). The\n18-layer plain net is of a similar form. See Table 1 for de-\ntailed architectures.\nThe results in Table 2 show that the deeper 34-layer plain\nnet has higher validation error than the shallower 18-layer\nplain net. To reveal the reasons, in Fig. 4 (left) we com-\npare their training/validation errors during the training pro-\ncedure. We have observed the degradation problem - the\n4\nlayer name output size\n18-layer\n34-layer\n50-layer\n101-layer\n152-layer\nconv1\n112×112\n7×7, 64, stride 2\nconv2 x\n56×56\n3×3 max pool, stride 2\n\u0014\n3×3, 64\n3×3, 64\n\u0015\n×2\n\u0014\n3×3, 64\n3×3, 64\n\u0015\n×3\n\n\n1×1, 64\n3×3, 64\n1×1, 256\n\n×3\n\n\n1×1, 64\n3×3, 64\n1×1, 256\n\n×3\n\n\n1×1, 64\n3×3, 64\n1×1, 256\n\n×3\nconv3 x\n28×28\n\u0014\n3×3, 128\n3×3, 128\n\u0015\n×2\n\u0014\n3×3, 128\n3×3, 128\n\u0015\n×4\n\n\n1×1, 128\n3×3, 128\n1×1, 512\n\n×4\n\n\n1×1, 128\n3×3, 128\n1×1, 512\n\n×4\n\n\n1×1, 128\n3×3, 128\n1×1, 512\n\n×8\nconv4 x\n14×14\n\u0014\n3×3, 256\n3×3, 256\n\u0015\n×2\n\u0014\n3×3, 256\n3×3, 256\n\u0015\n×6\n\n\n1×1, 256\n3×3, 256\n1×1, 1024\n\n×6\n\n\n1×1, 256\n3×3, 256\n1×1, 1024\n\n×23\n\n\n1×1, 256\n3×3, 256\n1×1, 1024\n\n×36\nconv5 x\n7×7\n\u0014\n3×3, 512\n3×3, 512\n\u0015\n×2\n\u0014\n3×3, 512\n3×3, 512\n\u0015\n×3\n\n\n1×1, 512\n3×3, 512\n1×1, 2048\n\n×3\n\n\n1×1, 512\n3×3, 512\n1×1, 2048\n\n×3\n\n\n1×1, 512\n3×3, 512\n1×1, 2048\n\n×3\n1×1\naverage pool, 1000-d fc, softmax\nFLOPs\n1.8×109\n3.6×109\n3.8×109\n7.6×109\n11.3×109\nTable 1. Architectures for ImageNet. Building blocks are shown in brackets (see also Fig. 5), with the numbers of blocks stacked. Down-\nsampling is performed by conv3 1, conv4 1, and conv5 1 with a stride of 2.\n0\n10\n20\n30\n40\n50\n20\n30\n40\n50\n60\niter. (1e4)\nerror (%)\n \n \nplain-18\nplain-34\n0\n10\n20\n30\n40\n50\n20\n30\n40\n50\n60\niter. (1e4)\nerror (%)\n \n \nResNet-18\nResNet-34\n18-layer\n34-layer\n18-layer\n34-layer\nFigure 4. Training on ImageNet. Thin curves denote training error, and bold curves denote validation error of the center crops. Left: plain\nnetworks of 18 and 34 layers. Right: ResNets of 18 and 34 layers. In this plot, the residual networks have no extra parameter compared to\ntheir plain counterparts.\nplain\nResNet\n18 layers\n27.94\n27.88\n34 layers\n28.54\n25.03\nTable 2. Top-1 error (%, 10-crop testing) on ImageNet validation.\nHere the ResNets have no extra parameter compared to their plain\ncounterparts. Fig. 4 shows the training procedures.\n34-layer plain net has higher training error throughout the\nwhole training procedure, even though the solution space\nof the 18-layer plain network is a subspace of that of the\n34-layer one.\nWe argue that this optimization difﬁculty is unlikely to\nbe caused by vanishing gradients. These plain networks are\ntrained with BN [16], which ensures forward propagated\nsignals to have non-zero variances. We also verify that the\nbackward propagated gradients exhibit healthy norms with\nBN. So neither forward nor backward signals vanish. In\nfact, the 34-layer plain net is still able to achieve compet-\nitive accuracy (Table 3), suggesting that the solver works\nto some extent. We conjecture that the deep plain nets may\nhave exponentially low convergence rates, which impact the\nreducing of the training error3. The reason for such opti-\nmization difﬁculties will be studied in the future.\nResidual Networks. Next we evaluate 18-layer and 34-\nlayer residual nets (ResNets). The baseline architectures\nare the same as the above plain nets, expect that a shortcut\nconnection is added to each pair of 3×3 ﬁlters as in Fig. 3\n(right). In the ﬁrst comparison (Table 2 and Fig. 4 right),\nwe use identity mapping for all shortcuts and zero-padding\nfor increasing dimensions (option A). So they have no extra\nparameter compared to the plain counterparts.\nWe have three major observations from Table 2 and\nFig. 4. First, the situation is reversed with residual learn-\ning – the 34-layer ResNet is better than the 18-layer ResNet\n(by 2.8%). More importantly, the 34-layer ResNet exhibits\nconsiderably lower training error and is generalizable to the\nvalidation data. This indicates that the degradation problem\nis well addressed in this setting and we manage to obtain\naccuracy gains from increased depth.\nSecond, compared to its plain counterpart, the 34-layer\n3We have experimented with more training iterations (3×) and still ob-\nserved the degradation problem, suggesting that this problem cannot be\nfeasibly addressed by simply using more iterations.\n5\nmodel\ntop-1 err.\ntop-5 err.\nVGG-16 [41]\n28.07\n9.33\nGoogLeNet [44]\n-\n9.15\nPReLU-net [13]\n24.27\n7.38\nplain-34\n28.54\n10.02\nResNet-34 A\n25.03\n7.76\nResNet-34 B\n24.52\n7.46\nResNet-34 C\n24.19\n7.40\nResNet-50\n22.85\n6.71\nResNet-101\n21.75\n6.05\nResNet-152\n21.43\n5.71\nTable 3. Error rates (%, 10-crop testing) on ImageNet validation.\nVGG-16 is based on our test. ResNet-50/101/152 are of option B\nthat only uses projections for increasing dimensions.\nmethod\ntop-1 err.\ntop-5 err.\nVGG [41] (ILSVRC’14)\n-\n8.43†\nGoogLeNet [44] (ILSVRC’14)\n-\n7.89\nVGG [41] (v5)\n24.4\n7.1\nPReLU-net [13]\n21.59\n5.71\nBN-inception [16]\n21.99\n5.81\nResNet-34 B\n21.84\n5.71\nResNet-34 C\n21.53\n5.60\nResNet-50\n20.74\n5.25\nResNet-101\n19.87\n4.60\nResNet-152\n19.38\n4.49\nTable 4. Error rates (%) of single-model results on the ImageNet\nvalidation set (except † reported on the test set).\nmethod\ntop-5 err. (test)\nVGG [41] (ILSVRC’14)\n7.32\nGoogLeNet [44] (ILSVRC’14)\n6.66\nVGG [41] (v5)\n6.8\nPReLU-net [13]\n4.94\nBN-inception [16]\n4.82\nResNet (ILSVRC’15)\n3.57\nTable 5. Error rates (%) of ensembles. The top-5 error is on the\ntest set of ImageNet and reported by the test server.\nResNet reduces the top-1 error by 3.5% (Table 2), resulting\nfrom the successfully reduced training error (Fig. 4 right vs.\nleft). This comparison veriﬁes the effectiveness of residual\nlearning on extremely deep systems.\nLast, we also note that the 18-layer plain/residual nets\nare comparably accurate (Table 2), but the 18-layer ResNet\nconverges faster (Fig. 4 right vs. left). When the net is “not\noverly deep” (18 layers here), the current SGD solver is still\nable to ﬁnd good solutions to the plain net. In this case, the\nResNet eases the optimization by providing faster conver-\ngence at the early stage.\nIdentity vs. Projection Shortcuts. We have shown that\n3x3, 64\n1x1, 64\nrelu\n1x1, 256\nrelu\nrelu\n3x3, 64\n3x3, 64\nrelu\nrelu\n64-d\n256-d\nFigure 5. A deeper residual function F for ImageNet. Left: a\nbuilding block (on 56×56 feature maps) as in Fig. 3 for ResNet-\n34. Right: a “bottleneck” building block for ResNet-50/101/152.\nparameter-free, identity shortcuts help with training. Next\nwe investigate projection shortcuts (Eqn.(2)). In Table 3 we\ncompare three options: (A) zero-padding shortcuts are used\nfor increasing dimensions, and all shortcuts are parameter-\nfree (the same as Table 2 and Fig. 4 right); (B) projec-\ntion shortcuts are used for increasing dimensions, and other\nshortcuts are identity; and (C) all shortcuts are projections.\nTable 3 shows that all three options are considerably bet-\nter than the plain counterpart. B is slightly better than A. We\nargue that this is because the zero-padded dimensions in A\nindeed have no residual learning. C is marginally better than\nB, and we attribute this to the extra parameters introduced\nby many (thirteen) projection shortcuts. But the small dif-\nferences among A/B/C indicate that projection shortcuts are\nnot essential for addressing the degradation problem. So we\ndo not use option C in the rest of this paper, to reduce mem-\nory/time complexity and model sizes. Identity shortcuts are\nparticularly important for not increasing the complexity of\nthe bottleneck architectures that are introduced below.\nDeeper Bottleneck Architectures. Next we describe our\ndeeper nets for ImageNet. Because of concerns on the train-\ning time that we can afford, we modify the building block\nas a bottleneck design4. For each residual function F, we\nuse a stack of 3 layers instead of 2 (Fig. 5). The three layers\nare 1×1, 3×3, and 1×1 convolutions, where the 1×1 layers\nare responsible for reducing and then increasing (restoring)\ndimensions, leaving the 3×3 layer a bottleneck with smaller\ninput/output dimensions. Fig. 5 shows an example, where\nboth designs have similar time complexity.\nThe parameter-free identity shortcuts are particularly im-\nportant for the bottleneck architectures. If the identity short-\ncut in Fig. 5 (right) is replaced with projection, one can\nshow that the time complexity and model size are doubled,\nas the shortcut is connected to the two high-dimensional\nends. So identity shortcuts lead to more efﬁcient models\nfor the bottleneck designs.\n50-layer ResNet: We replace each 2-layer block in the\n4Deeper non-bottleneck ResNets (e.g., Fig. 5 left) also gain accuracy\nfrom increased depth (as shown on CIFAR-10), but are not as economical\nas the bottleneck ResNets. So the usage of bottleneck designs is mainly due\nto practical considerations. We further note that the degradation problem\nof plain nets is also witnessed for the bottleneck designs.\n6\n34-layer net with this 3-layer bottleneck block, resulting in\na 50-layer ResNet (Table 1). We use option B for increasing\ndimensions. This model has 3.8 billion FLOPs.\n101-layer and 152-layer ResNets: We construct 101-\nlayer and 152-layer ResNets by using more 3-layer blocks\n(Table 1). Remarkably, although the depth is signiﬁcantly\nincreased, the 152-layer ResNet (11.3 billion FLOPs) still\nhas lower complexity than VGG-16/19 nets (15.3/19.6 bil-\nlion FLOPs).\nThe 50/101/152-layer ResNets are more accurate than\nthe 34-layer ones by considerable margins (Table 3 and 4).\nWe do not observe the degradation problem and thus en-\njoy signiﬁcant accuracy gains from considerably increased\ndepth. The beneﬁts of depth are witnessed for all evaluation\nmetrics (Table 3 and 4).\nComparisons with State-of-the-art Methods. In Table 4\nwe compare with the previous best single-model results.\nOur baseline 34-layer ResNets have achieved very compet-\nitive accuracy. Our 152-layer ResNet has a single-model\ntop-5 validation error of 4.49%. This single-model result\noutperforms all previous ensemble results (Table 5). We\ncombine six models of different depth to form an ensemble\n(only with two 152-layer ones at the time of submitting).\nThis leads to 3.57% top-5 error on the test set (Table 5).\nThis entry won the 1st place in ILSVRC 2015.\n4.2. CIFAR-10 and Analysis\nWe conducted more studies on the CIFAR-10 dataset\n[20], which consists of 50k training images and 10k test-\ning images in 10 classes. We present experiments trained\non the training set and evaluated on the test set. Our focus\nis on the behaviors of extremely deep networks, but not on\npushing the state-of-the-art results, so we intentionally use\nsimple architectures as follows.\nThe plain/residual architectures follow the form in Fig. 3\n(middle/right). The network inputs are 32×32 images, with\nthe per-pixel mean subtracted. The ﬁrst layer is 3×3 convo-\nlutions. Then we use a stack of 6n layers with 3×3 convo-\nlutions on the feature maps of sizes {32, 16, 8} respectively,\nwith 2n layers for each feature map size. The numbers of\nﬁlters are {16, 32, 64} respectively. The subsampling is per-\nformed by convolutions with a stride of 2. The network ends\nwith a global average pooling, a 10-way fully-connected\nlayer, and softmax. There are totally 6n+2 stacked weighted\nlayers. The following table summarizes the architecture:\noutput map size\n32×32\n16×16\n8×8\n# layers\n1+2n\n2n\n2n\n# ﬁlters\n16\n32\n64\nWhen shortcut connections are used, they are connected\nto the pairs of 3×3 layers (totally 3n shortcuts). On this\ndataset we use identity shortcuts in all cases (i.e., option A),\nmethod\nerror (%)\nMaxout [10]\n9.38\nNIN [25]\n8.81\nDSN [24]\n8.22\n# layers\n# params\nFitNet [35]\n19\n2.5M\n8.39\nHighway [42, 43]\n19\n2.3M\n7.54 (7.72±0.16)\nHighway [42, 43]\n32\n1.25M\n8.80\nResNet\n20\n0.27M\n8.75\nResNet\n32\n0.46M\n7.51\nResNet\n44\n0.66M\n7.17\nResNet\n56\n0.85M\n6.97\nResNet\n110\n1.7M\n6.43 (6.61±0.16)\nResNet\n1202\n19.4M\n7.93\nTable 6. Classiﬁcation error on the CIFAR-10 test set. All meth-\nods are with data augmentation. For ResNet-110, we run it 5 times\nand show “best (mean±std)” as in [43].\nso our residual models have exactly the same depth, width,\nand number of parameters as the plain counterparts.\nWe use a weight decay of 0.0001 and momentum of 0.9,\nand adopt the weight initialization in [13] and BN [16] but\nwith no dropout. These models are trained with a mini-\nbatch size of 128 on two GPUs. We start with a learning\nrate of 0.1, divide it by 10 at 32k and 48k iterations, and\nterminate training at 64k iterations, which is determined on\na 45k/5k train/val split. We follow the simple data augmen-\ntation in [24] for training: 4 pixels are padded on each side,\nand a 32×32 crop is randomly sampled from the padded\nimage or its horizontal ﬂip. For testing, we only evaluate\nthe single view of the original 32×32 image.\nWe compare n = {3, 5, 7, 9}, leading to 20, 32, 44, and\n56-layer networks. Fig. 6 (left) shows the behaviors of the\nplain nets. The deep plain nets suffer from increased depth,\nand exhibit higher training error when going deeper. This\nphenomenon is similar to that on ImageNet (Fig. 4, left) and\non MNIST (see [42]), suggesting that such an optimization\ndifﬁculty is a fundamental problem.\nFig. 6 (middle) shows the behaviors of ResNets. Also\nsimilar to the ImageNet cases (Fig. 4, right), our ResNets\nmanage to overcome the optimization difﬁculty and demon-\nstrate accuracy gains when the depth increases.\nWe further explore n = 18 that leads to a 110-layer\nResNet. In this case, we ﬁnd that the initial learning rate\nof 0.1 is slightly too large to start converging5. So we use\n0.01 to warm up the training until the training error is below\n80% (about 400 iterations), and then go back to 0.1 and con-\ntinue training. The rest of the learning schedule is as done\npreviously. This 110-layer network converges well (Fig. 6,\nmiddle). It has fewer parameters than other deep and thin\n5With an initial learning rate of 0.1, it starts converging (<90% error)\nafter several epochs, but still reaches similar accuracy.\n7\n0\n1\n2\n3\n4\n5\n6\n0\n5\n10\n20\niter. (1e4)\nerror (%)\n \n \nplain-20\nplain-32\nplain-44\nplain-56\n0\n1\n2\n3\n4\n5\n6\n0\n5\n10\n20\niter. (1e4)\nerror (%)\n \n \nResNet-20\nResNet-32\nResNet-44\nResNet-56\nResNet-110\n56-layer\n20-layer\n110-layer\n20-layer\n4\n5\n6\n0\n1\n5\n10\n20\niter. (1e4)\nerror (%)\n \n \nresidual-110\nresidual-1202\nFigure 6. Training on CIFAR-10. Dashed lines denote training error, and bold lines denote testing error. Left: plain networks. The error\nof plain-110 is higher than 60% and not displayed. Middle: ResNets. Right: ResNets with 110 and 1202 layers.\n0\n20\n40\n60\n80\n100\n1\n2\n3\nlayer index (sorted by magnitude)\nstd\n \n \nplain-20\nplain-56\nResNet-20\nResNet-56\nResNet-110\n0\n20\n40\n60\n80\n100\n1\n2\n3\nlayer index (original)\nstd\n \n \nplain-20\nplain-56\nResNet-20\nResNet-56\nResNet-110\nFigure 7. Standard deviations (std) of layer responses on CIFAR-\n10. The responses are the outputs of each 3×3 layer, after BN and\nbefore nonlinearity. Top: the layers are shown in their original\norder. Bottom: the responses are ranked in descending order.\nnetworks such as FitNet [35] and Highway [42] (Table 6),\nyet is among the state-of-the-art results (6.43%, Table 6).\nAnalysis of Layer Responses. Fig. 7 shows the standard\ndeviations (std) of the layer responses. The responses are\nthe outputs of each 3×3 layer, after BN and before other\nnonlinearity (ReLU/addition).\nFor ResNets, this analy-\nsis reveals the response strength of the residual functions.\nFig. 7 shows that ResNets have generally smaller responses\nthan their plain counterparts. These results support our ba-\nsic motivation (Sec.3.1) that the residual functions might\nbe generally closer to zero than the non-residual functions.\nWe also notice that the deeper ResNet has smaller magni-\ntudes of responses, as evidenced by the comparisons among\nResNet-20, 56, and 110 in Fig. 7. When there are more\nlayers, an individual layer of ResNets tends to modify the\nsignal less.\nExploring Over 1000 layers. We explore an aggressively\ndeep model of over 1000 layers.\nWe set n = 200 that\nleads to a 1202-layer network, which is trained as described\nabove. Our method shows no optimization difﬁculty, and\nthis 103-layer network is able to achieve training error\n<0.1% (Fig. 6, right).\nIts test error is still fairly good\n(7.93%, Table 6).\nBut there are still open problems on such aggressively\ndeep models. The testing result of this 1202-layer network\nis worse than that of our 110-layer network, although both\ntraining data\n07+12\n07++12\ntest data\nVOC 07 test\nVOC 12 test\nVGG-16\n73.2\n70.4\nResNet-101\n76.4\n73.8\nTable 7. Object detection mAP (%) on the PASCAL VOC\n2007/2012 test sets using baseline Faster R-CNN. See also Ta-\nble 10 and 11 for better results.\nmetric\nmAP@.5\nmAP@[.5, .95]\nVGG-16\n41.5\n21.2\nResNet-101\n48.4\n27.2\nTable 8. Object detection mAP (%) on the COCO validation set\nusing baseline Faster R-CNN. See also Table 9 for better results.\nhave similar training error. We argue that this is because of\noverﬁtting. The 1202-layer network may be unnecessarily\nlarge (19.4M) for this small dataset. Strong regularization\nsuch as maxout [10] or dropout [14] is applied to obtain the\nbest results ([10, 25, 24, 35]) on this dataset. In this paper,\nwe use no maxout/dropout and just simply impose regular-\nization via deep and thin architectures by design, without\ndistracting from the focus on the difﬁculties of optimiza-\ntion. But combining with stronger regularization may im-\nprove results, which we will study in the future.\n4.3. Object Detection on PASCAL and MS COCO\nOur method has good generalization performance on\nother recognition tasks. Table 7 and 8 show the object de-\ntection baseline results on PASCAL VOC 2007 and 2012\n[5] and COCO [26]. We adopt Faster R-CNN [32] as the de-\ntection method. Here we are interested in the improvements\nof replacing VGG-16 [41] with ResNet-101. The detection\nimplementation (see appendix) of using both models is the\nsame, so the gains can only be attributed to better networks.\nMost remarkably, on the challenging COCO dataset we ob-\ntain a 6.0% increase in COCO’s standard metric (mAP@[.5,\n.95]), which is a 28% relative improvement. This gain is\nsolely due to the learned representations.\nBased on deep residual nets, we won the 1st places in\nseveral tracks in ILSVRC & COCO 2015 competitions: Im-\nageNet detection, ImageNet localization, COCO detection,\nand COCO segmentation. The details are in the appendix.\n8\nReferences\n[1] Y. Bengio, P. Simard, and P. Frasconi. Learning long-term dependen-\ncies with gradient descent is difﬁcult. IEEE Transactions on Neural\nNetworks, 5(2):157–166, 1994.\n[2] C. M. Bishop.\nNeural networks for pattern recognition.\nOxford\nuniversity press, 1995.\n[3] W. L. Briggs, S. F. McCormick, et al. A Multigrid Tutorial. Siam,\n2000.\n[4] K. Chatﬁeld, V. Lempitsky, A. Vedaldi, and A. Zisserman. The devil\nis in the details: an evaluation of recent feature encoding methods.\nIn BMVC, 2011.\n[5] M. Everingham, L. Van Gool, C. K. Williams, J. Winn, and A. Zis-\nserman. The Pascal Visual Object Classes (VOC) Challenge. IJCV,\npages 303–338, 2010.\n[6] S. Gidaris and N. Komodakis. Object detection via a multi-region &\nsemantic segmentation-aware cnn model. In ICCV, 2015.\n[7] R. Girshick. Fast R-CNN. In ICCV, 2015.\n[8] R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich feature hier-\narchies for accurate object detection and semantic segmentation. In\nCVPR, 2014.\n[9] X. Glorot and Y. Bengio. Understanding the difﬁculty of training\ndeep feedforward neural networks. In AISTATS, 2010.\n[10] I. J. Goodfellow, D. Warde-Farley, M. Mirza, A. Courville, and\nY. Bengio. Maxout networks. arXiv:1302.4389, 2013.\n[11] K. He and J. Sun. Convolutional neural networks at constrained time\ncost. In CVPR, 2015.\n[12] K. He, X. Zhang, S. Ren, and J. Sun. Spatial pyramid pooling in deep\nconvolutional networks for visual recognition. In ECCV, 2014.\n[13] K. He, X. Zhang, S. Ren, and J. Sun. Delving deep into rectiﬁers:\nSurpassing human-level performance on imagenet classiﬁcation. In\nICCV, 2015.\n[14] G. E. Hinton, N. Srivastava, A. Krizhevsky, I. Sutskever, and\nR. R. Salakhutdinov. Improving neural networks by preventing co-\nadaptation of feature detectors. arXiv:1207.0580, 2012.\n[15] S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural\ncomputation, 9(8):1735–1780, 1997.\n[16] S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep\nnetwork training by reducing internal covariate shift. In ICML, 2015.\n[17] H. Jegou, M. Douze, and C. Schmid. Product quantization for nearest\nneighbor search. TPAMI, 33, 2011.\n[18] H. Jegou, F. Perronnin, M. Douze, J. Sanchez, P. Perez, and\nC. Schmid. Aggregating local image descriptors into compact codes.\nTPAMI, 2012.\n[19] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick,\nS. Guadarrama, and T. Darrell. Caffe: Convolutional architecture for\nfast feature embedding. arXiv:1408.5093, 2014.\n[20] A. Krizhevsky. Learning multiple layers of features from tiny im-\nages. Tech Report, 2009.\n[21] A. Krizhevsky, I. Sutskever, and G. Hinton. Imagenet classiﬁcation\nwith deep convolutional neural networks. In NIPS, 2012.\n[22] Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard,\nW. Hubbard, and L. D. Jackel. Backpropagation applied to hand-\nwritten zip code recognition. Neural computation, 1989.\n[23] Y. LeCun, L. Bottou, G. B. Orr, and K.-R. M¨uller. Efﬁcient backprop.\nIn Neural Networks: Tricks of the Trade, pages 9–50. Springer, 1998.\n[24] C.-Y. Lee, S. Xie, P. Gallagher, Z. Zhang, and Z. Tu.\nDeeply-\nsupervised nets. arXiv:1409.5185, 2014.\n[25] M. Lin, Q. Chen, and S. Yan. Network in network. arXiv:1312.4400,\n2013.\n[26] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan,\nP. Doll´ar, and C. L. Zitnick. Microsoft COCO: Common objects in\ncontext. In ECCV. 2014.\n[27] J. Long, E. Shelhamer, and T. Darrell. Fully convolutional networks\nfor semantic segmentation. In CVPR, 2015.\n[28] G. Mont´ufar, R. Pascanu, K. Cho, and Y. Bengio. On the number of\nlinear regions of deep neural networks. In NIPS, 2014.\n[29] V. Nair and G. E. Hinton. Rectiﬁed linear units improve restricted\nboltzmann machines. In ICML, 2010.\n[30] F. Perronnin and C. Dance. Fisher kernels on visual vocabularies for\nimage categorization. In CVPR, 2007.\n[31] T. Raiko, H. Valpola, and Y. LeCun. Deep learning made easier by\nlinear transformations in perceptrons. In AISTATS, 2012.\n[32] S. Ren, K. He, R. Girshick, and J. Sun. Faster R-CNN: Towards\nreal-time object detection with region proposal networks. In NIPS,\n2015.\n[33] S. Ren, K. He, R. Girshick, X. Zhang, and J. Sun. Object detection\nnetworks on convolutional feature maps. arXiv:1504.06066, 2015.\n[34] B. D. Ripley. Pattern recognition and neural networks. Cambridge\nuniversity press, 1996.\n[35] A. Romero, N. Ballas, S. E. Kahou, A. Chassang, C. Gatta, and\nY. Bengio. Fitnets: Hints for thin deep nets. In ICLR, 2015.\n[36] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma,\nZ. Huang, A. Karpathy, A. Khosla, M. Bernstein, et al. Imagenet\nlarge scale visual recognition challenge. arXiv:1409.0575, 2014.\n[37] A. M. Saxe, J. L. McClelland, and S. Ganguli. Exact solutions to\nthe nonlinear dynamics of learning in deep linear neural networks.\narXiv:1312.6120, 2013.\n[38] N. N. Schraudolph. Accelerated gradient descent by factor-centering\ndecomposition. Technical report, 1998.\n[39] N. N. Schraudolph. Centering neural network gradient factors. In\nNeural Networks: Tricks of the Trade, pages 207–226. Springer,\n1998.\n[40] P. Sermanet, D. Eigen, X. Zhang, M. Mathieu, R. Fergus, and Y. Le-\nCun.\nOverfeat: Integrated recognition, localization and detection\nusing convolutional networks. In ICLR, 2014.\n[41] K. Simonyan and A. Zisserman. Very deep convolutional networks\nfor large-scale image recognition. In ICLR, 2015.\n[42] R. K. Srivastava, K. Greff, and J. Schmidhuber. Highway networks.\narXiv:1505.00387, 2015.\n[43] R. K. Srivastava, K. Greff, and J. Schmidhuber. Training very deep\nnetworks. 1507.06228, 2015.\n[44] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Er-\nhan, V. Vanhoucke, and A. Rabinovich. Going deeper with convolu-\ntions. In CVPR, 2015.\n[45] R. Szeliski. Fast surface interpolation using hierarchical basis func-\ntions. TPAMI, 1990.\n[46] R. Szeliski. Locally adapted hierarchical basis preconditioning. In\nSIGGRAPH, 2006.\n[47] T. Vatanen, T. Raiko, H. Valpola, and Y. LeCun. Pushing stochas-\ntic gradient towards second-order methods–backpropagation learn-\ning with transformations in nonlinearities.\nIn Neural Information\nProcessing, 2013.\n[48] A. Vedaldi and B. Fulkerson. VLFeat: An open and portable library\nof computer vision algorithms, 2008.\n[49] W. Venables and B. Ripley. Modern applied statistics with s-plus.\n1999.\n[50] M. D. Zeiler and R. Fergus. Visualizing and understanding convolu-\ntional neural networks. In ECCV, 2014.\n9\nA. Object Detection Baselines\nIn this section we introduce our detection method based\non the baseline Faster R-CNN [32] system. The models are\ninitialized by the ImageNet classiﬁcation models, and then\nﬁne-tuned on the object detection data. We have experi-\nmented with ResNet-50/101 at the time of the ILSVRC &\nCOCO 2015 detection competitions.\nUnlike VGG-16 used in [32], our ResNet has no hidden\nfc layers. We adopt the idea of “Networks on Conv fea-\nture maps” (NoC) [33] to address this issue. We compute\nthe full-image shared conv feature maps using those lay-\ners whose strides on the image are no greater than 16 pixels\n(i.e., conv1, conv2 x, conv3 x, and conv4 x, totally 91 conv\nlayers in ResNet-101; Table 1). We consider these layers as\nanalogous to the 13 conv layers in VGG-16, and by doing\nso, both ResNet and VGG-16 have conv feature maps of the\nsame total stride (16 pixels). These layers are shared by a\nregion proposal network (RPN, generating 300 proposals)\n[32] and a Fast R-CNN detection network [7]. RoI pool-\ning [7] is performed before conv5 1. On this RoI-pooled\nfeature, all layers of conv5 x and up are adopted for each\nregion, playing the roles of VGG-16’s fc layers. The ﬁnal\nclassiﬁcation layer is replaced by two sibling layers (classi-\nﬁcation and box regression [7]).\nFor the usage of BN layers, after pre-training, we com-\npute the BN statistics (means and variances) for each layer\non the ImageNet training set. Then the BN layers are ﬁxed\nduring ﬁne-tuning for object detection. As such, the BN\nlayers become linear activations with constant offsets and\nscales, and BN statistics are not updated by ﬁne-tuning. We\nﬁx the BN layers mainly for reducing memory consumption\nin Faster R-CNN training.\nPASCAL VOC\nFollowing [7, 32], for the PASCAL VOC 2007 test set,\nwe use the 5k trainval images in VOC 2007 and 16k train-\nval images in VOC 2012 for training (“07+12”). For the\nPASCAL VOC 2012 test set, we use the 10k trainval+test\nimages in VOC 2007 and 16k trainval images in VOC 2012\nfor training (“07++12”). The hyper-parameters for train-\ning Faster R-CNN are the same as in [32]. Table 7 shows\nthe results. ResNet-101 improves the mAP by >3% over\nVGG-16. This gain is solely because of the improved fea-\ntures learned by ResNet.\nMS COCO\nThe MS COCO dataset [26] involves 80 object cate-\ngories. We evaluate the PASCAL VOC metric (mAP @\nIoU = 0.5) and the standard COCO metric (mAP @ IoU =\n.5:.05:.95). We use the 80k images on the train set for train-\ning and the 40k images on the val set for evaluation. Our\ndetection system for COCO is similar to that for PASCAL\nVOC. We train the COCO models with an 8-GPU imple-\nmentation, and thus the RPN step has a mini-batch size of\n8 images (i.e., 1 per GPU) and the Fast R-CNN step has a\nmini-batch size of 16 images. The RPN step and Fast R-\nCNN step are both trained for 240k iterations with a learn-\ning rate of 0.001 and then for 80k iterations with 0.0001.\nTable 8 shows the results on the MS COCO validation\nset. ResNet-101 has a 6% increase of mAP@[.5, .95] over\nVGG-16, which is a 28% relative improvement, solely con-\ntributed by the features learned by the better network. Re-\nmarkably, the mAP@[.5, .95]’s absolute increase (6.0%) is\nnearly as big as mAP@.5’s (6.9%). This suggests that a\ndeeper network can improve both recognition and localiza-\ntion.\nB. Object Detection Improvements\nFor completeness, we report the improvements made for\nthe competitions. These improvements are based on deep\nfeatures and thus should beneﬁt from residual learning.\nMS COCO\nBox reﬁnement. Our box reﬁnement partially follows the it-\nerative localization in [6]. In Faster R-CNN, the ﬁnal output\nis a regressed box that is different from its proposal box. So\nfor inference, we pool a new feature from the regressed box\nand obtain a new classiﬁcation score and a new regressed\nbox. We combine these 300 new predictions with the orig-\ninal 300 predictions. Non-maximum suppression (NMS) is\napplied on the union set of predicted boxes using an IoU\nthreshold of 0.3 [8], followed by box voting [6]. Box re-\nﬁnement improves mAP by about 2 points (Table 9).\nGlobal context.\nWe combine global context in the Fast\nR-CNN step. Given the full-image conv feature map, we\npool a feature by global Spatial Pyramid Pooling [12] (with\na “single-level” pyramid) which can be implemented as\n“RoI” pooling using the entire image’s bounding box as the\nRoI. This pooled feature is fed into the post-RoI layers to\nobtain a global context feature. This global feature is con-\ncatenated with the original per-region feature, followed by\nthe sibling classiﬁcation and box regression layers. This\nnew structure is trained end-to-end.\nGlobal context im-\nproves mAP@.5 by about 1 point (Table 9).\nMulti-scale testing. In the above, all results are obtained by\nsingle-scale training/testing as in [32], where the image’s\nshorter side is s = 600 pixels. Multi-scale training/testing\nhas been developed in [12, 7] by selecting a scale from a\nfeature pyramid, and in [33] by using maxout layers. In\nour current implementation, we have performed multi-scale\ntesting following [33]; we have not performed multi-scale\ntraining because of limited time. In addition, we have per-\nformed multi-scale testing only for the Fast R-CNN step\n(but not yet for the RPN step). With a trained model, we\ncompute conv feature maps on an image pyramid, where the\nimage’s shorter sides are s ∈{200, 400, 600, 800, 1000}.\n10\ntraining data\nCOCO train\nCOCO trainval\ntest data\nCOCO val\nCOCO test-dev\nmAP\n@.5\n@[.5, .95]\n@.5\n@[.5, .95]\nbaseline Faster R-CNN (VGG-16)\n41.5\n21.2\nbaseline Faster R-CNN (ResNet-101)\n48.4\n27.2\n+box reﬁnement\n49.9\n29.9\n+context\n51.1\n30.0\n53.3\n32.2\n+multi-scale testing\n53.8\n32.5\n55.7\n34.9\nensemble\n59.0\n37.4\nTable 9. Object detection improvements on MS COCO using Faster R-CNN and ResNet-101.\nsystem\nnet\ndata\nmAP\nareo\nbike\nbird\nboat\nbottle\nbus\ncar\ncat\nchair\ncow\ntable\ndog\nhorse\nmbike person\nplant\nsheep\nsofa\ntrain\ntv\nbaseline\nVGG-16\n07+12\n73.2\n76.5 79.0 70.9 65.5 52.1 83.1 84.7 86.4 52.0 81.9 65.7 84.8 84.6 77.5 76.7 38.8 73.6 73.9 83.0 72.6\nbaseline\nResNet-101\n07+12\n76.4\n79.8 80.7 76.2 68.3 55.9 85.1 85.3 89.8 56.7 87.8 69.4 88.3 88.9 80.9 78.4 41.7 78.6 79.8 85.3 72.0\nbaseline+++ ResNet-101\nCOCO+07+12\n85.6\n90.0 89.6 87.8 80.8 76.1 89.9 89.9 89.6 75.5 90.0 80.7 89.6 90.3 89.1 88.7 65.4 88.1 85.6 89.0 86.8\nTable 10. Detection results on the PASCAL VOC 2007 test set. The baseline is the Faster R-CNN system. The system “baseline+++”\ninclude box reﬁnement, context, and multi-scale testing in Table 9.\nsystem\nnet\ndata\nmAP\nareo\nbike\nbird\nboat\nbottle\nbus\ncar\ncat\nchair\ncow\ntable\ndog\nhorse\nmbike person\nplant\nsheep\nsofa\ntrain\ntv\nbaseline\nVGG-16\n07++12\n70.4\n84.9 79.8 74.3 53.9 49.8 77.5 75.9 88.5 45.6 77.1 55.3 86.9 81.7 80.9 79.6 40.1 72.6 60.9 81.2 61.5\nbaseline\nResNet-101\n07++12\n73.8\n86.5 81.6 77.2 58.0 51.0 78.6 76.6 93.2 48.6 80.4 59.0 92.1 85.3 84.8 80.7 48.1 77.3 66.5 84.7 65.6\nbaseline+++ ResNet-101 COCO+07++12\n83.8\n92.1 88.4 84.8 75.9 71.4 86.3 87.8 94.2 66.8 89.4 69.2 93.9 91.9 90.9 89.6 67.9 88.2 76.8 90.3 80.0\nTable 11. Detection results on the PASCAL VOC 2012 test set (http://host.robots.ox.ac.uk:8080/leaderboard/\ndisplaylb.php?challengeid=11&compid=4). The baseline is the Faster R-CNN system. The system “baseline+++” include\nbox reﬁnement, context, and multi-scale testing in Table 9.\nWe select two adjacent scales from the pyramid following\n[33]. RoI pooling and subsequent layers are performed on\nthe feature maps of these two scales [33], which are merged\nby maxout as in [33]. Multi-scale testing improves the mAP\nby over 2 points (Table 9).\nUsing validation data. Next we use the 80k+40k trainval set\nfor training and the 20k test-dev set for evaluation. The test-\ndev set has no publicly available ground truth and the result\nis reported by the evaluation server. Under this setting, the\nresults are an mAP@.5 of 55.7% and an mAP@[.5, .95] of\n34.9% (Table 9). This is our single-model result.\nEnsemble. In Faster R-CNN, the system is designed to learn\nregion proposals and also object classiﬁers, so an ensemble\ncan be used to boost both tasks. We use an ensemble for\nproposing regions, and the union set of proposals are pro-\ncessed by an ensemble of per-region classiﬁers. Table 9\nshows our result based on an ensemble of 3 networks. The\nmAP is 59.0% and 37.4% on the test-dev set. This result\nwon the 1st place in the detection task in COCO 2015.\nPASCAL VOC\nWe revisit the PASCAL VOC dataset based on the above\nmodel. With the single model on the COCO dataset (55.7%\nmAP@.5 in Table 9), we ﬁne-tune this model on the PAS-\nCAL VOC sets. The improvements of box reﬁnement, con-\ntext, and multi-scale testing are also adopted. By doing so\nval2\ntest\nGoogLeNet [44] (ILSVRC’14)\n-\n43.9\nour single model (ILSVRC’15)\n60.5\n58.8\nour ensemble (ILSVRC’15)\n63.6\n62.1\nTable 12. Our results (mAP, %) on the ImageNet detection dataset.\nOur detection system is Faster R-CNN [32] with the improvements\nin Table 9, using ResNet-101.\nwe achieve 85.6% mAP on PASCAL VOC 2007 (Table 10)\nand 83.8% on PASCAL VOC 2012 (Table 11)6. The result\non PASCAL VOC 2012 is 10 points higher than the previ-\nous state-of-the-art result [6].\nImageNet Detection\nThe ImageNet Detection (DET) task involves 200 object\ncategories. The accuracy is evaluated by mAP@.5. Our\nobject detection algorithm for ImageNet DET is the same\nas that for MS COCO in Table 9. The networks are pre-\ntrained on the 1000-class ImageNet classiﬁcation set, and\nare ﬁne-tuned on the DET data. We split the validation set\ninto two parts (val1/val2) following [8]. We ﬁne-tune the\ndetection models using the DET training set and the val1\nset. The val2 set is used for validation. We do not use other\nILSVRC 2015 data. Our single model with ResNet-101 has\n6http://host.robots.ox.ac.uk:8080/anonymous/3OJ4OJ.html,\nsubmitted on 2015-11-26.\n11\nLOC\nmethod\nLOC\nnetwork\ntesting LOC error\non GT CLS\nclassiﬁcation\nnetwork\ntop-5 LOC error\non predicted CLS\nVGG’s [41]\nVGG-16\n1-crop\n33.1 [41]\nRPN\nResNet-101 1-crop\n13.3\nRPN\nResNet-101 dense\n11.7\nRPN\nResNet-101 dense\nResNet-101\n14.4\nRPN+RCNN ResNet-101 dense\nResNet-101\n10.6\nRPN+RCNN\nensemble\ndense\nensemble\n8.9\nTable 13. Localization error (%) on the ImageNet validation. In\nthe column of “LOC error on GT class” ([41]), the ground truth\nclass is used. In the “testing” column, “1-crop” denotes testing\non a center crop of 224×224 pixels, “dense” denotes dense (fully\nconvolutional) and multi-scale testing.\n58.8% mAP and our ensemble of 3 models has 62.1% mAP\non the DET test set (Table 12). This result won the 1st place\nin the ImageNet detection task in ILSVRC 2015, surpassing\nthe second place by 8.5 points (absolute).\nC. ImageNet Localization\nThe ImageNet Localization (LOC) task [36] requires to\nclassify and localize the objects. Following [40, 41], we\nassume that the image-level classiﬁers are ﬁrst adopted for\npredicting the class labels of an image, and the localiza-\ntion algorithm only accounts for predicting bounding boxes\nbased on the predicted classes. We adopt the “per-class re-\ngression” (PCR) strategy [40, 41], learning a bounding box\nregressor for each class. We pre-train the networks for Im-\nageNet classiﬁcation and then ﬁne-tune them for localiza-\ntion. We train networks on the provided 1000-class Ima-\ngeNet training set.\nOur localization algorithm is based on the RPN frame-\nwork of [32] with a few modiﬁcations. Unlike the way in\n[32] that is category-agnostic, our RPN for localization is\ndesigned in a per-class form. This RPN ends with two sib-\nling 1×1 convolutional layers for binary classiﬁcation (cls)\nand box regression (reg), as in [32]. The cls and reg layers\nare both in a per-class from, in contrast to [32]. Speciﬁ-\ncally, the cls layer has a 1000-d output, and each dimension\nis binary logistic regression for predicting being or not be-\ning an object class; the reg layer has a 1000×4-d output\nconsisting of box regressors for 1000 classes. As in [32],\nour bounding box regression is with reference to multiple\ntranslation-invariant “anchor” boxes at each position.\nAs in our ImageNet classiﬁcation training (Sec. 3.4), we\nrandomly sample 224×224 crops for data augmentation.\nWe use a mini-batch size of 256 images for ﬁne-tuning. To\navoid negative samples being dominate, 8 anchors are ran-\ndomly sampled for each image, where the sampled positive\nand negative anchors have a ratio of 1:1 [32]. For testing,\nthe network is applied on the image fully-convolutionally.\nTable 13 compares the localization results. Following\n[41], we ﬁrst perform “oracle” testing using the ground truth\nclass as the classiﬁcation prediction. VGG’s paper [41] re-\nmethod\ntop-5 localization err\nval\ntest\nOverFeat [40] (ILSVRC’13)\n30.0\n29.9\nGoogLeNet [44] (ILSVRC’14)\n-\n26.7\nVGG [41] (ILSVRC’14)\n26.9\n25.3\nours (ILSVRC’15)\n8.9\n9.0\nTable 14. Comparisons of localization error (%) on the ImageNet\ndataset with state-of-the-art methods.\nports a center-crop error of 33.1% (Table 13) using ground\ntruth classes. Under the same setting, our RPN method us-\ning ResNet-101 net signiﬁcantly reduces the center-crop er-\nror to 13.3%. This comparison demonstrates the excellent\nperformance of our framework. With dense (fully convolu-\ntional) and multi-scale testing, our ResNet-101 has an error\nof 11.7% using ground truth classes. Using ResNet-101 for\npredicting classes (4.6% top-5 classiﬁcation error, Table 4),\nthe top-5 localization error is 14.4%.\nThe above results are only based on the proposal network\n(RPN) in Faster R-CNN [32]. One may use the detection\nnetwork (Fast R-CNN [7]) in Faster R-CNN to improve the\nresults. But we notice that on this dataset, one image usually\ncontains a single dominate object, and the proposal regions\nhighly overlap with each other and thus have very similar\nRoI-pooled features. As a result, the image-centric training\nof Fast R-CNN [7] generates samples of small variations,\nwhich may not be desired for stochastic training. Motivated\nby this, in our current experiment we use the original R-\nCNN [8] that is RoI-centric, in place of Fast R-CNN.\nOur R-CNN implementation is as follows. We apply the\nper-class RPN trained as above on the training images to\npredict bounding boxes for the ground truth class. These\npredicted boxes play a role of class-dependent proposals.\nFor each training image, the highest scored 200 proposals\nare extracted as training samples to train an R-CNN classi-\nﬁer. The image region is cropped from a proposal, warped\nto 224×224 pixels, and fed into the classiﬁcation network\nas in R-CNN [8]. The outputs of this network consist of two\nsibling fc layers for cls and reg, also in a per-class form.\nThis R-CNN network is ﬁne-tuned on the training set us-\ning a mini-batch size of 256 in the RoI-centric fashion. For\ntesting, the RPN generates the highest scored 200 proposals\nfor each predicted class, and the R-CNN network is used to\nupdate these proposals’ scores and box positions.\nThis method reduces the top-5 localization error to\n10.6% (Table 13). This is our single-model result on the\nvalidation set. Using an ensemble of networks for both clas-\nsiﬁcation and localization, we achieve a top-5 localization\nerror of 9.0% on the test set. This number signiﬁcantly out-\nperforms the ILSVRC 14 results (Table 14), showing a 64%\nrelative reduction of error. This result won the 1st place in\nthe ImageNet localization task in ILSVRC 2015.\n12\n",
        "sentence": " In recent years, Deep Neural Networks (DNNs), especially Convolutional Neural Networks (CNNs), have demonstrated highly competitive results on object recognition and image classification [1, 2, 3, 4]."
    },
    {
        "title": "Visualizing higher-layer features of a deep network",
        "author": [
            "Dumitru Erhan",
            "Yoshua Bengio",
            "Aaron Courville",
            "Pascal Vincent"
        ],
        "venue": "University de Montreal Technical Report",
        "citeRegEx": "5",
        "shortCiteRegEx": "5",
        "year": 2009,
        "abstract": "",
        "full_text": "",
        "sentence": " Significant visualization techniques have been developed to understand the deep image representations on trained networks [5, 6, 7, 8, 9, 10, 11]. Another stream of visualization aims to understand what each neuron has learned in a pretrained network and synthesize an image that maximally activates individual features [5, 9] or the class prediction scores [6]."
    },
    {
        "title": "Deep inside convolutional networks: Visualising image classification models and saliency maps",
        "author": [
            "Karen Simonyan",
            "Andrea Vedaldi",
            "Andrew Zisserman"
        ],
        "venue": "arXiv preprint arXiv:1312.6034,",
        "citeRegEx": "6",
        "shortCiteRegEx": "6",
        "year": 2013,
        "abstract": "This paper addresses the visualisation of image classification models, learnt\nusing deep Convolutional Networks (ConvNets). We consider two visualisation\ntechniques, based on computing the gradient of the class score with respect to\nthe input image. The first one generates an image, which maximises the class\nscore [Erhan et al., 2009], thus visualising the notion of the class, captured\nby a ConvNet. The second technique computes a class saliency map, specific to a\ngiven image and class. We show that such maps can be employed for weakly\nsupervised object segmentation using classification ConvNets. Finally, we\nestablish the connection between the gradient-based ConvNet visualisation\nmethods and deconvolutional networks [Zeiler et al., 2013].",
        "full_text": "Deep Inside Convolutional Networks: Visualising\nImage Classiﬁcation Models and Saliency Maps\nKaren Simonyan\nAndrea Vedaldi\nAndrew Zisserman\nVisual Geometry Group, University of Oxford\n{karen,vedaldi,az}@robots.ox.ac.uk\nAbstract\nThis paper addresses the visualisation of image classiﬁcation models, learnt us-\ning deep Convolutional Networks (ConvNets).\nWe consider two visualisation\ntechniques, based on computing the gradient of the class score with respect to\nthe input image. The ﬁrst one generates an image, which maximises the class\nscore [5], thus visualising the notion of the class, captured by a ConvNet. The\nsecond technique computes a class saliency map, speciﬁc to a given image and\nclass. We show that such maps can be employed for weakly supervised object\nsegmentation using classiﬁcation ConvNets. Finally, we establish the connection\nbetween the gradient-based ConvNet visualisation methods and deconvolutional\nnetworks [13].\n1\nIntroduction\nWith the deep Convolutional Networks (ConvNets) [10] now being the architecture of choice for\nlarge-scale image recognition [4, 8], the problem of understanding the aspects of visual appearance,\ncaptured inside a deep model, has become particularly relevant and is the subject of this paper.\nIn previous work, Erhan et al. [5] visualised deep models by ﬁnding an input image which max-\nimises the neuron activity of interest by carrying out an optimisation using gradient ascent in the\nimage space. The method was used to visualise the hidden feature layers of unsupervised deep ar-\nchitectures, such as the Deep Belief Network (DBN) [7], and it was later employed by Le et al.[9]\nto visualise the class models, captured by a deep unsupervised auto-encoder. Recently, the problem\nof ConvNet visualisation was addressed by Zeiler et al.[13]. For convolutional layer visualisation,\nthey proposed the Deconvolutional Network (DeconvNet) architecture, which aims to approximately\nreconstruct the input of each layer from its output.\nIn this paper, we address the visualisation of deep image classiﬁcation ConvNets, trained on the\nlarge-scale ImageNet challenge dataset [2]. To this end, we make the following three contributions.\nFirst, we demonstrate that understandable visualisations of ConvNet classiﬁcation models can be ob-\ntained using the numerical optimisation of the input image [5] (Sect. 2). Note, in our case, unlike [5],\nthe net is trained in a supervised manner, so we know which neuron in the ﬁnal fully-connected clas-\nsiﬁcation layer should be maximised to visualise the class of interest (in the unsupervised case, [9]\nhad to use a separate annotated image set to ﬁnd out the neuron responsible for a particular class). To\nthe best of our knowledge, we are the ﬁrst to apply the method of [5] to the visualisation of ImageNet\nclassiﬁcation ConvNets [8]. Second, we propose a method for computing the spatial support of a\ngiven class in a given image (image-speciﬁc class saliency map) using a single back-propagation\npass through a classiﬁcation ConvNet (Sect. 3). As discussed in Sect. 3.2, such saliency maps can\nbe used for weakly supervised object localisation. Finally, we show in Sect. 4 that the gradient-based\nvisualisation methods generalise the deconvolutional network reconstruction procedure [13].\nConvNet implementation details.\nOur visualisation experiments were carried out using a single\ndeep ConvNet, trained on the ILSVRC-2013 dataset [2], which includes 1.2M training images,\nlabelled into 1000 classes. Our ConvNet is similar to that of [8] and is implemented using their\n1\narXiv:1312.6034v2  [cs.CV]  19 Apr 2014\ncuda-convnet toolbox1, although our net is less wide, and we used additional image jittering,\nbased on zeroing-out random parts of an image. Our weight layer conﬁguration is: conv64-conv256-\nconv256-conv256-conv256-full4096-full4096-full1000, where convN denotes a convolutional layer\nwith N ﬁlters, fullM – a fully-connected layer with M outputs. On ILSVRC-2013 validation set, the\nnetwork achieves the top-1/top-5 classiﬁcation error of 39.7%/17.7%, which is slightly better than\n40.7%/18.2%, reported in [8] for a single ConvNet.\n2\nClass Model Visualisation\nIn this section we describe a technique for visualising the class models, learnt by the image clas-\nsiﬁcation ConvNets. Given a learnt classiﬁcation ConvNet and a class of interest, the visualisation\nmethod consists in numerically generating an image [5], which is representative of the class in terms\nof the ConvNet class scoring model.\nMore formally, let Sc(I) be the score of the class c, computed by the classiﬁcation layer of the\nConvNet for an image I. We would like to ﬁnd an L2-regularised image, such that the score Sc is\nhigh:\narg max\nI\nSc(I) −λ∥I∥2\n2,\n(1)\nwhere λ is the regularisation parameter. A locally-optimal I can be found by the back-propagation\nmethod. The procedure is related to the ConvNet training procedure, where the back-propagation is\nused to optimise the layer weights. The difference is that in our case the optimisation is performed\nwith respect to the input image, while the weights are ﬁxed to those found during the training stage.\nWe initialised the optimisation with the zero image (in our case, the ConvNet was trained on the\nzero-centred image data), and then added the training set mean image to the result. The class model\nvisualisations for several classes are shown in Fig. 1.\nIt should be noted that we used the (unnormalised) class scores Sc, rather than the class posteriors,\nreturned by the soft-max layer: Pc =\nexp Sc\nP\nc exp Sc . The reason is that the maximisation of the class\nposterior can be achieved by minimising the scores of other classes. Therefore, we optimise Sc to\nensure that the optimisation concentrates only on the class in question c. We also experimented\nwith optimising the posterior Pc, but the results were not visually prominent, thus conﬁrming our\nintuition.\n3\nImage-Speciﬁc Class Saliency Visualisation\nIn this section we describe how a classiﬁcation ConvNet can be queried about the spatial support of\na particular class in a given image. Given an image I0, a class c, and a classiﬁcation ConvNet with\nthe class score function Sc(I), we would like to rank the pixels of I0 based on their inﬂuence on the\nscore Sc(I0).\nWe start with a motivational example. Consider the linear score model for the class c:\nSc(I) = wT\nc I + bc,\n(2)\nwhere the image I is represented in the vectorised (one-dimensional) form, and wc and bc are respec-\ntively the weight vector and the bias of the model. In this case, it is easy to see that the magnitude\nof elements of w deﬁnes the importance of the corresponding pixels of I for the class c.\nIn the case of deep ConvNets, the class score Sc(I) is a highly non-linear function of I, so the\nreasoning of the previous paragraph can not be immediately applied. However, given an image\nI0, we can approximate Sc(I) with a linear function in the neighbourhood of I0 by computing the\nﬁrst-order Taylor expansion:\nSc(I) ≈wT I + b,\n(3)\nwhere w is the derivative of Sc with respect to the image I at the point (image) I0:\nw = ∂Sc\n∂I\n\f\f\f\f\nI0\n.\n(4)\nAnother interpretation of computing the image-speciﬁc class saliency using the class score deriva-\ntive (4) is that the magnitude of the derivative indicates which pixels need to be changed the least\n1http://code.google.com/p/cuda-convnet/\n2\ndumbbell \ncup \ndalmatian \nbell pepper \nlemon \nhusky \nwashing machine \ncomputer keyboard \nkit fox \ngoose \nlimousine \nostrich \nFigure 1: Numerically computed images, illustrating the class appearance models, learnt by a\nConvNet, trained on ILSVRC-2013. Note how different aspects of class appearance are captured\nin a single image. Better viewed in colour.\n3\nto affect the class score the most. One can expect that such pixels correspond to the object location\nin the image. We note that a similar technique has been previously applied by [1] in the context of\nBayesian classiﬁcation.\n3.1\nClass Saliency Extraction\nGiven an image I0 (with m rows and n columns) and a class c, the class saliency map M ∈Rm×n\nis computed as follows. First, the derivative w (4) is found by back-propagation. After that, the\nsaliency map is obtained by rearranging the elements of the vector w. In the case of a grey-scale\nimage, the number of elements in w is equal to the number of pixels in I0, so the map can be\ncomputed as Mij = |wh(i,j)|, where h(i, j) is the index of the element of w, corresponding to the\nimage pixel in the i-th row and j-th column. In the case of the multi-channel (e.g. RGB) image, let\nus assume that the colour channel c of the pixel (i, j) of image I corresponds to the element of w\nwith the index h(i, j, c). To derive a single class saliency value for each pixel (i, j), we took the\nmaximum magnitude of w across all colour channels: Mij = maxc |wh(i,j,c)|.\nIt is important to note that the saliency maps are extracted using a classiﬁcation ConvNet trained\non the image labels, so no additional annotation is required (such as object bounding boxes or\nsegmentation masks). The computation of the image-speciﬁc saliency map for a single class is\nextremely quick, since it only requires a single back-propagation pass.\nWe visualise the saliency maps for the highest-scoring class (top-1 class prediction) on randomly se-\nlected ILSVRC-2013 test set images in Fig. 2. Similarly to the ConvNet classiﬁcation procedure [8],\nwhere the class predictions are computed on 10 cropped and reﬂected sub-images, we computed 10\nsaliency maps on the 10 sub-images, and then averaged them.\n3.2\nWeakly Supervised Object Localisation\nThe weakly supervised class saliency maps (Sect. 3.1) encode the location of the object of the given\nclass in the given image, and thus can be used for object localisation (in spite of being trained on\nimage labels only). Here we brieﬂy describe a simple object localisation procedure, which we used\nfor the localisation task of the ILSVRC-2013 challenge [12].\nGiven an image and the corresponding class saliency map, we compute the object segmentation mask\nusing the GraphCut colour segmentation [3]. The use of the colour segmentation is motivated by the\nfact that the saliency map might capture only the most discriminative part of an object, so saliency\nthresholding might not be able to highlight the whole object. Therefore, it is important to be able\nto propagate the thresholded map to other parts of the object, which we aim to achieve here using\nthe colour continuity cues. Foreground and background colour models were set to be the Gaussian\nMixture Models. The foreground model was estimated from the pixels with the saliency higher than\na threshold, set to the 95% quantile of the saliency distribution in the image; the background model\nwas estimated from the pixels with the saliency smaller than the 30% quantile (Fig. 3, right-middle).\nThe GraphCut segmentation [3] was then performed using the publicly available implementation2.\nOnce the image pixel labelling into foreground and background is computed, the object segmentation\nmask is set to the largest connected component of the foreground pixels (Fig. 3, right).\nWe entered our object localisation method into the ILSVRC-2013 localisation challenge. Consid-\nering that the challenge requires the object bounding boxes to be reported, we computed them as\nthe bounding boxes of the object segmentation masks. The procedure was repeated for each of the\ntop-5 predicted classes. The method achieved 46.4% top-5 error on the test set of ILSVRC-2013.\nIt should be noted that the method is weakly supervised (unlike the challenge winner with 29.9%\nerror), and the object localisation task was not taken into account during training. In spite of its\nsimplicity, the method still outperformed our submission to ILSVRC-2012 challenge (which used\nthe same dataset), which achieved 50.0% localisation error using a fully-supervised algorithm based\non the part-based models [6] and Fisher vector feature encoding [11].\n4\nRelation to Deconvolutional Networks\nIn this section we establish the connection between the gradient-based visualisation and the\nDeconvNet architecture of [13]. As we show below, DeconvNet-based reconstruction of the n-th\nlayer input Xn is either equivalent or similar to computing the gradient of the visualised neuron ac-\n2http://www.robots.ox.ac.uk/˜vgg/software/iseg/\n4\nFigure 2:\nImage-speciﬁc class saliency maps for the top-1 predicted class in ILSVRC-2013\ntest images. The maps were extracted using a single back-propagation pass through a classiﬁcation\nConvNet. No additional annotation (except for the image labels) was used in training.\n5\nFigure 3:\nWeakly supervised object segmentation using ConvNets (Sect. 3.2). Left: images\nfrom the test set of ILSVRC-2013. Left-middle: the corresponding saliency maps for the top-1\npredicted class. Right-middle: thresholded saliency maps: blue shows the areas used to compute\nthe foreground colour model, cyan – background colour model, pixels shown in red are not used for\ncolour model estimation. Right: the resulting foreground segmentation masks.\n6\ntivity f with respect to Xn, so DeconvNet effectively corresponds to the gradient back-propagation\nthrough a ConvNet.\nFor the convolutional layer Xn+1 = Xn⋆Kn, the gradient is computed as ∂f/∂Xn = ∂f/∂Xn+1⋆\nc\nKn, where Kn and c\nKn are the convolution kernel and its ﬂipped version, respectively. The convo-\nlution with the ﬂipped kernel exactly corresponds to computing the n-th layer reconstruction Rn in\na DeconvNet: Rn = Rn+1 ⋆c\nKn.\nFor the RELU rectiﬁcation layer Xn+1 = max(Xn, 0), the sub-gradient takes the form: ∂f/∂Xn =\n∂f/∂Xn+1 1 (Xn > 0), where 1 is the element-wise indicator function. This is slightly different\nfrom the DeconvNet RELU reconstruction: Rn = Rn+1 1 (Rn+1 > 0), where the sign indicator is\ncomputed on the output reconstruction Rn+1 instead of the layer input Xn.\nFinally, consider a max-pooling layer Xn+1(p) = maxq∈Ω(p) Xn(q), where the element p of\nthe output feature map is computed by pooling over the corresponding spatial neighbourhood\nΩ(p) of the input.\nThe sub-gradient is computed as ∂f/∂Xn(s) = ∂f/∂Xn+1(p) 1(s =\narg maxq∈Ω(p) Xn(q)). Here, arg max corresponds to the max-pooling “switch” in a DeconvNet.\nWe can conclude that apart from the RELU layer, computing the approximate feature map recon-\nstruction Rn using a DeconvNet is equivalent to computing the derivative ∂f/∂Xn using back-\npropagation, which is a part of our visualisation algorithms. Thus, gradient-based visualisation can\nbe seen as the generalisation of that of [13], since the gradient-based techniques can be applied to\nthe visualisation of activities in any layer, not just a convolutional one. In particular, in this paper\nwe visualised the class score neurons in the ﬁnal fully-connected layer.\nIt should be noted that our class model visualisation (Sect. 2) depicts the notion of a class, memo-\nrised by a ConvNet, and is not speciﬁc to any particular image. At the same time, the class saliency\nvisualisation (Sect. 3) is image-speciﬁc, and in this sense is related to the image-speciﬁc convolu-\ntional layer visualisation of [13] (the main difference being that we visualise a neuron in a fully\nconnected layer rather than a convolutional layer).\n5\nConclusion\nIn this paper, we presented two visualisation techniques for deep classiﬁcation ConvNets. The ﬁrst\ngenerates an artiﬁcial image, which is representative of a class of interest. The second computes\nan image-speciﬁc class saliency map, highlighting the areas of the given image, discriminative with\nrespect to the given class. We showed that such saliency map can be used to initialise GraphCut-\nbased object segmentation without the need to train dedicated segmentation or detection models.\nFinally, we demonstrated that gradient-based visualisation techniques generalise the DeconvNet\nreconstruction procedure [13]. In our future research, we are planning to incorporate the image-\nspeciﬁc saliency maps into learning formulations in a more principled manner.\nAcknowledgements\nThis work was supported by ERC grant VisRec no. 228180. We gratefully acknowledge the support\nof NVIDIA Corporation with the donation of the Tesla K40 GPU used for this research.\nReferences\n[1] D. Baehrens, T. Schroeter, S. Harmeling, M. Kawanabe, K. Hansen, and K.-R. M¨uller. How to explain\nindividual classiﬁcation decisions. JMLR, 11:1803–1831, 2010.\n[2] A. Berg, J. Deng, and L. Fei-Fei.\nLarge scale visual recognition challenge (ILSVRC), 2010.\nURL\nhttp://www.image-net.org/challenges/LSVRC/2010/.\n[3] Y. Boykov and M. P. Jolly. Interactive graph cuts for optimal boundary and region segmentation of objects\nin N-D images. In Proc. ICCV, volume 2, pages 105–112, 2001.\n[4] D. C. Ciresan, U. Meier, and J. Schmidhuber. Multi-column deep neural networks for image classiﬁcation.\nIn Proc. CVPR, pages 3642–3649, 2012.\n[5] D. Erhan, Y. Bengio, A. Courville, and P. Vincent. Visualizing higher-layer features of a deep network.\nTechnical Report 1341, University of Montreal, Jun 2009.\n[6] P. Felzenszwalb, D. Mcallester, and D. Ramanan. A discriminatively trained, multiscale, deformable part\nmodel. In Proc. CVPR, 2008.\n7\n[7] G. E. Hinton, S. Osindero, and Y. W. Teh. A fast learning algorithm for deep belief nets. Neural Compu-\ntation, 18(7):1527–1554, 2006.\n[8] A. Krizhevsky, I. Sutskever, and G. E. Hinton. ImageNet classiﬁcation with deep convolutional neural\nnetworks. In NIPS, pages 1106–1114, 2012.\n[9] Q. Le, M. Ranzato, R. Monga, M. Devin, K. Chen, G. Corrado, J. Dean, and A. Ng. Building high-level\nfeatures using large scale unsupervised learning. In Proc. ICML, 2012.\n[10] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition.\nProceedings of the IEEE, 86(11):2278–2324, 1998.\n[11] F. Perronnin, J. S´anchez, and T. Mensink. Improving the Fisher kernel for large-scale image classiﬁcation.\nIn Proc. ECCV, 2010.\n[12] K. Simonyan, A. Vedaldi, and A. Zisserman. Deep Fisher networks and class saliency maps for ob-\nject classiﬁcation and localisation. In ILSVRC workshop, 2013. URL http://image-net.org/\nchallenges/LSVRC/2013/slides/ILSVRC_az.pdf.\n[13] M. D. Zeiler and R. Fergus.\nVisualizing and understanding convolutional networks.\nCoRR,\nabs/1311.2901v3, 2013.\n8\n",
        "sentence": " Significant visualization techniques have been developed to understand the deep image representations on trained networks [5, 6, 7, 8, 9, 10, 11]. Another stream of visualization aims to understand what each neuron has learned in a pretrained network and synthesize an image that maximally activates individual features [5, 9] or the class prediction scores [6]. In addition, some regularizers are incorporated as a natural image prior to improve the visualization quality, including α−norm [6], total variation[7], jitter[7], Gaussian blur [9], data-driven patch priors [17], etc."
    },
    {
        "title": "Understanding deep image representations by inverting them",
        "author": [
            "Aravindh Mahendran",
            "Andrea Vedaldi"
        ],
        "venue": "In CVPR,",
        "citeRegEx": "7",
        "shortCiteRegEx": "7",
        "year": 2015,
        "abstract": "Image representations, from SIFT and Bag of Visual Words to Convolutional\nNeural Networks (CNNs), are a crucial component of almost any image\nunderstanding system. Nevertheless, our understanding of them remains limited.\nIn this paper we conduct a direct analysis of the visual information contained\nin representations by asking the following question: given an encoding of an\nimage, to which extent is it possible to reconstruct the image itself? To\nanswer this question we contribute a general framework to invert\nrepresentations. We show that this method can invert representations such as\nHOG and SIFT more accurately than recent alternatives while being applicable to\nCNNs too. We then use this technique to study the inverse of recent\nstate-of-the-art CNN image representations for the first time. Among our\nfindings, we show that several layers in CNNs retain photographically accurate\ninformation about the image, with different degrees of geometric and\nphotometric invariance.",
        "full_text": "Understanding Deep Image Representations by Inverting Them\nAravindh Mahendran\nUniversity of Oxford\nAndrea Vedaldi\nUniversity of Oxford\nAbstract\nImage representations, from SIFT and Bag of Visual\nWords to Convolutional Neural Networks (CNNs), are a\ncrucial component of almost any image understanding sys-\ntem. Nevertheless, our understanding of them remains lim-\nited. In this paper we conduct a direct analysis of the visual\ninformation contained in representations by asking the fol-\nlowing question: given an encoding of an image, to which\nextent is it possible to reconstruct the image itself? To an-\nswer this question we contribute a general framework to in-\nvert representations. We show that this method can invert\nrepresentations such as HOG and SIFT more accurately\nthan recent alternatives while being applicable to CNNs\ntoo. We then use this technique to study the inverse of re-\ncent state-of-the-art CNN image representations for the ﬁrst\ntime. Among our ﬁndings, we show that several layers in\nCNNs retain photographically accurate information about\nthe image, with different degrees of geometric and photo-\nmetric invariance.\n1. Introduction\nMost image understanding and computer vision methods\nbuild on image representations such as textons [15], his-\ntogram of oriented gradients (SIFT [18] and HOG [4]), bag\nof visual words [3][25], sparse [35] and local coding [32],\nsuper vector coding [37], VLAD [9], Fisher Vectors [21],\nand, lately, deep neural networks, particularly of the convo-\nlutional variety [13, 23, 36]. However, despite the progress\nin the development of visual representations, their design is\nstill driven empirically and a good understanding of their\nproperties is lacking. While this is true of shallower hand-\ncrafted features, it is even more so for the latest generation\nof deep representations, where millions of parameters are\nlearned from data.\nIn this paper we conduct a direct analysis of representa-\ntions by characterising the image information that they re-\ntain (Fig. 1). We do so by modeling a representation as a\nfunction Φ(x) of the image x and then computing an ap-\nproximated inverse φ−1, reconstructing x from the code\nΦ(x). A common hypothesis is that representations col-\nlapse irrelevant differences in images (e.g. illumination or\nFigure 1. What is encoded by a CNN? The ﬁgure shows ﬁve\npossible reconstructions of the reference image obtained from the\n1,000-dimensional code extracted at the penultimate layer of a ref-\nerence CNN[13] (before the softmax is applied) trained on the Im-\nageNet data. From the viewpoint of the model, all these images are\npractically equivalent. This image is best viewed in color/screen.\nviewpoint), so that Φ should not be uniquely invertible.\nHence, we pose this as a reconstruction problem and ﬁnd\na number of possible reconstructions rather than a single\none. By doing so, we obtain insights into the invariances\ncaptured by the representation.\nOur contributions are as follows. First, we propose a\ngeneral method to invert representations, including SIFT,\nHOG, and CNNs (Sect. 2). Crucially, this method uses only\ninformation from the image representation and a generic\nnatural image prior, starting from random noise as initial\nsolution, and hence captures only the information contained\nin the representation itself. We discuss and evaluate differ-\nent regularization penalties as natural image priors. Sec-\nond, we show that, despite its simplicity and generality, this\nmethod recovers signiﬁcantly better reconstructions from\nDSIFT and HOG compared to recent alternatives [31]. As\nwe do so, we emphasise a number of subtle differences be-\ntween these representations and their effect on invertibility.\nThird, we apply the inversion technique to the analysis of\nrecent deep CNNs, exploring their invariance by sampling\npossible approximate reconstructions. We relate this to the\ndepth of the representation, showing that the CNN gradually\nbuilds an increasing amount of invariance, layer after layer.\nFourth, we study the locality of the information stored in\n1\narXiv:1412.0035v1  [cs.CV]  26 Nov 2014\nthe representations by reconstructing images from selected\ngroups of neurons, either spatially or by channel.\nThe rest of the paper is organised as follows. Sect. 2 in-\ntroduces the inversion method, posing this as a regularised\nregression problem and proposing a number of image priors\nto aid the reconstruction. Sect. 3 introduces various repre-\nsentations: HOG and DSIFT as examples of shallow repre-\nsentations, and state-of-the-art CNNs as an example of deep\nrepresentations. It also shows how HOG and DSIFT can be\nimplemented as CNNs, simplifying the computation of their\nderivatives. Sect. 4 and 5 apply the inversion technique to\nthe analysis of respectively shallow (HOG and DSIFT) and\ndeep (CNNs) representations. Finally, Sect. 6 summarises\nour ﬁndings.\nWe use the matconvnet toolbox [30] for implementing\nconvolutional neural networks.\nRelated work. There is a signiﬁcant amount of work in un-\nderstanding representations by means of visualisations. The\nworks most related to ours are Weinzaepfel et al. [33] and\nVondrick et al. [31] which invert sparse DSIFT and HOG\nfeatures respectively. While our goal is similar to theirs,\nour method is substantially different from a technical view-\npoint, being based on the direct solution of a regularised\nregression problem. The beneﬁt is that our technique ap-\nplies equally to shallow (SIFT, HOG) and deep (CNN) rep-\nresentations.\nCompared to existing inversion techniques\nfor dense shallow representations [31], it is also shown to\nachieve superior results, both quantitatively and qualita-\ntively.\nAn interesting conclusion of [31, 33] is that, while HOG\nand SIFT may not be exactly invertible, they capture a sig-\nniﬁcant amount of information about the image. This is in\napparent contradiction with the results of Tatu et al. [27]\nwho show that it is possible to make any two images\nlook nearly identical in SIFT space up to the injection of\nadversarial noise. A symmetric effect was demonstrated\nfor CNNs by Szegedy et al. [26], where an imperceptible\namount of adversarial noise sufﬁces to change the predicted\nclass of an image. The apparent inconsistency is easily re-\nsolved, however, as the methods of [26, 27] require the in-\njection of high-pass structured noise which is very unlikely\nto occur in natural images.\nOur work is also related to the DeConvNet method of\nZeiler and Fergus [36], who backtrack the network com-\nputations to identify which image patches are responsible\nfor certain neural activations. Simonyan et al. [24], how-\never, demonstrated that DeConvNets can be interpreted as a\nsensitivity analysis of the network input/output relation. A\nconsequence is that DeConvNets do not study the problem\nof representation inversion in the sense adopted here, which\nhas signiﬁcant methodological consequences; for example,\nDeConvNets require auxiliary information about the acti-\nvations in several intermediate layers, while our inversion\nuses only the ﬁnal image code. In other words, DeConvNets\nlook at how certain network outputs are obtained, whereas\nwe look for what information is preserved by the network\noutput.\nThe problem of inverting representations, particularly\nCNN-based ones, is related to the problem of inverting\nneural networks, which received signiﬁcant attention in the\npast. Algorithms similar to the back-propagation technique\ndeveloped here were proposed by [14, 16, 19, 34], along\nwith alternative optimisation strategies based on sampling.\nHowever, these methods did not use natural image priors as\nwe do, nor were applied to the current generation of deep\nnetworks. Other works [10, 28] specialised on inverting\nnetworks in the context of dynamical systems and will not\nbe discussed further here. Others [1] proposed to learn a\nsecond neural network to act as the inverse of the original\none, but this is complicated by the fact that the inverse is\nusually not unique. Finally, auto-encoder architectures [8]\ntrain networks together with their inverses as a form of su-\npervision; here we are interested instead in visualising feed-\nforward and discriminatively-trained CNNs now popular in\ncomputer vision.\n2. Inverting representations\nThis section introduces our method to compute an ap-\nproximate inverse of an image representation. This is for-\nmulated as the problem of ﬁnding an image whose repre-\nsentation best matches the one given [34]. Formally, given\na representation function Φ : RH×W ×C →Rd and a rep-\nresentation Φ0 = Φ(x0) to be inverted, reconstruction ﬁnds\nthe image x ∈RH×W ×C that minimizes the objective:\nx∗=\nargmin\nx∈RH×W ×C ℓ(Φ(x), Φ0) + λR(x)\n(1)\nwhere the loss ℓcompares the image representation Φ(x) to\nthe target one Φ0 and R : RH×W ×C →R is a regulariser\ncapturing a natural image prior.\nMinimising (1) results in an image x∗that “resembles”\nx0 from the viewpoint of the representation. While there\nmay be no unique solution to this problem, sampling the\nspace of possible reconstructions can be used to charac-\nterise the space of images that the representation deems to\nbe equivalent, revealing its invariances.\nWe next discusses the choice of loss and regularizer.\nLoss function. There are many possible choices of the loss\nfunction ℓ. While we use the Euclidean distance:\nℓ(Φ(x), Φ0) = ∥Φ(x) −Φ0∥2,\n(2)\nit is possible to change the nature of the loss entirely, for ex-\nample to optimize selected neural responses. The latter was\nused in [5, 24] to generate images representative of given\nneurons.\nRegularisers.\nDiscriminatively-trained representations\nmay discard a signiﬁcant amount of low-level image statis-\n2\ntics as these are usually not interesting for high-level tasks.\nAs this information is nonetheless useful for visualization, it\ncan be partially recovered by restricting the inversion to the\nsubset of natural images X ⊂RH×W ×C. However, min-\nimising over X requires addressing the challenge of mod-\neling this set. As a proxy one can incorporate in the re-\nconstruction an appropriate image prior. Here we experi-\nment with two such priors. The ﬁrst one is simply the α-\nnorm Rα(x) = ∥x∥α\nα, where x is the vectorised and mean-\nsubtracted image. By choosing a relatively large exponent\n(α = 6 is used in the experiments) the range of the image\nis encouraged to stay within a target interval instead of di-\nverging.\nA second richer regulariser is total variation (TV)\nRV β(x), encouraging images to consist of piece-wise con-\nstant patches. For continuous functions (or distributions)\nf : RH×W ⊃Ω→R, the TV norm is given by:\nRV β(f) =\nZ\nΩ\n \u0012∂f\n∂u(u, v)\n\u00132\n+\n\u0012∂f\n∂v (u, v)\n\u00132! β\n2\ndu dv\nwhere β = 1. Here images are discrete (x ∈RH×W ) and\nthe TV norm is replaced by the ﬁnite-difference approxima-\ntion:\nRV β(x) =\nX\ni,j\n\u0010\n(xi,j+1 −xij)2 + (xi+1,j −xij)2\u0011 β\n2 .\nIt was observed empirically that the TV regularizer (β = 1)\nin the presence of subsampling, also caused by max pooling\nin CNNs, leads to “spikes” in the reconstruction. This is a\nknown problem in TV-based image interpolation (see e.g.\nFig. 3 in [2]) and is illustrated in Fig. 2.left when inverting\na layer in a CNN. The “spikes” occur at the locations of\nthe samples because: (1) the TV norm along any path be-\ntween two samples depends only on the overall amount of\nintensity change (not on the sharpness of the changes) and\n(2) integrated on the 2D image, it is optimal to concentrate\nsharp changes around a boundary with a small perimeter.\nHyper-Laplacian priors with β < 1 are often used as a better\nmatch of the gradient statistics of natural images [12], but\nthey only exacerbate this issue. Instead, we trade-off the\nsharpness of the image with the removal of such artifacts\nby choosing β > 1 which, by penalising large gradients,\ndistributes changes across regions rather than concentrating\nthem at a point or curve. We refer to this as the V β regular-\nizer. As seen in Fig. 2 (right), the spikes are removed with\nβ = 2 but the image is washed out as edges are penalized\nmore than with β = 1.\nWhen the target of the reconstruction is a colour image,\nboth regularisers are summed for each colour channel.\nBalancing the different terms. Balancing loss and regu-\nlariser(s) requires some attention. While an optimal tuning\ncan be achieved by cross-validation, it is important to start\nFigure 2. Left: Spikes in a inverse of norm1 features - detail\nshown. Right: Spikes removed by a V β regularizer with β = 2.\nfrom reasonable settings of the parameters. First, the loss is\nreplaced by the normalized version ∥Φ(x) −Φ0∥2\n2/∥Φ0∥2\n2.\nThis ﬁxes its dynamic range, as after normalisation the loss\nnear the optimum can be expected to be contained in the\n[0, 1) interval, touching zero at the optimum. In order to\nmake the dynamic range of the regulariser(s) comparable\none can aim for a solution x∗which has roughly unitary\nEuclidean norm. While representations are largely insensi-\ntive to the scaling of the image range, this is not exactly true\nfor the ﬁrst few layers of CNNs, where biases are tuned to a\n“natural” working range. This can be addressed by consid-\nering the objective ∥Φ(σx) −Φ0∥2\n2/∥Φ0∥2\n2 + R(x) where\nthe scaling σ is the average Euclidean norm of natural im-\nages in a training set.\nSecond, the multiplier λα of the α-norm regularizer\nshould be selected to encourage the reconstructed image\nσx to be contained in a natural range [−B, B] (e.g. in\nmost CNN implementations B\n= 128).\nIf most pix-\nels in σx have a magnitude similar to B, then Rα(x) ≈\nHWBα/σα, and λα ≈σα/(HWBα). A similar argu-\nment suggests to pick the V β-norm regulariser coefﬁcient\nas λV β ≈σβ/(HW(aB)β), where a is a small fraction\n(e.g. a = 1%) relating the dynamic range of the image to\nthat of its gradient.\nThe ﬁnal form of the objective function is\n∥Φ(σx) −Φ0∥2\n2/∥Φ0∥2\n2 + λαRα(x) + λV βRV β(x) (3)\nIt is in general non convex because of the nature of Φ. We\nnext discuss how to optimize it.\n2.1. Optimisation\nFinding an optimizer of the objective (1) may seem a\nhopeless task as most representations Φ involve strong non-\nlinearities; in particular, deep representations are a chain\nof several non-linear layers. Nevertheless, simple gradient\ndescent (GD) procedures have been shown to be very effec-\ntive in learning such models from data, which is arguably\nan even harder task. Hence, it is not unreasonable to use\nGD to solve (1) too. We extend GD to incorporate a few ex-\ntensions that proved useful in learning deep networks [13],\nas discussed below.\nMomentum. GD is extended to use momentum:\nµt+1 ←mµt −ηt∇E(x),\nxt+1 ←xt + µt\n3\nwhere E(x) = ℓ(Φ(x), Φ0) + λR(x) is the objective func-\ntion. The vector µt is a weighed average of the last several\ngradients, with decaying factor m = 0.9. Learning pro-\nceeds a few hundred iterations with a ﬁxed learning rate ηt\nand is reduced tenfold, until convergence.\nComputing derivatives. Applying GD requires comput-\ning the derivatives of the loss function composed with the\nrepresentation Φ(x). While the squared Euclidean loss is\nsmooth, this is not the case for the representation. A key\nfeature of CNNs is the ability of computing the deriva-\ntives of each computational layer, composing the latter in\nan overall derivative of the whole function using back-\npropagation. Our translation of HOG and DSIFT into CNN\nallows us to apply the same technique to these computer\nvision representations too.\n3. Representations\nThis section describes the image representations stud-\nied in the paper: DSIFT (Dense-SIFT), HOG, and refer-\nence deep CNNs. Furthermore, it shows how to implement\nDSIFT and HOG in a standard CNN framework in order to\ncompute their derivatives. Being able to compute deriva-\ntives is the only requirement imposed by the algorithm of\nSect. 2.1. Implementing DSIFT and HOG in a standard\nCNN framework makes derivative computation convenient.\nCNN-A: deep networks.\nAs a reference deep network\nwe consider the Caffe-Alex [11] model (CNN-A), which\nclosely reproduces the network by Krizhevsky et al. [13].\nThis and many other similar networks alternate the fol-\nlowing computational building blocks: linear convolution,\nReLU gating, spatial max-pooling, and group normalisa-\ntion. Each such block takes as input a d-dimensional image\nand produces as output a k-dimensional one. Blocks can\nadditionally pad the image (with zeros for the convolutional\nblocks and with −∞for max pooling) or subsample the\ndata. The last several layers are deemed “fully connected”\nas the support of the linear ﬁlters coincides with the size of\nthe image; however, they are equivalent to ﬁltering layers in\nall other respects. Table 2 details the structure of CNN-A.\nCNN-DSIFT and CNN-HOG. This section shows how\nDSIFT [17, 20] and HOG [4] can be implemented as CNNs.\nThis formalises the relation between CNNs and these stan-\ndard representations.\nIt also makes derivative computa-\ntion for these representations simple; for the inversion al-\ngorithm of Sect. 2. The DSIFT and HOG implementations\nin the VLFeat library [29] are used as numerical references.\nThese are equivalent to Lowe’s [17] SIFT and the DPM\nV5 HOG [6, 7].\nSIFT and HOG involve: computing and binning image\ngradients, pooling binned gradients into cell histograms,\ngrouping cells into blocks, and normalising the blocks. De-\nnote by g the gradient at a given pixel and consider binning\nthis into one of K orientations (where K = 8 for SIFT and\nK = 18 for HOG). This can be obtained in two steps: di-\nrectional ﬁltering and gating. The k-th directional ﬁlter is\nGk = u1kGx + u2kGy where\nuk =\n\u0014\ncos 2πk\nK\nsin 2πk\nK\n\u0015\n,\nGx =\n\n\n0\n0\n0\n−1\n0\n1\n0\n0\n0\n\n,\nGy = G⊤\nx .\nThe output of a directional ﬁlter is the projection ⟨g, uk⟩of\nthe gradient along direction uk. A suitable gating function\nimplements binning into a histogram element hk. DSIFT\nuses bilinear orientation binning, given by\nhk = ∥g∥max\n\u001a\n0, 1 −K\n2π cos−1 ⟨g, uk⟩\n∥g∥\n\u001b\n,\nwhereas HOG (in the DPM V5 variant) uses hard assign-\nments hk = ∥g∥1 [⟨g, uk⟩> ∥g∥cos π/K]. Filtering is\na standard CNN operation but these binning functions are\nnot. While their implementation is simple, an interesting\nalternative is the approximated bilinear binning:\nhk ≈∥g∥max\n\u001a\n0,\n1\n1 −a\n⟨g, uk⟩\n∥g∥\n−\na\n1 −a\n\u001b\n∝max {0, ⟨g, uk⟩−a∥g∥} ,\na = cos 2π/K.\nThe norm-dependent offset ∥g∥is still non-standard, but the\nReLU operator is, which shows to which extent approxi-\nmate binning can be achieved in typical CNNs.\nThe next step is to pool the binned gradients into cell\nhistograms using bilinear spatial pooling, followed by ex-\ntracting blocks of 2 × 2 (HOG) or 4 × 4 (SIFT) cells. Both\nsuch operations can be implemented by banks of linear ﬁl-\nters. Cell blocks are then l2 normalised, which is a special\ncase of the standard local response normalisation layer. For\nHOG, blocks are further decomposed back into cells, which\nrequires another ﬁlter bank. Finally, the descriptor values\nare clamped from above by applying y = min{x, 0.2} to\neach component, which can be reduced to a combination of\nlinear and ReLU layers.\nThe conclusion is that approximations to DSIFT and\nHOG can be implemented with conventional CNN compo-\nnents plus the non-conventional gradient norm offset. How-\never, all the ﬁlters involved are much sparser and simpler\nthan the generic 3D ﬁlters in learned CNNs. Nonetheless,\nin the rest of the paper we will use exact CNN equivalents of\nDSIFT and HOG, using modiﬁed or additional CNN com-\nponents as needed.\n1 These CNNs are numerically indis-\n1This requires addressing a few more subtleties. In DSIFT gradient\ncontributions are usually weighted by a Gaussian centered at each descrip-\ntor (a 4 × 4 cell block); here we use the VLFeat approximation (fast op-\ntion) of weighting cells rather than gradients, which can be incorporated in\nthe block-forming ﬁlters. In UoCTTI HOG, cells contain both oriented and\nunoriented gradients (27 components in total) as well as 4 texture compo-\nnents. The latter are ignored for simplicity, while the unoriented gradients\nare obtained as average of the oriented ones in the block-forming ﬁlters.\n4\ndescriptors\nHOG\nHOG\nHOGb\nDSIFT\nmethod\nHOGgle\nour\nour\nour\nerror (%)\n66.20\n28.10\n10.67\n10.89\n±13.7\n±7.9\n±5.2\n±7.5\nTable 1. Average reconstruction error of different representation\ninversion methods, applied to HOG and DSIFT. HOGb denotes\nHOG with bilinear orientation assignments. The standard devia-\ntion shown is the standard deviation of the error and not the stan-\ndard deviation of the mean error.\nFigure 4. Effect of V β regularization. The same inversion algo-\nrithm visualized in Fig. 3(d) is used with a smaller (λV β = 0.5),\ncomparable (λV β = 5.0), and larger (λV β = 50) regularisation\ncoefﬁcient.\ntinguishable from the VLFeat reference implementations,\nbut, true to their CNN nature, allow computing the feature\nderivatives as required by the algorithm of Sect. 2.\nNext we apply the algorithm from Sect. 2 on CNN-A,\nCNN-DSIFT and CNN-HOG to analyze our method.\n4. Experiments with shallow representations\nThis section evaluates the representation inversion\nmethod of Sect. 2 by applying it to HOG and DSIFT. The\nanalysis includes both a qualitative (Fig. 3) and quantitative\n(Table 1) comparison with existing technique. The quanti-\ntative evaluation reports a normalized reconstruction error\n∥Φ(x∗) −Φ(xi)∥2/NΦ averaged over 100 images xi from\nthe ILSVRC 2012 challenge [22] validation data (images 1\nto 100). A normalization is essential to place the Euclidean\ndistance in the context of the volume occupied by the fea-\ntures: if the features are close together, then even an Eu-\nclidean distance of 0.1 is very large, but if the features are\nspread out, then even an Euclidean distance of 105 may be\nvery small. We use NΦ to be the average pairwise euclidean\ndistance between Φ(xi)’s across the 100 test images.\nWe ﬁx the parameters in equation 3 to λα = 2.16 × 108,\nλV β = 5, and β = 2.\nThe closest alternative to our method is HOGgle, a tech-\nnique introduced by Vondrick et al. [31] for the visual-\nisation of HOG features.\nThe HOGgle code is publicly\navailable from the authors’ website and is used through-\nout these experiments. Crucially, HOGgle is pre-trained to\ninvert the UoCTTI implementation of HOG, which is nu-\nmerically equivalent to CNN-HOG (Sect. 3), allowing for a\ndirect comparison between algorithms.\nCuriously, in UoCTTI HOG the l2 normalisation factor is computed con-\nsidering only the unoriented gradient components in a block, but applied\nto all, which requires modifying the normalization operator. Finally, when\nblocks are decomposed back to cells, they are averaged rather than stacked\nas in the original Dalal-Triggs HOG, which can be implemented in the\nblock-decomposition ﬁlters.\na\nb\nc\nd\nFigure 5. Test images for qualitative results.\nCompared to our method, HOGgle is fast (2-3s vs 60s\non the same CPU) but not very accurate, as it is apparent\nboth qualitatively (Fig. 3.c vs d) and quantitatively (66%\nvs 28% reconstruction error, see Table. 1). Interestingly,\n[31] propose a direct optimisation method similar to (1),\nbut show that it does not perform better than HOGgle. This\ndemonstrates the importance of the choice of regulariser\nand the ability of computing the derivative of the represen-\ntation. The effect of the regularizer λV β is further analysed\nin Fig. 4 (and later in Table 3): without this prior infor-\nmation, the reconstructions present a signiﬁcant amount of\ndiscretization artifacts.\nIn terms of speed, an advantage of optimizing (1) is that\nit can be switched to use GPU code immediately given the\nunderlying CNN framework; doing so results in a ten-fold\nspeedup. Furthermore the CNN-based implementation of\nHOG and DSIFT wastes signiﬁcant resources using generic\nﬁltering code despite the particular nature of the ﬁlters in\nthese two representations.\nHence we expect that an op-\ntimized implementation could be several times faster than\nthis.\nIt is also apparent that different representations can be\neasier or harder to invert. In particular, modifying HOG\nto use bilinear gradient orientation assignments as SIFT\n(Sect. 3) signiﬁcantly reduces the reconstruction error (from\n28% down to 11%) and improves the reconstruction quality\n(Fig. 3.e). More impressive is DSIFT: it is quantitatively\nsimilar to HOG with bilinear orientations, but produces sig-\nniﬁcantly more detailed images (Fig. 3.f). Since HOG uses\na ﬁner quantisation of the gradient compared to SIFT but\notherwise the same cell size and sampling, this result can\nbe imputed to the heavier block-normalisation of HOG that\nevidently discards more image information than SIFT.\n5. Experiments with deep representations\nFigure 8. Effect of V β regularization on CNNs. Inversions of the\nlast layers of CNN-A for Fig. 5.d with a progressively larger regu-\nlariser λV β. This image is best viewed in color/screen.\nThis section evaluates the inversion method applied to\nCNN-A described in Sect. 3.\nCompared to CNN-HOG\n5\n(a) Orig.\n(b) HOG\n(c) HOGgle [31]\n(d) HOG−1\n(e) HOGb−1\n(f) DSIFT−1\nFigure 3. Reconstruction quality of different representation inversion methods, applied to HOG and DSIFT. HOGb denotes HOG with\nbilinear orientation assignments. This image is best viewed on screen.\nlayer\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\nname\nconv1 relu1 mpool1 norm1 conv2 relu2 mpool2 norm2 conv3 relu3 conv4 relu4 conv5 relu5 mpool5\nfc6\nrelu6\nfc7\nrelu7\nfc8\ntype\ncnv\nrelu\nmpool\nnrm\ncnv\nrelu\nmpool\nnrm\ncnv\nrelu\ncnv\nrelu\ncnv\nrelu\nmpool\ncnv\nrelu\ncnv\nrelu\ncnv\nchannels\n96\n96\n96\n96\n256\n256\n256\n256\n384\n384\n384\n384\n256\n256\n256\n4096 4096 4096 4096 1000\nrec. ﬁeld\n11\n11\n19\n19\n51\n51\n67\n67\n99\n99\n131\n131\n163\n163\n195\n355\n355\n355\n355\n355\nTable 2. CNN-A structure. The table speciﬁes the structure of CNN-A along with receptive ﬁeld size of each neuron. The ﬁlters in layers\nfrom 16 to 20 operate as “fully connected”: given the standard image input size of 227 × 227 pixels, their support covers the whole image.\nNote also that their receptive ﬁeld is larger than 227 pixels, but can be contained in the image domain due to padding.\nand CNN-DSIFT, this network is signiﬁcantly larger and\ndeeper. It seems therefore that the inversion problem should\nbe considerably harder. Also, CNN-A is not handcrafted but\nlearned from 1.2M images of the ImageNet ILSVRC 2012\ndata [22].\nThe algorithm of Sect. 2.1 is used to invert the code ob-\ntained from each individual CNN layer for 100 ILSVRC\nvalidation images (these were not used to train the CNN-A\nmodel [13]). Similar to Sect. 4, the normalized inversion er-\nror is computed and reported in Table 3. The experiment is\nrepeated by ﬁxing λα to a ﬁxed value of 2.16×108 and grad-\nually increasing λV β ten-folds, starting from a relatively\nsmall value λ1 = 0.5. The ImageNet ILSVRC mean im-\nage is added back to the reconstruction before visualisation\nas this is subtracted when training the network. Somewhat\nsurprisingly, the quantitative results show that CNNs are, in\nfact, not much harder to invert than HOG. The error rarely\nexceeds 20%, which is comparable to the accuracy of HOG\n(Sect. 4). The last layer is in particular easy to invert with\nan average error of 8.5%.\nWe choose the regularizer coefﬁcients for each represen-\ntation/layer based on a quantitative and qualitative study\nof the reconstruction. We pick λ1 = 0.5 for layers 1-6,\nλ2 = 5.0 for layers 7-12 and λ3 = 50 for layers 13-20. The\nerror value corresponding to these parameters is marked in\nbold face in table 3. Increasing λV β causes a deterioration\nfor the ﬁrst layers, but for the latter layers it helps recover a\nmore visually interpretable reconstruction. Though this pa-\nrameter can be tuned by cross validation on the normalized\nreconstruction error, a selection based on qualitative analy-\nsis is preferred because the method should yield images that\nare visually meaningful.\nQualitatively, Fig. 6 illustrates the reconstruction for a\ntest image from each layer of CNN-A. The progression is\nremarkable. The ﬁrst few layers are essentially an invert-\nible code of the image. All the convolutional layers main-\ntain a photographically faithful representation of the image,\nalthough with increasing fuzziness. The 4,096-dimensional\nfully connected layers are perhaps more interesting, as they\ninvert back to a composition of parts similar but not iden-\ntical to the ones found in the original image. Going from\nrelu7 to fc8 reduces the dimensionality further to just 1,000;\nnevertheless some of these visual elements can still be iden-\ntiﬁed. Similar effects can be observed in the reconstructions\nin Fig. 7. This ﬁgure includes also the reconstruction of an\nabstract pattern, which is not included in any of the Ima-\ngeNet classes; still, all CNN codes capture distinctive visual\nfeatures of the original pattern, clearly indicating that even\nvery deep layers capture visual information.\nNext, Fig. 7 examines the invariance captured by the\nCNN model by considering multiple reconstructions out of\neach deep layer. A careful examination of these images re-\n6\nconv1\nrelu1\nmpool1\nnorm1\nconv2\nrelu2\nmpool2\nnorm2\nconv3\nrelu3\nconv4\nrelu4\nconv5\nrelu5\nmpool5\nfc6\nrelu6\nfc7\nrelu7\nfc8\nFigure 6. CNN reconstruction. Reconstruction of the image of Fig. 5.a from each layer of CNN-A. To generate these results, the regular-\nization coefﬁcient for each layer is chosen to match the highlighted rows in table 3. This ﬁgure is best viewed in color/screen.\nλV β\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\nconv1 relu1 pool1 norm1 conv2 relu2 pool2 norm2 conv3 relu3 conv4 relu4 conv5 relu5 pool5 fc6 relu6 fc7 relu7 fc8\nλ1\n10.0 11.3 21.9 20.3 12.4 12.9 15.5\n15.9\n14.5 16.5 14.9 13.8 12.6 15.6 16.6 12.4 15.8 12.8 10.5 5.3\n±5.0\n±5.5\n±9.2\n±5.0\n±3.1\n±5.3\n±4.7\n±4.6\n±4.7\n±5.3\n±3.8\n±3.8\n±2.8\n±5.1\n±4.6\n±3.5\n±4.5\n±6.4\n±1.9\n±1.1\nλ2\n20.2 22.4 30.3\n28.2\n20.0 17.4 18.2 18.4 14.4 15.1 13.3 14.0 15.4 13.9 15.5 14.2 13.7 15.4 10.8 5.9\n±9.3\n±10.3\n±13.6\n±7.6\n±4.9\n±5.0\n±5.5\n±5.0\n±3.6\n±3.3\n±2.6\n±2.8\n±2.7\n±3.2\n±3.5\n±3.7\n±3.1\n±10.3\n±1.6\n±0.9\nλ3\n40.8 45.2 54.1\n48.1\n39.7 32.8 32.7\n32.4\n25.6 26.9 23.3 23.9 25.7 20.1 19.0 18.6 18.7 17.1 15.5 8.5\n±17.0\n±18.7\n±22.7\n±11.8\n±9.1\n±7.7\n±8.0\n±7.0\n±5.6\n±5.2\n±4.1\n±4.6\n±4.3\n±4.3\n±4.3\n±4.9\n±3.8\n±3.4\n±2.1\n±1.3\nTable 3. Inversion error for CNN-A. Average inversion percentage error (normalized) for all the layers of CNN-A and various amounts\nof V β regularisation: λ1 = 0.5, λ2 = 10λ1 and λ3 = 100λ1. In bold face are the error values corresponding to the regularizer that works\nbest both qualitatively and quantitatively. The deviations speciﬁed in this table are the standard deviations of the errors and not the standard\ndeviations of the mean error value.\npool5\nrelu6\nrelu7\nfc8\npool5\nrelu6\nrelu7\nfc8\nFigure 7. CNN invariances. Multiple reconstructions of the images of Fig. 5.c–d from different deep codes obtained from CNN-A. This\nﬁgure is best seen in colour/screen.\nveals that the codes capture progressively larger deforma-\ntions of the object. In the “ﬂamingo” reconstruction, in par-\nticular, relu7 and fc8 invert back to multiple copies of the\nobject/parts at different positions and scales.\nNote that all these and the original images are nearly in-\ndistinguishable from the viewpoint of the CNN model; it is\ntherefore interesting to note the lack of detail in the deep-\nest reconstructions, showing that the network captures just a\nsketch of the objects, which evidently sufﬁces for classiﬁca-\ntion. Considerably lowering the regulariser parameter still\nyields very accurate inversions, but this time with barely any\nresemblance to a natural image. This conﬁrms that CNNs\nhave strong non-natural confounders.\nWe now examine reconstructions obtained from subset\nof neural responses in different CNN layers. Fig. 9 explores\nthe locality of the codes by reconstructing a central 5 × 5\npatch of features in each layer. The regulariser encourages\nportions of the image that do not contribute to the neural\n7\nconv1\nrelu1\nmpool1\nnorm1\nconv2\nrelu2\nmpool2\nnorm2\nconv3\nrelu3\nconv4\nrelu4\nconv5\nrelu5\nFigure 9. CNN receptive ﬁeld. Reconstructions of the image of Fig. 5.a from the central 5 × 5 neuron ﬁelds at different depths of CNN-A.\nThe white box marks the ﬁeld of view of the 5 × 5 neuron ﬁeld. The ﬁeld of view is the entire image for conv5 and relu5.\nconv1-grp1\nnorm1-grp1\nnorm2-grp1\nconv1-grp1\nnorm1-grp1\nnorm2-grp1\nconv1-grp2\nnorm1-grp2\nnorm2-grp2\nconv1-grp2\nnorm1-grp2\nnorm2-grp2\nFigure 10. CNN neural streams. Reconstructions of the images of Fig. 5.c-b from either of the two neural streams of CNN-A. This ﬁgure\nis best seen in colour/screen.\nresponses to be switched off. The locality of the features is\nobvious in the ﬁgure; what is less obvious is that the effec-\ntive receptive ﬁeld of the neurons is in some cases signiﬁ-\ncantly smaller than the theoretical one - shown as a white\nbox in the image.\nFinally, Fig. 10 reconstructs images from a subset of fea-\nture channels. CNN-A contains in fact two subsets of fea-\nture channels which are independent for the ﬁrst several lay-\ners (up to norm2) [13]. Reconstructing from each subset\nindividually, clearly shows that one group is tuned towards\nlow-frequency colour information whereas the second one\nis tuned to towards high-frequency luminance components.\nRemarkably, this behaviour emerges naturally in the learned\nnetwork without any mechanism directly encouraging this\npattern.\n6. Summary\nThis paper proposed an optimisation method to invert\nshallow and deep representations based on optimizing an\nobjective function with gradient descent. Compared to al-\nternatives, a key difference is the use of image priors such as\nthe V β norm that can recover the low-level image statistics\nremoved by the representation. This tool performs better\nFigure 11. Diversity in the CNN model. mpool5 reconstructions\nshow that the network retains rich information even at such deep\nlevels. This ﬁgure is best viewed in color/screen (zoom in).\nthan alternative reconstruction methods for HOG. Applied\nto CNNs, the visualisations shed light on the information\nrepresented at each layer. In particular, it is clear that a pro-\ngressively more invariant and abstract notion of the image\ncontent is formed in the network.\nIn the future, we shall experiment with more expres-\nsive natural image priors and analyze the effect of network\nhyper-parameters on the reconstructions. We shall extract\nsubsets of neurons that encode object parts and try to estab-\nlish sub-networks that capture different details of the image.\n8\nReferences\n[1] C. M. Bishop. Neural Networks for Pattern Recognition. Clarendon\nPress, Oxford, 1995.\n[2] Y. Chen, R. Ranftl, and T. Pock. A bi-level view of inpainting-based\nimage compression. In Proc. of Computer Vision Winter Workshop,\n2014.\n[3] G. Csurka, C. R. Dance, L. Dan, J. Willamowski, and C. Bray. Visual\ncategorization with bags of keypoints. In Proc. ECCV Workshop on\nStat. Learn. in Comp. Vision, 2004.\n[4] N. Dalal and B. Triggs. Histograms of oriented gradients for human\ndetection. In CVPR, 2005.\n[5] D. Erhan, Y. Bengio, A. Courville, and P. Vincent.\nVisualizing\nhigher-layer features of a deep network. Technical Report 1341, Uni-\nversity of Montreal, 2009.\n[6] P. F. Felzenszwalb, R. B. Girshick, D. McAllester, and D. Ramanan.\nObject detection with discriminatively trained part based models.\nIEEE Transactions on Pattern Analysis and Machine Intelligence,\n32(9):1627–1645, 2010.\n[7] R. B. Girshick, P. F. Felzenszwalb, and D. McAllester.\nDiscrim-\ninatively trained deformable part models, release 5.\nhttp://\npeople.cs.uchicago.edu/˜rbg/latent-release5/.\n[8] G. E. Hinton and R. R. Salakhutdinov. Reducing the dimensionality\nof data with neural networks. Science, 313(5786), 2006.\n[9] H. J´egou, M. Douze, C. Schmid, and P. P´erez. Aggregating local\ndescriptors into a compact image representation. In CVPR, 2010.\n[10] C. A. Jensen, R. D. Reed, R. J. Marks, M. El-Sharkawi, J.-B. Jung,\nR. Miyamoto, G. Anderson, and C. Eggen. Inversion of feedforward\nneural networks: algorithms and applications. Proc. of the IEEE,\n87(9), 1999.\n[11] Y. Jia.\nCaffe: An open source convolutional architecture for fast\nfeature embedding. http://caffe.berkeleyvision.org/,\n2013.\n[12] D. Krishnan and R. Fergus. Fast image deconvolution using hyper-\nlaplacian priors. In NIPS, 2009.\n[13] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classiﬁca-\ntion with deep convolutional neural networks. In NIPS, 2012.\n[14] S. Lee and R. M. Kil. Inverse mapping of continuous functions using\nlocal and global information. IEEE Trans. on Neural Networks, 5(3),\n1994.\n[15] T. Leung and J. Malik. Representing and recognizing the visual ap-\npearance of materials using three-dimensional textons. IJCV, 43(1),\n2001.\n[16] A. Linden and J. Kindermann. Inversion of multilayer nets. In Proc.\nInt. Conf. on Neural Networks, 1989.\n[17] D. G. Lowe. Object recognition from local scale-invariant features.\nIn ICCV, 1999.\n[18] D. G. Lowe. Distinctive image features from scale-invariant key-\npoints. IJCV, 2(60):91–110, 2004.\n[19] B.-L. Lu, H. Kita, and Y. Nishikawa. Inverting feedforward neural\nnetworks using linear and nonlinear programming. IEEE Trans. on\nNeural Networks, 10(6), 1999.\n[20] E. Nowak, F. Jurie, and B. Triggs. Sampling strategies for bag-of-\nfeatures image classiﬁcation. In ECCV, 2006.\n[21] F. Perronnin and C. Dance. Fisher kernels on visual vocabularies for\nimage categorizaton. In CVPR, 2006.\n[22] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma,\nZ. Huang, A. Karpathy, A. Khosla, M. Bernstein, A. C. Berg, and\nL. Fei-Fei.\nImageNet Large Scale Visual Recognition Challenge,\n2014.\n[23] P. Sermanet, D. Eigen, X. Zhang, M. Mathieu, R. Fergus, and Y. Le-\nCun. Overfeat: Integrated recognition, localization and detection us-\ning convolutional networks. In CoRR, volume abs/1312.6229, 2014.\n[24] K. Simonyan, A. Vedaldi, and A. Zisserman.\nDeep inside con-\nvolutional networks: Visualising image classiﬁcation models and\nsaliency maps. In Proc. ICLR, 2014.\n[25] J. Sivic and A. Zisserman. Video Google: A text retrieval approach\nto object matching in videos. In ICCV, 2003.\n[26] C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. J. Good-\nfellow, and R. Fergus.\nIntriguing properties of neural networks.\nCoRR, abs/1312.6199, 2013.\n[27] A. Tatu, F. Lauze, M. Nielsen, and B. Kimia. Exploring the rep-\nresentation capabilities of the HOG descriptor. In ICCV Workshop,\n2011.\n[28] A. R. V´arkonyi-K´oczy and A. R¨ovid. Observer based iterative neu-\nral network model inversion. In IEEE Int. Conf. on Fuzzy Systems,\n2005.\n[29] A. Vedaldi. An open implementation of the SIFT detector and de-\nscriptor. Technical Report 070012, UCLA CSD, 2007.\n[30] A. Vedaldi and K. Lenc. MatConvNet: CNNs for MATLAB. http:\n//www.vlfeat.org/matconvnet/, 2014.\n[31] C. Vondrick, A. Khosla, T. Malisiewicz, and A. Torralba. HOGgles:\nVisualizing object detection features. In ICCV, 2013.\n[32] J. Wang, J. Yang, K. Yu, F. Lv, T. Huang, and Y. Gong. Locality-\nconstrained linear coding for image classiﬁcation. CVPR, 2010.\n[33] P. Weinzaepfel, H. J´egou, and P. P´erez. Reconstructing an image\nfrom its local descriptors. In CVPR, 2011.\n[34] R. J. Williams. Inverting a connectionist network mapping by back-\npropagation of error. In Proc. CogSci, 1986.\n[35] J. Yang, K. Yu, and T. Huang. Supervised translation-invariant sparse\ncoding. In CVPR, 2010.\n[36] M. D. Zeiler and R. Fergus. Visualizing and understanding convolu-\ntional networks. In ECCV, 2014.\n[37] X. Zhou, K. Yu, T. Zhang, and T. S. Huang. Image classiﬁcation us-\ning super-vector coding of local image descriptors. In ECCV, 2010.\n9\n",
        "sentence": " Significant visualization techniques have been developed to understand the deep image representations on trained networks [5, 6, 7, 8, 9, 10, 11]. [7] use the pretrained CNN AlexNet [2] and define a squared Euclidean loss on the activations to capture the representation differences and reconstruct the image. In addition, some regularizers are incorporated as a natural image prior to improve the visualization quality, including α−norm [6], total variation[7], jitter[7], Gaussian blur [9], data-driven patch priors [17], etc. In addition, some regularizers are incorporated as a natural image prior to improve the visualization quality, including α−norm [6], total variation[7], jitter[7], Gaussian blur [9], data-driven patch priors [17], etc. Our methods are similar in spirit to existing methods [7, 8, 13]. We apply a regularizer R(x), total variation(TV) [7] defined as the squared sum on the adjacent pixel’s difference of x, to encourage the spatial smoothness in the output image. [7], we only consider the Euclidean loss over the activations and ignore the regularizer they used to capture the natural image prior. Figure 3: Reconstructions from layers of ranVGG (top) and the pretrained VGG (middle) and [7] (bottom)."
    },
    {
        "title": "Texture synthesis using convolutional neural networks",
        "author": [
            "Leon A. Gatys",
            "Alexander S. Ecker",
            "Matthias Bethge"
        ],
        "venue": "In NIPS,",
        "citeRegEx": "8",
        "shortCiteRegEx": "8",
        "year": 2015,
        "abstract": "Here we introduce a new model of natural textures based on the feature spaces\nof convolutional neural networks optimised for object recognition. Samples from\nthe model are of high perceptual quality demonstrating the generative power of\nneural networks trained in a purely discriminative fashion. Within the model,\ntextures are represented by the correlations between feature maps in several\nlayers of the network. We show that across layers the texture representations\nincreasingly capture the statistical properties of natural images while making\nobject information more and more explicit. The model provides a new tool to\ngenerate stimuli for neuroscience and might offer insights into the deep\nrepresentations learned by convolutional neural networks.",
        "full_text": "Texture Synthesis Using Convolutional Neural\nNetworks\nLeon A. Gatys\nCentre for Integrative Neuroscience, University of T¨ubingen, Germany\nBernstein Center for Computational Neuroscience, T¨ubingen, Germany\nGraduate School of Neural Information Processing, University of T¨ubingen, Germany\nleon.gatys@bethgelab.org\nAlexander S. Ecker\nCentre for Integrative Neuroscience, University of T¨ubingen, Germany\nBernstein Center for Computational Neuroscience, T¨ubingen, Germany\nMax Planck Institute for Biological Cybernetics, T¨ubingen, Germany\nBaylor College of Medicine, Houston, TX, USA\nMatthias Bethge\nCentre for Integrative Neuroscience, University of T¨ubingen, Germany\nBernstein Center for Computational Neuroscience, T¨ubingen, Germany\nMax Planck Institute for Biological Cybernetics, T¨ubingen, Germany\nAbstract\nHere we introduce a new model of natural textures based on the feature spaces\nof convolutional neural networks optimised for object recognition. Samples from\nthe model are of high perceptual quality demonstrating the generative power of\nneural networks trained in a purely discriminative fashion. Within the model, tex-\ntures are represented by the correlations between feature maps in several layers of\nthe network. We show that across layers the texture representations increasingly\ncapture the statistical properties of natural images while making object informa-\ntion more and more explicit. The model provides a new tool to generate stimuli\nfor neuroscience and might offer insights into the deep representations learned by\nconvolutional neural networks.\n1\nIntroduction\nThe goal of visual texture synthesis is to infer a generating process from an example texture, which\nthen allows to produce arbitrarily many new samples of that texture. The evaluation criterion for the\nquality of the synthesised texture is usually human inspection and textures are successfully synthe-\nsised if a human observer cannot tell the original texture from a synthesised one.\nIn general, there are two main approaches to ﬁnd a texture generating process. The ﬁrst approach is\nto generate a new texture by resampling either pixels [5, 28] or whole patches [6, 16] of the original\ntexture. These non-parametric resampling techniques and their numerous extensions and improve-\nments (see [27] for review) are capable of producing high quality natural textures very efﬁciently.\nHowever, they do not deﬁne an actual model for natural textures but rather give a mechanistic pro-\ncedure for how one can randomise a source texture without changing its perceptual properties.\nIn contrast, the second approach to texture synthesis is to explicitly deﬁne a parametric texture\nmodel. The model usually consists of a set of statistical measurements that are taken over the\n1\narXiv:1505.07376v3  [cs.CV]  6 Nov 2015\nconv3_\n1\n256\n...\n4\n3 2\n1\nconv1_ 2\n1\n1\n64\n...\nconv4_\n1\n512\n...\n4\n3 2\n1\nconv5_\n1\n512\n...\n4\n3 2\n1\n# feature \nmaps\npool1\npool2\npool4\npool3\nconv2_\n1\n128\n...\n2\n1\ninput\nGradient\ndescent\nFigure 1: Synthesis method. Texture analysis (left). The original texture is passed through the CNN\nand the Gram matrices Gl on the feature responses of a number of layers are computed. Texture\nsynthesis (right). A white noise image ˆ⃗x is passed through the CNN and a loss function El is\ncomputed on every layer included in the texture model. The total loss function L is a weighted sum\nof the contributions El from each layer. Using gradient descent on the total loss with respect to the\npixel values, a new image is found that produces the same Gram matrices ˆGl as the original texture.\nspatial extent of the image. In the model a texture is uniquely deﬁned by the outcome of those\nmeasurements and every image that produces the same outcome should be perceived as the same\ntexture. Therefore new samples of a texture can be generated by ﬁnding an image that produces the\nsame measurement outcomes as the original texture. Conceptually this idea was ﬁrst proposed by\nJulesz [13] who conjectured that a visual texture can be uniquely described by the Nth-order joint\nhistograms of its pixels. Later on, texture models were inspired by the linear response properties\nof the mammalian early visual system, which resemble those of oriented band-pass (Gabor) ﬁlters\n[10, 21]. These texture models are based on statistical measurements taken on the ﬁlter responses\nrather than directly on the image pixels. So far the best parametric model for texture synthesis\nis probably that proposed by Portilla and Simoncelli [21], which is based on a set of carefully\nhandcrafted summary statistics computed on the responses of a linear ﬁlter bank called Steerable\nPyramid [24]. However, although their model shows very good performance in synthesising a wide\nrange of textures, it still fails to capture the full scope of natural textures.\nIn this work, we propose a new parametric texture model to tackle this problem (Fig. 1). Instead\nof describing textures on the basis of a model for the early visual system [21, 10], we use a con-\nvolutional neural network – a functional model for the entire ventral stream – as the foundation for\nour texture model. We combine the conceptual framework of spatial summary statistics on feature\nresponses with the powerful feature space of a convolutional neural network that has been trained on\nobject recognition. In that way we obtain a texture model that is parameterised by spatially invariant\nrepresentations built on the hierarchical processing architecture of the convolutional neural network.\n2\n2\nConvolutional neural network\nWe use the VGG-19 network, a convolutional neural network trained on object recognition that was\nintroduced and extensively described previously [25]. Here we give only a brief summary of its\narchitecture.\nWe used the feature space provided by the 16 convolutional and 5 pooling layers of the VGG-19\nnetwork. We did not use any of the fully connected layers. The network’s architecture is based on\ntwo fundamental computations:\n1. Linearly rectiﬁed convolution with ﬁlters of size 3 × 3 × k where k is the number of input\nfeature maps. Stride and padding of the convolution is equal to one such that the output\nfeature map has the same spatial dimensions as the input feature maps.\n2. Maximum pooling in non-overlapping 2×2 regions, which down-samples the feature maps\nby a factor of two.\nThese two computations are applied in an alternating manner (see Fig. 1). A number of convolutional\nlayers is followed by a max-pooling layer. After each of the ﬁrst three pooling layers the number of\nfeature maps is doubled. Together with the spatial down-sampling, this transformation results in a\nreduction of the total number of feature responses by a factor of two. Fig. 1 provides a schematic\noverview over the network architecture and the number of feature maps in each layer. Since we\nuse only the convolutional layers, the input images can be arbitrarily large. The ﬁrst convolutional\nlayer has the same size as the image and for the following layers the ratio between the feature map\nsizes remains ﬁxed. Generally each layer in the network deﬁnes a non-linear ﬁlter bank, whose\ncomplexity increases with the position of the layer in the network.\nThe trained convolutional network is publicly available and its usability for new applications is\nsupported by the caffe-framework [12]. For texture generation we found that replacing the max-\npooling operation by average pooling improved the gradient ﬂow and one obtains slightly cleaner\nresults, which is why the images shown below were generated with average pooling. Finally, for\npractical reasons, we rescaled the weights in the network such that the mean activation of each ﬁlter\nover images and positions is equal to one. Such re-scaling can always be done without changing the\noutput of a neural network if the non-linearities in the network are rectifying linear 1.\n3\nTexture model\nThe texture model we describe in the following is much in the spirit of that proposed by Portilla\nand Simoncelli [21]. To generate a texture from a given source image, we ﬁrst extract features of\ndifferent sizes homogeneously from this image. Next we compute a spatial summary statistic on the\nfeature responses to obtain a stationary description of the source image (Fig. 1A). Finally we ﬁnd a\nnew image with the same stationary description by performing gradient descent on a random image\nthat has been initialised with white noise (Fig. 1B).\nThe main difference to Portilla and Simoncelli’s work is that instead of using a linear ﬁlter bank\nand a set of carefully chosen summary statistics, we use the feature space provided by a high-\nperforming deep neural network and only one spatial summary statistic: the correlations between\nfeature responses in each layer of the network.\nTo characterise a given vectorised texture ⃗x in our model, we ﬁrst pass ⃗x through the convolutional\nneural network and compute the activations for each layer l in the network. Since each layer in the\nnetwork can be understood as a non-linear ﬁlter bank, its activations in response to an image form a\nset of ﬁltered images (so-called feature maps). A layer with Nl distinct ﬁlters has Nl feature maps\neach of size Ml when vectorised. These feature maps can be stored in a matrix F l ∈RNl×Ml, where\nF l\njk is the activation of the jth ﬁlter at position k in layer l. Textures are per deﬁnition stationary,\nso a texture model needs to be agnostic to spatial information. A summary statistic that discards\nthe spatial information in the feature maps is given by the correlations between the responses of\n1Source code to generate textures with CNNs as well as the rescaled VGG-19 network can be found at\nhttp://github.com/leongatys/DeepTextures\n3\ndifferent features. These feature correlations are, up to a constant of proportionality, given by the\nGram matrix Gl ∈RNl×Nl, where Gl\nij is the inner product between feature map i and j in layer l:\nGl\nij =\nX\nk\nF l\nikF l\njk.\n(1)\nA set of Gram matrices {G1, G2, ..., GL} from some layers 1, . . . , L in the network in response to\na given texture provides a stationary description of the texture, which fully speciﬁes a texture in our\nmodel (Fig. 1A).\n4\nTexture generation\nTo generate a new texture on the basis of a given image, we use gradient descent from a white noise\nimage to ﬁnd another image that matches the Gram-matrix representation of the original image.\nThis optimisation is done by minimising the mean-squared distance between the entries of the Gram\nmatrix of the original image and the Gram matrix of the image being generated (Fig. 1B).\nLet ⃗x and ˆ⃗x be the original image and the image that is generated, and Gl and ˆGl their respective\nGram-matrix representations in layer l (Eq. 1). The contribution of layer l to the total loss is then\nEl =\n1\n4N 2\nl M 2\nl\nX\ni,j\n\u0010\nGl\nij −ˆGl\nij\n\u00112\n(2)\nand the total loss is\nL(⃗x, ˆ⃗x) =\nL\nX\nl=0\nwlEl\n(3)\nwhere wl are weighting factors of the contribution of each layer to the total loss. The derivative of\nEl with respect to the activations in layer l can be computed analytically:\n∂El\n∂ˆF l\nij\n=\n(\n1\nN 2\nl M 2\nl\n\u0010\n( ˆF l)T \u0010\nGl −ˆGl\u0011\u0011\nji\nif ˆF l\nij > 0\n0\nif ˆF l\nij < 0 .\n(4)\nThe gradients of El, and thus the gradient of L(⃗x, ˆ⃗x), with respect to the pixels ˆ⃗x can be readily\ncomputed using standard error back-propagation [18]. The gradient ∂L\n∂ˆ⃗x can be used as input for\nsome numerical optimisation strategy. In our work we use L-BFGS [30], which seemed a reasonable\nchoice for the high-dimensional optimisation problem at hand. The entire procedure relies mainly\non the standard forward-backward pass that is used to train the convolutional network. Therefore, in\nspite of the large complexity of the model, texture generation can be done in reasonable time using\nGPUs and performance-optimised toolboxes for training deep neural networks [12].\n5\nResults\nWe show textures generated by our model from four different source images (Fig. 2). Each row of\nimages was generated using an increasing number of layers in the texture model to constrain the\ngradient descent (the labels in the ﬁgure indicate the top-most layer included). In other words, for\nthe loss terms above a certain layer we set the weights wl = 0, while for the loss terms below\nand including that layer, we set wl = 1. For example the images in the ﬁrst row (‘conv1 1’) were\ngenerated only from the texture representation of the ﬁrst layer (‘conv1 1’) of the VGG network. The\nimages in the second row (‘pool1’) where generated by jointly matching the texture representations\non top of layer ‘conv1 1’, ‘conv1 2’ and ‘pool1’. In this way we obtain textures that show what\nstructure of natural textures are captured by certain computational processing stages of the texture\nmodel.\nThe ﬁrst three columns show images generated from natural textures. We ﬁnd that constraining all\nlayers up to layer ‘pool4’ generates complex natural textures that are almost indistinguishable from\nthe original texture (Fig. 2, ﬁfth row). In contrast, when constraining only the feature correlations\non the lowest layer, the textures contain little structure and are not far from spectrally matched noise\n4\nconv1_1\npool1\npool4\npool3\npool2\noriginal\nPortilla & Simoncelli\nFigure 2: Generated stimuli. Each row corresponds to a different processing stage in the network.\nWhen only constraining the texture representation on the lowest layer, the synthesised textures have\nlittle structure, similarly to spectrally matched noise (ﬁrst row). With increasing number of layers on\nwhich we match the texture representation we ﬁnd that we generate images with increasing degree of\nnaturalness (rows 2–5; labels on the left indicate the top-most layer included). The source textures in\nthe ﬁrst three columns were previously used by Portilla and Simoncelli [21]. For better comparison\nwe also show their results (last row). The last column shows textures generated from a non-texture\nimage to give a better intuition about how the texture model represents image information.\n5\nconv1_1\npool1\npool4\npool3\npool2\nC\nB\nconv1\nconv2\nconv5\nconv4\nconv3\nA\noriginal\n~852k parameters\n~1k parameters\n~177k parameters\n~10k parameters\nFigure 3: A, Number of parameters in the texture model. We explore several ways to reduce the\nnumber of parameters in the texture model (see main text) and compare the results. B, Textures\ngenerated from the different layers of the caffe reference network [12, 15]. The textures are of\nlesser quality than those generated with the VGG network. C, Textures generated with the VGG\narchitecture but random weights. Texture synthesis fails in this case, indicating that learned ﬁlters\nare crucial for texture generation.\n(Fig. 2, ﬁrst row). We can interpolate between these two extremes by using only the constraints\nfrom all layers up to some intermediate layer. We ﬁnd that the statistical structure of natural images\nis matched on an increasing scale as the number of layers we use for texture generation increases.\nWe did not include any layers above layer ‘pool4’ since this did not improve the quality of the\nsynthesised textures. For comparability we used source textures that were previously used by Portilla\nand Simoncelli [21] and also show the results of their texture model (Fig. 2, last row). 2\nTo give a better intuition for how the texture synthesis works, we also show textures generated from\na non-texture image taken from the ImageNet validation set [23] (Fig. 2, last column). Our algorithm\nproduces a texturised version of the image that preserves local spatial information but discards the\nglobal spatial arrangement of the image. The size of the regions in which spatial information is\npreserved increases with the number of layers used for texture generation. This property can be\nexplained by the increasing receptive ﬁeld sizes of the units over the layers of the deep convolutional\nneural network.\nWhen using summary statistics from all layers of the convolutional neural network, the number\nof parameters of the model is very large. For each layer with Nl feature maps, we match Nl ×\n(Nl + 1)/2 parameters, so if we use all layers up to and including ‘pool4’, our model has ∼852k\nparameters (Fig. 3A, fourth column). However, we ﬁnd that this texture model is heavily over-\nparameterised. In fact, when using only one layer on each scale in the network (i.e. ‘conv1 1’,\n2A curious ﬁnding is that the yellow box, which indicates the source of the original texture, is also placed\ntowards the bottom left corner in the textures generated by our model. As our texture model does not store\nany spatial information about the feature responses, the only possible explanation for such behaviour is that\nsome features in the network explicitly encode the information at the image boundaries. This is exactly what\nwe ﬁnd when inspecting feature maps in the VGG network: Some feature maps, at least from layer ‘conv3 1’\nonwards, only show high activations along their edges. This might originate from the zero-padding that is used\nfor the convolutions in the VGG network and it could be interesting to investigate the effect of such padding on\nlearning and object recognition performance.\n6\nClassification performance\n1.0\n0\n0.2\n0.4\n0.6\n0.8\npool1\npool5\npool4\npool3\npool2\nDecoding layer\ntop1 Gram\ntop5 VGG\ntop1 VGG\ntop5 Gram\nFigure 4: Performance of a linear classiﬁer on top of the texture representations in different layers in\nclassifying objects from the ImageNet dataset. High-level information is made increasingly explicit\nalong the hierarchy of our texture model.\nand ‘pool1-4’), the model contains ∼177k parameters while hardly loosing any quality (Fig. 3A,\nthird column). We can further reduce the number of parameters by doing PCA of the feature vector\nin the different layers of the network and then constructing the Gram matrix only for the ﬁrst k\nprincipal components. By using the ﬁrst 64 principal components for layers ‘conv1 1’, and ‘pool1-\n4’ we can further reduce the model to ∼10k parameters (Fig. 3A, second column). Interestingly,\nconstraining only the feature map averages in layers ‘conv1 1’, and ‘pool1-4’, (1024 parameters),\nalready produces interesting textures (Fig. 3A, ﬁrst column). These ad hoc methods for parameter\nreduction show that the texture representation can be compressed greatly with little effect on the\nperceptual quality of the synthesised textures. Finding minimal set of parameters that reproduces\nthe quality of the full model is an interesting topic of ongoing research and beyond the scope of the\npresent paper. A larger number of natural textures synthesised with the ≈177k parameter model\ncan be found in the Supplementary Material as well as on our website3. There one can also observe\nsome failures of the model in case of very regular, man-made structures (e.g. brick walls).\nIn general, we ﬁnd that the very deep architecture of the VGG network with small convolutional\nﬁlters seems to be particularly well suited for texture generation purposes. When performing the\nsame experiment with the caffe reference network [12], which is very similar to the AlexNet [15], the\nquality of the generated textures decreases in two ways. First, the statistical structure of the source\ntexture is not fully matched even when using all constraints (Fig 3B, ‘conv5’). Second, we observe\nan artifactual grid that overlays the generated textures (Fig 3B). We believe that the artifactual grid\noriginates from the larger receptive ﬁeld sizes and strides in the caffe reference network.\nWhile the results from the caffe reference network show that the architecture of the network is\nimportant, the learned feature spaces are equally crucial for texture generation. When synthesising\na texture with a network with the VGG architecture but random weights, texture generation fails\n(Fig. 3C), underscoring the importance of using a trained network.\nTo understand our texture features better in the context of the original object recognition task of the\nnetwork, we evaluated how well object identity can be linearly decoded from the texture features\nin different layers of the network. For each layer we computed the Gram-matrix representation of\neach image in the ImageNet training set [23] and trained a linear soft-max classiﬁer to predict object\nidentity. As we were not interested in optimising prediction performance, we did not use any data\naugmentation and trained and tested only on the 224 × 224 centre crop of the images. We computed\nthe accuracy of these linear classiﬁers on the ImageNet validation set and compared them to the\nperformance of the original VGG-19 network also evaluated on the 224 × 224 centre crops of the\nvalidation images.\nThe analysis suggests that our texture representation continuously disentangles object identity in-\nformation (Fig. 4). Object identity can be decoded increasingly well over the layers. In fact, linear\ndecoding from the ﬁnal pooling layer performs almost as well as the original network, suggesting\nthat our texture representation preserves almost all high-level information. At ﬁrst sight this might\nappear surprising since the texture representation does not necessarily preserve the global structure\nof objects in non-texture images (Fig. 2, last column). However, we believe that this “inconsis-\n3www.bethgelab.org/deeptextures\n7\ntency” is in fact to be expected and might provide an insight into how CNNs encode object identity.\nThe convolutional representations in the network are shift-equivariant and the network’s task (object\nrecognition) is agnostic to spatial information, thus we expect that object information can be read\nout independently from the spatial information in the feature maps. We show that this is indeed the\ncase: a linear classiﬁer on the Gram matrix of layer ‘pool5’ comes close to the performance of the\nfull network (87.7% vs. 88.6% top 5 accuracy, Fig. 4).\n6\nDiscussion\nWe introduced a new parametric texture model based on a high-performing convolutional neural\nnetwork. Our texture model exceeds previous work as the quality of the textures synthesised using\nour model shows a substantial improvement compared to the current state of the art in parametric\ntexture synthesis (Fig. 2, fourth row compared to last row).\nWhile our model is capable of producing natural textures of comparable quality to non-parametric\ntexture synthesis methods, our synthesis procedure is computationally more expensive. Neverthe-\nless, both in industry and academia, there is currently much effort taken in order to make the eval-\nuation of deep neural networks more efﬁcient [11, 4, 17]. Since our texture synthesis procedure\nbuilds exactly on the same operations, any progress made in the general ﬁeld of deep convolutional\nnetworks is likely to be transferable to our texture synthesis method. Thus we expect considerable\nimprovements in the practical applicability of our texture model in the near future.\nBy computing the Gram matrices on feature maps, our texture model transforms the representations\nfrom the convolutional neural network into a stationary feature space. This general strategy has\nrecently been employed to improve performance in object recognition and detection [9] or texture\nrecognition and segmentation [3]. In particular Cimpoi et al. report impressive performance in\nmaterial recognition and scene segmentation by using a stationary Fisher-Vector representation built\non the highest convolutional layer of readily trained neural networks [3]. In agreement with our\nresults, they show that performance in natural texture recognition continuously improves when using\nhigher convolutional layers as the input to their Fisher-Vector representation. As our main aim is\nto synthesise textures, we have not evaluated the Gram matrix representation on texture recognition\nbenchmarks, but would expect that it also provides a good feature space for those tasks.\nIn recent years, texture models inspired by biological vision have provided a fruitful new analysis\ntool for studying visual perception. In particular the parametric texture model proposed by Por-\ntilla and Simoncelli [21] has sparked a great number of studies in neuroscience and psychophysics\n[8, 7, 1, 22, 20]. Our texture model is based on deep convolutional neural networks that are the\nﬁrst artiﬁcial systems that rival biology in terms of difﬁcult perceptual inference tasks such as ob-\nject recognition [15, 25, 26]. At the same time, their hierarchical architecture and basic computa-\ntional properties admit a fundamental similarity to real neural systems. Together with the increasing\namount of evidence for the similarity of the representations in convolutional networks and those in\nthe ventral visual pathway [29, 2, 14], these properties make them compelling candidate models for\nstudying visual information processing in the brain. In fact, it was recently suggested that textures\ngenerated from the representations of performance-optimised convolutional networks “may there-\nfore prove useful as stimuli in perceptual or physiological investigations” [19]. We feel that our\ntexture model is the ﬁrst step in that direction and envision it to provide an exciting new tool in the\nstudy of visual information processing in biological systems.\nAcknowledgments\nThis work was funded by the German National Academic Foundation (L.A.G.), the Bernstein Center\nfor Computational Neuroscience (FKZ 01GQ1002) and the German Excellency Initiative through\nthe Centre for Integrative Neuroscience T¨ubingen (EXC307)(M.B., A.S.E, L.A.G.)\nReferences\n[1] B. Balas, L. Nakano, and R. Rosenholtz. A summary-statistic representation in peripheral vision explains\nvisual crowding. Journal of vision, 9(12):13, 2009.\n8\n[2] C. F. Cadieu, H. Hong, D. L. K. Yamins, N. Pinto, D. Ardila, E. A. Solomon, N. J. Majaj, and J. J.\nDiCarlo. Deep Neural Networks Rival the Representation of Primate IT Cortex for Core Visual Object\nRecognition. PLoS Comput Biol, 10(12):e1003963, December 2014.\n[3] M. Cimpoi, S. Maji, and A. Vedaldi. Deep convolutional ﬁlter banks for texture recognition and segmen-\ntation. arXiv:1411.6836 [cs], November 2014. arXiv: 1411.6836.\n[4] E. L. Denton, W. Zaremba, J. Bruna, Y. LeCun, and R. Fergus.\nExploiting Linear Structure Within\nConvolutional Networks for Efﬁcient Evaluation. In NIPS, 2014.\n[5] A. Efros and T. K. Leung. Texture synthesis by non-parametric sampling. In Computer Vision, 1999. The\nProceedings of the Seventh IEEE International Conference on, volume 2, pages 1033–1038. IEEE, 1999.\n[6] A. A. Efros and W. T. Freeman. Image quilting for texture synthesis and transfer. In Proceedings of the\n28th annual conference on Computer graphics and interactive techniques, pages 341–346. ACM, 2001.\n[7] J. Freeman and E. P. Simoncelli. Metamers of the ventral stream. Nature Neuroscience, 14(9):1195–1201,\nSeptember 2011.\n[8] J. Freeman, C. M. Ziemba, D. J. Heeger, E. P. Simoncelli, and A. J. Movshon. A functional and perceptual\nsignature of the second visual area in primates. Nature Neuroscience, 16(7):974–981, July 2013.\n[9] K. He, X. Zhang, S. Ren, and J. Sun. Spatial pyramid pooling in deep convolutional networks for visual\nrecognition. arXiv preprint arXiv:1406.4729, 2014.\n[10] D. J. Heeger and J. R. Bergen. Pyramid-based Texture Analysis/Synthesis. In Proceedings of the 22Nd\nAnnual Conference on Computer Graphics and Interactive Techniques, SIGGRAPH ’95, pages 229–238,\nNew York, NY, USA, 1995. ACM.\n[11] M. Jaderberg, A. Vedaldi, and A. Zisserman. Speeding up Convolutional Neural Networks with Low\nRank Expansions. In BMVC 2014, 2014.\n[12] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick, S. Guadarrama, and T. Darrell.\nCaffe: Convolutional architecture for fast feature embedding. In Proceedings of the ACM International\nConference on Multimedia, pages 675–678. ACM, 2014.\n[13] B. Julesz. Visual Pattern Discrimination. IRE Transactions on Information Theory, 8(2), February 1962.\n[14] S. Khaligh-Razavi and N. Kriegeskorte. Deep Supervised, but Not Unsupervised, Models May Explain\nIT Cortical Representation. PLoS Comput Biol, 10(11):e1003915, November 2014.\n[15] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classiﬁcation with deep convolutional neural\nnetworks. In Advances in Neural Information Processing Systems 27, pages 1097–1105, 2012.\n[16] V. Kwatra, A. Sch¨odl, I. Essa, G. Turk, and A. Bobick. Graphcut textures: image and video synthesis\nusing graph cuts. In ACM Transactions on Graphics (ToG), volume 22, pages 277–286. ACM, 2003.\n[17] V. Lebedev, Y. Ganin, M. Rakhuba, I. Oseledets, and V. Lempitsky. Speeding-up Convolutional Neural\nNetworks Using Fine-tuned CP-Decomposition. arXiv preprint arXiv:1412.6553, 2014.\n[18] Y. A. LeCun, L. Bottou, G. B. Orr, and K. R. M¨uller. Efﬁcient backprop. In Neural networks: Tricks of\nthe trade, pages 9–48. Springer, 2012.\n[19] A. J. Movshon and E. P. Simoncelli. Representation of naturalistic image structure in the primate visual\ncortex. Cold Spring Harbor Symposia on Quantitative Biology: Cognition, 2015.\n[20] G. Okazawa, S. Tajima, and H. Komatsu. Image statistics underlying natural texture selectivity of neurons\nin macaque V4. PNAS, 112(4):E351–E360, January 2015.\n[21] J. Portilla and E. P. Simoncelli. A Parametric Texture Model Based on Joint Statistics of Complex Wavelet\nCoefﬁcients. International Journal of Computer Vision, 40(1):49–70, October 2000.\n[22] R. Rosenholtz, J. Huang, A. Raj, B. J. Balas, and L. Ilie. A summary statistic representation in peripheral\nvision explains visual search. Journal of vision, 12(4):14, 2012.\n[23] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla,\nM. Bernstein, A. C. Berg, and L. Fei-Fei.\nImageNet Large Scale Visual Recognition Challenge.\narXiv:1409.0575 [cs], September 2014. arXiv: 1409.0575.\n[24] E. P. Simoncelli and W. T. Freeman.\nThe steerable pyramid: A ﬂexible architecture for multi-scale\nderivative computation. In Image Processing, International Conference on, volume 3, pages 3444–3444.\nIEEE Computer Society, 1995.\n[25] K. Simonyan and A. Zisserman. Very Deep Convolutional Networks for Large-Scale Image Recognition.\narXiv:1409.1556 [cs], September 2014. arXiv: 1409.1556.\n[26] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabi-\nnovich. Going Deeper with Convolutions. arXiv:1409.4842 [cs], September 2014. arXiv: 1409.4842.\n9\n[27] L. Wei, S. Lefebvre, V. Kwatra, and G. Turk. State of the art in example-based texture synthesis. In\nEurographics 2009, State of the Art Report, EG-STAR, pages 93–117. Eurographics Association, 2009.\n[28] L. Wei and M. Levoy. Fast texture synthesis using tree-structured vector quantization. In Proceedings\nof the 27th annual conference on Computer graphics and interactive techniques, pages 479–488. ACM\nPress/Addison-Wesley Publishing Co., 2000.\n[29] D. L. K. Yamins, H. Hong, C. F. Cadieu, E. A. Solomon, D. Seibert, and J. J. DiCarlo. Performance-\noptimized hierarchical models predict neural responses in higher visual cortex. PNAS, page 201403112,\nMay 2014.\n[30] C. Zhu, R. H. Byrd, P. Lu, and J. Nocedal. Algorithm 778: L-BFGS-B: Fortran subroutines for large-scale\nbound-constrained optimization. ACM Transactions on Mathematical Software (TOMS), 23(4):550–560,\n1997.\n10\n",
        "sentence": " Significant visualization techniques have been developed to understand the deep image representations on trained networks [5, 6, 7, 8, 9, 10, 11]. [8, 12] define a squared loss on the correlations between feature maps of some layers and synthesize natural textures of high perceptual quality using the pretrained CNN called VGG [3]. demonstrated that the VGG architecture with random weights failed in generating textures and resulted in white noise images in an experiment indicating the trained filters might be crucial for texture generation [8], we conjecture the success of deep visualization mainly originates from the intrinsic nonlinearity and complexity of the deep network hierarchical structure rather than from the training, and that the architecture itself may cause the inversion invariant to the original image. [8] on well-trained VGG. re-train the VGG-19 network using average pooling instead of maximum pooling, which they suggest could improve the gradient flow and obtain slightly better results [8]. Our methods are similar in spirit to existing methods [7, 8, 13]. [8] and use the correlations between feature responses on each layer as the texture representation. The derivative of E(x, x0, l) with respect to the activations F l in layer l is [8]: ∂E(x, x0, l) ∂F l i,k = 1 N2 l M 2 l {(F (x)) [G(F (x))−G(F (x0))]}i,k (7) [8] on the trained VGG using four convolutional layers up to conv4 1 are as shown at the bottom row. [8] on the pretrained VGG (bottom row)."
    },
    {
        "title": "Understanding neural networks through deep visualization",
        "author": [
            "Jason Yosinski",
            "Jeff Clune",
            "Anh Nguyen",
            "Thomas Fuchs",
            "Hod Lipson"
        ],
        "venue": "In Deep Learning Workshop, International Conference on Machine Learning (ICML),",
        "citeRegEx": "9",
        "shortCiteRegEx": "9",
        "year": 2015,
        "abstract": "Recent years have produced great advances in training large, deep neural\nnetworks (DNNs), including notable successes in training convolutional neural\nnetworks (convnets) to recognize natural images. However, our understanding of\nhow these models work, especially what computations they perform at\nintermediate layers, has lagged behind. Progress in the field will be further\naccelerated by the development of better tools for visualizing and interpreting\nneural nets. We introduce two such tools here. The first is a tool that\nvisualizes the activations produced on each layer of a trained convnet as it\nprocesses an image or video (e.g. a live webcam stream). We have found that\nlooking at live activations that change in response to user input helps build\nvaluable intuitions about how convnets work. The second tool enables\nvisualizing features at each layer of a DNN via regularized optimization in\nimage space. Because previous versions of this idea produced less recognizable\nimages, here we introduce several new regularization methods that combine to\nproduce qualitatively clearer, more interpretable visualizations. Both tools\nare open source and work on a pre-trained convnet with minimal setup.",
        "full_text": "Understanding Neural Networks Through Deep Visualization\nJason Yosinski\nYOSINSKI@CS.CORNELL.EDU\nCornell University\nJeff Clune\nJEFFCLUNE@UWYO.EDU\nAnh Nguyen\nANGUYEN8@UWYO.EDU\nUniversity of Wyoming\nThomas Fuchs\nFUCHS@CALTECH.EDU\nJet Propulsion Laboratory, California Institute of Technology\nHod Lipson\nHOD.LIPSON@CORNELL.EDU\nCornell University\nAbstract\nRecent years have produced great advances in\ntraining large, deep neural networks (DNNs), in-\ncluding notable successes in training convolu-\ntional neural networks (convnets) to recognize\nnatural images. However, our understanding of\nhow these models work, especially what compu-\ntations they perform at intermediate layers, has\nlagged behind.\nProgress in the ﬁeld will be\nfurther accelerated by the development of bet-\nter tools for visualizing and interpreting neural\nnets.\nWe introduce two such tools here.\nThe\nﬁrst is a tool that visualizes the activations pro-\nduced on each layer of a trained convnet as it\nprocesses an image or video (e.g. a live web-\ncam stream). We have found that looking at live\nactivations that change in response to user input\nhelps build valuable intuitions about how con-\nvnets work. The second tool enables visualizing\nfeatures at each layer of a DNN via regularized\noptimization in image space. Because previous\nversions of this idea produced less recognizable\nimages, here we introduce several new regular-\nization methods that combine to produce qualita-\ntively clearer, more interpretable visualizations.\nBoth tools are open source and work on a pre-\ntrained convnet with minimal setup.\nPublished in the Deep Learning Workshop, 31 st International\nConference on Machine Learning, Lille, France, 2015. Copyright\n2015 by the author(s).\n1. Introduction\nThe last several years have produced tremendous progress\nin training powerful, deep neural network models that are\napproaching and even surpassing human abilities on a vari-\nety of challenging machine learning tasks (Taigman et al.,\n2014; Schroff et al., 2015; Hannun et al., 2014). A ﬂagship\nexample is training deep, convolutional neural networks\n(CNNs) with supervised learning to classify natural images\n(Krizhevsky et al., 2012). That area has beneﬁtted from the\ncombined effects of faster computing (e.g. GPUs), better\ntraining techniques (e.g. dropout (Hinton et al., 2012)), bet-\nter activation units (e.g. rectiﬁed linear units (Glorot et al.,\n2011)), and larger labeled datasets (Deng et al., 2009; Lin\net al., 2014).\nWhile there has thus been considerable improvements in\nour knowledge of how to create high-performing architec-\ntures and learning algorithms, our understanding of how\nthese large neural models operate has lagged behind. Neu-\nral networks have long been known as “black boxes” be-\ncause it is difﬁcult to understand exactly how any particu-\nlar, trained neural network functions due to the large num-\nber of interacting, non-linear parts. Large modern neural\nnetworks are even harder to study because of their size;\nfor example, understanding the widely-used AlexNet DNN\ninvolves making sense of the values taken by the 60 mil-\nlion trained network parameters. Understanding what is\nlearned is interesting in its own right, but it is also one\nkey way of further improving models: the intuitions pro-\nvided by understanding the current generation of models\nshould suggest ways to make them better. For example,\nthe deconvolutional technique for visualizing the features\nlearned by the hidden units of DNNs suggested an archi-\ntectural change of smaller convolutional ﬁlters that led to\narXiv:1506.06579v1  [cs.CV]  22 Jun 2015\nstate of the art performance on the ImageNet benchmark in\n2013 (Zeiler & Fergus, 2013).\nWe also note that tools that enable understanding will es-\npecially beneﬁt the vast numbers of newcomers to deep\nlearning, who would like to take advantage of off-the-shelf\nsoftware packages — like Theano (Bergstra et al., 2010),\nPylearn2 (Goodfellow et al., 2013), Caffe (Jia et al., 2014),\nand Torch (Collobert et al., 2011) — in new domains, but\nwho may not have any intuition for why their models work\n(or do not). Experts can also beneﬁt as they iterate ideas for\nnew models or when they are searching for good hyperpa-\nrameters. We thus believe that both experts and newcomers\nwill beneﬁt from tools that provide intuitions about the in-\nner workings of DNNs. This paper provides two such tools,\nboth of which are open source so that scientists and prac-\ntitioners can integrate them with their own DNNs to better\nunderstand them.\nThe ﬁrst tool is software that interactively plots the activa-\ntions produced on each layer of a trained DNN for user-\nprovided images or video. Static images afford a slow, de-\ntailed investigation of a particular input, whereas video in-\nput highlights the DNNs changing responses to dynamic in-\nput. At present, the videos are processed live from a user’s\ncomputer camera, which is especially helpful because users\ncan move different items around the ﬁeld of view, occlude\nand combine them, and perform other manipulations to ac-\ntively learn how different features in the network respond.\nThe second tool we introduce enables better visualization\nof the learned features computed by individual neurons at\nevery layer of a DNN. Seeing what features have been\nlearned is important both to understand how current DNNs\nwork and to fuel intuitions for how to improve them.\nAttempting to understand what computations are per-\nformed at each layer in DNNs is an increasingly popular di-\nrection of research. One approach is to study each layer as\na group and investigate the type of computation performed\nby the set of neurons on a layer as a whole (Yosinski et al.,\n2014; Mahendran & Vedaldi, 2014). This approach is in-\nformative because the neurons in a layer interact with each\nother to pass information to higher layers, and thus each\nneuron’s contribution to the entire function performed by\nthe DNN depends on that neuron’s context in the layer.\nAnother approach is to try to interpret the function com-\nputed by each individual neuron. Past studies in this vein\nroughly divide into two different camps: dataset-centric\nand network-centric. The former requires both a trained\nDNN and running data through that network; the latter re-\nquires only the trained network itself. One dataset-centric\napproach is to display images from the training or test\nset that cause high or low activations for individual units.\nAnother is the deconvolution method of Zeiler & Fer-\ngus (2013), which highlights the portions of a particular\nimage that are responsible for the ﬁring of each neural unit.\nNetwork-centric approaches investigate a network directly\nwithout any data from a dataset.\nFor example, Erhan\net al. (2009) synthesized images that cause high activa-\ntions for particular units. Starting with some initial input\nx = x0, the activation ai(x) caused at some unit i by\nthis input is computed, and then steps are taken in input\nspace along the gradient ∂ai(x)/∂x to synthesize inputs\nthat cause higher and higher activations of unit i, eventu-\nally terminating at some x∗which is deemed to be a pre-\nferred input stimulus for the unit in question. In the case\nwhere the input space is an image, x∗can be displayed\ndirectly for interpretation. Others have followed suit, us-\ning the gradient to ﬁnd images that cause higher activations\n(Simonyan et al., 2013; Nguyen et al., 2014) or lower acti-\nvations (Szegedy et al., 2013) for output units.\nThese gradient-based approaches are attractive in their sim-\nplicity, but the optimization process tends to produce im-\nages that do not greatly resemble natural images.\nIn-\nstead, they are composed of a collection of “hacks” that\nhappen to cause high (or low) activations: extreme pixel\nvalues, structured high frequency patterns, and copies of\ncommon motifs without global structure (Simonyan et al.,\n2013; Nguyen et al., 2014; Szegedy et al., 2013; Good-\nfellow et al., 2014). The fact that activations may be ef-\nfected by such hacks is better understood thanks to sev-\neral recent studies.\nSpeciﬁcally, it has been shown that\nsuch hacks may be applied to correctly classiﬁed images\nto cause them to be misclassiﬁed even via imperceptibly\nsmall changes (Szegedy et al., 2013), that such hacks can\nbe found even without the gradient information to produce\nunrecognizable “fooling examples” (Nguyen et al., 2014),\nand that the abundance of non-natural looking images that\ncause extreme activations can be explained by the locally\nlinear behavior of neural nets (Goodfellow et al., 2014).\nWith such strong evidence that optimizing images to cause\nhigh activations produces unrecognizable images, is there\nany hope of using such methods to obtain useful visualiza-\ntions? It turns out there is, if one is able to appropriately\nregularize the optimization. Simonyan et al. (2013) showed\nthat slightly discernible images for the ﬁnal layers of a con-\nvnet could be produced with L2-regularization. Mahendran\nand Vedaldi (2014) also showed the importance of incor-\nporating natural-image priors in the optimization process\nwhen producing images that mimic an entire-layer’s ﬁring\npattern produced by a speciﬁc input image. We build on\nthese works and contribute three additional forms of reg-\nularization that, when combined, produce more recogniz-\nable, optimization-based samples than previous methods.\nBecause the optimization is stochastic, by starting at dif-\nferent random initial images, we can produce a set of opti-\n2\nFigure 1. The bottom shows a screenshot from the interactive visualization software. The webcam input is shown, along with the whole\nlayer of conv5 activations. The selected channel pane shows an enlarged version of the 13x13 conv5151 channel activations. Below it,\nthe deconv starting at the selected channel is shown. On the right, three selections of nine images are shown: synthetic images produced\nusing the regularized gradient ascent methods described in Section 3, the top 9 image patches from the training set (the images from the\ntraining set that caused the highest activations for the selected channel), and the deconv of the those top 9 images. All areas highlighted\nwith a green star relate to the particular selected channel, here conv5151; when the selection changes, these panels update. The top\ndepicts enlarged numerical optimization results for this and other channels. conv52 is a channel that responds most strongly to dog faces\n(as evidenced by the top nine images, which are not shown due to space constraints), but it also responds to ﬂowers on the blanket on the\nbottom and half way up the right side of the image (as seen in the inset red highlight). This response to ﬂowers can be partially seen in\nthe optimized images but would be missed in an analysis focusing only on the top nine images and their deconv versions, which contain\nno ﬂowers. conv5151 detects different types of faces. The top nine images are all of human faces, but here we see it responds also to the\ncat’s face (and in Figure 2 a lion’s face). Finally, conv5111 activates strongly for the cat’s face, the optimized images show catlike fur\nand ears, and the top nine images (not shown here) are also all of cats. For this image, the softmax output layer top two predictions are\n“Egyptian Cat” and “Computer Keyboard.” All ﬁgures in this paper are best viewed digitally, in color, signiﬁcantly zoomed in.\n3\nmized images whose variance provides information about\nthe invariances learned by the unit.\nTo summarize, this paper makes the following two contri-\nbutions:\n1. We describe and release a software tool that provides\na live, interactive visualization of every neuron in a\ntrained convnet as it responds to a user-provided im-\nage or video. The tool displays forward activation val-\nues, preferred stimuli via gradient ascent, top images\nfor each unit from the training set, deconv highlighting\n(Zeiler & Fergus, 2013) of top images, and backward\ndiffs computed via backprop or deconv starting from\narbitrary units. The combined effect of these comple-\nmentary visualizations promotes a greater understand-\ning of what a neuron computes than any single method\non its own. We also describe a few insights we have\ngained from using this tool. (Section 2).\n2. We extend past efforts to visualize preferred activation\npatterns in input space by adding several new types\nof regularization, which produce what we believe are\nthe most interpretable images for large convnets so far\n(Section 3).\nBoth of our tools are released as open source and are avail-\nable at http://yosinski.com/deepvis. While the\ntools could be adapted to integrate with any DNN soft-\nware framework, they work out of the box with the popular\nCaffe DNN software package (Jia et al., 2014). Users may\nrun visualizations with their own Caffe DNN or our pre-\ntrained DNN, which comes with pre-computed images op-\ntimized to activate each neuron in this trained network. Our\npre-trained network is nearly identical to the “AlexNet”\narchitecture (Krizhevsky et al., 2012), but with local re-\nponse normalization layers after pooling layers following\n(Jia et al., 2014). It was trained with the Caffe framework\non the ImageNet 2012 dataset (Deng et al., 2009).\n2. Visualizing Live Convnet Activations\nOur ﬁrst visualization method is straightforward: plotting\nthe activation values for the neurons in each layer of a con-\nvnet in response to an image or video. In fully connected\nneural networks, the order of the units is irrelevant, so plots\nof these vectors are not spatially informative. However, in\nconvolutional networks, ﬁlters are applied in a way that re-\nspects the underlying geometry of the input; in the case\nof 2D images, ﬁlters are applied in a 2D convolution over\nthe two spatial dimensions of the image. This convolution\nproduces activations on subsequent layers that are, for each\nchannel, also arranged spatially.\nFigure 1 shows examples of this type of plot for the conv5\nlayer. The conv5 layer has size 256×13×13, which we de-\npict as 256 separate 13×13 grayscale images. Each of the\n256 small images contains activations in the same spatial\nx-y spatial layout as the input data, and the 256 images are\nsimply and arbitrarily tiled into a 16×16 grid in row-major\norder. Figure 2 shows a zoomed in view of one particu-\nlar channel, conv5151, that responds to human and animal\nfaces. All layers can be viewed in the software tool, includ-\ning pooling and normalization layers.\nVisualizing these\nlayers provides intuitions about their effects and functions.\nAlthough this visualization is simple to implement, we ﬁnd\nit informative because all data ﬂowing through the network\ncan be visualized. There is nothing mysterious happening\nbehind the scenes. Because this convnet contains only a\nsingle path from input to output, every layer is a bottleneck\nthrough which all information must pass en-route to a clas-\nsiﬁcation decision. The layer sizes are all small enough that\nany one layer can easily ﬁt on a computer screen.1 So far,\nwe have gleaned several surprising intuitions from using\nthe tool:\n• One of the most interesting conclusions so far has\nbeen that representations on some layers seem to be\nsurprisingly local. Instead of ﬁnding distributed repre-\nsentations on all layers, we see, for example, detectors\nfor text, ﬂowers, fruit, and faces on conv4 and conv5.\nThese conclusions can be drawn either from the live\nvisualization or the optimized images (or, best, by us-\ning both in concert) and suggest several directions for\nfuture research (discussed in Section 4).\n• When using direct ﬁle input to classify photos from\nFlickr or Google Images, classiﬁcations are often cor-\nrect and highly conﬁdent (softmax probability for cor-\nrect class near 1). On the other hand, when using in-\nput from a webcam, predictions often cannot be cor-\nrect because no items from the training set are shown\nin the image. The training set’s 1000 classes, though\nnumerous, do not cover most common household ob-\njects. Thus, when shown a typical webcam view of a\nperson with no ImageNet classes present, the output\nhas no single high probability, as is expected. Sur-\nprisingly, however, this probability vector is noisy and\nvaries signiﬁcantly in response to tiny changes in the\ninput, often changing merely in response to the noise\nfrom the webcam. We might have instead expected\nunchanging and low conﬁdence predictions for a given\nscene when no object the network has been trained to\nclassify is present. Plotting the fully connected layers\n(fc6 and fc7) also reveals a similar sensitivity to small\ninput changes.\n1The layer with the most activations is conv1 which, when\ntiled, is only 550x550 before adding padding.\n4\nFigure 2. A view of the 13×13 activations of the 151st channel on\nthe conv5 layer of a deep neural network trained on ImageNet, a\ndataset that does not contain a face class, but does contain many\nimages with faces. The channel responds to human and animal\nfaces and is robust to changes in scale, pose, lighting, and context,\nwhich can be discerned by a user by actively changing the scene\nin front of a webcam or by loading static images (e.g. of the lions)\nand seeing the corresponding response of the unit. Photo of lions\nvia Flickr user arnolouise, licensed under CC BY-NC-SA 2.0.\n• Although the last three layers are sensitive to small\ninput changes, much of the lower layer computation\nis more robust. For example, when visualizing the\nconv5 layer, one can ﬁnd many invariant detectors\nfor faces, shoulders, text, etc.\nby moving oneself\nor objects in front of the camera. Even though the\n1000 classes contain no explicitly labeled faces or\ntext, the network learns to identify these concepts sim-\nply because they represent useful partial information\nfor making a later classiﬁcation decision. One face\ndetector, denoted conv5151 (channel number 151 on\nconv5), is shown in Figure 2 activating for human\nand lion faces and in Figure 1 activating for a cat\nface. Zhou et al. (2014) recently observed a similar\neffect where convnets trained only to recognize dif-\nferent scene types — playgrounds, restaurant patios,\nliving rooms, etc. — learn object detectors (e.g. for\nchairs, books, and sofas) on intermediate layers.\nThe reader is encouraged to try this visualization tool out\nfor him or herself.\nThe code, together with pre-trained\nmodels and images synthesized by gradient ascent, can be\ndownloaded at http://yosinski.com/deepvis.\n3. Visualizing via Regularized Optimization\nThe second contribution of this work is introducing several\nregularization methods to bias images found via optimiza-\ntion toward more visually interpretable examples. While\neach of these regularization methods helps on its own, in\ncombination they are even more effective. We found use-\nful combinations via a random hyperparameter search, as\ndiscussed below.\nFormally, consider an image x ∈RC×H×W , where C = 3\ncolor channels and the height (H) and width (W) are both\n227 pixels. When this image is presented to a neural net-\nwork, it causes an activation ai(x) for some unit i, where\nfor simplicity i is an index that runs over all units on all lay-\ners. We also deﬁne a parameterized regularization function\nRθ(x) that penalizes images in various ways.\nOur network was trained on ImageNet by ﬁrst subtract-\ning the per-pixel mean of examples in ImageNet before in-\nputting training examples to the network. Thus, the direct\ninput to the network, x, can be thought of as a zero-centered\ninput. We may pose the optimization problem as ﬁnding an\nimage x∗where\nx∗= arg max\nx\n(ai(x) −Rθ(x))\n(1)\nIn practice, we use a slightly different formulation. Be-\ncause we search for x∗by starting at some x0 and taking\ngradient steps, we instead deﬁne the regularization via an\noperator rθ(·) that maps x to a slightly more regularized\nversion of itself. This latter deﬁnition is strictly more ex-\npressive, allowing regularization operators rθ that are not\n5\nthe gradient of any Rθ. This method is easy to implement\nwithin a gradient descent framework by simply alternating\nbetween taking a step toward the gradient of ai(x) and tak-\ning a step in the direction given by rθ. With a gradient\ndescent step size of η, a single step in this process applies\nthe update:\nx ←rθ\n\u0012\nx + η ∂ai\n∂x\n\u0013\n(2)\nWe investigated the following four regularizations. All are\ndesigned to overcome different pathologies commonly en-\ncountered by gradient descent without regularization.\nL2 decay: A common regularization, L2 decay penalizes\nlarge values and is implemented as rθ(x) = (1−θdecay)·x.\nL2 decay tends to prevent a small number of extreme pixel\nvalues from dominating the example image. Such extreme\nsingle-pixel values neither occur naturally with great fre-\nquency nor are useful for visualization. L2 decay was also\nused by Simonyan et al. (2013).\nGaussian blur:\nProducing images via gradient ascent\ntends to produce examples with high frequency informa-\ntion (see Supplementary Section S1 for a possible reason).\nWhile these images cause high activations, they are neither\nrealistic nor interpretable (Nguyen et al., 2014). A useful\nregularization is thus to penalize high frequency informa-\ntion. We implement this as a Gaussian blur step rθ(x) =\nGaussianBlur(x, θb width). Convolving with a blur ker-\nnel is more computationally expensive than the other reg-\nularization methods, so we added another hyperparameter\nθb every to allow, for example, blurring every several op-\ntimization steps instead of every step. Blurring an image\nmultiple times with a small width Gaussian kernel is equiv-\nalent to blurring once with a larger width kernel, and the\neffect will be similar even if the image changes slightly\nduring the optimization process. This technique thus low-\ners computational costs without limiting the expressiveness\nof the regularization. Mahendran & Vedaldi (2014) used a\npenalty with a similar effect to blurring, called total varia-\ntion, in their work reconstructing images from layer codes.\nClipping pixels with small norm: The ﬁrst two regulariza-\ntions suppress high amplitude and high frequency informa-\ntion, so after applying both, we are left with an x∗that con-\ntains somewhat small, somewhat smooth values. However,\nx∗will still tend to contain non-zero pixel values every-\nwhere. Even if some pixels in x∗show the primary object\nor type of input causing the unit under consideration to ac-\ntivate, the gradient with respect to all other pixels in x∗will\nstill generally be non-zero, so these pixels will also shift to\nshow some pattern as well, contributing in whatever small\nway they can to ultimately raise the chosen unit’s activa-\ntion. We wish to bias the search away from such behavior\nand instead show only the main object, letting other regions\nbe exactly zero if they are not needed. We implement this\nbias using an rθ(x) that computes the norm of each pixel\n(over red, green, and blue channels) and then sets any pix-\nels with small norm to zero. The threshold for the norm,\nθn pct, is speciﬁed as a percentile of all pixel norms in x.\nClipping pixels with small contribution: Instead of clip-\nping pixels with small norms, we can try something slightly\nsmarter and clip pixels with small contributions to the acti-\nvation. One way of computing a pixel’s contribution to an\nactivation is to measure how much the activation increases\nor decreases when the pixel is set to zero; that is, to com-\npute the contribution as |ai(x) −ai(x−j)|, where x−j is x\nbut with the jth pixel set to zero. This approach is straight-\nforward but prohibitively slow, requiring a forward pass\nfor every pixel. Instead, we approximate this process by\nlinearizing ai(x) around x, in which case the contribution\nof each dimension of x can be estimated as the elemen-\ntwise product of x and the gradient. We then sum over\nall three channels and take the absolute value, computing\n|P\nc x ◦∇xai(x)|. We use the absolute value to ﬁnd pix-\nels with small contribution in either direction, positive or\nnegative. While we could choose to keep the pixel transi-\ntions where setting the pixel to zero would result in a large\nactivation increase, these shifts are already handled by gra-\ndient ascent, and here we prefer to clip only the pixels that\nare deemed not to matter, not to take large gradient steps\noutside the region where the linear approximation is most\nvalid. We deﬁne this rθ(x) as the operation that sets pixels\nwith contribution under the θc pct percentile to zero.\nIf the above regularization methods are applied individu-\nally, they are somewhat effective at producing more inter-\npretable images; Figure 3 shows the effects of each indi-\nvidual hyperparameter. However, preliminary experiments\nuncovered that their combined effect produces better vi-\nsualizations. To pick a reasonable set of hyperparameters\nfor all methods at once, we ran a random hyperparameter\nsearch of 300 possible combinations and settled on four that\ncomplement each other well. The four selected combina-\ntions are listed in Table 1 and optimized images using each\nare shown for the “Gorilla” class output unit in Figure 4.\nOf the four, some show high frequency information, oth-\ners low frequency; some contain dense pixel data, and oth-\ners contain only sparse outlines of important regions. We\nfound the version in the lower-left quadrant to be the best\nsingle set of hyperparameters, but often greater intuition\ncan be gleaned by considering all four at once. Figure 5\nshows the optimization results computed for a selection of\nunits on all layers. A single image for every ﬁlter of all\nﬁve convolutional layers is shown in Supplementary Fig-\nure S1. Nine images for each ﬁlter of all layers, including\neach of the 1000 ImageNet output classes, can be viewed\nat http://yosinski.com/deepvis.\n6\nFigure 4. Visualizations of the preferred inputs for different class units on layer fc8, the 1000-dimensional output of the network just\nbefore the ﬁnal softmax. In the lower left are 9 visualizations each (in 3×3 grids) for four different sets of regularization hyperparameters\nfor the Gorilla class (Table 1). For all other classes, we have selected four interpretable visualizations produced by our regularized\noptimization method. We chose the four combinations of regularization hyperparameters by performing a random hyperparameter search\nand selecting combinations that complement each other. For example, the lower left quadrant tends to show lower frequency patterns,\nthe upper right shows high frequency patterns, and the upper left shows a sparse set of important regions. Often greater intuition can\nbe gleaned by considering all four at once. In nearly every case, we have found that one can guess what class a neuron represents by\nviewing sets of these optimized, preferred images. Best viewed electronically, zoomed in.\n7\nFigure 5. Visualization of example features of eight layers of a deep, convolutional neural network. The images reﬂect the true sizes\nof the features at different layers. In each layer, we show visualizations from 4 random gradient descent runs for each channel. While\nthese images are hand picked to showcase the diversity and interpretability of the visualizations, one image for each ﬁlter of all ﬁve\nconvolutional layers is shown in Figure S1 in supplementary information. One can recognize important features of objects at different\nscales, such as edges, corners, wheels, eyes, shoulders, faces, handles, bottles, etc. The visualizations show the increase in complexity\nand variation on higher layers, comprised of simpler components from lower layers. The variation of patterns increases with increasing\nlayer number, indicating that increasingly invariant representations are learned. In particular, the jump from Layer 5 (the last convolution\nlayer) to Layer 6 (the ﬁrst fully-connected layer) brings about a large increase in variation. Best viewed electronically, zoomed in.\n8\n✓decay = 0.5\n✓n pct = 95\n✓c pct = 95\n✓b width = 1.0\n✓b every = 4\n✓decay = 0.0\n✓n pct = 0\n✓c pct = 0\n✓b every = 10\n✓b width = 0.0\nFigure 3. The effects of each regularization method from Sec-\ntion 3 when used individually. Each of the four rows shows a\nlinear sweep in hyperparameter space from no regularization (left)\nto strong regularization (right). When applied too strongly, some\nregularizations cause the optimization to fail (e.g. L2 decay, top\nrow) or the images to be less interpretable (small norm and small\ncontribution clipping, bottom two rows). For this reason, a ran-\ndom hyperparameter search was useful for ﬁnding joint hyperpa-\nrameter settings that worked well together (see Figure 4). Best\nviewed electronically, zoomed in.\n4. Discussion and Conclusion\nWe have introduced two visual tools for aiding in the inter-\npretation of trained neural nets. Intuition gained from these\ntools may prompt ideas for improved methods and future\nresearch. Here we discuss several such ideas.\nThe interactive tool reveals that representations on later\nconvolutional layers tend to be somewhat local, where\nchannels correspond to speciﬁc, natural parts (e.g. wheels,\nfaces) instead of being dimensions in a completely dis-\ntributed code.\nThat said, not all features correspond to\nnatural parts, raising the possibility of a different decom-\nposition of the world than humans might expect. These\nvisualizations suggest that further study into the exact na-\nture of learned representations — whether they are local to\na single channel or distributed across several — is likely to\nbe interesting (see Zhou et al. (2014) for work in this direc-\ntion). The locality of the representation also suggests that\nduring transfer learning, when new models are trained atop\nthe conv4 or conv5 representations, a bias toward sparse\nconnectivity could be helpful because it may be necessary\nto combine only a few features from these layers to create\nimportant features at higher layers.\nThe second tool — new regularizations that enable im-\nproved, interpretable, optimized visualizations of learned\nfeatures — will help researchers and practitioners under-\nstand, debug, and improve their models. The visualizations\nalso reveal a new twist in an ongoing story. Previous stud-\nTable 1. Four hyperparameter combinations that produce different\nstyles of recognizable images. We identiﬁed these four after re-\nviewing images produced by 300 randomly selected hyperparam-\neter combinations. From top to bottom, they are the hyperparam-\neter combinations that produced the top-left, top-right, bottom-\nleft, and bottom-right Gorilla class visualizations, respectively, in\nFigure 4. The third row hyperparameters produced most of the\nvisualizations for the other classes in Figure 4, and all of those in\nFigure 5.\nθdecay\nθb width\nθb every\nθn pct\nθc pct\n0\n0.5\n4\n50\n0\n0.3\n0\n0\n20\n0\n0.0001\n1.0\n4\n0\n0\n0\n0.5\n4\n0\n90\nies have shown that discriminative networks can easily be\nfooled or hacked by the addition of certain structured noise\nin image space (Szegedy et al., 2013; Nguyen et al., 2014).\nAn oft-cited reason for this property is that discriminative\ntraining leads networks to ignore non-discriminative infor-\nmation in their input, e.g. learning to detect jaguars by\nmatching the unique spots on their fur while ignoring the\nfact that they have four legs. For this reason it has been\nseen as a hopeless endeavor to create a generative model\nin which one randomly samples an x from a broad distri-\nbution on the space of all possible images and then itera-\ntively transforms x into a recognizable image by moving\nit to a region that satisﬁes both a prior p(x) and posterior\np(y|x) for some class label y. Past attempts have largely\nsupported this view by producing unrealistic images using\nthis method (Nguyen et al., 2014; Simonyan et al., 2013).\nHowever, the results presented here suggest an alternate\npossibility: the previously used priors may simply have\nbeen too weak (see Section S1 for one hypothesis of why\na strong p(x) model is needed). With the careful design or\nlearning of a p(x) model that biases toward realism, one\nmay be able to harness the large number of parameters\npresent in a discriminately learned p(y|x) model to gen-\nerate realistic images by enforcing probability under both\nmodels simultaneously. Even with the simple, hand-coded\np(x) models we use in this paper as regularizers, com-\nplex dependencies between distant pixels already arise (cf.\nthe beetles with structure spanning over 100 pixels in Fig-\nure 4). This implies that the discriminative parameters also\ncontain signiﬁcant “generative” structure from the training\ndataset; that is, the parameters encode not only the jaguar’s\nspots, but to some extent also its four legs. With better,\nlearned probabilistic models over the input and activations\nof higher layers, much more structure may be apparent.\nWork by Dai et al. (2015) shows some interesting results\nin this direction. While the images generated in this pa-\nper are far from being photo-realistic, they do suggest that\n9\ntransferring discriminatively trained parameters to gener-\native models — opposite the direction of the usual unsu-\npervised pretraining approach — may be a fruitful area for\nfurther investigation.\nAcknowledgments\nThe authors would like to thank the NASA Space Technol-\nogy Research Fellowship (JY) for funding, Wendy Shang,\nYoshua Bengio, Brian Cheung, and Andrej Karpathy for\nhelpful discussions, and Freckles the cat for her feline\ncountenance.\nReferences\nBergstra, James, Breuleux, Olivier, Bastien, Fr´ed´eric, Lam-\nblin, Pascal, Pascanu, Razvan, Desjardins, Guillaume, Turian,\nJoseph, Warde-Farley, David, and Bengio, Yoshua. Theano: a\nCPU and GPU math expression compiler. In Proceedings of\nthe Python for Scientiﬁc Computing Conference (SciPy), June\n2010. Oral Presentation.\nCollobert, Ronan, Kavukcuoglu, Koray, and Farabet, Cl´ement.\nTorch7:\nA matlab-like environment for machine learning.\nIn BigLearn, NIPS Workshop, number EPFL-CONF-192376,\n2011.\nDai, Jifeng, Lu, Yang, and Wu, Ying Nian. Generative modeling\nof convolutional neural networks. In International Conference\non Learning Representations (ICLR), 2015.\nDeng, Jia, Dong, Wei, Socher, Richard, Li, Li-Jia, Li, Kai,\nand Fei-Fei, Li. Imagenet: A large-scale hierarchical image\ndatabase. In Computer Vision and Pattern Recognition, 2009.\nCVPR 2009. IEEE Conference on, pp. 248–255. IEEE, 2009.\nErhan, Dumitru, Bengio, Yoshua, Courville, Aaron, and Vin-\ncent, Pascal. Visualizing higher-layer features of a deep net-\nwork. Technical report, Technical report, University of Mon-\ntreal, 2009.\nGlorot, Xavier, Bordes, Antoine, and Bengio, Yoshua.\nDeep\nsparse rectiﬁer networks.\nIn Proceedings of the 14th Inter-\nnational Conference on Artiﬁcial Intelligence and Statistics.\nJMLR W&CP Volume, volume 15, pp. 315–323, 2011.\nGoodfellow, Ian J, Warde-Farley, David, Lamblin, Pascal, Du-\nmoulin, Vincent, Mirza, Mehdi, Pascanu, Razvan, Bergstra,\nJames, Bastien, Fr´ed´eric, and Bengio, Yoshua.\nPylearn2:\na\nmachine\nlearning\nresearch\nlibrary.\narXiv\npreprint\narXiv:1308.4214, 2013.\nGoodfellow, Ian J, Shlens, Jonathon, and Szegedy, Christian. Ex-\nplaining and Harnessing Adversarial Examples. ArXiv e-prints,\nDecember 2014.\nHannun, A., Case, C., Casper, J., Catanzaro, B., Diamos, G.,\nElsen, E., Prenger, R., Satheesh, S., Sengupta, S., Coates, A.,\nand Ng, A. Y. Deep Speech: Scaling up end-to-end speech\nrecognition. ArXiv e-prints, December 2014.\nHinton,\nGeoffrey E, Srivastava,\nNitish,\nKrizhevsky,\nAlex,\nSutskever, Ilya, and Salakhutdinov, Ruslan R. Improving neu-\nral networks by preventing co-adaptation of feature detectors.\narXiv preprint arXiv:1207.0580, 2012.\nJia, Yangqing, Shelhamer, Evan, Donahue, Jeff, Karayev, Sergey,\nLong, Jonathan, Girshick, Ross, Guadarrama, Sergio, and Dar-\nrell, Trevor. Caffe: Convolutional architecture for fast feature\nembedding. arXiv preprint arXiv:1408.5093, 2014.\nKrizhevsky, Alex, Sutskever, Ilya, and Hinton, Geoff. Imagenet\nclassiﬁcation with deep convolutional neural networks.\nIn\nAdvances in Neural Information Processing Systems 25, pp.\n1106–1114, 2012.\nLin, Tsung-Yi, Maire, Michael, Belongie, Serge, Hays, James,\nPerona, Pietro, Ramanan, Deva, Doll´ar, Piotr, and Zitnick,\nC. Lawrence. Microsoft COCO: common objects in context.\nCoRR, abs/1405.0312, 2014. URL http://arxiv.org/\nabs/1405.0312.\nMahendran, A. and Vedaldi, A. Understanding Deep Image Rep-\nresentations by Inverting Them.\nArXiv e-prints, November\n2014.\nNguyen, Anh, Yosinski, Jason, and Clune, Jeff. Deep Neural Net-\nworks are Easily Fooled: High Conﬁdence Predictions for Un-\nrecognizable Images. ArXiv e-prints, December 2014.\nSchroff, F., Kalenichenko, D., and Philbin, J. FaceNet: A Uni-\nﬁed Embedding for Face Recognition and Clustering. ArXiv\ne-prints, March 2015.\nSimonyan, Karen, Vedaldi, Andrea, and Zisserman, Andrew.\nDeep inside convolutional networks:\nVisualising image\nclassiﬁcation models and saliency maps.\narXiv preprint\narXiv:1312.6034, presented at ICLR Workshop 2014, 2013.\nSzegedy, Christian, Zaremba, Wojciech, Sutskever, Ilya, Bruna,\nJoan, Erhan, Dumitru, Goodfellow, Ian J., and Fergus, Rob. In-\ntriguing properties of neural networks. CoRR, abs/1312.6199,\n2013.\nTaigman, Yaniv, Yang, Ming, Ranzato, Marc’Aurelio, and Wolf,\nLior. Deepface: Closing the gap to human-level performance\nin face veriﬁcation. In Computer Vision and Pattern Recogni-\ntion (CVPR), 2014 IEEE Conference on, pp. 1701–1708. IEEE,\n2014.\nTorralba, Antonio and Oliva, Aude. Statistics of natural image\ncategories. Network: computation in neural systems, 14(3):\n391–412, 2003.\nYosinski, J., Clune, J., Bengio, Y., and Lipson, H. How trans-\nferable are features in deep neural networks? In Ghahramani,\nZ., Welling, M., Cortes, C., Lawrence, N.D., and Weinberger,\nK.Q. (eds.), Advances in Neural Information Processing Sys-\ntems 27, pp. 3320–3328. Curran Associates, Inc., December\n2014.\nZeiler, Matthew D and Fergus, Rob.\nVisualizing and un-\nderstanding convolutional neural networks.\narXiv preprint\narXiv:1311.2901, 2013.\nZhou, Bolei, Khosla, Aditya, Lapedriza, `Agata, Oliva, Aude, and\nTorralba, Antonio. Object detectors emerge in deep scene cnns.\nCoRR, abs/1412.6856, 2014. URL http://arxiv.org/\nabs/1412.6856.\n10\nSupplementary Information for:\nUnderstanding Neural Networks Through Deep Visualization\nJason Yosinski\nYOSINSKI@CS.CORNELL.EDU\nJeff Clune\nJEFFCLUNE@UWYO.EDU\nAnh Nguyen\nANGUYEN8@UWYO.EDU\nThomas Fuchs\nFUCHS@CALTECH.EDU\nHod Lipson\nHOD.LIPSON@CORNELL.EDU\nS1. Why are gradient optimized images\ndominated by high frequencies?\nIn the main text we mentioned that images produced by\ngradient ascent to maximize the activations of neurons in\nconvolutional networks tend to be dominated by high fre-\nquency information (cf. the left column of Figure 3). One\nhypothesis for why this occurs centers around the differing\nstatistics of the activations of channels in a convnet. The\nconv1 layer consists of blobs of color and oriented Gabor\nedge ﬁlters of varying frequencies. The average activation\nvalues (after the rectiﬁer) of the edge ﬁlters vary across\nﬁlters, with low frequency ﬁlters generally having much\nhigher average activation values than high frequency ﬁlters.\nIn one experiment we observed that the average activation\nvalues of the ﬁve lowest frequency edge ﬁlters was 90 ver-\nsus an average for the ﬁve highest frequency ﬁlters of 5.4, a\ndifference of a factor of 17 (manuscript in preparation)2,3.\nThe activation values for blobs of color generally fall in the\nmiddle of the range. This phenomenon likely arises for rea-\nsons related to the 1/f power spectrum of natural images in\nwhich low spatial frequencies tend to contain higher energy\nthan high spatial frequencies (Torralba & Oliva, 2003).\nNow consider the connections from the conv1 ﬁlters to a\nsingle unit on conv2. In order to merge information from\nboth low frequency and high frequency conv1 ﬁlters, the\nconnection weights from high frequency conv1 units may\ngenerally have to be larger than connections from low fre-\nquency conv1 units in order to allow both signals to affect\nthe conv2 unit’s activation similarly. If this is the case, then\ndue to the larger multipliers, the activation of this particular\nconv2 unit is affected more by small changes in the activa-\ntions of high frequency ﬁlters than low frequency ﬁlters.\n2Li, Yosinski, Clune, Song, Hopcroft, Lipson. 2015. How\nsimilar are features learned by different deep neural networks? In\npreparation.\n3Activation values are averaged over the ImageNet validation\nset, over all spatial positions, over the channels with the ﬁve\n{highest, lowest} frequencies, and over four separately trained\nnetworks.\nSeen in the other direction: when gradient information is\npassed from higher layers to lower layers during backprop,\nthe partial derivative arriving at this conv2 unit (a scalar)\nwill be passed backward and multiplied by larger values\nwhen destined for high frequency conv1 ﬁlters than low fre-\nquency ﬁlters. Thus, following the gradient in pixel space\nmay tend to produce an overabundance of high frequency\nchanges instead of low frequency changes.\nThe above discussion focuses on the differing statistics of\nedge ﬁlters in conv1, but note that activation statistics on\nsubsequent layers also vary across each layer.4 This may\nproduce a similar (though more subtle to observe) effect in\nwhich rare higher layer features are also overrepresented\ncompared to more common higher layer features.\nOf course, this hypothesis is only one tentative explanation\nfor why high frequency information dominates the gradi-\nent. It relies on the assumption that the average activation\nof a unit is a representative statistic of the whole distribu-\ntion of activations for that unit. In our observation this\nhas been the case, with most units having similar, albeit\nscaled, distributions. However, more study is needed be-\nfore a deﬁnitive conclusion can be reached.\nS2. Conv Layer Montages\nOne example optimized image using the hyperparameter\nsettings from the third row of Table 1 for every ﬁlter of all\nﬁve convolutional layers is shown in Figure S1.\n4We have observed that statistics vary on higher layers, but in\na different manner: most channels on these layers have similar av-\nerage activations, with most of the variance across channels being\ndominated by a small number of channels with unusually small\nor unusually large averages (Li, Yosinski, Clune, Song, Hopcroft,\nLipson. 2015. How similar are features learned by different deep\nneural networks? In preparation.)\nconv5\nconv3\nconv4\nconv2\nconv1\nFigure S1. One optimized, preferred image for every channel of all ﬁve convolutional layers. These images were produced with the\nhyperparameter combinations from the third row of Table 1. Best viewed electronically, zoomed in.\n12\n",
        "sentence": " Significant visualization techniques have been developed to understand the deep image representations on trained networks [5, 6, 7, 8, 9, 10, 11]. Another stream of visualization aims to understand what each neuron has learned in a pretrained network and synthesize an image that maximally activates individual features [5, 9] or the class prediction scores [6]. In addition, some regularizers are incorporated as a natural image prior to improve the visualization quality, including α−norm [6], total variation[7], jitter[7], Gaussian blur [9], data-driven patch priors [17], etc."
    },
    {
        "title": "Inverting visual representations with convolutional networks",
        "author": [
            "Alexey Dosovitskiy",
            "Thomas Brox"
        ],
        "venue": "arXiv preprint arXiv:1506.02753,",
        "citeRegEx": "10",
        "shortCiteRegEx": "10",
        "year": 2015,
        "abstract": "Feature representations, both hand-designed and learned ones, are often hard\nto analyze and interpret, even when they are extracted from visual data. We\npropose a new approach to study image representations by inverting them with an\nup-convolutional neural network. We apply the method to shallow representations\n(HOG, SIFT, LBP), as well as to deep networks. For shallow representations our\napproach provides significantly better reconstructions than existing methods,\nrevealing that there is surprisingly rich information contained in these\nfeatures. Inverting a deep network trained on ImageNet provides several\ninsights into the properties of the feature representation learned by the\nnetwork. Most strikingly, the colors and the rough contours of an image can be\nreconstructed from activations in higher network layers and even from the\npredicted class probabilities.",
        "full_text": "Inverting Visual Representations with Convolutional Networks\nAlexey Dosovitskiy\nThomas Brox\nUniversity of Freiburg\nFreiburg im Breisgau, Germany\n{dosovits,brox}@cs.uni-freiburg.de\nAbstract\nFeature\nrepresentations,\nboth\nhand-designed\nand\nlearned ones, are often hard to analyze and interpret, even\nwhen they are extracted from visual data. We propose a\nnew approach to study image representations by inverting\nthem with an up-convolutional neural network. We apply\nthe method to shallow representations (HOG, SIFT, LBP),\nas well as to deep networks. For shallow representations\nour approach provides signiﬁcantly better reconstructions\nthan existing methods, revealing that there is surprisingly\nrich information contained in these features. Inverting a\ndeep network trained on ImageNet provides several insights\ninto the properties of the feature representation learned\nby the network. Most strikingly, the colors and the rough\ncontours of an image can be reconstructed from activations\nin higher network layers and even from the predicted class\nprobabilities.\n1. Introduction\nA feature representation useful for pattern recognition\ntasks is expected to concentrate on properties of the input\nimage which are important for the task and ignore the ir-\nrelevant properties of the input image. For example, hand-\ndesigned descriptors such as HOG [3] or SIFT [17], explic-\nitly discard the absolute brightness by only considering gra-\ndients, precise spatial information by binning the gradients\nand precise values of the gradients by normalizing the his-\ntograms. Convolutional neural networks (CNNs) trained in\na supervised manner [14, 13] are expected to discard infor-\nmation irrelevant for the task they are solving [28, 19, 22].\nIn this paper we propose a new approach to analyze\nwhich information is preserved by a feature representa-\ntion and which information is discarded. We train neural\nnetworks to invert feature representations in the following\nsense.\nGiven a feature vector, the network is trained to\npredict the expected pre-image, that is, the (weighted) av-\nerage of all natural images which could have produced the\nHOG\nSIFT\nAlexNet-CONV3\nAlexNet-FC8\nFigure 1: We train convolutional networks to reconstruct\nimages from different feature representations. Top row:\nInput features. Bottom row: Reconstructed image. Re-\nconstructions from HOG and SIFT are very realistic. Re-\nconstructions from AlexNet preserve color and rough object\npositions even when reconstructing from higher layers.\ngiven feature vector. The content of this expected pre-image\nshows image properties which can be conﬁdently inferred\nfrom the feature vector. The amount of blur corresponds to\nthe level of invariance of the feature representation. We ob-\ntain further insights into the structure of the feature space, as\nwe apply the networks to perturbed feature vectors, to inter-\npolations between two feature vectors, or to random feature\nvectors.\nWe apply our inversion method to AlexNet [13], a con-\nvolutional network trained for classiﬁcation on ImageNet,\nas well as to three widely used computer vision features:\nhistogram of oriented gradients (HOG) [3, 7], scale invari-\nant feature transform (SIFT) [17], and local binary pat-\nterns (LBP) [21]. The SIFT representation comes as a non-\nuniform, sparse set of oriented keypoints with their corre-\nsponding descriptors at various scales. This is an additional\nchallenge for the inversion task. LBP features are not dif-\nferentiable with respect to the input image. Thus, existing\nmethods based on gradients of representations [19] could\nnot be applied to them.\n1\narXiv:1506.02753v4  [cs.NE]  26 Apr 2016\n1.1. Related work\nOur approach is related to a large body of work on in-\nverting neural networks. These include works making use\nof backpropagation or sampling [15, 16, 18, 27, 9, 25] and,\nmost similar to our approach, other neural networks [2].\nHowever, only recent advances in neural network architec-\ntures allow us to invert a modern large convolutional net-\nwork with another network.\nOur approach is not to be confused with the Decon-\nvNet [28], which propagates high level activations back-\nward through a network to identify parts of the image re-\nsponsible for the activation. In addition to the high-level\nfeature activations, this reconstruction process uses extra\ninformation about maxima locations in intermediate max-\npooling layers. This information has been shown to be cru-\ncial for the approach to work [22]. A visualization method\nsimilar to DeconvNet is by Springenberg et al. [22], yet it\nalso makes use of intermediate layer activations.\nMahendran and Vedaldi [19] invert a differentiable im-\nage representation Φ using gradient descent. Given a fea-\nture vector Φ0, they seek for an image x∗which minimizes\na loss function – the squared Euclidean distance between\nΦ0 and Φ(x) plus a regularizer enforcing a natural image\nprior. This method is fundamentally different from our ap-\nproach in that it optimizes the difference between the fea-\nture vectors, not the image reconstruction error. Addition-\nally, it includes a hand-designed natural image prior, while\nin our case the network implicitly learns such a prior. Tech-\nnically, it involves optimization at test time, which requires\ncomputing the gradient of the feature representation and\nmakes it relatively slow (the authors report 6s per image on\na GPU). In contrast, the presented approach is only costly\nwhen training the inversion network. Reconstruction from\na given feature vector just requires a single forward pass\nthrough the network, which takes roughly 5ms per image on\na GPU. The method of [19] requires gradients of the feature\nrepresentation, therefore it could not be directly applied to\nnon-differentiable representations such as LBP, or record-\nings from a real brain [20].\nThere has been research on inverting various tradi-\ntional computer vision representations: HOG and dense\nSIFT [24], keypoint-based SIFT [26], Local Binary De-\nscriptors [4], Bag-of-Visual-Words [11]. All these meth-\nods are either tailored for inverting a speciﬁc feature repre-\nsentation or restricted to shallow representations, while our\nmethod can be applied to any feature representation.\n2. Method\nDenote by (x, φ) random variables representing a natu-\nral image and its feature vector, and denote their joint prob-\nability distribution by p(x, φ) = p(x)p(φ|x). Here p(x) is\nthe distribution of natural images and p(φ|x) is the distribu-\ntion of feature vectors given an image. As a special case, φ\nmay be a deterministic function of x. Ideally we would like\nto ﬁnd p(x|φ), but direct application of Bayes’ theorem is\nnot feasible. Therefore in this paper we resort to a point es-\ntimate f(φ) which minimizes the following mean squared\nerror objective:\nEx,φ ||x −f(φ)||2\n(1)\nThe minimizer of this loss is the conditional expectation:\nˆf(φ0) = Ex [x | φ = φ0],\n(2)\nthat is, the expected pre-image.\nGiven a training set of images and their features\n{xi, φi}, we learn the weights w of an an up-convolutional\nnetwork f(φ, w) to minimize a Monte-Carlo estimate of\nthe loss (1):\nˆw = arg min\nw\nX\ni\n||xi −f(φi, w)||2\n2.\n(3)\nThis means that simply training the network to predict im-\nages from their feature vectors results in estimating the ex-\npected pre-image.\n2.1. Feature representations to invert\nShallow features. We invert three traditional computer\nvision feature representations: histogram of oriented gradi-\nents (HOG), scale invariant feature transform (SIFT), and\nlocal binary patterns (LBP). We chose these features for a\nreason. There has been work on inverting HOG, so we can\ncompare to existing approaches. LBP is interesting because\nit is not differentiable, and hence gradient-based methods\ncannot invert it. SIFT is a keypoint-based representation,\nso the network has to stitch different keypoints into a single\nsmooth image.\nFor all three methods we use implementations from the\nVLFeat library [23] with the default settings. More pre-\ncisely, we use the HOG version from Felzenszwalb et al. [7]\nwith cell size 8, the version of SIFT which is very similar\nto the original implementation of Lowe [17] and the LBP\nversion similar to Ojala et al. [21] with cell size 16. Be-\nfore extracting the features we convert images to grayscale.\nMore details can be found in the supplementary material.\nAlexNet.\nWe also invert the representation of the\nAlexNet network [13] trained on ImageNet, available at\nthe Caffe [10] website. 1 It consists of 5 convolutional lay-\ners and 3 fully connected layers, with rectiﬁed linear units\n(ReLUs) after each layer, and local contrast normalization\nor max-pooling after some of them. Exact architecture is\nshown in the supplementary material.\nIn what follows,\n1More precisely, we used CaffeNet, which is almost identical to the\noriginal AlexNet.\nwhen we say ‘output of the layer’, we mean the output of the\nlast processing step of this layer. For example, the output of\nthe ﬁrst convolutional layer CONV1 would be the result af-\nter ReLU, pooling and normalization, and the output of the\nﬁrst fully connected layer FC6 is after ReLU. FC8 denotes\nthe last layer, before the softmax.\n2.2. Network architectures and training\nAn up-convolutional layer, also often referred to as ‘de-\nconvolutional’, is a combination of upsampling and convo-\nlution [6]. We upsample a feature map by a factor 2 by re-\nplacing each value by a 2 × 2 block with the original value\nin the top left corner and all other entries equal to zero. Ar-\nchitecture of one of our up-convolutional networks is shown\nin Table 1. Architectures of other networks are shown in the\nsupplementary material.\nHOG and LBP. For an image of size W × H, HOG\nand LBP features of an image form 3-dimensional arrays of\nsizes ⌈W/8⌉× ⌈H/8⌉× 31 and ⌈W/16⌉× ⌈H/16⌉× 58,\nrespectively. We use similar CNN architectures for invert-\ning both feature representations. The networks include a\ncontracting part, which processes the input features through\na series of convolutional layers with occasional stride of 2,\nresulting in a feature map 64 times smaller than the input\nimage. Then the expanding part of the network again up-\nsamples the feature map to the full image resolution by a se-\nries of up-convolutional layers. The contracting part allows\nthe network to aggregate information over large regions of\nthe input image. We found this is necessary to successfully\nestimate the absolute brightness.\nSparse SIFT. Running the SIFT detector and descrip-\ntor on an image gives a set of N keypoints, where the i-th\nkeypoint is described by its coordinates (xi, yi), scale si,\norientation αi, and a feature descriptor fi of dimensionality\nD. In order to apply a convolutional network, we arrange\nthe keypoints on a grid. We split the image into cells of\nsize d × d (we used d = 4 in our experiments), this yields\n⌈W/d⌉× ⌈H/d⌉cells. In the rare cases when there are\nseveral keypoints in a cell, we randomly select one. We\nthen assign a vector to each of the cells: a zero vector to\na cell without a keypoint and a vector (fi, xi mod d, yi\nmod d, sin αi, cos αi, log si) to a cell with a keypoint. This\nresults in a feature map F of size ⌈W/d⌉×⌈H/d⌉×(D+5).\nThen we apply a CNN to F, as described above.\nAlexNet. To reconstruct from each layer of AlexNet we\ntrained a separate network. We used two basic architectures:\none for reconstructing from convolutional layers and one for\nreconstructing from fully connected layers. The network for\nreconstructing from fully connected layers contains three\nfully connected layers and 5 up-convolutional layers, as\nshown in Table 1. The network for reconstructing from con-\nvolutional layers consists of three convolutional and several\nup-convolutional layers (the exact number depends on the\nLayer\nInput\nInSize\nK\nS\nOutSize\nfc1\nAlexNet-FC8\n1000\n−\n−\n4096\nfc2\nfc1\n4096\n−\n−\n4096\nfc3\nfc2\n4096\n−\n−\n4096\nreshape\nfc3\n4096\n−\n−\n4×4×256\nupconv1\nreshape\n4×4×256\n5\n2\n8×8×256\nupconv2\nupconv1\n8×8×256\n5\n2\n16×16×128\nupconv3\nupconv2\n16×16×128\n5\n2\n32×32×64\nupconv4\nupconv3\n32×32×64\n5\n2\n64×64×32\nupconv5\nupconv4\n64×64×32\n5\n2\n128×128×3\nTable 1: Network for reconstructing from AlexNet FC8 fea-\ntures. K stands for kernel size, S for stride.\nlayer to reconstruct from). Filters in all (up-)convolutional\nlayers have 5 × 5 spatial size. After each layer we apply\nleaky ReLU nonlinearity with slope 0.2, that is, r(x) = x if\nx ⩾0 and r(x) = 0.2 · x if x < 0.\nTraining details. We trained networks using a modiﬁed\nversion of Caffe [10]. As training data we used the Ima-\ngeNet [5] training set. In some cases we predicted down-\nsampled images to speed up computations. We used the\nAdam [12] optimizer with β1 = 0.9, β2 = 0.999 and mini-\nbatch size 64. For most networks we found an initial learn-\ning rate λ = 0.001 to work well. We gradually decreased\nthe learning rate towards the end of training. The duration of\ntraining depended on the network: from 15 epochs (passes\nthrough the dataset) for shallower networks to 60 epochs for\ndeeper ones.\nQuantitative evaluation. As a quantitative measure of\nperformance we used the average normalized reconstruc-\ntion error, that is the mean of ||xi −f(Φ(xi))||2/N, where\nxi is an example from the test set, f is the function imple-\nmented by the inversion network and N is a normalization\ncoefﬁcient equal to the average Euclidean distance between\nimages in the test set. The test set we used for quantita-\ntive and qualitative evaluations is a subset of the ImageNet\nvalidation set.\n3. Experiments: shallow representations\nFigures 1 and 3 show reconstructions of several im-\nages from the ImageNet validation set. Normalized recon-\nstruction error of different approaches is shown in Table 2.\nClearly, our method signiﬁcantly outperforms existing ap-\nproaches. This is to be expected, since our method explic-\nitly aims to minimize the reconstruction error.\nHoggles [24]\nHOG−1 [19]\nHOG our\nSIFT our\nLBP our\n0.61\n0.63\n0.24\n0.28\n0.38\nTable 2: Normalized error of different methods when recon-\nstructing from HOG.\nImage\nHOG\nHoggles [24]\nHOG−1 [19]\nOur\nFigure 2: Reconstructing an image from its HOG descriptors with different methods.\nColorization. As mentioned above, we compute the fea-\ntures based on grayscale images, but the task of the net-\nworks is to reconstruct the color images. The features do\nnot contain any color information, so to predict colors the\nnetwork has to analyze the content of the image and make\nuse of a natural image prior it learned during training. It\ndoes successfully learn to do so, as can be seen in Figures 1\nand 3. Quite often the colors are predicted correctly, espe-\ncially for sky, sea, grass, trees. In other cases, the network\ncannot predict the color (for example, people in the top row\nof Figure 3) and leaves some areas gray. Occasionally the\nnetwork predicts the wrong color, such as in the bottom row\nof Figure 3.\nHOG. Figure 2 shows an example image, its HOG rep-\nresentation, the results of inversion with existing meth-\nods [24, 19] and with our approach. Most interestingly, the\nnetwork is able to reconstruct the overall brightness of the\nimage very well, for example the dark regions are recon-\nstructed dark. This is quite surprising, since the HOG de-\nscriptors are normalized and should not contain information\nabout absolute brightness.\nNormalization is always performed with a smoothing\n’epsilon’, so one might imagine that some information\nabout the brightness is present even in the normalized fea-\ntures. We checked that the network does not make use of\nthis information: multiplying the input image by 10 or 0.1\nhardly changes the reconstruction. Therefore, we hypothe-\nsize that the network reconstructs the overall brightness by\n1) analyzing the distribution of the HOG features (if in a\ncell there is similar amount of gradient in all directions, it is\nprobably noise; if there is one dominating gradient, it must\nactually be in the image), 2) accumulating gradients over\nspace: if there is much black-to-white gradient in one di-\nrection, then probably the brightness in that direction goes\nfrom dark to bright and 3) using semantic information.\nSIFT. Figure 4 shows an image, the detected SIFT key-\npoints and the resulting reconstruction. There are roughly\nImage\nHOG our\nSIFT our\nLBP our\nFigure 3: Inversion of shallow image representations. Note\nhow in the ﬁrst row the color of grass and trees is predicted\ncorrectly in all cases, although it is not contained in the fea-\ntures.\nFigure 4: Reconstructing an image from SIFT descriptors\nwith different methods. (a) an image, (b) SIFT keypoints,\n(c) reconstruction of [26], (d) our reconstruction.\nImage\nCONV1\nCONV2\nCONV3\nCONV4\nCONV5\nFC6\nFC7\nFC8\nFigure 5: Reconstructions from different layers of AlexNet.\nImage\nCONV1\nCONV2\nCONV3\nCONV4\nCONV5\nFC6\nFC7\nFC8\nOur\n[19]\nAE\nFigure 6: Reconstructions from layers of AlexNet with our method (top), [19] (middle), and autoencoders (bottom).\n3000 keypoints detected in this image. Although made from\na sparse set of keypoints, the reconstruction looks very nat-\nural, just a little blurry. To achieve such a clear reconstruc-\ntion the network has to properly rotate and scale the descrip-\ntors and then stitch them together. Obviously it successfully\nlearns to do this.\nFor reference we also show a result of another existing\nmethod [26] for reconstructing images from sparse SIFT de-\nscriptors. The results are not directly comparable: while we\nuse the SIFT detector providing circular keypoints, Weinza-\nepfel et al. [26] use the Harris afﬁne keypoint detector which\nyields elliptic keypoints, and the number and the locations\nof the keypoints may be different from our case. However,\nthe rough number of keypoints is the same, so a qualitative\ncomparison is still valid.\n4. Experiments: AlexNet\nWe applied our inversion method to different layers of\nAlexNet and performed several additional experiments to\nbetter understand the feature representations. More results\nare shown in the supplementary material.\n4.1. Reconstructions from different layers\nFigure 5 shows reconstructions from various layers of\nAlexNet. When using features from convolutional layers,\nthe reconstructed images look very similar to the input, but\nlose ﬁne details as we progress to higher layers. There is\nan obvious drop in reconstruction quality when going from\nCONV5 to FC6. However, the reconstructions from higher\nconvolutional layers and even fully connected layers pre-\nserve color and the approximate object location very well.\nReconstructions from FC7 and FC8 still look similar to the\ninput images, but blurry. This means that high level features\nconv1 conv2 conv3 conv4 conv5\nfc6\nfc7\nfc8\n0\n0.2\n0.4\n0.6\n0.8\n1\nLayer to reconstruct from\nNormalized reconstruction error\n \n \nOur\nMahendran et al.\nAutoencoder\nOur−bin\nOur−drop50\nAutoencoder−bin\nOur−bin−drop50\nOur−bin−drop50least\nFigure 7: Average normalized reconstruction error depend-\ning on the network layer.\nare much less invariant to color and pose than one might ex-\npect: in principle fully connected layers need not preserve\nany information about colors and locations of objects in the\ninput image. This is somewhat in contrast with the results\nof [19], as shown in Figure 6. While their reconstructions\nare sharper, the color and position are completely lost in\nreconstructions from higher layers.\nFor quantitative evaluation before computing the error\nwe up-sample reconstructions to input image size with bi-\nlinear interpolation. Error curves shown in Figure 7 support\nthe conclusions made above.\nWhen reconstructing from\nFC6, the error is roughly twice as large as from CONV5.\nEven when reconstructing from FC8, the error is fairly low\nbecause the network manages to get the color and the rough\nplacement of large objects in images right. For lower lay-\ners, the reconstruction error of [19] is still much higher than\nof our method, even though visually the images look some-\nwhat sharper. The reason is that in their reconstructions the\ncolor and the precise placement of small details do not per-\nfectly match the input image, which results in a large overall\nerror.\n4.2. Autoencoder training\nOur inversion network can be interpreted as the decoder\nof the representation encoded by AlexNet. The difference to\nan autoencoder is that the encoder part stays ﬁxed and only\nthe decoder is optimized. For comparison we also trained\nautoencoders with the same architecture as our reconstruc-\ntion nets, i.e., we also allowed the training to ﬁne-tune the\nparameters of the AlexNet part. This provides an upper\nbound on the quality of reconstructions we might expect\nfrom the inversion networks (with ﬁxed AlexNet).\nAs shown in Figure 7, autoencoder training yields\nmuch lower reconstruction errors when reconstructing from\nhigher layers. Also the qualitative results in Figure 6 show\nImage\nall\ntop5\nnotop5\npomegranate (0.93)\nGranny Smith apple (0.99)\ncroquet ball (0.96)\nFigure 8: The effect of color on classiﬁcation and recon-\nstruction from layer FC8. Left to right: input image, recon-\nstruction from FC8, reconstruction from 5 largest activations\nin FC8, reconstruction from all FC8 activations except the 5\nlargest ones. Below each row the network prediction and its\nconﬁdence are shown.\nmuch better reconstructions with autoencoders. Even from\nCONV5 features, the input image can be reconstructed al-\nmost perfectly. When reconstructing from fully connected\nlayers, the autoencoder results get blurred, too, due to the\ncompressed representation, but by far not as much as with\nthe ﬁxed AlexNet weights. The gap between the autoen-\ncoder training and the training with ﬁxed AlexNet gives an\nestimate of the amount of image information lost due to the\ntraining objective of the AlexNet, which is not based on re-\nconstruction quality.\nAn interesting observation with autoencoders is that the\nreconstruction error is quite high even when reconstructing\nfrom CONV1 features, and the best reconstructions were ac-\ntually obtained from CONV4. Our explanation is that the\nconvolution with stride 4 and consequent max-pooling in\nCONV1 loses much information about the image. To de-\ncrease the reconstruction error, it is beneﬁcial for the net-\nwork to slightly blur the image instead of guessing the de-\ntails. When reconstructing from deeper layers, deeper net-\nworks can learn a better prior resulting in slightly sharper\nimages and slightly lower reconstruction error. For even\ndeeper layers, the representation gets too compressed and\nthe error increases again. We observed (not shown in the\npaper) that without stride 4 in the ﬁrst layer, the reconstruc-\ntion error of autoencoders got much lower.\n4.3. Case study: Colored apple\nWe performed a simple experiment illustrating how the\ncolor information inﬂuences classiﬁcation and how it is pre-\nserved in the high level features. We took an image of a\nred apple (Figure 8 top left) from Flickr and modiﬁed its\nImage\nCONV3\nCONV4\nCONV5\nFC6\nFC7\nFC8\nCONV3\nCONV4\nCONV5\nFC6\nFC7\nFC8\nNo\nper-\nturb\nBin\nDrop\n50\nFixed AlexNet\nAutoencoder\nFigure 9: Reconstructions from different layers of AlexNet with disturbed features.\nhue to make it green or blue. Then we extracted AlexNet\nFC8 features of the resulting images. Remind that FC8 is\nthe last layer of the network, so the FC8 features, after ap-\nplication of softmax, give the network’s prediction of class\nprobabilities. The largest activation, hence, corresponds to\nthe network’s prediction of the image class. To check how\nclass-dependent the results of inversion are, we passed three\nversions of each feature vector through the inversion net-\nwork: 1) just the vector itself, 2) all activations except the\n5 largest ones set to zero, 3) the 5 largest activations set to\nzero.\nThis leads to several conclusions. First, color clearly can\nbe very important for classiﬁcation, so the feature represen-\ntation of the network has to be sensitive to it, at least in\nsome cases. Second, the color of the image can be precisely\nreconstructed even from FC8 or, equivalently, from the pre-\ndicted class probabilities. Third, the reconstruction quality\ndoes not depend much on the top predictions of the network\nbut rather on the small probabilities of all other classes. This\nis consistent with the ’dark knowledge’ idea of [8]: small\nprobabilities of non-predicted classes carry more informa-\ntion than the prediction itself. More examples of this are\nshown in the supplementary material.\n4.4. Robustness of the feature representation\nWe have shown that high level feature maps preserve rich\ninformation about the image. How is this information rep-\nresented in the feature vector? It is difﬁcult to answer this\nquestion precisely, but we can gain some insight by perturb-\ning the feature representations in certain ways and observ-\ning images reconstructed from these perturbed features. If\nperturbing the features in a certain way does not change the\nreconstruction much, then the perturbed property is not im-\nportant. For example, if setting a non-zero feature to zero\ndoes not change the reconstruction, then this feature does\nnot carry information useful for the reconstruction.\nWe applied binarization and dropout. To binarize the fea-\nture vector, we kept the signs of all entries and set their ab-\nsolute values to a ﬁxed number, selected such that the Eu-\nclidean norm of the vector remained unchanged (we tried\nseveral other strategies, and this one led to the best result).\nFor all layers except FC8, feature vector entries are non-\nnegative, hence, binarization just sets all non-zero entries to\na ﬁxed positive value. To perform dropout, we randomly set\n50% of the feature vector entries to zero and then normal-\nize the vector to keep its Euclidean norm unchanged (again,\nwe found this normalization to work best). Qualitative re-\nsults of these perturbations of features in different layers\nof AlexNet are shown in Figure 9. Quantitative results are\nshown in Figure 7. Surprisingly, dropout leads to larger de-\ncrease in reconstruction accuracy than binarization, even in\nthe layers where it had been applied during training. In lay-\ners FC7 and especially FC6, binarization hardly changes the\nreconstruction quality at all. Although it is known that bina-\nrized ConvNet features perform well in classiﬁcation [1], it\ncomes as a surprise that for reconstructing the input image\nthe exact values of the features are not important. In FC6\nvirtually all information about the image is contained in the\nbinary code given by the pattern of non-zero activations.\nFigures 7 and 9 show that this binary code only emerges\nwhen training with the classiﬁcation objective and dropout,\nwhile autoencoders are very sensitive to perturbations in the\nfeatures.\nTo test the robustness of this binary code, we applied\nbinarization and dropout together. We tried dropping out\n50% random activations or 50% least non-zero activations\nand then binarizing. Dropping out the 50% least activations\nreduces the error much less than dropping out 50% random\nactivations and is even better than not applying any dropout\nfor most layers. However, layers FC6 and FC7 are the most\ninteresting ones: here dropping out 50% random activations\ndecreases the performance substantially, while dropping out\n50% least activations only results in a small decrease. Pos-\nsibly the exact values of the features in FC6 and FC7 do not\naffect the reconstruction much, but they estimate the impor-\ntance of different features.\n4.5. Interpolation and random feature vectors\nAnother way to analyze the feature representation is by\ntraversing the feature manifold and by observing the corre-\nCONV5\nFC6\nFC7\nFC8\nFigure 10: Interpolation between the features of two\nimages.\nsponding images generated by the reconstruction networks.\nWe have seen the reconstructions from feature vectors of\nactual images, but what if a feature vector was not gener-\nated from a natural image? In Figure 10 we show recon-\nstructions obtained with our networks when interpolating\nbetween feature vectors of two images.\nIt is interesting\nto see that interpolating CONV5 features leads to a simple\noverlay of images, but the behavior of interpolations when\nreconstructing from FC6 is very different: images smoothly\nmorph into each other. More examples, together with the\nresults for autoencoders, are shown in the supplementary\nmaterial.\nAnother analysis method is by sampling feature vectors\nrandomly. Our networks were trained to reconstruct images\ngiven their feature representations, but the distribution of\nthe feature vectors is unknown. Hence, there is no simple\nprincipled way to sample from our model. However, by\nassuming independence of the features (a very strong and\nwrong assumption!), we can approximate the distribution\nof each dimension of the feature vector separately. To this\nend we simply computed a histogram of each feature over\na set of 4096 images and sampled from those. We ensured\nthat the sparsity of the random samples is the same as that\nof the actual feature vectors. This procedure led to low con-\ntrast images, perhaps because by independently sampling\neach dimension we did not introduce interactions between\nthe features. Multiplying the feature vectors by a constant\nfactor α = 2 increases the contrast without affecting other\nproperties of the generated images.\nRandom samples obtained this way from four top layers\nof AlexNet are shown in Figure 11. No pre-selection was\nperformed. While samples from CONV5 look much like ab-\nstract art, the samples from fully convolutional layers are\nmuch more realistic. This shows that the networks learn\na natural image prior that allows them to produce some-\nwhat realistically looking images from random feature vec-\ntors. We found that a much simpler sampling procedure of\nCONV5\nFC6\nFC7\nFC8\nFigure 11: Images generated from random feature vectors\nof top layers of AlexNet.\nﬁtting a single shifted truncated Gaussian to all feature di-\nmensions produces qualitatively very similar images. These\nare shown in the supplementary material together with im-\nages generated from autoencoders, which look much less\nlike natural images.\n5. Conclusions\nWe have proposed to invert image representations with\nup-convolutional networks and have shown that this yields\nmore or less accurate reconstructions of the original images,\ndepending on the level of invariance of the feature represen-\ntation. The networks implicitly learn natural image priors\nwhich allow the retrieval of information that is obviously\nlost in the feature representation, such as color or bright-\nness in HOG or SIFT. The method is very fast at test time\nand does not require the gradient of the feature representa-\ntion to be inverted. Therefore, it can be applied to virtually\nany image representation.\nApplication of our method to the representations learned\nby the AlexNet convolutional network leads do several con-\nclusions: 1) Features from all layers of the network, includ-\ning the ﬁnal FC8 layer, preserve the precise colors and the\nrough position of objects in the image; 2) In higher layers,\nalmost all information about the input image is contained in\nthe pattern of non-zero activations, not their precise values;\n3) In the layer FC8, most information about the input image\nis contained in small probabilities of those classes that are\nnot in top-5 network predictions.\nAcknowledgements\nWe acknowledge funding by the ERC Starting Grant\nVideoLearn (279401). We are grateful to Aravindh Mahen-\ndran for sharing with us the reconstructions achieved with\nthe method of Mahendran and Vedaldi [19]. We thank Jost\nTobias Springenberg for comments.\nReferences\n[1] P. Agrawal, R. Girshick, and J. Malik. Analyzing the perfor-\nmance of multilayer neural networks for object recognition.\nIn ECCV, 2014. 7\n[2] C. M. Bishop. Neural Networks for Pattern Recognition. Ox-\nford Uni. Press, New York, USA, 1995. 2\n[3] N. Dalal and B. Triggs. Histograms of oriented gradients for\nhuman detection. In CVPR, pages 886–893, 2005. 1\n[4] E. d’Angelo, L. Jacques, A. Alahi, and P. Vandergheynst.\nFrom bits to images: Inversion of local binary descriptors.\nIEEE Trans. Pattern Anal. Mach. Intell., 36(5):874–887,\n2014. 2\n[5] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei.\nImageNet: A Large-Scale Hierarchical Image Database. In\nCVPR, 2009. 3\n[6] A. Dosovitskiy, J. T. Springenberg, and T. Brox. Learning\nto generate chairs with convolutional neural networks. In\nCVPR, 2015. 3\n[7] P. F. Felzenszwalb, R. B. Girshick, D. McAllester, and D. Ra-\nmanan. Object detection with discriminatively trained part\nbased models. TPAMI, 32(9):1627–1645, 2010. 1, 2\n[8] G. E. Hinton, O. Vinyals, and J. Dean. Distilling the knowl-\nedge in a neural network. arXiv:1503.02531, 2015. 7\n[9] C. Jensen, R. Reed, R. Marks, M. El-Sharkawi, J.-B. Jung,\nR. Miyamoto, G. Anderson, and C. Eggen. Inversion of feed-\nforward neural networks: Algorithms and applications. In\nProc. IEEE, pages 1536–1549, 1999. 2\n[10] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Gir-\nshick, S. Guadarrama, and T. Darrell. Caffe: Convolutional\narchitecture for fast feature embedding. arXiv:1408.5093,\n2014. 2, 3\n[11] H. Kato and T. Harada. Image reconstruction from bag-of-\nvisual-words. In CVPR, June 2014. 2\n[12] D. P. Kingma and J. Ba. Adam: A method for stochastic\noptimization. In ICLR, 2015. 3\n[13] A. Krizhevsky, I. Sutskever, and G. E. Hinton. ImageNet\nclassiﬁcation with deep convolutional neural networks. In\nNIPS, pages 1106–1114, 2012. 1, 2\n[14] Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E.\nHoward, W. Hubbard, and L. D. Jackel. Backpropagation\napplied to handwritten zip code recognition. Neural Compu-\ntation, 1(4):541–551, 1989. 1\n[15] S. Lee and R. M. Kil. Inverse mapping of continuous func-\ntions using local and global information. IEEE Transactions\non Neural Networks, 5(3):409–423, 1994. 2\n[16] A. Linden and J. Kindermann. Inversion of multilayer nets.\nIn Proc. Int. Conf. on Neural Networks, 1989. 2\n[17] D. G. Lowe. Distinctive image features from scale-invariant\nkeypoints.\nInternational Journal of Computer Vision,\n60(2):91–110, 2004. 1, 2\n[18] B. Lu, H. Kita, and Y. Nishikawa. Inverting feedforward neu-\nral networks using linear and nonlinear programming. IEEE\nTransactions on Neural Networks, 10(6):1271–1290, 1999.\n2\n[19] A. Mahendran and A. Vedaldi. Understanding deep image\nrepresentations by inverting them. In CVPR, 2015. 1, 2, 3,\n4, 5, 6, 8, 11, 13\n[20] S. Nishimoto, A. Vu, T. Naselaris, Y. Benjamini, B. Yu,\nand J. Gallant.\nReconstructing visual experiences from\nbrain activity evoked by natural movies. Current Biology,\n21(19):1641–1646, 2011. 2\n[21] T. Ojala, M. Pietik¨ainen, and T. M¨aenp¨a¨a. Multiresolution\ngray-scale and rotation invariant texture classiﬁcation with\nlocal binary patterns. TPAMI, 24(7):971–987, 2002. 1, 2\n[22] J. T. Springenberg, A. Dosovitskiy, T. Brox, and M. Ried-\nmiller. Striving for simplicity: The all convolutional net. In\nICLR Workshop Track, 2015. 1, 2\n[23] A. Vedaldi and B. Fulkerson. Vlfeat: an open and portable\nlibrary of computer vision algorithms. In International Con-\nference on Multimedia, pages 1469–1472, 2010. 2, 10\n[24] C. Vondrick, A. Khosla, T. Malisiewicz, and A. Torralba.\nHoggles: Visualizing object detection features. ICCV, 2013.\n2, 3, 4\n[25] A. R. Vrkonyi-Kczy. Observer-based iterative fuzzy and neu-\nral network model inversion for measurement and control\napplications. In I. J. Rudas, J. C. Fodor, and J. Kacprzyk,\neditors, Towards Intelligent Engineering and Information\nTechnology, volume 243 of Studies in Computational Intelli-\ngence, pages 681–702. Springer, 2009. 2\n[26] P. Weinzaepfel, H. Jegou, and P. Prez. Reconstructing an\nimage from its local descriptors. In CVPR. IEEE Computer\nSociety, 2011. 2, 4, 5\n[27] R. J. Williams. Inverting a connectionist network mapping\nby back-propagation of error. In Eighth Annual Conference\nof the Cognitive Society, pages 859–865, 1986. 2\n[28] M. D. Zeiler and R. Fergus. Visualizing and understanding\nconvolutional networks. In ECCV, 2014. 1, 2\nSupplementary material\nNetwork architectures\nTable 3 shows the architecture of\nAlexNet. Tables 4-8 show the architectures of networks\nwe used for inverting different features. After each fully\nconnected and convolutional layer there is always a leaky\nReLU nonlinearity. Networks for inverting HOG and LBP\nhave two streams. Stream A compresses the input features\nspatially and accumulates information over large regions.\nWe found this crucial to get good estimates of the overall\nbrightness of the image. Stream B does not compress spa-\ntially and hence can better preserve ﬁne local details. At\none points the outputs of the two streams are concatenated\nand processed jointly, denoted by “J”. K stands for kernel\nsize, S for stride.\nShallow features details\nAs mentioned, in the paper, for\nall three methods we use implementations from the VLFeat\nlibrary [23] with the default settings. We use the Felzen-\nszwalb et al. version of HOG with cell size 8. For SIFT\nwe used 3 levels per octave, the ﬁrst octave was 0 (corre-\nsponding to full resolution), the number of octaves was set\nautomatically, effectively searching keypoints of all possi-\nble sizes.\nLayer\nInput\nInSize\nK\nS\nOutSize\nconvA1\nHOG\n32×32×31\n5\n2\n16×16×256\nconvA2\nconvA1\n16×16×256\n5\n2\n8×8×512\nconvA3\nconvA2\n8×8×512\n3\n2\n4×4×1024\nupconvA1\nconvA3\n4×4×1024\n4\n2\n8×8×512\nupconvA2\nupconvA1\n8×8×512\n4\n2\n16×16×256\nupconvA3\nupconvA2\n16×16×256\n4\n2\n32×32×128\nconvB1\nHOG\n32×32×31\n5\n1\n32×32×128\nconvB2\nconvB1\n32×32×128\n3\n1\n32×32×128\nconvJ1\n{upconvA3, convB2}\n32×32×256\n3\n1\n32×32×256\nconvJ2\nconvJ1\n32×32×256\n3\n1\n32×32×128\nupconvJ4\nconvJ2\n32×32×128\n4\n2\n64×64×64\nupconvJ5\nupconvJ4\n64×64×64\n4\n2\n128×128×32\nupconvJ6\nupconvJ5\n128×128×32\n4\n2\n256×256×3\nTable 4: Network for reconstructing from HOG features.\nLayer\nInput\nInSize\nK\nS\nOutSize\nconv1\nSIFT\n64×64×133\n5\n2\n32×32×256\nconv2\nconv1\n32×32×256\n3\n2\n16×16×512\nconv3\nconv2\n16×16×512\n3\n2\n8×8×1024\nconv4\nconv3\n8×8×1024\n3\n2\n4×4×2048\nconv5\nconv4\n4×4×2048\n3\n1\n4×4×2048\nconv6\nconv5\n4×4×2048\n3\n1\n4×4×1024\nupconv1\nconv6\n4×4×1024\n4\n2\n8×8×512\nupconv2\nupconv1\n8×8×512\n4\n2\n16×16×256\nupconv3\nupconv2\n16×16×256\n4\n2\n32×32×128\nupconv4\nupconv3\n32×32×128\n4\n2\n64×64×64\nupconv5\nupconv4\n64×64×64\n4\n2\n128×128×32\nupconv6\nupconv5\n128×128×32\n4\n2\n256×256×3\nTable 5: Network for reconstructing from SIFT features.\nLayer\nInput\nInSize\nK\nS\nOutSize\nconvA1\nLBP\n16×16×58\n5\n2\n8×8×256\nconvA2\nconvA1\n8×8×256\n5\n2\n4×4×512\nconvA3\nconvA2\n4×4×512\n3\n1\n4×4×1024\nupconvA1\nconvA3\n4×4×1024\n4\n2\n8×8×512\nupconvA2\nupconvA1\n8×8×512\n4\n2\n16×16×256\nconvB1\nLBP\n16×16×58\n5\n1\n16×16×128\nconvB2\nconvB1\n16×16×128\n3\n1\n16×16×128\nconvJ1\n{upconvA2, convB2}\n16×16×384\n3\n1\n16×16×256\nconvJ2\nconvJ1\n16×16×256\n3\n1\n16×16×128\nupconvJ3\nconvJ2\n16×16×128\n4\n2\n32×32×128\nupconvJ4\nupconvJ3\n32×32×128\n4\n2\n64×64×64\nupconvJ5\nupconvJ4\n64×64×64\n4\n2\n128×128×32\nupconvJ6\nupconvJ5\n128×128×32\n4\n2\n256×256×3\nTable 6: Network for reconstructing from LBP features.\nLayer\nInput\nInSize\nK\nS\nOutSize\nconv1\nAlexNet-CONV5\n6×6×256\n3\n1\n6×6×256\nconv2\nconv1\n6×6×256\n3\n1\n6×6×256\nconv3\nconv2\n6×6×256\n3\n1\n6×6×256\nupconv1\nconv3\n6×6×256\n5\n2\n12×12×256\nupconv2\nupconv1\n12×12×256\n5\n2\n24×24×128\nupconv3\nupconv2\n24×24×128\n5\n2\n48×48×64\nupconv4\nupconv3\n48×48×64\n5\n2\n96×96×32\nupconv5\nupconv4\n96×96×32\n5\n2\n192×192×3\nTable 7: Network for reconstructing from AlexNet CONV5\nfeatures.\nThe LBP version we used works with 3 × 3 pixel neigh-\nborhoods. Each of the 8 non-central bits is equal to one if\nthe corresponding pixel is brighter than the central one. All\npossible 256 patterns are quantized into 58 patterns. These\ninclude 56 patterns with exactly one transition from 0 to 1\nwhen going around the central pixel, plus one quantized pat-\ntern comprising two uniform patterns, plus one quantized\npattern containing all other patterns. The quantized LBP\npatterns are then grouped into local histograms over cells of\n16 × 16 pixels.\nExperiments: shallow representations\nFigure 12 shows\nseveral images and their reconstructions from HOG, SIFT\nand LBP. HOG allows for the best reconstruction, SIFT\nslightly worse, LBP yet slightly worse. Colors are often\nLayer\nInput\nInSize\nK\nS\nOutSize\nfc1\nAlexNet-FC8\n1000\n−\n−\n4096\nfc2\nfc1\n4096\n−\n−\n4096\nfc3\nfc2\n4096\n−\n−\n4096\nreshape\nfc3\n4096\n−\n−\n4×4×256\nupconv1\nreshape\n4×4×256\n5\n2\n8×8×256\nupconv2\nupconv1\n8×8×256\n5\n2\n16×16×128\nupconv3\nupconv2\n16×16×128\n5\n2\n32×32×64\nupconv4\nupconv3\n32×32×64\n5\n2\n64×64×32\nupconv5\nupconv4\n64×64×32\n5\n2\n128×128×3\nTable 8: Network for reconstructing from AlexNet FC8 fea-\ntures.\nlayer\nCONV1\nCONV2\nCONV3 CONV4\nCONV5\nFC6\nFC7\nFC8\nprocessing\nconv1 mpool1 conv2 mpool2 conv3\nconv4 conv5 mpool5\nfc6\ndrop6\nfc7\ndrop7\nfc8\nsteps\nrelu1\nnorm1\nrelu2\nnorm2\nrelu3\nrelu4\nrelu5\nrelu6\nrelu7\nout size\n55\n27\n27\n13\n13\n13\n13\n6\n1\n1\n1\n1\n1\nout channels\n96\n96\n256\n256\n384\n384\n256\n256\n4096 4096 4096 4096 1000\nTable 3: Summary of the AlexNet network. Input image size is 227 × 227.\nImage\nHOG our\nSIFT our\nLBP our\nFigure 12: Inversion of shallow image representations.\nreconstructed correctly, but sometimes are wrong, for ex-\nample in the last row. Interestingly, all network typically\nagree on estimated colors.\nExperiments: AlexNet\nWe show here several additional\nﬁgures similar to ones from the main paper. Reconstruc-\ntions from different layers of AlexNet are shown in Fig-\nure 13 . Figure 14 shows results illustrating the ’dark knowl-\nedge’ hypothesis, similar to Figure 8 from the main paper.\nWe reconstruct from all FC8 features, as well as from only\n5 largest ones or all except the 5 largest ones. It turns out\nthat the top 5 activations are not very important.\nFigure 15 shows images generated by activating single\nneurons in different layers and setting all other neurons to\nzero. Particularly interpretable are images generated this\nway from FC8. Every FC8 neuron corresponds to a class.\nHence the image generated from the activation of, say, “ap-\nple” neuron, could be expected to be a stereotypical apple.\nWhat we observe looks rather like it might be the average of\nall images of the class. For some classes the reconstructions\nare somewhat interpretable, for others – not so much.\nQualitative comparison of reconstructions with our\nmethod to the reconstructions of [19] and the results with\nAlexNet-based autoencoders is given in Figure 16 .\nReconstructions from feature vectors obtained by inter-\npolating between feature vectors of two images are shown in\nFigure 17 , both for ﬁxed AlexNet and autoencoder training.\nMore examples of such interpolations with ﬁxed AlexNet\nare shown in Figure 18 .\nAs described in section 5.5 of the main paper, we tried\ntwo different distributions for sampling random feature ac-\ntivations: a histogram-based and a truncated Gaussian. Fig-\nure 19 shows the results with ﬁxed AlexNet network and\ntruncated Gaussian distribution. Figures 20 and 21 show\nimages generated with autoencoder-trained networks. Note\nthat images generated from autoencoders look much less\nrealistic than images generated with a network with ﬁxed\nAlexNet weights. This indicates that reconstructing from\nAlexNet features requires a strong natural image prior.\nImage\nCONV1\nCONV2\nCONV3\nCONV4\nCONV5\nFC6\nFC7\nFC8\nFigure 13: Reconstructions from different layers of AlexNet.\nImage\nall\ntop5\nnotop5\nFigure 14: Left to right: input image,\nreconstruction from fc8, reconstruction\nfrom 5 largest activations in FC8, recon-\nstruction from all FC8 activations except\n5 largest ones.\nFC6\nFC7\nFC8\nFigure 15: Reconstructions from single neuron activations in the fully con-\nnected layers of AlexNet. The FC8 neurons correspond to classes, left to\nright: kite, convertible, desktop computer, school bus, street sign, soup\nbowl, bell pepper, soccer ball.\nImage\nCONV1\nCONV2\nCONV3\nCONV4\nCONV5\nFC6\nFC7\nFC8\nOur\n[19]\nAE\nOur\n[19]\nAE\nFigure 16: Reconstructions from different layers of AlexNet with our method and [19].\nCONV4\nCONV5\nFC6\nFC7\nFC8\nFigure 17: Interpolation between the features of two images. Left: AlexNet weights ﬁxed, right: autoencoder.\nCONV4\nCONV5\nFC6\nFC7\nFC8\nFigure 18: More interpolations between the features of two images with ﬁxed AlexNet weights.\nCONV5\nFC6\nFC7\nFC8\nFigure 19: Images generated from random feature vectors of top layers of AlexNet with the simpler truncated Gaussian\ndistribution (see section 5.5 of the main paper).\nCONV5\nFC6\nFC7\nFC8\nFigure 20: Images generated from random feature vectors of top layers of AlexNet-based autoencoders with the histogram-\nbased distribution (see section 5.5 of the main paper).\nCONV5\nFC6\nFC7\nFC8\nFigure 21: Images generated from random feature vectors of top layers of AlexNet-based autoencoders with the simpler\ntruncated Gaussian distribution (see section 5.5 of the main paper).\n",
        "sentence": " Significant visualization techniques have been developed to understand the deep image representations on trained networks [5, 6, 7, 8, 9, 10, 11]. The philosophy is to train another neural network to inverse the representation and speedup the visualization on image reconstruction[10, 18], texture synthesis[19] or even style transfer[15]."
    },
    {
        "title": "Deep neural networks are easily fooled: High confidence predictions for unrecognizable images",
        "author": [
            "Anh Nguyen",
            "Jason Yosinski",
            "Jeff Clune"
        ],
        "venue": "In CVPR,",
        "citeRegEx": "11",
        "shortCiteRegEx": "11",
        "year": 2015,
        "abstract": "Deep neural networks (DNNs) have recently been achieving state-of-the-art\nperformance on a variety of pattern-recognition tasks, most notably visual\nclassification problems. Given that DNNs are now able to classify objects in\nimages with near-human-level performance, questions naturally arise as to what\ndifferences remain between computer and human vision. A recent study revealed\nthat changing an image (e.g. of a lion) in a way imperceptible to humans can\ncause a DNN to label the image as something else entirely (e.g. mislabeling a\nlion a library). Here we show a related result: it is easy to produce images\nthat are completely unrecognizable to humans, but that state-of-the-art DNNs\nbelieve to be recognizable objects with 99.99% confidence (e.g. labeling with\ncertainty that white noise static is a lion). Specifically, we take\nconvolutional neural networks trained to perform well on either the ImageNet or\nMNIST datasets and then find images with evolutionary algorithms or gradient\nascent that DNNs label with high confidence as belonging to each dataset class.\nIt is possible to produce images totally unrecognizable to human eyes that DNNs\nbelieve with near certainty are familiar objects, which we call \"fooling\nimages\" (more generally, fooling examples). Our results shed light on\ninteresting differences between human vision and current DNNs, and raise\nquestions about the generality of DNN computer vision.",
        "full_text": "Deep Neural Networks are Easily Fooled:\nHigh Conﬁdence Predictions for Unrecognizable Images\nAnh Nguyen\nUniversity of Wyoming\nanguyen8@uwyo.edu\nJason Yosinski\nCornell University\nyosinski@cs.cornell.edu\nJeff Clune\nUniversity of Wyoming\njeffclune@uwyo.edu\nFull Citation: Nguyen A, Yosinski J, Clune J. Deep Neural Networks are Easily Fooled: High Conﬁdence Predictions\nfor Unrecognizable Images. In Computer Vision and Pattern Recognition (CVPR ’15), IEEE, 2015.\nAbstract\nDeep neural networks (DNNs) have recently been\nachieving state-of-the-art performance on a variety of\npattern-recognition tasks, most notably visual classiﬁcation\nproblems. Given that DNNs are now able to classify objects\nin images with near-human-level performance, questions\nnaturally arise as to what differences remain between com-\nputer and human vision. A recent study [30] revealed that\nchanging an image (e.g. of a lion) in a way imperceptible to\nhumans can cause a DNN to label the image as something\nelse entirely (e.g. mislabeling a lion a library). Here we\nshow a related result: it is easy to produce images that are\ncompletely unrecognizable to humans, but that state-of-the-\nart DNNs believe to be recognizable objects with 99.99%\nconﬁdence (e.g.\nlabeling with certainty that white noise\nstatic is a lion). Speciﬁcally, we take convolutional neu-\nral networks trained to perform well on either the ImageNet\nor MNIST datasets and then ﬁnd images with evolutionary\nalgorithms or gradient ascent that DNNs label with high\nconﬁdence as belonging to each dataset class. It is possi-\nble to produce images totally unrecognizable to human eyes\nthat DNNs believe with near certainty are familiar objects,\nwhich we call “fooling images” (more generally, fooling ex-\namples). Our results shed light on interesting differences\nbetween human vision and current DNNs, and raise ques-\ntions about the generality of DNN computer vision.\n1. Introduction\nDeep neural networks (DNNs) learn hierarchical lay-\ners of representation from sensory input in order to per-\nform pattern recognition [2, 14]. Recently, these deep ar-\nchitectures have demonstrated impressive, state-of-the-art,\nand sometimes human-competitive results on many pattern\nrecognition tasks, especially vision classiﬁcation problems\n[16, 7, 31, 17]. Given the near-human ability of DNNs to\nclassify visual objects, questions arise as to what differences\nremain between computer and human vision.\nFigure 1.\nEvolved images that are unrecognizable to humans,\nbut that state-of-the-art DNNs trained on ImageNet believe with\n≥99.6% certainty to be a familiar object. This result highlights\ndifferences between how DNNs and humans recognize objects.\nImages are either directly (top) or indirectly (bottom) encoded.\nA recent study revealed a major difference between DNN\nand human vision [30]. Changing an image, originally cor-\nrectly classiﬁed (e.g. as a lion), in a way imperceptible to\nhuman eyes, can cause a DNN to label the image as some-\nthing else entirely (e.g. mislabeling a lion a library).\nIn this paper, we show another way that DNN and human\nvision differ: It is easy to produce images that are com-\npletely unrecognizable to humans (Fig. 1), but that state-of-\nthe-art DNNs believe to be recognizable objects with over\n99% conﬁdence (e.g. labeling with certainty that TV static\n1\narXiv:1412.1897v4  [cs.CV]  2 Apr 2015\nFigure 2.\nAlthough state-of-the-art deep neural networks can increasingly recognize natural images (left panel), they also are easily\nfooled into declaring with near-certainty that unrecognizable images are familiar objects (center). Images that fool DNNs are produced by\nevolutionary algorithms (right panel) that optimize images to generate high-conﬁdence DNN predictions for each class in the dataset the\nDNN is trained on (here, ImageNet).\nis a motorcycle). Speciﬁcally, we use evolutionary algo-\nrithms or gradient ascent to generate images that are given\nhigh prediction scores by convolutional neural networks\n(convnets) [16, 18]. These DNN models have been shown\nto perform well on both the ImageNet [10] and MNIST [19]\ndatasets. We also ﬁnd that, for MNIST DNNs, it is not easy\nto prevent the DNNs from being fooled by retraining them\nwith fooling images labeled as such. While retrained DNNs\nlearn to classify the negative examples as fooling images, a\nnew batch of fooling images can be produced that fool these\nnew networks, even after many retraining iterations.\nOur ﬁndings shed light on current differences between\nhuman vision and DNN-based computer vision. They also\nraise questions about how DNNs perform in general across\ndifferent types of images than the ones they have been\ntrained and traditionally tested on.\n2. Methods\n2.1. Deep neural network models\nTo test whether DNNs might give false positives for\nunrecognizable images, we need a DNN trained to near\nstate-of-the-art performance. We choose the well-known\n“AlexNet” architecture from [16], which is a convnet\ntrained on the 1.3-million-image ILSVRC 2012 ImageNet\ndataset [10, 24]. Speciﬁcally, we use the already-trained\nAlexNet DNN provided by the Caffe software package [15].\nIt obtains 42.6% top-1 error rate, similar to the 40.7% re-\nported by Krizhevsky 2012 [16]. While the Caffe-provided\nDNN has some small differences from Krizhevsky 2012\n[16], we do not believe our results would be qualitatively\nchanged by small architectural and optimization differences\nor their resulting small performance improvements. Simi-\nlarly, while recent papers have improved upon Krizhevsky\n2012, those differences are unlikely to change our results.\nWe chose AlexNet because it is widely known and a trained\nDNN similar to it is publicly available. In this paper, we\nrefer to this model as “ImageNet DNN”.\nTo test that our results hold for other DNN architectures\nand datasets, we also conduct experiments with the Caffe-\nprovided LeNet model [18] trained on the MNIST dataset\n[19]. The Caffe version has a minor difference from the\noriginal architecture in [18] in that its neural activation func-\ntions are rectiﬁed linear units (ReLUs) [22] instead of sig-\nmoids. This model obtains 0.94% error rate, similar to the\n0.8% of LeNet-5 [18]. We refer to this model as “MNIST\nDNN”.\n2.2. Generating images with evolution\nThe novel images we test DNNs on are produced by evo-\nlutionary algorithms (EAs) [12]. EAs are optimization al-\ngorithms inspired by Darwinian evolution. They contain\na population of “organisms” (here, images) that alternately\nface selection (keeping the best) and then random pertur-\nbation (mutation and/or crossover). Which organisms are\nselected depends on the ﬁtness function, which in these ex-\nperiments is the highest prediction value a DNN makes for\nthat image belonging to a class (Fig. 2).\nTraditional EAs optimize solutions to perform well on\none objective, or on all of a small set of objectives [12] (e.g.\nevolving images to match a single ImageNet class). We\ninstead use a new algorithm called the multi-dimensional\narchive of phenotypic elites MAP-Elites [6], which enables\nus to simultaneously evolve a population that contains in-\ndividuals that score well on many classes (e.g. all 1000\nImageNet classes).\nOur results are unaffected by using\nthe more computationally efﬁcient MAP-Elites over single-\ntarget evolution (data not shown). MAP-Elites works by\nkeeping the best individual found so far for each objective.\nEach iteration, it chooses a random organism from the pop-\nulation, mutates it randomly, and replaces the current cham-\npion for any objective if the new individual has higher ﬁt-\nness on that objective. Here, ﬁtness is determined by show-\ning the image to the DNN; if the image generates a higher\nprediction score for any class than has been seen before, the\nnewly generated individual becomes the champion in the\narchive for that class.\nWe test EAs with two different encodings [29, 5], mean-\ning how an image is represented as a genome. The ﬁrst\nhas a direct encoding, which has one grayscale integer for\neach of 28 × 28 pixels for MNIST, and three integers (H, S,\nV) for each of 256 × 256 pixels for ImageNet. Each pixel\nvalue is initialized with uniform random noise within the\n[0, 255] range. Those numbers are independently mutated;\nﬁrst by determining which numbers are mutated, via a rate\nthat starts at 0.1 (each number has a 10% chance of being\nchosen to be mutated) and drops by half every 1000 gener-\nations. The numbers chosen to be mutated are then altered\nvia the polynomial mutation operator [8] with a ﬁxed muta-\ntion strength of 15. The second EA has an indirect encod-\ning, which is more likely to produce regular images, mean-\ning images that contain compressible patterns (e.g. symme-\ntry and repetition) [20]. Indirectly encoded images tend to\nbe regular because elements in the genome can affect mul-\ntiple parts of the image [28]. Speciﬁcally, the indirect en-\ncoding here is a compositional pattern-producing network\n(CPPN), which can evolve complex, regular images that re-\nsemble natural and man-made objects [25, 28, 1].\nImportantly, images evolved with CPPNs can be recog-\nnized by DNNs (Fig. 3), providing an existence proof that\na CPPN-encoded EA can produce images that both humans\nand DNNs can recognize. These images were produced on\nPicBreeder.org [25], a site where users serve as the ﬁtness\nfunction in an evolutionary algorithm by selecting images\nthey like, which become the parents of the next generation.\nCPPNs are similar to artiﬁcial neural networks (ANNs).\nA CPPN takes in the (x, y) position of a pixel as input, and\noutputs a grayscale value (MNIST) or tuple of HSV color\nvalues (ImageNet) for that pixel. Like a neural network,\nthe function the CPPN computes depends on the number\nof neurons in the CPPN, how they are connected, and the\nweights between neurons. Each CPPN node can be one of\na set of activation functions (here: sine, sigmoid, Gaussian\nand linear), which can provide geometric regularities to the\nimage. For example, passing the x input into a Gaussian\nfunction will provide left-right symmetry, and passing the\ny input into a sine function provides top-bottom repetition.\nEvolution determines the topology, weights, and activation\nfunctions of each CPPN network in the population.\nAs is custom, and was done for the images in Fig. 3,\nCPPN networks start with no hidden nodes, and nodes are\nadded over time, encouraging evolution to ﬁrst search for\nsimple, regular images before adding complexity [27]. Our\nexperiments are implemented in the Sferes evolutionary\ncomputation framework [21]. Our code and parameters are\nFigure 3. Evolved, CPPN-encoded images produced with humans\nperforming selection on PicBreeder.org. Human image breeders\nnamed each object (centered text). Blue bars show the top three\nclassiﬁcations made by a DNN trained on ImageNet (size indi-\ncates conﬁdence). Often the ﬁrst classiﬁcation relates to the hu-\nman breeder’s label, showing that CPPN-encoded evolution can\nproduce images that humans and DNNs can recognize.\navailable at http://EvolvingAI.org/fooling.\n3. Results\n3.1. Evolving irregular images to match MNIST\nWe ﬁrst evolve directly encoded images to be conﬁdently\ndeclared by LeNet to be digits 0 thru 9 (recall that LeNet is\ntrained to recognize digits from the MNIST dataset). Mul-\ntiple, independent runs of evolution repeatedly produce im-\nages that MNIST DNNs believe with 99.99% conﬁdence to\nbe digits, but are unrecognizable as such (Fig. 4). In less\nthan 50 generations, each run of evolution repeatedly pro-\nduces unrecognizable images of each digit type classiﬁed by\nMNIST DNNs with ≥99.99% conﬁdence. By 200 genera-\ntions, median conﬁdence is 99.99%. Given the DNN’s near-\ncertainty, one might expect these images to resemble hand-\nwritten digits. On the contrary, the generated images look\nnothing like the handwritten digits in the MNIST dataset.\n3.2. Evolving regular images to match MNIST\nBecause CPPN encodings can evolve recognizable im-\nages (Fig. 3), we tested whether this more capable, regular\nencoding might produce more recognizable images than the\nirregular white-noise static of the direct encoding. The re-\nsult, while containing more strokes and other regularities,\nstill led to MNIST DNNs labeling unrecognizable images as\ndigits with 99.99% conﬁdence (Fig. 5) after only a few gen-\nerations. By 200 generations, median conﬁdence is 99.99%.\nCertain patterns repeatedly evolve in some digit classes\nthat appear indicative of that digit (Fig. 5). Images classi-\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\nFigure 4. Directly encoded, thus irregular, images that MNIST\nDNNs believe with 99.99% conﬁdence are digits 0-9. Each col-\numn is a digit class, and each row is the result after 200 generations\nof a randomly selected, independent run of evolution.\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\nFigure 5. Indirectly encoded, thus regular, images that MNIST\nDNNs believe with 99.99% conﬁdence are digits 0-9. The column\nand row descriptions are the same as for Fig. 4.\nﬁed as a 1 tend to have vertical bars, while images classi-\nﬁed as a 2 tend to have a horizontal bar in the lower half\nof the image. Qualitatively similar discriminative features\nare observed in 50 other runs as well (supplementary mate-\nrial). This result suggests that the EA exploits speciﬁc dis-\ncriminative features corresponding to the handwritten digits\nlearned by MNIST DNNs.\n3.3. Evolving irregular images to match ImageNet\nWe hypothesized that MNIST DNNs might be easily\nfooled because they are trained on a small dataset that could\nallow for overﬁtting (MNIST has only 60,000 training im-\nages). To test this hypothesis that a larger dataset might\nprevent the pathology, we evolved directly encoded images\nto be classiﬁed conﬁdently by a convolutional DNN [16]\ntrained on the ImageNet 2012 dataset, which has 1.3 mil-\nlion natural images in 1000 classes [9]. Conﬁdence scores\nfor images were averaged over 10 crops (1 center, 4 corners\nand 5 mirrors) of size 227 × 227.\nThe directly encoded EA was less successful at produc-\ning high-conﬁdence images in this case. Even after 20,000\ngenerations, evolution failed to produce high-conﬁdence\nimages for many categories (Fig. 6, median conﬁdence\n21.59%). However, evolution did manage to produce im-\nages for 45 classes that are classiﬁed with ≥99% conﬁ-\ndence to be natural images (Fig. 1). While in some cases\none might discern features of the target class in the image\nif told the class, humans without such priming would not\nrecognize the image as belonging to that class.\n1\n200\n400\n600\n800\n1000\nCategory\n0\n5\n10\n15\n20\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nConfidence\n(thousands)\nGeneration\nFigure 6. Median conﬁdence scores from 5 runs of directly en-\ncoded, evolved images for all 1000 ImageNet classes. Though\nrare, evolution can produce images that the DNN believes with\nover 99% conﬁdence to be in a natural, ImageNet class.\n3.4. Evolving regular images to match ImageNet\nOnce again, we test whether the CPPN encoding, which\nhas previously evolved images that both humans and DNNs\nrecognize similarly (Fig. 3), might produce more recogniz-\nable images than the direct encoding. The hypothesis is that\nthe larger ImageNet dataset and more powerful DNN ar-\nchitecture may interact with the CPPN encoding to ﬁnally\nproduce recognizable images.\nIn ﬁve independent runs, evolution produces many im-\nages with DNN conﬁdence scores ≥99.99%, but that are\nunrecognizable (Fig. 1 bottom). After 5000 generations, the\nmedian conﬁdence score reaches 88.11%, similar to that for\nnatural images (supplementary material) and signiﬁcantly\nhigher than the 21.59% for the direct encoding (Fig. 12,\np < 0.0001 via Mann-Whitney U test), which was given 4-\nfold more generations. High-conﬁdence images are found\nin most categories (Fig. 7).\nFigure 7. Median conﬁdence scores from 5 runs of CPPN-\nencoded, evolved images for all 1000 ImageNet classes. Evolution\ncan produce many images that the DNN believes with over 99%\nconﬁdence to belong to ImageNet classes.\nWhile a human not given the class labels for CPPN im-\nages would not label them as belonging to that class, the\ngenerated images do often contain some features of the tar-\nget class. For example, in Fig. 1, the starﬁsh image contains\nthe blue of water and the orange of a starﬁsh, the baseball\nhas red stitching on a white background, the remote control\nFigure 8. Evolving images to match DNN classes produces a\ntremendous diversity of images. Shown are images selected to\nshowcase diversity from 5 evolutionary runs. The diversity sug-\ngests that the images are non-random, but that instead evolutions\nproducing discriminative features of each target class. The mean\nDNN conﬁdence scores for these images is 99.12%.\nhas a grid of buttons, etc. For many of the produced images,\none can begin to identify why the DNN believes the image\nis of that class once given the class label. This is because\nevolution need only to produce features that are unique to,\nor discriminative for, a class, rather than produce an image\nthat contains all of the typical features of a class.\nThe pressure to create these discriminative features led\nto a surprising amount of diversity in the images pro-\nduced (Fig. 8). That diversity is especially noteworthy be-\ncause (1) it has been shown that imperceptible changes to an\nimage can change a DNN’s class label [30], so it could have\nbeen the case that evolution produced very similar, high-\nconﬁdence images for all classes, and (2) many of the im-\nages are related to each other phylogenetically, which leads\nevolution to produce similar images for closely related cat-\negories (Fig. 9). For example, one image type receives high\nconﬁdence scores for three types of lizards, and a different\nimage type receives high conﬁdence scores for three types\nof small, ﬂuffy dogs. Different runs of evolution, however,\nproduce different image types for these related categories,\nrevealing that there are different discriminative features per\nclass that evolution exploits. That suggests that there are\nmany different ways to fool the same DNN for each class.\nMany of the CPPN images feature a pattern repeated\nmany times. To test whether that repetition improves the\nconﬁdence score a DNN gives an image, or whether the\nrepetition stems solely from the fact that CPPNs tend to pro-\nduce regular images [28, 5], we ablated (i.e. removed) some\nof the repeated elements to see if the DNN conﬁdence score\nFigure 9. Images from the same evolutionary run that fool closely\nrelated classes are similar. Shown are the top images evolution\ngenerated for three classes that belong to the “lizard” parent class,\nand for three classes that belong to “toy dog” parent class. The top\nand bottom rows show images from independent runs of evolution.\nfor that image drops. Psychologists use the same ablation\ntechnique to learn which image features humans use to rec-\nognize objects [4]. In many images, ablating extra copies of\nthe repeated element did lead to a performance drop, albeit\na small one (Fig 10), meaning that the extra copies make\nthe DNN more conﬁdent that the image belongs to the tar-\nget class. This result is in line with a previous paper [26]\nthat produced images to maximize DNN conﬁdence scores\n(discussed below in Section 3.9), which also saw the emer-\ngence of features (e.g. a fox’s ears) repeated throughout an\nimage. These results suggest that DNNs tend to learn low-\nand middle-level features rather than the global structure of\nobjects. If DNNs were properly learning global structure,\nimages should receive lower DNN conﬁdence scores if they\ncontain repetitions of object subcomponents that rarely ap-\npear in natural images, such as many pairs of fox ears or\nendless remote buttons (Fig. 1).\nFigure 10. Before: CPPN-encoded images with repeated patterns.\nAfter: Manually removing repeated elements suggests that such\nrepetition increases conﬁdence scores.\nThe low-performing band of classes in Fig. 7 (class num-\nbers 157-286) are dogs and cats, which are overrepresented\nin the ImageNet dataset (i.e. there are many more classes of\ncats than classes of cars). One possible explanation for why\nimages in this band receive low conﬁdence scores is that the\nnetwork is tuned to identify many speciﬁc types of dogs and\ncats. Therefore, it ends up having more units dedicated to\nthis image type than others. In other words, the size of the\ndataset of cats and dogs it has been trained on is larger than\nfor other categories, meaning it is less overﬁt, and thus more\ndifﬁcult to fool. If true, this explanation means that larger\ndatasets are a way to ameliorate the problem of DNNs be-\ning easily fooled. An alternate, though not mutually exclu-\nsive, explanation is that, because there are more cat and dog\nclasses, the EA had difﬁculty ﬁnding an image that scores\nhigh in a speciﬁc dog category (e.g. Japanese spaniel), but\nlow in any other related categories (e.g. Blenheim spaniel),\nwhich is necessary to produce a high conﬁdence given that\nthe ﬁnal DNN layer is softmax. This explanation suggests\nthat datasets with more classes can help ameliorate fooling.\n3.5. Images that fool one DNN generalize to others\nThe results of the previous section suggest that there are\ndiscriminative features of a class of images that DNNs learn\nand evolution exploits. One question is whether different\nDNNs learn the same features for each class, or whether\neach trained DNN learns different discriminative features.\nOne way to shed light on that question is to see if im-\nages that fool one DNN also fool another. To test that, we\nevolved CPPN-encoded images with one DNN (DNNA)\nand then input these images to another DNN (DNNB). We\ntested two cases: (1) DNNA and DNNB have identical ar-\nchitectures and training, and differ only in their randomized\ninitializations; and (2) DNNA and DNNB have different\nDNN architectures, but are trained on the same dataset. We\nperformed this test for both MNIST and ImageNet DNNs.\nImages were evolved that are given ≥99.99% conﬁ-\ndence scores by both DNNA and DNNB. Thus, some\ngeneral properties of the DNNs are exploited by the CPPN-\nencoded EA. However, there are also images speciﬁcally\nﬁne-tuned to score high on DNNA, but not on DNNB.\nSee the supplementary material for more detail and data.\n3.6. Training networks to recognize fooling images\nOne might respond to the result that DNNs are eas-\nily fooled by saying that, while DNNs are easily fooled\nwhen images are optimized to produce high DNN conﬁ-\ndence scores, the problem could be solved by simply chang-\ning the training regimen to include negative examples. In\nother words, a network could be retrained and told that the\nimages that previously fooled it should not be considered\nmembers of any of the original classes, but instead should\nbe recognized as a new “fooling images” class.\nWe tested that hypothesis with CPPN-encoded images\non both MNIST and ImageNet DNNs. The process is as\nfollows: We train DNN1 on a dataset (e.g. ImageNet),\nthen evolve CPPN images that produce a high conﬁdence\nscore for DNN1 for the n classes in the dataset, then we\ntake those images and add them to the dataset in a new class\nn + 1; then we train DNN2 on this enlarged “+1” dataset;\n(optional) we repeat the process, but put the images that\nevolved for DNN2 in the n + 1 category (a n + 2 cate-\ngory is unnecessary because any images that fool a DNN\nare “fooling images” and can thus go in the n+1 category).\nSpeciﬁcally, to represent different types of images, each it-\neration we add to this n + 1 category m images randomly\nsampled from both the ﬁrst and last generations of multiple\nruns of evolution that produce high conﬁdence images for\nDNNi. Each evolution run on MNIST or ImageNet pro-\nduces 20 and 2000 images respectively, with half from the\nﬁrst generation and half from the last. Error-rates for trained\nDNNi are similar to DNN1 (supplementary material).\n3.7. Training MNIST DNNs with fooling images\nTo make the n+1 class have the same number of images\nas other MNIST classes, the ﬁrst iteration we add 6000 im-\nages to the training set (taken from 300 evolutionary runs).\nFor each additional iteration, we add 1000 new images to\nthe training set.\nThe immunity of LeNet is not boosted\nby retraining it with fooling images as negative examples.\nEvolution still produces many unrecognizable images for\nDNN2 with conﬁdence scores of 99.99%. Moreover, re-\npeating the process for 15 iterations does not help (Fig. 11),\neven though DNN15’s overrepresented 11th “fooling im-\nage class” contains 25% of the training set images.\n3.8. Training ImageNet DNNs with fooling images\nThe original ILSVRC 2012 training dataset was ex-\ntended with a 1001st class, to which we added 9000 images\n0\n1\n1\n2\n3\n4\n5\n6\n7\n8\n9\nMedian confidence\n99.99\n2\n97.42\n3\n99.83\n4\n72.52\n5\n97.55\n6\n99.68\n7\n76.13\n8\n99.96\n9\n99.51\n10\n99.48\n11\n98.62\n12\n99.97\n13\n99.93\n14\n99.15\n15\n99.15\nFigure 11. Training MNIST DNNi with images that fooled\nMNIST DNN1 through DNNi−1 does not prevent evolution\nfrom ﬁnding new fooling images for DNNi. Columns are dig-\nits. Rows are DNNi for i = 1...15. Each row shows the 10\nﬁnal, evolved images from one randomly selected run (of 30) per\niteration. Medians are taken from images from all 30 runs.\nthat fooled DNN1. That 7-fold increase over the 1300 im-\nages per ImageNet class is to emphasize the fooling images\nin training. Without this imbalance, training with negative\nexamples did not prevent fooling; MNIST retraining did not\nbeneﬁt from over representing the fooling image class.\nContrary to the result in the previous section, for Ima-\ngeNet models, evolution was less able to evolve high conﬁ-\ndence images for DNN2 than DNN1. The median conﬁ-\ndence score signiﬁcantly decreased from 88.1% for DNN1\nto 11.7% for DNN2 (Fig. 12, p < 0.0001 via Mann-\nWhitney U test). We suspect that ImageNet DNNs were\nbetter inoculated against being fooled than MNIST DNNs\nwhen trained with negative examples because it is easier to\nlearn to tell CPPN images apart from natural images than it\nis to tell CPPN images from MNIST digits.\nFigure 12. Training a new ImageNet DNN (DNN2) with images\nthat fooled a previous DNN (DNN1) makes it signiﬁcantly more\ndifﬁcult for evolution to produce high conﬁdence images.\nTo see whether this DNN2 had learned features speciﬁc\nto the CPPN images that fooled DNN1, or whether DNN2\nlearned features general to all CPPN images, even recog-\nnizable ones, we input recognizable CPPN images from\nPicbreeder.org to DNN2. DNN2 correctly labeled 45 of\n70 (64%, top-1 prediction) PicBreeder images as CPPN im-\nages, despite having never seen CPPN images like them be-\nfore. The retrained model thus learned features generic to\nCPPN images, helping to explain why producing new im-\nages that fool DNN2 is more difﬁcult.\n3.9. Producing fooling images via gradient ascent\nA different way to produce high conﬁdence, yet mostly\nunrecognizable images is by using gradient ascent in pixel\nspace [11, 26, 30]. We calculate the gradient of the posterior\nprobability for a speciﬁc class — here, a softmax output unit\nof the DNN — with respect to the input image using back-\nprop, and then we follow the gradient to increase a chosen\nunit’s activation. This technique follows [26], but whereas\nwe aim to ﬁnd images that produce high conﬁdence classi-\nﬁcations, they sought visually recognizable “class appear-\nance models.” By employing L2-regularization, they pro-\nduced images with some recognizable features of classes\n(e.g. dog faces, fox ears, and cup handles). However, their\nconﬁdence values are not reported, so to determine the de-\ngree to which DNNs are fooled by these backpropagated\nimages, we replicated their work (with some minor changes,\nsee supplementary material) and found that images can be\nmade that are also classiﬁed by DNNs with 99.99% conﬁ-\ndence, despite them being mostly unrecognizable (Fig. 13).\nThese optimized images reveal a third method of fooling\nDNNs that produces qualitatively different examples than\nthe two evolutionary methods in this paper.\nFigure 13. Images found by maximizing the softmax output for\nclasses via gradient ascent [11, 26]. Optimization begins at the Im-\nageNet mean (plus small Gaussian noise to break symmetry) and\ncontinues until the DNN conﬁdence for the target class reaches\n99.99%. Images are shown with the mean subtracted. Adding reg-\nularization makes images more recognizable but results in slightly\nlower conﬁdence scores (see supplementary material).\n4. Discussion\nOur experiments could have led to very different results.\nOne might have expected evolution to produce very similar,\nhigh conﬁdence images for all classes, given that [30] re-\ncently showed that imperceptible changes to an image can\ncause a DNN to switch from classifying it as class A to class\nB (Fig. 14). Instead, evolution produced a tremendous di-\nversity of images (Figs. 1, 8, 10, 15). Alternately, one might\nhave predicted that evolution would produce recognizable\nimages for each class given that, at least with the CPPN\nencoding, recognizable images have been evolved (Fig. 3).\nWe note that we did not set out to produce unrecognizable\nimages that fool DNNs. Instead, we had hoped the resul-\ntant images would be recognizable. A different prediction\ncould have been that evolution would fail to produce high\nconﬁdence scores at all because of local optima. It could\nalso have been the case that unrecognizable images would\nhave been given mostly low conﬁdences across all classes\ninstead of a very high conﬁdence for one class.\nIn fact, none of these outcomes resulted. Instead, evolu-\ntion produced high-conﬁdence, yet unrecognizable images.\nWhy? Our leading hypothesis centers around the difference\nbetween discriminative models and generative models. Dis-\nFigure 14. Interpreting our results and related research. (1) [30]\nfound that an imperceptible change to a correctly classiﬁed natural\nimage (blue dot) can result in an image (square) that a DNN classi-\nﬁes as an entirely different class (crossing the decision boundary).\nThe difference between the original image and the modiﬁed one\nis imperceptible to human eyes. (2) It is possible to ﬁnd high-\nconﬁdence images (pentagon) using our directly encoded EA or\ngradient ascent optimization starting from a random or blank im-\nage (I0) [11, 13, 26]. These images have blurry, discriminative\nfeatures of the represented classes, but do not look like images in\nthe training set. (3) We found that indirectly encoded EAs can ﬁnd\nhigh-conﬁdence, regular images (triangles) that have discrimina-\ntive features for a class, but are still far from the training set.\ncriminative models — or models that learn p(y|X) for a\nlabel vector y and input example X — like the models in\nthis study, create decision boundaries that partition data into\nclassiﬁcation regions. In a high-dimensional input space,\nthe area a discriminative model allocates to a class may be\nmuch larger than the area occupied by training examples for\nthat class (see lower 80% of Fig. 14). Synthetic images far\nfrom the decision boundary and deep into a classiﬁcation re-\ngion may produce high conﬁdence predictions even though\nthey are far from the natural images in the class. This per-\nspective is conﬁrmed and further investigated by a related\nstudy [13] that shows large regions of high conﬁdence ex-\nist in certain discriminative models due to a combination of\ntheir locally linear nature and high-dimensional input space.\nIn contrast, a generative model that represents the com-\nplete joint density p(y, X) would enable computing not\nonly p(y|X), but also p(X). Such models may be more dif-\nﬁcult to fool because fooling images could be recognized by\ntheir low marginal probability p(X), and the DNN’s conﬁ-\ndence in a label prediction for such images could be dis-\ncounted when p(X) is low. Unfortunately, current genera-\ntive models do not scale well [3] to the high-dimensionality\nof datasets like ImageNet, so testing to what extent they\nmay be fooled must wait for advances in generative models.\nIn this paper we focus on the fact that there exist images\nthat DNNs declare with near-certainty to be of a class, but\nare unrecognizable as such. However, it is also interesting\nthat some generated images are recognizable as members of\ntheir target class once the class label is known. Fig. 15 jux-\ntaposes examples with natural images from the target class.\nBaseball\nMatchstick\nPing-pong ball\nSunglasses\nFigure 15. Some evolved images do resemble their target class. In\neach pair, an evolved, CPPN-encoded image (left) is shown with a\ntraining set image from the target class (right).\nOther examples include the chain-link fence, computer key-\nboard, digital clock, bagel, strawberry, ski mask, spotlight,\nand monarch butterﬂy of Fig. 8. To test whether these im-\nages might be accepted as art, we submitted them to a se-\nlective art competition at the University of Wyoming Art\nMuseum, where they were accepted and displayed (supple-\nmentary material). A companion paper explores how these\nsuccesses suggest combining DNNs with evolutionary algo-\nrithms to make open-ended, creative search algorithms [23].\nThe CPPN EA presented can also be considered a novel\ntechnique to visualize the features learned by DNNs. The\ndiversity of patterns generated for the same class over dif-\nferent runs (Fig. 9) indicates the diversity of features learned\nfor that class.\nSuch feature-visualization tools help re-\nsearchers understand what DNNs have learned and whether\nfeatures can be transferred to other tasks [32].\nOne interesting implication of the fact that DNNs are\neasily fooled is that such false positives could be exploited\nwherever DNNs are deployed for recognizing images or\nother types of data. For example, one can imagine a security\ncamera that relies on face or voice recognition being com-\npromised. Swapping white-noise for a face, ﬁngerprints, or\na voice might be especially pernicious since other humans\nnearby might not recognize that someone is attempting to\ncompromise the system.\nAnother area of concern could\nbe image-based search engine rankings: background pat-\nterns that a visitor does not notice could fool a DNN-driven\nsearch engine into thinking a page is about an altogether\ndifferent topic. The fact that DNNs are increasingly used in\na wide variety of industries, including safety-critical ones\nsuch as driverless cars, raises the possibility of costly ex-\nploits via techniques that generate fooling images.\n5. Conclusion\nWe have demonstrated that discriminative DNN models\nare easily fooled in that they classify many unrecognizable\nimages with near-certainty as members of a recognizable\nclass. Two different ways of encoding evolutionary algo-\nrithms produce two qualitatively different types of unrec-\nognizable “fooling images”, and gradient ascent produces\na third. That DNNs see these objects as near-perfect ex-\namples of recognizable images sheds light on remaining\ndifferences between the way DNNs and humans recognize\nobjects, raising questions about the true generalization ca-\npabilities of DNNs and the potential for costly exploits of\nsolutions that use DNNs.\nAcknowledgments\nThe authors would like to thank Hod Lipson for help-\nful discussions and the NASA Space Technology Research\nFellowship (JY) for funding. We also thank Joost Huizinga,\nChristopher Stanton, and Jingyu Li for helpful feedback.\nReferences\n[1] J. E. Auerbach.\nAutomated evolution of interesting im-\nages.\nIn Artiﬁcial Life 13, number EPFL-CONF-191282.\nMIT Press, 2012. 3\n[2] Y. Bengio. Learning deep architectures for ai. Foundations\nand trends R⃝in Machine Learning, 2(1):1–127, 2009. 1\n[3] Y. Bengio, E. Thibodeau-Laufer, G. Alain, and J. Yosinski.\nDeep generative stochastic networks trainable by backprop.\nIn Proceedings of the 30th International Conference on Ma-\nchine Learning, 2014. 8\n[4] I. Biederman.\nVisual object recognition, volume 2.\nMIT\npress Cambridge, 1995. 5\n[5] J. Clune, K. Stanley, R. Pennock, and C. Ofria. On the per-\nformance of indirect encoding across the continuum of reg-\nularity. IEEE Transactions on Evolutionary Computation,\n15(4):346–367, 2011. 3, 5\n[6] A. Cully, J. Clune, and J.-B. Mouret. Robots that can adapt\nlike natural animals. arXiv preprint arXiv:1407.3501, 2014.\n2\n[7] G. E. Dahl, D. Yu, L. Deng, and A. Acero.\nContext-\ndependent pre-trained deep neural networks for large-\nvocabulary speech recognition.\nAudio, Speech, and Lan-\nguage Processing, IEEE Transactions on, 20(1):30–42,\n2012. 1\n[8] K. Deb. Multi-objective optimization using evolutionary al-\ngorithms, volume 16. John Wiley & Sons, 2001. 3\n[9] J. Deng, A. Berg, S. Satheesh, H. Su, A. Khosla, and L. Fei-\nFei.\nImagenet large scale visual recognition competition\n2012 (ilsvrc2012), 2012. 4\n[10] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-\nFei. Imagenet: A large-scale hierarchical image database.\nIn Computer Vision and Pattern Recognition, 2009. CVPR\n2009. IEEE Conference on, pages 248–255. IEEE, 2009. 2\n[11] D. Erhan, Y. Bengio, A. Courville, and P. Vincent. Visual-\nizing higher-layer features of a deep network. Dept. IRO,\nUniversit´e de Montr´eal, Tech. Rep, 2009. 7, 8\n[12] D. Floreano and C. Mattiussi. Bio-inspired artiﬁcial intel-\nligence: theories, methods, and technologies. MIT press,\n2008. 2\n[13] I. J. Goodfellow, J. Shlens, and C. Szegedy.\nExplain-\ning and harnessing adversarial examples.\narXiv preprint\narXiv:1412.6572, Dec. 2014. 8\n[14] G. E. Hinton.\nLearning multiple layers of representation.\nTrends in cognitive sciences, 11(10):428–434, 2007. 1\n[15] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Gir-\nshick, S. Guadarrama, and T. Darrell.\nCaffe: Convolu-\ntional architecture for fast feature embedding. arXiv preprint\narXiv:1408.5093, 2014. 2\n[16] A. Krizhevsky, I. Sutskever, and G. E. Hinton.\nImagenet\nclassiﬁcation with deep convolutional neural networks. In\nAdvances in neural information processing systems, pages\n1097–1105, 2012. 1, 2, 4\n[17] Q. V. Le, W. Y. Zou, S. Y. Yeung, and A. Y. Ng. Learn-\ning hierarchical invariant spatio-temporal features for action\nrecognition with independent subspace analysis.\nIn Com-\nputer Vision and Pattern Recognition (CVPR), 2011 IEEE\nConference on, pages 3361–3368. IEEE, 2011. 1\n[18] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-\nbased learning applied to document recognition. Proceed-\nings of the IEEE, 86(11):2278–2324, 1998. 2\n[19] Y. LeCun and C. Cortes. The mnist database of handwritten\ndigits, 1998. 2\n[20] H. Lipson. Principles of modularity, regularity, and hierar-\nchy for scalable systems. Journal of Biological Physics and\nChemistry, 7(4):125, 2007. 3\n[21] J.-B. Mouret and S. Doncieux. Sferes v2: Evolvin’in the\nmulti-core world. In Evolutionary Computation (CEC), 2010\nIEEE Congress on, pages 4079–4086. IEEE, 2010. 3\n[22] V. Nair and G. E. Hinton.\nRectiﬁed linear units improve\nrestricted boltzmann machines. In Proceedings of the 27th\nInternational Conference on Machine Learning (ICML-10),\npages 807–814, 2010. 2\n[23] A. Nguyen, J. Yosinski, and J. Clune. Introducing the inno-\nvation engine: Automated creativity and improved stochastic\noptimization via deep learning. In Proceedings of the Ge-\nnetic and Evolutionary Computation Conference, 2015. 8\n[24] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh,\nS. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein,\net al.\nImagenet large scale visual recognition challenge.\narXiv preprint arXiv:1409.0575, 2014. 2\n[25] J. Secretan, N. Beato, D. B. D Ambrosio, A. Rodriguez,\nA. Campbell, and K. O. Stanley. Picbreeder: evolving pic-\ntures collaboratively online. In Proceedings of the SIGCHI\nConference on Human Factors in Computing Systems, pages\n1759–1768. ACM, 2008. 3\n[26] K. Simonyan, A. Vedaldi, and A. Zisserman. Deep inside\nconvolutional networks:\nVisualising image classiﬁcation\nmodels and saliency maps. arXiv preprint arXiv:1312.6034,\n2013. 5, 7, 8\n[27] K. Stanley and R. Miikkulainen. Evolving neural networks\nthrough augmenting topologies. Evolutionary computation,\n10(2):99–127, 2002. 3\n[28] K. O. Stanley. Compositional pattern producing networks:\nA novel abstraction of development. Genetic programming\nand evolvable machines, 8(2):131–162, 2007. 3, 5\n[29] K. O. Stanley and R. Miikkulainen. A taxonomy for artiﬁcial\nembryogeny. Artiﬁcial Life, 9(2):93–130, 2003. 3\n[30] C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan,\nI. Goodfellow, and R. Fergus. Intriguing properties of neural\nnetworks. arXiv preprint arXiv:1312.6199, 2013. 1, 5, 7, 8\n[31] Y. Taigman, M. Yang, M. Ranzato, and L. Wolf. Deepface:\nClosing the gap to human-level performance in face veriﬁca-\ntion. In Computer Vision and Pattern Recognition (CVPR),\n2014 IEEE Conference on, pages 1701–1708. IEEE, 2014. 1\n[32] J. Yosinski, J. Clune, Y. Bengio, and H. Lipson. How trans-\nferable are features in deep neural networks? In Z. Ghahra-\nmani, M. Welling, C. Cortes, N. Lawrence, and K. Wein-\nberger, editors, Advances in Neural Information Processing\nSystems 27, pages 3320–3328. Curran Associates, Inc., Dec.\n2014. 8\nSupplementary Material for\nDeep Neural Networks are Easily Fooled:\nHigh Conﬁdence Predictions for Unrecognizable Images\nA. Images that fool one DNN generalize to fool\nother DNNs\nAs we wrote in the paper: “One question is whether\ndifferent DNNs learn the same features for each class, or\nwhether each trained DNN learns different discriminative\nfeatures. One way to shed light on that question is to see if\nimages that fool one DNN also fool another. To test that, we\nevolved CPPN-encoded images with one DNN (DNNA)\nand then input them to another DNN (DNNB), where\nDNNA and DNNB have identical architectures and train-\ning, and differ only in their randomized initializations. We\nperformed this test for both MNIST and ImageNet DNNs.”\nHere we show the details of this experiment and its results.\nA.1. Generalization across DNNs with the same ar-\nchitecture\nWe performed this test with two MNIST [3] DNNs\n(MNISTA and MNISTB) and two ImageNet [2] DNNs\n(ImageNetA and ImageNetB), where A and B differ only\nin their random initializations, but have the same architec-\nture. 300 images were produced with each MNIST DNN,\nand 1000 images were produced with each ImageNet DNN.\nTaking images evolved to score high on DNNA and\ninputting them to DNNB (and vice versa), we ﬁnd that\nthere are many evolved images that are given the same\ntop-1 prediction label by both DNNA and DNNB (Ta-\nble S1a).\nFurthermore, among those images, many are\ngiven ≥99.99% conﬁdence scores by both DNNA and\nDNNB (Table S1b).\nThus, evolution produces patterns\nthat are generally discriminative of a class to multiple, in-\ndependently trained DNNs. On the other hand, there are\nstill images labeled differently by DNNA and DNNB (Ta-\nble S1a). These images are speciﬁcally ﬁne-tuned to exploit\nthe original DNN. We also ﬁnd ≥92.18% of the images that\nare given the same top-1 prediction label by both networks,\nare given higher conﬁdence score by the original DNN (Ta-\nble S1c).\nFrom the experiment with MNIST DNNs, we observed\nthat images evolved to represent digit classes 9, 6, and 2\nfooled both networks DNNA and DNNB the most. Fur-\nDataset\nImageNet\nMNIST\nDNNA\non\nDNNB\nimages\nDNNB\non\nDNNA\nimages\nDNNA\non\nDNNB\nimages\nDNNB\non\nDNNA\nimages\nTop-1 matches\n62.8\n65.9\n43.3\n48.7\n(a) Average\n64.4\n46.0\nTop-1 matches\nscoring 99%\n5.0\n7.2\n27.3\n27.3\n(b) Average\n6.1\n27.3\nTop-1 matches\nscoring higher\non original DNN\n95.1\n98.0\n88.5\n95.9\n(c) Average\n96.6\n92.2\nTable S1.\nTop-1 matches: The percent of images that are given the same top-\n1 label by both DNNA and DNNB.\nTop-1 matches scoring 99%: The percent of images for which\nboth DNNA and DNNB believe the top-1 predicted label to be\nthe same and the two conﬁdence scores given are both ≥99%.\nTop-1 matches scoring higher: Of the images that are given the\nsame top-1 label by both DNNA and DNNB, the percent that\nare given a higher conﬁdence score by the original DNN than by\nthe other, testing DNN.\nthermore, these images revealed distinctive patterns (Fig-\nure S1).\n9\n6\n2\nFigure S1. CPPN-encoded, evolved images which are given ≥\n99% conﬁdence scores by both DNNA and DNNB to represent\ndigits 9, 6, and 2. Each column represents an image produced\nby an independent run of evolution, yet evolution converges on a\nsimilar design, which fools not just the DNN it evolved with, but\nanother, independently trained DNN as well.\n1\narXiv:1412.1897v4  [cs.CV]  2 Apr 2015\nA.2. Generalization across DNNs that have different\narchitectures\nHere we test whether images that fool a DNN with one\narchitecture also fool another DNN with a different archi-\ntecture. We performed this test with two well-known Ima-\ngeNet DNN architectures: AlexNet [2] and GoogLeNet [5],\nboth of which are provided by Caffe [1] and trained on the\nsame ILSVRC 2012 dataset [4]. GoogLeNet has a top-1\nerror rate of 31.3%.\n1000 images were produced with each ImageNet DNN.\nWe found that 20.7% of images evolved for GoogLeNet are\nalso given the same top-1 label by AlexNet (and 17.3% vice\nversa). Thus, many fooling examples are not ﬁt precisely to\na particular network, but generalize across different DNN\narchitectures.\nB. Does using an ensemble of networks instead\nof just one prevent fooling?\nWe also tested whether requiring an image to fool an en-\nsemble of multiple networks makes it impossible to produce\nfooling images. We tested an extreme case where each net-\nwork in the ensemble has a different architecture. Specif-\nically, we tested with an ensemble of 3 different DNN ar-\nchitectures: CaffeNet, AlexNet and GoogLeNet. CaffeNet\n[1] performs similarly to AlexNet [2], but has a slightly dif-\nferent architecture. The ﬁnal conﬁdence score given to an\nimage is calculated as the mean of the three scores given by\nthese three different DNNs. After only 4000 generations,\nevolution was still able to produce fooling images for 231\nof the 1000 classes with ≥90% conﬁdence. Moreover, the\nmedian is also high at 65.2% and the max is 100%.\nC. Training networks to recognize fooling im-\nages to prevent fooling\nAs we wrote in the paper: “One might respond to the\nresult that DNNs are easily fooled by saying that, while\nDNNs are easily fooled when images are optimized to pro-\nduce high DNN conﬁdence scores, the problem could be\nsolved by simply changing the training regimen to include\nnegative examples. In other words, a network could be re-\ntrained and told that the images that previously fooled it\nshould not be considered members of any of the original\nclasses, but instead should be recognized as a new fooling\nimages class.”\nWe tested this hypothesis with CPPN-encoded images on\nboth MNIST and ImageNet DNNs. The process is as fol-\nlows: We train DNN1 on a dataset (e.g. ImageNet), then\nevolve CPPN images that are given a high conﬁdence score\nby DNN1 for the n classes in the dataset, then we take\nthose images and add them to the dataset in a new class\nn + 1; then we train DNN2 on this enlarged “+1” dataset;\n(optional) we repeat the process, but put the images that\nevolved for DNN2 in the n + 1 category (a n + 2 cate-\ngory is unnecessary because any images that fool a DNN\nare “fooling images” and can thus go in the n+1 category).\nSpeciﬁcally, to represent different types of images, each\niteration we add to this n + 1 category m images. These\nimages are randomly sampled from both the ﬁrst and last\ngenerations of multiple runs of evolution that produce high\nconﬁdence images for DNNi. Each run of evolution on\nMNIST or ImageNet produces 20 or 2000 images, respec-\ntively, with half from the ﬁrst generation and half from the\nlast. As in the original experiments evolving images for\nMNIST, each evolution run on MNIST or ImageNet lasts\nfor 200 or 5000 generations, respectively. These generation\nnumbers were chosen from the previous experiments. The\nspeciﬁc training details are presented in the following sec-\ntions.\nC.1. Training MNIST DNNs with fooling images\nTo make the n+1 class have the same number of images\nas other MNIST classes, the ﬁrst iteration we add 6000 and\n1000 images to the training and validation sets, respectively.\nFor each additional iteration, we add 1000 and 100 new im-\nages to the training and validation sets (Table S2).\nMNIST DNNs (DNN1 −DNN15) were trained on im-\nages of size 28 × 28, using stochastic gradient descent\n(SGD) with a momentum of 0.9. Each iteration of SGD\nused a batch size of 64, and a multiplicative weight decay\nof 0.0005. The learning rate started at 0.01, and reduced\nevery iteration by an inverse learning rate policy (deﬁned\nin Caffe [1]) with power = 0.75 and gamma = 0.0001.\nDNN2 −DNN15 obtained similar error rates to the 0.94%\nof DNN1 trained on the original MNIST (Table S2).\nSince evolution still produced many unrecognizable im-\nages for DNN2 with conﬁdence scores of 99.99%, we re-\npeated the process for 15 iterations (Table S2). However,\nthe retraining does not help, even though DNN15’s over-\nrepresented 11th “fooling image class” contains ∼25% of\nthe training set images.\nC.2. Training ImageNet DNNs with fooling images\nThe original ILSVRC 2012 training dataset was ex-\ntended with a 1001st class, to which we added 9000 im-\nages and 2000 images that fooled DNN1 to the training\nand validation sets, respectively. That ∼7-fold increase over\nthe ∼1300 training images per ImageNet class is to empha-\nsize the fooling images in training. Without this imbalance,\ntraining with negative examples did not prevent fooling; re-\ntrained MNIST DNNs did not beneﬁt from this strategy of\nover representing the fooling image class (data not shown).\nThe images produced by DNN1 are of size 256 × 256\nbut cropped to 227 × 227 for training. DNN2 was trained\nusing SGD with a momentum of 0.9. Each iteration of SGD\ni\nError\nMNIST Error\nTrain\nVal\nScore\n1\n0.94\n0.94\n60000\n10000\n99.99\n2\n1.02\n0.87\n66000\n11000\n97.42\n3\n0.92\n0.87\n67000\n11100\n99.83\n4\n0.89\n0.83\n68000\n11200\n72.52\n5\n0.90\n0.96\n69000\n11300\n97.55\n6\n0.89\n0.99\n70000\n11400\n99.68\n7\n0.86\n0.98\n71000\n11500\n76.13\n8\n0.91\n1.01\n72000\n11600\n99.96\n9\n0.90\n0.86\n73000\n11700\n99.51\n10\n0.84\n0.94\n74000\n11800\n99.48\n11\n0.80\n0.93\n75000\n11900\n98.62\n12\n0.82\n0.98\n76000\n12000\n99.97\n13\n0.75\n0.90\n77000\n12100\n99.93\n14\n0.80\n0.96\n78000\n12200\n99.15\n15\n0.79\n0.95\n79000\n12300\n99.15\nTable S2. Details of 15 training iterations of MNIST DNNs.\nDNN1 is the model trained on the original MNIST dataset with-\nout CPPN images. DNN2 −DNN15 are models trained on the\nextended dataset with CPPN images added.\nError: The error (%) on the validation set (with CPPN images\nadded).\nMNIST Error: The error (%) on the original MNIST validation set\n(10,000 images).\nTrain: The number of images in the training set.\nVal: The number of images in the validation set.\nScore: The median conﬁdence scores (%) of images produced by\nevolution for that iteration. These numbers are also provided in\nthe paper.\nused a batch size of 256, and a multiplicative weight decay\nof 0.0005. The learning rate started at 0.01, and dropped\nby a factor of 10 every 100,000 iterations. Training stopped\nafter 450,000 iterations. The whole training procedure took\n∼10 days on an Nvidia K20 GPU.\nTraining DNN2 on ImageNet yielded a top-1 error rate\nof 41.0%, slightly better than the 42.6% for DNN1: we\nhypothesize the improved error rate is because the 1001st\nCPPN image class is easier than the other 1000 classes, be-\ncause it represents a different style of images, making it\neasier to classify them. Supporting this hypothesis is the\nfact that DNN2 obtained a top-1 error rate of 42.6% when\ntested on the original ILSVRC 2012 validation set.\nIn contrast to the result in the previous section, for Ima-\ngeNet models, evolution was less able to evolve high conﬁ-\ndence images for DNN2 compared to the high conﬁdences\nevolution produced for DNN1.\nThe median conﬁdence\nscore signiﬁcantly decreased from 88.1% for DNN1 to\n11.7% for DNN2 (p < 0.0001 via Mann-Whitney U test).\nD. Evolving regular images to match MNIST\nAs we wrote in the paper: “Because CPPN encodings\ncan evolve recognizable images, we tested whether this\nmore capable, regular encoding might produce more rec-\nognizable images than the irregular white-noise static of the\ndirect encoding. The result, while containing more strokes\nand other regularities, still led to LeNet labeling unrec-\nognizable images as digits with 99.99% conﬁdence after\nonly a few generations. By 200 generations, median con-\nﬁdence is 99.99%.”. Here we show 10 images ×50 runs\n= 500 images produced by the CPPN-encoded EA that an\nMNIST DNN believes with 99.99% to be handwritten digits\n(Fig. S4).\nLooking at these images produced by 50 independent\nruns of evolution, one can observe that images classiﬁed\nas a 1 tend to have vertical bars. Images classiﬁed as a 2\ntend to have a horizontal bar in the lower half of the image.\nMoreover, since an 8 can be drawn by mirroring a 3 hori-\nzontally, the DNN may have learned some common features\nfrom these two classes from the training set. Evolution re-\npeatedly produces similar patterns for class 3 and class 8.\nE. Gradient ascent with regularization\nIn the paper we showed images produced by direct gra-\ndient ascent to maximize the posterior probability (softmax\noutput) for 20 example classes. Directly optimizing this ob-\njective quickly produces conﬁdence over 99.99% for un-\nrecognizable images. By adding different types of regu-\nlarization, we can also produce more recognizable images.\nWe tried three types of regularization, highlighted in the\nFigs. S5, S6, and S7.\nFig. S5 shows L2-regularization, implemented as a\nweight decay each step. At each step of the optimization,\nthe current mean-subtracted image X is multiplied by a\nconstant 1 −γ for small γ. Fig. S5 shows γ = 0.01.\nFig. S6 shows weight decay (now with γ = 0.001) plus\ntwo other types of regularization. The ﬁrst additional reg-\nularization is a small blurring operator applied each step to\nbias the search toward images with less high frequency in-\nformation and more low frequency information. This was\nimplemented via a Gaussian blur with radius 0.3 after ev-\nery gradient step. The second additional regularization was\na pseudo-L1-regularization in which the (R, G, B) pixels\nwith norms lower than the 20th percentile were set to 0.\nThis tended to produce slightly sparser images.\nFinally, Fig. S7 shows a lower learning rate with the\nsame weight decay and slightly more aggressive blurring.\nBecause the operations of weight decay and blurring do not\ndepend on the learning rate, this produces an objective con-\ntaining far more regularization. As a result, many of the\nclasses never achieve 99%, but the visualizations are of a\ndifferent quality and, in some cases, more clear.\nAll images generated in this manner are optimized by\nstarting at the ImageNet mean plus a small amount of Gaus-\nsian noise to break symmetry and then following the gradi-\nent. The noise has a standard deviation of 1/255 along each\ndimension, where dimensions have been scaled to fall into\nthe range [0, 1]. Because of this random initialization, the\nﬁnal image produced depends on the random draw of Gaus-\nsian noise. Fig. S8 and Fig. S9 show the variety of images\nthat may be produced by taking different random draws of\nthis initial noise.\nF. Conﬁdence scores of real ImageNet images\nThe optimization methods presented can generate un-\nrecognizable images that are given high conﬁdence scores.\nHowever, to ﬁnd out if these high scores for fooling images\nare similar to the conﬁdence scores given by DNNs for the\nnatural images they were trained to classify, we evaluate the\nentire ImageNet validation set with the ImageNet DNN [2].\nAcross 50,000 validation images, the median conﬁdence\nscore is 60.3%. Across the cases when images are classi-\nﬁed correctly (i.e., the top-1 label matches the ground truth\nlabel), the DNN gives a median conﬁdence score of 86.7%.\nOn the contrary, when the top-1 prediction label does not\nmatch the ground truth, the images are given only 33.7%\nmedian conﬁdence. Thus, the median conﬁdence score of\n88.11% of synthetic images that match ImageNet is compa-\nrable to that of real images.\nG. Can the fooling images be considered art?\nTo test the hypothesis that the CPPN fooling images\ncould actually be considered art, we submitted a selection of\nthem to a selective art contest: the “University of Wyoming\n40th Annual Juried Student Exhibition”, which only ac-\ncepted 35.5% of the submissions. Not only were the images\naccepted, but they were also amongst the 21.3% of submis-\nsions to be given an award. The work was then displayed at\nthe University of Wyoming Art Museum (Fig. S2, S3). The\nsubmitted image is available at http://evolvingai.\norg/fooling.\nFigure S2. A selection of fooling images were accepted as art in\na selective art competition. They were then displayed alongside\nhuman-made art at a museum.\nFigure S3. Museum visitors view a montage of CPPN-encoded\nfooling images.\nSupplementary References\n[1] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Gir-\nshick, S. Guadarrama, and T. Darrell.\nCaffe:\nConvolu-\ntional architecture for fast feature embedding. arXiv preprint\narXiv:1408.5093, 2014. 2\n[2] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet clas-\nsiﬁcation with deep convolutional neural networks.\nIn Ad-\nvances in neural information processing systems, pages 1097–\n1105, 2012. 1, 2, 4\n[3] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-\nbased learning applied to document recognition. Proceedings\nof the IEEE, 86(11):2278–2324, 1998. 1\n[4] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma,\nZ. Huang, A. Karpathy, A. Khosla, M. Bernstein, et al. Ima-\ngenet large scale visual recognition challenge. arXiv preprint\narXiv:1409.0575, 2014. 2\n[5] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov,\nD. Erhan, V. Vanhoucke, and A. Rabinovich. Going deeper\nwith convolutions. arXiv preprint arXiv:1409.4842, 2014. 2\n0\n1\n1\n2\n3\n4\n5\n6\n7\n8\n9\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n0\n26\n1\n2\n3\n4\n5\n6\n7\n8\n9\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\nFigure S4.\n50 independent runs of evolution produced images that an MNIST DNN believes with 99.99% to be handwritten digits.\nColumns are digits. In each row are the ﬁnal (best) images evolved for each class during that run.\nFigure S5. Images found by directly maximizing an objective function consisting of the posterior probability (softmax output) added to\na regularization term, here L2-regularization. Optimization begins at the ImageNet mean plus small Gaussian noise to break symmetry.\nWhen regularization is added, conﬁdences are generally lower than 99.99% because the objective contains terms other than conﬁdence.\nHere, the average is 98.591%. For clarity, images are shown with the mean subtracted.\nFigure S6. As in Fig. S5, but with blurring and pseudo-L1-regularization, which is accomplished by setting the pixels with lowest norm to\nzero throughout the optimization.\nFigure S7. As in Fig. S5, but with slightly more aggressive blurring than in Fig. S6.\nFigure S8. Multiple images produced for each class in the manner of Fig. S5. Each column shows the result of a different local optimum,\nwhich was reached by starting at the ImageNet mean and adding different draws of small Gaussian noise.\nFigure S9. Multiple images produced for each class in the manner of Fig. S7. Each column shows the result of a different local optimum,\nwhich was reached by starting at the ImageNet mean and adding different draws of small Gaussian noise.\n",
        "sentence": " Significant visualization techniques have been developed to understand the deep image representations on trained networks [5, 6, 7, 8, 9, 10, 11]."
    },
    {
        "title": "Texture synthesis and the controlled generation of natural stimuli using convolutional neural networks",
        "author": [
            "L.A. Gatys",
            "A.S. Ecker",
            "M. Bethge"
        ],
        "venue": "arXiv preprint arXiv:1505.07376,",
        "citeRegEx": "12",
        "shortCiteRegEx": "12",
        "year": 2015,
        "abstract": "",
        "full_text": "",
        "sentence": " [8, 12] define a squared loss on the correlations between feature maps of some layers and synthesize natural textures of high perceptual quality using the pretrained CNN called VGG [3]."
    },
    {
        "title": "A neural algorithm of artistic style",
        "author": [
            "Leon A Gatys",
            "Alexander S Ecker",
            "Matthias Bethge"
        ],
        "venue": "arXiv preprint arXiv:1508.06576,",
        "citeRegEx": "13",
        "shortCiteRegEx": "13",
        "year": 2015,
        "abstract": "In fine art, especially painting, humans have mastered the skill to create\nunique visual experiences through composing a complex interplay between the\ncontent and style of an image. Thus far the algorithmic basis of this process\nis unknown and there exists no artificial system with similar capabilities.\nHowever, in other key areas of visual perception such as object and face\nrecognition near-human performance was recently demonstrated by a class of\nbiologically inspired vision models called Deep Neural Networks. Here we\nintroduce an artificial system based on a Deep Neural Network that creates\nartistic images of high perceptual quality. The system uses neural\nrepresentations to separate and recombine content and style of arbitrary\nimages, providing a neural algorithm for the creation of artistic images.\nMoreover, in light of the striking similarities between performance-optimised\nartificial neural networks and biological vision, our work offers a path\nforward to an algorithmic understanding of how humans create and perceive\nartistic imagery.",
        "full_text": "A Neural Algorithm of Artistic Style\nLeon A. Gatys,1,2,3∗Alexander S. Ecker,1,2,4,5 Matthias Bethge1,2,4\n1Werner Reichardt Centre for Integrative Neuroscience\nand Institute of Theoretical Physics, University of T¨ubingen, Germany\n2Bernstein Center for Computational Neuroscience, T¨ubingen, Germany\n3Graduate School for Neural Information Processing, T¨ubingen, Germany\n4Max Planck Institute for Biological Cybernetics, T¨ubingen, Germany\n5Department of Neuroscience, Baylor College of Medicine, Houston, TX, USA\n∗To whom correspondence should be addressed; E-mail: leon.gatys@bethgelab.org\nIn ﬁne art, especially painting, humans have mastered the skill to create unique\nvisual experiences through composing a complex interplay between the con-\ntent and style of an image. Thus far the algorithmic basis of this process is\nunknown and there exists no artiﬁcial system with similar capabilities. How-\never, in other key areas of visual perception such as object and face recognition\nnear-human performance was recently demonstrated by a class of biologically\ninspired vision models called Deep Neural Networks.1,2 Here we introduce an\nartiﬁcial system based on a Deep Neural Network that creates artistic images\nof high perceptual quality. The system uses neural representations to sepa-\nrate and recombine content and style of arbitrary images, providing a neural\nalgorithm for the creation of artistic images. Moreover, in light of the strik-\ning similarities between performance-optimised artiﬁcial neural networks and\nbiological vision,3–7 our work offers a path forward to an algorithmic under-\nstanding of how humans create and perceive artistic imagery.\n1\narXiv:1508.06576v2  [cs.CV]  2 Sep 2015\nThe class of Deep Neural Networks that are most powerful in image processing tasks are\ncalled Convolutional Neural Networks. Convolutional Neural Networks consist of layers of\nsmall computational units that process visual information hierarchically in a feed-forward man-\nner (Fig 1). Each layer of units can be understood as a collection of image ﬁlters, each of which\nextracts a certain feature from the input image. Thus, the output of a given layer consists of\nso-called feature maps: differently ﬁltered versions of the input image.\nWhen Convolutional Neural Networks are trained on object recognition, they develop a\nrepresentation of the image that makes object information increasingly explicit along the pro-\ncessing hierarchy.8 Therefore, along the processing hierarchy of the network, the input image\nis transformed into representations that increasingly care about the actual content of the im-\nage compared to its detailed pixel values. We can directly visualise the information each layer\ncontains about the input image by reconstructing the image only from the feature maps in that\nlayer9 (Fig 1, content reconstructions, see Methods for details on how to reconstruct the im-\nage). Higher layers in the network capture the high-level content in terms of objects and their\narrangement in the input image but do not constrain the exact pixel values of the reconstruc-\ntion. (Fig 1, content reconstructions d,e). In contrast, reconstructions from the lower layers\nsimply reproduce the exact pixel values of the original image (Fig 1, content reconstructions\na,b,c). We therefore refer to the feature responses in higher layers of the network as the content\nrepresentation.\nTo obtain a representation of the style of an input image, we use a feature space originally\ndesigned to capture texture information.8 This feature space is built on top of the ﬁlter responses\nin each layer of the network. It consists of the correlations between the different ﬁlter responses\nover the spatial extent of the feature maps (see Methods for details). By including the feature\ncorrelations of multiple layers, we obtain a stationary, multi-scale representation of the input\nimage, which captures its texture information but not the global arrangement.\n2\nFigure 1: Convolutional Neural Network (CNN). A given input image is represented as a set\nof ﬁltered images at each processing stage in the CNN. While the number of different ﬁlters\nincreases along the processing hierarchy, the size of the ﬁltered images is reduced by some\ndownsampling mechanism (e.g. max-pooling) leading to a decrease in the total number of\nunits per layer of the network. Content Reconstructions. We can visualise the information\nat different processing stages in the CNN by reconstructing the input image from only know-\ning the network’s responses in a particular layer. We reconstruct the input image from from\nlayers ‘conv1 1’ (a), ‘conv2 1’ (b), ‘conv3 1’ (c), ‘conv4 1’ (d) and ‘conv5 1’ (e) of the orig-\ninal VGG-Network. We ﬁnd that reconstruction from lower layers is almost perfect (a,b,c). In\nhigher layers of the network, detailed pixel information is lost while the high-level content of the\nimage is preserved (d,e). Style Reconstructions. On top of the original CNN representations\nwe built a new feature space that captures the style of an input image. The style representation\ncomputes correlations between the different features in different layers of the CNN. We recon-\nstruct the style of the input image from style representations built on different subsets of CNN\nlayers ( ‘conv1 1’ (a), ‘conv1 1’ and ‘conv2 1’ (b), ‘conv1 1’, ‘conv2 1’ and ‘conv3 1’ (c),\n‘conv1 1’, ‘conv2 1’, ‘conv3 1’ and ‘conv4 1’ (d), ‘conv1 1’, ‘conv2 1’, ‘conv3 1’, ‘conv4 1’\nand ‘conv5 1’ (e)). This creates images that match the style of a given image on an increasing\nscale while discarding information of the global arrangement of the scene.\n3\nAgain, we can visualise the information captured by these style feature spaces built on\ndifferent layers of the network by constructing an image that matches the style representation\nof a given input image (Fig 1, style reconstructions).10,11 Indeed reconstructions from the style\nfeatures produce texturised versions of the input image that capture its general appearance in\nterms of colour and localised structures. Moreover, the size and complexity of local image\nstructures from the input image increases along the hierarchy, a result that can be explained\nby the increasing receptive ﬁeld sizes and feature complexity. We refer to this multi-scale\nrepresentation as style representation.\nThe key ﬁnding of this paper is that the representations of content and style in the Convo-\nlutional Neural Network are separable. That is, we can manipulate both representations inde-\npendently to produce new, perceptually meaningful images. To demonstrate this ﬁnding, we\ngenerate images that mix the content and style representation from two different source images.\nIn particular, we match the content representation of a photograph depicting the “Neckarfront”\nin T¨ubingen, Germany and the style representations of several well-known artworks taken from\ndifferent periods of art (Fig 2).\nThe images are synthesised by ﬁnding an image that simultaneously matches the content\nrepresentation of the photograph and the style representation of the respective piece of art (see\nMethods for details). While the global arrangement of the original photograph is preserved,\nthe colours and local structures that compose the global scenery are provided by the artwork.\nEffectively, this renders the photograph in the style of the artwork, such that the appearance of\nthe synthesised image resembles the work of art, even though it shows the same content as the\nphotograph.\nAs outlined above, the style representation is a multi-scale representation that includes mul-\ntiple layers of the neural network. In the images we have shown in Fig 2, the style representation\nincluded layers from the whole network hierarchy. Style can also be deﬁned more locally by\n4\nFigure 2: Images that combine the content of a photograph with the style of several well-known\nartworks. The images were created by ﬁnding an image that simultaneously matches the content\nrepresentation of the photograph and the style representation of the artwork (see Methods). The\noriginal photograph depicting the Neckarfront in T¨ubingen, Germany, is shown in A (Photo:\nAndreas Praefcke). The painting that provided the style for the respective generated image\nis shown in the bottom left corner of each panel. B The Shipwreck of the Minotaur by J.M.W.\nTurner, 1805. C The Starry Night by Vincent van Gogh, 1889. D Der Schrei by Edvard Munch,\n1893. E Femme nue assise by Pablo Picasso, 1910. F Composition VII by Wassily Kandinsky,\n1913.\n5\nincluding only a smaller number of lower layers, leading to different visual experiences (Fig 3,\nalong the rows). When matching the style representations up to higher layers in the network,\nlocal images structures are matched on an increasingly large scale, leading to a smoother and\nmore continuous visual experience. Thus, the visually most appealing images are usually cre-\nated by matching the style representation up to the highest layers in the network (Fig 3, last\nrow).\nOf course, image content and style cannot be completely disentangled. When synthesising\nan image that combines the content of one image with the style of another, there usually does\nnot exist an image that perfectly matches both constraints at the same time. However, the\nloss function we minimise during image synthesis contains two terms for content and style\nrespectively, that are well separated (see Methods). We can therefore smoothly regulate the\nemphasis on either reconstructing the content or the style (Fig 3, along the columns). A strong\nemphasis on style will result in images that match the appearance of the artwork, effectively\ngiving a texturised version of it, but hardly show any of the photograph’s content (Fig 3, ﬁrst\ncolumn). When placing strong emphasis on content, one can clearly identify the photograph,\nbut the style of the painting is not as well-matched (Fig 3, last column). For a speciﬁc pair of\nsource images one can adjust the trade-off between content and style to create visually appealing\nimages.\nHere we present an artiﬁcial neural system that achieves a separation of image content from\nstyle, thus allowing to recast the content of one image in the style of any other image. We\ndemonstrate this by creating new, artistic images that combine the style of several well-known\npaintings with the content of an arbitrarily chosen photograph. In particular, we derive the\nneural representations for the content and style of an image from the feature responses of high-\nperforming Deep Neural Networks trained on object recognition. To our knowledge this is the\nﬁrst demonstration of image features separating content from style in whole natural images.\n6\nFigure 3: Detailed results for the style of the painting Composition VII by Wassily Kandinsky.\nThe rows show the result of matching the style representation of increasing subsets of the CNN\nlayers (see Methods). We ﬁnd that the local image structures captured by the style represen-\ntation increase in size and complexity when including style features from higher layers of the\nnetwork. This can be explained by the increasing receptive ﬁeld sizes and feature complex-\nity along the network’s processing hierarchy. The columns show different relative weightings\nbetween the content and style reconstruction. The number above each column indicates the\nratio α/β between the emphasis on matching the content of the photograph and the style of the\nartwork (see Methods).\n7\nPrevious work on separating content from style was evaluated on sensory inputs of much lesser\ncomplexity, such as characters in different handwriting or images of faces or small ﬁgures in\ndifferent poses.12,13\nIn our demonstration, we render a given photograph in the style of a range of well-known\nartworks. This problem is usually approached in a branch of computer vision called non-\nphotorealistic rendering (for recent review see14). Conceptually most closely related are meth-\nods using texture transfer to achieve artistic style transfer.15–19 However, these previous ap-\nproaches mainly rely on non-parametric techniques to directly manipulate the pixel representa-\ntion of an image. In contrast, by using Deep Neural Networks trained on object recognition, we\ncarry out manipulations in feature spaces that explicitly represent the high level content of an\nimage.\nFeatures from Deep Neural Networks trained on object recognition have been previously\nused for style recognition in order to classify artworks according to the period in which they\nwere created.20 There, classiﬁers are trained on top of the raw network activations, which we\ncall content representations. We conjecture that a transformation into a stationary feature space\nsuch as our style representation might achieve even better performance in style classiﬁcation.\nIn general, our method of synthesising images that mix content and style from different\nsources, provides a new, fascinating tool to study the perception and neural representation of\nart, style and content-independent image appearance in general. We can design novel stimuli\nthat introduce two independent, perceptually meaningful sources of variation: the appearance\nand the content of an image. We envision that this will be useful for a wide range of experimen-\ntal studies concerning visual perception ranging from psychophysics over functional imaging\nto even electrophysiological neural recordings. In fact, our work offers an algorithmic under-\nstanding of how neural representations can independently capture the content of an image and\nthe style in which it is presented. Importantly, the mathematical form of our style representa-\n8\ntions generates a clear, testable hypothesis about the representation of image appearance down\nto the single neuron level. The style representations simply compute the correlations between\ndifferent types of neurons in the network. Extracting correlations between neurons is a bio-\nlogically plausible computation that is, for example, implemented by so-called complex cells\nin the primary visual system (V1).21 Our results suggest that performing a complex-cell like\ncomputation at different processing stages along the ventral stream would be a possible way to\nobtain a content-independent representation of the appearance of a visual input.\nAll in all it is truly fascinating that a neural system, which is trained to perform one of the\ncore computational tasks of biological vision, automatically learns image representations that\nallow the separation of image content from style. The explanation could be that when learning\nobject recognition, the network has to become invariant to all image variation that preserves\nobject identity. Representations that factorise the variation in the content of an image and the\nvariation in its appearance would be extremely practical for this task. Thus, our ability to\nabstract content from style and therefore our ability to create and enjoy art might be primarily a\npreeminent signature of the powerful inference capabilities of our visual system.\nMethods\nThe results presented in the main text were generated on the basis of the VGG-Network,22\na Convolutional Neural Network that rivals human performance on a common visual object\nrecognition benchmark task23 and was introduced and extensively described in.22 We used the\nfeature space provided by the 16 convolutional and 5 pooling layers of the 19 layer VGG-\nNetwork. We do not use any of the fully connected layers.The model is publicly available and\ncan be explored in the caffe-framework.24 For image synthesis we found that replacing the\nmax-pooling operation by average pooling improves the gradient ﬂow and one obtains slightly\nmore appealing results, which is why the images shown were generated with average pooling.\n9\nGenerally each layer in the network deﬁnes a non-linear ﬁlter bank whose complexity in-\ncreases with the position of the layer in the network. Hence a given input image ⃗x is encoded\nin each layer of the CNN by the ﬁlter responses to that image. A layer with Nl distinct ﬁlters\nhas Nl feature maps each of size Ml, where Ml is the height times the width of the feature map.\nSo the responses in a layer l can be stored in a matrix F l ∈RNl×Ml where F l\nij is the activation\nof the ith ﬁlter at position j in layer l. To visualise the image information that is encoded at\ndifferent layers of the hierarchy (Fig 1, content reconstructions) we perform gradient descent\non a white noise image to ﬁnd another image that matches the feature responses of the original\nimage. So let ⃗p and ⃗x be the original image and the image that is generated and P l and F l their\nrespective feature representation in layer l. We then deﬁne the squared-error loss between the\ntwo feature representations\nLcontent(⃗p, ⃗x, l) = 1\n2\nX\ni,j\n\u0000F l\nij −P l\nij\n\u00012 .\n(1)\nThe derivative of this loss with respect to the activations in layer l equals\n∂Lcontent\n∂F l\nij\n=\n(\u0000F l −P l\u0001\nij\nif F l\nij > 0\n0\nif F l\nij < 0 .\n(2)\nfrom which the gradient with respect to the image ⃗x can be computed using standard error\nback-propagation. Thus we can change the initially random image ⃗x until it generates the same\nresponse in a certain layer of the CNN as the original image ⃗p. The ﬁve content reconstructions\nin Fig 1 are from layers ‘conv1 1’ (a), ‘conv2 1’ (b), ‘conv3 1’ (c), ‘conv4 1’ (d) and ‘conv5 1’\n(e) of the original VGG-Network.\nOn top of the CNN responses in each layer of the network we built a style representation\nthat computes the correlations between the different ﬁlter responses, where the expectation is\ntaken over the spatial extend of the input image. These feature correlations are given by the\nGram matrix Gl ∈RNl×Nl, where Gl\nij is the inner product between the vectorised feature map\n10\ni and j in layer l:\nGl\nij =\nX\nk\nF l\nikF l\njk.\n(3)\nTo generate a texture that matches the style of a given image (Fig 1, style reconstructions),\nwe use gradient descent from a white noise image to ﬁnd another image that matches the style\nrepresentation of the original image. This is done by minimising the mean-squared distance\nbetween the entries of the Gram matrix from the original image and the Gram matrix of the\nimage to be generated. So let ⃗a and ⃗x be the original image and the image that is generated and\nAl and Gl their respective style representations in layer l. The contribution of that layer to the\ntotal loss is then\nEl =\n1\n4N 2\nl M 2\nl\nX\ni,j\n\u0000Gl\nij −Al\nij\n\u00012\n(4)\nand the total loss is\nLstyle(⃗a, ⃗x) =\nL\nX\nl=0\nwlEl\n(5)\nwhere wl are weighting factors of the contribution of each layer to the total loss (see below for\nspeciﬁc values of wl in our results). The derivative of El with respect to the activations in layer\nl can be computed analytically:\n∂El\n∂F l\nij\n=\n(\n1\nN2\nl M2\nl\n\u0000(F l)T \u0000Gl −Al\u0001\u0001\nji\nif F l\nij > 0\n0\nif F l\nij < 0 .\n(6)\nThe gradients of El with respect to the activations in lower layers of the network can be readily\ncomputed using standard error back-propagation. The ﬁve style reconstructions in Fig 1 were\ngenerated by matching the style representations on layer ‘conv1 1’ (a), ‘conv1 1’ and ‘conv2 1’\n(b), ‘conv1 1’, ‘conv2 1’ and ‘conv3 1’ (c), ‘conv1 1’, ‘conv2 1’, ‘conv3 1’ and ‘conv4 1’ (d),\n‘conv1 1’, ‘conv2 1’, ‘conv3 1’, ‘conv4 1’ and ‘conv5 1’ (e).\nTo generate the images that mix the content of a photograph with the style of a painting\n(Fig 2) we jointly minimise the distance of a white noise image from the content representation\n11\nof the photograph in one layer of the network and the style representation of the painting in a\nnumber of layers of the CNN. So let ⃗p be the photograph and⃗a be the artwork. The loss function\nwe minimise is\nLtotal(⃗p,⃗a, ⃗x) = αLcontent(⃗p, ⃗x) + βLstyle(⃗a, ⃗x)\n(7)\nwhere α and β are the weighting factors for content and style reconstruction respectively. For\nthe images shown in Fig 2 we matched the content representation on layer ‘conv4 2’ and the\nstyle representations on layers ‘conv1 1’, ‘conv2 1’, ‘conv3 1’, ‘conv4 1’ and ‘conv5 1’ (wl =\n1/5 in those layers, wl = 0 in all other layers) . The ratio α/β was either 1×10−3 (Fig 2 B,C,D)\nor 1 × 10−4 (Fig 2 E,F). Fig 3 shows results for different relative weightings of the content and\nstyle reconstruction loss (along the columns) and for matching the style representations only\non layer ‘conv1 1’ (A), ‘conv1 1’ and ‘conv2 1’ (B), ‘conv1 1’, ‘conv2 1’ and ‘conv3 1’ (C),\n‘conv1 1’, ‘conv2 1’, ‘conv3 1’ and ‘conv4 1’ (D), ‘conv1 1’, ‘conv2 1’, ‘conv3 1’, ‘conv4 1’\nand ‘conv5 1’ (E). The factor wl was always equal to one divided by the number of active layers\nwith a non-zero loss-weight wl.\nAcknowledgments\nThis work was funded by the German National Academic Foundation\n(L.A.G.), the Bernstein Center for Computational Neuroscience (FKZ 01GQ1002) and the Ger-\nman Excellency Initiative through the Centre for Integrative Neuroscience T¨ubingen (EXC307)(M.B.,\nA.S.E, L.A.G.)\nReferences and Notes\n1. Krizhevsky, A., Sutskever, I. & Hinton, G. E. Imagenet classiﬁcation with deep convolu-\ntional neural networks. In Advances in neural information processing systems, 1097–1105\n(2012). URL http://papers.nips.cc/paper/4824-imagenet.\n12\n2. Taigman, Y., Yang, M., Ranzato, M. & Wolf, L. Deepface: Closing the gap to human-level\nperformance in face veriﬁcation. In Computer Vision and Pattern Recognition (CVPR),\n2014 IEEE Conference on, 1701–1708 (IEEE, 2014).\nURL http://ieeexplore.\nieee.org/xpls/abs_all.jsp?arnumber=6909616.\n3. G¨uc¸l¨u, U. & Gerven, M. A. J. v. Deep Neural Networks Reveal a Gradient in the Com-\nplexity of Neural Representations across the Ventral Stream. The Journal of Neuroscience\n35, 10005–10014 (2015). URL http://www.jneurosci.org/content/35/27/\n10005.\n4. Yamins, D. L. K. et al.\nPerformance-optimized hierarchical models predict neural re-\nsponses in higher visual cortex.\nProceedings of the National Academy of Sciences\n201403112 (2014).\nURL http://www.pnas.org/content/early/2014/05/\n08/1403112111.\n5. Cadieu, C. F. et al. Deep Neural Networks Rival the Representation of Primate IT Cortex\nfor Core Visual Object Recognition. PLoS Comput Biol 10, e1003963 (2014). URL http:\n//dx.doi.org/10.1371/journal.pcbi.1003963.\n6. K¨ummerer, M., Theis, L. & Bethge, M.\nDeep Gaze I: Boosting Saliency Prediction\nwith Feature Maps Trained on ImageNet. In ICLR Workshop (2015). URL /media/\npublications/1411.1045v4.pdf.\n7. Khaligh-Razavi, S.-M. & Kriegeskorte, N. Deep Supervised, but Not Unsupervised, Mod-\nels May Explain IT Cortical Representation. PLoS Comput Biol 10, e1003915 (2014). URL\nhttp://dx.doi.org/10.1371/journal.pcbi.1003915.\n13\n8. Gatys, L. A., Ecker, A. S. & Bethge, M. Texture synthesis and the controlled generation of\nnatural stimuli using convolutional neural networks. arXiv:1505.07376 [cs, q-bio] (2015).\nURL http://arxiv.org/abs/1505.07376. ArXiv: 1505.07376.\n9. Mahendran, A. & Vedaldi, A. Understanding Deep Image Representations by Inverting\nThem. arXiv:1412.0035 [cs] (2014). URL http://arxiv.org/abs/1412.0035.\nArXiv: 1412.0035.\n10. Heeger, D. J. & Bergen, J. R.\nPyramid-based Texture Analysis/Synthesis.\nIn Pro-\nceedings of the 22Nd Annual Conference on Computer Graphics and Interactive Tech-\nniques, SIGGRAPH ’95, 229–238 (ACM, New York, NY, USA, 1995).\nURL http:\n//doi.acm.org/10.1145/218380.218446.\n11. Portilla, J. & Simoncelli, E. P.\nA Parametric Texture Model Based on Joint Statis-\ntics of Complex Wavelet Coefﬁcients.\nInternational Journal of Computer Vision\n40, 49–70 (2000). URL http://link.springer.com/article/10.1023/A%\n3A1026553619983.\n12. Tenenbaum, J. B. & Freeman, W. T. Separating style and content with bilinear models. Neu-\nral computation 12, 1247–1283 (2000). URL http://www.mitpressjournals.\norg/doi/abs/10.1162/089976600300015349.\n13. Elgammal, A. & Lee, C.-S. Separating style and content on a nonlinear manifold. In\nComputer Vision and Pattern Recognition, 2004. CVPR 2004. Proceedings of the 2004\nIEEE Computer Society Conference on, vol. 1, I–478 (IEEE, 2004).\nURL http://\nieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1315070.\n14. Kyprianidis, J. E., Collomosse, J., Wang, T. & Isenberg, T. State of the ”Art”: A Taxon-\nomy of Artistic Stylization Techniques for Images and Video. Visualization and Computer\n14\nGraphics, IEEE Transactions on 19, 866–885 (2013). URL http://ieeexplore.\nieee.org/xpls/abs_all.jsp?arnumber=6243138.\n15. Hertzmann, A., Jacobs, C. E., Oliver, N., Curless, B. & Salesin, D. H. Image analogies.\nIn Proceedings of the 28th annual conference on Computer graphics and interactive tech-\nniques, 327–340 (ACM, 2001). URL http://dl.acm.org/citation.cfm?id=\n383295.\n16. Ashikhmin, N. Fast texture transfer. IEEE Computer Graphics and Applications 23, 38–43\n(2003).\n17. Efros, A. A. & Freeman, W. T. Image quilting for texture synthesis and transfer. In Pro-\nceedings of the 28th annual conference on Computer graphics and interactive techniques,\n341–346 (ACM, 2001). URL http://dl.acm.org/citation.cfm?id=383296.\n18. Lee, H., Seo, S., Ryoo, S. & Yoon, K. Directional Texture Transfer. In Proceedings of the\n8th International Symposium on Non-Photorealistic Animation and Rendering, NPAR ’10,\n43–48 (ACM, New York, NY, USA, 2010). URL http://doi.acm.org/10.1145/\n1809939.1809945.\n19. Xie, X., Tian, F. & Seah, H. S. Feature Guided Texture Synthesis (FGTS) for Artistic Style\nTransfer. In Proceedings of the 2Nd International Conference on Digital Interactive Media\nin Entertainment and Arts, DIMEA ’07, 44–49 (ACM, New York, NY, USA, 2007). URL\nhttp://doi.acm.org/10.1145/1306813.1306830.\n20. Karayev, S. et al. Recognizing image style. arXiv preprint arXiv:1311.3715 (2013). URL\nhttp://arxiv.org/abs/1311.3715.\n15\n21. Adelson, E. H. & Bergen, J. R. Spatiotemporal energy models for the perception of motion.\nJOSA A 2, 284–299 (1985). URL http://www.opticsinfobase.org/josaa/\nfulltext.cfm?uri=josaa-2-2-284.\n22. Simonyan, K. & Zisserman, A. Very Deep Convolutional Networks for Large-Scale Image\nRecognition. arXiv:1409.1556 [cs] (2014). URL http://arxiv.org/abs/1409.\n1556. ArXiv: 1409.1556.\n23. Russakovsky,\nO. et al.\nImageNet Large Scale Visual Recognition Challenge.\narXiv:1409.0575 [cs] (2014). URL http://arxiv.org/abs/1409.0575. ArXiv:\n1409.0575.\n24. Jia, Y. et al. Caffe: Convolutional architecture for fast feature embedding. In Proceedings\nof the ACM International Conference on Multimedia, 675–678 (ACM, 2014). URL http:\n//dl.acm.org/citation.cfm?id=2654889.\n16\n",
        "sentence": " [13] then combine the loss on the correlations as a proxy to the style of a painting and the loss on the activations to represent the content of an image, and successfully create artistic images by converting the artistic style to the content image, inspiring several followups [14, 15]. Our methods are similar in spirit to existing methods [7, 8, 13]. [13] from the feature responses of VGG trained on ImageNet, we use an untrained VGG and succeed in separating and recombining content and style of arbitrary images. [13] on several well-known artworks for the style: Night Starry by Vincent van Gohn 1989, Der Schrei by Edward Munch 1893, Picasso by Pablo Picasso 1907, Woman with a Hat by Henri Matisse 1905, Meadow with Poplars by Claude Monet 1875. Our results are comparable to their work [13] on the pretrained VGG (third row), and are in the same order of magnitude. [13]."
    },
    {
        "title": "Exploring the neural algorithm of artistic style",
        "author": [
            "Yaroslav Nikulin",
            "Roman Novak"
        ],
        "venue": "arXiv preprint arXiv:1602.07188,",
        "citeRegEx": "14",
        "shortCiteRegEx": "14",
        "year": 2016,
        "abstract": "We explore the method of style transfer presented in the article \"A Neural\nAlgorithm of Artistic Style\" by Leon A. Gatys, Alexander S. Ecker and Matthias\nBethge (arXiv:1508.06576).\n  We first demonstrate the power of the suggested style space on a few\nexamples. We then vary different hyper-parameters and program properties that\nwere not discussed in the original paper, among which are the recognition\nnetwork used, starting point of the gradient descent and different ways to\npartition style and content layers. We also give a brief comparison of some of\nthe existing algorithm implementations and deep learning frameworks used.\n  To study the style space further we attempt to generate synthetic images by\nmaximizing a single entry in one of the Gram matrices $\\mathcal{G}_l$ and some\ninteresting results are observed. Next, we try to mimic the sparsity and\nintensity distribution of Gram matrices obtained from a real painting and\ngenerate more complex textures.\n  Finally, we propose two new style representations built on top of network's\nfeatures and discuss how one could be used to achieve local and potentially\ncontent-aware style transfer.",
        "full_text": "Exploring the Neural Algorithm of Artistic Style\nYaroslav Nikulin∗and Roman Novak∗\nDepartment of Mathematics\n´Ecole normale sup´erieure de Cachan\n94230 Cachan, France\n{yaroslav.nikulin, rnovak}@ens-cachan.fr\nAbstract\nIn this work we explore the method of style transfer pre-\nsented in [1]. We ﬁrst demonstrate the power of the sug-\ngested style space on a few examples.\nWe then vary different hyper-parameters and program\nproperties that were not discussed in [1], among which are\nthe recognition network used, starting point of the gradient\ndescent and different ways to partition style and content lay-\ners. We also give a brief comparison of some of the existing\nalgorithm implementations and deep learning frameworks\nused.\nTo study the style space further, an idea similar to [2] is\nused to generate synthetic images by maximizing a single\nentry in one of the Gram matrices Gl and some interesting\nresults are observed. Next, we try to mimic the sparsity and\nintensity distribution of Gram matrices obtained from a real\npainting and generate more complex textures.\nFinally, we propose two new style representations built\non top of network’s features and discuss how one could be\nused to achieve local and potentially content-aware style\ntransfer.\n1. Introduction\nThe key idea behind style transfer is to perform a gra-\ndient descent from random noise minimizing the deviation\nfrom content (i.e. feature responses in the upper convolu-\ntional layers of a recognition network) of the target image\nand the deviation from the style representation of the style\nimage. The latter is deﬁned as a set of Gram correlation ma-\ntrices {Gl} with the entries Gl\nij = ⟨F l\ni , F l\nj⟩, where l is the\nnetwork layer, i and j are two ﬁlters of the layer l and F l\nk\nis the array (indexed by spatial coordinates) of neuron re-\nsponses of ﬁlter k in layer l. In other words, Gl\nij value says\nhow often features i and j in the layer l appear together.\nPlease refer to [1] for details.\n∗Authors contributed equally to this work.\nOn ﬁgure 1 we demonstrate the spectacular performance\nof the algorithm by running 500 iterations of the L-BFGS\n[3] optimization algorithm on a photo of a cat.\nIn section 8 we consider cases where the algorithm\ndoesn’t work and suggest possible solutions in section 9.\n2. Frameworks and Implementations\nWe have tried running implementations on Caffe [4] (us-\ning CUDA and OpenCL [5] backend) by Frank Liu [6] and\non Torch [7] by Kai Sheng Tai [8] and by Justin Johnson\n[9].\nWe have observed the OpenCL Caffe backend to be very\npromising but yet unstable compared to CUDA. Otherwise,\nboth Torch implementations turned out to be signiﬁcantly\nfaster.\nWe have thus built our work on top of the Torch imple-\nmentation by Kai Sheng Tai. Interestingly, the Torch cunn\n[10] backend performed slightly better than cuDNN [11] by\nNVIDIA.\n3. Networks\nIn [1] the VGG-19 [12] recognition network is used to\nproduce results.\nWe compare the impact of using other\nnetworks (AlexNet [13], GoogLeNet [14], VGG-16 and\nVGG-19) in ﬁgure 2, with AlexNet performing similarly to\nGoogLeNet and VGG-16 similarly to VGG-19.\nVGG networks perform much better at style transfer due\nto their architecture. For example, AlexNet and GoogLeNet\nstrongly compress the input at the ﬁrst convolutional layer\nusing large kernels and stride (11 × 11 with stride 4 and\n7 × 7 with stride 2 respectively) and thus a lot of ﬁne detail\nis lost. VGG networks use 3 × 3 kernels with stride 1 at all\nconvolutional layers and thus capture much more informa-\ntion.\nWe have therefore used the VGG-19 network for all our\nexperiments.\n1\narXiv:1602.07188v2  [cs.CV]  13 Mar 2016\n4. Initialization\nIn [1] gradient descent is always performed from white\nnoise. We try and compare two other initialization points:\ncontent and style. We demonstrate the impact of the ini-\ntialization strategy in ﬁgure 3, starting the gradient descent\nfrom the content photo, the style artwork or from white\nnoise.\nThe results highlight well the highly non-convex nature\nof our objective and that the starting point has a tremendous\ninﬂuence on the basin of attraction that the gradient descent\nwill converge to. We naturally observe that starting from\nthe content lets us preserve the most of it, while starting\nfrom the style image is prone to “leaking” the content of the\nartwork into the ﬁnal result. This also serves to reinforce\nthe observation made in [1] that style and content are not\nstrictly separable.\nWe ﬁnd that for most practical applications starting from\nthe content image produces the best result.\nThis corre-\nsponds well to the way most artworks are produced – start-\ning from the artist observing the content and drawing a\nrough sketch (i.e. content reconstruction comes ﬁrst), and\nonly then applying paint (i.e. style) on top.\nNote that noise initialization is still very useful for test-\ning, benchmarking and hyper-parameter tuning.\nFor ex-\nample, starting from content and observing no change one\nmight wonder whether the content weight is too high or the\nlearning rate is too small. Such questions do not occur when\nstarting from noise.\n5. Partial Style Transfer\nIn\n[1]\nthe\nimpact\nof\nshrinking\nthe\nstyle\nlayer\npool\n(from\nusing\nthe\nﬁrst\n5\nconvolutional\nlayers\n{conv1-1,...,conv5-1} down to using only the\nﬁrst convolutional layer {conv1-1}) is demonstrated.\nThe authors observe how the style feature scale and com-\nplexity goes down as they consider using lower and lower\nconvolutional layers to represent style.\nIn general, one\ndoesn’t beneﬁt from using only the lower layer style fea-\ntures (apart from the computation facilitation) and is better\noff simply reducing the style weight in the optimization\nobjective.\nIn our work we consider keeping the upper convolutional\nlayers and removing the bottom ones, while enforcing them\nas part of the content pool.\nFor\nexample,\ninstead\nof\nusing\n{conv4-2}\nas\nthe\ncontent\nlayer\nand\n{conv1-1,...,conv5-1}\nas\nthe\nstyle\nlayers,\nwe\ncould\ntry\nto\nset\n{conv1-1, conv2-1, conv4-1, conv4-2}\nas content and {conv3-1, conv5-1} as style layers\n(see ﬁgure 4). Notice how the partial style transfer man-\naged to reshape the content and make it more rectangular\n(according to the style image) while preserving the original\ncolors (which are mostly captured by the features in the\nbottom layers).\nWe thus conclude that relaxing the bottom layer style\nconstraints and enforcing them instead as the content con-\nstrains allows us to retain the colors and low-level features\nof the content photo and only transfer mid- to high-level\nproperties of the style, combining them into a visually ap-\npealing result.\n6. Generating Styles\nIn order to better understand the style space constructed\nin [1] we draw inspiration from the Deep Visualization tech-\nnique [2]. We disregard all the Gram matrices except for one\nwith a single non-zero entry and descend from white noise\nwithout content constraints. The motivation is to understand\nwhat parts of style a single element can describe and verify\nif the Gram matrices present a basis in the style space qual-\nitatively similar to the basis of VGG features in the space\nof natural images. By varying the non-zero element and\nits magnitude we can generate some curious textures with\ncomplexity increasing from the ﬁrst to the fourth layer (see\nﬁgure 5).\nNext we attempt to generate some more sophisticated\nstyles. For this purpose we consider Gram matrices of a\nsingle painting and visualize its histogram.\nWe observe\nthat sparsity and amplitude of the Gram matrix elements in-\ncrease from the ﬁrst to the ﬁfth layer (of course, this doesn’t\nnecessary need to generalize, yet it seems to be in accor-\ndance with the CNN paradigm where more complex and\nmore discriminative features are constructed from simpler\nones). Although it is impossible to judge about the distribu-\ntion of Gram matrices describing styles with such a limited\nsample, we can try to mimic the sparsity and amplitude of\nGram matrices representing the ”Starry Night” by van Gogh\n[15]. We generate a random matrix using absolute values of\nGaussian distribution and apply a random sparse zero-one\nmask to it. We vary only two parameters: the variance of\nthe Gaussian distribution and the sparsity of the mask. Inter-\nestingly enough, with such a simple construction, we were\nable to generate some intriguing textures (see ﬁgures 6 and\n7) suggesting that style density estimation and generation\nmight be a promising research direction.\n7. Spatial Style Transfer\nIn [1] each style layer l with k features introduces a k ×\nk Gram matrix Gl with feature correlation entries Gl\nij =\n⟨F l\ni , F l\nj⟩. This makes the style completely invariant to the\nspatial conﬁguration of the style image (which is by design).\nAs an experiment, we suggest a new style representation\ndesigned to capture less of the artistic details and more of\nthe spatial conﬁguration of an image with roughly the same\ncomputational and storage complexity.\n2\nWe construct Gl matrices of size k×(2X −1)×(2Y −1)\n(where X and Y are the spatial dimensions of the layer l)\nwith entries\nGl\ni(x, y) = (F l\ni ∗F l\ni )(x, y),\nwhere ∗stands for full 2D convolution. We thus impose a\nsoft constraint on the feature distributions across the spatial\ncoordinates (note that in principle we could store all pair-\nwise convolutions of F l\ni and F l\nj, but such an experiment\nwould be computationally infeasible).\nIn order for this objective to work we need to also rescale\nboth content and style images to the same dimensions and\nmodify the error derivative in [1] accordingly:\n∂El\nF l\ni (x, y) =\n1\nN 2\nl M 2\nl\n\u0002\u0000Gl −Al\u0001\ni ◦F l\ni\n\u0003\n(x, y),\nwhere ◦stands for valid correlation.\nWe demonstrate this new approach on classic style trans-\nfer and on style reconstruction from noise (without content\nconstraints) in ﬁgure 8.\nNotice how differently the proposed algorithm scales: it\nis easy to optimize on top layers (high k, small X and Y ) but\nis expensive on bottom layers (low k, huge X and Y ). This\nis contrary to the algorithm in [1], where the computation\ncost mostly depends on k2 and thus grows from bottom to\ntop layers.\n8. Illumination and Season Transfer\nNot all artistic styles are transferred equally well by the\nalgorithm in [1]. If the style is highly repetitive and homo-\ngeneous over the whole style image (see abstract art, tex-\ntures and patterns), it can be transferred with remarkable\nquality. However, once it becomes more subtle and var-\nied within the image (see Renaissance, Baroque, Realism\netc), style transfer falters. It fails to capture properties like\nthe dramatic use of lighting, exaggerated face features etc.\nThese properties get averaged-out, as the style representa-\ntion is computed over the whole image.\nThe same problem (i.e.\nglobal style representation)\nstands in the way of season and illumination transfer, as\nthese properties change different elements of the scene dif-\nferently.\nWe present some relatively successful examples of sea-\nson and illumination transfer on images that are well aligned\nand are fairly repetitive in ﬁgures 9, 10, 11, 12.\n9. Towards Content-Aware Style Transfer\nThe examples presented in ﬁgures 9, 10, 11, 12 are quite\nbad, but they aren’t too bad. Indeed, one can easily spot\na lot of regions where the texture was luckily transferred\ncorrectly. This serves to indicate that in principle the style\nrepresentation developed in [1] is capable of capturing pho-\ntorealistic properties.\nWhat remains is developing a content-aware style trans-\nfer, i.e. transferring style according to the content matches\nbetween the image to be repainted and the style image. Be-\nlow we discuss some possible leads towards implementing\nsuch a task.\nA direct approach could be to replace the global style\nvalues as deﬁned in [1]\nGl\nij = ⟨F l\ni , F l\nj⟩\nwith localized values of\nGl\nij\n\u0012x\ny\n\u0013\n=\nX\n−sl⩽dx,dy⩽sl\nw\n\u0012dx\ndy\n\u0013\nF l\ni\n\u0012x + dx\ny + dy\n\u0013\nF l\nj\n\u0012x + dx\ny + dy\n\u0013\nwhere we capture only feature correlations in a small (sl)\nregion around a point (x, y) and we make the contributions\ndecay as they get more distant from the point of interest:\nw\n\u0012dx\ndy\n\u0013\n=\n1\n1 + dx2 + dy2 .\nWe can then replace the global style loss\nEl =\n1\n4N 2\nl M 2\nl\n\r\rGl −Al\r\r2\n2\nas deﬁned in [1] with a global style-content covariation loss\nEl ∼\n\r\r\r\r\r\nX\nx,y\n\u0012\nFc,l\nk\n\u0012\nx\ny\n\u0013\nGl\nij\n\u0012\nx\ny\n\u0013\n−Pc,l\nk\n\u0012x\ny\n\u0013\nAl\nij\n\u0012x\ny\n\u0013\u0013\r\r\r\r\r\n2\n2\nwhere Fc,l\nk (x, y) is the weighted content response of\nneurons of ﬁlter k in layer c reachable from (x, y, l) and\nthe norm is that of a 3-dimensional tensor indexed with i, j\nand k.\nOf course such an objective dramatically increases the\ncomputational cost of our problems and while could be efﬁ-\nciently parallelized, would probably still remain unfeasible.\nIn our implementation we make some very rough as-\nsumptions to test it: we consider Fc ≡Pc (content of im-\nages is aligned; we thus perform a locality-sensitive style\ntransfer), s = 0 (pixel-wise style) and w ≡1 (no distance\ndecay). This leads to a simpliﬁed expression of\nGl\nij = F l\ni ⊙F l\nj\nand, consequently,\n∂El\n∂F l\ni (x, y) =\n1\nN 2\nl M 2\nl\n\u0014\u0000Gl\ni −Al\ni\n\u0001 \u0012\nx\ny\n\u0013\u0015 \u0014\nF l\ni\n\u0012\nx\ny\n\u0013\u0015T\n.\n3\nWe were only able to test this approach on small im-\nages (see ﬁgure 13 and 14). Note that due to very rigid\nconstraints the style picture basically gets painted over the\ncontent image.\nThe next step within this approach would be to expand\nthe style window sl and see whether a locality-sensitive\nstyle transfer is feasible.\nIf it yields good results, the\ncontent-aware transfer could be investigated.\nWe believe that if implemented well, such an algorithm\ncould tackle more exquisite artistic styles, season transfer,\nillumination transfer, super-resolution and possibly many\nother applications.\nReferences\n[1]\nL. A. Gatys, A. S. Ecker, and M. Bethge, “A neu-\nral algorithm of artistic style,” 2015. [Online]. Avail-\nable: http://arxiv.org/abs/1508.06576.\n[2]\nJ.\nYosinski,\nJ.\nClune,\nA.\nNguyen,\nT.\nFuchs,\nand H. Lipson, “Understanding neural networks\nthrough deep visualization,” 2015. [Online]. Avail-\nable: http://arxiv.org/abs/1506.06579.\n[3]\nW. Commons. (2015). Limited-memory bfgs, [On-\nline]. Available: https : / / en . wikipedia .\norg/wiki/Limited-memory_BFGS.\n[4]\nY. Jia, E. Shelhamer, J. Donahue, S. Karayev, J.\nLong, R. Girshick, S. Guadarrama, and T. Dar-\nrell, “Caffe: convolutional architecture for fast fea-\nture embedding,” arXiv preprint arXiv:1408.5093,\n2014. [Online]. Available: http : / / caffe .\nberkeleyvision.org/.\n[5]\nAMD. (2016). Opencl-caffe, [Online]. Available:\nhttps : / / github . com / amd / OpenCL -\ncaffe.\n[6]\nF. Liu. (2015). Style-transfer, [Online]. Available:\nhttps : / / github . com / fzliu / style -\ntransfer.\n[7]\nR. Collobert, K. Kavukcuoglu, and C. Farabet,\n“Torch7: a matlab-like environment for machine\nlearning,” in BigLearn, NIPS Workshop, 2011. [On-\nline]. Available: http://torch.ch.\n[8]\nK. S. Tai. (2015). Style-transfer, [Online]. Available:\nhttps : / / github . com / kaishengtai /\nneuralart.\n[9]\nJ. Johnson. (2015). Neural-style, [Online]. Avail-\nable: https://github.com/jcjohnson/\nneural-style.\n[10]\nTorch. (2015). Cunn, [Online]. Available: https:\n//github.com/torch/cunn.\n[11]\nNVIDIA.\n(2015).\nCudnn,\n[Online].\nAvailable:\nhttps://developer.nvidia.com/cudnn.\n[12]\nK. Simonyan and A. Zisserman, “Very deep convo-\nlutional networks for large-scale image recognition,”\nCoRR, vol. abs/1409.1556, 2014. [Online]. Avail-\nable: http://arxiv.org/pdf/1409.1556.\n[13]\nA. Krizhevsky, I. Sutskever, and G. E. Hinton, “Im-\nagenet classiﬁcation with deep convolutional neural\nnetworks,” in Advances in Neural Information Pro-\ncessing Systems 25, F. Pereira, C. Burges, L. Bottou,\nand K. Weinberger, Eds., Curran Associates, Inc.,\n2012, pp. 1097–1105. [Online]. Available: http :\n/ / papers . nips . cc / paper / 4824 -\nimagenet-classification-with-deep-\nconvolutional-neural-networks.pdf.\n[14]\nC. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed,\nD. Anguelov, D. Erhan, V. Vanhoucke, and A. Ra-\nbinovich, “Going deeper with convolutions,” 2014.\n[Online]. Available: http://arxiv.org/abs/\n1409.4842.\n[15]\nV. V. Gogh. (1889). Starry night, [Online]. Available:\nhttps://en.wikipedia.org/wiki/The_\nStarry_Night.\n[16]\nR. Roberts. (2012). Fawkes, [Online]. Available:\nhttp://www.allofthisisforyou.com/\ngallery-2/fawkes2012/.\n[17]\nA. Grey. (1996). Despair, [Online]. Available:\nhttp://alexgrey.com/art/paintings/\nsoul/despair/.\n[18]\n(), [Online]. Available: http://hqwallbase.\npw/105645-strange-birds/.\n[19]\nD. Kuzmenka. (2008). Hatosia, [Online]. Avail-\nable: https : / / www . fractalus . com /\ndan / Galleries / Fractals / Fractal %\n20Gallery%208/slides/Hatosia.html.\n[20]\nPando. (2013). Square vp jared ﬂiesler joins ma-\ntrix partners as a general partner, [Online]. Avail-\nable: https://pando.com/2013/03/07/\nsquare - vp - jared - fliesler - joins -\nmatrix - partners - as - a - general -\npartner/.\n[21]\n(),\n[Online].\nAvailable:\nhttp\n:\n/\n/\nwallpapercave.com/wp/Gu6gpja.jpg.\n[22]\nM. Apostolescu. (). Midnight cat, [Online]. Avail-\nable: http://www.013a.com/html/cat.\nhtm.\n[23]\n—, (). Farewell mr. eldritch, [Online]. Available:\nhttp : / / www . 013a . com / html / skull _\ncolor.htm.\n4\n[24]\n(). Using science ﬁction to teach creative thinking,\npart three: steampunk, [Online]. Available: http:\n/ / creativendeavors . blogspot . com /\n2014/01/using-science-fiction-to-\nteach-creative_3779.html.\n[25]\n(),\n[Online].\nAvailable:\nhttp\n:\n/\n/\nwallpapercave.com/wp/1sGOysa.jpg.\n[26]\n(), [Online]. Available: http : / / whvn . cc /\n186977.\n[27]\nPetFinder. (2015). The special grooming needs of a\nsenior cat, [Online]. Available: https://www.\npetfinder . com / cats / cat - grooming /\ngrooming-needs-senior-cat/.\n[28]\nG. Balla. (1923). Pessimismo e optimismo, [Online].\nAvailable: http : / / www . wikiart . org /\nen / giacomo - balla / pessimism - and -\noptimism-1923.\n[29]\nT. C. Fedro. (1969). Cubist 9, [Online]. Avail-\nable: http : / / www . ebsqart . com / Art -\nGalleries / Contemporary - Cubism / 43 /\nCubist-9/204218/.\n5\nFigure 1. Transferring different styles [16–26] to a photo of a cat [27] (top-left).\n6\nFigure 2. Style transfer using GoogLeNet (left) and VGG-19 (right).\nFigure 3. Impact of the initialization point on the ﬁnal result. Top row: the source photo (left), the style image (right, [28]). Bottom row:\nresults of 500 iterations starting from style (left), content (center) and noise (right) images.\n7\nFigure 4. Using {conv1-1, conv2-1, conv4-1, conv4-2} as content and {conv3-1, conv5-1} as style layers to repaint a\nphoto of a cat. Style image (top-left, [29]), full (top-right), low-weight (bottom-left) and partial style transfer (bottom-right). Notice that\nsimply using a low style weight does not allow to reproduce the same result.\n8\nFigure 5. Styles generated by targeting one Gram matrix Gl having a single non-zero entry. Top to bottom: layer l from 1 to 4 of the target\nGram matrix Gl. Position of the non-zero element is either ﬁxed (left) or generated randomly at each gradient descent iteration (right)\n9\nFigure 6. Styles generated by targeting one random sparse Gram matrix Gl. Top to bottom: layer l from 1 to 4 of the target Gram matrix\nGl. The matrix is either ﬁxed (left) or generated at each gradient descent iteration (right).\n10\nFigure 7. Styles generated targeting multiple random (but ﬁxed for the whole optimization procedure) sparse Gram matrices. Target\nmatrices are at convolutional layers {1, 2, 4}, {1, 2, 3} (top row), {1, 4}, {1, 2, 5} (bottom row).\n11\nFigure 8. Comparing the conventional and the “spatial” style transfer. Top row: content (left) and style (right); middle row: conventional\n(left) and “spatial” (right) style transfer; bottom row: style reconstruction from noise with no content constraint using conventional (left)\nand “spatial” (right) representations. Notice how the spatial style transfer gently imprints the scene structure into the photo. Reconstruction\nfrom noise demonstrates how it arrives at a rough but not exact conﬁguration of the style scene.\n12\nFigure 9. An attempt of season transfer. Top row: content (left) and style (right). Bottom row: reconstruction from content (left), style\n(center) and noise (right). Notice how the problem of global style is especially well observed when descending from white noise, because\nrandom textures are generated everywhere and partially stay on the sky / ground, which is undesirable.\nFigure 10. Another attempt of season transfer. Content (left), style (center) and the transfer result (right).\n13\nFigure 11. Yet another attempt of season transfer. Content (left), style (center) and the transfer result (right).\nFigure 12. An attempt of illumination transfer. Content (left), style (center) and the transfer result (right).\n14\nFigure 13. Summer to winter transition using a very rough approximation of a locality-sensitive style transfer. Due to very rigid constraints\nthe style gets basically painted over the content.\nFigure 14. Winter to summer transition using a very rough approximation of a locality-sensitive style transfer. As earlier, due to very rigid\nconstraints the style gets basically painted over the content.\n15\n",
        "sentence": " [13] then combine the loss on the correlations as a proxy to the style of a painting and the loss on the activations to represent the content of an image, and successfully create artistic images by converting the artistic style to the content image, inspiring several followups [14, 15]."
    },
    {
        "title": "Perceptual losses for real-time style transfer and superresolution",
        "author": [
            "Justin Johnson",
            "Alexandre Alahi",
            "Li Fei-Fei"
        ],
        "venue": "arXiv preprint arXiv:1603.08155,",
        "citeRegEx": "15",
        "shortCiteRegEx": "15",
        "year": 2016,
        "abstract": "",
        "full_text": "",
        "sentence": " [13] then combine the loss on the correlations as a proxy to the style of a painting and the loss on the activations to represent the content of an image, and successfully create artistic images by converting the artistic style to the content image, inspiring several followups [14, 15]. The philosophy is to train another neural network to inverse the representation and speedup the visualization on image reconstruction[10, 18], texture synthesis[19] or even style transfer[15]."
    },
    {
        "title": "Multifaceted feature visualization: Uncovering the different types of features learned by each neuron in deep neural networks",
        "author": [
            "Anh Mai Nguyen",
            "Jason Yosinski",
            "Jeff Clune"
        ],
        "venue": "arXiv preprint arXiv:1602.03616,",
        "citeRegEx": "16",
        "shortCiteRegEx": "16",
        "year": 2016,
        "abstract": "We can better understand deep neural networks by identifying which features\neach of their neurons have learned to detect. To do so, researchers have\ncreated Deep Visualization techniques including activation maximization, which\nsynthetically generates inputs (e.g. images) that maximally activate each\nneuron. A limitation of current techniques is that they assume each neuron\ndetects only one type of feature, but we know that neurons can be multifaceted,\nin that they fire in response to many different types of features: for example,\na grocery store class neuron must activate either for rows of produce or for a\nstorefront. Previous activation maximization techniques constructed images\nwithout regard for the multiple different facets of a neuron, creating\ninappropriate mixes of colors, parts of objects, scales, orientations, etc.\nHere, we introduce an algorithm that explicitly uncovers the multiple facets of\neach neuron by producing a synthetic visualization of each of the types of\nimages that activate a neuron. We also introduce regularization methods that\nproduce state-of-the-art results in terms of the interpretability of images\nobtained by activation maximization. By separately synthesizing each type of\nimage a neuron fires in response to, the visualizations have more appropriate\ncolors and coherent global structure. Multifaceted feature visualization thus\nprovides a clearer and more comprehensive description of the role of each\nneuron.",
        "full_text": "Multifaceted Feature Visualization: Uncovering the Different Types of Features\nLearned By Each Neuron in Deep Neural Networks\nAnh Nguyen\nANGUYEN8@UWYO.EDU\nUniversity of Wyoming\nJason Yosinski\nYOSINSKI@CS.CORNELL.EDU\nCornell University\nJeff Clune\nJEFFCLUNE@UWYO.EDU\nUniversity of Wyoming\nAbstract\nWe can better understand deep neural networks\nby identifying which features each of their neu-\nrons have learned to detect. To do so, researchers\nhave created Deep Visualization techniques in-\ncluding activation maximization, which synthet-\nically generates inputs (e.g. images) that maxi-\nmally activate each neuron. A limitation of cur-\nrent techniques is that they assume each neuron\ndetects only one type of feature, but we know\nthat neurons can be multifaceted, in that they\nﬁre in response to many different types of fea-\ntures: for example, a grocery store class neuron\nmust activate either for rows of produce or for\na storefront. Previous activation maximization\ntechniques constructed images without regard for\nthe multiple different facets of a neuron, creating\ninappropriate mixes of colors, parts of objects,\nscales, orientations, etc. Here, we introduce an\nalgorithm that explicitly uncovers the multiple\nfacets of each neuron by producing a synthetic\nvisualization of each of the types of images that\nactivate a neuron.\nWe also introduce regular-\nization methods that produce state-of-the-art re-\nsults in terms of the interpretability of images\nobtained by activation maximization. By sepa-\nrately synthesizing each type of image a neuron\nﬁres in response to, the visualizations have more\nappropriate colors and coherent global structure.\nMultifaceted feature visualization thus provides\na clearer and more comprehensive description of\nthe role of each neuron.\nFigure 1. Top: Visualizations of 8 types of images (feature facets)\nthat activate the same “grocery store” class neuron. Bottom: Ex-\nample training set images that activate the same neuron, and re-\nsemble the corresponding synthetic image in the top panel.\n1. Introduction\nRecently, deep neural networks (DNNs) have demonstrated\nstate-of-the-art—and\nsometimes\nhuman-competitive—\nresults on many pattern recognition tasks, especially vision\nclassiﬁcation problems (Krizhevsky et al., 2012; Szegedy\net al., 2014; Karpathy et al., 2014; He et al., 2015). That\nsuccess has motivated efforts to better understand the\narXiv:1602.03616v2  [cs.NE]  7 May 2016\nMultifaceted Feature Visualization\ninner workings of such networks, which enables us to\nfurther improve their architectures, learning algorithms,\nand hyperparameters. An active area of research in this\nvein, called Deep Visualization, involves taking a trained\nDNN and creating synthetic images that produce speciﬁc\nneural activations of interest within it (Zeiler et al., 2014;\nYosinski et al., 2015; Karpathy et al., 2015; Dosovitskiy\net al., 2015; Mahendran et al., 2015; Nguyen et al., 2015;\nBach et al., 2015a; Simonyan et al., 2013; Wei et al., 2015).\nThere are two general camps within Deep Visualization:\nactivation maximization (Erhan et al., 2009) and code in-\nversion (Mahendran et al., 2015). Activation maximization\nis the task of ﬁnding an image that maximally activates a\ncertain neuron (aka “unit”, “feature”, or “feature detector”),\nwhich can reveal what each neuron in a DNN has learned to\nﬁre in response to (i.e. which features it detects). This tech-\nnique can be performed for the output neurons, such as neu-\nrons that classify types of images (Simonyan et al., 2013),\nand can also be performed for each of the hidden neurons\nin a DNN (Erhan et al., 2009; Yosinski et al., 2015). Code\ninversion is the problem of synthesizing an image that, for\na speciﬁc DNN layer, produces a similar activation vector\nat that layer as a target activation vector produced by a spe-\nciﬁc real image (Mahendran et al., 2015; Dosovitskiy et al.,\n2015). It reveals what information about one speciﬁc image\nis encoded by the DNN code at a particular layer.\nBoth activation maximization and code inversion start from\na random image and calculate via backpropagation how the\ncolor of each pixel should be changed to either increase\nthe activation of a neuron (for activation maximization) or\nproduce a layer code closer to the target (for code inver-\nsion). However, previous studies have shown that doing so\nusing only the gradient information produces unrealistic-\nlooking images that are not recognizable (Simonyan et al.,\n2013; Nguyen et al., 2015), because the set of all possi-\nble images is so vast that it is possible to produce images\nthat satisfy the objective, but are still unrecognizable. In-\nstead, we must both satisfy the objective and try to limit\nthe set of images to those that resemble natural images. Bi-\nasing optimization to produce more natural images can be\naccomplished by incorporating natural image priors into\noptimization, which has been shown to substantially im-\nprove the recognizability of the images generated (Mahen-\ndran et al., 2015; Yosinski et al., 2015). Many regulariza-\ntion techniques have been introduced that improve image\nquality such as: Gaussian blur (Yosinski et al., 2015), α-\nnorm (Simonyan et al., 2013), total variation (Mahendran\net al., 2015), jitter (Mordvintsev et al., 2015), and data-\ndriven patch priors (Wei et al., 2015).\nWhile these techniques have improved Deep Visualization\nmethods over the last two years, the resultant images pro-\nvide room for improvement: (1) the color distribution is\nunnatural (Fig. 8b-e); (2) recognizable fragments of images\nare repeated, but these fragments do not ﬁt together into a\ncoherent whole: e.g. multiple ostrich heads without bodies,\nor eyes without faces (Fig. 8b) (Simonyan et al., 2013; Ma-\nhendran et al., 2015); (3) previous techniques have no sys-\ntematic methods to visualize different facets (types of stim-\nuli) that a neuron responds to, but high-level neurons are\nknown to be multifaceted. For example, a face-detecting\nneuron in a DNN was shown to respond to both human\nand lion faces (Yosinski et al., 2015). Neurons in human\nbrains are similarly multifaceted: a “Halle Berry” neuron\nwas found that responds to very different stimuli related to\nthe actress, from pictures of her in costume to her name\nprinted as text (Quiroga et al., 2005).\nWe name the class of algorithms that visualize differ-\nent facets of a neuron multifaceted feature visualization\n(MFV). Erhan et al., 2009 found that optimizing an image\nto maximally activate a neuron from multiple random start-\ning images usually yielded the same ﬁnal visualization. In\ncontrast, Wei et al., 2015 found that if the backpropagation\nneural pathway is masked out in a certain way, the resultant\nimages can sometimes reveal different feature facets. This\nis the ﬁrst MFV method to our knowledge; however, it was\nshown to visualize only two facets per output neuron, and\nis not able to systematically visualize all facets per neuron.\nIn this paper, we propose two Deep Visualization tech-\nniques. Most importantly, we introduce a novel MFV al-\ngorithm that:\n1. Sheds much more light on the inner workings of DNNs\nthan other state-of-the-art Deep Visualization methods\nby revealing the different facets of each neuron. It also\nreveals that neurons at all levels are multifaceted, and\nshows that higher-level neurons are more multifaceted\nthan lower-level ones (Fig. 6).\n2. Improves the quality of synthesized images, producing\nstate-of-the-art activation maximization results: the col-\nors are more natural and the images are more glob-\nally consistent (Fig. 7. See also Figs. 1, 3, 4) because\neach facet is separately synthesized. For example, MFV\nmakes one image of a green bell pepper and another of a\nred bell pepper instead of trying to simultaneously make\nboth (Fig. 4). The increased global structure and con-\ntextual details in the optimized images also support re-\ncent observations (Yosinski et al., 2015) that discrimina-\ntive DNNs encode not only knowledge of a sparse set of\ndiscriminative features for performing classiﬁcation, but\nalso more holistic information about typical input exam-\nples in a manner more reminiscent of generative models.\n3. Is simple to implement. The only main difference is how\nactivation maximization algorithms are initialized. To\nobtain an initialization per facet, we project the training\nMultifaceted Feature Visualization\nset images that maximally activate a neuron into a low-\ndimensional space (here, a 2D space via t-SNE), cluster\nthe images via k-means, and average the n (here, 15)\nclosest images to each cluster centroid to produce the\ninitial image (Section 3).\nWe also introduce a center-biased regularization technique\nthat attempts to produce one central object. It combats a\nﬂaw with all activation maximization techniques including\nMFV: they tend to produce many repeated object fragments\nin an image, instead of objects with more coherent global\nstructure. It does so by allowing, on average, more opti-\nmization iterations for center pixels than edge pixels.\n2. Methods\n2.1. Convolutional neural networks\nFor comparison with recent studies, we test our Deep\nVisualization methods on a variant of the well-known\n“AlexNet” convnet architecture (Krizhevsky et al., 2012),\nwhich is trained on the 1.3-million-image ILSVRC 2012\nImageNet dataset (Deng et al., 2009; Russakovsky et al.,\n2014).\nSpeciﬁcally, our DNN follows the CaffeNet ar-\nchitecture (Jia et al., 2014) with the weights provided\nby Yosinski et al., 2015. While this network is slightly dif-\nferent than AlexNet, the changes are inconsequential: the\nnetwork obtains a 20.1% top-5 error rate, which is similar\nto AlexNet’s 18.2% (Krizhevsky et al., 2012).\nThe last three layers of the DNN are fully connected: we\ncall them fc6, fc7 and fc8. fc8 is the last layer (before soft-\nmax transformation) and has 1000 outputs, one for each\nImageNet class. fc6 and fc7 both have 4096 outputs.\n2.2. Activation maximization\nInformally, we run an optimization algorithm that opti-\nmizes the colors of the pixels in the image to maximally\nactivate a particular neuron. That is accomplished by cal-\nculating the derivative of the target neuron activation with\nrespect to each pixel, which describes how to change the\npixel color to increase the activation of that neuron. We\nalso need to incorporate natural image priors to bias the\nimages to remain in the set of images that look as much as\npossible like natural (i.e. real-world) images.\nFormally, we may pose the activation maximization prob-\nlem for a unit with index j on a layer l of a network Φ as\nﬁnding an image x∗where:\nx∗= arg max\nx\n(Φl,j(x) −Rθ(x))\n(1)\nHere, Rθ(x) is a parameterized regularization function that\ncould include multiple regularizers (i.e. priors), each of\nwhich penalizes the search in a different way to collec-\ntively improve the image quality. We apply two regular-\nizers: total variation (TV) and jitter, as described in Ma-\nhendran et al. (2015), but via a slightly different, but qual-\nitatively similar, implementation. Supplementary Sec. S2\ndetails the differences and analyzes the beneﬁts of incor-\nporating different regularizers used in previous activation\nmaximization papers: Gaussian blur (Yosinski et al., 2015),\nα-norm (Simonyan et al., 2013), total variation (TV) (Ma-\nhendran et al., 2015), jitter (Mordvintsev et al., 2015), and a\ndata-driven patch prior (Wei et al., 2015). Our code and pa-\nrameters are available at http://EvolvingAI.org.\n2.3. Multifaceted feature visualization\nAlthough DNN feature detectors must recognize that very\ndifferent types of images all represent the same concept\n(e.g. a bell pepper detector must recognize green, red, and\norange peppers as in the same class), there is no systematic\nmethod for visualizing these different facets of feature de-\ntectors. In this section, we introduce a method for visualiz-\ning the multiple facets that each neuron in a DNN responds\nto. We demonstrate the technique on the ImageNet dataset,\nalthough the results generalize beyond computer vision to\nany domain (e.g. speech recognition, machine translation).\nWe start with the observation that in the training set, each\nImageNet class has multiple intra-class clusters that reﬂect\ndifferent facets of the class. For example, in the bell pep-\nper class, one ﬁnds bell peppers of different colors, alone\nor in groups, cut open or whole, etc. (Fig. 4). We hypothe-\nsized that activation maximization on the bell pepper class\nis made difﬁcult because which facet of the bell pepper to\nbe reconstructed is not speciﬁed, potentially meaning that\ndifferent areas of the image may optimize toward recon-\nstructing different fragments of different facets. We further\nhypothesized that if we initialize activation maximization\noptimization with the mean image of only one facet, that\nmay increase the likelihood that optimization would recon-\nstruct an image of that type. We also hypothesized that\nwe can get reconstructions of different facets by starting\noptimization with mean images from different facets. Our\nexperimental results support all of these hypotheses.\nSpeciﬁcally, to visualize different facets of an output neu-\nron C (e.g. the fc8 neuron that declares an image to be a\nmember of the “bell pepper” class), we take all (∼1300)\ntraining set examples from that class and follow Algo-\nrithm 1 to produce k = 10 facet visualizations. Note that,\nhere, we demonstrate the algorithm on the training set, and\nthe results are shown in Figs. 1, 3, 4 & 7. However, we\nalso apply the same technique on the validation set (see\nSec. 3.2), and show its results in Fig. 6.\nIntuitively, the idea is to (1) use a DNN’s learned, abstract,\nknowledge of images to determine the different types of\nimages that activate a neuron, and then (2) initialize a state-\nMultifaceted Feature Visualization\nAlgorithm 1 Multifaceted Feature Visualization\nInput: a set of images U and a number of facets k\n1. for each image in U, compute high-level (here fc7)\nhidden code Φi\n2. Reduce the dimensionality of each code Φi from 4096\nto 50 via PCA.\n3. Run t-SNE visualization on the entire set of codes Φi\nto produce a 2-D embedding (examples in Fig. 4).\n4. Locate k clusters in the embedding via k-means.\nfor each cluster\n5. Compute a mean image x0 by averaging the 15\nimages nearest to the cluster centroid.\n6. Run activation maximization (see Section 2.2), but\ninitialize it with x0 instead of a random image.\nOutput: a set of facet visualizations {x1, x2, ..., xk}.\nof-the-art activation maximization optimization algorithm\nwith the mean image computed from each of these clusters\nto visualize each facet. For (1), we ﬁrst embed all of the\nimages in a class in a two-dimensional space via PCA (Per-\nson, 1901; Jolliffe, 2002) and t-SNE (Van der Maaten et al.,\n2008), and then perform k-means clustering (MacQueen\net al., 1967) to ﬁnd k types of images (Fig. 4).\nNote\nthat here we only visualize 10 facets per neuron, but it\nis possible to visualize fewer or more facets by changing\nk. We compute a mean image by averaging m = 15 im-\nages (Algorithm 1, step 5) as it works the best compared to\nm = {1, 50, 100, 200} (data not shown). Supplementary\nSections S1 & S2.4 provide more intuition regarding why\ninitializing from mean images helps.\n2.4. Center-biased regularization\nIn activation maximization, to increase the activation of a\ngiven neuron (e.g. an “ostrich” neuron), each “drawing”\nstep often includes two types of updates: (1) intensifying\nthe colors of existing ostriches in the image, and (2) draw-\ning new fragments of ostriches (e.g. multiple heads). Since\nboth types of updates are non-separably encoded in the gra-\ndient, the results of previous activation maximization meth-\nods are often images with many repeated image fragments\n(Fig. 8b-h). To ameliorate this issue, we introduce a tech-\nnique called center-biased regularization.\nPreliminary experiments revealed that optimizing with a\nlarge smoothing effect (e.g. a high Gaussian blur radius\nor TV weight) produces a blurry, but single and centered\nobject. Based on this observation, the intuition behind our\ntechnique is to ﬁrst generate a blurry, centered object, and\nthen reﬁne this image by updating the center pixels more\nthan the edge pixels to produce a ﬁnal image that is sharp,\nand has a centrally-located object (Fig. 2).\nCenter-biased regularized images have far fewer duplicated\nFigure 2. Progressive result of optimizing an image to activate the\n“milk can” neuron via the center-biased regularization method.\nfragments (Fig. 8i), and thus tend to more closely represent\nthe style of training set images, which feature one centrally\nlocated object. However, this technique is not guaranteed to\nproduce a single object only. Instead, it is a way of biasing\noptimization toward creating objects near the image center.\nSupplementary Section S2.3 provides more details and re-\nsults for center-biased regularization, including how com-\nbining it with multifaceted visualization (initializing from\nmean images) further improves visualization quality.\n3. Results\n3.1. Neurons are multifaceted feature detectors\nMultifaceted feature visualization produces qualitatively\ndifferent images that activate the same neuron. For exam-\nple, it synthesizes differently colored peppers for the “bell\npepper” class neuron (Fig. 4) and differently colored cars\nfor the “convertible” car class neuron (Fig. 3b). It also pro-\nduces objects seen from different perspectives, such as cars\nseen from the back or front (Fig. 3b), or in different num-\nbers, such as one, two, or multiple peppers (Fig. 4).\nMost interestingly, multifaceted feature visualization can\nuncover that a deep neural network has learned to recognize\nextremely different facets of a class, such as the following.\nFrom the movie theater class: the inside of the theater with\nrows of seats and a stage, and the external facade viewed\nat night or day (Fig. 3a); from the pool table class: zoomed\nin pictures of pool balls on green felt, a pool table against a\nwhite background, and pool tables in dark rooms (Fig. 3c);\nfrom the grocery store class: rows of differently colored\nproduce, a scene with a cashier, and the facade of the store\nviewed from the parking lot (Fig. 1); from the bow-tie class,\na tie against a white background, a single person wearing\na bowtie, two people wearing bowties, and even cats wear-\ning bowties (Fig. 3d); in the ﬁshing reel class, zoomed in\npictures of different types of reels, pictures of reels next to\nﬁsh, and pictures of people holding ﬁshing poles against a\nlake or ocean background (Fig. 5). See supplementary info\n(Fig. S2) for many more examples.\nSomewhat surprisingly, some facets of a class are, after\nregularized optimization, actually classiﬁed as members of\nother classes. For example, many real images from the\ntraining set for the grocery store class feature rows of ap-\nples or artichokes, which should arguably instead be in the\nMultifaceted Feature Visualization\napple or artichoke classes (Fig. 1, bottom row).\nDNNs\nmay learn to assign a high probability to both grocery\nstore and apple for such images, and thus correctly classify\nthe image under the “top-5” classiﬁcation accuracy met-\nric (Krizhevsky et al., 2012). Multifaceted feature recon-\nstruction correctly recognizes that different facets of the\ngrocery store class involve each of these types of produce,\nand renders separate synthetic images resembling rows of\napples, artichokes, or oranges. As with the real training\nset images, the DNN labels such synthetic images as both\n“grocery store” and the type of produce. If optimization\nwere carried out without regularization, then following the\ngradient toward the grocery store class would surely assign\nthe highest probability to that class, considering how easy\nit is to steer the network to produce arbitrary outputs when\nthat is the only cost (Szegedy et al., 2013; Nguyen et al.,\n2015; Goodfellow et al., 2014). However, here, when the\noptimization process is regularized, and initialized from\nthe centroid of a cluster of training set images, the DNN\nis sometimes more conﬁdent that an image synthesized to\nlight up its “grocery store” class is a member of another\nclass (e.g. the “apple” class) than a member of the “gro-\ncery store” class! For example, visualizations of different\nfacets of the “grocery store” neuron are most conﬁdently\nlabeled by the DNN as in the “custard apple” and “ﬁg”\nclasses (Fig. 1, top row, leftmost two images), and only\nsecondly labeled as in the grocery store class. These results\nreinforce that MFV automatically discovers and reveals the\nmultifaceted nature of images in a class.\n3.2. Visualizing the multifaceted nature of hidden\nneurons\nIn addition to visualizing the class output neurons (layer fc8\nin this DNN), we can also perform multifaceted fea-\nture visualization for neurons on hidden layers.\nTo\ndo so, we collect the top n = 2% (i.e.\n1000) im-\nages (or image patches, depending on the receptive ﬁeld\nsize of the neuron) from the 50, 000-image validation\nset that most highly activate the neuron, which become\nthe set of images U in Algorithm 1.\nTo determine the\nbest n, we also experimented with a sweep of lower\nn = {0.05%, 0.1%, 0.15%, 0.2%, 0.25%, 0.3%}, but did\nnot observe qualitative improvements (data not shown). To\nvisualize a unit in a fully-connected layer l (e.g. fc6), we\nmodify step 1 of Algorithm 1 to compute the code at l\nfor each image in U (e.g. a vector of 4096 numbers, one\nfor each fc6 neuron). For a convolutional layer unit (e.g.\nconv3), the layer code is the column of features (across\nall channels) at a given spatial location (e.g. the code for\na conv3 unit at location (5, 7) is a vector of 384 numbers,\none for each channel).\nFig. 6 shows multiple facets reconstructed for example hid-\nden units of every layer. Because the overall quality of\n(a) Movie theater: outside (day & night) and inside views.\n(b) Convertible: with different colors and both front & rear views.\n(c) Pool table: Up close & from afar, with different backgrounds.\n(d) Bow tie: on a white background, on one or two people, and on\ncats.\nFigure 3. Multifaceted visualization of fc8 units uncovers inter-\nesting facets. We show 4 different facets for each neuron. In each\npair of images, the bottom is the facet visualization that represents\na cluster of images from the training set, and the top is the closest\nimage to the visualization from the same cluster.\nMultifaceted Feature Visualization\nFigure 4. Visualizing the different facets of a neuron that detects bell peppers. Diverse facets include a single, red bell pepper on a white\nbackground (1), multiple red peppers (5), yellow peppers (8), and green peppers on: the plant (4), a cutting board (6), or against a dark\nbackground (10). Center: training set Images from the bell pepper class are projected into two dimensions by t-SNE and clustered by\nk-means (see Sec. 2.3). Sides: synthetic images generated by multifaceted feature visualization for the “bell pepper” class neuron for\neach of the 10 numbered facets. Best viewed electronically, in color, with zoom.\nFigure 5. Visualizing the different facets of a neuron that detects images in the “ﬁshing reel” class. Diverse facets include reels on\nbackgrounds that are: white (2), dark (3), ocean blue (7) or forest green (8); reels placed next to ﬁsh laying on grass (4), people ﬁshing at\nsea (5), and a speciﬁc type of reel with holes in it (9). Each reconstruction is a facet visualization for a cluster of images in the “ﬁshing\nreel” class. The image components are as described in Fig. 4, except next to each facet visualization, we include the four images in each\nfacet closest to the center of that facet cluster. Best viewed electronically with zoom.\nMultifaceted Feature Visualization\nFigure 6. Multifaceted visualization of example neuron feature detectors from all eight layers of a deep convolutional neural network.\nThe images reﬂect the true sizes of the receptive ﬁelds at different layers. For each neuron, we show visualizations of 4 different\nfacets. These images are hand picked to showcase the diverse neurons and their multifaceted nature. Neurons in layers 1 and 2 do not\nhave noticeably different facets. However, starting from layer 3, the visualizations reveal that neurons at higher layers are increasingly\ncomplex and have diverse facets. This increased facet diversity is because higher level features are more invariant to large changes in\nthe image, such as color, number, pose, and context. Interestingly, units in layer 6 and 7 seem to blend different objects together. For\nexample, the visualization of the leftmost layer 6 neuron seemingly combines a turtle and a scuba diver. It turns out this neuron responds\nto images of “something underwater”, including turtles and scuba divers, but also whales and sharks (Fig. S5c). This is likely because\nsuch neurons are involved in a distributed code of abstract concepts that exist in many different classes of images. At the ﬁnal, 8th, layer,\nwhere neurons are trained to respond separately for each class, the facets are still diverse, but are within a class. Fig. S2 shows many\nmore examples for fc8. Best viewed electronically, in color, with zoom.\nMultifaceted Feature Visualization\nFigure 7. Visualizations of the facet with the most images for 90 example fc8 class neurons that showcase realistic color distributions\nand globally consistent objects. While subjective, we believe the improved color, detail, global consistency, and overall recognizability\nof these images represents the state of the art in visualization using activation maximization (Fig. 8). Moreover, the improved quality\nof the images reveals that even at the highest-level layer, deep neural networks encode much more information about classes than was\npreviously thought, such as the global structure, details, and context of objects. Best viewed in color with zoom.\nMultifaceted Feature Visualization\nFigure 8. Comparing previous state-of-the-art activation maximization methods to the two new methods we propose in this paper: (k)\nmultifaceted visualization (Sec. 3.3) and (i) center-biased regularization (Sec. 2.4). For a fair comparison, the categories were not cherry-\npicked to showcase our best images, but instead were selected based on the images available in previous papers (Simonyan et al., 2013;\nYosinski et al., 2015; Wei et al., 2015). For (e), we reproduced the algorithm of Mahendran et al. (2015), which applies both jitter and\nTV regularizers, although we combined them in a slightly different way (see Sec. S2.1). Interestingly, we found that the visualizations\nproduced by TV alone (g) are just as good as TV+Jitter (e); both are better than previous methods (b-d). Overall, while it is a subjective\njudgement and readers can decide for themselves, we believe that multifaceted feature visualization produces more recognizable images\nwith more natural colors and more realistic global structure.\nMultifaceted Feature Visualization\nthese visualizations is improved (discussed more below),\nit becomes easier to learn what each of these hidden neu-\nrons detects.\nLow-level layers (e.g.\nconv1 and conv2)\ndo not exhibit noticeably different facets. However, start-\ning at conv3, the qualitative difference between facets in-\ncreases: ﬁrst in slight differences of rotation, pose, and\ncolor, then increasingly to the number of objects present,\nand ultimately to very different types of objects (e.g. a\ncloseup of an ostrich face vs. a pair of ostriches viewed\nfrom afar).\nThis result mirrors the known phenomenon\nwhereby features in DNNs become more abstract at higher\nlayers, meaning that they are increasingly invariant to\nlarger and larger changes in the input image (Mahendran\net al., 2015; Bengio et al., 2015).\nAnother previously reported result that can be seen even\nmore clearly in these improved visualizations is that neu-\nrons in convolutional layers often represent only a single,\ncentered object, whereas neurons in fully connected lay-\ners are more likely to contain multiple objects (Yosinski\net al., 2015). Interestingly, and not previously reported to\nour knowledge, neurons in hidden, fully connected layers\noften seem to be an amalgam of very different, high level\nconcepts. For example, one neuron in fc6 looks like a com-\nbination of a scuba diver and a turtle (Fig. 6, leftmost fc6\nneuron). In fact, within the top 15 images that activate\nthat neuron, there are indeed pictures of turtles and scuba\ndivers, but also whales and sharks. Perhaps this neuron is\nbest described as a “something underwater” neuron. This\nresult could occur because our facets are not entirely pure\n(perhaps with a higher k or different optimization objec-\ntives, we would end up with separate t-SNE clusters for\nunderwater turtles, scuba divers, whales, and sharks). An\nalternate, but not mutually exclusive, explanation is that\nthese feature detectors are truly amalgams, or at least rep-\nresent abstract concepts (such as “something underwater”),\nbecause they are used to classify many different types of\nimages. It is not until the ﬁnal layer that units should clas-\nsify speciﬁc types of objects, and indeed our visualizations\nshow that these last-layer (fc8) class neurons are more pure\n(showing different facets of a single concept).\nTo further investigate these two hypotheses, we visualized\nthe input patterns responsible for fc6 and fc7 neuron activa-\ntions via two non-stochastic methods: Deconv (Zeiler et al.,\n2014) and Layer-wise Relevance Propagation (LRP) (Bach\net al., 2015a).\nHowever, these methods did not reveal\nwhether these units are truly amalgams (Sec. S3). Future\nresearch into these questions is necessary.\nFully connected hidden layer neuron reconstructions also\nrevealed cases when, given totally different initialization\nimages (e.g. of a white photocopier and a truck), optimiza-\ntion still converges to the same concept (e.g. “a yellow\ndog”) (Sec. S3 & Fig. S5). One might think that adding a\nL2 penalty for deviating from the mean image in high-level\n(e.g. fc7) code space might prevent such convergence and\nvisualize more facets. Our preliminary experiments with\nthis idea did not produce improvements, but we will con-\ntinue to investigate it in future work.\n3.3. Multifaceted feature visualization improves the\nstate of the art of activation maximization\nBeside uncovering multiple facets of a neuron, our tech-\nnique also improves over the previous state-of-the-art acti-\nvation maximization methods. Figs. 7 & 8 showcase multi-\nfaceted feature visualization on many classes and compare\nit to previous methods. Note that these two ﬁgures only\nshow one facet per class, speciﬁcally the largest among 10\nclusters (i.e. Algorithm 1, with k = 10). While that cluster\nmay be the largest because it contains the most canonical\nimages, it may also be large because it is amorphous and k-\nmeans clustering could not subdivide it. Thus, our results\nmight be even better when visualizing the most visually ho-\nmogenous facet, which we will investigate in future work.\nThe resulting images have a substantially more natural\ncolor palette than those produced by previous algorithms\n(Figs. 7 & 8). That is especially evident in the background\ncolors, which rarely change for other methods from class\nto class. In contrast, with multifaceted feature visualiza-\ntion, it is obvious when an image is set underwater or be-\nneath a clear, blue sky. An additional improvement is that\nthe images are more globally consistent, whereas previous\nmethods had the problem of frequently repeated fragments\nof images that did not form a coherent whole.\nInitializing from mean images may work because it biases\noptimization towards the general color layout of a facet,\nproviding some global structure (Fig. S4 shows example\nmean images). An alternate hypothesis is that the averaging\noperation leaves a translucent version of all n original im-\nages, and optimization recovers one or several of them. In-\ndeed, when initializing with interpolated averages between\ntwo images, optimization snaps to one or the other, instead\nof merging both (Fig. S1). However, these reconstructions\nsnap to the overall facet type of the seed image— mean-\ning the color scheme, global structure, and theme (e.g. os-\ntriches on a grassy plain)—but are not faithful to the details\nof the image (e.g. the number of ostriches). The effect is\nmore pronounced when averaging over 15 images: opti-\nmization often ignores the dominant object outlines that re-\nmain, and instead ﬁlls in different details (e.g. at a different\nscale), but in a way that still ﬁts the context of the overall\ncolor layout (e.g. Fig. S4; note the cheeseburger and milk\ncan are smaller than the mean image suggests). Such ob-\nservations are not conclusive, however, and our paper moti-\nvates future research into the precise dynamics of why ini-\ntializing with a mean facet image improves the quality of\nMultifaceted Feature Visualization\nactivation maximization.\n4. Discussion and Conclusion\nOne way to study a neuron’s different feature facets is to\nsimply perform t-SNE on the real images that maximally\nactivate it (Fig. 4). However, doing so does not reveal what\na neuron knows about a concept or class. Based on “fool-\ning” images, scientists previously concluded that DNNs\ntrained with supervised learning ignore an object’s global\nstructure, and instead only learn a few, discriminative fea-\ntures per class (e.g. color or texture) (Nguyen et al., 2015).\nOur work here strengthens later ﬁndings (Yosinski et al.,\n2015; Mahendran et al., 2015) showing that, in contrast,\nDNNs trained with supervised learning act more like gen-\nerative models by learning the global structure, details, and\ncontext of objects. They also learn their multiple facets.\nActivation maximization can also reveal what a DNN ig-\nnores when classifying. A reason that our method does not\nalways reconstruct a proper facet, e.g. of stuffed peppers\n(Fig. 4, cluster 7), could be that the bell pepper neuron ig-\nnores, or lightly weights, stufﬁng. The number of unique\nimages MFV produces for a unit depends on the k in k-\nmeans (here, manually set). Automatically identifying the\ntrue number of facets is an important, open scientiﬁc ques-\ntion raised, but not answered, by this paper.\nOverall, we have introduced a simple Multifaceted Feature\nVisualization algorithm, which (1) improves the state of the\nart of activation maximization by producing higher quality\nimages with more global structure, details, context, more\nnatural colors, and (2) shows the multiple feature facets\neach neuron detects, which provides a more comprehen-\nsive understanding of each neuron’s function. We also in-\ntroduced a novel center-biased regularization technique,\nwhich reduces optimization’s tendency to produce repeated\nobject fragments and instead tends to produce one central\nobject. Such improved Deep Visualization techniques will\nincrease our understanding of deep neural networks, which\nwill in turn improve our ability to create even more power-\nful deep learning algorithms.\nAcknowledgements\nWe thank Alexey Dosovitskiy for helpful discussions, and\nChristopher Stanton, Joost Huizinga, Richard Yang &\nCameron Wunder for editing. Jeff Clune was supported\nby an NSF CAREER award (CAREER: 1453549) and a\nhardware donation from the NVIDIA Corporation. Jason\nYosinski was supported by the NASA Space Technology\nResearch Fellowship and NSF grant 1527232.\nMultifaceted Feature Visualization\nReferences\nBach, Sebastian, Binder, Alexander, Montavon, Gr´egoire,\nKlauschen,\nFrederick,\nM¨uller,\nKlaus-Robert,\nand\nSamek, Wojciech. On pixel-wise explanations for non-\nlinear classiﬁer decisions by layer-wise relevance propa-\ngation. PloS one, 10(7), 2015a.\nBach, Sebastian, Binder, Alexander, Montavon, Gr´egoire,\nM¨uller, Klaus-Robert, and Samek, Wojciech. Lrp tool-\nbox for artiﬁcial neural networks 1.0–manual. 2015b.\nBengio, Yoshua, Goodfellow, Ian J., and Courville, Aaron.\nDeep learning.\nBook in preparation for MIT Press,\n2015.\nURL http://www.iro.umontreal.ca/\n˜bengioy/dlbook.\nDeng, Jia, Dong, Wei, Socher, Richard, Li, Li-Jia, Li, Kai,\nand Fei-Fei, Li.\nImagenet: A large-scale hierarchical\nimage database. In Computer Vision and Pattern Recog-\nnition, 2009. CVPR 2009. IEEE Conference on, pp. 248–\n255. IEEE, 2009.\nDosovitskiy, Alexey and Brox, Thomas.\nInverting vi-\nsual representations with convolutional networks. arXiv\npreprint arXiv:1506.02753, 2015.\nErhan, Dumitru, Bengio, Yoshua, Courville, Aaron, and\nVincent, Pascal. Visualizing higher-layer features of a\ndeep network. Dept. IRO, Universit´e de Montr´eal, Tech.\nRep, 4323, 2009.\nGetreuer, Pascal.\nRudin-osher-fatemi total variation de-\nnoising using split bregman. Image Processing On Line,\n10, 2012.\nGoldstein, Tom and Osher, Stanley.\nThe split bregman\nmethod for l1-regularized problems. SIAM Journal on\nImaging Sciences, 2(2):323–343, 2009.\nGoodfellow, Ian J, Shlens, Jonathon, and Szegedy, Chris-\ntian. Explaining and Harnessing Adversarial Examples.\nArXiv e-prints, December 2014.\nHe, Kaiming, Zhang, Xiangyu, Ren, Shaoqing, and Sun,\nJian. Deep residual learning for image recognition. arXiv\npreprint arXiv:1512.03385, 2015.\nJia, Yangqing, Shelhamer, Evan, Donahue, Jeff, Karayev,\nSergey, Long, Jonathan, Girshick, Ross, Guadarrama,\nSergio, and Darrell, Trevor.\nCaffe: Convolutional ar-\nchitecture for fast feature embedding.\narXiv preprint\narXiv:1408.5093, 2014.\nJolliffe, Ian. Principal component analysis. Wiley Online\nLibrary, 2002.\nKarpathy, Andrej and Fei-Fei, Li. Deep visual-semantic\nalignments for generating image descriptions.\narXiv\npreprint arXiv:1412.2306, 2014.\nKarpathy, Andrej, Johnson, Justin, and Li, Fei-Fei.\nVi-\nsualizing and understanding recurrent networks. arXiv\npreprint arXiv:1506.02078, 2015.\nKrizhevsky, Alex, Sutskever, Ilya, and Hinton, Geoffrey E.\nImagenet classiﬁcation with deep convolutional neural\nnetworks. In Advances in neural information processing\nsystems, pp. 1097–1105, 2012.\nMacQueen, James et al. Some methods for classiﬁcation\nand analysis of multivariate observations. In Proceed-\nings of the ﬁfth Berkeley symposium on mathematical\nstatistics and probability, volume 1, pp. 281–297. Oak-\nland, CA, USA., 1967.\nMahendran, Aravindh and Vedaldi, Andrea.\nVisualizing\ndeep convolutional neural networks using natural pre-\nimages. arXiv preprint arXiv:1512.02017, 2015.\nMordvintsev, Alexander, Olah, Christopher, and Tyka,\nMike. Inceptionism: Going deeper into neural networks.\nGoogle Research Blog. Retrieved June, 20, 2015.\nNguyen, Anh, Yosinski, Jason, and Clune, Jeff. Deep neu-\nral networks are easily fooled: High conﬁdence predic-\ntions for unrecognizable images. In Proc. of the Con-\nference on Computer Vision and Pattern Recognition,\n2015.\nPerson, K.\nOn lines and planes of closest ﬁt to system\nof points in space. philiosophical magazine, 2, 559-572,\n1901.\nQuiroga, R Quian, Reddy, Leila, Kreiman, Gabriel, Koch,\nChristof, and Fried, Itzhak. Invariant visual representa-\ntion by single neurons in the human brain. Nature, 435\n(7045):1102–1107, 2005.\nRussakovsky, Olga, Deng, Jia, Su, Hao, Krause, Jonathan,\nSatheesh, Sanjeev, Ma, Sean, Huang, Zhiheng, Karpa-\nthy, Andrej, Khosla, Aditya, Bernstein, Michael, et al.\nImagenet large scale visual recognition challenge. arXiv\npreprint arXiv:1409.0575, 2014.\nSamek, Wojciech, Binder, Alexander, Montavon, Gr´egoire,\nBach, Sebastian, and M¨uller, Klaus-Robert.\nEvaluat-\ning the visualization of what a deep neural network has\nlearned. arXiv preprint arXiv:1509.06321, 2015.\nSimonyan, Karen, Vedaldi, Andrea, and Zisserman, An-\ndrew. Deep inside convolutional networks: Visualising\nimage classiﬁcation models and saliency maps. arXiv\npreprint arXiv:1312.6034, 2013.\nStrong, David and Chan, Tony. Edge-preserving and scale-\ndependent properties of total variation regularization. In-\nverse problems, 19(6):S165, 2003.\nMultifaceted Feature Visualization\nSzegedy, Christian, Zaremba, Wojciech, Sutskever, Ilya,\nBruna, Joan, Erhan, Dumitru, Goodfellow, Ian J., and\nFergus, Rob. Intriguing properties of neural networks.\nCoRR, abs/1312.6199, 2013.\nSzegedy, Christian, Liu, Wei, Jia, Yangqing, Sermanet,\nPierre, Reed, Scott, Anguelov, Dragomir, Erhan, Du-\nmitru, Vanhoucke, Vincent, and Rabinovich, Andrew.\nGoing deeper with convolutions.\narXiv preprint\narXiv:1409.4842, 2014.\nVan der Maaten, Laurens and Hinton, Geoffrey. Visualizing\ndata using t-sne. Journal of Machine Learning Research,\n9(2579-2605):85, 2008.\nWei, Donglai, Zhou, Bolei, Torrabla, Antonio, and Free-\nman, William. Understanding intra-class knowledge in-\nside cnn. arXiv preprint arXiv:1507.02379, 2015.\nYosinski,\nJason,\nClune,\nJeff,\nNguyen,\nAnh,\nFuchs,\nThomas, and Lipson, Hod.\nUnderstanding neural\nnetworks through deep visualization.\narXiv preprint\narXiv:1506.06579, 2015.\nZeiler, Matthew D and Fergus, Rob. Visualizing and under-\nstanding convolutional networks. In Computer Vision–\nECCV 2014, pp. 818–833. Springer, 2014.\nSupplementary Information for:\nMultifaceted Feature Visualization: Uncovering the Different Types of Features\nLearned By Each Neuron in Deep Neural Networks\nAnh Nguyen\nANGUYEN8@UWYO.EDU\nJason Yosinski\nYOSINSKI@CS.CORNELL.EDU\nJeff Clune\nJEFFCLUNE@UWYO.EDU\nS1. Activation maximization initialized with\ninterpolated images\nWe investigated whether optimization starting from a mean\nimage that is the average of two different types of images\n(i.e. two different facets) would produce either (a) a visu-\nalization that resembles a mix of the two facets, or (b) a\nreconstruction of only one of the facets, which could occur\nif one facet win outs and optimization gravitates toward it,\nerasing the other facet.\nWe ﬁrst take all ∼1,300 of the images in a training set class\n(e.g. the ostrich class) and run Algorithm 1 (step 1-4) with\nk = 10 to produce 10 clusters of images. We select two\nrandom images, each from a different cluster, and gener-\nate a series of 8 intermediate linearly interpolated images\nbetween those two images (e.g. in the top row of Fig. S1a,\nthe leftmost and rightmost are two training set images). We\nthen run activation maximization with the same parameters\nas in the multifaceted visualization experiments in the main\ntext (Sec. 2.3), with the initialization being each of the in-\nterpolated images. Each panel in Fig. S1 shows a series of\ninterpolated images (top row) and the corresponding acti-\nvation maximization results (bottom row).\nWe found that when starting from a single real image,\nsometimes optimization produces a visualization that ﬁts\nin the same facet as the initial image, but with different de-\ntails. For example, in Fig. S1a, the leftmost and rightmost\nsynthetic images are started with the (non-modiﬁed) train-\ning set image above them and reproduce a similar type of\nimage (large ostrich heads against a blue sky or ostriches on\na grassy plain from afar), but the details differ. Because of\nregularization (TV and jitter) and optimization, activation\nmaximization completely redraws the main subject, and ac-\ntually produces more than one ostrich in both cases (zoom\nin to see more easily).\nIn other cases, optimization does not take the guide from\nthe initial image, but instead converges to a different facet\naltogether. For example, when seeded with either a stuffed\nor cut-open pepper, optimization instead seemingly pro-\nduces a whole pepper (Fig. S1c). This could happen be-\ncause a “stuffed pepper” facet and a regular “whole pepper”\nfacet share a lot of common details (reﬂective skin, colors),\nthus, the bias in the seed image might not be strong enough\nto pull the optimization toward the stuffed pepper facet.\nAnother hypothesis is that the DNN has never learned to\nassociate the stufﬁng detail with the bell pepper concept. A\nﬁnal hypothesis is that whole peppers are much more com-\nmon, making their basin of attraction much larger and the\ngradients toward them steeper.\nOverall, we observe that optimization often reconstructs\none facet or another, but not a hybrid of both (Figs. S1a\n& S1b). Interestingly, and for unknown reasons, in the ﬁsh-\ning reel example, optimization did not produce the ‘ﬁsher-\nman against water and sky from afar’ facet until the 3rd\ninterpolated image (Fig. S1b, 3rd image from the left). In-\nstead, in the leftmost two images of Fig. S1b, optimization\nproduced a totally different, and common, facet of a close-\nup of a ﬁshing reel (Fig. 5, most facets).\nS2. Comparison between different priors\nMany regularizers have been proposed to improve the qual-\nity of activation maximization images. Here, we compare\nthe beneﬁts of incorporating some of the leading regulariz-\ners from previous activation maximization papers.\nS2.1. Total variation\nOptimizing images via gradient descent to maximize the\nactivation of a neuron only, without any regularization,\nproduces unrecognizable images with overly high fre-\nquency information (Yosinski et al., 2015; Nguyen et al.,\n2015). Yosinski et al. (2015) proposed incorporating Gaus-\nsian blur with a small radius to smooth out the image.\nWhile this technique improves recognizability, it causes an\noverly blurry image because sharp edges are blurred away\n(Fig. 8c). Total variation (TV) regularization is a different\nsmoothing technique that combats this issue by minimizing\nthe total variation across adjacent pixels in an image while\nSupplementary Information for: Multifaceted Feature Visualization\n(a) Optimization is pulled toward one facet or the other instead of visualizing a combination of both. Interestingly, even when optimiza-\ntion is started from a single image (left and right extremes), optimization and regularization combine to produce an image in the same\nstyle (ostrich heads against a blue sky or ostriches on a grassy plain from afar), but with different details (e.g. the number of ostriches).\n(b) Optimization either draws a ﬁsherman against water and sky or a ﬁsh on grass. Interestingly, in the ﬁsh images, which are generated\nto maximize the reel class, optimization adds additional ﬁsh to the one in the seed image, instead of adding ﬁshing reels.\n(c) Optimization seems to converge to neither of the two facets, and instead produces a whole pepper (with inconsistent coloring). See\ntext in Sec. S1 for hypothesis as to why this may occur.\nFigure S1. Optimization seeded with interpolated real images (top rows) often reconstructs either one facet or the other, but not a hybrid\nof both (bottom rows).\npreserving the sharpness of edges (Strong et al., 2003). In\ncombination with other regularizers such as α-norm, this\nprior is effective in both code inversion and activation max-\nimization tasks (Mahendran et al., 2015).\nWhile Mahendran et al. (2015) incorporated the TV norm\nas a penalty in the objective function (Eq. 1), here we solve\nthe TV minimization problem separately from the activa-\ntion maximization problem. Speciﬁcally, each of our opti-\nmization iterations has two steps: (1) update the image x in\nthe direction that maximizes the activation of a given neu-\nron; (2) ﬁnd an image that is closest to x that has the small-\nest TV via an iterative split Bregman optimization (Gold-\nstein et al., 2009). Split Bregman is an algorithm that is de-\nsigned to ﬂexibly solve non-differential convex minimiza-\ntion problems by splitting the objective terms, and is es-\npecially efﬁcient for TV regularization (Getreuer, 2012).\nFor each iteration of maximizing the neural activation, we\nperform 100 iterations of minimizing TV. Empirically, we\nfound this strategy more ﬂexible, which allowed us to dis-\ncover slightly better results than the method in Mahendran\net al., 2015.\nIn extensive preliminary activation maximization experi-\nments (data not shown), we found that fairly good visu-\nalizations emerge by applying TV regularization only. Our\nresults with TV alone (Fig. 8g) were not qualitatively im-\nproved by adding jitter (Fig. 8e), and were better than meth-\nSupplementary Information for: Multifaceted Feature Visualization\nods that predate Mahendran et al., 2015 (Fig. 8b-d).\nS2.2. Jitter\nAn optimization regularization technique called “jitter”\nwas ﬁrst introduced by Mordvintsev et al., 2015 and later\nused in Mahendran et al., 2015. The method involves: (1)\ncreating a canvas (e.g. of size 272 × 272) that is larger\nthan the DNN input size (227 × 227) and (2) iteratively\noptimizing random 227 × 227 regions on the canvas. Op-\ntimization with jitter often results in high-resolution, crisp\nimages; however, it does not ameliorate the problems of\nunnatural coloration and the repetition of image fragments\nthat do not form a coherent, sensible whole.\nBecause we believe it represents the best previous activa-\ntion maximization technique, we reproduce the algorithm\nof Mahendran et al. (2015), which applies both jitter and\nTV regularizers (Figs. S4a & 8e). Mahendran et al. (2015)\nreport that TV is the most important prior in their frame-\nwork. We found that Gaussian blur works just as well if\ncombined with jitter while gradually reducing the blurring\neffect (i.e. radius) during optimization (Fig. 8h). This com-\nbination works because: (1) as shown in Yosinski et al.\n(2015), the smoothing effect by Gaussian blur enables op-\ntimization to ﬁnd better local optima that have more global\nstructure; (2) reducing the blur radius over time minimizes\nthe blurring artifact of this prior in the ﬁnal result; and (3)\njitter further enhances the sharpness of images.\nS2.3. Center-biased regularization\nPrevious activation maximization methods produce images\nwith many unnatural repeated image fragments (Fig. 8b-h).\nThough to a lesser degree, multifaceted feature visualiza-\ntion also produces such repetitions (Fig. 8k). Such rep-\netitions are not found in the vast majority of training set\nimages. For example, a canonical training set image in the\n“beacon” class often shows a single lighthouse (Fig. 8a);\nhowever, Deep Visualization techniques show many more\nbeacons or patches of beacons (Fig. 8b-h, k). To ameliorate\nthis issue, in this section we introduce a technique called\ncenter-biased regularization.\nOur method builds upon the idea of combining TV\n(Sec. S2.1) and jitter (Sec. S2.2) regularizers, follow-\ning (Mahendran et al., 2015), and adds an additional bias\ntowards a restricted “drawing” region in the center of the\nimage. In a preliminary experiment (data not shown), we\nfound that optimization with a large smoothing effect (e.g.\na high Gaussian blur radius or TV weight) often results in\na blurry, but single and centered object in the visualiza-\ntion. Based on this observation, the intuition behind our\ntechnique is to ﬁrst generate a blurry, centered-object im-\nage (Fig. S3, leftmost image), and then optimize the center\npixels more than the edge pixels to produce a ﬁnal image\nthat is sharp and has a centrally-located image (Fig. S3, 4\nright images). Multiple examples for different classes are\nshown in Fig. S4b.\nFigure S3. Progressive result of optimizing an image to activate\nthe “milk can” neuron via center-biased regularization. Each im-\nage is the result of one out of ﬁve optimization phases. This ﬁgure\nis also shown in the main text (Fig. 2).\nOPTIMIZATION SCHEDULE AND PARAMETERS\nSpeciﬁcally, our optimization schedule has ﬁve phases\n(Fig. S3) that each have a different set of parameters. As\ndescribed in Sec. S2.1, each of our optimization iterations\nhas two steps: (1) update the image x in the direction\nthat maximizes the activation of a given neuron; (2) ﬁnd a\nsmoothed image xs that is closest to x that has the smallest\nTV. This xs will become the initial image x in the next op-\ntimization iteration. We begin by describing the ﬁrst three\nphases, which are the most important. Each runs for 150\niterations.\nPhase 1-3: First, to generate a blurry image, we start with a\nlow L2 regularization parameter λ = 0.001 when ﬁnding a\nsmoothed image xs (step 2). A large λ forces xs to be close\nto x, and results in a sharp image; while a small λ allows xs\nto be far from x, and results in a smoothed image. Specif-\nically, for the ﬁrst 3 phases, λ = {0.001, 0.08, 2}. We\nalso use a lower learning rate for each phase when updating\nimage x in the activation maximization direction (step 1):\n11, 6, 1. The intuition is to force the optimization to lock\nin on the object (e.g. a milk can) that appears in phase 1\n(Fig. S3, leftmost). In other words, we try to minimize the\nchance of new duplicated fragments of milk cans to appear\ntoward the end of the optimization as seen in Fig. S4a.\nTo bias the main object to appear in the center, for each\nphase, we increase the canvas size (i.e. upsampling the\nimage x by 20% at the beginning of each phase) to be:\n227 × 227, 272 × 272, and 327 × 327. The regular jit-\ntering approach involves sampling and sending a random\n227 × 227 patch (anywhere across the canvas) to the DNN\nfor optimization. Here, we restrict such sampling so that\nthe center of a patch is within a small canvas-centered\nsquare. By the end of phase 3, the visualization often has a\nsingle, centered object (Fig. S3, phase 3).\nPhase 4-5: The purpose of phase 4 and 5 is to sharpen\nthe centered object without generating new duplicated frag-\nments. We attempt to do this by center-cropping the gra-\ndient image (i.e.\nthe gradient backpropagated from the\nSupplementary Information for: Multifaceted Feature Visualization\nDNN has the form of a 3 × 227 × 227 image) down to\n3 × 127 × 127. In addition, in phase 4, we restrict the op-\ntimization to the center of the image only, and optimize for\n30 iterations. That sharpens the region in the image center\n(Fig. S3, phase 4). Finally, to balance out the sharpness be-\ntween the center and edge pixels, in phase 5, we optimize\nfor 10 iterations while allowing jittering to occur anywhere\nin the image. Thus, the ﬁnal visualization is often a sharper\nversion of the phase 3 result (Fig. S3, phase 5 vs phase 3).\nThe center-biased regularized images have far fewer du-\nplicated fragments (Fig. 8i). They thus more closely rep-\nresent the style of training set images, which feature one\ncentrally located object. However, this technique is not\nguaranteed to produce a single object only. Instead, it is\na way of biasing optimization toward creating objects near\nthe image center. Sec. S2.4 shows the experiment combin-\ning center-biased regularization and multifaceted visualiza-\ntion (initializing from mean images) to further improve the\nvisualization quality.\nS2.4. Initialization with mean images\nAn issue with previous activation maximization methods is\nthat the images tend to have an unrealistic color distribu-\ntion (Fig. 8b-c, e-h). A straightforward approach to ame-\nliorate this problem is to incorporate an α-norm regularizer,\nwhich encourages the intensity of pixels to stay within a\ngiven range (Mahendran et al., 2015; Yosinski et al., 2015).\nWhile this method is effective in suppressing extreme color\nvalues, it does not improve the realism of the overall color\ndistribution of images. Wei et al., 2015 proposed a more ad-\nvanced data-driven patch prior regularization to enforce the\nvisualizations to match the colors of a set of natural image\npatches. While this prior substantially improved the colors\nfor code inversion, its results for activation maximization\nstill have several issues (as seen in Fig. 8d): (1) having du-\nplicated fragments (e.g. duplicated patches of lighthouses\nin a “beacon” image), and (2) lacking details, producing\nunnatural images.\nHere, our multifaceted visualization improves the color dis-\ntribution of images via a different approach: starting op-\ntimization from a mean image (Fig. S4d) computed from\nreal training examples (see Sec. 3.3). A possible explana-\ntion for why this works is that initializing from the mean\nimage puts the search in a low-dimensional manifold that\nis much closer to that of real images, and thus it is eas-\nier to ﬁnd a realistic looking image around this area. The\nmean image thus provides a general outline for a type of\nimage based on blurry colors and optimization can ﬁll in\nthe details. For example, the center-biased regularization\ntechnique was able to produce a single milk can, but did so\nwithout a relevant contextual setting (Fig. S4b). However,\nwhen this technique is initialized with a mean image (com-\nputed from 15 images from the “milk can” class) that has\na blurry brown surface (Fig. S4d, milk can), optimization\nturned that general layout into a complete, coherent picture:\na milk can on a table (Fig. S4c). The colors of the visual-\nizations are also substantially improved when optimization\nis initialized from mean images (Fig. S4b versus Fig. S4c).\nWe observed that multifaceted visualization images often\nexhibit centered objects (Fig. 1 & 3), and thus we found no\nsubstantial qualitative improvement when adding center-\nbiased regularization to it (Fig. S4c). That could be because\nthe mean image provides enough of a global layout for the\nstyle of the image, which in ImageNet is usually a single,\ncentered object.\nWhile simple, our technique results in images that have\nqualitatively more realistic colors than previous methods\n(Figs. 7 & 8k). This technique is also expected to work\nwith any dataset (e.g. grayscale images, or images of a\nspecial topic).\nNote that multifaceted feature visualization does not re-\nquire access to the training set. If the training set is un-\navailable, one can simply pass any natural images (or other\nmodes of input such as audio if not reconstructing images)\nto get a set of images (or other input types) that highly acti-\nvate a neuron. A similar idea was used in Wei et al. (2015),\nwho built an external dataset of patches that have similar\ncharacteristics to the DNN training set.\nSupplementary Information for: Multifaceted Feature Visualization\nFigure S2. Four facets each for example class output neurons (fc8) produced by multifaceted feature visualization. These images are hand\npicked to showcase the multifaceted nature of neurons across a variety of classes from natural to man-made subjects. The reconstructions\nare produced by applying Algorithm 1 with k = 10 on the ImageNet 2012 validation set. Best viewed electronically with zoom.\nSupplementary Information for: Multifaceted Feature Visualization\n(a) Total variation + Jitter (Mahendran et al., 2015)\n(b) Our Center-biased regularization method.\n(c) Center-biased regularization + multifaceted visualization.\n(d) Example mean images that serve as the initialization images\nfor multifaceted feature visualization. Speciﬁcally, these were the\nseeds for the visualizations in (c).\nFigure S4. (a) Previous state of the art activation maximization algorithms produce images with many repeated object fragments that do\nnot form a coherent whole or look natural. (b) Center-biased regularization produces images that have fewer repeated fragments, which\nboth represents the general style of images in the training set (one centered object) and make it easier to interpret and understand the\nfeature the neuron in question detects. (c) The realism of the colors of these images are improved when combined with multifaceted\nvisualization.\nSupplementary Information for: Multifaceted Feature Visualization\nS3. What are the hidden units in fully\nconnected layers for?\nWe reported in the main text (Sec. 3.2) that neurons in hid-\nden, fully connected layers often seem to be an amalgam\nof very different, abstract concepts. For example, a recon-\nstruction of one of the facets of a neuron in fc6 looks like\na combination of a scuba diver and a turtle (Fig. 6, left-\nmost fc6 neuron). Here, we document a series of visual-\nization experiments that we performed to further shed light\non the inner-workings of hidden, fully connected neurons\n(in our model, the neurons on fc6 and fc7), such as those\nvisualized in Fig. 6.\nThe reconstructions for neurons in hidden layers (Fig. 6)\nwere produced by running Algorithm 1 with k = 10 clus-\nters on the top 1000 validation set images that most highly\nactivate each hidden neuron. To understand what feature\na neuron ﬁres in response to, we can look at the differ-\nent types of images that highly activate it (i.e. its differ-\nent facets). Fig. S5 shows, for each cluster of images that\nhighly activate a neuron, 4 random images from that clus-\nter. Speciﬁcally, we show 4 random images from the set\nof 15 images that were averaged to compute the mean im-\nage that optimization is initialized with when visualizing\nthat facet. This visualization method is similar to the ap-\nproach of visualizing the top 9 images that activate a neu-\nron from Zeiler et al. (2014), except it is per facet.\nWe observe that many individual fc6 and fc7 neurons ﬁre\nfor very different types of images. For example, the fc6\nunit that resembles a combination of a scuba diver and a\nturtle (Fig. S5c, left neuron) does indeed ﬁre for turtles and\nscuba divers, but also for sharks, bells, human faces and\neven trucks. Even when multifaceted feature visualization\nis seeded with a mean image of very different concepts,\nsuch as human faces or automobiles, optimization for this\nneuron consistently converges to very similar images that\nresemble a turtle with goggles (Fig. S5c, left).\nIn contrast, and as we would expect given that they are\ntrained to ﬁre in response to one type of image, neurons\non the fc8 layer more clearly represent the same seman-\ntic concept. For these neurons, unlike with fc6 and fc7\nneurons, multifaceted feature visualization produces very\nunique facet reconstructions for fc8 neurons that represent\ndifferent facets of the same semantic concept (Fig. S2). For\nexample, the “restaurant” class neuron responds to inte-\nrior views of an entire restaurant in daylight and at night\n(Fig. S5a, top two rows), and to close-up views of plates of\nfood (Fig. S5a, bottom two rows). In these cases, and also\nfor convolutional layers, multifaceted feature reconstruc-\ntions closely reﬂect the content of the real images in each\nfacet. In other words, optimization stays near the initial\nimage it is seeded with.\nIn some cases, however, even fc8 neurons respond to an odd\nassortment of different types of images. For example, the\n“ostrich” class neuron ﬁres highly for many non-ostrich im-\nages like leopards and lizards (Fig. S5a, right panel). Even\nwhen viewing the top 9 images that activate a class neuron\n(ignoring facets), there is usually at least one image from\nanother class, even though there are at least 50 images from\nthat class in the 50, 000-image validation set. Because our\nMFV algorithm performs clusters on far more than 9 im-\nages, some of these clusters will represent an entire group\nof images that are not of that class (e.g. a “lizard” facet for\nthe ostrich neuron). Often, when MFV initializes optimiza-\ntion with the mean image of those non-class images, opti-\nmization “walks away” from that starting point to produce\nan image of the class: many examples of this are shown in\nFig. S5, such as optimization producing images of ostriches\neven when seeded with a mean image from a facet of non-\nostrich birds or even a facet composed mostly of wild dogs,\ncows, and giraffes.\nWhile informative, these cluster images could also be mis-\nleading because it is not clear whether a given neuron ﬁres\nfor very different types of images (Fig. S5c, photocopiers,\ntrucks, dogs, etc.) or whether that neuron instead ﬁres be-\ncause there is a common feature present in those different\nimages. We attempted to visualize the pixels in each im-\nage that are most responsible for that neuron ﬁring via the\ndeconvolution technique (Zeiler et al., 2014) and via Layer-\nwise Relevance Propagation (LRP) (Bach et al., 2015a)\n(Fig. S6). The Deconv results are produced via the Deep-\nVis toolbox (Yosinski et al., 2015), and the LRP results are\nproduced by the LRP toolbox (Bach et al., 2015b).\nUnfortunately, deconvolution visualizations often contain a\nlot of noise that makes it hard to identify the most impor-\ntant regions in an image (Fig. S6, bottom row). This ob-\nservation agrees with a recent evaluation of deconv (Samek\net al., 2015). For this reason, we also visualize the images\nwith LRP algorithm. The LRP heatmaps are also difﬁcult\nto interpret. For example, for the fc6 “scuba diver and tur-\ntle” neuron, they show that the outlines of very different\nshapes cause this neuron to ﬁre, including shapes of under-\nwater things like scuba divers, sharks and turtles (Fig. S6a,\ntop row in LRP panel), but also a metal bell, human heads,\nand automobile wheels (Fig. S6a, LRP panel). It is possible\nthat the neuron cares about the semantics of those disparate\nobject types, or simply that it ﬁres in response to a circu-\nlar outline pattern that vaguely resembles goggles, which is\npresent in most of these images (albeit at different scales).\nAnother mutually exclusive hypothesis for why the recon-\nstructions are not easily interpretable is that these neurons\nare part of highly distributed representations. Ultimately, it\nis unclear exactly what feature this neuron detects.\nOur conclusion remains the same for many other fc6 and\nSupplementary Information for: Multifaceted Feature Visualization\nfc7 units (see also Fig. S6b): while it is unclear exactly\nwhat they represent, these neurons do seem to represent an\namalgam of different types of images, and optimization of-\nten produces the same hybrid visualization no matter where\nit is started from, such as the yellow dog+bathtub on fc6\n(Fig. S5c). Other times there is a dominant visualization\nthat also appears no matter where optimization starts, but\nthat is more semantically homogeneous (instead of being a\nhybrid of very different concepts), such as an arch or quail-\nlike bird (Fig. S5b).\nOverall, the evidence in this section reveals how little we\nknow about the precise function of hidden neurons in fully\nconnected layers, and future research is required to make\nprogress on this interesting question.\nSupplementary Information for: Multifaceted Feature Visualization\n(a) fc8 units: “restaurant” (left) and “ostrich” (right).\n(b) fc7 units indexed 159 (left) and 466 (right).\n(c) fc6 units indexed 22 (left) and 461 (right).\nFigure S5. To further shed light on the facet reconstructions of fc6 and fc7 neurons discussed in the main text (Fig. 6), we investigate\ntwo neurons from each of those layers. For each neuron, we show 4 different facets (one per row). For each facet (row), the leftmost\nimage is produced by multifaceted feature visualization; next to it are 4 random images from the 15 validation set images that are used\nfor computing the mean image for that facet. Unlike fc8 units, the hidden fully connected neurons on fc6 and fc7 tend to respond to a\nvariety of different concepts (e.g. from animals to automobiles), and all of their facet reconstructions often converge to similar images.\nSupplementary Information for: Multifaceted Feature Visualization\n(a) fc6 unit that resembles “a scuba diver and a turtle”.\n(b) fc7 unit that resembles an arch.\nFigure S6. Visualizing the pixels that are responsible for the activations of an example neuron from fc6 and another from fc7 via Layer-\nwise Relevance Propagation (middle panels) (Bach et al., 2015a) and deconvolution (bottom panels) (Zeiler et al., 2014). See the caption\nof Fig. S5 for a description of the top panels. Layer-wise Relevance Propagation heatmaps show that these two neurons indeed ﬁre for\na variety of objects, supporting any of the following hypotheses: that the neurons represent an amalgam of different concepts, are part\nof a distributed representation, or represent a non-obvious abstract concept such as “curved lines” or “curved lines in the vague shape of\ngoggles.” The Deconv visualizations are quite noisy, making it hard to conclusively identify the most important regions in each image.\n",
        "sentence": " further try multifaceted visualization to separate and visualize different features that a neuron learns [16]."
    },
    {
        "title": "Understanding intra-class knowledge inside CNN",
        "author": [
            "Donglai Wei",
            "Bolei Zhou",
            "Antonio Torralba",
            "William T. Freeman"
        ],
        "venue": "arXiv preprint arXiv:1507.02379,",
        "citeRegEx": "17",
        "shortCiteRegEx": "17",
        "year": 2015,
        "abstract": "",
        "full_text": "",
        "sentence": " In addition, some regularizers are incorporated as a natural image prior to improve the visualization quality, including α−norm [6], total variation[7], jitter[7], Gaussian blur [9], data-driven patch priors [17], etc."
    },
    {
        "title": "Generating images with perceptual similarity metrics based on deep networks",
        "author": [
            "Alexey Dosovitskiy",
            "Thomas Brox"
        ],
        "venue": "arXiv preprint arXiv:1602.02644,",
        "citeRegEx": "18",
        "shortCiteRegEx": "18",
        "year": 2016,
        "abstract": "Image-generating machine learning models are typically trained with loss\nfunctions based on distance in the image space. This often leads to\nover-smoothed results. We propose a class of loss functions, which we call deep\nperceptual similarity metrics (DeePSiM), that mitigate this problem. Instead of\ncomputing distances in the image space, we compute distances between image\nfeatures extracted by deep neural networks. This metric better reflects\nperceptually similarity of images and thus leads to better results. We show\nthree applications: autoencoder training, a modification of a variational\nautoencoder, and inversion of deep convolutional networks. In all cases, the\ngenerated images look sharp and resemble natural images.",
        "full_text": "Generating Images with Perceptual Similarity Metrics based on Deep Networks\nAlexey Dosovitskiy\nDOSOVITS@CS.UNI-FREIBURG.DE\nUniversity of Freiburg, Germany\nThomas Brox\nBROX@CS.UNI-FREIBURG.DE\nUniversity of Freiburg, Germany\nAbstract\nImage-generating machine learning models are\ntypically trained with loss functions based on dis-\ntance in the image space.\nThis often leads to\nover-smoothed results.\nWe propose a class of\nloss functions, which we call deep perceptual\nsimilarity metrics (DeePSiM), that mitigate this\nproblem. Instead of computing distances in the\nimage space, we compute distances between im-\nage features extracted by deep neural networks.\nThis metric better reﬂects perceptually similarity\nof images and thus leads to better results. We\nshow three applications: autoencoder training, a\nmodiﬁcation of a variational autoencoder, and in-\nversion of deep convolutional networks. In all\ncases, the generated images look sharp and re-\nsemble natural images.\n1. Introduction\nRecently there has been a surge of interest in training neu-\nral networks to generate images.\nThese are being used\nfor a wide variety of applications: unsupervised and semi-\nsupervised learning, generative models, analysis of learned\nrepresentations, analysis by synthesis, learning of 3D rep-\nresentations, future prediction in videos.\nNevertheless,\nthere is little work on studying loss functions which are\nappropriate for the image generation task. Typically used\nsquared Euclidean distance between images often yields\nblurry results, see Fig.1b. This is especially the case when\nthere is inherent uncertainty in the prediction. For example,\nsuppose we aim to reconstruct an image from its feature\nrepresentation. The precise location of all details may not\nbe preserved in the features. A loss in image space leads\nto averaging all likely locations of details, and hence the\nreconstruction looks blurry.\nHowever, exact locations of all ﬁne details are not impor-\ntant for perceptual similarity of images. But the distribution\nof these details plays a key role. Our main insight is that in-\nvariance to irrelevant transformations and sensitivity to lo-\ncal image statistics can be achieved by measuring distances\nin a suitable feature space. In fact, convolutional networks\nprovide a feature representation with desirable properties.\nThey are invariant to small smooth deformations, but sensi-\ntive to perceptually important image properties, for exam-\nple sharp edges and textures.\nUsing a distance in feature space alone, however, does not\nyet yield a good loss function; see Fig. 1d. Since feature\nrepresentations are typically contractive, many images, in-\ncluding non-natural ones, get mapped to the same feature\nvector. Hence, we must introduce a natural image prior.\nTo this end, we build upon adversarial training as proposed\nby Goodfellow et al. (2014). We train a discriminator net-\nwork to distinguish the output of the generator from real\nimages. The objective of the generator is to trick the dis-\ncriminator, i.e., to generate images that the discriminator\ncannot distinguish from real ones. This yields a natural im-\nage prior that selects from all potential generator outputs\nthe most realistic one. A combination of similarity in an\nappropriate feature space with adversarial training allows\nto obtain the best results; see Fig. 1e.\nWe show three example applications: image compression\nwith an autoencoder, a generative model based on a varia-\ntional autoencoder, and inversion of the AlexNet convolu-\ntional network. We demonstrate that an autoencoder with\nOriginal\nImg loss Img + Adv Img + Feat\nOur\na)\nb)\nc)\nd)\ne)\nFigure 1: Reconstructions from layer FC6 of AlexNet with\ndifferent losses.\narXiv:1602.02644v2  [cs.LG]  9 Feb 2016\nDeep Perceptual Similarity Metrics\nDeePSiM loss can compress images while preserving infor-\nmation about ﬁne structures. On the generative modeling\nside, we show that a version of a variational autoencoder\ntrained with the new loss produces images with realistic\nimage statistics. Finally, reconstructions obtained with our\nmethod from high-level activations of AlexNet are dramat-\nically better than with existing approaches. They demon-\nstrate that even the predicted class probabilities contain rich\ntexture, color, and position information.\n2. Related work\nThere is a long history of neural network based models for\nimage generation. A prominent class of probabilistic mod-\nels of images are restricted Boltzmann machines (Hinton\n& Sejnowski, 1986; Smolensky, 1987; Hinton & Salakhut-\ndinov, 2006) and their deep variants (Hinton et al., 2006;\nSalakhutdinov & Hinton, 2009; Lee et al., 2009).\nAu-\ntoencoders (Hinton & Salakhutdinov, 2006; Vincent et al.,\n2008) have been widely used for unsupervised learning and\ngenerative modeling, too. Recently, stochastic neural net-\nworks (Bengio et al., 2014; Kingma et al., 2014; Gregor\net al., 2015) have become popular, and deterministic net-\nworks are being used for image generation tasks (Dosovit-\nskiy et al., 2015b). In all these models, loss is measured\nin the image space. By combining convolutions and un-\npooling (upsampling) layers (Lee et al., 2009; Goodfellow\net al., 2014; Dosovitskiy et al., 2015b) these models can be\napplied to large images.\nThere is a large body of work on assessing the perceptual\nsimilarity of images. Some prominent examples are the vis-\nible differences predictor (Daly, 1993), the spatio-temporal\nmodel for moving picture quality assessment\n(van den\nBranden Lambrecht & Verscheure, 1996), and the percep-\ntual distortion metric of Winkler (1998). The most popular\nperceptual image similarity metric is the structural similar-\nity metric (SSIM) (Wang et al., 2004), which compares the\nlocal statistics of image patches. We are not aware of any\nwork making use of similarity metrics for machine learn-\ning, except a recent pre-print of Ridgeway et al. (2015).\nThey train autoencoders by directly maximizing the SSIM\nsimilarity of images. This resembles in spirit what we do,\nbut technically is very different. While psychophysical ex-\nperiments go out of scope of this paper, we believe that\ndeep learned feature representations have better potential\nthan shallow hand-designed SSIM.\nGenerative adversarial networks (GANs) have been pro-\nposed by Goodfellow et al. (2014). In theory, this training\nprocedure can lead to a generator that perfectly models the\ndata distribution. Practically, training GANs is difﬁcult and\noften leads to oscillatory behavior, divergence, or modeling\nonly part of the data distribution. Recently, several modiﬁ-\ncations have been proposed that make GAN training more\nstable. Denton et al. (2015) employ a multi-scale approach,\ngradually generating higher resolution images.\nRadford\net al. (2015) make use of a convolutional-deconvolutional\narchitecture and batch normalization.\nGANs can be trained conditionally by feeding the condi-\ntioning variable to both the discriminator and the genera-\ntor (Mirza & Osindero, 2014). Usually this conditioning\nvariable is a one-hot encoding of the object class in the in-\nput image. Such GANs learn to generate images of ob-\njects from a given class. Recently Mathieu et al. (2015)\nused GANs for predicting future frames in videos by con-\nditioning on previous frames.\nOur approach looks sim-\nilar to a conditional GAN. However, in a GAN there is\nno loss directly comparing the generated image to some\nground truth. We found that the feature loss introduced in\nthe present paper is essential to train on complicated tasks\nsuch as feature inversion.\nMost related is concurrent work of Larsen et al. (2015). The\ngeneral idea is the same — to measure the similarity not in\nthe image space, but rather in a feature space. They also\nuse adversarial training to improve the realism of the gen-\nerated images. However, Larsen et al. (2015) only apply\nthis approach to a variational autoencoder trained on im-\nages of faces, and measure the similarity between features\nextracted from the discriminator. Our approach is much\nmore general, we apply it to various natural images, and\nwe demonstrate three different applications.\n3. Model\nSuppose we are given a supervised learning task and a\ntraining set of input-target pairs {xi, yi}, xi ∈RI, yi ∈\nRW ×H×C . Inputs and outputs can be arbitrary vectors.\nIn this work, we focus on targets that are images with an\narbitrary number of channels.\nThe aim is to learn the parameters θ of a differentiable gen-\nerator function Gθ(·): RI →RW ×H×C that optimally ap-\nproximates the input-target dependency according to a loss\nfunction L(Gθ(x), y).\nTypical choices are squared Eu-\nclidean (SE) loss L2(Gθ(x), y) = ||Gθ(x) −y||2\n2 or ℓ1\nloss L1(Gθ(x), y) = ||Gθ(x) −y||1. As we demonstrate\nin this paper, these losses are suboptimal for some image\ngeneration tasks.\nWe propose a new class of losses, which we call DeePSiM.\nThese go beyond simple distances in image space and can\ncapture complex and perceptually important properties of\nimages. These losses are weighted sums of three terms:\nfeature loss Lfeat, adversarial loss Ladv, and pixel space\nloss Limg:\nL = λfeat Lfeat + λadv Ladv + λimg Limg.\n(1)\nThey correspond to a network architecture, an overview of\nDeep Perceptual Similarity Metrics\nFigure 2: Schematic of our model. Black solid lines de-\nnote the forward pass. Dashed lines with arrows on both\nends are the losses. Thin dashed lines denote the ﬂow of\ngradients.\nwhich is shown in Fig. 2. The architecture consists of three\nconvolutional networks: the generator G that implements\nthe generator function, the discriminator Dϕ that discrimi-\nnates generated images from natural images, and the com-\nparator C that computes features from images.\nLoss in feature space. Given a differentiable comparator\nC : RW ×H×C →RF , we deﬁne\nLfeat =\nX\ni\n||C(Gθ(xi)) −C(yi)||2\n2.\n(2)\nC may be ﬁxed or may be trained; for example, it can be a\npart of the generator or the discriminator.\nLfeat alone does not provide a good loss for training. It is\nknown (Mahendran & Vedaldi, 2015) that optimizing just\nfor similarity in the feature space typically leads to high-\nfrequency artifacts. This is because for each natural image\nthere are many non-natural images mapped to the same fea-\nture vector 1. Therefore, a natural image prior is necessary\nto constrain the generated images to the manifold of natural\nimages.\nAdversarial loss. Instead of manually designing a prior,\nas in Mahendran & Vedaldi (2015), we learn it with\nan approach similar to Generative Adversarial Networks\n(GANs) of Goodfellow et al. (2014). Namely, we intro-\nduce a discriminator Dϕ which aims to discriminate the\ngenerated images from real ones, and which is trained con-\ncurrently with the generator Gθ. The generator is trained to\n“trick” the discriminator network into classifying the gen-\nerated images as real. Formally, the parameters ϕ of the\ndiscriminator are trained by minimizing\nLdiscr = −\nX\ni\nlog(Dϕ(yi))+log(1−Dϕ(Gθ(xi))), (3)\n1This is unless the feature representation is speciﬁcally de-\nsigned to map natural and non-natural images far apart, such as\nthe one extracted from the discriminator of a GAN.\nand the generator is trained to minimize\nLadv = −\nX\ni\nlog Dϕ(Gθ(xi)).\n(4)\nLoss in image space. Adversarial training is known to be\nunstable and sensitive to hyperparameters. We found that\nadding a loss in the image space\nLimg =\nX\ni\n||Gθ(xi) −yi||2\n2.\n(5)\nstabilizes training.\n3.1. Architectures\nGenerators. We used several different generators in exper-\niments. They are task-speciﬁc, so we describe these in cor-\nresponding sections below. All tested generators make use\nof up-convolutional (’deconvolutional’) layers, as in Doso-\nvitskiy et al. (2015b). An up-convolutional layer consists\nof up-sampling and a subsequent convolution. In this paper\nwe always up-sample by a factor of 2 and a ’bed of nails’\nupsampling.\nIn all networks we use leaky ReLU nonlinearities, that is,\nLReLU(x) = max(x, 0) + α min(x, 0). We used α =\n0.3 in our experiments. All generators have linear output\nlayers.\nComparators. We experimented with four comparators:\n1. AlexNet (Krizhevsky et al., 2012) is a network with 5\nconvolutional and 2 fully connected layers trained on image\nclassiﬁcation.\n2. The network of Wang & Gupta (2015) has the same\narchitecture as AlexNet, but is trained using videos with\ntriplet loss, which enforces frames of one video to be close\nin the feature space and frames from different videos to be\nfar apart. We refer to this network as VideoNet.\n3. AlexNet with random weights.\n4. Exemplar-CNN (Dosovitskiy et al., 2015a) is a net-\nwork with 3 convolutional layers and 1 fully connected\nlayer trained on a surrogate task of discriminating between\ndifferent image patches.\nThe exact layers used for comparison are speciﬁed in the\nexperiments sections.\nDiscriminator. The architecture of the discriminator was\nnearly the same in all experiments. The version used for\nthe autoencoder experiments is shown in Table 1. The dis-\ncriminator must ensure the local statistics of images to be\nnatural. Therefore after ﬁve convolutional layers with occa-\nsional stride we perform global average pooling. The result\nis processed by two fully connected layers, followed by a\nDeep Perceptual Similarity Metrics\nType\nconv\nconv\nconv\nconv\nconv\npool\nfc\nfc\nInSize\n64\n29\n25\n12\n10\n4\n−\n−\nOutCh\n32\n64\n128\n256\n256\n256\n512\n2\nKernel\n7\n5\n3\n3\n3\n4\n−\n−\nStride\n2\n1\n2\n1\n2\n4\n−\n−\nTable 1: Discriminator architecture.\n2-way softmax. We perform 50% dropout after the global\naverage pooling layer and the ﬁrst fully connected layer.\nThere are two modiﬁcations to this basic architecture. First,\nwhen dealing with large ImageNet (Deng et al., 2009) im-\nages we increase the stride in the ﬁrst layer from 2 to 4.\nSecond, when training networks to invert AlexNet, we ad-\nditionally feed the features to the discriminator. We process\nthem with two fully connected layers with 1024 and 512\nunits, respectively. Then we concatenate the result with the\noutput of global average pooling.\n3.2. Training details\nWe modiﬁed the caffe (Jia et al., 2014) framework to train\nthe networks. For optimization we used Adam (Kingma\n& Ba, 2015) with momentum β1 = 0.9, β2 = 0.999 and\ninitial learning rate 0.0002. To prevent the discriminator\nfrom overﬁtting during adversarial training we temporarily\nstopped updating it if the ratio of Ldiscr and Ladv was be-\nlow a certain threshold (0.1 in most experiments). We used\nbatch size 64 in all experiments. We trained for 500, 000-\n1, 000, 000 mini-batch iterations.\n4. Experiments\nWe started with a simple proof-of-concept experiment\nshowing how DeePSiM can be applied to training autoen-\ncoders. Then we used the proposed loss function within\nthe variational autoencoder (VAE) framework. Finally, we\napplied the method to invert the representation learned by\nAlexNet and analyzed some properties of the method.\nIn quantitative comparisons we report normalized Eu-\nclidean error ||a −b||2/N. The normalization coefﬁcient\nN is the average of Euclidean distances between all pairs\nof different samples from the test set. Therefore, the er-\nror of 100% means that the algorithm performs the same as\nrandomly drawing a sample from the test set.\n4.1. Autoencoder\nHere the target of the generator coincides with its input\n(that is, y = x), and the task of the generator is to en-\ncode the input to a compressed hidden representation and\nthen decode back the image. The architecture is shown in\nTable 2. All layers are convolutional or up-convolutional.\nInSize\n64\n32\n32\n16\n16\n8\n8\n8\nOutCh\n32\n32\n64\n64\n128\n128\n64\n8\nKernel\n5\n3\n5\n3\n3\n3\n3\n3\nStride\n↓2\n1\n↓2\n1\n↓2\n1\n1\n1\nInSize\n8\n8\n8\n16\n16\n32\n32\n64\nOutCh\n64\n128\n64\n64\n32\n32\n16\n3\nKernel\n3\n3\n4\n3\n4\n3\n4\n3\nStride\n1\n1\n↑2\n1\n↑2\n1\n↑2\n1\nTable 2: Autoencoder architecture. Top: encoder, bottom:\ndecoder. All layers are convolutional or ’up-convolutional’.\nSE loss\nℓ1 loss\nOur-ExCNN\nOur-AlexNet\n15.3\n15.7\n19.8\n21.5\nTable 3: Normalized Euclidean reconstruction error (in %)\nof autoencoders trained with different loss functions.\nThe hidden representation is an 8-channel feature map 8\ntimes smaller than the input image. We trained on the STL-\n10 (Coates et al., 2011) unlabeled dataset which contains\n100, 000 images 96 × 96 pixels. To prevent overﬁtting we\naugmented the data by cropping random 64 × 64 patches\nduring training.\nWe experimented with four loss functions: SE and ℓ1 in the\nimage space, as well as DeePSiM with AlexNet CONV3 or\nExemplar-CNN CONV3 as comparator.\nQualitative results are shown in Fig. 3, quantitative results\nin Table 3. While underperforming in terms of Euclidean\nloss, our approach can preserve more texture details, result-\ning in naturally looking non-blurry reconstructions. Inter-\nestingly, AlexNet as comparator tends to corrupt ﬁne de-\ntails (petals of the ﬂower, sails of the ship), perhaps be-\ncause it has stride of 4 in the ﬁrst layer. Exemplar-CNN\nas comparator does not preserve the exact color because it\nis explicitly trained to be invariant to color changes. We\nbelieve that with carefully selected or speciﬁcally trained\ncomparators yet better results can be obtained.\nWe stress that lower Euclidean error does not mean better\nreconstruction. For example, imagine a black-and-white\nstriped ”zebra” pattern. A monotonous gray image will\nhave twice smaller Euclidean error than the same pattern\nshifted by one stripe width.\nClassiﬁcation.\nReconstruction-based models are com-\nmonly used for unsupervised feature learning. We checked\nSE loss\nℓ1 loss\nOur-ExCNN\nOur-AlexNet\n34.6 ± 0.6\n35.7 ± 0.4\n50.1 ± 0.5\n52.3 ± 0.6\nTable 4: Classiﬁcation accuracy (in %) on STL with au-\ntoencoder features learned with different loss functions.\nDeep Perceptual Similarity Metrics\nImage\nAlexNet\nEx-CNN\nSE\nℓ1\nFigure 3: Autoencoder qualitative results. Best viewed on\nscreen.\nif our loss functions lead to learning more meaningful rep-\nresentations than usual ℓ1 and SE losses.\nTo this end,\nwe trained linear SVMs on the 8-channel hidden repre-\nsentations extracted by autoencoders trained with different\nlosses. We are just interested in relative performance and,\nthus, do not compare to the state of the art. We trained on\n10 folds of the STL-10 training set and tested on the test\nset.\nThe results are shown in Table 4. As expected, the fea-\ntures learned with DeePSiM perform signiﬁcantly better,\nindicating that they contain more semantically meaningful\ninformation. This suggests that other losses than standard\nℓ1 and SE may be useful for unsupervised learning. Note\nthat the Exemplar-CNN comparator is trained in an unsu-\npervised way.\n4.2. Variational autoencoder\nA standard VAE consists of an encoder Enc and a decoder\nDec. The encoder maps an input sample x to a distribution\nover latent variables z ∼Enc(x) = q(z|x). Dec maps\nfrom this latent space to a distribution over images ˜x ∼\nDec(z) = p(x|z). The loss function is\nX\ni\n−Eq(z|xi) log p(xi|z) + DKL(q(z|xi)||p(z)),\n(6)\nwhere p(z) is a prior distribution of latent variables and\nDKL is the Kullback-Leibler divergence. The ﬁrst term in\nEq. 6 is a reconstruction error. If we assume that the de-\ncoder predicts a Gaussian distribution at each pixel, then\nit reduces to squared Euclidean error in the image space.\nThe second term pulls the distribution of latent variables\ntowards the prior. Both q(z|x) and p(z) are commonly as-\nsumed to be Gaussian, in which case the KL divergence\ncan be computed analytically. Please refer to Kingma et al.\n(2014) for details.\nWe use the proposed loss instead of the ﬁrst term in Eq. 6.\nThis is similar to Larsen et al. (2015), but the comparator\ndoes not have to be a part of the discriminator. Techni-\ncally, there is little difference from training an autoencoder.\nFirst, instead of predicting a single latent vector z we pre-\ndict two vectors µ and σ and sample z = µ + σ ⊙ε, where\nε is standard Gaussian (zero mean, unit variance) and ⊙is\nelement-wise multiplication. Second, we add the KL diver-\ngence term to the loss:\nLKL = 1\n2\nX\ni\n\u0000||µi||2\n2 + ||σi||2\n2 −⟨log σ2\ni , 1⟩\n\u0001\n.\n(7)\nWe manually set the weighting of the KL term relative to\nthe rest of the loss. Proper probabilistic derivation is non-\nstraightforward, and we leave it for future research.\nWe trained on 227 × 227 pixel crops of 256 × 256 pixel\nILSVRC-2012 images.\nThe encoder architecture is the\nsame as AlexNet up to layer FC6, and the decoder archi-\ntecture is shown in Table 5. We initialized the encoder with\nAlexNet weights, however, this is not necessary, as shown\nFigure 4: Samples from VAE with the SE loss (topmost)\nand the proposed DeePSiM loss (top to bottom: AlexNet\nCONV5, AlexNet FC6, VideoNet CONV5).\nDeep Perceptual Similarity Metrics\nType\nfc\nfc\nfc\nreshape\nuconv\nconv\nInSize\n−\n−\n−\n1\n4\n8\nOutCh\n4096\n4096\n4096\n256\n256\n512\nKernel\n−\n−\n−\n−\n4\n3\nStride\n−\n−\n−\n−\n↑2\n1\nType\nuconv conv uconv conv uconv uconv uconv\nInSize\n8\n16\n16\n32\n32\n64\n128\nOutCh\n256\n256\n128\n128\n64\n32\n3\nKernel\n4\n3\n4\n3\n4\n4\n4\nStride\n↑2\n1\n↑2\n1\n↑2\n↑2\n↑2\nTable 5: Generator architecture for inverting layer FC6 of\nAlexNet.\nin the appendix. We sampled from the model by sampling\nthe latent variables from a standard Gaussian z = ε and\ngenerating images from that with the decoder.\nSamples generated with the usual SE loss, as well as\nthree different comparators (AlexNet CONV5, AlexNet\nFC6, VideoNet CONV5) are shown in Fig. 4. While Eu-\nclidean loss leads to very blurry samples, our method yields\nimages with realistic statistics. Interestingly, the samples\ntrained with the VideoNet comparator look qualitatively\nsimilar to the ones with AlexNet, showing that supervised\ntraining may not be necessary to yield a good comparator.\nMore results are shown in the appendix.\n4.3. Inverting AlexNet\nAnalysis of learned representations is an important but\nlargely unsolved problem. One approach is to invert the\nrepresentation. This may give insights into which infor-\nmation is preserved in the representation and what are its\ninvariance properties. However, inverting a non-trivial fea-\nture representation Φ, such as the one learned by a large\nconvolutional network, is a difﬁcult ill-posed problem.\nOur proposed approach inverts the AlexNet convolutional\nnetwork very successfully. Surprisingly rich information\nabout the image is preserved in deep layers of the network\nand even in the predicted class probabilities. While being\nan interesting result in itself, this also shows how DeeP-\nSiM is an excellent loss function when dealing with very\ndifﬁcult image restoration tasks.\nSuppose we are given a feature representation Φ, which\nwe aim to invert, and an image I. There are two inverse\nmappings: Φ−1\nR such that Φ(Φ−1\nR (φ)) ≈φ, and Φ−1\nL\nsuch\nthat Φ−1\nL (Φ(I)) ≈I. Recently two approaches to inver-\nsion have been proposed, which correspond to these two\nvariants of the inverse.\nMahendran & Vedaldi (2015), as well as Simonyan et al.\n(2014) and Yosinski et al. (2015), apply gradient-based\nImage\nCONV5\nFC6\nFC7\nFC8\nOur\nD&B\nM&V\nOur\nD&B\nM&V\nFigure 6: Comparison with Dosovitskiy & Brox (2015)\nand Mahendran & Vedaldi (2015). Our results look sig-\nniﬁcantly better, even our failure cases (second image).\noptimization to ﬁnd an image eI which minimizes the loss\n||Φ(I) −Φ(eI)||2\n2 + P(eI),\n(8)\nwhere P is a simple natural image prior, such as total varia-\ntion (TV) regularizer. This method produces images which\nare roughly natural and have features similar to the in-\nput features, corresponding to Φ−1\nR . However, the prior is\nlimited, so reconstructions from fully connected layers of\nAlexNet do not look much like natural images.\nDosovitskiy & Brox (2015) train up-convolutional net-\nworks on a large training set of natural images to perform\nthe inversion task. They use SE distance in the image space\nas loss function, which leads to approximating Φ−1\nL . The\nnetworks learn to reconstruct the color and rough positions\nof objects well, but produce over-smoothed results because\nthey average all potential reconstructions.\nOur method can be seen as combining the best of both\nworlds. Loss in the feature space helps preserve percep-\ntually important image features. Adversarial training keeps\nreconstructions realistic. Note that similar to Dosovitskiy\n& Brox (2015) and unlike Mahendran & Vedaldi (2015),\nour method does not require the feature representation be-\ning inverted to be differentiable.\nTechnical details. The generator in this setup takes the fea-\ntures extracted by AlexNet and generates an image from\nDeep Perceptual Similarity Metrics\nImage\nCONV5\nFC6\nFC7\nFC8\nFigure 5: Representative reconstructions from higher layers of AlexNet. General characteristics of images are preserved\nvery well. In some cases (simple objects, landscapes) reconstructions are nearly perfect even from FC8. In the leftmost\ncolumn the network generates dog images from FC7 and FC8.\nthem, that is, x = Φ(I), y = I.\nIn general we fol-\nlowed Dosovitskiy & Brox (2015) in designing the gener-\nators. The only modiﬁcation is that we inserted more con-\nvolutional layers, giving the network more capacity. We\nreconstruct from outputs of layers CONV5 –FC8. In each\nlayer we also include processing steps following the layer,\nthat is, pooling and non-linearities. So for example CONV5\nmeans pooled features (pool5), and FC6 means rectiﬁed\nvalues (relu6).\nArchitecture used for inverting FC6 is the same as the de-\ncoder of the VAE shown in Table 5.\nArchitectures for\nother layers are similar, except that for reconstruction from\nCONV5 fully connected layers are replaced by convolu-\ntional ones. The discriminator is the same as used for VAE.\nWe trained on the ILSVRC-2012 training set and evaluated\non the ILSVRC-2012 validation set.\nAblation study. We tested if all components of our loss\nare necessary. Results with some of these components re-\nmoved are shown in Fig. 7. Clearly the full model performs\nbest. In the following we will give some intuition why.\nTraining just with loss in the image space leads to av-\neraging all potential reconstructions, resulting in over-\nsmoothed images.\nOne might imagine that adversarial\ntraining would allow to make images sharp. This indeed\nhappens, but the resulting reconstructions do not corre-\nspond to actual objects originally contained in the im-\nage. The reason is that any “natural-looking” image which\nroughly ﬁts the blurry prediction minimizes this loss. With-\nout the adversarial loss predictions look very noisy. With-\nImage\nFull\n−Limg\n−Lfeat\n−Ladv\n−Lfeat\n−Ladv\nFigure 7: Reconstructions from FC6 with some compo-\nnents of the loss removed.\nout the image space loss the method works well, but one\ncan notice artifact on the borders of images, and training\nwas less stable in this case.\nSampling pre-images. Given a feature vector φ, it would\nbe interesting to sample multiple imageseI such that Φ(eI) =\nφ. A straightforward approach would inject noise into the\ngenerator along with the features, so that the network could\nrandomize its outputs. This does not yield the desired re-\nsult, since nothing in the loss function forces the generator\nto output multiple different reconstructions per feature vec-\ntor. A major problem is that in the training data we only\nhave one image per feature vector, i.e., a single sample per\nconditioning vector. We did not attack this problem in our\npaper, but we believe it is an important research direction.\nDeep Perceptual Similarity Metrics\nCONV5\nFC6\nFC7\nFC8\nMahendran&Vedaldi\n71/19\n80/19\n82/16\n84/09\nDosovitskiy & Brox\n35/−\n51/−\n56/−\n58/−\nOur just image loss\n−/−\n46/79\n−/−\n−/−\nOur AlexNet CONV5\n43/37\n55/48\n61/45\n63/29\nOur VideoNet CONV5\n−/−\n51/57\n−/−\n−/−\nFigure 8: Normalized inversion error (in %) when recon-\nstructing from different layers of AlexNet with different\nmethods. First in each pair – error in the image space, sec-\nond – in the feature space.\nCONV5\nFC6\nFC7\nFC8\n1\n2\n4\n8\nFigure 9: Iteratively re-encoding images with AlexNet and\nreconstructing. Iteration number shown on the left.\nBest results. Representative reconstructions from higher\nlayers of AlexNet are shown in Fig. 5. Comparison with\nexisting approaches is shown in Fig. 6. Reconstructions\nfrom CONV5 are near-perfect, combining the natural col-\nors and sharpness of details. Reconstructions from fully\nconnected layers are still very good, preserving the main\nfeatures of images, colors, and positions of large objects.\nNormalized Euclidean error in image space and in fea-\nture space (that is, the distance between the features of\nthe image and the reconstruction) are shown in Table 8.\nThe method of Mahendran&Vedaldi performs well in fea-\nture space, but not in image space, the method of Dosovit-\nskiy&Brox — vice versa. The presented approach is fairly\ngood on both metrics.\nIterative re-encoding. We performed another experiment\nillustrating how similar are the features of reconstructions\nto the original image features. Given an image, we compute\nits features, generate an image from those, and then itera-\ntively compute the features of the result and generate from\nthose. Results are shown in Fig. 9. Interestingly, several it-\nerations do not signiﬁcantly change the reconstruction, in-\ndicating that important perceptual features are preserved in\nthe generated images. More results are shown in the ap-\npendix.\nInterpolation. We can morph images into each other by\nlinearly interpolating between their features and generat-\ning the corresponding images. Fig. 11 shows that objects\nImage\nAlex5\nAlex6\nVideo5\nRand5\nFigure 10: Reconstructions from FC6 with different com-\nparators. The number indicates the layer from which fea-\ntures were taken.\nshown in the images smoothly warp into each other. More\nexamples are shown in the appendix.\nDifferent comparators. AlexNet network we used above\nas comparator has been trained on a huge labeled dataset.\nIs this supervision really necessary to learn a good com-\nparator? We show here results with several alternatives to\nCONV5 features of AlexNet: 1) FC6 features of AlexNet,\n2) CONV5 of AlexNet with random weights, 3) CONV5 of\nthe network of Wang & Gupta (2015) which we refer to as\nVideoNet.\nThe results are shown in Fig. 10. While AlexNet CONV5\ncomparator provides best reconstructions, other networks\npreserve key image features as well. We also ran prelim-\ninary experiments with CONV5 features from the discrim-\ninator serving as a comparator, but were not able to get\nsatisfactory results with those.\n5. Conclusion\nWe proposed a class of loss functions applicable to image\ngeneration that are based on distances in feature spaces.\nApplying these to three tasks — image auto-encoding, ran-\ndom natural image generation with a VAE and feature in-\nversion — reveals that our loss is clearly superior to the typ-\nical loss in image space. In particular, it allows reconstruc-\ntion of perceptually important details even from very low-\ndimensional image representations. We evaluated several\nfeature spaces to measure distances. More research is nec-\nessary to ﬁnd optimal features to be used depending on the\ntask. To control the degree of realism in generated images,\nan alternative to adversarial training is an approach making\nuse of feature statistics, similar to Gatys et al. (2015). We\nsee these as interesting directions of future work.\nDeep Perceptual Similarity Metrics\nImage pair 1\nImage pair 2\nFC6\nFC8\nFigure 11: Interpolation between images by interpolating\nbetween their features in FC6 and FC8.\nAcknowledgements\nThe authors are grateful to Jost Tobias Springenberg and\nPhilipp Fischer for useful discussions. We acknowledge\nfunding by the ERC Starting Grant VideoLearn (279401).\nReferences\nY. Bengio, E. Laufer, G. Alain, and J. Yosinski. Deep gen-\nerative stochastic networks trainable by backprop.\nIn\nICML, 2014.\nA. Coates, H. Lee, and A. Y. Ng. An analysis of single-\nlayer networks in unsupervised feature learning. AIS-\nTATS, 2011.\nS. Daly. Digital images and human vision. chapter The Vis-\nible Differences Predictor: An Algorithm for the Assess-\nment of Image Fidelity, pp. 179–206. MIT Press, 1993.\nJ. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei.\nImageNet: A Large-Scale Hierarchical Image Database.\nIn CVPR, 2009.\nE. L. Denton, S. Chintala, arthur Szlam, and R. Fergus.\nDeep Generative Image Models using a Laplacian Pyra-\nmid of Adversarial Networks. In Advances in Neural In-\nformation Processing Systems 28, pp. 1486–1494. Cur-\nran Associates, Inc., 2015.\nA. Dosovitskiy, P. Fischer, J. T. Springenberg, M. Ried-\nmiller, and T. Brox. Discriminative unsupervised feature\nlearning with exemplar convolutional neural networks.\nIEEE Transactions on Pattern Analysis and Machine In-\ntelligence, 2015a.\nA. Dosovitskiy and T. Brox. Inverting visual representa-\ntions with convolutional networks. arxiv/1506.02753v2,\n2015.\nURL http://arxiv.org/abs/1506.\n02753v2.\nA. Dosovitskiy, J. T. Springenberg, and T. Brox. Learning\nto generate chairs with convolutional neural networks. In\nCVPR, 2015b.\nL. A. Gatys, A. S. Ecker, and M. Bethge. A neural algo-\nrithm of artistic style. arxiv:1508.06576, 2015. URL\nhttp://arxiv.org/abs/1508.06576.\nI. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu,\nD. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio.\nGenerative adversarial nets. In NIPS, 2014.\nK. Gregor, I. Danihelka, A. Graves, D. J. Rezende, and\nD. Wierstra.\nDRAW: A recurrent neural network for\nimage generation.\nIn Proceedings of the 32nd Inter-\nnational Conference on Machine Learning, ICML 2015,\nLille, France, 6-11 July 2015, pp. 1462–1471, 2015.\nG. E. Hinton and R. R. Salakhutdinov. Reducing the di-\nmensionality of data with neural networks. Science, 313\n(5786):504–507, July 2006.\nG. E. Hinton and T. J. Sejnowski. Learning and relearning\nin boltzmann machines. In Parallel Distributed Process-\ning: Volume 1: Foundations, pp. 282–317. MIT Press,\nCambridge, 1986.\nG. E. Hinton, S. Osindero, and Y.-W. Teh. A fast learning\nalgorithm for deep belief nets. Neural Comput., 18(7):\n1527–1554, 2006.\nY. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long,\nR. Girshick, S. Guadarrama, and T. Darrell.\nCaffe:\nConvolutional architecture for fast feature embedding.\narXiv:1408.5093, 2014.\nD. P. Kingma and J. Ba. Adam: A method for stochastic\noptimization. In ICLR, 2015.\nD. Kingma, D. Rezende, S. Mohamed, and M. Welling.\nSemi-supervised learning with deep generative models.\nIn NIPS, 2014.\nA. Krizhevsky, I. Sutskever, and G. E. Hinton. ImageNet\nclassiﬁcation with deep convolutional neural networks.\nIn NIPS, pp. 1106–1114, 2012.\nA. B. L. Larsen, S. K. Sønderby, and O. Winther. Autoen-\ncoding beyond pixels using a learned similarity metric.\narxiv:1512.09300, 2015. URL http://arxiv.org/\nabs/1512.09300.\nDeep Perceptual Similarity Metrics\nH. Lee, R. Grosse, R. Ranganath, and A. Y. Ng. Convo-\nlutional deep belief networks for scalable unsupervised\nlearning of hierarchical representations. In ICML, pp.\n609–616, 2009.\nA. Mahendran and A. Vedaldi. Understanding deep image\nrepresentations by inverting them. In CVPR, 2015.\nM. Mathieu, C. Couprie, and Y. LeCun.\nDeep multi-\nscale video prediction beyond mean square error.\narXiv:1511.05440, 2015.\nURL http://arxiv.\norg/abs/1511.05440.\nM. Mirza and S. Osindero. Conditional generative adver-\nsarial nets. arxiv:1411.1784, 2014.\nA. Radford, L. Metz, and S. Chintala. Unsupervised Repre-\nsentation Learning with Deep Convolutional Generative\nAdversarial Networks. arXiv:1511.06434, 2015. URL\nhttp://arxiv.org/abs/1511.06434.\nK. Ridgeway, J. Snell, B. Roads, R. S. Zemel, and M. C.\nMozer.\nLearning to generate images with perceptual\nsimilarity metrics. arxiv:1511.06409, 2015.\nR. Salakhutdinov and G. E. Hinton. Deep boltzmann ma-\nchines. In AISTATS, 2009.\nK. Simonyan, A. Vedaldi, and A. Zisserman.\nDeep in-\nside convolutional networks: Visualising image clas-\nsiﬁcation models and saliency maps.\nIn ICLR work-\nshop track, 2014. URL http://arxiv.org/abs/\n1312.6034.\nP. Smolensky. Information processing in dynamical sys-\ntems: Foundations of harmony theory. In Parallel Dis-\ntributed Processing: Volume 1: Foundations, pp. 194–\n281. MIT Press, Cambridge, 1987.\nC. J. van den Branden Lambrecht and O. Verscheure. Per-\nceptual quality measure using a spatio-temporal model\nof the human visual system. Electronic Imaging: Sci-\nence & Technology, 1996.\nP. Vincent, H. Larochelle, Y. Bengio, and P.-A. Manzagol.\nExtracting and composing robust features with denoising\nautoencoders. In ICML, pp. 1096–1103, 2008.\nX. Wang and A. Gupta. Unsupervised learning of visual\nrepresentations using videos. In ICCV, 2015.\nZ. Wang, A. C. Bovik, H. R. Sheikh, and E. P. Simoncelli.\nImage quality assessment: From error visibility to struc-\ntural similarity. IEEE Transactions on Image Processing,\n13(4):600–612, 2004.\nS. Winkler. A perceptual distortion metric for digital color\nimages. In in Proc. SPIE, pp. 175–184, 1998.\nJ. Yosinski, J. Clune, A. Nguyen, T. Fuchs, and H. Lipson.\nUnderstanding neural networks through deep visualiza-\ntion. In Deep Learning Workshop, ICML, 2015.\nAppendix\nHere we show some additional results obtained with the\nproposed method.\nFigure 12 illustrates how position and color of an object\nis preserved in deep layers of AlexNet (Krizhevsky et al.,\n2012).\nFigure 13 shows results of generating images from interpo-\nlations between the features of natural images.\nFigure 14 shows samples from variational autoencoders\nwith different losses.\nFully unsupervised VAE with\nVideoNet (Wang & Gupta, 2015) loss and random initial-\nization of the encoder is in the bottom right. Samples from\nthis model are qualitatively similar to others, showing that\ninitialization with AlexNet is not necessary.\nFigures 15 and 16 show results of iteratively encoding im-\nages to a feature representation and reconstructing back to\nthe image space. As can be seen from Figure 16, the net-\nwork trained with loss in the image space does not preserve\nthe features well, resulting in reconstructions quickly di-\nverging from the original image.\nImage\nCONV5\nFC6\nFC7\nFC8\nFigure 12: Position (ﬁrst three columns) and color (last\nthree columns) preservation.\nDeep Perceptual Similarity Metrics\nFigure 13: Interpolation in feature spaces at different layers of AlexNet. Topmost: input images, Top left: CONV5, Top\nright: FC6, Bottom left: FC7, Bottom right: FC8.\nDeep Perceptual Similarity Metrics\nFigure 14: Samples from VAE with our approach, with different comparators. Top left: AlexNet CONV5 comparator, Top\nright: AlexNet FC6 comparator, Bottom left: VideoNet CONV5 comparator, Bottom right: VideoNet CONV5 comparator\nwith randomly initialized encoder.\nDeep Perceptual Similarity Metrics\nFigure 15: Iterative re-encoding and reconstructions for different layers of AlexNet. Each row of each block corresponds\nto an iteration number: 1, 2, 4, 6, 8, 12, 16, 20. Topmost: input images, Top left: CONV5, Top right: FC6, Bottom left:\nFC7, Bottom right: FC8.\nDeep Perceptual Similarity Metrics\nFigure 16: Iterative re-encoding and reconstructions with network trained to reconstruct from AlexNet FC6 layer with\nsquared Euclidean loss in the image space. On top the input images are shown. Then each row corresponds to an iteration\nnumber: 1, 2, 4, 6, 8, 12, 16, 20.\n",
        "sentence": " The philosophy is to train another neural network to inverse the representation and speedup the visualization on image reconstruction[10, 18], texture synthesis[19] or even style transfer[15]. Instead of designing a natural prior, some researchers incorporate adversarial training[20] to improve the realism of the generated images[18]."
    },
    {
        "title": "Texture networks: Feedforward synthesis of textures and stylized images",
        "author": [
            "Dmitry Ulyanov",
            "Vadim Lebedev",
            "Andrea Vedaldi",
            "Victor Lempitsky"
        ],
        "venue": "arXiv preprint arXiv:1603.03417,",
        "citeRegEx": "19",
        "shortCiteRegEx": "19",
        "year": 2016,
        "abstract": "",
        "full_text": "",
        "sentence": " The philosophy is to train another neural network to inverse the representation and speedup the visualization on image reconstruction[10, 18], texture synthesis[19] or even style transfer[15]."
    },
    {
        "title": "Generative adversarial nets",
        "author": [
            "Ian Goodfellow",
            "Jean Pouget-Abadie",
            "Mehdi Mirza",
            "Bing Xu",
            "David Warde-Farley",
            "Sherjil Ozair",
            "Aaron Courville",
            "Yoshua Bengio"
        ],
        "venue": null,
        "citeRegEx": "20",
        "shortCiteRegEx": "20",
        "year": 2014,
        "abstract": "Abstract não disponível",
        "full_text": "",
        "sentence": " Instead of designing a natural prior, some researchers incorporate adversarial training[20] to improve the realism of the generated images[18]."
    },
    {
        "title": "Caffe: Convolutional architecture for fast feature embedding",
        "author": [
            "Yangqing Jia",
            "Evan Shelhamer",
            "Jeff Donahue"
        ],
        "venue": "In Proceedings of the ACM International Conference on Multimedia,",
        "citeRegEx": "21",
        "shortCiteRegEx": "21",
        "year": 2014,
        "abstract": "Caffe provides multimedia scientists and practitioners with a clean and\nmodifiable framework for state-of-the-art deep learning algorithms and a\ncollection of reference models. The framework is a BSD-licensed C++ library\nwith Python and MATLAB bindings for training and deploying general-purpose\nconvolutional neural networks and other deep models efficiently on commodity\narchitectures. Caffe fits industry and internet-scale media needs by CUDA GPU\ncomputation, processing over 40 million images a day on a single K40 or Titan\nGPU ($\\approx$ 2.5 ms per image). By separating model representation from\nactual implementation, Caffe allows experimentation and seamless switching\namong platforms for ease of development and deployment from prototyping\nmachines to cloud environments. Caffe is maintained and developed by the\nBerkeley Vision and Learning Center (BVLC) with the help of an active community\nof contributors on GitHub. It powers ongoing research projects, large-scale\nindustrial applications, and startup prototypes in vision, speech, and\nmultimedia.",
        "full_text": "Caffe: Convolutional Architecture\nfor Fast Feature Embedding\n∗\nYangqing Jia∗, Evan Shelhamer∗, Jeff Donahue, Sergey Karayev,\nJonathan Long, Ross Girshick, Sergio Guadarrama, Trevor Darrell\nUC Berkeley EECS, Berkeley, CA 94702\n{jiayq,shelhamer,jdonahue,sergeyk,jonlong,rbg,sguada,trevor}@eecs.berkeley.edu\nABSTRACT\nCaﬀe provides multimedia scientists and practitioners with\na clean and modiﬁable framework for state-of-the-art deep\nlearning algorithms and a collection of reference models.\nThe framework is a BSD-licensed C++ library with Python\nand MATLAB bindings for training and deploying general-\npurpose convolutional neural networks and other deep mod-\nels eﬃciently on commodity architectures. Caﬀe ﬁts indus-\ntry and internet-scale media needs by CUDA GPU computa-\ntion, processing over 40 million images a day on a single K40\nor Titan GPU (≈2.5 ms per image). By separating model\nrepresentation from actual implementation, Caﬀe allows ex-\nperimentation and seamless switching among platforms for\nease of development and deployment from prototyping ma-\nchines to cloud environments.\nCaﬀe is maintained and developed by the Berkeley Vi-\nsion and Learning Center (BVLC) with the help of an ac-\ntive community of contributors on GitHub. It powers on-\ngoing research projects, large-scale industrial applications,\nand startup prototypes in vision, speech, and multimedia.\nCategories and Subject Descriptors\nI.5.1 [Pattern Recognition]: [Applications–Computer vi-\nsion]; D.2.2 [Software Engineering]: [Design Tools and\nTechniques–Software libraries]; I.5.1 [Pattern Recognition]:\n[Models–Neural Nets]\nGeneral Terms\nAlgorithms, Design, Experimentation\nKeywords\nOpen Source, Computer Vision, Neural Networks, Parallel\nComputation, Machine Learning\n∗Corresponding Authors.\nThe work was done while\nYangqing Jia was a graduate student at Berkeley.\nHe is\ncurrently a research scientist at Google, 1600 Amphitheater\nPkwy, Mountain View, CA 94043.\n.\n1.\nINTRODUCTION\nA key problem in multimedia data analysis is discovery of\neﬀective representations for sensory inputs—images, sound-\nwaves, haptics, etc.\nWhile performance of conventional,\nhandcrafted features has plateaued in recent years, new de-\nvelopments in deep compositional architectures have kept\nperformance levels rising [8].\nDeep models have outper-\nformed hand-engineered feature representations in many do-\nmains, and made learning possible in domains where engi-\nneered features were lacking entirely.\nWe are particularly motivated by large-scale visual recog-\nnition, where a speciﬁc type of deep architecture has achieved\na commanding lead on the state-of-the-art.\nThese Con-\nvolutional Neural Networks, or CNNs, are discriminatively\ntrained via back-propagation through layers of convolutional\nﬁlters and other operations such as rectiﬁcation and pooling.\nFollowing the early success of digit classiﬁcation in the 90’s,\nthese models have recently surpassed all known methods for\nlarge-scale visual recognition, and have been adopted by in-\ndustry heavyweights such as Google, Facebook, and Baidu\nfor image understanding and search.\nWhile deep neural networks have attracted enthusiastic\ninterest within computer vision and beyond, replication of\npublished results can involve months of work by a researcher\nor engineer. Sometimes researchers deem it worthwhile to\nrelease trained models along with the paper advertising their\nperformance. But trained models alone are not suﬃcient for\nrapid research progress and emerging commercial applica-\ntions, and few toolboxes oﬀer truly oﬀ-the-shelf deployment\nof state-of-the-art models—and those that do are often not\ncomputationally eﬃcient and thus unsuitable for commercial\ndeployment.\nTo address such problems, we present Caﬀe, a fully open-\nsource framework that aﬀords clear access to deep architec-\ntures.\nThe code is written in clean, eﬃcient C++, with\nCUDA used for GPU computation, and nearly complete,\nwell-supported bindings to Python/Numpy and MATLAB.\nCaﬀe adheres to software engineering best practices, pro-\nviding unit tests for correctness and experimental rigor and\nspeed for deployment. It is also well-suited for research use,\ndue to the careful modularity of the code, and the clean sep-\naration of network deﬁnition (usually the novel part of deep\nlearning research) from actual implementation.\nIn Caﬀe, multimedia scientists and practitioners have an\norderly and extensible toolkit for state-of-the-art deep learn-\ning algorithms, with reference models provided out of the\nbox. Fast CUDA code and GPU computation ﬁt industry\nneeds by achieving processing speeds of more than 40 mil-\narXiv:1408.5093v1  [cs.CV]  20 Jun 2014\nCore\nOpen\nPretrained\nFramework\nLicense\nlanguage\nBinding(s)\nCPU\nGPU\nsource\nTraining\nmodels\nDevelopment\nCaﬀe\nBSD\nC++\nPython,\ndistributed\nMATLAB\ncuda-convnet [7]\nunspeciﬁed\nC++\nPython\ndiscontinued\nDecaf [2]\nBSD\nPython\ndiscontinued\nOverFeat [9]\nunspeciﬁed\nLua\nC++,Python\ncentralized\nTheano/Pylearn2 [4]\nBSD\nPython\ndistributed\nTorch7 [1]\nBSD\nLua\ndistributed\nTable 1: Comparison of popular deep learning frameworks. Core language is the main library language, while\nbindings have an oﬃcially supported library interface for feature extraction, training, etc. CPU indicates\navailability of host-only computation, no GPU usage (e.g., for cluster deployment); GPU indicates the GPU\ncomputation capability essential for training modern CNNs.\nlion images per day on a single K40 or Titan GPU. The\nsame models can be run in CPU or GPU mode on a vari-\nety of hardware: Caﬀe separates the representation from the\nactual implementation, and seamless switching between het-\nerogeneous platforms furthers development and deployment—\nCaﬀe can even be run in the cloud.\nWhile Caﬀe was ﬁrst designed for vision, it has been adopted\nand improved by users in speech recognition, robotics, neu-\nroscience, and astronomy. We hope to see this trend con-\ntinue so that further sciences and industries can take advan-\ntage of deep learning.\nCaﬀe is maintained and developed by the BVLC with the\nactive eﬀorts of several graduate students, and welcomes\nopen-source contributions at http://github.com/BVLC/caffe.\nWe thank all of our contributors for their work!\n2.\nHIGHLIGHTS OF CAFFE\nCaﬀe provides a complete toolkit for training, testing,\nﬁnetuning, and deploying models, with well-documented ex-\namples for all of these tasks. As such, it’s an ideal starting\npoint for researchers and other developers looking to jump\ninto state-of-the-art machine learning. At the same time,\nit’s likely the fastest available implementation of these algo-\nrithms, making it immediately useful for industrial deploy-\nment.\nModularity. The software is designed from the begin-\nning to be as modular as possible, allowing easy extension to\nnew data formats, network layers, and loss functions. Lots\nof layers and loss functions are already implemented, and\nplentiful examples show how these are composed into train-\nable recognition systems for various tasks.\nSeparation of representation and implementation.\nCaﬀe model deﬁnitions are written as conﬁg ﬁles using the\nProtocol Buﬀer language.\nCaﬀe supports network archi-\ntectures in the form of arbitrary directed acyclic graphs.\nUpon instantiation, Caﬀe reserves exactly as much memory\nas needed for the network, and abstracts from its underly-\ning location in host or GPU. Switching between a CPU and\nGPU implementation is exactly one function call.\nTest coverage. Every single module in Caﬀe has a test,\nand no new code is accepted into the project without corre-\nsponding tests. This allows rapid improvements and refac-\ntoring of the codebase, and imparts a welcome feeling of\npeacefulness to the researchers using the code.\nPython and MATLAB bindings.\nFor rapid proto-\ntyping and interfacing with existing research code, Caﬀe\nprovides Python and MATLAB bindings. Both languages\nmay be used to construct networks and classify inputs. The\nPython bindings also expose the solver module for easy pro-\ntotyping of new training procedures.\nPre-trained reference models. Caﬀe provides (for aca-\ndemic and non-commercial use—not BSD license) reference\nmodels for visual tasks, including the landmark “AlexNet”\nImageNet model [8] with variations and the R-CNN detec-\ntion model [3].\nMore are scheduled for release.\nWe are\nstrong proponents of reproducible research: we hope that\na common software substrate will foster quick progress in\nthe search over network architectures and applications.\n2.1\nComparison to related software\nWe summarize the landscape of convolutional neural net-\nwork software used in recent publications in Table 1. While\nour list is incomplete, we have included the toolkits that are\nmost notable to the best of our knowledge. Caﬀe diﬀers from\nother contemporary CNN frameworks in two major ways:\n(1) The implementation is completely C++ based, which\neases integration into existing C++ systems and interfaces\ncommon in industry. The CPU mode removes the barrier of\nspecialized hardware for deployment and experiments once\na model is trained.\n(2) Reference models are provided oﬀ-the-shelf for quick\nexperimentation with state-of-the-art results, without the\nneed for costly re-learning. By ﬁnetuning for related tasks,\nsuch as those explored by [2], these models provide a warm-\nstart to new research and applications. Crucially, we publish\nnot only the trained models but also the recipes and code\nto reproduce them.\n3.\nARCHITECTURE\n3.1\nData Storage\nCaﬀe stores and communicates data in 4-dimensional ar-\nrays called blobs.\nBlobs provide a uniﬁed memory interface, holding batches\nof images (or other data), parameters, or parameter updates.\nBlobs conceal the computational and mental overhead of\nmixed CPU/GPU operation by synchronizing from the CPU\nhost to the GPU device as needed. In practice, one loads\ndata from the disk to a blob in CPU code, calls a CUDA\nkernel to do GPU computation, and ferries the blob oﬀto\nthe next layer, ignoring low-level details while maintaining\na high level of performance. Memory on the host and device\nis allocated on demand (lazily) for eﬃcient memory usage.\n\u0001\u0002\u0003\u0004\u0005\u0001\u0006\u0006\u0007\b\u0002\b\t\n\u000b\f\r\u000e\n\u0001\u0002\u0003\n\f\t\u0006\u000f\u0003\u0004\u0005\f\t\u0006\u000f\u000e\n\f\t\u0006\u000f\u0003\n\u0002\t\t\u0010\u0011\n\n\u0012\r\u0012\n\f\t\u0006\u000f\u0011\u0004\u0005\f\t\u0006\u000f\u000e\n\u0002\t\t\u0010\u0003\n\u0001\u0002\u0011\u0004\u0005\u0001\u0006\u0006\u0007\b\u0002\b\t\n\u000b\f\r\u000e\n\u0010\t\u0013\u0013\u0004\u0005\u0013\t\u0014\r\u0015\u0012\u0016\u0017\u0010\t\u0013\u0013\u000e\n\u0001\u0002\u0011\n\u0015\u0006\u0001\u0013\r\u0018\u0001\u0006\u0002\u000b\r\u0004\u0005\n\u0012\r\u0012\u000e\n\u0010\u0012\u0019\u0007\u0010\n\f\t\u0006\u000f\u0011\n\u0002\t\t\u0010\u0011\u0004\u0005\u0002\t\t\u0010\u000e\n\u0002\t\t\u0010\u0003\u0004\u0005\u0002\t\t\u0010\u000e\n\b\u0007\u0010\u000b\u0011\u0004\u0005\b\u0007\u0010\u000b\u000e\n\b\u0007\u0010\u000b\u0011\nFigure 1: An MNIST digit classiﬁcation example of a Caﬀe network, where blue boxes represent layers and\nyellow octagons represent data blobs produced by or fed into the layers.\nModels are saved to disk as Google Protocol Buﬀers1,\nwhich have several important features: minimal-size binary\nstrings when serialized, eﬃcient serialization, a human-readable\ntext format compatible with the binary version, and eﬃ-\ncient interface implementations in multiple languages, most\nnotably C++ and Python.\nLarge-scale data is stored in LevelDB2 databases. In our\ntest program, LevelDB and Protocol Buﬀers provide a through-\nput of 150MB/s on commodity machines with minimal CPU\nimpact. Thanks to layer-wise design (discussed below) and\ncode modularity, we have recently added support for other\ndata sources, including some contributed by the open source\ncommunity.\n3.2\nLayers\nA Caﬀe layer is the essence of a neural network layer: it\ntakes one or more blobs as input, and yields one or more\nblobs as output. Layers have two key responsibilities for the\noperation of the network as a whole: a forward pass that\ntakes the inputs and produces the outputs, and a backward\npass that takes the gradient with respect to the output, and\ncomputes the gradients with respect to the parameters and\nto the inputs, which are in turn back-propagated to earlier\nlayers.\nCaﬀe provides a complete set of layer types including: con-\nvolution, pooling, inner products, nonlinearities like rectiﬁed\nlinear and logistic, local response normalization, element-\nwise operations, and losses like softmax and hinge. These are\nall the types needed for state-of-the-art visual tasks. Coding\ncustom layers requires minimal eﬀort due to the composi-\ntional construction of networks.\n3.3\nNetworks and Run Mode\nCaﬀe does all the bookkeeping for any directed acyclic\ngraph of layers, ensuring correctness of the forward and\nbackward passes. Caﬀe models are end-to-end machine learn-\ning systems. A typical network begins with a data layer that\nloads from disk and ends with a loss layer that computes the\nobjective for a task such as classiﬁcation or reconstruction.\nThe network is run on CPU or GPU by setting a single\nswitch.\nLayers come with corresponding CPU and GPU\nroutines that produce identical results (with tests to prove\nit). The CPU/GPU switch is seamless and independent of\nthe model deﬁnition.\n3.4\nTraining A Network\nCaﬀe trains models by the fast and standard stochastic\ngradient descent algorithm.\nFigure 1 shows a typical ex-\nample of a Caﬀe network (for MNIST digit classiﬁcation)\nduring training: a data layer fetches the images and labels\n1https://code.google.com/p/protobuf/\n2https://code.google.com/p/leveldb/\nFigure 2: An example of the Caﬀe object classiﬁca-\ntion demo. Try it out yourself online!\nfrom disk, passes it through multiple layers such as con-\nvolution, pooling and rectiﬁed linear transforms, and feeds\nthe ﬁnal prediction into a classiﬁcation loss layer that pro-\nduces the loss and gradients which train the whole network.\nThis example is found in the Caﬀe source code at exam-\nples/lenet/lenet_train.prototxt. Data are processed in\nmini-batches that pass through the network sequentially. Vi-\ntal to training are learning rate decay schedules, momentum,\nand snapshots for stopping and resuming, all of which are\nimplemented and documented.\nFinetuning, the adaptation of an existing model to new\narchitectures or data, is a standard method in Caﬀe. From\na snapshot of an existing network and a model deﬁnition for\nthe new network, Caﬀe ﬁnetunes the old model weights for\nthe new task and initializes new weights as needed. This\ncapability is essential for tasks such as knowledge transfer\n[2], object detection [3], and object retrieval [5].\n4.\nAPPLICATIONS AND EXAMPLES\nIn its ﬁrst six months since public release, Caﬀe has al-\nready been used in a large number of research projects at\nUC Berkeley and other universities, achieving state-of-the-\nart performance on a number of tasks. Members of Berkeley\nEECS have also collaborated with several industry partners\nsuch as Facebook [11] and Adobe [6], using Caﬀe or its direct\nprecursor (Decaf) to obtain state-of-the-art results.\nObject Classiﬁcation Caﬀe has an online demo3 show-\ning state-of-the-art object classiﬁcation on images provided\nby the users, including via mobile phone. The demo takes\nthe image and tries to categorize it into one of the 1,000 Im-\nageNet categories4. A typical classiﬁcation result is shown\nin Figure 2.\nFurthermore, we have successfully trained a model with\nall 10,000 categories of the full ImageNet dataset by ﬁne-\ntuning this network. The resulting network has been applied\nto open vocabulary object retrieval [5].\n3http://demo.caffe.berkeleyvision.org/\n4http://www.image-net.org/challenges/LSVRC/2013/\nFigure 3:\nFeatures extracted from a deep network,\nvisualized in a 2-dimensional space. Note the clear\nseparation between categories, indicative of a suc-\ncessful embedding.\nLearning Semantic Features In addition to end-to-end\ntraining, Caﬀe can also be used to extract semantic features\nfrom images using a pre-trained network.\nThese features\ncan be used “downstream” in other vision tasks with great\nsuccess [2]. Figure 3 shows a two-dimensional embedding\nof all the ImageNet validation images, colored by a coarse\ncategory that they come from. The nice separation testiﬁes\nto a successful semantic embedding.\nIntriguingly, this learned feature is useful for a lot more\nthan object categories. For example, Karayev et al. have\nshown promising results ﬁnding images of diﬀerent styles\nsuch as “Vintage” and “Romantic” using Caﬀe features (Fig-\nure 4) [6].\nEthereal\nHDR\nMelancholy\nMinimal\nFigure 4:\nTop three most-conﬁdent positive pre-\ndictions on the Flickr Style dataset, using a Caﬀe-\ntrained classiﬁer.\nObject Detection Most notably, Caﬀe has enabled us\nto obtain by far the best performance on object detection,\nevaluated on the hardest academic datasets: the PASCAL\nVOC 2007-2012 and the ImageNet 2013 Detection challenge\n[3].\nGirshick et al. [3] have combined Caﬀe together with tech-\nniques such as Selective Search [10] to eﬀectively perform\nsimultaneous localization and recognition in natural images.\nFigure 5 shows a sketch of their approach.\nBeginners’ Guides To help users get started with in-\nstalling, using, and modifying Caﬀe, we have provided in-\nstructions and tutorials on the Caﬀe webpage. The tuto-\nrials range from small demos (MNIST digit recognition) to\nserious deployments (end-to-end learning on ImageNet).\nAlthough these tutorials serve as eﬀective documentation\nof the functionality of Caﬀe, the Caﬀe source code addition-\nally provides detailed inline documentation on all modules.\n1. Input \nimage\n2. Extract region \nproposals (~2k)\n3. Compute \nCNN features\naeroplane? no.\n...\nperson? yes.\ntvmonitor? no.\n4. Classify \nregions\nwarped region\n...\nCNN\nR-CNN: Regions with CNN features\nFigure 5: The R-CNN pipeline that uses Caﬀe for\nobject detection.\nThis documentation will be exposed in a standalone web\ninterface in the near future.\n5.\nAVAILABILITY\nSource code is published BSD-licensed on GitHub.5 Project\ndetails, step-wise tutorials, and pre-trained models are on\nthe homepage.6 Development is done in Linux and OS X,\nand users have reported Windows builds.\nA public Caﬀe\nAmazon EC2 instance is coming soon.\n6.\nACKNOWLEDGEMENTS\nWe would like to thank NVIDIA for GPU donation, the\nBVLC sponsors (http://bvlc.eecs.berkeley.edu/), and\nour open source community.\n7.\nREFERENCES\n[1] R. Collobert, K. Kavukcuoglu, and C. Farabet. Torch7: A\nMATLAB-like environment for machine learning. In\nBigLearn, NIPS Workshop, 2011.\n[2] J. Donahue, Y. Jia, O. Vinyals, J. Hoﬀman, N. Zhang,\nE. Tzeng, and T. Darrell. Decaf: A deep convolutional\nactivation feature for generic visual recognition. In ICML,\n2014.\n[3] R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich\nfeature hierarchies for accurate object detection and\nsemantic segmentation. In CVPR, 2014.\n[4] I. Goodfellow, D. Warde-Farley, P. Lamblin, V. Dumoulin,\nM. Mirza, R. Pascanu, J. Bergstra, F. Bastien, and\nY. Bengio. Pylearn2: a machine learning research library.\narXiv preprint 1308.4214, 2013.\n[5] S. Guadarrama, E. Rodner, K. Saenko, N. Zhang,\nR. Farrell, J. Donahue, and T. Darrell. Open-vocabulary\nobject retrieval. In RSS, 2014.\n[6] S. Karayev, M. Trentacoste, H. Han, A. Agarwala,\nT. Darrell, A. Hertzmann, and H. Winnemoeller.\nRecognizing image style. arXiv preprint 1311.3715, 2013.\n[7] A. Krizhevsky. cuda-convnet.\nhttps://code.google.com/p/cuda-convnet/, 2012.\n[8] A. Krizhevsky, I. Sutskever, and G. Hinton. ImageNet\nclassiﬁcation with deep convolutional neural networks. In\nNIPS, 2012.\n[9] P. Sermanet, D. Eigen, X. Zhang, M. Mathieu, R. Fergus,\nand Y. LeCun. Overfeat: Integrated recognition,\nlocalization and detection using convolutional networks. In\nICLR, 2014.\n[10] J. Uijlings, K. van de Sande, T. Gevers, and A. Smeulders.\nSelective search for object recognition. IJCV, 2013.\n[11] N. Zhang, M. Paluri, M. Ranzato, T. Darrell, and\nL. Bourdev. Panda: Pose aligned networks for deep\nattribute modeling. In CVPR, 2014.\n5https://github.com/BVLC/caffe/\n6http://caffe.berkeleyvision.org/\n",
        "sentence": " 3 million-image ILSVRC 2012 ImageNet dataset [1] using the Caffe-framework [21]."
    },
    {
        "title": "Deep manifold traversal: Changing labels with convolutional features",
        "author": [
            "Jacob R. Gardner",
            "Paul Upchurch",
            "Matt J. Kusner",
            "Yixuan Li",
            "Kilian Q. Weinberger",
            "John E. Hopcroft"
        ],
        "venue": "arXiv preprint arXiv:1511.06421,",
        "citeRegEx": "22",
        "shortCiteRegEx": "22",
        "year": 2015,
        "abstract": "Many tasks in computer vision can be cast as a \"label changing\" problem,\nwhere the goal is to make a semantic change to the appearance of an image or\nsome subject in an image in order to alter the class membership. Although\nsuccessful task-specific methods have been developed for some label changing\napplications, to date no general purpose method exists. Motivated by this we\npropose deep manifold traversal, a method that addresses the problem in its\nmost general form: it first approximates the manifold of natural images then\nmorphs a test image along a traversal path away from a source class and towards\na target class while staying near the manifold throughout. The resulting\nalgorithm is surprisingly effective and versatile. It is completely data\ndriven, requiring only an example set of images from the desired source and\ntarget domains. We demonstrate deep manifold traversal on highly diverse label\nchanging tasks: changing an individual's appearance (age and hair color),\nchanging the season of an outdoor image, and transforming a city skyline\ntowards nighttime.",
        "full_text": "Deep Manifold Traversal: Changing Labels with\nConvolutional Features\nJacob R. Gardner*, Paul Upchurch*, Matt J. Kusner, Yixuan Li, Kilian Q.\nWeinberger, Kavita Bala, John E. Hopcroft\nCornell University, Washington University in St. Louis\n*Authors contributing equally\nAbstract. Many tasks in computer vision can be cast as a “label chang-\ning” problem, where the goal is to make a semantic change to the appear-\nance of an image or some subject in an image in order to alter the class\nmembership. Although successful task-speciﬁc methods have been devel-\noped for some label changing applications, to date no general purpose\nmethod exists. Motivated by this we propose deep manifold traversal,\na method that addresses the problem in its most general form: it ﬁrst\napproximates the manifold of natural images then morphs a test image\nalong a traversal path away from a source class and towards a target\nclass while staying near the manifold throughout. The resulting algo-\nrithm is surprisingly eﬀective and versatile. It is completely data driven,\nrequiring only an example set of images from the desired source and tar-\nget domains. We demonstrate deep manifold traversal on highly diverse\nlabel changing tasks: changing an individual’s appearance (age and hair\ncolor), changing the season of an outdoor image, and transforming a city\nskyline towards nighttime.\n1\nIntroduction\nMany tasks in computer vision can be cast as a label changing problem: given\nan input image, change the label of that image from some label ys to some\ntarget label yt. Recent examples of this general task include changing facial\nexpressions and hairstyle [1,2], example-based image colorization [3,4], aging of\nfaces [5,6], material editing [6], editing of outdoor scenes [7] changing seasons\n[8], and image morphs [9], relighting of photos [10,11] or hallucinating a night\nimage from a day image [12]. A variety of specialized algorithms exist for each\nof these tasks. However, these algorithms often incorporate substantial domain-\nspeciﬁc prior knowledge relevant to the task and may require hand annotation\nof images, rendering them unable to perform any other task. For example, it is\nunlikely that a facial aging algorithm would be able to change the season of an\noutdoor scene.\nThis motivates research into the most general form of changing image ap-\npearances. Our goal is to design a method that takes as input a set of source\nand target images (e.g. images of young and old people) and changes a given\ntest image to be semantically more similar to the target than the source images.\narXiv:1511.06421v3  [cs.LG]  17 Mar 2016\n2\nAuthors Suppressed Due to Excessive Length\nA given image could be transformed into a target image through linear inter-\npolation in pixel space. However, intermediate images would not be meaningful\nbecause the set of natural images does not span a linear subspace in the pixel\nspace. Instead, it is believed to constitute a low dimensional sub-manifold [13].\nIn order to make meaningful changes, the image traversal path must be conﬁned\nto the underlying manifold throughout.\nBengio et al. 2012 [14] hypothesizes that deep convolutional networks lin-\nearize the manifold of natural images into a subspace of deep features. This sug-\ngests that convolutional networks, and in particular the feature space learned by\nsuch networks, may be a natural choice for solving the label changing problem.\nHowever, recent work [15,16] has demonstrated that this problem can be surpris-\ningly hard for machine learning algorithms. In the context of object classiﬁcation\nthrough convolutional neural networks, it has been shown possible to change the\nprediction of an image with tiny alterations that can be imperceptible to humans.\nSuch changes do not aﬀect the appearance of the image and leave the class label\nuntouched [15]. In fact, the problem of changing class labels persists for most\ndiscriminative machine learning algorithms [17] and is still an open problem to\ndate.\nIn this paper we investigate how to make meaningful changes to input im-\nages while staying on the underlying manifold. We follow the intuition by Bengio\net al. 2012 [14] and utilize a deep convolutional network trained on 1.2 million\nimages [18] to simplify the manifold of natural images to a linear feature space.\nWe avoid the diﬃculty pointed out by Szegedy et al [15] by using kernel Maxi-\nmum Mean Discrepancy (MMD) [19] to estimate the distributions of source and\ntarget images in this feature space to guide the traversal. The traversal stays\non the manifold, because it is conﬁned to the subspace of deep features and is\nforced by the MMD guide to regions that correspond to likely images. Each point\nalong the path can be mapped back to an image with reverse image reconstruc-\ntion [20]. Furthermore, our method is linear in space and time so it naturally\nscales to large images (e.g., 900×600), which is much larger than most results\ndemonstrated by generative models.\nIn a nutshell, our algorithm works in three steps: 1. Source, target and the test\nimages are forward propagated through a convolutional network and mapped\ninto a deep feature space; 2. MMD is used to guide the traversal of the test\nimage in the deep feature space towards the target and away from the source\ndistribution while staying close to the manifold of natural images; 3. a point\nalong the traversal path is speciﬁed and a corresponding image is generated\nthrough reverse image reconstruction.\nThe resulting algorithm allows us to traverse the manifold of natural images\nfreely in a completely data-driven way. We only require labeled images from\nthe source and target classes, and no hand annotation (e.g., correspondences\nor strokes). While this method certainly does not replace specialized methods,\nit may function as a baseline for a wide variety of tasks, or perhaps enable\nsome tasks for which no specialized algorithms have been derived. Our results\nindicate that our method is highly general and performs better than current\nDeep Manifold Traversal: Changing Labels with Convolutional Features\n3\ngeneral methods (which make use of image morphing) on a number of diﬀerent\ntasks.\n2\nRelated Work\nSzegedy et al. [15] were the ﬁrst to show that deep networks can be ‘easily con-\nvinced’ that an input is in a diﬀerent class, by making subtle, imperceptible\nchanges to the input. Such changed inputs were termed ‘adversarial examples’\nand Goodfellow, et al. [17] showed that these examples are generally problem-\natic for high-dimensional linear classiﬁers. These results indicate it is inherently\ndiﬃcult to meaningfully change the label of an input with small changes.\nIn general, generative networks are somewhat orthogonal to our problem set-\nting, as they [21,22], (a) deal primarily with generating novel images rather than\nchanging existing ones, and (b) are typically restricted to very low resolution im-\nages, such as 32×32.\nMahendran and Vedaldi [20] recovered visual imagery by inverting deep con-\nvolutional feature representations. Their goal was to reveal invariance by compar-\ning a reconstructed image to the original image. Gatys, et al. [23] demonstrated\nhow to transfer the artistic style of famous artists to natural images by opti-\nmizing for feature targets during reconstruction. We draw upon these works as\nmeans to demonstrate our framework in the image domain. Yet, rather than re-\nconstructing imagery or transferring style, we construct new images which have\nthe qualities of a diﬀerent class.\nA few methods in the machine learning literature also deal with data-driven\nchanges to images. Reed et al. [24,25] propose to learn a model to disentangle\nfactors of variation (e.g., identity and viewpoint). In our work, we directly min-\nimize the discrepancy between an image and a target sub-manifold inside the\nsemantic space learned by a convolutional network trained on millions of images.\nAn advantage of our approach is the ability to run on much higher resolution\nimages up to 900x600 in this paper, compared to 48x48 images in [24].\nAnalogical reasoning methods [26,27,28,29,25,30] solve for D in the expres-\nsion: A is to B as C is to D. Other methods generate images in a controlled\nfashion [31,32]. Our method also has multiple inputs but we do not solve for\nanalogies nor do we learn a disentangled model.\nIn concept, our work is similar to methods [33,34,35] which use video or\nphoto collections to capture the personality and character of one person’s face\nand apply it to a diﬀerent person (a form of puppetry [36,37,38]). This diﬃcult\nproblem requires a complex pipeline to achieve high quality results. For example,\nSuwajanakorn et al. [33] combines several vision methods: ﬁducial point detec-\ntion [39], 3D face reconstruction [40], optical ﬂow [41] and texture mapping.\nOur work is conceptually similar because we also use photo collections to deﬁne\nthe source and target. However, we can produce plausible results without any\nadditional machinery and our usage of a high-level semantic CNN feature space\nmakes our method applicable to a wide-variety of domains.\n4\nAuthors Suppressed Due to Excessive Length\nOur task is related to a large body of image morphing work (survey by\nWolberg [42]). Image morphing warps images into an alignment map then color\ninterpolates between mapped points. Unlike image morphing, we do not warp\nimages to a map. A recent work by Liao et al. [9] aligns based on structural\nsimilarity [43]. Their goal is to achieve semantic alignment partially invariant to\nlighting, shape and color. We achieve this with a high-level semantic CNN feature\nspace. Their method also requires manual annotations to reﬁne the mapping\nwhereas our method is fully automated.\nKemelmacher et al. [44] creates plausible transformations between two images\nof the same person by selecting an ordered sequence of photos from a large photo\ncollection. Qualitatively, the person may appear to change expression as if the\nimage was changing. Unlike their method, we actually change the original image\nwhile preserving the clothing and background.\n3\nBackground: Maximum Mean Discrepancy\nThe Maximum Mean Discrepancy [45] (MMD) statistic tests whether two prob-\nability distributions, source P s and target P t, are the same. The MMD metric\nmeasures the maximum diﬀerence between the mean function values:\nMMD(P s, P t, F) = sup\nf∈F\n\u0000E [f(zs)]zs∼P s −E\n\u0002\nf(zt)\n\u0003\nzt∼P t\n\u0001\n(1)\ngiven some function class F. MMD can be thought of as producing a test function\nthat distinguishes samples from these two distributions. In particular, the MMD\ntest function is large when evaluated on samples drawn from a source distribution\nP s, and small when evaluated on samples drawn from a target distribution P t.\nWhen F is a reproducing kernel Hilbert space, the function maximizing this\ndiﬀerence can be found analytically [19], and is called the witness function:\nf ∗(z) = E [k(zs, z)]zs∼P s −E\n\u0002\nk(zt, z)\n\u0003\nzt∼P t\n(2)\nThe MMD using this function is a powerful measure of discrepancy between two\nprobability distributions. For example, it is easy to show that if F is universal,\nthen P s = P t if and only if MMD(P s, P t, F) = 0 [19].\nGiven ﬁnite samples zs\n1, ..., zs\nm\niid\n∼P and zt\n1, ..., zt\nn\niid\n∼P t, the witness function\ncan be estimated empirically:\nf ∗(z) ≈1\nm\nm\nX\ni=1\nk(zs\ni, z) −1\nn\nn\nX\ni=1\nk(zt\ni, z)\n(3)\nIntuitively, f ∗(z) measures the degree to which z is representative of either\nP s — by taking a positive value — or P t — by taking a negative value. In this\nwork, we will make use of the Gaussian kernel, deﬁned as k(z, z′) = e−1\n2σ |z−z′|2,\nwhere σ is the kernel bandwidth. While kernel methods often generalize poorly\non images in pixel space because of violated smoothness assumptions, we expect\nthat these assumptions hold after deep visual feature extraction [46]. For a more\nthorough review of the MMD statistic, see [19].\nDeep Manifold Traversal: Changing Labels with Convolutional Features\n5\n4\nDeep Manifold Traversal\nProcess\nVGG 19-layer Network\nDeep Manifold Traversal\nconv4\nconv3\nconv2\nconv1\nconv5\nconv4\nconv3\nconv2\nconv1\nconv5\nFig. 1. Top: Input image ¯xs is transformed\nby a ConvNet to deep features (orange).\nMiddle: The manifold is traversed (black\narrow) from source, ¯zs, to target, ¯zt, in fea-\nture space. Bottom: ¯zt is inverted to re-\ncover ¯xt, subject to total variation regular-\nizer RV β.\nIn this section, we will discuss our\nmethod for manifold traversal from\none class into another. Importantly,\nany transformation should preserve\nthe class-independent aspects of the\noriginal image, only changing the\nclass-identifying features. In our set-\nting, we are given a labeled set of im-\nages from a source domain, xs\n1, ..., xs\nm\neach with source label ys, and a set of\nlabeled images from a target domain,\nxt\n1, ..., xt\nn each with target label yt.\nWe are also given a speciﬁc input im-\nage ¯xs with label ys. Informally, our\ngoal is to change ¯xs →¯xt in a mean-\ningful way such that ¯xt has true label\nyt. Figure 1 provides an overview of\nour approach.\nManifold representation.\nThe ﬁrst\nstep of our approach is to approxi-\nmate the manifold of natural images\nand obtain a mapping from input im-\nages in pixel space, x, to a high-level\nfeature representation, xi −→φi. By\nmodifying these deep visual features\nrather than the raw pixels of x di-\nrectly, we make changes to the image\nin a space in which the manifold of natural images is simpliﬁed, which more\neasily allows for images to remain on the manifold.\nNetwork details. Following the method of [23] we use the feature representations\nfrom deeper layers of a normalized, 19-layer VGG [18] network. Speciﬁcally,\nwe use layers conv3 1 (256 × 63 × 63), conv4 1 (512 × 32 × 32) and conv5 1\n(512 × 16 × 16), which have the indicated dimensionalities when the color input\nis 250 × 250. These layers are the ﬁrst convolutions in the 3rd, 4th and 5th\npooling regions. After ReLU, ﬂattening and concatenation, a feature vector has\n1.67 million dimensions for a 250 × 250 input image.\nImage transformation. Our approach to image transformation will be to change\nthe deep visual features ¯zs = φ(¯xs) to look more like the deep visual features\ncharacteristic of label yt. Because the deep convolutional network has mapped\nthe original images in to a more linear subspace, we move linearly away from\nsource high-level features and towards target high-level features. Speciﬁcally, we\n6\nAuthors Suppressed Due to Excessive Length\nseek to add some linear combination of the source, target, and test images’ deep\nfeatures:\n¯zt = φ(¯xt) = ¯zs + Vδ.\n(4)\nwhere V ∈RK×D is the matrix of deep convolutional features for the source,\ntarget, and test images: [φt\n1, ..., φt\nn, φs\n1, ..., φs\nm, ¯zs]. This linear combination should\nproduce a set of deep features less like the source domain, more like the target\ndomain, but still strongly like the original image. Thus, δ should ideally contain\nnegative values in most source indices, and positive values in most target indices.\nTo obtain this transformation, we propose an optimization guided by the\nMMD witness function from section 3. We make use of the empirical witness\nfunction f ∗(¯zs + Vδ) to measure the degree to which the transformed VGG\nfeatures ¯zs + Vδ resembles objects with source label ys or those with target\nlabel yt:\nf ∗(¯zs + Vδ) = 1\nm\nm\nX\ni=1\nk(φs\ni, ¯zs + Vδ) −1\nn\nn\nX\nj=1\nk(φt\nj, ¯zs + Vδ).\n(5)\nObserving that–given the deﬁnition of V–each φs\ni and φt\nj and ¯zs can be them-\nselves written as Vei, Vej and VeK for one-hot vectors ei, ej, and eK we rewrite\nthe above as:\nf ∗(VeK + Vδ) = 1\nm\nm\nX\ni=1\nk(Ves\ni, VeK + Vδ) −1\nn\nn\nX\nj=1\nk(Vet\nj, VeK + Vδ).\n(6)\nWhen using the squared exponential kernel, we can factor V:\nf ∗(VeK + Vδ) = 1\nm\nm\nX\ni=1\nexp\n\u001a\n−1\nσ (es\ni −(eK + δ))V⊤V(es\ni −(eK + δ))\n\u001b\n−1\nn\nn\nX\nj=1\nexp\n\u001a\n−1\nσ (et\nj −(eK + δ))V⊤V(et\nj −(eK + δ))\n\u001b\n.\n(7)\nIf the K × K matrix V⊤V is precomputed for a dataset, this function can\nbe computed in time independent of the number of convolutional features, and\ntherefore original image resolution.\nThe witness function f ∗(VeK + Vδ) has a negative value if the transformed\nvisual features VeK +Vδ are more characteristic of label yt than of label ys. To\ntransform ¯xs to have target label yt, we therefore wish to minimize f ∗(Vφ(¯z)s +\nVδ) in δ. However, when performed unbounded, this optimization moves too\nfar along the manifold to a mode of the target domain, preserving little of the\ninformation contained in ¯zs. We therefore follow the techniques used in [15] and\nenforce a budget of change, and instead obtain ¯zt by minimizing:\nφ(¯zt) = V(eK + δ)\nwhere: δ = arg min\nδ\nf ∗(VeK + Vδ) + λ∥Vδ∥2\n2\n(8)\nDeep Manifold Traversal: Changing Labels with Convolutional Features\n7\nMinimizing the witness function encodes two “forces”: φ(¯zs) is pushed away\nfrom visual features characteristic of the source label ys and simultaneously\npulled towards visual features characteristic of the target label yt.\nReconstruction. The optimization results in the transformed representation ¯zt =\nVeK + Vδ. In order to obtain our corresponding target image ¯xt = φ−1(¯zt), we\nneed to “invert” the CNN. The deep CNN mapping is not invertible, so we\ncannot obtain the image in pixel space ¯xt from ¯zt directly. The mapping is\nhowever diﬀerentiable and we can adopt the approaches of [20] and [23] to ﬁnd\n¯xt with gradient descent by minimizing the loss function\nLΩ3,4,5(¯xt) = 1\n2∥Ω3,4,5(¯xt) −¯zt∥2.\n(9)\nRegularization. Following the method of [20], we add a total variation regularizer\nRV β(¯xt) =\nX\ni,j\n\u0000(xi,j+1 −xi,j)2 + (xi+1,j −xi,j)2\u0001 β\n2 .\n(10)\nHere, xi,j refers to the pixel with i, j coordinate in image x. The addition of\nthis regularizer greatly improves image quality. The ﬁnal optimization problem\nbecomes\n¯xt = arg min\n¯xt\nLΩ3,4,5(¯xt) + λV βRV β(¯xt).\n(11)\nWe minimize (11) with bounded L-BFGS initialized with δ = 0. We set λV β =\n0.001 and β = 2 in our experiments. After reconstruction we have completed the\nmanifold traversal from source to the target: ¯xs →¯zs →¯zt →¯xt. We will provide\nsource code for our method on GitHub at http://anonymized.\n5\nExperimental Results - LFW\nWe evaluate our method on several manifold traversal tasks using the Labeled\nFaces in the Wild (LFW) dataset. This dataset contains 13,143 images (250×250)\nof faces with predicted annotations for 73 diﬀerent attributes (e.g., “sunglasses”,\n“soft lighting”, “round face”, “curly hair”, “mustache”, etc.). We use these anno-\ntations as labels for our manifold traversal experiments. Because the predicted\nannotations [47] have label noise, we take the 2,000 most conﬁdently labeled\nimages to construct an image set. For example, in our aging task below, we take\nthe bottom (i.e., most negative) and top (i.e., most positive) 2,000 images in the\n“senior” class as our source and target image sets.\nAll single transformation image results shown for LFW use the same λ value\nof λ = 4e-8. All experiments were run with RBF kernel width σ = 7.7e5. In the\ntasks below, test images were chosen at random, with the exception of Aaron\nEckhart (the ﬁrst image in LFW), who we included in all tests in order to show\nmultiple tasks on the same image. Due to space constraints we only show a small\nnumber of results per experiment. More results are in the supplemental.\n8\nAuthors Suppressed Due to Excessive Length\nOriginal Image\nTraversal to \nmore senior\nFig. 2. (Zoom in for details.) Face aging via manifold traversal on random (except\nAaron Eckhart) 250x250 test images from LFW. All aging results shown were run with\nthe same value of λ.\n5.1\nAging faces via manifold traversal.\nTo demonstrate the ability of our algorithm to make meaningful changes to\nthe true label of a test instance, we ﬁrst consider the task of computationally\naging faces. To do this, we ﬁrst follow our procedure above for selecting 2,000\nsource (young) and target (old) images. We select 7 test images at random plus\nAaron Eckhart from the remainder of LFW. We then perform manifold traversal\ntowards “senior” on these 8 images, using the same value of λ for each traversal.\nThe results of our aging experiment are shown in ﬁgure 2. In each case, deep\nmanifold traversal generates an older-appearing version of the original image by\nadding wrinkles, graying hair and adding bags under eyes. Note that the images\nremain sharp despite the relatively high resolution compared to existing purely\nlearning-based approaches for facial morphing [24].\nOne important aspect of the transformations made by deep manifold traver-\nsal is that changes are localized to the face and hair. Clothing, background,\nlighting, and other features of the image irrelevant to the desired label change\nwere not signiﬁcantly aﬀected. Thus, our algorithm succeeds in preserving as\nmuch character of the original image as possible while still changing the true\nlabel of the image.\nFinally, we note that an advantage of our technique over many other ap-\nproaches is that we do not need a photocollection of the test individual. For\nexample, Aaron Eckhart and Mark Rosenbaum (ﬁrst and 7th column in the\nﬁgure) only have one image in the dataset.\nComparison on aging. In this section, we compare several methods to deep\nmanifold traversal on the aging task. We compare to two alternative data driven\napproaches that motivate the need for performing traversal with deep features.\nFirst, we compare to “shallow” manifold traversal, where we perform our linear\ntraversal algorithm, but in the original pixel space rather than after extracting\ndeep convolutional features. We also compare to interpolation in pixel space be-\ntween the original input image and the average “senior” image. We also compare\nto a state-of-the-art technique for image morphing [9], which only requires one\ntarget image but requires manual annotation of correspondances between the\ntest and target image.\nDeep Manifold Traversal: Changing Labels with Convolutional Features\n9\nLiao et al., 2014\nInterpolation\nShallow MT\nDeep MT\nOriginal Image\n(Annotated for Liao et al)\nExample Target Image\n(Annotated for Liao et al)\nFig. 3. (Zoom in for details.) Several methods used to change the age of an input\nimage of Harrison Ford.\nIn the case of aging, the image morphing algorithm requires both a young\nand an aged photo of the same person, which would not typically be available.\nTherefore, we chose to evaluate the aging task on Harrison Ford, as young and\nold images of him are both readily available from Google image search. For\nthe image morphing baseline, we show the “halfway” image. The annotationed\ncorrespondences are shown as red dots on the original and target image.\nThe results of our experiment are shown in ﬁgure 3. Deep manifold traversal\nclearly perorms better than both of the other data-driven baselines, producing\na sharp image with characteristic aging features. This suggests that traversal in\nthe deep convolutional feature space is indeed necessary. When compared to the\nimage morphing task, the visual clarity of the face are comparable. However,\nthe image morphing algorithm introduces some warping of the face in the inter-\nmediate stages. Perhaps the most obvious diﬀerence between the two methods\nis that deep manifold traversal preserves both the background and the clothing\nof the original image, thus avoiding changes that are irrelevant to the desired\nchange.\nComparison with Szegedy et al. 2014. Existing work has shown that it is possible\nto make imperceptible changes to images so that deep convolutional networks\nmake high-conﬁdence misclassiﬁcations [15]. In this section, we demonstrate that\nwhen we vary λ in our manifold traversal algorithm, we can gradually change\nboth the class label of an image and a machine learning classiﬁer’s prediction,\nnot just the prediction alone.\nTo do this, we use the convolutional layers of VGG as a feature extractor, and\ntrain an SVM using the top 2000 “senior” and “non-senior” faces from LFW to\ndistinguish between VGG features extracted from images with positive “senior”\nattribute values and negative ones. We then use Platt scaling to transform the\nSVM decision values into probabilities [48] ranging between 0 and 1, where lower\nprobability value indicates the likelihood for being more “senior”.\nWe construct adversarial “senior” images–which we display on the left in\nﬁgure 4–as well as perform manifold traversal with three diﬀerent lambdas, which\nwe display on the right in ﬁgure 4. All manifold traversal results were generated\nusing the same set of lambda values: 6e-8, 5e-8, and 4e-8.\nBelow each image is the class probability of “not senior” assigned to that\nimage by the Platt-scaled SVM. In order to make outputs on both sides com-\n10\nAuthors Suppressed Due to Excessive Length\nSzegedy et al. 2014\nManifold traversal\n1\n1\n10-31\n10-34\n10-31\nOriginal\nOutputs: older\nOutputs: older\n10-25\n10-26\n10-34\n10-13\n10-26\n10-13\n10-21\n10-21\n10-25\nFig. 4. (Zoom in for details.) Left: “Aging” images generated using the method of\n[15]. Right: “Aging” images generated by deep manifold traversal. The image progres-\nsion towards the right was generated by gradually decreasing the value of λ. Numbers\nbelow each image show the Platt scaled probabilities of an SVM trained on VGG\nfeatures to distinguish old age, where lower values indicate more “senior”.\nparable, we set the adversarial regularizer so that the adversarial images have\ncomparable decision values to those generated by manifold traversal.\nWe note several important features of this result. First, the original images all\nhave very high probability of being “not senior”. However, after both the adver-\nsarial and the DMT modiﬁcations, we were able to change the SVM prediction\nto be completely conﬁdent that the transformed images were of seniors. We ﬁnd\nthat deep manifold traversal makes meaningful change to the true label of the\nimages as well, clearly aging the person in each image. In contrast, the compara-\nble adversarial images fail to change the original images in a human-perceptibly\nmeaningful way.\n5.2\nChanging hair color via manifold traversal.\nTo show the versatility of manifold traversal, we also perform manifold traversal\nto change hair color. This task is diﬀerent from aging because diﬀerent hair styles\nrequire manifold traversal to focus on a larger variety of shapes than aging does.\nWe perform two traversals: one towards blonde (lighter) hair, and one towards\nblack (darker) hair. To help ensure that the randomly selected test images did\nnot already have blonde or black hair, we selected our 8 random test images\nfrom among the top 90th percentile of the “brown hair” attribute.\nThe results of our hair color experiment are shown in ﬁgure 5. The middle\nrow displays the original images in LFW. The top and bottom row show the\nresults of manifold traversal towards lighter hair (“blonde hair”) and darker\nhair (“black hair”) respectively.\nWe note that the hair color traversal generally succeeded despite the variety\nof hair styles, while again preserving features of the image like clothing and\nbackground. The varying hair styles suggest that manifold traversal is able to\ntransform more complex shapes than simply faces.\nOf particular interest in this experiment are the changes made other than the\ncolor of hair on the top of the head. In most cases facial hair such as eyebrows\nDeep Manifold Traversal: Changing Labels with Convolutional Features\n11\nFig. 5. (Zoom in for details.) Changing hair color of random (except Aaron Eck-\nhart) 250x250 images from LFW with manifold traversal. Top. Manifold traversal to\nlighter hair. Middle. Original image. Bottom. Manifold traversal to darker hair. All\ntraversals were performed with the same value of lambda.\nand beard hair was changed to the appropriate color as well (for example in\nthe ﬁrst column). Furthermore, when traversing to blonde hair, eye color was\noccasionally also changed to blue to match (for example, in the 2nd, 5th, and\n6th columns).\n6\nExperimental Results - AMOS\nDoes our technique work outside the context of faces? To test this, we also\nevaluate our method on two tasks using data from the Archive of Many Outdoor\nScenes (AMOS) collection of webcams [49]. This dataset contains images from\nthousands of webcams taken nearly hourly (with some missing data) over the\ncourse of several years. While this data lacks the rich set of annotations that LFW\nhas, we are able to construct two tasks based on image timestamps–traversing\nfrom winter to summer and traversing from day to dusk.\n6.1\nChanging from winter to summer.\nIn this section, we look at if we can learn to transform images from winter to\nsummer given a speciﬁc webcam. We collect 2762 images from January and\nFebruary to form the source “winter” set, and 2858 images from June and July\nform the target “summer” set. We then select two winter test images which do\nnot occur in either the source or target set and perform deep manifold traversal.\nThe results of both deep manifold traversal and the image morphing algo-\nrithm of [9] on this task are shown in ﬁgure 6. In both test images, deep manifold\ntraversal adds leaves to the trees in the foreground, as well as dense foliage to\nthe forest in the background. In the second test image, the grass is made sig-\nniﬁcantly greener, and the snow on the ground begins to fade. We notice that\nduring partial traversal a tree trunk is added in the second experiment (likely\ndue to a viewpoint change), which fades upon complete traversal.\n12\nAuthors Suppressed Due to Excessive Length\nOriginal Winter\nDeep MT\nManifold traversal to Summer\nDeep MT\nLiao et al, 2014\nLiao et al, 2014\nFig. 6. (Zoom in for details.) Changing from winter to summer with deep manifold\ntraversal (1st and 3rd row). Tree branches are replaced with leaves (red arrows), dirt\nappears at the base of a large tree (yellow arrow). At a partial traversal a tree trunk\n(blue arrow) is duplicated. This may be due to a viewpoint change. For comparison\nwe show image morphing [9] (2nd and 4th row). [9] requires manual annotations (red\ndots) and uses a single target image rather than a photo collection.\nThe image morphing algorithm also performs reasonably well when adding\nleaves to trees, producing leaves of comparable quality to deep manifold traversal.\nHowever, we note two notable image artifacts in the morphing algorithm results.\nFirst, the trunk of the foreground tree is clearly still visible, despite the fact\nthat there is dense foliage. Second, while the image morphing algorithm did not\nduplicate the trunk of the tree on the right, there is signiﬁcant image warping\nnear that tree and the bank of the lake. One possible reason for this may be due to\nthe fact that the image morphing technique relies on a single target image. This\nmeans that, if a natural event causes the camera viewpoint to change slightly,\nthe algorithm must also morph the viewpoint, which may be the cause of the\nodd riverbank location in the morphed image. Deep manifold traversal, however,\nis robust to such changes during full traversal as such small irregularities are not\nvital to the label change.\nDeep Manifold Traversal: Changing Labels with Convolutional Features\n13\nOriginal City\nDeep Manifold Traversal to City Lights\nFig. 7. (Zoom in for details.) Deep manifold traversal at 900×600 pixels. The city\n(left) is changed to make it more similar to nighttime (right). Our data-driven method\nselects multiple factors to change. The tone of the buildings changes from daytime gray\nto nighttime blue and nighttime artiﬁcial lighting appears in windows (red insets). The\nwaterfront pavilion light and car headlamps are reﬂected on the water (blue insets).\n6.2\nScalability\nHow well does our method scale to larger images? As a demonstration, we per-\nformed a manifold traversal on a 4k resolution AMOS webcam. The images were\ndownsampled to 900×600 then a manifold traversal was performed on a random\ntest image. The traversal was from 2051 day images toward 1507 night images —\nday and night selected by timestamp, dawn and dusk excluded. The test image\nwas not one of the source or target images. Figure 7 shows the traversal result.\nOur method found that changing tone, adding artiﬁcial lighting and reﬂections\nof light oﬀthe water (see insets in the ﬁgure) are the cues which make the image\nmore like nighttime. Interestingly, the sky remains blue as it would during the\nday. One hypothesis for this is that, because VGG was trained on an object\nrecognition dataset, the sky is treated as background and not represented in the\nhigh-level feature space–for example, when classifying birds or airplanes, the sky\nis background.\nThe feature matrix is 3559×14088192, which requires 186 GB of storage.\nManifold traversal takes 132 minutes and reconstruction takes 43 minutes. In\ncomparison, LFW (250×250, 2000 source and 2000 target images) requires 25 GB\n(feature matrix is 4001×1671424) and 18 minutes to transform. Our method can\ntransform large images (larger than most generative model demonstrations) and\nis primarily limited by memory constraints. Furthermore, the manifold traversal\ntime is linear in image size.\n7\nDiscussion and Future Work\nIn the LFW experiments we use 2000 source and 2000 target images to deﬁne\nthe manifold. It is possible to use fewer images at the cost of reduced output\nquality (ﬁgure 8). There are ways to address this limitation. Video sources can\ngenerally produce thousands of images easily. Data augmenation could increase\n14\nAuthors Suppressed Due to Excessive Length\nthe eﬀective size of a small image set. Exploring ways to reduce the number of\nimages while maintaining quality would be future research.\n2000\n200\n50\nFig. 8. The eﬀect of varying the\nnumber images used to deﬁne a\nmanifold.\nWe ﬁnd that images must be well aligned.\nFor example, in ﬁgure 6 the small tree on the\nright is displaced between the source and tar-\nget image sets. As a result, a ghostly tree\ntrunk appears at some lambdas (but disap-\npears when lambda is suﬃciently small). We\nnote that only the subject needs to be aligned.\nFor example, there is variety in the LFW\nbackgrounds yet this does not prevent our\nmethod from operating on the aligned faces. It may be possible to overcome\nthis limitation by incorporating an image alignment mapping [33] or to auto-\nmatically identify photos taken from the same viewpoint [50].\nAlthough we gain much from using VGG features, those features are roughly\n10x larger than the input image. As a result, holding thousands of 960x540 image\nfeature vectors requires over 128 GB of main memory. These limitations can be\novercome by out-of-core methods at the cost of speed. Reducing the size of the\ndeep neural network feature space is future research.\nMany of the best state-of-the-art methods are computational pipelines which\ncombine domain-speciﬁc knowledge and specialized algorithms to solve sub-\nproblems of a larger problem. An exciting direction of future research is to see if\nour generic method can simplify existing state-of-the-art methods by replacing\npieces of the pipeline with our data-driven approach.\nOne possible use case for deep manifold traversal is in data augmentation.\nTypical data augmentation involves transforming images with label-invariant\nchanges such as horizontal ﬂipping, with the goal of constructing a larger dataset.\nIf we seek to train a deep neural network that, for examples, distinguishes be-\ntween young and old faces, we could augment our data by performing manifold\ntraversal on other aspects–such as facial expressions or hair color.\n8\nConclusion\nWe introduced a single general purpose approach to make semantically mean-\ningful changes to images in an automated fashion. In contrast to prior work, our\napproach is not speciﬁc for any given task. We leverage the combination of MMD\nand deep features from convolutional networks to naturally conﬁne the traversal\npath onto the manifold of natural images. The resulting algorithm scales linearly\nin space and time (after pre-processing), is extremely general and only requires\nminimal supervision through example images from source and target domains.\nHowever, we believe that the true power of our method lies in its versatility.\nWithout modiﬁcations it can be applied to changing the appearance of faces,\ncity skylines or nature scenes. As future work we plan to investigate the use\nof manifold traversal for active learning and automated image augmentation as\npre-processing for supervised computer vision tasks. We hope that our work will\nDeep Manifold Traversal: Changing Labels with Convolutional Features\n15\nbe used as a baseline for a variety of computer vision tasks and will enable new\napplication in areas where no specialized algorithms exist.\nReferences\n1. Chai, M., Wang, L., Weng, Y., Yu, Y., Guo, B., Zhou, K.: Single-view hair modeling\nfor portrait manipulation. ACM Transactions on Graphics (TOG) 31(4) (2012)\n116\n2. Kemelmacher-Shlizerman, I.:\nInternet based morphable model.\nIn: Computer\nVision (ICCV), 2013 IEEE International Conference on, IEEE (2013) 3256–3263\n3. Irony, R., Cohen-Or, D., Lischinski, D.: Colorization by example. In: Eurographics\nSymp. on Rendering. Volume 2., Citeseer (2005)\n4. Gupta, R.K., Chia, A.Y.S., Rajan, D., Ng, E.S., Zhiyong, H.: Image colorization\nusing similar images. In: Proceedings of the 20th ACM international conference\non Multimedia, ACM (2012) 369–378\n5. Kemelmacher-Shlizerman, I., Suwajanakorn, S., Seitz, S.M.:\nIllumination-aware\nage progression.\nIn: Computer Vision and Pattern Recognition (CVPR), 2014\nIEEE Conference on, IEEE (2014) 3334–3341\n6. Boyadzhiev, I., Bala, K., Paris, S., Adelson, E.: Band-sifting decomposition for\nimage-based material editing. ACM Trans. Graph. 34(5) (November 2015) 163:1–\n163:16\n7. Laﬀont, P.Y., Ren, Z., Tao, X., Qian, C., Hays, J.: Transient attributes for high-\nlevel understanding and editing of outdoor scenes. ACM Transactions on Graphics\n(TOG) 33(4) (2014) 149\n8. Neubert, P., Sunderhauf, N., Protzel, P.: Appearance change prediction for long-\nterm navigation across seasons. In: Mobile Robots (ECMR), 2013 European Con-\nference on, IEEE (2013) 198–203\n9. Liao, J., Lima, R.S., Nehab, D., Hoppe, H., Sander, P.V., Yu, J.: Automating image\nmorphing using structural similarity on a halfway domain. ACM Transactions on\nGraphics (TOG) 33(5) (2014) 168\n10. Kopf, J., Neubert, B., Chen, B., Cohen, M., Cohen-Or, D., Deussen, O., Uytten-\ndaele, M., Lischinski, D.: Deep photo: Model-based photograph enhancement and\nviewing. In: ACM Transactions on Graphics (TOG). Volume 27., ACM (2008) 116\n11. Laﬀont, P.Y., Bousseau, A., Paris, S., Durand, F., Drettakis, G.: Coherent intrinsic\nimages from photo collections. ACM Transactions on Graphics 31(6) (2012)\n12. Shih, Y., Paris, S., Durand, F., Freeman, W.T.: Data-driven hallucination of dif-\nferent times of day from a single outdoor photo. ACM Transactions on Graphics\n(TOG) 32(6) (2013) 200\n13. Weinberger, K.Q., Saul, L.K.:\nUnsupervised learning of image manifolds by\nsemideﬁnite programming. International Journal of Computer Vision 70(1) (2006)\n77–90\n14. Bengio, Y., Mesnil, G., Dauphin, Y., Rifai, S.: Better mixing via deep representa-\ntions. arXiv preprint arXiv:1207.4404 (2012)\n15. Szegedy, C., Zaremba, W., Sutskever, I., Bruna, J., Erhan, D., Goodfellow, I.,\nFergus, R.: Intriguing properties of neural networks. International Conference on\nLearning Representation (2014)\n16. Nguyen, A., Yosinski, J., Clune, J.: Deep neural networks are easily fooled: High\nconﬁdence predictions for unrecognizable images. arXiv preprint arXiv:1412.1897\n(2014)\n16\nAuthors Suppressed Due to Excessive Length\n17. Goodfellow, I.J., Shlens, J., Szegedy, C.: Explaining and harnessing adversarial\nexamples. arXiv preprint arXiv:1412.6572 (2014)\n18. Simonyan, K., Zisserman, A.: Very deep convolutional networks for large-scale im-\nage recognition. In: International Conference on Learning Representations. (2015)\n19. Gretton, A., Borgwardt, K.M., Rasch, M.J., Sch¨olkopf, B., Smola, A.: A kernel\ntwo-sample test. The Journal of Machine Learning Research 13(1) (2012) 723–773\n20. Mahendran, A., Vedaldi, A.: Understanding deep image representations by invert-\ning them. In: Proceedings of the IEEE Conf. on Computer Vision and Pattern\nRecognition (CVPR). (2015)\n21. Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S.,\nCourville, A., Bengio, Y.: Generative adversarial nets. In: Advances in Neural\nInformation Processing Systems. (2014) 2672–2680\n22. Denton, E.L., Chintala, S., Fergus, R., et al.: Deep generative image models using\na laplacian pyramid of adversarial networks. In: Advances in Neural Information\nProcessing Systems. (2015) 1486–1494\n23. Gatys, L.A., Ecker, A.S., Bethge, M.: A neural algorithm of artistic style. arXiv\npreprint arXiv:1508.06576 (2015)\n24. Reed, S., Sohn, K., Zhang, Y., Lee, H.: Learning to disentangle factors of variation\nwith manifold interaction. In: Proceedings of the 31st International Conference on\nMachine Learning (ICML-14). (2014) 1431–1439\n25. Reed, S.E., Zhang, Y., Zhang, Y., Lee, H.:\nDeep visual analogy-making.\nIn:\nAdvances in Neural Information Processing Systems. (2015) 1252–1260\n26. Tenenbaum, J.B., Freeman, W.T.: Separating style and content with bilinear mod-\nels. Neural computation 12(6) (2000) 1247–1283\n27. Hertzmann, A., Jacobs, C.E., Oliver, N., Curless, B., Salesin, D.H.: Image analo-\ngies. In: Proceedings of the 28th annual conference on Computer graphics and\ninteractive techniques, ACM (2001) 327–340\n28. Memisevic, R., Hinton, G.: Unsupervised learning of image transformations. In:\nComputer Vision and Pattern Recognition, 2007. CVPR’07. IEEE Conference on,\nIEEE (2007) 1–8\n29. Mikolov, T., Sutskever, I., Chen, K., Corrado, G.S., Dean, J.: Distributed repre-\nsentations of words and phrases and their compositionality. In: Advances in neural\ninformation processing systems. (2013) 3111–3119\n30. Sadeghi, F., Zitnick, C.L., Farhadi, A.: Visalogy: Answering visual analogy ques-\ntions. In: Advances in Neural Information Processing Systems. (2015) 1873–1881\n31. Dosovitskiy, A., Tobias Springenberg, J., Brox, T.: Learning to generate chairs\nwith convolutional neural networks. In: Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition. (2015) 1538–1546\n32. Kulkarni, T.D., Whitney, W.F., Kohli, P., Tenenbaum, J.:\nDeep convolutional\ninverse graphics network. In: Advances in Neural Information Processing Systems.\n(2015) 2530–2538\n33. Suwajanakorn, S., Seitz, S.M., Kemelmacher-Shlizerman, I.:\nWhat makes tom\nhanks look like tom hanks. In: Proceedings of the IEEE International Conference\non Computer Vision. (2015) 3952–3960\n34. Garrido, P., Valgaerts, L., Rehmsen, O., Thormahlen, T., Perez, P., Theobalt, C.:\nAutomatic face reenactment. In: Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition. (2014) 4217–4224\n35. Thies, J., Zollh¨ofer, M., Nießner, M., Valgaerts, L., Stamminger, M., Theobalt,\nC.: Real-time expression transfer for facial reenactment. ACM Transactions on\nGraphics (TOG) 34(6) (2015) 183\nDeep Manifold Traversal: Changing Labels with Convolutional Features\n17\n36. Sumner, R.W., Popovi´c, J.: Deformation transfer for triangle meshes. In: ACM\nTransactions on Graphics (TOG). Volume 23., ACM (2004) 399–405\n37. Weise, T., Li, H., Van Gool, L., Pauly, M.: Face/oﬀ: Live facial puppetry. In:\nProceedings of the 2009 ACM SIGGRAPH/Eurographics Symposium on Computer\nanimation, ACM (2009) 7–16\n38. Kholgade, N., Matthews, I., Sheikh, Y.:\nContent retargeting using parameter-\nparallel facial layers. In: Proceedings of the 2011 ACM SIGGRAPH/Eurographics\nSymposium on Computer Animation, ACM (2011) 195–204\n39. Xiong, X., Torre, F.: Supervised descent method and its applications to face align-\nment. In: Proceedings of the IEEE conference on computer vision and pattern\nrecognition. (2013) 532–539\n40. Suwajanakorn, S., Kemelmacher-Shlizerman, I., Seitz, S.M.:\nTotal moving face\nreconstruction. In: Computer Vision–ECCV 2014. Springer (2014) 796–812\n41. Kemelmacher-Shlizerman, I., Seitz, S.M.: Collection ﬂow. In: Computer Vision and\nPattern Recognition (CVPR), 2012 IEEE Conference on, IEEE (2012) 1792–1799\n42. Wolberg, G.: Image morphing: a survey. The visual computer 14(8) (1998) 360–372\n43. Wang, Z., Bovik, A.C., Sheikh, H.R., Simoncelli, E.P.: Image quality assessment:\nfrom error visibility to structural similarity. Image Processing, IEEE Transactions\non 13(4) (2004) 600–612\n44. Kemelmacher-Shlizerman, I., Shechtman, E., Garg, R., Seitz, S.M.:\nExploring\nphotobios. In: ACM Transactions on Graphics (TOG). Volume 30., ACM (2011)\n61\n45. Fortet, R., Mourier, E.: Convergence de la r´epartition empirique vers la r´epartition\nth´eorique. Annales scientiﬁques de l’´Ecole Normale Sup´erieure 70(3) (1953) 267–\n285\n46. Bengio, Y., LeCun, Y., et al.: Scaling learning algorithms towards ai. Large-scale\nkernel machines 34(5) (2007)\n47. Kumar, N., Berg, A.C., Belhumeur, P.N., Nayar, S.K.: Attribute and simile classi-\nﬁers for face veriﬁcation. In: IEEE International Conference on Computer Vision\n(ICCV). (Oct 2009)\n48. Platt, J., et al.: Probabilistic outputs for support vector machines and comparisons\nto regularized likelihood methods. Advances in large margin classiﬁers 10(3) (1999)\n61–74\n49. Jacobs, N., Roman, N., Pless, R.: Consistent temporal variations in many outdoor\nscenes.\nIn: Computer Vision and Pattern Recognition, 2007. CVPR’07. IEEE\nConference on, IEEE (2007) 1–6\n50. Snavely, N., Seitz, S.M., Szeliski, R.:\nModeling the world from internet photo\ncollections. International Journal of Computer Vision 80(2) (2008) 189–210\n",
        "sentence": " What is the difference of a trained network and a random weight network with the same architecture, and how could we explore the difference? What else could one do using the generative power of untrained, random weight networks? Explore other visualization tasks in computer vision developed on the well-trained network, such as image morphing [22], would be a promising aspect."
    }
]